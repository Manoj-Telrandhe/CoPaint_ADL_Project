2024-12-01-22:46:46-root-INFO: Prepare model...
2024-12-01-22:47:01-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-01-22:47:27-root-INFO: Start sampling
2024-12-01-22:47:34-root-INFO: step: 249 lr_xt 0.00019059
2024-12-01-22:47:34-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-01-22:47:34-root-INFO: Loss too large (77070.016->78612.734)! Learning rate decreased to 0.00015.
2024-12-01-22:47:35-root-INFO: grad norm: 15661.119 11248.847 10896.517
2024-12-01-22:47:36-root-INFO: grad norm: 14205.484 10378.063 9700.079
2024-12-01-22:47:37-root-INFO: grad norm: 15590.165 11542.289 10479.924
2024-12-01-22:47:37-root-INFO: Loss too large (28301.689->35849.891)! Learning rate decreased to 0.00012.
2024-12-01-22:47:38-root-INFO: grad norm: 16983.947 12430.700 11572.905
2024-12-01-22:47:38-root-INFO: Loss too large (28112.924->29010.963)! Learning rate decreased to 0.00010.
2024-12-01-22:47:39-root-INFO: grad norm: 13497.759 10437.687 8558.282
2024-12-01-22:47:40-root-INFO: grad norm: 13451.685 10626.441 8247.822
2024-12-01-22:47:40-root-INFO: Loss too large (22321.516->22378.939)! Learning rate decreased to 0.00008.
2024-12-01-22:47:41-root-INFO: grad norm: 9588.011 7719.607 5686.617
2024-12-01-22:47:42-root-INFO: Loss Change: 77070.016 -> 18636.641
2024-12-01-22:47:42-root-INFO: Regularization Change: 0.000 -> 18.363
2024-12-01-22:47:42-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-01-22:47:42-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-22:47:42-root-INFO: step: 248 lr_xt 0.00020082
2024-12-01-22:47:43-root-INFO: grad norm: 7519.618 5979.487 4559.648
2024-12-01-22:47:43-root-INFO: Loss too large (18707.439->30939.887)! Learning rate decreased to 0.00016.
2024-12-01-22:47:43-root-INFO: Loss too large (18707.439->24725.209)! Learning rate decreased to 0.00013.
2024-12-01-22:47:43-root-INFO: Loss too large (18707.439->21085.277)! Learning rate decreased to 0.00010.
2024-12-01-22:47:44-root-INFO: Loss too large (18707.439->19047.543)! Learning rate decreased to 0.00008.
2024-12-01-22:47:45-root-INFO: grad norm: 6003.949 4797.414 3610.017
2024-12-01-22:47:46-root-INFO: grad norm: 4837.984 3872.228 2900.335
2024-12-01-22:47:46-root-INFO: grad norm: 3852.226 3060.125 2339.931
2024-12-01-22:47:47-root-INFO: grad norm: 3088.244 2486.528 1831.510
2024-12-01-22:47:48-root-INFO: grad norm: 2466.972 1954.534 1505.240
2024-12-01-22:47:49-root-INFO: grad norm: 1997.205 1618.510 1170.152
2024-12-01-22:47:50-root-INFO: grad norm: 1630.513 1298.123 986.636
2024-12-01-22:47:51-root-INFO: Loss Change: 18707.439 -> 16490.832
2024-12-01-22:47:51-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-01-22:47:51-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-01-22:47:51-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-22:47:51-root-INFO: step: 247 lr_xt 0.00021156
2024-12-01-22:47:51-root-INFO: grad norm: 1317.367 1091.389 737.784
2024-12-01-22:47:51-root-INFO: Loss too large (16314.063->16375.607)! Learning rate decreased to 0.00017.
2024-12-01-22:47:52-root-INFO: grad norm: 2396.896 1881.360 1485.124
2024-12-01-22:47:53-root-INFO: Loss too large (16294.557->16717.855)! Learning rate decreased to 0.00014.
2024-12-01-22:47:53-root-INFO: Loss too large (16294.557->16415.754)! Learning rate decreased to 0.00011.
2024-12-01-22:47:54-root-INFO: grad norm: 2587.634 2070.535 1552.009
2024-12-01-22:47:55-root-INFO: grad norm: 2841.153 2266.517 1713.200
2024-12-01-22:47:56-root-INFO: grad norm: 3133.720 2503.176 1885.288
2024-12-01-22:47:56-root-INFO: grad norm: 3486.983 2802.552 2074.791
2024-12-01-22:47:57-root-INFO: Loss too large (16179.890->16200.476)! Learning rate decreased to 0.00009.
2024-12-01-22:47:58-root-INFO: grad norm: 2508.692 2005.886 1506.638
2024-12-01-22:47:59-root-INFO: grad norm: 1822.854 1496.066 1041.434
2024-12-01-22:47:59-root-INFO: Loss Change: 16314.063 -> 15773.587
2024-12-01-22:47:59-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-01-22:47:59-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-01-22:47:59-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-22:47:59-root-INFO: step: 246 lr_xt 0.00022285
2024-12-01-22:48:00-root-INFO: grad norm: 1388.054 1137.204 795.902
2024-12-01-22:48:00-root-INFO: Loss too large (15552.621->15703.446)! Learning rate decreased to 0.00018.
2024-12-01-22:48:00-root-INFO: Loss too large (15552.621->15579.428)! Learning rate decreased to 0.00014.
2024-12-01-22:48:01-root-INFO: grad norm: 1924.359 1584.418 1092.144
2024-12-01-22:48:01-root-INFO: Loss too large (15514.295->15563.256)! Learning rate decreased to 0.00011.
2024-12-01-22:48:02-root-INFO: grad norm: 2028.961 1629.542 1208.832
2024-12-01-22:48:03-root-INFO: grad norm: 2161.696 1787.441 1215.723
2024-12-01-22:48:04-root-INFO: grad norm: 2301.691 1840.233 1382.506
2024-12-01-22:48:05-root-INFO: grad norm: 2461.027 2033.062 1386.835
2024-12-01-22:48:06-root-INFO: grad norm: 2626.114 2095.638 1582.648
2024-12-01-22:48:07-root-INFO: grad norm: 2813.728 2318.915 1593.642
2024-12-01-22:48:07-root-INFO: Loss Change: 15552.621 -> 15230.121
2024-12-01-22:48:07-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-01-22:48:07-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-01-22:48:07-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-22:48:08-root-INFO: step: 245 lr_xt 0.00023469
2024-12-01-22:48:08-root-INFO: grad norm: 2958.225 2359.247 1784.671
2024-12-01-22:48:08-root-INFO: Loss too large (15122.838->16865.518)! Learning rate decreased to 0.00019.
2024-12-01-22:48:09-root-INFO: Loss too large (15122.838->15898.766)! Learning rate decreased to 0.00015.
2024-12-01-22:48:09-root-INFO: Loss too large (15122.838->15349.445)! Learning rate decreased to 0.00012.
2024-12-01-22:48:10-root-INFO: grad norm: 3003.776 2476.383 1700.058
2024-12-01-22:48:11-root-INFO: grad norm: 3063.883 2445.738 1845.467
2024-12-01-22:48:12-root-INFO: grad norm: 3136.825 2590.533 1768.844
2024-12-01-22:48:13-root-INFO: grad norm: 3208.263 2562.178 1930.854
2024-12-01-22:48:13-root-INFO: grad norm: 3285.457 2712.746 1853.439
2024-12-01-22:48:14-root-INFO: grad norm: 3359.686 2685.075 2019.372
2024-12-01-22:48:15-root-INFO: grad norm: 3439.363 2837.483 1943.683
2024-12-01-22:48:16-root-INFO: Loss Change: 15122.838 -> 14724.047
2024-12-01-22:48:16-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-01-22:48:16-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-01-22:48:16-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-22:48:16-root-INFO: step: 244 lr_xt 0.00024712
2024-12-01-22:48:16-root-INFO: grad norm: 3397.622 2753.166 1990.958
2024-12-01-22:48:17-root-INFO: Loss too large (14595.439->16827.289)! Learning rate decreased to 0.00020.
2024-12-01-22:48:17-root-INFO: Loss too large (14595.439->15547.885)! Learning rate decreased to 0.00016.
2024-12-01-22:48:17-root-INFO: Loss too large (14595.439->14826.275)! Learning rate decreased to 0.00013.
2024-12-01-22:48:18-root-INFO: grad norm: 3217.122 2666.922 1799.277
2024-12-01-22:48:19-root-INFO: grad norm: 3128.469 2530.781 1839.148
2024-12-01-22:48:20-root-INFO: grad norm: 3062.881 2555.772 1687.977
2024-12-01-22:48:21-root-INFO: grad norm: 3012.812 2432.823 1777.190
2024-12-01-22:48:22-root-INFO: grad norm: 2968.713 2482.037 1628.726
2024-12-01-22:48:22-root-INFO: grad norm: 2930.179 2365.576 1729.161
2024-12-01-22:48:23-root-INFO: grad norm: 2891.449 2419.140 1583.742
2024-12-01-22:48:24-root-INFO: Loss Change: 14595.439 -> 13887.536
2024-12-01-22:48:24-root-INFO: Regularization Change: 0.000 -> 0.588
2024-12-01-22:48:24-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-01-22:48:24-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:48:24-root-INFO: step: 243 lr_xt 0.00026017
2024-12-01-22:48:24-root-INFO: grad norm: 2945.220 2366.531 1753.240
2024-12-01-22:48:25-root-INFO: Loss too large (13795.908->15472.102)! Learning rate decreased to 0.00021.
2024-12-01-22:48:25-root-INFO: Loss too large (13795.908->14497.475)! Learning rate decreased to 0.00017.
2024-12-01-22:48:25-root-INFO: Loss too large (13795.908->13948.880)! Learning rate decreased to 0.00013.
2024-12-01-22:48:26-root-INFO: grad norm: 2746.854 2306.285 1492.065
2024-12-01-22:48:27-root-INFO: grad norm: 2608.020 2104.973 1539.759
2024-12-01-22:48:28-root-INFO: grad norm: 2483.908 2097.583 1330.393
2024-12-01-22:48:29-root-INFO: grad norm: 2376.961 1922.696 1397.564
2024-12-01-22:48:30-root-INFO: grad norm: 2275.743 1927.019 1210.621
2024-12-01-22:48:31-root-INFO: grad norm: 2187.331 1773.529 1280.239
2024-12-01-22:48:32-root-INFO: grad norm: 2101.926 1783.249 1112.707
2024-12-01-22:48:32-root-INFO: Loss Change: 13795.908 -> 13043.015
2024-12-01-22:48:32-root-INFO: Regularization Change: 0.000 -> 0.626
2024-12-01-22:48:32-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-01-22:48:32-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:48:33-root-INFO: step: 242 lr_xt 0.00027387
2024-12-01-22:48:33-root-INFO: grad norm: 1942.960 1606.379 1092.995
2024-12-01-22:48:33-root-INFO: Loss too large (12825.621->13417.414)! Learning rate decreased to 0.00022.
2024-12-01-22:48:33-root-INFO: Loss too large (12825.621->13036.129)! Learning rate decreased to 0.00018.
2024-12-01-22:48:34-root-INFO: Loss too large (12825.621->12826.286)! Learning rate decreased to 0.00014.
2024-12-01-22:48:35-root-INFO: grad norm: 1777.546 1511.255 935.830
2024-12-01-22:48:35-root-INFO: grad norm: 1650.149 1354.940 941.875
2024-12-01-22:48:36-root-INFO: grad norm: 1541.260 1327.013 783.912
2024-12-01-22:48:37-root-INFO: grad norm: 1446.992 1189.778 823.538
2024-12-01-22:48:38-root-INFO: grad norm: 1362.924 1181.670 679.130
2024-12-01-22:48:39-root-INFO: grad norm: 1287.436 1064.129 724.653
2024-12-01-22:48:40-root-INFO: grad norm: 1217.567 1061.821 595.824
2024-12-01-22:48:40-root-INFO: Loss Change: 12825.621 -> 12147.587
2024-12-01-22:48:40-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-01-22:48:40-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-01-22:48:40-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:48:41-root-INFO: step: 241 lr_xt 0.00028824
2024-12-01-22:48:41-root-INFO: grad norm: 1090.465 929.449 570.298
2024-12-01-22:48:41-root-INFO: Loss too large (12037.784->12048.053)! Learning rate decreased to 0.00023.
2024-12-01-22:48:42-root-INFO: grad norm: 1720.255 1472.943 888.661
2024-12-01-22:48:42-root-INFO: Loss too large (11989.389->12138.465)! Learning rate decreased to 0.00018.
2024-12-01-22:48:43-root-INFO: grad norm: 2222.039 1845.812 1237.108
2024-12-01-22:48:44-root-INFO: Loss too large (11976.643->12027.305)! Learning rate decreased to 0.00015.
2024-12-01-22:48:44-root-INFO: grad norm: 1986.465 1697.033 1032.531
2024-12-01-22:48:45-root-INFO: grad norm: 1787.540 1490.120 987.341
2024-12-01-22:48:46-root-INFO: grad norm: 1609.969 1387.264 817.006
2024-12-01-22:48:47-root-INFO: grad norm: 1457.465 1221.544 795.007
2024-12-01-22:48:48-root-INFO: grad norm: 1324.285 1152.752 651.838
2024-12-01-22:48:48-root-INFO: Loss Change: 12037.784 -> 11410.083
2024-12-01-22:48:48-root-INFO: Regularization Change: 0.000 -> 0.833
2024-12-01-22:48:48-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-01-22:48:48-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:48:49-root-INFO: step: 240 lr_xt 0.00030331
2024-12-01-22:48:49-root-INFO: grad norm: 1252.876 1079.758 635.469
2024-12-01-22:48:49-root-INFO: Loss too large (11225.434->11306.651)! Learning rate decreased to 0.00024.
2024-12-01-22:48:50-root-INFO: grad norm: 1999.509 1718.470 1022.202
2024-12-01-22:48:50-root-INFO: Loss too large (11200.448->11460.254)! Learning rate decreased to 0.00019.
2024-12-01-22:48:51-root-INFO: Loss too large (11200.448->11213.080)! Learning rate decreased to 0.00016.
2024-12-01-22:48:51-root-INFO: grad norm: 1722.446 1449.495 930.476
2024-12-01-22:48:52-root-INFO: grad norm: 1493.587 1299.497 736.281
2024-12-01-22:48:53-root-INFO: grad norm: 1311.007 1111.420 695.331
2024-12-01-22:48:54-root-INFO: grad norm: 1160.566 1022.907 548.246
2024-12-01-22:48:55-root-INFO: grad norm: 1039.564 892.307 533.369
2024-12-01-22:48:56-root-INFO: grad norm: 941.447 841.685 421.769
2024-12-01-22:48:56-root-INFO: Loss Change: 11225.434 -> 10576.661
2024-12-01-22:48:56-root-INFO: Regularization Change: 0.000 -> 0.818
2024-12-01-22:48:56-root-INFO: Undo step: 240
2024-12-01-22:48:56-root-INFO: Undo step: 241
2024-12-01-22:48:56-root-INFO: Undo step: 242
2024-12-01-22:48:56-root-INFO: Undo step: 243
2024-12-01-22:48:56-root-INFO: Undo step: 244
2024-12-01-22:48:57-root-INFO: step: 245 lr_xt 0.00023469
2024-12-01-22:48:57-root-INFO: grad norm: 10652.068 7752.218 7305.455
2024-12-01-22:48:57-root-INFO: Loss too large (18129.504->29306.137)! Learning rate decreased to 0.00019.
2024-12-01-22:48:57-root-INFO: Loss too large (18129.504->22130.701)! Learning rate decreased to 0.00015.
2024-12-01-22:48:58-root-INFO: grad norm: 10003.694 7273.771 6867.762
2024-12-01-22:48:59-root-INFO: grad norm: 10209.068 7606.504 6809.271
2024-12-01-22:49:00-root-INFO: Loss too large (16858.938->17805.725)! Learning rate decreased to 0.00012.
2024-12-01-22:49:00-root-INFO: grad norm: 7251.959 5466.224 4765.639
2024-12-01-22:49:01-root-INFO: grad norm: 5378.996 4194.053 3368.014
2024-12-01-22:49:02-root-INFO: grad norm: 4639.674 3594.135 2934.071
2024-12-01-22:49:03-root-INFO: grad norm: 4082.418 3291.093 2415.542
2024-12-01-22:49:04-root-INFO: grad norm: 3796.172 2981.337 2350.010
2024-12-01-22:49:04-root-INFO: Loss Change: 18129.504 -> 13098.807
2024-12-01-22:49:04-root-INFO: Regularization Change: 0.000 -> 1.447
2024-12-01-22:49:04-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-01-22:49:04-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-22:49:05-root-INFO: step: 244 lr_xt 0.00024712
2024-12-01-22:49:05-root-INFO: grad norm: 3530.322 2897.729 2016.516
2024-12-01-22:49:05-root-INFO: Loss too large (12995.697->15110.261)! Learning rate decreased to 0.00020.
2024-12-01-22:49:06-root-INFO: Loss too large (12995.697->13896.571)! Learning rate decreased to 0.00016.
2024-12-01-22:49:06-root-INFO: Loss too large (12995.697->13195.001)! Learning rate decreased to 0.00013.
2024-12-01-22:49:07-root-INFO: grad norm: 3161.829 2544.106 1877.416
2024-12-01-22:49:08-root-INFO: grad norm: 2894.469 2416.490 1593.275
2024-12-01-22:49:09-root-INFO: grad norm: 2707.293 2169.015 1620.126
2024-12-01-22:49:09-root-INFO: grad norm: 2539.404 2136.413 1372.702
2024-12-01-22:49:10-root-INFO: grad norm: 2404.271 1923.106 1442.977
2024-12-01-22:49:11-root-INFO: grad norm: 2277.241 1925.176 1216.357
2024-12-01-22:49:12-root-INFO: grad norm: 2163.301 1731.265 1297.147
2024-12-01-22:49:13-root-INFO: Loss Change: 12995.697 -> 12094.690
2024-12-01-22:49:13-root-INFO: Regularization Change: 0.000 -> 0.639
2024-12-01-22:49:13-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-01-22:49:13-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:49:13-root-INFO: step: 243 lr_xt 0.00026017
2024-12-01-22:49:13-root-INFO: grad norm: 1887.745 1583.562 1027.578
2024-12-01-22:49:14-root-INFO: Loss too large (11850.253->12293.518)! Learning rate decreased to 0.00021.
2024-12-01-22:49:14-root-INFO: Loss too large (11850.253->11989.724)! Learning rate decreased to 0.00017.
2024-12-01-22:49:15-root-INFO: grad norm: 2377.678 1902.028 1426.759
2024-12-01-22:49:15-root-INFO: Loss too large (11822.580->11886.777)! Learning rate decreased to 0.00013.
2024-12-01-22:49:16-root-INFO: grad norm: 2142.635 1816.637 1136.097
2024-12-01-22:49:17-root-INFO: grad norm: 1958.927 1572.540 1168.123
2024-12-01-22:49:18-root-INFO: grad norm: 1798.280 1538.653 930.784
2024-12-01-22:49:18-root-INFO: grad norm: 1662.770 1339.256 985.494
2024-12-01-22:49:19-root-INFO: grad norm: 1538.685 1326.215 780.195
2024-12-01-22:49:20-root-INFO: grad norm: 1431.036 1158.153 840.563
2024-12-01-22:49:21-root-INFO: Loss Change: 11850.253 -> 11248.811
2024-12-01-22:49:21-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-01-22:49:21-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-01-22:49:21-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:49:21-root-INFO: step: 242 lr_xt 0.00027387
2024-12-01-22:49:21-root-INFO: grad norm: 1378.467 1170.648 727.841
2024-12-01-22:49:22-root-INFO: Loss too large (11039.906->11206.756)! Learning rate decreased to 0.00022.
2024-12-01-22:49:22-root-INFO: Loss too large (11039.906->11063.952)! Learning rate decreased to 0.00018.
2024-12-01-22:49:23-root-INFO: grad norm: 1664.195 1352.925 969.092
2024-12-01-22:49:24-root-INFO: grad norm: 2128.325 1805.789 1126.451
2024-12-01-22:49:24-root-INFO: Loss too large (10979.543->11030.674)! Learning rate decreased to 0.00014.
2024-12-01-22:49:25-root-INFO: grad norm: 1868.545 1518.241 1089.222
2024-12-01-22:49:26-root-INFO: grad norm: 1645.802 1413.517 842.991
2024-12-01-22:49:27-root-INFO: grad norm: 1464.905 1194.969 847.346
2024-12-01-22:49:28-root-INFO: grad norm: 1305.808 1135.184 645.361
2024-12-01-22:49:29-root-INFO: grad norm: 1169.396 961.256 665.938
2024-12-01-22:49:29-root-INFO: Loss Change: 11039.906 -> 10539.056
2024-12-01-22:49:29-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-01-22:49:29-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-01-22:49:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:49:29-root-INFO: step: 241 lr_xt 0.00028824
2024-12-01-22:49:30-root-INFO: grad norm: 1108.340 959.714 554.407
2024-12-01-22:49:30-root-INFO: Loss too large (10438.283->10530.087)! Learning rate decreased to 0.00023.
2024-12-01-22:49:30-root-INFO: Loss too large (10438.283->10440.939)! Learning rate decreased to 0.00018.
2024-12-01-22:49:31-root-INFO: grad norm: 1313.892 1091.924 730.763
2024-12-01-22:49:32-root-INFO: grad norm: 1616.765 1386.101 832.257
2024-12-01-22:49:33-root-INFO: Loss too large (10368.969->10370.104)! Learning rate decreased to 0.00015.
2024-12-01-22:49:34-root-INFO: grad norm: 1372.524 1136.832 769.047
2024-12-01-22:49:34-root-INFO: grad norm: 1171.718 1022.695 571.855
2024-12-01-22:49:35-root-INFO: grad norm: 1013.728 849.445 553.252
2024-12-01-22:49:36-root-INFO: grad norm: 886.325 791.072 399.720
2024-12-01-22:49:37-root-INFO: grad norm: 788.082 674.103 408.239
2024-12-01-22:49:38-root-INFO: Loss Change: 10438.283 -> 10013.917
2024-12-01-22:49:38-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-01-22:49:38-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-01-22:49:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:49:38-root-INFO: step: 240 lr_xt 0.00030331
2024-12-01-22:49:38-root-INFO: grad norm: 703.714 626.699 320.097
2024-12-01-22:49:39-root-INFO: grad norm: 992.682 832.779 540.275
2024-12-01-22:49:40-root-INFO: Loss too large (9765.995->9840.168)! Learning rate decreased to 0.00024.
2024-12-01-22:49:40-root-INFO: grad norm: 1571.464 1358.702 789.575
2024-12-01-22:49:41-root-INFO: Loss too large (9765.485->9903.620)! Learning rate decreased to 0.00019.
2024-12-01-22:49:42-root-INFO: grad norm: 1852.201 1538.972 1030.638
2024-12-01-22:49:42-root-INFO: Loss too large (9759.311->9768.876)! Learning rate decreased to 0.00016.
2024-12-01-22:49:43-root-INFO: grad norm: 1441.661 1249.436 719.233
2024-12-01-22:49:44-root-INFO: grad norm: 1144.971 958.760 625.889
2024-12-01-22:49:45-root-INFO: grad norm: 926.559 824.462 422.816
2024-12-01-22:49:46-root-INFO: grad norm: 772.084 661.319 398.460
2024-12-01-22:49:46-root-INFO: Loss Change: 9838.956 -> 9434.340
2024-12-01-22:49:46-root-INFO: Regularization Change: 0.000 -> 0.609
2024-12-01-22:49:46-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-22:49:46-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:49:47-root-INFO: step: 239 lr_xt 0.00031912
2024-12-01-22:49:47-root-INFO: grad norm: 638.261 575.365 276.284
2024-12-01-22:49:48-root-INFO: grad norm: 1012.530 839.094 566.691
2024-12-01-22:49:48-root-INFO: Loss too large (9282.635->9367.000)! Learning rate decreased to 0.00026.
2024-12-01-22:49:48-root-INFO: Loss too large (9282.635->9284.328)! Learning rate decreased to 0.00020.
2024-12-01-22:49:49-root-INFO: grad norm: 1084.440 939.640 541.375
2024-12-01-22:49:50-root-INFO: grad norm: 1187.966 991.845 653.840
2024-12-01-22:49:51-root-INFO: grad norm: 1320.925 1143.930 660.505
2024-12-01-22:49:52-root-INFO: grad norm: 1478.112 1235.437 811.488
2024-12-01-22:49:53-root-INFO: grad norm: 1665.955 1434.547 847.044
2024-12-01-22:49:54-root-INFO: grad norm: 1874.539 1566.716 1029.220
2024-12-01-22:49:54-root-INFO: Loss too large (9132.104->9132.209)! Learning rate decreased to 0.00016.
2024-12-01-22:49:55-root-INFO: Loss Change: 9327.952 -> 9016.367
2024-12-01-22:49:55-root-INFO: Regularization Change: 0.000 -> 0.663
2024-12-01-22:49:55-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-22:49:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:49:55-root-INFO: step: 238 lr_xt 0.00033570
2024-12-01-22:49:55-root-INFO: grad norm: 1140.054 971.751 596.175
2024-12-01-22:49:56-root-INFO: Loss too large (8821.750->8925.636)! Learning rate decreased to 0.00027.
2024-12-01-22:49:57-root-INFO: grad norm: 1594.679 1332.421 876.159
2024-12-01-22:49:57-root-INFO: Loss too large (8818.650->8951.741)! Learning rate decreased to 0.00021.
2024-12-01-22:49:58-root-INFO: grad norm: 1709.131 1480.936 853.204
2024-12-01-22:49:59-root-INFO: grad norm: 1833.620 1539.791 995.594
2024-12-01-22:49:59-root-INFO: grad norm: 1975.312 1708.576 991.274
2024-12-01-22:50:00-root-INFO: grad norm: 2122.115 1786.976 1144.590
2024-12-01-22:50:01-root-INFO: grad norm: 2282.748 1969.362 1154.362
2024-12-01-22:50:01-root-INFO: Loss too large (8751.480->8761.985)! Learning rate decreased to 0.00017.
2024-12-01-22:50:02-root-INFO: grad norm: 1556.222 1313.350 834.829
2024-12-01-22:50:03-root-INFO: Loss Change: 8821.750 -> 8483.542
2024-12-01-22:50:03-root-INFO: Regularization Change: 0.000 -> 0.548
2024-12-01-22:50:03-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-22:50:03-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:50:03-root-INFO: step: 237 lr_xt 0.00035308
2024-12-01-22:50:03-root-INFO: grad norm: 772.380 694.849 337.279
2024-12-01-22:50:04-root-INFO: Loss too large (8395.471->8406.859)! Learning rate decreased to 0.00028.
2024-12-01-22:50:05-root-INFO: grad norm: 1037.517 881.981 546.399
2024-12-01-22:50:05-root-INFO: Loss too large (8369.178->8387.182)! Learning rate decreased to 0.00023.
2024-12-01-22:50:06-root-INFO: grad norm: 1060.641 945.920 479.786
2024-12-01-22:50:07-root-INFO: grad norm: 1089.375 927.514 571.364
2024-12-01-22:50:08-root-INFO: grad norm: 1121.375 995.477 516.243
2024-12-01-22:50:09-root-INFO: grad norm: 1153.344 982.233 604.500
2024-12-01-22:50:10-root-INFO: grad norm: 1185.810 1048.530 553.832
2024-12-01-22:50:10-root-INFO: grad norm: 1216.672 1036.243 637.566
2024-12-01-22:50:11-root-INFO: Loss Change: 8395.471 -> 8134.676
2024-12-01-22:50:11-root-INFO: Regularization Change: 0.000 -> 0.589
2024-12-01-22:50:11-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-01-22:50:11-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:50:11-root-INFO: step: 236 lr_xt 0.00037130
2024-12-01-22:50:11-root-INFO: grad norm: 1141.246 998.816 552.096
2024-12-01-22:50:12-root-INFO: Loss too large (7986.059->8163.712)! Learning rate decreased to 0.00030.
2024-12-01-22:50:12-root-INFO: Loss too large (7986.059->8023.642)! Learning rate decreased to 0.00024.
2024-12-01-22:50:13-root-INFO: grad norm: 1141.315 980.980 583.333
2024-12-01-22:50:14-root-INFO: grad norm: 1144.037 1007.705 541.620
2024-12-01-22:50:15-root-INFO: grad norm: 1145.387 982.285 589.091
2024-12-01-22:50:16-root-INFO: grad norm: 1144.942 1009.917 539.407
2024-12-01-22:50:16-root-INFO: grad norm: 1142.044 978.666 588.624
2024-12-01-22:50:17-root-INFO: grad norm: 1137.821 1004.378 534.661
2024-12-01-22:50:18-root-INFO: grad norm: 1130.647 968.752 582.995
2024-12-01-22:50:19-root-INFO: Loss Change: 7986.059 -> 7704.528
2024-12-01-22:50:19-root-INFO: Regularization Change: 0.000 -> 0.534
2024-12-01-22:50:19-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-01-22:50:19-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:50:19-root-INFO: step: 235 lr_xt 0.00039040
2024-12-01-22:50:19-root-INFO: grad norm: 737.539 662.556 324.012
2024-12-01-22:50:19-root-INFO: Loss too large (7584.374->7609.599)! Learning rate decreased to 0.00031.
2024-12-01-22:50:20-root-INFO: grad norm: 966.705 829.579 496.303
2024-12-01-22:50:21-root-INFO: Loss too large (7566.756->7579.002)! Learning rate decreased to 0.00025.
2024-12-01-22:50:21-root-INFO: grad norm: 922.571 826.320 410.283
2024-12-01-22:50:22-root-INFO: grad norm: 886.635 765.627 447.144
2024-12-01-22:50:23-root-INFO: grad norm: 852.858 766.704 373.541
2024-12-01-22:50:24-root-INFO: grad norm: 820.830 711.200 409.824
2024-12-01-22:50:25-root-INFO: grad norm: 790.137 712.547 341.459
2024-12-01-22:50:26-root-INFO: grad norm: 761.579 661.911 376.665
2024-12-01-22:50:26-root-INFO: Loss Change: 7584.374 -> 7322.411
2024-12-01-22:50:26-root-INFO: Regularization Change: 0.000 -> 0.543
2024-12-01-22:50:26-root-INFO: Undo step: 235
2024-12-01-22:50:26-root-INFO: Undo step: 236
2024-12-01-22:50:26-root-INFO: Undo step: 237
2024-12-01-22:50:26-root-INFO: Undo step: 238
2024-12-01-22:50:26-root-INFO: Undo step: 239
2024-12-01-22:50:26-root-INFO: step: 240 lr_xt 0.00030331
2024-12-01-22:50:27-root-INFO: grad norm: 9861.724 8250.400 5402.267
2024-12-01-22:50:27-root-INFO: Loss too large (14239.250->24368.225)! Learning rate decreased to 0.00024.
2024-12-01-22:50:27-root-INFO: Loss too large (14239.250->18182.379)! Learning rate decreased to 0.00019.
2024-12-01-22:50:28-root-INFO: grad norm: 9017.941 7411.253 5137.763
2024-12-01-22:50:29-root-INFO: grad norm: 8595.501 7121.753 4812.824
2024-12-01-22:50:29-root-INFO: Loss too large (12667.936->13111.750)! Learning rate decreased to 0.00016.
2024-12-01-22:50:30-root-INFO: grad norm: 5632.345 4711.009 3087.022
2024-12-01-22:50:31-root-INFO: grad norm: 3568.275 2975.689 1969.229
2024-12-01-22:50:32-root-INFO: grad norm: 2523.760 2106.447 1390.053
2024-12-01-22:50:33-root-INFO: grad norm: 1796.112 1527.221 945.311
2024-12-01-22:50:33-root-INFO: grad norm: 1331.873 1111.758 733.402
2024-12-01-22:50:34-root-INFO: Loss Change: 14239.250 -> 8969.070
2024-12-01-22:50:34-root-INFO: Regularization Change: 0.000 -> 1.717
2024-12-01-22:50:34-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-22:50:34-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:50:34-root-INFO: step: 239 lr_xt 0.00031912
2024-12-01-22:50:35-root-INFO: grad norm: 900.349 783.947 442.783
2024-12-01-22:50:35-root-INFO: Loss too large (8831.061->8855.849)! Learning rate decreased to 0.00026.
2024-12-01-22:50:36-root-INFO: grad norm: 1273.162 1062.404 701.599
2024-12-01-22:50:36-root-INFO: Loss too large (8806.023->8848.390)! Learning rate decreased to 0.00020.
2024-12-01-22:50:37-root-INFO: grad norm: 1363.687 1170.584 699.553
2024-12-01-22:50:38-root-INFO: grad norm: 1467.368 1229.053 801.621
2024-12-01-22:50:39-root-INFO: grad norm: 1587.701 1359.369 820.311
2024-12-01-22:50:40-root-INFO: grad norm: 1715.549 1438.304 935.089
2024-12-01-22:50:40-root-INFO: grad norm: 1859.402 1586.521 969.704
2024-12-01-22:50:41-root-INFO: grad norm: 2007.965 1684.078 1093.528
2024-12-01-22:50:42-root-INFO: Loss Change: 8831.061 -> 8639.167
2024-12-01-22:50:42-root-INFO: Regularization Change: 0.000 -> 0.668
2024-12-01-22:50:42-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-22:50:42-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:50:42-root-INFO: step: 238 lr_xt 0.00033570
2024-12-01-22:50:43-root-INFO: grad norm: 1778.918 1501.000 954.750
2024-12-01-22:50:43-root-INFO: Loss too large (8408.122->8848.449)! Learning rate decreased to 0.00027.
2024-12-01-22:50:43-root-INFO: Loss too large (8408.122->8523.351)! Learning rate decreased to 0.00021.
2024-12-01-22:50:44-root-INFO: grad norm: 1740.689 1469.641 932.821
2024-12-01-22:50:45-root-INFO: grad norm: 1783.306 1540.607 898.170
2024-12-01-22:50:46-root-INFO: grad norm: 1829.945 1546.378 978.476
2024-12-01-22:50:47-root-INFO: grad norm: 1884.997 1626.021 953.557
2024-12-01-22:50:48-root-INFO: grad norm: 1937.255 1639.996 1031.198
2024-12-01-22:50:49-root-INFO: grad norm: 1994.409 1717.330 1014.121
2024-12-01-22:50:49-root-INFO: grad norm: 2049.129 1736.679 1087.600
2024-12-01-22:50:50-root-INFO: Loss Change: 8408.122 -> 8158.927
2024-12-01-22:50:50-root-INFO: Regularization Change: 0.000 -> 0.527
2024-12-01-22:50:50-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-22:50:50-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:50:50-root-INFO: step: 237 lr_xt 0.00035308
2024-12-01-22:50:51-root-INFO: grad norm: 1641.530 1420.366 822.909
2024-12-01-22:50:51-root-INFO: Loss too large (8010.581->8438.461)! Learning rate decreased to 0.00028.
2024-12-01-22:50:51-root-INFO: Loss too large (8010.581->8137.898)! Learning rate decreased to 0.00023.
2024-12-01-22:50:52-root-INFO: grad norm: 1615.923 1377.674 844.524
2024-12-01-22:50:53-root-INFO: grad norm: 1602.745 1394.536 789.975
2024-12-01-22:50:54-root-INFO: grad norm: 1591.157 1358.337 828.674
2024-12-01-22:50:55-root-INFO: grad norm: 1577.942 1372.131 779.203
2024-12-01-22:50:56-root-INFO: grad norm: 1564.743 1337.601 811.938
2024-12-01-22:50:57-root-INFO: grad norm: 1550.944 1348.098 766.850
2024-12-01-22:50:57-root-INFO: grad norm: 1538.500 1316.498 796.126
2024-12-01-22:50:58-root-INFO: Loss Change: 8010.581 -> 7729.644
2024-12-01-22:50:58-root-INFO: Regularization Change: 0.000 -> 0.471
2024-12-01-22:50:58-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-01-22:50:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:50:58-root-INFO: step: 236 lr_xt 0.00037130
2024-12-01-22:50:59-root-INFO: grad norm: 1353.501 1170.128 680.267
2024-12-01-22:50:59-root-INFO: Loss too large (7571.622->7857.739)! Learning rate decreased to 0.00030.
2024-12-01-22:50:59-root-INFO: Loss too large (7571.622->7648.981)! Learning rate decreased to 0.00024.
2024-12-01-22:51:00-root-INFO: grad norm: 1312.852 1131.713 665.436
2024-12-01-22:51:01-root-INFO: grad norm: 1278.476 1113.015 629.045
2024-12-01-22:51:02-root-INFO: grad norm: 1246.078 1074.069 631.734
2024-12-01-22:51:03-root-INFO: grad norm: 1212.809 1058.002 592.906
2024-12-01-22:51:04-root-INFO: grad norm: 1179.971 1017.600 597.345
2024-12-01-22:51:05-root-INFO: grad norm: 1147.120 1002.472 557.615
2024-12-01-22:51:06-root-INFO: grad norm: 1116.374 963.440 563.980
2024-12-01-22:51:06-root-INFO: Loss Change: 7571.622 -> 7301.018
2024-12-01-22:51:06-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-01-22:51:06-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-01-22:51:06-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-22:51:07-root-INFO: step: 235 lr_xt 0.00039040
2024-12-01-22:51:07-root-INFO: grad norm: 663.289 587.377 308.123
2024-12-01-22:51:07-root-INFO: Loss too large (7188.518->7204.297)! Learning rate decreased to 0.00031.
2024-12-01-22:51:08-root-INFO: grad norm: 839.216 727.840 417.772
2024-12-01-22:51:08-root-INFO: Loss too large (7171.390->7175.652)! Learning rate decreased to 0.00025.
2024-12-01-22:51:09-root-INFO: grad norm: 787.937 701.974 357.879
2024-12-01-22:51:10-root-INFO: grad norm: 746.386 652.299 362.766
2024-12-01-22:51:11-root-INFO: grad norm: 706.763 632.260 315.851
2024-12-01-22:51:12-root-INFO: grad norm: 671.609 589.512 321.767
2024-12-01-22:51:13-root-INFO: grad norm: 637.897 573.052 280.221
2024-12-01-22:51:14-root-INFO: grad norm: 608.896 536.920 287.178
2024-12-01-22:51:14-root-INFO: Loss Change: 7188.518 -> 6972.130
2024-12-01-22:51:14-root-INFO: Regularization Change: 0.000 -> 0.434
2024-12-01-22:51:14-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-01-22:51:14-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-22:51:14-root-INFO: step: 234 lr_xt 0.00041042
2024-12-01-22:51:15-root-INFO: grad norm: 475.204 428.149 206.173
2024-12-01-22:51:16-root-INFO: grad norm: 743.703 649.998 361.382
2024-12-01-22:51:16-root-INFO: Loss too large (6895.149->6948.695)! Learning rate decreased to 0.00033.
2024-12-01-22:51:17-root-INFO: grad norm: 985.589 866.569 469.514
2024-12-01-22:51:17-root-INFO: Loss too large (6892.675->6920.562)! Learning rate decreased to 0.00026.
2024-12-01-22:51:18-root-INFO: grad norm: 905.108 789.927 441.856
2024-12-01-22:51:19-root-INFO: grad norm: 831.734 735.508 388.342
2024-12-01-22:51:20-root-INFO: grad norm: 769.002 673.599 370.984
2024-12-01-22:51:21-root-INFO: grad norm: 710.002 631.227 325.048
2024-12-01-22:51:22-root-INFO: grad norm: 658.607 579.625 312.726
2024-12-01-22:51:22-root-INFO: Loss Change: 6911.684 -> 6717.248
2024-12-01-22:51:22-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-01-22:51:22-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-22:51:22-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:51:22-root-INFO: step: 233 lr_xt 0.00043139
2024-12-01-22:51:23-root-INFO: grad norm: 524.308 448.450 271.646
2024-12-01-22:51:24-root-INFO: grad norm: 694.629 596.579 355.812
2024-12-01-22:51:24-root-INFO: Loss too large (6670.826->6702.674)! Learning rate decreased to 0.00035.
2024-12-01-22:51:25-root-INFO: grad norm: 856.979 748.256 417.762
2024-12-01-22:51:25-root-INFO: Loss too large (6657.663->6664.887)! Learning rate decreased to 0.00028.
2024-12-01-22:51:26-root-INFO: grad norm: 752.468 659.499 362.311
2024-12-01-22:51:27-root-INFO: grad norm: 670.077 595.397 307.419
2024-12-01-22:51:28-root-INFO: grad norm: 602.129 532.500 281.075
2024-12-01-22:51:28-root-INFO: grad norm: 543.454 487.857 239.454
2024-12-01-22:51:29-root-INFO: grad norm: 495.629 442.236 223.775
2024-12-01-22:51:30-root-INFO: Loss Change: 6703.206 -> 6485.905
2024-12-01-22:51:30-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-01-22:51:30-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-01-22:51:30-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:51:30-root-INFO: step: 232 lr_xt 0.00045336
2024-12-01-22:51:30-root-INFO: grad norm: 462.763 388.043 252.135
2024-12-01-22:51:31-root-INFO: grad norm: 467.688 400.275 241.893
2024-12-01-22:51:32-root-INFO: grad norm: 629.638 534.945 332.081
2024-12-01-22:51:32-root-INFO: Loss too large (6309.175->6328.438)! Learning rate decreased to 0.00036.
2024-12-01-22:51:33-root-INFO: grad norm: 740.874 647.584 359.903
2024-12-01-22:51:34-root-INFO: grad norm: 927.246 807.420 455.915
2024-12-01-22:51:35-root-INFO: Loss too large (6291.688->6310.384)! Learning rate decreased to 0.00029.
2024-12-01-22:51:35-root-INFO: grad norm: 803.961 712.780 371.885
2024-12-01-22:51:36-root-INFO: grad norm: 711.844 627.384 336.321
2024-12-01-22:51:37-root-INFO: grad norm: 632.007 564.726 283.757
2024-12-01-22:51:38-root-INFO: Loss Change: 6376.846 -> 6170.462
2024-12-01-22:51:38-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-01-22:51:38-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-01-22:51:38-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:51:38-root-INFO: step: 231 lr_xt 0.00047637
2024-12-01-22:51:38-root-INFO: grad norm: 724.592 641.680 336.571
2024-12-01-22:51:39-root-INFO: Loss too large (6141.776->6207.094)! Learning rate decreased to 0.00038.
2024-12-01-22:51:39-root-INFO: Loss too large (6141.776->6142.885)! Learning rate decreased to 0.00030.
2024-12-01-22:51:40-root-INFO: grad norm: 629.856 562.815 282.768
2024-12-01-22:51:41-root-INFO: grad norm: 556.897 494.672 255.800
2024-12-01-22:51:41-root-INFO: grad norm: 493.951 445.325 213.713
2024-12-01-22:51:42-root-INFO: grad norm: 443.012 396.380 197.845
2024-12-01-22:51:43-root-INFO: grad norm: 400.185 364.546 165.090
2024-12-01-22:51:44-root-INFO: grad norm: 365.746 330.513 156.623
2024-12-01-22:51:45-root-INFO: grad norm: 337.858 311.130 131.705
2024-12-01-22:51:46-root-INFO: Loss Change: 6141.776 -> 5953.734
2024-12-01-22:51:46-root-INFO: Regularization Change: 0.000 -> 0.380
2024-12-01-22:51:46-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-01-22:51:46-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:51:46-root-INFO: step: 230 lr_xt 0.00050047
2024-12-01-22:51:46-root-INFO: grad norm: 554.527 489.093 261.321
2024-12-01-22:51:46-root-INFO: Loss too large (5908.240->5927.376)! Learning rate decreased to 0.00040.
2024-12-01-22:51:47-root-INFO: grad norm: 676.846 605.730 302.013
2024-12-01-22:51:48-root-INFO: Loss too large (5895.470->5895.589)! Learning rate decreased to 0.00032.
2024-12-01-22:51:48-root-INFO: grad norm: 581.514 514.087 271.795
2024-12-01-22:51:49-root-INFO: grad norm: 502.471 453.025 217.360
2024-12-01-22:51:50-root-INFO: grad norm: 441.402 393.597 199.793
2024-12-01-22:51:51-root-INFO: grad norm: 391.240 355.993 162.290
2024-12-01-22:51:52-root-INFO: grad norm: 351.898 317.431 151.886
2024-12-01-22:51:53-root-INFO: grad norm: 320.531 294.850 125.712
2024-12-01-22:51:53-root-INFO: Loss Change: 5908.240 -> 5736.740
2024-12-01-22:51:53-root-INFO: Regularization Change: 0.000 -> 0.406
2024-12-01-22:51:53-root-INFO: Undo step: 230
2024-12-01-22:51:53-root-INFO: Undo step: 231
2024-12-01-22:51:53-root-INFO: Undo step: 232
2024-12-01-22:51:53-root-INFO: Undo step: 233
2024-12-01-22:51:53-root-INFO: Undo step: 234
2024-12-01-22:51:54-root-INFO: step: 235 lr_xt 0.00039040
2024-12-01-22:51:54-root-INFO: grad norm: 9872.682 7344.503 6597.585
2024-12-01-22:51:54-root-INFO: Loss too large (17762.184->17954.252)! Learning rate decreased to 0.00031.
2024-12-01-22:51:55-root-INFO: grad norm: 6445.360 4968.700 4105.446
2024-12-01-22:51:56-root-INFO: grad norm: 5121.653 4086.723 3087.074
2024-12-01-22:51:57-root-INFO: grad norm: 5482.946 4527.798 3092.207
2024-12-01-22:51:57-root-INFO: Loss too large (9932.978->10501.189)! Learning rate decreased to 0.00025.
2024-12-01-22:51:58-root-INFO: grad norm: 4288.805 3607.081 2320.088
2024-12-01-22:51:59-root-INFO: grad norm: 3779.035 3203.137 2005.248
2024-12-01-22:52:00-root-INFO: grad norm: 3406.913 2900.583 1787.084
2024-12-01-22:52:01-root-INFO: grad norm: 3144.318 2676.334 1650.446
2024-12-01-22:52:01-root-INFO: Loss Change: 17762.184 -> 7648.310
2024-12-01-22:52:01-root-INFO: Regularization Change: 0.000 -> 5.641
2024-12-01-22:52:01-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-01-22:52:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-22:52:02-root-INFO: step: 234 lr_xt 0.00041042
2024-12-01-22:52:02-root-INFO: grad norm: 2714.097 2322.137 1404.992
2024-12-01-22:52:02-root-INFO: Loss too large (7547.029->8808.642)! Learning rate decreased to 0.00033.
2024-12-01-22:52:02-root-INFO: Loss too large (7547.029->7893.382)! Learning rate decreased to 0.00026.
2024-12-01-22:52:03-root-INFO: grad norm: 2444.570 2090.448 1267.261
2024-12-01-22:52:04-root-INFO: grad norm: 2202.548 1890.579 1130.012
2024-12-01-22:52:05-root-INFO: grad norm: 1998.827 1713.456 1029.261
2024-12-01-22:52:06-root-INFO: grad norm: 1811.967 1559.022 923.403
2024-12-01-22:52:07-root-INFO: grad norm: 1647.886 1416.479 842.091
2024-12-01-22:52:08-root-INFO: grad norm: 1498.284 1292.330 758.115
2024-12-01-22:52:08-root-INFO: grad norm: 1367.782 1179.029 693.339
2024-12-01-22:52:09-root-INFO: Loss Change: 7547.029 -> 6720.572
2024-12-01-22:52:09-root-INFO: Regularization Change: 0.000 -> 0.958
2024-12-01-22:52:09-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-22:52:09-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:52:09-root-INFO: step: 233 lr_xt 0.00043139
2024-12-01-22:52:10-root-INFO: grad norm: 1152.393 979.309 607.423
2024-12-01-22:52:10-root-INFO: Loss too large (6682.414->6806.113)! Learning rate decreased to 0.00035.
2024-12-01-22:52:11-root-INFO: grad norm: 1409.016 1213.095 716.746
2024-12-01-22:52:11-root-INFO: Loss too large (6669.595->6725.032)! Learning rate decreased to 0.00028.
2024-12-01-22:52:12-root-INFO: grad norm: 1226.525 1061.166 615.053
2024-12-01-22:52:13-root-INFO: grad norm: 1076.733 935.221 533.588
2024-12-01-22:52:14-root-INFO: grad norm: 946.987 822.898 468.639
2024-12-01-22:52:14-root-INFO: grad norm: 838.036 731.614 408.711
2024-12-01-22:52:15-root-INFO: grad norm: 742.482 648.493 361.576
2024-12-01-22:52:16-root-INFO: grad norm: 662.312 581.753 316.575
2024-12-01-22:52:17-root-INFO: Loss Change: 6682.414 -> 6330.556
2024-12-01-22:52:17-root-INFO: Regularization Change: 0.000 -> 0.640
2024-12-01-22:52:17-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-01-22:52:17-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:52:17-root-INFO: step: 232 lr_xt 0.00045336
2024-12-01-22:52:17-root-INFO: grad norm: 539.642 450.132 297.648
2024-12-01-22:52:18-root-INFO: grad norm: 470.856 381.446 276.050
2024-12-01-22:52:19-root-INFO: grad norm: 525.650 434.904 295.240
2024-12-01-22:52:20-root-INFO: grad norm: 713.763 582.294 412.785
2024-12-01-22:52:20-root-INFO: Loss too large (6113.987->6142.602)! Learning rate decreased to 0.00036.
2024-12-01-22:52:21-root-INFO: grad norm: 783.430 662.064 418.848
2024-12-01-22:52:22-root-INFO: grad norm: 918.685 787.498 473.106
2024-12-01-22:52:22-root-INFO: Loss too large (6088.535->6097.675)! Learning rate decreased to 0.00029.
2024-12-01-22:52:23-root-INFO: grad norm: 749.937 655.457 364.392
2024-12-01-22:52:24-root-INFO: grad norm: 636.935 561.870 299.982
2024-12-01-22:52:24-root-INFO: Loss Change: 6220.917 -> 5987.577
2024-12-01-22:52:24-root-INFO: Regularization Change: 0.000 -> 0.677
2024-12-01-22:52:24-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-01-22:52:24-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:52:25-root-INFO: step: 231 lr_xt 0.00047637
2024-12-01-22:52:25-root-INFO: grad norm: 420.480 375.652 188.916
2024-12-01-22:52:26-root-INFO: grad norm: 652.308 572.997 311.738
2024-12-01-22:52:26-root-INFO: Loss too large (5900.222->5946.578)! Learning rate decreased to 0.00038.
2024-12-01-22:52:27-root-INFO: grad norm: 806.186 710.837 380.325
2024-12-01-22:52:27-root-INFO: Loss too large (5897.307->5908.377)! Learning rate decreased to 0.00030.
2024-12-01-22:52:28-root-INFO: grad norm: 675.926 597.235 316.522
2024-12-01-22:52:29-root-INFO: grad norm: 570.477 508.062 259.454
2024-12-01-22:52:30-root-INFO: grad norm: 487.444 434.765 220.410
2024-12-01-22:52:31-root-INFO: grad norm: 420.986 379.957 181.279
2024-12-01-22:52:31-root-INFO: grad norm: 369.051 333.815 157.373
2024-12-01-22:52:32-root-INFO: Loss Change: 5906.646 -> 5756.183
2024-12-01-22:52:32-root-INFO: Regularization Change: 0.000 -> 0.390
2024-12-01-22:52:32-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-01-22:52:32-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:52:32-root-INFO: step: 230 lr_xt 0.00050047
2024-12-01-22:52:33-root-INFO: grad norm: 284.185 259.446 115.970
2024-12-01-22:52:34-root-INFO: grad norm: 254.474 231.897 104.788
2024-12-01-22:52:34-root-INFO: grad norm: 248.276 230.822 91.444
2024-12-01-22:52:35-root-INFO: grad norm: 262.501 239.148 108.236
2024-12-01-22:52:36-root-INFO: grad norm: 311.369 279.281 137.669
2024-12-01-22:52:37-root-INFO: grad norm: 432.602 381.929 203.161
2024-12-01-22:52:37-root-INFO: Loss too large (5581.549->5584.653)! Learning rate decreased to 0.00040.
2024-12-01-22:52:38-root-INFO: grad norm: 494.820 436.499 233.057
2024-12-01-22:52:39-root-INFO: grad norm: 585.174 519.301 269.732
2024-12-01-22:52:40-root-INFO: Loss Change: 5694.878 -> 5555.349
2024-12-01-22:52:40-root-INFO: Regularization Change: 0.000 -> 0.667
2024-12-01-22:52:40-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-01-22:52:40-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:52:40-root-INFO: step: 229 lr_xt 0.00052570
2024-12-01-22:52:40-root-INFO: grad norm: 983.387 863.537 470.481
2024-12-01-22:52:41-root-INFO: Loss too large (5569.094->5692.847)! Learning rate decreased to 0.00042.
2024-12-01-22:52:41-root-INFO: grad norm: 1137.296 1011.098 520.695
2024-12-01-22:52:42-root-INFO: Loss too large (5564.103->5599.435)! Learning rate decreased to 0.00034.
2024-12-01-22:52:43-root-INFO: grad norm: 892.774 789.602 416.622
2024-12-01-22:52:43-root-INFO: grad norm: 700.585 626.806 312.943
2024-12-01-22:52:44-root-INFO: grad norm: 561.785 500.067 255.999
2024-12-01-22:52:45-root-INFO: grad norm: 454.820 411.442 193.846
2024-12-01-22:52:46-root-INFO: grad norm: 377.180 340.127 163.029
2024-12-01-22:52:47-root-INFO: grad norm: 319.533 294.302 124.451
2024-12-01-22:52:47-root-INFO: Loss Change: 5569.094 -> 5340.514
2024-12-01-22:52:47-root-INFO: Regularization Change: 0.000 -> 0.408
2024-12-01-22:52:47-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-01-22:52:47-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:52:48-root-INFO: step: 228 lr_xt 0.00055211
2024-12-01-22:52:48-root-INFO: grad norm: 437.710 382.155 213.420
2024-12-01-22:52:49-root-INFO: grad norm: 649.995 574.925 303.240
2024-12-01-22:52:49-root-INFO: Loss too large (5335.774->5383.743)! Learning rate decreased to 0.00044.
2024-12-01-22:52:50-root-INFO: grad norm: 734.268 649.193 343.071
2024-12-01-22:52:51-root-INFO: grad norm: 853.645 761.979 384.836
2024-12-01-22:52:51-root-INFO: Loss too large (5329.217->5340.753)! Learning rate decreased to 0.00035.
2024-12-01-22:52:52-root-INFO: grad norm: 653.222 580.617 299.303
2024-12-01-22:52:53-root-INFO: grad norm: 504.530 455.919 216.075
2024-12-01-22:52:54-root-INFO: grad norm: 400.930 359.947 176.589
2024-12-01-22:52:54-root-INFO: grad norm: 326.874 301.082 127.265
2024-12-01-22:52:55-root-INFO: Loss Change: 5336.729 -> 5195.240
2024-12-01-22:52:55-root-INFO: Regularization Change: 0.000 -> 0.417
2024-12-01-22:52:55-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-01-22:52:55-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:52:55-root-INFO: step: 227 lr_xt 0.00057976
2024-12-01-22:52:56-root-INFO: grad norm: 670.114 585.755 325.490
2024-12-01-22:52:56-root-INFO: Loss too large (5241.291->5257.632)! Learning rate decreased to 0.00046.
2024-12-01-22:52:57-root-INFO: grad norm: 699.229 630.523 302.262
2024-12-01-22:52:58-root-INFO: grad norm: 786.562 699.150 360.373
2024-12-01-22:52:58-root-INFO: grad norm: 903.028 811.321 396.508
2024-12-01-22:52:59-root-INFO: Loss too large (5198.513->5210.609)! Learning rate decreased to 0.00037.
2024-12-01-22:53:00-root-INFO: grad norm: 677.037 603.137 307.578
2024-12-01-22:53:00-root-INFO: grad norm: 510.943 463.806 214.351
2024-12-01-22:53:01-root-INFO: grad norm: 398.054 357.745 174.545
2024-12-01-22:53:02-root-INFO: grad norm: 319.202 295.371 121.020
2024-12-01-22:53:03-root-INFO: Loss Change: 5241.291 -> 5048.061
2024-12-01-22:53:03-root-INFO: Regularization Change: 0.000 -> 0.491
2024-12-01-22:53:03-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-01-22:53:03-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-22:53:03-root-INFO: step: 226 lr_xt 0.00060869
2024-12-01-22:53:03-root-INFO: grad norm: 334.545 285.126 174.996
2024-12-01-22:53:04-root-INFO: grad norm: 389.871 343.664 184.105
2024-12-01-22:53:05-root-INFO: grad norm: 542.968 472.989 266.638
2024-12-01-22:53:05-root-INFO: Loss too large (4989.008->5013.958)! Learning rate decreased to 0.00049.
2024-12-01-22:53:06-root-INFO: grad norm: 580.619 521.368 255.527
2024-12-01-22:53:07-root-INFO: grad norm: 636.261 564.100 294.311
2024-12-01-22:53:08-root-INFO: grad norm: 706.008 636.225 306.048
2024-12-01-22:53:09-root-INFO: grad norm: 783.618 696.435 359.216
2024-12-01-22:53:09-root-INFO: Loss too large (4959.584->4959.776)! Learning rate decreased to 0.00039.
2024-12-01-22:53:10-root-INFO: grad norm: 558.452 506.392 235.448
2024-12-01-22:53:11-root-INFO: Loss Change: 5015.230 -> 4882.819
2024-12-01-22:53:11-root-INFO: Regularization Change: 0.000 -> 0.548
2024-12-01-22:53:11-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-01-22:53:11-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:53:11-root-INFO: step: 225 lr_xt 0.00063896
2024-12-01-22:53:11-root-INFO: grad norm: 765.571 670.260 369.934
2024-12-01-22:53:12-root-INFO: Loss too large (4907.347->4956.689)! Learning rate decreased to 0.00051.
2024-12-01-22:53:12-root-INFO: grad norm: 786.045 713.688 329.418
2024-12-01-22:53:13-root-INFO: grad norm: 854.734 760.922 389.318
2024-12-01-22:53:14-root-INFO: grad norm: 938.440 849.246 399.312
2024-12-01-22:53:14-root-INFO: Loss too large (4870.214->4878.724)! Learning rate decreased to 0.00041.
2024-12-01-22:53:15-root-INFO: grad norm: 660.673 589.212 298.863
2024-12-01-22:53:16-root-INFO: grad norm: 469.804 430.659 187.745
2024-12-01-22:53:17-root-INFO: grad norm: 348.239 312.530 153.607
2024-12-01-22:53:18-root-INFO: grad norm: 270.775 253.642 94.788
2024-12-01-22:53:19-root-INFO: Loss Change: 4907.347 -> 4706.795
2024-12-01-22:53:19-root-INFO: Regularization Change: 0.000 -> 0.485
2024-12-01-22:53:19-root-INFO: Undo step: 225
2024-12-01-22:53:19-root-INFO: Undo step: 226
2024-12-01-22:53:19-root-INFO: Undo step: 227
2024-12-01-22:53:19-root-INFO: Undo step: 228
2024-12-01-22:53:19-root-INFO: Undo step: 229
2024-12-01-22:53:19-root-INFO: step: 230 lr_xt 0.00050047
2024-12-01-22:53:19-root-INFO: grad norm: 3734.171 2866.052 2393.694
2024-12-01-22:53:20-root-INFO: grad norm: 3359.842 2560.602 2175.283
2024-12-01-22:53:21-root-INFO: grad norm: 3540.957 2847.976 2104.141
2024-12-01-22:53:21-root-INFO: Loss too large (7399.718->8070.468)! Learning rate decreased to 0.00040.
2024-12-01-22:53:22-root-INFO: grad norm: 3207.095 2708.592 1717.263
2024-12-01-22:53:22-root-INFO: Loss too large (6825.318->6925.584)! Learning rate decreased to 0.00032.
2024-12-01-22:53:23-root-INFO: grad norm: 2318.996 2053.478 1077.482
2024-12-01-22:53:24-root-INFO: grad norm: 1833.142 1609.013 878.343
2024-12-01-22:53:25-root-INFO: grad norm: 1486.336 1324.616 674.229
2024-12-01-22:53:26-root-INFO: grad norm: 1204.600 1061.640 569.195
2024-12-01-22:53:26-root-INFO: Loss Change: 8411.084 -> 5626.254
2024-12-01-22:53:26-root-INFO: Regularization Change: 0.000 -> 4.145
2024-12-01-22:53:26-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-01-22:53:26-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:53:27-root-INFO: step: 229 lr_xt 0.00052570
2024-12-01-22:53:27-root-INFO: grad norm: 1214.814 1086.886 542.633
2024-12-01-22:53:27-root-INFO: Loss too large (5575.098->5794.936)! Learning rate decreased to 0.00042.
2024-12-01-22:53:28-root-INFO: Loss too large (5575.098->5586.372)! Learning rate decreased to 0.00034.
2024-12-01-22:53:29-root-INFO: grad norm: 935.489 836.216 419.382
2024-12-01-22:53:29-root-INFO: grad norm: 762.043 686.547 330.701
2024-12-01-22:53:30-root-INFO: grad norm: 624.296 558.386 279.196
2024-12-01-22:53:31-root-INFO: grad norm: 522.493 475.401 216.779
2024-12-01-22:53:32-root-INFO: grad norm: 444.247 399.451 194.409
2024-12-01-22:53:33-root-INFO: grad norm: 386.534 355.571 151.583
2024-12-01-22:53:34-root-INFO: grad norm: 341.841 310.505 142.976
2024-12-01-22:53:34-root-INFO: Loss Change: 5575.098 -> 5248.945
2024-12-01-22:53:34-root-INFO: Regularization Change: 0.000 -> 0.550
2024-12-01-22:53:34-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-01-22:53:34-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:53:35-root-INFO: step: 228 lr_xt 0.00055211
2024-12-01-22:53:35-root-INFO: grad norm: 466.718 414.949 213.642
2024-12-01-22:53:36-root-INFO: grad norm: 700.325 620.652 324.417
2024-12-01-22:53:36-root-INFO: Loss too large (5218.193->5276.048)! Learning rate decreased to 0.00044.
2024-12-01-22:53:37-root-INFO: grad norm: 812.602 730.820 355.280
2024-12-01-22:53:37-root-INFO: Loss too large (5212.669->5217.091)! Learning rate decreased to 0.00035.
2024-12-01-22:53:38-root-INFO: grad norm: 640.086 575.288 280.630
2024-12-01-22:53:39-root-INFO: grad norm: 521.706 474.331 217.226
2024-12-01-22:53:40-root-INFO: grad norm: 431.498 391.060 182.381
2024-12-01-22:53:41-root-INFO: grad norm: 364.629 335.146 143.635
2024-12-01-22:53:42-root-INFO: grad norm: 314.235 288.558 124.410
2024-12-01-22:53:42-root-INFO: Loss Change: 5222.341 -> 5054.122
2024-12-01-22:53:42-root-INFO: Regularization Change: 0.000 -> 0.485
2024-12-01-22:53:42-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-01-22:53:42-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-22:53:42-root-INFO: step: 227 lr_xt 0.00057976
2024-12-01-22:53:43-root-INFO: grad norm: 682.727 604.760 316.830
2024-12-01-22:53:43-root-INFO: Loss too large (5077.009->5107.502)! Learning rate decreased to 0.00046.
2024-12-01-22:53:44-root-INFO: grad norm: 755.982 684.085 321.769
2024-12-01-22:53:45-root-INFO: grad norm: 887.041 799.974 383.252
2024-12-01-22:53:45-root-INFO: Loss too large (5051.966->5062.771)! Learning rate decreased to 0.00037.
2024-12-01-22:53:46-root-INFO: grad norm: 690.017 625.011 292.376
2024-12-01-22:53:47-root-INFO: grad norm: 551.750 500.376 232.491
2024-12-01-22:53:48-root-INFO: grad norm: 444.541 405.356 182.490
2024-12-01-22:53:49-root-INFO: grad norm: 367.222 336.534 146.958
2024-12-01-22:53:49-root-INFO: grad norm: 309.852 285.995 119.226
2024-12-01-22:53:50-root-INFO: Loss Change: 5077.009 -> 4882.577
2024-12-01-22:53:50-root-INFO: Regularization Change: 0.000 -> 0.483
2024-12-01-22:53:50-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-01-22:53:50-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-22:53:50-root-INFO: step: 226 lr_xt 0.00060869
2024-12-01-22:53:51-root-INFO: grad norm: 330.473 287.327 163.265
2024-12-01-22:53:52-root-INFO: grad norm: 395.370 347.794 188.034
2024-12-01-22:53:53-root-INFO: grad norm: 572.871 507.872 265.043
2024-12-01-22:53:53-root-INFO: Loss too large (4814.546->4853.062)! Learning rate decreased to 0.00049.
2024-12-01-22:53:54-root-INFO: grad norm: 647.921 584.147 280.310
2024-12-01-22:53:54-root-INFO: Loss too large (4807.023->4808.315)! Learning rate decreased to 0.00039.
2024-12-01-22:53:55-root-INFO: grad norm: 498.034 450.672 211.974
2024-12-01-22:53:56-root-INFO: grad norm: 391.699 359.577 155.345
2024-12-01-22:53:57-root-INFO: grad norm: 317.912 291.112 127.757
2024-12-01-22:53:58-root-INFO: grad norm: 265.509 247.876 95.146
2024-12-01-22:53:58-root-INFO: Loss Change: 4836.456 -> 4705.460
2024-12-01-22:53:58-root-INFO: Regularization Change: 0.000 -> 0.455
2024-12-01-22:53:58-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-01-22:53:58-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:53:58-root-INFO: step: 225 lr_xt 0.00063896
2024-12-01-22:53:59-root-INFO: grad norm: 636.733 560.741 301.660
2024-12-01-22:53:59-root-INFO: Loss too large (4705.275->4738.200)! Learning rate decreased to 0.00051.
2024-12-01-22:54:00-root-INFO: grad norm: 688.382 629.206 279.229
2024-12-01-22:54:00-root-INFO: Loss too large (4683.714->4686.330)! Learning rate decreased to 0.00041.
2024-12-01-22:54:01-root-INFO: grad norm: 531.365 479.214 229.570
2024-12-01-22:54:02-root-INFO: grad norm: 414.298 382.998 157.973
2024-12-01-22:54:03-root-INFO: grad norm: 333.605 304.070 137.238
2024-12-01-22:54:04-root-INFO: grad norm: 275.178 257.889 96.001
2024-12-01-22:54:04-root-INFO: grad norm: 234.818 217.762 87.859
2024-12-01-22:54:05-root-INFO: grad norm: 207.014 197.078 63.363
2024-12-01-22:54:06-root-INFO: Loss Change: 4705.275 -> 4547.146
2024-12-01-22:54:06-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-01-22:54:06-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-01-22:54:06-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:54:06-root-INFO: step: 224 lr_xt 0.00067063
2024-12-01-22:54:06-root-INFO: grad norm: 414.804 365.017 197.039
2024-12-01-22:54:07-root-INFO: grad norm: 612.361 558.657 250.774
2024-12-01-22:54:08-root-INFO: Loss too large (4545.029->4607.163)! Learning rate decreased to 0.00054.
2024-12-01-22:54:08-root-INFO: Loss too large (4545.029->4545.440)! Learning rate decreased to 0.00043.
2024-12-01-22:54:09-root-INFO: grad norm: 474.175 430.840 198.037
2024-12-01-22:54:10-root-INFO: grad norm: 373.746 346.966 138.927
2024-12-01-22:54:11-root-INFO: grad norm: 303.128 277.924 121.016
2024-12-01-22:54:11-root-INFO: grad norm: 251.871 236.880 85.596
2024-12-01-22:54:12-root-INFO: grad norm: 216.008 201.101 78.852
2024-12-01-22:54:13-root-INFO: grad norm: 191.467 182.592 57.617
2024-12-01-22:54:14-root-INFO: Loss Change: 4547.987 -> 4424.693
2024-12-01-22:54:14-root-INFO: Regularization Change: 0.000 -> 0.391
2024-12-01-22:54:14-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-01-22:54:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:54:14-root-INFO: step: 223 lr_xt 0.00070376
2024-12-01-22:54:14-root-INFO: grad norm: 437.653 388.955 200.635
2024-12-01-22:54:15-root-INFO: Loss too large (4430.504->4438.415)! Learning rate decreased to 0.00056.
2024-12-01-22:54:16-root-INFO: grad norm: 468.486 433.006 178.844
2024-12-01-22:54:16-root-INFO: grad norm: 536.342 488.271 221.933
2024-12-01-22:54:17-root-INFO: grad norm: 626.365 578.005 241.337
2024-12-01-22:54:18-root-INFO: Loss too large (4405.759->4412.852)! Learning rate decreased to 0.00045.
2024-12-01-22:54:19-root-INFO: grad norm: 481.120 439.673 195.357
2024-12-01-22:54:19-root-INFO: grad norm: 371.438 345.646 135.996
2024-12-01-22:54:20-root-INFO: grad norm: 294.811 271.161 115.694
2024-12-01-22:54:21-root-INFO: grad norm: 240.245 226.307 80.642
2024-12-01-22:54:22-root-INFO: Loss Change: 4430.504 -> 4311.789
2024-12-01-22:54:22-root-INFO: Regularization Change: 0.000 -> 0.391
2024-12-01-22:54:22-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-22:54:22-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:54:22-root-INFO: step: 222 lr_xt 0.00073840
2024-12-01-22:54:22-root-INFO: grad norm: 266.262 244.450 105.544
2024-12-01-22:54:23-root-INFO: grad norm: 396.361 366.666 150.526
2024-12-01-22:54:24-root-INFO: Loss too large (4243.952->4264.014)! Learning rate decreased to 0.00059.
2024-12-01-22:54:24-root-INFO: grad norm: 448.382 412.021 176.877
2024-12-01-22:54:25-root-INFO: grad norm: 512.536 473.503 196.182
2024-12-01-22:54:26-root-INFO: Loss too large (4234.918->4235.349)! Learning rate decreased to 0.00047.
2024-12-01-22:54:27-root-INFO: grad norm: 385.104 354.532 150.373
2024-12-01-22:54:27-root-INFO: grad norm: 294.600 274.808 106.159
2024-12-01-22:54:28-root-INFO: grad norm: 233.805 217.286 86.324
2024-12-01-22:54:29-root-INFO: grad norm: 193.607 183.178 62.685
2024-12-01-22:54:30-root-INFO: Loss Change: 4247.713 -> 4158.954
2024-12-01-22:54:30-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-01-22:54:30-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-22:54:30-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:54:30-root-INFO: step: 221 lr_xt 0.00077462
2024-12-01-22:54:30-root-INFO: grad norm: 279.117 252.551 118.846
2024-12-01-22:54:31-root-INFO: grad norm: 399.932 370.230 151.245
2024-12-01-22:54:32-root-INFO: Loss too large (4131.661->4151.682)! Learning rate decreased to 0.00062.
2024-12-01-22:54:32-root-INFO: grad norm: 433.915 397.762 173.401
2024-12-01-22:54:33-root-INFO: grad norm: 477.094 442.466 178.445
2024-12-01-22:54:34-root-INFO: grad norm: 525.250 482.552 207.441
2024-12-01-22:54:35-root-INFO: grad norm: 580.076 536.597 220.344
2024-12-01-22:54:35-root-INFO: Loss too large (4114.068->4115.410)! Learning rate decreased to 0.00050.
2024-12-01-22:54:36-root-INFO: grad norm: 412.570 379.516 161.807
2024-12-01-22:54:37-root-INFO: grad norm: 299.365 279.499 107.238
2024-12-01-22:54:38-root-INFO: Loss Change: 4134.551 -> 4047.967
2024-12-01-22:54:38-root-INFO: Regularization Change: 0.000 -> 0.392
2024-12-01-22:54:38-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-22:54:38-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:54:38-root-INFO: step: 220 lr_xt 0.00081248
2024-12-01-22:54:38-root-INFO: grad norm: 548.530 491.585 243.371
2024-12-01-22:54:39-root-INFO: Loss too large (4064.657->4097.517)! Learning rate decreased to 0.00065.
2024-12-01-22:54:39-root-INFO: grad norm: 568.787 527.815 211.965
2024-12-01-22:54:40-root-INFO: grad norm: 617.867 567.138 245.183
2024-12-01-22:54:41-root-INFO: grad norm: 676.758 627.687 253.004
2024-12-01-22:54:42-root-INFO: Loss too large (4039.451->4043.860)! Learning rate decreased to 0.00052.
2024-12-01-22:54:43-root-INFO: grad norm: 473.329 435.970 184.310
2024-12-01-22:54:44-root-INFO: grad norm: 334.628 312.935 118.522
2024-12-01-22:54:44-root-INFO: grad norm: 246.607 228.549 92.630
2024-12-01-22:54:45-root-INFO: grad norm: 191.303 181.227 61.267
2024-12-01-22:54:46-root-INFO: Loss Change: 4064.657 -> 3930.613
2024-12-01-22:54:46-root-INFO: Regularization Change: 0.000 -> 0.421
2024-12-01-22:54:46-root-INFO: Undo step: 220
2024-12-01-22:54:46-root-INFO: Undo step: 221
2024-12-01-22:54:46-root-INFO: Undo step: 222
2024-12-01-22:54:46-root-INFO: Undo step: 223
2024-12-01-22:54:46-root-INFO: Undo step: 224
2024-12-01-22:54:46-root-INFO: step: 225 lr_xt 0.00063896
2024-12-01-22:54:47-root-INFO: grad norm: 1807.181 1484.967 1029.939
2024-12-01-22:54:47-root-INFO: Loss too large (5651.126->5732.896)! Learning rate decreased to 0.00051.
2024-12-01-22:54:48-root-INFO: grad norm: 1839.837 1714.112 668.445
2024-12-01-22:54:48-root-INFO: Loss too large (5372.829->5401.743)! Learning rate decreased to 0.00041.
2024-12-01-22:54:49-root-INFO: grad norm: 1465.162 1309.130 657.935
2024-12-01-22:54:50-root-INFO: grad norm: 1171.402 1090.014 429.011
2024-12-01-22:54:51-root-INFO: grad norm: 941.459 840.431 424.287
2024-12-01-22:54:51-root-INFO: grad norm: 754.381 695.167 292.973
2024-12-01-22:54:52-root-INFO: grad norm: 608.768 541.869 277.447
2024-12-01-22:54:53-root-INFO: grad norm: 495.727 452.617 202.195
2024-12-01-22:54:54-root-INFO: Loss Change: 5651.126 -> 4602.366
2024-12-01-22:54:54-root-INFO: Regularization Change: 0.000 -> 2.283
2024-12-01-22:54:54-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-01-22:54:54-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:54:54-root-INFO: step: 224 lr_xt 0.00067063
2024-12-01-22:54:54-root-INFO: grad norm: 302.885 263.888 148.671
2024-12-01-22:54:55-root-INFO: grad norm: 286.975 251.234 138.694
2024-12-01-22:54:56-root-INFO: grad norm: 337.436 305.899 142.438
2024-12-01-22:54:57-root-INFO: grad norm: 490.624 449.138 197.449
2024-12-01-22:54:57-root-INFO: Loss too large (4469.218->4494.965)! Learning rate decreased to 0.00054.
2024-12-01-22:54:58-root-INFO: grad norm: 565.997 517.111 230.108
2024-12-01-22:54:59-root-INFO: grad norm: 659.889 605.610 262.087
2024-12-01-22:54:59-root-INFO: Loss too large (4456.983->4460.335)! Learning rate decreased to 0.00043.
2024-12-01-22:55:00-root-INFO: grad norm: 506.238 462.915 204.906
2024-12-01-22:55:01-root-INFO: grad norm: 395.608 363.310 156.561
2024-12-01-22:55:02-root-INFO: Loss Change: 4544.908 -> 4377.987
2024-12-01-22:55:02-root-INFO: Regularization Change: 0.000 -> 0.741
2024-12-01-22:55:02-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-01-22:55:02-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:55:02-root-INFO: step: 223 lr_xt 0.00070376
2024-12-01-22:55:02-root-INFO: grad norm: 233.901 209.798 103.414
2024-12-01-22:55:03-root-INFO: grad norm: 196.023 172.504 93.100
2024-12-01-22:55:04-root-INFO: grad norm: 176.721 163.110 68.011
2024-12-01-22:55:05-root-INFO: grad norm: 166.498 152.102 67.723
2024-12-01-22:55:06-root-INFO: grad norm: 162.997 151.883 59.158
2024-12-01-22:55:07-root-INFO: grad norm: 168.564 156.669 62.199
2024-12-01-22:55:07-root-INFO: grad norm: 191.941 178.024 71.755
2024-12-01-22:55:08-root-INFO: grad norm: 251.091 232.552 94.689
2024-12-01-22:55:09-root-INFO: Loss Change: 4350.593 -> 4227.440
2024-12-01-22:55:09-root-INFO: Regularization Change: 0.000 -> 0.756
2024-12-01-22:55:09-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-22:55:09-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:55:09-root-INFO: step: 222 lr_xt 0.00073840
2024-12-01-22:55:10-root-INFO: grad norm: 439.021 402.598 175.083
2024-12-01-22:55:10-root-INFO: Loss too large (4163.799->4189.271)! Learning rate decreased to 0.00059.
2024-12-01-22:55:11-root-INFO: grad norm: 486.473 447.257 191.356
2024-12-01-22:55:12-root-INFO: grad norm: 541.705 497.474 214.392
2024-12-01-22:55:12-root-INFO: grad norm: 605.695 556.180 239.855
2024-12-01-22:55:13-root-INFO: Loss too large (4151.945->4154.316)! Learning rate decreased to 0.00047.
2024-12-01-22:55:14-root-INFO: grad norm: 437.551 401.984 172.800
2024-12-01-22:55:14-root-INFO: grad norm: 321.580 297.253 122.694
2024-12-01-22:55:15-root-INFO: grad norm: 246.200 227.820 93.340
2024-12-01-22:55:16-root-INFO: grad norm: 197.604 184.894 69.725
2024-12-01-22:55:17-root-INFO: Loss Change: 4163.799 -> 4061.498
2024-12-01-22:55:17-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-01-22:55:17-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-22:55:17-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:55:17-root-INFO: step: 221 lr_xt 0.00077462
2024-12-01-22:55:17-root-INFO: grad norm: 268.345 241.909 116.143
2024-12-01-22:55:18-root-INFO: grad norm: 374.982 345.387 146.012
2024-12-01-22:55:18-root-INFO: Loss too large (4037.661->4051.767)! Learning rate decreased to 0.00062.
2024-12-01-22:55:19-root-INFO: grad norm: 395.643 361.542 160.688
2024-12-01-22:55:20-root-INFO: grad norm: 422.094 389.478 162.697
2024-12-01-22:55:21-root-INFO: grad norm: 451.342 413.182 181.632
2024-12-01-22:55:22-root-INFO: grad norm: 483.681 445.286 188.858
2024-12-01-22:55:23-root-INFO: grad norm: 518.486 474.814 208.278
2024-12-01-22:55:24-root-INFO: grad norm: 556.506 511.281 219.751
2024-12-01-22:55:24-root-INFO: Loss Change: 4041.734 -> 3998.608
2024-12-01-22:55:24-root-INFO: Regularization Change: 0.000 -> 0.452
2024-12-01-22:55:24-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-22:55:24-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:55:24-root-INFO: step: 220 lr_xt 0.00081248
2024-12-01-22:55:25-root-INFO: grad norm: 884.921 800.600 376.996
2024-12-01-22:55:25-root-INFO: Loss too large (4052.313->4191.410)! Learning rate decreased to 0.00065.
2024-12-01-22:55:26-root-INFO: grad norm: 915.262 840.690 361.862
2024-12-01-22:55:26-root-INFO: Loss too large (4037.038->4041.819)! Learning rate decreased to 0.00052.
2024-12-01-22:55:27-root-INFO: grad norm: 606.174 553.014 248.238
2024-12-01-22:55:28-root-INFO: grad norm: 402.835 372.826 152.567
2024-12-01-22:55:29-root-INFO: grad norm: 279.252 255.865 111.870
2024-12-01-22:55:30-root-INFO: grad norm: 204.624 191.425 72.299
2024-12-01-22:55:31-root-INFO: grad norm: 162.897 151.544 59.748
2024-12-01-22:55:31-root-INFO: grad norm: 140.747 133.466 44.680
2024-12-01-22:55:32-root-INFO: Loss Change: 4052.313 -> 3847.138
2024-12-01-22:55:32-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-01-22:55:32-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-22:55:32-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:55:32-root-INFO: step: 219 lr_xt 0.00085206
2024-12-01-22:55:33-root-INFO: grad norm: 151.252 137.320 63.408
2024-12-01-22:55:33-root-INFO: grad norm: 152.354 144.929 46.982
2024-12-01-22:55:34-root-INFO: grad norm: 174.151 160.634 67.271
2024-12-01-22:55:35-root-INFO: grad norm: 221.044 207.340 76.621
2024-12-01-22:55:36-root-INFO: grad norm: 304.615 279.868 120.266
2024-12-01-22:55:36-root-INFO: Loss too large (3785.582->3791.030)! Learning rate decreased to 0.00068.
2024-12-01-22:55:37-root-INFO: grad norm: 307.680 285.638 114.359
2024-12-01-22:55:38-root-INFO: grad norm: 311.142 285.830 122.925
2024-12-01-22:55:39-root-INFO: grad norm: 314.958 291.808 118.517
2024-12-01-22:55:39-root-INFO: Loss Change: 3824.800 -> 3750.193
2024-12-01-22:55:39-root-INFO: Regularization Change: 0.000 -> 0.548
2024-12-01-22:55:39-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-01-22:55:39-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-22:55:40-root-INFO: step: 218 lr_xt 0.00089342
2024-12-01-22:55:40-root-INFO: grad norm: 507.592 461.059 212.306
2024-12-01-22:55:40-root-INFO: Loss too large (3760.183->3793.881)! Learning rate decreased to 0.00071.
2024-12-01-22:55:41-root-INFO: grad norm: 494.577 457.447 188.012
2024-12-01-22:55:42-root-INFO: grad norm: 486.937 446.075 195.255
2024-12-01-22:55:43-root-INFO: grad norm: 480.073 443.612 183.518
2024-12-01-22:55:44-root-INFO: grad norm: 474.458 435.911 187.330
2024-12-01-22:55:45-root-INFO: grad norm: 468.849 433.021 179.755
2024-12-01-22:55:46-root-INFO: grad norm: 463.569 426.342 182.014
2024-12-01-22:55:46-root-INFO: grad norm: 458.492 423.333 176.081
2024-12-01-22:55:47-root-INFO: Loss Change: 3760.183 -> 3672.880
2024-12-01-22:55:47-root-INFO: Regularization Change: 0.000 -> 0.442
2024-12-01-22:55:47-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-01-22:55:47-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:55:47-root-INFO: step: 217 lr_xt 0.00093664
2024-12-01-22:55:48-root-INFO: grad norm: 623.106 563.047 266.908
2024-12-01-22:55:48-root-INFO: Loss too large (3698.037->3752.390)! Learning rate decreased to 0.00075.
2024-12-01-22:55:49-root-INFO: grad norm: 584.078 540.299 221.864
2024-12-01-22:55:50-root-INFO: grad norm: 560.225 514.174 222.436
2024-12-01-22:55:51-root-INFO: grad norm: 538.542 498.235 204.424
2024-12-01-22:55:51-root-INFO: grad norm: 519.357 478.219 202.579
2024-12-01-22:55:52-root-INFO: grad norm: 501.014 463.512 190.188
2024-12-01-22:55:53-root-INFO: grad norm: 484.239 446.430 187.583
2024-12-01-22:55:54-root-INFO: grad norm: 468.259 433.272 177.602
2024-12-01-22:55:55-root-INFO: Loss Change: 3698.037 -> 3585.368
2024-12-01-22:55:55-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-01-22:55:55-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-01-22:55:55-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:55:55-root-INFO: step: 216 lr_xt 0.00098179
2024-12-01-22:55:55-root-INFO: grad norm: 632.069 572.496 267.879
2024-12-01-22:55:55-root-INFO: Loss too large (3617.240->3671.569)! Learning rate decreased to 0.00079.
2024-12-01-22:55:56-root-INFO: grad norm: 584.604 539.294 225.663
2024-12-01-22:55:57-root-INFO: grad norm: 555.437 511.333 216.908
2024-12-01-22:55:58-root-INFO: grad norm: 529.453 489.708 201.263
2024-12-01-22:55:59-root-INFO: grad norm: 507.603 469.133 193.844
2024-12-01-22:56:00-root-INFO: grad norm: 487.351 451.373 183.774
2024-12-01-22:56:01-root-INFO: grad norm: 468.967 433.967 177.771
2024-12-01-22:56:01-root-INFO: grad norm: 451.771 418.801 169.417
2024-12-01-22:56:02-root-INFO: Loss Change: 3617.240 -> 3493.460
2024-12-01-22:56:02-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-01-22:56:02-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-01-22:56:02-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:56:02-root-INFO: step: 215 lr_xt 0.00102894
2024-12-01-22:56:03-root-INFO: grad norm: 451.955 417.208 173.783
2024-12-01-22:56:03-root-INFO: Loss too large (3479.332->3507.224)! Learning rate decreased to 0.00082.
2024-12-01-22:56:04-root-INFO: grad norm: 416.593 387.945 151.815
2024-12-01-22:56:05-root-INFO: grad norm: 385.425 356.081 147.508
2024-12-01-22:56:06-root-INFO: grad norm: 357.733 333.894 128.403
2024-12-01-22:56:06-root-INFO: grad norm: 332.673 307.610 126.678
2024-12-01-22:56:07-root-INFO: grad norm: 310.240 289.954 110.342
2024-12-01-22:56:08-root-INFO: grad norm: 290.176 268.582 109.843
2024-12-01-22:56:09-root-INFO: grad norm: 272.321 254.834 96.011
2024-12-01-22:56:10-root-INFO: Loss Change: 3479.332 -> 3383.566
2024-12-01-22:56:10-root-INFO: Regularization Change: 0.000 -> 0.442
2024-12-01-22:56:10-root-INFO: Undo step: 215
2024-12-01-22:56:10-root-INFO: Undo step: 216
2024-12-01-22:56:10-root-INFO: Undo step: 217
2024-12-01-22:56:10-root-INFO: Undo step: 218
2024-12-01-22:56:10-root-INFO: Undo step: 219
2024-12-01-22:56:10-root-INFO: step: 220 lr_xt 0.00081248
2024-12-01-22:56:10-root-INFO: grad norm: 3397.592 2776.112 1958.784
2024-12-01-22:56:11-root-INFO: grad norm: 3542.857 3275.741 1349.576
2024-12-01-22:56:12-root-INFO: Loss too large (7134.868->7974.192)! Learning rate decreased to 0.00065.
2024-12-01-22:56:12-root-INFO: grad norm: 3181.778 2942.267 1211.105
2024-12-01-22:56:13-root-INFO: grad norm: 2824.996 2617.259 1063.278
2024-12-01-22:56:14-root-INFO: grad norm: 2809.523 2536.396 1208.353
2024-12-01-22:56:15-root-INFO: grad norm: 2680.484 2431.174 1128.888
2024-12-01-22:56:16-root-INFO: grad norm: 2539.803 2296.653 1084.429
2024-12-01-22:56:16-root-INFO: Loss too large (5111.545->5120.884)! Learning rate decreased to 0.00052.
2024-12-01-22:56:17-root-INFO: grad norm: 1569.712 1413.587 682.472
2024-12-01-22:56:18-root-INFO: Loss Change: 7765.579 -> 4174.122
2024-12-01-22:56:18-root-INFO: Regularization Change: 0.000 -> 7.925
2024-12-01-22:56:18-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-22:56:18-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-22:56:18-root-INFO: step: 219 lr_xt 0.00085206
2024-12-01-22:56:18-root-INFO: grad norm: 897.830 813.304 380.309
2024-12-01-22:56:19-root-INFO: Loss too large (4158.301->4269.562)! Learning rate decreased to 0.00068.
2024-12-01-22:56:20-root-INFO: grad norm: 870.153 783.136 379.293
2024-12-01-22:56:21-root-INFO: grad norm: 845.508 766.907 356.001
2024-12-01-22:56:21-root-INFO: grad norm: 823.747 740.450 360.963
2024-12-01-22:56:22-root-INFO: grad norm: 803.582 729.655 336.671
2024-12-01-22:56:23-root-INFO: grad norm: 785.571 706.041 344.426
2024-12-01-22:56:24-root-INFO: grad norm: 768.533 698.402 320.745
2024-12-01-22:56:25-root-INFO: grad norm: 753.335 677.353 329.707
2024-12-01-22:56:26-root-INFO: Loss Change: 4158.301 -> 3925.301
2024-12-01-22:56:26-root-INFO: Regularization Change: 0.000 -> 1.040
2024-12-01-22:56:26-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-01-22:56:26-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-22:56:26-root-INFO: step: 218 lr_xt 0.00089342
2024-12-01-22:56:26-root-INFO: grad norm: 541.396 497.908 212.598
2024-12-01-22:56:27-root-INFO: Loss too large (3851.843->3872.423)! Learning rate decreased to 0.00071.
2024-12-01-22:56:27-root-INFO: grad norm: 506.180 453.784 224.274
2024-12-01-22:56:28-root-INFO: grad norm: 482.887 443.423 191.194
2024-12-01-22:56:29-root-INFO: grad norm: 463.275 417.090 201.642
2024-12-01-22:56:30-root-INFO: grad norm: 445.507 409.249 176.044
2024-12-01-22:56:31-root-INFO: grad norm: 429.894 388.037 185.030
2024-12-01-22:56:32-root-INFO: grad norm: 415.415 382.002 163.229
2024-12-01-22:56:33-root-INFO: grad norm: 402.854 364.270 172.042
2024-12-01-22:56:33-root-INFO: Loss Change: 3851.843 -> 3696.961
2024-12-01-22:56:33-root-INFO: Regularization Change: 0.000 -> 0.742
2024-12-01-22:56:33-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-01-22:56:33-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:56:34-root-INFO: step: 217 lr_xt 0.00093664
2024-12-01-22:56:34-root-INFO: grad norm: 273.820 250.098 111.481
2024-12-01-22:56:35-root-INFO: grad norm: 303.478 270.401 137.775
2024-12-01-22:56:36-root-INFO: grad norm: 394.968 365.532 149.620
2024-12-01-22:56:36-root-INFO: Loss too large (3637.908->3649.300)! Learning rate decreased to 0.00075.
2024-12-01-22:56:37-root-INFO: grad norm: 372.618 338.254 156.295
2024-12-01-22:56:38-root-INFO: grad norm: 355.197 329.628 132.326
2024-12-01-22:56:38-root-INFO: grad norm: 340.321 310.108 140.183
2024-12-01-22:56:39-root-INFO: grad norm: 326.963 303.908 120.604
2024-12-01-22:56:40-root-INFO: grad norm: 315.250 287.962 128.299
2024-12-01-22:56:41-root-INFO: Loss Change: 3663.137 -> 3556.599
2024-12-01-22:56:41-root-INFO: Regularization Change: 0.000 -> 0.693
2024-12-01-22:56:41-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-01-22:56:41-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:56:41-root-INFO: step: 216 lr_xt 0.00098179
2024-12-01-22:56:41-root-INFO: grad norm: 224.200 205.490 89.663
2024-12-01-22:56:42-root-INFO: grad norm: 195.046 176.533 82.941
2024-12-01-22:56:43-root-INFO: grad norm: 223.830 211.528 73.183
2024-12-01-22:56:44-root-INFO: grad norm: 295.909 272.979 114.215
2024-12-01-22:56:44-root-INFO: Loss too large (3480.702->3485.599)! Learning rate decreased to 0.00079.
2024-12-01-22:56:45-root-INFO: grad norm: 288.617 270.958 99.406
2024-12-01-22:56:46-root-INFO: grad norm: 283.402 262.003 108.032
2024-12-01-22:56:47-root-INFO: grad norm: 279.248 262.305 95.788
2024-12-01-22:56:48-root-INFO: grad norm: 276.041 255.630 104.172
2024-12-01-22:56:48-root-INFO: Loss Change: 3522.270 -> 3432.008
2024-12-01-22:56:48-root-INFO: Regularization Change: 0.000 -> 0.638
2024-12-01-22:56:48-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-01-22:56:48-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:56:49-root-INFO: step: 215 lr_xt 0.00102894
2024-12-01-22:56:49-root-INFO: grad norm: 236.067 222.391 79.182
2024-12-01-22:56:50-root-INFO: grad norm: 316.199 292.104 121.067
2024-12-01-22:56:50-root-INFO: Loss too large (3410.576->3418.950)! Learning rate decreased to 0.00082.
2024-12-01-22:56:51-root-INFO: grad norm: 303.004 285.844 100.522
2024-12-01-22:56:52-root-INFO: grad norm: 291.907 270.620 109.428
2024-12-01-22:56:53-root-INFO: grad norm: 282.208 266.651 92.403
2024-12-01-22:56:54-root-INFO: grad norm: 273.840 254.422 101.281
2024-12-01-22:56:55-root-INFO: grad norm: 266.578 252.092 86.680
2024-12-01-22:56:56-root-INFO: grad norm: 260.305 242.256 95.240
2024-12-01-22:56:56-root-INFO: Loss Change: 3413.585 -> 3342.250
2024-12-01-22:56:56-root-INFO: Regularization Change: 0.000 -> 0.497
2024-12-01-22:56:56-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-01-22:56:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:56:56-root-INFO: step: 214 lr_xt 0.00107819
2024-12-01-22:56:57-root-INFO: grad norm: 175.319 164.191 61.468
2024-12-01-22:56:58-root-INFO: grad norm: 179.055 167.653 62.875
2024-12-01-22:56:58-root-INFO: grad norm: 230.245 220.392 66.633
2024-12-01-22:56:59-root-INFO: Loss too large (3281.736->3282.608)! Learning rate decreased to 0.00086.
2024-12-01-22:57:00-root-INFO: grad norm: 226.201 212.700 76.976
2024-12-01-22:57:01-root-INFO: grad norm: 224.794 214.550 67.086
2024-12-01-22:57:01-root-INFO: grad norm: 224.494 211.222 76.045
2024-12-01-22:57:02-root-INFO: grad norm: 225.022 214.642 67.555
2024-12-01-22:57:03-root-INFO: grad norm: 226.246 213.071 76.079
2024-12-01-22:57:04-root-INFO: Loss Change: 3302.167 -> 3234.343
2024-12-01-22:57:04-root-INFO: Regularization Change: 0.000 -> 0.521
2024-12-01-22:57:04-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-01-22:57:04-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:57:04-root-INFO: step: 213 lr_xt 0.00112961
2024-12-01-22:57:04-root-INFO: grad norm: 200.006 179.531 88.155
2024-12-01-22:57:05-root-INFO: grad norm: 170.684 159.143 61.699
2024-12-01-22:57:06-root-INFO: grad norm: 200.809 191.930 59.053
2024-12-01-22:57:07-root-INFO: grad norm: 279.775 266.136 86.290
2024-12-01-22:57:07-root-INFO: Loss too large (3195.041->3207.262)! Learning rate decreased to 0.00090.
2024-12-01-22:57:08-root-INFO: grad norm: 299.448 286.654 86.596
2024-12-01-22:57:09-root-INFO: grad norm: 326.221 309.988 101.625
2024-12-01-22:57:10-root-INFO: grad norm: 358.521 342.302 106.612
2024-12-01-22:57:11-root-INFO: grad norm: 391.029 371.359 122.457
2024-12-01-22:57:11-root-INFO: Loss Change: 3228.161 -> 3177.318
2024-12-01-22:57:11-root-INFO: Regularization Change: 0.000 -> 0.627
2024-12-01-22:57:11-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-01-22:57:11-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:57:11-root-INFO: step: 212 lr_xt 0.00118329
2024-12-01-22:57:12-root-INFO: grad norm: 396.242 378.365 117.677
2024-12-01-22:57:12-root-INFO: Loss too large (3159.607->3201.579)! Learning rate decreased to 0.00095.
2024-12-01-22:57:13-root-INFO: grad norm: 418.838 398.121 130.095
2024-12-01-22:57:14-root-INFO: grad norm: 444.151 424.362 131.100
2024-12-01-22:57:14-root-INFO: Loss too large (3152.546->3152.942)! Learning rate decreased to 0.00076.
2024-12-01-22:57:15-root-INFO: grad norm: 300.977 285.729 94.585
2024-12-01-22:57:16-root-INFO: grad norm: 203.817 196.240 55.056
2024-12-01-22:57:17-root-INFO: grad norm: 151.037 143.266 47.823
2024-12-01-22:57:18-root-INFO: grad norm: 120.169 116.457 29.640
2024-12-01-22:57:18-root-INFO: grad norm: 104.251 99.620 30.728
2024-12-01-22:57:19-root-INFO: Loss Change: 3159.607 -> 3074.301
2024-12-01-22:57:19-root-INFO: Regularization Change: 0.000 -> 0.361
2024-12-01-22:57:19-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-01-22:57:19-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-22:57:19-root-INFO: step: 211 lr_xt 0.00123933
2024-12-01-22:57:20-root-INFO: grad norm: 254.053 220.117 126.853
2024-12-01-22:57:20-root-INFO: grad norm: 197.005 179.958 80.163
2024-12-01-22:57:21-root-INFO: grad norm: 184.232 166.124 79.652
2024-12-01-22:57:22-root-INFO: grad norm: 189.206 175.601 70.450
2024-12-01-22:57:23-root-INFO: grad norm: 224.043 208.393 82.266
2024-12-01-22:57:23-root-INFO: Loss too large (3006.799->3009.359)! Learning rate decreased to 0.00099.
2024-12-01-22:57:24-root-INFO: grad norm: 230.840 220.231 69.177
2024-12-01-22:57:25-root-INFO: grad norm: 273.322 260.757 81.918
2024-12-01-22:57:25-root-INFO: Loss too large (2995.769->2996.773)! Learning rate decreased to 0.00079.
2024-12-01-22:57:26-root-INFO: grad norm: 228.951 219.257 65.916
2024-12-01-22:57:27-root-INFO: Loss Change: 3065.271 -> 2975.961
2024-12-01-22:57:27-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-01-22:57:27-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-01-22:57:27-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:57:27-root-INFO: step: 210 lr_xt 0.00129780
2024-12-01-22:57:28-root-INFO: grad norm: 246.621 232.781 81.455
2024-12-01-22:57:28-root-INFO: Loss too large (2974.007->2990.613)! Learning rate decreased to 0.00104.
2024-12-01-22:57:29-root-INFO: grad norm: 303.500 291.562 84.283
2024-12-01-22:57:29-root-INFO: Loss too large (2971.511->2977.282)! Learning rate decreased to 0.00083.
2024-12-01-22:57:30-root-INFO: grad norm: 257.426 245.059 78.830
2024-12-01-22:57:31-root-INFO: grad norm: 217.967 209.613 59.763
2024-12-01-22:57:32-root-INFO: grad norm: 190.271 181.147 58.213
2024-12-01-22:57:33-root-INFO: grad norm: 167.033 160.864 44.975
2024-12-01-22:57:34-root-INFO: grad norm: 150.352 143.346 45.360
2024-12-01-22:57:35-root-INFO: grad norm: 136.513 131.643 36.139
2024-12-01-22:57:35-root-INFO: Loss Change: 2974.007 -> 2912.644
2024-12-01-22:57:35-root-INFO: Regularization Change: 0.000 -> 0.350
2024-12-01-22:57:35-root-INFO: Undo step: 210
2024-12-01-22:57:35-root-INFO: Undo step: 211
2024-12-01-22:57:35-root-INFO: Undo step: 212
2024-12-01-22:57:35-root-INFO: Undo step: 213
2024-12-01-22:57:35-root-INFO: Undo step: 214
2024-12-01-22:57:35-root-INFO: step: 215 lr_xt 0.00102894
2024-12-01-22:57:36-root-INFO: grad norm: 2621.065 2224.815 1385.704
2024-12-01-22:57:36-root-INFO: Loss too large (5441.690->5625.530)! Learning rate decreased to 0.00082.
2024-12-01-22:57:37-root-INFO: grad norm: 1886.834 1735.531 740.320
2024-12-01-22:57:38-root-INFO: grad norm: 1411.923 1313.627 517.601
2024-12-01-22:57:39-root-INFO: grad norm: 1301.054 1206.039 488.071
2024-12-01-22:57:40-root-INFO: grad norm: 1169.420 1088.964 426.262
2024-12-01-22:57:40-root-INFO: grad norm: 1066.021 986.367 404.326
2024-12-01-22:57:41-root-INFO: grad norm: 955.263 888.242 351.502
2024-12-01-22:57:42-root-INFO: grad norm: 865.177 799.415 330.858
2024-12-01-22:57:43-root-INFO: Loss Change: 5441.690 -> 3506.261
2024-12-01-22:57:43-root-INFO: Regularization Change: 0.000 -> 5.191
2024-12-01-22:57:43-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-01-22:57:43-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:57:43-root-INFO: step: 214 lr_xt 0.00107819
2024-12-01-22:57:43-root-INFO: grad norm: 620.881 579.717 222.311
2024-12-01-22:57:44-root-INFO: Loss too large (3433.577->3467.650)! Learning rate decreased to 0.00086.
2024-12-01-22:57:44-root-INFO: grad norm: 536.171 494.358 207.581
2024-12-01-22:57:45-root-INFO: grad norm: 466.113 434.445 168.875
2024-12-01-22:57:46-root-INFO: grad norm: 409.741 378.005 158.116
2024-12-01-22:57:47-root-INFO: grad norm: 360.076 335.875 129.778
2024-12-01-22:57:48-root-INFO: grad norm: 319.234 294.537 123.119
2024-12-01-22:57:49-root-INFO: grad norm: 283.478 264.825 101.132
2024-12-01-22:57:50-root-INFO: grad norm: 253.807 234.138 97.965
2024-12-01-22:57:50-root-INFO: Loss Change: 3433.577 -> 3222.900
2024-12-01-22:57:50-root-INFO: Regularization Change: 0.000 -> 0.973
2024-12-01-22:57:50-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-01-22:57:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:57:50-root-INFO: step: 213 lr_xt 0.00112961
2024-12-01-22:57:51-root-INFO: grad norm: 181.224 161.428 82.361
2024-12-01-22:57:52-root-INFO: grad norm: 140.587 128.222 57.652
2024-12-01-22:57:52-root-INFO: grad norm: 127.414 119.092 45.292
2024-12-01-22:57:53-root-INFO: grad norm: 124.324 115.396 46.263
2024-12-01-22:57:54-root-INFO: grad norm: 127.184 120.227 41.487
2024-12-01-22:57:55-root-INFO: grad norm: 136.114 126.719 49.693
2024-12-01-22:57:56-root-INFO: grad norm: 153.194 145.096 49.148
2024-12-01-22:57:57-root-INFO: grad norm: 181.277 168.863 65.929
2024-12-01-22:57:58-root-INFO: Loss Change: 3207.491 -> 3089.835
2024-12-01-22:57:58-root-INFO: Regularization Change: 0.000 -> 1.088
2024-12-01-22:57:58-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-01-22:57:58-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-22:57:58-root-INFO: step: 212 lr_xt 0.00118329
2024-12-01-22:57:58-root-INFO: grad norm: 202.336 190.188 69.055
2024-12-01-22:57:59-root-INFO: grad norm: 240.761 223.950 88.389
2024-12-01-22:58:00-root-INFO: grad norm: 297.456 281.123 97.209
2024-12-01-22:58:01-root-INFO: Loss too large (3059.662->3061.449)! Learning rate decreased to 0.00095.
2024-12-01-22:58:01-root-INFO: grad norm: 253.467 236.595 90.931
2024-12-01-22:58:02-root-INFO: grad norm: 217.528 206.650 67.930
2024-12-01-22:58:03-root-INFO: grad norm: 189.626 177.263 67.350
2024-12-01-22:58:04-root-INFO: grad norm: 167.137 159.361 50.385
2024-12-01-22:58:05-root-INFO: grad norm: 149.793 140.317 52.432
2024-12-01-22:58:05-root-INFO: Loss Change: 3073.162 -> 2987.667
2024-12-01-22:58:05-root-INFO: Regularization Change: 0.000 -> 0.646
2024-12-01-22:58:05-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-01-22:58:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-22:58:06-root-INFO: step: 211 lr_xt 0.00123933
2024-12-01-22:58:06-root-INFO: grad norm: 219.930 190.718 109.526
2024-12-01-22:58:07-root-INFO: grad norm: 169.453 155.805 66.627
2024-12-01-22:58:08-root-INFO: grad norm: 152.701 139.112 62.972
2024-12-01-22:58:09-root-INFO: grad norm: 144.704 133.714 55.314
2024-12-01-22:58:10-root-INFO: grad norm: 140.111 128.980 54.728
2024-12-01-22:58:10-root-INFO: grad norm: 137.981 128.119 51.228
2024-12-01-22:58:11-root-INFO: grad norm: 138.095 127.388 53.317
2024-12-01-22:58:12-root-INFO: grad norm: 141.936 132.571 50.703
2024-12-01-22:58:13-root-INFO: Loss Change: 2970.131 -> 2866.518
2024-12-01-22:58:13-root-INFO: Regularization Change: 0.000 -> 0.919
2024-12-01-22:58:13-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-01-22:58:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:58:13-root-INFO: step: 210 lr_xt 0.00129780
2024-12-01-22:58:13-root-INFO: grad norm: 200.354 183.660 80.067
2024-12-01-22:58:14-root-INFO: grad norm: 225.346 213.366 72.498
2024-12-01-22:58:15-root-INFO: grad norm: 270.087 253.421 93.408
2024-12-01-22:58:15-root-INFO: Loss too large (2852.030->2853.584)! Learning rate decreased to 0.00104.
2024-12-01-22:58:16-root-INFO: grad norm: 230.919 221.635 64.819
2024-12-01-22:58:17-root-INFO: grad norm: 207.094 196.445 65.554
2024-12-01-22:58:18-root-INFO: grad norm: 190.840 184.253 49.703
2024-12-01-22:58:19-root-INFO: grad norm: 178.892 170.236 54.973
2024-12-01-22:58:20-root-INFO: grad norm: 169.504 163.841 43.448
2024-12-01-22:58:20-root-INFO: Loss Change: 2864.652 -> 2791.731
2024-12-01-22:58:20-root-INFO: Regularization Change: 0.000 -> 0.609
2024-12-01-22:58:20-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-01-22:58:20-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:58:21-root-INFO: step: 209 lr_xt 0.00135882
2024-12-01-22:58:21-root-INFO: grad norm: 213.401 195.702 85.091
2024-12-01-22:58:22-root-INFO: grad norm: 245.766 234.615 73.190
2024-12-01-22:58:22-root-INFO: Loss too large (2766.589->2769.299)! Learning rate decreased to 0.00109.
2024-12-01-22:58:23-root-INFO: grad norm: 227.427 216.756 68.849
2024-12-01-22:58:24-root-INFO: grad norm: 228.561 220.227 61.159
2024-12-01-22:58:25-root-INFO: grad norm: 237.238 226.669 70.022
2024-12-01-22:58:25-root-INFO: grad norm: 251.882 242.245 69.009
2024-12-01-22:58:26-root-INFO: grad norm: 267.172 255.035 79.613
2024-12-01-22:58:27-root-INFO: grad norm: 285.660 274.211 80.060
2024-12-01-22:58:28-root-INFO: Loss Change: 2776.329 -> 2721.234
2024-12-01-22:58:28-root-INFO: Regularization Change: 0.000 -> 0.600
2024-12-01-22:58:28-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-01-22:58:28-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:58:28-root-INFO: step: 208 lr_xt 0.00142247
2024-12-01-22:58:28-root-INFO: grad norm: 334.578 317.528 105.446
2024-12-01-22:58:29-root-INFO: Loss too large (2718.652->2738.523)! Learning rate decreased to 0.00114.
2024-12-01-22:58:29-root-INFO: grad norm: 339.665 325.770 96.156
2024-12-01-22:58:30-root-INFO: grad norm: 369.695 353.399 108.551
2024-12-01-22:58:31-root-INFO: grad norm: 417.530 399.485 121.421
2024-12-01-22:58:32-root-INFO: Loss too large (2702.292->2710.676)! Learning rate decreased to 0.00091.
2024-12-01-22:58:33-root-INFO: grad norm: 303.212 288.828 92.282
2024-12-01-22:58:33-root-INFO: grad norm: 214.036 204.908 61.840
2024-12-01-22:58:34-root-INFO: grad norm: 163.132 155.342 49.808
2024-12-01-22:58:35-root-INFO: grad norm: 128.894 123.880 35.601
2024-12-01-22:58:36-root-INFO: Loss Change: 2718.652 -> 2631.425
2024-12-01-22:58:36-root-INFO: Regularization Change: 0.000 -> 0.472
2024-12-01-22:58:36-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-01-22:58:36-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:58:36-root-INFO: step: 207 lr_xt 0.00150141
2024-12-01-22:58:36-root-INFO: grad norm: 120.162 110.842 46.400
2024-12-01-22:58:37-root-INFO: grad norm: 126.645 122.142 33.473
2024-12-01-22:58:38-root-INFO: grad norm: 173.374 165.736 50.894
2024-12-01-22:58:38-root-INFO: Loss too large (2592.612->2597.257)! Learning rate decreased to 0.00120.
2024-12-01-22:58:39-root-INFO: grad norm: 212.684 204.102 59.807
2024-12-01-22:58:39-root-INFO: Loss too large (2588.156->2588.970)! Learning rate decreased to 0.00096.
2024-12-01-22:58:40-root-INFO: grad norm: 190.585 181.408 58.428
2024-12-01-22:58:41-root-INFO: grad norm: 173.521 165.703 51.498
2024-12-01-22:58:42-root-INFO: grad norm: 161.023 153.237 49.464
2024-12-01-22:58:43-root-INFO: grad norm: 150.574 143.812 44.616
2024-12-01-22:58:43-root-INFO: Loss Change: 2610.380 -> 2552.737
2024-12-01-22:58:43-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-01-22:58:43-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-01-22:58:43-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:58:44-root-INFO: step: 206 lr_xt 0.00157117
2024-12-01-22:58:44-root-INFO: grad norm: 146.067 137.812 48.407
2024-12-01-22:58:45-root-INFO: grad norm: 217.256 208.760 60.162
2024-12-01-22:58:45-root-INFO: Loss too large (2525.462->2549.056)! Learning rate decreased to 0.00126.
2024-12-01-22:58:45-root-INFO: Loss too large (2525.462->2529.066)! Learning rate decreased to 0.00101.
2024-12-01-22:58:46-root-INFO: grad norm: 209.829 200.131 63.052
2024-12-01-22:58:47-root-INFO: grad norm: 213.816 203.342 66.101
2024-12-01-22:58:48-root-INFO: grad norm: 219.345 208.582 67.865
2024-12-01-22:58:49-root-INFO: grad norm: 226.965 215.659 70.743
2024-12-01-22:58:50-root-INFO: grad norm: 235.015 223.386 73.012
2024-12-01-22:58:51-root-INFO: grad norm: 244.930 232.641 76.608
2024-12-01-22:58:51-root-INFO: Loss Change: 2530.572 -> 2490.127
2024-12-01-22:58:51-root-INFO: Regularization Change: 0.000 -> 0.440
2024-12-01-22:58:51-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-22:58:51-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:58:52-root-INFO: step: 205 lr_xt 0.00164390
2024-12-01-22:58:52-root-INFO: grad norm: 249.445 238.079 74.437
2024-12-01-22:58:52-root-INFO: Loss too large (2476.002->2514.800)! Learning rate decreased to 0.00132.
2024-12-01-22:58:52-root-INFO: Loss too large (2476.002->2483.586)! Learning rate decreased to 0.00105.
2024-12-01-22:58:53-root-INFO: grad norm: 259.547 245.540 84.112
2024-12-01-22:58:54-root-INFO: grad norm: 289.700 275.263 90.311
2024-12-01-22:58:55-root-INFO: grad norm: 331.087 313.515 106.428
2024-12-01-22:58:55-root-INFO: Loss too large (2464.820->2468.954)! Learning rate decreased to 0.00084.
2024-12-01-22:58:56-root-INFO: grad norm: 246.164 233.837 76.919
2024-12-01-22:58:57-root-INFO: grad norm: 181.048 171.588 57.756
2024-12-01-22:58:58-root-INFO: grad norm: 141.723 135.145 42.679
2024-12-01-22:58:59-root-INFO: grad norm: 114.749 109.157 35.386
2024-12-01-22:58:59-root-INFO: Loss Change: 2476.002 -> 2418.904
2024-12-01-22:58:59-root-INFO: Regularization Change: 0.000 -> 0.317
2024-12-01-22:58:59-root-INFO: Undo step: 205
2024-12-01-22:58:59-root-INFO: Undo step: 206
2024-12-01-22:58:59-root-INFO: Undo step: 207
2024-12-01-22:58:59-root-INFO: Undo step: 208
2024-12-01-22:58:59-root-INFO: Undo step: 209
2024-12-01-22:59:00-root-INFO: step: 210 lr_xt 0.00129780
2024-12-01-22:59:00-root-INFO: grad norm: 2878.201 2507.106 1413.669
2024-12-01-22:59:01-root-INFO: grad norm: 2751.927 2601.465 897.484
2024-12-01-22:59:01-root-INFO: Loss too large (5511.088->6209.243)! Learning rate decreased to 0.00104.
2024-12-01-22:59:02-root-INFO: grad norm: 2198.633 2036.705 828.141
2024-12-01-22:59:03-root-INFO: grad norm: 1701.358 1601.942 573.062
2024-12-01-22:59:04-root-INFO: grad norm: 1242.381 1165.076 431.401
2024-12-01-22:59:05-root-INFO: grad norm: 955.951 889.746 349.561
2024-12-01-22:59:05-root-INFO: grad norm: 727.141 680.881 255.217
2024-12-01-22:59:06-root-INFO: grad norm: 569.698 530.669 207.235
2024-12-01-22:59:07-root-INFO: Loss Change: 6639.840 -> 2927.129
2024-12-01-22:59:07-root-INFO: Regularization Change: 0.000 -> 9.196
2024-12-01-22:59:07-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-01-22:59:07-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:59:07-root-INFO: step: 209 lr_xt 0.00135882
2024-12-01-22:59:07-root-INFO: grad norm: 379.085 359.604 119.961
2024-12-01-22:59:08-root-INFO: grad norm: 427.973 400.405 151.119
2024-12-01-22:59:09-root-INFO: grad norm: 507.459 478.795 168.137
2024-12-01-22:59:10-root-INFO: Loss too large (2860.307->2873.846)! Learning rate decreased to 0.00109.
2024-12-01-22:59:10-root-INFO: grad norm: 402.294 376.855 140.789
2024-12-01-22:59:11-root-INFO: grad norm: 321.868 304.811 103.391
2024-12-01-22:59:12-root-INFO: grad norm: 263.576 246.803 92.524
2024-12-01-22:59:13-root-INFO: grad norm: 219.441 208.638 68.004
2024-12-01-22:59:14-root-INFO: grad norm: 187.172 175.193 65.884
2024-12-01-22:59:15-root-INFO: Loss Change: 2885.613 -> 2697.385
2024-12-01-22:59:15-root-INFO: Regularization Change: 0.000 -> 1.436
2024-12-01-22:59:15-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-01-22:59:15-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:59:15-root-INFO: step: 208 lr_xt 0.00142247
2024-12-01-22:59:15-root-INFO: grad norm: 165.082 152.476 63.269
2024-12-01-22:59:16-root-INFO: grad norm: 151.816 141.649 54.625
2024-12-01-22:59:17-root-INFO: grad norm: 170.135 162.699 49.749
2024-12-01-22:59:18-root-INFO: grad norm: 205.665 193.870 68.649
2024-12-01-22:59:19-root-INFO: grad norm: 267.224 255.146 79.429
2024-12-01-22:59:19-root-INFO: Loss too large (2633.068->2638.164)! Learning rate decreased to 0.00114.
2024-12-01-22:59:20-root-INFO: grad norm: 245.254 232.121 79.179
2024-12-01-22:59:21-root-INFO: grad norm: 225.124 215.388 65.489
2024-12-01-22:59:22-root-INFO: grad norm: 210.000 198.823 67.599
2024-12-01-22:59:22-root-INFO: Loss Change: 2684.332 -> 2585.308
2024-12-01-22:59:22-root-INFO: Regularization Change: 0.000 -> 1.039
2024-12-01-22:59:22-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-01-22:59:22-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:59:23-root-INFO: step: 207 lr_xt 0.00150141
2024-12-01-22:59:23-root-INFO: grad norm: 223.634 211.063 73.924
2024-12-01-22:59:24-root-INFO: grad norm: 299.987 282.860 99.911
2024-12-01-22:59:24-root-INFO: Loss too large (2564.168->2581.156)! Learning rate decreased to 0.00120.
2024-12-01-22:59:25-root-INFO: grad norm: 300.103 285.409 92.758
2024-12-01-22:59:26-root-INFO: grad norm: 303.109 286.715 98.332
2024-12-01-22:59:27-root-INFO: grad norm: 307.577 292.517 95.066
2024-12-01-22:59:28-root-INFO: grad norm: 311.914 295.444 100.015
2024-12-01-22:59:28-root-INFO: grad norm: 316.882 301.253 98.291
2024-12-01-22:59:29-root-INFO: grad norm: 320.676 303.973 102.145
2024-12-01-22:59:30-root-INFO: Loss Change: 2564.504 -> 2506.812
2024-12-01-22:59:30-root-INFO: Regularization Change: 0.000 -> 0.726
2024-12-01-22:59:30-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-01-22:59:30-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:59:30-root-INFO: step: 206 lr_xt 0.00157117
2024-12-01-22:59:31-root-INFO: grad norm: 371.726 348.682 128.844
2024-12-01-22:59:31-root-INFO: Loss too large (2491.018->2538.123)! Learning rate decreased to 0.00126.
2024-12-01-22:59:32-root-INFO: grad norm: 382.385 361.489 124.675
2024-12-01-22:59:33-root-INFO: grad norm: 400.853 379.332 129.577
2024-12-01-22:59:33-root-INFO: Loss too large (2480.266->2483.177)! Learning rate decreased to 0.00101.
2024-12-01-22:59:34-root-INFO: grad norm: 274.178 259.644 88.082
2024-12-01-22:59:34-root-INFO: grad norm: 184.507 175.336 57.445
2024-12-01-22:59:35-root-INFO: grad norm: 138.299 131.126 43.960
2024-12-01-22:59:36-root-INFO: grad norm: 109.677 104.547 33.151
2024-12-01-22:59:37-root-INFO: grad norm: 94.208 89.587 29.143
2024-12-01-22:59:38-root-INFO: Loss Change: 2491.018 -> 2400.282
2024-12-01-22:59:38-root-INFO: Regularization Change: 0.000 -> 0.478
2024-12-01-22:59:38-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-22:59:38-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-22:59:38-root-INFO: step: 205 lr_xt 0.00164390
2024-12-01-22:59:38-root-INFO: grad norm: 156.010 139.754 69.339
2024-12-01-22:59:39-root-INFO: grad norm: 201.380 187.696 72.968
2024-12-01-22:59:39-root-INFO: Loss too large (2379.978->2392.916)! Learning rate decreased to 0.00132.
2024-12-01-22:59:40-root-INFO: grad norm: 248.001 233.186 84.433
2024-12-01-22:59:40-root-INFO: Loss too large (2377.039->2382.679)! Learning rate decreased to 0.00105.
2024-12-01-22:59:41-root-INFO: grad norm: 217.069 205.322 70.439
2024-12-01-22:59:42-root-INFO: grad norm: 190.369 180.421 60.733
2024-12-01-22:59:43-root-INFO: grad norm: 171.318 162.467 54.355
2024-12-01-22:59:44-root-INFO: grad norm: 154.734 146.826 48.834
2024-12-01-22:59:45-root-INFO: grad norm: 142.477 135.246 44.812
2024-12-01-22:59:45-root-INFO: Loss Change: 2386.960 -> 2332.186
2024-12-01-22:59:45-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-01-22:59:45-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-01-22:59:45-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-22:59:46-root-INFO: step: 204 lr_xt 0.00171973
2024-12-01-22:59:46-root-INFO: grad norm: 230.848 209.314 97.359
2024-12-01-22:59:46-root-INFO: Loss too large (2325.085->2339.965)! Learning rate decreased to 0.00138.
2024-12-01-22:59:47-root-INFO: grad norm: 283.512 266.959 95.457
2024-12-01-22:59:47-root-INFO: Loss too large (2320.767->2337.010)! Learning rate decreased to 0.00110.
2024-12-01-22:59:48-root-INFO: grad norm: 284.557 268.755 93.507
2024-12-01-22:59:49-root-INFO: grad norm: 289.717 274.769 91.857
2024-12-01-22:59:50-root-INFO: grad norm: 295.409 279.531 95.547
2024-12-01-22:59:51-root-INFO: grad norm: 299.106 283.900 94.155
2024-12-01-22:59:52-root-INFO: grad norm: 301.991 285.750 97.699
2024-12-01-22:59:53-root-INFO: grad norm: 302.746 287.418 95.110
2024-12-01-22:59:53-root-INFO: Loss Change: 2325.085 -> 2283.417
2024-12-01-22:59:53-root-INFO: Regularization Change: 0.000 -> 0.452
2024-12-01-22:59:53-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-01-22:59:53-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-22:59:53-root-INFO: step: 203 lr_xt 0.00179875
2024-12-01-22:59:54-root-INFO: grad norm: 357.148 337.040 118.148
2024-12-01-22:59:54-root-INFO: Loss too large (2282.752->2394.089)! Learning rate decreased to 0.00144.
2024-12-01-22:59:54-root-INFO: Loss too large (2282.752->2323.428)! Learning rate decreased to 0.00115.
2024-12-01-22:59:55-root-INFO: grad norm: 360.606 342.384 113.181
2024-12-01-22:59:56-root-INFO: grad norm: 362.474 343.249 116.478
2024-12-01-22:59:57-root-INFO: grad norm: 359.915 342.026 112.057
2024-12-01-22:59:58-root-INFO: grad norm: 353.670 335.012 113.357
2024-12-01-22:59:59-root-INFO: grad norm: 345.586 328.506 107.302
2024-12-01-22:59:59-root-INFO: grad norm: 334.287 316.685 107.044
2024-12-01-23:00:00-root-INFO: grad norm: 323.707 307.775 100.302
2024-12-01-23:00:01-root-INFO: Loss Change: 2282.752 -> 2233.933
2024-12-01-23:00:01-root-INFO: Regularization Change: 0.000 -> 0.390
2024-12-01-23:00:01-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-01-23:00:01-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:00:01-root-INFO: step: 202 lr_xt 0.00188111
2024-12-01-23:00:01-root-INFO: grad norm: 507.844 469.917 192.572
2024-12-01-23:00:02-root-INFO: Loss too large (2267.238->2451.396)! Learning rate decreased to 0.00150.
2024-12-01-23:00:02-root-INFO: Loss too large (2267.238->2327.004)! Learning rate decreased to 0.00120.
2024-12-01-23:00:03-root-INFO: grad norm: 480.372 457.361 146.893
2024-12-01-23:00:04-root-INFO: grad norm: 482.745 456.772 156.212
2024-12-01-23:00:04-root-INFO: Loss too large (2233.516->2238.910)! Learning rate decreased to 0.00096.
2024-12-01-23:00:05-root-INFO: grad norm: 315.723 301.077 95.046
2024-12-01-23:00:06-root-INFO: grad norm: 195.976 185.231 63.999
2024-12-01-23:00:07-root-INFO: grad norm: 139.684 133.548 40.946
2024-12-01-23:00:07-root-INFO: grad norm: 104.876 99.244 33.904
2024-12-01-23:00:08-root-INFO: grad norm: 87.035 83.431 24.786
2024-12-01-23:00:09-root-INFO: Loss Change: 2267.238 -> 2142.926
2024-12-01-23:00:09-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-01-23:00:09-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-01-23:00:09-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:00:09-root-INFO: step: 201 lr_xt 0.00196691
2024-12-01-23:00:09-root-INFO: grad norm: 76.803 72.442 25.512
2024-12-01-23:00:10-root-INFO: grad norm: 69.846 66.595 21.061
2024-12-01-23:00:11-root-INFO: grad norm: 72.898 69.605 21.665
2024-12-01-23:00:12-root-INFO: grad norm: 98.883 94.345 29.612
2024-12-01-23:00:12-root-INFO: Loss too large (2114.748->2114.938)! Learning rate decreased to 0.00157.
2024-12-01-23:00:13-root-INFO: grad norm: 156.164 148.567 48.115
2024-12-01-23:00:14-root-INFO: Loss too large (2111.707->2119.288)! Learning rate decreased to 0.00126.
2024-12-01-23:00:14-root-INFO: grad norm: 196.034 186.954 58.970
2024-12-01-23:00:15-root-INFO: Loss too large (2110.684->2111.580)! Learning rate decreased to 0.00101.
2024-12-01-23:00:16-root-INFO: grad norm: 170.345 162.053 52.500
2024-12-01-23:00:16-root-INFO: grad norm: 151.980 144.985 45.580
2024-12-01-23:00:17-root-INFO: Loss Change: 2141.630 -> 2091.876
2024-12-01-23:00:17-root-INFO: Regularization Change: 0.000 -> 0.619
2024-12-01-23:00:17-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-01-23:00:17-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:00:17-root-INFO: step: 200 lr_xt 0.00205630
2024-12-01-23:00:18-root-INFO: grad norm: 304.182 283.052 111.392
2024-12-01-23:00:18-root-INFO: Loss too large (2093.317->2195.457)! Learning rate decreased to 0.00165.
2024-12-01-23:00:18-root-INFO: Loss too large (2093.317->2134.415)! Learning rate decreased to 0.00132.
2024-12-01-23:00:18-root-INFO: Loss too large (2093.317->2097.948)! Learning rate decreased to 0.00105.
2024-12-01-23:00:19-root-INFO: grad norm: 265.718 254.313 77.013
2024-12-01-23:00:20-root-INFO: grad norm: 254.666 241.844 79.788
2024-12-01-23:00:21-root-INFO: grad norm: 247.159 236.386 72.176
2024-12-01-23:00:22-root-INFO: grad norm: 240.239 228.429 74.398
2024-12-01-23:00:23-root-INFO: grad norm: 235.033 224.854 68.421
2024-12-01-23:00:24-root-INFO: grad norm: 230.048 218.770 71.147
2024-12-01-23:00:24-root-INFO: grad norm: 226.349 216.605 65.696
2024-12-01-23:00:25-root-INFO: Loss Change: 2093.317 -> 2035.865
2024-12-01-23:00:25-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-01-23:00:25-root-INFO: Undo step: 200
2024-12-01-23:00:25-root-INFO: Undo step: 201
2024-12-01-23:00:25-root-INFO: Undo step: 202
2024-12-01-23:00:25-root-INFO: Undo step: 203
2024-12-01-23:00:25-root-INFO: Undo step: 204
2024-12-01-23:00:25-root-INFO: step: 205 lr_xt 0.00164390
2024-12-01-23:00:26-root-INFO: grad norm: 1331.822 1086.518 770.213
2024-12-01-23:00:27-root-INFO: grad norm: 653.840 606.777 243.573
2024-12-01-23:00:28-root-INFO: grad norm: 542.105 479.453 252.987
2024-12-01-23:00:28-root-INFO: grad norm: 584.824 563.462 156.622
2024-12-01-23:00:29-root-INFO: Loss too large (2742.844->2758.796)! Learning rate decreased to 0.00132.
2024-12-01-23:00:30-root-INFO: grad norm: 552.260 521.799 180.878
2024-12-01-23:00:30-root-INFO: grad norm: 573.144 554.964 143.211
2024-12-01-23:00:31-root-INFO: grad norm: 592.593 568.497 167.265
2024-12-01-23:00:32-root-INFO: Loss too large (2606.743->2610.082)! Learning rate decreased to 0.00105.
2024-12-01-23:00:32-root-INFO: grad norm: 409.872 396.467 103.966
2024-12-01-23:00:33-root-INFO: Loss Change: 4302.839 -> 2488.492
2024-12-01-23:00:33-root-INFO: Regularization Change: 0.000 -> 10.161
2024-12-01-23:00:33-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-01-23:00:33-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-23:00:33-root-INFO: step: 204 lr_xt 0.00171973
2024-12-01-23:00:34-root-INFO: grad norm: 275.874 259.825 92.723
2024-12-01-23:00:35-root-INFO: grad norm: 358.142 348.179 83.889
2024-12-01-23:00:35-root-INFO: Loss too large (2446.694->2501.228)! Learning rate decreased to 0.00138.
2024-12-01-23:00:36-root-INFO: grad norm: 452.423 436.304 119.688
2024-12-01-23:00:36-root-INFO: Loss too large (2442.076->2473.395)! Learning rate decreased to 0.00110.
2024-12-01-23:00:37-root-INFO: grad norm: 384.596 374.460 87.717
2024-12-01-23:00:38-root-INFO: grad norm: 318.527 306.747 85.822
2024-12-01-23:00:38-root-INFO: grad norm: 285.378 277.490 66.631
2024-12-01-23:00:39-root-INFO: grad norm: 259.055 248.912 71.777
2024-12-01-23:00:40-root-INFO: grad norm: 246.323 239.343 58.223
2024-12-01-23:00:41-root-INFO: Loss Change: 2460.684 -> 2331.712
2024-12-01-23:00:41-root-INFO: Regularization Change: 0.000 -> 1.135
2024-12-01-23:00:41-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-01-23:00:41-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:00:41-root-INFO: step: 203 lr_xt 0.00179875
2024-12-01-23:00:42-root-INFO: grad norm: 262.575 251.875 74.193
2024-12-01-23:00:42-root-INFO: Loss too large (2319.206->2364.667)! Learning rate decreased to 0.00144.
2024-12-01-23:00:42-root-INFO: Loss too large (2319.206->2330.724)! Learning rate decreased to 0.00115.
2024-12-01-23:00:43-root-INFO: grad norm: 270.004 262.683 62.448
2024-12-01-23:00:44-root-INFO: grad norm: 289.558 279.185 76.808
2024-12-01-23:00:45-root-INFO: grad norm: 310.826 302.842 69.993
2024-12-01-23:00:46-root-INFO: grad norm: 344.645 332.964 88.964
2024-12-01-23:00:46-root-INFO: Loss too large (2291.944->2296.022)! Learning rate decreased to 0.00092.
2024-12-01-23:00:47-root-INFO: grad norm: 257.539 250.662 59.115
2024-12-01-23:00:48-root-INFO: grad norm: 187.742 180.071 53.119
2024-12-01-23:00:49-root-INFO: grad norm: 152.258 147.283 38.605
2024-12-01-23:00:49-root-INFO: Loss Change: 2319.206 -> 2239.254
2024-12-01-23:00:49-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-01-23:00:49-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-01-23:00:49-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:00:50-root-INFO: step: 202 lr_xt 0.00188111
2024-12-01-23:00:50-root-INFO: grad norm: 459.908 434.755 150.012
2024-12-01-23:00:50-root-INFO: Loss too large (2267.573->2320.877)! Learning rate decreased to 0.00150.
2024-12-01-23:00:50-root-INFO: Loss too large (2267.573->2272.471)! Learning rate decreased to 0.00120.
2024-12-01-23:00:51-root-INFO: grad norm: 294.507 286.064 70.014
2024-12-01-23:00:52-root-INFO: grad norm: 356.040 344.798 88.761
2024-12-01-23:00:52-root-INFO: Loss too large (2222.689->2240.746)! Learning rate decreased to 0.00096.
2024-12-01-23:00:53-root-INFO: grad norm: 319.285 311.092 71.867
2024-12-01-23:00:54-root-INFO: grad norm: 287.950 278.083 74.731
2024-12-01-23:00:55-root-INFO: grad norm: 270.871 264.078 60.286
2024-12-01-23:00:56-root-INFO: grad norm: 255.127 246.425 66.065
2024-12-01-23:00:57-root-INFO: grad norm: 246.281 240.096 54.848
2024-12-01-23:00:57-root-INFO: Loss Change: 2267.573 -> 2167.616
2024-12-01-23:00:57-root-INFO: Regularization Change: 0.000 -> 0.519
2024-12-01-23:00:57-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-01-23:00:57-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:00:58-root-INFO: step: 201 lr_xt 0.00196691
2024-12-01-23:00:58-root-INFO: grad norm: 220.004 212.146 58.273
2024-12-01-23:00:58-root-INFO: Loss too large (2162.263->2227.016)! Learning rate decreased to 0.00157.
2024-12-01-23:00:59-root-INFO: Loss too large (2162.263->2191.186)! Learning rate decreased to 0.00126.
2024-12-01-23:00:59-root-INFO: Loss too large (2162.263->2169.881)! Learning rate decreased to 0.00101.
2024-12-01-23:01:00-root-INFO: grad norm: 226.525 220.180 53.241
2024-12-01-23:01:01-root-INFO: grad norm: 240.407 232.483 61.211
2024-12-01-23:01:02-root-INFO: grad norm: 255.040 248.222 58.579
2024-12-01-23:01:02-root-INFO: grad norm: 276.939 268.092 69.440
2024-12-01-23:01:03-root-INFO: Loss too large (2145.865->2146.847)! Learning rate decreased to 0.00081.
2024-12-01-23:01:04-root-INFO: grad norm: 201.604 196.010 47.160
2024-12-01-23:01:05-root-INFO: grad norm: 143.882 138.496 38.998
2024-12-01-23:01:06-root-INFO: grad norm: 113.917 110.077 29.326
2024-12-01-23:01:06-root-INFO: Loss Change: 2162.263 -> 2115.093
2024-12-01-23:01:06-root-INFO: Regularization Change: 0.000 -> 0.277
2024-12-01-23:01:06-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-01-23:01:06-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:01:06-root-INFO: step: 200 lr_xt 0.00205630
2024-12-01-23:01:07-root-INFO: grad norm: 468.665 451.057 127.256
2024-12-01-23:01:07-root-INFO: Loss too large (2125.966->2282.210)! Learning rate decreased to 0.00165.
2024-12-01-23:01:07-root-INFO: Loss too large (2125.966->2215.886)! Learning rate decreased to 0.00132.
2024-12-01-23:01:08-root-INFO: Loss too large (2125.966->2167.795)! Learning rate decreased to 0.00105.
2024-12-01-23:01:08-root-INFO: Loss too large (2125.966->2133.814)! Learning rate decreased to 0.00084.
2024-12-01-23:01:09-root-INFO: grad norm: 261.781 256.249 53.532
2024-12-01-23:01:10-root-INFO: grad norm: 164.492 158.542 43.841
2024-12-01-23:01:11-root-INFO: grad norm: 133.623 129.531 32.815
2024-12-01-23:01:11-root-INFO: grad norm: 115.700 110.890 33.015
2024-12-01-23:01:12-root-INFO: grad norm: 104.821 101.442 26.402
2024-12-01-23:01:13-root-INFO: grad norm: 96.577 92.446 27.945
2024-12-01-23:01:14-root-INFO: grad norm: 90.896 87.821 23.444
2024-12-01-23:01:15-root-INFO: Loss Change: 2125.966 -> 2053.732
2024-12-01-23:01:15-root-INFO: Regularization Change: 0.000 -> 0.250
2024-12-01-23:01:15-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-01-23:01:15-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:01:15-root-INFO: step: 199 lr_xt 0.00214940
2024-12-01-23:01:15-root-INFO: grad norm: 243.486 231.693 74.857
2024-12-01-23:01:16-root-INFO: Loss too large (2051.433->2129.138)! Learning rate decreased to 0.00172.
2024-12-01-23:01:16-root-INFO: Loss too large (2051.433->2094.079)! Learning rate decreased to 0.00138.
2024-12-01-23:01:16-root-INFO: Loss too large (2051.433->2070.326)! Learning rate decreased to 0.00110.
2024-12-01-23:01:16-root-INFO: Loss too large (2051.433->2054.944)! Learning rate decreased to 0.00088.
2024-12-01-23:01:17-root-INFO: grad norm: 199.996 195.456 42.370
2024-12-01-23:01:18-root-INFO: grad norm: 159.768 153.977 42.623
2024-12-01-23:01:19-root-INFO: grad norm: 155.617 151.287 36.451
2024-12-01-23:01:20-root-INFO: grad norm: 153.464 148.253 39.652
2024-12-01-23:01:21-root-INFO: grad norm: 154.441 150.176 36.045
2024-12-01-23:01:22-root-INFO: grad norm: 156.882 151.698 39.996
2024-12-01-23:01:23-root-INFO: grad norm: 160.284 155.917 37.160
2024-12-01-23:01:23-root-INFO: Loss Change: 2051.433 -> 2014.964
2024-12-01-23:01:23-root-INFO: Regularization Change: 0.000 -> 0.211
2024-12-01-23:01:23-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-01-23:01:23-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-23:01:24-root-INFO: step: 198 lr_xt 0.00224635
2024-12-01-23:01:24-root-INFO: grad norm: 481.766 464.841 126.575
2024-12-01-23:01:24-root-INFO: Loss too large (2038.445->2304.343)! Learning rate decreased to 0.00180.
2024-12-01-23:01:24-root-INFO: Loss too large (2038.445->2208.966)! Learning rate decreased to 0.00144.
2024-12-01-23:01:25-root-INFO: Loss too large (2038.445->2138.399)! Learning rate decreased to 0.00115.
2024-12-01-23:01:25-root-INFO: Loss too large (2038.445->2087.139)! Learning rate decreased to 0.00092.
2024-12-01-23:01:25-root-INFO: Loss too large (2038.445->2050.723)! Learning rate decreased to 0.00074.
2024-12-01-23:01:26-root-INFO: grad norm: 288.743 282.894 57.821
2024-12-01-23:01:27-root-INFO: grad norm: 136.229 130.620 38.690
2024-12-01-23:01:28-root-INFO: grad norm: 105.383 101.421 28.622
2024-12-01-23:01:29-root-INFO: grad norm: 85.390 81.543 25.341
2024-12-01-23:01:30-root-INFO: grad norm: 74.367 71.183 21.526
2024-12-01-23:01:31-root-INFO: grad norm: 67.613 64.435 20.484
2024-12-01-23:01:31-root-INFO: grad norm: 63.757 60.904 18.857
2024-12-01-23:01:32-root-INFO: Loss Change: 2038.445 -> 1974.150
2024-12-01-23:01:32-root-INFO: Regularization Change: 0.000 -> 0.181
2024-12-01-23:01:32-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-01-23:01:32-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:01:32-root-INFO: step: 197 lr_xt 0.00234729
2024-12-01-23:01:32-root-INFO: grad norm: 332.482 322.993 78.869
2024-12-01-23:01:33-root-INFO: Loss too large (1973.727->2130.408)! Learning rate decreased to 0.00188.
2024-12-01-23:01:33-root-INFO: Loss too large (1973.727->2079.192)! Learning rate decreased to 0.00150.
2024-12-01-23:01:33-root-INFO: Loss too large (1973.727->2039.828)! Learning rate decreased to 0.00120.
2024-12-01-23:01:34-root-INFO: Loss too large (1973.727->2009.882)! Learning rate decreased to 0.00096.
2024-12-01-23:01:34-root-INFO: Loss too large (1973.727->1987.790)! Learning rate decreased to 0.00077.
2024-12-01-23:01:35-root-INFO: grad norm: 246.815 242.185 47.581
2024-12-01-23:01:36-root-INFO: grad norm: 132.024 127.591 33.925
2024-12-01-23:01:37-root-INFO: grad norm: 118.420 115.108 27.813
2024-12-01-23:01:37-root-INFO: grad norm: 106.855 103.480 26.644
2024-12-01-23:01:38-root-INFO: grad norm: 100.008 97.137 23.792
2024-12-01-23:01:39-root-INFO: grad norm: 94.216 91.290 23.297
2024-12-01-23:01:40-root-INFO: grad norm: 90.689 88.067 21.650
2024-12-01-23:01:41-root-INFO: Loss Change: 1973.727 -> 1934.317
2024-12-01-23:01:41-root-INFO: Regularization Change: 0.000 -> 0.148
2024-12-01-23:01:41-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-01-23:01:41-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:01:41-root-INFO: step: 196 lr_xt 0.00245238
2024-12-01-23:01:41-root-INFO: grad norm: 448.172 435.436 106.080
2024-12-01-23:01:42-root-INFO: Loss too large (1952.508->2133.802)! Learning rate decreased to 0.00196.
2024-12-01-23:01:42-root-INFO: Loss too large (1952.508->2085.420)! Learning rate decreased to 0.00157.
2024-12-01-23:01:42-root-INFO: Loss too large (1952.508->2044.600)! Learning rate decreased to 0.00126.
2024-12-01-23:01:42-root-INFO: Loss too large (1952.508->2010.119)! Learning rate decreased to 0.00100.
2024-12-01-23:01:43-root-INFO: Loss too large (1952.508->1981.288)! Learning rate decreased to 0.00080.
2024-12-01-23:01:43-root-INFO: Loss too large (1952.508->1958.124)! Learning rate decreased to 0.00064.
2024-12-01-23:01:44-root-INFO: grad norm: 257.351 253.206 46.003
2024-12-01-23:01:45-root-INFO: grad norm: 78.608 72.729 29.828
2024-12-01-23:01:46-root-INFO: grad norm: 68.783 64.602 23.615
2024-12-01-23:01:47-root-INFO: grad norm: 64.024 60.181 21.847
2024-12-01-23:01:47-root-INFO: grad norm: 61.425 58.168 19.736
2024-12-01-23:01:48-root-INFO: grad norm: 59.695 56.671 18.759
2024-12-01-23:01:49-root-INFO: grad norm: 58.565 55.800 17.783
2024-12-01-23:01:50-root-INFO: Loss Change: 1952.508 -> 1900.217
2024-12-01-23:01:50-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-01-23:01:50-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-01-23:01:50-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:01:50-root-INFO: step: 195 lr_xt 0.00256175
2024-12-01-23:01:50-root-INFO: grad norm: 162.408 157.534 39.489
2024-12-01-23:01:51-root-INFO: Loss too large (1893.531->1961.806)! Learning rate decreased to 0.00205.
2024-12-01-23:01:51-root-INFO: Loss too large (1893.531->1938.023)! Learning rate decreased to 0.00164.
2024-12-01-23:01:51-root-INFO: Loss too large (1893.531->1920.011)! Learning rate decreased to 0.00131.
2024-12-01-23:01:52-root-INFO: Loss too large (1893.531->1907.163)! Learning rate decreased to 0.00105.
2024-12-01-23:01:52-root-INFO: Loss too large (1893.531->1898.601)! Learning rate decreased to 0.00084.
2024-12-01-23:01:53-root-INFO: grad norm: 180.803 177.312 35.359
2024-12-01-23:01:54-root-INFO: grad norm: 220.236 215.940 43.288
2024-12-01-23:01:54-root-INFO: Loss too large (1890.406->1894.545)! Learning rate decreased to 0.00067.
2024-12-01-23:01:55-root-INFO: grad norm: 179.032 175.734 34.204
2024-12-01-23:01:55-root-INFO: grad norm: 130.677 127.894 26.827
2024-12-01-23:01:56-root-INFO: grad norm: 115.264 112.742 23.980
2024-12-01-23:01:57-root-INFO: grad norm: 99.230 96.846 21.622
2024-12-01-23:01:58-root-INFO: grad norm: 90.898 88.613 20.255
2024-12-01-23:01:59-root-INFO: Loss Change: 1893.531 -> 1871.244
2024-12-01-23:01:59-root-INFO: Regularization Change: 0.000 -> 0.109
2024-12-01-23:01:59-root-INFO: Undo step: 195
2024-12-01-23:01:59-root-INFO: Undo step: 196
2024-12-01-23:01:59-root-INFO: Undo step: 197
2024-12-01-23:01:59-root-INFO: Undo step: 198
2024-12-01-23:01:59-root-INFO: Undo step: 199
2024-12-01-23:01:59-root-INFO: step: 200 lr_xt 0.00205630
2024-12-01-23:01:59-root-INFO: grad norm: 1892.115 1773.361 659.765
2024-12-01-23:02:00-root-INFO: grad norm: 1189.367 1146.433 316.675
2024-12-01-23:02:01-root-INFO: grad norm: 818.895 774.056 267.259
2024-12-01-23:02:02-root-INFO: grad norm: 701.271 664.089 225.316
2024-12-01-23:02:03-root-INFO: grad norm: 607.193 581.230 175.657
2024-12-01-23:02:04-root-INFO: grad norm: 548.121 524.913 157.807
2024-12-01-23:02:05-root-INFO: grad norm: 502.966 486.301 128.396
2024-12-01-23:02:05-root-INFO: grad norm: 487.970 472.142 123.276
2024-12-01-23:02:06-root-INFO: Loss Change: 4233.293 -> 2377.575
2024-12-01-23:02:06-root-INFO: Regularization Change: 0.000 -> 15.740
2024-12-01-23:02:06-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-01-23:02:06-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-23:02:06-root-INFO: step: 199 lr_xt 0.00214940
2024-12-01-23:02:07-root-INFO: grad norm: 727.797 710.971 155.590
2024-12-01-23:02:07-root-INFO: Loss too large (2402.884->2524.776)! Learning rate decreased to 0.00172.
2024-12-01-23:02:08-root-INFO: grad norm: 438.531 424.636 109.515
2024-12-01-23:02:09-root-INFO: grad norm: 306.104 297.073 73.805
2024-12-01-23:02:10-root-INFO: grad norm: 255.642 246.001 69.542
2024-12-01-23:02:10-root-INFO: grad norm: 235.769 225.029 70.348
2024-12-01-23:02:11-root-INFO: grad norm: 246.624 238.048 64.471
2024-12-01-23:02:12-root-INFO: grad norm: 328.110 317.034 84.532
2024-12-01-23:02:12-root-INFO: Loss too large (2156.507->2180.551)! Learning rate decreased to 0.00138.
2024-12-01-23:02:13-root-INFO: grad norm: 259.056 251.561 61.866
2024-12-01-23:02:14-root-INFO: Loss Change: 2402.884 -> 2113.034
2024-12-01-23:02:14-root-INFO: Regularization Change: 0.000 -> 1.750
2024-12-01-23:02:14-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-01-23:02:14-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-23:02:14-root-INFO: step: 198 lr_xt 0.00224635
2024-12-01-23:02:14-root-INFO: grad norm: 445.197 429.399 117.546
2024-12-01-23:02:15-root-INFO: Loss too large (2131.856->2259.355)! Learning rate decreased to 0.00180.
2024-12-01-23:02:15-root-INFO: Loss too large (2131.856->2200.042)! Learning rate decreased to 0.00144.
2024-12-01-23:02:15-root-INFO: Loss too large (2131.856->2157.037)! Learning rate decreased to 0.00115.
2024-12-01-23:02:16-root-INFO: grad norm: 265.435 259.207 57.162
2024-12-01-23:02:17-root-INFO: grad norm: 125.203 118.960 39.044
2024-12-01-23:02:18-root-INFO: grad norm: 102.071 96.254 33.966
2024-12-01-23:02:19-root-INFO: grad norm: 97.252 92.464 30.139
2024-12-01-23:02:20-root-INFO: grad norm: 99.297 93.891 32.315
2024-12-01-23:02:21-root-INFO: grad norm: 104.392 100.409 28.560
2024-12-01-23:02:22-root-INFO: grad norm: 118.351 113.309 34.176
2024-12-01-23:02:22-root-INFO: Loss Change: 2131.856 -> 2025.365
2024-12-01-23:02:22-root-INFO: Regularization Change: 0.000 -> 0.674
2024-12-01-23:02:22-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-01-23:02:22-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:02:22-root-INFO: step: 197 lr_xt 0.00234729
2024-12-01-23:02:23-root-INFO: grad norm: 131.966 123.322 46.975
2024-12-01-23:02:23-root-INFO: Loss too large (2005.539->2011.182)! Learning rate decreased to 0.00188.
2024-12-01-23:02:24-root-INFO: grad norm: 213.434 206.588 53.626
2024-12-01-23:02:24-root-INFO: Loss too large (2003.229->2049.872)! Learning rate decreased to 0.00150.
2024-12-01-23:02:24-root-INFO: Loss too large (2003.229->2015.259)! Learning rate decreased to 0.00120.
2024-12-01-23:02:25-root-INFO: grad norm: 279.378 271.766 64.772
2024-12-01-23:02:26-root-INFO: Loss too large (1997.685->2012.286)! Learning rate decreased to 0.00096.
2024-12-01-23:02:26-root-INFO: Loss too large (1997.685->1997.693)! Learning rate decreased to 0.00077.
2024-12-01-23:02:27-root-INFO: grad norm: 186.398 182.626 37.311
2024-12-01-23:02:28-root-INFO: grad norm: 106.976 102.526 30.533
2024-12-01-23:02:28-root-INFO: grad norm: 94.247 90.674 25.706
2024-12-01-23:02:29-root-INFO: grad norm: 84.359 79.893 27.085
2024-12-01-23:02:30-root-INFO: grad norm: 79.568 75.794 24.215
2024-12-01-23:02:31-root-INFO: Loss Change: 2005.539 -> 1959.425
2024-12-01-23:02:31-root-INFO: Regularization Change: 0.000 -> 0.331
2024-12-01-23:02:31-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-01-23:02:31-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:02:31-root-INFO: step: 196 lr_xt 0.00245238
2024-12-01-23:02:31-root-INFO: grad norm: 388.210 376.510 94.589
2024-12-01-23:02:32-root-INFO: Loss too large (1970.039->2117.561)! Learning rate decreased to 0.00196.
2024-12-01-23:02:32-root-INFO: Loss too large (1970.039->2075.301)! Learning rate decreased to 0.00157.
2024-12-01-23:02:32-root-INFO: Loss too large (1970.039->2039.776)! Learning rate decreased to 0.00126.
2024-12-01-23:02:33-root-INFO: Loss too large (1970.039->2009.688)! Learning rate decreased to 0.00100.
2024-12-01-23:02:33-root-INFO: Loss too large (1970.039->1985.133)! Learning rate decreased to 0.00080.
2024-12-01-23:02:34-root-INFO: grad norm: 263.985 259.905 46.237
2024-12-01-23:02:35-root-INFO: grad norm: 101.790 95.777 34.467
2024-12-01-23:02:35-root-INFO: grad norm: 94.647 90.773 26.801
2024-12-01-23:02:36-root-INFO: grad norm: 89.667 84.791 29.167
2024-12-01-23:02:37-root-INFO: grad norm: 86.813 83.411 24.063
2024-12-01-23:02:38-root-INFO: grad norm: 84.624 80.203 26.994
2024-12-01-23:02:39-root-INFO: grad norm: 83.397 80.239 22.733
2024-12-01-23:02:39-root-INFO: Loss Change: 1970.039 -> 1911.950
2024-12-01-23:02:39-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-01-23:02:40-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-01-23:02:40-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:02:40-root-INFO: step: 195 lr_xt 0.00256175
2024-12-01-23:02:40-root-INFO: grad norm: 197.537 191.818 47.189
2024-12-01-23:02:40-root-INFO: Loss too large (1907.084->1989.976)! Learning rate decreased to 0.00205.
2024-12-01-23:02:41-root-INFO: Loss too large (1907.084->1962.265)! Learning rate decreased to 0.00164.
2024-12-01-23:02:41-root-INFO: Loss too large (1907.084->1940.278)! Learning rate decreased to 0.00131.
2024-12-01-23:02:41-root-INFO: Loss too large (1907.084->1923.999)! Learning rate decreased to 0.00105.
2024-12-01-23:02:41-root-INFO: Loss too large (1907.084->1912.842)! Learning rate decreased to 0.00084.
2024-12-01-23:02:42-root-INFO: grad norm: 193.376 189.892 36.540
2024-12-01-23:02:43-root-INFO: grad norm: 195.867 191.529 40.995
2024-12-01-23:02:43-root-INFO: Loss too large (1899.041->1899.164)! Learning rate decreased to 0.00067.
2024-12-01-23:02:44-root-INFO: grad norm: 149.233 146.402 28.932
2024-12-01-23:02:45-root-INFO: grad norm: 104.502 100.903 27.189
2024-12-01-23:02:46-root-INFO: grad norm: 89.509 86.673 22.352
2024-12-01-23:02:47-root-INFO: grad norm: 77.828 74.235 23.375
2024-12-01-23:02:48-root-INFO: grad norm: 72.031 68.952 20.835
2024-12-01-23:02:48-root-INFO: Loss Change: 1907.084 -> 1877.214
2024-12-01-23:02:48-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-01-23:02:48-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-01-23:02:48-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:02:49-root-INFO: step: 194 lr_xt 0.00267557
2024-12-01-23:02:49-root-INFO: grad norm: 310.766 301.418 75.652
2024-12-01-23:02:49-root-INFO: Loss too large (1882.628->2047.176)! Learning rate decreased to 0.00214.
2024-12-01-23:02:49-root-INFO: Loss too large (1882.628->2001.944)! Learning rate decreased to 0.00171.
2024-12-01-23:02:50-root-INFO: Loss too large (1882.628->1964.147)! Learning rate decreased to 0.00137.
2024-12-01-23:02:50-root-INFO: Loss too large (1882.628->1932.768)! Learning rate decreased to 0.00110.
2024-12-01-23:02:50-root-INFO: Loss too large (1882.628->1907.965)! Learning rate decreased to 0.00088.
2024-12-01-23:02:51-root-INFO: Loss too large (1882.628->1889.859)! Learning rate decreased to 0.00070.
2024-12-01-23:02:51-root-INFO: grad norm: 239.979 236.410 41.230
2024-12-01-23:02:52-root-INFO: grad norm: 157.569 152.827 38.367
2024-12-01-23:02:53-root-INFO: grad norm: 142.419 139.561 28.390
2024-12-01-23:02:54-root-INFO: grad norm: 125.223 121.391 30.742
2024-12-01-23:02:55-root-INFO: grad norm: 117.010 114.391 24.618
2024-12-01-23:02:56-root-INFO: grad norm: 108.240 104.792 27.103
2024-12-01-23:02:57-root-INFO: grad norm: 103.410 100.912 22.595
2024-12-01-23:02:57-root-INFO: Loss Change: 1882.628 -> 1843.413
2024-12-01-23:02:57-root-INFO: Regularization Change: 0.000 -> 0.147
2024-12-01-23:02:57-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-01-23:02:57-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:02:58-root-INFO: step: 193 lr_xt 0.00279399
2024-12-01-23:02:58-root-INFO: grad norm: 508.626 498.276 102.088
2024-12-01-23:02:58-root-INFO: Loss too large (1865.867->2101.558)! Learning rate decreased to 0.00224.
2024-12-01-23:02:58-root-INFO: Loss too large (1865.867->2051.298)! Learning rate decreased to 0.00179.
2024-12-01-23:02:59-root-INFO: Loss too large (1865.867->2007.771)! Learning rate decreased to 0.00143.
2024-12-01-23:02:59-root-INFO: Loss too large (1865.867->1968.842)! Learning rate decreased to 0.00114.
2024-12-01-23:02:59-root-INFO: Loss too large (1865.867->1933.027)! Learning rate decreased to 0.00092.
2024-12-01-23:02:59-root-INFO: Loss too large (1865.867->1900.418)! Learning rate decreased to 0.00073.
2024-12-01-23:03:00-root-INFO: Loss too large (1865.867->1872.831)! Learning rate decreased to 0.00059.
2024-12-01-23:03:01-root-INFO: grad norm: 288.580 284.666 47.369
2024-12-01-23:03:01-root-INFO: grad norm: 79.271 73.917 28.638
2024-12-01-23:03:02-root-INFO: grad norm: 71.510 67.280 24.229
2024-12-01-23:03:03-root-INFO: grad norm: 66.774 62.412 23.741
2024-12-01-23:03:04-root-INFO: grad norm: 64.028 60.266 21.622
2024-12-01-23:03:05-root-INFO: grad norm: 62.186 58.425 21.297
2024-12-01-23:03:06-root-INFO: grad norm: 60.947 57.510 20.178
2024-12-01-23:03:06-root-INFO: Loss Change: 1865.867 -> 1809.231
2024-12-01-23:03:06-root-INFO: Regularization Change: 0.000 -> 0.121
2024-12-01-23:03:06-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-01-23:03:06-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-23:03:07-root-INFO: step: 192 lr_xt 0.00291718
2024-12-01-23:03:07-root-INFO: grad norm: 206.463 200.615 48.793
2024-12-01-23:03:07-root-INFO: Loss too large (1799.900->1945.657)! Learning rate decreased to 0.00233.
2024-12-01-23:03:07-root-INFO: Loss too large (1799.900->1899.438)! Learning rate decreased to 0.00187.
2024-12-01-23:03:08-root-INFO: Loss too large (1799.900->1863.713)! Learning rate decreased to 0.00149.
2024-12-01-23:03:08-root-INFO: Loss too large (1799.900->1837.204)! Learning rate decreased to 0.00119.
2024-12-01-23:03:08-root-INFO: Loss too large (1799.900->1818.590)! Learning rate decreased to 0.00096.
2024-12-01-23:03:08-root-INFO: Loss too large (1799.900->1806.328)! Learning rate decreased to 0.00076.
2024-12-01-23:03:09-root-INFO: grad norm: 214.508 211.295 36.989
2024-12-01-23:03:10-root-INFO: grad norm: 277.036 272.784 48.350
2024-12-01-23:03:10-root-INFO: Loss too large (1796.711->1805.421)! Learning rate decreased to 0.00061.
2024-12-01-23:03:11-root-INFO: grad norm: 228.547 225.467 37.392
2024-12-01-23:03:12-root-INFO: grad norm: 165.424 162.393 31.523
2024-12-01-23:03:13-root-INFO: grad norm: 149.460 147.034 26.821
2024-12-01-23:03:14-root-INFO: grad norm: 131.030 128.290 26.655
2024-12-01-23:03:15-root-INFO: grad norm: 122.013 119.766 23.308
2024-12-01-23:03:15-root-INFO: Loss Change: 1799.900 -> 1775.690
2024-12-01-23:03:15-root-INFO: Regularization Change: 0.000 -> 0.100
2024-12-01-23:03:15-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-01-23:03:15-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-23:03:16-root-INFO: step: 191 lr_xt 0.00304531
2024-12-01-23:03:16-root-INFO: grad norm: 621.709 610.586 117.076
2024-12-01-23:03:16-root-INFO: Loss too large (1822.937->2094.455)! Learning rate decreased to 0.00244.
2024-12-01-23:03:17-root-INFO: Loss too large (1822.937->2042.942)! Learning rate decreased to 0.00195.
2024-12-01-23:03:17-root-INFO: Loss too large (1822.937->1998.596)! Learning rate decreased to 0.00156.
2024-12-01-23:03:17-root-INFO: Loss too large (1822.937->1959.122)! Learning rate decreased to 0.00125.
2024-12-01-23:03:17-root-INFO: Loss too large (1822.937->1922.631)! Learning rate decreased to 0.00100.
2024-12-01-23:03:18-root-INFO: Loss too large (1822.937->1887.617)! Learning rate decreased to 0.00080.
2024-12-01-23:03:18-root-INFO: Loss too large (1822.937->1853.880)! Learning rate decreased to 0.00064.
2024-12-01-23:03:18-root-INFO: Loss too large (1822.937->1823.557)! Learning rate decreased to 0.00051.
2024-12-01-23:03:19-root-INFO: grad norm: 324.792 320.206 54.386
2024-12-01-23:03:20-root-INFO: grad norm: 85.402 78.283 34.135
2024-12-01-23:03:21-root-INFO: grad norm: 77.684 71.485 30.408
2024-12-01-23:03:22-root-INFO: grad norm: 72.164 66.578 27.839
2024-12-01-23:03:23-root-INFO: grad norm: 68.239 63.274 25.552
2024-12-01-23:03:23-root-INFO: grad norm: 65.568 61.014 24.009
2024-12-01-23:03:24-root-INFO: grad norm: 63.444 59.288 22.586
2024-12-01-23:03:25-root-INFO: Loss Change: 1822.937 -> 1750.340
2024-12-01-23:03:25-root-INFO: Regularization Change: 0.000 -> 0.127
2024-12-01-23:03:25-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-01-23:03:25-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-23:03:25-root-INFO: step: 190 lr_xt 0.00317856
2024-12-01-23:03:25-root-INFO: grad norm: 380.266 373.010 73.932
2024-12-01-23:03:26-root-INFO: Loss too large (1759.545->2001.057)! Learning rate decreased to 0.00254.
2024-12-01-23:03:26-root-INFO: Loss too large (1759.545->1954.826)! Learning rate decreased to 0.00203.
2024-12-01-23:03:26-root-INFO: Loss too large (1759.545->1914.240)! Learning rate decreased to 0.00163.
2024-12-01-23:03:26-root-INFO: Loss too large (1759.545->1877.177)! Learning rate decreased to 0.00130.
2024-12-01-23:03:27-root-INFO: Loss too large (1759.545->1842.571)! Learning rate decreased to 0.00104.
2024-12-01-23:03:27-root-INFO: Loss too large (1759.545->1811.205)! Learning rate decreased to 0.00083.
2024-12-01-23:03:27-root-INFO: Loss too large (1759.545->1785.002)! Learning rate decreased to 0.00067.
2024-12-01-23:03:28-root-INFO: Loss too large (1759.545->1765.309)! Learning rate decreased to 0.00053.
2024-12-01-23:03:29-root-INFO: grad norm: 265.596 262.307 41.671
2024-12-01-23:03:29-root-INFO: grad norm: 146.287 142.189 34.382
2024-12-01-23:03:30-root-INFO: grad norm: 121.502 118.909 24.967
2024-12-01-23:03:31-root-INFO: grad norm: 99.012 95.424 26.415
2024-12-01-23:03:32-root-INFO: grad norm: 86.780 84.205 20.982
2024-12-01-23:03:33-root-INFO: grad norm: 76.729 73.410 22.323
2024-12-01-23:03:34-root-INFO: grad norm: 70.565 67.954 19.017
2024-12-01-23:03:34-root-INFO: Loss Change: 1759.545 -> 1722.870
2024-12-01-23:03:34-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-01-23:03:34-root-INFO: Undo step: 190
2024-12-01-23:03:34-root-INFO: Undo step: 191
2024-12-01-23:03:34-root-INFO: Undo step: 192
2024-12-01-23:03:34-root-INFO: Undo step: 193
2024-12-01-23:03:34-root-INFO: Undo step: 194
2024-12-01-23:03:35-root-INFO: step: 195 lr_xt 0.00256175
2024-12-01-23:03:35-root-INFO: grad norm: 1050.931 991.441 348.568
2024-12-01-23:03:36-root-INFO: grad norm: 955.292 930.740 215.186
2024-12-01-23:03:36-root-INFO: Loss too large (2790.103->2952.883)! Learning rate decreased to 0.00205.
2024-12-01-23:03:37-root-INFO: grad norm: 906.303 877.317 227.379
2024-12-01-23:03:37-root-INFO: Loss too large (2520.246->2529.608)! Learning rate decreased to 0.00164.
2024-12-01-23:03:38-root-INFO: grad norm: 593.790 577.630 137.587
2024-12-01-23:03:39-root-INFO: grad norm: 379.898 370.460 84.152
2024-12-01-23:03:39-root-INFO: Loss too large (2111.079->2116.786)! Learning rate decreased to 0.00131.
2024-12-01-23:03:40-root-INFO: grad norm: 270.811 263.357 63.100
2024-12-01-23:03:41-root-INFO: grad norm: 175.764 169.793 45.424
2024-12-01-23:03:42-root-INFO: grad norm: 205.278 199.232 49.452
2024-12-01-23:03:42-root-INFO: Loss Change: 3321.000 -> 2023.540
2024-12-01-23:03:42-root-INFO: Regularization Change: 0.000 -> 8.212
2024-12-01-23:03:42-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-01-23:03:43-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:03:43-root-INFO: step: 194 lr_xt 0.00267557
2024-12-01-23:03:43-root-INFO: grad norm: 618.942 607.639 117.745
2024-12-01-23:03:43-root-INFO: Loss too large (2045.620->2184.447)! Learning rate decreased to 0.00214.
2024-12-01-23:03:44-root-INFO: Loss too large (2045.620->2149.233)! Learning rate decreased to 0.00171.
2024-12-01-23:03:44-root-INFO: Loss too large (2045.620->2117.726)! Learning rate decreased to 0.00137.
2024-12-01-23:03:44-root-INFO: Loss too large (2045.620->2089.094)! Learning rate decreased to 0.00110.
2024-12-01-23:03:45-root-INFO: Loss too large (2045.620->2062.818)! Learning rate decreased to 0.00088.
2024-12-01-23:03:45-root-INFO: grad norm: 243.147 237.994 49.791
2024-12-01-23:03:46-root-INFO: grad norm: 216.954 211.098 50.065
2024-12-01-23:03:47-root-INFO: grad norm: 117.162 111.114 37.158
2024-12-01-23:03:48-root-INFO: grad norm: 112.831 106.951 35.950
2024-12-01-23:03:49-root-INFO: grad norm: 113.228 107.941 34.198
2024-12-01-23:03:50-root-INFO: grad norm: 116.132 111.144 33.672
2024-12-01-23:03:51-root-INFO: grad norm: 125.269 120.695 33.541
2024-12-01-23:03:51-root-INFO: Loss Change: 2045.620 -> 1931.672
2024-12-01-23:03:51-root-INFO: Regularization Change: 0.000 -> 0.458
2024-12-01-23:03:51-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-01-23:03:51-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-23:03:52-root-INFO: step: 193 lr_xt 0.00279399
2024-12-01-23:03:52-root-INFO: grad norm: 204.643 195.347 60.978
2024-12-01-23:03:52-root-INFO: Loss too large (1917.064->1986.469)! Learning rate decreased to 0.00224.
2024-12-01-23:03:53-root-INFO: Loss too large (1917.064->1963.080)! Learning rate decreased to 0.00179.
2024-12-01-23:03:53-root-INFO: Loss too large (1917.064->1944.203)! Learning rate decreased to 0.00143.
2024-12-01-23:03:53-root-INFO: Loss too large (1917.064->1929.793)! Learning rate decreased to 0.00114.
2024-12-01-23:03:53-root-INFO: Loss too large (1917.064->1919.647)! Learning rate decreased to 0.00092.
2024-12-01-23:03:54-root-INFO: grad norm: 215.555 211.278 42.726
2024-12-01-23:03:55-root-INFO: grad norm: 282.316 277.133 53.847
2024-12-01-23:03:55-root-INFO: Loss too large (1906.305->1916.504)! Learning rate decreased to 0.00073.
2024-12-01-23:03:56-root-INFO: grad norm: 233.101 229.030 43.371
2024-12-01-23:03:57-root-INFO: grad norm: 160.724 156.593 36.206
2024-12-01-23:03:58-root-INFO: grad norm: 157.364 153.721 33.664
2024-12-01-23:03:59-root-INFO: grad norm: 156.172 152.288 34.613
2024-12-01-23:04:00-root-INFO: grad norm: 157.974 154.545 32.736
2024-12-01-23:04:00-root-INFO: Loss Change: 1917.064 -> 1873.573
2024-12-01-23:04:00-root-INFO: Regularization Change: 0.000 -> 0.247
2024-12-01-23:04:00-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-01-23:04:00-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-23:04:01-root-INFO: step: 192 lr_xt 0.00291718
2024-12-01-23:04:01-root-INFO: grad norm: 342.933 336.803 64.552
2024-12-01-23:04:01-root-INFO: Loss too large (1867.132->2020.775)! Learning rate decreased to 0.00233.
2024-12-01-23:04:01-root-INFO: Loss too large (1867.132->1990.745)! Learning rate decreased to 0.00187.
2024-12-01-23:04:02-root-INFO: Loss too large (1867.132->1963.408)! Learning rate decreased to 0.00149.
2024-12-01-23:04:02-root-INFO: Loss too large (1867.132->1938.119)! Learning rate decreased to 0.00119.
2024-12-01-23:04:02-root-INFO: Loss too large (1867.132->1914.624)! Learning rate decreased to 0.00096.
2024-12-01-23:04:03-root-INFO: Loss too large (1867.132->1893.658)! Learning rate decreased to 0.00076.
2024-12-01-23:04:03-root-INFO: Loss too large (1867.132->1876.609)! Learning rate decreased to 0.00061.
2024-12-01-23:04:04-root-INFO: grad norm: 254.769 250.931 44.056
2024-12-01-23:04:04-root-INFO: grad norm: 141.782 137.891 32.990
2024-12-01-23:04:05-root-INFO: grad norm: 131.463 128.055 29.742
2024-12-01-23:04:06-root-INFO: grad norm: 120.910 117.306 29.301
2024-12-01-23:04:07-root-INFO: grad norm: 116.078 112.763 27.543
2024-12-01-23:04:08-root-INFO: grad norm: 111.656 108.204 27.546
2024-12-01-23:04:09-root-INFO: grad norm: 109.703 106.490 26.359
2024-12-01-23:04:09-root-INFO: Loss Change: 1867.132 -> 1827.950
2024-12-01-23:04:09-root-INFO: Regularization Change: 0.000 -> 0.127
2024-12-01-23:04:09-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-01-23:04:09-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-23:04:10-root-INFO: step: 191 lr_xt 0.00304531
2024-12-01-23:04:10-root-INFO: grad norm: 618.074 607.316 114.817
2024-12-01-23:04:10-root-INFO: Loss too large (1866.504->2065.540)! Learning rate decreased to 0.00244.
2024-12-01-23:04:11-root-INFO: Loss too large (1866.504->2032.681)! Learning rate decreased to 0.00195.
2024-12-01-23:04:11-root-INFO: Loss too large (1866.504->2002.825)! Learning rate decreased to 0.00156.
2024-12-01-23:04:11-root-INFO: Loss too large (1866.504->1974.690)! Learning rate decreased to 0.00125.
2024-12-01-23:04:11-root-INFO: Loss too large (1866.504->1947.625)! Learning rate decreased to 0.00100.
2024-12-01-23:04:12-root-INFO: Loss too large (1866.504->1921.231)! Learning rate decreased to 0.00080.
2024-12-01-23:04:12-root-INFO: Loss too large (1866.504->1895.224)! Learning rate decreased to 0.00064.
2024-12-01-23:04:12-root-INFO: Loss too large (1866.504->1870.320)! Learning rate decreased to 0.00051.
2024-12-01-23:04:13-root-INFO: grad norm: 317.341 312.567 54.839
2024-12-01-23:04:14-root-INFO: grad norm: 101.479 94.656 36.584
2024-12-01-23:04:15-root-INFO: grad norm: 88.479 81.490 34.467
2024-12-01-23:04:16-root-INFO: grad norm: 80.786 74.994 30.036
2024-12-01-23:04:17-root-INFO: grad norm: 75.597 69.935 28.705
2024-12-01-23:04:18-root-INFO: grad norm: 72.207 67.238 26.323
2024-12-01-23:04:18-root-INFO: grad norm: 69.847 65.015 25.527
2024-12-01-23:04:19-root-INFO: Loss Change: 1866.504 -> 1793.869
2024-12-01-23:04:19-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-01-23:04:19-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-01-23:04:19-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-23:04:19-root-INFO: step: 190 lr_xt 0.00317856
2024-12-01-23:04:20-root-INFO: grad norm: 367.910 360.757 72.194
2024-12-01-23:04:20-root-INFO: Loss too large (1798.210->1983.240)! Learning rate decreased to 0.00254.
2024-12-01-23:04:20-root-INFO: Loss too large (1798.210->1952.701)! Learning rate decreased to 0.00203.
2024-12-01-23:04:21-root-INFO: Loss too large (1798.210->1924.463)! Learning rate decreased to 0.00163.
2024-12-01-23:04:21-root-INFO: Loss too large (1798.210->1897.730)! Learning rate decreased to 0.00130.
2024-12-01-23:04:21-root-INFO: Loss too large (1798.210->1871.964)! Learning rate decreased to 0.00104.
2024-12-01-23:04:21-root-INFO: Loss too large (1798.210->1847.342)! Learning rate decreased to 0.00083.
2024-12-01-23:04:22-root-INFO: Loss too large (1798.210->1825.304)! Learning rate decreased to 0.00067.
2024-12-01-23:04:22-root-INFO: Loss too large (1798.210->1807.586)! Learning rate decreased to 0.00053.
2024-12-01-23:04:23-root-INFO: grad norm: 279.198 275.548 45.000
2024-12-01-23:04:24-root-INFO: grad norm: 164.475 159.797 38.949
2024-12-01-23:04:25-root-INFO: grad norm: 149.931 146.967 29.663
2024-12-01-23:04:25-root-INFO: grad norm: 134.222 130.302 32.201
2024-12-01-23:04:26-root-INFO: grad norm: 126.320 123.574 26.196
2024-12-01-23:04:27-root-INFO: grad norm: 118.152 114.628 28.643
2024-12-01-23:04:28-root-INFO: grad norm: 113.556 110.937 24.248
2024-12-01-23:04:29-root-INFO: Loss Change: 1798.210 -> 1761.720
2024-12-01-23:04:29-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-01-23:04:29-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-01-23:04:29-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-23:04:29-root-INFO: step: 189 lr_xt 0.00331709
2024-12-01-23:04:29-root-INFO: grad norm: 442.359 435.707 76.426
2024-12-01-23:04:30-root-INFO: Loss too large (1772.881->1981.732)! Learning rate decreased to 0.00265.
2024-12-01-23:04:30-root-INFO: Loss too large (1772.881->1954.255)! Learning rate decreased to 0.00212.
2024-12-01-23:04:30-root-INFO: Loss too large (1772.881->1927.447)! Learning rate decreased to 0.00170.
2024-12-01-23:04:30-root-INFO: Loss too large (1772.881->1900.961)! Learning rate decreased to 0.00136.
2024-12-01-23:04:31-root-INFO: Loss too large (1772.881->1874.571)! Learning rate decreased to 0.00109.
2024-12-01-23:04:31-root-INFO: Loss too large (1772.881->1848.038)! Learning rate decreased to 0.00087.
2024-12-01-23:04:31-root-INFO: Loss too large (1772.881->1821.800)! Learning rate decreased to 0.00070.
2024-12-01-23:04:32-root-INFO: Loss too large (1772.881->1797.778)! Learning rate decreased to 0.00056.
2024-12-01-23:04:32-root-INFO: Loss too large (1772.881->1778.292)! Learning rate decreased to 0.00045.
2024-12-01-23:04:33-root-INFO: grad norm: 283.887 280.485 43.816
2024-12-01-23:04:34-root-INFO: grad norm: 122.642 118.137 32.935
2024-12-01-23:04:34-root-INFO: grad norm: 101.091 97.870 25.315
2024-12-01-23:04:35-root-INFO: grad norm: 84.595 80.434 26.202
2024-12-01-23:04:36-root-INFO: grad norm: 75.879 72.459 22.522
2024-12-01-23:04:37-root-INFO: grad norm: 69.822 65.915 23.032
2024-12-01-23:04:38-root-INFO: grad norm: 66.274 62.833 21.077
2024-12-01-23:04:39-root-INFO: Loss Change: 1772.881 -> 1735.296
2024-12-01-23:04:39-root-INFO: Regularization Change: 0.000 -> 0.069
2024-12-01-23:04:39-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-01-23:04:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-23:04:39-root-INFO: step: 188 lr_xt 0.00346111
2024-12-01-23:04:39-root-INFO: grad norm: 324.027 318.274 60.790
2024-12-01-23:04:40-root-INFO: Loss too large (1737.945->1932.133)! Learning rate decreased to 0.00277.
2024-12-01-23:04:40-root-INFO: Loss too large (1737.945->1902.203)! Learning rate decreased to 0.00222.
2024-12-01-23:04:40-root-INFO: Loss too large (1737.945->1874.412)! Learning rate decreased to 0.00177.
2024-12-01-23:04:40-root-INFO: Loss too large (1737.945->1847.902)! Learning rate decreased to 0.00142.
2024-12-01-23:04:41-root-INFO: Loss too large (1737.945->1822.094)! Learning rate decreased to 0.00113.
2024-12-01-23:04:41-root-INFO: Loss too large (1737.945->1797.175)! Learning rate decreased to 0.00091.
2024-12-01-23:04:41-root-INFO: Loss too large (1737.945->1774.594)! Learning rate decreased to 0.00073.
2024-12-01-23:04:42-root-INFO: Loss too large (1737.945->1756.107)! Learning rate decreased to 0.00058.
2024-12-01-23:04:42-root-INFO: Loss too large (1737.945->1742.549)! Learning rate decreased to 0.00046.
2024-12-01-23:04:43-root-INFO: grad norm: 258.429 255.003 41.938
2024-12-01-23:04:44-root-INFO: grad norm: 181.439 177.419 37.983
2024-12-01-23:04:44-root-INFO: grad norm: 161.132 158.346 29.835
2024-12-01-23:04:45-root-INFO: grad norm: 139.570 136.191 30.524
2024-12-01-23:04:46-root-INFO: grad norm: 128.356 125.768 25.646
2024-12-01-23:04:47-root-INFO: grad norm: 116.855 113.783 26.618
2024-12-01-23:04:48-root-INFO: grad norm: 109.867 107.370 23.293
2024-12-01-23:04:49-root-INFO: Loss Change: 1737.945 -> 1708.745
2024-12-01-23:04:49-root-INFO: Regularization Change: 0.000 -> 0.072
2024-12-01-23:04:49-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-01-23:04:49-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-23:04:49-root-INFO: step: 187 lr_xt 0.00361079
2024-12-01-23:04:49-root-INFO: grad norm: 497.342 490.160 84.219
2024-12-01-23:04:49-root-INFO: Loss too large (1738.406->1964.004)! Learning rate decreased to 0.00289.
2024-12-01-23:04:50-root-INFO: Loss too large (1738.406->1940.707)! Learning rate decreased to 0.00231.
2024-12-01-23:04:50-root-INFO: Loss too large (1738.406->1917.290)! Learning rate decreased to 0.00185.
2024-12-01-23:04:50-root-INFO: Loss too large (1738.406->1893.159)! Learning rate decreased to 0.00148.
2024-12-01-23:04:51-root-INFO: Loss too large (1738.406->1868.136)! Learning rate decreased to 0.00118.
2024-12-01-23:04:51-root-INFO: Loss too large (1738.406->1842.056)! Learning rate decreased to 0.00095.
2024-12-01-23:04:51-root-INFO: Loss too large (1738.406->1814.749)! Learning rate decreased to 0.00076.
2024-12-01-23:04:52-root-INFO: Loss too large (1738.406->1787.034)! Learning rate decreased to 0.00061.
2024-12-01-23:04:52-root-INFO: Loss too large (1738.406->1761.453)! Learning rate decreased to 0.00048.
2024-12-01-23:04:52-root-INFO: Loss too large (1738.406->1740.757)! Learning rate decreased to 0.00039.
2024-12-01-23:04:53-root-INFO: grad norm: 303.580 299.990 46.545
2024-12-01-23:04:54-root-INFO: grad norm: 127.479 121.828 37.536
2024-12-01-23:04:55-root-INFO: grad norm: 102.346 98.205 28.819
2024-12-01-23:04:56-root-INFO: grad norm: 85.090 79.754 29.659
2024-12-01-23:04:57-root-INFO: grad norm: 76.216 71.792 25.591
2024-12-01-23:04:57-root-INFO: grad norm: 70.658 65.830 25.669
2024-12-01-23:04:58-root-INFO: grad norm: 67.388 63.130 23.574
2024-12-01-23:04:59-root-INFO: Loss Change: 1738.406 -> 1696.538
2024-12-01-23:04:59-root-INFO: Regularization Change: 0.000 -> 0.066
2024-12-01-23:04:59-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-01-23:04:59-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:04:59-root-INFO: step: 186 lr_xt 0.00376634
2024-12-01-23:04:59-root-INFO: grad norm: 323.325 317.983 58.528
2024-12-01-23:05:00-root-INFO: Loss too large (1697.302->1905.285)! Learning rate decreased to 0.00301.
2024-12-01-23:05:00-root-INFO: Loss too large (1697.302->1878.845)! Learning rate decreased to 0.00241.
2024-12-01-23:05:00-root-INFO: Loss too large (1697.302->1853.189)! Learning rate decreased to 0.00193.
2024-12-01-23:05:00-root-INFO: Loss too large (1697.302->1827.720)! Learning rate decreased to 0.00154.
2024-12-01-23:05:01-root-INFO: Loss too large (1697.302->1801.988)! Learning rate decreased to 0.00123.
2024-12-01-23:05:01-root-INFO: Loss too large (1697.302->1775.967)! Learning rate decreased to 0.00099.
2024-12-01-23:05:01-root-INFO: Loss too large (1697.302->1750.906)! Learning rate decreased to 0.00079.
2024-12-01-23:05:02-root-INFO: Loss too large (1697.302->1728.981)! Learning rate decreased to 0.00063.
2024-12-01-23:05:02-root-INFO: Loss too large (1697.302->1711.799)! Learning rate decreased to 0.00051.
2024-12-01-23:05:02-root-INFO: Loss too large (1697.302->1699.721)! Learning rate decreased to 0.00040.
2024-12-01-23:05:03-root-INFO: grad norm: 250.309 246.920 41.050
2024-12-01-23:05:04-root-INFO: grad norm: 173.108 168.961 37.664
2024-12-01-23:05:05-root-INFO: grad norm: 146.443 143.441 29.502
2024-12-01-23:05:06-root-INFO: grad norm: 121.150 117.460 29.672
2024-12-01-23:05:06-root-INFO: grad norm: 106.681 103.717 24.973
2024-12-01-23:05:07-root-INFO: grad norm: 93.797 90.320 25.302
2024-12-01-23:05:08-root-INFO: grad norm: 85.469 82.474 22.426
2024-12-01-23:05:09-root-INFO: Loss Change: 1697.302 -> 1669.809
2024-12-01-23:05:09-root-INFO: Regularization Change: 0.000 -> 0.060
2024-12-01-23:05:09-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-01-23:05:09-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:05:09-root-INFO: step: 185 lr_xt 0.00392795
2024-12-01-23:05:09-root-INFO: grad norm: 303.540 299.601 48.744
2024-12-01-23:05:10-root-INFO: Loss too large (1671.942->1882.747)! Learning rate decreased to 0.00314.
2024-12-01-23:05:10-root-INFO: Loss too large (1671.942->1860.456)! Learning rate decreased to 0.00251.
2024-12-01-23:05:10-root-INFO: Loss too large (1671.942->1837.301)! Learning rate decreased to 0.00201.
2024-12-01-23:05:11-root-INFO: Loss too large (1671.942->1813.163)! Learning rate decreased to 0.00161.
2024-12-01-23:05:11-root-INFO: Loss too large (1671.942->1787.864)! Learning rate decreased to 0.00129.
2024-12-01-23:05:11-root-INFO: Loss too large (1671.942->1761.505)! Learning rate decreased to 0.00103.
2024-12-01-23:05:11-root-INFO: Loss too large (1671.942->1735.457)! Learning rate decreased to 0.00082.
2024-12-01-23:05:12-root-INFO: Loss too large (1671.942->1712.133)! Learning rate decreased to 0.00066.
2024-12-01-23:05:12-root-INFO: Loss too large (1671.942->1693.418)! Learning rate decreased to 0.00053.
2024-12-01-23:05:12-root-INFO: Loss too large (1671.942->1679.899)! Learning rate decreased to 0.00042.
2024-12-01-23:05:13-root-INFO: grad norm: 272.190 269.392 38.927
2024-12-01-23:05:14-root-INFO: grad norm: 231.846 228.676 38.211
2024-12-01-23:05:15-root-INFO: grad norm: 217.668 215.256 32.312
2024-12-01-23:05:16-root-INFO: grad norm: 199.983 197.202 33.239
2024-12-01-23:05:17-root-INFO: grad norm: 191.121 188.886 29.141
2024-12-01-23:05:18-root-INFO: grad norm: 180.349 177.803 30.194
2024-12-01-23:05:18-root-INFO: grad norm: 174.230 172.105 27.130
2024-12-01-23:05:19-root-INFO: Loss Change: 1671.942 -> 1652.150
2024-12-01-23:05:19-root-INFO: Regularization Change: 0.000 -> 0.047
2024-12-01-23:05:19-root-INFO: Undo step: 185
2024-12-01-23:05:19-root-INFO: Undo step: 186
2024-12-01-23:05:19-root-INFO: Undo step: 187
2024-12-01-23:05:19-root-INFO: Undo step: 188
2024-12-01-23:05:19-root-INFO: Undo step: 189
2024-12-01-23:05:19-root-INFO: step: 190 lr_xt 0.00317856
2024-12-01-23:05:20-root-INFO: grad norm: 915.083 838.345 366.818
2024-12-01-23:05:21-root-INFO: grad norm: 859.548 828.679 228.284
2024-12-01-23:05:21-root-INFO: Loss too large (2651.063->3078.572)! Learning rate decreased to 0.00254.
2024-12-01-23:05:21-root-INFO: Loss too large (2651.063->2779.107)! Learning rate decreased to 0.00203.
2024-12-01-23:05:22-root-INFO: grad norm: 692.367 677.862 140.982
2024-12-01-23:05:23-root-INFO: grad norm: 382.048 370.177 94.497
2024-12-01-23:05:24-root-INFO: grad norm: 357.069 348.966 75.641
2024-12-01-23:05:24-root-INFO: Loss too large (2133.434->2136.996)! Learning rate decreased to 0.00163.
2024-12-01-23:05:25-root-INFO: grad norm: 441.497 430.666 97.197
2024-12-01-23:05:25-root-INFO: Loss too large (2083.020->2122.946)! Learning rate decreased to 0.00130.
2024-12-01-23:05:25-root-INFO: Loss too large (2083.020->2093.926)! Learning rate decreased to 0.00104.
2024-12-01-23:05:26-root-INFO: grad norm: 265.406 258.734 59.134
2024-12-01-23:05:27-root-INFO: grad norm: 121.579 115.016 39.405
2024-12-01-23:05:28-root-INFO: Loss Change: 3345.078 -> 2001.326
2024-12-01-23:05:28-root-INFO: Regularization Change: 0.000 -> 10.418
2024-12-01-23:05:28-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-01-23:05:28-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-23:05:28-root-INFO: step: 189 lr_xt 0.00331709
2024-12-01-23:05:28-root-INFO: grad norm: 250.036 241.281 65.586
2024-12-01-23:05:29-root-INFO: Loss too large (1995.859->2103.313)! Learning rate decreased to 0.00265.
2024-12-01-23:05:29-root-INFO: Loss too large (1995.859->2072.483)! Learning rate decreased to 0.00212.
2024-12-01-23:05:29-root-INFO: Loss too large (1995.859->2045.819)! Learning rate decreased to 0.00170.
2024-12-01-23:05:29-root-INFO: Loss too large (1995.859->2023.525)! Learning rate decreased to 0.00136.
2024-12-01-23:05:30-root-INFO: Loss too large (1995.859->2006.150)! Learning rate decreased to 0.00109.
2024-12-01-23:05:31-root-INFO: grad norm: 259.609 254.069 53.344
2024-12-01-23:05:32-root-INFO: grad norm: 313.463 306.381 66.257
2024-12-01-23:05:32-root-INFO: Loss too large (1975.929->1989.520)! Learning rate decreased to 0.00087.
2024-12-01-23:05:33-root-INFO: grad norm: 263.869 258.445 53.225
2024-12-01-23:05:34-root-INFO: grad norm: 195.026 189.155 47.493
2024-12-01-23:05:35-root-INFO: grad norm: 201.790 197.366 42.026
2024-12-01-23:05:35-root-INFO: grad norm: 224.049 218.181 50.938
2024-12-01-23:05:36-root-INFO: grad norm: 236.248 231.673 46.270
2024-12-01-23:05:37-root-INFO: Loss Change: 1995.859 -> 1924.679
2024-12-01-23:05:37-root-INFO: Regularization Change: 0.000 -> 0.517
2024-12-01-23:05:37-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-01-23:05:37-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-23:05:37-root-INFO: step: 188 lr_xt 0.00346111
2024-12-01-23:05:37-root-INFO: grad norm: 502.039 492.362 98.096
2024-12-01-23:05:38-root-INFO: Loss too large (1934.257->2174.461)! Learning rate decreased to 0.00277.
2024-12-01-23:05:38-root-INFO: Loss too large (1934.257->2130.873)! Learning rate decreased to 0.00222.
2024-12-01-23:05:38-root-INFO: Loss too large (1934.257->2091.164)! Learning rate decreased to 0.00177.
2024-12-01-23:05:38-root-INFO: Loss too large (1934.257->2054.062)! Learning rate decreased to 0.00142.
2024-12-01-23:05:39-root-INFO: Loss too large (1934.257->2019.113)! Learning rate decreased to 0.00113.
2024-12-01-23:05:39-root-INFO: Loss too large (1934.257->1986.260)! Learning rate decreased to 0.00091.
2024-12-01-23:05:39-root-INFO: Loss too large (1934.257->1956.120)! Learning rate decreased to 0.00073.
2024-12-01-23:05:40-root-INFO: grad norm: 315.816 309.986 60.399
2024-12-01-23:05:41-root-INFO: grad norm: 100.416 94.931 32.732
2024-12-01-23:05:42-root-INFO: grad norm: 98.508 94.020 29.397
2024-12-01-23:05:43-root-INFO: grad norm: 98.803 93.716 31.294
2024-12-01-23:05:44-root-INFO: grad norm: 100.750 96.749 28.110
2024-12-01-23:05:44-root-INFO: grad norm: 105.918 101.125 31.502
2024-12-01-23:05:45-root-INFO: grad norm: 113.375 109.866 27.990
2024-12-01-23:05:46-root-INFO: Loss Change: 1934.257 -> 1855.321
2024-12-01-23:05:46-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-01-23:05:46-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-01-23:05:46-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-23:05:46-root-INFO: step: 187 lr_xt 0.00361079
2024-12-01-23:05:47-root-INFO: grad norm: 451.304 442.224 90.075
2024-12-01-23:05:47-root-INFO: Loss too large (1877.391->2109.421)! Learning rate decreased to 0.00289.
2024-12-01-23:05:47-root-INFO: Loss too large (1877.391->2078.236)! Learning rate decreased to 0.00231.
2024-12-01-23:05:48-root-INFO: Loss too large (1877.391->2046.906)! Learning rate decreased to 0.00185.
2024-12-01-23:05:48-root-INFO: Loss too large (1877.391->2015.148)! Learning rate decreased to 0.00148.
2024-12-01-23:05:48-root-INFO: Loss too large (1877.391->1983.106)! Learning rate decreased to 0.00118.
2024-12-01-23:05:48-root-INFO: Loss too large (1877.391->1951.119)! Learning rate decreased to 0.00095.
2024-12-01-23:05:49-root-INFO: Loss too large (1877.391->1920.353)! Learning rate decreased to 0.00076.
2024-12-01-23:05:49-root-INFO: Loss too large (1877.391->1893.353)! Learning rate decreased to 0.00061.
2024-12-01-23:05:50-root-INFO: grad norm: 322.931 317.894 56.811
2024-12-01-23:05:51-root-INFO: grad norm: 166.670 161.023 43.016
2024-12-01-23:05:52-root-INFO: grad norm: 156.006 152.781 31.556
2024-12-01-23:05:52-root-INFO: grad norm: 146.190 141.195 37.889
2024-12-01-23:05:53-root-INFO: grad norm: 141.699 138.726 28.877
2024-12-01-23:05:54-root-INFO: grad norm: 138.452 133.825 35.492
2024-12-01-23:05:55-root-INFO: grad norm: 137.719 134.914 27.653
2024-12-01-23:05:56-root-INFO: Loss Change: 1877.391 -> 1820.184
2024-12-01-23:05:56-root-INFO: Regularization Change: 0.000 -> 0.164
2024-12-01-23:05:56-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-01-23:05:56-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:05:56-root-INFO: step: 186 lr_xt 0.00376634
2024-12-01-23:05:56-root-INFO: grad norm: 378.524 371.302 73.590
2024-12-01-23:05:57-root-INFO: Loss too large (1818.189->2068.176)! Learning rate decreased to 0.00301.
2024-12-01-23:05:57-root-INFO: Loss too large (1818.189->2029.179)! Learning rate decreased to 0.00241.
2024-12-01-23:05:57-root-INFO: Loss too large (1818.189->1993.122)! Learning rate decreased to 0.00193.
2024-12-01-23:05:57-root-INFO: Loss too large (1818.189->1958.847)! Learning rate decreased to 0.00154.
2024-12-01-23:05:58-root-INFO: Loss too large (1818.189->1925.768)! Learning rate decreased to 0.00123.
2024-12-01-23:05:58-root-INFO: Loss too large (1818.189->1893.843)! Learning rate decreased to 0.00099.
2024-12-01-23:05:58-root-INFO: Loss too large (1818.189->1864.274)! Learning rate decreased to 0.00079.
2024-12-01-23:05:58-root-INFO: Loss too large (1818.189->1839.347)! Learning rate decreased to 0.00063.
2024-12-01-23:05:59-root-INFO: Loss too large (1818.189->1820.751)! Learning rate decreased to 0.00051.
2024-12-01-23:06:00-root-INFO: grad norm: 265.243 261.096 46.723
2024-12-01-23:06:01-root-INFO: grad norm: 159.436 154.755 38.350
2024-12-01-23:06:02-root-INFO: grad norm: 133.017 129.866 28.780
2024-12-01-23:06:02-root-INFO: grad norm: 111.249 107.008 30.423
2024-12-01-23:06:03-root-INFO: grad norm: 99.730 96.534 25.043
2024-12-01-23:06:04-root-INFO: grad norm: 90.759 86.676 26.915
2024-12-01-23:06:05-root-INFO: grad norm: 85.272 81.973 23.488
2024-12-01-23:06:05-root-INFO: Loss Change: 1818.189 -> 1775.604
2024-12-01-23:06:05-root-INFO: Regularization Change: 0.000 -> 0.107
2024-12-01-23:06:05-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-01-23:06:05-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:06:06-root-INFO: step: 185 lr_xt 0.00392795
2024-12-01-23:06:06-root-INFO: grad norm: 250.800 245.482 51.372
2024-12-01-23:06:06-root-INFO: Loss too large (1773.652->1982.715)! Learning rate decreased to 0.00314.
2024-12-01-23:06:07-root-INFO: Loss too large (1773.652->1949.805)! Learning rate decreased to 0.00251.
2024-12-01-23:06:07-root-INFO: Loss too large (1773.652->1917.960)! Learning rate decreased to 0.00201.
2024-12-01-23:06:07-root-INFO: Loss too large (1773.652->1886.801)! Learning rate decreased to 0.00161.
2024-12-01-23:06:07-root-INFO: Loss too large (1773.652->1856.702)! Learning rate decreased to 0.00129.
2024-12-01-23:06:08-root-INFO: Loss too large (1773.652->1829.282)! Learning rate decreased to 0.00103.
2024-12-01-23:06:08-root-INFO: Loss too large (1773.652->1806.553)! Learning rate decreased to 0.00082.
2024-12-01-23:06:08-root-INFO: Loss too large (1773.652->1789.537)! Learning rate decreased to 0.00066.
2024-12-01-23:06:08-root-INFO: Loss too large (1773.652->1777.998)! Learning rate decreased to 0.00053.
2024-12-01-23:06:09-root-INFO: grad norm: 231.083 227.680 39.510
2024-12-01-23:06:10-root-INFO: grad norm: 211.440 207.126 42.492
2024-12-01-23:06:11-root-INFO: grad norm: 203.373 200.294 35.257
2024-12-01-23:06:12-root-INFO: grad norm: 194.961 190.997 39.115
2024-12-01-23:06:13-root-INFO: grad norm: 191.422 188.496 33.344
2024-12-01-23:06:14-root-INFO: grad norm: 188.471 184.684 37.596
2024-12-01-23:06:15-root-INFO: grad norm: 187.844 184.995 32.593
2024-12-01-23:06:16-root-INFO: Loss Change: 1773.652 -> 1748.253
2024-12-01-23:06:16-root-INFO: Regularization Change: 0.000 -> 0.092
2024-12-01-23:06:16-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-01-23:06:16-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:06:16-root-INFO: step: 184 lr_xt 0.00409583
2024-12-01-23:06:16-root-INFO: grad norm: 641.638 632.318 108.965
2024-12-01-23:06:16-root-INFO: Loss too large (1788.805->2099.857)! Learning rate decreased to 0.00328.
2024-12-01-23:06:17-root-INFO: Loss too large (1788.805->2056.145)! Learning rate decreased to 0.00262.
2024-12-01-23:06:17-root-INFO: Loss too large (1788.805->2019.998)! Learning rate decreased to 0.00210.
2024-12-01-23:06:17-root-INFO: Loss too large (1788.805->1987.414)! Learning rate decreased to 0.00168.
2024-12-01-23:06:18-root-INFO: Loss too large (1788.805->1955.728)! Learning rate decreased to 0.00134.
2024-12-01-23:06:18-root-INFO: Loss too large (1788.805->1923.500)! Learning rate decreased to 0.00107.
2024-12-01-23:06:18-root-INFO: Loss too large (1788.805->1889.973)! Learning rate decreased to 0.00086.
2024-12-01-23:06:18-root-INFO: Loss too large (1788.805->1854.775)! Learning rate decreased to 0.00069.
2024-12-01-23:06:19-root-INFO: Loss too large (1788.805->1818.940)! Learning rate decreased to 0.00055.
2024-12-01-23:06:20-root-INFO: grad norm: 419.770 413.864 70.169
2024-12-01-23:06:20-root-INFO: grad norm: 141.080 135.975 37.606
2024-12-01-23:06:21-root-INFO: grad norm: 148.954 145.891 30.055
2024-12-01-23:06:22-root-INFO: grad norm: 164.887 160.505 37.759
2024-12-01-23:06:23-root-INFO: grad norm: 180.490 177.690 31.667
2024-12-01-23:06:24-root-INFO: grad norm: 204.950 200.743 41.314
2024-12-01-23:06:25-root-INFO: grad norm: 223.531 220.551 36.376
2024-12-01-23:06:25-root-INFO: Loss Change: 1788.805 -> 1715.258
2024-12-01-23:06:25-root-INFO: Regularization Change: 0.000 -> 0.143
2024-12-01-23:06:25-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-01-23:06:25-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:06:26-root-INFO: step: 183 lr_xt 0.00427020
2024-12-01-23:06:26-root-INFO: grad norm: 472.137 465.651 77.991
2024-12-01-23:06:26-root-INFO: Loss too large (1725.765->2014.200)! Learning rate decreased to 0.00342.
2024-12-01-23:06:27-root-INFO: Loss too large (1725.765->1980.294)! Learning rate decreased to 0.00273.
2024-12-01-23:06:27-root-INFO: Loss too large (1725.765->1949.690)! Learning rate decreased to 0.00219.
2024-12-01-23:06:27-root-INFO: Loss too large (1725.765->1919.891)! Learning rate decreased to 0.00175.
2024-12-01-23:06:28-root-INFO: Loss too large (1725.765->1889.651)! Learning rate decreased to 0.00140.
2024-12-01-23:06:28-root-INFO: Loss too large (1725.765->1858.420)! Learning rate decreased to 0.00112.
2024-12-01-23:06:28-root-INFO: Loss too large (1725.765->1825.956)! Learning rate decreased to 0.00090.
2024-12-01-23:06:28-root-INFO: Loss too large (1725.765->1792.813)! Learning rate decreased to 0.00072.
2024-12-01-23:06:29-root-INFO: Loss too large (1725.765->1761.512)! Learning rate decreased to 0.00057.
2024-12-01-23:06:29-root-INFO: Loss too large (1725.765->1735.579)! Learning rate decreased to 0.00046.
2024-12-01-23:06:30-root-INFO: grad norm: 342.993 338.491 55.389
2024-12-01-23:06:31-root-INFO: grad norm: 213.250 209.044 42.143
2024-12-01-23:06:32-root-INFO: grad norm: 187.073 184.077 33.349
2024-12-01-23:06:32-root-INFO: grad norm: 161.442 157.729 34.424
2024-12-01-23:06:33-root-INFO: grad norm: 147.123 144.395 28.198
2024-12-01-23:06:34-root-INFO: grad norm: 133.414 129.928 30.298
2024-12-01-23:06:35-root-INFO: grad norm: 124.595 121.984 25.374
2024-12-01-23:06:36-root-INFO: Loss Change: 1725.765 -> 1680.435
2024-12-01-23:06:36-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-01-23:06:36-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-01-23:06:36-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:06:36-root-INFO: step: 182 lr_xt 0.00445127
2024-12-01-23:06:36-root-INFO: grad norm: 432.251 426.672 69.225
2024-12-01-23:06:37-root-INFO: Loss too large (1694.906->1971.864)! Learning rate decreased to 0.00356.
2024-12-01-23:06:37-root-INFO: Loss too large (1694.906->1946.880)! Learning rate decreased to 0.00285.
2024-12-01-23:06:37-root-INFO: Loss too large (1694.906->1922.169)! Learning rate decreased to 0.00228.
2024-12-01-23:06:37-root-INFO: Loss too large (1694.906->1896.126)! Learning rate decreased to 0.00182.
2024-12-01-23:06:38-root-INFO: Loss too large (1694.906->1868.092)! Learning rate decreased to 0.00146.
2024-12-01-23:06:38-root-INFO: Loss too large (1694.906->1837.889)! Learning rate decreased to 0.00117.
2024-12-01-23:06:38-root-INFO: Loss too large (1694.906->1805.545)! Learning rate decreased to 0.00093.
2024-12-01-23:06:39-root-INFO: Loss too large (1694.906->1771.972)! Learning rate decreased to 0.00075.
2024-12-01-23:06:39-root-INFO: Loss too large (1694.906->1740.056)! Learning rate decreased to 0.00060.
2024-12-01-23:06:39-root-INFO: Loss too large (1694.906->1713.437)! Learning rate decreased to 0.00048.
2024-12-01-23:06:40-root-INFO: grad norm: 370.023 365.565 57.262
2024-12-01-23:06:41-root-INFO: grad norm: 296.458 292.230 49.886
2024-12-01-23:06:42-root-INFO: grad norm: 284.281 280.815 44.258
2024-12-01-23:06:43-root-INFO: grad norm: 269.649 265.798 45.408
2024-12-01-23:06:44-root-INFO: grad norm: 263.629 260.418 41.025
2024-12-01-23:06:45-root-INFO: grad norm: 256.594 252.941 43.142
2024-12-01-23:06:45-root-INFO: grad norm: 253.450 250.380 39.327
2024-12-01-23:06:46-root-INFO: Loss Change: 1694.906 -> 1661.195
2024-12-01-23:06:46-root-INFO: Regularization Change: 0.000 -> 0.079
2024-12-01-23:06:46-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-01-23:06:46-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-23:06:46-root-INFO: step: 181 lr_xt 0.00463927
2024-12-01-23:06:47-root-INFO: grad norm: 591.721 585.327 86.753
2024-12-01-23:06:47-root-INFO: Loss too large (1691.466->1981.302)! Learning rate decreased to 0.00371.
2024-12-01-23:06:47-root-INFO: Loss too large (1691.466->1956.639)! Learning rate decreased to 0.00297.
2024-12-01-23:06:47-root-INFO: Loss too large (1691.466->1934.739)! Learning rate decreased to 0.00238.
2024-12-01-23:06:48-root-INFO: Loss too large (1691.466->1912.812)! Learning rate decreased to 0.00190.
2024-12-01-23:06:48-root-INFO: Loss too large (1691.466->1888.877)! Learning rate decreased to 0.00152.
2024-12-01-23:06:48-root-INFO: Loss too large (1691.466->1861.957)! Learning rate decreased to 0.00122.
2024-12-01-23:06:49-root-INFO: Loss too large (1691.466->1831.663)! Learning rate decreased to 0.00097.
2024-12-01-23:06:49-root-INFO: Loss too large (1691.466->1797.764)! Learning rate decreased to 0.00078.
2024-12-01-23:06:49-root-INFO: Loss too large (1691.466->1760.683)! Learning rate decreased to 0.00062.
2024-12-01-23:06:50-root-INFO: Loss too large (1691.466->1723.515)! Learning rate decreased to 0.00050.
2024-12-01-23:06:50-root-INFO: Loss too large (1691.466->1691.595)! Learning rate decreased to 0.00040.
2024-12-01-23:06:51-root-INFO: grad norm: 364.502 360.458 54.150
2024-12-01-23:06:52-root-INFO: grad norm: 200.009 196.282 38.430
2024-12-01-23:06:52-root-INFO: grad norm: 160.490 158.069 27.770
2024-12-01-23:06:53-root-INFO: grad norm: 128.212 124.847 29.181
2024-12-01-23:06:54-root-INFO: grad norm: 109.398 106.976 22.893
2024-12-01-23:06:55-root-INFO: grad norm: 94.830 91.521 24.834
2024-12-01-23:06:56-root-INFO: grad norm: 85.547 82.936 20.975
2024-12-01-23:06:56-root-INFO: Loss Change: 1691.466 -> 1634.870
2024-12-01-23:06:56-root-INFO: Regularization Change: 0.000 -> 0.068
2024-12-01-23:06:56-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-01-23:06:56-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-23:06:57-root-INFO: step: 180 lr_xt 0.00483443
2024-12-01-23:06:57-root-INFO: grad norm: 428.273 422.546 69.803
2024-12-01-23:06:57-root-INFO: Loss too large (1653.918->1945.328)! Learning rate decreased to 0.00387.
2024-12-01-23:06:57-root-INFO: Loss too large (1653.918->1918.486)! Learning rate decreased to 0.00309.
2024-12-01-23:06:58-root-INFO: Loss too large (1653.918->1893.973)! Learning rate decreased to 0.00248.
2024-12-01-23:06:58-root-INFO: Loss too large (1653.918->1869.279)! Learning rate decreased to 0.00198.
2024-12-01-23:06:58-root-INFO: Loss too large (1653.918->1843.004)! Learning rate decreased to 0.00158.
2024-12-01-23:06:59-root-INFO: Loss too large (1653.918->1814.462)! Learning rate decreased to 0.00127.
2024-12-01-23:06:59-root-INFO: Loss too large (1653.918->1783.269)! Learning rate decreased to 0.00101.
2024-12-01-23:06:59-root-INFO: Loss too large (1653.918->1749.680)! Learning rate decreased to 0.00081.
2024-12-01-23:06:59-root-INFO: Loss too large (1653.918->1715.919)! Learning rate decreased to 0.00065.
2024-12-01-23:07:00-root-INFO: Loss too large (1653.918->1685.906)! Learning rate decreased to 0.00052.
2024-12-01-23:07:00-root-INFO: Loss too large (1653.918->1662.728)! Learning rate decreased to 0.00042.
2024-12-01-23:07:01-root-INFO: grad norm: 348.651 344.795 51.710
2024-12-01-23:07:02-root-INFO: grad norm: 271.396 266.959 48.874
2024-12-01-23:07:03-root-INFO: grad norm: 243.648 240.705 37.757
2024-12-01-23:07:04-root-INFO: grad norm: 215.707 211.817 40.784
2024-12-01-23:07:05-root-INFO: grad norm: 199.171 196.575 32.054
2024-12-01-23:07:05-root-INFO: grad norm: 182.520 178.960 35.876
2024-12-01-23:07:06-root-INFO: grad norm: 171.238 168.856 28.459
2024-12-01-23:07:07-root-INFO: Loss Change: 1653.918 -> 1615.735
2024-12-01-23:07:07-root-INFO: Regularization Change: 0.000 -> 0.075
2024-12-01-23:07:07-root-INFO: Undo step: 180
2024-12-01-23:07:07-root-INFO: Undo step: 181
2024-12-01-23:07:07-root-INFO: Undo step: 182
2024-12-01-23:07:07-root-INFO: Undo step: 183
2024-12-01-23:07:07-root-INFO: Undo step: 184
2024-12-01-23:07:07-root-INFO: step: 185 lr_xt 0.00392795
2024-12-01-23:07:08-root-INFO: grad norm: 601.940 574.538 179.549
2024-12-01-23:07:08-root-INFO: grad norm: 791.750 768.822 189.160
2024-12-01-23:07:09-root-INFO: Loss too large (2711.571->2962.510)! Learning rate decreased to 0.00314.
2024-12-01-23:07:10-root-INFO: grad norm: 651.982 637.925 134.657
2024-12-01-23:07:10-root-INFO: grad norm: 455.803 444.237 102.031
2024-12-01-23:07:11-root-INFO: grad norm: 369.598 360.666 80.759
2024-12-01-23:07:12-root-INFO: grad norm: 297.836 291.210 62.477
2024-12-01-23:07:12-root-INFO: Loss too large (1968.181->2123.068)! Learning rate decreased to 0.00251.
2024-12-01-23:07:13-root-INFO: Loss too large (1968.181->2023.447)! Learning rate decreased to 0.00201.
2024-12-01-23:07:14-root-INFO: grad norm: 550.426 540.184 105.686
2024-12-01-23:07:14-root-INFO: Loss too large (1960.945->2046.585)! Learning rate decreased to 0.00161.
2024-12-01-23:07:14-root-INFO: Loss too large (1960.945->2009.003)! Learning rate decreased to 0.00129.
2024-12-01-23:07:14-root-INFO: Loss too large (1960.945->1973.760)! Learning rate decreased to 0.00103.
2024-12-01-23:07:15-root-INFO: grad norm: 269.548 264.873 49.988
2024-12-01-23:07:16-root-INFO: Loss Change: 2713.566 -> 1878.165
2024-12-01-23:07:16-root-INFO: Regularization Change: 0.000 -> 11.231
2024-12-01-23:07:16-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-01-23:07:16-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:07:16-root-INFO: step: 184 lr_xt 0.00409583
2024-12-01-23:07:17-root-INFO: grad norm: 194.511 186.624 54.827
2024-12-01-23:07:17-root-INFO: Loss too large (1870.110->1920.830)! Learning rate decreased to 0.00328.
2024-12-01-23:07:17-root-INFO: Loss too large (1870.110->1889.616)! Learning rate decreased to 0.00262.
2024-12-01-23:07:18-root-INFO: grad norm: 354.962 349.443 62.350
2024-12-01-23:07:18-root-INFO: Loss too large (1869.052->2262.988)! Learning rate decreased to 0.00210.
2024-12-01-23:07:19-root-INFO: Loss too large (1869.052->2113.857)! Learning rate decreased to 0.00168.
2024-12-01-23:07:19-root-INFO: Loss too large (1869.052->1996.210)! Learning rate decreased to 0.00134.
2024-12-01-23:07:19-root-INFO: Loss too large (1869.052->1910.282)! Learning rate decreased to 0.00107.
2024-12-01-23:07:20-root-INFO: grad norm: 546.465 538.357 93.788
2024-12-01-23:07:20-root-INFO: Loss too large (1855.704->1910.148)! Learning rate decreased to 0.00086.
2024-12-01-23:07:21-root-INFO: Loss too large (1855.704->1879.633)! Learning rate decreased to 0.00069.
2024-12-01-23:07:22-root-INFO: grad norm: 326.306 320.998 58.619
2024-12-01-23:07:23-root-INFO: grad norm: 96.044 91.985 27.627
2024-12-01-23:07:23-root-INFO: grad norm: 92.930 89.091 26.434
2024-12-01-23:07:24-root-INFO: grad norm: 90.780 86.743 26.769
2024-12-01-23:07:25-root-INFO: grad norm: 89.502 85.820 25.408
2024-12-01-23:07:26-root-INFO: Loss Change: 1870.110 -> 1779.941
2024-12-01-23:07:26-root-INFO: Regularization Change: 0.000 -> 0.628
2024-12-01-23:07:26-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-01-23:07:26-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:07:26-root-INFO: step: 183 lr_xt 0.00427020
2024-12-01-23:07:27-root-INFO: grad norm: 236.065 230.593 50.534
2024-12-01-23:07:27-root-INFO: Loss too large (1778.208->1957.214)! Learning rate decreased to 0.00342.
2024-12-01-23:07:27-root-INFO: Loss too large (1778.208->1929.085)! Learning rate decreased to 0.00273.
2024-12-01-23:07:28-root-INFO: Loss too large (1778.208->1901.496)! Learning rate decreased to 0.00219.
2024-12-01-23:07:28-root-INFO: Loss too large (1778.208->1874.446)! Learning rate decreased to 0.00175.
2024-12-01-23:07:28-root-INFO: Loss too large (1778.208->1848.657)! Learning rate decreased to 0.00140.
2024-12-01-23:07:28-root-INFO: Loss too large (1778.208->1825.462)! Learning rate decreased to 0.00112.
2024-12-01-23:07:29-root-INFO: Loss too large (1778.208->1806.199)! Learning rate decreased to 0.00090.
2024-12-01-23:07:29-root-INFO: Loss too large (1778.208->1791.573)! Learning rate decreased to 0.00072.
2024-12-01-23:07:29-root-INFO: Loss too large (1778.208->1781.465)! Learning rate decreased to 0.00057.
2024-12-01-23:07:30-root-INFO: grad norm: 235.090 231.024 43.536
2024-12-01-23:07:31-root-INFO: grad norm: 244.062 239.218 48.386
2024-12-01-23:07:32-root-INFO: grad norm: 252.108 248.080 44.887
2024-12-01-23:07:33-root-INFO: grad norm: 271.448 266.725 50.416
2024-12-01-23:07:33-root-INFO: Loss too large (1761.689->1763.347)! Learning rate decreased to 0.00046.
2024-12-01-23:07:34-root-INFO: grad norm: 214.616 211.012 39.166
2024-12-01-23:07:35-root-INFO: grad norm: 162.085 158.164 35.436
2024-12-01-23:07:36-root-INFO: grad norm: 144.117 140.923 30.169
2024-12-01-23:07:37-root-INFO: Loss Change: 1778.208 -> 1743.197
2024-12-01-23:07:37-root-INFO: Regularization Change: 0.000 -> 0.124
2024-12-01-23:07:37-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-01-23:07:37-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-23:07:37-root-INFO: step: 182 lr_xt 0.00445127
2024-12-01-23:07:37-root-INFO: grad norm: 447.784 441.366 75.540
2024-12-01-23:07:38-root-INFO: Loss too large (1755.248->2001.338)! Learning rate decreased to 0.00356.
2024-12-01-23:07:38-root-INFO: Loss too large (1755.248->1980.884)! Learning rate decreased to 0.00285.
2024-12-01-23:07:38-root-INFO: Loss too large (1755.248->1959.714)! Learning rate decreased to 0.00228.
2024-12-01-23:07:38-root-INFO: Loss too large (1755.248->1936.831)! Learning rate decreased to 0.00182.
2024-12-01-23:07:39-root-INFO: Loss too large (1755.248->1911.917)! Learning rate decreased to 0.00146.
2024-12-01-23:07:39-root-INFO: Loss too large (1755.248->1884.895)! Learning rate decreased to 0.00117.
2024-12-01-23:07:39-root-INFO: Loss too large (1755.248->1855.996)! Learning rate decreased to 0.00093.
2024-12-01-23:07:40-root-INFO: Loss too large (1755.248->1826.221)! Learning rate decreased to 0.00075.
2024-12-01-23:07:40-root-INFO: Loss too large (1755.248->1797.710)! Learning rate decreased to 0.00060.
2024-12-01-23:07:40-root-INFO: Loss too large (1755.248->1773.136)! Learning rate decreased to 0.00048.
2024-12-01-23:07:41-root-INFO: grad norm: 357.450 352.500 59.284
2024-12-01-23:07:42-root-INFO: grad norm: 249.053 244.625 46.753
2024-12-01-23:07:43-root-INFO: grad norm: 247.101 243.467 42.218
2024-12-01-23:07:44-root-INFO: grad norm: 248.106 243.945 45.251
2024-12-01-23:07:45-root-INFO: grad norm: 251.264 247.654 42.440
2024-12-01-23:07:45-root-INFO: grad norm: 258.628 254.543 45.783
2024-12-01-23:07:46-root-INFO: Loss too large (1722.036->1722.143)! Learning rate decreased to 0.00038.
2024-12-01-23:07:47-root-INFO: grad norm: 192.333 189.264 34.221
2024-12-01-23:07:47-root-INFO: Loss Change: 1755.248 -> 1712.862
2024-12-01-23:07:47-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-01-23:07:47-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-01-23:07:47-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-23:07:48-root-INFO: step: 181 lr_xt 0.00463927
2024-12-01-23:07:48-root-INFO: grad norm: 477.059 470.658 77.890
2024-12-01-23:07:48-root-INFO: Loss too large (1727.814->1985.627)! Learning rate decreased to 0.00371.
2024-12-01-23:07:48-root-INFO: Loss too large (1727.814->1966.469)! Learning rate decreased to 0.00297.
2024-12-01-23:07:49-root-INFO: Loss too large (1727.814->1947.219)! Learning rate decreased to 0.00238.
2024-12-01-23:07:49-root-INFO: Loss too large (1727.814->1926.416)! Learning rate decreased to 0.00190.
2024-12-01-23:07:49-root-INFO: Loss too large (1727.814->1903.413)! Learning rate decreased to 0.00152.
2024-12-01-23:07:50-root-INFO: Loss too large (1727.814->1877.935)! Learning rate decreased to 0.00122.
2024-12-01-23:07:50-root-INFO: Loss too large (1727.814->1849.870)! Learning rate decreased to 0.00097.
2024-12-01-23:07:50-root-INFO: Loss too large (1727.814->1819.612)! Learning rate decreased to 0.00078.
2024-12-01-23:07:50-root-INFO: Loss too large (1727.814->1788.754)! Learning rate decreased to 0.00062.
2024-12-01-23:07:51-root-INFO: Loss too large (1727.814->1760.114)! Learning rate decreased to 0.00050.
2024-12-01-23:07:51-root-INFO: Loss too large (1727.814->1736.589)! Learning rate decreased to 0.00040.
2024-12-01-23:07:52-root-INFO: grad norm: 345.279 340.910 54.753
2024-12-01-23:07:53-root-INFO: grad norm: 222.189 218.053 42.671
2024-12-01-23:07:54-root-INFO: grad norm: 198.211 195.242 34.179
2024-12-01-23:07:55-root-INFO: grad norm: 175.160 171.523 35.511
2024-12-01-23:07:55-root-INFO: grad norm: 162.841 160.111 29.693
2024-12-01-23:07:56-root-INFO: grad norm: 151.310 147.941 31.753
2024-12-01-23:07:57-root-INFO: grad norm: 144.416 141.794 27.390
2024-12-01-23:07:58-root-INFO: Loss Change: 1727.814 -> 1687.319
2024-12-01-23:07:58-root-INFO: Regularization Change: 0.000 -> 0.066
2024-12-01-23:07:58-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-01-23:07:58-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-23:07:58-root-INFO: step: 180 lr_xt 0.00483443
2024-12-01-23:07:58-root-INFO: grad norm: 551.371 544.184 88.733
2024-12-01-23:07:59-root-INFO: Loss too large (1713.391->1984.797)! Learning rate decreased to 0.00387.
2024-12-01-23:07:59-root-INFO: Loss too large (1713.391->1962.466)! Learning rate decreased to 0.00309.
2024-12-01-23:07:59-root-INFO: Loss too large (1713.391->1942.919)! Learning rate decreased to 0.00248.
2024-12-01-23:08:00-root-INFO: Loss too large (1713.391->1923.777)! Learning rate decreased to 0.00198.
2024-12-01-23:08:00-root-INFO: Loss too large (1713.391->1903.385)! Learning rate decreased to 0.00158.
2024-12-01-23:08:00-root-INFO: Loss too large (1713.391->1880.840)! Learning rate decreased to 0.00127.
2024-12-01-23:08:00-root-INFO: Loss too large (1713.391->1855.603)! Learning rate decreased to 0.00101.
2024-12-01-23:08:01-root-INFO: Loss too large (1713.391->1827.343)! Learning rate decreased to 0.00081.
2024-12-01-23:08:01-root-INFO: Loss too large (1713.391->1796.383)! Learning rate decreased to 0.00065.
2024-12-01-23:08:01-root-INFO: Loss too large (1713.391->1764.547)! Learning rate decreased to 0.00052.
2024-12-01-23:08:02-root-INFO: Loss too large (1713.391->1735.133)! Learning rate decreased to 0.00042.
2024-12-01-23:08:02-root-INFO: grad norm: 431.145 425.817 67.570
2024-12-01-23:08:03-root-INFO: grad norm: 296.780 291.938 53.395
2024-12-01-23:08:04-root-INFO: grad norm: 301.695 297.905 47.668
2024-12-01-23:08:05-root-INFO: grad norm: 309.191 304.612 53.015
2024-12-01-23:08:05-root-INFO: Loss too large (1678.719->1679.231)! Learning rate decreased to 0.00033.
2024-12-01-23:08:06-root-INFO: grad norm: 225.949 222.876 37.141
2024-12-01-23:08:07-root-INFO: grad norm: 160.643 157.087 33.612
2024-12-01-23:08:08-root-INFO: grad norm: 130.947 128.296 26.215
2024-12-01-23:08:08-root-INFO: Loss Change: 1713.391 -> 1663.070
2024-12-01-23:08:08-root-INFO: Regularization Change: 0.000 -> 0.074
2024-12-01-23:08:08-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-01-23:08:08-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-23:08:09-root-INFO: step: 179 lr_xt 0.00503698
2024-12-01-23:08:09-root-INFO: grad norm: 498.189 492.190 77.080
2024-12-01-23:08:09-root-INFO: Loss too large (1682.631->1950.225)! Learning rate decreased to 0.00403.
2024-12-01-23:08:10-root-INFO: Loss too large (1682.631->1933.516)! Learning rate decreased to 0.00322.
2024-12-01-23:08:10-root-INFO: Loss too large (1682.631->1917.674)! Learning rate decreased to 0.00258.
2024-12-01-23:08:10-root-INFO: Loss too large (1682.631->1900.840)! Learning rate decreased to 0.00206.
2024-12-01-23:08:10-root-INFO: Loss too large (1682.631->1881.798)! Learning rate decreased to 0.00165.
2024-12-01-23:08:11-root-INFO: Loss too large (1682.631->1859.931)! Learning rate decreased to 0.00132.
2024-12-01-23:08:11-root-INFO: Loss too large (1682.631->1834.849)! Learning rate decreased to 0.00106.
2024-12-01-23:08:11-root-INFO: Loss too large (1682.631->1806.333)! Learning rate decreased to 0.00085.
2024-12-01-23:08:12-root-INFO: Loss too large (1682.631->1774.905)! Learning rate decreased to 0.00068.
2024-12-01-23:08:12-root-INFO: Loss too large (1682.631->1742.654)! Learning rate decreased to 0.00054.
2024-12-01-23:08:12-root-INFO: Loss too large (1682.631->1713.005)! Learning rate decreased to 0.00043.
2024-12-01-23:08:13-root-INFO: Loss too large (1682.631->1689.198)! Learning rate decreased to 0.00035.
2024-12-01-23:08:13-root-INFO: grad norm: 363.684 359.344 56.016
2024-12-01-23:08:14-root-INFO: grad norm: 252.999 249.016 44.717
2024-12-01-23:08:15-root-INFO: grad norm: 219.833 216.876 35.936
2024-12-01-23:08:16-root-INFO: grad norm: 188.938 185.519 35.782
2024-12-01-23:08:17-root-INFO: grad norm: 170.388 167.794 29.621
2024-12-01-23:08:18-root-INFO: grad norm: 153.129 150.008 30.758
2024-12-01-23:08:19-root-INFO: grad norm: 141.269 138.844 26.066
2024-12-01-23:08:19-root-INFO: Loss Change: 1682.631 -> 1643.596
2024-12-01-23:08:19-root-INFO: Regularization Change: 0.000 -> 0.053
2024-12-01-23:08:19-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-01-23:08:19-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-23:08:19-root-INFO: step: 178 lr_xt 0.00524717
2024-12-01-23:08:20-root-INFO: grad norm: 539.202 532.938 81.955
2024-12-01-23:08:20-root-INFO: Loss too large (1669.933->1942.949)! Learning rate decreased to 0.00420.
2024-12-01-23:08:20-root-INFO: Loss too large (1669.933->1923.466)! Learning rate decreased to 0.00336.
2024-12-01-23:08:21-root-INFO: Loss too large (1669.933->1906.983)! Learning rate decreased to 0.00269.
2024-12-01-23:08:21-root-INFO: Loss too large (1669.933->1891.182)! Learning rate decreased to 0.00215.
2024-12-01-23:08:21-root-INFO: Loss too large (1669.933->1874.218)! Learning rate decreased to 0.00172.
2024-12-01-23:08:21-root-INFO: Loss too large (1669.933->1854.916)! Learning rate decreased to 0.00138.
2024-12-01-23:08:22-root-INFO: Loss too large (1669.933->1832.582)! Learning rate decreased to 0.00110.
2024-12-01-23:08:22-root-INFO: Loss too large (1669.933->1806.690)! Learning rate decreased to 0.00088.
2024-12-01-23:08:22-root-INFO: Loss too large (1669.933->1777.031)! Learning rate decreased to 0.00070.
2024-12-01-23:08:23-root-INFO: Loss too large (1669.933->1744.583)! Learning rate decreased to 0.00056.
2024-12-01-23:08:23-root-INFO: Loss too large (1669.933->1712.270)! Learning rate decreased to 0.00045.
2024-12-01-23:08:23-root-INFO: Loss too large (1669.933->1684.087)! Learning rate decreased to 0.00036.
2024-12-01-23:08:24-root-INFO: grad norm: 432.064 427.006 65.920
2024-12-01-23:08:25-root-INFO: grad norm: 335.831 331.274 55.134
2024-12-01-23:08:26-root-INFO: grad norm: 319.116 315.297 49.226
2024-12-01-23:08:27-root-INFO: grad norm: 301.218 297.133 49.440
2024-12-01-23:08:28-root-INFO: grad norm: 292.190 288.686 45.114
2024-12-01-23:08:29-root-INFO: grad norm: 282.310 278.522 46.091
2024-12-01-23:08:29-root-INFO: grad norm: 276.605 273.300 42.632
2024-12-01-23:08:30-root-INFO: Loss Change: 1669.933 -> 1627.920
2024-12-01-23:08:30-root-INFO: Regularization Change: 0.000 -> 0.066
2024-12-01-23:08:30-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-01-23:08:30-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-23:08:30-root-INFO: step: 177 lr_xt 0.00546525
2024-12-01-23:08:31-root-INFO: grad norm: 555.976 550.155 80.244
2024-12-01-23:08:31-root-INFO: Loss too large (1648.686->1917.441)! Learning rate decreased to 0.00437.
2024-12-01-23:08:31-root-INFO: Loss too large (1648.686->1901.944)! Learning rate decreased to 0.00350.
2024-12-01-23:08:31-root-INFO: Loss too large (1648.686->1888.441)! Learning rate decreased to 0.00280.
2024-12-01-23:08:32-root-INFO: Loss too large (1648.686->1875.137)! Learning rate decreased to 0.00224.
2024-12-01-23:08:32-root-INFO: Loss too large (1648.686->1860.412)! Learning rate decreased to 0.00179.
2024-12-01-23:08:32-root-INFO: Loss too large (1648.686->1843.089)! Learning rate decreased to 0.00143.
2024-12-01-23:08:33-root-INFO: Loss too large (1648.686->1822.457)! Learning rate decreased to 0.00115.
2024-12-01-23:08:33-root-INFO: Loss too large (1648.686->1797.998)! Learning rate decreased to 0.00092.
2024-12-01-23:08:33-root-INFO: Loss too large (1648.686->1769.318)! Learning rate decreased to 0.00073.
2024-12-01-23:08:34-root-INFO: Loss too large (1648.686->1736.803)! Learning rate decreased to 0.00059.
2024-12-01-23:08:34-root-INFO: Loss too large (1648.686->1702.783)! Learning rate decreased to 0.00047.
2024-12-01-23:08:34-root-INFO: Loss too large (1648.686->1671.419)! Learning rate decreased to 0.00038.
2024-12-01-23:08:35-root-INFO: grad norm: 472.504 467.064 71.495
2024-12-01-23:08:36-root-INFO: grad norm: 395.079 390.571 59.507
2024-12-01-23:08:36-root-INFO: Loss too large (1627.547->1628.623)! Learning rate decreased to 0.00030.
2024-12-01-23:08:37-root-INFO: grad norm: 281.667 278.243 43.790
2024-12-01-23:08:38-root-INFO: grad norm: 201.735 198.526 35.837
2024-12-01-23:08:39-root-INFO: grad norm: 162.093 159.463 29.081
2024-12-01-23:08:40-root-INFO: grad norm: 131.719 128.800 27.577
2024-12-01-23:08:41-root-INFO: grad norm: 112.446 109.887 23.851
2024-12-01-23:08:41-root-INFO: Loss Change: 1648.686 -> 1603.974
2024-12-01-23:08:41-root-INFO: Regularization Change: 0.000 -> 0.051
2024-12-01-23:08:41-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-01-23:08:41-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-23:08:41-root-INFO: step: 176 lr_xt 0.00569148
2024-12-01-23:08:42-root-INFO: grad norm: 530.682 524.994 77.490
2024-12-01-23:08:42-root-INFO: Loss too large (1633.232->1909.114)! Learning rate decreased to 0.00455.
2024-12-01-23:08:42-root-INFO: Loss too large (1633.232->1891.589)! Learning rate decreased to 0.00364.
2024-12-01-23:08:43-root-INFO: Loss too large (1633.232->1877.015)! Learning rate decreased to 0.00291.
2024-12-01-23:08:43-root-INFO: Loss too large (1633.232->1863.312)! Learning rate decreased to 0.00233.
2024-12-01-23:08:43-root-INFO: Loss too large (1633.232->1848.630)! Learning rate decreased to 0.00186.
2024-12-01-23:08:44-root-INFO: Loss too large (1633.232->1831.592)! Learning rate decreased to 0.00149.
2024-12-01-23:08:44-root-INFO: Loss too large (1633.232->1811.311)! Learning rate decreased to 0.00119.
2024-12-01-23:08:44-root-INFO: Loss too large (1633.232->1787.117)! Learning rate decreased to 0.00095.
2024-12-01-23:08:44-root-INFO: Loss too large (1633.232->1758.494)! Learning rate decreased to 0.00076.
2024-12-01-23:08:45-root-INFO: Loss too large (1633.232->1725.799)! Learning rate decreased to 0.00061.
2024-12-01-23:08:45-root-INFO: Loss too large (1633.232->1691.451)! Learning rate decreased to 0.00049.
2024-12-01-23:08:45-root-INFO: Loss too large (1633.232->1659.718)! Learning rate decreased to 0.00039.
2024-12-01-23:08:46-root-INFO: Loss too large (1633.232->1634.673)! Learning rate decreased to 0.00031.
2024-12-01-23:08:46-root-INFO: grad norm: 374.777 370.621 55.660
2024-12-01-23:08:47-root-INFO: grad norm: 275.603 271.711 46.156
2024-12-01-23:08:48-root-INFO: grad norm: 232.714 229.750 37.024
2024-12-01-23:08:49-root-INFO: grad norm: 196.830 193.497 36.070
2024-12-01-23:08:50-root-INFO: grad norm: 172.570 169.980 29.785
2024-12-01-23:08:51-root-INFO: grad norm: 151.672 148.628 30.232
2024-12-01-23:08:52-root-INFO: grad norm: 136.046 133.607 25.645
2024-12-01-23:08:52-root-INFO: Loss Change: 1633.232 -> 1589.664
2024-12-01-23:08:52-root-INFO: Regularization Change: 0.000 -> 0.053
2024-12-01-23:08:52-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-01-23:08:52-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-23:08:52-root-INFO: step: 175 lr_xt 0.00592610
2024-12-01-23:08:53-root-INFO: grad norm: 500.569 495.517 70.934
2024-12-01-23:08:53-root-INFO: Loss too large (1614.460->1888.017)! Learning rate decreased to 0.00474.
2024-12-01-23:08:53-root-INFO: Loss too large (1614.460->1872.143)! Learning rate decreased to 0.00379.
2024-12-01-23:08:54-root-INFO: Loss too large (1614.460->1859.005)! Learning rate decreased to 0.00303.
2024-12-01-23:08:54-root-INFO: Loss too large (1614.460->1846.538)! Learning rate decreased to 0.00243.
2024-12-01-23:08:54-root-INFO: Loss too large (1614.460->1832.911)! Learning rate decreased to 0.00194.
2024-12-01-23:08:54-root-INFO: Loss too large (1614.460->1816.743)! Learning rate decreased to 0.00155.
2024-12-01-23:08:55-root-INFO: Loss too large (1614.460->1797.137)! Learning rate decreased to 0.00124.
2024-12-01-23:08:55-root-INFO: Loss too large (1614.460->1773.412)! Learning rate decreased to 0.00099.
2024-12-01-23:08:55-root-INFO: Loss too large (1614.460->1745.023)! Learning rate decreased to 0.00080.
2024-12-01-23:08:56-root-INFO: Loss too large (1614.460->1712.253)! Learning rate decreased to 0.00064.
2024-12-01-23:08:56-root-INFO: Loss too large (1614.460->1677.525)! Learning rate decreased to 0.00051.
2024-12-01-23:08:56-root-INFO: Loss too large (1614.460->1645.273)! Learning rate decreased to 0.00041.
2024-12-01-23:08:57-root-INFO: Loss too large (1614.460->1619.750)! Learning rate decreased to 0.00033.
2024-12-01-23:08:57-root-INFO: grad norm: 390.920 386.817 56.485
2024-12-01-23:08:58-root-INFO: grad norm: 319.971 316.168 49.182
2024-12-01-23:08:59-root-INFO: grad norm: 284.729 281.556 42.390
2024-12-01-23:09:00-root-INFO: grad norm: 254.860 251.565 40.845
2024-12-01-23:09:01-root-INFO: grad norm: 233.643 230.881 35.820
2024-12-01-23:09:02-root-INFO: grad norm: 214.593 211.619 35.602
2024-12-01-23:09:03-root-INFO: grad norm: 199.533 197.031 31.502
2024-12-01-23:09:03-root-INFO: Loss Change: 1614.460 -> 1574.920
2024-12-01-23:09:03-root-INFO: Regularization Change: 0.000 -> 0.054
2024-12-01-23:09:03-root-INFO: Undo step: 175
2024-12-01-23:09:03-root-INFO: Undo step: 176
2024-12-01-23:09:03-root-INFO: Undo step: 177
2024-12-01-23:09:03-root-INFO: Undo step: 178
2024-12-01-23:09:03-root-INFO: Undo step: 179
2024-12-01-23:09:04-root-INFO: step: 180 lr_xt 0.00483443
2024-12-01-23:09:04-root-INFO: grad norm: 927.520 909.334 182.770
2024-12-01-23:09:04-root-INFO: Loss too large (2837.473->2918.312)! Learning rate decreased to 0.00387.
2024-12-01-23:09:05-root-INFO: grad norm: 668.130 653.885 137.231
2024-12-01-23:09:06-root-INFO: grad norm: 476.970 470.612 77.616
2024-12-01-23:09:07-root-INFO: grad norm: 299.225 292.916 61.119
2024-12-01-23:09:08-root-INFO: grad norm: 255.038 250.673 46.981
2024-12-01-23:09:09-root-INFO: grad norm: 370.424 365.935 57.498
2024-12-01-23:09:09-root-INFO: Loss too large (1839.656->1935.283)! Learning rate decreased to 0.00309.
2024-12-01-23:09:09-root-INFO: Loss too large (1839.656->1880.807)! Learning rate decreased to 0.00248.
2024-12-01-23:09:10-root-INFO: grad norm: 240.717 236.414 45.310
2024-12-01-23:09:11-root-INFO: grad norm: 117.105 114.006 26.763
2024-12-01-23:09:12-root-INFO: Loss Change: 2837.473 -> 1708.081
2024-12-01-23:09:12-root-INFO: Regularization Change: 0.000 -> 12.179
2024-12-01-23:09:12-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-01-23:09:12-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-23:09:12-root-INFO: step: 179 lr_xt 0.00503698
2024-12-01-23:09:12-root-INFO: grad norm: 135.196 131.995 29.248
2024-12-01-23:09:12-root-INFO: Loss too large (1702.257->1724.195)! Learning rate decreased to 0.00403.
2024-12-01-23:09:13-root-INFO: Loss too large (1702.257->1706.881)! Learning rate decreased to 0.00322.
2024-12-01-23:09:14-root-INFO: grad norm: 262.339 258.556 44.392
2024-12-01-23:09:14-root-INFO: Loss too large (1696.015->2024.177)! Learning rate decreased to 0.00258.
2024-12-01-23:09:14-root-INFO: Loss too large (1696.015->1888.901)! Learning rate decreased to 0.00206.
2024-12-01-23:09:14-root-INFO: Loss too large (1696.015->1787.215)! Learning rate decreased to 0.00165.
2024-12-01-23:09:15-root-INFO: Loss too large (1696.015->1720.075)! Learning rate decreased to 0.00132.
2024-12-01-23:09:16-root-INFO: grad norm: 381.290 378.511 45.956
2024-12-01-23:09:16-root-INFO: Loss too large (1682.499->1722.377)! Learning rate decreased to 0.00106.
2024-12-01-23:09:16-root-INFO: Loss too large (1682.499->1698.365)! Learning rate decreased to 0.00085.
2024-12-01-23:09:17-root-INFO: grad norm: 252.890 249.280 42.576
2024-12-01-23:09:18-root-INFO: grad norm: 106.267 103.675 23.331
2024-12-01-23:09:19-root-INFO: grad norm: 106.085 102.881 25.879
2024-12-01-23:09:20-root-INFO: grad norm: 113.628 111.357 22.604
2024-12-01-23:09:21-root-INFO: grad norm: 127.472 124.446 27.612
2024-12-01-23:09:21-root-INFO: Loss Change: 1702.257 -> 1629.044
2024-12-01-23:09:21-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-01-23:09:21-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-01-23:09:21-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-23:09:21-root-INFO: step: 178 lr_xt 0.00524717
2024-12-01-23:09:22-root-INFO: grad norm: 380.874 377.751 48.672
2024-12-01-23:09:22-root-INFO: Loss too large (1639.080->1863.463)! Learning rate decreased to 0.00420.
2024-12-01-23:09:22-root-INFO: Loss too large (1639.080->1841.309)! Learning rate decreased to 0.00336.
2024-12-01-23:09:23-root-INFO: Loss too large (1639.080->1817.274)! Learning rate decreased to 0.00269.
2024-12-01-23:09:23-root-INFO: Loss too large (1639.080->1791.054)! Learning rate decreased to 0.00215.
2024-12-01-23:09:23-root-INFO: Loss too large (1639.080->1763.175)! Learning rate decreased to 0.00172.
2024-12-01-23:09:23-root-INFO: Loss too large (1639.080->1734.496)! Learning rate decreased to 0.00138.
2024-12-01-23:09:24-root-INFO: Loss too large (1639.080->1706.190)! Learning rate decreased to 0.00110.
2024-12-01-23:09:24-root-INFO: Loss too large (1639.080->1679.719)! Learning rate decreased to 0.00088.
2024-12-01-23:09:24-root-INFO: Loss too large (1639.080->1656.800)! Learning rate decreased to 0.00070.
2024-12-01-23:09:25-root-INFO: grad norm: 288.048 284.324 46.168
2024-12-01-23:09:26-root-INFO: grad norm: 165.326 163.235 26.210
2024-12-01-23:09:26-root-INFO: grad norm: 170.887 167.996 31.301
2024-12-01-23:09:27-root-INFO: grad norm: 189.019 187.167 26.396
2024-12-01-23:09:28-root-INFO: grad norm: 205.093 202.186 34.411
2024-12-01-23:09:28-root-INFO: grad norm: 239.279 237.364 30.210
2024-12-01-23:09:29-root-INFO: Loss too large (1600.297->1603.110)! Learning rate decreased to 0.00056.
2024-12-01-23:09:29-root-INFO: grad norm: 196.291 193.498 32.999
2024-12-01-23:09:30-root-INFO: Loss Change: 1639.080 -> 1590.305
2024-12-01-23:09:30-root-INFO: Regularization Change: 0.000 -> 0.165
2024-12-01-23:09:30-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-01-23:09:30-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-23:09:30-root-INFO: step: 177 lr_xt 0.00546525
2024-12-01-23:09:30-root-INFO: grad norm: 346.242 343.540 43.172
2024-12-01-23:09:30-root-INFO: Loss too large (1595.456->1825.354)! Learning rate decreased to 0.00437.
2024-12-01-23:09:31-root-INFO: Loss too large (1595.456->1806.732)! Learning rate decreased to 0.00350.
2024-12-01-23:09:31-root-INFO: Loss too large (1595.456->1785.696)! Learning rate decreased to 0.00280.
2024-12-01-23:09:31-root-INFO: Loss too large (1595.456->1762.016)! Learning rate decreased to 0.00224.
2024-12-01-23:09:31-root-INFO: Loss too large (1595.456->1736.072)! Learning rate decreased to 0.00179.
2024-12-01-23:09:31-root-INFO: Loss too large (1595.456->1708.563)! Learning rate decreased to 0.00143.
2024-12-01-23:09:32-root-INFO: Loss too large (1595.456->1680.513)! Learning rate decreased to 0.00115.
2024-12-01-23:09:32-root-INFO: Loss too large (1595.456->1653.326)! Learning rate decreased to 0.00092.
2024-12-01-23:09:32-root-INFO: Loss too large (1595.456->1628.853)! Learning rate decreased to 0.00073.
2024-12-01-23:09:32-root-INFO: Loss too large (1595.456->1608.939)! Learning rate decreased to 0.00059.
2024-12-01-23:09:33-root-INFO: grad norm: 288.402 284.897 44.829
2024-12-01-23:09:34-root-INFO: grad norm: 217.533 215.573 29.139
2024-12-01-23:09:34-root-INFO: grad norm: 211.903 209.008 34.904
2024-12-01-23:09:35-root-INFO: grad norm: 208.890 207.083 27.414
2024-12-01-23:09:36-root-INFO: grad norm: 211.097 208.303 34.233
2024-12-01-23:09:36-root-INFO: grad norm: 217.951 216.174 27.774
2024-12-01-23:09:37-root-INFO: grad norm: 224.143 221.342 35.325
2024-12-01-23:09:37-root-INFO: Loss Change: 1595.456 -> 1564.154
2024-12-01-23:09:37-root-INFO: Regularization Change: 0.000 -> 0.106
2024-12-01-23:09:37-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-01-23:09:37-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-23:09:38-root-INFO: step: 176 lr_xt 0.00569148
2024-12-01-23:09:38-root-INFO: grad norm: 527.947 524.077 63.810
2024-12-01-23:09:38-root-INFO: Loss too large (1591.049->1840.503)! Learning rate decreased to 0.00455.
2024-12-01-23:09:38-root-INFO: Loss too large (1591.049->1822.882)! Learning rate decreased to 0.00364.
2024-12-01-23:09:38-root-INFO: Loss too large (1591.049->1805.804)! Learning rate decreased to 0.00291.
2024-12-01-23:09:39-root-INFO: Loss too large (1591.049->1787.442)! Learning rate decreased to 0.00233.
2024-12-01-23:09:39-root-INFO: Loss too large (1591.049->1766.379)! Learning rate decreased to 0.00186.
2024-12-01-23:09:39-root-INFO: Loss too large (1591.049->1742.175)! Learning rate decreased to 0.00149.
2024-12-01-23:09:39-root-INFO: Loss too large (1591.049->1714.965)! Learning rate decreased to 0.00119.
2024-12-01-23:09:40-root-INFO: Loss too large (1591.049->1685.231)! Learning rate decreased to 0.00095.
2024-12-01-23:09:40-root-INFO: Loss too large (1591.049->1653.921)! Learning rate decreased to 0.00076.
2024-12-01-23:09:40-root-INFO: Loss too large (1591.049->1622.863)! Learning rate decreased to 0.00061.
2024-12-01-23:09:40-root-INFO: Loss too large (1591.049->1595.121)! Learning rate decreased to 0.00049.
2024-12-01-23:09:41-root-INFO: grad norm: 326.303 322.866 47.237
2024-12-01-23:09:42-root-INFO: grad norm: 156.618 154.643 24.795
2024-12-01-23:09:42-root-INFO: grad norm: 134.908 132.552 25.101
2024-12-01-23:09:43-root-INFO: grad norm: 116.542 114.730 20.467
2024-12-01-23:09:43-root-INFO: grad norm: 105.785 103.549 21.638
2024-12-01-23:09:44-root-INFO: grad norm: 96.810 95.058 18.334
2024-12-01-23:09:45-root-INFO: grad norm: 90.883 88.718 19.717
2024-12-01-23:09:45-root-INFO: Loss Change: 1591.049 -> 1535.468
2024-12-01-23:09:45-root-INFO: Regularization Change: 0.000 -> 0.091
2024-12-01-23:09:45-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-01-23:09:45-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-23:09:45-root-INFO: step: 175 lr_xt 0.00592610
2024-12-01-23:09:45-root-INFO: grad norm: 336.021 333.290 42.754
2024-12-01-23:09:46-root-INFO: Loss too large (1544.878->1787.245)! Learning rate decreased to 0.00474.
2024-12-01-23:09:46-root-INFO: Loss too large (1544.878->1770.443)! Learning rate decreased to 0.00379.
2024-12-01-23:09:46-root-INFO: Loss too large (1544.878->1752.050)! Learning rate decreased to 0.00303.
2024-12-01-23:09:46-root-INFO: Loss too large (1544.878->1731.095)! Learning rate decreased to 0.00243.
2024-12-01-23:09:47-root-INFO: Loss too large (1544.878->1707.417)! Learning rate decreased to 0.00194.
2024-12-01-23:09:47-root-INFO: Loss too large (1544.878->1681.245)! Learning rate decreased to 0.00155.
2024-12-01-23:09:47-root-INFO: Loss too large (1544.878->1653.188)! Learning rate decreased to 0.00124.
2024-12-01-23:09:47-root-INFO: Loss too large (1544.878->1624.333)! Learning rate decreased to 0.00099.
2024-12-01-23:09:47-root-INFO: Loss too large (1544.878->1596.586)! Learning rate decreased to 0.00080.
2024-12-01-23:09:48-root-INFO: Loss too large (1544.878->1572.469)! Learning rate decreased to 0.00064.
2024-12-01-23:09:48-root-INFO: Loss too large (1544.878->1553.960)! Learning rate decreased to 0.00051.
2024-12-01-23:09:48-root-INFO: grad norm: 288.525 285.460 41.943
2024-12-01-23:09:49-root-INFO: grad norm: 239.107 237.000 31.673
2024-12-01-23:09:50-root-INFO: grad norm: 223.053 220.478 33.791
2024-12-01-23:09:50-root-INFO: grad norm: 206.689 204.840 27.585
2024-12-01-23:09:51-root-INFO: grad norm: 198.737 196.370 30.585
2024-12-01-23:09:51-root-INFO: grad norm: 191.037 189.330 25.480
2024-12-01-23:09:52-root-INFO: grad norm: 187.203 184.953 28.936
2024-12-01-23:09:53-root-INFO: Loss Change: 1544.878 -> 1515.769
2024-12-01-23:09:53-root-INFO: Regularization Change: 0.000 -> 0.081
2024-12-01-23:09:53-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-01-23:09:53-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-23:09:53-root-INFO: step: 174 lr_xt 0.00616941
2024-12-01-23:09:53-root-INFO: grad norm: 339.416 336.985 40.546
2024-12-01-23:09:53-root-INFO: Loss too large (1525.353->1769.999)! Learning rate decreased to 0.00494.
2024-12-01-23:09:53-root-INFO: Loss too large (1525.353->1757.132)! Learning rate decreased to 0.00395.
2024-12-01-23:09:54-root-INFO: Loss too large (1525.353->1742.048)! Learning rate decreased to 0.00316.
2024-12-01-23:09:54-root-INFO: Loss too large (1525.353->1723.820)! Learning rate decreased to 0.00253.
2024-12-01-23:09:54-root-INFO: Loss too large (1525.353->1702.239)! Learning rate decreased to 0.00202.
2024-12-01-23:09:54-root-INFO: Loss too large (1525.353->1677.447)! Learning rate decreased to 0.00162.
2024-12-01-23:09:54-root-INFO: Loss too large (1525.353->1649.898)! Learning rate decreased to 0.00129.
2024-12-01-23:09:55-root-INFO: Loss too large (1525.353->1620.463)! Learning rate decreased to 0.00104.
2024-12-01-23:09:55-root-INFO: Loss too large (1525.353->1590.865)! Learning rate decreased to 0.00083.
2024-12-01-23:09:55-root-INFO: Loss too large (1525.353->1563.855)! Learning rate decreased to 0.00066.
2024-12-01-23:09:55-root-INFO: Loss too large (1525.353->1542.117)! Learning rate decreased to 0.00053.
2024-12-01-23:09:55-root-INFO: Loss too large (1525.353->1526.846)! Learning rate decreased to 0.00042.
2024-12-01-23:09:56-root-INFO: grad norm: 238.647 236.018 35.324
2024-12-01-23:09:57-root-INFO: grad norm: 163.749 162.154 22.798
2024-12-01-23:09:57-root-INFO: grad norm: 130.257 128.161 23.275
2024-12-01-23:09:58-root-INFO: grad norm: 104.826 103.298 17.835
2024-12-01-23:09:58-root-INFO: grad norm: 89.617 87.581 18.993
2024-12-01-23:09:59-root-INFO: grad norm: 78.630 76.979 16.028
2024-12-01-23:10:00-root-INFO: grad norm: 71.678 69.631 17.006
2024-12-01-23:10:00-root-INFO: Loss Change: 1525.353 -> 1498.438
2024-12-01-23:10:00-root-INFO: Regularization Change: 0.000 -> 0.052
2024-12-01-23:10:00-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-01-23:10:00-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-23:10:00-root-INFO: step: 173 lr_xt 0.00642166
2024-12-01-23:10:00-root-INFO: grad norm: 332.458 329.547 43.904
2024-12-01-23:10:01-root-INFO: Loss too large (1512.706->1759.932)! Learning rate decreased to 0.00514.
2024-12-01-23:10:01-root-INFO: Loss too large (1512.706->1744.121)! Learning rate decreased to 0.00411.
2024-12-01-23:10:01-root-INFO: Loss too large (1512.706->1727.591)! Learning rate decreased to 0.00329.
2024-12-01-23:10:01-root-INFO: Loss too large (1512.706->1708.856)! Learning rate decreased to 0.00263.
2024-12-01-23:10:01-root-INFO: Loss too large (1512.706->1687.277)! Learning rate decreased to 0.00210.
2024-12-01-23:10:02-root-INFO: Loss too large (1512.706->1662.718)! Learning rate decreased to 0.00168.
2024-12-01-23:10:02-root-INFO: Loss too large (1512.706->1635.473)! Learning rate decreased to 0.00135.
2024-12-01-23:10:02-root-INFO: Loss too large (1512.706->1606.316)! Learning rate decreased to 0.00108.
2024-12-01-23:10:02-root-INFO: Loss too large (1512.706->1576.938)! Learning rate decreased to 0.00086.
2024-12-01-23:10:02-root-INFO: Loss too large (1512.706->1550.101)! Learning rate decreased to 0.00069.
2024-12-01-23:10:03-root-INFO: Loss too large (1512.706->1528.532)! Learning rate decreased to 0.00055.
2024-12-01-23:10:03-root-INFO: Loss too large (1512.706->1513.447)! Learning rate decreased to 0.00044.
2024-12-01-23:10:03-root-INFO: grad norm: 245.300 242.532 36.749
2024-12-01-23:10:04-root-INFO: grad norm: 179.308 177.112 27.977
2024-12-01-23:10:05-root-INFO: grad norm: 146.923 144.625 25.883
2024-12-01-23:10:05-root-INFO: grad norm: 121.106 119.082 22.048
2024-12-01-23:10:06-root-INFO: grad norm: 104.430 102.246 21.244
2024-12-01-23:10:07-root-INFO: grad norm: 91.377 89.376 19.019
2024-12-01-23:10:07-root-INFO: grad norm: 82.346 80.197 18.690
2024-12-01-23:10:08-root-INFO: Loss Change: 1512.706 -> 1481.291
2024-12-01-23:10:08-root-INFO: Regularization Change: 0.000 -> 0.071
2024-12-01-23:10:08-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-01-23:10:08-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-23:10:08-root-INFO: step: 172 lr_xt 0.00668315
2024-12-01-23:10:08-root-INFO: grad norm: 229.854 227.685 31.504
2024-12-01-23:10:08-root-INFO: Loss too large (1485.873->1717.945)! Learning rate decreased to 0.00535.
2024-12-01-23:10:09-root-INFO: Loss too large (1485.873->1702.724)! Learning rate decreased to 0.00428.
2024-12-01-23:10:09-root-INFO: Loss too large (1485.873->1684.670)! Learning rate decreased to 0.00342.
2024-12-01-23:10:09-root-INFO: Loss too large (1485.873->1663.490)! Learning rate decreased to 0.00274.
2024-12-01-23:10:09-root-INFO: Loss too large (1485.873->1639.287)! Learning rate decreased to 0.00219.
2024-12-01-23:10:09-root-INFO: Loss too large (1485.873->1612.526)! Learning rate decreased to 0.00175.
2024-12-01-23:10:10-root-INFO: Loss too large (1485.873->1584.163)! Learning rate decreased to 0.00140.
2024-12-01-23:10:10-root-INFO: Loss too large (1485.873->1556.084)! Learning rate decreased to 0.00112.
2024-12-01-23:10:10-root-INFO: Loss too large (1485.873->1530.934)! Learning rate decreased to 0.00090.
2024-12-01-23:10:10-root-INFO: Loss too large (1485.873->1510.891)! Learning rate decreased to 0.00072.
2024-12-01-23:10:10-root-INFO: Loss too large (1485.873->1496.655)! Learning rate decreased to 0.00057.
2024-12-01-23:10:11-root-INFO: Loss too large (1485.873->1487.571)! Learning rate decreased to 0.00046.
2024-12-01-23:10:11-root-INFO: grad norm: 200.172 197.962 29.666
2024-12-01-23:10:12-root-INFO: grad norm: 174.061 172.196 25.414
2024-12-01-23:10:12-root-INFO: grad norm: 155.921 153.961 24.642
2024-12-01-23:10:13-root-INFO: grad norm: 139.638 137.940 21.708
2024-12-01-23:10:14-root-INFO: grad norm: 127.227 125.410 21.428
2024-12-01-23:10:14-root-INFO: grad norm: 116.049 114.442 19.245
2024-12-01-23:10:15-root-INFO: grad norm: 107.137 105.399 19.219
2024-12-01-23:10:15-root-INFO: Loss Change: 1485.873 -> 1465.551
2024-12-01-23:10:15-root-INFO: Regularization Change: 0.000 -> 0.057
2024-12-01-23:10:15-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-01-23:10:15-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-23:10:15-root-INFO: step: 171 lr_xt 0.00695416
2024-12-01-23:10:16-root-INFO: grad norm: 317.880 315.563 38.313
2024-12-01-23:10:16-root-INFO: Loss too large (1475.324->1728.249)! Learning rate decreased to 0.00556.
2024-12-01-23:10:16-root-INFO: Loss too large (1475.324->1715.623)! Learning rate decreased to 0.00445.
2024-12-01-23:10:16-root-INFO: Loss too large (1475.324->1701.764)! Learning rate decreased to 0.00356.
2024-12-01-23:10:16-root-INFO: Loss too large (1475.324->1685.290)! Learning rate decreased to 0.00285.
2024-12-01-23:10:17-root-INFO: Loss too large (1475.324->1665.465)! Learning rate decreased to 0.00228.
2024-12-01-23:10:17-root-INFO: Loss too large (1475.324->1642.045)! Learning rate decreased to 0.00182.
2024-12-01-23:10:17-root-INFO: Loss too large (1475.324->1615.161)! Learning rate decreased to 0.00146.
2024-12-01-23:10:17-root-INFO: Loss too large (1475.324->1585.365)! Learning rate decreased to 0.00117.
2024-12-01-23:10:17-root-INFO: Loss too large (1475.324->1554.108)! Learning rate decreased to 0.00093.
2024-12-01-23:10:18-root-INFO: Loss too large (1475.324->1524.310)! Learning rate decreased to 0.00075.
2024-12-01-23:10:18-root-INFO: Loss too large (1475.324->1499.443)! Learning rate decreased to 0.00060.
2024-12-01-23:10:18-root-INFO: Loss too large (1475.324->1481.515)! Learning rate decreased to 0.00048.
2024-12-01-23:10:19-root-INFO: grad norm: 276.023 273.239 39.104
2024-12-01-23:10:19-root-INFO: grad norm: 241.834 239.931 30.279
2024-12-01-23:10:20-root-INFO: grad norm: 220.152 217.774 32.270
2024-12-01-23:10:21-root-INFO: grad norm: 200.876 199.193 25.951
2024-12-01-23:10:21-root-INFO: grad norm: 186.444 184.308 28.142
2024-12-01-23:10:22-root-INFO: grad norm: 173.220 171.674 23.090
2024-12-01-23:10:23-root-INFO: grad norm: 162.584 160.614 25.231
2024-12-01-23:10:23-root-INFO: Loss Change: 1475.324 -> 1449.740
2024-12-01-23:10:23-root-INFO: Regularization Change: 0.000 -> 0.062
2024-12-01-23:10:23-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-01-23:10:23-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-23:10:23-root-INFO: step: 170 lr_xt 0.00723499
2024-12-01-23:10:24-root-INFO: grad norm: 354.037 351.379 43.302
2024-12-01-23:10:24-root-INFO: Loss too large (1466.540->1723.003)! Learning rate decreased to 0.00579.
2024-12-01-23:10:24-root-INFO: Loss too large (1466.540->1710.067)! Learning rate decreased to 0.00463.
2024-12-01-23:10:24-root-INFO: Loss too large (1466.540->1697.060)! Learning rate decreased to 0.00370.
2024-12-01-23:10:24-root-INFO: Loss too large (1466.540->1682.277)! Learning rate decreased to 0.00296.
2024-12-01-23:10:25-root-INFO: Loss too large (1466.540->1664.461)! Learning rate decreased to 0.00237.
2024-12-01-23:10:25-root-INFO: Loss too large (1466.540->1642.981)! Learning rate decreased to 0.00190.
2024-12-01-23:10:25-root-INFO: Loss too large (1466.540->1617.663)! Learning rate decreased to 0.00152.
2024-12-01-23:10:25-root-INFO: Loss too large (1466.540->1588.755)! Learning rate decreased to 0.00121.
2024-12-01-23:10:25-root-INFO: Loss too large (1466.540->1557.095)! Learning rate decreased to 0.00097.
2024-12-01-23:10:26-root-INFO: Loss too large (1466.540->1524.923)! Learning rate decreased to 0.00078.
2024-12-01-23:10:26-root-INFO: Loss too large (1466.540->1496.080)! Learning rate decreased to 0.00062.
2024-12-01-23:10:26-root-INFO: Loss too large (1466.540->1474.025)! Learning rate decreased to 0.00050.
2024-12-01-23:10:27-root-INFO: grad norm: 305.119 302.323 41.206
2024-12-01-23:10:27-root-INFO: grad norm: 268.486 266.308 34.129
2024-12-01-23:10:28-root-INFO: grad norm: 246.040 243.639 34.286
2024-12-01-23:10:28-root-INFO: grad norm: 226.662 224.740 29.459
2024-12-01-23:10:29-root-INFO: grad norm: 212.016 209.846 30.251
2024-12-01-23:10:30-root-INFO: grad norm: 198.756 197.001 26.354
2024-12-01-23:10:30-root-INFO: grad norm: 187.863 185.858 27.376
2024-12-01-23:10:31-root-INFO: Loss Change: 1466.540 -> 1435.696
2024-12-01-23:10:31-root-INFO: Regularization Change: 0.000 -> 0.075
2024-12-01-23:10:31-root-INFO: Undo step: 170
2024-12-01-23:10:31-root-INFO: Undo step: 171
2024-12-01-23:10:31-root-INFO: Undo step: 172
2024-12-01-23:10:31-root-INFO: Undo step: 173
2024-12-01-23:10:31-root-INFO: Undo step: 174
2024-12-01-23:10:31-root-INFO: step: 175 lr_xt 0.00592610
2024-12-01-23:10:31-root-INFO: grad norm: 631.743 613.114 152.285
2024-12-01-23:10:32-root-INFO: grad norm: 870.572 857.400 150.865
2024-12-01-23:10:32-root-INFO: grad norm: 553.925 539.367 126.161
2024-12-01-23:10:32-root-INFO: Loss too large (2490.108->2655.029)! Learning rate decreased to 0.00474.
2024-12-01-23:10:33-root-INFO: grad norm: 523.858 517.081 83.991
2024-12-01-23:10:34-root-INFO: grad norm: 507.523 499.590 89.382
2024-12-01-23:10:34-root-INFO: Loss too large (2114.685->2165.733)! Learning rate decreased to 0.00379.
2024-12-01-23:10:34-root-INFO: grad norm: 365.680 360.880 59.050
2024-12-01-23:10:35-root-INFO: grad norm: 215.731 212.124 39.282
2024-12-01-23:10:36-root-INFO: grad norm: 167.658 164.824 30.698
2024-12-01-23:10:36-root-INFO: Loss Change: 2588.005 -> 1759.406
2024-12-01-23:10:36-root-INFO: Regularization Change: 0.000 -> 24.605
2024-12-01-23:10:36-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-01-23:10:36-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-23:10:36-root-INFO: step: 174 lr_xt 0.00616941
2024-12-01-23:10:37-root-INFO: grad norm: 161.025 158.308 29.452
2024-12-01-23:10:37-root-INFO: Loss too large (1763.039->1770.298)! Learning rate decreased to 0.00494.
2024-12-01-23:10:37-root-INFO: grad norm: 200.368 197.083 36.133
2024-12-01-23:10:38-root-INFO: grad norm: 257.101 254.222 38.369
2024-12-01-23:10:39-root-INFO: grad norm: 289.701 285.406 49.699
2024-12-01-23:10:39-root-INFO: grad norm: 324.257 319.593 54.799
2024-12-01-23:10:40-root-INFO: grad norm: 286.828 282.427 50.051
2024-12-01-23:10:41-root-INFO: grad norm: 224.229 220.982 38.019
2024-12-01-23:10:41-root-INFO: grad norm: 190.984 187.560 35.999
2024-12-01-23:10:42-root-INFO: Loss Change: 1763.039 -> 1532.387
2024-12-01-23:10:42-root-INFO: Regularization Change: 0.000 -> 9.439
2024-12-01-23:10:42-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-01-23:10:42-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-23:10:42-root-INFO: step: 173 lr_xt 0.00642166
2024-12-01-23:10:42-root-INFO: grad norm: 305.894 301.079 54.062
2024-12-01-23:10:42-root-INFO: Loss too large (1565.862->1712.099)! Learning rate decreased to 0.00514.
2024-12-01-23:10:42-root-INFO: Loss too large (1565.862->1635.339)! Learning rate decreased to 0.00411.
2024-12-01-23:10:43-root-INFO: Loss too large (1565.862->1572.930)! Learning rate decreased to 0.00329.
2024-12-01-23:10:43-root-INFO: grad norm: 230.670 225.032 50.687
2024-12-01-23:10:44-root-INFO: grad norm: 200.232 195.078 45.138
2024-12-01-23:10:44-root-INFO: Loss too large (1477.468->1479.088)! Learning rate decreased to 0.00263.
2024-12-01-23:10:45-root-INFO: grad norm: 160.939 156.468 37.672
2024-12-01-23:10:46-root-INFO: grad norm: 134.103 130.659 30.198
2024-12-01-23:10:46-root-INFO: grad norm: 131.423 127.933 30.084
2024-12-01-23:10:47-root-INFO: grad norm: 140.518 137.158 30.543
2024-12-01-23:10:47-root-INFO: grad norm: 160.938 157.145 34.733
2024-12-01-23:10:48-root-INFO: Loss Change: 1565.862 -> 1405.935
2024-12-01-23:10:48-root-INFO: Regularization Change: 0.000 -> 2.405
2024-12-01-23:10:48-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-01-23:10:48-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-23:10:48-root-INFO: step: 172 lr_xt 0.00668315
2024-12-01-23:10:48-root-INFO: grad norm: 265.455 261.782 44.010
2024-12-01-23:10:48-root-INFO: Loss too large (1410.298->1633.988)! Learning rate decreased to 0.00535.
2024-12-01-23:10:49-root-INFO: Loss too large (1410.298->1583.692)! Learning rate decreased to 0.00428.
2024-12-01-23:10:49-root-INFO: Loss too large (1410.298->1533.576)! Learning rate decreased to 0.00342.
2024-12-01-23:10:49-root-INFO: Loss too large (1410.298->1485.967)! Learning rate decreased to 0.00274.
2024-12-01-23:10:49-root-INFO: Loss too large (1410.298->1444.528)! Learning rate decreased to 0.00219.
2024-12-01-23:10:49-root-INFO: Loss too large (1410.298->1412.441)! Learning rate decreased to 0.00175.
2024-12-01-23:10:50-root-INFO: grad norm: 185.110 181.259 37.560
2024-12-01-23:10:51-root-INFO: grad norm: 135.062 132.549 25.929
2024-12-01-23:10:51-root-INFO: grad norm: 135.473 132.548 27.998
2024-12-01-23:10:52-root-INFO: grad norm: 158.175 156.089 25.604
2024-12-01-23:10:52-root-INFO: Loss too large (1359.168->1360.776)! Learning rate decreased to 0.00140.
2024-12-01-23:10:53-root-INFO: grad norm: 138.579 135.740 27.909
2024-12-01-23:10:53-root-INFO: grad norm: 128.495 126.631 21.810
2024-12-01-23:10:54-root-INFO: grad norm: 130.889 128.201 26.391
2024-12-01-23:10:54-root-INFO: Loss Change: 1410.298 -> 1340.586
2024-12-01-23:10:54-root-INFO: Regularization Change: 0.000 -> 0.542
2024-12-01-23:10:54-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-01-23:10:54-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-23:10:55-root-INFO: step: 171 lr_xt 0.00695416
2024-12-01-23:10:55-root-INFO: grad norm: 234.919 232.796 31.508
2024-12-01-23:10:55-root-INFO: Loss too large (1348.369->1564.376)! Learning rate decreased to 0.00556.
2024-12-01-23:10:55-root-INFO: Loss too large (1348.369->1533.589)! Learning rate decreased to 0.00445.
2024-12-01-23:10:56-root-INFO: Loss too large (1348.369->1499.824)! Learning rate decreased to 0.00356.
2024-12-01-23:10:56-root-INFO: Loss too large (1348.369->1464.216)! Learning rate decreased to 0.00285.
2024-12-01-23:10:56-root-INFO: Loss too large (1348.369->1429.008)! Learning rate decreased to 0.00228.
2024-12-01-23:10:56-root-INFO: Loss too large (1348.369->1396.858)! Learning rate decreased to 0.00182.
2024-12-01-23:10:56-root-INFO: Loss too large (1348.369->1370.293)! Learning rate decreased to 0.00146.
2024-12-01-23:10:57-root-INFO: Loss too large (1348.369->1350.864)! Learning rate decreased to 0.00117.
2024-12-01-23:10:57-root-INFO: grad norm: 176.575 173.074 34.987
2024-12-01-23:10:58-root-INFO: grad norm: 135.730 134.083 21.085
2024-12-01-23:10:58-root-INFO: grad norm: 122.664 119.939 25.714
2024-12-01-23:10:59-root-INFO: grad norm: 114.961 113.442 18.626
2024-12-01-23:11:00-root-INFO: grad norm: 113.395 110.865 23.820
2024-12-01-23:11:00-root-INFO: grad norm: 115.293 113.814 18.404
2024-12-01-23:11:01-root-INFO: grad norm: 120.146 117.678 24.227
2024-12-01-23:11:01-root-INFO: Loss Change: 1348.369 -> 1311.608
2024-12-01-23:11:01-root-INFO: Regularization Change: 0.000 -> 0.220
2024-12-01-23:11:01-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-01-23:11:01-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-23:11:02-root-INFO: step: 170 lr_xt 0.00723499
2024-12-01-23:11:02-root-INFO: grad norm: 217.185 214.957 31.034
2024-12-01-23:11:02-root-INFO: Loss too large (1318.522->1539.070)! Learning rate decreased to 0.00579.
2024-12-01-23:11:02-root-INFO: Loss too large (1318.522->1510.245)! Learning rate decreased to 0.00463.
2024-12-01-23:11:02-root-INFO: Loss too large (1318.522->1478.780)! Learning rate decreased to 0.00370.
2024-12-01-23:11:03-root-INFO: Loss too large (1318.522->1445.316)! Learning rate decreased to 0.00296.
2024-12-01-23:11:03-root-INFO: Loss too large (1318.522->1411.500)! Learning rate decreased to 0.00237.
2024-12-01-23:11:03-root-INFO: Loss too large (1318.522->1379.593)! Learning rate decreased to 0.00190.
2024-12-01-23:11:03-root-INFO: Loss too large (1318.522->1352.154)! Learning rate decreased to 0.00152.
2024-12-01-23:11:03-root-INFO: Loss too large (1318.522->1331.144)! Learning rate decreased to 0.00121.
2024-12-01-23:11:04-root-INFO: grad norm: 218.150 214.677 38.771
2024-12-01-23:11:05-root-INFO: grad norm: 220.486 218.440 29.969
2024-12-01-23:11:05-root-INFO: Loss too large (1311.319->1311.463)! Learning rate decreased to 0.00097.
2024-12-01-23:11:06-root-INFO: grad norm: 157.459 154.815 28.734
2024-12-01-23:11:06-root-INFO: grad norm: 117.577 116.068 18.774
2024-12-01-23:11:07-root-INFO: grad norm: 97.504 95.337 20.441
2024-12-01-23:11:07-root-INFO: grad norm: 83.274 81.851 15.331
2024-12-01-23:11:08-root-INFO: grad norm: 74.238 72.196 17.291
2024-12-01-23:11:08-root-INFO: Loss Change: 1318.522 -> 1285.503
2024-12-01-23:11:08-root-INFO: Regularization Change: 0.000 -> 0.177
2024-12-01-23:11:08-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-01-23:11:08-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-23:11:09-root-INFO: step: 169 lr_xt 0.00752595
2024-12-01-23:11:09-root-INFO: grad norm: 164.232 162.389 24.531
2024-12-01-23:11:09-root-INFO: Loss too large (1289.164->1493.611)! Learning rate decreased to 0.00602.
2024-12-01-23:11:09-root-INFO: Loss too large (1289.164->1464.086)! Learning rate decreased to 0.00482.
2024-12-01-23:11:09-root-INFO: Loss too large (1289.164->1432.080)! Learning rate decreased to 0.00385.
2024-12-01-23:11:10-root-INFO: Loss too large (1289.164->1399.058)! Learning rate decreased to 0.00308.
2024-12-01-23:11:10-root-INFO: Loss too large (1289.164->1367.155)! Learning rate decreased to 0.00247.
2024-12-01-23:11:10-root-INFO: Loss too large (1289.164->1338.919)! Learning rate decreased to 0.00197.
2024-12-01-23:11:10-root-INFO: Loss too large (1289.164->1316.447)! Learning rate decreased to 0.00158.
2024-12-01-23:11:11-root-INFO: Loss too large (1289.164->1300.477)! Learning rate decreased to 0.00126.
2024-12-01-23:11:11-root-INFO: Loss too large (1289.164->1290.332)! Learning rate decreased to 0.00101.
2024-12-01-23:11:11-root-INFO: grad norm: 144.476 141.984 26.719
2024-12-01-23:11:12-root-INFO: grad norm: 129.752 128.188 20.084
2024-12-01-23:11:13-root-INFO: grad norm: 119.885 117.744 22.557
2024-12-01-23:11:13-root-INFO: grad norm: 112.220 110.782 17.906
2024-12-01-23:11:14-root-INFO: grad norm: 106.746 104.792 20.328
2024-12-01-23:11:15-root-INFO: grad norm: 102.502 101.126 16.740
2024-12-01-23:11:15-root-INFO: grad norm: 99.519 97.676 19.061
2024-12-01-23:11:16-root-INFO: Loss Change: 1289.164 -> 1266.408
2024-12-01-23:11:16-root-INFO: Regularization Change: 0.000 -> 0.146
2024-12-01-23:11:16-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-01-23:11:16-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-23:11:16-root-INFO: step: 168 lr_xt 0.00782735
2024-12-01-23:11:16-root-INFO: grad norm: 208.463 206.721 26.894
2024-12-01-23:11:16-root-INFO: Loss too large (1275.581->1506.442)! Learning rate decreased to 0.00626.
2024-12-01-23:11:17-root-INFO: Loss too large (1275.581->1484.611)! Learning rate decreased to 0.00501.
2024-12-01-23:11:17-root-INFO: Loss too large (1275.581->1458.453)! Learning rate decreased to 0.00401.
2024-12-01-23:11:17-root-INFO: Loss too large (1275.581->1428.216)! Learning rate decreased to 0.00321.
2024-12-01-23:11:17-root-INFO: Loss too large (1275.581->1395.109)! Learning rate decreased to 0.00256.
2024-12-01-23:11:17-root-INFO: Loss too large (1275.581->1361.138)! Learning rate decreased to 0.00205.
2024-12-01-23:11:18-root-INFO: Loss too large (1275.581->1329.325)! Learning rate decreased to 0.00164.
2024-12-01-23:11:18-root-INFO: Loss too large (1275.581->1302.974)! Learning rate decreased to 0.00131.
2024-12-01-23:11:18-root-INFO: Loss too large (1275.581->1284.003)! Learning rate decreased to 0.00105.
2024-12-01-23:11:19-root-INFO: grad norm: 200.861 198.091 33.244
2024-12-01-23:11:19-root-INFO: grad norm: 193.307 191.617 25.506
2024-12-01-23:11:20-root-INFO: grad norm: 186.197 183.676 30.537
2024-12-01-23:11:21-root-INFO: grad norm: 179.784 178.146 24.214
2024-12-01-23:11:21-root-INFO: grad norm: 174.029 171.675 28.525
2024-12-01-23:11:22-root-INFO: grad norm: 169.023 167.428 23.164
2024-12-01-23:11:23-root-INFO: grad norm: 164.624 162.388 27.042
2024-12-01-23:11:23-root-INFO: Loss Change: 1275.581 -> 1254.710
2024-12-01-23:11:23-root-INFO: Regularization Change: 0.000 -> 0.137
2024-12-01-23:11:23-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-01-23:11:23-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-23:11:23-root-INFO: step: 167 lr_xt 0.00813950
2024-12-01-23:11:23-root-INFO: grad norm: 245.500 243.184 33.644
2024-12-01-23:11:24-root-INFO: Loss too large (1266.543->1514.590)! Learning rate decreased to 0.00651.
2024-12-01-23:11:24-root-INFO: Loss too large (1266.543->1491.623)! Learning rate decreased to 0.00521.
2024-12-01-23:11:24-root-INFO: Loss too large (1266.543->1465.880)! Learning rate decreased to 0.00417.
2024-12-01-23:11:24-root-INFO: Loss too large (1266.543->1436.082)! Learning rate decreased to 0.00333.
2024-12-01-23:11:24-root-INFO: Loss too large (1266.543->1402.425)! Learning rate decreased to 0.00267.
2024-12-01-23:11:25-root-INFO: Loss too large (1266.543->1366.255)! Learning rate decreased to 0.00213.
2024-12-01-23:11:25-root-INFO: Loss too large (1266.543->1330.208)! Learning rate decreased to 0.00171.
2024-12-01-23:11:25-root-INFO: Loss too large (1266.543->1298.229)! Learning rate decreased to 0.00137.
2024-12-01-23:11:25-root-INFO: Loss too large (1266.543->1273.900)! Learning rate decreased to 0.00109.
2024-12-01-23:11:26-root-INFO: grad norm: 220.139 217.215 35.765
2024-12-01-23:11:27-root-INFO: grad norm: 203.733 201.785 28.100
2024-12-01-23:11:27-root-INFO: grad norm: 191.284 188.784 30.827
2024-12-01-23:11:28-root-INFO: grad norm: 181.739 179.968 25.316
2024-12-01-23:11:29-root-INFO: grad norm: 173.419 171.164 27.880
2024-12-01-23:11:30-root-INFO: grad norm: 166.697 165.028 23.530
2024-12-01-23:11:30-root-INFO: grad norm: 160.609 158.513 25.862
2024-12-01-23:11:31-root-INFO: Loss Change: 1266.543 -> 1235.549
2024-12-01-23:11:31-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-01-23:11:31-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-01-23:11:31-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-23:11:31-root-INFO: step: 166 lr_xt 0.00846273
2024-12-01-23:11:31-root-INFO: grad norm: 234.481 232.362 31.453
2024-12-01-23:11:31-root-INFO: Loss too large (1246.038->1490.315)! Learning rate decreased to 0.00677.
2024-12-01-23:11:32-root-INFO: Loss too large (1246.038->1468.330)! Learning rate decreased to 0.00542.
2024-12-01-23:11:32-root-INFO: Loss too large (1246.038->1442.622)! Learning rate decreased to 0.00433.
2024-12-01-23:11:32-root-INFO: Loss too large (1246.038->1412.309)! Learning rate decreased to 0.00347.
2024-12-01-23:11:32-root-INFO: Loss too large (1246.038->1377.999)! Learning rate decreased to 0.00277.
2024-12-01-23:11:32-root-INFO: Loss too large (1246.038->1341.329)! Learning rate decreased to 0.00222.
2024-12-01-23:11:33-root-INFO: Loss too large (1246.038->1305.184)! Learning rate decreased to 0.00177.
2024-12-01-23:11:33-root-INFO: Loss too large (1246.038->1273.725)! Learning rate decreased to 0.00142.
2024-12-01-23:11:33-root-INFO: Loss too large (1246.038->1250.453)! Learning rate decreased to 0.00114.
2024-12-01-23:11:34-root-INFO: grad norm: 199.556 196.972 32.011
2024-12-01-23:11:34-root-INFO: grad norm: 180.360 178.599 25.147
2024-12-01-23:11:35-root-INFO: grad norm: 164.963 162.808 26.579
2024-12-01-23:11:36-root-INFO: grad norm: 154.467 152.873 22.129
2024-12-01-23:11:36-root-INFO: grad norm: 145.209 143.277 23.606
2024-12-01-23:11:37-root-INFO: grad norm: 138.428 136.928 20.324
2024-12-01-23:11:38-root-INFO: grad norm: 132.288 130.491 21.730
2024-12-01-23:11:38-root-INFO: Loss Change: 1246.038 -> 1213.713
2024-12-01-23:11:38-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-01-23:11:38-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-01-23:11:38-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-23:11:38-root-INFO: step: 165 lr_xt 0.00879737
2024-12-01-23:11:39-root-INFO: grad norm: 195.952 194.276 25.571
2024-12-01-23:11:39-root-INFO: Loss too large (1220.832->1455.515)! Learning rate decreased to 0.00704.
2024-12-01-23:11:39-root-INFO: Loss too large (1220.832->1434.029)! Learning rate decreased to 0.00563.
2024-12-01-23:11:39-root-INFO: Loss too large (1220.832->1407.867)! Learning rate decreased to 0.00450.
2024-12-01-23:11:39-root-INFO: Loss too large (1220.832->1377.220)! Learning rate decreased to 0.00360.
2024-12-01-23:11:40-root-INFO: Loss too large (1220.832->1343.252)! Learning rate decreased to 0.00288.
2024-12-01-23:11:40-root-INFO: Loss too large (1220.832->1307.908)! Learning rate decreased to 0.00231.
2024-12-01-23:11:40-root-INFO: Loss too large (1220.832->1274.471)! Learning rate decreased to 0.00184.
2024-12-01-23:11:40-root-INFO: Loss too large (1220.832->1246.867)! Learning rate decreased to 0.00148.
2024-12-01-23:11:40-root-INFO: Loss too large (1220.832->1227.404)! Learning rate decreased to 0.00118.
2024-12-01-23:11:41-root-INFO: grad norm: 188.249 185.744 30.609
2024-12-01-23:11:42-root-INFO: grad norm: 181.194 179.582 24.116
2024-12-01-23:11:43-root-INFO: grad norm: 172.632 170.429 27.494
2024-12-01-23:11:43-root-INFO: grad norm: 165.966 164.413 22.657
2024-12-01-23:11:44-root-INFO: grad norm: 158.625 156.619 25.149
2024-12-01-23:11:44-root-INFO: grad norm: 153.098 151.595 21.397
2024-12-01-23:11:45-root-INFO: grad norm: 147.302 145.430 23.409
2024-12-01-23:11:45-root-INFO: Loss Change: 1220.832 -> 1197.426
2024-12-01-23:11:45-root-INFO: Regularization Change: 0.000 -> 0.168
2024-12-01-23:11:45-root-INFO: Undo step: 165
2024-12-01-23:11:45-root-INFO: Undo step: 166
2024-12-01-23:11:45-root-INFO: Undo step: 167
2024-12-01-23:11:45-root-INFO: Undo step: 168
2024-12-01-23:11:45-root-INFO: Undo step: 169
2024-12-01-23:11:46-root-INFO: step: 170 lr_xt 0.00723499
2024-12-01-23:11:46-root-INFO: grad norm: 428.156 418.052 92.470
2024-12-01-23:11:46-root-INFO: grad norm: 422.686 415.537 77.413
2024-12-01-23:11:47-root-INFO: Loss too large (1814.817->2283.017)! Learning rate decreased to 0.00579.
2024-12-01-23:11:47-root-INFO: Loss too large (1814.817->1939.513)! Learning rate decreased to 0.00463.
2024-12-01-23:11:47-root-INFO: grad norm: 611.537 601.818 108.592
2024-12-01-23:11:48-root-INFO: Loss too large (1725.699->1806.121)! Learning rate decreased to 0.00370.
2024-12-01-23:11:48-root-INFO: grad norm: 300.517 293.966 62.404
2024-12-01-23:11:49-root-INFO: grad norm: 285.526 280.693 52.313
2024-12-01-23:11:50-root-INFO: grad norm: 240.424 233.934 55.484
2024-12-01-23:11:50-root-INFO: grad norm: 167.438 163.880 34.334
2024-12-01-23:11:50-root-INFO: Loss too large (1392.564->1392.932)! Learning rate decreased to 0.00296.
2024-12-01-23:11:51-root-INFO: grad norm: 200.670 196.105 42.560
2024-12-01-23:11:51-root-INFO: Loss too large (1377.406->1395.941)! Learning rate decreased to 0.00237.
2024-12-01-23:11:52-root-INFO: Loss Change: 2147.419 -> 1368.271
2024-12-01-23:11:52-root-INFO: Regularization Change: 0.000 -> 14.880
2024-12-01-23:11:52-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-01-23:11:52-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-23:11:52-root-INFO: step: 169 lr_xt 0.00752595
2024-12-01-23:11:52-root-INFO: grad norm: 282.273 278.537 45.771
2024-12-01-23:11:52-root-INFO: Loss too large (1377.224->1584.071)! Learning rate decreased to 0.00602.
2024-12-01-23:11:52-root-INFO: Loss too large (1377.224->1551.682)! Learning rate decreased to 0.00482.
2024-12-01-23:11:53-root-INFO: Loss too large (1377.224->1513.173)! Learning rate decreased to 0.00385.
2024-12-01-23:11:53-root-INFO: Loss too large (1377.224->1470.192)! Learning rate decreased to 0.00308.
2024-12-01-23:11:53-root-INFO: Loss too large (1377.224->1426.704)! Learning rate decreased to 0.00247.
2024-12-01-23:11:53-root-INFO: Loss too large (1377.224->1387.833)! Learning rate decreased to 0.00197.
2024-12-01-23:11:54-root-INFO: grad norm: 220.332 215.728 44.810
2024-12-01-23:11:54-root-INFO: grad norm: 180.988 177.599 34.863
2024-12-01-23:11:55-root-INFO: grad norm: 183.077 179.539 35.815
2024-12-01-23:11:56-root-INFO: grad norm: 194.937 191.671 35.537
2024-12-01-23:11:56-root-INFO: grad norm: 207.743 203.786 40.350
2024-12-01-23:11:57-root-INFO: grad norm: 223.585 220.182 38.861
2024-12-01-23:11:57-root-INFO: Loss too large (1316.472->1318.233)! Learning rate decreased to 0.00158.
2024-12-01-23:11:58-root-INFO: grad norm: 170.367 167.219 32.597
2024-12-01-23:11:58-root-INFO: Loss Change: 1377.224 -> 1292.829
2024-12-01-23:11:58-root-INFO: Regularization Change: 0.000 -> 0.747
2024-12-01-23:11:58-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-01-23:11:58-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-23:11:58-root-INFO: step: 168 lr_xt 0.00782735
2024-12-01-23:11:59-root-INFO: grad norm: 209.627 206.640 35.261
2024-12-01-23:11:59-root-INFO: Loss too large (1300.027->1515.223)! Learning rate decreased to 0.00626.
2024-12-01-23:11:59-root-INFO: Loss too large (1300.027->1484.319)! Learning rate decreased to 0.00501.
2024-12-01-23:11:59-root-INFO: Loss too large (1300.027->1447.847)! Learning rate decreased to 0.00401.
2024-12-01-23:11:59-root-INFO: Loss too large (1300.027->1408.061)! Learning rate decreased to 0.00321.
2024-12-01-23:12:00-root-INFO: Loss too large (1300.027->1368.569)! Learning rate decreased to 0.00256.
2024-12-01-23:12:00-root-INFO: Loss too large (1300.027->1333.931)! Learning rate decreased to 0.00205.
2024-12-01-23:12:00-root-INFO: Loss too large (1300.027->1307.805)! Learning rate decreased to 0.00164.
2024-12-01-23:12:01-root-INFO: grad norm: 185.782 182.457 34.994
2024-12-01-23:12:01-root-INFO: grad norm: 172.816 170.028 30.920
2024-12-01-23:12:02-root-INFO: grad norm: 168.039 165.088 31.352
2024-12-01-23:12:03-root-INFO: grad norm: 167.238 164.504 30.120
2024-12-01-23:12:03-root-INFO: grad norm: 169.868 166.917 31.526
2024-12-01-23:12:04-root-INFO: grad norm: 174.235 171.475 30.886
2024-12-01-23:12:05-root-INFO: grad norm: 180.037 176.915 33.382
2024-12-01-23:12:05-root-INFO: Loss Change: 1300.027 -> 1264.209
2024-12-01-23:12:05-root-INFO: Regularization Change: 0.000 -> 0.393
2024-12-01-23:12:05-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-01-23:12:05-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-23:12:05-root-INFO: step: 167 lr_xt 0.00813950
2024-12-01-23:12:05-root-INFO: grad norm: 239.073 235.856 39.090
2024-12-01-23:12:06-root-INFO: Loss too large (1272.797->1507.546)! Learning rate decreased to 0.00651.
2024-12-01-23:12:06-root-INFO: Loss too large (1272.797->1479.539)! Learning rate decreased to 0.00521.
2024-12-01-23:12:06-root-INFO: Loss too large (1272.797->1445.754)! Learning rate decreased to 0.00417.
2024-12-01-23:12:06-root-INFO: Loss too large (1272.797->1406.650)! Learning rate decreased to 0.00333.
2024-12-01-23:12:06-root-INFO: Loss too large (1272.797->1364.521)! Learning rate decreased to 0.00267.
2024-12-01-23:12:07-root-INFO: Loss too large (1272.797->1323.592)! Learning rate decreased to 0.00213.
2024-12-01-23:12:07-root-INFO: Loss too large (1272.797->1289.256)! Learning rate decreased to 0.00171.
2024-12-01-23:12:07-root-INFO: grad norm: 229.493 225.379 43.259
2024-12-01-23:12:08-root-INFO: grad norm: 221.343 218.377 36.114
2024-12-01-23:12:09-root-INFO: grad norm: 213.901 210.130 39.990
2024-12-01-23:12:09-root-INFO: grad norm: 207.522 204.609 34.643
2024-12-01-23:12:10-root-INFO: grad norm: 202.411 198.840 37.853
2024-12-01-23:12:11-root-INFO: grad norm: 198.480 195.594 33.721
2024-12-01-23:12:11-root-INFO: grad norm: 195.688 192.209 36.739
2024-12-01-23:12:12-root-INFO: Loss Change: 1272.797 -> 1235.033
2024-12-01-23:12:12-root-INFO: Regularization Change: 0.000 -> 0.395
2024-12-01-23:12:12-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-01-23:12:12-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-23:12:12-root-INFO: step: 166 lr_xt 0.00846273
2024-12-01-23:12:12-root-INFO: grad norm: 236.194 233.232 37.285
2024-12-01-23:12:12-root-INFO: Loss too large (1241.756->1475.922)! Learning rate decreased to 0.00677.
2024-12-01-23:12:13-root-INFO: Loss too large (1241.756->1447.770)! Learning rate decreased to 0.00542.
2024-12-01-23:12:13-root-INFO: Loss too large (1241.756->1413.136)! Learning rate decreased to 0.00433.
2024-12-01-23:12:13-root-INFO: Loss too large (1241.756->1372.711)! Learning rate decreased to 0.00347.
2024-12-01-23:12:13-root-INFO: Loss too large (1241.756->1329.158)! Learning rate decreased to 0.00277.
2024-12-01-23:12:13-root-INFO: Loss too large (1241.756->1287.170)! Learning rate decreased to 0.00222.
2024-12-01-23:12:14-root-INFO: Loss too large (1241.756->1252.573)! Learning rate decreased to 0.00177.
2024-12-01-23:12:14-root-INFO: grad norm: 210.231 206.410 39.902
2024-12-01-23:12:15-root-INFO: grad norm: 197.127 194.405 32.648
2024-12-01-23:12:15-root-INFO: grad norm: 188.016 184.651 35.412
2024-12-01-23:12:16-root-INFO: grad norm: 182.530 179.864 31.084
2024-12-01-23:12:17-root-INFO: grad norm: 178.852 175.646 33.709
2024-12-01-23:12:17-root-INFO: grad norm: 176.848 174.189 30.551
2024-12-01-23:12:18-root-INFO: grad norm: 176.003 172.824 33.300
2024-12-01-23:12:18-root-INFO: Loss Change: 1241.756 -> 1201.578
2024-12-01-23:12:18-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-01-23:12:18-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-01-23:12:18-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-23:12:19-root-INFO: step: 165 lr_xt 0.00879737
2024-12-01-23:12:19-root-INFO: grad norm: 216.901 214.186 34.209
2024-12-01-23:12:19-root-INFO: Loss too large (1209.839->1442.783)! Learning rate decreased to 0.00704.
2024-12-01-23:12:19-root-INFO: Loss too large (1209.839->1415.591)! Learning rate decreased to 0.00563.
2024-12-01-23:12:19-root-INFO: Loss too large (1209.839->1381.451)! Learning rate decreased to 0.00450.
2024-12-01-23:12:20-root-INFO: Loss too large (1209.839->1341.514)! Learning rate decreased to 0.00360.
2024-12-01-23:12:20-root-INFO: Loss too large (1209.839->1298.642)! Learning rate decreased to 0.00288.
2024-12-01-23:12:20-root-INFO: Loss too large (1209.839->1257.630)! Learning rate decreased to 0.00231.
2024-12-01-23:12:20-root-INFO: Loss too large (1209.839->1224.192)! Learning rate decreased to 0.00184.
2024-12-01-23:12:21-root-INFO: grad norm: 209.569 205.627 40.457
2024-12-01-23:12:22-root-INFO: grad norm: 202.498 199.793 32.984
2024-12-01-23:12:22-root-INFO: grad norm: 193.926 190.283 37.411
2024-12-01-23:12:23-root-INFO: grad norm: 188.097 185.409 31.684
2024-12-01-23:12:23-root-INFO: grad norm: 182.370 178.917 35.317
2024-12-01-23:12:24-root-INFO: grad norm: 178.768 176.088 30.838
2024-12-01-23:12:25-root-INFO: grad norm: 175.672 172.310 34.203
2024-12-01-23:12:25-root-INFO: Loss Change: 1209.839 -> 1177.067
2024-12-01-23:12:25-root-INFO: Regularization Change: 0.000 -> 0.356
2024-12-01-23:12:25-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-01-23:12:25-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-23:12:25-root-INFO: step: 164 lr_xt 0.00914377
2024-12-01-23:12:26-root-INFO: grad norm: 214.732 211.804 35.337
2024-12-01-23:12:26-root-INFO: Loss too large (1185.184->1420.532)! Learning rate decreased to 0.00732.
2024-12-01-23:12:26-root-INFO: Loss too large (1185.184->1391.499)! Learning rate decreased to 0.00585.
2024-12-01-23:12:26-root-INFO: Loss too large (1185.184->1355.332)! Learning rate decreased to 0.00468.
2024-12-01-23:12:26-root-INFO: Loss too large (1185.184->1313.241)! Learning rate decreased to 0.00375.
2024-12-01-23:12:27-root-INFO: Loss too large (1185.184->1268.446)! Learning rate decreased to 0.00300.
2024-12-01-23:12:27-root-INFO: Loss too large (1185.184->1226.359)! Learning rate decreased to 0.00240.
2024-12-01-23:12:27-root-INFO: Loss too large (1185.184->1193.080)! Learning rate decreased to 0.00192.
2024-12-01-23:12:28-root-INFO: grad norm: 191.159 187.294 38.243
2024-12-01-23:12:28-root-INFO: grad norm: 180.611 177.884 31.271
2024-12-01-23:12:29-root-INFO: grad norm: 171.088 167.697 33.893
2024-12-01-23:12:30-root-INFO: grad norm: 165.907 163.240 29.625
2024-12-01-23:12:30-root-INFO: grad norm: 161.435 158.235 31.982
2024-12-01-23:12:31-root-INFO: grad norm: 159.086 156.435 28.920
2024-12-01-23:12:31-root-INFO: grad norm: 157.454 154.311 31.304
2024-12-01-23:12:32-root-INFO: Loss Change: 1185.184 -> 1147.356
2024-12-01-23:12:32-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-01-23:12:32-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-01-23:12:32-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-23:12:32-root-INFO: step: 163 lr_xt 0.00950228
2024-12-01-23:12:32-root-INFO: grad norm: 187.331 184.631 31.691
2024-12-01-23:12:32-root-INFO: Loss too large (1151.561->1384.100)! Learning rate decreased to 0.00760.
2024-12-01-23:12:32-root-INFO: Loss too large (1151.561->1353.008)! Learning rate decreased to 0.00608.
2024-12-01-23:12:32-root-INFO: Loss too large (1151.561->1314.851)! Learning rate decreased to 0.00487.
2024-12-01-23:12:32-root-INFO: Loss too large (1151.561->1271.612)! Learning rate decreased to 0.00389.
2024-12-01-23:12:33-root-INFO: Loss too large (1151.561->1227.291)! Learning rate decreased to 0.00311.
2024-12-01-23:12:33-root-INFO: Loss too large (1151.561->1187.890)! Learning rate decreased to 0.00249.
2024-12-01-23:12:33-root-INFO: Loss too large (1151.561->1158.663)! Learning rate decreased to 0.00199.
2024-12-01-23:12:33-root-INFO: grad norm: 170.822 167.352 34.255
2024-12-01-23:12:34-root-INFO: grad norm: 163.649 161.083 28.868
2024-12-01-23:12:34-root-INFO: grad norm: 156.264 153.081 31.377
2024-12-01-23:12:35-root-INFO: grad norm: 152.575 150.041 27.689
2024-12-01-23:12:35-root-INFO: grad norm: 149.109 146.046 30.070
2024-12-01-23:12:36-root-INFO: grad norm: 147.507 144.974 27.220
2024-12-01-23:12:36-root-INFO: grad norm: 146.416 143.377 29.680
2024-12-01-23:12:36-root-INFO: Loss Change: 1151.561 -> 1120.154
2024-12-01-23:12:36-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-01-23:12:36-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-01-23:12:36-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-23:12:37-root-INFO: step: 162 lr_xt 0.00987325
2024-12-01-23:12:37-root-INFO: grad norm: 192.268 189.014 35.225
2024-12-01-23:12:37-root-INFO: Loss too large (1132.964->1371.752)! Learning rate decreased to 0.00790.
2024-12-01-23:12:37-root-INFO: Loss too large (1132.964->1340.278)! Learning rate decreased to 0.00632.
2024-12-01-23:12:37-root-INFO: Loss too large (1132.964->1301.147)! Learning rate decreased to 0.00506.
2024-12-01-23:12:37-root-INFO: Loss too large (1132.964->1255.802)! Learning rate decreased to 0.00404.
2024-12-01-23:12:38-root-INFO: Loss too large (1132.964->1208.505)! Learning rate decreased to 0.00324.
2024-12-01-23:12:38-root-INFO: Loss too large (1132.964->1166.256)! Learning rate decreased to 0.00259.
2024-12-01-23:12:38-root-INFO: Loss too large (1132.964->1135.324)! Learning rate decreased to 0.00207.
2024-12-01-23:12:38-root-INFO: grad norm: 165.636 162.074 34.169
2024-12-01-23:12:39-root-INFO: grad norm: 156.782 153.952 29.653
2024-12-01-23:12:39-root-INFO: grad norm: 147.414 144.284 30.215
2024-12-01-23:12:40-root-INFO: grad norm: 142.957 140.267 27.602
2024-12-01-23:12:40-root-INFO: grad norm: 138.301 135.366 28.342
2024-12-01-23:12:41-root-INFO: grad norm: 136.077 133.449 26.613
2024-12-01-23:12:41-root-INFO: grad norm: 134.105 131.241 27.565
2024-12-01-23:12:41-root-INFO: Loss Change: 1132.964 -> 1093.953
2024-12-01-23:12:41-root-INFO: Regularization Change: 0.000 -> 0.416
2024-12-01-23:12:41-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-01-23:12:41-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-23:12:42-root-INFO: step: 161 lr_xt 0.01025704
2024-12-01-23:12:42-root-INFO: grad norm: 160.507 157.876 28.941
2024-12-01-23:12:42-root-INFO: Loss too large (1097.536->1326.852)! Learning rate decreased to 0.00821.
2024-12-01-23:12:42-root-INFO: Loss too large (1097.536->1292.216)! Learning rate decreased to 0.00656.
2024-12-01-23:12:42-root-INFO: Loss too large (1097.536->1250.486)! Learning rate decreased to 0.00525.
2024-12-01-23:12:42-root-INFO: Loss too large (1097.536->1204.571)! Learning rate decreased to 0.00420.
2024-12-01-23:12:43-root-INFO: Loss too large (1097.536->1160.019)! Learning rate decreased to 0.00336.
2024-12-01-23:12:43-root-INFO: Loss too large (1097.536->1123.689)! Learning rate decreased to 0.00269.
2024-12-01-23:12:43-root-INFO: Loss too large (1097.536->1099.304)! Learning rate decreased to 0.00215.
2024-12-01-23:12:43-root-INFO: grad norm: 138.101 134.992 29.137
2024-12-01-23:12:44-root-INFO: grad norm: 130.285 127.890 24.864
2024-12-01-23:12:44-root-INFO: grad norm: 121.728 119.001 25.622
2024-12-01-23:12:45-root-INFO: grad norm: 118.035 115.720 23.261
2024-12-01-23:12:45-root-INFO: grad norm: 114.424 111.855 24.107
2024-12-01-23:12:46-root-INFO: grad norm: 113.040 110.747 22.654
2024-12-01-23:12:46-root-INFO: grad norm: 112.259 109.732 23.684
2024-12-01-23:12:47-root-INFO: Loss Change: 1097.536 -> 1065.259
2024-12-01-23:12:47-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-01-23:12:47-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-01-23:12:47-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-23:12:47-root-INFO: step: 160 lr_xt 0.01065404
2024-12-01-23:12:47-root-INFO: grad norm: 153.809 150.988 29.320
2024-12-01-23:12:47-root-INFO: Loss too large (1074.624->1307.378)! Learning rate decreased to 0.00852.
2024-12-01-23:12:47-root-INFO: Loss too large (1074.624->1272.119)! Learning rate decreased to 0.00682.
2024-12-01-23:12:48-root-INFO: Loss too large (1074.624->1229.352)! Learning rate decreased to 0.00545.
2024-12-01-23:12:48-root-INFO: Loss too large (1074.624->1182.000)! Learning rate decreased to 0.00436.
2024-12-01-23:12:48-root-INFO: Loss too large (1074.624->1136.203)! Learning rate decreased to 0.00349.
2024-12-01-23:12:48-root-INFO: Loss too large (1074.624->1099.469)! Learning rate decreased to 0.00279.
2024-12-01-23:12:48-root-INFO: Loss too large (1074.624->1075.431)! Learning rate decreased to 0.00223.
2024-12-01-23:12:49-root-INFO: grad norm: 132.567 129.642 27.691
2024-12-01-23:12:49-root-INFO: grad norm: 126.286 123.783 25.020
2024-12-01-23:12:50-root-INFO: grad norm: 118.723 116.114 24.753
2024-12-01-23:12:50-root-INFO: grad norm: 115.606 113.206 23.435
2024-12-01-23:12:51-root-INFO: grad norm: 112.071 109.599 23.411
2024-12-01-23:12:51-root-INFO: grad norm: 110.664 108.308 22.711
2024-12-01-23:12:52-root-INFO: grad norm: 109.449 107.020 22.930
2024-12-01-23:12:52-root-INFO: Loss Change: 1074.624 -> 1042.671
2024-12-01-23:12:52-root-INFO: Regularization Change: 0.000 -> 0.402
2024-12-01-23:12:52-root-INFO: Undo step: 160
2024-12-01-23:12:52-root-INFO: Undo step: 161
2024-12-01-23:12:52-root-INFO: Undo step: 162
2024-12-01-23:12:52-root-INFO: Undo step: 163
2024-12-01-23:12:52-root-INFO: Undo step: 164
2024-12-01-23:12:52-root-INFO: step: 165 lr_xt 0.00879737
2024-12-01-23:12:52-root-INFO: grad norm: 462.164 454.454 84.065
2024-12-01-23:12:53-root-INFO: grad norm: 292.573 282.977 74.314
2024-12-01-23:12:53-root-INFO: grad norm: 365.375 356.906 78.209
2024-12-01-23:12:53-root-INFO: Loss too large (1656.159->1863.314)! Learning rate decreased to 0.00704.
2024-12-01-23:12:54-root-INFO: grad norm: 628.627 618.287 113.548
2024-12-01-23:12:54-root-INFO: Loss too large (1572.313->1709.515)! Learning rate decreased to 0.00563.
2024-12-01-23:12:54-root-INFO: Loss too large (1572.313->1641.922)! Learning rate decreased to 0.00450.
2024-12-01-23:12:54-root-INFO: Loss too large (1572.313->1579.098)! Learning rate decreased to 0.00360.
2024-12-01-23:12:55-root-INFO: grad norm: 193.793 187.091 50.523
2024-12-01-23:12:55-root-INFO: grad norm: 196.424 189.191 52.813
2024-12-01-23:12:56-root-INFO: grad norm: 237.270 227.745 66.551
2024-12-01-23:12:56-root-INFO: Loss too large (1205.581->1328.016)! Learning rate decreased to 0.00288.
2024-12-01-23:12:56-root-INFO: Loss too large (1205.581->1272.115)! Learning rate decreased to 0.00231.
2024-12-01-23:12:56-root-INFO: Loss too large (1205.581->1232.466)! Learning rate decreased to 0.00184.
2024-12-01-23:12:56-root-INFO: Loss too large (1205.581->1206.288)! Learning rate decreased to 0.00148.
2024-12-01-23:12:57-root-INFO: grad norm: 194.205 188.355 47.307
2024-12-01-23:12:57-root-INFO: Loss Change: 1995.832 -> 1176.145
2024-12-01-23:12:57-root-INFO: Regularization Change: 0.000 -> 22.243
2024-12-01-23:12:57-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-01-23:12:57-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-23:12:57-root-INFO: step: 164 lr_xt 0.00914377
2024-12-01-23:12:58-root-INFO: grad norm: 122.300 118.036 32.012
2024-12-01-23:12:58-root-INFO: Loss too large (1164.910->1343.357)! Learning rate decreased to 0.00732.
2024-12-01-23:12:58-root-INFO: Loss too large (1164.910->1284.308)! Learning rate decreased to 0.00585.
2024-12-01-23:12:58-root-INFO: Loss too large (1164.910->1240.335)! Learning rate decreased to 0.00468.
2024-12-01-23:12:58-root-INFO: Loss too large (1164.910->1208.592)! Learning rate decreased to 0.00375.
2024-12-01-23:12:58-root-INFO: Loss too large (1164.910->1186.672)! Learning rate decreased to 0.00300.
2024-12-01-23:12:59-root-INFO: Loss too large (1164.910->1172.368)! Learning rate decreased to 0.00240.
2024-12-01-23:12:59-root-INFO: grad norm: 192.122 186.392 46.572
2024-12-01-23:12:59-root-INFO: Loss too large (1163.679->1188.500)! Learning rate decreased to 0.00192.
2024-12-01-23:12:59-root-INFO: Loss too large (1163.679->1164.502)! Learning rate decreased to 0.00153.
2024-12-01-23:13:00-root-INFO: grad norm: 178.773 172.636 46.439
2024-12-01-23:13:00-root-INFO: grad norm: 172.491 167.129 42.674
2024-12-01-23:13:01-root-INFO: grad norm: 166.779 161.358 42.176
2024-12-01-23:13:01-root-INFO: grad norm: 163.794 158.621 40.838
2024-12-01-23:13:02-root-INFO: grad norm: 161.550 156.527 39.970
2024-12-01-23:13:02-root-INFO: grad norm: 160.304 155.249 39.937
2024-12-01-23:13:02-root-INFO: Loss Change: 1164.910 -> 1118.771
2024-12-01-23:13:02-root-INFO: Regularization Change: 0.000 -> 0.651
2024-12-01-23:13:02-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-01-23:13:03-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-23:13:03-root-INFO: step: 163 lr_xt 0.00950228
2024-12-01-23:13:03-root-INFO: grad norm: 111.316 108.097 26.573
2024-12-01-23:13:03-root-INFO: Loss too large (1109.284->1302.469)! Learning rate decreased to 0.00760.
2024-12-01-23:13:03-root-INFO: Loss too large (1109.284->1242.563)! Learning rate decreased to 0.00608.
2024-12-01-23:13:03-root-INFO: Loss too large (1109.284->1197.677)! Learning rate decreased to 0.00487.
2024-12-01-23:13:03-root-INFO: Loss too large (1109.284->1164.760)! Learning rate decreased to 0.00389.
2024-12-01-23:13:04-root-INFO: Loss too large (1109.284->1141.404)! Learning rate decreased to 0.00311.
2024-12-01-23:13:04-root-INFO: Loss too large (1109.284->1125.520)! Learning rate decreased to 0.00249.
2024-12-01-23:13:04-root-INFO: Loss too large (1109.284->1115.260)! Learning rate decreased to 0.00199.
2024-12-01-23:13:04-root-INFO: grad norm: 148.189 143.667 36.327
2024-12-01-23:13:05-root-INFO: Loss too large (1109.047->1111.477)! Learning rate decreased to 0.00159.
2024-12-01-23:13:05-root-INFO: grad norm: 149.296 145.093 35.174
2024-12-01-23:13:05-root-INFO: grad norm: 149.197 144.761 36.112
2024-12-01-23:13:06-root-INFO: grad norm: 149.483 145.374 34.810
2024-12-01-23:13:06-root-INFO: grad norm: 149.175 144.825 35.760
2024-12-01-23:13:07-root-INFO: grad norm: 149.124 145.106 34.386
2024-12-01-23:13:07-root-INFO: grad norm: 148.807 144.545 35.360
2024-12-01-23:13:08-root-INFO: Loss Change: 1109.284 -> 1081.482
2024-12-01-23:13:08-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-01-23:13:08-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-01-23:13:08-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-23:13:08-root-INFO: step: 162 lr_xt 0.00987325
2024-12-01-23:13:08-root-INFO: grad norm: 86.007 83.467 20.748
2024-12-01-23:13:08-root-INFO: Loss too large (1075.222->1176.246)! Learning rate decreased to 0.00790.
2024-12-01-23:13:08-root-INFO: Loss too large (1075.222->1140.856)! Learning rate decreased to 0.00632.
2024-12-01-23:13:09-root-INFO: Loss too large (1075.222->1115.481)! Learning rate decreased to 0.00506.
2024-12-01-23:13:09-root-INFO: Loss too large (1075.222->1097.860)! Learning rate decreased to 0.00404.
2024-12-01-23:13:09-root-INFO: Loss too large (1075.222->1086.113)! Learning rate decreased to 0.00324.
2024-12-01-23:13:09-root-INFO: Loss too large (1075.222->1078.669)! Learning rate decreased to 0.00259.
2024-12-01-23:13:10-root-INFO: grad norm: 138.400 134.248 33.647
2024-12-01-23:13:10-root-INFO: Loss too large (1074.248->1094.104)! Learning rate decreased to 0.00207.
2024-12-01-23:13:10-root-INFO: Loss too large (1074.248->1077.394)! Learning rate decreased to 0.00166.
2024-12-01-23:13:10-root-INFO: grad norm: 144.818 141.199 32.171
2024-12-01-23:13:11-root-INFO: Loss too large (1068.739->1068.809)! Learning rate decreased to 0.00133.
2024-12-01-23:13:11-root-INFO: grad norm: 107.964 104.343 27.728
2024-12-01-23:13:12-root-INFO: grad norm: 74.580 72.598 17.077
2024-12-01-23:13:12-root-INFO: grad norm: 61.564 58.665 18.671
2024-12-01-23:13:12-root-INFO: grad norm: 51.923 50.108 13.607
2024-12-01-23:13:13-root-INFO: grad norm: 47.014 44.428 15.378
2024-12-01-23:13:13-root-INFO: Loss Change: 1075.222 -> 1047.803
2024-12-01-23:13:13-root-INFO: Regularization Change: 0.000 -> 0.312
2024-12-01-23:13:13-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-01-23:13:13-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-23:13:13-root-INFO: step: 161 lr_xt 0.01025704
2024-12-01-23:13:14-root-INFO: grad norm: 62.629 59.924 18.209
2024-12-01-23:13:14-root-INFO: Loss too large (1045.071->1121.479)! Learning rate decreased to 0.00821.
2024-12-01-23:13:14-root-INFO: Loss too large (1045.071->1090.335)! Learning rate decreased to 0.00656.
2024-12-01-23:13:14-root-INFO: Loss too large (1045.071->1069.021)! Learning rate decreased to 0.00525.
2024-12-01-23:13:14-root-INFO: Loss too large (1045.071->1055.812)! Learning rate decreased to 0.00420.
2024-12-01-23:13:14-root-INFO: Loss too large (1045.071->1048.223)! Learning rate decreased to 0.00336.
2024-12-01-23:13:15-root-INFO: grad norm: 146.413 142.868 32.022
2024-12-01-23:13:15-root-INFO: Loss too large (1044.174->1088.400)! Learning rate decreased to 0.00269.
2024-12-01-23:13:15-root-INFO: Loss too large (1044.174->1067.744)! Learning rate decreased to 0.00215.
2024-12-01-23:13:15-root-INFO: Loss too large (1044.174->1053.816)! Learning rate decreased to 0.00172.
2024-12-01-23:13:15-root-INFO: Loss too large (1044.174->1044.973)! Learning rate decreased to 0.00138.
2024-12-01-23:13:16-root-INFO: grad norm: 106.547 103.556 25.071
2024-12-01-23:13:16-root-INFO: grad norm: 69.065 67.252 15.721
2024-12-01-23:13:17-root-INFO: grad norm: 56.372 53.992 16.208
2024-12-01-23:13:17-root-INFO: grad norm: 47.232 45.559 12.460
2024-12-01-23:13:18-root-INFO: grad norm: 42.910 40.732 13.498
2024-12-01-23:13:18-root-INFO: grad norm: 40.195 38.436 11.761
2024-12-01-23:13:19-root-INFO: Loss Change: 1045.071 -> 1022.537
2024-12-01-23:13:19-root-INFO: Regularization Change: 0.000 -> 0.282
2024-12-01-23:13:19-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-01-23:13:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-23:13:19-root-INFO: step: 160 lr_xt 0.01065404
2024-12-01-23:13:19-root-INFO: grad norm: 88.488 85.619 22.350
2024-12-01-23:13:19-root-INFO: Loss too large (1024.816->1221.396)! Learning rate decreased to 0.00852.
2024-12-01-23:13:19-root-INFO: Loss too large (1024.816->1178.529)! Learning rate decreased to 0.00682.
2024-12-01-23:13:19-root-INFO: Loss too large (1024.816->1132.392)! Learning rate decreased to 0.00545.
2024-12-01-23:13:20-root-INFO: Loss too large (1024.816->1091.278)! Learning rate decreased to 0.00436.
2024-12-01-23:13:20-root-INFO: Loss too large (1024.816->1060.806)! Learning rate decreased to 0.00349.
2024-12-01-23:13:20-root-INFO: Loss too large (1024.816->1041.256)! Learning rate decreased to 0.00279.
2024-12-01-23:13:20-root-INFO: Loss too large (1024.816->1029.955)! Learning rate decreased to 0.00223.
2024-12-01-23:13:20-root-INFO: grad norm: 140.514 137.402 29.407
2024-12-01-23:13:21-root-INFO: Loss too large (1023.964->1033.296)! Learning rate decreased to 0.00179.
2024-12-01-23:13:21-root-INFO: Loss too large (1023.964->1024.932)! Learning rate decreased to 0.00143.
2024-12-01-23:13:21-root-INFO: grad norm: 103.643 100.839 23.941
2024-12-01-23:13:22-root-INFO: grad norm: 67.084 65.459 14.677
2024-12-01-23:13:22-root-INFO: grad norm: 55.020 52.789 15.511
2024-12-01-23:13:23-root-INFO: grad norm: 45.917 44.416 11.644
2024-12-01-23:13:23-root-INFO: grad norm: 41.562 39.543 12.796
2024-12-01-23:13:24-root-INFO: grad norm: 38.730 37.152 10.944
2024-12-01-23:13:24-root-INFO: Loss Change: 1024.816 -> 1003.229
2024-12-01-23:13:24-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-01-23:13:24-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-01-23:13:24-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-23:13:24-root-INFO: step: 159 lr_xt 0.01106461
2024-12-01-23:13:24-root-INFO: grad norm: 63.735 61.582 16.427
2024-12-01-23:13:24-root-INFO: Loss too large (1001.163->1120.171)! Learning rate decreased to 0.00885.
2024-12-01-23:13:25-root-INFO: Loss too large (1001.163->1078.804)! Learning rate decreased to 0.00708.
2024-12-01-23:13:25-root-INFO: Loss too large (1001.163->1046.716)! Learning rate decreased to 0.00567.
2024-12-01-23:13:25-root-INFO: Loss too large (1001.163->1025.113)! Learning rate decreased to 0.00453.
2024-12-01-23:13:25-root-INFO: Loss too large (1001.163->1011.963)! Learning rate decreased to 0.00363.
2024-12-01-23:13:25-root-INFO: Loss too large (1001.163->1004.527)! Learning rate decreased to 0.00290.
2024-12-01-23:13:26-root-INFO: grad norm: 126.751 123.995 26.288
2024-12-01-23:13:26-root-INFO: Loss too large (1000.601->1018.931)! Learning rate decreased to 0.00232.
2024-12-01-23:13:26-root-INFO: Loss too large (1000.601->1007.990)! Learning rate decreased to 0.00186.
2024-12-01-23:13:26-root-INFO: Loss too large (1000.601->1001.055)! Learning rate decreased to 0.00149.
2024-12-01-23:13:27-root-INFO: grad norm: 92.139 89.848 20.421
2024-12-01-23:13:27-root-INFO: grad norm: 58.879 57.372 13.233
2024-12-01-23:13:28-root-INFO: grad norm: 48.423 46.550 13.340
2024-12-01-23:13:28-root-INFO: grad norm: 41.163 39.711 10.837
2024-12-01-23:13:29-root-INFO: grad norm: 37.908 36.154 11.400
2024-12-01-23:13:29-root-INFO: grad norm: 35.966 34.447 10.341
2024-12-01-23:13:29-root-INFO: Loss Change: 1001.163 -> 981.996
2024-12-01-23:13:29-root-INFO: Regularization Change: 0.000 -> 0.239
2024-12-01-23:13:29-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-01-23:13:29-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-23:13:30-root-INFO: step: 158 lr_xt 0.01148915
2024-12-01-23:13:30-root-INFO: grad norm: 71.490 68.988 18.750
2024-12-01-23:13:30-root-INFO: Loss too large (983.123->1130.662)! Learning rate decreased to 0.00919.
2024-12-01-23:13:30-root-INFO: Loss too large (983.123->1083.629)! Learning rate decreased to 0.00735.
2024-12-01-23:13:30-root-INFO: Loss too large (983.123->1043.951)! Learning rate decreased to 0.00588.
2024-12-01-23:13:30-root-INFO: Loss too large (983.123->1015.666)! Learning rate decreased to 0.00471.
2024-12-01-23:13:30-root-INFO: Loss too large (983.123->997.954)! Learning rate decreased to 0.00376.
2024-12-01-23:13:31-root-INFO: Loss too large (983.123->987.841)! Learning rate decreased to 0.00301.
2024-12-01-23:13:31-root-INFO: grad norm: 140.142 137.226 28.439
2024-12-01-23:13:31-root-INFO: Loss too large (982.503->1004.622)! Learning rate decreased to 0.00241.
2024-12-01-23:13:31-root-INFO: Loss too large (982.503->991.680)! Learning rate decreased to 0.00193.
2024-12-01-23:13:32-root-INFO: Loss too large (982.503->983.299)! Learning rate decreased to 0.00154.
2024-12-01-23:13:32-root-INFO: grad norm: 97.359 95.071 20.984
2024-12-01-23:13:33-root-INFO: grad norm: 55.946 54.500 12.637
2024-12-01-23:13:33-root-INFO: grad norm: 45.606 43.784 12.764
2024-12-01-23:13:34-root-INFO: grad norm: 38.975 37.555 10.424
2024-12-01-23:13:34-root-INFO: grad norm: 36.183 34.513 10.865
2024-12-01-23:13:35-root-INFO: grad norm: 34.620 33.159 9.952
2024-12-01-23:13:35-root-INFO: Loss Change: 983.123 -> 962.092
2024-12-01-23:13:35-root-INFO: Regularization Change: 0.000 -> 0.265
2024-12-01-23:13:35-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-01-23:13:35-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-23:13:35-root-INFO: step: 157 lr_xt 0.01192805
2024-12-01-23:13:35-root-INFO: grad norm: 86.052 83.476 20.901
2024-12-01-23:13:35-root-INFO: Loss too large (964.834->1176.914)! Learning rate decreased to 0.00954.
2024-12-01-23:13:35-root-INFO: Loss too large (964.834->1132.347)! Learning rate decreased to 0.00763.
2024-12-01-23:13:36-root-INFO: Loss too large (964.834->1082.636)! Learning rate decreased to 0.00611.
2024-12-01-23:13:36-root-INFO: Loss too large (964.834->1037.055)! Learning rate decreased to 0.00489.
2024-12-01-23:13:36-root-INFO: Loss too large (964.834->1002.998)! Learning rate decreased to 0.00391.
2024-12-01-23:13:36-root-INFO: Loss too large (964.834->981.436)! Learning rate decreased to 0.00313.
2024-12-01-23:13:36-root-INFO: Loss too large (964.834->969.299)! Learning rate decreased to 0.00250.
2024-12-01-23:13:37-root-INFO: grad norm: 128.979 126.414 25.596
2024-12-01-23:13:37-root-INFO: Loss too large (963.088->970.436)! Learning rate decreased to 0.00200.
2024-12-01-23:13:37-root-INFO: Loss too large (963.088->963.261)! Learning rate decreased to 0.00160.
2024-12-01-23:13:38-root-INFO: grad norm: 89.499 87.203 20.142
2024-12-01-23:13:38-root-INFO: grad norm: 52.093 50.666 12.107
2024-12-01-23:13:38-root-INFO: grad norm: 42.873 40.971 12.627
2024-12-01-23:13:39-root-INFO: grad norm: 37.197 35.729 10.345
2024-12-01-23:13:39-root-INFO: grad norm: 34.828 33.105 10.817
2024-12-01-23:13:40-root-INFO: grad norm: 33.516 32.016 9.915
2024-12-01-23:13:40-root-INFO: Loss Change: 964.834 -> 943.573
2024-12-01-23:13:40-root-INFO: Regularization Change: 0.000 -> 0.249
2024-12-01-23:13:40-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-01-23:13:40-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-23:13:40-root-INFO: step: 156 lr_xt 0.01238172
2024-12-01-23:13:40-root-INFO: grad norm: 62.851 60.907 15.513
2024-12-01-23:13:41-root-INFO: Loss too large (943.725->1092.739)! Learning rate decreased to 0.00991.
2024-12-01-23:13:41-root-INFO: Loss too large (943.725->1044.913)! Learning rate decreased to 0.00792.
2024-12-01-23:13:41-root-INFO: Loss too large (943.725->1004.613)! Learning rate decreased to 0.00634.
2024-12-01-23:13:41-root-INFO: Loss too large (943.725->976.305)! Learning rate decreased to 0.00507.
2024-12-01-23:13:41-root-INFO: Loss too large (943.725->958.904)! Learning rate decreased to 0.00406.
2024-12-01-23:13:41-root-INFO: Loss too large (943.725->949.111)! Learning rate decreased to 0.00325.
2024-12-01-23:13:41-root-INFO: Loss too large (943.725->943.971)! Learning rate decreased to 0.00260.
2024-12-01-23:13:42-root-INFO: grad norm: 86.393 84.735 16.848
2024-12-01-23:13:42-root-INFO: Loss too large (941.494->942.917)! Learning rate decreased to 0.00208.
2024-12-01-23:13:43-root-INFO: grad norm: 80.673 78.803 17.268
2024-12-01-23:13:43-root-INFO: grad norm: 72.093 70.672 14.243
2024-12-01-23:13:44-root-INFO: grad norm: 68.416 66.698 15.234
2024-12-01-23:13:44-root-INFO: grad norm: 63.306 62.013 12.726
2024-12-01-23:13:44-root-INFO: grad norm: 60.710 59.093 13.916
2024-12-01-23:13:45-root-INFO: grad norm: 57.320 56.104 11.741
2024-12-01-23:13:45-root-INFO: Loss Change: 943.725 -> 924.977
2024-12-01-23:13:45-root-INFO: Regularization Change: 0.000 -> 0.314
2024-12-01-23:13:45-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-01-23:13:45-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-23:13:45-root-INFO: step: 155 lr_xt 0.01285057
2024-12-01-23:13:46-root-INFO: grad norm: 94.937 92.739 20.313
2024-12-01-23:13:46-root-INFO: Loss too large (928.906->1174.343)! Learning rate decreased to 0.01028.
2024-12-01-23:13:46-root-INFO: Loss too large (928.906->1133.931)! Learning rate decreased to 0.00822.
2024-12-01-23:13:46-root-INFO: Loss too large (928.906->1082.746)! Learning rate decreased to 0.00658.
2024-12-01-23:13:46-root-INFO: Loss too large (928.906->1028.160)! Learning rate decreased to 0.00526.
2024-12-01-23:13:46-root-INFO: Loss too large (928.906->982.245)! Learning rate decreased to 0.00421.
2024-12-01-23:13:46-root-INFO: Loss too large (928.906->951.590)! Learning rate decreased to 0.00337.
2024-12-01-23:13:47-root-INFO: Loss too large (928.906->934.360)! Learning rate decreased to 0.00269.
2024-12-01-23:13:47-root-INFO: grad norm: 127.859 125.468 24.612
2024-12-01-23:13:47-root-INFO: Loss too large (925.823->932.595)! Learning rate decreased to 0.00216.
2024-12-01-23:13:48-root-INFO: grad norm: 106.307 104.263 20.748
2024-12-01-23:13:48-root-INFO: grad norm: 71.579 70.175 14.112
2024-12-01-23:13:49-root-INFO: grad norm: 65.085 63.389 14.761
2024-12-01-23:13:49-root-INFO: grad norm: 56.592 55.387 11.616
2024-12-01-23:13:50-root-INFO: grad norm: 52.676 51.118 12.717
2024-12-01-23:13:50-root-INFO: grad norm: 48.019 46.898 10.315
2024-12-01-23:13:51-root-INFO: Loss Change: 928.906 -> 905.246
2024-12-01-23:13:51-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-01-23:13:51-root-INFO: Undo step: 155
2024-12-01-23:13:51-root-INFO: Undo step: 156
2024-12-01-23:13:51-root-INFO: Undo step: 157
2024-12-01-23:13:51-root-INFO: Undo step: 158
2024-12-01-23:13:51-root-INFO: Undo step: 159
2024-12-01-23:13:51-root-INFO: step: 160 lr_xt 0.01065404
2024-12-01-23:13:51-root-INFO: grad norm: 245.026 235.912 66.206
2024-12-01-23:13:51-root-INFO: grad norm: 216.943 211.985 46.115
2024-12-01-23:13:51-root-INFO: Loss too large (1227.005->1252.086)! Learning rate decreased to 0.00852.
2024-12-01-23:13:52-root-INFO: grad norm: 355.467 351.583 52.404
2024-12-01-23:13:52-root-INFO: Loss too large (1192.201->1911.298)! Learning rate decreased to 0.00682.
2024-12-01-23:13:52-root-INFO: Loss too large (1192.201->1673.299)! Learning rate decreased to 0.00545.
2024-12-01-23:13:52-root-INFO: Loss too large (1192.201->1507.343)! Learning rate decreased to 0.00436.
2024-12-01-23:13:53-root-INFO: Loss too large (1192.201->1394.068)! Learning rate decreased to 0.00349.
2024-12-01-23:13:53-root-INFO: Loss too large (1192.201->1317.087)! Learning rate decreased to 0.00279.
2024-12-01-23:13:53-root-INFO: Loss too large (1192.201->1264.037)! Learning rate decreased to 0.00223.
2024-12-01-23:13:53-root-INFO: Loss too large (1192.201->1226.757)! Learning rate decreased to 0.00179.
2024-12-01-23:13:53-root-INFO: Loss too large (1192.201->1200.351)! Learning rate decreased to 0.00143.
2024-12-01-23:13:54-root-INFO: grad norm: 268.270 264.963 41.990
2024-12-01-23:13:54-root-INFO: grad norm: 118.900 115.642 27.643
2024-12-01-23:13:55-root-INFO: grad norm: 99.593 96.419 24.944
2024-12-01-23:13:55-root-INFO: grad norm: 86.882 84.135 21.675
2024-12-01-23:13:56-root-INFO: grad norm: 79.980 77.169 21.016
2024-12-01-23:13:56-root-INFO: Loss Change: 1430.979 -> 1063.760
2024-12-01-23:13:56-root-INFO: Regularization Change: 0.000 -> 9.616
2024-12-01-23:13:56-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-01-23:13:56-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-23:13:56-root-INFO: step: 159 lr_xt 0.01106461
2024-12-01-23:13:56-root-INFO: grad norm: 63.922 61.526 17.337
2024-12-01-23:13:57-root-INFO: grad norm: 132.500 130.822 21.020
2024-12-01-23:13:57-root-INFO: Loss too large (1025.605->1309.109)! Learning rate decreased to 0.00885.
2024-12-01-23:13:57-root-INFO: Loss too large (1025.605->1229.493)! Learning rate decreased to 0.00708.
2024-12-01-23:13:57-root-INFO: Loss too large (1025.605->1171.099)! Learning rate decreased to 0.00567.
2024-12-01-23:13:57-root-INFO: Loss too large (1025.605->1127.429)! Learning rate decreased to 0.00453.
2024-12-01-23:13:58-root-INFO: Loss too large (1025.605->1094.468)! Learning rate decreased to 0.00363.
2024-12-01-23:13:58-root-INFO: Loss too large (1025.605->1069.766)! Learning rate decreased to 0.00290.
2024-12-01-23:13:58-root-INFO: Loss too large (1025.605->1051.683)! Learning rate decreased to 0.00232.
2024-12-01-23:13:58-root-INFO: Loss too large (1025.605->1038.925)! Learning rate decreased to 0.00186.
2024-12-01-23:13:58-root-INFO: Loss too large (1025.605->1030.354)! Learning rate decreased to 0.00149.
2024-12-01-23:13:59-root-INFO: grad norm: 132.313 130.642 20.961
2024-12-01-23:13:59-root-INFO: grad norm: 133.604 131.897 21.289
2024-12-01-23:13:59-root-INFO: Loss too large (1018.414->1018.532)! Learning rate decreased to 0.00119.
2024-12-01-23:14:00-root-INFO: grad norm: 103.001 101.408 18.048
2024-12-01-23:14:00-root-INFO: grad norm: 70.276 68.751 14.562
2024-12-01-23:14:01-root-INFO: grad norm: 60.636 58.944 14.221
2024-12-01-23:14:01-root-INFO: grad norm: 52.819 51.161 13.131
2024-12-01-23:14:01-root-INFO: Loss Change: 1056.266 -> 1001.808
2024-12-01-23:14:01-root-INFO: Regularization Change: 0.000 -> 1.117
2024-12-01-23:14:01-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-01-23:14:01-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-23:14:02-root-INFO: step: 158 lr_xt 0.01148915
2024-12-01-23:14:02-root-INFO: grad norm: 97.484 95.685 18.644
2024-12-01-23:14:02-root-INFO: Loss too large (1001.039->1238.493)! Learning rate decreased to 0.00919.
2024-12-01-23:14:02-root-INFO: Loss too large (1001.039->1205.510)! Learning rate decreased to 0.00735.
2024-12-01-23:14:02-root-INFO: Loss too large (1001.039->1165.105)! Learning rate decreased to 0.00588.
2024-12-01-23:14:02-root-INFO: Loss too large (1001.039->1118.976)! Learning rate decreased to 0.00471.
2024-12-01-23:14:03-root-INFO: Loss too large (1001.039->1074.113)! Learning rate decreased to 0.00376.
2024-12-01-23:14:03-root-INFO: Loss too large (1001.039->1039.352)! Learning rate decreased to 0.00301.
2024-12-01-23:14:03-root-INFO: Loss too large (1001.039->1017.263)! Learning rate decreased to 0.00241.
2024-12-01-23:14:03-root-INFO: Loss too large (1001.039->1005.018)! Learning rate decreased to 0.00193.
2024-12-01-23:14:03-root-INFO: grad norm: 151.037 149.314 22.748
2024-12-01-23:14:04-root-INFO: Loss too large (998.895->1007.100)! Learning rate decreased to 0.00154.
2024-12-01-23:14:04-root-INFO: Loss too large (998.895->999.693)! Learning rate decreased to 0.00123.
2024-12-01-23:14:04-root-INFO: grad norm: 108.178 106.526 18.835
2024-12-01-23:14:05-root-INFO: grad norm: 63.026 61.524 13.678
2024-12-01-23:14:05-root-INFO: grad norm: 53.375 51.577 13.738
2024-12-01-23:14:06-root-INFO: grad norm: 46.681 44.974 12.506
2024-12-01-23:14:06-root-INFO: grad norm: 43.737 41.867 12.651
2024-12-01-23:14:07-root-INFO: grad norm: 41.904 40.119 12.102
2024-12-01-23:14:07-root-INFO: Loss Change: 1001.039 -> 977.115
2024-12-01-23:14:07-root-INFO: Regularization Change: 0.000 -> 0.222
2024-12-01-23:14:07-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-01-23:14:07-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-23:14:07-root-INFO: step: 157 lr_xt 0.01192805
2024-12-01-23:14:07-root-INFO: grad norm: 106.421 104.419 20.544
2024-12-01-23:14:07-root-INFO: Loss too large (980.212->1237.944)! Learning rate decreased to 0.00954.
2024-12-01-23:14:08-root-INFO: Loss too large (980.212->1209.907)! Learning rate decreased to 0.00763.
2024-12-01-23:14:08-root-INFO: Loss too large (980.212->1172.990)! Learning rate decreased to 0.00611.
2024-12-01-23:14:08-root-INFO: Loss too large (980.212->1127.285)! Learning rate decreased to 0.00489.
2024-12-01-23:14:08-root-INFO: Loss too large (980.212->1077.052)! Learning rate decreased to 0.00391.
2024-12-01-23:14:08-root-INFO: Loss too large (980.212->1033.144)! Learning rate decreased to 0.00313.
2024-12-01-23:14:08-root-INFO: Loss too large (980.212->1003.260)! Learning rate decreased to 0.00250.
2024-12-01-23:14:08-root-INFO: Loss too large (980.212->986.362)! Learning rate decreased to 0.00200.
2024-12-01-23:14:09-root-INFO: grad norm: 159.181 157.446 23.439
2024-12-01-23:14:09-root-INFO: Loss too large (977.947->987.248)! Learning rate decreased to 0.00160.
2024-12-01-23:14:09-root-INFO: Loss too large (977.947->979.054)! Learning rate decreased to 0.00128.
2024-12-01-23:14:10-root-INFO: grad norm: 107.209 105.410 19.554
2024-12-01-23:14:10-root-INFO: grad norm: 54.512 52.856 13.333
2024-12-01-23:14:11-root-INFO: grad norm: 46.326 44.270 13.647
2024-12-01-23:14:11-root-INFO: grad norm: 41.522 39.622 12.417
2024-12-01-23:14:12-root-INFO: grad norm: 39.517 37.480 12.522
2024-12-01-23:14:12-root-INFO: grad norm: 38.356 36.437 11.980
2024-12-01-23:14:12-root-INFO: Loss Change: 980.212 -> 956.632
2024-12-01-23:14:12-root-INFO: Regularization Change: 0.000 -> 0.213
2024-12-01-23:14:12-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-01-23:14:12-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-23:14:13-root-INFO: step: 156 lr_xt 0.01238172
2024-12-01-23:14:13-root-INFO: grad norm: 70.375 68.678 15.359
2024-12-01-23:14:13-root-INFO: Loss too large (955.687->1155.094)! Learning rate decreased to 0.00991.
2024-12-01-23:14:13-root-INFO: Loss too large (955.687->1112.554)! Learning rate decreased to 0.00792.
2024-12-01-23:14:13-root-INFO: Loss too large (955.687->1065.099)! Learning rate decreased to 0.00634.
2024-12-01-23:14:13-root-INFO: Loss too large (955.687->1021.804)! Learning rate decreased to 0.00507.
2024-12-01-23:14:13-root-INFO: Loss too large (955.687->990.503)! Learning rate decreased to 0.00406.
2024-12-01-23:14:14-root-INFO: Loss too large (955.687->971.468)! Learning rate decreased to 0.00325.
2024-12-01-23:14:14-root-INFO: Loss too large (955.687->961.036)! Learning rate decreased to 0.00260.
2024-12-01-23:14:14-root-INFO: Loss too large (955.687->955.718)! Learning rate decreased to 0.00208.
2024-12-01-23:14:14-root-INFO: grad norm: 93.844 92.587 15.311
2024-12-01-23:14:15-root-INFO: Loss too large (953.233->954.070)! Learning rate decreased to 0.00166.
2024-12-01-23:14:15-root-INFO: grad norm: 84.384 82.888 15.818
2024-12-01-23:14:15-root-INFO: grad norm: 70.385 69.183 12.957
2024-12-01-23:14:16-root-INFO: grad norm: 65.202 63.737 13.743
2024-12-01-23:14:16-root-INFO: grad norm: 58.403 57.197 11.804
2024-12-01-23:14:17-root-INFO: grad norm: 54.922 53.467 12.560
2024-12-01-23:14:17-root-INFO: grad norm: 50.730 49.505 11.080
2024-12-01-23:14:18-root-INFO: Loss Change: 955.687 -> 936.224
2024-12-01-23:14:18-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-01-23:14:18-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-01-23:14:18-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-23:14:18-root-INFO: step: 155 lr_xt 0.01285057
2024-12-01-23:14:18-root-INFO: grad norm: 94.714 93.078 17.531
2024-12-01-23:14:18-root-INFO: Loss too large (939.305->1197.116)! Learning rate decreased to 0.01028.
2024-12-01-23:14:18-root-INFO: Loss too large (939.305->1165.407)! Learning rate decreased to 0.00822.
2024-12-01-23:14:18-root-INFO: Loss too large (939.305->1123.823)! Learning rate decreased to 0.00658.
2024-12-01-23:14:19-root-INFO: Loss too large (939.305->1072.948)! Learning rate decreased to 0.00526.
2024-12-01-23:14:19-root-INFO: Loss too large (939.305->1020.442)! Learning rate decreased to 0.00421.
2024-12-01-23:14:19-root-INFO: Loss too large (939.305->979.455)! Learning rate decreased to 0.00337.
2024-12-01-23:14:19-root-INFO: Loss too large (939.305->954.501)! Learning rate decreased to 0.00269.
2024-12-01-23:14:19-root-INFO: Loss too large (939.305->941.522)! Learning rate decreased to 0.00216.
2024-12-01-23:14:20-root-INFO: grad norm: 117.244 115.875 17.865
2024-12-01-23:14:20-root-INFO: Loss too large (935.488->938.424)! Learning rate decreased to 0.00172.
2024-12-01-23:14:20-root-INFO: grad norm: 96.005 94.539 16.717
2024-12-01-23:14:21-root-INFO: grad norm: 64.468 63.272 12.363
2024-12-01-23:14:21-root-INFO: grad norm: 57.306 55.836 12.897
2024-12-01-23:14:22-root-INFO: grad norm: 49.044 47.807 10.944
2024-12-01-23:14:22-root-INFO: grad norm: 45.022 43.523 11.521
2024-12-01-23:14:23-root-INFO: grad norm: 40.963 39.666 10.229
2024-12-01-23:14:23-root-INFO: Loss Change: 939.305 -> 917.166
2024-12-01-23:14:23-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-01-23:14:23-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-01-23:14:23-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-23:14:23-root-INFO: step: 154 lr_xt 0.01333503
2024-12-01-23:14:23-root-INFO: grad norm: 80.569 79.015 15.749
2024-12-01-23:14:23-root-INFO: Loss too large (918.814->1156.809)! Learning rate decreased to 0.01067.
2024-12-01-23:14:24-root-INFO: Loss too large (918.814->1117.822)! Learning rate decreased to 0.00853.
2024-12-01-23:14:24-root-INFO: Loss too large (918.814->1068.771)! Learning rate decreased to 0.00683.
2024-12-01-23:14:24-root-INFO: Loss too large (918.814->1015.173)! Learning rate decreased to 0.00546.
2024-12-01-23:14:24-root-INFO: Loss too large (918.814->970.276)! Learning rate decreased to 0.00437.
2024-12-01-23:14:24-root-INFO: Loss too large (918.814->941.383)! Learning rate decreased to 0.00350.
2024-12-01-23:14:24-root-INFO: Loss too large (918.814->925.699)! Learning rate decreased to 0.00280.
2024-12-01-23:14:25-root-INFO: grad norm: 138.549 137.218 19.163
2024-12-01-23:14:25-root-INFO: Loss too large (918.012->930.713)! Learning rate decreased to 0.00224.
2024-12-01-23:14:25-root-INFO: Loss too large (918.012->921.872)! Learning rate decreased to 0.00179.
2024-12-01-23:14:26-root-INFO: grad norm: 97.950 96.593 16.252
2024-12-01-23:14:26-root-INFO: grad norm: 46.014 44.789 10.549
2024-12-01-23:14:27-root-INFO: grad norm: 40.020 38.477 11.003
2024-12-01-23:14:27-root-INFO: grad norm: 35.540 34.145 9.859
2024-12-01-23:14:28-root-INFO: grad norm: 33.506 31.944 10.112
2024-12-01-23:14:28-root-INFO: grad norm: 32.176 30.727 9.547
2024-12-01-23:14:28-root-INFO: Loss Change: 918.814 -> 898.167
2024-12-01-23:14:28-root-INFO: Regularization Change: 0.000 -> 0.269
2024-12-01-23:14:28-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-01-23:14:28-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-23:14:29-root-INFO: step: 153 lr_xt 0.01383551
2024-12-01-23:14:29-root-INFO: grad norm: 66.128 64.492 14.618
2024-12-01-23:14:29-root-INFO: Loss too large (899.639->1093.062)! Learning rate decreased to 0.01107.
2024-12-01-23:14:29-root-INFO: Loss too large (899.639->1041.856)! Learning rate decreased to 0.00885.
2024-12-01-23:14:29-root-INFO: Loss too large (899.639->988.756)! Learning rate decreased to 0.00708.
2024-12-01-23:14:29-root-INFO: Loss too large (899.639->946.811)! Learning rate decreased to 0.00567.
2024-12-01-23:14:29-root-INFO: Loss too large (899.639->920.717)! Learning rate decreased to 0.00453.
2024-12-01-23:14:30-root-INFO: Loss too large (899.639->906.634)! Learning rate decreased to 0.00363.
2024-12-01-23:14:30-root-INFO: Loss too large (899.639->899.643)! Learning rate decreased to 0.00290.
2024-12-01-23:14:30-root-INFO: grad norm: 90.049 88.855 14.614
2024-12-01-23:14:30-root-INFO: Loss too large (896.467->898.531)! Learning rate decreased to 0.00232.
2024-12-01-23:14:31-root-INFO: grad norm: 80.058 78.746 14.434
2024-12-01-23:14:31-root-INFO: grad norm: 62.986 61.934 11.461
2024-12-01-23:14:32-root-INFO: grad norm: 58.519 57.239 12.171
2024-12-01-23:14:32-root-INFO: grad norm: 52.077 51.065 10.216
2024-12-01-23:14:33-root-INFO: grad norm: 49.165 47.904 11.062
2024-12-01-23:14:33-root-INFO: grad norm: 45.357 44.353 9.493
2024-12-01-23:14:33-root-INFO: Loss Change: 899.639 -> 878.005
2024-12-01-23:14:33-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-01-23:14:33-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-01-23:14:33-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-23:14:34-root-INFO: step: 152 lr_xt 0.01435246
2024-12-01-23:14:34-root-INFO: grad norm: 67.014 65.755 12.931
2024-12-01-23:14:34-root-INFO: Loss too large (878.915->1087.139)! Learning rate decreased to 0.01148.
2024-12-01-23:14:34-root-INFO: Loss too large (878.915->1037.887)! Learning rate decreased to 0.00919.
2024-12-01-23:14:34-root-INFO: Loss too large (878.915->982.195)! Learning rate decreased to 0.00735.
2024-12-01-23:14:34-root-INFO: Loss too large (878.915->934.389)! Learning rate decreased to 0.00588.
2024-12-01-23:14:35-root-INFO: Loss too large (878.915->903.645)! Learning rate decreased to 0.00470.
2024-12-01-23:14:35-root-INFO: Loss too large (878.915->887.135)! Learning rate decreased to 0.00376.
2024-12-01-23:14:35-root-INFO: Loss too large (878.915->879.087)! Learning rate decreased to 0.00301.
2024-12-01-23:14:35-root-INFO: grad norm: 79.514 78.544 12.385
2024-12-01-23:14:35-root-INFO: Loss too large (875.503->875.983)! Learning rate decreased to 0.00241.
2024-12-01-23:14:36-root-INFO: grad norm: 65.436 64.279 12.250
2024-12-01-23:14:36-root-INFO: grad norm: 46.517 45.537 9.500
2024-12-01-23:14:37-root-INFO: grad norm: 41.411 40.176 10.038
2024-12-01-23:14:37-root-INFO: grad norm: 36.230 35.157 8.752
2024-12-01-23:14:38-root-INFO: grad norm: 33.737 32.461 9.192
2024-12-01-23:14:38-root-INFO: grad norm: 31.542 30.404 8.399
2024-12-01-23:14:39-root-INFO: Loss Change: 878.915 -> 858.535
2024-12-01-23:14:39-root-INFO: Regularization Change: 0.000 -> 0.351
2024-12-01-23:14:39-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-01-23:14:39-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-23:14:39-root-INFO: step: 151 lr_xt 0.01488633
2024-12-01-23:14:39-root-INFO: grad norm: 60.971 59.578 12.958
2024-12-01-23:14:39-root-INFO: Loss too large (860.486->1038.979)! Learning rate decreased to 0.01191.
2024-12-01-23:14:39-root-INFO: Loss too large (860.486->981.528)! Learning rate decreased to 0.00953.
2024-12-01-23:14:39-root-INFO: Loss too large (860.486->928.389)! Learning rate decreased to 0.00762.
2024-12-01-23:14:39-root-INFO: Loss too large (860.486->892.272)! Learning rate decreased to 0.00610.
2024-12-01-23:14:40-root-INFO: Loss too large (860.486->872.325)! Learning rate decreased to 0.00488.
2024-12-01-23:14:40-root-INFO: Loss too large (860.486->862.402)! Learning rate decreased to 0.00390.
2024-12-01-23:14:40-root-INFO: grad norm: 93.306 92.318 13.542
2024-12-01-23:14:40-root-INFO: Loss too large (857.838->863.703)! Learning rate decreased to 0.00312.
2024-12-01-23:14:41-root-INFO: Loss too large (857.838->858.418)! Learning rate decreased to 0.00250.
2024-12-01-23:14:41-root-INFO: grad norm: 66.457 65.320 12.242
2024-12-01-23:14:42-root-INFO: grad norm: 36.553 35.465 8.853
2024-12-01-23:14:42-root-INFO: grad norm: 32.209 30.878 9.165
2024-12-01-23:14:43-root-INFO: grad norm: 29.486 28.269 8.384
2024-12-01-23:14:43-root-INFO: grad norm: 28.347 27.038 8.517
2024-12-01-23:14:43-root-INFO: grad norm: 27.669 26.440 8.153
2024-12-01-23:14:44-root-INFO: Loss Change: 860.486 -> 839.474
2024-12-01-23:14:44-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-01-23:14:44-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-01-23:14:44-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-23:14:44-root-INFO: step: 150 lr_xt 0.01543756
2024-12-01-23:14:44-root-INFO: grad norm: 38.737 37.427 9.989
2024-12-01-23:14:44-root-INFO: Loss too large (839.510->878.051)! Learning rate decreased to 0.01235.
2024-12-01-23:14:44-root-INFO: Loss too large (839.510->857.120)! Learning rate decreased to 0.00988.
2024-12-01-23:14:45-root-INFO: Loss too large (839.510->845.945)! Learning rate decreased to 0.00790.
2024-12-01-23:14:45-root-INFO: Loss too large (839.510->840.320)! Learning rate decreased to 0.00632.
2024-12-01-23:14:45-root-INFO: grad norm: 88.875 88.021 12.296
2024-12-01-23:14:45-root-INFO: Loss too large (837.677->860.572)! Learning rate decreased to 0.00506.
2024-12-01-23:14:45-root-INFO: Loss too large (837.677->849.752)! Learning rate decreased to 0.00405.
2024-12-01-23:14:46-root-INFO: Loss too large (837.677->842.394)! Learning rate decreased to 0.00324.
2024-12-01-23:14:46-root-INFO: grad norm: 75.337 74.342 12.209
2024-12-01-23:14:47-root-INFO: grad norm: 50.111 49.357 8.664
2024-12-01-23:14:47-root-INFO: grad norm: 46.651 45.641 9.655
2024-12-01-23:14:47-root-INFO: grad norm: 41.858 41.088 7.995
2024-12-01-23:14:48-root-INFO: grad norm: 39.849 38.839 8.915
2024-12-01-23:14:49-root-INFO: grad norm: 37.295 36.508 7.620
2024-12-01-23:14:49-root-INFO: Loss Change: 839.510 -> 818.191
2024-12-01-23:14:49-root-INFO: Regularization Change: 0.000 -> 0.616
2024-12-01-23:14:49-root-INFO: Undo step: 150
2024-12-01-23:14:49-root-INFO: Undo step: 151
2024-12-01-23:14:49-root-INFO: Undo step: 152
2024-12-01-23:14:49-root-INFO: Undo step: 153
2024-12-01-23:14:49-root-INFO: Undo step: 154
2024-12-01-23:14:49-root-INFO: step: 155 lr_xt 0.01285057
2024-12-01-23:14:49-root-INFO: grad norm: 280.260 272.866 63.952
2024-12-01-23:14:50-root-INFO: grad norm: 320.468 317.043 46.728
2024-12-01-23:14:50-root-INFO: grad norm: 177.606 174.239 34.418
2024-12-01-23:14:51-root-INFO: grad norm: 242.008 237.189 48.055
2024-12-01-23:14:51-root-INFO: Loss too large (1280.799->1302.120)! Learning rate decreased to 0.01028.
2024-12-01-23:14:51-root-INFO: grad norm: 250.654 249.508 23.947
2024-12-01-23:14:51-root-INFO: Loss too large (1158.263->1205.613)! Learning rate decreased to 0.00822.
2024-12-01-23:14:52-root-INFO: grad norm: 270.371 258.299 79.889
2024-12-01-23:14:52-root-INFO: Loss too large (1134.029->1153.709)! Learning rate decreased to 0.00658.
2024-12-01-23:14:52-root-INFO: grad norm: 211.402 207.672 39.536
2024-12-01-23:14:53-root-INFO: grad norm: 252.847 236.704 88.898
2024-12-01-23:14:53-root-INFO: Loss too large (956.976->1083.618)! Learning rate decreased to 0.00526.
2024-12-01-23:14:53-root-INFO: Loss too large (956.976->1024.900)! Learning rate decreased to 0.00421.
2024-12-01-23:14:53-root-INFO: Loss too large (956.976->985.292)! Learning rate decreased to 0.00337.
2024-12-01-23:14:54-root-INFO: Loss too large (956.976->958.693)! Learning rate decreased to 0.00269.
2024-12-01-23:14:54-root-INFO: Loss Change: 1572.723 -> 941.232
2024-12-01-23:14:54-root-INFO: Regularization Change: 0.000 -> 26.281
2024-12-01-23:14:54-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-01-23:14:54-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-23:14:54-root-INFO: step: 154 lr_xt 0.01333503
2024-12-01-23:14:54-root-INFO: grad norm: 111.776 108.376 27.362
2024-12-01-23:14:54-root-INFO: Loss too large (942.069->1099.687)! Learning rate decreased to 0.01067.
2024-12-01-23:14:55-root-INFO: Loss too large (942.069->1028.165)! Learning rate decreased to 0.00853.
2024-12-01-23:14:55-root-INFO: Loss too large (942.069->956.520)! Learning rate decreased to 0.00683.
2024-12-01-23:14:55-root-INFO: grad norm: 160.345 148.719 59.943
2024-12-01-23:14:55-root-INFO: Loss too large (918.539->965.844)! Learning rate decreased to 0.00546.
2024-12-01-23:14:55-root-INFO: Loss too large (918.539->939.443)! Learning rate decreased to 0.00437.
2024-12-01-23:14:56-root-INFO: Loss too large (918.539->921.965)! Learning rate decreased to 0.00350.
2024-12-01-23:14:56-root-INFO: grad norm: 87.883 83.810 26.443
2024-12-01-23:14:57-root-INFO: grad norm: 38.058 36.202 11.739
2024-12-01-23:14:57-root-INFO: grad norm: 36.122 34.435 10.909
2024-12-01-23:14:58-root-INFO: grad norm: 35.026 33.391 10.575
2024-12-01-23:14:58-root-INFO: grad norm: 34.162 32.577 10.287
2024-12-01-23:14:59-root-INFO: grad norm: 33.411 31.874 10.018
2024-12-01-23:14:59-root-INFO: Loss Change: 942.069 -> 871.049
2024-12-01-23:14:59-root-INFO: Regularization Change: 0.000 -> 1.536
2024-12-01-23:14:59-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-01-23:14:59-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-23:14:59-root-INFO: step: 153 lr_xt 0.01383551
2024-12-01-23:14:59-root-INFO: grad norm: 45.826 43.177 15.356
2024-12-01-23:14:59-root-INFO: Loss too large (869.122->875.384)! Learning rate decreased to 0.01107.
2024-12-01-23:15:00-root-INFO: grad norm: 126.837 116.293 50.632
2024-12-01-23:15:00-root-INFO: Loss too large (866.959->971.531)! Learning rate decreased to 0.00885.
2024-12-01-23:15:00-root-INFO: Loss too large (866.959->929.610)! Learning rate decreased to 0.00708.
2024-12-01-23:15:00-root-INFO: Loss too large (866.959->900.835)! Learning rate decreased to 0.00567.
2024-12-01-23:15:01-root-INFO: Loss too large (866.959->881.320)! Learning rate decreased to 0.00453.
2024-12-01-23:15:01-root-INFO: Loss too large (866.959->868.475)! Learning rate decreased to 0.00363.
2024-12-01-23:15:01-root-INFO: grad norm: 75.453 71.580 23.864
2024-12-01-23:15:02-root-INFO: grad norm: 31.437 29.993 9.421
2024-12-01-23:15:02-root-INFO: grad norm: 29.962 28.652 8.762
2024-12-01-23:15:03-root-INFO: grad norm: 29.238 27.958 8.558
2024-12-01-23:15:03-root-INFO: grad norm: 28.741 27.509 8.323
2024-12-01-23:15:04-root-INFO: grad norm: 28.323 27.110 8.200
2024-12-01-23:15:04-root-INFO: Loss Change: 869.122 -> 832.916
2024-12-01-23:15:04-root-INFO: Regularization Change: 0.000 -> 1.271
2024-12-01-23:15:04-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-01-23:15:04-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-23:15:04-root-INFO: step: 152 lr_xt 0.01435246
2024-12-01-23:15:04-root-INFO: grad norm: 36.642 34.815 11.426
2024-12-01-23:15:04-root-INFO: Loss too large (830.360->833.384)! Learning rate decreased to 0.01148.
2024-12-01-23:15:05-root-INFO: grad norm: 99.848 91.636 39.654
2024-12-01-23:15:05-root-INFO: Loss too large (828.375->898.382)! Learning rate decreased to 0.00919.
2024-12-01-23:15:05-root-INFO: Loss too large (828.375->869.713)! Learning rate decreased to 0.00735.
2024-12-01-23:15:05-root-INFO: Loss too large (828.375->850.031)! Learning rate decreased to 0.00588.
2024-12-01-23:15:05-root-INFO: Loss too large (828.375->836.789)! Learning rate decreased to 0.00470.
2024-12-01-23:15:06-root-INFO: grad norm: 78.974 74.952 24.881
2024-12-01-23:15:06-root-INFO: grad norm: 43.704 41.061 14.967
2024-12-01-23:15:07-root-INFO: grad norm: 39.119 36.974 12.776
2024-12-01-23:15:07-root-INFO: grad norm: 34.684 32.778 11.339
2024-12-01-23:15:08-root-INFO: grad norm: 32.762 31.100 10.301
2024-12-01-23:15:08-root-INFO: grad norm: 30.950 29.361 9.790
2024-12-01-23:15:08-root-INFO: Loss Change: 830.360 -> 798.705
2024-12-01-23:15:08-root-INFO: Regularization Change: 0.000 -> 1.388
2024-12-01-23:15:08-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-01-23:15:08-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-23:15:09-root-INFO: step: 151 lr_xt 0.01488633
2024-12-01-23:15:09-root-INFO: grad norm: 50.179 47.227 16.957
2024-12-01-23:15:09-root-INFO: Loss too large (799.617->844.248)! Learning rate decreased to 0.01191.
2024-12-01-23:15:09-root-INFO: Loss too large (799.617->817.131)! Learning rate decreased to 0.00953.
2024-12-01-23:15:09-root-INFO: Loss too large (799.617->803.478)! Learning rate decreased to 0.00762.
2024-12-01-23:15:10-root-INFO: grad norm: 82.536 76.744 30.374
2024-12-01-23:15:10-root-INFO: Loss too large (797.042->808.614)! Learning rate decreased to 0.00610.
2024-12-01-23:15:10-root-INFO: Loss too large (797.042->799.730)! Learning rate decreased to 0.00488.
2024-12-01-23:15:11-root-INFO: grad norm: 61.677 58.262 20.238
2024-12-01-23:15:11-root-INFO: grad norm: 35.013 33.378 10.573
2024-12-01-23:15:12-root-INFO: grad norm: 30.474 28.826 9.886
2024-12-01-23:15:12-root-INFO: grad norm: 27.136 26.014 7.722
2024-12-01-23:15:12-root-INFO: grad norm: 25.637 24.452 7.706
2024-12-01-23:15:13-root-INFO: grad norm: 24.537 23.581 6.783
2024-12-01-23:15:13-root-INFO: Loss Change: 799.617 -> 771.056
2024-12-01-23:15:13-root-INFO: Regularization Change: 0.000 -> 1.070
2024-12-01-23:15:13-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-01-23:15:13-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-23:15:13-root-INFO: step: 150 lr_xt 0.01543756
2024-12-01-23:15:14-root-INFO: grad norm: 32.734 30.959 10.632
2024-12-01-23:15:14-root-INFO: Loss too large (770.075->773.993)! Learning rate decreased to 0.01235.
2024-12-01-23:15:14-root-INFO: grad norm: 84.854 78.810 31.452
2024-12-01-23:15:14-root-INFO: Loss too large (769.178->819.220)! Learning rate decreased to 0.00988.
2024-12-01-23:15:15-root-INFO: Loss too large (769.178->797.232)! Learning rate decreased to 0.00790.
2024-12-01-23:15:15-root-INFO: Loss too large (769.178->782.451)! Learning rate decreased to 0.00632.
2024-12-01-23:15:15-root-INFO: Loss too large (769.178->772.770)! Learning rate decreased to 0.00506.
2024-12-01-23:15:15-root-INFO: grad norm: 60.788 57.604 19.415
2024-12-01-23:15:16-root-INFO: grad norm: 30.611 29.311 8.825
2024-12-01-23:15:16-root-INFO: grad norm: 26.452 25.107 8.327
2024-12-01-23:15:17-root-INFO: grad norm: 23.816 22.917 6.482
2024-12-01-23:15:17-root-INFO: grad norm: 22.686 21.714 6.568
2024-12-01-23:15:18-root-INFO: grad norm: 21.939 21.139 5.869
2024-12-01-23:15:18-root-INFO: Loss Change: 770.075 -> 745.270
2024-12-01-23:15:18-root-INFO: Regularization Change: 0.000 -> 1.145
2024-12-01-23:15:18-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-01-23:15:18-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-23:15:18-root-INFO: step: 149 lr_xt 0.01600663
2024-12-01-23:15:18-root-INFO: grad norm: 36.609 34.626 11.884
2024-12-01-23:15:19-root-INFO: Loss too large (745.081->756.548)! Learning rate decreased to 0.01281.
2024-12-01-23:15:19-root-INFO: Loss too large (745.081->747.603)! Learning rate decreased to 0.01024.
2024-12-01-23:15:19-root-INFO: grad norm: 68.394 64.316 23.265
2024-12-01-23:15:19-root-INFO: Loss too large (743.295->758.902)! Learning rate decreased to 0.00820.
2024-12-01-23:15:20-root-INFO: Loss too large (743.295->749.162)! Learning rate decreased to 0.00656.
2024-12-01-23:15:20-root-INFO: grad norm: 60.247 57.305 18.596
2024-12-01-23:15:21-root-INFO: grad norm: 44.614 42.451 13.725
2024-12-01-23:15:21-root-INFO: grad norm: 41.182 38.943 13.394
2024-12-01-23:15:22-root-INFO: grad norm: 37.148 35.387 11.303
2024-12-01-23:15:22-root-INFO: grad norm: 35.705 33.758 11.627
2024-12-01-23:15:22-root-INFO: grad norm: 34.079 32.475 10.333
2024-12-01-23:15:23-root-INFO: Loss Change: 745.081 -> 720.488
2024-12-01-23:15:23-root-INFO: Regularization Change: 0.000 -> 1.362
2024-12-01-23:15:23-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-01-23:15:23-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-23:15:23-root-INFO: step: 148 lr_xt 0.01659399
2024-12-01-23:15:23-root-INFO: grad norm: 46.108 43.785 14.450
2024-12-01-23:15:23-root-INFO: Loss too large (721.351->754.107)! Learning rate decreased to 0.01328.
2024-12-01-23:15:23-root-INFO: Loss too large (721.351->732.232)! Learning rate decreased to 0.01062.
2024-12-01-23:15:24-root-INFO: Loss too large (721.351->721.778)! Learning rate decreased to 0.00850.
2024-12-01-23:15:24-root-INFO: grad norm: 54.018 51.425 16.537
2024-12-01-23:15:24-root-INFO: Loss too large (717.087->718.024)! Learning rate decreased to 0.00680.
2024-12-01-23:15:25-root-INFO: grad norm: 44.726 42.451 14.083
2024-12-01-23:15:25-root-INFO: grad norm: 32.707 31.413 9.110
2024-12-01-23:15:26-root-INFO: grad norm: 29.182 27.622 9.411
2024-12-01-23:15:26-root-INFO: grad norm: 25.817 24.833 7.059
2024-12-01-23:15:27-root-INFO: grad norm: 24.224 22.990 7.632
2024-12-01-23:15:27-root-INFO: grad norm: 22.717 21.874 6.131
2024-12-01-23:15:27-root-INFO: Loss Change: 721.351 -> 696.281
2024-12-01-23:15:27-root-INFO: Regularization Change: 0.000 -> 1.202
2024-12-01-23:15:27-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-01-23:15:27-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-23:15:28-root-INFO: step: 147 lr_xt 0.01720013
2024-12-01-23:15:28-root-INFO: grad norm: 31.698 30.126 9.859
2024-12-01-23:15:28-root-INFO: Loss too large (695.712->700.948)! Learning rate decreased to 0.01376.
2024-12-01-23:15:28-root-INFO: grad norm: 68.742 65.266 21.582
2024-12-01-23:15:29-root-INFO: Loss too large (695.529->723.927)! Learning rate decreased to 0.01101.
2024-12-01-23:15:29-root-INFO: Loss too large (695.529->708.680)! Learning rate decreased to 0.00881.
2024-12-01-23:15:29-root-INFO: Loss too large (695.529->699.027)! Learning rate decreased to 0.00705.
2024-12-01-23:15:29-root-INFO: grad norm: 50.979 48.739 14.944
2024-12-01-23:15:30-root-INFO: grad norm: 28.413 27.438 7.379
2024-12-01-23:15:30-root-INFO: grad norm: 23.911 22.701 7.512
2024-12-01-23:15:31-root-INFO: grad norm: 20.854 20.133 5.436
2024-12-01-23:15:31-root-INFO: grad norm: 19.517 18.621 5.846
2024-12-01-23:15:32-root-INFO: grad norm: 18.561 17.913 4.863
2024-12-01-23:15:32-root-INFO: Loss Change: 695.712 -> 673.386
2024-12-01-23:15:32-root-INFO: Regularization Change: 0.000 -> 1.317
2024-12-01-23:15:32-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-01-23:15:32-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-23:15:32-root-INFO: step: 146 lr_xt 0.01782554
2024-12-01-23:15:32-root-INFO: grad norm: 27.190 25.994 7.975
2024-12-01-23:15:33-root-INFO: Loss too large (672.787->673.577)! Learning rate decreased to 0.01426.
2024-12-01-23:15:33-root-INFO: grad norm: 49.184 47.104 14.154
2024-12-01-23:15:33-root-INFO: Loss too large (670.848->682.869)! Learning rate decreased to 0.01141.
2024-12-01-23:15:33-root-INFO: Loss too large (670.848->674.944)! Learning rate decreased to 0.00913.
2024-12-01-23:15:34-root-INFO: grad norm: 47.698 45.816 13.266
2024-12-01-23:15:34-root-INFO: grad norm: 44.427 42.650 12.440
2024-12-01-23:15:35-root-INFO: grad norm: 42.958 41.195 12.182
2024-12-01-23:15:35-root-INFO: grad norm: 40.779 39.165 11.361
2024-12-01-23:15:36-root-INFO: grad norm: 39.849 38.178 11.419
2024-12-01-23:15:36-root-INFO: grad norm: 38.574 37.048 10.744
2024-12-01-23:15:37-root-INFO: Loss Change: 672.787 -> 652.527
2024-12-01-23:15:37-root-INFO: Regularization Change: 0.000 -> 1.765
2024-12-01-23:15:37-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-01-23:15:37-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-23:15:37-root-INFO: step: 145 lr_xt 0.01847071
2024-12-01-23:15:37-root-INFO: grad norm: 51.071 49.238 13.558
2024-12-01-23:15:37-root-INFO: Loss too large (655.331->686.246)! Learning rate decreased to 0.01478.
2024-12-01-23:15:37-root-INFO: Loss too large (655.331->663.932)! Learning rate decreased to 0.01182.
2024-12-01-23:15:38-root-INFO: grad norm: 67.317 64.854 18.041
2024-12-01-23:15:38-root-INFO: Loss too large (653.105->661.053)! Learning rate decreased to 0.00946.
2024-12-01-23:15:39-root-INFO: grad norm: 57.055 55.200 14.431
2024-12-01-23:15:39-root-INFO: grad norm: 41.934 40.751 9.890
2024-12-01-23:15:40-root-INFO: grad norm: 37.209 35.875 9.875
2024-12-01-23:15:40-root-INFO: grad norm: 33.042 32.093 7.863
2024-12-01-23:15:41-root-INFO: grad norm: 30.558 29.417 8.274
2024-12-01-23:15:41-root-INFO: grad norm: 28.361 27.530 6.816
2024-12-01-23:15:41-root-INFO: Loss Change: 655.331 -> 628.771
2024-12-01-23:15:41-root-INFO: Regularization Change: 0.000 -> 1.631
2024-12-01-23:15:41-root-INFO: Undo step: 145
2024-12-01-23:15:41-root-INFO: Undo step: 146
2024-12-01-23:15:41-root-INFO: Undo step: 147
2024-12-01-23:15:41-root-INFO: Undo step: 148
2024-12-01-23:15:41-root-INFO: Undo step: 149
2024-12-01-23:15:42-root-INFO: step: 150 lr_xt 0.01543756
2024-12-01-23:15:42-root-INFO: grad norm: 262.927 257.686 52.238
2024-12-01-23:15:42-root-INFO: grad norm: 175.169 171.058 37.725
2024-12-01-23:15:42-root-INFO: Loss too large (1017.468->1089.101)! Learning rate decreased to 0.01235.
2024-12-01-23:15:42-root-INFO: Loss too large (1017.468->1021.955)! Learning rate decreased to 0.00988.
2024-12-01-23:15:43-root-INFO: grad norm: 385.601 351.581 158.363
2024-12-01-23:15:43-root-INFO: Loss too large (934.972->1294.218)! Learning rate decreased to 0.00790.
2024-12-01-23:15:43-root-INFO: Loss too large (934.972->1124.597)! Learning rate decreased to 0.00632.
2024-12-01-23:15:43-root-INFO: Loss too large (934.972->1014.410)! Learning rate decreased to 0.00506.
2024-12-01-23:15:44-root-INFO: Loss too large (934.972->943.091)! Learning rate decreased to 0.00405.
2024-12-01-23:15:44-root-INFO: grad norm: 152.601 149.261 31.755
2024-12-01-23:15:44-root-INFO: grad norm: 98.139 95.271 23.554
2024-12-01-23:15:45-root-INFO: grad norm: 65.248 63.380 15.500
2024-12-01-23:15:45-root-INFO: grad norm: 53.370 51.856 12.619
2024-12-01-23:15:46-root-INFO: grad norm: 48.275 46.803 11.832
2024-12-01-23:15:46-root-INFO: Loss Change: 1242.534 -> 752.970
2024-12-01-23:15:46-root-INFO: Regularization Change: 0.000 -> 16.452
2024-12-01-23:15:46-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-01-23:15:46-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-23:15:46-root-INFO: step: 149 lr_xt 0.01600663
2024-12-01-23:15:47-root-INFO: grad norm: 49.824 48.130 12.881
2024-12-01-23:15:47-root-INFO: grad norm: 73.601 69.853 23.186
2024-12-01-23:15:47-root-INFO: Loss too large (730.724->763.375)! Learning rate decreased to 0.01281.
2024-12-01-23:15:47-root-INFO: Loss too large (730.724->744.816)! Learning rate decreased to 0.01024.
2024-12-01-23:15:48-root-INFO: Loss too large (730.724->733.237)! Learning rate decreased to 0.00820.
2024-12-01-23:15:48-root-INFO: grad norm: 67.049 64.911 16.796
2024-12-01-23:15:48-root-INFO: grad norm: 55.261 52.686 16.673
2024-12-01-23:15:49-root-INFO: grad norm: 52.011 50.139 13.830
2024-12-01-23:15:49-root-INFO: grad norm: 47.199 45.114 13.872
2024-12-01-23:15:50-root-INFO: grad norm: 45.336 43.607 12.404
2024-12-01-23:15:50-root-INFO: grad norm: 42.973 41.136 12.428
2024-12-01-23:15:51-root-INFO: Loss Change: 752.016 -> 683.899
2024-12-01-23:15:51-root-INFO: Regularization Change: 0.000 -> 4.869
2024-12-01-23:15:51-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-01-23:15:51-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-23:15:51-root-INFO: step: 148 lr_xt 0.01659399
2024-12-01-23:15:51-root-INFO: grad norm: 50.739 48.916 13.476
2024-12-01-23:15:51-root-INFO: Loss too large (684.543->702.280)! Learning rate decreased to 0.01328.
2024-12-01-23:15:51-root-INFO: Loss too large (684.543->685.637)! Learning rate decreased to 0.01062.
2024-12-01-23:15:52-root-INFO: grad norm: 60.978 58.856 15.945
2024-12-01-23:15:52-root-INFO: Loss too large (678.307->680.235)! Learning rate decreased to 0.00850.
2024-12-01-23:15:52-root-INFO: grad norm: 50.534 48.718 13.429
2024-12-01-23:15:53-root-INFO: grad norm: 36.439 35.182 9.490
2024-12-01-23:15:54-root-INFO: grad norm: 32.579 31.180 9.443
2024-12-01-23:15:54-root-INFO: grad norm: 28.961 27.907 7.742
2024-12-01-23:15:55-root-INFO: grad norm: 27.212 26.001 8.027
2024-12-01-23:15:56-root-INFO: grad norm: 25.555 24.607 6.896
2024-12-01-23:15:57-root-INFO: Loss Change: 684.543 -> 648.066
2024-12-01-23:15:57-root-INFO: Regularization Change: 0.000 -> 2.210
2024-12-01-23:15:57-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-01-23:15:57-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-23:15:57-root-INFO: step: 147 lr_xt 0.01720013
2024-12-01-23:15:57-root-INFO: grad norm: 31.316 30.052 8.808
2024-12-01-23:15:58-root-INFO: grad norm: 66.419 64.423 16.162
2024-12-01-23:15:59-root-INFO: Loss too large (646.912->683.728)! Learning rate decreased to 0.01376.
2024-12-01-23:15:59-root-INFO: Loss too large (646.912->663.368)! Learning rate decreased to 0.01101.
2024-12-01-23:15:59-root-INFO: Loss too large (646.912->650.854)! Learning rate decreased to 0.00881.
2024-12-01-23:16:00-root-INFO: grad norm: 51.797 50.136 13.013
2024-12-01-23:16:01-root-INFO: grad norm: 35.297 34.277 8.425
2024-12-01-23:16:02-root-INFO: grad norm: 30.984 29.765 8.605
2024-12-01-23:16:03-root-INFO: grad norm: 27.815 26.935 6.943
2024-12-01-23:16:03-root-INFO: grad norm: 26.251 25.209 7.322
2024-12-01-23:16:04-root-INFO: grad norm: 24.966 24.156 6.310
2024-12-01-23:16:05-root-INFO: Loss Change: 646.966 -> 621.161
2024-12-01-23:16:05-root-INFO: Regularization Change: 0.000 -> 1.964
2024-12-01-23:16:05-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-01-23:16:05-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-23:16:05-root-INFO: step: 146 lr_xt 0.01782554
2024-12-01-23:16:05-root-INFO: grad norm: 29.879 28.745 8.152
2024-12-01-23:16:06-root-INFO: Loss too large (620.421->622.658)! Learning rate decreased to 0.01426.
2024-12-01-23:16:07-root-INFO: grad norm: 48.118 46.850 10.976
2024-12-01-23:16:07-root-INFO: Loss too large (619.047->627.218)! Learning rate decreased to 0.01141.
2024-12-01-23:16:07-root-INFO: Loss too large (619.047->620.335)! Learning rate decreased to 0.00913.
2024-12-01-23:16:08-root-INFO: grad norm: 41.130 39.740 10.602
2024-12-01-23:16:09-root-INFO: grad norm: 34.904 33.965 8.043
2024-12-01-23:16:10-root-INFO: grad norm: 32.208 31.084 8.433
2024-12-01-23:16:11-root-INFO: grad norm: 29.658 28.826 6.974
2024-12-01-23:16:12-root-INFO: grad norm: 28.377 27.398 7.390
2024-12-01-23:16:13-root-INFO: grad norm: 27.077 26.301 6.435
2024-12-01-23:16:13-root-INFO: Loss Change: 620.421 -> 600.229
2024-12-01-23:16:13-root-INFO: Regularization Change: 0.000 -> 1.546
2024-12-01-23:16:13-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-01-23:16:13-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-23:16:13-root-INFO: step: 145 lr_xt 0.01847071
2024-12-01-23:16:14-root-INFO: grad norm: 32.979 31.744 8.942
2024-12-01-23:16:14-root-INFO: Loss too large (600.617->605.308)! Learning rate decreased to 0.01478.
2024-12-01-23:16:15-root-INFO: grad norm: 54.207 52.991 11.419
2024-12-01-23:16:15-root-INFO: Loss too large (600.090->611.628)! Learning rate decreased to 0.01182.
2024-12-01-23:16:16-root-INFO: Loss too large (600.090->602.999)! Learning rate decreased to 0.00946.
2024-12-01-23:16:17-root-INFO: grad norm: 44.439 43.069 10.948
2024-12-01-23:16:17-root-INFO: grad norm: 34.190 33.337 7.587
2024-12-01-23:16:18-root-INFO: grad norm: 31.161 30.122 7.980
2024-12-01-23:16:19-root-INFO: grad norm: 28.568 27.801 6.575
2024-12-01-23:16:20-root-INFO: grad norm: 27.450 26.541 7.004
2024-12-01-23:16:21-root-INFO: grad norm: 26.466 25.736 6.172
2024-12-01-23:16:21-root-INFO: Loss Change: 600.617 -> 581.290
2024-12-01-23:16:21-root-INFO: Regularization Change: 0.000 -> 1.469
2024-12-01-23:16:21-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-01-23:16:22-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-23:16:22-root-INFO: step: 144 lr_xt 0.01913614
2024-12-01-23:16:22-root-INFO: grad norm: 29.286 28.210 7.863
2024-12-01-23:16:22-root-INFO: Loss too large (580.671->585.900)! Learning rate decreased to 0.01531.
2024-12-01-23:16:23-root-INFO: Loss too large (580.671->581.029)! Learning rate decreased to 0.01225.
2024-12-01-23:16:23-root-INFO: grad norm: 37.960 37.091 8.077
2024-12-01-23:16:24-root-INFO: Loss too large (578.570->579.743)! Learning rate decreased to 0.00980.
2024-12-01-23:16:25-root-INFO: grad norm: 34.708 33.634 8.567
2024-12-01-23:16:26-root-INFO: grad norm: 30.970 30.195 6.886
2024-12-01-23:16:26-root-INFO: grad norm: 29.728 28.796 7.384
2024-12-01-23:16:27-root-INFO: grad norm: 28.368 27.630 6.426
2024-12-01-23:16:28-root-INFO: grad norm: 27.851 26.992 6.868
2024-12-01-23:16:29-root-INFO: grad norm: 27.304 26.583 6.230
2024-12-01-23:16:30-root-INFO: Loss Change: 580.671 -> 564.476
2024-12-01-23:16:30-root-INFO: Regularization Change: 0.000 -> 1.285
2024-12-01-23:16:30-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-01-23:16:30-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-23:16:30-root-INFO: step: 143 lr_xt 0.01982236
2024-12-01-23:16:30-root-INFO: grad norm: 29.192 28.212 7.502
2024-12-01-23:16:31-root-INFO: Loss too large (563.335->570.449)! Learning rate decreased to 0.01586.
2024-12-01-23:16:31-root-INFO: Loss too large (563.335->564.763)! Learning rate decreased to 0.01269.
2024-12-01-23:16:32-root-INFO: grad norm: 40.437 39.529 8.519
2024-12-01-23:16:32-root-INFO: Loss too large (561.800->564.225)! Learning rate decreased to 0.01015.
2024-12-01-23:16:33-root-INFO: grad norm: 36.984 35.915 8.832
2024-12-01-23:16:34-root-INFO: grad norm: 32.207 31.425 7.056
2024-12-01-23:16:35-root-INFO: grad norm: 31.115 30.201 7.489
2024-12-01-23:16:36-root-INFO: grad norm: 29.908 29.161 6.639
2024-12-01-23:16:36-root-INFO: grad norm: 29.526 28.671 7.052
2024-12-01-23:16:37-root-INFO: grad norm: 29.123 28.389 6.497
2024-12-01-23:16:38-root-INFO: Loss Change: 563.335 -> 548.589
2024-12-01-23:16:38-root-INFO: Regularization Change: 0.000 -> 1.243
2024-12-01-23:16:38-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-01-23:16:38-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-23:16:38-root-INFO: step: 142 lr_xt 0.02052986
2024-12-01-23:16:38-root-INFO: grad norm: 32.827 31.711 8.487
2024-12-01-23:16:39-root-INFO: Loss too large (548.196->557.834)! Learning rate decreased to 0.01642.
2024-12-01-23:16:39-root-INFO: Loss too large (548.196->550.337)! Learning rate decreased to 0.01314.
2024-12-01-23:16:40-root-INFO: grad norm: 44.546 43.587 9.192
2024-12-01-23:16:40-root-INFO: Loss too large (546.371->550.019)! Learning rate decreased to 0.01051.
2024-12-01-23:16:41-root-INFO: grad norm: 39.604 38.478 9.375
2024-12-01-23:16:42-root-INFO: grad norm: 32.429 31.647 7.078
2024-12-01-23:16:43-root-INFO: grad norm: 31.411 30.501 7.506
2024-12-01-23:16:43-root-INFO: grad norm: 30.540 29.788 6.733
2024-12-01-23:16:44-root-INFO: grad norm: 30.309 29.452 7.157
2024-12-01-23:16:45-root-INFO: grad norm: 30.146 29.401 6.662
2024-12-01-23:16:46-root-INFO: Loss Change: 548.196 -> 533.069
2024-12-01-23:16:46-root-INFO: Regularization Change: 0.000 -> 1.269
2024-12-01-23:16:46-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-01-23:16:46-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-23:16:46-root-INFO: step: 141 lr_xt 0.02125920
2024-12-01-23:16:46-root-INFO: grad norm: 29.635 28.733 7.257
2024-12-01-23:16:47-root-INFO: Loss too large (532.104->542.847)! Learning rate decreased to 0.01701.
2024-12-01-23:16:47-root-INFO: Loss too large (532.104->535.438)! Learning rate decreased to 0.01361.
2024-12-01-23:16:48-root-INFO: grad norm: 43.933 42.996 9.023
2024-12-01-23:16:48-root-INFO: Loss too large (531.445->535.617)! Learning rate decreased to 0.01088.
2024-12-01-23:16:49-root-INFO: grad norm: 39.073 38.025 8.988
2024-12-01-23:16:50-root-INFO: grad norm: 31.385 30.650 6.750
2024-12-01-23:16:51-root-INFO: grad norm: 30.497 29.665 7.077
2024-12-01-23:16:52-root-INFO: grad norm: 29.770 29.066 6.435
2024-12-01-23:16:53-root-INFO: grad norm: 29.609 28.816 6.806
2024-12-01-23:16:54-root-INFO: grad norm: 29.531 28.831 6.389
2024-12-01-23:16:54-root-INFO: Loss Change: 532.104 -> 518.985
2024-12-01-23:16:54-root-INFO: Regularization Change: 0.000 -> 1.188
2024-12-01-23:16:54-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-01-23:16:54-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-23:16:54-root-INFO: step: 140 lr_xt 0.02201089
2024-12-01-23:16:55-root-INFO: grad norm: 30.234 29.321 7.375
2024-12-01-23:16:55-root-INFO: Loss too large (518.433->529.786)! Learning rate decreased to 0.01761.
2024-12-01-23:16:55-root-INFO: Loss too large (518.433->521.929)! Learning rate decreased to 0.01409.
2024-12-01-23:16:56-root-INFO: grad norm: 44.333 43.413 8.986
2024-12-01-23:16:56-root-INFO: Loss too large (517.659->522.285)! Learning rate decreased to 0.01127.
2024-12-01-23:16:57-root-INFO: Loss too large (517.659->517.840)! Learning rate decreased to 0.00902.
2024-12-01-23:16:58-root-INFO: grad norm: 30.771 29.923 7.174
2024-12-01-23:16:59-root-INFO: grad norm: 18.866 18.355 4.362
2024-12-01-23:16:59-root-INFO: grad norm: 15.974 15.423 4.161
2024-12-01-23:17:00-root-INFO: grad norm: 13.980 13.515 3.575
2024-12-01-23:17:01-root-INFO: grad norm: 13.001 12.500 3.574
2024-12-01-23:17:02-root-INFO: grad norm: 12.327 11.869 3.331
2024-12-01-23:17:03-root-INFO: Loss Change: 518.433 -> 505.002
2024-12-01-23:17:03-root-INFO: Regularization Change: 0.000 -> 0.853
2024-12-01-23:17:03-root-INFO: Undo step: 140
2024-12-01-23:17:03-root-INFO: Undo step: 141
2024-12-01-23:17:03-root-INFO: Undo step: 142
2024-12-01-23:17:03-root-INFO: Undo step: 143
2024-12-01-23:17:03-root-INFO: Undo step: 144
2024-12-01-23:17:03-root-INFO: step: 145 lr_xt 0.01847071
2024-12-01-23:17:03-root-INFO: grad norm: 193.545 188.521 43.812
2024-12-01-23:17:04-root-INFO: grad norm: 194.059 188.203 47.313
2024-12-01-23:17:05-root-INFO: Loss too large (878.673->916.948)! Learning rate decreased to 0.01478.
2024-12-01-23:17:06-root-INFO: grad norm: 245.855 239.037 57.498
2024-12-01-23:17:06-root-INFO: Loss too large (750.394->928.213)! Learning rate decreased to 0.01182.
2024-12-01-23:17:06-root-INFO: Loss too large (750.394->807.854)! Learning rate decreased to 0.00946.
2024-12-01-23:17:07-root-INFO: grad norm: 138.369 136.406 23.227
2024-12-01-23:17:08-root-INFO: grad norm: 51.092 49.556 12.432
2024-12-01-23:17:09-root-INFO: grad norm: 36.744 35.417 9.786
2024-12-01-23:17:10-root-INFO: grad norm: 32.891 31.656 8.928
2024-12-01-23:17:10-root-INFO: grad norm: 30.234 29.125 8.116
2024-12-01-23:17:11-root-INFO: Loss Change: 960.216 -> 575.933
2024-12-01-23:17:11-root-INFO: Regularization Change: 0.000 -> 23.172
2024-12-01-23:17:11-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-01-23:17:11-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-23:17:11-root-INFO: step: 144 lr_xt 0.01913614
2024-12-01-23:17:12-root-INFO: grad norm: 33.260 32.123 8.621
2024-12-01-23:17:12-root-INFO: grad norm: 46.750 45.677 9.959
2024-12-01-23:17:13-root-INFO: Loss too large (566.376->580.312)! Learning rate decreased to 0.01531.
2024-12-01-23:17:13-root-INFO: Loss too large (566.376->567.337)! Learning rate decreased to 0.01225.
2024-12-01-23:17:14-root-INFO: grad norm: 55.265 54.101 11.282
2024-12-01-23:17:14-root-INFO: Loss too large (560.729->561.091)! Learning rate decreased to 0.00980.
2024-12-01-23:17:15-root-INFO: grad norm: 43.489 42.465 9.379
2024-12-01-23:17:16-root-INFO: grad norm: 30.074 29.259 6.954
2024-12-01-23:17:17-root-INFO: grad norm: 27.359 26.550 6.602
2024-12-01-23:17:17-root-INFO: grad norm: 25.247 24.506 6.075
2024-12-01-23:17:18-root-INFO: grad norm: 24.106 23.361 5.948
2024-12-01-23:17:19-root-INFO: Loss Change: 573.666 -> 535.494
2024-12-01-23:17:19-root-INFO: Regularization Change: 0.000 -> 3.355
2024-12-01-23:17:19-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-01-23:17:19-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-23:17:19-root-INFO: step: 143 lr_xt 0.01982236
2024-12-01-23:17:19-root-INFO: grad norm: 32.626 31.633 7.989
2024-12-01-23:17:20-root-INFO: Loss too large (535.051->539.502)! Learning rate decreased to 0.01586.
2024-12-01-23:17:20-root-INFO: Loss too large (535.051->535.192)! Learning rate decreased to 0.01269.
2024-12-01-23:17:21-root-INFO: grad norm: 37.479 36.656 7.810
2024-12-01-23:17:22-root-INFO: grad norm: 51.768 50.784 10.046
2024-12-01-23:17:22-root-INFO: Loss too large (531.664->535.275)! Learning rate decreased to 0.01015.
2024-12-01-23:17:23-root-INFO: grad norm: 43.932 43.014 8.937
2024-12-01-23:17:24-root-INFO: grad norm: 33.761 33.060 6.841
2024-12-01-23:17:25-root-INFO: grad norm: 31.280 30.560 6.674
2024-12-01-23:17:26-root-INFO: grad norm: 29.207 28.560 6.115
2024-12-01-23:17:26-root-INFO: grad norm: 28.139 27.457 6.157
2024-12-01-23:17:27-root-INFO: Loss Change: 535.051 -> 514.730
2024-12-01-23:17:27-root-INFO: Regularization Change: 0.000 -> 1.684
2024-12-01-23:17:27-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-01-23:17:27-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-23:17:27-root-INFO: step: 142 lr_xt 0.02052986
2024-12-01-23:17:28-root-INFO: grad norm: 41.085 39.980 9.467
2024-12-01-23:17:28-root-INFO: Loss too large (516.629->530.595)! Learning rate decreased to 0.01642.
2024-12-01-23:17:28-root-INFO: Loss too large (516.629->521.609)! Learning rate decreased to 0.01314.
2024-12-01-23:17:29-root-INFO: grad norm: 48.056 47.164 9.219
2024-12-01-23:17:29-root-INFO: Loss too large (516.136->516.872)! Learning rate decreased to 0.01051.
2024-12-01-23:17:30-root-INFO: grad norm: 42.159 41.405 7.938
2024-12-01-23:17:31-root-INFO: grad norm: 39.197 38.399 7.871
2024-12-01-23:17:32-root-INFO: grad norm: 36.240 35.550 7.037
2024-12-01-23:17:33-root-INFO: grad norm: 34.579 33.824 7.187
2024-12-01-23:17:34-root-INFO: grad norm: 33.016 32.349 6.602
2024-12-01-23:17:35-root-INFO: grad norm: 32.141 31.413 6.803
2024-12-01-23:17:35-root-INFO: Loss Change: 516.629 -> 498.748
2024-12-01-23:17:35-root-INFO: Regularization Change: 0.000 -> 1.368
2024-12-01-23:17:35-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-01-23:17:35-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-23:17:36-root-INFO: step: 141 lr_xt 0.02125920
2024-12-01-23:17:36-root-INFO: grad norm: 41.192 40.333 8.367
2024-12-01-23:17:36-root-INFO: Loss too large (499.641->517.896)! Learning rate decreased to 0.01701.
2024-12-01-23:17:36-root-INFO: Loss too large (499.641->508.379)! Learning rate decreased to 0.01361.
2024-12-01-23:17:37-root-INFO: Loss too large (499.641->502.016)! Learning rate decreased to 0.01088.
2024-12-01-23:17:38-root-INFO: grad norm: 38.140 37.351 7.720
2024-12-01-23:17:38-root-INFO: grad norm: 35.456 34.792 6.832
2024-12-01-23:17:39-root-INFO: grad norm: 34.152 33.409 7.082
2024-12-01-23:17:40-root-INFO: grad norm: 32.728 32.098 6.394
2024-12-01-23:17:41-root-INFO: grad norm: 31.941 31.229 6.706
2024-12-01-23:17:42-root-INFO: grad norm: 31.155 30.537 6.173
2024-12-01-23:17:43-root-INFO: grad norm: 30.719 30.025 6.492
2024-12-01-23:17:43-root-INFO: Loss Change: 499.641 -> 485.126
2024-12-01-23:17:43-root-INFO: Regularization Change: 0.000 -> 1.095
2024-12-01-23:17:43-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-01-23:17:43-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-23:17:44-root-INFO: step: 140 lr_xt 0.02201089
2024-12-01-23:17:44-root-INFO: grad norm: 42.148 41.227 8.761
2024-12-01-23:17:44-root-INFO: Loss too large (486.922->507.671)! Learning rate decreased to 0.01761.
2024-12-01-23:17:45-root-INFO: Loss too large (486.922->496.998)! Learning rate decreased to 0.01409.
2024-12-01-23:17:45-root-INFO: Loss too large (486.922->489.876)! Learning rate decreased to 0.01127.
2024-12-01-23:17:46-root-INFO: grad norm: 39.597 38.785 7.975
2024-12-01-23:17:47-root-INFO: grad norm: 38.163 37.478 7.200
2024-12-01-23:17:48-root-INFO: grad norm: 37.065 36.277 7.602
2024-12-01-23:17:49-root-INFO: grad norm: 35.664 35.008 6.805
2024-12-01-23:17:49-root-INFO: grad norm: 34.863 34.104 7.231
2024-12-01-23:17:50-root-INFO: grad norm: 33.936 33.294 6.569
2024-12-01-23:17:51-root-INFO: grad norm: 33.423 32.686 6.981
2024-12-01-23:17:52-root-INFO: Loss Change: 486.922 -> 473.469
2024-12-01-23:17:52-root-INFO: Regularization Change: 0.000 -> 1.044
2024-12-01-23:17:52-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-01-23:17:52-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-23:17:52-root-INFO: step: 139 lr_xt 0.02278550
2024-12-01-23:17:52-root-INFO: grad norm: 41.929 41.059 8.497
2024-12-01-23:17:53-root-INFO: Loss too large (475.475->497.506)! Learning rate decreased to 0.01823.
2024-12-01-23:17:53-root-INFO: Loss too large (475.475->486.020)! Learning rate decreased to 0.01458.
2024-12-01-23:17:53-root-INFO: Loss too large (475.475->478.462)! Learning rate decreased to 0.01167.
2024-12-01-23:17:54-root-INFO: grad norm: 39.111 38.341 7.726
2024-12-01-23:17:55-root-INFO: grad norm: 37.481 36.843 6.887
2024-12-01-23:17:56-root-INFO: grad norm: 36.214 35.463 7.333
2024-12-01-23:17:57-root-INFO: grad norm: 34.803 34.190 6.503
2024-12-01-23:17:58-root-INFO: grad norm: 33.932 33.208 6.972
2024-12-01-23:17:58-root-INFO: grad norm: 33.020 32.416 6.283
2024-12-01-23:17:59-root-INFO: grad norm: 32.482 31.776 6.735
2024-12-01-23:18:00-root-INFO: Loss Change: 475.475 -> 462.517
2024-12-01-23:18:00-root-INFO: Regularization Change: 0.000 -> 1.007
2024-12-01-23:18:00-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-01-23:18:00-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-23:18:00-root-INFO: step: 138 lr_xt 0.02358356
2024-12-01-23:18:01-root-INFO: grad norm: 40.669 39.889 7.931
2024-12-01-23:18:01-root-INFO: Loss too large (463.805->485.902)! Learning rate decreased to 0.01887.
2024-12-01-23:18:01-root-INFO: Loss too large (463.805->475.117)! Learning rate decreased to 0.01509.
2024-12-01-23:18:01-root-INFO: Loss too large (463.805->467.724)! Learning rate decreased to 0.01207.
2024-12-01-23:18:02-root-INFO: grad norm: 38.634 37.865 7.671
2024-12-01-23:18:03-root-INFO: grad norm: 36.521 35.895 6.733
2024-12-01-23:18:04-root-INFO: grad norm: 35.416 34.688 7.141
2024-12-01-23:18:05-root-INFO: grad norm: 34.111 33.514 6.353
2024-12-01-23:18:06-root-INFO: grad norm: 33.374 32.674 6.801
2024-12-01-23:18:07-root-INFO: grad norm: 32.586 32.001 6.148
2024-12-01-23:18:07-root-INFO: grad norm: 32.127 31.443 6.593
2024-12-01-23:18:08-root-INFO: Loss Change: 463.805 -> 452.179
2024-12-01-23:18:08-root-INFO: Regularization Change: 0.000 -> 0.953
2024-12-01-23:18:08-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-01-23:18:08-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-23:18:08-root-INFO: step: 137 lr_xt 0.02440563
2024-12-01-23:18:08-root-INFO: grad norm: 46.800 45.756 9.830
2024-12-01-23:18:09-root-INFO: Loss too large (455.913->483.571)! Learning rate decreased to 0.01952.
2024-12-01-23:18:09-root-INFO: Loss too large (455.913->469.067)! Learning rate decreased to 0.01562.
2024-12-01-23:18:09-root-INFO: Loss too large (455.913->459.404)! Learning rate decreased to 0.01250.
2024-12-01-23:18:10-root-INFO: grad norm: 42.046 41.244 8.175
2024-12-01-23:18:11-root-INFO: grad norm: 38.808 38.180 6.949
2024-12-01-23:18:12-root-INFO: grad norm: 36.828 36.113 7.223
2024-12-01-23:18:13-root-INFO: grad norm: 34.668 34.099 6.253
2024-12-01-23:18:13-root-INFO: grad norm: 33.354 32.685 6.646
2024-12-01-23:18:14-root-INFO: grad norm: 32.075 31.528 5.899
2024-12-01-23:18:15-root-INFO: grad norm: 31.285 30.641 6.316
2024-12-01-23:18:16-root-INFO: Loss Change: 455.913 -> 441.370
2024-12-01-23:18:16-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-01-23:18:16-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-01-23:18:16-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-23:18:16-root-INFO: step: 136 lr_xt 0.02525230
2024-12-01-23:18:16-root-INFO: grad norm: 37.882 37.171 7.306
2024-12-01-23:18:17-root-INFO: Loss too large (442.807->464.047)! Learning rate decreased to 0.02020.
2024-12-01-23:18:17-root-INFO: Loss too large (442.807->453.312)! Learning rate decreased to 0.01616.
2024-12-01-23:18:17-root-INFO: Loss too large (442.807->446.136)! Learning rate decreased to 0.01293.
2024-12-01-23:18:18-root-INFO: grad norm: 35.693 35.018 6.909
2024-12-01-23:18:19-root-INFO: grad norm: 33.550 32.994 6.081
2024-12-01-23:18:20-root-INFO: grad norm: 32.232 31.598 6.363
2024-12-01-23:18:21-root-INFO: grad norm: 30.834 30.312 5.648
2024-12-01-23:18:22-root-INFO: grad norm: 29.989 29.383 5.998
2024-12-01-23:18:22-root-INFO: grad norm: 29.167 28.660 5.417
2024-12-01-23:18:23-root-INFO: grad norm: 28.666 28.077 5.785
2024-12-01-23:18:24-root-INFO: Loss Change: 442.807 -> 431.799
2024-12-01-23:18:24-root-INFO: Regularization Change: 0.000 -> 0.936
2024-12-01-23:18:24-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-01-23:18:24-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-23:18:24-root-INFO: step: 135 lr_xt 0.02612413
2024-12-01-23:18:24-root-INFO: grad norm: 36.129 35.403 7.204
2024-12-01-23:18:25-root-INFO: Loss too large (433.018->453.154)! Learning rate decreased to 0.02090.
2024-12-01-23:18:25-root-INFO: Loss too large (433.018->442.431)! Learning rate decreased to 0.01672.
2024-12-01-23:18:25-root-INFO: Loss too large (433.018->435.548)! Learning rate decreased to 0.01338.
2024-12-01-23:18:26-root-INFO: grad norm: 34.086 33.472 6.443
2024-12-01-23:18:27-root-INFO: grad norm: 32.882 32.367 5.794
2024-12-01-23:18:28-root-INFO: grad norm: 31.881 31.294 6.087
2024-12-01-23:18:29-root-INFO: grad norm: 30.780 30.294 5.450
2024-12-01-23:18:30-root-INFO: grad norm: 29.980 29.416 5.787
2024-12-01-23:18:31-root-INFO: grad norm: 29.168 28.694 5.233
2024-12-01-23:18:31-root-INFO: grad norm: 28.599 28.051 5.572
2024-12-01-23:18:32-root-INFO: Loss Change: 433.018 -> 422.358
2024-12-01-23:18:32-root-INFO: Regularization Change: 0.000 -> 0.953
2024-12-01-23:18:32-root-INFO: Undo step: 135
2024-12-01-23:18:32-root-INFO: Undo step: 136
2024-12-01-23:18:32-root-INFO: Undo step: 137
2024-12-01-23:18:32-root-INFO: Undo step: 138
2024-12-01-23:18:32-root-INFO: Undo step: 139
2024-12-01-23:18:32-root-INFO: step: 140 lr_xt 0.02201089
2024-12-01-23:18:33-root-INFO: grad norm: 161.136 156.970 36.402
2024-12-01-23:18:34-root-INFO: grad norm: 128.937 126.303 25.930
2024-12-01-23:18:34-root-INFO: grad norm: 142.648 138.755 33.096
2024-12-01-23:18:35-root-INFO: Loss too large (638.552->720.736)! Learning rate decreased to 0.01761.
2024-12-01-23:18:35-root-INFO: Loss too large (638.552->648.559)! Learning rate decreased to 0.01409.
2024-12-01-23:18:36-root-INFO: grad norm: 113.103 110.764 22.883
2024-12-01-23:18:37-root-INFO: grad norm: 105.497 101.108 30.114
2024-12-01-23:18:37-root-INFO: Loss too large (544.062->555.986)! Learning rate decreased to 0.01127.
2024-12-01-23:18:38-root-INFO: grad norm: 73.105 71.333 15.995
2024-12-01-23:18:39-root-INFO: grad norm: 33.071 31.747 9.264
2024-12-01-23:18:40-root-INFO: grad norm: 27.420 26.311 7.720
2024-12-01-23:18:40-root-INFO: Loss Change: 859.056 -> 492.444
2024-12-01-23:18:40-root-INFO: Regularization Change: 0.000 -> 26.564
2024-12-01-23:18:40-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-01-23:18:40-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-23:18:41-root-INFO: step: 139 lr_xt 0.02278550
2024-12-01-23:18:41-root-INFO: grad norm: 28.816 27.843 7.427
2024-12-01-23:18:42-root-INFO: grad norm: 46.523 45.661 8.913
2024-12-01-23:18:42-root-INFO: Loss too large (488.472->510.807)! Learning rate decreased to 0.01823.
2024-12-01-23:18:42-root-INFO: Loss too large (488.472->494.952)! Learning rate decreased to 0.01458.
2024-12-01-23:18:43-root-INFO: grad norm: 55.763 54.944 9.522
2024-12-01-23:18:44-root-INFO: Loss too large (486.013->487.203)! Learning rate decreased to 0.01167.
2024-12-01-23:18:44-root-INFO: grad norm: 43.774 42.995 8.222
2024-12-01-23:18:45-root-INFO: grad norm: 33.351 32.748 6.317
2024-12-01-23:18:46-root-INFO: grad norm: 29.279 28.612 6.214
2024-12-01-23:18:47-root-INFO: grad norm: 26.321 25.764 5.387
2024-12-01-23:18:48-root-INFO: grad norm: 24.602 23.976 5.515
2024-12-01-23:18:48-root-INFO: Loss Change: 492.388 -> 462.220
2024-12-01-23:18:48-root-INFO: Regularization Change: 0.000 -> 3.172
2024-12-01-23:18:48-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-01-23:18:48-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-23:18:49-root-INFO: step: 138 lr_xt 0.02358356
2024-12-01-23:18:49-root-INFO: grad norm: 31.907 31.235 6.511
2024-12-01-23:18:49-root-INFO: Loss too large (462.567->472.246)! Learning rate decreased to 0.01887.
2024-12-01-23:18:50-root-INFO: Loss too large (462.567->465.861)! Learning rate decreased to 0.01509.
2024-12-01-23:18:50-root-INFO: grad norm: 39.465 38.749 7.482
2024-12-01-23:18:51-root-INFO: Loss too large (461.997->462.373)! Learning rate decreased to 0.01207.
2024-12-01-23:18:52-root-INFO: grad norm: 35.017 34.453 6.261
2024-12-01-23:18:52-root-INFO: grad norm: 32.431 31.808 6.331
2024-12-01-23:18:53-root-INFO: grad norm: 30.098 29.594 5.486
2024-12-01-23:18:54-root-INFO: grad norm: 28.619 28.032 5.767
2024-12-01-23:18:55-root-INFO: grad norm: 27.342 26.862 5.102
2024-12-01-23:18:56-root-INFO: grad norm: 26.526 25.959 5.452
2024-12-01-23:18:56-root-INFO: Loss Change: 462.567 -> 446.096
2024-12-01-23:18:56-root-INFO: Regularization Change: 0.000 -> 1.546
2024-12-01-23:18:56-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-01-23:18:56-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-23:18:57-root-INFO: step: 137 lr_xt 0.02440563
2024-12-01-23:18:57-root-INFO: grad norm: 42.282 41.366 8.750
2024-12-01-23:18:57-root-INFO: Loss too large (449.371->470.727)! Learning rate decreased to 0.01952.
2024-12-01-23:18:58-root-INFO: Loss too large (449.371->458.415)! Learning rate decreased to 0.01562.
2024-12-01-23:18:58-root-INFO: Loss too large (449.371->450.584)! Learning rate decreased to 0.01250.
2024-12-01-23:18:59-root-INFO: grad norm: 37.846 37.177 7.082
2024-12-01-23:19:00-root-INFO: grad norm: 35.518 34.984 6.137
2024-12-01-23:19:00-root-INFO: grad norm: 33.692 33.099 6.296
2024-12-01-23:19:01-root-INFO: grad norm: 31.858 31.382 5.484
2024-12-01-23:19:02-root-INFO: grad norm: 30.504 29.942 5.827
2024-12-01-23:19:03-root-INFO: grad norm: 29.227 28.774 5.122
2024-12-01-23:19:04-root-INFO: grad norm: 28.334 27.793 5.512
2024-12-01-23:19:04-root-INFO: Loss Change: 449.371 -> 433.155
2024-12-01-23:19:04-root-INFO: Regularization Change: 0.000 -> 1.289
2024-12-01-23:19:04-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-01-23:19:04-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-23:19:05-root-INFO: step: 136 lr_xt 0.02525230
2024-12-01-23:19:05-root-INFO: grad norm: 35.117 34.529 6.400
2024-12-01-23:19:05-root-INFO: Loss too large (434.291->452.459)! Learning rate decreased to 0.02020.
2024-12-01-23:19:06-root-INFO: Loss too large (434.291->442.624)! Learning rate decreased to 0.01616.
2024-12-01-23:19:06-root-INFO: Loss too large (434.291->436.334)! Learning rate decreased to 0.01293.
2024-12-01-23:19:07-root-INFO: grad norm: 33.303 32.748 6.055
2024-12-01-23:19:08-root-INFO: grad norm: 31.841 31.391 5.333
2024-12-01-23:19:08-root-INFO: grad norm: 30.725 30.196 5.674
2024-12-01-23:19:09-root-INFO: grad norm: 29.616 29.193 4.986
2024-12-01-23:19:10-root-INFO: grad norm: 28.802 28.291 5.399
2024-12-01-23:19:11-root-INFO: grad norm: 28.064 27.654 4.781
2024-12-01-23:19:12-root-INFO: grad norm: 27.544 27.045 5.220
2024-12-01-23:19:13-root-INFO: Loss Change: 434.291 -> 422.415
2024-12-01-23:19:13-root-INFO: Regularization Change: 0.000 -> 1.068
2024-12-01-23:19:13-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-01-23:19:13-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-23:19:13-root-INFO: step: 135 lr_xt 0.02612413
2024-12-01-23:19:13-root-INFO: grad norm: 36.583 35.970 6.671
2024-12-01-23:19:14-root-INFO: Loss too large (423.907->445.419)! Learning rate decreased to 0.02090.
2024-12-01-23:19:14-root-INFO: Loss too large (423.907->433.757)! Learning rate decreased to 0.01672.
2024-12-01-23:19:14-root-INFO: Loss too large (423.907->426.390)! Learning rate decreased to 0.01338.
2024-12-01-23:19:15-root-INFO: grad norm: 35.074 34.553 6.024
2024-12-01-23:19:16-root-INFO: grad norm: 34.248 33.828 5.350
2024-12-01-23:19:17-root-INFO: grad norm: 33.353 32.851 5.765
2024-12-01-23:19:18-root-INFO: grad norm: 32.377 31.980 5.054
2024-12-01-23:19:19-root-INFO: grad norm: 31.496 31.009 5.517
2024-12-01-23:19:19-root-INFO: grad norm: 30.633 30.248 4.842
2024-12-01-23:19:20-root-INFO: grad norm: 29.936 29.462 5.307
2024-12-01-23:19:21-root-INFO: Loss Change: 423.907 -> 412.700
2024-12-01-23:19:21-root-INFO: Regularization Change: 0.000 -> 1.029
2024-12-01-23:19:21-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-01-23:19:21-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-23:19:21-root-INFO: step: 134 lr_xt 0.02702170
2024-12-01-23:19:22-root-INFO: grad norm: 39.707 39.136 6.714
2024-12-01-23:19:22-root-INFO: Loss too large (414.720->441.224)! Learning rate decreased to 0.02162.
2024-12-01-23:19:22-root-INFO: Loss too large (414.720->427.087)! Learning rate decreased to 0.01729.
2024-12-01-23:19:22-root-INFO: Loss too large (414.720->418.017)! Learning rate decreased to 0.01384.
2024-12-01-23:19:23-root-INFO: grad norm: 37.108 36.601 6.119
2024-12-01-23:19:24-root-INFO: grad norm: 35.331 34.930 5.312
2024-12-01-23:19:25-root-INFO: grad norm: 33.671 33.177 5.746
2024-12-01-23:19:26-root-INFO: grad norm: 32.132 31.753 4.924
2024-12-01-23:19:27-root-INFO: grad norm: 30.873 30.397 5.398
2024-12-01-23:19:28-root-INFO: grad norm: 29.748 29.381 4.658
2024-12-01-23:19:29-root-INFO: grad norm: 28.889 28.429 5.137
2024-12-01-23:19:29-root-INFO: Loss Change: 414.720 -> 402.950
2024-12-01-23:19:29-root-INFO: Regularization Change: 0.000 -> 0.998
2024-12-01-23:19:29-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-01-23:19:29-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-23:19:29-root-INFO: step: 133 lr_xt 0.02794561
2024-12-01-23:19:30-root-INFO: grad norm: 38.701 38.101 6.786
2024-12-01-23:19:30-root-INFO: Loss too large (405.329->431.985)! Learning rate decreased to 0.02236.
2024-12-01-23:19:30-root-INFO: Loss too large (405.329->417.888)! Learning rate decreased to 0.01789.
2024-12-01-23:19:31-root-INFO: Loss too large (405.329->408.828)! Learning rate decreased to 0.01431.
2024-12-01-23:19:31-root-INFO: grad norm: 36.838 36.362 5.902
2024-12-01-23:19:32-root-INFO: grad norm: 35.447 35.051 5.281
2024-12-01-23:19:33-root-INFO: grad norm: 34.013 33.548 5.605
2024-12-01-23:19:34-root-INFO: grad norm: 32.585 32.219 4.874
2024-12-01-23:19:35-root-INFO: grad norm: 31.347 30.899 5.283
2024-12-01-23:19:36-root-INFO: grad norm: 30.224 29.872 4.593
2024-12-01-23:19:37-root-INFO: grad norm: 29.324 28.889 5.029
2024-12-01-23:19:37-root-INFO: Loss Change: 405.329 -> 394.277
2024-12-01-23:19:37-root-INFO: Regularization Change: 0.000 -> 0.985
2024-12-01-23:19:37-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-01-23:19:37-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-23:19:38-root-INFO: step: 132 lr_xt 0.02889645
2024-12-01-23:19:38-root-INFO: grad norm: 35.581 35.108 5.782
2024-12-01-23:19:38-root-INFO: Loss too large (395.657->419.530)! Learning rate decreased to 0.02312.
2024-12-01-23:19:39-root-INFO: Loss too large (395.657->406.927)! Learning rate decreased to 0.01849.
2024-12-01-23:19:39-root-INFO: Loss too large (395.657->398.865)! Learning rate decreased to 0.01479.
2024-12-01-23:19:40-root-INFO: grad norm: 33.998 33.541 5.561
2024-12-01-23:19:41-root-INFO: grad norm: 32.749 32.392 4.823
2024-12-01-23:19:42-root-INFO: grad norm: 31.511 31.063 5.298
2024-12-01-23:19:42-root-INFO: grad norm: 30.294 29.957 4.509
2024-12-01-23:19:43-root-INFO: grad norm: 29.250 28.818 5.007
2024-12-01-23:19:44-root-INFO: grad norm: 28.287 27.962 4.276
2024-12-01-23:19:45-root-INFO: grad norm: 27.515 27.098 4.773
2024-12-01-23:19:46-root-INFO: Loss Change: 395.657 -> 385.568
2024-12-01-23:19:46-root-INFO: Regularization Change: 0.000 -> 0.958
2024-12-01-23:19:46-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-01-23:19:46-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-23:19:46-root-INFO: step: 131 lr_xt 0.02987484
2024-12-01-23:19:46-root-INFO: grad norm: 36.672 36.108 6.406
2024-12-01-23:19:46-root-INFO: Loss too large (388.202->413.195)! Learning rate decreased to 0.02390.
2024-12-01-23:19:47-root-INFO: Loss too large (388.202->399.755)! Learning rate decreased to 0.01912.
2024-12-01-23:19:47-root-INFO: Loss too large (388.202->391.179)! Learning rate decreased to 0.01530.
2024-12-01-23:19:48-root-INFO: grad norm: 34.303 33.863 5.481
2024-12-01-23:19:49-root-INFO: grad norm: 32.359 32.012 4.724
2024-12-01-23:19:50-root-INFO: grad norm: 30.529 30.113 5.023
2024-12-01-23:19:51-root-INFO: grad norm: 28.826 28.516 4.212
2024-12-01-23:19:51-root-INFO: grad norm: 27.399 27.007 4.617
2024-12-01-23:19:52-root-INFO: grad norm: 26.157 25.866 3.888
2024-12-01-23:19:53-root-INFO: grad norm: 25.158 24.783 4.324
2024-12-01-23:19:54-root-INFO: Loss Change: 388.202 -> 377.049
2024-12-01-23:19:54-root-INFO: Regularization Change: 0.000 -> 1.005
2024-12-01-23:19:54-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-01-23:19:54-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-23:19:54-root-INFO: step: 130 lr_xt 0.03088137
2024-12-01-23:19:54-root-INFO: grad norm: 31.936 31.511 5.190
2024-12-01-23:19:55-root-INFO: Loss too large (378.539->399.245)! Learning rate decreased to 0.02471.
2024-12-01-23:19:55-root-INFO: Loss too large (378.539->388.151)! Learning rate decreased to 0.01976.
2024-12-01-23:19:55-root-INFO: Loss too large (378.539->381.159)! Learning rate decreased to 0.01581.
2024-12-01-23:19:56-root-INFO: grad norm: 30.679 30.272 4.981
2024-12-01-23:19:57-root-INFO: grad norm: 29.760 29.445 4.316
2024-12-01-23:19:58-root-INFO: grad norm: 28.788 28.391 4.764
2024-12-01-23:19:58-root-INFO: grad norm: 27.803 27.506 4.057
2024-12-01-23:19:59-root-INFO: grad norm: 26.917 26.536 4.512
2024-12-01-23:20:00-root-INFO: grad norm: 26.081 25.795 3.852
2024-12-01-23:20:01-root-INFO: grad norm: 25.377 25.010 4.299
2024-12-01-23:20:02-root-INFO: Loss Change: 378.539 -> 369.341
2024-12-01-23:20:02-root-INFO: Regularization Change: 0.000 -> 0.961
2024-12-01-23:20:02-root-INFO: Undo step: 130
2024-12-01-23:20:02-root-INFO: Undo step: 131
2024-12-01-23:20:02-root-INFO: Undo step: 132
2024-12-01-23:20:02-root-INFO: Undo step: 133
2024-12-01-23:20:02-root-INFO: Undo step: 134
2024-12-01-23:20:02-root-INFO: step: 135 lr_xt 0.02612413
2024-12-01-23:20:02-root-INFO: grad norm: 182.695 178.622 38.363
2024-12-01-23:20:03-root-INFO: grad norm: 112.811 110.599 22.229
2024-12-01-23:20:04-root-INFO: grad norm: 79.688 77.664 17.845
2024-12-01-23:20:05-root-INFO: grad norm: 79.941 78.082 17.139
2024-12-01-23:20:06-root-INFO: grad norm: 84.855 83.336 15.986
2024-12-01-23:20:06-root-INFO: Loss too large (481.799->487.378)! Learning rate decreased to 0.02090.
2024-12-01-23:20:07-root-INFO: grad norm: 66.188 64.846 13.258
2024-12-01-23:20:07-root-INFO: grad norm: 54.340 53.570 9.115
2024-12-01-23:20:08-root-INFO: grad norm: 50.401 49.536 9.293
2024-12-01-23:20:09-root-INFO: Loss Change: 800.855 -> 424.914
2024-12-01-23:20:09-root-INFO: Regularization Change: 0.000 -> 40.105
2024-12-01-23:20:09-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-01-23:20:09-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-23:20:09-root-INFO: step: 134 lr_xt 0.02702170
2024-12-01-23:20:09-root-INFO: grad norm: 55.588 54.755 9.588
2024-12-01-23:20:10-root-INFO: Loss too large (428.316->438.313)! Learning rate decreased to 0.02162.
2024-12-01-23:20:11-root-INFO: grad norm: 51.464 50.715 8.747
2024-12-01-23:20:12-root-INFO: grad norm: 47.254 46.700 7.212
2024-12-01-23:20:12-root-INFO: grad norm: 44.682 44.070 7.367
2024-12-01-23:20:13-root-INFO: grad norm: 42.839 42.373 6.303
2024-12-01-23:20:14-root-INFO: grad norm: 42.215 41.686 6.665
2024-12-01-23:20:15-root-INFO: grad norm: 41.884 41.464 5.911
2024-12-01-23:20:16-root-INFO: grad norm: 42.000 41.521 6.329
2024-12-01-23:20:16-root-INFO: Loss Change: 428.316 -> 394.811
2024-12-01-23:20:16-root-INFO: Regularization Change: 0.000 -> 4.671
2024-12-01-23:20:16-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-01-23:20:16-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-23:20:17-root-INFO: step: 133 lr_xt 0.02794561
2024-12-01-23:20:17-root-INFO: grad norm: 52.191 51.474 8.619
2024-12-01-23:20:17-root-INFO: Loss too large (399.254->412.906)! Learning rate decreased to 0.02236.
2024-12-01-23:20:18-root-INFO: grad norm: 50.934 50.428 7.157
2024-12-01-23:20:19-root-INFO: grad norm: 48.200 47.740 6.639
2024-12-01-23:20:20-root-INFO: grad norm: 45.916 45.443 6.579
2024-12-01-23:20:21-root-INFO: grad norm: 44.526 44.114 6.040
2024-12-01-23:20:22-root-INFO: grad norm: 44.910 44.426 6.576
2024-12-01-23:20:22-root-INFO: grad norm: 46.324 45.863 6.515
2024-12-01-23:20:23-root-INFO: grad norm: 49.905 49.265 7.969
2024-12-01-23:20:24-root-INFO: Loss too large (382.951->384.419)! Learning rate decreased to 0.01789.
2024-12-01-23:20:24-root-INFO: Loss Change: 399.254 -> 375.147
2024-12-01-23:20:24-root-INFO: Regularization Change: 0.000 -> 2.978
2024-12-01-23:20:24-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-01-23:20:24-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-23:20:24-root-INFO: step: 132 lr_xt 0.02889645
2024-12-01-23:20:25-root-INFO: grad norm: 42.961 42.352 7.208
2024-12-01-23:20:25-root-INFO: Loss too large (377.756->391.296)! Learning rate decreased to 0.02312.
2024-12-01-23:20:26-root-INFO: grad norm: 48.454 47.768 8.125
2024-12-01-23:20:26-root-INFO: Loss too large (377.714->380.598)! Learning rate decreased to 0.01849.
2024-12-01-23:20:27-root-INFO: grad norm: 36.381 35.881 6.012
2024-12-01-23:20:28-root-INFO: grad norm: 28.087 27.641 4.987
2024-12-01-23:20:29-root-INFO: grad norm: 23.350 22.978 4.152
2024-12-01-23:20:30-root-INFO: grad norm: 20.089 19.743 3.715
2024-12-01-23:20:30-root-INFO: grad norm: 17.995 17.687 3.316
2024-12-01-23:20:31-root-INFO: grad norm: 16.492 16.192 3.132
2024-12-01-23:20:32-root-INFO: Loss Change: 377.756 -> 357.216
2024-12-01-23:20:32-root-INFO: Regularization Change: 0.000 -> 1.820
2024-12-01-23:20:32-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-01-23:20:32-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-23:20:32-root-INFO: step: 131 lr_xt 0.02987484
2024-12-01-23:20:32-root-INFO: grad norm: 25.717 25.158 5.330
2024-12-01-23:20:33-root-INFO: Loss too large (359.018->363.715)! Learning rate decreased to 0.02390.
2024-12-01-23:20:34-root-INFO: grad norm: 32.771 32.445 4.614
2024-12-01-23:20:34-root-INFO: Loss too large (358.913->361.897)! Learning rate decreased to 0.01912.
2024-12-01-23:20:35-root-INFO: grad norm: 29.540 29.254 4.097
2024-12-01-23:20:36-root-INFO: grad norm: 26.733 26.469 3.744
2024-12-01-23:20:37-root-INFO: grad norm: 24.707 24.480 3.344
2024-12-01-23:20:37-root-INFO: grad norm: 23.029 22.787 3.332
2024-12-01-23:20:38-root-INFO: grad norm: 21.864 21.663 2.963
2024-12-01-23:20:39-root-INFO: grad norm: 20.948 20.719 3.090
2024-12-01-23:20:40-root-INFO: Loss Change: 359.018 -> 347.736
2024-12-01-23:20:40-root-INFO: Regularization Change: 0.000 -> 1.561
2024-12-01-23:20:40-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-01-23:20:40-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-23:20:40-root-INFO: step: 130 lr_xt 0.03088137
2024-12-01-23:20:40-root-INFO: grad norm: 27.143 26.796 4.325
2024-12-01-23:20:41-root-INFO: Loss too large (349.146->357.214)! Learning rate decreased to 0.02471.
2024-12-01-23:20:41-root-INFO: Loss too large (349.146->350.816)! Learning rate decreased to 0.01976.
2024-12-01-23:20:42-root-INFO: grad norm: 25.729 25.510 3.351
2024-12-01-23:20:43-root-INFO: grad norm: 24.909 24.712 3.127
2024-12-01-23:20:44-root-INFO: grad norm: 24.166 23.963 3.123
2024-12-01-23:20:45-root-INFO: grad norm: 23.582 23.406 2.874
2024-12-01-23:20:45-root-INFO: grad norm: 23.064 22.868 3.002
2024-12-01-23:20:46-root-INFO: grad norm: 22.693 22.526 2.745
2024-12-01-23:20:47-root-INFO: grad norm: 22.391 22.198 2.928
2024-12-01-23:20:48-root-INFO: Loss Change: 349.146 -> 339.400
2024-12-01-23:20:48-root-INFO: Regularization Change: 0.000 -> 1.341
2024-12-01-23:20:48-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-01-23:20:48-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-23:20:48-root-INFO: step: 129 lr_xt 0.03191668
2024-12-01-23:20:48-root-INFO: grad norm: 33.356 32.898 5.506
2024-12-01-23:20:49-root-INFO: Loss too large (342.422->355.793)! Learning rate decreased to 0.02553.
2024-12-01-23:20:49-root-INFO: Loss too large (342.422->345.837)! Learning rate decreased to 0.02043.
2024-12-01-23:20:50-root-INFO: grad norm: 31.320 31.104 3.674
2024-12-01-23:20:51-root-INFO: grad norm: 29.590 29.375 3.562
2024-12-01-23:20:52-root-INFO: grad norm: 27.778 27.573 3.368
2024-12-01-23:20:53-root-INFO: grad norm: 26.367 26.185 3.096
2024-12-01-23:20:53-root-INFO: grad norm: 25.100 24.903 3.142
2024-12-01-23:20:54-root-INFO: grad norm: 24.183 24.013 2.860
2024-12-01-23:20:55-root-INFO: grad norm: 23.414 23.222 2.994
2024-12-01-23:20:56-root-INFO: Loss Change: 342.422 -> 331.112
2024-12-01-23:20:56-root-INFO: Regularization Change: 0.000 -> 1.342
2024-12-01-23:20:56-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-01-23:20:56-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-23:20:56-root-INFO: step: 128 lr_xt 0.03298138
2024-12-01-23:20:56-root-INFO: grad norm: 32.842 32.416 5.275
2024-12-01-23:20:57-root-INFO: Loss too large (333.637->347.422)! Learning rate decreased to 0.02639.
2024-12-01-23:20:57-root-INFO: Loss too large (333.637->337.306)! Learning rate decreased to 0.02111.
2024-12-01-23:20:58-root-INFO: grad norm: 30.891 30.673 3.660
2024-12-01-23:20:59-root-INFO: grad norm: 29.176 28.965 3.508
2024-12-01-23:21:00-root-INFO: grad norm: 27.447 27.242 3.347
2024-12-01-23:21:00-root-INFO: grad norm: 26.112 25.930 3.080
2024-12-01-23:21:01-root-INFO: grad norm: 24.946 24.749 3.129
2024-12-01-23:21:02-root-INFO: grad norm: 24.100 23.930 2.864
2024-12-01-23:21:03-root-INFO: grad norm: 23.406 23.214 2.991
2024-12-01-23:21:04-root-INFO: Loss Change: 333.637 -> 322.878
2024-12-01-23:21:04-root-INFO: Regularization Change: 0.000 -> 1.305
2024-12-01-23:21:04-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-01-23:21:04-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-23:21:04-root-INFO: step: 127 lr_xt 0.03407612
2024-12-01-23:21:04-root-INFO: grad norm: 29.581 29.240 4.484
2024-12-01-23:21:04-root-INFO: Loss too large (324.897->337.070)! Learning rate decreased to 0.02726.
2024-12-01-23:21:05-root-INFO: Loss too large (324.897->328.291)! Learning rate decreased to 0.02181.
2024-12-01-23:21:06-root-INFO: grad norm: 28.606 28.410 3.345
2024-12-01-23:21:06-root-INFO: grad norm: 27.716 27.516 3.323
2024-12-01-23:21:07-root-INFO: grad norm: 26.729 26.535 3.215
2024-12-01-23:21:08-root-INFO: grad norm: 25.925 25.744 3.057
2024-12-01-23:21:09-root-INFO: grad norm: 25.189 24.998 3.095
2024-12-01-23:21:10-root-INFO: grad norm: 24.645 24.472 2.921
2024-12-01-23:21:11-root-INFO: grad norm: 24.189 24.000 3.018
2024-12-01-23:21:11-root-INFO: Loss Change: 324.897 -> 315.939
2024-12-01-23:21:11-root-INFO: Regularization Change: 0.000 -> 1.268
2024-12-01-23:21:11-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-01-23:21:11-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-23:21:12-root-INFO: step: 126 lr_xt 0.03520152
2024-12-01-23:21:12-root-INFO: grad norm: 29.678 29.363 4.310
2024-12-01-23:21:12-root-INFO: Loss too large (317.603->331.028)! Learning rate decreased to 0.02816.
2024-12-01-23:21:12-root-INFO: Loss too large (317.603->321.643)! Learning rate decreased to 0.02253.
2024-12-01-23:21:13-root-INFO: grad norm: 29.190 28.991 3.404
2024-12-01-23:21:14-root-INFO: grad norm: 28.686 28.484 3.403
2024-12-01-23:21:15-root-INFO: grad norm: 28.057 27.859 3.329
2024-12-01-23:21:16-root-INFO: grad norm: 27.515 27.324 3.234
2024-12-01-23:21:17-root-INFO: grad norm: 26.984 26.786 3.264
2024-12-01-23:21:18-root-INFO: grad norm: 26.584 26.397 3.154
2024-12-01-23:21:19-root-INFO: grad norm: 26.232 26.033 3.227
2024-12-01-23:21:19-root-INFO: Loss Change: 317.603 -> 309.564
2024-12-01-23:21:19-root-INFO: Regularization Change: 0.000 -> 1.245
2024-12-01-23:21:19-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-01-23:21:19-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-23:21:19-root-INFO: step: 125 lr_xt 0.03635823
2024-12-01-23:21:20-root-INFO: grad norm: 34.398 33.986 5.310
2024-12-01-23:21:20-root-INFO: Loss too large (312.342->330.462)! Learning rate decreased to 0.02909.
2024-12-01-23:21:20-root-INFO: Loss too large (312.342->317.785)! Learning rate decreased to 0.02327.
2024-12-01-23:21:21-root-INFO: grad norm: 32.739 32.511 3.855
2024-12-01-23:21:22-root-INFO: grad norm: 31.231 30.992 3.850
2024-12-01-23:21:23-root-INFO: grad norm: 29.718 29.493 3.651
2024-12-01-23:21:24-root-INFO: grad norm: 28.542 28.326 3.500
2024-12-01-23:21:25-root-INFO: grad norm: 27.534 27.313 3.477
2024-12-01-23:21:25-root-INFO: grad norm: 26.772 26.566 3.316
2024-12-01-23:21:26-root-INFO: grad norm: 26.151 25.935 3.359
2024-12-01-23:21:27-root-INFO: Loss Change: 312.342 -> 302.320
2024-12-01-23:21:27-root-INFO: Regularization Change: 0.000 -> 1.280
2024-12-01-23:21:27-root-INFO: Undo step: 125
2024-12-01-23:21:27-root-INFO: Undo step: 126
2024-12-01-23:21:27-root-INFO: Undo step: 127
2024-12-01-23:21:27-root-INFO: Undo step: 128
2024-12-01-23:21:27-root-INFO: Undo step: 129
2024-12-01-23:21:27-root-INFO: step: 130 lr_xt 0.03088137
2024-12-01-23:21:27-root-INFO: grad norm: 123.647 121.508 22.900
2024-12-01-23:21:28-root-INFO: grad norm: 80.989 79.714 14.314
2024-12-01-23:21:29-root-INFO: grad norm: 60.259 58.990 12.304
2024-12-01-23:21:30-root-INFO: grad norm: 52.441 51.716 8.687
2024-12-01-23:21:31-root-INFO: grad norm: 61.295 60.556 9.490
2024-12-01-23:21:31-root-INFO: Loss too large (394.909->404.086)! Learning rate decreased to 0.02471.
2024-12-01-23:21:32-root-INFO: grad norm: 55.465 54.863 8.151
2024-12-01-23:21:33-root-INFO: grad norm: 50.465 49.870 7.727
2024-12-01-23:21:34-root-INFO: grad norm: 47.255 46.798 6.556
2024-12-01-23:21:35-root-INFO: Loss Change: 713.794 -> 359.989
2024-12-01-23:21:35-root-INFO: Regularization Change: 0.000 -> 43.620
2024-12-01-23:21:35-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-01-23:21:35-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-23:21:35-root-INFO: step: 129 lr_xt 0.03191668
2024-12-01-23:21:35-root-INFO: grad norm: 37.976 37.617 5.204
2024-12-01-23:21:35-root-INFO: Loss too large (354.391->359.866)! Learning rate decreased to 0.02553.
2024-12-01-23:21:36-root-INFO: grad norm: 39.039 38.651 5.487
2024-12-01-23:21:37-root-INFO: grad norm: 41.708 41.314 5.718
2024-12-01-23:21:38-root-INFO: grad norm: 44.533 44.147 5.846
2024-12-01-23:21:39-root-INFO: grad norm: 47.327 46.894 6.392
2024-12-01-23:21:40-root-INFO: grad norm: 49.043 48.619 6.433
2024-12-01-23:21:41-root-INFO: grad norm: 50.063 49.587 6.884
2024-12-01-23:21:42-root-INFO: grad norm: 50.032 49.592 6.621
2024-12-01-23:21:42-root-INFO: Loss Change: 354.391 -> 338.107
2024-12-01-23:21:42-root-INFO: Regularization Change: 0.000 -> 4.879
2024-12-01-23:21:42-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-01-23:21:42-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-23:21:43-root-INFO: step: 128 lr_xt 0.03298138
2024-12-01-23:21:43-root-INFO: grad norm: 43.825 43.502 5.314
2024-12-01-23:21:43-root-INFO: Loss too large (333.707->346.609)! Learning rate decreased to 0.02639.
2024-12-01-23:21:44-root-INFO: grad norm: 44.293 43.899 5.896
2024-12-01-23:21:45-root-INFO: grad norm: 44.739 44.355 5.852
2024-12-01-23:21:46-root-INFO: grad norm: 45.537 45.163 5.821
2024-12-01-23:21:47-root-INFO: grad norm: 45.732 45.334 6.026
2024-12-01-23:21:48-root-INFO: grad norm: 45.867 45.501 5.785
2024-12-01-23:21:49-root-INFO: grad norm: 45.540 45.133 6.076
2024-12-01-23:21:49-root-INFO: grad norm: 45.251 44.889 5.710
2024-12-01-23:21:50-root-INFO: Loss Change: 333.707 -> 321.168
2024-12-01-23:21:50-root-INFO: Regularization Change: 0.000 -> 3.030
2024-12-01-23:21:50-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-01-23:21:50-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-23:21:50-root-INFO: step: 127 lr_xt 0.03407612
2024-12-01-23:21:51-root-INFO: grad norm: 41.869 41.578 4.931
2024-12-01-23:21:51-root-INFO: Loss too large (318.980->331.587)! Learning rate decreased to 0.02726.
2024-12-01-23:21:52-root-INFO: grad norm: 42.504 42.165 5.358
2024-12-01-23:21:53-root-INFO: grad norm: 43.007 42.660 5.455
2024-12-01-23:21:54-root-INFO: grad norm: 43.656 43.322 5.389
2024-12-01-23:21:54-root-INFO: grad norm: 43.865 43.501 5.644
2024-12-01-23:21:55-root-INFO: grad norm: 43.986 43.649 5.434
2024-12-01-23:21:56-root-INFO: grad norm: 43.859 43.483 5.736
2024-12-01-23:21:57-root-INFO: grad norm: 43.743 43.402 5.454
2024-12-01-23:21:58-root-INFO: Loss Change: 318.980 -> 310.150
2024-12-01-23:21:58-root-INFO: Regularization Change: 0.000 -> 2.339
2024-12-01-23:21:58-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-01-23:21:58-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-23:21:58-root-INFO: step: 126 lr_xt 0.03520152
2024-12-01-23:21:58-root-INFO: grad norm: 40.130 39.871 4.554
2024-12-01-23:21:59-root-INFO: Loss too large (307.796->319.858)! Learning rate decreased to 0.02816.
2024-12-01-23:21:59-root-INFO: grad norm: 40.808 40.506 4.956
2024-12-01-23:22:00-root-INFO: grad norm: 41.414 41.096 5.118
2024-12-01-23:22:01-root-INFO: grad norm: 42.007 41.698 5.086
2024-12-01-23:22:02-root-INFO: grad norm: 42.326 41.989 5.334
2024-12-01-23:22:03-root-INFO: grad norm: 42.594 42.275 5.204
2024-12-01-23:22:04-root-INFO: grad norm: 42.707 42.355 5.470
2024-12-01-23:22:05-root-INFO: grad norm: 42.845 42.516 5.299
2024-12-01-23:22:05-root-INFO: Loss Change: 307.796 -> 301.216
2024-12-01-23:22:05-root-INFO: Regularization Change: 0.000 -> 1.991
2024-12-01-23:22:05-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-01-23:22:05-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-23:22:06-root-INFO: step: 125 lr_xt 0.03635823
2024-12-01-23:22:06-root-INFO: grad norm: 37.636 37.418 4.047
2024-12-01-23:22:06-root-INFO: Loss too large (297.488->308.723)! Learning rate decreased to 0.02909.
2024-12-01-23:22:07-root-INFO: grad norm: 38.815 38.521 4.775
2024-12-01-23:22:08-root-INFO: Loss too large (296.441->296.757)! Learning rate decreased to 0.02327.
2024-12-01-23:22:08-root-INFO: grad norm: 25.948 25.713 3.482
2024-12-01-23:22:09-root-INFO: grad norm: 18.186 18.014 2.497
2024-12-01-23:22:10-root-INFO: grad norm: 13.916 13.732 2.255
2024-12-01-23:22:11-root-INFO: grad norm: 11.235 11.079 1.869
2024-12-01-23:22:12-root-INFO: grad norm: 9.579 9.396 1.866
2024-12-01-23:22:13-root-INFO: grad norm: 8.483 8.317 1.672
2024-12-01-23:22:13-root-INFO: Loss Change: 297.488 -> 280.369
2024-12-01-23:22:13-root-INFO: Regularization Change: 0.000 -> 1.528
2024-12-01-23:22:13-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-01-23:22:13-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-23:22:14-root-INFO: step: 124 lr_xt 0.03773645
2024-12-01-23:22:14-root-INFO: grad norm: 14.308 13.766 3.903
2024-12-01-23:22:15-root-INFO: grad norm: 18.257 17.944 3.363
2024-12-01-23:22:15-root-INFO: Loss too large (280.443->283.371)! Learning rate decreased to 0.03019.
2024-12-01-23:22:16-root-INFO: grad norm: 22.320 22.053 3.446
2024-12-01-23:22:16-root-INFO: Loss too large (280.219->281.625)! Learning rate decreased to 0.02415.
2024-12-01-23:22:17-root-INFO: grad norm: 19.414 19.183 2.988
2024-12-01-23:22:18-root-INFO: grad norm: 17.162 16.969 2.567
2024-12-01-23:22:19-root-INFO: grad norm: 15.720 15.521 2.498
2024-12-01-23:22:20-root-INFO: grad norm: 14.526 14.355 2.225
2024-12-01-23:22:21-root-INFO: grad norm: 13.665 13.480 2.241
2024-12-01-23:22:21-root-INFO: Loss Change: 281.286 -> 273.810
2024-12-01-23:22:21-root-INFO: Regularization Change: 0.000 -> 1.485
2024-12-01-23:22:21-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-01-23:22:21-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-23:22:21-root-INFO: step: 123 lr_xt 0.03896235
2024-12-01-23:22:22-root-INFO: grad norm: 20.055 19.727 3.610
2024-12-01-23:22:22-root-INFO: Loss too large (275.160->280.638)! Learning rate decreased to 0.03117.
2024-12-01-23:22:22-root-INFO: Loss too large (275.160->276.353)! Learning rate decreased to 0.02494.
2024-12-01-23:22:23-root-INFO: grad norm: 18.636 18.456 2.585
2024-12-01-23:22:24-root-INFO: grad norm: 17.668 17.483 2.548
2024-12-01-23:22:25-root-INFO: grad norm: 16.900 16.725 2.422
2024-12-01-23:22:26-root-INFO: grad norm: 16.222 16.056 2.313
2024-12-01-23:22:26-root-INFO: grad norm: 15.684 15.515 2.295
2024-12-01-23:22:27-root-INFO: grad norm: 15.226 15.069 2.182
2024-12-01-23:22:28-root-INFO: grad norm: 14.871 14.707 2.207
2024-12-01-23:22:29-root-INFO: Loss Change: 275.160 -> 268.333
2024-12-01-23:22:29-root-INFO: Regularization Change: 0.000 -> 1.117
2024-12-01-23:22:29-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-01-23:22:29-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-23:22:29-root-INFO: step: 122 lr_xt 0.04022160
2024-12-01-23:22:29-root-INFO: grad norm: 22.419 22.065 3.966
2024-12-01-23:22:30-root-INFO: Loss too large (270.204->278.011)! Learning rate decreased to 0.03218.
2024-12-01-23:22:30-root-INFO: Loss too large (270.204->272.310)! Learning rate decreased to 0.02574.
2024-12-01-23:22:31-root-INFO: grad norm: 21.104 20.926 2.738
2024-12-01-23:22:32-root-INFO: grad norm: 20.160 19.967 2.784
2024-12-01-23:22:33-root-INFO: grad norm: 19.356 19.180 2.603
2024-12-01-23:22:33-root-INFO: grad norm: 18.606 18.432 2.538
2024-12-01-23:22:34-root-INFO: grad norm: 17.997 17.827 2.473
2024-12-01-23:22:35-root-INFO: grad norm: 17.458 17.293 2.391
2024-12-01-23:22:36-root-INFO: grad norm: 17.031 16.864 2.375
2024-12-01-23:22:37-root-INFO: Loss Change: 270.204 -> 263.422
2024-12-01-23:22:37-root-INFO: Regularization Change: 0.000 -> 1.094
2024-12-01-23:22:37-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-01-23:22:37-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-23:22:37-root-INFO: step: 121 lr_xt 0.04151486
2024-12-01-23:22:37-root-INFO: grad norm: 24.057 23.736 3.917
2024-12-01-23:22:38-root-INFO: Loss too large (265.205->275.112)! Learning rate decreased to 0.03321.
2024-12-01-23:22:38-root-INFO: Loss too large (265.205->268.183)! Learning rate decreased to 0.02657.
2024-12-01-23:22:39-root-INFO: grad norm: 22.905 22.724 2.877
2024-12-01-23:22:40-root-INFO: grad norm: 21.981 21.787 2.912
2024-12-01-23:22:41-root-INFO: grad norm: 21.186 21.007 2.742
2024-12-01-23:22:41-root-INFO: grad norm: 20.450 20.271 2.696
2024-12-01-23:22:42-root-INFO: grad norm: 19.853 19.680 2.616
2024-12-01-23:22:43-root-INFO: grad norm: 19.326 19.155 2.564
2024-12-01-23:22:44-root-INFO: grad norm: 18.911 18.742 2.523
2024-12-01-23:22:45-root-INFO: Loss Change: 265.205 -> 258.583
2024-12-01-23:22:45-root-INFO: Regularization Change: 0.000 -> 1.074
2024-12-01-23:22:45-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-01-23:22:45-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-23:22:45-root-INFO: step: 120 lr_xt 0.04284282
2024-12-01-23:22:45-root-INFO: grad norm: 26.092 25.755 4.181
2024-12-01-23:22:45-root-INFO: Loss too large (261.025->273.401)! Learning rate decreased to 0.03427.
2024-12-01-23:22:46-root-INFO: Loss too large (261.025->265.000)! Learning rate decreased to 0.02742.
2024-12-01-23:22:47-root-INFO: grad norm: 24.871 24.688 3.011
2024-12-01-23:22:47-root-INFO: grad norm: 23.789 23.585 3.110
2024-12-01-23:22:48-root-INFO: grad norm: 22.887 22.706 2.875
2024-12-01-23:22:49-root-INFO: grad norm: 22.036 21.850 2.862
2024-12-01-23:22:50-root-INFO: grad norm: 21.364 21.188 2.735
2024-12-01-23:22:51-root-INFO: grad norm: 20.759 20.581 2.709
2024-12-01-23:22:52-root-INFO: grad norm: 20.293 20.122 2.630
2024-12-01-23:22:52-root-INFO: Loss Change: 261.025 -> 254.195
2024-12-01-23:22:52-root-INFO: Regularization Change: 0.000 -> 1.073
2024-12-01-23:22:52-root-INFO: Undo step: 120
2024-12-01-23:22:52-root-INFO: Undo step: 121
2024-12-01-23:22:52-root-INFO: Undo step: 122
2024-12-01-23:22:52-root-INFO: Undo step: 123
2024-12-01-23:22:52-root-INFO: Undo step: 124
2024-12-01-23:22:53-root-INFO: step: 125 lr_xt 0.03635823
2024-12-01-23:22:53-root-INFO: grad norm: 108.601 106.820 19.584
2024-12-01-23:22:54-root-INFO: grad norm: 78.792 77.542 13.977
2024-12-01-23:22:55-root-INFO: grad norm: 73.087 71.832 13.488
2024-12-01-23:22:56-root-INFO: grad norm: 71.583 70.627 11.660
2024-12-01-23:22:56-root-INFO: Loss too large (369.339->370.454)! Learning rate decreased to 0.02909.
2024-12-01-23:22:57-root-INFO: grad norm: 52.960 52.000 10.041
2024-12-01-23:22:58-root-INFO: grad norm: 42.364 41.760 7.127
2024-12-01-23:22:58-root-INFO: grad norm: 38.743 38.074 7.168
2024-12-01-23:22:59-root-INFO: grad norm: 36.525 36.036 5.952
2024-12-01-23:23:00-root-INFO: Loss Change: 603.776 -> 305.034
2024-12-01-23:23:00-root-INFO: Regularization Change: 0.000 -> 40.856
2024-12-01-23:23:00-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-01-23:23:00-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-23:23:00-root-INFO: step: 124 lr_xt 0.03773645
2024-12-01-23:23:01-root-INFO: grad norm: 32.218 31.690 5.809
2024-12-01-23:23:01-root-INFO: Loss too large (302.010->303.093)! Learning rate decreased to 0.03019.
2024-12-01-23:23:02-root-INFO: grad norm: 31.988 31.586 5.051
2024-12-01-23:23:02-root-INFO: grad norm: 32.447 32.001 5.359
2024-12-01-23:23:03-root-INFO: grad norm: 32.767 32.416 4.783
2024-12-01-23:23:04-root-INFO: Loss too large (290.398->290.645)! Learning rate decreased to 0.02415.
2024-12-01-23:23:04-root-INFO: grad norm: 24.530 24.180 4.130
2024-12-01-23:23:05-root-INFO: grad norm: 18.479 18.224 3.059
2024-12-01-23:23:06-root-INFO: grad norm: 15.663 15.381 2.960
2024-12-01-23:23:07-root-INFO: grad norm: 13.585 13.355 2.492
2024-12-01-23:23:08-root-INFO: Loss Change: 302.010 -> 275.652
2024-12-01-23:23:08-root-INFO: Regularization Change: 0.000 -> 3.992
2024-12-01-23:23:08-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-01-23:23:08-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-23:23:08-root-INFO: step: 123 lr_xt 0.03896235
2024-12-01-23:23:08-root-INFO: grad norm: 11.459 11.114 2.789
2024-12-01-23:23:09-root-INFO: grad norm: 15.610 15.371 2.718
2024-12-01-23:23:09-root-INFO: Loss too large (273.495->274.501)! Learning rate decreased to 0.03117.
2024-12-01-23:23:10-root-INFO: grad norm: 18.276 17.997 3.181
2024-12-01-23:23:11-root-INFO: grad norm: 22.828 22.560 3.486
2024-12-01-23:23:11-root-INFO: Loss too large (272.089->273.119)! Learning rate decreased to 0.02494.
2024-12-01-23:23:12-root-INFO: grad norm: 19.333 19.057 3.251
2024-12-01-23:23:13-root-INFO: grad norm: 16.141 15.930 2.602
2024-12-01-23:23:14-root-INFO: grad norm: 14.669 14.442 2.571
2024-12-01-23:23:15-root-INFO: grad norm: 13.409 13.224 2.224
2024-12-01-23:23:15-root-INFO: Loss Change: 275.191 -> 264.577
2024-12-01-23:23:15-root-INFO: Regularization Change: 0.000 -> 2.457
2024-12-01-23:23:15-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-01-23:23:15-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-23:23:16-root-INFO: step: 122 lr_xt 0.04022160
2024-12-01-23:23:16-root-INFO: grad norm: 11.465 11.124 2.776
2024-12-01-23:23:17-root-INFO: grad norm: 16.701 16.498 2.597
2024-12-01-23:23:17-root-INFO: Loss too large (263.477->266.019)! Learning rate decreased to 0.03218.
2024-12-01-23:23:17-root-INFO: Loss too large (263.477->263.527)! Learning rate decreased to 0.02574.
2024-12-01-23:23:18-root-INFO: grad norm: 14.449 14.230 2.503
2024-12-01-23:23:19-root-INFO: grad norm: 12.832 12.653 2.139
2024-12-01-23:23:20-root-INFO: grad norm: 11.965 11.767 2.169
2024-12-01-23:23:21-root-INFO: grad norm: 11.271 11.103 1.938
2024-12-01-23:23:22-root-INFO: grad norm: 10.892 10.706 2.005
2024-12-01-23:23:23-root-INFO: grad norm: 10.611 10.453 1.829
2024-12-01-23:23:23-root-INFO: Loss Change: 264.302 -> 256.336
2024-12-01-23:23:23-root-INFO: Regularization Change: 0.000 -> 1.714
2024-12-01-23:23:23-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-01-23:23:23-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-23:23:24-root-INFO: step: 121 lr_xt 0.04151486
2024-12-01-23:23:24-root-INFO: grad norm: 9.958 9.621 2.567
2024-12-01-23:23:25-root-INFO: grad norm: 14.314 14.132 2.275
2024-12-01-23:23:25-root-INFO: Loss too large (255.297->257.304)! Learning rate decreased to 0.03321.
2024-12-01-23:23:25-root-INFO: Loss too large (255.297->255.364)! Learning rate decreased to 0.02657.
2024-12-01-23:23:26-root-INFO: grad norm: 12.880 12.677 2.274
2024-12-01-23:23:27-root-INFO: grad norm: 11.925 11.762 1.967
2024-12-01-23:23:28-root-INFO: grad norm: 11.336 11.147 2.057
2024-12-01-23:23:29-root-INFO: grad norm: 10.839 10.681 1.844
2024-12-01-23:23:30-root-INFO: grad norm: 10.544 10.364 1.939
2024-12-01-23:23:31-root-INFO: grad norm: 10.321 10.168 1.773
2024-12-01-23:23:31-root-INFO: Loss Change: 256.081 -> 249.554
2024-12-01-23:23:31-root-INFO: Regularization Change: 0.000 -> 1.483
2024-12-01-23:23:31-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-01-23:23:31-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-23:23:32-root-INFO: step: 120 lr_xt 0.04284282
2024-12-01-23:23:32-root-INFO: grad norm: 12.100 11.768 2.816
2024-12-01-23:23:32-root-INFO: Loss too large (249.972->250.554)! Learning rate decreased to 0.03427.
2024-12-01-23:23:33-root-INFO: grad norm: 15.019 14.864 2.155
2024-12-01-23:23:33-root-INFO: Loss too large (249.331->250.036)! Learning rate decreased to 0.02742.
2024-12-01-23:23:34-root-INFO: grad norm: 14.504 14.321 2.297
2024-12-01-23:23:35-root-INFO: grad norm: 14.184 14.043 1.995
2024-12-01-23:23:36-root-INFO: grad norm: 14.028 13.864 2.138
2024-12-01-23:23:37-root-INFO: grad norm: 13.931 13.797 1.922
2024-12-01-23:23:38-root-INFO: grad norm: 13.946 13.794 2.057
2024-12-01-23:23:39-root-INFO: grad norm: 14.018 13.892 1.881
2024-12-01-23:23:39-root-INFO: Loss Change: 249.972 -> 244.660
2024-12-01-23:23:39-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-01-23:23:39-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-01-23:23:39-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-23:23:40-root-INFO: step: 119 lr_xt 0.04420613
2024-12-01-23:23:40-root-INFO: grad norm: 21.594 21.212 4.047
2024-12-01-23:23:40-root-INFO: Loss too large (246.298->254.876)! Learning rate decreased to 0.03536.
2024-12-01-23:23:41-root-INFO: Loss too large (246.298->248.669)! Learning rate decreased to 0.02829.
2024-12-01-23:23:42-root-INFO: grad norm: 21.542 21.414 2.342
2024-12-01-23:23:43-root-INFO: grad norm: 22.157 21.984 2.757
2024-12-01-23:23:43-root-INFO: grad norm: 22.787 22.657 2.435
2024-12-01-23:23:44-root-INFO: grad norm: 23.359 23.201 2.713
2024-12-01-23:23:45-root-INFO: grad norm: 23.827 23.693 2.532
2024-12-01-23:23:46-root-INFO: grad norm: 24.233 24.078 2.737
2024-12-01-23:23:47-root-INFO: grad norm: 24.483 24.344 2.602
2024-12-01-23:23:47-root-INFO: Loss Change: 246.298 -> 242.111
2024-12-01-23:23:47-root-INFO: Regularization Change: 0.000 -> 1.201
2024-12-01-23:23:48-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-01-23:23:48-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-23:23:48-root-INFO: step: 118 lr_xt 0.04560549
2024-12-01-23:23:48-root-INFO: grad norm: 28.614 28.366 3.758
2024-12-01-23:23:48-root-INFO: Loss too large (244.208->261.825)! Learning rate decreased to 0.03648.
2024-12-01-23:23:49-root-INFO: Loss too large (244.208->250.360)! Learning rate decreased to 0.02919.
2024-12-01-23:23:50-root-INFO: grad norm: 28.406 28.265 2.828
2024-12-01-23:23:50-root-INFO: grad norm: 27.930 27.756 3.112
2024-12-01-23:23:51-root-INFO: grad norm: 27.255 27.113 2.782
2024-12-01-23:23:52-root-INFO: grad norm: 26.528 26.366 2.929
2024-12-01-23:23:53-root-INFO: grad norm: 25.821 25.682 2.679
2024-12-01-23:23:54-root-INFO: grad norm: 25.149 24.995 2.782
2024-12-01-23:23:55-root-INFO: grad norm: 24.558 24.422 2.578
2024-12-01-23:23:55-root-INFO: Loss Change: 244.208 -> 237.918
2024-12-01-23:23:55-root-INFO: Regularization Change: 0.000 -> 1.095
2024-12-01-23:23:55-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-01-23:23:55-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-23:23:55-root-INFO: step: 117 lr_xt 0.04704158
2024-12-01-23:23:56-root-INFO: grad norm: 33.343 32.966 4.999
2024-12-01-23:23:56-root-INFO: Loss too large (242.034->264.823)! Learning rate decreased to 0.03763.
2024-12-01-23:23:56-root-INFO: Loss too large (242.034->250.030)! Learning rate decreased to 0.03011.
2024-12-01-23:23:57-root-INFO: grad norm: 31.807 31.650 3.156
2024-12-01-23:23:58-root-INFO: grad norm: 30.031 29.834 3.439
2024-12-01-23:23:59-root-INFO: grad norm: 28.508 28.357 2.929
2024-12-01-23:24:00-root-INFO: grad norm: 27.062 26.891 3.031
2024-12-01-23:24:00-root-INFO: grad norm: 25.950 25.807 2.720
2024-12-01-23:24:01-root-INFO: grad norm: 24.974 24.818 2.791
2024-12-01-23:24:02-root-INFO: grad norm: 24.238 24.101 2.576
2024-12-01-23:24:03-root-INFO: Loss Change: 242.034 -> 233.322
2024-12-01-23:24:03-root-INFO: Regularization Change: 0.000 -> 1.134
2024-12-01-23:24:03-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-01-23:24:03-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-23:24:03-root-INFO: step: 116 lr_xt 0.04851508
2024-12-01-23:24:03-root-INFO: grad norm: 29.829 29.543 4.119
2024-12-01-23:24:04-root-INFO: Loss too large (235.978->254.951)! Learning rate decreased to 0.03881.
2024-12-01-23:24:04-root-INFO: Loss too large (235.978->242.476)! Learning rate decreased to 0.03105.
2024-12-01-23:24:05-root-INFO: grad norm: 28.327 28.174 2.945
2024-12-01-23:24:06-root-INFO: grad norm: 26.799 26.622 3.073
2024-12-01-23:24:07-root-INFO: grad norm: 25.618 25.471 2.750
2024-12-01-23:24:07-root-INFO: grad norm: 24.637 24.476 2.811
2024-12-01-23:24:08-root-INFO: grad norm: 23.972 23.824 2.654
2024-12-01-23:24:09-root-INFO: grad norm: 23.579 23.421 2.726
2024-12-01-23:24:10-root-INFO: grad norm: 23.417 23.264 2.681
2024-12-01-23:24:11-root-INFO: Loss Change: 235.978 -> 228.959
2024-12-01-23:24:11-root-INFO: Regularization Change: 0.000 -> 1.092
2024-12-01-23:24:11-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-01-23:24:11-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-23:24:11-root-INFO: step: 115 lr_xt 0.05002669
2024-12-01-23:24:11-root-INFO: grad norm: 30.549 30.234 4.377
2024-12-01-23:24:11-root-INFO: Loss too large (232.253->253.516)! Learning rate decreased to 0.04002.
2024-12-01-23:24:12-root-INFO: Loss too large (232.253->240.102)! Learning rate decreased to 0.03202.
2024-12-01-23:24:13-root-INFO: grad norm: 29.651 29.464 3.324
2024-12-01-23:24:13-root-INFO: grad norm: 28.956 28.737 3.549
2024-12-01-23:24:14-root-INFO: grad norm: 28.344 28.145 3.357
2024-12-01-23:24:15-root-INFO: grad norm: 27.903 27.688 3.459
2024-12-01-23:24:15-root-INFO: Loss too large (228.771->228.848)! Learning rate decreased to 0.02561.
2024-12-01-23:24:16-root-INFO: grad norm: 17.965 17.815 2.312
2024-12-01-23:24:17-root-INFO: grad norm: 11.691 11.566 1.703
2024-12-01-23:24:18-root-INFO: grad norm: 8.508 8.396 1.377
2024-12-01-23:24:19-root-INFO: Loss Change: 232.253 -> 221.430
2024-12-01-23:24:19-root-INFO: Regularization Change: 0.000 -> 1.031
2024-12-01-23:24:19-root-INFO: Undo step: 115
2024-12-01-23:24:19-root-INFO: Undo step: 116
2024-12-01-23:24:19-root-INFO: Undo step: 117
2024-12-01-23:24:19-root-INFO: Undo step: 118
2024-12-01-23:24:19-root-INFO: Undo step: 119
2024-12-01-23:24:19-root-INFO: step: 120 lr_xt 0.04284282
2024-12-01-23:24:19-root-INFO: grad norm: 94.210 92.024 20.176
2024-12-01-23:24:20-root-INFO: grad norm: 56.989 56.145 9.769
2024-12-01-23:24:21-root-INFO: grad norm: 47.227 46.519 8.150
2024-12-01-23:24:22-root-INFO: grad norm: 44.201 43.843 5.611
2024-12-01-23:24:23-root-INFO: grad norm: 49.423 49.000 6.452
2024-12-01-23:24:23-root-INFO: Loss too large (299.600->300.610)! Learning rate decreased to 0.03427.
2024-12-01-23:24:24-root-INFO: grad norm: 39.046 38.788 4.483
2024-12-01-23:24:25-root-INFO: grad norm: 35.249 34.995 4.221
2024-12-01-23:24:26-root-INFO: grad norm: 33.244 33.028 3.785
2024-12-01-23:24:26-root-INFO: Loss Change: 531.335 -> 265.499
2024-12-01-23:24:26-root-INFO: Regularization Change: 0.000 -> 45.698
2024-12-01-23:24:26-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-01-23:24:26-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-23:24:27-root-INFO: step: 119 lr_xt 0.04420613
2024-12-01-23:24:27-root-INFO: grad norm: 34.380 34.068 4.621
2024-12-01-23:24:27-root-INFO: Loss too large (266.066->273.695)! Learning rate decreased to 0.03536.
2024-12-01-23:24:28-root-INFO: grad norm: 32.034 31.827 3.631
2024-12-01-23:24:29-root-INFO: grad norm: 29.274 29.069 3.461
2024-12-01-23:24:30-root-INFO: grad norm: 29.369 29.166 3.449
2024-12-01-23:24:31-root-INFO: grad norm: 29.737 29.516 3.613
2024-12-01-23:24:32-root-INFO: grad norm: 31.253 31.029 3.732
2024-12-01-23:24:32-root-INFO: grad norm: 32.561 32.318 3.977
2024-12-01-23:24:33-root-INFO: grad norm: 34.382 34.134 4.122
2024-12-01-23:24:34-root-INFO: Loss too large (251.948->252.201)! Learning rate decreased to 0.02829.
2024-12-01-23:24:34-root-INFO: Loss Change: 266.066 -> 245.565
2024-12-01-23:24:34-root-INFO: Regularization Change: 0.000 -> 4.514
2024-12-01-23:24:34-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-01-23:24:34-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-23:24:34-root-INFO: step: 118 lr_xt 0.04560549
2024-12-01-23:24:35-root-INFO: grad norm: 22.228 22.021 3.030
2024-12-01-23:24:35-root-INFO: Loss too large (245.290->250.070)! Learning rate decreased to 0.03648.
2024-12-01-23:24:36-root-INFO: grad norm: 25.241 25.030 3.262
2024-12-01-23:24:36-root-INFO: Loss too large (244.629->245.460)! Learning rate decreased to 0.02919.
2024-12-01-23:24:37-root-INFO: grad norm: 19.452 19.263 2.706
2024-12-01-23:24:38-root-INFO: grad norm: 15.343 15.184 2.200
2024-12-01-23:24:39-root-INFO: grad norm: 12.889 12.731 2.011
2024-12-01-23:24:40-root-INFO: grad norm: 11.100 10.956 1.782
2024-12-01-23:24:41-root-INFO: grad norm: 9.897 9.746 1.718
2024-12-01-23:24:42-root-INFO: grad norm: 8.992 8.851 1.589
2024-12-01-23:24:42-root-INFO: Loss Change: 245.290 -> 233.374
2024-12-01-23:24:42-root-INFO: Regularization Change: 0.000 -> 2.027
2024-12-01-23:24:42-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-01-23:24:42-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-23:24:42-root-INFO: step: 117 lr_xt 0.04704158
2024-12-01-23:24:43-root-INFO: grad norm: 10.100 9.681 2.880
2024-12-01-23:24:44-root-INFO: grad norm: 12.378 12.193 2.127
2024-12-01-23:24:44-root-INFO: Loss too large (232.383->233.554)! Learning rate decreased to 0.03763.
2024-12-01-23:24:45-root-INFO: grad norm: 15.730 15.541 2.435
2024-12-01-23:24:45-root-INFO: Loss too large (231.943->232.694)! Learning rate decreased to 0.03011.
2024-12-01-23:24:46-root-INFO: grad norm: 14.448 14.313 1.971
2024-12-01-23:24:47-root-INFO: grad norm: 13.402 13.251 2.003
2024-12-01-23:24:48-root-INFO: grad norm: 12.856 12.730 1.795
2024-12-01-23:24:49-root-INFO: grad norm: 12.421 12.281 1.861
2024-12-01-23:24:49-root-INFO: grad norm: 12.256 12.134 1.723
2024-12-01-23:24:50-root-INFO: Loss Change: 233.570 -> 226.966
2024-12-01-23:24:50-root-INFO: Regularization Change: 0.000 -> 1.819
2024-12-01-23:24:50-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-01-23:24:50-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-23:24:50-root-INFO: step: 116 lr_xt 0.04851508
2024-12-01-23:24:51-root-INFO: grad norm: 19.132 18.823 3.424
2024-12-01-23:24:51-root-INFO: Loss too large (228.576->235.707)! Learning rate decreased to 0.03881.
2024-12-01-23:24:51-root-INFO: Loss too large (228.576->230.550)! Learning rate decreased to 0.03105.
2024-12-01-23:24:52-root-INFO: grad norm: 18.573 18.439 2.227
2024-12-01-23:24:53-root-INFO: grad norm: 18.421 18.254 2.478
2024-12-01-23:24:54-root-INFO: grad norm: 18.446 18.316 2.189
2024-12-01-23:24:55-root-INFO: grad norm: 18.416 18.263 2.369
2024-12-01-23:24:56-root-INFO: grad norm: 18.462 18.332 2.190
2024-12-01-23:24:56-root-INFO: grad norm: 18.477 18.330 2.330
2024-12-01-23:24:57-root-INFO: grad norm: 18.530 18.400 2.194
2024-12-01-23:24:58-root-INFO: Loss Change: 228.576 -> 223.287
2024-12-01-23:24:58-root-INFO: Regularization Change: 0.000 -> 1.289
2024-12-01-23:24:58-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-01-23:24:58-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-23:24:58-root-INFO: step: 115 lr_xt 0.05002669
2024-12-01-23:24:58-root-INFO: grad norm: 25.680 25.354 4.075
2024-12-01-23:24:59-root-INFO: Loss too large (226.052->241.133)! Learning rate decreased to 0.04002.
2024-12-01-23:24:59-root-INFO: Loss too large (226.052->231.038)! Learning rate decreased to 0.03202.
2024-12-01-23:25:00-root-INFO: grad norm: 25.015 24.864 2.743
2024-12-01-23:25:01-root-INFO: grad norm: 24.352 24.163 3.031
2024-12-01-23:25:02-root-INFO: grad norm: 23.732 23.586 2.628
2024-12-01-23:25:02-root-INFO: grad norm: 23.054 22.883 2.800
2024-12-01-23:25:03-root-INFO: grad norm: 22.466 22.323 2.528
2024-12-01-23:25:04-root-INFO: grad norm: 21.914 21.754 2.648
2024-12-01-23:25:05-root-INFO: grad norm: 21.465 21.326 2.440
2024-12-01-23:25:06-root-INFO: Loss Change: 226.052 -> 219.782
2024-12-01-23:25:06-root-INFO: Regularization Change: 0.000 -> 1.200
2024-12-01-23:25:06-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-01-23:25:06-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-23:25:06-root-INFO: step: 114 lr_xt 0.05157710
2024-12-01-23:25:06-root-INFO: grad norm: 27.430 27.132 4.029
2024-12-01-23:25:06-root-INFO: Loss too large (222.599->240.267)! Learning rate decreased to 0.04126.
2024-12-01-23:25:07-root-INFO: Loss too large (222.599->228.367)! Learning rate decreased to 0.03301.
2024-12-01-23:25:08-root-INFO: grad norm: 26.132 25.975 2.862
2024-12-01-23:25:08-root-INFO: grad norm: 24.885 24.691 3.100
2024-12-01-23:25:09-root-INFO: grad norm: 23.842 23.693 2.669
2024-12-01-23:25:10-root-INFO: grad norm: 22.884 22.711 2.814
2024-12-01-23:25:11-root-INFO: grad norm: 22.128 21.984 2.523
2024-12-01-23:25:12-root-INFO: grad norm: 21.467 21.304 2.645
2024-12-01-23:25:13-root-INFO: grad norm: 20.960 20.819 2.420
2024-12-01-23:25:13-root-INFO: Loss Change: 222.599 -> 215.567
2024-12-01-23:25:13-root-INFO: Regularization Change: 0.000 -> 1.145
2024-12-01-23:25:13-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-01-23:25:13-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-23:25:14-root-INFO: step: 113 lr_xt 0.05316701
2024-12-01-23:25:14-root-INFO: grad norm: 25.885 25.585 3.931
2024-12-01-23:25:14-root-INFO: Loss too large (217.600->234.225)! Learning rate decreased to 0.04253.
2024-12-01-23:25:14-root-INFO: Loss too large (217.600->223.021)! Learning rate decreased to 0.03403.
2024-12-01-23:25:15-root-INFO: grad norm: 24.954 24.801 2.757
2024-12-01-23:25:16-root-INFO: grad norm: 24.162 23.975 2.994
2024-12-01-23:25:17-root-INFO: grad norm: 23.477 23.331 2.612
2024-12-01-23:25:18-root-INFO: grad norm: 22.808 22.638 2.785
2024-12-01-23:25:19-root-INFO: grad norm: 22.247 22.104 2.520
2024-12-01-23:25:20-root-INFO: grad norm: 21.723 21.560 2.660
2024-12-01-23:25:20-root-INFO: grad norm: 21.280 21.139 2.442
2024-12-01-23:25:21-root-INFO: Loss Change: 217.600 -> 211.629
2024-12-01-23:25:21-root-INFO: Regularization Change: 0.000 -> 1.098
2024-12-01-23:25:21-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-01-23:25:21-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-23:25:21-root-INFO: step: 112 lr_xt 0.05479712
2024-12-01-23:25:22-root-INFO: grad norm: 27.254 26.919 4.261
2024-12-01-23:25:22-root-INFO: Loss too large (214.359->233.064)! Learning rate decreased to 0.04384.
2024-12-01-23:25:22-root-INFO: Loss too large (214.359->220.432)! Learning rate decreased to 0.03507.
2024-12-01-23:25:23-root-INFO: grad norm: 25.945 25.783 2.890
2024-12-01-23:25:24-root-INFO: grad norm: 24.819 24.625 3.101
2024-12-01-23:25:25-root-INFO: grad norm: 23.838 23.689 2.660
2024-12-01-23:25:26-root-INFO: grad norm: 22.887 22.714 2.805
2024-12-01-23:25:26-root-INFO: grad norm: 22.067 21.925 2.504
2024-12-01-23:25:27-root-INFO: grad norm: 21.317 21.156 2.616
2024-12-01-23:25:28-root-INFO: grad norm: 20.674 20.537 2.375
2024-12-01-23:25:29-root-INFO: Loss Change: 214.359 -> 207.475
2024-12-01-23:25:29-root-INFO: Regularization Change: 0.000 -> 1.107
2024-12-01-23:25:29-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-01-23:25:29-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-23:25:29-root-INFO: step: 111 lr_xt 0.05646812
2024-12-01-23:25:29-root-INFO: grad norm: 24.177 23.930 3.449
2024-12-01-23:25:30-root-INFO: Loss too large (209.297->224.745)! Learning rate decreased to 0.04517.
2024-12-01-23:25:30-root-INFO: Loss too large (209.297->214.263)! Learning rate decreased to 0.03614.
2024-12-01-23:25:31-root-INFO: grad norm: 23.126 22.986 2.538
2024-12-01-23:25:32-root-INFO: grad norm: 22.172 22.003 2.735
2024-12-01-23:25:33-root-INFO: grad norm: 21.296 21.162 2.382
2024-12-01-23:25:34-root-INFO: grad norm: 20.498 20.344 2.503
2024-12-01-23:25:35-root-INFO: grad norm: 19.795 19.666 2.256
2024-12-01-23:25:35-root-INFO: grad norm: 19.172 19.027 2.349
2024-12-01-23:25:36-root-INFO: grad norm: 18.632 18.507 2.150
2024-12-01-23:25:37-root-INFO: Loss Change: 209.297 -> 203.310
2024-12-01-23:25:37-root-INFO: Regularization Change: 0.000 -> 1.057
2024-12-01-23:25:37-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-01-23:25:37-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-23:25:37-root-INFO: step: 110 lr_xt 0.05818072
2024-12-01-23:25:37-root-INFO: grad norm: 22.767 22.522 3.334
2024-12-01-23:25:38-root-INFO: Loss too large (205.329->219.573)! Learning rate decreased to 0.04654.
2024-12-01-23:25:38-root-INFO: Loss too large (205.329->209.976)! Learning rate decreased to 0.03724.
2024-12-01-23:25:39-root-INFO: grad norm: 22.034 21.902 2.402
2024-12-01-23:25:40-root-INFO: grad norm: 21.367 21.204 2.635
2024-12-01-23:25:41-root-INFO: grad norm: 20.726 20.600 2.278
2024-12-01-23:25:42-root-INFO: grad norm: 20.089 19.941 2.433
2024-12-01-23:25:42-root-INFO: grad norm: 19.513 19.391 2.179
2024-12-01-23:25:43-root-INFO: grad norm: 18.976 18.837 2.295
2024-12-01-23:25:44-root-INFO: grad norm: 18.507 18.388 2.090
2024-12-01-23:25:45-root-INFO: Loss Change: 205.329 -> 200.009
2024-12-01-23:25:45-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-01-23:25:45-root-INFO: Undo step: 110
2024-12-01-23:25:45-root-INFO: Undo step: 111
2024-12-01-23:25:45-root-INFO: Undo step: 112
2024-12-01-23:25:45-root-INFO: Undo step: 113
2024-12-01-23:25:45-root-INFO: Undo step: 114
2024-12-01-23:25:45-root-INFO: step: 115 lr_xt 0.05002669
2024-12-01-23:25:45-root-INFO: grad norm: 86.737 85.364 15.375
2024-12-01-23:25:46-root-INFO: grad norm: 64.674 63.843 10.330
2024-12-01-23:25:47-root-INFO: grad norm: 70.942 70.305 9.484
2024-12-01-23:25:48-root-INFO: grad norm: 76.289 75.744 9.097
2024-12-01-23:25:48-root-INFO: Loss too large (325.056->329.421)! Learning rate decreased to 0.04002.
2024-12-01-23:25:49-root-INFO: grad norm: 54.675 53.989 8.634
2024-12-01-23:25:50-root-INFO: grad norm: 39.495 39.106 5.532
2024-12-01-23:25:51-root-INFO: grad norm: 34.560 34.069 5.804
2024-12-01-23:25:52-root-INFO: grad norm: 32.548 32.195 4.780
2024-12-01-23:25:52-root-INFO: Loss Change: 486.295 -> 239.409
2024-12-01-23:25:52-root-INFO: Regularization Change: 0.000 -> 49.020
2024-12-01-23:25:52-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-01-23:25:52-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-23:25:53-root-INFO: step: 114 lr_xt 0.05157710
2024-12-01-23:25:53-root-INFO: grad norm: 28.979 28.658 4.298
2024-12-01-23:25:53-root-INFO: Loss too large (237.390->244.172)! Learning rate decreased to 0.04126.
2024-12-01-23:25:54-root-INFO: grad norm: 29.193 28.887 4.213
2024-12-01-23:25:55-root-INFO: grad norm: 29.815 29.514 4.230
2024-12-01-23:25:56-root-INFO: grad norm: 31.337 31.059 4.165
2024-12-01-23:25:57-root-INFO: grad norm: 33.062 32.762 4.443
2024-12-01-23:25:58-root-INFO: grad norm: 35.730 35.443 4.514
2024-12-01-23:25:58-root-INFO: Loss too large (230.999->232.610)! Learning rate decreased to 0.03301.
2024-12-01-23:25:59-root-INFO: grad norm: 25.704 25.456 3.564
2024-12-01-23:26:00-root-INFO: grad norm: 20.110 19.938 2.620
2024-12-01-23:26:00-root-INFO: Loss Change: 237.390 -> 217.690
2024-12-01-23:26:00-root-INFO: Regularization Change: 0.000 -> 4.417
2024-12-01-23:26:00-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-01-23:26:00-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-23:26:00-root-INFO: step: 113 lr_xt 0.05316701
2024-12-01-23:26:01-root-INFO: grad norm: 13.832 13.717 1.779
2024-12-01-23:26:01-root-INFO: Loss too large (216.189->217.829)! Learning rate decreased to 0.04253.
2024-12-01-23:26:02-root-INFO: grad norm: 17.477 17.340 2.189
2024-12-01-23:26:02-root-INFO: Loss too large (215.599->216.654)! Learning rate decreased to 0.03403.
2024-12-01-23:26:03-root-INFO: grad norm: 16.178 16.041 2.098
2024-12-01-23:26:04-root-INFO: grad norm: 15.320 15.181 2.054
2024-12-01-23:26:05-root-INFO: grad norm: 14.754 14.617 2.008
2024-12-01-23:26:06-root-INFO: grad norm: 14.349 14.214 1.969
2024-12-01-23:26:07-root-INFO: grad norm: 14.089 13.954 1.950
2024-12-01-23:26:08-root-INFO: grad norm: 13.923 13.788 1.934
2024-12-01-23:26:08-root-INFO: Loss Change: 216.189 -> 208.512
2024-12-01-23:26:08-root-INFO: Regularization Change: 0.000 -> 2.167
2024-12-01-23:26:08-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-01-23:26:08-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-23:26:09-root-INFO: step: 112 lr_xt 0.05479712
2024-12-01-23:26:09-root-INFO: grad norm: 10.217 10.062 1.778
2024-12-01-23:26:09-root-INFO: Loss too large (207.433->207.662)! Learning rate decreased to 0.04384.
2024-12-01-23:26:10-root-INFO: grad norm: 12.506 12.395 1.661
2024-12-01-23:26:10-root-INFO: Loss too large (206.667->207.179)! Learning rate decreased to 0.03507.
2024-12-01-23:26:11-root-INFO: grad norm: 12.416 12.311 1.613
2024-12-01-23:26:12-root-INFO: grad norm: 12.639 12.514 1.775
2024-12-01-23:26:13-root-INFO: grad norm: 12.912 12.796 1.721
2024-12-01-23:26:14-root-INFO: grad norm: 13.256 13.126 1.855
2024-12-01-23:26:15-root-INFO: grad norm: 13.593 13.470 1.822
2024-12-01-23:26:16-root-INFO: grad norm: 13.985 13.849 1.946
2024-12-01-23:26:16-root-INFO: Loss Change: 207.433 -> 202.560
2024-12-01-23:26:16-root-INFO: Regularization Change: 0.000 -> 1.687
2024-12-01-23:26:16-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-01-23:26:16-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-23:26:17-root-INFO: step: 111 lr_xt 0.05646812
2024-12-01-23:26:17-root-INFO: grad norm: 11.483 11.377 1.560
2024-12-01-23:26:17-root-INFO: Loss too large (201.806->204.182)! Learning rate decreased to 0.04517.
2024-12-01-23:26:17-root-INFO: Loss too large (201.806->202.134)! Learning rate decreased to 0.03614.
2024-12-01-23:26:18-root-INFO: grad norm: 11.484 11.352 1.735
2024-12-01-23:26:19-root-INFO: grad norm: 11.829 11.717 1.624
2024-12-01-23:26:20-root-INFO: grad norm: 12.285 12.154 1.787
2024-12-01-23:26:21-root-INFO: grad norm: 12.722 12.603 1.730
2024-12-01-23:26:22-root-INFO: grad norm: 13.217 13.084 1.872
2024-12-01-23:26:23-root-INFO: grad norm: 13.659 13.535 1.840
2024-12-01-23:26:23-root-INFO: grad norm: 14.144 14.006 1.969
2024-12-01-23:26:24-root-INFO: Loss Change: 201.806 -> 198.037
2024-12-01-23:26:24-root-INFO: Regularization Change: 0.000 -> 1.319
2024-12-01-23:26:24-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-01-23:26:24-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-23:26:24-root-INFO: step: 110 lr_xt 0.05818072
2024-12-01-23:26:25-root-INFO: grad norm: 11.776 11.663 1.630
2024-12-01-23:26:25-root-INFO: Loss too large (197.450->200.290)! Learning rate decreased to 0.04654.
2024-12-01-23:26:25-root-INFO: Loss too large (197.450->197.976)! Learning rate decreased to 0.03724.
2024-12-01-23:26:26-root-INFO: grad norm: 11.838 11.701 1.797
2024-12-01-23:26:27-root-INFO: grad norm: 12.305 12.195 1.642
2024-12-01-23:26:28-root-INFO: grad norm: 12.887 12.751 1.866
2024-12-01-23:26:29-root-INFO: grad norm: 13.417 13.300 1.768
2024-12-01-23:26:30-root-INFO: grad norm: 14.003 13.865 1.966
2024-12-01-23:26:30-root-INFO: grad norm: 14.510 14.386 1.897
2024-12-01-23:26:31-root-INFO: grad norm: 15.059 14.915 2.074
2024-12-01-23:26:32-root-INFO: Loss Change: 197.450 -> 194.400
2024-12-01-23:26:32-root-INFO: Regularization Change: 0.000 -> 1.221
2024-12-01-23:26:32-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-01-23:26:32-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-23:26:32-root-INFO: step: 109 lr_xt 0.05993563
2024-12-01-23:26:33-root-INFO: grad norm: 12.411 12.271 1.859
2024-12-01-23:26:33-root-INFO: Loss too large (193.789->197.137)! Learning rate decreased to 0.04795.
2024-12-01-23:26:33-root-INFO: Loss too large (193.789->194.449)! Learning rate decreased to 0.03836.
2024-12-01-23:26:34-root-INFO: grad norm: 12.221 12.086 1.813
2024-12-01-23:26:35-root-INFO: grad norm: 12.630 12.517 1.684
2024-12-01-23:26:36-root-INFO: grad norm: 13.181 13.049 1.860
2024-12-01-23:26:37-root-INFO: grad norm: 13.693 13.576 1.785
2024-12-01-23:26:38-root-INFO: grad norm: 14.266 14.131 1.959
2024-12-01-23:26:38-root-INFO: grad norm: 14.761 14.638 1.904
2024-12-01-23:26:39-root-INFO: grad norm: 15.295 15.155 2.067
2024-12-01-23:26:40-root-INFO: Loss Change: 193.789 -> 190.946
2024-12-01-23:26:40-root-INFO: Regularization Change: 0.000 -> 1.169
2024-12-01-23:26:40-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-01-23:26:40-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-23:26:40-root-INFO: step: 108 lr_xt 0.06173354
2024-12-01-23:26:41-root-INFO: grad norm: 13.059 12.959 1.609
2024-12-01-23:26:41-root-INFO: Loss too large (190.005->194.467)! Learning rate decreased to 0.04939.
2024-12-01-23:26:41-root-INFO: Loss too large (190.005->191.175)! Learning rate decreased to 0.03951.
2024-12-01-23:26:42-root-INFO: grad norm: 13.075 12.942 1.862
2024-12-01-23:26:43-root-INFO: grad norm: 13.393 13.284 1.705
2024-12-01-23:26:44-root-INFO: grad norm: 13.814 13.681 1.913
2024-12-01-23:26:44-root-INFO: grad norm: 14.213 14.098 1.810
2024-12-01-23:26:45-root-INFO: grad norm: 14.666 14.530 1.989
2024-12-01-23:26:46-root-INFO: grad norm: 15.070 14.948 1.911
2024-12-01-23:26:47-root-INFO: grad norm: 15.506 15.366 2.073
2024-12-01-23:26:48-root-INFO: Loss Change: 190.005 -> 187.424
2024-12-01-23:26:48-root-INFO: Regularization Change: 0.000 -> 1.094
2024-12-01-23:26:48-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-01-23:26:48-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-23:26:48-root-INFO: step: 107 lr_xt 0.06357517
2024-12-01-23:26:48-root-INFO: grad norm: 12.019 11.883 1.803
2024-12-01-23:26:48-root-INFO: Loss too large (186.588->189.896)! Learning rate decreased to 0.05086.
2024-12-01-23:26:49-root-INFO: Loss too large (186.588->187.245)! Learning rate decreased to 0.04069.
2024-12-01-23:26:50-root-INFO: grad norm: 11.810 11.684 1.717
2024-12-01-23:26:51-root-INFO: grad norm: 12.387 12.282 1.607
2024-12-01-23:26:51-root-INFO: grad norm: 13.133 13.007 1.812
2024-12-01-23:26:52-root-INFO: grad norm: 13.831 13.719 1.758
2024-12-01-23:26:53-root-INFO: grad norm: 14.584 14.453 1.952
2024-12-01-23:26:54-root-INFO: grad norm: 15.234 15.113 1.917
2024-12-01-23:26:55-root-INFO: grad norm: 15.905 15.767 2.095
2024-12-01-23:26:56-root-INFO: Loss Change: 186.588 -> 184.502
2024-12-01-23:26:56-root-INFO: Regularization Change: 0.000 -> 1.126
2024-12-01-23:26:56-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-01-23:26:56-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-23:26:56-root-INFO: step: 106 lr_xt 0.06546120
2024-12-01-23:26:56-root-INFO: grad norm: 12.818 12.722 1.567
2024-12-01-23:26:56-root-INFO: Loss too large (183.376->187.819)! Learning rate decreased to 0.05237.
2024-12-01-23:26:57-root-INFO: Loss too large (183.376->184.518)! Learning rate decreased to 0.04190.
2024-12-01-23:26:58-root-INFO: grad norm: 12.799 12.668 1.828
2024-12-01-23:26:59-root-INFO: grad norm: 13.280 13.180 1.628
2024-12-01-23:27:00-root-INFO: grad norm: 13.918 13.786 1.915
2024-12-01-23:27:00-root-INFO: grad norm: 14.554 14.442 1.798
2024-12-01-23:27:01-root-INFO: grad norm: 15.268 15.131 2.045
2024-12-01-23:27:02-root-INFO: grad norm: 15.935 15.813 1.970
2024-12-01-23:27:03-root-INFO: grad norm: 16.641 16.496 2.193
2024-12-01-23:27:03-root-INFO: Loss too large (181.498->181.548)! Learning rate decreased to 0.03352.
2024-12-01-23:27:04-root-INFO: Loss Change: 183.376 -> 179.709
2024-12-01-23:27:04-root-INFO: Regularization Change: 0.000 -> 1.049
2024-12-01-23:27:04-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-01-23:27:04-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-23:27:04-root-INFO: step: 105 lr_xt 0.06739236
2024-12-01-23:27:04-root-INFO: grad norm: 8.195 8.095 1.276
2024-12-01-23:27:05-root-INFO: Loss too large (179.459->180.683)! Learning rate decreased to 0.05391.
2024-12-01-23:27:05-root-INFO: Loss too large (179.459->179.520)! Learning rate decreased to 0.04313.
2024-12-01-23:27:06-root-INFO: grad norm: 8.183 8.079 1.299
2024-12-01-23:27:07-root-INFO: grad norm: 8.814 8.738 1.153
2024-12-01-23:27:08-root-INFO: grad norm: 9.638 9.536 1.395
2024-12-01-23:27:09-root-INFO: grad norm: 10.529 10.447 1.310
2024-12-01-23:27:10-root-INFO: grad norm: 11.538 11.431 1.565
2024-12-01-23:27:10-root-INFO: grad norm: 12.548 12.456 1.515
2024-12-01-23:27:11-root-INFO: grad norm: 13.640 13.523 1.778
2024-12-01-23:27:12-root-INFO: Loss too large (177.893->177.941)! Learning rate decreased to 0.03450.
2024-12-01-23:27:12-root-INFO: Loss Change: 179.459 -> 176.657
2024-12-01-23:27:12-root-INFO: Regularization Change: 0.000 -> 1.015
2024-12-01-23:27:12-root-INFO: Undo step: 105
2024-12-01-23:27:12-root-INFO: Undo step: 106
2024-12-01-23:27:12-root-INFO: Undo step: 107
2024-12-01-23:27:12-root-INFO: Undo step: 108
2024-12-01-23:27:12-root-INFO: Undo step: 109
2024-12-01-23:27:12-root-INFO: step: 110 lr_xt 0.05818072
2024-12-01-23:27:13-root-INFO: grad norm: 69.180 68.123 12.048
2024-12-01-23:27:14-root-INFO: grad norm: 58.154 57.573 8.200
2024-12-01-23:27:14-root-INFO: grad norm: 59.698 59.067 8.658
2024-12-01-23:27:15-root-INFO: grad norm: 66.977 66.484 8.112
2024-12-01-23:27:16-root-INFO: Loss too large (283.736->304.508)! Learning rate decreased to 0.04654.
2024-12-01-23:27:16-root-INFO: grad norm: 50.540 50.104 6.621
2024-12-01-23:27:17-root-INFO: grad norm: 36.333 36.041 4.597
2024-12-01-23:27:18-root-INFO: grad norm: 33.510 33.171 4.755
2024-12-01-23:27:19-root-INFO: grad norm: 33.353 33.100 4.099
2024-12-01-23:27:20-root-INFO: Loss Change: 403.303 -> 220.270
2024-12-01-23:27:20-root-INFO: Regularization Change: 0.000 -> 42.802
2024-12-01-23:27:20-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-01-23:27:20-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-23:27:20-root-INFO: step: 109 lr_xt 0.05993563
2024-12-01-23:27:20-root-INFO: grad norm: 31.527 31.334 3.481
2024-12-01-23:27:21-root-INFO: Loss too large (217.816->229.753)! Learning rate decreased to 0.04795.
2024-12-01-23:27:22-root-INFO: grad norm: 33.555 33.330 3.881
2024-12-01-23:27:22-root-INFO: Loss too large (215.379->216.864)! Learning rate decreased to 0.03836.
2024-12-01-23:27:23-root-INFO: grad norm: 24.038 23.836 3.112
2024-12-01-23:27:24-root-INFO: grad norm: 17.489 17.340 2.278
2024-12-01-23:27:25-root-INFO: grad norm: 14.028 13.870 2.103
2024-12-01-23:27:26-root-INFO: grad norm: 11.615 11.487 1.719
2024-12-01-23:27:26-root-INFO: grad norm: 10.057 9.916 1.673
2024-12-01-23:27:27-root-INFO: grad norm: 8.908 8.786 1.469
2024-12-01-23:27:28-root-INFO: Loss Change: 217.816 -> 194.202
2024-12-01-23:27:28-root-INFO: Regularization Change: 0.000 -> 4.095
2024-12-01-23:27:28-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-01-23:27:28-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-23:27:28-root-INFO: step: 108 lr_xt 0.06173354
2024-12-01-23:27:28-root-INFO: grad norm: 6.606 6.463 1.367
2024-12-01-23:27:29-root-INFO: grad norm: 7.781 7.678 1.261
2024-12-01-23:27:30-root-INFO: grad norm: 11.949 11.863 1.433
2024-12-01-23:27:30-root-INFO: Loss too large (191.656->193.754)! Learning rate decreased to 0.04939.
2024-12-01-23:27:31-root-INFO: grad norm: 15.406 15.305 1.763
2024-12-01-23:27:32-root-INFO: Loss too large (191.576->192.574)! Learning rate decreased to 0.03951.
2024-12-01-23:27:32-root-INFO: grad norm: 13.843 13.737 1.708
2024-12-01-23:27:33-root-INFO: grad norm: 12.647 12.541 1.636
2024-12-01-23:27:34-root-INFO: grad norm: 11.782 11.679 1.562
2024-12-01-23:27:35-root-INFO: grad norm: 11.095 10.991 1.509
2024-12-01-23:27:35-root-INFO: Loss Change: 193.584 -> 186.753
2024-12-01-23:27:35-root-INFO: Regularization Change: 0.000 -> 2.824
2024-12-01-23:27:35-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-01-23:27:35-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-23:27:36-root-INFO: step: 107 lr_xt 0.06357517
2024-12-01-23:27:36-root-INFO: grad norm: 8.087 7.910 1.681
2024-12-01-23:27:37-root-INFO: grad norm: 11.080 11.012 1.220
2024-12-01-23:27:37-root-INFO: Loss too large (185.838->187.682)! Learning rate decreased to 0.05086.
2024-12-01-23:27:38-root-INFO: grad norm: 13.933 13.868 1.342
2024-12-01-23:27:38-root-INFO: Loss too large (185.798->186.710)! Learning rate decreased to 0.04069.
2024-12-01-23:27:39-root-INFO: grad norm: 12.754 12.669 1.465
2024-12-01-23:27:40-root-INFO: grad norm: 12.146 12.062 1.423
2024-12-01-23:27:41-root-INFO: grad norm: 11.793 11.699 1.487
2024-12-01-23:27:42-root-INFO: grad norm: 11.607 11.520 1.417
2024-12-01-23:27:43-root-INFO: grad norm: 11.539 11.443 1.484
2024-12-01-23:27:43-root-INFO: Loss Change: 186.438 -> 181.779
2024-12-01-23:27:43-root-INFO: Regularization Change: 0.000 -> 1.913
2024-12-01-23:27:43-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-01-23:27:43-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-23:27:44-root-INFO: step: 106 lr_xt 0.06546120
2024-12-01-23:27:44-root-INFO: grad norm: 8.747 8.641 1.355
2024-12-01-23:27:44-root-INFO: Loss too large (181.127->181.812)! Learning rate decreased to 0.05237.
2024-12-01-23:27:45-root-INFO: grad norm: 11.046 10.968 1.304
2024-12-01-23:27:45-root-INFO: Loss too large (180.769->181.415)! Learning rate decreased to 0.04190.
2024-12-01-23:27:46-root-INFO: grad norm: 10.820 10.752 1.213
2024-12-01-23:27:47-root-INFO: grad norm: 10.927 10.835 1.414
2024-12-01-23:27:48-root-INFO: grad norm: 11.166 11.090 1.305
2024-12-01-23:27:49-root-INFO: grad norm: 11.508 11.411 1.492
2024-12-01-23:27:50-root-INFO: grad norm: 11.899 11.816 1.397
2024-12-01-23:27:51-root-INFO: grad norm: 12.353 12.251 1.582
2024-12-01-23:27:51-root-INFO: Loss Change: 181.127 -> 177.849
2024-12-01-23:27:51-root-INFO: Regularization Change: 0.000 -> 1.452
2024-12-01-23:27:51-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-01-23:27:51-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-23:27:51-root-INFO: step: 105 lr_xt 0.06739236
2024-12-01-23:27:52-root-INFO: grad norm: 9.860 9.785 1.215
2024-12-01-23:27:52-root-INFO: Loss too large (177.424->179.563)! Learning rate decreased to 0.05391.
2024-12-01-23:27:52-root-INFO: Loss too large (177.424->177.771)! Learning rate decreased to 0.04313.
2024-12-01-23:27:53-root-INFO: grad norm: 9.841 9.754 1.305
2024-12-01-23:27:54-root-INFO: grad norm: 10.369 10.307 1.136
2024-12-01-23:27:55-root-INFO: grad norm: 11.019 10.929 1.407
2024-12-01-23:27:56-root-INFO: grad norm: 11.689 11.618 1.285
2024-12-01-23:27:57-root-INFO: grad norm: 12.411 12.315 1.536
2024-12-01-23:27:57-root-INFO: grad norm: 13.104 13.025 1.437
2024-12-01-23:27:58-root-INFO: grad norm: 13.821 13.719 1.678
2024-12-01-23:27:59-root-INFO: Loss Change: 177.424 -> 175.090
2024-12-01-23:27:59-root-INFO: Regularization Change: 0.000 -> 1.239
2024-12-01-23:27:59-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-01-23:27:59-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-23:27:59-root-INFO: step: 104 lr_xt 0.06936934
2024-12-01-23:27:59-root-INFO: grad norm: 10.943 10.866 1.297
2024-12-01-23:28:00-root-INFO: Loss too large (174.182->177.110)! Learning rate decreased to 0.05550.
2024-12-01-23:28:00-root-INFO: Loss too large (174.182->174.785)! Learning rate decreased to 0.04440.
2024-12-01-23:28:01-root-INFO: grad norm: 10.702 10.622 1.308
2024-12-01-23:28:02-root-INFO: grad norm: 11.152 11.095 1.128
2024-12-01-23:28:03-root-INFO: grad norm: 11.729 11.641 1.434
2024-12-01-23:28:03-root-INFO: grad norm: 12.313 12.245 1.293
2024-12-01-23:28:04-root-INFO: grad norm: 12.928 12.833 1.565
2024-12-01-23:28:05-root-INFO: grad norm: 13.506 13.429 1.445
2024-12-01-23:28:06-root-INFO: grad norm: 14.088 13.985 1.696
2024-12-01-23:28:07-root-INFO: Loss Change: 174.182 -> 171.893
2024-12-01-23:28:07-root-INFO: Regularization Change: 0.000 -> 1.193
2024-12-01-23:28:07-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-01-23:28:07-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-23:28:07-root-INFO: step: 103 lr_xt 0.07139284
2024-12-01-23:28:07-root-INFO: grad norm: 11.136 11.087 1.043
2024-12-01-23:28:08-root-INFO: Loss too large (171.054->174.768)! Learning rate decreased to 0.05711.
2024-12-01-23:28:08-root-INFO: Loss too large (171.054->172.088)! Learning rate decreased to 0.04569.
2024-12-01-23:28:09-root-INFO: grad norm: 11.255 11.172 1.362
2024-12-01-23:28:09-root-INFO: grad norm: 11.712 11.648 1.224
2024-12-01-23:28:10-root-INFO: grad norm: 12.249 12.158 1.497
2024-12-01-23:28:11-root-INFO: grad norm: 12.784 12.709 1.385
2024-12-01-23:28:12-root-INFO: grad norm: 13.335 13.237 1.621
2024-12-01-23:28:13-root-INFO: grad norm: 13.851 13.767 1.526
2024-12-01-23:28:14-root-INFO: grad norm: 14.362 14.256 1.740
2024-12-01-23:28:14-root-INFO: Loss Change: 171.054 -> 169.114
2024-12-01-23:28:14-root-INFO: Regularization Change: 0.000 -> 1.129
2024-12-01-23:28:14-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-01-23:28:14-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-23:28:15-root-INFO: step: 102 lr_xt 0.07346356
2024-12-01-23:28:15-root-INFO: grad norm: 12.369 12.307 1.231
2024-12-01-23:28:15-root-INFO: Loss too large (168.208->173.270)! Learning rate decreased to 0.05877.
2024-12-01-23:28:16-root-INFO: Loss too large (168.208->169.741)! Learning rate decreased to 0.04702.
2024-12-01-23:28:16-root-INFO: grad norm: 12.664 12.570 1.534
2024-12-01-23:28:17-root-INFO: grad norm: 13.241 13.168 1.387
2024-12-01-23:28:18-root-INFO: grad norm: 13.849 13.748 1.669
2024-12-01-23:28:19-root-INFO: grad norm: 14.410 14.326 1.562
2024-12-01-23:28:20-root-INFO: grad norm: 14.947 14.838 1.799
2024-12-01-23:28:21-root-INFO: grad norm: 15.411 15.317 1.704
2024-12-01-23:28:22-root-INFO: grad norm: 15.836 15.720 1.909
2024-12-01-23:28:22-root-INFO: Loss Change: 168.208 -> 166.541
2024-12-01-23:28:22-root-INFO: Regularization Change: 0.000 -> 1.141
2024-12-01-23:28:22-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-01-23:28:22-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-23:28:23-root-INFO: step: 101 lr_xt 0.07558219
2024-12-01-23:28:23-root-INFO: grad norm: 12.824 12.765 1.229
2024-12-01-23:28:23-root-INFO: Loss too large (165.474->171.228)! Learning rate decreased to 0.06047.
2024-12-01-23:28:23-root-INFO: Loss too large (165.474->167.276)! Learning rate decreased to 0.04837.
2024-12-01-23:28:24-root-INFO: grad norm: 12.989 12.893 1.578
2024-12-01-23:28:25-root-INFO: grad norm: 13.370 13.296 1.401
2024-12-01-23:28:26-root-INFO: grad norm: 13.770 13.668 1.673
2024-12-01-23:28:27-root-INFO: grad norm: 14.135 14.051 1.535
2024-12-01-23:28:28-root-INFO: grad norm: 14.475 14.368 1.757
2024-12-01-23:28:29-root-INFO: grad norm: 14.764 14.673 1.631
2024-12-01-23:28:30-root-INFO: grad norm: 15.020 14.909 1.822
2024-12-01-23:28:30-root-INFO: Loss Change: 165.474 -> 163.481
2024-12-01-23:28:30-root-INFO: Regularization Change: 0.000 -> 1.108
2024-12-01-23:28:30-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-01-23:28:30-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-23:28:31-root-INFO: step: 100 lr_xt 0.07774943
2024-12-01-23:28:31-root-INFO: grad norm: 11.854 11.805 1.079
2024-12-01-23:28:31-root-INFO: Loss too large (162.491->167.369)! Learning rate decreased to 0.06220.
2024-12-01-23:28:32-root-INFO: Loss too large (162.491->163.955)! Learning rate decreased to 0.04976.
2024-12-01-23:28:32-root-INFO: grad norm: 11.832 11.742 1.452
2024-12-01-23:28:33-root-INFO: grad norm: 12.058 11.992 1.260
2024-12-01-23:28:34-root-INFO: grad norm: 12.317 12.223 1.521
2024-12-01-23:28:35-root-INFO: grad norm: 12.566 12.491 1.372
2024-12-01-23:28:36-root-INFO: grad norm: 12.807 12.710 1.575
2024-12-01-23:28:37-root-INFO: grad norm: 13.024 12.943 1.447
2024-12-01-23:28:38-root-INFO: grad norm: 13.226 13.126 1.621
2024-12-01-23:28:38-root-INFO: Loss Change: 162.491 -> 160.243
2024-12-01-23:28:38-root-INFO: Regularization Change: 0.000 -> 1.093
2024-12-01-23:28:38-root-INFO: Undo step: 100
2024-12-01-23:28:38-root-INFO: Undo step: 101
2024-12-01-23:28:38-root-INFO: Undo step: 102
2024-12-01-23:28:38-root-INFO: Undo step: 103
2024-12-01-23:28:38-root-INFO: Undo step: 104
2024-12-01-23:28:38-root-INFO: step: 105 lr_xt 0.06739236
2024-12-01-23:28:39-root-INFO: grad norm: 73.488 72.223 13.581
2024-12-01-23:28:40-root-INFO: grad norm: 46.124 45.454 7.831
2024-12-01-23:28:40-root-INFO: grad norm: 51.728 51.101 8.027
2024-12-01-23:28:41-root-INFO: Loss too large (265.870->277.210)! Learning rate decreased to 0.05391.
2024-12-01-23:28:42-root-INFO: grad norm: 45.301 44.917 5.883
2024-12-01-23:28:43-root-INFO: grad norm: 38.579 38.177 5.552
2024-12-01-23:28:43-root-INFO: grad norm: 34.995 34.721 4.370
2024-12-01-23:28:44-root-INFO: grad norm: 33.502 33.206 4.443
2024-12-01-23:28:45-root-INFO: grad norm: 33.333 33.093 3.995
2024-12-01-23:28:46-root-INFO: Loss Change: 431.620 -> 204.880
2024-12-01-23:28:46-root-INFO: Regularization Change: 0.000 -> 61.881
2024-12-01-23:28:46-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-01-23:28:46-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-23:28:46-root-INFO: step: 104 lr_xt 0.06936934
2024-12-01-23:28:46-root-INFO: grad norm: 31.388 31.215 3.298
2024-12-01-23:28:47-root-INFO: Loss too large (202.405->215.271)! Learning rate decreased to 0.05550.
2024-12-01-23:28:47-root-INFO: grad norm: 32.084 31.873 3.674
2024-12-01-23:28:48-root-INFO: grad norm: 32.782 32.575 3.685
2024-12-01-23:28:49-root-INFO: grad norm: 34.036 33.802 3.988
2024-12-01-23:28:49-root-INFO: Loss too large (197.066->197.405)! Learning rate decreased to 0.04440.
2024-12-01-23:28:50-root-INFO: grad norm: 23.006 22.812 2.977
2024-12-01-23:28:51-root-INFO: grad norm: 16.345 16.196 2.201
2024-12-01-23:28:52-root-INFO: grad norm: 12.812 12.658 1.981
2024-12-01-23:28:53-root-INFO: grad norm: 10.483 10.352 1.653
2024-12-01-23:28:53-root-INFO: Loss Change: 202.405 -> 175.953
2024-12-01-23:28:53-root-INFO: Regularization Change: 0.000 -> 5.799
2024-12-01-23:28:53-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-01-23:28:53-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-23:28:54-root-INFO: step: 103 lr_xt 0.07139284
2024-12-01-23:28:54-root-INFO: grad norm: 6.791 6.642 1.413
2024-12-01-23:28:55-root-INFO: grad norm: 8.681 8.567 1.403
2024-12-01-23:28:55-root-INFO: Loss too large (174.174->174.215)! Learning rate decreased to 0.05711.
2024-12-01-23:28:56-root-INFO: grad norm: 10.142 10.042 1.417
2024-12-01-23:28:57-root-INFO: grad norm: 12.674 12.564 1.665
2024-12-01-23:28:57-root-INFO: Loss too large (172.959->173.300)! Learning rate decreased to 0.04569.
2024-12-01-23:28:58-root-INFO: grad norm: 11.140 11.032 1.552
2024-12-01-23:28:59-root-INFO: grad norm: 9.997 9.889 1.467
2024-12-01-23:29:00-root-INFO: grad norm: 9.191 9.086 1.384
2024-12-01-23:29:01-root-INFO: grad norm: 8.574 8.470 1.332
2024-12-01-23:29:01-root-INFO: Loss Change: 175.476 -> 168.162
2024-12-01-23:29:01-root-INFO: Regularization Change: 0.000 -> 3.154
2024-12-01-23:29:01-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-01-23:29:01-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-23:29:02-root-INFO: step: 102 lr_xt 0.07346356
2024-12-01-23:29:02-root-INFO: grad norm: 6.479 6.359 1.240
2024-12-01-23:29:03-root-INFO: grad norm: 9.472 9.393 1.219
2024-12-01-23:29:03-root-INFO: Loss too large (167.032->168.654)! Learning rate decreased to 0.05877.
2024-12-01-23:29:03-root-INFO: Loss too large (167.032->167.047)! Learning rate decreased to 0.04702.
2024-12-01-23:29:04-root-INFO: grad norm: 8.745 8.666 1.172
2024-12-01-23:29:05-root-INFO: grad norm: 8.439 8.348 1.240
2024-12-01-23:29:06-root-INFO: grad norm: 8.246 8.164 1.159
2024-12-01-23:29:07-root-INFO: grad norm: 8.124 8.032 1.217
2024-12-01-23:29:08-root-INFO: grad norm: 8.050 7.969 1.141
2024-12-01-23:29:09-root-INFO: grad norm: 8.015 7.924 1.203
2024-12-01-23:29:09-root-INFO: Loss Change: 167.580 -> 162.789
2024-12-01-23:29:09-root-INFO: Regularization Change: 0.000 -> 2.035
2024-12-01-23:29:09-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-01-23:29:09-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-23:29:10-root-INFO: step: 101 lr_xt 0.07558219
2024-12-01-23:29:10-root-INFO: grad norm: 5.982 5.871 1.150
2024-12-01-23:29:11-root-INFO: grad norm: 9.220 9.153 1.105
2024-12-01-23:29:11-root-INFO: Loss too large (162.281->164.118)! Learning rate decreased to 0.06047.
2024-12-01-23:29:12-root-INFO: Loss too large (162.281->162.476)! Learning rate decreased to 0.04837.
2024-12-01-23:29:12-root-INFO: grad norm: 8.591 8.521 1.102
2024-12-01-23:29:13-root-INFO: grad norm: 8.403 8.318 1.186
2024-12-01-23:29:14-root-INFO: grad norm: 8.302 8.229 1.102
2024-12-01-23:29:15-root-INFO: grad norm: 8.278 8.192 1.194
2024-12-01-23:29:16-root-INFO: grad norm: 8.292 8.219 1.101
2024-12-01-23:29:17-root-INFO: grad norm: 8.342 8.255 1.204
2024-12-01-23:29:17-root-INFO: Loss Change: 162.492 -> 158.872
2024-12-01-23:29:17-root-INFO: Regularization Change: 0.000 -> 1.680
2024-12-01-23:29:17-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-01-23:29:17-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-23:29:18-root-INFO: step: 100 lr_xt 0.07774943
2024-12-01-23:29:18-root-INFO: grad norm: 6.501 6.411 1.081
2024-12-01-23:29:18-root-INFO: Loss too large (158.581->158.868)! Learning rate decreased to 0.06220.
2024-12-01-23:29:19-root-INFO: grad norm: 7.959 7.890 1.046
2024-12-01-23:29:20-root-INFO: Loss too large (158.262->158.398)! Learning rate decreased to 0.04976.
2024-12-01-23:29:20-root-INFO: grad norm: 7.556 7.493 0.976
2024-12-01-23:29:21-root-INFO: grad norm: 7.415 7.337 1.072
2024-12-01-23:29:22-root-INFO: grad norm: 7.345 7.281 0.961
2024-12-01-23:29:23-root-INFO: grad norm: 7.345 7.267 1.069
2024-12-01-23:29:24-root-INFO: grad norm: 7.386 7.323 0.963
2024-12-01-23:29:25-root-INFO: grad norm: 7.464 7.385 1.079
2024-12-01-23:29:25-root-INFO: Loss Change: 158.581 -> 155.365
2024-12-01-23:29:25-root-INFO: Regularization Change: 0.000 -> 1.394
2024-12-01-23:29:25-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-01-23:29:25-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-23:29:26-root-INFO: step: 99 lr_xt 0.07996596
2024-12-01-23:29:26-root-INFO: grad norm: 5.893 5.803 1.025
2024-12-01-23:29:26-root-INFO: Loss too large (155.018->155.125)! Learning rate decreased to 0.06397.
2024-12-01-23:29:27-root-INFO: grad norm: 7.014 6.960 0.868
2024-12-01-23:29:28-root-INFO: Loss too large (154.660->154.688)! Learning rate decreased to 0.05118.
2024-12-01-23:29:29-root-INFO: grad norm: 6.643 6.592 0.824
2024-12-01-23:29:29-root-INFO: grad norm: 6.524 6.460 0.917
2024-12-01-23:29:30-root-INFO: grad norm: 6.470 6.417 0.827
2024-12-01-23:29:31-root-INFO: grad norm: 6.484 6.417 0.931
2024-12-01-23:29:32-root-INFO: grad norm: 6.533 6.478 0.840
2024-12-01-23:29:33-root-INFO: grad norm: 6.617 6.549 0.950
2024-12-01-23:29:34-root-INFO: Loss Change: 155.018 -> 152.033
2024-12-01-23:29:34-root-INFO: Regularization Change: 0.000 -> 1.309
2024-12-01-23:29:34-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-01-23:29:34-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-23:29:34-root-INFO: step: 98 lr_xt 0.08223248
2024-12-01-23:29:34-root-INFO: grad norm: 5.386 5.286 1.036
2024-12-01-23:29:35-root-INFO: grad norm: 8.161 8.121 0.807
2024-12-01-23:29:35-root-INFO: Loss too large (151.864->153.448)! Learning rate decreased to 0.06579.
2024-12-01-23:29:36-root-INFO: Loss too large (151.864->152.002)! Learning rate decreased to 0.05263.
2024-12-01-23:29:37-root-INFO: grad norm: 7.320 7.281 0.755
2024-12-01-23:29:38-root-INFO: grad norm: 7.020 6.968 0.857
2024-12-01-23:29:38-root-INFO: grad norm: 6.838 6.794 0.779
2024-12-01-23:29:39-root-INFO: grad norm: 6.789 6.730 0.894
2024-12-01-23:29:40-root-INFO: grad norm: 6.801 6.753 0.807
2024-12-01-23:29:41-root-INFO: grad norm: 6.880 6.816 0.931
2024-12-01-23:29:42-root-INFO: Loss Change: 151.934 -> 149.163
2024-12-01-23:29:42-root-INFO: Regularization Change: 0.000 -> 1.359
2024-12-01-23:29:42-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-01-23:29:42-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-23:29:42-root-INFO: step: 97 lr_xt 0.08454965
2024-12-01-23:29:42-root-INFO: grad norm: 5.960 5.841 1.185
2024-12-01-23:29:42-root-INFO: Loss too large (148.986->149.129)! Learning rate decreased to 0.06764.
2024-12-01-23:29:43-root-INFO: grad norm: 6.717 6.676 0.738
2024-12-01-23:29:44-root-INFO: grad norm: 8.904 8.863 0.854
2024-12-01-23:29:44-root-INFO: Loss too large (148.615->149.212)! Learning rate decreased to 0.05411.
2024-12-01-23:29:45-root-INFO: grad norm: 8.197 8.148 0.889
2024-12-01-23:29:46-root-INFO: grad norm: 7.631 7.587 0.813
2024-12-01-23:29:47-root-INFO: grad norm: 7.312 7.259 0.880
2024-12-01-23:29:48-root-INFO: grad norm: 7.073 7.028 0.797
2024-12-01-23:29:49-root-INFO: grad norm: 6.960 6.904 0.885
2024-12-01-23:29:49-root-INFO: Loss Change: 148.986 -> 146.280
2024-12-01-23:29:49-root-INFO: Regularization Change: 0.000 -> 1.307
2024-12-01-23:29:49-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-01-23:29:49-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-23:29:50-root-INFO: step: 96 lr_xt 0.08691815
2024-12-01-23:29:50-root-INFO: grad norm: 5.913 5.845 0.895
2024-12-01-23:29:50-root-INFO: Loss too large (146.224->146.688)! Learning rate decreased to 0.06953.
2024-12-01-23:29:51-root-INFO: grad norm: 7.260 7.220 0.758
2024-12-01-23:29:52-root-INFO: Loss too large (146.062->146.222)! Learning rate decreased to 0.05563.
2024-12-01-23:29:52-root-INFO: grad norm: 6.658 6.619 0.725
2024-12-01-23:29:53-root-INFO: grad norm: 6.331 6.281 0.791
2024-12-01-23:29:54-root-INFO: grad norm: 6.073 6.032 0.705
2024-12-01-23:29:55-root-INFO: grad norm: 5.927 5.875 0.782
2024-12-01-23:29:56-root-INFO: grad norm: 5.835 5.793 0.697
2024-12-01-23:29:57-root-INFO: grad norm: 5.810 5.757 0.785
2024-12-01-23:29:57-root-INFO: Loss Change: 146.224 -> 143.593
2024-12-01-23:29:57-root-INFO: Regularization Change: 0.000 -> 1.193
2024-12-01-23:29:57-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-01-23:29:57-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-23:29:58-root-INFO: step: 95 lr_xt 0.08933865
2024-12-01-23:29:58-root-INFO: grad norm: 5.148 5.043 1.035
2024-12-01-23:29:58-root-INFO: Loss too large (143.714->143.750)! Learning rate decreased to 0.07147.
2024-12-01-23:29:59-root-INFO: grad norm: 5.754 5.709 0.721
2024-12-01-23:30:00-root-INFO: grad norm: 7.451 7.406 0.820
2024-12-01-23:30:00-root-INFO: Loss too large (143.271->143.596)! Learning rate decreased to 0.05718.
2024-12-01-23:30:01-root-INFO: grad norm: 6.772 6.730 0.752
2024-12-01-23:30:02-root-INFO: grad norm: 6.209 6.171 0.686
2024-12-01-23:30:03-root-INFO: grad norm: 5.847 5.804 0.704
2024-12-01-23:30:04-root-INFO: grad norm: 5.545 5.508 0.637
2024-12-01-23:30:05-root-INFO: grad norm: 5.354 5.310 0.686
2024-12-01-23:30:05-root-INFO: Loss Change: 143.714 -> 141.106
2024-12-01-23:30:05-root-INFO: Regularization Change: 0.000 -> 1.260
2024-12-01-23:30:05-root-INFO: Undo step: 95
2024-12-01-23:30:05-root-INFO: Undo step: 96
2024-12-01-23:30:05-root-INFO: Undo step: 97
2024-12-01-23:30:05-root-INFO: Undo step: 98
2024-12-01-23:30:05-root-INFO: Undo step: 99
2024-12-01-23:30:06-root-INFO: step: 100 lr_xt 0.07774943
2024-12-01-23:30:06-root-INFO: grad norm: 63.046 62.244 10.025
2024-12-01-23:30:07-root-INFO: grad norm: 45.446 45.043 6.036
2024-12-01-23:30:08-root-INFO: grad norm: 48.565 48.380 4.235
2024-12-01-23:30:08-root-INFO: grad norm: 49.078 48.827 4.961
2024-12-01-23:30:09-root-INFO: grad norm: 45.536 45.292 4.712
2024-12-01-23:30:10-root-INFO: grad norm: 44.455 44.072 5.828
2024-12-01-23:30:11-root-INFO: Loss too large (215.635->215.886)! Learning rate decreased to 0.06220.
2024-12-01-23:30:11-root-INFO: grad norm: 31.177 30.793 4.880
2024-12-01-23:30:12-root-INFO: grad norm: 25.548 25.256 3.855
2024-12-01-23:30:13-root-INFO: Loss Change: 371.409 -> 176.782
2024-12-01-23:30:13-root-INFO: Regularization Change: 0.000 -> 59.350
2024-12-01-23:30:13-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-01-23:30:13-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-23:30:13-root-INFO: step: 99 lr_xt 0.07996596
2024-12-01-23:30:13-root-INFO: grad norm: 21.471 21.200 3.398
2024-12-01-23:30:14-root-INFO: Loss too large (174.906->179.410)! Learning rate decreased to 0.06397.
2024-12-01-23:30:15-root-INFO: grad norm: 20.828 20.580 3.206
2024-12-01-23:30:16-root-INFO: grad norm: 21.225 20.936 3.493
2024-12-01-23:30:16-root-INFO: grad norm: 22.524 22.268 3.386
2024-12-01-23:30:17-root-INFO: Loss too large (169.142->169.522)! Learning rate decreased to 0.05118.
2024-12-01-23:30:18-root-INFO: grad norm: 16.071 15.837 2.735
2024-12-01-23:30:19-root-INFO: grad norm: 11.952 11.812 1.823
2024-12-01-23:30:19-root-INFO: grad norm: 9.591 9.437 1.713
2024-12-01-23:30:20-root-INFO: grad norm: 7.928 7.820 1.307
2024-12-01-23:30:21-root-INFO: Loss Change: 174.906 -> 157.064
2024-12-01-23:30:21-root-INFO: Regularization Change: 0.000 -> 5.114
2024-12-01-23:30:21-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-01-23:30:21-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-23:30:21-root-INFO: step: 98 lr_xt 0.08223248
2024-12-01-23:30:21-root-INFO: grad norm: 5.295 5.155 1.209
2024-12-01-23:30:22-root-INFO: grad norm: 5.861 5.747 1.147
2024-12-01-23:30:23-root-INFO: grad norm: 8.276 8.174 1.293
2024-12-01-23:30:24-root-INFO: Loss too large (154.734->155.377)! Learning rate decreased to 0.06579.
2024-12-01-23:30:24-root-INFO: grad norm: 9.839 9.730 1.466
2024-12-01-23:30:25-root-INFO: grad norm: 12.173 12.052 1.712
2024-12-01-23:30:26-root-INFO: Loss too large (154.164->154.665)! Learning rate decreased to 0.05263.
2024-12-01-23:30:27-root-INFO: grad norm: 10.378 10.270 1.499
2024-12-01-23:30:27-root-INFO: grad norm: 9.100 8.995 1.380
2024-12-01-23:30:28-root-INFO: grad norm: 8.137 8.046 1.214
2024-12-01-23:30:29-root-INFO: Loss Change: 156.754 -> 150.400
2024-12-01-23:30:29-root-INFO: Regularization Change: 0.000 -> 3.435
2024-12-01-23:30:29-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-01-23:30:29-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-23:30:29-root-INFO: step: 97 lr_xt 0.08454965
2024-12-01-23:30:30-root-INFO: grad norm: 5.559 5.407 1.294
2024-12-01-23:30:30-root-INFO: grad norm: 7.385 7.284 1.219
2024-12-01-23:30:31-root-INFO: Loss too large (149.333->149.869)! Learning rate decreased to 0.06764.
2024-12-01-23:30:32-root-INFO: grad norm: 8.731 8.654 1.160
2024-12-01-23:30:32-root-INFO: Loss too large (148.930->148.964)! Learning rate decreased to 0.05411.
2024-12-01-23:30:33-root-INFO: grad norm: 7.567 7.492 1.060
2024-12-01-23:30:34-root-INFO: grad norm: 6.827 6.761 0.950
2024-12-01-23:30:35-root-INFO: grad norm: 6.359 6.293 0.909
2024-12-01-23:30:35-root-INFO: grad norm: 6.011 5.946 0.884
2024-12-01-23:30:36-root-INFO: grad norm: 5.755 5.692 0.850
2024-12-01-23:30:37-root-INFO: Loss Change: 149.910 -> 145.399
2024-12-01-23:30:37-root-INFO: Regularization Change: 0.000 -> 2.149
2024-12-01-23:30:37-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-01-23:30:37-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-23:30:37-root-INFO: step: 96 lr_xt 0.08691815
2024-12-01-23:30:38-root-INFO: grad norm: 4.451 4.348 0.952
2024-12-01-23:30:38-root-INFO: grad norm: 5.984 5.914 0.914
2024-12-01-23:30:39-root-INFO: Loss too large (144.784->145.059)! Learning rate decreased to 0.06953.
2024-12-01-23:30:40-root-INFO: grad norm: 7.240 7.189 0.851
2024-12-01-23:30:41-root-INFO: grad norm: 9.072 9.020 0.971
2024-12-01-23:30:41-root-INFO: Loss too large (144.444->144.692)! Learning rate decreased to 0.05563.
2024-12-01-23:30:42-root-INFO: grad norm: 7.925 7.880 0.841
2024-12-01-23:30:43-root-INFO: grad norm: 7.301 7.250 0.858
2024-12-01-23:30:43-root-INFO: grad norm: 6.868 6.817 0.837
2024-12-01-23:30:44-root-INFO: grad norm: 6.590 6.534 0.854
2024-12-01-23:30:45-root-INFO: Loss Change: 145.274 -> 141.802
2024-12-01-23:30:45-root-INFO: Regularization Change: 0.000 -> 1.912
2024-12-01-23:30:45-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-01-23:30:45-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-23:30:45-root-INFO: step: 95 lr_xt 0.08933865
2024-12-01-23:30:45-root-INFO: grad norm: 5.145 5.045 1.006
2024-12-01-23:30:46-root-INFO: grad norm: 7.626 7.575 0.879
2024-12-01-23:30:47-root-INFO: Loss too large (141.657->142.861)! Learning rate decreased to 0.07147.
2024-12-01-23:30:48-root-INFO: grad norm: 9.393 9.348 0.922
2024-12-01-23:30:48-root-INFO: Loss too large (141.603->142.015)! Learning rate decreased to 0.05718.
2024-12-01-23:30:49-root-INFO: grad norm: 8.030 7.984 0.857
2024-12-01-23:30:50-root-INFO: grad norm: 7.115 7.065 0.841
2024-12-01-23:30:51-root-INFO: grad norm: 6.600 6.546 0.841
2024-12-01-23:30:52-root-INFO: grad norm: 6.245 6.190 0.824
2024-12-01-23:30:52-root-INFO: grad norm: 6.028 5.970 0.833
2024-12-01-23:30:53-root-INFO: Loss Change: 141.780 -> 138.661
2024-12-01-23:30:53-root-INFO: Regularization Change: 0.000 -> 1.631
2024-12-01-23:30:53-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-01-23:30:53-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-23:30:53-root-INFO: step: 94 lr_xt 0.09181181
2024-12-01-23:30:54-root-INFO: grad norm: 5.027 4.928 0.990
2024-12-01-23:30:54-root-INFO: grad norm: 7.714 7.666 0.861
2024-12-01-23:30:55-root-INFO: Loss too large (138.352->139.861)! Learning rate decreased to 0.07345.
2024-12-01-23:30:55-root-INFO: Loss too large (138.352->138.443)! Learning rate decreased to 0.05876.
2024-12-01-23:30:56-root-INFO: grad norm: 6.643 6.599 0.766
2024-12-01-23:30:57-root-INFO: grad norm: 6.108 6.064 0.736
2024-12-01-23:30:58-root-INFO: grad norm: 5.729 5.681 0.740
2024-12-01-23:30:59-root-INFO: grad norm: 5.516 5.467 0.736
2024-12-01-23:30:59-root-INFO: grad norm: 5.378 5.328 0.728
2024-12-01-23:31:00-root-INFO: grad norm: 5.320 5.269 0.735
2024-12-01-23:31:01-root-INFO: Loss Change: 138.364 -> 135.543
2024-12-01-23:31:01-root-INFO: Regularization Change: 0.000 -> 1.432
2024-12-01-23:31:01-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-01-23:31:01-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-23:31:01-root-INFO: step: 93 lr_xt 0.09433829
2024-12-01-23:31:01-root-INFO: grad norm: 5.551 5.425 1.176
2024-12-01-23:31:02-root-INFO: Loss too large (135.470->135.651)! Learning rate decreased to 0.07547.
2024-12-01-23:31:03-root-INFO: grad norm: 6.245 6.196 0.774
2024-12-01-23:31:03-root-INFO: grad norm: 7.931 7.876 0.932
2024-12-01-23:31:04-root-INFO: Loss too large (135.070->135.439)! Learning rate decreased to 0.06038.
2024-12-01-23:31:05-root-INFO: grad norm: 6.961 6.918 0.775
2024-12-01-23:31:05-root-INFO: grad norm: 6.188 6.143 0.750
2024-12-01-23:31:06-root-INFO: grad norm: 5.706 5.662 0.703
2024-12-01-23:31:07-root-INFO: grad norm: 5.340 5.295 0.693
2024-12-01-23:31:08-root-INFO: grad norm: 5.120 5.075 0.679
2024-12-01-23:31:09-root-INFO: Loss Change: 135.470 -> 132.686
2024-12-01-23:31:09-root-INFO: Regularization Change: 0.000 -> 1.367
2024-12-01-23:31:09-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-01-23:31:09-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-23:31:09-root-INFO: step: 92 lr_xt 0.09691873
2024-12-01-23:31:09-root-INFO: grad norm: 5.689 5.590 1.055
2024-12-01-23:31:10-root-INFO: Loss too large (132.871->133.398)! Learning rate decreased to 0.07753.
2024-12-01-23:31:11-root-INFO: grad norm: 6.957 6.887 0.980
2024-12-01-23:31:11-root-INFO: Loss too large (132.724->132.882)! Learning rate decreased to 0.06203.
2024-12-01-23:31:12-root-INFO: grad norm: 6.152 6.077 0.962
2024-12-01-23:31:13-root-INFO: grad norm: 5.595 5.531 0.845
2024-12-01-23:31:13-root-INFO: grad norm: 5.126 5.059 0.828
2024-12-01-23:31:14-root-INFO: grad norm: 4.773 4.713 0.757
2024-12-01-23:31:15-root-INFO: grad norm: 4.485 4.425 0.735
2024-12-01-23:31:16-root-INFO: grad norm: 4.273 4.217 0.690
2024-12-01-23:31:17-root-INFO: Loss Change: 132.871 -> 130.186
2024-12-01-23:31:17-root-INFO: Regularization Change: 0.000 -> 1.242
2024-12-01-23:31:17-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-01-23:31:17-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-23:31:17-root-INFO: step: 91 lr_xt 0.09955376
2024-12-01-23:31:17-root-INFO: grad norm: 5.744 5.639 1.096
2024-12-01-23:31:18-root-INFO: Loss too large (130.415->131.127)! Learning rate decreased to 0.07964.
2024-12-01-23:31:18-root-INFO: grad norm: 7.182 7.109 1.017
2024-12-01-23:31:19-root-INFO: Loss too large (130.362->130.621)! Learning rate decreased to 0.06371.
2024-12-01-23:31:20-root-INFO: grad norm: 6.362 6.289 0.959
2024-12-01-23:31:21-root-INFO: grad norm: 5.769 5.706 0.851
2024-12-01-23:31:22-root-INFO: grad norm: 5.256 5.190 0.831
2024-12-01-23:31:22-root-INFO: grad norm: 4.848 4.787 0.766
2024-12-01-23:31:23-root-INFO: grad norm: 4.492 4.429 0.747
2024-12-01-23:31:24-root-INFO: grad norm: 4.201 4.142 0.700
2024-12-01-23:31:25-root-INFO: Loss Change: 130.415 -> 127.841
2024-12-01-23:31:25-root-INFO: Regularization Change: 0.000 -> 1.202
2024-12-01-23:31:25-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-01-23:31:25-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-23:31:25-root-INFO: step: 90 lr_xt 0.10224402
2024-12-01-23:31:25-root-INFO: grad norm: 6.822 6.687 1.349
2024-12-01-23:31:26-root-INFO: Loss too large (128.408->129.868)! Learning rate decreased to 0.08180.
2024-12-01-23:31:26-root-INFO: Loss too large (128.408->128.605)! Learning rate decreased to 0.06544.
2024-12-01-23:31:27-root-INFO: grad norm: 6.164 6.104 0.864
2024-12-01-23:31:28-root-INFO: grad norm: 5.861 5.789 0.917
2024-12-01-23:31:29-root-INFO: grad norm: 5.756 5.701 0.798
2024-12-01-23:31:29-root-INFO: grad norm: 5.773 5.712 0.834
2024-12-01-23:31:30-root-INFO: grad norm: 5.930 5.880 0.772
2024-12-01-23:31:31-root-INFO: grad norm: 6.160 6.106 0.817
2024-12-01-23:31:32-root-INFO: grad norm: 6.500 6.453 0.781
2024-12-01-23:31:32-root-INFO: Loss Change: 128.408 -> 126.216
2024-12-01-23:31:32-root-INFO: Regularization Change: 0.000 -> 1.155
2024-12-01-23:31:32-root-INFO: Undo step: 90
2024-12-01-23:31:32-root-INFO: Undo step: 91
2024-12-01-23:31:32-root-INFO: Undo step: 92
2024-12-01-23:31:32-root-INFO: Undo step: 93
2024-12-01-23:31:32-root-INFO: Undo step: 94
2024-12-01-23:31:33-root-INFO: step: 95 lr_xt 0.08933865
2024-12-01-23:31:33-root-INFO: grad norm: 49.672 48.563 10.441
2024-12-01-23:31:34-root-INFO: grad norm: 33.977 33.503 5.653
2024-12-01-23:31:35-root-INFO: grad norm: 33.270 32.889 5.020
2024-12-01-23:31:36-root-INFO: grad norm: 36.110 35.877 4.097
2024-12-01-23:31:36-root-INFO: Loss too large (196.984->201.263)! Learning rate decreased to 0.07147.
2024-12-01-23:31:37-root-INFO: grad norm: 29.788 29.604 3.309
2024-12-01-23:31:37-root-INFO: grad norm: 27.624 27.361 3.807
2024-12-01-23:31:38-root-INFO: grad norm: 29.077 28.851 3.620
2024-12-01-23:31:39-root-INFO: grad norm: 30.362 30.004 4.651
2024-12-01-23:31:40-root-INFO: Loss Change: 331.311 -> 169.096
2024-12-01-23:31:40-root-INFO: Regularization Change: 0.000 -> 68.105
2024-12-01-23:31:40-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-01-23:31:40-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-23:31:40-root-INFO: step: 94 lr_xt 0.09181181
2024-12-01-23:31:40-root-INFO: grad norm: 35.225 34.884 4.885
2024-12-01-23:31:40-root-INFO: Loss too large (173.093->197.338)! Learning rate decreased to 0.07345.
2024-12-01-23:31:41-root-INFO: Loss too large (173.093->173.996)! Learning rate decreased to 0.05876.
2024-12-01-23:31:42-root-INFO: grad norm: 23.696 23.374 3.896
2024-12-01-23:31:43-root-INFO: grad norm: 16.365 16.212 2.231
2024-12-01-23:31:43-root-INFO: grad norm: 12.716 12.532 2.159
2024-12-01-23:31:44-root-INFO: grad norm: 10.313 10.199 1.529
2024-12-01-23:31:45-root-INFO: grad norm: 8.714 8.570 1.573
2024-12-01-23:31:46-root-INFO: grad norm: 7.566 7.466 1.228
2024-12-01-23:31:47-root-INFO: grad norm: 6.722 6.598 1.281
2024-12-01-23:31:47-root-INFO: Loss Change: 173.093 -> 141.743
2024-12-01-23:31:47-root-INFO: Regularization Change: 0.000 -> 5.911
2024-12-01-23:31:47-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-01-23:31:47-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-23:31:48-root-INFO: step: 93 lr_xt 0.09433829
2024-12-01-23:31:48-root-INFO: grad norm: 9.688 9.520 1.800
2024-12-01-23:31:48-root-INFO: Loss too large (142.364->143.985)! Learning rate decreased to 0.07547.
2024-12-01-23:31:49-root-INFO: grad norm: 11.852 11.700 1.895
2024-12-01-23:31:49-root-INFO: Loss too large (142.011->142.519)! Learning rate decreased to 0.06038.
2024-12-01-23:31:50-root-INFO: grad norm: 10.231 10.125 1.469
2024-12-01-23:31:51-root-INFO: grad norm: 9.037 8.923 1.432
2024-12-01-23:31:52-root-INFO: grad norm: 8.091 8.004 1.182
2024-12-01-23:31:53-root-INFO: grad norm: 7.330 7.232 1.195
2024-12-01-23:31:54-root-INFO: grad norm: 6.726 6.648 1.020
2024-12-01-23:31:55-root-INFO: grad norm: 6.224 6.136 1.044
2024-12-01-23:31:55-root-INFO: Loss Change: 142.364 -> 135.695
2024-12-01-23:31:55-root-INFO: Regularization Change: 0.000 -> 2.882
2024-12-01-23:31:55-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-01-23:31:55-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-23:31:56-root-INFO: step: 92 lr_xt 0.09691873
2024-12-01-23:31:56-root-INFO: grad norm: 8.536 8.417 1.424
2024-12-01-23:31:56-root-INFO: Loss too large (136.347->138.220)! Learning rate decreased to 0.07753.
2024-12-01-23:31:56-root-INFO: Loss too large (136.347->136.428)! Learning rate decreased to 0.06203.
2024-12-01-23:31:57-root-INFO: grad norm: 7.690 7.591 1.229
2024-12-01-23:31:58-root-INFO: grad norm: 7.096 7.016 1.058
2024-12-01-23:31:59-root-INFO: grad norm: 6.619 6.531 1.077
2024-12-01-23:32:00-root-INFO: grad norm: 6.223 6.153 0.934
2024-12-01-23:32:01-root-INFO: grad norm: 5.887 5.807 0.970
2024-12-01-23:32:02-root-INFO: grad norm: 5.605 5.540 0.857
2024-12-01-23:32:02-root-INFO: grad norm: 5.361 5.287 0.892
2024-12-01-23:32:03-root-INFO: Loss Change: 136.347 -> 131.626
2024-12-01-23:32:03-root-INFO: Regularization Change: 0.000 -> 1.993
2024-12-01-23:32:03-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-01-23:32:03-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-23:32:03-root-INFO: step: 91 lr_xt 0.09955376
2024-12-01-23:32:03-root-INFO: grad norm: 8.342 8.213 1.461
2024-12-01-23:32:04-root-INFO: Loss too large (132.258->134.536)! Learning rate decreased to 0.07964.
2024-12-01-23:32:04-root-INFO: Loss too large (132.258->132.635)! Learning rate decreased to 0.06371.
2024-12-01-23:32:05-root-INFO: grad norm: 7.715 7.625 1.173
2024-12-01-23:32:06-root-INFO: grad norm: 7.264 7.187 1.056
2024-12-01-23:32:07-root-INFO: grad norm: 6.918 6.840 1.032
2024-12-01-23:32:08-root-INFO: grad norm: 6.611 6.544 0.940
2024-12-01-23:32:08-root-INFO: grad norm: 6.346 6.276 0.944
2024-12-01-23:32:09-root-INFO: grad norm: 6.115 6.053 0.870
2024-12-01-23:32:10-root-INFO: grad norm: 5.909 5.843 0.880
2024-12-01-23:32:11-root-INFO: Loss Change: 132.258 -> 128.477
2024-12-01-23:32:11-root-INFO: Regularization Change: 0.000 -> 1.636
2024-12-01-23:32:11-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-01-23:32:11-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-23:32:11-root-INFO: step: 90 lr_xt 0.10224402
2024-12-01-23:32:11-root-INFO: grad norm: 8.682 8.551 1.504
2024-12-01-23:32:11-root-INFO: Loss too large (129.198->131.965)! Learning rate decreased to 0.08180.
2024-12-01-23:32:12-root-INFO: Loss too large (129.198->129.772)! Learning rate decreased to 0.06544.
2024-12-01-23:32:13-root-INFO: grad norm: 8.032 7.952 1.129
2024-12-01-23:32:13-root-INFO: grad norm: 7.577 7.502 1.064
2024-12-01-23:32:14-root-INFO: grad norm: 7.217 7.147 1.004
2024-12-01-23:32:15-root-INFO: grad norm: 6.895 6.830 0.943
2024-12-01-23:32:16-root-INFO: grad norm: 6.609 6.544 0.921
2024-12-01-23:32:17-root-INFO: grad norm: 6.359 6.299 0.869
2024-12-01-23:32:18-root-INFO: grad norm: 6.132 6.071 0.860
2024-12-01-23:32:18-root-INFO: Loss Change: 129.198 -> 125.735
2024-12-01-23:32:18-root-INFO: Regularization Change: 0.000 -> 1.466
2024-12-01-23:32:18-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-01-23:32:18-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-23:32:19-root-INFO: step: 89 lr_xt 0.10499012
2024-12-01-23:32:19-root-INFO: grad norm: 10.326 10.163 1.832
2024-12-01-23:32:19-root-INFO: Loss too large (126.758->131.218)! Learning rate decreased to 0.08399.
2024-12-01-23:32:20-root-INFO: Loss too large (126.758->127.976)! Learning rate decreased to 0.06719.
2024-12-01-23:32:20-root-INFO: grad norm: 9.529 9.430 1.371
2024-12-01-23:32:21-root-INFO: grad norm: 8.872 8.786 1.231
2024-12-01-23:32:22-root-INFO: grad norm: 8.450 8.367 1.180
2024-12-01-23:32:23-root-INFO: grad norm: 8.071 7.998 1.084
2024-12-01-23:32:24-root-INFO: grad norm: 7.731 7.655 1.079
2024-12-01-23:32:25-root-INFO: grad norm: 7.427 7.360 0.996
2024-12-01-23:32:26-root-INFO: grad norm: 7.155 7.085 1.003
2024-12-01-23:32:26-root-INFO: Loss Change: 126.758 -> 123.190
2024-12-01-23:32:26-root-INFO: Regularization Change: 0.000 -> 1.394
2024-12-01-23:32:26-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-01-23:32:26-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-23:32:26-root-INFO: step: 88 lr_xt 0.10779268
2024-12-01-23:32:27-root-INFO: grad norm: 9.487 9.366 1.510
2024-12-01-23:32:27-root-INFO: Loss too large (124.185->128.070)! Learning rate decreased to 0.08623.
2024-12-01-23:32:27-root-INFO: Loss too large (124.185->125.259)! Learning rate decreased to 0.06899.
2024-12-01-23:32:28-root-INFO: grad norm: 8.738 8.657 1.181
2024-12-01-23:32:29-root-INFO: grad norm: 8.026 7.953 1.083
2024-12-01-23:32:30-root-INFO: grad norm: 7.562 7.496 1.000
2024-12-01-23:32:31-root-INFO: grad norm: 7.166 7.103 0.947
2024-12-01-23:32:32-root-INFO: grad norm: 6.806 6.745 0.910
2024-12-01-23:32:33-root-INFO: grad norm: 6.501 6.443 0.864
2024-12-01-23:32:33-root-INFO: grad norm: 6.225 6.168 0.844
2024-12-01-23:32:34-root-INFO: Loss Change: 124.185 -> 120.902
2024-12-01-23:32:34-root-INFO: Regularization Change: 0.000 -> 1.305
2024-12-01-23:32:34-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-01-23:32:34-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-23:32:34-root-INFO: step: 87 lr_xt 0.11065228
2024-12-01-23:32:34-root-INFO: grad norm: 9.867 9.712 1.743
2024-12-01-23:32:35-root-INFO: Loss too large (121.784->125.976)! Learning rate decreased to 0.08852.
2024-12-01-23:32:35-root-INFO: Loss too large (121.784->122.946)! Learning rate decreased to 0.07082.
2024-12-01-23:32:36-root-INFO: grad norm: 8.903 8.817 1.232
2024-12-01-23:32:37-root-INFO: grad norm: 8.073 7.996 1.112
2024-12-01-23:32:38-root-INFO: grad norm: 7.623 7.554 1.025
2024-12-01-23:32:39-root-INFO: grad norm: 7.272 7.208 0.968
2024-12-01-23:32:39-root-INFO: grad norm: 6.937 6.873 0.940
2024-12-01-23:32:40-root-INFO: grad norm: 6.645 6.585 0.885
2024-12-01-23:32:41-root-INFO: grad norm: 6.390 6.330 0.874
2024-12-01-23:32:42-root-INFO: Loss Change: 121.784 -> 118.443
2024-12-01-23:32:42-root-INFO: Regularization Change: 0.000 -> 1.299
2024-12-01-23:32:42-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-01-23:32:42-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-23:32:42-root-INFO: step: 86 lr_xt 0.11356952
2024-12-01-23:32:42-root-INFO: grad norm: 9.571 9.432 1.625
2024-12-01-23:32:43-root-INFO: Loss too large (119.441->123.501)! Learning rate decreased to 0.09086.
2024-12-01-23:32:43-root-INFO: Loss too large (119.441->120.645)! Learning rate decreased to 0.07268.
2024-12-01-23:32:44-root-INFO: grad norm: 8.557 8.475 1.183
2024-12-01-23:32:45-root-INFO: grad norm: 7.554 7.485 1.024
2024-12-01-23:32:46-root-INFO: grad norm: 7.108 7.046 0.937
2024-12-01-23:32:46-root-INFO: grad norm: 6.819 6.761 0.893
2024-12-01-23:32:47-root-INFO: grad norm: 6.511 6.453 0.873
2024-12-01-23:32:48-root-INFO: grad norm: 6.270 6.216 0.826
2024-12-01-23:32:49-root-INFO: grad norm: 6.034 5.978 0.821
2024-12-01-23:32:50-root-INFO: Loss Change: 119.441 -> 116.255
2024-12-01-23:32:50-root-INFO: Regularization Change: 0.000 -> 1.266
2024-12-01-23:32:50-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-01-23:32:50-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-23:32:50-root-INFO: step: 85 lr_xt 0.11654496
2024-12-01-23:32:50-root-INFO: grad norm: 8.656 8.533 1.454
2024-12-01-23:32:50-root-INFO: Loss too large (116.961->120.348)! Learning rate decreased to 0.09324.
2024-12-01-23:32:51-root-INFO: Loss too large (116.961->117.952)! Learning rate decreased to 0.07459.
2024-12-01-23:32:52-root-INFO: grad norm: 7.845 7.772 1.071
2024-12-01-23:32:52-root-INFO: grad norm: 7.061 6.995 0.966
2024-12-01-23:32:53-root-INFO: grad norm: 6.727 6.667 0.896
2024-12-01-23:32:54-root-INFO: grad norm: 6.540 6.482 0.866
2024-12-01-23:32:55-root-INFO: grad norm: 6.279 6.222 0.848
2024-12-01-23:32:56-root-INFO: grad norm: 6.039 5.985 0.803
2024-12-01-23:32:57-root-INFO: grad norm: 5.849 5.794 0.797
2024-12-01-23:32:57-root-INFO: Loss Change: 116.961 -> 114.068
2024-12-01-23:32:57-root-INFO: Regularization Change: 0.000 -> 1.259
2024-12-01-23:32:57-root-INFO: Undo step: 85
2024-12-01-23:32:57-root-INFO: Undo step: 86
2024-12-01-23:32:57-root-INFO: Undo step: 87
2024-12-01-23:32:57-root-INFO: Undo step: 88
2024-12-01-23:32:57-root-INFO: Undo step: 89
2024-12-01-23:32:58-root-INFO: step: 90 lr_xt 0.10224402
2024-12-01-23:32:58-root-INFO: grad norm: 47.118 46.401 8.190
2024-12-01-23:32:59-root-INFO: grad norm: 30.047 29.636 4.953
2024-12-01-23:33:00-root-INFO: grad norm: 28.623 28.332 4.073
2024-12-01-23:33:01-root-INFO: grad norm: 37.267 36.958 4.795
2024-12-01-23:33:01-root-INFO: Loss too large (180.091->198.443)! Learning rate decreased to 0.08180.
2024-12-01-23:33:02-root-INFO: grad norm: 32.648 32.286 4.845
2024-12-01-23:33:03-root-INFO: grad norm: 28.648 28.369 3.990
2024-12-01-23:33:04-root-INFO: grad norm: 27.412 27.070 4.313
2024-12-01-23:33:04-root-INFO: grad norm: 27.005 26.732 3.831
2024-12-01-23:33:05-root-INFO: Loss Change: 301.463 -> 151.596
2024-12-01-23:33:05-root-INFO: Regularization Change: 0.000 -> 65.490
2024-12-01-23:33:05-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-01-23:33:05-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-23:33:05-root-INFO: step: 89 lr_xt 0.10499012
2024-12-01-23:33:06-root-INFO: grad norm: 24.245 24.002 3.430
2024-12-01-23:33:06-root-INFO: Loss too large (148.189->159.995)! Learning rate decreased to 0.08399.
2024-12-01-23:33:07-root-INFO: grad norm: 24.807 24.542 3.612
2024-12-01-23:33:07-root-INFO: Loss too large (145.447->146.035)! Learning rate decreased to 0.06719.
2024-12-01-23:33:08-root-INFO: grad norm: 16.898 16.679 2.708
2024-12-01-23:33:09-root-INFO: grad norm: 11.785 11.645 1.809
2024-12-01-23:33:09-root-INFO: grad norm: 9.224 9.095 1.533
2024-12-01-23:33:10-root-INFO: grad norm: 7.500 7.395 1.251
2024-12-01-23:33:11-root-INFO: grad norm: 6.381 6.281 1.122
2024-12-01-23:33:12-root-INFO: grad norm: 5.551 5.461 0.998
2024-12-01-23:33:13-root-INFO: Loss Change: 148.189 -> 126.618
2024-12-01-23:33:13-root-INFO: Regularization Change: 0.000 -> 5.584
2024-12-01-23:33:13-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-01-23:33:13-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-23:33:13-root-INFO: step: 88 lr_xt 0.10779268
2024-12-01-23:33:13-root-INFO: grad norm: 3.996 3.909 0.830
2024-12-01-23:33:14-root-INFO: grad norm: 4.552 4.481 0.803
2024-12-01-23:33:15-root-INFO: grad norm: 6.573 6.502 0.960
2024-12-01-23:33:15-root-INFO: Loss too large (125.013->125.600)! Learning rate decreased to 0.08623.
2024-12-01-23:33:16-root-INFO: grad norm: 7.992 7.910 1.141
2024-12-01-23:33:17-root-INFO: Loss too large (124.634->124.655)! Learning rate decreased to 0.06899.
2024-12-01-23:33:18-root-INFO: grad norm: 6.835 6.759 1.020
2024-12-01-23:33:18-root-INFO: grad norm: 5.961 5.886 0.943
2024-12-01-23:33:19-root-INFO: grad norm: 5.345 5.276 0.859
2024-12-01-23:33:20-root-INFO: grad norm: 4.847 4.778 0.817
2024-12-01-23:33:21-root-INFO: Loss Change: 126.639 -> 121.286
2024-12-01-23:33:21-root-INFO: Regularization Change: 0.000 -> 3.551
2024-12-01-23:33:21-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-01-23:33:21-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-23:33:21-root-INFO: step: 87 lr_xt 0.11065228
2024-12-01-23:33:21-root-INFO: grad norm: 3.821 3.692 0.984
2024-12-01-23:33:22-root-INFO: grad norm: 3.875 3.816 0.673
2024-12-01-23:33:23-root-INFO: grad norm: 5.020 4.972 0.695
2024-12-01-23:33:24-root-INFO: grad norm: 7.259 7.213 0.824
2024-12-01-23:33:24-root-INFO: Loss too large (119.548->120.428)! Learning rate decreased to 0.08852.
2024-12-01-23:33:25-root-INFO: grad norm: 7.670 7.632 0.761
2024-12-01-23:33:26-root-INFO: grad norm: 8.263 8.221 0.825
2024-12-01-23:33:27-root-INFO: grad norm: 8.870 8.836 0.784
2024-12-01-23:33:28-root-INFO: grad norm: 9.367 9.326 0.885
2024-12-01-23:33:28-root-INFO: Loss Change: 120.896 -> 118.301
2024-12-01-23:33:28-root-INFO: Regularization Change: 0.000 -> 3.611
2024-12-01-23:33:28-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-01-23:33:28-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-23:33:29-root-INFO: step: 86 lr_xt 0.11356952
2024-12-01-23:33:29-root-INFO: grad norm: 9.984 9.926 1.073
2024-12-01-23:33:29-root-INFO: Loss too large (118.221->120.498)! Learning rate decreased to 0.09086.
2024-12-01-23:33:30-root-INFO: grad norm: 10.119 10.070 0.995
2024-12-01-23:33:31-root-INFO: grad norm: 10.199 10.160 0.894
2024-12-01-23:33:32-root-INFO: grad norm: 10.399 10.349 1.016
2024-12-01-23:33:33-root-INFO: grad norm: 10.404 10.362 0.925
2024-12-01-23:33:33-root-INFO: grad norm: 10.410 10.359 1.027
2024-12-01-23:33:34-root-INFO: grad norm: 10.506 10.463 0.946
2024-12-01-23:33:35-root-INFO: grad norm: 10.715 10.660 1.083
2024-12-01-23:33:36-root-INFO: Loss Change: 118.221 -> 115.559
2024-12-01-23:33:36-root-INFO: Regularization Change: 0.000 -> 2.480
2024-12-01-23:33:36-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-01-23:33:36-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-23:33:36-root-INFO: step: 85 lr_xt 0.11654496
2024-12-01-23:33:36-root-INFO: grad norm: 10.782 10.713 1.218
2024-12-01-23:33:37-root-INFO: Loss too large (115.465->118.060)! Learning rate decreased to 0.09324.
2024-12-01-23:33:37-root-INFO: grad norm: 10.671 10.609 1.154
2024-12-01-23:33:38-root-INFO: grad norm: 10.723 10.670 1.067
2024-12-01-23:33:39-root-INFO: grad norm: 10.970 10.907 1.168
2024-12-01-23:33:40-root-INFO: grad norm: 10.908 10.854 1.082
2024-12-01-23:33:41-root-INFO: grad norm: 10.704 10.643 1.139
2024-12-01-23:33:42-root-INFO: grad norm: 10.746 10.693 1.063
2024-12-01-23:33:42-root-INFO: grad norm: 10.951 10.887 1.175
2024-12-01-23:33:43-root-INFO: Loss Change: 115.465 -> 112.717
2024-12-01-23:33:43-root-INFO: Regularization Change: 0.000 -> 2.186
2024-12-01-23:33:43-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-01-23:33:43-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-23:33:43-root-INFO: step: 84 lr_xt 0.11957917
2024-12-01-23:33:44-root-INFO: grad norm: 11.176 11.101 1.296
2024-12-01-23:33:44-root-INFO: Loss too large (112.746->115.639)! Learning rate decreased to 0.09566.
2024-12-01-23:33:45-root-INFO: grad norm: 10.952 10.872 1.322
2024-12-01-23:33:46-root-INFO: grad norm: 10.966 10.892 1.273
2024-12-01-23:33:47-root-INFO: grad norm: 11.169 11.083 1.384
2024-12-01-23:33:47-root-INFO: grad norm: 11.141 11.061 1.334
2024-12-01-23:33:48-root-INFO: grad norm: 11.094 10.999 1.449
2024-12-01-23:33:49-root-INFO: grad norm: 11.234 11.143 1.424
2024-12-01-23:33:50-root-INFO: grad norm: 11.467 11.360 1.562
2024-12-01-23:33:51-root-INFO: Loss Change: 112.746 -> 110.360
2024-12-01-23:33:51-root-INFO: Regularization Change: 0.000 -> 2.073
2024-12-01-23:33:51-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-01-23:33:51-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-23:33:51-root-INFO: step: 83 lr_xt 0.12267269
2024-12-01-23:33:51-root-INFO: grad norm: 13.714 13.547 2.133
2024-12-01-23:33:52-root-INFO: Loss too large (111.547->116.873)! Learning rate decreased to 0.09814.
2024-12-01-23:33:52-root-INFO: grad norm: 13.857 13.695 2.114
2024-12-01-23:33:53-root-INFO: grad norm: 14.121 13.972 2.044
2024-12-01-23:33:54-root-INFO: Loss too large (111.120->111.254)! Learning rate decreased to 0.07851.
2024-12-01-23:33:55-root-INFO: grad norm: 9.239 9.122 1.468
2024-12-01-23:33:55-root-INFO: grad norm: 6.390 6.323 0.927
2024-12-01-23:33:56-root-INFO: grad norm: 4.787 4.727 0.758
2024-12-01-23:33:57-root-INFO: grad norm: 3.777 3.734 0.566
2024-12-01-23:33:58-root-INFO: grad norm: 3.129 3.088 0.506
2024-12-01-23:33:59-root-INFO: Loss Change: 111.547 -> 104.961
2024-12-01-23:33:59-root-INFO: Regularization Change: 0.000 -> 1.785
2024-12-01-23:33:59-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-01-23:33:59-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-23:33:59-root-INFO: step: 82 lr_xt 0.12582604
2024-12-01-23:33:59-root-INFO: grad norm: 4.480 4.395 0.866
2024-12-01-23:33:59-root-INFO: Loss too large (105.352->105.709)! Learning rate decreased to 0.10066.
2024-12-01-23:34:00-root-INFO: grad norm: 5.311 5.242 0.852
2024-12-01-23:34:01-root-INFO: Loss too large (105.180->105.281)! Learning rate decreased to 0.08053.
2024-12-01-23:34:01-root-INFO: grad norm: 4.825 4.766 0.750
2024-12-01-23:34:02-root-INFO: grad norm: 4.517 4.462 0.702
2024-12-01-23:34:03-root-INFO: grad norm: 4.634 4.583 0.690
2024-12-01-23:34:04-root-INFO: grad norm: 4.356 4.304 0.673
2024-12-01-23:34:05-root-INFO: grad norm: 3.969 3.922 0.610
2024-12-01-23:34:06-root-INFO: grad norm: 3.892 3.845 0.603
2024-12-01-23:34:06-root-INFO: Loss Change: 105.352 -> 103.190
2024-12-01-23:34:06-root-INFO: Regularization Change: 0.000 -> 1.321
2024-12-01-23:34:06-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-01-23:34:06-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-23:34:07-root-INFO: step: 81 lr_xt 0.12903975
2024-12-01-23:34:07-root-INFO: grad norm: 6.451 6.325 1.267
2024-12-01-23:34:07-root-INFO: Loss too large (103.543->105.067)! Learning rate decreased to 0.10323.
2024-12-01-23:34:07-root-INFO: Loss too large (103.543->104.002)! Learning rate decreased to 0.08259.
2024-12-01-23:34:08-root-INFO: grad norm: 5.277 5.215 0.812
2024-12-01-23:34:09-root-INFO: grad norm: 3.876 3.820 0.661
2024-12-01-23:34:10-root-INFO: grad norm: 3.710 3.666 0.568
2024-12-01-23:34:11-root-INFO: grad norm: 4.052 4.006 0.611
2024-12-01-23:34:12-root-INFO: grad norm: 3.985 3.940 0.601
2024-12-01-23:34:13-root-INFO: grad norm: 3.969 3.924 0.591
2024-12-01-23:34:14-root-INFO: grad norm: 3.889 3.844 0.588
2024-12-01-23:34:14-root-INFO: Loss Change: 103.543 -> 101.131
2024-12-01-23:34:14-root-INFO: Regularization Change: 0.000 -> 1.250
2024-12-01-23:34:14-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-01-23:34:14-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-23:34:14-root-INFO: step: 80 lr_xt 0.13231432
2024-12-01-23:34:15-root-INFO: grad norm: 7.157 7.020 1.395
2024-12-01-23:34:15-root-INFO: Loss too large (101.942->103.853)! Learning rate decreased to 0.10585.
2024-12-01-23:34:15-root-INFO: Loss too large (101.942->102.574)! Learning rate decreased to 0.08468.
2024-12-01-23:34:16-root-INFO: grad norm: 5.561 5.495 0.856
2024-12-01-23:34:17-root-INFO: grad norm: 3.772 3.719 0.627
2024-12-01-23:34:18-root-INFO: grad norm: 3.338 3.299 0.507
2024-12-01-23:34:19-root-INFO: grad norm: 3.352 3.312 0.519
2024-12-01-23:34:20-root-INFO: grad norm: 3.465 3.426 0.521
2024-12-01-23:34:20-root-INFO: grad norm: 4.101 4.061 0.577
2024-12-01-23:34:21-root-INFO: grad norm: 4.036 3.992 0.598
2024-12-01-23:34:22-root-INFO: Loss Change: 101.942 -> 99.393
2024-12-01-23:34:22-root-INFO: Regularization Change: 0.000 -> 1.264
2024-12-01-23:34:22-root-INFO: Undo step: 80
2024-12-01-23:34:22-root-INFO: Undo step: 81
2024-12-01-23:34:22-root-INFO: Undo step: 82
2024-12-01-23:34:22-root-INFO: Undo step: 83
2024-12-01-23:34:22-root-INFO: Undo step: 84
2024-12-01-23:34:22-root-INFO: step: 85 lr_xt 0.11654496
2024-12-01-23:34:22-root-INFO: grad norm: 42.176 41.507 7.486
2024-12-01-23:34:23-root-INFO: grad norm: 32.433 31.981 5.398
2024-12-01-23:34:24-root-INFO: grad norm: 29.720 29.055 6.255
2024-12-01-23:34:25-root-INFO: grad norm: 33.038 32.619 5.243
2024-12-01-23:34:25-root-INFO: Loss too large (160.910->172.185)! Learning rate decreased to 0.09324.
2024-12-01-23:34:26-root-INFO: grad norm: 26.724 26.310 4.688
2024-12-01-23:34:27-root-INFO: grad norm: 21.689 21.479 3.012
2024-12-01-23:34:28-root-INFO: grad norm: 19.718 19.468 3.131
2024-12-01-23:34:29-root-INFO: grad norm: 18.517 18.334 2.594
2024-12-01-23:34:29-root-INFO: Loss Change: 287.514 -> 127.674
2024-12-01-23:34:29-root-INFO: Regularization Change: 0.000 -> 75.665
2024-12-01-23:34:29-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-01-23:34:29-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-23:34:30-root-INFO: step: 84 lr_xt 0.11957917
2024-12-01-23:34:30-root-INFO: grad norm: 16.394 16.232 2.298
2024-12-01-23:34:30-root-INFO: Loss too large (126.329->131.005)! Learning rate decreased to 0.09566.
2024-12-01-23:34:31-root-INFO: grad norm: 16.076 15.913 2.282
2024-12-01-23:34:32-root-INFO: grad norm: 16.017 15.829 2.444
2024-12-01-23:34:33-root-INFO: grad norm: 16.403 16.226 2.404
2024-12-01-23:34:34-root-INFO: grad norm: 16.240 16.039 2.551
2024-12-01-23:34:35-root-INFO: grad norm: 15.944 15.777 2.299
2024-12-01-23:34:36-root-INFO: grad norm: 15.900 15.714 2.424
2024-12-01-23:34:36-root-INFO: grad norm: 16.343 16.169 2.382
2024-12-01-23:34:37-root-INFO: Loss Change: 126.329 -> 117.808
2024-12-01-23:34:37-root-INFO: Regularization Change: 0.000 -> 6.751
2024-12-01-23:34:37-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-01-23:34:37-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-23:34:37-root-INFO: step: 83 lr_xt 0.12267269
2024-12-01-23:34:38-root-INFO: grad norm: 14.213 14.079 1.945
2024-12-01-23:34:38-root-INFO: Loss too large (116.148->120.411)! Learning rate decreased to 0.09814.
2024-12-01-23:34:39-root-INFO: grad norm: 13.948 13.808 1.974
2024-12-01-23:34:40-root-INFO: grad norm: 14.038 13.884 2.075
2024-12-01-23:34:40-root-INFO: grad norm: 14.762 14.609 2.124
2024-12-01-23:34:41-root-INFO: Loss too large (113.790->113.941)! Learning rate decreased to 0.07851.
2024-12-01-23:34:42-root-INFO: grad norm: 9.555 9.431 1.535
2024-12-01-23:34:43-root-INFO: grad norm: 6.158 6.091 0.906
2024-12-01-23:34:43-root-INFO: grad norm: 4.442 4.382 0.728
2024-12-01-23:34:44-root-INFO: grad norm: 3.549 3.501 0.584
2024-12-01-23:34:45-root-INFO: Loss Change: 116.148 -> 106.668
2024-12-01-23:34:45-root-INFO: Regularization Change: 0.000 -> 3.339
2024-12-01-23:34:45-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-01-23:34:45-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-23:34:45-root-INFO: step: 82 lr_xt 0.12582604
2024-12-01-23:34:46-root-INFO: grad norm: 2.781 2.719 0.586
2024-12-01-23:34:47-root-INFO: grad norm: 2.694 2.645 0.510
2024-12-01-23:34:47-root-INFO: grad norm: 3.109 3.062 0.541
2024-12-01-23:34:48-root-INFO: grad norm: 4.535 4.487 0.663
2024-12-01-23:34:49-root-INFO: Loss too large (105.122->105.334)! Learning rate decreased to 0.10066.
2024-12-01-23:34:49-root-INFO: grad norm: 4.844 4.787 0.738
2024-12-01-23:34:50-root-INFO: grad norm: 5.655 5.600 0.789
2024-12-01-23:34:51-root-INFO: Loss too large (104.450->104.539)! Learning rate decreased to 0.08053.
2024-12-01-23:34:52-root-INFO: grad norm: 4.441 4.385 0.701
2024-12-01-23:34:52-root-INFO: grad norm: 3.080 3.036 0.519
2024-12-01-23:34:53-root-INFO: Loss Change: 106.743 -> 103.072
2024-12-01-23:34:53-root-INFO: Regularization Change: 0.000 -> 3.032
2024-12-01-23:34:53-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-01-23:34:53-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-23:34:53-root-INFO: step: 81 lr_xt 0.12903975
2024-12-01-23:34:54-root-INFO: grad norm: 3.135 3.052 0.715
2024-12-01-23:34:54-root-INFO: grad norm: 3.836 3.788 0.605
2024-12-01-23:34:55-root-INFO: grad norm: 5.067 5.010 0.758
2024-12-01-23:34:56-root-INFO: Loss too large (102.423->102.538)! Learning rate decreased to 0.10323.
2024-12-01-23:34:57-root-INFO: grad norm: 5.253 5.205 0.713
2024-12-01-23:34:57-root-INFO: grad norm: 5.071 5.018 0.734
2024-12-01-23:34:58-root-INFO: grad norm: 4.635 4.587 0.662
2024-12-01-23:34:59-root-INFO: grad norm: 4.826 4.774 0.700
2024-12-01-23:35:00-root-INFO: grad norm: 5.670 5.626 0.704
2024-12-01-23:35:00-root-INFO: Loss too large (100.520->100.643)! Learning rate decreased to 0.08259.
2024-12-01-23:35:01-root-INFO: Loss Change: 102.905 -> 100.233
2024-12-01-23:35:01-root-INFO: Regularization Change: 0.000 -> 2.636
2024-12-01-23:35:01-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-01-23:35:01-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-23:35:01-root-INFO: step: 80 lr_xt 0.13231432
2024-12-01-23:35:02-root-INFO: grad norm: 5.951 5.866 1.003
2024-12-01-23:35:02-root-INFO: Loss too large (100.703->101.218)! Learning rate decreased to 0.10585.
2024-12-01-23:35:03-root-INFO: grad norm: 6.281 6.231 0.789
2024-12-01-23:35:03-root-INFO: Loss too large (100.192->100.264)! Learning rate decreased to 0.08468.
2024-12-01-23:35:04-root-INFO: grad norm: 4.776 4.739 0.600
2024-12-01-23:35:05-root-INFO: grad norm: 3.705 3.676 0.466
2024-12-01-23:35:06-root-INFO: grad norm: 3.300 3.269 0.453
2024-12-01-23:35:07-root-INFO: grad norm: 3.241 3.214 0.419
2024-12-01-23:35:08-root-INFO: grad norm: 3.164 3.131 0.459
2024-12-01-23:35:08-root-INFO: grad norm: 3.247 3.218 0.432
2024-12-01-23:35:09-root-INFO: Loss Change: 100.703 -> 97.806
2024-12-01-23:35:09-root-INFO: Regularization Change: 0.000 -> 1.571
2024-12-01-23:35:09-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-01-23:35:09-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-23:35:09-root-INFO: step: 79 lr_xt 0.13565022
2024-12-01-23:35:10-root-INFO: grad norm: 4.003 3.942 0.697
2024-12-01-23:35:10-root-INFO: Loss too large (97.796->97.961)! Learning rate decreased to 0.10852.
2024-12-01-23:35:11-root-INFO: grad norm: 5.346 5.312 0.598
2024-12-01-23:35:11-root-INFO: Loss too large (97.478->97.770)! Learning rate decreased to 0.08682.
2024-12-01-23:35:12-root-INFO: grad norm: 4.045 4.012 0.516
2024-12-01-23:35:13-root-INFO: grad norm: 2.280 2.249 0.374
2024-12-01-23:35:14-root-INFO: grad norm: 2.118 2.087 0.364
2024-12-01-23:35:15-root-INFO: grad norm: 2.290 2.261 0.365
2024-12-01-23:35:15-root-INFO: grad norm: 2.519 2.488 0.394
2024-12-01-23:35:16-root-INFO: grad norm: 3.199 3.170 0.428
2024-12-01-23:35:17-root-INFO: Loss Change: 97.796 -> 95.632
2024-12-01-23:35:17-root-INFO: Regularization Change: 0.000 -> 1.479
2024-12-01-23:35:17-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-01-23:35:17-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-23:35:17-root-INFO: step: 78 lr_xt 0.13904792
2024-12-01-23:35:17-root-INFO: grad norm: 3.769 3.722 0.594
2024-12-01-23:35:18-root-INFO: Loss too large (95.834->96.179)! Learning rate decreased to 0.11124.
2024-12-01-23:35:19-root-INFO: grad norm: 5.877 5.846 0.601
2024-12-01-23:35:19-root-INFO: Loss too large (95.635->96.132)! Learning rate decreased to 0.08899.
2024-12-01-23:35:19-root-INFO: Loss too large (95.635->95.744)! Learning rate decreased to 0.07119.
2024-12-01-23:35:20-root-INFO: grad norm: 3.687 3.652 0.503
2024-12-01-23:35:21-root-INFO: grad norm: 1.712 1.678 0.336
2024-12-01-23:35:22-root-INFO: grad norm: 1.616 1.582 0.330
2024-12-01-23:35:22-root-INFO: grad norm: 1.600 1.567 0.326
2024-12-01-23:35:23-root-INFO: grad norm: 1.590 1.557 0.324
2024-12-01-23:35:24-root-INFO: grad norm: 1.582 1.549 0.322
2024-12-01-23:35:25-root-INFO: Loss Change: 95.834 -> 93.876
2024-12-01-23:35:25-root-INFO: Regularization Change: 0.000 -> 1.010
2024-12-01-23:35:25-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-01-23:35:25-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-23:35:25-root-INFO: step: 77 lr_xt 0.14250787
2024-12-01-23:35:25-root-INFO: grad norm: 3.158 3.072 0.729
2024-12-01-23:35:26-root-INFO: grad norm: 4.803 4.733 0.817
2024-12-01-23:35:27-root-INFO: Loss too large (93.711->95.229)! Learning rate decreased to 0.11401.
2024-12-01-23:35:27-root-INFO: Loss too large (93.711->94.037)! Learning rate decreased to 0.09121.
2024-12-01-23:35:28-root-INFO: grad norm: 5.765 5.717 0.742
2024-12-01-23:35:28-root-INFO: Loss too large (93.426->93.638)! Learning rate decreased to 0.07296.
2024-12-01-23:35:29-root-INFO: grad norm: 3.957 3.914 0.580
2024-12-01-23:35:30-root-INFO: grad norm: 1.879 1.848 0.339
2024-12-01-23:35:31-root-INFO: grad norm: 1.774 1.744 0.328
2024-12-01-23:35:31-root-INFO: grad norm: 1.808 1.780 0.321
2024-12-01-23:35:32-root-INFO: grad norm: 1.858 1.828 0.330
2024-12-01-23:35:33-root-INFO: Loss Change: 93.784 -> 91.911
2024-12-01-23:35:33-root-INFO: Regularization Change: 0.000 -> 1.152
2024-12-01-23:35:33-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-01-23:35:33-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-23:35:33-root-INFO: step: 76 lr_xt 0.14603050
2024-12-01-23:35:33-root-INFO: grad norm: 4.532 4.449 0.867
2024-12-01-23:35:34-root-INFO: Loss too large (92.224->92.961)! Learning rate decreased to 0.11682.
2024-12-01-23:35:34-root-INFO: Loss too large (92.224->92.449)! Learning rate decreased to 0.09346.
2024-12-01-23:35:35-root-INFO: grad norm: 4.145 4.102 0.597
2024-12-01-23:35:36-root-INFO: grad norm: 4.155 4.117 0.566
2024-12-01-23:35:36-root-INFO: Loss too large (91.595->91.670)! Learning rate decreased to 0.07477.
2024-12-01-23:35:37-root-INFO: grad norm: 3.429 3.395 0.479
2024-12-01-23:35:38-root-INFO: grad norm: 2.520 2.491 0.378
2024-12-01-23:35:39-root-INFO: grad norm: 2.504 2.476 0.376
2024-12-01-23:35:40-root-INFO: grad norm: 2.549 2.523 0.365
2024-12-01-23:35:40-root-INFO: grad norm: 2.580 2.552 0.381
2024-12-01-23:35:41-root-INFO: Loss Change: 92.224 -> 90.386
2024-12-01-23:35:41-root-INFO: Regularization Change: 0.000 -> 0.997
2024-12-01-23:35:41-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-01-23:35:41-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-23:35:41-root-INFO: step: 75 lr_xt 0.14961620
2024-12-01-23:35:41-root-INFO: grad norm: 4.029 3.956 0.763
2024-12-01-23:35:42-root-INFO: Loss too large (90.565->91.185)! Learning rate decreased to 0.11969.
2024-12-01-23:35:42-root-INFO: Loss too large (90.565->90.752)! Learning rate decreased to 0.09575.
2024-12-01-23:35:43-root-INFO: grad norm: 3.962 3.924 0.548
2024-12-01-23:35:44-root-INFO: grad norm: 4.707 4.670 0.591
2024-12-01-23:35:44-root-INFO: Loss too large (90.097->90.304)! Learning rate decreased to 0.07660.
2024-12-01-23:35:45-root-INFO: grad norm: 3.669 3.636 0.496
2024-12-01-23:35:46-root-INFO: grad norm: 2.163 2.135 0.347
2024-12-01-23:35:47-root-INFO: grad norm: 2.189 2.163 0.341
2024-12-01-23:35:47-root-INFO: grad norm: 2.347 2.322 0.341
2024-12-01-23:35:48-root-INFO: grad norm: 2.451 2.424 0.360
2024-12-01-23:35:49-root-INFO: Loss Change: 90.565 -> 88.848
2024-12-01-23:35:49-root-INFO: Regularization Change: 0.000 -> 0.992
2024-12-01-23:35:49-root-INFO: Undo step: 75
2024-12-01-23:35:49-root-INFO: Undo step: 76
2024-12-01-23:35:49-root-INFO: Undo step: 77
2024-12-01-23:35:49-root-INFO: Undo step: 78
2024-12-01-23:35:49-root-INFO: Undo step: 79
2024-12-01-23:35:49-root-INFO: step: 80 lr_xt 0.13231432
2024-12-01-23:35:49-root-INFO: grad norm: 36.937 36.619 4.841
2024-12-01-23:35:50-root-INFO: grad norm: 21.351 21.137 3.010
2024-12-01-23:35:51-root-INFO: grad norm: 20.325 20.070 3.210
2024-12-01-23:35:52-root-INFO: grad norm: 24.913 24.656 3.575
2024-12-01-23:35:53-root-INFO: Loss too large (138.038->144.616)! Learning rate decreased to 0.10585.
2024-12-01-23:35:53-root-INFO: grad norm: 21.768 21.510 3.347
2024-12-01-23:35:54-root-INFO: grad norm: 18.801 18.615 2.638
2024-12-01-23:35:55-root-INFO: grad norm: 16.666 16.462 2.605
2024-12-01-23:35:56-root-INFO: grad norm: 15.227 15.071 2.180
2024-12-01-23:35:57-root-INFO: Loss Change: 248.364 -> 112.715
2024-12-01-23:35:57-root-INFO: Regularization Change: 0.000 -> 70.086
2024-12-01-23:35:57-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-01-23:35:57-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-23:35:57-root-INFO: step: 79 lr_xt 0.13565022
2024-12-01-23:35:57-root-INFO: grad norm: 12.710 12.593 1.725
2024-12-01-23:35:58-root-INFO: Loss too large (111.199->114.019)! Learning rate decreased to 0.10852.
2024-12-01-23:35:58-root-INFO: grad norm: 12.621 12.484 1.854
2024-12-01-23:35:59-root-INFO: grad norm: 12.361 12.218 1.876
2024-12-01-23:36:00-root-INFO: grad norm: 12.183 12.047 1.817
2024-12-01-23:36:01-root-INFO: grad norm: 12.239 12.097 1.861
2024-12-01-23:36:02-root-INFO: grad norm: 12.659 12.516 1.898
2024-12-01-23:36:03-root-INFO: grad norm: 12.467 12.315 1.944
2024-12-01-23:36:04-root-INFO: grad norm: 12.095 11.961 1.794
2024-12-01-23:36:04-root-INFO: Loss Change: 111.199 -> 102.557
2024-12-01-23:36:04-root-INFO: Regularization Change: 0.000 -> 7.182
2024-12-01-23:36:04-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-01-23:36:04-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-23:36:04-root-INFO: step: 78 lr_xt 0.13904792
2024-12-01-23:36:05-root-INFO: grad norm: 10.577 10.483 1.407
2024-12-01-23:36:05-root-INFO: Loss too large (101.711->104.632)! Learning rate decreased to 0.11124.
2024-12-01-23:36:06-root-INFO: grad norm: 11.048 10.931 1.607
2024-12-01-23:36:07-root-INFO: grad norm: 11.024 10.900 1.653
2024-12-01-23:36:08-root-INFO: grad norm: 10.839 10.720 1.603
2024-12-01-23:36:08-root-INFO: grad norm: 11.031 10.907 1.645
2024-12-01-23:36:09-root-INFO: grad norm: 11.867 11.738 1.745
2024-12-01-23:36:10-root-INFO: Loss too large (99.084->99.365)! Learning rate decreased to 0.08899.
2024-12-01-23:36:10-root-INFO: grad norm: 7.672 7.572 1.231
2024-12-01-23:36:11-root-INFO: grad norm: 4.802 4.744 0.742
2024-12-01-23:36:12-root-INFO: Loss Change: 101.711 -> 94.615
2024-12-01-23:36:12-root-INFO: Regularization Change: 0.000 -> 3.698
2024-12-01-23:36:12-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-01-23:36:12-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-23:36:12-root-INFO: step: 77 lr_xt 0.14250787
2024-12-01-23:36:12-root-INFO: grad norm: 2.993 2.922 0.647
2024-12-01-23:36:13-root-INFO: grad norm: 3.512 3.473 0.522
2024-12-01-23:36:14-root-INFO: grad norm: 6.182 6.142 0.700
2024-12-01-23:36:14-root-INFO: Loss too large (93.481->94.356)! Learning rate decreased to 0.11401.
2024-12-01-23:36:15-root-INFO: Loss too large (93.481->93.627)! Learning rate decreased to 0.09121.
2024-12-01-23:36:16-root-INFO: grad norm: 4.301 4.260 0.587
2024-12-01-23:36:16-root-INFO: grad norm: 2.441 2.403 0.432
2024-12-01-23:36:17-root-INFO: grad norm: 2.132 2.094 0.400
2024-12-01-23:36:18-root-INFO: grad norm: 2.012 1.975 0.388
2024-12-01-23:36:19-root-INFO: grad norm: 1.960 1.922 0.382
2024-12-01-23:36:20-root-INFO: Loss Change: 94.178 -> 90.832
2024-12-01-23:36:20-root-INFO: Regularization Change: 0.000 -> 2.680
2024-12-01-23:36:20-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-01-23:36:20-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-23:36:20-root-INFO: step: 76 lr_xt 0.14603050
2024-12-01-23:36:20-root-INFO: grad norm: 4.009 3.910 0.883
2024-12-01-23:36:21-root-INFO: Loss too large (91.039->91.193)! Learning rate decreased to 0.11682.
2024-12-01-23:36:21-root-INFO: grad norm: 4.382 4.322 0.725
2024-12-01-23:36:22-root-INFO: grad norm: 6.166 6.102 0.891
2024-12-01-23:36:23-root-INFO: Loss too large (90.542->91.010)! Learning rate decreased to 0.09346.
2024-12-01-23:36:23-root-INFO: grad norm: 4.389 4.330 0.714
2024-12-01-23:36:24-root-INFO: grad norm: 2.538 2.496 0.461
2024-12-01-23:36:25-root-INFO: grad norm: 2.100 2.066 0.375
2024-12-01-23:36:26-root-INFO: grad norm: 1.913 1.879 0.360
2024-12-01-23:36:27-root-INFO: grad norm: 1.813 1.780 0.346
2024-12-01-23:36:28-root-INFO: Loss Change: 91.039 -> 88.263
2024-12-01-23:36:28-root-INFO: Regularization Change: 0.000 -> 1.986
2024-12-01-23:36:28-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-01-23:36:28-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-23:36:28-root-INFO: step: 75 lr_xt 0.14961620
2024-12-01-23:36:28-root-INFO: grad norm: 3.197 3.115 0.719
2024-12-01-23:36:29-root-INFO: grad norm: 4.733 4.667 0.788
2024-12-01-23:36:29-root-INFO: Loss too large (88.289->89.347)! Learning rate decreased to 0.11969.
2024-12-01-23:36:30-root-INFO: Loss too large (88.289->88.383)! Learning rate decreased to 0.09575.
2024-12-01-23:36:31-root-INFO: grad norm: 4.761 4.711 0.687
2024-12-01-23:36:32-root-INFO: grad norm: 4.356 4.308 0.644
2024-12-01-23:36:32-root-INFO: grad norm: 3.847 3.806 0.564
2024-12-01-23:36:33-root-INFO: grad norm: 3.797 3.757 0.553
2024-12-01-23:36:34-root-INFO: grad norm: 3.912 3.875 0.536
2024-12-01-23:36:35-root-INFO: grad norm: 3.774 3.735 0.546
2024-12-01-23:36:36-root-INFO: Loss Change: 88.432 -> 86.185
2024-12-01-23:36:36-root-INFO: Regularization Change: 0.000 -> 1.830
2024-12-01-23:36:36-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-01-23:36:36-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-23:36:36-root-INFO: step: 74 lr_xt 0.15326538
2024-12-01-23:36:36-root-INFO: grad norm: 5.542 5.450 1.004
2024-12-01-23:36:36-root-INFO: Loss too large (86.593->87.808)! Learning rate decreased to 0.12261.
2024-12-01-23:36:37-root-INFO: Loss too large (86.593->87.010)! Learning rate decreased to 0.09809.
2024-12-01-23:36:37-root-INFO: grad norm: 4.454 4.403 0.670
2024-12-01-23:36:38-root-INFO: grad norm: 3.009 2.965 0.511
2024-12-01-23:36:39-root-INFO: grad norm: 3.004 2.969 0.457
2024-12-01-23:36:40-root-INFO: grad norm: 3.622 3.589 0.488
2024-12-01-23:36:41-root-INFO: grad norm: 3.627 3.591 0.512
2024-12-01-23:36:42-root-INFO: grad norm: 3.652 3.619 0.492
2024-12-01-23:36:43-root-INFO: grad norm: 3.613 3.577 0.508
2024-12-01-23:36:43-root-INFO: Loss Change: 86.593 -> 84.275
2024-12-01-23:36:43-root-INFO: Regularization Change: 0.000 -> 1.513
2024-12-01-23:36:43-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-01-23:36:43-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-23:36:44-root-INFO: step: 73 lr_xt 0.15697839
2024-12-01-23:36:44-root-INFO: grad norm: 5.514 5.416 1.033
2024-12-01-23:36:44-root-INFO: Loss too large (84.608->85.838)! Learning rate decreased to 0.12558.
2024-12-01-23:36:44-root-INFO: Loss too large (84.608->85.070)! Learning rate decreased to 0.10047.
2024-12-01-23:36:45-root-INFO: grad norm: 4.386 4.341 0.629
2024-12-01-23:36:46-root-INFO: grad norm: 2.838 2.794 0.500
2024-12-01-23:36:47-root-INFO: grad norm: 2.869 2.837 0.422
2024-12-01-23:36:48-root-INFO: grad norm: 3.610 3.578 0.480
2024-12-01-23:36:48-root-INFO: Loss too large (83.255->83.261)! Learning rate decreased to 0.08037.
2024-12-01-23:36:49-root-INFO: grad norm: 3.070 3.042 0.415
2024-12-01-23:36:50-root-INFO: grad norm: 2.436 2.409 0.361
2024-12-01-23:36:51-root-INFO: grad norm: 2.338 2.312 0.341
2024-12-01-23:36:51-root-INFO: Loss Change: 84.608 -> 82.419
2024-12-01-23:36:51-root-INFO: Regularization Change: 0.000 -> 1.225
2024-12-01-23:36:51-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-01-23:36:51-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-23:36:52-root-INFO: step: 72 lr_xt 0.16075558
2024-12-01-23:36:52-root-INFO: grad norm: 3.996 3.912 0.819
2024-12-01-23:36:52-root-INFO: Loss too large (82.553->83.191)! Learning rate decreased to 0.12860.
2024-12-01-23:36:53-root-INFO: Loss too large (82.553->82.703)! Learning rate decreased to 0.10288.
2024-12-01-23:36:53-root-INFO: grad norm: 3.885 3.847 0.538
2024-12-01-23:36:54-root-INFO: grad norm: 4.552 4.514 0.589
2024-12-01-23:36:55-root-INFO: Loss too large (82.063->82.218)! Learning rate decreased to 0.08231.
2024-12-01-23:36:56-root-INFO: grad norm: 3.555 3.524 0.465
2024-12-01-23:36:56-root-INFO: grad norm: 2.262 2.234 0.356
2024-12-01-23:36:57-root-INFO: grad norm: 2.216 2.191 0.327
2024-12-01-23:36:58-root-INFO: grad norm: 2.230 2.205 0.328
2024-12-01-23:36:59-root-INFO: grad norm: 2.253 2.229 0.325
2024-12-01-23:36:59-root-INFO: Loss Change: 82.553 -> 80.769
2024-12-01-23:37:00-root-INFO: Regularization Change: 0.000 -> 1.080
2024-12-01-23:37:00-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-01-23:37:00-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-23:37:00-root-INFO: step: 71 lr_xt 0.16459726
2024-12-01-23:37:00-root-INFO: grad norm: 3.266 3.221 0.543
2024-12-01-23:37:00-root-INFO: Loss too large (80.960->81.407)! Learning rate decreased to 0.13168.
2024-12-01-23:37:01-root-INFO: Loss too large (80.960->81.093)! Learning rate decreased to 0.10534.
2024-12-01-23:37:02-root-INFO: grad norm: 3.527 3.496 0.464
2024-12-01-23:37:02-root-INFO: grad norm: 4.577 4.546 0.528
2024-12-01-23:37:03-root-INFO: Loss too large (80.649->80.869)! Learning rate decreased to 0.08427.
2024-12-01-23:37:04-root-INFO: grad norm: 3.599 3.569 0.458
2024-12-01-23:37:05-root-INFO: grad norm: 2.204 2.178 0.334
2024-12-01-23:37:05-root-INFO: grad norm: 2.187 2.164 0.318
2024-12-01-23:37:06-root-INFO: grad norm: 2.244 2.221 0.317
2024-12-01-23:37:07-root-INFO: grad norm: 2.290 2.267 0.322
2024-12-01-23:37:08-root-INFO: Loss Change: 80.960 -> 79.405
2024-12-01-23:37:08-root-INFO: Regularization Change: 0.000 -> 1.037
2024-12-01-23:37:08-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-01-23:37:08-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-23:37:08-root-INFO: step: 70 lr_xt 0.16850375
2024-12-01-23:37:08-root-INFO: grad norm: 4.985 4.878 1.027
2024-12-01-23:37:09-root-INFO: Loss too large (79.650->80.807)! Learning rate decreased to 0.13480.
2024-12-01-23:37:09-root-INFO: Loss too large (79.650->80.091)! Learning rate decreased to 0.10784.
2024-12-01-23:37:10-root-INFO: grad norm: 4.366 4.326 0.589
2024-12-01-23:37:11-root-INFO: grad norm: 3.872 3.833 0.553
2024-12-01-23:37:11-root-INFO: Loss too large (78.890->78.959)! Learning rate decreased to 0.08627.
2024-12-01-23:37:12-root-INFO: grad norm: 3.302 3.276 0.413
2024-12-01-23:37:13-root-INFO: grad norm: 2.781 2.756 0.371
2024-12-01-23:37:13-root-INFO: grad norm: 2.746 2.724 0.349
2024-12-01-23:37:14-root-INFO: grad norm: 2.744 2.722 0.349
2024-12-01-23:37:15-root-INFO: grad norm: 2.759 2.737 0.347
2024-12-01-23:37:16-root-INFO: Loss Change: 79.650 -> 77.747
2024-12-01-23:37:16-root-INFO: Regularization Change: 0.000 -> 1.109
2024-12-01-23:37:16-root-INFO: Undo step: 70
2024-12-01-23:37:16-root-INFO: Undo step: 71
2024-12-01-23:37:16-root-INFO: Undo step: 72
2024-12-01-23:37:16-root-INFO: Undo step: 73
2024-12-01-23:37:16-root-INFO: Undo step: 74
2024-12-01-23:37:16-root-INFO: step: 75 lr_xt 0.14961620
2024-12-01-23:37:16-root-INFO: grad norm: 38.320 37.997 4.968
2024-12-01-23:37:17-root-INFO: grad norm: 18.241 18.010 2.895
2024-12-01-23:37:18-root-INFO: grad norm: 16.310 16.075 2.760
2024-12-01-23:37:19-root-INFO: grad norm: 20.639 20.419 3.006
2024-12-01-23:37:19-root-INFO: Loss too large (119.830->123.401)! Learning rate decreased to 0.11969.
2024-12-01-23:37:20-root-INFO: grad norm: 15.915 15.705 2.574
2024-12-01-23:37:21-root-INFO: grad norm: 11.954 11.842 1.632
2024-12-01-23:37:22-root-INFO: grad norm: 10.506 10.389 1.560
2024-12-01-23:37:23-root-INFO: grad norm: 10.227 10.122 1.459
2024-12-01-23:37:23-root-INFO: Loss Change: 235.661 -> 98.667
2024-12-01-23:37:23-root-INFO: Regularization Change: 0.000 -> 77.849
2024-12-01-23:37:23-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-01-23:37:23-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-23:37:23-root-INFO: step: 74 lr_xt 0.15326538
2024-12-01-23:37:24-root-INFO: grad norm: 9.023 8.960 1.070
2024-12-01-23:37:24-root-INFO: Loss too large (98.126->98.201)! Learning rate decreased to 0.12261.
2024-12-01-23:37:25-root-INFO: grad norm: 8.534 8.465 1.077
2024-12-01-23:37:26-root-INFO: grad norm: 8.522 8.452 1.087
2024-12-01-23:37:27-root-INFO: grad norm: 9.658 9.585 1.187
2024-12-01-23:37:27-root-INFO: Loss too large (93.879->93.912)! Learning rate decreased to 0.09809.
2024-12-01-23:37:28-root-INFO: grad norm: 6.230 6.160 0.933
2024-12-01-23:37:29-root-INFO: grad norm: 4.236 4.190 0.618
2024-12-01-23:37:30-root-INFO: grad norm: 3.551 3.509 0.543
2024-12-01-23:37:30-root-INFO: grad norm: 3.382 3.339 0.537
2024-12-01-23:37:31-root-INFO: Loss Change: 98.126 -> 88.622
2024-12-01-23:37:31-root-INFO: Regularization Change: 0.000 -> 6.338
2024-12-01-23:37:31-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-01-23:37:31-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-23:37:31-root-INFO: step: 73 lr_xt 0.15697839
2024-12-01-23:37:32-root-INFO: grad norm: 5.234 5.135 1.015
2024-12-01-23:37:32-root-INFO: Loss too large (88.770->89.096)! Learning rate decreased to 0.12558.
2024-12-01-23:37:33-root-INFO: grad norm: 4.649 4.581 0.792
2024-12-01-23:37:34-root-INFO: grad norm: 3.634 3.570 0.680
2024-12-01-23:37:35-root-INFO: grad norm: 3.874 3.825 0.620
2024-12-01-23:37:35-root-INFO: grad norm: 5.656 5.602 0.781
2024-12-01-23:37:36-root-INFO: Loss too large (86.345->86.648)! Learning rate decreased to 0.10047.
2024-12-01-23:37:37-root-INFO: grad norm: 4.549 4.495 0.700
2024-12-01-23:37:37-root-INFO: grad norm: 2.963 2.917 0.522
2024-12-01-23:37:38-root-INFO: grad norm: 2.894 2.856 0.465
2024-12-01-23:37:39-root-INFO: Loss Change: 88.770 -> 84.329
2024-12-01-23:37:39-root-INFO: Regularization Change: 0.000 -> 3.779
2024-12-01-23:37:39-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-01-23:37:39-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-23:37:39-root-INFO: step: 72 lr_xt 0.16075558
2024-12-01-23:37:39-root-INFO: grad norm: 5.446 5.345 1.048
2024-12-01-23:37:40-root-INFO: Loss too large (84.532->85.497)! Learning rate decreased to 0.12860.
2024-12-01-23:37:40-root-INFO: Loss too large (84.532->84.809)! Learning rate decreased to 0.10288.
2024-12-01-23:37:41-root-INFO: grad norm: 4.460 4.409 0.677
2024-12-01-23:37:42-root-INFO: grad norm: 3.109 3.061 0.546
2024-12-01-23:37:43-root-INFO: grad norm: 3.291 3.254 0.495
2024-12-01-23:37:44-root-INFO: grad norm: 4.443 4.408 0.558
2024-12-01-23:37:44-root-INFO: Loss too large (82.756->82.802)! Learning rate decreased to 0.08231.
2024-12-01-23:37:45-root-INFO: grad norm: 3.634 3.598 0.511
2024-12-01-23:37:46-root-INFO: grad norm: 2.536 2.504 0.402
2024-12-01-23:37:46-root-INFO: grad norm: 2.556 2.525 0.395
2024-12-01-23:37:47-root-INFO: Loss Change: 84.532 -> 81.520
2024-12-01-23:37:47-root-INFO: Regularization Change: 0.000 -> 1.905
2024-12-01-23:37:47-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-01-23:37:47-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-23:37:47-root-INFO: step: 71 lr_xt 0.16459726
2024-12-01-23:37:48-root-INFO: grad norm: 3.551 3.500 0.598
2024-12-01-23:37:48-root-INFO: Loss too large (81.683->82.045)! Learning rate decreased to 0.13168.
2024-12-01-23:37:48-root-INFO: Loss too large (81.683->81.746)! Learning rate decreased to 0.10534.
2024-12-01-23:37:49-root-INFO: grad norm: 3.743 3.705 0.533
2024-12-01-23:37:50-root-INFO: grad norm: 4.626 4.593 0.554
2024-12-01-23:37:50-root-INFO: Loss too large (81.156->81.288)! Learning rate decreased to 0.08427.
2024-12-01-23:37:51-root-INFO: grad norm: 3.673 3.639 0.501
2024-12-01-23:37:52-root-INFO: grad norm: 2.348 2.318 0.370
2024-12-01-23:37:53-root-INFO: grad norm: 2.391 2.363 0.366
2024-12-01-23:37:54-root-INFO: grad norm: 2.538 2.512 0.360
2024-12-01-23:37:55-root-INFO: grad norm: 2.632 2.604 0.381
2024-12-01-23:37:55-root-INFO: Loss Change: 81.683 -> 79.564
2024-12-01-23:37:55-root-INFO: Regularization Change: 0.000 -> 1.450
2024-12-01-23:37:55-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-01-23:37:55-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-23:37:56-root-INFO: step: 70 lr_xt 0.16850375
2024-12-01-23:37:56-root-INFO: grad norm: 5.428 5.303 1.158
2024-12-01-23:37:56-root-INFO: Loss too large (79.796->80.934)! Learning rate decreased to 0.13480.
2024-12-01-23:37:56-root-INFO: Loss too large (79.796->80.244)! Learning rate decreased to 0.10784.
2024-12-01-23:37:57-root-INFO: grad norm: 4.322 4.276 0.632
2024-12-01-23:37:58-root-INFO: grad norm: 2.778 2.736 0.485
2024-12-01-23:37:59-root-INFO: grad norm: 3.122 3.091 0.436
2024-12-01-23:38:00-root-INFO: grad norm: 4.743 4.713 0.532
2024-12-01-23:38:00-root-INFO: Loss too large (78.381->78.630)! Learning rate decreased to 0.08627.
2024-12-01-23:38:01-root-INFO: grad norm: 3.747 3.716 0.485
2024-12-01-23:38:02-root-INFO: grad norm: 2.265 2.238 0.346
2024-12-01-23:38:03-root-INFO: grad norm: 2.365 2.341 0.337
2024-12-01-23:38:03-root-INFO: Loss Change: 79.796 -> 77.322
2024-12-01-23:38:03-root-INFO: Regularization Change: 0.000 -> 1.554
2024-12-01-23:38:03-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-01-23:38:03-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-23:38:04-root-INFO: step: 69 lr_xt 0.17247530
2024-12-01-23:38:04-root-INFO: grad norm: 3.440 3.389 0.590
2024-12-01-23:38:04-root-INFO: Loss too large (77.488->78.023)! Learning rate decreased to 0.13798.
2024-12-01-23:38:05-root-INFO: Loss too large (77.488->77.679)! Learning rate decreased to 0.11038.
2024-12-01-23:38:05-root-INFO: grad norm: 3.759 3.726 0.496
2024-12-01-23:38:06-root-INFO: grad norm: 5.026 4.995 0.558
2024-12-01-23:38:07-root-INFO: Loss too large (77.157->77.466)! Learning rate decreased to 0.08831.
2024-12-01-23:38:07-root-INFO: grad norm: 3.764 3.732 0.485
2024-12-01-23:38:08-root-INFO: grad norm: 1.968 1.941 0.323
2024-12-01-23:38:09-root-INFO: grad norm: 2.032 2.008 0.309
2024-12-01-23:38:10-root-INFO: grad norm: 2.271 2.249 0.313
2024-12-01-23:38:11-root-INFO: grad norm: 2.469 2.446 0.336
2024-12-01-23:38:12-root-INFO: Loss Change: 77.488 -> 75.732
2024-12-01-23:38:12-root-INFO: Regularization Change: 0.000 -> 1.250
2024-12-01-23:38:12-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-01-23:38:12-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-23:38:12-root-INFO: step: 68 lr_xt 0.17651217
2024-12-01-23:38:12-root-INFO: grad norm: 5.737 5.620 1.156
2024-12-01-23:38:12-root-INFO: Loss too large (76.074->77.490)! Learning rate decreased to 0.14121.
2024-12-01-23:38:13-root-INFO: Loss too large (76.074->76.720)! Learning rate decreased to 0.11297.
2024-12-01-23:38:13-root-INFO: Loss too large (76.074->76.167)! Learning rate decreased to 0.09037.
2024-12-01-23:38:14-root-INFO: grad norm: 3.981 3.950 0.498
2024-12-01-23:38:15-root-INFO: grad norm: 2.195 2.164 0.373
2024-12-01-23:38:16-root-INFO: grad norm: 2.303 2.281 0.319
2024-12-01-23:38:17-root-INFO: grad norm: 2.669 2.647 0.340
2024-12-01-23:38:17-root-INFO: grad norm: 2.888 2.866 0.359
2024-12-01-23:38:18-root-INFO: grad norm: 3.279 3.257 0.371
2024-12-01-23:38:19-root-INFO: grad norm: 3.327 3.303 0.399
2024-12-01-23:38:20-root-INFO: Loss Change: 76.074 -> 74.003
2024-12-01-23:38:20-root-INFO: Regularization Change: 0.000 -> 1.154
2024-12-01-23:38:20-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-01-23:38:20-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-23:38:20-root-INFO: step: 67 lr_xt 0.18061458
2024-12-01-23:38:20-root-INFO: grad norm: 4.483 4.420 0.746
2024-12-01-23:38:21-root-INFO: Loss too large (74.078->75.221)! Learning rate decreased to 0.14449.
2024-12-01-23:38:21-root-INFO: Loss too large (74.078->74.698)! Learning rate decreased to 0.11559.
2024-12-01-23:38:21-root-INFO: Loss too large (74.078->74.292)! Learning rate decreased to 0.09247.
2024-12-01-23:38:22-root-INFO: grad norm: 3.789 3.762 0.448
2024-12-01-23:38:23-root-INFO: grad norm: 3.022 2.997 0.390
2024-12-01-23:38:24-root-INFO: grad norm: 3.180 3.157 0.378
2024-12-01-23:38:25-root-INFO: grad norm: 3.518 3.496 0.387
2024-12-01-23:38:26-root-INFO: grad norm: 3.483 3.459 0.409
2024-12-01-23:38:27-root-INFO: grad norm: 3.405 3.383 0.379
2024-12-01-23:38:28-root-INFO: grad norm: 3.411 3.387 0.401
2024-12-01-23:38:28-root-INFO: Loss Change: 74.078 -> 72.450
2024-12-01-23:38:28-root-INFO: Regularization Change: 0.000 -> 1.074
2024-12-01-23:38:28-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-01-23:38:28-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-23:38:29-root-INFO: step: 66 lr_xt 0.18478272
2024-12-01-23:38:29-root-INFO: grad norm: 4.640 4.590 0.677
2024-12-01-23:38:29-root-INFO: Loss too large (72.673->73.897)! Learning rate decreased to 0.14783.
2024-12-01-23:38:29-root-INFO: Loss too large (72.673->73.371)! Learning rate decreased to 0.11826.
2024-12-01-23:38:30-root-INFO: Loss too large (72.673->72.947)! Learning rate decreased to 0.09461.
2024-12-01-23:38:31-root-INFO: grad norm: 3.765 3.738 0.445
2024-12-01-23:38:31-root-INFO: grad norm: 2.627 2.604 0.346
2024-12-01-23:38:32-root-INFO: grad norm: 2.810 2.788 0.345
2024-12-01-23:38:33-root-INFO: grad norm: 3.209 3.190 0.349
2024-12-01-23:38:34-root-INFO: grad norm: 3.276 3.254 0.385
2024-12-01-23:38:35-root-INFO: grad norm: 3.380 3.361 0.361
2024-12-01-23:38:36-root-INFO: grad norm: 3.362 3.339 0.393
2024-12-01-23:38:36-root-INFO: Loss Change: 72.673 -> 71.080
2024-12-01-23:38:36-root-INFO: Regularization Change: 0.000 -> 1.046
2024-12-01-23:38:36-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-01-23:38:36-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-23:38:37-root-INFO: step: 65 lr_xt 0.18901677
2024-12-01-23:38:37-root-INFO: grad norm: 5.205 5.127 0.898
2024-12-01-23:38:37-root-INFO: Loss too large (71.399->72.865)! Learning rate decreased to 0.15121.
2024-12-01-23:38:38-root-INFO: Loss too large (71.399->72.171)! Learning rate decreased to 0.12097.
2024-12-01-23:38:38-root-INFO: Loss too large (71.399->71.648)! Learning rate decreased to 0.09678.
2024-12-01-23:38:39-root-INFO: grad norm: 3.870 3.843 0.456
2024-12-01-23:38:40-root-INFO: grad norm: 2.349 2.325 0.337
2024-12-01-23:38:40-root-INFO: grad norm: 2.545 2.526 0.315
2024-12-01-23:38:41-root-INFO: grad norm: 3.035 3.017 0.330
2024-12-01-23:38:42-root-INFO: grad norm: 3.197 3.176 0.367
2024-12-01-23:38:43-root-INFO: grad norm: 3.465 3.446 0.356
2024-12-01-23:38:44-root-INFO: grad norm: 3.429 3.407 0.391
2024-12-01-23:38:45-root-INFO: Loss Change: 71.399 -> 69.648
2024-12-01-23:38:45-root-INFO: Regularization Change: 0.000 -> 1.079
2024-12-01-23:38:45-root-INFO: Undo step: 65
2024-12-01-23:38:45-root-INFO: Undo step: 66
2024-12-01-23:38:45-root-INFO: Undo step: 67
2024-12-01-23:38:45-root-INFO: Undo step: 68
2024-12-01-23:38:45-root-INFO: Undo step: 69
2024-12-01-23:38:45-root-INFO: step: 70 lr_xt 0.16850375
2024-12-01-23:38:45-root-INFO: grad norm: 33.776 33.479 4.465
2024-12-01-23:38:46-root-INFO: grad norm: 19.715 19.498 2.915
2024-12-01-23:38:47-root-INFO: grad norm: 16.235 16.034 2.546
2024-12-01-23:38:48-root-INFO: grad norm: 17.365 17.211 2.308
2024-12-01-23:38:49-root-INFO: grad norm: 18.750 18.577 2.546
2024-12-01-23:38:50-root-INFO: grad norm: 19.566 19.414 2.435
2024-12-01-23:38:51-root-INFO: grad norm: 19.682 19.493 2.716
2024-12-01-23:38:51-root-INFO: grad norm: 19.629 19.457 2.594
2024-12-01-23:38:52-root-INFO: Loss Change: 226.818 -> 100.573
2024-12-01-23:38:52-root-INFO: Regularization Change: 0.000 -> 92.596
2024-12-01-23:38:52-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-01-23:38:52-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-23:38:52-root-INFO: step: 69 lr_xt 0.17247530
2024-12-01-23:38:53-root-INFO: grad norm: 17.043 16.851 2.548
2024-12-01-23:38:53-root-INFO: grad norm: 15.770 15.638 2.036
2024-12-01-23:38:54-root-INFO: grad norm: 14.942 14.780 2.195
2024-12-01-23:38:55-root-INFO: grad norm: 15.188 15.039 2.121
2024-12-01-23:38:56-root-INFO: grad norm: 14.768 14.582 2.338
2024-12-01-23:38:57-root-INFO: grad norm: 14.312 14.165 2.052
2024-12-01-23:38:58-root-INFO: grad norm: 14.090 13.917 2.197
2024-12-01-23:38:59-root-INFO: grad norm: 14.883 14.727 2.152
2024-12-01-23:38:59-root-INFO: Loss too large (87.621->88.098)! Learning rate decreased to 0.13798.
2024-12-01-23:38:59-root-INFO: Loss Change: 99.917 -> 82.409
2024-12-01-23:38:59-root-INFO: Regularization Change: 0.000 -> 10.701
2024-12-01-23:38:59-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-01-23:38:59-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-23:39:00-root-INFO: step: 68 lr_xt 0.17651217
2024-12-01-23:39:00-root-INFO: grad norm: 8.266 8.181 1.181
2024-12-01-23:39:01-root-INFO: grad norm: 8.419 8.335 1.185
2024-12-01-23:39:01-root-INFO: Loss too large (79.725->79.838)! Learning rate decreased to 0.14121.
2024-12-01-23:39:02-root-INFO: grad norm: 6.242 6.167 0.966
2024-12-01-23:39:03-root-INFO: grad norm: 5.385 5.333 0.745
2024-12-01-23:39:04-root-INFO: grad norm: 4.770 4.710 0.756
2024-12-01-23:39:05-root-INFO: grad norm: 4.370 4.324 0.630
2024-12-01-23:39:05-root-INFO: grad norm: 4.193 4.140 0.666
2024-12-01-23:39:06-root-INFO: grad norm: 4.169 4.127 0.592
2024-12-01-23:39:07-root-INFO: Loss Change: 81.091 -> 74.331
2024-12-01-23:39:07-root-INFO: Regularization Change: 0.000 -> 4.808
2024-12-01-23:39:07-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-01-23:39:07-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-23:39:07-root-INFO: step: 67 lr_xt 0.18061458
2024-12-01-23:39:08-root-INFO: grad norm: 3.931 3.896 0.525
2024-12-01-23:39:09-root-INFO: grad norm: 6.064 6.022 0.713
2024-12-01-23:39:09-root-INFO: Loss too large (73.973->74.975)! Learning rate decreased to 0.14449.
2024-12-01-23:39:09-root-INFO: Loss too large (73.973->74.056)! Learning rate decreased to 0.11559.
2024-12-01-23:39:10-root-INFO: grad norm: 3.886 3.843 0.577
2024-12-01-23:39:11-root-INFO: grad norm: 2.077 2.048 0.349
2024-12-01-23:39:12-root-INFO: grad norm: 1.760 1.735 0.295
2024-12-01-23:39:13-root-INFO: grad norm: 1.660 1.637 0.274
2024-12-01-23:39:13-root-INFO: grad norm: 1.618 1.595 0.275
2024-12-01-23:39:14-root-INFO: grad norm: 1.603 1.582 0.261
2024-12-01-23:39:15-root-INFO: Loss Change: 74.110 -> 71.087
2024-12-01-23:39:15-root-INFO: Regularization Change: 0.000 -> 2.463
2024-12-01-23:39:15-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-01-23:39:15-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-23:39:15-root-INFO: step: 66 lr_xt 0.18478272
2024-12-01-23:39:16-root-INFO: grad norm: 1.958 1.919 0.388
2024-12-01-23:39:16-root-INFO: grad norm: 2.069 2.034 0.380
2024-12-01-23:39:17-root-INFO: grad norm: 2.624 2.586 0.447
2024-12-01-23:39:18-root-INFO: grad norm: 3.643 3.595 0.590
2024-12-01-23:39:19-root-INFO: Loss too large (70.274->70.597)! Learning rate decreased to 0.14783.
2024-12-01-23:39:20-root-INFO: grad norm: 4.472 4.424 0.657
2024-12-01-23:39:20-root-INFO: Loss too large (70.046->70.288)! Learning rate decreased to 0.11826.
2024-12-01-23:39:21-root-INFO: grad norm: 3.810 3.770 0.551
2024-12-01-23:39:22-root-INFO: grad norm: 3.263 3.234 0.437
2024-12-01-23:39:22-root-INFO: grad norm: 3.136 3.105 0.438
2024-12-01-23:39:23-root-INFO: Loss Change: 71.099 -> 68.833
2024-12-01-23:39:23-root-INFO: Regularization Change: 0.000 -> 2.814
2024-12-01-23:39:23-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-01-23:39:23-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-23:39:23-root-INFO: step: 65 lr_xt 0.18901677
2024-12-01-23:39:24-root-INFO: grad norm: 4.919 4.834 0.911
2024-12-01-23:39:24-root-INFO: Loss too large (69.152->70.237)! Learning rate decreased to 0.15121.
2024-12-01-23:39:24-root-INFO: Loss too large (69.152->69.473)! Learning rate decreased to 0.12097.
2024-12-01-23:39:25-root-INFO: grad norm: 3.941 3.898 0.581
2024-12-01-23:39:26-root-INFO: grad norm: 2.844 2.812 0.427
2024-12-01-23:39:27-root-INFO: grad norm: 2.823 2.795 0.396
2024-12-01-23:39:28-root-INFO: grad norm: 3.031 3.008 0.374
2024-12-01-23:39:29-root-INFO: grad norm: 3.057 3.029 0.406
2024-12-01-23:39:30-root-INFO: grad norm: 3.122 3.099 0.375
2024-12-01-23:39:31-root-INFO: grad norm: 3.114 3.087 0.412
2024-12-01-23:39:31-root-INFO: Loss Change: 69.152 -> 66.984
2024-12-01-23:39:31-root-INFO: Regularization Change: 0.000 -> 1.713
2024-12-01-23:39:31-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-01-23:39:31-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-23:39:31-root-INFO: step: 64 lr_xt 0.19331686
2024-12-01-23:39:32-root-INFO: grad norm: 4.189 4.133 0.685
2024-12-01-23:39:32-root-INFO: Loss too large (67.083->68.015)! Learning rate decreased to 0.15465.
2024-12-01-23:39:32-root-INFO: Loss too large (67.083->67.416)! Learning rate decreased to 0.12372.
2024-12-01-23:39:33-root-INFO: grad norm: 3.702 3.666 0.519
2024-12-01-23:39:34-root-INFO: grad norm: 3.134 3.104 0.430
2024-12-01-23:39:35-root-INFO: grad norm: 3.161 3.132 0.426
2024-12-01-23:39:36-root-INFO: grad norm: 3.327 3.302 0.403
2024-12-01-23:39:37-root-INFO: grad norm: 3.297 3.268 0.435
2024-12-01-23:39:37-root-INFO: grad norm: 3.238 3.214 0.396
2024-12-01-23:39:38-root-INFO: grad norm: 3.240 3.211 0.429
2024-12-01-23:39:39-root-INFO: Loss Change: 67.083 -> 65.270
2024-12-01-23:39:39-root-INFO: Regularization Change: 0.000 -> 1.608
2024-12-01-23:39:39-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-01-23:39:39-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-23:39:39-root-INFO: step: 63 lr_xt 0.19768311
2024-12-01-23:39:39-root-INFO: grad norm: 4.368 4.304 0.747
2024-12-01-23:39:40-root-INFO: Loss too large (65.541->66.589)! Learning rate decreased to 0.15815.
2024-12-01-23:39:40-root-INFO: Loss too large (65.541->65.926)! Learning rate decreased to 0.12652.
2024-12-01-23:39:41-root-INFO: grad norm: 3.790 3.752 0.531
2024-12-01-23:39:42-root-INFO: grad norm: 3.105 3.074 0.437
2024-12-01-23:39:43-root-INFO: grad norm: 3.147 3.118 0.423
2024-12-01-23:39:43-root-INFO: grad norm: 3.370 3.346 0.409
2024-12-01-23:39:44-root-INFO: grad norm: 3.345 3.317 0.438
2024-12-01-23:39:45-root-INFO: grad norm: 3.286 3.261 0.404
2024-12-01-23:39:46-root-INFO: grad norm: 3.294 3.266 0.433
2024-12-01-23:39:47-root-INFO: Loss Change: 65.541 -> 63.753
2024-12-01-23:39:47-root-INFO: Regularization Change: 0.000 -> 1.579
2024-12-01-23:39:47-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-01-23:39:47-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-23:39:47-root-INFO: step: 62 lr_xt 0.20211560
2024-12-01-23:39:47-root-INFO: grad norm: 5.293 5.205 0.962
2024-12-01-23:39:48-root-INFO: Loss too large (63.945->65.558)! Learning rate decreased to 0.16169.
2024-12-01-23:39:48-root-INFO: Loss too large (63.945->64.577)! Learning rate decreased to 0.12935.
2024-12-01-23:39:49-root-INFO: grad norm: 4.288 4.240 0.637
2024-12-01-23:39:50-root-INFO: grad norm: 3.049 3.013 0.471
2024-12-01-23:39:51-root-INFO: grad norm: 3.070 3.040 0.425
2024-12-01-23:39:52-root-INFO: grad norm: 3.456 3.430 0.424
2024-12-01-23:39:52-root-INFO: Loss too large (62.666->62.681)! Learning rate decreased to 0.10348.
2024-12-01-23:39:53-root-INFO: grad norm: 2.742 2.720 0.351
2024-12-01-23:39:54-root-INFO: grad norm: 2.060 2.042 0.271
2024-12-01-23:39:54-root-INFO: grad norm: 1.858 1.840 0.256
2024-12-01-23:39:55-root-INFO: Loss Change: 63.945 -> 61.853
2024-12-01-23:39:55-root-INFO: Regularization Change: 0.000 -> 1.346
2024-12-01-23:39:55-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-01-23:39:55-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-23:39:55-root-INFO: step: 61 lr_xt 0.20661437
2024-12-01-23:39:56-root-INFO: grad norm: 2.503 2.468 0.417
2024-12-01-23:39:56-root-INFO: Loss too large (61.932->62.158)! Learning rate decreased to 0.16529.
2024-12-01-23:39:57-root-INFO: grad norm: 3.297 3.265 0.455
2024-12-01-23:39:57-root-INFO: Loss too large (61.921->62.063)! Learning rate decreased to 0.13223.
2024-12-01-23:39:58-root-INFO: grad norm: 3.691 3.666 0.429
2024-12-01-23:39:58-root-INFO: Loss too large (61.650->61.733)! Learning rate decreased to 0.10579.
2024-12-01-23:39:59-root-INFO: grad norm: 2.921 2.898 0.364
2024-12-01-23:40:00-root-INFO: grad norm: 2.073 2.055 0.271
2024-12-01-23:40:01-root-INFO: grad norm: 1.884 1.866 0.256
2024-12-01-23:40:02-root-INFO: grad norm: 1.720 1.704 0.231
2024-12-01-23:40:03-root-INFO: grad norm: 1.641 1.625 0.234
2024-12-01-23:40:03-root-INFO: Loss Change: 61.932 -> 60.520
2024-12-01-23:40:03-root-INFO: Regularization Change: 0.000 -> 1.213
2024-12-01-23:40:03-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-01-23:40:03-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-23:40:04-root-INFO: step: 60 lr_xt 0.21117946
2024-12-01-23:40:04-root-INFO: grad norm: 2.795 2.735 0.574
2024-12-01-23:40:04-root-INFO: Loss too large (60.688->61.020)! Learning rate decreased to 0.16894.
2024-12-01-23:40:04-root-INFO: Loss too large (60.688->60.698)! Learning rate decreased to 0.13515.
2024-12-01-23:40:05-root-INFO: grad norm: 2.865 2.840 0.382
2024-12-01-23:40:06-root-INFO: grad norm: 3.789 3.767 0.413
2024-12-01-23:40:07-root-INFO: Loss too large (60.387->60.562)! Learning rate decreased to 0.10812.
2024-12-01-23:40:07-root-INFO: grad norm: 3.110 3.087 0.372
2024-12-01-23:40:08-root-INFO: grad norm: 2.232 2.215 0.280
2024-12-01-23:40:09-root-INFO: grad norm: 2.098 2.081 0.268
2024-12-01-23:40:10-root-INFO: grad norm: 1.979 1.964 0.243
2024-12-01-23:40:11-root-INFO: grad norm: 1.931 1.914 0.250
2024-12-01-23:40:12-root-INFO: Loss Change: 60.688 -> 59.296
2024-12-01-23:40:12-root-INFO: Regularization Change: 0.000 -> 1.148
2024-12-01-23:40:12-root-INFO: Undo step: 60
2024-12-01-23:40:12-root-INFO: Undo step: 61
2024-12-01-23:40:12-root-INFO: Undo step: 62
2024-12-01-23:40:12-root-INFO: Undo step: 63
2024-12-01-23:40:12-root-INFO: Undo step: 64
2024-12-01-23:40:12-root-INFO: step: 65 lr_xt 0.18901677
2024-12-01-23:40:12-root-INFO: grad norm: 28.524 28.307 3.512
2024-12-01-23:40:13-root-INFO: grad norm: 13.921 13.758 2.127
2024-12-01-23:40:14-root-INFO: grad norm: 10.651 10.506 1.748
2024-12-01-23:40:15-root-INFO: grad norm: 10.309 10.173 1.668
2024-12-01-23:40:16-root-INFO: grad norm: 11.313 11.149 1.920
2024-12-01-23:40:17-root-INFO: grad norm: 13.248 13.082 2.094
2024-12-01-23:40:17-root-INFO: Loss too large (85.532->87.185)! Learning rate decreased to 0.15121.
2024-12-01-23:40:18-root-INFO: grad norm: 10.479 10.342 1.690
2024-12-01-23:40:19-root-INFO: grad norm: 8.581 8.491 1.239
2024-12-01-23:40:19-root-INFO: Loss Change: 193.413 -> 76.440
2024-12-01-23:40:19-root-INFO: Regularization Change: 0.000 -> 87.521
2024-12-01-23:40:19-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-01-23:40:19-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-23:40:19-root-INFO: step: 64 lr_xt 0.19331686
2024-12-01-23:40:20-root-INFO: grad norm: 6.601 6.542 0.875
2024-12-01-23:40:21-root-INFO: grad norm: 8.441 8.355 1.202
2024-12-01-23:40:21-root-INFO: Loss too large (75.122->76.392)! Learning rate decreased to 0.15465.
2024-12-01-23:40:22-root-INFO: grad norm: 7.167 7.085 1.079
2024-12-01-23:40:23-root-INFO: grad norm: 5.760 5.690 0.898
2024-12-01-23:40:23-root-INFO: grad norm: 5.337 5.273 0.818
2024-12-01-23:40:24-root-INFO: grad norm: 5.461 5.397 0.836
2024-12-01-23:40:25-root-INFO: grad norm: 5.210 5.143 0.832
2024-12-01-23:40:26-root-INFO: grad norm: 4.898 4.833 0.796
2024-12-01-23:40:27-root-INFO: Loss Change: 75.926 -> 68.535
2024-12-01-23:40:27-root-INFO: Regularization Change: 0.000 -> 8.307
2024-12-01-23:40:27-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-01-23:40:27-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-23:40:27-root-INFO: step: 63 lr_xt 0.19768311
2024-12-01-23:40:27-root-INFO: grad norm: 4.345 4.304 0.597
2024-12-01-23:40:28-root-INFO: grad norm: 6.972 6.909 0.938
2024-12-01-23:40:29-root-INFO: Loss too large (68.188->69.712)! Learning rate decreased to 0.15815.
2024-12-01-23:40:29-root-INFO: Loss too large (68.188->68.407)! Learning rate decreased to 0.12652.
2024-12-01-23:40:30-root-INFO: grad norm: 4.523 4.467 0.709
2024-12-01-23:40:31-root-INFO: grad norm: 2.321 2.284 0.414
2024-12-01-23:40:32-root-INFO: grad norm: 1.955 1.927 0.329
2024-12-01-23:40:33-root-INFO: grad norm: 1.804 1.775 0.319
2024-12-01-23:40:33-root-INFO: grad norm: 1.732 1.706 0.301
2024-12-01-23:40:34-root-INFO: grad norm: 1.686 1.659 0.299
2024-12-01-23:40:35-root-INFO: Loss Change: 68.288 -> 64.354
2024-12-01-23:40:35-root-INFO: Regularization Change: 0.000 -> 3.541
2024-12-01-23:40:35-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-01-23:40:35-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-23:40:35-root-INFO: step: 62 lr_xt 0.20211560
2024-12-01-23:40:36-root-INFO: grad norm: 2.580 2.494 0.663
2024-12-01-23:40:36-root-INFO: grad norm: 2.872 2.814 0.576
2024-12-01-23:40:37-root-INFO: grad norm: 4.540 4.470 0.793
2024-12-01-23:40:38-root-INFO: Loss too large (63.561->64.437)! Learning rate decreased to 0.16169.
2024-12-01-23:40:38-root-INFO: Loss too large (63.561->63.688)! Learning rate decreased to 0.12935.
2024-12-01-23:40:39-root-INFO: grad norm: 3.698 3.649 0.601
2024-12-01-23:40:40-root-INFO: grad norm: 2.973 2.937 0.459
2024-12-01-23:40:40-root-INFO: grad norm: 2.820 2.788 0.422
2024-12-01-23:40:41-root-INFO: grad norm: 2.699 2.671 0.391
2024-12-01-23:40:42-root-INFO: grad norm: 2.657 2.629 0.385
2024-12-01-23:40:43-root-INFO: Loss Change: 64.174 -> 61.516
2024-12-01-23:40:43-root-INFO: Regularization Change: 0.000 -> 3.108
2024-12-01-23:40:43-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-01-23:40:43-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-23:40:43-root-INFO: step: 61 lr_xt 0.20661437
2024-12-01-23:40:44-root-INFO: grad norm: 3.353 3.307 0.551
2024-12-01-23:40:44-root-INFO: Loss too large (61.589->61.960)! Learning rate decreased to 0.16529.
2024-12-01-23:40:44-root-INFO: Loss too large (61.589->61.590)! Learning rate decreased to 0.13223.
2024-12-01-23:40:45-root-INFO: grad norm: 3.014 2.982 0.439
2024-12-01-23:40:46-root-INFO: grad norm: 2.701 2.673 0.391
2024-12-01-23:40:47-root-INFO: grad norm: 2.640 2.613 0.378
2024-12-01-23:40:48-root-INFO: grad norm: 2.590 2.565 0.360
2024-12-01-23:40:48-root-INFO: grad norm: 2.584 2.558 0.364
2024-12-01-23:40:49-root-INFO: grad norm: 2.591 2.567 0.353
2024-12-01-23:40:50-root-INFO: grad norm: 2.611 2.586 0.364
2024-12-01-23:40:51-root-INFO: Loss Change: 61.589 -> 59.554
2024-12-01-23:40:51-root-INFO: Regularization Change: 0.000 -> 2.025
2024-12-01-23:40:51-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-01-23:40:51-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-23:40:51-root-INFO: step: 60 lr_xt 0.21117946
2024-12-01-23:40:51-root-INFO: grad norm: 3.825 3.754 0.732
2024-12-01-23:40:52-root-INFO: Loss too large (59.790->60.494)! Learning rate decreased to 0.16894.
2024-12-01-23:40:52-root-INFO: Loss too large (59.790->59.930)! Learning rate decreased to 0.13515.
2024-12-01-23:40:53-root-INFO: grad norm: 3.372 3.336 0.489
2024-12-01-23:40:54-root-INFO: grad norm: 3.045 3.014 0.427
2024-12-01-23:40:55-root-INFO: grad norm: 3.031 3.003 0.415
2024-12-01-23:40:56-root-INFO: grad norm: 3.026 3.000 0.395
2024-12-01-23:40:57-root-INFO: grad norm: 3.074 3.047 0.410
2024-12-01-23:40:58-root-INFO: grad norm: 3.100 3.075 0.395
2024-12-01-23:40:58-root-INFO: grad norm: 3.143 3.115 0.414
2024-12-01-23:40:59-root-INFO: Loss Change: 59.790 -> 57.929
2024-12-01-23:40:59-root-INFO: Regularization Change: 0.000 -> 1.877
2024-12-01-23:40:59-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-01-23:40:59-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-23:40:59-root-INFO: step: 59 lr_xt 0.21581084
2024-12-01-23:41:00-root-INFO: grad norm: 4.242 4.174 0.760
2024-12-01-23:41:00-root-INFO: Loss too large (58.096->59.184)! Learning rate decreased to 0.17265.
2024-12-01-23:41:00-root-INFO: Loss too large (58.096->58.424)! Learning rate decreased to 0.13812.
2024-12-01-23:41:01-root-INFO: grad norm: 3.756 3.720 0.523
2024-12-01-23:41:02-root-INFO: grad norm: 3.271 3.240 0.450
2024-12-01-23:41:03-root-INFO: grad norm: 3.268 3.240 0.432
2024-12-01-23:41:04-root-INFO: grad norm: 3.289 3.262 0.416
2024-12-01-23:41:05-root-INFO: grad norm: 3.266 3.239 0.425
2024-12-01-23:41:06-root-INFO: grad norm: 3.253 3.227 0.406
2024-12-01-23:41:07-root-INFO: grad norm: 3.223 3.196 0.421
2024-12-01-23:41:07-root-INFO: Loss Change: 58.096 -> 56.261
2024-12-01-23:41:07-root-INFO: Regularization Change: 0.000 -> 1.775
2024-12-01-23:41:07-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-01-23:41:07-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-23:41:07-root-INFO: step: 58 lr_xt 0.22050848
2024-12-01-23:41:08-root-INFO: grad norm: 4.243 4.180 0.729
2024-12-01-23:41:08-root-INFO: Loss too large (56.313->57.496)! Learning rate decreased to 0.17641.
2024-12-01-23:41:08-root-INFO: Loss too large (56.313->56.695)! Learning rate decreased to 0.14113.
2024-12-01-23:41:09-root-INFO: grad norm: 3.779 3.741 0.534
2024-12-01-23:41:10-root-INFO: grad norm: 3.306 3.275 0.451
2024-12-01-23:41:11-root-INFO: grad norm: 3.259 3.230 0.437
2024-12-01-23:41:12-root-INFO: grad norm: 3.254 3.228 0.412
2024-12-01-23:41:13-root-INFO: grad norm: 3.232 3.203 0.427
2024-12-01-23:41:14-root-INFO: grad norm: 3.213 3.188 0.401
2024-12-01-23:41:14-root-INFO: grad norm: 3.200 3.172 0.423
2024-12-01-23:41:15-root-INFO: Loss Change: 56.313 -> 54.548
2024-12-01-23:41:15-root-INFO: Regularization Change: 0.000 -> 1.712
2024-12-01-23:41:15-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-01-23:41:15-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-23:41:15-root-INFO: step: 57 lr_xt 0.22527231
2024-12-01-23:41:16-root-INFO: grad norm: 4.413 4.353 0.725
2024-12-01-23:41:16-root-INFO: Loss too large (54.580->55.884)! Learning rate decreased to 0.18022.
2024-12-01-23:41:16-root-INFO: Loss too large (54.580->55.043)! Learning rate decreased to 0.14417.
2024-12-01-23:41:17-root-INFO: grad norm: 3.825 3.784 0.561
2024-12-01-23:41:18-root-INFO: grad norm: 3.169 3.138 0.438
2024-12-01-23:41:19-root-INFO: grad norm: 3.170 3.140 0.433
2024-12-01-23:41:20-root-INFO: grad norm: 3.233 3.208 0.406
2024-12-01-23:41:21-root-INFO: grad norm: 3.248 3.219 0.431
2024-12-01-23:41:21-root-INFO: grad norm: 3.260 3.235 0.404
2024-12-01-23:41:22-root-INFO: grad norm: 3.257 3.229 0.431
2024-12-01-23:41:23-root-INFO: Loss Change: 54.580 -> 52.853
2024-12-01-23:41:23-root-INFO: Regularization Change: 0.000 -> 1.679
2024-12-01-23:41:23-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-01-23:41:23-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-23:41:23-root-INFO: step: 56 lr_xt 0.23010221
2024-12-01-23:41:23-root-INFO: grad norm: 4.105 4.046 0.693
2024-12-01-23:41:24-root-INFO: Loss too large (53.041->54.250)! Learning rate decreased to 0.18408.
2024-12-01-23:41:24-root-INFO: Loss too large (53.041->53.442)! Learning rate decreased to 0.14727.
2024-12-01-23:41:25-root-INFO: grad norm: 3.720 3.684 0.518
2024-12-01-23:41:26-root-INFO: grad norm: 3.361 3.332 0.439
2024-12-01-23:41:27-root-INFO: grad norm: 3.329 3.300 0.440
2024-12-01-23:41:28-root-INFO: grad norm: 3.312 3.287 0.407
2024-12-01-23:41:29-root-INFO: grad norm: 3.292 3.263 0.431
2024-12-01-23:41:29-root-INFO: grad norm: 3.270 3.245 0.398
2024-12-01-23:41:30-root-INFO: grad norm: 3.253 3.225 0.426
2024-12-01-23:41:31-root-INFO: Loss Change: 53.041 -> 51.424
2024-12-01-23:41:31-root-INFO: Regularization Change: 0.000 -> 1.655
2024-12-01-23:41:31-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-01-23:41:31-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-23:41:31-root-INFO: step: 55 lr_xt 0.23499803
2024-12-01-23:41:32-root-INFO: grad norm: 4.405 4.340 0.756
2024-12-01-23:41:32-root-INFO: Loss too large (51.628->53.047)! Learning rate decreased to 0.18800.
2024-12-01-23:41:32-root-INFO: Loss too large (51.628->52.138)! Learning rate decreased to 0.15040.
2024-12-01-23:41:33-root-INFO: grad norm: 3.858 3.818 0.555
2024-12-01-23:41:34-root-INFO: grad norm: 3.273 3.243 0.441
2024-12-01-23:41:35-root-INFO: grad norm: 3.252 3.223 0.433
2024-12-01-23:41:36-root-INFO: grad norm: 3.281 3.256 0.406
2024-12-01-23:41:37-root-INFO: grad norm: 3.276 3.248 0.429
2024-12-01-23:41:38-root-INFO: grad norm: 3.270 3.246 0.400
2024-12-01-23:41:39-root-INFO: grad norm: 3.261 3.233 0.426
2024-12-01-23:41:39-root-INFO: Loss Change: 51.628 -> 49.970
2024-12-01-23:41:39-root-INFO: Regularization Change: 0.000 -> 1.648
2024-12-01-23:41:39-root-INFO: Undo step: 55
2024-12-01-23:41:39-root-INFO: Undo step: 56
2024-12-01-23:41:39-root-INFO: Undo step: 57
2024-12-01-23:41:39-root-INFO: Undo step: 58
2024-12-01-23:41:39-root-INFO: Undo step: 59
2024-12-01-23:41:40-root-INFO: step: 60 lr_xt 0.21117946
2024-12-01-23:41:40-root-INFO: grad norm: 25.747 25.485 3.662
2024-12-01-23:41:41-root-INFO: grad norm: 13.374 13.265 1.705
2024-12-01-23:41:42-root-INFO: grad norm: 10.582 10.456 1.627
2024-12-01-23:41:43-root-INFO: grad norm: 11.871 11.748 1.703
2024-12-01-23:41:43-root-INFO: grad norm: 9.999 9.837 1.790
2024-12-01-23:41:44-root-INFO: grad norm: 9.524 9.424 1.383
2024-12-01-23:41:45-root-INFO: grad norm: 9.461 9.344 1.484
2024-12-01-23:41:46-root-INFO: grad norm: 10.202 10.068 1.650
2024-12-01-23:41:47-root-INFO: Loss Change: 169.227 -> 70.133
2024-12-01-23:41:47-root-INFO: Regularization Change: 0.000 -> 92.284
2024-12-01-23:41:47-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-01-23:41:47-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-23:41:47-root-INFO: step: 59 lr_xt 0.21581084
2024-12-01-23:41:47-root-INFO: grad norm: 9.584 9.463 1.516
2024-12-01-23:41:48-root-INFO: grad norm: 9.712 9.584 1.573
2024-12-01-23:41:49-root-INFO: grad norm: 9.849 9.702 1.699
2024-12-01-23:41:50-root-INFO: grad norm: 10.224 10.079 1.715
2024-12-01-23:41:50-root-INFO: Loss too large (65.683->66.300)! Learning rate decreased to 0.17265.
2024-12-01-23:41:51-root-INFO: grad norm: 7.036 6.920 1.273
2024-12-01-23:41:52-root-INFO: grad norm: 4.747 4.680 0.793
2024-12-01-23:41:53-root-INFO: grad norm: 3.982 3.925 0.668
2024-12-01-23:41:54-root-INFO: grad norm: 3.813 3.765 0.605
2024-12-01-23:41:54-root-INFO: Loss Change: 69.069 -> 58.349
2024-12-01-23:41:54-root-INFO: Regularization Change: 0.000 -> 9.623
2024-12-01-23:41:54-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-01-23:41:54-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-23:41:55-root-INFO: step: 58 lr_xt 0.22050848
2024-12-01-23:41:55-root-INFO: grad norm: 3.261 3.230 0.450
2024-12-01-23:41:56-root-INFO: grad norm: 4.805 4.763 0.635
2024-12-01-23:41:56-root-INFO: Loss too large (57.615->58.204)! Learning rate decreased to 0.17641.
2024-12-01-23:41:57-root-INFO: grad norm: 4.108 4.058 0.642
2024-12-01-23:41:58-root-INFO: grad norm: 3.124 3.080 0.525
2024-12-01-23:41:58-root-INFO: grad norm: 3.109 3.071 0.489
2024-12-01-23:41:59-root-INFO: grad norm: 3.395 3.356 0.513
2024-12-01-23:42:00-root-INFO: grad norm: 3.506 3.463 0.545
2024-12-01-23:42:01-root-INFO: grad norm: 3.719 3.676 0.560
2024-12-01-23:42:02-root-INFO: Loss Change: 57.951 -> 54.897
2024-12-01-23:42:02-root-INFO: Regularization Change: 0.000 -> 4.836
2024-12-01-23:42:02-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-01-23:42:02-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-23:42:02-root-INFO: step: 57 lr_xt 0.22527231
2024-12-01-23:42:02-root-INFO: grad norm: 3.317 3.287 0.443
2024-12-01-23:42:03-root-INFO: grad norm: 5.142 5.101 0.646
2024-12-01-23:42:03-root-INFO: Loss too large (54.301->55.289)! Learning rate decreased to 0.18022.
2024-12-01-23:42:04-root-INFO: Loss too large (54.301->54.432)! Learning rate decreased to 0.14417.
2024-12-01-23:42:05-root-INFO: grad norm: 3.553 3.513 0.537
2024-12-01-23:42:05-root-INFO: grad norm: 2.063 2.037 0.328
2024-12-01-23:42:06-root-INFO: grad norm: 1.792 1.772 0.272
2024-12-01-23:42:07-root-INFO: grad norm: 1.630 1.610 0.252
2024-12-01-23:42:08-root-INFO: grad norm: 1.545 1.527 0.237
2024-12-01-23:42:09-root-INFO: grad norm: 1.483 1.464 0.231
2024-12-01-23:42:09-root-INFO: Loss Change: 54.392 -> 51.762
2024-12-01-23:42:09-root-INFO: Regularization Change: 0.000 -> 2.679
2024-12-01-23:42:09-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-01-23:42:09-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-23:42:10-root-INFO: step: 56 lr_xt 0.23010221
2024-12-01-23:42:10-root-INFO: grad norm: 1.804 1.763 0.381
2024-12-01-23:42:11-root-INFO: grad norm: 1.731 1.707 0.286
2024-12-01-23:42:12-root-INFO: grad norm: 1.927 1.905 0.290
2024-12-01-23:42:13-root-INFO: grad norm: 2.635 2.617 0.307
2024-12-01-23:42:13-root-INFO: Loss too large (50.838->50.944)! Learning rate decreased to 0.18408.
2024-12-01-23:42:14-root-INFO: grad norm: 2.853 2.833 0.340
2024-12-01-23:42:15-root-INFO: grad norm: 3.535 3.514 0.387
2024-12-01-23:42:15-root-INFO: Loss too large (50.500->50.628)! Learning rate decreased to 0.14727.
2024-12-01-23:42:16-root-INFO: grad norm: 2.971 2.946 0.386
2024-12-01-23:42:17-root-INFO: grad norm: 2.400 2.379 0.318
2024-12-01-23:42:17-root-INFO: Loss Change: 51.769 -> 49.698
2024-12-01-23:42:17-root-INFO: Regularization Change: 0.000 -> 3.242
2024-12-01-23:42:17-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-01-23:42:17-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-23:42:18-root-INFO: step: 55 lr_xt 0.23499803
2024-12-01-23:42:18-root-INFO: grad norm: 2.193 2.159 0.384
2024-12-01-23:42:19-root-INFO: grad norm: 3.264 3.244 0.359
2024-12-01-23:42:19-root-INFO: Loss too large (49.423->49.905)! Learning rate decreased to 0.18800.
2024-12-01-23:42:20-root-INFO: Loss too large (49.423->49.463)! Learning rate decreased to 0.15040.
2024-12-01-23:42:20-root-INFO: grad norm: 2.774 2.753 0.347
2024-12-01-23:42:21-root-INFO: grad norm: 2.437 2.417 0.314
2024-12-01-23:42:22-root-INFO: grad norm: 2.347 2.325 0.318
2024-12-01-23:42:23-root-INFO: grad norm: 2.270 2.250 0.304
2024-12-01-23:42:24-root-INFO: grad norm: 2.239 2.218 0.310
2024-12-01-23:42:25-root-INFO: grad norm: 2.220 2.200 0.298
2024-12-01-23:42:25-root-INFO: Loss Change: 49.571 -> 47.947
2024-12-01-23:42:25-root-INFO: Regularization Change: 0.000 -> 2.089
2024-12-01-23:42:25-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-01-23:42:25-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-23:42:26-root-INFO: step: 54 lr_xt 0.23995961
2024-12-01-23:42:26-root-INFO: grad norm: 2.165 2.137 0.348
2024-12-01-23:42:27-root-INFO: grad norm: 3.602 3.581 0.384
2024-12-01-23:42:27-root-INFO: Loss too large (47.685->48.423)! Learning rate decreased to 0.19197.
2024-12-01-23:42:27-root-INFO: Loss too large (47.685->47.902)! Learning rate decreased to 0.15357.
2024-12-01-23:42:28-root-INFO: grad norm: 3.089 3.063 0.401
2024-12-01-23:42:29-root-INFO: grad norm: 2.578 2.554 0.348
2024-12-01-23:42:30-root-INFO: grad norm: 2.513 2.489 0.343
2024-12-01-23:42:31-root-INFO: grad norm: 2.457 2.435 0.331
2024-12-01-23:42:32-root-INFO: grad norm: 2.435 2.412 0.334
2024-12-01-23:42:32-root-INFO: grad norm: 2.419 2.397 0.323
2024-12-01-23:42:33-root-INFO: Loss Change: 47.749 -> 46.275
2024-12-01-23:42:33-root-INFO: Regularization Change: 0.000 -> 2.002
2024-12-01-23:42:33-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-01-23:42:33-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-23:42:33-root-INFO: step: 53 lr_xt 0.24498673
2024-12-01-23:42:34-root-INFO: grad norm: 2.168 2.146 0.307
2024-12-01-23:42:34-root-INFO: Loss too large (46.199->46.254)! Learning rate decreased to 0.19599.
2024-12-01-23:42:35-root-INFO: grad norm: 2.790 2.772 0.321
2024-12-01-23:42:35-root-INFO: Loss too large (46.035->46.159)! Learning rate decreased to 0.15679.
2024-12-01-23:42:36-root-INFO: grad norm: 2.685 2.662 0.356
2024-12-01-23:42:37-root-INFO: grad norm: 2.605 2.584 0.334
2024-12-01-23:42:38-root-INFO: grad norm: 2.592 2.567 0.355
2024-12-01-23:42:39-root-INFO: grad norm: 2.576 2.555 0.333
2024-12-01-23:42:40-root-INFO: grad norm: 2.574 2.550 0.353
2024-12-01-23:42:40-root-INFO: grad norm: 2.572 2.550 0.332
2024-12-01-23:42:41-root-INFO: Loss Change: 46.199 -> 44.916
2024-12-01-23:42:41-root-INFO: Regularization Change: 0.000 -> 1.776
2024-12-01-23:42:41-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-01-23:42:41-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-23:42:41-root-INFO: step: 52 lr_xt 0.25007913
2024-12-01-23:42:42-root-INFO: grad norm: 2.454 2.433 0.323
2024-12-01-23:42:42-root-INFO: Loss too large (44.777->45.017)! Learning rate decreased to 0.20006.
2024-12-01-23:42:43-root-INFO: grad norm: 3.273 3.253 0.358
2024-12-01-23:42:43-root-INFO: Loss too large (44.669->44.928)! Learning rate decreased to 0.16005.
2024-12-01-23:42:44-root-INFO: grad norm: 3.053 3.027 0.395
2024-12-01-23:42:45-root-INFO: grad norm: 2.811 2.789 0.346
2024-12-01-23:42:46-root-INFO: grad norm: 2.728 2.704 0.366
2024-12-01-23:42:47-root-INFO: grad norm: 2.642 2.621 0.330
2024-12-01-23:42:47-root-INFO: grad norm: 2.599 2.575 0.352
2024-12-01-23:42:48-root-INFO: grad norm: 2.557 2.537 0.321
2024-12-01-23:42:49-root-INFO: Loss Change: 44.777 -> 43.474
2024-12-01-23:42:49-root-INFO: Regularization Change: 0.000 -> 1.746
2024-12-01-23:42:49-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-01-23:42:49-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-23:42:49-root-INFO: step: 51 lr_xt 0.25523653
2024-12-01-23:42:49-root-INFO: grad norm: 2.439 2.409 0.379
2024-12-01-23:42:50-root-INFO: Loss too large (43.330->43.506)! Learning rate decreased to 0.20419.
2024-12-01-23:42:51-root-INFO: grad norm: 3.102 3.083 0.337
2024-12-01-23:42:51-root-INFO: Loss too large (43.184->43.406)! Learning rate decreased to 0.16335.
2024-12-01-23:42:52-root-INFO: grad norm: 2.923 2.900 0.363
2024-12-01-23:42:53-root-INFO: grad norm: 2.775 2.754 0.341
2024-12-01-23:42:53-root-INFO: grad norm: 2.735 2.712 0.357
2024-12-01-23:42:54-root-INFO: grad norm: 2.695 2.674 0.336
2024-12-01-23:42:55-root-INFO: grad norm: 2.674 2.650 0.355
2024-12-01-23:42:56-root-INFO: grad norm: 2.654 2.633 0.332
2024-12-01-23:42:57-root-INFO: Loss Change: 43.330 -> 42.084
2024-12-01-23:42:57-root-INFO: Regularization Change: 0.000 -> 1.747
2024-12-01-23:42:57-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-01-23:42:57-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-23:42:57-root-INFO: step: 50 lr_xt 0.26045862
2024-12-01-23:42:57-root-INFO: grad norm: 2.456 2.432 0.340
2024-12-01-23:42:57-root-INFO: Loss too large (41.717->41.978)! Learning rate decreased to 0.20837.
2024-12-01-23:42:58-root-INFO: grad norm: 3.250 3.230 0.358
2024-12-01-23:42:59-root-INFO: Loss too large (41.614->41.897)! Learning rate decreased to 0.16669.
2024-12-01-23:42:59-root-INFO: grad norm: 3.035 3.011 0.387
2024-12-01-23:43:00-root-INFO: grad norm: 2.814 2.792 0.346
2024-12-01-23:43:01-root-INFO: grad norm: 2.753 2.729 0.366
2024-12-01-23:43:02-root-INFO: grad norm: 2.692 2.671 0.335
2024-12-01-23:43:03-root-INFO: grad norm: 2.661 2.637 0.357
2024-12-01-23:43:04-root-INFO: grad norm: 2.630 2.609 0.328
2024-12-01-23:43:04-root-INFO: Loss Change: 41.717 -> 40.496
2024-12-01-23:43:04-root-INFO: Regularization Change: 0.000 -> 1.731
2024-12-01-23:43:04-root-INFO: Undo step: 50
2024-12-01-23:43:04-root-INFO: Undo step: 51
2024-12-01-23:43:04-root-INFO: Undo step: 52
2024-12-01-23:43:04-root-INFO: Undo step: 53
2024-12-01-23:43:04-root-INFO: Undo step: 54
2024-12-01-23:43:04-root-INFO: step: 55 lr_xt 0.23499803
2024-12-01-23:43:05-root-INFO: grad norm: 22.414 22.275 2.488
2024-12-01-23:43:06-root-INFO: grad norm: 10.417 10.089 2.591
2024-12-01-23:43:06-root-INFO: grad norm: 8.883 8.686 1.860
2024-12-01-23:43:07-root-INFO: grad norm: 7.468 6.979 2.658
2024-12-01-23:43:08-root-INFO: grad norm: 5.989 5.879 1.143
2024-12-01-23:43:09-root-INFO: grad norm: 5.748 5.613 1.239
2024-12-01-23:43:10-root-INFO: grad norm: 6.412 6.329 1.032
2024-12-01-23:43:11-root-INFO: grad norm: 6.242 6.130 1.179
2024-12-01-23:43:11-root-INFO: Loss Change: 154.761 -> 55.974
2024-12-01-23:43:11-root-INFO: Regularization Change: 0.000 -> 101.524
2024-12-01-23:43:11-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-01-23:43:11-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-23:43:12-root-INFO: step: 54 lr_xt 0.23995961
2024-12-01-23:43:12-root-INFO: grad norm: 6.559 6.442 1.234
2024-12-01-23:43:13-root-INFO: grad norm: 6.590 6.468 1.262
2024-12-01-23:43:14-root-INFO: grad norm: 7.086 6.986 1.185
2024-12-01-23:43:14-root-INFO: Loss too large (54.192->54.480)! Learning rate decreased to 0.19197.
2024-12-01-23:43:15-root-INFO: grad norm: 5.020 4.935 0.922
2024-12-01-23:43:16-root-INFO: grad norm: 3.228 3.173 0.596
2024-12-01-23:43:16-root-INFO: grad norm: 2.736 2.696 0.465
2024-12-01-23:43:17-root-INFO: grad norm: 2.579 2.543 0.431
2024-12-01-23:43:18-root-INFO: grad norm: 2.580 2.547 0.410
2024-12-01-23:43:19-root-INFO: Loss Change: 56.061 -> 48.577
2024-12-01-23:43:19-root-INFO: Regularization Change: 0.000 -> 9.583
2024-12-01-23:43:19-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-01-23:43:19-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-23:43:19-root-INFO: step: 53 lr_xt 0.24498673
2024-12-01-23:43:19-root-INFO: grad norm: 3.666 3.603 0.677
2024-12-01-23:43:19-root-INFO: Loss too large (48.774->48.916)! Learning rate decreased to 0.19599.
2024-12-01-23:43:20-root-INFO: grad norm: 3.580 3.529 0.603
2024-12-01-23:43:21-root-INFO: grad norm: 3.620 3.577 0.554
2024-12-01-23:43:22-root-INFO: grad norm: 3.602 3.555 0.585
2024-12-01-23:43:23-root-INFO: grad norm: 3.588 3.548 0.537
2024-12-01-23:43:24-root-INFO: grad norm: 3.580 3.533 0.578
2024-12-01-23:43:25-root-INFO: grad norm: 3.573 3.534 0.530
2024-12-01-23:43:26-root-INFO: grad norm: 3.562 3.515 0.574
2024-12-01-23:43:26-root-INFO: Loss Change: 48.774 -> 45.666
2024-12-01-23:43:26-root-INFO: Regularization Change: 0.000 -> 4.877
2024-12-01-23:43:26-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-01-23:43:26-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-23:43:26-root-INFO: step: 52 lr_xt 0.25007913
2024-12-01-23:43:27-root-INFO: grad norm: 4.365 4.291 0.802
2024-12-01-23:43:27-root-INFO: Loss too large (45.820->46.497)! Learning rate decreased to 0.20006.
2024-12-01-23:43:28-root-INFO: grad norm: 4.022 3.959 0.714
2024-12-01-23:43:29-root-INFO: grad norm: 3.617 3.571 0.571
2024-12-01-23:43:30-root-INFO: grad norm: 3.539 3.491 0.585
2024-12-01-23:43:30-root-INFO: grad norm: 3.505 3.466 0.520
2024-12-01-23:43:31-root-INFO: grad norm: 3.465 3.420 0.557
2024-12-01-23:43:32-root-INFO: grad norm: 3.423 3.386 0.503
2024-12-01-23:43:33-root-INFO: grad norm: 3.408 3.364 0.544
2024-12-01-23:43:34-root-INFO: Loss Change: 45.820 -> 43.190
2024-12-01-23:43:34-root-INFO: Regularization Change: 0.000 -> 3.721
2024-12-01-23:43:34-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-01-23:43:34-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-23:43:34-root-INFO: step: 51 lr_xt 0.25523653
2024-12-01-23:43:34-root-INFO: grad norm: 4.537 4.446 0.902
2024-12-01-23:43:35-root-INFO: Loss too large (43.454->44.305)! Learning rate decreased to 0.20419.
2024-12-01-23:43:36-root-INFO: grad norm: 4.118 4.052 0.732
2024-12-01-23:43:36-root-INFO: grad norm: 3.623 3.573 0.598
2024-12-01-23:43:37-root-INFO: grad norm: 3.528 3.480 0.581
2024-12-01-23:43:38-root-INFO: grad norm: 3.490 3.450 0.530
2024-12-01-23:43:39-root-INFO: grad norm: 3.429 3.384 0.551
2024-12-01-23:43:40-root-INFO: grad norm: 3.358 3.320 0.504
2024-12-01-23:43:41-root-INFO: grad norm: 3.338 3.295 0.533
2024-12-01-23:43:42-root-INFO: Loss Change: 43.454 -> 41.108
2024-12-01-23:43:42-root-INFO: Regularization Change: 0.000 -> 3.155
2024-12-01-23:43:42-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-01-23:43:42-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-23:43:42-root-INFO: step: 50 lr_xt 0.26045862
2024-12-01-23:43:42-root-INFO: grad norm: 4.341 4.259 0.843
2024-12-01-23:43:43-root-INFO: Loss too large (41.128->41.948)! Learning rate decreased to 0.20837.
2024-12-01-23:43:43-root-INFO: grad norm: 3.941 3.877 0.707
2024-12-01-23:43:44-root-INFO: grad norm: 3.474 3.427 0.568
2024-12-01-23:43:45-root-INFO: grad norm: 3.386 3.339 0.563
2024-12-01-23:43:46-root-INFO: grad norm: 3.352 3.313 0.508
2024-12-01-23:43:47-root-INFO: grad norm: 3.300 3.257 0.533
2024-12-01-23:43:48-root-INFO: grad norm: 3.244 3.207 0.486
2024-12-01-23:43:49-root-INFO: grad norm: 3.225 3.184 0.517
2024-12-01-23:43:49-root-INFO: Loss Change: 41.128 -> 39.050
2024-12-01-23:43:49-root-INFO: Regularization Change: 0.000 -> 2.818
2024-12-01-23:43:49-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-01-23:43:49-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-23:43:50-root-INFO: step: 49 lr_xt 0.26574501
2024-12-01-23:43:50-root-INFO: grad norm: 4.393 4.315 0.823
2024-12-01-23:43:50-root-INFO: Loss too large (39.423->40.248)! Learning rate decreased to 0.21260.
2024-12-01-23:43:51-root-INFO: grad norm: 3.908 3.845 0.697
2024-12-01-23:43:52-root-INFO: grad norm: 3.358 3.312 0.551
2024-12-01-23:43:53-root-INFO: grad norm: 3.250 3.205 0.540
2024-12-01-23:43:54-root-INFO: grad norm: 3.194 3.157 0.482
2024-12-01-23:43:55-root-INFO: grad norm: 3.144 3.103 0.504
2024-12-01-23:43:55-root-INFO: grad norm: 3.100 3.065 0.460
2024-12-01-23:43:56-root-INFO: grad norm: 3.078 3.039 0.488
2024-12-01-23:43:57-root-INFO: Loss Change: 39.423 -> 37.401
2024-12-01-23:43:57-root-INFO: Regularization Change: 0.000 -> 2.634
2024-12-01-23:43:57-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-01-23:43:57-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-23:43:57-root-INFO: step: 48 lr_xt 0.27109532
2024-12-01-23:43:58-root-INFO: grad norm: 3.778 3.708 0.723
2024-12-01-23:43:58-root-INFO: Loss too large (37.510->38.145)! Learning rate decreased to 0.21688.
2024-12-01-23:43:59-root-INFO: grad norm: 3.502 3.447 0.622
2024-12-01-23:44:00-root-INFO: grad norm: 3.239 3.199 0.511
2024-12-01-23:44:01-root-INFO: grad norm: 3.155 3.112 0.519
2024-12-01-23:44:01-root-INFO: grad norm: 3.079 3.045 0.459
2024-12-01-23:44:02-root-INFO: grad norm: 3.041 3.002 0.485
2024-12-01-23:44:03-root-INFO: grad norm: 3.004 2.972 0.440
2024-12-01-23:44:04-root-INFO: grad norm: 2.989 2.951 0.471
2024-12-01-23:44:05-root-INFO: Loss Change: 37.510 -> 35.830
2024-12-01-23:44:05-root-INFO: Regularization Change: 0.000 -> 2.450
2024-12-01-23:44:05-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-01-23:44:05-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-23:44:05-root-INFO: step: 47 lr_xt 0.27650911
2024-12-01-23:44:05-root-INFO: grad norm: 4.084 3.999 0.830
2024-12-01-23:44:05-root-INFO: Loss too large (36.122->36.901)! Learning rate decreased to 0.22121.
2024-12-01-23:44:06-root-INFO: grad norm: 3.703 3.642 0.670
2024-12-01-23:44:07-root-INFO: grad norm: 3.304 3.258 0.550
2024-12-01-23:44:08-root-INFO: grad norm: 3.192 3.148 0.529
2024-12-01-23:44:09-root-INFO: grad norm: 3.093 3.056 0.476
2024-12-01-23:44:10-root-INFO: grad norm: 3.040 3.001 0.486
2024-12-01-23:44:10-root-INFO: grad norm: 2.988 2.955 0.447
2024-12-01-23:44:11-root-INFO: grad norm: 2.965 2.928 0.467
2024-12-01-23:44:12-root-INFO: Loss Change: 36.122 -> 34.383
2024-12-01-23:44:12-root-INFO: Regularization Change: 0.000 -> 2.371
2024-12-01-23:44:12-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-01-23:44:12-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-23:44:12-root-INFO: step: 46 lr_xt 0.28198590
2024-12-01-23:44:12-root-INFO: grad norm: 3.841 3.771 0.729
2024-12-01-23:44:13-root-INFO: Loss too large (34.597->35.298)! Learning rate decreased to 0.22559.
2024-12-01-23:44:14-root-INFO: grad norm: 3.520 3.465 0.620
2024-12-01-23:44:14-root-INFO: grad norm: 3.196 3.155 0.510
2024-12-01-23:44:15-root-INFO: grad norm: 3.092 3.050 0.507
2024-12-01-23:44:16-root-INFO: grad norm: 2.993 2.959 0.449
2024-12-01-23:44:17-root-INFO: grad norm: 2.944 2.907 0.467
2024-12-01-23:44:18-root-INFO: grad norm: 2.898 2.867 0.426
2024-12-01-23:44:19-root-INFO: grad norm: 2.876 2.841 0.450
2024-12-01-23:44:19-root-INFO: Loss Change: 34.597 -> 32.994
2024-12-01-23:44:19-root-INFO: Regularization Change: 0.000 -> 2.278
2024-12-01-23:44:19-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-01-23:44:19-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-23:44:20-root-INFO: step: 45 lr_xt 0.28752516
2024-12-01-23:44:20-root-INFO: grad norm: 3.825 3.747 0.765
2024-12-01-23:44:20-root-INFO: Loss too large (33.148->33.853)! Learning rate decreased to 0.23002.
2024-12-01-23:44:21-root-INFO: grad norm: 3.501 3.444 0.630
2024-12-01-23:44:22-root-INFO: grad norm: 3.191 3.149 0.518
2024-12-01-23:44:23-root-INFO: grad norm: 3.074 3.031 0.510
2024-12-01-23:44:24-root-INFO: grad norm: 2.959 2.924 0.454
2024-12-01-23:44:24-root-INFO: grad norm: 2.907 2.869 0.470
2024-12-01-23:44:25-root-INFO: grad norm: 2.860 2.827 0.433
2024-12-01-23:44:26-root-INFO: grad norm: 2.840 2.803 0.458
2024-12-01-23:44:27-root-INFO: Loss Change: 33.148 -> 31.579
2024-12-01-23:44:27-root-INFO: Regularization Change: 0.000 -> 2.236
2024-12-01-23:44:27-root-INFO: Undo step: 45
2024-12-01-23:44:27-root-INFO: Undo step: 46
2024-12-01-23:44:27-root-INFO: Undo step: 47
2024-12-01-23:44:27-root-INFO: Undo step: 48
2024-12-01-23:44:27-root-INFO: Undo step: 49
2024-12-01-23:44:27-root-INFO: step: 50 lr_xt 0.26045862
2024-12-01-23:44:27-root-INFO: grad norm: 21.191 21.057 2.375
2024-12-01-23:44:28-root-INFO: grad norm: 10.603 10.503 1.451
2024-12-01-23:44:29-root-INFO: grad norm: 7.669 7.569 1.235
2024-12-01-23:44:30-root-INFO: grad norm: 6.933 6.863 0.986
2024-12-01-23:44:31-root-INFO: grad norm: 6.563 6.488 0.986
2024-12-01-23:44:32-root-INFO: grad norm: 5.806 5.747 0.826
2024-12-01-23:44:33-root-INFO: grad norm: 5.510 5.452 0.800
2024-12-01-23:44:34-root-INFO: grad norm: 5.662 5.606 0.793
2024-12-01-23:44:34-root-INFO: Loss Change: 137.432 -> 46.403
2024-12-01-23:44:34-root-INFO: Regularization Change: 0.000 -> 103.320
2024-12-01-23:44:34-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-01-23:44:34-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-23:44:35-root-INFO: step: 49 lr_xt 0.26574501
2024-12-01-23:44:35-root-INFO: grad norm: 5.743 5.700 0.704
2024-12-01-23:44:36-root-INFO: grad norm: 5.508 5.456 0.753
2024-12-01-23:44:37-root-INFO: grad norm: 5.254 5.189 0.828
2024-12-01-23:44:37-root-INFO: grad norm: 5.564 5.480 0.966
2024-12-01-23:44:38-root-INFO: grad norm: 5.884 5.794 1.023
2024-12-01-23:44:39-root-INFO: grad norm: 6.095 5.989 1.131
2024-12-01-23:44:40-root-INFO: grad norm: 6.301 6.209 1.074
2024-12-01-23:44:41-root-INFO: grad norm: 6.165 6.066 1.100
2024-12-01-23:44:42-root-INFO: Loss Change: 46.035 -> 40.645
2024-12-01-23:44:42-root-INFO: Regularization Change: 0.000 -> 13.085
2024-12-01-23:44:42-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-01-23:44:42-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-23:44:42-root-INFO: step: 48 lr_xt 0.27109532
2024-12-01-23:44:42-root-INFO: grad norm: 5.768 5.698 0.893
2024-12-01-23:44:43-root-INFO: grad norm: 5.535 5.463 0.885
2024-12-01-23:44:44-root-INFO: grad norm: 5.365 5.304 0.810
2024-12-01-23:44:45-root-INFO: grad norm: 5.252 5.193 0.786
2024-12-01-23:44:46-root-INFO: grad norm: 5.106 5.048 0.771
2024-12-01-23:44:47-root-INFO: grad norm: 5.065 5.011 0.739
2024-12-01-23:44:48-root-INFO: grad norm: 5.027 4.971 0.754
2024-12-01-23:44:49-root-INFO: grad norm: 4.964 4.912 0.717
2024-12-01-23:44:49-root-INFO: Loss Change: 40.327 -> 36.550
2024-12-01-23:44:49-root-INFO: Regularization Change: 0.000 -> 6.992
2024-12-01-23:44:49-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-01-23:44:49-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-23:44:49-root-INFO: step: 47 lr_xt 0.27650911
2024-12-01-23:44:50-root-INFO: grad norm: 4.764 4.726 0.601
2024-12-01-23:44:51-root-INFO: grad norm: 4.574 4.536 0.587
2024-12-01-23:44:52-root-INFO: grad norm: 4.333 4.295 0.571
2024-12-01-23:44:52-root-INFO: grad norm: 4.338 4.302 0.557
2024-12-01-23:44:53-root-INFO: grad norm: 4.437 4.401 0.569
2024-12-01-23:44:54-root-INFO: grad norm: 4.326 4.292 0.546
2024-12-01-23:44:55-root-INFO: grad norm: 4.138 4.101 0.554
2024-12-01-23:44:56-root-INFO: grad norm: 4.164 4.129 0.541
2024-12-01-23:44:57-root-INFO: Loss Change: 36.268 -> 33.743
2024-12-01-23:44:57-root-INFO: Regularization Change: 0.000 -> 5.008
2024-12-01-23:44:57-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-01-23:44:57-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-23:44:57-root-INFO: step: 46 lr_xt 0.28198590
2024-12-01-23:44:57-root-INFO: grad norm: 4.243 4.213 0.500
2024-12-01-23:44:58-root-INFO: grad norm: 4.006 3.977 0.481
2024-12-01-23:44:59-root-INFO: grad norm: 3.674 3.645 0.466
2024-12-01-23:45:00-root-INFO: grad norm: 3.728 3.700 0.449
2024-12-01-23:45:01-root-INFO: grad norm: 3.923 3.895 0.472
2024-12-01-23:45:02-root-INFO: grad norm: 3.847 3.820 0.454
2024-12-01-23:45:02-root-INFO: grad norm: 3.689 3.660 0.458
2024-12-01-23:45:03-root-INFO: grad norm: 3.708 3.681 0.443
2024-12-01-23:45:04-root-INFO: Loss Change: 33.565 -> 31.555
2024-12-01-23:45:04-root-INFO: Regularization Change: 0.000 -> 4.077
2024-12-01-23:45:04-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-01-23:45:04-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-23:45:04-root-INFO: step: 45 lr_xt 0.28752516
2024-12-01-23:45:04-root-INFO: grad norm: 3.902 3.868 0.512
2024-12-01-23:45:05-root-INFO: grad norm: 3.694 3.662 0.490
2024-12-01-23:45:06-root-INFO: grad norm: 3.410 3.379 0.461
2024-12-01-23:45:07-root-INFO: grad norm: 3.433 3.403 0.451
2024-12-01-23:45:08-root-INFO: grad norm: 3.595 3.566 0.456
2024-12-01-23:45:09-root-INFO: grad norm: 3.497 3.466 0.462
2024-12-01-23:45:10-root-INFO: grad norm: 3.322 3.292 0.450
2024-12-01-23:45:11-root-INFO: grad norm: 3.350 3.319 0.452
2024-12-01-23:45:11-root-INFO: Loss Change: 31.365 -> 29.596
2024-12-01-23:45:11-root-INFO: Regularization Change: 0.000 -> 3.583
2024-12-01-23:45:11-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-01-23:45:11-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-23:45:11-root-INFO: step: 44 lr_xt 0.29312635
2024-12-01-23:45:12-root-INFO: grad norm: 3.928 3.866 0.699
2024-12-01-23:45:12-root-INFO: Loss too large (29.694->29.701)! Learning rate decreased to 0.23450.
2024-12-01-23:45:13-root-INFO: grad norm: 2.694 2.655 0.455
2024-12-01-23:45:14-root-INFO: grad norm: 1.926 1.900 0.320
2024-12-01-23:45:15-root-INFO: grad norm: 1.725 1.703 0.273
2024-12-01-23:45:16-root-INFO: grad norm: 1.667 1.649 0.244
2024-12-01-23:45:16-root-INFO: grad norm: 1.688 1.669 0.253
2024-12-01-23:45:17-root-INFO: grad norm: 1.783 1.765 0.248
2024-12-01-23:45:18-root-INFO: grad norm: 1.883 1.862 0.279
2024-12-01-23:45:19-root-INFO: Loss Change: 29.694 -> 27.695
2024-12-01-23:45:19-root-INFO: Regularization Change: 0.000 -> 2.353
2024-12-01-23:45:19-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-01-23:45:19-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-23:45:19-root-INFO: step: 43 lr_xt 0.29878886
2024-12-01-23:45:19-root-INFO: grad norm: 2.967 2.902 0.618
2024-12-01-23:45:20-root-INFO: Loss too large (27.693->28.104)! Learning rate decreased to 0.23903.
2024-12-01-23:45:21-root-INFO: grad norm: 2.842 2.793 0.524
2024-12-01-23:45:21-root-INFO: grad norm: 2.791 2.754 0.450
2024-12-01-23:45:22-root-INFO: grad norm: 2.727 2.687 0.462
2024-12-01-23:45:23-root-INFO: grad norm: 2.629 2.598 0.405
2024-12-01-23:45:24-root-INFO: grad norm: 2.576 2.541 0.424
2024-12-01-23:45:25-root-INFO: grad norm: 2.508 2.479 0.379
2024-12-01-23:45:26-root-INFO: grad norm: 2.476 2.443 0.402
2024-12-01-23:45:26-root-INFO: Loss Change: 27.693 -> 26.473
2024-12-01-23:45:26-root-INFO: Regularization Change: 0.000 -> 2.052
2024-12-01-23:45:26-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-01-23:45:26-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-23:45:27-root-INFO: step: 42 lr_xt 0.30451205
2024-12-01-23:45:27-root-INFO: grad norm: 3.083 3.026 0.593
2024-12-01-23:45:27-root-INFO: Loss too large (26.549->27.000)! Learning rate decreased to 0.24361.
2024-12-01-23:45:28-root-INFO: grad norm: 2.875 2.827 0.522
2024-12-01-23:45:29-root-INFO: grad norm: 2.692 2.657 0.432
2024-12-01-23:45:30-root-INFO: grad norm: 2.599 2.561 0.439
2024-12-01-23:45:31-root-INFO: grad norm: 2.502 2.473 0.381
2024-12-01-23:45:31-root-INFO: grad norm: 2.455 2.422 0.403
2024-12-01-23:45:32-root-INFO: grad norm: 2.411 2.384 0.358
2024-12-01-23:45:33-root-INFO: grad norm: 2.392 2.360 0.387
2024-12-01-23:45:34-root-INFO: Loss Change: 26.549 -> 25.318
2024-12-01-23:45:34-root-INFO: Regularization Change: 0.000 -> 1.967
2024-12-01-23:45:34-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-01-23:45:34-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-23:45:34-root-INFO: step: 41 lr_xt 0.31082203
2024-12-01-23:45:34-root-INFO: grad norm: 3.714 3.621 0.824
2024-12-01-23:45:35-root-INFO: Loss too large (25.641->26.259)! Learning rate decreased to 0.24866.
2024-12-01-23:45:35-root-INFO: grad norm: 3.280 3.218 0.634
2024-12-01-23:45:36-root-INFO: grad norm: 2.904 2.861 0.496
2024-12-01-23:45:37-root-INFO: grad norm: 2.726 2.684 0.475
2024-12-01-23:45:38-root-INFO: grad norm: 2.556 2.524 0.403
2024-12-01-23:45:39-root-INFO: grad norm: 2.470 2.435 0.410
2024-12-01-23:45:40-root-INFO: grad norm: 2.392 2.364 0.362
2024-12-01-23:45:41-root-INFO: grad norm: 2.353 2.322 0.381
2024-12-01-23:45:41-root-INFO: Loss Change: 25.641 -> 24.130
2024-12-01-23:45:41-root-INFO: Regularization Change: 0.000 -> 2.013
2024-12-01-23:45:41-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-01-23:45:41-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-23:45:41-root-INFO: step: 40 lr_xt 0.31666177
2024-12-01-23:45:42-root-INFO: grad norm: 3.397 3.326 0.692
2024-12-01-23:45:42-root-INFO: Loss too large (24.379->24.962)! Learning rate decreased to 0.25333.
2024-12-01-23:45:43-root-INFO: grad norm: 3.089 3.036 0.575
2024-12-01-23:45:44-root-INFO: grad norm: 2.809 2.772 0.456
2024-12-01-23:45:45-root-INFO: grad norm: 2.671 2.632 0.455
2024-12-01-23:45:46-root-INFO: grad norm: 2.536 2.506 0.388
2024-12-01-23:45:47-root-INFO: grad norm: 2.462 2.428 0.405
2024-12-01-23:45:47-root-INFO: grad norm: 2.393 2.367 0.356
2024-12-01-23:45:48-root-INFO: grad norm: 2.357 2.326 0.381
2024-12-01-23:45:49-root-INFO: Loss Change: 24.379 -> 23.065
2024-12-01-23:45:49-root-INFO: Regularization Change: 0.000 -> 1.919
2024-12-01-23:45:49-root-INFO: Undo step: 40
2024-12-01-23:45:49-root-INFO: Undo step: 41
2024-12-01-23:45:49-root-INFO: Undo step: 42
2024-12-01-23:45:49-root-INFO: Undo step: 43
2024-12-01-23:45:49-root-INFO: Undo step: 44
2024-12-01-23:45:49-root-INFO: step: 45 lr_xt 0.28752516
2024-12-01-23:45:50-root-INFO: grad norm: 20.668 20.545 2.252
2024-12-01-23:45:50-root-INFO: grad norm: 10.559 10.453 1.491
2024-12-01-23:45:51-root-INFO: grad norm: 7.381 7.306 1.047
2024-12-01-23:45:52-root-INFO: grad norm: 6.152 6.084 0.913
2024-12-01-23:45:53-root-INFO: grad norm: 5.799 5.752 0.738
2024-12-01-23:45:54-root-INFO: grad norm: 5.055 5.007 0.689
2024-12-01-23:45:55-root-INFO: grad norm: 4.301 4.262 0.579
2024-12-01-23:45:56-root-INFO: grad norm: 4.233 4.197 0.554
2024-12-01-23:45:56-root-INFO: Loss Change: 132.695 -> 37.865
2024-12-01-23:45:56-root-INFO: Regularization Change: 0.000 -> 114.011
2024-12-01-23:45:56-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-01-23:45:56-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-23:45:57-root-INFO: step: 44 lr_xt 0.29312635
2024-12-01-23:45:57-root-INFO: grad norm: 5.075 5.024 0.722
2024-12-01-23:45:58-root-INFO: grad norm: 4.611 4.551 0.746
2024-12-01-23:45:59-root-INFO: grad norm: 4.032 3.978 0.658
2024-12-01-23:46:00-root-INFO: grad norm: 4.104 4.049 0.671
2024-12-01-23:46:00-root-INFO: grad norm: 4.661 4.609 0.692
2024-12-01-23:46:01-root-INFO: grad norm: 4.493 4.424 0.788
2024-12-01-23:46:02-root-INFO: grad norm: 4.204 4.138 0.745
2024-12-01-23:46:03-root-INFO: grad norm: 4.420 4.345 0.807
2024-12-01-23:46:04-root-INFO: Loss Change: 37.867 -> 32.335
2024-12-01-23:46:04-root-INFO: Regularization Change: 0.000 -> 13.347
2024-12-01-23:46:04-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-01-23:46:04-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-23:46:04-root-INFO: step: 43 lr_xt 0.29878886
2024-12-01-23:46:04-root-INFO: grad norm: 5.730 5.617 1.134
2024-12-01-23:46:05-root-INFO: Loss too large (32.542->32.758)! Learning rate decreased to 0.23903.
2024-12-01-23:46:05-root-INFO: grad norm: 3.981 3.899 0.804
2024-12-01-23:46:06-root-INFO: grad norm: 2.618 2.574 0.479
2024-12-01-23:46:07-root-INFO: grad norm: 2.242 2.208 0.390
2024-12-01-23:46:08-root-INFO: grad norm: 2.067 2.043 0.310
2024-12-01-23:46:09-root-INFO: grad norm: 1.997 1.974 0.305
2024-12-01-23:46:10-root-INFO: grad norm: 1.991 1.973 0.269
2024-12-01-23:46:11-root-INFO: grad norm: 2.003 1.983 0.288
2024-12-01-23:46:12-root-INFO: Loss Change: 32.542 -> 28.135
2024-12-01-23:46:12-root-INFO: Regularization Change: 0.000 -> 5.137
2024-12-01-23:46:12-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-01-23:46:12-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-23:46:12-root-INFO: step: 42 lr_xt 0.30451205
2024-12-01-23:46:12-root-INFO: grad norm: 2.521 2.479 0.456
2024-12-01-23:46:12-root-INFO: Loss too large (28.095->28.115)! Learning rate decreased to 0.24361.
2024-12-01-23:46:13-root-INFO: grad norm: 2.393 2.361 0.391
2024-12-01-23:46:14-root-INFO: grad norm: 2.349 2.327 0.321
2024-12-01-23:46:15-root-INFO: grad norm: 2.320 2.294 0.349
2024-12-01-23:46:16-root-INFO: grad norm: 2.297 2.277 0.301
2024-12-01-23:46:17-root-INFO: grad norm: 2.286 2.261 0.340
2024-12-01-23:46:18-root-INFO: grad norm: 2.280 2.261 0.296
2024-12-01-23:46:19-root-INFO: grad norm: 2.279 2.254 0.339
2024-12-01-23:46:19-root-INFO: Loss Change: 28.095 -> 26.219
2024-12-01-23:46:19-root-INFO: Regularization Change: 0.000 -> 3.550
2024-12-01-23:46:19-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-01-23:46:19-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-23:46:19-root-INFO: step: 41 lr_xt 0.31082203
2024-12-01-23:46:20-root-INFO: grad norm: 3.447 3.363 0.757
2024-12-01-23:46:20-root-INFO: Loss too large (26.420->26.769)! Learning rate decreased to 0.24866.
2024-12-01-23:46:21-root-INFO: grad norm: 3.048 2.994 0.568
2024-12-01-23:46:22-root-INFO: grad norm: 2.759 2.725 0.436
2024-12-01-23:46:23-root-INFO: grad norm: 2.628 2.592 0.436
2024-12-01-23:46:24-root-INFO: grad norm: 2.507 2.481 0.361
2024-12-01-23:46:24-root-INFO: grad norm: 2.447 2.416 0.387
2024-12-01-23:46:25-root-INFO: grad norm: 2.394 2.371 0.333
2024-12-01-23:46:26-root-INFO: grad norm: 2.372 2.343 0.368
2024-12-01-23:46:27-root-INFO: Loss Change: 26.420 -> 24.532
2024-12-01-23:46:27-root-INFO: Regularization Change: 0.000 -> 3.009
2024-12-01-23:46:27-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-01-23:46:27-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-23:46:27-root-INFO: step: 40 lr_xt 0.31666177
2024-12-01-23:46:27-root-INFO: grad norm: 3.352 3.286 0.662
2024-12-01-23:46:28-root-INFO: Loss too large (24.732->25.222)! Learning rate decreased to 0.25333.
2024-12-01-23:46:29-root-INFO: grad norm: 3.091 3.041 0.555
2024-12-01-23:46:30-root-INFO: grad norm: 2.897 2.864 0.436
2024-12-01-23:46:30-root-INFO: grad norm: 2.787 2.749 0.456
2024-12-01-23:46:31-root-INFO: grad norm: 2.685 2.658 0.381
2024-12-01-23:46:32-root-INFO: grad norm: 2.626 2.592 0.417
2024-12-01-23:46:33-root-INFO: grad norm: 2.566 2.541 0.357
2024-12-01-23:46:34-root-INFO: grad norm: 2.534 2.503 0.397
2024-12-01-23:46:34-root-INFO: Loss Change: 24.732 -> 23.166
2024-12-01-23:46:34-root-INFO: Regularization Change: 0.000 -> 2.573
2024-12-01-23:46:34-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-01-23:46:34-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-23:46:35-root-INFO: step: 39 lr_xt 0.32255964
2024-12-01-23:46:35-root-INFO: grad norm: 3.277 3.217 0.625
2024-12-01-23:46:35-root-INFO: Loss too large (23.264->23.793)! Learning rate decreased to 0.25805.
2024-12-01-23:46:36-root-INFO: grad norm: 3.029 2.982 0.530
2024-12-01-23:46:37-root-INFO: grad norm: 2.802 2.771 0.418
2024-12-01-23:46:38-root-INFO: grad norm: 2.699 2.663 0.436
2024-12-01-23:46:39-root-INFO: grad norm: 2.607 2.580 0.369
2024-12-01-23:46:40-root-INFO: grad norm: 2.550 2.518 0.402
2024-12-01-23:46:41-root-INFO: grad norm: 2.498 2.474 0.347
2024-12-01-23:46:42-root-INFO: grad norm: 2.469 2.438 0.385
2024-12-01-23:46:42-root-INFO: Loss Change: 23.264 -> 21.860
2024-12-01-23:46:42-root-INFO: Regularization Change: 0.000 -> 2.305
2024-12-01-23:46:42-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-01-23:46:42-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-23:46:42-root-INFO: step: 38 lr_xt 0.32851483
2024-12-01-23:46:43-root-INFO: grad norm: 3.347 3.276 0.687
2024-12-01-23:46:43-root-INFO: Loss too large (21.972->22.530)! Learning rate decreased to 0.26281.
2024-12-01-23:46:44-root-INFO: grad norm: 3.052 3.003 0.542
2024-12-01-23:46:45-root-INFO: grad norm: 2.788 2.755 0.428
2024-12-01-23:46:46-root-INFO: grad norm: 2.654 2.620 0.425
2024-12-01-23:46:47-root-INFO: grad norm: 2.545 2.518 0.365
2024-12-01-23:46:47-root-INFO: grad norm: 2.474 2.444 0.383
2024-12-01-23:46:48-root-INFO: grad norm: 2.409 2.385 0.336
2024-12-01-23:46:49-root-INFO: grad norm: 2.371 2.343 0.362
2024-12-01-23:46:50-root-INFO: Loss Change: 21.972 -> 20.595
2024-12-01-23:46:50-root-INFO: Regularization Change: 0.000 -> 2.147
2024-12-01-23:46:50-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-01-23:46:50-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-23:46:50-root-INFO: step: 37 lr_xt 0.33452649
2024-12-01-23:46:50-root-INFO: grad norm: 3.387 3.323 0.656
2024-12-01-23:46:51-root-INFO: Loss too large (20.806->21.412)! Learning rate decreased to 0.26762.
2024-12-01-23:46:51-root-INFO: grad norm: 3.028 2.981 0.531
2024-12-01-23:46:52-root-INFO: grad norm: 2.649 2.619 0.394
2024-12-01-23:46:53-root-INFO: grad norm: 2.491 2.460 0.395
2024-12-01-23:46:54-root-INFO: grad norm: 2.401 2.377 0.337
2024-12-01-23:46:55-root-INFO: grad norm: 2.328 2.301 0.359
2024-12-01-23:46:56-root-INFO: grad norm: 2.267 2.245 0.311
2024-12-01-23:46:57-root-INFO: grad norm: 2.228 2.202 0.339
2024-12-01-23:46:57-root-INFO: Loss Change: 20.806 -> 19.452
2024-12-01-23:46:57-root-INFO: Regularization Change: 0.000 -> 2.021
2024-12-01-23:46:57-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-01-23:46:57-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-23:46:58-root-INFO: step: 36 lr_xt 0.34059371
2024-12-01-23:46:58-root-INFO: grad norm: 2.917 2.861 0.565
2024-12-01-23:46:58-root-INFO: Loss too large (19.533->19.989)! Learning rate decreased to 0.27247.
2024-12-01-23:46:59-root-INFO: grad norm: 2.696 2.658 0.453
2024-12-01-23:47:00-root-INFO: grad norm: 2.486 2.459 0.367
2024-12-01-23:47:01-root-INFO: grad norm: 2.396 2.367 0.370
2024-12-01-23:47:02-root-INFO: grad norm: 2.336 2.314 0.324
2024-12-01-23:47:02-root-INFO: grad norm: 2.285 2.259 0.345
2024-12-01-23:47:03-root-INFO: grad norm: 2.235 2.214 0.303
2024-12-01-23:47:04-root-INFO: grad norm: 2.205 2.180 0.330
2024-12-01-23:47:05-root-INFO: Loss Change: 19.533 -> 18.416
2024-12-01-23:47:05-root-INFO: Regularization Change: 0.000 -> 1.907
2024-12-01-23:47:05-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-01-23:47:05-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-23:47:05-root-INFO: step: 35 lr_xt 0.34671555
2024-12-01-23:47:05-root-INFO: grad norm: 3.048 2.991 0.589
2024-12-01-23:47:06-root-INFO: Loss too large (18.597->19.102)! Learning rate decreased to 0.27737.
2024-12-01-23:47:07-root-INFO: grad norm: 2.769 2.727 0.481
2024-12-01-23:47:07-root-INFO: grad norm: 2.501 2.474 0.365
2024-12-01-23:47:08-root-INFO: grad norm: 2.359 2.330 0.370
2024-12-01-23:47:09-root-INFO: grad norm: 2.306 2.284 0.317
2024-12-01-23:47:10-root-INFO: grad norm: 2.229 2.204 0.338
2024-12-01-23:47:11-root-INFO: grad norm: 2.141 2.121 0.289
2024-12-01-23:47:12-root-INFO: grad norm: 2.090 2.067 0.313
2024-12-01-23:47:12-root-INFO: Loss Change: 18.597 -> 17.433
2024-12-01-23:47:12-root-INFO: Regularization Change: 0.000 -> 1.848
2024-12-01-23:47:13-root-INFO: Undo step: 35
2024-12-01-23:47:13-root-INFO: Undo step: 36
2024-12-01-23:47:13-root-INFO: Undo step: 37
2024-12-01-23:47:13-root-INFO: Undo step: 38
2024-12-01-23:47:13-root-INFO: Undo step: 39
2024-12-01-23:47:13-root-INFO: step: 40 lr_xt 0.31666177
2024-12-01-23:47:13-root-INFO: grad norm: 16.802 16.712 1.736
2024-12-01-23:47:14-root-INFO: grad norm: 7.549 7.491 0.930
2024-12-01-23:47:15-root-INFO: grad norm: 5.485 5.428 0.784
2024-12-01-23:47:16-root-INFO: grad norm: 4.695 4.647 0.669
2024-12-01-23:47:17-root-INFO: grad norm: 4.385 4.344 0.596
2024-12-01-23:47:18-root-INFO: grad norm: 4.147 4.109 0.560
2024-12-01-23:47:19-root-INFO: grad norm: 4.044 4.014 0.488
2024-12-01-23:47:19-root-INFO: grad norm: 3.881 3.846 0.517
2024-12-01-23:47:20-root-INFO: Loss Change: 109.374 -> 30.274
2024-12-01-23:47:20-root-INFO: Regularization Change: 0.000 -> 111.075
2024-12-01-23:47:20-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-01-23:47:20-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-23:47:20-root-INFO: step: 39 lr_xt 0.32255964
2024-12-01-23:47:21-root-INFO: grad norm: 4.198 4.159 0.570
2024-12-01-23:47:22-root-INFO: grad norm: 4.133 4.078 0.673
2024-12-01-23:47:22-root-INFO: grad norm: 4.056 4.013 0.588
2024-12-01-23:47:23-root-INFO: grad norm: 4.087 4.022 0.729
2024-12-01-23:47:24-root-INFO: grad norm: 4.279 4.220 0.711
2024-12-01-23:47:25-root-INFO: Loss too large (27.121->27.179)! Learning rate decreased to 0.25805.
2024-12-01-23:47:25-root-INFO: grad norm: 3.309 3.253 0.606
2024-12-01-23:47:26-root-INFO: grad norm: 2.515 2.480 0.413
2024-12-01-23:47:27-root-INFO: grad norm: 2.223 2.190 0.379
2024-12-01-23:47:28-root-INFO: Loss Change: 30.173 -> 24.497
2024-12-01-23:47:28-root-INFO: Regularization Change: 0.000 -> 11.436
2024-12-01-23:47:28-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-01-23:47:28-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-23:47:28-root-INFO: step: 38 lr_xt 0.32851483
2024-12-01-23:47:28-root-INFO: grad norm: 2.976 2.905 0.648
2024-12-01-23:47:29-root-INFO: grad norm: 3.538 3.466 0.710
2024-12-01-23:47:30-root-INFO: grad norm: 4.645 4.571 0.824
2024-12-01-23:47:30-root-INFO: Loss too large (24.428->25.219)! Learning rate decreased to 0.26281.
2024-12-01-23:47:31-root-INFO: grad norm: 3.760 3.693 0.709
2024-12-01-23:47:32-root-INFO: grad norm: 2.784 2.743 0.475
2024-12-01-23:47:33-root-INFO: grad norm: 2.392 2.356 0.411
2024-12-01-23:47:34-root-INFO: grad norm: 2.128 2.101 0.339
2024-12-01-23:47:35-root-INFO: grad norm: 1.974 1.948 0.322
2024-12-01-23:47:35-root-INFO: Loss Change: 24.513 -> 21.740
2024-12-01-23:47:35-root-INFO: Regularization Change: 0.000 -> 5.514
2024-12-01-23:47:35-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-01-23:47:35-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-23:47:36-root-INFO: step: 37 lr_xt 0.33452649
2024-12-01-23:47:36-root-INFO: grad norm: 2.967 2.904 0.609
2024-12-01-23:47:36-root-INFO: Loss too large (21.856->22.063)! Learning rate decreased to 0.26762.
2024-12-01-23:47:37-root-INFO: grad norm: 2.609 2.565 0.477
2024-12-01-23:47:38-root-INFO: grad norm: 2.304 2.273 0.372
2024-12-01-23:47:39-root-INFO: grad norm: 2.166 2.136 0.359
2024-12-01-23:47:40-root-INFO: grad norm: 2.068 2.045 0.313
2024-12-01-23:47:41-root-INFO: grad norm: 1.998 1.972 0.323
2024-12-01-23:47:41-root-INFO: grad norm: 1.944 1.922 0.289
2024-12-01-23:47:42-root-INFO: grad norm: 1.908 1.884 0.306
2024-12-01-23:47:43-root-INFO: Loss Change: 21.856 -> 19.915
2024-12-01-23:47:43-root-INFO: Regularization Change: 0.000 -> 3.463
2024-12-01-23:47:43-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-01-23:47:43-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-23:47:43-root-INFO: step: 36 lr_xt 0.34059371
2024-12-01-23:47:43-root-INFO: grad norm: 2.637 2.583 0.532
2024-12-01-23:47:44-root-INFO: Loss too large (19.949->20.198)! Learning rate decreased to 0.27247.
2024-12-01-23:47:45-root-INFO: grad norm: 2.422 2.385 0.420
2024-12-01-23:47:46-root-INFO: grad norm: 2.226 2.198 0.355
2024-12-01-23:47:46-root-INFO: grad norm: 2.155 2.127 0.348
2024-12-01-23:47:47-root-INFO: grad norm: 2.107 2.083 0.315
2024-12-01-23:47:48-root-INFO: grad norm: 2.066 2.040 0.328
2024-12-01-23:47:49-root-INFO: grad norm: 2.034 2.012 0.299
2024-12-01-23:47:50-root-INFO: grad norm: 2.010 1.985 0.318
2024-12-01-23:47:50-root-INFO: Loss Change: 19.949 -> 18.506
2024-12-01-23:47:50-root-INFO: Regularization Change: 0.000 -> 2.749
2024-12-01-23:47:50-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-01-23:47:50-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-23:47:50-root-INFO: step: 35 lr_xt 0.34671555
2024-12-01-23:47:51-root-INFO: grad norm: 2.890 2.833 0.573
2024-12-01-23:47:51-root-INFO: Loss too large (18.652->19.049)! Learning rate decreased to 0.27737.
2024-12-01-23:47:52-root-INFO: grad norm: 2.631 2.588 0.469
2024-12-01-23:47:53-root-INFO: grad norm: 2.383 2.354 0.374
2024-12-01-23:47:54-root-INFO: grad norm: 2.265 2.235 0.369
2024-12-01-23:47:55-root-INFO: grad norm: 2.182 2.158 0.324
2024-12-01-23:47:55-root-INFO: grad norm: 2.113 2.086 0.335
2024-12-01-23:47:56-root-INFO: grad norm: 2.051 2.029 0.299
2024-12-01-23:47:57-root-INFO: grad norm: 2.009 1.984 0.316
2024-12-01-23:47:58-root-INFO: Loss Change: 18.652 -> 17.300
2024-12-01-23:47:58-root-INFO: Regularization Change: 0.000 -> 2.363
2024-12-01-23:47:58-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-01-23:47:58-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-23:47:58-root-INFO: step: 34 lr_xt 0.35289102
2024-12-01-23:47:58-root-INFO: grad norm: 2.780 2.730 0.523
2024-12-01-23:47:59-root-INFO: Loss too large (17.389->17.786)! Learning rate decreased to 0.28231.
2024-12-01-23:47:59-root-INFO: grad norm: 2.491 2.454 0.430
2024-12-01-23:48:00-root-INFO: grad norm: 2.167 2.141 0.332
2024-12-01-23:48:01-root-INFO: grad norm: 2.054 2.028 0.326
2024-12-01-23:48:02-root-INFO: grad norm: 2.007 1.986 0.290
2024-12-01-23:48:03-root-INFO: grad norm: 1.954 1.930 0.305
2024-12-01-23:48:04-root-INFO: grad norm: 1.922 1.903 0.273
2024-12-01-23:48:05-root-INFO: grad norm: 1.888 1.864 0.294
2024-12-01-23:48:05-root-INFO: Loss Change: 17.389 -> 16.183
2024-12-01-23:48:05-root-INFO: Regularization Change: 0.000 -> 2.100
2024-12-01-23:48:05-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-01-23:48:05-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-23:48:06-root-INFO: step: 33 lr_xt 0.35911909
2024-12-01-23:48:06-root-INFO: grad norm: 2.791 2.736 0.551
2024-12-01-23:48:06-root-INFO: Loss too large (16.194->16.614)! Learning rate decreased to 0.28730.
2024-12-01-23:48:07-root-INFO: grad norm: 2.464 2.428 0.418
2024-12-01-23:48:08-root-INFO: grad norm: 2.082 2.056 0.324
2024-12-01-23:48:09-root-INFO: grad norm: 1.957 1.933 0.303
2024-12-01-23:48:10-root-INFO: grad norm: 1.915 1.895 0.276
2024-12-01-23:48:11-root-INFO: grad norm: 1.866 1.844 0.283
2024-12-01-23:48:11-root-INFO: grad norm: 1.841 1.823 0.259
2024-12-01-23:48:12-root-INFO: grad norm: 1.807 1.786 0.274
2024-12-01-23:48:13-root-INFO: Loss Change: 16.194 -> 15.050
2024-12-01-23:48:13-root-INFO: Regularization Change: 0.000 -> 1.942
2024-12-01-23:48:13-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-01-23:48:13-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-23:48:13-root-INFO: step: 32 lr_xt 0.36539868
2024-12-01-23:48:13-root-INFO: grad norm: 2.866 2.812 0.555
2024-12-01-23:48:14-root-INFO: Loss too large (15.282->15.741)! Learning rate decreased to 0.29232.
2024-12-01-23:48:15-root-INFO: grad norm: 2.492 2.455 0.427
2024-12-01-23:48:16-root-INFO: grad norm: 2.061 2.037 0.315
2024-12-01-23:48:16-root-INFO: grad norm: 1.906 1.883 0.298
2024-12-01-23:48:17-root-INFO: grad norm: 1.867 1.848 0.266
2024-12-01-23:48:18-root-INFO: grad norm: 1.815 1.794 0.277
2024-12-01-23:48:19-root-INFO: grad norm: 1.783 1.766 0.249
2024-12-01-23:48:20-root-INFO: grad norm: 1.747 1.727 0.265
2024-12-01-23:48:20-root-INFO: Loss Change: 15.282 -> 14.154
2024-12-01-23:48:20-root-INFO: Regularization Change: 0.000 -> 1.831
2024-12-01-23:48:20-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-01-23:48:20-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-23:48:21-root-INFO: step: 31 lr_xt 0.37172867
2024-12-01-23:48:21-root-INFO: grad norm: 2.429 2.385 0.461
2024-12-01-23:48:21-root-INFO: Loss too large (14.206->14.531)! Learning rate decreased to 0.29738.
2024-12-01-23:48:22-root-INFO: grad norm: 2.190 2.160 0.363
2024-12-01-23:48:23-root-INFO: grad norm: 1.949 1.927 0.291
2024-12-01-23:48:24-root-INFO: grad norm: 1.849 1.827 0.283
2024-12-01-23:48:25-root-INFO: grad norm: 1.827 1.809 0.254
2024-12-01-23:48:26-root-INFO: grad norm: 1.778 1.758 0.265
2024-12-01-23:48:26-root-INFO: grad norm: 1.708 1.692 0.234
2024-12-01-23:48:27-root-INFO: grad norm: 1.673 1.654 0.248
2024-12-01-23:48:28-root-INFO: Loss Change: 14.206 -> 13.268
2024-12-01-23:48:28-root-INFO: Regularization Change: 0.000 -> 1.711
2024-12-01-23:48:28-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-01-23:48:28-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-23:48:28-root-INFO: step: 30 lr_xt 0.37810791
2024-12-01-23:48:28-root-INFO: grad norm: 2.668 2.623 0.489
2024-12-01-23:48:29-root-INFO: Loss too large (13.402->13.847)! Learning rate decreased to 0.30249.
2024-12-01-23:48:30-root-INFO: grad norm: 2.303 2.272 0.378
2024-12-01-23:48:30-root-INFO: grad norm: 1.864 1.844 0.272
2024-12-01-23:48:31-root-INFO: grad norm: 1.703 1.683 0.258
2024-12-01-23:48:32-root-INFO: grad norm: 1.654 1.639 0.226
2024-12-01-23:48:33-root-INFO: grad norm: 1.612 1.594 0.239
2024-12-01-23:48:34-root-INFO: grad norm: 1.618 1.603 0.215
2024-12-01-23:48:35-root-INFO: grad norm: 1.595 1.578 0.234
2024-12-01-23:48:35-root-INFO: Loss Change: 13.402 -> 12.427
2024-12-01-23:48:35-root-INFO: Regularization Change: 0.000 -> 1.650
2024-12-01-23:48:35-root-INFO: Undo step: 30
2024-12-01-23:48:35-root-INFO: Undo step: 31
2024-12-01-23:48:35-root-INFO: Undo step: 32
2024-12-01-23:48:35-root-INFO: Undo step: 33
2024-12-01-23:48:35-root-INFO: Undo step: 34
2024-12-01-23:48:36-root-INFO: step: 35 lr_xt 0.34671555
2024-12-01-23:48:36-root-INFO: grad norm: 15.141 15.067 1.492
2024-12-01-23:48:37-root-INFO: grad norm: 8.042 7.984 0.962
2024-12-01-23:48:38-root-INFO: grad norm: 5.562 5.519 0.689
2024-12-01-23:48:39-root-INFO: grad norm: 4.831 4.800 0.550
2024-12-01-23:48:39-root-INFO: grad norm: 4.388 4.362 0.481
2024-12-01-23:48:40-root-INFO: grad norm: 4.057 4.034 0.437
2024-12-01-23:48:41-root-INFO: grad norm: 3.823 3.800 0.417
2024-12-01-23:48:42-root-INFO: grad norm: 3.704 3.682 0.404
2024-12-01-23:48:43-root-INFO: Loss Change: 96.238 -> 24.508
2024-12-01-23:48:43-root-INFO: Regularization Change: 0.000 -> 112.996
2024-12-01-23:48:43-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-01-23:48:43-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-23:48:43-root-INFO: step: 34 lr_xt 0.35289102
2024-12-01-23:48:43-root-INFO: grad norm: 3.841 3.813 0.469
2024-12-01-23:48:44-root-INFO: grad norm: 3.687 3.655 0.482
2024-12-01-23:48:45-root-INFO: grad norm: 3.537 3.509 0.448
2024-12-01-23:48:46-root-INFO: grad norm: 3.470 3.437 0.473
2024-12-01-23:48:47-root-INFO: grad norm: 3.435 3.405 0.454
2024-12-01-23:48:48-root-INFO: grad norm: 3.410 3.375 0.490
2024-12-01-23:48:48-root-INFO: grad norm: 3.416 3.382 0.479
2024-12-01-23:48:49-root-INFO: grad norm: 3.414 3.374 0.523
2024-12-01-23:48:50-root-INFO: Loss Change: 24.347 -> 19.385
2024-12-01-23:48:50-root-INFO: Regularization Change: 0.000 -> 13.704
2024-12-01-23:48:50-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-01-23:48:50-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-23:48:50-root-INFO: step: 33 lr_xt 0.35911909
2024-12-01-23:48:50-root-INFO: grad norm: 4.166 4.098 0.753
2024-12-01-23:48:51-root-INFO: grad norm: 4.003 3.935 0.736
2024-12-01-23:48:52-root-INFO: grad norm: 3.788 3.728 0.674
2024-12-01-23:48:53-root-INFO: grad norm: 3.828 3.759 0.721
2024-12-01-23:48:54-root-INFO: grad norm: 4.132 4.069 0.724
2024-12-01-23:48:54-root-INFO: Loss too large (18.050->18.387)! Learning rate decreased to 0.28730.
2024-12-01-23:48:55-root-INFO: grad norm: 3.040 2.992 0.537
2024-12-01-23:48:56-root-INFO: grad norm: 2.110 2.082 0.343
2024-12-01-23:48:57-root-INFO: grad norm: 1.754 1.732 0.279
2024-12-01-23:48:58-root-INFO: Loss Change: 19.443 -> 16.101
2024-12-01-23:48:58-root-INFO: Regularization Change: 0.000 -> 5.975
2024-12-01-23:48:58-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-01-23:48:58-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-23:48:58-root-INFO: step: 32 lr_xt 0.36539868
2024-12-01-23:48:58-root-INFO: grad norm: 2.529 2.478 0.506
2024-12-01-23:48:58-root-INFO: Loss too large (16.236->16.262)! Learning rate decreased to 0.29232.
2024-12-01-23:48:59-root-INFO: grad norm: 2.101 2.071 0.352
2024-12-01-23:49:00-root-INFO: grad norm: 1.830 1.809 0.274
2024-12-01-23:49:01-root-INFO: grad norm: 1.687 1.668 0.252
2024-12-01-23:49:02-root-INFO: grad norm: 1.596 1.581 0.223
2024-12-01-23:49:03-root-INFO: grad norm: 1.540 1.524 0.222
2024-12-01-23:49:04-root-INFO: grad norm: 1.515 1.501 0.206
2024-12-01-23:49:05-root-INFO: grad norm: 1.495 1.480 0.213
2024-12-01-23:49:05-root-INFO: Loss Change: 16.236 -> 14.549
2024-12-01-23:49:05-root-INFO: Regularization Change: 0.000 -> 3.230
2024-12-01-23:49:05-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-01-23:49:05-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-23:49:05-root-INFO: step: 31 lr_xt 0.37172867
2024-12-01-23:49:06-root-INFO: grad norm: 2.167 2.130 0.401
2024-12-01-23:49:06-root-INFO: Loss too large (14.533->14.661)! Learning rate decreased to 0.29738.
2024-12-01-23:49:07-root-INFO: grad norm: 1.945 1.922 0.301
2024-12-01-23:49:08-root-INFO: grad norm: 1.753 1.735 0.249
2024-12-01-23:49:08-root-INFO: grad norm: 1.673 1.655 0.241
2024-12-01-23:49:09-root-INFO: grad norm: 1.664 1.649 0.224
2024-12-01-23:49:10-root-INFO: grad norm: 1.636 1.620 0.228
2024-12-01-23:49:11-root-INFO: grad norm: 1.577 1.563 0.209
2024-12-01-23:49:12-root-INFO: grad norm: 1.554 1.539 0.217
2024-12-01-23:49:13-root-INFO: Loss Change: 14.533 -> 13.322
2024-12-01-23:49:13-root-INFO: Regularization Change: 0.000 -> 2.519
2024-12-01-23:49:13-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-01-23:49:13-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-23:49:13-root-INFO: step: 30 lr_xt 0.37810791
2024-12-01-23:49:13-root-INFO: grad norm: 2.534 2.494 0.444
2024-12-01-23:49:13-root-INFO: Loss too large (13.393->13.724)! Learning rate decreased to 0.30249.
2024-12-01-23:49:14-root-INFO: grad norm: 2.198 2.172 0.332
2024-12-01-23:49:15-root-INFO: grad norm: 1.822 1.805 0.250
2024-12-01-23:49:16-root-INFO: grad norm: 1.663 1.646 0.240
2024-12-01-23:49:17-root-INFO: grad norm: 1.583 1.569 0.211
2024-12-01-23:49:18-root-INFO: grad norm: 1.549 1.533 0.217
2024-12-01-23:49:19-root-INFO: grad norm: 1.600 1.587 0.206
2024-12-01-23:49:19-root-INFO: grad norm: 1.582 1.567 0.215
2024-12-01-23:49:20-root-INFO: Loss Change: 13.393 -> 12.264
2024-12-01-23:49:20-root-INFO: Regularization Change: 0.000 -> 2.138
2024-12-01-23:49:20-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-01-23:49:20-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-23:49:20-root-INFO: step: 29 lr_xt 0.38453518
2024-12-01-23:49:21-root-INFO: grad norm: 2.361 2.320 0.437
2024-12-01-23:49:21-root-INFO: Loss too large (12.354->12.592)! Learning rate decreased to 0.30763.
2024-12-01-23:49:22-root-INFO: grad norm: 2.040 2.016 0.311
2024-12-01-23:49:23-root-INFO: grad norm: 1.803 1.786 0.252
2024-12-01-23:49:24-root-INFO: grad norm: 1.699 1.683 0.235
2024-12-01-23:49:24-root-INFO: grad norm: 1.577 1.564 0.206
2024-12-01-23:49:25-root-INFO: grad norm: 1.526 1.512 0.206
2024-12-01-23:49:26-root-INFO: grad norm: 1.674 1.662 0.204
2024-12-01-23:49:26-root-INFO: Loss too large (11.519->11.521)! Learning rate decreased to 0.24610.
2024-12-01-23:49:27-root-INFO: grad norm: 1.168 1.159 0.148
2024-12-01-23:49:28-root-INFO: Loss Change: 12.354 -> 11.265
2024-12-01-23:49:28-root-INFO: Regularization Change: 0.000 -> 1.766
2024-12-01-23:49:28-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-01-23:49:28-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-23:49:28-root-INFO: step: 28 lr_xt 0.39100924
2024-12-01-23:49:28-root-INFO: grad norm: 1.655 1.619 0.342
2024-12-01-23:49:29-root-INFO: grad norm: 1.930 1.902 0.328
2024-12-01-23:49:30-root-INFO: Loss too large (11.180->11.387)! Learning rate decreased to 0.31281.
2024-12-01-23:49:30-root-INFO: grad norm: 2.475 2.459 0.288
2024-12-01-23:49:31-root-INFO: Loss too large (11.037->11.109)! Learning rate decreased to 0.25025.
2024-12-01-23:49:32-root-INFO: grad norm: 1.408 1.397 0.176
2024-12-01-23:49:32-root-INFO: grad norm: 0.831 0.825 0.093
2024-12-01-23:49:33-root-INFO: grad norm: 0.664 0.659 0.081
2024-12-01-23:49:34-root-INFO: grad norm: 0.588 0.584 0.066
2024-12-01-23:49:35-root-INFO: grad norm: 0.556 0.552 0.066
2024-12-01-23:49:36-root-INFO: Loss Change: 11.212 -> 10.403
2024-12-01-23:49:36-root-INFO: Regularization Change: 0.000 -> 1.406
2024-12-01-23:49:36-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-01-23:49:36-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-23:49:36-root-INFO: step: 27 lr_xt 0.39752879
2024-12-01-23:49:36-root-INFO: grad norm: 1.200 1.171 0.261
2024-12-01-23:49:37-root-INFO: grad norm: 1.304 1.283 0.237
2024-12-01-23:49:37-root-INFO: Loss too large (10.282->10.290)! Learning rate decreased to 0.31802.
2024-12-01-23:49:38-root-INFO: grad norm: 1.328 1.316 0.184
2024-12-01-23:49:39-root-INFO: grad norm: 1.350 1.338 0.181
2024-12-01-23:49:40-root-INFO: grad norm: 1.386 1.375 0.168
2024-12-01-23:49:41-root-INFO: grad norm: 1.404 1.392 0.177
2024-12-01-23:49:42-root-INFO: grad norm: 1.539 1.529 0.175
2024-12-01-23:49:42-root-INFO: Loss too large (9.897->9.920)! Learning rate decreased to 0.25442.
2024-12-01-23:49:43-root-INFO: grad norm: 1.119 1.110 0.134
2024-12-01-23:49:43-root-INFO: Loss Change: 10.386 -> 9.684
2024-12-01-23:49:43-root-INFO: Regularization Change: 0.000 -> 1.550
2024-12-01-23:49:43-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-01-23:49:43-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-23:49:44-root-INFO: step: 26 lr_xt 0.40409250
2024-12-01-23:49:44-root-INFO: grad norm: 1.564 1.532 0.316
2024-12-01-23:49:45-root-INFO: grad norm: 1.864 1.838 0.308
2024-12-01-23:49:45-root-INFO: Loss too large (9.716->9.961)! Learning rate decreased to 0.32327.
2024-12-01-23:49:46-root-INFO: grad norm: 2.484 2.469 0.272
2024-12-01-23:49:46-root-INFO: Loss too large (9.601->9.700)! Learning rate decreased to 0.25862.
2024-12-01-23:49:47-root-INFO: grad norm: 1.395 1.384 0.169
2024-12-01-23:49:48-root-INFO: grad norm: 0.806 0.801 0.085
2024-12-01-23:49:49-root-INFO: grad norm: 0.637 0.633 0.076
2024-12-01-23:49:49-root-INFO: grad norm: 0.554 0.551 0.060
2024-12-01-23:49:50-root-INFO: grad norm: 0.519 0.516 0.060
2024-12-01-23:49:51-root-INFO: Loss Change: 9.719 -> 9.028
2024-12-01-23:49:51-root-INFO: Regularization Change: 0.000 -> 1.237
2024-12-01-23:49:51-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-01-23:49:51-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-23:49:51-root-INFO: step: 25 lr_xt 0.41069899
2024-12-01-23:49:52-root-INFO: grad norm: 1.253 1.223 0.274
2024-12-01-23:49:52-root-INFO: grad norm: 1.334 1.314 0.234
2024-12-01-23:49:53-root-INFO: Loss too large (8.855->8.878)! Learning rate decreased to 0.32856.
2024-12-01-23:49:54-root-INFO: grad norm: 1.333 1.321 0.179
2024-12-01-23:49:54-root-INFO: grad norm: 1.332 1.322 0.168
2024-12-01-23:49:55-root-INFO: grad norm: 1.331 1.322 0.153
2024-12-01-23:49:56-root-INFO: grad norm: 1.342 1.333 0.159
2024-12-01-23:49:57-root-INFO: grad norm: 1.550 1.541 0.162
2024-12-01-23:49:57-root-INFO: Loss too large (8.499->8.543)! Learning rate decreased to 0.26285.
2024-12-01-23:49:58-root-INFO: grad norm: 1.094 1.087 0.123
2024-12-01-23:49:59-root-INFO: Loss Change: 8.956 -> 8.305
2024-12-01-23:49:59-root-INFO: Regularization Change: 0.000 -> 1.416
2024-12-01-23:49:59-root-INFO: Undo step: 25
2024-12-01-23:49:59-root-INFO: Undo step: 26
2024-12-01-23:49:59-root-INFO: Undo step: 27
2024-12-01-23:49:59-root-INFO: Undo step: 28
2024-12-01-23:49:59-root-INFO: Undo step: 29
2024-12-01-23:49:59-root-INFO: step: 30 lr_xt 0.37810791
2024-12-01-23:49:59-root-INFO: grad norm: 13.943 13.894 1.170
2024-12-01-23:50:00-root-INFO: grad norm: 6.668 6.628 0.729
2024-12-01-23:50:01-root-INFO: grad norm: 4.477 4.442 0.552
2024-12-01-23:50:02-root-INFO: grad norm: 3.689 3.660 0.460
2024-12-01-23:50:03-root-INFO: grad norm: 3.462 3.436 0.425
2024-12-01-23:50:04-root-INFO: grad norm: 3.695 3.673 0.407
2024-12-01-23:50:05-root-INFO: grad norm: 4.067 4.042 0.445
2024-12-01-23:50:05-root-INFO: grad norm: 4.087 4.066 0.413
2024-12-01-23:50:06-root-INFO: Loss Change: 86.015 -> 19.827
2024-12-01-23:50:06-root-INFO: Regularization Change: 0.000 -> 114.815
2024-12-01-23:50:06-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-01-23:50:06-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-23:50:06-root-INFO: step: 29 lr_xt 0.38453518
2024-12-01-23:50:07-root-INFO: grad norm: 3.944 3.920 0.432
2024-12-01-23:50:08-root-INFO: grad norm: 3.920 3.902 0.385
2024-12-01-23:50:08-root-INFO: grad norm: 3.737 3.717 0.387
2024-12-01-23:50:09-root-INFO: grad norm: 3.709 3.692 0.355
2024-12-01-23:50:10-root-INFO: grad norm: 3.638 3.619 0.373
2024-12-01-23:50:11-root-INFO: grad norm: 3.661 3.643 0.366
2024-12-01-23:50:12-root-INFO: grad norm: 3.652 3.633 0.381
2024-12-01-23:50:13-root-INFO: grad norm: 3.626 3.606 0.374
2024-12-01-23:50:13-root-INFO: Loss Change: 19.497 -> 15.065
2024-12-01-23:50:13-root-INFO: Regularization Change: 0.000 -> 14.008
2024-12-01-23:50:13-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-01-23:50:13-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-23:50:14-root-INFO: step: 28 lr_xt 0.39100924
2024-12-01-23:50:14-root-INFO: grad norm: 3.618 3.589 0.457
2024-12-01-23:50:15-root-INFO: grad norm: 3.575 3.548 0.433
2024-12-01-23:50:16-root-INFO: grad norm: 3.455 3.430 0.421
2024-12-01-23:50:17-root-INFO: grad norm: 3.380 3.356 0.398
2024-12-01-23:50:17-root-INFO: grad norm: 3.374 3.349 0.409
2024-12-01-23:50:18-root-INFO: grad norm: 3.444 3.421 0.396
2024-12-01-23:50:19-root-INFO: grad norm: 3.378 3.352 0.419
2024-12-01-23:50:20-root-INFO: grad norm: 3.259 3.237 0.375
2024-12-01-23:50:21-root-INFO: Loss Change: 14.722 -> 12.493
2024-12-01-23:50:21-root-INFO: Regularization Change: 0.000 -> 6.566
2024-12-01-23:50:21-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-01-23:50:21-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-23:50:21-root-INFO: step: 27 lr_xt 0.39752879
2024-12-01-23:50:21-root-INFO: grad norm: 3.418 3.385 0.477
2024-12-01-23:50:22-root-INFO: grad norm: 3.453 3.423 0.455
2024-12-01-23:50:23-root-INFO: grad norm: 3.328 3.300 0.429
2024-12-01-23:50:24-root-INFO: grad norm: 3.142 3.115 0.409
2024-12-01-23:50:25-root-INFO: grad norm: 3.063 3.040 0.381
2024-12-01-23:50:25-root-INFO: grad norm: 3.124 3.099 0.390
2024-12-01-23:50:26-root-INFO: grad norm: 3.020 2.996 0.381
2024-12-01-23:50:27-root-INFO: grad norm: 3.040 3.018 0.369
2024-12-01-23:50:28-root-INFO: Loss Change: 12.416 -> 10.906
2024-12-01-23:50:28-root-INFO: Regularization Change: 0.000 -> 4.194
2024-12-01-23:50:28-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-01-23:50:28-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-23:50:28-root-INFO: step: 26 lr_xt 0.40409250
2024-12-01-23:50:28-root-INFO: grad norm: 3.294 3.251 0.528
2024-12-01-23:50:29-root-INFO: grad norm: 3.200 3.162 0.490
2024-12-01-23:50:30-root-INFO: grad norm: 2.972 2.939 0.437
2024-12-01-23:50:31-root-INFO: grad norm: 2.777 2.745 0.420
2024-12-01-23:50:32-root-INFO: grad norm: 2.738 2.711 0.386
2024-12-01-23:50:32-root-INFO: grad norm: 2.943 2.915 0.405
2024-12-01-23:50:33-root-INFO: Loss too large (9.953->10.015)! Learning rate decreased to 0.32327.
2024-12-01-23:50:34-root-INFO: grad norm: 1.921 1.906 0.242
2024-12-01-23:50:34-root-INFO: grad norm: 1.227 1.219 0.147
2024-12-01-23:50:35-root-INFO: Loss Change: 10.927 -> 8.974
2024-12-01-23:50:35-root-INFO: Regularization Change: 0.000 -> 2.973
2024-12-01-23:50:35-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-01-23:50:35-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-23:50:35-root-INFO: step: 25 lr_xt 0.41069899
2024-12-01-23:50:36-root-INFO: grad norm: 1.509 1.472 0.331
2024-12-01-23:50:36-root-INFO: grad norm: 1.530 1.504 0.283
2024-12-01-23:50:37-root-INFO: grad norm: 1.729 1.707 0.272
2024-12-01-23:50:37-root-INFO: Loss too large (8.696->8.714)! Learning rate decreased to 0.32856.
2024-12-01-23:50:38-root-INFO: grad norm: 1.387 1.374 0.191
2024-12-01-23:50:39-root-INFO: grad norm: 1.136 1.128 0.133
2024-12-01-23:50:40-root-INFO: grad norm: 0.991 0.985 0.110
2024-12-01-23:50:41-root-INFO: grad norm: 0.959 0.954 0.100
2024-12-01-23:50:42-root-INFO: grad norm: 1.239 1.234 0.105
2024-12-01-23:50:42-root-INFO: Loss too large (8.126->8.130)! Learning rate decreased to 0.26285.
2024-12-01-23:50:43-root-INFO: Loss Change: 8.874 -> 8.084
2024-12-01-23:50:43-root-INFO: Regularization Change: 0.000 -> 1.860
2024-12-01-23:50:43-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-01-23:50:43-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-23:50:43-root-INFO: step: 24 lr_xt 0.41734684
2024-12-01-23:50:43-root-INFO: grad norm: 1.241 1.213 0.264
2024-12-01-23:50:44-root-INFO: grad norm: 1.165 1.145 0.218
2024-12-01-23:50:45-root-INFO: grad norm: 1.344 1.328 0.208
2024-12-01-23:50:45-root-INFO: Loss too large (7.837->7.873)! Learning rate decreased to 0.33388.
2024-12-01-23:50:46-root-INFO: grad norm: 1.356 1.346 0.160
2024-12-01-23:50:47-root-INFO: grad norm: 1.186 1.179 0.126
2024-12-01-23:50:48-root-INFO: grad norm: 1.155 1.149 0.118
2024-12-01-23:50:49-root-INFO: grad norm: 1.087 1.080 0.118
2024-12-01-23:50:50-root-INFO: grad norm: 1.373 1.368 0.118
2024-12-01-23:50:50-root-INFO: Loss too large (7.453->7.508)! Learning rate decreased to 0.26710.
2024-12-01-23:50:51-root-INFO: Loss Change: 8.061 -> 7.436
2024-12-01-23:50:51-root-INFO: Regularization Change: 0.000 -> 1.621
2024-12-01-23:50:51-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-01-23:50:51-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-23:50:51-root-INFO: step: 23 lr_xt 0.42403458
2024-12-01-23:50:51-root-INFO: grad norm: 1.388 1.360 0.276
2024-12-01-23:50:52-root-INFO: grad norm: 1.600 1.581 0.242
2024-12-01-23:50:52-root-INFO: Loss too large (7.230->7.322)! Learning rate decreased to 0.33923.
2024-12-01-23:50:53-root-INFO: grad norm: 1.330 1.320 0.163
2024-12-01-23:50:54-root-INFO: grad norm: 1.610 1.605 0.124
2024-12-01-23:50:54-root-INFO: Loss too large (7.020->7.061)! Learning rate decreased to 0.27138.
2024-12-01-23:50:55-root-INFO: grad norm: 0.940 0.935 0.091
2024-12-01-23:50:56-root-INFO: grad norm: 0.648 0.645 0.059
2024-12-01-23:50:57-root-INFO: grad norm: 0.575 0.572 0.058
2024-12-01-23:50:58-root-INFO: grad norm: 0.511 0.508 0.052
2024-12-01-23:50:58-root-INFO: Loss Change: 7.350 -> 6.688
2024-12-01-23:50:58-root-INFO: Regularization Change: 0.000 -> 1.178
2024-12-01-23:50:58-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-01-23:50:58-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-23:50:58-root-INFO: step: 22 lr_xt 0.43076069
2024-12-01-23:50:59-root-INFO: grad norm: 1.162 1.131 0.266
2024-12-01-23:51:00-root-INFO: grad norm: 1.214 1.192 0.232
2024-12-01-23:51:00-root-INFO: Loss too large (6.629->6.659)! Learning rate decreased to 0.34461.
2024-12-01-23:51:01-root-INFO: grad norm: 1.366 1.355 0.169
2024-12-01-23:51:01-root-INFO: Loss too large (6.521->6.553)! Learning rate decreased to 0.27569.
2024-12-01-23:51:02-root-INFO: grad norm: 0.901 0.895 0.107
2024-12-01-23:51:03-root-INFO: grad norm: 0.679 0.677 0.063
2024-12-01-23:51:04-root-INFO: grad norm: 0.583 0.580 0.060
2024-12-01-23:51:04-root-INFO: grad norm: 0.593 0.590 0.058
2024-12-01-23:51:05-root-INFO: grad norm: 0.627 0.624 0.061
2024-12-01-23:51:06-root-INFO: Loss Change: 6.697 -> 6.198
2024-12-01-23:51:06-root-INFO: Regularization Change: 0.000 -> 1.005
2024-12-01-23:51:06-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-01-23:51:06-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-23:51:06-root-INFO: step: 21 lr_xt 0.43752364
2024-12-01-23:51:07-root-INFO: grad norm: 1.681 1.656 0.288
2024-12-01-23:51:07-root-INFO: Loss too large (6.225->6.345)! Learning rate decreased to 0.35002.
2024-12-01-23:51:08-root-INFO: grad norm: 1.266 1.254 0.178
2024-12-01-23:51:09-root-INFO: grad norm: 1.292 1.285 0.135
2024-12-01-23:51:09-root-INFO: grad norm: 1.268 1.260 0.141
2024-12-01-23:51:10-root-INFO: grad norm: 1.210 1.204 0.114
2024-12-01-23:51:11-root-INFO: grad norm: 1.204 1.197 0.129
2024-12-01-23:51:12-root-INFO: grad norm: 1.745 1.739 0.140
2024-12-01-23:51:12-root-INFO: Loss too large (5.868->5.977)! Learning rate decreased to 0.28002.
2024-12-01-23:51:13-root-INFO: Loss too large (5.868->5.869)! Learning rate decreased to 0.22401.
2024-12-01-23:51:14-root-INFO: grad norm: 0.823 0.818 0.085
2024-12-01-23:51:14-root-INFO: Loss Change: 6.225 -> 5.711
2024-12-01-23:51:14-root-INFO: Regularization Change: 0.000 -> 1.047
2024-12-01-23:51:14-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-01-23:51:14-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-23:51:15-root-INFO: step: 20 lr_xt 0.44432183
2024-12-01-23:51:15-root-INFO: grad norm: 1.317 1.287 0.282
2024-12-01-23:51:16-root-INFO: grad norm: 1.349 1.328 0.235
2024-12-01-23:51:16-root-INFO: Loss too large (5.672->5.749)! Learning rate decreased to 0.35546.
2024-12-01-23:51:17-root-INFO: grad norm: 1.553 1.543 0.174
2024-12-01-23:51:17-root-INFO: Loss too large (5.575->5.653)! Learning rate decreased to 0.28437.
2024-12-01-23:51:18-root-INFO: grad norm: 1.058 1.052 0.114
2024-12-01-23:51:19-root-INFO: grad norm: 0.709 0.706 0.060
2024-12-01-23:51:20-root-INFO: grad norm: 0.565 0.562 0.056
2024-12-01-23:51:21-root-INFO: grad norm: 0.582 0.580 0.050
2024-12-01-23:51:21-root-INFO: grad norm: 0.614 0.612 0.056
2024-12-01-23:51:22-root-INFO: Loss Change: 5.765 -> 5.246
2024-12-01-23:51:22-root-INFO: Regularization Change: 0.000 -> 0.948
2024-12-01-23:51:22-root-INFO: Undo step: 20
2024-12-01-23:51:22-root-INFO: Undo step: 21
2024-12-01-23:51:22-root-INFO: Undo step: 22
2024-12-01-23:51:22-root-INFO: Undo step: 23
2024-12-01-23:51:22-root-INFO: Undo step: 24
2024-12-01-23:51:22-root-INFO: step: 25 lr_xt 0.41069899
2024-12-01-23:51:23-root-INFO: grad norm: 13.596 13.539 1.239
2024-12-01-23:51:23-root-INFO: grad norm: 6.145 6.101 0.738
2024-12-01-23:51:24-root-INFO: grad norm: 4.454 4.422 0.529
2024-12-01-23:51:25-root-INFO: grad norm: 4.192 4.169 0.430
2024-12-01-23:51:26-root-INFO: grad norm: 3.729 3.707 0.399
2024-12-01-23:51:27-root-INFO: grad norm: 3.620 3.601 0.369
2024-12-01-23:51:28-root-INFO: grad norm: 3.299 3.278 0.366
2024-12-01-23:51:29-root-INFO: grad norm: 2.888 2.872 0.299
2024-12-01-23:51:29-root-INFO: Loss Change: 81.819 -> 14.417
2024-12-01-23:51:29-root-INFO: Regularization Change: 0.000 -> 124.000
2024-12-01-23:51:29-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-01-23:51:29-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-23:51:30-root-INFO: step: 24 lr_xt 0.41734684
2024-12-01-23:51:30-root-INFO: grad norm: 3.135 3.105 0.433
2024-12-01-23:51:31-root-INFO: grad norm: 3.505 3.481 0.412
2024-12-01-23:51:32-root-INFO: grad norm: 2.940 2.916 0.381
2024-12-01-23:51:33-root-INFO: grad norm: 2.617 2.597 0.327
2024-12-01-23:51:34-root-INFO: grad norm: 2.495 2.479 0.284
2024-12-01-23:51:34-root-INFO: grad norm: 2.523 2.504 0.305
2024-12-01-23:51:35-root-INFO: grad norm: 2.670 2.653 0.300
2024-12-01-23:51:36-root-INFO: grad norm: 3.644 3.624 0.388
2024-12-01-23:51:37-root-INFO: Loss too large (10.865->11.230)! Learning rate decreased to 0.33388.
2024-12-01-23:51:37-root-INFO: Loss Change: 14.317 -> 10.607
2024-12-01-23:51:37-root-INFO: Regularization Change: 0.000 -> 12.488
2024-12-01-23:51:37-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-01-23:51:37-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-23:51:37-root-INFO: step: 23 lr_xt 0.42403458
2024-12-01-23:51:38-root-INFO: grad norm: 2.654 2.619 0.431
2024-12-01-23:51:38-root-INFO: grad norm: 2.281 2.256 0.338
2024-12-01-23:51:39-root-INFO: grad norm: 2.227 2.208 0.289
2024-12-01-23:51:40-root-INFO: grad norm: 3.082 3.067 0.300
2024-12-01-23:51:41-root-INFO: Loss too large (9.262->9.388)! Learning rate decreased to 0.33923.
2024-12-01-23:51:41-root-INFO: grad norm: 1.993 1.981 0.211
2024-12-01-23:51:42-root-INFO: grad norm: 1.708 1.700 0.163
2024-12-01-23:51:43-root-INFO: grad norm: 1.501 1.491 0.176
2024-12-01-23:51:44-root-INFO: grad norm: 1.264 1.258 0.123
2024-12-01-23:51:44-root-INFO: Loss Change: 10.459 -> 8.090
2024-12-01-23:51:44-root-INFO: Regularization Change: 0.000 -> 4.750
2024-12-01-23:51:44-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-01-23:51:44-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-23:51:45-root-INFO: step: 22 lr_xt 0.43076069
2024-12-01-23:51:45-root-INFO: grad norm: 1.506 1.481 0.272
2024-12-01-23:51:46-root-INFO: grad norm: 2.202 2.185 0.273
2024-12-01-23:51:46-root-INFO: Loss too large (7.876->8.105)! Learning rate decreased to 0.34461.
2024-12-01-23:51:47-root-INFO: grad norm: 1.577 1.563 0.205
2024-12-01-23:51:48-root-INFO: grad norm: 0.914 0.907 0.113
2024-12-01-23:51:49-root-INFO: grad norm: 0.731 0.727 0.079
2024-12-01-23:51:50-root-INFO: grad norm: 0.721 0.717 0.077
2024-12-01-23:51:50-root-INFO: grad norm: 0.777 0.773 0.083
2024-12-01-23:51:51-root-INFO: grad norm: 1.099 1.094 0.102
2024-12-01-23:51:52-root-INFO: Loss too large (6.969->6.978)! Learning rate decreased to 0.27569.
2024-12-01-23:51:52-root-INFO: Loss Change: 7.974 -> 6.927
2024-12-01-23:51:52-root-INFO: Regularization Change: 0.000 -> 2.540
2024-12-01-23:51:52-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-01-23:51:52-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-23:51:53-root-INFO: step: 21 lr_xt 0.43752364
2024-12-01-23:51:53-root-INFO: grad norm: 1.282 1.260 0.235
2024-12-01-23:51:54-root-INFO: grad norm: 1.884 1.872 0.219
2024-12-01-23:51:54-root-INFO: Loss too large (6.735->7.022)! Learning rate decreased to 0.35002.
2024-12-01-23:51:54-root-INFO: Loss too large (6.735->6.803)! Learning rate decreased to 0.28002.
2024-12-01-23:51:55-root-INFO: grad norm: 1.309 1.301 0.143
2024-12-01-23:51:56-root-INFO: grad norm: 0.795 0.791 0.074
2024-12-01-23:51:57-root-INFO: grad norm: 0.665 0.661 0.076
2024-12-01-23:51:58-root-INFO: grad norm: 0.653 0.649 0.064
2024-12-01-23:51:59-root-INFO: grad norm: 0.697 0.693 0.075
2024-12-01-23:51:59-root-INFO: grad norm: 0.942 0.939 0.075
2024-12-01-23:52:00-root-INFO: Loss Change: 6.812 -> 6.157
2024-12-01-23:52:00-root-INFO: Regularization Change: 0.000 -> 1.405
2024-12-01-23:52:00-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-01-23:52:00-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-23:52:00-root-INFO: step: 20 lr_xt 0.44432183
2024-12-01-23:52:01-root-INFO: grad norm: 1.211 1.186 0.245
2024-12-01-23:52:02-root-INFO: grad norm: 1.311 1.299 0.182
2024-12-01-23:52:02-root-INFO: Loss too large (5.957->6.002)! Learning rate decreased to 0.35546.
2024-12-01-23:52:03-root-INFO: grad norm: 1.096 1.090 0.118
2024-12-01-23:52:04-root-INFO: grad norm: 1.235 1.229 0.122
2024-12-01-23:52:04-root-INFO: grad norm: 1.202 1.195 0.131
2024-12-01-23:52:05-root-INFO: grad norm: 1.322 1.316 0.117
2024-12-01-23:52:06-root-INFO: Loss too large (5.667->5.737)! Learning rate decreased to 0.28437.
2024-12-01-23:52:06-root-INFO: grad norm: 1.103 1.097 0.114
2024-12-01-23:52:07-root-INFO: grad norm: 0.803 0.799 0.073
2024-12-01-23:52:08-root-INFO: Loss Change: 6.104 -> 5.467
2024-12-01-23:52:08-root-INFO: Regularization Change: 0.000 -> 1.452
2024-12-01-23:52:08-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-01-23:52:08-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-23:52:08-root-INFO: step: 19 lr_xt 0.45115363
2024-12-01-23:52:08-root-INFO: grad norm: 1.079 1.058 0.211
2024-12-01-23:52:09-root-INFO: grad norm: 1.566 1.557 0.166
2024-12-01-23:52:10-root-INFO: Loss too large (5.331->5.467)! Learning rate decreased to 0.36092.
2024-12-01-23:52:10-root-INFO: Loss too large (5.331->5.349)! Learning rate decreased to 0.28874.
2024-12-01-23:52:11-root-INFO: grad norm: 0.939 0.935 0.087
2024-12-01-23:52:12-root-INFO: grad norm: 0.726 0.723 0.062
2024-12-01-23:52:12-root-INFO: grad norm: 0.652 0.649 0.064
2024-12-01-23:52:13-root-INFO: grad norm: 0.642 0.640 0.057
2024-12-01-23:52:14-root-INFO: grad norm: 0.688 0.685 0.066
2024-12-01-23:52:15-root-INFO: grad norm: 0.932 0.930 0.066
2024-12-01-23:52:16-root-INFO: Loss too large (4.966->4.970)! Learning rate decreased to 0.23099.
2024-12-01-23:52:16-root-INFO: Loss Change: 5.397 -> 4.948
2024-12-01-23:52:16-root-INFO: Regularization Change: 0.000 -> 0.971
2024-12-01-23:52:16-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-01-23:52:16-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-23:52:16-root-INFO: step: 18 lr_xt 0.45801735
2024-12-01-23:52:17-root-INFO: grad norm: 1.204 1.180 0.241
2024-12-01-23:52:18-root-INFO: grad norm: 1.435 1.425 0.172
2024-12-01-23:52:18-root-INFO: Loss too large (4.855->4.891)! Learning rate decreased to 0.36641.
2024-12-01-23:52:19-root-INFO: grad norm: 0.884 0.879 0.089
2024-12-01-23:52:19-root-INFO: grad norm: 0.722 0.718 0.080
2024-12-01-23:52:20-root-INFO: grad norm: 0.733 0.730 0.064
2024-12-01-23:52:21-root-INFO: grad norm: 0.745 0.741 0.076
2024-12-01-23:52:22-root-INFO: grad norm: 0.882 0.878 0.080
2024-12-01-23:52:22-root-INFO: Loss too large (4.522->4.524)! Learning rate decreased to 0.29313.
2024-12-01-23:52:23-root-INFO: grad norm: 0.772 0.768 0.079
2024-12-01-23:52:24-root-INFO: Loss Change: 4.967 -> 4.438
2024-12-01-23:52:24-root-INFO: Regularization Change: 0.000 -> 1.202
2024-12-01-23:52:24-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-01-23:52:24-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-23:52:24-root-INFO: step: 17 lr_xt 0.46491129
2024-12-01-23:52:24-root-INFO: grad norm: 1.760 1.739 0.275
2024-12-01-23:52:25-root-INFO: Loss too large (4.539->4.740)! Learning rate decreased to 0.37193.
2024-12-01-23:52:25-root-INFO: Loss too large (4.539->4.548)! Learning rate decreased to 0.29754.
2024-12-01-23:52:26-root-INFO: grad norm: 1.066 1.060 0.111
2024-12-01-23:52:27-root-INFO: grad norm: 0.720 0.718 0.057
2024-12-01-23:52:28-root-INFO: grad norm: 0.554 0.551 0.053
2024-12-01-23:52:28-root-INFO: grad norm: 0.577 0.576 0.044
2024-12-01-23:52:29-root-INFO: grad norm: 0.637 0.634 0.055
2024-12-01-23:52:30-root-INFO: grad norm: 0.886 0.885 0.053
2024-12-01-23:52:30-root-INFO: Loss too large (4.120->4.131)! Learning rate decreased to 0.23803.
2024-12-01-23:52:31-root-INFO: grad norm: 0.610 0.608 0.052
2024-12-01-23:52:32-root-INFO: Loss Change: 4.539 -> 4.045
2024-12-01-23:52:32-root-INFO: Regularization Change: 0.000 -> 0.701
2024-12-01-23:52:32-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-01-23:52:32-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-23:52:32-root-INFO: step: 16 lr_xt 0.47183369
2024-12-01-23:52:33-root-INFO: grad norm: 1.215 1.192 0.238
2024-12-01-23:52:33-root-INFO: grad norm: 1.386 1.370 0.212
2024-12-01-23:52:34-root-INFO: Loss too large (4.097->4.341)! Learning rate decreased to 0.37747.
2024-12-01-23:52:35-root-INFO: grad norm: 2.044 2.037 0.170
2024-12-01-23:52:35-root-INFO: Loss too large (4.090->4.144)! Learning rate decreased to 0.30197.
2024-12-01-23:52:36-root-INFO: grad norm: 1.003 0.998 0.103
2024-12-01-23:52:37-root-INFO: grad norm: 0.612 0.611 0.044
2024-12-01-23:52:37-root-INFO: grad norm: 0.512 0.509 0.048
2024-12-01-23:52:38-root-INFO: grad norm: 0.467 0.466 0.033
2024-12-01-23:52:39-root-INFO: grad norm: 0.406 0.404 0.035
2024-12-01-23:52:40-root-INFO: Loss Change: 4.129 -> 3.690
2024-12-01-23:52:40-root-INFO: Regularization Change: 0.000 -> 0.848
2024-12-01-23:52:40-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-01-23:52:40-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-23:52:40-root-INFO: step: 15 lr_xt 0.47878275
2024-12-01-23:52:40-root-INFO: grad norm: 1.339 1.314 0.261
2024-12-01-23:52:41-root-INFO: grad norm: 1.396 1.379 0.211
2024-12-01-23:52:42-root-INFO: grad norm: 1.880 1.866 0.229
2024-12-01-23:52:42-root-INFO: Loss too large (3.795->4.083)! Learning rate decreased to 0.38303.
2024-12-01-23:52:43-root-INFO: Loss too large (3.795->3.806)! Learning rate decreased to 0.30642.
2024-12-01-23:52:44-root-INFO: grad norm: 1.078 1.072 0.116
2024-12-01-23:52:45-root-INFO: grad norm: 0.560 0.559 0.040
2024-12-01-23:52:45-root-INFO: grad norm: 0.435 0.434 0.033
2024-12-01-23:52:46-root-INFO: grad norm: 0.473 0.472 0.032
2024-12-01-23:52:47-root-INFO: grad norm: 0.730 0.729 0.034
2024-12-01-23:52:47-root-INFO: Loss too large (3.386->3.395)! Learning rate decreased to 0.24514.
2024-12-01-23:52:48-root-INFO: Loss Change: 3.810 -> 3.379
2024-12-01-23:52:48-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-01-23:52:48-root-INFO: Undo step: 15
2024-12-01-23:52:48-root-INFO: Undo step: 16
2024-12-01-23:52:48-root-INFO: Undo step: 17
2024-12-01-23:52:48-root-INFO: Undo step: 18
2024-12-01-23:52:48-root-INFO: Undo step: 19
2024-12-01-23:52:48-root-INFO: step: 20 lr_xt 0.44432183
2024-12-01-23:52:49-root-INFO: grad norm: 12.834 12.784 1.125
2024-12-01-23:52:49-root-INFO: grad norm: 5.514 5.483 0.584
2024-12-01-23:52:50-root-INFO: grad norm: 3.891 3.871 0.393
2024-12-01-23:52:51-root-INFO: grad norm: 3.255 3.231 0.393
2024-12-01-23:52:52-root-INFO: grad norm: 2.805 2.790 0.298
2024-12-01-23:52:53-root-INFO: grad norm: 2.606 2.590 0.290
2024-12-01-23:52:54-root-INFO: grad norm: 2.556 2.542 0.271
2024-12-01-23:52:55-root-INFO: grad norm: 2.502 2.487 0.274
2024-12-01-23:52:55-root-INFO: Loss Change: 73.450 -> 10.759
2024-12-01-23:52:55-root-INFO: Regularization Change: 0.000 -> 122.052
2024-12-01-23:52:55-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-01-23:52:55-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-23:52:56-root-INFO: step: 19 lr_xt 0.45115363
2024-12-01-23:52:56-root-INFO: grad norm: 2.761 2.735 0.379
2024-12-01-23:52:57-root-INFO: grad norm: 2.680 2.656 0.355
2024-12-01-23:52:57-root-INFO: grad norm: 2.553 2.533 0.316
2024-12-01-23:52:58-root-INFO: grad norm: 2.560 2.538 0.336
2024-12-01-23:52:59-root-INFO: grad norm: 2.457 2.436 0.320
2024-12-01-23:53:00-root-INFO: grad norm: 2.236 2.212 0.322
2024-12-01-23:53:01-root-INFO: grad norm: 2.295 2.274 0.308
2024-12-01-23:53:02-root-INFO: grad norm: 2.394 2.370 0.342
2024-12-01-23:53:03-root-INFO: Loss Change: 10.529 -> 7.297
2024-12-01-23:53:03-root-INFO: Regularization Change: 0.000 -> 11.407
2024-12-01-23:53:03-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-01-23:53:03-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-23:53:03-root-INFO: step: 18 lr_xt 0.45801735
2024-12-01-23:53:03-root-INFO: grad norm: 3.025 2.977 0.534
2024-12-01-23:53:04-root-INFO: grad norm: 2.666 2.626 0.461
2024-12-01-23:53:05-root-INFO: grad norm: 2.374 2.348 0.353
2024-12-01-23:53:06-root-INFO: grad norm: 2.630 2.609 0.329
2024-12-01-23:53:07-root-INFO: grad norm: 2.295 2.274 0.308
2024-12-01-23:53:08-root-INFO: grad norm: 2.184 2.162 0.311
2024-12-01-23:53:08-root-INFO: grad norm: 2.190 2.169 0.299
2024-12-01-23:53:09-root-INFO: grad norm: 2.200 2.177 0.314
2024-12-01-23:53:10-root-INFO: Loss Change: 7.424 -> 5.702
2024-12-01-23:53:10-root-INFO: Regularization Change: 0.000 -> 4.797
2024-12-01-23:53:10-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-01-23:53:10-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-23:53:10-root-INFO: step: 17 lr_xt 0.46491129
2024-12-01-23:53:11-root-INFO: grad norm: 2.781 2.737 0.489
2024-12-01-23:53:11-root-INFO: grad norm: 2.493 2.456 0.429
2024-12-01-23:53:12-root-INFO: grad norm: 2.252 2.226 0.341
2024-12-01-23:53:13-root-INFO: grad norm: 2.127 2.101 0.336
2024-12-01-23:53:14-root-INFO: grad norm: 2.005 1.983 0.293
2024-12-01-23:53:15-root-INFO: grad norm: 1.956 1.931 0.307
2024-12-01-23:53:16-root-INFO: grad norm: 2.351 2.331 0.306
2024-12-01-23:53:16-root-INFO: Loss too large (4.873->4.942)! Learning rate decreased to 0.37193.
2024-12-01-23:53:17-root-INFO: grad norm: 1.362 1.347 0.201
2024-12-01-23:53:17-root-INFO: Loss Change: 5.854 -> 4.407
2024-12-01-23:53:17-root-INFO: Regularization Change: 0.000 -> 2.722
2024-12-01-23:53:17-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-01-23:53:17-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-23:53:18-root-INFO: step: 16 lr_xt 0.47183369
2024-12-01-23:53:18-root-INFO: grad norm: 1.629 1.598 0.315
2024-12-01-23:53:19-root-INFO: grad norm: 1.496 1.470 0.278
2024-12-01-23:53:20-root-INFO: grad norm: 1.506 1.488 0.233
2024-12-01-23:53:21-root-INFO: grad norm: 1.516 1.494 0.258
2024-12-01-23:53:21-root-INFO: grad norm: 2.069 2.052 0.265
2024-12-01-23:53:22-root-INFO: Loss too large (4.184->4.316)! Learning rate decreased to 0.37747.
2024-12-01-23:53:22-root-INFO: grad norm: 1.240 1.226 0.182
2024-12-01-23:53:23-root-INFO: grad norm: 0.778 0.772 0.094
2024-12-01-23:53:24-root-INFO: grad norm: 0.631 0.626 0.086
2024-12-01-23:53:25-root-INFO: Loss Change: 4.506 -> 3.739
2024-12-01-23:53:25-root-INFO: Regularization Change: 0.000 -> 1.673
2024-12-01-23:53:25-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-01-23:53:25-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-23:53:25-root-INFO: step: 15 lr_xt 0.47878275
2024-12-01-23:53:25-root-INFO: grad norm: 1.529 1.501 0.292
2024-12-01-23:53:26-root-INFO: grad norm: 1.472 1.451 0.247
2024-12-01-23:53:27-root-INFO: grad norm: 1.877 1.861 0.247
2024-12-01-23:53:27-root-INFO: Loss too large (3.753->3.952)! Learning rate decreased to 0.38303.
2024-12-01-23:53:28-root-INFO: grad norm: 1.340 1.329 0.176
2024-12-01-23:53:29-root-INFO: grad norm: 0.920 0.915 0.097
2024-12-01-23:53:30-root-INFO: grad norm: 0.731 0.726 0.088
2024-12-01-23:53:31-root-INFO: grad norm: 0.799 0.796 0.068
2024-12-01-23:53:31-root-INFO: Loss too large (3.350->3.357)! Learning rate decreased to 0.30642.
2024-12-01-23:53:32-root-INFO: grad norm: 0.654 0.652 0.059
2024-12-01-23:53:33-root-INFO: Loss Change: 3.860 -> 3.270
2024-12-01-23:53:33-root-INFO: Regularization Change: 0.000 -> 1.128
2024-12-01-23:53:33-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-01-23:53:33-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-23:53:33-root-INFO: step: 14 lr_xt 0.48575663
2024-12-01-23:53:34-root-INFO: grad norm: 1.406 1.383 0.253
2024-12-01-23:53:34-root-INFO: Loss too large (3.385->3.409)! Learning rate decreased to 0.38861.
2024-12-01-23:53:35-root-INFO: grad norm: 0.982 0.974 0.131
2024-12-01-23:53:36-root-INFO: grad norm: 0.699 0.695 0.078
2024-12-01-23:53:37-root-INFO: grad norm: 0.540 0.536 0.063
2024-12-01-23:53:37-root-INFO: grad norm: 0.450 0.448 0.042
2024-12-01-23:53:38-root-INFO: grad norm: 0.401 0.399 0.043
2024-12-01-23:53:39-root-INFO: grad norm: 0.392 0.390 0.033
2024-12-01-23:53:40-root-INFO: grad norm: 0.515 0.513 0.044
2024-12-01-23:53:40-root-INFO: Loss too large (2.958->2.975)! Learning rate decreased to 0.31088.
2024-12-01-23:53:41-root-INFO: Loss Change: 3.385 -> 2.952
2024-12-01-23:53:41-root-INFO: Regularization Change: 0.000 -> 0.869
2024-12-01-23:53:41-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-01-23:53:41-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-23:53:41-root-INFO: step: 13 lr_xt 0.49275347
2024-12-01-23:53:42-root-INFO: grad norm: 1.564 1.542 0.261
2024-12-01-23:53:42-root-INFO: Loss too large (3.115->3.152)! Learning rate decreased to 0.39420.
2024-12-01-23:53:43-root-INFO: grad norm: 0.959 0.951 0.128
2024-12-01-23:53:44-root-INFO: grad norm: 0.714 0.710 0.071
2024-12-01-23:53:44-root-INFO: grad norm: 0.498 0.494 0.058
2024-12-01-23:53:45-root-INFO: grad norm: 0.437 0.435 0.041
2024-12-01-23:53:46-root-INFO: grad norm: 0.541 0.539 0.049
2024-12-01-23:53:46-root-INFO: Loss too large (2.737->2.752)! Learning rate decreased to 0.31536.
2024-12-01-23:53:47-root-INFO: grad norm: 0.737 0.736 0.041
2024-12-01-23:53:48-root-INFO: Loss too large (2.728->2.737)! Learning rate decreased to 0.25229.
2024-12-01-23:53:48-root-INFO: grad norm: 0.605 0.604 0.040
2024-12-01-23:53:49-root-INFO: Loss Change: 3.115 -> 2.678
2024-12-01-23:53:49-root-INFO: Regularization Change: 0.000 -> 0.690
2024-12-01-23:53:49-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-01-23:53:49-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-23:53:49-root-INFO: step: 12 lr_xt 0.49977135
2024-12-01-23:53:50-root-INFO: grad norm: 1.245 1.225 0.218
2024-12-01-23:53:50-root-INFO: Loss too large (2.817->2.821)! Learning rate decreased to 0.39982.
2024-12-01-23:53:51-root-INFO: grad norm: 0.862 0.856 0.107
2024-12-01-23:53:52-root-INFO: grad norm: 0.597 0.593 0.067
2024-12-01-23:53:53-root-INFO: grad norm: 0.618 0.615 0.062
2024-12-01-23:53:53-root-INFO: Loss too large (2.572->2.588)! Learning rate decreased to 0.31985.
2024-12-01-23:53:54-root-INFO: grad norm: 0.761 0.760 0.044
2024-12-01-23:53:54-root-INFO: Loss too large (2.558->2.563)! Learning rate decreased to 0.25588.
2024-12-01-23:53:55-root-INFO: grad norm: 0.612 0.611 0.039
2024-12-01-23:53:56-root-INFO: grad norm: 0.472 0.471 0.027
2024-12-01-23:53:57-root-INFO: grad norm: 0.470 0.469 0.031
2024-12-01-23:53:57-root-INFO: Loss Change: 2.817 -> 2.476
2024-12-01-23:53:57-root-INFO: Regularization Change: 0.000 -> 0.545
2024-12-01-23:53:57-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-01-23:53:57-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-23:53:58-root-INFO: step: 11 lr_xt 0.50680833
2024-12-01-23:53:58-root-INFO: grad norm: 1.301 1.280 0.230
2024-12-01-23:53:59-root-INFO: grad norm: 1.096 1.084 0.158
2024-12-01-23:54:00-root-INFO: grad norm: 1.071 1.062 0.138
2024-12-01-23:54:01-root-INFO: grad norm: 1.057 1.049 0.134
2024-12-01-23:54:01-root-INFO: grad norm: 1.098 1.091 0.120
2024-12-01-23:54:02-root-INFO: Loss too large (2.461->2.481)! Learning rate decreased to 0.40545.
2024-12-01-23:54:03-root-INFO: grad norm: 0.885 0.882 0.075
2024-12-01-23:54:03-root-INFO: grad norm: 0.698 0.696 0.046
2024-12-01-23:54:04-root-INFO: grad norm: 0.601 0.600 0.035
2024-12-01-23:54:05-root-INFO: Loss too large (2.288->2.294)! Learning rate decreased to 0.32436.
2024-12-01-23:54:05-root-INFO: Loss Change: 2.659 -> 2.271
2024-12-01-23:54:05-root-INFO: Regularization Change: 0.000 -> 0.912
2024-12-01-23:54:05-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-01-23:54:05-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-23:54:05-root-INFO: step: 10 lr_xt 0.51386241
2024-12-01-23:54:06-root-INFO: grad norm: 1.183 1.166 0.200
2024-12-01-23:54:06-root-INFO: grad norm: 1.179 1.174 0.116
2024-12-01-23:54:07-root-INFO: Loss too large (2.371->2.372)! Learning rate decreased to 0.41109.
2024-12-01-23:54:08-root-INFO: grad norm: 0.731 0.729 0.057
2024-12-01-23:54:09-root-INFO: grad norm: 0.455 0.453 0.037
2024-12-01-23:54:10-root-INFO: grad norm: 0.389 0.388 0.027
2024-12-01-23:54:10-root-INFO: grad norm: 0.529 0.528 0.028
2024-12-01-23:54:11-root-INFO: Loss too large (2.142->2.153)! Learning rate decreased to 0.32887.
2024-12-01-23:54:12-root-INFO: grad norm: 0.574 0.573 0.033
2024-12-01-23:54:12-root-INFO: grad norm: 0.621 0.620 0.034
2024-12-01-23:54:13-root-INFO: Loss Change: 2.454 -> 2.115
2024-12-01-23:54:13-root-INFO: Regularization Change: 0.000 -> 0.685
2024-12-01-23:54:13-root-INFO: Undo step: 10
2024-12-01-23:54:13-root-INFO: Undo step: 11
2024-12-01-23:54:13-root-INFO: Undo step: 12
2024-12-01-23:54:13-root-INFO: Undo step: 13
2024-12-01-23:54:13-root-INFO: Undo step: 14
2024-12-01-23:54:13-root-INFO: step: 15 lr_xt 0.47878275
2024-12-01-23:54:14-root-INFO: grad norm: 12.766 12.737 0.862
2024-12-01-23:54:15-root-INFO: grad norm: 4.992 4.974 0.419
2024-12-01-23:54:15-root-INFO: grad norm: 3.205 3.191 0.295
2024-12-01-23:54:16-root-INFO: grad norm: 2.435 2.424 0.232
2024-12-01-23:54:17-root-INFO: grad norm: 2.000 1.991 0.193
2024-12-01-23:54:18-root-INFO: grad norm: 1.699 1.690 0.175
2024-12-01-23:54:19-root-INFO: grad norm: 1.507 1.499 0.150
2024-12-01-23:54:20-root-INFO: grad norm: 1.382 1.373 0.152
2024-12-01-23:54:20-root-INFO: Loss Change: 68.642 -> 7.061
2024-12-01-23:54:20-root-INFO: Regularization Change: 0.000 -> 122.276
2024-12-01-23:54:20-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-01-23:54:20-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-23:54:21-root-INFO: step: 14 lr_xt 0.48575663
2024-12-01-23:54:21-root-INFO: grad norm: 1.931 1.914 0.260
2024-12-01-23:54:22-root-INFO: grad norm: 1.805 1.787 0.253
2024-12-01-23:54:23-root-INFO: grad norm: 1.776 1.766 0.194
2024-12-01-23:54:24-root-INFO: grad norm: 1.715 1.698 0.236
2024-12-01-23:54:24-root-INFO: grad norm: 1.709 1.697 0.202
2024-12-01-23:54:25-root-INFO: grad norm: 1.720 1.704 0.231
2024-12-01-23:54:26-root-INFO: grad norm: 1.916 1.905 0.209
2024-12-01-23:54:26-root-INFO: Loss too large (4.836->4.841)! Learning rate decreased to 0.38861.
2024-12-01-23:54:27-root-INFO: grad norm: 1.310 1.298 0.171
2024-12-01-23:54:28-root-INFO: Loss Change: 6.900 -> 4.346
2024-12-01-23:54:28-root-INFO: Regularization Change: 0.000 -> 8.675
2024-12-01-23:54:28-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-01-23:54:28-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-23:54:28-root-INFO: step: 13 lr_xt 0.49275347
2024-12-01-23:54:28-root-INFO: grad norm: 1.721 1.698 0.277
2024-12-01-23:54:29-root-INFO: grad norm: 1.557 1.536 0.250
2024-12-01-23:54:30-root-INFO: grad norm: 1.601 1.586 0.214
2024-12-01-23:54:30-root-INFO: Loss too large (3.969->3.983)! Learning rate decreased to 0.39420.
2024-12-01-23:54:31-root-INFO: grad norm: 1.251 1.239 0.174
2024-12-01-23:54:32-root-INFO: grad norm: 1.142 1.137 0.110
2024-12-01-23:54:33-root-INFO: grad norm: 0.865 0.859 0.102
2024-12-01-23:54:34-root-INFO: grad norm: 0.665 0.663 0.057
2024-12-01-23:54:35-root-INFO: grad norm: 0.553 0.550 0.057
2024-12-01-23:54:35-root-INFO: Loss Change: 4.385 -> 3.255
2024-12-01-23:54:35-root-INFO: Regularization Change: 0.000 -> 2.813
2024-12-01-23:54:35-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-01-23:54:35-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-23:54:36-root-INFO: step: 12 lr_xt 0.49977135
2024-12-01-23:54:36-root-INFO: grad norm: 1.188 1.171 0.200
2024-12-01-23:54:37-root-INFO: grad norm: 1.114 1.102 0.164
2024-12-01-23:54:38-root-INFO: grad norm: 1.197 1.190 0.133
2024-12-01-23:54:39-root-INFO: grad norm: 1.358 1.350 0.144
2024-12-01-23:54:39-root-INFO: grad norm: 1.175 1.168 0.128
2024-12-01-23:54:40-root-INFO: grad norm: 1.155 1.145 0.157
2024-12-01-23:54:41-root-INFO: grad norm: 1.208 1.198 0.155
2024-12-01-23:54:42-root-INFO: Loss too large (2.861->2.905)! Learning rate decreased to 0.39982.
2024-12-01-23:54:42-root-INFO: grad norm: 0.999 0.991 0.133
2024-12-01-23:54:43-root-INFO: Loss Change: 3.272 -> 2.722
2024-12-01-23:54:43-root-INFO: Regularization Change: 0.000 -> 1.860
2024-12-01-23:54:43-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-01-23:54:43-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-23:54:43-root-INFO: step: 11 lr_xt 0.50680833
2024-12-01-23:54:44-root-INFO: grad norm: 1.853 1.826 0.312
2024-12-01-23:54:44-root-INFO: Loss too large (2.929->2.955)! Learning rate decreased to 0.40545.
2024-12-01-23:54:45-root-INFO: grad norm: 1.124 1.114 0.152
2024-12-01-23:54:46-root-INFO: grad norm: 0.726 0.723 0.073
2024-12-01-23:54:46-root-INFO: grad norm: 0.515 0.513 0.051
2024-12-01-23:54:47-root-INFO: grad norm: 0.465 0.464 0.031
2024-12-01-23:54:48-root-INFO: grad norm: 0.577 0.576 0.027
2024-12-01-23:54:49-root-INFO: grad norm: 0.653 0.652 0.037
2024-12-01-23:54:50-root-INFO: grad norm: 0.763 0.762 0.040
2024-12-01-23:54:50-root-INFO: Loss too large (2.371->2.380)! Learning rate decreased to 0.32436.
2024-12-01-23:54:51-root-INFO: Loss Change: 2.929 -> 2.347
2024-12-01-23:54:51-root-INFO: Regularization Change: 0.000 -> 1.036
2024-12-01-23:54:51-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-01-23:54:51-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-23:54:51-root-INFO: step: 10 lr_xt 0.51386241
2024-12-01-23:54:51-root-INFO: grad norm: 1.162 1.147 0.184
2024-12-01-23:54:52-root-INFO: grad norm: 1.145 1.140 0.107
2024-12-01-23:54:53-root-INFO: Loss too large (2.372->2.372)! Learning rate decreased to 0.41109.
2024-12-01-23:54:53-root-INFO: grad norm: 0.788 0.786 0.050
2024-12-01-23:54:54-root-INFO: grad norm: 0.595 0.594 0.035
2024-12-01-23:54:55-root-INFO: grad norm: 0.625 0.624 0.034
2024-12-01-23:54:56-root-INFO: Loss too large (2.175->2.176)! Learning rate decreased to 0.32887.
2024-12-01-23:54:56-root-INFO: grad norm: 0.581 0.581 0.029
2024-12-01-23:54:57-root-INFO: grad norm: 0.541 0.540 0.034
2024-12-01-23:54:58-root-INFO: grad norm: 0.501 0.500 0.030
2024-12-01-23:54:59-root-INFO: Loss Change: 2.460 -> 2.095
2024-12-01-23:54:59-root-INFO: Regularization Change: 0.000 -> 0.705
2024-12-01-23:54:59-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-01-23:54:59-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-23:54:59-root-INFO: step: 9 lr_xt 0.52093157
2024-12-01-23:54:59-root-INFO: grad norm: 0.990 0.976 0.165
2024-12-01-23:55:00-root-INFO: grad norm: 0.835 0.829 0.093
2024-12-01-23:55:01-root-INFO: grad norm: 0.871 0.868 0.071
2024-12-01-23:55:01-root-INFO: Loss too large (2.099->2.099)! Learning rate decreased to 0.41675.
2024-12-01-23:55:02-root-INFO: grad norm: 0.737 0.736 0.041
2024-12-01-23:55:03-root-INFO: grad norm: 0.736 0.735 0.036
2024-12-01-23:55:04-root-INFO: grad norm: 0.809 0.808 0.041
2024-12-01-23:55:05-root-INFO: grad norm: 0.717 0.715 0.042
2024-12-01-23:55:06-root-INFO: grad norm: 0.617 0.616 0.040
2024-12-01-23:55:06-root-INFO: Loss Change: 2.218 -> 1.936
2024-12-01-23:55:06-root-INFO: Regularization Change: 0.000 -> 0.740
2024-12-01-23:55:06-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-01-23:55:06-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-23:55:07-root-INFO: step: 8 lr_xt 0.52801377
2024-12-01-23:55:07-root-INFO: grad norm: 1.049 1.039 0.148
2024-12-01-23:55:08-root-INFO: grad norm: 0.991 0.988 0.072
2024-12-01-23:55:09-root-INFO: grad norm: 0.856 0.854 0.058
2024-12-01-23:55:09-root-INFO: grad norm: 0.832 0.831 0.048
2024-12-01-23:55:10-root-INFO: grad norm: 0.901 0.900 0.047
2024-12-01-23:55:11-root-INFO: Loss too large (1.915->1.936)! Learning rate decreased to 0.42241.
2024-12-01-23:55:12-root-INFO: grad norm: 0.758 0.758 0.033
2024-12-01-23:55:12-root-INFO: grad norm: 0.637 0.636 0.034
2024-12-01-23:55:13-root-INFO: grad norm: 0.565 0.564 0.033
2024-12-01-23:55:14-root-INFO: Loss Change: 2.079 -> 1.785
2024-12-01-23:55:14-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-01-23:55:14-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-01-23:55:14-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-23:55:14-root-INFO: step: 7 lr_xt 0.53510690
2024-12-01-23:55:14-root-INFO: grad norm: 1.009 0.998 0.148
2024-12-01-23:55:15-root-INFO: grad norm: 0.756 0.753 0.066
2024-12-01-23:55:16-root-INFO: grad norm: 0.743 0.742 0.046
2024-12-01-23:55:17-root-INFO: grad norm: 0.831 0.830 0.039
2024-12-01-23:55:17-root-INFO: Loss too large (1.790->1.793)! Learning rate decreased to 0.42809.
2024-12-01-23:55:18-root-INFO: grad norm: 0.650 0.649 0.033
2024-12-01-23:55:19-root-INFO: grad norm: 0.552 0.551 0.031
2024-12-01-23:55:20-root-INFO: grad norm: 0.478 0.477 0.030
2024-12-01-23:55:21-root-INFO: grad norm: 0.443 0.442 0.029
2024-12-01-23:55:21-root-INFO: Loss Change: 1.952 -> 1.657
2024-12-01-23:55:21-root-INFO: Regularization Change: 0.000 -> 0.652
2024-12-01-23:55:21-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-01-23:55:21-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-23:55:22-root-INFO: step: 6 lr_xt 0.54220886
2024-12-01-23:55:22-root-INFO: grad norm: 0.903 0.893 0.130
2024-12-01-23:55:23-root-INFO: grad norm: 0.561 0.558 0.053
2024-12-01-23:55:24-root-INFO: grad norm: 0.487 0.485 0.037
2024-12-01-23:55:25-root-INFO: grad norm: 0.495 0.494 0.028
2024-12-01-23:55:26-root-INFO: grad norm: 0.554 0.553 0.027
2024-12-01-23:55:26-root-INFO: Loss too large (1.618->1.623)! Learning rate decreased to 0.43377.
2024-12-01-23:55:27-root-INFO: grad norm: 0.447 0.447 0.028
2024-12-01-23:55:28-root-INFO: grad norm: 0.370 0.369 0.026
2024-12-01-23:55:29-root-INFO: grad norm: 0.335 0.334 0.025
2024-12-01-23:55:29-root-INFO: Loss Change: 1.826 -> 1.550
2024-12-01-23:55:29-root-INFO: Regularization Change: 0.000 -> 0.636
2024-12-01-23:55:29-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-01-23:55:29-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-23:55:29-root-INFO: step: 5 lr_xt 0.54931747
2024-12-01-23:55:30-root-INFO: grad norm: 0.903 0.894 0.124
2024-12-01-23:55:31-root-INFO: grad norm: 0.497 0.495 0.046
2024-12-01-23:55:31-root-INFO: grad norm: 0.330 0.328 0.034
2024-12-01-23:55:32-root-INFO: grad norm: 0.285 0.284 0.028
2024-12-01-23:55:33-root-INFO: grad norm: 0.302 0.301 0.028
2024-12-01-23:55:34-root-INFO: grad norm: 0.307 0.306 0.026
2024-12-01-23:55:35-root-INFO: grad norm: 0.326 0.325 0.027
2024-12-01-23:55:36-root-INFO: grad norm: 0.336 0.335 0.026
2024-12-01-23:55:36-root-INFO: Loss Change: 1.740 -> 1.462
2024-12-01-23:55:36-root-INFO: Regularization Change: 0.000 -> 0.704
2024-12-01-23:55:36-root-INFO: Undo step: 5
2024-12-01-23:55:36-root-INFO: Undo step: 6
2024-12-01-23:55:36-root-INFO: Undo step: 7
2024-12-01-23:55:36-root-INFO: Undo step: 8
2024-12-01-23:55:36-root-INFO: Undo step: 9
2024-12-01-23:55:37-root-INFO: step: 10 lr_xt 0.51386241
2024-12-01-23:55:37-root-INFO: grad norm: 13.492 13.474 0.700
2024-12-01-23:55:38-root-INFO: grad norm: 4.715 4.699 0.390
2024-12-01-23:55:39-root-INFO: grad norm: 2.896 2.888 0.210
2024-12-01-23:55:40-root-INFO: grad norm: 2.441 2.435 0.174
2024-12-01-23:55:40-root-INFO: grad norm: 1.799 1.794 0.140
2024-12-01-23:55:41-root-INFO: grad norm: 1.584 1.577 0.143
2024-12-01-23:55:42-root-INFO: grad norm: 1.618 1.612 0.135
2024-12-01-23:55:43-root-INFO: grad norm: 1.447 1.439 0.144
2024-12-01-23:55:44-root-INFO: Loss Change: 60.587 -> 4.430
2024-12-01-23:55:44-root-INFO: Regularization Change: 0.000 -> 114.168
2024-12-01-23:55:44-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-01-23:55:44-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-23:55:44-root-INFO: step: 9 lr_xt 0.52093157
2024-12-01-23:55:44-root-INFO: grad norm: 1.704 1.692 0.207
2024-12-01-23:55:45-root-INFO: grad norm: 1.397 1.386 0.172
2024-12-01-23:55:46-root-INFO: grad norm: 1.216 1.211 0.106
2024-12-01-23:55:47-root-INFO: grad norm: 1.226 1.222 0.101
2024-12-01-23:55:48-root-INFO: grad norm: 0.955 0.952 0.073
2024-12-01-23:55:49-root-INFO: grad norm: 0.815 0.811 0.080
2024-12-01-23:55:50-root-INFO: grad norm: 0.732 0.729 0.061
2024-12-01-23:55:50-root-INFO: grad norm: 0.772 0.768 0.079
2024-12-01-23:55:51-root-INFO: Loss Change: 4.316 -> 2.644
2024-12-01-23:55:51-root-INFO: Regularization Change: 0.000 -> 6.056
2024-12-01-23:55:51-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-01-23:55:51-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-23:55:51-root-INFO: step: 8 lr_xt 0.52801377
2024-12-01-23:55:52-root-INFO: grad norm: 1.563 1.548 0.215
2024-12-01-23:55:52-root-INFO: grad norm: 1.136 1.126 0.146
2024-12-01-23:55:53-root-INFO: grad norm: 0.880 0.877 0.079
2024-12-01-23:55:54-root-INFO: grad norm: 0.763 0.759 0.078
2024-12-01-23:55:55-root-INFO: grad norm: 0.749 0.747 0.047
2024-12-01-23:55:56-root-INFO: grad norm: 0.771 0.769 0.046
2024-12-01-23:55:57-root-INFO: grad norm: 0.757 0.756 0.044
2024-12-01-23:55:57-root-INFO: grad norm: 0.866 0.864 0.046
2024-12-01-23:55:58-root-INFO: Loss too large (2.084->2.096)! Learning rate decreased to 0.42241.
2024-12-01-23:55:58-root-INFO: Loss Change: 2.723 -> 2.037
2024-12-01-23:55:58-root-INFO: Regularization Change: 0.000 -> 2.087
2024-12-01-23:55:58-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-01-23:55:58-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-23:55:59-root-INFO: step: 7 lr_xt 0.53510690
2024-12-01-23:55:59-root-INFO: grad norm: 1.053 1.046 0.123
2024-12-01-23:56:00-root-INFO: grad norm: 0.853 0.852 0.056
2024-12-01-23:56:01-root-INFO: grad norm: 0.784 0.783 0.040
2024-12-01-23:56:02-root-INFO: grad norm: 0.835 0.834 0.039
2024-12-01-23:56:02-root-INFO: grad norm: 0.817 0.816 0.046
2024-12-01-23:56:03-root-INFO: grad norm: 0.934 0.933 0.042
2024-12-01-23:56:04-root-INFO: grad norm: 0.751 0.750 0.047
2024-12-01-23:56:05-root-INFO: grad norm: 0.867 0.865 0.050
2024-12-01-23:56:06-root-INFO: Loss Change: 2.094 -> 1.795
2024-12-01-23:56:06-root-INFO: Regularization Change: 0.000 -> 1.131
2024-12-01-23:56:06-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-01-23:56:06-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-23:56:06-root-INFO: step: 6 lr_xt 0.54220886
2024-12-01-23:56:06-root-INFO: grad norm: 1.057 1.052 0.100
2024-12-01-23:56:07-root-INFO: grad norm: 0.819 0.818 0.043
2024-12-01-23:56:08-root-INFO: grad norm: 0.739 0.738 0.039
2024-12-01-23:56:09-root-INFO: grad norm: 0.742 0.741 0.044
2024-12-01-23:56:10-root-INFO: grad norm: 0.758 0.756 0.045
2024-12-01-23:56:11-root-INFO: grad norm: 0.758 0.756 0.046
2024-12-01-23:56:11-root-INFO: grad norm: 0.732 0.730 0.047
2024-12-01-23:56:12-root-INFO: grad norm: 0.717 0.715 0.047
2024-12-01-23:56:12-root-INFO: Loss Change: 1.874 -> 1.589
2024-12-01-23:56:12-root-INFO: Regularization Change: 0.000 -> 0.802
2024-12-01-23:56:12-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-01-23:56:12-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-23:56:13-root-INFO: step: 5 lr_xt 0.54931747
2024-12-01-23:56:13-root-INFO: grad norm: 0.902 0.897 0.094
2024-12-01-23:56:14-root-INFO: grad norm: 0.576 0.575 0.036
2024-12-01-23:56:14-root-INFO: grad norm: 0.490 0.489 0.030
2024-12-01-23:56:15-root-INFO: grad norm: 0.469 0.468 0.032
2024-12-01-23:56:16-root-INFO: grad norm: 0.461 0.460 0.030
2024-12-01-23:56:17-root-INFO: grad norm: 0.458 0.457 0.033
2024-12-01-23:56:17-root-INFO: grad norm: 0.460 0.459 0.030
2024-12-01-23:56:18-root-INFO: grad norm: 0.464 0.462 0.033
2024-12-01-23:56:18-root-INFO: Loss Change: 1.704 -> 1.451
2024-12-01-23:56:18-root-INFO: Regularization Change: 0.000 -> 0.659
2024-12-01-23:56:18-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-01-23:56:18-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-23:56:19-root-INFO: step: 4 lr_xt 0.55643055
2024-12-01-23:56:19-root-INFO: grad norm: 0.765 0.761 0.079
2024-12-01-23:56:19-root-INFO: grad norm: 0.396 0.395 0.030
2024-12-01-23:56:20-root-INFO: grad norm: 0.329 0.328 0.024
2024-12-01-23:56:20-root-INFO: grad norm: 0.345 0.344 0.025
2024-12-01-23:56:21-root-INFO: grad norm: 0.323 0.322 0.024
2024-12-01-23:56:22-root-INFO: grad norm: 0.319 0.318 0.024
2024-12-01-23:56:22-root-INFO: grad norm: 0.320 0.319 0.023
2024-12-01-23:56:23-root-INFO: grad norm: 0.328 0.327 0.024
2024-12-01-23:56:23-root-INFO: Loss Change: 1.582 -> 1.360
2024-12-01-23:56:23-root-INFO: Regularization Change: 0.000 -> 0.592
2024-12-01-23:56:23-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-01-23:56:23-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-23:56:24-root-INFO: step: 3 lr_xt 0.56354589
2024-12-01-23:56:24-root-INFO: grad norm: 0.724 0.721 0.068
2024-12-01-23:56:24-root-INFO: grad norm: 0.311 0.310 0.029
2024-12-01-23:56:25-root-INFO: grad norm: 0.256 0.255 0.024
2024-12-01-23:56:26-root-INFO: grad norm: 0.232 0.231 0.024
2024-12-01-23:56:26-root-INFO: grad norm: 0.239 0.238 0.022
2024-12-01-23:56:27-root-INFO: grad norm: 0.251 0.250 0.021
2024-12-01-23:56:27-root-INFO: grad norm: 0.266 0.265 0.021
2024-12-01-23:56:28-root-INFO: grad norm: 0.261 0.261 0.020
2024-12-01-23:56:28-root-INFO: Loss Change: 1.516 -> 1.297
2024-12-01-23:56:28-root-INFO: Regularization Change: 0.000 -> 0.589
2024-12-01-23:56:28-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-01-23:56:28-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-23:56:29-root-INFO: step: 2 lr_xt 0.57066124
2024-12-01-23:56:29-root-INFO: grad norm: 0.685 0.683 0.054
2024-12-01-23:56:29-root-INFO: grad norm: 0.401 0.400 0.030
2024-12-01-23:56:30-root-INFO: grad norm: 0.242 0.241 0.023
2024-12-01-23:56:31-root-INFO: grad norm: 0.178 0.177 0.023
2024-12-01-23:56:32-root-INFO: grad norm: 0.165 0.164 0.021
2024-12-01-23:56:32-root-INFO: grad norm: 0.202 0.201 0.020
2024-12-01-23:56:33-root-INFO: grad norm: 0.181 0.180 0.019
2024-12-01-23:56:34-root-INFO: grad norm: 0.163 0.162 0.018
2024-12-01-23:56:34-root-INFO: Loss Change: 1.449 -> 1.232
2024-12-01-23:56:34-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-01-23:56:34-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-01-23:56:34-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-23:56:34-root-INFO: step: 1 lr_xt 0.57777431
2024-12-01-23:56:35-root-INFO: grad norm: 0.768 0.765 0.060
2024-12-01-23:56:35-root-INFO: grad norm: 0.353 0.352 0.022
2024-12-01-23:56:36-root-INFO: grad norm: 0.278 0.277 0.018
2024-12-01-23:56:36-root-INFO: grad norm: 0.217 0.216 0.018
2024-12-01-23:56:37-root-INFO: grad norm: 0.205 0.204 0.017
2024-12-01-23:56:38-root-INFO: grad norm: 0.251 0.250 0.016
2024-12-01-23:56:38-root-INFO: grad norm: 0.189 0.189 0.015
2024-12-01-23:56:39-root-INFO: grad norm: 0.177 0.176 0.014
2024-12-01-23:56:40-root-INFO: Loss Change: 1.379 -> 1.139
2024-12-01-23:56:40-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-01-23:56:40-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-01-23:56:40-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-23:56:40-root-INFO: step: 0 lr_xt 0.58488282
2024-12-01-23:56:40-root-INFO: grad norm: 0.731 0.728 0.069
2024-12-01-23:56:41-root-INFO: grad norm: 0.410 0.409 0.022
2024-12-01-23:56:41-root-INFO: grad norm: 0.341 0.338 0.043
2024-12-01-23:56:42-root-INFO: grad norm: 0.315 0.311 0.050
2024-12-01-23:56:43-root-INFO: grad norm: 0.303 0.299 0.051
2024-12-01-23:56:43-root-INFO: grad norm: 0.296 0.292 0.050
2024-12-01-23:56:44-root-INFO: grad norm: 0.292 0.288 0.047
2024-12-01-23:56:44-root-INFO: grad norm: 0.287 0.284 0.045
2024-12-01-23:56:45-root-INFO: Loss Change: 1.283 -> 0.706
2024-12-01-23:56:45-root-INFO: Regularization Change: 0.000 -> 2.217
2024-12-01-23:56:45-root-INFO: Undo step: 0
2024-12-01-23:56:45-root-INFO: Undo step: 1
2024-12-01-23:56:45-root-INFO: Undo step: 2
2024-12-01-23:56:45-root-INFO: Undo step: 3
2024-12-01-23:56:45-root-INFO: Undo step: 4
2024-12-01-23:56:45-root-INFO: step: 5 lr_xt 0.54931747
2024-12-01-23:56:45-root-INFO: grad norm: 12.506 12.500 0.397
2024-12-01-23:56:46-root-INFO: grad norm: 6.732 6.729 0.221
2024-12-01-23:56:46-root-INFO: grad norm: 4.286 4.284 0.105
2024-12-01-23:56:47-root-INFO: grad norm: 2.294 2.293 0.079
2024-12-01-23:56:48-root-INFO: grad norm: 1.818 1.817 0.057
2024-12-01-23:56:48-root-INFO: grad norm: 1.194 1.192 0.062
2024-12-01-23:56:49-root-INFO: grad norm: 1.095 1.094 0.043
2024-12-01-23:56:49-root-INFO: grad norm: 1.544 1.543 0.054
2024-12-01-23:56:50-root-INFO: Loss Change: 38.130 -> 3.405
2024-12-01-23:56:50-root-INFO: Regularization Change: 0.000 -> 70.581
2024-12-01-23:56:50-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-01-23:56:50-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-23:56:50-root-INFO: step: 4 lr_xt 0.55643055
2024-12-01-23:56:50-root-INFO: grad norm: 1.229 1.226 0.087
2024-12-01-23:56:51-root-INFO: grad norm: 1.348 1.347 0.057
2024-12-01-23:56:51-root-INFO: grad norm: 0.846 0.845 0.040
2024-12-01-23:56:52-root-INFO: grad norm: 0.576 0.575 0.044
2024-12-01-23:56:52-root-INFO: grad norm: 0.572 0.571 0.037
2024-12-01-23:56:53-root-INFO: grad norm: 0.480 0.478 0.042
2024-12-01-23:56:54-root-INFO: grad norm: 0.480 0.478 0.035
2024-12-01-23:56:54-root-INFO: grad norm: 0.480 0.479 0.039
2024-12-01-23:56:55-root-INFO: Loss Change: 3.446 -> 2.462
2024-12-01-23:56:55-root-INFO: Regularization Change: 0.000 -> 3.190
2024-12-01-23:56:55-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-01-23:56:55-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-23:56:55-root-INFO: step: 3 lr_xt 0.56354589
2024-12-01-23:56:55-root-INFO: grad norm: 0.952 0.949 0.078
2024-12-01-23:56:56-root-INFO: grad norm: 0.679 0.678 0.048
2024-12-01-23:56:56-root-INFO: grad norm: 0.765 0.764 0.036
2024-12-01-23:56:57-root-INFO: grad norm: 1.114 1.113 0.047
2024-12-01-23:56:57-root-INFO: Loss too large (2.305->2.362)! Learning rate decreased to 0.45084.
2024-12-01-23:56:58-root-INFO: grad norm: 0.499 0.498 0.033
2024-12-01-23:56:58-root-INFO: grad norm: 1.639 1.638 0.038
2024-12-01-23:56:59-root-INFO: grad norm: 0.443 0.442 0.031
2024-12-01-23:57:00-root-INFO: grad norm: 1.334 1.333 0.062
2024-12-01-23:57:00-root-INFO: Loss too large (2.048->2.062)! Learning rate decreased to 0.36067.
2024-12-01-23:57:00-root-INFO: Loss Change: 2.605 -> 1.935
2024-12-01-23:57:00-root-INFO: Regularization Change: 0.000 -> 2.371
2024-12-01-23:57:00-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-01-23:57:00-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-23:57:00-root-INFO: step: 2 lr_xt 0.57066124
2024-12-01-23:57:01-root-INFO: grad norm: 0.742 0.739 0.058
2024-12-01-23:57:01-root-INFO: grad norm: 0.432 0.430 0.039
2024-12-01-23:57:02-root-INFO: grad norm: 0.440 0.439 0.032
2024-12-01-23:57:02-root-INFO: grad norm: 1.254 1.253 0.031
2024-12-01-23:57:03-root-INFO: Loss too large (1.761->1.871)! Learning rate decreased to 0.45653.
2024-12-01-23:57:03-root-INFO: Loss too large (1.761->1.846)! Learning rate decreased to 0.36522.
2024-12-01-23:57:03-root-INFO: grad norm: 1.113 1.113 0.029
2024-12-01-23:57:04-root-INFO: grad norm: 0.535 0.534 0.028
2024-12-01-23:57:05-root-INFO: grad norm: 0.200 0.198 0.027
2024-12-01-23:57:05-root-INFO: grad norm: 0.201 0.199 0.027
2024-12-01-23:57:06-root-INFO: Loss Change: 2.072 -> 1.627
2024-12-01-23:57:06-root-INFO: Regularization Change: 0.000 -> 0.874
2024-12-01-23:57:06-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-01-23:57:06-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-23:57:06-root-INFO: step: 1 lr_xt 0.57777431
2024-12-01-23:57:06-root-INFO: grad norm: 0.739 0.737 0.052
2024-12-01-23:57:07-root-INFO: grad norm: 1.077 1.076 0.032
2024-12-01-23:57:08-root-INFO: grad norm: 1.095 1.094 0.041
2024-12-01-23:57:08-root-INFO: grad norm: 1.496 1.496 0.026
2024-12-01-23:57:08-root-INFO: Loss too large (1.363->1.621)! Learning rate decreased to 0.46222.
2024-12-01-23:57:09-root-INFO: Loss too large (1.363->1.457)! Learning rate decreased to 0.36978.
2024-12-01-23:57:09-root-INFO: Loss too large (1.363->1.413)! Learning rate decreased to 0.29582.
2024-12-01-23:57:09-root-INFO: Loss too large (1.363->1.381)! Learning rate decreased to 0.23666.
2024-12-01-23:57:10-root-INFO: grad norm: 0.271 0.271 0.022
2024-12-01-23:57:10-root-INFO: grad norm: 0.267 0.266 0.022
2024-12-01-23:57:11-root-INFO: grad norm: 1.017 1.016 0.036
2024-12-01-23:57:12-root-INFO: grad norm: 0.171 0.170 0.015
2024-12-01-23:57:12-root-INFO: Loss Change: 1.763 -> 1.175
2024-12-01-23:57:12-root-INFO: Regularization Change: 0.000 -> 0.765
2024-12-01-23:57:12-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-01-23:57:12-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-23:57:12-root-INFO: loss_sample0_0: 1.1749898195266724
2024-12-01-23:57:12-root-INFO: It takes 4184.969 seconds for image sample0
2024-12-01-23:57:12-root-INFO: lpips_score_sample0: 0.157
2024-12-01-23:57:12-root-INFO: psnr_score_sample0: 17.133
2024-12-01-23:57:12-root-INFO: ssim_score_sample0: 0.694
2024-12-01-23:57:12-root-INFO: mean_lpips: 0.15697062015533447
2024-12-01-23:57:12-root-INFO: best_mean_lpips: 0.15697062015533447
2024-12-01-23:57:12-root-INFO: mean_psnr: 17.132761001586914
2024-12-01-23:57:12-root-INFO: best_mean_psnr: 17.132761001586914
2024-12-01-23:57:12-root-INFO: mean_ssim: 0.6943726539611816
2024-12-01-23:57:12-root-INFO: best_mean_ssim: 0.6943726539611816
2024-12-01-23:57:12-root-INFO: final_loss: 1.1749898195266724
2024-12-01-23:57:12-root-INFO: mean time: 4184.9692397117615
2024-12-01-23:57:12-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample2_iter8_lr0.03_10009 
 
Enjoy.
