2024-12-02-10:03:06-root-INFO: Prepare model...
2024-12-02-10:03:22-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-10:03:47-root-INFO: Start sampling
2024-12-02-10:03:53-root-INFO: step: 249 lr_xt 0.00019059
2024-12-02-10:03:53-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-10:03:53-root-INFO: Loss too large (77070.016->78612.711)! Learning rate decreased to 0.00015.
2024-12-02-10:03:53-root-INFO: grad norm: 15661.118 11248.847 10896.517
2024-12-02-10:03:54-root-INFO: grad norm: 14205.939 10378.375 9700.415
2024-12-02-10:03:54-root-INFO: grad norm: 15590.380 11542.534 10479.975
2024-12-02-10:03:54-root-INFO: Loss too large (28301.656->35851.266)! Learning rate decreased to 0.00012.
2024-12-02-10:03:55-root-INFO: grad norm: 16985.068 12431.671 11573.508
2024-12-02-10:03:55-root-INFO: Loss too large (28113.652->29013.715)! Learning rate decreased to 0.00010.
2024-12-02-10:03:56-root-INFO: grad norm: 13500.006 10439.545 8559.560
2024-12-02-10:03:56-root-INFO: grad norm: 13454.811 10629.021 8249.597
2024-12-02-10:03:56-root-INFO: Loss too large (22323.518->22381.537)! Learning rate decreased to 0.00008.
2024-12-02-10:03:57-root-INFO: grad norm: 9590.631 7721.713 5688.176
2024-12-02-10:03:57-root-INFO: Loss Change: 77070.016 -> 18637.420
2024-12-02-10:03:57-root-INFO: Regularization Change: 0.000 -> 18.364
2024-12-02-10:03:57-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-10:03:57-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-10:03:57-root-INFO: step: 248 lr_xt 0.00020082
2024-12-02-10:03:57-root-INFO: grad norm: 7521.934 5981.340 4561.038
2024-12-02-10:03:57-root-INFO: Loss too large (18708.215->30949.137)! Learning rate decreased to 0.00016.
2024-12-02-10:03:58-root-INFO: Loss too large (18708.215->24730.191)! Learning rate decreased to 0.00013.
2024-12-02-10:03:58-root-INFO: Loss too large (18708.215->21087.877)! Learning rate decreased to 0.00010.
2024-12-02-10:03:58-root-INFO: Loss too large (18708.215->19048.809)! Learning rate decreased to 0.00008.
2024-12-02-10:03:58-root-INFO: grad norm: 6005.946 4798.995 3611.237
2024-12-02-10:03:59-root-INFO: grad norm: 4839.737 3873.633 2901.383
2024-12-02-10:03:59-root-INFO: grad norm: 3853.663 3061.269 2340.802
2024-12-02-10:04:00-root-INFO: grad norm: 3089.307 2487.380 1832.146
2024-12-02-10:04:00-root-INFO: grad norm: 2467.766 1955.165 1505.722
2024-12-02-10:04:01-root-INFO: grad norm: 1998.080 1619.209 1170.679
2024-12-02-10:04:01-root-INFO: grad norm: 1631.250 1298.708 987.084
2024-12-02-10:04:01-root-INFO: Loss Change: 18708.215 -> 16490.857
2024-12-02-10:04:01-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-02-10:04:01-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-02-10:04:01-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-10:04:02-root-INFO: step: 247 lr_xt 0.00021156
2024-12-02-10:04:02-root-INFO: grad norm: 1318.062 1091.880 738.298
2024-12-02-10:04:02-root-INFO: Loss too large (16314.124->16376.085)! Learning rate decreased to 0.00017.
2024-12-02-10:04:02-root-INFO: grad norm: 2398.014 1882.190 1485.877
2024-12-02-10:04:03-root-INFO: Loss too large (16294.801->16718.371)! Learning rate decreased to 0.00014.
2024-12-02-10:04:03-root-INFO: Loss too large (16294.801->16415.959)! Learning rate decreased to 0.00011.
2024-12-02-10:04:03-root-INFO: grad norm: 2588.133 2070.939 1552.303
2024-12-02-10:04:04-root-INFO: grad norm: 2841.844 2267.058 1713.628
2024-12-02-10:04:04-root-INFO: grad norm: 3134.353 2503.666 1885.689
2024-12-02-10:04:05-root-INFO: grad norm: 3487.479 2802.920 2075.126
2024-12-02-10:04:05-root-INFO: Loss too large (16179.975->16200.606)! Learning rate decreased to 0.00009.
2024-12-02-10:04:05-root-INFO: grad norm: 2509.017 2006.140 1506.841
2024-12-02-10:04:06-root-INFO: grad norm: 1822.911 1496.103 1041.480
2024-12-02-10:04:06-root-INFO: Loss Change: 16314.124 -> 15773.592
2024-12-02-10:04:06-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-02-10:04:06-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-02-10:04:06-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-10:04:06-root-INFO: step: 246 lr_xt 0.00022285
2024-12-02-10:04:06-root-INFO: grad norm: 1388.071 1137.213 795.919
2024-12-02-10:04:07-root-INFO: Loss too large (15552.627->15703.460)! Learning rate decreased to 0.00018.
2024-12-02-10:04:07-root-INFO: Loss too large (15552.627->15579.438)! Learning rate decreased to 0.00014.
2024-12-02-10:04:07-root-INFO: grad norm: 1924.383 1584.416 1092.188
2024-12-02-10:04:07-root-INFO: Loss too large (15514.298->15563.257)! Learning rate decreased to 0.00011.
2024-12-02-10:04:08-root-INFO: grad norm: 2028.994 1629.561 1208.861
2024-12-02-10:04:08-root-INFO: grad norm: 2161.804 1787.511 1215.813
2024-12-02-10:04:09-root-INFO: grad norm: 2301.848 1840.351 1382.610
2024-12-02-10:04:09-root-INFO: grad norm: 2461.148 2033.141 1386.935
2024-12-02-10:04:10-root-INFO: grad norm: 2626.188 2095.692 1582.700
2024-12-02-10:04:10-root-INFO: grad norm: 2813.816 2318.972 1593.716
2024-12-02-10:04:10-root-INFO: Loss Change: 15552.627 -> 15230.150
2024-12-02-10:04:10-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-10:04:10-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-02-10:04:10-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-10:04:11-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-10:04:11-root-INFO: grad norm: 2958.335 2359.333 1784.739
2024-12-02-10:04:11-root-INFO: Loss too large (15122.857->16865.672)! Learning rate decreased to 0.00019.
2024-12-02-10:04:11-root-INFO: Loss too large (15122.857->15898.820)! Learning rate decreased to 0.00015.
2024-12-02-10:04:11-root-INFO: Loss too large (15122.857->15349.479)! Learning rate decreased to 0.00012.
2024-12-02-10:04:12-root-INFO: grad norm: 3003.884 2476.472 1700.119
2024-12-02-10:04:12-root-INFO: grad norm: 3064.028 2445.856 1845.550
2024-12-02-10:04:13-root-INFO: grad norm: 3136.981 2590.660 1768.934
2024-12-02-10:04:13-root-INFO: grad norm: 3208.415 2562.299 1930.945
2024-12-02-10:04:14-root-INFO: grad norm: 3285.589 2712.854 1853.515
2024-12-02-10:04:14-root-INFO: grad norm: 3359.913 2685.269 2019.493
2024-12-02-10:04:15-root-INFO: grad norm: 3439.715 2837.767 1943.893
2024-12-02-10:04:15-root-INFO: Loss Change: 15122.857 -> 14724.138
2024-12-02-10:04:15-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-10:04:15-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-10:04:15-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-10:04:15-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-10:04:15-root-INFO: grad norm: 3397.965 2753.456 1991.142
2024-12-02-10:04:15-root-INFO: Loss too large (14595.509->16827.936)! Learning rate decreased to 0.00020.
2024-12-02-10:04:16-root-INFO: Loss too large (14595.509->15548.236)! Learning rate decreased to 0.00016.
2024-12-02-10:04:16-root-INFO: Loss too large (14595.509->14826.454)! Learning rate decreased to 0.00013.
2024-12-02-10:04:16-root-INFO: grad norm: 3217.506 2667.246 1799.484
2024-12-02-10:04:17-root-INFO: grad norm: 3128.829 2531.085 1839.342
2024-12-02-10:04:17-root-INFO: grad norm: 3063.345 2556.156 1688.240
2024-12-02-10:04:18-root-INFO: grad norm: 3013.389 2433.298 1777.519
2024-12-02-10:04:18-root-INFO: grad norm: 2969.298 2482.512 1629.068
2024-12-02-10:04:18-root-INFO: grad norm: 2930.804 2366.085 1729.525
2024-12-02-10:04:19-root-INFO: grad norm: 2892.086 2419.660 1584.109
2024-12-02-10:04:19-root-INFO: Loss Change: 14595.509 -> 13887.652
2024-12-02-10:04:19-root-INFO: Regularization Change: 0.000 -> 0.588
2024-12-02-10:04:19-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-10:04:19-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:19-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-10:04:19-root-INFO: grad norm: 2945.544 2366.826 1753.386
2024-12-02-10:04:20-root-INFO: Loss too large (13796.012->15472.465)! Learning rate decreased to 0.00021.
2024-12-02-10:04:20-root-INFO: Loss too large (13796.012->14497.629)! Learning rate decreased to 0.00017.
2024-12-02-10:04:20-root-INFO: Loss too large (13796.012->13948.940)! Learning rate decreased to 0.00013.
2024-12-02-10:04:20-root-INFO: grad norm: 2747.067 2306.488 1492.143
2024-12-02-10:04:21-root-INFO: grad norm: 2608.201 2105.126 1539.856
2024-12-02-10:04:21-root-INFO: grad norm: 2483.966 2097.658 1330.382
2024-12-02-10:04:22-root-INFO: grad norm: 2377.076 1922.795 1397.622
2024-12-02-10:04:22-root-INFO: grad norm: 2275.876 1927.143 1210.674
2024-12-02-10:04:23-root-INFO: grad norm: 2187.504 1773.670 1280.339
2024-12-02-10:04:23-root-INFO: grad norm: 2102.109 1783.406 1112.801
2024-12-02-10:04:24-root-INFO: Loss Change: 13796.012 -> 13043.030
2024-12-02-10:04:24-root-INFO: Regularization Change: 0.000 -> 0.626
2024-12-02-10:04:24-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-10:04:24-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:24-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-10:04:24-root-INFO: grad norm: 1942.859 1606.320 1092.904
2024-12-02-10:04:24-root-INFO: Loss too large (12825.633->13417.292)! Learning rate decreased to 0.00022.
2024-12-02-10:04:24-root-INFO: Loss too large (12825.633->13036.045)! Learning rate decreased to 0.00018.
2024-12-02-10:04:24-root-INFO: Loss too large (12825.633->12826.250)! Learning rate decreased to 0.00014.
2024-12-02-10:04:25-root-INFO: grad norm: 1777.314 1511.077 935.676
2024-12-02-10:04:25-root-INFO: grad norm: 1649.931 1354.767 941.742
2024-12-02-10:04:26-root-INFO: grad norm: 1541.076 1326.866 783.799
2024-12-02-10:04:26-root-INFO: grad norm: 1446.835 1189.649 823.447
2024-12-02-10:04:27-root-INFO: grad norm: 1362.782 1181.555 679.046
2024-12-02-10:04:27-root-INFO: grad norm: 1287.304 1064.023 724.574
2024-12-02-10:04:28-root-INFO: grad norm: 1217.491 1061.756 595.784
2024-12-02-10:04:28-root-INFO: Loss Change: 12825.633 -> 12147.589
2024-12-02-10:04:28-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-10:04:28-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-10:04:28-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:28-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-10:04:28-root-INFO: grad norm: 1090.484 929.458 570.318
2024-12-02-10:04:28-root-INFO: Loss too large (12037.793->12048.074)! Learning rate decreased to 0.00023.
2024-12-02-10:04:29-root-INFO: grad norm: 1720.329 1472.995 888.715
2024-12-02-10:04:29-root-INFO: Loss too large (11989.396->12138.494)! Learning rate decreased to 0.00018.
2024-12-02-10:04:30-root-INFO: grad norm: 2222.133 1845.883 1237.170
2024-12-02-10:04:30-root-INFO: Loss too large (11976.660->12027.329)! Learning rate decreased to 0.00015.
2024-12-02-10:04:30-root-INFO: grad norm: 1986.586 1697.130 1032.605
2024-12-02-10:04:31-root-INFO: grad norm: 1787.571 1490.159 987.339
2024-12-02-10:04:31-root-INFO: grad norm: 1609.921 1387.227 816.973
2024-12-02-10:04:32-root-INFO: grad norm: 1457.280 1221.387 794.907
2024-12-02-10:04:32-root-INFO: grad norm: 1323.924 1152.449 651.642
2024-12-02-10:04:32-root-INFO: Loss Change: 12037.793 -> 11410.057
2024-12-02-10:04:32-root-INFO: Regularization Change: 0.000 -> 0.833
2024-12-02-10:04:32-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-10:04:32-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:33-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-10:04:33-root-INFO: grad norm: 1252.529 1079.470 635.275
2024-12-02-10:04:33-root-INFO: Loss too large (11225.404->11306.396)! Learning rate decreased to 0.00024.
2024-12-02-10:04:33-root-INFO: grad norm: 1998.674 1717.766 1021.752
2024-12-02-10:04:34-root-INFO: Loss too large (11200.316->11459.764)! Learning rate decreased to 0.00019.
2024-12-02-10:04:34-root-INFO: Loss too large (11200.316->11212.840)! Learning rate decreased to 0.00016.
2024-12-02-10:04:34-root-INFO: grad norm: 1721.466 1448.684 929.925
2024-12-02-10:04:35-root-INFO: grad norm: 1492.521 1298.601 735.699
2024-12-02-10:04:35-root-INFO: grad norm: 1310.062 1110.647 694.785
2024-12-02-10:04:36-root-INFO: grad norm: 1159.736 1022.221 547.770
2024-12-02-10:04:36-root-INFO: grad norm: 1038.998 891.853 533.025
2024-12-02-10:04:37-root-INFO: grad norm: 941.055 841.366 421.530
2024-12-02-10:04:37-root-INFO: Loss Change: 11225.404 -> 10576.644
2024-12-02-10:04:37-root-INFO: Regularization Change: 0.000 -> 0.818
2024-12-02-10:04:37-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-10:04:37-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:37-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-10:04:37-root-INFO: grad norm: 942.859 810.551 481.654
2024-12-02-10:04:38-root-INFO: grad norm: 1769.562 1504.629 931.367
2024-12-02-10:04:38-root-INFO: Loss too large (10456.129->10955.937)! Learning rate decreased to 0.00026.
2024-12-02-10:04:38-root-INFO: Loss too large (10456.129->10619.553)! Learning rate decreased to 0.00020.
2024-12-02-10:04:38-root-INFO: grad norm: 2115.005 1775.799 1148.818
2024-12-02-10:04:39-root-INFO: Loss too large (10434.150->10445.918)! Learning rate decreased to 0.00016.
2024-12-02-10:04:39-root-INFO: grad norm: 1711.489 1473.993 869.792
2024-12-02-10:04:39-root-INFO: grad norm: 1411.165 1199.725 742.997
2024-12-02-10:04:40-root-INFO: grad norm: 1177.291 1035.501 560.135
2024-12-02-10:04:40-root-INFO: grad norm: 1002.335 865.303 505.892
2024-12-02-10:04:41-root-INFO: grad norm: 870.584 783.762 378.991
2024-12-02-10:04:41-root-INFO: Loss Change: 10491.039 -> 9879.990
2024-12-02-10:04:41-root-INFO: Regularization Change: 0.000 -> 0.909
2024-12-02-10:04:41-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-10:04:41-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:42-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-10:04:42-root-INFO: grad norm: 1033.184 903.892 500.447
2024-12-02-10:04:42-root-INFO: grad norm: 1871.753 1622.962 932.446
2024-12-02-10:04:42-root-INFO: Loss too large (9677.869->10285.621)! Learning rate decreased to 0.00027.
2024-12-02-10:04:43-root-INFO: Loss too large (9677.869->9884.401)! Learning rate decreased to 0.00021.
2024-12-02-10:04:43-root-INFO: grad norm: 2132.609 1808.118 1130.809
2024-12-02-10:04:43-root-INFO: Loss too large (9661.564->9666.134)! Learning rate decreased to 0.00017.
2024-12-02-10:04:44-root-INFO: grad norm: 1611.856 1413.797 774.117
2024-12-02-10:04:44-root-INFO: grad norm: 1258.487 1081.174 644.091
2024-12-02-10:04:45-root-INFO: grad norm: 1004.922 902.275 442.455
2024-12-02-10:04:45-root-INFO: grad norm: 831.537 728.672 400.613
2024-12-02-10:04:46-root-INFO: grad norm: 712.650 657.188 275.633
2024-12-02-10:04:46-root-INFO: Loss Change: 9688.593 -> 9148.822
2024-12-02-10:04:46-root-INFO: Regularization Change: 0.000 -> 0.800
2024-12-02-10:04:46-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-10:04:46-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:46-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-10:04:46-root-INFO: grad norm: 834.754 726.645 410.854
2024-12-02-10:04:47-root-INFO: grad norm: 1542.636 1356.810 734.024
2024-12-02-10:04:47-root-INFO: Loss too large (9085.827->9481.225)! Learning rate decreased to 0.00028.
2024-12-02-10:04:47-root-INFO: Loss too large (9085.827->9207.477)! Learning rate decreased to 0.00023.
2024-12-02-10:04:48-root-INFO: grad norm: 1706.561 1463.796 877.299
2024-12-02-10:04:48-root-INFO: grad norm: 1902.049 1668.073 913.960
2024-12-02-10:04:49-root-INFO: grad norm: 2114.666 1818.122 1079.928
2024-12-02-10:04:49-root-INFO: grad norm: 2351.851 2055.547 1142.773
2024-12-02-10:04:49-root-INFO: Loss too large (9028.774->9049.720)! Learning rate decreased to 0.00018.
2024-12-02-10:04:50-root-INFO: grad norm: 1671.775 1439.831 849.539
2024-12-02-10:04:50-root-INFO: grad norm: 1202.812 1071.192 547.088
2024-12-02-10:04:51-root-INFO: Loss Change: 9095.519 -> 8655.846
2024-12-02-10:04:51-root-INFO: Regularization Change: 0.000 -> 0.810
2024-12-02-10:04:51-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-10:04:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:51-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-10:04:51-root-INFO: grad norm: 919.316 811.043 432.841
2024-12-02-10:04:51-root-INFO: Loss too large (8522.978->8578.549)! Learning rate decreased to 0.00030.
2024-12-02-10:04:52-root-INFO: grad norm: 1330.197 1174.792 623.931
2024-12-02-10:04:52-root-INFO: Loss too large (8508.370->8581.213)! Learning rate decreased to 0.00024.
2024-12-02-10:04:52-root-INFO: grad norm: 1413.625 1224.716 705.980
2024-12-02-10:04:53-root-INFO: grad norm: 1502.659 1325.010 708.755
2024-12-02-10:04:53-root-INFO: grad norm: 1593.969 1378.255 800.717
2024-12-02-10:04:54-root-INFO: grad norm: 1690.273 1486.516 804.545
2024-12-02-10:04:54-root-INFO: grad norm: 1783.663 1540.450 899.149
2024-12-02-10:04:55-root-INFO: grad norm: 1877.638 1647.729 900.285
2024-12-02-10:04:55-root-INFO: Loss Change: 8522.978 -> 8304.064
2024-12-02-10:04:55-root-INFO: Regularization Change: 0.000 -> 0.807
2024-12-02-10:04:55-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-10:04:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:04:55-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-10:04:55-root-INFO: grad norm: 2203.908 1914.183 1092.297
2024-12-02-10:04:56-root-INFO: Loss too large (8300.764->9255.553)! Learning rate decreased to 0.00031.
2024-12-02-10:04:56-root-INFO: Loss too large (8300.764->8603.885)! Learning rate decreased to 0.00025.
2024-12-02-10:04:56-root-INFO: grad norm: 2205.253 1945.422 1038.495
2024-12-02-10:04:57-root-INFO: grad norm: 2206.639 1916.444 1093.846
2024-12-02-10:04:57-root-INFO: grad norm: 2206.173 1946.205 1038.984
2024-12-02-10:04:58-root-INFO: grad norm: 2201.934 1913.328 1089.813
2024-12-02-10:04:58-root-INFO: grad norm: 2193.963 1934.625 1034.746
2024-12-02-10:04:59-root-INFO: grad norm: 2180.417 1895.336 1077.923
2024-12-02-10:04:59-root-INFO: grad norm: 2162.433 1906.584 1020.321
2024-12-02-10:04:59-root-INFO: Loss Change: 8300.764 -> 7960.797
2024-12-02-10:04:59-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-10:04:59-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-10:04:59-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-10:05:00-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-10:05:00-root-INFO: grad norm: 2123.579 1851.533 1039.911
2024-12-02-10:05:00-root-INFO: Loss too large (7912.605->8782.459)! Learning rate decreased to 0.00033.
2024-12-02-10:05:00-root-INFO: Loss too large (7912.605->8169.846)! Learning rate decreased to 0.00026.
2024-12-02-10:05:01-root-INFO: grad norm: 2020.097 1783.048 949.492
2024-12-02-10:05:01-root-INFO: grad norm: 1923.632 1677.536 941.401
2024-12-02-10:05:02-root-INFO: grad norm: 1826.980 1615.780 852.707
2024-12-02-10:05:02-root-INFO: grad norm: 1736.508 1514.300 849.916
2024-12-02-10:05:03-root-INFO: grad norm: 1648.517 1460.259 765.019
2024-12-02-10:05:03-root-INFO: grad norm: 1567.544 1367.209 766.767
2024-12-02-10:05:04-root-INFO: grad norm: 1488.691 1320.773 686.849
2024-12-02-10:05:04-root-INFO: Loss Change: 7912.605 -> 7455.579
2024-12-02-10:05:04-root-INFO: Regularization Change: 0.000 -> 0.619
2024-12-02-10:05:04-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-10:05:04-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:05:04-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-10:05:04-root-INFO: grad norm: 1552.574 1367.781 734.617
2024-12-02-10:05:05-root-INFO: Loss too large (7464.422->7859.541)! Learning rate decreased to 0.00035.
2024-12-02-10:05:05-root-INFO: Loss too large (7464.422->7547.907)! Learning rate decreased to 0.00028.
2024-12-02-10:05:05-root-INFO: grad norm: 1385.315 1230.876 635.645
2024-12-02-10:05:06-root-INFO: grad norm: 1253.302 1100.079 600.493
2024-12-02-10:05:06-root-INFO: grad norm: 1135.905 1014.991 509.975
2024-12-02-10:05:07-root-INFO: grad norm: 1036.036 910.110 495.046
2024-12-02-10:05:07-root-INFO: grad norm: 945.170 848.190 417.038
2024-12-02-10:05:08-root-INFO: grad norm: 865.808 761.938 411.186
2024-12-02-10:05:08-root-INFO: grad norm: 792.999 715.277 342.382
2024-12-02-10:05:08-root-INFO: Loss Change: 7464.422 -> 7060.485
2024-12-02-10:05:08-root-INFO: Regularization Change: 0.000 -> 0.601
2024-12-02-10:05:08-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-10:05:08-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:05:09-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-10:05:09-root-INFO: grad norm: 1169.333 1013.927 582.487
2024-12-02-10:05:09-root-INFO: Loss too large (7004.179->7190.067)! Learning rate decreased to 0.00036.
2024-12-02-10:05:09-root-INFO: Loss too large (7004.179->7022.120)! Learning rate decreased to 0.00029.
2024-12-02-10:05:10-root-INFO: grad norm: 1000.929 905.644 426.225
2024-12-02-10:05:10-root-INFO: grad norm: 887.651 780.195 423.344
2024-12-02-10:05:10-root-INFO: grad norm: 792.576 720.974 329.202
2024-12-02-10:05:11-root-INFO: grad norm: 714.174 632.394 331.847
2024-12-02-10:05:11-root-INFO: grad norm: 646.598 590.787 262.792
2024-12-02-10:05:12-root-INFO: grad norm: 589.050 524.755 267.605
2024-12-02-10:05:12-root-INFO: grad norm: 539.160 495.372 212.836
2024-12-02-10:05:13-root-INFO: Loss Change: 7004.179 -> 6685.027
2024-12-02-10:05:13-root-INFO: Regularization Change: 0.000 -> 0.545
2024-12-02-10:05:13-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-10:05:13-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:05:13-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-10:05:13-root-INFO: grad norm: 634.633 564.882 289.254
2024-12-02-10:05:13-root-INFO: Loss too large (6641.482->6669.127)! Learning rate decreased to 0.00038.
2024-12-02-10:05:13-root-INFO: grad norm: 798.186 723.327 337.488
2024-12-02-10:05:14-root-INFO: Loss too large (6628.451->6632.108)! Learning rate decreased to 0.00030.
2024-12-02-10:05:14-root-INFO: grad norm: 701.714 625.006 319.014
2024-12-02-10:05:14-root-INFO: grad norm: 620.041 566.535 251.970
2024-12-02-10:05:15-root-INFO: grad norm: 551.385 493.015 246.905
2024-12-02-10:05:15-root-INFO: grad norm: 493.812 455.013 191.867
2024-12-02-10:05:16-root-INFO: grad norm: 446.348 401.816 194.345
2024-12-02-10:05:16-root-INFO: grad norm: 406.596 377.955 149.900
2024-12-02-10:05:17-root-INFO: Loss Change: 6641.482 -> 6424.506
2024-12-02-10:05:17-root-INFO: Regularization Change: 0.000 -> 0.498
2024-12-02-10:05:17-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-10:05:17-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:05:17-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-10:05:17-root-INFO: grad norm: 593.029 524.313 277.091
2024-12-02-10:05:17-root-INFO: Loss too large (6363.676->6386.301)! Learning rate decreased to 0.00040.
2024-12-02-10:05:18-root-INFO: grad norm: 727.836 663.390 299.432
2024-12-02-10:05:18-root-INFO: grad norm: 911.044 812.417 412.286
2024-12-02-10:05:18-root-INFO: Loss too large (6348.751->6365.141)! Learning rate decreased to 0.00032.
2024-12-02-10:05:19-root-INFO: grad norm: 771.480 700.933 322.294
2024-12-02-10:05:19-root-INFO: grad norm: 660.997 591.421 295.191
2024-12-02-10:05:20-root-INFO: grad norm: 570.880 521.756 231.679
2024-12-02-10:05:20-root-INFO: grad norm: 498.088 448.003 217.681
2024-12-02-10:05:21-root-INFO: grad norm: 438.745 404.131 170.806
2024-12-02-10:05:21-root-INFO: Loss Change: 6363.676 -> 6163.516
2024-12-02-10:05:21-root-INFO: Regularization Change: 0.000 -> 0.502
2024-12-02-10:05:21-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-10:05:21-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:05:21-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-10:05:21-root-INFO: grad norm: 684.477 616.087 298.237
2024-12-02-10:05:22-root-INFO: Loss too large (6141.358->6183.847)! Learning rate decreased to 0.00042.
2024-12-02-10:05:22-root-INFO: grad norm: 831.959 758.830 341.076
2024-12-02-10:05:22-root-INFO: Loss too large (6128.576->6138.104)! Learning rate decreased to 0.00034.
2024-12-02-10:05:23-root-INFO: grad norm: 704.158 634.109 306.176
2024-12-02-10:05:23-root-INFO: grad norm: 599.034 547.790 242.421
2024-12-02-10:05:24-root-INFO: grad norm: 516.029 466.719 220.135
2024-12-02-10:05:24-root-INFO: grad norm: 449.606 413.116 177.428
2024-12-02-10:05:24-root-INFO: grad norm: 397.876 362.118 164.852
2024-12-02-10:05:25-root-INFO: grad norm: 356.485 329.543 135.950
2024-12-02-10:05:25-root-INFO: Loss Change: 6141.358 -> 5931.848
2024-12-02-10:05:25-root-INFO: Regularization Change: 0.000 -> 0.494
2024-12-02-10:05:25-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-10:05:25-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:05:25-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-10:05:26-root-INFO: grad norm: 494.614 435.455 234.566
2024-12-02-10:05:26-root-INFO: Loss too large (5907.729->5911.867)! Learning rate decreased to 0.00044.
2024-12-02-10:05:26-root-INFO: grad norm: 559.608 507.449 235.917
2024-12-02-10:05:27-root-INFO: grad norm: 670.272 605.819 286.789
2024-12-02-10:05:27-root-INFO: grad norm: 827.087 752.891 342.386
2024-12-02-10:05:27-root-INFO: Loss too large (5876.561->5891.337)! Learning rate decreased to 0.00035.
2024-12-02-10:05:28-root-INFO: grad norm: 686.796 622.560 290.013
2024-12-02-10:05:28-root-INFO: grad norm: 574.361 525.335 232.192
2024-12-02-10:05:29-root-INFO: grad norm: 488.107 443.742 203.326
2024-12-02-10:05:29-root-INFO: grad norm: 419.555 386.004 164.398
2024-12-02-10:05:29-root-INFO: Loss Change: 5907.729 -> 5733.072
2024-12-02-10:05:29-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-02-10:05:30-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-10:05:30-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:05:30-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-10:05:30-root-INFO: grad norm: 760.315 687.459 324.775
2024-12-02-10:05:30-root-INFO: Loss too large (5775.712->5841.130)! Learning rate decreased to 0.00046.
2024-12-02-10:05:30-root-INFO: grad norm: 902.253 824.733 365.889
2024-12-02-10:05:31-root-INFO: Loss too large (5763.354->5778.754)! Learning rate decreased to 0.00037.
2024-12-02-10:05:31-root-INFO: grad norm: 735.029 669.874 302.549
2024-12-02-10:05:32-root-INFO: grad norm: 605.149 554.389 242.607
2024-12-02-10:05:32-root-INFO: grad norm: 507.573 464.116 205.490
2024-12-02-10:05:32-root-INFO: grad norm: 431.119 396.234 169.888
2024-12-02-10:05:33-root-INFO: grad norm: 373.877 343.774 146.981
2024-12-02-10:05:34-root-INFO: grad norm: 329.964 304.883 126.184
2024-12-02-10:05:34-root-INFO: Loss Change: 5775.712 -> 5553.492
2024-12-02-10:05:34-root-INFO: Regularization Change: 0.000 -> 0.531
2024-12-02-10:05:34-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-10:05:34-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-10:05:34-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-10:05:34-root-INFO: grad norm: 374.290 321.644 191.410
2024-12-02-10:05:35-root-INFO: grad norm: 452.626 402.009 207.988
2024-12-02-10:05:35-root-INFO: Loss too large (5485.018->5485.401)! Learning rate decreased to 0.00049.
2024-12-02-10:05:35-root-INFO: grad norm: 494.626 445.419 215.074
2024-12-02-10:05:36-root-INFO: grad norm: 580.681 530.557 236.007
2024-12-02-10:05:36-root-INFO: grad norm: 696.843 634.755 287.535
2024-12-02-10:05:36-root-INFO: Loss too large (5447.727->5451.432)! Learning rate decreased to 0.00039.
2024-12-02-10:05:37-root-INFO: grad norm: 563.441 517.108 223.752
2024-12-02-10:05:37-root-INFO: grad norm: 463.761 423.886 188.137
2024-12-02-10:05:38-root-INFO: grad norm: 387.907 357.995 149.368
2024-12-02-10:05:38-root-INFO: Loss Change: 5510.404 -> 5340.300
2024-12-02-10:05:38-root-INFO: Regularization Change: 0.000 -> 0.602
2024-12-02-10:05:38-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-10:05:38-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:05:38-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-10:05:39-root-INFO: grad norm: 710.993 647.140 294.483
2024-12-02-10:05:39-root-INFO: Loss too large (5337.027->5404.449)! Learning rate decreased to 0.00051.
2024-12-02-10:05:39-root-INFO: grad norm: 843.931 776.535 330.475
2024-12-02-10:05:39-root-INFO: Loss too large (5327.413->5346.144)! Learning rate decreased to 0.00041.
2024-12-02-10:05:40-root-INFO: grad norm: 686.561 629.070 275.022
2024-12-02-10:05:40-root-INFO: grad norm: 561.259 517.216 217.944
2024-12-02-10:05:41-root-INFO: grad norm: 467.658 429.632 184.717
2024-12-02-10:05:41-root-INFO: grad norm: 394.625 364.426 151.402
2024-12-02-10:05:42-root-INFO: grad norm: 339.839 313.478 131.232
2024-12-02-10:05:42-root-INFO: grad norm: 298.459 276.628 112.048
2024-12-02-10:05:43-root-INFO: Loss Change: 5337.027 -> 5136.844
2024-12-02-10:05:43-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-10:05:43-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-10:05:43-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:05:43-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-10:05:43-root-INFO: grad norm: 485.962 440.006 206.287
2024-12-02-10:05:43-root-INFO: Loss too large (5130.720->5142.415)! Learning rate decreased to 0.00054.
2024-12-02-10:05:44-root-INFO: grad norm: 564.570 520.153 219.499
2024-12-02-10:05:44-root-INFO: grad norm: 697.816 644.152 268.357
2024-12-02-10:05:44-root-INFO: Loss too large (5110.500->5122.774)! Learning rate decreased to 0.00043.
2024-12-02-10:05:45-root-INFO: grad norm: 585.488 540.973 223.931
2024-12-02-10:05:45-root-INFO: grad norm: 497.458 460.133 189.055
2024-12-02-10:05:46-root-INFO: grad norm: 425.930 394.219 161.267
2024-12-02-10:05:46-root-INFO: grad norm: 369.710 342.936 138.132
2024-12-02-10:05:47-root-INFO: grad norm: 324.899 301.467 121.150
2024-12-02-10:05:47-root-INFO: Loss Change: 5130.720 -> 4969.329
2024-12-02-10:05:47-root-INFO: Regularization Change: 0.000 -> 0.522
2024-12-02-10:05:47-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-10:05:47-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:05:47-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-10:05:47-root-INFO: grad norm: 536.705 493.952 209.914
2024-12-02-10:05:47-root-INFO: Loss too large (4961.770->5001.406)! Learning rate decreased to 0.00056.
2024-12-02-10:05:48-root-INFO: grad norm: 658.677 609.711 249.214
2024-12-02-10:05:48-root-INFO: Loss too large (4954.389->4966.020)! Learning rate decreased to 0.00045.
2024-12-02-10:05:49-root-INFO: grad norm: 558.888 517.982 209.884
2024-12-02-10:05:49-root-INFO: grad norm: 476.916 442.256 178.488
2024-12-02-10:05:50-root-INFO: grad norm: 412.320 383.250 152.074
2024-12-02-10:05:50-root-INFO: grad norm: 359.584 333.964 133.297
2024-12-02-10:05:51-root-INFO: grad norm: 317.700 296.255 114.746
2024-12-02-10:05:51-root-INFO: grad norm: 284.380 264.755 103.811
2024-12-02-10:05:51-root-INFO: Loss Change: 4961.770 -> 4803.350
2024-12-02-10:05:51-root-INFO: Regularization Change: 0.000 -> 0.491
2024-12-02-10:05:51-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-10:05:51-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:05:52-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-10:05:52-root-INFO: grad norm: 326.921 304.154 119.864
2024-12-02-10:05:52-root-INFO: grad norm: 525.148 487.533 195.173
2024-12-02-10:05:52-root-INFO: Loss too large (4743.532->4797.696)! Learning rate decreased to 0.00059.
2024-12-02-10:05:53-root-INFO: Loss too large (4743.532->4745.648)! Learning rate decreased to 0.00047.
2024-12-02-10:05:53-root-INFO: grad norm: 443.537 413.624 160.125
2024-12-02-10:05:54-root-INFO: grad norm: 379.455 352.790 139.734
2024-12-02-10:05:54-root-INFO: grad norm: 328.841 307.551 116.399
2024-12-02-10:05:55-root-INFO: grad norm: 288.798 269.076 104.892
2024-12-02-10:05:55-root-INFO: grad norm: 257.862 241.921 89.257
2024-12-02-10:05:55-root-INFO: grad norm: 233.881 218.506 83.401
2024-12-02-10:05:56-root-INFO: Loss Change: 4745.158 -> 4617.276
2024-12-02-10:05:56-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-10:05:56-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-10:05:56-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:05:56-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-10:05:56-root-INFO: grad norm: 320.671 297.350 120.054
2024-12-02-10:05:57-root-INFO: grad norm: 508.876 472.935 187.848
2024-12-02-10:05:57-root-INFO: Loss too large (4583.045->4634.929)! Learning rate decreased to 0.00062.
2024-12-02-10:05:57-root-INFO: Loss too large (4583.045->4584.269)! Learning rate decreased to 0.00050.
2024-12-02-10:05:57-root-INFO: grad norm: 420.224 391.618 152.392
2024-12-02-10:05:58-root-INFO: grad norm: 352.523 328.796 127.142
2024-12-02-10:05:58-root-INFO: grad norm: 300.659 281.154 106.530
2024-12-02-10:05:59-root-INFO: grad norm: 261.191 244.219 92.618
2024-12-02-10:05:59-root-INFO: grad norm: 231.568 217.489 79.514
2024-12-02-10:06:00-root-INFO: grad norm: 209.313 196.373 72.452
2024-12-02-10:06:00-root-INFO: Loss Change: 4583.739 -> 4462.523
2024-12-02-10:06:00-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-02-10:06:00-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-10:06:00-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:06:00-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-10:06:01-root-INFO: grad norm: 492.183 451.681 195.522
2024-12-02-10:06:01-root-INFO: Loss too large (4467.199->4497.187)! Learning rate decreased to 0.00065.
2024-12-02-10:06:01-root-INFO: grad norm: 582.554 542.405 212.521
2024-12-02-10:06:01-root-INFO: Loss too large (4454.664->4462.373)! Learning rate decreased to 0.00052.
2024-12-02-10:06:02-root-INFO: grad norm: 484.629 451.207 176.856
2024-12-02-10:06:02-root-INFO: grad norm: 406.133 379.122 145.637
2024-12-02-10:06:03-root-INFO: grad norm: 345.484 323.010 122.570
2024-12-02-10:06:03-root-INFO: grad norm: 297.760 278.438 105.515
2024-12-02-10:06:04-root-INFO: grad norm: 260.355 244.522 89.407
2024-12-02-10:06:04-root-INFO: grad norm: 231.047 216.666 80.240
2024-12-02-10:06:05-root-INFO: Loss Change: 4467.199 -> 4320.739
2024-12-02-10:06:05-root-INFO: Regularization Change: 0.000 -> 0.507
2024-12-02-10:06:05-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-10:06:05-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:06:05-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-10:06:05-root-INFO: grad norm: 229.138 214.036 81.808
2024-12-02-10:06:06-root-INFO: grad norm: 319.724 299.715 111.331
2024-12-02-10:06:06-root-INFO: Loss too large (4286.950->4293.908)! Learning rate decreased to 0.00068.
2024-12-02-10:06:06-root-INFO: grad norm: 375.704 351.342 133.087
2024-12-02-10:06:07-root-INFO: grad norm: 450.803 420.619 162.184
2024-12-02-10:06:07-root-INFO: Loss too large (4272.003->4272.009)! Learning rate decreased to 0.00055.
2024-12-02-10:06:07-root-INFO: grad norm: 365.888 342.084 129.818
2024-12-02-10:06:08-root-INFO: grad norm: 301.093 281.394 107.119
2024-12-02-10:06:08-root-INFO: grad norm: 253.011 237.298 87.772
2024-12-02-10:06:09-root-INFO: grad norm: 217.382 203.684 75.944
2024-12-02-10:06:09-root-INFO: Loss Change: 4299.800 -> 4190.277
2024-12-02-10:06:09-root-INFO: Regularization Change: 0.000 -> 0.532
2024-12-02-10:06:09-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-10:06:09-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-10:06:09-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-10:06:10-root-INFO: grad norm: 372.164 345.703 137.825
2024-12-02-10:06:10-root-INFO: Loss too large (4176.860->4192.327)! Learning rate decreased to 0.00071.
2024-12-02-10:06:10-root-INFO: grad norm: 430.490 402.595 152.443
2024-12-02-10:06:11-root-INFO: grad norm: 510.061 476.837 181.078
2024-12-02-10:06:11-root-INFO: Loss too large (4164.280->4168.086)! Learning rate decreased to 0.00057.
2024-12-02-10:06:12-root-INFO: grad norm: 401.996 375.857 142.592
2024-12-02-10:06:12-root-INFO: grad norm: 321.850 301.759 111.932
2024-12-02-10:06:13-root-INFO: grad norm: 262.249 245.886 91.183
2024-12-02-10:06:13-root-INFO: grad norm: 219.512 206.870 73.419
2024-12-02-10:06:13-root-INFO: grad norm: 189.092 178.062 63.636
2024-12-02-10:06:14-root-INFO: Loss Change: 4176.860 -> 4062.899
2024-12-02-10:06:14-root-INFO: Regularization Change: 0.000 -> 0.473
2024-12-02-10:06:14-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-10:06:14-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:06:14-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-10:06:14-root-INFO: grad norm: 339.534 306.269 146.568
2024-12-02-10:06:14-root-INFO: Loss too large (4057.773->4059.524)! Learning rate decreased to 0.00075.
2024-12-02-10:06:15-root-INFO: grad norm: 361.743 337.892 129.178
2024-12-02-10:06:15-root-INFO: grad norm: 418.602 391.459 148.279
2024-12-02-10:06:16-root-INFO: grad norm: 493.347 460.780 176.274
2024-12-02-10:06:16-root-INFO: Loss too large (4033.085->4037.706)! Learning rate decreased to 0.00060.
2024-12-02-10:06:16-root-INFO: grad norm: 384.684 360.636 133.880
2024-12-02-10:06:17-root-INFO: grad norm: 303.653 284.186 106.974
2024-12-02-10:06:17-root-INFO: grad norm: 245.217 230.750 82.980
2024-12-02-10:06:18-root-INFO: grad norm: 203.513 190.956 70.380
2024-12-02-10:06:18-root-INFO: Loss Change: 4057.773 -> 3948.631
2024-12-02-10:06:18-root-INFO: Regularization Change: 0.000 -> 0.493
2024-12-02-10:06:18-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-10:06:18-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:06:18-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-10:06:18-root-INFO: grad norm: 369.371 338.268 148.356
2024-12-02-10:06:19-root-INFO: Loss too large (3942.063->3956.257)! Learning rate decreased to 0.00079.
2024-12-02-10:06:19-root-INFO: grad norm: 410.213 382.586 147.995
2024-12-02-10:06:20-root-INFO: grad norm: 481.318 451.136 167.760
2024-12-02-10:06:20-root-INFO: Loss too large (3927.170->3931.279)! Learning rate decreased to 0.00063.
2024-12-02-10:06:20-root-INFO: grad norm: 374.967 350.815 132.396
2024-12-02-10:06:21-root-INFO: grad norm: 297.376 279.697 101.003
2024-12-02-10:06:21-root-INFO: grad norm: 240.506 225.526 83.553
2024-12-02-10:06:22-root-INFO: grad norm: 199.702 188.723 65.303
2024-12-02-10:06:22-root-INFO: grad norm: 170.817 160.785 57.679
2024-12-02-10:06:22-root-INFO: Loss Change: 3942.063 -> 3834.038
2024-12-02-10:06:22-root-INFO: Regularization Change: 0.000 -> 0.471
2024-12-02-10:06:22-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-10:06:22-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:06:23-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-10:06:23-root-INFO: grad norm: 176.453 166.028 59.753
2024-12-02-10:06:23-root-INFO: grad norm: 246.328 231.116 85.224
2024-12-02-10:06:23-root-INFO: Loss too large (3811.590->3815.150)! Learning rate decreased to 0.00082.
2024-12-02-10:06:24-root-INFO: grad norm: 278.832 262.388 94.338
2024-12-02-10:06:24-root-INFO: grad norm: 318.843 298.805 111.249
2024-12-02-10:06:25-root-INFO: grad norm: 367.126 345.534 124.046
2024-12-02-10:06:25-root-INFO: grad norm: 424.476 397.244 149.590
2024-12-02-10:06:25-root-INFO: Loss too large (3792.162->3793.970)! Learning rate decreased to 0.00066.
2024-12-02-10:06:26-root-INFO: grad norm: 321.494 302.719 108.256
2024-12-02-10:06:26-root-INFO: grad norm: 248.479 233.033 86.243
2024-12-02-10:06:27-root-INFO: Loss Change: 3820.543 -> 3741.727
2024-12-02-10:06:27-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-02-10:06:27-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-10:06:27-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:06:27-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-10:06:27-root-INFO: grad norm: 339.119 313.069 130.343
2024-12-02-10:06:27-root-INFO: Loss too large (3728.734->3745.634)! Learning rate decreased to 0.00086.
2024-12-02-10:06:28-root-INFO: grad norm: 368.579 344.637 130.675
2024-12-02-10:06:28-root-INFO: grad norm: 416.939 392.084 141.803
2024-12-02-10:06:28-root-INFO: Loss too large (3716.702->3717.216)! Learning rate decreased to 0.00069.
2024-12-02-10:06:29-root-INFO: grad norm: 309.095 289.952 107.086
2024-12-02-10:06:29-root-INFO: grad norm: 235.072 222.090 77.036
2024-12-02-10:06:30-root-INFO: grad norm: 185.320 174.368 62.764
2024-12-02-10:06:30-root-INFO: grad norm: 152.555 145.022 47.347
2024-12-02-10:06:31-root-INFO: grad norm: 131.725 124.567 42.831
2024-12-02-10:06:31-root-INFO: Loss Change: 3728.734 -> 3639.214
2024-12-02-10:06:31-root-INFO: Regularization Change: 0.000 -> 0.411
2024-12-02-10:06:31-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-10:06:31-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:06:31-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-10:06:31-root-INFO: grad norm: 295.582 269.678 121.005
2024-12-02-10:06:32-root-INFO: Loss too large (3650.676->3652.664)! Learning rate decreased to 0.00090.
2024-12-02-10:06:32-root-INFO: grad norm: 307.502 287.896 108.045
2024-12-02-10:06:33-root-INFO: grad norm: 347.391 325.790 120.587
2024-12-02-10:06:33-root-INFO: grad norm: 401.066 376.302 138.747
2024-12-02-10:06:33-root-INFO: Loss too large (3628.173->3630.553)! Learning rate decreased to 0.00072.
2024-12-02-10:06:34-root-INFO: grad norm: 306.071 288.057 103.454
2024-12-02-10:06:34-root-INFO: grad norm: 239.429 225.342 80.915
2024-12-02-10:06:34-root-INFO: grad norm: 192.114 181.273 63.622
2024-12-02-10:06:35-root-INFO: grad norm: 159.621 150.625 52.830
2024-12-02-10:06:35-root-INFO: Loss Change: 3650.676 -> 3561.876
2024-12-02-10:06:35-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-10:06:35-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-10:06:35-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:06:35-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-10:06:36-root-INFO: grad norm: 142.912 135.442 45.599
2024-12-02-10:06:36-root-INFO: grad norm: 197.123 186.490 63.867
2024-12-02-10:06:36-root-INFO: Loss too large (3537.131->3538.118)! Learning rate decreased to 0.00095.
2024-12-02-10:06:37-root-INFO: grad norm: 221.744 210.105 70.897
2024-12-02-10:06:37-root-INFO: grad norm: 252.324 237.961 83.918
2024-12-02-10:06:38-root-INFO: grad norm: 290.705 275.307 93.355
2024-12-02-10:06:38-root-INFO: grad norm: 336.678 316.858 113.811
2024-12-02-10:06:38-root-INFO: Loss too large (3518.663->3518.839)! Learning rate decreased to 0.00076.
2024-12-02-10:06:39-root-INFO: grad norm: 257.505 243.895 82.610
2024-12-02-10:06:39-root-INFO: grad norm: 202.627 191.273 66.876
2024-12-02-10:06:40-root-INFO: Loss Change: 3544.713 -> 3479.703
2024-12-02-10:06:40-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-10:06:40-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-10:06:40-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-10:06:40-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-10:06:40-root-INFO: grad norm: 341.129 311.269 139.574
2024-12-02-10:06:40-root-INFO: Loss too large (3479.801->3489.084)! Learning rate decreased to 0.00099.
2024-12-02-10:06:41-root-INFO: grad norm: 346.577 323.697 123.839
2024-12-02-10:06:41-root-INFO: grad norm: 405.969 383.360 133.588
2024-12-02-10:06:41-root-INFO: Loss too large (3460.921->3468.820)! Learning rate decreased to 0.00079.
2024-12-02-10:06:42-root-INFO: grad norm: 322.222 303.514 108.197
2024-12-02-10:06:42-root-INFO: grad norm: 260.317 246.777 82.861
2024-12-02-10:06:43-root-INFO: grad norm: 215.411 203.393 70.945
2024-12-02-10:06:43-root-INFO: grad norm: 181.188 172.024 56.895
2024-12-02-10:06:44-root-INFO: grad norm: 156.436 147.987 50.716
2024-12-02-10:06:44-root-INFO: Loss Change: 3479.801 -> 3386.587
2024-12-02-10:06:44-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-02-10:06:44-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-10:06:44-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:06:44-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-10:06:44-root-INFO: grad norm: 208.495 195.677 71.978
2024-12-02-10:06:44-root-INFO: Loss too large (3382.454->3387.035)! Learning rate decreased to 0.00104.
2024-12-02-10:06:45-root-INFO: grad norm: 238.798 227.038 74.016
2024-12-02-10:06:45-root-INFO: grad norm: 285.123 270.108 91.307
2024-12-02-10:06:45-root-INFO: Loss too large (3372.943->3373.597)! Learning rate decreased to 0.00083.
2024-12-02-10:06:46-root-INFO: grad norm: 231.855 220.219 72.526
2024-12-02-10:06:46-root-INFO: grad norm: 191.541 181.456 61.334
2024-12-02-10:06:47-root-INFO: grad norm: 163.084 155.195 50.109
2024-12-02-10:06:47-root-INFO: grad norm: 142.049 134.878 44.565
2024-12-02-10:06:48-root-INFO: grad norm: 127.230 121.303 38.382
2024-12-02-10:06:48-root-INFO: Loss Change: 3382.454 -> 3317.352
2024-12-02-10:06:48-root-INFO: Regularization Change: 0.000 -> 0.411
2024-12-02-10:06:48-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-10:06:48-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:06:48-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-10:06:48-root-INFO: grad norm: 200.037 182.587 81.711
2024-12-02-10:06:49-root-INFO: grad norm: 283.536 265.911 98.410
2024-12-02-10:06:49-root-INFO: Loss too large (3298.558->3326.885)! Learning rate decreased to 0.00109.
2024-12-02-10:06:49-root-INFO: Loss too large (3298.558->3299.042)! Learning rate decreased to 0.00087.
2024-12-02-10:06:50-root-INFO: grad norm: 239.457 226.203 78.561
2024-12-02-10:06:50-root-INFO: grad norm: 215.421 204.695 67.129
2024-12-02-10:06:51-root-INFO: grad norm: 197.163 186.826 63.001
2024-12-02-10:06:51-root-INFO: grad norm: 183.196 174.313 56.355
2024-12-02-10:06:52-root-INFO: grad norm: 171.758 162.931 54.354
2024-12-02-10:06:52-root-INFO: grad norm: 162.923 155.103 49.870
2024-12-02-10:06:52-root-INFO: Loss Change: 3303.856 -> 3236.522
2024-12-02-10:06:52-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-02-10:06:52-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-10:06:52-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:06:52-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-10:06:53-root-INFO: grad norm: 234.176 217.200 87.535
2024-12-02-10:06:53-root-INFO: Loss too large (3240.251->3246.248)! Learning rate decreased to 0.00114.
2024-12-02-10:06:53-root-INFO: grad norm: 273.400 258.997 87.567
2024-12-02-10:06:53-root-INFO: Loss too large (3231.477->3236.187)! Learning rate decreased to 0.00091.
2024-12-02-10:06:54-root-INFO: grad norm: 255.699 242.180 82.044
2024-12-02-10:06:54-root-INFO: grad norm: 248.218 236.188 76.335
2024-12-02-10:06:55-root-INFO: grad norm: 244.224 231.423 78.030
2024-12-02-10:06:55-root-INFO: grad norm: 242.506 230.696 74.756
2024-12-02-10:06:56-root-INFO: grad norm: 242.340 229.778 77.011
2024-12-02-10:06:56-root-INFO: grad norm: 243.726 231.681 75.671
2024-12-02-10:06:56-root-INFO: Loss Change: 3240.251 -> 3177.876
2024-12-02-10:06:56-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-02-10:06:56-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-10:06:56-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:06:57-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-10:06:57-root-INFO: grad norm: 276.370 259.468 95.168
2024-12-02-10:06:57-root-INFO: Loss too large (3164.445->3203.649)! Learning rate decreased to 0.00120.
2024-12-02-10:06:57-root-INFO: Loss too large (3164.445->3171.608)! Learning rate decreased to 0.00096.
2024-12-02-10:06:58-root-INFO: grad norm: 266.588 254.083 80.690
2024-12-02-10:06:58-root-INFO: grad norm: 270.866 256.644 86.616
2024-12-02-10:06:58-root-INFO: grad norm: 279.198 266.029 84.734
2024-12-02-10:06:59-root-INFO: grad norm: 291.168 276.164 92.262
2024-12-02-10:06:59-root-INFO: grad norm: 305.095 290.242 94.033
2024-12-02-10:07:00-root-INFO: grad norm: 322.038 305.693 101.295
2024-12-02-10:07:00-root-INFO: grad norm: 339.597 322.539 106.276
2024-12-02-10:07:01-root-INFO: Loss Change: 3164.445 -> 3117.344
2024-12-02-10:07:01-root-INFO: Regularization Change: 0.000 -> 0.441
2024-12-02-10:07:01-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-10:07:01-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:07:01-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-10:07:01-root-INFO: grad norm: 386.008 366.224 121.994
2024-12-02-10:07:01-root-INFO: Loss too large (3102.976->3215.857)! Learning rate decreased to 0.00126.
2024-12-02-10:07:01-root-INFO: Loss too large (3102.976->3140.644)! Learning rate decreased to 0.00101.
2024-12-02-10:07:02-root-INFO: grad norm: 398.447 379.172 122.426
2024-12-02-10:07:02-root-INFO: grad norm: 421.459 401.261 128.908
2024-12-02-10:07:02-root-INFO: Loss too large (3093.264->3093.267)! Learning rate decreased to 0.00080.
2024-12-02-10:07:03-root-INFO: grad norm: 286.704 273.314 86.596
2024-12-02-10:07:03-root-INFO: grad norm: 199.405 189.185 63.021
2024-12-02-10:07:04-root-INFO: grad norm: 150.537 143.968 43.984
2024-12-02-10:07:04-root-INFO: grad norm: 121.317 115.300 37.732
2024-12-02-10:07:05-root-INFO: grad norm: 105.083 100.783 29.751
2024-12-02-10:07:05-root-INFO: Loss Change: 3102.976 -> 3018.097
2024-12-02-10:07:05-root-INFO: Regularization Change: 0.000 -> 0.366
2024-12-02-10:07:05-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-10:07:05-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:07:05-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-10:07:05-root-INFO: grad norm: 151.976 139.979 59.183
2024-12-02-10:07:06-root-INFO: grad norm: 233.667 221.526 74.342
2024-12-02-10:07:06-root-INFO: Loss too large (3000.692->3041.080)! Learning rate decreased to 0.00132.
2024-12-02-10:07:06-root-INFO: Loss too large (3000.692->3011.988)! Learning rate decreased to 0.00105.
2024-12-02-10:07:07-root-INFO: grad norm: 272.779 259.599 83.766
2024-12-02-10:07:07-root-INFO: Loss too large (2996.263->2998.377)! Learning rate decreased to 0.00084.
2024-12-02-10:07:07-root-INFO: grad norm: 228.405 217.972 68.240
2024-12-02-10:07:08-root-INFO: grad norm: 194.542 184.948 60.338
2024-12-02-10:07:08-root-INFO: grad norm: 170.401 162.777 50.400
2024-12-02-10:07:09-root-INFO: grad norm: 151.995 144.640 46.708
2024-12-02-10:07:09-root-INFO: grad norm: 138.648 132.585 40.551
2024-12-02-10:07:10-root-INFO: Loss Change: 3007.988 -> 2947.029
2024-12-02-10:07:10-root-INFO: Regularization Change: 0.000 -> 0.447
2024-12-02-10:07:10-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-10:07:10-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-10:07:10-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-10:07:10-root-INFO: grad norm: 174.226 161.997 64.122
2024-12-02-10:07:10-root-INFO: grad norm: 314.350 299.566 95.270
2024-12-02-10:07:10-root-INFO: Loss too large (2942.078->3058.831)! Learning rate decreased to 0.00138.
2024-12-02-10:07:11-root-INFO: Loss too large (2942.078->2989.980)! Learning rate decreased to 0.00110.
2024-12-02-10:07:11-root-INFO: Loss too large (2942.078->2950.800)! Learning rate decreased to 0.00088.
2024-12-02-10:07:11-root-INFO: grad norm: 289.995 275.968 89.100
2024-12-02-10:07:12-root-INFO: grad norm: 280.798 267.902 84.116
2024-12-02-10:07:12-root-INFO: grad norm: 273.790 260.489 84.299
2024-12-02-10:07:13-root-INFO: grad norm: 269.495 257.045 80.964
2024-12-02-10:07:13-root-INFO: grad norm: 266.988 254.182 81.695
2024-12-02-10:07:14-root-INFO: grad norm: 266.590 254.154 80.471
2024-12-02-10:07:14-root-INFO: Loss Change: 2943.390 -> 2889.080
2024-12-02-10:07:14-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-10:07:14-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-10:07:14-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:07:14-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-10:07:14-root-INFO: grad norm: 255.537 243.363 77.934
2024-12-02-10:07:14-root-INFO: Loss too large (2874.680->2954.881)! Learning rate decreased to 0.00144.
2024-12-02-10:07:15-root-INFO: Loss too large (2874.680->2908.576)! Learning rate decreased to 0.00115.
2024-12-02-10:07:15-root-INFO: Loss too large (2874.680->2881.811)! Learning rate decreased to 0.00092.
2024-12-02-10:07:15-root-INFO: grad norm: 251.550 240.220 74.644
2024-12-02-10:07:16-root-INFO: grad norm: 252.795 241.649 74.236
2024-12-02-10:07:16-root-INFO: grad norm: 256.973 245.288 76.611
2024-12-02-10:07:17-root-INFO: grad norm: 263.800 252.250 77.204
2024-12-02-10:07:17-root-INFO: grad norm: 272.835 260.313 81.705
2024-12-02-10:07:17-root-INFO: grad norm: 284.481 272.074 83.098
2024-12-02-10:07:18-root-INFO: grad norm: 297.834 284.030 89.621
2024-12-02-10:07:18-root-INFO: Loss Change: 2874.680 -> 2829.481
2024-12-02-10:07:18-root-INFO: Regularization Change: 0.000 -> 0.379
2024-12-02-10:07:18-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-10:07:18-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:07:19-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-10:07:19-root-INFO: grad norm: 324.742 308.937 100.076
2024-12-02-10:07:19-root-INFO: Loss too large (2818.631->2942.077)! Learning rate decreased to 0.00150.
2024-12-02-10:07:19-root-INFO: Loss too large (2818.631->2867.426)! Learning rate decreased to 0.00120.
2024-12-02-10:07:19-root-INFO: Loss too large (2818.631->2824.720)! Learning rate decreased to 0.00096.
2024-12-02-10:07:20-root-INFO: grad norm: 324.620 308.777 100.176
2024-12-02-10:07:20-root-INFO: grad norm: 365.343 349.830 105.330
2024-12-02-10:07:20-root-INFO: Loss too large (2797.998->2800.145)! Learning rate decreased to 0.00077.
2024-12-02-10:07:21-root-INFO: grad norm: 275.350 262.832 82.078
2024-12-02-10:07:21-root-INFO: grad norm: 212.993 203.571 62.648
2024-12-02-10:07:22-root-INFO: grad norm: 172.853 165.081 51.250
2024-12-02-10:07:22-root-INFO: grad norm: 145.389 139.015 42.580
2024-12-02-10:07:23-root-INFO: grad norm: 127.228 121.626 37.337
2024-12-02-10:07:23-root-INFO: Loss Change: 2818.631 -> 2735.708
2024-12-02-10:07:23-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-02-10:07:23-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-10:07:23-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:07:23-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-10:07:23-root-INFO: grad norm: 160.561 152.238 51.022
2024-12-02-10:07:24-root-INFO: Loss too large (2735.549->2760.487)! Learning rate decreased to 0.00157.
2024-12-02-10:07:24-root-INFO: Loss too large (2735.549->2743.465)! Learning rate decreased to 0.00126.
2024-12-02-10:07:24-root-INFO: grad norm: 252.408 242.174 71.144
2024-12-02-10:07:24-root-INFO: Loss too large (2734.120->2753.765)! Learning rate decreased to 0.00101.
2024-12-02-10:07:25-root-INFO: grad norm: 311.724 298.546 89.680
2024-12-02-10:07:25-root-INFO: Loss too large (2733.693->2738.901)! Learning rate decreased to 0.00081.
2024-12-02-10:07:26-root-INFO: grad norm: 259.796 249.066 73.891
2024-12-02-10:07:26-root-INFO: grad norm: 219.608 210.420 62.855
2024-12-02-10:07:26-root-INFO: grad norm: 190.883 182.998 54.298
2024-12-02-10:07:27-root-INFO: grad norm: 169.144 162.249 47.802
2024-12-02-10:07:27-root-INFO: grad norm: 153.436 147.086 43.685
2024-12-02-10:07:28-root-INFO: Loss Change: 2735.549 -> 2682.013
2024-12-02-10:07:28-root-INFO: Regularization Change: 0.000 -> 0.366
2024-12-02-10:07:28-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-10:07:28-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:07:28-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-10:07:28-root-INFO: grad norm: 133.731 123.079 52.304
2024-12-02-10:07:28-root-INFO: grad norm: 171.232 163.504 50.863
2024-12-02-10:07:28-root-INFO: Loss too large (2648.548->2686.395)! Learning rate decreased to 0.00165.
2024-12-02-10:07:29-root-INFO: Loss too large (2648.548->2663.069)! Learning rate decreased to 0.00132.
2024-12-02-10:07:29-root-INFO: Loss too large (2648.548->2650.043)! Learning rate decreased to 0.00105.
2024-12-02-10:07:29-root-INFO: grad norm: 221.938 213.538 60.478
2024-12-02-10:07:29-root-INFO: Loss too large (2643.271->2648.259)! Learning rate decreased to 0.00084.
2024-12-02-10:07:30-root-INFO: grad norm: 231.974 222.298 66.301
2024-12-02-10:07:30-root-INFO: grad norm: 245.207 236.019 66.496
2024-12-02-10:07:31-root-INFO: grad norm: 260.950 250.112 74.423
2024-12-02-10:07:31-root-INFO: grad norm: 280.001 269.639 75.467
2024-12-02-10:07:32-root-INFO: grad norm: 301.021 288.518 85.854
2024-12-02-10:07:32-root-INFO: Loss Change: 2665.930 -> 2619.066
2024-12-02-10:07:32-root-INFO: Regularization Change: 0.000 -> 0.517
2024-12-02-10:07:32-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-10:07:32-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:07:32-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-10:07:32-root-INFO: grad norm: 311.664 300.483 82.730
2024-12-02-10:07:33-root-INFO: Loss too large (2599.148->2839.264)! Learning rate decreased to 0.00172.
2024-12-02-10:07:33-root-INFO: Loss too large (2599.148->2727.476)! Learning rate decreased to 0.00138.
2024-12-02-10:07:33-root-INFO: Loss too large (2599.148->2657.473)! Learning rate decreased to 0.00110.
2024-12-02-10:07:33-root-INFO: Loss too large (2599.148->2616.008)! Learning rate decreased to 0.00088.
2024-12-02-10:07:34-root-INFO: grad norm: 323.921 310.805 91.243
2024-12-02-10:07:34-root-INFO: grad norm: 345.992 333.628 91.666
2024-12-02-10:07:35-root-INFO: grad norm: 371.385 357.009 102.332
2024-12-02-10:07:35-root-INFO: grad norm: 399.063 385.039 104.862
2024-12-02-10:07:35-root-INFO: Loss too large (2585.035->2585.115)! Learning rate decreased to 0.00070.
2024-12-02-10:07:36-root-INFO: grad norm: 275.282 264.506 76.266
2024-12-02-10:07:36-root-INFO: grad norm: 196.688 189.737 51.827
2024-12-02-10:07:36-root-INFO: grad norm: 150.184 144.087 42.357
2024-12-02-10:07:37-root-INFO: Loss Change: 2599.148 -> 2535.471
2024-12-02-10:07:37-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-02-10:07:37-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-10:07:37-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-10:07:37-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-10:07:37-root-INFO: grad norm: 141.447 131.654 51.715
2024-12-02-10:07:37-root-INFO: Loss too large (2529.883->2531.626)! Learning rate decreased to 0.00180.
2024-12-02-10:07:38-root-INFO: grad norm: 271.523 262.142 70.757
2024-12-02-10:07:38-root-INFO: Loss too large (2523.480->2651.212)! Learning rate decreased to 0.00144.
2024-12-02-10:07:38-root-INFO: Loss too large (2523.480->2585.500)! Learning rate decreased to 0.00115.
2024-12-02-10:07:38-root-INFO: Loss too large (2523.480->2546.478)! Learning rate decreased to 0.00092.
2024-12-02-10:07:38-root-INFO: Loss too large (2523.480->2524.554)! Learning rate decreased to 0.00074.
2024-12-02-10:07:39-root-INFO: grad norm: 226.517 218.000 61.529
2024-12-02-10:07:39-root-INFO: grad norm: 194.912 187.717 52.471
2024-12-02-10:07:40-root-INFO: grad norm: 171.273 165.024 45.843
2024-12-02-10:07:40-root-INFO: grad norm: 154.085 148.244 42.022
2024-12-02-10:07:41-root-INFO: grad norm: 141.058 135.989 37.472
2024-12-02-10:07:41-root-INFO: grad norm: 131.383 126.307 36.168
2024-12-02-10:07:41-root-INFO: Loss Change: 2529.883 -> 2476.779
2024-12-02-10:07:41-root-INFO: Regularization Change: 0.000 -> 0.352
2024-12-02-10:07:41-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-10:07:41-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:07:42-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-10:07:42-root-INFO: grad norm: 102.342 96.419 34.313
2024-12-02-10:07:42-root-INFO: grad norm: 189.152 182.046 51.358
2024-12-02-10:07:42-root-INFO: Loss too large (2444.910->2568.235)! Learning rate decreased to 0.00188.
2024-12-02-10:07:42-root-INFO: Loss too large (2444.910->2511.675)! Learning rate decreased to 0.00150.
2024-12-02-10:07:43-root-INFO: Loss too large (2444.910->2477.214)! Learning rate decreased to 0.00120.
2024-12-02-10:07:43-root-INFO: Loss too large (2444.910->2457.008)! Learning rate decreased to 0.00096.
2024-12-02-10:07:43-root-INFO: Loss too large (2444.910->2445.722)! Learning rate decreased to 0.00077.
2024-12-02-10:07:43-root-INFO: grad norm: 189.546 183.787 46.369
2024-12-02-10:07:44-root-INFO: grad norm: 192.582 185.564 51.514
2024-12-02-10:07:44-root-INFO: grad norm: 197.610 191.789 47.611
2024-12-02-10:07:45-root-INFO: grad norm: 204.442 197.124 54.210
2024-12-02-10:07:45-root-INFO: grad norm: 213.408 207.150 51.301
2024-12-02-10:07:46-root-INFO: grad norm: 224.249 216.360 58.958
2024-12-02-10:07:46-root-INFO: Loss Change: 2457.124 -> 2416.606
2024-12-02-10:07:46-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-02-10:07:46-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-10:07:46-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:07:46-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-10:07:46-root-INFO: grad norm: 238.704 229.615 65.243
2024-12-02-10:07:47-root-INFO: Loss too large (2406.508->2610.011)! Learning rate decreased to 0.00196.
2024-12-02-10:07:47-root-INFO: Loss too large (2406.508->2518.829)! Learning rate decreased to 0.00157.
2024-12-02-10:07:47-root-INFO: Loss too large (2406.508->2461.966)! Learning rate decreased to 0.00126.
2024-12-02-10:07:47-root-INFO: Loss too large (2406.508->2428.068)! Learning rate decreased to 0.00100.
2024-12-02-10:07:47-root-INFO: Loss too large (2406.508->2408.894)! Learning rate decreased to 0.00080.
2024-12-02-10:07:48-root-INFO: grad norm: 246.292 237.598 64.861
2024-12-02-10:07:48-root-INFO: grad norm: 271.073 262.375 68.115
2024-12-02-10:07:49-root-INFO: grad norm: 304.824 295.564 74.561
2024-12-02-10:07:49-root-INFO: Loss too large (2393.975->2394.040)! Learning rate decreased to 0.00064.
2024-12-02-10:07:49-root-INFO: grad norm: 225.118 217.921 56.468
2024-12-02-10:07:50-root-INFO: grad norm: 174.036 168.450 43.742
2024-12-02-10:07:50-root-INFO: grad norm: 139.351 134.516 36.387
2024-12-02-10:07:51-root-INFO: grad norm: 116.674 112.599 30.565
2024-12-02-10:07:51-root-INFO: Loss Change: 2406.508 -> 2360.074
2024-12-02-10:07:51-root-INFO: Regularization Change: 0.000 -> 0.206
2024-12-02-10:07:51-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-10:07:51-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:07:51-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-10:07:51-root-INFO: grad norm: 102.113 95.893 35.092
2024-12-02-10:07:52-root-INFO: Loss too large (2348.769->2353.993)! Learning rate decreased to 0.00205.
2024-12-02-10:07:52-root-INFO: grad norm: 266.520 258.861 63.435
2024-12-02-10:07:52-root-INFO: Loss too large (2347.736->2559.199)! Learning rate decreased to 0.00164.
2024-12-02-10:07:52-root-INFO: Loss too large (2347.736->2464.411)! Learning rate decreased to 0.00131.
2024-12-02-10:07:52-root-INFO: Loss too large (2347.736->2405.125)! Learning rate decreased to 0.00105.
2024-12-02-10:07:53-root-INFO: Loss too large (2347.736->2369.779)! Learning rate decreased to 0.00084.
2024-12-02-10:07:53-root-INFO: Loss too large (2347.736->2349.826)! Learning rate decreased to 0.00067.
2024-12-02-10:07:53-root-INFO: grad norm: 216.290 210.027 51.671
2024-12-02-10:07:54-root-INFO: grad norm: 180.691 175.253 43.996
2024-12-02-10:07:54-root-INFO: grad norm: 153.824 149.160 37.592
2024-12-02-10:07:55-root-INFO: grad norm: 134.408 130.097 33.768
2024-12-02-10:07:55-root-INFO: grad norm: 119.767 115.924 30.096
2024-12-02-10:07:56-root-INFO: grad norm: 109.009 105.268 28.311
2024-12-02-10:07:56-root-INFO: Loss Change: 2348.769 -> 2312.379
2024-12-02-10:07:56-root-INFO: Regularization Change: 0.000 -> 0.243
2024-12-02-10:07:56-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-10:07:56-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:07:56-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-10:07:56-root-INFO: grad norm: 112.791 104.747 41.832
2024-12-02-10:07:57-root-INFO: Loss too large (2302.909->2307.065)! Learning rate decreased to 0.00214.
2024-12-02-10:07:57-root-INFO: grad norm: 273.898 266.848 61.743
2024-12-02-10:07:57-root-INFO: Loss too large (2299.885->2564.008)! Learning rate decreased to 0.00171.
2024-12-02-10:07:57-root-INFO: Loss too large (2299.885->2450.931)! Learning rate decreased to 0.00137.
2024-12-02-10:07:58-root-INFO: Loss too large (2299.885->2378.573)! Learning rate decreased to 0.00110.
2024-12-02-10:07:58-root-INFO: Loss too large (2299.885->2334.461)! Learning rate decreased to 0.00088.
2024-12-02-10:07:58-root-INFO: Loss too large (2299.885->2308.905)! Learning rate decreased to 0.00070.
2024-12-02-10:07:58-root-INFO: grad norm: 263.296 255.853 62.161
2024-12-02-10:07:59-root-INFO: grad norm: 254.650 247.893 58.270
2024-12-02-10:07:59-root-INFO: grad norm: 247.285 240.429 57.826
2024-12-02-10:08:00-root-INFO: grad norm: 241.622 235.129 55.640
2024-12-02-10:08:00-root-INFO: grad norm: 237.199 230.664 55.296
2024-12-02-10:08:01-root-INFO: grad norm: 234.254 227.912 54.139
2024-12-02-10:08:01-root-INFO: Loss Change: 2302.909 -> 2272.854
2024-12-02-10:08:01-root-INFO: Regularization Change: 0.000 -> 0.257
2024-12-02-10:08:01-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-10:08:01-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:08:01-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-10:08:01-root-INFO: grad norm: 184.738 177.707 50.480
2024-12-02-10:08:02-root-INFO: Loss too large (2259.229->2419.966)! Learning rate decreased to 0.00224.
2024-12-02-10:08:02-root-INFO: Loss too large (2259.229->2349.624)! Learning rate decreased to 0.00179.
2024-12-02-10:08:02-root-INFO: Loss too large (2259.229->2305.822)! Learning rate decreased to 0.00143.
2024-12-02-10:08:02-root-INFO: Loss too large (2259.229->2279.552)! Learning rate decreased to 0.00114.
2024-12-02-10:08:02-root-INFO: Loss too large (2259.229->2264.470)! Learning rate decreased to 0.00092.
2024-12-02-10:08:03-root-INFO: grad norm: 242.716 236.103 56.273
2024-12-02-10:08:03-root-INFO: Loss too large (2256.319->2264.014)! Learning rate decreased to 0.00073.
2024-12-02-10:08:03-root-INFO: grad norm: 247.850 241.006 57.843
2024-12-02-10:08:04-root-INFO: grad norm: 256.403 249.998 56.949
2024-12-02-10:08:04-root-INFO: grad norm: 267.414 260.364 60.998
2024-12-02-10:08:05-root-INFO: grad norm: 279.811 272.933 61.661
2024-12-02-10:08:05-root-INFO: grad norm: 293.892 286.383 66.008
2024-12-02-10:08:06-root-INFO: grad norm: 308.385 300.814 67.912
2024-12-02-10:08:06-root-INFO: Loss Change: 2259.229 -> 2240.629
2024-12-02-10:08:06-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-02-10:08:06-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-10:08:06-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-10:08:06-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-10:08:06-root-INFO: grad norm: 253.633 247.105 57.171
2024-12-02-10:08:06-root-INFO: Loss too large (2218.263->2615.783)! Learning rate decreased to 0.00233.
2024-12-02-10:08:06-root-INFO: Loss too large (2218.263->2463.260)! Learning rate decreased to 0.00187.
2024-12-02-10:08:07-root-INFO: Loss too large (2218.263->2360.001)! Learning rate decreased to 0.00149.
2024-12-02-10:08:07-root-INFO: Loss too large (2218.263->2293.389)! Learning rate decreased to 0.00119.
2024-12-02-10:08:07-root-INFO: Loss too large (2218.263->2252.331)! Learning rate decreased to 0.00096.
2024-12-02-10:08:07-root-INFO: Loss too large (2218.263->2228.230)! Learning rate decreased to 0.00076.
2024-12-02-10:08:07-root-INFO: grad norm: 262.753 256.322 57.775
2024-12-02-10:08:08-root-INFO: grad norm: 278.222 271.703 59.876
2024-12-02-10:08:08-root-INFO: grad norm: 297.572 290.595 64.058
2024-12-02-10:08:09-root-INFO: grad norm: 320.924 313.674 67.833
2024-12-02-10:08:09-root-INFO: Loss too large (2210.126->2210.752)! Learning rate decreased to 0.00061.
2024-12-02-10:08:10-root-INFO: grad norm: 223.296 217.868 48.932
2024-12-02-10:08:10-root-INFO: grad norm: 159.786 155.687 35.961
2024-12-02-10:08:11-root-INFO: grad norm: 121.489 117.972 29.020
2024-12-02-10:08:11-root-INFO: Loss Change: 2218.263 -> 2182.931
2024-12-02-10:08:11-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-02-10:08:11-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-10:08:11-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:08:11-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-10:08:11-root-INFO: grad norm: 134.921 124.673 51.576
2024-12-02-10:08:12-root-INFO: grad norm: 211.369 206.287 46.070
2024-12-02-10:08:12-root-INFO: Loss too large (2168.196->2451.789)! Learning rate decreased to 0.00244.
2024-12-02-10:08:12-root-INFO: Loss too large (2168.196->2338.048)! Learning rate decreased to 0.00195.
2024-12-02-10:08:12-root-INFO: Loss too large (2168.196->2263.614)! Learning rate decreased to 0.00156.
2024-12-02-10:08:12-root-INFO: Loss too large (2168.196->2216.896)! Learning rate decreased to 0.00125.
2024-12-02-10:08:12-root-INFO: Loss too large (2168.196->2188.761)! Learning rate decreased to 0.00100.
2024-12-02-10:08:13-root-INFO: Loss too large (2168.196->2172.603)! Learning rate decreased to 0.00080.
2024-12-02-10:08:13-root-INFO: grad norm: 253.205 246.747 56.823
2024-12-02-10:08:13-root-INFO: Loss too large (2163.916->2166.172)! Learning rate decreased to 0.00064.
2024-12-02-10:08:14-root-INFO: grad norm: 217.180 212.552 44.596
2024-12-02-10:08:14-root-INFO: grad norm: 187.573 182.657 42.661
2024-12-02-10:08:15-root-INFO: grad norm: 164.234 160.572 34.487
2024-12-02-10:08:15-root-INFO: grad norm: 145.230 141.072 34.506
2024-12-02-10:08:16-root-INFO: grad norm: 130.234 127.075 28.509
2024-12-02-10:08:16-root-INFO: Loss Change: 2179.781 -> 2138.069
2024-12-02-10:08:16-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-02-10:08:16-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-10:08:16-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:08:16-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-10:08:16-root-INFO: grad norm: 95.430 89.114 34.141
2024-12-02-10:08:17-root-INFO: grad norm: 197.854 193.551 41.037
2024-12-02-10:08:17-root-INFO: Loss too large (2121.885->2432.838)! Learning rate decreased to 0.00254.
2024-12-02-10:08:17-root-INFO: Loss too large (2121.885->2312.971)! Learning rate decreased to 0.00203.
2024-12-02-10:08:17-root-INFO: Loss too large (2121.885->2233.114)! Learning rate decreased to 0.00163.
2024-12-02-10:08:17-root-INFO: Loss too large (2121.885->2182.145)! Learning rate decreased to 0.00130.
2024-12-02-10:08:17-root-INFO: Loss too large (2121.885->2150.867)! Learning rate decreased to 0.00104.
2024-12-02-10:08:18-root-INFO: Loss too large (2121.885->2132.447)! Learning rate decreased to 0.00083.
2024-12-02-10:08:18-root-INFO: Loss too large (2121.885->2122.147)! Learning rate decreased to 0.00067.
2024-12-02-10:08:18-root-INFO: grad norm: 186.118 181.515 41.135
2024-12-02-10:08:19-root-INFO: grad norm: 177.622 173.910 36.123
2024-12-02-10:08:19-root-INFO: grad norm: 171.143 166.846 38.112
2024-12-02-10:08:20-root-INFO: grad norm: 166.134 162.629 33.945
2024-12-02-10:08:20-root-INFO: grad norm: 162.062 157.917 36.416
2024-12-02-10:08:21-root-INFO: grad norm: 158.931 155.526 32.723
2024-12-02-10:08:21-root-INFO: Loss Change: 2129.675 -> 2099.505
2024-12-02-10:08:21-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-02-10:08:21-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-10:08:21-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:08:21-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-10:08:21-root-INFO: grad norm: 133.858 127.817 39.759
2024-12-02-10:08:21-root-INFO: Loss too large (2089.133->2205.767)! Learning rate decreased to 0.00265.
2024-12-02-10:08:21-root-INFO: Loss too large (2089.133->2155.671)! Learning rate decreased to 0.00212.
2024-12-02-10:08:22-root-INFO: Loss too large (2089.133->2124.567)! Learning rate decreased to 0.00170.
2024-12-02-10:08:22-root-INFO: Loss too large (2089.133->2105.846)! Learning rate decreased to 0.00136.
2024-12-02-10:08:22-root-INFO: Loss too large (2089.133->2094.976)! Learning rate decreased to 0.00109.
2024-12-02-10:08:22-root-INFO: grad norm: 227.669 223.427 43.746
2024-12-02-10:08:23-root-INFO: Loss too large (2088.967->2114.357)! Learning rate decreased to 0.00087.
2024-12-02-10:08:23-root-INFO: Loss too large (2088.967->2096.203)! Learning rate decreased to 0.00070.
2024-12-02-10:08:23-root-INFO: grad norm: 231.666 226.299 49.574
2024-12-02-10:08:24-root-INFO: grad norm: 237.148 232.885 44.764
2024-12-02-10:08:24-root-INFO: grad norm: 243.686 238.307 50.916
2024-12-02-10:08:25-root-INFO: grad norm: 250.855 246.366 47.246
2024-12-02-10:08:25-root-INFO: grad norm: 258.711 253.204 53.096
2024-12-02-10:08:26-root-INFO: grad norm: 266.698 261.914 50.290
2024-12-02-10:08:26-root-INFO: Loss Change: 2089.133 -> 2075.396
2024-12-02-10:08:26-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-02-10:08:26-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-10:08:26-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:08:26-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-10:08:26-root-INFO: grad norm: 172.086 167.058 41.296
2024-12-02-10:08:26-root-INFO: Loss too large (2056.650->2297.486)! Learning rate decreased to 0.00277.
2024-12-02-10:08:27-root-INFO: Loss too large (2056.650->2201.458)! Learning rate decreased to 0.00222.
2024-12-02-10:08:27-root-INFO: Loss too large (2056.650->2139.036)! Learning rate decreased to 0.00177.
2024-12-02-10:08:27-root-INFO: Loss too large (2056.650->2099.938)! Learning rate decreased to 0.00142.
2024-12-02-10:08:27-root-INFO: Loss too large (2056.650->2076.321)! Learning rate decreased to 0.00113.
2024-12-02-10:08:27-root-INFO: Loss too large (2056.650->2062.632)! Learning rate decreased to 0.00091.
2024-12-02-10:08:28-root-INFO: grad norm: 231.072 226.734 44.566
2024-12-02-10:08:28-root-INFO: Loss too large (2055.127->2063.519)! Learning rate decreased to 0.00073.
2024-12-02-10:08:28-root-INFO: grad norm: 241.219 236.491 47.525
2024-12-02-10:08:29-root-INFO: grad norm: 253.808 249.407 47.060
2024-12-02-10:08:29-root-INFO: grad norm: 268.262 263.221 51.763
2024-12-02-10:08:30-root-INFO: grad norm: 283.358 278.587 51.784
2024-12-02-10:08:30-root-INFO: grad norm: 299.352 293.912 56.811
2024-12-02-10:08:31-root-INFO: grad norm: 314.892 309.653 57.200
2024-12-02-10:08:31-root-INFO: Loss Change: 2056.650 -> 2046.550
2024-12-02-10:08:31-root-INFO: Regularization Change: 0.000 -> 0.142
2024-12-02-10:08:31-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-10:08:31-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-10:08:31-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-10:08:31-root-INFO: grad norm: 331.078 324.043 67.890
2024-12-02-10:08:31-root-INFO: Loss too large (2048.308->2956.142)! Learning rate decreased to 0.00289.
2024-12-02-10:08:32-root-INFO: Loss too large (2048.308->2667.913)! Learning rate decreased to 0.00231.
2024-12-02-10:08:32-root-INFO: Loss too large (2048.308->2446.926)! Learning rate decreased to 0.00185.
2024-12-02-10:08:32-root-INFO: Loss too large (2048.308->2287.965)! Learning rate decreased to 0.00148.
2024-12-02-10:08:32-root-INFO: Loss too large (2048.308->2180.207)! Learning rate decreased to 0.00118.
2024-12-02-10:08:32-root-INFO: Loss too large (2048.308->2111.145)! Learning rate decreased to 0.00095.
2024-12-02-10:08:32-root-INFO: Loss too large (2048.308->2069.335)! Learning rate decreased to 0.00076.
2024-12-02-10:08:33-root-INFO: grad norm: 344.264 338.813 61.021
2024-12-02-10:08:33-root-INFO: grad norm: 362.200 355.628 68.689
2024-12-02-10:08:34-root-INFO: Loss too large (2043.623->2044.109)! Learning rate decreased to 0.00061.
2024-12-02-10:08:34-root-INFO: grad norm: 243.950 240.032 43.546
2024-12-02-10:08:35-root-INFO: grad norm: 168.771 164.538 37.564
2024-12-02-10:08:35-root-INFO: grad norm: 123.488 120.789 25.675
2024-12-02-10:08:35-root-INFO: grad norm: 95.159 91.532 26.023
2024-12-02-10:08:36-root-INFO: grad norm: 78.388 75.691 20.386
2024-12-02-10:08:36-root-INFO: Loss Change: 2048.308 -> 2007.275
2024-12-02-10:08:36-root-INFO: Regularization Change: 0.000 -> 0.120
2024-12-02-10:08:36-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-10:08:36-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:08:36-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-10:08:37-root-INFO: grad norm: 112.174 107.464 32.165
2024-12-02-10:08:37-root-INFO: Loss too large (1996.722->2056.985)! Learning rate decreased to 0.00301.
2024-12-02-10:08:37-root-INFO: Loss too large (1996.722->2029.058)! Learning rate decreased to 0.00241.
2024-12-02-10:08:37-root-INFO: Loss too large (1996.722->2011.929)! Learning rate decreased to 0.00193.
2024-12-02-10:08:37-root-INFO: Loss too large (1996.722->2001.849)! Learning rate decreased to 0.00154.
2024-12-02-10:08:38-root-INFO: grad norm: 227.193 223.108 42.891
2024-12-02-10:08:38-root-INFO: Loss too large (1996.225->2067.281)! Learning rate decreased to 0.00123.
2024-12-02-10:08:38-root-INFO: Loss too large (1996.225->2030.330)! Learning rate decreased to 0.00099.
2024-12-02-10:08:38-root-INFO: Loss too large (1996.225->2008.352)! Learning rate decreased to 0.00079.
2024-12-02-10:08:39-root-INFO: grad norm: 254.378 250.570 43.851
2024-12-02-10:08:39-root-INFO: Loss too large (1995.989->1996.239)! Learning rate decreased to 0.00063.
2024-12-02-10:08:39-root-INFO: grad norm: 183.955 180.207 36.942
2024-12-02-10:08:40-root-INFO: grad norm: 138.119 135.537 26.580
2024-12-02-10:08:40-root-INFO: grad norm: 107.063 103.881 25.907
2024-12-02-10:08:41-root-INFO: grad norm: 87.061 84.612 20.504
2024-12-02-10:08:41-root-INFO: grad norm: 74.261 71.100 21.434
2024-12-02-10:08:41-root-INFO: Loss Change: 1996.722 -> 1972.269
2024-12-02-10:08:41-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-02-10:08:41-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-10:08:41-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:08:42-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-10:08:42-root-INFO: grad norm: 113.100 109.821 27.036
2024-12-02-10:08:42-root-INFO: Loss too large (1962.033->2074.286)! Learning rate decreased to 0.00314.
2024-12-02-10:08:42-root-INFO: Loss too large (1962.033->2027.962)! Learning rate decreased to 0.00251.
2024-12-02-10:08:42-root-INFO: Loss too large (1962.033->1998.533)! Learning rate decreased to 0.00201.
2024-12-02-10:08:42-root-INFO: Loss too large (1962.033->1980.469)! Learning rate decreased to 0.00161.
2024-12-02-10:08:43-root-INFO: Loss too large (1962.033->1969.763)! Learning rate decreased to 0.00129.
2024-12-02-10:08:43-root-INFO: Loss too large (1962.033->1963.685)! Learning rate decreased to 0.00103.
2024-12-02-10:08:43-root-INFO: grad norm: 163.333 159.498 35.188
2024-12-02-10:08:43-root-INFO: Loss too large (1960.443->1965.389)! Learning rate decreased to 0.00082.
2024-12-02-10:08:44-root-INFO: grad norm: 186.228 183.505 31.731
2024-12-02-10:08:44-root-INFO: grad norm: 214.422 210.404 41.313
2024-12-02-10:08:44-root-INFO: Loss too large (1958.654->1959.140)! Learning rate decreased to 0.00066.
2024-12-02-10:08:45-root-INFO: grad norm: 162.697 160.216 28.306
2024-12-02-10:08:45-root-INFO: grad norm: 126.023 122.787 28.374
2024-12-02-10:08:46-root-INFO: grad norm: 101.152 98.943 21.024
2024-12-02-10:08:46-root-INFO: grad norm: 84.117 81.028 22.587
2024-12-02-10:08:47-root-INFO: Loss Change: 1962.033 -> 1940.921
2024-12-02-10:08:47-root-INFO: Regularization Change: 0.000 -> 0.117
2024-12-02-10:08:47-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-10:08:47-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:08:47-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-10:08:47-root-INFO: grad norm: 218.861 213.147 49.686
2024-12-02-10:08:47-root-INFO: Loss too large (1948.496->2350.884)! Learning rate decreased to 0.00328.
2024-12-02-10:08:47-root-INFO: Loss too large (1948.496->2211.482)! Learning rate decreased to 0.00262.
2024-12-02-10:08:48-root-INFO: Loss too large (1948.496->2108.797)! Learning rate decreased to 0.00210.
2024-12-02-10:08:48-root-INFO: Loss too large (1948.496->2038.264)! Learning rate decreased to 0.00168.
2024-12-02-10:08:48-root-INFO: Loss too large (1948.496->1992.757)! Learning rate decreased to 0.00134.
2024-12-02-10:08:48-root-INFO: Loss too large (1948.496->1965.045)! Learning rate decreased to 0.00107.
2024-12-02-10:08:48-root-INFO: Loss too large (1948.496->1949.185)! Learning rate decreased to 0.00086.
2024-12-02-10:08:49-root-INFO: grad norm: 221.257 216.850 43.938
2024-12-02-10:08:49-root-INFO: grad norm: 257.546 253.931 43.001
2024-12-02-10:08:49-root-INFO: Loss too large (1940.534->1941.925)! Learning rate decreased to 0.00069.
2024-12-02-10:08:50-root-INFO: grad norm: 197.389 193.448 39.244
2024-12-02-10:08:50-root-INFO: grad norm: 155.306 152.721 28.217
2024-12-02-10:08:51-root-INFO: grad norm: 124.337 120.897 29.043
2024-12-02-10:08:51-root-INFO: grad norm: 102.334 100.006 21.700
2024-12-02-10:08:52-root-INFO: grad norm: 86.533 83.214 23.735
2024-12-02-10:08:52-root-INFO: Loss Change: 1948.496 -> 1914.388
2024-12-02-10:08:52-root-INFO: Regularization Change: 0.000 -> 0.147
2024-12-02-10:08:52-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-10:08:52-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:08:52-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-10:08:52-root-INFO: grad norm: 171.792 168.120 35.326
2024-12-02-10:08:52-root-INFO: Loss too large (1911.337->2229.600)! Learning rate decreased to 0.00342.
2024-12-02-10:08:53-root-INFO: Loss too large (1911.337->2114.716)! Learning rate decreased to 0.00273.
2024-12-02-10:08:53-root-INFO: Loss too large (1911.337->2033.735)! Learning rate decreased to 0.00219.
2024-12-02-10:08:53-root-INFO: Loss too large (1911.337->1979.982)! Learning rate decreased to 0.00175.
2024-12-02-10:08:53-root-INFO: Loss too large (1911.337->1946.076)! Learning rate decreased to 0.00140.
2024-12-02-10:08:53-root-INFO: Loss too large (1911.337->1925.664)! Learning rate decreased to 0.00112.
2024-12-02-10:08:53-root-INFO: Loss too large (1911.337->1913.986)! Learning rate decreased to 0.00090.
2024-12-02-10:08:54-root-INFO: grad norm: 186.074 182.442 36.585
2024-12-02-10:08:54-root-INFO: grad norm: 214.524 211.593 35.346
2024-12-02-10:08:54-root-INFO: Loss too large (1906.935->1907.046)! Learning rate decreased to 0.00072.
2024-12-02-10:08:55-root-INFO: grad norm: 163.169 159.835 32.815
2024-12-02-10:08:55-root-INFO: grad norm: 127.666 125.334 24.291
2024-12-02-10:08:56-root-INFO: grad norm: 102.433 99.322 25.051
2024-12-02-10:08:56-root-INFO: grad norm: 85.198 82.836 19.923
2024-12-02-10:08:57-root-INFO: grad norm: 73.466 70.297 21.343
2024-12-02-10:08:57-root-INFO: Loss Change: 1911.337 -> 1884.703
2024-12-02-10:08:57-root-INFO: Regularization Change: 0.000 -> 0.131
2024-12-02-10:08:57-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-10:08:57-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:08:57-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-10:08:57-root-INFO: grad norm: 138.410 135.294 29.204
2024-12-02-10:08:58-root-INFO: Loss too large (1881.877->2092.247)! Learning rate decreased to 0.00356.
2024-12-02-10:08:58-root-INFO: Loss too large (1881.877->2011.749)! Learning rate decreased to 0.00285.
2024-12-02-10:08:58-root-INFO: Loss too large (1881.877->1957.585)! Learning rate decreased to 0.00228.
2024-12-02-10:08:58-root-INFO: Loss too large (1881.877->1922.884)! Learning rate decreased to 0.00182.
2024-12-02-10:08:58-root-INFO: Loss too large (1881.877->1901.570)! Learning rate decreased to 0.00146.
2024-12-02-10:08:58-root-INFO: Loss too large (1881.877->1889.023)! Learning rate decreased to 0.00117.
2024-12-02-10:08:58-root-INFO: Loss too large (1881.877->1882.009)! Learning rate decreased to 0.00093.
2024-12-02-10:08:59-root-INFO: grad norm: 146.335 143.128 30.467
2024-12-02-10:08:59-root-INFO: grad norm: 166.282 164.054 27.131
2024-12-02-10:09:00-root-INFO: grad norm: 191.033 187.741 35.312
2024-12-02-10:09:00-root-INFO: grad norm: 219.421 216.992 32.554
2024-12-02-10:09:00-root-INFO: Loss too large (1875.681->1876.244)! Learning rate decreased to 0.00075.
2024-12-02-10:09:01-root-INFO: grad norm: 163.754 160.695 31.502
2024-12-02-10:09:01-root-INFO: grad norm: 125.839 123.980 21.550
2024-12-02-10:09:02-root-INFO: grad norm: 99.288 96.484 23.429
2024-12-02-10:09:02-root-INFO: Loss Change: 1881.877 -> 1858.294
2024-12-02-10:09:02-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-02-10:09:02-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-10:09:02-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-10:09:02-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-10:09:02-root-INFO: grad norm: 164.174 161.229 30.958
2024-12-02-10:09:03-root-INFO: Loss too large (1856.903->2160.692)! Learning rate decreased to 0.00371.
2024-12-02-10:09:03-root-INFO: Loss too large (1856.903->2051.805)! Learning rate decreased to 0.00297.
2024-12-02-10:09:03-root-INFO: Loss too large (1856.903->1974.304)! Learning rate decreased to 0.00238.
2024-12-02-10:09:03-root-INFO: Loss too large (1856.903->1922.606)! Learning rate decreased to 0.00190.
2024-12-02-10:09:03-root-INFO: Loss too large (1856.903->1889.934)! Learning rate decreased to 0.00152.
2024-12-02-10:09:03-root-INFO: Loss too large (1856.903->1870.264)! Learning rate decreased to 0.00122.
2024-12-02-10:09:04-root-INFO: Loss too large (1856.903->1859.027)! Learning rate decreased to 0.00097.
2024-12-02-10:09:04-root-INFO: grad norm: 168.294 164.927 33.495
2024-12-02-10:09:05-root-INFO: grad norm: 187.636 185.534 28.006
2024-12-02-10:09:05-root-INFO: grad norm: 210.648 207.204 37.932
2024-12-02-10:09:05-root-INFO: Loss too large (1850.865->1850.973)! Learning rate decreased to 0.00078.
2024-12-02-10:09:06-root-INFO: grad norm: 154.261 152.402 23.872
2024-12-02-10:09:06-root-INFO: grad norm: 115.472 112.580 25.682
2024-12-02-10:09:07-root-INFO: grad norm: 90.300 88.478 18.050
2024-12-02-10:09:07-root-INFO: grad norm: 73.914 71.012 20.508
2024-12-02-10:09:07-root-INFO: Loss Change: 1856.903 -> 1831.249
2024-12-02-10:09:07-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-02-10:09:07-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-10:09:07-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:09:08-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-10:09:08-root-INFO: grad norm: 167.032 163.003 36.465
2024-12-02-10:09:08-root-INFO: Loss too large (1835.595->2113.588)! Learning rate decreased to 0.00387.
2024-12-02-10:09:08-root-INFO: Loss too large (1835.595->2013.580)! Learning rate decreased to 0.00309.
2024-12-02-10:09:08-root-INFO: Loss too large (1835.595->1941.874)! Learning rate decreased to 0.00248.
2024-12-02-10:09:08-root-INFO: Loss too large (1835.595->1893.843)! Learning rate decreased to 0.00198.
2024-12-02-10:09:08-root-INFO: Loss too large (1835.595->1863.474)! Learning rate decreased to 0.00158.
2024-12-02-10:09:09-root-INFO: Loss too large (1835.595->1845.269)! Learning rate decreased to 0.00127.
2024-12-02-10:09:09-root-INFO: grad norm: 225.048 221.633 39.059
2024-12-02-10:09:09-root-INFO: Loss too large (1834.989->1848.671)! Learning rate decreased to 0.00101.
2024-12-02-10:09:10-root-INFO: grad norm: 243.516 240.954 35.231
2024-12-02-10:09:10-root-INFO: grad norm: 266.364 262.602 44.610
2024-12-02-10:09:10-root-INFO: Loss too large (1833.147->1833.865)! Learning rate decreased to 0.00081.
2024-12-02-10:09:11-root-INFO: grad norm: 186.282 184.216 27.664
2024-12-02-10:09:11-root-INFO: grad norm: 132.633 129.710 27.695
2024-12-02-10:09:12-root-INFO: grad norm: 99.150 97.335 18.885
2024-12-02-10:09:12-root-INFO: grad norm: 77.849 74.949 21.050
2024-12-02-10:09:12-root-INFO: Loss Change: 1835.595 -> 1806.035
2024-12-02-10:09:12-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-02-10:09:12-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-10:09:12-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:09:13-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-10:09:13-root-INFO: grad norm: 138.135 135.570 26.493
2024-12-02-10:09:13-root-INFO: Loss too large (1803.215->2033.203)! Learning rate decreased to 0.00403.
2024-12-02-10:09:13-root-INFO: Loss too large (1803.215->1947.403)! Learning rate decreased to 0.00322.
2024-12-02-10:09:13-root-INFO: Loss too large (1803.215->1888.032)! Learning rate decreased to 0.00258.
2024-12-02-10:09:13-root-INFO: Loss too large (1803.215->1849.390)! Learning rate decreased to 0.00206.
2024-12-02-10:09:14-root-INFO: Loss too large (1803.215->1825.468)! Learning rate decreased to 0.00165.
2024-12-02-10:09:14-root-INFO: Loss too large (1803.215->1811.328)! Learning rate decreased to 0.00132.
2024-12-02-10:09:14-root-INFO: Loss too large (1803.215->1803.409)! Learning rate decreased to 0.00106.
2024-12-02-10:09:14-root-INFO: grad norm: 136.773 133.949 27.648
2024-12-02-10:09:15-root-INFO: grad norm: 144.155 142.367 22.636
2024-12-02-10:09:15-root-INFO: grad norm: 153.528 150.729 29.182
2024-12-02-10:09:16-root-INFO: grad norm: 163.481 161.720 23.933
2024-12-02-10:09:16-root-INFO: grad norm: 174.040 171.207 31.276
2024-12-02-10:09:17-root-INFO: grad norm: 184.404 182.557 26.038
2024-12-02-10:09:17-root-INFO: grad norm: 194.855 191.934 33.613
2024-12-02-10:09:17-root-INFO: Loss Change: 1803.215 -> 1786.109
2024-12-02-10:09:17-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-02-10:09:17-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-10:09:17-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:09:17-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-10:09:18-root-INFO: grad norm: 282.597 279.214 43.600
2024-12-02-10:09:18-root-INFO: Loss too large (1792.817->2531.443)! Learning rate decreased to 0.00420.
2024-12-02-10:09:18-root-INFO: Loss too large (1792.817->2332.441)! Learning rate decreased to 0.00336.
2024-12-02-10:09:18-root-INFO: Loss too large (1792.817->2159.714)! Learning rate decreased to 0.00269.
2024-12-02-10:09:18-root-INFO: Loss too large (1792.817->2020.012)! Learning rate decreased to 0.00215.
2024-12-02-10:09:18-root-INFO: Loss too large (1792.817->1917.298)! Learning rate decreased to 0.00172.
2024-12-02-10:09:19-root-INFO: Loss too large (1792.817->1849.038)! Learning rate decreased to 0.00138.
2024-12-02-10:09:19-root-INFO: Loss too large (1792.817->1807.649)! Learning rate decreased to 0.00110.
2024-12-02-10:09:19-root-INFO: grad norm: 263.435 260.249 40.848
2024-12-02-10:09:20-root-INFO: grad norm: 252.635 250.137 35.440
2024-12-02-10:09:20-root-INFO: grad norm: 242.447 239.334 38.731
2024-12-02-10:09:21-root-INFO: grad norm: 232.774 230.548 32.115
2024-12-02-10:09:21-root-INFO: grad norm: 222.331 219.359 36.233
2024-12-02-10:09:22-root-INFO: grad norm: 212.277 210.254 29.236
2024-12-02-10:09:22-root-INFO: grad norm: 201.782 198.949 33.694
2024-12-02-10:09:22-root-INFO: Loss Change: 1792.817 -> 1754.881
2024-12-02-10:09:22-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-02-10:09:22-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-10:09:22-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:09:23-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-10:09:23-root-INFO: grad norm: 249.796 247.113 36.510
2024-12-02-10:09:23-root-INFO: Loss too large (1756.663->2401.575)! Learning rate decreased to 0.00437.
2024-12-02-10:09:23-root-INFO: Loss too large (1756.663->2217.188)! Learning rate decreased to 0.00350.
2024-12-02-10:09:23-root-INFO: Loss too large (1756.663->2061.527)! Learning rate decreased to 0.00280.
2024-12-02-10:09:23-root-INFO: Loss too large (1756.663->1940.181)! Learning rate decreased to 0.00224.
2024-12-02-10:09:23-root-INFO: Loss too large (1756.663->1854.331)! Learning rate decreased to 0.00179.
2024-12-02-10:09:24-root-INFO: Loss too large (1756.663->1799.032)! Learning rate decreased to 0.00143.
2024-12-02-10:09:24-root-INFO: Loss too large (1756.663->1766.250)! Learning rate decreased to 0.00115.
2024-12-02-10:09:24-root-INFO: grad norm: 220.826 217.961 35.457
2024-12-02-10:09:25-root-INFO: grad norm: 199.109 197.134 27.979
2024-12-02-10:09:25-root-INFO: grad norm: 180.248 177.596 30.810
2024-12-02-10:09:26-root-INFO: grad norm: 164.427 162.743 23.473
2024-12-02-10:09:26-root-INFO: grad norm: 149.698 147.216 27.148
2024-12-02-10:09:27-root-INFO: grad norm: 137.055 135.534 20.361
2024-12-02-10:09:27-root-INFO: grad norm: 125.364 122.991 24.278
2024-12-02-10:09:27-root-INFO: Loss Change: 1756.663 -> 1716.893
2024-12-02-10:09:27-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-02-10:09:27-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-10:09:27-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-10:09:27-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-10:09:28-root-INFO: grad norm: 201.353 198.464 33.988
2024-12-02-10:09:28-root-INFO: Loss too large (1722.083->2122.516)! Learning rate decreased to 0.00455.
2024-12-02-10:09:28-root-INFO: Loss too large (1722.083->1995.617)! Learning rate decreased to 0.00364.
2024-12-02-10:09:28-root-INFO: Loss too large (1722.083->1894.403)! Learning rate decreased to 0.00291.
2024-12-02-10:09:28-root-INFO: Loss too large (1722.083->1820.845)! Learning rate decreased to 0.00233.
2024-12-02-10:09:28-root-INFO: Loss too large (1722.083->1771.686)! Learning rate decreased to 0.00186.
2024-12-02-10:09:29-root-INFO: Loss too large (1722.083->1741.061)! Learning rate decreased to 0.00149.
2024-12-02-10:09:29-root-INFO: Loss too large (1722.083->1723.214)! Learning rate decreased to 0.00119.
2024-12-02-10:09:29-root-INFO: grad norm: 159.514 157.034 28.020
2024-12-02-10:09:30-root-INFO: grad norm: 129.549 127.804 21.195
2024-12-02-10:09:30-root-INFO: grad norm: 111.473 109.067 23.035
2024-12-02-10:09:30-root-INFO: grad norm: 97.531 95.954 17.469
2024-12-02-10:09:31-root-INFO: grad norm: 86.276 83.904 20.091
2024-12-02-10:09:32-root-INFO: grad norm: 77.533 75.909 15.785
2024-12-02-10:09:32-root-INFO: grad norm: 70.603 68.215 18.209
2024-12-02-10:09:32-root-INFO: Loss Change: 1722.083 -> 1683.183
2024-12-02-10:09:32-root-INFO: Regularization Change: 0.000 -> 0.268
2024-12-02-10:09:32-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-10:09:32-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:09:33-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-10:09:33-root-INFO: grad norm: 162.706 160.171 28.607
2024-12-02-10:09:33-root-INFO: Loss too large (1684.359->1933.801)! Learning rate decreased to 0.00474.
2024-12-02-10:09:33-root-INFO: Loss too large (1684.359->1847.391)! Learning rate decreased to 0.00379.
2024-12-02-10:09:33-root-INFO: Loss too large (1684.359->1783.727)! Learning rate decreased to 0.00303.
2024-12-02-10:09:33-root-INFO: Loss too large (1684.359->1739.854)! Learning rate decreased to 0.00243.
2024-12-02-10:09:33-root-INFO: Loss too large (1684.359->1711.230)! Learning rate decreased to 0.00194.
2024-12-02-10:09:34-root-INFO: Loss too large (1684.359->1693.488)! Learning rate decreased to 0.00155.
2024-12-02-10:09:34-root-INFO: grad norm: 176.988 174.462 29.793
2024-12-02-10:09:35-root-INFO: grad norm: 196.953 194.947 28.040
2024-12-02-10:09:35-root-INFO: Loss too large (1679.246->1680.484)! Learning rate decreased to 0.00124.
2024-12-02-10:09:35-root-INFO: grad norm: 152.980 150.538 27.228
2024-12-02-10:09:36-root-INFO: grad norm: 118.623 117.116 18.847
2024-12-02-10:09:36-root-INFO: grad norm: 96.508 94.231 20.839
2024-12-02-10:09:37-root-INFO: grad norm: 80.990 79.439 15.777
2024-12-02-10:09:37-root-INFO: grad norm: 70.648 68.341 17.905
2024-12-02-10:09:38-root-INFO: Loss Change: 1684.359 -> 1645.551
2024-12-02-10:09:38-root-INFO: Regularization Change: 0.000 -> 0.338
2024-12-02-10:09:38-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-10:09:38-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:09:38-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-10:09:38-root-INFO: grad norm: 80.662 78.703 17.669
2024-12-02-10:09:38-root-INFO: Loss too large (1644.264->1679.747)! Learning rate decreased to 0.00494.
2024-12-02-10:09:38-root-INFO: Loss too large (1644.264->1661.573)! Learning rate decreased to 0.00395.
2024-12-02-10:09:38-root-INFO: Loss too large (1644.264->1650.657)! Learning rate decreased to 0.00316.
2024-12-02-10:09:39-root-INFO: Loss too large (1644.264->1644.450)! Learning rate decreased to 0.00253.
2024-12-02-10:09:39-root-INFO: grad norm: 140.165 137.716 26.084
2024-12-02-10:09:39-root-INFO: Loss too large (1641.189->1662.252)! Learning rate decreased to 0.00202.
2024-12-02-10:09:39-root-INFO: Loss too large (1641.189->1646.333)! Learning rate decreased to 0.00162.
2024-12-02-10:09:40-root-INFO: grad norm: 160.612 158.986 22.797
2024-12-02-10:09:40-root-INFO: grad norm: 177.269 174.574 30.796
2024-12-02-10:09:41-root-INFO: grad norm: 197.731 195.896 26.875
2024-12-02-10:09:41-root-INFO: grad norm: 209.950 206.966 35.276
2024-12-02-10:09:42-root-INFO: grad norm: 221.517 219.539 29.536
2024-12-02-10:09:42-root-INFO: grad norm: 226.688 223.533 37.694
2024-12-02-10:09:43-root-INFO: Loss Change: 1644.264 -> 1617.424
2024-12-02-10:09:43-root-INFO: Regularization Change: 0.000 -> 0.628
2024-12-02-10:09:43-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-10:09:43-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:09:43-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-10:09:43-root-INFO: grad norm: 360.719 356.633 54.139
2024-12-02-10:09:43-root-INFO: Loss too large (1640.348->2238.870)! Learning rate decreased to 0.00514.
2024-12-02-10:09:43-root-INFO: Loss too large (1640.348->2087.106)! Learning rate decreased to 0.00411.
2024-12-02-10:09:43-root-INFO: Loss too large (1640.348->1950.012)! Learning rate decreased to 0.00329.
2024-12-02-10:09:44-root-INFO: Loss too large (1640.348->1834.905)! Learning rate decreased to 0.00263.
2024-12-02-10:09:44-root-INFO: Loss too large (1640.348->1746.333)! Learning rate decreased to 0.00210.
2024-12-02-10:09:44-root-INFO: Loss too large (1640.348->1683.360)! Learning rate decreased to 0.00168.
2024-12-02-10:09:44-root-INFO: Loss too large (1640.348->1641.510)! Learning rate decreased to 0.00135.
2024-12-02-10:09:45-root-INFO: grad norm: 205.342 202.407 34.591
2024-12-02-10:09:45-root-INFO: grad norm: 83.901 82.020 17.666
2024-12-02-10:09:46-root-INFO: grad norm: 65.153 62.729 17.607
2024-12-02-10:09:46-root-INFO: grad norm: 59.319 57.165 15.841
2024-12-02-10:09:47-root-INFO: grad norm: 56.927 54.702 15.761
2024-12-02-10:09:47-root-INFO: grad norm: 55.498 53.378 15.194
2024-12-02-10:09:48-root-INFO: grad norm: 54.425 52.312 15.019
2024-12-02-10:09:48-root-INFO: Loss Change: 1640.348 -> 1559.052
2024-12-02-10:09:48-root-INFO: Regularization Change: 0.000 -> 0.462
2024-12-02-10:09:48-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-10:09:48-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-10:09:48-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-10:09:48-root-INFO: grad norm: 74.138 71.967 17.810
2024-12-02-10:09:49-root-INFO: Loss too large (1553.941->1571.788)! Learning rate decreased to 0.00535.
2024-12-02-10:09:49-root-INFO: Loss too large (1553.941->1562.249)! Learning rate decreased to 0.00428.
2024-12-02-10:09:49-root-INFO: Loss too large (1553.941->1556.032)! Learning rate decreased to 0.00342.
2024-12-02-10:09:49-root-INFO: grad norm: 141.342 139.111 25.011
2024-12-02-10:09:50-root-INFO: Loss too large (1552.288->1611.307)! Learning rate decreased to 0.00274.
2024-12-02-10:09:50-root-INFO: Loss too large (1552.288->1575.349)! Learning rate decreased to 0.00219.
2024-12-02-10:09:50-root-INFO: Loss too large (1552.288->1555.433)! Learning rate decreased to 0.00175.
2024-12-02-10:09:50-root-INFO: grad norm: 155.850 153.778 25.328
2024-12-02-10:09:51-root-INFO: Loss too large (1545.500->1547.030)! Learning rate decreased to 0.00140.
2024-12-02-10:09:51-root-INFO: grad norm: 122.646 120.673 21.911
2024-12-02-10:09:51-root-INFO: grad norm: 86.365 84.596 17.391
2024-12-02-10:09:52-root-INFO: grad norm: 76.052 74.348 16.006
2024-12-02-10:09:52-root-INFO: grad norm: 66.192 64.443 15.113
2024-12-02-10:09:53-root-INFO: grad norm: 61.083 59.371 14.358
2024-12-02-10:09:53-root-INFO: Loss Change: 1553.941 -> 1518.227
2024-12-02-10:09:53-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-02-10:09:53-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-10:09:53-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:09:53-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-10:09:54-root-INFO: grad norm: 120.532 118.461 22.247
2024-12-02-10:09:54-root-INFO: Loss too large (1515.191->1609.169)! Learning rate decreased to 0.00556.
2024-12-02-10:09:54-root-INFO: Loss too large (1515.191->1582.885)! Learning rate decreased to 0.00445.
2024-12-02-10:09:54-root-INFO: Loss too large (1515.191->1560.492)! Learning rate decreased to 0.00356.
2024-12-02-10:09:54-root-INFO: Loss too large (1515.191->1542.611)! Learning rate decreased to 0.00285.
2024-12-02-10:09:55-root-INFO: Loss too large (1515.191->1529.327)! Learning rate decreased to 0.00228.
2024-12-02-10:09:55-root-INFO: Loss too large (1515.191->1520.181)! Learning rate decreased to 0.00182.
2024-12-02-10:09:55-root-INFO: grad norm: 134.055 132.034 23.185
2024-12-02-10:09:56-root-INFO: grad norm: 166.415 164.234 26.857
2024-12-02-10:09:56-root-INFO: Loss too large (1510.196->1515.030)! Learning rate decreased to 0.00146.
2024-12-02-10:09:56-root-INFO: grad norm: 136.846 134.829 23.411
2024-12-02-10:09:57-root-INFO: grad norm: 100.134 98.373 18.696
2024-12-02-10:09:57-root-INFO: grad norm: 90.779 89.117 17.296
2024-12-02-10:09:58-root-INFO: grad norm: 80.289 78.628 16.245
2024-12-02-10:09:58-root-INFO: grad norm: 74.902 73.290 15.452
2024-12-02-10:09:59-root-INFO: Loss Change: 1515.191 -> 1483.316
2024-12-02-10:09:59-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-02-10:09:59-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-10:09:59-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:09:59-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-10:09:59-root-INFO: grad norm: 153.733 150.910 29.327
2024-12-02-10:09:59-root-INFO: Loss too large (1487.937->1609.925)! Learning rate decreased to 0.00579.
2024-12-02-10:09:59-root-INFO: Loss too large (1487.937->1584.151)! Learning rate decreased to 0.00463.
2024-12-02-10:10:00-root-INFO: Loss too large (1487.937->1558.917)! Learning rate decreased to 0.00370.
2024-12-02-10:10:00-root-INFO: Loss too large (1487.937->1535.733)! Learning rate decreased to 0.00296.
2024-12-02-10:10:00-root-INFO: Loss too large (1487.937->1516.175)! Learning rate decreased to 0.00237.
2024-12-02-10:10:00-root-INFO: Loss too large (1487.937->1501.145)! Learning rate decreased to 0.00190.
2024-12-02-10:10:00-root-INFO: Loss too large (1487.937->1490.658)! Learning rate decreased to 0.00152.
2024-12-02-10:10:01-root-INFO: grad norm: 134.136 132.357 21.778
2024-12-02-10:10:01-root-INFO: grad norm: 111.945 109.775 21.934
2024-12-02-10:10:02-root-INFO: grad norm: 104.535 102.951 18.133
2024-12-02-10:10:02-root-INFO: grad norm: 95.546 93.651 18.936
2024-12-02-10:10:03-root-INFO: grad norm: 90.860 89.327 16.618
2024-12-02-10:10:03-root-INFO: grad norm: 85.237 83.478 17.228
2024-12-02-10:10:04-root-INFO: grad norm: 81.830 80.308 15.711
2024-12-02-10:10:04-root-INFO: Loss Change: 1487.937 -> 1451.879
2024-12-02-10:10:04-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-02-10:10:04-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-10:10:04-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:10:04-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-10:10:04-root-INFO: grad norm: 146.845 144.260 27.432
2024-12-02-10:10:05-root-INFO: Loss too large (1455.823->1579.089)! Learning rate decreased to 0.00602.
2024-12-02-10:10:05-root-INFO: Loss too large (1455.823->1554.752)! Learning rate decreased to 0.00482.
2024-12-02-10:10:05-root-INFO: Loss too large (1455.823->1529.672)! Learning rate decreased to 0.00385.
2024-12-02-10:10:05-root-INFO: Loss too large (1455.823->1506.005)! Learning rate decreased to 0.00308.
2024-12-02-10:10:05-root-INFO: Loss too large (1455.823->1485.812)! Learning rate decreased to 0.00247.
2024-12-02-10:10:05-root-INFO: Loss too large (1455.823->1470.258)! Learning rate decreased to 0.00197.
2024-12-02-10:10:05-root-INFO: Loss too large (1455.823->1459.428)! Learning rate decreased to 0.00158.
2024-12-02-10:10:06-root-INFO: grad norm: 138.802 137.023 22.151
2024-12-02-10:10:06-root-INFO: grad norm: 128.589 126.443 23.392
2024-12-02-10:10:07-root-INFO: grad norm: 124.544 122.863 20.394
2024-12-02-10:10:07-root-INFO: grad norm: 118.588 116.635 21.432
2024-12-02-10:10:08-root-INFO: grad norm: 115.618 113.951 19.562
2024-12-02-10:10:08-root-INFO: grad norm: 111.058 109.214 20.151
2024-12-02-10:10:09-root-INFO: grad norm: 108.700 106.993 19.188
2024-12-02-10:10:09-root-INFO: Loss Change: 1455.823 -> 1414.946
2024-12-02-10:10:09-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-02-10:10:09-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-10:10:09-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:10:09-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-10:10:09-root-INFO: grad norm: 151.984 149.791 25.729
2024-12-02-10:10:09-root-INFO: Loss too large (1416.594->1551.595)! Learning rate decreased to 0.00626.
2024-12-02-10:10:10-root-INFO: Loss too large (1416.594->1523.142)! Learning rate decreased to 0.00501.
2024-12-02-10:10:10-root-INFO: Loss too large (1416.594->1494.273)! Learning rate decreased to 0.00401.
2024-12-02-10:10:10-root-INFO: Loss too large (1416.594->1467.482)! Learning rate decreased to 0.00321.
2024-12-02-10:10:10-root-INFO: Loss too large (1416.594->1445.078)! Learning rate decreased to 0.00256.
2024-12-02-10:10:10-root-INFO: Loss too large (1416.594->1428.262)! Learning rate decreased to 0.00205.
2024-12-02-10:10:10-root-INFO: Loss too large (1416.594->1416.959)! Learning rate decreased to 0.00164.
2024-12-02-10:10:11-root-INFO: grad norm: 145.100 143.026 24.443
2024-12-02-10:10:11-root-INFO: grad norm: 135.715 133.727 23.146
2024-12-02-10:10:12-root-INFO: grad norm: 132.103 129.830 24.403
2024-12-02-10:10:12-root-INFO: grad norm: 125.374 123.430 21.996
2024-12-02-10:10:13-root-INFO: grad norm: 120.077 117.537 24.569
2024-12-02-10:10:13-root-INFO: grad norm: 114.762 112.974 20.181
2024-12-02-10:10:14-root-INFO: grad norm: 112.339 109.720 24.119
2024-12-02-10:10:14-root-INFO: Loss Change: 1416.594 -> 1351.269
2024-12-02-10:10:14-root-INFO: Regularization Change: 0.000 -> 0.837
2024-12-02-10:10:14-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-10:10:14-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-10:10:14-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-10:10:14-root-INFO: grad norm: 172.078 169.732 28.318
2024-12-02-10:10:15-root-INFO: Loss too large (1357.346->1548.447)! Learning rate decreased to 0.00651.
2024-12-02-10:10:15-root-INFO: Loss too large (1357.346->1518.240)! Learning rate decreased to 0.00521.
2024-12-02-10:10:15-root-INFO: Loss too large (1357.346->1483.483)! Learning rate decreased to 0.00417.
2024-12-02-10:10:15-root-INFO: Loss too large (1357.346->1446.630)! Learning rate decreased to 0.00333.
2024-12-02-10:10:15-root-INFO: Loss too large (1357.346->1412.056)! Learning rate decreased to 0.00267.
2024-12-02-10:10:15-root-INFO: Loss too large (1357.346->1383.896)! Learning rate decreased to 0.00213.
2024-12-02-10:10:15-root-INFO: Loss too large (1357.346->1364.041)! Learning rate decreased to 0.00171.
2024-12-02-10:10:16-root-INFO: grad norm: 176.085 173.311 31.130
2024-12-02-10:10:16-root-INFO: grad norm: 181.666 179.668 26.870
2024-12-02-10:10:17-root-INFO: grad norm: 186.119 183.378 31.826
2024-12-02-10:10:17-root-INFO: grad norm: 187.872 185.925 26.975
2024-12-02-10:10:18-root-INFO: grad norm: 187.951 185.282 31.560
2024-12-02-10:10:18-root-INFO: grad norm: 185.605 183.695 26.561
2024-12-02-10:10:19-root-INFO: grad norm: 182.693 180.141 30.434
2024-12-02-10:10:19-root-INFO: Loss Change: 1357.346 -> 1324.519
2024-12-02-10:10:19-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-02-10:10:19-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-10:10:19-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:10:19-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-10:10:20-root-INFO: grad norm: 230.154 227.680 33.654
2024-12-02-10:10:20-root-INFO: Loss too large (1333.463->1554.855)! Learning rate decreased to 0.00677.
2024-12-02-10:10:20-root-INFO: Loss too large (1333.463->1528.907)! Learning rate decreased to 0.00542.
2024-12-02-10:10:20-root-INFO: Loss too large (1333.463->1497.476)! Learning rate decreased to 0.00433.
2024-12-02-10:10:20-root-INFO: Loss too large (1333.463->1459.232)! Learning rate decreased to 0.00347.
2024-12-02-10:10:20-root-INFO: Loss too large (1333.463->1416.528)! Learning rate decreased to 0.00277.
2024-12-02-10:10:20-root-INFO: Loss too large (1333.463->1375.144)! Learning rate decreased to 0.00222.
2024-12-02-10:10:21-root-INFO: Loss too large (1333.463->1341.776)! Learning rate decreased to 0.00177.
2024-12-02-10:10:21-root-INFO: grad norm: 201.964 199.411 32.007
2024-12-02-10:10:22-root-INFO: grad norm: 184.782 182.813 26.899
2024-12-02-10:10:22-root-INFO: grad norm: 171.061 168.812 27.644
2024-12-02-10:10:23-root-INFO: grad norm: 161.879 160.118 23.816
2024-12-02-10:10:23-root-INFO: grad norm: 153.598 151.531 25.113
2024-12-02-10:10:24-root-INFO: grad norm: 147.780 146.133 22.001
2024-12-02-10:10:24-root-INFO: grad norm: 142.452 140.508 23.454
2024-12-02-10:10:25-root-INFO: Loss Change: 1333.463 -> 1283.952
2024-12-02-10:10:25-root-INFO: Regularization Change: 0.000 -> 0.462
2024-12-02-10:10:25-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-10:10:25-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:10:25-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-10:10:25-root-INFO: grad norm: 173.969 171.923 26.599
2024-12-02-10:10:25-root-INFO: Loss too large (1288.782->1504.468)! Learning rate decreased to 0.00704.
2024-12-02-10:10:25-root-INFO: Loss too large (1288.782->1473.425)! Learning rate decreased to 0.00563.
2024-12-02-10:10:25-root-INFO: Loss too large (1288.782->1435.864)! Learning rate decreased to 0.00450.
2024-12-02-10:10:25-root-INFO: Loss too large (1288.782->1393.957)! Learning rate decreased to 0.00360.
2024-12-02-10:10:26-root-INFO: Loss too large (1288.782->1352.835)! Learning rate decreased to 0.00288.
2024-12-02-10:10:26-root-INFO: Loss too large (1288.782->1318.599)! Learning rate decreased to 0.00231.
2024-12-02-10:10:26-root-INFO: Loss too large (1288.782->1294.700)! Learning rate decreased to 0.00184.
2024-12-02-10:10:26-root-INFO: grad norm: 165.705 163.709 25.642
2024-12-02-10:10:27-root-INFO: grad norm: 158.777 157.001 23.680
2024-12-02-10:10:27-root-INFO: grad norm: 150.662 148.793 23.659
2024-12-02-10:10:28-root-INFO: grad norm: 145.248 143.609 21.757
2024-12-02-10:10:28-root-INFO: grad norm: 139.414 137.644 22.146
2024-12-02-10:10:29-root-INFO: grad norm: 135.777 134.219 20.512
2024-12-02-10:10:29-root-INFO: grad norm: 132.145 130.442 21.147
2024-12-02-10:10:30-root-INFO: Loss Change: 1288.782 -> 1252.414
2024-12-02-10:10:30-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-02-10:10:30-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-10:10:30-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:10:30-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-10:10:30-root-INFO: grad norm: 171.425 169.325 26.747
2024-12-02-10:10:30-root-INFO: Loss too large (1259.603->1480.836)! Learning rate decreased to 0.00732.
2024-12-02-10:10:30-root-INFO: Loss too large (1259.603->1449.564)! Learning rate decreased to 0.00585.
2024-12-02-10:10:30-root-INFO: Loss too large (1259.603->1410.617)! Learning rate decreased to 0.00468.
2024-12-02-10:10:31-root-INFO: Loss too large (1259.603->1366.347)! Learning rate decreased to 0.00375.
2024-12-02-10:10:31-root-INFO: Loss too large (1259.603->1322.671)! Learning rate decreased to 0.00300.
2024-12-02-10:10:31-root-INFO: Loss too large (1259.603->1286.751)! Learning rate decreased to 0.00240.
2024-12-02-10:10:31-root-INFO: Loss too large (1259.603->1262.407)! Learning rate decreased to 0.00192.
2024-12-02-10:10:31-root-INFO: grad norm: 155.818 153.897 24.389
2024-12-02-10:10:32-root-INFO: grad norm: 147.906 146.172 22.583
2024-12-02-10:10:32-root-INFO: grad norm: 138.655 136.945 21.713
2024-12-02-10:10:33-root-INFO: grad norm: 133.692 132.098 20.582
2024-12-02-10:10:33-root-INFO: grad norm: 128.168 126.564 20.211
2024-12-02-10:10:34-root-INFO: grad norm: 125.243 123.724 19.449
2024-12-02-10:10:34-root-INFO: grad norm: 122.310 120.761 19.403
2024-12-02-10:10:35-root-INFO: Loss Change: 1259.603 -> 1220.560
2024-12-02-10:10:35-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-10:10:35-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-10:10:35-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-10:10:35-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-10:10:35-root-INFO: grad norm: 154.671 152.921 23.197
2024-12-02-10:10:35-root-INFO: Loss too large (1222.387->1443.014)! Learning rate decreased to 0.00760.
2024-12-02-10:10:35-root-INFO: Loss too large (1222.387->1409.930)! Learning rate decreased to 0.00608.
2024-12-02-10:10:35-root-INFO: Loss too large (1222.387->1368.939)! Learning rate decreased to 0.00487.
2024-12-02-10:10:36-root-INFO: Loss too large (1222.387->1323.428)! Learning rate decreased to 0.00389.
2024-12-02-10:10:36-root-INFO: Loss too large (1222.387->1280.219)! Learning rate decreased to 0.00311.
2024-12-02-10:10:36-root-INFO: Loss too large (1222.387->1246.334)! Learning rate decreased to 0.00249.
2024-12-02-10:10:36-root-INFO: Loss too large (1222.387->1224.422)! Learning rate decreased to 0.00199.
2024-12-02-10:10:37-root-INFO: grad norm: 140.664 139.032 21.370
2024-12-02-10:10:37-root-INFO: grad norm: 135.253 133.738 20.193
2024-12-02-10:10:38-root-INFO: grad norm: 128.882 127.331 19.934
2024-12-02-10:10:38-root-INFO: grad norm: 126.009 124.564 19.026
2024-12-02-10:10:39-root-INFO: grad norm: 122.871 121.362 19.197
2024-12-02-10:10:39-root-INFO: grad norm: 121.530 120.118 18.476
2024-12-02-10:10:40-root-INFO: grad norm: 120.394 118.903 18.889
2024-12-02-10:10:40-root-INFO: Loss Change: 1222.387 -> 1188.185
2024-12-02-10:10:40-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-02-10:10:40-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-10:10:40-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:10:40-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-10:10:40-root-INFO: grad norm: 162.490 160.106 27.735
2024-12-02-10:10:40-root-INFO: Loss too large (1201.351->1432.078)! Learning rate decreased to 0.00790.
2024-12-02-10:10:40-root-INFO: Loss too large (1201.351->1399.156)! Learning rate decreased to 0.00632.
2024-12-02-10:10:41-root-INFO: Loss too large (1201.351->1357.869)! Learning rate decreased to 0.00506.
2024-12-02-10:10:41-root-INFO: Loss too large (1201.351->1310.363)! Learning rate decreased to 0.00404.
2024-12-02-10:10:41-root-INFO: Loss too large (1201.351->1263.424)! Learning rate decreased to 0.00324.
2024-12-02-10:10:41-root-INFO: Loss too large (1201.351->1225.611)! Learning rate decreased to 0.00259.
2024-12-02-10:10:41-root-INFO: grad norm: 226.064 223.749 32.268
2024-12-02-10:10:42-root-INFO: Loss too large (1201.066->1222.536)! Learning rate decreased to 0.00207.
2024-12-02-10:10:42-root-INFO: grad norm: 185.239 183.208 27.359
2024-12-02-10:10:43-root-INFO: grad norm: 130.161 128.682 19.566
2024-12-02-10:10:43-root-INFO: grad norm: 119.711 118.083 19.679
2024-12-02-10:10:43-root-INFO: grad norm: 107.897 106.587 16.757
2024-12-02-10:10:44-root-INFO: grad norm: 103.430 101.939 17.498
2024-12-02-10:10:44-root-INFO: grad norm: 98.904 97.655 15.666
2024-12-02-10:10:45-root-INFO: Loss Change: 1201.351 -> 1158.127
2024-12-02-10:10:45-root-INFO: Regularization Change: 0.000 -> 0.545
2024-12-02-10:10:45-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-10:10:45-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:10:45-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-10:10:45-root-INFO: grad norm: 129.291 127.730 20.032
2024-12-02-10:10:45-root-INFO: Loss too large (1160.791->1375.663)! Learning rate decreased to 0.00821.
2024-12-02-10:10:45-root-INFO: Loss too large (1160.791->1336.870)! Learning rate decreased to 0.00656.
2024-12-02-10:10:46-root-INFO: Loss too large (1160.791->1291.262)! Learning rate decreased to 0.00525.
2024-12-02-10:10:46-root-INFO: Loss too large (1160.791->1244.396)! Learning rate decreased to 0.00420.
2024-12-02-10:10:46-root-INFO: Loss too large (1160.791->1204.278)! Learning rate decreased to 0.00336.
2024-12-02-10:10:46-root-INFO: Loss too large (1160.791->1176.140)! Learning rate decreased to 0.00269.
2024-12-02-10:10:46-root-INFO: grad norm: 179.875 177.913 26.493
2024-12-02-10:10:47-root-INFO: Loss too large (1159.659->1173.897)! Learning rate decreased to 0.00215.
2024-12-02-10:10:47-root-INFO: grad norm: 151.900 150.376 21.462
2024-12-02-10:10:48-root-INFO: grad norm: 112.716 111.315 17.715
2024-12-02-10:10:48-root-INFO: grad norm: 103.711 102.402 16.428
2024-12-02-10:10:49-root-INFO: grad norm: 93.349 92.080 15.343
2024-12-02-10:10:49-root-INFO: grad norm: 89.359 88.107 14.910
2024-12-02-10:10:49-root-INFO: grad norm: 85.305 84.085 14.374
2024-12-02-10:10:50-root-INFO: Loss Change: 1160.791 -> 1125.890
2024-12-02-10:10:50-root-INFO: Regularization Change: 0.000 -> 0.496
2024-12-02-10:10:50-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-10:10:50-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:10:50-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-10:10:50-root-INFO: grad norm: 122.054 120.294 20.651
2024-12-02-10:10:50-root-INFO: Loss too large (1132.946->1349.437)! Learning rate decreased to 0.00852.
2024-12-02-10:10:50-root-INFO: Loss too large (1132.946->1308.101)! Learning rate decreased to 0.00682.
2024-12-02-10:10:51-root-INFO: Loss too large (1132.946->1260.393)! Learning rate decreased to 0.00545.
2024-12-02-10:10:51-root-INFO: Loss too large (1132.946->1212.536)! Learning rate decreased to 0.00436.
2024-12-02-10:10:51-root-INFO: Loss too large (1132.946->1172.923)! Learning rate decreased to 0.00349.
2024-12-02-10:10:51-root-INFO: Loss too large (1132.946->1146.161)! Learning rate decreased to 0.00279.
2024-12-02-10:10:52-root-INFO: grad norm: 171.660 169.860 24.792
2024-12-02-10:10:52-root-INFO: Loss too large (1131.001->1145.055)! Learning rate decreased to 0.00223.
2024-12-02-10:10:52-root-INFO: Loss too large (1131.001->1131.186)! Learning rate decreased to 0.00179.
2024-12-02-10:10:52-root-INFO: grad norm: 108.966 107.526 17.656
2024-12-02-10:10:53-root-INFO: grad norm: 56.596 55.440 11.379
2024-12-02-10:10:53-root-INFO: grad norm: 46.435 45.070 11.177
2024-12-02-10:10:54-root-INFO: grad norm: 41.136 39.852 10.195
2024-12-02-10:10:54-root-INFO: grad norm: 39.127 37.752 10.280
2024-12-02-10:10:55-root-INFO: grad norm: 38.101 36.774 9.969
2024-12-02-10:10:55-root-INFO: Loss Change: 1132.946 -> 1099.538
2024-12-02-10:10:55-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-02-10:10:55-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-10:10:55-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:10:55-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-10:10:55-root-INFO: grad norm: 57.563 56.179 12.548
2024-12-02-10:10:56-root-INFO: Loss too large (1098.425->1159.283)! Learning rate decreased to 0.00885.
2024-12-02-10:10:56-root-INFO: Loss too large (1098.425->1131.795)! Learning rate decreased to 0.00708.
2024-12-02-10:10:56-root-INFO: Loss too large (1098.425->1114.264)! Learning rate decreased to 0.00567.
2024-12-02-10:10:56-root-INFO: Loss too large (1098.425->1104.052)! Learning rate decreased to 0.00453.
2024-12-02-10:10:56-root-INFO: Loss too large (1098.425->1098.537)! Learning rate decreased to 0.00363.
2024-12-02-10:10:57-root-INFO: grad norm: 100.802 99.515 16.054
2024-12-02-10:10:57-root-INFO: Loss too large (1095.819->1107.572)! Learning rate decreased to 0.00290.
2024-12-02-10:10:57-root-INFO: Loss too large (1095.819->1099.353)! Learning rate decreased to 0.00232.
2024-12-02-10:10:57-root-INFO: grad norm: 98.969 97.753 15.466
2024-12-02-10:10:58-root-INFO: grad norm: 96.033 94.778 15.473
2024-12-02-10:10:58-root-INFO: grad norm: 94.739 93.536 15.046
2024-12-02-10:10:59-root-INFO: grad norm: 92.968 91.734 15.094
2024-12-02-10:10:59-root-INFO: grad norm: 92.282 91.093 14.764
2024-12-02-10:11:00-root-INFO: grad norm: 91.551 90.331 14.896
2024-12-02-10:11:00-root-INFO: Loss Change: 1098.425 -> 1074.856
2024-12-02-10:11:00-root-INFO: Regularization Change: 0.000 -> 0.546
2024-12-02-10:11:00-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-10:11:00-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-10:11:00-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-10:11:01-root-INFO: grad norm: 123.761 122.171 19.779
2024-12-02-10:11:01-root-INFO: Loss too large (1081.309->1313.315)! Learning rate decreased to 0.00919.
2024-12-02-10:11:01-root-INFO: Loss too large (1081.309->1272.953)! Learning rate decreased to 0.00735.
2024-12-02-10:11:01-root-INFO: Loss too large (1081.309->1224.313)! Learning rate decreased to 0.00588.
2024-12-02-10:11:01-root-INFO: Loss too large (1081.309->1172.664)! Learning rate decreased to 0.00471.
2024-12-02-10:11:01-root-INFO: Loss too large (1081.309->1127.476)! Learning rate decreased to 0.00376.
2024-12-02-10:11:02-root-INFO: Loss too large (1081.309->1096.141)! Learning rate decreased to 0.00301.
2024-12-02-10:11:02-root-INFO: grad norm: 177.446 175.625 25.353
2024-12-02-10:11:02-root-INFO: Loss too large (1078.513->1096.524)! Learning rate decreased to 0.00241.
2024-12-02-10:11:02-root-INFO: Loss too large (1078.513->1081.073)! Learning rate decreased to 0.00193.
2024-12-02-10:11:03-root-INFO: grad norm: 112.850 111.577 16.900
2024-12-02-10:11:03-root-INFO: grad norm: 51.345 50.216 10.706
2024-12-02-10:11:04-root-INFO: grad norm: 42.977 41.710 10.360
2024-12-02-10:11:04-root-INFO: grad norm: 38.528 37.292 9.679
2024-12-02-10:11:05-root-INFO: grad norm: 36.822 35.527 9.678
2024-12-02-10:11:05-root-INFO: grad norm: 35.926 34.661 9.451
2024-12-02-10:11:06-root-INFO: Loss Change: 1081.309 -> 1046.598
2024-12-02-10:11:06-root-INFO: Regularization Change: 0.000 -> 0.411
2024-12-02-10:11:06-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-10:11:06-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:11:06-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-10:11:06-root-INFO: grad norm: 71.878 70.186 15.508
2024-12-02-10:11:06-root-INFO: Loss too large (1051.182->1182.076)! Learning rate decreased to 0.00954.
2024-12-02-10:11:06-root-INFO: Loss too large (1051.182->1135.641)! Learning rate decreased to 0.00763.
2024-12-02-10:11:06-root-INFO: Loss too large (1051.182->1098.691)! Learning rate decreased to 0.00611.
2024-12-02-10:11:07-root-INFO: Loss too large (1051.182->1073.874)! Learning rate decreased to 0.00489.
2024-12-02-10:11:07-root-INFO: Loss too large (1051.182->1059.214)! Learning rate decreased to 0.00391.
2024-12-02-10:11:07-root-INFO: Loss too large (1051.182->1051.352)! Learning rate decreased to 0.00313.
2024-12-02-10:11:07-root-INFO: grad norm: 100.888 99.658 15.704
2024-12-02-10:11:08-root-INFO: Loss too large (1047.542->1051.198)! Learning rate decreased to 0.00250.
2024-12-02-10:11:08-root-INFO: grad norm: 96.936 95.610 15.979
2024-12-02-10:11:08-root-INFO: grad norm: 89.695 88.578 14.109
2024-12-02-10:11:09-root-INFO: grad norm: 86.701 85.456 14.645
2024-12-02-10:11:09-root-INFO: grad norm: 81.943 80.879 13.160
2024-12-02-10:11:10-root-INFO: grad norm: 79.917 78.716 13.800
2024-12-02-10:11:10-root-INFO: grad norm: 77.019 75.982 12.594
2024-12-02-10:11:11-root-INFO: Loss Change: 1051.182 -> 1024.023
2024-12-02-10:11:11-root-INFO: Regularization Change: 0.000 -> 0.575
2024-12-02-10:11:11-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-10:11:11-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:11:11-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-10:11:11-root-INFO: grad norm: 99.863 98.531 16.259
2024-12-02-10:11:11-root-INFO: Loss too large (1026.595->1245.735)! Learning rate decreased to 0.00991.
2024-12-02-10:11:11-root-INFO: Loss too large (1026.595->1199.463)! Learning rate decreased to 0.00792.
2024-12-02-10:11:11-root-INFO: Loss too large (1026.595->1147.183)! Learning rate decreased to 0.00634.
2024-12-02-10:11:12-root-INFO: Loss too large (1026.595->1097.077)! Learning rate decreased to 0.00507.
2024-12-02-10:11:12-root-INFO: Loss too large (1026.595->1058.865)! Learning rate decreased to 0.00406.
2024-12-02-10:11:12-root-INFO: Loss too large (1026.595->1035.509)! Learning rate decreased to 0.00325.
2024-12-02-10:11:12-root-INFO: grad norm: 139.407 137.964 20.005
2024-12-02-10:11:12-root-INFO: Loss too large (1023.447->1034.142)! Learning rate decreased to 0.00260.
2024-12-02-10:11:13-root-INFO: Loss too large (1023.447->1024.080)! Learning rate decreased to 0.00208.
2024-12-02-10:11:13-root-INFO: grad norm: 88.840 87.703 14.170
2024-12-02-10:11:14-root-INFO: grad norm: 43.238 42.080 9.940
2024-12-02-10:11:14-root-INFO: grad norm: 37.515 36.230 9.732
2024-12-02-10:11:15-root-INFO: grad norm: 34.925 33.646 9.367
2024-12-02-10:11:15-root-INFO: grad norm: 34.032 32.722 9.352
2024-12-02-10:11:16-root-INFO: grad norm: 33.591 32.294 9.242
2024-12-02-10:11:16-root-INFO: Loss Change: 1026.595 -> 997.547
2024-12-02-10:11:16-root-INFO: Regularization Change: 0.000 -> 0.398
2024-12-02-10:11:16-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-10:11:16-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:11:16-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-10:11:16-root-INFO: grad norm: 57.424 56.009 12.671
2024-12-02-10:11:16-root-INFO: Loss too large (999.457->1078.462)! Learning rate decreased to 0.01028.
2024-12-02-10:11:17-root-INFO: Loss too large (999.457->1042.797)! Learning rate decreased to 0.00822.
2024-12-02-10:11:17-root-INFO: Loss too large (999.457->1019.985)! Learning rate decreased to 0.00658.
2024-12-02-10:11:17-root-INFO: Loss too large (999.457->1006.882)! Learning rate decreased to 0.00526.
2024-12-02-10:11:17-root-INFO: Loss too large (999.457->999.930)! Learning rate decreased to 0.00421.
2024-12-02-10:11:17-root-INFO: grad norm: 99.542 98.350 15.359
2024-12-02-10:11:18-root-INFO: Loss too large (996.552->1008.827)! Learning rate decreased to 0.00337.
2024-12-02-10:11:18-root-INFO: Loss too large (996.552->1000.481)! Learning rate decreased to 0.00269.
2024-12-02-10:11:18-root-INFO: grad norm: 90.450 89.330 14.189
2024-12-02-10:11:19-root-INFO: grad norm: 74.783 73.728 12.518
2024-12-02-10:11:19-root-INFO: grad norm: 70.256 69.172 12.296
2024-12-02-10:11:20-root-INFO: grad norm: 63.623 62.602 11.355
2024-12-02-10:11:20-root-INFO: grad norm: 60.860 59.782 11.406
2024-12-02-10:11:21-root-INFO: grad norm: 57.200 56.186 10.723
2024-12-02-10:11:21-root-INFO: Loss Change: 999.457 -> 972.334
2024-12-02-10:11:21-root-INFO: Regularization Change: 0.000 -> 0.623
2024-12-02-10:11:21-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-10:11:21-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-10:11:21-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-10:11:21-root-INFO: grad norm: 83.492 82.210 14.578
2024-12-02-10:11:21-root-INFO: Loss too large (975.517->1173.468)! Learning rate decreased to 0.01067.
2024-12-02-10:11:21-root-INFO: Loss too large (975.517->1121.395)! Learning rate decreased to 0.00853.
2024-12-02-10:11:22-root-INFO: Loss too large (975.517->1067.972)! Learning rate decreased to 0.00683.
2024-12-02-10:11:22-root-INFO: Loss too large (975.517->1023.725)! Learning rate decreased to 0.00546.
2024-12-02-10:11:22-root-INFO: Loss too large (975.517->994.778)! Learning rate decreased to 0.00437.
2024-12-02-10:11:22-root-INFO: Loss too large (975.517->978.909)! Learning rate decreased to 0.00350.
2024-12-02-10:11:23-root-INFO: grad norm: 108.843 107.628 16.219
2024-12-02-10:11:23-root-INFO: Loss too large (971.260->975.975)! Learning rate decreased to 0.00280.
2024-12-02-10:11:23-root-INFO: grad norm: 90.610 89.520 14.014
2024-12-02-10:11:24-root-INFO: grad norm: 61.521 60.495 11.189
2024-12-02-10:11:24-root-INFO: grad norm: 55.444 54.354 10.943
2024-12-02-10:11:25-root-INFO: grad norm: 48.250 47.201 10.004
2024-12-02-10:11:25-root-INFO: grad norm: 45.042 43.909 10.038
2024-12-02-10:11:26-root-INFO: grad norm: 41.654 40.565 9.465
2024-12-02-10:11:26-root-INFO: Loss Change: 975.517 -> 944.832
2024-12-02-10:11:26-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-02-10:11:26-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-10:11:26-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-10:11:26-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-10:11:26-root-INFO: grad norm: 68.347 67.060 13.205
2024-12-02-10:11:26-root-INFO: Loss too large (946.733->1092.562)! Learning rate decreased to 0.01107.
2024-12-02-10:11:27-root-INFO: Loss too large (946.733->1038.435)! Learning rate decreased to 0.00885.
2024-12-02-10:11:27-root-INFO: Loss too large (946.733->995.349)! Learning rate decreased to 0.00708.
2024-12-02-10:11:27-root-INFO: Loss too large (946.733->967.688)! Learning rate decreased to 0.00567.
2024-12-02-10:11:27-root-INFO: Loss too large (946.733->952.415)! Learning rate decreased to 0.00453.
2024-12-02-10:11:27-root-INFO: grad norm: 118.456 117.178 17.352
2024-12-02-10:11:28-root-INFO: Loss too large (944.814->960.466)! Learning rate decreased to 0.00363.
2024-12-02-10:11:28-root-INFO: Loss too large (944.814->949.537)! Learning rate decreased to 0.00290.
2024-12-02-10:11:28-root-INFO: grad norm: 89.066 88.027 13.566
2024-12-02-10:11:29-root-INFO: grad norm: 48.196 47.176 9.864
2024-12-02-10:11:29-root-INFO: grad norm: 42.373 41.248 9.701
2024-12-02-10:11:30-root-INFO: grad norm: 37.385 36.283 9.010
2024-12-02-10:11:30-root-INFO: grad norm: 35.160 33.971 9.065
2024-12-02-10:11:31-root-INFO: grad norm: 33.475 32.319 8.721
2024-12-02-10:11:31-root-INFO: Loss Change: 946.733 -> 916.370
2024-12-02-10:11:31-root-INFO: Regularization Change: 0.000 -> 0.673
2024-12-02-10:11:31-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-10:11:31-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-10:11:31-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-10:11:31-root-INFO: grad norm: 48.793 47.664 10.434
2024-12-02-10:11:31-root-INFO: Loss too large (916.182->975.105)! Learning rate decreased to 0.01148.
2024-12-02-10:11:32-root-INFO: Loss too large (916.182->945.365)! Learning rate decreased to 0.00919.
2024-12-02-10:11:32-root-INFO: Loss too large (916.182->928.140)! Learning rate decreased to 0.00735.
2024-12-02-10:11:32-root-INFO: Loss too large (916.182->919.009)! Learning rate decreased to 0.00588.
2024-12-02-10:11:32-root-INFO: grad norm: 102.235 101.073 15.370
2024-12-02-10:11:33-root-INFO: Loss too large (914.514->937.524)! Learning rate decreased to 0.00470.
2024-12-02-10:11:33-root-INFO: Loss too large (914.514->924.913)! Learning rate decreased to 0.00376.
2024-12-02-10:11:33-root-INFO: Loss too large (914.514->916.690)! Learning rate decreased to 0.00301.
2024-12-02-10:11:33-root-INFO: grad norm: 76.145 75.209 11.897
2024-12-02-10:11:34-root-INFO: grad norm: 43.442 42.420 9.370
2024-12-02-10:11:34-root-INFO: grad norm: 38.217 37.126 9.066
2024-12-02-10:11:35-root-INFO: grad norm: 34.185 33.071 8.656
2024-12-02-10:11:35-root-INFO: grad norm: 32.463 31.301 8.609
2024-12-02-10:11:36-root-INFO: grad norm: 31.283 30.129 8.418
2024-12-02-10:11:36-root-INFO: Loss Change: 916.182 -> 888.584
2024-12-02-10:11:36-root-INFO: Regularization Change: 0.000 -> 0.711
2024-12-02-10:11:36-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-10:11:36-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-10:11:36-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-10:11:36-root-INFO: grad norm: 52.925 51.723 11.218
2024-12-02-10:11:37-root-INFO: Loss too large (889.389->964.214)! Learning rate decreased to 0.01191.
2024-12-02-10:11:37-root-INFO: Loss too large (889.389->926.775)! Learning rate decreased to 0.00953.
2024-12-02-10:11:37-root-INFO: Loss too large (889.389->904.821)! Learning rate decreased to 0.00762.
2024-12-02-10:11:37-root-INFO: Loss too large (889.389->893.218)! Learning rate decreased to 0.00610.
2024-12-02-10:11:37-root-INFO: grad norm: 105.178 104.091 15.083
2024-12-02-10:11:38-root-INFO: Loss too large (887.533->910.447)! Learning rate decreased to 0.00488.
2024-12-02-10:11:38-root-INFO: Loss too large (887.533->897.544)! Learning rate decreased to 0.00390.
2024-12-02-10:11:38-root-INFO: Loss too large (887.533->889.142)! Learning rate decreased to 0.00312.
2024-12-02-10:11:38-root-INFO: grad norm: 72.679 71.727 11.726
2024-12-02-10:11:39-root-INFO: grad norm: 36.470 35.443 8.595
2024-12-02-10:11:39-root-INFO: grad norm: 32.627 31.477 8.588
2024-12-02-10:11:40-root-INFO: grad norm: 30.377 29.250 8.196
2024-12-02-10:11:40-root-INFO: grad norm: 29.422 28.250 8.221
2024-12-02-10:11:41-root-INFO: grad norm: 28.826 27.678 8.053
2024-12-02-10:11:41-root-INFO: Loss Change: 889.389 -> 861.743
2024-12-02-10:11:41-root-INFO: Regularization Change: 0.000 -> 0.721
2024-12-02-10:11:41-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-10:11:41-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:11:41-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-10:11:41-root-INFO: grad norm: 38.180 37.021 9.334
2024-12-02-10:11:42-root-INFO: Loss too large (861.490->875.383)! Learning rate decreased to 0.01235.
2024-12-02-10:11:42-root-INFO: Loss too large (861.490->865.416)! Learning rate decreased to 0.00988.
2024-12-02-10:11:42-root-INFO: grad norm: 109.381 108.300 15.336
2024-12-02-10:11:42-root-INFO: Loss too large (860.438->936.582)! Learning rate decreased to 0.00790.
2024-12-02-10:11:43-root-INFO: Loss too large (860.438->905.741)! Learning rate decreased to 0.00632.
2024-12-02-10:11:43-root-INFO: Loss too large (860.438->884.907)! Learning rate decreased to 0.00506.
2024-12-02-10:11:43-root-INFO: Loss too large (860.438->870.992)! Learning rate decreased to 0.00405.
2024-12-02-10:11:43-root-INFO: Loss too large (860.438->861.945)! Learning rate decreased to 0.00324.
2024-12-02-10:11:44-root-INFO: grad norm: 69.798 68.919 11.044
2024-12-02-10:11:44-root-INFO: grad norm: 30.327 29.290 7.863
2024-12-02-10:11:45-root-INFO: grad norm: 27.947 26.816 7.871
2024-12-02-10:11:45-root-INFO: grad norm: 26.826 25.714 7.642
2024-12-02-10:11:46-root-INFO: grad norm: 26.306 25.172 7.641
2024-12-02-10:11:46-root-INFO: grad norm: 25.951 24.832 7.541
2024-12-02-10:11:46-root-INFO: Loss Change: 861.490 -> 836.228
2024-12-02-10:11:46-root-INFO: Regularization Change: 0.000 -> 0.805
2024-12-02-10:11:46-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-10:11:46-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:11:47-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-10:11:47-root-INFO: grad norm: 42.901 41.772 9.779
2024-12-02-10:11:47-root-INFO: Loss too large (836.718->873.541)! Learning rate decreased to 0.01281.
2024-12-02-10:11:47-root-INFO: Loss too large (836.718->852.487)! Learning rate decreased to 0.01024.
2024-12-02-10:11:47-root-INFO: Loss too large (836.718->841.437)! Learning rate decreased to 0.00820.
2024-12-02-10:11:48-root-INFO: grad norm: 98.012 97.012 13.968
2024-12-02-10:11:48-root-INFO: Loss too large (835.950->871.079)! Learning rate decreased to 0.00656.
2024-12-02-10:11:48-root-INFO: Loss too large (835.950->854.183)! Learning rate decreased to 0.00525.
2024-12-02-10:11:48-root-INFO: Loss too large (835.950->843.013)! Learning rate decreased to 0.00420.
2024-12-02-10:11:49-root-INFO: grad norm: 75.781 74.920 11.388
2024-12-02-10:11:49-root-INFO: grad norm: 38.747 37.885 8.126
2024-12-02-10:11:50-root-INFO: grad norm: 34.961 33.980 8.223
2024-12-02-10:11:50-root-INFO: grad norm: 30.929 29.998 7.529
2024-12-02-10:11:51-root-INFO: grad norm: 29.014 27.971 7.712
2024-12-02-10:11:51-root-INFO: grad norm: 27.168 26.181 7.257
2024-12-02-10:11:52-root-INFO: Loss Change: 836.718 -> 811.476
2024-12-02-10:11:52-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-02-10:11:52-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-10:11:52-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:11:52-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-10:11:52-root-INFO: grad norm: 44.299 43.242 9.618
2024-12-02-10:11:52-root-INFO: Loss too large (813.514->862.051)! Learning rate decreased to 0.01328.
2024-12-02-10:11:52-root-INFO: Loss too large (813.514->834.581)! Learning rate decreased to 0.01062.
2024-12-02-10:11:53-root-INFO: Loss too large (813.514->820.317)! Learning rate decreased to 0.00850.
2024-12-02-10:11:53-root-INFO: grad norm: 96.599 95.666 13.398
2024-12-02-10:11:53-root-INFO: Loss too large (813.350->843.621)! Learning rate decreased to 0.00680.
2024-12-02-10:11:53-root-INFO: Loss too large (813.350->828.023)! Learning rate decreased to 0.00544.
2024-12-02-10:11:53-root-INFO: Loss too large (813.350->817.847)! Learning rate decreased to 0.00435.
2024-12-02-10:11:54-root-INFO: grad norm: 65.677 64.858 10.336
2024-12-02-10:11:54-root-INFO: grad norm: 26.860 25.889 7.158
2024-12-02-10:11:55-root-INFO: grad norm: 24.547 23.449 7.257
2024-12-02-10:11:55-root-INFO: grad norm: 23.215 22.154 6.938
2024-12-02-10:11:56-root-INFO: grad norm: 22.606 21.500 6.985
2024-12-02-10:11:56-root-INFO: grad norm: 22.215 21.139 6.830
2024-12-02-10:11:57-root-INFO: Loss Change: 813.514 -> 790.131
2024-12-02-10:11:57-root-INFO: Regularization Change: 0.000 -> 0.835
2024-12-02-10:11:57-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-10:11:57-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-10:11:57-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-10:11:57-root-INFO: grad norm: 32.624 31.556 8.278
2024-12-02-10:11:57-root-INFO: Loss too large (789.526->793.389)! Learning rate decreased to 0.01376.
2024-12-02-10:11:58-root-INFO: grad norm: 82.063 81.293 11.217
2024-12-02-10:11:58-root-INFO: Loss too large (788.625->850.741)! Learning rate decreased to 0.01101.
2024-12-02-10:11:58-root-INFO: Loss too large (788.625->823.089)! Learning rate decreased to 0.00881.
2024-12-02-10:11:58-root-INFO: Loss too large (788.625->805.376)! Learning rate decreased to 0.00705.
2024-12-02-10:11:58-root-INFO: Loss too large (788.625->794.208)! Learning rate decreased to 0.00564.
2024-12-02-10:11:59-root-INFO: grad norm: 64.245 63.465 9.979
2024-12-02-10:11:59-root-INFO: grad norm: 38.302 37.615 7.224
2024-12-02-10:12:00-root-INFO: grad norm: 34.958 34.124 7.590
2024-12-02-10:12:00-root-INFO: grad norm: 30.704 29.969 6.676
2024-12-02-10:12:01-root-INFO: grad norm: 28.960 28.087 7.055
2024-12-02-10:12:01-root-INFO: grad norm: 26.885 26.108 6.416
2024-12-02-10:12:02-root-INFO: Loss Change: 789.526 -> 764.357
2024-12-02-10:12:02-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-02-10:12:02-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-10:12:02-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:12:02-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-10:12:02-root-INFO: grad norm: 40.255 39.437 8.077
2024-12-02-10:12:02-root-INFO: Loss too large (765.484->803.148)! Learning rate decreased to 0.01426.
2024-12-02-10:12:02-root-INFO: Loss too large (765.484->779.957)! Learning rate decreased to 0.01141.
2024-12-02-10:12:03-root-INFO: Loss too large (765.484->768.800)! Learning rate decreased to 0.00913.
2024-12-02-10:12:03-root-INFO: grad norm: 67.745 67.041 9.737
2024-12-02-10:12:03-root-INFO: Loss too large (763.669->773.581)! Learning rate decreased to 0.00730.
2024-12-02-10:12:03-root-INFO: Loss too large (763.669->766.307)! Learning rate decreased to 0.00584.
2024-12-02-10:12:04-root-INFO: grad norm: 50.846 50.137 8.462
2024-12-02-10:12:04-root-INFO: grad norm: 27.523 26.787 6.321
2024-12-02-10:12:05-root-INFO: grad norm: 24.324 23.436 6.514
2024-12-02-10:12:05-root-INFO: grad norm: 21.847 21.000 6.022
2024-12-02-10:12:06-root-INFO: grad norm: 20.813 19.877 6.171
2024-12-02-10:12:06-root-INFO: grad norm: 20.090 19.198 5.921
2024-12-02-10:12:06-root-INFO: Loss Change: 765.484 -> 742.711
2024-12-02-10:12:06-root-INFO: Regularization Change: 0.000 -> 1.031
2024-12-02-10:12:06-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-10:12:06-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:12:07-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-10:12:07-root-INFO: grad norm: 34.925 33.979 8.074
2024-12-02-10:12:07-root-INFO: Loss too large (744.450->753.463)! Learning rate decreased to 0.01478.
2024-12-02-10:12:07-root-INFO: Loss too large (744.450->745.961)! Learning rate decreased to 0.01182.
2024-12-02-10:12:08-root-INFO: grad norm: 57.517 56.904 8.376
2024-12-02-10:12:08-root-INFO: Loss too large (742.234->753.209)! Learning rate decreased to 0.00946.
2024-12-02-10:12:08-root-INFO: Loss too large (742.234->744.944)! Learning rate decreased to 0.00757.
2024-12-02-10:12:08-root-INFO: grad norm: 50.397 49.642 8.693
2024-12-02-10:12:09-root-INFO: grad norm: 42.369 41.779 7.045
2024-12-02-10:12:09-root-INFO: grad norm: 39.624 38.866 7.713
2024-12-02-10:12:10-root-INFO: grad norm: 35.585 35.000 6.423
2024-12-02-10:12:10-root-INFO: grad norm: 34.002 33.231 7.200
2024-12-02-10:12:11-root-INFO: grad norm: 31.710 31.122 6.075
2024-12-02-10:12:11-root-INFO: Loss Change: 744.450 -> 719.554
2024-12-02-10:12:11-root-INFO: Regularization Change: 0.000 -> 1.590
2024-12-02-10:12:11-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-10:12:11-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:12:11-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-10:12:11-root-INFO: grad norm: 41.288 40.514 7.956
2024-12-02-10:12:11-root-INFO: Loss too large (720.728->751.302)! Learning rate decreased to 0.01531.
2024-12-02-10:12:12-root-INFO: Loss too large (720.728->730.860)! Learning rate decreased to 0.01225.
2024-12-02-10:12:12-root-INFO: Loss too large (720.728->721.396)! Learning rate decreased to 0.00980.
2024-12-02-10:12:12-root-INFO: grad norm: 49.724 49.124 7.700
2024-12-02-10:12:12-root-INFO: Loss too large (717.166->717.728)! Learning rate decreased to 0.00784.
2024-12-02-10:12:13-root-INFO: grad norm: 41.557 40.810 7.847
2024-12-02-10:12:13-root-INFO: grad norm: 31.272 30.678 6.066
2024-12-02-10:12:14-root-INFO: grad norm: 27.982 27.177 6.662
2024-12-02-10:12:14-root-INFO: grad norm: 24.905 24.271 5.586
2024-12-02-10:12:15-root-INFO: grad norm: 23.310 22.476 6.178
2024-12-02-10:12:15-root-INFO: grad norm: 21.962 21.293 5.377
2024-12-02-10:12:16-root-INFO: Loss Change: 720.728 -> 695.132
2024-12-02-10:12:16-root-INFO: Regularization Change: 0.000 -> 1.467
2024-12-02-10:12:16-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-10:12:16-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-10:12:16-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-10:12:16-root-INFO: grad norm: 31.341 30.600 6.775
2024-12-02-10:12:16-root-INFO: Loss too large (694.811->701.413)! Learning rate decreased to 0.01586.
2024-12-02-10:12:16-root-INFO: Loss too large (694.811->695.617)! Learning rate decreased to 0.01269.
2024-12-02-10:12:17-root-INFO: grad norm: 46.119 45.559 7.165
2024-12-02-10:12:17-root-INFO: Loss too large (692.733->696.856)! Learning rate decreased to 0.01015.
2024-12-02-10:12:17-root-INFO: grad norm: 51.861 51.050 9.137
2024-12-02-10:12:18-root-INFO: grad norm: 53.496 52.880 8.092
2024-12-02-10:12:18-root-INFO: grad norm: 51.810 51.013 9.051
2024-12-02-10:12:19-root-INFO: grad norm: 52.177 51.572 7.922
2024-12-02-10:12:19-root-INFO: grad norm: 51.951 51.159 9.041
2024-12-02-10:12:20-root-INFO: grad norm: 50.422 49.823 7.749
2024-12-02-10:12:20-root-INFO: Loss Change: 694.811 -> 672.898
2024-12-02-10:12:20-root-INFO: Regularization Change: 0.000 -> 2.257
2024-12-02-10:12:20-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-10:12:20-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-10:12:20-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-10:12:20-root-INFO: grad norm: 63.260 62.441 10.149
2024-12-02-10:12:20-root-INFO: Loss too large (677.189->742.983)! Learning rate decreased to 0.01642.
2024-12-02-10:12:21-root-INFO: Loss too large (677.189->699.699)! Learning rate decreased to 0.01314.
2024-12-02-10:12:21-root-INFO: Loss too large (677.189->679.776)! Learning rate decreased to 0.01051.
2024-12-02-10:12:21-root-INFO: grad norm: 53.873 53.283 7.951
2024-12-02-10:12:22-root-INFO: grad norm: 47.446 46.712 8.317
2024-12-02-10:12:22-root-INFO: grad norm: 43.886 43.338 6.912
2024-12-02-10:12:23-root-INFO: grad norm: 42.479 41.753 7.818
2024-12-02-10:12:23-root-INFO: grad norm: 40.167 39.613 6.647
2024-12-02-10:12:24-root-INFO: grad norm: 38.638 37.912 7.458
2024-12-02-10:12:24-root-INFO: grad norm: 37.261 36.704 6.418
2024-12-02-10:12:25-root-INFO: Loss Change: 677.189 -> 643.853
2024-12-02-10:12:25-root-INFO: Regularization Change: 0.000 -> 2.148
2024-12-02-10:12:25-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-10:12:25-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-10:12:25-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-10:12:25-root-INFO: grad norm: 49.698 48.983 8.396
2024-12-02-10:12:25-root-INFO: Loss too large (645.321->673.177)! Learning rate decreased to 0.01701.
2024-12-02-10:12:25-root-INFO: Loss too large (645.321->657.288)! Learning rate decreased to 0.01361.
2024-12-02-10:12:25-root-INFO: Loss too large (645.321->648.013)! Learning rate decreased to 0.01088.
2024-12-02-10:12:26-root-INFO: grad norm: 43.459 42.890 7.008
2024-12-02-10:12:26-root-INFO: grad norm: 35.496 34.855 6.712
2024-12-02-10:12:27-root-INFO: grad norm: 33.666 33.135 5.959
2024-12-02-10:12:27-root-INFO: grad norm: 33.590 32.933 6.613
2024-12-02-10:12:28-root-INFO: grad norm: 32.628 32.082 5.943
2024-12-02-10:12:28-root-INFO: grad norm: 32.557 31.876 6.625
2024-12-02-10:12:29-root-INFO: grad norm: 31.922 31.366 5.928
2024-12-02-10:12:29-root-INFO: Loss Change: 645.321 -> 619.523
2024-12-02-10:12:29-root-INFO: Regularization Change: 0.000 -> 1.937
2024-12-02-10:12:29-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-10:12:29-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-10:12:29-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-10:12:29-root-INFO: grad norm: 47.042 46.358 7.996
2024-12-02-10:12:29-root-INFO: Loss too large (621.234->646.881)! Learning rate decreased to 0.01761.
2024-12-02-10:12:30-root-INFO: Loss too large (621.234->633.292)! Learning rate decreased to 0.01409.
2024-12-02-10:12:30-root-INFO: Loss too large (621.234->624.805)! Learning rate decreased to 0.01127.
2024-12-02-10:12:30-root-INFO: grad norm: 43.185 42.608 7.035
2024-12-02-10:12:31-root-INFO: grad norm: 37.758 37.136 6.824
2024-12-02-10:12:31-root-INFO: grad norm: 37.029 36.489 6.303
2024-12-02-10:12:32-root-INFO: grad norm: 38.209 37.547 7.081
2024-12-02-10:12:32-root-INFO: grad norm: 37.141 36.587 6.392
2024-12-02-10:12:33-root-INFO: grad norm: 35.783 35.130 6.805
2024-12-02-10:12:33-root-INFO: grad norm: 35.512 34.962 6.231
2024-12-02-10:12:33-root-INFO: Loss Change: 621.234 -> 598.650
2024-12-02-10:12:33-root-INFO: Regularization Change: 0.000 -> 1.833
2024-12-02-10:12:33-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-10:12:33-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:12:33-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-10:12:34-root-INFO: grad norm: 49.118 48.458 8.028
2024-12-02-10:12:34-root-INFO: Loss too large (601.169->632.731)! Learning rate decreased to 0.01823.
2024-12-02-10:12:34-root-INFO: Loss too large (601.169->616.640)! Learning rate decreased to 0.01458.
2024-12-02-10:12:34-root-INFO: Loss too large (601.169->606.348)! Learning rate decreased to 0.01167.
2024-12-02-10:12:35-root-INFO: grad norm: 46.047 45.489 7.148
2024-12-02-10:12:35-root-INFO: grad norm: 41.070 40.498 6.830
2024-12-02-10:12:35-root-INFO: grad norm: 40.250 39.733 6.434
2024-12-02-10:12:36-root-INFO: grad norm: 41.410 40.788 7.148
2024-12-02-10:12:36-root-INFO: grad norm: 40.120 39.582 6.547
2024-12-02-10:12:37-root-INFO: grad norm: 38.509 37.903 6.805
2024-12-02-10:12:37-root-INFO: grad norm: 38.171 37.638 6.360
2024-12-02-10:12:38-root-INFO: Loss Change: 601.169 -> 579.725
2024-12-02-10:12:38-root-INFO: Regularization Change: 0.000 -> 1.777
2024-12-02-10:12:38-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-10:12:38-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:12:38-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-10:12:38-root-INFO: grad norm: 50.147 49.532 7.832
2024-12-02-10:12:38-root-INFO: Loss too large (581.509->615.966)! Learning rate decreased to 0.01887.
2024-12-02-10:12:38-root-INFO: Loss too large (581.509->599.056)! Learning rate decreased to 0.01509.
2024-12-02-10:12:38-root-INFO: Loss too large (581.509->588.035)! Learning rate decreased to 0.01207.
2024-12-02-10:12:39-root-INFO: grad norm: 46.511 45.953 7.182
2024-12-02-10:12:39-root-INFO: grad norm: 40.309 39.771 6.558
2024-12-02-10:12:40-root-INFO: grad norm: 39.586 39.081 6.303
2024-12-02-10:12:40-root-INFO: grad norm: 41.653 41.030 7.173
2024-12-02-10:12:41-root-INFO: grad norm: 40.139 39.603 6.543
2024-12-02-10:12:41-root-INFO: grad norm: 37.724 37.136 6.632
2024-12-02-10:12:42-root-INFO: grad norm: 37.596 37.070 6.266
2024-12-02-10:12:42-root-INFO: Loss Change: 581.509 -> 560.916
2024-12-02-10:12:42-root-INFO: Regularization Change: 0.000 -> 1.733
2024-12-02-10:12:42-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-10:12:42-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:12:42-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-10:12:42-root-INFO: grad norm: 58.539 57.817 9.166
2024-12-02-10:12:42-root-INFO: Loss too large (565.109->608.438)! Learning rate decreased to 0.01952.
2024-12-02-10:12:43-root-INFO: Loss too large (565.109->587.742)! Learning rate decreased to 0.01562.
2024-12-02-10:12:43-root-INFO: Loss too large (565.109->573.877)! Learning rate decreased to 0.01250.
2024-12-02-10:12:43-root-INFO: Loss too large (565.109->565.115)! Learning rate decreased to 0.01000.
2024-12-02-10:12:43-root-INFO: grad norm: 38.224 37.620 6.772
2024-12-02-10:12:44-root-INFO: grad norm: 20.623 20.082 4.690
2024-12-02-10:12:44-root-INFO: grad norm: 17.365 16.824 4.301
2024-12-02-10:12:45-root-INFO: grad norm: 15.971 15.367 4.349
2024-12-02-10:12:45-root-INFO: grad norm: 15.382 14.824 4.107
2024-12-02-10:12:46-root-INFO: grad norm: 15.119 14.510 4.245
2024-12-02-10:12:46-root-INFO: grad norm: 15.032 14.485 4.016
2024-12-02-10:12:47-root-INFO: Loss Change: 565.109 -> 541.850
2024-12-02-10:12:47-root-INFO: Regularization Change: 0.000 -> 1.266
2024-12-02-10:12:47-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-10:12:47-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-10:12:47-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-10:12:47-root-INFO: grad norm: 21.986 21.474 4.719
2024-12-02-10:12:47-root-INFO: Loss too large (541.931->544.914)! Learning rate decreased to 0.02020.
2024-12-02-10:12:47-root-INFO: Loss too large (541.931->542.460)! Learning rate decreased to 0.01616.
2024-12-02-10:12:48-root-INFO: grad norm: 30.849 30.328 5.643
2024-12-02-10:12:48-root-INFO: Loss too large (541.018->543.637)! Learning rate decreased to 0.01293.
2024-12-02-10:12:48-root-INFO: grad norm: 42.977 42.277 7.729
2024-12-02-10:12:48-root-INFO: Loss too large (539.451->542.282)! Learning rate decreased to 0.01034.
2024-12-02-10:12:49-root-INFO: grad norm: 34.073 33.521 6.109
2024-12-02-10:12:49-root-INFO: grad norm: 20.582 20.025 4.759
2024-12-02-10:12:50-root-INFO: grad norm: 19.523 19.045 4.294
2024-12-02-10:12:50-root-INFO: grad norm: 19.238 18.673 4.626
2024-12-02-10:12:51-root-INFO: grad norm: 19.263 18.786 4.261
2024-12-02-10:12:51-root-INFO: Loss Change: 541.931 -> 526.696
2024-12-02-10:12:51-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-02-10:12:51-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-10:12:51-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-10:12:51-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-10:12:51-root-INFO: grad norm: 27.434 26.948 5.138
2024-12-02-10:12:52-root-INFO: Loss too large (527.065->536.774)! Learning rate decreased to 0.02090.
2024-12-02-10:12:52-root-INFO: Loss too large (527.065->531.360)! Learning rate decreased to 0.01672.
2024-12-02-10:12:52-root-INFO: Loss too large (527.065->527.977)! Learning rate decreased to 0.01338.
2024-12-02-10:12:52-root-INFO: grad norm: 31.402 30.885 5.677
2024-12-02-10:12:53-root-INFO: grad norm: 45.084 44.400 7.822
2024-12-02-10:12:53-root-INFO: Loss too large (524.912->528.424)! Learning rate decreased to 0.01070.
2024-12-02-10:12:53-root-INFO: grad norm: 35.227 34.668 6.249
2024-12-02-10:12:54-root-INFO: grad norm: 20.439 19.921 4.572
2024-12-02-10:12:54-root-INFO: grad norm: 19.069 18.606 4.174
2024-12-02-10:12:55-root-INFO: grad norm: 18.753 18.216 4.458
2024-12-02-10:12:55-root-INFO: grad norm: 18.790 18.324 4.157
2024-12-02-10:12:56-root-INFO: Loss Change: 527.065 -> 511.944
2024-12-02-10:12:56-root-INFO: Regularization Change: 0.000 -> 1.281
2024-12-02-10:12:56-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-10:12:56-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-10:12:56-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-10:12:56-root-INFO: grad norm: 29.954 29.448 5.483
2024-12-02-10:12:56-root-INFO: Loss too large (512.274->525.465)! Learning rate decreased to 0.02162.
2024-12-02-10:12:56-root-INFO: Loss too large (512.274->518.799)! Learning rate decreased to 0.01729.
2024-12-02-10:12:56-root-INFO: Loss too large (512.274->514.476)! Learning rate decreased to 0.01384.
2024-12-02-10:12:57-root-INFO: grad norm: 33.463 32.950 5.837
2024-12-02-10:12:57-root-INFO: grad norm: 45.302 44.621 7.823
2024-12-02-10:12:58-root-INFO: Loss too large (510.183->513.839)! Learning rate decreased to 0.01107.
2024-12-02-10:12:58-root-INFO: grad norm: 35.023 34.477 6.163
2024-12-02-10:12:58-root-INFO: grad norm: 20.180 19.681 4.460
2024-12-02-10:12:59-root-INFO: grad norm: 18.648 18.201 4.059
2024-12-02-10:12:59-root-INFO: grad norm: 18.369 17.853 4.324
2024-12-02-10:13:00-root-INFO: grad norm: 18.472 18.018 4.072
2024-12-02-10:13:00-root-INFO: Loss Change: 512.274 -> 497.308
2024-12-02-10:13:00-root-INFO: Regularization Change: 0.000 -> 1.274
2024-12-02-10:13:00-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-10:13:00-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-10:13:00-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-10:13:01-root-INFO: grad norm: 28.708 28.241 5.156
2024-12-02-10:13:01-root-INFO: Loss too large (498.227->511.681)! Learning rate decreased to 0.02236.
2024-12-02-10:13:01-root-INFO: Loss too large (498.227->504.802)! Learning rate decreased to 0.01789.
2024-12-02-10:13:01-root-INFO: Loss too large (498.227->500.408)! Learning rate decreased to 0.01431.
2024-12-02-10:13:01-root-INFO: grad norm: 33.282 32.775 5.787
2024-12-02-10:13:02-root-INFO: grad norm: 47.786 47.110 8.008
2024-12-02-10:13:02-root-INFO: Loss too large (496.956->501.536)! Learning rate decreased to 0.01145.
2024-12-02-10:13:02-root-INFO: Loss too large (496.956->497.002)! Learning rate decreased to 0.00916.
2024-12-02-10:13:03-root-INFO: grad norm: 29.232 28.717 5.465
2024-12-02-10:13:03-root-INFO: grad norm: 13.635 13.130 3.675
2024-12-02-10:13:04-root-INFO: grad norm: 12.295 11.808 3.426
2024-12-02-10:13:04-root-INFO: grad norm: 11.736 11.222 3.433
2024-12-02-10:13:05-root-INFO: grad norm: 11.480 10.984 3.339
2024-12-02-10:13:05-root-INFO: Loss Change: 498.227 -> 484.958
2024-12-02-10:13:05-root-INFO: Regularization Change: 0.000 -> 0.964
2024-12-02-10:13:05-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-10:13:05-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-10:13:05-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-10:13:05-root-INFO: grad norm: 17.135 16.691 3.875
2024-12-02-10:13:06-root-INFO: grad norm: 36.371 35.923 5.695
2024-12-02-10:13:06-root-INFO: Loss too large (484.543->524.814)! Learning rate decreased to 0.02312.
2024-12-02-10:13:06-root-INFO: Loss too large (484.543->503.832)! Learning rate decreased to 0.01849.
2024-12-02-10:13:06-root-INFO: Loss too large (484.543->491.453)! Learning rate decreased to 0.01479.
2024-12-02-10:13:06-root-INFO: Loss too large (484.543->484.634)! Learning rate decreased to 0.01184.
2024-12-02-10:13:07-root-INFO: grad norm: 32.491 31.986 5.709
2024-12-02-10:13:07-root-INFO: grad norm: 29.373 28.884 5.333
2024-12-02-10:13:08-root-INFO: grad norm: 26.913 26.404 5.213
2024-12-02-10:13:08-root-INFO: grad norm: 26.238 25.757 5.000
2024-12-02-10:13:09-root-INFO: grad norm: 25.625 25.116 5.083
2024-12-02-10:13:09-root-INFO: grad norm: 25.310 24.833 4.890
2024-12-02-10:13:09-root-INFO: Loss Change: 484.768 -> 471.520
2024-12-02-10:13:09-root-INFO: Regularization Change: 0.000 -> 1.554
2024-12-02-10:13:09-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-10:13:09-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-10:13:10-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-10:13:10-root-INFO: grad norm: 35.954 35.414 6.207
2024-12-02-10:13:10-root-INFO: Loss too large (472.979->494.559)! Learning rate decreased to 0.02390.
2024-12-02-10:13:10-root-INFO: Loss too large (472.979->485.771)! Learning rate decreased to 0.01912.
2024-12-02-10:13:10-root-INFO: Loss too large (472.979->479.437)! Learning rate decreased to 0.01530.
2024-12-02-10:13:10-root-INFO: Loss too large (472.979->475.043)! Learning rate decreased to 0.01224.
2024-12-02-10:13:11-root-INFO: grad norm: 31.187 30.653 5.747
2024-12-02-10:13:11-root-INFO: grad norm: 24.895 24.415 4.865
2024-12-02-10:13:12-root-INFO: grad norm: 24.227 23.761 4.729
2024-12-02-10:13:12-root-INFO: grad norm: 24.235 23.755 4.802
2024-12-02-10:13:13-root-INFO: grad norm: 23.939 23.473 4.697
2024-12-02-10:13:13-root-INFO: grad norm: 23.616 23.141 4.711
2024-12-02-10:13:14-root-INFO: grad norm: 23.485 23.025 4.622
2024-12-02-10:13:14-root-INFO: Loss Change: 472.979 -> 459.519
2024-12-02-10:13:14-root-INFO: Regularization Change: 0.000 -> 1.133
2024-12-02-10:13:14-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-10:13:14-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-10:13:14-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-10:13:14-root-INFO: grad norm: 31.876 31.389 5.548
2024-12-02-10:13:14-root-INFO: Loss too large (460.295->478.115)! Learning rate decreased to 0.02471.
2024-12-02-10:13:15-root-INFO: Loss too large (460.295->471.046)! Learning rate decreased to 0.01976.
2024-12-02-10:13:15-root-INFO: Loss too large (460.295->465.855)! Learning rate decreased to 0.01581.
2024-12-02-10:13:15-root-INFO: Loss too large (460.295->462.206)! Learning rate decreased to 0.01265.
2024-12-02-10:13:15-root-INFO: grad norm: 28.952 28.421 5.519
2024-12-02-10:13:16-root-INFO: grad norm: 25.011 24.554 4.760
2024-12-02-10:13:16-root-INFO: grad norm: 24.665 24.191 4.813
2024-12-02-10:13:17-root-INFO: grad norm: 24.643 24.180 4.755
2024-12-02-10:13:17-root-INFO: grad norm: 24.356 23.887 4.758
2024-12-02-10:13:18-root-INFO: grad norm: 23.911 23.452 4.664
2024-12-02-10:13:18-root-INFO: grad norm: 23.756 23.295 4.659
2024-12-02-10:13:19-root-INFO: Loss Change: 460.295 -> 448.340
2024-12-02-10:13:19-root-INFO: Regularization Change: 0.000 -> 1.094
2024-12-02-10:13:19-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-10:13:19-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-10:13:19-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-10:13:19-root-INFO: grad norm: 33.963 33.477 5.729
2024-12-02-10:13:19-root-INFO: Loss too large (449.984->472.154)! Learning rate decreased to 0.02553.
2024-12-02-10:13:19-root-INFO: Loss too large (449.984->462.832)! Learning rate decreased to 0.02043.
2024-12-02-10:13:19-root-INFO: Loss too large (449.984->456.279)! Learning rate decreased to 0.01634.
2024-12-02-10:13:20-root-INFO: Loss too large (449.984->451.851)! Learning rate decreased to 0.01307.
2024-12-02-10:13:20-root-INFO: grad norm: 29.535 29.029 5.441
2024-12-02-10:13:21-root-INFO: grad norm: 24.492 24.054 4.611
2024-12-02-10:13:21-root-INFO: grad norm: 23.683 23.233 4.591
2024-12-02-10:13:21-root-INFO: grad norm: 23.351 22.913 4.503
2024-12-02-10:13:22-root-INFO: grad norm: 22.960 22.513 4.506
2024-12-02-10:13:22-root-INFO: grad norm: 22.499 22.067 4.391
2024-12-02-10:13:23-root-INFO: grad norm: 22.307 21.867 4.409
2024-12-02-10:13:23-root-INFO: Loss Change: 449.984 -> 437.455
2024-12-02-10:13:23-root-INFO: Regularization Change: 0.000 -> 1.112
2024-12-02-10:13:23-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-10:13:23-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-10:13:23-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-10:13:24-root-INFO: grad norm: 29.832 29.393 5.097
2024-12-02-10:13:24-root-INFO: Loss too large (438.076->456.285)! Learning rate decreased to 0.02639.
2024-12-02-10:13:24-root-INFO: Loss too large (438.076->448.144)! Learning rate decreased to 0.02111.
2024-12-02-10:13:24-root-INFO: Loss too large (438.076->442.618)! Learning rate decreased to 0.01689.
2024-12-02-10:13:24-root-INFO: Loss too large (438.076->439.021)! Learning rate decreased to 0.01351.
2024-12-02-10:13:25-root-INFO: grad norm: 26.555 26.067 5.068
2024-12-02-10:13:25-root-INFO: grad norm: 24.185 23.769 4.465
2024-12-02-10:13:26-root-INFO: grad norm: 23.349 22.894 4.590
2024-12-02-10:13:26-root-INFO: grad norm: 22.497 22.085 4.290
2024-12-02-10:13:27-root-INFO: grad norm: 22.126 21.683 4.403
2024-12-02-10:13:27-root-INFO: grad norm: 21.670 21.262 4.186
2024-12-02-10:13:27-root-INFO: grad norm: 21.470 21.035 4.297
2024-12-02-10:13:28-root-INFO: Loss Change: 438.076 -> 426.464
2024-12-02-10:13:28-root-INFO: Regularization Change: 0.000 -> 1.103
2024-12-02-10:13:28-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-10:13:28-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-10:13:28-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-10:13:28-root-INFO: grad norm: 30.881 30.417 5.332
2024-12-02-10:13:28-root-INFO: Loss too large (427.499->445.387)! Learning rate decreased to 0.02726.
2024-12-02-10:13:28-root-INFO: Loss too large (427.499->438.364)! Learning rate decreased to 0.02181.
2024-12-02-10:13:29-root-INFO: Loss too large (427.499->433.197)! Learning rate decreased to 0.01745.
2024-12-02-10:13:29-root-INFO: Loss too large (427.499->429.519)! Learning rate decreased to 0.01396.
2024-12-02-10:13:29-root-INFO: grad norm: 26.828 26.340 5.096
2024-12-02-10:13:30-root-INFO: grad norm: 20.816 20.430 3.987
2024-12-02-10:13:30-root-INFO: grad norm: 20.268 19.852 4.088
2024-12-02-10:13:31-root-INFO: grad norm: 20.242 19.859 3.918
2024-12-02-10:13:31-root-INFO: grad norm: 20.036 19.617 4.077
2024-12-02-10:13:32-root-INFO: grad norm: 19.868 19.488 3.865
2024-12-02-10:13:32-root-INFO: grad norm: 19.737 19.320 4.033
2024-12-02-10:13:32-root-INFO: Loss Change: 427.499 -> 416.419
2024-12-02-10:13:32-root-INFO: Regularization Change: 0.000 -> 1.063
2024-12-02-10:13:32-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-10:13:32-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-10:13:33-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-10:13:33-root-INFO: grad norm: 23.468 23.090 4.198
2024-12-02-10:13:33-root-INFO: Loss too large (416.718->428.396)! Learning rate decreased to 0.02816.
2024-12-02-10:13:33-root-INFO: Loss too large (416.718->423.119)! Learning rate decreased to 0.02253.
2024-12-02-10:13:33-root-INFO: Loss too large (416.718->419.501)! Learning rate decreased to 0.01802.
2024-12-02-10:13:33-root-INFO: Loss too large (416.718->417.146)! Learning rate decreased to 0.01442.
2024-12-02-10:13:34-root-INFO: grad norm: 21.941 21.505 4.355
2024-12-02-10:13:34-root-INFO: grad norm: 21.206 20.832 3.967
2024-12-02-10:13:35-root-INFO: grad norm: 20.695 20.272 4.164
2024-12-02-10:13:35-root-INFO: grad norm: 20.030 19.659 3.837
2024-12-02-10:13:36-root-INFO: grad norm: 19.756 19.343 4.019
2024-12-02-10:13:36-root-INFO: grad norm: 19.374 19.007 3.754
2024-12-02-10:13:36-root-INFO: grad norm: 19.203 18.796 3.934
2024-12-02-10:13:37-root-INFO: Loss Change: 416.718 -> 407.027
2024-12-02-10:13:37-root-INFO: Regularization Change: 0.000 -> 1.043
2024-12-02-10:13:37-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-10:13:37-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-10:13:37-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-10:13:37-root-INFO: grad norm: 24.831 24.465 4.252
2024-12-02-10:13:37-root-INFO: Loss too large (407.547->421.956)! Learning rate decreased to 0.02909.
2024-12-02-10:13:37-root-INFO: Loss too large (407.547->415.257)! Learning rate decreased to 0.02327.
2024-12-02-10:13:38-root-INFO: Loss too large (407.547->410.796)! Learning rate decreased to 0.01862.
2024-12-02-10:13:38-root-INFO: Loss too large (407.547->407.966)! Learning rate decreased to 0.01489.
2024-12-02-10:13:38-root-INFO: grad norm: 22.490 22.027 4.539
2024-12-02-10:13:39-root-INFO: grad norm: 21.398 21.040 3.900
2024-12-02-10:13:39-root-INFO: grad norm: 20.559 20.120 4.227
2024-12-02-10:13:40-root-INFO: grad norm: 19.515 19.159 3.713
2024-12-02-10:13:40-root-INFO: grad norm: 19.099 18.679 3.983
2024-12-02-10:13:41-root-INFO: grad norm: 18.552 18.199 3.602
2024-12-02-10:13:41-root-INFO: grad norm: 18.318 17.908 3.851
2024-12-02-10:13:41-root-INFO: Loss Change: 407.547 -> 397.591
2024-12-02-10:13:41-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-02-10:13:41-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-10:13:41-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-10:13:42-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-10:13:42-root-INFO: grad norm: 30.408 29.982 5.076
2024-12-02-10:13:42-root-INFO: Loss too large (399.714->421.571)! Learning rate decreased to 0.03019.
2024-12-02-10:13:42-root-INFO: Loss too large (399.714->411.658)! Learning rate decreased to 0.02415.
2024-12-02-10:13:42-root-INFO: Loss too large (399.714->405.007)! Learning rate decreased to 0.01932.
2024-12-02-10:13:42-root-INFO: Loss too large (399.714->400.709)! Learning rate decreased to 0.01546.
2024-12-02-10:13:43-root-INFO: grad norm: 25.499 25.009 4.975
2024-12-02-10:13:43-root-INFO: grad norm: 21.264 20.908 3.872
2024-12-02-10:13:44-root-INFO: grad norm: 19.965 19.533 4.130
2024-12-02-10:13:44-root-INFO: grad norm: 18.768 18.422 3.590
2024-12-02-10:13:45-root-INFO: grad norm: 18.213 17.797 3.871
2024-12-02-10:13:45-root-INFO: grad norm: 17.586 17.245 3.444
2024-12-02-10:13:46-root-INFO: grad norm: 17.299 16.893 3.724
2024-12-02-10:13:46-root-INFO: Loss Change: 399.714 -> 388.319
2024-12-02-10:13:46-root-INFO: Regularization Change: 0.000 -> 1.119
2024-12-02-10:13:46-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-10:13:46-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-10:13:46-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-10:13:46-root-INFO: grad norm: 22.813 22.459 4.008
2024-12-02-10:13:46-root-INFO: Loss too large (388.978->401.707)! Learning rate decreased to 0.03117.
2024-12-02-10:13:46-root-INFO: Loss too large (388.978->396.057)! Learning rate decreased to 0.02494.
2024-12-02-10:13:47-root-INFO: Loss too large (388.978->392.159)! Learning rate decreased to 0.01995.
2024-12-02-10:13:47-root-INFO: Loss too large (388.978->389.597)! Learning rate decreased to 0.01596.
2024-12-02-10:13:47-root-INFO: grad norm: 21.019 20.572 4.314
2024-12-02-10:13:48-root-INFO: grad norm: 19.705 19.362 3.662
2024-12-02-10:13:48-root-INFO: grad norm: 19.021 18.590 4.025
2024-12-02-10:13:49-root-INFO: grad norm: 18.184 17.847 3.483
2024-12-02-10:13:49-root-INFO: grad norm: 17.811 17.394 3.836
2024-12-02-10:13:50-root-INFO: grad norm: 17.307 16.976 3.369
2024-12-02-10:13:50-root-INFO: grad norm: 17.071 16.662 3.716
2024-12-02-10:13:50-root-INFO: Loss Change: 388.978 -> 379.854
2024-12-02-10:13:50-root-INFO: Regularization Change: 0.000 -> 1.055
2024-12-02-10:13:50-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-10:13:50-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-10:13:51-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-10:13:51-root-INFO: grad norm: 23.675 23.314 4.121
2024-12-02-10:13:51-root-INFO: Loss too large (380.760->394.207)! Learning rate decreased to 0.03218.
2024-12-02-10:13:51-root-INFO: Loss too large (380.760->388.339)! Learning rate decreased to 0.02574.
2024-12-02-10:13:51-root-INFO: Loss too large (380.760->384.211)! Learning rate decreased to 0.02059.
2024-12-02-10:13:51-root-INFO: Loss too large (380.760->381.446)! Learning rate decreased to 0.01647.
2024-12-02-10:13:52-root-INFO: grad norm: 21.037 20.577 4.373
2024-12-02-10:13:52-root-INFO: grad norm: 18.539 18.209 3.483
2024-12-02-10:13:53-root-INFO: grad norm: 17.607 17.190 3.809
2024-12-02-10:13:53-root-INFO: grad norm: 16.525 16.207 3.228
2024-12-02-10:13:54-root-INFO: grad norm: 16.038 15.640 3.549
2024-12-02-10:13:54-root-INFO: grad norm: 15.438 15.127 3.083
2024-12-02-10:13:55-root-INFO: grad norm: 15.141 14.755 3.398
2024-12-02-10:13:55-root-INFO: Loss Change: 380.760 -> 371.426
2024-12-02-10:13:55-root-INFO: Regularization Change: 0.000 -> 1.064
2024-12-02-10:13:55-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-10:13:55-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-10:13:55-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-10:13:55-root-INFO: grad norm: 20.929 20.593 3.736
2024-12-02-10:13:55-root-INFO: Loss too large (371.892->383.174)! Learning rate decreased to 0.03321.
2024-12-02-10:13:56-root-INFO: Loss too large (371.892->378.011)! Learning rate decreased to 0.02657.
2024-12-02-10:13:56-root-INFO: Loss too large (371.892->374.477)! Learning rate decreased to 0.02126.
2024-12-02-10:13:56-root-INFO: Loss too large (371.892->372.193)! Learning rate decreased to 0.01700.
2024-12-02-10:13:56-root-INFO: grad norm: 19.165 18.718 4.115
2024-12-02-10:13:57-root-INFO: grad norm: 18.033 17.710 3.401
2024-12-02-10:13:57-root-INFO: grad norm: 17.361 16.931 3.837
2024-12-02-10:13:58-root-INFO: grad norm: 16.538 16.222 3.218
2024-12-02-10:13:58-root-INFO: grad norm: 16.132 15.720 3.625
2024-12-02-10:13:59-root-INFO: grad norm: 15.594 15.284 3.094
2024-12-02-10:13:59-root-INFO: grad norm: 15.315 14.914 3.482
2024-12-02-10:13:59-root-INFO: Loss Change: 371.892 -> 363.234
2024-12-02-10:14:00-root-INFO: Regularization Change: 0.000 -> 1.062
2024-12-02-10:14:00-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-10:14:00-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-10:14:00-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-10:14:00-root-INFO: grad norm: 20.462 20.129 3.682
2024-12-02-10:14:00-root-INFO: Loss too large (364.080->375.325)! Learning rate decreased to 0.03427.
2024-12-02-10:14:00-root-INFO: Loss too large (364.080->370.107)! Learning rate decreased to 0.02742.
2024-12-02-10:14:00-root-INFO: Loss too large (364.080->366.570)! Learning rate decreased to 0.02194.
2024-12-02-10:14:00-root-INFO: Loss too large (364.080->364.307)! Learning rate decreased to 0.01755.
2024-12-02-10:14:01-root-INFO: grad norm: 18.636 18.194 4.032
2024-12-02-10:14:01-root-INFO: grad norm: 17.621 17.297 3.362
2024-12-02-10:14:02-root-INFO: grad norm: 16.979 16.547 3.805
2024-12-02-10:14:02-root-INFO: grad norm: 16.191 15.875 3.179
2024-12-02-10:14:03-root-INFO: grad norm: 15.783 15.366 3.603
2024-12-02-10:14:03-root-INFO: grad norm: 15.235 14.926 3.051
2024-12-02-10:14:04-root-INFO: grad norm: 14.942 14.537 3.456
2024-12-02-10:14:04-root-INFO: Loss Change: 364.080 -> 355.562
2024-12-02-10:14:04-root-INFO: Regularization Change: 0.000 -> 1.075
2024-12-02-10:14:04-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-10:14:04-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-10:14:04-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-10:14:04-root-INFO: grad norm: 20.899 20.552 3.794
2024-12-02-10:14:04-root-INFO: Loss too large (356.190->367.909)! Learning rate decreased to 0.03536.
2024-12-02-10:14:05-root-INFO: Loss too large (356.190->362.205)! Learning rate decreased to 0.02829.
2024-12-02-10:14:05-root-INFO: Loss too large (356.190->358.418)! Learning rate decreased to 0.02263.
2024-12-02-10:14:05-root-INFO: grad norm: 23.709 23.183 4.967
2024-12-02-10:14:06-root-INFO: grad norm: 29.655 29.217 5.073
2024-12-02-10:14:06-root-INFO: Loss too large (355.339->357.055)! Learning rate decreased to 0.01811.
2024-12-02-10:14:06-root-INFO: grad norm: 22.899 22.333 5.061
2024-12-02-10:14:07-root-INFO: grad norm: 15.537 15.241 3.018
2024-12-02-10:14:07-root-INFO: grad norm: 14.012 13.612 3.324
2024-12-02-10:14:08-root-INFO: grad norm: 12.810 12.527 2.676
2024-12-02-10:14:08-root-INFO: grad norm: 12.161 11.785 2.999
2024-12-02-10:14:08-root-INFO: Loss Change: 356.190 -> 346.679
2024-12-02-10:14:08-root-INFO: Regularization Change: 0.000 -> 1.259
2024-12-02-10:14:08-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-10:14:08-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-10:14:09-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-10:14:09-root-INFO: grad norm: 16.113 15.796 3.177
2024-12-02-10:14:09-root-INFO: Loss too large (347.369->354.028)! Learning rate decreased to 0.03648.
2024-12-02-10:14:09-root-INFO: Loss too large (347.369->350.695)! Learning rate decreased to 0.02919.
2024-12-02-10:14:09-root-INFO: Loss too large (347.369->348.463)! Learning rate decreased to 0.02335.
2024-12-02-10:14:10-root-INFO: grad norm: 19.116 18.624 4.307
2024-12-02-10:14:10-root-INFO: grad norm: 24.461 24.046 4.487
2024-12-02-10:14:10-root-INFO: Loss too large (346.521->347.561)! Learning rate decreased to 0.01868.
2024-12-02-10:14:11-root-INFO: grad norm: 19.854 19.321 4.569
2024-12-02-10:14:11-root-INFO: grad norm: 14.848 14.536 3.025
2024-12-02-10:14:12-root-INFO: grad norm: 13.345 12.946 3.239
2024-12-02-10:14:12-root-INFO: grad norm: 11.911 11.625 2.595
2024-12-02-10:14:13-root-INFO: grad norm: 11.113 10.752 2.810
2024-12-02-10:14:13-root-INFO: Loss Change: 347.369 -> 339.168
2024-12-02-10:14:13-root-INFO: Regularization Change: 0.000 -> 1.201
2024-12-02-10:14:13-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-10:14:13-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-10:14:13-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-10:14:13-root-INFO: grad norm: 16.445 16.136 3.169
2024-12-02-10:14:13-root-INFO: Loss too large (339.872->346.746)! Learning rate decreased to 0.03763.
2024-12-02-10:14:14-root-INFO: Loss too large (339.872->342.612)! Learning rate decreased to 0.03011.
2024-12-02-10:14:14-root-INFO: Loss too large (339.872->340.170)! Learning rate decreased to 0.02409.
2024-12-02-10:14:14-root-INFO: grad norm: 17.888 17.485 3.774
2024-12-02-10:14:15-root-INFO: grad norm: 22.159 21.775 4.108
2024-12-02-10:14:15-root-INFO: Loss too large (338.337->338.833)! Learning rate decreased to 0.01927.
2024-12-02-10:14:15-root-INFO: grad norm: 18.088 17.592 4.207
2024-12-02-10:14:16-root-INFO: grad norm: 14.403 14.089 2.991
2024-12-02-10:14:16-root-INFO: grad norm: 12.776 12.378 3.166
2024-12-02-10:14:17-root-INFO: grad norm: 11.294 11.008 2.529
2024-12-02-10:14:17-root-INFO: grad norm: 10.424 10.067 2.704
2024-12-02-10:14:18-root-INFO: Loss Change: 339.872 -> 331.341
2024-12-02-10:14:18-root-INFO: Regularization Change: 0.000 -> 1.239
2024-12-02-10:14:18-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-10:14:18-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-10:14:18-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-10:14:18-root-INFO: grad norm: 14.172 13.874 2.890
2024-12-02-10:14:18-root-INFO: Loss too large (331.528->336.306)! Learning rate decreased to 0.03881.
2024-12-02-10:14:18-root-INFO: Loss too large (331.528->333.428)! Learning rate decreased to 0.03105.
2024-12-02-10:14:18-root-INFO: Loss too large (331.528->331.681)! Learning rate decreased to 0.02484.
2024-12-02-10:14:19-root-INFO: grad norm: 16.049 15.609 3.732
2024-12-02-10:14:19-root-INFO: grad norm: 20.069 19.692 3.871
2024-12-02-10:14:20-root-INFO: Loss too large (330.178->330.506)! Learning rate decreased to 0.01987.
2024-12-02-10:14:20-root-INFO: grad norm: 16.642 16.149 4.021
2024-12-02-10:14:20-root-INFO: grad norm: 13.423 13.115 2.859
2024-12-02-10:14:21-root-INFO: grad norm: 11.903 11.511 3.028
2024-12-02-10:14:21-root-INFO: grad norm: 10.508 10.227 2.412
2024-12-02-10:14:22-root-INFO: grad norm: 9.658 9.311 2.566
2024-12-02-10:14:22-root-INFO: Loss Change: 331.528 -> 323.681
2024-12-02-10:14:22-root-INFO: Regularization Change: 0.000 -> 1.219
2024-12-02-10:14:22-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-10:14:22-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-10:14:22-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-10:14:23-root-INFO: grad norm: 14.012 13.713 2.880
2024-12-02-10:14:23-root-INFO: Loss too large (324.481->329.447)! Learning rate decreased to 0.04002.
2024-12-02-10:14:23-root-INFO: Loss too large (324.481->326.391)! Learning rate decreased to 0.03202.
2024-12-02-10:14:23-root-INFO: Loss too large (324.481->324.582)! Learning rate decreased to 0.02561.
2024-12-02-10:14:23-root-INFO: grad norm: 15.488 15.088 3.497
2024-12-02-10:14:24-root-INFO: grad norm: 18.859 18.492 3.701
2024-12-02-10:14:24-root-INFO: Loss too large (323.033->323.150)! Learning rate decreased to 0.02049.
2024-12-02-10:14:25-root-INFO: grad norm: 15.478 15.017 3.749
2024-12-02-10:14:25-root-INFO: grad norm: 12.557 12.252 2.754
2024-12-02-10:14:25-root-INFO: grad norm: 11.046 10.673 2.846
2024-12-02-10:14:26-root-INFO: grad norm: 9.727 9.449 2.313
2024-12-02-10:14:26-root-INFO: grad norm: 8.903 8.571 2.408
2024-12-02-10:14:27-root-INFO: Loss Change: 324.481 -> 316.780
2024-12-02-10:14:27-root-INFO: Regularization Change: 0.000 -> 1.218
2024-12-02-10:14:27-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-10:14:27-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-10:14:27-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-10:14:27-root-INFO: grad norm: 11.891 11.627 2.491
2024-12-02-10:14:27-root-INFO: Loss too large (317.097->319.846)! Learning rate decreased to 0.04126.
2024-12-02-10:14:27-root-INFO: Loss too large (317.097->317.805)! Learning rate decreased to 0.03301.
2024-12-02-10:14:28-root-INFO: grad norm: 16.910 16.508 3.667
2024-12-02-10:14:28-root-INFO: Loss too large (316.663->318.099)! Learning rate decreased to 0.02641.
2024-12-02-10:14:28-root-INFO: grad norm: 19.196 18.849 3.630
2024-12-02-10:14:29-root-INFO: grad norm: 20.557 20.007 4.720
2024-12-02-10:14:29-root-INFO: grad norm: 22.018 21.619 4.174
2024-12-02-10:14:30-root-INFO: grad norm: 22.255 21.644 5.177
2024-12-02-10:14:30-root-INFO: grad norm: 22.099 21.696 4.204
2024-12-02-10:14:31-root-INFO: grad norm: 21.881 21.287 5.066
2024-12-02-10:14:31-root-INFO: Loss Change: 317.097 -> 311.105
2024-12-02-10:14:31-root-INFO: Regularization Change: 0.000 -> 1.783
2024-12-02-10:14:31-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-10:14:31-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-10:14:31-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-10:14:32-root-INFO: grad norm: 26.589 26.147 4.830
2024-12-02-10:14:32-root-INFO: Loss too large (312.424->335.299)! Learning rate decreased to 0.04253.
2024-12-02-10:14:32-root-INFO: Loss too large (312.424->324.022)! Learning rate decreased to 0.03403.
2024-12-02-10:14:32-root-INFO: Loss too large (312.424->316.401)! Learning rate decreased to 0.02722.
2024-12-02-10:14:32-root-INFO: grad norm: 25.221 24.642 5.375
2024-12-02-10:14:33-root-INFO: grad norm: 23.610 23.225 4.245
2024-12-02-10:14:33-root-INFO: grad norm: 22.663 22.139 4.847
2024-12-02-10:14:34-root-INFO: grad norm: 21.578 21.215 3.939
2024-12-02-10:14:34-root-INFO: grad norm: 20.847 20.361 4.479
2024-12-02-10:14:35-root-INFO: grad norm: 20.040 19.694 3.711
2024-12-02-10:14:35-root-INFO: grad norm: 19.465 19.009 4.190
2024-12-02-10:14:36-root-INFO: Loss Change: 312.424 -> 302.788
2024-12-02-10:14:36-root-INFO: Regularization Change: 0.000 -> 1.638
2024-12-02-10:14:36-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-10:14:36-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-10:14:36-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-10:14:36-root-INFO: grad norm: 25.201 24.837 4.265
2024-12-02-10:14:36-root-INFO: Loss too large (304.490->326.315)! Learning rate decreased to 0.04384.
2024-12-02-10:14:36-root-INFO: Loss too large (304.490->315.066)! Learning rate decreased to 0.03507.
2024-12-02-10:14:36-root-INFO: Loss too large (304.490->307.672)! Learning rate decreased to 0.02806.
2024-12-02-10:14:37-root-INFO: grad norm: 23.656 23.217 4.539
2024-12-02-10:14:37-root-INFO: grad norm: 22.204 21.885 3.750
2024-12-02-10:14:38-root-INFO: grad norm: 21.097 20.694 4.103
2024-12-02-10:14:38-root-INFO: grad norm: 20.016 19.718 3.441
2024-12-02-10:14:39-root-INFO: grad norm: 19.206 18.837 3.746
2024-12-02-10:14:39-root-INFO: grad norm: 18.433 18.152 3.208
2024-12-02-10:14:40-root-INFO: grad norm: 17.844 17.502 3.475
2024-12-02-10:14:40-root-INFO: Loss Change: 304.490 -> 295.019
2024-12-02-10:14:40-root-INFO: Regularization Change: 0.000 -> 1.612
2024-12-02-10:14:40-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-10:14:40-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-10:14:40-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-10:14:40-root-INFO: grad norm: 22.111 21.804 3.670
2024-12-02-10:14:41-root-INFO: Loss too large (296.231->313.538)! Learning rate decreased to 0.04517.
2024-12-02-10:14:41-root-INFO: Loss too large (296.231->304.395)! Learning rate decreased to 0.03614.
2024-12-02-10:14:41-root-INFO: Loss too large (296.231->298.482)! Learning rate decreased to 0.02891.
2024-12-02-10:14:41-root-INFO: grad norm: 20.676 20.325 3.797
2024-12-02-10:14:42-root-INFO: grad norm: 19.367 19.095 3.232
2024-12-02-10:14:42-root-INFO: grad norm: 18.336 18.012 3.435
2024-12-02-10:14:43-root-INFO: grad norm: 17.395 17.142 2.952
2024-12-02-10:14:43-root-INFO: grad norm: 16.661 16.364 3.134
2024-12-02-10:14:44-root-INFO: grad norm: 15.999 15.762 2.740
2024-12-02-10:14:44-root-INFO: grad norm: 15.479 15.205 2.901
2024-12-02-10:14:45-root-INFO: Loss Change: 296.231 -> 287.561
2024-12-02-10:14:45-root-INFO: Regularization Change: 0.000 -> 1.568
2024-12-02-10:14:45-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-10:14:45-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-10:14:45-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-10:14:45-root-INFO: grad norm: 19.762 19.503 3.192
2024-12-02-10:14:45-root-INFO: Loss too large (288.676->302.965)! Learning rate decreased to 0.04654.
2024-12-02-10:14:45-root-INFO: Loss too large (288.676->295.317)! Learning rate decreased to 0.03724.
2024-12-02-10:14:45-root-INFO: Loss too large (288.676->290.430)! Learning rate decreased to 0.02979.
2024-12-02-10:14:46-root-INFO: grad norm: 18.857 18.601 3.099
2024-12-02-10:14:46-root-INFO: grad norm: 18.064 17.844 2.809
2024-12-02-10:14:47-root-INFO: grad norm: 17.368 17.128 2.878
2024-12-02-10:14:47-root-INFO: grad norm: 16.713 16.509 2.601
2024-12-02-10:14:48-root-INFO: grad norm: 16.168 15.945 2.676
2024-12-02-10:14:48-root-INFO: grad norm: 15.671 15.480 2.441
2024-12-02-10:14:49-root-INFO: grad norm: 15.264 15.055 2.514
2024-12-02-10:14:49-root-INFO: Loss Change: 288.676 -> 280.934
2024-12-02-10:14:49-root-INFO: Regularization Change: 0.000 -> 1.550
2024-12-02-10:14:49-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-10:14:49-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-10:14:49-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-10:14:49-root-INFO: grad norm: 19.987 19.764 2.978
2024-12-02-10:14:50-root-INFO: Loss too large (282.405->296.935)! Learning rate decreased to 0.04795.
2024-12-02-10:14:50-root-INFO: Loss too large (282.405->289.211)! Learning rate decreased to 0.03836.
2024-12-02-10:14:50-root-INFO: Loss too large (282.405->284.191)! Learning rate decreased to 0.03069.
2024-12-02-10:14:50-root-INFO: grad norm: 18.866 18.661 2.771
2024-12-02-10:14:51-root-INFO: grad norm: 17.934 17.748 2.575
2024-12-02-10:14:51-root-INFO: grad norm: 17.159 16.955 2.637
2024-12-02-10:14:52-root-INFO: grad norm: 16.455 16.277 2.411
2024-12-02-10:14:52-root-INFO: grad norm: 15.881 15.687 2.477
2024-12-02-10:14:53-root-INFO: grad norm: 15.372 15.202 2.280
2024-12-02-10:14:53-root-INFO: grad norm: 14.959 14.775 2.341
2024-12-02-10:14:54-root-INFO: Loss Change: 282.405 -> 274.641
2024-12-02-10:14:54-root-INFO: Regularization Change: 0.000 -> 1.553
2024-12-02-10:14:54-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-10:14:54-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-10:14:54-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-10:14:54-root-INFO: grad norm: 18.219 18.031 2.615
2024-12-02-10:14:54-root-INFO: Loss too large (275.272->287.508)! Learning rate decreased to 0.04939.
2024-12-02-10:14:54-root-INFO: Loss too large (275.272->280.903)! Learning rate decreased to 0.03951.
2024-12-02-10:14:55-root-INFO: Loss too large (275.272->276.625)! Learning rate decreased to 0.03161.
2024-12-02-10:14:55-root-INFO: grad norm: 17.101 16.928 2.425
2024-12-02-10:14:55-root-INFO: grad norm: 16.207 16.047 2.273
2024-12-02-10:14:56-root-INFO: grad norm: 15.476 15.309 2.267
2024-12-02-10:14:57-root-INFO: grad norm: 14.855 14.702 2.123
2024-12-02-10:14:57-root-INFO: grad norm: 14.352 14.193 2.131
2024-12-02-10:14:57-root-INFO: grad norm: 13.929 13.782 2.014
2024-12-02-10:14:58-root-INFO: grad norm: 13.588 13.436 2.027
2024-12-02-10:14:58-root-INFO: Loss Change: 275.272 -> 267.944
2024-12-02-10:14:58-root-INFO: Regularization Change: 0.000 -> 1.534
2024-12-02-10:14:58-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-10:14:58-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-10:14:58-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-10:14:59-root-INFO: grad norm: 19.346 19.173 2.584
2024-12-02-10:14:59-root-INFO: Loss too large (269.607->283.577)! Learning rate decreased to 0.05086.
2024-12-02-10:14:59-root-INFO: Loss too large (269.607->276.254)! Learning rate decreased to 0.04069.
2024-12-02-10:14:59-root-INFO: Loss too large (269.607->271.370)! Learning rate decreased to 0.03255.
2024-12-02-10:15:00-root-INFO: grad norm: 18.307 18.154 2.355
2024-12-02-10:15:00-root-INFO: grad norm: 17.418 17.270 2.269
2024-12-02-10:15:00-root-INFO: grad norm: 16.675 16.515 2.306
2024-12-02-10:15:01-root-INFO: grad norm: 16.012 15.865 2.169
2024-12-02-10:15:02-root-INFO: grad norm: 15.465 15.307 2.208
2024-12-02-10:15:02-root-INFO: grad norm: 14.997 14.852 2.080
2024-12-02-10:15:02-root-INFO: grad norm: 14.611 14.457 2.116
2024-12-02-10:15:03-root-INFO: Loss Change: 269.607 -> 262.227
2024-12-02-10:15:03-root-INFO: Regularization Change: 0.000 -> 1.561
2024-12-02-10:15:03-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-10:15:03-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-10:15:03-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-10:15:03-root-INFO: grad norm: 19.255 19.055 2.767
2024-12-02-10:15:03-root-INFO: Loss too large (263.421->278.105)! Learning rate decreased to 0.05237.
2024-12-02-10:15:03-root-INFO: Loss too large (263.421->270.285)! Learning rate decreased to 0.04190.
2024-12-02-10:15:04-root-INFO: Loss too large (263.421->265.164)! Learning rate decreased to 0.03352.
2024-12-02-10:15:04-root-INFO: grad norm: 18.034 17.868 2.443
2024-12-02-10:15:04-root-INFO: grad norm: 17.043 16.885 2.310
2024-12-02-10:15:05-root-INFO: grad norm: 16.190 16.036 2.228
2024-12-02-10:15:05-root-INFO: grad norm: 15.479 15.337 2.093
2024-12-02-10:15:06-root-INFO: grad norm: 14.885 14.743 2.050
2024-12-02-10:15:06-root-INFO: grad norm: 14.400 14.269 1.941
2024-12-02-10:15:07-root-INFO: grad norm: 13.998 13.866 1.919
2024-12-02-10:15:07-root-INFO: Loss Change: 263.421 -> 255.988
2024-12-02-10:15:07-root-INFO: Regularization Change: 0.000 -> 1.568
2024-12-02-10:15:07-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-10:15:07-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-10:15:07-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-10:15:07-root-INFO: grad norm: 17.887 17.727 2.380
2024-12-02-10:15:07-root-INFO: Loss too large (257.305->270.224)! Learning rate decreased to 0.05391.
2024-12-02-10:15:08-root-INFO: Loss too large (257.305->263.487)! Learning rate decreased to 0.04313.
2024-12-02-10:15:08-root-INFO: Loss too large (257.305->258.962)! Learning rate decreased to 0.03450.
2024-12-02-10:15:08-root-INFO: grad norm: 17.138 17.013 2.064
2024-12-02-10:15:09-root-INFO: grad norm: 16.427 16.303 2.016
2024-12-02-10:15:09-root-INFO: grad norm: 15.772 15.651 1.956
2024-12-02-10:15:10-root-INFO: grad norm: 15.209 15.093 1.877
2024-12-02-10:15:10-root-INFO: grad norm: 14.719 14.603 1.844
2024-12-02-10:15:11-root-INFO: grad norm: 14.318 14.208 1.775
2024-12-02-10:15:11-root-INFO: grad norm: 13.976 13.866 1.756
2024-12-02-10:15:11-root-INFO: Loss Change: 257.305 -> 250.586
2024-12-02-10:15:11-root-INFO: Regularization Change: 0.000 -> 1.540
2024-12-02-10:15:11-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-10:15:11-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-10:15:12-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-10:15:12-root-INFO: grad norm: 18.270 18.119 2.342
2024-12-02-10:15:12-root-INFO: Loss too large (251.986->265.219)! Learning rate decreased to 0.05550.
2024-12-02-10:15:12-root-INFO: Loss too large (251.986->258.171)! Learning rate decreased to 0.04440.
2024-12-02-10:15:12-root-INFO: Loss too large (251.986->253.422)! Learning rate decreased to 0.03552.
2024-12-02-10:15:13-root-INFO: grad norm: 16.829 16.711 1.984
2024-12-02-10:15:13-root-INFO: grad norm: 15.729 15.613 1.907
2024-12-02-10:15:14-root-INFO: grad norm: 14.804 14.691 1.827
2024-12-02-10:15:14-root-INFO: grad norm: 14.112 14.003 1.750
2024-12-02-10:15:15-root-INFO: grad norm: 13.528 13.421 1.698
2024-12-02-10:15:15-root-INFO: grad norm: 13.098 12.994 1.643
2024-12-02-10:15:15-root-INFO: grad norm: 12.738 12.636 1.608
2024-12-02-10:15:16-root-INFO: Loss Change: 251.986 -> 244.826
2024-12-02-10:15:16-root-INFO: Regularization Change: 0.000 -> 1.555
2024-12-02-10:15:16-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-10:15:16-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-10:15:16-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-10:15:16-root-INFO: grad norm: 15.933 15.819 1.904
2024-12-02-10:15:16-root-INFO: Loss too large (245.699->256.268)! Learning rate decreased to 0.05711.
2024-12-02-10:15:16-root-INFO: Loss too large (245.699->250.563)! Learning rate decreased to 0.04569.
2024-12-02-10:15:17-root-INFO: Loss too large (245.699->246.760)! Learning rate decreased to 0.03655.
2024-12-02-10:15:17-root-INFO: grad norm: 14.924 14.827 1.699
2024-12-02-10:15:18-root-INFO: grad norm: 14.127 14.032 1.630
2024-12-02-10:15:18-root-INFO: grad norm: 13.420 13.324 1.598
2024-12-02-10:15:18-root-INFO: grad norm: 12.902 12.811 1.534
2024-12-02-10:15:19-root-INFO: grad norm: 12.448 12.355 1.516
2024-12-02-10:15:19-root-INFO: grad norm: 12.125 12.036 1.470
2024-12-02-10:15:20-root-INFO: grad norm: 11.848 11.757 1.459
2024-12-02-10:15:20-root-INFO: Loss Change: 245.699 -> 239.340
2024-12-02-10:15:20-root-INFO: Regularization Change: 0.000 -> 1.523
2024-12-02-10:15:20-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-10:15:20-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-10:15:20-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-10:15:21-root-INFO: grad norm: 15.097 14.969 1.960
2024-12-02-10:15:21-root-INFO: Loss too large (240.033->249.928)! Learning rate decreased to 0.05877.
2024-12-02-10:15:21-root-INFO: Loss too large (240.033->244.597)! Learning rate decreased to 0.04702.
2024-12-02-10:15:21-root-INFO: Loss too large (240.033->241.045)! Learning rate decreased to 0.03761.
2024-12-02-10:15:21-root-INFO: grad norm: 14.565 14.473 1.634
2024-12-02-10:15:22-root-INFO: grad norm: 14.121 14.028 1.614
2024-12-02-10:15:22-root-INFO: grad norm: 13.676 13.591 1.520
2024-12-02-10:15:23-root-INFO: grad norm: 13.312 13.227 1.502
2024-12-02-10:15:23-root-INFO: grad norm: 12.956 12.875 1.441
2024-12-02-10:15:24-root-INFO: grad norm: 12.688 12.607 1.431
2024-12-02-10:15:24-root-INFO: grad norm: 12.432 12.354 1.386
2024-12-02-10:15:25-root-INFO: Loss Change: 240.033 -> 234.197
2024-12-02-10:15:25-root-INFO: Regularization Change: 0.000 -> 1.533
2024-12-02-10:15:25-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-10:15:25-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-10:15:25-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-10:15:25-root-INFO: grad norm: 15.611 15.500 1.857
2024-12-02-10:15:25-root-INFO: Loss too large (235.188->245.978)! Learning rate decreased to 0.06047.
2024-12-02-10:15:25-root-INFO: Loss too large (235.188->240.208)! Learning rate decreased to 0.04837.
2024-12-02-10:15:25-root-INFO: Loss too large (235.188->236.301)! Learning rate decreased to 0.03870.
2024-12-02-10:15:26-root-INFO: grad norm: 14.706 14.626 1.536
2024-12-02-10:15:26-root-INFO: grad norm: 13.971 13.888 1.517
2024-12-02-10:15:27-root-INFO: grad norm: 13.264 13.185 1.441
2024-12-02-10:15:27-root-INFO: grad norm: 12.765 12.686 1.415
2024-12-02-10:15:28-root-INFO: grad norm: 12.291 12.214 1.367
2024-12-02-10:15:28-root-INFO: grad norm: 11.968 11.891 1.351
2024-12-02-10:15:29-root-INFO: grad norm: 11.667 11.593 1.314
2024-12-02-10:15:29-root-INFO: Loss Change: 235.188 -> 229.150
2024-12-02-10:15:29-root-INFO: Regularization Change: 0.000 -> 1.509
2024-12-02-10:15:29-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-10:15:29-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-10:15:29-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-10:15:29-root-INFO: grad norm: 14.728 14.625 1.736
2024-12-02-10:15:29-root-INFO: Loss too large (230.109->240.142)! Learning rate decreased to 0.06220.
2024-12-02-10:15:30-root-INFO: Loss too large (230.109->234.663)! Learning rate decreased to 0.04976.
2024-12-02-10:15:30-root-INFO: Loss too large (230.109->231.026)! Learning rate decreased to 0.03981.
2024-12-02-10:15:30-root-INFO: grad norm: 13.782 13.711 1.401
2024-12-02-10:15:31-root-INFO: grad norm: 13.064 12.988 1.405
2024-12-02-10:15:31-root-INFO: grad norm: 12.366 12.296 1.309
2024-12-02-10:15:31-root-INFO: grad norm: 11.893 11.821 1.307
2024-12-02-10:15:32-root-INFO: grad norm: 11.433 11.366 1.243
2024-12-02-10:15:32-root-INFO: grad norm: 11.134 11.064 1.247
2024-12-02-10:15:33-root-INFO: grad norm: 10.849 10.783 1.199
2024-12-02-10:15:33-root-INFO: Loss Change: 230.109 -> 224.323
2024-12-02-10:15:33-root-INFO: Regularization Change: 0.000 -> 1.488
2024-12-02-10:15:33-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-10:15:33-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-10:15:33-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-10:15:33-root-INFO: grad norm: 13.436 13.344 1.574
2024-12-02-10:15:34-root-INFO: Loss too large (224.907->233.586)! Learning rate decreased to 0.06397.
2024-12-02-10:15:34-root-INFO: Loss too large (224.907->228.742)! Learning rate decreased to 0.05118.
2024-12-02-10:15:34-root-INFO: Loss too large (224.907->225.590)! Learning rate decreased to 0.04094.
2024-12-02-10:15:34-root-INFO: grad norm: 12.628 12.561 1.302
2024-12-02-10:15:35-root-INFO: grad norm: 12.042 11.972 1.290
2024-12-02-10:15:35-root-INFO: grad norm: 11.471 11.407 1.217
2024-12-02-10:15:36-root-INFO: grad norm: 11.087 11.021 1.209
2024-12-02-10:15:36-root-INFO: grad norm: 10.704 10.641 1.161
2024-12-02-10:15:37-root-INFO: grad norm: 10.457 10.393 1.161
2024-12-02-10:15:37-root-INFO: grad norm: 10.218 10.156 1.124
2024-12-02-10:15:37-root-INFO: Loss Change: 224.907 -> 219.515
2024-12-02-10:15:37-root-INFO: Regularization Change: 0.000 -> 1.475
2024-12-02-10:15:37-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-10:15:37-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-10:15:37-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-10:15:38-root-INFO: grad norm: 13.387 13.287 1.633
2024-12-02-10:15:38-root-INFO: Loss too large (220.469->229.546)! Learning rate decreased to 0.06579.
2024-12-02-10:15:38-root-INFO: Loss too large (220.469->224.501)! Learning rate decreased to 0.05263.
2024-12-02-10:15:38-root-INFO: Loss too large (220.469->221.236)! Learning rate decreased to 0.04210.
2024-12-02-10:15:39-root-INFO: grad norm: 12.696 12.630 1.293
2024-12-02-10:15:39-root-INFO: grad norm: 12.142 12.076 1.263
2024-12-02-10:15:39-root-INFO: grad norm: 11.575 11.515 1.180
2024-12-02-10:15:40-root-INFO: grad norm: 11.178 11.117 1.162
2024-12-02-10:15:40-root-INFO: grad norm: 10.757 10.699 1.115
2024-12-02-10:15:41-root-INFO: grad norm: 10.485 10.426 1.109
2024-12-02-10:15:41-root-INFO: grad norm: 10.203 10.146 1.074
2024-12-02-10:15:42-root-INFO: Loss Change: 220.469 -> 215.219
2024-12-02-10:15:42-root-INFO: Regularization Change: 0.000 -> 1.473
2024-12-02-10:15:42-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-10:15:42-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-10:15:42-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-10:15:42-root-INFO: grad norm: 13.323 13.233 1.545
2024-12-02-10:15:42-root-INFO: Loss too large (215.951->225.167)! Learning rate decreased to 0.06764.
2024-12-02-10:15:42-root-INFO: Loss too large (215.951->220.024)! Learning rate decreased to 0.05411.
2024-12-02-10:15:43-root-INFO: Loss too large (215.951->216.680)! Learning rate decreased to 0.04329.
2024-12-02-10:15:43-root-INFO: grad norm: 12.501 12.444 1.184
2024-12-02-10:15:43-root-INFO: grad norm: 11.879 11.820 1.181
2024-12-02-10:15:44-root-INFO: grad norm: 11.239 11.185 1.096
2024-12-02-10:15:45-root-INFO: grad norm: 10.824 10.768 1.093
2024-12-02-10:15:45-root-INFO: grad norm: 10.376 10.323 1.045
2024-12-02-10:15:46-root-INFO: grad norm: 10.104 10.050 1.047
2024-12-02-10:15:46-root-INFO: grad norm: 9.818 9.766 1.013
2024-12-02-10:15:46-root-INFO: Loss Change: 215.951 -> 210.698
2024-12-02-10:15:46-root-INFO: Regularization Change: 0.000 -> 1.472
2024-12-02-10:15:46-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-10:15:46-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-10:15:47-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-10:15:47-root-INFO: grad norm: 12.171 12.089 1.412
2024-12-02-10:15:47-root-INFO: Loss too large (211.363->219.268)! Learning rate decreased to 0.06953.
2024-12-02-10:15:47-root-INFO: Loss too large (211.363->214.716)! Learning rate decreased to 0.05563.
2024-12-02-10:15:47-root-INFO: Loss too large (211.363->211.837)! Learning rate decreased to 0.04450.
2024-12-02-10:15:48-root-INFO: grad norm: 11.212 11.157 1.112
2024-12-02-10:15:48-root-INFO: grad norm: 10.588 10.530 1.108
2024-12-02-10:15:49-root-INFO: grad norm: 9.970 9.918 1.022
2024-12-02-10:15:49-root-INFO: grad norm: 9.589 9.534 1.027
2024-12-02-10:15:50-root-INFO: grad norm: 9.199 9.147 0.977
2024-12-02-10:15:50-root-INFO: grad norm: 8.966 8.912 0.986
2024-12-02-10:15:51-root-INFO: grad norm: 8.734 8.682 0.951
2024-12-02-10:15:51-root-INFO: Loss Change: 211.363 -> 206.318
2024-12-02-10:15:51-root-INFO: Regularization Change: 0.000 -> 1.463
2024-12-02-10:15:51-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-10:15:51-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-10:15:51-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-10:15:51-root-INFO: grad norm: 11.611 11.526 1.406
2024-12-02-10:15:51-root-INFO: Loss too large (207.243->214.843)! Learning rate decreased to 0.07147.
2024-12-02-10:15:52-root-INFO: Loss too large (207.243->210.467)! Learning rate decreased to 0.05718.
2024-12-02-10:15:52-root-INFO: Loss too large (207.243->207.729)! Learning rate decreased to 0.04574.
2024-12-02-10:15:52-root-INFO: grad norm: 10.954 10.902 1.065
2024-12-02-10:15:53-root-INFO: grad norm: 10.489 10.434 1.071
2024-12-02-10:15:53-root-INFO: grad norm: 10.011 9.962 0.987
2024-12-02-10:15:54-root-INFO: grad norm: 9.695 9.644 0.997
2024-12-02-10:15:54-root-INFO: grad norm: 9.337 9.288 0.947
2024-12-02-10:15:55-root-INFO: grad norm: 9.121 9.070 0.961
2024-12-02-10:15:55-root-INFO: grad norm: 8.883 8.835 0.923
2024-12-02-10:15:56-root-INFO: Loss Change: 207.243 -> 202.508
2024-12-02-10:15:56-root-INFO: Regularization Change: 0.000 -> 1.468
2024-12-02-10:15:56-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-10:15:56-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-10:15:56-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-10:15:56-root-INFO: grad norm: 11.529 11.455 1.310
2024-12-02-10:15:56-root-INFO: Loss too large (203.074->210.812)! Learning rate decreased to 0.07345.
2024-12-02-10:15:56-root-INFO: Loss too large (203.074->206.377)! Learning rate decreased to 0.05876.
2024-12-02-10:15:56-root-INFO: Loss too large (203.074->203.568)! Learning rate decreased to 0.04701.
2024-12-02-10:15:57-root-INFO: grad norm: 10.879 10.833 0.998
2024-12-02-10:15:57-root-INFO: grad norm: 10.393 10.343 1.018
2024-12-02-10:15:58-root-INFO: grad norm: 9.860 9.815 0.941
2024-12-02-10:15:58-root-INFO: grad norm: 9.532 9.484 0.955
2024-12-02-10:15:59-root-INFO: grad norm: 9.146 9.100 0.909
2024-12-02-10:15:59-root-INFO: grad norm: 8.925 8.877 0.923
2024-12-02-10:16:00-root-INFO: grad norm: 8.674 8.629 0.889
2024-12-02-10:16:00-root-INFO: Loss Change: 203.074 -> 198.359
2024-12-02-10:16:00-root-INFO: Regularization Change: 0.000 -> 1.478
2024-12-02-10:16:00-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-10:16:00-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-10:16:00-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-10:16:00-root-INFO: grad norm: 11.417 11.339 1.335
2024-12-02-10:16:00-root-INFO: Loss too large (198.988->206.830)! Learning rate decreased to 0.07547.
2024-12-02-10:16:01-root-INFO: Loss too large (198.988->202.273)! Learning rate decreased to 0.06038.
2024-12-02-10:16:01-root-INFO: Loss too large (198.988->199.430)! Learning rate decreased to 0.04830.
2024-12-02-10:16:01-root-INFO: grad norm: 10.682 10.636 0.988
2024-12-02-10:16:02-root-INFO: grad norm: 10.161 10.112 1.001
2024-12-02-10:16:02-root-INFO: grad norm: 9.594 9.551 0.911
2024-12-02-10:16:03-root-INFO: grad norm: 9.252 9.205 0.929
2024-12-02-10:16:03-root-INFO: grad norm: 8.843 8.800 0.876
2024-12-02-10:16:04-root-INFO: grad norm: 8.617 8.571 0.894
2024-12-02-10:16:04-root-INFO: grad norm: 8.356 8.312 0.855
2024-12-02-10:16:04-root-INFO: Loss Change: 198.988 -> 194.249
2024-12-02-10:16:04-root-INFO: Regularization Change: 0.000 -> 1.504
2024-12-02-10:16:04-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-10:16:04-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-10:16:05-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-10:16:05-root-INFO: grad norm: 10.427 10.359 1.187
2024-12-02-10:16:05-root-INFO: Loss too large (194.836->201.525)! Learning rate decreased to 0.07753.
2024-12-02-10:16:05-root-INFO: Loss too large (194.836->197.528)! Learning rate decreased to 0.06203.
2024-12-02-10:16:05-root-INFO: Loss too large (194.836->195.102)! Learning rate decreased to 0.04962.
2024-12-02-10:16:06-root-INFO: grad norm: 9.638 9.591 0.958
2024-12-02-10:16:06-root-INFO: grad norm: 9.149 9.102 0.926
2024-12-02-10:16:07-root-INFO: grad norm: 8.642 8.597 0.885
2024-12-02-10:16:07-root-INFO: grad norm: 8.344 8.299 0.866
2024-12-02-10:16:08-root-INFO: grad norm: 8.011 7.966 0.850
2024-12-02-10:16:08-root-INFO: grad norm: 7.827 7.782 0.839
2024-12-02-10:16:08-root-INFO: grad norm: 7.629 7.583 0.829
2024-12-02-10:16:09-root-INFO: Loss Change: 194.836 -> 190.314
2024-12-02-10:16:09-root-INFO: Regularization Change: 0.000 -> 1.500
2024-12-02-10:16:09-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-10:16:09-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-10:16:09-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-10:16:09-root-INFO: grad norm: 9.885 9.822 1.122
2024-12-02-10:16:09-root-INFO: Loss too large (190.854->197.242)! Learning rate decreased to 0.07964.
2024-12-02-10:16:09-root-INFO: Loss too large (190.854->193.422)! Learning rate decreased to 0.06371.
2024-12-02-10:16:10-root-INFO: Loss too large (190.854->191.137)! Learning rate decreased to 0.05097.
2024-12-02-10:16:10-root-INFO: grad norm: 9.236 9.192 0.899
2024-12-02-10:16:10-root-INFO: grad norm: 8.796 8.751 0.892
2024-12-02-10:16:11-root-INFO: grad norm: 8.343 8.301 0.839
2024-12-02-10:16:11-root-INFO: grad norm: 8.067 8.023 0.842
2024-12-02-10:16:12-root-INFO: grad norm: 7.746 7.704 0.811
2024-12-02-10:16:12-root-INFO: grad norm: 7.567 7.522 0.816
2024-12-02-10:16:13-root-INFO: grad norm: 7.364 7.321 0.793
2024-12-02-10:16:13-root-INFO: Loss Change: 190.854 -> 186.560
2024-12-02-10:16:13-root-INFO: Regularization Change: 0.000 -> 1.490
2024-12-02-10:16:13-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-10:16:13-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-10:16:13-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-10:16:14-root-INFO: grad norm: 9.433 9.367 1.118
2024-12-02-10:16:14-root-INFO: Loss too large (187.216->193.088)! Learning rate decreased to 0.08180.
2024-12-02-10:16:14-root-INFO: Loss too large (187.216->189.503)! Learning rate decreased to 0.06544.
2024-12-02-10:16:14-root-INFO: Loss too large (187.216->187.390)! Learning rate decreased to 0.05235.
2024-12-02-10:16:15-root-INFO: grad norm: 8.736 8.695 0.849
2024-12-02-10:16:15-root-INFO: grad norm: 8.296 8.251 0.866
2024-12-02-10:16:16-root-INFO: grad norm: 7.839 7.799 0.797
2024-12-02-10:16:16-root-INFO: grad norm: 7.568 7.524 0.813
2024-12-02-10:16:17-root-INFO: grad norm: 7.251 7.209 0.774
2024-12-02-10:16:17-root-INFO: grad norm: 7.075 7.032 0.788
2024-12-02-10:16:18-root-INFO: grad norm: 6.877 6.835 0.759
2024-12-02-10:16:18-root-INFO: Loss Change: 187.216 -> 182.971
2024-12-02-10:16:18-root-INFO: Regularization Change: 0.000 -> 1.510
2024-12-02-10:16:18-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-10:16:18-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-10:16:18-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-10:16:18-root-INFO: grad norm: 9.866 9.791 1.213
2024-12-02-10:16:19-root-INFO: Loss too large (183.560->190.112)! Learning rate decreased to 0.08399.
2024-12-02-10:16:19-root-INFO: Loss too large (183.560->186.088)! Learning rate decreased to 0.06719.
2024-12-02-10:16:19-root-INFO: Loss too large (183.560->183.735)! Learning rate decreased to 0.05375.
2024-12-02-10:16:19-root-INFO: grad norm: 8.923 8.879 0.879
2024-12-02-10:16:20-root-INFO: grad norm: 8.348 8.304 0.859
2024-12-02-10:16:20-root-INFO: grad norm: 7.780 7.739 0.794
2024-12-02-10:16:21-root-INFO: grad norm: 7.447 7.405 0.791
2024-12-02-10:16:21-root-INFO: grad norm: 7.061 7.019 0.760
2024-12-02-10:16:22-root-INFO: grad norm: 6.851 6.808 0.761
2024-12-02-10:16:22-root-INFO: grad norm: 6.610 6.569 0.741
2024-12-02-10:16:23-root-INFO: Loss Change: 183.560 -> 179.118
2024-12-02-10:16:23-root-INFO: Regularization Change: 0.000 -> 1.551
2024-12-02-10:16:23-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-10:16:23-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-10:16:23-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-10:16:23-root-INFO: grad norm: 8.410 8.344 1.048
2024-12-02-10:16:23-root-INFO: Loss too large (179.803->184.608)! Learning rate decreased to 0.08623.
2024-12-02-10:16:23-root-INFO: Loss too large (179.803->181.563)! Learning rate decreased to 0.06899.
2024-12-02-10:16:23-root-INFO: Loss too large (179.803->179.822)! Learning rate decreased to 0.05519.
2024-12-02-10:16:24-root-INFO: grad norm: 7.722 7.682 0.788
2024-12-02-10:16:24-root-INFO: grad norm: 7.320 7.276 0.802
2024-12-02-10:16:25-root-INFO: grad norm: 6.889 6.849 0.741
2024-12-02-10:16:25-root-INFO: grad norm: 6.639 6.596 0.756
2024-12-02-10:16:26-root-INFO: grad norm: 6.343 6.302 0.720
2024-12-02-10:16:26-root-INFO: grad norm: 6.181 6.137 0.733
2024-12-02-10:16:27-root-INFO: grad norm: 5.994 5.952 0.707
2024-12-02-10:16:27-root-INFO: Loss Change: 179.803 -> 175.753
2024-12-02-10:16:27-root-INFO: Regularization Change: 0.000 -> 1.536
2024-12-02-10:16:27-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-10:16:27-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-10:16:27-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-10:16:27-root-INFO: grad norm: 8.436 8.368 1.068
2024-12-02-10:16:27-root-INFO: Loss too large (176.096->180.846)! Learning rate decreased to 0.08852.
2024-12-02-10:16:28-root-INFO: Loss too large (176.096->177.760)! Learning rate decreased to 0.07082.
2024-12-02-10:16:28-root-INFO: grad norm: 11.523 11.485 0.942
2024-12-02-10:16:28-root-INFO: Loss too large (176.034->177.371)! Learning rate decreased to 0.05665.
2024-12-02-10:16:29-root-INFO: grad norm: 9.866 9.825 0.898
2024-12-02-10:16:29-root-INFO: grad norm: 7.711 7.672 0.768
2024-12-02-10:16:30-root-INFO: grad norm: 6.929 6.889 0.749
2024-12-02-10:16:30-root-INFO: grad norm: 6.097 6.056 0.703
2024-12-02-10:16:31-root-INFO: grad norm: 5.696 5.653 0.698
2024-12-02-10:16:31-root-INFO: grad norm: 5.281 5.238 0.675
2024-12-02-10:16:32-root-INFO: Loss Change: 176.096 -> 171.814
2024-12-02-10:16:32-root-INFO: Regularization Change: 0.000 -> 1.663
2024-12-02-10:16:32-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-10:16:32-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-10:16:32-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-10:16:32-root-INFO: grad norm: 7.330 7.264 0.984
2024-12-02-10:16:32-root-INFO: Loss too large (172.313->175.641)! Learning rate decreased to 0.09086.
2024-12-02-10:16:32-root-INFO: Loss too large (172.313->173.348)! Learning rate decreased to 0.07268.
2024-12-02-10:16:33-root-INFO: grad norm: 9.693 9.655 0.858
2024-12-02-10:16:33-root-INFO: Loss too large (172.108->172.883)! Learning rate decreased to 0.05815.
2024-12-02-10:16:33-root-INFO: grad norm: 8.377 8.336 0.822
2024-12-02-10:16:34-root-INFO: grad norm: 6.820 6.782 0.720
2024-12-02-10:16:34-root-INFO: grad norm: 6.170 6.129 0.712
2024-12-02-10:16:35-root-INFO: grad norm: 5.473 5.432 0.672
2024-12-02-10:16:35-root-INFO: grad norm: 5.126 5.082 0.671
2024-12-02-10:16:36-root-INFO: grad norm: 4.766 4.721 0.650
2024-12-02-10:16:36-root-INFO: Loss Change: 172.313 -> 168.315
2024-12-02-10:16:36-root-INFO: Regularization Change: 0.000 -> 1.659
2024-12-02-10:16:36-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-10:16:36-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-10:16:36-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-10:16:36-root-INFO: grad norm: 6.587 6.523 0.919
2024-12-02-10:16:37-root-INFO: Loss too large (168.602->171.104)! Learning rate decreased to 0.09324.
2024-12-02-10:16:37-root-INFO: Loss too large (168.602->169.283)! Learning rate decreased to 0.07459.
2024-12-02-10:16:37-root-INFO: grad norm: 8.637 8.600 0.801
2024-12-02-10:16:37-root-INFO: Loss too large (168.320->168.866)! Learning rate decreased to 0.05967.
2024-12-02-10:16:38-root-INFO: grad norm: 7.573 7.534 0.766
2024-12-02-10:16:38-root-INFO: grad norm: 6.332 6.295 0.688
2024-12-02-10:16:39-root-INFO: grad norm: 5.782 5.742 0.678
2024-12-02-10:16:39-root-INFO: grad norm: 5.179 5.138 0.648
2024-12-02-10:16:40-root-INFO: grad norm: 4.874 4.831 0.644
2024-12-02-10:16:40-root-INFO: grad norm: 4.550 4.507 0.629
2024-12-02-10:16:41-root-INFO: Loss Change: 168.602 -> 164.784
2024-12-02-10:16:41-root-INFO: Regularization Change: 0.000 -> 1.677
2024-12-02-10:16:41-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-10:16:41-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-10:16:41-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-10:16:41-root-INFO: grad norm: 6.006 5.948 0.839
2024-12-02-10:16:41-root-INFO: Loss too large (164.985->167.048)! Learning rate decreased to 0.09566.
2024-12-02-10:16:41-root-INFO: Loss too large (164.985->165.510)! Learning rate decreased to 0.07653.
2024-12-02-10:16:42-root-INFO: grad norm: 7.940 7.903 0.757
2024-12-02-10:16:42-root-INFO: Loss too large (164.705->165.130)! Learning rate decreased to 0.06122.
2024-12-02-10:16:42-root-INFO: grad norm: 7.005 6.968 0.724
2024-12-02-10:16:43-root-INFO: grad norm: 5.880 5.842 0.663
2024-12-02-10:16:43-root-INFO: grad norm: 5.379 5.340 0.651
2024-12-02-10:16:44-root-INFO: grad norm: 4.823 4.782 0.628
2024-12-02-10:16:44-root-INFO: grad norm: 4.539 4.496 0.622
2024-12-02-10:16:45-root-INFO: grad norm: 4.237 4.193 0.610
2024-12-02-10:16:45-root-INFO: Loss Change: 164.985 -> 161.333
2024-12-02-10:16:45-root-INFO: Regularization Change: 0.000 -> 1.675
2024-12-02-10:16:45-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-10:16:45-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-10:16:45-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-10:16:45-root-INFO: grad norm: 6.207 6.144 0.879
2024-12-02-10:16:46-root-INFO: Loss too large (161.700->163.866)! Learning rate decreased to 0.09814.
2024-12-02-10:16:46-root-INFO: Loss too large (161.700->162.225)! Learning rate decreased to 0.07851.
2024-12-02-10:16:46-root-INFO: grad norm: 7.893 7.857 0.754
2024-12-02-10:16:46-root-INFO: Loss too large (161.369->161.747)! Learning rate decreased to 0.06281.
2024-12-02-10:16:47-root-INFO: grad norm: 6.826 6.789 0.713
2024-12-02-10:16:47-root-INFO: grad norm: 5.632 5.595 0.644
2024-12-02-10:16:48-root-INFO: grad norm: 5.102 5.063 0.628
2024-12-02-10:16:48-root-INFO: grad norm: 4.532 4.492 0.606
2024-12-02-10:16:49-root-INFO: grad norm: 4.241 4.199 0.598
2024-12-02-10:16:49-root-INFO: grad norm: 3.936 3.892 0.588
2024-12-02-10:16:50-root-INFO: Loss Change: 161.700 -> 157.972
2024-12-02-10:16:50-root-INFO: Regularization Change: 0.000 -> 1.707
2024-12-02-10:16:50-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-10:16:50-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-10:16:50-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-10:16:50-root-INFO: grad norm: 5.312 5.250 0.811
2024-12-02-10:16:50-root-INFO: Loss too large (158.273->159.545)! Learning rate decreased to 0.10066.
2024-12-02-10:16:50-root-INFO: Loss too large (158.273->158.450)! Learning rate decreased to 0.08053.
2024-12-02-10:16:51-root-INFO: grad norm: 6.601 6.564 0.697
2024-12-02-10:16:51-root-INFO: Loss too large (157.893->157.996)! Learning rate decreased to 0.06442.
2024-12-02-10:16:51-root-INFO: grad norm: 5.770 5.732 0.657
2024-12-02-10:16:52-root-INFO: grad norm: 4.896 4.858 0.612
2024-12-02-10:16:52-root-INFO: grad norm: 4.476 4.436 0.601
2024-12-02-10:16:53-root-INFO: grad norm: 4.025 3.982 0.583
2024-12-02-10:16:53-root-INFO: grad norm: 3.781 3.737 0.578
2024-12-02-10:16:53-root-INFO: grad norm: 3.528 3.482 0.568
2024-12-02-10:16:54-root-INFO: Loss Change: 158.273 -> 154.763
2024-12-02-10:16:54-root-INFO: Regularization Change: 0.000 -> 1.703
2024-12-02-10:16:54-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-10:16:54-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-10:16:54-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-10:16:54-root-INFO: grad norm: 5.349 5.282 0.846
2024-12-02-10:16:54-root-INFO: Loss too large (154.950->156.186)! Learning rate decreased to 0.10323.
2024-12-02-10:16:54-root-INFO: Loss too large (154.950->155.089)! Learning rate decreased to 0.08259.
2024-12-02-10:16:55-root-INFO: grad norm: 6.418 6.383 0.666
2024-12-02-10:16:55-root-INFO: Loss too large (154.533->154.576)! Learning rate decreased to 0.06607.
2024-12-02-10:16:56-root-INFO: grad norm: 5.537 5.499 0.649
2024-12-02-10:16:56-root-INFO: grad norm: 4.713 4.677 0.580
2024-12-02-10:16:57-root-INFO: grad norm: 4.300 4.261 0.581
2024-12-02-10:16:57-root-INFO: grad norm: 3.870 3.830 0.555
2024-12-02-10:16:58-root-INFO: grad norm: 3.635 3.593 0.557
2024-12-02-10:16:58-root-INFO: grad norm: 3.393 3.349 0.543
2024-12-02-10:16:58-root-INFO: Loss Change: 154.950 -> 151.435
2024-12-02-10:16:58-root-INFO: Regularization Change: 0.000 -> 1.731
2024-12-02-10:16:58-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-10:16:58-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-10:16:59-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-10:16:59-root-INFO: grad norm: 5.633 5.569 0.848
2024-12-02-10:16:59-root-INFO: Loss too large (151.925->153.497)! Learning rate decreased to 0.10585.
2024-12-02-10:16:59-root-INFO: Loss too large (151.925->152.185)! Learning rate decreased to 0.08468.
2024-12-02-10:17:00-root-INFO: grad norm: 6.710 6.674 0.689
2024-12-02-10:17:00-root-INFO: Loss too large (151.513->151.599)! Learning rate decreased to 0.06774.
2024-12-02-10:17:00-root-INFO: grad norm: 5.634 5.598 0.636
2024-12-02-10:17:01-root-INFO: grad norm: 4.604 4.569 0.570
2024-12-02-10:17:01-root-INFO: grad norm: 4.131 4.093 0.559
2024-12-02-10:17:02-root-INFO: grad norm: 3.655 3.615 0.538
2024-12-02-10:17:02-root-INFO: grad norm: 3.404 3.362 0.535
2024-12-02-10:17:03-root-INFO: grad norm: 3.155 3.111 0.525
2024-12-02-10:17:03-root-INFO: Loss Change: 151.925 -> 148.355
2024-12-02-10:17:03-root-INFO: Regularization Change: 0.000 -> 1.752
2024-12-02-10:17:03-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-10:17:03-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-10:17:03-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-10:17:03-root-INFO: grad norm: 4.692 4.630 0.760
2024-12-02-10:17:03-root-INFO: Loss too large (148.510->149.108)! Learning rate decreased to 0.10852.
2024-12-02-10:17:04-root-INFO: grad norm: 7.437 7.400 0.736
2024-12-02-10:17:04-root-INFO: Loss too large (148.377->149.799)! Learning rate decreased to 0.08682.
2024-12-02-10:17:04-root-INFO: Loss too large (148.377->148.544)! Learning rate decreased to 0.06945.
2024-12-02-10:17:05-root-INFO: grad norm: 5.948 5.914 0.642
2024-12-02-10:17:05-root-INFO: grad norm: 4.510 4.475 0.559
2024-12-02-10:17:06-root-INFO: grad norm: 3.915 3.878 0.542
2024-12-02-10:17:06-root-INFO: grad norm: 3.379 3.340 0.518
2024-12-02-10:17:07-root-INFO: grad norm: 3.105 3.062 0.514
2024-12-02-10:17:07-root-INFO: grad norm: 2.861 2.816 0.505
2024-12-02-10:17:07-root-INFO: Loss Change: 148.510 -> 145.024
2024-12-02-10:17:07-root-INFO: Regularization Change: 0.000 -> 1.899
2024-12-02-10:17:07-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-10:17:07-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-10:17:08-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-10:17:08-root-INFO: grad norm: 4.151 4.092 0.694
2024-12-02-10:17:08-root-INFO: Loss too large (145.336->145.682)! Learning rate decreased to 0.11124.
2024-12-02-10:17:08-root-INFO: grad norm: 6.552 6.517 0.675
2024-12-02-10:17:09-root-INFO: Loss too large (145.149->146.175)! Learning rate decreased to 0.08899.
2024-12-02-10:17:09-root-INFO: Loss too large (145.149->145.196)! Learning rate decreased to 0.07119.
2024-12-02-10:17:09-root-INFO: grad norm: 5.323 5.288 0.610
2024-12-02-10:17:10-root-INFO: grad norm: 4.177 4.143 0.536
2024-12-02-10:17:10-root-INFO: grad norm: 3.676 3.638 0.528
2024-12-02-10:17:11-root-INFO: grad norm: 3.215 3.175 0.504
2024-12-02-10:17:11-root-INFO: grad norm: 2.973 2.930 0.503
2024-12-02-10:17:12-root-INFO: grad norm: 2.755 2.711 0.492
2024-12-02-10:17:12-root-INFO: Loss Change: 145.336 -> 142.044
2024-12-02-10:17:12-root-INFO: Regularization Change: 0.000 -> 1.882
2024-12-02-10:17:12-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-10:17:12-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-10:17:12-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-10:17:12-root-INFO: grad norm: 4.132 4.075 0.682
2024-12-02-10:17:12-root-INFO: Loss too large (142.086->142.312)! Learning rate decreased to 0.11401.
2024-12-02-10:17:13-root-INFO: grad norm: 5.981 5.940 0.700
2024-12-02-10:17:13-root-INFO: Loss too large (141.828->142.430)! Learning rate decreased to 0.09121.
2024-12-02-10:17:14-root-INFO: grad norm: 6.400 6.362 0.696
2024-12-02-10:17:14-root-INFO: grad norm: 7.095 7.059 0.708
2024-12-02-10:17:14-root-INFO: grad norm: 7.219 7.183 0.719
2024-12-02-10:17:15-root-INFO: grad norm: 7.192 7.156 0.719
2024-12-02-10:17:15-root-INFO: grad norm: 7.093 7.057 0.717
2024-12-02-10:17:16-root-INFO: grad norm: 6.805 6.768 0.713
2024-12-02-10:17:16-root-INFO: Loss Change: 142.086 -> 139.269
2024-12-02-10:17:16-root-INFO: Regularization Change: 0.000 -> 2.790
2024-12-02-10:17:16-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-10:17:16-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-10:17:16-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-10:17:16-root-INFO: grad norm: 8.875 8.807 1.097
2024-12-02-10:17:17-root-INFO: Loss too large (140.014->144.440)! Learning rate decreased to 0.11682.
2024-12-02-10:17:17-root-INFO: Loss too large (140.014->140.904)! Learning rate decreased to 0.09346.
2024-12-02-10:17:17-root-INFO: grad norm: 7.969 7.923 0.862
2024-12-02-10:17:18-root-INFO: grad norm: 7.180 7.137 0.778
2024-12-02-10:17:18-root-INFO: grad norm: 6.630 6.591 0.722
2024-12-02-10:17:19-root-INFO: grad norm: 6.351 6.312 0.702
2024-12-02-10:17:19-root-INFO: grad norm: 6.039 6.001 0.680
2024-12-02-10:17:20-root-INFO: grad norm: 5.875 5.837 0.665
2024-12-02-10:17:20-root-INFO: grad norm: 5.681 5.643 0.654
2024-12-02-10:17:20-root-INFO: Loss Change: 140.014 -> 135.349
2024-12-02-10:17:20-root-INFO: Regularization Change: 0.000 -> 2.608
2024-12-02-10:17:20-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-10:17:20-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-10:17:21-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-10:17:21-root-INFO: grad norm: 7.416 7.352 0.975
2024-12-02-10:17:21-root-INFO: Loss too large (135.958->138.959)! Learning rate decreased to 0.11969.
2024-12-02-10:17:21-root-INFO: Loss too large (135.958->136.473)! Learning rate decreased to 0.09575.
2024-12-02-10:17:21-root-INFO: grad norm: 6.824 6.780 0.771
2024-12-02-10:17:22-root-INFO: grad norm: 6.352 6.309 0.737
2024-12-02-10:17:22-root-INFO: grad norm: 5.945 5.904 0.693
2024-12-02-10:17:23-root-INFO: grad norm: 5.653 5.614 0.660
2024-12-02-10:17:23-root-INFO: grad norm: 5.370 5.332 0.644
2024-12-02-10:17:24-root-INFO: grad norm: 5.172 5.135 0.615
2024-12-02-10:17:24-root-INFO: grad norm: 4.977 4.939 0.610
2024-12-02-10:17:25-root-INFO: Loss Change: 135.958 -> 131.773
2024-12-02-10:17:25-root-INFO: Regularization Change: 0.000 -> 2.591
2024-12-02-10:17:25-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-10:17:25-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-10:17:25-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-10:17:25-root-INFO: grad norm: 6.566 6.506 0.885
2024-12-02-10:17:25-root-INFO: Loss too large (132.281->134.393)! Learning rate decreased to 0.12261.
2024-12-02-10:17:25-root-INFO: Loss too large (132.281->132.574)! Learning rate decreased to 0.09809.
2024-12-02-10:17:26-root-INFO: grad norm: 5.902 5.858 0.717
2024-12-02-10:17:26-root-INFO: grad norm: 5.345 5.304 0.657
2024-12-02-10:17:27-root-INFO: grad norm: 4.955 4.915 0.624
2024-12-02-10:17:27-root-INFO: grad norm: 4.676 4.639 0.587
2024-12-02-10:17:28-root-INFO: grad norm: 4.425 4.387 0.576
2024-12-02-10:17:28-root-INFO: grad norm: 4.248 4.212 0.549
2024-12-02-10:17:29-root-INFO: grad norm: 4.084 4.047 0.546
2024-12-02-10:17:29-root-INFO: Loss Change: 132.281 -> 128.293
2024-12-02-10:17:29-root-INFO: Regularization Change: 0.000 -> 2.584
2024-12-02-10:17:29-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-10:17:29-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-10:17:29-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-10:17:29-root-INFO: grad norm: 5.899 5.833 0.878
2024-12-02-10:17:29-root-INFO: Loss too large (128.747->130.212)! Learning rate decreased to 0.12558.
2024-12-02-10:17:30-root-INFO: Loss too large (128.747->128.881)! Learning rate decreased to 0.10047.
2024-12-02-10:17:30-root-INFO: grad norm: 5.246 5.205 0.652
2024-12-02-10:17:31-root-INFO: grad norm: 4.696 4.656 0.613
2024-12-02-10:17:31-root-INFO: grad norm: 4.381 4.344 0.571
2024-12-02-10:17:31-root-INFO: grad norm: 4.141 4.105 0.547
2024-12-02-10:17:32-root-INFO: grad norm: 3.942 3.906 0.534
2024-12-02-10:17:32-root-INFO: grad norm: 3.796 3.761 0.514
2024-12-02-10:17:33-root-INFO: grad norm: 3.668 3.632 0.512
2024-12-02-10:17:33-root-INFO: Loss Change: 128.747 -> 124.954
2024-12-02-10:17:33-root-INFO: Regularization Change: 0.000 -> 2.603
2024-12-02-10:17:33-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-10:17:33-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-10:17:33-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-10:17:33-root-INFO: grad norm: 5.281 5.222 0.783
2024-12-02-10:17:34-root-INFO: Loss too large (125.228->126.310)! Learning rate decreased to 0.12860.
2024-12-02-10:17:34-root-INFO: Loss too large (125.228->125.262)! Learning rate decreased to 0.10288.
2024-12-02-10:17:34-root-INFO: grad norm: 4.746 4.706 0.618
2024-12-02-10:17:35-root-INFO: grad norm: 4.311 4.273 0.573
2024-12-02-10:17:35-root-INFO: grad norm: 4.028 3.990 0.548
2024-12-02-10:17:36-root-INFO: grad norm: 3.808 3.774 0.513
2024-12-02-10:17:36-root-INFO: grad norm: 3.637 3.601 0.512
2024-12-02-10:17:37-root-INFO: grad norm: 3.512 3.479 0.483
2024-12-02-10:17:37-root-INFO: grad norm: 3.410 3.374 0.492
2024-12-02-10:17:37-root-INFO: Loss Change: 125.228 -> 121.618
2024-12-02-10:17:37-root-INFO: Regularization Change: 0.000 -> 2.615
2024-12-02-10:17:37-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-10:17:37-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-10:17:38-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-10:17:38-root-INFO: grad norm: 4.577 4.529 0.661
2024-12-02-10:17:38-root-INFO: Loss too large (121.920->122.529)! Learning rate decreased to 0.13168.
2024-12-02-10:17:38-root-INFO: grad norm: 5.446 5.400 0.702
2024-12-02-10:17:39-root-INFO: grad norm: 6.708 6.660 0.805
2024-12-02-10:17:39-root-INFO: Loss too large (121.612->122.027)! Learning rate decreased to 0.10534.
2024-12-02-10:17:39-root-INFO: grad norm: 5.586 5.540 0.718
2024-12-02-10:17:40-root-INFO: grad norm: 4.486 4.450 0.572
2024-12-02-10:17:40-root-INFO: grad norm: 3.987 3.950 0.542
2024-12-02-10:17:41-root-INFO: grad norm: 3.687 3.654 0.492
2024-12-02-10:17:41-root-INFO: grad norm: 3.447 3.411 0.493
2024-12-02-10:17:42-root-INFO: Loss Change: 121.920 -> 118.426
2024-12-02-10:17:42-root-INFO: Regularization Change: 0.000 -> 2.916
2024-12-02-10:17:42-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-10:17:42-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-10:17:42-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-10:17:42-root-INFO: grad norm: 5.537 5.467 0.882
2024-12-02-10:17:42-root-INFO: Loss too large (118.755->119.915)! Learning rate decreased to 0.13480.
2024-12-02-10:17:42-root-INFO: Loss too large (118.755->118.793)! Learning rate decreased to 0.10784.
2024-12-02-10:17:43-root-INFO: grad norm: 4.713 4.672 0.620
2024-12-02-10:17:43-root-INFO: grad norm: 4.068 4.029 0.559
2024-12-02-10:17:44-root-INFO: grad norm: 3.726 3.690 0.519
2024-12-02-10:17:44-root-INFO: grad norm: 3.472 3.438 0.479
2024-12-02-10:17:45-root-INFO: grad norm: 3.308 3.273 0.478
2024-12-02-10:17:45-root-INFO: grad norm: 3.192 3.161 0.445
2024-12-02-10:17:46-root-INFO: grad norm: 3.117 3.083 0.459
2024-12-02-10:17:46-root-INFO: Loss Change: 118.755 -> 115.072
2024-12-02-10:17:46-root-INFO: Regularization Change: 0.000 -> 2.681
2024-12-02-10:17:46-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-10:17:46-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-10:17:46-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-10:17:46-root-INFO: grad norm: 4.296 4.247 0.647
2024-12-02-10:17:47-root-INFO: Loss too large (115.360->115.884)! Learning rate decreased to 0.13798.
2024-12-02-10:17:47-root-INFO: grad norm: 5.031 4.985 0.676
2024-12-02-10:17:48-root-INFO: grad norm: 6.302 6.253 0.782
2024-12-02-10:17:48-root-INFO: Loss too large (114.967->115.402)! Learning rate decreased to 0.11038.
2024-12-02-10:17:48-root-INFO: grad norm: 5.160 5.114 0.693
2024-12-02-10:17:49-root-INFO: grad norm: 3.877 3.841 0.524
2024-12-02-10:17:49-root-INFO: grad norm: 3.473 3.438 0.498
2024-12-02-10:17:50-root-INFO: grad norm: 3.265 3.234 0.448
2024-12-02-10:17:50-root-INFO: grad norm: 3.122 3.088 0.459
2024-12-02-10:17:50-root-INFO: Loss Change: 115.360 -> 111.975
2024-12-02-10:17:50-root-INFO: Regularization Change: 0.000 -> 2.961
2024-12-02-10:17:50-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-10:17:50-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-10:17:51-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-10:17:51-root-INFO: grad norm: 5.349 5.280 0.854
2024-12-02-10:17:51-root-INFO: Loss too large (112.295->113.344)! Learning rate decreased to 0.14121.
2024-12-02-10:17:51-root-INFO: Loss too large (112.295->112.356)! Learning rate decreased to 0.11297.
2024-12-02-10:17:52-root-INFO: grad norm: 4.513 4.469 0.625
2024-12-02-10:17:52-root-INFO: grad norm: 3.800 3.765 0.520
2024-12-02-10:17:52-root-INFO: grad norm: 3.534 3.498 0.506
2024-12-02-10:17:53-root-INFO: grad norm: 3.344 3.314 0.450
2024-12-02-10:17:53-root-INFO: grad norm: 3.237 3.203 0.468
2024-12-02-10:17:54-root-INFO: grad norm: 3.165 3.137 0.424
2024-12-02-10:17:54-root-INFO: grad norm: 3.126 3.094 0.452
2024-12-02-10:17:55-root-INFO: Loss Change: 112.295 -> 108.744
2024-12-02-10:17:55-root-INFO: Regularization Change: 0.000 -> 2.736
2024-12-02-10:17:55-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-10:17:55-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-10:17:55-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-10:17:55-root-INFO: grad norm: 4.293 4.243 0.656
2024-12-02-10:17:55-root-INFO: Loss too large (108.864->109.458)! Learning rate decreased to 0.14449.
2024-12-02-10:17:55-root-INFO: Loss too large (108.864->108.865)! Learning rate decreased to 0.11559.
2024-12-02-10:17:56-root-INFO: grad norm: 3.923 3.886 0.542
2024-12-02-10:17:56-root-INFO: grad norm: 3.624 3.592 0.481
2024-12-02-10:17:57-root-INFO: grad norm: 3.506 3.471 0.492
2024-12-02-10:17:57-root-INFO: grad norm: 3.406 3.377 0.442
2024-12-02-10:17:58-root-INFO: grad norm: 3.358 3.325 0.472
2024-12-02-10:17:58-root-INFO: grad norm: 3.327 3.300 0.426
2024-12-02-10:17:58-root-INFO: grad norm: 3.312 3.279 0.464
2024-12-02-10:17:59-root-INFO: Loss Change: 108.864 -> 105.721
2024-12-02-10:17:59-root-INFO: Regularization Change: 0.000 -> 2.714
2024-12-02-10:17:59-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-10:17:59-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-10:17:59-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-10:17:59-root-INFO: grad norm: 4.523 4.478 0.637
2024-12-02-10:17:59-root-INFO: Loss too large (105.979->106.723)! Learning rate decreased to 0.14783.
2024-12-02-10:17:59-root-INFO: Loss too large (105.979->106.097)! Learning rate decreased to 0.11826.
2024-12-02-10:18:00-root-INFO: grad norm: 4.029 3.990 0.554
2024-12-02-10:18:00-root-INFO: grad norm: 3.442 3.412 0.456
2024-12-02-10:18:01-root-INFO: grad norm: 3.352 3.318 0.475
2024-12-02-10:18:01-root-INFO: grad norm: 3.316 3.289 0.422
2024-12-02-10:18:02-root-INFO: grad norm: 3.294 3.262 0.460
2024-12-02-10:18:02-root-INFO: grad norm: 3.300 3.274 0.413
2024-12-02-10:18:03-root-INFO: grad norm: 3.299 3.267 0.457
2024-12-02-10:18:03-root-INFO: Loss Change: 105.979 -> 102.850
2024-12-02-10:18:03-root-INFO: Regularization Change: 0.000 -> 2.726
2024-12-02-10:18:03-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-10:18:03-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-10:18:03-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-10:18:03-root-INFO: grad norm: 4.933 4.876 0.746
2024-12-02-10:18:04-root-INFO: Loss too large (103.147->104.156)! Learning rate decreased to 0.15121.
2024-12-02-10:18:04-root-INFO: Loss too large (103.147->103.340)! Learning rate decreased to 0.12097.
2024-12-02-10:18:04-root-INFO: grad norm: 4.306 4.265 0.594
2024-12-02-10:18:05-root-INFO: grad norm: 3.634 3.603 0.476
2024-12-02-10:18:05-root-INFO: grad norm: 3.508 3.474 0.491
2024-12-02-10:18:06-root-INFO: grad norm: 3.453 3.427 0.429
2024-12-02-10:18:06-root-INFO: grad norm: 3.413 3.380 0.469
2024-12-02-10:18:07-root-INFO: grad norm: 3.396 3.371 0.415
2024-12-02-10:18:07-root-INFO: grad norm: 3.386 3.354 0.461
2024-12-02-10:18:08-root-INFO: Loss Change: 103.147 -> 99.896
2024-12-02-10:18:08-root-INFO: Regularization Change: 0.000 -> 2.801
2024-12-02-10:18:08-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-10:18:08-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-10:18:08-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-10:18:08-root-INFO: grad norm: 4.494 4.449 0.631
2024-12-02-10:18:08-root-INFO: Loss too large (100.070->100.965)! Learning rate decreased to 0.15465.
2024-12-02-10:18:08-root-INFO: Loss too large (100.070->100.267)! Learning rate decreased to 0.12372.
2024-12-02-10:18:09-root-INFO: grad norm: 4.117 4.079 0.560
2024-12-02-10:18:09-root-INFO: grad norm: 3.707 3.676 0.471
2024-12-02-10:18:10-root-INFO: grad norm: 3.627 3.593 0.494
2024-12-02-10:18:10-root-INFO: grad norm: 3.584 3.557 0.436
2024-12-02-10:18:11-root-INFO: grad norm: 3.553 3.520 0.478
2024-12-02-10:18:11-root-INFO: grad norm: 3.533 3.507 0.424
2024-12-02-10:18:11-root-INFO: grad norm: 3.523 3.492 0.471
2024-12-02-10:18:12-root-INFO: Loss Change: 100.070 -> 97.015
2024-12-02-10:18:12-root-INFO: Regularization Change: 0.000 -> 2.811
2024-12-02-10:18:12-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-10:18:12-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-10:18:12-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-10:18:12-root-INFO: grad norm: 4.673 4.620 0.700
2024-12-02-10:18:12-root-INFO: Loss too large (97.340->98.326)! Learning rate decreased to 0.15815.
2024-12-02-10:18:12-root-INFO: Loss too large (97.340->97.552)! Learning rate decreased to 0.12652.
2024-12-02-10:18:13-root-INFO: grad norm: 4.179 4.141 0.561
2024-12-02-10:18:13-root-INFO: grad norm: 3.665 3.634 0.476
2024-12-02-10:18:14-root-INFO: grad norm: 3.572 3.540 0.483
2024-12-02-10:18:14-root-INFO: grad norm: 3.535 3.509 0.432
2024-12-02-10:18:15-root-INFO: grad norm: 3.503 3.472 0.467
2024-12-02-10:18:15-root-INFO: grad norm: 3.489 3.464 0.418
2024-12-02-10:18:16-root-INFO: grad norm: 3.480 3.450 0.461
2024-12-02-10:18:16-root-INFO: Loss Change: 97.340 -> 94.234
2024-12-02-10:18:16-root-INFO: Regularization Change: 0.000 -> 2.857
2024-12-02-10:18:16-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-10:18:16-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-10:18:16-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-10:18:16-root-INFO: grad norm: 5.152 5.093 0.778
2024-12-02-10:18:17-root-INFO: Loss too large (94.382->95.712)! Learning rate decreased to 0.16169.
2024-12-02-10:18:17-root-INFO: Loss too large (94.382->94.754)! Learning rate decreased to 0.12935.
2024-12-02-10:18:17-root-INFO: grad norm: 4.487 4.444 0.621
2024-12-02-10:18:18-root-INFO: grad norm: 3.707 3.674 0.490
2024-12-02-10:18:18-root-INFO: grad norm: 3.622 3.588 0.495
2024-12-02-10:18:19-root-INFO: grad norm: 3.653 3.626 0.442
2024-12-02-10:18:19-root-INFO: grad norm: 3.623 3.591 0.479
2024-12-02-10:18:20-root-INFO: grad norm: 3.611 3.585 0.429
2024-12-02-10:18:20-root-INFO: grad norm: 3.601 3.570 0.471
2024-12-02-10:18:21-root-INFO: Loss Change: 94.382 -> 91.160
2024-12-02-10:18:21-root-INFO: Regularization Change: 0.000 -> 2.928
2024-12-02-10:18:21-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-10:18:21-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-10:18:21-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-10:18:21-root-INFO: grad norm: 4.474 4.433 0.610
2024-12-02-10:18:21-root-INFO: Loss too large (91.352->92.330)! Learning rate decreased to 0.16529.
2024-12-02-10:18:21-root-INFO: Loss too large (91.352->91.605)! Learning rate decreased to 0.13223.
2024-12-02-10:18:22-root-INFO: grad norm: 4.058 4.022 0.539
2024-12-02-10:18:22-root-INFO: grad norm: 3.552 3.523 0.452
2024-12-02-10:18:23-root-INFO: grad norm: 3.491 3.460 0.464
2024-12-02-10:18:23-root-INFO: grad norm: 3.484 3.458 0.421
2024-12-02-10:18:24-root-INFO: grad norm: 3.472 3.442 0.453
2024-12-02-10:18:24-root-INFO: grad norm: 3.485 3.460 0.414
2024-12-02-10:18:25-root-INFO: grad norm: 3.487 3.457 0.452
2024-12-02-10:18:25-root-INFO: Loss Change: 91.352 -> 88.392
2024-12-02-10:18:25-root-INFO: Regularization Change: 0.000 -> 2.891
2024-12-02-10:18:25-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-10:18:25-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-10:18:25-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-10:18:25-root-INFO: grad norm: 4.780 4.727 0.710
2024-12-02-10:18:26-root-INFO: Loss too large (88.680->89.930)! Learning rate decreased to 0.16894.
2024-12-02-10:18:26-root-INFO: Loss too large (88.680->89.046)! Learning rate decreased to 0.13515.
2024-12-02-10:18:26-root-INFO: grad norm: 4.284 4.246 0.570
2024-12-02-10:18:27-root-INFO: grad norm: 3.727 3.696 0.478
2024-12-02-10:18:27-root-INFO: grad norm: 3.667 3.635 0.484
2024-12-02-10:18:28-root-INFO: grad norm: 3.685 3.658 0.438
2024-12-02-10:18:28-root-INFO: grad norm: 3.658 3.627 0.472
2024-12-02-10:18:29-root-INFO: grad norm: 3.642 3.617 0.427
2024-12-02-10:18:29-root-INFO: grad norm: 3.634 3.604 0.466
2024-12-02-10:18:30-root-INFO: Loss Change: 88.680 -> 85.718
2024-12-02-10:18:30-root-INFO: Regularization Change: 0.000 -> 2.887
2024-12-02-10:18:30-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-10:18:30-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-10:18:30-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-10:18:30-root-INFO: grad norm: 4.939 4.886 0.722
2024-12-02-10:18:30-root-INFO: Loss too large (85.999->87.401)! Learning rate decreased to 0.17265.
2024-12-02-10:18:30-root-INFO: Loss too large (85.999->86.463)! Learning rate decreased to 0.13812.
2024-12-02-10:18:31-root-INFO: grad norm: 4.361 4.323 0.576
2024-12-02-10:18:31-root-INFO: grad norm: 3.658 3.627 0.475
2024-12-02-10:18:32-root-INFO: grad norm: 3.628 3.597 0.474
2024-12-02-10:18:32-root-INFO: grad norm: 3.714 3.688 0.438
2024-12-02-10:18:33-root-INFO: grad norm: 3.700 3.670 0.471
2024-12-02-10:18:33-root-INFO: grad norm: 3.699 3.674 0.430
2024-12-02-10:18:33-root-INFO: grad norm: 3.696 3.666 0.469
2024-12-02-10:18:34-root-INFO: Loss Change: 85.999 -> 83.093
2024-12-02-10:18:34-root-INFO: Regularization Change: 0.000 -> 2.854
2024-12-02-10:18:34-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-10:18:34-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-10:18:34-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-10:18:34-root-INFO: grad norm: 4.841 4.791 0.690
2024-12-02-10:18:34-root-INFO: Loss too large (83.214->84.690)! Learning rate decreased to 0.17641.
2024-12-02-10:18:34-root-INFO: Loss too large (83.214->83.720)! Learning rate decreased to 0.14113.
2024-12-02-10:18:35-root-INFO: grad norm: 4.371 4.331 0.590
2024-12-02-10:18:35-root-INFO: grad norm: 3.795 3.764 0.486
2024-12-02-10:18:36-root-INFO: grad norm: 3.745 3.712 0.496
2024-12-02-10:18:36-root-INFO: grad norm: 3.771 3.744 0.449
2024-12-02-10:18:37-root-INFO: grad norm: 3.745 3.713 0.485
2024-12-02-10:18:37-root-INFO: grad norm: 3.725 3.699 0.439
2024-12-02-10:18:38-root-INFO: grad norm: 3.721 3.690 0.479
2024-12-02-10:18:38-root-INFO: Loss Change: 83.214 -> 80.422
2024-12-02-10:18:38-root-INFO: Regularization Change: 0.000 -> 2.821
2024-12-02-10:18:38-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-10:18:38-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-10:18:38-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-10:18:39-root-INFO: grad norm: 5.119 5.068 0.722
2024-12-02-10:18:39-root-INFO: Loss too large (80.596->82.277)! Learning rate decreased to 0.18022.
2024-12-02-10:18:39-root-INFO: Loss too large (80.596->81.197)! Learning rate decreased to 0.14417.
2024-12-02-10:18:39-root-INFO: grad norm: 4.489 4.445 0.624
2024-12-02-10:18:40-root-INFO: grad norm: 3.704 3.672 0.487
2024-12-02-10:18:40-root-INFO: grad norm: 3.669 3.635 0.500
2024-12-02-10:18:41-root-INFO: grad norm: 3.755 3.728 0.454
2024-12-02-10:18:41-root-INFO: grad norm: 3.753 3.720 0.494
2024-12-02-10:18:42-root-INFO: grad norm: 3.770 3.743 0.452
2024-12-02-10:18:42-root-INFO: grad norm: 3.771 3.739 0.492
2024-12-02-10:18:43-root-INFO: Loss Change: 80.596 -> 77.777
2024-12-02-10:18:43-root-INFO: Regularization Change: 0.000 -> 2.822
2024-12-02-10:18:43-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-10:18:43-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-10:18:43-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-10:18:43-root-INFO: grad norm: 4.838 4.782 0.736
2024-12-02-10:18:43-root-INFO: Loss too large (78.060->79.687)! Learning rate decreased to 0.18408.
2024-12-02-10:18:43-root-INFO: Loss too large (78.060->78.613)! Learning rate decreased to 0.14727.
2024-12-02-10:18:44-root-INFO: grad norm: 4.418 4.376 0.607
2024-12-02-10:18:44-root-INFO: grad norm: 3.959 3.924 0.531
2024-12-02-10:18:45-root-INFO: grad norm: 3.908 3.872 0.530
2024-12-02-10:18:45-root-INFO: grad norm: 3.908 3.876 0.493
2024-12-02-10:18:45-root-INFO: grad norm: 3.876 3.841 0.515
2024-12-02-10:18:46-root-INFO: grad norm: 3.851 3.821 0.477
2024-12-02-10:18:46-root-INFO: grad norm: 3.836 3.802 0.506
2024-12-02-10:18:47-root-INFO: Loss Change: 78.060 -> 75.381
2024-12-02-10:18:47-root-INFO: Regularization Change: 0.000 -> 2.812
2024-12-02-10:18:47-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-10:18:47-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-10:18:47-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-10:18:47-root-INFO: grad norm: 5.029 4.973 0.747
2024-12-02-10:18:47-root-INFO: Loss too large (75.652->77.464)! Learning rate decreased to 0.18800.
2024-12-02-10:18:47-root-INFO: Loss too large (75.652->76.312)! Learning rate decreased to 0.15040.
2024-12-02-10:18:48-root-INFO: grad norm: 4.505 4.461 0.628
2024-12-02-10:18:48-root-INFO: grad norm: 3.886 3.852 0.516
2024-12-02-10:18:49-root-INFO: grad norm: 3.852 3.816 0.524
2024-12-02-10:18:49-root-INFO: grad norm: 3.909 3.880 0.478
2024-12-02-10:18:50-root-INFO: grad norm: 3.883 3.849 0.512
2024-12-02-10:18:50-root-INFO: grad norm: 3.864 3.836 0.466
2024-12-02-10:18:51-root-INFO: grad norm: 3.855 3.822 0.505
2024-12-02-10:18:51-root-INFO: Loss Change: 75.652 -> 72.970
2024-12-02-10:18:51-root-INFO: Regularization Change: 0.000 -> 2.806
2024-12-02-10:18:51-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-10:18:51-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-10:18:51-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-10:18:51-root-INFO: grad norm: 4.901 4.846 0.732
2024-12-02-10:18:52-root-INFO: Loss too large (73.125->74.895)! Learning rate decreased to 0.19197.
2024-12-02-10:18:52-root-INFO: Loss too large (73.125->73.772)! Learning rate decreased to 0.15357.
2024-12-02-10:18:52-root-INFO: grad norm: 4.457 4.415 0.610
2024-12-02-10:18:53-root-INFO: grad norm: 3.955 3.921 0.522
2024-12-02-10:18:53-root-INFO: grad norm: 3.928 3.893 0.525
2024-12-02-10:18:54-root-INFO: grad norm: 3.965 3.936 0.485
2024-12-02-10:18:54-root-INFO: grad norm: 3.942 3.908 0.517
2024-12-02-10:18:55-root-INFO: grad norm: 3.920 3.891 0.475
2024-12-02-10:18:55-root-INFO: grad norm: 3.914 3.881 0.511
2024-12-02-10:18:56-root-INFO: Loss Change: 73.125 -> 70.526
2024-12-02-10:18:56-root-INFO: Regularization Change: 0.000 -> 2.826
2024-12-02-10:18:56-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-10:18:56-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-10:18:56-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-10:18:56-root-INFO: grad norm: 4.997 4.944 0.723
2024-12-02-10:18:56-root-INFO: Loss too large (70.827->72.769)! Learning rate decreased to 0.19599.
2024-12-02-10:18:56-root-INFO: Loss too large (70.827->71.557)! Learning rate decreased to 0.15679.
2024-12-02-10:18:57-root-INFO: grad norm: 4.535 4.491 0.634
2024-12-02-10:18:57-root-INFO: grad norm: 4.004 3.969 0.525
2024-12-02-10:18:58-root-INFO: grad norm: 3.974 3.937 0.541
2024-12-02-10:18:58-root-INFO: grad norm: 4.004 3.973 0.493
2024-12-02-10:18:59-root-INFO: grad norm: 3.989 3.953 0.531
2024-12-02-10:18:59-root-INFO: grad norm: 3.981 3.951 0.487
2024-12-02-10:19:00-root-INFO: grad norm: 3.976 3.941 0.526
2024-12-02-10:19:00-root-INFO: Loss Change: 70.827 -> 68.273
2024-12-02-10:19:00-root-INFO: Regularization Change: 0.000 -> 2.805
2024-12-02-10:19:00-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-10:19:00-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-10:19:00-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-10:19:01-root-INFO: grad norm: 4.872 4.820 0.711
2024-12-02-10:19:01-root-INFO: Loss too large (68.478->70.397)! Learning rate decreased to 0.20006.
2024-12-02-10:19:01-root-INFO: Loss too large (68.478->69.176)! Learning rate decreased to 0.16005.
2024-12-02-10:19:01-root-INFO: grad norm: 4.478 4.433 0.628
2024-12-02-10:19:02-root-INFO: grad norm: 4.051 4.017 0.518
2024-12-02-10:19:02-root-INFO: grad norm: 3.990 3.953 0.541
2024-12-02-10:19:03-root-INFO: grad norm: 3.962 3.933 0.479
2024-12-02-10:19:03-root-INFO: grad norm: 3.928 3.894 0.520
2024-12-02-10:19:04-root-INFO: grad norm: 3.900 3.873 0.464
2024-12-02-10:19:04-root-INFO: grad norm: 3.885 3.852 0.509
2024-12-02-10:19:04-root-INFO: Loss Change: 68.478 -> 65.961
2024-12-02-10:19:04-root-INFO: Regularization Change: 0.000 -> 2.817
2024-12-02-10:19:04-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-10:19:04-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-10:19:05-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-10:19:05-root-INFO: grad norm: 5.044 4.983 0.784
2024-12-02-10:19:05-root-INFO: Loss too large (66.272->68.346)! Learning rate decreased to 0.20419.
2024-12-02-10:19:05-root-INFO: Loss too large (66.272->67.025)! Learning rate decreased to 0.16335.
2024-12-02-10:19:05-root-INFO: grad norm: 4.575 4.531 0.630
2024-12-02-10:19:06-root-INFO: grad norm: 4.089 4.054 0.533
2024-12-02-10:19:06-root-INFO: grad norm: 4.018 3.982 0.533
2024-12-02-10:19:07-root-INFO: grad norm: 3.977 3.947 0.483
2024-12-02-10:19:07-root-INFO: grad norm: 3.938 3.905 0.511
2024-12-02-10:19:08-root-INFO: grad norm: 3.902 3.874 0.465
2024-12-02-10:19:08-root-INFO: grad norm: 3.885 3.853 0.501
2024-12-02-10:19:09-root-INFO: Loss Change: 66.272 -> 63.704
2024-12-02-10:19:09-root-INFO: Regularization Change: 0.000 -> 2.845
2024-12-02-10:19:09-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-10:19:09-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-10:19:09-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-10:19:09-root-INFO: grad norm: 4.936 4.882 0.730
2024-12-02-10:19:09-root-INFO: Loss too large (63.770->65.771)! Learning rate decreased to 0.20837.
2024-12-02-10:19:09-root-INFO: Loss too large (63.770->64.494)! Learning rate decreased to 0.16669.
2024-12-02-10:19:10-root-INFO: grad norm: 4.488 4.446 0.617
2024-12-02-10:19:10-root-INFO: grad norm: 4.025 3.993 0.507
2024-12-02-10:19:11-root-INFO: grad norm: 3.955 3.920 0.525
2024-12-02-10:19:11-root-INFO: grad norm: 3.909 3.881 0.464
2024-12-02-10:19:11-root-INFO: grad norm: 3.877 3.844 0.503
2024-12-02-10:19:12-root-INFO: grad norm: 3.849 3.823 0.451
2024-12-02-10:19:12-root-INFO: grad norm: 3.835 3.803 0.494
2024-12-02-10:19:13-root-INFO: Loss Change: 63.770 -> 61.252
2024-12-02-10:19:13-root-INFO: Regularization Change: 0.000 -> 2.861
2024-12-02-10:19:13-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-10:19:13-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-10:19:13-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-10:19:13-root-INFO: grad norm: 5.006 4.947 0.764
2024-12-02-10:19:13-root-INFO: Loss too large (61.626->63.615)! Learning rate decreased to 0.21260.
2024-12-02-10:19:13-root-INFO: Loss too large (61.626->62.286)! Learning rate decreased to 0.17008.
2024-12-02-10:19:14-root-INFO: grad norm: 4.457 4.414 0.622
2024-12-02-10:19:14-root-INFO: grad norm: 3.949 3.917 0.507
2024-12-02-10:19:15-root-INFO: grad norm: 3.846 3.812 0.512
2024-12-02-10:19:15-root-INFO: grad norm: 3.766 3.738 0.457
2024-12-02-10:19:16-root-INFO: grad norm: 3.736 3.705 0.484
2024-12-02-10:19:16-root-INFO: grad norm: 3.718 3.691 0.442
2024-12-02-10:19:16-root-INFO: grad norm: 3.711 3.680 0.476
2024-12-02-10:19:17-root-INFO: Loss Change: 61.626 -> 59.039
2024-12-02-10:19:17-root-INFO: Regularization Change: 0.000 -> 2.889
2024-12-02-10:19:17-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-10:19:17-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-10:19:17-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-10:19:17-root-INFO: grad norm: 4.409 4.364 0.634
2024-12-02-10:19:17-root-INFO: Loss too large (59.174->60.843)! Learning rate decreased to 0.21688.
2024-12-02-10:19:18-root-INFO: Loss too large (59.174->59.718)! Learning rate decreased to 0.17350.
2024-12-02-10:19:18-root-INFO: grad norm: 4.133 4.095 0.562
2024-12-02-10:19:18-root-INFO: grad norm: 3.904 3.877 0.464
2024-12-02-10:19:19-root-INFO: grad norm: 3.826 3.793 0.501
2024-12-02-10:19:19-root-INFO: grad norm: 3.750 3.725 0.430
2024-12-02-10:19:20-root-INFO: grad norm: 3.717 3.687 0.478
2024-12-02-10:19:20-root-INFO: grad norm: 3.687 3.663 0.419
2024-12-02-10:19:21-root-INFO: grad norm: 3.674 3.644 0.468
2024-12-02-10:19:21-root-INFO: Loss Change: 59.174 -> 56.866
2024-12-02-10:19:21-root-INFO: Regularization Change: 0.000 -> 2.858
2024-12-02-10:19:21-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-10:19:21-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-10:19:21-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-10:19:21-root-INFO: grad norm: 4.723 4.666 0.737
2024-12-02-10:19:22-root-INFO: Loss too large (57.176->59.090)! Learning rate decreased to 0.22121.
2024-12-02-10:19:22-root-INFO: Loss too large (57.176->57.795)! Learning rate decreased to 0.17697.
2024-12-02-10:19:22-root-INFO: grad norm: 4.319 4.279 0.589
2024-12-02-10:19:23-root-INFO: grad norm: 3.981 3.951 0.492
2024-12-02-10:19:23-root-INFO: grad norm: 3.864 3.831 0.504
2024-12-02-10:19:24-root-INFO: grad norm: 3.749 3.723 0.440
2024-12-02-10:19:24-root-INFO: grad norm: 3.700 3.669 0.472
2024-12-02-10:19:25-root-INFO: grad norm: 3.653 3.629 0.420
2024-12-02-10:19:25-root-INFO: grad norm: 3.633 3.604 0.459
2024-12-02-10:19:25-root-INFO: Loss Change: 57.176 -> 54.745
2024-12-02-10:19:25-root-INFO: Regularization Change: 0.000 -> 2.884
2024-12-02-10:19:25-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-10:19:25-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-10:19:25-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-10:19:26-root-INFO: grad norm: 4.532 4.481 0.679
2024-12-02-10:19:26-root-INFO: Loss too large (54.979->56.759)! Learning rate decreased to 0.22559.
2024-12-02-10:19:26-root-INFO: Loss too large (54.979->55.521)! Learning rate decreased to 0.18047.
2024-12-02-10:19:26-root-INFO: grad norm: 4.146 4.107 0.563
2024-12-02-10:19:27-root-INFO: grad norm: 3.828 3.801 0.457
2024-12-02-10:19:27-root-INFO: grad norm: 3.696 3.664 0.481
2024-12-02-10:19:28-root-INFO: grad norm: 3.575 3.551 0.411
2024-12-02-10:19:28-root-INFO: grad norm: 3.522 3.493 0.449
2024-12-02-10:19:29-root-INFO: grad norm: 3.478 3.456 0.393
2024-12-02-10:19:29-root-INFO: grad norm: 3.462 3.434 0.437
2024-12-02-10:19:30-root-INFO: Loss Change: 54.979 -> 52.603
2024-12-02-10:19:30-root-INFO: Regularization Change: 0.000 -> 2.874
2024-12-02-10:19:30-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-10:19:30-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-10:19:30-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-10:19:30-root-INFO: grad norm: 4.386 4.335 0.672
2024-12-02-10:19:30-root-INFO: Loss too large (52.755->54.456)! Learning rate decreased to 0.23002.
2024-12-02-10:19:30-root-INFO: Loss too large (52.755->53.244)! Learning rate decreased to 0.18402.
2024-12-02-10:19:31-root-INFO: grad norm: 4.033 3.996 0.547
2024-12-02-10:19:31-root-INFO: grad norm: 3.756 3.729 0.447
2024-12-02-10:19:32-root-INFO: grad norm: 3.618 3.588 0.468
2024-12-02-10:19:32-root-INFO: grad norm: 3.494 3.471 0.400
2024-12-02-10:19:32-root-INFO: grad norm: 3.434 3.406 0.436
2024-12-02-10:19:33-root-INFO: grad norm: 3.385 3.364 0.382
2024-12-02-10:19:33-root-INFO: grad norm: 3.365 3.339 0.423
2024-12-02-10:19:34-root-INFO: Loss Change: 52.755 -> 50.425
2024-12-02-10:19:34-root-INFO: Regularization Change: 0.000 -> 2.889
2024-12-02-10:19:34-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-10:19:34-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-10:19:34-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-10:19:34-root-INFO: grad norm: 4.244 4.195 0.647
2024-12-02-10:19:34-root-INFO: Loss too large (50.702->52.346)! Learning rate decreased to 0.23450.
2024-12-02-10:19:34-root-INFO: Loss too large (50.702->51.161)! Learning rate decreased to 0.18760.
2024-12-02-10:19:35-root-INFO: grad norm: 3.935 3.900 0.521
2024-12-02-10:19:35-root-INFO: grad norm: 3.697 3.672 0.433
2024-12-02-10:19:36-root-INFO: grad norm: 3.561 3.532 0.453
2024-12-02-10:19:36-root-INFO: grad norm: 3.438 3.416 0.389
2024-12-02-10:19:37-root-INFO: grad norm: 3.373 3.347 0.423
2024-12-02-10:19:37-root-INFO: grad norm: 3.321 3.300 0.372
2024-12-02-10:19:37-root-INFO: grad norm: 3.298 3.272 0.411
2024-12-02-10:19:38-root-INFO: Loss Change: 50.702 -> 48.435
2024-12-02-10:19:38-root-INFO: Regularization Change: 0.000 -> 2.893
2024-12-02-10:19:38-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-10:19:38-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-10:19:38-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-10:19:38-root-INFO: grad norm: 4.153 4.106 0.624
2024-12-02-10:19:38-root-INFO: Loss too large (48.493->50.080)! Learning rate decreased to 0.23903.
2024-12-02-10:19:38-root-INFO: Loss too large (48.493->48.895)! Learning rate decreased to 0.19122.
2024-12-02-10:19:39-root-INFO: grad norm: 3.818 3.783 0.513
2024-12-02-10:19:39-root-INFO: grad norm: 3.574 3.550 0.415
2024-12-02-10:19:40-root-INFO: grad norm: 3.427 3.399 0.438
2024-12-02-10:19:40-root-INFO: grad norm: 3.306 3.285 0.373
2024-12-02-10:19:41-root-INFO: grad norm: 3.238 3.212 0.408
2024-12-02-10:19:41-root-INFO: grad norm: 3.187 3.167 0.357
2024-12-02-10:19:42-root-INFO: grad norm: 3.165 3.140 0.395
2024-12-02-10:19:42-root-INFO: Loss Change: 48.493 -> 46.228
2024-12-02-10:19:42-root-INFO: Regularization Change: 0.000 -> 2.913
2024-12-02-10:19:42-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-10:19:42-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-10:19:42-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-10:19:42-root-INFO: grad norm: 3.782 3.743 0.548
2024-12-02-10:19:42-root-INFO: Loss too large (46.300->47.680)! Learning rate decreased to 0.24361.
2024-12-02-10:19:43-root-INFO: Loss too large (46.300->46.635)! Learning rate decreased to 0.19489.
2024-12-02-10:19:43-root-INFO: grad norm: 3.573 3.542 0.469
2024-12-02-10:19:44-root-INFO: grad norm: 3.436 3.414 0.390
2024-12-02-10:19:44-root-INFO: grad norm: 3.337 3.310 0.423
2024-12-02-10:19:45-root-INFO: grad norm: 3.253 3.233 0.360
2024-12-02-10:19:45-root-INFO: grad norm: 3.197 3.172 0.402
2024-12-02-10:19:45-root-INFO: grad norm: 3.154 3.134 0.348
2024-12-02-10:19:46-root-INFO: grad norm: 3.128 3.103 0.392
2024-12-02-10:19:46-root-INFO: Loss Change: 46.300 -> 44.192
2024-12-02-10:19:46-root-INFO: Regularization Change: 0.000 -> 2.922
2024-12-02-10:19:46-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-10:19:46-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-10:19:46-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-10:19:46-root-INFO: grad norm: 4.297 4.238 0.712
2024-12-02-10:19:47-root-INFO: Loss too large (44.496->46.212)! Learning rate decreased to 0.24866.
2024-12-02-10:19:47-root-INFO: Loss too large (44.496->44.868)! Learning rate decreased to 0.19893.
2024-12-02-10:19:47-root-INFO: grad norm: 3.822 3.786 0.524
2024-12-02-10:19:48-root-INFO: grad norm: 3.499 3.475 0.406
2024-12-02-10:19:48-root-INFO: grad norm: 3.282 3.255 0.421
2024-12-02-10:19:49-root-INFO: grad norm: 3.107 3.087 0.348
2024-12-02-10:19:49-root-INFO: grad norm: 2.992 2.968 0.376
2024-12-02-10:19:50-root-INFO: grad norm: 2.906 2.888 0.323
2024-12-02-10:19:50-root-INFO: grad norm: 2.855 2.833 0.356
2024-12-02-10:19:50-root-INFO: Loss Change: 44.496 -> 42.064
2024-12-02-10:19:50-root-INFO: Regularization Change: 0.000 -> 3.005
2024-12-02-10:19:50-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-10:19:50-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-10:19:51-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-10:19:51-root-INFO: grad norm: 3.819 3.772 0.599
2024-12-02-10:19:51-root-INFO: Loss too large (42.281->43.677)! Learning rate decreased to 0.25333.
2024-12-02-10:19:51-root-INFO: Loss too large (42.281->42.558)! Learning rate decreased to 0.20266.
2024-12-02-10:19:51-root-INFO: grad norm: 3.444 3.413 0.464
2024-12-02-10:19:52-root-INFO: grad norm: 3.192 3.171 0.365
2024-12-02-10:19:52-root-INFO: grad norm: 3.012 2.988 0.382
2024-12-02-10:19:53-root-INFO: grad norm: 2.872 2.854 0.319
2024-12-02-10:19:53-root-INFO: grad norm: 2.776 2.754 0.347
2024-12-02-10:19:54-root-INFO: grad norm: 2.707 2.690 0.299
2024-12-02-10:19:54-root-INFO: grad norm: 2.665 2.645 0.331
2024-12-02-10:19:55-root-INFO: Loss Change: 42.281 -> 40.080
2024-12-02-10:19:55-root-INFO: Regularization Change: 0.000 -> 2.911
2024-12-02-10:19:55-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-10:19:55-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-10:19:55-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-10:19:55-root-INFO: grad norm: 3.360 3.319 0.520
2024-12-02-10:19:55-root-INFO: Loss too large (40.124->41.203)! Learning rate decreased to 0.25805.
2024-12-02-10:19:55-root-INFO: Loss too large (40.124->40.306)! Learning rate decreased to 0.20644.
2024-12-02-10:19:56-root-INFO: grad norm: 3.096 3.068 0.416
2024-12-02-10:19:56-root-INFO: grad norm: 2.936 2.917 0.334
2024-12-02-10:19:57-root-INFO: grad norm: 2.813 2.790 0.355
2024-12-02-10:19:57-root-INFO: grad norm: 2.716 2.699 0.300
2024-12-02-10:19:58-root-INFO: grad norm: 2.644 2.624 0.331
2024-12-02-10:19:58-root-INFO: grad norm: 2.593 2.578 0.286
2024-12-02-10:19:59-root-INFO: grad norm: 2.561 2.541 0.319
2024-12-02-10:19:59-root-INFO: Loss Change: 40.124 -> 38.120
2024-12-02-10:19:59-root-INFO: Regularization Change: 0.000 -> 2.875
2024-12-02-10:19:59-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-10:19:59-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-10:19:59-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-10:19:59-root-INFO: grad norm: 3.410 3.363 0.561
2024-12-02-10:20:00-root-INFO: Loss too large (38.197->39.308)! Learning rate decreased to 0.26281.
2024-12-02-10:20:00-root-INFO: Loss too large (38.197->38.377)! Learning rate decreased to 0.21025.
2024-12-02-10:20:00-root-INFO: grad norm: 3.083 3.055 0.417
2024-12-02-10:20:01-root-INFO: grad norm: 2.870 2.851 0.335
2024-12-02-10:20:01-root-INFO: grad norm: 2.710 2.689 0.341
2024-12-02-10:20:02-root-INFO: grad norm: 2.586 2.569 0.293
2024-12-02-10:20:02-root-INFO: grad norm: 2.494 2.475 0.309
2024-12-02-10:20:03-root-INFO: grad norm: 2.429 2.413 0.274
2024-12-02-10:20:03-root-INFO: grad norm: 2.385 2.367 0.294
2024-12-02-10:20:04-root-INFO: Loss Change: 38.197 -> 36.148
2024-12-02-10:20:04-root-INFO: Regularization Change: 0.000 -> 2.888
2024-12-02-10:20:04-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-10:20:04-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-10:20:04-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-10:20:04-root-INFO: grad norm: 3.325 3.280 0.545
2024-12-02-10:20:04-root-INFO: Loss too large (36.267->37.302)! Learning rate decreased to 0.26762.
2024-12-02-10:20:04-root-INFO: Loss too large (36.267->36.403)! Learning rate decreased to 0.21410.
2024-12-02-10:20:05-root-INFO: grad norm: 2.943 2.915 0.406
2024-12-02-10:20:05-root-INFO: grad norm: 2.698 2.680 0.312
2024-12-02-10:20:06-root-INFO: grad norm: 2.517 2.497 0.319
2024-12-02-10:20:06-root-INFO: grad norm: 2.383 2.368 0.268
2024-12-02-10:20:07-root-INFO: grad norm: 2.284 2.267 0.285
2024-12-02-10:20:07-root-INFO: grad norm: 2.215 2.201 0.249
2024-12-02-10:20:08-root-INFO: grad norm: 2.169 2.152 0.269
2024-12-02-10:20:08-root-INFO: Loss Change: 36.267 -> 34.219
2024-12-02-10:20:08-root-INFO: Regularization Change: 0.000 -> 2.882
2024-12-02-10:20:08-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-10:20:08-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-10:20:08-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-10:20:08-root-INFO: grad norm: 2.864 2.825 0.473
2024-12-02-10:20:09-root-INFO: Loss too large (34.237->34.982)! Learning rate decreased to 0.27247.
2024-12-02-10:20:09-root-INFO: Loss too large (34.237->34.306)! Learning rate decreased to 0.21798.
2024-12-02-10:20:09-root-INFO: grad norm: 2.619 2.596 0.347
2024-12-02-10:20:10-root-INFO: grad norm: 2.478 2.460 0.293
2024-12-02-10:20:10-root-INFO: grad norm: 2.371 2.352 0.294
2024-12-02-10:20:11-root-INFO: grad norm: 2.285 2.270 0.259
2024-12-02-10:20:11-root-INFO: grad norm: 2.219 2.203 0.273
2024-12-02-10:20:12-root-INFO: grad norm: 2.170 2.157 0.245
2024-12-02-10:20:12-root-INFO: grad norm: 2.136 2.120 0.262
2024-12-02-10:20:12-root-INFO: Loss Change: 34.237 -> 32.394
2024-12-02-10:20:12-root-INFO: Regularization Change: 0.000 -> 2.846
2024-12-02-10:20:12-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-10:20:12-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-10:20:13-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-10:20:13-root-INFO: grad norm: 2.954 2.914 0.482
2024-12-02-10:20:13-root-INFO: Loss too large (32.486->33.281)! Learning rate decreased to 0.27737.
2024-12-02-10:20:13-root-INFO: Loss too large (32.486->32.557)! Learning rate decreased to 0.22190.
2024-12-02-10:20:13-root-INFO: grad norm: 2.623 2.597 0.367
2024-12-02-10:20:14-root-INFO: grad norm: 2.401 2.385 0.281
2024-12-02-10:20:14-root-INFO: grad norm: 2.238 2.219 0.285
2024-12-02-10:20:15-root-INFO: grad norm: 2.117 2.103 0.240
2024-12-02-10:20:15-root-INFO: grad norm: 2.026 2.010 0.252
2024-12-02-10:20:16-root-INFO: grad norm: 1.961 1.948 0.222
2024-12-02-10:20:16-root-INFO: grad norm: 1.914 1.899 0.235
2024-12-02-10:20:17-root-INFO: Loss Change: 32.486 -> 30.594
2024-12-02-10:20:17-root-INFO: Regularization Change: 0.000 -> 2.831
2024-12-02-10:20:17-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-10:20:17-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-10:20:17-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-10:20:17-root-INFO: grad norm: 2.618 2.582 0.432
2024-12-02-10:20:17-root-INFO: Loss too large (30.594->31.153)! Learning rate decreased to 0.28231.
2024-12-02-10:20:18-root-INFO: grad norm: 3.303 3.268 0.479
2024-12-02-10:20:18-root-INFO: Loss too large (30.589->30.770)! Learning rate decreased to 0.22585.
2024-12-02-10:20:18-root-INFO: grad norm: 2.827 2.808 0.333
2024-12-02-10:20:19-root-INFO: grad norm: 2.450 2.429 0.316
2024-12-02-10:20:19-root-INFO: grad norm: 2.191 2.177 0.251
2024-12-02-10:20:20-root-INFO: grad norm: 1.999 1.983 0.252
2024-12-02-10:20:20-root-INFO: grad norm: 1.869 1.857 0.215
2024-12-02-10:20:21-root-INFO: grad norm: 1.774 1.760 0.221
2024-12-02-10:20:21-root-INFO: Loss Change: 30.594 -> 28.787
2024-12-02-10:20:21-root-INFO: Regularization Change: 0.000 -> 2.941
2024-12-02-10:20:21-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-10:20:21-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-10:20:21-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-10:20:21-root-INFO: grad norm: 2.513 2.474 0.444
2024-12-02-10:20:22-root-INFO: Loss too large (28.664->29.124)! Learning rate decreased to 0.28730.
2024-12-02-10:20:22-root-INFO: grad norm: 3.073 3.039 0.451
2024-12-02-10:20:22-root-INFO: Loss too large (28.617->28.717)! Learning rate decreased to 0.22984.
2024-12-02-10:20:23-root-INFO: grad norm: 2.589 2.569 0.314
2024-12-02-10:20:23-root-INFO: grad norm: 2.235 2.216 0.284
2024-12-02-10:20:24-root-INFO: grad norm: 1.985 1.972 0.231
2024-12-02-10:20:24-root-INFO: grad norm: 1.804 1.790 0.224
2024-12-02-10:20:25-root-INFO: grad norm: 1.679 1.668 0.196
2024-12-02-10:20:25-root-INFO: grad norm: 1.589 1.577 0.197
2024-12-02-10:20:26-root-INFO: Loss Change: 28.664 -> 26.901
2024-12-02-10:20:26-root-INFO: Regularization Change: 0.000 -> 2.891
2024-12-02-10:20:26-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-10:20:26-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-10:20:26-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-10:20:26-root-INFO: grad norm: 2.474 2.434 0.440
2024-12-02-10:20:26-root-INFO: Loss too large (26.981->27.370)! Learning rate decreased to 0.29232.
2024-12-02-10:20:27-root-INFO: grad norm: 2.901 2.866 0.444
2024-12-02-10:20:27-root-INFO: Loss too large (26.893->26.920)! Learning rate decreased to 0.23386.
2024-12-02-10:20:27-root-INFO: grad norm: 2.363 2.346 0.286
2024-12-02-10:20:28-root-INFO: grad norm: 2.002 1.985 0.260
2024-12-02-10:20:28-root-INFO: grad norm: 1.764 1.752 0.206
2024-12-02-10:20:29-root-INFO: grad norm: 1.596 1.583 0.202
2024-12-02-10:20:29-root-INFO: grad norm: 1.481 1.470 0.174
2024-12-02-10:20:30-root-INFO: grad norm: 1.397 1.386 0.176
2024-12-02-10:20:30-root-INFO: Loss Change: 26.981 -> 25.243
2024-12-02-10:20:30-root-INFO: Regularization Change: 0.000 -> 2.837
2024-12-02-10:20:30-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-10:20:30-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-10:20:30-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-10:20:30-root-INFO: grad norm: 2.051 2.017 0.369
2024-12-02-10:20:30-root-INFO: Loss too large (25.200->25.380)! Learning rate decreased to 0.29738.
2024-12-02-10:20:31-root-INFO: grad norm: 2.412 2.385 0.363
2024-12-02-10:20:31-root-INFO: grad norm: 2.993 2.970 0.375
2024-12-02-10:20:32-root-INFO: Loss too large (25.061->25.153)! Learning rate decreased to 0.23791.
2024-12-02-10:20:32-root-INFO: grad norm: 2.413 2.391 0.322
2024-12-02-10:20:33-root-INFO: grad norm: 1.941 1.928 0.230
2024-12-02-10:20:33-root-INFO: grad norm: 1.652 1.639 0.211
2024-12-02-10:20:34-root-INFO: grad norm: 1.478 1.467 0.175
2024-12-02-10:20:34-root-INFO: grad norm: 1.359 1.348 0.172
2024-12-02-10:20:34-root-INFO: Loss Change: 25.200 -> 23.607
2024-12-02-10:20:34-root-INFO: Regularization Change: 0.000 -> 2.901
2024-12-02-10:20:34-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-10:20:34-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-10:20:35-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-10:20:35-root-INFO: grad norm: 2.088 2.054 0.376
2024-12-02-10:20:35-root-INFO: Loss too large (23.566->23.747)! Learning rate decreased to 0.30249.
2024-12-02-10:20:35-root-INFO: grad norm: 2.349 2.321 0.362
2024-12-02-10:20:36-root-INFO: grad norm: 2.775 2.753 0.350
2024-12-02-10:20:36-root-INFO: Loss too large (23.362->23.379)! Learning rate decreased to 0.24199.
2024-12-02-10:20:37-root-INFO: grad norm: 2.162 2.142 0.291
2024-12-02-10:20:37-root-INFO: grad norm: 1.704 1.692 0.201
2024-12-02-10:20:37-root-INFO: grad norm: 1.436 1.424 0.185
2024-12-02-10:20:38-root-INFO: grad norm: 1.276 1.267 0.151
2024-12-02-10:20:38-root-INFO: grad norm: 1.170 1.160 0.151
2024-12-02-10:20:39-root-INFO: Loss Change: 23.566 -> 21.987
2024-12-02-10:20:39-root-INFO: Regularization Change: 0.000 -> 2.835
2024-12-02-10:20:39-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-10:20:39-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-10:20:39-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-10:20:39-root-INFO: grad norm: 1.909 1.873 0.365
2024-12-02-10:20:39-root-INFO: Loss too large (21.978->22.032)! Learning rate decreased to 0.30763.
2024-12-02-10:20:40-root-INFO: grad norm: 2.036 2.010 0.325
2024-12-02-10:20:40-root-INFO: grad norm: 2.397 2.377 0.312
2024-12-02-10:20:41-root-INFO: grad norm: 2.686 2.661 0.372
2024-12-02-10:20:41-root-INFO: grad norm: 2.842 2.821 0.346
2024-12-02-10:20:42-root-INFO: grad norm: 2.939 2.911 0.407
2024-12-02-10:20:42-root-INFO: grad norm: 3.011 2.988 0.373
2024-12-02-10:20:43-root-INFO: grad norm: 3.019 2.989 0.421
2024-12-02-10:20:43-root-INFO: Loss Change: 21.978 -> 20.914
2024-12-02-10:20:43-root-INFO: Regularization Change: 0.000 -> 3.869
2024-12-02-10:20:43-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-10:20:43-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-10:20:43-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-10:20:43-root-INFO: grad norm: 3.811 3.759 0.630
2024-12-02-10:20:43-root-INFO: Loss too large (21.074->21.830)! Learning rate decreased to 0.31281.
2024-12-02-10:20:44-root-INFO: grad norm: 3.422 3.378 0.548
2024-12-02-10:20:44-root-INFO: grad norm: 3.050 3.022 0.419
2024-12-02-10:20:45-root-INFO: grad norm: 2.816 2.786 0.410
2024-12-02-10:20:45-root-INFO: grad norm: 2.684 2.662 0.344
2024-12-02-10:20:46-root-INFO: grad norm: 2.559 2.533 0.359
2024-12-02-10:20:46-root-INFO: grad norm: 2.447 2.428 0.308
2024-12-02-10:20:47-root-INFO: grad norm: 2.368 2.345 0.328
2024-12-02-10:20:47-root-INFO: Loss Change: 21.074 -> 18.880
2024-12-02-10:20:47-root-INFO: Regularization Change: 0.000 -> 3.703
2024-12-02-10:20:47-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-10:20:47-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-10:20:47-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-10:20:47-root-INFO: grad norm: 3.028 2.985 0.509
2024-12-02-10:20:48-root-INFO: Loss too large (19.028->19.434)! Learning rate decreased to 0.31802.
2024-12-02-10:20:48-root-INFO: grad norm: 2.708 2.673 0.435
2024-12-02-10:20:49-root-INFO: grad norm: 2.412 2.389 0.336
2024-12-02-10:20:49-root-INFO: grad norm: 2.237 2.213 0.329
2024-12-02-10:20:50-root-INFO: grad norm: 2.148 2.130 0.277
2024-12-02-10:20:50-root-INFO: grad norm: 2.060 2.039 0.291
2024-12-02-10:20:51-root-INFO: grad norm: 1.982 1.966 0.251
2024-12-02-10:20:51-root-INFO: grad norm: 1.927 1.909 0.269
2024-12-02-10:20:51-root-INFO: Loss Change: 19.028 -> 17.217
2024-12-02-10:20:51-root-INFO: Regularization Change: 0.000 -> 3.479
2024-12-02-10:20:51-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-10:20:51-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-10:20:52-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-10:20:52-root-INFO: grad norm: 2.739 2.696 0.481
2024-12-02-10:20:52-root-INFO: Loss too large (17.348->17.623)! Learning rate decreased to 0.32327.
2024-12-02-10:20:52-root-INFO: grad norm: 2.389 2.356 0.394
2024-12-02-10:20:53-root-INFO: grad norm: 2.073 2.052 0.293
2024-12-02-10:20:53-root-INFO: grad norm: 1.906 1.885 0.281
2024-12-02-10:20:54-root-INFO: grad norm: 1.820 1.805 0.232
2024-12-02-10:20:54-root-INFO: grad norm: 1.741 1.724 0.244
2024-12-02-10:20:55-root-INFO: grad norm: 1.677 1.664 0.208
2024-12-02-10:20:55-root-INFO: grad norm: 1.629 1.614 0.224
2024-12-02-10:20:56-root-INFO: Loss Change: 17.348 -> 15.665
2024-12-02-10:20:56-root-INFO: Regularization Change: 0.000 -> 3.341
2024-12-02-10:20:56-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-10:20:56-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-10:20:56-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-10:20:56-root-INFO: grad norm: 2.374 2.335 0.431
2024-12-02-10:20:56-root-INFO: Loss too large (15.643->15.776)! Learning rate decreased to 0.32856.
2024-12-02-10:20:57-root-INFO: grad norm: 2.035 2.008 0.332
2024-12-02-10:20:57-root-INFO: grad norm: 1.749 1.731 0.247
2024-12-02-10:20:58-root-INFO: grad norm: 1.607 1.590 0.235
2024-12-02-10:20:58-root-INFO: grad norm: 1.533 1.521 0.194
2024-12-02-10:20:59-root-INFO: grad norm: 1.469 1.455 0.204
2024-12-02-10:20:59-root-INFO: grad norm: 1.417 1.407 0.172
2024-12-02-10:21:00-root-INFO: grad norm: 1.380 1.367 0.187
2024-12-02-10:21:00-root-INFO: Loss Change: 15.643 -> 14.121
2024-12-02-10:21:00-root-INFO: Regularization Change: 0.000 -> 3.191
2024-12-02-10:21:00-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-10:21:00-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-10:21:00-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-10:21:00-root-INFO: grad norm: 2.206 2.170 0.398
2024-12-02-10:21:00-root-INFO: Loss too large (14.243->14.334)! Learning rate decreased to 0.33388.
2024-12-02-10:21:01-root-INFO: grad norm: 1.863 1.837 0.307
2024-12-02-10:21:01-root-INFO: grad norm: 1.574 1.559 0.220
2024-12-02-10:21:02-root-INFO: grad norm: 1.446 1.430 0.213
2024-12-02-10:21:02-root-INFO: grad norm: 1.381 1.371 0.171
2024-12-02-10:21:03-root-INFO: grad norm: 1.327 1.314 0.185
2024-12-02-10:21:03-root-INFO: grad norm: 1.287 1.278 0.153
2024-12-02-10:21:04-root-INFO: grad norm: 1.257 1.245 0.172
2024-12-02-10:21:04-root-INFO: Loss Change: 14.243 -> 12.839
2024-12-02-10:21:04-root-INFO: Regularization Change: 0.000 -> 3.025
2024-12-02-10:21:04-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-10:21:04-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-10:21:04-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-10:21:04-root-INFO: grad norm: 2.035 2.000 0.378
2024-12-02-10:21:04-root-INFO: Loss too large (12.812->12.859)! Learning rate decreased to 0.33923.
2024-12-02-10:21:05-root-INFO: grad norm: 1.701 1.677 0.286
2024-12-02-10:21:05-root-INFO: grad norm: 1.455 1.440 0.202
2024-12-02-10:21:06-root-INFO: grad norm: 1.350 1.335 0.200
2024-12-02-10:21:06-root-INFO: grad norm: 1.295 1.285 0.159
2024-12-02-10:21:07-root-INFO: grad norm: 1.253 1.241 0.175
2024-12-02-10:21:07-root-INFO: grad norm: 1.225 1.217 0.145
2024-12-02-10:21:08-root-INFO: grad norm: 1.206 1.194 0.166
2024-12-02-10:21:08-root-INFO: Loss Change: 12.812 -> 11.530
2024-12-02-10:21:08-root-INFO: Regularization Change: 0.000 -> 2.860
2024-12-02-10:21:08-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-10:21:08-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-10:21:08-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-10:21:08-root-INFO: grad norm: 2.046 2.013 0.366
2024-12-02-10:21:09-root-INFO: Loss too large (11.609->11.712)! Learning rate decreased to 0.34461.
2024-12-02-10:21:09-root-INFO: grad norm: 1.722 1.699 0.282
2024-12-02-10:21:09-root-INFO: grad norm: 1.450 1.437 0.197
2024-12-02-10:21:10-root-INFO: grad norm: 1.349 1.334 0.200
2024-12-02-10:21:10-root-INFO: grad norm: 1.289 1.279 0.156
2024-12-02-10:21:11-root-INFO: grad norm: 1.251 1.238 0.177
2024-12-02-10:21:11-root-INFO: grad norm: 1.225 1.217 0.143
2024-12-02-10:21:12-root-INFO: grad norm: 1.207 1.195 0.168
2024-12-02-10:21:12-root-INFO: Loss Change: 11.609 -> 10.412
2024-12-02-10:21:12-root-INFO: Regularization Change: 0.000 -> 2.680
2024-12-02-10:21:12-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-10:21:12-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-10:21:12-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-10:21:12-root-INFO: grad norm: 1.956 1.926 0.346
2024-12-02-10:21:13-root-INFO: Loss too large (10.438->10.520)! Learning rate decreased to 0.35002.
2024-12-02-10:21:13-root-INFO: grad norm: 1.633 1.611 0.268
2024-12-02-10:21:14-root-INFO: grad norm: 1.381 1.370 0.180
2024-12-02-10:21:14-root-INFO: grad norm: 1.289 1.275 0.189
2024-12-02-10:21:15-root-INFO: grad norm: 1.227 1.219 0.143
2024-12-02-10:21:15-root-INFO: grad norm: 1.193 1.182 0.166
2024-12-02-10:21:15-root-INFO: grad norm: 1.170 1.162 0.131
2024-12-02-10:21:16-root-INFO: grad norm: 1.156 1.145 0.158
2024-12-02-10:21:16-root-INFO: Loss Change: 10.438 -> 9.327
2024-12-02-10:21:16-root-INFO: Regularization Change: 0.000 -> 2.521
2024-12-02-10:21:16-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-10:21:16-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-10:21:16-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-10:21:17-root-INFO: grad norm: 1.996 1.963 0.360
2024-12-02-10:21:17-root-INFO: Loss too large (9.408->9.504)! Learning rate decreased to 0.35546.
2024-12-02-10:21:17-root-INFO: grad norm: 1.634 1.613 0.263
2024-12-02-10:21:18-root-INFO: grad norm: 1.344 1.332 0.176
2024-12-02-10:21:18-root-INFO: grad norm: 1.243 1.230 0.179
2024-12-02-10:21:19-root-INFO: grad norm: 1.169 1.161 0.134
2024-12-02-10:21:19-root-INFO: grad norm: 1.133 1.123 0.155
2024-12-02-10:21:19-root-INFO: grad norm: 1.108 1.102 0.122
2024-12-02-10:21:20-root-INFO: grad norm: 1.095 1.085 0.147
2024-12-02-10:21:20-root-INFO: Loss Change: 9.408 -> 8.340
2024-12-02-10:21:20-root-INFO: Regularization Change: 0.000 -> 2.364
2024-12-02-10:21:20-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-10:21:20-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-10:21:20-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-10:21:21-root-INFO: grad norm: 1.816 1.788 0.316
2024-12-02-10:21:21-root-INFO: Loss too large (8.371->8.423)! Learning rate decreased to 0.36092.
2024-12-02-10:21:21-root-INFO: grad norm: 1.486 1.467 0.235
2024-12-02-10:21:22-root-INFO: grad norm: 1.250 1.241 0.153
2024-12-02-10:21:22-root-INFO: grad norm: 1.156 1.144 0.163
2024-12-02-10:21:23-root-INFO: grad norm: 1.077 1.071 0.117
2024-12-02-10:21:23-root-INFO: grad norm: 1.040 1.031 0.140
2024-12-02-10:21:24-root-INFO: grad norm: 1.012 1.006 0.106
2024-12-02-10:21:24-root-INFO: grad norm: 0.999 0.990 0.132
2024-12-02-10:21:24-root-INFO: Loss Change: 8.371 -> 7.404
2024-12-02-10:21:24-root-INFO: Regularization Change: 0.000 -> 2.205
2024-12-02-10:21:24-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-10:21:24-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-10:21:25-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-10:21:25-root-INFO: grad norm: 1.871 1.840 0.338
2024-12-02-10:21:25-root-INFO: Loss too large (7.511->7.556)! Learning rate decreased to 0.36641.
2024-12-02-10:21:25-root-INFO: grad norm: 1.493 1.474 0.235
2024-12-02-10:21:26-root-INFO: grad norm: 1.232 1.223 0.150
2024-12-02-10:21:26-root-INFO: grad norm: 1.121 1.111 0.155
2024-12-02-10:21:27-root-INFO: grad norm: 1.026 1.020 0.110
2024-12-02-10:21:27-root-INFO: grad norm: 0.979 0.970 0.129
2024-12-02-10:21:28-root-INFO: grad norm: 0.939 0.935 0.096
2024-12-02-10:21:28-root-INFO: grad norm: 0.920 0.912 0.119
2024-12-02-10:21:29-root-INFO: Loss Change: 7.511 -> 6.561
2024-12-02-10:21:29-root-INFO: Regularization Change: 0.000 -> 2.083
2024-12-02-10:21:29-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-10:21:29-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-10:21:29-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-10:21:29-root-INFO: grad norm: 1.678 1.651 0.300
2024-12-02-10:21:29-root-INFO: Loss too large (6.641->6.648)! Learning rate decreased to 0.37193.
2024-12-02-10:21:29-root-INFO: grad norm: 1.327 1.311 0.206
2024-12-02-10:21:30-root-INFO: grad norm: 1.115 1.107 0.129
2024-12-02-10:21:30-root-INFO: grad norm: 1.011 1.001 0.137
2024-12-02-10:21:31-root-INFO: grad norm: 0.920 0.916 0.093
2024-12-02-10:21:31-root-INFO: grad norm: 0.872 0.865 0.112
2024-12-02-10:21:32-root-INFO: grad norm: 0.830 0.827 0.079
2024-12-02-10:21:32-root-INFO: grad norm: 0.808 0.801 0.102
2024-12-02-10:21:33-root-INFO: Loss Change: 6.641 -> 5.806
2024-12-02-10:21:33-root-INFO: Regularization Change: 0.000 -> 1.879
2024-12-02-10:21:33-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-10:21:33-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-10:21:33-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-10:21:33-root-INFO: grad norm: 1.604 1.577 0.290
2024-12-02-10:21:33-root-INFO: grad norm: 1.723 1.698 0.294
2024-12-02-10:21:34-root-INFO: grad norm: 2.043 2.025 0.271
2024-12-02-10:21:34-root-INFO: Loss too large (5.848->5.999)! Learning rate decreased to 0.37747.
2024-12-02-10:21:34-root-INFO: grad norm: 1.543 1.525 0.233
2024-12-02-10:21:35-root-INFO: grad norm: 1.064 1.057 0.124
2024-12-02-10:21:35-root-INFO: grad norm: 0.883 0.875 0.126
2024-12-02-10:21:36-root-INFO: grad norm: 0.770 0.766 0.075
2024-12-02-10:21:36-root-INFO: grad norm: 0.708 0.702 0.093
2024-12-02-10:21:37-root-INFO: Loss Change: 5.908 -> 5.109
2024-12-02-10:21:37-root-INFO: Regularization Change: 0.000 -> 1.908
2024-12-02-10:21:37-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-10:21:37-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-10:21:37-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-10:21:37-root-INFO: grad norm: 1.576 1.545 0.308
2024-12-02-10:21:37-root-INFO: grad norm: 1.609 1.585 0.279
2024-12-02-10:21:38-root-INFO: grad norm: 1.843 1.826 0.256
2024-12-02-10:21:38-root-INFO: Loss too large (5.121->5.221)! Learning rate decreased to 0.38303.
2024-12-02-10:21:39-root-INFO: grad norm: 1.375 1.360 0.206
2024-12-02-10:21:39-root-INFO: grad norm: 0.959 0.953 0.112
2024-12-02-10:21:40-root-INFO: grad norm: 0.793 0.786 0.108
2024-12-02-10:21:40-root-INFO: grad norm: 0.700 0.697 0.068
2024-12-02-10:21:41-root-INFO: grad norm: 0.652 0.647 0.081
2024-12-02-10:21:41-root-INFO: Loss Change: 5.232 -> 4.482
2024-12-02-10:21:41-root-INFO: Regularization Change: 0.000 -> 1.743
2024-12-02-10:21:41-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-10:21:41-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-10:21:41-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-10:21:41-root-INFO: grad norm: 1.508 1.481 0.288
2024-12-02-10:21:42-root-INFO: grad norm: 1.489 1.468 0.251
2024-12-02-10:21:42-root-INFO: grad norm: 1.598 1.582 0.220
2024-12-02-10:21:42-root-INFO: Loss too large (4.478->4.512)! Learning rate decreased to 0.38861.
2024-12-02-10:21:43-root-INFO: grad norm: 1.179 1.166 0.175
2024-12-02-10:21:43-root-INFO: grad norm: 0.948 0.942 0.106
2024-12-02-10:21:44-root-INFO: grad norm: 0.790 0.784 0.103
2024-12-02-10:21:44-root-INFO: grad norm: 0.626 0.623 0.058
2024-12-02-10:21:45-root-INFO: grad norm: 0.591 0.587 0.071
2024-12-02-10:21:45-root-INFO: Loss Change: 4.612 -> 3.949
2024-12-02-10:21:45-root-INFO: Regularization Change: 0.000 -> 1.564
2024-12-02-10:21:45-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-10:21:45-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-10:21:45-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-10:21:45-root-INFO: grad norm: 1.619 1.592 0.294
2024-12-02-10:21:46-root-INFO: grad norm: 1.434 1.414 0.242
2024-12-02-10:21:46-root-INFO: grad norm: 1.443 1.430 0.196
2024-12-02-10:21:47-root-INFO: grad norm: 1.442 1.424 0.228
2024-12-02-10:21:47-root-INFO: grad norm: 1.577 1.562 0.214
2024-12-02-10:21:47-root-INFO: Loss too large (3.857->3.922)! Learning rate decreased to 0.39420.
2024-12-02-10:21:48-root-INFO: grad norm: 1.077 1.065 0.161
2024-12-02-10:21:48-root-INFO: grad norm: 0.732 0.728 0.075
2024-12-02-10:21:49-root-INFO: grad norm: 0.543 0.539 0.067
2024-12-02-10:21:49-root-INFO: Loss Change: 4.118 -> 3.462
2024-12-02-10:21:49-root-INFO: Regularization Change: 0.000 -> 1.544
2024-12-02-10:21:49-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-10:21:49-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-10:21:49-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-10:21:50-root-INFO: grad norm: 1.266 1.242 0.245
2024-12-02-10:21:50-root-INFO: grad norm: 1.118 1.101 0.197
2024-12-02-10:21:51-root-INFO: grad norm: 1.150 1.141 0.148
2024-12-02-10:21:51-root-INFO: Loss too large (3.411->3.445)! Learning rate decreased to 0.39982.
2024-12-02-10:21:51-root-INFO: grad norm: 1.133 1.129 0.099
2024-12-02-10:21:52-root-INFO: grad norm: 0.651 0.649 0.053
2024-12-02-10:21:52-root-INFO: grad norm: 0.553 0.550 0.057
2024-12-02-10:21:53-root-INFO: grad norm: 0.399 0.398 0.031
2024-12-02-10:21:53-root-INFO: grad norm: 0.380 0.378 0.040
2024-12-02-10:21:54-root-INFO: Loss Change: 3.599 -> 3.071
2024-12-02-10:21:54-root-INFO: Regularization Change: 0.000 -> 1.246
2024-12-02-10:21:54-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-10:21:54-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-10:21:54-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-10:21:54-root-INFO: grad norm: 1.411 1.385 0.270
2024-12-02-10:21:54-root-INFO: grad norm: 1.134 1.118 0.193
2024-12-02-10:21:55-root-INFO: grad norm: 1.080 1.069 0.151
2024-12-02-10:21:55-root-INFO: grad norm: 0.997 0.984 0.157
2024-12-02-10:21:56-root-INFO: grad norm: 1.056 1.047 0.142
2024-12-02-10:21:56-root-INFO: Loss too large (2.950->2.990)! Learning rate decreased to 0.40545.
2024-12-02-10:21:56-root-INFO: grad norm: 0.839 0.831 0.115
2024-12-02-10:21:57-root-INFO: grad norm: 0.814 0.810 0.078
2024-12-02-10:21:57-root-INFO: Loss too large (2.822->2.835)! Learning rate decreased to 0.32436.
2024-12-02-10:21:58-root-INFO: grad norm: 0.575 0.572 0.062
2024-12-02-10:21:58-root-INFO: Loss Change: 3.251 -> 2.735
2024-12-02-10:21:58-root-INFO: Regularization Change: 0.000 -> 1.150
2024-12-02-10:21:58-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-10:21:58-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-10:21:58-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-10:21:58-root-INFO: grad norm: 1.276 1.247 0.267
2024-12-02-10:21:59-root-INFO: grad norm: 1.074 1.058 0.184
2024-12-02-10:21:59-root-INFO: grad norm: 1.123 1.112 0.159
2024-12-02-10:21:59-root-INFO: Loss too large (2.743->2.781)! Learning rate decreased to 0.41109.
2024-12-02-10:22:00-root-INFO: grad norm: 0.730 0.723 0.101
2024-12-02-10:22:00-root-INFO: grad norm: 0.508 0.506 0.044
2024-12-02-10:22:01-root-INFO: grad norm: 0.541 0.540 0.034
2024-12-02-10:22:01-root-INFO: Loss too large (2.548->2.548)! Learning rate decreased to 0.32887.
2024-12-02-10:22:01-root-INFO: grad norm: 0.575 0.573 0.049
2024-12-02-10:22:02-root-INFO: grad norm: 0.652 0.651 0.038
2024-12-02-10:22:02-root-INFO: Loss Change: 2.939 -> 2.504
2024-12-02-10:22:02-root-INFO: Regularization Change: 0.000 -> 0.994
2024-12-02-10:22:02-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-10:22:02-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-10:22:02-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-10:22:02-root-INFO: grad norm: 1.123 1.106 0.194
2024-12-02-10:22:03-root-INFO: grad norm: 1.208 1.202 0.114
2024-12-02-10:22:03-root-INFO: grad norm: 0.866 0.860 0.103
2024-12-02-10:22:04-root-INFO: grad norm: 0.761 0.754 0.102
2024-12-02-10:22:04-root-INFO: grad norm: 0.743 0.738 0.087
2024-12-02-10:22:04-root-INFO: Loss too large (2.309->2.311)! Learning rate decreased to 0.41675.
2024-12-02-10:22:05-root-INFO: grad norm: 0.645 0.640 0.080
2024-12-02-10:22:05-root-INFO: grad norm: 0.661 0.659 0.057
2024-12-02-10:22:06-root-INFO: grad norm: 0.605 0.601 0.072
2024-12-02-10:22:06-root-INFO: Loss Change: 2.647 -> 2.183
2024-12-02-10:22:06-root-INFO: Regularization Change: 0.000 -> 1.162
2024-12-02-10:22:06-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-10:22:06-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-10:22:06-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-10:22:06-root-INFO: grad norm: 1.271 1.246 0.255
2024-12-02-10:22:07-root-INFO: grad norm: 0.973 0.960 0.159
2024-12-02-10:22:07-root-INFO: grad norm: 0.800 0.791 0.118
2024-12-02-10:22:08-root-INFO: grad norm: 0.846 0.837 0.123
2024-12-02-10:22:08-root-INFO: grad norm: 0.959 0.953 0.110
2024-12-02-10:22:09-root-INFO: grad norm: 0.760 0.752 0.112
2024-12-02-10:22:09-root-INFO: grad norm: 0.603 0.599 0.077
2024-12-02-10:22:10-root-INFO: grad norm: 0.555 0.550 0.078
2024-12-02-10:22:10-root-INFO: Loss Change: 2.407 -> 1.984
2024-12-02-10:22:10-root-INFO: Regularization Change: 0.000 -> 1.096
2024-12-02-10:22:10-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-10:22:10-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-10:22:10-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-10:22:10-root-INFO: grad norm: 1.396 1.368 0.281
2024-12-02-10:22:11-root-INFO: grad norm: 1.054 1.040 0.170
2024-12-02-10:22:11-root-INFO: grad norm: 0.851 0.843 0.118
2024-12-02-10:22:12-root-INFO: grad norm: 0.802 0.792 0.123
2024-12-02-10:22:12-root-INFO: grad norm: 0.794 0.788 0.098
2024-12-02-10:22:13-root-INFO: grad norm: 0.706 0.696 0.113
2024-12-02-10:22:13-root-INFO: grad norm: 0.631 0.626 0.081
2024-12-02-10:22:14-root-INFO: grad norm: 0.625 0.617 0.099
2024-12-02-10:22:14-root-INFO: Loss Change: 2.256 -> 1.816
2024-12-02-10:22:14-root-INFO: Regularization Change: 0.000 -> 1.009
2024-12-02-10:22:14-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-10:22:14-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-10:22:14-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-10:22:14-root-INFO: grad norm: 1.301 1.274 0.265
2024-12-02-10:22:15-root-INFO: grad norm: 0.906 0.893 0.151
2024-12-02-10:22:15-root-INFO: grad norm: 0.698 0.692 0.087
2024-12-02-10:22:16-root-INFO: grad norm: 0.575 0.569 0.087
2024-12-02-10:22:16-root-INFO: grad norm: 0.514 0.511 0.061
2024-12-02-10:22:17-root-INFO: grad norm: 0.460 0.456 0.064
2024-12-02-10:22:17-root-INFO: grad norm: 0.422 0.419 0.048
2024-12-02-10:22:17-root-INFO: grad norm: 0.393 0.390 0.054
2024-12-02-10:22:18-root-INFO: Loss Change: 2.067 -> 1.632
2024-12-02-10:22:18-root-INFO: Regularization Change: 0.000 -> 0.912
2024-12-02-10:22:18-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-10:22:18-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-10:22:18-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-10:22:18-root-INFO: grad norm: 1.074 1.051 0.217
2024-12-02-10:22:19-root-INFO: grad norm: 0.634 0.627 0.095
2024-12-02-10:22:19-root-INFO: grad norm: 0.450 0.446 0.053
2024-12-02-10:22:19-root-INFO: grad norm: 0.352 0.349 0.047
2024-12-02-10:22:20-root-INFO: grad norm: 0.304 0.302 0.034
2024-12-02-10:22:20-root-INFO: grad norm: 0.278 0.276 0.035
2024-12-02-10:22:21-root-INFO: grad norm: 0.249 0.248 0.027
2024-12-02-10:22:21-root-INFO: grad norm: 0.223 0.221 0.029
2024-12-02-10:22:22-root-INFO: Loss Change: 1.861 -> 1.496
2024-12-02-10:22:22-root-INFO: Regularization Change: 0.000 -> 0.836
2024-12-02-10:22:22-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-10:22:22-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-10:22:22-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-10:22:22-root-INFO: grad norm: 0.883 0.868 0.162
2024-12-02-10:22:22-root-INFO: grad norm: 0.490 0.486 0.064
2024-12-02-10:22:23-root-INFO: grad norm: 0.288 0.286 0.035
2024-12-02-10:22:23-root-INFO: grad norm: 0.227 0.225 0.032
2024-12-02-10:22:24-root-INFO: grad norm: 0.194 0.192 0.026
2024-12-02-10:22:24-root-INFO: grad norm: 0.175 0.173 0.027
2024-12-02-10:22:25-root-INFO: grad norm: 0.166 0.164 0.023
2024-12-02-10:22:25-root-INFO: grad norm: 0.160 0.158 0.024
2024-12-02-10:22:26-root-INFO: Loss Change: 1.686 -> 1.392
2024-12-02-10:22:26-root-INFO: Regularization Change: 0.000 -> 0.756
2024-12-02-10:22:26-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-10:22:26-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-10:22:26-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-10:22:26-root-INFO: grad norm: 0.835 0.822 0.150
2024-12-02-10:22:26-root-INFO: grad norm: 0.405 0.401 0.057
2024-12-02-10:22:27-root-INFO: grad norm: 0.259 0.257 0.034
2024-12-02-10:22:27-root-INFO: grad norm: 0.219 0.217 0.034
2024-12-02-10:22:28-root-INFO: grad norm: 0.209 0.207 0.028
2024-12-02-10:22:28-root-INFO: grad norm: 0.212 0.209 0.031
2024-12-02-10:22:29-root-INFO: grad norm: 0.199 0.197 0.028
2024-12-02-10:22:29-root-INFO: grad norm: 0.213 0.211 0.031
2024-12-02-10:22:30-root-INFO: Loss Change: 1.581 -> 1.313
2024-12-02-10:22:30-root-INFO: Regularization Change: 0.000 -> 0.696
2024-12-02-10:22:30-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-10:22:30-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-10:22:30-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-10:22:30-root-INFO: grad norm: 0.819 0.806 0.144
2024-12-02-10:22:30-root-INFO: grad norm: 0.428 0.425 0.051
2024-12-02-10:22:31-root-INFO: grad norm: 0.338 0.337 0.031
2024-12-02-10:22:31-root-INFO: grad norm: 0.223 0.221 0.032
2024-12-02-10:22:32-root-INFO: grad norm: 0.214 0.212 0.026
2024-12-02-10:22:32-root-INFO: grad norm: 0.267 0.265 0.027
2024-12-02-10:22:33-root-INFO: grad norm: 0.184 0.182 0.022
2024-12-02-10:22:33-root-INFO: grad norm: 0.173 0.171 0.024
2024-12-02-10:22:34-root-INFO: Loss Change: 1.497 -> 1.242
2024-12-02-10:22:34-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-02-10:22:34-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-10:22:34-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-10:22:34-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-10:22:34-root-INFO: grad norm: 0.747 0.740 0.100
2024-12-02-10:22:34-root-INFO: grad norm: 0.368 0.367 0.033
2024-12-02-10:22:35-root-INFO: grad norm: 0.317 0.316 0.027
2024-12-02-10:22:35-root-INFO: grad norm: 0.368 0.367 0.023
2024-12-02-10:22:36-root-INFO: grad norm: 0.213 0.212 0.021
2024-12-02-10:22:36-root-INFO: grad norm: 0.197 0.196 0.021
2024-12-02-10:22:37-root-INFO: grad norm: 0.191 0.190 0.018
2024-12-02-10:22:37-root-INFO: grad norm: 0.183 0.183 0.018
2024-12-02-10:22:38-root-INFO: Loss Change: 1.401 -> 1.140
2024-12-02-10:22:38-root-INFO: Regularization Change: 0.000 -> 0.789
2024-12-02-10:22:38-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-10:22:38-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-10:22:38-root-INFO: loss_sample0_0: 1.140455722808838
2024-12-02-10:22:38-root-INFO: It takes 1130.377 seconds for image sample0
2024-12-02-10:22:38-root-INFO: lpips_score_sample0: 0.146
2024-12-02-10:22:38-root-INFO: psnr_score_sample0: 17.785
2024-12-02-10:22:38-root-INFO: ssim_score_sample0: 0.723
2024-12-02-10:22:38-root-INFO: mean_lpips: 0.14617715775966644
2024-12-02-10:22:38-root-INFO: best_mean_lpips: 0.14617715775966644
2024-12-02-10:22:38-root-INFO: mean_psnr: 17.784637451171875
2024-12-02-10:22:38-root-INFO: best_mean_psnr: 17.784637451171875
2024-12-02-10:22:38-root-INFO: mean_ssim: 0.7232635021209717
2024-12-02-10:22:38-root-INFO: best_mean_ssim: 0.7232635021209717
2024-12-02-10:22:38-root-INFO: final_loss: 1.140455722808838
2024-12-02-10:22:38-root-INFO: mean time: 1130.3765287399292
2024-12-02-10:22:38-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample1_iter8_lr0.03_10009 
 
Enjoy.
