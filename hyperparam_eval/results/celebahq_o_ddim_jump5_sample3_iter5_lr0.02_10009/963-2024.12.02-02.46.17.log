2024-12-02-02:46:20-root-INFO: Prepare model...
2024-12-02-02:46:37-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-02:47:02-root-INFO: Start sampling
2024-12-02-02:47:07-root-INFO: step: 249 lr_xt 0.00012706
2024-12-02-02:47:07-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-02:47:07-root-INFO: grad norm: 23714.461 17716.807 15763.579
2024-12-02-02:47:08-root-INFO: grad norm: 26795.617 21197.357 16391.375
2024-12-02-02:47:08-root-INFO: Loss too large (43814.359->60950.188)! Learning rate decreased to 0.00010.
2024-12-02-02:47:08-root-INFO: Loss too large (43814.359->45481.734)! Learning rate decreased to 0.00008.
2024-12-02-02:47:09-root-INFO: grad norm: 21165.270 16200.714 13620.040
2024-12-02-02:47:09-root-INFO: grad norm: 19146.432 15681.396 10985.431
2024-12-02-02:47:09-root-INFO: Loss Change: 77070.016 -> 26879.289
2024-12-02-02:47:09-root-INFO: Regularization Change: 0.000 -> 12.887
2024-12-02-02:47:09-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-02-02:47:09-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-02:47:10-root-INFO: step: 248 lr_xt 0.00013388
2024-12-02-02:47:10-root-INFO: grad norm: 16835.762 13088.529 10589.300
2024-12-02-02:47:10-root-INFO: Loss too large (26924.457->41985.211)! Learning rate decreased to 0.00011.
2024-12-02-02:47:10-root-INFO: Loss too large (26924.457->30480.492)! Learning rate decreased to 0.00009.
2024-12-02-02:47:10-root-INFO: grad norm: 14964.313 12113.482 8786.025
2024-12-02-02:47:11-root-INFO: grad norm: 13332.560 10543.233 8160.721
2024-12-02-02:47:11-root-INFO: grad norm: 11797.590 9478.980 7023.679
2024-12-02-02:47:12-root-INFO: grad norm: 10414.552 8320.904 6263.021
2024-12-02-02:47:12-root-INFO: Loss Change: 26924.457 -> 19160.955
2024-12-02-02:47:12-root-INFO: Regularization Change: 0.000 -> 2.795
2024-12-02-02:47:12-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-02-02:47:12-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-02:47:12-root-INFO: step: 247 lr_xt 0.00014104
2024-12-02-02:47:12-root-INFO: grad norm: 8804.707 7051.816 5272.073
2024-12-02-02:47:13-root-INFO: Loss too large (18953.715->23038.189)! Learning rate decreased to 0.00011.
2024-12-02-02:47:13-root-INFO: Loss too large (18953.715->19964.941)! Learning rate decreased to 0.00009.
2024-12-02-02:47:13-root-INFO: grad norm: 7322.892 5943.256 4278.136
2024-12-02-02:47:14-root-INFO: grad norm: 5995.667 4786.701 3610.474
2024-12-02-02:47:14-root-INFO: grad norm: 4943.982 4020.708 2876.954
2024-12-02-02:47:15-root-INFO: grad norm: 4022.718 3207.685 2427.554
2024-12-02-02:47:15-root-INFO: Loss Change: 18953.715 -> 16716.326
2024-12-02-02:47:15-root-INFO: Regularization Change: 0.000 -> 0.649
2024-12-02-02:47:15-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-02-02:47:15-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-02:47:15-root-INFO: step: 246 lr_xt 0.00014856
2024-12-02-02:47:15-root-INFO: grad norm: 3163.139 2611.508 1784.790
2024-12-02-02:47:16-root-INFO: Loss too large (16469.160->16841.367)! Learning rate decreased to 0.00012.
2024-12-02-02:47:16-root-INFO: grad norm: 3733.541 2999.112 2223.658
2024-12-02-02:47:16-root-INFO: Loss too large (16466.955->16537.305)! Learning rate decreased to 0.00010.
2024-12-02-02:47:17-root-INFO: grad norm: 2894.060 2370.212 1660.626
2024-12-02-02:47:17-root-INFO: grad norm: 2242.430 1817.601 1313.324
2024-12-02-02:47:18-root-INFO: grad norm: 1775.953 1456.444 1016.258
2024-12-02-02:47:18-root-INFO: Loss Change: 16469.160 -> 15881.549
2024-12-02-02:47:18-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-02:47:18-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-02-02:47:18-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-02:47:19-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-02:47:19-root-INFO: grad norm: 1396.807 1149.113 794.110
2024-12-02-02:47:20-root-INFO: grad norm: 2001.389 1614.828 1182.323
2024-12-02-02:47:20-root-INFO: Loss too large (15714.606->15791.778)! Learning rate decreased to 0.00013.
2024-12-02-02:47:21-root-INFO: grad norm: 2205.684 1820.585 1245.196
2024-12-02-02:47:21-root-INFO: grad norm: 2437.610 1958.544 1451.223
2024-12-02-02:47:22-root-INFO: grad norm: 2714.867 2240.669 1532.940
2024-12-02-02:47:22-root-INFO: Loss Change: 15753.121 -> 15577.959
2024-12-02-02:47:22-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-02:47:22-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-02:47:22-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-02:47:22-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-02:47:22-root-INFO: grad norm: 3027.807 2437.723 1795.863
2024-12-02-02:47:23-root-INFO: Loss too large (15495.305->15710.886)! Learning rate decreased to 0.00013.
2024-12-02-02:47:23-root-INFO: grad norm: 3080.229 2534.654 1750.240
2024-12-02-02:47:24-root-INFO: grad norm: 3224.301 2601.384 1904.973
2024-12-02-02:47:24-root-INFO: grad norm: 3423.064 2837.987 1913.948
2024-12-02-02:47:25-root-INFO: grad norm: 3642.854 2939.733 2151.362
2024-12-02-02:47:25-root-INFO: Loss Change: 15495.305 -> 15239.139
2024-12-02-02:47:25-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-02-02:47:25-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-02:47:25-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:47:25-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-02:47:25-root-INFO: grad norm: 3729.949 3093.824 2083.453
2024-12-02-02:47:25-root-INFO: Loss too large (15090.762->15567.066)! Learning rate decreased to 0.00014.
2024-12-02-02:47:26-root-INFO: grad norm: 3749.857 3021.490 2220.816
2024-12-02-02:47:27-root-INFO: grad norm: 3820.119 3189.484 2102.498
2024-12-02-02:47:28-root-INFO: grad norm: 3910.977 3161.704 2302.036
2024-12-02-02:47:29-root-INFO: grad norm: 4023.667 3358.692 2215.645
2024-12-02-02:47:29-root-INFO: Loss Change: 15090.762 -> 14810.541
2024-12-02-02:47:29-root-INFO: Regularization Change: 0.000 -> 0.562
2024-12-02-02:47:29-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-02:47:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:47:30-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-02:47:30-root-INFO: grad norm: 3848.380 3157.678 2199.794
2024-12-02-02:47:30-root-INFO: Loss too large (14551.440->15040.276)! Learning rate decreased to 0.00015.
2024-12-02-02:47:31-root-INFO: grad norm: 3788.390 3177.435 2062.961
2024-12-02-02:47:32-root-INFO: grad norm: 3761.593 3070.550 2172.856
2024-12-02-02:47:33-root-INFO: grad norm: 3744.369 3147.754 2027.792
2024-12-02-02:47:34-root-INFO: grad norm: 3737.701 3049.896 2160.680
2024-12-02-02:47:35-root-INFO: Loss Change: 14551.440 -> 14081.938
2024-12-02-02:47:35-root-INFO: Regularization Change: 0.000 -> 0.597
2024-12-02-02:47:35-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-02:47:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:47:35-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-02:47:36-root-INFO: grad norm: 3714.774 3125.804 2007.211
2024-12-02-02:47:36-root-INFO: Loss too large (14018.206->14495.779)! Learning rate decreased to 0.00015.
2024-12-02-02:47:37-root-INFO: grad norm: 3580.959 2949.094 2031.285
2024-12-02-02:47:38-root-INFO: grad norm: 3457.552 2920.391 1850.940
2024-12-02-02:47:39-root-INFO: grad norm: 3350.978 2756.386 1905.620
2024-12-02-02:47:40-root-INFO: grad norm: 3245.067 2744.865 1730.947
2024-12-02-02:47:41-root-INFO: Loss Change: 14018.206 -> 13458.200
2024-12-02-02:47:41-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-02-02:47:41-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-02:47:41-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:47:41-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-02:47:41-root-INFO: grad norm: 2989.389 2492.424 1650.536
2024-12-02-02:47:42-root-INFO: Loss too large (13258.891->13467.074)! Learning rate decreased to 0.00016.
2024-12-02-02:47:43-root-INFO: grad norm: 2773.492 2367.595 1444.561
2024-12-02-02:47:43-root-INFO: grad norm: 2604.699 2157.913 1458.721
2024-12-02-02:47:44-root-INFO: grad norm: 2453.360 2105.045 1260.064
2024-12-02-02:47:45-root-INFO: grad norm: 2321.195 1923.874 1298.713
2024-12-02-02:47:46-root-INFO: Loss Change: 13258.891 -> 12626.756
2024-12-02-02:47:46-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-02:47:46-root-INFO: Undo step: 240
2024-12-02-02:47:46-root-INFO: Undo step: 241
2024-12-02-02:47:46-root-INFO: Undo step: 242
2024-12-02-02:47:46-root-INFO: Undo step: 243
2024-12-02-02:47:46-root-INFO: Undo step: 244
2024-12-02-02:47:46-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-02:47:47-root-INFO: grad norm: 12080.351 8986.139 8073.670
2024-12-02-02:47:47-root-INFO: Loss too large (20543.617->21792.303)! Learning rate decreased to 0.00013.
2024-12-02-02:47:48-root-INFO: grad norm: 8860.079 6653.778 5850.491
2024-12-02-02:47:49-root-INFO: grad norm: 7161.514 5583.635 4484.451
2024-12-02-02:47:50-root-INFO: grad norm: 6605.524 5110.796 4184.819
2024-12-02-02:47:51-root-INFO: grad norm: 6243.811 5009.570 3726.846
2024-12-02-02:47:51-root-INFO: Loss too large (15673.266->15686.825)! Learning rate decreased to 0.00010.
2024-12-02-02:47:52-root-INFO: Loss Change: 20543.617 -> 14932.276
2024-12-02-02:47:52-root-INFO: Regularization Change: 0.000 -> 1.563
2024-12-02-02:47:52-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-02:47:52-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-02:47:52-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-02:47:53-root-INFO: grad norm: 3831.615 2984.008 2403.533
2024-12-02-02:47:53-root-INFO: Loss too large (14881.581->15104.699)! Learning rate decreased to 0.00013.
2024-12-02-02:47:54-root-INFO: grad norm: 3521.393 2866.918 2044.746
2024-12-02-02:47:55-root-INFO: grad norm: 3408.767 2680.451 2105.915
2024-12-02-02:47:56-root-INFO: grad norm: 3363.526 2786.597 1883.662
2024-12-02-02:47:57-root-INFO: grad norm: 3381.255 2671.510 2072.661
2024-12-02-02:47:58-root-INFO: Loss Change: 14881.581 -> 14196.180
2024-12-02-02:47:58-root-INFO: Regularization Change: 0.000 -> 0.569
2024-12-02-02:47:58-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-02:47:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:47:58-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-02:47:58-root-INFO: grad norm: 3286.575 2713.065 1854.955
2024-12-02-02:47:59-root-INFO: Loss too large (13955.372->14217.866)! Learning rate decreased to 0.00014.
2024-12-02-02:48:00-root-INFO: grad norm: 3150.018 2485.671 1934.956
2024-12-02-02:48:01-root-INFO: grad norm: 3076.311 2576.084 1681.511
2024-12-02-02:48:02-root-INFO: grad norm: 3046.013 2413.240 1858.620
2024-12-02-02:48:03-root-INFO: grad norm: 3030.746 2544.488 1646.512
2024-12-02-02:48:04-root-INFO: Loss Change: 13955.372 -> 13460.870
2024-12-02-02:48:04-root-INFO: Regularization Change: 0.000 -> 0.487
2024-12-02-02:48:04-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-02:48:04-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:48:04-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-02:48:04-root-INFO: grad norm: 2762.934 2223.612 1639.925
2024-12-02-02:48:04-root-INFO: Loss too large (13209.483->13363.456)! Learning rate decreased to 0.00015.
2024-12-02-02:48:06-root-INFO: grad norm: 2630.778 2217.826 1415.004
2024-12-02-02:48:06-root-INFO: grad norm: 2541.534 2031.090 1527.765
2024-12-02-02:48:07-root-INFO: grad norm: 2465.059 2087.950 1310.335
2024-12-02-02:48:08-root-INFO: grad norm: 2400.224 1916.582 1444.918
2024-12-02-02:48:09-root-INFO: Loss Change: 13209.483 -> 12702.340
2024-12-02-02:48:09-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-02-02:48:09-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-02:48:09-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:48:09-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-02:48:10-root-INFO: grad norm: 2386.894 2014.380 1280.445
2024-12-02-02:48:10-root-INFO: Loss too large (12627.697->12731.990)! Learning rate decreased to 0.00015.
2024-12-02-02:48:11-root-INFO: grad norm: 2238.714 1809.819 1317.724
2024-12-02-02:48:12-root-INFO: grad norm: 2107.804 1792.199 1109.441
2024-12-02-02:48:13-root-INFO: grad norm: 1996.387 1612.882 1176.509
2024-12-02-02:48:14-root-INFO: grad norm: 1890.745 1613.326 985.949
2024-12-02-02:48:15-root-INFO: Loss Change: 12627.697 -> 12140.184
2024-12-02-02:48:15-root-INFO: Regularization Change: 0.000 -> 0.400
2024-12-02-02:48:15-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-02:48:15-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:48:15-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-02:48:15-root-INFO: grad norm: 1727.628 1424.707 977.195
2024-12-02-02:48:16-root-INFO: grad norm: 2231.171 1893.396 1180.329
2024-12-02-02:48:17-root-INFO: Loss too large (11929.777->12001.756)! Learning rate decreased to 0.00016.
2024-12-02-02:48:18-root-INFO: grad norm: 2024.901 1651.691 1171.385
2024-12-02-02:48:19-root-INFO: grad norm: 1841.102 1576.165 951.504
2024-12-02-02:48:20-root-INFO: grad norm: 1685.758 1376.385 973.316
2024-12-02-02:48:20-root-INFO: Loss Change: 11959.141 -> 11518.306
2024-12-02-02:48:20-root-INFO: Regularization Change: 0.000 -> 0.417
2024-12-02-02:48:20-root-INFO: Undo step: 240
2024-12-02-02:48:20-root-INFO: Undo step: 241
2024-12-02-02:48:20-root-INFO: Undo step: 242
2024-12-02-02:48:20-root-INFO: Undo step: 243
2024-12-02-02:48:20-root-INFO: Undo step: 244
2024-12-02-02:48:21-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-02:48:21-root-INFO: grad norm: 15772.855 12715.861 9332.194
2024-12-02-02:48:21-root-INFO: Loss too large (24166.697->25933.059)! Learning rate decreased to 0.00013.
2024-12-02-02:48:22-root-INFO: grad norm: 12877.771 10004.533 8108.410
2024-12-02-02:48:23-root-INFO: grad norm: 11670.799 9529.892 6737.115
2024-12-02-02:48:24-root-INFO: Loss too large (17371.021->17704.623)! Learning rate decreased to 0.00010.
2024-12-02-02:48:25-root-INFO: grad norm: 7625.434 6034.227 4662.118
2024-12-02-02:48:26-root-INFO: grad norm: 4784.538 3925.073 2735.984
2024-12-02-02:48:26-root-INFO: Loss Change: 24166.697 -> 13280.623
2024-12-02-02:48:26-root-INFO: Regularization Change: 0.000 -> 3.241
2024-12-02-02:48:26-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-02:48:26-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-02:48:27-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-02:48:27-root-INFO: grad norm: 3111.051 2473.327 1887.139
2024-12-02-02:48:27-root-INFO: Loss too large (13198.654->13306.451)! Learning rate decreased to 0.00013.
2024-12-02-02:48:28-root-INFO: grad norm: 2952.835 2420.934 1690.656
2024-12-02-02:48:29-root-INFO: grad norm: 2911.749 2331.879 1743.737
2024-12-02-02:48:30-root-INFO: grad norm: 2912.105 2405.431 1641.420
2024-12-02-02:48:31-root-INFO: grad norm: 2938.397 2358.202 1753.016
2024-12-02-02:48:32-root-INFO: Loss Change: 13198.654 -> 12615.065
2024-12-02-02:48:32-root-INFO: Regularization Change: 0.000 -> 0.502
2024-12-02-02:48:32-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-02:48:32-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:48:32-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-02:48:33-root-INFO: grad norm: 2702.511 2216.010 1546.889
2024-12-02-02:48:33-root-INFO: Loss too large (12335.611->12469.893)! Learning rate decreased to 0.00014.
2024-12-02-02:48:34-root-INFO: grad norm: 2575.098 2067.150 1535.584
2024-12-02-02:48:35-root-INFO: grad norm: 2506.504 2088.592 1385.765
2024-12-02-02:48:36-root-INFO: grad norm: 2475.455 1990.203 1472.063
2024-12-02-02:48:37-root-INFO: grad norm: 2453.380 2052.399 1344.148
2024-12-02-02:48:38-root-INFO: Loss Change: 12335.611 -> 11895.306
2024-12-02-02:48:38-root-INFO: Regularization Change: 0.000 -> 0.392
2024-12-02-02:48:38-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-02:48:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:48:38-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-02:48:38-root-INFO: grad norm: 2279.007 1869.463 1303.449
2024-12-02-02:48:39-root-INFO: Loss too large (11671.820->11738.971)! Learning rate decreased to 0.00015.
2024-12-02-02:48:40-root-INFO: grad norm: 2140.109 1798.068 1160.611
2024-12-02-02:48:41-root-INFO: grad norm: 2051.831 1664.205 1200.179
2024-12-02-02:48:42-root-INFO: grad norm: 1974.373 1673.802 1047.157
2024-12-02-02:48:43-root-INFO: grad norm: 1905.832 1543.667 1117.717
2024-12-02-02:48:43-root-INFO: Loss Change: 11671.820 -> 11238.491
2024-12-02-02:48:43-root-INFO: Regularization Change: 0.000 -> 0.349
2024-12-02-02:48:43-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-02:48:43-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:48:44-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-02:48:44-root-INFO: grad norm: 1865.782 1573.352 1002.848
2024-12-02-02:48:44-root-INFO: Loss too large (11155.319->11178.199)! Learning rate decreased to 0.00015.
2024-12-02-02:48:45-root-INFO: grad norm: 1711.440 1406.877 974.537
2024-12-02-02:48:46-root-INFO: grad norm: 1583.986 1355.970 818.753
2024-12-02-02:48:47-root-INFO: grad norm: 1478.413 1215.853 841.076
2024-12-02-02:48:48-root-INFO: grad norm: 1382.538 1193.513 697.809
2024-12-02-02:48:49-root-INFO: Loss Change: 11155.319 -> 10752.323
2024-12-02-02:48:49-root-INFO: Regularization Change: 0.000 -> 0.309
2024-12-02-02:48:49-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-02:48:49-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:48:49-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-02:48:50-root-INFO: grad norm: 1345.702 1134.244 724.157
2024-12-02-02:48:51-root-INFO: grad norm: 1637.647 1404.326 842.469
2024-12-02-02:48:52-root-INFO: grad norm: 2113.311 1739.958 1199.428
2024-12-02-02:48:52-root-INFO: Loss too large (10527.856->10589.551)! Learning rate decreased to 0.00016.
2024-12-02-02:48:53-root-INFO: grad norm: 1867.469 1600.615 962.014
2024-12-02-02:48:54-root-INFO: grad norm: 1658.524 1370.092 934.638
2024-12-02-02:48:55-root-INFO: Loss Change: 10587.360 -> 10246.900
2024-12-02-02:48:55-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-02:48:55-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-02:48:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:48:55-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-02:48:55-root-INFO: grad norm: 1377.727 1191.388 691.900
2024-12-02-02:48:56-root-INFO: grad norm: 1714.682 1425.015 953.661
2024-12-02-02:48:57-root-INFO: Loss too large (10114.233->10122.091)! Learning rate decreased to 0.00017.
2024-12-02-02:48:58-root-INFO: grad norm: 1476.971 1275.920 743.957
2024-12-02-02:48:59-root-INFO: grad norm: 1285.697 1076.399 703.123
2024-12-02-02:49:00-root-INFO: grad norm: 1126.530 987.538 542.070
2024-12-02-02:49:00-root-INFO: Loss Change: 10141.344 -> 9795.198
2024-12-02-02:49:00-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-02-02:49:00-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-02:49:00-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:01-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-02:49:01-root-INFO: grad norm: 1234.074 1058.639 634.211
2024-12-02-02:49:02-root-INFO: grad norm: 1324.864 1155.728 647.733
2024-12-02-02:49:03-root-INFO: grad norm: 1574.842 1319.219 860.109
2024-12-02-02:49:04-root-INFO: grad norm: 1916.460 1661.953 954.323
2024-12-02-02:49:04-root-INFO: Loss too large (9500.699->9534.465)! Learning rate decreased to 0.00018.
2024-12-02-02:49:05-root-INFO: grad norm: 1561.917 1314.589 843.468
2024-12-02-02:49:06-root-INFO: Loss Change: 9616.049 -> 9302.827
2024-12-02-02:49:06-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-02:49:06-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-02:49:06-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:06-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-02:49:07-root-INFO: grad norm: 1042.782 917.914 494.801
2024-12-02-02:49:08-root-INFO: grad norm: 1162.468 984.715 617.792
2024-12-02-02:49:09-root-INFO: grad norm: 1348.507 1192.167 630.246
2024-12-02-02:49:10-root-INFO: grad norm: 1580.300 1340.414 837.042
2024-12-02-02:49:11-root-INFO: grad norm: 1869.768 1634.695 907.637
2024-12-02-02:49:11-root-INFO: Loss too large (9094.751->9118.455)! Learning rate decreased to 0.00019.
2024-12-02-02:49:12-root-INFO: Loss Change: 9209.336 -> 8979.602
2024-12-02-02:49:12-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-02:49:12-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-02:49:12-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:12-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-02:49:12-root-INFO: grad norm: 1397.528 1205.432 707.119
2024-12-02-02:49:13-root-INFO: grad norm: 1598.083 1398.855 772.704
2024-12-02-02:49:14-root-INFO: grad norm: 1834.018 1568.720 950.125
2024-12-02-02:49:15-root-INFO: Loss too large (8798.933->8804.166)! Learning rate decreased to 0.00020.
2024-12-02-02:49:16-root-INFO: grad norm: 1370.865 1207.878 648.306
2024-12-02-02:49:17-root-INFO: grad norm: 1052.478 907.345 533.324
2024-12-02-02:49:17-root-INFO: Loss Change: 8830.824 -> 8516.458
2024-12-02-02:49:17-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-02-02:49:17-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-02:49:17-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:18-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-02:49:18-root-INFO: grad norm: 598.065 549.476 236.131
2024-12-02-02:49:19-root-INFO: grad norm: 593.314 529.518 267.644
2024-12-02-02:49:20-root-INFO: grad norm: 617.524 576.196 222.115
2024-12-02-02:49:21-root-INFO: grad norm: 649.557 575.539 301.129
2024-12-02-02:49:22-root-INFO: grad norm: 689.038 636.984 262.726
2024-12-02-02:49:23-root-INFO: Loss Change: 8411.070 -> 8165.086
2024-12-02-02:49:23-root-INFO: Regularization Change: 0.000 -> 0.351
2024-12-02-02:49:23-root-INFO: Undo step: 235
2024-12-02-02:49:23-root-INFO: Undo step: 236
2024-12-02-02:49:23-root-INFO: Undo step: 237
2024-12-02-02:49:23-root-INFO: Undo step: 238
2024-12-02-02:49:23-root-INFO: Undo step: 239
2024-12-02-02:49:23-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-02:49:23-root-INFO: grad norm: 5169.840 4156.795 3073.809
2024-12-02-02:49:24-root-INFO: grad norm: 4480.932 3776.436 2411.905
2024-12-02-02:49:25-root-INFO: Loss too large (10846.888->11060.960)! Learning rate decreased to 0.00016.
2024-12-02-02:49:26-root-INFO: grad norm: 3519.992 3007.431 1829.126
2024-12-02-02:49:27-root-INFO: grad norm: 2799.654 2366.695 1495.599
2024-12-02-02:49:28-root-INFO: grad norm: 2299.794 1969.118 1188.120
2024-12-02-02:49:28-root-INFO: Loss Change: 12228.540 -> 9640.786
2024-12-02-02:49:28-root-INFO: Regularization Change: 0.000 -> 1.510
2024-12-02-02:49:28-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-02:49:28-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:29-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-02:49:29-root-INFO: grad norm: 1739.074 1483.170 908.068
2024-12-02-02:49:30-root-INFO: grad norm: 2073.104 1771.728 1076.449
2024-12-02-02:49:30-root-INFO: Loss too large (9506.798->9513.530)! Learning rate decreased to 0.00017.
2024-12-02-02:49:31-root-INFO: grad norm: 1674.583 1434.592 863.814
2024-12-02-02:49:32-root-INFO: grad norm: 1381.005 1185.334 708.630
2024-12-02-02:49:33-root-INFO: grad norm: 1146.102 996.305 566.502
2024-12-02-02:49:34-root-INFO: Loss Change: 9536.896 -> 9097.320
2024-12-02-02:49:34-root-INFO: Regularization Change: 0.000 -> 0.366
2024-12-02-02:49:34-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-02:49:34-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:34-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-02:49:35-root-INFO: grad norm: 1172.293 1028.646 562.281
2024-12-02-02:49:36-root-INFO: grad norm: 1189.895 1043.618 571.587
2024-12-02-02:49:37-root-INFO: grad norm: 1338.496 1146.321 691.028
2024-12-02-02:49:38-root-INFO: grad norm: 1542.427 1349.191 747.505
2024-12-02-02:49:39-root-INFO: grad norm: 1787.087 1529.137 924.890
2024-12-02-02:49:39-root-INFO: Loss too large (8741.600->8744.042)! Learning rate decreased to 0.00018.
2024-12-02-02:49:40-root-INFO: Loss Change: 8905.682 -> 8627.264
2024-12-02-02:49:40-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-02-02:49:40-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-02:49:40-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:40-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-02:49:40-root-INFO: grad norm: 1077.679 947.944 512.636
2024-12-02-02:49:41-root-INFO: grad norm: 1159.936 999.306 588.930
2024-12-02-02:49:42-root-INFO: grad norm: 1292.355 1144.957 599.380
2024-12-02-02:49:43-root-INFO: grad norm: 1446.520 1248.134 731.152
2024-12-02-02:49:44-root-INFO: grad norm: 1636.848 1440.636 777.072
2024-12-02-02:49:45-root-INFO: Loss Change: 8549.025 -> 8420.748
2024-12-02-02:49:45-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-02-02:49:45-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-02:49:45-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:45-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-02:49:46-root-INFO: grad norm: 1757.788 1533.231 859.663
2024-12-02-02:49:47-root-INFO: grad norm: 1950.773 1710.888 937.218
2024-12-02-02:49:47-root-INFO: Loss too large (8245.398->8260.377)! Learning rate decreased to 0.00020.
2024-12-02-02:49:48-root-INFO: grad norm: 1403.170 1222.016 689.610
2024-12-02-02:49:49-root-INFO: grad norm: 1014.685 904.539 459.777
2024-12-02-02:49:50-root-INFO: grad norm: 766.993 677.233 360.046
2024-12-02-02:49:51-root-INFO: Loss Change: 8259.456 -> 7907.851
2024-12-02-02:49:51-root-INFO: Regularization Change: 0.000 -> 0.263
2024-12-02-02:49:51-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-02:49:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:49:51-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-02:49:51-root-INFO: grad norm: 455.570 422.964 169.252
2024-12-02-02:49:52-root-INFO: grad norm: 427.893 397.862 157.476
2024-12-02-02:49:53-root-INFO: grad norm: 422.089 399.884 135.099
2024-12-02-02:49:54-root-INFO: grad norm: 419.370 391.989 149.048
2024-12-02-02:49:55-root-INFO: grad norm: 418.439 397.372 131.101
2024-12-02-02:49:56-root-INFO: Loss Change: 7812.757 -> 7604.115
2024-12-02-02:49:56-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-02:49:56-root-INFO: Undo step: 235
2024-12-02-02:49:56-root-INFO: Undo step: 236
2024-12-02-02:49:56-root-INFO: Undo step: 237
2024-12-02-02:49:56-root-INFO: Undo step: 238
2024-12-02-02:49:56-root-INFO: Undo step: 239
2024-12-02-02:49:56-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-02:49:57-root-INFO: grad norm: 12759.886 9829.521 8136.044
2024-12-02-02:49:58-root-INFO: grad norm: 11208.284 9081.590 6568.894
2024-12-02-02:49:59-root-INFO: grad norm: 11122.723 9059.153 6453.425
2024-12-02-02:49:59-root-INFO: Loss too large (15537.908->17764.551)! Learning rate decreased to 0.00016.
2024-12-02-02:50:00-root-INFO: grad norm: 8471.692 7157.281 4532.428
2024-12-02-02:50:01-root-INFO: grad norm: 6381.945 5280.414 3584.196
2024-12-02-02:50:02-root-INFO: Loss Change: 19887.291 -> 10726.920
2024-12-02-02:50:02-root-INFO: Regularization Change: 0.000 -> 4.896
2024-12-02-02:50:02-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-02:50:02-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:50:02-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-02:50:02-root-INFO: grad norm: 5175.887 4390.333 2741.310
2024-12-02-02:50:03-root-INFO: Loss too large (10649.485->11122.592)! Learning rate decreased to 0.00017.
2024-12-02-02:50:04-root-INFO: grad norm: 4151.137 3469.055 2279.823
2024-12-02-02:50:05-root-INFO: grad norm: 3414.940 2889.852 1819.496
2024-12-02-02:50:06-root-INFO: grad norm: 2769.395 2330.820 1495.602
2024-12-02-02:50:07-root-INFO: grad norm: 2270.981 1917.825 1216.266
2024-12-02-02:50:07-root-INFO: Loss Change: 10649.485 -> 9066.962
2024-12-02-02:50:07-root-INFO: Regularization Change: 0.000 -> 0.820
2024-12-02-02:50:07-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-02:50:07-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:50:08-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-02:50:08-root-INFO: grad norm: 1752.449 1474.293 947.386
2024-12-02-02:50:09-root-INFO: grad norm: 1925.265 1631.862 1021.601
2024-12-02-02:50:10-root-INFO: grad norm: 2227.880 1910.203 1146.548
2024-12-02-02:50:10-root-INFO: Loss too large (8746.076->8778.688)! Learning rate decreased to 0.00018.
2024-12-02-02:50:11-root-INFO: grad norm: 1691.208 1432.189 899.456
2024-12-02-02:50:12-root-INFO: grad norm: 1288.274 1121.986 633.086
2024-12-02-02:50:13-root-INFO: Loss Change: 8811.725 -> 8400.891
2024-12-02-02:50:13-root-INFO: Regularization Change: 0.000 -> 0.370
2024-12-02-02:50:13-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-02:50:13-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:50:13-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-02:50:14-root-INFO: grad norm: 1193.069 1020.177 618.590
2024-12-02-02:50:15-root-INFO: grad norm: 1294.992 1134.645 624.168
2024-12-02-02:50:16-root-INFO: grad norm: 1424.701 1213.763 746.024
2024-12-02-02:50:17-root-INFO: grad norm: 1579.251 1380.069 767.752
2024-12-02-02:50:18-root-INFO: grad norm: 1742.447 1488.365 906.030
2024-12-02-02:50:18-root-INFO: Loss Change: 8350.031 -> 8213.287
2024-12-02-02:50:18-root-INFO: Regularization Change: 0.000 -> 0.419
2024-12-02-02:50:18-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-02:50:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:50:19-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-02:50:19-root-INFO: grad norm: 1860.380 1615.675 922.283
2024-12-02-02:50:20-root-INFO: grad norm: 1997.647 1717.395 1020.368
2024-12-02-02:50:21-root-INFO: grad norm: 2154.886 1876.223 1059.867
2024-12-02-02:50:21-root-INFO: Loss too large (8045.294->8054.410)! Learning rate decreased to 0.00020.
2024-12-02-02:50:22-root-INFO: grad norm: 1478.202 1272.995 751.375
2024-12-02-02:50:23-root-INFO: grad norm: 1014.896 899.857 469.331
2024-12-02-02:50:24-root-INFO: Loss Change: 8073.454 -> 7703.295
2024-12-02-02:50:24-root-INFO: Regularization Change: 0.000 -> 0.304
2024-12-02-02:50:24-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-02:50:24-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-02:50:24-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-02:50:25-root-INFO: grad norm: 968.256 845.698 471.502
2024-12-02-02:50:26-root-INFO: grad norm: 980.109 879.080 433.397
2024-12-02-02:50:27-root-INFO: grad norm: 1002.477 872.040 494.476
2024-12-02-02:50:28-root-INFO: grad norm: 1028.588 920.685 458.619
2024-12-02-02:50:29-root-INFO: grad norm: 1055.737 920.530 516.920
2024-12-02-02:50:29-root-INFO: Loss Change: 7629.764 -> 7451.502
2024-12-02-02:50:29-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-02-02:50:29-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-02:50:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-02:50:30-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-02:50:30-root-INFO: grad norm: 992.328 885.688 447.518
2024-12-02-02:50:31-root-INFO: grad norm: 996.302 873.169 479.784
2024-12-02-02:50:32-root-INFO: grad norm: 1006.886 902.309 446.830
2024-12-02-02:50:33-root-INFO: grad norm: 1017.199 893.069 486.950
2024-12-02-02:50:34-root-INFO: grad norm: 1028.200 920.818 457.482
2024-12-02-02:50:35-root-INFO: Loss Change: 7394.278 -> 7221.588
2024-12-02-02:50:35-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-02:50:35-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-02:50:35-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:50:35-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-02:50:35-root-INFO: grad norm: 1109.008 988.754 502.259
2024-12-02-02:50:36-root-INFO: grad norm: 1036.460 927.704 462.185
2024-12-02-02:50:37-root-INFO: grad norm: 998.747 884.350 464.134
2024-12-02-02:50:38-root-INFO: grad norm: 967.487 869.168 424.943
2024-12-02-02:50:39-root-INFO: grad norm: 941.214 834.200 435.882
2024-12-02-02:50:40-root-INFO: Loss Change: 7197.820 -> 6982.685
2024-12-02-02:50:40-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-02-02:50:40-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-02:50:40-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:50:40-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-02:50:41-root-INFO: grad norm: 604.034 537.925 274.762
2024-12-02-02:50:42-root-INFO: grad norm: 514.287 457.368 235.172
2024-12-02-02:50:43-root-INFO: grad norm: 490.995 456.262 181.385
2024-12-02-02:50:44-root-INFO: grad norm: 479.710 434.877 202.494
2024-12-02-02:50:45-root-INFO: grad norm: 471.930 438.813 173.669
2024-12-02-02:50:45-root-INFO: Loss Change: 6815.801 -> 6634.580
2024-12-02-02:50:45-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-02:50:45-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-02:50:45-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:50:46-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-02:50:46-root-INFO: grad norm: 573.589 520.822 240.310
2024-12-02-02:50:47-root-INFO: grad norm: 557.461 515.224 212.853
2024-12-02-02:50:48-root-INFO: grad norm: 545.910 495.468 229.193
2024-12-02-02:50:49-root-INFO: grad norm: 535.162 494.391 204.879
2024-12-02-02:50:50-root-INFO: grad norm: 526.913 478.662 220.274
2024-12-02-02:50:51-root-INFO: Loss Change: 6577.829 -> 6429.577
2024-12-02-02:50:51-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-02-02:50:51-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-02:50:51-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:50:51-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-02:50:51-root-INFO: grad norm: 327.032 311.546 99.445
2024-12-02-02:50:52-root-INFO: grad norm: 314.536 294.195 111.274
2024-12-02-02:50:53-root-INFO: grad norm: 308.862 294.183 94.085
2024-12-02-02:50:54-root-INFO: grad norm: 304.319 286.213 103.405
2024-12-02-02:50:55-root-INFO: grad norm: 300.494 285.881 92.566
2024-12-02-02:50:56-root-INFO: Loss Change: 6322.669 -> 6188.885
2024-12-02-02:50:56-root-INFO: Regularization Change: 0.000 -> 0.225
2024-12-02-02:50:56-root-INFO: Undo step: 230
2024-12-02-02:50:56-root-INFO: Undo step: 231
2024-12-02-02:50:56-root-INFO: Undo step: 232
2024-12-02-02:50:56-root-INFO: Undo step: 233
2024-12-02-02:50:56-root-INFO: Undo step: 234
2024-12-02-02:50:56-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-02:50:57-root-INFO: grad norm: 9398.501 7673.518 5426.688
2024-12-02-02:50:58-root-INFO: grad norm: 7050.858 5945.136 3790.773
2024-12-02-02:50:58-root-INFO: Loss too large (11468.649->11657.950)! Learning rate decreased to 0.00021.
2024-12-02-02:50:59-root-INFO: grad norm: 5074.613 4620.146 2099.036
2024-12-02-02:51:00-root-INFO: grad norm: 3751.355 3261.522 1853.413
2024-12-02-02:51:01-root-INFO: grad norm: 2885.573 2621.862 1205.144
2024-12-02-02:51:02-root-INFO: Loss Change: 17401.514 -> 7664.941
2024-12-02-02:51:02-root-INFO: Regularization Change: 0.000 -> 5.315
2024-12-02-02:51:02-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-02:51:02-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-02:51:02-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-02:51:02-root-INFO: grad norm: 1973.190 1711.554 981.867
2024-12-02-02:51:03-root-INFO: grad norm: 2150.819 1945.948 916.139
2024-12-02-02:51:04-root-INFO: grad norm: 2359.898 2071.788 1129.961
2024-12-02-02:51:05-root-INFO: grad norm: 2557.132 2288.616 1140.685
2024-12-02-02:51:06-root-INFO: grad norm: 2765.960 2438.025 1306.358
2024-12-02-02:51:06-root-INFO: Loss too large (7393.124->7433.615)! Learning rate decreased to 0.00022.
2024-12-02-02:51:07-root-INFO: Loss Change: 7570.647 -> 7092.482
2024-12-02-02:51:07-root-INFO: Regularization Change: 0.000 -> 0.903
2024-12-02-02:51:07-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-02:51:07-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:51:08-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-02:51:08-root-INFO: grad norm: 2015.732 1818.630 869.343
2024-12-02-02:51:09-root-INFO: grad norm: 2055.992 1821.880 952.815
2024-12-02-02:51:10-root-INFO: grad norm: 2107.628 1883.603 945.588
2024-12-02-02:51:11-root-INFO: grad norm: 2159.715 1916.833 995.048
2024-12-02-02:51:12-root-INFO: grad norm: 2200.268 1960.590 998.631
2024-12-02-02:51:13-root-INFO: Loss Change: 7123.583 -> 6928.436
2024-12-02-02:51:13-root-INFO: Regularization Change: 0.000 -> 0.720
2024-12-02-02:51:13-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-02:51:13-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:51:13-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-02:51:13-root-INFO: grad norm: 1611.802 1440.173 723.745
2024-12-02-02:51:14-root-INFO: grad norm: 1608.725 1439.095 719.028
2024-12-02-02:51:15-root-INFO: grad norm: 1626.919 1463.755 710.133
2024-12-02-02:51:16-root-INFO: grad norm: 1645.736 1473.553 732.862
2024-12-02-02:51:17-root-INFO: grad norm: 1666.867 1498.753 729.510
2024-12-02-02:51:18-root-INFO: Loss Change: 6620.512 -> 6489.052
2024-12-02-02:51:18-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-02-02:51:18-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-02:51:18-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:51:18-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-02:51:19-root-INFO: grad norm: 1793.934 1610.397 790.456
2024-12-02-02:51:19-root-INFO: grad norm: 1778.012 1602.915 769.409
2024-12-02-02:51:20-root-INFO: grad norm: 1759.812 1579.796 775.361
2024-12-02-02:51:21-root-INFO: grad norm: 1738.836 1567.336 753.001
2024-12-02-02:51:22-root-INFO: grad norm: 1719.293 1542.671 759.035
2024-12-02-02:51:23-root-INFO: Loss Change: 6433.324 -> 6274.878
2024-12-02-02:51:23-root-INFO: Regularization Change: 0.000 -> 0.497
2024-12-02-02:51:23-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-02:51:23-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:51:23-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-02:51:24-root-INFO: grad norm: 1301.602 1180.983 547.218
2024-12-02-02:51:25-root-INFO: grad norm: 1269.380 1143.124 551.901
2024-12-02-02:51:26-root-INFO: grad norm: 1237.603 1122.246 521.753
2024-12-02-02:51:27-root-INFO: grad norm: 1209.454 1089.163 525.836
2024-12-02-02:51:28-root-INFO: grad norm: 1181.831 1071.423 498.776
2024-12-02-02:51:28-root-INFO: Loss Change: 6100.166 -> 5968.872
2024-12-02-02:51:28-root-INFO: Regularization Change: 0.000 -> 0.343
2024-12-02-02:51:28-root-INFO: Undo step: 230
2024-12-02-02:51:28-root-INFO: Undo step: 231
2024-12-02-02:51:29-root-INFO: Undo step: 232
2024-12-02-02:51:29-root-INFO: Undo step: 233
2024-12-02-02:51:29-root-INFO: Undo step: 234
2024-12-02-02:51:29-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-02:51:29-root-INFO: grad norm: 2464.293 1904.750 1563.544
2024-12-02-02:51:30-root-INFO: grad norm: 1451.004 1104.516 940.986
2024-12-02-02:51:31-root-INFO: grad norm: 1220.098 997.725 702.270
2024-12-02-02:51:32-root-INFO: grad norm: 1130.169 928.109 644.900
2024-12-02-02:51:33-root-INFO: grad norm: 1125.115 972.647 565.547
2024-12-02-02:51:34-root-INFO: Loss Change: 7823.727 -> 6992.937
2024-12-02-02:51:34-root-INFO: Regularization Change: 0.000 -> 0.739
2024-12-02-02:51:34-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-02:51:34-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-02:51:34-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-02:51:35-root-INFO: grad norm: 1055.794 895.059 559.974
2024-12-02-02:51:36-root-INFO: grad norm: 1063.815 936.171 505.259
2024-12-02-02:51:37-root-INFO: grad norm: 1098.327 954.238 543.830
2024-12-02-02:51:38-root-INFO: grad norm: 1142.181 1008.837 535.561
2024-12-02-02:51:39-root-INFO: grad norm: 1193.740 1043.045 580.581
2024-12-02-02:51:39-root-INFO: Loss Change: 6918.241 -> 6724.893
2024-12-02-02:51:39-root-INFO: Regularization Change: 0.000 -> 0.395
2024-12-02-02:51:39-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-02:51:39-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:51:40-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-02:51:40-root-INFO: grad norm: 1401.565 1251.381 631.213
2024-12-02-02:51:41-root-INFO: grad norm: 1377.321 1208.237 661.193
2024-12-02-02:51:42-root-INFO: grad norm: 1381.294 1223.682 640.763
2024-12-02-02:51:43-root-INFO: grad norm: 1388.375 1219.884 662.924
2024-12-02-02:51:44-root-INFO: grad norm: 1394.062 1232.726 650.996
2024-12-02-02:51:45-root-INFO: Loss Change: 6694.877 -> 6510.619
2024-12-02-02:51:45-root-INFO: Regularization Change: 0.000 -> 0.411
2024-12-02-02:51:45-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-02:51:45-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:51:45-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-02:51:45-root-INFO: grad norm: 841.455 739.999 400.560
2024-12-02-02:51:46-root-INFO: grad norm: 780.681 691.932 361.515
2024-12-02-02:51:47-root-INFO: grad norm: 767.173 686.602 342.247
2024-12-02-02:51:48-root-INFO: grad norm: 759.469 676.965 344.255
2024-12-02-02:51:49-root-INFO: grad norm: 754.182 674.621 337.161
2024-12-02-02:51:50-root-INFO: Loss Change: 6294.504 -> 6135.674
2024-12-02-02:51:50-root-INFO: Regularization Change: 0.000 -> 0.274
2024-12-02-02:51:50-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-02:51:50-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:51:50-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-02:51:51-root-INFO: grad norm: 904.954 808.906 405.725
2024-12-02-02:51:52-root-INFO: grad norm: 878.277 785.880 392.127
2024-12-02-02:51:53-root-INFO: grad norm: 854.577 762.341 386.184
2024-12-02-02:51:54-root-INFO: grad norm: 829.453 742.508 369.696
2024-12-02-02:51:55-root-INFO: grad norm: 805.360 718.421 363.972
2024-12-02-02:51:55-root-INFO: Loss Change: 6087.167 -> 5956.328
2024-12-02-02:51:55-root-INFO: Regularization Change: 0.000 -> 0.249
2024-12-02-02:51:55-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-02:51:55-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:51:56-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-02:51:56-root-INFO: grad norm: 449.369 412.058 179.278
2024-12-02-02:51:57-root-INFO: grad norm: 425.282 384.666 181.375
2024-12-02-02:51:58-root-INFO: grad norm: 407.543 374.020 161.863
2024-12-02-02:51:59-root-INFO: grad norm: 391.883 355.748 164.363
2024-12-02-02:52:00-root-INFO: grad norm: 377.348 347.057 148.132
2024-12-02-02:52:01-root-INFO: Loss Change: 5840.320 -> 5731.735
2024-12-02-02:52:01-root-INFO: Regularization Change: 0.000 -> 0.186
2024-12-02-02:52:01-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-02:52:01-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:52:01-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-02:52:01-root-INFO: grad norm: 739.557 662.974 327.736
2024-12-02-02:52:02-root-INFO: grad norm: 684.638 619.030 292.456
2024-12-02-02:52:03-root-INFO: grad norm: 647.562 580.553 286.870
2024-12-02-02:52:04-root-INFO: grad norm: 613.791 555.591 260.879
2024-12-02-02:52:05-root-INFO: grad norm: 582.874 523.668 255.956
2024-12-02-02:52:06-root-INFO: Loss Change: 5724.828 -> 5594.304
2024-12-02-02:52:06-root-INFO: Regularization Change: 0.000 -> 0.239
2024-12-02-02:52:06-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-02:52:06-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:52:06-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-02:52:07-root-INFO: grad norm: 395.709 345.607 192.720
2024-12-02-02:52:08-root-INFO: grad norm: 336.366 306.196 139.235
2024-12-02-02:52:09-root-INFO: grad norm: 312.317 288.038 120.732
2024-12-02-02:52:10-root-INFO: grad norm: 296.119 272.576 115.711
2024-12-02-02:52:11-root-INFO: grad norm: 282.504 262.790 103.682
2024-12-02-02:52:11-root-INFO: Loss Change: 5540.296 -> 5440.354
2024-12-02-02:52:11-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-02:52:11-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-02:52:11-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:52:12-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-02:52:12-root-INFO: grad norm: 695.561 618.824 317.589
2024-12-02-02:52:13-root-INFO: grad norm: 611.221 552.997 260.356
2024-12-02-02:52:14-root-INFO: grad norm: 554.754 499.122 242.133
2024-12-02-02:52:15-root-INFO: grad norm: 506.345 459.609 212.472
2024-12-02-02:52:16-root-INFO: grad norm: 466.480 421.564 199.719
2024-12-02-02:52:17-root-INFO: Loss Change: 5463.052 -> 5327.741
2024-12-02-02:52:17-root-INFO: Regularization Change: 0.000 -> 0.248
2024-12-02-02:52:17-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-02:52:17-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-02:52:17-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-02:52:17-root-INFO: grad norm: 460.932 401.395 226.585
2024-12-02-02:52:18-root-INFO: grad norm: 373.780 340.378 154.448
2024-12-02-02:52:19-root-INFO: grad norm: 336.010 308.116 134.041
2024-12-02-02:52:20-root-INFO: grad norm: 307.732 281.731 123.800
2024-12-02-02:52:21-root-INFO: grad norm: 284.459 262.912 108.602
2024-12-02-02:52:22-root-INFO: Loss Change: 5261.174 -> 5157.872
2024-12-02-02:52:22-root-INFO: Regularization Change: 0.000 -> 0.193
2024-12-02-02:52:22-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-02:52:22-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:52:22-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-02:52:23-root-INFO: grad norm: 691.730 614.411 317.788
2024-12-02-02:52:24-root-INFO: grad norm: 586.665 532.162 246.941
2024-12-02-02:52:25-root-INFO: grad norm: 516.521 462.985 228.994
2024-12-02-02:52:26-root-INFO: grad norm: 456.567 415.212 189.874
2024-12-02-02:52:27-root-INFO: grad norm: 408.753 368.892 176.062
2024-12-02-02:52:27-root-INFO: Loss Change: 5147.589 -> 5014.373
2024-12-02-02:52:27-root-INFO: Regularization Change: 0.000 -> 0.249
2024-12-02-02:52:27-root-INFO: Undo step: 225
2024-12-02-02:52:27-root-INFO: Undo step: 226
2024-12-02-02:52:27-root-INFO: Undo step: 227
2024-12-02-02:52:27-root-INFO: Undo step: 228
2024-12-02-02:52:27-root-INFO: Undo step: 229
2024-12-02-02:52:28-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-02:52:28-root-INFO: grad norm: 6956.538 5748.878 3917.119
2024-12-02-02:52:29-root-INFO: grad norm: 3858.607 3388.722 1845.377
2024-12-02-02:52:30-root-INFO: grad norm: 2659.092 2345.986 1251.846
2024-12-02-02:52:31-root-INFO: grad norm: 2274.595 2001.824 1080.040
2024-12-02-02:52:32-root-INFO: grad norm: 1962.853 1743.084 902.468
2024-12-02-02:52:33-root-INFO: Loss Change: 12062.471 -> 6554.898
2024-12-02-02:52:33-root-INFO: Regularization Change: 0.000 -> 3.912
2024-12-02-02:52:33-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-02:52:33-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:52:33-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-02:52:33-root-INFO: grad norm: 1830.114 1638.058 816.139
2024-12-02-02:52:34-root-INFO: grad norm: 1528.471 1377.019 663.358
2024-12-02-02:52:35-root-INFO: grad norm: 1344.397 1196.574 612.874
2024-12-02-02:52:36-root-INFO: grad norm: 1187.073 1071.694 510.503
2024-12-02-02:52:37-root-INFO: grad norm: 1068.974 951.840 486.524
2024-12-02-02:52:38-root-INFO: Loss Change: 6538.017 -> 5986.416
2024-12-02-02:52:38-root-INFO: Regularization Change: 0.000 -> 0.804
2024-12-02-02:52:38-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-02:52:38-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:52:38-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-02:52:39-root-INFO: grad norm: 811.401 729.602 355.038
2024-12-02-02:52:40-root-INFO: grad norm: 711.865 634.106 323.514
2024-12-02-02:52:41-root-INFO: grad norm: 640.098 584.747 260.378
2024-12-02-02:52:42-root-INFO: grad norm: 584.284 521.956 262.582
2024-12-02-02:52:43-root-INFO: grad norm: 536.195 492.153 212.814
2024-12-02-02:52:43-root-INFO: Loss Change: 5908.816 -> 5687.573
2024-12-02-02:52:43-root-INFO: Regularization Change: 0.000 -> 0.391
2024-12-02-02:52:43-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-02:52:43-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:52:44-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-02:52:44-root-INFO: grad norm: 848.176 756.477 383.595
2024-12-02-02:52:45-root-INFO: grad norm: 709.809 651.540 281.644
2024-12-02-02:52:46-root-INFO: grad norm: 633.212 565.085 285.720
2024-12-02-02:52:47-root-INFO: grad norm: 570.551 523.998 225.732
2024-12-02-02:52:48-root-INFO: grad norm: 520.745 466.167 232.085
2024-12-02-02:52:49-root-INFO: Loss Change: 5722.875 -> 5500.286
2024-12-02-02:52:49-root-INFO: Regularization Change: 0.000 -> 0.392
2024-12-02-02:52:49-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-02:52:49-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-02:52:49-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-02:52:49-root-INFO: grad norm: 491.227 437.016 224.324
2024-12-02-02:52:50-root-INFO: grad norm: 412.583 372.241 177.935
2024-12-02-02:52:51-root-INFO: grad norm: 371.184 342.948 141.999
2024-12-02-02:52:52-root-INFO: grad norm: 340.648 307.521 146.532
2024-12-02-02:52:53-root-INFO: grad norm: 315.653 293.205 116.910
2024-12-02-02:52:54-root-INFO: Loss Change: 5436.952 -> 5300.376
2024-12-02-02:52:54-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-02:52:54-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-02:52:54-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:52:54-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-02:52:55-root-INFO: grad norm: 680.305 605.513 310.113
2024-12-02-02:52:56-root-INFO: grad norm: 549.266 507.104 211.041
2024-12-02-02:52:57-root-INFO: grad norm: 483.215 430.733 219.010
2024-12-02-02:52:58-root-INFO: grad norm: 428.959 396.203 164.405
2024-12-02-02:52:58-root-INFO: grad norm: 386.374 346.211 171.530
2024-12-02-02:52:59-root-INFO: Loss Change: 5291.284 -> 5124.882
2024-12-02-02:52:59-root-INFO: Regularization Change: 0.000 -> 0.313
2024-12-02-02:52:59-root-INFO: Undo step: 225
2024-12-02-02:52:59-root-INFO: Undo step: 226
2024-12-02-02:52:59-root-INFO: Undo step: 227
2024-12-02-02:52:59-root-INFO: Undo step: 228
2024-12-02-02:52:59-root-INFO: Undo step: 229
2024-12-02-02:53:00-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-02:53:00-root-INFO: grad norm: 4929.564 4203.671 2574.830
2024-12-02-02:53:01-root-INFO: grad norm: 3384.579 2999.796 1567.354
2024-12-02-02:53:02-root-INFO: grad norm: 2728.869 2346.407 1393.233
2024-12-02-02:53:03-root-INFO: grad norm: 2485.255 2213.789 1129.438
2024-12-02-02:53:04-root-INFO: grad norm: 2294.500 1996.513 1130.783
2024-12-02-02:53:05-root-INFO: Loss Change: 9581.356 -> 6568.253
2024-12-02-02:53:05-root-INFO: Regularization Change: 0.000 -> 3.388
2024-12-02-02:53:05-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-02:53:05-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:53:05-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-02:53:05-root-INFO: grad norm: 2327.092 2096.293 1010.403
2024-12-02-02:53:06-root-INFO: grad norm: 2083.944 1842.185 974.257
2024-12-02-02:53:07-root-INFO: grad norm: 1928.348 1733.013 845.692
2024-12-02-02:53:08-root-INFO: grad norm: 1782.339 1579.699 825.399
2024-12-02-02:53:09-root-INFO: grad norm: 1667.870 1498.535 732.246
2024-12-02-02:53:10-root-INFO: Loss Change: 6516.111 -> 5869.422
2024-12-02-02:53:10-root-INFO: Regularization Change: 0.000 -> 1.103
2024-12-02-02:53:10-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-02:53:10-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:53:10-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-02:53:11-root-INFO: grad norm: 1310.744 1155.624 618.533
2024-12-02-02:53:12-root-INFO: grad norm: 1190.075 1070.560 519.790
2024-12-02-02:53:13-root-INFO: grad norm: 1093.111 972.909 498.337
2024-12-02-02:53:14-root-INFO: grad norm: 1012.622 911.310 441.496
2024-12-02-02:53:15-root-INFO: grad norm: 938.494 837.800 422.920
2024-12-02-02:53:15-root-INFO: Loss Change: 5760.823 -> 5507.514
2024-12-02-02:53:15-root-INFO: Regularization Change: 0.000 -> 0.473
2024-12-02-02:53:15-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-02:53:15-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-02:53:16-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-02:53:16-root-INFO: grad norm: 1284.006 1155.978 558.916
2024-12-02-02:53:17-root-INFO: grad norm: 1131.393 1019.158 491.292
2024-12-02-02:53:18-root-INFO: grad norm: 1035.877 933.403 449.223
2024-12-02-02:53:19-root-INFO: grad norm: 949.398 855.465 411.747
2024-12-02-02:53:20-root-INFO: grad norm: 880.658 793.954 381.046
2024-12-02-02:53:21-root-INFO: Loss Change: 5553.286 -> 5325.446
2024-12-02-02:53:21-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-02:53:21-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-02:53:21-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-02:53:21-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-02:53:21-root-INFO: grad norm: 797.675 710.886 361.838
2024-12-02-02:53:22-root-INFO: grad norm: 699.036 632.505 297.638
2024-12-02-02:53:23-root-INFO: grad norm: 624.638 563.178 270.190
2024-12-02-02:53:24-root-INFO: grad norm: 565.172 510.808 241.856
2024-12-02-02:53:25-root-INFO: grad norm: 511.944 463.443 217.501
2024-12-02-02:53:26-root-INFO: Loss Change: 5283.276 -> 5141.678
2024-12-02-02:53:26-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-02:53:26-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-02:53:26-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:53:26-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-02:53:27-root-INFO: grad norm: 918.165 822.375 408.320
2024-12-02-02:53:28-root-INFO: grad norm: 782.847 713.158 322.885
2024-12-02-02:53:29-root-INFO: grad norm: 703.424 633.023 306.737
2024-12-02-02:53:30-root-INFO: grad norm: 633.165 576.774 261.209
2024-12-02-02:53:31-root-INFO: grad norm: 576.018 519.751 248.305
2024-12-02-02:53:31-root-INFO: Loss Change: 5158.732 -> 4986.743
2024-12-02-02:53:31-root-INFO: Regularization Change: 0.000 -> 0.317
2024-12-02-02:53:31-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-02:53:31-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:53:32-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-02:53:32-root-INFO: grad norm: 338.000 307.271 140.812
2024-12-02-02:53:33-root-INFO: grad norm: 281.524 255.489 118.240
2024-12-02-02:53:34-root-INFO: grad norm: 261.613 242.155 99.007
2024-12-02-02:53:35-root-INFO: grad norm: 247.790 227.207 98.877
2024-12-02-02:53:36-root-INFO: grad norm: 237.223 220.229 88.170
2024-12-02-02:53:37-root-INFO: Loss Change: 4946.969 -> 4834.445
2024-12-02-02:53:37-root-INFO: Regularization Change: 0.000 -> 0.246
2024-12-02-02:53:37-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-02:53:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:53:37-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-02:53:37-root-INFO: grad norm: 506.792 454.750 223.697
2024-12-02-02:53:38-root-INFO: grad norm: 442.074 405.316 176.488
2024-12-02-02:53:39-root-INFO: grad norm: 405.686 368.269 170.173
2024-12-02-02:53:40-root-INFO: grad norm: 374.166 343.697 147.894
2024-12-02-02:53:41-root-INFO: grad norm: 348.077 317.609 142.417
2024-12-02-02:53:42-root-INFO: Loss Change: 4839.740 -> 4718.515
2024-12-02-02:53:42-root-INFO: Regularization Change: 0.000 -> 0.274
2024-12-02-02:53:42-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-02:53:42-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:53:42-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-02:53:43-root-INFO: grad norm: 259.881 239.255 101.467
2024-12-02-02:53:44-root-INFO: grad norm: 241.764 223.294 92.680
2024-12-02-02:53:45-root-INFO: grad norm: 229.704 213.183 85.537
2024-12-02-02:53:46-root-INFO: grad norm: 220.204 204.364 82.008
2024-12-02-02:53:47-root-INFO: grad norm: 212.164 197.431 77.683
2024-12-02-02:53:47-root-INFO: Loss Change: 4638.694 -> 4551.184
2024-12-02-02:53:47-root-INFO: Regularization Change: 0.000 -> 0.214
2024-12-02-02:53:47-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-02:53:47-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:53:48-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-02:53:48-root-INFO: grad norm: 333.792 302.599 140.892
2024-12-02-02:53:49-root-INFO: grad norm: 285.716 262.907 111.863
2024-12-02-02:53:50-root-INFO: grad norm: 259.635 238.949 101.555
2024-12-02-02:53:51-root-INFO: grad norm: 238.680 220.934 90.311
2024-12-02-02:53:52-root-INFO: grad norm: 222.472 205.944 84.148
2024-12-02-02:53:53-root-INFO: Loss Change: 4517.336 -> 4425.564
2024-12-02-02:53:53-root-INFO: Regularization Change: 0.000 -> 0.226
2024-12-02-02:53:53-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-02:53:53-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:53:53-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-02:53:53-root-INFO: grad norm: 341.926 303.408 157.661
2024-12-02-02:53:54-root-INFO: grad norm: 276.599 256.024 104.685
2024-12-02-02:53:55-root-INFO: grad norm: 247.044 226.184 99.357
2024-12-02-02:53:56-root-INFO: grad norm: 225.006 209.072 83.164
2024-12-02-02:53:57-root-INFO: grad norm: 208.674 193.368 78.444
2024-12-02-02:53:58-root-INFO: Loss Change: 4413.688 -> 4303.908
2024-12-02-02:53:58-root-INFO: Regularization Change: 0.000 -> 0.283
2024-12-02-02:53:58-root-INFO: Undo step: 220
2024-12-02-02:53:58-root-INFO: Undo step: 221
2024-12-02-02:53:58-root-INFO: Undo step: 222
2024-12-02-02:53:58-root-INFO: Undo step: 223
2024-12-02-02:53:58-root-INFO: Undo step: 224
2024-12-02-02:53:58-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-02:53:59-root-INFO: grad norm: 3774.718 2864.010 2458.849
2024-12-02-02:54:00-root-INFO: grad norm: 1252.173 1082.558 629.289
2024-12-02-02:54:01-root-INFO: grad norm: 760.387 653.240 389.187
2024-12-02-02:54:02-root-INFO: grad norm: 591.543 526.143 270.364
2024-12-02-02:54:03-root-INFO: grad norm: 504.087 442.318 241.781
2024-12-02-02:54:03-root-INFO: Loss Change: 8902.826 -> 5086.845
2024-12-02-02:54:03-root-INFO: Regularization Change: 0.000 -> 4.412
2024-12-02-02:54:03-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-02:54:03-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:04-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-02:54:04-root-INFO: grad norm: 619.139 552.111 280.192
2024-12-02-02:54:05-root-INFO: grad norm: 553.568 496.782 244.223
2024-12-02-02:54:06-root-INFO: grad norm: 523.106 466.884 235.921
2024-12-02-02:54:07-root-INFO: grad norm: 500.267 452.883 212.519
2024-12-02-02:54:08-root-INFO: grad norm: 482.402 430.952 216.776
2024-12-02-02:54:09-root-INFO: Loss Change: 5025.416 -> 4808.629
2024-12-02-02:54:09-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-02-02:54:09-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-02:54:09-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:09-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-02:54:09-root-INFO: grad norm: 319.084 292.298 127.971
2024-12-02-02:54:10-root-INFO: grad norm: 246.793 213.303 124.132
2024-12-02-02:54:11-root-INFO: grad norm: 230.268 207.955 98.886
2024-12-02-02:54:12-root-INFO: grad norm: 221.021 196.986 100.232
2024-12-02-02:54:13-root-INFO: grad norm: 214.195 194.116 90.544
2024-12-02-02:54:14-root-INFO: Loss Change: 4746.857 -> 4621.707
2024-12-02-02:54:14-root-INFO: Regularization Change: 0.000 -> 0.286
2024-12-02-02:54:14-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-02:54:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:14-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-02:54:15-root-INFO: grad norm: 254.608 227.653 114.013
2024-12-02-02:54:16-root-INFO: grad norm: 244.248 222.844 99.987
2024-12-02-02:54:17-root-INFO: grad norm: 239.061 215.678 103.117
2024-12-02-02:54:18-root-INFO: grad norm: 234.745 215.407 93.301
2024-12-02-02:54:19-root-INFO: grad norm: 231.108 209.726 97.089
2024-12-02-02:54:19-root-INFO: Loss Change: 4547.984 -> 4455.886
2024-12-02-02:54:19-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-02-02:54:19-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-02:54:19-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:20-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-02:54:20-root-INFO: grad norm: 214.551 195.026 89.425
2024-12-02-02:54:21-root-INFO: grad norm: 184.992 168.308 76.775
2024-12-02-02:54:22-root-INFO: grad norm: 179.917 164.584 72.679
2024-12-02-02:54:23-root-INFO: grad norm: 176.374 163.071 67.199
2024-12-02-02:54:24-root-INFO: grad norm: 173.313 159.332 68.197
2024-12-02-02:54:24-root-INFO: Loss Change: 4407.277 -> 4326.726
2024-12-02-02:54:25-root-INFO: Regularization Change: 0.000 -> 0.206
2024-12-02-02:54:25-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-02:54:25-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:25-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-02:54:25-root-INFO: grad norm: 407.317 358.869 192.666
2024-12-02-02:54:26-root-INFO: grad norm: 367.857 339.361 141.961
2024-12-02-02:54:27-root-INFO: grad norm: 352.037 316.614 153.902
2024-12-02-02:54:28-root-INFO: grad norm: 340.419 314.204 130.998
2024-12-02-02:54:29-root-INFO: grad norm: 331.217 300.620 139.040
2024-12-02-02:54:30-root-INFO: Loss Change: 4317.993 -> 4223.189
2024-12-02-02:54:30-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-02:54:30-root-INFO: Undo step: 220
2024-12-02-02:54:30-root-INFO: Undo step: 221
2024-12-02-02:54:30-root-INFO: Undo step: 222
2024-12-02-02:54:30-root-INFO: Undo step: 223
2024-12-02-02:54:30-root-INFO: Undo step: 224
2024-12-02-02:54:30-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-02:54:30-root-INFO: grad norm: 3575.140 3163.390 1665.709
2024-12-02-02:54:31-root-INFO: grad norm: 3320.619 3058.214 1293.768
2024-12-02-02:54:32-root-INFO: grad norm: 3174.383 2879.690 1335.698
2024-12-02-02:54:33-root-INFO: grad norm: 3035.965 2773.084 1235.754
2024-12-02-02:54:34-root-INFO: grad norm: 2898.850 2633.371 1211.896
2024-12-02-02:54:35-root-INFO: Loss Change: 7885.183 -> 6323.245
2024-12-02-02:54:35-root-INFO: Regularization Change: 0.000 -> 4.364
2024-12-02-02:54:35-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-02:54:35-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:35-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-02:54:36-root-INFO: grad norm: 2961.344 2697.114 1222.757
2024-12-02-02:54:37-root-INFO: grad norm: 2899.158 2644.057 1189.150
2024-12-02-02:54:37-root-INFO: Loss too large (6048.453->6134.615)! Learning rate decreased to 0.00036.
2024-12-02-02:54:38-root-INFO: grad norm: 1885.540 1722.490 767.000
2024-12-02-02:54:39-root-INFO: grad norm: 1140.710 1030.853 488.428
2024-12-02-02:54:40-root-INFO: grad norm: 781.802 720.244 304.077
2024-12-02-02:54:41-root-INFO: Loss Change: 6371.649 -> 5015.894
2024-12-02-02:54:41-root-INFO: Regularization Change: 0.000 -> 1.209
2024-12-02-02:54:41-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-02:54:41-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:41-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-02:54:41-root-INFO: grad norm: 365.641 334.977 146.575
2024-12-02-02:54:42-root-INFO: grad norm: 314.213 288.420 124.674
2024-12-02-02:54:43-root-INFO: grad norm: 300.915 275.617 120.770
2024-12-02-02:54:44-root-INFO: grad norm: 289.857 269.557 106.564
2024-12-02-02:54:45-root-INFO: grad norm: 279.958 257.177 110.617
2024-12-02-02:54:46-root-INFO: Loss Change: 4933.846 -> 4727.768
2024-12-02-02:54:46-root-INFO: Regularization Change: 0.000 -> 0.482
2024-12-02-02:54:46-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-02:54:46-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:46-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-02:54:47-root-INFO: grad norm: 308.459 287.126 112.720
2024-12-02-02:54:48-root-INFO: grad norm: 299.165 272.351 123.794
2024-12-02-02:54:49-root-INFO: grad norm: 293.876 275.459 102.400
2024-12-02-02:54:50-root-INFO: grad norm: 289.187 264.056 117.913
2024-12-02-02:54:51-root-INFO: grad norm: 284.827 267.131 98.831
2024-12-02-02:54:51-root-INFO: Loss Change: 4651.208 -> 4507.943
2024-12-02-02:54:51-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-02:54:51-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-02:54:51-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:52-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-02:54:52-root-INFO: grad norm: 244.032 230.217 80.942
2024-12-02-02:54:53-root-INFO: grad norm: 216.474 201.332 79.538
2024-12-02-02:54:54-root-INFO: grad norm: 209.698 198.013 69.023
2024-12-02-02:54:55-root-INFO: grad norm: 204.100 191.271 71.220
2024-12-02-02:54:56-root-INFO: grad norm: 199.057 188.066 65.230
2024-12-02-02:54:57-root-INFO: Loss Change: 4454.953 -> 4344.134
2024-12-02-02:54:57-root-INFO: Regularization Change: 0.000 -> 0.285
2024-12-02-02:54:57-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-02:54:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:54:57-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-02:54:58-root-INFO: grad norm: 446.516 402.089 194.168
2024-12-02-02:54:59-root-INFO: grad norm: 388.306 356.674 153.511
2024-12-02-02:55:00-root-INFO: grad norm: 362.364 332.529 143.987
2024-12-02-02:55:01-root-INFO: grad norm: 340.259 311.696 136.462
2024-12-02-02:55:02-root-INFO: grad norm: 321.175 297.156 121.868
2024-12-02-02:55:02-root-INFO: Loss Change: 4341.300 -> 4218.054
2024-12-02-02:55:02-root-INFO: Regularization Change: 0.000 -> 0.329
2024-12-02-02:55:02-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-02:55:02-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:55:03-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-02:55:03-root-INFO: grad norm: 262.439 239.791 106.650
2024-12-02-02:55:04-root-INFO: grad norm: 239.194 223.855 84.279
2024-12-02-02:55:05-root-INFO: grad norm: 222.307 205.646 84.440
2024-12-02-02:55:06-root-INFO: grad norm: 207.967 195.662 70.475
2024-12-02-02:55:07-root-INFO: grad norm: 195.598 181.767 72.246
2024-12-02-02:55:08-root-INFO: Loss Change: 4178.520 -> 4096.381
2024-12-02-02:55:08-root-INFO: Regularization Change: 0.000 -> 0.230
2024-12-02-02:55:08-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-02:55:08-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-02:55:08-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-02:55:08-root-INFO: grad norm: 378.602 343.671 158.838
2024-12-02-02:55:09-root-INFO: grad norm: 320.688 296.876 121.267
2024-12-02-02:55:10-root-INFO: grad norm: 280.483 259.877 105.520
2024-12-02-02:55:11-root-INFO: grad norm: 248.278 229.888 93.775
2024-12-02-02:55:12-root-INFO: grad norm: 223.116 209.128 77.759
2024-12-02-02:55:13-root-INFO: Loss Change: 4082.646 -> 3991.776
2024-12-02-02:55:13-root-INFO: Regularization Change: 0.000 -> 0.249
2024-12-02-02:55:13-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-02:55:13-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:55:13-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-02:55:14-root-INFO: grad norm: 230.532 198.788 116.739
2024-12-02-02:55:15-root-INFO: grad norm: 166.039 154.967 59.616
2024-12-02-02:55:16-root-INFO: grad norm: 152.348 143.283 51.769
2024-12-02-02:55:17-root-INFO: grad norm: 144.935 137.186 46.758
2024-12-02-02:55:18-root-INFO: grad norm: 140.369 133.787 42.480
2024-12-02-02:55:18-root-INFO: Loss Change: 3979.913 -> 3901.260
2024-12-02-02:55:18-root-INFO: Regularization Change: 0.000 -> 0.233
2024-12-02-02:55:18-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-02:55:18-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:55:19-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-02:55:19-root-INFO: grad norm: 293.948 259.666 137.764
2024-12-02-02:55:20-root-INFO: grad norm: 220.734 205.508 80.561
2024-12-02-02:55:21-root-INFO: grad norm: 193.632 181.929 66.297
2024-12-02-02:55:22-root-INFO: grad norm: 175.978 164.750 61.853
2024-12-02-02:55:23-root-INFO: grad norm: 163.542 155.914 49.365
2024-12-02-02:55:23-root-INFO: Loss Change: 3890.408 -> 3806.859
2024-12-02-02:55:23-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-02-02:55:23-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-02:55:23-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:55:24-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-02:55:24-root-INFO: grad norm: 138.960 131.970 43.518
2024-12-02-02:55:25-root-INFO: grad norm: 132.897 127.221 38.425
2024-12-02-02:55:26-root-INFO: grad norm: 129.219 123.337 38.542
2024-12-02-02:55:27-root-INFO: grad norm: 126.583 121.460 35.647
2024-12-02-02:55:28-root-INFO: grad norm: 124.287 118.833 36.412
2024-12-02-02:55:29-root-INFO: Loss Change: 3785.546 -> 3730.839
2024-12-02-02:55:29-root-INFO: Regularization Change: 0.000 -> 0.188
2024-12-02-02:55:29-root-INFO: Undo step: 215
2024-12-02-02:55:29-root-INFO: Undo step: 216
2024-12-02-02:55:29-root-INFO: Undo step: 217
2024-12-02-02:55:29-root-INFO: Undo step: 218
2024-12-02-02:55:29-root-INFO: Undo step: 219
2024-12-02-02:55:29-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-02:55:29-root-INFO: grad norm: 2172.701 1711.552 1338.364
2024-12-02-02:55:30-root-INFO: grad norm: 1011.624 851.106 546.812
2024-12-02-02:55:31-root-INFO: grad norm: 773.534 697.727 333.964
2024-12-02-02:55:32-root-INFO: grad norm: 643.225 570.647 296.818
2024-12-02-02:55:33-root-INFO: grad norm: 547.345 497.069 229.148
2024-12-02-02:55:34-root-INFO: Loss Change: 6471.260 -> 4402.614
2024-12-02-02:55:34-root-INFO: Regularization Change: 0.000 -> 4.109
2024-12-02-02:55:34-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-02:55:34-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:55:34-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-02:55:35-root-INFO: grad norm: 451.524 401.009 207.523
2024-12-02-02:55:36-root-INFO: grad norm: 378.280 343.420 158.614
2024-12-02-02:55:37-root-INFO: grad norm: 321.698 290.307 138.606
2024-12-02-02:55:38-root-INFO: grad norm: 278.555 255.864 110.121
2024-12-02-02:55:39-root-INFO: grad norm: 245.607 224.577 99.438
2024-12-02-02:55:39-root-INFO: Loss Change: 4353.783 -> 4113.603
2024-12-02-02:55:39-root-INFO: Regularization Change: 0.000 -> 0.675
2024-12-02-02:55:39-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-02:55:39-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-02:55:40-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-02:55:40-root-INFO: grad norm: 387.507 351.265 163.629
2024-12-02-02:55:41-root-INFO: grad norm: 320.765 294.091 128.064
2024-12-02-02:55:42-root-INFO: grad norm: 273.602 252.739 104.789
2024-12-02-02:55:43-root-INFO: grad norm: 237.038 217.344 94.597
2024-12-02-02:55:44-root-INFO: grad norm: 210.335 196.701 74.495
2024-12-02-02:55:45-root-INFO: Loss Change: 4083.453 -> 3983.388
2024-12-02-02:55:45-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-02:55:45-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-02:55:45-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:55:45-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-02:55:45-root-INFO: grad norm: 234.876 203.018 118.110
2024-12-02-02:55:46-root-INFO: grad norm: 173.657 160.882 65.376
2024-12-02-02:55:47-root-INFO: grad norm: 157.006 147.916 52.646
2024-12-02-02:55:48-root-INFO: grad norm: 147.952 138.876 51.022
2024-12-02-02:55:49-root-INFO: grad norm: 142.268 135.579 43.108
2024-12-02-02:55:50-root-INFO: Loss Change: 3954.213 -> 3877.450
2024-12-02-02:55:50-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-02-02:55:50-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-02:55:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:55:50-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-02:55:51-root-INFO: grad norm: 293.879 258.046 140.632
2024-12-02-02:55:52-root-INFO: grad norm: 222.416 203.045 90.785
2024-12-02-02:55:53-root-INFO: grad norm: 189.729 176.788 68.870
2024-12-02-02:55:54-root-INFO: grad norm: 168.591 155.011 66.291
2024-12-02-02:55:55-root-INFO: grad norm: 154.518 146.571 48.915
2024-12-02-02:55:55-root-INFO: Loss Change: 3863.895 -> 3781.292
2024-12-02-02:55:55-root-INFO: Regularization Change: 0.000 -> 0.247
2024-12-02-02:55:55-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-02:55:55-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:55:56-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-02:55:56-root-INFO: grad norm: 130.626 123.329 43.048
2024-12-02-02:55:57-root-INFO: grad norm: 125.910 120.386 36.888
2024-12-02-02:55:58-root-INFO: grad norm: 123.630 118.058 36.695
2024-12-02-02:55:59-root-INFO: grad norm: 121.678 116.653 34.606
2024-12-02-02:56:00-root-INFO: grad norm: 120.089 114.954 34.741
2024-12-02-02:56:01-root-INFO: Loss Change: 3751.707 -> 3699.430
2024-12-02-02:56:01-root-INFO: Regularization Change: 0.000 -> 0.180
2024-12-02-02:56:01-root-INFO: Undo step: 215
2024-12-02-02:56:01-root-INFO: Undo step: 216
2024-12-02-02:56:01-root-INFO: Undo step: 217
2024-12-02-02:56:01-root-INFO: Undo step: 218
2024-12-02-02:56:01-root-INFO: Undo step: 219
2024-12-02-02:56:01-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-02:56:01-root-INFO: grad norm: 3984.832 3565.031 1780.292
2024-12-02-02:56:02-root-INFO: grad norm: 2388.380 2146.391 1047.552
2024-12-02-02:56:03-root-INFO: grad norm: 1700.261 1544.305 711.344
2024-12-02-02:56:04-root-INFO: grad norm: 1407.326 1281.928 580.711
2024-12-02-02:56:05-root-INFO: grad norm: 1140.354 1037.460 473.376
2024-12-02-02:56:06-root-INFO: Loss Change: 8444.875 -> 4588.905
2024-12-02-02:56:06-root-INFO: Regularization Change: 0.000 -> 5.821
2024-12-02-02:56:06-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-02:56:06-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-02:56:06-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-02:56:07-root-INFO: grad norm: 966.397 885.089 387.997
2024-12-02-02:56:08-root-INFO: grad norm: 768.012 700.424 315.037
2024-12-02-02:56:09-root-INFO: grad norm: 632.172 581.386 248.257
2024-12-02-02:56:10-root-INFO: grad norm: 513.649 468.826 209.852
2024-12-02-02:56:11-root-INFO: grad norm: 428.315 395.537 164.332
2024-12-02-02:56:11-root-INFO: Loss Change: 4536.548 -> 4184.113
2024-12-02-02:56:11-root-INFO: Regularization Change: 0.000 -> 0.804
2024-12-02-02:56:11-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-02:56:11-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-02:56:12-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-02:56:12-root-INFO: grad norm: 243.598 225.766 91.484
2024-12-02-02:56:13-root-INFO: grad norm: 219.436 202.581 84.338
2024-12-02-02:56:14-root-INFO: grad norm: 204.083 189.952 74.620
2024-12-02-02:56:15-root-INFO: grad norm: 192.845 179.361 70.844
2024-12-02-02:56:16-root-INFO: grad norm: 184.025 171.660 66.318
2024-12-02-02:56:17-root-INFO: Loss Change: 4131.691 -> 4013.374
2024-12-02-02:56:17-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-02-02:56:17-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-02:56:17-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:56:17-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-02:56:17-root-INFO: grad norm: 286.622 251.629 137.240
2024-12-02-02:56:18-root-INFO: grad norm: 218.667 204.847 76.504
2024-12-02-02:56:19-root-INFO: grad norm: 191.395 176.768 73.383
2024-12-02-02:56:20-root-INFO: grad norm: 174.569 164.094 59.563
2024-12-02-02:56:21-root-INFO: grad norm: 163.737 153.281 57.575
2024-12-02-02:56:22-root-INFO: Loss Change: 3989.219 -> 3889.839
2024-12-02-02:56:22-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-02:56:22-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-02:56:22-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:56:22-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-02:56:23-root-INFO: grad norm: 254.388 221.856 124.471
2024-12-02-02:56:24-root-INFO: grad norm: 198.359 184.803 72.068
2024-12-02-02:56:25-root-INFO: grad norm: 173.754 159.413 69.124
2024-12-02-02:56:26-root-INFO: grad norm: 158.778 148.740 55.559
2024-12-02-02:56:27-root-INFO: grad norm: 149.390 139.506 53.436
2024-12-02-02:56:27-root-INFO: Loss Change: 3872.166 -> 3783.746
2024-12-02-02:56:27-root-INFO: Regularization Change: 0.000 -> 0.275
2024-12-02-02:56:27-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-02:56:27-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:56:28-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-02:56:28-root-INFO: grad norm: 140.831 131.757 49.735
2024-12-02-02:56:29-root-INFO: grad norm: 135.339 128.141 43.550
2024-12-02-02:56:30-root-INFO: grad norm: 132.483 125.468 42.540
2024-12-02-02:56:31-root-INFO: grad norm: 130.339 123.816 40.719
2024-12-02-02:56:32-root-INFO: grad norm: 128.639 122.239 40.069
2024-12-02-02:56:32-root-INFO: Loss Change: 3748.739 -> 3688.742
2024-12-02-02:56:33-root-INFO: Regularization Change: 0.000 -> 0.206
2024-12-02-02:56:33-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-02:56:33-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:56:33-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-02:56:33-root-INFO: grad norm: 200.690 177.446 93.752
2024-12-02-02:56:34-root-INFO: grad norm: 154.545 146.180 50.156
2024-12-02-02:56:35-root-INFO: grad norm: 137.345 128.260 49.124
2024-12-02-02:56:36-root-INFO: grad norm: 128.895 122.695 39.495
2024-12-02-02:56:37-root-INFO: grad norm: 124.519 118.170 39.252
2024-12-02-02:56:38-root-INFO: Loss Change: 3657.387 -> 3591.763
2024-12-02-02:56:38-root-INFO: Regularization Change: 0.000 -> 0.224
2024-12-02-02:56:38-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-02:56:38-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:56:38-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-02:56:38-root-INFO: grad norm: 230.517 202.413 110.305
2024-12-02-02:56:39-root-INFO: grad norm: 165.480 154.439 59.432
2024-12-02-02:56:40-root-INFO: grad norm: 140.156 129.643 53.257
2024-12-02-02:56:41-root-INFO: grad norm: 128.224 121.568 40.774
2024-12-02-02:56:42-root-INFO: grad norm: 122.336 115.741 39.624
2024-12-02-02:56:43-root-INFO: Loss Change: 3591.696 -> 3516.954
2024-12-02-02:56:43-root-INFO: Regularization Change: 0.000 -> 0.260
2024-12-02-02:56:43-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-02:56:43-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:56:43-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-02:56:44-root-INFO: grad norm: 122.158 114.977 41.267
2024-12-02-02:56:45-root-INFO: grad norm: 118.123 112.607 35.676
2024-12-02-02:56:46-root-INFO: grad norm: 115.972 110.524 35.130
2024-12-02-02:56:47-root-INFO: grad norm: 114.463 109.443 33.525
2024-12-02-02:56:48-root-INFO: grad norm: 113.338 108.392 33.116
2024-12-02-02:56:48-root-INFO: Loss Change: 3496.441 -> 3443.327
2024-12-02-02:56:48-root-INFO: Regularization Change: 0.000 -> 0.210
2024-12-02-02:56:48-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-02:56:48-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-02:56:49-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-02:56:49-root-INFO: grad norm: 243.074 212.144 118.658
2024-12-02-02:56:50-root-INFO: grad norm: 155.724 145.499 55.498
2024-12-02-02:56:51-root-INFO: grad norm: 126.992 118.416 45.878
2024-12-02-02:56:52-root-INFO: grad norm: 116.530 110.706 36.378
2024-12-02-02:56:53-root-INFO: grad norm: 112.311 106.956 34.267
2024-12-02-02:56:54-root-INFO: Loss Change: 3420.615 -> 3345.605
2024-12-02-02:56:54-root-INFO: Regularization Change: 0.000 -> 0.275
2024-12-02-02:56:54-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-02:56:54-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:56:54-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-02:56:54-root-INFO: grad norm: 130.667 121.229 48.758
2024-12-02-02:56:55-root-INFO: grad norm: 114.087 109.085 33.410
2024-12-02-02:56:56-root-INFO: grad norm: 109.430 104.461 32.603
2024-12-02-02:56:57-root-INFO: grad norm: 107.330 102.876 30.598
2024-12-02-02:56:58-root-INFO: grad norm: 106.026 101.582 30.376
2024-12-02-02:56:59-root-INFO: Loss Change: 3328.637 -> 3276.275
2024-12-02-02:56:59-root-INFO: Regularization Change: 0.000 -> 0.223
2024-12-02-02:56:59-root-INFO: Undo step: 210
2024-12-02-02:56:59-root-INFO: Undo step: 211
2024-12-02-02:56:59-root-INFO: Undo step: 212
2024-12-02-02:56:59-root-INFO: Undo step: 213
2024-12-02-02:56:59-root-INFO: Undo step: 214
2024-12-02-02:56:59-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-02:57:00-root-INFO: grad norm: 2885.942 2442.232 1537.584
2024-12-02-02:57:01-root-INFO: grad norm: 1806.404 1652.541 729.523
2024-12-02-02:57:02-root-INFO: grad norm: 1788.974 1513.072 954.485
2024-12-02-02:57:03-root-INFO: grad norm: 1241.978 1161.567 439.626
2024-12-02-02:57:04-root-INFO: grad norm: 433.035 384.404 199.382
2024-12-02-02:57:04-root-INFO: Loss Change: 7317.004 -> 3938.977
2024-12-02-02:57:04-root-INFO: Regularization Change: 0.000 -> 7.192
2024-12-02-02:57:04-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-02:57:04-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:57:05-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-02:57:05-root-INFO: grad norm: 353.548 322.694 144.445
2024-12-02-02:57:06-root-INFO: grad norm: 243.635 226.430 89.932
2024-12-02-02:57:07-root-INFO: grad norm: 205.388 190.301 77.264
2024-12-02-02:57:08-root-INFO: grad norm: 187.102 174.414 67.728
2024-12-02-02:57:09-root-INFO: grad norm: 176.579 164.822 63.355
2024-12-02-02:57:10-root-INFO: Loss Change: 3889.018 -> 3734.147
2024-12-02-02:57:10-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-02:57:10-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-02:57:10-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:57:10-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-02:57:10-root-INFO: grad norm: 292.562 262.227 129.730
2024-12-02-02:57:11-root-INFO: grad norm: 223.373 201.447 96.511
2024-12-02-02:57:12-root-INFO: grad norm: 189.874 176.388 70.282
2024-12-02-02:57:13-root-INFO: grad norm: 171.404 157.022 68.727
2024-12-02-02:57:14-root-INFO: grad norm: 160.545 150.743 55.239
2024-12-02-02:57:15-root-INFO: Loss Change: 3727.186 -> 3610.443
2024-12-02-02:57:15-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-02-02:57:15-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-02:57:15-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:57:15-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-02:57:16-root-INFO: grad norm: 154.057 142.457 58.648
2024-12-02-02:57:16-root-INFO: grad norm: 147.554 138.358 51.275
2024-12-02-02:57:18-root-INFO: grad norm: 143.873 134.798 50.288
2024-12-02-02:57:19-root-INFO: grad norm: 141.039 132.743 47.660
2024-12-02-02:57:19-root-INFO: grad norm: 138.501 130.409 46.647
2024-12-02-02:57:20-root-INFO: Loss Change: 3585.801 -> 3504.341
2024-12-02-02:57:20-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-02:57:20-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-02:57:20-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-02:57:20-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-02:57:21-root-INFO: grad norm: 287.216 254.796 132.558
2024-12-02-02:57:22-root-INFO: grad norm: 217.196 195.247 95.146
2024-12-02-02:57:23-root-INFO: grad norm: 193.806 178.250 76.077
2024-12-02-02:57:24-root-INFO: grad norm: 186.719 169.618 78.062
2024-12-02-02:57:25-root-INFO: grad norm: 189.038 174.412 72.909
2024-12-02-02:57:25-root-INFO: Loss Change: 3477.891 -> 3385.785
2024-12-02-02:57:25-root-INFO: Regularization Change: 0.000 -> 0.362
2024-12-02-02:57:25-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-02:57:25-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:57:26-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-02:57:26-root-INFO: grad norm: 170.466 156.348 67.927
2024-12-02-02:57:27-root-INFO: grad norm: 162.192 153.012 53.791
2024-12-02-02:57:28-root-INFO: grad norm: 165.791 153.002 63.851
2024-12-02-02:57:29-root-INFO: grad norm: 176.518 164.059 65.140
2024-12-02-02:57:30-root-INFO: grad norm: 191.920 176.148 76.190
2024-12-02-02:57:31-root-INFO: Loss Change: 3366.236 -> 3302.711
2024-12-02-02:57:31-root-INFO: Regularization Change: 0.000 -> 0.302
2024-12-02-02:57:31-root-INFO: Undo step: 210
2024-12-02-02:57:31-root-INFO: Undo step: 211
2024-12-02-02:57:31-root-INFO: Undo step: 212
2024-12-02-02:57:31-root-INFO: Undo step: 213
2024-12-02-02:57:31-root-INFO: Undo step: 214
2024-12-02-02:57:31-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-02:57:31-root-INFO: grad norm: 2765.844 2297.246 1540.309
2024-12-02-02:57:32-root-INFO: grad norm: 1404.740 1323.381 471.124
2024-12-02-02:57:33-root-INFO: grad norm: 1071.021 1009.531 357.676
2024-12-02-02:57:34-root-INFO: grad norm: 943.702 876.330 350.169
2024-12-02-02:57:35-root-INFO: grad norm: 827.167 782.792 267.288
2024-12-02-02:57:36-root-INFO: Loss Change: 6731.530 -> 3963.270
2024-12-02-02:57:36-root-INFO: Regularization Change: 0.000 -> 5.882
2024-12-02-02:57:36-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-02:57:36-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:57:36-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-02:57:37-root-INFO: grad norm: 830.513 770.733 309.388
2024-12-02-02:57:38-root-INFO: grad norm: 717.979 681.993 224.452
2024-12-02-02:57:39-root-INFO: grad norm: 636.203 591.289 234.801
2024-12-02-02:57:40-root-INFO: grad norm: 557.212 528.326 177.081
2024-12-02-02:57:41-root-INFO: grad norm: 494.464 458.962 183.982
2024-12-02-02:57:41-root-INFO: Loss Change: 3939.420 -> 3686.045
2024-12-02-02:57:41-root-INFO: Regularization Change: 0.000 -> 0.788
2024-12-02-02:57:41-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-02:57:41-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:57:42-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-02:57:42-root-INFO: grad norm: 290.542 274.243 95.946
2024-12-02-02:57:43-root-INFO: grad norm: 247.575 227.304 98.116
2024-12-02-02:57:44-root-INFO: grad norm: 224.645 212.718 72.224
2024-12-02-02:57:45-root-INFO: grad norm: 207.045 191.900 77.729
2024-12-02-02:57:46-root-INFO: grad norm: 192.282 182.674 60.020
2024-12-02-02:57:47-root-INFO: Loss Change: 3642.109 -> 3531.171
2024-12-02-02:57:47-root-INFO: Regularization Change: 0.000 -> 0.407
2024-12-02-02:57:47-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-02:57:47-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-02:57:47-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-02:57:47-root-INFO: grad norm: 202.826 187.607 77.085
2024-12-02-02:57:48-root-INFO: grad norm: 182.808 174.375 54.886
2024-12-02-02:57:49-root-INFO: grad norm: 168.197 157.012 60.312
2024-12-02-02:57:50-root-INFO: grad norm: 156.457 149.572 45.900
2024-12-02-02:57:51-root-INFO: grad norm: 147.336 138.413 50.493
2024-12-02-02:57:52-root-INFO: Loss Change: 3510.601 -> 3433.673
2024-12-02-02:57:52-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-02-02:57:52-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-02:57:52-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-02:57:52-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-02:57:53-root-INFO: grad norm: 243.838 212.529 119.536
2024-12-02-02:57:54-root-INFO: grad norm: 188.412 176.291 66.486
2024-12-02-02:57:55-root-INFO: grad norm: 164.351 151.075 64.712
2024-12-02-02:57:56-root-INFO: grad norm: 148.682 141.268 46.364
2024-12-02-02:57:57-root-INFO: grad norm: 137.669 128.952 48.209
2024-12-02-02:57:57-root-INFO: Loss Change: 3406.140 -> 3322.847
2024-12-02-02:57:57-root-INFO: Regularization Change: 0.000 -> 0.322
2024-12-02-02:57:57-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-02:57:57-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:57:58-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-02:57:58-root-INFO: grad norm: 131.711 122.019 49.589
2024-12-02-02:57:59-root-INFO: grad norm: 121.626 116.250 35.759
2024-12-02-02:58:00-root-INFO: grad norm: 117.578 111.654 36.851
2024-12-02-02:58:01-root-INFO: grad norm: 114.604 109.952 32.321
2024-12-02-02:58:02-root-INFO: grad norm: 112.229 107.089 33.573
2024-12-02-02:58:02-root-INFO: Loss Change: 3304.331 -> 3246.885
2024-12-02-02:58:02-root-INFO: Regularization Change: 0.000 -> 0.248
2024-12-02-02:58:02-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-02:58:02-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:03-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-02:58:03-root-INFO: grad norm: 184.737 163.583 85.840
2024-12-02-02:58:04-root-INFO: grad norm: 149.828 139.404 54.909
2024-12-02-02:58:05-root-INFO: grad norm: 133.893 123.670 51.314
2024-12-02-02:58:06-root-INFO: grad norm: 123.726 117.275 39.430
2024-12-02-02:58:07-root-INFO: grad norm: 116.784 109.846 39.653
2024-12-02-02:58:08-root-INFO: Loss Change: 3224.690 -> 3162.190
2024-12-02-02:58:08-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-02:58:08-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-02:58:08-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:08-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-02:58:09-root-INFO: grad norm: 170.554 150.495 80.248
2024-12-02-02:58:10-root-INFO: grad norm: 143.009 132.588 53.590
2024-12-02-02:58:11-root-INFO: grad norm: 130.331 119.613 51.757
2024-12-02-02:58:12-root-INFO: grad norm: 122.363 115.353 40.822
2024-12-02-02:58:13-root-INFO: grad norm: 116.747 108.687 42.626
2024-12-02-02:58:13-root-INFO: Loss Change: 3150.418 -> 3090.093
2024-12-02-02:58:13-root-INFO: Regularization Change: 0.000 -> 0.277
2024-12-02-02:58:13-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-02:58:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:14-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-02:58:14-root-INFO: grad norm: 129.679 118.081 53.606
2024-12-02-02:58:15-root-INFO: grad norm: 112.589 106.820 35.577
2024-12-02-02:58:16-root-INFO: grad norm: 108.490 101.545 38.192
2024-12-02-02:58:17-root-INFO: grad norm: 106.585 101.561 32.338
2024-12-02-02:58:18-root-INFO: grad norm: 105.449 98.880 36.635
2024-12-02-02:58:19-root-INFO: Loss Change: 3061.868 -> 3010.139
2024-12-02-02:58:19-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-02-02:58:19-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-02:58:19-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:19-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-02:58:19-root-INFO: grad norm: 130.571 117.880 56.153
2024-12-02-02:58:20-root-INFO: grad norm: 116.631 109.674 39.677
2024-12-02-02:58:21-root-INFO: grad norm: 116.055 106.320 46.526
2024-12-02-02:58:22-root-INFO: grad norm: 119.448 111.799 42.058
2024-12-02-02:58:23-root-INFO: grad norm: 125.851 114.209 52.865
2024-12-02-02:58:24-root-INFO: Loss Change: 2985.956 -> 2938.337
2024-12-02-02:58:24-root-INFO: Regularization Change: 0.000 -> 0.258
2024-12-02-02:58:24-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-02:58:24-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:24-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-02:58:25-root-INFO: grad norm: 124.405 113.499 50.936
2024-12-02-02:58:26-root-INFO: grad norm: 100.951 95.350 33.157
2024-12-02-02:58:27-root-INFO: grad norm: 96.293 91.739 29.262
2024-12-02-02:58:28-root-INFO: grad norm: 96.435 90.984 31.964
2024-12-02-02:58:29-root-INFO: grad norm: 99.819 94.980 30.703
2024-12-02-02:58:29-root-INFO: Loss Change: 2915.931 -> 2866.631
2024-12-02-02:58:30-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-02:58:30-root-INFO: Undo step: 205
2024-12-02-02:58:30-root-INFO: Undo step: 206
2024-12-02-02:58:30-root-INFO: Undo step: 207
2024-12-02-02:58:30-root-INFO: Undo step: 208
2024-12-02-02:58:30-root-INFO: Undo step: 209
2024-12-02-02:58:30-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-02:58:30-root-INFO: grad norm: 1513.173 1348.965 685.557
2024-12-02-02:58:31-root-INFO: grad norm: 832.871 739.737 382.705
2024-12-02-02:58:32-root-INFO: grad norm: 493.650 450.907 200.932
2024-12-02-02:58:33-root-INFO: grad norm: 358.133 317.297 166.077
2024-12-02-02:58:34-root-INFO: grad norm: 283.854 260.325 113.155
2024-12-02-02:58:35-root-INFO: Loss Change: 5179.937 -> 3435.192
2024-12-02-02:58:35-root-INFO: Regularization Change: 0.000 -> 5.447
2024-12-02-02:58:35-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-02:58:35-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:35-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-02:58:36-root-INFO: grad norm: 270.223 245.336 113.274
2024-12-02-02:58:36-root-INFO: grad norm: 211.731 197.487 76.348
2024-12-02-02:58:37-root-INFO: grad norm: 190.298 176.841 70.290
2024-12-02-02:58:38-root-INFO: grad norm: 176.906 165.384 62.800
2024-12-02-02:58:39-root-INFO: grad norm: 166.838 156.099 58.891
2024-12-02-02:58:40-root-INFO: Loss Change: 3393.408 -> 3225.035
2024-12-02-02:58:40-root-INFO: Regularization Change: 0.000 -> 0.746
2024-12-02-02:58:40-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-02:58:40-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:40-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-02:58:41-root-INFO: grad norm: 207.772 189.565 85.055
2024-12-02-02:58:42-root-INFO: grad norm: 164.742 153.524 59.751
2024-12-02-02:58:43-root-INFO: grad norm: 149.987 140.940 51.305
2024-12-02-02:58:44-root-INFO: grad norm: 140.960 132.310 48.621
2024-12-02-02:58:45-root-INFO: grad norm: 133.981 126.043 45.432
2024-12-02-02:58:45-root-INFO: Loss Change: 3210.487 -> 3103.036
2024-12-02-02:58:45-root-INFO: Regularization Change: 0.000 -> 0.497
2024-12-02-02:58:45-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-02:58:45-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:46-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-02:58:46-root-INFO: grad norm: 155.645 143.088 61.247
2024-12-02-02:58:47-root-INFO: grad norm: 129.825 120.476 48.375
2024-12-02-02:58:48-root-INFO: grad norm: 121.523 113.606 43.144
2024-12-02-02:58:49-root-INFO: grad norm: 115.739 107.759 42.230
2024-12-02-02:58:50-root-INFO: grad norm: 111.125 103.728 39.866
2024-12-02-02:58:51-root-INFO: Loss Change: 3071.192 -> 2996.103
2024-12-02-02:58:51-root-INFO: Regularization Change: 0.000 -> 0.371
2024-12-02-02:58:51-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-02:58:51-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:51-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-02:58:51-root-INFO: grad norm: 139.050 125.215 60.466
2024-12-02-02:58:52-root-INFO: grad norm: 112.295 102.214 46.503
2024-12-02-02:58:53-root-INFO: grad norm: 104.944 97.338 39.226
2024-12-02-02:58:54-root-INFO: grad norm: 100.957 93.162 38.902
2024-12-02-02:58:55-root-INFO: grad norm: 98.157 91.444 35.676
2024-12-02-02:58:56-root-INFO: Loss Change: 2971.589 -> 2914.686
2024-12-02-02:58:56-root-INFO: Regularization Change: 0.000 -> 0.290
2024-12-02-02:58:56-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-02:58:56-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:58:56-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-02:58:57-root-INFO: grad norm: 138.252 122.615 63.867
2024-12-02-02:58:58-root-INFO: grad norm: 108.410 98.153 46.031
2024-12-02-02:58:59-root-INFO: grad norm: 101.348 94.011 37.858
2024-12-02-02:59:00-root-INFO: grad norm: 97.751 90.122 37.859
2024-12-02-02:59:01-root-INFO: grad norm: 95.373 89.066 34.109
2024-12-02-02:59:02-root-INFO: Loss Change: 2896.130 -> 2842.352
2024-12-02-02:59:02-root-INFO: Regularization Change: 0.000 -> 0.284
2024-12-02-02:59:02-root-INFO: Undo step: 205
2024-12-02-02:59:02-root-INFO: Undo step: 206
2024-12-02-02:59:02-root-INFO: Undo step: 207
2024-12-02-02:59:02-root-INFO: Undo step: 208
2024-12-02-02:59:02-root-INFO: Undo step: 209
2024-12-02-02:59:02-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-02:59:02-root-INFO: grad norm: 2915.403 2560.064 1394.863
2024-12-02-02:59:03-root-INFO: grad norm: 1894.585 1818.918 530.085
2024-12-02-02:59:04-root-INFO: grad norm: 1484.816 1436.516 375.633
2024-12-02-02:59:05-root-INFO: grad norm: 1106.887 1067.108 294.074
2024-12-02-02:59:06-root-INFO: grad norm: 669.026 625.178 238.219
2024-12-02-02:59:07-root-INFO: Loss Change: 6824.653 -> 3366.050
2024-12-02-02:59:07-root-INFO: Regularization Change: 0.000 -> 8.774
2024-12-02-02:59:07-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-02:59:07-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:59:07-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-02:59:07-root-INFO: grad norm: 497.501 465.639 175.179
2024-12-02-02:59:08-root-INFO: grad norm: 313.790 291.397 116.415
2024-12-02-02:59:10-root-INFO: grad norm: 229.685 214.568 81.950
2024-12-02-02:59:10-root-INFO: grad norm: 186.082 171.870 71.325
2024-12-02-02:59:11-root-INFO: grad norm: 163.098 150.968 61.722
2024-12-02-02:59:12-root-INFO: Loss Change: 3335.633 -> 3109.814
2024-12-02-02:59:12-root-INFO: Regularization Change: 0.000 -> 0.849
2024-12-02-02:59:12-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-02:59:12-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:59:12-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-02:59:13-root-INFO: grad norm: 209.879 184.935 99.237
2024-12-02-02:59:14-root-INFO: grad norm: 143.899 133.781 53.007
2024-12-02-02:59:15-root-INFO: grad norm: 131.459 121.335 50.589
2024-12-02-02:59:16-root-INFO: grad norm: 124.733 115.792 46.375
2024-12-02-02:59:17-root-INFO: grad norm: 119.937 111.432 44.360
2024-12-02-02:59:17-root-INFO: Loss Change: 3089.688 -> 2998.644
2024-12-02-02:59:17-root-INFO: Regularization Change: 0.000 -> 0.408
2024-12-02-02:59:17-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-02:59:17-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:59:18-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-02:59:18-root-INFO: grad norm: 152.848 136.732 68.315
2024-12-02-02:59:19-root-INFO: grad norm: 113.511 107.061 37.720
2024-12-02-02:59:20-root-INFO: grad norm: 107.147 100.449 37.287
2024-12-02-02:59:21-root-INFO: grad norm: 104.197 98.226 34.765
2024-12-02-02:59:22-root-INFO: grad norm: 102.027 96.168 34.079
2024-12-02-02:59:23-root-INFO: Loss Change: 2961.032 -> 2899.559
2024-12-02-02:59:23-root-INFO: Regularization Change: 0.000 -> 0.295
2024-12-02-02:59:23-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-02:59:23-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:59:23-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-02:59:23-root-INFO: grad norm: 156.498 139.323 71.279
2024-12-02-02:59:24-root-INFO: grad norm: 108.005 101.505 36.903
2024-12-02-02:59:25-root-INFO: grad norm: 99.171 92.759 35.082
2024-12-02-02:59:26-root-INFO: grad norm: 96.359 90.870 32.058
2024-12-02-02:59:27-root-INFO: grad norm: 94.649 89.286 31.407
2024-12-02-02:59:28-root-INFO: Loss Change: 2880.690 -> 2823.745
2024-12-02-02:59:28-root-INFO: Regularization Change: 0.000 -> 0.279
2024-12-02-02:59:28-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-02:59:28-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-02:59:28-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-02:59:28-root-INFO: grad norm: 163.598 144.441 76.818
2024-12-02-02:59:29-root-INFO: grad norm: 107.565 101.032 36.913
2024-12-02-02:59:30-root-INFO: grad norm: 95.140 89.058 33.469
2024-12-02-02:59:31-root-INFO: grad norm: 91.314 86.419 29.495
2024-12-02-02:59:32-root-INFO: grad norm: 89.516 84.710 28.936
2024-12-02-02:59:33-root-INFO: Loss Change: 2808.405 -> 2753.258
2024-12-02-02:59:33-root-INFO: Regularization Change: 0.000 -> 0.276
2024-12-02-02:59:33-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-02:59:33-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-02:59:34-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-02:59:34-root-INFO: grad norm: 190.387 169.140 87.400
2024-12-02-02:59:35-root-INFO: grad norm: 123.471 115.061 44.788
2024-12-02-02:59:36-root-INFO: grad norm: 101.842 94.913 36.925
2024-12-02-02:59:37-root-INFO: grad norm: 92.888 87.650 30.752
2024-12-02-02:59:38-root-INFO: grad norm: 88.609 83.722 29.022
2024-12-02-02:59:38-root-INFO: Loss Change: 2742.927 -> 2683.508
2024-12-02-02:59:38-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-02-02:59:38-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-02:59:38-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-02:59:39-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-02:59:39-root-INFO: grad norm: 100.213 93.016 37.291
2024-12-02-02:59:40-root-INFO: grad norm: 88.886 84.368 27.977
2024-12-02-02:59:41-root-INFO: grad norm: 85.746 81.344 27.123
2024-12-02-02:59:42-root-INFO: grad norm: 83.947 79.928 25.661
2024-12-02-02:59:43-root-INFO: grad norm: 82.719 78.685 25.516
2024-12-02-02:59:44-root-INFO: Loss Change: 2669.124 -> 2625.912
2024-12-02-02:59:44-root-INFO: Regularization Change: 0.000 -> 0.255
2024-12-02-02:59:44-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-02:59:44-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-02:59:44-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-02:59:44-root-INFO: grad norm: 271.678 246.132 115.012
2024-12-02-02:59:45-root-INFO: grad norm: 191.129 180.198 63.712
2024-12-02-02:59:46-root-INFO: grad norm: 171.012 159.761 61.006
2024-12-02-02:59:47-root-INFO: grad norm: 157.120 148.640 50.921
2024-12-02-02:59:48-root-INFO: grad norm: 146.582 137.935 49.600
2024-12-02-02:59:49-root-INFO: Loss Change: 2636.726 -> 2561.324
2024-12-02-02:59:49-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-02:59:49-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-02:59:49-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-02:59:49-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-02:59:50-root-INFO: grad norm: 134.320 128.051 40.554
2024-12-02-02:59:51-root-INFO: grad norm: 126.407 120.236 39.013
2024-12-02-02:59:52-root-INFO: grad norm: 124.008 117.742 38.923
2024-12-02-02:59:53-root-INFO: grad norm: 123.522 117.099 39.314
2024-12-02-02:59:54-root-INFO: grad norm: 124.545 118.367 38.740
2024-12-02-02:59:54-root-INFO: Loss Change: 2552.218 -> 2509.052
2024-12-02-02:59:54-root-INFO: Regularization Change: 0.000 -> 0.296
2024-12-02-02:59:54-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-02:59:54-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-02:59:55-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-02:59:55-root-INFO: grad norm: 310.300 287.833 115.924
2024-12-02-02:59:56-root-INFO: grad norm: 323.499 308.053 98.766
2024-12-02-02:59:57-root-INFO: grad norm: 355.625 335.975 116.575
2024-12-02-02:59:58-root-INFO: grad norm: 384.831 366.533 117.252
2024-12-02-02:59:59-root-INFO: grad norm: 403.647 381.584 131.624
2024-12-02-03:00:00-root-INFO: Loss Change: 2517.925 -> 2499.469
2024-12-02-03:00:00-root-INFO: Regularization Change: 0.000 -> 0.530
2024-12-02-03:00:00-root-INFO: Undo step: 200
2024-12-02-03:00:00-root-INFO: Undo step: 201
2024-12-02-03:00:00-root-INFO: Undo step: 202
2024-12-02-03:00:00-root-INFO: Undo step: 203
2024-12-02-03:00:00-root-INFO: Undo step: 204
2024-12-02-03:00:00-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-03:00:01-root-INFO: grad norm: 2060.514 1956.275 647.076
2024-12-02-03:00:01-root-INFO: grad norm: 1058.091 1017.359 290.750
2024-12-02-03:00:02-root-INFO: grad norm: 524.412 500.867 155.371
2024-12-02-03:00:03-root-INFO: grad norm: 314.226 298.112 99.332
2024-12-02-03:00:04-root-INFO: grad norm: 220.832 206.019 79.518
2024-12-02-03:00:05-root-INFO: Loss Change: 5217.617 -> 2911.582
2024-12-02-03:00:05-root-INFO: Regularization Change: 0.000 -> 6.487
2024-12-02-03:00:05-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-03:00:05-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-03:00:05-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-03:00:06-root-INFO: grad norm: 237.502 217.349 95.743
2024-12-02-03:00:07-root-INFO: grad norm: 166.419 156.022 57.900
2024-12-02-03:00:08-root-INFO: grad norm: 149.939 139.689 54.486
2024-12-02-03:00:09-root-INFO: grad norm: 140.403 130.699 51.292
2024-12-02-03:00:10-root-INFO: grad norm: 133.403 123.997 49.205
2024-12-02-03:00:10-root-INFO: Loss Change: 2891.545 -> 2750.486
2024-12-02-03:00:10-root-INFO: Regularization Change: 0.000 -> 0.774
2024-12-02-03:00:10-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-03:00:10-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:00:11-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-03:00:11-root-INFO: grad norm: 138.118 126.893 54.542
2024-12-02-03:00:12-root-INFO: grad norm: 123.926 115.147 45.813
2024-12-02-03:00:13-root-INFO: grad norm: 118.448 109.761 44.525
2024-12-02-03:00:14-root-INFO: grad norm: 114.344 106.013 42.845
2024-12-02-03:00:15-root-INFO: grad norm: 110.819 102.736 41.548
2024-12-02-03:00:16-root-INFO: Loss Change: 2735.634 -> 2650.866
2024-12-02-03:00:16-root-INFO: Regularization Change: 0.000 -> 0.509
2024-12-02-03:00:16-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-03:00:16-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:00:16-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-03:00:16-root-INFO: grad norm: 287.574 261.301 120.085
2024-12-02-03:00:17-root-INFO: grad norm: 153.258 147.084 43.061
2024-12-02-03:00:18-root-INFO: grad norm: 120.645 113.043 42.149
2024-12-02-03:00:19-root-INFO: grad norm: 108.097 102.102 35.499
2024-12-02-03:00:20-root-INFO: grad norm: 101.630 95.037 36.007
2024-12-02-03:00:21-root-INFO: Loss Change: 2654.442 -> 2551.782
2024-12-02-03:00:21-root-INFO: Regularization Change: 0.000 -> 0.518
2024-12-02-03:00:21-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-03:00:21-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:00:21-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-03:00:21-root-INFO: grad norm: 101.761 94.647 37.378
2024-12-02-03:00:22-root-INFO: grad norm: 94.545 88.800 32.455
2024-12-02-03:00:23-root-INFO: grad norm: 91.853 86.281 31.507
2024-12-02-03:00:25-root-INFO: grad norm: 89.804 84.373 30.757
2024-12-02-03:00:25-root-INFO: grad norm: 88.023 82.733 30.053
2024-12-02-03:00:26-root-INFO: Loss Change: 2537.184 -> 2481.595
2024-12-02-03:00:26-root-INFO: Regularization Change: 0.000 -> 0.365
2024-12-02-03:00:26-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-03:00:26-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:00:26-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-03:00:27-root-INFO: grad norm: 288.715 270.731 100.304
2024-12-02-03:00:28-root-INFO: grad norm: 217.665 211.888 49.814
2024-12-02-03:00:29-root-INFO: grad norm: 170.115 165.055 41.181
2024-12-02-03:00:30-root-INFO: grad norm: 172.878 168.698 37.783
2024-12-02-03:00:31-root-INFO: grad norm: 185.924 180.311 45.340
2024-12-02-03:00:31-root-INFO: Loss Change: 2479.261 -> 2419.574
2024-12-02-03:00:31-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-03:00:31-root-INFO: Undo step: 200
2024-12-02-03:00:31-root-INFO: Undo step: 201
2024-12-02-03:00:31-root-INFO: Undo step: 202
2024-12-02-03:00:31-root-INFO: Undo step: 203
2024-12-02-03:00:31-root-INFO: Undo step: 204
2024-12-02-03:00:32-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-03:00:32-root-INFO: grad norm: 2080.722 1935.430 763.883
2024-12-02-03:00:33-root-INFO: grad norm: 1171.837 1114.563 361.872
2024-12-02-03:00:34-root-INFO: grad norm: 670.478 646.690 177.011
2024-12-02-03:00:35-root-INFO: grad norm: 349.285 316.274 148.224
2024-12-02-03:00:36-root-INFO: grad norm: 264.299 250.257 85.000
2024-12-02-03:00:37-root-INFO: Loss Change: 5234.308 -> 2957.697
2024-12-02-03:00:37-root-INFO: Regularization Change: 0.000 -> 7.252
2024-12-02-03:00:37-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-03:00:37-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-03:00:37-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-03:00:37-root-INFO: grad norm: 237.433 219.922 89.490
2024-12-02-03:00:38-root-INFO: grad norm: 196.968 182.204 74.820
2024-12-02-03:00:39-root-INFO: grad norm: 171.471 157.961 66.714
2024-12-02-03:00:40-root-INFO: grad norm: 152.658 140.041 60.770
2024-12-02-03:00:41-root-INFO: grad norm: 141.143 128.919 57.457
2024-12-02-03:00:42-root-INFO: Loss Change: 2926.936 -> 2751.997
2024-12-02-03:00:42-root-INFO: Regularization Change: 0.000 -> 0.971
2024-12-02-03:00:42-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-03:00:42-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:00:42-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-03:00:43-root-INFO: grad norm: 147.770 135.769 58.333
2024-12-02-03:00:44-root-INFO: grad norm: 132.880 121.307 54.238
2024-12-02-03:00:45-root-INFO: grad norm: 124.961 115.489 47.723
2024-12-02-03:00:46-root-INFO: grad norm: 119.528 110.149 46.412
2024-12-02-03:00:47-root-INFO: grad norm: 115.256 106.957 42.944
2024-12-02-03:00:47-root-INFO: Loss Change: 2733.715 -> 2641.889
2024-12-02-03:00:47-root-INFO: Regularization Change: 0.000 -> 0.550
2024-12-02-03:00:47-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-03:00:47-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:00:48-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-03:00:48-root-INFO: grad norm: 250.642 222.002 116.348
2024-12-02-03:00:49-root-INFO: grad norm: 140.599 132.785 46.219
2024-12-02-03:00:50-root-INFO: grad norm: 121.052 113.541 41.978
2024-12-02-03:00:51-root-INFO: grad norm: 112.456 104.896 40.535
2024-12-02-03:00:52-root-INFO: grad norm: 107.107 100.445 37.183
2024-12-02-03:00:52-root-INFO: Loss Change: 2639.798 -> 2538.369
2024-12-02-03:00:52-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-02-03:00:52-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-03:00:52-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:00:53-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-03:00:53-root-INFO: grad norm: 103.689 96.239 38.593
2024-12-02-03:00:54-root-INFO: grad norm: 97.290 91.205 33.868
2024-12-02-03:00:55-root-INFO: grad norm: 94.501 88.506 33.122
2024-12-02-03:00:56-root-INFO: grad norm: 92.183 86.468 31.954
2024-12-02-03:00:57-root-INFO: grad norm: 90.136 84.570 31.184
2024-12-02-03:00:58-root-INFO: Loss Change: 2519.985 -> 2461.574
2024-12-02-03:00:58-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-03:00:58-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-03:00:58-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:00:58-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-03:00:58-root-INFO: grad norm: 331.294 310.909 114.417
2024-12-02-03:00:59-root-INFO: grad norm: 231.028 223.667 57.852
2024-12-02-03:01:00-root-INFO: grad norm: 178.189 174.384 36.626
2024-12-02-03:01:01-root-INFO: grad norm: 186.312 181.290 42.966
2024-12-02-03:01:02-root-INFO: grad norm: 205.434 200.541 44.567
2024-12-02-03:01:03-root-INFO: Loss Change: 2460.132 -> 2400.465
2024-12-02-03:01:03-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-02-03:01:03-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-03:01:03-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:01:03-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-03:01:04-root-INFO: grad norm: 231.964 224.896 56.822
2024-12-02-03:01:05-root-INFO: grad norm: 242.842 238.753 44.377
2024-12-02-03:01:06-root-INFO: grad norm: 278.304 272.400 57.017
2024-12-02-03:01:06-root-INFO: Loss too large (2366.586->2375.941)! Learning rate decreased to 0.00115.
2024-12-02-03:01:07-root-INFO: grad norm: 227.961 224.114 41.703
2024-12-02-03:01:08-root-INFO: grad norm: 179.134 174.290 41.378
2024-12-02-03:01:09-root-INFO: Loss Change: 2379.809 -> 2333.784
2024-12-02-03:01:09-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-02-03:01:09-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-03:01:09-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-03:01:09-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-03:01:09-root-INFO: grad norm: 227.966 212.444 82.679
2024-12-02-03:01:10-root-INFO: Loss too large (2326.949->2327.830)! Learning rate decreased to 0.00120.
2024-12-02-03:01:11-root-INFO: grad norm: 221.106 215.834 47.997
2024-12-02-03:01:12-root-INFO: grad norm: 285.790 275.550 75.815
2024-12-02-03:01:12-root-INFO: Loss too large (2311.268->2321.815)! Learning rate decreased to 0.00096.
2024-12-02-03:01:13-root-INFO: grad norm: 219.095 213.235 50.335
2024-12-02-03:01:14-root-INFO: grad norm: 138.516 132.806 39.361
2024-12-02-03:01:15-root-INFO: Loss Change: 2326.949 -> 2283.111
2024-12-02-03:01:15-root-INFO: Regularization Change: 0.000 -> 0.204
2024-12-02-03:01:15-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-03:01:15-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:01:15-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-03:01:15-root-INFO: grad norm: 115.634 106.906 44.072
2024-12-02-03:01:16-root-INFO: grad norm: 172.084 167.119 41.038
2024-12-02-03:01:17-root-INFO: Loss too large (2263.168->2286.657)! Learning rate decreased to 0.00125.
2024-12-02-03:01:17-root-INFO: Loss too large (2263.168->2267.180)! Learning rate decreased to 0.00100.
2024-12-02-03:01:18-root-INFO: grad norm: 225.094 218.299 54.889
2024-12-02-03:01:18-root-INFO: Loss too large (2257.618->2263.484)! Learning rate decreased to 0.00080.
2024-12-02-03:01:19-root-INFO: grad norm: 185.071 180.300 41.752
2024-12-02-03:01:20-root-INFO: grad norm: 125.613 120.965 33.852
2024-12-02-03:01:21-root-INFO: Loss Change: 2267.458 -> 2241.068
2024-12-02-03:01:21-root-INFO: Regularization Change: 0.000 -> 0.141
2024-12-02-03:01:21-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-03:01:21-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:01:21-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-03:01:22-root-INFO: grad norm: 161.112 147.870 63.964
2024-12-02-03:01:22-root-INFO: Loss too large (2231.108->2232.645)! Learning rate decreased to 0.00131.
2024-12-02-03:01:23-root-INFO: grad norm: 208.009 202.987 45.430
2024-12-02-03:01:23-root-INFO: Loss too large (2226.807->2241.502)! Learning rate decreased to 0.00105.
2024-12-02-03:01:24-root-INFO: grad norm: 316.160 306.517 77.489
2024-12-02-03:01:25-root-INFO: Loss too large (2223.315->2242.202)! Learning rate decreased to 0.00084.
2024-12-02-03:01:25-root-INFO: Loss too large (2223.315->2228.357)! Learning rate decreased to 0.00067.
2024-12-02-03:01:26-root-INFO: grad norm: 204.421 199.833 43.068
2024-12-02-03:01:27-root-INFO: grad norm: 85.875 81.232 27.854
2024-12-02-03:01:28-root-INFO: Loss Change: 2231.108 -> 2199.347
2024-12-02-03:01:28-root-INFO: Regularization Change: 0.000 -> 0.127
2024-12-02-03:01:28-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-03:01:28-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:01:28-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-03:01:28-root-INFO: grad norm: 134.640 128.078 41.521
2024-12-02-03:01:28-root-INFO: Loss too large (2188.718->2200.177)! Learning rate decreased to 0.00137.
2024-12-02-03:01:29-root-INFO: Loss too large (2188.718->2193.105)! Learning rate decreased to 0.00109.
2024-12-02-03:01:30-root-INFO: grad norm: 188.564 184.249 40.109
2024-12-02-03:01:30-root-INFO: Loss too large (2188.619->2194.990)! Learning rate decreased to 0.00087.
2024-12-02-03:01:31-root-INFO: grad norm: 245.716 239.138 56.474
2024-12-02-03:01:31-root-INFO: Loss too large (2184.367->2191.337)! Learning rate decreased to 0.00070.
2024-12-02-03:01:32-root-INFO: grad norm: 201.393 197.023 41.727
2024-12-02-03:01:33-root-INFO: grad norm: 138.652 134.282 34.538
2024-12-02-03:01:34-root-INFO: Loss Change: 2188.718 -> 2169.750
2024-12-02-03:01:34-root-INFO: Regularization Change: 0.000 -> 0.086
2024-12-02-03:01:34-root-INFO: Undo step: 195
2024-12-02-03:01:34-root-INFO: Undo step: 196
2024-12-02-03:01:34-root-INFO: Undo step: 197
2024-12-02-03:01:34-root-INFO: Undo step: 198
2024-12-02-03:01:34-root-INFO: Undo step: 199
2024-12-02-03:01:34-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-03:01:35-root-INFO: grad norm: 1213.755 1068.198 576.328
2024-12-02-03:01:36-root-INFO: grad norm: 868.643 842.312 212.254
2024-12-02-03:01:37-root-INFO: grad norm: 850.486 830.999 181.015
2024-12-02-03:01:38-root-INFO: grad norm: 829.514 813.377 162.823
2024-12-02-03:01:38-root-INFO: Loss too large (2850.686->2851.641)! Learning rate decreased to 0.00110.
2024-12-02-03:01:39-root-INFO: grad norm: 536.426 523.443 117.301
2024-12-02-03:01:40-root-INFO: Loss Change: 3982.850 -> 2600.868
2024-12-02-03:01:40-root-INFO: Regularization Change: 0.000 -> 6.543
2024-12-02-03:01:40-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-03:01:40-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:01:40-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-03:01:40-root-INFO: grad norm: 238.147 229.441 63.804
2024-12-02-03:01:41-root-INFO: grad norm: 224.792 217.921 55.153
2024-12-02-03:01:42-root-INFO: grad norm: 242.764 235.404 59.326
2024-12-02-03:01:43-root-INFO: grad norm: 279.671 272.472 63.049
2024-12-02-03:01:44-root-INFO: grad norm: 360.799 352.955 74.822
2024-12-02-03:01:45-root-INFO: Loss too large (2463.020->2477.960)! Learning rate decreased to 0.00115.
2024-12-02-03:01:45-root-INFO: Loss Change: 2565.615 -> 2445.960
2024-12-02-03:01:45-root-INFO: Regularization Change: 0.000 -> 1.055
2024-12-02-03:01:45-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-03:01:45-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-03:01:46-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-03:01:46-root-INFO: grad norm: 427.128 406.777 130.271
2024-12-02-03:01:46-root-INFO: Loss too large (2456.292->2465.400)! Learning rate decreased to 0.00120.
2024-12-02-03:01:47-root-INFO: grad norm: 383.195 375.005 78.802
2024-12-02-03:01:48-root-INFO: grad norm: 361.226 352.487 78.974
2024-12-02-03:01:49-root-INFO: grad norm: 340.547 333.268 70.033
2024-12-02-03:01:50-root-INFO: grad norm: 333.550 325.200 74.162
2024-12-02-03:01:51-root-INFO: Loss Change: 2456.292 -> 2343.655
2024-12-02-03:01:51-root-INFO: Regularization Change: 0.000 -> 0.671
2024-12-02-03:01:51-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-03:01:51-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:01:51-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-03:01:52-root-INFO: grad norm: 257.085 251.539 53.112
2024-12-02-03:01:52-root-INFO: Loss too large (2313.185->2315.291)! Learning rate decreased to 0.00125.
2024-12-02-03:01:53-root-INFO: grad norm: 255.167 247.944 60.286
2024-12-02-03:01:54-root-INFO: grad norm: 271.162 265.438 55.423
2024-12-02-03:01:55-root-INFO: grad norm: 326.198 316.062 80.688
2024-12-02-03:01:55-root-INFO: Loss too large (2281.547->2290.399)! Learning rate decreased to 0.00100.
2024-12-02-03:01:56-root-INFO: grad norm: 281.307 274.474 61.626
2024-12-02-03:01:57-root-INFO: Loss Change: 2313.185 -> 2255.358
2024-12-02-03:01:57-root-INFO: Regularization Change: 0.000 -> 0.395
2024-12-02-03:01:57-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-03:01:57-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:01:57-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-03:01:58-root-INFO: grad norm: 734.006 711.521 180.285
2024-12-02-03:01:58-root-INFO: Loss too large (2291.330->2466.693)! Learning rate decreased to 0.00131.
2024-12-02-03:01:58-root-INFO: Loss too large (2291.330->2395.411)! Learning rate decreased to 0.00105.
2024-12-02-03:01:59-root-INFO: Loss too large (2291.330->2341.917)! Learning rate decreased to 0.00084.
2024-12-02-03:01:59-root-INFO: Loss too large (2291.330->2301.515)! Learning rate decreased to 0.00067.
2024-12-02-03:02:00-root-INFO: grad norm: 278.112 271.133 61.912
2024-12-02-03:02:01-root-INFO: grad norm: 219.874 215.137 45.394
2024-12-02-03:02:02-root-INFO: grad norm: 102.181 95.242 37.013
2024-12-02-03:02:03-root-INFO: grad norm: 94.686 89.206 31.744
2024-12-02-03:02:04-root-INFO: Loss Change: 2291.330 -> 2191.935
2024-12-02-03:02:04-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-02-03:02:04-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-03:02:04-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:02:04-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-03:02:04-root-INFO: grad norm: 212.611 203.732 60.799
2024-12-02-03:02:05-root-INFO: Loss too large (2182.327->2227.392)! Learning rate decreased to 0.00137.
2024-12-02-03:02:05-root-INFO: Loss too large (2182.327->2209.533)! Learning rate decreased to 0.00109.
2024-12-02-03:02:05-root-INFO: Loss too large (2182.327->2196.124)! Learning rate decreased to 0.00087.
2024-12-02-03:02:06-root-INFO: Loss too large (2182.327->2186.682)! Learning rate decreased to 0.00070.
2024-12-02-03:02:07-root-INFO: grad norm: 215.847 211.497 43.112
2024-12-02-03:02:08-root-INFO: grad norm: 242.280 234.984 59.009
2024-12-02-03:02:08-root-INFO: Loss too large (2172.901->2175.775)! Learning rate decreased to 0.00056.
2024-12-02-03:02:09-root-INFO: grad norm: 202.675 198.591 40.485
2024-12-02-03:02:10-root-INFO: grad norm: 156.480 150.768 41.890
2024-12-02-03:02:11-root-INFO: Loss Change: 2182.327 -> 2158.237
2024-12-02-03:02:11-root-INFO: Regularization Change: 0.000 -> 0.074
2024-12-02-03:02:11-root-INFO: Undo step: 195
2024-12-02-03:02:11-root-INFO: Undo step: 196
2024-12-02-03:02:11-root-INFO: Undo step: 197
2024-12-02-03:02:11-root-INFO: Undo step: 198
2024-12-02-03:02:11-root-INFO: Undo step: 199
2024-12-02-03:02:11-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-03:02:11-root-INFO: grad norm: 1043.485 897.860 531.702
2024-12-02-03:02:12-root-INFO: grad norm: 551.856 527.339 162.662
2024-12-02-03:02:13-root-INFO: grad norm: 370.691 349.438 123.714
2024-12-02-03:02:14-root-INFO: grad norm: 333.886 318.643 99.733
2024-12-02-03:02:15-root-INFO: grad norm: 343.371 332.436 85.966
2024-12-02-03:02:16-root-INFO: Loss Change: 4055.121 -> 2622.483
2024-12-02-03:02:16-root-INFO: Regularization Change: 0.000 -> 6.860
2024-12-02-03:02:16-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-03:02:16-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-03:02:16-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-03:02:17-root-INFO: grad norm: 671.141 652.072 158.850
2024-12-02-03:02:17-root-INFO: Loss too large (2630.046->2710.504)! Learning rate decreased to 0.00115.
2024-12-02-03:02:17-root-INFO: Loss too large (2630.046->2659.507)! Learning rate decreased to 0.00092.
2024-12-02-03:02:18-root-INFO: grad norm: 338.104 328.441 80.256
2024-12-02-03:02:19-root-INFO: grad norm: 234.462 225.402 64.549
2024-12-02-03:02:20-root-INFO: grad norm: 157.799 147.578 55.871
2024-12-02-03:02:21-root-INFO: grad norm: 150.120 140.224 53.604
2024-12-02-03:02:22-root-INFO: Loss Change: 2630.046 -> 2460.871
2024-12-02-03:02:22-root-INFO: Regularization Change: 0.000 -> 0.523
2024-12-02-03:02:22-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-03:02:22-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-03:02:22-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-03:02:23-root-INFO: grad norm: 522.860 505.549 133.427
2024-12-02-03:02:23-root-INFO: Loss too large (2470.465->2561.347)! Learning rate decreased to 0.00120.
2024-12-02-03:02:23-root-INFO: Loss too large (2470.465->2524.945)! Learning rate decreased to 0.00096.
2024-12-02-03:02:24-root-INFO: Loss too large (2470.465->2494.913)! Learning rate decreased to 0.00077.
2024-12-02-03:02:24-root-INFO: Loss too large (2470.465->2471.546)! Learning rate decreased to 0.00061.
2024-12-02-03:02:25-root-INFO: grad norm: 291.039 283.094 67.539
2024-12-02-03:02:26-root-INFO: grad norm: 147.985 138.657 51.711
2024-12-02-03:02:27-root-INFO: grad norm: 137.607 128.132 50.178
2024-12-02-03:02:28-root-INFO: grad norm: 132.152 123.532 46.948
2024-12-02-03:02:28-root-INFO: Loss Change: 2470.465 -> 2382.962
2024-12-02-03:02:28-root-INFO: Regularization Change: 0.000 -> 0.187
2024-12-02-03:02:28-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-03:02:28-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:02:29-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-03:02:29-root-INFO: grad norm: 362.917 351.012 92.194
2024-12-02-03:02:29-root-INFO: Loss too large (2379.678->2455.028)! Learning rate decreased to 0.00125.
2024-12-02-03:02:30-root-INFO: Loss too large (2379.678->2427.144)! Learning rate decreased to 0.00100.
2024-12-02-03:02:30-root-INFO: Loss too large (2379.678->2405.117)! Learning rate decreased to 0.00080.
2024-12-02-03:02:30-root-INFO: Loss too large (2379.678->2388.304)! Learning rate decreased to 0.00064.
2024-12-02-03:02:31-root-INFO: grad norm: 286.680 279.465 63.911
2024-12-02-03:02:32-root-INFO: grad norm: 175.081 167.154 52.085
2024-12-02-03:02:33-root-INFO: grad norm: 183.074 176.076 50.135
2024-12-02-03:02:34-root-INFO: grad norm: 206.185 199.092 53.616
2024-12-02-03:02:35-root-INFO: Loss Change: 2379.678 -> 2326.172
2024-12-02-03:02:35-root-INFO: Regularization Change: 0.000 -> 0.171
2024-12-02-03:02:35-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-03:02:35-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:02:35-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-03:02:36-root-INFO: grad norm: 171.401 156.723 69.397
2024-12-02-03:02:37-root-INFO: grad norm: 320.661 313.171 68.899
2024-12-02-03:02:37-root-INFO: Loss too large (2293.655->2528.128)! Learning rate decreased to 0.00131.
2024-12-02-03:02:37-root-INFO: Loss too large (2293.655->2416.580)! Learning rate decreased to 0.00105.
2024-12-02-03:02:38-root-INFO: Loss too large (2293.655->2344.344)! Learning rate decreased to 0.00084.
2024-12-02-03:02:38-root-INFO: Loss too large (2293.655->2302.264)! Learning rate decreased to 0.00067.
2024-12-02-03:02:39-root-INFO: grad norm: 394.982 386.095 83.317
2024-12-02-03:02:39-root-INFO: Loss too large (2280.618->2295.908)! Learning rate decreased to 0.00054.
2024-12-02-03:02:40-root-INFO: Loss too large (2280.618->2281.354)! Learning rate decreased to 0.00043.
2024-12-02-03:02:41-root-INFO: grad norm: 259.832 254.066 54.434
2024-12-02-03:02:42-root-INFO: grad norm: 138.275 132.002 41.177
2024-12-02-03:02:42-root-INFO: Loss Change: 2300.929 -> 2252.227
2024-12-02-03:02:42-root-INFO: Regularization Change: 0.000 -> 0.172
2024-12-02-03:02:42-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-03:02:42-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:02:43-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-03:02:43-root-INFO: grad norm: 165.668 157.805 50.434
2024-12-02-03:02:43-root-INFO: Loss too large (2238.969->2264.940)! Learning rate decreased to 0.00137.
2024-12-02-03:02:44-root-INFO: Loss too large (2238.969->2252.432)! Learning rate decreased to 0.00109.
2024-12-02-03:02:44-root-INFO: Loss too large (2238.969->2243.842)! Learning rate decreased to 0.00087.
2024-12-02-03:02:45-root-INFO: grad norm: 272.492 266.555 56.569
2024-12-02-03:02:45-root-INFO: Loss too large (2238.411->2267.569)! Learning rate decreased to 0.00070.
2024-12-02-03:02:46-root-INFO: Loss too large (2238.411->2243.618)! Learning rate decreased to 0.00056.
2024-12-02-03:02:47-root-INFO: grad norm: 314.646 307.522 66.576
2024-12-02-03:02:47-root-INFO: Loss too large (2231.232->2234.950)! Learning rate decreased to 0.00045.
2024-12-02-03:02:48-root-INFO: grad norm: 256.220 250.592 53.410
2024-12-02-03:02:49-root-INFO: grad norm: 191.378 185.611 46.626
2024-12-02-03:02:50-root-INFO: Loss Change: 2238.969 -> 2212.492
2024-12-02-03:02:50-root-INFO: Regularization Change: 0.000 -> 0.080
2024-12-02-03:02:50-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-03:02:50-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:02:50-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-03:02:50-root-INFO: grad norm: 170.789 160.123 59.410
2024-12-02-03:02:51-root-INFO: Loss too large (2198.612->2225.922)! Learning rate decreased to 0.00143.
2024-12-02-03:02:51-root-INFO: Loss too large (2198.612->2211.605)! Learning rate decreased to 0.00114.
2024-12-02-03:02:51-root-INFO: Loss too large (2198.612->2202.243)! Learning rate decreased to 0.00091.
2024-12-02-03:02:52-root-INFO: grad norm: 289.424 283.724 57.155
2024-12-02-03:02:53-root-INFO: Loss too large (2196.589->2248.757)! Learning rate decreased to 0.00073.
2024-12-02-03:02:53-root-INFO: Loss too large (2196.589->2215.006)! Learning rate decreased to 0.00058.
2024-12-02-03:02:54-root-INFO: grad norm: 424.803 415.956 86.242
2024-12-02-03:02:54-root-INFO: Loss too large (2196.139->2214.024)! Learning rate decreased to 0.00047.
2024-12-02-03:02:55-root-INFO: Loss too large (2196.139->2197.552)! Learning rate decreased to 0.00037.
2024-12-02-03:02:56-root-INFO: grad norm: 278.939 273.565 54.493
2024-12-02-03:02:57-root-INFO: grad norm: 154.025 148.336 41.475
2024-12-02-03:02:57-root-INFO: Loss Change: 2198.612 -> 2170.666
2024-12-02-03:02:57-root-INFO: Regularization Change: 0.000 -> 0.073
2024-12-02-03:02:58-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-03:02:58-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:02:58-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-03:02:58-root-INFO: grad norm: 413.209 401.758 96.603
2024-12-02-03:02:58-root-INFO: Loss too large (2171.353->2342.072)! Learning rate decreased to 0.00149.
2024-12-02-03:02:59-root-INFO: Loss too large (2171.353->2305.651)! Learning rate decreased to 0.00119.
2024-12-02-03:02:59-root-INFO: Loss too large (2171.353->2272.808)! Learning rate decreased to 0.00095.
2024-12-02-03:02:59-root-INFO: Loss too large (2171.353->2243.403)! Learning rate decreased to 0.00076.
2024-12-02-03:03:00-root-INFO: Loss too large (2171.353->2217.267)! Learning rate decreased to 0.00061.
2024-12-02-03:03:00-root-INFO: Loss too large (2171.353->2194.897)! Learning rate decreased to 0.00049.
2024-12-02-03:03:00-root-INFO: Loss too large (2171.353->2177.442)! Learning rate decreased to 0.00039.
2024-12-02-03:03:02-root-INFO: grad norm: 327.048 320.966 62.781
2024-12-02-03:03:03-root-INFO: grad norm: 232.499 224.963 58.718
2024-12-02-03:03:03-root-INFO: grad norm: 212.331 207.167 46.541
2024-12-02-03:03:04-root-INFO: grad norm: 190.895 184.540 48.846
2024-12-02-03:03:05-root-INFO: Loss Change: 2171.353 -> 2138.825
2024-12-02-03:03:05-root-INFO: Regularization Change: 0.000 -> 0.052
2024-12-02-03:03:05-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-03:03:05-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-03:03:05-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-03:03:06-root-INFO: grad norm: 128.724 120.428 45.465
2024-12-02-03:03:07-root-INFO: grad norm: 278.734 272.733 57.529
2024-12-02-03:03:07-root-INFO: Loss too large (2116.569->2410.847)! Learning rate decreased to 0.00156.
2024-12-02-03:03:07-root-INFO: Loss too large (2116.569->2295.494)! Learning rate decreased to 0.00124.
2024-12-02-03:03:08-root-INFO: Loss too large (2116.569->2216.768)! Learning rate decreased to 0.00100.
2024-12-02-03:03:08-root-INFO: Loss too large (2116.569->2166.051)! Learning rate decreased to 0.00080.
2024-12-02-03:03:08-root-INFO: Loss too large (2116.569->2135.303)! Learning rate decreased to 0.00064.
2024-12-02-03:03:09-root-INFO: Loss too large (2116.569->2117.893)! Learning rate decreased to 0.00051.
2024-12-02-03:03:10-root-INFO: grad norm: 338.969 333.068 62.977
2024-12-02-03:03:10-root-INFO: Loss too large (2108.891->2120.922)! Learning rate decreased to 0.00041.
2024-12-02-03:03:10-root-INFO: Loss too large (2108.891->2109.669)! Learning rate decreased to 0.00033.
2024-12-02-03:03:11-root-INFO: grad norm: 242.274 237.627 47.221
2024-12-02-03:03:12-root-INFO: grad norm: 163.104 158.649 37.859
2024-12-02-03:03:13-root-INFO: Loss Change: 2122.354 -> 2093.591
2024-12-02-03:03:13-root-INFO: Regularization Change: 0.000 -> 0.101
2024-12-02-03:03:13-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-03:03:13-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:03:13-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-03:03:14-root-INFO: grad norm: 491.556 479.254 109.280
2024-12-02-03:03:14-root-INFO: Loss too large (2109.591->2325.622)! Learning rate decreased to 0.00162.
2024-12-02-03:03:14-root-INFO: Loss too large (2109.591->2287.617)! Learning rate decreased to 0.00130.
2024-12-02-03:03:15-root-INFO: Loss too large (2109.591->2252.562)! Learning rate decreased to 0.00104.
2024-12-02-03:03:15-root-INFO: Loss too large (2109.591->2220.038)! Learning rate decreased to 0.00083.
2024-12-02-03:03:15-root-INFO: Loss too large (2109.591->2189.813)! Learning rate decreased to 0.00067.
2024-12-02-03:03:16-root-INFO: Loss too large (2109.591->2161.668)! Learning rate decreased to 0.00053.
2024-12-02-03:03:16-root-INFO: Loss too large (2109.591->2136.324)! Learning rate decreased to 0.00043.
2024-12-02-03:03:16-root-INFO: Loss too large (2109.591->2115.722)! Learning rate decreased to 0.00034.
2024-12-02-03:03:17-root-INFO: grad norm: 372.524 366.386 67.343
2024-12-02-03:03:18-root-INFO: grad norm: 250.919 242.799 63.313
2024-12-02-03:03:19-root-INFO: grad norm: 223.160 217.980 47.805
2024-12-02-03:03:20-root-INFO: grad norm: 194.840 188.052 50.980
2024-12-02-03:03:21-root-INFO: Loss Change: 2109.591 -> 2072.813
2024-12-02-03:03:21-root-INFO: Regularization Change: 0.000 -> 0.046
2024-12-02-03:03:21-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-03:03:21-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:03:21-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-03:03:22-root-INFO: grad norm: 363.870 353.656 85.608
2024-12-02-03:03:22-root-INFO: Loss too large (2075.388->2267.120)! Learning rate decreased to 0.00170.
2024-12-02-03:03:22-root-INFO: Loss too large (2075.388->2231.417)! Learning rate decreased to 0.00136.
2024-12-02-03:03:22-root-INFO: Loss too large (2075.388->2199.002)! Learning rate decreased to 0.00108.
2024-12-02-03:03:23-root-INFO: Loss too large (2075.388->2169.190)! Learning rate decreased to 0.00087.
2024-12-02-03:03:23-root-INFO: Loss too large (2075.388->2141.557)! Learning rate decreased to 0.00069.
2024-12-02-03:03:24-root-INFO: Loss too large (2075.388->2116.692)! Learning rate decreased to 0.00056.
2024-12-02-03:03:24-root-INFO: Loss too large (2075.388->2096.196)! Learning rate decreased to 0.00044.
2024-12-02-03:03:24-root-INFO: Loss too large (2075.388->2081.200)! Learning rate decreased to 0.00036.
2024-12-02-03:03:25-root-INFO: grad norm: 334.802 329.563 58.996
2024-12-02-03:03:26-root-INFO: grad norm: 305.576 297.834 68.351
2024-12-02-03:03:27-root-INFO: grad norm: 294.608 289.930 52.290
2024-12-02-03:03:28-root-INFO: grad norm: 281.880 275.169 61.141
2024-12-02-03:03:29-root-INFO: Loss Change: 2075.388 -> 2053.564
2024-12-02-03:03:29-root-INFO: Regularization Change: 0.000 -> 0.045
2024-12-02-03:03:29-root-INFO: Undo step: 190
2024-12-02-03:03:29-root-INFO: Undo step: 191
2024-12-02-03:03:29-root-INFO: Undo step: 192
2024-12-02-03:03:29-root-INFO: Undo step: 193
2024-12-02-03:03:29-root-INFO: Undo step: 194
2024-12-02-03:03:29-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-03:03:30-root-INFO: grad norm: 1357.641 1275.968 463.783
2024-12-02-03:03:31-root-INFO: grad norm: 1446.621 1394.541 384.665
2024-12-02-03:03:32-root-INFO: grad norm: 986.428 965.224 203.428
2024-12-02-03:03:33-root-INFO: grad norm: 469.790 455.344 115.605
2024-12-02-03:03:34-root-INFO: grad norm: 293.344 277.052 96.398
2024-12-02-03:03:34-root-INFO: Loss Change: 4022.109 -> 2456.869
2024-12-02-03:03:34-root-INFO: Regularization Change: 0.000 -> 9.808
2024-12-02-03:03:34-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-03:03:34-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:03:35-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-03:03:35-root-INFO: grad norm: 292.326 283.403 71.675
2024-12-02-03:03:36-root-INFO: grad norm: 240.564 229.868 70.936
2024-12-02-03:03:37-root-INFO: grad norm: 271.708 261.627 73.323
2024-12-02-03:03:38-root-INFO: grad norm: 329.875 321.477 73.961
2024-12-02-03:03:38-root-INFO: Loss too large (2329.003->2330.352)! Learning rate decreased to 0.00143.
2024-12-02-03:03:39-root-INFO: grad norm: 287.743 278.605 71.939
2024-12-02-03:03:40-root-INFO: Loss Change: 2438.356 -> 2276.504
2024-12-02-03:03:40-root-INFO: Regularization Change: 0.000 -> 1.182
2024-12-02-03:03:40-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-03:03:40-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:03:40-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-03:03:41-root-INFO: grad norm: 206.572 200.421 50.038
2024-12-02-03:03:42-root-INFO: grad norm: 430.066 419.057 96.686
2024-12-02-03:03:42-root-INFO: Loss too large (2238.065->2338.988)! Learning rate decreased to 0.00149.
2024-12-02-03:03:42-root-INFO: Loss too large (2238.065->2286.721)! Learning rate decreased to 0.00119.
2024-12-02-03:03:43-root-INFO: Loss too large (2238.065->2250.153)! Learning rate decreased to 0.00095.
2024-12-02-03:03:44-root-INFO: grad norm: 258.521 253.454 50.933
2024-12-02-03:03:45-root-INFO: grad norm: 122.182 116.823 35.789
2024-12-02-03:03:46-root-INFO: grad norm: 105.463 99.685 34.428
2024-12-02-03:03:46-root-INFO: Loss Change: 2239.484 -> 2160.354
2024-12-02-03:03:46-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-02-03:03:46-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-03:03:46-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-03:03:47-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-03:03:47-root-INFO: grad norm: 146.117 139.297 44.118
2024-12-02-03:03:47-root-INFO: Loss too large (2140.879->2144.046)! Learning rate decreased to 0.00156.
2024-12-02-03:03:48-root-INFO: grad norm: 252.735 247.841 49.494
2024-12-02-03:03:49-root-INFO: Loss too large (2137.462->2224.461)! Learning rate decreased to 0.00124.
2024-12-02-03:03:49-root-INFO: Loss too large (2137.462->2171.150)! Learning rate decreased to 0.00100.
2024-12-02-03:03:49-root-INFO: Loss too large (2137.462->2141.048)! Learning rate decreased to 0.00080.
2024-12-02-03:03:50-root-INFO: grad norm: 281.943 276.892 53.132
2024-12-02-03:03:51-root-INFO: Loss too large (2126.082->2131.957)! Learning rate decreased to 0.00064.
2024-12-02-03:03:52-root-INFO: grad norm: 234.246 229.980 44.497
2024-12-02-03:03:53-root-INFO: grad norm: 177.751 173.720 37.640
2024-12-02-03:03:53-root-INFO: Loss Change: 2140.879 -> 2106.037
2024-12-02-03:03:53-root-INFO: Regularization Change: 0.000 -> 0.163
2024-12-02-03:03:53-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-03:03:53-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:03:54-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-03:03:54-root-INFO: grad norm: 209.104 199.078 63.974
2024-12-02-03:03:54-root-INFO: Loss too large (2097.099->2151.798)! Learning rate decreased to 0.00162.
2024-12-02-03:03:55-root-INFO: Loss too large (2097.099->2128.894)! Learning rate decreased to 0.00130.
2024-12-02-03:03:55-root-INFO: Loss too large (2097.099->2112.222)! Learning rate decreased to 0.00104.
2024-12-02-03:03:55-root-INFO: Loss too large (2097.099->2100.898)! Learning rate decreased to 0.00083.
2024-12-02-03:03:56-root-INFO: grad norm: 254.540 249.983 47.947
2024-12-02-03:03:57-root-INFO: Loss too large (2093.866->2095.883)! Learning rate decreased to 0.00067.
2024-12-02-03:03:58-root-INFO: grad norm: 266.830 260.994 55.502
2024-12-02-03:03:58-root-INFO: Loss too large (2084.297->2085.525)! Learning rate decreased to 0.00053.
2024-12-02-03:03:59-root-INFO: grad norm: 212.222 208.132 41.465
2024-12-02-03:04:00-root-INFO: grad norm: 157.689 153.134 37.627
2024-12-02-03:04:01-root-INFO: Loss Change: 2097.099 -> 2066.361
2024-12-02-03:04:01-root-INFO: Regularization Change: 0.000 -> 0.093
2024-12-02-03:04:01-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-03:04:01-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:04:01-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-03:04:01-root-INFO: grad norm: 231.703 224.809 56.099
2024-12-02-03:04:02-root-INFO: Loss too large (2058.069->2155.430)! Learning rate decreased to 0.00170.
2024-12-02-03:04:02-root-INFO: Loss too large (2058.069->2126.369)! Learning rate decreased to 0.00136.
2024-12-02-03:04:02-root-INFO: Loss too large (2058.069->2101.977)! Learning rate decreased to 0.00108.
2024-12-02-03:04:03-root-INFO: Loss too large (2058.069->2082.644)! Learning rate decreased to 0.00087.
2024-12-02-03:04:03-root-INFO: Loss too large (2058.069->2068.591)! Learning rate decreased to 0.00069.
2024-12-02-03:04:03-root-INFO: Loss too large (2058.069->2059.309)! Learning rate decreased to 0.00056.
2024-12-02-03:04:04-root-INFO: grad norm: 218.258 214.391 40.901
2024-12-02-03:04:05-root-INFO: grad norm: 210.632 205.845 44.651
2024-12-02-03:04:06-root-INFO: grad norm: 209.415 205.791 38.792
2024-12-02-03:04:07-root-INFO: grad norm: 210.169 206.085 41.233
2024-12-02-03:04:08-root-INFO: Loss Change: 2058.069 -> 2038.662
2024-12-02-03:04:08-root-INFO: Regularization Change: 0.000 -> 0.068
2024-12-02-03:04:08-root-INFO: Undo step: 190
2024-12-02-03:04:08-root-INFO: Undo step: 191
2024-12-02-03:04:08-root-INFO: Undo step: 192
2024-12-02-03:04:08-root-INFO: Undo step: 193
2024-12-02-03:04:08-root-INFO: Undo step: 194
2024-12-02-03:04:08-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-03:04:09-root-INFO: grad norm: 972.769 914.266 332.261
2024-12-02-03:04:10-root-INFO: grad norm: 653.281 633.006 161.491
2024-12-02-03:04:10-root-INFO: Loss too large (3075.095->3216.239)! Learning rate decreased to 0.00137.
2024-12-02-03:04:11-root-INFO: grad norm: 1026.996 990.271 272.186
2024-12-02-03:04:11-root-INFO: Loss too large (3069.888->3287.899)! Learning rate decreased to 0.00109.
2024-12-02-03:04:12-root-INFO: grad norm: 936.383 904.396 242.654
2024-12-02-03:04:13-root-INFO: grad norm: 731.730 702.461 204.884
2024-12-02-03:04:14-root-INFO: Loss Change: 4001.552 -> 2734.130
2024-12-02-03:04:14-root-INFO: Regularization Change: 0.000 -> 7.661
2024-12-02-03:04:14-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-03:04:14-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:04:14-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-03:04:15-root-INFO: grad norm: 666.746 639.890 187.327
2024-12-02-03:04:15-root-INFO: Loss too large (2734.901->2854.078)! Learning rate decreased to 0.00143.
2024-12-02-03:04:16-root-INFO: grad norm: 758.216 731.724 198.675
2024-12-02-03:04:16-root-INFO: Loss too large (2674.388->2736.828)! Learning rate decreased to 0.00114.
2024-12-02-03:04:17-root-INFO: grad norm: 591.621 570.589 156.348
2024-12-02-03:04:18-root-INFO: grad norm: 432.269 416.672 115.069
2024-12-02-03:04:19-root-INFO: grad norm: 438.630 425.891 104.941
2024-12-02-03:04:20-root-INFO: Loss Change: 2734.901 -> 2478.134
2024-12-02-03:04:20-root-INFO: Regularization Change: 0.000 -> 1.224
2024-12-02-03:04:20-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-03:04:20-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-03:04:20-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-03:04:21-root-INFO: grad norm: 339.055 330.462 75.849
2024-12-02-03:04:21-root-INFO: Loss too large (2425.129->2577.664)! Learning rate decreased to 0.00149.
2024-12-02-03:04:21-root-INFO: Loss too large (2425.129->2477.043)! Learning rate decreased to 0.00119.
2024-12-02-03:04:22-root-INFO: grad norm: 554.194 543.103 110.316
2024-12-02-03:04:23-root-INFO: Loss too large (2420.022->2476.603)! Learning rate decreased to 0.00095.
2024-12-02-03:04:23-root-INFO: Loss too large (2420.022->2440.079)! Learning rate decreased to 0.00076.
2024-12-02-03:04:24-root-INFO: grad norm: 310.277 303.685 63.617
2024-12-02-03:04:25-root-INFO: grad norm: 126.095 118.376 43.442
2024-12-02-03:04:26-root-INFO: grad norm: 122.262 114.584 42.645
2024-12-02-03:04:26-root-INFO: Loss Change: 2425.129 -> 2337.237
2024-12-02-03:04:26-root-INFO: Regularization Change: 0.000 -> 0.253
2024-12-02-03:04:26-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-03:04:26-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-03:04:27-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-03:04:27-root-INFO: grad norm: 129.963 122.022 44.733
2024-12-02-03:04:28-root-INFO: grad norm: 199.415 191.410 55.932
2024-12-02-03:04:28-root-INFO: Loss too large (2298.645->2349.770)! Learning rate decreased to 0.00156.
2024-12-02-03:04:29-root-INFO: Loss too large (2298.645->2318.647)! Learning rate decreased to 0.00124.
2024-12-02-03:04:29-root-INFO: Loss too large (2298.645->2301.102)! Learning rate decreased to 0.00100.
2024-12-02-03:04:30-root-INFO: grad norm: 268.916 263.194 55.180
2024-12-02-03:04:30-root-INFO: Loss too large (2291.961->2306.645)! Learning rate decreased to 0.00080.
2024-12-02-03:04:31-root-INFO: Loss too large (2291.961->2293.343)! Learning rate decreased to 0.00064.
2024-12-02-03:04:32-root-INFO: grad norm: 231.191 225.692 50.121
2024-12-02-03:04:33-root-INFO: grad norm: 201.495 196.831 43.101
2024-12-02-03:04:33-root-INFO: Loss Change: 2313.991 -> 2270.529
2024-12-02-03:04:33-root-INFO: Regularization Change: 0.000 -> 0.237
2024-12-02-03:04:33-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-03:04:33-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:04:34-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-03:04:34-root-INFO: grad norm: 182.670 173.563 56.958
2024-12-02-03:04:34-root-INFO: Loss too large (2259.586->2291.906)! Learning rate decreased to 0.00162.
2024-12-02-03:04:35-root-INFO: Loss too large (2259.586->2274.311)! Learning rate decreased to 0.00130.
2024-12-02-03:04:35-root-INFO: Loss too large (2259.586->2262.777)! Learning rate decreased to 0.00104.
2024-12-02-03:04:36-root-INFO: grad norm: 267.812 262.488 53.134
2024-12-02-03:04:37-root-INFO: Loss too large (2255.868->2278.612)! Learning rate decreased to 0.00083.
2024-12-02-03:04:37-root-INFO: Loss too large (2255.868->2256.280)! Learning rate decreased to 0.00067.
2024-12-02-03:04:38-root-INFO: grad norm: 262.111 256.574 53.587
2024-12-02-03:04:39-root-INFO: grad norm: 262.113 257.167 50.682
2024-12-02-03:04:40-root-INFO: grad norm: 263.939 258.983 50.909
2024-12-02-03:04:40-root-INFO: Loss Change: 2259.586 -> 2232.525
2024-12-02-03:04:40-root-INFO: Regularization Change: 0.000 -> 0.153
2024-12-02-03:04:40-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-03:04:40-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:04:41-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-03:04:41-root-INFO: grad norm: 123.559 117.305 38.811
2024-12-02-03:04:42-root-INFO: grad norm: 344.186 338.841 60.422
2024-12-02-03:04:42-root-INFO: Loss too large (2206.600->2355.799)! Learning rate decreased to 0.00170.
2024-12-02-03:04:43-root-INFO: Loss too large (2206.600->2315.699)! Learning rate decreased to 0.00136.
2024-12-02-03:04:43-root-INFO: Loss too large (2206.600->2279.926)! Learning rate decreased to 0.00108.
2024-12-02-03:04:43-root-INFO: Loss too large (2206.600->2248.412)! Learning rate decreased to 0.00087.
2024-12-02-03:04:44-root-INFO: Loss too large (2206.600->2222.961)! Learning rate decreased to 0.00069.
2024-12-02-03:04:45-root-INFO: grad norm: 329.776 324.900 56.502
2024-12-02-03:04:46-root-INFO: grad norm: 352.769 347.616 60.076
2024-12-02-03:04:46-root-INFO: Loss too large (2194.417->2201.303)! Learning rate decreased to 0.00056.
2024-12-02-03:04:47-root-INFO: grad norm: 276.823 272.181 50.483
2024-12-02-03:04:48-root-INFO: Loss Change: 2207.089 -> 2175.539
2024-12-02-03:04:48-root-INFO: Regularization Change: 0.000 -> 0.169
2024-12-02-03:04:48-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-03:04:48-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:04:48-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-03:04:49-root-INFO: grad norm: 436.090 428.973 78.460
2024-12-02-03:04:49-root-INFO: Loss too large (2185.981->2364.331)! Learning rate decreased to 0.00177.
2024-12-02-03:04:49-root-INFO: Loss too large (2185.981->2329.986)! Learning rate decreased to 0.00142.
2024-12-02-03:04:50-root-INFO: Loss too large (2185.981->2295.390)! Learning rate decreased to 0.00113.
2024-12-02-03:04:50-root-INFO: Loss too large (2185.981->2261.033)! Learning rate decreased to 0.00091.
2024-12-02-03:04:50-root-INFO: Loss too large (2185.981->2227.822)! Learning rate decreased to 0.00072.
2024-12-02-03:04:51-root-INFO: Loss too large (2185.981->2198.969)! Learning rate decreased to 0.00058.
2024-12-02-03:04:52-root-INFO: grad norm: 336.569 331.978 55.401
2024-12-02-03:04:53-root-INFO: grad norm: 241.382 237.067 45.436
2024-12-02-03:04:54-root-INFO: grad norm: 222.516 218.476 42.209
2024-12-02-03:04:55-root-INFO: grad norm: 203.595 199.807 39.088
2024-12-02-03:04:55-root-INFO: Loss Change: 2185.981 -> 2144.597
2024-12-02-03:04:55-root-INFO: Regularization Change: 0.000 -> 0.090
2024-12-02-03:04:55-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-03:04:55-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:04:56-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-03:04:56-root-INFO: grad norm: 129.822 124.275 37.542
2024-12-02-03:04:56-root-INFO: Loss too large (2130.106->2139.625)! Learning rate decreased to 0.00185.
2024-12-02-03:04:57-root-INFO: Loss too large (2130.106->2130.167)! Learning rate decreased to 0.00148.
2024-12-02-03:04:58-root-INFO: grad norm: 229.564 224.893 46.072
2024-12-02-03:04:58-root-INFO: Loss too large (2125.243->2201.283)! Learning rate decreased to 0.00118.
2024-12-02-03:04:58-root-INFO: Loss too large (2125.243->2163.513)! Learning rate decreased to 0.00095.
2024-12-02-03:04:59-root-INFO: Loss too large (2125.243->2139.962)! Learning rate decreased to 0.00076.
2024-12-02-03:04:59-root-INFO: Loss too large (2125.243->2126.340)! Learning rate decreased to 0.00060.
2024-12-02-03:05:00-root-INFO: grad norm: 233.589 230.003 40.778
2024-12-02-03:05:01-root-INFO: grad norm: 250.136 246.103 44.737
2024-12-02-03:05:02-root-INFO: grad norm: 271.763 267.981 45.176
2024-12-02-03:05:02-root-INFO: Loss too large (2114.451->2114.546)! Learning rate decreased to 0.00048.
2024-12-02-03:05:03-root-INFO: Loss Change: 2130.106 -> 2107.897
2024-12-02-03:05:03-root-INFO: Regularization Change: 0.000 -> 0.103
2024-12-02-03:05:03-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-03:05:03-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-03:05:03-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-03:05:04-root-INFO: grad norm: 100.476 93.831 35.934
2024-12-02-03:05:05-root-INFO: grad norm: 171.584 167.155 38.734
2024-12-02-03:05:05-root-INFO: Loss too large (2089.216->2209.419)! Learning rate decreased to 0.00193.
2024-12-02-03:05:05-root-INFO: Loss too large (2089.216->2160.362)! Learning rate decreased to 0.00154.
2024-12-02-03:05:06-root-INFO: Loss too large (2089.216->2127.724)! Learning rate decreased to 0.00123.
2024-12-02-03:05:06-root-INFO: Loss too large (2089.216->2107.145)! Learning rate decreased to 0.00099.
2024-12-02-03:05:06-root-INFO: Loss too large (2089.216->2094.894)! Learning rate decreased to 0.00079.
2024-12-02-03:05:07-root-INFO: grad norm: 256.223 252.349 44.391
2024-12-02-03:05:08-root-INFO: Loss too large (2088.084->2100.428)! Learning rate decreased to 0.00063.
2024-12-02-03:05:08-root-INFO: Loss too large (2088.084->2089.056)! Learning rate decreased to 0.00050.
2024-12-02-03:05:09-root-INFO: grad norm: 205.121 201.524 38.243
2024-12-02-03:05:10-root-INFO: grad norm: 164.145 160.974 32.106
2024-12-02-03:05:11-root-INFO: Loss Change: 2103.709 -> 2073.405
2024-12-02-03:05:11-root-INFO: Regularization Change: 0.000 -> 0.156
2024-12-02-03:05:11-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-03:05:11-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:05:11-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-03:05:11-root-INFO: grad norm: 148.528 144.541 34.185
2024-12-02-03:05:12-root-INFO: Loss too large (2058.894->2127.170)! Learning rate decreased to 0.00201.
2024-12-02-03:05:12-root-INFO: Loss too large (2058.894->2101.868)! Learning rate decreased to 0.00161.
2024-12-02-03:05:12-root-INFO: Loss too large (2058.894->2083.231)! Learning rate decreased to 0.00129.
2024-12-02-03:05:13-root-INFO: Loss too large (2058.894->2070.569)! Learning rate decreased to 0.00103.
2024-12-02-03:05:13-root-INFO: Loss too large (2058.894->2062.588)! Learning rate decreased to 0.00082.
2024-12-02-03:05:14-root-INFO: grad norm: 229.002 225.413 40.385
2024-12-02-03:05:14-root-INFO: Loss too large (2057.943->2070.031)! Learning rate decreased to 0.00066.
2024-12-02-03:05:15-root-INFO: Loss too large (2057.943->2058.625)! Learning rate decreased to 0.00053.
2024-12-02-03:05:16-root-INFO: grad norm: 203.163 199.960 35.935
2024-12-02-03:05:17-root-INFO: grad norm: 186.899 183.516 35.401
2024-12-02-03:05:18-root-INFO: grad norm: 172.088 169.166 31.575
2024-12-02-03:05:18-root-INFO: Loss Change: 2058.894 -> 2042.003
2024-12-02-03:05:18-root-INFO: Regularization Change: 0.000 -> 0.057
2024-12-02-03:05:18-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-03:05:18-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:05:19-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-03:05:19-root-INFO: grad norm: 86.092 82.300 25.269
2024-12-02-03:05:20-root-INFO: grad norm: 169.923 167.038 31.181
2024-12-02-03:05:20-root-INFO: Loss too large (2017.018->2119.981)! Learning rate decreased to 0.00209.
2024-12-02-03:05:21-root-INFO: Loss too large (2017.018->2086.166)! Learning rate decreased to 0.00168.
2024-12-02-03:05:21-root-INFO: Loss too large (2017.018->2059.120)! Learning rate decreased to 0.00134.
2024-12-02-03:05:21-root-INFO: Loss too large (2017.018->2039.531)! Learning rate decreased to 0.00107.
2024-12-02-03:05:22-root-INFO: Loss too large (2017.018->2026.559)! Learning rate decreased to 0.00086.
2024-12-02-03:05:22-root-INFO: Loss too large (2017.018->2018.654)! Learning rate decreased to 0.00069.
2024-12-02-03:05:23-root-INFO: grad norm: 209.881 206.553 37.227
2024-12-02-03:05:23-root-INFO: Loss too large (2014.260->2017.535)! Learning rate decreased to 0.00055.
2024-12-02-03:05:24-root-INFO: grad norm: 213.645 210.950 33.825
2024-12-02-03:05:25-root-INFO: grad norm: 217.638 214.336 37.768
2024-12-02-03:05:26-root-INFO: Loss Change: 2027.403 -> 2006.650
2024-12-02-03:05:26-root-INFO: Regularization Change: 0.000 -> 0.136
2024-12-02-03:05:26-root-INFO: Undo step: 185
2024-12-02-03:05:26-root-INFO: Undo step: 186
2024-12-02-03:05:26-root-INFO: Undo step: 187
2024-12-02-03:05:26-root-INFO: Undo step: 188
2024-12-02-03:05:26-root-INFO: Undo step: 189
2024-12-02-03:05:26-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-03:05:27-root-INFO: grad norm: 887.703 813.034 356.361
2024-12-02-03:05:28-root-INFO: grad norm: 482.651 458.945 149.405
2024-12-02-03:05:29-root-INFO: grad norm: 274.454 253.808 104.434
2024-12-02-03:05:30-root-INFO: grad norm: 481.824 472.979 91.896
2024-12-02-03:05:30-root-INFO: Loss too large (2429.101->2517.452)! Learning rate decreased to 0.00170.
2024-12-02-03:05:30-root-INFO: Loss too large (2429.101->2489.932)! Learning rate decreased to 0.00136.
2024-12-02-03:05:31-root-INFO: Loss too large (2429.101->2462.735)! Learning rate decreased to 0.00108.
2024-12-02-03:05:31-root-INFO: Loss too large (2429.101->2437.483)! Learning rate decreased to 0.00087.
2024-12-02-03:05:32-root-INFO: grad norm: 340.545 331.685 77.175
2024-12-02-03:05:33-root-INFO: Loss Change: 3530.855 -> 2346.206
2024-12-02-03:05:33-root-INFO: Regularization Change: 0.000 -> 7.160
2024-12-02-03:05:33-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-03:05:33-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:05:33-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-03:05:33-root-INFO: grad norm: 310.065 301.275 73.304
2024-12-02-03:05:34-root-INFO: Loss too large (2338.233->2412.743)! Learning rate decreased to 0.00177.
2024-12-02-03:05:34-root-INFO: Loss too large (2338.233->2386.492)! Learning rate decreased to 0.00142.
2024-12-02-03:05:34-root-INFO: Loss too large (2338.233->2363.509)! Learning rate decreased to 0.00113.
2024-12-02-03:05:35-root-INFO: Loss too large (2338.233->2344.852)! Learning rate decreased to 0.00091.
2024-12-02-03:05:35-root-INFO: grad norm: 323.944 316.762 67.835
2024-12-02-03:05:37-root-INFO: grad norm: 384.992 377.977 73.161
2024-12-02-03:05:37-root-INFO: Loss too large (2305.162->2317.363)! Learning rate decreased to 0.00072.
2024-12-02-03:05:38-root-INFO: grad norm: 329.001 322.485 65.156
2024-12-02-03:05:39-root-INFO: grad norm: 264.458 257.836 58.811
2024-12-02-03:05:40-root-INFO: Loss Change: 2338.233 -> 2261.446
2024-12-02-03:05:40-root-INFO: Regularization Change: 0.000 -> 0.345
2024-12-02-03:05:40-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-03:05:40-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:05:40-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-03:05:40-root-INFO: grad norm: 197.601 190.166 53.693
2024-12-02-03:05:41-root-INFO: Loss too large (2236.017->2322.241)! Learning rate decreased to 0.00185.
2024-12-02-03:05:41-root-INFO: Loss too large (2236.017->2280.864)! Learning rate decreased to 0.00148.
2024-12-02-03:05:41-root-INFO: Loss too large (2236.017->2253.975)! Learning rate decreased to 0.00118.
2024-12-02-03:05:42-root-INFO: Loss too large (2236.017->2237.939)! Learning rate decreased to 0.00095.
2024-12-02-03:05:43-root-INFO: grad norm: 333.836 328.156 61.317
2024-12-02-03:05:43-root-INFO: Loss too large (2229.326->2255.995)! Learning rate decreased to 0.00076.
2024-12-02-03:05:43-root-INFO: Loss too large (2229.326->2237.023)! Learning rate decreased to 0.00060.
2024-12-02-03:05:44-root-INFO: grad norm: 305.366 299.999 57.000
2024-12-02-03:05:45-root-INFO: grad norm: 279.106 273.715 54.593
2024-12-02-03:05:46-root-INFO: grad norm: 281.249 276.145 53.340
2024-12-02-03:05:47-root-INFO: Loss Change: 2236.017 -> 2194.625
2024-12-02-03:05:47-root-INFO: Regularization Change: 0.000 -> 0.170
2024-12-02-03:05:47-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-03:05:47-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-03:05:47-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-03:05:48-root-INFO: grad norm: 535.514 528.275 87.751
2024-12-02-03:05:48-root-INFO: Loss too large (2214.867->2407.501)! Learning rate decreased to 0.00193.
2024-12-02-03:05:48-root-INFO: Loss too large (2214.867->2379.867)! Learning rate decreased to 0.00154.
2024-12-02-03:05:49-root-INFO: Loss too large (2214.867->2348.334)! Learning rate decreased to 0.00123.
2024-12-02-03:05:49-root-INFO: Loss too large (2214.867->2314.168)! Learning rate decreased to 0.00099.
2024-12-02-03:05:49-root-INFO: Loss too large (2214.867->2278.767)! Learning rate decreased to 0.00079.
2024-12-02-03:05:49-root-INFO: Loss too large (2214.867->2244.038)! Learning rate decreased to 0.00063.
2024-12-02-03:05:50-root-INFO: grad norm: 404.424 399.748 61.316
2024-12-02-03:05:51-root-INFO: grad norm: 258.430 252.971 52.836
2024-12-02-03:05:52-root-INFO: grad norm: 289.589 285.397 49.093
2024-12-02-03:05:53-root-INFO: grad norm: 340.952 335.891 58.528
2024-12-02-03:05:54-root-INFO: Loss too large (2161.824->2166.153)! Learning rate decreased to 0.00050.
2024-12-02-03:05:54-root-INFO: Loss Change: 2214.867 -> 2154.591
2024-12-02-03:05:54-root-INFO: Regularization Change: 0.000 -> 0.146
2024-12-02-03:05:54-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-03:05:54-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:05:55-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-03:05:55-root-INFO: grad norm: 143.732 137.822 40.792
2024-12-02-03:05:55-root-INFO: Loss too large (2130.404->2187.051)! Learning rate decreased to 0.00201.
2024-12-02-03:05:56-root-INFO: Loss too large (2130.404->2160.800)! Learning rate decreased to 0.00161.
2024-12-02-03:05:56-root-INFO: Loss too large (2130.404->2144.015)! Learning rate decreased to 0.00129.
2024-12-02-03:05:56-root-INFO: Loss too large (2130.404->2133.957)! Learning rate decreased to 0.00103.
2024-12-02-03:05:57-root-INFO: grad norm: 295.266 290.756 51.411
2024-12-02-03:05:58-root-INFO: Loss too large (2128.381->2173.800)! Learning rate decreased to 0.00082.
2024-12-02-03:05:58-root-INFO: Loss too large (2128.381->2151.218)! Learning rate decreased to 0.00066.
2024-12-02-03:05:58-root-INFO: Loss too large (2128.381->2134.955)! Learning rate decreased to 0.00053.
2024-12-02-03:05:59-root-INFO: grad norm: 279.692 275.777 46.632
2024-12-02-03:06:00-root-INFO: grad norm: 267.434 263.176 47.533
2024-12-02-03:06:01-root-INFO: grad norm: 265.113 261.341 44.558
2024-12-02-03:06:02-root-INFO: Loss Change: 2130.404 -> 2108.993
2024-12-02-03:06:02-root-INFO: Regularization Change: 0.000 -> 0.094
2024-12-02-03:06:02-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-03:06:02-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:06:02-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-03:06:03-root-INFO: grad norm: 411.237 406.385 62.985
2024-12-02-03:06:03-root-INFO: Loss too large (2107.206->2316.327)! Learning rate decreased to 0.00209.
2024-12-02-03:06:03-root-INFO: Loss too large (2107.206->2288.432)! Learning rate decreased to 0.00168.
2024-12-02-03:06:04-root-INFO: Loss too large (2107.206->2257.171)! Learning rate decreased to 0.00134.
2024-12-02-03:06:04-root-INFO: Loss too large (2107.206->2223.602)! Learning rate decreased to 0.00107.
2024-12-02-03:06:04-root-INFO: Loss too large (2107.206->2188.846)! Learning rate decreased to 0.00086.
2024-12-02-03:06:05-root-INFO: Loss too large (2107.206->2155.165)! Learning rate decreased to 0.00069.
2024-12-02-03:06:05-root-INFO: Loss too large (2107.206->2126.221)! Learning rate decreased to 0.00055.
2024-12-02-03:06:06-root-INFO: grad norm: 377.106 373.231 53.922
2024-12-02-03:06:07-root-INFO: grad norm: 348.672 344.472 53.954
2024-12-02-03:06:08-root-INFO: grad norm: 346.687 342.967 50.650
2024-12-02-03:06:09-root-INFO: grad norm: 347.268 343.250 52.676
2024-12-02-03:06:09-root-INFO: Loss too large (2084.198->2084.607)! Learning rate decreased to 0.00044.
2024-12-02-03:06:10-root-INFO: Loss Change: 2107.206 -> 2074.464
2024-12-02-03:06:10-root-INFO: Regularization Change: 0.000 -> 0.077
2024-12-02-03:06:10-root-INFO: Undo step: 185
2024-12-02-03:06:10-root-INFO: Undo step: 186
2024-12-02-03:06:10-root-INFO: Undo step: 187
2024-12-02-03:06:10-root-INFO: Undo step: 188
2024-12-02-03:06:10-root-INFO: Undo step: 189
2024-12-02-03:06:10-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-03:06:11-root-INFO: grad norm: 611.945 563.191 239.360
2024-12-02-03:06:12-root-INFO: grad norm: 349.230 329.783 114.910
2024-12-02-03:06:13-root-INFO: grad norm: 500.400 479.752 142.259
2024-12-02-03:06:13-root-INFO: Loss too large (2764.023->2979.817)! Learning rate decreased to 0.00170.
2024-12-02-03:06:13-root-INFO: Loss too large (2764.023->2833.750)! Learning rate decreased to 0.00136.
2024-12-02-03:06:14-root-INFO: grad norm: 615.613 594.243 160.793
2024-12-02-03:06:14-root-INFO: Loss too large (2744.118->2748.715)! Learning rate decreased to 0.00108.
2024-12-02-03:06:15-root-INFO: grad norm: 490.795 469.394 143.348
2024-12-02-03:06:16-root-INFO: Loss Change: 3276.220 -> 2602.391
2024-12-02-03:06:16-root-INFO: Regularization Change: 0.000 -> 4.442
2024-12-02-03:06:16-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-03:06:16-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:06:16-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-03:06:17-root-INFO: grad norm: 348.434 337.365 87.126
2024-12-02-03:06:17-root-INFO: Loss too large (2570.350->2639.700)! Learning rate decreased to 0.00177.
2024-12-02-03:06:18-root-INFO: Loss too large (2570.350->2578.858)! Learning rate decreased to 0.00142.
2024-12-02-03:06:19-root-INFO: grad norm: 399.061 382.408 114.079
2024-12-02-03:06:20-root-INFO: grad norm: 450.258 435.642 113.788
2024-12-02-03:06:21-root-INFO: grad norm: 475.131 457.611 127.834
2024-12-02-03:06:22-root-INFO: grad norm: 474.870 459.183 121.046
2024-12-02-03:06:22-root-INFO: Loss Change: 2570.350 -> 2406.169
2024-12-02-03:06:22-root-INFO: Regularization Change: 0.000 -> 1.738
2024-12-02-03:06:22-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-03:06:22-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-03:06:23-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-03:06:23-root-INFO: grad norm: 495.185 478.411 127.791
2024-12-02-03:06:23-root-INFO: Loss too large (2407.012->2557.766)! Learning rate decreased to 0.00185.
2024-12-02-03:06:24-root-INFO: Loss too large (2407.012->2419.126)! Learning rate decreased to 0.00148.
2024-12-02-03:06:25-root-INFO: grad norm: 419.545 407.944 97.977
2024-12-02-03:06:26-root-INFO: grad norm: 362.250 349.665 94.654
2024-12-02-03:06:27-root-INFO: grad norm: 312.292 303.658 72.926
2024-12-02-03:06:28-root-INFO: grad norm: 287.990 278.131 74.710
2024-12-02-03:06:28-root-INFO: Loss Change: 2407.012 -> 2201.864
2024-12-02-03:06:28-root-INFO: Regularization Change: 0.000 -> 1.252
2024-12-02-03:06:28-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-03:06:28-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-03:06:29-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-03:06:29-root-INFO: grad norm: 341.739 334.629 69.348
2024-12-02-03:06:29-root-INFO: Loss too large (2192.698->2301.569)! Learning rate decreased to 0.00193.
2024-12-02-03:06:30-root-INFO: Loss too large (2192.698->2243.339)! Learning rate decreased to 0.00154.
2024-12-02-03:06:30-root-INFO: Loss too large (2192.698->2202.923)! Learning rate decreased to 0.00123.
2024-12-02-03:06:31-root-INFO: grad norm: 296.008 291.626 50.742
2024-12-02-03:06:32-root-INFO: grad norm: 325.926 320.757 57.821
2024-12-02-03:06:32-root-INFO: Loss too large (2147.240->2164.530)! Learning rate decreased to 0.00099.
2024-12-02-03:06:33-root-INFO: grad norm: 281.332 278.333 40.969
2024-12-02-03:06:34-root-INFO: grad norm: 249.444 244.749 48.168
2024-12-02-03:06:35-root-INFO: Loss too large (2115.911->2118.744)! Learning rate decreased to 0.00079.
2024-12-02-03:06:35-root-INFO: Loss Change: 2192.698 -> 2109.936
2024-12-02-03:06:35-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-03:06:35-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-03:06:35-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:06:36-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-03:06:36-root-INFO: grad norm: 171.333 166.806 39.122
2024-12-02-03:06:36-root-INFO: Loss too large (2082.969->2171.335)! Learning rate decreased to 0.00201.
2024-12-02-03:06:37-root-INFO: Loss too large (2082.969->2129.305)! Learning rate decreased to 0.00161.
2024-12-02-03:06:37-root-INFO: Loss too large (2082.969->2102.587)! Learning rate decreased to 0.00129.
2024-12-02-03:06:37-root-INFO: Loss too large (2082.969->2086.955)! Learning rate decreased to 0.00103.
2024-12-02-03:06:38-root-INFO: grad norm: 271.904 267.518 48.639
2024-12-02-03:06:39-root-INFO: Loss too large (2078.644->2096.074)! Learning rate decreased to 0.00082.
2024-12-02-03:06:39-root-INFO: Loss too large (2078.644->2081.787)! Learning rate decreased to 0.00066.
2024-12-02-03:06:40-root-INFO: grad norm: 231.567 228.435 37.959
2024-12-02-03:06:41-root-INFO: grad norm: 196.662 192.553 39.992
2024-12-02-03:06:42-root-INFO: grad norm: 192.487 189.344 34.647
2024-12-02-03:06:43-root-INFO: Loss Change: 2082.969 -> 2051.482
2024-12-02-03:06:43-root-INFO: Regularization Change: 0.000 -> 0.125
2024-12-02-03:06:43-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-03:06:43-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:06:43-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-03:06:43-root-INFO: grad norm: 335.501 330.338 58.630
2024-12-02-03:06:44-root-INFO: Loss too large (2043.072->2209.377)! Learning rate decreased to 0.00209.
2024-12-02-03:06:44-root-INFO: Loss too large (2043.072->2177.369)! Learning rate decreased to 0.00168.
2024-12-02-03:06:44-root-INFO: Loss too large (2043.072->2145.250)! Learning rate decreased to 0.00134.
2024-12-02-03:06:45-root-INFO: Loss too large (2043.072->2114.303)! Learning rate decreased to 0.00107.
2024-12-02-03:06:45-root-INFO: Loss too large (2043.072->2086.031)! Learning rate decreased to 0.00086.
2024-12-02-03:06:45-root-INFO: Loss too large (2043.072->2062.393)! Learning rate decreased to 0.00069.
2024-12-02-03:06:46-root-INFO: Loss too large (2043.072->2044.931)! Learning rate decreased to 0.00055.
2024-12-02-03:06:47-root-INFO: grad norm: 245.385 242.406 38.121
2024-12-02-03:06:47-root-INFO: grad norm: 174.561 170.632 36.826
2024-12-02-03:06:48-root-INFO: grad norm: 152.178 149.034 30.777
2024-12-02-03:06:49-root-INFO: grad norm: 134.616 130.774 31.930
2024-12-02-03:06:50-root-INFO: Loss Change: 2043.072 -> 2010.477
2024-12-02-03:06:50-root-INFO: Regularization Change: 0.000 -> 0.065
2024-12-02-03:06:50-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-03:06:50-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:06:50-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-03:06:51-root-INFO: grad norm: 230.025 223.819 53.069
2024-12-02-03:06:51-root-INFO: Loss too large (2005.343->2140.565)! Learning rate decreased to 0.00218.
2024-12-02-03:06:51-root-INFO: Loss too large (2005.343->2108.330)! Learning rate decreased to 0.00175.
2024-12-02-03:06:52-root-INFO: Loss too large (2005.343->2078.728)! Learning rate decreased to 0.00140.
2024-12-02-03:06:52-root-INFO: Loss too large (2005.343->2052.772)! Learning rate decreased to 0.00112.
2024-12-02-03:06:52-root-INFO: Loss too large (2005.343->2031.808)! Learning rate decreased to 0.00089.
2024-12-02-03:06:53-root-INFO: Loss too large (2005.343->2016.515)! Learning rate decreased to 0.00072.
2024-12-02-03:06:53-root-INFO: Loss too large (2005.343->2006.490)! Learning rate decreased to 0.00057.
2024-12-02-03:06:54-root-INFO: grad norm: 225.980 222.588 39.008
2024-12-02-03:06:55-root-INFO: grad norm: 230.090 225.413 46.160
2024-12-02-03:06:56-root-INFO: grad norm: 235.633 232.613 37.605
2024-12-02-03:06:57-root-INFO: grad norm: 244.981 240.776 45.192
2024-12-02-03:06:58-root-INFO: Loss Change: 2005.343 -> 1987.215
2024-12-02-03:06:58-root-INFO: Regularization Change: 0.000 -> 0.077
2024-12-02-03:06:58-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-03:06:58-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:06:58-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-03:06:58-root-INFO: grad norm: 155.898 151.498 36.781
2024-12-02-03:06:59-root-INFO: Loss too large (1971.781->2098.809)! Learning rate decreased to 0.00228.
2024-12-02-03:06:59-root-INFO: Loss too large (1971.781->2052.116)! Learning rate decreased to 0.00182.
2024-12-02-03:06:59-root-INFO: Loss too large (1971.781->2018.953)! Learning rate decreased to 0.00146.
2024-12-02-03:07:00-root-INFO: Loss too large (1971.781->1996.717)! Learning rate decreased to 0.00117.
2024-12-02-03:07:00-root-INFO: Loss too large (1971.781->1982.665)! Learning rate decreased to 0.00093.
2024-12-02-03:07:00-root-INFO: Loss too large (1971.781->1974.331)! Learning rate decreased to 0.00075.
2024-12-02-03:07:01-root-INFO: grad norm: 239.782 235.737 43.857
2024-12-02-03:07:02-root-INFO: Loss too large (1969.757->1979.964)! Learning rate decreased to 0.00060.
2024-12-02-03:07:02-root-INFO: Loss too large (1969.757->1970.440)! Learning rate decreased to 0.00048.
2024-12-02-03:07:03-root-INFO: grad norm: 203.286 200.149 35.572
2024-12-02-03:07:04-root-INFO: grad norm: 173.960 170.312 35.442
2024-12-02-03:07:05-root-INFO: grad norm: 156.632 153.527 31.035
2024-12-02-03:07:06-root-INFO: Loss Change: 1971.781 -> 1953.235
2024-12-02-03:07:06-root-INFO: Regularization Change: 0.000 -> 0.052
2024-12-02-03:07:06-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-03:07:06-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:07:06-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-03:07:06-root-INFO: grad norm: 371.452 366.614 59.755
2024-12-02-03:07:07-root-INFO: Loss too large (1959.249->2172.382)! Learning rate decreased to 0.00237.
2024-12-02-03:07:07-root-INFO: Loss too large (1959.249->2146.308)! Learning rate decreased to 0.00190.
2024-12-02-03:07:07-root-INFO: Loss too large (1959.249->2116.848)! Learning rate decreased to 0.00152.
2024-12-02-03:07:08-root-INFO: Loss too large (1959.249->2084.909)! Learning rate decreased to 0.00122.
2024-12-02-03:07:08-root-INFO: Loss too large (1959.249->2051.544)! Learning rate decreased to 0.00097.
2024-12-02-03:07:08-root-INFO: Loss too large (1959.249->2018.484)! Learning rate decreased to 0.00078.
2024-12-02-03:07:09-root-INFO: Loss too large (1959.249->1988.918)! Learning rate decreased to 0.00062.
2024-12-02-03:07:09-root-INFO: Loss too large (1959.249->1966.128)! Learning rate decreased to 0.00050.
2024-12-02-03:07:10-root-INFO: grad norm: 310.010 306.860 44.081
2024-12-02-03:07:11-root-INFO: grad norm: 264.799 261.106 44.070
2024-12-02-03:07:12-root-INFO: grad norm: 242.780 239.907 37.244
2024-12-02-03:07:13-root-INFO: grad norm: 224.816 221.510 38.413
2024-12-02-03:07:14-root-INFO: Loss Change: 1959.249 -> 1931.661
2024-12-02-03:07:14-root-INFO: Regularization Change: 0.000 -> 0.059
2024-12-02-03:07:14-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-03:07:14-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-03:07:14-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-03:07:14-root-INFO: grad norm: 98.996 93.920 31.290
2024-12-02-03:07:15-root-INFO: Loss too large (1918.524->1936.265)! Learning rate decreased to 0.00247.
2024-12-02-03:07:15-root-INFO: Loss too large (1918.524->1926.137)! Learning rate decreased to 0.00198.
2024-12-02-03:07:15-root-INFO: Loss too large (1918.524->1919.968)! Learning rate decreased to 0.00158.
2024-12-02-03:07:16-root-INFO: grad norm: 261.720 258.862 38.576
2024-12-02-03:07:17-root-INFO: Loss too large (1916.520->2055.743)! Learning rate decreased to 0.00127.
2024-12-02-03:07:17-root-INFO: Loss too large (1916.520->2004.220)! Learning rate decreased to 0.00101.
2024-12-02-03:07:17-root-INFO: Loss too large (1916.520->1966.295)! Learning rate decreased to 0.00081.
2024-12-02-03:07:18-root-INFO: Loss too large (1916.520->1940.265)! Learning rate decreased to 0.00065.
2024-12-02-03:07:18-root-INFO: Loss too large (1916.520->1923.728)! Learning rate decreased to 0.00052.
2024-12-02-03:07:19-root-INFO: grad norm: 271.604 268.180 42.990
2024-12-02-03:07:20-root-INFO: grad norm: 281.147 278.378 39.358
2024-12-02-03:07:21-root-INFO: grad norm: 290.246 286.798 44.607
2024-12-02-03:07:22-root-INFO: Loss Change: 1918.524 -> 1908.636
2024-12-02-03:07:22-root-INFO: Regularization Change: 0.000 -> 0.084
2024-12-02-03:07:22-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-03:07:22-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:07:22-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-03:07:23-root-INFO: grad norm: 122.455 118.105 32.348
2024-12-02-03:07:23-root-INFO: Loss too large (1895.061->1972.420)! Learning rate decreased to 0.00258.
2024-12-02-03:07:23-root-INFO: Loss too large (1895.061->1943.031)! Learning rate decreased to 0.00206.
2024-12-02-03:07:24-root-INFO: Loss too large (1895.061->1922.631)! Learning rate decreased to 0.00165.
2024-12-02-03:07:24-root-INFO: Loss too large (1895.061->1909.171)! Learning rate decreased to 0.00132.
2024-12-02-03:07:24-root-INFO: Loss too large (1895.061->1900.749)! Learning rate decreased to 0.00106.
2024-12-02-03:07:25-root-INFO: Loss too large (1895.061->1895.789)! Learning rate decreased to 0.00084.
2024-12-02-03:07:26-root-INFO: grad norm: 220.324 216.432 41.225
2024-12-02-03:07:26-root-INFO: Loss too large (1893.092->1912.182)! Learning rate decreased to 0.00068.
2024-12-02-03:07:26-root-INFO: Loss too large (1893.092->1899.683)! Learning rate decreased to 0.00054.
2024-12-02-03:07:27-root-INFO: grad norm: 260.937 258.039 38.780
2024-12-02-03:07:27-root-INFO: Loss too large (1892.250->1892.426)! Learning rate decreased to 0.00043.
2024-12-02-03:07:28-root-INFO: grad norm: 207.207 203.658 38.184
2024-12-02-03:07:30-root-INFO: grad norm: 171.291 168.552 30.513
2024-12-02-03:07:30-root-INFO: Loss Change: 1895.061 -> 1878.981
2024-12-02-03:07:30-root-INFO: Regularization Change: 0.000 -> 0.049
2024-12-02-03:07:30-root-INFO: Undo step: 180
2024-12-02-03:07:30-root-INFO: Undo step: 181
2024-12-02-03:07:30-root-INFO: Undo step: 182
2024-12-02-03:07:30-root-INFO: Undo step: 183
2024-12-02-03:07:30-root-INFO: Undo step: 184
2024-12-02-03:07:31-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-03:07:31-root-INFO: grad norm: 711.628 684.677 193.992
2024-12-02-03:07:32-root-INFO: grad norm: 620.551 596.740 170.250
2024-12-02-03:07:33-root-INFO: grad norm: 476.428 464.911 104.121
2024-12-02-03:07:34-root-INFO: grad norm: 327.171 315.530 86.496
2024-12-02-03:07:35-root-INFO: grad norm: 398.043 388.623 86.087
2024-12-02-03:07:35-root-INFO: Loss too large (2308.043->2329.836)! Learning rate decreased to 0.00209.
2024-12-02-03:07:36-root-INFO: Loss Change: 3406.993 -> 2277.074
2024-12-02-03:07:36-root-INFO: Regularization Change: 0.000 -> 9.931
2024-12-02-03:07:36-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-03:07:36-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:07:36-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-03:07:37-root-INFO: grad norm: 291.137 286.079 54.029
2024-12-02-03:07:37-root-INFO: Loss too large (2232.783->2249.570)! Learning rate decreased to 0.00218.
2024-12-02-03:07:38-root-INFO: grad norm: 421.248 413.213 81.881
2024-12-02-03:07:38-root-INFO: Loss too large (2176.683->2256.265)! Learning rate decreased to 0.00175.
2024-12-02-03:07:39-root-INFO: Loss too large (2176.683->2216.846)! Learning rate decreased to 0.00140.
2024-12-02-03:07:39-root-INFO: Loss too large (2176.683->2183.908)! Learning rate decreased to 0.00112.
2024-12-02-03:07:40-root-INFO: grad norm: 264.399 259.903 48.550
2024-12-02-03:07:41-root-INFO: grad norm: 128.615 120.550 44.827
2024-12-02-03:07:42-root-INFO: grad norm: 120.301 112.978 41.333
2024-12-02-03:07:43-root-INFO: Loss Change: 2232.783 -> 2075.264
2024-12-02-03:07:43-root-INFO: Regularization Change: 0.000 -> 0.706
2024-12-02-03:07:43-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-03:07:43-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:07:43-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-03:07:43-root-INFO: grad norm: 233.109 226.566 54.840
2024-12-02-03:07:44-root-INFO: Loss too large (2067.866->2156.814)! Learning rate decreased to 0.00228.
2024-12-02-03:07:44-root-INFO: Loss too large (2067.866->2127.688)! Learning rate decreased to 0.00182.
2024-12-02-03:07:44-root-INFO: Loss too large (2067.866->2103.159)! Learning rate decreased to 0.00146.
2024-12-02-03:07:45-root-INFO: Loss too large (2067.866->2083.955)! Learning rate decreased to 0.00117.
2024-12-02-03:07:45-root-INFO: Loss too large (2067.866->2070.282)! Learning rate decreased to 0.00093.
2024-12-02-03:07:46-root-INFO: grad norm: 238.480 234.281 44.553
2024-12-02-03:07:47-root-INFO: grad norm: 278.804 273.272 55.263
2024-12-02-03:07:47-root-INFO: Loss too large (2049.813->2054.326)! Learning rate decreased to 0.00075.
2024-12-02-03:07:48-root-INFO: grad norm: 240.535 236.433 44.231
2024-12-02-03:07:49-root-INFO: grad norm: 203.674 198.252 46.682
2024-12-02-03:07:50-root-INFO: Loss Change: 2067.866 -> 2024.371
2024-12-02-03:07:50-root-INFO: Regularization Change: 0.000 -> 0.193
2024-12-02-03:07:50-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-03:07:50-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:07:50-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-03:07:51-root-INFO: grad norm: 116.096 110.084 36.877
2024-12-02-03:07:51-root-INFO: Loss too large (2004.911->2014.732)! Learning rate decreased to 0.00237.
2024-12-02-03:07:52-root-INFO: grad norm: 376.732 372.008 59.473
2024-12-02-03:07:52-root-INFO: Loss too large (2004.451->2160.328)! Learning rate decreased to 0.00190.
2024-12-02-03:07:53-root-INFO: Loss too large (2004.451->2130.768)! Learning rate decreased to 0.00152.
2024-12-02-03:07:53-root-INFO: Loss too large (2004.451->2099.290)! Learning rate decreased to 0.00122.
2024-12-02-03:07:53-root-INFO: Loss too large (2004.451->2067.678)! Learning rate decreased to 0.00097.
2024-12-02-03:07:54-root-INFO: Loss too large (2004.451->2038.428)! Learning rate decreased to 0.00078.
2024-12-02-03:07:54-root-INFO: Loss too large (2004.451->2014.463)! Learning rate decreased to 0.00062.
2024-12-02-03:07:55-root-INFO: grad norm: 297.659 293.895 47.186
2024-12-02-03:07:56-root-INFO: grad norm: 224.137 219.736 44.197
2024-12-02-03:07:57-root-INFO: grad norm: 213.913 210.246 39.437
2024-12-02-03:07:58-root-INFO: Loss Change: 2004.911 -> 1970.796
2024-12-02-03:07:58-root-INFO: Regularization Change: 0.000 -> 0.191
2024-12-02-03:07:58-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-03:07:58-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-03:07:58-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-03:07:58-root-INFO: grad norm: 443.425 438.092 68.567
2024-12-02-03:07:59-root-INFO: Loss too large (1983.439->2189.286)! Learning rate decreased to 0.00247.
2024-12-02-03:07:59-root-INFO: Loss too large (1983.439->2166.212)! Learning rate decreased to 0.00198.
2024-12-02-03:07:59-root-INFO: Loss too large (1983.439->2138.771)! Learning rate decreased to 0.00158.
2024-12-02-03:08:00-root-INFO: Loss too large (1983.439->2107.681)! Learning rate decreased to 0.00127.
2024-12-02-03:08:00-root-INFO: Loss too large (1983.439->2074.209)! Learning rate decreased to 0.00101.
2024-12-02-03:08:00-root-INFO: Loss too large (1983.439->2040.048)! Learning rate decreased to 0.00081.
2024-12-02-03:08:01-root-INFO: Loss too large (1983.439->2008.317)! Learning rate decreased to 0.00065.
2024-12-02-03:08:02-root-INFO: grad norm: 367.497 363.840 51.719
2024-12-02-03:08:03-root-INFO: grad norm: 288.628 284.207 50.323
2024-12-02-03:08:03-root-INFO: Loss too large (1956.685->1957.228)! Learning rate decreased to 0.00052.
2024-12-02-03:08:04-root-INFO: grad norm: 218.719 215.248 38.812
2024-12-02-03:08:05-root-INFO: grad norm: 166.047 161.577 38.270
2024-12-02-03:08:06-root-INFO: Loss Change: 1983.439 -> 1937.609
2024-12-02-03:08:06-root-INFO: Regularization Change: 0.000 -> 0.080
2024-12-02-03:08:06-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-03:08:06-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:08:06-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-03:08:06-root-INFO: grad norm: 135.956 129.797 40.459
2024-12-02-03:08:07-root-INFO: Loss too large (1929.481->1993.414)! Learning rate decreased to 0.00258.
2024-12-02-03:08:07-root-INFO: Loss too large (1929.481->1970.692)! Learning rate decreased to 0.00206.
2024-12-02-03:08:07-root-INFO: Loss too large (1929.481->1953.246)! Learning rate decreased to 0.00165.
2024-12-02-03:08:08-root-INFO: Loss too large (1929.481->1940.962)! Learning rate decreased to 0.00132.
2024-12-02-03:08:08-root-INFO: Loss too large (1929.481->1933.006)! Learning rate decreased to 0.00106.
2024-12-02-03:08:09-root-INFO: grad norm: 254.826 251.392 41.691
2024-12-02-03:08:09-root-INFO: Loss too large (1928.286->1968.462)! Learning rate decreased to 0.00084.
2024-12-02-03:08:10-root-INFO: Loss too large (1928.286->1943.682)! Learning rate decreased to 0.00068.
2024-12-02-03:08:10-root-INFO: Loss too large (1928.286->1928.964)! Learning rate decreased to 0.00054.
2024-12-02-03:08:11-root-INFO: grad norm: 227.280 222.838 44.715
2024-12-02-03:08:12-root-INFO: grad norm: 214.492 211.118 37.896
2024-12-02-03:08:13-root-INFO: grad norm: 203.470 199.248 41.232
2024-12-02-03:08:14-root-INFO: Loss Change: 1929.481 -> 1909.229
2024-12-02-03:08:14-root-INFO: Regularization Change: 0.000 -> 0.082
2024-12-02-03:08:14-root-INFO: Undo step: 180
2024-12-02-03:08:14-root-INFO: Undo step: 181
2024-12-02-03:08:14-root-INFO: Undo step: 182
2024-12-02-03:08:14-root-INFO: Undo step: 183
2024-12-02-03:08:14-root-INFO: Undo step: 184
2024-12-02-03:08:14-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-03:08:14-root-INFO: grad norm: 834.496 785.327 282.213
2024-12-02-03:08:15-root-INFO: grad norm: 1020.364 993.802 231.303
2024-12-02-03:08:16-root-INFO: Loss too large (2820.451->3204.022)! Learning rate decreased to 0.00209.
2024-12-02-03:08:16-root-INFO: Loss too large (2820.451->2877.936)! Learning rate decreased to 0.00168.
2024-12-02-03:08:17-root-INFO: grad norm: 875.420 859.918 164.018
2024-12-02-03:08:18-root-INFO: grad norm: 224.947 209.682 81.452
2024-12-02-03:08:19-root-INFO: grad norm: 217.564 207.227 66.264
2024-12-02-03:08:20-root-INFO: Loss Change: 3246.433 -> 2302.067
2024-12-02-03:08:20-root-INFO: Regularization Change: 0.000 -> 6.476
2024-12-02-03:08:20-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-03:08:20-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:08:20-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-03:08:20-root-INFO: grad norm: 269.708 263.823 56.037
2024-12-02-03:08:21-root-INFO: grad norm: 509.813 503.945 77.127
2024-12-02-03:08:22-root-INFO: Loss too large (2204.511->2330.081)! Learning rate decreased to 0.00218.
2024-12-02-03:08:22-root-INFO: Loss too large (2204.511->2290.027)! Learning rate decreased to 0.00175.
2024-12-02-03:08:22-root-INFO: Loss too large (2204.511->2252.199)! Learning rate decreased to 0.00140.
2024-12-02-03:08:23-root-INFO: Loss too large (2204.511->2217.331)! Learning rate decreased to 0.00112.
2024-12-02-03:08:24-root-INFO: grad norm: 277.186 272.290 51.865
2024-12-02-03:08:25-root-INFO: grad norm: 141.969 135.778 41.467
2024-12-02-03:08:26-root-INFO: grad norm: 151.690 145.886 41.559
2024-12-02-03:08:26-root-INFO: Loss Change: 2267.141 -> 2088.126
2024-12-02-03:08:26-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-02-03:08:26-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-03:08:26-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:08:27-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-03:08:27-root-INFO: grad norm: 134.495 129.191 37.396
2024-12-02-03:08:28-root-INFO: grad norm: 428.150 423.889 60.251
2024-12-02-03:08:28-root-INFO: Loss too large (2065.873->2231.394)! Learning rate decreased to 0.00228.
2024-12-02-03:08:29-root-INFO: Loss too large (2065.873->2196.494)! Learning rate decreased to 0.00182.
2024-12-02-03:08:29-root-INFO: Loss too large (2065.873->2162.334)! Learning rate decreased to 0.00146.
2024-12-02-03:08:29-root-INFO: Loss too large (2065.873->2129.174)! Learning rate decreased to 0.00117.
2024-12-02-03:08:30-root-INFO: Loss too large (2065.873->2098.050)! Learning rate decreased to 0.00093.
2024-12-02-03:08:30-root-INFO: Loss too large (2065.873->2071.036)! Learning rate decreased to 0.00075.
2024-12-02-03:08:31-root-INFO: grad norm: 291.936 288.120 47.053
2024-12-02-03:08:32-root-INFO: grad norm: 170.498 166.310 37.561
2024-12-02-03:08:33-root-INFO: grad norm: 166.972 163.074 35.867
2024-12-02-03:08:34-root-INFO: Loss Change: 2066.005 -> 2007.802
2024-12-02-03:08:34-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-02-03:08:34-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-03:08:34-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-03:08:34-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-03:08:34-root-INFO: grad norm: 358.855 354.799 53.801
2024-12-02-03:08:35-root-INFO: Loss too large (2009.307->2178.178)! Learning rate decreased to 0.00237.
2024-12-02-03:08:35-root-INFO: Loss too large (2009.307->2146.761)! Learning rate decreased to 0.00190.
2024-12-02-03:08:35-root-INFO: Loss too large (2009.307->2115.124)! Learning rate decreased to 0.00152.
2024-12-02-03:08:36-root-INFO: Loss too large (2009.307->2083.849)! Learning rate decreased to 0.00122.
2024-12-02-03:08:36-root-INFO: Loss too large (2009.307->2054.318)! Learning rate decreased to 0.00097.
2024-12-02-03:08:36-root-INFO: Loss too large (2009.307->2028.769)! Learning rate decreased to 0.00078.
2024-12-02-03:08:37-root-INFO: grad norm: 309.171 305.688 46.277
2024-12-02-03:08:38-root-INFO: grad norm: 249.521 245.890 42.411
2024-12-02-03:08:39-root-INFO: grad norm: 256.374 253.102 40.825
2024-12-02-03:08:40-root-INFO: grad norm: 271.657 268.159 43.456
2024-12-02-03:08:41-root-INFO: Loss too large (1974.314->1975.236)! Learning rate decreased to 0.00062.
2024-12-02-03:08:41-root-INFO: Loss Change: 2009.307 -> 1967.067
2024-12-02-03:08:41-root-INFO: Regularization Change: 0.000 -> 0.148
2024-12-02-03:08:41-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-03:08:41-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-03:08:42-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-03:08:42-root-INFO: grad norm: 101.849 96.946 31.222
2024-12-02-03:08:43-root-INFO: grad norm: 331.410 327.965 47.664
2024-12-02-03:08:43-root-INFO: Loss too large (1937.904->2123.262)! Learning rate decreased to 0.00247.
2024-12-02-03:08:44-root-INFO: Loss too large (1937.904->2092.718)! Learning rate decreased to 0.00198.
2024-12-02-03:08:44-root-INFO: Loss too large (1937.904->2060.905)! Learning rate decreased to 0.00158.
2024-12-02-03:08:44-root-INFO: Loss too large (1937.904->2028.489)! Learning rate decreased to 0.00127.
2024-12-02-03:08:45-root-INFO: Loss too large (1937.904->1997.107)! Learning rate decreased to 0.00101.
2024-12-02-03:08:45-root-INFO: Loss too large (1937.904->1969.405)! Learning rate decreased to 0.00081.
2024-12-02-03:08:45-root-INFO: Loss too large (1937.904->1947.750)! Learning rate decreased to 0.00065.
2024-12-02-03:08:46-root-INFO: grad norm: 290.616 287.494 42.481
2024-12-02-03:08:47-root-INFO: grad norm: 258.481 255.306 40.391
2024-12-02-03:08:48-root-INFO: grad norm: 252.655 249.730 38.339
2024-12-02-03:08:49-root-INFO: Loss Change: 1945.332 -> 1911.840
2024-12-02-03:08:49-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-03:08:49-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-03:08:49-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:08:49-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-03:08:50-root-INFO: grad norm: 474.685 470.173 65.294
2024-12-02-03:08:50-root-INFO: Loss too large (1928.233->2134.791)! Learning rate decreased to 0.00258.
2024-12-02-03:08:50-root-INFO: Loss too large (1928.233->2111.013)! Learning rate decreased to 0.00206.
2024-12-02-03:08:51-root-INFO: Loss too large (1928.233->2084.077)! Learning rate decreased to 0.00165.
2024-12-02-03:08:51-root-INFO: Loss too large (1928.233->2053.759)! Learning rate decreased to 0.00132.
2024-12-02-03:08:51-root-INFO: Loss too large (1928.233->2020.589)! Learning rate decreased to 0.00106.
2024-12-02-03:08:52-root-INFO: Loss too large (1928.233->1985.797)! Learning rate decreased to 0.00084.
2024-12-02-03:08:52-root-INFO: Loss too large (1928.233->1952.334)! Learning rate decreased to 0.00068.
2024-12-02-03:08:53-root-INFO: grad norm: 368.312 365.147 48.183
2024-12-02-03:08:54-root-INFO: grad norm: 260.569 256.995 43.012
2024-12-02-03:08:55-root-INFO: grad norm: 270.921 268.140 38.716
2024-12-02-03:08:56-root-INFO: grad norm: 286.199 282.818 43.864
2024-12-02-03:08:56-root-INFO: Loss too large (1885.074->1885.847)! Learning rate decreased to 0.00054.
2024-12-02-03:08:57-root-INFO: Loss Change: 1928.233 -> 1877.606
2024-12-02-03:08:57-root-INFO: Regularization Change: 0.000 -> 0.118
2024-12-02-03:08:57-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-03:08:57-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:08:57-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-03:08:58-root-INFO: grad norm: 88.842 84.043 28.805
2024-12-02-03:08:59-root-INFO: grad norm: 121.488 117.581 30.562
2024-12-02-03:08:59-root-INFO: Loss too large (1839.388->1894.039)! Learning rate decreased to 0.00269.
2024-12-02-03:08:59-root-INFO: Loss too large (1839.388->1871.658)! Learning rate decreased to 0.00215.
2024-12-02-03:09:00-root-INFO: Loss too large (1839.388->1856.140)! Learning rate decreased to 0.00172.
2024-12-02-03:09:00-root-INFO: Loss too large (1839.388->1846.105)! Learning rate decreased to 0.00138.
2024-12-02-03:09:00-root-INFO: Loss too large (1839.388->1840.074)! Learning rate decreased to 0.00110.
2024-12-02-03:09:01-root-INFO: grad norm: 212.394 209.859 32.718
2024-12-02-03:09:02-root-INFO: Loss too large (1836.757->1867.678)! Learning rate decreased to 0.00088.
2024-12-02-03:09:02-root-INFO: Loss too large (1836.757->1849.230)! Learning rate decreased to 0.00070.
2024-12-02-03:09:02-root-INFO: Loss too large (1836.757->1838.293)! Learning rate decreased to 0.00056.
2024-12-02-03:09:03-root-INFO: grad norm: 203.924 201.114 33.738
2024-12-02-03:09:04-root-INFO: grad norm: 198.863 196.301 31.820
2024-12-02-03:09:05-root-INFO: Loss Change: 1859.321 -> 1825.715
2024-12-02-03:09:05-root-INFO: Regularization Change: 0.000 -> 0.238
2024-12-02-03:09:05-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-03:09:05-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:09:05-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-03:09:06-root-INFO: grad norm: 410.155 406.473 54.833
2024-12-02-03:09:06-root-INFO: Loss too large (1833.625->2053.586)! Learning rate decreased to 0.00280.
2024-12-02-03:09:06-root-INFO: Loss too large (1833.625->2032.206)! Learning rate decreased to 0.00224.
2024-12-02-03:09:07-root-INFO: Loss too large (1833.625->2007.313)! Learning rate decreased to 0.00179.
2024-12-02-03:09:07-root-INFO: Loss too large (1833.625->1978.630)! Learning rate decreased to 0.00143.
2024-12-02-03:09:07-root-INFO: Loss too large (1833.625->1946.485)! Learning rate decreased to 0.00115.
2024-12-02-03:09:08-root-INFO: Loss too large (1833.625->1911.935)! Learning rate decreased to 0.00092.
2024-12-02-03:09:08-root-INFO: Loss too large (1833.625->1877.892)! Learning rate decreased to 0.00073.
2024-12-02-03:09:08-root-INFO: Loss too large (1833.625->1848.642)! Learning rate decreased to 0.00059.
2024-12-02-03:09:09-root-INFO: grad norm: 350.842 348.113 43.680
2024-12-02-03:09:10-root-INFO: grad norm: 302.059 298.934 43.334
2024-12-02-03:09:11-root-INFO: grad norm: 289.828 287.291 38.265
2024-12-02-03:09:12-root-INFO: grad norm: 278.270 275.324 40.382
2024-12-02-03:09:13-root-INFO: Loss Change: 1833.625 -> 1802.386
2024-12-02-03:09:13-root-INFO: Regularization Change: 0.000 -> 0.093
2024-12-02-03:09:13-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-03:09:13-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:09:13-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-03:09:14-root-INFO: grad norm: 146.974 143.973 29.551
2024-12-02-03:09:14-root-INFO: Loss too large (1785.090->1955.396)! Learning rate decreased to 0.00291.
2024-12-02-03:09:14-root-INFO: Loss too large (1785.090->1901.905)! Learning rate decreased to 0.00233.
2024-12-02-03:09:15-root-INFO: Loss too large (1785.090->1860.685)! Learning rate decreased to 0.00187.
2024-12-02-03:09:15-root-INFO: Loss too large (1785.090->1830.649)! Learning rate decreased to 0.00149.
2024-12-02-03:09:15-root-INFO: Loss too large (1785.090->1810.033)! Learning rate decreased to 0.00119.
2024-12-02-03:09:16-root-INFO: Loss too large (1785.090->1796.716)! Learning rate decreased to 0.00096.
2024-12-02-03:09:16-root-INFO: Loss too large (1785.090->1788.641)! Learning rate decreased to 0.00076.
2024-12-02-03:09:17-root-INFO: grad norm: 230.187 227.222 36.826
2024-12-02-03:09:17-root-INFO: Loss too large (1784.090->1792.185)! Learning rate decreased to 0.00061.
2024-12-02-03:09:18-root-INFO: grad norm: 261.639 259.104 36.329
2024-12-02-03:09:19-root-INFO: grad norm: 296.204 293.319 41.239
2024-12-02-03:09:20-root-INFO: Loss too large (1781.801->1782.487)! Learning rate decreased to 0.00049.
2024-12-02-03:09:21-root-INFO: grad norm: 225.301 222.840 33.211
2024-12-02-03:09:21-root-INFO: Loss Change: 1785.090 -> 1768.060
2024-12-02-03:09:21-root-INFO: Regularization Change: 0.000 -> 0.061
2024-12-02-03:09:21-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-03:09:21-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-03:09:22-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-03:09:22-root-INFO: grad norm: 403.440 399.872 53.533
2024-12-02-03:09:22-root-INFO: Loss too large (1781.967->2005.695)! Learning rate decreased to 0.00304.
2024-12-02-03:09:23-root-INFO: Loss too large (1781.967->1988.548)! Learning rate decreased to 0.00243.
2024-12-02-03:09:23-root-INFO: Loss too large (1781.967->1966.983)! Learning rate decreased to 0.00194.
2024-12-02-03:09:23-root-INFO: Loss too large (1781.967->1940.608)! Learning rate decreased to 0.00155.
2024-12-02-03:09:24-root-INFO: Loss too large (1781.967->1909.768)! Learning rate decreased to 0.00124.
2024-12-02-03:09:24-root-INFO: Loss too large (1781.967->1875.236)! Learning rate decreased to 0.00099.
2024-12-02-03:09:24-root-INFO: Loss too large (1781.967->1839.426)! Learning rate decreased to 0.00080.
2024-12-02-03:09:25-root-INFO: Loss too large (1781.967->1806.890)! Learning rate decreased to 0.00064.
2024-12-02-03:09:26-root-INFO: grad norm: 394.148 391.398 46.477
2024-12-02-03:09:27-root-INFO: grad norm: 382.746 379.481 49.888
2024-12-02-03:09:27-root-INFO: Loss too large (1770.534->1770.693)! Learning rate decreased to 0.00051.
2024-12-02-03:09:28-root-INFO: grad norm: 275.863 273.511 35.949
2024-12-02-03:09:29-root-INFO: grad norm: 216.181 213.366 34.775
2024-12-02-03:09:30-root-INFO: Loss Change: 1781.967 -> 1743.084
2024-12-02-03:09:30-root-INFO: Regularization Change: 0.000 -> 0.080
2024-12-02-03:09:30-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-03:09:30-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:09:30-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-03:09:30-root-INFO: grad norm: 111.048 107.129 29.242
2024-12-02-03:09:31-root-INFO: Loss too large (1731.831->1785.023)! Learning rate decreased to 0.00316.
2024-12-02-03:09:31-root-INFO: Loss too large (1731.831->1764.464)! Learning rate decreased to 0.00253.
2024-12-02-03:09:31-root-INFO: Loss too large (1731.831->1749.403)! Learning rate decreased to 0.00202.
2024-12-02-03:09:32-root-INFO: Loss too large (1731.831->1739.314)! Learning rate decreased to 0.00162.
2024-12-02-03:09:32-root-INFO: Loss too large (1731.831->1733.104)! Learning rate decreased to 0.00129.
2024-12-02-03:09:33-root-INFO: grad norm: 247.640 245.382 33.371
2024-12-02-03:09:33-root-INFO: Loss too large (1729.625->1795.276)! Learning rate decreased to 0.00104.
2024-12-02-03:09:34-root-INFO: Loss too large (1729.625->1764.667)! Learning rate decreased to 0.00083.
2024-12-02-03:09:34-root-INFO: Loss too large (1729.625->1744.115)! Learning rate decreased to 0.00066.
2024-12-02-03:09:34-root-INFO: Loss too large (1729.625->1731.409)! Learning rate decreased to 0.00053.
2024-12-02-03:09:35-root-INFO: grad norm: 219.614 216.933 34.212
2024-12-02-03:09:36-root-INFO: grad norm: 199.863 197.629 29.800
2024-12-02-03:09:37-root-INFO: grad norm: 184.672 182.058 30.963
2024-12-02-03:09:38-root-INFO: Loss Change: 1731.831 -> 1712.351
2024-12-02-03:09:38-root-INFO: Regularization Change: 0.000 -> 0.080
2024-12-02-03:09:38-root-INFO: Undo step: 175
2024-12-02-03:09:38-root-INFO: Undo step: 176
2024-12-02-03:09:38-root-INFO: Undo step: 177
2024-12-02-03:09:38-root-INFO: Undo step: 178
2024-12-02-03:09:38-root-INFO: Undo step: 179
2024-12-02-03:09:38-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-03:09:38-root-INFO: grad norm: 884.246 828.352 309.394
2024-12-02-03:09:40-root-INFO: grad norm: 786.944 767.606 173.383
2024-12-02-03:09:40-root-INFO: grad norm: 694.563 679.248 145.052
2024-12-02-03:09:41-root-INFO: Loss too large (2503.927->2614.875)! Learning rate decreased to 0.00258.
2024-12-02-03:09:42-root-INFO: grad norm: 834.889 825.422 125.376
2024-12-02-03:09:43-root-INFO: grad norm: 383.675 373.076 89.557
2024-12-02-03:09:43-root-INFO: Loss Change: 3196.069 -> 2218.329
2024-12-02-03:09:43-root-INFO: Regularization Change: 0.000 -> 11.784
2024-12-02-03:09:43-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-03:09:43-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:09:44-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-03:09:44-root-INFO: grad norm: 362.964 356.694 67.172
2024-12-02-03:09:45-root-INFO: grad norm: 454.795 446.548 86.221
2024-12-02-03:09:46-root-INFO: grad norm: 485.031 478.741 77.856
2024-12-02-03:09:47-root-INFO: grad norm: 397.116 389.826 75.739
2024-12-02-03:09:48-root-INFO: grad norm: 336.934 331.030 62.797
2024-12-02-03:09:49-root-INFO: Loss Change: 2216.482 -> 1917.910
2024-12-02-03:09:49-root-INFO: Regularization Change: 0.000 -> 6.262
2024-12-02-03:09:49-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-03:09:49-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:09:49-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-03:09:49-root-INFO: grad norm: 616.409 608.588 97.880
2024-12-02-03:09:50-root-INFO: Loss too large (1954.383->2068.549)! Learning rate decreased to 0.00280.
2024-12-02-03:09:50-root-INFO: Loss too large (1954.383->2003.545)! Learning rate decreased to 0.00224.
2024-12-02-03:09:51-root-INFO: grad norm: 225.838 219.211 54.308
2024-12-02-03:09:52-root-INFO: grad norm: 195.642 191.463 40.223
2024-12-02-03:09:53-root-INFO: grad norm: 156.081 152.350 33.923
2024-12-02-03:09:54-root-INFO: grad norm: 148.260 144.124 34.776
2024-12-02-03:09:55-root-INFO: Loss Change: 1954.383 -> 1727.196
2024-12-02-03:09:55-root-INFO: Regularization Change: 0.000 -> 1.336
2024-12-02-03:09:55-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-03:09:55-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:09:55-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-03:09:55-root-INFO: grad norm: 223.978 220.534 39.129
2024-12-02-03:09:56-root-INFO: Loss too large (1705.949->2049.126)! Learning rate decreased to 0.00291.
2024-12-02-03:09:56-root-INFO: Loss too large (1705.949->1948.249)! Learning rate decreased to 0.00233.
2024-12-02-03:09:57-root-INFO: Loss too large (1705.949->1861.535)! Learning rate decreased to 0.00187.
2024-12-02-03:09:57-root-INFO: Loss too large (1705.949->1793.199)! Learning rate decreased to 0.00149.
2024-12-02-03:09:57-root-INFO: Loss too large (1705.949->1744.990)! Learning rate decreased to 0.00119.
2024-12-02-03:09:58-root-INFO: Loss too large (1705.949->1714.994)! Learning rate decreased to 0.00096.
2024-12-02-03:09:58-root-INFO: grad norm: 287.187 283.956 42.957
2024-12-02-03:09:59-root-INFO: Loss too large (1698.779->1711.106)! Learning rate decreased to 0.00076.
2024-12-02-03:10:00-root-INFO: grad norm: 251.180 247.690 41.726
2024-12-02-03:10:01-root-INFO: grad norm: 214.294 211.284 35.793
2024-12-02-03:10:02-root-INFO: grad norm: 224.026 220.907 37.252
2024-12-02-03:10:03-root-INFO: Loss Change: 1705.949 -> 1676.968
2024-12-02-03:10:03-root-INFO: Regularization Change: 0.000 -> 0.126
2024-12-02-03:10:03-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-03:10:03-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-03:10:03-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-03:10:03-root-INFO: grad norm: 502.379 498.189 64.753
2024-12-02-03:10:04-root-INFO: Loss too large (1702.476->1917.464)! Learning rate decreased to 0.00304.
2024-12-02-03:10:04-root-INFO: Loss too large (1702.476->1893.673)! Learning rate decreased to 0.00243.
2024-12-02-03:10:04-root-INFO: Loss too large (1702.476->1866.605)! Learning rate decreased to 0.00194.
2024-12-02-03:10:05-root-INFO: Loss too large (1702.476->1836.307)! Learning rate decreased to 0.00155.
2024-12-02-03:10:05-root-INFO: Loss too large (1702.476->1803.928)! Learning rate decreased to 0.00124.
2024-12-02-03:10:05-root-INFO: Loss too large (1702.476->1770.877)! Learning rate decreased to 0.00099.
2024-12-02-03:10:06-root-INFO: Loss too large (1702.476->1738.657)! Learning rate decreased to 0.00080.
2024-12-02-03:10:06-root-INFO: Loss too large (1702.476->1709.506)! Learning rate decreased to 0.00064.
2024-12-02-03:10:07-root-INFO: grad norm: 302.354 298.791 46.282
2024-12-02-03:10:08-root-INFO: grad norm: 125.067 121.427 29.952
2024-12-02-03:10:09-root-INFO: grad norm: 120.055 117.171 26.158
2024-12-02-03:10:10-root-INFO: grad norm: 121.465 118.304 27.531
2024-12-02-03:10:11-root-INFO: Loss Change: 1702.476 -> 1644.073
2024-12-02-03:10:11-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-03:10:11-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-03:10:11-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:10:11-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-03:10:11-root-INFO: grad norm: 169.427 165.933 34.231
2024-12-02-03:10:12-root-INFO: Loss too large (1640.479->1775.957)! Learning rate decreased to 0.00316.
2024-12-02-03:10:12-root-INFO: Loss too large (1640.479->1745.361)! Learning rate decreased to 0.00253.
2024-12-02-03:10:12-root-INFO: Loss too large (1640.479->1716.797)! Learning rate decreased to 0.00202.
2024-12-02-03:10:13-root-INFO: Loss too large (1640.479->1691.868)! Learning rate decreased to 0.00162.
2024-12-02-03:10:13-root-INFO: Loss too large (1640.479->1671.806)! Learning rate decreased to 0.00129.
2024-12-02-03:10:13-root-INFO: Loss too large (1640.479->1656.969)! Learning rate decreased to 0.00104.
2024-12-02-03:10:14-root-INFO: Loss too large (1640.479->1646.887)! Learning rate decreased to 0.00083.
2024-12-02-03:10:14-root-INFO: Loss too large (1640.479->1640.609)! Learning rate decreased to 0.00066.
2024-12-02-03:10:15-root-INFO: grad norm: 186.665 183.950 31.721
2024-12-02-03:10:16-root-INFO: grad norm: 225.497 222.678 35.547
2024-12-02-03:10:16-root-INFO: Loss too large (1634.131->1635.476)! Learning rate decreased to 0.00053.
2024-12-02-03:10:17-root-INFO: grad norm: 192.602 189.985 31.644
2024-12-02-03:10:18-root-INFO: grad norm: 165.142 162.468 29.596
2024-12-02-03:10:19-root-INFO: Loss Change: 1640.479 -> 1622.203
2024-12-02-03:10:19-root-INFO: Regularization Change: 0.000 -> 0.058
2024-12-02-03:10:19-root-INFO: Undo step: 175
2024-12-02-03:10:19-root-INFO: Undo step: 176
2024-12-02-03:10:19-root-INFO: Undo step: 177
2024-12-02-03:10:19-root-INFO: Undo step: 178
2024-12-02-03:10:19-root-INFO: Undo step: 179
2024-12-02-03:10:19-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-03:10:20-root-INFO: grad norm: 649.045 626.757 168.629
2024-12-02-03:10:21-root-INFO: grad norm: 469.891 450.957 132.042
2024-12-02-03:10:22-root-INFO: grad norm: 583.326 567.604 134.521
2024-12-02-03:10:22-root-INFO: Loss too large (2156.328->2289.304)! Learning rate decreased to 0.00258.
2024-12-02-03:10:22-root-INFO: Loss too large (2156.328->2160.323)! Learning rate decreased to 0.00206.
2024-12-02-03:10:23-root-INFO: grad norm: 331.635 315.035 103.608
2024-12-02-03:10:24-root-INFO: grad norm: 176.304 167.719 54.346
2024-12-02-03:10:25-root-INFO: Loss Change: 2756.374 -> 1876.559
2024-12-02-03:10:25-root-INFO: Regularization Change: 0.000 -> 7.174
2024-12-02-03:10:25-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-03:10:25-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:10:25-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-03:10:26-root-INFO: grad norm: 269.734 260.342 70.560
2024-12-02-03:10:26-root-INFO: Loss too large (1870.079->1976.979)! Learning rate decreased to 0.00269.
2024-12-02-03:10:26-root-INFO: Loss too large (1870.079->1938.461)! Learning rate decreased to 0.00215.
2024-12-02-03:10:27-root-INFO: Loss too large (1870.079->1904.722)! Learning rate decreased to 0.00172.
2024-12-02-03:10:27-root-INFO: Loss too large (1870.079->1878.473)! Learning rate decreased to 0.00138.
2024-12-02-03:10:28-root-INFO: grad norm: 267.377 253.634 84.620
2024-12-02-03:10:29-root-INFO: grad norm: 294.587 284.980 74.621
2024-12-02-03:10:29-root-INFO: Loss too large (1836.455->1841.802)! Learning rate decreased to 0.00110.
2024-12-02-03:10:30-root-INFO: grad norm: 250.072 237.720 77.622
2024-12-02-03:10:31-root-INFO: grad norm: 211.409 203.692 56.597
2024-12-02-03:10:32-root-INFO: Loss Change: 1870.079 -> 1793.392
2024-12-02-03:10:32-root-INFO: Regularization Change: 0.000 -> 0.482
2024-12-02-03:10:32-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-03:10:32-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:10:32-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-03:10:33-root-INFO: grad norm: 150.767 143.963 44.782
2024-12-02-03:10:33-root-INFO: Loss too large (1768.628->1828.745)! Learning rate decreased to 0.00280.
2024-12-02-03:10:33-root-INFO: Loss too large (1768.628->1795.163)! Learning rate decreased to 0.00224.
2024-12-02-03:10:34-root-INFO: Loss too large (1768.628->1774.553)! Learning rate decreased to 0.00179.
2024-12-02-03:10:35-root-INFO: grad norm: 298.631 289.976 71.377
2024-12-02-03:10:35-root-INFO: Loss too large (1763.215->1824.999)! Learning rate decreased to 0.00143.
2024-12-02-03:10:35-root-INFO: Loss too large (1763.215->1794.691)! Learning rate decreased to 0.00115.
2024-12-02-03:10:36-root-INFO: Loss too large (1763.215->1771.387)! Learning rate decreased to 0.00092.
2024-12-02-03:10:37-root-INFO: grad norm: 253.561 242.890 72.783
2024-12-02-03:10:38-root-INFO: grad norm: 210.868 203.876 53.847
2024-12-02-03:10:39-root-INFO: grad norm: 205.169 196.573 58.763
2024-12-02-03:10:39-root-INFO: Loss Change: 1768.628 -> 1723.657
2024-12-02-03:10:39-root-INFO: Regularization Change: 0.000 -> 0.278
2024-12-02-03:10:39-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-03:10:39-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-03:10:40-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-03:10:40-root-INFO: grad norm: 345.705 335.897 81.762
2024-12-02-03:10:40-root-INFO: Loss too large (1727.662->1930.042)! Learning rate decreased to 0.00291.
2024-12-02-03:10:41-root-INFO: Loss too large (1727.662->1896.211)! Learning rate decreased to 0.00233.
2024-12-02-03:10:41-root-INFO: Loss too large (1727.662->1858.984)! Learning rate decreased to 0.00187.
2024-12-02-03:10:41-root-INFO: Loss too large (1727.662->1820.398)! Learning rate decreased to 0.00149.
2024-12-02-03:10:42-root-INFO: Loss too large (1727.662->1783.515)! Learning rate decreased to 0.00119.
2024-12-02-03:10:42-root-INFO: Loss too large (1727.662->1751.868)! Learning rate decreased to 0.00096.
2024-12-02-03:10:42-root-INFO: Loss too large (1727.662->1727.983)! Learning rate decreased to 0.00076.
2024-12-02-03:10:43-root-INFO: grad norm: 243.994 234.634 66.935
2024-12-02-03:10:44-root-INFO: grad norm: 169.161 162.974 45.332
2024-12-02-03:10:45-root-INFO: grad norm: 148.986 142.619 43.090
2024-12-02-03:10:46-root-INFO: grad norm: 134.515 129.128 37.686
2024-12-02-03:10:47-root-INFO: Loss Change: 1727.662 -> 1679.076
2024-12-02-03:10:47-root-INFO: Regularization Change: 0.000 -> 0.134
2024-12-02-03:10:47-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-03:10:47-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-03:10:47-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-03:10:48-root-INFO: grad norm: 132.280 126.103 39.952
2024-12-02-03:10:48-root-INFO: Loss too large (1669.080->1725.531)! Learning rate decreased to 0.00304.
2024-12-02-03:10:48-root-INFO: Loss too large (1669.080->1701.005)! Learning rate decreased to 0.00243.
2024-12-02-03:10:49-root-INFO: Loss too large (1669.080->1683.995)! Learning rate decreased to 0.00194.
2024-12-02-03:10:49-root-INFO: Loss too large (1669.080->1673.123)! Learning rate decreased to 0.00155.
2024-12-02-03:10:50-root-INFO: grad norm: 244.909 236.708 62.847
2024-12-02-03:10:50-root-INFO: Loss too large (1666.761->1729.255)! Learning rate decreased to 0.00124.
2024-12-02-03:10:51-root-INFO: Loss too large (1666.761->1694.391)! Learning rate decreased to 0.00099.
2024-12-02-03:10:51-root-INFO: Loss too large (1666.761->1672.431)! Learning rate decreased to 0.00080.
2024-12-02-03:10:52-root-INFO: grad norm: 245.777 239.194 56.502
2024-12-02-03:10:53-root-INFO: grad norm: 247.751 239.527 63.306
2024-12-02-03:10:54-root-INFO: grad norm: 250.979 244.431 56.953
2024-12-02-03:10:55-root-INFO: Loss Change: 1669.080 -> 1650.815
2024-12-02-03:10:55-root-INFO: Regularization Change: 0.000 -> 0.157
2024-12-02-03:10:55-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-03:10:55-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:10:55-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-03:10:55-root-INFO: grad norm: 139.715 135.182 35.299
2024-12-02-03:10:56-root-INFO: Loss too large (1630.863->1757.987)! Learning rate decreased to 0.00316.
2024-12-02-03:10:56-root-INFO: Loss too large (1630.863->1711.789)! Learning rate decreased to 0.00253.
2024-12-02-03:10:56-root-INFO: Loss too large (1630.863->1678.181)! Learning rate decreased to 0.00202.
2024-12-02-03:10:57-root-INFO: Loss too large (1630.863->1655.301)! Learning rate decreased to 0.00162.
2024-12-02-03:10:57-root-INFO: Loss too large (1630.863->1640.760)! Learning rate decreased to 0.00129.
2024-12-02-03:10:57-root-INFO: Loss too large (1630.863->1632.176)! Learning rate decreased to 0.00104.
2024-12-02-03:10:58-root-INFO: grad norm: 206.299 200.164 49.934
2024-12-02-03:10:59-root-INFO: Loss too large (1627.552->1636.428)! Learning rate decreased to 0.00083.
2024-12-02-03:10:59-root-INFO: grad norm: 237.720 230.363 58.685
2024-12-02-03:11:00-root-INFO: grad norm: 276.313 269.624 60.430
2024-12-02-03:11:01-root-INFO: Loss too large (1625.580->1627.775)! Learning rate decreased to 0.00066.
2024-12-02-03:11:02-root-INFO: grad norm: 216.067 209.312 53.605
2024-12-02-03:11:02-root-INFO: Loss Change: 1630.863 -> 1610.545
2024-12-02-03:11:02-root-INFO: Regularization Change: 0.000 -> 0.098
2024-12-02-03:11:02-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-03:11:02-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:11:03-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-03:11:03-root-INFO: grad norm: 267.894 261.163 59.674
2024-12-02-03:11:03-root-INFO: Loss too large (1615.077->1835.016)! Learning rate decreased to 0.00329.
2024-12-02-03:11:04-root-INFO: Loss too large (1615.077->1804.665)! Learning rate decreased to 0.00263.
2024-12-02-03:11:04-root-INFO: Loss too large (1615.077->1769.810)! Learning rate decreased to 0.00211.
2024-12-02-03:11:04-root-INFO: Loss too large (1615.077->1732.212)! Learning rate decreased to 0.00168.
2024-12-02-03:11:05-root-INFO: Loss too large (1615.077->1695.062)! Learning rate decreased to 0.00135.
2024-12-02-03:11:05-root-INFO: Loss too large (1615.077->1662.232)! Learning rate decreased to 0.00108.
2024-12-02-03:11:05-root-INFO: Loss too large (1615.077->1636.545)! Learning rate decreased to 0.00086.
2024-12-02-03:11:06-root-INFO: Loss too large (1615.077->1618.824)! Learning rate decreased to 0.00069.
2024-12-02-03:11:07-root-INFO: grad norm: 227.552 221.018 54.137
2024-12-02-03:11:08-root-INFO: grad norm: 198.136 192.880 45.334
2024-12-02-03:11:09-root-INFO: grad norm: 181.277 175.787 44.276
2024-12-02-03:11:10-root-INFO: grad norm: 168.064 163.479 38.989
2024-12-02-03:11:10-root-INFO: Loss Change: 1615.077 -> 1589.973
2024-12-02-03:11:10-root-INFO: Regularization Change: 0.000 -> 0.078
2024-12-02-03:11:10-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-03:11:10-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:11:11-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-03:11:11-root-INFO: grad norm: 125.844 120.255 37.088
2024-12-02-03:11:11-root-INFO: Loss too large (1587.377->1644.382)! Learning rate decreased to 0.00342.
2024-12-02-03:11:12-root-INFO: Loss too large (1587.377->1618.553)! Learning rate decreased to 0.00274.
2024-12-02-03:11:12-root-INFO: Loss too large (1587.377->1601.082)! Learning rate decreased to 0.00219.
2024-12-02-03:11:12-root-INFO: Loss too large (1587.377->1590.190)! Learning rate decreased to 0.00175.
2024-12-02-03:11:13-root-INFO: grad norm: 263.454 256.120 61.730
2024-12-02-03:11:14-root-INFO: Loss too large (1583.996->1691.553)! Learning rate decreased to 0.00140.
2024-12-02-03:11:14-root-INFO: Loss too large (1583.996->1646.199)! Learning rate decreased to 0.00112.
2024-12-02-03:11:14-root-INFO: Loss too large (1583.996->1613.764)! Learning rate decreased to 0.00090.
2024-12-02-03:11:15-root-INFO: Loss too large (1583.996->1592.417)! Learning rate decreased to 0.00072.
2024-12-02-03:11:16-root-INFO: grad norm: 256.292 250.920 52.202
2024-12-02-03:11:17-root-INFO: grad norm: 248.914 242.057 58.022
2024-12-02-03:11:18-root-INFO: grad norm: 240.838 235.713 49.422
2024-12-02-03:11:18-root-INFO: Loss Change: 1587.377 -> 1568.517
2024-12-02-03:11:18-root-INFO: Regularization Change: 0.000 -> 0.136
2024-12-02-03:11:18-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-03:11:18-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-03:11:19-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-03:11:19-root-INFO: grad norm: 163.432 159.370 36.211
2024-12-02-03:11:19-root-INFO: Loss too large (1557.448->1782.568)! Learning rate decreased to 0.00356.
2024-12-02-03:11:20-root-INFO: Loss too large (1557.448->1719.644)! Learning rate decreased to 0.00285.
2024-12-02-03:11:20-root-INFO: Loss too large (1557.448->1668.403)! Learning rate decreased to 0.00228.
2024-12-02-03:11:20-root-INFO: Loss too large (1557.448->1628.599)! Learning rate decreased to 0.00182.
2024-12-02-03:11:21-root-INFO: Loss too large (1557.448->1599.397)! Learning rate decreased to 0.00146.
2024-12-02-03:11:21-root-INFO: Loss too large (1557.448->1579.264)! Learning rate decreased to 0.00117.
2024-12-02-03:11:21-root-INFO: Loss too large (1557.448->1566.267)! Learning rate decreased to 0.00093.
2024-12-02-03:11:22-root-INFO: Loss too large (1557.448->1558.464)! Learning rate decreased to 0.00075.
2024-12-02-03:11:23-root-INFO: grad norm: 173.116 168.564 39.437
2024-12-02-03:11:24-root-INFO: grad norm: 186.079 181.307 41.870
2024-12-02-03:11:25-root-INFO: grad norm: 198.655 194.213 41.775
2024-12-02-03:11:26-root-INFO: grad norm: 210.790 205.379 47.457
2024-12-02-03:11:26-root-INFO: Loss Change: 1557.448 -> 1545.515
2024-12-02-03:11:26-root-INFO: Regularization Change: 0.000 -> 0.079
2024-12-02-03:11:26-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-03:11:26-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:11:26-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-03:11:27-root-INFO: grad norm: 360.155 353.397 69.442
2024-12-02-03:11:27-root-INFO: Loss too large (1561.581->1810.058)! Learning rate decreased to 0.00371.
2024-12-02-03:11:28-root-INFO: Loss too large (1561.581->1789.234)! Learning rate decreased to 0.00297.
2024-12-02-03:11:28-root-INFO: Loss too large (1561.581->1763.135)! Learning rate decreased to 0.00237.
2024-12-02-03:11:28-root-INFO: Loss too large (1561.581->1730.575)! Learning rate decreased to 0.00190.
2024-12-02-03:11:28-root-INFO: Loss too large (1561.581->1691.735)! Learning rate decreased to 0.00152.
2024-12-02-03:11:29-root-INFO: Loss too large (1561.581->1648.699)! Learning rate decreased to 0.00122.
2024-12-02-03:11:29-root-INFO: Loss too large (1561.581->1606.629)! Learning rate decreased to 0.00097.
2024-12-02-03:11:29-root-INFO: Loss too large (1561.581->1571.803)! Learning rate decreased to 0.00078.
2024-12-02-03:11:30-root-INFO: grad norm: 322.574 314.237 72.863
2024-12-02-03:11:31-root-INFO: grad norm: 302.404 296.697 58.472
2024-12-02-03:11:32-root-INFO: grad norm: 285.312 277.807 65.009
2024-12-02-03:11:33-root-INFO: grad norm: 272.399 267.186 53.037
2024-12-02-03:11:34-root-INFO: Loss Change: 1561.581 -> 1527.005
2024-12-02-03:11:34-root-INFO: Regularization Change: 0.000 -> 0.135
2024-12-02-03:11:34-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-03:11:34-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:11:34-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-03:11:35-root-INFO: grad norm: 173.667 169.520 37.724
2024-12-02-03:11:35-root-INFO: Loss too large (1519.457->1743.379)! Learning rate decreased to 0.00386.
2024-12-02-03:11:35-root-INFO: Loss too large (1519.457->1681.003)! Learning rate decreased to 0.00309.
2024-12-02-03:11:36-root-INFO: Loss too large (1519.457->1630.406)! Learning rate decreased to 0.00247.
2024-12-02-03:11:36-root-INFO: Loss too large (1519.457->1590.778)! Learning rate decreased to 0.00198.
2024-12-02-03:11:36-root-INFO: Loss too large (1519.457->1561.284)! Learning rate decreased to 0.00158.
2024-12-02-03:11:37-root-INFO: Loss too large (1519.457->1540.629)! Learning rate decreased to 0.00126.
2024-12-02-03:11:37-root-INFO: Loss too large (1519.457->1527.117)! Learning rate decreased to 0.00101.
2024-12-02-03:11:38-root-INFO: grad norm: 239.483 234.214 49.960
2024-12-02-03:11:38-root-INFO: Loss too large (1518.943->1526.458)! Learning rate decreased to 0.00081.
2024-12-02-03:11:39-root-INFO: grad norm: 246.381 240.162 55.008
2024-12-02-03:11:40-root-INFO: grad norm: 247.641 242.742 49.014
2024-12-02-03:11:41-root-INFO: grad norm: 245.978 239.630 55.521
2024-12-02-03:11:42-root-INFO: Loss Change: 1519.457 -> 1505.495
2024-12-02-03:11:42-root-INFO: Regularization Change: 0.000 -> 0.118
2024-12-02-03:11:42-root-INFO: Undo step: 170
2024-12-02-03:11:42-root-INFO: Undo step: 171
2024-12-02-03:11:42-root-INFO: Undo step: 172
2024-12-02-03:11:42-root-INFO: Undo step: 173
2024-12-02-03:11:42-root-INFO: Undo step: 174
2024-12-02-03:11:42-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-03:11:43-root-INFO: grad norm: 613.832 580.862 198.467
2024-12-02-03:11:44-root-INFO: grad norm: 396.916 382.674 105.373
2024-12-02-03:11:44-root-INFO: Loss too large (2000.407->2057.208)! Learning rate decreased to 0.00316.
2024-12-02-03:11:44-root-INFO: Loss too large (2000.407->2020.656)! Learning rate decreased to 0.00253.
2024-12-02-03:11:45-root-INFO: grad norm: 320.652 310.323 80.732
2024-12-02-03:11:46-root-INFO: grad norm: 354.539 345.307 80.382
2024-12-02-03:11:47-root-INFO: Loss too large (1827.576->1937.055)! Learning rate decreased to 0.00202.
2024-12-02-03:11:47-root-INFO: Loss too large (1827.576->1898.574)! Learning rate decreased to 0.00162.
2024-12-02-03:11:47-root-INFO: Loss too large (1827.576->1863.166)! Learning rate decreased to 0.00129.
2024-12-02-03:11:48-root-INFO: Loss too large (1827.576->1833.822)! Learning rate decreased to 0.00104.
2024-12-02-03:11:49-root-INFO: grad norm: 306.140 299.760 62.174
2024-12-02-03:11:49-root-INFO: Loss Change: 2621.925 -> 1780.619
2024-12-02-03:11:49-root-INFO: Regularization Change: 0.000 -> 7.211
2024-12-02-03:11:49-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-03:11:49-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:11:50-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-03:11:50-root-INFO: grad norm: 382.385 375.513 72.166
2024-12-02-03:11:50-root-INFO: Loss too large (1785.615->1983.909)! Learning rate decreased to 0.00329.
2024-12-02-03:11:51-root-INFO: Loss too large (1785.615->1959.878)! Learning rate decreased to 0.00263.
2024-12-02-03:11:51-root-INFO: Loss too large (1785.615->1929.687)! Learning rate decreased to 0.00211.
2024-12-02-03:11:51-root-INFO: Loss too large (1785.615->1895.163)! Learning rate decreased to 0.00168.
2024-12-02-03:11:52-root-INFO: Loss too large (1785.615->1859.136)! Learning rate decreased to 0.00135.
2024-12-02-03:11:52-root-INFO: Loss too large (1785.615->1824.663)! Learning rate decreased to 0.00108.
2024-12-02-03:11:52-root-INFO: Loss too large (1785.615->1794.994)! Learning rate decreased to 0.00086.
2024-12-02-03:11:53-root-INFO: grad norm: 311.963 306.131 60.040
2024-12-02-03:11:54-root-INFO: grad norm: 260.143 254.520 53.796
2024-12-02-03:11:55-root-INFO: grad norm: 273.080 268.239 51.192
2024-12-02-03:11:56-root-INFO: grad norm: 303.707 298.585 55.543
2024-12-02-03:11:57-root-INFO: Loss too large (1729.477->1730.042)! Learning rate decreased to 0.00069.
2024-12-02-03:11:57-root-INFO: Loss Change: 1785.615 -> 1718.403
2024-12-02-03:11:57-root-INFO: Regularization Change: 0.000 -> 0.260
2024-12-02-03:11:57-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-03:11:57-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:11:58-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-03:11:58-root-INFO: grad norm: 137.023 133.583 30.511
2024-12-02-03:11:58-root-INFO: Loss too large (1696.929->1740.390)! Learning rate decreased to 0.00342.
2024-12-02-03:11:59-root-INFO: Loss too large (1696.929->1714.866)! Learning rate decreased to 0.00274.
2024-12-02-03:11:59-root-INFO: Loss too large (1696.929->1698.863)! Learning rate decreased to 0.00219.
2024-12-02-03:12:00-root-INFO: grad norm: 346.075 341.919 53.477
2024-12-02-03:12:00-root-INFO: Loss too large (1689.865->1833.103)! Learning rate decreased to 0.00175.
2024-12-02-03:12:01-root-INFO: Loss too large (1689.865->1800.043)! Learning rate decreased to 0.00140.
2024-12-02-03:12:01-root-INFO: Loss too large (1689.865->1765.681)! Learning rate decreased to 0.00112.
2024-12-02-03:12:01-root-INFO: Loss too large (1689.865->1733.072)! Learning rate decreased to 0.00090.
2024-12-02-03:12:02-root-INFO: Loss too large (1689.865->1705.715)! Learning rate decreased to 0.00072.
2024-12-02-03:12:03-root-INFO: grad norm: 331.175 325.931 58.702
2024-12-02-03:12:04-root-INFO: grad norm: 318.635 314.506 51.133
2024-12-02-03:12:05-root-INFO: grad norm: 315.903 311.028 55.286
2024-12-02-03:12:05-root-INFO: Loss Change: 1696.929 -> 1663.503
2024-12-02-03:12:05-root-INFO: Regularization Change: 0.000 -> 0.257
2024-12-02-03:12:05-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-03:12:05-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-03:12:06-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-03:12:06-root-INFO: grad norm: 416.575 411.643 63.911
2024-12-02-03:12:06-root-INFO: Loss too large (1673.567->1903.537)! Learning rate decreased to 0.00356.
2024-12-02-03:12:07-root-INFO: Loss too large (1673.567->1886.411)! Learning rate decreased to 0.00285.
2024-12-02-03:12:07-root-INFO: Loss too large (1673.567->1863.747)! Learning rate decreased to 0.00228.
2024-12-02-03:12:07-root-INFO: Loss too large (1673.567->1835.159)! Learning rate decreased to 0.00182.
2024-12-02-03:12:08-root-INFO: Loss too large (1673.567->1801.594)! Learning rate decreased to 0.00146.
2024-12-02-03:12:08-root-INFO: Loss too large (1673.567->1764.522)! Learning rate decreased to 0.00117.
2024-12-02-03:12:08-root-INFO: Loss too large (1673.567->1726.339)! Learning rate decreased to 0.00093.
2024-12-02-03:12:09-root-INFO: Loss too large (1673.567->1691.149)! Learning rate decreased to 0.00075.
2024-12-02-03:12:10-root-INFO: grad norm: 363.087 357.707 62.272
2024-12-02-03:12:11-root-INFO: grad norm: 332.581 328.234 53.595
2024-12-02-03:12:12-root-INFO: grad norm: 331.025 326.182 56.417
2024-12-02-03:12:13-root-INFO: grad norm: 329.748 325.578 52.272
2024-12-02-03:12:14-root-INFO: Loss Change: 1673.567 -> 1635.388
2024-12-02-03:12:14-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-02-03:12:14-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-03:12:14-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:12:14-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-03:12:14-root-INFO: grad norm: 223.256 220.202 36.796
2024-12-02-03:12:15-root-INFO: Loss too large (1610.684->1952.203)! Learning rate decreased to 0.00371.
2024-12-02-03:12:15-root-INFO: Loss too large (1610.684->1873.525)! Learning rate decreased to 0.00297.
2024-12-02-03:12:15-root-INFO: Loss too large (1610.684->1805.757)! Learning rate decreased to 0.00237.
2024-12-02-03:12:16-root-INFO: Loss too large (1610.684->1748.025)! Learning rate decreased to 0.00190.
2024-12-02-03:12:16-root-INFO: Loss too large (1610.684->1700.625)! Learning rate decreased to 0.00152.
2024-12-02-03:12:16-root-INFO: Loss too large (1610.684->1663.890)! Learning rate decreased to 0.00122.
2024-12-02-03:12:17-root-INFO: Loss too large (1610.684->1637.394)! Learning rate decreased to 0.00097.
2024-12-02-03:12:17-root-INFO: Loss too large (1610.684->1619.784)! Learning rate decreased to 0.00078.
2024-12-02-03:12:18-root-INFO: grad norm: 273.960 270.263 44.852
2024-12-02-03:12:18-root-INFO: Loss too large (1609.124->1610.873)! Learning rate decreased to 0.00062.
2024-12-02-03:12:19-root-INFO: grad norm: 235.377 231.970 39.901
2024-12-02-03:12:20-root-INFO: grad norm: 213.396 210.231 36.618
2024-12-02-03:12:21-root-INFO: grad norm: 200.961 197.836 35.302
2024-12-02-03:12:22-root-INFO: Loss Change: 1610.684 -> 1586.102
2024-12-02-03:12:22-root-INFO: Regularization Change: 0.000 -> 0.088
2024-12-02-03:12:22-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-03:12:22-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:12:22-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-03:12:23-root-INFO: grad norm: 329.744 325.475 52.890
2024-12-02-03:12:23-root-INFO: Loss too large (1594.812->1837.411)! Learning rate decreased to 0.00386.
2024-12-02-03:12:23-root-INFO: Loss too large (1594.812->1817.034)! Learning rate decreased to 0.00309.
2024-12-02-03:12:24-root-INFO: Loss too large (1594.812->1791.895)! Learning rate decreased to 0.00247.
2024-12-02-03:12:24-root-INFO: Loss too large (1594.812->1761.982)! Learning rate decreased to 0.00198.
2024-12-02-03:12:24-root-INFO: Loss too large (1594.812->1728.091)! Learning rate decreased to 0.00158.
2024-12-02-03:12:25-root-INFO: Loss too large (1594.812->1691.587)! Learning rate decreased to 0.00126.
2024-12-02-03:12:25-root-INFO: Loss too large (1594.812->1655.227)! Learning rate decreased to 0.00101.
2024-12-02-03:12:25-root-INFO: Loss too large (1594.812->1623.104)! Learning rate decreased to 0.00081.
2024-12-02-03:12:26-root-INFO: Loss too large (1594.812->1598.776)! Learning rate decreased to 0.00065.
2024-12-02-03:12:27-root-INFO: grad norm: 287.694 284.081 45.450
2024-12-02-03:12:28-root-INFO: grad norm: 267.408 263.949 42.871
2024-12-02-03:12:29-root-INFO: grad norm: 257.224 253.818 41.723
2024-12-02-03:12:30-root-INFO: grad norm: 251.701 248.585 39.488
2024-12-02-03:12:31-root-INFO: Loss Change: 1594.812 -> 1562.758
2024-12-02-03:12:31-root-INFO: Regularization Change: 0.000 -> 0.107
2024-12-02-03:12:31-root-INFO: Undo step: 170
2024-12-02-03:12:31-root-INFO: Undo step: 171
2024-12-02-03:12:31-root-INFO: Undo step: 172
2024-12-02-03:12:31-root-INFO: Undo step: 173
2024-12-02-03:12:31-root-INFO: Undo step: 174
2024-12-02-03:12:31-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-03:12:31-root-INFO: grad norm: 632.654 603.768 188.985
2024-12-02-03:12:32-root-INFO: grad norm: 804.616 788.805 158.724
2024-12-02-03:12:33-root-INFO: grad norm: 796.259 776.634 175.693
2024-12-02-03:12:34-root-INFO: Loss too large (2516.043->2736.819)! Learning rate decreased to 0.00316.
2024-12-02-03:12:35-root-INFO: grad norm: 491.219 480.513 101.996
2024-12-02-03:12:36-root-INFO: grad norm: 187.312 181.057 48.002
2024-12-02-03:12:36-root-INFO: Loss Change: 2765.555 -> 1893.925
2024-12-02-03:12:36-root-INFO: Regularization Change: 0.000 -> 14.336
2024-12-02-03:12:36-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-03:12:36-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:12:37-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-03:12:37-root-INFO: grad norm: 185.590 181.710 37.752
2024-12-02-03:12:38-root-INFO: grad norm: 124.813 118.156 40.219
2024-12-02-03:12:39-root-INFO: grad norm: 149.752 142.928 44.690
2024-12-02-03:12:40-root-INFO: grad norm: 301.908 282.546 106.378
2024-12-02-03:12:40-root-INFO: Loss too large (1741.314->1853.294)! Learning rate decreased to 0.00329.
2024-12-02-03:12:41-root-INFO: Loss too large (1741.314->1792.686)! Learning rate decreased to 0.00263.
2024-12-02-03:12:41-root-INFO: Loss too large (1741.314->1743.687)! Learning rate decreased to 0.00211.
2024-12-02-03:12:42-root-INFO: grad norm: 251.010 240.745 71.049
2024-12-02-03:12:43-root-INFO: Loss Change: 1886.154 -> 1677.833
2024-12-02-03:12:43-root-INFO: Regularization Change: 0.000 -> 3.315
2024-12-02-03:12:43-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-03:12:43-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-03:12:43-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-03:12:43-root-INFO: grad norm: 334.399 319.610 98.348
2024-12-02-03:12:44-root-INFO: Loss too large (1700.877->1859.130)! Learning rate decreased to 0.00342.
2024-12-02-03:12:44-root-INFO: Loss too large (1700.877->1797.866)! Learning rate decreased to 0.00274.
2024-12-02-03:12:44-root-INFO: Loss too large (1700.877->1741.373)! Learning rate decreased to 0.00219.
2024-12-02-03:12:45-root-INFO: grad norm: 282.058 270.853 78.710
2024-12-02-03:12:46-root-INFO: grad norm: 253.460 243.291 71.074
2024-12-02-03:12:47-root-INFO: Loss too large (1642.877->1655.706)! Learning rate decreased to 0.00175.
2024-12-02-03:12:48-root-INFO: grad norm: 230.376 224.070 53.531
2024-12-02-03:12:49-root-INFO: grad norm: 250.514 244.120 56.238
2024-12-02-03:12:49-root-INFO: Loss too large (1615.904->1624.932)! Learning rate decreased to 0.00140.
2024-12-02-03:12:50-root-INFO: Loss Change: 1700.877 -> 1607.546
2024-12-02-03:12:50-root-INFO: Regularization Change: 0.000 -> 0.707
2024-12-02-03:12:50-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-03:12:50-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-03:12:50-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-03:12:50-root-INFO: grad norm: 184.304 180.933 35.089
2024-12-02-03:12:51-root-INFO: Loss too large (1591.959->1765.617)! Learning rate decreased to 0.00356.
2024-12-02-03:12:51-root-INFO: Loss too large (1591.959->1701.418)! Learning rate decreased to 0.00285.
2024-12-02-03:12:51-root-INFO: Loss too large (1591.959->1651.409)! Learning rate decreased to 0.00228.
2024-12-02-03:12:52-root-INFO: Loss too large (1591.959->1615.685)! Learning rate decreased to 0.00182.
2024-12-02-03:12:52-root-INFO: Loss too large (1591.959->1592.962)! Learning rate decreased to 0.00146.
2024-12-02-03:12:53-root-INFO: grad norm: 194.792 189.653 44.451
2024-12-02-03:12:53-root-INFO: Loss too large (1580.533->1581.283)! Learning rate decreased to 0.00117.
2024-12-02-03:12:54-root-INFO: grad norm: 169.731 166.640 32.247
2024-12-02-03:12:55-root-INFO: grad norm: 159.329 155.219 35.956
2024-12-02-03:12:56-root-INFO: grad norm: 165.564 162.598 31.195
2024-12-02-03:12:57-root-INFO: Loss Change: 1591.959 -> 1555.500
2024-12-02-03:12:57-root-INFO: Regularization Change: 0.000 -> 0.229
2024-12-02-03:12:57-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-03:12:57-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:12:57-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-03:12:58-root-INFO: grad norm: 302.980 298.574 51.483
2024-12-02-03:12:58-root-INFO: Loss too large (1566.924->1756.726)! Learning rate decreased to 0.00371.
2024-12-02-03:12:58-root-INFO: Loss too large (1566.924->1723.174)! Learning rate decreased to 0.00297.
2024-12-02-03:12:59-root-INFO: Loss too large (1566.924->1686.943)! Learning rate decreased to 0.00237.
2024-12-02-03:12:59-root-INFO: Loss too large (1566.924->1650.327)! Learning rate decreased to 0.00190.
2024-12-02-03:12:59-root-INFO: Loss too large (1566.924->1615.593)! Learning rate decreased to 0.00152.
2024-12-02-03:13:00-root-INFO: Loss too large (1566.924->1585.379)! Learning rate decreased to 0.00122.
2024-12-02-03:13:01-root-INFO: grad norm: 263.505 258.472 51.252
2024-12-02-03:13:02-root-INFO: grad norm: 232.530 228.561 42.782
2024-12-02-03:13:03-root-INFO: grad norm: 235.593 231.438 44.052
2024-12-02-03:13:04-root-INFO: grad norm: 241.539 237.881 41.878
2024-12-02-03:13:04-root-INFO: Loss too large (1535.527->1536.084)! Learning rate decreased to 0.00097.
2024-12-02-03:13:05-root-INFO: Loss Change: 1566.924 -> 1525.173
2024-12-02-03:13:05-root-INFO: Regularization Change: 0.000 -> 0.210
2024-12-02-03:13:05-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-03:13:05-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:13:05-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-03:13:05-root-INFO: grad norm: 118.118 114.815 27.739
2024-12-02-03:13:06-root-INFO: Loss too large (1514.115->1592.868)! Learning rate decreased to 0.00386.
2024-12-02-03:13:06-root-INFO: Loss too large (1514.115->1561.181)! Learning rate decreased to 0.00309.
2024-12-02-03:13:06-root-INFO: Loss too large (1514.115->1538.763)! Learning rate decreased to 0.00247.
2024-12-02-03:13:07-root-INFO: Loss too large (1514.115->1523.982)! Learning rate decreased to 0.00198.
2024-12-02-03:13:07-root-INFO: Loss too large (1514.115->1515.015)! Learning rate decreased to 0.00158.
2024-12-02-03:13:08-root-INFO: grad norm: 185.148 181.927 34.386
2024-12-02-03:13:08-root-INFO: Loss too large (1510.115->1527.328)! Learning rate decreased to 0.00126.
2024-12-02-03:13:09-root-INFO: Loss too large (1510.115->1514.151)! Learning rate decreased to 0.00101.
2024-12-02-03:13:10-root-INFO: grad norm: 183.770 180.882 32.452
2024-12-02-03:13:11-root-INFO: grad norm: 185.680 182.789 32.637
2024-12-02-03:13:12-root-INFO: grad norm: 190.803 187.874 33.303
2024-12-02-03:13:12-root-INFO: Loss Change: 1514.115 -> 1495.549
2024-12-02-03:13:12-root-INFO: Regularization Change: 0.000 -> 0.156
2024-12-02-03:13:12-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-03:13:12-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:13:13-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-03:13:13-root-INFO: grad norm: 286.375 282.613 46.268
2024-12-02-03:13:13-root-INFO: Loss too large (1506.048->1713.947)! Learning rate decreased to 0.00401.
2024-12-02-03:13:14-root-INFO: Loss too large (1506.048->1681.960)! Learning rate decreased to 0.00321.
2024-12-02-03:13:14-root-INFO: Loss too large (1506.048->1646.140)! Learning rate decreased to 0.00257.
2024-12-02-03:13:14-root-INFO: Loss too large (1506.048->1608.624)! Learning rate decreased to 0.00206.
2024-12-02-03:13:15-root-INFO: Loss too large (1506.048->1571.729)! Learning rate decreased to 0.00164.
2024-12-02-03:13:15-root-INFO: Loss too large (1506.048->1538.365)! Learning rate decreased to 0.00132.
2024-12-02-03:13:15-root-INFO: Loss too large (1506.048->1511.605)! Learning rate decreased to 0.00105.
2024-12-02-03:13:16-root-INFO: grad norm: 239.774 236.342 40.422
2024-12-02-03:13:17-root-INFO: grad norm: 217.332 214.252 36.456
2024-12-02-03:13:18-root-INFO: grad norm: 209.393 206.404 35.256
2024-12-02-03:13:19-root-INFO: grad norm: 204.719 201.921 33.731
2024-12-02-03:13:20-root-INFO: Loss Change: 1506.048 -> 1472.092
2024-12-02-03:13:20-root-INFO: Regularization Change: 0.000 -> 0.175
2024-12-02-03:13:20-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-03:13:20-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:13:20-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-03:13:21-root-INFO: grad norm: 118.595 116.440 22.503
2024-12-02-03:13:21-root-INFO: Loss too large (1461.286->1576.059)! Learning rate decreased to 0.00417.
2024-12-02-03:13:22-root-INFO: Loss too large (1461.286->1539.148)! Learning rate decreased to 0.00334.
2024-12-02-03:13:22-root-INFO: Loss too large (1461.286->1510.819)! Learning rate decreased to 0.00267.
2024-12-02-03:13:22-root-INFO: Loss too large (1461.286->1490.145)! Learning rate decreased to 0.00214.
2024-12-02-03:13:22-root-INFO: Loss too large (1461.286->1475.910)! Learning rate decreased to 0.00171.
2024-12-02-03:13:23-root-INFO: Loss too large (1461.286->1466.726)! Learning rate decreased to 0.00137.
2024-12-02-03:13:24-root-INFO: grad norm: 179.893 177.460 29.486
2024-12-02-03:13:24-root-INFO: Loss too large (1461.231->1469.762)! Learning rate decreased to 0.00109.
2024-12-02-03:13:25-root-INFO: grad norm: 206.076 203.201 34.305
2024-12-02-03:13:25-root-INFO: Loss too large (1459.764->1459.900)! Learning rate decreased to 0.00088.
2024-12-02-03:13:26-root-INFO: grad norm: 158.246 155.973 26.726
2024-12-02-03:13:27-root-INFO: grad norm: 127.686 125.614 22.913
2024-12-02-03:13:28-root-INFO: Loss Change: 1461.286 -> 1443.678
2024-12-02-03:13:28-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-03:13:28-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-03:13:28-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-03:13:28-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-03:13:29-root-INFO: grad norm: 216.657 213.388 37.499
2024-12-02-03:13:29-root-INFO: Loss too large (1453.059->1657.167)! Learning rate decreased to 0.00434.
2024-12-02-03:13:29-root-INFO: Loss too large (1453.059->1622.609)! Learning rate decreased to 0.00347.
2024-12-02-03:13:30-root-INFO: Loss too large (1453.059->1585.750)! Learning rate decreased to 0.00278.
2024-12-02-03:13:30-root-INFO: Loss too large (1453.059->1548.602)! Learning rate decreased to 0.00222.
2024-12-02-03:13:30-root-INFO: Loss too large (1453.059->1513.849)! Learning rate decreased to 0.00178.
2024-12-02-03:13:31-root-INFO: Loss too large (1453.059->1484.485)! Learning rate decreased to 0.00142.
2024-12-02-03:13:31-root-INFO: Loss too large (1453.059->1462.668)! Learning rate decreased to 0.00114.
2024-12-02-03:13:32-root-INFO: grad norm: 234.920 231.787 38.239
2024-12-02-03:13:33-root-INFO: grad norm: 242.224 239.505 36.192
2024-12-02-03:13:34-root-INFO: grad norm: 243.943 240.565 40.456
2024-12-02-03:13:35-root-INFO: grad norm: 239.335 236.705 35.380
2024-12-02-03:13:36-root-INFO: Loss Change: 1453.059 -> 1433.617
2024-12-02-03:13:36-root-INFO: Regularization Change: 0.000 -> 0.180
2024-12-02-03:13:36-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-03:13:36-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:13:36-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-03:13:36-root-INFO: grad norm: 138.488 136.206 25.034
2024-12-02-03:13:37-root-INFO: Loss too large (1421.744->1563.255)! Learning rate decreased to 0.00451.
2024-12-02-03:13:37-root-INFO: Loss too large (1421.744->1519.510)! Learning rate decreased to 0.00361.
2024-12-02-03:13:37-root-INFO: Loss too large (1421.744->1485.336)! Learning rate decreased to 0.00289.
2024-12-02-03:13:38-root-INFO: Loss too large (1421.744->1459.643)! Learning rate decreased to 0.00231.
2024-12-02-03:13:38-root-INFO: Loss too large (1421.744->1441.329)! Learning rate decreased to 0.00185.
2024-12-02-03:13:39-root-INFO: Loss too large (1421.744->1429.097)! Learning rate decreased to 0.00148.
2024-12-02-03:13:39-root-INFO: grad norm: 189.852 187.333 30.824
2024-12-02-03:13:40-root-INFO: Loss too large (1421.540->1427.839)! Learning rate decreased to 0.00118.
2024-12-02-03:13:41-root-INFO: grad norm: 198.166 195.490 32.459
2024-12-02-03:13:42-root-INFO: grad norm: 200.503 198.151 30.620
2024-12-02-03:13:43-root-INFO: grad norm: 201.335 198.603 33.057
2024-12-02-03:13:44-root-INFO: Loss Change: 1421.744 -> 1408.756
2024-12-02-03:13:44-root-INFO: Regularization Change: 0.000 -> 0.170
2024-12-02-03:13:44-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-03:13:44-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:13:44-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-03:13:44-root-INFO: grad norm: 246.797 244.048 36.735
2024-12-02-03:13:45-root-INFO: Loss too large (1419.353->1635.078)! Learning rate decreased to 0.00469.
2024-12-02-03:13:45-root-INFO: Loss too large (1419.353->1602.988)! Learning rate decreased to 0.00375.
2024-12-02-03:13:45-root-INFO: Loss too large (1419.353->1564.997)! Learning rate decreased to 0.00300.
2024-12-02-03:13:46-root-INFO: Loss too large (1419.353->1523.755)! Learning rate decreased to 0.00240.
2024-12-02-03:13:46-root-INFO: Loss too large (1419.353->1482.682)! Learning rate decreased to 0.00192.
2024-12-02-03:13:46-root-INFO: Loss too large (1419.353->1446.207)! Learning rate decreased to 0.00154.
2024-12-02-03:13:47-root-INFO: grad norm: 294.057 289.493 51.604
2024-12-02-03:13:48-root-INFO: Loss too large (1418.655->1427.155)! Learning rate decreased to 0.00123.
2024-12-02-03:13:49-root-INFO: grad norm: 225.261 222.656 34.161
2024-12-02-03:13:50-root-INFO: grad norm: 177.048 174.598 29.352
2024-12-02-03:13:51-root-INFO: grad norm: 163.707 161.430 27.212
2024-12-02-03:13:51-root-INFO: Loss Change: 1419.353 -> 1381.335
2024-12-02-03:13:51-root-INFO: Regularization Change: 0.000 -> 0.204
2024-12-02-03:13:51-root-INFO: Undo step: 165
2024-12-02-03:13:51-root-INFO: Undo step: 166
2024-12-02-03:13:51-root-INFO: Undo step: 167
2024-12-02-03:13:51-root-INFO: Undo step: 168
2024-12-02-03:13:51-root-INFO: Undo step: 169
2024-12-02-03:13:52-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-03:13:52-root-INFO: grad norm: 635.027 620.093 136.907
2024-12-02-03:13:53-root-INFO: grad norm: 419.546 412.208 78.124
2024-12-02-03:13:54-root-INFO: grad norm: 497.562 484.432 113.549
2024-12-02-03:13:55-root-INFO: grad norm: 484.523 474.490 98.092
2024-12-02-03:13:56-root-INFO: grad norm: 355.927 345.584 85.183
2024-12-02-03:13:57-root-INFO: Loss Change: 2461.435 -> 1621.603
2024-12-02-03:13:57-root-INFO: Regularization Change: 0.000 -> 15.565
2024-12-02-03:13:57-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-03:13:57-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:13:57-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-03:13:57-root-INFO: grad norm: 279.563 266.941 83.055
2024-12-02-03:13:58-root-INFO: Loss too large (1628.095->1651.372)! Learning rate decreased to 0.00401.
2024-12-02-03:13:59-root-INFO: grad norm: 302.409 293.739 71.894
2024-12-02-03:13:59-root-INFO: Loss too large (1594.182->1634.967)! Learning rate decreased to 0.00321.
2024-12-02-03:14:00-root-INFO: grad norm: 316.332 312.797 47.162
2024-12-02-03:14:00-root-INFO: Loss too large (1561.720->1567.187)! Learning rate decreased to 0.00257.
2024-12-02-03:14:01-root-INFO: grad norm: 227.914 222.497 49.399
2024-12-02-03:14:02-root-INFO: grad norm: 179.965 174.812 42.757
2024-12-02-03:14:03-root-INFO: Loss Change: 1628.095 -> 1470.280
2024-12-02-03:14:03-root-INFO: Regularization Change: 0.000 -> 1.851
2024-12-02-03:14:03-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-03:14:03-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:14:03-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-03:14:04-root-INFO: grad norm: 175.024 171.965 32.582
2024-12-02-03:14:04-root-INFO: Loss too large (1456.558->1609.489)! Learning rate decreased to 0.00417.
2024-12-02-03:14:04-root-INFO: Loss too large (1456.558->1547.268)! Learning rate decreased to 0.00334.
2024-12-02-03:14:05-root-INFO: Loss too large (1456.558->1501.446)! Learning rate decreased to 0.00267.
2024-12-02-03:14:05-root-INFO: Loss too large (1456.558->1470.563)! Learning rate decreased to 0.00214.
2024-12-02-03:14:06-root-INFO: grad norm: 210.535 208.128 31.746
2024-12-02-03:14:06-root-INFO: Loss too large (1451.827->1457.674)! Learning rate decreased to 0.00171.
2024-12-02-03:14:07-root-INFO: grad norm: 189.649 186.568 34.043
2024-12-02-03:14:08-root-INFO: grad norm: 184.970 182.696 28.911
2024-12-02-03:14:09-root-INFO: grad norm: 195.273 192.229 34.341
2024-12-02-03:14:10-root-INFO: Loss Change: 1456.558 -> 1424.885
2024-12-02-03:14:10-root-INFO: Regularization Change: 0.000 -> 0.404
2024-12-02-03:14:10-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-03:14:10-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-03:14:10-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-03:14:10-root-INFO: grad norm: 260.930 258.292 37.008
2024-12-02-03:14:11-root-INFO: Loss too large (1434.746->1622.526)! Learning rate decreased to 0.00434.
2024-12-02-03:14:11-root-INFO: Loss too large (1434.746->1580.092)! Learning rate decreased to 0.00347.
2024-12-02-03:14:12-root-INFO: Loss too large (1434.746->1533.711)! Learning rate decreased to 0.00278.
2024-12-02-03:14:12-root-INFO: Loss too large (1434.746->1488.416)! Learning rate decreased to 0.00222.
2024-12-02-03:14:12-root-INFO: Loss too large (1434.746->1449.693)! Learning rate decreased to 0.00178.
2024-12-02-03:14:13-root-INFO: grad norm: 241.603 237.998 41.581
2024-12-02-03:14:14-root-INFO: grad norm: 229.725 227.435 32.357
2024-12-02-03:14:15-root-INFO: grad norm: 225.414 222.069 38.690
2024-12-02-03:14:16-root-INFO: grad norm: 222.320 220.119 31.206
2024-12-02-03:14:17-root-INFO: Loss Change: 1434.746 -> 1394.590
2024-12-02-03:14:17-root-INFO: Regularization Change: 0.000 -> 0.432
2024-12-02-03:14:17-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-03:14:17-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:14:17-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-03:14:18-root-INFO: grad norm: 176.492 173.719 31.161
2024-12-02-03:14:18-root-INFO: Loss too large (1379.989->1562.309)! Learning rate decreased to 0.00451.
2024-12-02-03:14:18-root-INFO: Loss too large (1379.989->1498.615)! Learning rate decreased to 0.00361.
2024-12-02-03:14:19-root-INFO: Loss too large (1379.989->1449.707)! Learning rate decreased to 0.00289.
2024-12-02-03:14:19-root-INFO: Loss too large (1379.989->1414.376)! Learning rate decreased to 0.00231.
2024-12-02-03:14:19-root-INFO: Loss too large (1379.989->1390.750)! Learning rate decreased to 0.00185.
2024-12-02-03:14:20-root-INFO: grad norm: 188.804 186.690 28.175
2024-12-02-03:14:21-root-INFO: grad norm: 218.456 215.358 36.655
2024-12-02-03:14:22-root-INFO: Loss too large (1373.229->1377.646)! Learning rate decreased to 0.00148.
2024-12-02-03:14:23-root-INFO: grad norm: 171.970 170.009 25.896
2024-12-02-03:14:24-root-INFO: grad norm: 141.295 138.889 25.961
2024-12-02-03:14:24-root-INFO: Loss Change: 1379.989 -> 1348.258
2024-12-02-03:14:24-root-INFO: Regularization Change: 0.000 -> 0.255
2024-12-02-03:14:24-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-03:14:24-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:14:25-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-03:14:25-root-INFO: grad norm: 178.383 176.309 27.122
2024-12-02-03:14:25-root-INFO: Loss too large (1352.272->1533.509)! Learning rate decreased to 0.00469.
2024-12-02-03:14:26-root-INFO: Loss too large (1352.272->1489.687)! Learning rate decreased to 0.00375.
2024-12-02-03:14:26-root-INFO: Loss too large (1352.272->1445.080)! Learning rate decreased to 0.00300.
2024-12-02-03:14:26-root-INFO: Loss too large (1352.272->1404.784)! Learning rate decreased to 0.00240.
2024-12-02-03:14:27-root-INFO: Loss too large (1352.272->1373.482)! Learning rate decreased to 0.00192.
2024-12-02-03:14:27-root-INFO: Loss too large (1352.272->1352.943)! Learning rate decreased to 0.00154.
2024-12-02-03:14:28-root-INFO: grad norm: 158.563 156.093 27.879
2024-12-02-03:14:29-root-INFO: grad norm: 152.139 150.314 23.492
2024-12-02-03:14:30-root-INFO: grad norm: 149.251 146.900 26.384
2024-12-02-03:14:31-root-INFO: grad norm: 149.528 147.795 22.697
2024-12-02-03:14:32-root-INFO: Loss Change: 1352.272 -> 1324.513
2024-12-02-03:14:32-root-INFO: Regularization Change: 0.000 -> 0.222
2024-12-02-03:14:32-root-INFO: Undo step: 165
2024-12-02-03:14:32-root-INFO: Undo step: 166
2024-12-02-03:14:32-root-INFO: Undo step: 167
2024-12-02-03:14:32-root-INFO: Undo step: 168
2024-12-02-03:14:32-root-INFO: Undo step: 169
2024-12-02-03:14:32-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-03:14:32-root-INFO: grad norm: 494.503 480.326 117.558
2024-12-02-03:14:33-root-INFO: grad norm: 382.008 373.373 80.762
2024-12-02-03:14:34-root-INFO: grad norm: 313.449 304.898 72.715
2024-12-02-03:14:35-root-INFO: grad norm: 441.050 435.634 68.911
2024-12-02-03:14:36-root-INFO: Loss too large (1761.443->1846.258)! Learning rate decreased to 0.00386.
2024-12-02-03:14:37-root-INFO: grad norm: 334.071 323.394 83.783
2024-12-02-03:14:37-root-INFO: Loss Change: 2257.933 -> 1558.809
2024-12-02-03:14:37-root-INFO: Regularization Change: 0.000 -> 11.624
2024-12-02-03:14:37-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-03:14:37-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:14:38-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-03:14:38-root-INFO: grad norm: 317.321 311.944 58.170
2024-12-02-03:14:38-root-INFO: Loss too large (1571.550->1715.569)! Learning rate decreased to 0.00401.
2024-12-02-03:14:39-root-INFO: Loss too large (1571.550->1634.139)! Learning rate decreased to 0.00321.
2024-12-02-03:14:40-root-INFO: grad norm: 304.152 292.832 82.208
2024-12-02-03:14:41-root-INFO: grad norm: 280.044 275.047 52.671
2024-12-02-03:14:42-root-INFO: grad norm: 270.988 259.750 77.230
2024-12-02-03:14:43-root-INFO: grad norm: 257.914 252.383 53.125
2024-12-02-03:14:43-root-INFO: Loss Change: 1571.550 -> 1468.567
2024-12-02-03:14:43-root-INFO: Regularization Change: 0.000 -> 2.125
2024-12-02-03:14:43-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-03:14:43-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-03:14:44-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-03:14:44-root-INFO: grad norm: 254.578 243.573 74.041
2024-12-02-03:14:44-root-INFO: Loss too large (1450.267->1595.656)! Learning rate decreased to 0.00417.
2024-12-02-03:14:45-root-INFO: Loss too large (1450.267->1515.521)! Learning rate decreased to 0.00334.
2024-12-02-03:14:45-root-INFO: Loss too large (1450.267->1460.094)! Learning rate decreased to 0.00267.
2024-12-02-03:14:46-root-INFO: grad norm: 207.567 202.654 44.896
2024-12-02-03:14:47-root-INFO: grad norm: 189.245 180.784 55.955
2024-12-02-03:14:48-root-INFO: grad norm: 189.895 185.242 41.778
2024-12-02-03:14:49-root-INFO: grad norm: 200.659 191.655 59.433
2024-12-02-03:14:49-root-INFO: Loss too large (1381.292->1381.592)! Learning rate decreased to 0.00214.
2024-12-02-03:14:50-root-INFO: Loss Change: 1450.267 -> 1365.812
2024-12-02-03:14:50-root-INFO: Regularization Change: 0.000 -> 0.915
2024-12-02-03:14:50-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-03:14:50-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-03:14:50-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-03:14:51-root-INFO: grad norm: 188.873 183.522 44.641
2024-12-02-03:14:51-root-INFO: Loss too large (1367.143->1487.026)! Learning rate decreased to 0.00434.
2024-12-02-03:14:51-root-INFO: Loss too large (1367.143->1427.484)! Learning rate decreased to 0.00347.
2024-12-02-03:14:52-root-INFO: Loss too large (1367.143->1382.423)! Learning rate decreased to 0.00278.
2024-12-02-03:14:53-root-INFO: grad norm: 208.802 199.862 60.443
2024-12-02-03:14:53-root-INFO: Loss too large (1354.816->1360.195)! Learning rate decreased to 0.00222.
2024-12-02-03:14:54-root-INFO: grad norm: 161.975 157.580 37.477
2024-12-02-03:14:55-root-INFO: grad norm: 122.521 117.112 36.003
2024-12-02-03:14:56-root-INFO: grad norm: 109.754 106.399 26.930
2024-12-02-03:14:57-root-INFO: Loss Change: 1367.143 -> 1307.842
2024-12-02-03:14:57-root-INFO: Regularization Change: 0.000 -> 0.561
2024-12-02-03:14:57-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-03:14:57-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:14:57-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-03:14:57-root-INFO: grad norm: 71.178 68.036 20.916
2024-12-02-03:14:58-root-INFO: grad norm: 131.711 128.616 28.383
2024-12-02-03:14:59-root-INFO: Loss too large (1288.024->1372.886)! Learning rate decreased to 0.00451.
2024-12-02-03:14:59-root-INFO: Loss too large (1288.024->1331.087)! Learning rate decreased to 0.00361.
2024-12-02-03:14:59-root-INFO: Loss too large (1288.024->1302.699)! Learning rate decreased to 0.00289.
2024-12-02-03:15:00-root-INFO: grad norm: 184.919 177.559 51.651
2024-12-02-03:15:01-root-INFO: Loss too large (1286.150->1300.868)! Learning rate decreased to 0.00231.
2024-12-02-03:15:02-root-INFO: grad norm: 163.992 159.906 36.377
2024-12-02-03:15:03-root-INFO: grad norm: 135.395 129.838 38.389
2024-12-02-03:15:03-root-INFO: Loss Change: 1295.864 -> 1263.537
2024-12-02-03:15:03-root-INFO: Regularization Change: 0.000 -> 0.672
2024-12-02-03:15:03-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-03:15:03-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:15:04-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-03:15:04-root-INFO: grad norm: 156.055 151.835 36.042
2024-12-02-03:15:04-root-INFO: Loss too large (1265.980->1394.352)! Learning rate decreased to 0.00469.
2024-12-02-03:15:05-root-INFO: Loss too large (1265.980->1334.237)! Learning rate decreased to 0.00375.
2024-12-02-03:15:05-root-INFO: Loss too large (1265.980->1289.836)! Learning rate decreased to 0.00300.
2024-12-02-03:15:06-root-INFO: grad norm: 213.898 205.424 59.608
2024-12-02-03:15:06-root-INFO: Loss too large (1263.392->1284.418)! Learning rate decreased to 0.00240.
2024-12-02-03:15:07-root-INFO: grad norm: 174.657 170.379 38.419
2024-12-02-03:15:08-root-INFO: grad norm: 118.946 114.114 33.558
2024-12-02-03:15:09-root-INFO: grad norm: 107.626 104.504 25.734
2024-12-02-03:15:10-root-INFO: Loss Change: 1265.980 -> 1224.729
2024-12-02-03:15:10-root-INFO: Regularization Change: 0.000 -> 0.454
2024-12-02-03:15:10-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-03:15:10-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:15:10-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-03:15:11-root-INFO: grad norm: 63.797 61.196 18.030
2024-12-02-03:15:12-root-INFO: grad norm: 117.934 115.015 26.074
2024-12-02-03:15:12-root-INFO: Loss too large (1210.468->1291.820)! Learning rate decreased to 0.00488.
2024-12-02-03:15:12-root-INFO: Loss too large (1210.468->1248.563)! Learning rate decreased to 0.00390.
2024-12-02-03:15:13-root-INFO: Loss too large (1210.468->1221.639)! Learning rate decreased to 0.00312.
2024-12-02-03:15:14-root-INFO: grad norm: 159.966 154.060 43.064
2024-12-02-03:15:14-root-INFO: Loss too large (1207.101->1218.941)! Learning rate decreased to 0.00250.
2024-12-02-03:15:15-root-INFO: grad norm: 139.495 135.871 31.591
2024-12-02-03:15:16-root-INFO: grad norm: 109.698 105.563 29.834
2024-12-02-03:15:17-root-INFO: Loss Change: 1215.757 -> 1187.698
2024-12-02-03:15:17-root-INFO: Regularization Change: 0.000 -> 0.593
2024-12-02-03:15:17-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-03:15:17-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-03:15:17-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-03:15:17-root-INFO: grad norm: 122.892 119.433 28.951
2024-12-02-03:15:18-root-INFO: Loss too large (1185.566->1281.879)! Learning rate decreased to 0.00507.
2024-12-02-03:15:18-root-INFO: Loss too large (1185.566->1229.930)! Learning rate decreased to 0.00405.
2024-12-02-03:15:18-root-INFO: Loss too large (1185.566->1197.878)! Learning rate decreased to 0.00324.
2024-12-02-03:15:19-root-INFO: grad norm: 160.516 154.846 42.285
2024-12-02-03:15:20-root-INFO: Loss too large (1180.989->1192.130)! Learning rate decreased to 0.00259.
2024-12-02-03:15:21-root-INFO: grad norm: 133.537 130.135 29.950
2024-12-02-03:15:22-root-INFO: grad norm: 95.720 92.247 25.550
2024-12-02-03:15:23-root-INFO: grad norm: 85.718 83.201 20.621
2024-12-02-03:15:23-root-INFO: Loss Change: 1185.566 -> 1152.810
2024-12-02-03:15:23-root-INFO: Regularization Change: 0.000 -> 0.393
2024-12-02-03:15:23-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-03:15:23-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:15:24-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-03:15:24-root-INFO: grad norm: 61.599 59.407 16.287
2024-12-02-03:15:25-root-INFO: grad norm: 70.962 69.678 13.439
2024-12-02-03:15:26-root-INFO: grad norm: 137.917 134.818 29.069
2024-12-02-03:15:26-root-INFO: Loss too large (1139.731->1207.533)! Learning rate decreased to 0.00527.
2024-12-02-03:15:27-root-INFO: Loss too large (1139.731->1172.693)! Learning rate decreased to 0.00421.
2024-12-02-03:15:27-root-INFO: Loss too large (1139.731->1150.100)! Learning rate decreased to 0.00337.
2024-12-02-03:15:28-root-INFO: grad norm: 132.546 129.517 28.177
2024-12-02-03:15:29-root-INFO: grad norm: 171.248 165.702 43.228
2024-12-02-03:15:29-root-INFO: Loss too large (1130.909->1146.990)! Learning rate decreased to 0.00270.
2024-12-02-03:15:30-root-INFO: Loss Change: 1151.760 -> 1130.797
2024-12-02-03:15:30-root-INFO: Regularization Change: 0.000 -> 0.943
2024-12-02-03:15:30-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-03:15:30-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:15:30-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-03:15:31-root-INFO: grad norm: 151.476 148.027 32.138
2024-12-02-03:15:31-root-INFO: Loss too large (1132.336->1267.253)! Learning rate decreased to 0.00547.
2024-12-02-03:15:31-root-INFO: Loss too large (1132.336->1188.614)! Learning rate decreased to 0.00438.
2024-12-02-03:15:32-root-INFO: Loss too large (1132.336->1138.337)! Learning rate decreased to 0.00350.
2024-12-02-03:15:33-root-INFO: grad norm: 150.561 145.797 37.572
2024-12-02-03:15:33-root-INFO: Loss too large (1113.883->1121.485)! Learning rate decreased to 0.00280.
2024-12-02-03:15:34-root-INFO: grad norm: 114.761 111.926 25.350
2024-12-02-03:15:35-root-INFO: grad norm: 70.434 68.066 18.111
2024-12-02-03:15:36-root-INFO: grad norm: 60.436 58.583 14.850
2024-12-02-03:15:37-root-INFO: Loss Change: 1132.336 -> 1086.100
2024-12-02-03:15:37-root-INFO: Regularization Change: 0.000 -> 0.447
2024-12-02-03:15:37-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-03:15:37-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:15:37-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-03:15:37-root-INFO: grad norm: 48.711 47.059 12.581
2024-12-02-03:15:38-root-INFO: grad norm: 57.008 55.753 11.896
2024-12-02-03:15:39-root-INFO: grad norm: 101.037 99.084 19.767
2024-12-02-03:15:40-root-INFO: Loss too large (1071.718->1121.128)! Learning rate decreased to 0.00568.
2024-12-02-03:15:40-root-INFO: Loss too large (1071.718->1090.172)! Learning rate decreased to 0.00455.
2024-12-02-03:15:40-root-INFO: Loss too large (1071.718->1073.296)! Learning rate decreased to 0.00364.
2024-12-02-03:15:41-root-INFO: grad norm: 104.584 101.716 24.323
2024-12-02-03:15:42-root-INFO: Loss too large (1064.778->1065.962)! Learning rate decreased to 0.00291.
2024-12-02-03:15:43-root-INFO: grad norm: 85.957 83.749 19.359
2024-12-02-03:15:43-root-INFO: Loss Change: 1083.855 -> 1053.051
2024-12-02-03:15:43-root-INFO: Regularization Change: 0.000 -> 0.774
2024-12-02-03:15:43-root-INFO: Undo step: 160
2024-12-02-03:15:43-root-INFO: Undo step: 161
2024-12-02-03:15:43-root-INFO: Undo step: 162
2024-12-02-03:15:43-root-INFO: Undo step: 163
2024-12-02-03:15:43-root-INFO: Undo step: 164
2024-12-02-03:15:44-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-03:15:44-root-INFO: grad norm: 298.511 278.987 106.185
2024-12-02-03:15:45-root-INFO: grad norm: 276.220 267.001 70.767
2024-12-02-03:15:45-root-INFO: Loss too large (1443.094->1452.196)! Learning rate decreased to 0.00469.
2024-12-02-03:15:46-root-INFO: grad norm: 394.155 379.487 106.526
2024-12-02-03:15:47-root-INFO: Loss too large (1387.545->1452.256)! Learning rate decreased to 0.00375.
2024-12-02-03:15:48-root-INFO: grad norm: 258.710 252.515 56.280
2024-12-02-03:15:48-root-INFO: grad norm: 93.845 89.241 29.034
2024-12-02-03:15:49-root-INFO: Loss Change: 1731.775 -> 1179.249
2024-12-02-03:15:49-root-INFO: Regularization Change: 0.000 -> 10.074
2024-12-02-03:15:49-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-03:15:49-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:15:50-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-03:15:50-root-INFO: grad norm: 92.106 88.248 26.379
2024-12-02-03:15:51-root-INFO: grad norm: 159.773 154.315 41.404
2024-12-02-03:15:51-root-INFO: Loss too large (1150.003->1218.568)! Learning rate decreased to 0.00488.
2024-12-02-03:15:51-root-INFO: Loss too large (1150.003->1181.926)! Learning rate decreased to 0.00390.
2024-12-02-03:15:52-root-INFO: Loss too large (1150.003->1157.850)! Learning rate decreased to 0.00312.
2024-12-02-03:15:53-root-INFO: grad norm: 143.244 139.729 31.539
2024-12-02-03:15:54-root-INFO: grad norm: 121.874 117.835 31.116
2024-12-02-03:15:55-root-INFO: grad norm: 114.768 111.659 26.531
2024-12-02-03:15:55-root-INFO: Loss Change: 1167.829 -> 1104.959
2024-12-02-03:15:55-root-INFO: Regularization Change: 0.000 -> 1.258
2024-12-02-03:15:55-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-03:15:55-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-03:15:56-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-03:15:56-root-INFO: grad norm: 82.208 79.786 19.807
2024-12-02-03:15:56-root-INFO: Loss too large (1093.153->1099.454)! Learning rate decreased to 0.00507.
2024-12-02-03:15:57-root-INFO: grad norm: 122.996 120.150 26.307
2024-12-02-03:15:58-root-INFO: Loss too large (1091.483->1113.904)! Learning rate decreased to 0.00405.
2024-12-02-03:15:59-root-INFO: grad norm: 184.636 179.445 43.472
2024-12-02-03:15:59-root-INFO: Loss too large (1089.689->1115.852)! Learning rate decreased to 0.00324.
2024-12-02-03:15:59-root-INFO: Loss too large (1089.689->1092.832)! Learning rate decreased to 0.00259.
2024-12-02-03:16:00-root-INFO: grad norm: 120.489 117.608 26.191
2024-12-02-03:16:01-root-INFO: grad norm: 60.653 58.684 15.332
2024-12-02-03:16:02-root-INFO: Loss Change: 1093.153 -> 1055.806
2024-12-02-03:16:02-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-02-03:16:02-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-03:16:02-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:16:02-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-03:16:03-root-INFO: grad norm: 102.154 99.035 25.049
2024-12-02-03:16:03-root-INFO: Loss too large (1060.601->1091.343)! Learning rate decreased to 0.00527.
2024-12-02-03:16:03-root-INFO: Loss too large (1060.601->1067.649)! Learning rate decreased to 0.00421.
2024-12-02-03:16:04-root-INFO: grad norm: 136.388 133.237 29.147
2024-12-02-03:16:05-root-INFO: Loss too large (1055.425->1065.671)! Learning rate decreased to 0.00337.
2024-12-02-03:16:06-root-INFO: grad norm: 123.811 120.901 26.685
2024-12-02-03:16:07-root-INFO: grad norm: 107.143 104.582 23.287
2024-12-02-03:16:08-root-INFO: grad norm: 100.873 98.305 22.616
2024-12-02-03:16:08-root-INFO: Loss Change: 1060.601 -> 1026.646
2024-12-02-03:16:08-root-INFO: Regularization Change: 0.000 -> 0.638
2024-12-02-03:16:08-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-03:16:08-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:16:09-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-03:16:09-root-INFO: grad norm: 68.486 66.822 15.005
2024-12-02-03:16:09-root-INFO: Loss too large (1017.281->1021.687)! Learning rate decreased to 0.00547.
2024-12-02-03:16:10-root-INFO: grad norm: 100.133 97.983 20.642
2024-12-02-03:16:11-root-INFO: Loss too large (1015.723->1028.944)! Learning rate decreased to 0.00438.
2024-12-02-03:16:12-root-INFO: grad norm: 139.865 136.607 30.012
2024-12-02-03:16:12-root-INFO: Loss too large (1012.931->1026.235)! Learning rate decreased to 0.00350.
2024-12-02-03:16:13-root-INFO: grad norm: 119.388 116.915 24.173
2024-12-02-03:16:14-root-INFO: grad norm: 87.138 85.092 18.772
2024-12-02-03:16:15-root-INFO: Loss Change: 1017.281 -> 991.986
2024-12-02-03:16:15-root-INFO: Regularization Change: 0.000 -> 0.628
2024-12-02-03:16:15-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-03:16:15-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:16:15-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-03:16:15-root-INFO: grad norm: 114.508 111.922 24.197
2024-12-02-03:16:16-root-INFO: Loss too large (997.609->1054.475)! Learning rate decreased to 0.00568.
2024-12-02-03:16:16-root-INFO: Loss too large (997.609->1014.019)! Learning rate decreased to 0.00455.
2024-12-02-03:16:17-root-INFO: grad norm: 146.548 143.587 29.309
2024-12-02-03:16:17-root-INFO: Loss too large (993.502->1005.553)! Learning rate decreased to 0.00364.
2024-12-02-03:16:18-root-INFO: grad norm: 117.652 115.176 24.010
2024-12-02-03:16:19-root-INFO: grad norm: 79.996 78.397 15.914
2024-12-02-03:16:20-root-INFO: grad norm: 69.507 67.601 16.165
2024-12-02-03:16:21-root-INFO: Loss Change: 997.609 -> 962.375
2024-12-02-03:16:21-root-INFO: Regularization Change: 0.000 -> 0.562
2024-12-02-03:16:21-root-INFO: Undo step: 160
2024-12-02-03:16:21-root-INFO: Undo step: 161
2024-12-02-03:16:21-root-INFO: Undo step: 162
2024-12-02-03:16:21-root-INFO: Undo step: 163
2024-12-02-03:16:21-root-INFO: Undo step: 164
2024-12-02-03:16:21-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-03:16:22-root-INFO: grad norm: 450.590 435.988 113.781
2024-12-02-03:16:23-root-INFO: grad norm: 444.342 432.707 101.016
2024-12-02-03:16:23-root-INFO: Loss too large (1486.774->1669.754)! Learning rate decreased to 0.00469.
2024-12-02-03:16:23-root-INFO: Loss too large (1486.774->1553.975)! Learning rate decreased to 0.00375.
2024-12-02-03:16:24-root-INFO: grad norm: 425.119 411.541 106.584
2024-12-02-03:16:25-root-INFO: grad norm: 330.087 319.972 81.088
2024-12-02-03:16:26-root-INFO: grad norm: 327.617 311.907 100.234
2024-12-02-03:16:27-root-INFO: Loss too large (1224.001->1301.521)! Learning rate decreased to 0.00300.
2024-12-02-03:16:27-root-INFO: Loss too large (1224.001->1243.346)! Learning rate decreased to 0.00240.
2024-12-02-03:16:28-root-INFO: Loss Change: 1799.937 -> 1204.167
2024-12-02-03:16:28-root-INFO: Regularization Change: 0.000 -> 9.948
2024-12-02-03:16:28-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-03:16:28-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-03:16:28-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-03:16:28-root-INFO: grad norm: 230.210 220.154 67.298
2024-12-02-03:16:29-root-INFO: Loss too large (1213.237->1327.399)! Learning rate decreased to 0.00488.
2024-12-02-03:16:29-root-INFO: Loss too large (1213.237->1234.687)! Learning rate decreased to 0.00390.
2024-12-02-03:16:30-root-INFO: grad norm: 278.291 264.923 85.216
2024-12-02-03:16:30-root-INFO: Loss too large (1167.160->1221.816)! Learning rate decreased to 0.00312.
2024-12-02-03:16:31-root-INFO: Loss too large (1167.160->1177.959)! Learning rate decreased to 0.00250.
2024-12-02-03:16:32-root-INFO: grad norm: 175.441 167.583 51.919
2024-12-02-03:16:33-root-INFO: grad norm: 71.095 68.016 20.695
2024-12-02-03:16:34-root-INFO: grad norm: 62.434 59.364 19.337
2024-12-02-03:16:34-root-INFO: Loss Change: 1213.237 -> 1090.591
2024-12-02-03:16:34-root-INFO: Regularization Change: 0.000 -> 1.088
2024-12-02-03:16:34-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-03:16:34-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-03:16:35-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-03:16:35-root-INFO: grad norm: 63.013 60.120 18.875
2024-12-02-03:16:36-root-INFO: grad norm: 86.042 82.035 25.951
2024-12-02-03:16:36-root-INFO: Loss too large (1067.342->1079.307)! Learning rate decreased to 0.00507.
2024-12-02-03:16:37-root-INFO: Loss too large (1067.342->1068.988)! Learning rate decreased to 0.00405.
2024-12-02-03:16:38-root-INFO: grad norm: 107.858 103.118 31.622
2024-12-02-03:16:38-root-INFO: Loss too large (1063.193->1064.597)! Learning rate decreased to 0.00324.
2024-12-02-03:16:39-root-INFO: grad norm: 117.517 112.211 34.913
2024-12-02-03:16:40-root-INFO: grad norm: 121.547 116.397 35.007
2024-12-02-03:16:41-root-INFO: Loss Change: 1081.838 -> 1044.703
2024-12-02-03:16:41-root-INFO: Regularization Change: 0.000 -> 0.949
2024-12-02-03:16:41-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-03:16:41-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:16:41-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-03:16:41-root-INFO: grad norm: 86.368 83.515 22.019
2024-12-02-03:16:42-root-INFO: Loss too large (1039.711->1044.907)! Learning rate decreased to 0.00527.
2024-12-02-03:16:43-root-INFO: grad norm: 117.041 112.803 31.211
2024-12-02-03:16:43-root-INFO: Loss too large (1036.433->1057.732)! Learning rate decreased to 0.00421.
2024-12-02-03:16:44-root-INFO: grad norm: 180.272 173.533 48.830
2024-12-02-03:16:44-root-INFO: Loss too large (1035.009->1065.784)! Learning rate decreased to 0.00337.
2024-12-02-03:16:45-root-INFO: Loss too large (1035.009->1042.143)! Learning rate decreased to 0.00270.
2024-12-02-03:16:46-root-INFO: grad norm: 123.522 118.137 36.074
2024-12-02-03:16:47-root-INFO: grad norm: 61.797 59.372 17.143
2024-12-02-03:16:47-root-INFO: Loss Change: 1039.711 -> 1004.559
2024-12-02-03:16:47-root-INFO: Regularization Change: 0.000 -> 0.585
2024-12-02-03:16:47-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-03:16:47-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:16:48-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-03:16:48-root-INFO: grad norm: 80.866 77.088 24.429
2024-12-02-03:16:48-root-INFO: Loss too large (1002.346->1023.921)! Learning rate decreased to 0.00547.
2024-12-02-03:16:49-root-INFO: Loss too large (1002.346->1006.931)! Learning rate decreased to 0.00438.
2024-12-02-03:16:50-root-INFO: grad norm: 113.241 108.920 30.984
2024-12-02-03:16:50-root-INFO: Loss too large (998.487->1006.415)! Learning rate decreased to 0.00350.
2024-12-02-03:16:51-root-INFO: grad norm: 106.531 102.200 30.066
2024-12-02-03:16:52-root-INFO: grad norm: 97.348 93.629 26.651
2024-12-02-03:16:53-root-INFO: grad norm: 94.038 90.200 26.590
2024-12-02-03:16:54-root-INFO: Loss Change: 1002.346 -> 976.739
2024-12-02-03:16:54-root-INFO: Regularization Change: 0.000 -> 0.535
2024-12-02-03:16:54-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-03:16:54-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:16:54-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-03:16:54-root-INFO: grad norm: 58.034 56.127 14.756
2024-12-02-03:16:55-root-INFO: grad norm: 89.999 87.268 22.002
2024-12-02-03:16:56-root-INFO: Loss too large (967.980->999.149)! Learning rate decreased to 0.00568.
2024-12-02-03:16:56-root-INFO: Loss too large (967.980->975.532)! Learning rate decreased to 0.00455.
2024-12-02-03:16:57-root-INFO: grad norm: 118.937 115.375 28.886
2024-12-02-03:16:57-root-INFO: Loss too large (963.742->973.550)! Learning rate decreased to 0.00364.
2024-12-02-03:16:58-root-INFO: grad norm: 105.282 101.302 28.675
2024-12-02-03:16:59-root-INFO: grad norm: 86.038 83.112 22.246
2024-12-02-03:17:00-root-INFO: Loss Change: 971.696 -> 948.635
2024-12-02-03:17:00-root-INFO: Regularization Change: 0.000 -> 0.672
2024-12-02-03:17:00-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-03:17:00-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:17:00-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-03:17:01-root-INFO: grad norm: 102.560 98.658 28.022
2024-12-02-03:17:01-root-INFO: Loss too large (949.784->1003.501)! Learning rate decreased to 0.00590.
2024-12-02-03:17:01-root-INFO: Loss too large (949.784->965.290)! Learning rate decreased to 0.00472.
2024-12-02-03:17:02-root-INFO: grad norm: 139.281 134.935 34.520
2024-12-02-03:17:03-root-INFO: Loss too large (946.385->961.647)! Learning rate decreased to 0.00378.
2024-12-02-03:17:03-root-INFO: Loss too large (946.385->946.425)! Learning rate decreased to 0.00302.
2024-12-02-03:17:04-root-INFO: grad norm: 85.499 82.188 23.562
2024-12-02-03:17:05-root-INFO: grad norm: 41.232 39.767 10.893
2024-12-02-03:17:06-root-INFO: grad norm: 34.553 32.876 10.635
2024-12-02-03:17:07-root-INFO: Loss Change: 949.784 -> 920.420
2024-12-02-03:17:07-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-02-03:17:07-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-03:17:07-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-03:17:07-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-03:17:07-root-INFO: grad norm: 61.008 58.487 17.357
2024-12-02-03:17:08-root-INFO: Loss too large (920.549->921.742)! Learning rate decreased to 0.00613.
2024-12-02-03:17:09-root-INFO: grad norm: 84.644 82.728 17.906
2024-12-02-03:17:09-root-INFO: Loss too large (916.753->924.653)! Learning rate decreased to 0.00490.
2024-12-02-03:17:10-root-INFO: grad norm: 94.414 91.324 23.957
2024-12-02-03:17:11-root-INFO: grad norm: 125.640 122.132 29.478
2024-12-02-03:17:11-root-INFO: Loss too large (913.724->925.013)! Learning rate decreased to 0.00392.
2024-12-02-03:17:12-root-INFO: grad norm: 102.434 98.892 26.707
2024-12-02-03:17:13-root-INFO: Loss Change: 920.549 -> 898.583
2024-12-02-03:17:13-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-02-03:17:13-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-03:17:13-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:17:13-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-03:17:14-root-INFO: grad norm: 43.929 42.089 12.582
2024-12-02-03:17:15-root-INFO: grad norm: 53.528 51.460 14.736
2024-12-02-03:17:15-root-INFO: Loss too large (888.338->892.586)! Learning rate decreased to 0.00636.
2024-12-02-03:17:16-root-INFO: grad norm: 88.960 86.889 19.086
2024-12-02-03:17:16-root-INFO: Loss too large (886.770->900.373)! Learning rate decreased to 0.00509.
2024-12-02-03:17:17-root-INFO: Loss too large (886.770->890.159)! Learning rate decreased to 0.00407.
2024-12-02-03:17:18-root-INFO: grad norm: 74.184 71.528 19.673
2024-12-02-03:17:19-root-INFO: grad norm: 56.553 54.994 13.188
2024-12-02-03:17:19-root-INFO: Loss Change: 895.069 -> 873.812
2024-12-02-03:17:19-root-INFO: Regularization Change: 0.000 -> 0.619
2024-12-02-03:17:19-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-03:17:19-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:17:20-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-03:17:20-root-INFO: grad norm: 70.678 68.158 18.703
2024-12-02-03:17:20-root-INFO: Loss too large (874.994->895.814)! Learning rate decreased to 0.00660.
2024-12-02-03:17:21-root-INFO: Loss too large (874.994->879.357)! Learning rate decreased to 0.00528.
2024-12-02-03:17:22-root-INFO: grad norm: 85.496 83.426 18.702
2024-12-02-03:17:22-root-INFO: Loss too large (871.357->873.941)! Learning rate decreased to 0.00423.
2024-12-02-03:17:23-root-INFO: grad norm: 69.405 66.985 18.168
2024-12-02-03:17:24-root-INFO: grad norm: 50.533 49.113 11.898
2024-12-02-03:17:25-root-INFO: grad norm: 43.984 42.157 12.546
2024-12-02-03:17:26-root-INFO: Loss Change: 874.994 -> 854.504
2024-12-02-03:17:26-root-INFO: Regularization Change: 0.000 -> 0.397
2024-12-02-03:17:26-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-03:17:26-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:17:26-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-03:17:26-root-INFO: grad norm: 43.104 41.407 11.976
2024-12-02-03:17:27-root-INFO: grad norm: 58.651 57.790 10.009
2024-12-02-03:17:28-root-INFO: Loss too large (849.887->853.591)! Learning rate decreased to 0.00685.
2024-12-02-03:17:29-root-INFO: grad norm: 70.177 69.177 11.805
2024-12-02-03:17:29-root-INFO: Loss too large (847.564->847.632)! Learning rate decreased to 0.00548.
2024-12-02-03:17:30-root-INFO: grad norm: 60.499 59.679 9.924
2024-12-02-03:17:31-root-INFO: grad norm: 56.211 54.817 12.441
2024-12-02-03:17:31-root-INFO: Loss Change: 853.776 -> 835.343
2024-12-02-03:17:31-root-INFO: Regularization Change: 0.000 -> 0.702
2024-12-02-03:17:31-root-INFO: Undo step: 155
2024-12-02-03:17:31-root-INFO: Undo step: 156
2024-12-02-03:17:31-root-INFO: Undo step: 157
2024-12-02-03:17:32-root-INFO: Undo step: 158
2024-12-02-03:17:32-root-INFO: Undo step: 159
2024-12-02-03:17:32-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-03:17:32-root-INFO: grad norm: 370.323 356.844 99.004
2024-12-02-03:17:33-root-INFO: grad norm: 343.324 337.659 62.110
2024-12-02-03:17:34-root-INFO: grad norm: 275.722 265.919 72.868
2024-12-02-03:17:35-root-INFO: Loss too large (1253.559->1261.124)! Learning rate decreased to 0.00568.
2024-12-02-03:17:36-root-INFO: grad norm: 199.114 193.998 44.846
2024-12-02-03:17:36-root-INFO: grad norm: 107.683 104.279 26.862
2024-12-02-03:17:37-root-INFO: Loss Change: 1603.884 -> 1046.502
2024-12-02-03:17:37-root-INFO: Regularization Change: 0.000 -> 13.158
2024-12-02-03:17:37-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-03:17:37-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:17:37-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-03:17:38-root-INFO: grad norm: 111.644 108.603 25.881
2024-12-02-03:17:39-root-INFO: grad norm: 159.932 155.032 39.284
2024-12-02-03:17:39-root-INFO: Loss too large (1023.920->1053.083)! Learning rate decreased to 0.00590.
2024-12-02-03:17:40-root-INFO: grad norm: 152.279 148.913 31.840
2024-12-02-03:17:41-root-INFO: grad norm: 139.726 135.925 32.370
2024-12-02-03:17:42-root-INFO: grad norm: 139.471 136.408 29.065
2024-12-02-03:17:43-root-INFO: Loss Change: 1042.402 -> 959.697
2024-12-02-03:17:43-root-INFO: Regularization Change: 0.000 -> 3.146
2024-12-02-03:17:43-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-03:17:43-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-03:17:43-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-03:17:44-root-INFO: grad norm: 110.300 107.745 23.601
2024-12-02-03:17:44-root-INFO: Loss too large (943.393->954.439)! Learning rate decreased to 0.00613.
2024-12-02-03:17:45-root-INFO: grad norm: 112.991 110.405 24.038
2024-12-02-03:17:46-root-INFO: grad norm: 120.505 117.841 25.201
2024-12-02-03:17:46-root-INFO: Loss too large (926.075->926.119)! Learning rate decreased to 0.00490.
2024-12-02-03:17:47-root-INFO: grad norm: 89.315 87.069 19.904
2024-12-02-03:17:48-root-INFO: grad norm: 62.676 61.034 14.253
2024-12-02-03:17:49-root-INFO: Loss Change: 943.393 -> 892.340
2024-12-02-03:17:49-root-INFO: Regularization Change: 0.000 -> 1.221
2024-12-02-03:17:49-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-03:17:49-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:17:49-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-03:17:49-root-INFO: grad norm: 82.722 80.067 20.789
2024-12-02-03:17:50-root-INFO: Loss too large (894.264->897.607)! Learning rate decreased to 0.00636.
2024-12-02-03:17:51-root-INFO: grad norm: 91.524 89.733 18.018
2024-12-02-03:17:52-root-INFO: grad norm: 100.521 98.481 20.149
2024-12-02-03:17:53-root-INFO: grad norm: 112.516 110.504 21.182
2024-12-02-03:17:53-root-INFO: Loss too large (880.442->882.153)! Learning rate decreased to 0.00509.
2024-12-02-03:17:54-root-INFO: grad norm: 83.073 81.186 17.604
2024-12-02-03:17:55-root-INFO: Loss Change: 894.264 -> 858.875
2024-12-02-03:17:55-root-INFO: Regularization Change: 0.000 -> 1.027
2024-12-02-03:17:55-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-03:17:55-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:17:55-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-03:17:55-root-INFO: grad norm: 39.515 38.164 10.245
2024-12-02-03:17:56-root-INFO: grad norm: 49.125 47.778 11.421
2024-12-02-03:17:57-root-INFO: grad norm: 77.761 76.228 15.367
2024-12-02-03:17:58-root-INFO: Loss too large (845.550->855.733)! Learning rate decreased to 0.00660.
2024-12-02-03:17:59-root-INFO: grad norm: 85.597 84.048 16.213
2024-12-02-03:18:00-root-INFO: grad norm: 95.956 94.301 17.744
2024-12-02-03:18:00-root-INFO: Loss too large (840.879->842.901)! Learning rate decreased to 0.00528.
2024-12-02-03:18:01-root-INFO: Loss Change: 852.954 -> 833.598
2024-12-02-03:18:01-root-INFO: Regularization Change: 0.000 -> 1.017
2024-12-02-03:18:01-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-03:18:01-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:18:01-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-03:18:01-root-INFO: grad norm: 99.834 97.862 19.740
2024-12-02-03:18:02-root-INFO: Loss too large (840.048->853.473)! Learning rate decreased to 0.00685.
2024-12-02-03:18:03-root-INFO: grad norm: 105.177 103.758 17.218
2024-12-02-03:18:03-root-INFO: Loss too large (833.814->834.284)! Learning rate decreased to 0.00548.
2024-12-02-03:18:04-root-INFO: grad norm: 73.802 72.395 14.343
2024-12-02-03:18:05-root-INFO: grad norm: 50.056 49.092 9.780
2024-12-02-03:18:06-root-INFO: grad norm: 40.182 39.085 9.326
2024-12-02-03:18:07-root-INFO: Loss Change: 840.048 -> 804.921
2024-12-02-03:18:07-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-02-03:18:07-root-INFO: Undo step: 155
2024-12-02-03:18:07-root-INFO: Undo step: 156
2024-12-02-03:18:07-root-INFO: Undo step: 157
2024-12-02-03:18:07-root-INFO: Undo step: 158
2024-12-02-03:18:07-root-INFO: Undo step: 159
2024-12-02-03:18:07-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-03:18:07-root-INFO: grad norm: 352.728 340.642 91.543
2024-12-02-03:18:08-root-INFO: grad norm: 256.125 249.041 59.824
2024-12-02-03:18:10-root-INFO: grad norm: 212.050 208.491 38.690
2024-12-02-03:18:11-root-INFO: grad norm: 164.663 160.662 36.079
2024-12-02-03:18:12-root-INFO: grad norm: 195.534 187.934 53.985
2024-12-02-03:18:12-root-INFO: Loss too large (1013.115->1037.119)! Learning rate decreased to 0.00568.
2024-12-02-03:18:13-root-INFO: Loss Change: 1567.791 -> 1000.906
2024-12-02-03:18:13-root-INFO: Regularization Change: 0.000 -> 15.939
2024-12-02-03:18:13-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-03:18:13-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-03:18:13-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-03:18:13-root-INFO: grad norm: 147.448 141.631 41.007
2024-12-02-03:18:14-root-INFO: grad norm: 201.216 189.966 66.338
2024-12-02-03:18:15-root-INFO: Loss too large (953.346->1025.602)! Learning rate decreased to 0.00590.
2024-12-02-03:18:15-root-INFO: Loss too large (953.346->984.301)! Learning rate decreased to 0.00472.
2024-12-02-03:18:15-root-INFO: Loss too large (953.346->956.418)! Learning rate decreased to 0.00378.
2024-12-02-03:18:16-root-INFO: grad norm: 108.833 103.805 32.697
2024-12-02-03:18:17-root-INFO: grad norm: 47.428 45.312 14.008
2024-12-02-03:18:18-root-INFO: grad norm: 42.686 41.035 11.758
2024-12-02-03:18:19-root-INFO: Loss Change: 998.962 -> 894.044
2024-12-02-03:18:19-root-INFO: Regularization Change: 0.000 -> 1.595
2024-12-02-03:18:19-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-03:18:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-03:18:19-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-03:18:20-root-INFO: grad norm: 62.656 59.759 18.834
2024-12-02-03:18:21-root-INFO: grad norm: 89.274 86.449 22.280
2024-12-02-03:18:21-root-INFO: Loss too large (883.300->893.792)! Learning rate decreased to 0.00613.
2024-12-02-03:18:22-root-INFO: grad norm: 105.215 101.249 28.617
2024-12-02-03:18:23-root-INFO: grad norm: 127.570 123.416 32.288
2024-12-02-03:18:23-root-INFO: Loss too large (878.880->883.940)! Learning rate decreased to 0.00490.
2024-12-02-03:18:24-root-INFO: grad norm: 98.131 94.711 25.682
2024-12-02-03:18:25-root-INFO: Loss Change: 890.679 -> 854.026
2024-12-02-03:18:25-root-INFO: Regularization Change: 0.000 -> 1.246
2024-12-02-03:18:25-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-03:18:25-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:18:25-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-03:18:25-root-INFO: grad norm: 49.005 47.569 11.778
2024-12-02-03:18:26-root-INFO: grad norm: 64.203 62.943 12.658
2024-12-02-03:18:27-root-INFO: Loss too large (838.990->842.377)! Learning rate decreased to 0.00636.
2024-12-02-03:18:28-root-INFO: grad norm: 78.130 77.167 12.227
2024-12-02-03:18:29-root-INFO: grad norm: 95.138 93.556 17.276
2024-12-02-03:18:29-root-INFO: Loss too large (835.450->836.494)! Learning rate decreased to 0.00509.
2024-12-02-03:18:30-root-INFO: grad norm: 77.779 76.649 13.212
2024-12-02-03:18:31-root-INFO: Loss Change: 845.749 -> 820.736
2024-12-02-03:18:31-root-INFO: Regularization Change: 0.000 -> 0.938
2024-12-02-03:18:31-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-03:18:31-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:18:31-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-03:18:31-root-INFO: grad norm: 87.933 86.219 17.278
2024-12-02-03:18:32-root-INFO: Loss too large (823.814->837.250)! Learning rate decreased to 0.00660.
2024-12-02-03:18:33-root-INFO: grad norm: 101.593 100.041 17.691
2024-12-02-03:18:33-root-INFO: Loss too large (822.267->823.343)! Learning rate decreased to 0.00528.
2024-12-02-03:18:34-root-INFO: grad norm: 76.668 74.980 16.001
2024-12-02-03:18:35-root-INFO: grad norm: 59.193 58.207 10.757
2024-12-02-03:18:36-root-INFO: grad norm: 50.221 48.998 11.017
2024-12-02-03:18:37-root-INFO: Loss Change: 823.814 -> 795.563
2024-12-02-03:18:37-root-INFO: Regularization Change: 0.000 -> 0.658
2024-12-02-03:18:37-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-03:18:37-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-03:18:37-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-03:18:37-root-INFO: grad norm: 33.726 31.928 10.866
2024-12-02-03:18:38-root-INFO: grad norm: 32.686 31.077 10.128
2024-12-02-03:18:39-root-INFO: grad norm: 51.036 48.589 15.613
2024-12-02-03:18:40-root-INFO: Loss too large (782.474->784.965)! Learning rate decreased to 0.00685.
2024-12-02-03:18:41-root-INFO: grad norm: 55.521 52.857 16.992
2024-12-02-03:18:42-root-INFO: grad norm: 62.286 59.949 16.899
2024-12-02-03:18:42-root-INFO: Loss too large (776.916->777.673)! Learning rate decreased to 0.00548.
2024-12-02-03:18:43-root-INFO: Loss Change: 792.282 -> 773.880
2024-12-02-03:18:43-root-INFO: Regularization Change: 0.000 -> 0.834
2024-12-02-03:18:43-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-03:18:43-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-03:18:43-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-03:18:43-root-INFO: grad norm: 76.393 74.625 16.336
2024-12-02-03:18:44-root-INFO: Loss too large (775.756->790.268)! Learning rate decreased to 0.00711.
2024-12-02-03:18:44-root-INFO: Loss too large (775.756->777.047)! Learning rate decreased to 0.00569.
2024-12-02-03:18:45-root-INFO: grad norm: 68.304 67.475 10.607
2024-12-02-03:18:46-root-INFO: grad norm: 62.366 61.251 11.736
2024-12-02-03:18:47-root-INFO: grad norm: 56.897 56.200 8.875
2024-12-02-03:18:48-root-INFO: grad norm: 52.796 51.876 9.813
2024-12-02-03:18:49-root-INFO: Loss Change: 775.756 -> 755.355
2024-12-02-03:18:49-root-INFO: Regularization Change: 0.000 -> 0.549
2024-12-02-03:18:49-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-03:18:49-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-03:18:49-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-03:18:49-root-INFO: grad norm: 45.929 43.716 14.086
2024-12-02-03:18:50-root-INFO: grad norm: 59.656 56.985 17.651
2024-12-02-03:18:51-root-INFO: Loss too large (751.774->755.212)! Learning rate decreased to 0.00738.
2024-12-02-03:18:52-root-INFO: grad norm: 69.572 67.430 17.134
2024-12-02-03:18:52-root-INFO: Loss too large (747.514->750.849)! Learning rate decreased to 0.00590.
2024-12-02-03:18:53-root-INFO: grad norm: 60.896 59.036 14.935
2024-12-02-03:18:54-root-INFO: grad norm: 51.988 51.116 9.477
2024-12-02-03:18:55-root-INFO: Loss Change: 752.795 -> 737.183
2024-12-02-03:18:55-root-INFO: Regularization Change: 0.000 -> 0.628
2024-12-02-03:18:55-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-03:18:55-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-03:18:55-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-03:18:55-root-INFO: grad norm: 64.599 63.586 11.395
2024-12-02-03:18:56-root-INFO: Loss too large (738.949->750.711)! Learning rate decreased to 0.00765.
2024-12-02-03:18:56-root-INFO: Loss too large (738.949->740.240)! Learning rate decreased to 0.00612.
2024-12-02-03:18:57-root-INFO: grad norm: 59.570 59.055 7.820
2024-12-02-03:18:58-root-INFO: grad norm: 55.676 54.943 9.009
2024-12-02-03:18:59-root-INFO: grad norm: 51.901 51.400 7.197
2024-12-02-03:19:00-root-INFO: grad norm: 49.067 48.396 8.081
2024-12-02-03:19:01-root-INFO: Loss Change: 738.949 -> 722.594
2024-12-02-03:19:01-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-02-03:19:01-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-03:19:01-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-03:19:01-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-03:19:01-root-INFO: grad norm: 39.686 37.978 11.515
2024-12-02-03:19:02-root-INFO: grad norm: 53.769 51.635 14.999
2024-12-02-03:19:03-root-INFO: Loss too large (718.684->724.337)! Learning rate decreased to 0.00794.
2024-12-02-03:19:04-root-INFO: grad norm: 69.109 67.119 16.464
2024-12-02-03:19:04-root-INFO: Loss too large (717.024->721.133)! Learning rate decreased to 0.00635.
2024-12-02-03:19:05-root-INFO: grad norm: 60.384 58.565 14.709
2024-12-02-03:19:06-root-INFO: grad norm: 50.840 49.968 9.374
2024-12-02-03:19:07-root-INFO: Loss Change: 720.487 -> 706.951
2024-12-02-03:19:07-root-INFO: Regularization Change: 0.000 -> 0.614
2024-12-02-03:19:07-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-03:19:07-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:19:07-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-03:19:07-root-INFO: grad norm: 65.577 64.567 11.467
2024-12-02-03:19:08-root-INFO: Loss too large (710.146->723.966)! Learning rate decreased to 0.00823.
2024-12-02-03:19:08-root-INFO: Loss too large (710.146->711.964)! Learning rate decreased to 0.00659.
2024-12-02-03:19:09-root-INFO: grad norm: 59.924 59.424 7.726
2024-12-02-03:19:10-root-INFO: grad norm: 55.494 54.762 8.983
2024-12-02-03:19:11-root-INFO: grad norm: 50.669 50.183 7.001
2024-12-02-03:19:12-root-INFO: grad norm: 47.344 46.691 7.834
2024-12-02-03:19:13-root-INFO: Loss Change: 710.146 -> 693.865
2024-12-02-03:19:13-root-INFO: Regularization Change: 0.000 -> 0.515
2024-12-02-03:19:13-root-INFO: Undo step: 150
2024-12-02-03:19:13-root-INFO: Undo step: 151
2024-12-02-03:19:13-root-INFO: Undo step: 152
2024-12-02-03:19:13-root-INFO: Undo step: 153
2024-12-02-03:19:13-root-INFO: Undo step: 154
2024-12-02-03:19:13-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-03:19:13-root-INFO: grad norm: 403.591 393.509 89.648
2024-12-02-03:19:14-root-INFO: grad norm: 205.782 202.156 38.458
2024-12-02-03:19:15-root-INFO: grad norm: 140.372 136.856 31.218
2024-12-02-03:19:16-root-INFO: grad norm: 177.652 172.767 41.373
2024-12-02-03:19:16-root-INFO: Loss too large (893.159->925.652)! Learning rate decreased to 0.00685.
2024-12-02-03:19:17-root-INFO: grad norm: 152.115 149.356 28.840
2024-12-02-03:19:18-root-INFO: Loss Change: 1500.436 -> 830.743
2024-12-02-03:19:18-root-INFO: Regularization Change: 0.000 -> 18.843
2024-12-02-03:19:18-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-03:19:18-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-03:19:18-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-03:19:19-root-INFO: grad norm: 98.039 96.040 19.701
2024-12-02-03:19:20-root-INFO: grad norm: 112.305 110.523 19.927
2024-12-02-03:19:21-root-INFO: grad norm: 153.432 150.679 28.933
2024-12-02-03:19:21-root-INFO: Loss too large (814.720->848.731)! Learning rate decreased to 0.00711.
2024-12-02-03:19:22-root-INFO: grad norm: 132.506 130.626 22.244
2024-12-02-03:19:23-root-INFO: grad norm: 101.449 99.668 18.927
2024-12-02-03:19:24-root-INFO: Loss Change: 825.026 -> 772.646
2024-12-02-03:19:24-root-INFO: Regularization Change: 0.000 -> 2.532
2024-12-02-03:19:24-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-03:19:24-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-03:19:24-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-03:19:24-root-INFO: grad norm: 86.681 84.380 19.839
2024-12-02-03:19:25-root-INFO: grad norm: 109.610 106.681 25.167
2024-12-02-03:19:26-root-INFO: Loss too large (763.382->777.944)! Learning rate decreased to 0.00738.
2024-12-02-03:19:27-root-INFO: grad norm: 95.404 93.283 20.006
2024-12-02-03:19:28-root-INFO: grad norm: 78.459 76.575 17.092
2024-12-02-03:19:29-root-INFO: grad norm: 72.551 70.947 15.170
2024-12-02-03:19:29-root-INFO: Loss Change: 768.964 -> 730.924
2024-12-02-03:19:29-root-INFO: Regularization Change: 0.000 -> 1.479
2024-12-02-03:19:29-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-03:19:29-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-03:19:30-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-03:19:30-root-INFO: grad norm: 74.391 73.266 12.889
2024-12-02-03:19:30-root-INFO: Loss too large (730.187->735.436)! Learning rate decreased to 0.00765.
2024-12-02-03:19:31-root-INFO: grad norm: 72.866 71.910 11.767
2024-12-02-03:19:32-root-INFO: grad norm: 70.714 69.834 11.116
2024-12-02-03:19:33-root-INFO: grad norm: 69.929 69.152 10.399
2024-12-02-03:19:34-root-INFO: grad norm: 68.342 67.624 9.882
2024-12-02-03:19:35-root-INFO: Loss Change: 730.187 -> 708.641
2024-12-02-03:19:35-root-INFO: Regularization Change: 0.000 -> 1.013
2024-12-02-03:19:35-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-03:19:35-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-03:19:35-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-03:19:36-root-INFO: grad norm: 57.536 55.874 13.727
2024-12-02-03:19:36-root-INFO: Loss too large (704.004->704.452)! Learning rate decreased to 0.00794.
2024-12-02-03:19:37-root-INFO: grad norm: 54.268 52.892 12.139
2024-12-02-03:19:38-root-INFO: grad norm: 53.635 52.549 10.738
2024-12-02-03:19:39-root-INFO: grad norm: 53.441 52.408 10.457
2024-12-02-03:19:40-root-INFO: grad norm: 53.789 52.951 9.458
2024-12-02-03:19:41-root-INFO: Loss Change: 704.004 -> 686.739
2024-12-02-03:19:41-root-INFO: Regularization Change: 0.000 -> 0.832
2024-12-02-03:19:41-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-03:19:41-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:19:41-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-03:19:42-root-INFO: grad norm: 68.645 67.887 10.174
2024-12-02-03:19:42-root-INFO: Loss too large (689.338->695.496)! Learning rate decreased to 0.00823.
2024-12-02-03:19:43-root-INFO: grad norm: 68.926 68.397 8.523
2024-12-02-03:19:44-root-INFO: grad norm: 67.521 66.968 8.631
2024-12-02-03:19:45-root-INFO: grad norm: 64.754 64.271 7.891
2024-12-02-03:19:46-root-INFO: grad norm: 62.662 62.184 7.720
2024-12-02-03:19:46-root-INFO: Loss Change: 689.338 -> 672.743
2024-12-02-03:19:46-root-INFO: Regularization Change: 0.000 -> 0.850
2024-12-02-03:19:46-root-INFO: Undo step: 150
2024-12-02-03:19:46-root-INFO: Undo step: 151
2024-12-02-03:19:46-root-INFO: Undo step: 152
2024-12-02-03:19:47-root-INFO: Undo step: 153
2024-12-02-03:19:47-root-INFO: Undo step: 154
2024-12-02-03:19:47-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-03:19:47-root-INFO: grad norm: 429.667 416.881 104.041
2024-12-02-03:19:48-root-INFO: grad norm: 324.063 319.322 55.231
2024-12-02-03:19:49-root-INFO: grad norm: 177.693 174.414 33.980
2024-12-02-03:19:50-root-INFO: grad norm: 199.357 194.313 44.562
2024-12-02-03:19:51-root-INFO: Loss too large (891.261->942.680)! Learning rate decreased to 0.00685.
2024-12-02-03:19:51-root-INFO: Loss too large (891.261->899.868)! Learning rate decreased to 0.00548.
2024-12-02-03:19:52-root-INFO: grad norm: 150.498 146.497 34.472
2024-12-02-03:19:53-root-INFO: Loss Change: 1458.528 -> 808.760
2024-12-02-03:19:53-root-INFO: Regularization Change: 0.000 -> 14.747
2024-12-02-03:19:53-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-03:19:53-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-03:19:53-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-03:19:53-root-INFO: grad norm: 67.821 65.579 17.293
2024-12-02-03:19:54-root-INFO: grad norm: 60.610 58.700 15.097
2024-12-02-03:19:55-root-INFO: grad norm: 75.418 73.051 18.746
2024-12-02-03:19:56-root-INFO: grad norm: 92.761 90.689 19.495
2024-12-02-03:19:57-root-INFO: grad norm: 128.683 125.271 29.440
2024-12-02-03:19:57-root-INFO: Loss too large (764.380->785.186)! Learning rate decreased to 0.00711.
2024-12-02-03:19:58-root-INFO: Loss Change: 802.006 -> 760.759
2024-12-02-03:19:58-root-INFO: Regularization Change: 0.000 -> 3.032
2024-12-02-03:19:58-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-03:19:58-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-03:19:59-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-03:19:59-root-INFO: grad norm: 95.306 92.819 21.629
2024-12-02-03:20:00-root-INFO: grad norm: 91.149 88.639 21.242
2024-12-02-03:20:00-root-INFO: Loss too large (735.325->739.416)! Learning rate decreased to 0.00738.
2024-12-02-03:20:01-root-INFO: grad norm: 72.351 70.338 16.951
2024-12-02-03:20:02-root-INFO: grad norm: 53.063 51.677 12.046
2024-12-02-03:20:03-root-INFO: grad norm: 47.553 46.274 10.955
2024-12-02-03:20:04-root-INFO: Loss Change: 757.132 -> 704.456
2024-12-02-03:20:04-root-INFO: Regularization Change: 0.000 -> 1.523
2024-12-02-03:20:04-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-03:20:04-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-03:20:04-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-03:20:05-root-INFO: grad norm: 55.172 54.039 11.124
2024-12-02-03:20:05-root-INFO: grad norm: 74.451 73.498 11.870
2024-12-02-03:20:06-root-INFO: Loss too large (703.911->708.520)! Learning rate decreased to 0.00765.
2024-12-02-03:20:07-root-INFO: grad norm: 67.713 66.887 10.543
2024-12-02-03:20:08-root-INFO: grad norm: 61.476 60.635 10.132
2024-12-02-03:20:09-root-INFO: grad norm: 57.071 56.346 9.067
2024-12-02-03:20:09-root-INFO: Loss Change: 704.540 -> 683.901
2024-12-02-03:20:09-root-INFO: Regularization Change: 0.000 -> 0.974
2024-12-02-03:20:09-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-03:20:09-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-03:20:10-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-03:20:10-root-INFO: grad norm: 35.574 34.411 9.018
2024-12-02-03:20:11-root-INFO: grad norm: 40.427 39.598 8.146
2024-12-02-03:20:12-root-INFO: grad norm: 51.965 51.021 9.858
2024-12-02-03:20:13-root-INFO: Loss too large (672.240->672.785)! Learning rate decreased to 0.00794.
2024-12-02-03:20:14-root-INFO: grad norm: 48.111 47.425 8.096
2024-12-02-03:20:14-root-INFO: grad norm: 45.408 44.621 8.419
2024-12-02-03:20:15-root-INFO: Loss Change: 678.327 -> 661.592
2024-12-02-03:20:15-root-INFO: Regularization Change: 0.000 -> 0.864
2024-12-02-03:20:15-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-03:20:15-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:20:16-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-03:20:16-root-INFO: grad norm: 61.418 60.652 9.671
2024-12-02-03:20:16-root-INFO: Loss too large (664.497->667.599)! Learning rate decreased to 0.00823.
2024-12-02-03:20:17-root-INFO: grad norm: 58.466 57.807 8.753
2024-12-02-03:20:18-root-INFO: grad norm: 55.265 54.681 8.015
2024-12-02-03:20:19-root-INFO: grad norm: 51.630 50.958 8.306
2024-12-02-03:20:20-root-INFO: grad norm: 48.956 48.409 7.297
2024-12-02-03:20:21-root-INFO: Loss Change: 664.497 -> 647.143
2024-12-02-03:20:21-root-INFO: Regularization Change: 0.000 -> 0.758
2024-12-02-03:20:21-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-03:20:21-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:20:21-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-03:20:22-root-INFO: grad norm: 33.128 32.147 8.004
2024-12-02-03:20:23-root-INFO: grad norm: 42.382 41.715 7.489
2024-12-02-03:20:23-root-INFO: Loss too large (640.534->641.187)! Learning rate decreased to 0.00854.
2024-12-02-03:20:24-root-INFO: grad norm: 40.386 39.567 8.092
2024-12-02-03:20:25-root-INFO: grad norm: 38.248 37.700 6.451
2024-12-02-03:20:26-root-INFO: grad norm: 37.405 36.694 7.263
2024-12-02-03:20:27-root-INFO: Loss Change: 643.142 -> 629.964
2024-12-02-03:20:27-root-INFO: Regularization Change: 0.000 -> 0.672
2024-12-02-03:20:27-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-03:20:27-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:20:27-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-03:20:27-root-INFO: grad norm: 59.724 58.852 10.164
2024-12-02-03:20:28-root-INFO: Loss too large (634.186->638.423)! Learning rate decreased to 0.00885.
2024-12-02-03:20:29-root-INFO: grad norm: 57.115 56.367 9.210
2024-12-02-03:20:30-root-INFO: grad norm: 54.259 53.648 8.119
2024-12-02-03:20:31-root-INFO: grad norm: 51.188 50.438 8.731
2024-12-02-03:20:32-root-INFO: grad norm: 48.750 48.192 7.359
2024-12-02-03:20:32-root-INFO: Loss Change: 634.186 -> 619.367
2024-12-02-03:20:32-root-INFO: Regularization Change: 0.000 -> 0.746
2024-12-02-03:20:32-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-03:20:32-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-03:20:33-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-03:20:33-root-INFO: grad norm: 36.304 35.362 8.219
2024-12-02-03:20:34-root-INFO: grad norm: 51.800 51.016 8.981
2024-12-02-03:20:34-root-INFO: Loss too large (615.126->619.332)! Learning rate decreased to 0.00917.
2024-12-02-03:20:35-root-INFO: grad norm: 47.135 46.205 9.319
2024-12-02-03:20:36-root-INFO: grad norm: 40.134 39.561 6.757
2024-12-02-03:20:37-root-INFO: grad norm: 39.314 38.587 7.529
2024-12-02-03:20:38-root-INFO: Loss Change: 616.292 -> 604.535
2024-12-02-03:20:38-root-INFO: Regularization Change: 0.000 -> 0.667
2024-12-02-03:20:38-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-03:20:38-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:20:38-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-03:20:39-root-INFO: grad norm: 54.723 54.030 8.682
2024-12-02-03:20:39-root-INFO: Loss too large (606.977->611.713)! Learning rate decreased to 0.00951.
2024-12-02-03:20:40-root-INFO: grad norm: 52.772 52.057 8.658
2024-12-02-03:20:41-root-INFO: grad norm: 50.364 49.837 7.265
2024-12-02-03:20:42-root-INFO: grad norm: 48.275 47.597 8.064
2024-12-02-03:20:43-root-INFO: grad norm: 46.548 46.059 6.735
2024-12-02-03:20:44-root-INFO: Loss Change: 606.977 -> 594.772
2024-12-02-03:20:44-root-INFO: Regularization Change: 0.000 -> 0.713
2024-12-02-03:20:44-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-03:20:44-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:20:44-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-03:20:44-root-INFO: grad norm: 33.905 32.931 8.067
2024-12-02-03:20:45-root-INFO: grad norm: 47.852 47.121 8.330
2024-12-02-03:20:46-root-INFO: Loss too large (590.133->594.044)! Learning rate decreased to 0.00985.
2024-12-02-03:20:47-root-INFO: grad norm: 42.010 41.175 8.334
2024-12-02-03:20:48-root-INFO: grad norm: 33.309 32.816 5.707
2024-12-02-03:20:49-root-INFO: grad norm: 33.749 33.133 6.417
2024-12-02-03:20:49-root-INFO: Loss Change: 591.994 -> 580.328
2024-12-02-03:20:49-root-INFO: Regularization Change: 0.000 -> 0.667
2024-12-02-03:20:49-root-INFO: Undo step: 145
2024-12-02-03:20:49-root-INFO: Undo step: 146
2024-12-02-03:20:49-root-INFO: Undo step: 147
2024-12-02-03:20:49-root-INFO: Undo step: 148
2024-12-02-03:20:49-root-INFO: Undo step: 149
2024-12-02-03:20:50-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-03:20:50-root-INFO: grad norm: 227.195 221.772 49.343
2024-12-02-03:20:51-root-INFO: grad norm: 141.138 138.272 28.298
2024-12-02-03:20:52-root-INFO: grad norm: 163.991 159.398 38.544
2024-12-02-03:20:52-root-INFO: Loss too large (827.507->832.561)! Learning rate decreased to 0.00823.
2024-12-02-03:20:53-root-INFO: grad norm: 126.695 124.940 21.012
2024-12-02-03:20:54-root-INFO: grad norm: 88.663 86.441 19.726
2024-12-02-03:20:55-root-INFO: Loss Change: 1103.069 -> 730.101
2024-12-02-03:20:55-root-INFO: Regularization Change: 0.000 -> 12.742
2024-12-02-03:20:55-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-03:20:55-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:20:55-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-03:20:56-root-INFO: grad norm: 66.017 64.388 14.574
2024-12-02-03:20:57-root-INFO: grad norm: 75.656 73.511 17.889
2024-12-02-03:20:57-root-INFO: grad norm: 90.816 89.666 14.408
2024-12-02-03:20:59-root-INFO: grad norm: 113.403 111.665 19.779
2024-12-02-03:20:59-root-INFO: Loss too large (700.900->710.730)! Learning rate decreased to 0.00854.
2024-12-02-03:21:00-root-INFO: grad norm: 94.561 93.630 13.234
2024-12-02-03:21:00-root-INFO: Loss Change: 722.879 -> 672.155
2024-12-02-03:21:00-root-INFO: Regularization Change: 0.000 -> 3.138
2024-12-02-03:21:00-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-03:21:00-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:21:01-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-03:21:01-root-INFO: grad norm: 99.776 98.523 15.762
2024-12-02-03:21:01-root-INFO: Loss too large (680.243->685.711)! Learning rate decreased to 0.00885.
2024-12-02-03:21:02-root-INFO: grad norm: 83.618 83.026 9.936
2024-12-02-03:21:03-root-INFO: grad norm: 70.256 69.376 11.080
2024-12-02-03:21:04-root-INFO: grad norm: 59.959 59.371 8.376
2024-12-02-03:21:05-root-INFO: grad norm: 52.911 52.142 8.985
2024-12-02-03:21:06-root-INFO: Loss Change: 680.243 -> 635.938
2024-12-02-03:21:06-root-INFO: Regularization Change: 0.000 -> 1.588
2024-12-02-03:21:06-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-03:21:06-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-03:21:06-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-03:21:07-root-INFO: grad norm: 32.510 31.829 6.622
2024-12-02-03:21:08-root-INFO: grad norm: 38.238 37.386 8.024
2024-12-02-03:21:09-root-INFO: grad norm: 49.165 48.658 7.043
2024-12-02-03:21:09-root-INFO: Loss too large (624.563->624.858)! Learning rate decreased to 0.00917.
2024-12-02-03:21:10-root-INFO: grad norm: 45.130 44.463 7.725
2024-12-02-03:21:11-root-INFO: grad norm: 41.616 41.124 6.383
2024-12-02-03:21:12-root-INFO: Loss Change: 631.158 -> 612.819
2024-12-02-03:21:12-root-INFO: Regularization Change: 0.000 -> 1.083
2024-12-02-03:21:12-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-03:21:12-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:21:12-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-03:21:12-root-INFO: grad norm: 56.966 56.178 9.445
2024-12-02-03:21:13-root-INFO: Loss too large (615.755->618.036)! Learning rate decreased to 0.00951.
2024-12-02-03:21:14-root-INFO: grad norm: 52.462 51.973 7.147
2024-12-02-03:21:15-root-INFO: grad norm: 48.073 47.468 7.604
2024-12-02-03:21:16-root-INFO: grad norm: 43.912 43.420 6.555
2024-12-02-03:21:17-root-INFO: grad norm: 40.637 40.075 6.737
2024-12-02-03:21:17-root-INFO: Loss Change: 615.755 -> 596.758
2024-12-02-03:21:17-root-INFO: Regularization Change: 0.000 -> 0.882
2024-12-02-03:21:17-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-03:21:17-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:21:18-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-03:21:18-root-INFO: grad norm: 22.928 22.074 6.198
2024-12-02-03:21:19-root-INFO: grad norm: 23.229 22.470 5.891
2024-12-02-03:21:20-root-INFO: grad norm: 26.905 26.333 5.518
2024-12-02-03:21:21-root-INFO: grad norm: 34.670 34.058 6.489
2024-12-02-03:21:22-root-INFO: grad norm: 44.023 43.517 6.652
2024-12-02-03:21:22-root-INFO: Loss too large (584.429->584.603)! Learning rate decreased to 0.00985.
2024-12-02-03:21:23-root-INFO: Loss Change: 593.361 -> 580.470
2024-12-02-03:21:23-root-INFO: Regularization Change: 0.000 -> 0.975
2024-12-02-03:21:23-root-INFO: Undo step: 145
2024-12-02-03:21:23-root-INFO: Undo step: 146
2024-12-02-03:21:23-root-INFO: Undo step: 147
2024-12-02-03:21:23-root-INFO: Undo step: 148
2024-12-02-03:21:23-root-INFO: Undo step: 149
2024-12-02-03:21:23-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-03:21:24-root-INFO: grad norm: 280.138 274.267 57.049
2024-12-02-03:21:25-root-INFO: grad norm: 133.822 131.171 26.507
2024-12-02-03:21:26-root-INFO: grad norm: 98.841 96.891 19.536
2024-12-02-03:21:27-root-INFO: grad norm: 80.261 78.629 16.103
2024-12-02-03:21:28-root-INFO: grad norm: 73.876 72.211 15.596
2024-12-02-03:21:28-root-INFO: Loss Change: 1236.572 -> 706.856
2024-12-02-03:21:28-root-INFO: Regularization Change: 0.000 -> 19.255
2024-12-02-03:21:28-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-03:21:28-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:21:29-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-03:21:29-root-INFO: grad norm: 63.439 61.835 14.175
2024-12-02-03:21:30-root-INFO: grad norm: 63.794 61.768 15.948
2024-12-02-03:21:31-root-INFO: grad norm: 66.647 64.934 15.013
2024-12-02-03:21:32-root-INFO: grad norm: 70.830 68.908 16.391
2024-12-02-03:21:33-root-INFO: grad norm: 76.161 74.527 15.694
2024-12-02-03:21:34-root-INFO: Loss Change: 698.553 -> 655.310
2024-12-02-03:21:34-root-INFO: Regularization Change: 0.000 -> 3.251
2024-12-02-03:21:34-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-03:21:34-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-03:21:34-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-03:21:34-root-INFO: grad norm: 105.058 103.436 18.391
2024-12-02-03:21:36-root-INFO: grad norm: 106.257 105.021 16.162
2024-12-02-03:21:36-root-INFO: grad norm: 102.034 100.591 17.099
2024-12-02-03:21:37-root-INFO: grad norm: 98.215 96.970 15.590
2024-12-02-03:21:38-root-INFO: grad norm: 94.116 92.732 16.084
2024-12-02-03:21:39-root-INFO: Loss Change: 663.481 -> 634.800
2024-12-02-03:21:39-root-INFO: Regularization Change: 0.000 -> 2.567
2024-12-02-03:21:39-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-03:21:39-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-03:21:39-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-03:21:40-root-INFO: grad norm: 79.462 78.140 14.435
2024-12-02-03:21:41-root-INFO: grad norm: 80.332 78.780 15.716
2024-12-02-03:21:42-root-INFO: grad norm: 81.305 80.093 13.984
2024-12-02-03:21:43-root-INFO: grad norm: 82.477 81.090 15.063
2024-12-02-03:21:44-root-INFO: grad norm: 83.219 82.055 13.870
2024-12-02-03:21:45-root-INFO: Loss Change: 625.487 -> 608.735
2024-12-02-03:21:45-root-INFO: Regularization Change: 0.000 -> 1.897
2024-12-02-03:21:45-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-03:21:45-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:21:45-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-03:21:45-root-INFO: grad norm: 100.274 98.975 16.088
2024-12-02-03:21:46-root-INFO: grad norm: 96.273 95.220 14.199
2024-12-02-03:21:47-root-INFO: grad norm: 89.225 87.980 14.850
2024-12-02-03:21:48-root-INFO: grad norm: 85.032 83.886 13.917
2024-12-02-03:21:49-root-INFO: grad norm: 81.595 80.278 14.602
2024-12-02-03:21:50-root-INFO: Loss Change: 616.258 -> 591.519
2024-12-02-03:21:50-root-INFO: Regularization Change: 0.000 -> 2.015
2024-12-02-03:21:50-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-03:21:50-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:21:50-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-03:21:50-root-INFO: grad norm: 64.495 62.822 14.596
2024-12-02-03:21:51-root-INFO: grad norm: 66.030 64.161 15.599
2024-12-02-03:21:52-root-INFO: grad norm: 67.749 66.312 13.883
2024-12-02-03:21:53-root-INFO: grad norm: 70.026 68.400 14.998
2024-12-02-03:21:54-root-INFO: grad norm: 71.654 70.345 13.630
2024-12-02-03:21:55-root-INFO: Loss Change: 582.403 -> 572.947
2024-12-02-03:21:55-root-INFO: Regularization Change: 0.000 -> 1.507
2024-12-02-03:21:55-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-03:21:55-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:21:55-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-03:21:56-root-INFO: grad norm: 89.450 88.045 15.794
2024-12-02-03:21:57-root-INFO: grad norm: 85.675 84.576 13.677
2024-12-02-03:21:58-root-INFO: grad norm: 80.120 78.794 14.517
2024-12-02-03:21:59-root-INFO: grad norm: 75.895 74.702 13.402
2024-12-02-03:22:00-root-INFO: grad norm: 72.450 71.068 14.081
2024-12-02-03:22:01-root-INFO: Loss Change: 579.631 -> 559.067
2024-12-02-03:22:01-root-INFO: Regularization Change: 0.000 -> 1.822
2024-12-02-03:22:01-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-03:22:01-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-03:22:01-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-03:22:01-root-INFO: grad norm: 62.667 61.066 14.073
2024-12-02-03:22:02-root-INFO: grad norm: 63.653 61.919 14.753
2024-12-02-03:22:03-root-INFO: grad norm: 63.546 62.136 13.311
2024-12-02-03:22:04-root-INFO: grad norm: 63.661 62.091 14.049
2024-12-02-03:22:05-root-INFO: grad norm: 63.604 62.294 12.843
2024-12-02-03:22:06-root-INFO: Loss Change: 553.771 -> 541.609
2024-12-02-03:22:06-root-INFO: Regularization Change: 0.000 -> 1.421
2024-12-02-03:22:06-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-03:22:06-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-03:22:06-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-03:22:06-root-INFO: grad norm: 83.660 82.370 14.633
2024-12-02-03:22:07-root-INFO: Loss too large (550.131->550.935)! Learning rate decreased to 0.01095.
2024-12-02-03:22:08-root-INFO: grad norm: 54.803 54.148 8.447
2024-12-02-03:22:09-root-INFO: grad norm: 34.657 33.990 6.769
2024-12-02-03:22:10-root-INFO: grad norm: 25.848 25.339 5.108
2024-12-02-03:22:11-root-INFO: grad norm: 20.606 20.032 4.830
2024-12-02-03:22:11-root-INFO: Loss Change: 550.131 -> 519.153
2024-12-02-03:22:11-root-INFO: Regularization Change: 0.000 -> 0.893
2024-12-02-03:22:11-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-03:22:11-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-03:22:12-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-03:22:12-root-INFO: grad norm: 18.215 17.452 5.218
2024-12-02-03:22:13-root-INFO: grad norm: 18.423 17.713 5.064
2024-12-02-03:22:14-root-INFO: grad norm: 20.801 20.137 5.212
2024-12-02-03:22:15-root-INFO: grad norm: 25.750 25.036 6.022
2024-12-02-03:22:16-root-INFO: grad norm: 29.534 28.835 6.388
2024-12-02-03:22:17-root-INFO: Loss Change: 518.313 -> 509.852
2024-12-02-03:22:17-root-INFO: Regularization Change: 0.000 -> 0.848
2024-12-02-03:22:17-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-03:22:17-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-03:22:17-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-03:22:17-root-INFO: grad norm: 41.274 40.635 7.238
2024-12-02-03:22:18-root-INFO: Loss too large (510.395->511.590)! Learning rate decreased to 0.01174.
2024-12-02-03:22:19-root-INFO: grad norm: 32.507 31.968 5.894
2024-12-02-03:22:20-root-INFO: grad norm: 22.689 22.148 4.923
2024-12-02-03:22:21-root-INFO: grad norm: 20.796 20.306 4.488
2024-12-02-03:22:22-root-INFO: grad norm: 19.102 18.601 4.346
2024-12-02-03:22:22-root-INFO: Loss Change: 510.395 -> 498.186
2024-12-02-03:22:22-root-INFO: Regularization Change: 0.000 -> 0.605
2024-12-02-03:22:22-root-INFO: Undo step: 140
2024-12-02-03:22:22-root-INFO: Undo step: 141
2024-12-02-03:22:22-root-INFO: Undo step: 142
2024-12-02-03:22:22-root-INFO: Undo step: 143
2024-12-02-03:22:22-root-INFO: Undo step: 144
2024-12-02-03:22:23-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-03:22:23-root-INFO: grad norm: 277.310 271.317 57.343
2024-12-02-03:22:24-root-INFO: grad norm: 136.119 132.024 33.139
2024-12-02-03:22:25-root-INFO: grad norm: 71.557 69.468 17.164
2024-12-02-03:22:26-root-INFO: grad norm: 56.668 54.888 14.094
2024-12-02-03:22:27-root-INFO: grad norm: 50.344 49.199 10.676
2024-12-02-03:22:28-root-INFO: Loss Change: 1135.753 -> 593.996
2024-12-02-03:22:28-root-INFO: Regularization Change: 0.000 -> 21.326
2024-12-02-03:22:28-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-03:22:28-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:22:28-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-03:22:28-root-INFO: grad norm: 36.906 35.640 9.585
2024-12-02-03:22:29-root-INFO: grad norm: 30.815 29.658 8.362
2024-12-02-03:22:30-root-INFO: grad norm: 28.193 27.188 7.463
2024-12-02-03:22:31-root-INFO: grad norm: 27.287 26.352 7.082
2024-12-02-03:22:32-root-INFO: grad norm: 27.831 27.025 6.650
2024-12-02-03:22:33-root-INFO: Loss Change: 591.100 -> 552.872
2024-12-02-03:22:33-root-INFO: Regularization Change: 0.000 -> 2.520
2024-12-02-03:22:33-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-03:22:33-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-03:22:33-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-03:22:34-root-INFO: grad norm: 43.154 42.551 7.189
2024-12-02-03:22:35-root-INFO: grad norm: 49.470 48.935 7.257
2024-12-02-03:22:36-root-INFO: grad norm: 60.105 59.589 7.864
2024-12-02-03:22:36-root-INFO: Loss too large (549.350->551.615)! Learning rate decreased to 0.01057.
2024-12-02-03:22:37-root-INFO: grad norm: 46.725 46.201 6.976
2024-12-02-03:22:38-root-INFO: grad norm: 33.659 33.208 5.492
2024-12-02-03:22:39-root-INFO: Loss Change: 553.597 -> 533.612
2024-12-02-03:22:39-root-INFO: Regularization Change: 0.000 -> 1.167
2024-12-02-03:22:39-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-03:22:39-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-03:22:39-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-03:22:39-root-INFO: grad norm: 26.497 25.560 6.984
2024-12-02-03:22:40-root-INFO: grad norm: 29.860 29.170 6.380
2024-12-02-03:22:41-root-INFO: grad norm: 33.229 32.595 6.457
2024-12-02-03:22:42-root-INFO: grad norm: 40.867 40.198 7.368
2024-12-02-03:22:43-root-INFO: Loss too large (524.168->525.470)! Learning rate decreased to 0.01095.
2024-12-02-03:22:44-root-INFO: grad norm: 33.146 32.571 6.148
2024-12-02-03:22:44-root-INFO: Loss Change: 532.510 -> 517.233
2024-12-02-03:22:44-root-INFO: Regularization Change: 0.000 -> 0.978
2024-12-02-03:22:44-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-03:22:44-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-03:22:45-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-03:22:45-root-INFO: grad norm: 26.475 25.946 5.263
2024-12-02-03:22:46-root-INFO: grad norm: 29.658 29.167 5.375
2024-12-02-03:22:47-root-INFO: grad norm: 39.615 39.119 6.246
2024-12-02-03:22:47-root-INFO: Loss too large (513.088->514.769)! Learning rate decreased to 0.01134.
2024-12-02-03:22:48-root-INFO: grad norm: 32.674 32.193 5.585
2024-12-02-03:22:49-root-INFO: grad norm: 23.228 22.780 4.541
2024-12-02-03:22:50-root-INFO: Loss Change: 516.822 -> 504.907
2024-12-02-03:22:50-root-INFO: Regularization Change: 0.000 -> 0.777
2024-12-02-03:22:50-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-03:22:50-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-03:22:50-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-03:22:51-root-INFO: grad norm: 25.148 24.484 5.742
2024-12-02-03:22:52-root-INFO: grad norm: 31.229 30.687 5.791
2024-12-02-03:22:52-root-INFO: Loss too large (502.312->502.553)! Learning rate decreased to 0.01174.
2024-12-02-03:22:53-root-INFO: grad norm: 27.840 27.342 5.237
2024-12-02-03:22:54-root-INFO: grad norm: 24.117 23.640 4.775
2024-12-02-03:22:55-root-INFO: grad norm: 23.042 22.578 4.603
2024-12-02-03:22:56-root-INFO: Loss Change: 504.672 -> 493.687
2024-12-02-03:22:56-root-INFO: Regularization Change: 0.000 -> 0.686
2024-12-02-03:22:56-root-INFO: Undo step: 140
2024-12-02-03:22:56-root-INFO: Undo step: 141
2024-12-02-03:22:56-root-INFO: Undo step: 142
2024-12-02-03:22:56-root-INFO: Undo step: 143
2024-12-02-03:22:56-root-INFO: Undo step: 144
2024-12-02-03:22:56-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-03:22:56-root-INFO: grad norm: 323.369 317.837 59.560
2024-12-02-03:22:57-root-INFO: grad norm: 186.826 184.458 29.648
2024-12-02-03:22:58-root-INFO: grad norm: 141.600 138.565 29.163
2024-12-02-03:22:59-root-INFO: grad norm: 111.940 110.307 19.053
2024-12-02-03:23:00-root-INFO: grad norm: 103.078 100.994 20.623
2024-12-02-03:23:01-root-INFO: Loss Change: 1229.239 -> 617.592
2024-12-02-03:23:01-root-INFO: Regularization Change: 0.000 -> 25.427
2024-12-02-03:23:01-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-03:23:01-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-03:23:01-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-03:23:02-root-INFO: grad norm: 107.984 106.284 19.083
2024-12-02-03:23:03-root-INFO: grad norm: 97.317 95.585 18.280
2024-12-02-03:23:04-root-INFO: grad norm: 81.662 80.362 14.510
2024-12-02-03:23:05-root-INFO: grad norm: 74.541 73.161 14.278
2024-12-02-03:23:06-root-INFO: grad norm: 64.425 63.310 11.934
2024-12-02-03:23:06-root-INFO: Loss Change: 619.985 -> 560.750
2024-12-02-03:23:06-root-INFO: Regularization Change: 0.000 -> 3.991
2024-12-02-03:23:06-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-03:23:06-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-03:23:07-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-03:23:07-root-INFO: grad norm: 61.220 60.118 11.567
2024-12-02-03:23:08-root-INFO: grad norm: 53.765 52.798 10.150
2024-12-02-03:23:09-root-INFO: grad norm: 52.350 51.408 9.887
2024-12-02-03:23:10-root-INFO: grad norm: 50.282 49.423 9.253
2024-12-02-03:23:11-root-INFO: grad norm: 49.869 49.038 9.071
2024-12-02-03:23:12-root-INFO: Loss Change: 560.097 -> 527.292
2024-12-02-03:23:12-root-INFO: Regularization Change: 0.000 -> 2.060
2024-12-02-03:23:12-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-03:23:12-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-03:23:12-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-03:23:12-root-INFO: grad norm: 43.716 42.817 8.817
2024-12-02-03:23:13-root-INFO: grad norm: 40.658 39.746 8.564
2024-12-02-03:23:14-root-INFO: grad norm: 39.609 38.943 7.234
2024-12-02-03:23:15-root-INFO: grad norm: 38.218 37.444 7.652
2024-12-02-03:23:16-root-INFO: grad norm: 36.217 35.615 6.578
2024-12-02-03:23:17-root-INFO: Loss Change: 524.873 -> 508.817
2024-12-02-03:23:17-root-INFO: Regularization Change: 0.000 -> 1.371
2024-12-02-03:23:17-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-03:23:17-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-03:23:17-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-03:23:18-root-INFO: grad norm: 42.354 41.608 7.916
2024-12-02-03:23:19-root-INFO: grad norm: 44.342 43.791 6.966
2024-12-02-03:23:19-root-INFO: Loss too large (504.308->504.969)! Learning rate decreased to 0.01134.
2024-12-02-03:23:20-root-INFO: grad norm: 34.059 33.518 6.043
2024-12-02-03:23:21-root-INFO: grad norm: 24.571 24.125 4.662
2024-12-02-03:23:22-root-INFO: grad norm: 21.756 21.268 4.581
2024-12-02-03:23:23-root-INFO: Loss Change: 509.163 -> 491.740
2024-12-02-03:23:23-root-INFO: Regularization Change: 0.000 -> 0.820
2024-12-02-03:23:23-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-03:23:23-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-03:23:23-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-03:23:23-root-INFO: grad norm: 25.258 24.745 5.061
2024-12-02-03:23:24-root-INFO: grad norm: 27.505 27.022 5.129
2024-12-02-03:23:25-root-INFO: grad norm: 36.448 36.103 5.006
2024-12-02-03:23:26-root-INFO: Loss too large (487.669->488.853)! Learning rate decreased to 0.01174.
2024-12-02-03:23:27-root-INFO: grad norm: 29.890 29.453 5.094
2024-12-02-03:23:28-root-INFO: grad norm: 21.716 21.373 3.845
2024-12-02-03:23:28-root-INFO: Loss Change: 491.353 -> 480.330
2024-12-02-03:23:28-root-INFO: Regularization Change: 0.000 -> 0.738
2024-12-02-03:23:28-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-03:23:28-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:23:29-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-03:23:29-root-INFO: grad norm: 26.109 25.500 5.609
2024-12-02-03:23:30-root-INFO: grad norm: 33.699 33.330 4.972
2024-12-02-03:23:30-root-INFO: Loss too large (478.594->479.638)! Learning rate decreased to 0.01215.
2024-12-02-03:23:31-root-INFO: grad norm: 29.013 28.586 4.959
2024-12-02-03:23:32-root-INFO: grad norm: 23.498 23.150 4.032
2024-12-02-03:23:33-root-INFO: grad norm: 22.568 22.165 4.247
2024-12-02-03:23:34-root-INFO: Loss Change: 480.487 -> 470.150
2024-12-02-03:23:34-root-INFO: Regularization Change: 0.000 -> 0.659
2024-12-02-03:23:34-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-03:23:34-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:23:34-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-03:23:35-root-INFO: grad norm: 24.934 24.568 4.258
2024-12-02-03:23:36-root-INFO: grad norm: 28.517 28.106 4.825
2024-12-02-03:23:37-root-INFO: grad norm: 38.301 37.977 4.967
2024-12-02-03:23:37-root-INFO: Loss too large (467.564->469.930)! Learning rate decreased to 0.01258.
2024-12-02-03:23:38-root-INFO: grad norm: 30.071 29.695 4.745
2024-12-02-03:23:39-root-INFO: grad norm: 19.020 18.718 3.377
2024-12-02-03:23:40-root-INFO: Loss Change: 469.733 -> 460.298
2024-12-02-03:23:40-root-INFO: Regularization Change: 0.000 -> 0.666
2024-12-02-03:23:40-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-03:23:40-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:23:40-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-03:23:40-root-INFO: grad norm: 28.072 27.325 6.434
2024-12-02-03:23:41-root-INFO: grad norm: 33.537 33.148 5.095
2024-12-02-03:23:42-root-INFO: Loss too large (459.386->460.520)! Learning rate decreased to 0.01302.
2024-12-02-03:23:43-root-INFO: grad norm: 29.204 28.805 4.810
2024-12-02-03:23:44-root-INFO: grad norm: 25.691 25.360 4.107
2024-12-02-03:23:45-root-INFO: grad norm: 24.575 24.212 4.205
2024-12-02-03:23:46-root-INFO: Loss Change: 461.573 -> 451.373
2024-12-02-03:23:46-root-INFO: Regularization Change: 0.000 -> 0.695
2024-12-02-03:23:46-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-03:23:46-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-03:23:46-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-03:23:46-root-INFO: grad norm: 25.927 25.620 3.977
2024-12-02-03:23:47-root-INFO: Loss too large (451.380->451.629)! Learning rate decreased to 0.01347.
2024-12-02-03:23:48-root-INFO: grad norm: 24.376 24.023 4.129
2024-12-02-03:23:49-root-INFO: grad norm: 23.348 23.059 3.656
2024-12-02-03:23:50-root-INFO: grad norm: 23.040 22.703 3.928
2024-12-02-03:23:51-root-INFO: grad norm: 22.702 22.414 3.606
2024-12-02-03:23:51-root-INFO: Loss Change: 451.380 -> 444.019
2024-12-02-03:23:51-root-INFO: Regularization Change: 0.000 -> 0.588
2024-12-02-03:23:51-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-03:23:51-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-03:23:52-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-03:23:52-root-INFO: grad norm: 27.246 26.785 4.993
2024-12-02-03:23:53-root-INFO: grad norm: 36.699 36.357 4.999
2024-12-02-03:23:53-root-INFO: Loss too large (442.844->445.386)! Learning rate decreased to 0.01393.
2024-12-02-03:23:54-root-INFO: grad norm: 29.892 29.551 4.497
2024-12-02-03:23:55-root-INFO: grad norm: 21.476 21.178 3.565
2024-12-02-03:23:56-root-INFO: grad norm: 21.120 20.801 3.657
2024-12-02-03:23:57-root-INFO: Loss Change: 443.964 -> 434.566
2024-12-02-03:23:57-root-INFO: Regularization Change: 0.000 -> 0.664
2024-12-02-03:23:57-root-INFO: Undo step: 135
2024-12-02-03:23:57-root-INFO: Undo step: 136
2024-12-02-03:23:57-root-INFO: Undo step: 137
2024-12-02-03:23:57-root-INFO: Undo step: 138
2024-12-02-03:23:57-root-INFO: Undo step: 139
2024-12-02-03:23:57-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-03:23:58-root-INFO: grad norm: 201.105 196.685 41.933
2024-12-02-03:23:59-root-INFO: grad norm: 93.311 90.085 24.323
2024-12-02-03:23:59-root-INFO: grad norm: 60.688 58.517 16.088
2024-12-02-03:24:00-root-INFO: grad norm: 49.117 46.840 14.780
2024-12-02-03:24:01-root-INFO: grad norm: 48.461 46.950 12.006
2024-12-02-03:24:02-root-INFO: Loss Change: 974.529 -> 539.701
2024-12-02-03:24:02-root-INFO: Regularization Change: 0.000 -> 22.085
2024-12-02-03:24:02-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-03:24:02-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:24:02-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-03:24:03-root-INFO: grad norm: 48.717 47.351 11.455
2024-12-02-03:24:04-root-INFO: grad norm: 61.019 60.042 10.877
2024-12-02-03:24:05-root-INFO: grad norm: 44.381 43.444 9.070
2024-12-02-03:24:06-root-INFO: grad norm: 37.124 36.381 7.393
2024-12-02-03:24:07-root-INFO: grad norm: 35.029 34.125 7.908
2024-12-02-03:24:07-root-INFO: Loss Change: 537.602 -> 490.089
2024-12-02-03:24:08-root-INFO: Regularization Change: 0.000 -> 3.513
2024-12-02-03:24:08-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-03:24:08-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:24:08-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-03:24:08-root-INFO: grad norm: 36.237 35.527 7.137
2024-12-02-03:24:09-root-INFO: grad norm: 39.748 38.976 7.794
2024-12-02-03:24:09-root-INFO: Loss too large (481.434->482.188)! Learning rate decreased to 0.01258.
2024-12-02-03:24:10-root-INFO: grad norm: 34.737 34.192 6.130
2024-12-02-03:24:11-root-INFO: grad norm: 28.126 27.525 5.780
2024-12-02-03:24:12-root-INFO: grad norm: 29.491 28.967 5.534
2024-12-02-03:24:13-root-INFO: Loss Change: 489.495 -> 466.907
2024-12-02-03:24:13-root-INFO: Regularization Change: 0.000 -> 1.447
2024-12-02-03:24:13-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-03:24:13-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:24:13-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-03:24:14-root-INFO: grad norm: 46.425 45.847 7.299
2024-12-02-03:24:14-root-INFO: Loss too large (468.562->471.757)! Learning rate decreased to 0.01302.
2024-12-02-03:24:15-root-INFO: grad norm: 37.965 37.487 6.005
2024-12-02-03:24:16-root-INFO: grad norm: 25.884 25.418 4.886
2024-12-02-03:24:17-root-INFO: grad norm: 27.546 27.076 5.065
2024-12-02-03:24:18-root-INFO: grad norm: 34.802 34.367 5.483
2024-12-02-03:24:18-root-INFO: Loss too large (454.418->454.976)! Learning rate decreased to 0.01041.
2024-12-02-03:24:19-root-INFO: Loss Change: 468.562 -> 453.065
2024-12-02-03:24:19-root-INFO: Regularization Change: 0.000 -> 0.996
2024-12-02-03:24:19-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-03:24:19-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-03:24:19-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-03:24:20-root-INFO: grad norm: 28.669 28.224 5.034
2024-12-02-03:24:21-root-INFO: grad norm: 48.419 47.963 6.634
2024-12-02-03:24:21-root-INFO: Loss too large (451.706->457.639)! Learning rate decreased to 0.01347.
2024-12-02-03:24:21-root-INFO: Loss too large (451.706->453.366)! Learning rate decreased to 0.01077.
2024-12-02-03:24:22-root-INFO: grad norm: 31.929 31.549 4.907
2024-12-02-03:24:23-root-INFO: grad norm: 14.423 13.933 3.730
2024-12-02-03:24:24-root-INFO: grad norm: 13.656 13.153 3.672
2024-12-02-03:24:25-root-INFO: Loss Change: 452.517 -> 440.086
2024-12-02-03:24:25-root-INFO: Regularization Change: 0.000 -> 0.634
2024-12-02-03:24:25-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-03:24:25-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-03:24:25-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-03:24:26-root-INFO: grad norm: 20.799 20.367 4.214
2024-12-02-03:24:27-root-INFO: grad norm: 24.388 24.019 4.226
2024-12-02-03:24:27-root-INFO: Loss too large (437.540->438.072)! Learning rate decreased to 0.01393.
2024-12-02-03:24:28-root-INFO: grad norm: 30.883 30.557 4.477
2024-12-02-03:24:28-root-INFO: Loss too large (436.044->436.705)! Learning rate decreased to 0.01115.
2024-12-02-03:24:29-root-INFO: grad norm: 26.706 26.363 4.264
2024-12-02-03:24:30-root-INFO: grad norm: 22.203 21.859 3.891
2024-12-02-03:24:31-root-INFO: Loss Change: 439.829 -> 430.984
2024-12-02-03:24:31-root-INFO: Regularization Change: 0.000 -> 0.626
2024-12-02-03:24:31-root-INFO: Undo step: 135
2024-12-02-03:24:31-root-INFO: Undo step: 136
2024-12-02-03:24:31-root-INFO: Undo step: 137
2024-12-02-03:24:31-root-INFO: Undo step: 138
2024-12-02-03:24:31-root-INFO: Undo step: 139
2024-12-02-03:24:31-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-03:24:32-root-INFO: grad norm: 190.659 186.560 39.321
2024-12-02-03:24:33-root-INFO: grad norm: 99.414 96.689 23.115
2024-12-02-03:24:34-root-INFO: grad norm: 75.364 73.823 15.160
2024-12-02-03:24:35-root-INFO: grad norm: 82.006 80.211 17.065
2024-12-02-03:24:36-root-INFO: grad norm: 82.553 81.228 14.730
2024-12-02-03:24:36-root-INFO: Loss Change: 977.063 -> 547.519
2024-12-02-03:24:36-root-INFO: Regularization Change: 0.000 -> 22.813
2024-12-02-03:24:36-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-03:24:36-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:24:37-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-03:24:37-root-INFO: grad norm: 72.022 70.736 13.552
2024-12-02-03:24:38-root-INFO: grad norm: 69.060 67.965 12.252
2024-12-02-03:24:39-root-INFO: grad norm: 62.791 61.725 11.526
2024-12-02-03:24:39-root-INFO: Loss too large (515.116->515.160)! Learning rate decreased to 0.01215.
2024-12-02-03:24:40-root-INFO: grad norm: 48.833 48.040 8.766
2024-12-02-03:24:41-root-INFO: grad norm: 33.596 32.861 6.987
2024-12-02-03:24:42-root-INFO: Loss Change: 540.180 -> 489.872
2024-12-02-03:24:42-root-INFO: Regularization Change: 0.000 -> 3.063
2024-12-02-03:24:42-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-03:24:42-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:24:42-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-03:24:43-root-INFO: grad norm: 35.235 34.610 6.610
2024-12-02-03:24:44-root-INFO: grad norm: 40.233 39.592 7.157
2024-12-02-03:24:45-root-INFO: grad norm: 43.364 42.777 7.115
2024-12-02-03:24:46-root-INFO: grad norm: 49.055 48.423 7.851
2024-12-02-03:24:46-root-INFO: Loss too large (477.312->478.216)! Learning rate decreased to 0.01258.
2024-12-02-03:24:47-root-INFO: grad norm: 39.036 38.496 6.474
2024-12-02-03:24:48-root-INFO: Loss Change: 489.684 -> 466.231
2024-12-02-03:24:48-root-INFO: Regularization Change: 0.000 -> 1.757
2024-12-02-03:24:48-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-03:24:48-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-03:24:48-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-03:24:48-root-INFO: grad norm: 31.094 30.549 5.800
2024-12-02-03:24:49-root-INFO: grad norm: 32.386 31.906 5.553
2024-12-02-03:24:50-root-INFO: grad norm: 38.933 38.554 5.419
2024-12-02-03:24:51-root-INFO: Loss too large (458.700->459.029)! Learning rate decreased to 0.01302.
2024-12-02-03:24:52-root-INFO: grad norm: 31.189 30.762 5.138
2024-12-02-03:24:53-root-INFO: grad norm: 22.743 22.381 4.041
2024-12-02-03:24:53-root-INFO: Loss Change: 465.105 -> 448.360
2024-12-02-03:24:53-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-02-03:24:53-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-03:24:53-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-03:24:54-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-03:24:54-root-INFO: grad norm: 21.922 21.470 4.428
2024-12-02-03:24:55-root-INFO: grad norm: 26.562 26.198 4.378
2024-12-02-03:24:56-root-INFO: grad norm: 30.168 29.758 4.958
2024-12-02-03:24:57-root-INFO: grad norm: 36.403 36.023 5.244
2024-12-02-03:24:57-root-INFO: Loss too large (442.117->443.170)! Learning rate decreased to 0.01347.
2024-12-02-03:24:59-root-INFO: grad norm: 29.972 29.581 4.828
2024-12-02-03:24:59-root-INFO: Loss Change: 447.837 -> 435.833
2024-12-02-03:24:59-root-INFO: Regularization Change: 0.000 -> 1.010
2024-12-02-03:24:59-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-03:24:59-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-03:25:00-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-03:25:00-root-INFO: grad norm: 24.731 24.388 4.103
2024-12-02-03:25:01-root-INFO: grad norm: 26.764 26.403 4.382
2024-12-02-03:25:02-root-INFO: grad norm: 33.186 32.885 4.465
2024-12-02-03:25:02-root-INFO: Loss too large (431.513->432.205)! Learning rate decreased to 0.01393.
2024-12-02-03:25:03-root-INFO: grad norm: 26.918 26.572 4.302
2024-12-02-03:25:04-root-INFO: grad norm: 20.200 19.911 3.404
2024-12-02-03:25:05-root-INFO: Loss Change: 435.012 -> 424.177
2024-12-02-03:25:05-root-INFO: Regularization Change: 0.000 -> 0.847
2024-12-02-03:25:05-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-03:25:05-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-03:25:05-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-03:25:05-root-INFO: grad norm: 24.358 23.900 4.703
2024-12-02-03:25:06-root-INFO: grad norm: 30.283 29.973 4.319
2024-12-02-03:25:07-root-INFO: Loss too large (422.762->423.502)! Learning rate decreased to 0.01441.
2024-12-02-03:25:08-root-INFO: grad norm: 27.026 26.677 4.328
2024-12-02-03:25:09-root-INFO: grad norm: 23.893 23.610 3.667
2024-12-02-03:25:10-root-INFO: grad norm: 22.227 21.906 3.759
2024-12-02-03:25:10-root-INFO: Loss Change: 424.297 -> 414.749
2024-12-02-03:25:10-root-INFO: Regularization Change: 0.000 -> 0.749
2024-12-02-03:25:10-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-03:25:10-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-03:25:11-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-03:25:11-root-INFO: grad norm: 21.493 21.152 3.809
2024-12-02-03:25:12-root-INFO: grad norm: 23.715 23.405 3.821
2024-12-02-03:25:13-root-INFO: grad norm: 30.918 30.641 4.125
2024-12-02-03:25:13-root-INFO: Loss too large (411.757->412.881)! Learning rate decreased to 0.01490.
2024-12-02-03:25:14-root-INFO: grad norm: 25.945 25.640 3.964
2024-12-02-03:25:15-root-INFO: grad norm: 20.577 20.312 3.294
2024-12-02-03:25:16-root-INFO: Loss Change: 414.342 -> 405.693
2024-12-02-03:25:16-root-INFO: Regularization Change: 0.000 -> 0.768
2024-12-02-03:25:16-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-03:25:16-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-03:25:16-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-03:25:17-root-INFO: grad norm: 23.637 23.254 4.236
2024-12-02-03:25:18-root-INFO: grad norm: 30.926 30.635 4.233
2024-12-02-03:25:18-root-INFO: Loss too large (404.909->406.621)! Learning rate decreased to 0.01541.
2024-12-02-03:25:19-root-INFO: grad norm: 28.157 27.851 4.141
2024-12-02-03:25:20-root-INFO: grad norm: 25.281 25.022 3.606
2024-12-02-03:25:21-root-INFO: grad norm: 23.944 23.669 3.619
2024-12-02-03:25:22-root-INFO: Loss Change: 405.755 -> 397.834
2024-12-02-03:25:22-root-INFO: Regularization Change: 0.000 -> 0.720
2024-12-02-03:25:22-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-03:25:22-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-03:25:22-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-03:25:22-root-INFO: grad norm: 22.312 22.001 3.711
2024-12-02-03:25:23-root-INFO: grad norm: 25.477 25.196 3.773
2024-12-02-03:25:24-root-INFO: grad norm: 32.931 32.661 4.210
2024-12-02-03:25:25-root-INFO: Loss too large (395.792->397.677)! Learning rate decreased to 0.01593.
2024-12-02-03:25:26-root-INFO: grad norm: 27.763 27.489 3.887
2024-12-02-03:25:27-root-INFO: grad norm: 22.593 22.344 3.343
2024-12-02-03:25:27-root-INFO: Loss Change: 397.435 -> 389.680
2024-12-02-03:25:27-root-INFO: Regularization Change: 0.000 -> 0.777
2024-12-02-03:25:27-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-03:25:27-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-03:25:28-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-03:25:28-root-INFO: grad norm: 25.250 24.921 4.065
2024-12-02-03:25:28-root-INFO: Loss too large (390.284->390.321)! Learning rate decreased to 0.01647.
2024-12-02-03:25:29-root-INFO: grad norm: 23.042 22.809 3.266
2024-12-02-03:25:30-root-INFO: grad norm: 21.991 21.742 3.299
2024-12-02-03:25:31-root-INFO: grad norm: 21.061 20.830 3.112
2024-12-02-03:25:32-root-INFO: grad norm: 20.312 20.076 3.085
2024-12-02-03:25:33-root-INFO: Loss Change: 390.284 -> 382.586
2024-12-02-03:25:33-root-INFO: Regularization Change: 0.000 -> 0.658
2024-12-02-03:25:33-root-INFO: Undo step: 130
2024-12-02-03:25:33-root-INFO: Undo step: 131
2024-12-02-03:25:33-root-INFO: Undo step: 132
2024-12-02-03:25:33-root-INFO: Undo step: 133
2024-12-02-03:25:33-root-INFO: Undo step: 134
2024-12-02-03:25:33-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-03:25:34-root-INFO: grad norm: 167.277 164.457 30.588
2024-12-02-03:25:35-root-INFO: grad norm: 81.248 78.721 20.105
2024-12-02-03:25:36-root-INFO: grad norm: 61.797 60.108 14.349
2024-12-02-03:25:37-root-INFO: grad norm: 52.918 51.325 12.885
2024-12-02-03:25:38-root-INFO: grad norm: 44.152 43.004 10.002
2024-12-02-03:25:38-root-INFO: Loss Change: 825.950 -> 472.271
2024-12-02-03:25:38-root-INFO: Regularization Change: 0.000 -> 21.900
2024-12-02-03:25:38-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-03:25:38-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-03:25:39-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-03:25:39-root-INFO: grad norm: 47.364 46.332 9.832
2024-12-02-03:25:40-root-INFO: grad norm: 41.720 40.944 8.007
2024-12-02-03:25:41-root-INFO: grad norm: 39.694 38.941 7.697
2024-12-02-03:25:42-root-INFO: grad norm: 37.937 37.307 6.886
2024-12-02-03:25:43-root-INFO: grad norm: 37.313 36.690 6.789
2024-12-02-03:25:44-root-INFO: Loss Change: 473.450 -> 433.928
2024-12-02-03:25:44-root-INFO: Regularization Change: 0.000 -> 3.721
2024-12-02-03:25:44-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-03:25:44-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-03:25:44-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-03:25:44-root-INFO: grad norm: 30.376 29.860 5.575
2024-12-02-03:25:45-root-INFO: grad norm: 29.799 29.245 5.716
2024-12-02-03:25:46-root-INFO: grad norm: 31.522 31.098 5.151
2024-12-02-03:25:47-root-INFO: grad norm: 33.812 33.346 5.592
2024-12-02-03:25:48-root-INFO: grad norm: 38.147 37.742 5.546
2024-12-02-03:25:49-root-INFO: Loss Change: 430.673 -> 416.970
2024-12-02-03:25:49-root-INFO: Regularization Change: 0.000 -> 2.040
2024-12-02-03:25:49-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-03:25:49-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-03:25:49-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-03:25:49-root-INFO: grad norm: 47.929 47.398 7.112
2024-12-02-03:25:51-root-INFO: grad norm: 52.735 52.320 6.597
2024-12-02-03:25:51-root-INFO: Loss too large (416.092->417.819)! Learning rate decreased to 0.01541.
2024-12-02-03:25:52-root-INFO: grad norm: 37.846 37.463 5.370
2024-12-02-03:25:53-root-INFO: grad norm: 26.312 26.010 3.976
2024-12-02-03:25:54-root-INFO: grad norm: 22.162 21.823 3.860
2024-12-02-03:25:54-root-INFO: Loss Change: 418.557 -> 397.988
2024-12-02-03:25:54-root-INFO: Regularization Change: 0.000 -> 1.200
2024-12-02-03:25:54-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-03:25:54-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-03:25:55-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-03:25:55-root-INFO: grad norm: 23.170 22.794 4.155
2024-12-02-03:25:56-root-INFO: grad norm: 26.364 26.070 3.926
2024-12-02-03:25:57-root-INFO: grad norm: 36.058 35.793 4.365
2024-12-02-03:25:57-root-INFO: Loss too large (395.181->398.093)! Learning rate decreased to 0.01593.
2024-12-02-03:25:58-root-INFO: grad norm: 30.054 29.766 4.146
2024-12-02-03:25:59-root-INFO: grad norm: 21.982 21.740 3.252
2024-12-02-03:26:00-root-INFO: Loss Change: 397.582 -> 387.700
2024-12-02-03:26:00-root-INFO: Regularization Change: 0.000 -> 0.952
2024-12-02-03:26:00-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-03:26:00-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-03:26:01-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-03:26:01-root-INFO: grad norm: 20.778 20.415 3.864
2024-12-02-03:26:02-root-INFO: grad norm: 27.206 26.932 3.857
2024-12-02-03:26:02-root-INFO: Loss too large (385.937->387.055)! Learning rate decreased to 0.01647.
2024-12-02-03:26:03-root-INFO: grad norm: 24.389 24.117 3.632
2024-12-02-03:26:04-root-INFO: grad norm: 21.078 20.833 3.202
2024-12-02-03:26:05-root-INFO: grad norm: 20.170 19.918 3.178
2024-12-02-03:26:06-root-INFO: Loss Change: 387.403 -> 378.746
2024-12-02-03:26:06-root-INFO: Regularization Change: 0.000 -> 0.785
2024-12-02-03:26:06-root-INFO: Undo step: 130
2024-12-02-03:26:06-root-INFO: Undo step: 131
2024-12-02-03:26:06-root-INFO: Undo step: 132
2024-12-02-03:26:06-root-INFO: Undo step: 133
2024-12-02-03:26:06-root-INFO: Undo step: 134
2024-12-02-03:26:06-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-03:26:07-root-INFO: grad norm: 155.884 152.408 32.734
2024-12-02-03:26:08-root-INFO: grad norm: 79.567 77.035 19.916
2024-12-02-03:26:09-root-INFO: grad norm: 57.264 55.521 14.018
2024-12-02-03:26:10-root-INFO: grad norm: 46.786 45.430 11.180
2024-12-02-03:26:11-root-INFO: grad norm: 41.104 39.985 9.526
2024-12-02-03:26:11-root-INFO: Loss Change: 864.877 -> 470.119
2024-12-02-03:26:11-root-INFO: Regularization Change: 0.000 -> 25.963
2024-12-02-03:26:11-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-03:26:11-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-03:26:12-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-03:26:12-root-INFO: grad norm: 45.468 44.509 9.290
2024-12-02-03:26:13-root-INFO: grad norm: 42.436 41.693 7.902
2024-12-02-03:26:14-root-INFO: grad norm: 40.358 39.711 7.200
2024-12-02-03:26:15-root-INFO: grad norm: 39.674 39.019 7.178
2024-12-02-03:26:16-root-INFO: grad norm: 41.162 40.645 6.507
2024-12-02-03:26:17-root-INFO: Loss Change: 471.219 -> 436.463
2024-12-02-03:26:17-root-INFO: Regularization Change: 0.000 -> 3.744
2024-12-02-03:26:17-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-03:26:17-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-03:26:17-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-03:26:17-root-INFO: grad norm: 37.835 37.169 7.068
2024-12-02-03:26:18-root-INFO: grad norm: 41.884 41.362 6.591
2024-12-02-03:26:19-root-INFO: grad norm: 41.355 40.758 7.004
2024-12-02-03:26:20-root-INFO: grad norm: 39.422 39.017 5.638
2024-12-02-03:26:21-root-INFO: grad norm: 42.200 41.676 6.631
2024-12-02-03:26:22-root-INFO: Loss Change: 433.679 -> 416.044
2024-12-02-03:26:22-root-INFO: Regularization Change: 0.000 -> 2.312
2024-12-02-03:26:22-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-03:26:22-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-03:26:22-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-03:26:23-root-INFO: grad norm: 56.021 55.512 7.533
2024-12-02-03:26:23-root-INFO: Loss too large (418.070->420.643)! Learning rate decreased to 0.01541.
2024-12-02-03:26:24-root-INFO: grad norm: 40.202 39.728 6.152
2024-12-02-03:26:25-root-INFO: grad norm: 26.014 25.671 4.207
2024-12-02-03:26:26-root-INFO: grad norm: 21.429 20.990 4.316
2024-12-02-03:26:27-root-INFO: grad norm: 19.035 18.679 3.663
2024-12-02-03:26:28-root-INFO: Loss Change: 418.070 -> 395.306
2024-12-02-03:26:28-root-INFO: Regularization Change: 0.000 -> 1.203
2024-12-02-03:26:28-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-03:26:28-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-03:26:28-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-03:26:28-root-INFO: grad norm: 19.401 18.792 4.824
2024-12-02-03:26:29-root-INFO: grad norm: 22.856 22.480 4.133
2024-12-02-03:26:30-root-INFO: grad norm: 24.449 23.975 4.791
2024-12-02-03:26:31-root-INFO: grad norm: 29.055 28.674 4.691
2024-12-02-03:26:32-root-INFO: Loss too large (387.971->389.059)! Learning rate decreased to 0.01593.
2024-12-02-03:26:33-root-INFO: grad norm: 23.906 23.469 4.550
2024-12-02-03:26:33-root-INFO: Loss Change: 394.666 -> 382.736
2024-12-02-03:26:33-root-INFO: Regularization Change: 0.000 -> 1.096
2024-12-02-03:26:33-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-03:26:33-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-03:26:34-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-03:26:34-root-INFO: grad norm: 21.967 21.599 4.006
2024-12-02-03:26:35-root-INFO: grad norm: 25.076 24.703 4.308
2024-12-02-03:26:36-root-INFO: grad norm: 34.717 34.362 4.948
2024-12-02-03:26:36-root-INFO: Loss too large (380.617->383.358)! Learning rate decreased to 0.01647.
2024-12-02-03:26:37-root-INFO: grad norm: 27.192 26.811 4.533
2024-12-02-03:26:38-root-INFO: grad norm: 16.775 16.498 3.041
2024-12-02-03:26:39-root-INFO: Loss Change: 382.817 -> 372.949
2024-12-02-03:26:39-root-INFO: Regularization Change: 0.000 -> 0.899
2024-12-02-03:26:39-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-03:26:39-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-03:26:39-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-03:26:39-root-INFO: grad norm: 17.506 17.032 4.047
2024-12-02-03:26:40-root-INFO: grad norm: 20.364 20.068 3.459
2024-12-02-03:26:41-root-INFO: grad norm: 22.521 22.172 3.950
2024-12-02-03:26:42-root-INFO: grad norm: 28.066 27.752 4.186
2024-12-02-03:26:43-root-INFO: Loss too large (368.043->369.534)! Learning rate decreased to 0.01702.
2024-12-02-03:26:44-root-INFO: grad norm: 22.739 22.411 3.849
2024-12-02-03:26:44-root-INFO: Loss Change: 372.593 -> 363.460
2024-12-02-03:26:44-root-INFO: Regularization Change: 0.000 -> 0.900
2024-12-02-03:26:44-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-03:26:44-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-03:26:45-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-03:26:45-root-INFO: grad norm: 20.958 20.589 3.917
2024-12-02-03:26:46-root-INFO: grad norm: 23.983 23.693 3.722
2024-12-02-03:26:47-root-INFO: grad norm: 33.737 33.438 4.481
2024-12-02-03:26:47-root-INFO: Loss too large (361.997->365.191)! Learning rate decreased to 0.01759.
2024-12-02-03:26:48-root-INFO: grad norm: 27.184 26.880 4.054
2024-12-02-03:26:49-root-INFO: grad norm: 18.386 18.152 2.923
2024-12-02-03:26:50-root-INFO: Loss Change: 363.312 -> 355.309
2024-12-02-03:26:50-root-INFO: Regularization Change: 0.000 -> 0.820
2024-12-02-03:26:50-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-03:26:50-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-03:26:50-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-03:26:51-root-INFO: grad norm: 15.390 15.070 3.125
2024-12-02-03:26:52-root-INFO: grad norm: 20.320 20.068 3.191
2024-12-02-03:26:52-root-INFO: Loss too large (353.083->353.544)! Learning rate decreased to 0.01817.
2024-12-02-03:26:53-root-INFO: grad norm: 18.950 18.683 3.168
2024-12-02-03:26:54-root-INFO: grad norm: 17.309 17.074 2.843
2024-12-02-03:26:55-root-INFO: grad norm: 16.988 16.732 2.937
2024-12-02-03:26:56-root-INFO: Loss Change: 354.400 -> 347.780
2024-12-02-03:26:56-root-INFO: Regularization Change: 0.000 -> 0.686
2024-12-02-03:26:56-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-03:26:56-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-03:26:56-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-03:26:56-root-INFO: grad norm: 20.176 19.867 3.517
2024-12-02-03:26:57-root-INFO: grad norm: 24.395 24.151 3.444
2024-12-02-03:26:58-root-INFO: Loss too large (348.224->348.259)! Learning rate decreased to 0.01877.
2024-12-02-03:26:59-root-INFO: grad norm: 21.880 21.650 3.162
2024-12-02-03:27:00-root-INFO: grad norm: 20.683 20.447 3.118
2024-12-02-03:27:01-root-INFO: grad norm: 19.435 19.215 2.916
2024-12-02-03:27:01-root-INFO: Loss Change: 348.296 -> 342.276
2024-12-02-03:27:01-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-02-03:27:01-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-03:27:01-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-03:27:02-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-03:27:02-root-INFO: grad norm: 19.526 19.194 3.587
2024-12-02-03:27:03-root-INFO: grad norm: 25.965 25.732 3.474
2024-12-02-03:27:03-root-INFO: Loss too large (340.919->342.680)! Learning rate decreased to 0.01939.
2024-12-02-03:27:04-root-INFO: grad norm: 21.479 21.243 3.176
2024-12-02-03:27:05-root-INFO: grad norm: 15.736 15.525 2.571
2024-12-02-03:27:06-root-INFO: grad norm: 15.530 15.309 2.611
2024-12-02-03:27:07-root-INFO: Loss Change: 342.024 -> 334.825
2024-12-02-03:27:07-root-INFO: Regularization Change: 0.000 -> 0.707
2024-12-02-03:27:07-root-INFO: Undo step: 125
2024-12-02-03:27:07-root-INFO: Undo step: 126
2024-12-02-03:27:07-root-INFO: Undo step: 127
2024-12-02-03:27:07-root-INFO: Undo step: 128
2024-12-02-03:27:07-root-INFO: Undo step: 129
2024-12-02-03:27:07-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-03:27:08-root-INFO: grad norm: 182.226 179.665 30.439
2024-12-02-03:27:09-root-INFO: grad norm: 85.698 84.120 16.368
2024-12-02-03:27:10-root-INFO: grad norm: 57.924 56.113 14.368
2024-12-02-03:27:11-root-INFO: grad norm: 46.168 45.082 9.955
2024-12-02-03:27:12-root-INFO: grad norm: 42.429 41.338 9.559
2024-12-02-03:27:12-root-INFO: Loss Change: 759.635 -> 421.475
2024-12-02-03:27:12-root-INFO: Regularization Change: 0.000 -> 26.538
2024-12-02-03:27:12-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-03:27:12-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-03:27:13-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-03:27:13-root-INFO: grad norm: 48.267 47.427 8.965
2024-12-02-03:27:14-root-INFO: grad norm: 48.432 47.708 8.343
2024-12-02-03:27:15-root-INFO: grad norm: 40.489 39.881 6.990
2024-12-02-03:27:16-root-INFO: grad norm: 29.745 29.221 5.556
2024-12-02-03:27:17-root-INFO: grad norm: 27.741 27.235 5.276
2024-12-02-03:27:18-root-INFO: Loss Change: 423.821 -> 381.173
2024-12-02-03:27:18-root-INFO: Regularization Change: 0.000 -> 4.166
2024-12-02-03:27:18-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-03:27:18-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-03:27:18-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-03:27:18-root-INFO: grad norm: 23.140 22.649 4.743
2024-12-02-03:27:19-root-INFO: grad norm: 25.126 24.615 5.045
2024-12-02-03:27:20-root-INFO: grad norm: 34.574 34.155 5.366
2024-12-02-03:27:21-root-INFO: Loss too large (372.651->374.592)! Learning rate decreased to 0.01759.
2024-12-02-03:27:22-root-INFO: grad norm: 26.945 26.490 4.931
2024-12-02-03:27:23-root-INFO: grad norm: 15.795 15.363 3.671
2024-12-02-03:27:23-root-INFO: Loss Change: 378.319 -> 362.515
2024-12-02-03:27:23-root-INFO: Regularization Change: 0.000 -> 1.518
2024-12-02-03:27:23-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-03:27:23-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-03:27:24-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-03:27:24-root-INFO: grad norm: 18.062 17.530 4.353
2024-12-02-03:27:25-root-INFO: grad norm: 21.876 21.517 3.949
2024-12-02-03:27:26-root-INFO: grad norm: 25.052 24.641 4.521
2024-12-02-03:27:27-root-INFO: grad norm: 33.438 33.097 4.767
2024-12-02-03:27:27-root-INFO: Loss too large (357.273->360.076)! Learning rate decreased to 0.01817.
2024-12-02-03:27:28-root-INFO: grad norm: 26.108 25.732 4.415
2024-12-02-03:27:29-root-INFO: Loss Change: 362.341 -> 350.843
2024-12-02-03:27:29-root-INFO: Regularization Change: 0.000 -> 1.230
2024-12-02-03:27:29-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-03:27:29-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-03:27:29-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-03:27:29-root-INFO: grad norm: 15.864 15.428 3.695
2024-12-02-03:27:30-root-INFO: grad norm: 17.314 16.969 3.440
2024-12-02-03:27:31-root-INFO: grad norm: 25.309 25.026 3.777
2024-12-02-03:27:32-root-INFO: Loss too large (347.481->349.079)! Learning rate decreased to 0.01877.
2024-12-02-03:27:33-root-INFO: grad norm: 22.226 21.913 3.720
2024-12-02-03:27:34-root-INFO: grad norm: 17.226 16.942 3.118
2024-12-02-03:27:34-root-INFO: Loss Change: 350.518 -> 342.293
2024-12-02-03:27:34-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-03:27:34-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-03:27:35-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-03:27:35-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-03:27:35-root-INFO: grad norm: 20.317 19.886 4.163
2024-12-02-03:27:36-root-INFO: grad norm: 27.134 26.875 3.743
2024-12-02-03:27:36-root-INFO: Loss too large (341.041->342.841)! Learning rate decreased to 0.01939.
2024-12-02-03:27:37-root-INFO: grad norm: 22.694 22.390 3.706
2024-12-02-03:27:38-root-INFO: grad norm: 16.775 16.519 2.920
2024-12-02-03:27:39-root-INFO: grad norm: 16.168 15.876 3.062
2024-12-02-03:27:40-root-INFO: Loss Change: 342.391 -> 333.784
2024-12-02-03:27:40-root-INFO: Regularization Change: 0.000 -> 0.849
2024-12-02-03:27:40-root-INFO: Undo step: 125
2024-12-02-03:27:40-root-INFO: Undo step: 126
2024-12-02-03:27:40-root-INFO: Undo step: 127
2024-12-02-03:27:40-root-INFO: Undo step: 128
2024-12-02-03:27:40-root-INFO: Undo step: 129
2024-12-02-03:27:40-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-03:27:41-root-INFO: grad norm: 138.800 135.887 28.288
2024-12-02-03:27:42-root-INFO: grad norm: 65.276 63.798 13.812
2024-12-02-03:27:43-root-INFO: grad norm: 49.133 47.884 11.004
2024-12-02-03:27:44-root-INFO: grad norm: 36.330 35.435 8.012
2024-12-02-03:27:45-root-INFO: grad norm: 31.414 30.578 7.199
2024-12-02-03:27:45-root-INFO: Loss Change: 743.203 -> 413.038
2024-12-02-03:27:45-root-INFO: Regularization Change: 0.000 -> 24.160
2024-12-02-03:27:45-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-03:27:45-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-03:27:46-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-03:27:46-root-INFO: grad norm: 38.511 37.938 6.618
2024-12-02-03:27:47-root-INFO: grad norm: 37.208 36.635 6.507
2024-12-02-03:27:48-root-INFO: grad norm: 36.142 35.761 5.236
2024-12-02-03:27:49-root-INFO: grad norm: 34.767 34.283 5.781
2024-12-02-03:27:50-root-INFO: grad norm: 32.460 32.131 4.606
2024-12-02-03:27:51-root-INFO: Loss Change: 413.693 -> 381.776
2024-12-02-03:27:51-root-INFO: Regularization Change: 0.000 -> 3.964
2024-12-02-03:27:51-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-03:27:51-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-03:27:51-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-03:27:51-root-INFO: grad norm: 29.104 28.696 4.856
2024-12-02-03:27:52-root-INFO: grad norm: 29.159 28.851 4.229
2024-12-02-03:27:53-root-INFO: grad norm: 27.970 27.600 4.536
2024-12-02-03:27:54-root-INFO: grad norm: 25.384 25.100 3.786
2024-12-02-03:27:55-root-INFO: grad norm: 26.027 25.677 4.253
2024-12-02-03:27:56-root-INFO: Loss Change: 379.404 -> 360.355
2024-12-02-03:27:56-root-INFO: Regularization Change: 0.000 -> 2.205
2024-12-02-03:27:56-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-03:27:56-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-03:27:56-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-03:27:57-root-INFO: grad norm: 33.975 33.657 4.637
2024-12-02-03:27:57-root-INFO: Loss too large (361.418->361.423)! Learning rate decreased to 0.01817.
2024-12-02-03:27:58-root-INFO: grad norm: 25.122 24.810 3.945
2024-12-02-03:27:59-root-INFO: grad norm: 16.639 16.343 3.125
2024-12-02-03:28:00-root-INFO: grad norm: 14.673 14.331 3.148
2024-12-02-03:28:01-root-INFO: grad norm: 13.587 13.283 2.862
2024-12-02-03:28:02-root-INFO: Loss Change: 361.418 -> 347.207
2024-12-02-03:28:02-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-02-03:28:02-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-03:28:02-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-03:28:02-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-03:28:02-root-INFO: grad norm: 15.769 15.403 3.376
2024-12-02-03:28:03-root-INFO: grad norm: 19.046 18.794 3.089
2024-12-02-03:28:04-root-INFO: grad norm: 20.855 20.607 3.206
2024-12-02-03:28:05-root-INFO: grad norm: 24.642 24.421 3.295
2024-12-02-03:28:06-root-INFO: Loss too large (342.020->342.601)! Learning rate decreased to 0.01877.
2024-12-02-03:28:07-root-INFO: grad norm: 20.008 19.776 3.041
2024-12-02-03:28:07-root-INFO: Loss Change: 347.141 -> 337.496
2024-12-02-03:28:07-root-INFO: Regularization Change: 0.000 -> 1.067
2024-12-02-03:28:07-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-03:28:07-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-03:28:08-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-03:28:08-root-INFO: grad norm: 16.941 16.586 3.452
2024-12-02-03:28:09-root-INFO: grad norm: 18.394 18.144 3.020
2024-12-02-03:28:10-root-INFO: grad norm: 24.056 23.850 3.143
2024-12-02-03:28:10-root-INFO: Loss too large (334.370->335.150)! Learning rate decreased to 0.01939.
2024-12-02-03:28:11-root-INFO: grad norm: 20.153 19.925 3.024
2024-12-02-03:28:12-root-INFO: grad norm: 15.449 15.233 2.575
2024-12-02-03:28:13-root-INFO: Loss Change: 337.107 -> 329.067
2024-12-02-03:28:13-root-INFO: Regularization Change: 0.000 -> 0.906
2024-12-02-03:28:13-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-03:28:13-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-03:28:13-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-03:28:14-root-INFO: grad norm: 19.306 18.880 4.032
2024-12-02-03:28:15-root-INFO: grad norm: 22.181 21.961 3.120
2024-12-02-03:28:15-root-INFO: Loss too large (327.634->327.861)! Learning rate decreased to 0.02013.
2024-12-02-03:28:16-root-INFO: grad norm: 18.711 18.494 2.841
2024-12-02-03:28:17-root-INFO: grad norm: 15.437 15.222 2.570
2024-12-02-03:28:18-root-INFO: grad norm: 14.453 14.243 2.454
2024-12-02-03:28:19-root-INFO: Loss Change: 329.554 -> 321.152
2024-12-02-03:28:19-root-INFO: Regularization Change: 0.000 -> 0.838
2024-12-02-03:28:19-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-03:28:19-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-03:28:19-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-03:28:19-root-INFO: grad norm: 15.327 15.036 2.970
2024-12-02-03:28:20-root-INFO: grad norm: 17.742 17.536 2.693
2024-12-02-03:28:21-root-INFO: grad norm: 23.828 23.636 3.012
2024-12-02-03:28:21-root-INFO: Loss too large (319.591->320.867)! Learning rate decreased to 0.02078.
2024-12-02-03:28:22-root-INFO: grad norm: 19.850 19.652 2.791
2024-12-02-03:28:23-root-INFO: grad norm: 14.896 14.697 2.427
2024-12-02-03:28:24-root-INFO: Loss Change: 321.242 -> 314.814
2024-12-02-03:28:24-root-INFO: Regularization Change: 0.000 -> 0.812
2024-12-02-03:28:24-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-03:28:24-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-03:28:24-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-03:28:25-root-INFO: grad norm: 14.902 14.624 2.862
2024-12-02-03:28:26-root-INFO: grad norm: 18.343 18.151 2.652
2024-12-02-03:28:26-root-INFO: Loss too large (313.608->313.793)! Learning rate decreased to 0.02145.
2024-12-02-03:28:27-root-INFO: grad norm: 16.155 15.969 2.446
2024-12-02-03:28:28-root-INFO: grad norm: 13.876 13.681 2.320
2024-12-02-03:28:29-root-INFO: grad norm: 13.097 12.910 2.206
2024-12-02-03:28:30-root-INFO: Loss Change: 314.837 -> 308.502
2024-12-02-03:28:30-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-02-03:28:30-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-03:28:30-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-03:28:30-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-03:28:30-root-INFO: grad norm: 14.828 14.558 2.815
2024-12-02-03:28:31-root-INFO: grad norm: 17.230 17.038 2.563
2024-12-02-03:28:32-root-INFO: grad norm: 23.068 22.890 2.858
2024-12-02-03:28:33-root-INFO: Loss too large (307.213->308.596)! Learning rate decreased to 0.02214.
2024-12-02-03:28:34-root-INFO: grad norm: 19.409 19.223 2.683
2024-12-02-03:28:35-root-INFO: grad norm: 15.183 15.004 2.328
2024-12-02-03:28:35-root-INFO: Loss Change: 308.408 -> 302.854
2024-12-02-03:28:35-root-INFO: Regularization Change: 0.000 -> 0.773
2024-12-02-03:28:35-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-03:28:35-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-03:28:36-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-03:28:36-root-INFO: grad norm: 15.167 14.920 2.724
2024-12-02-03:28:37-root-INFO: grad norm: 18.961 18.788 2.558
2024-12-02-03:28:37-root-INFO: Loss too large (301.995->302.549)! Learning rate decreased to 0.02285.
2024-12-02-03:28:38-root-INFO: grad norm: 16.457 16.285 2.373
2024-12-02-03:28:39-root-INFO: grad norm: 13.857 13.680 2.205
2024-12-02-03:28:40-root-INFO: grad norm: 13.061 12.889 2.115
2024-12-02-03:28:41-root-INFO: Loss Change: 302.999 -> 297.131
2024-12-02-03:28:41-root-INFO: Regularization Change: 0.000 -> 0.702
2024-12-02-03:28:41-root-INFO: Undo step: 120
2024-12-02-03:28:41-root-INFO: Undo step: 121
2024-12-02-03:28:41-root-INFO: Undo step: 122
2024-12-02-03:28:41-root-INFO: Undo step: 123
2024-12-02-03:28:41-root-INFO: Undo step: 124
2024-12-02-03:28:41-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-03:28:42-root-INFO: grad norm: 117.639 115.583 21.901
2024-12-02-03:28:43-root-INFO: grad norm: 60.377 59.009 12.778
2024-12-02-03:28:44-root-INFO: grad norm: 43.661 42.664 9.279
2024-12-02-03:28:45-root-INFO: grad norm: 33.432 32.595 7.433
2024-12-02-03:28:46-root-INFO: grad norm: 27.593 26.907 6.115
2024-12-02-03:28:46-root-INFO: Loss Change: 662.737 -> 370.874
2024-12-02-03:28:46-root-INFO: Regularization Change: 0.000 -> 25.027
2024-12-02-03:28:46-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-03:28:46-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-03:28:46-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-03:28:47-root-INFO: grad norm: 25.862 25.338 5.181
2024-12-02-03:28:48-root-INFO: grad norm: 22.110 21.578 4.820
2024-12-02-03:28:49-root-INFO: grad norm: 20.043 19.665 3.877
2024-12-02-03:28:50-root-INFO: grad norm: 18.783 18.357 3.979
2024-12-02-03:28:51-root-INFO: grad norm: 18.060 17.753 3.312
2024-12-02-03:28:52-root-INFO: Loss Change: 369.709 -> 337.353
2024-12-02-03:28:52-root-INFO: Regularization Change: 0.000 -> 4.132
2024-12-02-03:28:52-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-03:28:52-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-03:28:52-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-03:28:52-root-INFO: grad norm: 14.776 14.440 3.130
2024-12-02-03:28:53-root-INFO: grad norm: 14.061 13.755 2.920
2024-12-02-03:28:54-root-INFO: grad norm: 14.245 13.941 2.928
2024-12-02-03:28:55-root-INFO: grad norm: 15.151 14.901 2.745
2024-12-02-03:28:56-root-INFO: grad norm: 16.540 16.277 2.936
2024-12-02-03:28:57-root-INFO: Loss Change: 336.081 -> 322.214
2024-12-02-03:28:57-root-INFO: Regularization Change: 0.000 -> 2.041
2024-12-02-03:28:57-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-03:28:57-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-03:28:57-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-03:28:58-root-INFO: grad norm: 24.904 24.596 3.905
2024-12-02-03:28:59-root-INFO: grad norm: 26.634 26.387 3.617
2024-12-02-03:29:00-root-INFO: grad norm: 28.574 28.344 3.613
2024-12-02-03:29:01-root-INFO: grad norm: 29.333 29.080 3.846
2024-12-02-03:29:02-root-INFO: grad norm: 29.452 29.232 3.589
2024-12-02-03:29:02-root-INFO: Loss Change: 323.369 -> 316.450
2024-12-02-03:29:02-root-INFO: Regularization Change: 0.000 -> 1.812
2024-12-02-03:29:02-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-03:29:02-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-03:29:03-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-03:29:03-root-INFO: grad norm: 26.561 26.386 3.047
2024-12-02-03:29:04-root-INFO: grad norm: 26.712 26.510 3.272
2024-12-02-03:29:05-root-INFO: grad norm: 27.529 27.320 3.384
2024-12-02-03:29:06-root-INFO: grad norm: 28.144 27.949 3.309
2024-12-02-03:29:07-root-INFO: grad norm: 29.407 29.193 3.538
2024-12-02-03:29:07-root-INFO: Loss Change: 314.570 -> 307.966
2024-12-02-03:29:07-root-INFO: Regularization Change: 0.000 -> 1.646
2024-12-02-03:29:08-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-03:29:08-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-03:29:08-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-03:29:08-root-INFO: grad norm: 35.663 35.347 4.736
2024-12-02-03:29:09-root-INFO: grad norm: 35.485 35.250 4.072
2024-12-02-03:29:10-root-INFO: grad norm: 34.262 34.041 3.883
2024-12-02-03:29:11-root-INFO: grad norm: 33.395 33.169 3.882
2024-12-02-03:29:12-root-INFO: grad norm: 32.663 32.460 3.636
2024-12-02-03:29:13-root-INFO: Loss Change: 310.447 -> 302.139
2024-12-02-03:29:13-root-INFO: Regularization Change: 0.000 -> 1.840
2024-12-02-03:29:13-root-INFO: Undo step: 120
2024-12-02-03:29:13-root-INFO: Undo step: 121
2024-12-02-03:29:13-root-INFO: Undo step: 122
2024-12-02-03:29:13-root-INFO: Undo step: 123
2024-12-02-03:29:13-root-INFO: Undo step: 124
2024-12-02-03:29:13-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-03:29:14-root-INFO: grad norm: 129.350 127.835 19.740
2024-12-02-03:29:15-root-INFO: grad norm: 63.259 62.062 12.244
2024-12-02-03:29:16-root-INFO: grad norm: 44.641 43.641 9.394
2024-12-02-03:29:17-root-INFO: grad norm: 38.684 37.851 7.984
2024-12-02-03:29:18-root-INFO: grad norm: 38.567 37.923 7.020
2024-12-02-03:29:18-root-INFO: Loss Change: 705.479 -> 385.043
2024-12-02-03:29:18-root-INFO: Regularization Change: 0.000 -> 27.883
2024-12-02-03:29:18-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-03:29:18-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-03:29:19-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-03:29:19-root-INFO: grad norm: 41.934 41.458 6.297
2024-12-02-03:29:20-root-INFO: grad norm: 42.744 42.333 5.913
2024-12-02-03:29:21-root-INFO: grad norm: 40.696 40.327 5.464
2024-12-02-03:29:22-root-INFO: grad norm: 43.391 43.083 5.163
2024-12-02-03:29:23-root-INFO: grad norm: 44.081 43.766 5.256
2024-12-02-03:29:24-root-INFO: Loss Change: 383.194 -> 353.144
2024-12-02-03:29:24-root-INFO: Regularization Change: 0.000 -> 5.152
2024-12-02-03:29:24-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-03:29:24-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-03:29:24-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-03:29:24-root-INFO: grad norm: 44.586 44.251 5.455
2024-12-02-03:29:25-root-INFO: grad norm: 40.005 39.733 4.656
2024-12-02-03:29:26-root-INFO: grad norm: 35.346 35.095 4.201
2024-12-02-03:29:27-root-INFO: grad norm: 34.158 33.908 4.123
2024-12-02-03:29:28-root-INFO: grad norm: 33.153 32.925 3.883
2024-12-02-03:29:29-root-INFO: Loss Change: 354.370 -> 331.217
2024-12-02-03:29:29-root-INFO: Regularization Change: 0.000 -> 3.253
2024-12-02-03:29:29-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-03:29:29-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-03:29:29-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-03:29:30-root-INFO: grad norm: 32.633 32.441 3.534
2024-12-02-03:29:31-root-INFO: grad norm: 32.189 31.985 3.619
2024-12-02-03:29:32-root-INFO: grad norm: 31.230 31.039 3.445
2024-12-02-03:29:33-root-INFO: grad norm: 30.649 30.463 3.374
2024-12-02-03:29:34-root-INFO: grad norm: 30.067 29.882 3.335
2024-12-02-03:29:34-root-INFO: Loss Change: 330.554 -> 317.860
2024-12-02-03:29:34-root-INFO: Regularization Change: 0.000 -> 2.213
2024-12-02-03:29:34-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-03:29:34-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-03:29:35-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-03:29:35-root-INFO: grad norm: 32.481 32.208 4.203
2024-12-02-03:29:36-root-INFO: grad norm: 31.112 30.930 3.359
2024-12-02-03:29:37-root-INFO: grad norm: 29.074 28.890 3.270
2024-12-02-03:29:38-root-INFO: grad norm: 28.043 27.867 3.142
2024-12-02-03:29:39-root-INFO: grad norm: 27.213 27.043 3.041
2024-12-02-03:29:40-root-INFO: Loss Change: 318.233 -> 307.251
2024-12-02-03:29:40-root-INFO: Regularization Change: 0.000 -> 1.894
2024-12-02-03:29:40-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-03:29:40-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-03:29:40-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-03:29:40-root-INFO: grad norm: 26.382 26.233 2.801
2024-12-02-03:29:41-root-INFO: grad norm: 25.902 25.747 2.832
2024-12-02-03:29:42-root-INFO: grad norm: 25.863 25.717 2.742
2024-12-02-03:29:43-root-INFO: grad norm: 25.653 25.507 2.729
2024-12-02-03:29:44-root-INFO: grad norm: 25.480 25.335 2.714
2024-12-02-03:29:45-root-INFO: Loss Change: 306.725 -> 298.755
2024-12-02-03:29:45-root-INFO: Regularization Change: 0.000 -> 1.616
2024-12-02-03:29:45-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-03:29:45-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-03:29:45-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-03:29:45-root-INFO: grad norm: 30.613 30.321 4.218
2024-12-02-03:29:46-root-INFO: grad norm: 29.324 29.180 2.903
2024-12-02-03:29:47-root-INFO: grad norm: 27.162 26.995 3.004
2024-12-02-03:29:48-root-INFO: grad norm: 26.029 25.886 2.721
2024-12-02-03:29:49-root-INFO: grad norm: 25.240 25.087 2.769
2024-12-02-03:29:50-root-INFO: Loss Change: 300.012 -> 291.189
2024-12-02-03:29:50-root-INFO: Regularization Change: 0.000 -> 1.638
2024-12-02-03:29:50-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-03:29:50-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-03:29:50-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-03:29:51-root-INFO: grad norm: 24.722 24.595 2.504
2024-12-02-03:29:52-root-INFO: grad norm: 23.905 23.766 2.572
2024-12-02-03:29:53-root-INFO: grad norm: 23.460 23.328 2.480
2024-12-02-03:29:54-root-INFO: grad norm: 23.123 22.992 2.457
2024-12-02-03:29:55-root-INFO: grad norm: 22.947 22.817 2.441
2024-12-02-03:29:55-root-INFO: Loss Change: 291.455 -> 284.641
2024-12-02-03:29:55-root-INFO: Regularization Change: 0.000 -> 1.423
2024-12-02-03:29:55-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-03:29:55-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-03:29:56-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-03:29:56-root-INFO: grad norm: 28.639 28.367 3.931
2024-12-02-03:29:57-root-INFO: grad norm: 27.608 27.472 2.737
2024-12-02-03:29:58-root-INFO: grad norm: 26.717 26.557 2.919
2024-12-02-03:29:59-root-INFO: grad norm: 26.164 26.030 2.651
2024-12-02-03:30:00-root-INFO: grad norm: 25.695 25.547 2.748
2024-12-02-03:30:01-root-INFO: Loss Change: 286.346 -> 279.778
2024-12-02-03:30:01-root-INFO: Regularization Change: 0.000 -> 1.576
2024-12-02-03:30:01-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-03:30:01-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-03:30:01-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-03:30:01-root-INFO: grad norm: 23.420 23.317 2.202
2024-12-02-03:30:02-root-INFO: grad norm: 23.261 23.117 2.586
2024-12-02-03:30:03-root-INFO: grad norm: 23.436 23.322 2.313
2024-12-02-03:30:04-root-INFO: grad norm: 23.569 23.434 2.526
2024-12-02-03:30:05-root-INFO: grad norm: 23.726 23.607 2.377
2024-12-02-03:30:06-root-INFO: Loss Change: 278.564 -> 273.306
2024-12-02-03:30:06-root-INFO: Regularization Change: 0.000 -> 1.398
2024-12-02-03:30:06-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-03:30:06-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-03:30:06-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-03:30:07-root-INFO: grad norm: 28.506 28.285 3.539
2024-12-02-03:30:08-root-INFO: grad norm: 27.575 27.445 2.671
2024-12-02-03:30:09-root-INFO: grad norm: 26.501 26.351 2.814
2024-12-02-03:30:10-root-INFO: grad norm: 25.846 25.719 2.561
2024-12-02-03:30:11-root-INFO: grad norm: 25.235 25.096 2.644
2024-12-02-03:30:11-root-INFO: Loss Change: 275.204 -> 269.300
2024-12-02-03:30:11-root-INFO: Regularization Change: 0.000 -> 1.586
2024-12-02-03:30:11-root-INFO: Undo step: 115
2024-12-02-03:30:12-root-INFO: Undo step: 116
2024-12-02-03:30:12-root-INFO: Undo step: 117
2024-12-02-03:30:12-root-INFO: Undo step: 118
2024-12-02-03:30:12-root-INFO: Undo step: 119
2024-12-02-03:30:12-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-03:30:12-root-INFO: grad norm: 112.822 111.052 19.906
2024-12-02-03:30:13-root-INFO: grad norm: 61.095 60.143 10.741
2024-12-02-03:30:14-root-INFO: grad norm: 41.595 40.613 8.985
2024-12-02-03:30:15-root-INFO: grad norm: 35.631 34.976 6.804
2024-12-02-03:30:16-root-INFO: grad norm: 33.126 32.438 6.714
2024-12-02-03:30:17-root-INFO: Loss Change: 638.384 -> 332.262
2024-12-02-03:30:17-root-INFO: Regularization Change: 0.000 -> 32.661
2024-12-02-03:30:17-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-03:30:17-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-03:30:17-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-03:30:17-root-INFO: grad norm: 32.860 32.244 6.333
2024-12-02-03:30:18-root-INFO: grad norm: 29.282 28.791 5.341
2024-12-02-03:30:19-root-INFO: grad norm: 27.135 26.760 4.499
2024-12-02-03:30:20-root-INFO: grad norm: 25.534 25.191 4.172
2024-12-02-03:30:21-root-INFO: grad norm: 25.159 24.886 3.695
2024-12-02-03:30:22-root-INFO: Loss Change: 332.649 -> 302.816
2024-12-02-03:30:22-root-INFO: Regularization Change: 0.000 -> 4.468
2024-12-02-03:30:22-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-03:30:22-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-03:30:22-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-03:30:23-root-INFO: grad norm: 25.876 25.617 3.656
2024-12-02-03:30:24-root-INFO: grad norm: 27.216 26.997 3.448
2024-12-02-03:30:25-root-INFO: grad norm: 28.683 28.471 3.480
2024-12-02-03:30:26-root-INFO: grad norm: 30.423 30.222 3.489
2024-12-02-03:30:27-root-INFO: grad norm: 31.661 31.462 3.546
2024-12-02-03:30:27-root-INFO: Loss Change: 302.928 -> 294.029
2024-12-02-03:30:27-root-INFO: Regularization Change: 0.000 -> 2.658
2024-12-02-03:30:27-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-03:30:27-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-03:30:28-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-03:30:28-root-INFO: grad norm: 30.438 30.229 3.553
2024-12-02-03:30:29-root-INFO: grad norm: 30.185 30.000 3.336
2024-12-02-03:30:30-root-INFO: grad norm: 30.742 30.571 3.236
2024-12-02-03:30:31-root-INFO: grad norm: 31.054 30.871 3.365
2024-12-02-03:30:32-root-INFO: grad norm: 31.297 31.119 3.334
2024-12-02-03:30:33-root-INFO: Loss Change: 292.791 -> 283.948
2024-12-02-03:30:33-root-INFO: Regularization Change: 0.000 -> 2.356
2024-12-02-03:30:33-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-03:30:33-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-03:30:33-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-03:30:33-root-INFO: grad norm: 35.017 34.737 4.422
2024-12-02-03:30:34-root-INFO: grad norm: 34.792 34.584 3.795
2024-12-02-03:30:35-root-INFO: grad norm: 34.349 34.125 3.915
2024-12-02-03:30:36-root-INFO: grad norm: 33.533 33.327 3.706
2024-12-02-03:30:37-root-INFO: grad norm: 32.908 32.690 3.779
2024-12-02-03:30:38-root-INFO: Loss Change: 285.550 -> 277.157
2024-12-02-03:30:38-root-INFO: Regularization Change: 0.000 -> 2.341
2024-12-02-03:30:38-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-03:30:38-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-03:30:38-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-03:30:39-root-INFO: grad norm: 29.132 28.986 2.915
2024-12-02-03:30:40-root-INFO: grad norm: 28.574 28.375 3.368
2024-12-02-03:30:41-root-INFO: grad norm: 28.300 28.134 3.065
2024-12-02-03:30:42-root-INFO: grad norm: 28.162 27.969 3.289
2024-12-02-03:30:43-root-INFO: grad norm: 28.103 27.930 3.116
2024-12-02-03:30:43-root-INFO: Loss Change: 275.499 -> 268.597
2024-12-02-03:30:43-root-INFO: Regularization Change: 0.000 -> 1.908
2024-12-02-03:30:43-root-INFO: Undo step: 115
2024-12-02-03:30:43-root-INFO: Undo step: 116
2024-12-02-03:30:43-root-INFO: Undo step: 117
2024-12-02-03:30:43-root-INFO: Undo step: 118
2024-12-02-03:30:43-root-INFO: Undo step: 119
2024-12-02-03:30:44-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-03:30:44-root-INFO: grad norm: 104.256 102.054 21.316
2024-12-02-03:30:45-root-INFO: grad norm: 51.186 50.032 10.807
2024-12-02-03:30:46-root-INFO: grad norm: 33.816 32.874 7.927
2024-12-02-03:30:47-root-INFO: grad norm: 27.474 26.787 6.105
2024-12-02-03:30:48-root-INFO: grad norm: 24.231 23.602 5.486
2024-12-02-03:30:48-root-INFO: Loss Change: 590.309 -> 329.969
2024-12-02-03:30:48-root-INFO: Regularization Change: 0.000 -> 27.066
2024-12-02-03:30:48-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-03:30:48-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-03:30:49-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-03:30:49-root-INFO: grad norm: 28.142 27.634 5.325
2024-12-02-03:30:50-root-INFO: grad norm: 28.108 27.699 4.775
2024-12-02-03:30:51-root-INFO: grad norm: 29.365 29.054 4.263
2024-12-02-03:30:52-root-INFO: grad norm: 31.462 31.139 4.493
2024-12-02-03:30:53-root-INFO: grad norm: 33.212 32.956 4.118
2024-12-02-03:30:54-root-INFO: Loss Change: 330.319 -> 305.335
2024-12-02-03:30:54-root-INFO: Regularization Change: 0.000 -> 4.905
2024-12-02-03:30:54-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-03:30:54-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-03:30:54-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-03:30:54-root-INFO: grad norm: 33.857 33.602 4.146
2024-12-02-03:30:55-root-INFO: grad norm: 34.332 34.101 3.979
2024-12-02-03:30:56-root-INFO: grad norm: 33.713 33.473 4.017
2024-12-02-03:30:57-root-INFO: grad norm: 32.673 32.462 3.714
2024-12-02-03:30:58-root-INFO: grad norm: 31.212 30.989 3.727
2024-12-02-03:30:59-root-INFO: Loss Change: 304.907 -> 289.637
2024-12-02-03:30:59-root-INFO: Regularization Change: 0.000 -> 3.120
2024-12-02-03:30:59-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-03:30:59-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-03:30:59-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-03:31:00-root-INFO: grad norm: 34.523 34.206 4.665
2024-12-02-03:31:01-root-INFO: grad norm: 32.805 32.577 3.861
2024-12-02-03:31:02-root-INFO: grad norm: 31.122 30.910 3.625
2024-12-02-03:31:03-root-INFO: grad norm: 29.426 29.214 3.525
2024-12-02-03:31:04-root-INFO: grad norm: 28.169 27.974 3.310
2024-12-02-03:31:04-root-INFO: Loss Change: 291.479 -> 277.802
2024-12-02-03:31:04-root-INFO: Regularization Change: 0.000 -> 2.484
2024-12-02-03:31:04-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-03:31:04-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-03:31:05-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-03:31:05-root-INFO: grad norm: 25.470 25.297 2.960
2024-12-02-03:31:06-root-INFO: grad norm: 24.619 24.443 2.940
2024-12-02-03:31:07-root-INFO: grad norm: 24.089 23.921 2.836
2024-12-02-03:31:08-root-INFO: grad norm: 23.901 23.732 2.839
2024-12-02-03:31:09-root-INFO: grad norm: 23.887 23.719 2.826
2024-12-02-03:31:10-root-INFO: Loss Change: 276.422 -> 268.292
2024-12-02-03:31:10-root-INFO: Regularization Change: 0.000 -> 1.830
2024-12-02-03:31:10-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-03:31:10-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-03:31:10-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-03:31:10-root-INFO: grad norm: 28.821 28.556 3.899
2024-12-02-03:31:11-root-INFO: grad norm: 28.812 28.623 3.300
2024-12-02-03:31:12-root-INFO: grad norm: 29.115 28.922 3.349
2024-12-02-03:31:13-root-INFO: grad norm: 29.252 29.066 3.300
2024-12-02-03:31:14-root-INFO: grad norm: 29.697 29.509 3.331
2024-12-02-03:31:15-root-INFO: Loss Change: 270.163 -> 264.836
2024-12-02-03:31:15-root-INFO: Regularization Change: 0.000 -> 1.970
2024-12-02-03:31:15-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-03:31:15-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-03:31:15-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-03:31:16-root-INFO: grad norm: 26.728 26.584 2.774
2024-12-02-03:31:17-root-INFO: grad norm: 26.899 26.722 3.084
2024-12-02-03:31:18-root-INFO: grad norm: 27.057 26.893 2.981
2024-12-02-03:31:19-root-INFO: grad norm: 27.441 27.264 3.112
2024-12-02-03:31:20-root-INFO: grad norm: 27.598 27.426 3.070
2024-12-02-03:31:20-root-INFO: Loss Change: 263.249 -> 257.547
2024-12-02-03:31:20-root-INFO: Regularization Change: 0.000 -> 1.787
2024-12-02-03:31:20-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-03:31:20-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-03:31:21-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-03:31:21-root-INFO: grad norm: 32.360 32.099 4.100
2024-12-02-03:31:22-root-INFO: grad norm: 31.554 31.357 3.516
2024-12-02-03:31:23-root-INFO: grad norm: 30.328 30.133 3.430
2024-12-02-03:31:24-root-INFO: grad norm: 29.595 29.412 3.290
2024-12-02-03:31:25-root-INFO: grad norm: 28.765 28.584 3.217
2024-12-02-03:31:25-root-INFO: Loss Change: 259.551 -> 253.471
2024-12-02-03:31:25-root-INFO: Regularization Change: 0.000 -> 2.005
2024-12-02-03:31:25-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-03:31:25-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-03:31:26-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-03:31:26-root-INFO: grad norm: 24.773 24.640 2.557
2024-12-02-03:31:27-root-INFO: grad norm: 24.623 24.461 2.821
2024-12-02-03:31:28-root-INFO: grad norm: 24.832 24.682 2.726
2024-12-02-03:31:29-root-INFO: grad norm: 25.299 25.139 2.838
2024-12-02-03:31:30-root-INFO: grad norm: 25.663 25.506 2.834
2024-12-02-03:31:31-root-INFO: Loss Change: 251.282 -> 246.624
2024-12-02-03:31:31-root-INFO: Regularization Change: 0.000 -> 1.666
2024-12-02-03:31:31-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-03:31:31-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-03:31:31-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-03:31:31-root-INFO: grad norm: 29.907 29.695 3.551
2024-12-02-03:31:32-root-INFO: grad norm: 29.270 29.091 3.232
2024-12-02-03:31:33-root-INFO: grad norm: 28.362 28.193 3.089
2024-12-02-03:31:34-root-INFO: grad norm: 27.835 27.666 3.064
2024-12-02-03:31:35-root-INFO: grad norm: 27.254 27.095 2.939
2024-12-02-03:31:36-root-INFO: Loss Change: 248.493 -> 243.438
2024-12-02-03:31:36-root-INFO: Regularization Change: 0.000 -> 1.915
2024-12-02-03:31:36-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-03:31:36-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-03:31:36-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-03:31:37-root-INFO: grad norm: 24.556 24.436 2.418
2024-12-02-03:31:38-root-INFO: grad norm: 24.701 24.553 2.699
2024-12-02-03:31:39-root-INFO: grad norm: 24.871 24.731 2.634
2024-12-02-03:31:40-root-INFO: grad norm: 25.162 25.018 2.693
2024-12-02-03:31:41-root-INFO: grad norm: 25.347 25.201 2.717
2024-12-02-03:31:41-root-INFO: Loss Change: 242.110 -> 238.137
2024-12-02-03:31:41-root-INFO: Regularization Change: 0.000 -> 1.680
2024-12-02-03:31:41-root-INFO: Undo step: 110
2024-12-02-03:31:41-root-INFO: Undo step: 111
2024-12-02-03:31:41-root-INFO: Undo step: 112
2024-12-02-03:31:41-root-INFO: Undo step: 113
2024-12-02-03:31:41-root-INFO: Undo step: 114
2024-12-02-03:31:42-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-03:31:42-root-INFO: grad norm: 83.978 83.153 11.739
2024-12-02-03:31:43-root-INFO: grad norm: 46.996 46.353 7.744
2024-12-02-03:31:44-root-INFO: grad norm: 40.625 40.091 6.570
2024-12-02-03:31:45-root-INFO: grad norm: 34.781 34.395 5.170
2024-12-02-03:31:46-root-INFO: grad norm: 29.885 29.535 4.563
2024-12-02-03:31:47-root-INFO: Loss Change: 487.240 -> 287.166
2024-12-02-03:31:47-root-INFO: Regularization Change: 0.000 -> 25.183
2024-12-02-03:31:47-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-03:31:47-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-03:31:47-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-03:31:47-root-INFO: grad norm: 27.753 27.469 3.960
2024-12-02-03:31:48-root-INFO: grad norm: 24.583 24.327 3.538
2024-12-02-03:31:49-root-INFO: grad norm: 22.770 22.533 3.282
2024-12-02-03:31:50-root-INFO: grad norm: 21.669 21.453 3.049
2024-12-02-03:31:51-root-INFO: grad norm: 20.949 20.740 2.952
2024-12-02-03:31:52-root-INFO: Loss Change: 286.835 -> 264.926
2024-12-02-03:31:52-root-INFO: Regularization Change: 0.000 -> 3.953
2024-12-02-03:31:52-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-03:31:52-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-03:31:52-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-03:31:53-root-INFO: grad norm: 22.295 22.075 3.124
2024-12-02-03:31:54-root-INFO: grad norm: 21.582 21.376 2.969
2024-12-02-03:31:55-root-INFO: grad norm: 21.261 21.081 2.755
2024-12-02-03:31:56-root-INFO: grad norm: 21.039 20.850 2.816
2024-12-02-03:31:57-root-INFO: grad norm: 20.935 20.764 2.670
2024-12-02-03:31:57-root-INFO: Loss Change: 264.965 -> 254.612
2024-12-02-03:31:57-root-INFO: Regularization Change: 0.000 -> 2.282
2024-12-02-03:31:57-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-03:31:57-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-03:31:58-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-03:31:58-root-INFO: grad norm: 18.870 18.704 2.492
2024-12-02-03:31:59-root-INFO: grad norm: 18.016 17.868 2.300
2024-12-02-03:32:00-root-INFO: grad norm: 17.723 17.568 2.345
2024-12-02-03:32:01-root-INFO: grad norm: 17.788 17.639 2.299
2024-12-02-03:32:02-root-INFO: grad norm: 17.931 17.774 2.370
2024-12-02-03:32:03-root-INFO: Loss Change: 253.318 -> 246.016
2024-12-02-03:32:03-root-INFO: Regularization Change: 0.000 -> 1.711
2024-12-02-03:32:03-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-03:32:03-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-03:32:03-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-03:32:03-root-INFO: grad norm: 20.858 20.652 2.924
2024-12-02-03:32:04-root-INFO: grad norm: 20.760 20.579 2.733
2024-12-02-03:32:05-root-INFO: grad norm: 20.884 20.716 2.643
2024-12-02-03:32:06-root-INFO: grad norm: 20.992 20.813 2.733
2024-12-02-03:32:07-root-INFO: grad norm: 21.173 21.008 2.641
2024-12-02-03:32:08-root-INFO: Loss Change: 246.650 -> 241.270
2024-12-02-03:32:08-root-INFO: Regularization Change: 0.000 -> 1.660
2024-12-02-03:32:08-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-03:32:08-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-03:32:08-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-03:32:09-root-INFO: grad norm: 19.188 19.057 2.236
2024-12-02-03:32:10-root-INFO: grad norm: 19.144 18.991 2.413
2024-12-02-03:32:11-root-INFO: grad norm: 19.269 19.113 2.452
2024-12-02-03:32:12-root-INFO: grad norm: 19.589 19.433 2.464
2024-12-02-03:32:13-root-INFO: grad norm: 19.888 19.722 2.560
2024-12-02-03:32:13-root-INFO: Loss Change: 240.332 -> 235.729
2024-12-02-03:32:13-root-INFO: Regularization Change: 0.000 -> 1.488
2024-12-02-03:32:13-root-INFO: Undo step: 110
2024-12-02-03:32:13-root-INFO: Undo step: 111
2024-12-02-03:32:13-root-INFO: Undo step: 112
2024-12-02-03:32:13-root-INFO: Undo step: 113
2024-12-02-03:32:13-root-INFO: Undo step: 114
2024-12-02-03:32:14-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-03:32:14-root-INFO: grad norm: 77.691 76.355 14.342
2024-12-02-03:32:15-root-INFO: grad norm: 39.721 38.947 7.804
2024-12-02-03:32:16-root-INFO: grad norm: 28.240 27.483 6.494
2024-12-02-03:32:17-root-INFO: grad norm: 22.496 21.939 4.975
2024-12-02-03:32:18-root-INFO: grad norm: 19.468 18.932 4.536
2024-12-02-03:32:19-root-INFO: Loss Change: 454.034 -> 280.085
2024-12-02-03:32:19-root-INFO: Regularization Change: 0.000 -> 21.668
2024-12-02-03:32:19-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-03:32:19-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-03:32:19-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-03:32:19-root-INFO: grad norm: 18.893 18.506 3.806
2024-12-02-03:32:20-root-INFO: grad norm: 16.674 16.306 3.482
2024-12-02-03:32:21-root-INFO: grad norm: 15.428 15.138 2.977
2024-12-02-03:32:22-root-INFO: grad norm: 14.865 14.584 2.876
2024-12-02-03:32:23-root-INFO: grad norm: 14.594 14.360 2.602
2024-12-02-03:32:24-root-INFO: Loss Change: 280.152 -> 260.162
2024-12-02-03:32:24-root-INFO: Regularization Change: 0.000 -> 3.560
2024-12-02-03:32:24-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-03:32:24-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-03:32:24-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-03:32:25-root-INFO: grad norm: 16.013 15.795 2.634
2024-12-02-03:32:26-root-INFO: grad norm: 15.546 15.343 2.505
2024-12-02-03:32:26-root-INFO: grad norm: 15.579 15.415 2.248
2024-12-02-03:32:27-root-INFO: grad norm: 15.642 15.468 2.326
2024-12-02-03:32:28-root-INFO: grad norm: 15.735 15.593 2.109
2024-12-02-03:32:29-root-INFO: Loss Change: 259.932 -> 249.608
2024-12-02-03:32:29-root-INFO: Regularization Change: 0.000 -> 2.128
2024-12-02-03:32:29-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-03:32:29-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-03:32:30-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-03:32:30-root-INFO: grad norm: 16.076 15.882 2.490
2024-12-02-03:32:31-root-INFO: grad norm: 15.228 15.100 1.971
2024-12-02-03:32:32-root-INFO: grad norm: 15.077 14.946 1.983
2024-12-02-03:32:33-root-INFO: grad norm: 15.080 14.963 1.881
2024-12-02-03:32:34-root-INFO: grad norm: 15.080 14.961 1.888
2024-12-02-03:32:34-root-INFO: Loss Change: 249.235 -> 241.300
2024-12-02-03:32:34-root-INFO: Regularization Change: 0.000 -> 1.667
2024-12-02-03:32:34-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-03:32:34-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-03:32:35-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-03:32:35-root-INFO: grad norm: 15.470 15.344 1.972
2024-12-02-03:32:36-root-INFO: grad norm: 14.988 14.869 1.889
2024-12-02-03:32:37-root-INFO: grad norm: 14.744 14.642 1.724
2024-12-02-03:32:38-root-INFO: grad norm: 14.587 14.476 1.790
2024-12-02-03:32:39-root-INFO: grad norm: 14.455 14.359 1.664
2024-12-02-03:32:40-root-INFO: Loss Change: 241.149 -> 235.004
2024-12-02-03:32:40-root-INFO: Regularization Change: 0.000 -> 1.401
2024-12-02-03:32:40-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-03:32:40-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-03:32:40-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-03:32:40-root-INFO: grad norm: 14.989 14.853 2.015
2024-12-02-03:32:41-root-INFO: grad norm: 14.583 14.487 1.672
2024-12-02-03:32:42-root-INFO: grad norm: 14.522 14.423 1.694
2024-12-02-03:32:43-root-INFO: grad norm: 14.513 14.422 1.624
2024-12-02-03:32:44-root-INFO: grad norm: 14.502 14.410 1.628
2024-12-02-03:32:45-root-INFO: Loss Change: 235.073 -> 229.765
2024-12-02-03:32:45-root-INFO: Regularization Change: 0.000 -> 1.278
2024-12-02-03:32:45-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-03:32:45-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-03:32:45-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-03:32:46-root-INFO: grad norm: 15.886 15.756 2.031
2024-12-02-03:32:47-root-INFO: grad norm: 15.266 15.171 1.702
2024-12-02-03:32:48-root-INFO: grad norm: 14.963 14.880 1.581
2024-12-02-03:32:49-root-INFO: grad norm: 14.739 14.652 1.601
2024-12-02-03:32:50-root-INFO: grad norm: 14.533 14.454 1.512
2024-12-02-03:32:50-root-INFO: Loss Change: 230.345 -> 225.421
2024-12-02-03:32:50-root-INFO: Regularization Change: 0.000 -> 1.242
2024-12-02-03:32:50-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-03:32:50-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-03:32:51-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-03:32:51-root-INFO: grad norm: 14.753 14.638 1.836
2024-12-02-03:32:52-root-INFO: grad norm: 14.017 13.940 1.468
2024-12-02-03:32:53-root-INFO: grad norm: 13.631 13.549 1.496
2024-12-02-03:32:54-root-INFO: grad norm: 13.313 13.238 1.408
2024-12-02-03:32:55-root-INFO: grad norm: 13.057 12.980 1.414
2024-12-02-03:32:56-root-INFO: Loss Change: 225.213 -> 220.485
2024-12-02-03:32:56-root-INFO: Regularization Change: 0.000 -> 1.138
2024-12-02-03:32:56-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-03:32:56-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-03:32:56-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-03:32:56-root-INFO: grad norm: 15.275 15.129 2.106
2024-12-02-03:32:57-root-INFO: grad norm: 14.641 14.556 1.570
2024-12-02-03:32:58-root-INFO: grad norm: 14.501 14.421 1.522
2024-12-02-03:32:59-root-INFO: grad norm: 14.435 14.357 1.500
2024-12-02-03:33:00-root-INFO: grad norm: 14.398 14.324 1.462
2024-12-02-03:33:01-root-INFO: Loss Change: 221.339 -> 217.204
2024-12-02-03:33:01-root-INFO: Regularization Change: 0.000 -> 1.182
2024-12-02-03:33:01-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-03:33:01-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-03:33:01-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-03:33:02-root-INFO: grad norm: 13.764 13.659 1.693
2024-12-02-03:33:03-root-INFO: grad norm: 12.948 12.877 1.357
2024-12-02-03:33:04-root-INFO: grad norm: 12.676 12.608 1.318
2024-12-02-03:33:05-root-INFO: grad norm: 12.513 12.446 1.292
2024-12-02-03:33:06-root-INFO: grad norm: 12.410 12.346 1.266
2024-12-02-03:33:06-root-INFO: Loss Change: 216.741 -> 212.575
2024-12-02-03:33:06-root-INFO: Regularization Change: 0.000 -> 1.095
2024-12-02-03:33:06-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-03:33:06-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-03:33:07-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-03:33:07-root-INFO: grad norm: 14.522 14.394 1.926
2024-12-02-03:33:08-root-INFO: grad norm: 14.356 14.281 1.469
2024-12-02-03:33:09-root-INFO: grad norm: 14.478 14.397 1.530
2024-12-02-03:33:10-root-INFO: grad norm: 14.621 14.548 1.466
2024-12-02-03:33:11-root-INFO: grad norm: 14.789 14.711 1.511
2024-12-02-03:33:12-root-INFO: Loss Change: 213.457 -> 210.218
2024-12-02-03:33:12-root-INFO: Regularization Change: 0.000 -> 1.168
2024-12-02-03:33:12-root-INFO: Undo step: 105
2024-12-02-03:33:12-root-INFO: Undo step: 106
2024-12-02-03:33:12-root-INFO: Undo step: 107
2024-12-02-03:33:12-root-INFO: Undo step: 108
2024-12-02-03:33:12-root-INFO: Undo step: 109
2024-12-02-03:33:12-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-03:33:12-root-INFO: grad norm: 77.257 76.413 11.391
2024-12-02-03:33:13-root-INFO: grad norm: 45.481 44.797 7.860
2024-12-02-03:33:14-root-INFO: grad norm: 38.696 38.177 6.316
2024-12-02-03:33:15-root-INFO: grad norm: 34.580 34.196 5.138
2024-12-02-03:33:16-root-INFO: grad norm: 30.525 30.174 4.620
2024-12-02-03:33:17-root-INFO: Loss Change: 455.353 -> 265.740
2024-12-02-03:33:17-root-INFO: Regularization Change: 0.000 -> 28.665
2024-12-02-03:33:17-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-03:33:17-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-03:33:17-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-03:33:17-root-INFO: grad norm: 29.526 29.239 4.104
2024-12-02-03:33:18-root-INFO: grad norm: 27.484 27.222 3.783
2024-12-02-03:33:19-root-INFO: grad norm: 26.089 25.877 3.320
2024-12-02-03:33:20-root-INFO: grad norm: 24.663 24.446 3.262
2024-12-02-03:33:21-root-INFO: grad norm: 23.689 23.509 2.915
2024-12-02-03:33:22-root-INFO: Loss Change: 266.272 -> 242.543
2024-12-02-03:33:22-root-INFO: Regularization Change: 0.000 -> 5.002
2024-12-02-03:33:22-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-03:33:22-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-03:33:22-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-03:33:23-root-INFO: grad norm: 20.165 20.015 2.456
2024-12-02-03:33:24-root-INFO: grad norm: 19.177 19.025 2.410
2024-12-02-03:33:25-root-INFO: grad norm: 18.513 18.363 2.349
2024-12-02-03:33:26-root-INFO: grad norm: 18.174 18.036 2.233
2024-12-02-03:33:27-root-INFO: grad norm: 17.932 17.790 2.255
2024-12-02-03:33:27-root-INFO: Loss Change: 241.217 -> 230.975
2024-12-02-03:33:27-root-INFO: Regularization Change: 0.000 -> 2.588
2024-12-02-03:33:27-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-03:33:27-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-03:33:28-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-03:33:28-root-INFO: grad norm: 21.351 21.138 3.007
2024-12-02-03:33:29-root-INFO: grad norm: 21.045 20.891 2.540
2024-12-02-03:33:30-root-INFO: grad norm: 21.015 20.863 2.520
2024-12-02-03:33:31-root-INFO: grad norm: 21.007 20.859 2.486
2024-12-02-03:33:32-root-INFO: grad norm: 20.963 20.824 2.413
2024-12-02-03:33:33-root-INFO: Loss Change: 232.285 -> 224.813
2024-12-02-03:33:33-root-INFO: Regularization Change: 0.000 -> 2.210
2024-12-02-03:33:33-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-03:33:33-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-03:33:33-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-03:33:33-root-INFO: grad norm: 17.927 17.826 1.905
2024-12-02-03:33:34-root-INFO: grad norm: 17.664 17.542 2.077
2024-12-02-03:33:35-root-INFO: grad norm: 17.879 17.767 1.999
2024-12-02-03:33:36-root-INFO: grad norm: 18.177 18.058 2.076
2024-12-02-03:33:37-root-INFO: grad norm: 18.554 18.436 2.092
2024-12-02-03:33:38-root-INFO: Loss Change: 223.334 -> 218.571
2024-12-02-03:33:38-root-INFO: Regularization Change: 0.000 -> 1.794
2024-12-02-03:33:38-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-03:33:38-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-03:33:38-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-03:33:39-root-INFO: grad norm: 21.934 21.735 2.944
2024-12-02-03:33:40-root-INFO: grad norm: 22.148 22.009 2.476
2024-12-02-03:33:41-root-INFO: grad norm: 22.356 22.205 2.591
2024-12-02-03:33:42-root-INFO: grad norm: 22.482 22.343 2.504
2024-12-02-03:33:43-root-INFO: grad norm: 22.497 22.355 2.526
2024-12-02-03:33:43-root-INFO: Loss Change: 220.190 -> 215.488
2024-12-02-03:33:43-root-INFO: Regularization Change: 0.000 -> 1.987
2024-12-02-03:33:43-root-INFO: Undo step: 105
2024-12-02-03:33:43-root-INFO: Undo step: 106
2024-12-02-03:33:43-root-INFO: Undo step: 107
2024-12-02-03:33:43-root-INFO: Undo step: 108
2024-12-02-03:33:43-root-INFO: Undo step: 109
2024-12-02-03:33:44-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-03:33:44-root-INFO: grad norm: 78.078 77.123 12.176
2024-12-02-03:33:45-root-INFO: grad norm: 39.035 38.517 6.335
2024-12-02-03:33:46-root-INFO: grad norm: 28.252 27.751 5.297
2024-12-02-03:33:47-root-INFO: grad norm: 23.319 22.895 4.425
2024-12-02-03:33:48-root-INFO: grad norm: 20.458 20.055 4.043
2024-12-02-03:33:49-root-INFO: Loss Change: 454.640 -> 259.238
2024-12-02-03:33:49-root-INFO: Regularization Change: 0.000 -> 27.236
2024-12-02-03:33:49-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-03:33:49-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-03:33:49-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-03:33:49-root-INFO: grad norm: 22.607 22.232 4.101
2024-12-02-03:33:50-root-INFO: grad norm: 21.438 21.135 3.590
2024-12-02-03:33:51-root-INFO: grad norm: 20.828 20.560 3.331
2024-12-02-03:33:52-root-INFO: grad norm: 20.576 20.319 3.245
2024-12-02-03:33:53-root-INFO: grad norm: 20.609 20.383 3.046
2024-12-02-03:33:54-root-INFO: Loss Change: 260.352 -> 239.673
2024-12-02-03:33:54-root-INFO: Regularization Change: 0.000 -> 4.715
2024-12-02-03:33:54-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-03:33:54-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-03:33:54-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-03:33:55-root-INFO: grad norm: 17.777 17.587 2.593
2024-12-02-03:33:56-root-INFO: grad norm: 18.133 17.931 2.700
2024-12-02-03:33:57-root-INFO: grad norm: 18.874 18.684 2.667
2024-12-02-03:33:58-root-INFO: grad norm: 19.877 19.694 2.695
2024-12-02-03:33:59-root-INFO: grad norm: 21.007 20.825 2.757
2024-12-02-03:33:59-root-INFO: Loss Change: 238.053 -> 229.807
2024-12-02-03:33:59-root-INFO: Regularization Change: 0.000 -> 2.715
2024-12-02-03:33:59-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-03:33:59-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-03:34:00-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-03:34:00-root-INFO: grad norm: 28.088 27.822 3.854
2024-12-02-03:34:01-root-INFO: grad norm: 28.883 28.677 3.443
2024-12-02-03:34:02-root-INFO: grad norm: 29.503 29.301 3.447
2024-12-02-03:34:03-root-INFO: grad norm: 29.544 29.345 3.429
2024-12-02-03:34:04-root-INFO: grad norm: 29.366 29.177 3.324
2024-12-02-03:34:05-root-INFO: Loss Change: 232.645 -> 226.929
2024-12-02-03:34:05-root-INFO: Regularization Change: 0.000 -> 2.914
2024-12-02-03:34:05-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-03:34:05-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-03:34:05-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-03:34:05-root-INFO: grad norm: 25.521 25.383 2.642
2024-12-02-03:34:06-root-INFO: grad norm: 25.306 25.129 2.993
2024-12-02-03:34:07-root-INFO: grad norm: 25.331 25.164 2.904
2024-12-02-03:34:08-root-INFO: grad norm: 25.452 25.276 2.990
2024-12-02-03:34:09-root-INFO: grad norm: 25.545 25.368 3.005
2024-12-02-03:34:10-root-INFO: Loss Change: 224.480 -> 218.678
2024-12-02-03:34:10-root-INFO: Regularization Change: 0.000 -> 2.414
2024-12-02-03:34:10-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-03:34:10-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-03:34:10-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-03:34:11-root-INFO: grad norm: 29.973 29.717 3.905
2024-12-02-03:34:12-root-INFO: grad norm: 29.311 29.111 3.415
2024-12-02-03:34:13-root-INFO: grad norm: 28.326 28.131 3.316
2024-12-02-03:34:14-root-INFO: grad norm: 27.456 27.267 3.213
2024-12-02-03:34:15-root-INFO: grad norm: 26.567 26.391 3.053
2024-12-02-03:34:15-root-INFO: Loss Change: 221.589 -> 214.995
2024-12-02-03:34:15-root-INFO: Regularization Change: 0.000 -> 2.604
2024-12-02-03:34:15-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-03:34:15-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-03:34:16-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-03:34:16-root-INFO: grad norm: 22.132 22.017 2.260
2024-12-02-03:34:17-root-INFO: grad norm: 21.787 21.639 2.533
2024-12-02-03:34:18-root-INFO: grad norm: 21.795 21.657 2.452
2024-12-02-03:34:19-root-INFO: grad norm: 21.936 21.791 2.519
2024-12-02-03:34:20-root-INFO: grad norm: 22.112 21.969 2.514
2024-12-02-03:34:21-root-INFO: Loss Change: 212.537 -> 208.275
2024-12-02-03:34:21-root-INFO: Regularization Change: 0.000 -> 1.968
2024-12-02-03:34:21-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-03:34:21-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-03:34:21-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-03:34:21-root-INFO: grad norm: 26.141 25.925 3.354
2024-12-02-03:34:22-root-INFO: grad norm: 25.694 25.515 3.030
2024-12-02-03:34:23-root-INFO: grad norm: 25.060 24.891 2.912
2024-12-02-03:34:24-root-INFO: grad norm: 24.479 24.312 2.857
2024-12-02-03:34:25-root-INFO: grad norm: 23.903 23.749 2.712
2024-12-02-03:34:26-root-INFO: Loss Change: 210.588 -> 205.662
2024-12-02-03:34:26-root-INFO: Regularization Change: 0.000 -> 2.250
2024-12-02-03:34:26-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-03:34:26-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-03:34:27-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-03:34:27-root-INFO: grad norm: 21.006 20.895 2.158
2024-12-02-03:34:28-root-INFO: grad norm: 21.045 20.905 2.420
2024-12-02-03:34:29-root-INFO: grad norm: 21.219 21.089 2.342
2024-12-02-03:34:30-root-INFO: grad norm: 21.429 21.293 2.412
2024-12-02-03:34:31-root-INFO: grad norm: 21.603 21.468 2.412
2024-12-02-03:34:32-root-INFO: Loss Change: 203.913 -> 200.780
2024-12-02-03:34:32-root-INFO: Regularization Change: 0.000 -> 1.898
2024-12-02-03:34:32-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-03:34:32-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-03:34:32-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-03:34:32-root-INFO: grad norm: 25.284 25.081 3.197
2024-12-02-03:34:33-root-INFO: grad norm: 24.732 24.572 2.813
2024-12-02-03:34:34-root-INFO: grad norm: 24.000 23.842 2.755
2024-12-02-03:34:35-root-INFO: grad norm: 23.318 23.167 2.657
2024-12-02-03:34:36-root-INFO: grad norm: 22.642 22.498 2.555
2024-12-02-03:34:37-root-INFO: Loss Change: 203.009 -> 198.121
2024-12-02-03:34:37-root-INFO: Regularization Change: 0.000 -> 2.225
2024-12-02-03:34:37-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-03:34:37-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-03:34:37-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-03:34:37-root-INFO: grad norm: 19.317 19.228 1.850
2024-12-02-03:34:38-root-INFO: grad norm: 18.941 18.818 2.154
2024-12-02-03:34:39-root-INFO: grad norm: 18.744 18.635 2.017
2024-12-02-03:34:40-root-INFO: grad norm: 18.644 18.526 2.102
2024-12-02-03:34:41-root-INFO: grad norm: 18.599 18.486 2.048
2024-12-02-03:34:42-root-INFO: Loss Change: 196.636 -> 193.153
2024-12-02-03:34:42-root-INFO: Regularization Change: 0.000 -> 1.732
2024-12-02-03:34:42-root-INFO: Undo step: 100
2024-12-02-03:34:42-root-INFO: Undo step: 101
2024-12-02-03:34:42-root-INFO: Undo step: 102
2024-12-02-03:34:42-root-INFO: Undo step: 103
2024-12-02-03:34:42-root-INFO: Undo step: 104
2024-12-02-03:34:42-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-03:34:43-root-INFO: grad norm: 61.416 60.881 8.085
2024-12-02-03:34:44-root-INFO: grad norm: 33.237 32.660 6.162
2024-12-02-03:34:45-root-INFO: grad norm: 23.326 22.820 4.830
2024-12-02-03:34:46-root-INFO: grad norm: 18.499 18.025 4.160
2024-12-02-03:34:47-root-INFO: grad norm: 15.823 15.406 3.606
2024-12-02-03:34:47-root-INFO: Loss Change: 421.707 -> 232.240
2024-12-02-03:34:47-root-INFO: Regularization Change: 0.000 -> 33.781
2024-12-02-03:34:47-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-03:34:47-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-03:34:48-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-03:34:48-root-INFO: grad norm: 15.470 15.086 3.424
2024-12-02-03:34:49-root-INFO: grad norm: 13.743 13.431 2.915
2024-12-02-03:34:50-root-INFO: grad norm: 12.828 12.537 2.717
2024-12-02-03:34:51-root-INFO: grad norm: 12.344 12.082 2.529
2024-12-02-03:34:52-root-INFO: grad norm: 12.152 11.922 2.354
2024-12-02-03:34:53-root-INFO: Loss Change: 232.156 -> 213.811
2024-12-02-03:34:53-root-INFO: Regularization Change: 0.000 -> 4.425
2024-12-02-03:34:53-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-03:34:53-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-03:34:53-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-03:34:53-root-INFO: grad norm: 10.392 10.190 2.037
2024-12-02-03:34:54-root-INFO: grad norm: 9.952 9.757 1.963
2024-12-02-03:34:55-root-INFO: grad norm: 9.895 9.712 1.892
2024-12-02-03:34:56-root-INFO: grad norm: 9.960 9.798 1.790
2024-12-02-03:34:57-root-INFO: grad norm: 10.157 9.998 1.794
2024-12-02-03:34:58-root-INFO: Loss Change: 213.170 -> 204.710
2024-12-02-03:34:58-root-INFO: Regularization Change: 0.000 -> 2.247
2024-12-02-03:34:58-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-03:34:58-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-03:34:58-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-03:34:59-root-INFO: grad norm: 12.192 12.016 2.064
2024-12-02-03:35:00-root-INFO: grad norm: 12.599 12.457 1.885
2024-12-02-03:35:01-root-INFO: grad norm: 13.229 13.098 1.856
2024-12-02-03:35:02-root-INFO: grad norm: 14.021 13.885 1.946
2024-12-02-03:35:03-root-INFO: grad norm: 14.759 14.636 1.899
2024-12-02-03:35:03-root-INFO: Loss Change: 204.755 -> 199.830
2024-12-02-03:35:03-root-INFO: Regularization Change: 0.000 -> 1.846
2024-12-02-03:35:03-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-03:35:03-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-03:35:04-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-03:35:04-root-INFO: grad norm: 13.140 13.046 1.571
2024-12-02-03:35:05-root-INFO: grad norm: 13.436 13.327 1.710
2024-12-02-03:35:06-root-INFO: grad norm: 14.004 13.896 1.736
2024-12-02-03:35:07-root-INFO: grad norm: 14.612 14.505 1.769
2024-12-02-03:35:08-root-INFO: grad norm: 15.305 15.192 1.859
2024-12-02-03:35:09-root-INFO: Loss Change: 198.980 -> 195.581
2024-12-02-03:35:09-root-INFO: Regularization Change: 0.000 -> 1.646
2024-12-02-03:35:09-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-03:35:09-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-03:35:09-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-03:35:09-root-INFO: grad norm: 19.114 18.934 2.616
2024-12-02-03:35:10-root-INFO: grad norm: 19.294 19.160 2.266
2024-12-02-03:35:11-root-INFO: grad norm: 19.339 19.200 2.317
2024-12-02-03:35:12-root-INFO: grad norm: 19.204 19.071 2.260
2024-12-02-03:35:13-root-INFO: grad norm: 18.998 18.868 2.223
2024-12-02-03:35:14-root-INFO: Loss Change: 197.048 -> 192.874
2024-12-02-03:35:14-root-INFO: Regularization Change: 0.000 -> 1.943
2024-12-02-03:35:14-root-INFO: Undo step: 100
2024-12-02-03:35:14-root-INFO: Undo step: 101
2024-12-02-03:35:14-root-INFO: Undo step: 102
2024-12-02-03:35:14-root-INFO: Undo step: 103
2024-12-02-03:35:14-root-INFO: Undo step: 104
2024-12-02-03:35:14-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-03:35:15-root-INFO: grad norm: 71.742 71.190 8.882
2024-12-02-03:35:15-root-INFO: grad norm: 37.494 36.969 6.254
2024-12-02-03:35:17-root-INFO: grad norm: 23.866 23.314 5.100
2024-12-02-03:35:17-root-INFO: grad norm: 18.077 17.580 4.210
2024-12-02-03:35:18-root-INFO: grad norm: 14.974 14.522 3.652
2024-12-02-03:35:19-root-INFO: Loss Change: 434.512 -> 236.223
2024-12-02-03:35:19-root-INFO: Regularization Change: 0.000 -> 32.886
2024-12-02-03:35:19-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-03:35:19-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-03:35:19-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-03:35:20-root-INFO: grad norm: 15.722 15.376 3.280
2024-12-02-03:35:21-root-INFO: grad norm: 14.759 14.467 2.919
2024-12-02-03:35:22-root-INFO: grad norm: 14.340 14.096 2.633
2024-12-02-03:35:23-root-INFO: grad norm: 14.126 13.896 2.541
2024-12-02-03:35:24-root-INFO: grad norm: 14.031 13.839 2.313
2024-12-02-03:35:24-root-INFO: Loss Change: 236.215 -> 217.289
2024-12-02-03:35:24-root-INFO: Regularization Change: 0.000 -> 4.767
2024-12-02-03:35:24-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-03:35:24-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-03:35:25-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-03:35:25-root-INFO: grad norm: 11.553 11.378 2.003
2024-12-02-03:35:26-root-INFO: grad norm: 11.322 11.158 1.923
2024-12-02-03:35:27-root-INFO: grad norm: 11.365 11.203 1.912
2024-12-02-03:35:28-root-INFO: grad norm: 11.526 11.387 1.784
2024-12-02-03:35:29-root-INFO: grad norm: 11.737 11.591 1.842
2024-12-02-03:35:30-root-INFO: Loss Change: 216.316 -> 207.179
2024-12-02-03:35:30-root-INFO: Regularization Change: 0.000 -> 2.500
2024-12-02-03:35:30-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-03:35:30-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-03:35:30-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-03:35:30-root-INFO: grad norm: 15.256 15.098 2.191
2024-12-02-03:35:31-root-INFO: grad norm: 15.513 15.379 2.035
2024-12-02-03:35:32-root-INFO: grad norm: 15.822 15.698 1.981
2024-12-02-03:35:33-root-INFO: grad norm: 16.046 15.914 2.052
2024-12-02-03:35:34-root-INFO: grad norm: 16.237 16.118 1.963
2024-12-02-03:35:35-root-INFO: Loss Change: 207.783 -> 202.081
2024-12-02-03:35:35-root-INFO: Regularization Change: 0.000 -> 2.098
2024-12-02-03:35:35-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-03:35:35-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-03:35:35-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-03:35:36-root-INFO: grad norm: 13.800 13.702 1.639
2024-12-02-03:35:37-root-INFO: grad norm: 13.876 13.770 1.705
2024-12-02-03:35:38-root-INFO: grad norm: 14.075 13.966 1.747
2024-12-02-03:35:39-root-INFO: grad norm: 14.343 14.238 1.724
2024-12-02-03:35:40-root-INFO: grad norm: 14.592 14.479 1.807
2024-12-02-03:35:40-root-INFO: Loss Change: 201.013 -> 196.492
2024-12-02-03:35:40-root-INFO: Regularization Change: 0.000 -> 1.703
2024-12-02-03:35:40-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-03:35:40-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-03:35:41-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-03:35:41-root-INFO: grad norm: 18.477 18.306 2.509
2024-12-02-03:35:42-root-INFO: grad norm: 18.247 18.114 2.195
2024-12-02-03:35:43-root-INFO: grad norm: 17.947 17.815 2.171
2024-12-02-03:35:44-root-INFO: grad norm: 17.627 17.497 2.138
2024-12-02-03:35:45-root-INFO: grad norm: 17.288 17.165 2.057
2024-12-02-03:35:46-root-INFO: Loss Change: 197.997 -> 193.412
2024-12-02-03:35:46-root-INFO: Regularization Change: 0.000 -> 1.887
2024-12-02-03:35:46-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-03:35:46-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-03:35:46-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-03:35:46-root-INFO: grad norm: 14.626 14.530 1.672
2024-12-02-03:35:47-root-INFO: grad norm: 14.360 14.257 1.717
2024-12-02-03:35:48-root-INFO: grad norm: 14.259 14.155 1.723
2024-12-02-03:35:49-root-INFO: grad norm: 14.238 14.137 1.690
2024-12-02-03:35:50-root-INFO: grad norm: 14.242 14.135 1.739
2024-12-02-03:35:51-root-INFO: Loss Change: 192.140 -> 188.244
2024-12-02-03:35:51-root-INFO: Regularization Change: 0.000 -> 1.538
2024-12-02-03:35:51-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-03:35:51-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-03:35:51-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-03:35:52-root-INFO: grad norm: 17.827 17.666 2.392
2024-12-02-03:35:53-root-INFO: grad norm: 17.606 17.477 2.128
2024-12-02-03:35:54-root-INFO: grad norm: 17.340 17.214 2.082
2024-12-02-03:35:54-root-INFO: grad norm: 17.074 16.947 2.084
2024-12-02-03:35:56-root-INFO: grad norm: 16.781 16.663 1.989
2024-12-02-03:35:56-root-INFO: Loss Change: 189.745 -> 185.972
2024-12-02-03:35:56-root-INFO: Regularization Change: 0.000 -> 1.781
2024-12-02-03:35:56-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-03:35:56-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-03:35:57-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-03:35:57-root-INFO: grad norm: 13.725 13.637 1.555
2024-12-02-03:35:58-root-INFO: grad norm: 13.616 13.519 1.621
2024-12-02-03:35:59-root-INFO: grad norm: 13.666 13.567 1.638
2024-12-02-03:36:00-root-INFO: grad norm: 13.783 13.688 1.621
2024-12-02-03:36:01-root-INFO: grad norm: 13.911 13.807 1.694
2024-12-02-03:36:01-root-INFO: Loss Change: 184.621 -> 181.524
2024-12-02-03:36:01-root-INFO: Regularization Change: 0.000 -> 1.466
2024-12-02-03:36:01-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-03:36:01-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-03:36:02-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-03:36:02-root-INFO: grad norm: 16.743 16.593 2.237
2024-12-02-03:36:03-root-INFO: grad norm: 16.607 16.483 2.021
2024-12-02-03:36:04-root-INFO: grad norm: 16.433 16.313 1.982
2024-12-02-03:36:05-root-INFO: grad norm: 16.246 16.124 1.983
2024-12-02-03:36:06-root-INFO: grad norm: 16.032 15.919 1.900
2024-12-02-03:36:07-root-INFO: Loss Change: 182.753 -> 179.510
2024-12-02-03:36:07-root-INFO: Regularization Change: 0.000 -> 1.721
2024-12-02-03:36:07-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-03:36:07-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-03:36:07-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-03:36:07-root-INFO: grad norm: 13.114 13.042 1.376
2024-12-02-03:36:08-root-INFO: grad norm: 13.098 13.005 1.561
2024-12-02-03:36:09-root-INFO: grad norm: 13.200 13.109 1.546
2024-12-02-03:36:10-root-INFO: grad norm: 13.365 13.272 1.574
2024-12-02-03:36:11-root-INFO: grad norm: 13.535 13.437 1.623
2024-12-02-03:36:12-root-INFO: Loss Change: 178.467 -> 175.821
2024-12-02-03:36:12-root-INFO: Regularization Change: 0.000 -> 1.447
2024-12-02-03:36:12-root-INFO: Undo step: 95
2024-12-02-03:36:12-root-INFO: Undo step: 96
2024-12-02-03:36:12-root-INFO: Undo step: 97
2024-12-02-03:36:12-root-INFO: Undo step: 98
2024-12-02-03:36:12-root-INFO: Undo step: 99
2024-12-02-03:36:12-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-03:36:13-root-INFO: grad norm: 57.241 56.502 9.174
2024-12-02-03:36:14-root-INFO: grad norm: 27.595 27.048 5.466
2024-12-02-03:36:15-root-INFO: grad norm: 18.571 18.086 4.215
2024-12-02-03:36:16-root-INFO: grad norm: 14.545 14.157 3.337
2024-12-02-03:36:17-root-INFO: grad norm: 12.268 11.917 2.913
2024-12-02-03:36:17-root-INFO: Loss Change: 381.124 -> 213.874
2024-12-02-03:36:17-root-INFO: Regularization Change: 0.000 -> 32.366
2024-12-02-03:36:17-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-03:36:17-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-03:36:18-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-03:36:18-root-INFO: grad norm: 11.666 11.397 2.487
2024-12-02-03:36:19-root-INFO: grad norm: 10.621 10.375 2.271
2024-12-02-03:36:20-root-INFO: grad norm: 10.363 10.165 2.012
2024-12-02-03:36:21-root-INFO: grad norm: 10.857 10.682 1.944
2024-12-02-03:36:22-root-INFO: grad norm: 11.824 11.685 1.804
2024-12-02-03:36:23-root-INFO: Loss Change: 213.438 -> 197.611
2024-12-02-03:36:23-root-INFO: Regularization Change: 0.000 -> 4.743
2024-12-02-03:36:23-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-03:36:23-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-03:36:23-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-03:36:23-root-INFO: grad norm: 14.115 13.987 1.897
2024-12-02-03:36:24-root-INFO: grad norm: 14.901 14.798 1.750
2024-12-02-03:36:25-root-INFO: grad norm: 15.451 15.354 1.730
2024-12-02-03:36:26-root-INFO: grad norm: 15.423 15.332 1.674
2024-12-02-03:36:27-root-INFO: grad norm: 14.957 14.873 1.589
2024-12-02-03:36:28-root-INFO: Loss Change: 197.749 -> 190.244
2024-12-02-03:36:28-root-INFO: Regularization Change: 0.000 -> 2.844
2024-12-02-03:36:28-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-03:36:28-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-03:36:28-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-03:36:29-root-INFO: grad norm: 14.491 14.396 1.656
2024-12-02-03:36:30-root-INFO: grad norm: 13.345 13.274 1.372
2024-12-02-03:36:31-root-INFO: grad norm: 12.705 12.631 1.371
2024-12-02-03:36:32-root-INFO: grad norm: 12.187 12.122 1.257
2024-12-02-03:36:33-root-INFO: grad norm: 11.850 11.782 1.272
2024-12-02-03:36:33-root-INFO: Loss Change: 190.119 -> 183.204
2024-12-02-03:36:33-root-INFO: Regularization Change: 0.000 -> 2.103
2024-12-02-03:36:33-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-03:36:33-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-03:36:34-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-03:36:34-root-INFO: grad norm: 11.497 11.421 1.322
2024-12-02-03:36:35-root-INFO: grad norm: 10.911 10.846 1.191
2024-12-02-03:36:36-root-INFO: grad norm: 10.571 10.511 1.123
2024-12-02-03:36:37-root-INFO: grad norm: 10.344 10.282 1.135
2024-12-02-03:36:38-root-INFO: grad norm: 10.169 10.112 1.077
2024-12-02-03:36:39-root-INFO: Loss Change: 183.183 -> 178.380
2024-12-02-03:36:39-root-INFO: Regularization Change: 0.000 -> 1.625
2024-12-02-03:36:39-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-03:36:39-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-03:36:39-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-03:36:39-root-INFO: grad norm: 10.141 10.056 1.314
2024-12-02-03:36:40-root-INFO: grad norm: 9.660 9.605 1.034
2024-12-02-03:36:41-root-INFO: grad norm: 9.452 9.390 1.083
2024-12-02-03:36:42-root-INFO: grad norm: 9.335 9.283 0.987
2024-12-02-03:36:43-root-INFO: grad norm: 9.279 9.222 1.033
2024-12-02-03:36:44-root-INFO: Loss Change: 178.569 -> 174.488
2024-12-02-03:36:44-root-INFO: Regularization Change: 0.000 -> 1.414
2024-12-02-03:36:44-root-INFO: Undo step: 95
2024-12-02-03:36:44-root-INFO: Undo step: 96
2024-12-02-03:36:44-root-INFO: Undo step: 97
2024-12-02-03:36:44-root-INFO: Undo step: 98
2024-12-02-03:36:44-root-INFO: Undo step: 99
2024-12-02-03:36:44-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-03:36:45-root-INFO: grad norm: 79.133 78.758 7.696
2024-12-02-03:36:46-root-INFO: grad norm: 40.230 39.847 5.538
2024-12-02-03:36:47-root-INFO: grad norm: 25.902 25.621 3.807
2024-12-02-03:36:48-root-INFO: grad norm: 19.728 19.473 3.166
2024-12-02-03:36:49-root-INFO: grad norm: 16.890 16.653 2.818
2024-12-02-03:36:49-root-INFO: Loss Change: 416.456 -> 212.203
2024-12-02-03:36:49-root-INFO: Regularization Change: 0.000 -> 34.850
2024-12-02-03:36:49-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-03:36:49-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-03:36:50-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-03:36:50-root-INFO: grad norm: 17.470 17.246 2.792
2024-12-02-03:36:51-root-INFO: grad norm: 16.316 16.115 2.554
2024-12-02-03:36:52-root-INFO: grad norm: 15.505 15.336 2.287
2024-12-02-03:36:53-root-INFO: grad norm: 14.863 14.684 2.301
2024-12-02-03:36:54-root-INFO: grad norm: 14.381 14.231 2.070
2024-12-02-03:36:55-root-INFO: Loss Change: 212.559 -> 195.631
2024-12-02-03:36:55-root-INFO: Regularization Change: 0.000 -> 4.888
2024-12-02-03:36:55-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-03:36:55-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-03:36:55-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-03:36:55-root-INFO: grad norm: 11.924 11.798 1.728
2024-12-02-03:36:56-root-INFO: grad norm: 11.663 11.536 1.714
2024-12-02-03:36:57-root-INFO: grad norm: 11.612 11.480 1.746
2024-12-02-03:36:58-root-INFO: grad norm: 11.699 11.580 1.661
2024-12-02-03:36:59-root-INFO: grad norm: 11.840 11.710 1.754
2024-12-02-03:37:00-root-INFO: Loss Change: 194.736 -> 186.485
2024-12-02-03:37:00-root-INFO: Regularization Change: 0.000 -> 2.666
2024-12-02-03:37:00-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-03:37:00-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-03:37:00-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-03:37:01-root-INFO: grad norm: 15.945 15.748 2.504
2024-12-02-03:37:02-root-INFO: grad norm: 15.624 15.473 2.164
2024-12-02-03:37:03-root-INFO: grad norm: 15.528 15.398 2.004
2024-12-02-03:37:04-root-INFO: grad norm: 15.565 15.432 2.031
2024-12-02-03:37:05-root-INFO: grad norm: 15.647 15.529 1.916
2024-12-02-03:37:05-root-INFO: Loss Change: 187.642 -> 182.055
2024-12-02-03:37:05-root-INFO: Regularization Change: 0.000 -> 2.343
2024-12-02-03:37:05-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-03:37:05-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-03:37:06-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-03:37:06-root-INFO: grad norm: 13.714 13.623 1.581
2024-12-02-03:37:07-root-INFO: grad norm: 13.810 13.711 1.651
2024-12-02-03:37:08-root-INFO: grad norm: 14.026 13.924 1.684
2024-12-02-03:37:09-root-INFO: grad norm: 14.330 14.231 1.677
2024-12-02-03:37:10-root-INFO: grad norm: 14.643 14.537 1.758
2024-12-02-03:37:11-root-INFO: Loss Change: 181.107 -> 177.084
2024-12-02-03:37:11-root-INFO: Regularization Change: 0.000 -> 1.936
2024-12-02-03:37:11-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-03:37:11-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-03:37:11-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-03:37:11-root-INFO: grad norm: 18.487 18.311 2.542
2024-12-02-03:37:12-root-INFO: grad norm: 18.403 18.267 2.232
2024-12-02-03:37:13-root-INFO: grad norm: 18.258 18.128 2.172
2024-12-02-03:37:14-root-INFO: grad norm: 18.081 17.949 2.184
2024-12-02-03:37:15-root-INFO: grad norm: 17.890 17.768 2.085
2024-12-02-03:37:16-root-INFO: Loss Change: 178.982 -> 175.100
2024-12-02-03:37:16-root-INFO: Regularization Change: 0.000 -> 2.260
2024-12-02-03:37:16-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-03:37:16-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-03:37:16-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-03:37:17-root-INFO: grad norm: 15.758 15.668 1.688
2024-12-02-03:37:18-root-INFO: grad norm: 15.962 15.861 1.789
2024-12-02-03:37:19-root-INFO: grad norm: 16.105 15.997 1.859
2024-12-02-03:37:20-root-INFO: grad norm: 16.272 16.168 1.837
2024-12-02-03:37:21-root-INFO: grad norm: 16.367 16.253 1.932
2024-12-02-03:37:21-root-INFO: Loss Change: 173.862 -> 170.648
2024-12-02-03:37:21-root-INFO: Regularization Change: 0.000 -> 1.960
2024-12-02-03:37:21-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-03:37:21-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-03:37:22-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-03:37:22-root-INFO: grad norm: 19.945 19.765 2.676
2024-12-02-03:37:23-root-INFO: grad norm: 19.178 19.035 2.341
2024-12-02-03:37:24-root-INFO: grad norm: 18.454 18.331 2.126
2024-12-02-03:37:25-root-INFO: grad norm: 17.901 17.779 2.089
2024-12-02-03:37:26-root-INFO: grad norm: 17.342 17.234 1.939
2024-12-02-03:37:27-root-INFO: Loss Change: 172.423 -> 167.936
2024-12-02-03:37:27-root-INFO: Regularization Change: 0.000 -> 2.279
2024-12-02-03:37:27-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-03:37:27-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-03:37:27-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-03:37:27-root-INFO: grad norm: 15.313 15.231 1.582
2024-12-02-03:37:28-root-INFO: grad norm: 14.912 14.824 1.617
2024-12-02-03:37:29-root-INFO: grad norm: 14.742 14.651 1.636
2024-12-02-03:37:30-root-INFO: grad norm: 14.595 14.507 1.593
2024-12-02-03:37:31-root-INFO: grad norm: 14.467 14.373 1.650
2024-12-02-03:37:32-root-INFO: Loss Change: 167.087 -> 163.613
2024-12-02-03:37:32-root-INFO: Regularization Change: 0.000 -> 1.851
2024-12-02-03:37:32-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-03:37:32-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-03:37:32-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-03:37:33-root-INFO: grad norm: 17.069 16.929 2.182
2024-12-02-03:37:34-root-INFO: grad norm: 16.799 16.681 1.987
2024-12-02-03:37:35-root-INFO: grad norm: 16.422 16.313 1.887
2024-12-02-03:37:36-root-INFO: grad norm: 16.091 15.982 1.874
2024-12-02-03:37:37-root-INFO: grad norm: 15.697 15.596 1.773
2024-12-02-03:37:37-root-INFO: Loss Change: 164.947 -> 161.407
2024-12-02-03:37:37-root-INFO: Regularization Change: 0.000 -> 2.039
2024-12-02-03:37:37-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-03:37:37-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-03:37:38-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-03:37:38-root-INFO: grad norm: 13.516 13.448 1.355
2024-12-02-03:37:39-root-INFO: grad norm: 13.051 12.973 1.422
2024-12-02-03:37:40-root-INFO: grad norm: 12.632 12.554 1.398
2024-12-02-03:37:41-root-INFO: grad norm: 12.514 12.434 1.416
2024-12-02-03:37:42-root-INFO: grad norm: 12.466 12.386 1.412
2024-12-02-03:37:43-root-INFO: Loss Change: 160.522 -> 157.420
2024-12-02-03:37:43-root-INFO: Regularization Change: 0.000 -> 1.643
2024-12-02-03:37:43-root-INFO: Undo step: 90
2024-12-02-03:37:43-root-INFO: Undo step: 91
2024-12-02-03:37:43-root-INFO: Undo step: 92
2024-12-02-03:37:43-root-INFO: Undo step: 93
2024-12-02-03:37:43-root-INFO: Undo step: 94
2024-12-02-03:37:43-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-03:37:43-root-INFO: grad norm: 44.862 44.327 6.906
2024-12-02-03:37:44-root-INFO: grad norm: 24.889 24.486 4.463
2024-12-02-03:37:45-root-INFO: grad norm: 18.691 18.318 3.717
2024-12-02-03:37:46-root-INFO: grad norm: 15.864 15.597 2.899
2024-12-02-03:37:47-root-INFO: grad norm: 14.554 14.294 2.741
2024-12-02-03:37:48-root-INFO: Loss Change: 327.763 -> 196.073
2024-12-02-03:37:48-root-INFO: Regularization Change: 0.000 -> 32.486
2024-12-02-03:37:48-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-03:37:48-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-03:37:48-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-03:37:49-root-INFO: grad norm: 16.531 16.338 2.521
2024-12-02-03:37:50-root-INFO: grad norm: 16.917 16.732 2.497
2024-12-02-03:37:51-root-INFO: grad norm: 17.700 17.557 2.245
2024-12-02-03:37:52-root-INFO: grad norm: 18.423 18.265 2.412
2024-12-02-03:37:53-root-INFO: grad norm: 19.020 18.890 2.222
2024-12-02-03:37:53-root-INFO: Loss Change: 196.622 -> 182.691
2024-12-02-03:37:53-root-INFO: Regularization Change: 0.000 -> 6.015
2024-12-02-03:37:53-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-03:37:53-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-03:37:54-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-03:37:54-root-INFO: grad norm: 16.976 16.868 1.913
2024-12-02-03:37:55-root-INFO: grad norm: 17.087 16.978 1.921
2024-12-02-03:37:56-root-INFO: grad norm: 17.100 16.982 2.008
2024-12-02-03:37:57-root-INFO: grad norm: 17.115 17.007 1.918
2024-12-02-03:37:58-root-INFO: grad norm: 16.961 16.841 2.021
2024-12-02-03:37:59-root-INFO: Loss Change: 180.990 -> 173.045
2024-12-02-03:37:59-root-INFO: Regularization Change: 0.000 -> 3.591
2024-12-02-03:37:59-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-03:37:59-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-03:37:59-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-03:37:59-root-INFO: grad norm: 19.071 18.910 2.470
2024-12-02-03:38:00-root-INFO: grad norm: 18.344 18.206 2.251
2024-12-02-03:38:01-root-INFO: grad norm: 17.585 17.462 2.075
2024-12-02-03:38:02-root-INFO: grad norm: 16.939 16.810 2.092
2024-12-02-03:38:03-root-INFO: grad norm: 16.334 16.222 1.909
2024-12-02-03:38:04-root-INFO: Loss Change: 174.526 -> 167.429
2024-12-02-03:38:04-root-INFO: Regularization Change: 0.000 -> 3.033
2024-12-02-03:38:04-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-03:38:04-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-03:38:04-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-03:38:05-root-INFO: grad norm: 13.834 13.751 1.521
2024-12-02-03:38:06-root-INFO: grad norm: 13.569 13.478 1.568
2024-12-02-03:38:07-root-INFO: grad norm: 13.375 13.280 1.586
2024-12-02-03:38:08-root-INFO: grad norm: 13.248 13.158 1.541
2024-12-02-03:38:09-root-INFO: grad norm: 13.138 13.043 1.580
2024-12-02-03:38:09-root-INFO: Loss Change: 166.287 -> 161.650
2024-12-02-03:38:09-root-INFO: Regularization Change: 0.000 -> 2.164
2024-12-02-03:38:09-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-03:38:09-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-03:38:10-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-03:38:10-root-INFO: grad norm: 15.647 15.503 2.116
2024-12-02-03:38:11-root-INFO: grad norm: 15.017 14.910 1.794
2024-12-02-03:38:12-root-INFO: grad norm: 14.356 14.256 1.694
2024-12-02-03:38:13-root-INFO: grad norm: 13.896 13.800 1.633
2024-12-02-03:38:14-root-INFO: grad norm: 13.512 13.420 1.573
2024-12-02-03:38:15-root-INFO: Loss Change: 163.020 -> 158.303
2024-12-02-03:38:15-root-INFO: Regularization Change: 0.000 -> 2.165
2024-12-02-03:38:15-root-INFO: Undo step: 90
2024-12-02-03:38:15-root-INFO: Undo step: 91
2024-12-02-03:38:15-root-INFO: Undo step: 92
2024-12-02-03:38:15-root-INFO: Undo step: 93
2024-12-02-03:38:15-root-INFO: Undo step: 94
2024-12-02-03:38:15-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-03:38:15-root-INFO: grad norm: 62.849 62.214 8.911
2024-12-02-03:38:16-root-INFO: grad norm: 37.211 36.741 5.897
2024-12-02-03:38:17-root-INFO: grad norm: 23.914 23.554 4.135
2024-12-02-03:38:18-root-INFO: grad norm: 20.638 20.397 3.148
2024-12-02-03:38:19-root-INFO: grad norm: 19.502 19.304 2.772
2024-12-02-03:38:20-root-INFO: Loss Change: 361.273 -> 195.298
2024-12-02-03:38:20-root-INFO: Regularization Change: 0.000 -> 37.462
2024-12-02-03:38:20-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-03:38:20-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-03:38:21-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-03:38:21-root-INFO: grad norm: 19.756 19.589 2.567
2024-12-02-03:38:22-root-INFO: grad norm: 19.039 18.898 2.317
2024-12-02-03:38:23-root-INFO: grad norm: 18.707 18.586 2.120
2024-12-02-03:38:24-root-INFO: grad norm: 18.645 18.531 2.061
2024-12-02-03:38:25-root-INFO: grad norm: 18.963 18.863 1.947
2024-12-02-03:38:25-root-INFO: Loss Change: 195.551 -> 180.031
2024-12-02-03:38:25-root-INFO: Regularization Change: 0.000 -> 5.960
2024-12-02-03:38:25-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-03:38:25-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-03:38:26-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-03:38:26-root-INFO: grad norm: 17.864 17.785 1.677
2024-12-02-03:38:27-root-INFO: grad norm: 17.915 17.840 1.639
2024-12-02-03:38:28-root-INFO: grad norm: 18.103 18.031 1.616
2024-12-02-03:38:29-root-INFO: grad norm: 18.300 18.226 1.646
2024-12-02-03:38:30-root-INFO: grad norm: 18.256 18.177 1.699
2024-12-02-03:38:31-root-INFO: Loss Change: 178.860 -> 171.741
2024-12-02-03:38:31-root-INFO: Regularization Change: 0.000 -> 3.640
2024-12-02-03:38:31-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-03:38:31-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-03:38:31-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-03:38:31-root-INFO: grad norm: 20.349 20.219 2.296
2024-12-02-03:38:32-root-INFO: grad norm: 19.522 19.411 2.072
2024-12-02-03:38:33-root-INFO: grad norm: 18.694 18.587 1.996
2024-12-02-03:38:34-root-INFO: grad norm: 18.117 18.003 2.030
2024-12-02-03:38:35-root-INFO: grad norm: 17.987 17.878 1.979
2024-12-02-03:38:36-root-INFO: Loss Change: 173.164 -> 166.219
2024-12-02-03:38:36-root-INFO: Regularization Change: 0.000 -> 3.236
2024-12-02-03:38:36-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-03:38:36-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-03:38:36-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-03:38:37-root-INFO: grad norm: 15.574 15.491 1.609
2024-12-02-03:38:38-root-INFO: grad norm: 15.453 15.359 1.698
2024-12-02-03:38:39-root-INFO: grad norm: 15.611 15.507 1.794
2024-12-02-03:38:40-root-INFO: grad norm: 16.158 16.056 1.813
2024-12-02-03:38:41-root-INFO: grad norm: 16.350 16.235 1.943
2024-12-02-03:38:41-root-INFO: Loss Change: 164.844 -> 161.053
2024-12-02-03:38:41-root-INFO: Regularization Change: 0.000 -> 2.449
2024-12-02-03:38:41-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-03:38:41-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-03:38:42-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-03:38:42-root-INFO: grad norm: 19.077 18.914 2.485
2024-12-02-03:38:43-root-INFO: grad norm: 19.008 18.877 2.231
2024-12-02-03:38:44-root-INFO: grad norm: 19.151 19.026 2.177
2024-12-02-03:38:45-root-INFO: grad norm: 18.723 18.593 2.206
2024-12-02-03:38:46-root-INFO: grad norm: 18.231 18.122 1.991
2024-12-02-03:38:47-root-INFO: Loss Change: 162.736 -> 158.445
2024-12-02-03:38:47-root-INFO: Regularization Change: 0.000 -> 2.775
2024-12-02-03:38:47-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-03:38:47-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-03:38:47-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-03:38:47-root-INFO: grad norm: 15.798 15.720 1.571
2024-12-02-03:38:48-root-INFO: grad norm: 15.825 15.745 1.591
2024-12-02-03:38:49-root-INFO: grad norm: 15.926 15.837 1.676
2024-12-02-03:38:50-root-INFO: grad norm: 15.879 15.797 1.613
2024-12-02-03:38:51-root-INFO: grad norm: 15.697 15.603 1.715
2024-12-02-03:38:52-root-INFO: Loss Change: 156.811 -> 153.826
2024-12-02-03:38:52-root-INFO: Regularization Change: 0.000 -> 2.303
2024-12-02-03:38:52-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-03:38:52-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-03:38:52-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-03:38:53-root-INFO: grad norm: 17.682 17.554 2.122
2024-12-02-03:38:54-root-INFO: grad norm: 17.114 17.015 1.839
2024-12-02-03:38:55-root-INFO: grad norm: 16.347 16.255 1.728
2024-12-02-03:38:56-root-INFO: grad norm: 15.554 15.463 1.678
2024-12-02-03:38:57-root-INFO: grad norm: 14.962 14.879 1.579
2024-12-02-03:38:57-root-INFO: Loss Change: 155.425 -> 150.357
2024-12-02-03:38:57-root-INFO: Regularization Change: 0.000 -> 2.378
2024-12-02-03:38:57-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-03:38:57-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-03:38:58-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-03:38:58-root-INFO: grad norm: 13.019 12.958 1.255
2024-12-02-03:38:59-root-INFO: grad norm: 11.599 11.539 1.180
2024-12-02-03:39:00-root-INFO: grad norm: 11.086 11.021 1.199
2024-12-02-03:39:01-root-INFO: grad norm: 10.980 10.906 1.270
2024-12-02-03:39:02-root-INFO: grad norm: 10.891 10.816 1.272
2024-12-02-03:39:03-root-INFO: Loss Change: 149.141 -> 145.768
2024-12-02-03:39:03-root-INFO: Regularization Change: 0.000 -> 1.656
2024-12-02-03:39:03-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-03:39:03-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-03:39:03-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-03:39:03-root-INFO: grad norm: 14.127 13.982 2.022
2024-12-02-03:39:04-root-INFO: grad norm: 13.028 12.932 1.582
2024-12-02-03:39:05-root-INFO: grad norm: 12.438 12.353 1.449
2024-12-02-03:39:06-root-INFO: grad norm: 11.983 11.908 1.331
2024-12-02-03:39:07-root-INFO: grad norm: 11.643 11.572 1.283
2024-12-02-03:39:08-root-INFO: Loss Change: 147.162 -> 143.444
2024-12-02-03:39:08-root-INFO: Regularization Change: 0.000 -> 1.786
2024-12-02-03:39:08-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-03:39:08-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-03:39:08-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-03:39:09-root-INFO: grad norm: 10.486 10.437 1.016
2024-12-02-03:39:10-root-INFO: grad norm: 9.089 9.036 0.974
2024-12-02-03:39:11-root-INFO: grad norm: 8.710 8.658 0.951
2024-12-02-03:39:12-root-INFO: grad norm: 8.737 8.674 1.051
2024-12-02-03:39:13-root-INFO: grad norm: 8.707 8.647 1.020
2024-12-02-03:39:13-root-INFO: Loss Change: 142.708 -> 139.891
2024-12-02-03:39:13-root-INFO: Regularization Change: 0.000 -> 1.369
2024-12-02-03:39:13-root-INFO: Undo step: 85
2024-12-02-03:39:13-root-INFO: Undo step: 86
2024-12-02-03:39:13-root-INFO: Undo step: 87
2024-12-02-03:39:13-root-INFO: Undo step: 88
2024-12-02-03:39:13-root-INFO: Undo step: 89
2024-12-02-03:39:14-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-03:39:14-root-INFO: grad norm: 49.919 49.450 6.824
2024-12-02-03:39:15-root-INFO: grad norm: 28.712 28.264 5.054
2024-12-02-03:39:16-root-INFO: grad norm: 20.145 19.827 3.562
2024-12-02-03:39:17-root-INFO: grad norm: 15.151 14.866 2.922
2024-12-02-03:39:18-root-INFO: grad norm: 12.016 11.779 2.374
2024-12-02-03:39:19-root-INFO: Loss Change: 320.772 -> 173.731
2024-12-02-03:39:19-root-INFO: Regularization Change: 0.000 -> 37.532
2024-12-02-03:39:19-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-03:39:19-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-03:39:19-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-03:39:19-root-INFO: grad norm: 11.347 11.144 2.137
2024-12-02-03:39:20-root-INFO: grad norm: 9.779 9.578 1.974
2024-12-02-03:39:21-root-INFO: grad norm: 8.809 8.642 1.707
2024-12-02-03:39:22-root-INFO: grad norm: 8.138 7.966 1.666
2024-12-02-03:39:23-root-INFO: grad norm: 7.694 7.553 1.467
2024-12-02-03:39:24-root-INFO: Loss Change: 173.633 -> 158.616
2024-12-02-03:39:24-root-INFO: Regularization Change: 0.000 -> 5.299
2024-12-02-03:39:24-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-03:39:24-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-03:39:24-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-03:39:24-root-INFO: grad norm: 6.676 6.538 1.351
2024-12-02-03:39:25-root-INFO: grad norm: 6.303 6.175 1.265
2024-12-02-03:39:26-root-INFO: grad norm: 6.005 5.881 1.215
2024-12-02-03:39:27-root-INFO: grad norm: 5.819 5.708 1.132
2024-12-02-03:39:28-root-INFO: grad norm: 5.660 5.549 1.118
2024-12-02-03:39:29-root-INFO: Loss Change: 158.569 -> 151.401
2024-12-02-03:39:29-root-INFO: Regularization Change: 0.000 -> 2.640
2024-12-02-03:39:29-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-03:39:29-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-03:39:29-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-03:39:30-root-INFO: grad norm: 8.750 8.589 1.670
2024-12-02-03:39:31-root-INFO: grad norm: 7.979 7.871 1.313
2024-12-02-03:39:32-root-INFO: grad norm: 7.330 7.230 1.207
2024-12-02-03:39:33-root-INFO: grad norm: 7.296 7.204 1.155
2024-12-02-03:39:34-root-INFO: grad norm: 7.401 7.314 1.131
2024-12-02-03:39:34-root-INFO: Loss Change: 151.755 -> 146.748
2024-12-02-03:39:34-root-INFO: Regularization Change: 0.000 -> 2.050
2024-12-02-03:39:34-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-03:39:34-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-03:39:35-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-03:39:35-root-INFO: grad norm: 5.622 5.549 0.901
2024-12-02-03:39:36-root-INFO: grad norm: 5.502 5.429 0.895
2024-12-02-03:39:37-root-INFO: grad norm: 5.645 5.575 0.885
2024-12-02-03:39:38-root-INFO: grad norm: 6.327 6.259 0.929
2024-12-02-03:39:39-root-INFO: grad norm: 6.196 6.123 0.947
2024-12-02-03:39:40-root-INFO: Loss Change: 146.258 -> 142.505
2024-12-02-03:39:40-root-INFO: Regularization Change: 0.000 -> 1.568
2024-12-02-03:39:40-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-03:39:40-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-03:39:40-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-03:39:40-root-INFO: grad norm: 8.642 8.507 1.521
2024-12-02-03:39:41-root-INFO: grad norm: 8.481 8.397 1.193
2024-12-02-03:39:42-root-INFO: grad norm: 8.282 8.195 1.192
2024-12-02-03:39:43-root-INFO: grad norm: 8.479 8.398 1.166
2024-12-02-03:39:44-root-INFO: grad norm: 9.072 8.990 1.215
2024-12-02-03:39:45-root-INFO: Loss Change: 143.088 -> 140.291
2024-12-02-03:39:45-root-INFO: Regularization Change: 0.000 -> 1.692
2024-12-02-03:39:45-root-INFO: Undo step: 85
2024-12-02-03:39:45-root-INFO: Undo step: 86
2024-12-02-03:39:45-root-INFO: Undo step: 87
2024-12-02-03:39:45-root-INFO: Undo step: 88
2024-12-02-03:39:45-root-INFO: Undo step: 89
2024-12-02-03:39:46-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-03:39:46-root-INFO: grad norm: 46.318 45.967 5.690
2024-12-02-03:39:47-root-INFO: grad norm: 23.883 23.604 3.641
2024-12-02-03:39:48-root-INFO: grad norm: 17.090 16.846 2.878
2024-12-02-03:39:49-root-INFO: grad norm: 13.086 12.861 2.415
2024-12-02-03:39:50-root-INFO: grad norm: 10.534 10.330 2.064
2024-12-02-03:39:51-root-INFO: Loss Change: 340.307 -> 176.117
2024-12-02-03:39:51-root-INFO: Regularization Change: 0.000 -> 45.073
2024-12-02-03:39:51-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-03:39:51-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-03:39:51-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-03:39:51-root-INFO: grad norm: 9.583 9.394 1.895
2024-12-02-03:39:52-root-INFO: grad norm: 8.203 8.026 1.699
2024-12-02-03:39:53-root-INFO: grad norm: 7.375 7.228 1.465
2024-12-02-03:39:54-root-INFO: grad norm: 6.794 6.641 1.430
2024-12-02-03:39:55-root-INFO: grad norm: 6.373 6.247 1.260
2024-12-02-03:39:56-root-INFO: Loss Change: 175.636 -> 158.853
2024-12-02-03:39:56-root-INFO: Regularization Change: 0.000 -> 5.923
2024-12-02-03:39:56-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-03:39:56-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-03:39:56-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-03:39:57-root-INFO: grad norm: 5.630 5.501 1.199
2024-12-02-03:39:58-root-INFO: grad norm: 5.100 4.978 1.107
2024-12-02-03:39:59-root-INFO: grad norm: 4.801 4.686 1.044
2024-12-02-03:39:59-root-INFO: grad norm: 4.568 4.460 0.990
2024-12-02-03:40:00-root-INFO: grad norm: 4.373 4.269 0.949
2024-12-02-03:40:01-root-INFO: Loss Change: 158.903 -> 151.010
2024-12-02-03:40:01-root-INFO: Regularization Change: 0.000 -> 2.864
2024-12-02-03:40:01-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-03:40:01-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-03:40:01-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-03:40:02-root-INFO: grad norm: 6.485 6.328 1.414
2024-12-02-03:40:03-root-INFO: grad norm: 6.078 5.983 1.069
2024-12-02-03:40:04-root-INFO: grad norm: 6.064 5.975 1.040
2024-12-02-03:40:05-root-INFO: grad norm: 6.081 5.996 1.017
2024-12-02-03:40:06-root-INFO: grad norm: 6.254 6.174 1.001
2024-12-02-03:40:06-root-INFO: Loss Change: 151.040 -> 145.816
2024-12-02-03:40:06-root-INFO: Regularization Change: 0.000 -> 2.110
2024-12-02-03:40:07-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-03:40:07-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-03:40:07-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-03:40:07-root-INFO: grad norm: 4.809 4.729 0.873
2024-12-02-03:40:08-root-INFO: grad norm: 4.622 4.549 0.817
2024-12-02-03:40:09-root-INFO: grad norm: 4.605 4.537 0.787
2024-12-02-03:40:10-root-INFO: grad norm: 4.768 4.701 0.793
2024-12-02-03:40:11-root-INFO: grad norm: 4.711 4.645 0.787
2024-12-02-03:40:12-root-INFO: Loss Change: 145.394 -> 141.258
2024-12-02-03:40:12-root-INFO: Regularization Change: 0.000 -> 1.601
2024-12-02-03:40:12-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-03:40:12-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-03:40:12-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-03:40:12-root-INFO: grad norm: 7.424 7.298 1.363
2024-12-02-03:40:13-root-INFO: grad norm: 6.533 6.460 0.974
2024-12-02-03:40:14-root-INFO: grad norm: 5.855 5.783 0.918
2024-12-02-03:40:15-root-INFO: grad norm: 5.799 5.739 0.831
2024-12-02-03:40:16-root-INFO: grad norm: 5.806 5.744 0.845
2024-12-02-03:40:17-root-INFO: Loss Change: 141.624 -> 137.990
2024-12-02-03:40:17-root-INFO: Regularization Change: 0.000 -> 1.487
2024-12-02-03:40:17-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-03:40:17-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-03:40:17-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-03:40:18-root-INFO: grad norm: 4.784 4.733 0.692
2024-12-02-03:40:19-root-INFO: grad norm: 4.849 4.801 0.685
2024-12-02-03:40:20-root-INFO: grad norm: 5.988 5.949 0.677
2024-12-02-03:40:21-root-INFO: grad norm: 5.176 5.131 0.685
2024-12-02-03:40:22-root-INFO: grad norm: 4.244 4.198 0.624
2024-12-02-03:40:22-root-INFO: Loss Change: 137.548 -> 134.536
2024-12-02-03:40:22-root-INFO: Regularization Change: 0.000 -> 1.254
2024-12-02-03:40:22-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-03:40:22-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-03:40:23-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-03:40:23-root-INFO: grad norm: 6.850 6.739 1.226
2024-12-02-03:40:24-root-INFO: grad norm: 6.906 6.842 0.937
2024-12-02-03:40:25-root-INFO: grad norm: 8.200 8.132 1.049
2024-12-02-03:40:25-root-INFO: Loss too large (133.959->133.966)! Learning rate decreased to 0.06543.
2024-12-02-03:40:26-root-INFO: grad norm: 5.380 5.326 0.761
2024-12-02-03:40:27-root-INFO: grad norm: 3.545 3.495 0.591
2024-12-02-03:40:28-root-INFO: Loss Change: 134.971 -> 131.815
2024-12-02-03:40:28-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-03:40:28-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-03:40:28-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-03:40:28-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-03:40:29-root-INFO: grad norm: 4.215 4.147 0.757
2024-12-02-03:40:29-root-INFO: grad norm: 4.066 4.019 0.621
2024-12-02-03:40:31-root-INFO: grad norm: 4.338 4.294 0.613
2024-12-02-03:40:31-root-INFO: grad norm: 4.008 3.961 0.609
2024-12-02-03:40:33-root-INFO: grad norm: 3.100 3.051 0.546
2024-12-02-03:40:33-root-INFO: Loss Change: 131.946 -> 129.417
2024-12-02-03:40:33-root-INFO: Regularization Change: 0.000 -> 1.102
2024-12-02-03:40:33-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-03:40:33-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-03:40:34-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-03:40:34-root-INFO: grad norm: 4.052 3.972 0.799
2024-12-02-03:40:35-root-INFO: grad norm: 5.579 5.545 0.613
2024-12-02-03:40:35-root-INFO: Loss too large (128.766->128.795)! Learning rate decreased to 0.06882.
2024-12-02-03:40:36-root-INFO: grad norm: 4.001 3.961 0.567
2024-12-02-03:40:37-root-INFO: grad norm: 2.491 2.443 0.486
2024-12-02-03:40:38-root-INFO: grad norm: 2.472 2.424 0.485
2024-12-02-03:40:39-root-INFO: Loss Change: 129.267 -> 126.986
2024-12-02-03:40:39-root-INFO: Regularization Change: 0.000 -> 0.761
2024-12-02-03:40:39-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-03:40:39-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-03:40:39-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-03:40:39-root-INFO: grad norm: 4.086 3.979 0.931
2024-12-02-03:40:40-root-INFO: grad norm: 3.591 3.544 0.580
2024-12-02-03:40:41-root-INFO: grad norm: 3.494 3.443 0.594
2024-12-02-03:40:42-root-INFO: grad norm: 3.546 3.503 0.549
2024-12-02-03:40:43-root-INFO: grad norm: 4.202 4.159 0.595
2024-12-02-03:40:44-root-INFO: Loss Change: 127.314 -> 125.168
2024-12-02-03:40:44-root-INFO: Regularization Change: 0.000 -> 1.132
2024-12-02-03:40:44-root-INFO: Undo step: 80
2024-12-02-03:40:44-root-INFO: Undo step: 81
2024-12-02-03:40:44-root-INFO: Undo step: 82
2024-12-02-03:40:44-root-INFO: Undo step: 83
2024-12-02-03:40:44-root-INFO: Undo step: 84
2024-12-02-03:40:44-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-03:40:45-root-INFO: grad norm: 50.193 49.677 7.177
2024-12-02-03:40:46-root-INFO: grad norm: 27.341 27.065 3.874
2024-12-02-03:40:47-root-INFO: grad norm: 16.149 15.870 2.987
2024-12-02-03:40:48-root-INFO: grad norm: 11.678 11.471 2.187
2024-12-02-03:40:49-root-INFO: grad norm: 9.559 9.368 1.897
2024-12-02-03:40:49-root-INFO: Loss Change: 302.364 -> 157.529
2024-12-02-03:40:49-root-INFO: Regularization Change: 0.000 -> 41.106
2024-12-02-03:40:49-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-03:40:49-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-03:40:50-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-03:40:50-root-INFO: grad norm: 8.562 8.399 1.664
2024-12-02-03:40:51-root-INFO: grad norm: 7.912 7.771 1.484
2024-12-02-03:40:52-root-INFO: grad norm: 6.845 6.705 1.378
2024-12-02-03:40:53-root-INFO: grad norm: 5.548 5.406 1.244
2024-12-02-03:40:54-root-INFO: grad norm: 5.195 5.063 1.165
2024-12-02-03:40:55-root-INFO: Loss Change: 157.182 -> 143.700
2024-12-02-03:40:55-root-INFO: Regularization Change: 0.000 -> 5.328
2024-12-02-03:40:55-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-03:40:55-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-03:40:55-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-03:40:55-root-INFO: grad norm: 6.719 6.602 1.250
2024-12-02-03:40:56-root-INFO: grad norm: 5.793 5.684 1.119
2024-12-02-03:40:57-root-INFO: grad norm: 4.695 4.586 1.005
2024-12-02-03:40:58-root-INFO: grad norm: 4.577 4.479 0.942
2024-12-02-03:40:59-root-INFO: grad norm: 4.611 4.521 0.907
2024-12-02-03:41:00-root-INFO: Loss Change: 143.608 -> 136.747
2024-12-02-03:41:00-root-INFO: Regularization Change: 0.000 -> 2.846
2024-12-02-03:41:00-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-03:41:00-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-03:41:00-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-03:41:01-root-INFO: grad norm: 4.323 4.235 0.868
2024-12-02-03:41:02-root-INFO: grad norm: 4.197 4.118 0.810
2024-12-02-03:41:03-root-INFO: grad norm: 4.310 4.240 0.773
2024-12-02-03:41:04-root-INFO: grad norm: 4.502 4.439 0.755
2024-12-02-03:41:05-root-INFO: grad norm: 4.838 4.783 0.730
2024-12-02-03:41:05-root-INFO: Loss Change: 136.652 -> 132.252
2024-12-02-03:41:05-root-INFO: Regularization Change: 0.000 -> 2.037
2024-12-02-03:41:05-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-03:41:05-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-03:41:06-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-03:41:06-root-INFO: grad norm: 6.908 6.820 1.096
2024-12-02-03:41:07-root-INFO: grad norm: 7.665 7.612 0.902
2024-12-02-03:41:08-root-INFO: grad norm: 10.500 10.443 1.096
2024-12-02-03:41:08-root-INFO: Loss too large (131.507->131.699)! Learning rate decreased to 0.06882.
2024-12-02-03:41:09-root-INFO: grad norm: 6.262 6.208 0.824
2024-12-02-03:41:10-root-INFO: grad norm: 5.244 5.196 0.703
2024-12-02-03:41:11-root-INFO: Loss Change: 132.351 -> 128.422
2024-12-02-03:41:11-root-INFO: Regularization Change: 0.000 -> 1.372
2024-12-02-03:41:11-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-03:41:11-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-03:41:11-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-03:41:12-root-INFO: grad norm: 5.106 5.008 0.996
2024-12-02-03:41:13-root-INFO: grad norm: 4.760 4.699 0.759
2024-12-02-03:41:14-root-INFO: grad norm: 4.768 4.711 0.737
2024-12-02-03:41:15-root-INFO: grad norm: 4.595 4.536 0.732
2024-12-02-03:41:16-root-INFO: grad norm: 4.166 4.111 0.669
2024-12-02-03:41:16-root-INFO: Loss Change: 128.635 -> 125.548
2024-12-02-03:41:16-root-INFO: Regularization Change: 0.000 -> 1.516
2024-12-02-03:41:16-root-INFO: Undo step: 80
2024-12-02-03:41:16-root-INFO: Undo step: 81
2024-12-02-03:41:16-root-INFO: Undo step: 82
2024-12-02-03:41:16-root-INFO: Undo step: 83
2024-12-02-03:41:16-root-INFO: Undo step: 84
2024-12-02-03:41:17-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-03:41:17-root-INFO: grad norm: 45.711 45.322 5.946
2024-12-02-03:41:18-root-INFO: grad norm: 23.935 23.636 3.771
2024-12-02-03:41:19-root-INFO: grad norm: 16.494 16.198 3.107
2024-12-02-03:41:20-root-INFO: grad norm: 13.670 13.459 2.390
2024-12-02-03:41:21-root-INFO: grad norm: 12.230 12.017 2.277
2024-12-02-03:41:22-root-INFO: Loss Change: 312.915 -> 159.762
2024-12-02-03:41:22-root-INFO: Regularization Change: 0.000 -> 44.254
2024-12-02-03:41:22-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-03:41:22-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-03:41:22-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-03:41:22-root-INFO: grad norm: 12.887 12.733 1.990
2024-12-02-03:41:23-root-INFO: grad norm: 12.725 12.577 1.938
2024-12-02-03:41:24-root-INFO: grad norm: 12.833 12.724 1.667
2024-12-02-03:41:25-root-INFO: grad norm: 12.806 12.688 1.737
2024-12-02-03:41:26-root-INFO: grad norm: 12.591 12.496 1.546
2024-12-02-03:41:27-root-INFO: Loss Change: 160.016 -> 146.383
2024-12-02-03:41:27-root-INFO: Regularization Change: 0.000 -> 6.408
2024-12-02-03:41:27-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-03:41:27-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-03:41:27-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-03:41:28-root-INFO: grad norm: 11.093 11.018 1.285
2024-12-02-03:41:29-root-INFO: grad norm: 11.758 11.681 1.343
2024-12-02-03:41:29-root-INFO: grad norm: 10.877 10.793 1.347
2024-12-02-03:41:30-root-INFO: grad norm: 10.463 10.394 1.198
2024-12-02-03:41:31-root-INFO: grad norm: 10.572 10.509 1.147
2024-12-02-03:41:32-root-INFO: Loss Change: 145.415 -> 138.883
2024-12-02-03:41:32-root-INFO: Regularization Change: 0.000 -> 3.444
2024-12-02-03:41:32-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-03:41:32-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-03:41:32-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-03:41:33-root-INFO: grad norm: 11.598 11.516 1.382
2024-12-02-03:41:34-root-INFO: grad norm: 11.451 11.387 1.212
2024-12-02-03:41:35-root-INFO: grad norm: 10.781 10.719 1.153
2024-12-02-03:41:36-root-INFO: grad norm: 9.848 9.783 1.138
2024-12-02-03:41:37-root-INFO: grad norm: 9.591 9.528 1.091
2024-12-02-03:41:37-root-INFO: Loss Change: 139.464 -> 133.354
2024-12-02-03:41:37-root-INFO: Regularization Change: 0.000 -> 2.739
2024-12-02-03:41:37-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-03:41:37-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-03:41:38-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-03:41:38-root-INFO: grad norm: 8.583 8.538 0.879
2024-12-02-03:41:39-root-INFO: grad norm: 8.118 8.076 0.827
2024-12-02-03:41:40-root-INFO: grad norm: 7.500 7.462 0.757
2024-12-02-03:41:41-root-INFO: grad norm: 7.619 7.578 0.797
2024-12-02-03:41:42-root-INFO: grad norm: 8.778 8.744 0.768
2024-12-02-03:41:42-root-INFO: Loss too large (129.454->129.547)! Learning rate decreased to 0.06882.
2024-12-02-03:41:43-root-INFO: Loss Change: 132.645 -> 128.846
2024-12-02-03:41:43-root-INFO: Regularization Change: 0.000 -> 1.843
2024-12-02-03:41:43-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-03:41:43-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-03:41:43-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-03:41:44-root-INFO: grad norm: 7.822 7.741 1.123
2024-12-02-03:41:45-root-INFO: grad norm: 7.113 7.064 0.833
2024-12-02-03:41:46-root-INFO: grad norm: 7.275 7.227 0.836
2024-12-02-03:41:47-root-INFO: grad norm: 8.720 8.685 0.782
2024-12-02-03:41:47-root-INFO: Loss too large (126.951->127.090)! Learning rate decreased to 0.07057.
2024-12-02-03:41:48-root-INFO: grad norm: 5.650 5.610 0.669
2024-12-02-03:41:49-root-INFO: Loss Change: 129.438 -> 125.020
2024-12-02-03:41:49-root-INFO: Regularization Change: 0.000 -> 1.413
2024-12-02-03:41:49-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-03:41:49-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-03:41:49-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-03:41:49-root-INFO: grad norm: 3.700 3.615 0.787
2024-12-02-03:41:50-root-INFO: grad norm: 3.466 3.423 0.547
2024-12-02-03:41:51-root-INFO: grad norm: 3.897 3.857 0.557
2024-12-02-03:41:52-root-INFO: grad norm: 6.612 6.580 0.650
2024-12-02-03:41:53-root-INFO: Loss too large (123.212->123.417)! Learning rate decreased to 0.07235.
2024-12-02-03:41:54-root-INFO: grad norm: 4.243 4.199 0.605
2024-12-02-03:41:54-root-INFO: Loss Change: 124.801 -> 122.013
2024-12-02-03:41:54-root-INFO: Regularization Change: 0.000 -> 1.143
2024-12-02-03:41:54-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-03:41:54-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-03:41:55-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-03:41:55-root-INFO: grad norm: 3.981 3.900 0.797
2024-12-02-03:41:56-root-INFO: grad norm: 4.268 4.232 0.556
2024-12-02-03:41:57-root-INFO: grad norm: 4.687 4.649 0.598
2024-12-02-03:41:58-root-INFO: grad norm: 6.811 6.784 0.605
2024-12-02-03:41:58-root-INFO: Loss too large (120.960->121.207)! Learning rate decreased to 0.07416.
2024-12-02-03:41:59-root-INFO: grad norm: 4.336 4.298 0.572
2024-12-02-03:42:00-root-INFO: Loss Change: 122.314 -> 119.734
2024-12-02-03:42:00-root-INFO: Regularization Change: 0.000 -> 1.069
2024-12-02-03:42:00-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-03:42:00-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-03:42:00-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-03:42:01-root-INFO: grad norm: 3.427 3.345 0.746
2024-12-02-03:42:02-root-INFO: grad norm: 3.246 3.204 0.519
2024-12-02-03:42:03-root-INFO: grad norm: 3.596 3.560 0.511
2024-12-02-03:42:03-root-INFO: grad norm: 6.009 5.983 0.554
2024-12-02-03:42:04-root-INFO: Loss too large (118.168->118.395)! Learning rate decreased to 0.07600.
2024-12-02-03:42:05-root-INFO: grad norm: 4.080 4.043 0.548
2024-12-02-03:42:05-root-INFO: Loss Change: 119.498 -> 117.074
2024-12-02-03:42:05-root-INFO: Regularization Change: 0.000 -> 1.015
2024-12-02-03:42:06-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-03:42:06-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-03:42:06-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-03:42:06-root-INFO: grad norm: 3.816 3.724 0.831
2024-12-02-03:42:07-root-INFO: grad norm: 3.810 3.759 0.622
2024-12-02-03:42:08-root-INFO: grad norm: 5.731 5.682 0.742
2024-12-02-03:42:08-root-INFO: Loss too large (116.315->116.561)! Learning rate decreased to 0.07788.
2024-12-02-03:42:09-root-INFO: grad norm: 4.493 4.446 0.649
2024-12-02-03:42:10-root-INFO: grad norm: 2.649 2.608 0.463
2024-12-02-03:42:11-root-INFO: Loss Change: 117.149 -> 114.893
2024-12-02-03:42:11-root-INFO: Regularization Change: 0.000 -> 0.926
2024-12-02-03:42:11-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-03:42:11-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-03:42:11-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-03:42:12-root-INFO: grad norm: 3.076 3.008 0.643
2024-12-02-03:42:13-root-INFO: grad norm: 3.276 3.245 0.455
2024-12-02-03:42:14-root-INFO: grad norm: 3.938 3.905 0.509
2024-12-02-03:42:15-root-INFO: grad norm: 6.821 6.793 0.619
2024-12-02-03:42:15-root-INFO: Loss too large (113.958->114.347)! Learning rate decreased to 0.07980.
2024-12-02-03:42:15-root-INFO: Loss too large (113.958->114.041)! Learning rate decreased to 0.06384.
2024-12-02-03:42:16-root-INFO: grad norm: 4.063 4.028 0.539
2024-12-02-03:42:17-root-INFO: Loss Change: 114.937 -> 112.908
2024-12-02-03:42:17-root-INFO: Regularization Change: 0.000 -> 0.835
2024-12-02-03:42:17-root-INFO: Undo step: 75
2024-12-02-03:42:17-root-INFO: Undo step: 76
2024-12-02-03:42:17-root-INFO: Undo step: 77
2024-12-02-03:42:17-root-INFO: Undo step: 78
2024-12-02-03:42:17-root-INFO: Undo step: 79
2024-12-02-03:42:18-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-03:42:18-root-INFO: grad norm: 40.634 40.253 5.549
2024-12-02-03:42:19-root-INFO: grad norm: 22.816 22.569 3.348
2024-12-02-03:42:20-root-INFO: grad norm: 15.946 15.691 2.836
2024-12-02-03:42:21-root-INFO: grad norm: 12.168 11.974 2.166
2024-12-02-03:42:22-root-INFO: grad norm: 10.159 9.991 1.837
2024-12-02-03:42:22-root-INFO: Loss Change: 284.911 -> 143.548
2024-12-02-03:42:22-root-INFO: Regularization Change: 0.000 -> 47.714
2024-12-02-03:42:22-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-03:42:23-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-03:42:23-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-03:42:23-root-INFO: grad norm: 10.435 10.264 1.886
2024-12-02-03:42:24-root-INFO: grad norm: 9.324 9.206 1.483
2024-12-02-03:42:25-root-INFO: grad norm: 8.645 8.532 1.392
2024-12-02-03:42:26-root-INFO: grad norm: 8.161 8.068 1.226
2024-12-02-03:42:27-root-INFO: grad norm: 7.817 7.730 1.163
2024-12-02-03:42:28-root-INFO: Loss Change: 143.705 -> 129.734
2024-12-02-03:42:28-root-INFO: Regularization Change: 0.000 -> 6.559
2024-12-02-03:42:28-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-03:42:28-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-03:42:28-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-03:42:28-root-INFO: grad norm: 6.545 6.480 0.915
2024-12-02-03:42:29-root-INFO: grad norm: 6.501 6.438 0.904
2024-12-02-03:42:30-root-INFO: grad norm: 6.425 6.368 0.855
2024-12-02-03:42:31-root-INFO: grad norm: 6.438 6.382 0.847
2024-12-02-03:42:32-root-INFO: grad norm: 6.447 6.394 0.825
2024-12-02-03:42:33-root-INFO: Loss Change: 129.444 -> 123.356
2024-12-02-03:42:33-root-INFO: Regularization Change: 0.000 -> 3.141
2024-12-02-03:42:33-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-03:42:33-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-03:42:33-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-03:42:34-root-INFO: grad norm: 8.466 8.367 1.296
2024-12-02-03:42:35-root-INFO: grad norm: 7.566 7.497 1.016
2024-12-02-03:42:36-root-INFO: grad norm: 6.598 6.540 0.877
2024-12-02-03:42:37-root-INFO: grad norm: 6.272 6.222 0.792
2024-12-02-03:42:38-root-INFO: grad norm: 6.121 6.070 0.785
2024-12-02-03:42:38-root-INFO: Loss Change: 123.592 -> 118.845
2024-12-02-03:42:38-root-INFO: Regularization Change: 0.000 -> 2.330
2024-12-02-03:42:38-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-03:42:38-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-03:42:39-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-03:42:39-root-INFO: grad norm: 4.793 4.749 0.645
2024-12-02-03:42:40-root-INFO: grad norm: 4.747 4.710 0.593
2024-12-02-03:42:41-root-INFO: grad norm: 5.019 4.982 0.614
2024-12-02-03:42:42-root-INFO: grad norm: 6.402 6.361 0.722
2024-12-02-03:42:43-root-INFO: grad norm: 5.625 5.578 0.725
2024-12-02-03:42:44-root-INFO: Loss Change: 118.311 -> 115.169
2024-12-02-03:42:44-root-INFO: Regularization Change: 0.000 -> 1.699
2024-12-02-03:42:44-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-03:42:44-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-03:42:44-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-03:42:44-root-INFO: grad norm: 6.309 6.221 1.051
2024-12-02-03:42:46-root-INFO: grad norm: 5.993 5.942 0.784
2024-12-02-03:42:46-root-INFO: grad norm: 6.131 6.078 0.805
2024-12-02-03:42:47-root-INFO: grad norm: 6.362 6.308 0.823
2024-12-02-03:42:48-root-INFO: grad norm: 7.415 7.362 0.889
2024-12-02-03:42:49-root-INFO: Loss too large (113.418->113.542)! Learning rate decreased to 0.07980.
2024-12-02-03:42:49-root-INFO: Loss Change: 115.525 -> 112.911
2024-12-02-03:42:49-root-INFO: Regularization Change: 0.000 -> 1.518
2024-12-02-03:42:49-root-INFO: Undo step: 75
2024-12-02-03:42:49-root-INFO: Undo step: 76
2024-12-02-03:42:49-root-INFO: Undo step: 77
2024-12-02-03:42:49-root-INFO: Undo step: 78
2024-12-02-03:42:49-root-INFO: Undo step: 79
2024-12-02-03:42:50-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-03:42:50-root-INFO: grad norm: 39.307 38.893 5.686
2024-12-02-03:42:51-root-INFO: grad norm: 19.802 19.542 3.197
2024-12-02-03:42:52-root-INFO: grad norm: 13.940 13.698 2.587
2024-12-02-03:42:53-root-INFO: grad norm: 11.635 11.447 2.081
2024-12-02-03:42:54-root-INFO: grad norm: 10.320 10.171 1.747
2024-12-02-03:42:55-root-INFO: Loss Change: 271.741 -> 141.433
2024-12-02-03:42:55-root-INFO: Regularization Change: 0.000 -> 43.276
2024-12-02-03:42:55-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-03:42:55-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-03:42:55-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-03:42:55-root-INFO: grad norm: 10.873 10.745 1.663
2024-12-02-03:42:56-root-INFO: grad norm: 10.947 10.865 1.343
2024-12-02-03:42:57-root-INFO: grad norm: 10.354 10.276 1.269
2024-12-02-03:42:58-root-INFO: grad norm: 9.705 9.640 1.119
2024-12-02-03:42:59-root-INFO: grad norm: 9.539 9.474 1.116
2024-12-02-03:43:00-root-INFO: Loss Change: 141.475 -> 128.568
2024-12-02-03:43:00-root-INFO: Regularization Change: 0.000 -> 6.369
2024-12-02-03:43:00-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-03:43:00-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-03:43:01-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-03:43:01-root-INFO: grad norm: 9.475 9.429 0.928
2024-12-02-03:43:02-root-INFO: grad norm: 8.393 8.348 0.872
2024-12-02-03:43:03-root-INFO: grad norm: 7.814 7.773 0.803
2024-12-02-03:43:04-root-INFO: grad norm: 9.025 8.973 0.967
2024-12-02-03:43:05-root-INFO: grad norm: 8.086 8.039 0.878
2024-12-02-03:43:06-root-INFO: Loss Change: 128.324 -> 122.244
2024-12-02-03:43:06-root-INFO: Regularization Change: 0.000 -> 3.306
2024-12-02-03:43:06-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-03:43:06-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-03:43:06-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-03:43:06-root-INFO: grad norm: 8.796 8.712 1.212
2024-12-02-03:43:07-root-INFO: grad norm: 8.348 8.302 0.866
2024-12-02-03:43:08-root-INFO: grad norm: 8.213 8.167 0.866
2024-12-02-03:43:09-root-INFO: grad norm: 9.475 9.443 0.776
2024-12-02-03:43:10-root-INFO: Loss too large (119.225->119.333)! Learning rate decreased to 0.07600.
2024-12-02-03:43:11-root-INFO: grad norm: 5.709 5.670 0.661
2024-12-02-03:43:11-root-INFO: Loss Change: 122.407 -> 116.920
2024-12-02-03:43:11-root-INFO: Regularization Change: 0.000 -> 2.002
2024-12-02-03:43:11-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-03:43:11-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-03:43:12-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-03:43:12-root-INFO: grad norm: 3.974 3.896 0.781
2024-12-02-03:43:13-root-INFO: grad norm: 4.960 4.925 0.587
2024-12-02-03:43:14-root-INFO: grad norm: 4.486 4.448 0.582
2024-12-02-03:43:15-root-INFO: grad norm: 3.384 3.342 0.531
2024-12-02-03:43:16-root-INFO: grad norm: 4.162 4.124 0.559
2024-12-02-03:43:17-root-INFO: Loss Change: 116.637 -> 113.742
2024-12-02-03:43:17-root-INFO: Regularization Change: 0.000 -> 1.727
2024-12-02-03:43:17-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-03:43:17-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-03:43:17-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-03:43:17-root-INFO: grad norm: 9.969 9.894 1.225
2024-12-02-03:43:18-root-INFO: Loss too large (114.096->114.480)! Learning rate decreased to 0.07980.
2024-12-02-03:43:19-root-INFO: grad norm: 4.285 4.238 0.634
2024-12-02-03:43:20-root-INFO: grad norm: 3.943 3.907 0.532
2024-12-02-03:43:21-root-INFO: grad norm: 3.600 3.565 0.507
2024-12-02-03:43:22-root-INFO: grad norm: 4.284 4.249 0.550
2024-12-02-03:43:22-root-INFO: Loss Change: 114.096 -> 111.122
2024-12-02-03:43:22-root-INFO: Regularization Change: 0.000 -> 1.113
2024-12-02-03:43:22-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-03:43:22-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-03:43:23-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-03:43:23-root-INFO: grad norm: 4.288 4.240 0.636
2024-12-02-03:43:24-root-INFO: grad norm: 6.817 6.791 0.589
2024-12-02-03:43:24-root-INFO: Loss too large (110.567->110.902)! Learning rate decreased to 0.08174.
2024-12-02-03:43:25-root-INFO: Loss too large (110.567->110.636)! Learning rate decreased to 0.06539.
2024-12-02-03:43:26-root-INFO: grad norm: 4.050 4.016 0.524
2024-12-02-03:43:27-root-INFO: grad norm: 2.601 2.562 0.448
2024-12-02-03:43:28-root-INFO: grad norm: 2.642 2.605 0.437
2024-12-02-03:43:28-root-INFO: Loss Change: 111.062 -> 108.818
2024-12-02-03:43:28-root-INFO: Regularization Change: 0.000 -> 0.705
2024-12-02-03:43:28-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-03:43:28-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-03:43:29-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-03:43:29-root-INFO: grad norm: 3.320 3.240 0.721
2024-12-02-03:43:30-root-INFO: grad norm: 3.470 3.440 0.455
2024-12-02-03:43:31-root-INFO: grad norm: 4.245 4.216 0.498
2024-12-02-03:43:32-root-INFO: grad norm: 8.558 8.535 0.632
2024-12-02-03:43:32-root-INFO: Loss too large (107.959->108.518)! Learning rate decreased to 0.08372.
2024-12-02-03:43:33-root-INFO: Loss too large (107.959->108.201)! Learning rate decreased to 0.06698.
2024-12-02-03:43:34-root-INFO: grad norm: 3.852 3.819 0.503
2024-12-02-03:43:34-root-INFO: Loss Change: 108.854 -> 106.895
2024-12-02-03:43:34-root-INFO: Regularization Change: 0.000 -> 0.971
2024-12-02-03:43:34-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-03:43:34-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-03:43:35-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-03:43:35-root-INFO: grad norm: 4.377 4.317 0.723
2024-12-02-03:43:36-root-INFO: grad norm: 7.171 7.151 0.541
2024-12-02-03:43:36-root-INFO: Loss too large (106.602->107.017)! Learning rate decreased to 0.08574.
2024-12-02-03:43:37-root-INFO: Loss too large (106.602->106.704)! Learning rate decreased to 0.06859.
2024-12-02-03:43:38-root-INFO: grad norm: 3.950 3.921 0.480
2024-12-02-03:43:39-root-INFO: grad norm: 2.662 2.629 0.413
2024-12-02-03:43:40-root-INFO: grad norm: 2.841 2.811 0.409
2024-12-02-03:43:40-root-INFO: Loss Change: 106.902 -> 104.918
2024-12-02-03:43:40-root-INFO: Regularization Change: 0.000 -> 0.664
2024-12-02-03:43:40-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-03:43:40-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-03:43:41-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-03:43:41-root-INFO: grad norm: 3.057 3.017 0.494
2024-12-02-03:43:42-root-INFO: grad norm: 5.381 5.361 0.462
2024-12-02-03:43:42-root-INFO: Loss too large (104.691->105.107)! Learning rate decreased to 0.08779.
2024-12-02-03:43:43-root-INFO: Loss too large (104.691->104.846)! Learning rate decreased to 0.07023.
2024-12-02-03:43:44-root-INFO: grad norm: 4.081 4.053 0.477
2024-12-02-03:43:45-root-INFO: grad norm: 2.282 2.249 0.389
2024-12-02-03:43:46-root-INFO: grad norm: 2.400 2.369 0.385
2024-12-02-03:43:46-root-INFO: Loss Change: 104.964 -> 103.326
2024-12-02-03:43:46-root-INFO: Regularization Change: 0.000 -> 0.632
2024-12-02-03:43:46-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-03:43:46-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-03:43:47-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-03:43:47-root-INFO: grad norm: 5.326 5.231 0.999
2024-12-02-03:43:47-root-INFO: Loss too large (103.519->103.745)! Learning rate decreased to 0.08987.
2024-12-02-03:43:48-root-INFO: grad norm: 4.560 4.522 0.588
2024-12-02-03:43:49-root-INFO: grad norm: 3.862 3.821 0.566
2024-12-02-03:43:50-root-INFO: grad norm: 4.228 4.195 0.530
2024-12-02-03:43:51-root-INFO: grad norm: 5.567 5.536 0.584
2024-12-02-03:43:52-root-INFO: Loss too large (101.975->102.256)! Learning rate decreased to 0.07189.
2024-12-02-03:43:52-root-INFO: Loss too large (101.975->101.980)! Learning rate decreased to 0.05752.
2024-12-02-03:43:53-root-INFO: Loss Change: 103.519 -> 101.764
2024-12-02-03:43:53-root-INFO: Regularization Change: 0.000 -> 0.793
2024-12-02-03:43:53-root-INFO: Undo step: 70
2024-12-02-03:43:53-root-INFO: Undo step: 71
2024-12-02-03:43:53-root-INFO: Undo step: 72
2024-12-02-03:43:53-root-INFO: Undo step: 73
2024-12-02-03:43:53-root-INFO: Undo step: 74
2024-12-02-03:43:53-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-03:43:53-root-INFO: grad norm: 36.657 36.423 4.132
2024-12-02-03:43:54-root-INFO: grad norm: 20.014 19.829 2.714
2024-12-02-03:43:55-root-INFO: grad norm: 12.904 12.730 2.109
2024-12-02-03:43:56-root-INFO: grad norm: 9.324 9.176 1.655
2024-12-02-03:43:57-root-INFO: grad norm: 7.507 7.374 1.407
2024-12-02-03:43:58-root-INFO: Loss Change: 238.421 -> 127.788
2024-12-02-03:43:58-root-INFO: Regularization Change: 0.000 -> 42.243
2024-12-02-03:43:58-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-03:43:58-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-03:43:58-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-03:43:58-root-INFO: grad norm: 7.266 7.141 1.342
2024-12-02-03:43:59-root-INFO: grad norm: 6.122 6.020 1.109
2024-12-02-03:44:00-root-INFO: grad norm: 5.896 5.805 1.030
2024-12-02-03:44:01-root-INFO: grad norm: 5.527 5.441 0.972
2024-12-02-03:44:02-root-INFO: grad norm: 5.469 5.392 0.912
2024-12-02-03:44:03-root-INFO: Loss Change: 127.847 -> 116.457
2024-12-02-03:44:03-root-INFO: Regularization Change: 0.000 -> 5.797
2024-12-02-03:44:03-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-03:44:03-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-03:44:03-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-03:44:04-root-INFO: grad norm: 4.861 4.794 0.805
2024-12-02-03:44:05-root-INFO: grad norm: 4.501 4.440 0.741
2024-12-02-03:44:06-root-INFO: grad norm: 4.461 4.403 0.721
2024-12-02-03:44:07-root-INFO: grad norm: 4.739 4.687 0.702
2024-12-02-03:44:08-root-INFO: grad norm: 4.333 4.276 0.698
2024-12-02-03:44:08-root-INFO: Loss Change: 116.158 -> 110.330
2024-12-02-03:44:08-root-INFO: Regularization Change: 0.000 -> 2.957
2024-12-02-03:44:08-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-03:44:08-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-03:44:09-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-03:44:09-root-INFO: grad norm: 5.007 4.909 0.991
2024-12-02-03:44:10-root-INFO: grad norm: 4.821 4.761 0.757
2024-12-02-03:44:11-root-INFO: grad norm: 5.387 5.331 0.775
2024-12-02-03:44:12-root-INFO: grad norm: 4.461 4.403 0.712
2024-12-02-03:44:13-root-INFO: grad norm: 3.220 3.166 0.590
2024-12-02-03:44:14-root-INFO: Loss Change: 110.425 -> 106.396
2024-12-02-03:44:14-root-INFO: Regularization Change: 0.000 -> 2.087
2024-12-02-03:44:14-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-03:44:14-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-03:44:14-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-03:44:14-root-INFO: grad norm: 2.918 2.875 0.499
2024-12-02-03:44:15-root-INFO: grad norm: 3.005 2.966 0.479
2024-12-02-03:44:16-root-INFO: grad norm: 4.468 4.439 0.507
2024-12-02-03:44:17-root-INFO: grad norm: 3.866 3.827 0.546
2024-12-02-03:44:18-root-INFO: grad norm: 2.551 2.511 0.453
2024-12-02-03:44:19-root-INFO: Loss Change: 106.375 -> 103.431
2024-12-02-03:44:19-root-INFO: Regularization Change: 0.000 -> 1.641
2024-12-02-03:44:19-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-03:44:19-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-03:44:19-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-03:44:20-root-INFO: grad norm: 3.517 3.431 0.773
2024-12-02-03:44:21-root-INFO: grad norm: 3.188 3.158 0.438
2024-12-02-03:44:22-root-INFO: grad norm: 3.566 3.535 0.468
2024-12-02-03:44:23-root-INFO: grad norm: 6.214 6.191 0.528
2024-12-02-03:44:23-root-INFO: Loss too large (101.915->102.187)! Learning rate decreased to 0.08987.
2024-12-02-03:44:23-root-INFO: Loss too large (101.915->101.930)! Learning rate decreased to 0.07189.
2024-12-02-03:44:24-root-INFO: grad norm: 3.821 3.789 0.496
2024-12-02-03:44:25-root-INFO: Loss Change: 103.312 -> 100.833
2024-12-02-03:44:25-root-INFO: Regularization Change: 0.000 -> 1.116
2024-12-02-03:44:25-root-INFO: Undo step: 70
2024-12-02-03:44:25-root-INFO: Undo step: 71
2024-12-02-03:44:25-root-INFO: Undo step: 72
2024-12-02-03:44:25-root-INFO: Undo step: 73
2024-12-02-03:44:25-root-INFO: Undo step: 74
2024-12-02-03:44:25-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-03:44:26-root-INFO: grad norm: 32.744 32.406 4.698
2024-12-02-03:44:27-root-INFO: grad norm: 19.674 19.420 3.152
2024-12-02-03:44:28-root-INFO: grad norm: 13.416 13.151 2.653
2024-12-02-03:44:29-root-INFO: grad norm: 10.035 9.843 1.950
2024-12-02-03:44:30-root-INFO: grad norm: 8.260 8.070 1.766
2024-12-02-03:44:30-root-INFO: Loss Change: 238.522 -> 129.579
2024-12-02-03:44:30-root-INFO: Regularization Change: 0.000 -> 42.582
2024-12-02-03:44:30-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-03:44:30-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-03:44:31-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-03:44:31-root-INFO: grad norm: 8.178 8.051 1.431
2024-12-02-03:44:32-root-INFO: grad norm: 7.262 7.120 1.428
2024-12-02-03:44:33-root-INFO: grad norm: 6.836 6.740 1.141
2024-12-02-03:44:34-root-INFO: grad norm: 6.428 6.312 1.216
2024-12-02-03:44:35-root-INFO: grad norm: 6.108 6.025 1.005
2024-12-02-03:44:36-root-INFO: Loss Change: 129.653 -> 117.280
2024-12-02-03:44:36-root-INFO: Regularization Change: 0.000 -> 6.622
2024-12-02-03:44:36-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-03:44:36-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-03:44:36-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-03:44:36-root-INFO: grad norm: 5.499 5.423 0.912
2024-12-02-03:44:37-root-INFO: grad norm: 6.121 6.058 0.877
2024-12-02-03:44:38-root-INFO: grad norm: 5.539 5.459 0.935
2024-12-02-03:44:39-root-INFO: grad norm: 4.639 4.577 0.755
2024-12-02-03:44:40-root-INFO: grad norm: 4.679 4.612 0.792
2024-12-02-03:44:41-root-INFO: Loss Change: 116.812 -> 110.563
2024-12-02-03:44:41-root-INFO: Regularization Change: 0.000 -> 3.428
2024-12-02-03:44:41-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-03:44:41-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-03:44:41-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-03:44:42-root-INFO: grad norm: 7.310 7.228 1.096
2024-12-02-03:44:42-root-INFO: grad norm: 6.772 6.697 1.005
2024-12-02-03:44:43-root-INFO: grad norm: 5.920 5.868 0.782
2024-12-02-03:44:44-root-INFO: grad norm: 5.902 5.848 0.796
2024-12-02-03:44:45-root-INFO: grad norm: 6.440 6.398 0.739
2024-12-02-03:44:46-root-INFO: Loss Change: 110.875 -> 106.882
2024-12-02-03:44:46-root-INFO: Regularization Change: 0.000 -> 2.679
2024-12-02-03:44:46-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-03:44:46-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-03:44:46-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-03:44:47-root-INFO: grad norm: 6.341 6.301 0.713
2024-12-02-03:44:48-root-INFO: grad norm: 7.136 7.098 0.734
2024-12-02-03:44:49-root-INFO: grad norm: 6.443 6.394 0.792
2024-12-02-03:44:50-root-INFO: grad norm: 5.658 5.625 0.612
2024-12-02-03:44:51-root-INFO: grad norm: 5.398 5.367 0.584
2024-12-02-03:44:51-root-INFO: Loss Change: 106.684 -> 103.079
2024-12-02-03:44:51-root-INFO: Regularization Change: 0.000 -> 2.103
2024-12-02-03:44:51-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-03:44:51-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-03:44:52-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-03:44:52-root-INFO: grad norm: 6.747 6.668 1.033
2024-12-02-03:44:53-root-INFO: grad norm: 6.344 6.296 0.783
2024-12-02-03:44:54-root-INFO: grad norm: 7.632 7.585 0.845
2024-12-02-03:44:54-root-INFO: Loss too large (102.054->102.174)! Learning rate decreased to 0.08987.
2024-12-02-03:44:55-root-INFO: grad norm: 4.986 4.939 0.688
2024-12-02-03:44:56-root-INFO: grad norm: 2.932 2.900 0.437
2024-12-02-03:44:57-root-INFO: Loss Change: 103.329 -> 99.630
2024-12-02-03:44:57-root-INFO: Regularization Change: 0.000 -> 1.432
2024-12-02-03:44:57-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-03:44:57-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-03:44:57-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-03:44:58-root-INFO: grad norm: 2.781 2.737 0.494
2024-12-02-03:44:59-root-INFO: grad norm: 2.966 2.932 0.447
2024-12-02-03:45:00-root-INFO: grad norm: 5.140 5.112 0.527
2024-12-02-03:45:00-root-INFO: Loss too large (98.873->99.148)! Learning rate decreased to 0.09199.
2024-12-02-03:45:00-root-INFO: Loss too large (98.873->98.888)! Learning rate decreased to 0.07359.
2024-12-02-03:45:01-root-INFO: grad norm: 3.879 3.845 0.514
2024-12-02-03:45:02-root-INFO: grad norm: 2.314 2.284 0.369
2024-12-02-03:45:03-root-INFO: Loss Change: 99.697 -> 97.665
2024-12-02-03:45:03-root-INFO: Regularization Change: 0.000 -> 0.905
2024-12-02-03:45:03-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-03:45:03-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-03:45:03-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-03:45:04-root-INFO: grad norm: 3.426 3.347 0.734
2024-12-02-03:45:05-root-INFO: grad norm: 3.008 2.967 0.493
2024-12-02-03:45:06-root-INFO: grad norm: 4.285 4.251 0.540
2024-12-02-03:45:06-root-INFO: Loss too large (96.560->96.733)! Learning rate decreased to 0.09414.
2024-12-02-03:45:07-root-INFO: grad norm: 4.150 4.113 0.552
2024-12-02-03:45:08-root-INFO: grad norm: 4.083 4.056 0.471
2024-12-02-03:45:09-root-INFO: Loss Change: 97.607 -> 95.763
2024-12-02-03:45:09-root-INFO: Regularization Change: 0.000 -> 1.191
2024-12-02-03:45:09-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-03:45:09-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-03:45:09-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-03:45:09-root-INFO: grad norm: 4.075 4.042 0.513
2024-12-02-03:45:10-root-INFO: grad norm: 6.468 6.443 0.572
2024-12-02-03:45:11-root-INFO: Loss too large (95.168->95.709)! Learning rate decreased to 0.09633.
2024-12-02-03:45:11-root-INFO: Loss too large (95.168->95.368)! Learning rate decreased to 0.07706.
2024-12-02-03:45:12-root-INFO: grad norm: 3.856 3.824 0.492
2024-12-02-03:45:13-root-INFO: grad norm: 2.266 2.241 0.339
2024-12-02-03:45:14-root-INFO: grad norm: 2.178 2.154 0.322
2024-12-02-03:45:15-root-INFO: Loss Change: 95.641 -> 93.558
2024-12-02-03:45:15-root-INFO: Regularization Change: 0.000 -> 0.720
2024-12-02-03:45:15-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-03:45:15-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-03:45:15-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-03:45:15-root-INFO: grad norm: 2.380 2.338 0.444
2024-12-02-03:45:16-root-INFO: grad norm: 2.160 2.135 0.330
2024-12-02-03:45:17-root-INFO: grad norm: 2.450 2.426 0.339
2024-12-02-03:45:18-root-INFO: grad norm: 4.655 4.637 0.407
2024-12-02-03:45:18-root-INFO: Loss too large (92.488->92.915)! Learning rate decreased to 0.09855.
2024-12-02-03:45:19-root-INFO: Loss too large (92.488->92.653)! Learning rate decreased to 0.07884.
2024-12-02-03:45:20-root-INFO: grad norm: 3.866 3.838 0.457
2024-12-02-03:45:20-root-INFO: Loss Change: 93.589 -> 91.776
2024-12-02-03:45:20-root-INFO: Regularization Change: 0.000 -> 0.961
2024-12-02-03:45:20-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-03:45:20-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-03:45:21-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-03:45:21-root-INFO: grad norm: 4.543 4.476 0.773
2024-12-02-03:45:21-root-INFO: Loss too large (91.988->92.142)! Learning rate decreased to 0.10081.
2024-12-02-03:45:22-root-INFO: grad norm: 4.209 4.175 0.535
2024-12-02-03:45:23-root-INFO: grad norm: 4.507 4.479 0.500
2024-12-02-03:45:24-root-INFO: Loss too large (91.175->91.307)! Learning rate decreased to 0.08065.
2024-12-02-03:45:25-root-INFO: grad norm: 3.806 3.779 0.454
2024-12-02-03:45:25-root-INFO: grad norm: 2.734 2.713 0.340
2024-12-02-03:45:26-root-INFO: Loss Change: 91.988 -> 90.275
2024-12-02-03:45:26-root-INFO: Regularization Change: 0.000 -> 0.723
2024-12-02-03:45:26-root-INFO: Undo step: 65
2024-12-02-03:45:26-root-INFO: Undo step: 66
2024-12-02-03:45:26-root-INFO: Undo step: 67
2024-12-02-03:45:26-root-INFO: Undo step: 68
2024-12-02-03:45:26-root-INFO: Undo step: 69
2024-12-02-03:45:27-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-03:45:27-root-INFO: grad norm: 33.846 33.543 4.517
2024-12-02-03:45:28-root-INFO: grad norm: 18.240 18.047 2.651
2024-12-02-03:45:29-root-INFO: grad norm: 10.813 10.644 1.901
2024-12-02-03:45:30-root-INFO: grad norm: 8.035 7.896 1.489
2024-12-02-03:45:31-root-INFO: grad norm: 6.599 6.478 1.258
2024-12-02-03:45:32-root-INFO: Loss Change: 222.478 -> 115.531
2024-12-02-03:45:32-root-INFO: Regularization Change: 0.000 -> 45.019
2024-12-02-03:45:32-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-03:45:32-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-03:45:32-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-03:45:32-root-INFO: grad norm: 6.517 6.407 1.191
2024-12-02-03:45:33-root-INFO: grad norm: 5.777 5.686 1.017
2024-12-02-03:45:34-root-INFO: grad norm: 5.139 5.053 0.931
2024-12-02-03:45:35-root-INFO: grad norm: 4.952 4.877 0.859
2024-12-02-03:45:36-root-INFO: grad norm: 5.275 5.209 0.835
2024-12-02-03:45:37-root-INFO: Loss Change: 115.660 -> 105.606
2024-12-02-03:45:37-root-INFO: Regularization Change: 0.000 -> 6.241
2024-12-02-03:45:37-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-03:45:37-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-03:45:37-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-03:45:38-root-INFO: grad norm: 4.885 4.819 0.805
2024-12-02-03:45:38-root-INFO: grad norm: 4.059 4.005 0.661
2024-12-02-03:45:39-root-INFO: grad norm: 4.113 4.063 0.639
2024-12-02-03:45:40-root-INFO: grad norm: 4.872 4.828 0.647
2024-12-02-03:45:41-root-INFO: grad norm: 4.236 4.189 0.630
2024-12-02-03:45:42-root-INFO: Loss Change: 105.166 -> 99.284
2024-12-02-03:45:42-root-INFO: Regularization Change: 0.000 -> 3.288
2024-12-02-03:45:42-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-03:45:42-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-03:45:42-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-03:45:43-root-INFO: grad norm: 3.813 3.735 0.766
2024-12-02-03:45:44-root-INFO: grad norm: 3.778 3.726 0.626
2024-12-02-03:45:45-root-INFO: grad norm: 5.019 4.973 0.678
2024-12-02-03:45:46-root-INFO: grad norm: 4.279 4.231 0.640
2024-12-02-03:45:47-root-INFO: grad norm: 2.977 2.928 0.539
2024-12-02-03:45:47-root-INFO: Loss Change: 99.252 -> 95.406
2024-12-02-03:45:47-root-INFO: Regularization Change: 0.000 -> 2.327
2024-12-02-03:45:47-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-03:45:47-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-03:45:48-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-03:45:48-root-INFO: grad norm: 2.693 2.647 0.496
2024-12-02-03:45:49-root-INFO: grad norm: 2.507 2.467 0.446
2024-12-02-03:45:50-root-INFO: grad norm: 2.693 2.656 0.445
2024-12-02-03:45:51-root-INFO: grad norm: 3.768 3.737 0.482
2024-12-02-03:45:52-root-INFO: grad norm: 3.955 3.921 0.517
2024-12-02-03:45:53-root-INFO: Loss Change: 95.367 -> 92.602
2024-12-02-03:45:53-root-INFO: Regularization Change: 0.000 -> 1.879
2024-12-02-03:45:53-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-03:45:53-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-03:45:53-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-03:45:53-root-INFO: grad norm: 6.116 6.042 0.949
2024-12-02-03:45:54-root-INFO: Loss too large (92.903->92.961)! Learning rate decreased to 0.10081.
2024-12-02-03:45:55-root-INFO: grad norm: 4.133 4.094 0.562
2024-12-02-03:45:56-root-INFO: grad norm: 2.296 2.259 0.411
2024-12-02-03:45:57-root-INFO: grad norm: 2.062 2.029 0.369
2024-12-02-03:45:58-root-INFO: grad norm: 1.983 1.950 0.359
2024-12-02-03:45:58-root-INFO: Loss Change: 92.903 -> 90.104
2024-12-02-03:45:58-root-INFO: Regularization Change: 0.000 -> 1.136
2024-12-02-03:45:58-root-INFO: Undo step: 65
2024-12-02-03:45:58-root-INFO: Undo step: 66
2024-12-02-03:45:58-root-INFO: Undo step: 67
2024-12-02-03:45:58-root-INFO: Undo step: 68
2024-12-02-03:45:59-root-INFO: Undo step: 69
2024-12-02-03:45:59-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-03:45:59-root-INFO: grad norm: 32.296 31.998 4.379
2024-12-02-03:46:00-root-INFO: grad norm: 16.078 15.787 3.045
2024-12-02-03:46:01-root-INFO: grad norm: 10.427 10.264 1.838
2024-12-02-03:46:02-root-INFO: grad norm: 8.022 7.883 1.482
2024-12-02-03:46:03-root-INFO: grad norm: 6.664 6.541 1.271
2024-12-02-03:46:04-root-INFO: Loss Change: 227.126 -> 115.007
2024-12-02-03:46:04-root-INFO: Regularization Change: 0.000 -> 49.407
2024-12-02-03:46:04-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-03:46:04-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-03:46:04-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-03:46:05-root-INFO: grad norm: 6.153 6.047 1.140
2024-12-02-03:46:06-root-INFO: grad norm: 5.687 5.595 1.019
2024-12-02-03:46:07-root-INFO: grad norm: 5.700 5.622 0.939
2024-12-02-03:46:08-root-INFO: grad norm: 5.694 5.620 0.915
2024-12-02-03:46:09-root-INFO: grad norm: 6.197 6.133 0.882
2024-12-02-03:46:09-root-INFO: Loss Change: 115.007 -> 104.420
2024-12-02-03:46:09-root-INFO: Regularization Change: 0.000 -> 6.814
2024-12-02-03:46:09-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-03:46:09-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-03:46:10-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-03:46:10-root-INFO: grad norm: 5.422 5.361 0.810
2024-12-02-03:46:11-root-INFO: grad norm: 4.661 4.609 0.695
2024-12-02-03:46:12-root-INFO: grad norm: 4.785 4.738 0.673
2024-12-02-03:46:13-root-INFO: grad norm: 5.674 5.631 0.701
2024-12-02-03:46:14-root-INFO: grad norm: 5.150 5.103 0.692
2024-12-02-03:46:15-root-INFO: Loss Change: 103.793 -> 97.731
2024-12-02-03:46:15-root-INFO: Regularization Change: 0.000 -> 3.538
2024-12-02-03:46:15-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-03:46:15-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-03:46:15-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-03:46:15-root-INFO: grad norm: 4.858 4.782 0.859
2024-12-02-03:46:16-root-INFO: grad norm: 4.865 4.818 0.672
2024-12-02-03:46:17-root-INFO: grad norm: 5.977 5.930 0.744
2024-12-02-03:46:18-root-INFO: grad norm: 5.435 5.388 0.710
2024-12-02-03:46:19-root-INFO: grad norm: 4.402 4.359 0.614
2024-12-02-03:46:20-root-INFO: Loss Change: 97.712 -> 93.824
2024-12-02-03:46:20-root-INFO: Regularization Change: 0.000 -> 2.498
2024-12-02-03:46:20-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-03:46:20-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-03:46:20-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-03:46:20-root-INFO: grad norm: 4.091 4.059 0.514
2024-12-02-03:46:21-root-INFO: grad norm: 4.085 4.054 0.506
2024-12-02-03:46:22-root-INFO: grad norm: 4.318 4.289 0.505
2024-12-02-03:46:23-root-INFO: grad norm: 5.761 5.730 0.603
2024-12-02-03:46:24-root-INFO: Loss too large (92.142->92.182)! Learning rate decreased to 0.09855.
2024-12-02-03:46:25-root-INFO: grad norm: 4.236 4.203 0.535
2024-12-02-03:46:25-root-INFO: Loss Change: 93.714 -> 90.714
2024-12-02-03:46:25-root-INFO: Regularization Change: 0.000 -> 1.633
2024-12-02-03:46:25-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-03:46:25-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-03:46:26-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-03:46:26-root-INFO: grad norm: 4.000 3.913 0.829
2024-12-02-03:46:27-root-INFO: grad norm: 4.079 4.037 0.585
2024-12-02-03:46:28-root-INFO: grad norm: 5.864 5.823 0.692
2024-12-02-03:46:28-root-INFO: Loss too large (89.941->90.197)! Learning rate decreased to 0.10081.
2024-12-02-03:46:29-root-INFO: grad norm: 4.100 4.063 0.544
2024-12-02-03:46:30-root-INFO: grad norm: 2.235 2.202 0.386
2024-12-02-03:46:31-root-INFO: Loss Change: 90.841 -> 88.216
2024-12-02-03:46:31-root-INFO: Regularization Change: 0.000 -> 1.330
2024-12-02-03:46:31-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-03:46:31-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-03:46:31-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-03:46:32-root-INFO: grad norm: 2.532 2.482 0.497
2024-12-02-03:46:33-root-INFO: grad norm: 2.438 2.408 0.384
2024-12-02-03:46:34-root-INFO: grad norm: 3.002 2.973 0.418
2024-12-02-03:46:35-root-INFO: grad norm: 3.665 3.637 0.458
2024-12-02-03:46:36-root-INFO: grad norm: 5.858 5.829 0.590
2024-12-02-03:46:36-root-INFO: Loss too large (86.740->87.169)! Learning rate decreased to 0.10310.
2024-12-02-03:46:36-root-INFO: Loss too large (86.740->86.764)! Learning rate decreased to 0.08248.
2024-12-02-03:46:37-root-INFO: Loss Change: 88.149 -> 86.445
2024-12-02-03:46:37-root-INFO: Regularization Change: 0.000 -> 1.372
2024-12-02-03:46:37-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-03:46:37-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-03:46:37-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-03:46:38-root-INFO: grad norm: 3.829 3.796 0.499
2024-12-02-03:46:39-root-INFO: grad norm: 4.490 4.467 0.455
2024-12-02-03:46:39-root-INFO: Loss too large (85.797->85.919)! Learning rate decreased to 0.10543.
2024-12-02-03:46:40-root-INFO: grad norm: 3.648 3.623 0.430
2024-12-02-03:46:41-root-INFO: grad norm: 2.386 2.358 0.365
2024-12-02-03:46:42-root-INFO: grad norm: 2.465 2.440 0.349
2024-12-02-03:46:43-root-INFO: Loss Change: 86.466 -> 84.223
2024-12-02-03:46:43-root-INFO: Regularization Change: 0.000 -> 1.122
2024-12-02-03:46:43-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-03:46:43-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-03:46:43-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-03:46:43-root-INFO: grad norm: 4.309 4.237 0.787
2024-12-02-03:46:44-root-INFO: grad norm: 4.234 4.194 0.581
2024-12-02-03:46:45-root-INFO: grad norm: 4.510 4.470 0.596
2024-12-02-03:46:46-root-INFO: Loss too large (83.347->83.514)! Learning rate decreased to 0.10779.
2024-12-02-03:46:47-root-INFO: grad norm: 3.794 3.763 0.487
2024-12-02-03:46:48-root-INFO: grad norm: 2.810 2.783 0.392
2024-12-02-03:46:48-root-INFO: Loss Change: 84.223 -> 82.199
2024-12-02-03:46:48-root-INFO: Regularization Change: 0.000 -> 1.185
2024-12-02-03:46:48-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-03:46:48-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-03:46:49-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-03:46:49-root-INFO: grad norm: 2.702 2.675 0.380
2024-12-02-03:46:50-root-INFO: grad norm: 3.932 3.911 0.399
2024-12-02-03:46:50-root-INFO: Loss too large (81.829->81.960)! Learning rate decreased to 0.11019.
2024-12-02-03:46:51-root-INFO: grad norm: 3.507 3.483 0.402
2024-12-02-03:46:52-root-INFO: grad norm: 2.746 2.722 0.359
2024-12-02-03:46:53-root-INFO: grad norm: 2.858 2.836 0.354
2024-12-02-03:46:54-root-INFO: Loss Change: 82.146 -> 80.521
2024-12-02-03:46:54-root-INFO: Regularization Change: 0.000 -> 0.998
2024-12-02-03:46:54-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-03:46:54-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-03:46:54-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-03:46:55-root-INFO: grad norm: 4.385 4.326 0.720
2024-12-02-03:46:55-root-INFO: Loss too large (80.706->80.823)! Learning rate decreased to 0.11263.
2024-12-02-03:46:56-root-INFO: grad norm: 3.695 3.668 0.447
2024-12-02-03:46:57-root-INFO: grad norm: 2.869 2.842 0.394
2024-12-02-03:46:58-root-INFO: grad norm: 2.991 2.969 0.367
2024-12-02-03:46:59-root-INFO: grad norm: 3.383 3.362 0.381
2024-12-02-03:47:00-root-INFO: Loss Change: 80.706 -> 79.170
2024-12-02-03:47:00-root-INFO: Regularization Change: 0.000 -> 1.005
2024-12-02-03:47:00-root-INFO: Undo step: 60
2024-12-02-03:47:00-root-INFO: Undo step: 61
2024-12-02-03:47:00-root-INFO: Undo step: 62
2024-12-02-03:47:00-root-INFO: Undo step: 63
2024-12-02-03:47:00-root-INFO: Undo step: 64
2024-12-02-03:47:00-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-03:47:00-root-INFO: grad norm: 27.386 27.211 3.086
2024-12-02-03:47:01-root-INFO: grad norm: 16.381 16.252 2.057
2024-12-02-03:47:02-root-INFO: grad norm: 10.107 9.964 1.695
2024-12-02-03:47:03-root-INFO: grad norm: 7.799 7.693 1.279
2024-12-02-03:47:04-root-INFO: grad norm: 6.611 6.512 1.142
2024-12-02-03:47:05-root-INFO: Loss Change: 207.139 -> 102.291
2024-12-02-03:47:05-root-INFO: Regularization Change: 0.000 -> 52.677
2024-12-02-03:47:05-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-03:47:05-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-03:47:05-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-03:47:05-root-INFO: grad norm: 6.341 6.245 1.098
2024-12-02-03:47:06-root-INFO: grad norm: 5.930 5.848 0.981
2024-12-02-03:47:07-root-INFO: grad norm: 6.200 6.133 0.908
2024-12-02-03:47:08-root-INFO: grad norm: 6.019 5.951 0.901
2024-12-02-03:47:09-root-INFO: grad norm: 5.754 5.696 0.813
2024-12-02-03:47:10-root-INFO: Loss Change: 102.354 -> 92.671
2024-12-02-03:47:10-root-INFO: Regularization Change: 0.000 -> 6.891
2024-12-02-03:47:10-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-03:47:10-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-03:47:10-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-03:47:11-root-INFO: grad norm: 5.447 5.405 0.675
2024-12-02-03:47:12-root-INFO: grad norm: 5.713 5.669 0.707
2024-12-02-03:47:13-root-INFO: grad norm: 5.496 5.452 0.696
2024-12-02-03:47:14-root-INFO: grad norm: 5.086 5.043 0.657
2024-12-02-03:47:15-root-INFO: grad norm: 5.100 5.057 0.661
2024-12-02-03:47:15-root-INFO: Loss Change: 92.454 -> 87.592
2024-12-02-03:47:15-root-INFO: Regularization Change: 0.000 -> 3.521
2024-12-02-03:47:15-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-03:47:15-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-03:47:16-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-03:47:16-root-INFO: grad norm: 7.098 7.004 1.150
2024-12-02-03:47:17-root-INFO: grad norm: 5.879 5.812 0.883
2024-12-02-03:47:18-root-INFO: grad norm: 4.518 4.467 0.679
2024-12-02-03:47:19-root-INFO: grad norm: 4.094 4.050 0.597
2024-12-02-03:47:20-root-INFO: grad norm: 3.973 3.932 0.568
2024-12-02-03:47:21-root-INFO: Loss Change: 87.855 -> 83.719
2024-12-02-03:47:21-root-INFO: Regularization Change: 0.000 -> 2.643
2024-12-02-03:47:21-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-03:47:21-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-03:47:21-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-03:47:21-root-INFO: grad norm: 3.627 3.597 0.468
2024-12-02-03:47:22-root-INFO: grad norm: 4.161 4.128 0.521
2024-12-02-03:47:23-root-INFO: grad norm: 4.481 4.445 0.572
2024-12-02-03:47:24-root-INFO: grad norm: 5.225 5.187 0.625
2024-12-02-03:47:25-root-INFO: Loss too large (82.140->82.218)! Learning rate decreased to 0.11019.
2024-12-02-03:47:26-root-INFO: grad norm: 4.123 4.087 0.544
2024-12-02-03:47:26-root-INFO: Loss Change: 83.532 -> 80.898
2024-12-02-03:47:26-root-INFO: Regularization Change: 0.000 -> 1.662
2024-12-02-03:47:26-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-03:47:26-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-03:47:27-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-03:47:27-root-INFO: grad norm: 4.160 4.085 0.790
2024-12-02-03:47:28-root-INFO: grad norm: 4.371 4.325 0.629
2024-12-02-03:47:29-root-INFO: grad norm: 5.442 5.397 0.697
2024-12-02-03:47:29-root-INFO: Loss too large (80.343->80.564)! Learning rate decreased to 0.11263.
2024-12-02-03:47:30-root-INFO: grad norm: 4.160 4.120 0.570
2024-12-02-03:47:31-root-INFO: grad norm: 2.599 2.569 0.391
2024-12-02-03:47:32-root-INFO: Loss Change: 81.124 -> 78.724
2024-12-02-03:47:32-root-INFO: Regularization Change: 0.000 -> 1.392
2024-12-02-03:47:32-root-INFO: Undo step: 60
2024-12-02-03:47:32-root-INFO: Undo step: 61
2024-12-02-03:47:32-root-INFO: Undo step: 62
2024-12-02-03:47:32-root-INFO: Undo step: 63
2024-12-02-03:47:32-root-INFO: Undo step: 64
2024-12-02-03:47:32-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-03:47:33-root-INFO: grad norm: 27.711 27.495 3.458
2024-12-02-03:47:34-root-INFO: grad norm: 14.000 13.841 2.104
2024-12-02-03:47:35-root-INFO: grad norm: 9.438 9.310 1.554
2024-12-02-03:47:36-root-INFO: grad norm: 7.327 7.222 1.238
2024-12-02-03:47:37-root-INFO: grad norm: 6.464 6.381 1.034
2024-12-02-03:47:37-root-INFO: Loss Change: 202.828 -> 103.283
2024-12-02-03:47:37-root-INFO: Regularization Change: 0.000 -> 48.417
2024-12-02-03:47:37-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-03:47:37-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-03:47:38-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-03:47:38-root-INFO: grad norm: 5.950 5.875 0.941
2024-12-02-03:47:39-root-INFO: grad norm: 5.384 5.326 0.789
2024-12-02-03:47:40-root-INFO: grad norm: 5.010 4.951 0.767
2024-12-02-03:47:41-root-INFO: grad norm: 4.878 4.832 0.668
2024-12-02-03:47:42-root-INFO: grad norm: 4.648 4.596 0.688
2024-12-02-03:47:43-root-INFO: Loss Change: 103.127 -> 92.509
2024-12-02-03:47:43-root-INFO: Regularization Change: 0.000 -> 6.872
2024-12-02-03:47:43-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-03:47:43-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-03:47:43-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-03:47:43-root-INFO: grad norm: 5.112 5.045 0.829
2024-12-02-03:47:44-root-INFO: grad norm: 4.718 4.667 0.690
2024-12-02-03:47:45-root-INFO: grad norm: 4.176 4.131 0.613
2024-12-02-03:47:46-root-INFO: grad norm: 4.303 4.257 0.626
2024-12-02-03:47:47-root-INFO: grad norm: 5.235 5.199 0.616
2024-12-02-03:47:48-root-INFO: Loss Change: 92.630 -> 87.976
2024-12-02-03:47:48-root-INFO: Regularization Change: 0.000 -> 3.849
2024-12-02-03:47:48-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-03:47:48-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-03:47:48-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-03:47:48-root-INFO: grad norm: 4.539 4.497 0.610
2024-12-02-03:47:49-root-INFO: grad norm: 3.431 3.397 0.478
2024-12-02-03:47:50-root-INFO: grad norm: 3.335 3.302 0.473
2024-12-02-03:47:51-root-INFO: grad norm: 3.565 3.538 0.439
2024-12-02-03:47:52-root-INFO: grad norm: 4.007 3.976 0.495
2024-12-02-03:47:53-root-INFO: Loss Change: 87.563 -> 83.495
2024-12-02-03:47:53-root-INFO: Regularization Change: 0.000 -> 2.901
2024-12-02-03:47:53-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-03:47:53-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-03:47:53-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-03:47:54-root-INFO: grad norm: 6.284 6.244 0.710
2024-12-02-03:47:54-root-INFO: Loss too large (83.596->83.636)! Learning rate decreased to 0.11019.
2024-12-02-03:47:55-root-INFO: grad norm: 4.061 4.028 0.516
2024-12-02-03:47:56-root-INFO: grad norm: 2.606 2.576 0.392
2024-12-02-03:47:57-root-INFO: grad norm: 2.282 2.253 0.363
2024-12-02-03:47:58-root-INFO: grad norm: 2.205 2.176 0.357
2024-12-02-03:47:59-root-INFO: Loss Change: 83.596 -> 80.465
2024-12-02-03:47:59-root-INFO: Regularization Change: 0.000 -> 1.394
2024-12-02-03:47:59-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-03:47:59-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-03:47:59-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-03:47:59-root-INFO: grad norm: 3.171 3.110 0.620
2024-12-02-03:48:00-root-INFO: grad norm: 3.456 3.422 0.479
2024-12-02-03:48:01-root-INFO: grad norm: 5.301 5.272 0.555
2024-12-02-03:48:02-root-INFO: Loss too large (79.722->79.955)! Learning rate decreased to 0.11263.
2024-12-02-03:48:03-root-INFO: grad norm: 3.827 3.796 0.488
2024-12-02-03:48:04-root-INFO: grad norm: 2.226 2.198 0.351
2024-12-02-03:48:04-root-INFO: Loss Change: 80.538 -> 78.068
2024-12-02-03:48:04-root-INFO: Regularization Change: 0.000 -> 1.474
2024-12-02-03:48:04-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-03:48:04-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-03:48:05-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-03:48:05-root-INFO: grad norm: 2.492 2.435 0.531
2024-12-02-03:48:06-root-INFO: grad norm: 2.321 2.294 0.353
2024-12-02-03:48:07-root-INFO: grad norm: 3.127 3.104 0.378
2024-12-02-03:48:08-root-INFO: grad norm: 3.556 3.531 0.421
2024-12-02-03:48:09-root-INFO: grad norm: 4.672 4.649 0.455
2024-12-02-03:48:09-root-INFO: Loss too large (76.414->76.632)! Learning rate decreased to 0.11510.
2024-12-02-03:48:10-root-INFO: Loss Change: 78.041 -> 76.317
2024-12-02-03:48:10-root-INFO: Regularization Change: 0.000 -> 1.700
2024-12-02-03:48:10-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-03:48:10-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-03:48:10-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-03:48:11-root-INFO: grad norm: 3.745 3.719 0.438
2024-12-02-03:48:12-root-INFO: grad norm: 3.613 3.595 0.368
2024-12-02-03:48:13-root-INFO: grad norm: 3.936 3.914 0.414
2024-12-02-03:48:14-root-INFO: grad norm: 4.450 4.430 0.419
2024-12-02-03:48:14-root-INFO: Loss too large (74.645->74.704)! Learning rate decreased to 0.11760.
2024-12-02-03:48:15-root-INFO: grad norm: 3.658 3.635 0.412
2024-12-02-03:48:16-root-INFO: Loss Change: 76.141 -> 73.669
2024-12-02-03:48:16-root-INFO: Regularization Change: 0.000 -> 1.522
2024-12-02-03:48:16-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-03:48:16-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-03:48:16-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-03:48:16-root-INFO: grad norm: 3.892 3.839 0.640
2024-12-02-03:48:17-root-INFO: grad norm: 4.041 4.007 0.524
2024-12-02-03:48:18-root-INFO: grad norm: 4.737 4.708 0.527
2024-12-02-03:48:19-root-INFO: Loss too large (72.922->73.133)! Learning rate decreased to 0.12015.
2024-12-02-03:48:20-root-INFO: grad norm: 3.823 3.795 0.463
2024-12-02-03:48:21-root-INFO: grad norm: 2.656 2.635 0.331
2024-12-02-03:48:21-root-INFO: Loss Change: 73.627 -> 71.607
2024-12-02-03:48:21-root-INFO: Regularization Change: 0.000 -> 1.294
2024-12-02-03:48:21-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-03:48:21-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-03:48:22-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-03:48:22-root-INFO: grad norm: 2.823 2.788 0.439
2024-12-02-03:48:23-root-INFO: grad norm: 3.428 3.411 0.334
2024-12-02-03:48:24-root-INFO: grad norm: 3.978 3.958 0.398
2024-12-02-03:48:25-root-INFO: grad norm: 4.830 4.811 0.421
2024-12-02-03:48:25-root-INFO: Loss too large (70.848->71.002)! Learning rate decreased to 0.12272.
2024-12-02-03:48:26-root-INFO: grad norm: 3.700 3.678 0.406
2024-12-02-03:48:27-root-INFO: Loss Change: 71.624 -> 69.780
2024-12-02-03:48:27-root-INFO: Regularization Change: 0.000 -> 1.331
2024-12-02-03:48:27-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-03:48:27-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-03:48:27-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-03:48:28-root-INFO: grad norm: 3.516 3.459 0.628
2024-12-02-03:48:29-root-INFO: grad norm: 3.850 3.819 0.484
2024-12-02-03:48:30-root-INFO: grad norm: 5.247 5.220 0.531
2024-12-02-03:48:30-root-INFO: Loss too large (69.411->69.802)! Learning rate decreased to 0.12533.
2024-12-02-03:48:31-root-INFO: grad norm: 3.913 3.887 0.457
2024-12-02-03:48:32-root-INFO: grad norm: 2.330 2.311 0.298
2024-12-02-03:48:33-root-INFO: Loss Change: 69.867 -> 67.980
2024-12-02-03:48:33-root-INFO: Regularization Change: 0.000 -> 1.214
2024-12-02-03:48:33-root-INFO: Undo step: 55
2024-12-02-03:48:33-root-INFO: Undo step: 56
2024-12-02-03:48:33-root-INFO: Undo step: 57
2024-12-02-03:48:33-root-INFO: Undo step: 58
2024-12-02-03:48:33-root-INFO: Undo step: 59
2024-12-02-03:48:33-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-03:48:33-root-INFO: grad norm: 24.461 24.266 3.084
2024-12-02-03:48:34-root-INFO: grad norm: 14.850 14.739 1.814
2024-12-02-03:48:35-root-INFO: grad norm: 9.303 9.186 1.474
2024-12-02-03:48:36-root-INFO: grad norm: 7.225 7.133 1.146
2024-12-02-03:48:37-root-INFO: grad norm: 5.467 5.390 0.911
2024-12-02-03:48:38-root-INFO: Loss Change: 180.363 -> 90.327
2024-12-02-03:48:38-root-INFO: Regularization Change: 0.000 -> 49.350
2024-12-02-03:48:38-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-03:48:38-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-03:48:38-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-03:48:39-root-INFO: grad norm: 4.938 4.849 0.931
2024-12-02-03:48:40-root-INFO: grad norm: 4.163 4.103 0.703
2024-12-02-03:48:41-root-INFO: grad norm: 3.873 3.814 0.670
2024-12-02-03:48:42-root-INFO: grad norm: 4.062 4.015 0.614
2024-12-02-03:48:43-root-INFO: grad norm: 5.504 5.464 0.659
2024-12-02-03:48:43-root-INFO: Loss Change: 90.258 -> 82.144
2024-12-02-03:48:43-root-INFO: Regularization Change: 0.000 -> 6.837
2024-12-02-03:48:43-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-03:48:43-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-03:48:44-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-03:48:44-root-INFO: grad norm: 4.378 4.341 0.572
2024-12-02-03:48:45-root-INFO: grad norm: 2.859 2.818 0.483
2024-12-02-03:48:46-root-INFO: grad norm: 2.690 2.650 0.460
2024-12-02-03:48:47-root-INFO: grad norm: 2.781 2.746 0.441
2024-12-02-03:48:48-root-INFO: grad norm: 3.158 3.125 0.458
2024-12-02-03:48:49-root-INFO: Loss Change: 81.844 -> 76.498
2024-12-02-03:48:49-root-INFO: Regularization Change: 0.000 -> 3.794
2024-12-02-03:48:49-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-03:48:49-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-03:48:49-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-03:48:49-root-INFO: grad norm: 5.782 5.726 0.799
2024-12-02-03:48:50-root-INFO: Loss too large (76.548->76.562)! Learning rate decreased to 0.12015.
2024-12-02-03:48:51-root-INFO: grad norm: 4.394 4.359 0.552
2024-12-02-03:48:52-root-INFO: grad norm: 2.772 2.740 0.425
2024-12-02-03:48:53-root-INFO: grad norm: 2.715 2.686 0.396
2024-12-02-03:48:54-root-INFO: grad norm: 2.840 2.814 0.383
2024-12-02-03:48:54-root-INFO: Loss Change: 76.548 -> 73.330
2024-12-02-03:48:54-root-INFO: Regularization Change: 0.000 -> 1.778
2024-12-02-03:48:54-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-03:48:54-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-03:48:55-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-03:48:55-root-INFO: grad norm: 3.059 3.022 0.472
2024-12-02-03:48:56-root-INFO: grad norm: 4.037 4.018 0.397
2024-12-02-03:48:57-root-INFO: grad norm: 3.950 3.925 0.446
2024-12-02-03:48:58-root-INFO: grad norm: 3.726 3.701 0.428
2024-12-02-03:48:59-root-INFO: grad norm: 4.142 4.112 0.496
2024-12-02-03:49:00-root-INFO: Loss Change: 73.289 -> 70.976
2024-12-02-03:49:00-root-INFO: Regularization Change: 0.000 -> 2.240
2024-12-02-03:49:00-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-03:49:00-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-03:49:00-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-03:49:00-root-INFO: grad norm: 6.191 6.129 0.873
2024-12-02-03:49:01-root-INFO: Loss too large (71.227->71.533)! Learning rate decreased to 0.12533.
2024-12-02-03:49:02-root-INFO: grad norm: 4.349 4.316 0.539
2024-12-02-03:49:03-root-INFO: grad norm: 2.322 2.295 0.353
2024-12-02-03:49:04-root-INFO: grad norm: 2.159 2.136 0.317
2024-12-02-03:49:05-root-INFO: grad norm: 2.133 2.112 0.301
2024-12-02-03:49:05-root-INFO: Loss Change: 71.227 -> 68.445
2024-12-02-03:49:05-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-02-03:49:05-root-INFO: Undo step: 55
2024-12-02-03:49:05-root-INFO: Undo step: 56
2024-12-02-03:49:05-root-INFO: Undo step: 57
2024-12-02-03:49:05-root-INFO: Undo step: 58
2024-12-02-03:49:05-root-INFO: Undo step: 59
2024-12-02-03:49:06-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-03:49:06-root-INFO: grad norm: 24.088 23.828 3.526
2024-12-02-03:49:07-root-INFO: grad norm: 14.282 14.118 2.155
2024-12-02-03:49:08-root-INFO: grad norm: 10.942 10.754 2.020
2024-12-02-03:49:09-root-INFO: grad norm: 9.154 9.041 1.432
2024-12-02-03:49:10-root-INFO: grad norm: 8.132 7.978 1.576
2024-12-02-03:49:11-root-INFO: Loss Change: 185.672 -> 93.236
2024-12-02-03:49:11-root-INFO: Regularization Change: 0.000 -> 52.893
2024-12-02-03:49:11-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-03:49:11-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-03:49:11-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-03:49:11-root-INFO: grad norm: 7.209 7.115 1.156
2024-12-02-03:49:12-root-INFO: grad norm: 6.402 6.288 1.202
2024-12-02-03:49:13-root-INFO: grad norm: 5.796 5.723 0.917
2024-12-02-03:49:14-root-INFO: grad norm: 5.481 5.399 0.944
2024-12-02-03:49:15-root-INFO: grad norm: 5.263 5.200 0.811
2024-12-02-03:49:16-root-INFO: Loss Change: 93.070 -> 82.494
2024-12-02-03:49:16-root-INFO: Regularization Change: 0.000 -> 7.743
2024-12-02-03:49:16-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-03:49:16-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-03:49:16-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-03:49:17-root-INFO: grad norm: 6.114 6.034 0.983
2024-12-02-03:49:18-root-INFO: grad norm: 5.762 5.701 0.836
2024-12-02-03:49:19-root-INFO: grad norm: 5.175 5.119 0.759
2024-12-02-03:49:20-root-INFO: grad norm: 5.011 4.956 0.736
2024-12-02-03:49:21-root-INFO: grad norm: 4.878 4.831 0.675
2024-12-02-03:49:21-root-INFO: Loss Change: 82.454 -> 77.385
2024-12-02-03:49:21-root-INFO: Regularization Change: 0.000 -> 4.241
2024-12-02-03:49:21-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-03:49:21-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-03:49:22-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-03:49:22-root-INFO: grad norm: 4.650 4.605 0.645
2024-12-02-03:49:23-root-INFO: grad norm: 4.598 4.557 0.615
2024-12-02-03:49:24-root-INFO: grad norm: 4.584 4.540 0.633
2024-12-02-03:49:25-root-INFO: grad norm: 4.638 4.599 0.596
2024-12-02-03:49:26-root-INFO: grad norm: 4.579 4.536 0.631
2024-12-02-03:49:27-root-INFO: Loss Change: 76.954 -> 73.188
2024-12-02-03:49:27-root-INFO: Regularization Change: 0.000 -> 3.024
2024-12-02-03:49:27-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-03:49:27-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-03:49:27-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-03:49:27-root-INFO: grad norm: 5.239 5.172 0.835
2024-12-02-03:49:28-root-INFO: grad norm: 4.786 4.739 0.667
2024-12-02-03:49:29-root-INFO: grad norm: 4.182 4.144 0.561
2024-12-02-03:49:30-root-INFO: grad norm: 4.142 4.102 0.573
2024-12-02-03:49:31-root-INFO: grad norm: 4.221 4.189 0.519
2024-12-02-03:49:32-root-INFO: Loss Change: 73.352 -> 70.509
2024-12-02-03:49:32-root-INFO: Regularization Change: 0.000 -> 2.599
2024-12-02-03:49:32-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-03:49:32-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-03:49:32-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-03:49:32-root-INFO: grad norm: 4.027 3.996 0.497
2024-12-02-03:49:33-root-INFO: grad norm: 4.027 4.000 0.465
2024-12-02-03:49:34-root-INFO: grad norm: 4.108 4.076 0.511
2024-12-02-03:49:35-root-INFO: grad norm: 4.272 4.245 0.484
2024-12-02-03:49:36-root-INFO: grad norm: 4.307 4.273 0.539
2024-12-02-03:49:37-root-INFO: Loss Change: 70.262 -> 67.755
2024-12-02-03:49:37-root-INFO: Regularization Change: 0.000 -> 2.208
2024-12-02-03:49:37-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-03:49:37-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-03:49:37-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-03:49:38-root-INFO: grad norm: 5.047 4.987 0.773
2024-12-02-03:49:39-root-INFO: grad norm: 4.676 4.636 0.607
2024-12-02-03:49:40-root-INFO: grad norm: 4.166 4.134 0.518
2024-12-02-03:49:41-root-INFO: grad norm: 4.211 4.179 0.520
2024-12-02-03:49:42-root-INFO: grad norm: 4.344 4.317 0.480
2024-12-02-03:49:42-root-INFO: Loss Change: 67.789 -> 65.706
2024-12-02-03:49:42-root-INFO: Regularization Change: 0.000 -> 2.190
2024-12-02-03:49:42-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-03:49:42-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-03:49:43-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-03:49:43-root-INFO: grad norm: 4.386 4.361 0.470
2024-12-02-03:49:44-root-INFO: grad norm: 4.349 4.326 0.447
2024-12-02-03:49:45-root-INFO: grad norm: 4.330 4.303 0.489
2024-12-02-03:49:46-root-INFO: grad norm: 4.439 4.414 0.467
2024-12-02-03:49:47-root-INFO: grad norm: 4.435 4.405 0.520
2024-12-02-03:49:48-root-INFO: Loss Change: 65.603 -> 63.521
2024-12-02-03:49:48-root-INFO: Regularization Change: 0.000 -> 2.015
2024-12-02-03:49:48-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-03:49:48-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-03:49:48-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-03:49:48-root-INFO: grad norm: 4.971 4.924 0.680
2024-12-02-03:49:49-root-INFO: grad norm: 4.620 4.585 0.570
2024-12-02-03:49:50-root-INFO: grad norm: 4.170 4.142 0.477
2024-12-02-03:49:51-root-INFO: grad norm: 4.171 4.140 0.503
2024-12-02-03:49:52-root-INFO: grad norm: 4.249 4.223 0.465
2024-12-02-03:49:53-root-INFO: Loss Change: 63.590 -> 61.760
2024-12-02-03:49:53-root-INFO: Regularization Change: 0.000 -> 2.025
2024-12-02-03:49:53-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-03:49:53-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-03:49:53-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-03:49:54-root-INFO: grad norm: 4.512 4.478 0.555
2024-12-02-03:49:55-root-INFO: grad norm: 4.495 4.463 0.535
2024-12-02-03:49:56-root-INFO: grad norm: 4.544 4.512 0.534
2024-12-02-03:49:57-root-INFO: grad norm: 4.653 4.617 0.576
2024-12-02-03:49:58-root-INFO: grad norm: 4.656 4.622 0.561
2024-12-02-03:49:58-root-INFO: Loss Change: 61.675 -> 59.874
2024-12-02-03:49:58-root-INFO: Regularization Change: 0.000 -> 1.991
2024-12-02-03:49:58-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-03:49:58-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-03:49:59-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-03:49:59-root-INFO: grad norm: 5.212 5.156 0.759
2024-12-02-03:50:00-root-INFO: grad norm: 4.812 4.775 0.600
2024-12-02-03:50:01-root-INFO: grad norm: 4.297 4.261 0.559
2024-12-02-03:50:02-root-INFO: grad norm: 4.240 4.207 0.526
2024-12-02-03:50:03-root-INFO: grad norm: 4.274 4.242 0.519
2024-12-02-03:50:04-root-INFO: Loss Change: 59.745 -> 57.935
2024-12-02-03:50:04-root-INFO: Regularization Change: 0.000 -> 2.042
2024-12-02-03:50:04-root-INFO: Undo step: 50
2024-12-02-03:50:04-root-INFO: Undo step: 51
2024-12-02-03:50:04-root-INFO: Undo step: 52
2024-12-02-03:50:04-root-INFO: Undo step: 53
2024-12-02-03:50:04-root-INFO: Undo step: 54
2024-12-02-03:50:04-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-03:50:04-root-INFO: grad norm: 21.711 21.564 2.516
2024-12-02-03:50:05-root-INFO: grad norm: 10.913 10.795 1.599
2024-12-02-03:50:06-root-INFO: grad norm: 7.529 7.427 1.235
2024-12-02-03:50:07-root-INFO: grad norm: 5.840 5.752 1.011
2024-12-02-03:50:08-root-INFO: grad norm: 4.825 4.750 0.849
2024-12-02-03:50:09-root-INFO: Loss Change: 159.150 -> 78.458
2024-12-02-03:50:09-root-INFO: Regularization Change: 0.000 -> 50.608
2024-12-02-03:50:09-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-03:50:09-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-03:50:09-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-03:50:10-root-INFO: grad norm: 4.334 4.260 0.797
2024-12-02-03:50:11-root-INFO: grad norm: 3.718 3.663 0.638
2024-12-02-03:50:12-root-INFO: grad norm: 3.332 3.279 0.594
2024-12-02-03:50:13-root-INFO: grad norm: 3.057 3.012 0.524
2024-12-02-03:50:14-root-INFO: grad norm: 2.868 2.824 0.504
2024-12-02-03:50:14-root-INFO: Loss Change: 78.167 -> 69.451
2024-12-02-03:50:14-root-INFO: Regularization Change: 0.000 -> 7.047
2024-12-02-03:50:14-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-03:50:14-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-03:50:15-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-03:50:15-root-INFO: grad norm: 3.011 2.962 0.543
2024-12-02-03:50:16-root-INFO: grad norm: 2.808 2.773 0.446
2024-12-02-03:50:17-root-INFO: grad norm: 2.848 2.816 0.429
2024-12-02-03:50:18-root-INFO: grad norm: 2.996 2.966 0.422
2024-12-02-03:50:19-root-INFO: grad norm: 3.269 3.241 0.424
2024-12-02-03:50:20-root-INFO: Loss Change: 69.381 -> 65.360
2024-12-02-03:50:20-root-INFO: Regularization Change: 0.000 -> 3.634
2024-12-02-03:50:20-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-03:50:20-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-03:50:20-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-03:50:20-root-INFO: grad norm: 3.737 3.702 0.509
2024-12-02-03:50:21-root-INFO: grad norm: 3.708 3.681 0.447
2024-12-02-03:50:22-root-INFO: grad norm: 3.648 3.622 0.441
2024-12-02-03:50:23-root-INFO: grad norm: 3.652 3.627 0.421
2024-12-02-03:50:24-root-INFO: grad norm: 3.523 3.498 0.420
2024-12-02-03:50:25-root-INFO: Loss Change: 65.241 -> 62.273
2024-12-02-03:50:25-root-INFO: Regularization Change: 0.000 -> 2.718
2024-12-02-03:50:25-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-03:50:25-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-03:50:25-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-03:50:26-root-INFO: grad norm: 4.118 4.072 0.617
2024-12-02-03:50:27-root-INFO: grad norm: 3.655 3.629 0.438
2024-12-02-03:50:28-root-INFO: grad norm: 3.180 3.155 0.395
2024-12-02-03:50:28-root-INFO: grad norm: 3.179 3.157 0.371
2024-12-02-03:50:30-root-INFO: grad norm: 3.239 3.218 0.368
2024-12-02-03:50:30-root-INFO: Loss Change: 62.274 -> 59.770
2024-12-02-03:50:30-root-INFO: Regularization Change: 0.000 -> 2.322
2024-12-02-03:50:30-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-03:50:30-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-03:50:31-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-03:50:31-root-INFO: grad norm: 3.500 3.471 0.446
2024-12-02-03:50:32-root-INFO: grad norm: 3.382 3.361 0.379
2024-12-02-03:50:33-root-INFO: grad norm: 3.383 3.361 0.381
2024-12-02-03:50:34-root-INFO: grad norm: 3.422 3.401 0.373
2024-12-02-03:50:35-root-INFO: grad norm: 3.360 3.340 0.371
2024-12-02-03:50:35-root-INFO: Loss Change: 59.411 -> 57.344
2024-12-02-03:50:35-root-INFO: Regularization Change: 0.000 -> 2.051
2024-12-02-03:50:35-root-INFO: Undo step: 50
2024-12-02-03:50:35-root-INFO: Undo step: 51
2024-12-02-03:50:36-root-INFO: Undo step: 52
2024-12-02-03:50:36-root-INFO: Undo step: 53
2024-12-02-03:50:36-root-INFO: Undo step: 54
2024-12-02-03:50:36-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-03:50:36-root-INFO: grad norm: 23.969 23.804 2.808
2024-12-02-03:50:37-root-INFO: grad norm: 13.425 13.276 1.992
2024-12-02-03:50:38-root-INFO: grad norm: 8.423 8.305 1.405
2024-12-02-03:50:39-root-INFO: grad norm: 6.251 6.161 1.055
2024-12-02-03:50:40-root-INFO: grad norm: 5.145 5.065 0.901
2024-12-02-03:50:41-root-INFO: Loss Change: 168.159 -> 78.564
2024-12-02-03:50:41-root-INFO: Regularization Change: 0.000 -> 54.525
2024-12-02-03:50:41-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-03:50:41-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-03:50:41-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-03:50:41-root-INFO: grad norm: 4.570 4.495 0.824
2024-12-02-03:50:42-root-INFO: grad norm: 3.975 3.919 0.665
2024-12-02-03:50:43-root-INFO: grad norm: 3.664 3.613 0.605
2024-12-02-03:50:44-root-INFO: grad norm: 3.618 3.571 0.581
2024-12-02-03:50:45-root-INFO: grad norm: 3.593 3.554 0.526
2024-12-02-03:50:46-root-INFO: Loss Change: 78.214 -> 68.939
2024-12-02-03:50:46-root-INFO: Regularization Change: 0.000 -> 7.666
2024-12-02-03:50:46-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-03:50:46-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-03:50:46-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-03:50:47-root-INFO: grad norm: 4.719 4.652 0.790
2024-12-02-03:50:48-root-INFO: grad norm: 4.063 4.023 0.565
2024-12-02-03:50:49-root-INFO: grad norm: 3.209 3.162 0.546
2024-12-02-03:50:49-root-INFO: grad norm: 3.232 3.200 0.456
2024-12-02-03:50:50-root-INFO: grad norm: 3.578 3.540 0.522
2024-12-02-03:50:51-root-INFO: Loss Change: 69.035 -> 64.484
2024-12-02-03:50:51-root-INFO: Regularization Change: 0.000 -> 4.067
2024-12-02-03:50:51-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-03:50:51-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-03:50:51-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-03:50:52-root-INFO: grad norm: 3.540 3.514 0.433
2024-12-02-03:50:53-root-INFO: grad norm: 3.624 3.594 0.469
2024-12-02-03:50:54-root-INFO: grad norm: 3.627 3.599 0.452
2024-12-02-03:50:55-root-INFO: grad norm: 3.627 3.595 0.481
2024-12-02-03:50:56-root-INFO: grad norm: 3.628 3.598 0.461
2024-12-02-03:50:56-root-INFO: Loss Change: 64.247 -> 61.012
2024-12-02-03:50:56-root-INFO: Regularization Change: 0.000 -> 2.852
2024-12-02-03:50:56-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-03:50:56-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-03:50:57-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-03:50:57-root-INFO: grad norm: 4.699 4.624 0.831
2024-12-02-03:50:58-root-INFO: grad norm: 4.141 4.103 0.561
2024-12-02-03:50:59-root-INFO: grad norm: 3.359 3.318 0.521
2024-12-02-03:51:00-root-INFO: grad norm: 3.417 3.388 0.444
2024-12-02-03:51:01-root-INFO: grad norm: 3.749 3.714 0.509
2024-12-02-03:51:02-root-INFO: Loss Change: 61.178 -> 58.729
2024-12-02-03:51:02-root-INFO: Regularization Change: 0.000 -> 2.518
2024-12-02-03:51:02-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-03:51:02-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-03:51:02-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-03:51:02-root-INFO: grad norm: 3.498 3.476 0.394
2024-12-02-03:51:03-root-INFO: grad norm: 3.457 3.429 0.438
2024-12-02-03:51:04-root-INFO: grad norm: 3.520 3.494 0.427
2024-12-02-03:51:05-root-INFO: grad norm: 3.623 3.592 0.469
2024-12-02-03:51:06-root-INFO: grad norm: 3.634 3.605 0.457
2024-12-02-03:51:07-root-INFO: Loss Change: 58.229 -> 55.973
2024-12-02-03:51:07-root-INFO: Regularization Change: 0.000 -> 2.118
2024-12-02-03:51:07-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-03:51:07-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-03:51:07-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-03:51:08-root-INFO: grad norm: 4.882 4.811 0.826
2024-12-02-03:51:09-root-INFO: grad norm: 4.297 4.256 0.595
2024-12-02-03:51:10-root-INFO: grad norm: 3.595 3.555 0.531
2024-12-02-03:51:11-root-INFO: grad norm: 3.642 3.610 0.482
2024-12-02-03:51:12-root-INFO: grad norm: 3.858 3.823 0.520
2024-12-02-03:51:12-root-INFO: Loss Change: 56.286 -> 54.314
2024-12-02-03:51:12-root-INFO: Regularization Change: 0.000 -> 2.171
2024-12-02-03:51:12-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-03:51:12-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-03:51:12-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-03:51:13-root-INFO: grad norm: 3.720 3.698 0.407
2024-12-02-03:51:14-root-INFO: grad norm: 3.730 3.701 0.461
2024-12-02-03:51:15-root-INFO: grad norm: 3.792 3.763 0.468
2024-12-02-03:51:16-root-INFO: grad norm: 3.885 3.853 0.494
2024-12-02-03:51:17-root-INFO: grad norm: 3.895 3.863 0.498
2024-12-02-03:51:17-root-INFO: Loss Change: 54.084 -> 52.245
2024-12-02-03:51:17-root-INFO: Regularization Change: 0.000 -> 1.945
2024-12-02-03:51:17-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-03:51:17-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-03:51:18-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-03:51:18-root-INFO: grad norm: 4.977 4.904 0.849
2024-12-02-03:51:19-root-INFO: grad norm: 4.542 4.496 0.646
2024-12-02-03:51:20-root-INFO: grad norm: 4.016 3.973 0.582
2024-12-02-03:51:21-root-INFO: grad norm: 3.948 3.911 0.543
2024-12-02-03:51:22-root-INFO: grad norm: 3.951 3.914 0.542
2024-12-02-03:51:23-root-INFO: Loss Change: 52.541 -> 50.750
2024-12-02-03:51:23-root-INFO: Regularization Change: 0.000 -> 2.089
2024-12-02-03:51:23-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-03:51:23-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-03:51:23-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-03:51:23-root-INFO: grad norm: 3.594 3.569 0.425
2024-12-02-03:51:24-root-INFO: grad norm: 3.518 3.490 0.449
2024-12-02-03:51:25-root-INFO: grad norm: 3.572 3.542 0.464
2024-12-02-03:51:26-root-INFO: grad norm: 3.679 3.648 0.481
2024-12-02-03:51:27-root-INFO: grad norm: 3.694 3.661 0.494
2024-12-02-03:51:28-root-INFO: Loss Change: 50.454 -> 48.793
2024-12-02-03:51:28-root-INFO: Regularization Change: 0.000 -> 1.839
2024-12-02-03:51:28-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-03:51:28-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-03:51:28-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-03:51:29-root-INFO: grad norm: 4.587 4.521 0.776
2024-12-02-03:51:30-root-INFO: grad norm: 4.199 4.155 0.611
2024-12-02-03:51:31-root-INFO: grad norm: 3.770 3.732 0.534
2024-12-02-03:51:31-root-INFO: grad norm: 3.683 3.647 0.514
2024-12-02-03:51:32-root-INFO: grad norm: 3.638 3.604 0.494
2024-12-02-03:51:33-root-INFO: Loss Change: 48.925 -> 47.254
2024-12-02-03:51:33-root-INFO: Regularization Change: 0.000 -> 1.967
2024-12-02-03:51:33-root-INFO: Undo step: 45
2024-12-02-03:51:33-root-INFO: Undo step: 46
2024-12-02-03:51:33-root-INFO: Undo step: 47
2024-12-02-03:51:33-root-INFO: Undo step: 48
2024-12-02-03:51:33-root-INFO: Undo step: 49
2024-12-02-03:51:34-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-03:51:34-root-INFO: grad norm: 20.076 19.931 2.401
2024-12-02-03:51:35-root-INFO: grad norm: 11.203 11.093 1.567
2024-12-02-03:51:36-root-INFO: grad norm: 7.565 7.475 1.163
2024-12-02-03:51:37-root-INFO: grad norm: 5.890 5.809 0.972
2024-12-02-03:51:38-root-INFO: grad norm: 4.898 4.836 0.777
2024-12-02-03:51:39-root-INFO: Loss Change: 147.503 -> 68.628
2024-12-02-03:51:39-root-INFO: Regularization Change: 0.000 -> 53.627
2024-12-02-03:51:39-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-03:51:39-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-03:51:39-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-03:51:39-root-INFO: grad norm: 4.412 4.349 0.743
2024-12-02-03:51:40-root-INFO: grad norm: 3.735 3.686 0.603
2024-12-02-03:51:41-root-INFO: grad norm: 3.319 3.273 0.554
2024-12-02-03:51:42-root-INFO: grad norm: 3.025 2.987 0.477
2024-12-02-03:51:43-root-INFO: grad norm: 2.815 2.776 0.465
2024-12-02-03:51:44-root-INFO: Loss Change: 68.466 -> 59.486
2024-12-02-03:51:44-root-INFO: Regularization Change: 0.000 -> 7.982
2024-12-02-03:51:44-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-03:51:44-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-03:51:44-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-03:51:45-root-INFO: grad norm: 2.768 2.733 0.441
2024-12-02-03:51:46-root-INFO: grad norm: 2.512 2.480 0.397
2024-12-02-03:51:47-root-INFO: grad norm: 2.453 2.427 0.358
2024-12-02-03:51:48-root-INFO: grad norm: 2.550 2.524 0.362
2024-12-02-03:51:49-root-INFO: grad norm: 2.739 2.716 0.358
2024-12-02-03:51:49-root-INFO: Loss Change: 59.290 -> 55.079
2024-12-02-03:51:49-root-INFO: Regularization Change: 0.000 -> 4.065
2024-12-02-03:51:49-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-03:51:49-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-03:51:49-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-03:51:50-root-INFO: grad norm: 4.029 3.978 0.639
2024-12-02-03:51:51-root-INFO: grad norm: 3.737 3.704 0.496
2024-12-02-03:51:52-root-INFO: grad norm: 3.430 3.400 0.458
2024-12-02-03:51:53-root-INFO: grad norm: 3.516 3.488 0.446
2024-12-02-03:51:54-root-INFO: grad norm: 3.701 3.673 0.455
2024-12-02-03:51:54-root-INFO: Loss Change: 55.148 -> 52.347
2024-12-02-03:51:54-root-INFO: Regularization Change: 0.000 -> 3.167
2024-12-02-03:51:54-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-03:51:54-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-03:51:55-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-03:51:55-root-INFO: grad norm: 3.539 3.518 0.387
2024-12-02-03:51:56-root-INFO: grad norm: 3.673 3.650 0.414
2024-12-02-03:51:57-root-INFO: grad norm: 3.868 3.840 0.465
2024-12-02-03:51:58-root-INFO: grad norm: 4.137 4.109 0.480
2024-12-02-03:51:59-root-INFO: grad norm: 4.199 4.166 0.522
2024-12-02-03:52:00-root-INFO: Loss Change: 52.036 -> 49.763
2024-12-02-03:52:00-root-INFO: Regularization Change: 0.000 -> 2.643
2024-12-02-03:52:00-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-03:52:00-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-03:52:00-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-03:52:00-root-INFO: grad norm: 5.129 5.071 0.773
2024-12-02-03:52:01-root-INFO: grad norm: 4.765 4.721 0.639
2024-12-02-03:52:02-root-INFO: grad norm: 4.321 4.286 0.549
2024-12-02-03:52:03-root-INFO: grad norm: 4.208 4.172 0.548
2024-12-02-03:52:04-root-INFO: grad norm: 4.081 4.049 0.510
2024-12-02-03:52:05-root-INFO: Loss Change: 49.914 -> 47.677
2024-12-02-03:52:05-root-INFO: Regularization Change: 0.000 -> 2.635
2024-12-02-03:52:05-root-INFO: Undo step: 45
2024-12-02-03:52:05-root-INFO: Undo step: 46
2024-12-02-03:52:05-root-INFO: Undo step: 47
2024-12-02-03:52:05-root-INFO: Undo step: 48
2024-12-02-03:52:05-root-INFO: Undo step: 49
2024-12-02-03:52:05-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-03:52:06-root-INFO: grad norm: 19.556 19.401 2.454
2024-12-02-03:52:07-root-INFO: grad norm: 11.375 11.278 1.487
2024-12-02-03:52:08-root-INFO: grad norm: 8.503 8.431 1.101
2024-12-02-03:52:09-root-INFO: grad norm: 6.966 6.905 0.924
2024-12-02-03:52:10-root-INFO: grad norm: 6.102 6.046 0.827
2024-12-02-03:52:10-root-INFO: Loss Change: 144.155 -> 68.329
2024-12-02-03:52:10-root-INFO: Regularization Change: 0.000 -> 53.527
2024-12-02-03:52:10-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-03:52:10-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-03:52:11-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-03:52:11-root-INFO: grad norm: 6.278 6.207 0.938
2024-12-02-03:52:12-root-INFO: grad norm: 5.603 5.544 0.816
2024-12-02-03:52:13-root-INFO: grad norm: 5.015 4.963 0.721
2024-12-02-03:52:14-root-INFO: grad norm: 4.978 4.926 0.716
2024-12-02-03:52:15-root-INFO: grad norm: 5.006 4.961 0.670
2024-12-02-03:52:16-root-INFO: Loss Change: 68.517 -> 59.878
2024-12-02-03:52:16-root-INFO: Regularization Change: 0.000 -> 8.425
2024-12-02-03:52:16-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-03:52:16-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-03:52:16-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-03:52:16-root-INFO: grad norm: 4.644 4.606 0.587
2024-12-02-03:52:17-root-INFO: grad norm: 4.400 4.364 0.559
2024-12-02-03:52:18-root-INFO: grad norm: 4.396 4.358 0.575
2024-12-02-03:52:19-root-INFO: grad norm: 4.416 4.382 0.547
2024-12-02-03:52:20-root-INFO: grad norm: 4.362 4.325 0.565
2024-12-02-03:52:21-root-INFO: Loss Change: 59.523 -> 54.892
2024-12-02-03:52:21-root-INFO: Regularization Change: 0.000 -> 4.476
2024-12-02-03:52:21-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-03:52:21-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-03:52:21-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-03:52:22-root-INFO: grad norm: 5.257 5.191 0.830
2024-12-02-03:52:23-root-INFO: grad norm: 4.789 4.742 0.664
2024-12-02-03:52:24-root-INFO: grad norm: 4.355 4.318 0.572
2024-12-02-03:52:25-root-INFO: grad norm: 4.260 4.223 0.557
2024-12-02-03:52:25-root-INFO: grad norm: 4.164 4.131 0.521
2024-12-02-03:52:26-root-INFO: Loss Change: 55.131 -> 51.857
2024-12-02-03:52:26-root-INFO: Regularization Change: 0.000 -> 3.432
2024-12-02-03:52:26-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-03:52:26-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-03:52:27-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-03:52:27-root-INFO: grad norm: 3.765 3.742 0.420
2024-12-02-03:52:28-root-INFO: grad norm: 3.655 3.631 0.419
2024-12-02-03:52:29-root-INFO: grad norm: 3.692 3.664 0.448
2024-12-02-03:52:30-root-INFO: grad norm: 3.754 3.728 0.440
2024-12-02-03:52:31-root-INFO: grad norm: 3.773 3.744 0.467
2024-12-02-03:52:31-root-INFO: Loss Change: 51.511 -> 49.028
2024-12-02-03:52:31-root-INFO: Regularization Change: 0.000 -> 2.673
2024-12-02-03:52:31-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-03:52:31-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-03:52:32-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-03:52:32-root-INFO: grad norm: 4.636 4.579 0.726
2024-12-02-03:52:33-root-INFO: grad norm: 4.263 4.223 0.579
2024-12-02-03:52:34-root-INFO: grad norm: 3.932 3.902 0.487
2024-12-02-03:52:35-root-INFO: grad norm: 3.808 3.777 0.486
2024-12-02-03:52:36-root-INFO: grad norm: 3.699 3.672 0.445
2024-12-02-03:52:37-root-INFO: Loss Change: 49.119 -> 46.799
2024-12-02-03:52:37-root-INFO: Regularization Change: 0.000 -> 2.527
2024-12-02-03:52:37-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-03:52:37-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-03:52:37-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-03:52:37-root-INFO: grad norm: 3.296 3.278 0.343
2024-12-02-03:52:38-root-INFO: grad norm: 3.177 3.158 0.345
2024-12-02-03:52:39-root-INFO: grad norm: 3.206 3.184 0.370
2024-12-02-03:52:40-root-INFO: grad norm: 3.258 3.237 0.365
2024-12-02-03:52:41-root-INFO: grad norm: 3.296 3.272 0.392
2024-12-02-03:52:42-root-INFO: Loss Change: 46.545 -> 44.661
2024-12-02-03:52:42-root-INFO: Regularization Change: 0.000 -> 2.137
2024-12-02-03:52:42-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-03:52:42-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-03:52:42-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-03:52:43-root-INFO: grad norm: 4.196 4.145 0.650
2024-12-02-03:52:44-root-INFO: grad norm: 3.841 3.807 0.508
2024-12-02-03:52:45-root-INFO: grad norm: 3.530 3.505 0.421
2024-12-02-03:52:46-root-INFO: grad norm: 3.389 3.363 0.418
2024-12-02-03:52:47-root-INFO: grad norm: 3.284 3.262 0.382
2024-12-02-03:52:47-root-INFO: Loss Change: 44.664 -> 42.690
2024-12-02-03:52:47-root-INFO: Regularization Change: 0.000 -> 2.153
2024-12-02-03:52:47-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-03:52:47-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-03:52:48-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-03:52:48-root-INFO: grad norm: 2.924 2.908 0.300
2024-12-02-03:52:49-root-INFO: grad norm: 2.809 2.794 0.288
2024-12-02-03:52:50-root-INFO: grad norm: 2.819 2.801 0.318
2024-12-02-03:52:51-root-INFO: grad norm: 2.854 2.837 0.305
2024-12-02-03:52:52-root-INFO: grad norm: 2.888 2.868 0.336
2024-12-02-03:52:53-root-INFO: Loss Change: 42.439 -> 40.842
2024-12-02-03:52:53-root-INFO: Regularization Change: 0.000 -> 1.868
2024-12-02-03:52:53-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-03:52:53-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-03:52:53-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-03:52:53-root-INFO: grad norm: 4.046 3.988 0.685
2024-12-02-03:52:55-root-INFO: grad norm: 3.594 3.560 0.488
2024-12-02-03:52:55-root-INFO: grad norm: 3.249 3.227 0.379
2024-12-02-03:52:56-root-INFO: grad norm: 3.088 3.065 0.376
2024-12-02-03:52:57-root-INFO: grad norm: 2.987 2.969 0.334
2024-12-02-03:52:58-root-INFO: Loss Change: 41.029 -> 39.140
2024-12-02-03:52:58-root-INFO: Regularization Change: 0.000 -> 2.024
2024-12-02-03:52:58-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-03:52:58-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-03:52:58-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-03:52:59-root-INFO: grad norm: 2.557 2.541 0.289
2024-12-02-03:53:00-root-INFO: grad norm: 2.309 2.298 0.226
2024-12-02-03:53:01-root-INFO: grad norm: 2.283 2.269 0.250
2024-12-02-03:53:02-root-INFO: grad norm: 2.314 2.301 0.242
2024-12-02-03:53:03-root-INFO: grad norm: 2.350 2.335 0.271
2024-12-02-03:53:03-root-INFO: Loss Change: 38.874 -> 37.367
2024-12-02-03:53:03-root-INFO: Regularization Change: 0.000 -> 1.712
2024-12-02-03:53:03-root-INFO: Undo step: 40
2024-12-02-03:53:03-root-INFO: Undo step: 41
2024-12-02-03:53:03-root-INFO: Undo step: 42
2024-12-02-03:53:03-root-INFO: Undo step: 43
2024-12-02-03:53:03-root-INFO: Undo step: 44
2024-12-02-03:53:04-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-03:53:04-root-INFO: grad norm: 18.808 18.638 2.525
2024-12-02-03:53:05-root-INFO: grad norm: 10.119 10.016 1.443
2024-12-02-03:53:06-root-INFO: grad norm: 7.211 7.145 0.975
2024-12-02-03:53:07-root-INFO: grad norm: 5.741 5.679 0.844
2024-12-02-03:53:08-root-INFO: grad norm: 4.959 4.911 0.687
2024-12-02-03:53:09-root-INFO: Loss Change: 130.458 -> 58.256
2024-12-02-03:53:09-root-INFO: Regularization Change: 0.000 -> 54.933
2024-12-02-03:53:09-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-03:53:09-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-03:53:09-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-03:53:09-root-INFO: grad norm: 4.312 4.270 0.598
2024-12-02-03:53:10-root-INFO: grad norm: 3.884 3.847 0.533
2024-12-02-03:53:11-root-INFO: grad norm: 3.657 3.620 0.521
2024-12-02-03:53:12-root-INFO: grad norm: 3.563 3.530 0.488
2024-12-02-03:53:13-root-INFO: grad norm: 3.486 3.450 0.500
2024-12-02-03:53:14-root-INFO: Loss Change: 57.951 -> 49.698
2024-12-02-03:53:14-root-INFO: Regularization Change: 0.000 -> 8.366
2024-12-02-03:53:14-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-03:53:14-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-03:53:14-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-03:53:14-root-INFO: grad norm: 4.209 4.153 0.688
2024-12-02-03:53:15-root-INFO: grad norm: 3.880 3.835 0.590
2024-12-02-03:53:16-root-INFO: grad norm: 3.538 3.502 0.499
2024-12-02-03:53:17-root-INFO: grad norm: 3.380 3.342 0.503
2024-12-02-03:53:18-root-INFO: grad norm: 3.265 3.233 0.459
2024-12-02-03:53:19-root-INFO: Loss Change: 49.543 -> 45.213
2024-12-02-03:53:19-root-INFO: Regularization Change: 0.000 -> 4.558
2024-12-02-03:53:19-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-03:53:19-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-03:53:19-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-03:53:20-root-INFO: grad norm: 2.925 2.901 0.379
2024-12-02-03:53:21-root-INFO: grad norm: 2.742 2.719 0.356
2024-12-02-03:53:22-root-INFO: grad norm: 2.704 2.679 0.372
2024-12-02-03:53:23-root-INFO: grad norm: 2.701 2.677 0.359
2024-12-02-03:53:23-root-INFO: grad norm: 2.704 2.678 0.373
2024-12-02-03:53:24-root-INFO: Loss Change: 44.932 -> 42.022
2024-12-02-03:53:24-root-INFO: Regularization Change: 0.000 -> 3.117
2024-12-02-03:53:24-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-03:53:24-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-03:53:24-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-03:53:25-root-INFO: grad norm: 3.784 3.717 0.708
2024-12-02-03:53:26-root-INFO: grad norm: 3.352 3.312 0.515
2024-12-02-03:53:27-root-INFO: grad norm: 3.009 2.979 0.421
2024-12-02-03:53:28-root-INFO: grad norm: 2.860 2.831 0.404
2024-12-02-03:53:29-root-INFO: grad norm: 2.757 2.732 0.373
2024-12-02-03:53:29-root-INFO: Loss Change: 42.104 -> 39.556
2024-12-02-03:53:29-root-INFO: Regularization Change: 0.000 -> 2.704
2024-12-02-03:53:29-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-03:53:29-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-03:53:30-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-03:53:30-root-INFO: grad norm: 2.488 2.468 0.316
2024-12-02-03:53:31-root-INFO: grad norm: 2.240 2.225 0.262
2024-12-02-03:53:32-root-INFO: grad norm: 2.224 2.208 0.260
2024-12-02-03:53:33-root-INFO: grad norm: 2.249 2.233 0.269
2024-12-02-03:53:34-root-INFO: grad norm: 2.274 2.258 0.270
2024-12-02-03:53:35-root-INFO: Loss Change: 39.295 -> 37.341
2024-12-02-03:53:35-root-INFO: Regularization Change: 0.000 -> 2.168
2024-12-02-03:53:35-root-INFO: Undo step: 40
2024-12-02-03:53:35-root-INFO: Undo step: 41
2024-12-02-03:53:35-root-INFO: Undo step: 42
2024-12-02-03:53:35-root-INFO: Undo step: 43
2024-12-02-03:53:35-root-INFO: Undo step: 44
2024-12-02-03:53:35-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-03:53:35-root-INFO: grad norm: 19.364 19.243 2.157
2024-12-02-03:53:36-root-INFO: grad norm: 9.762 9.666 1.372
2024-12-02-03:53:37-root-INFO: grad norm: 6.778 6.706 0.984
2024-12-02-03:53:38-root-INFO: grad norm: 5.351 5.289 0.815
2024-12-02-03:53:39-root-INFO: grad norm: 4.562 4.506 0.712
2024-12-02-03:53:40-root-INFO: Loss Change: 134.100 -> 58.067
2024-12-02-03:53:40-root-INFO: Regularization Change: 0.000 -> 59.050
2024-12-02-03:53:40-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-03:53:40-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-03:53:40-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-03:53:41-root-INFO: grad norm: 3.988 3.938 0.630
2024-12-02-03:53:42-root-INFO: grad norm: 3.544 3.503 0.539
2024-12-02-03:53:43-root-INFO: grad norm: 3.295 3.257 0.498
2024-12-02-03:53:43-root-INFO: grad norm: 3.151 3.113 0.485
2024-12-02-03:53:44-root-INFO: grad norm: 3.078 3.046 0.447
2024-12-02-03:53:45-root-INFO: Loss Change: 57.765 -> 49.418
2024-12-02-03:53:45-root-INFO: Regularization Change: 0.000 -> 8.413
2024-12-02-03:53:45-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-03:53:45-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-03:53:45-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-03:53:46-root-INFO: grad norm: 3.712 3.650 0.676
2024-12-02-03:53:47-root-INFO: grad norm: 3.620 3.584 0.514
2024-12-02-03:53:48-root-INFO: grad norm: 3.656 3.618 0.525
2024-12-02-03:53:49-root-INFO: grad norm: 3.687 3.656 0.479
2024-12-02-03:53:50-root-INFO: grad norm: 3.714 3.679 0.505
2024-12-02-03:53:50-root-INFO: Loss Change: 49.190 -> 45.164
2024-12-02-03:53:50-root-INFO: Regularization Change: 0.000 -> 4.605
2024-12-02-03:53:50-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-03:53:50-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-03:53:51-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-03:53:51-root-INFO: grad norm: 3.471 3.451 0.372
2024-12-02-03:53:52-root-INFO: grad norm: 3.370 3.347 0.394
2024-12-02-03:53:53-root-INFO: grad norm: 3.334 3.312 0.380
2024-12-02-03:53:54-root-INFO: grad norm: 3.318 3.293 0.405
2024-12-02-03:53:55-root-INFO: grad norm: 3.285 3.262 0.386
2024-12-02-03:53:56-root-INFO: Loss Change: 44.846 -> 41.896
2024-12-02-03:53:56-root-INFO: Regularization Change: 0.000 -> 3.286
2024-12-02-03:53:56-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-03:53:56-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-03:53:56-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-03:53:56-root-INFO: grad norm: 4.278 4.206 0.780
2024-12-02-03:53:57-root-INFO: grad norm: 3.791 3.754 0.533
2024-12-02-03:53:58-root-INFO: grad norm: 3.369 3.338 0.457
2024-12-02-03:53:59-root-INFO: grad norm: 3.156 3.131 0.399
2024-12-02-03:54:00-root-INFO: grad norm: 2.996 2.970 0.390
2024-12-02-03:54:01-root-INFO: Loss Change: 42.025 -> 39.298
2024-12-02-03:54:01-root-INFO: Regularization Change: 0.000 -> 2.846
2024-12-02-03:54:01-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-03:54:01-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-03:54:01-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-03:54:02-root-INFO: grad norm: 2.673 2.655 0.309
2024-12-02-03:54:03-root-INFO: grad norm: 2.397 2.382 0.262
2024-12-02-03:54:04-root-INFO: grad norm: 2.340 2.326 0.250
2024-12-02-03:54:05-root-INFO: grad norm: 2.357 2.341 0.274
2024-12-02-03:54:06-root-INFO: grad norm: 2.381 2.366 0.265
2024-12-02-03:54:06-root-INFO: Loss Change: 39.023 -> 37.003
2024-12-02-03:54:06-root-INFO: Regularization Change: 0.000 -> 2.237
2024-12-02-03:54:06-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-03:54:06-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-03:54:07-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-03:54:07-root-INFO: grad norm: 3.140 3.096 0.524
2024-12-02-03:54:08-root-INFO: grad norm: 2.947 2.922 0.378
2024-12-02-03:54:09-root-INFO: grad norm: 2.803 2.781 0.349
2024-12-02-03:54:10-root-INFO: grad norm: 2.735 2.716 0.323
2024-12-02-03:54:11-root-INFO: grad norm: 2.669 2.650 0.318
2024-12-02-03:54:11-root-INFO: Loss Change: 36.965 -> 35.202
2024-12-02-03:54:11-root-INFO: Regularization Change: 0.000 -> 2.114
2024-12-02-03:54:11-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-03:54:11-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-03:54:12-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-03:54:12-root-INFO: grad norm: 2.481 2.466 0.277
2024-12-02-03:54:13-root-INFO: grad norm: 2.226 2.214 0.230
2024-12-02-03:54:14-root-INFO: grad norm: 2.184 2.172 0.220
2024-12-02-03:54:15-root-INFO: grad norm: 2.196 2.182 0.243
2024-12-02-03:54:16-root-INFO: grad norm: 2.210 2.198 0.236
2024-12-02-03:54:17-root-INFO: Loss Change: 34.899 -> 33.263
2024-12-02-03:54:17-root-INFO: Regularization Change: 0.000 -> 1.872
2024-12-02-03:54:17-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-03:54:17-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-03:54:17-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-03:54:17-root-INFO: grad norm: 3.174 3.127 0.545
2024-12-02-03:54:18-root-INFO: grad norm: 2.787 2.763 0.367
2024-12-02-03:54:19-root-INFO: grad norm: 2.448 2.429 0.304
2024-12-02-03:54:20-root-INFO: grad norm: 2.329 2.313 0.278
2024-12-02-03:54:21-root-INFO: grad norm: 2.233 2.217 0.264
2024-12-02-03:54:22-root-INFO: Loss Change: 33.277 -> 31.633
2024-12-02-03:54:22-root-INFO: Regularization Change: 0.000 -> 1.847
2024-12-02-03:54:22-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-03:54:22-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-03:54:22-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-03:54:23-root-INFO: grad norm: 2.056 2.042 0.242
2024-12-02-03:54:24-root-INFO: grad norm: 1.862 1.852 0.195
2024-12-02-03:54:25-root-INFO: grad norm: 1.847 1.838 0.188
2024-12-02-03:54:26-root-INFO: grad norm: 1.906 1.895 0.206
2024-12-02-03:54:27-root-INFO: grad norm: 1.939 1.928 0.204
2024-12-02-03:54:27-root-INFO: Loss Change: 31.407 -> 30.045
2024-12-02-03:54:27-root-INFO: Regularization Change: 0.000 -> 1.633
2024-12-02-03:54:27-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-03:54:27-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-03:54:28-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-03:54:28-root-INFO: grad norm: 2.724 2.684 0.466
2024-12-02-03:54:29-root-INFO: grad norm: 2.447 2.426 0.321
2024-12-02-03:54:30-root-INFO: grad norm: 2.217 2.201 0.265
2024-12-02-03:54:31-root-INFO: grad norm: 2.134 2.120 0.247
2024-12-02-03:54:32-root-INFO: grad norm: 2.099 2.086 0.235
2024-12-02-03:54:32-root-INFO: Loss Change: 30.052 -> 28.681
2024-12-02-03:54:32-root-INFO: Regularization Change: 0.000 -> 1.676
2024-12-02-03:54:32-root-INFO: Undo step: 35
2024-12-02-03:54:33-root-INFO: Undo step: 36
2024-12-02-03:54:33-root-INFO: Undo step: 37
2024-12-02-03:54:33-root-INFO: Undo step: 38
2024-12-02-03:54:33-root-INFO: Undo step: 39
2024-12-02-03:54:33-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-03:54:33-root-INFO: grad norm: 16.438 16.344 1.754
2024-12-02-03:54:34-root-INFO: grad norm: 10.270 10.198 1.217
2024-12-02-03:54:35-root-INFO: grad norm: 7.381 7.297 1.114
2024-12-02-03:54:36-root-INFO: grad norm: 5.209 5.159 0.717
2024-12-02-03:54:37-root-INFO: grad norm: 4.206 4.164 0.596
2024-12-02-03:54:38-root-INFO: Loss Change: 115.322 -> 48.289
2024-12-02-03:54:38-root-INFO: Regularization Change: 0.000 -> 58.136
2024-12-02-03:54:38-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-03:54:38-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-03:54:38-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-03:54:38-root-INFO: grad norm: 3.871 3.826 0.594
2024-12-02-03:54:39-root-INFO: grad norm: 3.387 3.356 0.455
2024-12-02-03:54:40-root-INFO: grad norm: 3.154 3.127 0.407
2024-12-02-03:54:41-root-INFO: grad norm: 3.104 3.082 0.366
2024-12-02-03:54:42-root-INFO: grad norm: 3.167 3.147 0.354
2024-12-02-03:54:43-root-INFO: Loss Change: 48.031 -> 40.343
2024-12-02-03:54:43-root-INFO: Regularization Change: 0.000 -> 8.656
2024-12-02-03:54:43-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-03:54:43-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-03:54:43-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-03:54:44-root-INFO: grad norm: 3.254 3.233 0.369
2024-12-02-03:54:45-root-INFO: grad norm: 3.068 3.054 0.286
2024-12-02-03:54:46-root-INFO: grad norm: 2.973 2.960 0.274
2024-12-02-03:54:47-root-INFO: grad norm: 2.876 2.864 0.272
2024-12-02-03:54:48-root-INFO: grad norm: 2.777 2.765 0.260
2024-12-02-03:54:48-root-INFO: Loss Change: 40.023 -> 36.115
2024-12-02-03:54:48-root-INFO: Regularization Change: 0.000 -> 4.522
2024-12-02-03:54:48-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-03:54:48-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-03:54:48-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-03:54:49-root-INFO: grad norm: 3.194 3.153 0.515
2024-12-02-03:54:50-root-INFO: grad norm: 2.776 2.758 0.317
2024-12-02-03:54:51-root-INFO: grad norm: 2.531 2.516 0.281
2024-12-02-03:54:52-root-INFO: grad norm: 2.391 2.379 0.248
2024-12-02-03:54:53-root-INFO: grad norm: 2.385 2.371 0.255
2024-12-02-03:54:53-root-INFO: Loss Change: 35.993 -> 33.179
2024-12-02-03:54:53-root-INFO: Regularization Change: 0.000 -> 3.173
2024-12-02-03:54:53-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-03:54:53-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-03:54:54-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-03:54:54-root-INFO: grad norm: 2.347 2.329 0.286
2024-12-02-03:54:55-root-INFO: grad norm: 2.170 2.160 0.208
2024-12-02-03:54:56-root-INFO: grad norm: 2.159 2.150 0.199
2024-12-02-03:54:57-root-INFO: grad norm: 2.226 2.215 0.219
2024-12-02-03:54:58-root-INFO: grad norm: 2.278 2.268 0.215
2024-12-02-03:54:59-root-INFO: Loss Change: 32.955 -> 30.961
2024-12-02-03:54:59-root-INFO: Regularization Change: 0.000 -> 2.442
2024-12-02-03:54:59-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-03:54:59-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-03:54:59-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-03:54:59-root-INFO: grad norm: 2.958 2.918 0.483
2024-12-02-03:55:00-root-INFO: grad norm: 2.753 2.735 0.322
2024-12-02-03:55:01-root-INFO: grad norm: 2.624 2.608 0.290
2024-12-02-03:55:02-root-INFO: grad norm: 2.554 2.539 0.273
2024-12-02-03:55:03-root-INFO: grad norm: 2.540 2.525 0.270
2024-12-02-03:55:04-root-INFO: Loss Change: 30.932 -> 29.215
2024-12-02-03:55:04-root-INFO: Regularization Change: 0.000 -> 2.237
2024-12-02-03:55:04-root-INFO: Undo step: 35
2024-12-02-03:55:04-root-INFO: Undo step: 36
2024-12-02-03:55:04-root-INFO: Undo step: 37
2024-12-02-03:55:04-root-INFO: Undo step: 38
2024-12-02-03:55:04-root-INFO: Undo step: 39
2024-12-02-03:55:04-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-03:55:05-root-INFO: grad norm: 17.685 17.593 1.798
2024-12-02-03:55:06-root-INFO: grad norm: 10.375 10.310 1.158
2024-12-02-03:55:07-root-INFO: grad norm: 7.377 7.297 1.083
2024-12-02-03:55:08-root-INFO: grad norm: 5.458 5.409 0.732
2024-12-02-03:55:09-root-INFO: grad norm: 4.455 4.413 0.609
2024-12-02-03:55:09-root-INFO: Loss Change: 121.458 -> 48.729
2024-12-02-03:55:09-root-INFO: Regularization Change: 0.000 -> 61.513
2024-12-02-03:55:09-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-03:55:09-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-03:55:10-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-03:55:10-root-INFO: grad norm: 4.240 4.190 0.649
2024-12-02-03:55:11-root-INFO: grad norm: 3.671 3.638 0.489
2024-12-02-03:55:12-root-INFO: grad norm: 3.347 3.316 0.454
2024-12-02-03:55:13-root-INFO: grad norm: 3.124 3.099 0.397
2024-12-02-03:55:14-root-INFO: grad norm: 2.995 2.969 0.394
2024-12-02-03:55:15-root-INFO: Loss Change: 48.539 -> 40.383
2024-12-02-03:55:15-root-INFO: Regularization Change: 0.000 -> 8.888
2024-12-02-03:55:15-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-03:55:15-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-03:55:15-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-03:55:15-root-INFO: grad norm: 2.818 2.794 0.362
2024-12-02-03:55:16-root-INFO: grad norm: 2.583 2.564 0.307
2024-12-02-03:55:17-root-INFO: grad norm: 2.523 2.507 0.282
2024-12-02-03:55:18-root-INFO: grad norm: 2.505 2.487 0.296
2024-12-02-03:55:19-root-INFO: grad norm: 2.498 2.483 0.271
2024-12-02-03:55:20-root-INFO: Loss Change: 40.002 -> 36.022
2024-12-02-03:55:20-root-INFO: Regularization Change: 0.000 -> 4.577
2024-12-02-03:55:20-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-03:55:20-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-03:55:20-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-03:55:21-root-INFO: grad norm: 3.328 3.281 0.560
2024-12-02-03:55:22-root-INFO: grad norm: 3.087 3.064 0.371
2024-12-02-03:55:23-root-INFO: grad norm: 2.992 2.969 0.367
2024-12-02-03:55:24-root-INFO: grad norm: 2.910 2.891 0.330
2024-12-02-03:55:25-root-INFO: grad norm: 2.855 2.834 0.348
2024-12-02-03:55:25-root-INFO: Loss Change: 35.973 -> 33.202
2024-12-02-03:55:25-root-INFO: Regularization Change: 0.000 -> 3.356
2024-12-02-03:55:25-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-03:55:25-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-03:55:26-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-03:55:26-root-INFO: grad norm: 2.604 2.590 0.267
2024-12-02-03:55:27-root-INFO: grad norm: 2.488 2.472 0.281
2024-12-02-03:55:28-root-INFO: grad norm: 2.495 2.481 0.269
2024-12-02-03:55:29-root-INFO: grad norm: 2.566 2.548 0.302
2024-12-02-03:55:30-root-INFO: grad norm: 2.584 2.568 0.290
2024-12-02-03:55:31-root-INFO: Loss Change: 32.896 -> 30.820
2024-12-02-03:55:31-root-INFO: Regularization Change: 0.000 -> 2.583
2024-12-02-03:55:31-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-03:55:31-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-03:55:31-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-03:55:31-root-INFO: grad norm: 3.366 3.319 0.561
2024-12-02-03:55:32-root-INFO: grad norm: 3.057 3.031 0.399
2024-12-02-03:55:33-root-INFO: grad norm: 2.749 2.728 0.344
2024-12-02-03:55:34-root-INFO: grad norm: 2.628 2.610 0.311
2024-12-02-03:55:35-root-INFO: grad norm: 2.571 2.552 0.309
2024-12-02-03:55:36-root-INFO: Loss Change: 30.857 -> 28.958
2024-12-02-03:55:36-root-INFO: Regularization Change: 0.000 -> 2.348
2024-12-02-03:55:36-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-03:55:36-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-03:55:36-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-03:55:36-root-INFO: grad norm: 2.247 2.234 0.235
2024-12-02-03:55:37-root-INFO: grad norm: 2.029 2.017 0.214
2024-12-02-03:55:38-root-INFO: grad norm: 2.009 1.997 0.216
2024-12-02-03:55:39-root-INFO: grad norm: 2.054 2.041 0.228
2024-12-02-03:55:40-root-INFO: grad norm: 2.071 2.058 0.230
2024-12-02-03:55:41-root-INFO: Loss Change: 28.639 -> 27.083
2024-12-02-03:55:41-root-INFO: Regularization Change: 0.000 -> 1.923
2024-12-02-03:55:41-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-03:55:41-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-03:55:41-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-03:55:42-root-INFO: grad norm: 2.864 2.823 0.485
2024-12-02-03:55:43-root-INFO: grad norm: 2.591 2.571 0.322
2024-12-02-03:55:44-root-INFO: grad norm: 2.342 2.325 0.283
2024-12-02-03:55:45-root-INFO: grad norm: 2.252 2.237 0.260
2024-12-02-03:55:46-root-INFO: grad norm: 2.208 2.194 0.254
2024-12-02-03:55:46-root-INFO: Loss Change: 26.933 -> 25.465
2024-12-02-03:55:46-root-INFO: Regularization Change: 0.000 -> 1.876
2024-12-02-03:55:46-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-03:55:46-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-03:55:47-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-03:55:47-root-INFO: grad norm: 1.980 1.965 0.248
2024-12-02-03:55:48-root-INFO: grad norm: 1.729 1.720 0.169
2024-12-02-03:55:49-root-INFO: grad norm: 1.695 1.686 0.172
2024-12-02-03:55:50-root-INFO: grad norm: 1.690 1.680 0.178
2024-12-02-03:55:51-root-INFO: grad norm: 1.715 1.705 0.184
2024-12-02-03:55:52-root-INFO: Loss Change: 25.217 -> 23.908
2024-12-02-03:55:52-root-INFO: Regularization Change: 0.000 -> 1.621
2024-12-02-03:55:52-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-03:55:52-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-03:55:52-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-03:55:52-root-INFO: grad norm: 2.403 2.369 0.406
2024-12-02-03:55:53-root-INFO: grad norm: 2.195 2.178 0.268
2024-12-02-03:55:54-root-INFO: grad norm: 2.044 2.030 0.235
2024-12-02-03:55:55-root-INFO: grad norm: 1.998 1.985 0.229
2024-12-02-03:55:56-root-INFO: grad norm: 2.011 1.999 0.219
2024-12-02-03:55:57-root-INFO: Loss Change: 23.860 -> 22.682
2024-12-02-03:55:57-root-INFO: Regularization Change: 0.000 -> 1.626
2024-12-02-03:55:57-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-03:55:57-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-03:55:57-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-03:55:58-root-INFO: grad norm: 1.806 1.793 0.220
2024-12-02-03:55:58-root-INFO: grad norm: 1.561 1.554 0.148
2024-12-02-03:55:59-root-INFO: grad norm: 1.536 1.528 0.158
2024-12-02-03:56:00-root-INFO: grad norm: 1.553 1.545 0.155
2024-12-02-03:56:01-root-INFO: grad norm: 1.570 1.561 0.168
2024-12-02-03:56:02-root-INFO: Loss Change: 22.395 -> 21.245
2024-12-02-03:56:02-root-INFO: Regularization Change: 0.000 -> 1.460
2024-12-02-03:56:02-root-INFO: Undo step: 30
2024-12-02-03:56:02-root-INFO: Undo step: 31
2024-12-02-03:56:02-root-INFO: Undo step: 32
2024-12-02-03:56:02-root-INFO: Undo step: 33
2024-12-02-03:56:02-root-INFO: Undo step: 34
2024-12-02-03:56:02-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-03:56:03-root-INFO: grad norm: 15.703 15.634 1.466
2024-12-02-03:56:04-root-INFO: grad norm: 8.482 8.422 1.008
2024-12-02-03:56:05-root-INFO: grad norm: 5.871 5.825 0.736
2024-12-02-03:56:06-root-INFO: grad norm: 4.605 4.565 0.603
2024-12-02-03:56:07-root-INFO: grad norm: 3.950 3.916 0.511
2024-12-02-03:56:07-root-INFO: Loss Change: 106.902 -> 40.538
2024-12-02-03:56:07-root-INFO: Regularization Change: 0.000 -> 62.864
2024-12-02-03:56:07-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-03:56:07-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-03:56:08-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-03:56:08-root-INFO: grad norm: 3.613 3.584 0.460
2024-12-02-03:56:09-root-INFO: grad norm: 3.264 3.242 0.386
2024-12-02-03:56:10-root-INFO: grad norm: 3.120 3.098 0.372
2024-12-02-03:56:11-root-INFO: grad norm: 3.046 3.027 0.346
2024-12-02-03:56:12-root-INFO: grad norm: 3.014 2.993 0.352
2024-12-02-03:56:13-root-INFO: Loss Change: 40.159 -> 32.820
2024-12-02-03:56:13-root-INFO: Regularization Change: 0.000 -> 9.039
2024-12-02-03:56:13-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-03:56:13-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-03:56:13-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-03:56:13-root-INFO: grad norm: 3.399 3.361 0.509
2024-12-02-03:56:14-root-INFO: grad norm: 3.109 3.085 0.387
2024-12-02-03:56:15-root-INFO: grad norm: 2.847 2.828 0.329
2024-12-02-03:56:16-root-INFO: grad norm: 2.721 2.702 0.321
2024-12-02-03:56:17-root-INFO: grad norm: 2.645 2.629 0.295
2024-12-02-03:56:18-root-INFO: Loss Change: 32.516 -> 28.726
2024-12-02-03:56:18-root-INFO: Regularization Change: 0.000 -> 4.722
2024-12-02-03:56:18-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-03:56:18-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-03:56:18-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-03:56:19-root-INFO: grad norm: 2.682 2.664 0.313
2024-12-02-03:56:20-root-INFO: grad norm: 2.320 2.308 0.229
2024-12-02-03:56:21-root-INFO: grad norm: 2.089 2.076 0.234
2024-12-02-03:56:22-root-INFO: grad norm: 2.116 2.104 0.225
2024-12-02-03:56:23-root-INFO: grad norm: 2.177 2.163 0.242
2024-12-02-03:56:23-root-INFO: Loss Change: 28.467 -> 25.978
2024-12-02-03:56:23-root-INFO: Regularization Change: 0.000 -> 3.157
2024-12-02-03:56:23-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-03:56:23-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-03:56:24-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-03:56:24-root-INFO: grad norm: 2.728 2.695 0.423
2024-12-02-03:56:25-root-INFO: grad norm: 2.557 2.538 0.307
2024-12-02-03:56:26-root-INFO: grad norm: 2.389 2.375 0.256
2024-12-02-03:56:27-root-INFO: grad norm: 2.325 2.311 0.255
2024-12-02-03:56:28-root-INFO: grad norm: 2.258 2.247 0.231
2024-12-02-03:56:29-root-INFO: Loss Change: 25.874 -> 23.947
2024-12-02-03:56:29-root-INFO: Regularization Change: 0.000 -> 2.525
2024-12-02-03:56:29-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-03:56:29-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-03:56:29-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-03:56:29-root-INFO: grad norm: 2.318 2.303 0.262
2024-12-02-03:56:30-root-INFO: grad norm: 1.998 1.990 0.181
2024-12-02-03:56:31-root-INFO: grad norm: 1.756 1.746 0.183
2024-12-02-03:56:32-root-INFO: grad norm: 1.766 1.758 0.169
2024-12-02-03:56:33-root-INFO: grad norm: 1.851 1.841 0.186
2024-12-02-03:56:34-root-INFO: Loss Change: 23.679 -> 22.113
2024-12-02-03:56:34-root-INFO: Regularization Change: 0.000 -> 2.073
2024-12-02-03:56:34-root-INFO: Undo step: 30
2024-12-02-03:56:34-root-INFO: Undo step: 31
2024-12-02-03:56:34-root-INFO: Undo step: 32
2024-12-02-03:56:34-root-INFO: Undo step: 33
2024-12-02-03:56:34-root-INFO: Undo step: 34
2024-12-02-03:56:34-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-03:56:35-root-INFO: grad norm: 15.647 15.560 1.656
2024-12-02-03:56:36-root-INFO: grad norm: 8.610 8.541 1.089
2024-12-02-03:56:37-root-INFO: grad norm: 6.059 6.000 0.847
2024-12-02-03:56:38-root-INFO: grad norm: 4.733 4.684 0.678
2024-12-02-03:56:39-root-INFO: grad norm: 3.924 3.881 0.583
2024-12-02-03:56:39-root-INFO: Loss Change: 104.837 -> 40.489
2024-12-02-03:56:39-root-INFO: Regularization Change: 0.000 -> 60.871
2024-12-02-03:56:39-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-03:56:39-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-03:56:40-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-03:56:40-root-INFO: grad norm: 3.661 3.619 0.556
2024-12-02-03:56:41-root-INFO: grad norm: 3.149 3.117 0.448
2024-12-02-03:56:42-root-INFO: grad norm: 2.843 2.817 0.384
2024-12-02-03:56:43-root-INFO: grad norm: 2.670 2.646 0.356
2024-12-02-03:56:44-root-INFO: grad norm: 2.581 2.561 0.324
2024-12-02-03:56:44-root-INFO: Loss Change: 40.165 -> 32.655
2024-12-02-03:56:44-root-INFO: Regularization Change: 0.000 -> 8.993
2024-12-02-03:56:44-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-03:56:44-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-03:56:45-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-03:56:45-root-INFO: grad norm: 2.945 2.911 0.443
2024-12-02-03:56:46-root-INFO: grad norm: 2.787 2.766 0.344
2024-12-02-03:56:47-root-INFO: grad norm: 2.706 2.688 0.315
2024-12-02-03:56:48-root-INFO: grad norm: 2.604 2.585 0.312
2024-12-02-03:56:49-root-INFO: grad norm: 2.456 2.440 0.283
2024-12-02-03:56:50-root-INFO: Loss Change: 32.242 -> 28.578
2024-12-02-03:56:50-root-INFO: Regularization Change: 0.000 -> 4.680
2024-12-02-03:56:50-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-03:56:50-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-03:56:50-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-03:56:50-root-INFO: grad norm: 2.449 2.429 0.316
2024-12-02-03:56:52-root-INFO: grad norm: 2.163 2.151 0.226
2024-12-02-03:56:52-root-INFO: grad norm: 2.105 2.093 0.230
2024-12-02-03:56:53-root-INFO: grad norm: 2.077 2.065 0.217
2024-12-02-03:56:54-root-INFO: grad norm: 2.052 2.039 0.228
2024-12-02-03:56:55-root-INFO: Loss Change: 28.300 -> 25.717
2024-12-02-03:56:55-root-INFO: Regularization Change: 0.000 -> 3.198
2024-12-02-03:56:55-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-03:56:55-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-03:56:56-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-03:56:56-root-INFO: grad norm: 2.460 2.428 0.393
2024-12-02-03:56:57-root-INFO: grad norm: 2.231 2.212 0.286
2024-12-02-03:56:58-root-INFO: grad norm: 2.107 2.093 0.242
2024-12-02-03:56:59-root-INFO: grad norm: 2.070 2.055 0.248
2024-12-02-03:57:00-root-INFO: grad norm: 2.096 2.083 0.236
2024-12-02-03:57:00-root-INFO: Loss Change: 25.566 -> 23.679
2024-12-02-03:57:01-root-INFO: Regularization Change: 0.000 -> 2.531
2024-12-02-03:57:01-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-03:57:01-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-03:57:01-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-03:57:01-root-INFO: grad norm: 2.044 2.029 0.248
2024-12-02-03:57:02-root-INFO: grad norm: 1.862 1.853 0.188
2024-12-02-03:57:03-root-INFO: grad norm: 1.870 1.859 0.203
2024-12-02-03:57:04-root-INFO: grad norm: 1.940 1.929 0.202
2024-12-02-03:57:05-root-INFO: grad norm: 1.963 1.950 0.219
2024-12-02-03:57:06-root-INFO: Loss Change: 23.352 -> 21.788
2024-12-02-03:57:06-root-INFO: Regularization Change: 0.000 -> 2.085
2024-12-02-03:57:06-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-03:57:06-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-03:57:06-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-03:57:06-root-INFO: grad norm: 2.694 2.656 0.452
2024-12-02-03:57:07-root-INFO: grad norm: 2.331 2.312 0.300
2024-12-02-03:57:08-root-INFO: grad norm: 1.983 1.969 0.236
2024-12-02-03:57:09-root-INFO: grad norm: 1.867 1.854 0.225
2024-12-02-03:57:10-root-INFO: grad norm: 1.811 1.799 0.204
2024-12-02-03:57:11-root-INFO: Loss Change: 21.761 -> 20.245
2024-12-02-03:57:11-root-INFO: Regularization Change: 0.000 -> 1.938
2024-12-02-03:57:11-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-03:57:11-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-03:57:11-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-03:57:12-root-INFO: grad norm: 1.708 1.691 0.238
2024-12-02-03:57:13-root-INFO: grad norm: 1.431 1.424 0.144
2024-12-02-03:57:14-root-INFO: grad norm: 1.407 1.399 0.144
2024-12-02-03:57:15-root-INFO: grad norm: 1.457 1.450 0.144
2024-12-02-03:57:16-root-INFO: grad norm: 1.479 1.470 0.156
2024-12-02-03:57:16-root-INFO: Loss Change: 19.855 -> 18.617
2024-12-02-03:57:16-root-INFO: Regularization Change: 0.000 -> 1.624
2024-12-02-03:57:16-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-03:57:16-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-03:57:17-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-03:57:17-root-INFO: grad norm: 2.211 2.181 0.361
2024-12-02-03:57:18-root-INFO: grad norm: 1.900 1.886 0.234
2024-12-02-03:57:19-root-INFO: grad norm: 1.583 1.573 0.180
2024-12-02-03:57:20-root-INFO: grad norm: 1.507 1.497 0.177
2024-12-02-03:57:21-root-INFO: grad norm: 1.490 1.481 0.157
2024-12-02-03:57:22-root-INFO: Loss Change: 18.572 -> 17.405
2024-12-02-03:57:22-root-INFO: Regularization Change: 0.000 -> 1.553
2024-12-02-03:57:22-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-03:57:22-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-03:57:22-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-03:57:22-root-INFO: grad norm: 1.528 1.509 0.239
2024-12-02-03:57:23-root-INFO: grad norm: 1.235 1.229 0.125
2024-12-02-03:57:24-root-INFO: grad norm: 1.217 1.211 0.120
2024-12-02-03:57:25-root-INFO: grad norm: 1.272 1.267 0.118
2024-12-02-03:57:26-root-INFO: grad norm: 1.296 1.289 0.128
2024-12-02-03:57:27-root-INFO: Loss Change: 17.192 -> 16.153
2024-12-02-03:57:27-root-INFO: Regularization Change: 0.000 -> 1.385
2024-12-02-03:57:27-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-03:57:27-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-03:57:27-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-03:57:28-root-INFO: grad norm: 2.073 2.044 0.346
2024-12-02-03:57:29-root-INFO: grad norm: 1.692 1.681 0.194
2024-12-02-03:57:30-root-INFO: grad norm: 1.327 1.319 0.145
2024-12-02-03:57:31-root-INFO: grad norm: 1.256 1.248 0.140
2024-12-02-03:57:32-root-INFO: grad norm: 1.250 1.244 0.124
2024-12-02-03:57:32-root-INFO: Loss Change: 15.993 -> 14.949
2024-12-02-03:57:32-root-INFO: Regularization Change: 0.000 -> 1.376
2024-12-02-03:57:32-root-INFO: Undo step: 25
2024-12-02-03:57:32-root-INFO: Undo step: 26
2024-12-02-03:57:32-root-INFO: Undo step: 27
2024-12-02-03:57:32-root-INFO: Undo step: 28
2024-12-02-03:57:32-root-INFO: Undo step: 29
2024-12-02-03:57:33-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-03:57:33-root-INFO: grad norm: 14.362 14.300 1.334
2024-12-02-03:57:34-root-INFO: grad norm: 8.464 8.417 0.893
2024-12-02-03:57:35-root-INFO: grad norm: 5.488 5.450 0.648
2024-12-02-03:57:36-root-INFO: grad norm: 4.473 4.443 0.516
2024-12-02-03:57:37-root-INFO: grad norm: 3.860 3.834 0.446
2024-12-02-03:57:38-root-INFO: Loss Change: 94.992 -> 32.706
2024-12-02-03:57:38-root-INFO: Regularization Change: 0.000 -> 65.117
2024-12-02-03:57:38-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-03:57:38-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-03:57:38-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-03:57:38-root-INFO: grad norm: 3.683 3.649 0.503
2024-12-02-03:57:39-root-INFO: grad norm: 3.135 3.112 0.376
2024-12-02-03:57:40-root-INFO: grad norm: 2.774 2.756 0.316
2024-12-02-03:57:41-root-INFO: grad norm: 2.511 2.493 0.296
2024-12-02-03:57:42-root-INFO: grad norm: 2.393 2.378 0.269
2024-12-02-03:57:43-root-INFO: Loss Change: 32.463 -> 25.262
2024-12-02-03:57:43-root-INFO: Regularization Change: 0.000 -> 9.275
2024-12-02-03:57:43-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-03:57:43-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-03:57:43-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-03:57:44-root-INFO: grad norm: 2.261 2.240 0.305
2024-12-02-03:57:45-root-INFO: grad norm: 1.958 1.946 0.211
2024-12-02-03:57:46-root-INFO: grad norm: 1.858 1.846 0.209
2024-12-02-03:57:47-root-INFO: grad norm: 1.836 1.826 0.195
2024-12-02-03:57:48-root-INFO: grad norm: 1.794 1.783 0.202
2024-12-02-03:57:48-root-INFO: Loss Change: 24.750 -> 21.349
2024-12-02-03:57:48-root-INFO: Regularization Change: 0.000 -> 4.500
2024-12-02-03:57:48-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-03:57:48-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-03:57:49-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-03:57:49-root-INFO: grad norm: 2.232 2.202 0.363
2024-12-02-03:57:50-root-INFO: grad norm: 1.942 1.927 0.243
2024-12-02-03:57:51-root-INFO: grad norm: 1.738 1.727 0.193
2024-12-02-03:57:52-root-INFO: grad norm: 1.653 1.642 0.195
2024-12-02-03:57:53-root-INFO: grad norm: 1.641 1.632 0.174
2024-12-02-03:57:54-root-INFO: Loss Change: 21.212 -> 18.977
2024-12-02-03:57:54-root-INFO: Regularization Change: 0.000 -> 3.005
2024-12-02-03:57:54-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-03:57:54-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-03:57:54-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-03:57:54-root-INFO: grad norm: 1.678 1.659 0.255
2024-12-02-03:57:55-root-INFO: grad norm: 1.386 1.379 0.138
2024-12-02-03:57:56-root-INFO: grad norm: 1.343 1.336 0.143
2024-12-02-03:57:57-root-INFO: grad norm: 1.361 1.355 0.131
2024-12-02-03:57:58-root-INFO: grad norm: 1.349 1.342 0.145
2024-12-02-03:57:59-root-INFO: Loss Change: 18.705 -> 17.050
2024-12-02-03:57:59-root-INFO: Regularization Change: 0.000 -> 2.214
2024-12-02-03:57:59-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-03:57:59-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-03:57:59-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-03:58:00-root-INFO: grad norm: 1.915 1.885 0.336
2024-12-02-03:58:01-root-INFO: grad norm: 1.569 1.557 0.194
2024-12-02-03:58:01-root-INFO: grad norm: 1.343 1.335 0.146
2024-12-02-03:58:02-root-INFO: grad norm: 1.279 1.270 0.147
2024-12-02-03:58:03-root-INFO: grad norm: 1.281 1.274 0.128
2024-12-02-03:58:04-root-INFO: Loss Change: 16.811 -> 15.436
2024-12-02-03:58:04-root-INFO: Regularization Change: 0.000 -> 1.853
2024-12-02-03:58:04-root-INFO: Undo step: 25
2024-12-02-03:58:04-root-INFO: Undo step: 26
2024-12-02-03:58:04-root-INFO: Undo step: 27
2024-12-02-03:58:04-root-INFO: Undo step: 28
2024-12-02-03:58:04-root-INFO: Undo step: 29
2024-12-02-03:58:04-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-03:58:05-root-INFO: grad norm: 14.271 14.216 1.254
2024-12-02-03:58:06-root-INFO: grad norm: 8.452 8.415 0.795
2024-12-02-03:58:07-root-INFO: grad norm: 6.106 6.060 0.743
2024-12-02-03:58:08-root-INFO: grad norm: 4.705 4.676 0.524
2024-12-02-03:58:09-root-INFO: grad norm: 3.972 3.943 0.479
2024-12-02-03:58:09-root-INFO: Loss Change: 92.919 -> 32.803
2024-12-02-03:58:09-root-INFO: Regularization Change: 0.000 -> 62.058
2024-12-02-03:58:09-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-03:58:09-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-03:58:10-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-03:58:10-root-INFO: grad norm: 3.912 3.871 0.567
2024-12-02-03:58:11-root-INFO: grad norm: 3.344 3.317 0.419
2024-12-02-03:58:12-root-INFO: grad norm: 2.937 2.916 0.351
2024-12-02-03:58:13-root-INFO: grad norm: 2.626 2.607 0.314
2024-12-02-03:58:14-root-INFO: grad norm: 2.390 2.372 0.286
2024-12-02-03:58:15-root-INFO: Loss Change: 32.611 -> 25.241
2024-12-02-03:58:15-root-INFO: Regularization Change: 0.000 -> 9.365
2024-12-02-03:58:15-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-03:58:15-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-03:58:15-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-03:58:15-root-INFO: grad norm: 2.169 2.151 0.280
2024-12-02-03:58:16-root-INFO: grad norm: 1.851 1.839 0.203
2024-12-02-03:58:17-root-INFO: grad norm: 1.713 1.702 0.191
2024-12-02-03:58:18-root-INFO: grad norm: 1.612 1.602 0.182
2024-12-02-03:58:19-root-INFO: grad norm: 1.531 1.521 0.173
2024-12-02-03:58:20-root-INFO: Loss Change: 24.715 -> 21.237
2024-12-02-03:58:20-root-INFO: Regularization Change: 0.000 -> 4.537
2024-12-02-03:58:20-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-03:58:20-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-03:58:20-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-03:58:20-root-INFO: grad norm: 1.871 1.843 0.320
2024-12-02-03:58:21-root-INFO: grad norm: 1.598 1.586 0.196
2024-12-02-03:58:22-root-INFO: grad norm: 1.487 1.477 0.175
2024-12-02-03:58:23-root-INFO: grad norm: 1.406 1.397 0.162
2024-12-02-03:58:24-root-INFO: grad norm: 1.340 1.331 0.153
2024-12-02-03:58:25-root-INFO: Loss Change: 21.056 -> 18.793
2024-12-02-03:58:25-root-INFO: Regularization Change: 0.000 -> 2.976
2024-12-02-03:58:25-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-03:58:25-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-03:58:25-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-03:58:26-root-INFO: grad norm: 1.556 1.533 0.267
2024-12-02-03:58:27-root-INFO: grad norm: 1.261 1.253 0.142
2024-12-02-03:58:28-root-INFO: grad norm: 1.243 1.236 0.131
2024-12-02-03:58:29-root-INFO: grad norm: 1.245 1.238 0.127
2024-12-02-03:58:30-root-INFO: grad norm: 1.302 1.296 0.124
2024-12-02-03:58:30-root-INFO: Loss Change: 18.549 -> 16.919
2024-12-02-03:58:30-root-INFO: Regularization Change: 0.000 -> 2.241
2024-12-02-03:58:30-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-03:58:30-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-03:58:31-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-03:58:31-root-INFO: grad norm: 1.592 1.570 0.266
2024-12-02-03:58:32-root-INFO: grad norm: 1.324 1.317 0.133
2024-12-02-03:58:33-root-INFO: grad norm: 1.284 1.278 0.122
2024-12-02-03:58:34-root-INFO: grad norm: 1.283 1.278 0.115
2024-12-02-03:58:35-root-INFO: grad norm: 1.282 1.277 0.113
2024-12-02-03:58:36-root-INFO: Loss Change: 16.602 -> 15.243
2024-12-02-03:58:36-root-INFO: Regularization Change: 0.000 -> 1.854
2024-12-02-03:58:36-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-03:58:36-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-03:58:36-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-03:58:36-root-INFO: grad norm: 1.960 1.940 0.280
2024-12-02-03:58:37-root-INFO: grad norm: 1.547 1.540 0.139
2024-12-02-03:58:38-root-INFO: grad norm: 1.172 1.167 0.112
2024-12-02-03:58:39-root-INFO: grad norm: 1.128 1.123 0.104
2024-12-02-03:58:40-root-INFO: grad norm: 1.170 1.166 0.102
2024-12-02-03:58:41-root-INFO: Loss Change: 15.179 -> 13.960
2024-12-02-03:58:41-root-INFO: Regularization Change: 0.000 -> 1.639
2024-12-02-03:58:41-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-03:58:41-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-03:58:41-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-03:58:42-root-INFO: grad norm: 1.560 1.540 0.252
2024-12-02-03:58:43-root-INFO: grad norm: 1.295 1.289 0.123
2024-12-02-03:58:44-root-INFO: grad norm: 1.264 1.260 0.103
2024-12-02-03:58:45-root-INFO: grad norm: 1.314 1.310 0.101
2024-12-02-03:58:46-root-INFO: grad norm: 1.308 1.304 0.098
2024-12-02-03:58:46-root-INFO: Loss Change: 13.707 -> 12.689
2024-12-02-03:58:46-root-INFO: Regularization Change: 0.000 -> 1.442
2024-12-02-03:58:46-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-03:58:46-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-03:58:47-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-03:58:47-root-INFO: grad norm: 1.942 1.923 0.268
2024-12-02-03:58:48-root-INFO: grad norm: 1.508 1.503 0.128
2024-12-02-03:58:49-root-INFO: grad norm: 1.143 1.139 0.096
2024-12-02-03:58:50-root-INFO: grad norm: 1.097 1.094 0.089
2024-12-02-03:58:51-root-INFO: grad norm: 1.140 1.137 0.089
2024-12-02-03:58:52-root-INFO: Loss Change: 12.617 -> 11.641
2024-12-02-03:58:52-root-INFO: Regularization Change: 0.000 -> 1.338
2024-12-02-03:58:52-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-03:58:52-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-03:58:52-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-03:58:52-root-INFO: grad norm: 1.569 1.551 0.235
2024-12-02-03:58:53-root-INFO: grad norm: 1.291 1.286 0.111
2024-12-02-03:58:54-root-INFO: grad norm: 1.191 1.187 0.088
2024-12-02-03:58:55-root-INFO: grad norm: 1.290 1.287 0.087
2024-12-02-03:58:56-root-INFO: grad norm: 1.274 1.271 0.088
2024-12-02-03:58:57-root-INFO: Loss Change: 11.490 -> 10.640
2024-12-02-03:58:57-root-INFO: Regularization Change: 0.000 -> 1.227
2024-12-02-03:58:57-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-03:58:57-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-03:58:57-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-03:58:58-root-INFO: grad norm: 1.844 1.824 0.269
2024-12-02-03:58:59-root-INFO: grad norm: 1.449 1.444 0.116
2024-12-02-03:59:00-root-INFO: grad norm: 1.257 1.254 0.094
2024-12-02-03:59:01-root-INFO: grad norm: 1.229 1.225 0.089
2024-12-02-03:59:02-root-INFO: grad norm: 1.180 1.177 0.082
2024-12-02-03:59:02-root-INFO: Loss Change: 10.592 -> 9.741
2024-12-02-03:59:02-root-INFO: Regularization Change: 0.000 -> 1.202
2024-12-02-03:59:02-root-INFO: Undo step: 20
2024-12-02-03:59:02-root-INFO: Undo step: 21
2024-12-02-03:59:02-root-INFO: Undo step: 22
2024-12-02-03:59:02-root-INFO: Undo step: 23
2024-12-02-03:59:02-root-INFO: Undo step: 24
2024-12-02-03:59:03-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-03:59:03-root-INFO: grad norm: 13.560 13.515 1.108
2024-12-02-03:59:04-root-INFO: grad norm: 7.304 7.268 0.719
2024-12-02-03:59:05-root-INFO: grad norm: 5.025 4.992 0.571
2024-12-02-03:59:06-root-INFO: grad norm: 3.886 3.861 0.440
2024-12-02-03:59:07-root-INFO: grad norm: 3.200 3.177 0.385
2024-12-02-03:59:08-root-INFO: Loss Change: 83.387 -> 25.099
2024-12-02-03:59:08-root-INFO: Regularization Change: 0.000 -> 64.682
2024-12-02-03:59:08-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-03:59:08-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-03:59:08-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-03:59:08-root-INFO: grad norm: 3.084 3.059 0.396
2024-12-02-03:59:09-root-INFO: grad norm: 2.595 2.577 0.310
2024-12-02-03:59:10-root-INFO: grad norm: 2.300 2.286 0.254
2024-12-02-03:59:11-root-INFO: grad norm: 2.091 2.077 0.245
2024-12-02-03:59:12-root-INFO: grad norm: 1.937 1.925 0.210
2024-12-02-03:59:13-root-INFO: Loss Change: 24.837 -> 18.432
2024-12-02-03:59:13-root-INFO: Regularization Change: 0.000 -> 8.955
2024-12-02-03:59:13-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-03:59:13-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-03:59:13-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-03:59:14-root-INFO: grad norm: 1.940 1.922 0.264
2024-12-02-03:59:15-root-INFO: grad norm: 1.650 1.640 0.182
2024-12-02-03:59:16-root-INFO: grad norm: 1.547 1.538 0.164
2024-12-02-03:59:17-root-INFO: grad norm: 1.480 1.473 0.152
2024-12-02-03:59:17-root-INFO: grad norm: 1.425 1.417 0.147
2024-12-02-03:59:18-root-INFO: Loss Change: 17.959 -> 14.996
2024-12-02-03:59:18-root-INFO: Regularization Change: 0.000 -> 4.214
2024-12-02-03:59:18-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-03:59:18-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-03:59:19-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-03:59:19-root-INFO: grad norm: 1.943 1.923 0.283
2024-12-02-03:59:20-root-INFO: grad norm: 1.668 1.658 0.179
2024-12-02-03:59:21-root-INFO: grad norm: 1.520 1.513 0.142
2024-12-02-03:59:22-root-INFO: grad norm: 1.463 1.456 0.150
2024-12-02-03:59:23-root-INFO: grad norm: 1.425 1.419 0.128
2024-12-02-03:59:23-root-INFO: Loss Change: 14.824 -> 12.961
2024-12-02-03:59:23-root-INFO: Regularization Change: 0.000 -> 2.725
2024-12-02-03:59:23-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-03:59:23-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-03:59:24-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-03:59:24-root-INFO: grad norm: 1.516 1.501 0.215
2024-12-02-03:59:25-root-INFO: grad norm: 1.241 1.235 0.115
2024-12-02-03:59:26-root-INFO: grad norm: 1.206 1.200 0.111
2024-12-02-03:59:27-root-INFO: grad norm: 1.236 1.232 0.100
2024-12-02-03:59:28-root-INFO: grad norm: 1.243 1.238 0.113
2024-12-02-03:59:29-root-INFO: Loss Change: 12.677 -> 11.331
2024-12-02-03:59:29-root-INFO: Regularization Change: 0.000 -> 1.964
2024-12-02-03:59:29-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-03:59:29-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-03:59:29-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-03:59:29-root-INFO: grad norm: 2.057 2.032 0.321
2024-12-02-03:59:30-root-INFO: grad norm: 1.658 1.649 0.173
2024-12-02-03:59:31-root-INFO: grad norm: 1.376 1.371 0.120
2024-12-02-03:59:32-root-INFO: grad norm: 1.275 1.269 0.119
2024-12-02-03:59:33-root-INFO: grad norm: 1.256 1.252 0.102
2024-12-02-03:59:34-root-INFO: Loss Change: 11.265 -> 10.112
2024-12-02-03:59:34-root-INFO: Regularization Change: 0.000 -> 1.661
2024-12-02-03:59:34-root-INFO: Undo step: 20
2024-12-02-03:59:34-root-INFO: Undo step: 21
2024-12-02-03:59:34-root-INFO: Undo step: 22
2024-12-02-03:59:34-root-INFO: Undo step: 23
2024-12-02-03:59:34-root-INFO: Undo step: 24
2024-12-02-03:59:34-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-03:59:35-root-INFO: grad norm: 13.564 13.515 1.147
2024-12-02-03:59:36-root-INFO: grad norm: 7.247 7.209 0.742
2024-12-02-03:59:37-root-INFO: grad norm: 4.985 4.956 0.543
2024-12-02-03:59:38-root-INFO: grad norm: 3.863 3.841 0.417
2024-12-02-03:59:39-root-INFO: grad norm: 3.188 3.169 0.355
2024-12-02-03:59:39-root-INFO: Loss Change: 84.509 -> 26.039
2024-12-02-03:59:39-root-INFO: Regularization Change: 0.000 -> 65.192
2024-12-02-03:59:39-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-03:59:39-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-03:59:40-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-03:59:40-root-INFO: grad norm: 3.150 3.126 0.384
2024-12-02-03:59:41-root-INFO: grad norm: 2.725 2.706 0.319
2024-12-02-03:59:42-root-INFO: grad norm: 2.488 2.473 0.272
2024-12-02-03:59:43-root-INFO: grad norm: 2.335 2.317 0.285
2024-12-02-03:59:44-root-INFO: grad norm: 2.259 2.246 0.249
2024-12-02-03:59:44-root-INFO: Loss Change: 25.739 -> 19.146
2024-12-02-03:59:44-root-INFO: Regularization Change: 0.000 -> 9.451
2024-12-02-03:59:44-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-03:59:44-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-03:59:45-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-03:59:45-root-INFO: grad norm: 2.154 2.138 0.256
2024-12-02-03:59:46-root-INFO: grad norm: 1.880 1.870 0.197
2024-12-02-03:59:47-root-INFO: grad norm: 1.801 1.790 0.201
2024-12-02-03:59:48-root-INFO: grad norm: 1.767 1.758 0.179
2024-12-02-03:59:49-root-INFO: grad norm: 1.714 1.704 0.193
2024-12-02-03:59:50-root-INFO: Loss Change: 18.566 -> 15.419
2024-12-02-03:59:50-root-INFO: Regularization Change: 0.000 -> 4.498
2024-12-02-03:59:50-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-03:59:50-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-03:59:50-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-03:59:50-root-INFO: grad norm: 2.383 2.357 0.349
2024-12-02-03:59:51-root-INFO: grad norm: 2.003 1.987 0.252
2024-12-02-03:59:52-root-INFO: grad norm: 1.647 1.637 0.183
2024-12-02-03:59:53-root-INFO: grad norm: 1.570 1.559 0.190
2024-12-02-03:59:54-root-INFO: grad norm: 1.543 1.535 0.158
2024-12-02-03:59:55-root-INFO: Loss Change: 15.273 -> 13.210
2024-12-02-03:59:55-root-INFO: Regularization Change: 0.000 -> 2.958
2024-12-02-03:59:55-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-03:59:55-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-03:59:55-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-03:59:56-root-INFO: grad norm: 1.538 1.525 0.196
2024-12-02-03:59:57-root-INFO: grad norm: 1.289 1.283 0.118
2024-12-02-03:59:58-root-INFO: grad norm: 1.258 1.252 0.123
2024-12-02-03:59:59-root-INFO: grad norm: 1.264 1.259 0.106
2024-12-02-04:00:00-root-INFO: grad norm: 1.267 1.261 0.123
2024-12-02-04:00:00-root-INFO: Loss Change: 12.867 -> 11.439
2024-12-02-04:00:00-root-INFO: Regularization Change: 0.000 -> 2.091
2024-12-02-04:00:00-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-04:00:00-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-04:00:01-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-04:00:01-root-INFO: grad norm: 2.075 2.050 0.323
2024-12-02-04:00:02-root-INFO: grad norm: 1.731 1.720 0.194
2024-12-02-04:00:03-root-INFO: grad norm: 1.478 1.472 0.139
2024-12-02-04:00:04-root-INFO: grad norm: 1.421 1.413 0.152
2024-12-02-04:00:05-root-INFO: grad norm: 1.423 1.417 0.126
2024-12-02-04:00:06-root-INFO: Loss Change: 11.345 -> 10.172
2024-12-02-04:00:06-root-INFO: Regularization Change: 0.000 -> 1.768
2024-12-02-04:00:06-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-04:00:06-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-04:00:06-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-04:00:06-root-INFO: grad norm: 1.380 1.369 0.171
2024-12-02-04:00:07-root-INFO: grad norm: 1.096 1.092 0.087
2024-12-02-04:00:08-root-INFO: grad norm: 1.096 1.091 0.099
2024-12-02-04:00:09-root-INFO: grad norm: 1.199 1.196 0.088
2024-12-02-04:00:10-root-INFO: grad norm: 1.203 1.198 0.111
2024-12-02-04:00:11-root-INFO: Loss Change: 9.890 -> 8.959
2024-12-02-04:00:11-root-INFO: Regularization Change: 0.000 -> 1.393
2024-12-02-04:00:11-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-04:00:11-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-04:00:11-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-04:00:12-root-INFO: grad norm: 2.053 2.028 0.323
2024-12-02-04:00:13-root-INFO: grad norm: 1.635 1.625 0.178
2024-12-02-04:00:14-root-INFO: grad norm: 1.299 1.294 0.113
2024-12-02-04:00:15-root-INFO: grad norm: 1.242 1.236 0.124
2024-12-02-04:00:16-root-INFO: grad norm: 1.382 1.378 0.104
2024-12-02-04:00:16-root-INFO: Loss Change: 8.944 -> 8.074
2024-12-02-04:00:16-root-INFO: Regularization Change: 0.000 -> 1.357
2024-12-02-04:00:16-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-04:00:16-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-04:00:17-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-04:00:17-root-INFO: grad norm: 1.288 1.276 0.174
2024-12-02-04:00:18-root-INFO: grad norm: 0.904 0.901 0.074
2024-12-02-04:00:19-root-INFO: grad norm: 0.841 0.839 0.067
2024-12-02-04:00:20-root-INFO: grad norm: 0.829 0.827 0.060
2024-12-02-04:00:21-root-INFO: grad norm: 0.829 0.826 0.068
2024-12-02-04:00:22-root-INFO: Loss Change: 7.900 -> 7.118
2024-12-02-04:00:22-root-INFO: Regularization Change: 0.000 -> 1.101
2024-12-02-04:00:22-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-04:00:22-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-04:00:22-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-04:00:22-root-INFO: grad norm: 1.626 1.605 0.263
2024-12-02-04:00:23-root-INFO: grad norm: 1.306 1.298 0.141
2024-12-02-04:00:24-root-INFO: grad norm: 1.159 1.155 0.089
2024-12-02-04:00:25-root-INFO: grad norm: 1.129 1.124 0.106
2024-12-02-04:00:26-root-INFO: grad norm: 1.165 1.162 0.079
2024-12-02-04:00:27-root-INFO: Loss Change: 7.115 -> 6.463
2024-12-02-04:00:27-root-INFO: Regularization Change: 0.000 -> 1.072
2024-12-02-04:00:27-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-04:00:27-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-04:00:27-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-04:00:28-root-INFO: grad norm: 1.273 1.257 0.200
2024-12-02-04:00:29-root-INFO: grad norm: 0.774 0.771 0.070
2024-12-02-04:00:30-root-INFO: grad norm: 0.721 0.718 0.056
2024-12-02-04:00:31-root-INFO: grad norm: 0.840 0.839 0.051
2024-12-02-04:00:32-root-INFO: grad norm: 0.860 0.857 0.067
2024-12-02-04:00:32-root-INFO: Loss Change: 6.365 -> 5.694
2024-12-02-04:00:32-root-INFO: Regularization Change: 0.000 -> 0.954
2024-12-02-04:00:32-root-INFO: Undo step: 15
2024-12-02-04:00:32-root-INFO: Undo step: 16
2024-12-02-04:00:32-root-INFO: Undo step: 17
2024-12-02-04:00:32-root-INFO: Undo step: 18
2024-12-02-04:00:32-root-INFO: Undo step: 19
2024-12-02-04:00:33-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-04:00:33-root-INFO: grad norm: 12.673 12.624 1.110
2024-12-02-04:00:34-root-INFO: grad norm: 6.870 6.839 0.657
2024-12-02-04:00:35-root-INFO: grad norm: 4.741 4.714 0.510
2024-12-02-04:00:36-root-INFO: grad norm: 3.667 3.647 0.379
2024-12-02-04:00:37-root-INFO: grad norm: 3.058 3.039 0.339
2024-12-02-04:00:38-root-INFO: Loss Change: 75.961 -> 19.282
2024-12-02-04:00:38-root-INFO: Regularization Change: 0.000 -> 68.066
2024-12-02-04:00:38-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-04:00:38-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-04:00:38-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-04:00:38-root-INFO: grad norm: 3.347 3.323 0.398
2024-12-02-04:00:39-root-INFO: grad norm: 2.523 2.505 0.295
2024-12-02-04:00:40-root-INFO: grad norm: 2.172 2.161 0.220
2024-12-02-04:00:41-root-INFO: grad norm: 1.953 1.942 0.211
2024-12-02-04:00:42-root-INFO: grad norm: 1.807 1.798 0.176
2024-12-02-04:00:43-root-INFO: Loss Change: 18.837 -> 13.003
2024-12-02-04:00:43-root-INFO: Regularization Change: 0.000 -> 8.805
2024-12-02-04:00:43-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-04:00:43-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-04:00:43-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-04:00:44-root-INFO: grad norm: 1.803 1.787 0.239
2024-12-02-04:00:45-root-INFO: grad norm: 1.476 1.469 0.143
2024-12-02-04:00:45-root-INFO: grad norm: 1.374 1.368 0.130
2024-12-02-04:00:46-root-INFO: grad norm: 1.324 1.319 0.117
2024-12-02-04:00:47-root-INFO: grad norm: 1.297 1.291 0.119
2024-12-02-04:00:48-root-INFO: Loss Change: 12.606 -> 10.120
2024-12-02-04:00:48-root-INFO: Regularization Change: 0.000 -> 3.829
2024-12-02-04:00:48-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-04:00:48-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-04:00:48-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-04:00:49-root-INFO: grad norm: 2.090 2.070 0.285
2024-12-02-04:00:50-root-INFO: grad norm: 1.621 1.613 0.161
2024-12-02-04:00:51-root-INFO: grad norm: 1.350 1.346 0.105
2024-12-02-04:00:52-root-INFO: grad norm: 1.237 1.232 0.111
2024-12-02-04:00:53-root-INFO: grad norm: 1.261 1.257 0.095
2024-12-02-04:00:53-root-INFO: Loss Change: 9.963 -> 8.429
2024-12-02-04:00:53-root-INFO: Regularization Change: 0.000 -> 2.382
2024-12-02-04:00:53-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-04:00:53-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-04:00:54-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-04:00:54-root-INFO: grad norm: 1.365 1.351 0.194
2024-12-02-04:00:55-root-INFO: grad norm: 1.023 1.019 0.085
2024-12-02-04:00:56-root-INFO: grad norm: 1.012 1.009 0.082
2024-12-02-04:00:57-root-INFO: grad norm: 1.253 1.251 0.075
2024-12-02-04:00:58-root-INFO: grad norm: 1.022 1.019 0.086
2024-12-02-04:00:59-root-INFO: Loss Change: 8.204 -> 7.137
2024-12-02-04:00:59-root-INFO: Regularization Change: 0.000 -> 1.626
2024-12-02-04:00:59-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-04:00:59-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-04:00:59-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-04:00:59-root-INFO: grad norm: 1.647 1.625 0.268
2024-12-02-04:01:00-root-INFO: grad norm: 1.179 1.173 0.117
2024-12-02-04:01:01-root-INFO: grad norm: 1.055 1.052 0.080
2024-12-02-04:01:02-root-INFO: grad norm: 1.039 1.035 0.090
2024-12-02-04:01:03-root-INFO: grad norm: 1.148 1.146 0.074
2024-12-02-04:01:04-root-INFO: Loss Change: 7.075 -> 6.199
2024-12-02-04:01:04-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-02-04:01:04-root-INFO: Undo step: 15
2024-12-02-04:01:04-root-INFO: Undo step: 16
2024-12-02-04:01:04-root-INFO: Undo step: 17
2024-12-02-04:01:04-root-INFO: Undo step: 18
2024-12-02-04:01:04-root-INFO: Undo step: 19
2024-12-02-04:01:04-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-04:01:05-root-INFO: grad norm: 13.628 13.589 1.038
2024-12-02-04:01:06-root-INFO: grad norm: 7.120 7.089 0.668
2024-12-02-04:01:07-root-INFO: grad norm: 4.792 4.767 0.487
2024-12-02-04:01:08-root-INFO: grad norm: 3.644 3.625 0.373
2024-12-02-04:01:09-root-INFO: grad norm: 2.969 2.953 0.315
2024-12-02-04:01:09-root-INFO: Loss Change: 80.741 -> 19.887
2024-12-02-04:01:09-root-INFO: Regularization Change: 0.000 -> 72.156
2024-12-02-04:01:09-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-04:01:09-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-04:01:10-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-04:01:10-root-INFO: grad norm: 2.802 2.780 0.344
2024-12-02-04:01:11-root-INFO: grad norm: 2.365 2.350 0.266
2024-12-02-04:01:12-root-INFO: grad norm: 2.136 2.125 0.219
2024-12-02-04:01:13-root-INFO: grad norm: 1.941 1.929 0.214
2024-12-02-04:01:14-root-INFO: grad norm: 1.774 1.764 0.183
2024-12-02-04:01:15-root-INFO: Loss Change: 19.337 -> 13.294
2024-12-02-04:01:15-root-INFO: Regularization Change: 0.000 -> 9.191
2024-12-02-04:01:15-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-04:01:15-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-04:01:15-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-04:01:15-root-INFO: grad norm: 1.790 1.774 0.235
2024-12-02-04:01:16-root-INFO: grad norm: 1.484 1.477 0.145
2024-12-02-04:01:17-root-INFO: grad norm: 1.386 1.380 0.135
2024-12-02-04:01:18-root-INFO: grad norm: 1.360 1.355 0.126
2024-12-02-04:01:19-root-INFO: grad norm: 1.273 1.267 0.125
2024-12-02-04:01:20-root-INFO: Loss Change: 12.858 -> 10.185
2024-12-02-04:01:20-root-INFO: Regularization Change: 0.000 -> 4.066
2024-12-02-04:01:20-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-04:01:20-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-04:01:20-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-04:01:21-root-INFO: grad norm: 1.772 1.750 0.279
2024-12-02-04:01:22-root-INFO: grad norm: 1.468 1.459 0.160
2024-12-02-04:01:23-root-INFO: grad norm: 1.334 1.328 0.121
2024-12-02-04:01:24-root-INFO: grad norm: 1.335 1.329 0.128
2024-12-02-04:01:25-root-INFO: grad norm: 1.609 1.604 0.122
2024-12-02-04:01:25-root-INFO: Loss Change: 9.981 -> 8.484
2024-12-02-04:01:25-root-INFO: Regularization Change: 0.000 -> 2.565
2024-12-02-04:01:25-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-04:01:25-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-04:01:26-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-04:01:26-root-INFO: grad norm: 1.414 1.402 0.185
2024-12-02-04:01:27-root-INFO: grad norm: 1.079 1.075 0.093
2024-12-02-04:01:28-root-INFO: grad norm: 0.970 0.966 0.079
2024-12-02-04:01:29-root-INFO: grad norm: 0.923 0.920 0.076
2024-12-02-04:01:30-root-INFO: grad norm: 0.922 0.919 0.072
2024-12-02-04:01:31-root-INFO: Loss Change: 8.221 -> 7.038
2024-12-02-04:01:31-root-INFO: Regularization Change: 0.000 -> 1.711
2024-12-02-04:01:31-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-04:01:31-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-04:01:31-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-04:01:31-root-INFO: grad norm: 1.527 1.507 0.248
2024-12-02-04:01:32-root-INFO: grad norm: 1.167 1.163 0.097
2024-12-02-04:01:33-root-INFO: grad norm: 1.049 1.046 0.067
2024-12-02-04:01:34-root-INFO: grad norm: 1.087 1.085 0.064
2024-12-02-04:01:35-root-INFO: grad norm: 0.922 0.920 0.063
2024-12-02-04:01:36-root-INFO: Loss Change: 6.931 -> 6.006
2024-12-02-04:01:36-root-INFO: Regularization Change: 0.000 -> 1.328
2024-12-02-04:01:36-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-04:01:36-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-04:01:36-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-04:01:37-root-INFO: grad norm: 1.271 1.248 0.240
2024-12-02-04:01:38-root-INFO: grad norm: 0.933 0.927 0.099
2024-12-02-04:01:39-root-INFO: grad norm: 1.176 1.173 0.078
2024-12-02-04:01:40-root-INFO: grad norm: 0.830 0.827 0.071
2024-12-02-04:01:41-root-INFO: grad norm: 0.698 0.696 0.046
2024-12-02-04:01:41-root-INFO: Loss Change: 5.934 -> 5.201
2024-12-02-04:01:41-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-02-04:01:41-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-04:01:41-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-04:01:42-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-04:01:42-root-INFO: grad norm: 1.291 1.269 0.238
2024-12-02-04:01:43-root-INFO: grad norm: 0.921 0.915 0.100
2024-12-02-04:01:44-root-INFO: grad norm: 0.865 0.863 0.065
2024-12-02-04:01:45-root-INFO: grad norm: 0.826 0.823 0.069
2024-12-02-04:01:46-root-INFO: grad norm: 0.777 0.775 0.052
2024-12-02-04:01:47-root-INFO: Loss Change: 5.195 -> 4.594
2024-12-02-04:01:47-root-INFO: Regularization Change: 0.000 -> 0.942
2024-12-02-04:01:47-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-04:01:47-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-04:01:47-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-04:01:47-root-INFO: grad norm: 1.153 1.139 0.175
2024-12-02-04:01:48-root-INFO: grad norm: 0.883 0.882 0.054
2024-12-02-04:01:49-root-INFO: grad norm: 0.767 0.766 0.046
2024-12-02-04:01:50-root-INFO: grad norm: 0.636 0.635 0.039
2024-12-02-04:01:51-root-INFO: grad norm: 0.719 0.718 0.050
2024-12-02-04:01:52-root-INFO: Loss Change: 4.572 -> 4.051
2024-12-02-04:01:52-root-INFO: Regularization Change: 0.000 -> 0.822
2024-12-02-04:01:52-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-04:01:52-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-04:01:52-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-04:01:53-root-INFO: grad norm: 1.577 1.555 0.260
2024-12-02-04:01:54-root-INFO: grad norm: 0.863 0.858 0.092
2024-12-02-04:01:55-root-INFO: grad norm: 0.613 0.612 0.038
2024-12-02-04:01:56-root-INFO: grad norm: 0.564 0.563 0.032
2024-12-02-04:01:57-root-INFO: grad norm: 0.637 0.636 0.037
2024-12-02-04:01:57-root-INFO: Loss Change: 4.159 -> 3.590
2024-12-02-04:01:57-root-INFO: Regularization Change: 0.000 -> 0.732
2024-12-02-04:01:57-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-04:01:57-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-04:01:58-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-04:01:58-root-INFO: grad norm: 1.469 1.447 0.252
2024-12-02-04:01:59-root-INFO: grad norm: 0.844 0.840 0.083
2024-12-02-04:02:00-root-INFO: grad norm: 0.524 0.522 0.042
2024-12-02-04:02:01-root-INFO: grad norm: 0.472 0.471 0.035
2024-12-02-04:02:02-root-INFO: grad norm: 0.519 0.518 0.030
2024-12-02-04:02:03-root-INFO: Loss Change: 3.729 -> 3.192
2024-12-02-04:02:03-root-INFO: Regularization Change: 0.000 -> 0.724
2024-12-02-04:02:03-root-INFO: Undo step: 10
2024-12-02-04:02:03-root-INFO: Undo step: 11
2024-12-02-04:02:03-root-INFO: Undo step: 12
2024-12-02-04:02:03-root-INFO: Undo step: 13
2024-12-02-04:02:03-root-INFO: Undo step: 14
2024-12-02-04:02:03-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-04:02:03-root-INFO: grad norm: 12.766 12.739 0.827
2024-12-02-04:02:04-root-INFO: grad norm: 6.275 6.254 0.513
2024-12-02-04:02:05-root-INFO: grad norm: 4.190 4.174 0.362
2024-12-02-04:02:06-root-INFO: grad norm: 3.188 3.174 0.291
2024-12-02-04:02:07-root-INFO: grad norm: 2.615 2.604 0.235
2024-12-02-04:02:08-root-INFO: Loss Change: 67.430 -> 13.532
2024-12-02-04:02:08-root-INFO: Regularization Change: 0.000 -> 68.740
2024-12-02-04:02:08-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-04:02:08-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-04:02:08-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-04:02:09-root-INFO: grad norm: 2.420 2.408 0.238
2024-12-02-04:02:10-root-INFO: grad norm: 1.984 1.977 0.176
2024-12-02-04:02:11-root-INFO: grad norm: 1.757 1.750 0.148
2024-12-02-04:02:12-root-INFO: grad norm: 1.638 1.632 0.135
2024-12-02-04:02:13-root-INFO: grad norm: 1.473 1.467 0.126
2024-12-02-04:02:13-root-INFO: Loss Change: 12.988 -> 8.277
2024-12-02-04:02:13-root-INFO: Regularization Change: 0.000 -> 7.628
2024-12-02-04:02:13-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-04:02:13-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-04:02:14-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-04:02:14-root-INFO: grad norm: 1.729 1.712 0.243
2024-12-02-04:02:15-root-INFO: grad norm: 1.352 1.345 0.132
2024-12-02-04:02:16-root-INFO: grad norm: 1.170 1.167 0.094
2024-12-02-04:02:17-root-INFO: grad norm: 1.121 1.117 0.095
2024-12-02-04:02:18-root-INFO: grad norm: 1.172 1.169 0.084
2024-12-02-04:02:19-root-INFO: Loss Change: 8.003 -> 6.102
2024-12-02-04:02:19-root-INFO: Regularization Change: 0.000 -> 3.171
2024-12-02-04:02:19-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-04:02:19-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-04:02:19-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-04:02:19-root-INFO: grad norm: 1.328 1.318 0.165
2024-12-02-04:02:20-root-INFO: grad norm: 0.930 0.927 0.073
2024-12-02-04:02:21-root-INFO: grad norm: 0.883 0.882 0.059
2024-12-02-04:02:22-root-INFO: grad norm: 0.988 0.986 0.060
2024-12-02-04:02:23-root-INFO: grad norm: 0.910 0.907 0.064
2024-12-02-04:02:24-root-INFO: Loss Change: 5.891 -> 4.771
2024-12-02-04:02:24-root-INFO: Regularization Change: 0.000 -> 1.768
2024-12-02-04:02:24-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-04:02:24-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-04:02:24-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-04:02:24-root-INFO: grad norm: 1.402 1.382 0.239
2024-12-02-04:02:25-root-INFO: grad norm: 0.983 0.979 0.092
2024-12-02-04:02:26-root-INFO: grad norm: 0.827 0.824 0.061
2024-12-02-04:02:27-root-INFO: grad norm: 0.831 0.829 0.061
2024-12-02-04:02:28-root-INFO: grad norm: 0.886 0.885 0.054
2024-12-02-04:02:29-root-INFO: Loss Change: 4.727 -> 3.976
2024-12-02-04:02:29-root-INFO: Regularization Change: 0.000 -> 1.235
2024-12-02-04:02:29-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-04:02:29-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-04:02:29-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-04:02:30-root-INFO: grad norm: 1.227 1.214 0.178
2024-12-02-04:02:31-root-INFO: grad norm: 0.785 0.784 0.048
2024-12-02-04:02:32-root-INFO: grad norm: 0.766 0.765 0.040
2024-12-02-04:02:33-root-INFO: grad norm: 0.819 0.817 0.046
2024-12-02-04:02:34-root-INFO: grad norm: 0.780 0.779 0.050
2024-12-02-04:02:34-root-INFO: Loss Change: 3.947 -> 3.344
2024-12-02-04:02:34-root-INFO: Regularization Change: 0.000 -> 0.924
2024-12-02-04:02:34-root-INFO: Undo step: 10
2024-12-02-04:02:34-root-INFO: Undo step: 11
2024-12-02-04:02:34-root-INFO: Undo step: 12
2024-12-02-04:02:34-root-INFO: Undo step: 13
2024-12-02-04:02:34-root-INFO: Undo step: 14
2024-12-02-04:02:35-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-04:02:35-root-INFO: grad norm: 13.157 13.122 0.948
2024-12-02-04:02:36-root-INFO: grad norm: 6.507 6.482 0.566
2024-12-02-04:02:37-root-INFO: grad norm: 4.353 4.334 0.401
2024-12-02-04:02:38-root-INFO: grad norm: 3.384 3.370 0.309
2024-12-02-04:02:39-root-INFO: grad norm: 2.726 2.713 0.271
2024-12-02-04:02:40-root-INFO: Loss Change: 71.536 -> 14.298
2024-12-02-04:02:40-root-INFO: Regularization Change: 0.000 -> 72.144
2024-12-02-04:02:40-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-04:02:40-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-04:02:40-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-04:02:40-root-INFO: grad norm: 2.555 2.540 0.274
2024-12-02-04:02:41-root-INFO: grad norm: 2.085 2.074 0.213
2024-12-02-04:02:42-root-INFO: grad norm: 1.825 1.818 0.161
2024-12-02-04:02:43-root-INFO: grad norm: 1.655 1.647 0.159
2024-12-02-04:02:44-root-INFO: grad norm: 1.566 1.560 0.133
2024-12-02-04:02:45-root-INFO: Loss Change: 13.748 -> 8.687
2024-12-02-04:02:45-root-INFO: Regularization Change: 0.000 -> 8.274
2024-12-02-04:02:45-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-04:02:45-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-04:02:45-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-04:02:46-root-INFO: grad norm: 1.613 1.603 0.184
2024-12-02-04:02:47-root-INFO: grad norm: 1.267 1.262 0.106
2024-12-02-04:02:48-root-INFO: grad norm: 1.190 1.186 0.096
2024-12-02-04:02:49-root-INFO: grad norm: 1.240 1.237 0.089
2024-12-02-04:02:50-root-INFO: grad norm: 1.131 1.127 0.091
2024-12-02-04:02:50-root-INFO: Loss Change: 8.301 -> 6.227
2024-12-02-04:02:50-root-INFO: Regularization Change: 0.000 -> 3.367
2024-12-02-04:02:50-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-04:02:50-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-04:02:51-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-04:02:51-root-INFO: grad norm: 1.528 1.511 0.226
2024-12-02-04:02:52-root-INFO: grad norm: 1.214 1.209 0.115
2024-12-02-04:02:53-root-INFO: grad norm: 1.152 1.149 0.084
2024-12-02-04:02:54-root-INFO: grad norm: 1.012 1.009 0.084
2024-12-02-04:02:55-root-INFO: grad norm: 0.845 0.843 0.062
2024-12-02-04:02:55-root-INFO: Loss Change: 6.044 -> 4.844
2024-12-02-04:02:55-root-INFO: Regularization Change: 0.000 -> 1.907
2024-12-02-04:02:55-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-04:02:56-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-04:02:56-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-04:02:56-root-INFO: grad norm: 1.159 1.145 0.178
2024-12-02-04:02:57-root-INFO: grad norm: 0.804 0.802 0.057
2024-12-02-04:02:58-root-INFO: grad norm: 0.822 0.820 0.048
2024-12-02-04:02:59-root-INFO: grad norm: 0.978 0.976 0.052
2024-12-02-04:03:00-root-INFO: grad norm: 0.819 0.817 0.056
2024-12-02-04:03:01-root-INFO: Loss Change: 4.715 -> 3.937
2024-12-02-04:03:01-root-INFO: Regularization Change: 0.000 -> 1.230
2024-12-02-04:03:01-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-04:03:01-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-04:03:01-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-04:03:01-root-INFO: grad norm: 1.338 1.317 0.238
2024-12-02-04:03:02-root-INFO: grad norm: 0.949 0.945 0.087
2024-12-02-04:03:03-root-INFO: grad norm: 0.928 0.926 0.063
2024-12-02-04:03:04-root-INFO: grad norm: 0.817 0.815 0.059
2024-12-02-04:03:05-root-INFO: grad norm: 0.704 0.702 0.046
2024-12-02-04:03:06-root-INFO: Loss Change: 3.950 -> 3.328
2024-12-02-04:03:06-root-INFO: Regularization Change: 0.000 -> 0.954
2024-12-02-04:03:06-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-04:03:06-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-04:03:06-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-04:03:07-root-INFO: grad norm: 1.068 1.056 0.156
2024-12-02-04:03:08-root-INFO: grad norm: 0.688 0.687 0.040
2024-12-02-04:03:09-root-INFO: grad norm: 0.677 0.677 0.035
2024-12-02-04:03:10-root-INFO: grad norm: 0.696 0.695 0.038
2024-12-02-04:03:11-root-INFO: grad norm: 0.706 0.705 0.043
2024-12-02-04:03:11-root-INFO: Loss Change: 3.334 -> 2.887
2024-12-02-04:03:11-root-INFO: Regularization Change: 0.000 -> 0.729
2024-12-02-04:03:11-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-04:03:11-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-04:03:12-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-04:03:12-root-INFO: grad norm: 1.283 1.265 0.212
2024-12-02-04:03:13-root-INFO: grad norm: 0.833 0.830 0.063
2024-12-02-04:03:14-root-INFO: grad norm: 0.689 0.687 0.045
2024-12-02-04:03:15-root-INFO: grad norm: 0.586 0.585 0.037
2024-12-02-04:03:16-root-INFO: grad norm: 0.535 0.534 0.032
2024-12-02-04:03:17-root-INFO: Loss Change: 3.007 -> 2.536
2024-12-02-04:03:17-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-02-04:03:17-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-04:03:17-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-04:03:17-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-04:03:17-root-INFO: grad norm: 0.964 0.951 0.159
2024-12-02-04:03:18-root-INFO: grad norm: 0.454 0.452 0.033
2024-12-02-04:03:19-root-INFO: grad norm: 0.385 0.384 0.028
2024-12-02-04:03:20-root-INFO: grad norm: 0.357 0.356 0.025
2024-12-02-04:03:21-root-INFO: grad norm: 0.340 0.340 0.024
2024-12-02-04:03:22-root-INFO: Loss Change: 2.639 -> 2.261
2024-12-02-04:03:22-root-INFO: Regularization Change: 0.000 -> 0.549
2024-12-02-04:03:22-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-04:03:22-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-04:03:22-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-04:03:23-root-INFO: grad norm: 0.943 0.930 0.155
2024-12-02-04:03:24-root-INFO: grad norm: 0.517 0.516 0.032
2024-12-02-04:03:25-root-INFO: grad norm: 0.438 0.437 0.030
2024-12-02-04:03:25-root-INFO: grad norm: 0.366 0.365 0.026
2024-12-02-04:03:27-root-INFO: grad norm: 0.325 0.324 0.025
2024-12-02-04:03:27-root-INFO: Loss Change: 2.398 -> 1.998
2024-12-02-04:03:27-root-INFO: Regularization Change: 0.000 -> 0.559
2024-12-02-04:03:27-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-04:03:27-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-04:03:28-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-04:03:28-root-INFO: grad norm: 0.897 0.886 0.141
2024-12-02-04:03:29-root-INFO: grad norm: 0.411 0.409 0.034
2024-12-02-04:03:30-root-INFO: grad norm: 0.338 0.336 0.032
2024-12-02-04:03:31-root-INFO: grad norm: 0.305 0.304 0.029
2024-12-02-04:03:32-root-INFO: grad norm: 0.284 0.283 0.028
2024-12-02-04:03:32-root-INFO: Loss Change: 2.152 -> 1.818
2024-12-02-04:03:32-root-INFO: Regularization Change: 0.000 -> 0.491
2024-12-02-04:03:32-root-INFO: Undo step: 5
2024-12-02-04:03:32-root-INFO: Undo step: 6
2024-12-02-04:03:32-root-INFO: Undo step: 7
2024-12-02-04:03:32-root-INFO: Undo step: 8
2024-12-02-04:03:32-root-INFO: Undo step: 9
2024-12-02-04:03:33-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-04:03:33-root-INFO: grad norm: 12.295 12.280 0.601
2024-12-02-04:03:34-root-INFO: grad norm: 5.725 5.712 0.388
2024-12-02-04:03:35-root-INFO: grad norm: 3.701 3.691 0.282
2024-12-02-04:03:36-root-INFO: grad norm: 2.963 2.955 0.215
2024-12-02-04:03:37-root-INFO: grad norm: 2.318 2.312 0.159
2024-12-02-04:03:38-root-INFO: Loss Change: 55.470 -> 8.540
2024-12-02-04:03:38-root-INFO: Regularization Change: 0.000 -> 64.112
2024-12-02-04:03:38-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-04:03:38-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-04:03:38-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-04:03:38-root-INFO: grad norm: 2.062 2.056 0.152
2024-12-02-04:03:39-root-INFO: grad norm: 1.600 1.595 0.120
2024-12-02-04:03:40-root-INFO: grad norm: 1.384 1.381 0.092
2024-12-02-04:03:41-root-INFO: grad norm: 1.318 1.314 0.099
2024-12-02-04:03:42-root-INFO: grad norm: 1.343 1.341 0.076
2024-12-02-04:03:43-root-INFO: Loss Change: 8.102 -> 4.709
2024-12-02-04:03:43-root-INFO: Regularization Change: 0.000 -> 5.859
2024-12-02-04:03:43-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-04:03:43-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-04:03:43-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-04:03:44-root-INFO: grad norm: 1.380 1.374 0.125
2024-12-02-04:03:45-root-INFO: grad norm: 0.997 0.996 0.061
2024-12-02-04:03:46-root-INFO: grad norm: 0.964 0.962 0.060
2024-12-02-04:03:47-root-INFO: grad norm: 0.953 0.952 0.048
2024-12-02-04:03:48-root-INFO: grad norm: 0.903 0.901 0.062
2024-12-02-04:03:48-root-INFO: Loss Change: 4.499 -> 3.251
2024-12-02-04:03:48-root-INFO: Regularization Change: 0.000 -> 2.169
2024-12-02-04:03:48-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-04:03:48-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-04:03:49-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-04:03:49-root-INFO: grad norm: 1.279 1.265 0.188
2024-12-02-04:03:50-root-INFO: grad norm: 0.848 0.846 0.060
2024-12-02-04:03:51-root-INFO: grad norm: 0.732 0.731 0.040
2024-12-02-04:03:52-root-INFO: grad norm: 0.613 0.612 0.035
2024-12-02-04:03:53-root-INFO: grad norm: 0.556 0.556 0.031
2024-12-02-04:03:54-root-INFO: Loss Change: 3.222 -> 2.502
2024-12-02-04:03:54-root-INFO: Regularization Change: 0.000 -> 1.130
2024-12-02-04:03:54-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-04:03:54-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-04:03:54-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-04:03:54-root-INFO: grad norm: 0.914 0.903 0.141
2024-12-02-04:03:55-root-INFO: grad norm: 0.497 0.496 0.031
2024-12-02-04:03:56-root-INFO: grad norm: 0.427 0.426 0.028
2024-12-02-04:03:57-root-INFO: grad norm: 0.391 0.390 0.025
2024-12-02-04:03:58-root-INFO: grad norm: 0.365 0.364 0.024
2024-12-02-04:03:59-root-INFO: Loss Change: 2.527 -> 2.089
2024-12-02-04:03:59-root-INFO: Regularization Change: 0.000 -> 0.694
2024-12-02-04:03:59-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-04:03:59-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-04:03:59-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-04:03:59-root-INFO: grad norm: 0.895 0.883 0.147
2024-12-02-04:04:00-root-INFO: grad norm: 0.449 0.448 0.034
2024-12-02-04:04:01-root-INFO: grad norm: 0.365 0.364 0.033
2024-12-02-04:04:02-root-INFO: grad norm: 0.329 0.328 0.027
2024-12-02-04:04:03-root-INFO: grad norm: 0.304 0.303 0.027
2024-12-02-04:04:04-root-INFO: Loss Change: 2.186 -> 1.831
2024-12-02-04:04:04-root-INFO: Regularization Change: 0.000 -> 0.539
2024-12-02-04:04:04-root-INFO: Undo step: 5
2024-12-02-04:04:04-root-INFO: Undo step: 6
2024-12-02-04:04:04-root-INFO: Undo step: 7
2024-12-02-04:04:04-root-INFO: Undo step: 8
2024-12-02-04:04:04-root-INFO: Undo step: 9
2024-12-02-04:04:05-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-04:04:05-root-INFO: grad norm: 12.896 12.878 0.669
2024-12-02-04:04:06-root-INFO: grad norm: 5.751 5.732 0.464
2024-12-02-04:04:07-root-INFO: grad norm: 3.831 3.820 0.292
2024-12-02-04:04:08-root-INFO: grad norm: 3.127 3.118 0.241
2024-12-02-04:04:09-root-INFO: grad norm: 2.419 2.411 0.198
2024-12-02-04:04:09-root-INFO: Loss Change: 58.510 -> 8.904
2024-12-02-04:04:09-root-INFO: Regularization Change: 0.000 -> 66.963
2024-12-02-04:04:09-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-04:04:09-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-04:04:10-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-04:04:10-root-INFO: grad norm: 2.360 2.354 0.170
2024-12-02-04:04:11-root-INFO: grad norm: 1.780 1.774 0.143
2024-12-02-04:04:12-root-INFO: grad norm: 1.441 1.438 0.091
2024-12-02-04:04:13-root-INFO: grad norm: 1.250 1.247 0.092
2024-12-02-04:04:14-root-INFO: grad norm: 1.125 1.123 0.071
2024-12-02-04:04:15-root-INFO: Loss Change: 8.460 -> 4.732
2024-12-02-04:04:15-root-INFO: Regularization Change: 0.000 -> 6.318
2024-12-02-04:04:15-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-04:04:15-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-04:04:15-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-04:04:15-root-INFO: grad norm: 1.303 1.295 0.145
2024-12-02-04:04:16-root-INFO: grad norm: 0.955 0.953 0.062
2024-12-02-04:04:17-root-INFO: grad norm: 0.874 0.872 0.054
2024-12-02-04:04:18-root-INFO: grad norm: 0.843 0.842 0.046
2024-12-02-04:04:19-root-INFO: grad norm: 0.830 0.828 0.056
2024-12-02-04:04:20-root-INFO: Loss Change: 4.496 -> 3.250
2024-12-02-04:04:20-root-INFO: Regularization Change: 0.000 -> 2.183
2024-12-02-04:04:20-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-04:04:20-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-04:04:20-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-04:04:21-root-INFO: grad norm: 1.265 1.251 0.182
2024-12-02-04:04:22-root-INFO: grad norm: 0.875 0.873 0.060
2024-12-02-04:04:23-root-INFO: grad norm: 0.741 0.740 0.040
2024-12-02-04:04:24-root-INFO: grad norm: 0.643 0.642 0.040
2024-12-02-04:04:25-root-INFO: grad norm: 0.583 0.583 0.031
2024-12-02-04:04:25-root-INFO: Loss Change: 3.194 -> 2.486
2024-12-02-04:04:25-root-INFO: Regularization Change: 0.000 -> 1.133
2024-12-02-04:04:25-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-04:04:25-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-04:04:26-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-04:04:26-root-INFO: grad norm: 0.924 0.914 0.138
2024-12-02-04:04:27-root-INFO: grad norm: 0.506 0.505 0.031
2024-12-02-04:04:28-root-INFO: grad norm: 0.432 0.431 0.026
2024-12-02-04:04:29-root-INFO: grad norm: 0.395 0.394 0.024
2024-12-02-04:04:30-root-INFO: grad norm: 0.367 0.367 0.023
2024-12-02-04:04:31-root-INFO: Loss Change: 2.497 -> 2.063
2024-12-02-04:04:31-root-INFO: Regularization Change: 0.000 -> 0.686
2024-12-02-04:04:31-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-04:04:31-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-04:04:31-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-04:04:32-root-INFO: grad norm: 0.905 0.893 0.146
2024-12-02-04:04:33-root-INFO: grad norm: 0.464 0.462 0.033
2024-12-02-04:04:33-root-INFO: grad norm: 0.372 0.370 0.029
2024-12-02-04:04:35-root-INFO: grad norm: 0.331 0.330 0.025
2024-12-02-04:04:36-root-INFO: grad norm: 0.309 0.308 0.025
2024-12-02-04:04:36-root-INFO: Loss Change: 2.152 -> 1.805
2024-12-02-04:04:36-root-INFO: Regularization Change: 0.000 -> 0.522
2024-12-02-04:04:36-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-04:04:36-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-04:04:37-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-04:04:37-root-INFO: grad norm: 0.798 0.790 0.117
2024-12-02-04:04:38-root-INFO: grad norm: 0.403 0.402 0.034
2024-12-02-04:04:39-root-INFO: grad norm: 0.327 0.325 0.032
2024-12-02-04:04:40-root-INFO: grad norm: 0.295 0.294 0.028
2024-12-02-04:04:41-root-INFO: grad norm: 0.274 0.273 0.029
2024-12-02-04:04:42-root-INFO: Loss Change: 1.913 -> 1.629
2024-12-02-04:04:42-root-INFO: Regularization Change: 0.000 -> 0.433
2024-12-02-04:04:42-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-04:04:42-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-04:04:42-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-04:04:42-root-INFO: grad norm: 0.773 0.766 0.103
2024-12-02-04:04:43-root-INFO: grad norm: 0.384 0.383 0.037
2024-12-02-04:04:44-root-INFO: grad norm: 0.307 0.305 0.035
2024-12-02-04:04:45-root-INFO: grad norm: 0.273 0.271 0.033
2024-12-02-04:04:46-root-INFO: grad norm: 0.253 0.251 0.032
2024-12-02-04:04:47-root-INFO: Loss Change: 1.761 -> 1.501
2024-12-02-04:04:47-root-INFO: Regularization Change: 0.000 -> 0.397
2024-12-02-04:04:47-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-04:04:47-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-04:04:47-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-04:04:48-root-INFO: grad norm: 0.736 0.731 0.085
2024-12-02-04:04:49-root-INFO: grad norm: 0.371 0.369 0.039
2024-12-02-04:04:50-root-INFO: grad norm: 0.304 0.302 0.034
2024-12-02-04:04:51-root-INFO: grad norm: 0.255 0.252 0.034
2024-12-02-04:04:51-root-INFO: grad norm: 0.234 0.232 0.032
2024-12-02-04:04:52-root-INFO: Loss Change: 1.634 -> 1.396
2024-12-02-04:04:52-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-02-04:04:52-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-04:04:52-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-04:04:52-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-04:04:53-root-INFO: grad norm: 0.725 0.721 0.073
2024-12-02-04:04:54-root-INFO: grad norm: 0.357 0.356 0.035
2024-12-02-04:04:55-root-INFO: grad norm: 0.280 0.278 0.029
2024-12-02-04:04:56-root-INFO: grad norm: 0.247 0.246 0.028
2024-12-02-04:04:57-root-INFO: grad norm: 0.226 0.224 0.028
2024-12-02-04:04:57-root-INFO: Loss Change: 1.512 -> 1.274
2024-12-02-04:04:57-root-INFO: Regularization Change: 0.000 -> 0.376
2024-12-02-04:04:57-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-04:04:57-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-04:04:58-root-INFO: step: 0 lr_xt 0.38992188
2024-12-02-04:04:58-root-INFO: grad norm: 0.828 0.824 0.081
2024-12-02-04:04:59-root-INFO: grad norm: 0.510 0.510 0.029
2024-12-02-04:05:00-root-INFO: grad norm: 0.407 0.406 0.030
2024-12-02-04:05:01-root-INFO: grad norm: 0.357 0.355 0.042
2024-12-02-04:05:02-root-INFO: grad norm: 0.329 0.326 0.048
2024-12-02-04:05:03-root-INFO: Loss Change: 1.414 -> 0.968
2024-12-02-04:05:03-root-INFO: Regularization Change: 0.000 -> 0.792
2024-12-02-04:05:03-root-INFO: Undo step: 0
2024-12-02-04:05:03-root-INFO: Undo step: 1
2024-12-02-04:05:03-root-INFO: Undo step: 2
2024-12-02-04:05:03-root-INFO: Undo step: 3
2024-12-02-04:05:03-root-INFO: Undo step: 4
2024-12-02-04:05:03-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-04:05:03-root-INFO: grad norm: 11.673 11.666 0.398
2024-12-02-04:05:04-root-INFO: grad norm: 5.304 5.300 0.208
2024-12-02-04:05:05-root-INFO: grad norm: 2.870 2.867 0.135
2024-12-02-04:05:06-root-INFO: grad norm: 1.902 1.899 0.106
2024-12-02-04:05:07-root-INFO: grad norm: 1.365 1.362 0.079
2024-12-02-04:05:08-root-INFO: Loss Change: 35.819 -> 3.736
2024-12-02-04:05:08-root-INFO: Regularization Change: 0.000 -> 44.618
2024-12-02-04:05:08-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-04:05:08-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-04:05:08-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-04:05:09-root-INFO: grad norm: 1.306 1.303 0.088
2024-12-02-04:05:10-root-INFO: grad norm: 0.945 0.943 0.047
2024-12-02-04:05:11-root-INFO: grad norm: 0.786 0.785 0.040
2024-12-02-04:05:12-root-INFO: grad norm: 0.675 0.674 0.036
2024-12-02-04:05:13-root-INFO: grad norm: 0.591 0.590 0.033
2024-12-02-04:05:13-root-INFO: Loss Change: 3.520 -> 2.204
2024-12-02-04:05:13-root-INFO: Regularization Change: 0.000 -> 2.359
2024-12-02-04:05:13-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-04:05:13-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-04:05:14-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-04:05:14-root-INFO: grad norm: 0.884 0.880 0.093
2024-12-02-04:05:15-root-INFO: grad norm: 0.643 0.642 0.037
2024-12-02-04:05:16-root-INFO: grad norm: 0.469 0.467 0.033
2024-12-02-04:05:17-root-INFO: grad norm: 0.411 0.410 0.029
2024-12-02-04:05:18-root-INFO: grad norm: 0.371 0.370 0.029
2024-12-02-04:05:19-root-INFO: Loss Change: 2.198 -> 1.680
2024-12-02-04:05:19-root-INFO: Regularization Change: 0.000 -> 0.880
2024-12-02-04:05:19-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-04:05:19-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-04:05:19-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-04:05:19-root-INFO: grad norm: 0.766 0.762 0.078
2024-12-02-04:05:20-root-INFO: grad norm: 0.443 0.441 0.035
2024-12-02-04:05:21-root-INFO: grad norm: 0.357 0.355 0.033
2024-12-02-04:05:22-root-INFO: grad norm: 0.313 0.311 0.031
2024-12-02-04:05:23-root-INFO: grad norm: 0.283 0.281 0.029
2024-12-02-04:05:24-root-INFO: Loss Change: 1.743 -> 1.426
2024-12-02-04:05:24-root-INFO: Regularization Change: 0.000 -> 0.526
2024-12-02-04:05:24-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-04:05:24-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-04:05:24-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-04:05:24-root-INFO: grad norm: 0.704 0.700 0.070
2024-12-02-04:05:25-root-INFO: grad norm: 0.374 0.372 0.035
2024-12-02-04:05:26-root-INFO: grad norm: 0.311 0.309 0.029
2024-12-02-04:05:27-root-INFO: grad norm: 0.255 0.253 0.028
2024-12-02-04:05:28-root-INFO: grad norm: 0.233 0.231 0.027
2024-12-02-04:05:29-root-INFO: Loss Change: 1.507 -> 1.261
2024-12-02-04:05:29-root-INFO: Regularization Change: 0.000 -> 0.401
2024-12-02-04:05:29-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-04:05:29-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-04:05:29-root-INFO: step: 0 lr_xt 0.38992188
2024-12-02-04:05:30-root-INFO: grad norm: 0.810 0.806 0.082
2024-12-02-04:05:31-root-INFO: grad norm: 0.503 0.502 0.030
2024-12-02-04:05:32-root-INFO: grad norm: 0.404 0.403 0.029
2024-12-02-04:05:33-root-INFO: grad norm: 0.356 0.354 0.040
2024-12-02-04:05:34-root-INFO: grad norm: 0.330 0.326 0.047
2024-12-02-04:05:34-root-INFO: Loss Change: 1.386 -> 0.952
2024-12-02-04:05:34-root-INFO: Regularization Change: 0.000 -> 0.774
2024-12-02-04:05:34-root-INFO: Undo step: 0
2024-12-02-04:05:34-root-INFO: Undo step: 1
2024-12-02-04:05:34-root-INFO: Undo step: 2
2024-12-02-04:05:34-root-INFO: Undo step: 3
2024-12-02-04:05:34-root-INFO: Undo step: 4
2024-12-02-04:05:35-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-04:05:35-root-INFO: grad norm: 13.235 13.227 0.462
2024-12-02-04:05:36-root-INFO: grad norm: 5.361 5.355 0.235
2024-12-02-04:05:37-root-INFO: grad norm: 2.696 2.693 0.128
2024-12-02-04:05:38-root-INFO: grad norm: 2.108 2.105 0.109
2024-12-02-04:05:39-root-INFO: grad norm: 1.638 1.636 0.075
2024-12-02-04:05:40-root-INFO: Loss Change: 37.960 -> 4.126
2024-12-02-04:05:40-root-INFO: Regularization Change: 0.000 -> 48.368
2024-12-02-04:05:40-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-04:05:40-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-04:05:40-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-04:05:40-root-INFO: grad norm: 1.772 1.770 0.085
2024-12-02-04:05:41-root-INFO: grad norm: 1.018 1.017 0.045
2024-12-02-04:05:43-root-INFO: grad norm: 0.834 0.833 0.041
2024-12-02-04:05:43-root-INFO: grad norm: 0.695 0.694 0.037
2024-12-02-04:05:44-root-INFO: grad norm: 0.597 0.596 0.034
2024-12-02-04:05:45-root-INFO: Loss Change: 3.907 -> 2.447
2024-12-02-04:05:45-root-INFO: Regularization Change: 0.000 -> 2.633
2024-12-02-04:05:45-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-04:05:45-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-04:05:45-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-04:05:46-root-INFO: grad norm: 0.907 0.901 0.100
2024-12-02-04:05:47-root-INFO: grad norm: 0.596 0.594 0.047
2024-12-02-04:05:47-root-INFO: grad norm: 0.479 0.478 0.039
2024-12-02-04:05:48-root-INFO: grad norm: 0.432 0.431 0.033
2024-12-02-04:05:48-root-INFO: grad norm: 0.410 0.408 0.037
2024-12-02-04:05:48-root-INFO: Loss Change: 2.445 -> 1.952
2024-12-02-04:05:48-root-INFO: Regularization Change: 0.000 -> 0.860
2024-12-02-04:05:48-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-04:05:48-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-04:05:49-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-04:05:49-root-INFO: grad norm: 0.729 0.726 0.068
2024-12-02-04:05:50-root-INFO: grad norm: 0.425 0.424 0.033
2024-12-02-04:05:51-root-INFO: grad norm: 0.375 0.373 0.031
2024-12-02-04:05:52-root-INFO: grad norm: 0.317 0.316 0.030
2024-12-02-04:05:53-root-INFO: grad norm: 0.289 0.288 0.029
2024-12-02-04:05:53-root-INFO: Loss Change: 2.015 -> 1.695
2024-12-02-04:05:53-root-INFO: Regularization Change: 0.000 -> 0.541
2024-12-02-04:05:53-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-04:05:53-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-04:05:54-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-04:05:54-root-INFO: grad norm: 0.728 0.724 0.077
2024-12-02-04:05:55-root-INFO: grad norm: 0.431 0.430 0.036
2024-12-02-04:05:56-root-INFO: grad norm: 0.565 0.565 0.030
2024-12-02-04:05:57-root-INFO: grad norm: 0.297 0.296 0.028
2024-12-02-04:05:58-root-INFO: grad norm: 0.241 0.240 0.027
2024-12-02-04:05:59-root-INFO: Loss Change: 1.778 -> 1.367
2024-12-02-04:05:59-root-INFO: Regularization Change: 0.000 -> 0.514
2024-12-02-04:05:59-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-04:05:59-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-04:05:59-root-INFO: loss_sample0_0: 1.367163896560669
2024-12-02-04:05:59-root-INFO: It takes 4736.797 seconds for image sample0
2024-12-02-04:05:59-root-INFO: lpips_score_sample0: 0.144
2024-12-02-04:05:59-root-INFO: psnr_score_sample0: 18.630
2024-12-02-04:05:59-root-INFO: ssim_score_sample0: 0.743
2024-12-02-04:05:59-root-INFO: mean_lpips: 0.14390744268894196
2024-12-02-04:05:59-root-INFO: best_mean_lpips: 0.14390744268894196
2024-12-02-04:05:59-root-INFO: mean_psnr: 18.629634857177734
2024-12-02-04:05:59-root-INFO: best_mean_psnr: 18.629634857177734
2024-12-02-04:05:59-root-INFO: mean_ssim: 0.743384599685669
2024-12-02-04:05:59-root-INFO: best_mean_ssim: 0.743384599685669
2024-12-02-04:05:59-root-INFO: final_loss: 1.367163896560669
2024-12-02-04:05:59-root-INFO: mean time: 4736.796899318695
2024-12-02-04:05:59-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample3_iter5_lr0.02_10009 
 
Enjoy.
