2024-12-01-21:40:09-root-INFO: Prepare model...
2024-12-01-21:40:25-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-01-21:40:50-root-INFO: Start sampling
2024-12-01-21:40:55-root-INFO: step: 249 lr_xt 0.00012706
2024-12-01-21:40:56-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-01-21:40:56-root-INFO: grad norm: 23712.389 17715.242 15762.220
2024-12-01-21:40:57-root-INFO: grad norm: 26791.344 21193.820 16388.963
2024-12-01-21:40:57-root-INFO: Loss too large (43809.766->60939.734)! Learning rate decreased to 0.00010.
2024-12-01-21:40:57-root-INFO: Loss too large (43809.766->45474.074)! Learning rate decreased to 0.00008.
2024-12-01-21:40:57-root-INFO: grad norm: 21161.297 16197.437 13617.765
2024-12-01-21:40:58-root-INFO: grad norm: 19142.758 15678.414 10983.285
2024-12-01-21:40:58-root-INFO: grad norm: 17822.953 14000.132 11029.688
2024-12-01-21:40:59-root-INFO: grad norm: 16609.697 13519.507 9649.090
2024-12-01-21:40:59-root-INFO: grad norm: 15350.011 12274.433 9217.437
2024-12-01-21:41:00-root-INFO: Loss Change: 77070.016 -> 21843.348
2024-12-01-21:41:00-root-INFO: Regularization Change: 0.000 -> 14.876
2024-12-01-21:41:00-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-01-21:41:00-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-21:41:00-root-INFO: step: 248 lr_xt 0.00013388
2024-12-01-21:41:00-root-INFO: grad norm: 13708.881 11022.428 8151.042
2024-12-01-21:41:00-root-INFO: Loss too large (22204.102->32159.193)! Learning rate decreased to 0.00011.
2024-12-01-21:41:00-root-INFO: Loss too large (22204.102->25110.287)! Learning rate decreased to 0.00009.
2024-12-01-21:41:01-root-INFO: grad norm: 12007.229 9574.400 7245.991
2024-12-01-21:41:01-root-INFO: grad norm: 10369.762 8293.522 6224.906
2024-12-01-21:41:02-root-INFO: grad norm: 8986.682 7203.367 5373.263
2024-12-01-21:41:02-root-INFO: grad norm: 7668.182 6109.822 4633.691
2024-12-01-21:41:03-root-INFO: grad norm: 6572.261 5283.019 3909.518
2024-12-01-21:41:03-root-INFO: grad norm: 5545.816 4409.952 3362.797
2024-12-01-21:41:03-root-INFO: grad norm: 4705.362 3787.989 2791.338
2024-12-01-21:41:04-root-INFO: Loss Change: 22204.102 -> 16910.156
2024-12-01-21:41:04-root-INFO: Regularization Change: 0.000 -> 1.223
2024-12-01-21:41:04-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-01-21:41:04-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-21:41:04-root-INFO: step: 247 lr_xt 0.00014104
2024-12-01-21:41:04-root-INFO: grad norm: 3929.438 3124.638 2382.670
2024-12-01-21:41:04-root-INFO: Loss too large (16757.711->17453.270)! Learning rate decreased to 0.00011.
2024-12-01-21:41:04-root-INFO: Loss too large (16757.711->16866.484)! Learning rate decreased to 0.00009.
2024-12-01-21:41:05-root-INFO: grad norm: 3143.953 2561.581 1822.839
2024-12-01-21:41:05-root-INFO: grad norm: 2509.827 2011.681 1500.789
2024-12-01-21:41:06-root-INFO: grad norm: 2036.493 1659.851 1179.914
2024-12-01-21:41:06-root-INFO: grad norm: 1668.479 1356.170 971.917
2024-12-01-21:41:07-root-INFO: grad norm: 1397.260 1141.802 805.371
2024-12-01-21:41:07-root-INFO: grad norm: 1193.311 991.126 664.575
2024-12-01-21:41:07-root-INFO: grad norm: 1046.862 862.716 592.994
2024-12-01-21:41:08-root-INFO: Loss Change: 16757.711 -> 15907.410
2024-12-01-21:41:08-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-01-21:41:08-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-01-21:41:08-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-21:41:08-root-INFO: step: 246 lr_xt 0.00014856
2024-12-01-21:41:08-root-INFO: grad norm: 1025.905 863.666 553.681
2024-12-01-21:41:09-root-INFO: grad norm: 1243.529 1016.838 715.824
2024-12-01-21:41:09-root-INFO: grad norm: 1798.096 1493.555 1001.220
2024-12-01-21:41:09-root-INFO: Loss too large (15567.947->15621.217)! Learning rate decreased to 0.00012.
2024-12-01-21:41:10-root-INFO: grad norm: 2025.069 1622.911 1211.225
2024-12-01-21:41:10-root-INFO: grad norm: 2314.922 1916.303 1298.710
2024-12-01-21:41:11-root-INFO: grad norm: 2648.881 2115.695 1593.864
2024-12-01-21:41:11-root-INFO: grad norm: 3057.902 2515.715 1738.373
2024-12-01-21:41:11-root-INFO: Loss too large (15468.615->15497.019)! Learning rate decreased to 0.00010.
2024-12-01-21:41:12-root-INFO: grad norm: 2306.225 1839.776 1390.646
2024-12-01-21:41:12-root-INFO: Loss Change: 15687.757 -> 15196.355
2024-12-01-21:41:12-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-01-21:41:12-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-01-21:41:12-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-21:41:13-root-INFO: step: 245 lr_xt 0.00015646
2024-12-01-21:41:13-root-INFO: grad norm: 1697.032 1421.733 926.602
2024-12-01-21:41:13-root-INFO: Loss too large (15078.083->15098.536)! Learning rate decreased to 0.00013.
2024-12-01-21:41:14-root-INFO: grad norm: 1816.281 1445.598 1099.602
2024-12-01-21:41:15-root-INFO: grad norm: 1980.408 1663.058 1075.292
2024-12-01-21:41:15-root-INFO: grad norm: 2167.716 1725.063 1312.689
2024-12-01-21:41:16-root-INFO: grad norm: 2388.905 1994.614 1314.679
2024-12-01-21:41:17-root-INFO: grad norm: 2632.521 2097.657 1590.598
2024-12-01-21:41:18-root-INFO: grad norm: 2918.323 2421.016 1629.505
2024-12-01-21:41:18-root-INFO: grad norm: 3216.820 2568.696 1936.423
2024-12-01-21:41:19-root-INFO: Loss Change: 15078.083 -> 14829.146
2024-12-01-21:41:19-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-01-21:41:19-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-01-21:41:19-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-21:41:19-root-INFO: step: 244 lr_xt 0.00016475
2024-12-01-21:41:19-root-INFO: grad norm: 3664.756 2986.209 2124.380
2024-12-01-21:41:20-root-INFO: Loss too large (14750.472->15196.662)! Learning rate decreased to 0.00013.
2024-12-01-21:41:20-root-INFO: grad norm: 3766.785 3043.227 2219.783
2024-12-01-21:41:21-root-INFO: grad norm: 3956.399 3263.206 2237.092
2024-12-01-21:41:21-root-INFO: Loss too large (14635.260->14640.985)! Learning rate decreased to 0.00011.
2024-12-01-21:41:22-root-INFO: grad norm: 2667.114 2153.258 1573.843
2024-12-01-21:41:23-root-INFO: grad norm: 1816.354 1545.234 954.670
2024-12-01-21:41:23-root-INFO: grad norm: 1325.077 1081.210 766.038
2024-12-01-21:41:24-root-INFO: grad norm: 1033.124 918.883 472.227
2024-12-01-21:41:25-root-INFO: grad norm: 877.982 746.697 461.841
2024-12-01-21:41:25-root-INFO: Loss Change: 14750.472 -> 13815.982
2024-12-01-21:41:25-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-01-21:41:25-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-01-21:41:25-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:41:25-root-INFO: step: 243 lr_xt 0.00017345
2024-12-01-21:41:25-root-INFO: grad norm: 1012.129 835.405 571.405
2024-12-01-21:41:26-root-INFO: grad norm: 976.555 786.516 578.836
2024-12-01-21:41:26-root-INFO: grad norm: 1060.013 889.876 575.975
2024-12-01-21:41:27-root-INFO: grad norm: 1233.059 958.995 775.088
2024-12-01-21:41:27-root-INFO: grad norm: 1536.190 1257.048 883.012
2024-12-01-21:41:28-root-INFO: grad norm: 2010.872 1554.785 1275.245
2024-12-01-21:41:28-root-INFO: Loss too large (13320.436->13353.941)! Learning rate decreased to 0.00014.
2024-12-01-21:41:28-root-INFO: grad norm: 1904.610 1573.791 1072.718
2024-12-01-21:41:29-root-INFO: grad norm: 1858.692 1478.225 1126.760
2024-12-01-21:41:29-root-INFO: Loss Change: 13697.784 -> 13074.570
2024-12-01-21:41:29-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-01-21:41:29-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-01-21:41:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:41:29-root-INFO: step: 242 lr_xt 0.00018258
2024-12-01-21:41:29-root-INFO: grad norm: 1943.609 1603.837 1097.873
2024-12-01-21:41:30-root-INFO: Loss too large (12881.092->12896.298)! Learning rate decreased to 0.00015.
2024-12-01-21:41:30-root-INFO: grad norm: 1836.463 1490.806 1072.425
2024-12-01-21:41:31-root-INFO: grad norm: 1781.551 1512.996 940.621
2024-12-01-21:41:31-root-INFO: grad norm: 1754.294 1431.927 1013.475
2024-12-01-21:41:32-root-INFO: grad norm: 1742.467 1491.340 901.163
2024-12-01-21:41:32-root-INFO: grad norm: 1741.800 1427.790 997.639
2024-12-01-21:41:33-root-INFO: grad norm: 1743.472 1493.675 899.240
2024-12-01-21:41:33-root-INFO: grad norm: 1746.354 1436.133 993.618
2024-12-01-21:41:33-root-INFO: Loss Change: 12881.092 -> 12240.789
2024-12-01-21:41:33-root-INFO: Regularization Change: 0.000 -> 0.714
2024-12-01-21:41:33-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-01-21:41:33-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:41:33-root-INFO: step: 241 lr_xt 0.00019216
2024-12-01-21:41:34-root-INFO: grad norm: 1779.562 1515.113 933.421
2024-12-01-21:41:34-root-INFO: Loss too large (12146.975->12162.457)! Learning rate decreased to 0.00015.
2024-12-01-21:41:34-root-INFO: grad norm: 1715.047 1433.505 941.514
2024-12-01-21:41:35-root-INFO: grad norm: 1661.643 1428.508 848.776
2024-12-01-21:41:35-root-INFO: grad norm: 1611.732 1345.701 886.999
2024-12-01-21:41:36-root-INFO: grad norm: 1562.370 1348.397 789.194
2024-12-01-21:41:36-root-INFO: grad norm: 1516.282 1267.977 831.471
2024-12-01-21:41:37-root-INFO: grad norm: 1470.305 1272.880 735.918
2024-12-01-21:41:37-root-INFO: grad norm: 1423.906 1193.947 775.886
2024-12-01-21:41:37-root-INFO: Loss Change: 12146.975 -> 11490.863
2024-12-01-21:41:37-root-INFO: Regularization Change: 0.000 -> 0.749
2024-12-01-21:41:37-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-01-21:41:37-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:41:38-root-INFO: step: 240 lr_xt 0.00020221
2024-12-01-21:41:38-root-INFO: grad norm: 1329.392 1144.690 675.996
2024-12-01-21:41:38-root-INFO: grad norm: 1670.970 1411.050 895.031
2024-12-01-21:41:39-root-INFO: grad norm: 2236.513 1918.749 1149.083
2024-12-01-21:41:39-root-INFO: Loss too large (11228.460->11319.374)! Learning rate decreased to 0.00016.
2024-12-01-21:41:39-root-INFO: grad norm: 2074.186 1742.719 1124.803
2024-12-01-21:41:40-root-INFO: grad norm: 1923.462 1657.126 976.545
2024-12-01-21:41:41-root-INFO: grad norm: 1787.340 1505.408 963.500
2024-12-01-21:41:41-root-INFO: grad norm: 1660.695 1437.530 831.515
2024-12-01-21:41:42-root-INFO: grad norm: 1546.646 1306.992 826.974
2024-12-01-21:41:43-root-INFO: Loss Change: 11297.742 -> 10670.547
2024-12-01-21:41:43-root-INFO: Regularization Change: 0.000 -> 0.879
2024-12-01-21:41:43-root-INFO: Undo step: 240
2024-12-01-21:41:43-root-INFO: Undo step: 241
2024-12-01-21:41:43-root-INFO: Undo step: 242
2024-12-01-21:41:43-root-INFO: Undo step: 243
2024-12-01-21:41:43-root-INFO: Undo step: 244
2024-12-01-21:41:43-root-INFO: step: 245 lr_xt 0.00015646
2024-12-01-21:41:43-root-INFO: grad norm: 11563.175 8561.347 7772.410
2024-12-01-21:41:44-root-INFO: Loss too large (18744.660->19528.684)! Learning rate decreased to 0.00013.
2024-12-01-21:41:44-root-INFO: grad norm: 8241.476 6168.023 5466.022
2024-12-01-21:41:45-root-INFO: grad norm: 6543.211 5075.356 4129.693
2024-12-01-21:41:46-root-INFO: grad norm: 5957.965 4601.708 3784.394
2024-12-01-21:41:47-root-INFO: grad norm: 5586.591 4470.275 3350.618
2024-12-01-21:41:47-root-INFO: grad norm: 5505.509 4330.614 3399.473
2024-12-01-21:41:48-root-INFO: grad norm: 5514.271 4478.891 3216.631
2024-12-01-21:41:48-root-INFO: Loss too large (13823.389->13860.619)! Learning rate decreased to 0.00010.
2024-12-01-21:41:49-root-INFO: grad norm: 3597.953 2846.144 2201.075
2024-12-01-21:41:50-root-INFO: Loss Change: 18744.660 -> 12953.363
2024-12-01-21:41:50-root-INFO: Regularization Change: 0.000 -> 1.436
2024-12-01-21:41:50-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-01-21:41:50-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-21:41:50-root-INFO: step: 244 lr_xt 0.00016475
2024-12-01-21:41:50-root-INFO: grad norm: 2566.695 2097.814 1478.884
2024-12-01-21:41:50-root-INFO: Loss too large (12879.163->12937.248)! Learning rate decreased to 0.00013.
2024-12-01-21:41:51-root-INFO: grad norm: 2438.864 1952.753 1461.100
2024-12-01-21:41:52-root-INFO: grad norm: 2417.113 2023.131 1322.638
2024-12-01-21:41:53-root-INFO: grad norm: 2443.128 1951.839 1469.420
2024-12-01-21:41:53-root-INFO: grad norm: 2488.481 2090.915 1349.300
2024-12-01-21:41:54-root-INFO: grad norm: 2543.182 2032.299 1528.900
2024-12-01-21:41:55-root-INFO: grad norm: 2604.827 2186.335 1416.002
2024-12-01-21:41:56-root-INFO: grad norm: 2666.633 2134.160 1598.840
2024-12-01-21:41:56-root-INFO: Loss Change: 12879.163 -> 12244.890
2024-12-01-21:41:56-root-INFO: Regularization Change: 0.000 -> 0.685
2024-12-01-21:41:56-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-01-21:41:56-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:41:56-root-INFO: step: 243 lr_xt 0.00017345
2024-12-01-21:41:57-root-INFO: grad norm: 2473.190 2065.307 1360.579
2024-12-01-21:41:57-root-INFO: Loss too large (11984.316->12088.974)! Learning rate decreased to 0.00014.
2024-12-01-21:41:57-root-INFO: grad norm: 2381.962 1912.163 1420.343
2024-12-01-21:41:58-root-INFO: grad norm: 2339.284 1976.391 1251.451
2024-12-01-21:41:58-root-INFO: grad norm: 2310.454 1856.606 1375.214
2024-12-01-21:41:59-root-INFO: grad norm: 2287.396 1934.377 1220.805
2024-12-01-21:41:59-root-INFO: grad norm: 2266.928 1824.354 1345.621
2024-12-01-21:42:00-root-INFO: grad norm: 2244.817 1897.167 1199.984
2024-12-01-21:42:00-root-INFO: grad norm: 2226.863 1795.358 1317.424
2024-12-01-21:42:01-root-INFO: Loss Change: 11984.316 -> 11393.032
2024-12-01-21:42:01-root-INFO: Regularization Change: 0.000 -> 0.605
2024-12-01-21:42:01-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-01-21:42:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:01-root-INFO: step: 242 lr_xt 0.00018258
2024-12-01-21:42:01-root-INFO: grad norm: 2172.901 1819.262 1188.185
2024-12-01-21:42:01-root-INFO: Loss too large (11183.318->11251.537)! Learning rate decreased to 0.00015.
2024-12-01-21:42:01-root-INFO: grad norm: 2022.603 1649.514 1170.480
2024-12-01-21:42:02-root-INFO: grad norm: 1906.431 1622.955 1000.248
2024-12-01-21:42:02-root-INFO: grad norm: 1813.437 1474.581 1055.541
2024-12-01-21:42:03-root-INFO: grad norm: 1726.703 1478.954 891.179
2024-12-01-21:42:03-root-INFO: grad norm: 1650.738 1343.703 958.852
2024-12-01-21:42:04-root-INFO: grad norm: 1575.672 1354.703 804.688
2024-12-01-21:42:04-root-INFO: grad norm: 1504.606 1227.927 869.503
2024-12-01-21:42:05-root-INFO: Loss Change: 11183.318 -> 10615.431
2024-12-01-21:42:05-root-INFO: Regularization Change: 0.000 -> 0.551
2024-12-01-21:42:05-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-01-21:42:05-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:05-root-INFO: step: 241 lr_xt 0.00019216
2024-12-01-21:42:05-root-INFO: grad norm: 1470.866 1255.207 766.747
2024-12-01-21:42:05-root-INFO: grad norm: 1928.070 1589.885 1090.743
2024-12-01-21:42:06-root-INFO: Loss too large (10513.223->10569.489)! Learning rate decreased to 0.00015.
2024-12-01-21:42:06-root-INFO: grad norm: 1755.997 1499.508 913.785
2024-12-01-21:42:06-root-INFO: grad norm: 1603.638 1323.399 905.687
2024-12-01-21:42:07-root-INFO: grad norm: 1463.953 1260.965 743.724
2024-12-01-21:42:07-root-INFO: grad norm: 1340.618 1110.185 751.495
2024-12-01-21:42:08-root-INFO: grad norm: 1226.106 1066.701 604.554
2024-12-01-21:42:08-root-INFO: grad norm: 1124.436 936.782 621.929
2024-12-01-21:42:09-root-INFO: Loss Change: 10516.619 -> 10067.589
2024-12-01-21:42:09-root-INFO: Regularization Change: 0.000 -> 0.519
2024-12-01-21:42:09-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-01-21:42:09-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:09-root-INFO: step: 240 lr_xt 0.00020221
2024-12-01-21:42:09-root-INFO: grad norm: 956.506 829.408 476.430
2024-12-01-21:42:09-root-INFO: grad norm: 1088.119 917.138 585.543
2024-12-01-21:42:10-root-INFO: grad norm: 1338.801 1164.122 661.217
2024-12-01-21:42:10-root-INFO: grad norm: 1679.073 1394.507 935.220
2024-12-01-21:42:11-root-INFO: Loss too large (9795.507->9817.454)! Learning rate decreased to 0.00016.
2024-12-01-21:42:11-root-INFO: grad norm: 1431.160 1240.549 713.622
2024-12-01-21:42:12-root-INFO: grad norm: 1229.784 1026.885 676.665
2024-12-01-21:42:12-root-INFO: grad norm: 1062.506 936.017 502.785
2024-12-01-21:42:13-root-INFO: grad norm: 929.204 784.613 497.799
2024-12-01-21:42:13-root-INFO: Loss Change: 9888.499 -> 9495.627
2024-12-01-21:42:13-root-INFO: Regularization Change: 0.000 -> 0.540
2024-12-01-21:42:13-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-01-21:42:13-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:13-root-INFO: step: 239 lr_xt 0.00021275
2024-12-01-21:42:13-root-INFO: grad norm: 758.699 676.907 342.669
2024-12-01-21:42:14-root-INFO: grad norm: 848.691 720.776 448.061
2024-12-01-21:42:14-root-INFO: grad norm: 986.540 868.170 468.553
2024-12-01-21:42:15-root-INFO: grad norm: 1170.564 982.542 636.263
2024-12-01-21:42:15-root-INFO: grad norm: 1411.372 1221.787 706.545
2024-12-01-21:42:16-root-INFO: grad norm: 1708.612 1427.663 938.687
2024-12-01-21:42:16-root-INFO: Loss too large (9252.208->9270.773)! Learning rate decreased to 0.00017.
2024-12-01-21:42:16-root-INFO: grad norm: 1375.941 1193.575 684.538
2024-12-01-21:42:17-root-INFO: grad norm: 1124.621 946.813 606.892
2024-12-01-21:42:17-root-INFO: Loss Change: 9387.154 -> 9042.379
2024-12-01-21:42:17-root-INFO: Regularization Change: 0.000 -> 0.570
2024-12-01-21:42:17-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-01-21:42:17-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:17-root-INFO: step: 238 lr_xt 0.00022380
2024-12-01-21:42:18-root-INFO: grad norm: 838.364 715.968 436.168
2024-12-01-21:42:18-root-INFO: grad norm: 799.055 677.825 423.135
2024-12-01-21:42:19-root-INFO: grad norm: 868.506 771.567 398.732
2024-12-01-21:42:19-root-INFO: grad norm: 969.822 817.060 522.464
2024-12-01-21:42:20-root-INFO: grad norm: 1102.344 971.573 520.778
2024-12-01-21:42:20-root-INFO: grad norm: 1263.694 1064.122 681.591
2024-12-01-21:42:20-root-INFO: grad norm: 1463.321 1275.673 716.915
2024-12-01-21:42:21-root-INFO: grad norm: 1690.032 1423.414 911.098
2024-12-01-21:42:21-root-INFO: Loss too large (8658.301->8665.676)! Learning rate decreased to 0.00018.
2024-12-01-21:42:21-root-INFO: Loss Change: 8860.640 -> 8559.766
2024-12-01-21:42:21-root-INFO: Regularization Change: 0.000 -> 0.600
2024-12-01-21:42:21-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-01-21:42:21-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:22-root-INFO: step: 237 lr_xt 0.00023539
2024-12-01-21:42:22-root-INFO: grad norm: 941.932 837.964 430.177
2024-12-01-21:42:22-root-INFO: grad norm: 1013.359 863.111 530.977
2024-12-01-21:42:23-root-INFO: grad norm: 1112.504 989.074 509.312
2024-12-01-21:42:23-root-INFO: grad norm: 1228.795 1044.276 647.631
2024-12-01-21:42:24-root-INFO: grad norm: 1365.208 1201.470 648.277
2024-12-01-21:42:24-root-INFO: grad norm: 1514.562 1286.622 799.062
2024-12-01-21:42:25-root-INFO: grad norm: 1684.279 1471.128 820.110
2024-12-01-21:42:25-root-INFO: Loss too large (8324.223->8325.775)! Learning rate decreased to 0.00019.
2024-12-01-21:42:25-root-INFO: grad norm: 1208.719 1029.671 633.072
2024-12-01-21:42:26-root-INFO: Loss Change: 8463.039 -> 8150.336
2024-12-01-21:42:26-root-INFO: Regularization Change: 0.000 -> 0.550
2024-12-01-21:42:26-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-01-21:42:26-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:26-root-INFO: step: 236 lr_xt 0.00024753
2024-12-01-21:42:26-root-INFO: grad norm: 813.971 722.567 374.760
2024-12-01-21:42:26-root-INFO: grad norm: 861.460 746.163 430.528
2024-12-01-21:42:27-root-INFO: grad norm: 922.508 820.642 421.389
2024-12-01-21:42:27-root-INFO: grad norm: 988.744 850.175 504.795
2024-12-01-21:42:28-root-INFO: grad norm: 1060.935 938.762 494.276
2024-12-01-21:42:28-root-INFO: grad norm: 1136.720 973.624 586.677
2024-12-01-21:42:29-root-INFO: grad norm: 1219.584 1073.808 578.204
2024-12-01-21:42:30-root-INFO: grad norm: 1305.259 1115.874 677.145
2024-12-01-21:42:30-root-INFO: Loss Change: 8006.466 -> 7786.271
2024-12-01-21:42:30-root-INFO: Regularization Change: 0.000 -> 0.597
2024-12-01-21:42:30-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-01-21:42:30-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:31-root-INFO: step: 235 lr_xt 0.00026027
2024-12-01-21:42:31-root-INFO: grad norm: 978.229 867.947 451.219
2024-12-01-21:42:32-root-INFO: grad norm: 994.060 854.867 507.305
2024-12-01-21:42:33-root-INFO: grad norm: 1023.327 913.208 461.789
2024-12-01-21:42:33-root-INFO: grad norm: 1054.397 907.505 536.831
2024-12-01-21:42:34-root-INFO: grad norm: 1087.678 967.491 496.995
2024-12-01-21:42:35-root-INFO: grad norm: 1121.345 965.681 569.980
2024-12-01-21:42:36-root-INFO: grad norm: 1156.289 1025.440 534.302
2024-12-01-21:42:37-root-INFO: grad norm: 1191.266 1026.198 605.006
2024-12-01-21:42:37-root-INFO: Loss Change: 7650.819 -> 7416.343
2024-12-01-21:42:37-root-INFO: Regularization Change: 0.000 -> 0.565
2024-12-01-21:42:37-root-INFO: Undo step: 235
2024-12-01-21:42:37-root-INFO: Undo step: 236
2024-12-01-21:42:37-root-INFO: Undo step: 237
2024-12-01-21:42:37-root-INFO: Undo step: 238
2024-12-01-21:42:37-root-INFO: Undo step: 239
2024-12-01-21:42:37-root-INFO: step: 240 lr_xt 0.00020221
2024-12-01-21:42:38-root-INFO: grad norm: 10404.202 8725.396 5667.000
2024-12-01-21:42:38-root-INFO: Loss too large (14752.286->15345.129)! Learning rate decreased to 0.00016.
2024-12-01-21:42:39-root-INFO: grad norm: 7405.115 6083.553 4222.098
2024-12-01-21:42:40-root-INFO: grad norm: 5369.453 4477.744 2963.247
2024-12-01-21:42:40-root-INFO: grad norm: 4204.622 3502.670 2325.973
2024-12-01-21:42:41-root-INFO: grad norm: 3252.470 2727.821 1771.313
2024-12-01-21:42:41-root-INFO: grad norm: 2604.464 2169.485 1441.030
2024-12-01-21:42:42-root-INFO: grad norm: 2071.623 1754.801 1101.043
2024-12-01-21:42:42-root-INFO: grad norm: 1671.269 1394.117 921.725
2024-12-01-21:42:42-root-INFO: Loss Change: 14752.286 -> 9043.087
2024-12-01-21:42:42-root-INFO: Regularization Change: 0.000 -> 1.697
2024-12-01-21:42:42-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-01-21:42:42-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:43-root-INFO: step: 239 lr_xt 0.00021275
2024-12-01-21:42:43-root-INFO: grad norm: 1202.717 1032.115 617.468
2024-12-01-21:42:43-root-INFO: grad norm: 1380.028 1157.234 751.854
2024-12-01-21:42:44-root-INFO: grad norm: 1609.935 1374.322 838.528
2024-12-01-21:42:44-root-INFO: grad norm: 1881.511 1575.116 1029.121
2024-12-01-21:42:44-root-INFO: Loss too large (8845.744->8858.484)! Learning rate decreased to 0.00017.
2024-12-01-21:42:45-root-INFO: grad norm: 1436.451 1232.215 738.267
2024-12-01-21:42:45-root-INFO: grad norm: 1122.254 950.148 597.221
2024-12-01-21:42:46-root-INFO: grad norm: 891.459 783.974 424.364
2024-12-01-21:42:46-root-INFO: grad norm: 730.776 634.501 362.548
2024-12-01-21:42:47-root-INFO: Loss Change: 8901.297 -> 8505.154
2024-12-01-21:42:47-root-INFO: Regularization Change: 0.000 -> 0.524
2024-12-01-21:42:47-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-01-21:42:47-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:47-root-INFO: step: 238 lr_xt 0.00022380
2024-12-01-21:42:47-root-INFO: grad norm: 696.272 590.414 369.062
2024-12-01-21:42:47-root-INFO: grad norm: 509.863 453.659 232.711
2024-12-01-21:42:48-root-INFO: grad norm: 484.347 444.021 193.488
2024-12-01-21:42:48-root-INFO: grad norm: 470.979 429.757 192.692
2024-12-01-21:42:49-root-INFO: grad norm: 464.059 433.847 164.704
2024-12-01-21:42:49-root-INFO: grad norm: 461.648 424.500 181.435
2024-12-01-21:42:50-root-INFO: grad norm: 463.230 435.594 157.607
2024-12-01-21:42:50-root-INFO: grad norm: 468.330 429.782 186.066
2024-12-01-21:42:51-root-INFO: Loss Change: 8335.826 -> 7991.535
2024-12-01-21:42:51-root-INFO: Regularization Change: 0.000 -> 0.582
2024-12-01-21:42:51-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-01-21:42:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:51-root-INFO: step: 237 lr_xt 0.00023539
2024-12-01-21:42:51-root-INFO: grad norm: 450.650 409.074 189.060
2024-12-01-21:42:51-root-INFO: grad norm: 411.810 390.306 131.334
2024-12-01-21:42:52-root-INFO: grad norm: 403.380 378.905 138.370
2024-12-01-21:42:52-root-INFO: grad norm: 400.513 383.015 117.090
2024-12-01-21:42:53-root-INFO: grad norm: 399.748 375.803 136.272
2024-12-01-21:42:53-root-INFO: grad norm: 400.216 382.426 117.995
2024-12-01-21:42:54-root-INFO: grad norm: 401.805 376.347 140.749
2024-12-01-21:42:54-root-INFO: grad norm: 404.400 385.107 123.418
2024-12-01-21:42:55-root-INFO: Loss Change: 7920.005 -> 7642.830
2024-12-01-21:42:55-root-INFO: Regularization Change: 0.000 -> 0.519
2024-12-01-21:42:55-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-01-21:42:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:55-root-INFO: step: 236 lr_xt 0.00024753
2024-12-01-21:42:55-root-INFO: grad norm: 473.914 431.943 194.985
2024-12-01-21:42:55-root-INFO: grad norm: 460.580 426.926 172.823
2024-12-01-21:42:56-root-INFO: grad norm: 464.698 424.170 189.800
2024-12-01-21:42:56-root-INFO: grad norm: 472.197 437.139 178.546
2024-12-01-21:42:57-root-INFO: grad norm: 481.651 436.803 202.956
2024-12-01-21:42:57-root-INFO: grad norm: 493.476 454.094 193.178
2024-12-01-21:42:58-root-INFO: grad norm: 506.067 455.338 220.841
2024-12-01-21:42:58-root-INFO: grad norm: 520.878 476.089 211.313
2024-12-01-21:42:58-root-INFO: Loss Change: 7509.629 -> 7269.010
2024-12-01-21:42:58-root-INFO: Regularization Change: 0.000 -> 0.487
2024-12-01-21:42:58-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-01-21:42:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-21:42:58-root-INFO: step: 235 lr_xt 0.00026027
2024-12-01-21:42:59-root-INFO: grad norm: 882.883 769.991 431.969
2024-12-01-21:42:59-root-INFO: grad norm: 876.408 777.999 403.495
2024-12-01-21:43:00-root-INFO: grad norm: 886.009 770.226 437.908
2024-12-01-21:43:00-root-INFO: grad norm: 898.358 796.255 415.963
2024-12-01-21:43:01-root-INFO: grad norm: 912.798 793.810 450.627
2024-12-01-21:43:01-root-INFO: grad norm: 929.065 821.497 433.939
2024-12-01-21:43:02-root-INFO: grad norm: 944.917 821.588 466.756
2024-12-01-21:43:02-root-INFO: grad norm: 962.278 849.110 452.760
2024-12-01-21:43:02-root-INFO: Loss Change: 7231.253 -> 7029.182
2024-12-01-21:43:02-root-INFO: Regularization Change: 0.000 -> 0.446
2024-12-01-21:43:02-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-01-21:43:02-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-21:43:02-root-INFO: step: 234 lr_xt 0.00027361
2024-12-01-21:43:03-root-INFO: grad norm: 1065.567 926.644 526.083
2024-12-01-21:43:03-root-INFO: grad norm: 1049.342 922.717 499.711
2024-12-01-21:43:04-root-INFO: grad norm: 1040.951 906.380 511.913
2024-12-01-21:43:04-root-INFO: grad norm: 1032.991 909.161 490.404
2024-12-01-21:43:05-root-INFO: grad norm: 1026.212 893.816 504.186
2024-12-01-21:43:06-root-INFO: grad norm: 1019.340 897.135 483.945
2024-12-01-21:43:07-root-INFO: grad norm: 1012.677 882.184 497.259
2024-12-01-21:43:08-root-INFO: grad norm: 1006.063 885.350 477.828
2024-12-01-21:43:08-root-INFO: Loss Change: 6991.083 -> 6785.738
2024-12-01-21:43:08-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-01-21:43:08-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-01-21:43:08-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:08-root-INFO: step: 233 lr_xt 0.00028759
2024-12-01-21:43:09-root-INFO: grad norm: 1172.443 1025.130 568.974
2024-12-01-21:43:10-root-INFO: grad norm: 1078.149 948.345 512.881
2024-12-01-21:43:10-root-INFO: grad norm: 1024.946 895.922 497.833
2024-12-01-21:43:11-root-INFO: grad norm: 977.970 862.734 460.560
2024-12-01-21:43:12-root-INFO: grad norm: 936.897 820.175 452.868
2024-12-01-21:43:13-root-INFO: grad norm: 898.028 793.407 420.666
2024-12-01-21:43:14-root-INFO: grad norm: 864.107 757.151 416.417
2024-12-01-21:43:15-root-INFO: grad norm: 831.380 735.510 387.580
2024-12-01-21:43:15-root-INFO: Loss Change: 6811.803 -> 6550.853
2024-12-01-21:43:15-root-INFO: Regularization Change: 0.000 -> 0.468
2024-12-01-21:43:15-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-01-21:43:15-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:15-root-INFO: step: 232 lr_xt 0.00030224
2024-12-01-21:43:16-root-INFO: grad norm: 1251.580 1080.717 631.272
2024-12-01-21:43:17-root-INFO: grad norm: 1135.482 1008.361 522.042
2024-12-01-21:43:17-root-INFO: grad norm: 1069.378 932.839 522.857
2024-12-01-21:43:18-root-INFO: grad norm: 1010.447 898.397 462.477
2024-12-01-21:43:19-root-INFO: grad norm: 959.272 840.431 462.471
2024-12-01-21:43:20-root-INFO: grad norm: 910.616 809.586 416.883
2024-12-01-21:43:21-root-INFO: grad norm: 869.239 763.714 415.110
2024-12-01-21:43:21-root-INFO: grad norm: 829.185 737.364 379.265
2024-12-01-21:43:21-root-INFO: Loss Change: 6517.724 -> 6248.528
2024-12-01-21:43:21-root-INFO: Regularization Change: 0.000 -> 0.466
2024-12-01-21:43:21-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-01-21:43:21-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:22-root-INFO: step: 231 lr_xt 0.00031758
2024-12-01-21:43:22-root-INFO: grad norm: 945.635 834.014 445.696
2024-12-01-21:43:22-root-INFO: grad norm: 880.138 781.897 404.078
2024-12-01-21:43:23-root-INFO: grad norm: 825.034 727.228 389.641
2024-12-01-21:43:23-root-INFO: grad norm: 771.753 687.330 350.970
2024-12-01-21:43:24-root-INFO: grad norm: 724.087 639.036 340.493
2024-12-01-21:43:24-root-INFO: grad norm: 679.036 606.368 305.628
2024-12-01-21:43:24-root-INFO: grad norm: 640.584 566.368 299.291
2024-12-01-21:43:25-root-INFO: grad norm: 604.094 540.986 268.820
2024-12-01-21:43:25-root-INFO: Loss Change: 6225.788 -> 6015.402
2024-12-01-21:43:25-root-INFO: Regularization Change: 0.000 -> 0.419
2024-12-01-21:43:25-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-01-21:43:25-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:25-root-INFO: step: 230 lr_xt 0.00033364
2024-12-01-21:43:26-root-INFO: grad norm: 829.766 728.881 396.541
2024-12-01-21:43:26-root-INFO: grad norm: 751.819 672.305 336.509
2024-12-01-21:43:27-root-INFO: grad norm: 692.288 610.375 326.658
2024-12-01-21:43:27-root-INFO: grad norm: 637.473 571.173 283.078
2024-12-01-21:43:28-root-INFO: grad norm: 590.793 522.901 274.974
2024-12-01-21:43:28-root-INFO: grad norm: 548.045 492.309 240.802
2024-12-01-21:43:28-root-INFO: grad norm: 511.082 454.199 234.325
2024-12-01-21:43:29-root-INFO: grad norm: 476.926 429.981 206.335
2024-12-01-21:43:29-root-INFO: Loss Change: 5984.818 -> 5784.390
2024-12-01-21:43:29-root-INFO: Regularization Change: 0.000 -> 0.425
2024-12-01-21:43:29-root-INFO: Undo step: 230
2024-12-01-21:43:29-root-INFO: Undo step: 231
2024-12-01-21:43:29-root-INFO: Undo step: 232
2024-12-01-21:43:29-root-INFO: Undo step: 233
2024-12-01-21:43:29-root-INFO: Undo step: 234
2024-12-01-21:43:29-root-INFO: step: 235 lr_xt 0.00026027
2024-12-01-21:43:30-root-INFO: grad norm: 9770.587 7253.690 6545.866
2024-12-01-21:43:30-root-INFO: grad norm: 4668.967 3701.778 2845.363
2024-12-01-21:43:31-root-INFO: grad norm: 3325.882 2736.308 1890.532
2024-12-01-21:43:31-root-INFO: grad norm: 3079.402 2594.488 1658.718
2024-12-01-21:43:31-root-INFO: grad norm: 3045.762 2580.205 1618.397
2024-12-01-21:43:32-root-INFO: grad norm: 3069.158 2609.078 1616.306
2024-12-01-21:43:32-root-INFO: grad norm: 3115.091 2649.765 1637.844
2024-12-01-21:43:33-root-INFO: grad norm: 3162.508 2695.011 1654.804
2024-12-01-21:43:33-root-INFO: Loss Change: 17662.699 -> 7832.640
2024-12-01-21:43:33-root-INFO: Regularization Change: 0.000 -> 5.395
2024-12-01-21:43:33-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-01-21:43:33-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-21:43:34-root-INFO: step: 234 lr_xt 0.00027361
2024-12-01-21:43:34-root-INFO: grad norm: 3006.790 2565.741 1567.725
2024-12-01-21:43:34-root-INFO: grad norm: 2941.301 2515.259 1524.705
2024-12-01-21:43:35-root-INFO: grad norm: 2874.853 2458.868 1489.546
2024-12-01-21:43:35-root-INFO: grad norm: 2811.010 2406.831 1452.219
2024-12-01-21:43:36-root-INFO: grad norm: 2744.649 2351.620 1415.268
2024-12-01-21:43:36-root-INFO: grad norm: 2678.017 2295.882 1378.658
2024-12-01-21:43:37-root-INFO: grad norm: 2609.999 2239.528 1340.377
2024-12-01-21:43:37-root-INFO: grad norm: 2541.926 2181.856 1304.182
2024-12-01-21:43:37-root-INFO: Loss Change: 7728.276 -> 7069.101
2024-12-01-21:43:37-root-INFO: Regularization Change: 0.000 -> 1.036
2024-12-01-21:43:37-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-01-21:43:37-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:37-root-INFO: step: 233 lr_xt 0.00028759
2024-12-01-21:43:38-root-INFO: grad norm: 2260.257 1934.174 1169.500
2024-12-01-21:43:38-root-INFO: grad norm: 2072.707 1788.517 1047.531
2024-12-01-21:43:39-root-INFO: grad norm: 1936.204 1672.549 975.431
2024-12-01-21:43:39-root-INFO: grad norm: 1824.052 1577.620 915.578
2024-12-01-21:43:39-root-INFO: grad norm: 1717.134 1485.557 861.204
2024-12-01-21:43:40-root-INFO: grad norm: 1622.749 1405.535 811.040
2024-12-01-21:43:40-root-INFO: grad norm: 1532.307 1327.145 765.931
2024-12-01-21:43:41-root-INFO: grad norm: 1451.080 1258.315 722.688
2024-12-01-21:43:41-root-INFO: Loss Change: 7001.328 -> 6473.705
2024-12-01-21:43:41-root-INFO: Regularization Change: 0.000 -> 0.672
2024-12-01-21:43:41-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-01-21:43:41-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:41-root-INFO: step: 232 lr_xt 0.00030224
2024-12-01-21:43:42-root-INFO: grad norm: 1009.311 870.771 510.359
2024-12-01-21:43:42-root-INFO: grad norm: 827.707 717.141 413.287
2024-12-01-21:43:42-root-INFO: grad norm: 753.990 666.605 352.332
2024-12-01-21:43:43-root-INFO: grad norm: 694.827 609.485 333.634
2024-12-01-21:43:43-root-INFO: grad norm: 642.499 568.544 299.269
2024-12-01-21:43:44-root-INFO: grad norm: 598.463 529.033 279.789
2024-12-01-21:43:44-root-INFO: grad norm: 558.832 495.737 257.949
2024-12-01-21:43:45-root-INFO: grad norm: 523.297 465.638 238.792
2024-12-01-21:43:45-root-INFO: Loss Change: 6311.137 -> 6045.364
2024-12-01-21:43:45-root-INFO: Regularization Change: 0.000 -> 0.485
2024-12-01-21:43:45-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-01-21:43:45-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:45-root-INFO: step: 231 lr_xt 0.00031758
2024-12-01-21:43:45-root-INFO: grad norm: 371.572 335.494 159.718
2024-12-01-21:43:46-root-INFO: grad norm: 336.262 307.382 136.339
2024-12-01-21:43:46-root-INFO: grad norm: 318.030 293.233 123.116
2024-12-01-21:43:47-root-INFO: grad norm: 302.478 279.647 115.284
2024-12-01-21:43:47-root-INFO: grad norm: 289.242 269.270 105.616
2024-12-01-21:43:48-root-INFO: grad norm: 278.270 259.406 100.710
2024-12-01-21:43:48-root-INFO: grad norm: 268.410 252.018 92.363
2024-12-01-21:43:49-root-INFO: grad norm: 260.061 244.189 89.460
2024-12-01-21:43:49-root-INFO: Loss Change: 5963.599 -> 5814.967
2024-12-01-21:43:49-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-01-21:43:49-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-01-21:43:49-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:49-root-INFO: step: 230 lr_xt 0.00033364
2024-12-01-21:43:49-root-INFO: grad norm: 318.431 287.775 136.322
2024-12-01-21:43:50-root-INFO: grad norm: 260.446 244.071 90.891
2024-12-01-21:43:50-root-INFO: grad norm: 249.098 233.099 87.834
2024-12-01-21:43:51-root-INFO: grad norm: 241.223 228.614 76.966
2024-12-01-21:43:51-root-INFO: grad norm: 235.245 222.363 76.779
2024-12-01-21:43:52-root-INFO: grad norm: 230.432 219.584 69.871
2024-12-01-21:43:53-root-INFO: grad norm: 226.644 215.489 70.231
2024-12-01-21:43:54-root-INFO: grad norm: 223.524 213.739 65.413
2024-12-01-21:43:54-root-INFO: Loss Change: 5758.192 -> 5618.204
2024-12-01-21:43:54-root-INFO: Regularization Change: 0.000 -> 0.361
2024-12-01-21:43:54-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-01-21:43:55-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:43:55-root-INFO: step: 229 lr_xt 0.00035047
2024-12-01-21:43:55-root-INFO: grad norm: 489.092 429.868 233.291
2024-12-01-21:43:56-root-INFO: grad norm: 375.520 342.071 154.929
2024-12-01-21:43:57-root-INFO: grad norm: 334.953 303.048 142.674
2024-12-01-21:43:58-root-INFO: grad norm: 303.906 280.677 116.531
2024-12-01-21:43:59-root-INFO: grad norm: 280.654 258.480 109.338
2024-12-01-21:43:59-root-INFO: grad norm: 262.486 245.544 92.772
2024-12-01-21:44:00-root-INFO: grad norm: 248.218 231.808 88.755
2024-12-01-21:44:01-root-INFO: grad norm: 237.191 224.233 77.324
2024-12-01-21:44:02-root-INFO: Loss Change: 5600.563 -> 5431.361
2024-12-01-21:44:02-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-01-21:44:02-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-01-21:44:02-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:44:02-root-INFO: step: 228 lr_xt 0.00036807
2024-12-01-21:44:02-root-INFO: grad norm: 371.253 323.238 182.609
2024-12-01-21:44:03-root-INFO: grad norm: 295.691 271.605 116.892
2024-12-01-21:44:04-root-INFO: grad norm: 265.285 245.101 101.498
2024-12-01-21:44:05-root-INFO: grad norm: 246.685 232.282 83.059
2024-12-01-21:44:06-root-INFO: grad norm: 233.745 219.114 81.397
2024-12-01-21:44:07-root-INFO: grad norm: 224.241 213.190 69.527
2024-12-01-21:44:07-root-INFO: grad norm: 217.211 205.357 70.776
2024-12-01-21:44:08-root-INFO: grad norm: 211.721 202.374 62.213
2024-12-01-21:44:09-root-INFO: Loss Change: 5424.437 -> 5288.182
2024-12-01-21:44:09-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-01-21:44:09-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-01-21:44:09-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:44:09-root-INFO: step: 227 lr_xt 0.00038651
2024-12-01-21:44:09-root-INFO: grad norm: 562.866 489.948 277.071
2024-12-01-21:44:10-root-INFO: grad norm: 419.385 382.626 171.701
2024-12-01-21:44:10-root-INFO: grad norm: 352.074 316.620 153.976
2024-12-01-21:44:11-root-INFO: grad norm: 304.940 282.979 113.628
2024-12-01-21:44:11-root-INFO: grad norm: 271.486 249.163 107.810
2024-12-01-21:44:12-root-INFO: grad norm: 247.574 233.228 83.050
2024-12-01-21:44:12-root-INFO: grad norm: 230.784 215.435 82.760
2024-12-01-21:44:13-root-INFO: grad norm: 218.776 208.370 66.669
2024-12-01-21:44:13-root-INFO: Loss Change: 5323.265 -> 5139.636
2024-12-01-21:44:13-root-INFO: Regularization Change: 0.000 -> 0.475
2024-12-01-21:44:13-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-01-21:44:13-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-21:44:13-root-INFO: step: 226 lr_xt 0.00040579
2024-12-01-21:44:13-root-INFO: grad norm: 294.834 250.179 156.005
2024-12-01-21:44:14-root-INFO: grad norm: 215.894 201.138 78.447
2024-12-01-21:44:14-root-INFO: grad norm: 201.617 189.761 68.119
2024-12-01-21:44:15-root-INFO: grad norm: 196.067 186.644 60.054
2024-12-01-21:44:15-root-INFO: grad norm: 193.023 182.961 61.506
2024-12-01-21:44:16-root-INFO: grad norm: 190.705 181.922 57.209
2024-12-01-21:44:16-root-INFO: grad norm: 188.827 179.566 58.410
2024-12-01-21:44:17-root-INFO: grad norm: 187.288 178.760 55.873
2024-12-01-21:44:17-root-INFO: Loss Change: 5105.854 -> 4976.711
2024-12-01-21:44:17-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-01-21:44:17-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-01-21:44:17-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:44:17-root-INFO: step: 225 lr_xt 0.00042598
2024-12-01-21:44:17-root-INFO: grad norm: 488.608 422.809 244.889
2024-12-01-21:44:18-root-INFO: grad norm: 336.774 311.999 126.782
2024-12-01-21:44:18-root-INFO: grad norm: 279.493 250.726 123.501
2024-12-01-21:44:19-root-INFO: grad norm: 242.292 228.374 80.935
2024-12-01-21:44:19-root-INFO: grad norm: 218.176 200.816 85.286
2024-12-01-21:44:20-root-INFO: grad norm: 203.028 193.386 61.824
2024-12-01-21:44:20-root-INFO: grad norm: 193.542 181.217 67.963
2024-12-01-21:44:20-root-INFO: grad norm: 187.291 179.111 54.746
2024-12-01-21:44:21-root-INFO: Loss Change: 4972.366 -> 4809.714
2024-12-01-21:44:21-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-01-21:44:21-root-INFO: Undo step: 225
2024-12-01-21:44:21-root-INFO: Undo step: 226
2024-12-01-21:44:21-root-INFO: Undo step: 227
2024-12-01-21:44:21-root-INFO: Undo step: 228
2024-12-01-21:44:21-root-INFO: Undo step: 229
2024-12-01-21:44:21-root-INFO: step: 230 lr_xt 0.00033364
2024-12-01-21:44:21-root-INFO: grad norm: 3734.335 2861.911 2398.901
2024-12-01-21:44:22-root-INFO: grad norm: 1470.670 1175.291 884.060
2024-12-01-21:44:22-root-INFO: grad norm: 956.997 857.616 424.663
2024-12-01-21:44:23-root-INFO: grad norm: 791.395 689.616 388.248
2024-12-01-21:44:23-root-INFO: grad norm: 698.174 639.754 279.574
2024-12-01-21:44:24-root-INFO: grad norm: 625.316 552.346 293.146
2024-12-01-21:44:24-root-INFO: grad norm: 563.563 515.625 227.450
2024-12-01-21:44:25-root-INFO: grad norm: 511.141 455.305 232.298
2024-12-01-21:44:25-root-INFO: Loss Change: 8499.283 -> 5692.768
2024-12-01-21:44:25-root-INFO: Regularization Change: 0.000 -> 3.628
2024-12-01-21:44:25-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-01-21:44:25-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:44:25-root-INFO: step: 229 lr_xt 0.00035047
2024-12-01-21:44:25-root-INFO: grad norm: 669.062 597.457 301.146
2024-12-01-21:44:26-root-INFO: grad norm: 531.945 476.752 235.953
2024-12-01-21:44:26-root-INFO: grad norm: 478.580 436.419 196.411
2024-12-01-21:44:27-root-INFO: grad norm: 434.858 390.641 191.053
2024-12-01-21:44:27-root-INFO: grad norm: 398.547 366.489 156.607
2024-12-01-21:44:28-root-INFO: grad norm: 367.883 332.699 157.000
2024-12-01-21:44:28-root-INFO: grad norm: 342.710 317.436 129.169
2024-12-01-21:44:29-root-INFO: grad norm: 321.621 293.230 132.122
2024-12-01-21:44:29-root-INFO: Loss Change: 5595.968 -> 5344.237
2024-12-01-21:44:29-root-INFO: Regularization Change: 0.000 -> 0.615
2024-12-01-21:44:29-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-01-21:44:29-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:44:29-root-INFO: step: 228 lr_xt 0.00036807
2024-12-01-21:44:29-root-INFO: grad norm: 454.673 404.501 207.620
2024-12-01-21:44:30-root-INFO: grad norm: 377.280 340.876 161.691
2024-12-01-21:44:30-root-INFO: grad norm: 338.585 313.047 129.000
2024-12-01-21:44:31-root-INFO: grad norm: 310.815 285.355 123.200
2024-12-01-21:44:31-root-INFO: grad norm: 289.630 270.206 104.281
2024-12-01-21:44:32-root-INFO: grad norm: 272.684 252.842 102.116
2024-12-01-21:44:32-root-INFO: grad norm: 258.949 243.215 88.888
2024-12-01-21:44:33-root-INFO: grad norm: 247.637 231.606 87.653
2024-12-01-21:44:33-root-INFO: Loss Change: 5317.207 -> 5146.550
2024-12-01-21:44:33-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-01-21:44:33-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-01-21:44:33-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-21:44:33-root-INFO: step: 227 lr_xt 0.00038651
2024-12-01-21:44:33-root-INFO: grad norm: 618.080 545.720 290.193
2024-12-01-21:44:34-root-INFO: grad norm: 491.256 447.692 202.248
2024-12-01-21:44:34-root-INFO: grad norm: 426.067 387.066 178.082
2024-12-01-21:44:35-root-INFO: grad norm: 375.480 344.671 148.953
2024-12-01-21:44:35-root-INFO: grad norm: 335.965 308.980 131.923
2024-12-01-21:44:36-root-INFO: grad norm: 303.774 281.109 115.137
2024-12-01-21:44:36-root-INFO: grad norm: 278.719 259.114 102.683
2024-12-01-21:44:37-root-INFO: grad norm: 258.676 241.580 92.480
2024-12-01-21:44:37-root-INFO: Loss Change: 5162.296 -> 4969.262
2024-12-01-21:44:37-root-INFO: Regularization Change: 0.000 -> 0.492
2024-12-01-21:44:37-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-01-21:44:37-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-21:44:37-root-INFO: step: 226 lr_xt 0.00040579
2024-12-01-21:44:38-root-INFO: grad norm: 312.179 270.555 155.742
2024-12-01-21:44:38-root-INFO: grad norm: 233.594 216.570 87.540
2024-12-01-21:44:38-root-INFO: grad norm: 213.647 201.449 71.156
2024-12-01-21:44:39-root-INFO: grad norm: 203.636 193.755 62.661
2024-12-01-21:44:39-root-INFO: grad norm: 197.047 187.305 61.192
2024-12-01-21:44:40-root-INFO: grad norm: 191.944 183.743 55.505
2024-12-01-21:44:40-root-INFO: grad norm: 187.876 179.412 55.757
2024-12-01-21:44:41-root-INFO: grad norm: 184.593 177.220 51.647
2024-12-01-21:44:41-root-INFO: Loss Change: 4922.736 -> 4795.827
2024-12-01-21:44:41-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-01-21:44:41-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-01-21:44:41-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:44:41-root-INFO: step: 225 lr_xt 0.00042598
2024-12-01-21:44:42-root-INFO: grad norm: 541.320 473.920 261.586
2024-12-01-21:44:42-root-INFO: grad norm: 409.710 379.685 153.953
2024-12-01-21:44:42-root-INFO: grad norm: 349.908 317.056 148.023
2024-12-01-21:44:43-root-INFO: grad norm: 304.080 284.666 106.909
2024-12-01-21:44:43-root-INFO: grad norm: 269.393 247.882 105.484
2024-12-01-21:44:44-root-INFO: grad norm: 242.243 229.027 78.919
2024-12-01-21:44:44-root-INFO: grad norm: 221.667 206.910 79.528
2024-12-01-21:44:45-root-INFO: grad norm: 205.802 196.369 61.591
2024-12-01-21:44:45-root-INFO: Loss Change: 4783.246 -> 4630.258
2024-12-01-21:44:45-root-INFO: Regularization Change: 0.000 -> 0.417
2024-12-01-21:44:45-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-01-21:44:45-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:44:45-root-INFO: step: 224 lr_xt 0.00044709
2024-12-01-21:44:45-root-INFO: grad norm: 417.359 367.648 197.543
2024-12-01-21:44:46-root-INFO: grad norm: 335.065 311.526 123.368
2024-12-01-21:44:47-root-INFO: grad norm: 292.269 266.989 118.903
2024-12-01-21:44:47-root-INFO: grad norm: 258.972 243.693 87.635
2024-12-01-21:44:48-root-INFO: grad norm: 232.887 215.703 87.798
2024-12-01-21:44:48-root-INFO: grad norm: 212.517 201.934 66.228
2024-12-01-21:44:48-root-INFO: grad norm: 197.053 184.918 68.082
2024-12-01-21:44:49-root-INFO: grad norm: 185.104 177.212 53.472
2024-12-01-21:44:49-root-INFO: Loss Change: 4629.936 -> 4502.986
2024-12-01-21:44:49-root-INFO: Regularization Change: 0.000 -> 0.394
2024-12-01-21:44:49-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-01-21:44:49-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:44:49-root-INFO: step: 223 lr_xt 0.00046917
2024-12-01-21:44:50-root-INFO: grad norm: 434.172 385.818 199.122
2024-12-01-21:44:50-root-INFO: grad norm: 342.741 320.060 122.610
2024-12-01-21:44:50-root-INFO: grad norm: 294.139 268.418 120.289
2024-12-01-21:44:51-root-INFO: grad norm: 256.018 241.500 84.986
2024-12-01-21:44:51-root-INFO: grad norm: 226.966 209.851 86.466
2024-12-01-21:44:52-root-INFO: grad norm: 204.923 194.977 63.066
2024-12-01-21:44:52-root-INFO: grad norm: 188.554 176.574 66.136
2024-12-01-21:44:53-root-INFO: grad norm: 176.199 168.778 50.599
2024-12-01-21:44:53-root-INFO: Loss Change: 4507.893 -> 4386.472
2024-12-01-21:44:53-root-INFO: Regularization Change: 0.000 -> 0.381
2024-12-01-21:44:53-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-01-21:44:53-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:44:53-root-INFO: step: 222 lr_xt 0.00049227
2024-12-01-21:44:54-root-INFO: grad norm: 218.977 201.922 84.726
2024-12-01-21:44:54-root-INFO: grad norm: 191.028 181.261 60.301
2024-12-01-21:44:54-root-INFO: grad norm: 175.032 165.121 58.063
2024-12-01-21:44:55-root-INFO: grad norm: 163.936 157.061 46.978
2024-12-01-21:44:55-root-INFO: grad norm: 156.097 148.605 47.779
2024-12-01-21:44:56-root-INFO: grad norm: 150.668 144.907 41.267
2024-12-01-21:44:56-root-INFO: grad norm: 146.741 140.471 42.435
2024-12-01-21:44:57-root-INFO: grad norm: 143.899 138.622 38.613
2024-12-01-21:44:57-root-INFO: Loss Change: 4320.323 -> 4235.147
2024-12-01-21:44:57-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-01-21:44:57-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-01-21:44:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:44:57-root-INFO: step: 221 lr_xt 0.00051641
2024-12-01-21:44:58-root-INFO: grad norm: 228.828 206.788 97.985
2024-12-01-21:44:58-root-INFO: grad norm: 183.293 174.776 55.224
2024-12-01-21:44:59-root-INFO: grad norm: 164.989 154.811 57.051
2024-12-01-21:44:59-root-INFO: grad norm: 153.295 147.436 41.976
2024-12-01-21:45:00-root-INFO: grad norm: 145.711 138.364 45.686
2024-12-01-21:45:00-root-INFO: grad norm: 140.789 135.632 37.756
2024-12-01-21:45:00-root-INFO: grad norm: 137.479 131.436 40.312
2024-12-01-21:45:01-root-INFO: grad norm: 135.214 130.254 36.287
2024-12-01-21:45:01-root-INFO: Loss Change: 4206.987 -> 4124.467
2024-12-01-21:45:01-root-INFO: Regularization Change: 0.000 -> 0.322
2024-12-01-21:45:01-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-01-21:45:01-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:45:01-root-INFO: step: 220 lr_xt 0.00054166
2024-12-01-21:45:02-root-INFO: grad norm: 411.261 364.702 190.075
2024-12-01-21:45:02-root-INFO: grad norm: 299.869 281.292 103.903
2024-12-01-21:45:03-root-INFO: grad norm: 243.429 222.390 98.996
2024-12-01-21:45:03-root-INFO: grad norm: 203.694 193.036 65.026
2024-12-01-21:45:03-root-INFO: grad norm: 176.893 164.503 65.038
2024-12-01-21:45:04-root-INFO: grad norm: 158.802 151.756 46.777
2024-12-01-21:45:04-root-INFO: grad norm: 146.911 138.790 48.167
2024-12-01-21:45:05-root-INFO: grad norm: 139.136 133.577 38.938
2024-12-01-21:45:05-root-INFO: Loss Change: 4125.167 -> 4008.570
2024-12-01-21:45:05-root-INFO: Regularization Change: 0.000 -> 0.405
2024-12-01-21:45:05-root-INFO: Undo step: 220
2024-12-01-21:45:05-root-INFO: Undo step: 221
2024-12-01-21:45:05-root-INFO: Undo step: 222
2024-12-01-21:45:05-root-INFO: Undo step: 223
2024-12-01-21:45:05-root-INFO: Undo step: 224
2024-12-01-21:45:05-root-INFO: step: 225 lr_xt 0.00042598
2024-12-01-21:45:06-root-INFO: grad norm: 1845.967 1524.881 1040.352
2024-12-01-21:45:06-root-INFO: grad norm: 1411.619 1328.613 476.922
2024-12-01-21:45:06-root-INFO: grad norm: 1237.891 1098.225 571.206
2024-12-01-21:45:07-root-INFO: grad norm: 1080.908 1008.205 389.724
2024-12-01-21:45:07-root-INFO: grad norm: 943.956 842.506 425.718
2024-12-01-21:45:08-root-INFO: grad norm: 818.721 756.314 313.517
2024-12-01-21:45:08-root-INFO: grad norm: 710.184 635.495 317.030
2024-12-01-21:45:09-root-INFO: grad norm: 615.204 564.565 244.420
2024-12-01-21:45:09-root-INFO: Loss Change: 5730.742 -> 4681.288
2024-12-01-21:45:09-root-INFO: Regularization Change: 0.000 -> 2.284
2024-12-01-21:45:09-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-01-21:45:09-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:45:09-root-INFO: step: 224 lr_xt 0.00044709
2024-12-01-21:45:09-root-INFO: grad norm: 372.363 330.225 172.062
2024-12-01-21:45:10-root-INFO: grad norm: 298.891 270.063 128.069
2024-12-01-21:45:10-root-INFO: grad norm: 269.495 242.227 118.126
2024-12-01-21:45:11-root-INFO: grad norm: 246.063 223.903 102.052
2024-12-01-21:45:11-root-INFO: grad norm: 227.296 205.759 96.576
2024-12-01-21:45:12-root-INFO: grad norm: 212.233 194.162 85.696
2024-12-01-21:45:12-root-INFO: grad norm: 200.116 182.668 81.726
2024-12-01-21:45:13-root-INFO: grad norm: 190.541 175.470 74.270
2024-12-01-21:45:13-root-INFO: Loss Change: 4616.382 -> 4457.801
2024-12-01-21:45:13-root-INFO: Regularization Change: 0.000 -> 0.530
2024-12-01-21:45:13-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-01-21:45:13-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:45:13-root-INFO: step: 223 lr_xt 0.00046917
2024-12-01-21:45:13-root-INFO: grad norm: 316.717 283.674 140.851
2024-12-01-21:45:14-root-INFO: grad norm: 242.786 223.948 93.766
2024-12-01-21:45:14-root-INFO: grad norm: 215.103 197.376 85.509
2024-12-01-21:45:15-root-INFO: grad norm: 195.522 182.214 70.899
2024-12-01-21:45:15-root-INFO: grad norm: 181.538 168.432 67.727
2024-12-01-21:45:16-root-INFO: grad norm: 171.380 160.666 59.645
2024-12-01-21:45:16-root-INFO: grad norm: 164.359 153.755 58.080
2024-12-01-21:45:17-root-INFO: grad norm: 159.219 149.938 53.566
2024-12-01-21:45:17-root-INFO: Loss Change: 4443.111 -> 4331.584
2024-12-01-21:45:17-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-01-21:45:17-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-01-21:45:17-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:45:17-root-INFO: step: 222 lr_xt 0.00049227
2024-12-01-21:45:17-root-INFO: grad norm: 198.278 180.858 81.267
2024-12-01-21:45:18-root-INFO: grad norm: 174.676 163.422 61.683
2024-12-01-21:45:18-root-INFO: grad norm: 163.912 153.570 57.300
2024-12-01-21:45:19-root-INFO: grad norm: 156.662 147.665 52.326
2024-12-01-21:45:19-root-INFO: grad norm: 151.828 143.125 50.666
2024-12-01-21:45:20-root-INFO: grad norm: 148.324 140.383 47.881
2024-12-01-21:45:20-root-INFO: grad norm: 145.768 137.939 47.128
2024-12-01-21:45:21-root-INFO: grad norm: 143.745 136.426 45.282
2024-12-01-21:45:21-root-INFO: Loss Change: 4260.961 -> 4173.509
2024-12-01-21:45:21-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-01-21:45:21-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-01-21:45:21-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:45:21-root-INFO: step: 221 lr_xt 0.00051641
2024-12-01-21:45:21-root-INFO: grad norm: 211.178 190.033 92.106
2024-12-01-21:45:22-root-INFO: grad norm: 170.638 161.053 56.385
2024-12-01-21:45:22-root-INFO: grad norm: 155.796 145.454 55.818
2024-12-01-21:45:23-root-INFO: grad norm: 147.077 139.724 45.924
2024-12-01-21:45:23-root-INFO: grad norm: 141.754 133.610 47.357
2024-12-01-21:45:24-root-INFO: grad norm: 138.301 131.545 42.697
2024-12-01-21:45:24-root-INFO: grad norm: 136.028 128.850 43.603
2024-12-01-21:45:24-root-INFO: grad norm: 134.366 127.855 41.319
2024-12-01-21:45:25-root-INFO: Loss Change: 4148.758 -> 4067.213
2024-12-01-21:45:25-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-01-21:45:25-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-01-21:45:25-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:45:25-root-INFO: step: 220 lr_xt 0.00054166
2024-12-01-21:45:25-root-INFO: grad norm: 389.423 344.966 180.690
2024-12-01-21:45:26-root-INFO: grad norm: 274.734 255.601 100.732
2024-12-01-21:45:26-root-INFO: grad norm: 220.391 200.310 91.914
2024-12-01-21:45:27-root-INFO: grad norm: 184.451 173.032 63.892
2024-12-01-21:45:27-root-INFO: grad norm: 161.768 149.667 61.390
2024-12-01-21:45:27-root-INFO: grad norm: 147.643 139.478 48.419
2024-12-01-21:45:28-root-INFO: grad norm: 139.066 130.543 47.937
2024-12-01-21:45:28-root-INFO: grad norm: 133.798 126.992 42.128
2024-12-01-21:45:29-root-INFO: Loss Change: 4067.822 -> 3957.135
2024-12-01-21:45:29-root-INFO: Regularization Change: 0.000 -> 0.392
2024-12-01-21:45:29-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-01-21:45:29-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:45:29-root-INFO: step: 219 lr_xt 0.00056804
2024-12-01-21:45:29-root-INFO: grad norm: 150.168 135.997 63.682
2024-12-01-21:45:30-root-INFO: grad norm: 132.655 125.405 43.253
2024-12-01-21:45:30-root-INFO: grad norm: 129.630 121.840 44.260
2024-12-01-21:45:30-root-INFO: grad norm: 127.850 120.839 41.757
2024-12-01-21:45:31-root-INFO: grad norm: 126.412 119.292 41.826
2024-12-01-21:45:31-root-INFO: grad norm: 125.234 118.516 40.467
2024-12-01-21:45:32-root-INFO: grad norm: 124.233 117.521 40.283
2024-12-01-21:45:32-root-INFO: grad norm: 123.232 116.782 39.345
2024-12-01-21:45:33-root-INFO: Loss Change: 3934.362 -> 3859.865
2024-12-01-21:45:33-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-01-21:45:33-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-01-21:45:33-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-21:45:33-root-INFO: step: 218 lr_xt 0.00059561
2024-12-01-21:45:33-root-INFO: grad norm: 253.631 227.738 111.642
2024-12-01-21:45:33-root-INFO: grad norm: 187.996 176.670 64.264
2024-12-01-21:45:34-root-INFO: grad norm: 155.171 142.784 60.753
2024-12-01-21:45:34-root-INFO: grad norm: 137.307 129.977 44.263
2024-12-01-21:45:35-root-INFO: grad norm: 127.944 119.928 44.576
2024-12-01-21:45:35-root-INFO: grad norm: 123.073 116.817 38.740
2024-12-01-21:45:36-root-INFO: grad norm: 120.304 113.776 39.090
2024-12-01-21:45:36-root-INFO: grad norm: 118.548 112.702 36.766
2024-12-01-21:45:37-root-INFO: Loss Change: 3848.096 -> 3765.082
2024-12-01-21:45:37-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-01-21:45:37-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-01-21:45:37-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:45:37-root-INFO: step: 217 lr_xt 0.00062443
2024-12-01-21:45:37-root-INFO: grad norm: 262.463 226.782 132.124
2024-12-01-21:45:37-root-INFO: grad norm: 174.943 164.240 60.254
2024-12-01-21:45:38-root-INFO: grad norm: 144.577 132.732 57.312
2024-12-01-21:45:38-root-INFO: grad norm: 129.363 122.243 42.324
2024-12-01-21:45:39-root-INFO: grad norm: 121.786 114.074 42.649
2024-12-01-21:45:39-root-INFO: grad norm: 117.787 111.621 37.611
2024-12-01-21:45:40-root-INFO: grad norm: 115.479 109.129 37.765
2024-12-01-21:45:41-root-INFO: grad norm: 113.927 108.161 35.785
2024-12-01-21:45:41-root-INFO: Loss Change: 3762.377 -> 3677.285
2024-12-01-21:45:41-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-01-21:45:41-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-01-21:45:41-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:45:41-root-INFO: step: 216 lr_xt 0.00065452
2024-12-01-21:45:41-root-INFO: grad norm: 272.297 239.036 130.412
2024-12-01-21:45:42-root-INFO: grad norm: 178.332 165.705 65.910
2024-12-01-21:45:42-root-INFO: grad norm: 145.688 133.873 57.471
2024-12-01-21:45:43-root-INFO: grad norm: 129.142 121.260 44.427
2024-12-01-21:45:43-root-INFO: grad norm: 120.700 112.933 42.599
2024-12-01-21:45:44-root-INFO: grad norm: 115.976 109.547 38.078
2024-12-01-21:45:44-root-INFO: grad norm: 113.195 106.852 37.360
2024-12-01-21:45:45-root-INFO: grad norm: 111.361 105.560 35.475
2024-12-01-21:45:45-root-INFO: Loss Change: 3680.252 -> 3589.555
2024-12-01-21:45:45-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-01-21:45:45-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-01-21:45:45-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:45:45-root-INFO: step: 215 lr_xt 0.00068596
2024-12-01-21:45:45-root-INFO: grad norm: 123.487 114.180 47.029
2024-12-01-21:45:46-root-INFO: grad norm: 113.675 107.372 37.328
2024-12-01-21:45:46-root-INFO: grad norm: 110.238 103.755 37.247
2024-12-01-21:45:47-root-INFO: grad norm: 108.291 102.642 34.518
2024-12-01-21:45:47-root-INFO: grad norm: 106.960 101.277 34.401
2024-12-01-21:45:48-root-INFO: grad norm: 105.911 100.579 33.183
2024-12-01-21:45:48-root-INFO: grad norm: 105.073 99.807 32.848
2024-12-01-21:45:49-root-INFO: grad norm: 104.307 99.240 32.115
2024-12-01-21:45:49-root-INFO: Loss Change: 3568.420 -> 3504.088
2024-12-01-21:45:49-root-INFO: Regularization Change: 0.000 -> 0.350
2024-12-01-21:45:49-root-INFO: Undo step: 215
2024-12-01-21:45:49-root-INFO: Undo step: 216
2024-12-01-21:45:49-root-INFO: Undo step: 217
2024-12-01-21:45:49-root-INFO: Undo step: 218
2024-12-01-21:45:49-root-INFO: Undo step: 219
2024-12-01-21:45:49-root-INFO: step: 220 lr_xt 0.00054166
2024-12-01-21:45:49-root-INFO: grad norm: 3528.181 2917.344 1984.229
2024-12-01-21:45:50-root-INFO: grad norm: 1866.091 1747.044 655.843
2024-12-01-21:45:50-root-INFO: grad norm: 1631.477 1508.517 621.364
2024-12-01-21:45:51-root-INFO: grad norm: 1385.527 1313.760 440.138
2024-12-01-21:45:51-root-INFO: grad norm: 1151.760 1054.635 462.920
2024-12-01-21:45:52-root-INFO: grad norm: 936.169 877.680 325.716
2024-12-01-21:45:52-root-INFO: grad norm: 758.337 691.222 311.908
2024-12-01-21:45:53-root-INFO: grad norm: 614.074 570.841 226.336
2024-12-01-21:45:53-root-INFO: Loss Change: 8048.274 -> 4219.189
2024-12-01-21:45:53-root-INFO: Regularization Change: 0.000 -> 6.722
2024-12-01-21:45:53-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-01-21:45:53-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-21:45:53-root-INFO: step: 219 lr_xt 0.00056804
2024-12-01-21:45:53-root-INFO: grad norm: 467.328 422.313 200.118
2024-12-01-21:45:54-root-INFO: grad norm: 370.638 340.658 146.032
2024-12-01-21:45:54-root-INFO: grad norm: 303.134 275.569 126.302
2024-12-01-21:45:55-root-INFO: grad norm: 257.551 234.589 106.303
2024-12-01-21:45:55-root-INFO: grad norm: 227.106 206.332 94.890
2024-12-01-21:45:56-root-INFO: grad norm: 206.731 187.672 86.699
2024-12-01-21:45:56-root-INFO: grad norm: 193.163 175.870 79.885
2024-12-01-21:45:57-root-INFO: grad norm: 183.771 166.970 76.765
2024-12-01-21:45:57-root-INFO: Loss Change: 4206.543 -> 3995.089
2024-12-01-21:45:57-root-INFO: Regularization Change: 0.000 -> 0.851
2024-12-01-21:45:57-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-01-21:45:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-21:45:57-root-INFO: step: 218 lr_xt 0.00059561
2024-12-01-21:45:57-root-INFO: grad norm: 268.427 244.724 110.288
2024-12-01-21:45:58-root-INFO: grad norm: 214.492 197.707 83.179
2024-12-01-21:45:58-root-INFO: grad norm: 188.607 171.282 78.963
2024-12-01-21:45:58-root-INFO: grad norm: 173.179 160.041 66.167
2024-12-01-21:45:59-root-INFO: grad norm: 163.692 149.974 65.595
2024-12-01-21:45:59-root-INFO: grad norm: 157.497 145.783 59.602
2024-12-01-21:46:00-root-INFO: grad norm: 153.124 141.158 59.341
2024-12-01-21:46:00-root-INFO: grad norm: 149.661 138.816 55.933
2024-12-01-21:46:01-root-INFO: Loss Change: 3964.933 -> 3832.411
2024-12-01-21:46:01-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-01-21:46:01-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-01-21:46:01-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:01-root-INFO: step: 217 lr_xt 0.00062443
2024-12-01-21:46:01-root-INFO: grad norm: 283.755 247.377 139.001
2024-12-01-21:46:02-root-INFO: grad norm: 203.833 189.904 74.058
2024-12-01-21:46:02-root-INFO: grad norm: 173.669 157.611 72.938
2024-12-01-21:46:03-root-INFO: grad norm: 156.515 146.230 55.801
2024-12-01-21:46:03-root-INFO: grad norm: 146.544 134.997 57.019
2024-12-01-21:46:04-root-INFO: grad norm: 140.350 131.389 49.345
2024-12-01-21:46:04-root-INFO: grad norm: 136.213 126.614 50.228
2024-12-01-21:46:04-root-INFO: grad norm: 133.166 124.854 46.311
2024-12-01-21:46:05-root-INFO: Loss Change: 3822.006 -> 3705.893
2024-12-01-21:46:05-root-INFO: Regularization Change: 0.000 -> 0.526
2024-12-01-21:46:05-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-01-21:46:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:05-root-INFO: step: 216 lr_xt 0.00065452
2024-12-01-21:46:05-root-INFO: grad norm: 290.689 258.544 132.872
2024-12-01-21:46:06-root-INFO: grad norm: 203.577 191.666 68.613
2024-12-01-21:46:06-root-INFO: grad norm: 170.285 156.600 66.884
2024-12-01-21:46:07-root-INFO: grad norm: 150.054 142.346 47.473
2024-12-01-21:46:07-root-INFO: grad norm: 137.505 128.257 49.574
2024-12-01-21:46:08-root-INFO: grad norm: 129.658 123.413 39.756
2024-12-01-21:46:08-root-INFO: grad norm: 124.525 117.247 41.948
2024-12-01-21:46:09-root-INFO: grad norm: 121.057 115.348 36.737
2024-12-01-21:46:09-root-INFO: Loss Change: 3691.161 -> 3587.484
2024-12-01-21:46:09-root-INFO: Regularization Change: 0.000 -> 0.472
2024-12-01-21:46:09-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-01-21:46:09-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:09-root-INFO: step: 215 lr_xt 0.00068596
2024-12-01-21:46:09-root-INFO: grad norm: 137.066 127.412 50.531
2024-12-01-21:46:10-root-INFO: grad norm: 126.929 120.731 39.179
2024-12-01-21:46:10-root-INFO: grad norm: 121.614 113.906 42.608
2024-12-01-21:46:11-root-INFO: grad norm: 118.099 112.219 36.800
2024-12-01-21:46:11-root-INFO: grad norm: 115.535 108.819 38.818
2024-12-01-21:46:11-root-INFO: grad norm: 113.673 108.001 35.458
2024-12-01-21:46:12-root-INFO: grad norm: 112.109 106.020 36.444
2024-12-01-21:46:12-root-INFO: grad norm: 110.769 105.335 34.268
2024-12-01-21:46:13-root-INFO: Loss Change: 3572.281 -> 3498.855
2024-12-01-21:46:13-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-01-21:46:13-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-01-21:46:13-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:13-root-INFO: step: 214 lr_xt 0.00071879
2024-12-01-21:46:13-root-INFO: grad norm: 210.572 188.821 93.205
2024-12-01-21:46:14-root-INFO: grad norm: 157.177 150.155 46.457
2024-12-01-21:46:14-root-INFO: grad norm: 137.699 128.730 48.882
2024-12-01-21:46:15-root-INFO: grad norm: 125.845 121.099 34.234
2024-12-01-21:46:15-root-INFO: grad norm: 118.212 111.753 38.540
2024-12-01-21:46:16-root-INFO: grad norm: 113.082 108.982 30.175
2024-12-01-21:46:16-root-INFO: grad norm: 109.472 104.147 33.726
2024-12-01-21:46:16-root-INFO: grad norm: 106.896 103.023 28.513
2024-12-01-21:46:17-root-INFO: Loss Change: 3470.182 -> 3395.282
2024-12-01-21:46:17-root-INFO: Regularization Change: 0.000 -> 0.390
2024-12-01-21:46:17-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-01-21:46:17-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:17-root-INFO: step: 213 lr_xt 0.00075308
2024-12-01-21:46:17-root-INFO: grad norm: 265.812 236.483 121.375
2024-12-01-21:46:18-root-INFO: grad norm: 190.534 181.790 57.059
2024-12-01-21:46:18-root-INFO: grad norm: 168.873 157.646 60.544
2024-12-01-21:46:19-root-INFO: grad norm: 157.636 151.816 42.436
2024-12-01-21:46:19-root-INFO: grad norm: 150.719 142.238 49.846
2024-12-01-21:46:20-root-INFO: grad norm: 146.048 141.009 38.033
2024-12-01-21:46:20-root-INFO: grad norm: 142.737 135.264 45.581
2024-12-01-21:46:20-root-INFO: grad norm: 140.396 135.621 36.306
2024-12-01-21:46:21-root-INFO: Loss Change: 3403.395 -> 3318.482
2024-12-01-21:46:21-root-INFO: Regularization Change: 0.000 -> 0.435
2024-12-01-21:46:21-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-01-21:46:21-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:21-root-INFO: step: 212 lr_xt 0.00078886
2024-12-01-21:46:21-root-INFO: grad norm: 153.452 145.240 49.526
2024-12-01-21:46:22-root-INFO: grad norm: 150.101 145.039 38.650
2024-12-01-21:46:22-root-INFO: grad norm: 148.159 140.769 46.209
2024-12-01-21:46:23-root-INFO: grad norm: 146.985 142.072 37.686
2024-12-01-21:46:23-root-INFO: grad norm: 146.425 139.326 45.040
2024-12-01-21:46:24-root-INFO: grad norm: 146.414 141.485 37.669
2024-12-01-21:46:24-root-INFO: grad norm: 146.861 139.894 44.698
2024-12-01-21:46:25-root-INFO: grad norm: 147.853 142.804 38.308
2024-12-01-21:46:25-root-INFO: Loss Change: 3303.942 -> 3244.131
2024-12-01-21:46:25-root-INFO: Regularization Change: 0.000 -> 0.377
2024-12-01-21:46:25-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-01-21:46:25-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-21:46:25-root-INFO: step: 211 lr_xt 0.00082622
2024-12-01-21:46:25-root-INFO: grad norm: 306.574 275.399 134.694
2024-12-01-21:46:26-root-INFO: grad norm: 225.819 216.011 65.827
2024-12-01-21:46:26-root-INFO: grad norm: 229.668 218.504 70.734
2024-12-01-21:46:27-root-INFO: grad norm: 251.724 241.706 70.308
2024-12-01-21:46:27-root-INFO: grad norm: 279.178 266.906 81.866
2024-12-01-21:46:28-root-INFO: grad norm: 314.951 302.148 88.888
2024-12-01-21:46:28-root-INFO: grad norm: 350.449 335.458 101.402
2024-12-01-21:46:29-root-INFO: grad norm: 392.155 375.786 112.117
2024-12-01-21:46:29-root-INFO: Loss too large (3177.319->3179.394)! Learning rate decreased to 0.00066.
2024-12-01-21:46:29-root-INFO: Loss Change: 3236.719 -> 3159.221
2024-12-01-21:46:29-root-INFO: Regularization Change: 0.000 -> 0.447
2024-12-01-21:46:29-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-01-21:46:29-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:46:29-root-INFO: step: 210 lr_xt 0.00086520
2024-12-01-21:46:29-root-INFO: grad norm: 330.727 314.397 102.640
2024-12-01-21:46:30-root-INFO: grad norm: 355.641 342.262 96.632
2024-12-01-21:46:30-root-INFO: grad norm: 382.696 365.586 113.150
2024-12-01-21:46:31-root-INFO: grad norm: 414.768 398.229 115.958
2024-12-01-21:46:31-root-INFO: Loss too large (3145.900->3147.304)! Learning rate decreased to 0.00069.
2024-12-01-21:46:31-root-INFO: grad norm: 290.483 277.053 87.303
2024-12-01-21:46:32-root-INFO: grad norm: 201.414 194.351 52.874
2024-12-01-21:46:32-root-INFO: grad norm: 152.825 145.484 46.795
2024-12-01-21:46:33-root-INFO: grad norm: 123.089 119.119 31.010
2024-12-01-21:46:33-root-INFO: Loss Change: 3158.021 -> 3085.422
2024-12-01-21:46:33-root-INFO: Regularization Change: 0.000 -> 0.324
2024-12-01-21:46:33-root-INFO: Undo step: 210
2024-12-01-21:46:33-root-INFO: Undo step: 211
2024-12-01-21:46:33-root-INFO: Undo step: 212
2024-12-01-21:46:33-root-INFO: Undo step: 213
2024-12-01-21:46:33-root-INFO: Undo step: 214
2024-12-01-21:46:34-root-INFO: step: 215 lr_xt 0.00068596
2024-12-01-21:46:34-root-INFO: grad norm: 2705.076 2315.699 1398.205
2024-12-01-21:46:35-root-INFO: grad norm: 1615.820 1510.348 574.217
2024-12-01-21:46:36-root-INFO: grad norm: 897.093 833.306 332.231
2024-12-01-21:46:37-root-INFO: grad norm: 637.041 592.781 233.306
2024-12-01-21:46:37-root-INFO: grad norm: 448.013 416.175 165.875
2024-12-01-21:46:38-root-INFO: grad norm: 334.964 309.725 127.559
2024-12-01-21:46:39-root-INFO: grad norm: 260.779 241.517 98.362
2024-12-01-21:46:40-root-INFO: grad norm: 216.891 199.528 85.030
2024-12-01-21:46:41-root-INFO: Loss Change: 5600.486 -> 3567.705
2024-12-01-21:46:41-root-INFO: Regularization Change: 0.000 -> 4.497
2024-12-01-21:46:41-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-01-21:46:41-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:41-root-INFO: step: 214 lr_xt 0.00071879
2024-12-01-21:46:41-root-INFO: grad norm: 203.294 182.930 88.685
2024-12-01-21:46:42-root-INFO: grad norm: 172.666 158.407 68.709
2024-12-01-21:46:43-root-INFO: grad norm: 161.938 148.061 65.589
2024-12-01-21:46:44-root-INFO: grad norm: 154.382 141.889 60.840
2024-12-01-21:46:45-root-INFO: grad norm: 148.559 136.427 58.801
2024-12-01-21:46:45-root-INFO: grad norm: 143.708 132.306 56.098
2024-12-01-21:46:46-root-INFO: grad norm: 139.647 128.608 54.418
2024-12-01-21:46:47-root-INFO: grad norm: 135.955 125.426 52.459
2024-12-01-21:46:48-root-INFO: Loss Change: 3530.555 -> 3394.803
2024-12-01-21:46:48-root-INFO: Regularization Change: 0.000 -> 0.766
2024-12-01-21:46:48-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-01-21:46:48-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:48-root-INFO: step: 213 lr_xt 0.00075308
2024-12-01-21:46:48-root-INFO: grad norm: 249.841 222.252 114.125
2024-12-01-21:46:49-root-INFO: grad norm: 176.026 163.032 66.375
2024-12-01-21:46:50-root-INFO: grad norm: 147.005 134.044 60.356
2024-12-01-21:46:51-root-INFO: grad norm: 132.937 123.303 49.685
2024-12-01-21:46:52-root-INFO: grad norm: 125.522 115.957 48.058
2024-12-01-21:46:52-root-INFO: grad norm: 121.088 112.698 44.287
2024-12-01-21:46:53-root-INFO: grad norm: 118.053 109.816 43.324
2024-12-01-21:46:54-root-INFO: grad norm: 115.759 108.068 41.490
2024-12-01-21:46:55-root-INFO: Loss Change: 3391.920 -> 3280.796
2024-12-01-21:46:55-root-INFO: Regularization Change: 0.000 -> 0.607
2024-12-01-21:46:55-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-01-21:46:55-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-21:46:55-root-INFO: step: 212 lr_xt 0.00078886
2024-12-01-21:46:55-root-INFO: grad norm: 122.272 113.147 46.349
2024-12-01-21:46:56-root-INFO: grad norm: 115.942 108.666 40.425
2024-12-01-21:46:57-root-INFO: grad norm: 112.698 105.301 40.155
2024-12-01-21:46:58-root-INFO: grad norm: 110.657 103.753 38.475
2024-12-01-21:46:59-root-INFO: grad norm: 109.010 102.150 38.062
2024-12-01-21:47:00-root-INFO: grad norm: 107.556 100.978 37.036
2024-12-01-21:47:01-root-INFO: grad norm: 106.298 99.841 36.482
2024-12-01-21:47:01-root-INFO: grad norm: 105.192 98.920 35.780
2024-12-01-21:47:02-root-INFO: Loss Change: 3265.327 -> 3188.607
2024-12-01-21:47:02-root-INFO: Regularization Change: 0.000 -> 0.483
2024-12-01-21:47:02-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-01-21:47:02-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-21:47:02-root-INFO: step: 211 lr_xt 0.00082622
2024-12-01-21:47:03-root-INFO: grad norm: 250.712 218.614 122.739
2024-12-01-21:47:03-root-INFO: grad norm: 145.303 135.408 52.705
2024-12-01-21:47:04-root-INFO: grad norm: 118.850 109.663 45.819
2024-12-01-21:47:05-root-INFO: grad norm: 109.055 102.646 36.834
2024-12-01-21:47:06-root-INFO: grad norm: 104.691 98.347 35.890
2024-12-01-21:47:07-root-INFO: grad norm: 102.317 96.754 33.276
2024-12-01-21:47:08-root-INFO: grad norm: 100.801 95.246 33.002
2024-12-01-21:47:09-root-INFO: grad norm: 99.678 94.475 31.785
2024-12-01-21:47:09-root-INFO: Loss Change: 3176.444 -> 3081.043
2024-12-01-21:47:09-root-INFO: Regularization Change: 0.000 -> 0.534
2024-12-01-21:47:09-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-01-21:47:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:47:10-root-INFO: step: 210 lr_xt 0.00086520
2024-12-01-21:47:10-root-INFO: grad norm: 124.689 114.316 49.793
2024-12-01-21:47:11-root-INFO: grad norm: 106.956 102.085 31.909
2024-12-01-21:47:11-root-INFO: grad norm: 101.404 95.534 34.000
2024-12-01-21:47:12-root-INFO: grad norm: 98.730 93.998 30.199
2024-12-01-21:47:13-root-INFO: grad norm: 96.973 91.828 31.165
2024-12-01-21:47:14-root-INFO: grad norm: 95.841 91.216 29.414
2024-12-01-21:47:15-root-INFO: grad norm: 94.996 90.232 29.706
2024-12-01-21:47:16-root-INFO: grad norm: 94.280 89.791 28.745
2024-12-01-21:47:16-root-INFO: Loss Change: 3073.926 -> 3007.113
2024-12-01-21:47:16-root-INFO: Regularization Change: 0.000 -> 0.453
2024-12-01-21:47:16-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-01-21:47:16-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:47:17-root-INFO: step: 209 lr_xt 0.00090588
2024-12-01-21:47:17-root-INFO: grad norm: 156.213 138.800 71.675
2024-12-01-21:47:18-root-INFO: grad norm: 106.742 100.366 36.338
2024-12-01-21:47:19-root-INFO: grad norm: 97.569 92.025 32.422
2024-12-01-21:47:20-root-INFO: grad norm: 94.503 89.783 29.493
2024-12-01-21:47:21-root-INFO: grad norm: 92.999 88.581 28.324
2024-12-01-21:47:21-root-INFO: grad norm: 91.983 87.750 27.582
2024-12-01-21:47:22-root-INFO: grad norm: 91.180 87.104 26.956
2024-12-01-21:47:23-root-INFO: grad norm: 90.479 86.488 26.574
2024-12-01-21:47:24-root-INFO: Loss Change: 2989.412 -> 2918.400
2024-12-01-21:47:24-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-01-21:47:24-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-01-21:47:24-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:47:24-root-INFO: step: 208 lr_xt 0.00094831
2024-12-01-21:47:24-root-INFO: grad norm: 161.216 142.657 75.098
2024-12-01-21:47:25-root-INFO: grad norm: 107.661 101.669 35.415
2024-12-01-21:47:26-root-INFO: grad norm: 95.395 90.168 31.142
2024-12-01-21:47:27-root-INFO: grad norm: 91.720 87.514 27.455
2024-12-01-21:47:28-root-INFO: grad norm: 90.051 86.101 26.379
2024-12-01-21:47:29-root-INFO: grad norm: 89.069 85.332 25.531
2024-12-01-21:47:29-root-INFO: grad norm: 88.294 84.678 25.009
2024-12-01-21:47:30-root-INFO: grad norm: 87.627 84.091 24.640
2024-12-01-21:47:31-root-INFO: Loss Change: 2913.051 -> 2841.108
2024-12-01-21:47:31-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-01-21:47:31-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-01-21:47:31-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:47:31-root-INFO: step: 207 lr_xt 0.00100094
2024-12-01-21:47:31-root-INFO: grad norm: 117.049 107.093 47.238
2024-12-01-21:47:32-root-INFO: grad norm: 97.294 92.537 30.051
2024-12-01-21:47:33-root-INFO: grad norm: 96.086 92.124 27.306
2024-12-01-21:47:34-root-INFO: grad norm: 97.540 92.964 29.528
2024-12-01-21:47:35-root-INFO: grad norm: 100.827 97.133 27.042
2024-12-01-21:47:36-root-INFO: grad norm: 106.396 101.469 32.004
2024-12-01-21:47:37-root-INFO: grad norm: 115.434 111.336 30.488
2024-12-01-21:47:38-root-INFO: grad norm: 128.725 122.825 38.525
2024-12-01-21:47:38-root-INFO: Loss Change: 2819.525 -> 2758.777
2024-12-01-21:47:38-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-01-21:47:38-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-01-21:47:38-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:47:39-root-INFO: step: 206 lr_xt 0.00104745
2024-12-01-21:47:39-root-INFO: grad norm: 187.588 173.720 70.786
2024-12-01-21:47:40-root-INFO: grad norm: 211.214 201.240 64.140
2024-12-01-21:47:41-root-INFO: grad norm: 269.408 258.073 77.326
2024-12-01-21:47:41-root-INFO: Loss too large (2728.067->2732.666)! Learning rate decreased to 0.00084.
2024-12-01-21:47:42-root-INFO: grad norm: 237.358 226.826 69.921
2024-12-01-21:47:43-root-INFO: grad norm: 208.319 200.089 57.978
2024-12-01-21:47:44-root-INFO: grad norm: 186.712 178.416 55.038
2024-12-01-21:47:44-root-INFO: grad norm: 168.020 161.545 46.194
2024-12-01-21:47:45-root-INFO: grad norm: 154.074 147.278 45.254
2024-12-01-21:47:46-root-INFO: Loss Change: 2737.470 -> 2681.913
2024-12-01-21:47:46-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-01-21:47:46-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-21:47:46-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:47:46-root-INFO: step: 205 lr_xt 0.00109594
2024-12-01-21:47:46-root-INFO: grad norm: 190.961 174.504 77.554
2024-12-01-21:47:47-root-INFO: grad norm: 229.636 218.772 69.798
2024-12-01-21:47:47-root-INFO: Loss too large (2664.719->2667.934)! Learning rate decreased to 0.00088.
2024-12-01-21:47:48-root-INFO: grad norm: 224.883 215.002 65.925
2024-12-01-21:47:49-root-INFO: grad norm: 223.152 213.245 65.753
2024-12-01-21:47:50-root-INFO: grad norm: 222.842 213.453 64.000
2024-12-01-21:47:51-root-INFO: grad norm: 223.488 213.588 65.781
2024-12-01-21:47:52-root-INFO: grad norm: 225.066 215.673 64.341
2024-12-01-21:47:53-root-INFO: grad norm: 227.172 217.137 66.775
2024-12-01-21:47:53-root-INFO: Loss Change: 2671.391 -> 2622.710
2024-12-01-21:47:53-root-INFO: Regularization Change: 0.000 -> 0.369
2024-12-01-21:47:53-root-INFO: Undo step: 205
2024-12-01-21:47:53-root-INFO: Undo step: 206
2024-12-01-21:47:53-root-INFO: Undo step: 207
2024-12-01-21:47:53-root-INFO: Undo step: 208
2024-12-01-21:47:53-root-INFO: Undo step: 209
2024-12-01-21:47:54-root-INFO: step: 210 lr_xt 0.00086520
2024-12-01-21:47:54-root-INFO: grad norm: 2918.669 2551.081 1417.962
2024-12-01-21:47:55-root-INFO: grad norm: 1456.955 1382.680 459.252
2024-12-01-21:47:56-root-INFO: grad norm: 1042.124 959.650 406.319
2024-12-01-21:47:56-root-INFO: grad norm: 722.497 684.583 230.973
2024-12-01-21:47:57-root-INFO: grad norm: 464.431 433.428 166.844
2024-12-01-21:47:58-root-INFO: grad norm: 324.699 303.017 116.662
2024-12-01-21:47:59-root-INFO: grad norm: 241.616 225.490 86.790
2024-12-01-21:48:00-root-INFO: grad norm: 197.018 181.817 75.885
2024-12-01-21:48:01-root-INFO: Loss Change: 6774.140 -> 3071.504
2024-12-01-21:48:01-root-INFO: Regularization Change: 0.000 -> 8.421
2024-12-01-21:48:01-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-01-21:48:01-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:48:01-root-INFO: step: 209 lr_xt 0.00090588
2024-12-01-21:48:01-root-INFO: grad norm: 180.093 165.507 71.000
2024-12-01-21:48:02-root-INFO: grad norm: 155.255 143.304 59.734
2024-12-01-21:48:03-root-INFO: grad norm: 146.640 136.383 53.878
2024-12-01-21:48:04-root-INFO: grad norm: 140.359 130.521 51.624
2024-12-01-21:48:05-root-INFO: grad norm: 135.086 125.987 48.739
2024-12-01-21:48:06-root-INFO: grad norm: 130.483 121.811 46.774
2024-12-01-21:48:06-root-INFO: grad norm: 126.413 118.228 44.749
2024-12-01-21:48:07-root-INFO: grad norm: 122.731 114.908 43.114
2024-12-01-21:48:08-root-INFO: Loss Change: 3044.079 -> 2901.224
2024-12-01-21:48:08-root-INFO: Regularization Change: 0.000 -> 1.020
2024-12-01-21:48:08-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-01-21:48:08-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:48:08-root-INFO: step: 208 lr_xt 0.00094831
2024-12-01-21:48:08-root-INFO: grad norm: 174.572 157.297 75.716
2024-12-01-21:48:09-root-INFO: grad norm: 132.217 124.760 43.775
2024-12-01-21:48:10-root-INFO: grad norm: 119.936 112.293 42.130
2024-12-01-21:48:11-root-INFO: grad norm: 114.383 108.290 36.833
2024-12-01-21:48:12-root-INFO: grad norm: 110.810 104.313 37.385
2024-12-01-21:48:13-root-INFO: grad norm: 107.995 102.380 34.370
2024-12-01-21:48:14-root-INFO: grad norm: 105.555 99.647 34.817
2024-12-01-21:48:15-root-INFO: grad norm: 103.349 98.098 32.524
2024-12-01-21:48:15-root-INFO: Loss Change: 2893.026 -> 2790.896
2024-12-01-21:48:15-root-INFO: Regularization Change: 0.000 -> 0.738
2024-12-01-21:48:15-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-01-21:48:15-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:48:15-root-INFO: step: 207 lr_xt 0.00100094
2024-12-01-21:48:16-root-INFO: grad norm: 131.959 120.549 53.676
2024-12-01-21:48:17-root-INFO: grad norm: 107.635 101.554 35.665
2024-12-01-21:48:17-root-INFO: grad norm: 102.063 96.485 33.278
2024-12-01-21:48:18-root-INFO: grad norm: 99.329 93.933 32.294
2024-12-01-21:48:19-root-INFO: grad norm: 97.347 92.364 30.744
2024-12-01-21:48:20-root-INFO: grad norm: 95.660 90.610 30.670
2024-12-01-21:48:21-root-INFO: grad norm: 94.191 89.532 29.256
2024-12-01-21:48:22-root-INFO: grad norm: 92.880 88.106 29.394
2024-12-01-21:48:22-root-INFO: Loss Change: 2769.643 -> 2689.337
2024-12-01-21:48:22-root-INFO: Regularization Change: 0.000 -> 0.627
2024-12-01-21:48:22-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-01-21:48:22-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:48:23-root-INFO: step: 206 lr_xt 0.00104745
2024-12-01-21:48:23-root-INFO: grad norm: 130.012 118.155 54.245
2024-12-01-21:48:24-root-INFO: grad norm: 107.324 101.382 35.218
2024-12-01-21:48:25-root-INFO: grad norm: 106.292 101.373 31.962
2024-12-01-21:48:25-root-INFO: grad norm: 111.405 105.652 35.337
2024-12-01-21:48:26-root-INFO: grad norm: 121.035 116.024 34.463
2024-12-01-21:48:27-root-INFO: grad norm: 135.331 128.590 42.178
2024-12-01-21:48:28-root-INFO: grad norm: 156.806 150.222 44.959
2024-12-01-21:48:29-root-INFO: grad norm: 185.237 176.318 56.786
2024-12-01-21:48:29-root-INFO: Loss Change: 2663.587 -> 2605.739
2024-12-01-21:48:29-root-INFO: Regularization Change: 0.000 -> 0.550
2024-12-01-21:48:29-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-21:48:29-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-21:48:30-root-INFO: step: 205 lr_xt 0.00109594
2024-12-01-21:48:30-root-INFO: grad norm: 292.073 271.699 107.175
2024-12-01-21:48:30-root-INFO: Loss too large (2595.205->2597.302)! Learning rate decreased to 0.00088.
2024-12-01-21:48:31-root-INFO: grad norm: 250.085 237.421 78.576
2024-12-01-21:48:32-root-INFO: grad norm: 223.719 213.353 67.309
2024-12-01-21:48:33-root-INFO: grad norm: 204.868 194.890 63.158
2024-12-01-21:48:34-root-INFO: grad norm: 188.563 180.180 55.598
2024-12-01-21:48:35-root-INFO: grad norm: 175.952 167.483 53.932
2024-12-01-21:48:36-root-INFO: grad norm: 164.876 157.626 48.354
2024-12-01-21:48:36-root-INFO: grad norm: 156.055 148.631 47.561
2024-12-01-21:48:37-root-INFO: Loss Change: 2595.205 -> 2530.459
2024-12-01-21:48:37-root-INFO: Regularization Change: 0.000 -> 0.351
2024-12-01-21:48:37-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-21:48:37-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-21:48:37-root-INFO: step: 204 lr_xt 0.00114648
2024-12-01-21:48:37-root-INFO: grad norm: 229.086 207.345 97.410
2024-12-01-21:48:38-root-INFO: grad norm: 286.325 270.725 93.220
2024-12-01-21:48:39-root-INFO: Loss too large (2515.296->2527.904)! Learning rate decreased to 0.00092.
2024-12-01-21:48:40-root-INFO: grad norm: 290.798 276.375 90.446
2024-12-01-21:48:40-root-INFO: grad norm: 298.846 284.309 92.070
2024-12-01-21:48:41-root-INFO: grad norm: 308.502 293.687 94.454
2024-12-01-21:48:42-root-INFO: grad norm: 316.304 301.075 96.963
2024-12-01-21:48:43-root-INFO: grad norm: 323.493 307.916 99.175
2024-12-01-21:48:44-root-INFO: grad norm: 327.759 312.063 100.215
2024-12-01-21:48:45-root-INFO: Loss Change: 2520.894 -> 2478.909
2024-12-01-21:48:45-root-INFO: Regularization Change: 0.000 -> 0.402
2024-12-01-21:48:45-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-21:48:45-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:48:45-root-INFO: step: 203 lr_xt 0.00119917
2024-12-01-21:48:45-root-INFO: grad norm: 391.491 370.998 125.004
2024-12-01-21:48:45-root-INFO: Loss too large (2477.126->2516.562)! Learning rate decreased to 0.00096.
2024-12-01-21:48:46-root-INFO: grad norm: 395.408 376.645 120.359
2024-12-01-21:48:47-root-INFO: grad norm: 396.057 376.880 121.750
2024-12-01-21:48:48-root-INFO: grad norm: 391.353 372.958 118.574
2024-12-01-21:48:49-root-INFO: grad norm: 381.858 363.343 117.461
2024-12-01-21:48:50-root-INFO: grad norm: 370.164 352.771 112.135
2024-12-01-21:48:50-root-INFO: grad norm: 354.979 337.708 109.376
2024-12-01-21:48:51-root-INFO: grad norm: 340.564 324.578 103.119
2024-12-01-21:48:52-root-INFO: Loss Change: 2477.126 -> 2423.493
2024-12-01-21:48:52-root-INFO: Regularization Change: 0.000 -> 0.345
2024-12-01-21:48:52-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-01-21:48:52-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:48:52-root-INFO: step: 202 lr_xt 0.00125407
2024-12-01-21:48:52-root-INFO: grad norm: 494.406 454.789 193.916
2024-12-01-21:48:53-root-INFO: Loss too large (2444.129->2487.144)! Learning rate decreased to 0.00100.
2024-12-01-21:48:53-root-INFO: grad norm: 472.825 450.368 143.984
2024-12-01-21:48:54-root-INFO: grad norm: 473.479 448.489 151.790
2024-12-01-21:48:55-root-INFO: grad norm: 467.071 445.388 140.656
2024-12-01-21:48:56-root-INFO: grad norm: 452.605 429.713 142.119
2024-12-01-21:48:57-root-INFO: grad norm: 433.495 413.279 130.840
2024-12-01-21:48:58-root-INFO: grad norm: 407.033 386.475 127.720
2024-12-01-21:48:59-root-INFO: grad norm: 382.774 364.883 115.659
2024-12-01-21:48:59-root-INFO: Loss Change: 2444.129 -> 2357.650
2024-12-01-21:48:59-root-INFO: Regularization Change: 0.000 -> 0.462
2024-12-01-21:48:59-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-01-21:48:59-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:49:00-root-INFO: step: 201 lr_xt 0.00131127
2024-12-01-21:49:00-root-INFO: grad norm: 329.020 312.448 103.103
2024-12-01-21:49:00-root-INFO: Loss too large (2352.194->2377.267)! Learning rate decreased to 0.00105.
2024-12-01-21:49:01-root-INFO: grad norm: 322.373 306.297 100.532
2024-12-01-21:49:02-root-INFO: grad norm: 318.716 302.824 99.386
2024-12-01-21:49:03-root-INFO: grad norm: 317.397 301.890 97.997
2024-12-01-21:49:04-root-INFO: grad norm: 318.053 302.129 99.375
2024-12-01-21:49:05-root-INFO: grad norm: 319.625 304.208 98.070
2024-12-01-21:49:06-root-INFO: grad norm: 322.561 306.375 100.895
2024-12-01-21:49:07-root-INFO: grad norm: 325.344 309.786 99.405
2024-12-01-21:49:07-root-INFO: Loss Change: 2352.194 -> 2307.632
2024-12-01-21:49:07-root-INFO: Regularization Change: 0.000 -> 0.365
2024-12-01-21:49:07-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-01-21:49:07-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:49:07-root-INFO: step: 200 lr_xt 0.00137086
2024-12-01-21:49:08-root-INFO: grad norm: 474.525 442.228 172.070
2024-12-01-21:49:08-root-INFO: Loss too large (2320.993->2386.614)! Learning rate decreased to 0.00110.
2024-12-01-21:49:09-root-INFO: grad norm: 480.003 457.238 146.070
2024-12-01-21:49:10-root-INFO: grad norm: 492.045 466.526 156.403
2024-12-01-21:49:10-root-INFO: Loss too large (2309.446->2311.546)! Learning rate decreased to 0.00088.
2024-12-01-21:49:11-root-INFO: grad norm: 320.086 304.965 97.219
2024-12-01-21:49:12-root-INFO: grad norm: 208.048 196.931 67.097
2024-12-01-21:49:13-root-INFO: grad norm: 150.126 143.299 44.758
2024-12-01-21:49:14-root-INFO: grad norm: 114.847 108.743 36.945
2024-12-01-21:49:15-root-INFO: grad norm: 95.633 91.406 28.118
2024-12-01-21:49:15-root-INFO: Loss Change: 2320.993 -> 2220.649
2024-12-01-21:49:15-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-01-21:49:15-root-INFO: Undo step: 200
2024-12-01-21:49:15-root-INFO: Undo step: 201
2024-12-01-21:49:15-root-INFO: Undo step: 202
2024-12-01-21:49:15-root-INFO: Undo step: 203
2024-12-01-21:49:15-root-INFO: Undo step: 204
2024-12-01-21:49:16-root-INFO: step: 205 lr_xt 0.00109594
2024-12-01-21:49:16-root-INFO: grad norm: 1277.285 1061.217 710.827
2024-12-01-21:49:17-root-INFO: grad norm: 549.964 470.388 284.949
2024-12-01-21:49:18-root-INFO: grad norm: 447.149 423.602 143.190
2024-12-01-21:49:18-root-INFO: grad norm: 440.027 399.977 183.419
2024-12-01-21:49:19-root-INFO: grad norm: 445.101 432.074 106.897
2024-12-01-21:49:20-root-INFO: grad norm: 459.608 432.516 155.465
2024-12-01-21:49:21-root-INFO: grad norm: 465.195 452.557 107.697
2024-12-01-21:49:22-root-INFO: grad norm: 469.759 448.635 139.284
2024-12-01-21:49:23-root-INFO: Loss Change: 4380.863 -> 2740.717
2024-12-01-21:49:23-root-INFO: Regularization Change: 0.000 -> 8.444
2024-12-01-21:49:23-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-21:49:23-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-21:49:23-root-INFO: step: 204 lr_xt 0.00114648
2024-12-01-21:49:23-root-INFO: grad norm: 522.714 505.406 133.397
2024-12-01-21:49:24-root-INFO: grad norm: 520.458 504.057 129.628
2024-12-01-21:49:25-root-INFO: grad norm: 531.134 517.956 117.576
2024-12-01-21:49:26-root-INFO: grad norm: 553.060 536.190 135.554
2024-12-01-21:49:26-root-INFO: Loss too large (2661.737->2667.775)! Learning rate decreased to 0.00092.
2024-12-01-21:49:27-root-INFO: grad norm: 384.834 375.023 86.343
2024-12-01-21:49:28-root-INFO: grad norm: 262.542 252.976 70.226
2024-12-01-21:49:29-root-INFO: grad norm: 206.549 200.037 51.455
2024-12-01-21:49:30-root-INFO: grad norm: 170.528 162.662 51.194
2024-12-01-21:49:30-root-INFO: Loss Change: 2728.921 -> 2528.854
2024-12-01-21:49:30-root-INFO: Regularization Change: 0.000 -> 1.020
2024-12-01-21:49:30-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-21:49:30-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:49:31-root-INFO: step: 203 lr_xt 0.00119917
2024-12-01-21:49:31-root-INFO: grad norm: 147.874 140.809 45.163
2024-12-01-21:49:32-root-INFO: grad norm: 167.056 159.776 48.778
2024-12-01-21:49:33-root-INFO: grad norm: 210.641 204.755 49.445
2024-12-01-21:49:34-root-INFO: grad norm: 297.228 287.841 74.112
2024-12-01-21:49:34-root-INFO: Loss too large (2488.994->2503.312)! Learning rate decreased to 0.00096.
2024-12-01-21:49:35-root-INFO: grad norm: 297.720 290.671 64.404
2024-12-01-21:49:36-root-INFO: grad norm: 303.575 294.197 74.875
2024-12-01-21:49:36-root-INFO: grad norm: 312.789 305.530 66.998
2024-12-01-21:49:37-root-INFO: grad norm: 328.565 318.783 79.576
2024-12-01-21:49:38-root-INFO: Loss Change: 2512.299 -> 2456.757
2024-12-01-21:49:38-root-INFO: Regularization Change: 0.000 -> 0.675
2024-12-01-21:49:38-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-01-21:49:38-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:49:38-root-INFO: step: 202 lr_xt 0.00125407
2024-12-01-21:49:38-root-INFO: grad norm: 365.992 351.209 102.970
2024-12-01-21:49:39-root-INFO: grad norm: 516.012 502.362 117.900
2024-12-01-21:49:40-root-INFO: Loss too large (2443.317->2545.237)! Learning rate decreased to 0.00100.
2024-12-01-21:49:40-root-INFO: Loss too large (2443.317->2464.658)! Learning rate decreased to 0.00080.
2024-12-01-21:49:41-root-INFO: grad norm: 397.121 388.132 84.016
2024-12-01-21:49:42-root-INFO: grad norm: 294.741 285.949 71.455
2024-12-01-21:49:42-root-INFO: grad norm: 241.855 235.907 53.310
2024-12-01-21:49:43-root-INFO: grad norm: 200.268 193.566 51.376
2024-12-01-21:49:44-root-INFO: grad norm: 175.542 170.739 40.782
2024-12-01-21:49:45-root-INFO: grad norm: 156.611 150.757 42.415
2024-12-01-21:49:46-root-INFO: Loss Change: 2449.390 -> 2344.459
2024-12-01-21:49:46-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-01-21:49:46-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-01-21:49:46-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:49:46-root-INFO: step: 201 lr_xt 0.00131127
2024-12-01-21:49:46-root-INFO: grad norm: 171.062 165.934 41.567
2024-12-01-21:49:47-root-INFO: Loss too large (2343.084->2346.270)! Learning rate decreased to 0.00105.
2024-12-01-21:49:48-root-INFO: grad norm: 226.734 220.101 54.444
2024-12-01-21:49:48-root-INFO: Loss too large (2338.681->2342.162)! Learning rate decreased to 0.00084.
2024-12-01-21:49:49-root-INFO: grad norm: 221.437 215.846 49.444
2024-12-01-21:49:50-root-INFO: grad norm: 219.779 213.294 52.996
2024-12-01-21:49:51-root-INFO: grad norm: 221.687 216.187 49.073
2024-12-01-21:49:52-root-INFO: grad norm: 227.349 220.730 54.460
2024-12-01-21:49:52-root-INFO: grad norm: 235.249 229.610 51.201
2024-12-01-21:49:53-root-INFO: grad norm: 247.622 240.626 58.442
2024-12-01-21:49:54-root-INFO: Loss Change: 2343.084 -> 2301.663
2024-12-01-21:49:54-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-01-21:49:54-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-01-21:49:54-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:49:54-root-INFO: step: 200 lr_xt 0.00137086
2024-12-01-21:49:55-root-INFO: grad norm: 234.903 224.744 68.333
2024-12-01-21:49:56-root-INFO: grad norm: 380.154 371.037 82.757
2024-12-01-21:49:56-root-INFO: Loss too large (2271.222->2384.889)! Learning rate decreased to 0.00110.
2024-12-01-21:49:56-root-INFO: Loss too large (2271.222->2319.301)! Learning rate decreased to 0.00088.
2024-12-01-21:49:56-root-INFO: Loss too large (2271.222->2279.828)! Learning rate decreased to 0.00070.
2024-12-01-21:49:57-root-INFO: grad norm: 301.258 294.561 63.169
2024-12-01-21:49:58-root-INFO: grad norm: 234.169 227.602 55.067
2024-12-01-21:49:59-root-INFO: grad norm: 194.710 189.936 42.848
2024-12-01-21:50:00-root-INFO: grad norm: 163.233 157.825 41.671
2024-12-01-21:50:01-root-INFO: grad norm: 142.929 138.898 33.704
2024-12-01-21:50:02-root-INFO: grad norm: 126.986 122.108 34.859
2024-12-01-21:50:02-root-INFO: Loss Change: 2274.723 -> 2213.680
2024-12-01-21:50:02-root-INFO: Regularization Change: 0.000 -> 0.305
2024-12-01-21:50:02-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-21:50:02-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:50:03-root-INFO: step: 199 lr_xt 0.00143293
2024-12-01-21:50:03-root-INFO: grad norm: 135.230 125.774 49.677
2024-12-01-21:50:04-root-INFO: grad norm: 114.595 110.618 29.929
2024-12-01-21:50:05-root-INFO: grad norm: 202.616 198.299 41.601
2024-12-01-21:50:05-root-INFO: Loss too large (2183.723->2223.143)! Learning rate decreased to 0.00115.
2024-12-01-21:50:05-root-INFO: Loss too large (2183.723->2198.785)! Learning rate decreased to 0.00092.
2024-12-01-21:50:06-root-INFO: Loss too large (2183.723->2185.370)! Learning rate decreased to 0.00073.
2024-12-01-21:50:07-root-INFO: grad norm: 197.060 191.421 46.802
2024-12-01-21:50:08-root-INFO: grad norm: 195.050 190.786 40.559
2024-12-01-21:50:08-root-INFO: grad norm: 194.480 189.011 45.797
2024-12-01-21:50:09-root-INFO: grad norm: 195.232 191.006 40.399
2024-12-01-21:50:10-root-INFO: grad norm: 197.362 191.908 46.078
2024-12-01-21:50:11-root-INFO: Loss Change: 2201.219 -> 2159.749
2024-12-01-21:50:11-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-01-21:50:11-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-01-21:50:11-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-21:50:11-root-INFO: step: 198 lr_xt 0.00149757
2024-12-01-21:50:12-root-INFO: grad norm: 254.983 246.387 65.651
2024-12-01-21:50:12-root-INFO: Loss too large (2155.044->2172.706)! Learning rate decreased to 0.00120.
2024-12-01-21:50:13-root-INFO: grad norm: 314.806 308.474 62.821
2024-12-01-21:50:13-root-INFO: Loss too large (2153.806->2194.190)! Learning rate decreased to 0.00096.
2024-12-01-21:50:13-root-INFO: Loss too large (2153.806->2162.449)! Learning rate decreased to 0.00077.
2024-12-01-21:50:14-root-INFO: grad norm: 313.599 307.095 63.536
2024-12-01-21:50:15-root-INFO: grad norm: 348.209 339.918 75.534
2024-12-01-21:50:15-root-INFO: Loss too large (2142.365->2146.368)! Learning rate decreased to 0.00061.
2024-12-01-21:50:16-root-INFO: grad norm: 254.815 249.580 51.389
2024-12-01-21:50:17-root-INFO: grad norm: 181.213 176.169 42.459
2024-12-01-21:50:18-root-INFO: grad norm: 140.017 136.485 31.250
2024-12-01-21:50:19-root-INFO: grad norm: 110.671 106.712 29.336
2024-12-01-21:50:20-root-INFO: Loss Change: 2155.044 -> 2108.638
2024-12-01-21:50:20-root-INFO: Regularization Change: 0.000 -> 0.197
2024-12-01-21:50:20-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-01-21:50:20-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:50:20-root-INFO: step: 197 lr_xt 0.00156486
2024-12-01-21:50:20-root-INFO: grad norm: 218.705 211.424 55.961
2024-12-01-21:50:21-root-INFO: Loss too large (2098.092->2113.010)! Learning rate decreased to 0.00125.
2024-12-01-21:50:21-root-INFO: Loss too large (2098.092->2100.914)! Learning rate decreased to 0.00100.
2024-12-01-21:50:22-root-INFO: grad norm: 182.094 178.152 37.683
2024-12-01-21:50:23-root-INFO: grad norm: 198.452 193.167 45.493
2024-12-01-21:50:23-root-INFO: Loss too large (2084.776->2092.557)! Learning rate decreased to 0.00080.
2024-12-01-21:50:24-root-INFO: grad norm: 225.994 221.493 44.875
2024-12-01-21:50:25-root-INFO: grad norm: 273.835 267.591 58.143
2024-12-01-21:50:25-root-INFO: Loss too large (2083.531->2087.772)! Learning rate decreased to 0.00064.
2024-12-01-21:50:26-root-INFO: grad norm: 220.782 216.590 42.818
2024-12-01-21:50:27-root-INFO: grad norm: 174.838 170.403 39.127
2024-12-01-21:50:28-root-INFO: grad norm: 144.875 141.699 30.169
2024-12-01-21:50:28-root-INFO: Loss Change: 2098.092 -> 2062.719
2024-12-01-21:50:28-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-01-21:50:28-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-01-21:50:28-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:50:28-root-INFO: step: 196 lr_xt 0.00163492
2024-12-01-21:50:29-root-INFO: grad norm: 335.199 322.269 92.199
2024-12-01-21:50:29-root-INFO: Loss too large (2067.921->2166.608)! Learning rate decreased to 0.00131.
2024-12-01-21:50:29-root-INFO: Loss too large (2067.921->2117.448)! Learning rate decreased to 0.00105.
2024-12-01-21:50:29-root-INFO: Loss too large (2067.921->2084.940)! Learning rate decreased to 0.00084.
2024-12-01-21:50:30-root-INFO: grad norm: 312.893 307.480 57.947
2024-12-01-21:50:31-root-INFO: grad norm: 358.948 350.259 78.498
2024-12-01-21:50:31-root-INFO: Loss too large (2058.279->2072.528)! Learning rate decreased to 0.00067.
2024-12-01-21:50:32-root-INFO: grad norm: 308.048 302.502 58.192
2024-12-01-21:50:33-root-INFO: grad norm: 258.568 252.514 55.622
2024-12-01-21:50:34-root-INFO: grad norm: 225.868 221.570 43.853
2024-12-01-21:50:35-root-INFO: grad norm: 194.809 190.106 42.545
2024-12-01-21:50:36-root-INFO: grad norm: 172.823 169.305 34.695
2024-12-01-21:50:36-root-INFO: Loss Change: 2067.921 -> 2023.436
2024-12-01-21:50:37-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-01-21:50:37-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-01-21:50:37-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:50:37-root-INFO: step: 195 lr_xt 0.00170783
2024-12-01-21:50:37-root-INFO: grad norm: 248.902 242.170 57.498
2024-12-01-21:50:38-root-INFO: Loss too large (2018.736->2117.515)! Learning rate decreased to 0.00137.
2024-12-01-21:50:38-root-INFO: Loss too large (2018.736->2070.379)! Learning rate decreased to 0.00109.
2024-12-01-21:50:38-root-INFO: Loss too large (2018.736->2040.859)! Learning rate decreased to 0.00087.
2024-12-01-21:50:38-root-INFO: Loss too large (2018.736->2023.325)! Learning rate decreased to 0.00070.
2024-12-01-21:50:39-root-INFO: grad norm: 215.232 210.765 43.619
2024-12-01-21:50:40-root-INFO: grad norm: 189.510 184.897 41.559
2024-12-01-21:50:41-root-INFO: grad norm: 174.587 170.911 35.637
2024-12-01-21:50:42-root-INFO: grad norm: 161.117 157.262 35.034
2024-12-01-21:50:43-root-INFO: grad norm: 150.949 147.679 31.249
2024-12-01-21:50:44-root-INFO: grad norm: 141.399 137.916 31.190
2024-12-01-21:50:45-root-INFO: grad norm: 133.844 130.818 28.302
2024-12-01-21:50:45-root-INFO: Loss Change: 2018.736 -> 1988.913
2024-12-01-21:50:45-root-INFO: Regularization Change: 0.000 -> 0.123
2024-12-01-21:50:45-root-INFO: Undo step: 195
2024-12-01-21:50:45-root-INFO: Undo step: 196
2024-12-01-21:50:45-root-INFO: Undo step: 197
2024-12-01-21:50:45-root-INFO: Undo step: 198
2024-12-01-21:50:45-root-INFO: Undo step: 199
2024-12-01-21:50:46-root-INFO: step: 200 lr_xt 0.00137086
2024-12-01-21:50:46-root-INFO: grad norm: 1911.365 1800.827 640.574
2024-12-01-21:50:47-root-INFO: grad norm: 987.210 949.008 271.971
2024-12-01-21:50:48-root-INFO: grad norm: 445.522 411.319 171.192
2024-12-01-21:50:48-root-INFO: grad norm: 299.117 279.997 105.225
2024-12-01-21:50:49-root-INFO: grad norm: 231.765 219.125 75.493
2024-12-01-21:50:50-root-INFO: grad norm: 194.987 184.682 62.552
2024-12-01-21:50:51-root-INFO: grad norm: 172.839 163.456 56.172
2024-12-01-21:50:52-root-INFO: grad norm: 157.898 148.512 53.628
2024-12-01-21:50:53-root-INFO: Loss Change: 4282.549 -> 2427.962
2024-12-01-21:50:53-root-INFO: Regularization Change: 0.000 -> 10.912
2024-12-01-21:50:53-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-21:50:53-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-21:50:53-root-INFO: step: 199 lr_xt 0.00143293
2024-12-01-21:50:53-root-INFO: grad norm: 163.642 151.386 62.137
2024-12-01-21:50:54-root-INFO: grad norm: 146.771 137.363 51.704
2024-12-01-21:50:55-root-INFO: grad norm: 137.200 127.689 50.193
2024-12-01-21:50:56-root-INFO: grad norm: 131.159 122.580 46.659
2024-12-01-21:50:57-root-INFO: grad norm: 128.970 120.541 45.860
2024-12-01-21:50:57-root-INFO: grad norm: 130.777 123.125 44.079
2024-12-01-21:50:58-root-INFO: grad norm: 139.987 132.228 45.956
2024-12-01-21:50:59-root-INFO: grad norm: 158.337 151.299 46.683
2024-12-01-21:51:00-root-INFO: Loss Change: 2406.839 -> 2250.686
2024-12-01-21:51:00-root-INFO: Regularization Change: 0.000 -> 1.819
2024-12-01-21:51:00-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-01-21:51:00-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-21:51:00-root-INFO: step: 198 lr_xt 0.00149757
2024-12-01-21:51:00-root-INFO: grad norm: 428.387 411.007 120.782
2024-12-01-21:51:01-root-INFO: Loss too large (2261.763->2312.672)! Learning rate decreased to 0.00120.
2024-12-01-21:51:01-root-INFO: Loss too large (2261.763->2269.402)! Learning rate decreased to 0.00096.
2024-12-01-21:51:02-root-INFO: grad norm: 281.047 274.305 61.191
2024-12-01-21:51:03-root-INFO: grad norm: 151.909 144.333 47.374
2024-12-01-21:51:04-root-INFO: grad norm: 123.436 117.159 38.861
2024-12-01-21:51:05-root-INFO: grad norm: 107.466 100.629 37.719
2024-12-01-21:51:05-root-INFO: grad norm: 99.273 93.263 34.018
2024-12-01-21:51:06-root-INFO: grad norm: 94.064 87.615 34.229
2024-12-01-21:51:07-root-INFO: grad norm: 90.752 84.950 31.927
2024-12-01-21:51:08-root-INFO: Loss Change: 2261.763 -> 2157.483
2024-12-01-21:51:08-root-INFO: Regularization Change: 0.000 -> 0.527
2024-12-01-21:51:08-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-01-21:51:08-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:51:08-root-INFO: step: 197 lr_xt 0.00156486
2024-12-01-21:51:08-root-INFO: grad norm: 271.784 261.797 72.997
2024-12-01-21:51:09-root-INFO: Loss too large (2149.377->2185.226)! Learning rate decreased to 0.00125.
2024-12-01-21:51:09-root-INFO: Loss too large (2149.377->2162.598)! Learning rate decreased to 0.00100.
2024-12-01-21:51:10-root-INFO: grad norm: 247.296 241.879 51.478
2024-12-01-21:51:11-root-INFO: grad norm: 209.256 202.160 54.033
2024-12-01-21:51:11-root-INFO: grad norm: 208.825 203.399 47.294
2024-12-01-21:51:12-root-INFO: grad norm: 213.651 206.807 53.642
2024-12-01-21:51:13-root-INFO: grad norm: 218.909 213.458 48.549
2024-12-01-21:51:14-root-INFO: grad norm: 231.556 224.582 56.398
2024-12-01-21:51:14-root-INFO: Loss too large (2105.592->2105.641)! Learning rate decreased to 0.00080.
2024-12-01-21:51:15-root-INFO: grad norm: 177.706 172.985 40.689
2024-12-01-21:51:16-root-INFO: Loss Change: 2149.377 -> 2088.795
2024-12-01-21:51:16-root-INFO: Regularization Change: 0.000 -> 0.376
2024-12-01-21:51:16-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-01-21:51:16-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:51:16-root-INFO: step: 196 lr_xt 0.00163492
2024-12-01-21:51:16-root-INFO: grad norm: 438.685 425.126 108.223
2024-12-01-21:51:17-root-INFO: Loss too large (2103.654->2204.178)! Learning rate decreased to 0.00131.
2024-12-01-21:51:17-root-INFO: Loss too large (2103.654->2159.648)! Learning rate decreased to 0.00105.
2024-12-01-21:51:17-root-INFO: Loss too large (2103.654->2125.153)! Learning rate decreased to 0.00084.
2024-12-01-21:51:18-root-INFO: grad norm: 297.690 292.440 55.657
2024-12-01-21:51:19-root-INFO: grad norm: 138.303 131.659 42.353
2024-12-01-21:51:20-root-INFO: grad norm: 118.690 113.916 33.324
2024-12-01-21:51:21-root-INFO: grad norm: 105.554 100.020 33.728
2024-12-01-21:51:22-root-INFO: grad norm: 97.678 93.414 28.546
2024-12-01-21:51:23-root-INFO: grad norm: 92.361 87.305 30.140
2024-12-01-21:51:23-root-INFO: grad norm: 89.035 85.042 26.365
2024-12-01-21:51:24-root-INFO: Loss Change: 2103.654 -> 2031.762
2024-12-01-21:51:24-root-INFO: Regularization Change: 0.000 -> 0.286
2024-12-01-21:51:24-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-01-21:51:24-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:51:24-root-INFO: step: 195 lr_xt 0.00170783
2024-12-01-21:51:25-root-INFO: grad norm: 192.878 186.331 49.827
2024-12-01-21:51:25-root-INFO: Loss too large (2024.864->2060.975)! Learning rate decreased to 0.00137.
2024-12-01-21:51:25-root-INFO: Loss too large (2024.864->2041.784)! Learning rate decreased to 0.00109.
2024-12-01-21:51:25-root-INFO: Loss too large (2024.864->2029.623)! Learning rate decreased to 0.00087.
2024-12-01-21:51:26-root-INFO: grad norm: 194.853 190.420 41.324
2024-12-01-21:51:27-root-INFO: grad norm: 202.078 196.495 47.171
2024-12-01-21:51:28-root-INFO: grad norm: 206.062 201.686 42.240
2024-12-01-21:51:29-root-INFO: grad norm: 214.842 209.370 48.177
2024-12-01-21:51:29-root-INFO: Loss too large (2008.930->2009.012)! Learning rate decreased to 0.00070.
2024-12-01-21:51:30-root-INFO: grad norm: 161.414 157.710 34.383
2024-12-01-21:51:31-root-INFO: grad norm: 114.101 109.947 30.508
2024-12-01-21:51:32-root-INFO: grad norm: 96.345 93.052 24.973
2024-12-01-21:51:33-root-INFO: Loss Change: 2024.864 -> 1989.317
2024-12-01-21:51:33-root-INFO: Regularization Change: 0.000 -> 0.190
2024-12-01-21:51:33-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-01-21:51:33-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:51:33-root-INFO: step: 194 lr_xt 0.00178371
2024-12-01-21:51:33-root-INFO: grad norm: 316.425 306.644 78.066
2024-12-01-21:51:33-root-INFO: Loss too large (1993.386->2092.805)! Learning rate decreased to 0.00143.
2024-12-01-21:51:34-root-INFO: Loss too large (1993.386->2054.013)! Learning rate decreased to 0.00114.
2024-12-01-21:51:34-root-INFO: Loss too large (1993.386->2024.329)! Learning rate decreased to 0.00091.
2024-12-01-21:51:34-root-INFO: Loss too large (1993.386->2003.136)! Learning rate decreased to 0.00073.
2024-12-01-21:51:35-root-INFO: grad norm: 252.368 248.345 44.885
2024-12-01-21:51:36-root-INFO: grad norm: 169.023 163.230 43.873
2024-12-01-21:51:37-root-INFO: grad norm: 149.355 146.056 31.217
2024-12-01-21:51:38-root-INFO: grad norm: 129.372 124.867 33.841
2024-12-01-21:51:39-root-INFO: grad norm: 119.214 116.207 26.608
2024-12-01-21:51:40-root-INFO: grad norm: 109.282 105.279 29.307
2024-12-01-21:51:41-root-INFO: grad norm: 103.558 100.691 24.197
2024-12-01-21:51:41-root-INFO: Loss Change: 1993.386 -> 1950.346
2024-12-01-21:51:41-root-INFO: Regularization Change: 0.000 -> 0.171
2024-12-01-21:51:41-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-01-21:51:41-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:51:42-root-INFO: step: 193 lr_xt 0.00186266
2024-12-01-21:51:42-root-INFO: grad norm: 466.513 455.765 99.560
2024-12-01-21:51:42-root-INFO: Loss too large (1967.078->2125.420)! Learning rate decreased to 0.00149.
2024-12-01-21:51:42-root-INFO: Loss too large (1967.078->2077.940)! Learning rate decreased to 0.00119.
2024-12-01-21:51:43-root-INFO: Loss too large (1967.078->2036.795)! Learning rate decreased to 0.00095.
2024-12-01-21:51:43-root-INFO: Loss too large (1967.078->2001.724)! Learning rate decreased to 0.00076.
2024-12-01-21:51:43-root-INFO: Loss too large (1967.078->1973.960)! Learning rate decreased to 0.00061.
2024-12-01-21:51:44-root-INFO: grad norm: 280.369 276.233 47.981
2024-12-01-21:51:45-root-INFO: grad norm: 97.686 92.023 32.778
2024-12-01-21:51:46-root-INFO: grad norm: 81.603 77.740 24.811
2024-12-01-21:51:47-root-INFO: grad norm: 72.659 68.054 25.456
2024-12-01-21:51:48-root-INFO: grad norm: 68.150 64.403 22.286
2024-12-01-21:51:48-root-INFO: grad norm: 65.435 61.413 22.588
2024-12-01-21:51:49-root-INFO: grad norm: 63.841 60.253 21.103
2024-12-01-21:51:50-root-INFO: Loss Change: 1967.078 -> 1912.481
2024-12-01-21:51:50-root-INFO: Regularization Change: 0.000 -> 0.134
2024-12-01-21:51:50-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-01-21:51:50-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-21:51:50-root-INFO: step: 192 lr_xt 0.00194479
2024-12-01-21:51:50-root-INFO: grad norm: 214.543 208.373 51.082
2024-12-01-21:51:51-root-INFO: Loss too large (1901.706->1990.096)! Learning rate decreased to 0.00156.
2024-12-01-21:51:51-root-INFO: Loss too large (1901.706->1953.035)! Learning rate decreased to 0.00124.
2024-12-01-21:51:51-root-INFO: Loss too large (1901.706->1927.957)! Learning rate decreased to 0.00100.
2024-12-01-21:51:52-root-INFO: Loss too large (1901.706->1911.854)! Learning rate decreased to 0.00080.
2024-12-01-21:51:52-root-INFO: Loss too large (1901.706->1902.137)! Learning rate decreased to 0.00064.
2024-12-01-21:51:53-root-INFO: grad norm: 170.996 167.877 32.512
2024-12-01-21:51:54-root-INFO: grad norm: 142.402 138.470 33.231
2024-12-01-21:51:55-root-INFO: grad norm: 129.566 127.007 25.619
2024-12-01-21:51:55-root-INFO: grad norm: 117.242 113.922 27.704
2024-12-01-21:51:56-root-INFO: grad norm: 110.296 107.888 22.922
2024-12-01-21:51:57-root-INFO: grad norm: 103.219 100.117 25.115
2024-12-01-21:51:58-root-INFO: grad norm: 98.831 96.468 21.483
2024-12-01-21:51:59-root-INFO: Loss Change: 1901.706 -> 1875.576
2024-12-01-21:51:59-root-INFO: Regularization Change: 0.000 -> 0.104
2024-12-01-21:51:59-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-01-21:51:59-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-21:51:59-root-INFO: step: 191 lr_xt 0.00203021
2024-12-01-21:51:59-root-INFO: grad norm: 549.864 538.733 110.077
2024-12-01-21:51:59-root-INFO: Loss too large (1911.576->2100.987)! Learning rate decreased to 0.00162.
2024-12-01-21:52:00-root-INFO: Loss too large (1911.576->2056.833)! Learning rate decreased to 0.00130.
2024-12-01-21:52:00-root-INFO: Loss too large (1911.576->2016.901)! Learning rate decreased to 0.00104.
2024-12-01-21:52:00-root-INFO: Loss too large (1911.576->1979.521)! Learning rate decreased to 0.00083.
2024-12-01-21:52:01-root-INFO: Loss too large (1911.576->1944.989)! Learning rate decreased to 0.00067.
2024-12-01-21:52:01-root-INFO: Loss too large (1911.576->1915.678)! Learning rate decreased to 0.00053.
2024-12-01-21:52:02-root-INFO: grad norm: 314.063 309.494 53.376
2024-12-01-21:52:03-root-INFO: grad norm: 97.268 89.948 37.019
2024-12-01-21:52:03-root-INFO: grad norm: 84.013 78.514 29.893
2024-12-01-21:52:04-root-INFO: grad norm: 75.549 69.840 28.811
2024-12-01-21:52:05-root-INFO: grad norm: 70.370 65.645 25.349
2024-12-01-21:52:06-root-INFO: grad norm: 67.029 62.378 24.534
2024-12-01-21:52:07-root-INFO: grad norm: 64.682 60.585 22.654
2024-12-01-21:52:08-root-INFO: Loss Change: 1911.576 -> 1847.791
2024-12-01-21:52:08-root-INFO: Regularization Change: 0.000 -> 0.129
2024-12-01-21:52:08-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-01-21:52:08-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-21:52:08-root-INFO: step: 190 lr_xt 0.00211904
2024-12-01-21:52:08-root-INFO: grad norm: 366.490 358.935 74.030
2024-12-01-21:52:08-root-INFO: Loss too large (1854.300->2028.023)! Learning rate decreased to 0.00170.
2024-12-01-21:52:09-root-INFO: Loss too large (1854.300->1983.178)! Learning rate decreased to 0.00136.
2024-12-01-21:52:09-root-INFO: Loss too large (1854.300->1943.443)! Learning rate decreased to 0.00108.
2024-12-01-21:52:09-root-INFO: Loss too large (1854.300->1909.034)! Learning rate decreased to 0.00087.
2024-12-01-21:52:09-root-INFO: Loss too large (1854.300->1881.305)! Learning rate decreased to 0.00069.
2024-12-01-21:52:10-root-INFO: Loss too large (1854.300->1860.994)! Learning rate decreased to 0.00056.
2024-12-01-21:52:11-root-INFO: grad norm: 261.959 258.539 42.191
2024-12-01-21:52:11-root-INFO: grad norm: 152.965 148.519 36.608
2024-12-01-21:52:12-root-INFO: grad norm: 128.197 125.619 25.584
2024-12-01-21:52:13-root-INFO: grad norm: 104.930 101.160 27.873
2024-12-01-21:52:14-root-INFO: grad norm: 92.295 89.779 21.404
2024-12-01-21:52:15-root-INFO: grad norm: 81.510 78.055 23.480
2024-12-01-21:52:16-root-INFO: grad norm: 74.829 72.260 19.439
2024-12-01-21:52:16-root-INFO: Loss Change: 1854.300 -> 1817.665
2024-12-01-21:52:16-root-INFO: Regularization Change: 0.000 -> 0.094
2024-12-01-21:52:16-root-INFO: Undo step: 190
2024-12-01-21:52:16-root-INFO: Undo step: 191
2024-12-01-21:52:16-root-INFO: Undo step: 192
2024-12-01-21:52:16-root-INFO: Undo step: 193
2024-12-01-21:52:16-root-INFO: Undo step: 194
2024-12-01-21:52:17-root-INFO: step: 195 lr_xt 0.00170783
2024-12-01-21:52:17-root-INFO: grad norm: 1096.814 1040.426 347.155
2024-12-01-21:52:18-root-INFO: grad norm: 914.419 893.689 193.602
2024-12-01-21:52:19-root-INFO: grad norm: 1095.762 1076.697 203.516
2024-12-01-21:52:19-root-INFO: Loss too large (2688.061->2794.197)! Learning rate decreased to 0.00137.
2024-12-01-21:52:20-root-INFO: grad norm: 746.861 736.137 126.107
2024-12-01-21:52:21-root-INFO: grad norm: 406.141 396.754 86.813
2024-12-01-21:52:22-root-INFO: grad norm: 424.558 419.095 67.889
2024-12-01-21:52:23-root-INFO: grad norm: 330.554 322.007 74.681
2024-12-01-21:52:24-root-INFO: grad norm: 214.233 207.795 52.124
2024-12-01-21:52:25-root-INFO: Loss Change: 3343.084 -> 2115.366
2024-12-01-21:52:25-root-INFO: Regularization Change: 0.000 -> 7.110
2024-12-01-21:52:25-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-01-21:52:25-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:52:25-root-INFO: step: 194 lr_xt 0.00178371
2024-12-01-21:52:25-root-INFO: grad norm: 279.636 266.945 83.287
2024-12-01-21:52:25-root-INFO: Loss too large (2106.542->2131.622)! Learning rate decreased to 0.00143.
2024-12-01-21:52:26-root-INFO: grad norm: 325.191 319.280 61.723
2024-12-01-21:52:27-root-INFO: Loss too large (2103.233->2118.356)! Learning rate decreased to 0.00114.
2024-12-01-21:52:28-root-INFO: grad norm: 369.848 362.344 74.123
2024-12-01-21:52:28-root-INFO: Loss too large (2083.194->2100.340)! Learning rate decreased to 0.00091.
2024-12-01-21:52:29-root-INFO: grad norm: 267.819 262.515 53.039
2024-12-01-21:52:30-root-INFO: grad norm: 138.125 132.826 37.891
2024-12-01-21:52:31-root-INFO: grad norm: 145.541 140.606 37.578
2024-12-01-21:52:31-root-INFO: grad norm: 168.681 164.082 39.119
2024-12-01-21:52:32-root-INFO: grad norm: 184.625 180.298 39.739
2024-12-01-21:52:33-root-INFO: Loss Change: 2106.542 -> 2022.628
2024-12-01-21:52:33-root-INFO: Regularization Change: 0.000 -> 0.577
2024-12-01-21:52:33-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-01-21:52:33-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-21:52:33-root-INFO: step: 193 lr_xt 0.00186266
2024-12-01-21:52:34-root-INFO: grad norm: 601.709 591.269 111.602
2024-12-01-21:52:34-root-INFO: Loss too large (2047.952->2162.958)! Learning rate decreased to 0.00149.
2024-12-01-21:52:34-root-INFO: Loss too large (2047.952->2130.542)! Learning rate decreased to 0.00119.
2024-12-01-21:52:34-root-INFO: Loss too large (2047.952->2100.408)! Learning rate decreased to 0.00095.
2024-12-01-21:52:35-root-INFO: Loss too large (2047.952->2071.978)! Learning rate decreased to 0.00076.
2024-12-01-21:52:35-root-INFO: grad norm: 283.049 278.617 49.890
2024-12-01-21:52:36-root-INFO: grad norm: 174.006 169.323 40.099
2024-12-01-21:52:37-root-INFO: grad norm: 151.968 147.178 37.854
2024-12-01-21:52:38-root-INFO: grad norm: 147.658 143.644 34.195
2024-12-01-21:52:39-root-INFO: grad norm: 146.193 141.797 35.582
2024-12-01-21:52:40-root-INFO: grad norm: 147.818 144.187 32.559
2024-12-01-21:52:41-root-INFO: grad norm: 153.599 149.461 35.412
2024-12-01-21:52:41-root-INFO: Loss Change: 2047.952 -> 1961.194
2024-12-01-21:52:41-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-01-21:52:41-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-01-21:52:41-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-21:52:42-root-INFO: step: 192 lr_xt 0.00194479
2024-12-01-21:52:42-root-INFO: grad norm: 125.082 119.720 36.228
2024-12-01-21:52:42-root-INFO: Loss too large (1939.454->1951.966)! Learning rate decreased to 0.00156.
2024-12-01-21:52:43-root-INFO: Loss too large (1939.454->1942.198)! Learning rate decreased to 0.00124.
2024-12-01-21:52:43-root-INFO: grad norm: 229.414 225.650 41.385
2024-12-01-21:52:44-root-INFO: Loss too large (1937.063->1963.560)! Learning rate decreased to 0.00100.
2024-12-01-21:52:44-root-INFO: Loss too large (1937.063->1949.061)! Learning rate decreased to 0.00080.
2024-12-01-21:52:44-root-INFO: Loss too large (1937.063->1939.167)! Learning rate decreased to 0.00064.
2024-12-01-21:52:45-root-INFO: grad norm: 192.949 189.542 36.096
2024-12-01-21:52:46-root-INFO: grad norm: 160.943 157.206 34.480
2024-12-01-21:52:47-root-INFO: grad norm: 151.622 148.391 31.133
2024-12-01-21:52:48-root-INFO: grad norm: 141.365 137.795 31.571
2024-12-01-21:52:49-root-INFO: grad norm: 137.352 134.224 29.147
2024-12-01-21:52:50-root-INFO: grad norm: 133.721 130.253 30.254
2024-12-01-21:52:50-root-INFO: Loss Change: 1939.454 -> 1909.822
2024-12-01-21:52:50-root-INFO: Regularization Change: 0.000 -> 0.170
2024-12-01-21:52:50-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-01-21:52:50-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-21:52:50-root-INFO: step: 191 lr_xt 0.00203021
2024-12-01-21:52:51-root-INFO: grad norm: 298.884 289.437 74.550
2024-12-01-21:52:51-root-INFO: Loss too large (1911.733->2005.584)! Learning rate decreased to 0.00162.
2024-12-01-21:52:51-root-INFO: Loss too large (1911.733->1979.451)! Learning rate decreased to 0.00130.
2024-12-01-21:52:52-root-INFO: Loss too large (1911.733->1956.018)! Learning rate decreased to 0.00104.
2024-12-01-21:52:52-root-INFO: Loss too large (1911.733->1936.048)! Learning rate decreased to 0.00083.
2024-12-01-21:52:52-root-INFO: Loss too large (1911.733->1920.536)! Learning rate decreased to 0.00067.
2024-12-01-21:52:53-root-INFO: grad norm: 273.010 268.748 48.053
2024-12-01-21:52:54-root-INFO: grad norm: 243.189 237.360 52.923
2024-12-01-21:52:54-root-INFO: Loss too large (1894.818->1895.366)! Learning rate decreased to 0.00053.
2024-12-01-21:52:55-root-INFO: grad norm: 186.760 183.150 36.546
2024-12-01-21:52:56-root-INFO: grad norm: 128.980 123.995 35.512
2024-12-01-21:52:57-root-INFO: grad norm: 109.960 106.286 28.185
2024-12-01-21:52:57-root-INFO: grad norm: 93.661 89.017 29.127
2024-12-01-21:52:58-root-INFO: grad norm: 84.966 81.198 25.021
2024-12-01-21:52:59-root-INFO: Loss Change: 1911.733 -> 1871.200
2024-12-01-21:52:59-root-INFO: Regularization Change: 0.000 -> 0.136
2024-12-01-21:52:59-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-01-21:52:59-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-21:52:59-root-INFO: step: 190 lr_xt 0.00211904
2024-12-01-21:52:59-root-INFO: grad norm: 408.252 401.013 76.539
2024-12-01-21:53:00-root-INFO: Loss too large (1877.530->2023.734)! Learning rate decreased to 0.00170.
2024-12-01-21:53:00-root-INFO: Loss too large (1877.530->1993.667)! Learning rate decreased to 0.00136.
2024-12-01-21:53:00-root-INFO: Loss too large (1877.530->1965.180)! Learning rate decreased to 0.00108.
2024-12-01-21:53:01-root-INFO: Loss too large (1877.530->1937.802)! Learning rate decreased to 0.00087.
2024-12-01-21:53:01-root-INFO: Loss too large (1877.530->1912.377)! Learning rate decreased to 0.00069.
2024-12-01-21:53:01-root-INFO: Loss too large (1877.530->1890.924)! Learning rate decreased to 0.00056.
2024-12-01-21:53:02-root-INFO: grad norm: 297.357 293.746 46.197
2024-12-01-21:53:03-root-INFO: grad norm: 155.372 150.731 37.694
2024-12-01-21:53:04-root-INFO: grad norm: 143.930 140.998 28.907
2024-12-01-21:53:05-root-INFO: grad norm: 131.757 127.708 32.413
2024-12-01-21:53:05-root-INFO: grad norm: 125.335 122.597 26.055
2024-12-01-21:53:06-root-INFO: grad norm: 118.764 115.075 29.368
2024-12-01-21:53:07-root-INFO: grad norm: 115.048 112.444 24.341
2024-12-01-21:53:08-root-INFO: Loss Change: 1877.530 -> 1837.096
2024-12-01-21:53:08-root-INFO: Regularization Change: 0.000 -> 0.102
2024-12-01-21:53:08-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-01-21:53:08-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-21:53:08-root-INFO: step: 189 lr_xt 0.00221139
2024-12-01-21:53:08-root-INFO: grad norm: 420.840 414.270 74.073
2024-12-01-21:53:09-root-INFO: Loss too large (1845.232->2003.876)! Learning rate decreased to 0.00177.
2024-12-01-21:53:09-root-INFO: Loss too large (1845.232->1975.954)! Learning rate decreased to 0.00142.
2024-12-01-21:53:09-root-INFO: Loss too large (1845.232->1948.418)! Learning rate decreased to 0.00113.
2024-12-01-21:53:09-root-INFO: Loss too large (1845.232->1920.955)! Learning rate decreased to 0.00091.
2024-12-01-21:53:10-root-INFO: Loss too large (1845.232->1894.096)! Learning rate decreased to 0.00072.
2024-12-01-21:53:10-root-INFO: Loss too large (1845.232->1869.912)! Learning rate decreased to 0.00058.
2024-12-01-21:53:10-root-INFO: Loss too large (1845.232->1850.644)! Learning rate decreased to 0.00046.
2024-12-01-21:53:11-root-INFO: grad norm: 276.243 272.968 42.409
2024-12-01-21:53:12-root-INFO: grad norm: 130.342 125.846 33.937
2024-12-01-21:53:13-root-INFO: grad norm: 107.289 104.200 25.558
2024-12-01-21:53:14-root-INFO: grad norm: 88.844 84.580 27.194
2024-12-01-21:53:15-root-INFO: grad norm: 79.112 75.730 22.883
2024-12-01-21:53:15-root-INFO: grad norm: 72.142 68.063 23.912
2024-12-01-21:53:16-root-INFO: grad norm: 68.078 64.574 21.559
2024-12-01-21:53:17-root-INFO: Loss Change: 1845.232 -> 1808.452
2024-12-01-21:53:17-root-INFO: Regularization Change: 0.000 -> 0.072
2024-12-01-21:53:17-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-21:53:17-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-21:53:17-root-INFO: step: 188 lr_xt 0.00230740
2024-12-01-21:53:17-root-INFO: grad norm: 307.402 301.738 58.737
2024-12-01-21:53:18-root-INFO: Loss too large (1807.471->1949.161)! Learning rate decreased to 0.00185.
2024-12-01-21:53:18-root-INFO: Loss too large (1807.471->1919.469)! Learning rate decreased to 0.00148.
2024-12-01-21:53:18-root-INFO: Loss too large (1807.471->1891.606)! Learning rate decreased to 0.00118.
2024-12-01-21:53:19-root-INFO: Loss too large (1807.471->1865.568)! Learning rate decreased to 0.00095.
2024-12-01-21:53:19-root-INFO: Loss too large (1807.471->1842.706)! Learning rate decreased to 0.00076.
2024-12-01-21:53:19-root-INFO: Loss too large (1807.471->1824.497)! Learning rate decreased to 0.00060.
2024-12-01-21:53:20-root-INFO: Loss too large (1807.471->1811.437)! Learning rate decreased to 0.00048.
2024-12-01-21:53:20-root-INFO: grad norm: 246.829 243.577 39.937
2024-12-01-21:53:21-root-INFO: grad norm: 180.999 176.972 37.967
2024-12-01-21:53:22-root-INFO: grad norm: 161.749 159.007 29.658
2024-12-01-21:53:23-root-INFO: grad norm: 140.977 137.467 31.261
2024-12-01-21:53:24-root-INFO: grad norm: 130.131 127.544 25.822
2024-12-01-21:53:25-root-INFO: grad norm: 118.903 115.654 27.603
2024-12-01-21:53:26-root-INFO: grad norm: 112.075 109.563 23.596
2024-12-01-21:53:26-root-INFO: Loss Change: 1807.471 -> 1778.915
2024-12-01-21:53:26-root-INFO: Regularization Change: 0.000 -> 0.075
2024-12-01-21:53:26-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-21:53:26-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-21:53:26-root-INFO: step: 187 lr_xt 0.00240719
2024-12-01-21:53:27-root-INFO: grad norm: 472.459 465.403 81.346
2024-12-01-21:53:27-root-INFO: Loss too large (1805.030->1986.447)! Learning rate decreased to 0.00193.
2024-12-01-21:53:27-root-INFO: Loss too large (1805.030->1961.877)! Learning rate decreased to 0.00154.
2024-12-01-21:53:28-root-INFO: Loss too large (1805.030->1936.255)! Learning rate decreased to 0.00123.
2024-12-01-21:53:28-root-INFO: Loss too large (1805.030->1909.496)! Learning rate decreased to 0.00099.
2024-12-01-21:53:28-root-INFO: Loss too large (1805.030->1881.523)! Learning rate decreased to 0.00079.
2024-12-01-21:53:28-root-INFO: Loss too large (1805.030->1853.404)! Learning rate decreased to 0.00063.
2024-12-01-21:53:29-root-INFO: Loss too large (1805.030->1827.871)! Learning rate decreased to 0.00050.
2024-12-01-21:53:29-root-INFO: Loss too large (1805.030->1807.553)! Learning rate decreased to 0.00040.
2024-12-01-21:53:30-root-INFO: grad norm: 292.893 289.447 44.795
2024-12-01-21:53:31-root-INFO: grad norm: 130.651 125.073 37.767
2024-12-01-21:53:32-root-INFO: grad norm: 104.059 100.098 28.438
2024-12-01-21:53:33-root-INFO: grad norm: 85.493 80.086 29.923
2024-12-01-21:53:33-root-INFO: grad norm: 76.191 71.765 25.591
2024-12-01-21:53:34-root-INFO: grad norm: 70.418 65.426 26.040
2024-12-01-21:53:35-root-INFO: grad norm: 67.119 62.756 23.802
2024-12-01-21:53:36-root-INFO: Loss Change: 1805.030 -> 1764.916
2024-12-01-21:53:36-root-INFO: Regularization Change: 0.000 -> 0.066
2024-12-01-21:53:36-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-21:53:36-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:53:36-root-INFO: step: 186 lr_xt 0.00251089
2024-12-01-21:53:36-root-INFO: grad norm: 303.844 298.524 56.606
2024-12-01-21:53:37-root-INFO: Loss too large (1762.008->1919.181)! Learning rate decreased to 0.00201.
2024-12-01-21:53:37-root-INFO: Loss too large (1762.008->1892.006)! Learning rate decreased to 0.00161.
2024-12-01-21:53:37-root-INFO: Loss too large (1762.008->1865.052)! Learning rate decreased to 0.00129.
2024-12-01-21:53:38-root-INFO: Loss too large (1762.008->1838.307)! Learning rate decreased to 0.00103.
2024-12-01-21:53:38-root-INFO: Loss too large (1762.008->1813.183)! Learning rate decreased to 0.00082.
2024-12-01-21:53:38-root-INFO: Loss too large (1762.008->1791.761)! Learning rate decreased to 0.00066.
2024-12-01-21:53:38-root-INFO: Loss too large (1762.008->1775.337)! Learning rate decreased to 0.00053.
2024-12-01-21:53:39-root-INFO: Loss too large (1762.008->1763.995)! Learning rate decreased to 0.00042.
2024-12-01-21:53:40-root-INFO: grad norm: 236.834 233.577 39.141
2024-12-01-21:53:40-root-INFO: grad norm: 168.035 163.827 37.369
2024-12-01-21:53:41-root-INFO: grad norm: 142.527 139.558 28.939
2024-12-01-21:53:42-root-INFO: grad norm: 118.390 114.555 29.888
2024-12-01-21:53:43-root-INFO: grad norm: 104.538 101.538 24.866
2024-12-01-21:53:44-root-INFO: grad norm: 92.264 88.583 25.802
2024-12-01-21:53:45-root-INFO: grad norm: 84.332 81.253 22.583
2024-12-01-21:53:46-root-INFO: Loss Change: 1762.008 -> 1735.555
2024-12-01-21:53:46-root-INFO: Regularization Change: 0.000 -> 0.061
2024-12-01-21:53:46-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-21:53:46-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:53:46-root-INFO: step: 185 lr_xt 0.00261863
2024-12-01-21:53:46-root-INFO: grad norm: 284.722 280.715 47.598
2024-12-01-21:53:47-root-INFO: Loss too large (1735.130->1899.534)! Learning rate decreased to 0.00209.
2024-12-01-21:53:47-root-INFO: Loss too large (1735.130->1874.254)! Learning rate decreased to 0.00168.
2024-12-01-21:53:47-root-INFO: Loss too large (1735.130->1848.048)! Learning rate decreased to 0.00134.
2024-12-01-21:53:48-root-INFO: Loss too large (1735.130->1821.149)! Learning rate decreased to 0.00107.
2024-12-01-21:53:48-root-INFO: Loss too large (1735.130->1795.187)! Learning rate decreased to 0.00086.
2024-12-01-21:53:48-root-INFO: Loss too large (1735.130->1772.526)! Learning rate decreased to 0.00069.
2024-12-01-21:53:48-root-INFO: Loss too large (1735.130->1754.742)! Learning rate decreased to 0.00055.
2024-12-01-21:53:49-root-INFO: Loss too large (1735.130->1742.126)! Learning rate decreased to 0.00044.
2024-12-01-21:53:50-root-INFO: grad norm: 257.796 255.110 37.115
2024-12-01-21:53:50-root-INFO: grad norm: 224.391 221.100 38.291
2024-12-01-21:53:51-root-INFO: grad norm: 211.448 209.078 31.570
2024-12-01-21:53:52-root-INFO: grad norm: 195.554 192.616 33.770
2024-12-01-21:53:53-root-INFO: grad norm: 187.454 185.239 28.733
2024-12-01-21:53:54-root-INFO: grad norm: 177.779 175.055 31.003
2024-12-01-21:53:55-root-INFO: grad norm: 172.276 170.160 26.916
2024-12-01-21:53:55-root-INFO: Loss Change: 1735.130 -> 1715.727
2024-12-01-21:53:55-root-INFO: Regularization Change: 0.000 -> 0.050
2024-12-01-21:53:55-root-INFO: Undo step: 185
2024-12-01-21:53:55-root-INFO: Undo step: 186
2024-12-01-21:53:55-root-INFO: Undo step: 187
2024-12-01-21:53:55-root-INFO: Undo step: 188
2024-12-01-21:53:55-root-INFO: Undo step: 189
2024-12-01-21:53:56-root-INFO: step: 190 lr_xt 0.00211904
2024-12-01-21:53:56-root-INFO: grad norm: 874.783 799.302 355.474
2024-12-01-21:53:57-root-INFO: grad norm: 650.141 628.187 167.522
2024-12-01-21:53:57-root-INFO: Loss too large (2587.986->2683.790)! Learning rate decreased to 0.00170.
2024-12-01-21:53:58-root-INFO: grad norm: 678.346 661.875 148.576
2024-12-01-21:53:59-root-INFO: grad norm: 662.278 645.420 148.477
2024-12-01-21:54:00-root-INFO: grad norm: 574.229 563.071 112.651
2024-12-01-21:54:01-root-INFO: grad norm: 424.367 411.248 104.702
2024-12-01-21:54:02-root-INFO: grad norm: 375.534 367.746 76.086
2024-12-01-21:54:02-root-INFO: grad norm: 361.205 348.638 94.450
2024-12-01-21:54:03-root-INFO: Loss too large (2126.610->2135.254)! Learning rate decreased to 0.00136.
2024-12-01-21:54:03-root-INFO: Loss Change: 3330.341 -> 2100.924
2024-12-01-21:54:03-root-INFO: Regularization Change: 0.000 -> 9.297
2024-12-01-21:54:03-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-01-21:54:03-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-21:54:04-root-INFO: step: 189 lr_xt 0.00221139
2024-12-01-21:54:04-root-INFO: grad norm: 248.955 244.164 48.607
2024-12-01-21:54:04-root-INFO: Loss too large (2068.059->2170.617)! Learning rate decreased to 0.00177.
2024-12-01-21:54:04-root-INFO: Loss too large (2068.059->2106.381)! Learning rate decreased to 0.00142.
2024-12-01-21:54:05-root-INFO: Loss too large (2068.059->2070.633)! Learning rate decreased to 0.00113.
2024-12-01-21:54:06-root-INFO: grad norm: 324.876 316.671 72.553
2024-12-01-21:54:06-root-INFO: Loss too large (2052.788->2071.855)! Learning rate decreased to 0.00091.
2024-12-01-21:54:06-root-INFO: Loss too large (2052.788->2054.818)! Learning rate decreased to 0.00072.
2024-12-01-21:54:07-root-INFO: grad norm: 238.815 233.703 49.146
2024-12-01-21:54:08-root-INFO: grad norm: 150.206 144.208 42.022
2024-12-01-21:54:09-root-INFO: grad norm: 138.621 134.127 35.008
2024-12-01-21:54:10-root-INFO: grad norm: 129.412 123.644 38.207
2024-12-01-21:54:11-root-INFO: grad norm: 125.748 121.403 32.767
2024-12-01-21:54:12-root-INFO: grad norm: 124.225 118.653 36.785
2024-12-01-21:54:12-root-INFO: Loss Change: 2068.059 -> 1990.091
2024-12-01-21:54:12-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-01-21:54:12-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-21:54:12-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-21:54:13-root-INFO: step: 188 lr_xt 0.00230740
2024-12-01-21:54:13-root-INFO: grad norm: 147.934 139.412 49.485
2024-12-01-21:54:13-root-INFO: Loss too large (1974.262->1985.947)! Learning rate decreased to 0.00185.
2024-12-01-21:54:13-root-INFO: Loss too large (1974.262->1974.998)! Learning rate decreased to 0.00148.
2024-12-01-21:54:14-root-INFO: grad norm: 237.919 233.559 45.340
2024-12-01-21:54:15-root-INFO: Loss too large (1968.929->2035.465)! Learning rate decreased to 0.00118.
2024-12-01-21:54:15-root-INFO: Loss too large (1968.929->1994.787)! Learning rate decreased to 0.00095.
2024-12-01-21:54:15-root-INFO: Loss too large (1968.929->1971.887)! Learning rate decreased to 0.00076.
2024-12-01-21:54:16-root-INFO: grad norm: 265.715 260.038 54.636
2024-12-01-21:54:16-root-INFO: Loss too large (1960.323->1963.465)! Learning rate decreased to 0.00060.
2024-12-01-21:54:17-root-INFO: grad norm: 219.307 215.186 42.311
2024-12-01-21:54:18-root-INFO: grad norm: 169.289 164.364 40.536
2024-12-01-21:54:19-root-INFO: grad norm: 155.130 151.505 33.339
2024-12-01-21:54:20-root-INFO: grad norm: 141.705 136.962 36.356
2024-12-01-21:54:21-root-INFO: grad norm: 135.814 132.311 30.649
2024-12-01-21:54:21-root-INFO: Loss Change: 1974.262 -> 1927.715
2024-12-01-21:54:21-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-01-21:54:21-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-21:54:21-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-21:54:22-root-INFO: step: 187 lr_xt 0.00240719
2024-12-01-21:54:22-root-INFO: grad norm: 465.512 456.474 91.284
2024-12-01-21:54:22-root-INFO: Loss too large (1948.852->2129.805)! Learning rate decreased to 0.00193.
2024-12-01-21:54:22-root-INFO: Loss too large (1948.852->2099.537)! Learning rate decreased to 0.00154.
2024-12-01-21:54:23-root-INFO: Loss too large (1948.852->2068.435)! Learning rate decreased to 0.00123.
2024-12-01-21:54:23-root-INFO: Loss too large (1948.852->2036.856)! Learning rate decreased to 0.00099.
2024-12-01-21:54:23-root-INFO: Loss too large (1948.852->2005.503)! Learning rate decreased to 0.00079.
2024-12-01-21:54:23-root-INFO: Loss too large (1948.852->1976.407)! Learning rate decreased to 0.00063.
2024-12-01-21:54:24-root-INFO: Loss too large (1948.852->1952.500)! Learning rate decreased to 0.00050.
2024-12-01-21:54:25-root-INFO: grad norm: 295.039 290.492 51.598
2024-12-01-21:54:26-root-INFO: grad norm: 138.775 132.905 39.934
2024-12-01-21:54:27-root-INFO: grad norm: 116.238 112.487 29.292
2024-12-01-21:54:27-root-INFO: grad norm: 99.741 94.368 32.297
2024-12-01-21:54:28-root-INFO: grad norm: 91.182 87.181 26.714
2024-12-01-21:54:29-root-INFO: grad norm: 85.262 80.273 28.737
2024-12-01-21:54:30-root-INFO: grad norm: 81.663 77.568 25.533
2024-12-01-21:54:31-root-INFO: Loss Change: 1948.852 -> 1894.180
2024-12-01-21:54:31-root-INFO: Regularization Change: 0.000 -> 0.125
2024-12-01-21:54:31-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-21:54:31-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:54:31-root-INFO: step: 186 lr_xt 0.00251089
2024-12-01-21:54:31-root-INFO: grad norm: 277.698 270.899 61.071
2024-12-01-21:54:31-root-INFO: Loss too large (1885.438->2035.383)! Learning rate decreased to 0.00201.
2024-12-01-21:54:32-root-INFO: Loss too large (1885.438->2001.188)! Learning rate decreased to 0.00161.
2024-12-01-21:54:32-root-INFO: Loss too large (1885.438->1969.638)! Learning rate decreased to 0.00129.
2024-12-01-21:54:32-root-INFO: Loss too large (1885.438->1941.286)! Learning rate decreased to 0.00103.
2024-12-01-21:54:33-root-INFO: Loss too large (1885.438->1917.638)! Learning rate decreased to 0.00082.
2024-12-01-21:54:33-root-INFO: Loss too large (1885.438->1899.765)! Learning rate decreased to 0.00066.
2024-12-01-21:54:33-root-INFO: Loss too large (1885.438->1887.578)! Learning rate decreased to 0.00053.
2024-12-01-21:54:34-root-INFO: grad norm: 234.994 231.106 42.570
2024-12-01-21:54:35-root-INFO: grad norm: 194.384 189.522 43.200
2024-12-01-21:54:36-root-INFO: grad norm: 179.195 175.908 34.165
2024-12-01-21:54:37-root-INFO: grad norm: 163.614 159.275 37.428
2024-12-01-21:54:37-root-INFO: grad norm: 155.845 152.787 30.721
2024-12-01-21:54:38-root-INFO: grad norm: 148.432 144.362 34.520
2024-12-01-21:54:39-root-INFO: grad norm: 144.651 141.728 28.935
2024-12-01-21:54:40-root-INFO: Loss Change: 1885.438 -> 1851.705
2024-12-01-21:54:40-root-INFO: Regularization Change: 0.000 -> 0.112
2024-12-01-21:54:40-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-21:54:40-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:54:40-root-INFO: step: 185 lr_xt 0.00261863
2024-12-01-21:54:40-root-INFO: grad norm: 349.764 343.765 64.505
2024-12-01-21:54:41-root-INFO: Loss too large (1853.940->2046.104)! Learning rate decreased to 0.00209.
2024-12-01-21:54:41-root-INFO: Loss too large (1853.940->2015.095)! Learning rate decreased to 0.00168.
2024-12-01-21:54:41-root-INFO: Loss too large (1853.940->1983.788)! Learning rate decreased to 0.00134.
2024-12-01-21:54:41-root-INFO: Loss too large (1853.940->1952.227)! Learning rate decreased to 0.00107.
2024-12-01-21:54:42-root-INFO: Loss too large (1853.940->1921.404)! Learning rate decreased to 0.00086.
2024-12-01-21:54:42-root-INFO: Loss too large (1853.940->1893.759)! Learning rate decreased to 0.00069.
2024-12-01-21:54:42-root-INFO: Loss too large (1853.940->1871.705)! Learning rate decreased to 0.00055.
2024-12-01-21:54:43-root-INFO: Loss too large (1853.940->1856.132)! Learning rate decreased to 0.00044.
2024-12-01-21:54:43-root-INFO: grad norm: 248.361 244.798 41.918
2024-12-01-21:54:44-root-INFO: grad norm: 159.814 155.578 36.551
2024-12-01-21:54:45-root-INFO: grad norm: 129.892 126.940 27.537
2024-12-01-21:54:46-root-INFO: grad norm: 106.363 102.329 29.015
2024-12-01-21:54:47-root-INFO: grad norm: 93.803 90.616 24.240
2024-12-01-21:54:48-root-INFO: grad norm: 84.702 80.677 25.799
2024-12-01-21:54:49-root-INFO: grad norm: 79.318 75.896 23.046
2024-12-01-21:54:49-root-INFO: Loss Change: 1853.940 -> 1821.645
2024-12-01-21:54:49-root-INFO: Regularization Change: 0.000 -> 0.071
2024-12-01-21:54:49-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-01-21:54:49-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:54:49-root-INFO: step: 184 lr_xt 0.00273055
2024-12-01-21:54:50-root-INFO: grad norm: 493.710 485.334 90.556
2024-12-01-21:54:50-root-INFO: Loss too large (1842.350->2076.482)! Learning rate decreased to 0.00218.
2024-12-01-21:54:50-root-INFO: Loss too large (1842.350->2042.636)! Learning rate decreased to 0.00175.
2024-12-01-21:54:51-root-INFO: Loss too large (1842.350->2010.000)! Learning rate decreased to 0.00140.
2024-12-01-21:54:51-root-INFO: Loss too large (1842.350->1977.448)! Learning rate decreased to 0.00112.
2024-12-01-21:54:51-root-INFO: Loss too large (1842.350->1944.351)! Learning rate decreased to 0.00089.
2024-12-01-21:54:51-root-INFO: Loss too large (1842.350->1910.841)! Learning rate decreased to 0.00072.
2024-12-01-21:54:52-root-INFO: Loss too large (1842.350->1879.005)! Learning rate decreased to 0.00057.
2024-12-01-21:54:52-root-INFO: Loss too large (1842.350->1852.268)! Learning rate decreased to 0.00046.
2024-12-01-21:54:53-root-INFO: grad norm: 360.906 356.131 58.517
2024-12-01-21:54:54-root-INFO: grad norm: 221.674 216.205 48.936
2024-12-01-21:54:55-root-INFO: grad norm: 197.754 194.515 35.644
2024-12-01-21:54:55-root-INFO: grad norm: 173.508 168.796 40.161
2024-12-01-21:54:56-root-INFO: grad norm: 159.921 157.015 30.350
2024-12-01-21:54:57-root-INFO: grad norm: 146.530 142.252 35.146
2024-12-01-21:54:58-root-INFO: grad norm: 138.010 135.297 27.230
2024-12-01-21:54:59-root-INFO: Loss Change: 1842.350 -> 1789.912
2024-12-01-21:54:59-root-INFO: Regularization Change: 0.000 -> 0.106
2024-12-01-21:54:59-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-01-21:54:59-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:54:59-root-INFO: step: 183 lr_xt 0.00284680
2024-12-01-21:54:59-root-INFO: grad norm: 337.781 331.905 62.727
2024-12-01-21:55:00-root-INFO: Loss too large (1792.642->2001.923)! Learning rate decreased to 0.00228.
2024-12-01-21:55:00-root-INFO: Loss too large (1792.642->1968.618)! Learning rate decreased to 0.00182.
2024-12-01-21:55:00-root-INFO: Loss too large (1792.642->1936.444)! Learning rate decreased to 0.00146.
2024-12-01-21:55:00-root-INFO: Loss too large (1792.642->1904.642)! Learning rate decreased to 0.00117.
2024-12-01-21:55:01-root-INFO: Loss too large (1792.642->1873.349)! Learning rate decreased to 0.00093.
2024-12-01-21:55:01-root-INFO: Loss too large (1792.642->1844.348)! Learning rate decreased to 0.00075.
2024-12-01-21:55:01-root-INFO: Loss too large (1792.642->1820.160)! Learning rate decreased to 0.00060.
2024-12-01-21:55:02-root-INFO: Loss too large (1792.642->1802.211)! Learning rate decreased to 0.00048.
2024-12-01-21:55:02-root-INFO: grad norm: 310.315 306.231 50.181
2024-12-01-21:55:03-root-INFO: grad norm: 281.266 276.637 50.817
2024-12-01-21:55:04-root-INFO: grad norm: 272.493 268.856 44.372
2024-12-01-21:55:05-root-INFO: grad norm: 262.136 257.925 46.796
2024-12-01-21:55:06-root-INFO: grad norm: 258.231 254.794 41.988
2024-12-01-21:55:07-root-INFO: grad norm: 254.014 250.019 44.877
2024-12-01-21:55:08-root-INFO: grad norm: 252.554 249.226 40.860
2024-12-01-21:55:08-root-INFO: Loss Change: 1792.642 -> 1763.708
2024-12-01-21:55:08-root-INFO: Regularization Change: 0.000 -> 0.089
2024-12-01-21:55:08-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-01-21:55:08-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:55:08-root-INFO: step: 182 lr_xt 0.00296752
2024-12-01-21:55:09-root-INFO: grad norm: 583.330 576.747 87.392
2024-12-01-21:55:09-root-INFO: Loss too large (1789.589->2031.918)! Learning rate decreased to 0.00237.
2024-12-01-21:55:09-root-INFO: Loss too large (1789.589->2009.782)! Learning rate decreased to 0.00190.
2024-12-01-21:55:10-root-INFO: Loss too large (1789.589->1985.772)! Learning rate decreased to 0.00152.
2024-12-01-21:55:10-root-INFO: Loss too large (1789.589->1958.928)! Learning rate decreased to 0.00122.
2024-12-01-21:55:10-root-INFO: Loss too large (1789.589->1929.015)! Learning rate decreased to 0.00097.
2024-12-01-21:55:10-root-INFO: Loss too large (1789.589->1896.012)! Learning rate decreased to 0.00078.
2024-12-01-21:55:11-root-INFO: Loss too large (1789.589->1860.323)! Learning rate decreased to 0.00062.
2024-12-01-21:55:11-root-INFO: Loss too large (1789.589->1824.553)! Learning rate decreased to 0.00050.
2024-12-01-21:55:11-root-INFO: Loss too large (1789.589->1793.553)! Learning rate decreased to 0.00040.
2024-12-01-21:55:12-root-INFO: grad norm: 370.414 365.999 57.017
2024-12-01-21:55:13-root-INFO: grad norm: 200.138 196.335 38.831
2024-12-01-21:55:14-root-INFO: grad norm: 165.002 162.262 29.948
2024-12-01-21:55:15-root-INFO: grad norm: 134.862 131.395 30.379
2024-12-01-21:55:16-root-INFO: grad norm: 117.243 114.564 24.922
2024-12-01-21:55:17-root-INFO: grad norm: 102.820 99.414 26.247
2024-12-01-21:55:18-root-INFO: grad norm: 93.429 90.638 22.666
2024-12-01-21:55:18-root-INFO: Loss Change: 1789.589 -> 1734.876
2024-12-01-21:55:18-root-INFO: Regularization Change: 0.000 -> 0.070
2024-12-01-21:55:18-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-01-21:55:18-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-21:55:18-root-INFO: step: 181 lr_xt 0.00309285
2024-12-01-21:55:19-root-INFO: grad norm: 423.232 417.787 67.669
2024-12-01-21:55:19-root-INFO: Loss too large (1748.561->1991.559)! Learning rate decreased to 0.00247.
2024-12-01-21:55:19-root-INFO: Loss too large (1748.561->1966.762)! Learning rate decreased to 0.00198.
2024-12-01-21:55:20-root-INFO: Loss too large (1748.561->1940.167)! Learning rate decreased to 0.00158.
2024-12-01-21:55:20-root-INFO: Loss too large (1748.561->1911.274)! Learning rate decreased to 0.00127.
2024-12-01-21:55:20-root-INFO: Loss too large (1748.561->1879.871)! Learning rate decreased to 0.00101.
2024-12-01-21:55:20-root-INFO: Loss too large (1748.561->1846.255)! Learning rate decreased to 0.00081.
2024-12-01-21:55:21-root-INFO: Loss too large (1748.561->1812.538)! Learning rate decreased to 0.00065.
2024-12-01-21:55:21-root-INFO: Loss too large (1748.561->1782.531)! Learning rate decreased to 0.00052.
2024-12-01-21:55:21-root-INFO: Loss too large (1748.561->1759.267)! Learning rate decreased to 0.00042.
2024-12-01-21:55:22-root-INFO: grad norm: 345.390 341.632 50.811
2024-12-01-21:55:23-root-INFO: grad norm: 267.939 263.843 46.670
2024-12-01-21:55:24-root-INFO: grad norm: 242.004 239.127 37.200
2024-12-01-21:55:25-root-INFO: grad norm: 215.382 211.764 39.314
2024-12-01-21:55:26-root-INFO: grad norm: 200.297 197.726 31.989
2024-12-01-21:55:27-root-INFO: grad norm: 184.951 181.603 35.034
2024-12-01-21:55:27-root-INFO: grad norm: 175.028 172.627 28.890
2024-12-01-21:55:28-root-INFO: Loss Change: 1748.561 -> 1714.515
2024-12-01-21:55:28-root-INFO: Regularization Change: 0.000 -> 0.064
2024-12-01-21:55:28-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-01-21:55:28-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-21:55:28-root-INFO: step: 180 lr_xt 0.00322295
2024-12-01-21:55:29-root-INFO: grad norm: 541.749 535.458 82.318
2024-12-01-21:55:29-root-INFO: Loss too large (1741.027->1994.172)! Learning rate decreased to 0.00258.
2024-12-01-21:55:29-root-INFO: Loss too large (1741.027->1973.109)! Learning rate decreased to 0.00206.
2024-12-01-21:55:29-root-INFO: Loss too large (1741.027->1950.953)! Learning rate decreased to 0.00165.
2024-12-01-21:55:30-root-INFO: Loss too large (1741.027->1926.249)! Learning rate decreased to 0.00132.
2024-12-01-21:55:30-root-INFO: Loss too large (1741.027->1898.365)! Learning rate decreased to 0.00106.
2024-12-01-21:55:30-root-INFO: Loss too large (1741.027->1866.985)! Learning rate decreased to 0.00084.
2024-12-01-21:55:31-root-INFO: Loss too large (1741.027->1832.172)! Learning rate decreased to 0.00068.
2024-12-01-21:55:31-root-INFO: Loss too large (1741.027->1795.967)! Learning rate decreased to 0.00054.
2024-12-01-21:55:31-root-INFO: Loss too large (1741.027->1763.000)! Learning rate decreased to 0.00043.
2024-12-01-21:55:32-root-INFO: grad norm: 438.946 434.223 64.215
2024-12-01-21:55:33-root-INFO: grad norm: 330.810 326.175 55.185
2024-12-01-21:55:34-root-INFO: grad norm: 320.676 317.139 47.498
2024-12-01-21:55:35-root-INFO: grad norm: 308.318 304.074 50.981
2024-12-01-21:55:36-root-INFO: grad norm: 302.357 299.032 44.722
2024-12-01-21:55:36-root-INFO: grad norm: 294.964 290.972 48.362
2024-12-01-21:55:37-root-INFO: grad norm: 290.921 287.740 42.900
2024-12-01-21:55:38-root-INFO: Loss Change: 1741.027 -> 1696.267
2024-12-01-21:55:38-root-INFO: Regularization Change: 0.000 -> 0.082
2024-12-01-21:55:38-root-INFO: Undo step: 180
2024-12-01-21:55:38-root-INFO: Undo step: 181
2024-12-01-21:55:38-root-INFO: Undo step: 182
2024-12-01-21:55:38-root-INFO: Undo step: 183
2024-12-01-21:55:38-root-INFO: Undo step: 184
2024-12-01-21:55:38-root-INFO: step: 185 lr_xt 0.00261863
2024-12-01-21:55:38-root-INFO: grad norm: 616.116 589.441 179.328
2024-12-01-21:55:39-root-INFO: grad norm: 603.304 584.086 151.059
2024-12-01-21:55:40-root-INFO: grad norm: 596.686 584.882 118.098
2024-12-01-21:55:41-root-INFO: grad norm: 539.416 528.379 108.556
2024-12-01-21:55:42-root-INFO: grad norm: 461.210 452.009 91.664
2024-12-01-21:55:43-root-INFO: grad norm: 377.774 371.015 71.139
2024-12-01-21:55:44-root-INFO: grad norm: 353.711 346.963 68.760
2024-12-01-21:55:44-root-INFO: grad norm: 361.461 354.819 68.975
2024-12-01-21:55:45-root-INFO: Loss too large (1996.900->2232.689)! Learning rate decreased to 0.00209.
2024-12-01-21:55:45-root-INFO: Loss too large (1996.900->2121.839)! Learning rate decreased to 0.00168.
2024-12-01-21:55:45-root-INFO: Loss too large (1996.900->2044.351)! Learning rate decreased to 0.00134.
2024-12-01-21:55:46-root-INFO: Loss Change: 2782.821 -> 1994.428
2024-12-01-21:55:46-root-INFO: Regularization Change: 0.000 -> 11.250
2024-12-01-21:55:46-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-01-21:55:46-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:55:46-root-INFO: step: 184 lr_xt 0.00273055
2024-12-01-21:55:46-root-INFO: grad norm: 837.976 824.279 150.891
2024-12-01-21:55:47-root-INFO: Loss too large (2074.105->2161.340)! Learning rate decreased to 0.00218.
2024-12-01-21:55:47-root-INFO: Loss too large (2074.105->2135.696)! Learning rate decreased to 0.00175.
2024-12-01-21:55:47-root-INFO: Loss too large (2074.105->2108.007)! Learning rate decreased to 0.00140.
2024-12-01-21:55:48-root-INFO: Loss too large (2074.105->2077.350)! Learning rate decreased to 0.00112.
2024-12-01-21:55:49-root-INFO: grad norm: 225.821 220.974 46.534
2024-12-01-21:55:49-root-INFO: grad norm: 250.448 245.940 47.306
2024-12-01-21:55:50-root-INFO: grad norm: 232.650 228.739 42.476
2024-12-01-21:55:51-root-INFO: grad norm: 326.529 319.760 66.141
2024-12-01-21:55:51-root-INFO: Loss too large (1899.011->1923.779)! Learning rate decreased to 0.00089.
2024-12-01-21:55:52-root-INFO: Loss too large (1899.011->1905.133)! Learning rate decreased to 0.00072.
2024-12-01-21:55:53-root-INFO: grad norm: 253.367 249.355 44.912
2024-12-01-21:55:53-root-INFO: grad norm: 182.579 177.442 43.006
2024-12-01-21:55:54-root-INFO: grad norm: 187.152 183.906 34.709
2024-12-01-21:55:55-root-INFO: Loss Change: 2074.105 -> 1861.519
2024-12-01-21:55:55-root-INFO: Regularization Change: 0.000 -> 0.488
2024-12-01-21:55:55-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-01-21:55:55-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:55:55-root-INFO: step: 183 lr_xt 0.00284680
2024-12-01-21:55:56-root-INFO: grad norm: 391.188 384.071 74.280
2024-12-01-21:55:56-root-INFO: Loss too large (1866.774->2036.880)! Learning rate decreased to 0.00228.
2024-12-01-21:55:56-root-INFO: Loss too large (1866.774->2009.725)! Learning rate decreased to 0.00182.
2024-12-01-21:55:56-root-INFO: Loss too large (1866.774->1981.831)! Learning rate decreased to 0.00146.
2024-12-01-21:55:57-root-INFO: Loss too large (1866.774->1953.456)! Learning rate decreased to 0.00117.
2024-12-01-21:55:57-root-INFO: Loss too large (1866.774->1925.524)! Learning rate decreased to 0.00093.
2024-12-01-21:55:57-root-INFO: Loss too large (1866.774->1899.805)! Learning rate decreased to 0.00075.
2024-12-01-21:55:58-root-INFO: Loss too large (1866.774->1878.334)! Learning rate decreased to 0.00060.
2024-12-01-21:55:58-root-INFO: grad norm: 297.295 292.620 52.511
2024-12-01-21:55:59-root-INFO: grad norm: 189.862 184.856 43.311
2024-12-01-21:56:00-root-INFO: grad norm: 189.378 185.790 36.690
2024-12-01-21:56:01-root-INFO: grad norm: 197.536 192.861 42.721
2024-12-01-21:56:02-root-INFO: grad norm: 208.193 204.619 38.407
2024-12-01-21:56:03-root-INFO: grad norm: 232.437 227.764 46.375
2024-12-01-21:56:03-root-INFO: Loss too large (1824.038->1824.043)! Learning rate decreased to 0.00048.
2024-12-01-21:56:04-root-INFO: grad norm: 188.504 185.138 35.464
2024-12-01-21:56:04-root-INFO: Loss Change: 1866.774 -> 1813.713
2024-12-01-21:56:04-root-INFO: Regularization Change: 0.000 -> 0.162
2024-12-01-21:56:04-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-01-21:56:04-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-21:56:05-root-INFO: step: 182 lr_xt 0.00296752
2024-12-01-21:56:05-root-INFO: grad norm: 450.660 443.813 78.258
2024-12-01-21:56:05-root-INFO: Loss too large (1825.176->2027.329)! Learning rate decreased to 0.00237.
2024-12-01-21:56:06-root-INFO: Loss too large (1825.176->2005.061)! Learning rate decreased to 0.00190.
2024-12-01-21:56:06-root-INFO: Loss too large (1825.176->1980.495)! Learning rate decreased to 0.00152.
2024-12-01-21:56:06-root-INFO: Loss too large (1825.176->1953.615)! Learning rate decreased to 0.00122.
2024-12-01-21:56:07-root-INFO: Loss too large (1825.176->1924.707)! Learning rate decreased to 0.00097.
2024-12-01-21:56:07-root-INFO: Loss too large (1825.176->1894.799)! Learning rate decreased to 0.00078.
2024-12-01-21:56:07-root-INFO: Loss too large (1825.176->1866.056)! Learning rate decreased to 0.00062.
2024-12-01-21:56:08-root-INFO: Loss too large (1825.176->1841.225)! Learning rate decreased to 0.00050.
2024-12-01-21:56:08-root-INFO: grad norm: 345.423 340.658 57.180
2024-12-01-21:56:09-root-INFO: grad norm: 225.621 221.006 45.403
2024-12-01-21:56:10-root-INFO: grad norm: 221.782 218.377 38.712
2024-12-01-21:56:11-root-INFO: grad norm: 221.379 217.060 43.518
2024-12-01-21:56:12-root-INFO: grad norm: 224.803 221.446 38.703
2024-12-01-21:56:13-root-INFO: grad norm: 233.568 229.331 44.289
2024-12-01-21:56:14-root-INFO: grad norm: 241.323 237.867 40.696
2024-12-01-21:56:14-root-INFO: Loss Change: 1825.176 -> 1783.050
2024-12-01-21:56:14-root-INFO: Regularization Change: 0.000 -> 0.104
2024-12-01-21:56:14-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-01-21:56:14-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-21:56:15-root-INFO: step: 181 lr_xt 0.00309285
2024-12-01-21:56:15-root-INFO: grad norm: 575.592 568.015 93.087
2024-12-01-21:56:15-root-INFO: Loss too large (1805.830->2025.911)! Learning rate decreased to 0.00247.
2024-12-01-21:56:15-root-INFO: Loss too large (1805.830->2007.372)! Learning rate decreased to 0.00198.
2024-12-01-21:56:16-root-INFO: Loss too large (1805.830->1986.567)! Learning rate decreased to 0.00158.
2024-12-01-21:56:16-root-INFO: Loss too large (1805.830->1962.905)! Learning rate decreased to 0.00127.
2024-12-01-21:56:16-root-INFO: Loss too large (1805.830->1936.144)! Learning rate decreased to 0.00101.
2024-12-01-21:56:17-root-INFO: Loss too large (1805.830->1906.224)! Learning rate decreased to 0.00081.
2024-12-01-21:56:17-root-INFO: Loss too large (1805.830->1873.824)! Learning rate decreased to 0.00065.
2024-12-01-21:56:17-root-INFO: Loss too large (1805.830->1841.234)! Learning rate decreased to 0.00052.
2024-12-01-21:56:18-root-INFO: Loss too large (1805.830->1812.106)! Learning rate decreased to 0.00042.
2024-12-01-21:56:18-root-INFO: grad norm: 368.993 364.390 58.103
2024-12-01-21:56:19-root-INFO: grad norm: 195.308 191.040 40.607
2024-12-01-21:56:20-root-INFO: grad norm: 176.019 173.172 31.530
2024-12-01-21:56:21-root-INFO: grad norm: 158.424 154.560 34.774
2024-12-01-21:56:22-root-INFO: grad norm: 148.898 146.193 28.253
2024-12-01-21:56:23-root-INFO: grad norm: 140.590 136.954 31.770
2024-12-01-21:56:24-root-INFO: grad norm: 135.904 133.271 26.623
2024-12-01-21:56:24-root-INFO: Loss Change: 1805.830 -> 1750.115
2024-12-01-21:56:24-root-INFO: Regularization Change: 0.000 -> 0.077
2024-12-01-21:56:24-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-01-21:56:24-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-21:56:25-root-INFO: step: 180 lr_xt 0.00322295
2024-12-01-21:56:25-root-INFO: grad norm: 522.453 515.196 86.779
2024-12-01-21:56:25-root-INFO: Loss too large (1771.469->1999.299)! Learning rate decreased to 0.00258.
2024-12-01-21:56:26-root-INFO: Loss too large (1771.469->1981.202)! Learning rate decreased to 0.00206.
2024-12-01-21:56:26-root-INFO: Loss too large (1771.469->1961.405)! Learning rate decreased to 0.00165.
2024-12-01-21:56:26-root-INFO: Loss too large (1771.469->1939.087)! Learning rate decreased to 0.00132.
2024-12-01-21:56:26-root-INFO: Loss too large (1771.469->1913.816)! Learning rate decreased to 0.00106.
2024-12-01-21:56:27-root-INFO: Loss too large (1771.469->1885.409)! Learning rate decreased to 0.00084.
2024-12-01-21:56:27-root-INFO: Loss too large (1771.469->1854.409)! Learning rate decreased to 0.00068.
2024-12-01-21:56:27-root-INFO: Loss too large (1771.469->1822.846)! Learning rate decreased to 0.00054.
2024-12-01-21:56:28-root-INFO: Loss too large (1771.469->1794.024)! Learning rate decreased to 0.00043.
2024-12-01-21:56:28-root-INFO: grad norm: 421.112 415.985 65.510
2024-12-01-21:56:29-root-INFO: grad norm: 306.059 300.898 55.967
2024-12-01-21:56:30-root-INFO: Loss too large (1745.608->1745.927)! Learning rate decreased to 0.00035.
2024-12-01-21:56:31-root-INFO: grad norm: 227.686 224.484 38.051
2024-12-01-21:56:31-root-INFO: grad norm: 165.260 161.080 36.935
2024-12-01-21:56:32-root-INFO: grad norm: 137.021 134.128 28.006
2024-12-01-21:56:33-root-INFO: grad norm: 115.182 111.212 29.981
2024-12-01-21:56:34-root-INFO: grad norm: 102.267 99.225 24.757
2024-12-01-21:56:34-root-INFO: Loss Change: 1771.469 -> 1724.162
2024-12-01-21:56:34-root-INFO: Regularization Change: 0.000 -> 0.070
2024-12-01-21:56:34-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-01-21:56:34-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-21:56:35-root-INFO: step: 179 lr_xt 0.00335799
2024-12-01-21:56:35-root-INFO: grad norm: 459.952 453.943 74.103
2024-12-01-21:56:35-root-INFO: Loss too large (1739.251->1973.084)! Learning rate decreased to 0.00269.
2024-12-01-21:56:36-root-INFO: Loss too large (1739.251->1956.405)! Learning rate decreased to 0.00215.
2024-12-01-21:56:36-root-INFO: Loss too large (1739.251->1937.312)! Learning rate decreased to 0.00172.
2024-12-01-21:56:36-root-INFO: Loss too large (1739.251->1915.235)! Learning rate decreased to 0.00138.
2024-12-01-21:56:37-root-INFO: Loss too large (1739.251->1889.857)! Learning rate decreased to 0.00110.
2024-12-01-21:56:37-root-INFO: Loss too large (1739.251->1861.112)! Learning rate decreased to 0.00088.
2024-12-01-21:56:37-root-INFO: Loss too large (1739.251->1829.816)! Learning rate decreased to 0.00070.
2024-12-01-21:56:37-root-INFO: Loss too large (1739.251->1798.303)! Learning rate decreased to 0.00056.
2024-12-01-21:56:38-root-INFO: Loss too large (1739.251->1769.901)! Learning rate decreased to 0.00045.
2024-12-01-21:56:38-root-INFO: Loss too large (1739.251->1747.454)! Learning rate decreased to 0.00036.
2024-12-01-21:56:39-root-INFO: grad norm: 356.010 351.825 54.425
2024-12-01-21:56:40-root-INFO: grad norm: 265.342 261.020 47.694
2024-12-01-21:56:40-root-INFO: grad norm: 237.518 234.465 37.960
2024-12-01-21:56:41-root-INFO: grad norm: 210.957 207.169 39.798
2024-12-01-21:56:42-root-INFO: grad norm: 195.945 193.236 32.469
2024-12-01-21:56:43-root-INFO: grad norm: 181.709 178.225 35.413
2024-12-01-21:56:44-root-INFO: grad norm: 172.632 170.111 29.396
2024-12-01-21:56:45-root-INFO: Loss Change: 1739.251 -> 1703.852
2024-12-01-21:56:45-root-INFO: Regularization Change: 0.000 -> 0.055
2024-12-01-21:56:45-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-01-21:56:45-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-21:56:45-root-INFO: step: 178 lr_xt 0.00349812
2024-12-01-21:56:45-root-INFO: grad norm: 552.793 546.162 85.364
2024-12-01-21:56:45-root-INFO: Loss too large (1729.361->1966.713)! Learning rate decreased to 0.00280.
2024-12-01-21:56:46-root-INFO: Loss too large (1729.361->1952.086)! Learning rate decreased to 0.00224.
2024-12-01-21:56:46-root-INFO: Loss too large (1729.361->1936.380)! Learning rate decreased to 0.00179.
2024-12-01-21:56:46-root-INFO: Loss too large (1729.361->1918.184)! Learning rate decreased to 0.00143.
2024-12-01-21:56:47-root-INFO: Loss too large (1729.361->1896.743)! Learning rate decreased to 0.00115.
2024-12-01-21:56:47-root-INFO: Loss too large (1729.361->1871.558)! Learning rate decreased to 0.00092.
2024-12-01-21:56:47-root-INFO: Loss too large (1729.361->1842.360)! Learning rate decreased to 0.00073.
2024-12-01-21:56:47-root-INFO: Loss too large (1729.361->1809.832)! Learning rate decreased to 0.00059.
2024-12-01-21:56:48-root-INFO: Loss too large (1729.361->1776.572)! Learning rate decreased to 0.00047.
2024-12-01-21:56:48-root-INFO: Loss too large (1729.361->1746.639)! Learning rate decreased to 0.00038.
2024-12-01-21:56:49-root-INFO: grad norm: 445.200 440.087 67.280
2024-12-01-21:56:50-root-INFO: grad norm: 348.450 343.576 58.081
2024-12-01-21:56:51-root-INFO: grad norm: 339.541 335.592 51.638
2024-12-01-21:56:51-root-INFO: grad norm: 329.847 325.342 54.335
2024-12-01-21:56:52-root-INFO: grad norm: 325.916 322.158 49.348
2024-12-01-21:56:53-root-INFO: grad norm: 321.659 317.376 52.314
2024-12-01-21:56:54-root-INFO: grad norm: 319.918 316.265 48.208
2024-12-01-21:56:55-root-INFO: Loss Change: 1729.361 -> 1687.954
2024-12-01-21:56:55-root-INFO: Regularization Change: 0.000 -> 0.067
2024-12-01-21:56:55-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-01-21:56:55-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-21:56:55-root-INFO: step: 177 lr_xt 0.00364350
2024-12-01-21:56:55-root-INFO: grad norm: 588.536 582.231 85.918
2024-12-01-21:56:56-root-INFO: Loss too large (1709.262->1947.946)! Learning rate decreased to 0.00291.
2024-12-01-21:56:56-root-INFO: Loss too large (1709.262->1935.709)! Learning rate decreased to 0.00233.
2024-12-01-21:56:56-root-INFO: Loss too large (1709.262->1922.403)! Learning rate decreased to 0.00187.
2024-12-01-21:56:57-root-INFO: Loss too large (1709.262->1906.558)! Learning rate decreased to 0.00149.
2024-12-01-21:56:57-root-INFO: Loss too large (1709.262->1887.275)! Learning rate decreased to 0.00119.
2024-12-01-21:56:57-root-INFO: Loss too large (1709.262->1863.997)! Learning rate decreased to 0.00096.
2024-12-01-21:56:57-root-INFO: Loss too large (1709.262->1836.275)! Learning rate decreased to 0.00076.
2024-12-01-21:56:58-root-INFO: Loss too large (1709.262->1804.132)! Learning rate decreased to 0.00061.
2024-12-01-21:56:58-root-INFO: Loss too large (1709.262->1769.253)! Learning rate decreased to 0.00049.
2024-12-01-21:56:58-root-INFO: Loss too large (1709.262->1735.588)! Learning rate decreased to 0.00039.
2024-12-01-21:56:59-root-INFO: grad norm: 488.537 483.000 73.341
2024-12-01-21:57:00-root-INFO: grad norm: 399.148 394.386 61.469
2024-12-01-21:57:00-root-INFO: Loss too large (1683.814->1686.662)! Learning rate decreased to 0.00031.
2024-12-01-21:57:01-root-INFO: grad norm: 298.413 294.901 45.648
2024-12-01-21:57:02-root-INFO: grad norm: 223.985 220.467 39.539
2024-12-01-21:57:03-root-INFO: grad norm: 187.430 184.732 31.684
2024-12-01-21:57:04-root-INFO: grad norm: 157.347 154.203 31.297
2024-12-01-21:57:05-root-INFO: grad norm: 137.559 135.056 26.123
2024-12-01-21:57:05-root-INFO: Loss Change: 1709.262 -> 1660.040
2024-12-01-21:57:05-root-INFO: Regularization Change: 0.000 -> 0.054
2024-12-01-21:57:06-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-01-21:57:06-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-21:57:06-root-INFO: step: 176 lr_xt 0.00379432
2024-12-01-21:57:06-root-INFO: grad norm: 546.525 540.507 80.881
2024-12-01-21:57:06-root-INFO: Loss too large (1689.313->1933.077)! Learning rate decreased to 0.00304.
2024-12-01-21:57:07-root-INFO: Loss too large (1689.313->1920.492)! Learning rate decreased to 0.00243.
2024-12-01-21:57:07-root-INFO: Loss too large (1689.313->1907.153)! Learning rate decreased to 0.00194.
2024-12-01-21:57:07-root-INFO: Loss too large (1689.313->1891.459)! Learning rate decreased to 0.00155.
2024-12-01-21:57:07-root-INFO: Loss too large (1689.313->1872.404)! Learning rate decreased to 0.00124.
2024-12-01-21:57:08-root-INFO: Loss too large (1689.313->1849.314)! Learning rate decreased to 0.00099.
2024-12-01-21:57:08-root-INFO: Loss too large (1689.313->1821.654)! Learning rate decreased to 0.00080.
2024-12-01-21:57:08-root-INFO: Loss too large (1689.313->1789.483)! Learning rate decreased to 0.00064.
2024-12-01-21:57:09-root-INFO: Loss too large (1689.313->1754.669)! Learning rate decreased to 0.00051.
2024-12-01-21:57:09-root-INFO: Loss too large (1689.313->1721.249)! Learning rate decreased to 0.00041.
2024-12-01-21:57:09-root-INFO: Loss too large (1689.313->1693.752)! Learning rate decreased to 0.00033.
2024-12-01-21:57:10-root-INFO: grad norm: 400.234 395.922 58.590
2024-12-01-21:57:11-root-INFO: grad norm: 306.492 302.234 50.912
2024-12-01-21:57:12-root-INFO: grad norm: 270.942 267.779 41.281
2024-12-01-21:57:13-root-INFO: grad norm: 240.015 236.311 42.005
2024-12-01-21:57:14-root-INFO: grad norm: 220.048 217.287 34.750
2024-12-01-21:57:15-root-INFO: grad norm: 201.966 198.593 36.759
2024-12-01-21:57:16-root-INFO: grad norm: 188.765 186.239 30.776
2024-12-01-21:57:16-root-INFO: Loss Change: 1689.313 -> 1645.113
2024-12-01-21:57:16-root-INFO: Regularization Change: 0.000 -> 0.053
2024-12-01-21:57:16-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-01-21:57:16-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-21:57:16-root-INFO: step: 175 lr_xt 0.00395074
2024-12-01-21:57:17-root-INFO: grad norm: 544.456 538.935 77.340
2024-12-01-21:57:17-root-INFO: Loss too large (1672.181->1916.267)! Learning rate decreased to 0.00316.
2024-12-01-21:57:17-root-INFO: Loss too large (1672.181->1904.870)! Learning rate decreased to 0.00253.
2024-12-01-21:57:18-root-INFO: Loss too large (1672.181->1892.915)! Learning rate decreased to 0.00202.
2024-12-01-21:57:18-root-INFO: Loss too large (1672.181->1878.701)! Learning rate decreased to 0.00162.
2024-12-01-21:57:18-root-INFO: Loss too large (1672.181->1861.059)! Learning rate decreased to 0.00129.
2024-12-01-21:57:19-root-INFO: Loss too large (1672.181->1839.218)! Learning rate decreased to 0.00104.
2024-12-01-21:57:19-root-INFO: Loss too large (1672.181->1812.544)! Learning rate decreased to 0.00083.
2024-12-01-21:57:19-root-INFO: Loss too large (1672.181->1780.777)! Learning rate decreased to 0.00066.
2024-12-01-21:57:19-root-INFO: Loss too large (1672.181->1745.297)! Learning rate decreased to 0.00053.
2024-12-01-21:57:20-root-INFO: Loss too large (1672.181->1710.063)! Learning rate decreased to 0.00042.
2024-12-01-21:57:20-root-INFO: Loss too large (1672.181->1680.203)! Learning rate decreased to 0.00034.
2024-12-01-21:57:21-root-INFO: grad norm: 429.175 424.812 61.037
2024-12-01-21:57:22-root-INFO: grad norm: 360.280 356.070 54.915
2024-12-01-21:57:23-root-INFO: grad norm: 333.146 329.667 48.018
2024-12-01-21:57:24-root-INFO: grad norm: 310.141 306.382 48.139
2024-12-01-21:57:24-root-INFO: grad norm: 295.572 292.435 42.950
2024-12-01-21:57:25-root-INFO: grad norm: 282.437 278.958 44.194
2024-12-01-21:57:26-root-INFO: grad norm: 273.074 270.148 39.866
2024-12-01-21:57:27-root-INFO: Loss Change: 1672.181 -> 1630.326
2024-12-01-21:57:27-root-INFO: Regularization Change: 0.000 -> 0.055
2024-12-01-21:57:27-root-INFO: Undo step: 175
2024-12-01-21:57:27-root-INFO: Undo step: 176
2024-12-01-21:57:27-root-INFO: Undo step: 177
2024-12-01-21:57:27-root-INFO: Undo step: 178
2024-12-01-21:57:27-root-INFO: Undo step: 179
2024-12-01-21:57:27-root-INFO: step: 180 lr_xt 0.00322295
2024-12-01-21:57:27-root-INFO: grad norm: 1001.153 983.815 185.514
2024-12-01-21:57:28-root-INFO: grad norm: 639.870 628.436 120.421
2024-12-01-21:57:29-root-INFO: grad norm: 481.760 474.271 84.617
2024-12-01-21:57:30-root-INFO: grad norm: 305.136 300.428 53.397
2024-12-01-21:57:31-root-INFO: grad norm: 223.311 218.999 43.670
2024-12-01-21:57:32-root-INFO: grad norm: 183.447 179.314 38.721
2024-12-01-21:57:33-root-INFO: grad norm: 204.765 201.275 37.644
2024-12-01-21:57:33-root-INFO: Loss too large (1821.677->1873.304)! Learning rate decreased to 0.00258.
2024-12-01-21:57:34-root-INFO: grad norm: 415.263 411.110 58.581
2024-12-01-21:57:34-root-INFO: Loss too large (1820.639->1907.399)! Learning rate decreased to 0.00206.
2024-12-01-21:57:35-root-INFO: Loss too large (1820.639->1869.701)! Learning rate decreased to 0.00165.
2024-12-01-21:57:35-root-INFO: Loss too large (1820.639->1836.343)! Learning rate decreased to 0.00132.
2024-12-01-21:57:35-root-INFO: Loss Change: 2934.899 -> 1809.322
2024-12-01-21:57:36-root-INFO: Regularization Change: 0.000 -> 11.055
2024-12-01-21:57:36-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-01-21:57:36-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-21:57:36-root-INFO: step: 179 lr_xt 0.00335799
2024-12-01-21:57:36-root-INFO: grad norm: 232.443 228.741 41.316
2024-12-01-21:57:36-root-INFO: Loss too large (1795.053->1915.544)! Learning rate decreased to 0.00269.
2024-12-01-21:57:37-root-INFO: Loss too large (1795.053->1823.882)! Learning rate decreased to 0.00215.
2024-12-01-21:57:37-root-INFO: grad norm: 362.490 359.002 50.165
2024-12-01-21:57:38-root-INFO: Loss too large (1773.351->1836.313)! Learning rate decreased to 0.00172.
2024-12-01-21:57:38-root-INFO: Loss too large (1773.351->1803.804)! Learning rate decreased to 0.00138.
2024-12-01-21:57:38-root-INFO: Loss too large (1773.351->1777.382)! Learning rate decreased to 0.00110.
2024-12-01-21:57:39-root-INFO: grad norm: 217.096 213.697 38.266
2024-12-01-21:57:40-root-INFO: grad norm: 103.659 100.241 26.399
2024-12-01-21:57:41-root-INFO: grad norm: 98.183 94.561 26.420
2024-12-01-21:57:42-root-INFO: grad norm: 99.465 96.306 24.867
2024-12-01-21:57:43-root-INFO: grad norm: 110.278 106.886 27.139
2024-12-01-21:57:44-root-INFO: grad norm: 160.046 157.807 26.679
2024-12-01-21:57:44-root-INFO: Loss too large (1692.182->1694.657)! Learning rate decreased to 0.00088.
2024-12-01-21:57:45-root-INFO: Loss Change: 1795.053 -> 1690.015
2024-12-01-21:57:45-root-INFO: Regularization Change: 0.000 -> 0.648
2024-12-01-21:57:45-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-01-21:57:45-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-21:57:45-root-INFO: step: 178 lr_xt 0.00349812
2024-12-01-21:57:45-root-INFO: grad norm: 111.726 107.787 29.406
2024-12-01-21:57:45-root-INFO: Loss too large (1676.340->1708.553)! Learning rate decreased to 0.00280.
2024-12-01-21:57:46-root-INFO: Loss too large (1676.340->1688.734)! Learning rate decreased to 0.00224.
2024-12-01-21:57:46-root-INFO: Loss too large (1676.340->1677.661)! Learning rate decreased to 0.00179.
2024-12-01-21:57:47-root-INFO: grad norm: 285.001 282.611 36.828
2024-12-01-21:57:47-root-INFO: Loss too large (1672.036->1757.269)! Learning rate decreased to 0.00143.
2024-12-01-21:57:48-root-INFO: Loss too large (1672.036->1731.014)! Learning rate decreased to 0.00115.
2024-12-01-21:57:48-root-INFO: Loss too large (1672.036->1707.810)! Learning rate decreased to 0.00092.
2024-12-01-21:57:48-root-INFO: Loss too large (1672.036->1689.086)! Learning rate decreased to 0.00073.
2024-12-01-21:57:48-root-INFO: Loss too large (1672.036->1675.502)! Learning rate decreased to 0.00059.
2024-12-01-21:57:49-root-INFO: grad norm: 217.079 214.020 36.310
2024-12-01-21:57:50-root-INFO: grad norm: 151.995 149.998 24.556
2024-12-01-21:57:51-root-INFO: grad norm: 136.508 133.697 27.560
2024-12-01-21:57:52-root-INFO: grad norm: 123.893 121.901 22.127
2024-12-01-21:57:53-root-INFO: grad norm: 118.813 116.052 25.465
2024-12-01-21:57:54-root-INFO: grad norm: 115.917 113.965 21.183
2024-12-01-21:57:54-root-INFO: Loss Change: 1676.340 -> 1640.323
2024-12-01-21:57:54-root-INFO: Regularization Change: 0.000 -> 0.210
2024-12-01-21:57:54-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-01-21:57:54-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-21:57:55-root-INFO: step: 177 lr_xt 0.00364350
2024-12-01-21:57:55-root-INFO: grad norm: 100.161 97.246 23.985
2024-12-01-21:57:55-root-INFO: Loss too large (1631.498->1663.351)! Learning rate decreased to 0.00291.
2024-12-01-21:57:56-root-INFO: Loss too large (1631.498->1649.918)! Learning rate decreased to 0.00233.
2024-12-01-21:57:56-root-INFO: Loss too large (1631.498->1640.553)! Learning rate decreased to 0.00187.
2024-12-01-21:57:56-root-INFO: Loss too large (1631.498->1634.490)! Learning rate decreased to 0.00149.
2024-12-01-21:57:57-root-INFO: grad norm: 236.230 233.217 37.608
2024-12-01-21:57:57-root-INFO: Loss too large (1630.860->1741.704)! Learning rate decreased to 0.00119.
2024-12-01-21:57:58-root-INFO: Loss too large (1630.860->1691.223)! Learning rate decreased to 0.00096.
2024-12-01-21:57:58-root-INFO: Loss too large (1630.860->1658.177)! Learning rate decreased to 0.00076.
2024-12-01-21:57:58-root-INFO: Loss too large (1630.860->1638.370)! Learning rate decreased to 0.00061.
2024-12-01-21:57:59-root-INFO: grad norm: 271.684 269.514 34.269
2024-12-01-21:57:59-root-INFO: Loss too large (1627.600->1630.964)! Learning rate decreased to 0.00049.
2024-12-01-21:58:00-root-INFO: grad norm: 215.873 213.041 34.854
2024-12-01-21:58:01-root-INFO: grad norm: 163.222 161.471 23.848
2024-12-01-21:58:02-root-INFO: grad norm: 143.290 140.770 26.752
2024-12-01-21:58:03-root-INFO: grad norm: 125.478 123.758 20.706
2024-12-01-21:58:04-root-INFO: grad norm: 115.722 113.270 23.692
2024-12-01-21:58:05-root-INFO: Loss Change: 1631.498 -> 1606.938
2024-12-01-21:58:05-root-INFO: Regularization Change: 0.000 -> 0.124
2024-12-01-21:58:05-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-01-21:58:05-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-21:58:05-root-INFO: step: 176 lr_xt 0.00379432
2024-12-01-21:58:05-root-INFO: grad norm: 401.504 398.222 51.231
2024-12-01-21:58:06-root-INFO: Loss too large (1621.316->1839.958)! Learning rate decreased to 0.00304.
2024-12-01-21:58:06-root-INFO: Loss too large (1621.316->1820.244)! Learning rate decreased to 0.00243.
2024-12-01-21:58:06-root-INFO: Loss too large (1621.316->1797.657)! Learning rate decreased to 0.00194.
2024-12-01-21:58:07-root-INFO: Loss too large (1621.316->1772.267)! Learning rate decreased to 0.00155.
2024-12-01-21:58:07-root-INFO: Loss too large (1621.316->1744.442)! Learning rate decreased to 0.00124.
2024-12-01-21:58:07-root-INFO: Loss too large (1621.316->1714.955)! Learning rate decreased to 0.00099.
2024-12-01-21:58:07-root-INFO: Loss too large (1621.316->1685.212)! Learning rate decreased to 0.00080.
2024-12-01-21:58:08-root-INFO: Loss too large (1621.316->1657.524)! Learning rate decreased to 0.00064.
2024-12-01-21:58:08-root-INFO: Loss too large (1621.316->1634.549)! Learning rate decreased to 0.00051.
2024-12-01-21:58:09-root-INFO: grad norm: 322.793 319.513 45.903
2024-12-01-21:58:10-root-INFO: grad norm: 238.280 236.003 32.866
2024-12-01-21:58:11-root-INFO: grad norm: 224.587 221.964 34.226
2024-12-01-21:58:11-root-INFO: grad norm: 211.169 209.150 29.135
2024-12-01-21:58:12-root-INFO: grad norm: 206.290 203.821 31.822
2024-12-01-21:58:13-root-INFO: grad norm: 202.608 200.716 27.627
2024-12-01-21:58:14-root-INFO: grad norm: 202.250 199.848 31.080
2024-12-01-21:58:15-root-INFO: Loss Change: 1621.316 -> 1586.741
2024-12-01-21:58:15-root-INFO: Regularization Change: 0.000 -> 0.083
2024-12-01-21:58:15-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-01-21:58:15-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-21:58:15-root-INFO: step: 175 lr_xt 0.00395074
2024-12-01-21:58:15-root-INFO: grad norm: 472.414 468.885 57.643
2024-12-01-21:58:16-root-INFO: Loss too large (1605.505->1831.743)! Learning rate decreased to 0.00316.
2024-12-01-21:58:16-root-INFO: Loss too large (1605.505->1815.340)! Learning rate decreased to 0.00253.
2024-12-01-21:58:16-root-INFO: Loss too large (1605.505->1796.242)! Learning rate decreased to 0.00202.
2024-12-01-21:58:17-root-INFO: Loss too large (1605.505->1773.836)! Learning rate decreased to 0.00162.
2024-12-01-21:58:17-root-INFO: Loss too large (1605.505->1748.094)! Learning rate decreased to 0.00129.
2024-12-01-21:58:17-root-INFO: Loss too large (1605.505->1719.294)! Learning rate decreased to 0.00104.
2024-12-01-21:58:17-root-INFO: Loss too large (1605.505->1688.192)! Learning rate decreased to 0.00083.
2024-12-01-21:58:18-root-INFO: Loss too large (1605.505->1656.462)! Learning rate decreased to 0.00066.
2024-12-01-21:58:18-root-INFO: Loss too large (1605.505->1627.178)! Learning rate decreased to 0.00053.
2024-12-01-21:58:19-root-INFO: grad norm: 377.806 374.258 51.656
2024-12-01-21:58:20-root-INFO: grad norm: 275.735 273.392 35.867
2024-12-01-21:58:21-root-INFO: grad norm: 274.157 271.360 39.059
2024-12-01-21:58:21-root-INFO: grad norm: 273.488 271.277 34.703
2024-12-01-21:58:22-root-INFO: Loss too large (1575.334->1575.649)! Learning rate decreased to 0.00042.
2024-12-01-21:58:23-root-INFO: grad norm: 194.701 192.380 29.977
2024-12-01-21:58:24-root-INFO: grad norm: 136.510 134.825 21.386
2024-12-01-21:58:24-root-INFO: grad norm: 109.400 107.318 21.243
2024-12-01-21:58:25-root-INFO: Loss Change: 1605.505 -> 1560.298
2024-12-01-21:58:25-root-INFO: Regularization Change: 0.000 -> 0.079
2024-12-01-21:58:25-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-01-21:58:25-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-21:58:25-root-INFO: step: 174 lr_xt 0.00411294
2024-12-01-21:58:26-root-INFO: grad norm: 228.312 226.308 30.186
2024-12-01-21:58:26-root-INFO: Loss too large (1564.703->1759.100)! Learning rate decreased to 0.00329.
2024-12-01-21:58:26-root-INFO: Loss too large (1564.703->1737.056)! Learning rate decreased to 0.00263.
2024-12-01-21:58:27-root-INFO: Loss too large (1564.703->1712.204)! Learning rate decreased to 0.00211.
2024-12-01-21:58:27-root-INFO: Loss too large (1564.703->1685.200)! Learning rate decreased to 0.00168.
2024-12-01-21:58:27-root-INFO: Loss too large (1564.703->1657.210)! Learning rate decreased to 0.00135.
2024-12-01-21:58:27-root-INFO: Loss too large (1564.703->1630.184)! Learning rate decreased to 0.00108.
2024-12-01-21:58:28-root-INFO: Loss too large (1564.703->1606.498)! Learning rate decreased to 0.00086.
2024-12-01-21:58:28-root-INFO: Loss too large (1564.703->1587.880)! Learning rate decreased to 0.00069.
2024-12-01-21:58:28-root-INFO: Loss too large (1564.703->1574.723)! Learning rate decreased to 0.00055.
2024-12-01-21:58:29-root-INFO: Loss too large (1564.703->1566.316)! Learning rate decreased to 0.00044.
2024-12-01-21:58:29-root-INFO: grad norm: 194.196 191.838 30.171
2024-12-01-21:58:30-root-INFO: grad norm: 163.921 162.218 23.570
2024-12-01-21:58:31-root-INFO: grad norm: 145.497 143.369 24.795
2024-12-01-21:58:32-root-INFO: grad norm: 129.115 127.525 20.203
2024-12-01-21:58:33-root-INFO: grad norm: 117.742 115.712 21.766
2024-12-01-21:58:34-root-INFO: grad norm: 107.671 106.110 18.264
2024-12-01-21:58:34-root-INFO: grad norm: 100.210 98.227 19.840
2024-12-01-21:58:35-root-INFO: Loss Change: 1564.703 -> 1545.274
2024-12-01-21:58:35-root-INFO: Regularization Change: 0.000 -> 0.053
2024-12-01-21:58:35-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-01-21:58:35-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-21:58:35-root-INFO: step: 173 lr_xt 0.00428111
2024-12-01-21:58:36-root-INFO: grad norm: 371.951 368.785 48.426
2024-12-01-21:58:36-root-INFO: Loss too large (1561.664->1787.193)! Learning rate decreased to 0.00342.
2024-12-01-21:58:36-root-INFO: Loss too large (1561.664->1770.739)! Learning rate decreased to 0.00274.
2024-12-01-21:58:36-root-INFO: Loss too large (1561.664->1751.440)! Learning rate decreased to 0.00219.
2024-12-01-21:58:37-root-INFO: Loss too large (1561.664->1728.880)! Learning rate decreased to 0.00175.
2024-12-01-21:58:37-root-INFO: Loss too large (1561.664->1703.038)! Learning rate decreased to 0.00140.
2024-12-01-21:58:37-root-INFO: Loss too large (1561.664->1674.310)! Learning rate decreased to 0.00112.
2024-12-01-21:58:38-root-INFO: Loss too large (1561.664->1643.750)! Learning rate decreased to 0.00090.
2024-12-01-21:58:38-root-INFO: Loss too large (1561.664->1613.630)! Learning rate decreased to 0.00072.
2024-12-01-21:58:38-root-INFO: Loss too large (1561.664->1587.231)! Learning rate decreased to 0.00057.
2024-12-01-21:58:38-root-INFO: Loss too large (1561.664->1567.190)! Learning rate decreased to 0.00046.
2024-12-01-21:58:39-root-INFO: grad norm: 295.761 292.811 41.674
2024-12-01-21:58:40-root-INFO: grad norm: 232.690 230.303 33.246
2024-12-01-21:58:41-root-INFO: grad norm: 204.724 202.335 31.184
2024-12-01-21:58:42-root-INFO: grad norm: 179.461 177.389 27.191
2024-12-01-21:58:43-root-INFO: grad norm: 163.146 161.005 26.341
2024-12-01-21:58:43-root-INFO: grad norm: 148.199 146.309 23.593
2024-12-01-21:58:44-root-INFO: grad norm: 137.383 135.392 23.304
2024-12-01-21:58:45-root-INFO: Loss Change: 1561.664 -> 1527.520
2024-12-01-21:58:45-root-INFO: Regularization Change: 0.000 -> 0.075
2024-12-01-21:58:45-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-01-21:58:45-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-21:58:45-root-INFO: step: 172 lr_xt 0.00445543
2024-12-01-21:58:45-root-INFO: grad norm: 303.061 300.504 39.280
2024-12-01-21:58:46-root-INFO: Loss too large (1535.217->1759.350)! Learning rate decreased to 0.00356.
2024-12-01-21:58:46-root-INFO: Loss too large (1535.217->1742.597)! Learning rate decreased to 0.00285.
2024-12-01-21:58:46-root-INFO: Loss too large (1535.217->1722.342)! Learning rate decreased to 0.00228.
2024-12-01-21:58:47-root-INFO: Loss too large (1535.217->1698.544)! Learning rate decreased to 0.00182.
2024-12-01-21:58:47-root-INFO: Loss too large (1535.217->1671.490)! Learning rate decreased to 0.00146.
2024-12-01-21:58:47-root-INFO: Loss too large (1535.217->1641.899)! Learning rate decreased to 0.00117.
2024-12-01-21:58:48-root-INFO: Loss too large (1535.217->1611.421)! Learning rate decreased to 0.00093.
2024-12-01-21:58:48-root-INFO: Loss too large (1535.217->1582.948)! Learning rate decreased to 0.00075.
2024-12-01-21:58:48-root-INFO: Loss too large (1535.217->1559.528)! Learning rate decreased to 0.00060.
2024-12-01-21:58:49-root-INFO: Loss too large (1535.217->1542.699)! Learning rate decreased to 0.00048.
2024-12-01-21:58:49-root-INFO: grad norm: 276.881 274.335 37.461
2024-12-01-21:58:50-root-INFO: grad norm: 252.614 250.396 33.399
2024-12-01-21:58:51-root-INFO: grad norm: 238.073 235.794 32.863
2024-12-01-21:58:52-root-INFO: grad norm: 224.040 222.026 29.967
2024-12-01-21:58:53-root-INFO: grad norm: 214.338 212.227 30.013
2024-12-01-21:58:54-root-INFO: grad norm: 204.845 202.971 27.650
2024-12-01-21:58:55-root-INFO: grad norm: 197.832 195.837 28.024
2024-12-01-21:58:55-root-INFO: Loss Change: 1535.217 -> 1512.646
2024-12-01-21:58:55-root-INFO: Regularization Change: 0.000 -> 0.062
2024-12-01-21:58:55-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-21:58:55-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-21:58:56-root-INFO: step: 171 lr_xt 0.00463611
2024-12-01-21:58:56-root-INFO: grad norm: 407.715 404.914 47.710
2024-12-01-21:58:56-root-INFO: Loss too large (1528.104->1765.004)! Learning rate decreased to 0.00371.
2024-12-01-21:58:56-root-INFO: Loss too large (1528.104->1751.900)! Learning rate decreased to 0.00297.
2024-12-01-21:58:57-root-INFO: Loss too large (1528.104->1736.056)! Learning rate decreased to 0.00237.
2024-12-01-21:58:57-root-INFO: Loss too large (1528.104->1716.542)! Learning rate decreased to 0.00190.
2024-12-01-21:58:57-root-INFO: Loss too large (1528.104->1692.982)! Learning rate decreased to 0.00152.
2024-12-01-21:58:58-root-INFO: Loss too large (1528.104->1665.337)! Learning rate decreased to 0.00122.
2024-12-01-21:58:58-root-INFO: Loss too large (1528.104->1633.998)! Learning rate decreased to 0.00097.
2024-12-01-21:58:58-root-INFO: Loss too large (1528.104->1600.346)! Learning rate decreased to 0.00078.
2024-12-01-21:58:58-root-INFO: Loss too large (1528.104->1567.696)! Learning rate decreased to 0.00062.
2024-12-01-21:58:59-root-INFO: Loss too large (1528.104->1540.487)! Learning rate decreased to 0.00050.
2024-12-01-21:59:00-root-INFO: grad norm: 350.629 347.450 47.105
2024-12-01-21:59:00-root-INFO: grad norm: 307.996 305.753 37.107
2024-12-01-21:59:01-root-INFO: grad norm: 288.761 286.046 39.501
2024-12-01-21:59:02-root-INFO: grad norm: 271.473 269.453 33.049
2024-12-01-21:59:03-root-INFO: grad norm: 260.322 257.831 35.922
2024-12-01-21:59:04-root-INFO: grad norm: 249.780 247.895 30.630
2024-12-01-21:59:05-root-INFO: grad norm: 242.165 239.825 33.586
2024-12-01-21:59:05-root-INFO: Loss Change: 1528.104 -> 1496.461
2024-12-01-21:59:05-root-INFO: Regularization Change: 0.000 -> 0.068
2024-12-01-21:59:05-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-01-21:59:05-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-21:59:06-root-INFO: step: 170 lr_xt 0.00482333
2024-12-01-21:59:06-root-INFO: grad norm: 423.138 420.114 50.498
2024-12-01-21:59:06-root-INFO: Loss too large (1517.081->1752.894)! Learning rate decreased to 0.00386.
2024-12-01-21:59:07-root-INFO: Loss too large (1517.081->1740.159)! Learning rate decreased to 0.00309.
2024-12-01-21:59:07-root-INFO: Loss too large (1517.081->1725.147)! Learning rate decreased to 0.00247.
2024-12-01-21:59:07-root-INFO: Loss too large (1517.081->1706.621)! Learning rate decreased to 0.00198.
2024-12-01-21:59:07-root-INFO: Loss too large (1517.081->1683.974)! Learning rate decreased to 0.00158.
2024-12-01-21:59:08-root-INFO: Loss too large (1517.081->1657.020)! Learning rate decreased to 0.00126.
2024-12-01-21:59:08-root-INFO: Loss too large (1517.081->1625.998)! Learning rate decreased to 0.00101.
2024-12-01-21:59:08-root-INFO: Loss too large (1517.081->1591.973)! Learning rate decreased to 0.00081.
2024-12-01-21:59:09-root-INFO: Loss too large (1517.081->1557.938)! Learning rate decreased to 0.00065.
2024-12-01-21:59:09-root-INFO: Loss too large (1517.081->1528.699)! Learning rate decreased to 0.00052.
2024-12-01-21:59:10-root-INFO: grad norm: 358.143 355.102 46.574
2024-12-01-21:59:11-root-INFO: grad norm: 314.813 312.393 38.954
2024-12-01-21:59:11-root-INFO: grad norm: 295.045 292.428 39.214
2024-12-01-21:59:12-root-INFO: grad norm: 277.993 275.826 34.645
2024-12-01-21:59:13-root-INFO: grad norm: 266.547 264.133 35.796
2024-12-01-21:59:14-root-INFO: grad norm: 255.973 253.965 32.001
2024-12-01-21:59:15-root-INFO: grad norm: 247.951 245.676 33.508
2024-12-01-21:59:15-root-INFO: Loss Change: 1517.081 -> 1480.523
2024-12-01-21:59:15-root-INFO: Regularization Change: 0.000 -> 0.081
2024-12-01-21:59:15-root-INFO: Undo step: 170
2024-12-01-21:59:15-root-INFO: Undo step: 171
2024-12-01-21:59:15-root-INFO: Undo step: 172
2024-12-01-21:59:15-root-INFO: Undo step: 173
2024-12-01-21:59:15-root-INFO: Undo step: 174
2024-12-01-21:59:16-root-INFO: step: 175 lr_xt 0.00395074
2024-12-01-21:59:16-root-INFO: grad norm: 666.725 646.659 162.341
2024-12-01-21:59:17-root-INFO: grad norm: 699.537 687.558 128.904
2024-12-01-21:59:17-root-INFO: Loss too large (2198.777->2344.945)! Learning rate decreased to 0.00316.
2024-12-01-21:59:18-root-INFO: grad norm: 510.394 501.570 94.493
2024-12-01-21:59:19-root-INFO: grad norm: 538.936 531.684 88.113
2024-12-01-21:59:20-root-INFO: grad norm: 533.508 528.778 70.883
2024-12-01-21:59:20-root-INFO: Loss too large (1894.547->1922.695)! Learning rate decreased to 0.00253.
2024-12-01-21:59:21-root-INFO: grad norm: 463.266 458.331 67.439
2024-12-01-21:59:22-root-INFO: grad norm: 274.964 269.504 54.523
2024-12-01-21:59:23-root-INFO: grad norm: 169.714 166.599 32.369
2024-12-01-21:59:23-root-INFO: Loss Change: 2639.447 -> 1623.354
2024-12-01-21:59:23-root-INFO: Regularization Change: 0.000 -> 12.505
2024-12-01-21:59:23-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-01-21:59:23-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-21:59:24-root-INFO: step: 174 lr_xt 0.00411294
2024-12-01-21:59:24-root-INFO: grad norm: 274.572 271.747 39.285
2024-12-01-21:59:24-root-INFO: Loss too large (1625.245->1771.649)! Learning rate decreased to 0.00329.
2024-12-01-21:59:24-root-INFO: Loss too large (1625.245->1733.837)! Learning rate decreased to 0.00263.
2024-12-01-21:59:25-root-INFO: Loss too large (1625.245->1697.859)! Learning rate decreased to 0.00211.
2024-12-01-21:59:25-root-INFO: Loss too large (1625.245->1665.786)! Learning rate decreased to 0.00168.
2024-12-01-21:59:25-root-INFO: Loss too large (1625.245->1639.621)! Learning rate decreased to 0.00135.
2024-12-01-21:59:26-root-INFO: grad norm: 240.632 236.158 46.185
2024-12-01-21:59:27-root-INFO: grad norm: 215.308 212.835 32.540
2024-12-01-21:59:28-root-INFO: grad norm: 229.190 225.123 42.987
2024-12-01-21:59:29-root-INFO: grad norm: 268.783 266.105 37.848
2024-12-01-21:59:29-root-INFO: Loss too large (1587.736->1597.701)! Learning rate decreased to 0.00108.
2024-12-01-21:59:30-root-INFO: grad norm: 225.732 221.858 41.642
2024-12-01-21:59:31-root-INFO: grad norm: 189.435 187.237 28.775
2024-12-01-21:59:32-root-INFO: grad norm: 190.825 187.379 36.101
2024-12-01-21:59:32-root-INFO: Loss Change: 1625.245 -> 1556.984
2024-12-01-21:59:32-root-INFO: Regularization Change: 0.000 -> 0.522
2024-12-01-21:59:32-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-01-21:59:32-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-21:59:32-root-INFO: step: 173 lr_xt 0.00428111
2024-12-01-21:59:33-root-INFO: grad norm: 346.740 343.183 49.538
2024-12-01-21:59:33-root-INFO: Loss too large (1572.363->1759.357)! Learning rate decreased to 0.00342.
2024-12-01-21:59:33-root-INFO: Loss too large (1572.363->1727.980)! Learning rate decreased to 0.00274.
2024-12-01-21:59:34-root-INFO: Loss too large (1572.363->1694.192)! Learning rate decreased to 0.00219.
2024-12-01-21:59:34-root-INFO: Loss too large (1572.363->1659.060)! Learning rate decreased to 0.00175.
2024-12-01-21:59:34-root-INFO: Loss too large (1572.363->1624.560)! Learning rate decreased to 0.00140.
2024-12-01-21:59:34-root-INFO: Loss too large (1572.363->1593.396)! Learning rate decreased to 0.00112.
2024-12-01-21:59:35-root-INFO: grad norm: 276.854 272.503 48.892
2024-12-01-21:59:36-root-INFO: grad norm: 205.079 202.530 32.235
2024-12-01-21:59:37-root-INFO: grad norm: 215.021 211.586 38.276
2024-12-01-21:59:38-root-INFO: grad norm: 238.702 236.178 34.623
2024-12-01-21:59:38-root-INFO: Loss too large (1531.864->1534.938)! Learning rate decreased to 0.00090.
2024-12-01-21:59:39-root-INFO: grad norm: 190.810 187.708 34.268
2024-12-01-21:59:39-root-INFO: grad norm: 154.482 152.465 24.884
2024-12-01-21:59:40-root-INFO: grad norm: 142.244 139.571 27.450
2024-12-01-21:59:41-root-INFO: Loss Change: 1572.363 -> 1507.662
2024-12-01-21:59:41-root-INFO: Regularization Change: 0.000 -> 0.303
2024-12-01-21:59:41-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-01-21:59:41-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-21:59:41-root-INFO: step: 172 lr_xt 0.00445543
2024-12-01-21:59:42-root-INFO: grad norm: 219.969 217.597 32.217
2024-12-01-21:59:42-root-INFO: Loss too large (1508.037->1677.002)! Learning rate decreased to 0.00356.
2024-12-01-21:59:42-root-INFO: Loss too large (1508.037->1646.779)! Learning rate decreased to 0.00285.
2024-12-01-21:59:42-root-INFO: Loss too large (1508.037->1615.201)! Learning rate decreased to 0.00228.
2024-12-01-21:59:43-root-INFO: Loss too large (1508.037->1584.124)! Learning rate decreased to 0.00182.
2024-12-01-21:59:43-root-INFO: Loss too large (1508.037->1555.907)! Learning rate decreased to 0.00146.
2024-12-01-21:59:43-root-INFO: Loss too large (1508.037->1532.831)! Learning rate decreased to 0.00117.
2024-12-01-21:59:44-root-INFO: Loss too large (1508.037->1516.082)! Learning rate decreased to 0.00093.
2024-12-01-21:59:44-root-INFO: grad norm: 213.614 210.422 36.789
2024-12-01-21:59:45-root-INFO: grad norm: 210.257 208.048 30.401
2024-12-01-21:59:46-root-INFO: grad norm: 211.703 208.620 35.998
2024-12-01-21:59:47-root-INFO: grad norm: 215.990 213.796 30.707
2024-12-01-21:59:48-root-INFO: grad norm: 220.716 217.607 36.915
2024-12-01-21:59:49-root-INFO: grad norm: 227.794 225.547 31.916
2024-12-01-21:59:50-root-INFO: grad norm: 233.000 229.815 38.394
2024-12-01-21:59:50-root-INFO: Loss Change: 1508.037 -> 1483.408
2024-12-01-21:59:50-root-INFO: Regularization Change: 0.000 -> 0.195
2024-12-01-21:59:50-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-21:59:50-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-21:59:51-root-INFO: step: 171 lr_xt 0.00463611
2024-12-01-21:59:51-root-INFO: grad norm: 355.639 352.732 45.374
2024-12-01-21:59:51-root-INFO: Loss too large (1497.271->1707.985)! Learning rate decreased to 0.00371.
2024-12-01-21:59:51-root-INFO: Loss too large (1497.271->1683.524)! Learning rate decreased to 0.00297.
2024-12-01-21:59:52-root-INFO: Loss too large (1497.271->1655.257)! Learning rate decreased to 0.00237.
2024-12-01-21:59:52-root-INFO: Loss too large (1497.271->1623.217)! Learning rate decreased to 0.00190.
2024-12-01-21:59:52-root-INFO: Loss too large (1497.271->1588.434)! Learning rate decreased to 0.00152.
2024-12-01-21:59:53-root-INFO: Loss too large (1497.271->1552.873)! Learning rate decreased to 0.00122.
2024-12-01-21:59:53-root-INFO: Loss too large (1497.271->1519.913)! Learning rate decreased to 0.00097.
2024-12-01-21:59:54-root-INFO: grad norm: 308.524 304.238 51.244
2024-12-01-21:59:55-root-INFO: grad norm: 268.338 265.949 35.727
2024-12-01-21:59:55-root-INFO: Loss too large (1475.543->1475.795)! Learning rate decreased to 0.00078.
2024-12-01-21:59:56-root-INFO: grad norm: 191.602 188.675 33.364
2024-12-01-21:59:57-root-INFO: grad norm: 145.310 143.625 22.068
2024-12-01-21:59:58-root-INFO: grad norm: 122.896 120.494 24.180
2024-12-01-21:59:58-root-INFO: grad norm: 107.050 105.484 18.246
2024-12-01-21:59:59-root-INFO: grad norm: 97.099 94.847 20.790
2024-12-01-22:00:00-root-INFO: Loss Change: 1497.271 -> 1445.570
2024-12-01-22:00:00-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-01-22:00:00-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-01-22:00:00-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-22:00:00-root-INFO: step: 170 lr_xt 0.00482333
2024-12-01-22:00:00-root-INFO: grad norm: 196.895 194.606 29.938
2024-12-01-22:00:01-root-INFO: Loss too large (1448.780->1624.862)! Learning rate decreased to 0.00386.
2024-12-01-22:00:01-root-INFO: Loss too large (1448.780->1594.468)! Learning rate decreased to 0.00309.
2024-12-01-22:00:01-root-INFO: Loss too large (1448.780->1562.802)! Learning rate decreased to 0.00247.
2024-12-01-22:00:02-root-INFO: Loss too large (1448.780->1531.424)! Learning rate decreased to 0.00198.
2024-12-01-22:00:02-root-INFO: Loss too large (1448.780->1502.650)! Learning rate decreased to 0.00158.
2024-12-01-22:00:02-root-INFO: Loss too large (1448.780->1478.886)! Learning rate decreased to 0.00126.
2024-12-01-22:00:02-root-INFO: Loss too large (1448.780->1461.452)! Learning rate decreased to 0.00101.
2024-12-01-22:00:03-root-INFO: Loss too large (1448.780->1450.109)! Learning rate decreased to 0.00081.
2024-12-01-22:00:04-root-INFO: grad norm: 176.177 173.395 31.183
2024-12-01-22:00:04-root-INFO: grad norm: 160.807 158.876 24.845
2024-12-01-22:00:05-root-INFO: grad norm: 151.451 148.953 27.392
2024-12-01-22:00:06-root-INFO: grad norm: 144.507 142.763 22.381
2024-12-01-22:00:07-root-INFO: grad norm: 140.269 137.935 25.486
2024-12-01-22:00:08-root-INFO: grad norm: 137.457 135.814 21.188
2024-12-01-22:00:09-root-INFO: grad norm: 136.213 133.979 24.569
2024-12-01-22:00:09-root-INFO: Loss Change: 1448.780 -> 1422.102
2024-12-01-22:00:09-root-INFO: Regularization Change: 0.000 -> 0.144
2024-12-01-22:00:09-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-01-22:00:09-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-22:00:10-root-INFO: step: 169 lr_xt 0.00501730
2024-12-01-22:00:10-root-INFO: grad norm: 258.311 255.851 35.559
2024-12-01-22:00:10-root-INFO: Loss too large (1430.366->1642.067)! Learning rate decreased to 0.00401.
2024-12-01-22:00:11-root-INFO: Loss too large (1430.366->1616.835)! Learning rate decreased to 0.00321.
2024-12-01-22:00:11-root-INFO: Loss too large (1430.366->1587.666)! Learning rate decreased to 0.00257.
2024-12-01-22:00:11-root-INFO: Loss too large (1430.366->1555.277)! Learning rate decreased to 0.00206.
2024-12-01-22:00:11-root-INFO: Loss too large (1430.366->1521.171)! Learning rate decreased to 0.00164.
2024-12-01-22:00:12-root-INFO: Loss too large (1430.366->1488.046)! Learning rate decreased to 0.00132.
2024-12-01-22:00:12-root-INFO: Loss too large (1430.366->1459.503)! Learning rate decreased to 0.00105.
2024-12-01-22:00:12-root-INFO: Loss too large (1430.366->1438.267)! Learning rate decreased to 0.00084.
2024-12-01-22:00:13-root-INFO: grad norm: 236.231 233.252 37.399
2024-12-01-22:00:14-root-INFO: grad norm: 220.194 218.037 30.744
2024-12-01-22:00:15-root-INFO: grad norm: 209.925 207.259 33.355
2024-12-01-22:00:16-root-INFO: grad norm: 202.018 200.023 28.320
2024-12-01-22:00:17-root-INFO: grad norm: 196.550 194.055 31.218
2024-12-01-22:00:18-root-INFO: grad norm: 192.326 190.422 26.991
2024-12-01-22:00:19-root-INFO: grad norm: 189.437 187.045 30.007
2024-12-01-22:00:19-root-INFO: Loss Change: 1430.366 -> 1402.543
2024-12-01-22:00:19-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-01-22:00:19-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-01-22:00:19-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-22:00:20-root-INFO: step: 168 lr_xt 0.00521823
2024-12-01-22:00:20-root-INFO: grad norm: 311.423 308.954 39.134
2024-12-01-22:00:20-root-INFO: Loss too large (1418.759->1642.542)! Learning rate decreased to 0.00417.
2024-12-01-22:00:20-root-INFO: Loss too large (1418.759->1621.792)! Learning rate decreased to 0.00334.
2024-12-01-22:00:21-root-INFO: Loss too large (1418.759->1596.528)! Learning rate decreased to 0.00267.
2024-12-01-22:00:21-root-INFO: Loss too large (1418.759->1566.494)! Learning rate decreased to 0.00214.
2024-12-01-22:00:21-root-INFO: Loss too large (1418.759->1532.277)! Learning rate decreased to 0.00171.
2024-12-01-22:00:22-root-INFO: Loss too large (1418.759->1495.470)! Learning rate decreased to 0.00137.
2024-12-01-22:00:22-root-INFO: Loss too large (1418.759->1459.628)! Learning rate decreased to 0.00109.
2024-12-01-22:00:22-root-INFO: Loss too large (1418.759->1429.766)! Learning rate decreased to 0.00088.
2024-12-01-22:00:23-root-INFO: grad norm: 276.727 273.523 41.992
2024-12-01-22:00:24-root-INFO: grad norm: 257.274 255.103 33.350
2024-12-01-22:00:25-root-INFO: grad norm: 243.345 240.504 37.081
2024-12-01-22:00:26-root-INFO: grad norm: 233.546 231.524 30.663
2024-12-01-22:00:26-root-INFO: grad norm: 225.606 222.953 34.502
2024-12-01-22:00:27-root-INFO: grad norm: 219.625 217.692 29.073
2024-12-01-22:00:28-root-INFO: grad norm: 214.548 212.013 32.888
2024-12-01-22:00:29-root-INFO: Loss Change: 1418.759 -> 1385.997
2024-12-01-22:00:29-root-INFO: Regularization Change: 0.000 -> 0.134
2024-12-01-22:00:29-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-01-22:00:29-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-22:00:29-root-INFO: step: 167 lr_xt 0.00542633
2024-12-01-22:00:29-root-INFO: grad norm: 303.006 300.290 40.477
2024-12-01-22:00:30-root-INFO: Loss too large (1400.414->1626.348)! Learning rate decreased to 0.00434.
2024-12-01-22:00:30-root-INFO: Loss too large (1400.414->1603.472)! Learning rate decreased to 0.00347.
2024-12-01-22:00:30-root-INFO: Loss too large (1400.414->1576.507)! Learning rate decreased to 0.00278.
2024-12-01-22:00:31-root-INFO: Loss too large (1400.414->1544.996)! Learning rate decreased to 0.00222.
2024-12-01-22:00:31-root-INFO: Loss too large (1400.414->1509.514)! Learning rate decreased to 0.00178.
2024-12-01-22:00:31-root-INFO: Loss too large (1400.414->1471.874)! Learning rate decreased to 0.00142.
2024-12-01-22:00:32-root-INFO: Loss too large (1400.414->1435.997)! Learning rate decreased to 0.00114.
2024-12-01-22:00:32-root-INFO: Loss too large (1400.414->1406.971)! Learning rate decreased to 0.00091.
2024-12-01-22:00:33-root-INFO: grad norm: 258.972 256.011 39.050
2024-12-01-22:00:34-root-INFO: grad norm: 238.065 235.828 32.564
2024-12-01-22:00:35-root-INFO: grad norm: 223.673 221.072 34.013
2024-12-01-22:00:35-root-INFO: grad norm: 214.196 212.168 29.404
2024-12-01-22:00:36-root-INFO: grad norm: 206.315 203.905 31.446
2024-12-01-22:00:37-root-INFO: grad norm: 200.540 198.631 27.606
2024-12-01-22:00:38-root-INFO: grad norm: 195.379 193.093 29.802
2024-12-01-22:00:39-root-INFO: Loss Change: 1400.414 -> 1362.819
2024-12-01-22:00:39-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-01-22:00:39-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-01-22:00:39-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-22:00:39-root-INFO: step: 166 lr_xt 0.00564182
2024-12-01-22:00:39-root-INFO: grad norm: 280.137 277.686 36.980
2024-12-01-22:00:40-root-INFO: Loss too large (1375.575->1598.031)! Learning rate decreased to 0.00451.
2024-12-01-22:00:40-root-INFO: Loss too large (1375.575->1574.411)! Learning rate decreased to 0.00361.
2024-12-01-22:00:40-root-INFO: Loss too large (1375.575->1546.233)! Learning rate decreased to 0.00289.
2024-12-01-22:00:41-root-INFO: Loss too large (1375.575->1513.487)! Learning rate decreased to 0.00231.
2024-12-01-22:00:41-root-INFO: Loss too large (1375.575->1477.137)! Learning rate decreased to 0.00185.
2024-12-01-22:00:41-root-INFO: Loss too large (1375.575->1439.497)! Learning rate decreased to 0.00148.
2024-12-01-22:00:42-root-INFO: Loss too large (1375.575->1404.966)! Learning rate decreased to 0.00118.
2024-12-01-22:00:42-root-INFO: Loss too large (1375.575->1378.386)! Learning rate decreased to 0.00095.
2024-12-01-22:00:43-root-INFO: grad norm: 228.978 226.408 34.209
2024-12-01-22:00:44-root-INFO: grad norm: 207.204 205.215 28.638
2024-12-01-22:00:44-root-INFO: grad norm: 190.274 188.056 28.963
2024-12-01-22:00:45-root-INFO: grad norm: 179.888 178.103 25.276
2024-12-01-22:00:46-root-INFO: grad norm: 170.807 168.770 26.300
2024-12-01-22:00:47-root-INFO: grad norm: 164.652 162.981 23.398
2024-12-01-22:00:48-root-INFO: grad norm: 159.072 157.147 24.671
2024-12-01-22:00:49-root-INFO: Loss Change: 1375.575 -> 1337.528
2024-12-01-22:00:49-root-INFO: Regularization Change: 0.000 -> 0.165
2024-12-01-22:00:49-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-01-22:00:49-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-22:00:49-root-INFO: step: 165 lr_xt 0.00586491
2024-12-01-22:00:49-root-INFO: grad norm: 235.185 233.226 30.295
2024-12-01-22:00:49-root-INFO: Loss too large (1346.074->1563.111)! Learning rate decreased to 0.00469.
2024-12-01-22:00:50-root-INFO: Loss too large (1346.074->1539.297)! Learning rate decreased to 0.00375.
2024-12-01-22:00:50-root-INFO: Loss too large (1346.074->1510.585)! Learning rate decreased to 0.00300.
2024-12-01-22:00:50-root-INFO: Loss too large (1346.074->1477.573)! Learning rate decreased to 0.00240.
2024-12-01-22:00:51-root-INFO: Loss too large (1346.074->1441.660)! Learning rate decreased to 0.00192.
2024-12-01-22:00:51-root-INFO: Loss too large (1346.074->1405.866)! Learning rate decreased to 0.00154.
2024-12-01-22:00:51-root-INFO: Loss too large (1346.074->1374.820)! Learning rate decreased to 0.00123.
2024-12-01-22:00:51-root-INFO: Loss too large (1346.074->1352.201)! Learning rate decreased to 0.00098.
2024-12-01-22:00:52-root-INFO: grad norm: 217.073 214.571 32.863
2024-12-01-22:00:53-root-INFO: grad norm: 206.249 204.454 27.154
2024-12-01-22:00:54-root-INFO: grad norm: 194.475 192.234 29.434
2024-12-01-22:00:55-root-INFO: grad norm: 186.775 185.090 25.035
2024-12-01-22:00:56-root-INFO: grad norm: 178.447 176.378 27.097
2024-12-01-22:00:57-root-INFO: grad norm: 172.746 171.138 23.519
2024-12-01-22:00:57-root-INFO: grad norm: 166.648 164.698 25.419
2024-12-01-22:00:58-root-INFO: Loss Change: 1346.074 -> 1317.780
2024-12-01-22:00:58-root-INFO: Regularization Change: 0.000 -> 0.158
2024-12-01-22:00:58-root-INFO: Undo step: 165
2024-12-01-22:00:58-root-INFO: Undo step: 166
2024-12-01-22:00:58-root-INFO: Undo step: 167
2024-12-01-22:00:58-root-INFO: Undo step: 168
2024-12-01-22:00:58-root-INFO: Undo step: 169
2024-12-01-22:00:58-root-INFO: step: 170 lr_xt 0.00482333
2024-12-01-22:00:59-root-INFO: grad norm: 449.875 441.234 87.748
2024-12-01-22:01:00-root-INFO: grad norm: 391.202 387.152 56.144
2024-12-01-22:01:00-root-INFO: Loss too large (1921.306->1928.190)! Learning rate decreased to 0.00386.
2024-12-01-22:01:01-root-INFO: grad norm: 445.589 440.751 65.483
2024-12-01-22:01:02-root-INFO: grad norm: 339.448 334.723 56.439
2024-12-01-22:01:03-root-INFO: grad norm: 456.768 453.279 56.353
2024-12-01-22:01:03-root-INFO: Loss too large (1684.130->1690.194)! Learning rate decreased to 0.00309.
2024-12-01-22:01:04-root-INFO: grad norm: 230.344 226.546 41.656
2024-12-01-22:01:05-root-INFO: grad norm: 186.776 184.128 31.337
2024-12-01-22:01:05-root-INFO: grad norm: 114.687 112.599 21.784
2024-12-01-22:01:06-root-INFO: Loss Change: 2276.049 -> 1454.275
2024-12-01-22:01:06-root-INFO: Regularization Change: 0.000 -> 13.311
2024-12-01-22:01:06-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-01-22:01:06-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-22:01:06-root-INFO: step: 169 lr_xt 0.00501730
2024-12-01-22:01:07-root-INFO: grad norm: 95.893 92.694 24.565
2024-12-01-22:01:07-root-INFO: grad norm: 120.454 118.648 20.781
2024-12-01-22:01:08-root-INFO: Loss too large (1427.710->1448.704)! Learning rate decreased to 0.00401.
2024-12-01-22:01:08-root-INFO: Loss too large (1427.710->1428.508)! Learning rate decreased to 0.00321.
2024-12-01-22:01:09-root-INFO: grad norm: 183.016 179.961 33.303
2024-12-01-22:01:09-root-INFO: Loss too large (1418.233->1492.621)! Learning rate decreased to 0.00257.
2024-12-01-22:01:10-root-INFO: Loss too large (1418.233->1458.221)! Learning rate decreased to 0.00206.
2024-12-01-22:01:10-root-INFO: Loss too large (1418.233->1433.365)! Learning rate decreased to 0.00164.
2024-12-01-22:01:11-root-INFO: grad norm: 223.071 220.569 33.319
2024-12-01-22:01:11-root-INFO: Loss too large (1417.605->1424.893)! Learning rate decreased to 0.00132.
2024-12-01-22:01:12-root-INFO: grad norm: 201.253 198.880 30.817
2024-12-01-22:01:13-root-INFO: grad norm: 192.688 190.489 29.031
2024-12-01-22:01:14-root-INFO: grad norm: 189.833 187.510 29.604
2024-12-01-22:01:15-root-INFO: grad norm: 191.875 189.709 28.750
2024-12-01-22:01:15-root-INFO: Loss Change: 1447.139 -> 1392.551
2024-12-01-22:01:15-root-INFO: Regularization Change: 0.000 -> 1.044
2024-12-01-22:01:15-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-01-22:01:15-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-22:01:15-root-INFO: step: 168 lr_xt 0.00521823
2024-12-01-22:01:16-root-INFO: grad norm: 276.504 274.207 35.562
2024-12-01-22:01:16-root-INFO: Loss too large (1405.224->1606.772)! Learning rate decreased to 0.00417.
2024-12-01-22:01:16-root-INFO: Loss too large (1405.224->1575.778)! Learning rate decreased to 0.00334.
2024-12-01-22:01:17-root-INFO: Loss too large (1405.224->1539.205)! Learning rate decreased to 0.00267.
2024-12-01-22:01:17-root-INFO: Loss too large (1405.224->1498.764)! Learning rate decreased to 0.00214.
2024-12-01-22:01:17-root-INFO: Loss too large (1405.224->1457.908)! Learning rate decreased to 0.00171.
2024-12-01-22:01:18-root-INFO: Loss too large (1405.224->1422.172)! Learning rate decreased to 0.00137.
2024-12-01-22:01:18-root-INFO: grad norm: 259.459 256.714 37.647
2024-12-01-22:01:19-root-INFO: grad norm: 249.029 246.797 33.272
2024-12-01-22:01:20-root-INFO: grad norm: 242.316 239.774 35.009
2024-12-01-22:01:21-root-INFO: grad norm: 237.923 235.727 32.247
2024-12-01-22:01:22-root-INFO: grad norm: 235.087 232.629 33.908
2024-12-01-22:01:23-root-INFO: grad norm: 233.231 231.053 31.805
2024-12-01-22:01:24-root-INFO: grad norm: 232.201 229.779 33.451
2024-12-01-22:01:24-root-INFO: Loss Change: 1405.224 -> 1368.308
2024-12-01-22:01:24-root-INFO: Regularization Change: 0.000 -> 0.303
2024-12-01-22:01:24-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-01-22:01:24-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-22:01:25-root-INFO: step: 167 lr_xt 0.00542633
2024-12-01-22:01:25-root-INFO: grad norm: 285.793 283.337 37.388
2024-12-01-22:01:25-root-INFO: Loss too large (1379.297->1587.818)! Learning rate decreased to 0.00434.
2024-12-01-22:01:25-root-INFO: Loss too large (1379.297->1555.402)! Learning rate decreased to 0.00347.
2024-12-01-22:01:26-root-INFO: Loss too large (1379.297->1517.319)! Learning rate decreased to 0.00278.
2024-12-01-22:01:26-root-INFO: Loss too large (1379.297->1474.917)! Learning rate decreased to 0.00222.
2024-12-01-22:01:26-root-INFO: Loss too large (1379.297->1431.538)! Learning rate decreased to 0.00178.
2024-12-01-22:01:27-root-INFO: Loss too large (1379.297->1393.164)! Learning rate decreased to 0.00142.
2024-12-01-22:01:27-root-INFO: grad norm: 260.109 257.412 37.358
2024-12-01-22:01:28-root-INFO: grad norm: 247.854 245.670 32.827
2024-12-01-22:01:29-root-INFO: grad norm: 238.649 236.239 33.830
2024-12-01-22:01:30-root-INFO: grad norm: 232.478 230.366 31.262
2024-12-01-22:01:31-root-INFO: grad norm: 227.448 225.176 32.069
2024-12-01-22:01:32-root-INFO: grad norm: 224.067 221.991 30.429
2024-12-01-22:01:33-root-INFO: grad norm: 221.513 219.310 31.163
2024-12-01-22:01:34-root-INFO: Loss Change: 1379.297 -> 1336.103
2024-12-01-22:01:34-root-INFO: Regularization Change: 0.000 -> 0.324
2024-12-01-22:01:34-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-01-22:01:34-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-22:01:34-root-INFO: step: 166 lr_xt 0.00564182
2024-12-01-22:01:34-root-INFO: grad norm: 265.285 263.121 33.815
2024-12-01-22:01:35-root-INFO: Loss too large (1344.353->1548.175)! Learning rate decreased to 0.00451.
2024-12-01-22:01:35-root-INFO: Loss too large (1344.353->1514.331)! Learning rate decreased to 0.00361.
2024-12-01-22:01:35-root-INFO: Loss too large (1344.353->1474.789)! Learning rate decreased to 0.00289.
2024-12-01-22:01:35-root-INFO: Loss too large (1344.353->1431.500)! Learning rate decreased to 0.00231.
2024-12-01-22:01:36-root-INFO: Loss too large (1344.353->1388.623)! Learning rate decreased to 0.00185.
2024-12-01-22:01:36-root-INFO: Loss too large (1344.353->1352.502)! Learning rate decreased to 0.00148.
2024-12-01-22:01:37-root-INFO: grad norm: 231.120 228.818 32.537
2024-12-01-22:01:38-root-INFO: grad norm: 218.773 216.831 29.079
2024-12-01-22:01:39-root-INFO: grad norm: 209.790 207.729 29.331
2024-12-01-22:01:39-root-INFO: grad norm: 205.196 203.305 27.794
2024-12-01-22:01:40-root-INFO: grad norm: 201.966 199.995 28.146
2024-12-01-22:01:41-root-INFO: grad norm: 200.437 198.562 27.348
2024-12-01-22:01:42-root-INFO: grad norm: 199.865 197.923 27.794
2024-12-01-22:01:43-root-INFO: Loss Change: 1344.353 -> 1302.385
2024-12-01-22:01:43-root-INFO: Regularization Change: 0.000 -> 0.312
2024-12-01-22:01:43-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-01-22:01:43-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-22:01:43-root-INFO: step: 165 lr_xt 0.00586491
2024-12-01-22:01:43-root-INFO: grad norm: 245.209 243.319 30.388
2024-12-01-22:01:43-root-INFO: Loss too large (1312.049->1515.829)! Learning rate decreased to 0.00469.
2024-12-01-22:01:44-root-INFO: Loss too large (1312.049->1483.049)! Learning rate decreased to 0.00375.
2024-12-01-22:01:44-root-INFO: Loss too large (1312.049->1444.253)! Learning rate decreased to 0.00300.
2024-12-01-22:01:44-root-INFO: Loss too large (1312.049->1401.574)! Learning rate decreased to 0.00240.
2024-12-01-22:01:45-root-INFO: Loss too large (1312.049->1359.369)! Learning rate decreased to 0.00192.
2024-12-01-22:01:45-root-INFO: Loss too large (1312.049->1324.077)! Learning rate decreased to 0.00154.
2024-12-01-22:01:46-root-INFO: grad norm: 231.469 229.188 32.415
2024-12-01-22:01:47-root-INFO: grad norm: 223.894 222.068 28.533
2024-12-01-22:01:48-root-INFO: grad norm: 214.715 212.636 29.807
2024-12-01-22:01:48-root-INFO: grad norm: 209.645 207.850 27.373
2024-12-01-22:01:49-root-INFO: grad norm: 204.117 202.153 28.250
2024-12-01-22:01:50-root-INFO: grad norm: 201.117 199.338 26.691
2024-12-01-22:01:51-root-INFO: grad norm: 198.201 196.299 27.393
2024-12-01-22:01:52-root-INFO: Loss Change: 1312.049 -> 1277.125
2024-12-01-22:01:52-root-INFO: Regularization Change: 0.000 -> 0.303
2024-12-01-22:01:52-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-01-22:01:52-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-22:01:52-root-INFO: step: 164 lr_xt 0.00609585
2024-12-01-22:01:52-root-INFO: grad norm: 238.934 236.991 30.409
2024-12-01-22:01:52-root-INFO: Loss too large (1286.172->1488.929)! Learning rate decreased to 0.00488.
2024-12-01-22:01:53-root-INFO: Loss too large (1286.172->1454.034)! Learning rate decreased to 0.00390.
2024-12-01-22:01:53-root-INFO: Loss too large (1286.172->1413.271)! Learning rate decreased to 0.00312.
2024-12-01-22:01:53-root-INFO: Loss too large (1286.172->1369.078)! Learning rate decreased to 0.00250.
2024-12-01-22:01:54-root-INFO: Loss too large (1286.172->1326.365)! Learning rate decreased to 0.00200.
2024-12-01-22:01:54-root-INFO: Loss too large (1286.172->1291.900)! Learning rate decreased to 0.00160.
2024-12-01-22:01:55-root-INFO: grad norm: 209.367 207.256 29.653
2024-12-01-22:01:56-root-INFO: grad norm: 199.976 198.191 26.662
2024-12-01-22:01:57-root-INFO: grad norm: 190.926 189.064 26.598
2024-12-01-22:01:57-root-INFO: grad norm: 186.722 184.979 25.454
2024-12-01-22:01:58-root-INFO: grad norm: 182.797 181.041 25.278
2024-12-01-22:01:59-root-INFO: grad norm: 181.007 179.284 24.917
2024-12-01-22:02:00-root-INFO: grad norm: 179.729 178.016 24.758
2024-12-01-22:02:01-root-INFO: Loss Change: 1286.172 -> 1247.263
2024-12-01-22:02:01-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-01-22:02:01-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-01-22:02:01-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-22:02:01-root-INFO: step: 163 lr_xt 0.00633485
2024-12-01-22:02:01-root-INFO: grad norm: 212.157 210.426 27.048
2024-12-01-22:02:01-root-INFO: Loss too large (1252.351->1452.083)! Learning rate decreased to 0.00507.
2024-12-01-22:02:02-root-INFO: Loss too large (1252.351->1415.543)! Learning rate decreased to 0.00405.
2024-12-01-22:02:02-root-INFO: Loss too large (1252.351->1373.756)! Learning rate decreased to 0.00324.
2024-12-01-22:02:02-root-INFO: Loss too large (1252.351->1329.709)! Learning rate decreased to 0.00259.
2024-12-01-22:02:03-root-INFO: Loss too large (1252.351->1289.001)! Learning rate decreased to 0.00208.
2024-12-01-22:02:03-root-INFO: Loss too large (1252.351->1257.955)! Learning rate decreased to 0.00166.
2024-12-01-22:02:04-root-INFO: grad norm: 189.955 188.129 26.273
2024-12-01-22:02:05-root-INFO: grad norm: 183.207 181.598 24.232
2024-12-01-22:02:05-root-INFO: grad norm: 176.073 174.391 24.282
2024-12-01-22:02:06-root-INFO: Loss too large (1231.120->1231.154)! Learning rate decreased to 0.00133.
2024-12-01-22:02:07-root-INFO: grad norm: 122.198 120.652 19.378
2024-12-01-22:02:07-root-INFO: grad norm: 84.422 83.134 14.691
2024-12-01-22:02:08-root-INFO: grad norm: 68.301 66.645 14.951
2024-12-01-22:02:09-root-INFO: grad norm: 57.099 55.624 12.898
2024-12-01-22:02:10-root-INFO: Loss Change: 1252.351 -> 1209.178
2024-12-01-22:02:10-root-INFO: Regularization Change: 0.000 -> 0.265
2024-12-01-22:02:10-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-01-22:02:10-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-22:02:10-root-INFO: step: 162 lr_xt 0.00658217
2024-12-01-22:02:10-root-INFO: grad norm: 116.118 114.033 21.901
2024-12-01-22:02:11-root-INFO: Loss too large (1214.302->1346.081)! Learning rate decreased to 0.00527.
2024-12-01-22:02:11-root-INFO: Loss too large (1214.302->1306.163)! Learning rate decreased to 0.00421.
2024-12-01-22:02:11-root-INFO: Loss too large (1214.302->1270.917)! Learning rate decreased to 0.00337.
2024-12-01-22:02:12-root-INFO: Loss too large (1214.302->1243.999)! Learning rate decreased to 0.00270.
2024-12-01-22:02:12-root-INFO: Loss too large (1214.302->1226.100)! Learning rate decreased to 0.00216.
2024-12-01-22:02:12-root-INFO: Loss too large (1214.302->1215.539)! Learning rate decreased to 0.00173.
2024-12-01-22:02:13-root-INFO: grad norm: 128.453 126.946 19.616
2024-12-01-22:02:14-root-INFO: grad norm: 140.118 138.432 21.669
2024-12-01-22:02:15-root-INFO: grad norm: 157.368 155.866 21.693
2024-12-01-22:02:15-root-INFO: Loss too large (1206.137->1207.687)! Learning rate decreased to 0.00138.
2024-12-01-22:02:16-root-INFO: grad norm: 120.572 118.955 19.678
2024-12-01-22:02:17-root-INFO: grad norm: 88.726 87.485 14.785
2024-12-01-22:02:18-root-INFO: grad norm: 74.410 72.750 15.632
2024-12-01-22:02:18-root-INFO: grad norm: 62.741 61.407 12.869
2024-12-01-22:02:19-root-INFO: Loss Change: 1214.302 -> 1186.670
2024-12-01-22:02:19-root-INFO: Regularization Change: 0.000 -> 0.279
2024-12-01-22:02:19-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-01-22:02:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-22:02:19-root-INFO: step: 161 lr_xt 0.00683803
2024-12-01-22:02:20-root-INFO: grad norm: 98.882 97.400 17.057
2024-12-01-22:02:20-root-INFO: Loss too large (1186.429->1301.149)! Learning rate decreased to 0.00547.
2024-12-01-22:02:20-root-INFO: Loss too large (1186.429->1264.112)! Learning rate decreased to 0.00438.
2024-12-01-22:02:21-root-INFO: Loss too large (1186.429->1233.011)! Learning rate decreased to 0.00350.
2024-12-01-22:02:21-root-INFO: Loss too large (1186.429->1210.433)! Learning rate decreased to 0.00280.
2024-12-01-22:02:21-root-INFO: Loss too large (1186.429->1195.990)! Learning rate decreased to 0.00224.
2024-12-01-22:02:21-root-INFO: Loss too large (1186.429->1187.669)! Learning rate decreased to 0.00179.
2024-12-01-22:02:22-root-INFO: grad norm: 113.171 111.837 17.325
2024-12-01-22:02:23-root-INFO: grad norm: 124.618 123.264 18.322
2024-12-01-22:02:24-root-INFO: grad norm: 142.175 140.776 19.892
2024-12-01-22:02:24-root-INFO: Loss too large (1179.976->1181.454)! Learning rate decreased to 0.00143.
2024-12-01-22:02:25-root-INFO: grad norm: 110.644 109.298 17.203
2024-12-01-22:02:26-root-INFO: grad norm: 81.647 80.472 13.801
2024-12-01-22:02:27-root-INFO: grad norm: 69.037 67.625 13.892
2024-12-01-22:02:28-root-INFO: grad norm: 58.507 57.242 12.099
2024-12-01-22:02:29-root-INFO: Loss Change: 1186.429 -> 1162.565
2024-12-01-22:02:29-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-01-22:02:29-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-01-22:02:29-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-22:02:29-root-INFO: step: 160 lr_xt 0.00710269
2024-12-01-22:02:29-root-INFO: grad norm: 112.906 111.359 18.631
2024-12-01-22:02:30-root-INFO: Loss too large (1167.822->1312.878)! Learning rate decreased to 0.00568.
2024-12-01-22:02:30-root-INFO: Loss too large (1167.822->1272.186)! Learning rate decreased to 0.00455.
2024-12-01-22:02:30-root-INFO: Loss too large (1167.822->1234.025)! Learning rate decreased to 0.00364.
2024-12-01-22:02:30-root-INFO: Loss too large (1167.822->1203.542)! Learning rate decreased to 0.00291.
2024-12-01-22:02:31-root-INFO: Loss too large (1167.822->1182.833)! Learning rate decreased to 0.00233.
2024-12-01-22:02:31-root-INFO: Loss too large (1167.822->1170.550)! Learning rate decreased to 0.00186.
2024-12-01-22:02:32-root-INFO: grad norm: 128.522 127.217 18.269
2024-12-01-22:02:32-root-INFO: Loss too large (1164.099->1164.835)! Learning rate decreased to 0.00149.
2024-12-01-22:02:33-root-INFO: grad norm: 102.517 101.128 16.819
2024-12-01-22:02:34-root-INFO: grad norm: 77.373 76.212 13.355
2024-12-01-22:02:35-root-INFO: grad norm: 66.360 64.907 13.807
2024-12-01-22:02:36-root-INFO: grad norm: 56.786 55.536 11.851
2024-12-01-22:02:37-root-INFO: grad norm: 51.403 49.854 12.520
2024-12-01-22:02:38-root-INFO: grad norm: 47.036 45.667 11.267
2024-12-01-22:02:38-root-INFO: Loss Change: 1167.822 -> 1142.452
2024-12-01-22:02:38-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-01-22:02:38-root-INFO: Undo step: 160
2024-12-01-22:02:38-root-INFO: Undo step: 161
2024-12-01-22:02:38-root-INFO: Undo step: 162
2024-12-01-22:02:38-root-INFO: Undo step: 163
2024-12-01-22:02:38-root-INFO: Undo step: 164
2024-12-01-22:02:38-root-INFO: step: 165 lr_xt 0.00586491
2024-12-01-22:02:39-root-INFO: grad norm: 453.690 447.011 77.558
2024-12-01-22:02:40-root-INFO: grad norm: 389.927 382.577 75.354
2024-12-01-22:02:40-root-INFO: Loss too large (1732.882->2024.840)! Learning rate decreased to 0.00469.
2024-12-01-22:02:40-root-INFO: Loss too large (1732.882->1822.312)! Learning rate decreased to 0.00375.
2024-12-01-22:02:41-root-INFO: grad norm: 521.696 517.243 68.013
2024-12-01-22:02:41-root-INFO: Loss too large (1679.006->1685.350)! Learning rate decreased to 0.00300.
2024-12-01-22:02:42-root-INFO: grad norm: 259.864 254.599 52.041
2024-12-01-22:02:43-root-INFO: grad norm: 359.680 355.746 53.050
2024-12-01-22:02:44-root-INFO: Loss too large (1443.871->1552.376)! Learning rate decreased to 0.00240.
2024-12-01-22:02:44-root-INFO: Loss too large (1443.871->1475.789)! Learning rate decreased to 0.00192.
2024-12-01-22:02:45-root-INFO: grad norm: 359.020 355.690 48.782
2024-12-01-22:02:46-root-INFO: grad norm: 376.347 371.443 60.554
2024-12-01-22:02:46-root-INFO: grad norm: 356.267 353.000 48.137
2024-12-01-22:02:47-root-INFO: Loss Change: 2091.810 -> 1370.165
2024-12-01-22:02:47-root-INFO: Regularization Change: 0.000 -> 12.911
2024-12-01-22:02:47-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-01-22:02:47-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-22:02:47-root-INFO: step: 164 lr_xt 0.00609585
2024-12-01-22:02:48-root-INFO: grad norm: 340.440 335.854 55.691
2024-12-01-22:02:48-root-INFO: Loss too large (1353.696->1908.811)! Learning rate decreased to 0.00488.
2024-12-01-22:02:48-root-INFO: Loss too large (1353.696->1731.321)! Learning rate decreased to 0.00390.
2024-12-01-22:02:49-root-INFO: Loss too large (1353.696->1599.278)! Learning rate decreased to 0.00312.
2024-12-01-22:02:49-root-INFO: Loss too large (1353.696->1500.369)! Learning rate decreased to 0.00250.
2024-12-01-22:02:49-root-INFO: Loss too large (1353.696->1426.325)! Learning rate decreased to 0.00200.
2024-12-01-22:02:49-root-INFO: Loss too large (1353.696->1372.331)! Learning rate decreased to 0.00160.
2024-12-01-22:02:50-root-INFO: grad norm: 280.929 277.950 40.801
2024-12-01-22:02:51-root-INFO: grad norm: 253.175 249.713 41.726
2024-12-01-22:02:52-root-INFO: grad norm: 253.022 250.273 37.195
2024-12-01-22:02:53-root-INFO: grad norm: 263.364 260.018 41.845
2024-12-01-22:02:53-root-INFO: Loss too large (1285.070->1290.861)! Learning rate decreased to 0.00128.
2024-12-01-22:02:54-root-INFO: grad norm: 205.923 203.306 32.729
2024-12-01-22:02:55-root-INFO: grad norm: 156.555 154.024 28.037
2024-12-01-22:02:56-root-INFO: grad norm: 143.567 141.050 26.762
2024-12-01-22:02:56-root-INFO: Loss Change: 1353.696 -> 1242.494
2024-12-01-22:02:56-root-INFO: Regularization Change: 0.000 -> 0.778
2024-12-01-22:02:56-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-01-22:02:56-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-22:02:57-root-INFO: step: 163 lr_xt 0.00633485
2024-12-01-22:02:57-root-INFO: grad norm: 85.918 83.065 21.956
2024-12-01-22:02:57-root-INFO: Loss too large (1233.047->1255.355)! Learning rate decreased to 0.00507.
2024-12-01-22:02:57-root-INFO: Loss too large (1233.047->1242.552)! Learning rate decreased to 0.00405.
2024-12-01-22:02:58-root-INFO: Loss too large (1233.047->1234.536)! Learning rate decreased to 0.00324.
2024-12-01-22:02:59-root-INFO: grad norm: 178.497 176.292 27.975
2024-12-01-22:02:59-root-INFO: Loss too large (1229.901->1327.602)! Learning rate decreased to 0.00259.
2024-12-01-22:02:59-root-INFO: Loss too large (1229.901->1283.687)! Learning rate decreased to 0.00208.
2024-12-01-22:02:59-root-INFO: Loss too large (1229.901->1250.574)! Learning rate decreased to 0.00166.
2024-12-01-22:03:00-root-INFO: Loss too large (1229.901->1230.290)! Learning rate decreased to 0.00133.
2024-12-01-22:03:01-root-INFO: grad norm: 168.066 165.827 27.337
2024-12-01-22:03:01-root-INFO: grad norm: 164.871 162.661 26.905
2024-12-01-22:03:02-root-INFO: grad norm: 162.059 159.950 26.058
2024-12-01-22:03:03-root-INFO: grad norm: 161.437 159.265 26.391
2024-12-01-22:03:04-root-INFO: grad norm: 162.243 160.217 25.565
2024-12-01-22:03:05-root-INFO: grad norm: 162.941 160.810 26.265
2024-12-01-22:03:06-root-INFO: Loss Change: 1233.047 -> 1194.812
2024-12-01-22:03:06-root-INFO: Regularization Change: 0.000 -> 0.549
2024-12-01-22:03:06-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-01-22:03:06-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-22:03:06-root-INFO: step: 162 lr_xt 0.00658217
2024-12-01-22:03:06-root-INFO: grad norm: 86.128 83.690 20.347
2024-12-01-22:03:06-root-INFO: Loss too large (1187.152->1230.434)! Learning rate decreased to 0.00527.
2024-12-01-22:03:07-root-INFO: Loss too large (1187.152->1212.294)! Learning rate decreased to 0.00421.
2024-12-01-22:03:07-root-INFO: Loss too large (1187.152->1199.945)! Learning rate decreased to 0.00337.
2024-12-01-22:03:07-root-INFO: Loss too large (1187.152->1191.938)! Learning rate decreased to 0.00270.
2024-12-01-22:03:08-root-INFO: grad norm: 165.880 163.761 26.428
2024-12-01-22:03:08-root-INFO: Loss too large (1187.054->1246.770)! Learning rate decreased to 0.00216.
2024-12-01-22:03:09-root-INFO: Loss too large (1187.054->1212.195)! Learning rate decreased to 0.00173.
2024-12-01-22:03:09-root-INFO: Loss too large (1187.054->1191.007)! Learning rate decreased to 0.00138.
2024-12-01-22:03:10-root-INFO: grad norm: 175.700 173.774 25.947
2024-12-01-22:03:10-root-INFO: Loss too large (1180.058->1181.821)! Learning rate decreased to 0.00110.
2024-12-01-22:03:11-root-INFO: grad norm: 134.462 132.404 23.436
2024-12-01-22:03:12-root-INFO: grad norm: 91.687 89.956 17.735
2024-12-01-22:03:13-root-INFO: grad norm: 77.418 75.207 18.370
2024-12-01-22:03:14-root-INFO: grad norm: 65.205 63.248 15.856
2024-12-01-22:03:14-root-INFO: grad norm: 58.918 56.532 16.598
2024-12-01-22:03:15-root-INFO: Loss Change: 1187.152 -> 1156.413
2024-12-01-22:03:15-root-INFO: Regularization Change: 0.000 -> 0.317
2024-12-01-22:03:15-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-01-22:03:15-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-22:03:15-root-INFO: step: 161 lr_xt 0.00683803
2024-12-01-22:03:16-root-INFO: grad norm: 66.208 63.944 17.166
2024-12-01-22:03:16-root-INFO: Loss too large (1152.945->1189.896)! Learning rate decreased to 0.00547.
2024-12-01-22:03:16-root-INFO: Loss too large (1152.945->1171.271)! Learning rate decreased to 0.00438.
2024-12-01-22:03:16-root-INFO: Loss too large (1152.945->1160.151)! Learning rate decreased to 0.00350.
2024-12-01-22:03:17-root-INFO: Loss too large (1152.945->1154.006)! Learning rate decreased to 0.00280.
2024-12-01-22:03:18-root-INFO: grad norm: 153.863 152.120 23.094
2024-12-01-22:03:18-root-INFO: Loss too large (1150.885->1191.368)! Learning rate decreased to 0.00224.
2024-12-01-22:03:18-root-INFO: Loss too large (1150.885->1172.872)! Learning rate decreased to 0.00179.
2024-12-01-22:03:18-root-INFO: Loss too large (1150.885->1160.262)! Learning rate decreased to 0.00143.
2024-12-01-22:03:19-root-INFO: Loss too large (1150.885->1152.159)! Learning rate decreased to 0.00115.
2024-12-01-22:03:20-root-INFO: grad norm: 118.712 117.005 20.062
2024-12-01-22:03:21-root-INFO: grad norm: 81.246 79.632 16.117
2024-12-01-22:03:21-root-INFO: grad norm: 68.756 66.850 16.077
2024-12-01-22:03:22-root-INFO: grad norm: 58.139 56.301 14.503
2024-12-01-22:03:23-root-INFO: grad norm: 52.797 50.687 14.777
2024-12-01-22:03:24-root-INFO: grad norm: 48.788 46.763 13.911
2024-12-01-22:03:25-root-INFO: Loss Change: 1152.945 -> 1128.626
2024-12-01-22:03:25-root-INFO: Regularization Change: 0.000 -> 0.257
2024-12-01-22:03:25-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-01-22:03:25-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-22:03:25-root-INFO: step: 160 lr_xt 0.00710269
2024-12-01-22:03:25-root-INFO: grad norm: 108.218 106.376 19.881
2024-12-01-22:03:25-root-INFO: Loss too large (1131.843->1316.235)! Learning rate decreased to 0.00568.
2024-12-01-22:03:26-root-INFO: Loss too large (1131.843->1271.727)! Learning rate decreased to 0.00455.
2024-12-01-22:03:26-root-INFO: Loss too large (1131.843->1225.171)! Learning rate decreased to 0.00364.
2024-12-01-22:03:26-root-INFO: Loss too large (1131.843->1185.405)! Learning rate decreased to 0.00291.
2024-12-01-22:03:27-root-INFO: Loss too large (1131.843->1157.659)! Learning rate decreased to 0.00233.
2024-12-01-22:03:27-root-INFO: Loss too large (1131.843->1141.058)! Learning rate decreased to 0.00186.
2024-12-01-22:03:27-root-INFO: Loss too large (1131.843->1132.161)! Learning rate decreased to 0.00149.
2024-12-01-22:03:28-root-INFO: grad norm: 121.138 119.628 19.071
2024-12-01-22:03:29-root-INFO: grad norm: 127.036 125.384 20.420
2024-12-01-22:03:30-root-INFO: grad norm: 138.137 136.667 20.099
2024-12-01-22:03:30-root-INFO: Loss too large (1122.913->1123.914)! Learning rate decreased to 0.00119.
2024-12-01-22:03:31-root-INFO: grad norm: 107.134 105.529 18.474
2024-12-01-22:03:32-root-INFO: grad norm: 73.553 72.131 14.395
2024-12-01-22:03:33-root-INFO: grad norm: 62.426 60.648 14.793
2024-12-01-22:03:34-root-INFO: grad norm: 52.963 51.316 13.105
2024-12-01-22:03:34-root-INFO: Loss Change: 1131.843 -> 1107.614
2024-12-01-22:03:34-root-INFO: Regularization Change: 0.000 -> 0.217
2024-12-01-22:03:34-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-01-22:03:34-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-22:03:34-root-INFO: step: 159 lr_xt 0.00737641
2024-12-01-22:03:35-root-INFO: grad norm: 80.226 78.629 15.929
2024-12-01-22:03:35-root-INFO: Loss too large (1105.762->1229.732)! Learning rate decreased to 0.00590.
2024-12-01-22:03:35-root-INFO: Loss too large (1105.762->1186.374)! Learning rate decreased to 0.00472.
2024-12-01-22:03:36-root-INFO: Loss too large (1105.762->1151.962)! Learning rate decreased to 0.00378.
2024-12-01-22:03:36-root-INFO: Loss too large (1105.762->1128.974)! Learning rate decreased to 0.00302.
2024-12-01-22:03:36-root-INFO: Loss too large (1105.762->1115.354)! Learning rate decreased to 0.00242.
2024-12-01-22:03:37-root-INFO: Loss too large (1105.762->1107.926)! Learning rate decreased to 0.00193.
2024-12-01-22:03:37-root-INFO: grad norm: 127.703 126.267 19.097
2024-12-01-22:03:38-root-INFO: Loss too large (1104.180->1110.307)! Learning rate decreased to 0.00155.
2024-12-01-22:03:38-root-INFO: Loss too large (1104.180->1104.560)! Learning rate decreased to 0.00124.
2024-12-01-22:03:39-root-INFO: grad norm: 98.165 96.744 16.642
2024-12-01-22:03:40-root-INFO: grad norm: 66.819 65.392 13.738
2024-12-01-22:03:41-root-INFO: grad norm: 56.747 55.101 13.568
2024-12-01-22:03:42-root-INFO: grad norm: 48.647 46.998 12.561
2024-12-01-22:03:42-root-INFO: grad norm: 44.758 42.932 12.656
2024-12-01-22:03:43-root-INFO: grad norm: 42.053 40.259 12.152
2024-12-01-22:03:44-root-INFO: Loss Change: 1105.762 -> 1085.598
2024-12-01-22:03:44-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-01-22:03:44-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-01-22:03:44-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-22:03:44-root-INFO: step: 158 lr_xt 0.00765943
2024-12-01-22:03:44-root-INFO: grad norm: 82.600 80.832 16.997
2024-12-01-22:03:45-root-INFO: Loss too large (1086.585->1214.783)! Learning rate decreased to 0.00613.
2024-12-01-22:03:45-root-INFO: Loss too large (1086.585->1169.243)! Learning rate decreased to 0.00490.
2024-12-01-22:03:45-root-INFO: Loss too large (1086.585->1133.030)! Learning rate decreased to 0.00392.
2024-12-01-22:03:46-root-INFO: Loss too large (1086.585->1109.045)! Learning rate decreased to 0.00314.
2024-12-01-22:03:46-root-INFO: Loss too large (1086.585->1095.067)! Learning rate decreased to 0.00251.
2024-12-01-22:03:46-root-INFO: Loss too large (1086.585->1087.612)! Learning rate decreased to 0.00201.
2024-12-01-22:03:47-root-INFO: grad norm: 123.517 122.093 18.700
2024-12-01-22:03:47-root-INFO: Loss too large (1083.974->1088.855)! Learning rate decreased to 0.00161.
2024-12-01-22:03:48-root-INFO: grad norm: 120.171 118.771 18.291
2024-12-01-22:03:49-root-INFO: grad norm: 114.037 112.742 17.133
2024-12-01-22:03:50-root-INFO: grad norm: 111.821 110.481 17.262
2024-12-01-22:03:51-root-INFO: grad norm: 107.709 106.489 16.164
2024-12-01-22:03:52-root-INFO: grad norm: 106.096 104.791 16.587
2024-12-01-22:03:53-root-INFO: grad norm: 103.218 102.046 15.512
2024-12-01-22:03:53-root-INFO: Loss Change: 1086.585 -> 1066.499
2024-12-01-22:03:53-root-INFO: Regularization Change: 0.000 -> 0.297
2024-12-01-22:03:53-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-01-22:03:53-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-22:03:54-root-INFO: step: 157 lr_xt 0.00795203
2024-12-01-22:03:54-root-INFO: grad norm: 150.847 149.279 21.695
2024-12-01-22:03:54-root-INFO: Loss too large (1075.333->1320.770)! Learning rate decreased to 0.00636.
2024-12-01-22:03:54-root-INFO: Loss too large (1075.333->1285.472)! Learning rate decreased to 0.00509.
2024-12-01-22:03:55-root-INFO: Loss too large (1075.333->1237.005)! Learning rate decreased to 0.00407.
2024-12-01-22:03:55-root-INFO: Loss too large (1075.333->1178.704)! Learning rate decreased to 0.00326.
2024-12-01-22:03:55-root-INFO: Loss too large (1075.333->1124.195)! Learning rate decreased to 0.00261.
2024-12-01-22:03:56-root-INFO: Loss too large (1075.333->1087.278)! Learning rate decreased to 0.00208.
2024-12-01-22:03:57-root-INFO: grad norm: 202.745 201.028 26.330
2024-12-01-22:03:57-root-INFO: Loss too large (1068.250->1086.022)! Learning rate decreased to 0.00167.
2024-12-01-22:03:57-root-INFO: Loss too large (1068.250->1072.723)! Learning rate decreased to 0.00133.
2024-12-01-22:03:58-root-INFO: grad norm: 129.215 127.795 19.106
2024-12-01-22:03:59-root-INFO: grad norm: 50.104 48.523 12.489
2024-12-01-22:04:00-root-INFO: grad norm: 43.563 41.715 12.555
2024-12-01-22:04:01-root-INFO: grad norm: 39.821 38.045 11.761
2024-12-01-22:04:02-root-INFO: grad norm: 38.191 36.330 11.776
2024-12-01-22:04:02-root-INFO: grad norm: 37.252 35.462 11.407
2024-12-01-22:04:03-root-INFO: Loss Change: 1075.333 -> 1042.752
2024-12-01-22:04:03-root-INFO: Regularization Change: 0.000 -> 0.236
2024-12-01-22:04:03-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-01-22:04:03-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-22:04:03-root-INFO: step: 156 lr_xt 0.00825448
2024-12-01-22:04:04-root-INFO: grad norm: 68.802 67.364 13.992
2024-12-01-22:04:04-root-INFO: Loss too large (1042.820->1150.192)! Learning rate decreased to 0.00660.
2024-12-01-22:04:04-root-INFO: Loss too large (1042.820->1107.023)! Learning rate decreased to 0.00528.
2024-12-01-22:04:05-root-INFO: Loss too large (1042.820->1076.470)! Learning rate decreased to 0.00423.
2024-12-01-22:04:05-root-INFO: Loss too large (1042.820->1058.022)! Learning rate decreased to 0.00338.
2024-12-01-22:04:05-root-INFO: Loss too large (1042.820->1047.909)! Learning rate decreased to 0.00270.
2024-12-01-22:04:06-root-INFO: grad norm: 137.928 136.665 18.624
2024-12-01-22:04:06-root-INFO: Loss too large (1042.744->1059.298)! Learning rate decreased to 0.00216.
2024-12-01-22:04:06-root-INFO: Loss too large (1042.744->1049.370)! Learning rate decreased to 0.00173.
2024-12-01-22:04:07-root-INFO: Loss too large (1042.744->1042.862)! Learning rate decreased to 0.00138.
2024-12-01-22:04:08-root-INFO: grad norm: 89.440 88.232 14.648
2024-12-01-22:04:08-root-INFO: grad norm: 45.661 44.261 11.220
2024-12-01-22:04:09-root-INFO: grad norm: 39.648 38.037 11.188
2024-12-01-22:04:10-root-INFO: grad norm: 36.821 35.220 10.739
2024-12-01-22:04:11-root-INFO: grad norm: 35.809 34.164 10.730
2024-12-01-22:04:12-root-INFO: grad norm: 35.285 33.672 10.548
2024-12-01-22:04:12-root-INFO: Loss Change: 1042.820 -> 1024.279
2024-12-01-22:04:12-root-INFO: Regularization Change: 0.000 -> 0.212
2024-12-01-22:04:12-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-01-22:04:12-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-22:04:13-root-INFO: step: 155 lr_xt 0.00856705
2024-12-01-22:04:13-root-INFO: grad norm: 70.644 69.156 14.419
2024-12-01-22:04:13-root-INFO: Loss too large (1025.292->1139.426)! Learning rate decreased to 0.00685.
2024-12-01-22:04:13-root-INFO: Loss too large (1025.292->1093.396)! Learning rate decreased to 0.00548.
2024-12-01-22:04:14-root-INFO: Loss too large (1025.292->1060.506)! Learning rate decreased to 0.00439.
2024-12-01-22:04:14-root-INFO: Loss too large (1025.292->1040.729)! Learning rate decreased to 0.00351.
2024-12-01-22:04:14-root-INFO: Loss too large (1025.292->1030.017)! Learning rate decreased to 0.00281.
2024-12-01-22:04:15-root-INFO: grad norm: 137.038 135.765 18.635
2024-12-01-22:04:15-root-INFO: Loss too large (1024.639->1040.658)! Learning rate decreased to 0.00225.
2024-12-01-22:04:16-root-INFO: Loss too large (1024.639->1030.935)! Learning rate decreased to 0.00180.
2024-12-01-22:04:16-root-INFO: grad norm: 111.915 110.746 16.130
2024-12-01-22:04:17-root-INFO: grad norm: 70.155 69.055 12.373
2024-12-01-22:04:18-root-INFO: grad norm: 64.279 63.045 12.535
2024-12-01-22:04:19-root-INFO: grad norm: 56.634 55.510 11.229
2024-12-01-22:04:20-root-INFO: grad norm: 53.058 51.781 11.572
2024-12-01-22:04:21-root-INFO: grad norm: 48.833 47.669 10.599
2024-12-01-22:04:21-root-INFO: Loss Change: 1025.292 -> 1003.528
2024-12-01-22:04:21-root-INFO: Regularization Change: 0.000 -> 0.312
2024-12-01-22:04:21-root-INFO: Undo step: 155
2024-12-01-22:04:21-root-INFO: Undo step: 156
2024-12-01-22:04:21-root-INFO: Undo step: 157
2024-12-01-22:04:21-root-INFO: Undo step: 158
2024-12-01-22:04:21-root-INFO: Undo step: 159
2024-12-01-22:04:22-root-INFO: step: 160 lr_xt 0.00710269
2024-12-01-22:04:22-root-INFO: grad norm: 275.707 268.547 62.423
2024-12-01-22:04:23-root-INFO: grad norm: 485.774 479.754 76.241
2024-12-01-22:04:23-root-INFO: Loss too large (1474.326->2114.905)! Learning rate decreased to 0.00568.
2024-12-01-22:04:23-root-INFO: Loss too large (1474.326->1855.684)! Learning rate decreased to 0.00455.
2024-12-01-22:04:24-root-INFO: Loss too large (1474.326->1667.821)! Learning rate decreased to 0.00364.
2024-12-01-22:04:24-root-INFO: Loss too large (1474.326->1536.588)! Learning rate decreased to 0.00291.
2024-12-01-22:04:25-root-INFO: grad norm: 324.660 321.611 44.389
2024-12-01-22:04:26-root-INFO: grad norm: 105.811 100.946 31.714
2024-12-01-22:04:27-root-INFO: grad norm: 122.132 119.245 26.397
2024-12-01-22:04:27-root-INFO: Loss too large (1200.898->1205.415)! Learning rate decreased to 0.00233.
2024-12-01-22:04:28-root-INFO: grad norm: 172.229 169.468 30.711
2024-12-01-22:04:28-root-INFO: Loss too large (1199.297->1212.516)! Learning rate decreased to 0.00186.
2024-12-01-22:04:29-root-INFO: grad norm: 260.921 258.259 37.180
2024-12-01-22:04:29-root-INFO: Loss too large (1190.658->1215.879)! Learning rate decreased to 0.00149.
2024-12-01-22:04:29-root-INFO: Loss too large (1190.658->1197.849)! Learning rate decreased to 0.00119.
2024-12-01-22:04:30-root-INFO: grad norm: 173.959 171.473 29.305
2024-12-01-22:04:31-root-INFO: Loss Change: 1531.040 -> 1165.245
2024-12-01-22:04:31-root-INFO: Regularization Change: 0.000 -> 7.272
2024-12-01-22:04:31-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-01-22:04:31-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-22:04:31-root-INFO: step: 159 lr_xt 0.00737641
2024-12-01-22:04:31-root-INFO: grad norm: 69.598 66.691 19.905
2024-12-01-22:04:32-root-INFO: grad norm: 163.709 161.687 25.651
2024-12-01-22:04:32-root-INFO: Loss too large (1137.362->1337.660)! Learning rate decreased to 0.00590.
2024-12-01-22:04:33-root-INFO: Loss too large (1137.362->1274.752)! Learning rate decreased to 0.00472.
2024-12-01-22:04:33-root-INFO: Loss too large (1137.362->1229.327)! Learning rate decreased to 0.00378.
2024-12-01-22:04:33-root-INFO: Loss too large (1137.362->1196.225)! Learning rate decreased to 0.00302.
2024-12-01-22:04:34-root-INFO: Loss too large (1137.362->1172.220)! Learning rate decreased to 0.00242.
2024-12-01-22:04:34-root-INFO: Loss too large (1137.362->1155.183)! Learning rate decreased to 0.00193.
2024-12-01-22:04:34-root-INFO: Loss too large (1137.362->1143.548)! Learning rate decreased to 0.00155.
2024-12-01-22:04:35-root-INFO: grad norm: 148.378 146.533 23.320
2024-12-01-22:04:36-root-INFO: grad norm: 124.296 122.640 20.224
2024-12-01-22:04:37-root-INFO: grad norm: 122.156 120.361 20.865
2024-12-01-22:04:38-root-INFO: grad norm: 118.893 117.326 19.241
2024-12-01-22:04:38-root-INFO: grad norm: 118.087 116.375 20.036
2024-12-01-22:04:39-root-INFO: grad norm: 117.189 115.677 18.765
2024-12-01-22:04:40-root-INFO: Loss Change: 1158.916 -> 1105.145
2024-12-01-22:04:40-root-INFO: Regularization Change: 0.000 -> 1.097
2024-12-01-22:04:40-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-01-22:04:40-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-22:04:40-root-INFO: step: 158 lr_xt 0.00765943
2024-12-01-22:04:41-root-INFO: grad norm: 149.419 147.622 23.102
2024-12-01-22:04:41-root-INFO: Loss too large (1107.561->1344.710)! Learning rate decreased to 0.00613.
2024-12-01-22:04:41-root-INFO: Loss too large (1107.561->1307.778)! Learning rate decreased to 0.00490.
2024-12-01-22:04:41-root-INFO: Loss too large (1107.561->1259.731)! Learning rate decreased to 0.00392.
2024-12-01-22:04:42-root-INFO: Loss too large (1107.561->1202.347)! Learning rate decreased to 0.00314.
2024-12-01-22:04:42-root-INFO: Loss too large (1107.561->1149.749)! Learning rate decreased to 0.00251.
2024-12-01-22:04:42-root-INFO: Loss too large (1107.561->1115.789)! Learning rate decreased to 0.00201.
2024-12-01-22:04:43-root-INFO: grad norm: 198.020 196.268 26.287
2024-12-01-22:04:43-root-INFO: Loss too large (1099.088->1113.519)! Learning rate decreased to 0.00161.
2024-12-01-22:04:44-root-INFO: Loss too large (1099.088->1101.786)! Learning rate decreased to 0.00129.
2024-12-01-22:04:45-root-INFO: grad norm: 128.302 126.661 20.457
2024-12-01-22:04:45-root-INFO: grad norm: 56.620 54.864 13.992
2024-12-01-22:04:46-root-INFO: grad norm: 51.306 49.296 14.221
2024-12-01-22:04:47-root-INFO: grad norm: 48.339 46.412 13.513
2024-12-01-22:04:48-root-INFO: grad norm: 46.949 44.948 13.559
2024-12-01-22:04:49-root-INFO: grad norm: 46.041 44.100 13.228
2024-12-01-22:04:49-root-INFO: Loss Change: 1107.561 -> 1068.430
2024-12-01-22:04:49-root-INFO: Regularization Change: 0.000 -> 0.313
2024-12-01-22:04:49-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-01-22:04:49-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-22:04:50-root-INFO: step: 157 lr_xt 0.00795203
2024-12-01-22:04:50-root-INFO: grad norm: 100.082 98.121 19.718
2024-12-01-22:04:50-root-INFO: Loss too large (1070.031->1254.014)! Learning rate decreased to 0.00636.
2024-12-01-22:04:51-root-INFO: Loss too large (1070.031->1204.301)! Learning rate decreased to 0.00509.
2024-12-01-22:04:51-root-INFO: Loss too large (1070.031->1152.145)! Learning rate decreased to 0.00407.
2024-12-01-22:04:51-root-INFO: Loss too large (1070.031->1110.797)! Learning rate decreased to 0.00326.
2024-12-01-22:04:51-root-INFO: Loss too large (1070.031->1085.303)! Learning rate decreased to 0.00261.
2024-12-01-22:04:52-root-INFO: Loss too large (1070.031->1071.931)! Learning rate decreased to 0.00208.
2024-12-01-22:04:52-root-INFO: grad norm: 136.108 134.694 19.568
2024-12-01-22:04:53-root-INFO: Loss too large (1065.694->1070.025)! Learning rate decreased to 0.00167.
2024-12-01-22:04:54-root-INFO: grad norm: 117.133 115.438 19.853
2024-12-01-22:04:54-root-INFO: grad norm: 85.353 84.011 15.078
2024-12-01-22:04:55-root-INFO: grad norm: 79.668 77.979 16.321
2024-12-01-22:04:56-root-INFO: grad norm: 71.778 70.432 13.837
2024-12-01-22:04:57-root-INFO: grad norm: 68.345 66.681 14.992
2024-12-01-22:04:58-root-INFO: grad norm: 63.957 62.607 13.070
2024-12-01-22:04:59-root-INFO: Loss Change: 1070.031 -> 1039.373
2024-12-01-22:04:59-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-01-22:04:59-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-01-22:04:59-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-22:04:59-root-INFO: step: 156 lr_xt 0.00825448
2024-12-01-22:04:59-root-INFO: grad norm: 94.893 93.335 17.127
2024-12-01-22:04:59-root-INFO: Loss too large (1039.910->1227.638)! Learning rate decreased to 0.00660.
2024-12-01-22:05:00-root-INFO: Loss too large (1039.910->1176.000)! Learning rate decreased to 0.00528.
2024-12-01-22:05:00-root-INFO: Loss too large (1039.910->1121.560)! Learning rate decreased to 0.00423.
2024-12-01-22:05:00-root-INFO: Loss too large (1039.910->1079.334)! Learning rate decreased to 0.00338.
2024-12-01-22:05:01-root-INFO: Loss too large (1039.910->1054.172)! Learning rate decreased to 0.00270.
2024-12-01-22:05:01-root-INFO: Loss too large (1039.910->1041.373)! Learning rate decreased to 0.00216.
2024-12-01-22:05:02-root-INFO: grad norm: 117.585 116.362 16.913
2024-12-01-22:05:02-root-INFO: Loss too large (1035.557->1037.932)! Learning rate decreased to 0.00173.
2024-12-01-22:05:03-root-INFO: grad norm: 97.501 96.101 16.463
2024-12-01-22:05:04-root-INFO: grad norm: 67.560 66.318 12.896
2024-12-01-22:05:05-root-INFO: grad norm: 61.274 59.792 13.397
2024-12-01-22:05:06-root-INFO: grad norm: 53.882 52.548 11.918
2024-12-01-22:05:07-root-INFO: grad norm: 50.372 48.837 12.340
2024-12-01-22:05:08-root-INFO: grad norm: 46.672 45.266 11.372
2024-12-01-22:05:08-root-INFO: Loss Change: 1039.910 -> 1013.328
2024-12-01-22:05:08-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-01-22:05:08-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-01-22:05:08-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-22:05:09-root-INFO: step: 155 lr_xt 0.00856705
2024-12-01-22:05:09-root-INFO: grad norm: 85.642 84.076 16.302
2024-12-01-22:05:09-root-INFO: Loss too large (1014.764->1184.733)! Learning rate decreased to 0.00685.
2024-12-01-22:05:09-root-INFO: Loss too large (1014.764->1129.115)! Learning rate decreased to 0.00548.
2024-12-01-22:05:10-root-INFO: Loss too large (1014.764->1077.908)! Learning rate decreased to 0.00439.
2024-12-01-22:05:10-root-INFO: Loss too large (1014.764->1043.120)! Learning rate decreased to 0.00351.
2024-12-01-22:05:10-root-INFO: Loss too large (1014.764->1023.910)! Learning rate decreased to 0.00281.
2024-12-01-22:05:11-root-INFO: grad norm: 163.004 161.575 21.536
2024-12-01-22:05:12-root-INFO: Loss too large (1014.474->1034.626)! Learning rate decreased to 0.00225.
2024-12-01-22:05:12-root-INFO: Loss too large (1014.474->1022.592)! Learning rate decreased to 0.00180.
2024-12-01-22:05:13-root-INFO: grad norm: 117.647 116.320 17.619
2024-12-01-22:05:14-root-INFO: grad norm: 49.861 48.484 11.636
2024-12-01-22:05:14-root-INFO: grad norm: 45.575 43.984 11.934
2024-12-01-22:05:15-root-INFO: grad norm: 41.618 40.144 10.976
2024-12-01-22:05:16-root-INFO: grad norm: 39.473 37.853 11.191
2024-12-01-22:05:17-root-INFO: grad norm: 37.666 36.148 10.583
2024-12-01-22:05:18-root-INFO: Loss Change: 1014.764 -> 989.652
2024-12-01-22:05:18-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-01-22:05:18-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-01-22:05:18-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-22:05:18-root-INFO: step: 154 lr_xt 0.00889002
2024-12-01-22:05:18-root-INFO: grad norm: 71.254 69.737 14.626
2024-12-01-22:05:19-root-INFO: Loss too large (989.930->1112.446)! Learning rate decreased to 0.00711.
2024-12-01-22:05:19-root-INFO: Loss too large (989.930->1060.163)! Learning rate decreased to 0.00569.
2024-12-01-22:05:19-root-INFO: Loss too large (989.930->1023.672)! Learning rate decreased to 0.00455.
2024-12-01-22:05:20-root-INFO: Loss too large (989.930->1003.001)! Learning rate decreased to 0.00364.
2024-12-01-22:05:20-root-INFO: Loss too large (989.930->992.517)! Learning rate decreased to 0.00291.
2024-12-01-22:05:21-root-INFO: grad norm: 113.540 112.437 15.785
2024-12-01-22:05:21-root-INFO: Loss too large (987.610->994.700)! Learning rate decreased to 0.00233.
2024-12-01-22:05:21-root-INFO: Loss too large (987.610->988.738)! Learning rate decreased to 0.00186.
2024-12-01-22:05:22-root-INFO: grad norm: 82.570 81.308 14.385
2024-12-01-22:05:23-root-INFO: grad norm: 46.134 44.835 10.872
2024-12-01-22:05:24-root-INFO: grad norm: 40.714 39.170 11.104
2024-12-01-22:05:25-root-INFO: grad norm: 36.841 35.369 10.312
2024-12-01-22:05:26-root-INFO: grad norm: 35.152 33.573 10.417
2024-12-01-22:05:27-root-INFO: grad norm: 34.066 32.555 10.031
2024-12-01-22:05:27-root-INFO: Loss Change: 989.930 -> 967.488
2024-12-01-22:05:27-root-INFO: Regularization Change: 0.000 -> 0.322
2024-12-01-22:05:27-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-01-22:05:27-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-22:05:28-root-INFO: step: 153 lr_xt 0.00922367
2024-12-01-22:05:28-root-INFO: grad norm: 64.517 62.937 14.193
2024-12-01-22:05:28-root-INFO: Loss too large (968.184->1051.432)! Learning rate decreased to 0.00738.
2024-12-01-22:05:29-root-INFO: Loss too large (968.184->1009.733)! Learning rate decreased to 0.00590.
2024-12-01-22:05:29-root-INFO: Loss too large (968.184->985.383)! Learning rate decreased to 0.00472.
2024-12-01-22:05:29-root-INFO: Loss too large (968.184->972.818)! Learning rate decreased to 0.00378.
2024-12-01-22:05:30-root-INFO: grad norm: 127.842 126.653 17.392
2024-12-01-22:05:30-root-INFO: Loss too large (966.809->985.249)! Learning rate decreased to 0.00302.
2024-12-01-22:05:31-root-INFO: Loss too large (966.809->974.749)! Learning rate decreased to 0.00242.
2024-12-01-22:05:31-root-INFO: Loss too large (966.809->967.687)! Learning rate decreased to 0.00193.
2024-12-01-22:05:32-root-INFO: grad norm: 81.214 80.007 13.945
2024-12-01-22:05:33-root-INFO: grad norm: 36.769 35.257 10.435
2024-12-01-22:05:34-root-INFO: grad norm: 34.247 32.626 10.411
2024-12-01-22:05:35-root-INFO: grad norm: 33.053 31.494 10.031
2024-12-01-22:05:35-root-INFO: grad norm: 32.475 30.914 9.946
2024-12-01-22:05:36-root-INFO: grad norm: 32.082 30.566 9.745
2024-12-01-22:05:37-root-INFO: Loss Change: 968.184 -> 945.561
2024-12-01-22:05:37-root-INFO: Regularization Change: 0.000 -> 0.358
2024-12-01-22:05:37-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-01-22:05:37-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-22:05:37-root-INFO: step: 152 lr_xt 0.00956831
2024-12-01-22:05:37-root-INFO: grad norm: 48.272 46.856 11.608
2024-12-01-22:05:38-root-INFO: Loss too large (944.844->972.243)! Learning rate decreased to 0.00765.
2024-12-01-22:05:38-root-INFO: Loss too large (944.844->955.916)! Learning rate decreased to 0.00612.
2024-12-01-22:05:38-root-INFO: Loss too large (944.844->947.586)! Learning rate decreased to 0.00490.
2024-12-01-22:05:39-root-INFO: grad norm: 110.743 109.728 14.964
2024-12-01-22:05:39-root-INFO: Loss too large (943.591->967.286)! Learning rate decreased to 0.00392.
2024-12-01-22:05:40-root-INFO: Loss too large (943.591->955.691)! Learning rate decreased to 0.00314.
2024-12-01-22:05:40-root-INFO: Loss too large (943.591->947.751)! Learning rate decreased to 0.00251.
2024-12-01-22:05:41-root-INFO: grad norm: 84.093 83.051 13.197
2024-12-01-22:05:42-root-INFO: grad norm: 44.187 43.100 9.738
2024-12-01-22:05:43-root-INFO: grad norm: 40.070 38.802 10.000
2024-12-01-22:05:43-root-INFO: grad norm: 36.122 34.927 9.213
2024-12-01-22:05:44-root-INFO: grad norm: 34.202 32.881 9.413
2024-12-01-22:05:45-root-INFO: grad norm: 32.553 31.306 8.923
2024-12-01-22:05:46-root-INFO: Loss Change: 944.844 -> 921.552
2024-12-01-22:05:46-root-INFO: Regularization Change: 0.000 -> 0.504
2024-12-01-22:05:46-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-01-22:05:46-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-22:05:46-root-INFO: step: 151 lr_xt 0.00992422
2024-12-01-22:05:46-root-INFO: grad norm: 59.293 57.949 12.553
2024-12-01-22:05:47-root-INFO: Loss too large (922.932->984.722)! Learning rate decreased to 0.00794.
2024-12-01-22:05:47-root-INFO: Loss too large (922.932->950.169)! Learning rate decreased to 0.00635.
2024-12-01-22:05:47-root-INFO: Loss too large (922.932->932.020)! Learning rate decreased to 0.00508.
2024-12-01-22:05:47-root-INFO: Loss too large (922.932->923.297)! Learning rate decreased to 0.00406.
2024-12-01-22:05:48-root-INFO: grad norm: 86.984 86.066 12.605
2024-12-01-22:05:49-root-INFO: Loss too large (919.431->923.722)! Learning rate decreased to 0.00325.
2024-12-01-22:05:49-root-INFO: grad norm: 78.157 77.123 12.669
2024-12-01-22:05:50-root-INFO: grad norm: 59.543 58.672 10.143
2024-12-01-22:05:51-root-INFO: grad norm: 57.332 56.289 10.885
2024-12-01-22:05:52-root-INFO: grad norm: 53.617 52.768 9.506
2024-12-01-22:05:53-root-INFO: grad norm: 52.365 51.345 10.287
2024-12-01-22:05:54-root-INFO: grad norm: 50.402 49.571 9.113
2024-12-01-22:05:55-root-INFO: Loss Change: 922.932 -> 897.228
2024-12-01-22:05:55-root-INFO: Regularization Change: 0.000 -> 0.665
2024-12-01-22:05:55-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-01-22:05:55-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-22:05:55-root-INFO: step: 150 lr_xt 0.01029171
2024-12-01-22:05:55-root-INFO: grad norm: 61.933 60.873 11.407
2024-12-01-22:05:55-root-INFO: Loss too large (898.101->980.452)! Learning rate decreased to 0.00823.
2024-12-01-22:05:56-root-INFO: Loss too large (898.101->935.289)! Learning rate decreased to 0.00659.
2024-12-01-22:05:56-root-INFO: Loss too large (898.101->910.840)! Learning rate decreased to 0.00527.
2024-12-01-22:05:56-root-INFO: Loss too large (898.101->899.294)! Learning rate decreased to 0.00422.
2024-12-01-22:05:57-root-INFO: grad norm: 82.713 81.920 11.432
2024-12-01-22:05:58-root-INFO: Loss too large (894.304->897.370)! Learning rate decreased to 0.00337.
2024-12-01-22:05:58-root-INFO: grad norm: 69.765 68.835 11.356
2024-12-01-22:05:59-root-INFO: grad norm: 47.000 46.184 8.724
2024-12-01-22:06:00-root-INFO: grad norm: 43.854 42.850 9.330
2024-12-01-22:06:01-root-INFO: grad norm: 39.779 38.923 8.207
2024-12-01-22:06:02-root-INFO: grad norm: 38.049 37.030 8.747
2024-12-01-22:06:03-root-INFO: grad norm: 35.961 35.083 7.898
2024-12-01-22:06:03-root-INFO: Loss Change: 898.101 -> 872.653
2024-12-01-22:06:03-root-INFO: Regularization Change: 0.000 -> 0.626
2024-12-01-22:06:03-root-INFO: Undo step: 150
2024-12-01-22:06:03-root-INFO: Undo step: 151
2024-12-01-22:06:03-root-INFO: Undo step: 152
2024-12-01-22:06:03-root-INFO: Undo step: 153
2024-12-01-22:06:03-root-INFO: Undo step: 154
2024-12-01-22:06:04-root-INFO: step: 155 lr_xt 0.00856705
2024-12-01-22:06:04-root-INFO: grad norm: 291.415 283.515 67.392
2024-12-01-22:06:05-root-INFO: grad norm: 295.894 293.336 38.827
2024-12-01-22:06:06-root-INFO: grad norm: 165.380 163.433 25.305
2024-12-01-22:06:06-root-INFO: Loss too large (1149.292->1295.218)! Learning rate decreased to 0.00685.
2024-12-01-22:06:06-root-INFO: Loss too large (1149.292->1234.420)! Learning rate decreased to 0.00548.
2024-12-01-22:06:07-root-INFO: grad norm: 556.245 552.070 68.026
2024-12-01-22:06:07-root-INFO: Loss too large (1145.378->1682.959)! Learning rate decreased to 0.00439.
2024-12-01-22:06:08-root-INFO: Loss too large (1145.378->1477.518)! Learning rate decreased to 0.00351.
2024-12-01-22:06:08-root-INFO: Loss too large (1145.378->1337.590)! Learning rate decreased to 0.00281.
2024-12-01-22:06:08-root-INFO: Loss too large (1145.378->1243.881)! Learning rate decreased to 0.00225.
2024-12-01-22:06:08-root-INFO: Loss too large (1145.378->1181.005)! Learning rate decreased to 0.00180.
2024-12-01-22:06:09-root-INFO: grad norm: 182.050 180.734 21.846
2024-12-01-22:06:10-root-INFO: grad norm: 142.537 141.156 19.791
2024-12-01-22:06:11-root-INFO: grad norm: 99.964 98.344 17.926
2024-12-01-22:06:12-root-INFO: grad norm: 62.007 59.695 16.774
2024-12-01-22:06:12-root-INFO: Loss Change: 1644.979 -> 1033.534
2024-12-01-22:06:12-root-INFO: Regularization Change: 0.000 -> 12.402
2024-12-01-22:06:12-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-01-22:06:12-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-22:06:13-root-INFO: step: 154 lr_xt 0.00889002
2024-12-01-22:06:13-root-INFO: grad norm: 69.042 67.209 15.805
2024-12-01-22:06:13-root-INFO: Loss too large (1028.874->1030.558)! Learning rate decreased to 0.00711.
2024-12-01-22:06:14-root-INFO: grad norm: 219.659 217.716 29.147
2024-12-01-22:06:14-root-INFO: Loss too large (1020.555->1203.759)! Learning rate decreased to 0.00569.
2024-12-01-22:06:15-root-INFO: Loss too large (1020.555->1137.348)! Learning rate decreased to 0.00455.
2024-12-01-22:06:15-root-INFO: Loss too large (1020.555->1091.623)! Learning rate decreased to 0.00364.
2024-12-01-22:06:15-root-INFO: Loss too large (1020.555->1059.686)! Learning rate decreased to 0.00291.
2024-12-01-22:06:16-root-INFO: Loss too large (1020.555->1037.326)! Learning rate decreased to 0.00233.
2024-12-01-22:06:16-root-INFO: Loss too large (1020.555->1021.959)! Learning rate decreased to 0.00186.
2024-12-01-22:06:17-root-INFO: grad norm: 112.626 111.397 16.592
2024-12-01-22:06:18-root-INFO: grad norm: 53.515 51.535 14.421
2024-12-01-22:06:18-root-INFO: grad norm: 48.806 46.661 14.310
2024-12-01-22:06:19-root-INFO: grad norm: 47.244 45.121 14.003
2024-12-01-22:06:20-root-INFO: grad norm: 46.052 43.928 13.822
2024-12-01-22:06:21-root-INFO: grad norm: 44.991 42.886 13.601
2024-12-01-22:06:22-root-INFO: Loss Change: 1028.874 -> 973.981
2024-12-01-22:06:22-root-INFO: Regularization Change: 0.000 -> 1.095
2024-12-01-22:06:22-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-01-22:06:22-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-22:06:22-root-INFO: step: 153 lr_xt 0.00922367
2024-12-01-22:06:22-root-INFO: grad norm: 59.539 57.755 14.466
2024-12-01-22:06:23-root-INFO: Loss too large (971.542->989.916)! Learning rate decreased to 0.00738.
2024-12-01-22:06:23-root-INFO: Loss too large (971.542->975.288)! Learning rate decreased to 0.00590.
2024-12-01-22:06:24-root-INFO: grad norm: 154.704 153.119 22.084
2024-12-01-22:06:24-root-INFO: Loss too large (968.335->1035.331)! Learning rate decreased to 0.00472.
2024-12-01-22:06:25-root-INFO: Loss too large (968.335->1007.574)! Learning rate decreased to 0.00378.
2024-12-01-22:06:25-root-INFO: Loss too large (968.335->988.076)! Learning rate decreased to 0.00302.
2024-12-01-22:06:25-root-INFO: Loss too large (968.335->974.666)! Learning rate decreased to 0.00242.
2024-12-01-22:06:26-root-INFO: grad norm: 105.812 104.739 15.031
2024-12-01-22:06:27-root-INFO: grad norm: 44.462 42.615 12.682
2024-12-01-22:06:28-root-INFO: grad norm: 41.294 39.491 12.069
2024-12-01-22:06:29-root-INFO: grad norm: 39.203 37.333 11.963
2024-12-01-22:06:30-root-INFO: grad norm: 37.950 36.136 11.592
2024-12-01-22:06:30-root-INFO: grad norm: 36.989 35.171 11.452
2024-12-01-22:06:31-root-INFO: Loss Change: 971.542 -> 932.899
2024-12-01-22:06:31-root-INFO: Regularization Change: 0.000 -> 0.856
2024-12-01-22:06:31-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-01-22:06:31-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-22:06:31-root-INFO: step: 152 lr_xt 0.00956831
2024-12-01-22:06:32-root-INFO: grad norm: 48.567 47.178 11.533
2024-12-01-22:06:32-root-INFO: Loss too large (930.375->943.662)! Learning rate decreased to 0.00765.
2024-12-01-22:06:32-root-INFO: Loss too large (930.375->933.476)! Learning rate decreased to 0.00612.
2024-12-01-22:06:33-root-INFO: grad norm: 123.845 122.553 17.846
2024-12-01-22:06:33-root-INFO: Loss too large (928.594->974.694)! Learning rate decreased to 0.00490.
2024-12-01-22:06:34-root-INFO: Loss too large (928.594->954.630)! Learning rate decreased to 0.00392.
2024-12-01-22:06:34-root-INFO: Loss too large (928.594->940.704)! Learning rate decreased to 0.00314.
2024-12-01-22:06:34-root-INFO: Loss too large (928.594->931.357)! Learning rate decreased to 0.00251.
2024-12-01-22:06:35-root-INFO: grad norm: 85.525 84.604 12.514
2024-12-01-22:06:36-root-INFO: grad norm: 40.975 39.534 10.768
2024-12-01-22:06:37-root-INFO: grad norm: 36.713 35.289 10.128
2024-12-01-22:06:38-root-INFO: grad norm: 34.181 32.668 10.056
2024-12-01-22:06:39-root-INFO: grad norm: 33.015 31.544 9.744
2024-12-01-22:06:39-root-INFO: grad norm: 32.242 30.765 9.646
2024-12-01-22:06:40-root-INFO: Loss Change: 930.375 -> 902.038
2024-12-01-22:06:40-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-01-22:06:40-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-01-22:06:40-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-22:06:40-root-INFO: step: 151 lr_xt 0.00992422
2024-12-01-22:06:41-root-INFO: grad norm: 53.234 51.994 11.422
2024-12-01-22:06:41-root-INFO: Loss too large (902.628->932.312)! Learning rate decreased to 0.00794.
2024-12-01-22:06:41-root-INFO: Loss too large (902.628->913.732)! Learning rate decreased to 0.00635.
2024-12-01-22:06:41-root-INFO: Loss too large (902.628->904.482)! Learning rate decreased to 0.00508.
2024-12-01-22:06:42-root-INFO: grad norm: 98.869 97.764 14.742
2024-12-01-22:06:43-root-INFO: Loss too large (900.154->914.367)! Learning rate decreased to 0.00406.
2024-12-01-22:06:43-root-INFO: Loss too large (900.154->905.318)! Learning rate decreased to 0.00325.
2024-12-01-22:06:44-root-INFO: grad norm: 83.665 82.817 11.882
2024-12-01-22:06:45-root-INFO: grad norm: 55.754 54.732 10.626
2024-12-01-22:06:46-root-INFO: grad norm: 51.025 50.081 9.770
2024-12-01-22:06:47-root-INFO: grad norm: 45.395 44.370 9.592
2024-12-01-22:06:48-root-INFO: grad norm: 43.124 42.152 9.103
2024-12-01-22:06:49-root-INFO: grad norm: 40.667 39.653 9.025
2024-12-01-22:06:49-root-INFO: Loss Change: 902.628 -> 874.770
2024-12-01-22:06:49-root-INFO: Regularization Change: 0.000 -> 0.746
2024-12-01-22:06:49-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-01-22:06:49-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-22:06:50-root-INFO: step: 150 lr_xt 0.01029171
2024-12-01-22:06:50-root-INFO: grad norm: 51.020 50.157 9.346
2024-12-01-22:06:50-root-INFO: Loss too large (874.234->910.741)! Learning rate decreased to 0.00823.
2024-12-01-22:06:50-root-INFO: Loss too large (874.234->888.969)! Learning rate decreased to 0.00659.
2024-12-01-22:06:51-root-INFO: Loss too large (874.234->878.124)! Learning rate decreased to 0.00527.
2024-12-01-22:06:52-root-INFO: grad norm: 96.148 95.216 13.356
2024-12-01-22:06:52-root-INFO: Loss too large (872.995->886.504)! Learning rate decreased to 0.00422.
2024-12-01-22:06:52-root-INFO: Loss too large (872.995->877.731)! Learning rate decreased to 0.00337.
2024-12-01-22:06:53-root-INFO: grad norm: 77.239 76.507 10.607
2024-12-01-22:06:54-root-INFO: grad norm: 46.429 45.583 8.823
2024-12-01-22:06:55-root-INFO: grad norm: 41.495 40.656 8.305
2024-12-01-22:06:56-root-INFO: grad norm: 36.593 35.702 8.023
2024-12-01-22:06:56-root-INFO: grad norm: 34.453 33.563 7.780
2024-12-01-22:06:57-root-INFO: grad norm: 32.436 31.527 7.625
2024-12-01-22:06:58-root-INFO: Loss Change: 874.234 -> 849.990
2024-12-01-22:06:58-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-01-22:06:58-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-01-22:06:58-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-22:06:58-root-INFO: step: 149 lr_xt 0.01067108
2024-12-01-22:06:58-root-INFO: grad norm: 53.765 52.895 9.629
2024-12-01-22:06:59-root-INFO: Loss too large (851.156->892.430)! Learning rate decreased to 0.00854.
2024-12-01-22:06:59-root-INFO: Loss too large (851.156->867.527)! Learning rate decreased to 0.00683.
2024-12-01-22:06:59-root-INFO: Loss too large (851.156->855.255)! Learning rate decreased to 0.00546.
2024-12-01-22:07:00-root-INFO: grad norm: 92.584 91.679 12.912
2024-12-01-22:07:01-root-INFO: Loss too large (849.501->860.318)! Learning rate decreased to 0.00437.
2024-12-01-22:07:01-root-INFO: Loss too large (849.501->852.430)! Learning rate decreased to 0.00350.
2024-12-01-22:07:02-root-INFO: grad norm: 68.748 68.047 9.797
2024-12-01-22:07:03-root-INFO: grad norm: 36.992 36.155 7.826
2024-12-01-22:07:03-root-INFO: grad norm: 32.332 31.458 7.467
2024-12-01-22:07:04-root-INFO: grad norm: 28.986 28.083 7.180
2024-12-01-22:07:05-root-INFO: grad norm: 27.520 26.600 7.057
2024-12-01-22:07:06-root-INFO: grad norm: 26.492 25.577 6.902
2024-12-01-22:07:07-root-INFO: Loss Change: 851.156 -> 827.293
2024-12-01-22:07:07-root-INFO: Regularization Change: 0.000 -> 0.644
2024-12-01-22:07:07-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-01-22:07:07-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-22:07:07-root-INFO: step: 148 lr_xt 0.01106266
2024-12-01-22:07:07-root-INFO: grad norm: 40.317 39.421 8.454
2024-12-01-22:07:07-root-INFO: Loss too large (827.196->834.208)! Learning rate decreased to 0.00885.
2024-12-01-22:07:08-root-INFO: Loss too large (827.196->827.704)! Learning rate decreased to 0.00708.
2024-12-01-22:07:09-root-INFO: grad norm: 75.010 74.249 10.656
2024-12-01-22:07:09-root-INFO: Loss too large (824.704->837.113)! Learning rate decreased to 0.00566.
2024-12-01-22:07:09-root-INFO: Loss too large (824.704->829.351)! Learning rate decreased to 0.00453.
2024-12-01-22:07:10-root-INFO: grad norm: 67.980 67.306 9.548
2024-12-01-22:07:11-root-INFO: grad norm: 52.564 51.896 8.351
2024-12-01-22:07:12-root-INFO: grad norm: 49.242 48.560 8.166
2024-12-01-22:07:13-root-INFO: grad norm: 44.719 44.067 7.607
2024-12-01-22:07:14-root-INFO: grad norm: 43.221 42.538 7.651
2024-12-01-22:07:14-root-INFO: grad norm: 41.398 40.757 7.256
2024-12-01-22:07:15-root-INFO: Loss Change: 827.196 -> 802.652
2024-12-01-22:07:15-root-INFO: Regularization Change: 0.000 -> 0.961
2024-12-01-22:07:15-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-01-22:07:15-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-22:07:15-root-INFO: step: 147 lr_xt 0.01146675
2024-12-01-22:07:16-root-INFO: grad norm: 50.618 49.923 8.358
2024-12-01-22:07:16-root-INFO: Loss too large (802.224->830.405)! Learning rate decreased to 0.00917.
2024-12-01-22:07:16-root-INFO: Loss too large (802.224->811.398)! Learning rate decreased to 0.00734.
2024-12-01-22:07:16-root-INFO: Loss too large (802.224->802.465)! Learning rate decreased to 0.00587.
2024-12-01-22:07:17-root-INFO: grad norm: 63.188 62.536 9.057
2024-12-01-22:07:18-root-INFO: Loss too large (798.521->799.482)! Learning rate decreased to 0.00470.
2024-12-01-22:07:19-root-INFO: grad norm: 53.408 52.768 8.243
2024-12-01-22:07:19-root-INFO: grad norm: 39.183 38.572 6.889
2024-12-01-22:07:20-root-INFO: grad norm: 35.415 34.731 6.923
2024-12-01-22:07:21-root-INFO: grad norm: 31.590 30.951 6.325
2024-12-01-22:07:22-root-INFO: grad norm: 29.862 29.156 6.456
2024-12-01-22:07:23-root-INFO: grad norm: 28.157 27.498 6.055
2024-12-01-22:07:24-root-INFO: Loss Change: 802.224 -> 777.301
2024-12-01-22:07:24-root-INFO: Regularization Change: 0.000 -> 0.861
2024-12-01-22:07:24-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-01-22:07:24-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-22:07:24-root-INFO: step: 146 lr_xt 0.01188369
2024-12-01-22:07:24-root-INFO: grad norm: 40.180 39.555 7.063
2024-12-01-22:07:24-root-INFO: Loss too large (777.235->786.554)! Learning rate decreased to 0.00951.
2024-12-01-22:07:25-root-INFO: Loss too large (777.235->778.674)! Learning rate decreased to 0.00761.
2024-12-01-22:07:26-root-INFO: grad norm: 66.037 65.390 9.222
2024-12-01-22:07:26-root-INFO: Loss too large (775.032->781.695)! Learning rate decreased to 0.00608.
2024-12-01-22:07:26-root-INFO: Loss too large (775.032->776.050)! Learning rate decreased to 0.00487.
2024-12-01-22:07:27-root-INFO: grad norm: 51.305 50.736 7.619
2024-12-01-22:07:28-root-INFO: grad norm: 32.408 31.811 6.192
2024-12-01-22:07:29-root-INFO: grad norm: 28.375 27.713 6.093
2024-12-01-22:07:30-root-INFO: grad norm: 25.305 24.648 5.726
2024-12-01-22:07:31-root-INFO: grad norm: 23.941 23.240 5.749
2024-12-01-22:07:31-root-INFO: grad norm: 22.919 22.238 5.548
2024-12-01-22:07:32-root-INFO: Loss Change: 777.235 -> 754.342
2024-12-01-22:07:32-root-INFO: Regularization Change: 0.000 -> 0.888
2024-12-01-22:07:32-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-01-22:07:32-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-22:07:32-root-INFO: step: 145 lr_xt 0.01231381
2024-12-01-22:07:33-root-INFO: grad norm: 36.557 35.807 7.368
2024-12-01-22:07:33-root-INFO: Loss too large (754.938->757.069)! Learning rate decreased to 0.00985.
2024-12-01-22:07:34-root-INFO: grad norm: 70.817 70.136 9.793
2024-12-01-22:07:34-root-INFO: Loss too large (753.118->770.419)! Learning rate decreased to 0.00788.
2024-12-01-22:07:34-root-INFO: Loss too large (753.118->760.269)! Learning rate decreased to 0.00630.
2024-12-01-22:07:35-root-INFO: Loss too large (753.118->753.858)! Learning rate decreased to 0.00504.
2024-12-01-22:07:36-root-INFO: grad norm: 50.106 49.531 7.565
2024-12-01-22:07:36-root-INFO: grad norm: 27.637 27.037 5.726
2024-12-01-22:07:37-root-INFO: grad norm: 24.095 23.406 5.721
2024-12-01-22:07:38-root-INFO: grad norm: 22.126 21.455 5.408
2024-12-01-22:07:39-root-INFO: grad norm: 21.317 20.610 5.443
2024-12-01-22:07:40-root-INFO: grad norm: 20.823 20.137 5.301
2024-12-01-22:07:40-root-INFO: Loss Change: 754.938 -> 732.277
2024-12-01-22:07:40-root-INFO: Regularization Change: 0.000 -> 0.962
2024-12-01-22:07:40-root-INFO: Undo step: 145
2024-12-01-22:07:40-root-INFO: Undo step: 146
2024-12-01-22:07:40-root-INFO: Undo step: 147
2024-12-01-22:07:40-root-INFO: Undo step: 148
2024-12-01-22:07:40-root-INFO: Undo step: 149
2024-12-01-22:07:41-root-INFO: step: 150 lr_xt 0.01029171
2024-12-01-22:07:41-root-INFO: grad norm: 248.384 244.933 41.263
2024-12-01-22:07:42-root-INFO: grad norm: 146.758 144.756 24.154
2024-12-01-22:07:42-root-INFO: Loss too large (1050.376->1095.920)! Learning rate decreased to 0.00823.
2024-12-01-22:07:43-root-INFO: grad norm: 420.406 417.141 52.289
2024-12-01-22:07:43-root-INFO: Loss too large (1040.343->1473.171)! Learning rate decreased to 0.00659.
2024-12-01-22:07:44-root-INFO: Loss too large (1040.343->1279.582)! Learning rate decreased to 0.00527.
2024-12-01-22:07:44-root-INFO: Loss too large (1040.343->1152.012)! Learning rate decreased to 0.00422.
2024-12-01-22:07:44-root-INFO: Loss too large (1040.343->1068.226)! Learning rate decreased to 0.00337.
2024-12-01-22:07:45-root-INFO: grad norm: 175.059 174.023 19.015
2024-12-01-22:07:46-root-INFO: grad norm: 110.015 108.807 16.254
2024-12-01-22:07:47-root-INFO: grad norm: 67.947 66.439 14.237
2024-12-01-22:07:48-root-INFO: grad norm: 58.994 57.434 13.478
2024-12-01-22:07:49-root-INFO: grad norm: 54.247 52.744 12.681
2024-12-01-22:07:49-root-INFO: Loss Change: 1291.919 -> 862.722
2024-12-01-22:07:49-root-INFO: Regularization Change: 0.000 -> 11.865
2024-12-01-22:07:49-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-01-22:07:49-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-22:07:50-root-INFO: step: 149 lr_xt 0.01067108
2024-12-01-22:07:50-root-INFO: grad norm: 56.515 55.065 12.719
2024-12-01-22:07:51-root-INFO: grad norm: 105.600 104.504 15.177
2024-12-01-22:07:51-root-INFO: Loss too large (843.150->896.593)! Learning rate decreased to 0.00854.
2024-12-01-22:07:51-root-INFO: Loss too large (843.150->871.680)! Learning rate decreased to 0.00683.
2024-12-01-22:07:52-root-INFO: Loss too large (843.150->855.261)! Learning rate decreased to 0.00546.
2024-12-01-22:07:52-root-INFO: Loss too large (843.150->844.641)! Learning rate decreased to 0.00437.
2024-12-01-22:07:53-root-INFO: grad norm: 79.744 78.864 11.818
2024-12-01-22:07:54-root-INFO: grad norm: 44.180 42.973 10.255
2024-12-01-22:07:54-root-INFO: grad norm: 40.007 38.846 9.569
2024-12-01-22:07:55-root-INFO: grad norm: 36.673 35.475 9.297
2024-12-01-22:07:56-root-INFO: grad norm: 34.672 33.513 8.891
2024-12-01-22:07:57-root-INFO: grad norm: 32.999 31.838 8.675
2024-12-01-22:07:58-root-INFO: Loss Change: 860.509 -> 796.841
2024-12-01-22:07:58-root-INFO: Regularization Change: 0.000 -> 2.542
2024-12-01-22:07:58-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-01-22:07:58-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-22:07:58-root-INFO: step: 148 lr_xt 0.01106266
2024-12-01-22:07:58-root-INFO: grad norm: 42.452 41.371 9.519
2024-12-01-22:07:59-root-INFO: grad norm: 108.872 108.007 13.694
2024-12-01-22:07:59-root-INFO: Loss too large (793.094->860.218)! Learning rate decreased to 0.00885.
2024-12-01-22:08:00-root-INFO: Loss too large (793.094->830.584)! Learning rate decreased to 0.00708.
2024-12-01-22:08:00-root-INFO: Loss too large (793.094->811.091)! Learning rate decreased to 0.00566.
2024-12-01-22:08:00-root-INFO: Loss too large (793.094->798.422)! Learning rate decreased to 0.00453.
2024-12-01-22:08:01-root-INFO: grad norm: 72.927 72.216 10.156
2024-12-01-22:08:02-root-INFO: grad norm: 29.990 28.938 7.874
2024-12-01-22:08:03-root-INFO: grad norm: 27.935 26.865 7.658
2024-12-01-22:08:04-root-INFO: grad norm: 26.776 25.713 7.468
2024-12-01-22:08:05-root-INFO: grad norm: 26.049 24.998 7.324
2024-12-01-22:08:06-root-INFO: grad norm: 25.478 24.445 7.181
2024-12-01-22:08:06-root-INFO: Loss Change: 795.636 -> 760.684
2024-12-01-22:08:06-root-INFO: Regularization Change: 0.000 -> 1.449
2024-12-01-22:08:06-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-01-22:08:06-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-22:08:06-root-INFO: step: 147 lr_xt 0.01146675
2024-12-01-22:08:07-root-INFO: grad norm: 32.219 31.210 7.999
2024-12-01-22:08:08-root-INFO: grad norm: 52.798 52.145 8.276
2024-12-01-22:08:08-root-INFO: Loss too large (753.442->764.023)! Learning rate decreased to 0.00917.
2024-12-01-22:08:08-root-INFO: Loss too large (753.442->756.873)! Learning rate decreased to 0.00734.
2024-12-01-22:08:09-root-INFO: grad norm: 58.907 58.236 8.860
2024-12-01-22:08:10-root-INFO: grad norm: 79.123 78.462 10.209
2024-12-01-22:08:10-root-INFO: Loss too large (748.454->755.118)! Learning rate decreased to 0.00587.
2024-12-01-22:08:11-root-INFO: grad norm: 64.458 63.855 8.798
2024-12-01-22:08:12-root-INFO: grad norm: 38.481 37.844 6.976
2024-12-01-22:08:13-root-INFO: grad norm: 34.495 33.805 6.865
2024-12-01-22:08:14-root-INFO: grad norm: 30.504 29.825 6.403
2024-12-01-22:08:14-root-INFO: Loss Change: 758.709 -> 728.582
2024-12-01-22:08:14-root-INFO: Regularization Change: 0.000 -> 1.676
2024-12-01-22:08:14-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-01-22:08:14-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-22:08:15-root-INFO: step: 146 lr_xt 0.01188369
2024-12-01-22:08:15-root-INFO: grad norm: 40.561 39.912 7.226
2024-12-01-22:08:15-root-INFO: Loss too large (728.464->731.994)! Learning rate decreased to 0.00951.
2024-12-01-22:08:16-root-INFO: grad norm: 67.736 67.133 9.022
2024-12-01-22:08:16-root-INFO: Loss too large (726.641->736.661)! Learning rate decreased to 0.00761.
2024-12-01-22:08:17-root-INFO: Loss too large (726.641->728.838)! Learning rate decreased to 0.00608.
2024-12-01-22:08:17-root-INFO: grad norm: 51.224 50.662 7.566
2024-12-01-22:08:18-root-INFO: grad norm: 29.906 29.266 6.152
2024-12-01-22:08:19-root-INFO: grad norm: 25.805 25.093 6.018
2024-12-01-22:08:20-root-INFO: grad norm: 22.789 22.065 5.699
2024-12-01-22:08:21-root-INFO: grad norm: 21.461 20.696 5.676
2024-12-01-22:08:22-root-INFO: grad norm: 20.502 19.751 5.500
2024-12-01-22:08:22-root-INFO: Loss Change: 728.464 -> 704.080
2024-12-01-22:08:22-root-INFO: Regularization Change: 0.000 -> 1.157
2024-12-01-22:08:22-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-01-22:08:22-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-22:08:22-root-INFO: step: 145 lr_xt 0.01231381
2024-12-01-22:08:23-root-INFO: grad norm: 35.150 34.394 7.255
2024-12-01-22:08:24-root-INFO: grad norm: 61.869 61.295 8.409
2024-12-01-22:08:24-root-INFO: Loss too large (703.301->720.257)! Learning rate decreased to 0.00985.
2024-12-01-22:08:24-root-INFO: Loss too large (703.301->708.666)! Learning rate decreased to 0.00788.
2024-12-01-22:08:25-root-INFO: grad norm: 57.530 56.934 8.258
2024-12-01-22:08:26-root-INFO: grad norm: 50.697 50.158 7.378
2024-12-01-22:08:27-root-INFO: grad norm: 48.176 47.584 7.533
2024-12-01-22:08:28-root-INFO: grad norm: 44.650 44.129 6.800
2024-12-01-22:08:29-root-INFO: grad norm: 43.044 42.456 7.092
2024-12-01-22:08:30-root-INFO: grad norm: 41.092 40.584 6.439
2024-12-01-22:08:30-root-INFO: Loss Change: 704.655 -> 681.318
2024-12-01-22:08:30-root-INFO: Regularization Change: 0.000 -> 1.648
2024-12-01-22:08:30-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-01-22:08:30-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-22:08:30-root-INFO: step: 144 lr_xt 0.01275743
2024-12-01-22:08:31-root-INFO: grad norm: 49.406 48.777 7.859
2024-12-01-22:08:31-root-INFO: Loss too large (682.034->688.137)! Learning rate decreased to 0.01021.
2024-12-01-22:08:32-root-INFO: grad norm: 61.082 60.491 8.472
2024-12-01-22:08:32-root-INFO: Loss too large (680.622->682.716)! Learning rate decreased to 0.00816.
2024-12-01-22:08:33-root-INFO: grad norm: 50.441 49.850 7.695
2024-12-01-22:08:34-root-INFO: grad norm: 40.341 39.808 6.534
2024-12-01-22:08:35-root-INFO: grad norm: 35.973 35.377 6.522
2024-12-01-22:08:35-root-INFO: grad norm: 32.582 32.047 5.881
2024-12-01-22:08:36-root-INFO: grad norm: 30.498 29.898 6.020
2024-12-01-22:08:37-root-INFO: grad norm: 28.973 28.433 5.567
2024-12-01-22:08:38-root-INFO: Loss Change: 682.034 -> 657.011
2024-12-01-22:08:38-root-INFO: Regularization Change: 0.000 -> 1.427
2024-12-01-22:08:38-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-01-22:08:38-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-22:08:38-root-INFO: step: 143 lr_xt 0.01321490
2024-12-01-22:08:38-root-INFO: grad norm: 39.719 39.123 6.859
2024-12-01-22:08:38-root-INFO: Loss too large (656.793->659.763)! Learning rate decreased to 0.01057.
2024-12-01-22:08:39-root-INFO: grad norm: 46.159 45.569 7.361
2024-12-01-22:08:40-root-INFO: grad norm: 60.447 59.775 8.990
2024-12-01-22:08:40-root-INFO: Loss too large (654.184->657.806)! Learning rate decreased to 0.00846.
2024-12-01-22:08:41-root-INFO: grad norm: 47.094 46.472 7.626
2024-12-01-22:08:42-root-INFO: grad norm: 30.237 29.672 5.814
2024-12-01-22:08:43-root-INFO: grad norm: 27.469 26.938 5.376
2024-12-01-22:08:44-root-INFO: grad norm: 25.801 25.233 5.382
2024-12-01-22:08:45-root-INFO: grad norm: 24.978 24.442 5.147
2024-12-01-22:08:45-root-INFO: Loss Change: 656.793 -> 635.040
2024-12-01-22:08:45-root-INFO: Regularization Change: 0.000 -> 1.428
2024-12-01-22:08:45-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-01-22:08:45-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-22:08:46-root-INFO: step: 142 lr_xt 0.01368658
2024-12-01-22:08:46-root-INFO: grad norm: 38.165 37.498 7.102
2024-12-01-22:08:46-root-INFO: Loss too large (635.392->636.846)! Learning rate decreased to 0.01095.
2024-12-01-22:08:47-root-INFO: grad norm: 41.948 41.328 7.183
2024-12-01-22:08:48-root-INFO: grad norm: 56.150 55.499 8.527
2024-12-01-22:08:48-root-INFO: Loss too large (631.684->635.348)! Learning rate decreased to 0.00876.
2024-12-01-22:08:49-root-INFO: grad norm: 44.206 43.552 7.575
2024-12-01-22:08:50-root-INFO: grad norm: 27.636 27.086 5.488
2024-12-01-22:08:51-root-INFO: grad norm: 25.872 25.322 5.307
2024-12-01-22:08:52-root-INFO: grad norm: 24.989 24.448 5.172
2024-12-01-22:08:53-root-INFO: grad norm: 24.766 24.224 5.152
2024-12-01-22:08:53-root-INFO: Loss Change: 635.392 -> 614.403
2024-12-01-22:08:53-root-INFO: Regularization Change: 0.000 -> 1.422
2024-12-01-22:08:53-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-01-22:08:53-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-22:08:53-root-INFO: step: 141 lr_xt 0.01417280
2024-12-01-22:08:54-root-INFO: grad norm: 35.156 34.609 6.177
2024-12-01-22:08:54-root-INFO: Loss too large (614.493->617.017)! Learning rate decreased to 0.01134.
2024-12-01-22:08:55-root-INFO: grad norm: 39.493 38.879 6.938
2024-12-01-22:08:56-root-INFO: grad norm: 50.724 50.119 7.814
2024-12-01-22:08:56-root-INFO: Loss too large (611.382->614.498)! Learning rate decreased to 0.00907.
2024-12-01-22:08:57-root-INFO: grad norm: 40.539 39.908 7.125
2024-12-01-22:08:58-root-INFO: grad norm: 26.013 25.510 5.091
2024-12-01-22:08:58-root-INFO: grad norm: 24.615 24.093 5.043
2024-12-01-22:08:59-root-INFO: grad norm: 23.774 23.275 4.846
2024-12-01-22:09:00-root-INFO: grad norm: 23.531 23.020 4.877
2024-12-01-22:09:01-root-INFO: Loss Change: 614.493 -> 595.912
2024-12-01-22:09:01-root-INFO: Regularization Change: 0.000 -> 1.334
2024-12-01-22:09:01-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-01-22:09:01-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-22:09:01-root-INFO: step: 140 lr_xt 0.01467393
2024-12-01-22:09:01-root-INFO: grad norm: 32.869 32.333 5.911
2024-12-01-22:09:02-root-INFO: Loss too large (595.885->597.547)! Learning rate decreased to 0.01174.
2024-12-01-22:09:03-root-INFO: grad norm: 37.624 37.023 6.700
2024-12-01-22:09:04-root-INFO: grad norm: 52.301 51.702 7.896
2024-12-01-22:09:04-root-INFO: Loss too large (593.452->597.438)! Learning rate decreased to 0.00939.
2024-12-01-22:09:05-root-INFO: grad norm: 41.546 40.892 7.338
2024-12-01-22:09:06-root-INFO: grad norm: 25.827 25.344 4.967
2024-12-01-22:09:06-root-INFO: grad norm: 24.597 24.085 4.993
2024-12-01-22:09:07-root-INFO: grad norm: 24.237 23.756 4.801
2024-12-01-22:09:08-root-INFO: grad norm: 24.240 23.737 4.912
2024-12-01-22:09:09-root-INFO: Loss Change: 595.885 -> 578.329
2024-12-01-22:09:09-root-INFO: Regularization Change: 0.000 -> 1.315
2024-12-01-22:09:09-root-INFO: Undo step: 140
2024-12-01-22:09:09-root-INFO: Undo step: 141
2024-12-01-22:09:09-root-INFO: Undo step: 142
2024-12-01-22:09:09-root-INFO: Undo step: 143
2024-12-01-22:09:09-root-INFO: Undo step: 144
2024-12-01-22:09:09-root-INFO: step: 145 lr_xt 0.01231381
2024-12-01-22:09:09-root-INFO: grad norm: 257.675 253.438 46.534
2024-12-01-22:09:10-root-INFO: Loss too large (1047.338->1070.119)! Learning rate decreased to 0.00985.
2024-12-01-22:09:11-root-INFO: grad norm: 205.180 202.888 30.585
2024-12-01-22:09:11-root-INFO: grad norm: 116.920 114.918 21.545
2024-12-01-22:09:12-root-INFO: grad norm: 102.075 100.289 19.007
2024-12-01-22:09:13-root-INFO: grad norm: 74.394 72.976 14.453
2024-12-01-22:09:14-root-INFO: grad norm: 68.966 67.832 12.457
2024-12-01-22:09:15-root-INFO: grad norm: 65.205 64.158 11.636
2024-12-01-22:09:16-root-INFO: grad norm: 63.729 62.925 10.092
2024-12-01-22:09:16-root-INFO: Loss Change: 1047.338 -> 657.100
2024-12-01-22:09:16-root-INFO: Regularization Change: 0.000 -> 19.984
2024-12-01-22:09:16-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-01-22:09:16-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-22:09:17-root-INFO: step: 144 lr_xt 0.01275743
2024-12-01-22:09:17-root-INFO: grad norm: 57.085 56.236 9.804
2024-12-01-22:09:18-root-INFO: grad norm: 66.233 65.573 9.324
2024-12-01-22:09:19-root-INFO: grad norm: 106.057 105.240 13.139
2024-12-01-22:09:19-root-INFO: Loss too large (645.728->681.710)! Learning rate decreased to 0.01021.
2024-12-01-22:09:19-root-INFO: Loss too large (645.728->657.482)! Learning rate decreased to 0.00816.
2024-12-01-22:09:20-root-INFO: grad norm: 70.433 69.845 9.081
2024-12-01-22:09:21-root-INFO: grad norm: 26.497 25.761 6.200
2024-12-01-22:09:22-root-INFO: grad norm: 23.091 22.338 5.847
2024-12-01-22:09:22-root-INFO: grad norm: 21.362 20.600 5.652
2024-12-01-22:09:23-root-INFO: grad norm: 20.390 19.641 5.477
2024-12-01-22:09:24-root-INFO: Loss Change: 652.691 -> 605.983
2024-12-01-22:09:24-root-INFO: Regularization Change: 0.000 -> 3.110
2024-12-01-22:09:24-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-01-22:09:24-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-22:09:24-root-INFO: step: 143 lr_xt 0.01321490
2024-12-01-22:09:25-root-INFO: grad norm: 25.367 24.623 6.099
2024-12-01-22:09:25-root-INFO: grad norm: 26.443 25.821 5.701
2024-12-01-22:09:26-root-INFO: grad norm: 33.248 32.602 6.524
2024-12-01-22:09:27-root-INFO: grad norm: 44.903 44.273 7.492
2024-12-01-22:09:27-root-INFO: Loss too large (594.336->595.971)! Learning rate decreased to 0.01057.
2024-12-01-22:09:28-root-INFO: grad norm: 45.652 44.975 7.831
2024-12-01-22:09:29-root-INFO: grad norm: 45.261 44.621 7.585
2024-12-01-22:09:30-root-INFO: grad norm: 45.306 44.625 7.829
2024-12-01-22:09:31-root-INFO: grad norm: 44.988 44.342 7.599
2024-12-01-22:09:31-root-INFO: Loss Change: 604.193 -> 579.406
2024-12-01-22:09:31-root-INFO: Regularization Change: 0.000 -> 2.631
2024-12-01-22:09:31-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-01-22:09:31-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-22:09:32-root-INFO: step: 142 lr_xt 0.01368658
2024-12-01-22:09:32-root-INFO: grad norm: 58.099 57.292 9.650
2024-12-01-22:09:32-root-INFO: Loss too large (582.030->586.705)! Learning rate decreased to 0.01095.
2024-12-01-22:09:33-root-INFO: grad norm: 52.270 51.544 8.683
2024-12-01-22:09:34-root-INFO: grad norm: 48.138 47.451 8.105
2024-12-01-22:09:35-root-INFO: grad norm: 45.982 45.320 7.774
2024-12-01-22:09:36-root-INFO: grad norm: 44.536 43.878 7.630
2024-12-01-22:09:37-root-INFO: grad norm: 43.228 42.578 7.470
2024-12-01-22:09:38-root-INFO: grad norm: 42.312 41.663 7.380
2024-12-01-22:09:39-root-INFO: grad norm: 41.537 40.891 7.297
2024-12-01-22:09:39-root-INFO: Loss Change: 582.030 -> 556.733
2024-12-01-22:09:39-root-INFO: Regularization Change: 0.000 -> 1.801
2024-12-01-22:09:39-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-01-22:09:39-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-22:09:40-root-INFO: step: 141 lr_xt 0.01417280
2024-12-01-22:09:40-root-INFO: grad norm: 51.983 51.298 8.412
2024-12-01-22:09:40-root-INFO: Loss too large (558.140->563.764)! Learning rate decreased to 0.01134.
2024-12-01-22:09:41-root-INFO: grad norm: 46.740 46.040 8.061
2024-12-01-22:09:42-root-INFO: grad norm: 39.936 39.335 6.897
2024-12-01-22:09:43-root-INFO: grad norm: 38.797 38.188 6.848
2024-12-01-22:09:44-root-INFO: grad norm: 38.730 38.145 6.706
2024-12-01-22:09:45-root-INFO: grad norm: 38.123 37.507 6.822
2024-12-01-22:09:45-root-INFO: grad norm: 37.864 37.282 6.610
2024-12-01-22:09:46-root-INFO: grad norm: 37.482 36.867 6.764
2024-12-01-22:09:47-root-INFO: Loss Change: 558.140 -> 537.849
2024-12-01-22:09:47-root-INFO: Regularization Change: 0.000 -> 1.548
2024-12-01-22:09:47-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-01-22:09:47-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-22:09:47-root-INFO: step: 140 lr_xt 0.01467393
2024-12-01-22:09:47-root-INFO: grad norm: 49.311 48.605 8.314
2024-12-01-22:09:48-root-INFO: Loss too large (539.696->545.392)! Learning rate decreased to 0.01174.
2024-12-01-22:09:49-root-INFO: grad norm: 45.610 44.881 8.124
2024-12-01-22:09:49-root-INFO: grad norm: 41.356 40.753 7.038
2024-12-01-22:09:50-root-INFO: grad norm: 40.500 39.838 7.292
2024-12-01-22:09:51-root-INFO: grad norm: 40.127 39.540 6.839
2024-12-01-22:09:52-root-INFO: grad norm: 39.408 38.753 7.153
2024-12-01-22:09:53-root-INFO: grad norm: 38.591 38.014 6.650
2024-12-01-22:09:54-root-INFO: grad norm: 38.178 37.534 6.982
2024-12-01-22:09:54-root-INFO: Loss Change: 539.696 -> 521.782
2024-12-01-22:09:54-root-INFO: Regularization Change: 0.000 -> 1.442
2024-12-01-22:09:54-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-01-22:09:54-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-22:09:55-root-INFO: step: 139 lr_xt 0.01519033
2024-12-01-22:09:55-root-INFO: grad norm: 47.082 46.402 7.973
2024-12-01-22:09:55-root-INFO: Loss too large (523.828->528.964)! Learning rate decreased to 0.01215.
2024-12-01-22:09:56-root-INFO: grad norm: 43.540 42.827 7.847
2024-12-01-22:09:57-root-INFO: grad norm: 39.971 39.405 6.700
2024-12-01-22:09:58-root-INFO: grad norm: 38.921 38.268 7.101
2024-12-01-22:09:59-root-INFO: grad norm: 38.166 37.618 6.440
2024-12-01-22:10:00-root-INFO: grad norm: 37.467 36.827 6.895
2024-12-01-22:10:01-root-INFO: grad norm: 36.780 36.240 6.275
2024-12-01-22:10:01-root-INFO: grad norm: 36.361 35.731 6.739
2024-12-01-22:10:02-root-INFO: Loss Change: 523.828 -> 507.137
2024-12-01-22:10:02-root-INFO: Regularization Change: 0.000 -> 1.370
2024-12-01-22:10:02-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-01-22:10:02-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-22:10:02-root-INFO: step: 138 lr_xt 0.01572237
2024-12-01-22:10:03-root-INFO: grad norm: 44.248 43.622 7.417
2024-12-01-22:10:03-root-INFO: Loss too large (508.222->513.646)! Learning rate decreased to 0.01258.
2024-12-01-22:10:04-root-INFO: grad norm: 41.298 40.591 7.612
2024-12-01-22:10:05-root-INFO: grad norm: 37.269 36.729 6.323
2024-12-01-22:10:05-root-INFO: grad norm: 36.701 36.060 6.829
2024-12-01-22:10:06-root-INFO: grad norm: 36.556 36.025 6.206
2024-12-01-22:10:07-root-INFO: grad norm: 36.108 35.470 6.759
2024-12-01-22:10:08-root-INFO: grad norm: 35.625 35.098 6.103
2024-12-01-22:10:09-root-INFO: grad norm: 35.321 34.689 6.654
2024-12-01-22:10:09-root-INFO: Loss Change: 508.222 -> 493.471
2024-12-01-22:10:09-root-INFO: Regularization Change: 0.000 -> 1.291
2024-12-01-22:10:09-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-01-22:10:09-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-22:10:10-root-INFO: step: 137 lr_xt 0.01627042
2024-12-01-22:10:10-root-INFO: grad norm: 50.416 49.581 9.142
2024-12-01-22:10:10-root-INFO: Loss too large (497.270->502.591)! Learning rate decreased to 0.01302.
2024-12-01-22:10:11-root-INFO: grad norm: 44.158 43.368 8.312
2024-12-01-22:10:12-root-INFO: grad norm: 37.752 37.190 6.493
2024-12-01-22:10:13-root-INFO: grad norm: 36.165 35.524 6.779
2024-12-01-22:10:14-root-INFO: grad norm: 35.096 34.580 5.996
2024-12-01-22:10:15-root-INFO: grad norm: 34.259 33.637 6.497
2024-12-01-22:10:15-root-INFO: grad norm: 33.622 33.118 5.796
2024-12-01-22:10:16-root-INFO: grad norm: 33.135 32.521 6.350
2024-12-01-22:10:17-root-INFO: Loss Change: 497.270 -> 479.578
2024-12-01-22:10:17-root-INFO: Regularization Change: 0.000 -> 1.375
2024-12-01-22:10:17-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-01-22:10:17-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-22:10:17-root-INFO: step: 136 lr_xt 0.01683487
2024-12-01-22:10:18-root-INFO: grad norm: 40.030 39.412 7.007
2024-12-01-22:10:18-root-INFO: Loss too large (481.033->485.188)! Learning rate decreased to 0.01347.
2024-12-01-22:10:19-root-INFO: grad norm: 37.187 36.512 7.053
2024-12-01-22:10:20-root-INFO: grad norm: 33.889 33.387 5.807
2024-12-01-22:10:21-root-INFO: grad norm: 32.809 32.197 6.306
2024-12-01-22:10:21-root-INFO: grad norm: 31.767 31.289 5.488
2024-12-01-22:10:22-root-INFO: grad norm: 31.226 30.634 6.053
2024-12-01-22:10:23-root-INFO: grad norm: 30.749 30.278 5.359
2024-12-01-22:10:24-root-INFO: grad norm: 30.468 29.883 5.944
2024-12-01-22:10:25-root-INFO: Loss Change: 481.033 -> 467.371
2024-12-01-22:10:25-root-INFO: Regularization Change: 0.000 -> 1.248
2024-12-01-22:10:25-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-01-22:10:25-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-22:10:25-root-INFO: step: 135 lr_xt 0.01741608
2024-12-01-22:10:25-root-INFO: grad norm: 37.543 36.908 6.875
2024-12-01-22:10:26-root-INFO: Loss too large (468.387->471.512)! Learning rate decreased to 0.01393.
2024-12-01-22:10:26-root-INFO: grad norm: 35.180 34.523 6.768
2024-12-01-22:10:27-root-INFO: grad norm: 33.502 33.011 5.713
2024-12-01-22:10:28-root-INFO: grad norm: 32.545 31.938 6.257
2024-12-01-22:10:29-root-INFO: grad norm: 31.452 30.985 5.399
2024-12-01-22:10:30-root-INFO: grad norm: 30.838 30.256 5.965
2024-12-01-22:10:31-root-INFO: grad norm: 30.199 29.742 5.233
2024-12-01-22:10:32-root-INFO: grad norm: 29.823 29.251 5.810
2024-12-01-22:10:32-root-INFO: Loss Change: 468.387 -> 455.352
2024-12-01-22:10:32-root-INFO: Regularization Change: 0.000 -> 1.249
2024-12-01-22:10:32-root-INFO: Undo step: 135
2024-12-01-22:10:32-root-INFO: Undo step: 136
2024-12-01-22:10:32-root-INFO: Undo step: 137
2024-12-01-22:10:32-root-INFO: Undo step: 138
2024-12-01-22:10:32-root-INFO: Undo step: 139
2024-12-01-22:10:33-root-INFO: step: 140 lr_xt 0.01467393
2024-12-01-22:10:33-root-INFO: grad norm: 167.218 163.033 37.176
2024-12-01-22:10:34-root-INFO: grad norm: 98.318 93.787 29.504
2024-12-01-22:10:35-root-INFO: grad norm: 161.104 158.701 27.722
2024-12-01-22:10:35-root-INFO: Loss too large (639.119->761.508)! Learning rate decreased to 0.01174.
2024-12-01-22:10:35-root-INFO: Loss too large (639.119->693.642)! Learning rate decreased to 0.00939.
2024-12-01-22:10:36-root-INFO: Loss too large (639.119->650.874)! Learning rate decreased to 0.00751.
2024-12-01-22:10:37-root-INFO: grad norm: 101.909 99.645 21.359
2024-12-01-22:10:38-root-INFO: grad norm: 43.215 40.859 14.074
2024-12-01-22:10:38-root-INFO: grad norm: 35.580 33.362 12.366
2024-12-01-22:10:39-root-INFO: grad norm: 31.843 29.870 11.035
2024-12-01-22:10:40-root-INFO: grad norm: 29.263 27.501 9.999
2024-12-01-22:10:41-root-INFO: Loss Change: 886.201 -> 543.928
2024-12-01-22:10:41-root-INFO: Regularization Change: 0.000 -> 17.573
2024-12-01-22:10:41-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-01-22:10:41-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-22:10:41-root-INFO: step: 139 lr_xt 0.01519033
2024-12-01-22:10:41-root-INFO: grad norm: 31.896 30.474 9.419
2024-12-01-22:10:42-root-INFO: grad norm: 34.756 33.738 8.349
2024-12-01-22:10:43-root-INFO: grad norm: 49.260 48.331 9.522
2024-12-01-22:10:43-root-INFO: Loss too large (529.639->537.536)! Learning rate decreased to 0.01215.
2024-12-01-22:10:44-root-INFO: grad norm: 71.793 70.621 12.920
2024-12-01-22:10:44-root-INFO: Loss too large (527.288->537.910)! Learning rate decreased to 0.00972.
2024-12-01-22:10:45-root-INFO: Loss too large (527.288->527.293)! Learning rate decreased to 0.00778.
2024-12-01-22:10:45-root-INFO: grad norm: 46.692 45.792 9.124
2024-12-01-22:10:46-root-INFO: grad norm: 24.117 23.272 6.329
2024-12-01-22:10:47-root-INFO: grad norm: 20.324 19.462 5.856
2024-12-01-22:10:48-root-INFO: grad norm: 18.388 17.568 5.431
2024-12-01-22:10:49-root-INFO: Loss Change: 543.456 -> 504.786
2024-12-01-22:10:49-root-INFO: Regularization Change: 0.000 -> 3.063
2024-12-01-22:10:49-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-01-22:10:49-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-22:10:49-root-INFO: step: 138 lr_xt 0.01572237
2024-12-01-22:10:49-root-INFO: grad norm: 24.079 23.366 5.817
2024-12-01-22:10:50-root-INFO: grad norm: 34.237 33.528 6.929
2024-12-01-22:10:50-root-INFO: Loss too large (501.067->504.318)! Learning rate decreased to 0.01258.
2024-12-01-22:10:51-root-INFO: grad norm: 44.560 43.827 8.045
2024-12-01-22:10:52-root-INFO: Loss too large (500.424->501.475)! Learning rate decreased to 0.01006.
2024-12-01-22:10:52-root-INFO: grad norm: 40.579 39.772 8.057
2024-12-01-22:10:53-root-INFO: grad norm: 38.370 37.663 7.328
2024-12-01-22:10:54-root-INFO: grad norm: 35.974 35.233 7.266
2024-12-01-22:10:55-root-INFO: grad norm: 34.160 33.501 6.677
2024-12-01-22:10:56-root-INFO: grad norm: 32.546 31.860 6.646
2024-12-01-22:10:56-root-INFO: Loss Change: 504.288 -> 485.167
2024-12-01-22:10:56-root-INFO: Regularization Change: 0.000 -> 1.804
2024-12-01-22:10:56-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-01-22:10:56-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-22:10:57-root-INFO: step: 137 lr_xt 0.01627042
2024-12-01-22:10:57-root-INFO: grad norm: 48.354 47.585 8.589
2024-12-01-22:10:57-root-INFO: Loss too large (488.557->496.653)! Learning rate decreased to 0.01302.
2024-12-01-22:10:58-root-INFO: Loss too large (488.557->488.726)! Learning rate decreased to 0.01041.
2024-12-01-22:10:58-root-INFO: grad norm: 39.578 38.778 7.918
2024-12-01-22:10:59-root-INFO: grad norm: 33.557 32.951 6.347
2024-12-01-22:11:00-root-INFO: grad norm: 30.135 29.510 6.107
2024-12-01-22:11:01-root-INFO: grad norm: 27.182 26.645 5.375
2024-12-01-22:11:02-root-INFO: grad norm: 25.390 24.829 5.310
2024-12-01-22:11:03-root-INFO: grad norm: 23.778 23.290 4.795
2024-12-01-22:11:04-root-INFO: grad norm: 22.793 22.268 4.862
2024-12-01-22:11:04-root-INFO: Loss Change: 488.557 -> 469.039
2024-12-01-22:11:04-root-INFO: Regularization Change: 0.000 -> 1.216
2024-12-01-22:11:04-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-01-22:11:04-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-22:11:05-root-INFO: step: 136 lr_xt 0.01683487
2024-12-01-22:11:05-root-INFO: grad norm: 29.437 28.910 5.549
2024-12-01-22:11:05-root-INFO: Loss too large (469.796->472.032)! Learning rate decreased to 0.01347.
2024-12-01-22:11:06-root-INFO: grad norm: 34.368 33.748 6.499
2024-12-01-22:11:07-root-INFO: grad norm: 43.842 43.283 6.982
2024-12-01-22:11:07-root-INFO: Loss too large (468.546->470.680)! Learning rate decreased to 0.01077.
2024-12-01-22:11:08-root-INFO: grad norm: 35.197 34.576 6.582
2024-12-01-22:11:09-root-INFO: grad norm: 25.818 25.371 4.788
2024-12-01-22:11:10-root-INFO: grad norm: 22.816 22.317 4.743
2024-12-01-22:11:11-root-INFO: grad norm: 20.265 19.855 4.059
2024-12-01-22:11:12-root-INFO: grad norm: 18.847 18.376 4.187
2024-12-01-22:11:12-root-INFO: Loss Change: 469.796 -> 456.419
2024-12-01-22:11:12-root-INFO: Regularization Change: 0.000 -> 1.098
2024-12-01-22:11:12-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-01-22:11:12-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-22:11:12-root-INFO: step: 135 lr_xt 0.01741608
2024-12-01-22:11:13-root-INFO: grad norm: 26.958 26.427 5.323
2024-12-01-22:11:13-root-INFO: Loss too large (457.111->458.048)! Learning rate decreased to 0.01393.
2024-12-01-22:11:14-root-INFO: grad norm: 30.546 29.987 5.815
2024-12-01-22:11:15-root-INFO: grad norm: 39.070 38.585 6.137
2024-12-01-22:11:15-root-INFO: Loss too large (455.505->457.048)! Learning rate decreased to 0.01115.
2024-12-01-22:11:16-root-INFO: grad norm: 32.252 31.694 5.976
2024-12-01-22:11:17-root-INFO: grad norm: 25.621 25.223 4.501
2024-12-01-22:11:18-root-INFO: grad norm: 22.485 22.017 4.566
2024-12-01-22:11:18-root-INFO: grad norm: 19.764 19.396 3.796
2024-12-01-22:11:19-root-INFO: grad norm: 18.165 17.724 3.978
2024-12-01-22:11:20-root-INFO: Loss Change: 457.111 -> 444.897
2024-12-01-22:11:20-root-INFO: Regularization Change: 0.000 -> 1.028
2024-12-01-22:11:20-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-01-22:11:20-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-22:11:20-root-INFO: step: 134 lr_xt 0.01801447
2024-12-01-22:11:20-root-INFO: grad norm: 27.179 26.695 5.108
2024-12-01-22:11:21-root-INFO: Loss too large (445.695->447.015)! Learning rate decreased to 0.01441.
2024-12-01-22:11:22-root-INFO: grad norm: 30.503 29.984 5.604
2024-12-01-22:11:23-root-INFO: grad norm: 38.403 37.957 5.832
2024-12-01-22:11:23-root-INFO: Loss too large (444.332->445.798)! Learning rate decreased to 0.01153.
2024-12-01-22:11:24-root-INFO: grad norm: 31.141 30.613 5.710
2024-12-01-22:11:24-root-INFO: grad norm: 24.394 24.034 4.176
2024-12-01-22:11:25-root-INFO: grad norm: 21.235 20.789 4.332
2024-12-01-22:11:26-root-INFO: grad norm: 18.592 18.256 3.520
2024-12-01-22:11:27-root-INFO: grad norm: 17.053 16.629 3.780
2024-12-01-22:11:28-root-INFO: Loss Change: 445.695 -> 434.129
2024-12-01-22:11:28-root-INFO: Regularization Change: 0.000 -> 0.979
2024-12-01-22:11:28-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-01-22:11:28-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-22:11:28-root-INFO: step: 133 lr_xt 0.01863041
2024-12-01-22:11:28-root-INFO: grad norm: 25.598 25.087 5.088
2024-12-01-22:11:29-root-INFO: Loss too large (435.297->436.278)! Learning rate decreased to 0.01490.
2024-12-01-22:11:29-root-INFO: grad norm: 28.719 28.232 5.263
2024-12-01-22:11:30-root-INFO: grad norm: 36.205 35.799 5.408
2024-12-01-22:11:31-root-INFO: Loss too large (433.904->435.230)! Learning rate decreased to 0.01192.
2024-12-01-22:11:31-root-INFO: grad norm: 29.662 29.173 5.363
2024-12-01-22:11:32-root-INFO: grad norm: 23.692 23.362 3.940
2024-12-01-22:11:33-root-INFO: grad norm: 20.584 20.165 4.132
2024-12-01-22:11:34-root-INFO: grad norm: 17.973 17.663 3.328
2024-12-01-22:11:35-root-INFO: grad norm: 16.382 15.982 3.597
2024-12-01-22:11:36-root-INFO: Loss Change: 435.297 -> 424.359
2024-12-01-22:11:36-root-INFO: Regularization Change: 0.000 -> 0.960
2024-12-01-22:11:36-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-01-22:11:36-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-22:11:36-root-INFO: step: 132 lr_xt 0.01926430
2024-12-01-22:11:36-root-INFO: grad norm: 22.271 21.867 4.222
2024-12-01-22:11:36-root-INFO: Loss too large (424.903->425.637)! Learning rate decreased to 0.01541.
2024-12-01-22:11:37-root-INFO: grad norm: 25.508 25.045 4.837
2024-12-01-22:11:38-root-INFO: grad norm: 32.385 32.014 4.890
2024-12-01-22:11:38-root-INFO: Loss too large (423.638->424.678)! Learning rate decreased to 0.01233.
2024-12-01-22:11:39-root-INFO: grad norm: 26.953 26.486 4.997
2024-12-01-22:11:40-root-INFO: grad norm: 21.850 21.543 3.650
2024-12-01-22:11:41-root-INFO: grad norm: 19.168 18.763 3.920
2024-12-01-22:11:42-root-INFO: grad norm: 16.853 16.560 3.124
2024-12-01-22:11:43-root-INFO: grad norm: 15.416 15.029 3.434
2024-12-01-22:11:43-root-INFO: Loss Change: 424.903 -> 415.059
2024-12-01-22:11:43-root-INFO: Regularization Change: 0.000 -> 0.928
2024-12-01-22:11:43-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-01-22:11:43-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-22:11:44-root-INFO: step: 131 lr_xt 0.01991656
2024-12-01-22:11:44-root-INFO: grad norm: 23.919 23.448 4.724
2024-12-01-22:11:44-root-INFO: Loss too large (416.564->417.318)! Learning rate decreased to 0.01593.
2024-12-01-22:11:45-root-INFO: grad norm: 26.584 26.144 4.816
2024-12-01-22:11:46-root-INFO: grad norm: 32.776 32.432 4.734
2024-12-01-22:11:46-root-INFO: Loss too large (415.045->415.898)! Learning rate decreased to 0.01275.
2024-12-01-22:11:47-root-INFO: grad norm: 26.525 26.088 4.798
2024-12-01-22:11:48-root-INFO: grad norm: 21.149 20.868 3.436
2024-12-01-22:11:49-root-INFO: grad norm: 18.091 17.706 3.713
2024-12-01-22:11:50-root-INFO: grad norm: 15.611 15.337 2.915
2024-12-01-22:11:51-root-INFO: grad norm: 14.018 13.644 3.218
2024-12-01-22:11:51-root-INFO: Loss Change: 416.564 -> 406.197
2024-12-01-22:11:51-root-INFO: Regularization Change: 0.000 -> 0.961
2024-12-01-22:11:51-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-01-22:11:51-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-22:11:52-root-INFO: step: 130 lr_xt 0.02058758
2024-12-01-22:11:52-root-INFO: grad norm: 20.567 20.196 3.887
2024-12-01-22:11:52-root-INFO: Loss too large (406.895->407.443)! Learning rate decreased to 0.01647.
2024-12-01-22:11:53-root-INFO: grad norm: 23.437 23.016 4.419
2024-12-01-22:11:54-root-INFO: grad norm: 29.593 29.278 4.310
2024-12-01-22:11:54-root-INFO: Loss too large (405.658->406.486)! Learning rate decreased to 0.01318.
2024-12-01-22:11:55-root-INFO: grad norm: 24.749 24.326 4.560
2024-12-01-22:11:56-root-INFO: grad norm: 20.359 20.090 3.296
2024-12-01-22:11:57-root-INFO: grad norm: 17.774 17.399 3.630
2024-12-01-22:11:58-root-INFO: grad norm: 15.567 15.306 2.840
2024-12-01-22:11:59-root-INFO: grad norm: 14.110 13.749 3.173
2024-12-01-22:11:59-root-INFO: Loss Change: 406.895 -> 397.739
2024-12-01-22:11:59-root-INFO: Regularization Change: 0.000 -> 0.920
2024-12-01-22:11:59-root-INFO: Undo step: 130
2024-12-01-22:11:59-root-INFO: Undo step: 131
2024-12-01-22:11:59-root-INFO: Undo step: 132
2024-12-01-22:11:59-root-INFO: Undo step: 133
2024-12-01-22:11:59-root-INFO: Undo step: 134
2024-12-01-22:11:59-root-INFO: step: 135 lr_xt 0.01741608
2024-12-01-22:12:00-root-INFO: grad norm: 185.253 181.583 36.694
2024-12-01-22:12:01-root-INFO: grad norm: 82.420 79.579 21.452
2024-12-01-22:12:01-root-INFO: grad norm: 64.312 62.274 16.062
2024-12-01-22:12:02-root-INFO: grad norm: 59.250 57.642 13.711
2024-12-01-22:12:03-root-INFO: grad norm: 55.778 54.391 12.363
2024-12-01-22:12:04-root-INFO: grad norm: 53.277 52.217 10.579
2024-12-01-22:12:05-root-INFO: grad norm: 52.884 51.882 10.242
2024-12-01-22:12:06-root-INFO: grad norm: 51.342 50.535 9.068
2024-12-01-22:12:06-root-INFO: Loss Change: 827.776 -> 461.076
2024-12-01-22:12:06-root-INFO: Regularization Change: 0.000 -> 29.685
2024-12-01-22:12:06-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-01-22:12:06-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-22:12:07-root-INFO: step: 134 lr_xt 0.01801447
2024-12-01-22:12:07-root-INFO: grad norm: 46.385 45.565 8.681
2024-12-01-22:12:08-root-INFO: grad norm: 44.724 44.039 7.798
2024-12-01-22:12:09-root-INFO: grad norm: 44.322 43.611 7.907
2024-12-01-22:12:10-root-INFO: grad norm: 43.713 43.143 7.038
2024-12-01-22:12:10-root-INFO: grad norm: 43.427 42.796 7.379
2024-12-01-22:12:11-root-INFO: grad norm: 42.943 42.438 6.562
2024-12-01-22:12:12-root-INFO: grad norm: 42.602 42.019 7.024
2024-12-01-22:12:13-root-INFO: grad norm: 42.272 41.807 6.252
2024-12-01-22:12:14-root-INFO: Loss Change: 457.998 -> 427.037
2024-12-01-22:12:14-root-INFO: Regularization Change: 0.000 -> 4.247
2024-12-01-22:12:14-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-01-22:12:14-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-22:12:14-root-INFO: step: 133 lr_xt 0.01863041
2024-12-01-22:12:14-root-INFO: grad norm: 38.929 38.400 6.394
2024-12-01-22:12:15-root-INFO: grad norm: 39.125 38.675 5.919
2024-12-01-22:12:16-root-INFO: grad norm: 40.708 40.206 6.371
2024-12-01-22:12:17-root-INFO: grad norm: 42.190 41.772 5.920
2024-12-01-22:12:18-root-INFO: grad norm: 43.290 42.800 6.497
2024-12-01-22:12:19-root-INFO: grad norm: 44.208 43.797 6.011
2024-12-01-22:12:20-root-INFO: grad norm: 44.550 44.061 6.582
2024-12-01-22:12:21-root-INFO: grad norm: 45.268 44.850 6.137
2024-12-01-22:12:21-root-INFO: Loss Change: 425.033 -> 410.817
2024-12-01-22:12:21-root-INFO: Regularization Change: 0.000 -> 2.663
2024-12-01-22:12:21-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-01-22:12:21-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-22:12:22-root-INFO: step: 132 lr_xt 0.01926430
2024-12-01-22:12:22-root-INFO: grad norm: 44.130 43.746 5.812
2024-12-01-22:12:23-root-INFO: grad norm: 42.929 42.523 5.889
2024-12-01-22:12:24-root-INFO: grad norm: 43.375 42.958 6.002
2024-12-01-22:12:25-root-INFO: grad norm: 43.409 43.009 5.879
2024-12-01-22:12:25-root-INFO: grad norm: 43.665 43.256 5.967
2024-12-01-22:12:26-root-INFO: grad norm: 43.526 43.129 5.865
2024-12-01-22:12:27-root-INFO: grad norm: 43.421 43.019 5.897
2024-12-01-22:12:28-root-INFO: grad norm: 43.043 42.651 5.802
2024-12-01-22:12:29-root-INFO: Loss Change: 409.875 -> 395.239
2024-12-01-22:12:29-root-INFO: Regularization Change: 0.000 -> 2.123
2024-12-01-22:12:29-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-01-22:12:29-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-22:12:29-root-INFO: step: 131 lr_xt 0.01991656
2024-12-01-22:12:29-root-INFO: grad norm: 40.401 40.055 5.276
2024-12-01-22:12:30-root-INFO: grad norm: 38.360 38.011 5.161
2024-12-01-22:12:31-root-INFO: grad norm: 38.220 37.884 5.058
2024-12-01-22:12:32-root-INFO: grad norm: 38.072 37.757 4.886
2024-12-01-22:12:33-root-INFO: grad norm: 38.180 37.856 4.964
2024-12-01-22:12:34-root-INFO: grad norm: 37.913 37.619 4.715
2024-12-01-22:12:35-root-INFO: grad norm: 37.554 37.242 4.833
2024-12-01-22:12:35-root-INFO: grad norm: 36.973 36.695 4.526
2024-12-01-22:12:36-root-INFO: Loss Change: 394.155 -> 380.301
2024-12-01-22:12:36-root-INFO: Regularization Change: 0.000 -> 1.949
2024-12-01-22:12:36-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-01-22:12:36-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-22:12:36-root-INFO: step: 130 lr_xt 0.02058758
2024-12-01-22:12:37-root-INFO: grad norm: 34.594 34.355 4.058
2024-12-01-22:12:38-root-INFO: grad norm: 34.144 33.868 4.329
2024-12-01-22:12:38-root-INFO: grad norm: 34.440 34.191 4.135
2024-12-01-22:12:39-root-INFO: grad norm: 34.541 34.293 4.137
2024-12-01-22:12:40-root-INFO: grad norm: 34.548 34.303 4.114
2024-12-01-22:12:41-root-INFO: grad norm: 34.335 34.101 3.996
2024-12-01-22:12:42-root-INFO: grad norm: 33.986 33.746 4.034
2024-12-01-22:12:43-root-INFO: grad norm: 33.579 33.357 3.852
2024-12-01-22:12:43-root-INFO: Loss Change: 379.406 -> 368.490
2024-12-01-22:12:43-root-INFO: Regularization Change: 0.000 -> 1.743
2024-12-01-22:12:43-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-01-22:12:43-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-22:12:43-root-INFO: step: 129 lr_xt 0.02127779
2024-12-01-22:12:44-root-INFO: grad norm: 29.115 28.898 3.542
2024-12-01-22:12:45-root-INFO: grad norm: 28.638 28.413 3.587
2024-12-01-22:12:45-root-INFO: grad norm: 29.608 29.412 3.397
2024-12-01-22:12:46-root-INFO: grad norm: 30.552 30.346 3.541
2024-12-01-22:12:47-root-INFO: grad norm: 31.491 31.286 3.589
2024-12-01-22:12:48-root-INFO: grad norm: 32.051 31.851 3.583
2024-12-01-22:12:49-root-INFO: grad norm: 32.375 32.165 3.686
2024-12-01-22:12:50-root-INFO: grad norm: 32.433 32.236 3.574
2024-12-01-22:12:51-root-INFO: Loss Change: 366.744 -> 357.948
2024-12-01-22:12:51-root-INFO: Regularization Change: 0.000 -> 1.697
2024-12-01-22:12:51-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-01-22:12:51-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-22:12:51-root-INFO: step: 128 lr_xt 0.02198759
2024-12-01-22:12:51-root-INFO: grad norm: 27.612 27.413 3.307
2024-12-01-22:12:52-root-INFO: grad norm: 27.756 27.563 3.264
2024-12-01-22:12:53-root-INFO: grad norm: 28.920 28.746 3.171
2024-12-01-22:12:54-root-INFO: grad norm: 29.884 29.702 3.294
2024-12-01-22:12:55-root-INFO: grad norm: 30.696 30.507 3.404
2024-12-01-22:12:55-root-INFO: grad norm: 31.158 30.975 3.374
2024-12-01-22:12:56-root-INFO: grad norm: 31.367 31.168 3.528
2024-12-01-22:12:57-root-INFO: grad norm: 31.389 31.205 3.393
2024-12-01-22:12:58-root-INFO: Loss Change: 355.684 -> 347.754
2024-12-01-22:12:58-root-INFO: Regularization Change: 0.000 -> 1.635
2024-12-01-22:12:58-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-01-22:12:58-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-22:12:58-root-INFO: step: 127 lr_xt 0.02271741
2024-12-01-22:12:58-root-INFO: grad norm: 28.696 28.513 3.234
2024-12-01-22:12:59-root-INFO: grad norm: 29.137 28.933 3.440
2024-12-01-22:13:00-root-INFO: grad norm: 30.120 29.934 3.347
2024-12-01-22:13:01-root-INFO: grad norm: 30.840 30.647 3.440
2024-12-01-22:13:02-root-INFO: grad norm: 31.369 31.173 3.505
2024-12-01-22:13:03-root-INFO: grad norm: 31.620 31.430 3.468
2024-12-01-22:13:04-root-INFO: grad norm: 31.649 31.446 3.584
2024-12-01-22:13:04-root-INFO: grad norm: 31.571 31.381 3.460
2024-12-01-22:13:05-root-INFO: Loss Change: 346.730 -> 339.177
2024-12-01-22:13:05-root-INFO: Regularization Change: 0.000 -> 1.578
2024-12-01-22:13:05-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-01-22:13:05-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-22:13:05-root-INFO: step: 126 lr_xt 0.02346768
2024-12-01-22:13:06-root-INFO: grad norm: 28.850 28.687 3.067
2024-12-01-22:13:07-root-INFO: grad norm: 29.352 29.166 3.295
2024-12-01-22:13:07-root-INFO: grad norm: 30.146 29.969 3.265
2024-12-01-22:13:08-root-INFO: grad norm: 30.692 30.512 3.322
2024-12-01-22:13:09-root-INFO: grad norm: 31.044 30.855 3.422
2024-12-01-22:13:10-root-INFO: grad norm: 31.205 31.023 3.367
2024-12-01-22:13:11-root-INFO: grad norm: 31.190 30.992 3.504
2024-12-01-22:13:12-root-INFO: grad norm: 31.120 30.935 3.383
2024-12-01-22:13:12-root-INFO: Loss Change: 338.064 -> 330.830
2024-12-01-22:13:12-root-INFO: Regularization Change: 0.000 -> 1.538
2024-12-01-22:13:12-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-01-22:13:12-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-22:13:13-root-INFO: step: 125 lr_xt 0.02423882
2024-12-01-22:13:13-root-INFO: grad norm: 26.779 26.591 3.170
2024-12-01-22:13:14-root-INFO: grad norm: 26.719 26.532 3.157
2024-12-01-22:13:15-root-INFO: grad norm: 27.477 27.302 3.091
2024-12-01-22:13:15-root-INFO: grad norm: 28.191 28.010 3.190
2024-12-01-22:13:16-root-INFO: grad norm: 28.850 28.660 3.301
2024-12-01-22:13:17-root-INFO: grad norm: 29.338 29.152 3.297
2024-12-01-22:13:18-root-INFO: grad norm: 29.689 29.488 3.453
2024-12-01-22:13:19-root-INFO: grad norm: 29.935 29.743 3.385
2024-12-01-22:13:20-root-INFO: Loss Change: 328.879 -> 322.095
2024-12-01-22:13:20-root-INFO: Regularization Change: 0.000 -> 1.552
2024-12-01-22:13:20-root-INFO: Undo step: 125
2024-12-01-22:13:20-root-INFO: Undo step: 126
2024-12-01-22:13:20-root-INFO: Undo step: 127
2024-12-01-22:13:20-root-INFO: Undo step: 128
2024-12-01-22:13:20-root-INFO: Undo step: 129
2024-12-01-22:13:20-root-INFO: step: 130 lr_xt 0.02058758
2024-12-01-22:13:20-root-INFO: grad norm: 127.485 125.306 23.470
2024-12-01-22:13:21-root-INFO: grad norm: 65.060 63.719 13.144
2024-12-01-22:13:22-root-INFO: grad norm: 44.818 43.502 10.781
2024-12-01-22:13:23-root-INFO: grad norm: 35.528 34.521 8.400
2024-12-01-22:13:23-root-INFO: grad norm: 31.069 30.093 7.726
2024-12-01-22:13:24-root-INFO: grad norm: 29.534 28.703 6.957
2024-12-01-22:13:25-root-INFO: grad norm: 28.892 28.013 7.075
2024-12-01-22:13:26-root-INFO: grad norm: 29.456 28.675 6.737
2024-12-01-22:13:27-root-INFO: Loss Change: 733.540 -> 380.471
2024-12-01-22:13:27-root-INFO: Regularization Change: 0.000 -> 35.327
2024-12-01-22:13:27-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-01-22:13:27-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-22:13:27-root-INFO: step: 129 lr_xt 0.02127779
2024-12-01-22:13:27-root-INFO: grad norm: 28.092 27.372 6.320
2024-12-01-22:13:28-root-INFO: grad norm: 26.116 25.426 5.963
2024-12-01-22:13:29-root-INFO: grad norm: 25.100 24.410 5.844
2024-12-01-22:13:30-root-INFO: grad norm: 23.957 23.302 5.564
2024-12-01-22:13:31-root-INFO: grad norm: 23.171 22.507 5.509
2024-12-01-22:13:32-root-INFO: grad norm: 22.259 21.643 5.200
2024-12-01-22:13:33-root-INFO: grad norm: 21.683 21.048 5.208
2024-12-01-22:13:33-root-INFO: grad norm: 21.096 20.518 4.902
2024-12-01-22:13:34-root-INFO: Loss Change: 378.406 -> 349.427
2024-12-01-22:13:34-root-INFO: Regularization Change: 0.000 -> 4.676
2024-12-01-22:13:34-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-01-22:13:34-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-22:13:34-root-INFO: step: 128 lr_xt 0.02198759
2024-12-01-22:13:35-root-INFO: grad norm: 21.564 21.005 4.881
2024-12-01-22:13:35-root-INFO: grad norm: 19.062 18.564 4.329
2024-12-01-22:13:36-root-INFO: grad norm: 18.113 17.645 4.087
2024-12-01-22:13:37-root-INFO: grad norm: 17.323 16.873 3.924
2024-12-01-22:13:38-root-INFO: grad norm: 16.858 16.433 3.764
2024-12-01-22:13:39-root-INFO: grad norm: 16.474 16.070 3.626
2024-12-01-22:13:40-root-INFO: grad norm: 16.363 15.985 3.497
2024-12-01-22:13:41-root-INFO: grad norm: 16.454 16.102 3.386
2024-12-01-22:13:41-root-INFO: Loss Change: 348.722 -> 331.937
2024-12-01-22:13:41-root-INFO: Regularization Change: 0.000 -> 2.782
2024-12-01-22:13:41-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-01-22:13:41-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-22:13:41-root-INFO: step: 127 lr_xt 0.02271741
2024-12-01-22:13:42-root-INFO: grad norm: 21.204 20.897 3.593
2024-12-01-22:13:43-root-INFO: grad norm: 23.842 23.621 3.238
2024-12-01-22:13:43-root-INFO: grad norm: 27.518 27.323 3.276
2024-12-01-22:13:44-root-INFO: grad norm: 31.765 31.554 3.647
2024-12-01-22:13:45-root-INFO: Loss too large (330.270->330.716)! Learning rate decreased to 0.01817.
2024-12-01-22:13:46-root-INFO: grad norm: 23.876 23.704 2.857
2024-12-01-22:13:46-root-INFO: grad norm: 18.134 17.924 2.756
2024-12-01-22:13:47-root-INFO: grad norm: 14.786 14.601 2.330
2024-12-01-22:13:48-root-INFO: grad norm: 12.452 12.222 2.386
2024-12-01-22:13:49-root-INFO: Loss Change: 332.299 -> 320.246
2024-12-01-22:13:49-root-INFO: Regularization Change: 0.000 -> 1.664
2024-12-01-22:13:49-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-01-22:13:49-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-22:13:49-root-INFO: step: 126 lr_xt 0.02346768
2024-12-01-22:13:49-root-INFO: grad norm: 17.021 16.736 3.102
2024-12-01-22:13:50-root-INFO: grad norm: 19.601 19.384 2.906
2024-12-01-22:13:51-root-INFO: grad norm: 23.444 23.248 3.020
2024-12-01-22:13:52-root-INFO: grad norm: 28.046 27.825 3.511
2024-12-01-22:13:52-root-INFO: Loss too large (319.391->320.007)! Learning rate decreased to 0.01877.
2024-12-01-22:13:53-root-INFO: grad norm: 21.909 21.734 2.770
2024-12-01-22:13:54-root-INFO: grad norm: 17.311 17.110 2.629
2024-12-01-22:13:55-root-INFO: grad norm: 14.412 14.241 2.217
2024-12-01-22:13:56-root-INFO: grad norm: 12.298 12.092 2.240
2024-12-01-22:13:56-root-INFO: Loss Change: 320.775 -> 311.261
2024-12-01-22:13:56-root-INFO: Regularization Change: 0.000 -> 1.421
2024-12-01-22:13:56-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-01-22:13:56-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-22:13:56-root-INFO: step: 125 lr_xt 0.02423882
2024-12-01-22:13:57-root-INFO: grad norm: 20.756 20.403 3.811
2024-12-01-22:13:58-root-INFO: grad norm: 23.770 23.554 3.196
2024-12-01-22:13:58-root-INFO: Loss too large (311.794->312.127)! Learning rate decreased to 0.01939.
2024-12-01-22:13:59-root-INFO: grad norm: 19.195 19.012 2.644
2024-12-01-22:14:00-root-INFO: grad norm: 15.737 15.546 2.448
2024-12-01-22:14:01-root-INFO: grad norm: 13.381 13.209 2.140
2024-12-01-22:14:01-root-INFO: grad norm: 11.576 11.378 2.131
2024-12-01-22:14:02-root-INFO: grad norm: 10.296 10.112 1.940
2024-12-01-22:14:03-root-INFO: grad norm: 9.303 9.092 1.972
2024-12-01-22:14:04-root-INFO: Loss Change: 312.605 -> 303.225
2024-12-01-22:14:04-root-INFO: Regularization Change: 0.000 -> 1.196
2024-12-01-22:14:04-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-01-22:14:04-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-22:14:04-root-INFO: step: 124 lr_xt 0.02515763
2024-12-01-22:14:04-root-INFO: grad norm: 23.084 22.640 4.502
2024-12-01-22:14:05-root-INFO: grad norm: 26.152 25.904 3.590
2024-12-01-22:14:05-root-INFO: Loss too large (304.889->305.651)! Learning rate decreased to 0.02013.
2024-12-01-22:14:06-root-INFO: grad norm: 21.078 20.875 2.919
2024-12-01-22:14:07-root-INFO: grad norm: 17.390 17.196 2.592
2024-12-01-22:14:08-root-INFO: grad norm: 14.791 14.620 2.245
2024-12-01-22:14:09-root-INFO: grad norm: 12.784 12.598 2.174
2024-12-01-22:14:10-root-INFO: grad norm: 11.323 11.151 1.968
2024-12-01-22:14:11-root-INFO: grad norm: 10.163 9.970 1.969
2024-12-01-22:14:11-root-INFO: Loss Change: 305.690 -> 295.954
2024-12-01-22:14:11-root-INFO: Regularization Change: 0.000 -> 1.199
2024-12-01-22:14:11-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-01-22:14:11-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-22:14:11-root-INFO: step: 123 lr_xt 0.02597490
2024-12-01-22:14:12-root-INFO: grad norm: 16.304 16.023 3.014
2024-12-01-22:14:13-root-INFO: grad norm: 19.964 19.786 2.660
2024-12-01-22:14:13-root-INFO: Loss too large (296.348->296.847)! Learning rate decreased to 0.02078.
2024-12-01-22:14:14-root-INFO: grad norm: 17.239 17.071 2.399
2024-12-01-22:14:15-root-INFO: grad norm: 14.973 14.807 2.222
2024-12-01-22:14:16-root-INFO: grad norm: 13.269 13.112 2.033
2024-12-01-22:14:16-root-INFO: grad norm: 11.855 11.687 1.990
2024-12-01-22:14:17-root-INFO: grad norm: 10.771 10.611 1.852
2024-12-01-22:14:18-root-INFO: grad norm: 9.864 9.688 1.854
2024-12-01-22:14:19-root-INFO: Loss Change: 296.801 -> 289.504
2024-12-01-22:14:19-root-INFO: Regularization Change: 0.000 -> 1.076
2024-12-01-22:14:19-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-01-22:14:19-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-22:14:19-root-INFO: step: 122 lr_xt 0.02681440
2024-12-01-22:14:19-root-INFO: grad norm: 16.878 16.577 3.172
2024-12-01-22:14:20-root-INFO: grad norm: 20.367 20.184 2.721
2024-12-01-22:14:20-root-INFO: Loss too large (290.192->290.802)! Learning rate decreased to 0.02145.
2024-12-01-22:14:21-root-INFO: grad norm: 17.395 17.229 2.396
2024-12-01-22:14:22-root-INFO: grad norm: 15.024 14.862 2.202
2024-12-01-22:14:23-root-INFO: grad norm: 13.222 13.072 1.987
2024-12-01-22:14:24-root-INFO: grad norm: 11.749 11.588 1.939
2024-12-01-22:14:25-root-INFO: grad norm: 10.610 10.458 1.790
2024-12-01-22:14:26-root-INFO: grad norm: 9.667 9.500 1.791
2024-12-01-22:14:26-root-INFO: Loss Change: 290.588 -> 283.449
2024-12-01-22:14:26-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-01-22:14:26-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-01-22:14:26-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-22:14:27-root-INFO: step: 121 lr_xt 0.02767658
2024-12-01-22:14:27-root-INFO: grad norm: 16.179 15.917 2.899
2024-12-01-22:14:28-root-INFO: grad norm: 19.868 19.699 2.584
2024-12-01-22:14:28-root-INFO: Loss too large (283.912->284.655)! Learning rate decreased to 0.02214.
2024-12-01-22:14:29-root-INFO: grad norm: 17.220 17.067 2.286
2024-12-01-22:14:30-root-INFO: grad norm: 15.040 14.887 2.135
2024-12-01-22:14:31-root-INFO: grad norm: 13.377 13.236 1.935
2024-12-01-22:14:31-root-INFO: grad norm: 11.985 11.834 1.895
2024-12-01-22:14:32-root-INFO: grad norm: 10.898 10.756 1.756
2024-12-01-22:14:33-root-INFO: grad norm: 9.978 9.823 1.754
2024-12-01-22:14:34-root-INFO: Loss Change: 284.215 -> 277.535
2024-12-01-22:14:34-root-INFO: Regularization Change: 0.000 -> 1.024
2024-12-01-22:14:34-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-01-22:14:34-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-22:14:34-root-INFO: step: 120 lr_xt 0.02856188
2024-12-01-22:14:34-root-INFO: grad norm: 16.859 16.594 2.980
2024-12-01-22:14:35-root-INFO: grad norm: 20.890 20.729 2.583
2024-12-01-22:14:35-root-INFO: Loss too large (278.693->279.728)! Learning rate decreased to 0.02285.
2024-12-01-22:14:36-root-INFO: grad norm: 18.124 17.972 2.341
2024-12-01-22:14:37-root-INFO: grad norm: 15.849 15.704 2.143
2024-12-01-22:14:38-root-INFO: grad norm: 14.096 13.959 1.961
2024-12-01-22:14:39-root-INFO: grad norm: 12.633 12.490 1.894
2024-12-01-22:14:40-root-INFO: grad norm: 11.478 11.342 1.760
2024-12-01-22:14:41-root-INFO: grad norm: 10.501 10.356 1.743
2024-12-01-22:14:41-root-INFO: Loss Change: 278.829 -> 272.230
2024-12-01-22:14:41-root-INFO: Regularization Change: 0.000 -> 1.019
2024-12-01-22:14:41-root-INFO: Undo step: 120
2024-12-01-22:14:41-root-INFO: Undo step: 121
2024-12-01-22:14:41-root-INFO: Undo step: 122
2024-12-01-22:14:41-root-INFO: Undo step: 123
2024-12-01-22:14:41-root-INFO: Undo step: 124
2024-12-01-22:14:42-root-INFO: step: 125 lr_xt 0.02423882
2024-12-01-22:14:42-root-INFO: grad norm: 118.835 116.805 21.868
2024-12-01-22:14:43-root-INFO: grad norm: 62.190 60.801 13.073
2024-12-01-22:14:44-root-INFO: grad norm: 43.516 41.948 11.577
2024-12-01-22:14:45-root-INFO: grad norm: 35.955 34.869 8.772
2024-12-01-22:14:46-root-INFO: grad norm: 32.037 30.852 8.632
2024-12-01-22:14:46-root-INFO: grad norm: 30.320 29.445 7.230
2024-12-01-22:14:47-root-INFO: grad norm: 29.509 28.511 7.612
2024-12-01-22:14:48-root-INFO: grad norm: 29.706 28.931 6.742
2024-12-01-22:14:49-root-INFO: Loss Change: 644.470 -> 327.610
2024-12-01-22:14:49-root-INFO: Regularization Change: 0.000 -> 33.679
2024-12-01-22:14:49-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-01-22:14:49-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-22:14:49-root-INFO: step: 124 lr_xt 0.02515763
2024-12-01-22:14:49-root-INFO: grad norm: 25.765 24.947 6.439
2024-12-01-22:14:50-root-INFO: grad norm: 24.817 24.145 5.737
2024-12-01-22:14:51-root-INFO: grad norm: 25.361 24.640 6.004
2024-12-01-22:14:52-root-INFO: grad norm: 26.644 26.046 5.614
2024-12-01-22:14:53-root-INFO: grad norm: 28.542 27.873 6.143
2024-12-01-22:14:54-root-INFO: grad norm: 30.736 30.193 5.755
2024-12-01-22:14:55-root-INFO: grad norm: 33.194 32.568 6.415
2024-12-01-22:14:55-root-INFO: grad norm: 35.209 34.701 5.962
2024-12-01-22:14:56-root-INFO: Loss Change: 324.793 -> 305.845
2024-12-01-22:14:56-root-INFO: Regularization Change: 0.000 -> 4.848
2024-12-01-22:14:56-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-01-22:14:56-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-22:14:56-root-INFO: step: 123 lr_xt 0.02597490
2024-12-01-22:14:56-root-INFO: grad norm: 33.102 32.569 5.917
2024-12-01-22:14:57-root-INFO: grad norm: 34.485 34.011 5.695
2024-12-01-22:14:58-root-INFO: grad norm: 35.945 35.416 6.145
2024-12-01-22:14:59-root-INFO: grad norm: 36.990 36.546 5.710
2024-12-01-22:15:00-root-INFO: grad norm: 37.646 37.130 6.214
2024-12-01-22:15:01-root-INFO: grad norm: 38.100 37.673 5.690
2024-12-01-22:15:02-root-INFO: grad norm: 38.192 37.693 6.152
2024-12-01-22:15:02-root-INFO: grad norm: 38.144 37.733 5.586
2024-12-01-22:15:03-root-INFO: Loss Change: 303.776 -> 294.334
2024-12-01-22:15:03-root-INFO: Regularization Change: 0.000 -> 2.625
2024-12-01-22:15:03-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-01-22:15:03-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-22:15:03-root-INFO: step: 122 lr_xt 0.02681440
2024-12-01-22:15:04-root-INFO: grad norm: 32.951 32.582 4.918
2024-12-01-22:15:04-root-INFO: grad norm: 33.311 32.944 4.936
2024-12-01-22:15:05-root-INFO: grad norm: 33.732 33.341 5.126
2024-12-01-22:15:06-root-INFO: grad norm: 34.023 33.676 4.848
2024-12-01-22:15:07-root-INFO: grad norm: 34.176 33.785 5.154
2024-12-01-22:15:08-root-INFO: grad norm: 34.195 33.857 4.793
2024-12-01-22:15:09-root-INFO: grad norm: 34.125 33.740 5.108
2024-12-01-22:15:10-root-INFO: grad norm: 33.983 33.654 4.717
2024-12-01-22:15:10-root-INFO: Loss Change: 291.591 -> 283.114
2024-12-01-22:15:10-root-INFO: Regularization Change: 0.000 -> 1.962
2024-12-01-22:15:10-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-01-22:15:10-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-22:15:11-root-INFO: step: 121 lr_xt 0.02767658
2024-12-01-22:15:11-root-INFO: grad norm: 29.068 28.771 4.147
2024-12-01-22:15:12-root-INFO: grad norm: 29.686 29.385 4.217
2024-12-01-22:15:13-root-INFO: grad norm: 30.597 30.274 4.438
2024-12-01-22:15:14-root-INFO: grad norm: 31.408 31.111 4.310
2024-12-01-22:15:15-root-INFO: grad norm: 32.048 31.715 4.608
2024-12-01-22:15:15-root-INFO: grad norm: 32.536 32.236 4.407
2024-12-01-22:15:16-root-INFO: grad norm: 32.814 32.477 4.691
2024-12-01-22:15:17-root-INFO: grad norm: 32.994 32.693 4.442
2024-12-01-22:15:18-root-INFO: Loss Change: 280.624 -> 274.998
2024-12-01-22:15:18-root-INFO: Regularization Change: 0.000 -> 1.683
2024-12-01-22:15:18-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-01-22:15:18-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-22:15:18-root-INFO: step: 120 lr_xt 0.02856188
2024-12-01-22:15:19-root-INFO: grad norm: 28.000 27.746 3.763
2024-12-01-22:15:19-root-INFO: grad norm: 28.793 28.507 4.052
2024-12-01-22:15:20-root-INFO: grad norm: 29.799 29.508 4.156
2024-12-01-22:15:21-root-INFO: grad norm: 30.723 30.438 4.171
2024-12-01-22:15:22-root-INFO: grad norm: 31.400 31.093 4.380
2024-12-01-22:15:23-root-INFO: grad norm: 31.932 31.643 4.285
2024-12-01-22:15:24-root-INFO: grad norm: 32.213 31.898 4.492
2024-12-01-22:15:25-root-INFO: grad norm: 32.388 32.098 4.327
2024-12-01-22:15:25-root-INFO: Loss Change: 272.726 -> 268.256
2024-12-01-22:15:25-root-INFO: Regularization Change: 0.000 -> 1.540
2024-12-01-22:15:25-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-01-22:15:25-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-22:15:26-root-INFO: step: 119 lr_xt 0.02947075
2024-12-01-22:15:26-root-INFO: grad norm: 26.920 26.660 3.731
2024-12-01-22:15:27-root-INFO: grad norm: 27.479 27.195 3.937
2024-12-01-22:15:28-root-INFO: grad norm: 28.452 28.180 3.925
2024-12-01-22:15:29-root-INFO: grad norm: 29.469 29.191 4.037
2024-12-01-22:15:29-root-INFO: grad norm: 30.247 29.957 4.179
2024-12-01-22:15:30-root-INFO: grad norm: 30.922 30.641 4.162
2024-12-01-22:15:31-root-INFO: grad norm: 31.330 31.029 4.332
2024-12-01-22:15:32-root-INFO: grad norm: 31.627 31.343 4.224
2024-12-01-22:15:33-root-INFO: Loss Change: 265.366 -> 261.450
2024-12-01-22:15:33-root-INFO: Regularization Change: 0.000 -> 1.499
2024-12-01-22:15:33-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-01-22:15:33-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-22:15:33-root-INFO: step: 118 lr_xt 0.03040366
2024-12-01-22:15:33-root-INFO: grad norm: 28.909 28.677 3.656
2024-12-01-22:15:34-root-INFO: grad norm: 29.520 29.250 3.983
2024-12-01-22:15:35-root-INFO: grad norm: 30.084 29.815 4.014
2024-12-01-22:15:36-root-INFO: grad norm: 30.512 30.245 4.021
2024-12-01-22:15:37-root-INFO: grad norm: 30.751 30.474 4.118
2024-12-01-22:15:38-root-INFO: grad norm: 30.863 30.598 4.035
2024-12-01-22:15:38-root-INFO: grad norm: 30.850 30.571 4.139
2024-12-01-22:15:39-root-INFO: grad norm: 30.775 30.512 4.009
2024-12-01-22:15:40-root-INFO: Loss Change: 260.557 -> 255.971
2024-12-01-22:15:40-root-INFO: Regularization Change: 0.000 -> 1.370
2024-12-01-22:15:40-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-01-22:15:40-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-22:15:40-root-INFO: step: 117 lr_xt 0.03136105
2024-12-01-22:15:41-root-INFO: grad norm: 24.959 24.758 3.159
2024-12-01-22:15:42-root-INFO: grad norm: 25.801 25.551 3.586
2024-12-01-22:15:42-root-INFO: grad norm: 27.154 26.926 3.513
2024-12-01-22:15:43-root-INFO: grad norm: 28.463 28.216 3.738
2024-12-01-22:15:44-root-INFO: grad norm: 29.474 29.226 3.811
2024-12-01-22:15:45-root-INFO: grad norm: 30.305 30.054 3.891
2024-12-01-22:15:46-root-INFO: grad norm: 30.790 30.532 3.979
2024-12-01-22:15:47-root-INFO: grad norm: 31.110 30.858 3.952
2024-12-01-22:15:47-root-INFO: Loss Change: 253.285 -> 250.902
2024-12-01-22:15:47-root-INFO: Regularization Change: 0.000 -> 1.396
2024-12-01-22:15:47-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-01-22:15:47-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-22:15:48-root-INFO: step: 116 lr_xt 0.03234339
2024-12-01-22:15:48-root-INFO: grad norm: 27.245 27.045 3.296
2024-12-01-22:15:49-root-INFO: grad norm: 27.830 27.582 3.709
2024-12-01-22:15:50-root-INFO: grad norm: 28.413 28.175 3.670
2024-12-01-22:15:51-root-INFO: grad norm: 29.032 28.783 3.798
2024-12-01-22:15:52-root-INFO: grad norm: 29.417 29.163 3.852
2024-12-01-22:15:52-root-INFO: grad norm: 29.730 29.477 3.867
2024-12-01-22:15:53-root-INFO: grad norm: 29.853 29.591 3.949
2024-12-01-22:15:54-root-INFO: grad norm: 29.915 29.661 3.893
2024-12-01-22:15:55-root-INFO: Loss Change: 248.835 -> 245.289
2024-12-01-22:15:55-root-INFO: Regularization Change: 0.000 -> 1.324
2024-12-01-22:15:55-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-01-22:15:55-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-22:15:55-root-INFO: step: 115 lr_xt 0.03335113
2024-12-01-22:15:55-root-INFO: grad norm: 25.120 24.940 3.002
2024-12-01-22:15:56-root-INFO: grad norm: 25.762 25.521 3.515
2024-12-01-22:15:57-root-INFO: grad norm: 26.577 26.351 3.465
2024-12-01-22:15:58-root-INFO: grad norm: 27.381 27.139 3.635
2024-12-01-22:15:59-root-INFO: grad norm: 27.971 27.727 3.685
2024-12-01-22:16:00-root-INFO: grad norm: 28.456 28.210 3.735
2024-12-01-22:16:01-root-INFO: grad norm: 28.732 28.479 3.807
2024-12-01-22:16:01-root-INFO: grad norm: 28.916 28.668 3.778
2024-12-01-22:16:02-root-INFO: Loss Change: 243.158 -> 240.305
2024-12-01-22:16:02-root-INFO: Regularization Change: 0.000 -> 1.306
2024-12-01-22:16:02-root-INFO: Undo step: 115
2024-12-01-22:16:02-root-INFO: Undo step: 116
2024-12-01-22:16:02-root-INFO: Undo step: 117
2024-12-01-22:16:02-root-INFO: Undo step: 118
2024-12-01-22:16:02-root-INFO: Undo step: 119
2024-12-01-22:16:02-root-INFO: step: 120 lr_xt 0.02856188
2024-12-01-22:16:03-root-INFO: grad norm: 101.644 99.567 20.440
2024-12-01-22:16:03-root-INFO: grad norm: 52.825 51.859 10.056
2024-12-01-22:16:04-root-INFO: grad norm: 34.335 33.534 7.375
2024-12-01-22:16:05-root-INFO: grad norm: 25.356 24.797 5.293
2024-12-01-22:16:06-root-INFO: grad norm: 20.669 20.119 4.738
2024-12-01-22:16:07-root-INFO: grad norm: 17.662 17.252 3.787
2024-12-01-22:16:08-root-INFO: grad norm: 15.566 15.143 3.606
2024-12-01-22:16:09-root-INFO: grad norm: 14.113 13.776 3.064
2024-12-01-22:16:09-root-INFO: Loss Change: 564.023 -> 277.404
2024-12-01-22:16:09-root-INFO: Regularization Change: 0.000 -> 38.726
2024-12-01-22:16:09-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-01-22:16:10-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-22:16:10-root-INFO: step: 119 lr_xt 0.02947075
2024-12-01-22:16:10-root-INFO: grad norm: 14.410 13.962 3.564
2024-12-01-22:16:11-root-INFO: grad norm: 12.170 11.859 2.733
2024-12-01-22:16:12-root-INFO: grad norm: 12.084 11.825 2.490
2024-12-01-22:16:13-root-INFO: grad norm: 12.626 12.382 2.469
2024-12-01-22:16:14-root-INFO: grad norm: 14.179 13.985 2.340
2024-12-01-22:16:14-root-INFO: grad norm: 15.816 15.616 2.502
2024-12-01-22:16:15-root-INFO: grad norm: 18.394 18.230 2.453
2024-12-01-22:16:16-root-INFO: grad norm: 19.804 19.616 2.720
2024-12-01-22:16:17-root-INFO: Loss Change: 276.928 -> 259.126
2024-12-01-22:16:17-root-INFO: Regularization Change: 0.000 -> 4.653
2024-12-01-22:16:17-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-01-22:16:17-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-22:16:17-root-INFO: step: 118 lr_xt 0.03040366
2024-12-01-22:16:17-root-INFO: grad norm: 24.474 24.215 3.553
2024-12-01-22:16:18-root-INFO: grad norm: 24.635 24.434 3.148
2024-12-01-22:16:19-root-INFO: grad norm: 24.313 24.121 3.053
2024-12-01-22:16:20-root-INFO: grad norm: 24.944 24.739 3.195
2024-12-01-22:16:21-root-INFO: grad norm: 25.300 25.107 3.116
2024-12-01-22:16:22-root-INFO: grad norm: 26.275 26.064 3.318
2024-12-01-22:16:23-root-INFO: grad norm: 26.961 26.764 3.260
2024-12-01-22:16:23-root-INFO: grad norm: 27.779 27.563 3.456
2024-12-01-22:16:24-root-INFO: Loss Change: 260.404 -> 252.561
2024-12-01-22:16:24-root-INFO: Regularization Change: 0.000 -> 2.561
2024-12-01-22:16:24-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-01-22:16:24-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-22:16:24-root-INFO: step: 117 lr_xt 0.03136105
2024-12-01-22:16:25-root-INFO: grad norm: 37.523 37.066 5.841
2024-12-01-22:16:25-root-INFO: grad norm: 36.496 36.235 4.354
2024-12-01-22:16:26-root-INFO: grad norm: 34.594 34.321 4.340
2024-12-01-22:16:27-root-INFO: grad norm: 33.135 32.888 4.033
2024-12-01-22:16:28-root-INFO: grad norm: 31.683 31.444 3.888
2024-12-01-22:16:29-root-INFO: grad norm: 30.544 30.312 3.754
2024-12-01-22:16:30-root-INFO: grad norm: 29.573 29.354 3.593
2024-12-01-22:16:31-root-INFO: grad norm: 28.821 28.603 3.544
2024-12-01-22:16:31-root-INFO: Loss Change: 257.175 -> 245.240
2024-12-01-22:16:31-root-INFO: Regularization Change: 0.000 -> 1.950
2024-12-01-22:16:31-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-01-22:16:31-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-22:16:32-root-INFO: step: 116 lr_xt 0.03234339
2024-12-01-22:16:32-root-INFO: grad norm: 34.568 34.199 5.035
2024-12-01-22:16:33-root-INFO: grad norm: 32.723 32.488 3.914
2024-12-01-22:16:34-root-INFO: grad norm: 30.697 30.462 3.783
2024-12-01-22:16:35-root-INFO: grad norm: 29.268 29.056 3.510
2024-12-01-22:16:35-root-INFO: grad norm: 28.020 27.817 3.367
2024-12-01-22:16:36-root-INFO: grad norm: 27.170 26.975 3.242
2024-12-01-22:16:37-root-INFO: grad norm: 26.512 26.327 3.126
2024-12-01-22:16:38-root-INFO: grad norm: 26.122 25.942 3.067
2024-12-01-22:16:39-root-INFO: Loss Change: 248.475 -> 238.033
2024-12-01-22:16:39-root-INFO: Regularization Change: 0.000 -> 1.628
2024-12-01-22:16:39-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-01-22:16:39-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-22:16:39-root-INFO: step: 115 lr_xt 0.03335113
2024-12-01-22:16:39-root-INFO: grad norm: 32.650 32.303 4.744
2024-12-01-22:16:40-root-INFO: grad norm: 31.316 31.117 3.526
2024-12-01-22:16:41-root-INFO: grad norm: 29.857 29.649 3.519
2024-12-01-22:16:42-root-INFO: grad norm: 28.967 28.790 3.198
2024-12-01-22:16:43-root-INFO: grad norm: 28.370 28.185 3.233
2024-12-01-22:16:44-root-INFO: grad norm: 28.320 28.145 3.144
2024-12-01-22:16:44-root-INFO: grad norm: 28.681 28.490 3.305
2024-12-01-22:16:45-root-INFO: grad norm: 29.501 29.299 3.453
2024-12-01-22:16:46-root-INFO: Loss Change: 241.600 -> 235.006
2024-12-01-22:16:46-root-INFO: Regularization Change: 0.000 -> 1.532
2024-12-01-22:16:46-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-01-22:16:46-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-22:16:46-root-INFO: step: 114 lr_xt 0.03438473
2024-12-01-22:16:46-root-INFO: grad norm: 35.430 35.114 4.726
2024-12-01-22:16:47-root-INFO: grad norm: 34.859 34.618 4.087
2024-12-01-22:16:48-root-INFO: grad norm: 33.894 33.639 4.151
2024-12-01-22:16:49-root-INFO: grad norm: 32.821 32.582 3.957
2024-12-01-22:16:50-root-INFO: grad norm: 31.638 31.397 3.898
2024-12-01-22:16:51-root-INFO: grad norm: 30.472 30.248 3.689
2024-12-01-22:16:52-root-INFO: grad norm: 29.420 29.196 3.620
2024-12-01-22:16:53-root-INFO: grad norm: 28.471 28.266 3.416
2024-12-01-22:16:53-root-INFO: Loss Change: 238.224 -> 229.175
2024-12-01-22:16:53-root-INFO: Regularization Change: 0.000 -> 1.422
2024-12-01-22:16:53-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-01-22:16:53-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-22:16:54-root-INFO: step: 113 lr_xt 0.03544467
2024-12-01-22:16:54-root-INFO: grad norm: 32.091 31.781 4.454
2024-12-01-22:16:55-root-INFO: grad norm: 30.832 30.636 3.469
2024-12-01-22:16:56-root-INFO: grad norm: 29.604 29.394 3.522
2024-12-01-22:16:57-root-INFO: grad norm: 28.543 28.363 3.204
2024-12-01-22:16:57-root-INFO: grad norm: 27.576 27.386 3.237
2024-12-01-22:16:58-root-INFO: grad norm: 26.790 26.617 3.035
2024-12-01-22:16:59-root-INFO: grad norm: 26.117 25.937 3.059
2024-12-01-22:17:00-root-INFO: grad norm: 25.587 25.420 2.913
2024-12-01-22:17:01-root-INFO: Loss Change: 231.429 -> 223.224
2024-12-01-22:17:01-root-INFO: Regularization Change: 0.000 -> 1.323
2024-12-01-22:17:01-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-01-22:17:01-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-22:17:01-root-INFO: step: 112 lr_xt 0.03653141
2024-12-01-22:17:01-root-INFO: grad norm: 31.021 30.696 4.482
2024-12-01-22:17:02-root-INFO: grad norm: 29.552 29.368 3.287
2024-12-01-22:17:03-root-INFO: grad norm: 28.122 27.924 3.334
2024-12-01-22:17:04-root-INFO: grad norm: 26.985 26.819 2.991
2024-12-01-22:17:05-root-INFO: grad norm: 25.955 25.779 3.022
2024-12-01-22:17:06-root-INFO: grad norm: 25.154 24.995 2.825
2024-12-01-22:17:06-root-INFO: grad norm: 24.480 24.314 2.848
2024-12-01-22:17:07-root-INFO: grad norm: 23.966 23.812 2.716
2024-12-01-22:17:08-root-INFO: Loss Change: 226.271 -> 218.070
2024-12-01-22:17:08-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-01-22:17:08-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-01-22:17:08-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-22:17:08-root-INFO: step: 111 lr_xt 0.03764541
2024-12-01-22:17:08-root-INFO: grad norm: 27.349 27.107 3.630
2024-12-01-22:17:09-root-INFO: grad norm: 26.263 26.105 2.873
2024-12-01-22:17:10-root-INFO: grad norm: 25.239 25.068 2.938
2024-12-01-22:17:11-root-INFO: grad norm: 24.347 24.197 2.702
2024-12-01-22:17:12-root-INFO: grad norm: 23.588 23.431 2.721
2024-12-01-22:17:13-root-INFO: grad norm: 22.970 22.823 2.590
2024-12-01-22:17:14-root-INFO: grad norm: 22.461 22.311 2.597
2024-12-01-22:17:15-root-INFO: grad norm: 22.062 21.918 2.511
2024-12-01-22:17:15-root-INFO: Loss Change: 220.108 -> 213.297
2024-12-01-22:17:15-root-INFO: Regularization Change: 0.000 -> 1.250
2024-12-01-22:17:15-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-01-22:17:15-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-22:17:15-root-INFO: step: 110 lr_xt 0.03878715
2024-12-01-22:17:16-root-INFO: grad norm: 26.144 25.896 3.595
2024-12-01-22:17:17-root-INFO: grad norm: 25.461 25.310 2.769
2024-12-01-22:17:17-root-INFO: grad norm: 24.730 24.560 2.898
2024-12-01-22:17:18-root-INFO: grad norm: 24.062 23.918 2.635
2024-12-01-22:17:19-root-INFO: grad norm: 23.413 23.257 2.697
2024-12-01-22:17:20-root-INFO: grad norm: 22.861 22.719 2.537
2024-12-01-22:17:21-root-INFO: grad norm: 22.368 22.220 2.570
2024-12-01-22:17:22-root-INFO: grad norm: 21.965 21.827 2.457
2024-12-01-22:17:22-root-INFO: Loss Change: 215.599 -> 209.539
2024-12-01-22:17:22-root-INFO: Regularization Change: 0.000 -> 1.244
2024-12-01-22:17:22-root-INFO: Undo step: 110
2024-12-01-22:17:22-root-INFO: Undo step: 111
2024-12-01-22:17:22-root-INFO: Undo step: 112
2024-12-01-22:17:22-root-INFO: Undo step: 113
2024-12-01-22:17:22-root-INFO: Undo step: 114
2024-12-01-22:17:23-root-INFO: step: 115 lr_xt 0.03335113
2024-12-01-22:17:23-root-INFO: grad norm: 85.701 84.324 15.302
2024-12-01-22:17:24-root-INFO: grad norm: 46.148 45.381 8.378
2024-12-01-22:17:25-root-INFO: grad norm: 35.605 34.952 6.785
2024-12-01-22:17:26-root-INFO: grad norm: 30.234 29.825 4.958
2024-12-01-22:17:26-root-INFO: grad norm: 26.191 25.727 4.906
2024-12-01-22:17:27-root-INFO: grad norm: 25.027 24.743 3.758
2024-12-01-22:17:28-root-INFO: grad norm: 24.483 24.126 4.168
2024-12-01-22:17:29-root-INFO: grad norm: 24.499 24.269 3.351
2024-12-01-22:17:30-root-INFO: Loss Change: 491.231 -> 247.212
2024-12-01-22:17:30-root-INFO: Regularization Change: 0.000 -> 39.599
2024-12-01-22:17:30-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-01-22:17:30-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-22:17:30-root-INFO: step: 114 lr_xt 0.03438473
2024-12-01-22:17:30-root-INFO: grad norm: 20.373 20.145 3.034
2024-12-01-22:17:31-root-INFO: grad norm: 20.948 20.737 2.970
2024-12-01-22:17:32-root-INFO: grad norm: 21.893 21.637 3.339
2024-12-01-22:17:33-root-INFO: grad norm: 23.115 22.908 3.088
2024-12-01-22:17:34-root-INFO: grad norm: 24.205 23.943 3.553
2024-12-01-22:17:35-root-INFO: grad norm: 25.309 25.091 3.313
2024-12-01-22:17:36-root-INFO: grad norm: 26.100 25.826 3.771
2024-12-01-22:17:36-root-INFO: grad norm: 26.761 26.529 3.521
2024-12-01-22:17:37-root-INFO: Loss Change: 245.125 -> 232.007
2024-12-01-22:17:37-root-INFO: Regularization Change: 0.000 -> 4.589
2024-12-01-22:17:37-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-01-22:17:37-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-22:17:37-root-INFO: step: 113 lr_xt 0.03544467
2024-12-01-22:17:38-root-INFO: grad norm: 22.973 22.796 2.846
2024-12-01-22:17:38-root-INFO: grad norm: 23.578 23.360 3.196
2024-12-01-22:17:39-root-INFO: grad norm: 24.344 24.101 3.428
2024-12-01-22:17:40-root-INFO: grad norm: 25.133 24.902 3.398
2024-12-01-22:17:41-root-INFO: grad norm: 25.698 25.435 3.669
2024-12-01-22:17:42-root-INFO: grad norm: 26.205 25.960 3.576
2024-12-01-22:17:43-root-INFO: grad norm: 26.503 26.226 3.827
2024-12-01-22:17:44-root-INFO: grad norm: 26.740 26.482 3.701
2024-12-01-22:17:44-root-INFO: Loss Change: 229.501 -> 222.757
2024-12-01-22:17:44-root-INFO: Regularization Change: 0.000 -> 2.528
2024-12-01-22:17:44-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-01-22:17:44-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-22:17:45-root-INFO: step: 112 lr_xt 0.03653141
2024-12-01-22:17:45-root-INFO: grad norm: 21.780 21.599 2.804
2024-12-01-22:17:46-root-INFO: grad norm: 22.315 22.075 3.259
2024-12-01-22:17:47-root-INFO: grad norm: 23.179 22.930 3.389
2024-12-01-22:17:47-root-INFO: grad norm: 24.174 23.916 3.523
2024-12-01-22:17:48-root-INFO: grad norm: 24.891 24.616 3.685
2024-12-01-22:17:49-root-INFO: grad norm: 25.572 25.299 3.727
2024-12-01-22:17:49-root-INFO: Loss too large (217.046->217.072)! Learning rate decreased to 0.02923.
2024-12-01-22:17:50-root-INFO: grad norm: 16.997 16.788 2.660
2024-12-01-22:17:51-root-INFO: grad norm: 11.551 11.402 1.848
2024-12-01-22:17:52-root-INFO: Loss Change: 219.998 -> 210.171
2024-12-01-22:17:52-root-INFO: Regularization Change: 0.000 -> 1.695
2024-12-01-22:17:52-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-01-22:17:52-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-22:17:52-root-INFO: step: 111 lr_xt 0.03764541
2024-12-01-22:17:52-root-INFO: grad norm: 6.215 6.057 1.393
2024-12-01-22:17:53-root-INFO: grad norm: 5.722 5.577 1.278
2024-12-01-22:17:54-root-INFO: grad norm: 5.916 5.775 1.281
2024-12-01-22:17:55-root-INFO: grad norm: 6.310 6.175 1.302
2024-12-01-22:17:56-root-INFO: grad norm: 6.846 6.708 1.366
2024-12-01-22:17:57-root-INFO: grad norm: 7.580 7.446 1.415
2024-12-01-22:17:58-root-INFO: grad norm: 8.475 8.334 1.539
2024-12-01-22:17:58-root-INFO: grad norm: 9.644 9.504 1.638
2024-12-01-22:17:59-root-INFO: Loss Change: 209.721 -> 205.148
2024-12-01-22:17:59-root-INFO: Regularization Change: 0.000 -> 1.611
2024-12-01-22:17:59-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-01-22:17:59-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-22:17:59-root-INFO: step: 110 lr_xt 0.03878715
2024-12-01-22:18:00-root-INFO: grad norm: 8.526 8.375 1.595
2024-12-01-22:18:01-root-INFO: grad norm: 8.814 8.659 1.645
2024-12-01-22:18:01-root-INFO: grad norm: 9.878 9.734 1.679
2024-12-01-22:18:02-root-INFO: grad norm: 11.293 11.127 1.927
2024-12-01-22:18:03-root-INFO: grad norm: 12.809 12.642 2.062
2024-12-01-22:18:04-root-INFO: grad norm: 14.644 14.455 2.343
2024-12-01-22:18:04-root-INFO: Loss too large (202.982->203.115)! Learning rate decreased to 0.03103.
2024-12-01-22:18:05-root-INFO: grad norm: 11.020 10.871 1.808
2024-12-01-22:18:06-root-INFO: grad norm: 8.402 8.275 1.457
2024-12-01-22:18:07-root-INFO: Loss Change: 204.794 -> 200.225
2024-12-01-22:18:07-root-INFO: Regularization Change: 0.000 -> 1.236
2024-12-01-22:18:07-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-01-22:18:07-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-22:18:07-root-INFO: step: 109 lr_xt 0.03995709
2024-12-01-22:18:07-root-INFO: grad norm: 6.699 6.455 1.790
2024-12-01-22:18:08-root-INFO: grad norm: 5.642 5.521 1.162
2024-12-01-22:18:09-root-INFO: grad norm: 5.759 5.633 1.196
2024-12-01-22:18:10-root-INFO: grad norm: 6.113 6.007 1.134
2024-12-01-22:18:11-root-INFO: grad norm: 6.583 6.472 1.207
2024-12-01-22:18:12-root-INFO: grad norm: 7.169 7.065 1.215
2024-12-01-22:18:13-root-INFO: grad norm: 7.860 7.749 1.312
2024-12-01-22:18:13-root-INFO: grad norm: 8.681 8.572 1.367
2024-12-01-22:18:14-root-INFO: Loss Change: 200.417 -> 196.558
2024-12-01-22:18:14-root-INFO: Regularization Change: 0.000 -> 1.372
2024-12-01-22:18:14-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-01-22:18:14-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-22:18:14-root-INFO: step: 108 lr_xt 0.04115569
2024-12-01-22:18:15-root-INFO: grad norm: 12.703 12.517 2.163
2024-12-01-22:18:16-root-INFO: grad norm: 14.045 13.929 1.801
2024-12-01-22:18:16-root-INFO: Loss too large (196.747->196.819)! Learning rate decreased to 0.03292.
2024-12-01-22:18:17-root-INFO: grad norm: 10.437 10.328 1.503
2024-12-01-22:18:18-root-INFO: grad norm: 7.998 7.911 1.176
2024-12-01-22:18:19-root-INFO: grad norm: 6.408 6.311 1.108
2024-12-01-22:18:19-root-INFO: grad norm: 5.346 5.255 0.982
2024-12-01-22:18:20-root-INFO: grad norm: 4.643 4.541 0.965
2024-12-01-22:18:21-root-INFO: grad norm: 4.180 4.079 0.913
2024-12-01-22:18:22-root-INFO: Loss Change: 197.011 -> 192.355
2024-12-01-22:18:22-root-INFO: Regularization Change: 0.000 -> 0.916
2024-12-01-22:18:22-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-01-22:18:22-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-22:18:22-root-INFO: step: 107 lr_xt 0.04238344
2024-12-01-22:18:22-root-INFO: grad norm: 10.970 10.721 2.320
2024-12-01-22:18:23-root-INFO: grad norm: 12.189 12.082 1.614
2024-12-01-22:18:24-root-INFO: Loss too large (193.205->193.363)! Learning rate decreased to 0.03391.
2024-12-01-22:18:24-root-INFO: grad norm: 9.919 9.810 1.465
2024-12-01-22:18:25-root-INFO: grad norm: 8.316 8.230 1.191
2024-12-01-22:18:26-root-INFO: grad norm: 7.049 6.954 1.152
2024-12-01-22:18:27-root-INFO: grad norm: 6.101 6.015 1.023
2024-12-01-22:18:28-root-INFO: grad norm: 5.356 5.262 1.000
2024-12-01-22:18:29-root-INFO: grad norm: 4.799 4.707 0.933
2024-12-01-22:18:29-root-INFO: Loss Change: 193.632 -> 189.374
2024-12-01-22:18:29-root-INFO: Regularization Change: 0.000 -> 0.921
2024-12-01-22:18:29-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-01-22:18:29-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-22:18:29-root-INFO: step: 106 lr_xt 0.04364080
2024-12-01-22:18:30-root-INFO: grad norm: 10.448 10.206 2.236
2024-12-01-22:18:31-root-INFO: grad norm: 12.106 11.994 1.642
2024-12-01-22:18:31-root-INFO: Loss too large (189.921->190.169)! Learning rate decreased to 0.03491.
2024-12-01-22:18:32-root-INFO: grad norm: 10.026 9.906 1.549
2024-12-01-22:18:32-root-INFO: grad norm: 8.509 8.423 1.205
2024-12-01-22:18:33-root-INFO: grad norm: 7.277 7.178 1.196
2024-12-01-22:18:34-root-INFO: grad norm: 6.337 6.254 1.023
2024-12-01-22:18:35-root-INFO: grad norm: 5.579 5.485 1.019
2024-12-01-22:18:36-root-INFO: grad norm: 4.997 4.911 0.924
2024-12-01-22:18:37-root-INFO: Loss Change: 190.176 -> 186.166
2024-12-01-22:18:37-root-INFO: Regularization Change: 0.000 -> 0.912
2024-12-01-22:18:37-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-01-22:18:37-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-22:18:37-root-INFO: step: 105 lr_xt 0.04492824
2024-12-01-22:18:37-root-INFO: grad norm: 9.472 9.260 1.993
2024-12-01-22:18:38-root-INFO: grad norm: 11.178 11.079 1.478
2024-12-01-22:18:38-root-INFO: Loss too large (186.968->187.218)! Learning rate decreased to 0.03594.
2024-12-01-22:18:39-root-INFO: grad norm: 9.458 9.350 1.425
2024-12-01-22:18:40-root-INFO: grad norm: 8.163 8.083 1.141
2024-12-01-22:18:41-root-INFO: grad norm: 7.095 7.003 1.141
2024-12-01-22:18:42-root-INFO: grad norm: 6.256 6.177 0.990
2024-12-01-22:18:43-root-INFO: grad norm: 5.565 5.476 0.991
2024-12-01-22:18:44-root-INFO: grad norm: 5.021 4.940 0.902
2024-12-01-22:18:44-root-INFO: Loss Change: 187.191 -> 183.523
2024-12-01-22:18:44-root-INFO: Regularization Change: 0.000 -> 0.888
2024-12-01-22:18:44-root-INFO: Undo step: 105
2024-12-01-22:18:44-root-INFO: Undo step: 106
2024-12-01-22:18:44-root-INFO: Undo step: 107
2024-12-01-22:18:44-root-INFO: Undo step: 108
2024-12-01-22:18:44-root-INFO: Undo step: 109
2024-12-01-22:18:45-root-INFO: step: 110 lr_xt 0.03878715
2024-12-01-22:18:45-root-INFO: grad norm: 69.434 68.398 11.950
2024-12-01-22:18:46-root-INFO: grad norm: 39.661 38.919 7.635
2024-12-01-22:18:47-root-INFO: grad norm: 29.309 28.635 6.251
2024-12-01-22:18:47-root-INFO: grad norm: 24.357 23.862 4.884
2024-12-01-22:18:48-root-INFO: grad norm: 21.944 21.506 4.365
2024-12-01-22:18:49-root-INFO: grad norm: 20.750 20.423 3.673
2024-12-01-22:18:50-root-INFO: grad norm: 20.332 20.024 3.525
2024-12-01-22:18:51-root-INFO: grad norm: 20.627 20.384 3.156
2024-12-01-22:18:51-root-INFO: Loss Change: 405.328 -> 221.157
2024-12-01-22:18:51-root-INFO: Regularization Change: 0.000 -> 34.609
2024-12-01-22:18:51-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-01-22:18:51-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-22:18:52-root-INFO: step: 109 lr_xt 0.03995709
2024-12-01-22:18:52-root-INFO: grad norm: 17.808 17.637 2.465
2024-12-01-22:18:53-root-INFO: grad norm: 18.349 18.165 2.590
2024-12-01-22:18:54-root-INFO: grad norm: 19.631 19.446 2.685
2024-12-01-22:18:55-root-INFO: grad norm: 21.512 21.334 2.763
2024-12-01-22:18:55-root-INFO: grad norm: 23.363 23.172 2.982
2024-12-01-22:18:56-root-INFO: grad norm: 25.576 25.386 3.112
2024-12-01-22:18:57-root-INFO: Loss too large (212.380->212.475)! Learning rate decreased to 0.03197.
2024-12-01-22:18:57-root-INFO: grad norm: 18.321 18.151 2.492
2024-12-01-22:18:58-root-INFO: grad norm: 13.358 13.220 1.917
2024-12-01-22:18:59-root-INFO: Loss Change: 219.753 -> 204.024
2024-12-01-22:18:59-root-INFO: Regularization Change: 0.000 -> 4.148
2024-12-01-22:18:59-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-01-22:18:59-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-22:18:59-root-INFO: step: 108 lr_xt 0.04115569
2024-12-01-22:18:59-root-INFO: grad norm: 8.269 8.136 1.476
2024-12-01-22:19:00-root-INFO: grad norm: 8.281 8.156 1.430
2024-12-01-22:19:01-root-INFO: grad norm: 9.059 8.940 1.463
2024-12-01-22:19:02-root-INFO: grad norm: 10.321 10.208 1.524
2024-12-01-22:19:03-root-INFO: grad norm: 11.819 11.703 1.648
2024-12-01-22:19:04-root-INFO: grad norm: 13.848 13.733 1.784
2024-12-01-22:19:05-root-INFO: grad norm: 15.909 15.783 1.991
2024-12-01-22:19:06-root-INFO: grad norm: 18.456 18.320 2.231
2024-12-01-22:19:06-root-INFO: Loss too large (198.472->199.008)! Learning rate decreased to 0.03292.
2024-12-01-22:19:07-root-INFO: Loss Change: 203.194 -> 196.678
2024-12-01-22:19:07-root-INFO: Regularization Change: 0.000 -> 2.572
2024-12-01-22:19:07-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-01-22:19:07-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-22:19:07-root-INFO: step: 107 lr_xt 0.04238344
2024-12-01-22:19:07-root-INFO: grad norm: 10.485 10.366 1.574
2024-12-01-22:19:08-root-INFO: grad norm: 10.567 10.478 1.366
2024-12-01-22:19:09-root-INFO: grad norm: 11.929 11.842 1.442
2024-12-01-22:19:10-root-INFO: grad norm: 13.929 13.826 1.691
2024-12-01-22:19:10-root-INFO: Loss too large (194.066->194.201)! Learning rate decreased to 0.03391.
2024-12-01-22:19:11-root-INFO: grad norm: 11.048 10.953 1.450
2024-12-01-22:19:12-root-INFO: grad norm: 8.817 8.719 1.314
2024-12-01-22:19:13-root-INFO: grad norm: 7.408 7.313 1.185
2024-12-01-22:19:14-root-INFO: grad norm: 6.343 6.243 1.125
2024-12-01-22:19:14-root-INFO: Loss Change: 195.976 -> 189.852
2024-12-01-22:19:14-root-INFO: Regularization Change: 0.000 -> 1.575
2024-12-01-22:19:14-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-01-22:19:14-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-22:19:14-root-INFO: step: 106 lr_xt 0.04364080
2024-12-01-22:19:15-root-INFO: grad norm: 6.208 5.972 1.695
2024-12-01-22:19:16-root-INFO: grad norm: 4.890 4.773 1.063
2024-12-01-22:19:16-root-INFO: grad norm: 4.804 4.674 1.111
2024-12-01-22:19:17-root-INFO: grad norm: 5.053 4.952 1.006
2024-12-01-22:19:18-root-INFO: grad norm: 5.558 5.448 1.100
2024-12-01-22:19:19-root-INFO: grad norm: 6.395 6.304 1.071
2024-12-01-22:19:20-root-INFO: grad norm: 7.643 7.543 1.236
2024-12-01-22:19:21-root-INFO: grad norm: 9.423 9.333 1.294
2024-12-01-22:19:21-root-INFO: Loss Change: 189.815 -> 185.842
2024-12-01-22:19:21-root-INFO: Regularization Change: 0.000 -> 1.745
2024-12-01-22:19:21-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-01-22:19:21-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-22:19:22-root-INFO: step: 105 lr_xt 0.04492824
2024-12-01-22:19:22-root-INFO: grad norm: 16.993 16.763 2.787
2024-12-01-22:19:22-root-INFO: Loss too large (187.624->188.508)! Learning rate decreased to 0.03594.
2024-12-01-22:19:23-root-INFO: grad norm: 13.998 13.903 1.624
2024-12-01-22:19:24-root-INFO: grad norm: 11.839 11.726 1.634
2024-12-01-22:19:25-root-INFO: grad norm: 10.169 10.089 1.270
2024-12-01-22:19:26-root-INFO: grad norm: 8.828 8.734 1.283
2024-12-01-22:19:27-root-INFO: grad norm: 7.769 7.694 1.078
2024-12-01-22:19:27-root-INFO: grad norm: 6.906 6.819 1.096
2024-12-01-22:19:28-root-INFO: grad norm: 6.216 6.141 0.966
2024-12-01-22:19:29-root-INFO: Loss Change: 187.624 -> 181.640
2024-12-01-22:19:29-root-INFO: Regularization Change: 0.000 -> 1.040
2024-12-01-22:19:29-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-01-22:19:29-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-22:19:29-root-INFO: step: 104 lr_xt 0.04624623
2024-12-01-22:19:30-root-INFO: grad norm: 11.646 11.415 2.309
2024-12-01-22:19:30-root-INFO: grad norm: 13.808 13.703 1.701
2024-12-01-22:19:31-root-INFO: Loss too large (182.636->183.238)! Learning rate decreased to 0.03700.
2024-12-01-22:19:31-root-INFO: grad norm: 11.546 11.435 1.596
2024-12-01-22:19:32-root-INFO: grad norm: 9.873 9.799 1.211
2024-12-01-22:19:33-root-INFO: grad norm: 8.543 8.453 1.234
2024-12-01-22:19:34-root-INFO: grad norm: 7.491 7.422 1.013
2024-12-01-22:19:35-root-INFO: grad norm: 6.640 6.557 1.048
2024-12-01-22:19:36-root-INFO: grad norm: 5.957 5.888 0.905
2024-12-01-22:19:36-root-INFO: Loss Change: 182.734 -> 178.314
2024-12-01-22:19:36-root-INFO: Regularization Change: 0.000 -> 1.022
2024-12-01-22:19:36-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-01-22:19:36-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-22:19:37-root-INFO: step: 103 lr_xt 0.04759523
2024-12-01-22:19:37-root-INFO: grad norm: 10.298 10.125 1.876
2024-12-01-22:19:38-root-INFO: grad norm: 12.543 12.449 1.536
2024-12-01-22:19:38-root-INFO: Loss too large (179.264->179.782)! Learning rate decreased to 0.03808.
2024-12-01-22:19:39-root-INFO: grad norm: 10.573 10.472 1.455
2024-12-01-22:19:40-root-INFO: grad norm: 9.058 8.987 1.134
2024-12-01-22:19:41-root-INFO: grad norm: 7.860 7.777 1.141
2024-12-01-22:19:41-root-INFO: grad norm: 6.901 6.834 0.955
2024-12-01-22:19:42-root-INFO: grad norm: 6.130 6.052 0.975
2024-12-01-22:19:43-root-INFO: grad norm: 5.508 5.442 0.855
2024-12-01-22:19:44-root-INFO: Loss Change: 179.277 -> 175.397
2024-12-01-22:19:44-root-INFO: Regularization Change: 0.000 -> 0.962
2024-12-01-22:19:44-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-01-22:19:44-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-22:19:44-root-INFO: step: 102 lr_xt 0.04897571
2024-12-01-22:19:44-root-INFO: grad norm: 9.044 8.865 1.788
2024-12-01-22:19:45-root-INFO: grad norm: 10.830 10.744 1.361
2024-12-01-22:19:45-root-INFO: Loss too large (175.641->175.990)! Learning rate decreased to 0.03918.
2024-12-01-22:19:46-root-INFO: grad norm: 9.368 9.274 1.322
2024-12-01-22:19:47-root-INFO: grad norm: 8.232 8.167 1.033
2024-12-01-22:19:48-root-INFO: grad norm: 7.318 7.240 1.066
2024-12-01-22:19:49-root-INFO: grad norm: 6.561 6.500 0.896
2024-12-01-22:19:50-root-INFO: grad norm: 5.939 5.866 0.932
2024-12-01-22:19:51-root-INFO: grad norm: 5.419 5.357 0.817
2024-12-01-22:19:51-root-INFO: Loss Change: 175.869 -> 172.261
2024-12-01-22:19:51-root-INFO: Regularization Change: 0.000 -> 0.965
2024-12-01-22:19:51-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-01-22:19:51-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-22:19:51-root-INFO: step: 101 lr_xt 0.05038813
2024-12-01-22:19:52-root-INFO: grad norm: 9.520 9.348 1.802
2024-12-01-22:19:53-root-INFO: grad norm: 11.705 11.621 1.400
2024-12-01-22:19:53-root-INFO: Loss too large (173.057->173.615)! Learning rate decreased to 0.04031.
2024-12-01-22:19:54-root-INFO: grad norm: 10.070 9.976 1.374
2024-12-01-22:19:55-root-INFO: grad norm: 8.780 8.717 1.052
2024-12-01-22:19:56-root-INFO: grad norm: 7.748 7.671 1.093
2024-12-01-22:19:57-root-INFO: grad norm: 6.895 6.837 0.896
2024-12-01-22:19:58-root-INFO: grad norm: 6.201 6.129 0.941
2024-12-01-22:19:58-root-INFO: grad norm: 5.621 5.563 0.807
2024-12-01-22:19:59-root-INFO: Loss Change: 173.087 -> 169.559
2024-12-01-22:19:59-root-INFO: Regularization Change: 0.000 -> 0.941
2024-12-01-22:19:59-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-01-22:19:59-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-22:19:59-root-INFO: step: 100 lr_xt 0.05183295
2024-12-01-22:20:00-root-INFO: grad norm: 9.682 9.516 1.786
2024-12-01-22:20:00-root-INFO: Loss too large (170.467->170.488)! Learning rate decreased to 0.04147.
2024-12-01-22:20:01-root-INFO: grad norm: 8.069 8.009 0.988
2024-12-01-22:20:02-root-INFO: grad norm: 7.077 6.993 1.085
2024-12-01-22:20:03-root-INFO: grad norm: 6.279 6.223 0.838
2024-12-01-22:20:04-root-INFO: grad norm: 5.642 5.569 0.907
2024-12-01-22:20:05-root-INFO: grad norm: 5.112 5.055 0.763
2024-12-01-22:20:05-root-INFO: grad norm: 4.682 4.611 0.814
2024-12-01-22:20:06-root-INFO: grad norm: 4.322 4.262 0.719
2024-12-01-22:20:07-root-INFO: Loss Change: 170.467 -> 166.942
2024-12-01-22:20:07-root-INFO: Regularization Change: 0.000 -> 0.889
2024-12-01-22:20:07-root-INFO: Undo step: 100
2024-12-01-22:20:07-root-INFO: Undo step: 101
2024-12-01-22:20:07-root-INFO: Undo step: 102
2024-12-01-22:20:07-root-INFO: Undo step: 103
2024-12-01-22:20:07-root-INFO: Undo step: 104
2024-12-01-22:20:07-root-INFO: step: 105 lr_xt 0.04492824
2024-12-01-22:20:07-root-INFO: grad norm: 72.977 71.783 13.148
2024-12-01-22:20:08-root-INFO: grad norm: 36.030 35.195 7.710
2024-12-01-22:20:09-root-INFO: grad norm: 26.357 25.766 5.549
2024-12-01-22:20:10-root-INFO: grad norm: 21.592 21.006 4.997
2024-12-01-22:20:11-root-INFO: grad norm: 19.035 18.613 3.987
2024-12-01-22:20:12-root-INFO: grad norm: 17.505 17.070 3.879
2024-12-01-22:20:12-root-INFO: grad norm: 16.504 16.185 3.226
2024-12-01-22:20:13-root-INFO: grad norm: 15.854 15.508 3.297
2024-12-01-22:20:14-root-INFO: Loss Change: 435.636 -> 205.165
2024-12-01-22:20:14-root-INFO: Regularization Change: 0.000 -> 50.040
2024-12-01-22:20:14-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-01-22:20:14-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-22:20:14-root-INFO: step: 104 lr_xt 0.04624623
2024-12-01-22:20:14-root-INFO: grad norm: 18.611 18.297 3.407
2024-12-01-22:20:15-root-INFO: grad norm: 18.312 17.999 3.372
2024-12-01-22:20:16-root-INFO: grad norm: 18.707 18.456 3.057
2024-12-01-22:20:17-root-INFO: grad norm: 19.367 19.078 3.335
2024-12-01-22:20:18-root-INFO: grad norm: 20.592 20.350 3.146
2024-12-01-22:20:19-root-INFO: grad norm: 22.003 21.724 3.491
2024-12-01-22:20:20-root-INFO: grad norm: 23.848 23.600 3.435
2024-12-01-22:20:20-root-INFO: Loss too large (194.388->194.659)! Learning rate decreased to 0.03700.
2024-12-01-22:20:21-root-INFO: grad norm: 17.618 17.392 2.816
2024-12-01-22:20:21-root-INFO: Loss Change: 205.910 -> 187.854
2024-12-01-22:20:21-root-INFO: Regularization Change: 0.000 -> 5.911
2024-12-01-22:20:21-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-01-22:20:21-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-22:20:22-root-INFO: step: 103 lr_xt 0.04759523
2024-12-01-22:20:22-root-INFO: grad norm: 18.293 18.058 2.922
2024-12-01-22:20:23-root-INFO: grad norm: 20.974 20.758 3.000
2024-12-01-22:20:23-root-INFO: Loss too large (189.435->189.727)! Learning rate decreased to 0.03808.
2024-12-01-22:20:24-root-INFO: grad norm: 15.815 15.655 2.246
2024-12-01-22:20:25-root-INFO: grad norm: 12.485 12.329 1.968
2024-12-01-22:20:26-root-INFO: grad norm: 10.227 10.097 1.626
2024-12-01-22:20:27-root-INFO: grad norm: 8.600 8.461 1.538
2024-12-01-22:20:27-root-INFO: grad norm: 7.436 7.312 1.354
2024-12-01-22:20:28-root-INFO: grad norm: 6.566 6.433 1.317
2024-12-01-22:20:29-root-INFO: Loss Change: 189.549 -> 178.991
2024-12-01-22:20:29-root-INFO: Regularization Change: 0.000 -> 2.411
2024-12-01-22:20:29-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-01-22:20:29-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-22:20:29-root-INFO: step: 102 lr_xt 0.04897571
2024-12-01-22:20:30-root-INFO: grad norm: 9.712 9.509 1.977
2024-12-01-22:20:31-root-INFO: grad norm: 11.252 11.111 1.775
2024-12-01-22:20:31-root-INFO: grad norm: 13.869 13.718 2.039
2024-12-01-22:20:32-root-INFO: Loss too large (178.507->178.842)! Learning rate decreased to 0.03918.
2024-12-01-22:20:33-root-INFO: grad norm: 11.628 11.498 1.731
2024-12-01-22:20:34-root-INFO: grad norm: 9.960 9.844 1.518
2024-12-01-22:20:35-root-INFO: grad norm: 8.652 8.542 1.380
2024-12-01-22:20:35-root-INFO: grad norm: 7.651 7.546 1.261
2024-12-01-22:20:36-root-INFO: grad norm: 6.843 6.740 1.182
2024-12-01-22:20:37-root-INFO: Loss Change: 179.306 -> 173.317
2024-12-01-22:20:37-root-INFO: Regularization Change: 0.000 -> 1.807
2024-12-01-22:20:37-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-01-22:20:37-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-22:20:37-root-INFO: step: 101 lr_xt 0.05038813
2024-12-01-22:20:38-root-INFO: grad norm: 10.661 10.473 1.993
2024-12-01-22:20:38-root-INFO: grad norm: 13.076 12.950 1.805
2024-12-01-22:20:39-root-INFO: Loss too large (174.049->174.570)! Learning rate decreased to 0.04031.
2024-12-01-22:20:40-root-INFO: grad norm: 11.135 11.014 1.643
2024-12-01-22:20:40-root-INFO: grad norm: 9.609 9.510 1.379
2024-12-01-22:20:41-root-INFO: grad norm: 8.445 8.342 1.313
2024-12-01-22:20:42-root-INFO: grad norm: 7.490 7.400 1.158
2024-12-01-22:20:43-root-INFO: grad norm: 6.744 6.649 1.129
2024-12-01-22:20:44-root-INFO: grad norm: 6.123 6.037 1.024
2024-12-01-22:20:44-root-INFO: Loss Change: 174.202 -> 169.153
2024-12-01-22:20:44-root-INFO: Regularization Change: 0.000 -> 1.394
2024-12-01-22:20:44-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-01-22:20:44-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-22:20:45-root-INFO: step: 100 lr_xt 0.05183295
2024-12-01-22:20:45-root-INFO: grad norm: 9.976 9.797 1.883
2024-12-01-22:20:46-root-INFO: grad norm: 12.316 12.209 1.626
2024-12-01-22:20:46-root-INFO: Loss too large (169.893->170.458)! Learning rate decreased to 0.04147.
2024-12-01-22:20:47-root-INFO: grad norm: 10.557 10.443 1.549
2024-12-01-22:20:48-root-INFO: grad norm: 9.134 9.048 1.254
2024-12-01-22:20:49-root-INFO: grad norm: 8.046 7.950 1.239
2024-12-01-22:20:50-root-INFO: grad norm: 7.131 7.052 1.058
2024-12-01-22:20:51-root-INFO: grad norm: 6.416 6.327 1.061
2024-12-01-22:20:52-root-INFO: grad norm: 5.810 5.734 0.937
2024-12-01-22:20:52-root-INFO: Loss Change: 169.989 -> 165.602
2024-12-01-22:20:52-root-INFO: Regularization Change: 0.000 -> 1.235
2024-12-01-22:20:52-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-01-22:20:52-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-22:20:53-root-INFO: step: 99 lr_xt 0.05331064
2024-12-01-22:20:53-root-INFO: grad norm: 9.273 9.108 1.740
2024-12-01-22:20:54-root-INFO: grad norm: 11.365 11.261 1.530
2024-12-01-22:20:54-root-INFO: Loss too large (166.062->166.531)! Learning rate decreased to 0.04265.
2024-12-01-22:20:55-root-INFO: grad norm: 9.699 9.595 1.417
2024-12-01-22:20:56-root-INFO: grad norm: 8.343 8.262 1.158
2024-12-01-22:20:57-root-INFO: grad norm: 7.315 7.228 1.126
2024-12-01-22:20:58-root-INFO: grad norm: 6.446 6.373 0.967
2024-12-01-22:20:58-root-INFO: grad norm: 5.775 5.695 0.961
2024-12-01-22:20:59-root-INFO: grad norm: 5.208 5.137 0.855
2024-12-01-22:21:00-root-INFO: Loss Change: 166.191 -> 162.208
2024-12-01-22:21:00-root-INFO: Regularization Change: 0.000 -> 1.148
2024-12-01-22:21:00-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-01-22:21:00-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-22:21:00-root-INFO: step: 98 lr_xt 0.05482165
2024-12-01-22:21:00-root-INFO: grad norm: 8.968 8.781 1.818
2024-12-01-22:21:01-root-INFO: grad norm: 11.009 10.902 1.534
2024-12-01-22:21:02-root-INFO: Loss too large (162.890->163.389)! Learning rate decreased to 0.04386.
2024-12-01-22:21:02-root-INFO: grad norm: 9.464 9.355 1.432
2024-12-01-22:21:03-root-INFO: grad norm: 8.215 8.136 1.132
2024-12-01-22:21:04-root-INFO: grad norm: 7.255 7.169 1.119
2024-12-01-22:21:05-root-INFO: grad norm: 6.434 6.365 0.938
2024-12-01-22:21:06-root-INFO: grad norm: 5.791 5.713 0.949
2024-12-01-22:21:07-root-INFO: grad norm: 5.239 5.173 0.825
2024-12-01-22:21:07-root-INFO: Loss Change: 163.000 -> 159.289
2024-12-01-22:21:07-root-INFO: Regularization Change: 0.000 -> 1.097
2024-12-01-22:21:07-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-01-22:21:07-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-22:21:08-root-INFO: step: 97 lr_xt 0.05636643
2024-12-01-22:21:08-root-INFO: grad norm: 9.389 9.209 1.830
2024-12-01-22:21:09-root-INFO: grad norm: 11.480 11.383 1.490
2024-12-01-22:21:09-root-INFO: Loss too large (160.001->160.649)! Learning rate decreased to 0.04509.
2024-12-01-22:21:10-root-INFO: grad norm: 9.803 9.705 1.383
2024-12-01-22:21:11-root-INFO: grad norm: 8.430 8.359 1.094
2024-12-01-22:21:12-root-INFO: grad norm: 7.394 7.315 1.078
2024-12-01-22:21:13-root-INFO: grad norm: 6.499 6.436 0.901
2024-12-01-22:21:14-root-INFO: grad norm: 5.811 5.740 0.909
2024-12-01-22:21:14-root-INFO: grad norm: 5.217 5.157 0.787
2024-12-01-22:21:15-root-INFO: Loss Change: 160.064 -> 156.380
2024-12-01-22:21:15-root-INFO: Regularization Change: 0.000 -> 1.070
2024-12-01-22:21:15-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-01-22:21:15-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-22:21:15-root-INFO: step: 96 lr_xt 0.05794543
2024-12-01-22:21:16-root-INFO: grad norm: 8.182 8.032 1.557
2024-12-01-22:21:16-root-INFO: grad norm: 10.093 10.009 1.295
2024-12-01-22:21:17-root-INFO: Loss too large (156.962->157.401)! Learning rate decreased to 0.04636.
2024-12-01-22:21:18-root-INFO: grad norm: 8.615 8.524 1.252
2024-12-01-22:21:18-root-INFO: grad norm: 7.379 7.316 0.961
2024-12-01-22:21:19-root-INFO: grad norm: 6.448 6.373 0.980
2024-12-01-22:21:20-root-INFO: grad norm: 5.651 5.594 0.800
2024-12-01-22:21:21-root-INFO: grad norm: 5.042 4.973 0.829
2024-12-01-22:21:22-root-INFO: grad norm: 4.524 4.468 0.709
2024-12-01-22:21:22-root-INFO: Loss Change: 157.040 -> 153.719
2024-12-01-22:21:22-root-INFO: Regularization Change: 0.000 -> 1.033
2024-12-01-22:21:22-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-01-22:21:22-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-22:21:23-root-INFO: step: 95 lr_xt 0.05955910
2024-12-01-22:21:23-root-INFO: grad norm: 8.266 8.108 1.606
2024-12-01-22:21:24-root-INFO: grad norm: 9.994 9.912 1.279
2024-12-01-22:21:24-root-INFO: Loss too large (154.480->154.922)! Learning rate decreased to 0.04765.
2024-12-01-22:21:25-root-INFO: grad norm: 8.485 8.397 1.222
2024-12-01-22:21:26-root-INFO: grad norm: 7.256 7.196 0.933
2024-12-01-22:21:27-root-INFO: grad norm: 6.339 6.267 0.952
2024-12-01-22:21:28-root-INFO: grad norm: 5.549 5.495 0.774
2024-12-01-22:21:28-root-INFO: grad norm: 4.947 4.882 0.805
2024-12-01-22:21:29-root-INFO: grad norm: 4.433 4.380 0.685
2024-12-01-22:21:30-root-INFO: Loss Change: 154.599 -> 151.299
2024-12-01-22:21:30-root-INFO: Regularization Change: 0.000 -> 1.032
2024-12-01-22:21:30-root-INFO: Undo step: 95
2024-12-01-22:21:30-root-INFO: Undo step: 96
2024-12-01-22:21:30-root-INFO: Undo step: 97
2024-12-01-22:21:30-root-INFO: Undo step: 98
2024-12-01-22:21:30-root-INFO: Undo step: 99
2024-12-01-22:21:30-root-INFO: step: 100 lr_xt 0.05183295
2024-12-01-22:21:30-root-INFO: grad norm: 66.664 65.802 10.686
2024-12-01-22:21:31-root-INFO: grad norm: 34.906 34.423 5.790
2024-12-01-22:21:32-root-INFO: grad norm: 25.233 24.869 4.269
2024-12-01-22:21:33-root-INFO: grad norm: 21.121 20.825 3.525
2024-12-01-22:21:34-root-INFO: grad norm: 18.991 18.747 3.032
2024-12-01-22:21:35-root-INFO: grad norm: 17.931 17.718 2.760
2024-12-01-22:21:36-root-INFO: grad norm: 17.499 17.294 2.670
2024-12-01-22:21:37-root-INFO: grad norm: 17.749 17.574 2.489
2024-12-01-22:21:37-root-INFO: Loss Change: 381.746 -> 186.135
2024-12-01-22:21:37-root-INFO: Regularization Change: 0.000 -> 45.954
2024-12-01-22:21:37-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-01-22:21:37-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-22:21:37-root-INFO: step: 99 lr_xt 0.05331064
2024-12-01-22:21:38-root-INFO: grad norm: 15.728 15.582 2.134
2024-12-01-22:21:39-root-INFO: grad norm: 16.184 16.036 2.186
2024-12-01-22:21:39-root-INFO: grad norm: 17.062 16.897 2.369
2024-12-01-22:21:40-root-INFO: grad norm: 18.278 18.130 2.322
2024-12-01-22:21:41-root-INFO: grad norm: 19.507 19.327 2.650
2024-12-01-22:21:42-root-INFO: grad norm: 20.808 20.647 2.581
2024-12-01-22:21:43-root-INFO: grad norm: 21.839 21.637 2.962
2024-12-01-22:21:44-root-INFO: grad norm: 22.771 22.594 2.836
2024-12-01-22:21:44-root-INFO: Loss Change: 184.722 -> 175.771
2024-12-01-22:21:44-root-INFO: Regularization Change: 0.000 -> 5.896
2024-12-01-22:21:44-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-01-22:21:44-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-22:21:45-root-INFO: step: 98 lr_xt 0.05482165
2024-12-01-22:21:45-root-INFO: grad norm: 20.373 20.233 2.385
2024-12-01-22:21:46-root-INFO: grad norm: 21.129 20.963 2.641
2024-12-01-22:21:47-root-INFO: grad norm: 21.761 21.568 2.893
2024-12-01-22:21:48-root-INFO: grad norm: 22.311 22.131 2.827
2024-12-01-22:21:48-root-INFO: grad norm: 22.633 22.420 3.102
2024-12-01-22:21:49-root-INFO: grad norm: 22.831 22.642 2.931
2024-12-01-22:21:50-root-INFO: grad norm: 22.848 22.625 3.184
2024-12-01-22:21:51-root-INFO: grad norm: 22.767 22.575 2.955
2024-12-01-22:21:52-root-INFO: Loss Change: 173.898 -> 168.561
2024-12-01-22:21:52-root-INFO: Regularization Change: 0.000 -> 3.173
2024-12-01-22:21:52-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-01-22:21:52-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-22:21:52-root-INFO: step: 97 lr_xt 0.05636643
2024-12-01-22:21:52-root-INFO: grad norm: 19.067 18.928 2.299
2024-12-01-22:21:53-root-INFO: grad norm: 19.138 18.981 2.445
2024-12-01-22:21:54-root-INFO: grad norm: 19.275 19.099 2.599
2024-12-01-22:21:55-root-INFO: grad norm: 19.451 19.288 2.513
2024-12-01-22:21:56-root-INFO: grad norm: 19.575 19.388 2.696
2024-12-01-22:21:57-root-INFO: grad norm: 19.679 19.511 2.564
2024-12-01-22:21:58-root-INFO: grad norm: 19.731 19.539 2.742
2024-12-01-22:21:59-root-INFO: grad norm: 19.754 19.583 2.591
2024-12-01-22:21:59-root-INFO: Loss Change: 166.222 -> 161.590
2024-12-01-22:21:59-root-INFO: Regularization Change: 0.000 -> 2.323
2024-12-01-22:21:59-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-01-22:21:59-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-22:22:00-root-INFO: step: 96 lr_xt 0.05794543
2024-12-01-22:22:00-root-INFO: grad norm: 16.922 16.801 2.018
2024-12-01-22:22:01-root-INFO: grad norm: 16.975 16.829 2.217
2024-12-01-22:22:02-root-INFO: grad norm: 17.100 16.948 2.275
2024-12-01-22:22:03-root-INFO: grad norm: 17.248 17.099 2.262
2024-12-01-22:22:04-root-INFO: grad norm: 17.382 17.222 2.356
2024-12-01-22:22:04-root-INFO: grad norm: 17.505 17.354 2.299
2024-12-01-22:22:05-root-INFO: grad norm: 17.602 17.437 2.402
2024-12-01-22:22:06-root-INFO: grad norm: 17.679 17.526 2.326
2024-12-01-22:22:07-root-INFO: Loss Change: 160.102 -> 156.398
2024-12-01-22:22:07-root-INFO: Regularization Change: 0.000 -> 1.933
2024-12-01-22:22:07-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-01-22:22:07-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-22:22:07-root-INFO: step: 95 lr_xt 0.05955910
2024-12-01-22:22:07-root-INFO: grad norm: 14.376 14.287 1.600
2024-12-01-22:22:08-root-INFO: grad norm: 14.501 14.380 1.870
2024-12-01-22:22:09-root-INFO: grad norm: 14.742 14.622 1.882
2024-12-01-22:22:10-root-INFO: grad norm: 15.025 14.898 1.954
2024-12-01-22:22:11-root-INFO: grad norm: 15.304 15.172 2.005
2024-12-01-22:22:11-root-INFO: grad norm: 15.577 15.444 2.036
2024-12-01-22:22:12-root-INFO: grad norm: 15.823 15.683 2.098
2024-12-01-22:22:13-root-INFO: grad norm: 16.049 15.910 2.107
2024-12-01-22:22:14-root-INFO: Loss Change: 154.943 -> 152.191
2024-12-01-22:22:14-root-INFO: Regularization Change: 0.000 -> 1.741
2024-12-01-22:22:14-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-01-22:22:14-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-22:22:14-root-INFO: step: 94 lr_xt 0.06120788
2024-12-01-22:22:14-root-INFO: grad norm: 13.307 13.222 1.502
2024-12-01-22:22:15-root-INFO: grad norm: 13.619 13.508 1.736
2024-12-01-22:22:16-root-INFO: grad norm: 13.990 13.877 1.773
2024-12-01-22:22:17-root-INFO: grad norm: 14.391 14.272 1.849
2024-12-01-22:22:18-root-INFO: grad norm: 14.750 14.624 1.920
2024-12-01-22:22:19-root-INFO: grad norm: 15.091 14.964 1.956
2024-12-01-22:22:20-root-INFO: grad norm: 15.380 15.246 2.031
2024-12-01-22:22:20-root-INFO: grad norm: 15.640 15.506 2.042
2024-12-01-22:22:21-root-INFO: Loss Change: 150.658 -> 148.611
2024-12-01-22:22:21-root-INFO: Regularization Change: 0.000 -> 1.614
2024-12-01-22:22:21-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-01-22:22:21-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-22:22:21-root-INFO: step: 93 lr_xt 0.06289219
2024-12-01-22:22:22-root-INFO: grad norm: 12.689 12.615 1.367
2024-12-01-22:22:23-root-INFO: grad norm: 12.764 12.667 1.572
2024-12-01-22:22:23-root-INFO: grad norm: 12.985 12.890 1.575
2024-12-01-22:22:24-root-INFO: grad norm: 13.264 13.158 1.671
2024-12-01-22:22:25-root-INFO: grad norm: 13.535 13.427 1.707
2024-12-01-22:22:26-root-INFO: grad norm: 13.808 13.695 1.763
2024-12-01-22:22:27-root-INFO: grad norm: 14.057 13.941 1.808
2024-12-01-22:22:28-root-INFO: grad norm: 14.293 14.174 1.843
2024-12-01-22:22:28-root-INFO: Loss Change: 147.046 -> 144.787
2024-12-01-22:22:28-root-INFO: Regularization Change: 0.000 -> 1.547
2024-12-01-22:22:28-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-01-22:22:28-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-22:22:29-root-INFO: step: 92 lr_xt 0.06461248
2024-12-01-22:22:29-root-INFO: grad norm: 12.112 12.030 1.406
2024-12-01-22:22:30-root-INFO: grad norm: 12.331 12.229 1.583
2024-12-01-22:22:31-root-INFO: grad norm: 12.642 12.537 1.626
2024-12-01-22:22:32-root-INFO: grad norm: 12.989 12.880 1.675
2024-12-01-22:22:33-root-INFO: grad norm: 13.329 13.214 1.746
2024-12-01-22:22:34-root-INFO: grad norm: 13.662 13.547 1.771
2024-12-01-22:22:34-root-INFO: grad norm: 13.966 13.844 1.844
2024-12-01-22:22:35-root-INFO: grad norm: 14.248 14.127 1.854
2024-12-01-22:22:36-root-INFO: Loss Change: 143.823 -> 142.033
2024-12-01-22:22:36-root-INFO: Regularization Change: 0.000 -> 1.483
2024-12-01-22:22:36-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-01-22:22:36-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-22:22:36-root-INFO: step: 91 lr_xt 0.06636917
2024-12-01-22:22:36-root-INFO: grad norm: 11.685 11.614 1.284
2024-12-01-22:22:37-root-INFO: grad norm: 11.874 11.778 1.506
2024-12-01-22:22:38-root-INFO: grad norm: 12.120 12.025 1.511
2024-12-01-22:22:39-root-INFO: grad norm: 12.403 12.299 1.600
2024-12-01-22:22:40-root-INFO: grad norm: 12.674 12.569 1.630
2024-12-01-22:22:41-root-INFO: grad norm: 12.946 12.836 1.686
2024-12-01-22:22:42-root-INFO: grad norm: 13.191 13.078 1.721
2024-12-01-22:22:42-root-INFO: grad norm: 13.423 13.307 1.758
2024-12-01-22:22:43-root-INFO: Loss Change: 140.782 -> 138.970
2024-12-01-22:22:43-root-INFO: Regularization Change: 0.000 -> 1.424
2024-12-01-22:22:43-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-01-22:22:43-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-22:22:43-root-INFO: step: 90 lr_xt 0.06816268
2024-12-01-22:22:44-root-INFO: grad norm: 11.151 11.086 1.204
2024-12-01-22:22:44-root-INFO: grad norm: 11.197 11.106 1.426
2024-12-01-22:22:45-root-INFO: grad norm: 11.336 11.251 1.386
2024-12-01-22:22:46-root-INFO: grad norm: 11.512 11.416 1.478
2024-12-01-22:22:47-root-INFO: grad norm: 11.683 11.589 1.476
2024-12-01-22:22:48-root-INFO: grad norm: 11.860 11.760 1.537
2024-12-01-22:22:49-root-INFO: grad norm: 12.021 11.922 1.543
2024-12-01-22:22:50-root-INFO: grad norm: 12.179 12.075 1.586
2024-12-01-22:22:50-root-INFO: Loss Change: 137.996 -> 135.904
2024-12-01-22:22:50-root-INFO: Regularization Change: 0.000 -> 1.406
2024-12-01-22:22:50-root-INFO: Undo step: 90
2024-12-01-22:22:50-root-INFO: Undo step: 91
2024-12-01-22:22:50-root-INFO: Undo step: 92
2024-12-01-22:22:50-root-INFO: Undo step: 93
2024-12-01-22:22:50-root-INFO: Undo step: 94
2024-12-01-22:22:51-root-INFO: step: 95 lr_xt 0.05955910
2024-12-01-22:22:51-root-INFO: grad norm: 50.329 49.198 10.607
2024-12-01-22:22:52-root-INFO: grad norm: 30.583 29.675 7.398
2024-12-01-22:22:53-root-INFO: grad norm: 23.246 22.618 5.365
2024-12-01-22:22:54-root-INFO: grad norm: 17.510 16.992 4.226
2024-12-01-22:22:54-root-INFO: grad norm: 14.690 14.295 3.386
2024-12-01-22:22:55-root-INFO: grad norm: 12.920 12.566 3.004
2024-12-01-22:22:56-root-INFO: grad norm: 11.905 11.598 2.689
2024-12-01-22:22:57-root-INFO: grad norm: 11.329 11.048 2.510
2024-12-01-22:22:58-root-INFO: Loss Change: 343.519 -> 165.116
2024-12-01-22:22:58-root-INFO: Regularization Change: 0.000 -> 54.282
2024-12-01-22:22:58-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-01-22:22:58-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-22:22:58-root-INFO: step: 94 lr_xt 0.06120788
2024-12-01-22:22:58-root-INFO: grad norm: 9.566 9.338 2.077
2024-12-01-22:22:59-root-INFO: grad norm: 9.007 8.782 2.001
2024-12-01-22:23:00-root-INFO: grad norm: 8.825 8.606 1.950
2024-12-01-22:23:01-root-INFO: grad norm: 8.822 8.618 1.887
2024-12-01-22:23:02-root-INFO: grad norm: 8.978 8.771 1.918
2024-12-01-22:23:03-root-INFO: grad norm: 9.254 9.061 1.880
2024-12-01-22:23:04-root-INFO: grad norm: 9.656 9.452 1.973
2024-12-01-22:23:05-root-INFO: grad norm: 10.143 9.953 1.955
2024-12-01-22:23:05-root-INFO: Loss Change: 164.319 -> 151.422
2024-12-01-22:23:05-root-INFO: Regularization Change: 0.000 -> 6.608
2024-12-01-22:23:05-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-01-22:23:05-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-22:23:05-root-INFO: step: 93 lr_xt 0.06289219
2024-12-01-22:23:06-root-INFO: grad norm: 8.295 8.138 1.603
2024-12-01-22:23:07-root-INFO: grad norm: 8.171 8.019 1.572
2024-12-01-22:23:08-root-INFO: grad norm: 8.462 8.302 1.638
2024-12-01-22:23:08-root-INFO: grad norm: 8.878 8.721 1.665
2024-12-01-22:23:09-root-INFO: grad norm: 9.390 9.220 1.774
2024-12-01-22:23:10-root-INFO: grad norm: 9.957 9.793 1.801
2024-12-01-22:23:11-root-INFO: grad norm: 10.594 10.415 1.940
2024-12-01-22:23:12-root-INFO: grad norm: 11.252 11.079 1.968
2024-12-01-22:23:12-root-INFO: Loss Change: 150.423 -> 145.049
2024-12-01-22:23:12-root-INFO: Regularization Change: 0.000 -> 3.338
2024-12-01-22:23:12-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-01-22:23:12-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-22:23:13-root-INFO: step: 92 lr_xt 0.06461248
2024-12-01-22:23:13-root-INFO: grad norm: 9.857 9.694 1.786
2024-12-01-22:23:14-root-INFO: grad norm: 10.064 9.909 1.756
2024-12-01-22:23:15-root-INFO: grad norm: 10.491 10.317 1.904
2024-12-01-22:23:16-root-INFO: grad norm: 11.004 10.846 1.858
2024-12-01-22:23:16-root-INFO: grad norm: 11.585 11.405 2.034
2024-12-01-22:23:17-root-INFO: grad norm: 12.196 12.031 2.002
2024-12-01-22:23:18-root-INFO: grad norm: 12.833 12.645 2.191
2024-12-01-22:23:19-root-INFO: grad norm: 13.465 13.290 2.163
2024-12-01-22:23:20-root-INFO: Loss Change: 144.396 -> 141.508
2024-12-01-22:23:20-root-INFO: Regularization Change: 0.000 -> 2.385
2024-12-01-22:23:20-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-01-22:23:20-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-22:23:20-root-INFO: step: 91 lr_xt 0.06636917
2024-12-01-22:23:20-root-INFO: grad norm: 11.134 11.001 1.715
2024-12-01-22:23:21-root-INFO: grad norm: 11.502 11.353 1.845
2024-12-01-22:23:22-root-INFO: grad norm: 11.950 11.787 1.965
2024-12-01-22:23:23-root-INFO: grad norm: 12.444 12.284 1.992
2024-12-01-22:23:24-root-INFO: grad norm: 12.919 12.742 2.126
2024-12-01-22:23:25-root-INFO: grad norm: 13.389 13.219 2.126
2024-12-01-22:23:26-root-INFO: grad norm: 13.809 13.623 2.259
2024-12-01-22:23:26-root-INFO: grad norm: 14.201 14.024 2.235
2024-12-01-22:23:27-root-INFO: Loss Change: 140.255 -> 138.211
2024-12-01-22:23:27-root-INFO: Regularization Change: 0.000 -> 1.927
2024-12-01-22:23:27-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-01-22:23:27-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-22:23:27-root-INFO: step: 90 lr_xt 0.06816268
2024-12-01-22:23:28-root-INFO: grad norm: 11.796 11.668 1.733
2024-12-01-22:23:28-root-INFO: grad norm: 11.869 11.724 1.850
2024-12-01-22:23:29-root-INFO: grad norm: 12.026 11.875 1.900
2024-12-01-22:23:30-root-INFO: grad norm: 12.222 12.072 1.907
2024-12-01-22:23:31-root-INFO: grad norm: 12.387 12.228 1.974
2024-12-01-22:23:32-root-INFO: grad norm: 12.558 12.404 1.957
2024-12-01-22:23:33-root-INFO: grad norm: 12.691 12.529 2.022
2024-12-01-22:23:34-root-INFO: grad norm: 12.819 12.664 1.991
2024-12-01-22:23:34-root-INFO: Loss Change: 137.035 -> 134.463
2024-12-01-22:23:34-root-INFO: Regularization Change: 0.000 -> 1.690
2024-12-01-22:23:34-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-01-22:23:34-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-22:23:34-root-INFO: step: 89 lr_xt 0.06999342
2024-12-01-22:23:35-root-INFO: grad norm: 9.502 9.395 1.423
2024-12-01-22:23:36-root-INFO: grad norm: 9.452 9.337 1.467
2024-12-01-22:23:36-root-INFO: grad norm: 9.611 9.490 1.518
2024-12-01-22:23:37-root-INFO: grad norm: 9.869 9.750 1.526
2024-12-01-22:23:38-root-INFO: grad norm: 10.123 9.995 1.607
2024-12-01-22:23:39-root-INFO: grad norm: 10.425 10.301 1.603
2024-12-01-22:23:40-root-INFO: grad norm: 10.697 10.562 1.689
2024-12-01-22:23:41-root-INFO: grad norm: 10.987 10.858 1.678
2024-12-01-22:23:41-root-INFO: Loss Change: 132.954 -> 130.762
2024-12-01-22:23:41-root-INFO: Regularization Change: 0.000 -> 1.597
2024-12-01-22:23:41-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-01-22:23:41-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-22:23:42-root-INFO: step: 88 lr_xt 0.07186179
2024-12-01-22:23:42-root-INFO: grad norm: 8.957 8.879 1.176
2024-12-01-22:23:43-root-INFO: grad norm: 8.973 8.871 1.349
2024-12-01-22:23:44-root-INFO: grad norm: 9.075 8.976 1.341
2024-12-01-22:23:45-root-INFO: grad norm: 9.217 9.112 1.391
2024-12-01-22:23:45-root-INFO: grad norm: 9.340 9.233 1.406
2024-12-01-22:23:46-root-INFO: grad norm: 9.476 9.367 1.433
2024-12-01-22:23:47-root-INFO: grad norm: 9.593 9.483 1.451
2024-12-01-22:23:48-root-INFO: grad norm: 9.714 9.603 1.467
2024-12-01-22:23:49-root-INFO: Loss Change: 130.191 -> 127.921
2024-12-01-22:23:49-root-INFO: Regularization Change: 0.000 -> 1.482
2024-12-01-22:23:49-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-01-22:23:49-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-22:23:49-root-INFO: step: 87 lr_xt 0.07376819
2024-12-01-22:23:49-root-INFO: grad norm: 7.160 7.079 1.076
2024-12-01-22:23:50-root-INFO: grad norm: 6.914 6.833 1.054
2024-12-01-22:23:51-root-INFO: grad norm: 7.001 6.922 1.046
2024-12-01-22:23:52-root-INFO: grad norm: 7.203 7.120 1.093
2024-12-01-22:23:53-root-INFO: grad norm: 7.387 7.302 1.118
2024-12-01-22:23:54-root-INFO: grad norm: 7.637 7.549 1.154
2024-12-01-22:23:55-root-INFO: grad norm: 7.839 7.748 1.185
2024-12-01-22:23:55-root-INFO: grad norm: 8.071 7.980 1.210
2024-12-01-22:23:56-root-INFO: Loss Change: 126.885 -> 124.711
2024-12-01-22:23:56-root-INFO: Regularization Change: 0.000 -> 1.471
2024-12-01-22:23:56-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-01-22:23:56-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-22:23:56-root-INFO: step: 86 lr_xt 0.07571301
2024-12-01-22:23:57-root-INFO: grad norm: 5.883 5.832 0.767
2024-12-01-22:23:57-root-INFO: grad norm: 5.755 5.692 0.852
2024-12-01-22:23:58-root-INFO: grad norm: 5.883 5.824 0.833
2024-12-01-22:23:59-root-INFO: grad norm: 6.130 6.063 0.902
2024-12-01-22:24:00-root-INFO: grad norm: 6.324 6.256 0.921
2024-12-01-22:24:01-root-INFO: grad norm: 6.573 6.503 0.963
2024-12-01-22:24:02-root-INFO: grad norm: 6.777 6.704 0.989
2024-12-01-22:24:03-root-INFO: grad norm: 7.002 6.928 1.017
2024-12-01-22:24:03-root-INFO: Loss Change: 124.100 -> 122.088
2024-12-01-22:24:03-root-INFO: Regularization Change: 0.000 -> 1.429
2024-12-01-22:24:03-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-01-22:24:03-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-22:24:04-root-INFO: step: 85 lr_xt 0.07769664
2024-12-01-22:24:04-root-INFO: grad norm: 5.385 5.336 0.730
2024-12-01-22:24:05-root-INFO: grad norm: 5.285 5.229 0.768
2024-12-01-22:24:06-root-INFO: grad norm: 5.459 5.406 0.761
2024-12-01-22:24:07-root-INFO: grad norm: 5.778 5.717 0.833
2024-12-01-22:24:07-root-INFO: grad norm: 5.980 5.918 0.859
2024-12-01-22:24:08-root-INFO: grad norm: 6.213 6.148 0.897
2024-12-01-22:24:09-root-INFO: grad norm: 6.438 6.370 0.926
2024-12-01-22:24:10-root-INFO: grad norm: 6.681 6.612 0.958
2024-12-01-22:24:11-root-INFO: Loss Change: 121.574 -> 119.662
2024-12-01-22:24:11-root-INFO: Regularization Change: 0.000 -> 1.429
2024-12-01-22:24:11-root-INFO: Undo step: 85
2024-12-01-22:24:11-root-INFO: Undo step: 86
2024-12-01-22:24:11-root-INFO: Undo step: 87
2024-12-01-22:24:11-root-INFO: Undo step: 88
2024-12-01-22:24:11-root-INFO: Undo step: 89
2024-12-01-22:24:11-root-INFO: step: 90 lr_xt 0.06816268
2024-12-01-22:24:11-root-INFO: grad norm: 49.319 48.655 8.067
2024-12-01-22:24:12-root-INFO: grad norm: 26.776 26.325 4.893
2024-12-01-22:24:13-root-INFO: grad norm: 18.713 18.331 3.763
2024-12-01-22:24:14-root-INFO: grad norm: 15.997 15.679 3.176
2024-12-01-22:24:15-root-INFO: grad norm: 14.661 14.392 2.794
2024-12-01-22:24:15-root-INFO: grad norm: 14.080 13.844 2.566
2024-12-01-22:24:16-root-INFO: grad norm: 13.851 13.637 2.425
2024-12-01-22:24:17-root-INFO: grad norm: 13.872 13.674 2.338
2024-12-01-22:24:18-root-INFO: Loss Change: 312.424 -> 147.817
2024-12-01-22:24:18-root-INFO: Regularization Change: 0.000 -> 53.951
2024-12-01-22:24:18-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-01-22:24:18-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-22:24:18-root-INFO: step: 89 lr_xt 0.06999342
2024-12-01-22:24:18-root-INFO: grad norm: 10.881 10.739 1.754
2024-12-01-22:24:19-root-INFO: grad norm: 10.868 10.716 1.812
2024-12-01-22:24:20-root-INFO: grad norm: 11.113 10.967 1.792
2024-12-01-22:24:21-root-INFO: grad norm: 11.497 11.349 1.834
2024-12-01-22:24:22-root-INFO: grad norm: 11.934 11.784 1.886
2024-12-01-22:24:23-root-INFO: grad norm: 12.430 12.279 1.933
2024-12-01-22:24:24-root-INFO: grad norm: 12.940 12.782 2.021
2024-12-01-22:24:25-root-INFO: grad norm: 13.463 13.304 2.064
2024-12-01-22:24:25-root-INFO: Loss Change: 146.006 -> 136.105
2024-12-01-22:24:25-root-INFO: Regularization Change: 0.000 -> 6.433
2024-12-01-22:24:26-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-01-22:24:26-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-22:24:26-root-INFO: step: 88 lr_xt 0.07186179
2024-12-01-22:24:26-root-INFO: grad norm: 11.828 11.718 1.606
2024-12-01-22:24:27-root-INFO: grad norm: 11.984 11.849 1.794
2024-12-01-22:24:28-root-INFO: grad norm: 12.220 12.086 1.810
2024-12-01-22:24:29-root-INFO: grad norm: 12.469 12.329 1.862
2024-12-01-22:24:30-root-INFO: grad norm: 12.706 12.560 1.915
2024-12-01-22:24:30-root-INFO: grad norm: 12.918 12.773 1.928
2024-12-01-22:24:31-root-INFO: grad norm: 13.098 12.946 1.991
2024-12-01-22:24:32-root-INFO: grad norm: 13.241 13.093 1.975
2024-12-01-22:24:33-root-INFO: Loss Change: 135.358 -> 130.491
2024-12-01-22:24:33-root-INFO: Regularization Change: 0.000 -> 3.267
2024-12-01-22:24:33-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-01-22:24:33-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-22:24:33-root-INFO: step: 87 lr_xt 0.07376819
2024-12-01-22:24:34-root-INFO: grad norm: 10.232 10.134 1.416
2024-12-01-22:24:35-root-INFO: grad norm: 10.142 10.025 1.532
2024-12-01-22:24:35-root-INFO: grad norm: 10.257 10.143 1.526
2024-12-01-22:24:36-root-INFO: grad norm: 10.450 10.332 1.563
2024-12-01-22:24:37-root-INFO: grad norm: 10.616 10.492 1.615
2024-12-01-22:24:38-root-INFO: grad norm: 10.805 10.684 1.617
2024-12-01-22:24:39-root-INFO: grad norm: 10.958 10.829 1.678
2024-12-01-22:24:40-root-INFO: grad norm: 11.110 10.985 1.661
2024-12-01-22:24:40-root-INFO: Loss Change: 128.857 -> 125.267
2024-12-01-22:24:40-root-INFO: Regularization Change: 0.000 -> 2.380
2024-12-01-22:24:40-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-01-22:24:40-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-22:24:41-root-INFO: step: 86 lr_xt 0.07571301
2024-12-01-22:24:41-root-INFO: grad norm: 8.539 8.468 1.099
2024-12-01-22:24:42-root-INFO: grad norm: 8.467 8.374 1.248
2024-12-01-22:24:43-root-INFO: grad norm: 8.544 8.452 1.249
2024-12-01-22:24:44-root-INFO: grad norm: 8.708 8.612 1.284
2024-12-01-22:24:45-root-INFO: grad norm: 8.836 8.735 1.327
2024-12-01-22:24:45-root-INFO: grad norm: 8.994 8.896 1.331
2024-12-01-22:24:46-root-INFO: grad norm: 9.126 9.021 1.379
2024-12-01-22:24:47-root-INFO: grad norm: 9.264 9.162 1.368
2024-12-01-22:24:48-root-INFO: Loss Change: 124.217 -> 121.291
2024-12-01-22:24:48-root-INFO: Regularization Change: 0.000 -> 1.967
2024-12-01-22:24:48-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-01-22:24:48-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-22:24:48-root-INFO: step: 85 lr_xt 0.07769664
2024-12-01-22:24:48-root-INFO: grad norm: 7.186 7.126 0.925
2024-12-01-22:24:49-root-INFO: grad norm: 7.102 7.023 1.056
2024-12-01-22:24:50-root-INFO: grad norm: 7.223 7.148 1.039
2024-12-01-22:24:51-root-INFO: grad norm: 7.453 7.372 1.102
2024-12-01-22:24:52-root-INFO: grad norm: 7.612 7.527 1.130
2024-12-01-22:24:53-root-INFO: grad norm: 7.795 7.709 1.154
2024-12-01-22:24:53-root-INFO: grad norm: 7.960 7.871 1.189
2024-12-01-22:24:54-root-INFO: grad norm: 8.132 8.043 1.200
2024-12-01-22:24:55-root-INFO: Loss Change: 120.490 -> 117.981
2024-12-01-22:24:55-root-INFO: Regularization Change: 0.000 -> 1.780
2024-12-01-22:24:55-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-01-22:24:55-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-22:24:55-root-INFO: step: 84 lr_xt 0.07971945
2024-12-01-22:24:55-root-INFO: grad norm: 6.567 6.514 0.831
2024-12-01-22:24:56-root-INFO: grad norm: 6.598 6.526 0.971
2024-12-01-22:24:57-root-INFO: grad norm: 6.747 6.676 0.973
2024-12-01-22:24:58-root-INFO: grad norm: 6.983 6.908 1.020
2024-12-01-22:24:59-root-INFO: grad norm: 7.135 7.057 1.052
2024-12-01-22:25:00-root-INFO: grad norm: 7.296 7.218 1.065
2024-12-01-22:25:01-root-INFO: grad norm: 7.454 7.372 1.101
2024-12-01-22:25:02-root-INFO: grad norm: 7.619 7.538 1.108
2024-12-01-22:25:02-root-INFO: Loss Change: 117.318 -> 115.124
2024-12-01-22:25:02-root-INFO: Regularization Change: 0.000 -> 1.637
2024-12-01-22:25:02-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-01-22:25:02-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-22:25:03-root-INFO: step: 83 lr_xt 0.08178179
2024-12-01-22:25:03-root-INFO: grad norm: 5.760 5.714 0.727
2024-12-01-22:25:04-root-INFO: grad norm: 5.617 5.558 0.811
2024-12-01-22:25:04-root-INFO: grad norm: 5.755 5.697 0.811
2024-12-01-22:25:05-root-INFO: grad norm: 6.043 5.981 0.864
2024-12-01-22:25:06-root-INFO: grad norm: 6.187 6.121 0.903
2024-12-01-22:25:07-root-INFO: grad norm: 6.337 6.272 0.911
2024-12-01-22:25:08-root-INFO: grad norm: 6.507 6.437 0.952
2024-12-01-22:25:09-root-INFO: grad norm: 6.693 6.624 0.958
2024-12-01-22:25:09-root-INFO: Loss Change: 114.508 -> 112.377
2024-12-01-22:25:09-root-INFO: Regularization Change: 0.000 -> 1.590
2024-12-01-22:25:09-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-01-22:25:09-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-22:25:10-root-INFO: step: 82 lr_xt 0.08388403
2024-12-01-22:25:10-root-INFO: grad norm: 5.429 5.390 0.651
2024-12-01-22:25:11-root-INFO: grad norm: 5.465 5.412 0.760
2024-12-01-22:25:12-root-INFO: grad norm: 5.597 5.544 0.769
2024-12-01-22:25:12-root-INFO: grad norm: 5.846 5.789 0.812
2024-12-01-22:25:13-root-INFO: grad norm: 5.952 5.892 0.843
2024-12-01-22:25:14-root-INFO: grad norm: 6.046 5.986 0.848
2024-12-01-22:25:15-root-INFO: grad norm: 6.188 6.125 0.879
2024-12-01-22:25:16-root-INFO: grad norm: 6.353 6.291 0.890
2024-12-01-22:25:16-root-INFO: Loss Change: 112.091 -> 110.125
2024-12-01-22:25:16-root-INFO: Regularization Change: 0.000 -> 1.523
2024-12-01-22:25:16-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-01-22:25:16-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-22:25:17-root-INFO: step: 81 lr_xt 0.08602650
2024-12-01-22:25:17-root-INFO: grad norm: 4.941 4.901 0.626
2024-12-01-22:25:18-root-INFO: grad norm: 4.763 4.716 0.665
2024-12-01-22:25:19-root-INFO: grad norm: 4.897 4.855 0.644
2024-12-01-22:25:20-root-INFO: grad norm: 5.229 5.179 0.720
2024-12-01-22:25:21-root-INFO: grad norm: 5.308 5.256 0.736
2024-12-01-22:25:21-root-INFO: grad norm: 5.345 5.292 0.746
2024-12-01-22:25:22-root-INFO: grad norm: 5.502 5.448 0.768
2024-12-01-22:25:23-root-INFO: grad norm: 5.711 5.656 0.791
2024-12-01-22:25:24-root-INFO: Loss Change: 109.584 -> 107.610
2024-12-01-22:25:24-root-INFO: Regularization Change: 0.000 -> 1.520
2024-12-01-22:25:24-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-01-22:25:24-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-22:25:24-root-INFO: step: 80 lr_xt 0.08820955
2024-12-01-22:25:24-root-INFO: grad norm: 4.270 4.221 0.646
2024-12-01-22:25:25-root-INFO: grad norm: 4.030 3.990 0.567
2024-12-01-22:25:26-root-INFO: grad norm: 4.141 4.104 0.557
2024-12-01-22:25:27-root-INFO: grad norm: 4.512 4.470 0.611
2024-12-01-22:25:28-root-INFO: grad norm: 4.539 4.493 0.643
2024-12-01-22:25:29-root-INFO: grad norm: 4.493 4.449 0.627
2024-12-01-22:25:30-root-INFO: grad norm: 4.663 4.615 0.665
2024-12-01-22:25:31-root-INFO: grad norm: 4.931 4.884 0.680
2024-12-01-22:25:31-root-INFO: Loss Change: 107.336 -> 105.392
2024-12-01-22:25:31-root-INFO: Regularization Change: 0.000 -> 1.518
2024-12-01-22:25:31-root-INFO: Undo step: 80
2024-12-01-22:25:31-root-INFO: Undo step: 81
2024-12-01-22:25:31-root-INFO: Undo step: 82
2024-12-01-22:25:31-root-INFO: Undo step: 83
2024-12-01-22:25:31-root-INFO: Undo step: 84
2024-12-01-22:25:31-root-INFO: step: 85 lr_xt 0.07769664
2024-12-01-22:25:32-root-INFO: grad norm: 43.038 42.540 6.529
2024-12-01-22:25:32-root-INFO: grad norm: 24.184 23.744 4.590
2024-12-01-22:25:33-root-INFO: grad norm: 17.405 17.065 3.425
2024-12-01-22:25:34-root-INFO: grad norm: 13.844 13.552 2.830
2024-12-01-22:25:35-root-INFO: grad norm: 11.989 11.782 2.214
2024-12-01-22:25:36-root-INFO: grad norm: 10.653 10.459 2.022
2024-12-01-22:25:37-root-INFO: grad norm: 9.605 9.465 1.637
2024-12-01-22:25:38-root-INFO: grad norm: 8.855 8.711 1.586
2024-12-01-22:25:38-root-INFO: Loss Change: 295.377 -> 130.352
2024-12-01-22:25:38-root-INFO: Regularization Change: 0.000 -> 62.610
2024-12-01-22:25:38-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-01-22:25:38-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-22:25:39-root-INFO: step: 84 lr_xt 0.07971945
2024-12-01-22:25:39-root-INFO: grad norm: 7.381 7.285 1.181
2024-12-01-22:25:40-root-INFO: grad norm: 6.758 6.653 1.189
2024-12-01-22:25:41-root-INFO: grad norm: 6.401 6.312 1.063
2024-12-01-22:25:41-root-INFO: grad norm: 6.163 6.069 1.070
2024-12-01-22:25:42-root-INFO: grad norm: 6.048 5.964 1.007
2024-12-01-22:25:43-root-INFO: grad norm: 6.100 6.013 1.028
2024-12-01-22:25:44-root-INFO: grad norm: 6.175 6.091 1.016
2024-12-01-22:25:45-root-INFO: grad norm: 6.450 6.365 1.043
2024-12-01-22:25:46-root-INFO: Loss Change: 129.744 -> 119.211
2024-12-01-22:25:46-root-INFO: Regularization Change: 0.000 -> 6.749
2024-12-01-22:25:46-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-01-22:25:46-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-22:25:46-root-INFO: step: 83 lr_xt 0.08178179
2024-12-01-22:25:46-root-INFO: grad norm: 5.076 5.019 0.762
2024-12-01-22:25:47-root-INFO: grad norm: 4.745 4.679 0.786
2024-12-01-22:25:48-root-INFO: grad norm: 4.785 4.721 0.780
2024-12-01-22:25:49-root-INFO: grad norm: 5.007 4.943 0.795
2024-12-01-22:25:50-root-INFO: grad norm: 5.057 4.988 0.829
2024-12-01-22:25:51-root-INFO: grad norm: 5.088 5.025 0.800
2024-12-01-22:25:52-root-INFO: grad norm: 5.231 5.163 0.846
2024-12-01-22:25:52-root-INFO: grad norm: 5.442 5.378 0.830
2024-12-01-22:25:53-root-INFO: Loss Change: 118.692 -> 113.723
2024-12-01-22:25:53-root-INFO: Regularization Change: 0.000 -> 3.386
2024-12-01-22:25:53-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-01-22:25:53-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-22:25:53-root-INFO: step: 82 lr_xt 0.08388403
2024-12-01-22:25:54-root-INFO: grad norm: 4.383 4.339 0.622
2024-12-01-22:25:55-root-INFO: grad norm: 4.312 4.261 0.662
2024-12-01-22:25:55-root-INFO: grad norm: 4.453 4.398 0.694
2024-12-01-22:25:56-root-INFO: grad norm: 4.749 4.698 0.698
2024-12-01-22:25:57-root-INFO: grad norm: 4.848 4.789 0.756
2024-12-01-22:25:58-root-INFO: grad norm: 4.925 4.872 0.723
2024-12-01-22:25:59-root-INFO: grad norm: 5.101 5.040 0.783
2024-12-01-22:26:00-root-INFO: grad norm: 5.345 5.290 0.768
2024-12-01-22:26:01-root-INFO: Loss Change: 113.561 -> 110.237
2024-12-01-22:26:01-root-INFO: Regularization Change: 0.000 -> 2.434
2024-12-01-22:26:01-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-01-22:26:01-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-22:26:01-root-INFO: step: 81 lr_xt 0.08602650
2024-12-01-22:26:01-root-INFO: grad norm: 4.256 4.212 0.612
2024-12-01-22:26:02-root-INFO: grad norm: 4.001 3.956 0.599
2024-12-01-22:26:03-root-INFO: grad norm: 4.133 4.088 0.605
2024-12-01-22:26:04-root-INFO: grad norm: 4.507 4.462 0.638
2024-12-01-22:26:05-root-INFO: grad norm: 4.548 4.496 0.684
2024-12-01-22:26:06-root-INFO: grad norm: 4.501 4.454 0.649
2024-12-01-22:26:06-root-INFO: grad norm: 4.697 4.644 0.703
2024-12-01-22:26:07-root-INFO: grad norm: 5.042 4.993 0.704
2024-12-01-22:26:08-root-INFO: Loss Change: 109.752 -> 107.036
2024-12-01-22:26:08-root-INFO: Regularization Change: 0.000 -> 2.036
2024-12-01-22:26:08-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-01-22:26:08-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-22:26:08-root-INFO: step: 80 lr_xt 0.08820955
2024-12-01-22:26:08-root-INFO: grad norm: 4.067 4.015 0.649
2024-12-01-22:26:09-root-INFO: grad norm: 3.755 3.716 0.540
2024-12-01-22:26:10-root-INFO: grad norm: 3.824 3.784 0.554
2024-12-01-22:26:11-root-INFO: grad norm: 4.183 4.145 0.565
2024-12-01-22:26:12-root-INFO: grad norm: 4.122 4.075 0.622
2024-12-01-22:26:13-root-INFO: grad norm: 3.903 3.863 0.556
2024-12-01-22:26:14-root-INFO: grad norm: 4.096 4.048 0.622
2024-12-01-22:26:15-root-INFO: grad norm: 4.555 4.513 0.619
2024-12-01-22:26:15-root-INFO: Loss Change: 106.861 -> 104.443
2024-12-01-22:26:15-root-INFO: Regularization Change: 0.000 -> 1.832
2024-12-01-22:26:15-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-01-22:26:15-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-22:26:16-root-INFO: step: 79 lr_xt 0.09043348
2024-12-01-22:26:16-root-INFO: grad norm: 4.169 4.121 0.633
2024-12-01-22:26:17-root-INFO: grad norm: 3.607 3.565 0.546
2024-12-01-22:26:18-root-INFO: grad norm: 3.745 3.706 0.539
2024-12-01-22:26:18-root-INFO: grad norm: 4.364 4.325 0.586
2024-12-01-22:26:19-root-INFO: grad norm: 4.136 4.089 0.617
2024-12-01-22:26:20-root-INFO: grad norm: 3.525 3.485 0.530
2024-12-01-22:26:21-root-INFO: grad norm: 3.824 3.780 0.578
2024-12-01-22:26:22-root-INFO: grad norm: 4.769 4.727 0.632
2024-12-01-22:26:22-root-INFO: Loss too large (101.943->102.017)! Learning rate decreased to 0.07235.
2024-12-01-22:26:23-root-INFO: Loss Change: 104.216 -> 101.771
2024-12-01-22:26:23-root-INFO: Regularization Change: 0.000 -> 1.636
2024-12-01-22:26:23-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-01-22:26:23-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-22:26:23-root-INFO: step: 78 lr_xt 0.09269861
2024-12-01-22:26:23-root-INFO: grad norm: 3.711 3.668 0.567
2024-12-01-22:26:24-root-INFO: grad norm: 3.866 3.831 0.521
2024-12-01-22:26:25-root-INFO: grad norm: 3.792 3.753 0.542
2024-12-01-22:26:26-root-INFO: grad norm: 3.673 3.639 0.502
2024-12-01-22:26:27-root-INFO: grad norm: 3.744 3.702 0.555
2024-12-01-22:26:28-root-INFO: grad norm: 3.967 3.932 0.531
2024-12-01-22:26:28-root-INFO: Loss too large (100.343->100.345)! Learning rate decreased to 0.07416.
2024-12-01-22:26:29-root-INFO: grad norm: 3.332 3.294 0.502
2024-12-01-22:26:30-root-INFO: grad norm: 2.521 2.489 0.398
2024-12-01-22:26:30-root-INFO: Loss Change: 101.892 -> 99.591
2024-12-01-22:26:30-root-INFO: Regularization Change: 0.000 -> 1.404
2024-12-01-22:26:30-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-01-22:26:30-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-22:26:31-root-INFO: step: 77 lr_xt 0.09500525
2024-12-01-22:26:31-root-INFO: grad norm: 3.181 3.114 0.645
2024-12-01-22:26:32-root-INFO: grad norm: 2.990 2.956 0.453
2024-12-01-22:26:33-root-INFO: grad norm: 3.240 3.210 0.436
2024-12-01-22:26:33-root-INFO: grad norm: 4.471 4.442 0.505
2024-12-01-22:26:34-root-INFO: Loss too large (98.625->98.743)! Learning rate decreased to 0.07600.
2024-12-01-22:26:35-root-INFO: grad norm: 3.639 3.606 0.494
2024-12-01-22:26:36-root-INFO: grad norm: 2.473 2.445 0.373
2024-12-01-22:26:36-root-INFO: grad norm: 2.519 2.488 0.394
2024-12-01-22:26:37-root-INFO: grad norm: 2.740 2.713 0.384
2024-12-01-22:26:38-root-INFO: Loss Change: 99.414 -> 97.475
2024-12-01-22:26:38-root-INFO: Regularization Change: 0.000 -> 1.259
2024-12-01-22:26:38-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-01-22:26:38-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-22:26:38-root-INFO: step: 76 lr_xt 0.09735366
2024-12-01-22:26:38-root-INFO: grad norm: 3.311 3.248 0.647
2024-12-01-22:26:39-root-INFO: grad norm: 3.193 3.162 0.448
2024-12-01-22:26:40-root-INFO: grad norm: 3.528 3.499 0.450
2024-12-01-22:26:41-root-INFO: grad norm: 5.038 5.009 0.537
2024-12-01-22:26:41-root-INFO: Loss too large (96.814->97.053)! Learning rate decreased to 0.07788.
2024-12-01-22:26:42-root-INFO: grad norm: 3.811 3.777 0.509
2024-12-01-22:26:43-root-INFO: grad norm: 2.063 2.034 0.345
2024-12-01-22:26:44-root-INFO: grad norm: 2.065 2.035 0.349
2024-12-01-22:26:45-root-INFO: grad norm: 2.272 2.245 0.349
2024-12-01-22:26:45-root-INFO: Loss Change: 97.538 -> 95.565
2024-12-01-22:26:45-root-INFO: Regularization Change: 0.000 -> 1.253
2024-12-01-22:26:45-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-01-22:26:45-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-22:26:46-root-INFO: step: 75 lr_xt 0.09974414
2024-12-01-22:26:46-root-INFO: grad norm: 3.147 3.096 0.565
2024-12-01-22:26:47-root-INFO: grad norm: 3.552 3.525 0.437
2024-12-01-22:26:48-root-INFO: grad norm: 3.857 3.829 0.466
2024-12-01-22:26:49-root-INFO: grad norm: 5.038 5.010 0.525
2024-12-01-22:26:49-root-INFO: Loss too large (94.993->95.281)! Learning rate decreased to 0.07980.
2024-12-01-22:26:49-root-INFO: Loss too large (94.993->94.995)! Learning rate decreased to 0.06384.
2024-12-01-22:26:50-root-INFO: grad norm: 3.374 3.343 0.452
2024-12-01-22:26:50-root-INFO: grad norm: 1.802 1.774 0.318
2024-12-01-22:26:51-root-INFO: grad norm: 1.719 1.690 0.315
2024-12-01-22:26:52-root-INFO: grad norm: 1.667 1.638 0.306
2024-12-01-22:26:52-root-INFO: Loss Change: 95.655 -> 93.887
2024-12-01-22:26:52-root-INFO: Regularization Change: 0.000 -> 0.986
2024-12-01-22:26:52-root-INFO: Undo step: 75
2024-12-01-22:26:52-root-INFO: Undo step: 76
2024-12-01-22:26:52-root-INFO: Undo step: 77
2024-12-01-22:26:52-root-INFO: Undo step: 78
2024-12-01-22:26:52-root-INFO: Undo step: 79
2024-12-01-22:26:52-root-INFO: step: 80 lr_xt 0.08820955
2024-12-01-22:26:53-root-INFO: grad norm: 37.652 37.306 5.092
2024-12-01-22:26:53-root-INFO: grad norm: 19.592 19.346 3.099
2024-12-01-22:26:54-root-INFO: grad norm: 13.767 13.545 2.464
2024-12-01-22:26:55-root-INFO: grad norm: 11.351 11.186 1.926
2024-12-01-22:26:55-root-INFO: grad norm: 10.267 10.120 1.734
2024-12-01-22:26:56-root-INFO: grad norm: 9.898 9.777 1.547
2024-12-01-22:26:57-root-INFO: grad norm: 9.919 9.807 1.487
2024-12-01-22:26:57-root-INFO: grad norm: 10.136 10.033 1.445
2024-12-01-22:26:58-root-INFO: Loss Change: 254.594 -> 117.700
2024-12-01-22:26:58-root-INFO: Regularization Change: 0.000 -> 58.721
2024-12-01-22:26:58-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-01-22:26:58-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-22:26:58-root-INFO: step: 79 lr_xt 0.09043348
2024-12-01-22:26:58-root-INFO: grad norm: 8.883 8.813 1.112
2024-12-01-22:26:59-root-INFO: grad norm: 8.948 8.862 1.243
2024-12-01-22:26:59-root-INFO: grad norm: 9.152 9.067 1.243
2024-12-01-22:27:00-root-INFO: grad norm: 9.512 9.420 1.321
2024-12-01-22:27:01-root-INFO: grad norm: 9.741 9.645 1.364
2024-12-01-22:27:01-root-INFO: grad norm: 10.003 9.906 1.391
2024-12-01-22:27:02-root-INFO: grad norm: 10.086 9.983 1.436
2024-12-01-22:27:02-root-INFO: grad norm: 10.097 9.999 1.403
2024-12-01-22:27:03-root-INFO: Loss Change: 116.721 -> 107.933
2024-12-01-22:27:03-root-INFO: Regularization Change: 0.000 -> 7.043
2024-12-01-22:27:03-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-01-22:27:03-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-22:27:03-root-INFO: step: 78 lr_xt 0.09269861
2024-12-01-22:27:03-root-INFO: grad norm: 8.530 8.466 1.041
2024-12-01-22:27:04-root-INFO: grad norm: 8.575 8.492 1.190
2024-12-01-22:27:05-root-INFO: grad norm: 8.563 8.480 1.190
2024-12-01-22:27:05-root-INFO: grad norm: 8.589 8.506 1.190
2024-12-01-22:27:06-root-INFO: grad norm: 8.577 8.491 1.217
2024-12-01-22:27:07-root-INFO: grad norm: 8.560 8.477 1.184
2024-12-01-22:27:07-root-INFO: grad norm: 8.535 8.448 1.217
2024-12-01-22:27:08-root-INFO: grad norm: 8.513 8.432 1.174
2024-12-01-22:27:08-root-INFO: Loss Change: 107.318 -> 102.489
2024-12-01-22:27:08-root-INFO: Regularization Change: 0.000 -> 3.677
2024-12-01-22:27:08-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-01-22:27:08-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-22:27:09-root-INFO: step: 77 lr_xt 0.09500525
2024-12-01-22:27:09-root-INFO: grad norm: 6.740 6.691 0.811
2024-12-01-22:27:10-root-INFO: grad norm: 6.688 6.624 0.921
2024-12-01-22:27:10-root-INFO: grad norm: 6.641 6.574 0.939
2024-12-01-22:27:11-root-INFO: grad norm: 6.649 6.586 0.915
2024-12-01-22:27:11-root-INFO: grad norm: 6.646 6.576 0.964
2024-12-01-22:27:12-root-INFO: grad norm: 6.652 6.588 0.917
2024-12-01-22:27:12-root-INFO: grad norm: 6.669 6.598 0.970
2024-12-01-22:27:13-root-INFO: grad norm: 6.703 6.639 0.923
2024-12-01-22:27:13-root-INFO: Loss Change: 101.497 -> 98.055
2024-12-01-22:27:13-root-INFO: Regularization Change: 0.000 -> 2.664
2024-12-01-22:27:13-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-01-22:27:13-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-22:27:14-root-INFO: step: 76 lr_xt 0.09735366
2024-12-01-22:27:14-root-INFO: grad norm: 5.245 5.206 0.635
2024-12-01-22:27:15-root-INFO: grad norm: 5.237 5.188 0.712
2024-12-01-22:27:15-root-INFO: grad norm: 5.280 5.229 0.729
2024-12-01-22:27:16-root-INFO: grad norm: 5.441 5.390 0.738
2024-12-01-22:27:16-root-INFO: grad norm: 5.457 5.401 0.780
2024-12-01-22:27:17-root-INFO: grad norm: 5.423 5.372 0.746
2024-12-01-22:27:18-root-INFO: grad norm: 5.529 5.472 0.792
2024-12-01-22:27:18-root-INFO: grad norm: 5.726 5.673 0.777
2024-12-01-22:27:19-root-INFO: Loss Change: 97.505 -> 94.887
2024-12-01-22:27:19-root-INFO: Regularization Change: 0.000 -> 2.217
2024-12-01-22:27:19-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-01-22:27:19-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-22:27:19-root-INFO: step: 75 lr_xt 0.09974414
2024-12-01-22:27:19-root-INFO: grad norm: 4.654 4.621 0.552
2024-12-01-22:27:20-root-INFO: grad norm: 4.390 4.347 0.616
2024-12-01-22:27:20-root-INFO: grad norm: 4.587 4.544 0.628
2024-12-01-22:27:21-root-INFO: grad norm: 5.116 5.072 0.672
2024-12-01-22:27:21-root-INFO: Loss too large (93.396->93.428)! Learning rate decreased to 0.07980.
2024-12-01-22:27:22-root-INFO: grad norm: 3.948 3.906 0.575
2024-12-01-22:27:23-root-INFO: grad norm: 2.777 2.743 0.437
2024-12-01-22:27:23-root-INFO: grad norm: 2.562 2.528 0.418
2024-12-01-22:27:24-root-INFO: grad norm: 2.473 2.442 0.391
2024-12-01-22:27:24-root-INFO: Loss Change: 94.586 -> 91.823
2024-12-01-22:27:24-root-INFO: Regularization Change: 0.000 -> 1.565
2024-12-01-22:27:24-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-01-22:27:24-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-22:27:25-root-INFO: step: 74 lr_xt 0.10217692
2024-12-01-22:27:25-root-INFO: grad norm: 2.821 2.761 0.578
2024-12-01-22:27:25-root-INFO: grad norm: 2.546 2.516 0.391
2024-12-01-22:27:26-root-INFO: grad norm: 2.695 2.666 0.393
2024-12-01-22:27:27-root-INFO: grad norm: 3.518 3.493 0.418
2024-12-01-22:27:27-root-INFO: grad norm: 3.676 3.644 0.480
2024-12-01-22:27:28-root-INFO: grad norm: 4.100 4.072 0.479
2024-12-01-22:27:28-root-INFO: Loss too large (90.492->90.561)! Learning rate decreased to 0.08174.
2024-12-01-22:27:29-root-INFO: grad norm: 3.438 3.405 0.471
2024-12-01-22:27:29-root-INFO: grad norm: 2.503 2.475 0.378
2024-12-01-22:27:30-root-INFO: Loss Change: 91.887 -> 89.702
2024-12-01-22:27:30-root-INFO: Regularization Change: 0.000 -> 1.620
2024-12-01-22:27:30-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-01-22:27:30-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-22:27:30-root-INFO: step: 73 lr_xt 0.10465226
2024-12-01-22:27:30-root-INFO: grad norm: 3.095 3.028 0.643
2024-12-01-22:27:31-root-INFO: grad norm: 3.000 2.970 0.419
2024-12-01-22:27:31-root-INFO: grad norm: 3.323 3.296 0.421
2024-12-01-22:27:32-root-INFO: grad norm: 4.653 4.627 0.490
2024-12-01-22:27:32-root-INFO: Loss too large (88.979->89.167)! Learning rate decreased to 0.08372.
2024-12-01-22:27:33-root-INFO: grad norm: 3.701 3.670 0.472
2024-12-01-22:27:34-root-INFO: grad norm: 2.334 2.306 0.362
2024-12-01-22:27:34-root-INFO: grad norm: 2.394 2.367 0.361
2024-12-01-22:27:35-root-INFO: grad norm: 2.597 2.571 0.367
2024-12-01-22:27:35-root-INFO: Loss Change: 89.743 -> 87.751
2024-12-01-22:27:35-root-INFO: Regularization Change: 0.000 -> 1.414
2024-12-01-22:27:35-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-01-22:27:35-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-22:27:35-root-INFO: step: 72 lr_xt 0.10717038
2024-12-01-22:27:36-root-INFO: grad norm: 3.207 3.153 0.585
2024-12-01-22:27:36-root-INFO: grad norm: 3.633 3.608 0.424
2024-12-01-22:27:37-root-INFO: grad norm: 3.854 3.828 0.447
2024-12-01-22:27:37-root-INFO: grad norm: 4.737 4.711 0.489
2024-12-01-22:27:38-root-INFO: Loss too large (86.989->87.237)! Learning rate decreased to 0.08574.
2024-12-01-22:27:38-root-INFO: grad norm: 3.738 3.708 0.473
2024-12-01-22:27:39-root-INFO: grad norm: 2.269 2.242 0.349
2024-12-01-22:27:40-root-INFO: grad norm: 2.368 2.342 0.350
2024-12-01-22:27:40-root-INFO: grad norm: 2.663 2.639 0.359
2024-12-01-22:27:41-root-INFO: Loss Change: 87.718 -> 85.806
2024-12-01-22:27:41-root-INFO: Regularization Change: 0.000 -> 1.389
2024-12-01-22:27:41-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-01-22:27:41-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-22:27:41-root-INFO: step: 71 lr_xt 0.10973151
2024-12-01-22:27:41-root-INFO: grad norm: 2.848 2.819 0.408
2024-12-01-22:27:42-root-INFO: grad norm: 4.057 4.034 0.436
2024-12-01-22:27:42-root-INFO: Loss too large (85.696->85.874)! Learning rate decreased to 0.08779.
2024-12-01-22:27:43-root-INFO: grad norm: 3.600 3.571 0.449
2024-12-01-22:27:43-root-INFO: grad norm: 2.915 2.890 0.379
2024-12-01-22:27:44-root-INFO: grad norm: 3.011 2.984 0.400
2024-12-01-22:27:45-root-INFO: grad norm: 3.233 3.209 0.394
2024-12-01-22:27:45-root-INFO: grad norm: 3.256 3.229 0.424
2024-12-01-22:27:46-root-INFO: grad norm: 3.296 3.271 0.400
2024-12-01-22:27:46-root-INFO: Loss Change: 85.890 -> 84.357
2024-12-01-22:27:46-root-INFO: Regularization Change: 0.000 -> 1.195
2024-12-01-22:27:46-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-01-22:27:46-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-22:27:46-root-INFO: step: 70 lr_xt 0.11233583
2024-12-01-22:27:47-root-INFO: grad norm: 3.687 3.620 0.703
2024-12-01-22:27:47-root-INFO: grad norm: 4.306 4.281 0.455
2024-12-01-22:27:47-root-INFO: Loss too large (83.868->84.040)! Learning rate decreased to 0.08987.
2024-12-01-22:27:48-root-INFO: grad norm: 3.682 3.658 0.415
2024-12-01-22:27:49-root-INFO: grad norm: 3.013 2.988 0.385
2024-12-01-22:27:49-root-INFO: grad norm: 3.147 3.123 0.392
2024-12-01-22:27:50-root-INFO: grad norm: 3.458 3.434 0.407
2024-12-01-22:27:51-root-INFO: grad norm: 3.459 3.432 0.429
2024-12-01-22:27:51-root-INFO: grad norm: 3.446 3.422 0.408
2024-12-01-22:27:52-root-INFO: Loss Change: 84.240 -> 82.518
2024-12-01-22:27:52-root-INFO: Regularization Change: 0.000 -> 1.264
2024-12-01-22:27:52-root-INFO: Undo step: 70
2024-12-01-22:27:52-root-INFO: Undo step: 71
2024-12-01-22:27:52-root-INFO: Undo step: 72
2024-12-01-22:27:52-root-INFO: Undo step: 73
2024-12-01-22:27:52-root-INFO: Undo step: 74
2024-12-01-22:27:52-root-INFO: step: 75 lr_xt 0.09974414
2024-12-01-22:27:52-root-INFO: grad norm: 40.550 40.256 4.876
2024-12-01-22:27:53-root-INFO: grad norm: 17.576 17.365 2.718
2024-12-01-22:27:53-root-INFO: grad norm: 11.183 11.000 2.015
2024-12-01-22:27:54-root-INFO: grad norm: 8.541 8.396 1.565
2024-12-01-22:27:54-root-INFO: grad norm: 7.140 7.012 1.347
2024-12-01-22:27:55-root-INFO: grad norm: 6.310 6.198 1.182
2024-12-01-22:27:56-root-INFO: grad norm: 5.372 5.271 1.034
2024-12-01-22:27:57-root-INFO: grad norm: 4.909 4.816 0.947
2024-12-01-22:27:57-root-INFO: Loss Change: 244.185 -> 103.298
2024-12-01-22:27:57-root-INFO: Regularization Change: 0.000 -> 64.984
2024-12-01-22:27:57-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-01-22:27:57-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-22:27:57-root-INFO: step: 74 lr_xt 0.10217692
2024-12-01-22:27:57-root-INFO: grad norm: 6.026 5.908 1.186
2024-12-01-22:27:58-root-INFO: grad norm: 5.947 5.862 0.999
2024-12-01-22:27:59-root-INFO: grad norm: 6.396 6.320 0.983
2024-12-01-22:27:59-root-INFO: grad norm: 6.071 5.993 0.973
2024-12-01-22:28:00-root-INFO: grad norm: 5.433 5.366 0.849
2024-12-01-22:28:00-root-INFO: grad norm: 5.617 5.555 0.832
2024-12-01-22:28:01-root-INFO: grad norm: 6.602 6.539 0.905
2024-12-01-22:28:02-root-INFO: grad norm: 6.027 5.957 0.914
2024-12-01-22:28:02-root-INFO: Loss Change: 103.470 -> 94.614
2024-12-01-22:28:02-root-INFO: Regularization Change: 0.000 -> 7.275
2024-12-01-22:28:02-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-01-22:28:02-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-22:28:02-root-INFO: step: 73 lr_xt 0.10465226
2024-12-01-22:28:03-root-INFO: grad norm: 6.806 6.681 1.301
2024-12-01-22:28:03-root-INFO: grad norm: 6.532 6.468 0.915
2024-12-01-22:28:04-root-INFO: grad norm: 7.471 7.401 1.019
2024-12-01-22:28:05-root-INFO: grad norm: 6.246 6.174 0.941
2024-12-01-22:28:05-root-INFO: grad norm: 5.143 5.094 0.704
2024-12-01-22:28:06-root-INFO: grad norm: 4.699 4.663 0.576
2024-12-01-22:28:06-root-INFO: grad norm: 4.468 4.435 0.548
2024-12-01-22:28:07-root-INFO: grad norm: 4.649 4.622 0.505
2024-12-01-22:28:07-root-INFO: Loss Change: 95.036 -> 89.897
2024-12-01-22:28:07-root-INFO: Regularization Change: 0.000 -> 3.994
2024-12-01-22:28:07-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-01-22:28:07-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-22:28:08-root-INFO: step: 72 lr_xt 0.10717038
2024-12-01-22:28:08-root-INFO: grad norm: 5.764 5.705 0.822
2024-12-01-22:28:08-root-INFO: grad norm: 5.914 5.886 0.576
2024-12-01-22:28:09-root-INFO: grad norm: 5.114 5.083 0.566
2024-12-01-22:28:10-root-INFO: grad norm: 3.888 3.862 0.450
2024-12-01-22:28:10-root-INFO: grad norm: 3.989 3.962 0.468
2024-12-01-22:28:11-root-INFO: grad norm: 5.238 5.213 0.509
2024-12-01-22:28:11-root-INFO: Loss too large (87.372->87.480)! Learning rate decreased to 0.08574.
2024-12-01-22:28:12-root-INFO: grad norm: 3.990 3.956 0.522
2024-12-01-22:28:12-root-INFO: grad norm: 2.479 2.447 0.397
2024-12-01-22:28:13-root-INFO: Loss Change: 90.013 -> 86.121
2024-12-01-22:28:13-root-INFO: Regularization Change: 0.000 -> 2.522
2024-12-01-22:28:13-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-01-22:28:13-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-22:28:13-root-INFO: step: 71 lr_xt 0.10973151
2024-12-01-22:28:13-root-INFO: grad norm: 2.769 2.730 0.460
2024-12-01-22:28:14-root-INFO: grad norm: 3.432 3.406 0.422
2024-12-01-22:28:15-root-INFO: grad norm: 3.858 3.825 0.503
2024-12-01-22:28:15-root-INFO: grad norm: 5.176 5.147 0.552
2024-12-01-22:28:15-root-INFO: Loss too large (85.371->85.638)! Learning rate decreased to 0.08779.
2024-12-01-22:28:16-root-INFO: grad norm: 3.882 3.844 0.537
2024-12-01-22:28:17-root-INFO: grad norm: 2.049 2.016 0.366
2024-12-01-22:28:17-root-INFO: grad norm: 2.099 2.068 0.359
2024-12-01-22:28:18-root-INFO: grad norm: 2.366 2.338 0.362
2024-12-01-22:28:18-root-INFO: Loss Change: 86.199 -> 83.789
2024-12-01-22:28:18-root-INFO: Regularization Change: 0.000 -> 1.842
2024-12-01-22:28:18-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-01-22:28:18-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-22:28:19-root-INFO: step: 70 lr_xt 0.11233583
2024-12-01-22:28:19-root-INFO: grad norm: 3.362 3.257 0.833
2024-12-01-22:28:20-root-INFO: grad norm: 2.819 2.789 0.407
2024-12-01-22:28:20-root-INFO: grad norm: 3.205 3.182 0.391
2024-12-01-22:28:21-root-INFO: grad norm: 5.326 5.303 0.500
2024-12-01-22:28:21-root-INFO: Loss too large (82.822->83.217)! Learning rate decreased to 0.08987.
2024-12-01-22:28:21-root-INFO: Loss too large (82.822->82.884)! Learning rate decreased to 0.07189.
2024-12-01-22:28:22-root-INFO: grad norm: 3.765 3.735 0.471
2024-12-01-22:28:22-root-INFO: grad norm: 2.124 2.096 0.340
2024-12-01-22:28:23-root-INFO: grad norm: 2.063 2.036 0.330
2024-12-01-22:28:24-root-INFO: grad norm: 2.019 1.993 0.323
2024-12-01-22:28:24-root-INFO: Loss Change: 83.697 -> 81.499
2024-12-01-22:28:24-root-INFO: Regularization Change: 0.000 -> 1.381
2024-12-01-22:28:24-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-01-22:28:24-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-22:28:24-root-INFO: step: 69 lr_xt 0.11498353
2024-12-01-22:28:25-root-INFO: grad norm: 2.334 2.295 0.428
2024-12-01-22:28:25-root-INFO: grad norm: 2.602 2.580 0.339
2024-12-01-22:28:26-root-INFO: grad norm: 3.456 3.431 0.409
2024-12-01-22:28:26-root-INFO: Loss too large (81.182->81.235)! Learning rate decreased to 0.09199.
2024-12-01-22:28:27-root-INFO: grad norm: 4.232 4.209 0.440
2024-12-01-22:28:27-root-INFO: Loss too large (80.940->81.008)! Learning rate decreased to 0.07359.
2024-12-01-22:28:27-root-INFO: grad norm: 3.507 3.480 0.437
2024-12-01-22:28:28-root-INFO: grad norm: 2.608 2.584 0.349
2024-12-01-22:28:29-root-INFO: grad norm: 2.525 2.500 0.353
2024-12-01-22:28:29-root-INFO: grad norm: 2.439 2.416 0.332
2024-12-01-22:28:30-root-INFO: Loss Change: 81.605 -> 79.902
2024-12-01-22:28:30-root-INFO: Regularization Change: 0.000 -> 1.175
2024-12-01-22:28:30-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-01-22:28:30-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-22:28:30-root-INFO: step: 68 lr_xt 0.11767478
2024-12-01-22:28:30-root-INFO: grad norm: 3.257 3.145 0.848
2024-12-01-22:28:31-root-INFO: grad norm: 2.362 2.327 0.406
2024-12-01-22:28:31-root-INFO: grad norm: 2.080 2.050 0.352
2024-12-01-22:28:32-root-INFO: grad norm: 1.952 1.925 0.323
2024-12-01-22:28:33-root-INFO: grad norm: 2.031 2.007 0.312
2024-12-01-22:28:33-root-INFO: grad norm: 2.875 2.856 0.336
2024-12-01-22:28:33-root-INFO: Loss too large (78.259->78.335)! Learning rate decreased to 0.09414.
2024-12-01-22:28:34-root-INFO: grad norm: 3.417 3.393 0.400
2024-12-01-22:28:35-root-INFO: grad norm: 4.747 4.724 0.467
2024-12-01-22:28:35-root-INFO: Loss too large (78.076->78.279)! Learning rate decreased to 0.07531.
2024-12-01-22:28:35-root-INFO: Loss Change: 79.870 -> 78.000
2024-12-01-22:28:35-root-INFO: Regularization Change: 0.000 -> 1.748
2024-12-01-22:28:35-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-01-22:28:35-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-22:28:35-root-INFO: step: 67 lr_xt 0.12040972
2024-12-01-22:28:36-root-INFO: grad norm: 3.949 3.923 0.451
2024-12-01-22:28:36-root-INFO: Loss too large (77.900->77.934)! Learning rate decreased to 0.09633.
2024-12-01-22:28:36-root-INFO: grad norm: 4.556 4.531 0.477
2024-12-01-22:28:37-root-INFO: Loss too large (77.534->77.712)! Learning rate decreased to 0.07706.
2024-12-01-22:28:37-root-INFO: grad norm: 3.787 3.763 0.428
2024-12-01-22:28:38-root-INFO: grad norm: 2.787 2.765 0.355
2024-12-01-22:28:38-root-INFO: grad norm: 2.811 2.789 0.349
2024-12-01-22:28:39-root-INFO: grad norm: 2.871 2.850 0.342
2024-12-01-22:28:40-root-INFO: grad norm: 2.903 2.881 0.357
2024-12-01-22:28:40-root-INFO: grad norm: 2.955 2.935 0.342
2024-12-01-22:28:41-root-INFO: Loss Change: 77.900 -> 76.266
2024-12-01-22:28:41-root-INFO: Regularization Change: 0.000 -> 0.875
2024-12-01-22:28:41-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-01-22:28:41-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-22:28:41-root-INFO: step: 66 lr_xt 0.12318848
2024-12-01-22:28:41-root-INFO: grad norm: 2.790 2.764 0.384
2024-12-01-22:28:42-root-INFO: grad norm: 4.914 4.896 0.428
2024-12-01-22:28:42-root-INFO: Loss too large (76.224->76.793)! Learning rate decreased to 0.09855.
2024-12-01-22:28:42-root-INFO: Loss too large (76.224->76.432)! Learning rate decreased to 0.07884.
2024-12-01-22:28:43-root-INFO: grad norm: 3.881 3.856 0.443
2024-12-01-22:28:43-root-INFO: grad norm: 2.612 2.591 0.327
2024-12-01-22:28:44-root-INFO: grad norm: 2.637 2.616 0.335
2024-12-01-22:28:45-root-INFO: grad norm: 2.697 2.678 0.319
2024-12-01-22:28:45-root-INFO: grad norm: 2.734 2.713 0.341
2024-12-01-22:28:46-root-INFO: grad norm: 2.790 2.772 0.321
2024-12-01-22:28:46-root-INFO: Loss Change: 76.266 -> 74.901
2024-12-01-22:28:46-root-INFO: Regularization Change: 0.000 -> 0.920
2024-12-01-22:28:46-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-01-22:28:46-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-22:28:47-root-INFO: step: 65 lr_xt 0.12601118
2024-12-01-22:28:47-root-INFO: grad norm: 2.966 2.913 0.559
2024-12-01-22:28:47-root-INFO: grad norm: 4.100 4.083 0.377
2024-12-01-22:28:48-root-INFO: Loss too large (74.678->75.082)! Learning rate decreased to 0.10081.
2024-12-01-22:28:48-root-INFO: Loss too large (74.678->74.774)! Learning rate decreased to 0.08065.
2024-12-01-22:28:48-root-INFO: grad norm: 3.596 3.574 0.396
2024-12-01-22:28:49-root-INFO: grad norm: 3.184 3.165 0.348
2024-12-01-22:28:50-root-INFO: grad norm: 3.161 3.140 0.364
2024-12-01-22:28:50-root-INFO: grad norm: 3.133 3.115 0.337
2024-12-01-22:28:51-root-INFO: grad norm: 3.125 3.104 0.361
2024-12-01-22:28:52-root-INFO: grad norm: 3.113 3.095 0.333
2024-12-01-22:28:52-root-INFO: Loss Change: 74.905 -> 73.521
2024-12-01-22:28:52-root-INFO: Regularization Change: 0.000 -> 0.961
2024-12-01-22:28:52-root-INFO: Undo step: 65
2024-12-01-22:28:52-root-INFO: Undo step: 66
2024-12-01-22:28:52-root-INFO: Undo step: 67
2024-12-01-22:28:52-root-INFO: Undo step: 68
2024-12-01-22:28:52-root-INFO: Undo step: 69
2024-12-01-22:28:52-root-INFO: step: 70 lr_xt 0.11233583
2024-12-01-22:28:53-root-INFO: grad norm: 34.387 34.059 4.739
2024-12-01-22:28:53-root-INFO: grad norm: 17.952 17.716 2.896
2024-12-01-22:28:54-root-INFO: grad norm: 11.166 10.955 2.157
2024-12-01-22:28:54-root-INFO: grad norm: 8.486 8.322 1.660
2024-12-01-22:28:55-root-INFO: grad norm: 6.960 6.820 1.391
2024-12-01-22:28:56-root-INFO: grad norm: 5.988 5.872 1.173
2024-12-01-22:28:56-root-INFO: grad norm: 5.317 5.214 1.040
2024-12-01-22:28:57-root-INFO: grad norm: 4.837 4.749 0.920
2024-12-01-22:28:58-root-INFO: Loss Change: 232.258 -> 93.510
2024-12-01-22:28:58-root-INFO: Regularization Change: 0.000 -> 72.270
2024-12-01-22:28:58-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-01-22:28:58-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-22:28:58-root-INFO: step: 69 lr_xt 0.11498353
2024-12-01-22:28:58-root-INFO: grad norm: 4.548 4.471 0.833
2024-12-01-22:28:59-root-INFO: grad norm: 4.234 4.159 0.790
2024-12-01-22:28:59-root-INFO: grad norm: 4.121 4.060 0.708
2024-12-01-22:29:00-root-INFO: grad norm: 4.084 4.022 0.709
2024-12-01-22:29:01-root-INFO: grad norm: 4.245 4.196 0.642
2024-12-01-22:29:01-root-INFO: grad norm: 4.286 4.231 0.684
2024-12-01-22:29:02-root-INFO: grad norm: 4.476 4.434 0.613
2024-12-01-22:29:03-root-INFO: grad norm: 4.406 4.354 0.676
2024-12-01-22:29:03-root-INFO: Loss Change: 93.509 -> 85.135
2024-12-01-22:29:03-root-INFO: Regularization Change: 0.000 -> 7.772
2024-12-01-22:29:03-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-01-22:29:03-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-22:29:03-root-INFO: step: 68 lr_xt 0.11767478
2024-12-01-22:29:03-root-INFO: grad norm: 6.923 6.813 1.226
2024-12-01-22:29:04-root-INFO: grad norm: 5.491 5.418 0.893
2024-12-01-22:29:05-root-INFO: grad norm: 3.975 3.921 0.653
2024-12-01-22:29:05-root-INFO: grad norm: 3.666 3.617 0.599
2024-12-01-22:29:06-root-INFO: grad norm: 3.614 3.570 0.561
2024-12-01-22:29:07-root-INFO: grad norm: 3.690 3.644 0.579
2024-12-01-22:29:07-root-INFO: grad norm: 4.063 4.023 0.565
2024-12-01-22:29:08-root-INFO: grad norm: 4.144 4.097 0.618
2024-12-01-22:29:08-root-INFO: Loss Change: 85.499 -> 80.357
2024-12-01-22:29:08-root-INFO: Regularization Change: 0.000 -> 4.217
2024-12-01-22:29:08-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-01-22:29:08-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-22:29:08-root-INFO: step: 67 lr_xt 0.12040972
2024-12-01-22:29:09-root-INFO: grad norm: 5.616 5.541 0.914
2024-12-01-22:29:09-root-INFO: grad norm: 4.965 4.904 0.773
2024-12-01-22:29:10-root-INFO: grad norm: 3.957 3.908 0.620
2024-12-01-22:29:10-root-INFO: grad norm: 3.998 3.949 0.618
2024-12-01-22:29:11-root-INFO: grad norm: 4.463 4.424 0.595
2024-12-01-22:29:11-root-INFO: grad norm: 4.305 4.257 0.646
2024-12-01-22:29:12-root-INFO: grad norm: 3.976 3.936 0.565
2024-12-01-22:29:13-root-INFO: grad norm: 4.037 3.992 0.605
2024-12-01-22:29:13-root-INFO: Loss Change: 80.518 -> 77.080
2024-12-01-22:29:13-root-INFO: Regularization Change: 0.000 -> 3.023
2024-12-01-22:29:13-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-01-22:29:13-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-22:29:13-root-INFO: step: 66 lr_xt 0.12318848
2024-12-01-22:29:14-root-INFO: grad norm: 5.565 5.499 0.858
2024-12-01-22:29:14-root-INFO: Loss too large (77.381->77.392)! Learning rate decreased to 0.09855.
2024-12-01-22:29:14-root-INFO: grad norm: 3.942 3.901 0.564
2024-12-01-22:29:15-root-INFO: grad norm: 2.422 2.392 0.376
2024-12-01-22:29:16-root-INFO: grad norm: 2.195 2.167 0.348
2024-12-01-22:29:16-root-INFO: grad norm: 2.066 2.043 0.308
2024-12-01-22:29:17-root-INFO: grad norm: 2.007 1.982 0.321
2024-12-01-22:29:18-root-INFO: grad norm: 1.969 1.947 0.292
2024-12-01-22:29:18-root-INFO: grad norm: 1.952 1.927 0.312
2024-12-01-22:29:19-root-INFO: Loss Change: 77.381 -> 74.647
2024-12-01-22:29:19-root-INFO: Regularization Change: 0.000 -> 1.689
2024-12-01-22:29:19-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-01-22:29:19-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-22:29:19-root-INFO: step: 65 lr_xt 0.12601118
2024-12-01-22:29:19-root-INFO: grad norm: 3.730 3.656 0.738
2024-12-01-22:29:20-root-INFO: grad norm: 3.879 3.837 0.569
2024-12-01-22:29:20-root-INFO: grad norm: 4.962 4.926 0.596
2024-12-01-22:29:21-root-INFO: Loss too large (74.342->74.580)! Learning rate decreased to 0.10081.
2024-12-01-22:29:21-root-INFO: grad norm: 3.858 3.822 0.525
2024-12-01-22:29:22-root-INFO: grad norm: 2.467 2.441 0.358
2024-12-01-22:29:23-root-INFO: grad norm: 2.358 2.333 0.343
2024-12-01-22:29:23-root-INFO: grad norm: 2.336 2.315 0.311
2024-12-01-22:29:24-root-INFO: grad norm: 2.342 2.318 0.332
2024-12-01-22:29:24-root-INFO: Loss Change: 74.825 -> 72.596
2024-12-01-22:29:24-root-INFO: Regularization Change: 0.000 -> 1.717
2024-12-01-22:29:24-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-01-22:29:24-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-22:29:25-root-INFO: step: 64 lr_xt 0.12887791
2024-12-01-22:29:25-root-INFO: grad norm: 3.523 3.474 0.591
2024-12-01-22:29:25-root-INFO: grad norm: 3.926 3.889 0.543
2024-12-01-22:29:26-root-INFO: grad norm: 5.181 5.144 0.618
2024-12-01-22:29:26-root-INFO: Loss too large (72.357->72.738)! Learning rate decreased to 0.10310.
2024-12-01-22:29:27-root-INFO: grad norm: 4.033 3.998 0.537
2024-12-01-22:29:27-root-INFO: grad norm: 2.507 2.479 0.371
2024-12-01-22:29:28-root-INFO: grad norm: 2.405 2.381 0.341
2024-12-01-22:29:29-root-INFO: grad norm: 2.427 2.406 0.320
2024-12-01-22:29:29-root-INFO: grad norm: 2.471 2.448 0.337
2024-12-01-22:29:30-root-INFO: Loss Change: 72.651 -> 70.699
2024-12-01-22:29:30-root-INFO: Regularization Change: 0.000 -> 1.581
2024-12-01-22:29:30-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-01-22:29:30-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-22:29:30-root-INFO: step: 63 lr_xt 0.13178874
2024-12-01-22:29:30-root-INFO: grad norm: 3.754 3.697 0.649
2024-12-01-22:29:30-root-INFO: Loss too large (70.923->70.937)! Learning rate decreased to 0.10543.
2024-12-01-22:29:31-root-INFO: grad norm: 3.343 3.315 0.428
2024-12-01-22:29:32-root-INFO: grad norm: 3.167 3.142 0.392
2024-12-01-22:29:32-root-INFO: grad norm: 3.110 3.085 0.393
2024-12-01-22:29:33-root-INFO: grad norm: 3.054 3.033 0.361
2024-12-01-22:29:34-root-INFO: grad norm: 3.034 3.010 0.384
2024-12-01-22:29:34-root-INFO: grad norm: 3.021 3.001 0.350
2024-12-01-22:29:35-root-INFO: grad norm: 3.015 2.990 0.381
2024-12-01-22:29:35-root-INFO: Loss Change: 70.923 -> 69.165
2024-12-01-22:29:35-root-INFO: Regularization Change: 0.000 -> 1.362
2024-12-01-22:29:35-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-01-22:29:35-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-22:29:36-root-INFO: step: 62 lr_xt 0.13474373
2024-12-01-22:29:36-root-INFO: grad norm: 5.100 5.021 0.895
2024-12-01-22:29:36-root-INFO: Loss too large (69.313->69.576)! Learning rate decreased to 0.10779.
2024-12-01-22:29:37-root-INFO: grad norm: 4.061 4.025 0.537
2024-12-01-22:29:37-root-INFO: grad norm: 3.009 2.980 0.417
2024-12-01-22:29:38-root-INFO: grad norm: 2.994 2.969 0.389
2024-12-01-22:29:39-root-INFO: grad norm: 3.109 3.087 0.369
2024-12-01-22:29:39-root-INFO: grad norm: 3.132 3.107 0.392
2024-12-01-22:29:40-root-INFO: grad norm: 3.190 3.169 0.365
2024-12-01-22:29:41-root-INFO: grad norm: 3.188 3.163 0.396
2024-12-01-22:29:41-root-INFO: Loss Change: 69.313 -> 67.321
2024-12-01-22:29:41-root-INFO: Regularization Change: 0.000 -> 1.373
2024-12-01-22:29:41-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-01-22:29:41-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-22:29:41-root-INFO: step: 61 lr_xt 0.13774291
2024-12-01-22:29:41-root-INFO: grad norm: 4.112 4.073 0.570
2024-12-01-22:29:42-root-INFO: Loss too large (67.476->67.714)! Learning rate decreased to 0.11019.
2024-12-01-22:29:42-root-INFO: grad norm: 3.638 3.610 0.453
2024-12-01-22:29:43-root-INFO: grad norm: 3.076 3.052 0.381
2024-12-01-22:29:44-root-INFO: grad norm: 3.054 3.030 0.383
2024-12-01-22:29:44-root-INFO: grad norm: 3.077 3.056 0.355
2024-12-01-22:29:45-root-INFO: grad norm: 3.074 3.051 0.380
2024-12-01-22:29:45-root-INFO: grad norm: 3.084 3.064 0.349
2024-12-01-22:29:46-root-INFO: grad norm: 3.080 3.057 0.380
2024-12-01-22:29:46-root-INFO: Loss Change: 67.476 -> 65.815
2024-12-01-22:29:46-root-INFO: Regularization Change: 0.000 -> 1.306
2024-12-01-22:29:46-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-01-22:29:47-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-22:29:47-root-INFO: step: 60 lr_xt 0.14078630
2024-12-01-22:29:47-root-INFO: grad norm: 4.516 4.455 0.743
2024-12-01-22:29:47-root-INFO: Loss too large (66.100->66.378)! Learning rate decreased to 0.11263.
2024-12-01-22:29:48-root-INFO: grad norm: 3.830 3.800 0.478
2024-12-01-22:29:48-root-INFO: grad norm: 3.155 3.129 0.399
2024-12-01-22:29:49-root-INFO: grad norm: 3.163 3.138 0.391
2024-12-01-22:29:50-root-INFO: grad norm: 3.256 3.235 0.368
2024-12-01-22:29:50-root-INFO: grad norm: 3.256 3.232 0.394
2024-12-01-22:29:51-root-INFO: grad norm: 3.261 3.241 0.363
2024-12-01-22:29:51-root-INFO: grad norm: 3.255 3.231 0.393
2024-12-01-22:29:52-root-INFO: Loss Change: 66.100 -> 64.380
2024-12-01-22:29:52-root-INFO: Regularization Change: 0.000 -> 1.315
2024-12-01-22:29:52-root-INFO: Undo step: 60
2024-12-01-22:29:52-root-INFO: Undo step: 61
2024-12-01-22:29:52-root-INFO: Undo step: 62
2024-12-01-22:29:52-root-INFO: Undo step: 63
2024-12-01-22:29:52-root-INFO: Undo step: 64
2024-12-01-22:29:52-root-INFO: step: 65 lr_xt 0.12601118
2024-12-01-22:29:52-root-INFO: grad norm: 28.657 28.431 3.590
2024-12-01-22:29:53-root-INFO: grad norm: 13.928 13.742 2.266
2024-12-01-22:29:53-root-INFO: grad norm: 9.346 9.177 1.768
2024-12-01-22:29:54-root-INFO: grad norm: 7.259 7.143 1.290
2024-12-01-22:29:55-root-INFO: grad norm: 6.193 6.088 1.133
2024-12-01-22:29:55-root-INFO: grad norm: 5.927 5.855 0.922
2024-12-01-22:29:56-root-INFO: grad norm: 5.557 5.479 0.927
2024-12-01-22:29:56-root-INFO: grad norm: 5.322 5.261 0.803
2024-12-01-22:29:56-root-INFO: Loss Change: 196.793 -> 82.920
2024-12-01-22:29:56-root-INFO: Regularization Change: 0.000 -> 70.112
2024-12-01-22:29:56-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-01-22:29:56-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-22:29:57-root-INFO: step: 64 lr_xt 0.12887791
2024-12-01-22:29:57-root-INFO: grad norm: 4.991 4.935 0.740
2024-12-01-22:29:57-root-INFO: grad norm: 4.729 4.679 0.682
2024-12-01-22:29:58-root-INFO: grad norm: 4.647 4.593 0.709
2024-12-01-22:29:59-root-INFO: grad norm: 4.682 4.632 0.687
2024-12-01-22:29:59-root-INFO: grad norm: 4.592 4.537 0.711
2024-12-01-22:30:00-root-INFO: grad norm: 4.485 4.433 0.677
2024-12-01-22:30:01-root-INFO: grad norm: 4.407 4.353 0.688
2024-12-01-22:30:02-root-INFO: grad norm: 4.312 4.262 0.657
2024-12-01-22:30:03-root-INFO: Loss Change: 82.663 -> 74.815
2024-12-01-22:30:03-root-INFO: Regularization Change: 0.000 -> 8.061
2024-12-01-22:30:03-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-01-22:30:03-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-22:30:03-root-INFO: step: 63 lr_xt 0.13178874
2024-12-01-22:30:03-root-INFO: grad norm: 4.074 4.032 0.583
2024-12-01-22:30:04-root-INFO: grad norm: 3.920 3.880 0.560
2024-12-01-22:30:05-root-INFO: grad norm: 3.921 3.880 0.570
2024-12-01-22:30:06-root-INFO: grad norm: 4.008 3.965 0.583
2024-12-01-22:30:07-root-INFO: grad norm: 3.988 3.943 0.596
2024-12-01-22:30:08-root-INFO: grad norm: 3.958 3.914 0.588
2024-12-01-22:30:09-root-INFO: grad norm: 3.935 3.890 0.597
2024-12-01-22:30:09-root-INFO: grad norm: 3.903 3.859 0.582
2024-12-01-22:30:10-root-INFO: Loss Change: 74.700 -> 70.630
2024-12-01-22:30:10-root-INFO: Regularization Change: 0.000 -> 4.329
2024-12-01-22:30:10-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-01-22:30:10-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-22:30:10-root-INFO: step: 62 lr_xt 0.13474373
2024-12-01-22:30:11-root-INFO: grad norm: 3.687 3.638 0.600
2024-12-01-22:30:12-root-INFO: grad norm: 3.479 3.446 0.477
2024-12-01-22:30:12-root-INFO: grad norm: 3.560 3.526 0.495
2024-12-01-22:30:13-root-INFO: grad norm: 3.837 3.800 0.531
2024-12-01-22:30:14-root-INFO: grad norm: 3.866 3.824 0.562
2024-12-01-22:30:15-root-INFO: grad norm: 3.897 3.856 0.561
2024-12-01-22:30:16-root-INFO: grad norm: 3.896 3.853 0.583
2024-12-01-22:30:17-root-INFO: grad norm: 3.880 3.838 0.567
2024-12-01-22:30:17-root-INFO: Loss Change: 70.197 -> 67.372
2024-12-01-22:30:17-root-INFO: Regularization Change: 0.000 -> 3.203
2024-12-01-22:30:17-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-01-22:30:17-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-22:30:18-root-INFO: step: 61 lr_xt 0.13774291
2024-12-01-22:30:18-root-INFO: grad norm: 3.637 3.606 0.476
2024-12-01-22:30:19-root-INFO: grad norm: 3.601 3.566 0.504
2024-12-01-22:30:20-root-INFO: grad norm: 3.636 3.599 0.520
2024-12-01-22:30:21-root-INFO: grad norm: 3.724 3.687 0.524
2024-12-01-22:30:21-root-INFO: grad norm: 3.728 3.688 0.547
2024-12-01-22:30:22-root-INFO: grad norm: 3.723 3.685 0.531
2024-12-01-22:30:23-root-INFO: grad norm: 3.717 3.676 0.553
2024-12-01-22:30:24-root-INFO: grad norm: 3.703 3.665 0.529
2024-12-01-22:30:25-root-INFO: Loss Change: 67.223 -> 64.897
2024-12-01-22:30:25-root-INFO: Regularization Change: 0.000 -> 2.618
2024-12-01-22:30:25-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-01-22:30:25-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-22:30:25-root-INFO: step: 60 lr_xt 0.14078630
2024-12-01-22:30:25-root-INFO: grad norm: 3.464 3.432 0.467
2024-12-01-22:30:26-root-INFO: grad norm: 3.359 3.331 0.437
2024-12-01-22:30:27-root-INFO: grad norm: 3.455 3.424 0.468
2024-12-01-22:30:28-root-INFO: grad norm: 3.691 3.659 0.490
2024-12-01-22:30:29-root-INFO: grad norm: 3.721 3.683 0.531
2024-12-01-22:30:30-root-INFO: grad norm: 3.744 3.708 0.517
2024-12-01-22:30:30-root-INFO: grad norm: 3.758 3.717 0.550
2024-12-01-22:30:31-root-INFO: grad norm: 3.764 3.727 0.525
2024-12-01-22:30:32-root-INFO: Loss Change: 64.789 -> 62.841
2024-12-01-22:30:32-root-INFO: Regularization Change: 0.000 -> 2.351
2024-12-01-22:30:32-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-01-22:30:32-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-22:30:32-root-INFO: step: 59 lr_xt 0.14387389
2024-12-01-22:30:32-root-INFO: grad norm: 3.523 3.491 0.470
2024-12-01-22:30:33-root-INFO: grad norm: 3.502 3.472 0.453
2024-12-01-22:30:34-root-INFO: grad norm: 3.601 3.569 0.481
2024-12-01-22:30:35-root-INFO: grad norm: 3.808 3.775 0.506
2024-12-01-22:30:35-root-INFO: Loss too large (61.764->61.792)! Learning rate decreased to 0.11510.
2024-12-01-22:30:36-root-INFO: grad norm: 2.998 2.969 0.411
2024-12-01-22:30:37-root-INFO: grad norm: 2.217 2.194 0.318
2024-12-01-22:30:38-root-INFO: grad norm: 1.979 1.958 0.283
2024-12-01-22:30:39-root-INFO: grad norm: 1.784 1.765 0.261
2024-12-01-22:30:40-root-INFO: Loss Change: 62.649 -> 60.521
2024-12-01-22:30:40-root-INFO: Regularization Change: 0.000 -> 1.723
2024-12-01-22:30:40-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-01-22:30:40-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-22:30:40-root-INFO: step: 58 lr_xt 0.14700566
2024-12-01-22:30:40-root-INFO: grad norm: 1.978 1.934 0.417
2024-12-01-22:30:41-root-INFO: grad norm: 1.582 1.564 0.241
2024-12-01-22:30:42-root-INFO: grad norm: 1.561 1.545 0.226
2024-12-01-22:30:43-root-INFO: grad norm: 1.751 1.736 0.236
2024-12-01-22:30:44-root-INFO: grad norm: 2.077 2.060 0.269
2024-12-01-22:30:45-root-INFO: grad norm: 2.784 2.765 0.324
2024-12-01-22:30:45-root-INFO: Loss too large (59.274->59.280)! Learning rate decreased to 0.11760.
2024-12-01-22:30:46-root-INFO: grad norm: 2.585 2.563 0.333
2024-12-01-22:30:47-root-INFO: grad norm: 2.347 2.328 0.299
2024-12-01-22:30:47-root-INFO: Loss Change: 60.374 -> 58.723
2024-12-01-22:30:47-root-INFO: Regularization Change: 0.000 -> 1.836
2024-12-01-22:30:47-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-01-22:30:47-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-22:30:47-root-INFO: step: 57 lr_xt 0.15018154
2024-12-01-22:30:48-root-INFO: grad norm: 2.182 2.149 0.378
2024-12-01-22:30:48-root-INFO: grad norm: 2.101 2.086 0.253
2024-12-01-22:30:49-root-INFO: grad norm: 2.462 2.445 0.290
2024-12-01-22:30:50-root-INFO: grad norm: 3.366 3.347 0.357
2024-12-01-22:30:50-root-INFO: Loss too large (57.901->58.022)! Learning rate decreased to 0.12015.
2024-12-01-22:30:51-root-INFO: grad norm: 2.992 2.968 0.379
2024-12-01-22:30:52-root-INFO: grad norm: 2.529 2.510 0.314
2024-12-01-22:30:53-root-INFO: grad norm: 2.415 2.394 0.319
2024-12-01-22:30:54-root-INFO: grad norm: 2.291 2.273 0.287
2024-12-01-22:30:55-root-INFO: Loss Change: 58.457 -> 56.933
2024-12-01-22:30:55-root-INFO: Regularization Change: 0.000 -> 1.602
2024-12-01-22:30:55-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-01-22:30:55-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-22:30:55-root-INFO: step: 56 lr_xt 0.15340147
2024-12-01-22:30:55-root-INFO: grad norm: 2.285 2.258 0.351
2024-12-01-22:30:56-root-INFO: grad norm: 2.561 2.546 0.273
2024-12-01-22:30:57-root-INFO: grad norm: 3.039 3.019 0.352
2024-12-01-22:30:58-root-INFO: grad norm: 4.033 4.010 0.424
2024-12-01-22:30:58-root-INFO: Loss too large (56.508->56.761)! Learning rate decreased to 0.12272.
2024-12-01-22:30:59-root-INFO: grad norm: 3.353 3.325 0.429
2024-12-01-22:31:00-root-INFO: grad norm: 2.505 2.485 0.316
2024-12-01-22:31:01-root-INFO: grad norm: 2.367 2.347 0.312
2024-12-01-22:31:02-root-INFO: grad norm: 2.241 2.224 0.277
2024-12-01-22:31:02-root-INFO: Loss Change: 56.918 -> 55.421
2024-12-01-22:31:02-root-INFO: Regularization Change: 0.000 -> 1.579
2024-12-01-22:31:02-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-01-22:31:02-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-22:31:02-root-INFO: step: 55 lr_xt 0.15666536
2024-12-01-22:31:03-root-INFO: grad norm: 2.226 2.190 0.404
2024-12-01-22:31:04-root-INFO: grad norm: 2.283 2.268 0.262
2024-12-01-22:31:05-root-INFO: grad norm: 2.864 2.845 0.327
2024-12-01-22:31:05-root-INFO: grad norm: 3.961 3.939 0.417
2024-12-01-22:31:06-root-INFO: Loss too large (54.981->55.227)! Learning rate decreased to 0.12533.
2024-12-01-22:31:06-root-INFO: grad norm: 3.446 3.420 0.423
2024-12-01-22:31:07-root-INFO: grad norm: 2.889 2.868 0.348
2024-12-01-22:31:08-root-INFO: grad norm: 2.720 2.698 0.343
2024-12-01-22:31:09-root-INFO: grad norm: 2.573 2.554 0.310
2024-12-01-22:31:10-root-INFO: Loss Change: 55.343 -> 53.941
2024-12-01-22:31:10-root-INFO: Regularization Change: 0.000 -> 1.579
2024-12-01-22:31:10-root-INFO: Undo step: 55
2024-12-01-22:31:10-root-INFO: Undo step: 56
2024-12-01-22:31:10-root-INFO: Undo step: 57
2024-12-01-22:31:10-root-INFO: Undo step: 58
2024-12-01-22:31:10-root-INFO: Undo step: 59
2024-12-01-22:31:10-root-INFO: step: 60 lr_xt 0.14078630
2024-12-01-22:31:10-root-INFO: grad norm: 25.459 25.197 3.642
2024-12-01-22:31:11-root-INFO: grad norm: 13.618 13.472 1.991
2024-12-01-22:31:12-root-INFO: grad norm: 9.428 9.280 1.663
2024-12-01-22:31:13-root-INFO: grad norm: 7.793 7.687 1.280
2024-12-01-22:31:14-root-INFO: grad norm: 6.523 6.414 1.187
2024-12-01-22:31:14-root-INFO: grad norm: 5.348 5.264 0.946
2024-12-01-22:31:15-root-INFO: grad norm: 5.050 4.970 0.891
2024-12-01-22:31:16-root-INFO: grad norm: 5.145 5.082 0.803
2024-12-01-22:31:17-root-INFO: Loss Change: 174.325 -> 72.611
2024-12-01-22:31:17-root-INFO: Regularization Change: 0.000 -> 70.793
2024-12-01-22:31:17-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-01-22:31:17-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-22:31:17-root-INFO: step: 59 lr_xt 0.14387389
2024-12-01-22:31:17-root-INFO: grad norm: 4.710 4.658 0.699
2024-12-01-22:31:18-root-INFO: grad norm: 4.312 4.262 0.654
2024-12-01-22:31:19-root-INFO: grad norm: 4.275 4.224 0.656
2024-12-01-22:31:20-root-INFO: grad norm: 4.381 4.335 0.633
2024-12-01-22:31:21-root-INFO: grad norm: 4.267 4.217 0.652
2024-12-01-22:31:22-root-INFO: grad norm: 4.112 4.067 0.604
2024-12-01-22:31:23-root-INFO: grad norm: 4.056 4.009 0.618
2024-12-01-22:31:24-root-INFO: grad norm: 4.005 3.963 0.580
2024-12-01-22:31:24-root-INFO: Loss Change: 72.323 -> 64.720
2024-12-01-22:31:24-root-INFO: Regularization Change: 0.000 -> 8.634
2024-12-01-22:31:24-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-01-22:31:24-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-22:31:24-root-INFO: step: 58 lr_xt 0.14700566
2024-12-01-22:31:25-root-INFO: grad norm: 3.760 3.728 0.492
2024-12-01-22:31:26-root-INFO: grad norm: 3.628 3.594 0.494
2024-12-01-22:31:26-root-INFO: grad norm: 3.672 3.635 0.519
2024-12-01-22:31:27-root-INFO: grad norm: 3.817 3.780 0.524
2024-12-01-22:31:28-root-INFO: grad norm: 3.807 3.766 0.558
2024-12-01-22:31:29-root-INFO: grad norm: 3.789 3.752 0.533
2024-12-01-22:31:30-root-INFO: grad norm: 3.770 3.727 0.562
2024-12-01-22:31:31-root-INFO: grad norm: 3.737 3.699 0.529
2024-12-01-22:31:31-root-INFO: Loss Change: 64.390 -> 60.602
2024-12-01-22:31:31-root-INFO: Regularization Change: 0.000 -> 4.551
2024-12-01-22:31:31-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-01-22:31:31-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-22:31:32-root-INFO: step: 57 lr_xt 0.15018154
2024-12-01-22:31:32-root-INFO: grad norm: 3.443 3.414 0.447
2024-12-01-22:31:33-root-INFO: grad norm: 3.349 3.321 0.431
2024-12-01-22:31:34-root-INFO: grad norm: 3.428 3.395 0.473
2024-12-01-22:31:35-root-INFO: grad norm: 3.625 3.593 0.479
2024-12-01-22:31:36-root-INFO: grad norm: 3.656 3.618 0.528
2024-12-01-22:31:37-root-INFO: grad norm: 3.695 3.660 0.504
2024-12-01-22:31:37-root-INFO: grad norm: 3.688 3.647 0.545
2024-12-01-22:31:38-root-INFO: grad norm: 3.663 3.628 0.508
2024-12-01-22:31:39-root-INFO: Loss Change: 60.165 -> 57.558
2024-12-01-22:31:39-root-INFO: Regularization Change: 0.000 -> 3.298
2024-12-01-22:31:39-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-01-22:31:39-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-22:31:39-root-INFO: step: 56 lr_xt 0.15340147
2024-12-01-22:31:40-root-INFO: grad norm: 3.469 3.443 0.425
2024-12-01-22:31:40-root-INFO: grad norm: 3.311 3.283 0.425
2024-12-01-22:31:41-root-INFO: grad norm: 3.371 3.339 0.461
2024-12-01-22:31:42-root-INFO: grad norm: 3.506 3.476 0.459
2024-12-01-22:31:43-root-INFO: grad norm: 3.536 3.500 0.506
2024-12-01-22:31:44-root-INFO: grad norm: 3.570 3.538 0.477
2024-12-01-22:31:45-root-INFO: grad norm: 3.569 3.531 0.521
2024-12-01-22:31:46-root-INFO: grad norm: 3.554 3.521 0.480
2024-12-01-22:31:46-root-INFO: Loss Change: 57.440 -> 55.252
2024-12-01-22:31:46-root-INFO: Regularization Change: 0.000 -> 2.743
2024-12-01-22:31:47-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-01-22:31:47-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-22:31:47-root-INFO: step: 55 lr_xt 0.15666536
2024-12-01-22:31:47-root-INFO: grad norm: 3.317 3.289 0.428
2024-12-01-22:31:48-root-INFO: grad norm: 3.201 3.177 0.395
2024-12-01-22:31:49-root-INFO: grad norm: 3.280 3.251 0.437
2024-12-01-22:31:50-root-INFO: grad norm: 3.458 3.430 0.442
2024-12-01-22:31:51-root-INFO: grad norm: 3.505 3.471 0.493
2024-12-01-22:31:52-root-INFO: grad norm: 3.569 3.538 0.470
2024-12-01-22:31:52-root-INFO: grad norm: 3.572 3.535 0.515
2024-12-01-22:31:53-root-INFO: grad norm: 3.562 3.529 0.477
2024-12-01-22:31:54-root-INFO: Loss Change: 55.039 -> 53.199
2024-12-01-22:31:54-root-INFO: Regularization Change: 0.000 -> 2.453
2024-12-01-22:31:54-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-01-22:31:54-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-22:31:54-root-INFO: step: 54 lr_xt 0.15997308
2024-12-01-22:31:55-root-INFO: grad norm: 3.417 3.391 0.424
2024-12-01-22:31:55-root-INFO: grad norm: 3.295 3.269 0.418
2024-12-01-22:31:56-root-INFO: grad norm: 3.349 3.319 0.445
2024-12-01-22:31:57-root-INFO: grad norm: 3.478 3.449 0.453
2024-12-01-22:31:58-root-INFO: grad norm: 3.507 3.472 0.491
2024-12-01-22:31:59-root-INFO: grad norm: 3.541 3.510 0.470
2024-12-01-22:32:00-root-INFO: grad norm: 3.537 3.501 0.507
2024-12-01-22:32:01-root-INFO: grad norm: 3.520 3.488 0.473
2024-12-01-22:32:01-root-INFO: Loss Change: 52.948 -> 51.211
2024-12-01-22:32:01-root-INFO: Regularization Change: 0.000 -> 2.293
2024-12-01-22:32:01-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-01-22:32:01-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-22:32:02-root-INFO: step: 53 lr_xt 0.16332449
2024-12-01-22:32:02-root-INFO: grad norm: 3.308 3.283 0.406
2024-12-01-22:32:03-root-INFO: grad norm: 3.236 3.212 0.395
2024-12-01-22:32:03-root-INFO: grad norm: 3.289 3.260 0.442
2024-12-01-22:32:04-root-INFO: grad norm: 3.412 3.384 0.434
2024-12-01-22:32:05-root-INFO: grad norm: 3.446 3.412 0.484
2024-12-01-22:32:06-root-INFO: grad norm: 3.490 3.461 0.455
2024-12-01-22:32:07-root-INFO: grad norm: 3.492 3.456 0.501
2024-12-01-22:32:08-root-INFO: grad norm: 3.483 3.453 0.461
2024-12-01-22:32:08-root-INFO: Loss Change: 51.087 -> 49.519
2024-12-01-22:32:08-root-INFO: Regularization Change: 0.000 -> 2.157
2024-12-01-22:32:08-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-01-22:32:08-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-22:32:09-root-INFO: step: 52 lr_xt 0.16671942
2024-12-01-22:32:09-root-INFO: grad norm: 3.221 3.198 0.382
2024-12-01-22:32:10-root-INFO: grad norm: 3.092 3.069 0.374
2024-12-01-22:32:11-root-INFO: grad norm: 3.143 3.115 0.420
2024-12-01-22:32:12-root-INFO: grad norm: 3.248 3.222 0.405
2024-12-01-22:32:12-root-INFO: grad norm: 3.292 3.261 0.455
2024-12-01-22:32:13-root-INFO: grad norm: 3.347 3.320 0.425
2024-12-01-22:32:14-root-INFO: grad norm: 3.361 3.328 0.472
2024-12-01-22:32:15-root-INFO: grad norm: 3.368 3.340 0.434
2024-12-01-22:32:16-root-INFO: Loss Change: 49.315 -> 47.797
2024-12-01-22:32:16-root-INFO: Regularization Change: 0.000 -> 2.093
2024-12-01-22:32:16-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-01-22:32:16-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-22:32:16-root-INFO: step: 51 lr_xt 0.17015769
2024-12-01-22:32:16-root-INFO: grad norm: 3.237 3.212 0.408
2024-12-01-22:32:17-root-INFO: grad norm: 3.085 3.064 0.362
2024-12-01-22:32:18-root-INFO: grad norm: 3.134 3.110 0.393
2024-12-01-22:32:19-root-INFO: grad norm: 3.269 3.244 0.403
2024-12-01-22:32:20-root-INFO: grad norm: 3.318 3.288 0.444
2024-12-01-22:32:21-root-INFO: grad norm: 3.385 3.358 0.428
2024-12-01-22:32:22-root-INFO: grad norm: 3.395 3.363 0.468
2024-12-01-22:32:22-root-INFO: grad norm: 3.397 3.369 0.437
2024-12-01-22:32:23-root-INFO: Loss Change: 47.612 -> 46.166
2024-12-01-22:32:23-root-INFO: Regularization Change: 0.000 -> 2.075
2024-12-01-22:32:23-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-01-22:32:23-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-22:32:23-root-INFO: step: 50 lr_xt 0.17363908
2024-12-01-22:32:24-root-INFO: grad norm: 3.211 3.188 0.381
2024-12-01-22:32:24-root-INFO: grad norm: 3.073 3.051 0.361
2024-12-01-22:32:25-root-INFO: grad norm: 3.110 3.085 0.396
2024-12-01-22:32:26-root-INFO: grad norm: 3.215 3.190 0.397
2024-12-01-22:32:27-root-INFO: grad norm: 3.257 3.227 0.440
2024-12-01-22:32:28-root-INFO: grad norm: 3.316 3.289 0.420
2024-12-01-22:32:29-root-INFO: grad norm: 3.328 3.296 0.462
2024-12-01-22:32:30-root-INFO: grad norm: 3.335 3.308 0.430
2024-12-01-22:32:30-root-INFO: Loss Change: 45.766 -> 44.348
2024-12-01-22:32:30-root-INFO: Regularization Change: 0.000 -> 2.050
2024-12-01-22:32:30-root-INFO: Undo step: 50
2024-12-01-22:32:30-root-INFO: Undo step: 51
2024-12-01-22:32:30-root-INFO: Undo step: 52
2024-12-01-22:32:30-root-INFO: Undo step: 53
2024-12-01-22:32:30-root-INFO: Undo step: 54
2024-12-01-22:32:31-root-INFO: step: 55 lr_xt 0.15666536
2024-12-01-22:32:31-root-INFO: grad norm: 22.757 22.623 2.464
2024-12-01-22:32:32-root-INFO: grad norm: 12.703 12.542 2.018
2024-12-01-22:32:33-root-INFO: grad norm: 8.785 8.686 1.320
2024-12-01-22:32:34-root-INFO: grad norm: 6.987 6.879 1.226
2024-12-01-22:32:34-root-INFO: grad norm: 6.010 5.932 0.970
2024-12-01-22:32:35-root-INFO: grad norm: 5.381 5.303 0.915
2024-12-01-22:32:36-root-INFO: grad norm: 4.864 4.799 0.793
2024-12-01-22:32:37-root-INFO: grad norm: 4.479 4.418 0.735
2024-12-01-22:32:38-root-INFO: Loss Change: 158.486 -> 62.110
2024-12-01-22:32:38-root-INFO: Regularization Change: 0.000 -> 76.373
2024-12-01-22:32:38-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-01-22:32:38-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-22:32:38-root-INFO: step: 54 lr_xt 0.15997308
2024-12-01-22:32:38-root-INFO: grad norm: 4.393 4.322 0.786
2024-12-01-22:32:39-root-INFO: grad norm: 3.950 3.898 0.637
2024-12-01-22:32:40-root-INFO: grad norm: 3.657 3.609 0.589
2024-12-01-22:32:41-root-INFO: grad norm: 3.446 3.403 0.540
2024-12-01-22:32:42-root-INFO: grad norm: 3.265 3.225 0.514
2024-12-01-22:32:43-root-INFO: grad norm: 3.130 3.092 0.481
2024-12-01-22:32:44-root-INFO: grad norm: 3.019 2.983 0.465
2024-12-01-22:32:44-root-INFO: grad norm: 2.940 2.907 0.444
2024-12-01-22:32:45-root-INFO: Loss Change: 61.869 -> 54.435
2024-12-01-22:32:45-root-INFO: Regularization Change: 0.000 -> 9.155
2024-12-01-22:32:45-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-01-22:32:45-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-22:32:45-root-INFO: step: 53 lr_xt 0.16332449
2024-12-01-22:32:46-root-INFO: grad norm: 3.514 3.455 0.641
2024-12-01-22:32:46-root-INFO: grad norm: 3.359 3.318 0.525
2024-12-01-22:32:47-root-INFO: grad norm: 3.394 3.357 0.494
2024-12-01-22:32:48-root-INFO: grad norm: 3.406 3.368 0.504
2024-12-01-22:32:49-root-INFO: grad norm: 3.455 3.422 0.479
2024-12-01-22:32:50-root-INFO: grad norm: 3.464 3.427 0.503
2024-12-01-22:32:51-root-INFO: grad norm: 3.488 3.456 0.474
2024-12-01-22:32:52-root-INFO: grad norm: 3.486 3.449 0.503
2024-12-01-22:32:52-root-INFO: Loss Change: 54.507 -> 50.814
2024-12-01-22:32:52-root-INFO: Regularization Change: 0.000 -> 4.873
2024-12-01-22:32:52-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-01-22:32:53-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-22:32:53-root-INFO: step: 52 lr_xt 0.16671942
2024-12-01-22:32:53-root-INFO: grad norm: 4.319 4.256 0.734
2024-12-01-22:32:54-root-INFO: grad norm: 4.004 3.956 0.622
2024-12-01-22:32:55-root-INFO: grad norm: 3.709 3.673 0.516
2024-12-01-22:32:56-root-INFO: grad norm: 3.595 3.556 0.529
2024-12-01-22:32:57-root-INFO: grad norm: 3.483 3.451 0.467
2024-12-01-22:32:58-root-INFO: grad norm: 3.427 3.391 0.493
2024-12-01-22:32:58-root-INFO: grad norm: 3.371 3.342 0.446
2024-12-01-22:32:59-root-INFO: grad norm: 3.342 3.308 0.475
2024-12-01-22:33:00-root-INFO: Loss Change: 50.925 -> 48.029
2024-12-01-22:33:00-root-INFO: Regularization Change: 0.000 -> 3.516
2024-12-01-22:33:00-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-01-22:33:00-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-22:33:00-root-INFO: step: 51 lr_xt 0.17015769
2024-12-01-22:33:01-root-INFO: grad norm: 4.423 4.343 0.834
2024-12-01-22:33:01-root-INFO: grad norm: 4.034 3.986 0.623
2024-12-01-22:33:02-root-INFO: grad norm: 3.707 3.668 0.534
2024-12-01-22:33:03-root-INFO: grad norm: 3.589 3.551 0.519
2024-12-01-22:33:04-root-INFO: grad norm: 3.471 3.439 0.473
2024-12-01-22:33:05-root-INFO: grad norm: 3.417 3.383 0.483
2024-12-01-22:33:06-root-INFO: grad norm: 3.362 3.332 0.447
2024-12-01-22:33:07-root-INFO: grad norm: 3.334 3.301 0.466
2024-12-01-22:33:07-root-INFO: Loss Change: 48.218 -> 45.756
2024-12-01-22:33:07-root-INFO: Regularization Change: 0.000 -> 2.909
2024-12-01-22:33:07-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-01-22:33:07-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-22:33:08-root-INFO: step: 50 lr_xt 0.17363908
2024-12-01-22:33:08-root-INFO: grad norm: 4.282 4.209 0.788
2024-12-01-22:33:09-root-INFO: grad norm: 3.901 3.853 0.612
2024-12-01-22:33:09-root-INFO: grad norm: 3.559 3.521 0.513
2024-12-01-22:33:10-root-INFO: grad norm: 3.438 3.401 0.503
2024-12-01-22:33:11-root-INFO: grad norm: 3.323 3.292 0.454
2024-12-01-22:33:12-root-INFO: grad norm: 3.269 3.235 0.466
2024-12-01-22:33:13-root-INFO: grad norm: 3.215 3.186 0.430
2024-12-01-22:33:14-root-INFO: grad norm: 3.187 3.156 0.449
2024-12-01-22:33:14-root-INFO: Loss Change: 45.707 -> 43.528
2024-12-01-22:33:14-root-INFO: Regularization Change: 0.000 -> 2.572
2024-12-01-22:33:14-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-01-22:33:14-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-22:33:15-root-INFO: step: 49 lr_xt 0.17716334
2024-12-01-22:33:15-root-INFO: grad norm: 4.309 4.240 0.770
2024-12-01-22:33:16-root-INFO: grad norm: 3.841 3.794 0.602
2024-12-01-22:33:16-root-INFO: grad norm: 3.435 3.399 0.496
2024-12-01-22:33:17-root-INFO: grad norm: 3.294 3.258 0.484
2024-12-01-22:33:18-root-INFO: grad norm: 3.151 3.120 0.434
2024-12-01-22:33:19-root-INFO: grad norm: 3.085 3.053 0.442
2024-12-01-22:33:20-root-INFO: grad norm: 3.021 2.993 0.409
2024-12-01-22:33:21-root-INFO: grad norm: 2.990 2.960 0.423
2024-12-01-22:33:21-root-INFO: Loss Change: 43.822 -> 41.735
2024-12-01-22:33:21-root-INFO: Regularization Change: 0.000 -> 2.382
2024-12-01-22:33:21-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-01-22:33:21-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-22:33:22-root-INFO: step: 48 lr_xt 0.18073022
2024-12-01-22:33:22-root-INFO: grad norm: 3.662 3.604 0.652
2024-12-01-22:33:23-root-INFO: grad norm: 3.392 3.351 0.525
2024-12-01-22:33:24-root-INFO: grad norm: 3.205 3.173 0.447
2024-12-01-22:33:25-root-INFO: grad norm: 3.112 3.078 0.455
2024-12-01-22:33:25-root-INFO: grad norm: 3.018 2.991 0.408
2024-12-01-22:33:26-root-INFO: grad norm: 2.970 2.940 0.427
2024-12-01-22:33:27-root-INFO: grad norm: 2.924 2.897 0.393
2024-12-01-22:33:28-root-INFO: grad norm: 2.901 2.871 0.415
2024-12-01-22:33:29-root-INFO: Loss Change: 41.794 -> 40.051
2024-12-01-22:33:29-root-INFO: Regularization Change: 0.000 -> 2.201
2024-12-01-22:33:29-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-01-22:33:29-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-22:33:29-root-INFO: step: 47 lr_xt 0.18433941
2024-12-01-22:33:29-root-INFO: grad norm: 4.042 3.964 0.786
2024-12-01-22:33:30-root-INFO: grad norm: 3.650 3.603 0.584
2024-12-01-22:33:31-root-INFO: grad norm: 3.355 3.316 0.509
2024-12-01-22:33:32-root-INFO: grad norm: 3.230 3.193 0.489
2024-12-01-22:33:33-root-INFO: grad norm: 3.107 3.074 0.455
2024-12-01-22:33:34-root-INFO: grad norm: 3.055 3.020 0.456
2024-12-01-22:33:34-root-INFO: grad norm: 3.006 2.974 0.437
2024-12-01-22:33:35-root-INFO: grad norm: 2.989 2.956 0.446
2024-12-01-22:33:36-root-INFO: Loss Change: 40.303 -> 38.533
2024-12-01-22:33:36-root-INFO: Regularization Change: 0.000 -> 2.128
2024-12-01-22:33:36-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-01-22:33:36-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-22:33:36-root-INFO: step: 46 lr_xt 0.18799060
2024-12-01-22:33:36-root-INFO: grad norm: 3.792 3.724 0.718
2024-12-01-22:33:37-root-INFO: grad norm: 3.490 3.442 0.578
2024-12-01-22:33:38-root-INFO: grad norm: 3.281 3.241 0.510
2024-12-01-22:33:39-root-INFO: grad norm: 3.193 3.152 0.506
2024-12-01-22:33:40-root-INFO: grad norm: 3.103 3.067 0.474
2024-12-01-22:33:41-root-INFO: grad norm: 3.059 3.021 0.480
2024-12-01-22:33:42-root-INFO: grad norm: 3.011 2.976 0.458
2024-12-01-22:33:43-root-INFO: grad norm: 2.984 2.948 0.467
2024-12-01-22:33:43-root-INFO: Loss Change: 38.675 -> 37.066
2024-12-01-22:33:43-root-INFO: Regularization Change: 0.000 -> 2.037
2024-12-01-22:33:43-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-01-22:33:43-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-22:33:43-root-INFO: step: 45 lr_xt 0.19168344
2024-12-01-22:33:44-root-INFO: grad norm: 3.832 3.756 0.763
2024-12-01-22:33:45-root-INFO: grad norm: 3.509 3.456 0.609
2024-12-01-22:33:46-root-INFO: grad norm: 3.278 3.235 0.533
2024-12-01-22:33:46-root-INFO: grad norm: 3.158 3.115 0.521
2024-12-01-22:33:47-root-INFO: grad norm: 3.031 2.993 0.479
2024-12-01-22:33:48-root-INFO: grad norm: 2.958 2.919 0.477
2024-12-01-22:33:49-root-INFO: grad norm: 2.882 2.847 0.448
2024-12-01-22:33:50-root-INFO: grad norm: 2.837 2.801 0.450
2024-12-01-22:33:51-root-INFO: Loss Change: 37.149 -> 35.526
2024-12-01-22:33:51-root-INFO: Regularization Change: 0.000 -> 2.002
2024-12-01-22:33:51-root-INFO: Undo step: 45
2024-12-01-22:33:51-root-INFO: Undo step: 46
2024-12-01-22:33:51-root-INFO: Undo step: 47
2024-12-01-22:33:51-root-INFO: Undo step: 48
2024-12-01-22:33:51-root-INFO: Undo step: 49
2024-12-01-22:33:51-root-INFO: step: 50 lr_xt 0.17363908
2024-12-01-22:33:51-root-INFO: grad norm: 21.461 21.311 2.527
2024-12-01-22:33:52-root-INFO: grad norm: 11.093 10.976 1.602
2024-12-01-22:33:53-root-INFO: grad norm: 7.766 7.667 1.234
2024-12-01-22:33:54-root-INFO: grad norm: 6.538 6.455 1.039
2024-12-01-22:33:55-root-INFO: grad norm: 6.099 6.035 0.881
2024-12-01-22:33:56-root-INFO: grad norm: 5.382 5.324 0.792
2024-12-01-22:33:56-root-INFO: grad norm: 4.698 4.650 0.668
2024-12-01-22:33:57-root-INFO: grad norm: 4.513 4.464 0.661
2024-12-01-22:33:58-root-INFO: Loss Change: 140.703 -> 52.720
2024-12-01-22:33:58-root-INFO: Regularization Change: 0.000 -> 77.961
2024-12-01-22:33:58-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-01-22:33:58-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-22:33:58-root-INFO: step: 49 lr_xt 0.17716334
2024-12-01-22:33:59-root-INFO: grad norm: 5.153 5.094 0.772
2024-12-01-22:33:59-root-INFO: grad norm: 4.618 4.560 0.726
2024-12-01-22:34:00-root-INFO: grad norm: 4.244 4.197 0.632
2024-12-01-22:34:01-root-INFO: grad norm: 4.326 4.271 0.684
2024-12-01-22:34:02-root-INFO: grad norm: 4.530 4.485 0.636
2024-12-01-22:34:03-root-INFO: grad norm: 4.478 4.422 0.710
2024-12-01-22:34:04-root-INFO: grad norm: 4.346 4.301 0.622
2024-12-01-22:34:05-root-INFO: grad norm: 4.252 4.198 0.675
2024-12-01-22:34:05-root-INFO: Loss Change: 52.761 -> 45.412
2024-12-01-22:34:05-root-INFO: Regularization Change: 0.000 -> 10.157
2024-12-01-22:34:05-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-01-22:34:05-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-22:34:06-root-INFO: step: 48 lr_xt 0.18073022
2024-12-01-22:34:06-root-INFO: grad norm: 4.762 4.686 0.848
2024-12-01-22:34:07-root-INFO: grad norm: 4.390 4.326 0.745
2024-12-01-22:34:08-root-INFO: grad norm: 3.999 3.952 0.611
2024-12-01-22:34:08-root-INFO: grad norm: 3.857 3.806 0.623
2024-12-01-22:34:09-root-INFO: grad norm: 3.716 3.677 0.540
2024-12-01-22:34:10-root-INFO: grad norm: 3.624 3.579 0.570
2024-12-01-22:34:11-root-INFO: grad norm: 3.529 3.493 0.500
2024-12-01-22:34:12-root-INFO: grad norm: 3.464 3.422 0.535
2024-12-01-22:34:12-root-INFO: Loss Change: 45.497 -> 41.485
2024-12-01-22:34:12-root-INFO: Regularization Change: 0.000 -> 5.252
2024-12-01-22:34:12-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-01-22:34:12-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-22:34:13-root-INFO: step: 47 lr_xt 0.18433941
2024-12-01-22:34:13-root-INFO: grad norm: 4.512 4.426 0.875
2024-12-01-22:34:14-root-INFO: grad norm: 4.054 3.995 0.687
2024-12-01-22:34:15-root-INFO: grad norm: 3.621 3.577 0.568
2024-12-01-22:34:16-root-INFO: grad norm: 3.477 3.433 0.550
2024-12-01-22:34:16-root-INFO: grad norm: 3.333 3.297 0.488
2024-12-01-22:34:17-root-INFO: grad norm: 3.254 3.215 0.498
2024-12-01-22:34:18-root-INFO: grad norm: 3.173 3.141 0.449
2024-12-01-22:34:19-root-INFO: grad norm: 3.122 3.086 0.469
2024-12-01-22:34:20-root-INFO: Loss Change: 41.727 -> 38.812
2024-12-01-22:34:20-root-INFO: Regularization Change: 0.000 -> 3.669
2024-12-01-22:34:20-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-01-22:34:20-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-22:34:20-root-INFO: step: 46 lr_xt 0.18799060
2024-12-01-22:34:20-root-INFO: grad norm: 3.892 3.823 0.726
2024-12-01-22:34:21-root-INFO: grad norm: 3.539 3.490 0.588
2024-12-01-22:34:22-root-INFO: grad norm: 3.237 3.200 0.482
2024-12-01-22:34:23-root-INFO: grad norm: 3.120 3.082 0.485
2024-12-01-22:34:24-root-INFO: grad norm: 2.997 2.967 0.422
2024-12-01-22:34:25-root-INFO: grad norm: 2.937 2.904 0.441
2024-12-01-22:34:25-root-INFO: grad norm: 2.875 2.848 0.393
2024-12-01-22:34:26-root-INFO: grad norm: 2.841 2.810 0.418
2024-12-01-22:34:27-root-INFO: Loss Change: 38.932 -> 36.686
2024-12-01-22:34:27-root-INFO: Regularization Change: 0.000 -> 2.924
2024-12-01-22:34:27-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-01-22:34:27-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-22:34:27-root-INFO: step: 45 lr_xt 0.19168344
2024-12-01-22:34:27-root-INFO: grad norm: 3.745 3.672 0.735
2024-12-01-22:34:28-root-INFO: grad norm: 3.375 3.327 0.564
2024-12-01-22:34:29-root-INFO: grad norm: 3.077 3.043 0.458
2024-12-01-22:34:30-root-INFO: grad norm: 2.948 2.914 0.452
2024-12-01-22:34:31-root-INFO: grad norm: 2.813 2.786 0.393
2024-12-01-22:34:32-root-INFO: grad norm: 2.746 2.716 0.406
2024-12-01-22:34:33-root-INFO: grad norm: 2.679 2.654 0.362
2024-12-01-22:34:34-root-INFO: grad norm: 2.642 2.614 0.382
2024-12-01-22:34:34-root-INFO: Loss Change: 36.753 -> 34.772
2024-12-01-22:34:34-root-INFO: Regularization Change: 0.000 -> 2.551
2024-12-01-22:34:34-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-01-22:34:34-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-22:34:34-root-INFO: step: 44 lr_xt 0.19541757
2024-12-01-22:34:35-root-INFO: grad norm: 3.440 3.374 0.668
2024-12-01-22:34:36-root-INFO: grad norm: 3.128 3.086 0.507
2024-12-01-22:34:36-root-INFO: grad norm: 2.889 2.857 0.424
2024-12-01-22:34:37-root-INFO: grad norm: 2.771 2.740 0.414
2024-12-01-22:34:38-root-INFO: grad norm: 2.649 2.623 0.365
2024-12-01-22:34:39-root-INFO: grad norm: 2.582 2.555 0.373
2024-12-01-22:34:40-root-INFO: grad norm: 2.516 2.493 0.336
2024-12-01-22:34:41-root-INFO: grad norm: 2.478 2.453 0.351
2024-12-01-22:34:41-root-INFO: Loss Change: 34.911 -> 33.162
2024-12-01-22:34:41-root-INFO: Regularization Change: 0.000 -> 2.315
2024-12-01-22:34:41-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-01-22:34:41-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-22:34:42-root-INFO: step: 43 lr_xt 0.19919257
2024-12-01-22:34:42-root-INFO: grad norm: 3.342 3.277 0.659
2024-12-01-22:34:43-root-INFO: grad norm: 2.982 2.943 0.485
2024-12-01-22:34:44-root-INFO: grad norm: 2.727 2.698 0.395
2024-12-01-22:34:44-root-INFO: grad norm: 2.593 2.565 0.383
2024-12-01-22:34:45-root-INFO: grad norm: 2.460 2.437 0.334
2024-12-01-22:34:46-root-INFO: grad norm: 2.385 2.361 0.340
2024-12-01-22:34:47-root-INFO: grad norm: 2.314 2.294 0.304
2024-12-01-22:34:48-root-INFO: grad norm: 2.274 2.251 0.316
2024-12-01-22:34:49-root-INFO: Loss Change: 33.147 -> 31.490
2024-12-01-22:34:49-root-INFO: Regularization Change: 0.000 -> 2.165
2024-12-01-22:34:49-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-01-22:34:49-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-22:34:49-root-INFO: step: 42 lr_xt 0.20300803
2024-12-01-22:34:49-root-INFO: grad norm: 2.872 2.824 0.522
2024-12-01-22:34:50-root-INFO: grad norm: 2.639 2.607 0.409
2024-12-01-22:34:51-root-INFO: grad norm: 2.492 2.468 0.342
2024-12-01-22:34:52-root-INFO: grad norm: 2.399 2.375 0.344
2024-12-01-22:34:53-root-INFO: grad norm: 2.307 2.287 0.300
2024-12-01-22:34:53-root-INFO: grad norm: 2.249 2.227 0.313
2024-12-01-22:34:54-root-INFO: grad norm: 2.193 2.175 0.278
2024-12-01-22:34:55-root-INFO: grad norm: 2.158 2.138 0.296
2024-12-01-22:34:56-root-INFO: Loss Change: 31.515 -> 30.075
2024-12-01-22:34:56-root-INFO: Regularization Change: 0.000 -> 2.036
2024-12-01-22:34:56-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-01-22:34:56-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-22:34:56-root-INFO: step: 41 lr_xt 0.20721469
2024-12-01-22:34:56-root-INFO: grad norm: 3.389 3.306 0.745
2024-12-01-22:34:57-root-INFO: grad norm: 2.861 2.821 0.475
2024-12-01-22:34:58-root-INFO: grad norm: 2.538 2.511 0.375
2024-12-01-22:34:59-root-INFO: grad norm: 2.366 2.341 0.348
2024-12-01-22:35:00-root-INFO: grad norm: 2.208 2.188 0.298
2024-12-01-22:35:01-root-INFO: grad norm: 2.113 2.092 0.298
2024-12-01-22:35:02-root-INFO: grad norm: 2.029 2.012 0.263
2024-12-01-22:35:03-root-INFO: grad norm: 1.978 1.959 0.271
2024-12-01-22:35:03-root-INFO: Loss Change: 30.271 -> 28.619
2024-12-01-22:35:03-root-INFO: Regularization Change: 0.000 -> 2.062
2024-12-01-22:35:03-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-01-22:35:03-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-22:35:04-root-INFO: step: 40 lr_xt 0.21110784
2024-12-01-22:35:04-root-INFO: grad norm: 2.933 2.872 0.599
2024-12-01-22:35:05-root-INFO: grad norm: 2.563 2.528 0.420
2024-12-01-22:35:06-root-INFO: grad norm: 2.344 2.320 0.337
2024-12-01-22:35:07-root-INFO: grad norm: 2.208 2.184 0.325
2024-12-01-22:35:07-root-INFO: grad norm: 2.084 2.066 0.278
2024-12-01-22:35:08-root-INFO: grad norm: 2.002 1.982 0.283
2024-12-01-22:35:09-root-INFO: grad norm: 1.930 1.914 0.248
2024-12-01-22:35:10-root-INFO: grad norm: 1.883 1.865 0.259
2024-12-01-22:35:11-root-INFO: Loss Change: 28.752 -> 27.313
2024-12-01-22:35:11-root-INFO: Regularization Change: 0.000 -> 1.955
2024-12-01-22:35:11-root-INFO: Undo step: 40
2024-12-01-22:35:11-root-INFO: Undo step: 41
2024-12-01-22:35:11-root-INFO: Undo step: 42
2024-12-01-22:35:11-root-INFO: Undo step: 43
2024-12-01-22:35:11-root-INFO: Undo step: 44
2024-12-01-22:35:11-root-INFO: step: 45 lr_xt 0.19168344
2024-12-01-22:35:11-root-INFO: grad norm: 20.533 20.397 2.365
2024-12-01-22:35:12-root-INFO: grad norm: 11.137 11.013 1.658
2024-12-01-22:35:13-root-INFO: grad norm: 7.662 7.586 1.075
2024-12-01-22:35:14-root-INFO: grad norm: 5.994 5.923 0.916
2024-12-01-22:35:15-root-INFO: grad norm: 5.147 5.089 0.772
2024-12-01-22:35:16-root-INFO: grad norm: 4.628 4.576 0.692
2024-12-01-22:35:17-root-INFO: grad norm: 4.339 4.292 0.637
2024-12-01-22:35:17-root-INFO: grad norm: 4.082 4.040 0.588
2024-12-01-22:35:18-root-INFO: Loss Change: 135.379 -> 44.714
2024-12-01-22:35:18-root-INFO: Regularization Change: 0.000 -> 87.279
2024-12-01-22:35:18-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-01-22:35:18-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-22:35:18-root-INFO: step: 44 lr_xt 0.19541757
2024-12-01-22:35:18-root-INFO: grad norm: 4.498 4.430 0.777
2024-12-01-22:35:19-root-INFO: grad norm: 4.060 4.016 0.593
2024-12-01-22:35:20-root-INFO: grad norm: 3.667 3.627 0.541
2024-12-01-22:35:21-root-INFO: grad norm: 3.499 3.464 0.491
2024-12-01-22:35:22-root-INFO: grad norm: 3.350 3.317 0.469
2024-12-01-22:35:23-root-INFO: grad norm: 3.235 3.204 0.444
2024-12-01-22:35:24-root-INFO: grad norm: 3.124 3.095 0.425
2024-12-01-22:35:25-root-INFO: grad norm: 3.039 3.011 0.411
2024-12-01-22:35:25-root-INFO: Loss Change: 44.773 -> 37.854
2024-12-01-22:35:25-root-INFO: Regularization Change: 0.000 -> 10.232
2024-12-01-22:35:25-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-01-22:35:25-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-22:35:25-root-INFO: step: 43 lr_xt 0.19919257
2024-12-01-22:35:26-root-INFO: grad norm: 3.750 3.686 0.693
2024-12-01-22:35:26-root-INFO: grad norm: 3.376 3.338 0.502
2024-12-01-22:35:27-root-INFO: grad norm: 3.089 3.058 0.434
2024-12-01-22:35:28-root-INFO: grad norm: 2.945 2.916 0.408
2024-12-01-22:35:29-root-INFO: grad norm: 2.810 2.785 0.373
2024-12-01-22:35:30-root-INFO: grad norm: 2.724 2.699 0.368
2024-12-01-22:35:31-root-INFO: grad norm: 2.644 2.622 0.341
2024-12-01-22:35:31-root-INFO: grad norm: 2.589 2.566 0.344
2024-12-01-22:35:32-root-INFO: Loss Change: 37.761 -> 34.065
2024-12-01-22:35:32-root-INFO: Regularization Change: 0.000 -> 5.398
2024-12-01-22:35:32-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-01-22:35:32-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-22:35:32-root-INFO: step: 42 lr_xt 0.20300803
2024-12-01-22:35:33-root-INFO: grad norm: 3.100 3.053 0.537
2024-12-01-22:35:33-root-INFO: grad norm: 2.867 2.837 0.412
2024-12-01-22:35:34-root-INFO: grad norm: 2.712 2.688 0.355
2024-12-01-22:35:35-root-INFO: grad norm: 2.616 2.592 0.355
2024-12-01-22:35:36-root-INFO: grad norm: 2.525 2.505 0.316
2024-12-01-22:35:37-root-INFO: grad norm: 2.464 2.442 0.329
2024-12-01-22:35:38-root-INFO: grad norm: 2.408 2.390 0.295
2024-12-01-22:35:38-root-INFO: grad norm: 2.369 2.348 0.313
2024-12-01-22:35:39-root-INFO: Loss Change: 34.049 -> 31.545
2024-12-01-22:35:39-root-INFO: Regularization Change: 0.000 -> 3.772
2024-12-01-22:35:39-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-01-22:35:39-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-22:35:39-root-INFO: step: 41 lr_xt 0.20721469
2024-12-01-22:35:40-root-INFO: grad norm: 3.544 3.463 0.752
2024-12-01-22:35:41-root-INFO: grad norm: 3.036 3.000 0.470
2024-12-01-22:35:41-root-INFO: grad norm: 2.725 2.699 0.374
2024-12-01-22:35:42-root-INFO: grad norm: 2.550 2.526 0.354
2024-12-01-22:35:43-root-INFO: grad norm: 2.395 2.376 0.303
2024-12-01-22:35:44-root-INFO: grad norm: 2.296 2.275 0.308
2024-12-01-22:35:45-root-INFO: grad norm: 2.209 2.193 0.270
2024-12-01-22:35:46-root-INFO: grad norm: 2.152 2.134 0.283
2024-12-01-22:35:46-root-INFO: Loss Change: 31.706 -> 29.412
2024-12-01-22:35:46-root-INFO: Regularization Change: 0.000 -> 3.113
2024-12-01-22:35:46-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-01-22:35:46-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-22:35:47-root-INFO: step: 40 lr_xt 0.21110784
2024-12-01-22:35:47-root-INFO: grad norm: 3.060 3.002 0.594
2024-12-01-22:35:48-root-INFO: grad norm: 2.702 2.670 0.413
2024-12-01-22:35:49-root-INFO: grad norm: 2.484 2.462 0.328
2024-12-01-22:35:50-root-INFO: grad norm: 2.343 2.320 0.325
2024-12-01-22:35:51-root-INFO: grad norm: 2.219 2.202 0.274
2024-12-01-22:35:51-root-INFO: grad norm: 2.133 2.113 0.287
2024-12-01-22:35:52-root-INFO: grad norm: 2.059 2.044 0.247
2024-12-01-22:35:53-root-INFO: grad norm: 2.007 1.990 0.264
2024-12-01-22:35:54-root-INFO: Loss Change: 29.521 -> 27.677
2024-12-01-22:35:54-root-INFO: Regularization Change: 0.000 -> 2.624
2024-12-01-22:35:54-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-01-22:35:54-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-22:35:54-root-INFO: step: 39 lr_xt 0.21503976
2024-12-01-22:35:54-root-INFO: grad norm: 2.701 2.651 0.519
2024-12-01-22:35:55-root-INFO: grad norm: 2.407 2.380 0.359
2024-12-01-22:35:56-root-INFO: grad norm: 2.247 2.228 0.292
2024-12-01-22:35:57-root-INFO: grad norm: 2.135 2.115 0.292
2024-12-01-22:35:58-root-INFO: grad norm: 2.038 2.022 0.249
2024-12-01-22:35:59-root-INFO: grad norm: 1.966 1.948 0.263
2024-12-01-22:35:59-root-INFO: grad norm: 1.905 1.891 0.227
2024-12-01-22:36:00-root-INFO: grad norm: 1.860 1.844 0.245
2024-12-01-22:36:01-root-INFO: Loss Change: 27.673 -> 26.096
2024-12-01-22:36:01-root-INFO: Regularization Change: 0.000 -> 2.338
2024-12-01-22:36:01-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-01-22:36:01-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-22:36:01-root-INFO: step: 38 lr_xt 0.21900989
2024-12-01-22:36:01-root-INFO: grad norm: 2.684 2.624 0.563
2024-12-01-22:36:02-root-INFO: grad norm: 2.337 2.310 0.353
2024-12-01-22:36:03-root-INFO: grad norm: 2.158 2.139 0.287
2024-12-01-22:36:04-root-INFO: grad norm: 2.038 2.019 0.276
2024-12-01-22:36:05-root-INFO: grad norm: 1.933 1.919 0.237
2024-12-01-22:36:06-root-INFO: grad norm: 1.856 1.840 0.244
2024-12-01-22:36:07-root-INFO: grad norm: 1.790 1.778 0.213
2024-12-01-22:36:07-root-INFO: grad norm: 1.741 1.727 0.225
2024-12-01-22:36:08-root-INFO: Loss Change: 26.082 -> 24.598
2024-12-01-22:36:08-root-INFO: Regularization Change: 0.000 -> 2.173
2024-12-01-22:36:08-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-01-22:36:08-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-22:36:08-root-INFO: step: 37 lr_xt 0.22301766
2024-12-01-22:36:09-root-INFO: grad norm: 2.654 2.598 0.541
2024-12-01-22:36:09-root-INFO: grad norm: 2.276 2.248 0.358
2024-12-01-22:36:10-root-INFO: grad norm: 2.071 2.053 0.276
2024-12-01-22:36:11-root-INFO: grad norm: 1.923 1.903 0.272
2024-12-01-22:36:12-root-INFO: grad norm: 1.799 1.785 0.226
2024-12-01-22:36:13-root-INFO: grad norm: 1.704 1.688 0.235
2024-12-01-22:36:14-root-INFO: grad norm: 1.626 1.614 0.199
2024-12-01-22:36:15-root-INFO: grad norm: 1.565 1.551 0.212
2024-12-01-22:36:15-root-INFO: Loss Change: 24.658 -> 23.235
2024-12-01-22:36:15-root-INFO: Regularization Change: 0.000 -> 2.044
2024-12-01-22:36:15-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-01-22:36:15-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-22:36:16-root-INFO: step: 36 lr_xt 0.22706247
2024-12-01-22:36:16-root-INFO: grad norm: 2.234 2.184 0.469
2024-12-01-22:36:17-root-INFO: grad norm: 1.959 1.936 0.299
2024-12-01-22:36:18-root-INFO: grad norm: 1.828 1.811 0.251
2024-12-01-22:36:18-root-INFO: grad norm: 1.737 1.720 0.242
2024-12-01-22:36:19-root-INFO: grad norm: 1.658 1.644 0.210
2024-12-01-22:36:20-root-INFO: grad norm: 1.596 1.582 0.216
2024-12-01-22:36:21-root-INFO: grad norm: 1.544 1.533 0.188
2024-12-01-22:36:22-root-INFO: grad norm: 1.504 1.490 0.201
2024-12-01-22:36:22-root-INFO: Loss Change: 23.222 -> 21.991
2024-12-01-22:36:22-root-INFO: Regularization Change: 0.000 -> 1.931
2024-12-01-22:36:22-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-01-22:36:22-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-22:36:23-root-INFO: step: 35 lr_xt 0.23114370
2024-12-01-22:36:23-root-INFO: grad norm: 2.327 2.276 0.485
2024-12-01-22:36:24-root-INFO: grad norm: 1.988 1.962 0.319
2024-12-01-22:36:25-root-INFO: grad norm: 1.819 1.802 0.243
2024-12-01-22:36:26-root-INFO: grad norm: 1.696 1.678 0.243
2024-12-01-22:36:26-root-INFO: grad norm: 1.596 1.583 0.200
2024-12-01-22:36:27-root-INFO: grad norm: 1.516 1.502 0.212
2024-12-01-22:36:28-root-INFO: grad norm: 1.452 1.441 0.179
2024-12-01-22:36:29-root-INFO: grad norm: 1.400 1.386 0.193
2024-12-01-22:36:30-root-INFO: Loss Change: 22.047 -> 20.820
2024-12-01-22:36:30-root-INFO: Regularization Change: 0.000 -> 1.866
2024-12-01-22:36:30-root-INFO: Undo step: 35
2024-12-01-22:36:30-root-INFO: Undo step: 36
2024-12-01-22:36:30-root-INFO: Undo step: 37
2024-12-01-22:36:30-root-INFO: Undo step: 38
2024-12-01-22:36:30-root-INFO: Undo step: 39
2024-12-01-22:36:30-root-INFO: step: 40 lr_xt 0.21110784
2024-12-01-22:36:30-root-INFO: grad norm: 17.052 16.919 2.124
2024-12-01-22:36:31-root-INFO: grad norm: 9.108 9.035 1.147
2024-12-01-22:36:32-root-INFO: grad norm: 6.192 6.122 0.929
2024-12-01-22:36:33-root-INFO: grad norm: 4.786 4.732 0.716
2024-12-01-22:36:34-root-INFO: grad norm: 3.994 3.946 0.624
2024-12-01-22:36:35-root-INFO: grad norm: 3.509 3.468 0.534
2024-12-01-22:36:35-root-INFO: grad norm: 3.183 3.145 0.488
2024-12-01-22:36:36-root-INFO: grad norm: 2.951 2.919 0.433
2024-12-01-22:36:37-root-INFO: Loss Change: 112.516 -> 36.287
2024-12-01-22:36:37-root-INFO: Regularization Change: 0.000 -> 83.462
2024-12-01-22:36:37-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-01-22:36:37-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-22:36:37-root-INFO: step: 39 lr_xt 0.21503976
2024-12-01-22:36:37-root-INFO: grad norm: 2.752 2.723 0.398
2024-12-01-22:36:38-root-INFO: grad norm: 2.416 2.393 0.338
2024-12-01-22:36:39-root-INFO: grad norm: 2.256 2.234 0.317
2024-12-01-22:36:40-root-INFO: grad norm: 2.144 2.124 0.293
2024-12-01-22:36:41-root-INFO: grad norm: 2.059 2.038 0.292
2024-12-01-22:36:42-root-INFO: grad norm: 1.996 1.978 0.265
2024-12-01-22:36:43-root-INFO: grad norm: 1.949 1.929 0.274
2024-12-01-22:36:43-root-INFO: grad norm: 1.917 1.901 0.247
2024-12-01-22:36:44-root-INFO: Loss Change: 35.984 -> 29.793
2024-12-01-22:36:44-root-INFO: Regularization Change: 0.000 -> 10.480
2024-12-01-22:36:44-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-01-22:36:44-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-22:36:44-root-INFO: step: 38 lr_xt 0.21900989
2024-12-01-22:36:45-root-INFO: grad norm: 2.009 1.978 0.350
2024-12-01-22:36:46-root-INFO: grad norm: 1.670 1.656 0.215
2024-12-01-22:36:46-root-INFO: grad norm: 1.582 1.569 0.205
2024-12-01-22:36:47-root-INFO: grad norm: 1.538 1.526 0.193
2024-12-01-22:36:48-root-INFO: grad norm: 1.511 1.498 0.197
2024-12-01-22:36:49-root-INFO: grad norm: 1.496 1.484 0.185
2024-12-01-22:36:50-root-INFO: grad norm: 1.490 1.477 0.193
2024-12-01-22:36:51-root-INFO: grad norm: 1.494 1.483 0.181
2024-12-01-22:36:51-root-INFO: Loss Change: 29.512 -> 26.370
2024-12-01-22:36:51-root-INFO: Regularization Change: 0.000 -> 5.414
2024-12-01-22:36:51-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-01-22:36:51-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-22:36:52-root-INFO: step: 37 lr_xt 0.22301766
2024-12-01-22:36:52-root-INFO: grad norm: 1.822 1.783 0.372
2024-12-01-22:36:53-root-INFO: grad norm: 1.485 1.466 0.237
2024-12-01-22:36:54-root-INFO: grad norm: 1.394 1.378 0.207
2024-12-01-22:36:55-root-INFO: grad norm: 1.343 1.328 0.202
2024-12-01-22:36:55-root-INFO: grad norm: 1.298 1.284 0.190
2024-12-01-22:36:56-root-INFO: grad norm: 1.265 1.251 0.186
2024-12-01-22:36:57-root-INFO: grad norm: 1.232 1.219 0.179
2024-12-01-22:36:58-root-INFO: grad norm: 1.208 1.195 0.175
2024-12-01-22:36:59-root-INFO: Loss Change: 26.190 -> 24.029
2024-12-01-22:36:59-root-INFO: Regularization Change: 0.000 -> 3.698
2024-12-01-22:36:59-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-01-22:36:59-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-22:36:59-root-INFO: step: 36 lr_xt 0.22706247
2024-12-01-22:36:59-root-INFO: grad norm: 1.688 1.643 0.387
2024-12-01-22:37:00-root-INFO: grad norm: 1.401 1.379 0.249
2024-12-01-22:37:01-root-INFO: grad norm: 1.294 1.274 0.225
2024-12-01-22:37:02-root-INFO: grad norm: 1.241 1.224 0.207
2024-12-01-22:37:03-root-INFO: grad norm: 1.193 1.177 0.195
2024-12-01-22:37:03-root-INFO: grad norm: 1.161 1.146 0.189
2024-12-01-22:37:04-root-INFO: grad norm: 1.131 1.117 0.179
2024-12-01-22:37:05-root-INFO: grad norm: 1.111 1.097 0.179
2024-12-01-22:37:06-root-INFO: Loss Change: 23.910 -> 22.245
2024-12-01-22:37:06-root-INFO: Regularization Change: 0.000 -> 2.875
2024-12-01-22:37:06-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-01-22:37:06-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-22:37:06-root-INFO: step: 35 lr_xt 0.23114370
2024-12-01-22:37:06-root-INFO: grad norm: 1.951 1.897 0.455
2024-12-01-22:37:07-root-INFO: grad norm: 1.691 1.662 0.314
2024-12-01-22:37:08-root-INFO: grad norm: 1.644 1.622 0.267
2024-12-01-22:37:09-root-INFO: grad norm: 1.642 1.620 0.270
2024-12-01-22:37:10-root-INFO: grad norm: 1.645 1.627 0.244
2024-12-01-22:37:11-root-INFO: grad norm: 1.659 1.639 0.259
2024-12-01-22:37:12-root-INFO: grad norm: 1.667 1.650 0.237
2024-12-01-22:37:12-root-INFO: grad norm: 1.678 1.658 0.255
2024-12-01-22:37:13-root-INFO: Loss Change: 22.227 -> 20.862
2024-12-01-22:37:13-root-INFO: Regularization Change: 0.000 -> 2.429
2024-12-01-22:37:13-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-01-22:37:13-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-22:37:13-root-INFO: step: 34 lr_xt 0.23526068
2024-12-01-22:37:14-root-INFO: grad norm: 2.444 2.395 0.486
2024-12-01-22:37:14-root-INFO: grad norm: 2.187 2.155 0.375
2024-12-01-22:37:15-root-INFO: grad norm: 2.007 1.984 0.308
2024-12-01-22:37:16-root-INFO: grad norm: 1.888 1.864 0.305
2024-12-01-22:37:17-root-INFO: grad norm: 1.776 1.756 0.264
2024-12-01-22:37:18-root-INFO: grad norm: 1.687 1.665 0.270
2024-12-01-22:37:19-root-INFO: grad norm: 1.607 1.589 0.239
2024-12-01-22:37:20-root-INFO: grad norm: 1.543 1.523 0.247
2024-12-01-22:37:20-root-INFO: Loss Change: 20.864 -> 19.514
2024-12-01-22:37:20-root-INFO: Regularization Change: 0.000 -> 2.142
2024-12-01-22:37:20-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-01-22:37:20-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-22:37:20-root-INFO: step: 33 lr_xt 0.23941272
2024-12-01-22:37:21-root-INFO: grad norm: 2.415 2.355 0.538
2024-12-01-22:37:22-root-INFO: grad norm: 2.060 2.025 0.376
2024-12-01-22:37:22-root-INFO: grad norm: 1.809 1.782 0.315
2024-12-01-22:37:23-root-INFO: grad norm: 1.669 1.644 0.285
2024-12-01-22:37:24-root-INFO: grad norm: 1.546 1.525 0.254
2024-12-01-22:37:25-root-INFO: grad norm: 1.458 1.437 0.244
2024-12-01-22:37:26-root-INFO: grad norm: 1.382 1.364 0.223
2024-12-01-22:37:27-root-INFO: grad norm: 1.324 1.306 0.221
2024-12-01-22:37:27-root-INFO: Loss Change: 19.427 -> 18.151
2024-12-01-22:37:27-root-INFO: Regularization Change: 0.000 -> 1.970
2024-12-01-22:37:27-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-01-22:37:27-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-22:37:28-root-INFO: step: 32 lr_xt 0.24359912
2024-12-01-22:37:28-root-INFO: grad norm: 2.337 2.278 0.523
2024-12-01-22:37:29-root-INFO: grad norm: 1.931 1.895 0.373
2024-12-01-22:37:30-root-INFO: grad norm: 1.674 1.647 0.300
2024-12-01-22:37:30-root-INFO: grad norm: 1.536 1.510 0.281
2024-12-01-22:37:31-root-INFO: grad norm: 1.420 1.399 0.245
2024-12-01-22:37:32-root-INFO: grad norm: 1.340 1.318 0.241
2024-12-01-22:37:33-root-INFO: grad norm: 1.270 1.251 0.218
2024-12-01-22:37:34-root-INFO: grad norm: 1.219 1.200 0.219
2024-12-01-22:37:35-root-INFO: Loss Change: 18.259 -> 17.062
2024-12-01-22:37:35-root-INFO: Regularization Change: 0.000 -> 1.847
2024-12-01-22:37:35-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-01-22:37:35-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-22:37:35-root-INFO: step: 31 lr_xt 0.24781911
2024-12-01-22:37:35-root-INFO: grad norm: 1.906 1.856 0.437
2024-12-01-22:37:36-root-INFO: grad norm: 1.602 1.572 0.310
2024-12-01-22:37:37-root-INFO: grad norm: 1.444 1.420 0.264
2024-12-01-22:37:38-root-INFO: grad norm: 1.352 1.329 0.249
2024-12-01-22:37:39-root-INFO: grad norm: 1.271 1.251 0.224
2024-12-01-22:37:39-root-INFO: grad norm: 1.215 1.194 0.220
2024-12-01-22:37:40-root-INFO: grad norm: 1.165 1.147 0.204
2024-12-01-22:37:41-root-INFO: grad norm: 1.129 1.110 0.204
2024-12-01-22:37:42-root-INFO: Loss Change: 17.032 -> 16.008
2024-12-01-22:37:42-root-INFO: Regularization Change: 0.000 -> 1.722
2024-12-01-22:37:42-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-01-22:37:42-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-22:37:42-root-INFO: step: 30 lr_xt 0.25207194
2024-12-01-22:37:42-root-INFO: grad norm: 2.047 1.995 0.457
2024-12-01-22:37:43-root-INFO: grad norm: 1.699 1.664 0.342
2024-12-01-22:37:44-root-INFO: grad norm: 1.441 1.415 0.276
2024-12-01-22:37:45-root-INFO: grad norm: 1.326 1.301 0.256
2024-12-01-22:37:46-root-INFO: grad norm: 1.238 1.217 0.227
2024-12-01-22:37:47-root-INFO: grad norm: 1.180 1.158 0.225
2024-12-01-22:37:48-root-INFO: grad norm: 1.132 1.113 0.206
2024-12-01-22:37:48-root-INFO: grad norm: 1.099 1.079 0.208
2024-12-01-22:37:49-root-INFO: Loss Change: 16.021 -> 15.016
2024-12-01-22:37:49-root-INFO: Regularization Change: 0.000 -> 1.653
2024-12-01-22:37:49-root-INFO: Undo step: 30
2024-12-01-22:37:49-root-INFO: Undo step: 31
2024-12-01-22:37:49-root-INFO: Undo step: 32
2024-12-01-22:37:49-root-INFO: Undo step: 33
2024-12-01-22:37:49-root-INFO: Undo step: 34
2024-12-01-22:37:49-root-INFO: step: 35 lr_xt 0.23114370
2024-12-01-22:37:50-root-INFO: grad norm: 15.300 15.204 1.714
2024-12-01-22:37:50-root-INFO: grad norm: 8.542 8.456 1.210
2024-12-01-22:37:51-root-INFO: grad norm: 5.840 5.779 0.841
2024-12-01-22:37:52-root-INFO: grad norm: 4.586 4.536 0.670
2024-12-01-22:37:53-root-INFO: grad norm: 3.800 3.761 0.545
2024-12-01-22:37:54-root-INFO: grad norm: 3.252 3.218 0.470
2024-12-01-22:37:55-root-INFO: grad norm: 2.902 2.873 0.407
2024-12-01-22:37:56-root-INFO: grad norm: 2.664 2.639 0.369
2024-12-01-22:37:56-root-INFO: Loss Change: 98.223 -> 29.534
2024-12-01-22:37:56-root-INFO: Regularization Change: 0.000 -> 83.744
2024-12-01-22:37:56-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-01-22:37:57-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-22:37:57-root-INFO: step: 34 lr_xt 0.23526068
2024-12-01-22:37:57-root-INFO: grad norm: 2.478 2.456 0.335
2024-12-01-22:37:58-root-INFO: grad norm: 2.174 2.157 0.277
2024-12-01-22:37:59-root-INFO: grad norm: 2.028 2.010 0.269
2024-12-01-22:38:00-root-INFO: grad norm: 1.926 1.909 0.250
2024-12-01-22:38:00-root-INFO: grad norm: 1.845 1.828 0.253
2024-12-01-22:38:01-root-INFO: grad norm: 1.791 1.775 0.239
2024-12-01-22:38:02-root-INFO: grad norm: 1.742 1.724 0.246
2024-12-01-22:38:03-root-INFO: grad norm: 1.709 1.693 0.237
2024-12-01-22:38:04-root-INFO: Loss Change: 29.199 -> 23.380
2024-12-01-22:38:04-root-INFO: Regularization Change: 0.000 -> 10.820
2024-12-01-22:38:04-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-01-22:38:04-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-22:38:04-root-INFO: step: 33 lr_xt 0.23941272
2024-12-01-22:38:04-root-INFO: grad norm: 1.726 1.699 0.308
2024-12-01-22:38:05-root-INFO: grad norm: 1.430 1.419 0.178
2024-12-01-22:38:06-root-INFO: grad norm: 1.351 1.339 0.176
2024-12-01-22:38:07-root-INFO: grad norm: 1.305 1.294 0.167
2024-12-01-22:38:08-root-INFO: grad norm: 1.269 1.258 0.169
2024-12-01-22:38:09-root-INFO: grad norm: 1.245 1.234 0.167
2024-12-01-22:38:09-root-INFO: grad norm: 1.223 1.211 0.169
2024-12-01-22:38:10-root-INFO: grad norm: 1.211 1.199 0.171
2024-12-01-22:38:11-root-INFO: Loss Change: 22.986 -> 20.136
2024-12-01-22:38:11-root-INFO: Regularization Change: 0.000 -> 5.368
2024-12-01-22:38:11-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-01-22:38:11-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-22:38:11-root-INFO: step: 32 lr_xt 0.24359912
2024-12-01-22:38:12-root-INFO: grad norm: 1.599 1.563 0.336
2024-12-01-22:38:12-root-INFO: grad norm: 1.229 1.218 0.168
2024-12-01-22:38:13-root-INFO: grad norm: 1.154 1.144 0.151
2024-12-01-22:38:14-root-INFO: grad norm: 1.124 1.114 0.144
2024-12-01-22:38:15-root-INFO: grad norm: 1.103 1.094 0.142
2024-12-01-22:38:16-root-INFO: grad norm: 1.090 1.081 0.144
2024-12-01-22:38:17-root-INFO: grad norm: 1.079 1.070 0.142
2024-12-01-22:38:18-root-INFO: grad norm: 1.075 1.065 0.148
2024-12-01-22:38:18-root-INFO: Loss Change: 20.015 -> 18.094
2024-12-01-22:38:18-root-INFO: Regularization Change: 0.000 -> 3.600
2024-12-01-22:38:18-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-01-22:38:18-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-22:38:19-root-INFO: step: 31 lr_xt 0.24781911
2024-12-01-22:38:19-root-INFO: grad norm: 1.561 1.526 0.325
2024-12-01-22:38:20-root-INFO: grad norm: 1.287 1.275 0.181
2024-12-01-22:38:21-root-INFO: grad norm: 1.212 1.201 0.161
2024-12-01-22:38:21-root-INFO: grad norm: 1.165 1.155 0.152
2024-12-01-22:38:22-root-INFO: grad norm: 1.123 1.114 0.142
2024-12-01-22:38:23-root-INFO: grad norm: 1.089 1.080 0.142
2024-12-01-22:38:24-root-INFO: grad norm: 1.057 1.048 0.134
2024-12-01-22:38:25-root-INFO: grad norm: 1.030 1.021 0.136
2024-12-01-22:38:25-root-INFO: Loss Change: 17.962 -> 16.487
2024-12-01-22:38:26-root-INFO: Regularization Change: 0.000 -> 2.760
2024-12-01-22:38:26-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-01-22:38:26-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-22:38:26-root-INFO: step: 30 lr_xt 0.25207194
2024-12-01-22:38:26-root-INFO: grad norm: 1.683 1.644 0.360
2024-12-01-22:38:27-root-INFO: grad norm: 1.354 1.338 0.207
2024-12-01-22:38:28-root-INFO: grad norm: 1.232 1.218 0.186
2024-12-01-22:38:29-root-INFO: grad norm: 1.164 1.150 0.178
2024-12-01-22:38:30-root-INFO: grad norm: 1.113 1.100 0.175
2024-12-01-22:38:31-root-INFO: grad norm: 1.088 1.073 0.179
2024-12-01-22:38:31-root-INFO: grad norm: 1.084 1.068 0.185
2024-12-01-22:38:32-root-INFO: grad norm: 1.097 1.079 0.200
2024-12-01-22:38:33-root-INFO: Loss Change: 16.397 -> 15.152
2024-12-01-22:38:33-root-INFO: Regularization Change: 0.000 -> 2.308
2024-12-01-22:38:33-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-01-22:38:33-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-22:38:33-root-INFO: step: 29 lr_xt 0.25635679
2024-12-01-22:38:33-root-INFO: grad norm: 2.002 1.948 0.463
2024-12-01-22:38:34-root-INFO: grad norm: 1.748 1.706 0.380
2024-12-01-22:38:35-root-INFO: grad norm: 1.657 1.621 0.346
2024-12-01-22:38:36-root-INFO: grad norm: 1.602 1.560 0.362
2024-12-01-22:38:37-root-INFO: grad norm: 1.538 1.504 0.326
2024-12-01-22:38:38-root-INFO: grad norm: 1.488 1.450 0.334
2024-12-01-22:38:39-root-INFO: grad norm: 1.437 1.404 0.305
2024-12-01-22:38:39-root-INFO: grad norm: 1.396 1.362 0.308
2024-12-01-22:38:40-root-INFO: Loss Change: 15.152 -> 14.026
2024-12-01-22:38:40-root-INFO: Regularization Change: 0.000 -> 2.032
2024-12-01-22:38:40-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-01-22:38:40-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-22:38:40-root-INFO: step: 28 lr_xt 0.26067283
2024-12-01-22:38:41-root-INFO: grad norm: 2.290 2.232 0.511
2024-12-01-22:38:41-root-INFO: grad norm: 1.907 1.857 0.433
2024-12-01-22:38:42-root-INFO: grad norm: 1.572 1.537 0.331
2024-12-01-22:38:43-root-INFO: grad norm: 1.401 1.370 0.291
2024-12-01-22:38:44-root-INFO: grad norm: 1.315 1.288 0.264
2024-12-01-22:38:45-root-INFO: grad norm: 1.254 1.229 0.251
2024-12-01-22:38:46-root-INFO: grad norm: 1.206 1.183 0.236
2024-12-01-22:38:47-root-INFO: grad norm: 1.173 1.151 0.229
2024-12-01-22:38:47-root-INFO: Loss Change: 13.977 -> 12.853
2024-12-01-22:38:47-root-INFO: Regularization Change: 0.000 -> 1.840
2024-12-01-22:38:47-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-01-22:38:47-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-22:38:48-root-INFO: step: 27 lr_xt 0.26501920
2024-12-01-22:38:48-root-INFO: grad norm: 1.945 1.898 0.426
2024-12-01-22:38:49-root-INFO: grad norm: 1.604 1.571 0.326
2024-12-01-22:38:50-root-INFO: grad norm: 1.330 1.305 0.257
2024-12-01-22:38:51-root-INFO: grad norm: 1.200 1.180 0.220
2024-12-01-22:38:51-root-INFO: grad norm: 1.188 1.168 0.214
2024-12-01-22:38:52-root-INFO: grad norm: 1.141 1.122 0.210
2024-12-01-22:38:53-root-INFO: grad norm: 1.053 1.035 0.192
2024-12-01-22:38:54-root-INFO: grad norm: 1.024 1.008 0.180
2024-12-01-22:38:54-root-INFO: Loss Change: 12.879 -> 11.920
2024-12-01-22:38:54-root-INFO: Regularization Change: 0.000 -> 1.669
2024-12-01-22:38:54-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-01-22:38:54-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-22:38:55-root-INFO: step: 26 lr_xt 0.26939500
2024-12-01-22:38:55-root-INFO: grad norm: 2.072 2.027 0.431
2024-12-01-22:38:56-root-INFO: grad norm: 1.590 1.556 0.331
2024-12-01-22:38:57-root-INFO: grad norm: 1.269 1.247 0.234
2024-12-01-22:38:58-root-INFO: grad norm: 1.101 1.084 0.190
2024-12-01-22:38:59-root-INFO: grad norm: 0.987 0.971 0.176
2024-12-01-22:38:59-root-INFO: grad norm: 0.935 0.922 0.153
2024-12-01-22:39:00-root-INFO: grad norm: 1.000 0.986 0.164
2024-12-01-22:39:01-root-INFO: grad norm: 0.986 0.972 0.169
2024-12-01-22:39:02-root-INFO: Loss Change: 11.954 -> 11.035
2024-12-01-22:39:02-root-INFO: Regularization Change: 0.000 -> 1.567
2024-12-01-22:39:02-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-01-22:39:02-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-22:39:02-root-INFO: step: 25 lr_xt 0.27379933
2024-12-01-22:39:02-root-INFO: grad norm: 1.764 1.720 0.391
2024-12-01-22:39:03-root-INFO: grad norm: 1.377 1.353 0.254
2024-12-01-22:39:04-root-INFO: grad norm: 1.131 1.112 0.206
2024-12-01-22:39:05-root-INFO: grad norm: 1.039 1.024 0.176
2024-12-01-22:39:06-root-INFO: grad norm: 1.183 1.168 0.183
2024-12-01-22:39:07-root-INFO: grad norm: 1.051 1.034 0.186
2024-12-01-22:39:08-root-INFO: grad norm: 0.870 0.858 0.146
2024-12-01-22:39:09-root-INFO: grad norm: 0.775 0.766 0.116
2024-12-01-22:39:09-root-INFO: Loss Change: 10.968 -> 10.119
2024-12-01-22:39:09-root-INFO: Regularization Change: 0.000 -> 1.486
2024-12-01-22:39:09-root-INFO: Undo step: 25
2024-12-01-22:39:09-root-INFO: Undo step: 26
2024-12-01-22:39:09-root-INFO: Undo step: 27
2024-12-01-22:39:09-root-INFO: Undo step: 28
2024-12-01-22:39:09-root-INFO: Undo step: 29
2024-12-01-22:39:09-root-INFO: step: 30 lr_xt 0.25207194
2024-12-01-22:39:10-root-INFO: grad norm: 14.155 14.071 1.538
2024-12-01-22:39:11-root-INFO: grad norm: 7.928 7.879 0.875
2024-12-01-22:39:12-root-INFO: grad norm: 5.300 5.262 0.629
2024-12-01-22:39:12-root-INFO: grad norm: 4.053 4.019 0.526
2024-12-01-22:39:13-root-INFO: grad norm: 3.326 3.298 0.427
2024-12-01-22:39:14-root-INFO: grad norm: 2.860 2.834 0.380
2024-12-01-22:39:15-root-INFO: grad norm: 2.526 2.504 0.335
2024-12-01-22:39:16-root-INFO: grad norm: 2.279 2.258 0.305
2024-12-01-22:39:16-root-INFO: Loss Change: 87.612 -> 23.553
2024-12-01-22:39:16-root-INFO: Regularization Change: 0.000 -> 85.845
2024-12-01-22:39:16-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-01-22:39:16-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-22:39:17-root-INFO: step: 29 lr_xt 0.25635679
2024-12-01-22:39:17-root-INFO: grad norm: 2.337 2.303 0.395
2024-12-01-22:39:18-root-INFO: grad norm: 1.992 1.973 0.272
2024-12-01-22:39:19-root-INFO: grad norm: 1.833 1.815 0.254
2024-12-01-22:39:20-root-INFO: grad norm: 1.714 1.698 0.237
2024-12-01-22:39:20-root-INFO: grad norm: 1.619 1.602 0.232
2024-12-01-22:39:21-root-INFO: grad norm: 1.542 1.527 0.221
2024-12-01-22:39:22-root-INFO: grad norm: 1.485 1.469 0.222
2024-12-01-22:39:23-root-INFO: grad norm: 1.445 1.428 0.218
2024-12-01-22:39:24-root-INFO: Loss Change: 23.306 -> 17.729
2024-12-01-22:39:24-root-INFO: Regularization Change: 0.000 -> 11.249
2024-12-01-22:39:24-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-01-22:39:24-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-22:39:24-root-INFO: step: 28 lr_xt 0.26067283
2024-12-01-22:39:24-root-INFO: grad norm: 2.147 2.099 0.450
2024-12-01-22:39:25-root-INFO: grad norm: 1.966 1.933 0.360
2024-12-01-22:39:26-root-INFO: grad norm: 1.957 1.926 0.345
2024-12-01-22:39:27-root-INFO: grad norm: 1.951 1.914 0.376
2024-12-01-22:39:28-root-INFO: grad norm: 1.937 1.904 0.353
2024-12-01-22:39:29-root-INFO: grad norm: 1.917 1.878 0.384
2024-12-01-22:39:30-root-INFO: grad norm: 1.899 1.866 0.355
2024-12-01-22:39:31-root-INFO: grad norm: 1.871 1.832 0.381
2024-12-01-22:39:31-root-INFO: Loss Change: 17.492 -> 14.902
2024-12-01-22:39:31-root-INFO: Regularization Change: 0.000 -> 5.359
2024-12-01-22:39:31-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-01-22:39:31-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-22:39:31-root-INFO: step: 27 lr_xt 0.26501920
2024-12-01-22:39:32-root-INFO: grad norm: 2.599 2.538 0.557
2024-12-01-22:39:33-root-INFO: grad norm: 2.260 2.210 0.469
2024-12-01-22:39:33-root-INFO: grad norm: 1.932 1.897 0.366
2024-12-01-22:39:34-root-INFO: grad norm: 1.750 1.716 0.340
2024-12-01-22:39:35-root-INFO: grad norm: 1.648 1.621 0.302
2024-12-01-22:39:36-root-INFO: grad norm: 1.572 1.544 0.299
2024-12-01-22:39:37-root-INFO: grad norm: 1.512 1.487 0.272
2024-12-01-22:39:38-root-INFO: grad norm: 1.469 1.443 0.276
2024-12-01-22:39:38-root-INFO: Loss Change: 14.927 -> 13.032
2024-12-01-22:39:38-root-INFO: Regularization Change: 0.000 -> 3.442
2024-12-01-22:39:38-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-01-22:39:38-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-22:39:39-root-INFO: step: 26 lr_xt 0.26939500
2024-12-01-22:39:39-root-INFO: grad norm: 2.337 2.281 0.506
2024-12-01-22:39:40-root-INFO: grad norm: 1.949 1.908 0.399
2024-12-01-22:39:41-root-INFO: grad norm: 1.613 1.586 0.297
2024-12-01-22:39:42-root-INFO: grad norm: 1.431 1.407 0.262
2024-12-01-22:39:42-root-INFO: grad norm: 1.390 1.368 0.246
2024-12-01-22:39:43-root-INFO: grad norm: 1.344 1.321 0.248
2024-12-01-22:39:44-root-INFO: grad norm: 1.272 1.252 0.225
2024-12-01-22:39:45-root-INFO: grad norm: 1.248 1.228 0.223
2024-12-01-22:39:46-root-INFO: Loss Change: 13.041 -> 11.629
2024-12-01-22:39:46-root-INFO: Regularization Change: 0.000 -> 2.552
2024-12-01-22:39:46-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-01-22:39:46-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-22:39:46-root-INFO: step: 25 lr_xt 0.27379933
2024-12-01-22:39:46-root-INFO: grad norm: 2.217 2.167 0.469
2024-12-01-22:39:47-root-INFO: grad norm: 1.753 1.719 0.344
2024-12-01-22:39:48-root-INFO: grad norm: 1.368 1.346 0.243
2024-12-01-22:39:49-root-INFO: grad norm: 1.176 1.160 0.198
2024-12-01-22:39:50-root-INFO: grad norm: 1.055 1.040 0.178
2024-12-01-22:39:51-root-INFO: grad norm: 1.010 0.997 0.162
2024-12-01-22:39:51-root-INFO: grad norm: 1.178 1.164 0.178
2024-12-01-22:39:52-root-INFO: grad norm: 1.110 1.094 0.189
2024-12-01-22:39:53-root-INFO: Loss Change: 11.541 -> 10.394
2024-12-01-22:39:53-root-INFO: Regularization Change: 0.000 -> 2.057
2024-12-01-22:39:53-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-01-22:39:53-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-22:39:53-root-INFO: step: 24 lr_xt 0.27823123
2024-12-01-22:39:53-root-INFO: grad norm: 1.885 1.841 0.407
2024-12-01-22:39:54-root-INFO: grad norm: 1.471 1.448 0.261
2024-12-01-22:39:55-root-INFO: grad norm: 1.614 1.595 0.249
2024-12-01-22:39:56-root-INFO: grad norm: 1.321 1.298 0.247
2024-12-01-22:39:57-root-INFO: grad norm: 1.070 1.056 0.172
2024-12-01-22:39:58-root-INFO: grad norm: 0.949 0.938 0.145
2024-12-01-22:39:59-root-INFO: grad norm: 0.869 0.859 0.137
2024-12-01-22:39:59-root-INFO: grad norm: 0.850 0.841 0.126
2024-12-01-22:40:00-root-INFO: Loss Change: 10.463 -> 9.472
2024-12-01-22:40:00-root-INFO: Regularization Change: 0.000 -> 1.778
2024-12-01-22:40:00-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-01-22:40:00-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-22:40:00-root-INFO: step: 23 lr_xt 0.28268972
2024-12-01-22:40:01-root-INFO: grad norm: 1.880 1.838 0.397
2024-12-01-22:40:01-root-INFO: grad norm: 1.360 1.337 0.249
2024-12-01-22:40:02-root-INFO: grad norm: 1.058 1.045 0.167
2024-12-01-22:40:03-root-INFO: grad norm: 0.919 0.909 0.133
2024-12-01-22:40:04-root-INFO: grad norm: 0.855 0.846 0.127
2024-12-01-22:40:05-root-INFO: grad norm: 0.862 0.853 0.123
2024-12-01-22:40:06-root-INFO: grad norm: 1.140 1.131 0.145
2024-12-01-22:40:07-root-INFO: grad norm: 0.972 0.960 0.154
2024-12-01-22:40:07-root-INFO: Loss Change: 9.402 -> 8.565
2024-12-01-22:40:07-root-INFO: Regularization Change: 0.000 -> 1.568
2024-12-01-22:40:07-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-01-22:40:07-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-22:40:08-root-INFO: step: 22 lr_xt 0.28717380
2024-12-01-22:40:08-root-INFO: grad norm: 1.629 1.590 0.357
2024-12-01-22:40:09-root-INFO: grad norm: 1.169 1.154 0.187
2024-12-01-22:40:10-root-INFO: grad norm: 1.448 1.435 0.192
2024-12-01-22:40:11-root-INFO: grad norm: 1.106 1.090 0.187
2024-12-01-22:40:12-root-INFO: grad norm: 0.875 0.866 0.127
2024-12-01-22:40:12-root-INFO: grad norm: 0.780 0.774 0.104
2024-12-01-22:40:13-root-INFO: grad norm: 0.706 0.699 0.099
2024-12-01-22:40:14-root-INFO: grad norm: 0.681 0.676 0.088
2024-12-01-22:40:15-root-INFO: Loss Change: 8.596 -> 7.811
2024-12-01-22:40:15-root-INFO: Regularization Change: 0.000 -> 1.421
2024-12-01-22:40:15-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-01-22:40:15-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-22:40:15-root-INFO: step: 21 lr_xt 0.29168243
2024-12-01-22:40:15-root-INFO: grad norm: 1.754 1.719 0.348
2024-12-01-22:40:16-root-INFO: grad norm: 1.199 1.181 0.209
2024-12-01-22:40:17-root-INFO: grad norm: 0.922 0.913 0.133
2024-12-01-22:40:18-root-INFO: grad norm: 0.807 0.800 0.108
2024-12-01-22:40:19-root-INFO: grad norm: 0.730 0.724 0.100
2024-12-01-22:40:20-root-INFO: grad norm: 0.732 0.725 0.096
2024-12-01-22:40:21-root-INFO: grad norm: 1.114 1.108 0.121
2024-12-01-22:40:21-root-INFO: grad norm: 0.876 0.866 0.134
2024-12-01-22:40:22-root-INFO: Loss Change: 7.808 -> 7.134
2024-12-01-22:40:22-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-01-22:40:22-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-01-22:40:22-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-22:40:22-root-INFO: step: 20 lr_xt 0.29621455
2024-12-01-22:40:23-root-INFO: grad norm: 1.578 1.539 0.348
2024-12-01-22:40:23-root-INFO: grad norm: 1.057 1.045 0.156
2024-12-01-22:40:24-root-INFO: grad norm: 1.297 1.288 0.156
2024-12-01-22:40:25-root-INFO: grad norm: 0.972 0.961 0.149
2024-12-01-22:40:26-root-INFO: grad norm: 0.770 0.764 0.098
2024-12-01-22:40:27-root-INFO: grad norm: 0.692 0.687 0.081
2024-12-01-22:40:28-root-INFO: grad norm: 0.683 0.678 0.086
2024-12-01-22:40:29-root-INFO: grad norm: 0.730 0.724 0.090
2024-12-01-22:40:29-root-INFO: Loss Change: 7.185 -> 6.500
2024-12-01-22:40:29-root-INFO: Regularization Change: 0.000 -> 1.240
2024-12-01-22:40:29-root-INFO: Undo step: 20
2024-12-01-22:40:29-root-INFO: Undo step: 21
2024-12-01-22:40:29-root-INFO: Undo step: 22
2024-12-01-22:40:29-root-INFO: Undo step: 23
2024-12-01-22:40:29-root-INFO: Undo step: 24
2024-12-01-22:40:30-root-INFO: step: 25 lr_xt 0.27379933
2024-12-01-22:40:30-root-INFO: grad norm: 13.617 13.558 1.261
2024-12-01-22:40:31-root-INFO: grad norm: 7.300 7.256 0.800
2024-12-01-22:40:32-root-INFO: grad norm: 5.039 5.007 0.568
2024-12-01-22:40:32-root-INFO: grad norm: 3.914 3.888 0.447
2024-12-01-22:40:33-root-INFO: grad norm: 3.226 3.201 0.398
2024-12-01-22:40:34-root-INFO: grad norm: 2.752 2.730 0.346
2024-12-01-22:40:35-root-INFO: grad norm: 2.406 2.388 0.300
2024-12-01-22:40:36-root-INFO: grad norm: 2.285 2.268 0.278
2024-12-01-22:40:37-root-INFO: Loss Change: 82.615 -> 18.011
2024-12-01-22:40:37-root-INFO: Regularization Change: 0.000 -> 93.582
2024-12-01-22:40:37-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-01-22:40:37-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-22:40:37-root-INFO: step: 24 lr_xt 0.27823123
2024-12-01-22:40:37-root-INFO: grad norm: 2.256 2.232 0.328
2024-12-01-22:40:38-root-INFO: grad norm: 1.884 1.870 0.232
2024-12-01-22:40:39-root-INFO: grad norm: 1.692 1.681 0.192
2024-12-01-22:40:40-root-INFO: grad norm: 1.558 1.547 0.184
2024-12-01-22:40:41-root-INFO: grad norm: 1.458 1.448 0.165
2024-12-01-22:40:42-root-INFO: grad norm: 1.555 1.546 0.172
2024-12-01-22:40:43-root-INFO: grad norm: 1.463 1.450 0.189
2024-12-01-22:40:43-root-INFO: grad norm: 1.360 1.347 0.192
2024-12-01-22:40:44-root-INFO: Loss Change: 17.751 -> 12.689
2024-12-01-22:40:44-root-INFO: Regularization Change: 0.000 -> 10.937
2024-12-01-22:40:44-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-01-22:40:44-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-22:40:44-root-INFO: step: 23 lr_xt 0.28268972
2024-12-01-22:40:45-root-INFO: grad norm: 1.601 1.571 0.305
2024-12-01-22:40:46-root-INFO: grad norm: 1.316 1.306 0.162
2024-12-01-22:40:46-root-INFO: grad norm: 1.347 1.337 0.164
2024-12-01-22:40:47-root-INFO: grad norm: 2.085 2.073 0.219
2024-12-01-22:40:48-root-INFO: grad norm: 1.675 1.654 0.262
2024-12-01-22:40:49-root-INFO: grad norm: 1.618 1.604 0.217
2024-12-01-22:40:50-root-INFO: grad norm: 1.528 1.512 0.223
2024-12-01-22:40:51-root-INFO: grad norm: 1.415 1.402 0.191
2024-12-01-22:40:51-root-INFO: Loss Change: 12.385 -> 10.303
2024-12-01-22:40:51-root-INFO: Regularization Change: 0.000 -> 4.809
2024-12-01-22:40:51-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-01-22:40:51-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-22:40:52-root-INFO: step: 22 lr_xt 0.28717380
2024-12-01-22:40:52-root-INFO: grad norm: 1.527 1.502 0.274
2024-12-01-22:40:53-root-INFO: grad norm: 1.314 1.305 0.156
2024-12-01-22:40:54-root-INFO: grad norm: 1.356 1.346 0.168
2024-12-01-22:40:55-root-INFO: grad norm: 2.048 2.036 0.216
2024-12-01-22:40:55-root-INFO: Loss too large (9.516->9.549)! Learning rate decreased to 0.22974.
2024-12-01-22:40:56-root-INFO: grad norm: 1.434 1.420 0.200
2024-12-01-22:40:57-root-INFO: grad norm: 1.032 1.025 0.123
2024-12-01-22:40:57-root-INFO: grad norm: 0.949 0.942 0.113
2024-12-01-22:40:58-root-INFO: grad norm: 0.890 0.884 0.099
2024-12-01-22:40:59-root-INFO: Loss Change: 10.119 -> 8.822
2024-12-01-22:40:59-root-INFO: Regularization Change: 0.000 -> 2.339
2024-12-01-22:40:59-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-01-22:40:59-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-22:40:59-root-INFO: step: 21 lr_xt 0.29168243
2024-12-01-22:41:00-root-INFO: grad norm: 1.265 1.236 0.266
2024-12-01-22:41:00-root-INFO: grad norm: 0.980 0.971 0.130
2024-12-01-22:41:01-root-INFO: grad norm: 1.431 1.424 0.144
2024-12-01-22:41:02-root-INFO: grad norm: 1.139 1.128 0.162
2024-12-01-22:41:03-root-INFO: grad norm: 1.062 1.052 0.144
2024-12-01-22:41:04-root-INFO: grad norm: 1.295 1.287 0.144
2024-12-01-22:41:05-root-INFO: grad norm: 1.271 1.260 0.164
2024-12-01-22:41:06-root-INFO: grad norm: 1.231 1.224 0.136
2024-12-01-22:41:06-root-INFO: Loss Change: 8.680 -> 7.776
2024-12-01-22:41:06-root-INFO: Regularization Change: 0.000 -> 2.235
2024-12-01-22:41:06-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-01-22:41:06-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-22:41:07-root-INFO: step: 20 lr_xt 0.29621455
2024-12-01-22:41:07-root-INFO: grad norm: 1.559 1.531 0.294
2024-12-01-22:41:08-root-INFO: grad norm: 1.473 1.465 0.153
2024-12-01-22:41:08-root-INFO: Loss too large (7.429->7.445)! Learning rate decreased to 0.23697.
2024-12-01-22:41:09-root-INFO: grad norm: 1.209 1.201 0.142
2024-12-01-22:41:10-root-INFO: grad norm: 0.876 0.870 0.102
2024-12-01-22:41:11-root-INFO: grad norm: 0.844 0.839 0.096
2024-12-01-22:41:12-root-INFO: grad norm: 1.134 1.129 0.105
2024-12-01-22:41:13-root-INFO: grad norm: 0.939 0.932 0.114
2024-12-01-22:41:13-root-INFO: grad norm: 0.794 0.790 0.084
2024-12-01-22:41:14-root-INFO: Loss Change: 7.664 -> 6.862
2024-12-01-22:41:14-root-INFO: Regularization Change: 0.000 -> 1.247
2024-12-01-22:41:14-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-01-22:41:14-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-22:41:14-root-INFO: step: 19 lr_xt 0.30076908
2024-12-01-22:41:15-root-INFO: grad norm: 1.263 1.236 0.259
2024-12-01-22:41:16-root-INFO: grad norm: 0.910 0.901 0.125
2024-12-01-22:41:16-root-INFO: grad norm: 0.727 0.720 0.097
2024-12-01-22:41:17-root-INFO: grad norm: 0.762 0.757 0.090
2024-12-01-22:41:18-root-INFO: grad norm: 1.461 1.457 0.118
2024-12-01-22:41:18-root-INFO: Loss too large (6.386->6.393)! Learning rate decreased to 0.24062.
2024-12-01-22:41:19-root-INFO: grad norm: 0.767 0.761 0.097
2024-12-01-22:41:20-root-INFO: grad norm: 0.587 0.584 0.060
2024-12-01-22:41:21-root-INFO: grad norm: 0.572 0.569 0.053
2024-12-01-22:41:22-root-INFO: Loss Change: 6.777 -> 6.112
2024-12-01-22:41:22-root-INFO: Regularization Change: 0.000 -> 1.238
2024-12-01-22:41:22-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-01-22:41:22-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-22:41:22-root-INFO: step: 18 lr_xt 0.30534490
2024-12-01-22:41:22-root-INFO: grad norm: 1.394 1.361 0.300
2024-12-01-22:41:23-root-INFO: grad norm: 1.139 1.129 0.149
2024-12-01-22:41:24-root-INFO: grad norm: 1.675 1.667 0.169
2024-12-01-22:41:24-root-INFO: Loss too large (5.916->5.980)! Learning rate decreased to 0.24428.
2024-12-01-22:41:25-root-INFO: grad norm: 1.098 1.090 0.132
2024-12-01-22:41:26-root-INFO: grad norm: 0.780 0.777 0.071
2024-12-01-22:41:27-root-INFO: grad norm: 0.681 0.677 0.071
2024-12-01-22:41:28-root-INFO: grad norm: 0.650 0.647 0.062
2024-12-01-22:41:29-root-INFO: grad norm: 0.682 0.679 0.069
2024-12-01-22:41:29-root-INFO: Loss Change: 6.132 -> 5.531
2024-12-01-22:41:29-root-INFO: Regularization Change: 0.000 -> 1.012
2024-12-01-22:41:29-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-01-22:41:29-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-22:41:29-root-INFO: step: 17 lr_xt 0.30994086
2024-12-01-22:41:30-root-INFO: grad norm: 1.781 1.755 0.304
2024-12-01-22:41:31-root-INFO: grad norm: 1.208 1.197 0.156
2024-12-01-22:41:31-root-INFO: grad norm: 1.076 1.071 0.106
2024-12-01-22:41:32-root-INFO: grad norm: 1.029 1.022 0.118
2024-12-01-22:41:33-root-INFO: grad norm: 0.956 0.951 0.097
2024-12-01-22:41:34-root-INFO: grad norm: 0.961 0.955 0.110
2024-12-01-22:41:35-root-INFO: grad norm: 1.369 1.364 0.114
2024-12-01-22:41:35-root-INFO: Loss too large (5.102->5.132)! Learning rate decreased to 0.24795.
2024-12-01-22:41:36-root-INFO: grad norm: 0.854 0.849 0.097
2024-12-01-22:41:36-root-INFO: Loss Change: 5.584 -> 4.977
2024-12-01-22:41:36-root-INFO: Regularization Change: 0.000 -> 1.128
2024-12-01-22:41:36-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-01-22:41:36-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-22:41:37-root-INFO: step: 16 lr_xt 0.31455579
2024-12-01-22:41:37-root-INFO: grad norm: 1.381 1.356 0.259
2024-12-01-22:41:38-root-INFO: grad norm: 0.887 0.880 0.110
2024-12-01-22:41:39-root-INFO: grad norm: 0.740 0.736 0.071
2024-12-01-22:41:40-root-INFO: grad norm: 0.681 0.677 0.073
2024-12-01-22:41:40-root-INFO: grad norm: 0.644 0.641 0.056
2024-12-01-22:41:41-root-INFO: grad norm: 0.753 0.751 0.062
2024-12-01-22:41:42-root-INFO: grad norm: 0.745 0.742 0.063
2024-12-01-22:41:43-root-INFO: grad norm: 0.767 0.764 0.064
2024-12-01-22:41:43-root-INFO: Loss Change: 5.032 -> 4.487
2024-12-01-22:41:43-root-INFO: Regularization Change: 0.000 -> 1.115
2024-12-01-22:41:43-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-01-22:41:43-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-22:41:44-root-INFO: step: 15 lr_xt 0.31918850
2024-12-01-22:41:44-root-INFO: grad norm: 1.387 1.363 0.253
2024-12-01-22:41:45-root-INFO: grad norm: 0.846 0.841 0.090
2024-12-01-22:41:46-root-INFO: grad norm: 0.753 0.751 0.057
2024-12-01-22:41:47-root-INFO: grad norm: 1.154 1.152 0.068
2024-12-01-22:41:47-root-INFO: Loss too large (4.220->4.226)! Learning rate decreased to 0.25535.
2024-12-01-22:41:48-root-INFO: grad norm: 0.611 0.608 0.054
2024-12-01-22:41:49-root-INFO: grad norm: 0.545 0.543 0.041
2024-12-01-22:41:49-root-INFO: grad norm: 0.461 0.460 0.036
2024-12-01-22:41:50-root-INFO: grad norm: 0.519 0.517 0.043
2024-12-01-22:41:51-root-INFO: Loss Change: 4.545 -> 4.015
2024-12-01-22:41:51-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-01-22:41:51-root-INFO: Undo step: 15
2024-12-01-22:41:51-root-INFO: Undo step: 16
2024-12-01-22:41:51-root-INFO: Undo step: 17
2024-12-01-22:41:51-root-INFO: Undo step: 18
2024-12-01-22:41:51-root-INFO: Undo step: 19
2024-12-01-22:41:51-root-INFO: step: 20 lr_xt 0.29621455
2024-12-01-22:41:52-root-INFO: grad norm: 12.761 12.714 1.091
2024-12-01-22:41:52-root-INFO: grad norm: 6.784 6.748 0.700
2024-12-01-22:41:53-root-INFO: grad norm: 4.637 4.612 0.479
2024-12-01-22:41:54-root-INFO: grad norm: 3.578 3.558 0.379
2024-12-01-22:41:55-root-INFO: grad norm: 2.957 2.940 0.318
2024-12-01-22:41:56-root-INFO: grad norm: 2.562 2.546 0.279
2024-12-01-22:41:57-root-INFO: grad norm: 2.286 2.271 0.257
2024-12-01-22:41:58-root-INFO: grad norm: 2.112 2.100 0.234
2024-12-01-22:41:58-root-INFO: Loss Change: 73.844 -> 13.565
2024-12-01-22:41:58-root-INFO: Regularization Change: 0.000 -> 93.106
2024-12-01-22:41:58-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-01-22:41:58-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-22:41:59-root-INFO: step: 19 lr_xt 0.30076908
2024-12-01-22:41:59-root-INFO: grad norm: 2.413 2.392 0.318
2024-12-01-22:42:00-root-INFO: grad norm: 1.945 1.931 0.233
2024-12-01-22:42:01-root-INFO: grad norm: 1.724 1.714 0.192
2024-12-01-22:42:02-root-INFO: grad norm: 1.584 1.574 0.174
2024-12-01-22:42:03-root-INFO: grad norm: 1.511 1.501 0.171
2024-12-01-22:42:03-root-INFO: grad norm: 1.467 1.458 0.161
2024-12-01-22:42:04-root-INFO: grad norm: 1.522 1.513 0.168
2024-12-01-22:42:05-root-INFO: grad norm: 1.386 1.377 0.157
2024-12-01-22:42:06-root-INFO: Loss Change: 13.203 -> 8.832
2024-12-01-22:42:06-root-INFO: Regularization Change: 0.000 -> 10.135
2024-12-01-22:42:06-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-01-22:42:06-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-22:42:06-root-INFO: step: 18 lr_xt 0.30534490
2024-12-01-22:42:06-root-INFO: grad norm: 1.720 1.694 0.302
2024-12-01-22:42:07-root-INFO: grad norm: 1.288 1.278 0.164
2024-12-01-22:42:08-root-INFO: grad norm: 1.307 1.300 0.135
2024-12-01-22:42:09-root-INFO: grad norm: 1.172 1.164 0.133
2024-12-01-22:42:10-root-INFO: grad norm: 1.030 1.023 0.116
2024-12-01-22:42:11-root-INFO: grad norm: 0.971 0.966 0.099
2024-12-01-22:42:12-root-INFO: grad norm: 0.978 0.972 0.105
2024-12-01-22:42:13-root-INFO: grad norm: 1.225 1.221 0.097
2024-12-01-22:42:13-root-INFO: Loss Change: 8.643 -> 6.849
2024-12-01-22:42:13-root-INFO: Regularization Change: 0.000 -> 4.269
2024-12-01-22:42:13-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-01-22:42:13-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-22:42:13-root-INFO: step: 17 lr_xt 0.30994086
2024-12-01-22:42:14-root-INFO: grad norm: 1.343 1.323 0.226
2024-12-01-22:42:15-root-INFO: grad norm: 0.957 0.950 0.112
2024-12-01-22:42:15-root-INFO: grad norm: 1.055 1.050 0.099
2024-12-01-22:42:16-root-INFO: grad norm: 0.934 0.928 0.100
2024-12-01-22:42:17-root-INFO: grad norm: 0.781 0.776 0.085
2024-12-01-22:42:18-root-INFO: grad norm: 0.716 0.712 0.072
2024-12-01-22:42:19-root-INFO: grad norm: 0.703 0.699 0.073
2024-12-01-22:42:20-root-INFO: grad norm: 0.817 0.814 0.066
2024-12-01-22:42:20-root-INFO: Loss Change: 6.697 -> 5.596
2024-12-01-22:42:20-root-INFO: Regularization Change: 0.000 -> 2.454
2024-12-01-22:42:20-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-01-22:42:20-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-22:42:21-root-INFO: step: 16 lr_xt 0.31455579
2024-12-01-22:42:21-root-INFO: grad norm: 1.287 1.267 0.228
2024-12-01-22:42:22-root-INFO: grad norm: 1.040 1.036 0.089
2024-12-01-22:42:23-root-INFO: grad norm: 0.772 0.769 0.064
2024-12-01-22:42:24-root-INFO: grad norm: 0.641 0.638 0.063
2024-12-01-22:42:25-root-INFO: grad norm: 0.761 0.759 0.056
2024-12-01-22:42:26-root-INFO: grad norm: 0.737 0.733 0.071
2024-12-01-22:42:26-root-INFO: grad norm: 0.660 0.657 0.061
2024-12-01-22:42:27-root-INFO: grad norm: 0.747 0.743 0.074
2024-12-01-22:42:28-root-INFO: Loss Change: 5.521 -> 4.759
2024-12-01-22:42:28-root-INFO: Regularization Change: 0.000 -> 1.718
2024-12-01-22:42:28-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-01-22:42:28-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-22:42:28-root-INFO: step: 15 lr_xt 0.31918850
2024-12-01-22:42:29-root-INFO: grad norm: 1.883 1.855 0.323
2024-12-01-22:42:29-root-INFO: grad norm: 1.033 1.024 0.135
2024-12-01-22:42:30-root-INFO: grad norm: 0.754 0.750 0.072
2024-12-01-22:42:31-root-INFO: grad norm: 0.703 0.699 0.071
2024-12-01-22:42:32-root-INFO: grad norm: 0.589 0.587 0.053
2024-12-01-22:42:33-root-INFO: grad norm: 0.613 0.610 0.061
2024-12-01-22:42:34-root-INFO: grad norm: 0.932 0.930 0.064
2024-12-01-22:42:35-root-INFO: grad norm: 0.688 0.684 0.069
2024-12-01-22:42:35-root-INFO: Loss Change: 4.817 -> 4.096
2024-12-01-22:42:35-root-INFO: Regularization Change: 0.000 -> 1.334
2024-12-01-22:42:35-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-01-22:42:35-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-22:42:36-root-INFO: step: 14 lr_xt 0.32383775
2024-12-01-22:42:36-root-INFO: grad norm: 1.344 1.319 0.258
2024-12-01-22:42:37-root-INFO: grad norm: 0.778 0.772 0.098
2024-12-01-22:42:38-root-INFO: grad norm: 0.582 0.580 0.053
2024-12-01-22:42:38-root-INFO: grad norm: 0.595 0.593 0.048
2024-12-01-22:42:39-root-INFO: grad norm: 0.695 0.693 0.051
2024-12-01-22:42:40-root-INFO: grad norm: 1.065 1.063 0.065
2024-12-01-22:42:40-root-INFO: Loss too large (3.751->3.758)! Learning rate decreased to 0.25907.
2024-12-01-22:42:41-root-INFO: grad norm: 0.639 0.636 0.059
2024-12-01-22:42:42-root-INFO: grad norm: 0.473 0.472 0.039
2024-12-01-22:42:43-root-INFO: Loss Change: 4.153 -> 3.609
2024-12-01-22:42:43-root-INFO: Regularization Change: 0.000 -> 0.958
2024-12-01-22:42:43-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-01-22:42:43-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-22:42:43-root-INFO: step: 13 lr_xt 0.32850231
2024-12-01-22:42:43-root-INFO: grad norm: 1.358 1.332 0.262
2024-12-01-22:42:44-root-INFO: grad norm: 0.838 0.832 0.103
2024-12-01-22:42:45-root-INFO: grad norm: 0.532 0.529 0.055
2024-12-01-22:42:46-root-INFO: grad norm: 0.456 0.454 0.046
2024-12-01-22:42:47-root-INFO: grad norm: 0.591 0.589 0.042
2024-12-01-22:42:48-root-INFO: grad norm: 0.687 0.685 0.060
2024-12-01-22:42:49-root-INFO: grad norm: 0.886 0.884 0.059
2024-12-01-22:42:49-root-INFO: Loss too large (3.302->3.314)! Learning rate decreased to 0.26280.
2024-12-01-22:42:50-root-INFO: grad norm: 0.648 0.645 0.058
2024-12-01-22:42:50-root-INFO: Loss Change: 3.705 -> 3.215
2024-12-01-22:42:50-root-INFO: Regularization Change: 0.000 -> 0.869
2024-12-01-22:42:50-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-01-22:42:50-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-22:42:51-root-INFO: step: 12 lr_xt 0.33318090
2024-12-01-22:42:51-root-INFO: grad norm: 1.185 1.162 0.233
2024-12-01-22:42:52-root-INFO: grad norm: 0.835 0.830 0.094
2024-12-01-22:42:53-root-INFO: grad norm: 1.021 1.019 0.076
2024-12-01-22:42:54-root-INFO: grad norm: 0.644 0.640 0.063
2024-12-01-22:42:55-root-INFO: grad norm: 0.522 0.521 0.035
2024-12-01-22:42:56-root-INFO: grad norm: 0.564 0.563 0.032
2024-12-01-22:42:57-root-INFO: grad norm: 0.634 0.632 0.049
2024-12-01-22:42:57-root-INFO: grad norm: 0.772 0.770 0.048
2024-12-01-22:42:58-root-INFO: Loss too large (2.934->2.939)! Learning rate decreased to 0.26654.
2024-12-01-22:42:58-root-INFO: Loss Change: 3.314 -> 2.917
2024-12-01-22:42:58-root-INFO: Regularization Change: 0.000 -> 0.831
2024-12-01-22:42:58-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-01-22:42:58-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-22:42:59-root-INFO: step: 11 lr_xt 0.33787222
2024-12-01-22:42:59-root-INFO: grad norm: 1.186 1.168 0.204
2024-12-01-22:43:00-root-INFO: grad norm: 0.805 0.803 0.056
2024-12-01-22:43:01-root-INFO: grad norm: 0.678 0.676 0.049
2024-12-01-22:43:01-root-INFO: grad norm: 0.579 0.578 0.041
2024-12-01-22:43:02-root-INFO: grad norm: 0.654 0.652 0.051
2024-12-01-22:43:03-root-INFO: grad norm: 0.783 0.781 0.050
2024-12-01-22:43:04-root-INFO: grad norm: 0.657 0.655 0.057
2024-12-01-22:43:05-root-INFO: grad norm: 0.474 0.472 0.039
2024-12-01-22:43:06-root-INFO: Loss Change: 3.025 -> 2.615
2024-12-01-22:43:06-root-INFO: Regularization Change: 0.000 -> 0.782
2024-12-01-22:43:06-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-01-22:43:06-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-22:43:06-root-INFO: step: 10 lr_xt 0.34257494
2024-12-01-22:43:06-root-INFO: grad norm: 1.131 1.112 0.205
2024-12-01-22:43:07-root-INFO: grad norm: 0.640 0.638 0.049
2024-12-01-22:43:08-root-INFO: grad norm: 0.647 0.646 0.043
2024-12-01-22:43:09-root-INFO: grad norm: 0.758 0.756 0.047
2024-12-01-22:43:10-root-INFO: grad norm: 0.676 0.673 0.054
2024-12-01-22:43:11-root-INFO: grad norm: 0.563 0.561 0.042
2024-12-01-22:43:12-root-INFO: grad norm: 0.625 0.623 0.050
2024-12-01-22:43:12-root-INFO: grad norm: 0.710 0.708 0.047
2024-12-01-22:43:13-root-INFO: Loss Change: 2.760 -> 2.408
2024-12-01-22:43:13-root-INFO: Regularization Change: 0.000 -> 0.752
2024-12-01-22:43:13-root-INFO: Undo step: 10
2024-12-01-22:43:13-root-INFO: Undo step: 11
2024-12-01-22:43:13-root-INFO: Undo step: 12
2024-12-01-22:43:13-root-INFO: Undo step: 13
2024-12-01-22:43:13-root-INFO: Undo step: 14
2024-12-01-22:43:13-root-INFO: step: 15 lr_xt 0.31918850
2024-12-01-22:43:14-root-INFO: grad norm: 12.750 12.719 0.887
2024-12-01-22:43:15-root-INFO: grad norm: 6.345 6.322 0.530
2024-12-01-22:43:15-root-INFO: grad norm: 4.243 4.226 0.380
2024-12-01-22:43:16-root-INFO: grad norm: 3.212 3.197 0.306
2024-12-01-22:43:17-root-INFO: grad norm: 2.600 2.588 0.253
2024-12-01-22:43:18-root-INFO: grad norm: 2.200 2.189 0.220
2024-12-01-22:43:19-root-INFO: grad norm: 1.911 1.901 0.194
2024-12-01-22:43:20-root-INFO: grad norm: 1.688 1.679 0.174
2024-12-01-22:43:20-root-INFO: Loss Change: 68.748 -> 9.541
2024-12-01-22:43:20-root-INFO: Regularization Change: 0.000 -> 95.220
2024-12-01-22:43:21-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-01-22:43:21-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-22:43:21-root-INFO: step: 14 lr_xt 0.32383775
2024-12-01-22:43:21-root-INFO: grad norm: 1.811 1.797 0.229
2024-12-01-22:43:22-root-INFO: grad norm: 1.448 1.440 0.155
2024-12-01-22:43:23-root-INFO: grad norm: 1.286 1.279 0.125
2024-12-01-22:43:23-root-INFO: grad norm: 1.170 1.164 0.119
2024-12-01-22:43:24-root-INFO: grad norm: 1.077 1.072 0.105
2024-12-01-22:43:25-root-INFO: grad norm: 1.001 0.996 0.101
2024-12-01-22:43:26-root-INFO: grad norm: 0.937 0.933 0.091
2024-12-01-22:43:27-root-INFO: grad norm: 0.878 0.873 0.087
2024-12-01-22:43:27-root-INFO: Loss Change: 9.178 -> 5.612
2024-12-01-22:43:27-root-INFO: Regularization Change: 0.000 -> 8.851
2024-12-01-22:43:27-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-01-22:43:27-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-22:43:28-root-INFO: step: 13 lr_xt 0.32850231
2024-12-01-22:43:28-root-INFO: grad norm: 1.373 1.354 0.225
2024-12-01-22:43:29-root-INFO: grad norm: 0.999 0.992 0.119
2024-12-01-22:43:30-root-INFO: grad norm: 1.046 1.042 0.084
2024-12-01-22:43:30-root-INFO: grad norm: 0.875 0.870 0.091
2024-12-01-22:43:31-root-INFO: grad norm: 0.714 0.711 0.061
2024-12-01-22:43:32-root-INFO: grad norm: 0.646 0.643 0.063
2024-12-01-22:43:33-root-INFO: grad norm: 0.616 0.614 0.053
2024-12-01-22:43:34-root-INFO: grad norm: 0.622 0.620 0.059
2024-12-01-22:43:34-root-INFO: Loss Change: 5.466 -> 4.088
2024-12-01-22:43:34-root-INFO: Regularization Change: 0.000 -> 3.356
2024-12-01-22:43:34-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-01-22:43:34-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-22:43:35-root-INFO: step: 12 lr_xt 0.33318090
2024-12-01-22:43:35-root-INFO: grad norm: 1.420 1.402 0.226
2024-12-01-22:43:36-root-INFO: grad norm: 0.888 0.882 0.107
2024-12-01-22:43:37-root-INFO: grad norm: 0.639 0.636 0.053
2024-12-01-22:43:38-root-INFO: grad norm: 0.534 0.531 0.050
2024-12-01-22:43:39-root-INFO: grad norm: 0.487 0.485 0.039
2024-12-01-22:43:39-root-INFO: grad norm: 0.460 0.459 0.040
2024-12-01-22:43:40-root-INFO: grad norm: 0.444 0.442 0.036
2024-12-01-22:43:41-root-INFO: grad norm: 0.446 0.444 0.035
2024-12-01-22:43:42-root-INFO: Loss Change: 4.050 -> 3.252
2024-12-01-22:43:42-root-INFO: Regularization Change: 0.000 -> 1.785
2024-12-01-22:43:42-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-01-22:43:42-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-22:43:42-root-INFO: step: 11 lr_xt 0.33787222
2024-12-01-22:43:42-root-INFO: grad norm: 1.129 1.109 0.213
2024-12-01-22:43:43-root-INFO: grad norm: 0.591 0.587 0.067
2024-12-01-22:43:44-root-INFO: grad norm: 0.451 0.449 0.038
2024-12-01-22:43:45-root-INFO: grad norm: 0.403 0.402 0.033
2024-12-01-22:43:46-root-INFO: grad norm: 0.378 0.377 0.027
2024-12-01-22:43:47-root-INFO: grad norm: 0.369 0.368 0.029
2024-12-01-22:43:48-root-INFO: grad norm: 0.392 0.391 0.027
2024-12-01-22:43:48-root-INFO: grad norm: 0.482 0.480 0.038
2024-12-01-22:43:49-root-INFO: Loss Change: 3.277 -> 2.754
2024-12-01-22:43:49-root-INFO: Regularization Change: 0.000 -> 1.167
2024-12-01-22:43:49-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-01-22:43:49-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-22:43:49-root-INFO: step: 10 lr_xt 0.34257494
2024-12-01-22:43:50-root-INFO: grad norm: 1.475 1.453 0.255
2024-12-01-22:43:51-root-INFO: grad norm: 0.823 0.817 0.096
2024-12-01-22:43:51-root-INFO: grad norm: 0.485 0.483 0.046
2024-12-01-22:43:52-root-INFO: grad norm: 0.379 0.377 0.032
2024-12-01-22:43:53-root-INFO: grad norm: 0.374 0.373 0.027
2024-12-01-22:43:54-root-INFO: grad norm: 0.422 0.420 0.033
2024-12-01-22:43:55-root-INFO: grad norm: 0.536 0.535 0.033
2024-12-01-22:43:56-root-INFO: grad norm: 0.619 0.617 0.053
2024-12-01-22:43:56-root-INFO: Loss Change: 2.883 -> 2.407
2024-12-01-22:43:56-root-INFO: Regularization Change: 0.000 -> 0.887
2024-12-01-22:43:56-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-01-22:43:56-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-22:43:57-root-INFO: step: 9 lr_xt 0.34728771
2024-12-01-22:43:57-root-INFO: grad norm: 1.332 1.311 0.235
2024-12-01-22:43:58-root-INFO: grad norm: 0.826 0.821 0.089
2024-12-01-22:43:59-root-INFO: grad norm: 0.586 0.584 0.056
2024-12-01-22:44:00-root-INFO: grad norm: 0.547 0.545 0.046
2024-12-01-22:44:00-root-INFO: grad norm: 0.569 0.568 0.040
2024-12-01-22:44:01-root-INFO: grad norm: 0.566 0.564 0.047
2024-12-01-22:44:02-root-INFO: grad norm: 0.550 0.548 0.040
2024-12-01-22:44:03-root-INFO: grad norm: 0.542 0.540 0.045
2024-12-01-22:44:04-root-INFO: Loss Change: 2.554 -> 2.146
2024-12-01-22:44:04-root-INFO: Regularization Change: 0.000 -> 0.711
2024-12-01-22:44:04-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-01-22:44:04-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-22:44:04-root-INFO: step: 8 lr_xt 0.35200918
2024-12-01-22:44:04-root-INFO: grad norm: 1.237 1.216 0.225
2024-12-01-22:44:05-root-INFO: grad norm: 0.737 0.734 0.072
2024-12-01-22:44:06-root-INFO: grad norm: 0.580 0.578 0.050
2024-12-01-22:44:07-root-INFO: grad norm: 0.499 0.498 0.039
2024-12-01-22:44:08-root-INFO: grad norm: 0.456 0.455 0.033
2024-12-01-22:44:09-root-INFO: grad norm: 0.431 0.430 0.033
2024-12-01-22:44:09-root-INFO: grad norm: 0.421 0.420 0.029
2024-12-01-22:44:10-root-INFO: grad norm: 0.410 0.408 0.031
2024-12-01-22:44:11-root-INFO: Loss Change: 2.330 -> 1.951
2024-12-01-22:44:11-root-INFO: Regularization Change: 0.000 -> 0.638
2024-12-01-22:44:11-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-01-22:44:11-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-22:44:11-root-INFO: step: 7 lr_xt 0.35673794
2024-12-01-22:44:12-root-INFO: grad norm: 1.131 1.112 0.206
2024-12-01-22:44:12-root-INFO: grad norm: 0.610 0.608 0.051
2024-12-01-22:44:13-root-INFO: grad norm: 0.467 0.465 0.042
2024-12-01-22:44:14-root-INFO: grad norm: 0.384 0.383 0.029
2024-12-01-22:44:15-root-INFO: grad norm: 0.346 0.345 0.029
2024-12-01-22:44:16-root-INFO: grad norm: 0.321 0.320 0.026
2024-12-01-22:44:17-root-INFO: grad norm: 0.307 0.306 0.026
2024-12-01-22:44:18-root-INFO: grad norm: 0.301 0.300 0.024
2024-12-01-22:44:18-root-INFO: Loss Change: 2.142 -> 1.795
2024-12-01-22:44:18-root-INFO: Regularization Change: 0.000 -> 0.595
2024-12-01-22:44:18-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-01-22:44:18-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-22:44:18-root-INFO: step: 6 lr_xt 0.36147257
2024-12-01-22:44:19-root-INFO: grad norm: 0.999 0.984 0.173
2024-12-01-22:44:20-root-INFO: grad norm: 0.481 0.479 0.039
2024-12-01-22:44:20-root-INFO: grad norm: 0.356 0.354 0.034
2024-12-01-22:44:21-root-INFO: grad norm: 0.297 0.295 0.026
2024-12-01-22:44:22-root-INFO: grad norm: 0.265 0.264 0.026
2024-12-01-22:44:23-root-INFO: grad norm: 0.242 0.241 0.024
2024-12-01-22:44:24-root-INFO: grad norm: 0.227 0.225 0.024
2024-12-01-22:44:25-root-INFO: grad norm: 0.214 0.213 0.023
2024-12-01-22:44:25-root-INFO: Loss Change: 1.982 -> 1.667
2024-12-01-22:44:25-root-INFO: Regularization Change: 0.000 -> 0.562
2024-12-01-22:44:25-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-01-22:44:25-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-22:44:26-root-INFO: step: 5 lr_xt 0.36621164
2024-12-01-22:44:26-root-INFO: grad norm: 0.942 0.930 0.154
2024-12-01-22:44:27-root-INFO: grad norm: 0.410 0.408 0.035
2024-12-01-22:44:28-root-INFO: grad norm: 0.286 0.284 0.032
2024-12-01-22:44:28-root-INFO: grad norm: 0.241 0.240 0.028
2024-12-01-22:44:29-root-INFO: grad norm: 0.218 0.216 0.027
2024-12-01-22:44:30-root-INFO: grad norm: 0.202 0.200 0.026
2024-12-01-22:44:31-root-INFO: grad norm: 0.191 0.190 0.025
2024-12-01-22:44:32-root-INFO: grad norm: 0.183 0.181 0.025
2024-12-01-22:44:33-root-INFO: Loss Change: 1.861 -> 1.562
2024-12-01-22:44:33-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-01-22:44:33-root-INFO: Undo step: 5
2024-12-01-22:44:33-root-INFO: Undo step: 6
2024-12-01-22:44:33-root-INFO: Undo step: 7
2024-12-01-22:44:33-root-INFO: Undo step: 8
2024-12-01-22:44:33-root-INFO: Undo step: 9
2024-12-01-22:44:33-root-INFO: step: 10 lr_xt 0.34257494
2024-12-01-22:44:33-root-INFO: grad norm: 13.449 13.430 0.705
2024-12-01-22:44:34-root-INFO: grad norm: 5.787 5.770 0.451
2024-12-01-22:44:35-root-INFO: grad norm: 3.924 3.915 0.266
2024-12-01-22:44:36-root-INFO: grad norm: 2.860 2.852 0.222
2024-12-01-22:44:37-root-INFO: grad norm: 2.401 2.395 0.162
2024-12-01-22:44:38-root-INFO: grad norm: 1.831 1.826 0.142
2024-12-01-22:44:39-root-INFO: grad norm: 1.573 1.568 0.118
2024-12-01-22:44:40-root-INFO: grad norm: 1.369 1.365 0.106
2024-12-01-22:44:40-root-INFO: Loss Change: 60.278 -> 5.839
2024-12-01-22:44:40-root-INFO: Regularization Change: 0.000 -> 89.106
2024-12-01-22:44:40-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-01-22:44:40-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-22:44:41-root-INFO: step: 9 lr_xt 0.34728771
2024-12-01-22:44:41-root-INFO: grad norm: 1.641 1.630 0.187
2024-12-01-22:44:42-root-INFO: grad norm: 1.271 1.266 0.109
2024-12-01-22:44:43-root-INFO: grad norm: 1.037 1.035 0.072
2024-12-01-22:44:44-root-INFO: grad norm: 1.001 0.998 0.080
2024-12-01-22:44:45-root-INFO: grad norm: 1.093 1.092 0.065
2024-12-01-22:44:45-root-INFO: grad norm: 0.950 0.946 0.079
2024-12-01-22:44:46-root-INFO: grad norm: 0.800 0.798 0.054
2024-12-01-22:44:47-root-INFO: grad norm: 0.842 0.839 0.070
2024-12-01-22:44:48-root-INFO: Loss Change: 5.569 -> 3.304
2024-12-01-22:44:48-root-INFO: Regularization Change: 0.000 -> 5.992
2024-12-01-22:44:48-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-01-22:44:48-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-22:44:48-root-INFO: step: 8 lr_xt 0.35200918
2024-12-01-22:44:48-root-INFO: grad norm: 1.402 1.389 0.194
2024-12-01-22:44:49-root-INFO: grad norm: 0.934 0.930 0.084
2024-12-01-22:44:50-root-INFO: grad norm: 0.732 0.731 0.046
2024-12-01-22:44:51-root-INFO: grad norm: 0.723 0.721 0.057
2024-12-01-22:44:52-root-INFO: grad norm: 0.721 0.720 0.040
2024-12-01-22:44:53-root-INFO: grad norm: 0.700 0.698 0.056
2024-12-01-22:44:53-root-INFO: grad norm: 0.673 0.672 0.040
2024-12-01-22:44:54-root-INFO: grad norm: 0.648 0.646 0.052
2024-12-01-22:44:55-root-INFO: Loss Change: 3.265 -> 2.395
2024-12-01-22:44:55-root-INFO: Regularization Change: 0.000 -> 2.046
2024-12-01-22:44:55-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-01-22:44:55-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-22:44:55-root-INFO: step: 7 lr_xt 0.35673794
2024-12-01-22:44:56-root-INFO: grad norm: 1.201 1.186 0.189
2024-12-01-22:44:57-root-INFO: grad norm: 0.726 0.724 0.056
2024-12-01-22:44:57-root-INFO: grad norm: 0.577 0.576 0.036
2024-12-01-22:44:58-root-INFO: grad norm: 0.473 0.472 0.032
2024-12-01-22:44:59-root-INFO: grad norm: 0.431 0.431 0.025
2024-12-01-22:45:00-root-INFO: grad norm: 0.402 0.401 0.028
2024-12-01-22:45:01-root-INFO: grad norm: 0.386 0.386 0.023
2024-12-01-22:45:02-root-INFO: grad norm: 0.380 0.379 0.026
2024-12-01-22:45:03-root-INFO: Loss Change: 2.458 -> 1.953
2024-12-01-22:45:03-root-INFO: Regularization Change: 0.000 -> 1.038
2024-12-01-22:45:03-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-01-22:45:03-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-22:45:03-root-INFO: step: 6 lr_xt 0.36147257
2024-12-01-22:45:03-root-INFO: grad norm: 0.989 0.976 0.157
2024-12-01-22:45:04-root-INFO: grad norm: 0.515 0.514 0.035
2024-12-01-22:45:05-root-INFO: grad norm: 0.398 0.397 0.027
2024-12-01-22:45:06-root-INFO: grad norm: 0.348 0.347 0.023
2024-12-01-22:45:07-root-INFO: grad norm: 0.318 0.318 0.021
2024-12-01-22:45:07-root-INFO: grad norm: 0.294 0.293 0.021
2024-12-01-22:45:08-root-INFO: grad norm: 0.279 0.278 0.019
2024-12-01-22:45:09-root-INFO: grad norm: 0.266 0.266 0.020
2024-12-01-22:45:10-root-INFO: Loss Change: 2.063 -> 1.713
2024-12-01-22:45:10-root-INFO: Regularization Change: 0.000 -> 0.695
2024-12-01-22:45:10-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-01-22:45:10-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-22:45:10-root-INFO: step: 5 lr_xt 0.36621164
2024-12-01-22:45:10-root-INFO: grad norm: 0.926 0.915 0.143
2024-12-01-22:45:11-root-INFO: grad norm: 0.424 0.423 0.030
2024-12-01-22:45:12-root-INFO: grad norm: 0.283 0.282 0.026
2024-12-01-22:45:13-root-INFO: grad norm: 0.243 0.242 0.023
2024-12-01-22:45:14-root-INFO: grad norm: 0.224 0.223 0.022
2024-12-01-22:45:15-root-INFO: grad norm: 0.212 0.211 0.021
2024-12-01-22:45:16-root-INFO: grad norm: 0.203 0.202 0.021
2024-12-01-22:45:17-root-INFO: grad norm: 0.196 0.195 0.020
2024-12-01-22:45:17-root-INFO: Loss Change: 1.861 -> 1.559
2024-12-01-22:45:17-root-INFO: Regularization Change: 0.000 -> 0.578
2024-12-01-22:45:17-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-01-22:45:17-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-22:45:17-root-INFO: step: 4 lr_xt 0.37095370
2024-12-01-22:45:18-root-INFO: grad norm: 0.821 0.814 0.114
2024-12-01-22:45:19-root-INFO: grad norm: 0.373 0.372 0.030
2024-12-01-22:45:20-root-INFO: grad norm: 0.261 0.260 0.026
2024-12-01-22:45:20-root-INFO: grad norm: 0.233 0.232 0.024
2024-12-01-22:45:21-root-INFO: grad norm: 0.214 0.213 0.023
2024-12-01-22:45:22-root-INFO: grad norm: 0.205 0.203 0.023
2024-12-01-22:45:23-root-INFO: grad norm: 0.202 0.201 0.022
2024-12-01-22:45:24-root-INFO: grad norm: 0.191 0.189 0.022
2024-12-01-22:45:25-root-INFO: Loss Change: 1.707 -> 1.452
2024-12-01-22:45:25-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-01-22:45:25-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-01-22:45:25-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-22:45:25-root-INFO: step: 3 lr_xt 0.37569726
2024-12-01-22:45:25-root-INFO: grad norm: 0.807 0.801 0.097
2024-12-01-22:45:26-root-INFO: grad norm: 0.367 0.366 0.032
2024-12-01-22:45:27-root-INFO: grad norm: 0.250 0.249 0.028
2024-12-01-22:45:28-root-INFO: grad norm: 0.216 0.214 0.027
2024-12-01-22:45:29-root-INFO: grad norm: 0.192 0.190 0.026
2024-12-01-22:45:29-root-INFO: grad norm: 0.181 0.179 0.025
2024-12-01-22:45:30-root-INFO: grad norm: 0.175 0.174 0.024
2024-12-01-22:45:31-root-INFO: grad norm: 0.170 0.168 0.024
2024-12-01-22:45:32-root-INFO: Loss Change: 1.615 -> 1.369
2024-12-01-22:45:32-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-01-22:45:32-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-01-22:45:32-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-22:45:32-root-INFO: step: 2 lr_xt 0.38044082
2024-12-01-22:45:32-root-INFO: grad norm: 0.793 0.788 0.089
2024-12-01-22:45:33-root-INFO: grad norm: 0.355 0.353 0.030
2024-12-01-22:45:34-root-INFO: grad norm: 0.260 0.259 0.029
2024-12-01-22:45:35-root-INFO: grad norm: 0.236 0.235 0.028
2024-12-01-22:45:36-root-INFO: grad norm: 0.192 0.190 0.027
2024-12-01-22:45:37-root-INFO: grad norm: 0.182 0.180 0.026
2024-12-01-22:45:37-root-INFO: grad norm: 0.197 0.195 0.025
2024-12-01-22:45:38-root-INFO: grad norm: 0.190 0.188 0.024
2024-12-01-22:45:39-root-INFO: Loss Change: 1.529 -> 1.299
2024-12-01-22:45:39-root-INFO: Regularization Change: 0.000 -> 0.456
2024-12-01-22:45:39-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-01-22:45:39-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-22:45:39-root-INFO: step: 1 lr_xt 0.38518288
2024-12-01-22:45:39-root-INFO: grad norm: 0.784 0.780 0.075
2024-12-01-22:45:40-root-INFO: grad norm: 0.328 0.327 0.026
2024-12-01-22:45:41-root-INFO: grad norm: 0.247 0.246 0.023
2024-12-01-22:45:42-root-INFO: grad norm: 0.221 0.220 0.023
2024-12-01-22:45:43-root-INFO: grad norm: 0.218 0.217 0.022
2024-12-01-22:45:44-root-INFO: grad norm: 0.199 0.198 0.022
2024-12-01-22:45:45-root-INFO: grad norm: 0.182 0.180 0.021
2024-12-01-22:45:46-root-INFO: grad norm: 0.174 0.173 0.021
2024-12-01-22:45:46-root-INFO: Loss Change: 1.441 -> 1.199
2024-12-01-22:45:46-root-INFO: Regularization Change: 0.000 -> 0.516
2024-12-01-22:45:46-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-01-22:45:46-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-22:45:47-root-INFO: step: 0 lr_xt 0.38992188
2024-12-01-22:45:47-root-INFO: grad norm: 0.763 0.759 0.077
2024-12-01-22:45:48-root-INFO: grad norm: 0.471 0.470 0.023
2024-12-01-22:45:49-root-INFO: grad norm: 0.383 0.383 0.025
2024-12-01-22:45:50-root-INFO: grad norm: 0.342 0.340 0.038
2024-12-01-22:45:50-root-INFO: grad norm: 0.320 0.316 0.046
2024-12-01-22:45:51-root-INFO: grad norm: 0.306 0.302 0.050
2024-12-01-22:45:52-root-INFO: grad norm: 0.298 0.294 0.051
2024-12-01-22:45:53-root-INFO: grad norm: 0.293 0.288 0.052
2024-12-01-22:45:54-root-INFO: Loss Change: 1.341 -> 0.848
2024-12-01-22:45:54-root-INFO: Regularization Change: 0.000 -> 1.297
2024-12-01-22:45:54-root-INFO: Undo step: 0
2024-12-01-22:45:54-root-INFO: Undo step: 1
2024-12-01-22:45:54-root-INFO: Undo step: 2
2024-12-01-22:45:54-root-INFO: Undo step: 3
2024-12-01-22:45:54-root-INFO: Undo step: 4
2024-12-01-22:45:54-root-INFO: step: 5 lr_xt 0.36621164
2024-12-01-22:45:54-root-INFO: grad norm: 12.486 12.479 0.409
2024-12-01-22:45:55-root-INFO: grad norm: 5.425 5.422 0.176
2024-12-01-22:45:56-root-INFO: grad norm: 3.412 3.409 0.124
2024-12-01-22:45:57-root-INFO: grad norm: 2.021 2.019 0.083
2024-12-01-22:45:58-root-INFO: grad norm: 1.442 1.440 0.076
2024-12-01-22:45:59-root-INFO: grad norm: 1.195 1.194 0.056
2024-12-01-22:46:00-root-INFO: grad norm: 1.022 1.020 0.059
2024-12-01-22:46:01-root-INFO: grad norm: 0.871 0.870 0.044
2024-12-01-22:46:01-root-INFO: Loss Change: 37.973 -> 2.969
2024-12-01-22:46:01-root-INFO: Regularization Change: 0.000 -> 56.714
2024-12-01-22:46:01-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-01-22:46:01-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-22:46:01-root-INFO: step: 4 lr_xt 0.37095370
2024-12-01-22:46:02-root-INFO: grad norm: 1.051 1.048 0.067
2024-12-01-22:46:03-root-INFO: grad norm: 0.672 0.671 0.033
2024-12-01-22:46:04-root-INFO: grad norm: 0.575 0.574 0.030
2024-12-01-22:46:04-root-INFO: grad norm: 0.509 0.508 0.029
2024-12-01-22:46:05-root-INFO: grad norm: 0.460 0.460 0.028
2024-12-01-22:46:06-root-INFO: grad norm: 0.551 0.551 0.027
2024-12-01-22:46:07-root-INFO: grad norm: 0.389 0.388 0.025
2024-12-01-22:46:08-root-INFO: grad norm: 0.358 0.357 0.025
2024-12-01-22:46:08-root-INFO: Loss Change: 2.859 -> 1.912
2024-12-01-22:46:08-root-INFO: Regularization Change: 0.000 -> 2.350
2024-12-01-22:46:08-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-01-22:46:08-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-22:46:09-root-INFO: step: 3 lr_xt 0.37569726
2024-12-01-22:46:09-root-INFO: grad norm: 0.776 0.772 0.076
2024-12-01-22:46:10-root-INFO: grad norm: 0.428 0.427 0.029
2024-12-01-22:46:11-root-INFO: grad norm: 0.351 0.350 0.026
2024-12-01-22:46:12-root-INFO: grad norm: 0.317 0.316 0.025
2024-12-01-22:46:12-root-INFO: grad norm: 0.292 0.291 0.024
2024-12-01-22:46:13-root-INFO: grad norm: 0.273 0.272 0.024
2024-12-01-22:46:14-root-INFO: grad norm: 0.257 0.256 0.023
2024-12-01-22:46:15-root-INFO: grad norm: 0.243 0.242 0.023
2024-12-01-22:46:16-root-INFO: Loss Change: 1.970 -> 1.580
2024-12-01-22:46:16-root-INFO: Regularization Change: 0.000 -> 0.952
2024-12-01-22:46:16-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-01-22:46:16-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-22:46:16-root-INFO: step: 2 lr_xt 0.38044082
2024-12-01-22:46:16-root-INFO: grad norm: 0.731 0.727 0.077
2024-12-01-22:46:17-root-INFO: grad norm: 0.375 0.373 0.031
2024-12-01-22:46:18-root-INFO: grad norm: 0.312 0.311 0.027
2024-12-01-22:46:19-root-INFO: grad norm: 0.261 0.260 0.026
2024-12-01-22:46:20-root-INFO: grad norm: 0.233 0.232 0.025
2024-12-01-22:46:21-root-INFO: grad norm: 0.223 0.222 0.023
2024-12-01-22:46:21-root-INFO: grad norm: 0.205 0.203 0.023
2024-12-01-22:46:22-root-INFO: grad norm: 0.195 0.194 0.021
2024-12-01-22:46:23-root-INFO: Loss Change: 1.690 -> 1.407
2024-12-01-22:46:23-root-INFO: Regularization Change: 0.000 -> 0.638
2024-12-01-22:46:23-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-01-22:46:23-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-22:46:23-root-INFO: step: 1 lr_xt 0.38518288
2024-12-01-22:46:23-root-INFO: grad norm: 0.740 0.733 0.100
2024-12-01-22:46:24-root-INFO: grad norm: 0.362 0.361 0.027
2024-12-01-22:46:25-root-INFO: grad norm: 0.520 0.519 0.022
2024-12-01-22:46:26-root-INFO: grad norm: 0.225 0.224 0.018
2024-12-01-22:46:27-root-INFO: grad norm: 0.209 0.208 0.018
2024-12-01-22:46:28-root-INFO: grad norm: 0.197 0.196 0.017
2024-12-01-22:46:29-root-INFO: grad norm: 0.192 0.192 0.017
2024-12-01-22:46:30-root-INFO: grad norm: 0.177 0.176 0.016
2024-12-01-22:46:30-root-INFO: Loss Change: 1.526 -> 1.195
2024-12-01-22:46:30-root-INFO: Regularization Change: 0.000 -> 0.602
2024-12-01-22:46:30-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-01-22:46:30-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-22:46:30-root-INFO: loss_sample0_0: 1.195175290107727
2024-12-01-22:46:31-root-INFO: It takes 3940.318 seconds for image sample0
2024-12-01-22:46:31-root-INFO: lpips_score_sample0: 0.133
2024-12-01-22:46:31-root-INFO: psnr_score_sample0: 17.555
2024-12-01-22:46:31-root-INFO: ssim_score_sample0: 0.715
2024-12-01-22:46:31-root-INFO: mean_lpips: 0.13283520936965942
2024-12-01-22:46:31-root-INFO: best_mean_lpips: 0.13283520936965942
2024-12-01-22:46:31-root-INFO: mean_psnr: 17.55508041381836
2024-12-01-22:46:31-root-INFO: best_mean_psnr: 17.55508041381836
2024-12-01-22:46:31-root-INFO: mean_ssim: 0.7145026922225952
2024-12-01-22:46:31-root-INFO: best_mean_ssim: 0.7145026922225952
2024-12-01-22:46:31-root-INFO: final_loss: 1.195175290107727
2024-12-01-22:46:31-root-INFO: mean time: 3940.3178753852844
2024-12-01-22:46:31-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample2_iter8_lr0.02_10009 
 
Enjoy.
