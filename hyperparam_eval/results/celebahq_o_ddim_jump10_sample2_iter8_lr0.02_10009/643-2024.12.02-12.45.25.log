2024-12-02-12:45:29-root-INFO: Prepare model...
2024-12-02-12:45:46-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-12:46:08-root-INFO: Start sampling
2024-12-02-12:46:15-root-INFO: step: 249 lr_xt 0.00012706
2024-12-02-12:46:15-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-12:46:15-root-INFO: grad norm: 23714.531 17716.891 15763.591
2024-12-02-12:46:16-root-INFO: grad norm: 26795.592 21197.270 16391.447
2024-12-02-12:46:16-root-INFO: Loss too large (43815.273->60949.164)! Learning rate decreased to 0.00010.
2024-12-02-12:46:16-root-INFO: Loss too large (43815.273->45480.668)! Learning rate decreased to 0.00008.
2024-12-02-12:46:17-root-INFO: grad norm: 21164.383 16200.010 13619.500
2024-12-02-12:46:17-root-INFO: grad norm: 19145.453 15680.585 10984.882
2024-12-02-12:46:18-root-INFO: grad norm: 17825.619 14002.300 11031.240
2024-12-02-12:46:18-root-INFO: grad norm: 16612.221 13521.523 9650.610
2024-12-02-12:46:19-root-INFO: grad norm: 15351.146 12275.325 9218.137
2024-12-02-12:46:19-root-INFO: Loss Change: 77070.016 -> 21842.971
2024-12-02-12:46:19-root-INFO: Regularization Change: 0.000 -> 14.877
2024-12-02-12:46:19-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-02-12:46:19-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-12:46:19-root-INFO: step: 248 lr_xt 0.00013388
2024-12-02-12:46:19-root-INFO: grad norm: 13708.375 11022.005 8150.764
2024-12-02-12:46:19-root-INFO: Loss too large (22203.645->32157.822)! Learning rate decreased to 0.00011.
2024-12-02-12:46:20-root-INFO: Loss too large (22203.645->25109.592)! Learning rate decreased to 0.00009.
2024-12-02-12:46:20-root-INFO: grad norm: 12006.669 9573.952 7245.656
2024-12-02-12:46:21-root-INFO: grad norm: 10369.205 8293.078 6224.569
2024-12-02-12:46:21-root-INFO: grad norm: 8986.155 7202.951 5372.940
2024-12-02-12:46:21-root-INFO: grad norm: 7667.684 6109.427 4633.388
2024-12-02-12:46:22-root-INFO: grad norm: 6571.083 5282.086 3908.798
2024-12-02-12:46:22-root-INFO: grad norm: 5544.211 4408.660 3361.844
2024-12-02-12:46:23-root-INFO: grad norm: 4704.111 3786.989 2790.587
2024-12-02-12:46:23-root-INFO: Loss Change: 22203.645 -> 16909.949
2024-12-02-12:46:23-root-INFO: Regularization Change: 0.000 -> 1.223
2024-12-02-12:46:23-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-02-12:46:23-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-12:46:23-root-INFO: step: 247 lr_xt 0.00014104
2024-12-02-12:46:23-root-INFO: grad norm: 3928.349 3123.766 2382.018
2024-12-02-12:46:24-root-INFO: Loss too large (16757.510->17452.559)! Learning rate decreased to 0.00011.
2024-12-02-12:46:24-root-INFO: Loss too large (16757.510->16866.113)! Learning rate decreased to 0.00009.
2024-12-02-12:46:24-root-INFO: grad norm: 3143.009 2560.822 1822.277
2024-12-02-12:46:25-root-INFO: grad norm: 2509.070 2011.080 1500.329
2024-12-02-12:46:25-root-INFO: grad norm: 2035.830 1659.319 1179.519
2024-12-02-12:46:26-root-INFO: grad norm: 1667.899 1355.714 971.559
2024-12-02-12:46:26-root-INFO: grad norm: 1396.816 1141.453 805.097
2024-12-02-12:46:27-root-INFO: grad norm: 1192.977 990.862 664.370
2024-12-02-12:46:27-root-INFO: grad norm: 1046.524 862.454 592.777
2024-12-02-12:46:27-root-INFO: Loss Change: 16757.510 -> 15907.394
2024-12-02-12:46:27-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-12:46:27-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-02-12:46:27-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-12:46:27-root-INFO: step: 246 lr_xt 0.00014856
2024-12-02-12:46:28-root-INFO: grad norm: 1025.713 863.519 553.553
2024-12-02-12:46:28-root-INFO: grad norm: 1243.270 1016.612 715.695
2024-12-02-12:46:28-root-INFO: grad norm: 1797.462 1493.033 1000.860
2024-12-02-12:46:29-root-INFO: Loss too large (15567.889->15621.016)! Learning rate decreased to 0.00012.
2024-12-02-12:46:29-root-INFO: grad norm: 2024.245 1622.253 1210.728
2024-12-02-12:46:30-root-INFO: grad norm: 2313.854 1915.452 1298.062
2024-12-02-12:46:30-root-INFO: grad norm: 2647.527 2114.611 1593.053
2024-12-02-12:46:30-root-INFO: grad norm: 3056.320 2514.434 1737.445
2024-12-02-12:46:31-root-INFO: Loss too large (15468.349->15496.668)! Learning rate decreased to 0.00010.
2024-12-02-12:46:31-root-INFO: grad norm: 2305.053 1838.846 1389.933
2024-12-02-12:46:31-root-INFO: Loss Change: 15687.739 -> 15196.258
2024-12-02-12:46:31-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-02-12:46:31-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-02-12:46:31-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-12:46:32-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-12:46:32-root-INFO: grad norm: 1696.241 1421.098 926.129
2024-12-02-12:46:32-root-INFO: Loss too large (15078.010->15098.340)! Learning rate decreased to 0.00013.
2024-12-02-12:46:32-root-INFO: grad norm: 1815.406 1444.894 1099.081
2024-12-02-12:46:33-root-INFO: grad norm: 1979.362 1662.217 1074.666
2024-12-02-12:46:33-root-INFO: grad norm: 2166.350 1723.967 1311.873
2024-12-02-12:46:34-root-INFO: grad norm: 2387.099 1993.163 1313.599
2024-12-02-12:46:34-root-INFO: grad norm: 2630.107 2095.720 1589.156
2024-12-02-12:46:35-root-INFO: grad norm: 2915.547 2418.770 1627.872
2024-12-02-12:46:35-root-INFO: grad norm: 3213.979 2566.417 1934.725
2024-12-02-12:46:35-root-INFO: Loss Change: 15078.010 -> 14828.500
2024-12-02-12:46:35-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-02-12:46:35-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-12:46:35-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-12:46:36-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-12:46:36-root-INFO: grad norm: 3662.189 2984.056 2122.978
2024-12-02-12:46:36-root-INFO: Loss too large (14749.840->15195.176)! Learning rate decreased to 0.00013.
2024-12-02-12:46:36-root-INFO: grad norm: 3763.768 3040.748 2218.062
2024-12-02-12:46:37-root-INFO: grad norm: 3953.133 3260.531 2235.219
2024-12-02-12:46:37-root-INFO: Loss too large (14634.407->14640.084)! Learning rate decreased to 0.00011.
2024-12-02-12:46:37-root-INFO: grad norm: 2665.408 2151.879 1572.837
2024-12-02-12:46:38-root-INFO: grad norm: 1815.430 1544.490 954.115
2024-12-02-12:46:38-root-INFO: grad norm: 1324.465 1080.731 765.654
2024-12-02-12:46:39-root-INFO: grad norm: 1032.758 918.593 471.992
2024-12-02-12:46:39-root-INFO: grad norm: 877.750 746.534 461.661
2024-12-02-12:46:40-root-INFO: Loss Change: 14749.840 -> 13815.977
2024-12-02-12:46:40-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-02-12:46:40-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-12:46:40-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:46:40-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-12:46:40-root-INFO: grad norm: 1012.135 835.383 571.448
2024-12-02-12:46:40-root-INFO: grad norm: 976.570 786.520 578.857
2024-12-02-12:46:41-root-INFO: grad norm: 1059.958 889.737 576.089
2024-12-02-12:46:41-root-INFO: grad norm: 1232.780 958.713 774.994
2024-12-02-12:46:42-root-INFO: grad norm: 1535.382 1256.232 882.769
2024-12-02-12:46:42-root-INFO: grad norm: 2009.371 1553.408 1274.556
2024-12-02-12:46:42-root-INFO: Loss too large (13320.209->13353.455)! Learning rate decreased to 0.00014.
2024-12-02-12:46:43-root-INFO: grad norm: 1902.963 1572.300 1071.980
2024-12-02-12:46:43-root-INFO: grad norm: 1856.782 1476.596 1125.746
2024-12-02-12:46:44-root-INFO: Loss Change: 13697.792 -> 13074.315
2024-12-02-12:46:44-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-02-12:46:44-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-12:46:44-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:46:44-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-12:46:44-root-INFO: grad norm: 1941.722 1602.220 1096.894
2024-12-02-12:46:44-root-INFO: Loss too large (12880.828->12895.684)! Learning rate decreased to 0.00015.
2024-12-02-12:46:45-root-INFO: grad norm: 1834.207 1488.912 1071.193
2024-12-02-12:46:45-root-INFO: grad norm: 1779.021 1510.874 939.241
2024-12-02-12:46:46-root-INFO: grad norm: 1751.727 1429.812 1012.020
2024-12-02-12:46:46-root-INFO: grad norm: 1740.007 1489.301 899.783
2024-12-02-12:46:47-root-INFO: grad norm: 1739.278 1425.712 996.210
2024-12-02-12:46:47-root-INFO: grad norm: 1740.909 1491.555 897.791
2024-12-02-12:46:48-root-INFO: grad norm: 1743.896 1434.121 992.205
2024-12-02-12:46:48-root-INFO: Loss Change: 12880.828 -> 12240.491
2024-12-02-12:46:48-root-INFO: Regularization Change: 0.000 -> 0.714
2024-12-02-12:46:48-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-12:46:48-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:46:48-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-12:46:49-root-INFO: grad norm: 1777.332 1513.278 932.146
2024-12-02-12:46:49-root-INFO: Loss too large (12146.662->12161.826)! Learning rate decreased to 0.00015.
2024-12-02-12:46:49-root-INFO: grad norm: 1713.125 1431.928 940.413
2024-12-02-12:46:50-root-INFO: grad norm: 1659.982 1427.137 847.833
2024-12-02-12:46:50-root-INFO: grad norm: 1610.159 1344.414 886.094
2024-12-02-12:46:51-root-INFO: grad norm: 1560.792 1347.102 788.282
2024-12-02-12:46:51-root-INFO: grad norm: 1514.715 1266.692 830.573
2024-12-02-12:46:52-root-INFO: grad norm: 1468.783 1271.630 735.037
2024-12-02-12:46:53-root-INFO: grad norm: 1422.376 1192.704 774.990
2024-12-02-12:46:53-root-INFO: Loss Change: 12146.662 -> 11490.700
2024-12-02-12:46:53-root-INFO: Regularization Change: 0.000 -> 0.749
2024-12-02-12:46:53-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-12:46:53-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:46:53-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-12:46:53-root-INFO: grad norm: 1328.058 1143.592 675.231
2024-12-02-12:46:54-root-INFO: grad norm: 1669.003 1409.421 893.925
2024-12-02-12:46:54-root-INFO: grad norm: 2233.573 1916.302 1147.447
2024-12-02-12:46:54-root-INFO: Loss too large (11227.926->11318.254)! Learning rate decreased to 0.00016.
2024-12-02-12:46:55-root-INFO: grad norm: 2071.255 1740.282 1123.173
2024-12-02-12:46:55-root-INFO: grad norm: 1920.710 1654.832 975.017
2024-12-02-12:46:56-root-INFO: grad norm: 1784.779 1503.289 962.059
2024-12-02-12:46:56-root-INFO: grad norm: 1658.229 1435.477 830.138
2024-12-02-12:46:57-root-INFO: grad norm: 1544.262 1305.026 825.622
2024-12-02-12:46:57-root-INFO: Loss Change: 11297.599 -> 10670.289
2024-12-02-12:46:57-root-INFO: Regularization Change: 0.000 -> 0.879
2024-12-02-12:46:57-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-12:46:57-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:46:57-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-12:46:57-root-INFO: grad norm: 1344.007 1169.454 662.368
2024-12-02-12:46:58-root-INFO: grad norm: 1696.746 1436.272 903.367
2024-12-02-12:46:58-root-INFO: grad norm: 2203.857 1891.538 1130.961
2024-12-02-12:46:59-root-INFO: Loss too large (10526.176->10601.539)! Learning rate decreased to 0.00017.
2024-12-02-12:46:59-root-INFO: grad norm: 1953.764 1653.244 1041.144
2024-12-02-12:47:00-root-INFO: grad norm: 1733.572 1500.018 869.032
2024-12-02-12:47:00-root-INFO: grad norm: 1542.444 1309.818 814.562
2024-12-02-12:47:01-root-INFO: grad norm: 1374.558 1201.168 668.284
2024-12-02-12:47:01-root-INFO: grad norm: 1230.396 1050.977 639.783
2024-12-02-12:47:01-root-INFO: Loss Change: 10571.225 -> 9962.159
2024-12-02-12:47:01-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-12:47:01-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-12:47:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:02-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-12:47:02-root-INFO: grad norm: 1004.913 858.158 522.890
2024-12-02-12:47:02-root-INFO: grad norm: 1050.417 903.388 535.972
2024-12-02-12:47:03-root-INFO: grad norm: 1231.845 1082.196 588.466
2024-12-02-12:47:03-root-INFO: grad norm: 1493.791 1272.144 782.982
2024-12-02-12:47:04-root-INFO: grad norm: 1851.861 1611.543 912.315
2024-12-02-12:47:04-root-INFO: Loss too large (9541.749->9570.717)! Learning rate decreased to 0.00018.
2024-12-02-12:47:04-root-INFO: grad norm: 1553.075 1327.897 805.438
2024-12-02-12:47:05-root-INFO: grad norm: 1309.919 1156.782 614.608
2024-12-02-12:47:05-root-INFO: grad norm: 1118.364 964.767 565.653
2024-12-02-12:47:06-root-INFO: Loss Change: 9738.753 -> 9208.703
2024-12-02-12:47:06-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-02-12:47:06-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-12:47:06-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:06-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-12:47:06-root-INFO: grad norm: 744.894 676.604 311.569
2024-12-02-12:47:06-root-INFO: grad norm: 796.281 698.717 381.914
2024-12-02-12:47:07-root-INFO: grad norm: 892.843 810.013 375.564
2024-12-02-12:47:07-root-INFO: grad norm: 1025.823 890.120 509.901
2024-12-02-12:47:08-root-INFO: grad norm: 1200.404 1070.696 542.751
2024-12-02-12:47:08-root-INFO: grad norm: 1415.523 1220.828 716.439
2024-12-02-12:47:09-root-INFO: grad norm: 1683.376 1482.874 796.769
2024-12-02-12:47:09-root-INFO: Loss too large (8878.984->8890.061)! Learning rate decreased to 0.00019.
2024-12-02-12:47:09-root-INFO: grad norm: 1330.642 1148.714 671.613
2024-12-02-12:47:10-root-INFO: Loss Change: 9121.887 -> 8702.750
2024-12-02-12:47:10-root-INFO: Regularization Change: 0.000 -> 0.812
2024-12-02-12:47:10-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-12:47:10-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:10-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-12:47:10-root-INFO: grad norm: 985.061 872.786 456.716
2024-12-02-12:47:11-root-INFO: grad norm: 1104.728 963.696 540.105
2024-12-02-12:47:11-root-INFO: grad norm: 1254.530 1110.431 583.770
2024-12-02-12:47:12-root-INFO: grad norm: 1430.610 1238.308 716.406
2024-12-02-12:47:12-root-INFO: grad norm: 1635.757 1438.794 778.185
2024-12-02-12:47:13-root-INFO: grad norm: 1862.563 1608.196 939.599
2024-12-02-12:47:13-root-INFO: Loss too large (8442.844->8451.315)! Learning rate decreased to 0.00020.
2024-12-02-12:47:13-root-INFO: grad norm: 1369.403 1210.698 639.900
2024-12-02-12:47:14-root-INFO: grad norm: 1032.400 897.155 510.845
2024-12-02-12:47:14-root-INFO: Loss Change: 8561.852 -> 8164.139
2024-12-02-12:47:14-root-INFO: Regularization Change: 0.000 -> 0.702
2024-12-02-12:47:14-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-12:47:14-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:14-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-12:47:14-root-INFO: grad norm: 523.866 484.210 199.939
2024-12-02-12:47:15-root-INFO: grad norm: 520.154 469.353 224.206
2024-12-02-12:47:15-root-INFO: grad norm: 529.921 495.669 187.425
2024-12-02-12:47:16-root-INFO: grad norm: 545.813 490.261 239.909
2024-12-02-12:47:16-root-INFO: grad norm: 565.669 526.365 207.174
2024-12-02-12:47:17-root-INFO: grad norm: 589.794 526.031 266.735
2024-12-02-12:47:17-root-INFO: grad norm: 618.016 570.309 238.099
2024-12-02-12:47:18-root-INFO: grad norm: 650.389 575.913 302.208
2024-12-02-12:47:18-root-INFO: Loss Change: 8058.457 -> 7726.406
2024-12-02-12:47:18-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-02-12:47:18-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-12:47:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-12:47:19-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-12:47:19-root-INFO: grad norm: 605.275 551.474 249.469
2024-12-02-12:47:19-root-INFO: grad norm: 605.967 539.643 275.648
2024-12-02-12:47:20-root-INFO: grad norm: 618.978 567.608 246.890
2024-12-02-12:47:20-root-INFO: grad norm: 633.617 562.302 292.039
2024-12-02-12:47:21-root-INFO: grad norm: 649.326 593.899 262.504
2024-12-02-12:47:21-root-INFO: grad norm: 665.323 588.442 310.470
2024-12-02-12:47:22-root-INFO: grad norm: 681.719 621.352 280.467
2024-12-02-12:47:22-root-INFO: grad norm: 697.612 615.477 328.404
2024-12-02-12:47:22-root-INFO: Loss Change: 7661.216 -> 7368.023
2024-12-02-12:47:22-root-INFO: Regularization Change: 0.000 -> 0.668
2024-12-02-12:47:22-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-12:47:22-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:47:22-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-12:47:23-root-INFO: grad norm: 589.326 513.676 288.864
2024-12-02-12:47:23-root-INFO: grad norm: 533.641 471.809 249.337
2024-12-02-12:47:24-root-INFO: grad norm: 512.561 469.169 206.397
2024-12-02-12:47:24-root-INFO: grad norm: 500.707 448.811 221.982
2024-12-02-12:47:25-root-INFO: grad norm: 492.801 455.152 188.914
2024-12-02-12:47:25-root-INFO: grad norm: 486.013 436.936 212.827
2024-12-02-12:47:26-root-INFO: grad norm: 480.374 444.703 181.655
2024-12-02-12:47:26-root-INFO: grad norm: 475.437 427.745 207.545
2024-12-02-12:47:26-root-INFO: Loss Change: 7325.431 -> 7039.955
2024-12-02-12:47:26-root-INFO: Regularization Change: 0.000 -> 0.636
2024-12-02-12:47:26-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-12:47:26-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:47:27-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-12:47:27-root-INFO: grad norm: 478.967 405.986 254.134
2024-12-02-12:47:27-root-INFO: grad norm: 376.776 346.407 148.198
2024-12-02-12:47:28-root-INFO: grad norm: 349.896 318.747 144.317
2024-12-02-12:47:28-root-INFO: grad norm: 337.552 318.560 111.631
2024-12-02-12:47:29-root-INFO: grad norm: 329.986 305.430 124.913
2024-12-02-12:47:29-root-INFO: grad norm: 324.367 307.314 103.789
2024-12-02-12:47:30-root-INFO: grad norm: 319.655 297.315 117.401
2024-12-02-12:47:30-root-INFO: grad norm: 315.494 299.274 99.856
2024-12-02-12:47:30-root-INFO: Loss Change: 6914.946 -> 6666.213
2024-12-02-12:47:30-root-INFO: Regularization Change: 0.000 -> 0.574
2024-12-02-12:47:30-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-12:47:30-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:47:31-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-12:47:31-root-INFO: grad norm: 406.315 367.365 173.593
2024-12-02-12:47:31-root-INFO: grad norm: 387.994 362.713 137.764
2024-12-02-12:47:32-root-INFO: grad norm: 375.235 342.237 153.865
2024-12-02-12:47:32-root-INFO: grad norm: 364.035 341.586 125.859
2024-12-02-12:47:33-root-INFO: grad norm: 354.021 323.611 143.549
2024-12-02-12:47:33-root-INFO: grad norm: 344.572 324.006 117.259
2024-12-02-12:47:34-root-INFO: grad norm: 335.760 307.970 133.751
2024-12-02-12:47:34-root-INFO: grad norm: 327.366 308.386 109.848
2024-12-02-12:47:34-root-INFO: Loss Change: 6615.115 -> 6413.511
2024-12-02-12:47:34-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-02-12:47:34-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-12:47:34-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:47:35-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-12:47:35-root-INFO: grad norm: 517.792 458.851 239.926
2024-12-02-12:47:35-root-INFO: grad norm: 480.476 444.052 183.508
2024-12-02-12:47:36-root-INFO: grad norm: 450.380 404.460 198.125
2024-12-02-12:47:36-root-INFO: grad norm: 424.013 392.480 160.457
2024-12-02-12:47:37-root-INFO: grad norm: 400.616 362.394 170.772
2024-12-02-12:47:37-root-INFO: grad norm: 379.523 352.255 141.258
2024-12-02-12:47:38-root-INFO: grad norm: 361.094 328.528 149.860
2024-12-02-12:47:38-root-INFO: grad norm: 344.766 321.046 125.671
2024-12-02-12:47:38-root-INFO: Loss Change: 6348.473 -> 6156.078
2024-12-02-12:47:39-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-12:47:39-root-INFO: Undo step: 230
2024-12-02-12:47:39-root-INFO: Undo step: 231
2024-12-02-12:47:39-root-INFO: Undo step: 232
2024-12-02-12:47:39-root-INFO: Undo step: 233
2024-12-02-12:47:39-root-INFO: Undo step: 234
2024-12-02-12:47:39-root-INFO: Undo step: 235
2024-12-02-12:47:39-root-INFO: Undo step: 236
2024-12-02-12:47:39-root-INFO: Undo step: 237
2024-12-02-12:47:39-root-INFO: Undo step: 238
2024-12-02-12:47:39-root-INFO: Undo step: 239
2024-12-02-12:47:39-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-12:47:39-root-INFO: grad norm: 19862.537 16842.971 10527.810
2024-12-02-12:47:39-root-INFO: grad norm: 15472.353 12686.601 8856.854
2024-12-02-12:47:40-root-INFO: grad norm: 9195.425 7335.608 5544.791
2024-12-02-12:47:40-root-INFO: grad norm: 7460.477 6264.433 4051.617
2024-12-02-12:47:41-root-INFO: grad norm: 6689.960 5465.363 3858.156
2024-12-02-12:47:41-root-INFO: Loss too large (11792.019->11956.769)! Learning rate decreased to 0.00016.
2024-12-02-12:47:41-root-INFO: grad norm: 4645.127 3905.128 2515.388
2024-12-02-12:47:42-root-INFO: grad norm: 3324.066 2762.831 1848.290
2024-12-02-12:47:42-root-INFO: grad norm: 2656.376 2209.311 1474.884
2024-12-02-12:47:43-root-INFO: Loss Change: 29656.699 -> 9251.938
2024-12-02-12:47:43-root-INFO: Regularization Change: 0.000 -> 5.819
2024-12-02-12:47:43-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-12:47:43-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:43-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-12:47:43-root-INFO: grad norm: 1999.231 1702.481 1048.086
2024-12-02-12:47:43-root-INFO: Loss too large (9103.597->9106.066)! Learning rate decreased to 0.00017.
2024-12-02-12:47:44-root-INFO: grad norm: 1646.818 1373.715 908.249
2024-12-02-12:47:44-root-INFO: grad norm: 1371.433 1188.039 685.123
2024-12-02-12:47:45-root-INFO: grad norm: 1163.594 979.130 628.693
2024-12-02-12:47:45-root-INFO: grad norm: 997.374 878.372 472.460
2024-12-02-12:47:46-root-INFO: grad norm: 871.305 744.933 451.937
2024-12-02-12:47:46-root-INFO: grad norm: 771.759 691.669 342.354
2024-12-02-12:47:47-root-INFO: grad norm: 695.117 607.141 338.477
2024-12-02-12:47:47-root-INFO: Loss Change: 9103.597 -> 8459.053
2024-12-02-12:47:47-root-INFO: Regularization Change: 0.000 -> 0.682
2024-12-02-12:47:47-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-12:47:47-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:47-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-12:47:47-root-INFO: grad norm: 779.379 649.486 430.811
2024-12-02-12:47:48-root-INFO: grad norm: 576.657 503.598 280.931
2024-12-02-12:47:48-root-INFO: grad norm: 541.974 483.482 244.911
2024-12-02-12:47:49-root-INFO: grad norm: 519.147 464.540 231.768
2024-12-02-12:47:49-root-INFO: grad norm: 502.646 456.639 210.078
2024-12-02-12:47:50-root-INFO: grad norm: 491.779 444.618 210.147
2024-12-02-12:47:50-root-INFO: grad norm: 486.656 446.781 192.926
2024-12-02-12:47:51-root-INFO: grad norm: 488.532 442.025 208.032
2024-12-02-12:47:51-root-INFO: Loss Change: 8269.195 -> 7860.940
2024-12-02-12:47:51-root-INFO: Regularization Change: 0.000 -> 0.688
2024-12-02-12:47:51-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-12:47:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:51-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-12:47:51-root-INFO: grad norm: 464.074 420.254 196.853
2024-12-02-12:47:52-root-INFO: grad norm: 422.330 398.403 140.135
2024-12-02-12:47:52-root-INFO: grad norm: 415.067 387.078 149.839
2024-12-02-12:47:53-root-INFO: grad norm: 415.339 393.861 131.832
2024-12-02-12:47:53-root-INFO: grad norm: 419.788 389.771 155.886
2024-12-02-12:47:54-root-INFO: grad norm: 428.653 403.859 143.671
2024-12-02-12:47:54-root-INFO: grad norm: 441.736 405.792 174.538
2024-12-02-12:47:55-root-INFO: grad norm: 460.363 428.653 167.900
2024-12-02-12:47:55-root-INFO: Loss Change: 7782.943 -> 7504.065
2024-12-02-12:47:55-root-INFO: Regularization Change: 0.000 -> 0.529
2024-12-02-12:47:55-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-12:47:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:55-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-12:47:55-root-INFO: grad norm: 532.412 481.758 226.654
2024-12-02-12:47:56-root-INFO: grad norm: 542.782 491.636 230.014
2024-12-02-12:47:56-root-INFO: grad norm: 575.134 514.830 256.376
2024-12-02-12:47:57-root-INFO: grad norm: 616.564 554.621 269.343
2024-12-02-12:47:57-root-INFO: grad norm: 667.234 591.391 308.962
2024-12-02-12:47:58-root-INFO: grad norm: 727.499 648.292 330.110
2024-12-02-12:47:58-root-INFO: grad norm: 795.229 698.952 379.284
2024-12-02-12:47:59-root-INFO: grad norm: 873.740 772.421 408.395
2024-12-02-12:47:59-root-INFO: Loss Change: 7366.900 -> 7163.654
2024-12-02-12:47:59-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-02-12:47:59-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-12:47:59-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-12:47:59-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-12:47:59-root-INFO: grad norm: 1301.998 1144.177 621.336
2024-12-02-12:48:00-root-INFO: grad norm: 1377.419 1215.217 648.483
2024-12-02-12:48:00-root-INFO: grad norm: 1466.610 1282.796 710.901
2024-12-02-12:48:01-root-INFO: grad norm: 1568.378 1380.823 743.732
2024-12-02-12:48:01-root-INFO: Loss too large (7097.979->7099.458)! Learning rate decreased to 0.00021.
2024-12-02-12:48:01-root-INFO: grad norm: 1070.673 939.962 512.652
2024-12-02-12:48:02-root-INFO: grad norm: 739.677 661.984 329.999
2024-12-02-12:48:02-root-INFO: grad norm: 544.273 487.505 242.017
2024-12-02-12:48:03-root-INFO: grad norm: 427.270 395.208 162.388
2024-12-02-12:48:03-root-INFO: Loss Change: 7137.338 -> 6853.322
2024-12-02-12:48:03-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-02-12:48:03-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-12:48:03-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-12:48:03-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-12:48:03-root-INFO: grad norm: 438.058 396.736 185.729
2024-12-02-12:48:04-root-INFO: grad norm: 426.074 392.524 165.721
2024-12-02-12:48:04-root-INFO: grad norm: 430.867 393.216 176.145
2024-12-02-12:48:05-root-INFO: grad norm: 439.409 404.023 172.761
2024-12-02-12:48:05-root-INFO: grad norm: 449.379 408.394 187.500
2024-12-02-12:48:06-root-INFO: grad norm: 459.876 420.744 185.634
2024-12-02-12:48:06-root-INFO: grad norm: 470.899 425.947 200.787
2024-12-02-12:48:07-root-INFO: grad norm: 483.485 440.271 199.797
2024-12-02-12:48:07-root-INFO: Loss Change: 6805.658 -> 6616.039
2024-12-02-12:48:07-root-INFO: Regularization Change: 0.000 -> 0.425
2024-12-02-12:48:07-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-12:48:07-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:48:07-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-12:48:08-root-INFO: grad norm: 739.013 652.903 346.205
2024-12-02-12:48:08-root-INFO: grad norm: 681.876 607.286 310.094
2024-12-02-12:48:08-root-INFO: grad norm: 673.960 600.407 306.158
2024-12-02-12:48:09-root-INFO: grad norm: 675.120 604.272 301.070
2024-12-02-12:48:09-root-INFO: grad norm: 680.826 607.049 308.247
2024-12-02-12:48:10-root-INFO: grad norm: 688.139 615.621 307.484
2024-12-02-12:48:11-root-INFO: grad norm: 695.099 619.174 315.889
2024-12-02-12:48:11-root-INFO: grad norm: 701.960 627.510 314.609
2024-12-02-12:48:11-root-INFO: Loss Change: 6629.298 -> 6427.445
2024-12-02-12:48:11-root-INFO: Regularization Change: 0.000 -> 0.451
2024-12-02-12:48:11-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-12:48:11-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:48:11-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-12:48:12-root-INFO: grad norm: 1195.754 1052.739 567.069
2024-12-02-12:48:12-root-INFO: grad norm: 1144.800 1023.982 511.887
2024-12-02-12:48:13-root-INFO: grad norm: 1130.450 1003.352 520.770
2024-12-02-12:48:13-root-INFO: grad norm: 1120.442 1003.500 498.374
2024-12-02-12:48:14-root-INFO: grad norm: 1113.557 990.683 508.485
2024-12-02-12:48:14-root-INFO: grad norm: 1107.979 991.687 494.139
2024-12-02-12:48:15-root-INFO: grad norm: 1105.221 984.057 503.136
2024-12-02-12:48:15-root-INFO: grad norm: 1103.276 986.825 493.350
2024-12-02-12:48:15-root-INFO: Loss Change: 6379.719 -> 6182.523
2024-12-02-12:48:15-root-INFO: Regularization Change: 0.000 -> 0.428
2024-12-02-12:48:15-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-12:48:16-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:48:16-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-12:48:16-root-INFO: grad norm: 1219.910 1091.076 545.650
2024-12-02-12:48:16-root-INFO: grad norm: 1193.268 1068.551 531.118
2024-12-02-12:48:17-root-INFO: grad norm: 1169.940 1044.188 527.666
2024-12-02-12:48:17-root-INFO: grad norm: 1145.526 1026.299 508.861
2024-12-02-12:48:18-root-INFO: grad norm: 1122.503 1001.279 507.398
2024-12-02-12:48:18-root-INFO: grad norm: 1098.874 984.906 487.323
2024-12-02-12:48:19-root-INFO: grad norm: 1076.597 960.139 487.026
2024-12-02-12:48:19-root-INFO: grad norm: 1054.353 945.351 466.873
2024-12-02-12:48:20-root-INFO: Loss Change: 6161.256 -> 5978.069
2024-12-02-12:48:20-root-INFO: Regularization Change: 0.000 -> 0.381
2024-12-02-12:48:20-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-12:48:20-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:48:20-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-12:48:20-root-INFO: grad norm: 1270.078 1134.854 570.268
2024-12-02-12:48:21-root-INFO: grad norm: 1207.948 1086.389 528.108
2024-12-02-12:48:21-root-INFO: grad norm: 1156.967 1034.023 519.008
2024-12-02-12:48:22-root-INFO: grad norm: 1106.708 995.585 483.335
2024-12-02-12:48:22-root-INFO: grad norm: 1062.054 949.712 475.400
2024-12-02-12:48:22-root-INFO: grad norm: 1018.335 916.367 444.159
2024-12-02-12:48:23-root-INFO: grad norm: 978.467 875.363 437.192
2024-12-02-12:48:23-root-INFO: grad norm: 939.294 845.626 408.890
2024-12-02-12:48:24-root-INFO: Loss Change: 5955.167 -> 5745.071
2024-12-02-12:48:24-root-INFO: Regularization Change: 0.000 -> 0.379
2024-12-02-12:48:24-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-12:48:24-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:48:24-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-12:48:24-root-INFO: grad norm: 1230.245 1106.670 537.387
2024-12-02-12:48:24-root-INFO: grad norm: 1162.559 1052.005 494.801
2024-12-02-12:48:25-root-INFO: grad norm: 1112.343 1000.011 487.120
2024-12-02-12:48:25-root-INFO: grad norm: 1063.286 961.819 453.302
2024-12-02-12:48:26-root-INFO: grad norm: 1019.249 916.630 445.711
2024-12-02-12:48:26-root-INFO: grad norm: 975.784 882.734 415.855
2024-12-02-12:48:27-root-INFO: grad norm: 938.020 843.875 409.580
2024-12-02-12:48:27-root-INFO: grad norm: 901.237 815.547 383.549
2024-12-02-12:48:28-root-INFO: Loss Change: 5774.446 -> 5553.879
2024-12-02-12:48:28-root-INFO: Regularization Change: 0.000 -> 0.424
2024-12-02-12:48:28-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-12:48:28-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:48:28-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-12:48:28-root-INFO: grad norm: 1025.522 921.402 450.237
2024-12-02-12:48:28-root-INFO: grad norm: 944.951 854.514 403.407
2024-12-02-12:48:29-root-INFO: grad norm: 884.841 797.974 382.335
2024-12-02-12:48:29-root-INFO: grad norm: 828.407 750.767 350.152
2024-12-02-12:48:30-root-INFO: grad norm: 778.429 701.892 336.598
2024-12-02-12:48:30-root-INFO: grad norm: 730.421 662.975 306.560
2024-12-02-12:48:31-root-INFO: grad norm: 688.337 620.809 297.327
2024-12-02-12:48:31-root-INFO: grad norm: 648.533 589.560 270.211
2024-12-02-12:48:31-root-INFO: Loss Change: 5564.506 -> 5369.976
2024-12-02-12:48:31-root-INFO: Regularization Change: 0.000 -> 0.385
2024-12-02-12:48:31-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-12:48:31-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:48:32-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-12:48:32-root-INFO: grad norm: 1034.963 929.420 455.332
2024-12-02-12:48:32-root-INFO: grad norm: 937.607 853.208 388.772
2024-12-02-12:48:33-root-INFO: grad norm: 865.770 781.279 373.042
2024-12-02-12:48:33-root-INFO: grad norm: 799.582 727.653 331.439
2024-12-02-12:48:34-root-INFO: grad norm: 744.782 673.023 318.966
2024-12-02-12:48:34-root-INFO: grad norm: 693.059 631.047 286.549
2024-12-02-12:48:35-root-INFO: grad norm: 648.411 586.651 276.183
2024-12-02-12:48:35-root-INFO: grad norm: 606.352 552.685 249.403
2024-12-02-12:48:36-root-INFO: Loss Change: 5450.079 -> 5221.547
2024-12-02-12:48:36-root-INFO: Regularization Change: 0.000 -> 0.472
2024-12-02-12:48:36-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-12:48:36-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-12:48:36-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-12:48:36-root-INFO: grad norm: 576.855 512.486 264.803
2024-12-02-12:48:36-root-INFO: grad norm: 498.342 452.065 209.720
2024-12-02-12:48:37-root-INFO: grad norm: 451.409 409.843 189.207
2024-12-02-12:48:37-root-INFO: grad norm: 412.216 377.901 164.660
2024-12-02-12:48:38-root-INFO: grad norm: 378.911 344.291 158.231
2024-12-02-12:48:38-root-INFO: grad norm: 349.710 322.468 135.320
2024-12-02-12:48:39-root-INFO: grad norm: 324.839 295.818 134.209
2024-12-02-12:48:39-root-INFO: grad norm: 303.158 280.970 113.844
2024-12-02-12:48:40-root-INFO: Loss Change: 5168.435 -> 5016.261
2024-12-02-12:48:40-root-INFO: Regularization Change: 0.000 -> 0.409
2024-12-02-12:48:40-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-12:48:40-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:48:40-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-12:48:40-root-INFO: grad norm: 690.544 615.786 312.505
2024-12-02-12:48:41-root-INFO: grad norm: 606.274 556.607 240.325
2024-12-02-12:48:41-root-INFO: grad norm: 552.117 496.927 240.617
2024-12-02-12:48:41-root-INFO: grad norm: 504.549 463.504 199.331
2024-12-02-12:48:42-root-INFO: grad norm: 464.106 419.327 198.896
2024-12-02-12:48:42-root-INFO: grad norm: 427.658 393.463 167.565
2024-12-02-12:48:43-root-INFO: grad norm: 396.184 358.989 167.598
2024-12-02-12:48:43-root-INFO: grad norm: 367.737 339.161 142.129
2024-12-02-12:48:44-root-INFO: Loss Change: 5019.167 -> 4845.397
2024-12-02-12:48:44-root-INFO: Regularization Change: 0.000 -> 0.453
2024-12-02-12:48:44-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-12:48:44-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:48:44-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-12:48:44-root-INFO: grad norm: 574.744 512.612 259.922
2024-12-02-12:48:45-root-INFO: grad norm: 516.584 474.783 203.567
2024-12-02-12:48:45-root-INFO: grad norm: 476.964 431.311 203.631
2024-12-02-12:48:46-root-INFO: grad norm: 441.485 406.563 172.093
2024-12-02-12:48:46-root-INFO: grad norm: 411.045 372.918 172.886
2024-12-02-12:48:47-root-INFO: grad norm: 383.482 353.858 147.793
2024-12-02-12:48:47-root-INFO: grad norm: 359.381 326.934 149.228
2024-12-02-12:48:48-root-INFO: grad norm: 337.447 312.122 128.260
2024-12-02-12:48:48-root-INFO: Loss Change: 4843.464 -> 4690.142
2024-12-02-12:48:48-root-INFO: Regularization Change: 0.000 -> 0.454
2024-12-02-12:48:48-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-12:48:48-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:48:48-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-12:48:48-root-INFO: grad norm: 579.001 520.247 254.137
2024-12-02-12:48:49-root-INFO: grad norm: 520.491 478.883 203.915
2024-12-02-12:48:49-root-INFO: grad norm: 478.044 433.185 202.182
2024-12-02-12:48:50-root-INFO: grad norm: 439.599 405.030 170.873
2024-12-02-12:48:50-root-INFO: grad norm: 406.709 369.748 169.407
2024-12-02-12:48:51-root-INFO: grad norm: 376.760 347.771 144.927
2024-12-02-12:48:51-root-INFO: grad norm: 350.704 319.708 144.153
2024-12-02-12:48:52-root-INFO: grad norm: 327.293 302.751 124.349
2024-12-02-12:48:52-root-INFO: Loss Change: 4697.942 -> 4548.330
2024-12-02-12:48:52-root-INFO: Regularization Change: 0.000 -> 0.453
2024-12-02-12:48:52-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-12:48:52-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:48:52-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-12:48:52-root-INFO: grad norm: 372.421 339.991 151.999
2024-12-02-12:48:53-root-INFO: grad norm: 336.898 311.352 128.685
2024-12-02-12:48:53-root-INFO: grad norm: 308.625 282.820 123.539
2024-12-02-12:48:54-root-INFO: grad norm: 284.183 263.541 106.329
2024-12-02-12:48:54-root-INFO: grad norm: 263.265 242.072 103.487
2024-12-02-12:48:55-root-INFO: grad norm: 245.121 228.047 89.881
2024-12-02-12:48:55-root-INFO: grad norm: 229.788 212.153 88.281
2024-12-02-12:48:56-root-INFO: grad norm: 216.521 202.132 77.615
2024-12-02-12:48:56-root-INFO: Loss Change: 4491.940 -> 4377.279
2024-12-02-12:48:56-root-INFO: Regularization Change: 0.000 -> 0.407
2024-12-02-12:48:56-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-12:48:56-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:48:56-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-12:48:56-root-INFO: grad norm: 321.937 291.825 135.948
2024-12-02-12:48:57-root-INFO: grad norm: 278.043 257.897 103.908
2024-12-02-12:48:57-root-INFO: grad norm: 249.928 229.505 98.952
2024-12-02-12:48:58-root-INFO: grad norm: 226.987 211.773 81.703
2024-12-02-12:48:58-root-INFO: grad norm: 208.691 192.812 79.848
2024-12-02-12:48:58-root-INFO: grad norm: 194.017 181.782 67.807
2024-12-02-12:48:59-root-INFO: grad norm: 182.498 169.721 67.084
2024-12-02-12:48:59-root-INFO: grad norm: 173.399 163.079 58.929
2024-12-02-12:49:00-root-INFO: Loss Change: 4358.521 -> 4251.971
2024-12-02-12:49:00-root-INFO: Regularization Change: 0.000 -> 0.401
2024-12-02-12:49:00-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-12:49:00-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:49:00-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-12:49:00-root-INFO: grad norm: 442.592 393.023 203.520
2024-12-02-12:49:01-root-INFO: grad norm: 372.789 345.045 141.122
2024-12-02-12:49:01-root-INFO: grad norm: 325.172 295.572 135.552
2024-12-02-12:49:02-root-INFO: grad norm: 286.304 265.470 107.218
2024-12-02-12:49:02-root-INFO: grad norm: 255.262 234.236 101.451
2024-12-02-12:49:02-root-INFO: grad norm: 229.663 213.772 83.944
2024-12-02-12:49:03-root-INFO: grad norm: 209.161 193.562 79.262
2024-12-02-12:49:03-root-INFO: grad norm: 192.695 180.288 68.025
2024-12-02-12:49:04-root-INFO: Loss Change: 4250.493 -> 4120.449
2024-12-02-12:49:04-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-02-12:49:04-root-INFO: Undo step: 220
2024-12-02-12:49:04-root-INFO: Undo step: 221
2024-12-02-12:49:04-root-INFO: Undo step: 222
2024-12-02-12:49:04-root-INFO: Undo step: 223
2024-12-02-12:49:04-root-INFO: Undo step: 224
2024-12-02-12:49:04-root-INFO: Undo step: 225
2024-12-02-12:49:04-root-INFO: Undo step: 226
2024-12-02-12:49:04-root-INFO: Undo step: 227
2024-12-02-12:49:04-root-INFO: Undo step: 228
2024-12-02-12:49:04-root-INFO: Undo step: 229
2024-12-02-12:49:04-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-12:49:04-root-INFO: grad norm: 6034.430 4494.625 4026.498
2024-12-02-12:49:04-root-INFO: grad norm: 3442.955 2614.485 2240.181
2024-12-02-12:49:05-root-INFO: grad norm: 2460.974 1966.102 1480.148
2024-12-02-12:49:05-root-INFO: grad norm: 1939.187 1591.341 1108.188
2024-12-02-12:49:06-root-INFO: grad norm: 1667.264 1410.342 889.216
2024-12-02-12:49:06-root-INFO: grad norm: 1495.950 1296.575 746.162
2024-12-02-12:49:07-root-INFO: grad norm: 1383.398 1200.166 688.035
2024-12-02-12:49:07-root-INFO: grad norm: 1295.801 1149.159 598.778
2024-12-02-12:49:08-root-INFO: Loss Change: 14040.818 -> 6732.323
2024-12-02-12:49:08-root-INFO: Regularization Change: 0.000 -> 10.060
2024-12-02-12:49:08-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-12:49:08-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:49:08-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-12:49:08-root-INFO: grad norm: 1271.328 1139.079 564.601
2024-12-02-12:49:09-root-INFO: grad norm: 1148.821 1040.338 487.326
2024-12-02-12:49:09-root-INFO: grad norm: 1069.621 953.024 485.629
2024-12-02-12:49:09-root-INFO: grad norm: 997.325 904.550 420.055
2024-12-02-12:49:10-root-INFO: grad norm: 933.575 837.931 411.623
2024-12-02-12:49:10-root-INFO: grad norm: 876.009 795.057 367.799
2024-12-02-12:49:11-root-INFO: grad norm: 824.963 744.638 355.077
2024-12-02-12:49:11-root-INFO: grad norm: 777.708 705.730 326.766
2024-12-02-12:49:12-root-INFO: Loss Change: 6556.101 -> 5863.565
2024-12-02-12:49:12-root-INFO: Regularization Change: 0.000 -> 1.690
2024-12-02-12:49:12-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-12:49:12-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:49:12-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-12:49:12-root-INFO: grad norm: 838.693 759.898 354.910
2024-12-02-12:49:12-root-INFO: grad norm: 755.336 685.184 317.893
2024-12-02-12:49:13-root-INFO: grad norm: 695.112 633.133 286.920
2024-12-02-12:49:13-root-INFO: grad norm: 642.488 583.610 268.682
2024-12-02-12:49:14-root-INFO: grad norm: 597.902 546.116 243.402
2024-12-02-12:49:14-root-INFO: grad norm: 558.304 506.910 233.977
2024-12-02-12:49:15-root-INFO: grad norm: 522.573 478.531 209.977
2024-12-02-12:49:15-root-INFO: grad norm: 490.108 444.920 205.554
2024-12-02-12:49:16-root-INFO: Loss Change: 5817.801 -> 5478.270
2024-12-02-12:49:16-root-INFO: Regularization Change: 0.000 -> 0.876
2024-12-02-12:49:16-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-12:49:16-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-12:49:16-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-12:49:16-root-INFO: grad norm: 839.628 762.401 351.739
2024-12-02-12:49:17-root-INFO: grad norm: 734.935 668.594 305.143
2024-12-02-12:49:17-root-INFO: grad norm: 665.616 609.995 266.366
2024-12-02-12:49:18-root-INFO: grad norm: 606.612 551.176 253.343
2024-12-02-12:49:18-root-INFO: grad norm: 557.291 512.657 218.530
2024-12-02-12:49:19-root-INFO: grad norm: 513.654 466.434 215.127
2024-12-02-12:49:19-root-INFO: grad norm: 475.779 438.938 183.573
2024-12-02-12:49:20-root-INFO: grad norm: 441.655 401.247 184.553
2024-12-02-12:49:20-root-INFO: Loss Change: 5487.233 -> 5209.560
2024-12-02-12:49:20-root-INFO: Regularization Change: 0.000 -> 0.696
2024-12-02-12:49:20-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-12:49:20-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-12:49:20-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-12:49:20-root-INFO: grad norm: 441.949 390.939 206.120
2024-12-02-12:49:21-root-INFO: grad norm: 367.922 333.736 154.878
2024-12-02-12:49:21-root-INFO: grad norm: 334.695 309.738 126.820
2024-12-02-12:49:22-root-INFO: grad norm: 310.342 284.684 123.560
2024-12-02-12:49:22-root-INFO: grad norm: 290.866 270.513 106.891
2024-12-02-12:49:23-root-INFO: grad norm: 274.641 253.241 106.286
2024-12-02-12:49:23-root-INFO: grad norm: 260.666 243.477 93.090
2024-12-02-12:49:24-root-INFO: grad norm: 248.655 230.424 93.456
2024-12-02-12:49:24-root-INFO: Loss Change: 5144.057 -> 4959.857
2024-12-02-12:49:24-root-INFO: Regularization Change: 0.000 -> 0.554
2024-12-02-12:49:24-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-12:49:24-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:49:24-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-12:49:25-root-INFO: grad norm: 625.120 564.974 267.545
2024-12-02-12:49:25-root-INFO: grad norm: 523.337 480.536 207.286
2024-12-02-12:49:26-root-INFO: grad norm: 463.473 426.320 181.818
2024-12-02-12:49:26-root-INFO: grad norm: 413.534 380.052 163.006
2024-12-02-12:49:27-root-INFO: grad norm: 372.125 345.128 139.153
2024-12-02-12:49:27-root-INFO: grad norm: 336.596 310.069 130.973
2024-12-02-12:49:27-root-INFO: grad norm: 307.356 287.050 109.864
2024-12-02-12:49:28-root-INFO: grad norm: 282.254 261.113 107.180
2024-12-02-12:49:28-root-INFO: Loss Change: 4928.693 -> 4748.547
2024-12-02-12:49:28-root-INFO: Regularization Change: 0.000 -> 0.490
2024-12-02-12:49:28-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-12:49:28-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:49:28-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-12:49:29-root-INFO: grad norm: 497.013 449.891 211.235
2024-12-02-12:49:29-root-INFO: grad norm: 425.567 391.309 167.286
2024-12-02-12:49:30-root-INFO: grad norm: 380.582 352.064 144.545
2024-12-02-12:49:30-root-INFO: grad norm: 342.865 316.201 132.566
2024-12-02-12:49:31-root-INFO: grad norm: 311.659 290.431 113.053
2024-12-02-12:49:31-root-INFO: grad norm: 285.106 263.886 107.935
2024-12-02-12:49:32-root-INFO: grad norm: 262.870 246.688 90.805
2024-12-02-12:49:32-root-INFO: grad norm: 243.683 226.670 89.455
2024-12-02-12:49:32-root-INFO: Loss Change: 4734.928 -> 4589.841
2024-12-02-12:49:32-root-INFO: Regularization Change: 0.000 -> 0.439
2024-12-02-12:49:32-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-12:49:32-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:49:33-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-12:49:33-root-INFO: grad norm: 491.486 446.417 205.598
2024-12-02-12:49:33-root-INFO: grad norm: 419.439 386.775 162.278
2024-12-02-12:49:34-root-INFO: grad norm: 369.304 340.989 141.818
2024-12-02-12:49:34-root-INFO: grad norm: 327.078 302.322 124.826
2024-12-02-12:49:35-root-INFO: grad norm: 292.472 272.461 106.327
2024-12-02-12:49:35-root-INFO: grad norm: 263.433 244.590 97.839
2024-12-02-12:49:36-root-INFO: grad norm: 239.828 225.360 82.038
2024-12-02-12:49:36-root-INFO: grad norm: 220.565 205.994 78.837
2024-12-02-12:49:37-root-INFO: Loss Change: 4584.632 -> 4451.953
2024-12-02-12:49:37-root-INFO: Regularization Change: 0.000 -> 0.409
2024-12-02-12:49:37-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-12:49:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:49:37-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-12:49:37-root-INFO: grad norm: 275.338 256.190 100.883
2024-12-02-12:49:37-root-INFO: grad norm: 240.478 223.805 87.983
2024-12-02-12:49:38-root-INFO: grad norm: 217.073 205.337 70.408
2024-12-02-12:49:38-root-INFO: grad norm: 198.884 186.721 68.486
2024-12-02-12:49:39-root-INFO: grad norm: 184.891 176.473 55.153
2024-12-02-12:49:39-root-INFO: grad norm: 174.236 164.850 56.415
2024-12-02-12:49:40-root-INFO: grad norm: 166.055 159.500 46.195
2024-12-02-12:49:40-root-INFO: grad norm: 159.855 152.193 48.898
2024-12-02-12:49:41-root-INFO: Loss Change: 4393.740 -> 4298.062
2024-12-02-12:49:41-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-02-12:49:41-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-12:49:41-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:49:41-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-12:49:41-root-INFO: grad norm: 252.422 232.489 98.315
2024-12-02-12:49:41-root-INFO: grad norm: 211.815 197.948 75.380
2024-12-02-12:49:42-root-INFO: grad norm: 189.815 180.047 60.104
2024-12-02-12:49:42-root-INFO: grad norm: 173.887 164.384 56.698
2024-12-02-12:49:43-root-INFO: grad norm: 162.278 155.501 46.406
2024-12-02-12:49:43-root-INFO: grad norm: 153.863 146.690 46.431
2024-12-02-12:49:44-root-INFO: grad norm: 147.954 142.613 39.397
2024-12-02-12:49:44-root-INFO: grad norm: 143.737 137.788 40.926
2024-12-02-12:49:45-root-INFO: Loss Change: 4267.070 -> 4178.390
2024-12-02-12:49:45-root-INFO: Regularization Change: 0.000 -> 0.344
2024-12-02-12:49:45-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-12:49:45-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:49:45-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-12:49:45-root-INFO: grad norm: 413.664 368.755 187.451
2024-12-02-12:49:45-root-INFO: grad norm: 326.377 302.373 122.851
2024-12-02-12:49:46-root-INFO: grad norm: 270.637 248.728 106.672
2024-12-02-12:49:46-root-INFO: grad norm: 228.888 213.015 83.752
2024-12-02-12:49:47-root-INFO: grad norm: 198.251 185.522 69.894
2024-12-02-12:49:47-root-INFO: grad norm: 176.011 165.476 59.979
2024-12-02-12:49:48-root-INFO: grad norm: 160.199 152.278 49.750
2024-12-02-12:49:48-root-INFO: grad norm: 149.376 141.946 46.526
2024-12-02-12:49:49-root-INFO: Loss Change: 4175.179 -> 4057.347
2024-12-02-12:49:49-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-02-12:49:49-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-12:49:49-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:49:49-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-12:49:49-root-INFO: grad norm: 163.067 153.579 54.811
2024-12-02-12:49:49-root-INFO: grad norm: 143.922 137.042 43.966
2024-12-02-12:49:50-root-INFO: grad norm: 136.459 131.248 37.351
2024-12-02-12:49:50-root-INFO: grad norm: 131.879 126.433 37.505
2024-12-02-12:49:51-root-INFO: grad norm: 128.992 124.334 34.351
2024-12-02-12:49:51-root-INFO: grad norm: 126.953 122.050 34.942
2024-12-02-12:49:52-root-INFO: grad norm: 125.491 121.041 33.122
2024-12-02-12:49:52-root-INFO: grad norm: 124.371 119.744 33.610
2024-12-02-12:49:53-root-INFO: Loss Change: 4033.190 -> 3957.568
2024-12-02-12:49:53-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-02-12:49:53-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-12:49:53-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-12:49:53-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-12:49:53-root-INFO: grad norm: 261.332 237.739 108.510
2024-12-02-12:49:53-root-INFO: grad norm: 206.779 193.002 74.214
2024-12-02-12:49:54-root-INFO: grad norm: 172.476 161.189 61.371
2024-12-02-12:49:54-root-INFO: grad norm: 150.397 142.106 49.247
2024-12-02-12:49:55-root-INFO: grad norm: 137.010 130.651 41.255
2024-12-02-12:49:55-root-INFO: grad norm: 128.726 123.071 37.735
2024-12-02-12:49:56-root-INFO: grad norm: 123.603 119.045 33.258
2024-12-02-12:49:56-root-INFO: grad norm: 120.369 115.823 32.771
2024-12-02-12:49:57-root-INFO: Loss Change: 3939.475 -> 3855.594
2024-12-02-12:49:57-root-INFO: Regularization Change: 0.000 -> 0.359
2024-12-02-12:49:57-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-12:49:57-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:49:57-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-12:49:57-root-INFO: grad norm: 286.658 250.850 138.734
2024-12-02-12:49:57-root-INFO: grad norm: 203.729 190.211 72.975
2024-12-02-12:49:58-root-INFO: grad norm: 166.234 154.744 60.727
2024-12-02-12:49:58-root-INFO: grad norm: 143.786 135.855 47.091
2024-12-02-12:49:59-root-INFO: grad norm: 130.445 124.281 39.623
2024-12-02-12:49:59-root-INFO: grad norm: 122.699 117.354 35.822
2024-12-02-12:50:00-root-INFO: grad norm: 118.084 113.703 31.864
2024-12-02-12:50:00-root-INFO: grad norm: 115.241 110.928 31.229
2024-12-02-12:50:01-root-INFO: Loss Change: 3856.899 -> 3768.810
2024-12-02-12:50:01-root-INFO: Regularization Change: 0.000 -> 0.377
2024-12-02-12:50:01-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-12:50:01-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:01-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-12:50:01-root-INFO: grad norm: 278.494 244.485 133.363
2024-12-02-12:50:01-root-INFO: grad norm: 201.236 187.204 73.828
2024-12-02-12:50:02-root-INFO: grad norm: 163.782 151.619 61.938
2024-12-02-12:50:02-root-INFO: grad norm: 140.476 132.528 46.582
2024-12-02-12:50:03-root-INFO: grad norm: 126.929 120.500 39.885
2024-12-02-12:50:03-root-INFO: grad norm: 118.813 113.597 34.816
2024-12-02-12:50:04-root-INFO: grad norm: 114.077 109.660 31.438
2024-12-02-12:50:04-root-INFO: grad norm: 111.170 107.008 30.131
2024-12-02-12:50:04-root-INFO: Loss Change: 3755.021 -> 3665.304
2024-12-02-12:50:05-root-INFO: Regularization Change: 0.000 -> 0.405
2024-12-02-12:50:05-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-12:50:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:05-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-12:50:05-root-INFO: grad norm: 129.472 122.242 42.661
2024-12-02-12:50:05-root-INFO: grad norm: 116.345 111.046 34.713
2024-12-02-12:50:06-root-INFO: grad norm: 110.761 106.387 30.822
2024-12-02-12:50:06-root-INFO: grad norm: 107.795 103.699 29.433
2024-12-02-12:50:07-root-INFO: grad norm: 106.001 102.211 28.091
2024-12-02-12:50:08-root-INFO: grad norm: 104.969 101.213 27.828
2024-12-02-12:50:08-root-INFO: grad norm: 104.169 100.575 27.128
2024-12-02-12:50:09-root-INFO: grad norm: 103.487 99.909 26.974
2024-12-02-12:50:09-root-INFO: Loss Change: 3651.794 -> 3589.066
2024-12-02-12:50:09-root-INFO: Regularization Change: 0.000 -> 0.339
2024-12-02-12:50:09-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-12:50:09-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:09-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-12:50:09-root-INFO: grad norm: 204.607 180.779 95.827
2024-12-02-12:50:10-root-INFO: grad norm: 145.977 137.313 49.543
2024-12-02-12:50:10-root-INFO: grad norm: 123.333 116.114 41.576
2024-12-02-12:50:11-root-INFO: grad norm: 111.778 106.942 32.522
2024-12-02-12:50:11-root-INFO: grad norm: 105.979 101.858 29.264
2024-12-02-12:50:12-root-INFO: grad norm: 103.000 99.309 27.329
2024-12-02-12:50:12-root-INFO: grad norm: 101.357 97.957 26.031
2024-12-02-12:50:13-root-INFO: grad norm: 100.314 96.990 25.608
2024-12-02-12:50:13-root-INFO: Loss Change: 3570.731 -> 3499.540
2024-12-02-12:50:13-root-INFO: Regularization Change: 0.000 -> 0.369
2024-12-02-12:50:13-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-12:50:13-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:13-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-12:50:13-root-INFO: grad norm: 252.281 221.368 121.003
2024-12-02-12:50:14-root-INFO: grad norm: 171.284 159.597 62.185
2024-12-02-12:50:14-root-INFO: grad norm: 136.175 125.808 52.114
2024-12-02-12:50:15-root-INFO: grad norm: 118.014 112.129 36.801
2024-12-02-12:50:15-root-INFO: grad norm: 108.690 103.198 34.112
2024-12-02-12:50:16-root-INFO: grad norm: 103.897 99.629 29.473
2024-12-02-12:50:16-root-INFO: grad norm: 101.182 97.071 28.549
2024-12-02-12:50:17-root-INFO: grad norm: 99.542 95.793 27.061
2024-12-02-12:50:17-root-INFO: Loss Change: 3514.328 -> 3428.663
2024-12-02-12:50:17-root-INFO: Regularization Change: 0.000 -> 0.440
2024-12-02-12:50:17-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-12:50:17-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:17-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-12:50:17-root-INFO: grad norm: 104.192 99.030 32.390
2024-12-02-12:50:18-root-INFO: grad norm: 99.903 96.186 26.998
2024-12-02-12:50:18-root-INFO: grad norm: 98.154 94.390 26.920
2024-12-02-12:50:19-root-INFO: grad norm: 97.093 93.632 25.692
2024-12-02-12:50:19-root-INFO: grad norm: 96.384 92.967 25.437
2024-12-02-12:50:20-root-INFO: grad norm: 95.774 92.466 24.954
2024-12-02-12:50:20-root-INFO: grad norm: 95.255 91.995 24.705
2024-12-02-12:50:20-root-INFO: grad norm: 94.809 91.611 24.416
2024-12-02-12:50:21-root-INFO: Loss Change: 3413.057 -> 3353.703
2024-12-02-12:50:21-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-12:50:21-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-12:50:21-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-12:50:21-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-12:50:21-root-INFO: grad norm: 270.676 234.621 134.975
2024-12-02-12:50:22-root-INFO: grad norm: 158.290 148.019 56.092
2024-12-02-12:50:22-root-INFO: grad norm: 119.992 111.966 43.147
2024-12-02-12:50:23-root-INFO: grad norm: 105.475 101.087 30.107
2024-12-02-12:50:23-root-INFO: grad norm: 99.493 95.307 28.555
2024-12-02-12:50:24-root-INFO: grad norm: 96.655 93.228 25.508
2024-12-02-12:50:24-root-INFO: grad norm: 95.095 91.631 25.430
2024-12-02-12:50:25-root-INFO: grad norm: 94.124 90.925 24.333
2024-12-02-12:50:25-root-INFO: Loss Change: 3352.392 -> 3263.635
2024-12-02-12:50:25-root-INFO: Regularization Change: 0.000 -> 0.473
2024-12-02-12:50:25-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-12:50:25-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:50:25-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-12:50:25-root-INFO: grad norm: 121.424 112.655 45.307
2024-12-02-12:50:26-root-INFO: grad norm: 102.390 97.953 29.816
2024-12-02-12:50:26-root-INFO: grad norm: 97.018 92.685 28.671
2024-12-02-12:50:27-root-INFO: grad norm: 94.730 91.212 25.575
2024-12-02-12:50:27-root-INFO: grad norm: 93.535 89.773 26.261
2024-12-02-12:50:28-root-INFO: grad norm: 92.759 89.428 24.635
2024-12-02-12:50:28-root-INFO: grad norm: 92.148 88.662 25.105
2024-12-02-12:50:29-root-INFO: grad norm: 91.653 88.421 24.125
2024-12-02-12:50:29-root-INFO: Loss Change: 3253.630 -> 3191.277
2024-12-02-12:50:29-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-02-12:50:29-root-INFO: Undo step: 210
2024-12-02-12:50:29-root-INFO: Undo step: 211
2024-12-02-12:50:29-root-INFO: Undo step: 212
2024-12-02-12:50:29-root-INFO: Undo step: 213
2024-12-02-12:50:29-root-INFO: Undo step: 214
2024-12-02-12:50:29-root-INFO: Undo step: 215
2024-12-02-12:50:29-root-INFO: Undo step: 216
2024-12-02-12:50:29-root-INFO: Undo step: 217
2024-12-02-12:50:29-root-INFO: Undo step: 218
2024-12-02-12:50:29-root-INFO: Undo step: 219
2024-12-02-12:50:29-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-12:50:29-root-INFO: grad norm: 13947.431 12812.335 5511.344
2024-12-02-12:50:30-root-INFO: grad norm: 3198.532 2778.193 1585.008
2024-12-02-12:50:30-root-INFO: grad norm: 2274.987 2155.439 727.771
2024-12-02-12:50:31-root-INFO: grad norm: 1942.484 1688.864 959.678
2024-12-02-12:50:31-root-INFO: grad norm: 1528.303 1465.169 434.732
2024-12-02-12:50:32-root-INFO: grad norm: 1213.216 1075.066 562.252
2024-12-02-12:50:32-root-INFO: grad norm: 1070.665 1019.992 325.486
2024-12-02-12:50:32-root-INFO: grad norm: 994.001 876.349 469.097
2024-12-02-12:50:33-root-INFO: Loss Change: 30262.824 -> 7600.056
2024-12-02-12:50:33-root-INFO: Regularization Change: 0.000 -> 47.744
2024-12-02-12:50:33-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-12:50:33-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-12:50:33-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-12:50:33-root-INFO: grad norm: 1007.787 917.904 416.037
2024-12-02-12:50:34-root-INFO: grad norm: 1248.458 1125.430 540.421
2024-12-02-12:50:34-root-INFO: grad norm: 1494.789 1362.956 613.796
2024-12-02-12:50:35-root-INFO: grad norm: 838.526 782.773 300.653
2024-12-02-12:50:35-root-INFO: grad norm: 594.284 556.865 207.545
2024-12-02-12:50:35-root-INFO: grad norm: 514.385 465.475 218.917
2024-12-02-12:50:36-root-INFO: grad norm: 455.560 426.375 160.435
2024-12-02-12:50:36-root-INFO: grad norm: 408.704 368.824 176.090
2024-12-02-12:50:37-root-INFO: Loss Change: 7331.894 -> 4373.406
2024-12-02-12:50:37-root-INFO: Regularization Change: 0.000 -> 8.004
2024-12-02-12:50:37-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-12:50:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-12:50:37-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-12:50:37-root-INFO: grad norm: 516.239 485.184 176.350
2024-12-02-12:50:38-root-INFO: grad norm: 443.593 405.322 180.247
2024-12-02-12:50:38-root-INFO: grad norm: 390.332 369.510 125.781
2024-12-02-12:50:39-root-INFO: grad norm: 346.432 315.930 142.140
2024-12-02-12:50:39-root-INFO: grad norm: 310.618 294.266 99.455
2024-12-02-12:50:39-root-INFO: grad norm: 281.133 257.569 112.668
2024-12-02-12:50:40-root-INFO: grad norm: 257.407 244.137 81.582
2024-12-02-12:50:40-root-INFO: grad norm: 238.211 219.901 91.587
2024-12-02-12:50:41-root-INFO: Loss Change: 4330.813 -> 3991.090
2024-12-02-12:50:41-root-INFO: Regularization Change: 0.000 -> 1.500
2024-12-02-12:50:41-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-12:50:41-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:41-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-12:50:41-root-INFO: grad norm: 379.870 345.954 156.899
2024-12-02-12:50:41-root-INFO: grad norm: 298.282 278.086 107.889
2024-12-02-12:50:42-root-INFO: grad norm: 259.197 247.061 78.385
2024-12-02-12:50:42-root-INFO: grad norm: 230.603 215.011 83.354
2024-12-02-12:50:43-root-INFO: grad norm: 209.430 200.823 59.424
2024-12-02-12:50:43-root-INFO: grad norm: 193.566 181.523 67.211
2024-12-02-12:50:44-root-INFO: grad norm: 181.603 174.588 49.988
2024-12-02-12:50:44-root-INFO: grad norm: 172.430 162.730 57.018
2024-12-02-12:50:45-root-INFO: Loss Change: 3977.452 -> 3789.714
2024-12-02-12:50:45-root-INFO: Regularization Change: 0.000 -> 0.849
2024-12-02-12:50:45-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-12:50:45-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:45-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-12:50:45-root-INFO: grad norm: 362.254 330.251 148.869
2024-12-02-12:50:45-root-INFO: grad norm: 282.580 263.411 102.305
2024-12-02-12:50:46-root-INFO: grad norm: 240.323 228.558 74.272
2024-12-02-12:50:46-root-INFO: grad norm: 209.318 195.470 74.870
2024-12-02-12:50:47-root-INFO: grad norm: 186.536 179.198 51.805
2024-12-02-12:50:47-root-INFO: grad norm: 169.824 159.419 58.530
2024-12-02-12:50:48-root-INFO: grad norm: 157.665 152.175 41.245
2024-12-02-12:50:48-root-INFO: grad norm: 148.777 140.504 48.921
2024-12-02-12:50:49-root-INFO: Loss Change: 3763.388 -> 3623.852
2024-12-02-12:50:49-root-INFO: Regularization Change: 0.000 -> 0.629
2024-12-02-12:50:49-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-12:50:49-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:49-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-12:50:49-root-INFO: grad norm: 172.951 164.663 52.897
2024-12-02-12:50:49-root-INFO: grad norm: 155.784 146.380 53.307
2024-12-02-12:50:50-root-INFO: grad norm: 144.851 139.025 40.669
2024-12-02-12:50:50-root-INFO: grad norm: 137.422 130.197 43.972
2024-12-02-12:50:51-root-INFO: grad norm: 132.159 127.130 36.112
2024-12-02-12:50:51-root-INFO: grad norm: 128.254 122.134 39.146
2024-12-02-12:50:52-root-INFO: grad norm: 125.300 120.564 34.123
2024-12-02-12:50:52-root-INFO: grad norm: 123.003 117.500 36.381
2024-12-02-12:50:53-root-INFO: Loss Change: 3611.474 -> 3519.301
2024-12-02-12:50:53-root-INFO: Regularization Change: 0.000 -> 0.496
2024-12-02-12:50:53-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-12:50:53-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:53-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-12:50:53-root-INFO: grad norm: 242.232 217.433 106.767
2024-12-02-12:50:54-root-INFO: grad norm: 179.773 168.448 62.797
2024-12-02-12:50:54-root-INFO: grad norm: 156.344 149.312 46.360
2024-12-02-12:50:54-root-INFO: grad norm: 140.869 133.209 45.820
2024-12-02-12:50:55-root-INFO: grad norm: 130.459 125.858 34.344
2024-12-02-12:50:55-root-INFO: grad norm: 123.404 117.404 38.013
2024-12-02-12:50:56-root-INFO: grad norm: 118.585 114.700 30.106
2024-12-02-12:50:56-root-INFO: grad norm: 115.203 110.119 33.846
2024-12-02-12:50:57-root-INFO: Loss Change: 3490.243 -> 3400.271
2024-12-02-12:50:57-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-12:50:57-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-12:50:57-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:50:57-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-12:50:57-root-INFO: grad norm: 297.773 261.436 142.548
2024-12-02-12:50:58-root-INFO: grad norm: 205.946 192.253 73.842
2024-12-02-12:50:58-root-INFO: grad norm: 173.008 162.673 58.900
2024-12-02-12:50:59-root-INFO: grad norm: 153.496 145.307 49.466
2024-12-02-12:50:59-root-INFO: grad norm: 140.280 134.216 40.800
2024-12-02-12:51:00-root-INFO: grad norm: 130.831 124.338 40.704
2024-12-02-12:51:00-root-INFO: grad norm: 123.950 119.362 33.410
2024-12-02-12:51:01-root-INFO: grad norm: 118.839 113.221 36.105
2024-12-02-12:51:01-root-INFO: Loss Change: 3411.532 -> 3311.034
2024-12-02-12:51:01-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-12:51:01-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-12:51:01-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-12:51:01-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-12:51:01-root-INFO: grad norm: 130.701 124.923 38.430
2024-12-02-12:51:02-root-INFO: grad norm: 121.909 116.210 36.839
2024-12-02-12:51:02-root-INFO: grad norm: 116.586 112.464 30.730
2024-12-02-12:51:03-root-INFO: grad norm: 112.762 107.966 32.536
2024-12-02-12:51:03-root-INFO: grad norm: 109.897 106.232 28.145
2024-12-02-12:51:04-root-INFO: grad norm: 107.669 103.288 30.403
2024-12-02-12:51:04-root-INFO: grad norm: 105.883 102.430 26.818
2024-12-02-12:51:05-root-INFO: grad norm: 104.427 100.296 29.080
2024-12-02-12:51:05-root-INFO: Loss Change: 3291.164 -> 3222.651
2024-12-02-12:51:05-root-INFO: Regularization Change: 0.000 -> 0.426
2024-12-02-12:51:05-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-12:51:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-12:51:05-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-12:51:05-root-INFO: grad norm: 321.164 278.378 160.161
2024-12-02-12:51:06-root-INFO: grad norm: 191.264 178.040 69.882
2024-12-02-12:51:06-root-INFO: grad norm: 155.807 146.974 51.715
2024-12-02-12:51:07-root-INFO: grad norm: 144.916 138.060 44.046
2024-12-02-12:51:07-root-INFO: grad norm: 141.950 136.929 37.418
2024-12-02-12:51:08-root-INFO: grad norm: 142.170 135.915 41.707
2024-12-02-12:51:08-root-INFO: grad norm: 144.011 139.438 36.004
2024-12-02-12:51:09-root-INFO: grad norm: 147.085 140.628 43.104
2024-12-02-12:51:09-root-INFO: Loss Change: 3227.382 -> 3125.487
2024-12-02-12:51:09-root-INFO: Regularization Change: 0.000 -> 0.530
2024-12-02-12:51:09-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-12:51:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:51:09-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-12:51:09-root-INFO: grad norm: 196.460 184.817 66.627
2024-12-02-12:51:10-root-INFO: grad norm: 191.393 184.257 51.771
2024-12-02-12:51:10-root-INFO: grad norm: 201.193 193.490 55.136
2024-12-02-12:51:11-root-INFO: grad norm: 216.734 208.832 57.989
2024-12-02-12:51:11-root-INFO: grad norm: 237.026 228.695 62.289
2024-12-02-12:51:12-root-INFO: grad norm: 261.623 251.738 71.234
2024-12-02-12:51:12-root-INFO: grad norm: 291.568 281.717 75.147
2024-12-02-12:51:13-root-INFO: grad norm: 325.757 313.092 89.950
2024-12-02-12:51:13-root-INFO: Loss Change: 3109.548 -> 3062.028
2024-12-02-12:51:13-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-12:51:13-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-12:51:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:51:13-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-12:51:13-root-INFO: grad norm: 437.986 410.913 151.601
2024-12-02-12:51:14-root-INFO: grad norm: 486.029 468.131 130.678
2024-12-02-12:51:14-root-INFO: Loss too large (3047.996->3057.990)! Learning rate decreased to 0.00072.
2024-12-02-12:51:14-root-INFO: grad norm: 379.391 365.774 100.732
2024-12-02-12:51:15-root-INFO: grad norm: 308.793 298.091 80.592
2024-12-02-12:51:15-root-INFO: grad norm: 256.023 246.971 67.477
2024-12-02-12:51:16-root-INFO: grad norm: 217.648 210.124 56.733
2024-12-02-12:51:16-root-INFO: grad norm: 188.924 182.434 49.094
2024-12-02-12:51:16-root-INFO: grad norm: 167.607 161.747 43.935
2024-12-02-12:51:17-root-INFO: Loss Change: 3052.439 -> 2954.885
2024-12-02-12:51:17-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-02-12:51:17-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-12:51:17-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:51:17-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-12:51:17-root-INFO: grad norm: 224.244 200.927 99.568
2024-12-02-12:51:18-root-INFO: grad norm: 190.338 183.096 52.004
2024-12-02-12:51:18-root-INFO: grad norm: 228.605 220.094 61.796
2024-12-02-12:51:19-root-INFO: grad norm: 314.918 304.181 81.531
2024-12-02-12:51:19-root-INFO: Loss too large (2923.723->2935.434)! Learning rate decreased to 0.00076.
2024-12-02-12:51:19-root-INFO: grad norm: 310.896 300.794 78.608
2024-12-02-12:51:20-root-INFO: grad norm: 310.074 299.266 81.152
2024-12-02-12:51:20-root-INFO: grad norm: 310.874 300.967 77.856
2024-12-02-12:51:21-root-INFO: grad norm: 313.199 302.190 82.309
2024-12-02-12:51:21-root-INFO: Loss Change: 2950.075 -> 2890.429
2024-12-02-12:51:21-root-INFO: Regularization Change: 0.000 -> 0.443
2024-12-02-12:51:21-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-12:51:21-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:51:21-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-12:51:22-root-INFO: grad norm: 317.140 300.454 101.517
2024-12-02-12:51:22-root-INFO: Loss too large (2869.938->2879.806)! Learning rate decreased to 0.00080.
2024-12-02-12:51:22-root-INFO: grad norm: 325.813 316.250 78.356
2024-12-02-12:51:23-root-INFO: grad norm: 361.282 348.877 93.859
2024-12-02-12:51:23-root-INFO: grad norm: 405.021 391.841 102.483
2024-12-02-12:51:23-root-INFO: Loss too large (2855.878->2856.971)! Learning rate decreased to 0.00064.
2024-12-02-12:51:24-root-INFO: grad norm: 295.969 285.803 76.907
2024-12-02-12:51:24-root-INFO: grad norm: 224.963 217.946 55.749
2024-12-02-12:51:25-root-INFO: grad norm: 178.547 172.142 47.394
2024-12-02-12:51:25-root-INFO: grad norm: 148.115 143.357 37.240
2024-12-02-12:51:26-root-INFO: Loss Change: 2869.938 -> 2799.815
2024-12-02-12:51:26-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-02-12:51:26-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-12:51:26-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:51:26-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-12:51:26-root-INFO: grad norm: 170.955 157.482 66.521
2024-12-02-12:51:26-root-INFO: grad norm: 190.987 185.221 46.574
2024-12-02-12:51:27-root-INFO: Loss too large (2767.791->2769.571)! Learning rate decreased to 0.00084.
2024-12-02-12:51:27-root-INFO: grad norm: 230.258 222.796 58.145
2024-12-02-12:51:28-root-INFO: grad norm: 297.361 287.826 74.695
2024-12-02-12:51:28-root-INFO: Loss too large (2761.790->2766.611)! Learning rate decreased to 0.00067.
2024-12-02-12:51:28-root-INFO: grad norm: 265.516 257.238 65.786
2024-12-02-12:51:29-root-INFO: grad norm: 239.967 232.234 60.427
2024-12-02-12:51:29-root-INFO: grad norm: 220.013 213.149 54.525
2024-12-02-12:51:29-root-INFO: grad norm: 203.935 197.279 51.677
2024-12-02-12:51:30-root-INFO: Loss Change: 2779.091 -> 2725.432
2024-12-02-12:51:30-root-INFO: Regularization Change: 0.000 -> 0.325
2024-12-02-12:51:30-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-12:51:30-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:51:30-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-12:51:30-root-INFO: grad norm: 211.766 196.200 79.688
2024-12-02-12:51:31-root-INFO: grad norm: 319.941 310.524 77.052
2024-12-02-12:51:31-root-INFO: Loss too large (2706.826->2754.969)! Learning rate decreased to 0.00088.
2024-12-02-12:51:31-root-INFO: Loss too large (2706.826->2720.218)! Learning rate decreased to 0.00070.
2024-12-02-12:51:31-root-INFO: grad norm: 325.603 315.707 79.663
2024-12-02-12:51:32-root-INFO: grad norm: 338.566 328.045 83.748
2024-12-02-12:51:32-root-INFO: grad norm: 353.179 342.936 84.441
2024-12-02-12:51:33-root-INFO: grad norm: 369.474 357.900 91.751
2024-12-02-12:51:33-root-INFO: grad norm: 385.810 374.933 90.961
2024-12-02-12:51:34-root-INFO: grad norm: 402.576 389.920 100.150
2024-12-02-12:51:34-root-INFO: Loss Change: 2710.933 -> 2681.225
2024-12-02-12:51:34-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-02-12:51:34-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-12:51:34-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-12:51:34-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-12:51:34-root-INFO: grad norm: 411.720 394.922 116.406
2024-12-02-12:51:35-root-INFO: Loss too large (2668.188->2756.121)! Learning rate decreased to 0.00092.
2024-12-02-12:51:35-root-INFO: Loss too large (2668.188->2694.708)! Learning rate decreased to 0.00073.
2024-12-02-12:51:35-root-INFO: grad norm: 443.803 430.534 107.712
2024-12-02-12:51:35-root-INFO: Loss too large (2660.439->2662.934)! Learning rate decreased to 0.00059.
2024-12-02-12:51:36-root-INFO: grad norm: 326.420 315.331 84.360
2024-12-02-12:51:36-root-INFO: grad norm: 248.011 240.908 58.928
2024-12-02-12:51:37-root-INFO: grad norm: 197.340 190.307 52.213
2024-12-02-12:51:37-root-INFO: grad norm: 162.663 157.662 40.025
2024-12-02-12:51:38-root-INFO: grad norm: 139.295 134.205 37.313
2024-12-02-12:51:39-root-INFO: grad norm: 123.070 118.880 31.838
2024-12-02-12:51:39-root-INFO: Loss Change: 2668.188 -> 2594.217
2024-12-02-12:51:39-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-02-12:51:39-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-12:51:39-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:51:39-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-12:51:39-root-INFO: grad norm: 115.299 108.937 37.772
2024-12-02-12:51:40-root-INFO: grad norm: 172.377 167.194 41.952
2024-12-02-12:51:40-root-INFO: Loss too large (2569.599->2583.290)! Learning rate decreased to 0.00096.
2024-12-02-12:51:40-root-INFO: Loss too large (2569.599->2572.700)! Learning rate decreased to 0.00077.
2024-12-02-12:51:41-root-INFO: grad norm: 219.384 213.283 51.378
2024-12-02-12:51:41-root-INFO: Loss too large (2567.023->2567.462)! Learning rate decreased to 0.00061.
2024-12-02-12:51:41-root-INFO: grad norm: 200.593 194.628 48.556
2024-12-02-12:51:42-root-INFO: grad norm: 185.965 180.706 43.911
2024-12-02-12:51:42-root-INFO: grad norm: 174.206 168.904 42.650
2024-12-02-12:51:43-root-INFO: grad norm: 165.025 160.301 39.206
2024-12-02-12:51:43-root-INFO: grad norm: 157.728 152.811 39.076
2024-12-02-12:51:43-root-INFO: Loss Change: 2576.825 -> 2538.016
2024-12-02-12:51:43-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-02-12:51:43-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-12:51:43-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:51:44-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-12:51:44-root-INFO: grad norm: 196.111 171.860 94.465
2024-12-02-12:51:44-root-INFO: grad norm: 130.713 124.764 38.985
2024-12-02-12:51:45-root-INFO: grad norm: 232.302 226.023 53.643
2024-12-02-12:51:45-root-INFO: Loss too large (2503.190->2555.332)! Learning rate decreased to 0.00100.
2024-12-02-12:51:45-root-INFO: Loss too large (2503.190->2525.917)! Learning rate decreased to 0.00080.
2024-12-02-12:51:45-root-INFO: Loss too large (2503.190->2509.042)! Learning rate decreased to 0.00064.
2024-12-02-12:51:46-root-INFO: grad norm: 255.741 248.078 62.139
2024-12-02-12:51:46-root-INFO: grad norm: 283.492 276.184 63.956
2024-12-02-12:51:47-root-INFO: grad norm: 317.067 308.214 74.405
2024-12-02-12:51:47-root-INFO: Loss too large (2496.371->2496.449)! Learning rate decreased to 0.00051.
2024-12-02-12:51:47-root-INFO: grad norm: 232.696 226.485 53.402
2024-12-02-12:51:48-root-INFO: grad norm: 175.397 170.009 43.141
2024-12-02-12:51:48-root-INFO: Loss Change: 2536.882 -> 2474.299
2024-12-02-12:51:48-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-02-12:51:48-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-12:51:48-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:51:48-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-12:51:49-root-INFO: grad norm: 152.068 145.711 43.509
2024-12-02-12:51:49-root-INFO: Loss too large (2466.097->2479.298)! Learning rate decreased to 0.00105.
2024-12-02-12:51:49-root-INFO: Loss too large (2466.097->2469.637)! Learning rate decreased to 0.00084.
2024-12-02-12:51:49-root-INFO: grad norm: 231.157 225.349 51.489
2024-12-02-12:51:50-root-INFO: Loss too large (2464.437->2473.875)! Learning rate decreased to 0.00067.
2024-12-02-12:51:50-root-INFO: grad norm: 273.108 265.713 63.125
2024-12-02-12:51:50-root-INFO: Loss too large (2463.378->2463.937)! Learning rate decreased to 0.00054.
2024-12-02-12:51:51-root-INFO: grad norm: 217.198 211.687 48.616
2024-12-02-12:51:51-root-INFO: grad norm: 178.294 173.059 42.887
2024-12-02-12:51:52-root-INFO: grad norm: 149.693 145.518 35.105
2024-12-02-12:51:52-root-INFO: grad norm: 129.366 125.241 32.407
2024-12-02-12:51:53-root-INFO: grad norm: 114.455 110.840 28.538
2024-12-02-12:51:53-root-INFO: Loss Change: 2466.097 -> 2434.172
2024-12-02-12:51:53-root-INFO: Regularization Change: 0.000 -> 0.142
2024-12-02-12:51:53-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-12:51:53-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:51:53-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-12:51:53-root-INFO: grad norm: 157.356 142.595 66.541
2024-12-02-12:51:54-root-INFO: grad norm: 303.466 295.407 69.474
2024-12-02-12:51:54-root-INFO: Loss too large (2420.098->2562.522)! Learning rate decreased to 0.00110.
2024-12-02-12:51:54-root-INFO: Loss too large (2420.098->2493.064)! Learning rate decreased to 0.00088.
2024-12-02-12:51:54-root-INFO: Loss too large (2420.098->2451.053)! Learning rate decreased to 0.00070.
2024-12-02-12:51:55-root-INFO: Loss too large (2420.098->2426.904)! Learning rate decreased to 0.00056.
2024-12-02-12:51:55-root-INFO: grad norm: 281.253 274.464 61.423
2024-12-02-12:51:56-root-INFO: grad norm: 265.210 258.684 58.472
2024-12-02-12:51:56-root-INFO: grad norm: 252.289 245.932 56.276
2024-12-02-12:51:56-root-INFO: grad norm: 242.526 236.668 52.979
2024-12-02-12:51:57-root-INFO: grad norm: 234.783 228.787 52.721
2024-12-02-12:51:57-root-INFO: grad norm: 229.114 223.618 49.883
2024-12-02-12:51:58-root-INFO: Loss Change: 2425.209 -> 2390.157
2024-12-02-12:51:58-root-INFO: Regularization Change: 0.000 -> 0.197
2024-12-02-12:51:58-root-INFO: Undo step: 200
2024-12-02-12:51:58-root-INFO: Undo step: 201
2024-12-02-12:51:58-root-INFO: Undo step: 202
2024-12-02-12:51:58-root-INFO: Undo step: 203
2024-12-02-12:51:58-root-INFO: Undo step: 204
2024-12-02-12:51:58-root-INFO: Undo step: 205
2024-12-02-12:51:58-root-INFO: Undo step: 206
2024-12-02-12:51:58-root-INFO: Undo step: 207
2024-12-02-12:51:58-root-INFO: Undo step: 208
2024-12-02-12:51:58-root-INFO: Undo step: 209
2024-12-02-12:51:58-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-12:51:58-root-INFO: grad norm: 4347.864 3951.832 1812.995
2024-12-02-12:51:59-root-INFO: grad norm: 1788.929 1679.213 616.856
2024-12-02-12:51:59-root-INFO: grad norm: 1366.008 1292.510 442.037
2024-12-02-12:51:59-root-INFO: grad norm: 1285.855 1224.759 391.649
2024-12-02-12:52:00-root-INFO: grad norm: 1173.286 1118.123 355.532
2024-12-02-12:52:00-root-INFO: grad norm: 1064.543 1008.612 340.520
2024-12-02-12:52:01-root-INFO: grad norm: 950.475 907.078 283.921
2024-12-02-12:52:01-root-INFO: grad norm: 848.664 802.355 276.508
2024-12-02-12:52:02-root-INFO: Loss Change: 9191.592 -> 3511.439
2024-12-02-12:52:02-root-INFO: Regularization Change: 0.000 -> 17.523
2024-12-02-12:52:02-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-12:52:02-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:52:02-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-12:52:02-root-INFO: grad norm: 656.938 637.221 159.741
2024-12-02-12:52:02-root-INFO: grad norm: 581.985 551.209 186.747
2024-12-02-12:52:03-root-INFO: grad norm: 525.153 504.538 145.695
2024-12-02-12:52:03-root-INFO: grad norm: 476.426 451.296 152.688
2024-12-02-12:52:04-root-INFO: grad norm: 434.512 416.581 123.532
2024-12-02-12:52:04-root-INFO: grad norm: 398.806 378.555 125.468
2024-12-02-12:52:05-root-INFO: grad norm: 368.279 352.695 105.998
2024-12-02-12:52:05-root-INFO: grad norm: 342.179 325.492 105.553
2024-12-02-12:52:06-root-INFO: Loss Change: 3442.719 -> 3163.451
2024-12-02-12:52:06-root-INFO: Regularization Change: 0.000 -> 1.491
2024-12-02-12:52:06-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-12:52:06-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:52:06-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-12:52:06-root-INFO: grad norm: 308.836 297.285 83.673
2024-12-02-12:52:06-root-INFO: grad norm: 272.851 260.941 79.734
2024-12-02-12:52:07-root-INFO: grad norm: 255.327 246.145 67.859
2024-12-02-12:52:07-root-INFO: grad norm: 244.845 234.474 70.504
2024-12-02-12:52:08-root-INFO: grad norm: 237.593 228.260 65.939
2024-12-02-12:52:08-root-INFO: grad norm: 232.361 223.063 65.075
2024-12-02-12:52:09-root-INFO: grad norm: 228.667 219.389 64.476
2024-12-02-12:52:09-root-INFO: grad norm: 226.255 217.686 61.677
2024-12-02-12:52:09-root-INFO: Loss Change: 3134.930 -> 2992.090
2024-12-02-12:52:09-root-INFO: Regularization Change: 0.000 -> 1.004
2024-12-02-12:52:09-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-12:52:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:52:10-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-12:52:10-root-INFO: grad norm: 246.006 237.637 63.619
2024-12-02-12:52:10-root-INFO: grad norm: 235.902 227.482 62.463
2024-12-02-12:52:11-root-INFO: grad norm: 238.713 231.429 58.520
2024-12-02-12:52:11-root-INFO: grad norm: 247.287 239.395 61.974
2024-12-02-12:52:12-root-INFO: grad norm: 259.015 250.696 65.118
2024-12-02-12:52:12-root-INFO: grad norm: 273.754 265.619 66.241
2024-12-02-12:52:13-root-INFO: grad norm: 291.004 281.434 74.012
2024-12-02-12:52:13-root-INFO: grad norm: 310.597 301.752 73.596
2024-12-02-12:52:13-root-INFO: Loss Change: 2955.872 -> 2869.799
2024-12-02-12:52:13-root-INFO: Regularization Change: 0.000 -> 0.790
2024-12-02-12:52:13-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-12:52:14-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:52:14-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-12:52:14-root-INFO: grad norm: 354.085 342.524 89.743
2024-12-02-12:52:14-root-INFO: grad norm: 375.595 365.792 85.254
2024-12-02-12:52:15-root-INFO: grad norm: 405.354 393.692 96.534
2024-12-02-12:52:16-root-INFO: grad norm: 438.032 427.131 97.112
2024-12-02-12:52:16-root-INFO: grad norm: 468.080 454.240 112.981
2024-12-02-12:52:17-root-INFO: grad norm: 493.305 481.036 109.337
2024-12-02-12:52:17-root-INFO: grad norm: 510.113 494.781 124.125
2024-12-02-12:52:18-root-INFO: grad norm: 518.145 505.038 115.803
2024-12-02-12:52:18-root-INFO: Loss Change: 2842.684 -> 2803.093
2024-12-02-12:52:18-root-INFO: Regularization Change: 0.000 -> 0.703
2024-12-02-12:52:18-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-12:52:18-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-12:52:18-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-12:52:18-root-INFO: grad norm: 556.957 540.902 132.767
2024-12-02-12:52:19-root-INFO: grad norm: 555.850 542.580 120.732
2024-12-02-12:52:19-root-INFO: grad norm: 545.965 531.225 126.007
2024-12-02-12:52:20-root-INFO: grad norm: 531.742 518.800 116.601
2024-12-02-12:52:20-root-INFO: grad norm: 509.616 495.735 118.134
2024-12-02-12:52:21-root-INFO: grad norm: 484.905 472.773 107.788
2024-12-02-12:52:21-root-INFO: grad norm: 458.514 446.017 106.320
2024-12-02-12:52:22-root-INFO: grad norm: 432.935 421.852 97.333
2024-12-02-12:52:22-root-INFO: Loss Change: 2792.847 -> 2685.861
2024-12-02-12:52:22-root-INFO: Regularization Change: 0.000 -> 0.663
2024-12-02-12:52:22-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-12:52:22-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-12:52:22-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-12:52:22-root-INFO: grad norm: 478.084 462.757 120.083
2024-12-02-12:52:23-root-INFO: grad norm: 457.531 445.393 104.688
2024-12-02-12:52:23-root-INFO: grad norm: 443.797 432.616 98.993
2024-12-02-12:52:24-root-INFO: grad norm: 435.743 424.793 97.069
2024-12-02-12:52:24-root-INFO: grad norm: 428.334 417.585 95.355
2024-12-02-12:52:25-root-INFO: grad norm: 422.394 411.987 93.186
2024-12-02-12:52:25-root-INFO: grad norm: 416.773 406.256 93.037
2024-12-02-12:52:26-root-INFO: grad norm: 411.898 401.838 90.476
2024-12-02-12:52:26-root-INFO: Loss Change: 2684.064 -> 2596.511
2024-12-02-12:52:26-root-INFO: Regularization Change: 0.000 -> 0.635
2024-12-02-12:52:26-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-12:52:26-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:52:26-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-12:52:26-root-INFO: grad norm: 425.491 415.731 90.611
2024-12-02-12:52:27-root-INFO: grad norm: 426.052 416.455 89.920
2024-12-02-12:52:27-root-INFO: grad norm: 426.077 416.205 91.188
2024-12-02-12:52:28-root-INFO: grad norm: 425.237 415.884 88.694
2024-12-02-12:52:28-root-INFO: grad norm: 422.930 413.101 90.648
2024-12-02-12:52:29-root-INFO: grad norm: 419.619 410.467 87.163
2024-12-02-12:52:29-root-INFO: grad norm: 415.415 405.773 88.981
2024-12-02-12:52:30-root-INFO: grad norm: 410.737 401.777 85.326
2024-12-02-12:52:30-root-INFO: Loss Change: 2583.980 -> 2522.636
2024-12-02-12:52:30-root-INFO: Regularization Change: 0.000 -> 0.556
2024-12-02-12:52:30-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-12:52:30-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:52:30-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-12:52:31-root-INFO: grad norm: 552.398 529.563 157.183
2024-12-02-12:52:31-root-INFO: grad norm: 541.700 529.467 114.469
2024-12-02-12:52:31-root-INFO: grad norm: 535.247 521.936 118.624
2024-12-02-12:52:32-root-INFO: grad norm: 525.992 514.724 108.291
2024-12-02-12:52:33-root-INFO: grad norm: 506.894 495.027 109.037
2024-12-02-12:52:33-root-INFO: grad norm: 486.101 475.645 100.280
2024-12-02-12:52:34-root-INFO: grad norm: 462.344 451.795 98.196
2024-12-02-12:52:34-root-INFO: grad norm: 441.321 431.752 91.404
2024-12-02-12:52:34-root-INFO: Loss Change: 2550.392 -> 2449.632
2024-12-02-12:52:34-root-INFO: Regularization Change: 0.000 -> 0.676
2024-12-02-12:52:34-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-12:52:34-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:52:35-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-12:52:35-root-INFO: grad norm: 425.833 416.875 86.884
2024-12-02-12:52:35-root-INFO: grad norm: 416.344 406.852 88.398
2024-12-02-12:52:36-root-INFO: grad norm: 411.200 402.507 84.103
2024-12-02-12:52:36-root-INFO: grad norm: 409.621 400.730 84.881
2024-12-02-12:52:37-root-INFO: grad norm: 410.622 401.832 84.508
2024-12-02-12:52:37-root-INFO: grad norm: 412.866 404.159 84.341
2024-12-02-12:52:38-root-INFO: grad norm: 416.500 407.506 86.087
2024-12-02-12:52:38-root-INFO: grad norm: 419.891 411.181 85.079
2024-12-02-12:52:39-root-INFO: Loss Change: 2445.381 -> 2391.290
2024-12-02-12:52:39-root-INFO: Regularization Change: 0.000 -> 0.540
2024-12-02-12:52:39-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-12:52:39-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:52:39-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-12:52:39-root-INFO: grad norm: 553.912 536.025 139.627
2024-12-02-12:52:39-root-INFO: Loss too large (2416.608->2421.205)! Learning rate decreased to 0.00110.
2024-12-02-12:52:40-root-INFO: grad norm: 370.555 362.510 76.796
2024-12-02-12:52:40-root-INFO: grad norm: 252.367 246.734 53.023
2024-12-02-12:52:41-root-INFO: grad norm: 189.276 184.743 41.178
2024-12-02-12:52:41-root-INFO: grad norm: 149.318 145.856 31.967
2024-12-02-12:52:41-root-INFO: grad norm: 125.501 122.160 28.765
2024-12-02-12:52:42-root-INFO: grad norm: 110.055 107.190 24.945
2024-12-02-12:52:42-root-INFO: grad norm: 100.300 97.401 23.940
2024-12-02-12:52:43-root-INFO: Loss Change: 2416.608 -> 2270.829
2024-12-02-12:52:43-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-02-12:52:43-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-12:52:43-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:52:43-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-12:52:43-root-INFO: grad norm: 165.913 157.380 52.523
2024-12-02-12:52:44-root-INFO: grad norm: 206.448 201.740 43.839
2024-12-02-12:52:44-root-INFO: Loss too large (2255.003->2260.263)! Learning rate decreased to 0.00115.
2024-12-02-12:52:44-root-INFO: grad norm: 201.690 197.432 41.223
2024-12-02-12:52:45-root-INFO: grad norm: 203.099 199.223 39.490
2024-12-02-12:52:45-root-INFO: grad norm: 209.181 205.048 41.380
2024-12-02-12:52:46-root-INFO: grad norm: 218.978 214.977 41.672
2024-12-02-12:52:46-root-INFO: grad norm: 233.982 229.489 45.633
2024-12-02-12:52:47-root-INFO: grad norm: 252.202 247.781 47.017
2024-12-02-12:52:47-root-INFO: Loss Change: 2259.278 -> 2227.240
2024-12-02-12:52:47-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-02-12:52:47-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-12:52:47-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-12:52:47-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-12:52:47-root-INFO: grad norm: 393.241 381.199 96.568
2024-12-02-12:52:47-root-INFO: Loss too large (2240.364->2303.375)! Learning rate decreased to 0.00120.
2024-12-02-12:52:48-root-INFO: Loss too large (2240.364->2247.712)! Learning rate decreased to 0.00096.
2024-12-02-12:52:48-root-INFO: grad norm: 296.532 291.232 55.816
2024-12-02-12:52:49-root-INFO: grad norm: 225.880 221.422 44.659
2024-12-02-12:52:49-root-INFO: grad norm: 183.047 179.390 36.405
2024-12-02-12:52:50-root-INFO: grad norm: 151.467 148.375 30.445
2024-12-02-12:52:50-root-INFO: grad norm: 130.909 128.025 27.327
2024-12-02-12:52:51-root-INFO: grad norm: 115.734 113.128 24.420
2024-12-02-12:52:51-root-INFO: grad norm: 105.388 102.850 22.988
2024-12-02-12:52:51-root-INFO: Loss Change: 2240.364 -> 2164.499
2024-12-02-12:52:51-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-02-12:52:52-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-12:52:52-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:52:52-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-12:52:52-root-INFO: grad norm: 189.771 182.717 51.261
2024-12-02-12:52:52-root-INFO: Loss too large (2152.496->2165.057)! Learning rate decreased to 0.00125.
2024-12-02-12:52:52-root-INFO: grad norm: 244.294 240.713 41.679
2024-12-02-12:52:53-root-INFO: Loss too large (2151.941->2160.932)! Learning rate decreased to 0.00100.
2024-12-02-12:52:53-root-INFO: grad norm: 232.560 228.411 43.732
2024-12-02-12:52:54-root-INFO: grad norm: 224.722 221.393 38.536
2024-12-02-12:52:54-root-INFO: grad norm: 218.929 215.159 40.455
2024-12-02-12:52:55-root-INFO: grad norm: 215.847 212.636 37.092
2024-12-02-12:52:55-root-INFO: grad norm: 214.707 211.071 39.343
2024-12-02-12:52:55-root-INFO: grad norm: 215.480 212.294 36.916
2024-12-02-12:52:56-root-INFO: Loss Change: 2152.496 -> 2119.884
2024-12-02-12:52:56-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-02-12:52:56-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-12:52:56-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:52:56-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-12:52:56-root-INFO: grad norm: 344.755 334.611 83.017
2024-12-02-12:52:56-root-INFO: Loss too large (2125.340->2215.343)! Learning rate decreased to 0.00131.
2024-12-02-12:52:56-root-INFO: Loss too large (2125.340->2157.229)! Learning rate decreased to 0.00105.
2024-12-02-12:52:57-root-INFO: grad norm: 357.674 352.808 58.800
2024-12-02-12:52:57-root-INFO: grad norm: 384.880 378.891 67.635
2024-12-02-12:52:58-root-INFO: Loss too large (2119.670->2125.137)! Learning rate decreased to 0.00084.
2024-12-02-12:52:58-root-INFO: grad norm: 268.649 264.702 45.883
2024-12-02-12:52:59-root-INFO: grad norm: 183.770 180.675 33.583
2024-12-02-12:52:59-root-INFO: grad norm: 137.566 135.013 26.377
2024-12-02-12:53:00-root-INFO: grad norm: 106.413 104.043 22.332
2024-12-02-12:53:00-root-INFO: grad norm: 88.360 86.166 19.568
2024-12-02-12:53:00-root-INFO: Loss Change: 2125.340 -> 2065.756
2024-12-02-12:53:00-root-INFO: Regularization Change: 0.000 -> 0.240
2024-12-02-12:53:00-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-12:53:00-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:53:01-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-12:53:01-root-INFO: grad norm: 132.519 128.238 33.411
2024-12-02-12:53:01-root-INFO: Loss too large (2059.687->2067.477)! Learning rate decreased to 0.00137.
2024-12-02-12:53:01-root-INFO: Loss too large (2059.687->2060.041)! Learning rate decreased to 0.00109.
2024-12-02-12:53:02-root-INFO: grad norm: 149.916 147.129 28.776
2024-12-02-12:53:02-root-INFO: grad norm: 179.744 176.815 32.319
2024-12-02-12:53:02-root-INFO: Loss too large (2053.944->2053.969)! Learning rate decreased to 0.00087.
2024-12-02-12:53:03-root-INFO: grad norm: 148.646 146.103 27.377
2024-12-02-12:53:03-root-INFO: grad norm: 124.050 121.727 23.893
2024-12-02-12:53:04-root-INFO: grad norm: 107.289 105.099 21.567
2024-12-02-12:53:04-root-INFO: grad norm: 94.385 92.242 19.996
2024-12-02-12:53:05-root-INFO: grad norm: 85.370 83.295 18.709
2024-12-02-12:53:05-root-INFO: Loss Change: 2059.687 -> 2028.588
2024-12-02-12:53:05-root-INFO: Regularization Change: 0.000 -> 0.210
2024-12-02-12:53:05-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-12:53:05-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:53:05-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-12:53:05-root-INFO: grad norm: 185.289 177.755 52.299
2024-12-02-12:53:05-root-INFO: Loss too large (2023.492->2049.392)! Learning rate decreased to 0.00143.
2024-12-02-12:53:06-root-INFO: Loss too large (2023.492->2031.218)! Learning rate decreased to 0.00114.
2024-12-02-12:53:06-root-INFO: grad norm: 225.049 222.147 36.029
2024-12-02-12:53:06-root-INFO: Loss too large (2021.036->2025.772)! Learning rate decreased to 0.00091.
2024-12-02-12:53:07-root-INFO: grad norm: 204.400 201.125 36.442
2024-12-02-12:53:07-root-INFO: grad norm: 188.764 186.114 31.517
2024-12-02-12:53:08-root-INFO: grad norm: 174.005 171.342 30.327
2024-12-02-12:53:08-root-INFO: grad norm: 162.653 160.285 27.657
2024-12-02-12:53:09-root-INFO: grad norm: 152.070 149.673 26.894
2024-12-02-12:53:09-root-INFO: grad norm: 143.798 141.623 24.916
2024-12-02-12:53:09-root-INFO: Loss Change: 2023.492 -> 1989.513
2024-12-02-12:53:09-root-INFO: Regularization Change: 0.000 -> 0.221
2024-12-02-12:53:09-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-12:53:09-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:53:09-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-12:53:10-root-INFO: grad norm: 261.146 253.752 61.704
2024-12-02-12:53:10-root-INFO: Loss too large (1989.060->2064.227)! Learning rate decreased to 0.00149.
2024-12-02-12:53:10-root-INFO: Loss too large (1989.060->2020.778)! Learning rate decreased to 0.00119.
2024-12-02-12:53:10-root-INFO: Loss too large (1989.060->1994.936)! Learning rate decreased to 0.00095.
2024-12-02-12:53:11-root-INFO: grad norm: 233.840 230.995 36.362
2024-12-02-12:53:11-root-INFO: grad norm: 220.244 216.652 39.619
2024-12-02-12:53:12-root-INFO: grad norm: 209.381 206.671 33.578
2024-12-02-12:53:12-root-INFO: grad norm: 198.125 195.180 34.036
2024-12-02-12:53:13-root-INFO: grad norm: 189.076 186.598 30.511
2024-12-02-12:53:13-root-INFO: grad norm: 179.842 177.171 30.879
2024-12-02-12:53:14-root-INFO: grad norm: 172.347 170.048 28.056
2024-12-02-12:53:14-root-INFO: Loss Change: 1989.060 -> 1949.041
2024-12-02-12:53:14-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-02-12:53:14-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-12:53:14-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-12:53:14-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-12:53:14-root-INFO: grad norm: 267.805 262.752 51.778
2024-12-02-12:53:14-root-INFO: Loss too large (1944.761->2045.141)! Learning rate decreased to 0.00156.
2024-12-02-12:53:14-root-INFO: Loss too large (1944.761->1991.831)! Learning rate decreased to 0.00124.
2024-12-02-12:53:15-root-INFO: Loss too large (1944.761->1959.364)! Learning rate decreased to 0.00100.
2024-12-02-12:53:15-root-INFO: grad norm: 258.127 255.504 36.704
2024-12-02-12:53:16-root-INFO: grad norm: 249.659 246.212 41.341
2024-12-02-12:53:16-root-INFO: grad norm: 241.211 238.555 35.699
2024-12-02-12:53:17-root-INFO: grad norm: 230.765 227.722 37.349
2024-12-02-12:53:17-root-INFO: grad norm: 221.368 218.852 33.282
2024-12-02-12:53:18-root-INFO: grad norm: 210.469 207.675 34.178
2024-12-02-12:53:18-root-INFO: grad norm: 201.018 198.668 30.645
2024-12-02-12:53:18-root-INFO: Loss Change: 1944.761 -> 1908.860
2024-12-02-12:53:18-root-INFO: Regularization Change: 0.000 -> 0.221
2024-12-02-12:53:18-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-12:53:19-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-12:53:19-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-12:53:19-root-INFO: grad norm: 365.125 355.437 83.549
2024-12-02-12:53:19-root-INFO: Loss too large (1925.875->2076.459)! Learning rate decreased to 0.00162.
2024-12-02-12:53:19-root-INFO: Loss too large (1925.875->1994.417)! Learning rate decreased to 0.00130.
2024-12-02-12:53:19-root-INFO: Loss too large (1925.875->1942.833)! Learning rate decreased to 0.00104.
2024-12-02-12:53:20-root-INFO: grad norm: 312.025 308.757 45.038
2024-12-02-12:53:20-root-INFO: grad norm: 286.886 282.902 47.644
2024-12-02-12:53:21-root-INFO: grad norm: 268.249 265.102 40.974
2024-12-02-12:53:21-root-INFO: grad norm: 246.151 242.938 39.638
2024-12-02-12:53:22-root-INFO: grad norm: 228.843 226.106 35.286
2024-12-02-12:53:22-root-INFO: grad norm: 209.679 206.911 33.952
2024-12-02-12:53:23-root-INFO: grad norm: 194.568 192.170 30.454
2024-12-02-12:53:23-root-INFO: Loss Change: 1925.875 -> 1866.630
2024-12-02-12:53:23-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-02-12:53:23-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-12:53:23-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-12:53:23-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-12:53:23-root-INFO: grad norm: 294.305 288.429 58.515
2024-12-02-12:53:24-root-INFO: Loss too large (1872.155->1988.255)! Learning rate decreased to 0.00170.
2024-12-02-12:53:24-root-INFO: Loss too large (1872.155->1925.217)! Learning rate decreased to 0.00136.
2024-12-02-12:53:24-root-INFO: Loss too large (1872.155->1886.532)! Learning rate decreased to 0.00108.
2024-12-02-12:53:24-root-INFO: grad norm: 257.482 254.870 36.581
2024-12-02-12:53:25-root-INFO: grad norm: 229.222 226.146 37.428
2024-12-02-12:53:25-root-INFO: grad norm: 208.731 206.328 31.581
2024-12-02-12:53:26-root-INFO: grad norm: 186.810 184.273 30.686
2024-12-02-12:53:27-root-INFO: grad norm: 169.864 167.764 26.626
2024-12-02-12:53:27-root-INFO: grad norm: 152.639 150.418 25.944
2024-12-02-12:53:28-root-INFO: grad norm: 139.060 137.181 22.780
2024-12-02-12:53:28-root-INFO: Loss Change: 1872.155 -> 1824.343
2024-12-02-12:53:28-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-02-12:53:28-root-INFO: Undo step: 190
2024-12-02-12:53:28-root-INFO: Undo step: 191
2024-12-02-12:53:28-root-INFO: Undo step: 192
2024-12-02-12:53:28-root-INFO: Undo step: 193
2024-12-02-12:53:28-root-INFO: Undo step: 194
2024-12-02-12:53:28-root-INFO: Undo step: 195
2024-12-02-12:53:28-root-INFO: Undo step: 196
2024-12-02-12:53:28-root-INFO: Undo step: 197
2024-12-02-12:53:28-root-INFO: Undo step: 198
2024-12-02-12:53:28-root-INFO: Undo step: 199
2024-12-02-12:53:28-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-12:53:28-root-INFO: grad norm: 1240.439 1059.553 645.008
2024-12-02-12:53:29-root-INFO: grad norm: 792.494 686.925 395.196
2024-12-02-12:53:29-root-INFO: grad norm: 675.045 626.563 251.205
2024-12-02-12:53:30-root-INFO: grad norm: 668.461 627.655 229.977
2024-12-02-12:53:30-root-INFO: grad norm: 642.417 607.344 209.362
2024-12-02-12:53:31-root-INFO: grad norm: 583.938 551.816 191.003
2024-12-02-12:53:31-root-INFO: grad norm: 545.047 517.237 171.881
2024-12-02-12:53:32-root-INFO: grad norm: 496.649 469.421 162.188
2024-12-02-12:53:32-root-INFO: Loss Change: 4728.575 -> 2707.271
2024-12-02-12:53:32-root-INFO: Regularization Change: 0.000 -> 12.422
2024-12-02-12:53:32-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-12:53:32-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-12:53:32-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-12:53:32-root-INFO: grad norm: 427.664 408.352 127.064
2024-12-02-12:53:33-root-INFO: grad norm: 409.241 388.197 129.541
2024-12-02-12:53:33-root-INFO: grad norm: 401.060 383.661 116.849
2024-12-02-12:53:34-root-INFO: grad norm: 399.762 380.112 123.795
2024-12-02-12:53:34-root-INFO: grad norm: 399.414 383.279 112.380
2024-12-02-12:53:35-root-INFO: grad norm: 402.712 383.839 121.839
2024-12-02-12:53:35-root-INFO: grad norm: 403.972 388.649 110.204
2024-12-02-12:53:36-root-INFO: grad norm: 405.847 387.576 120.400
2024-12-02-12:53:36-root-INFO: Loss Change: 2662.635 -> 2475.669
2024-12-02-12:53:36-root-INFO: Regularization Change: 0.000 -> 2.055
2024-12-02-12:53:36-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-12:53:36-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-12:53:36-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-12:53:36-root-INFO: grad norm: 363.767 350.885 95.949
2024-12-02-12:53:37-root-INFO: grad norm: 378.567 364.258 103.097
2024-12-02-12:53:37-root-INFO: Loss too large (2423.798->2423.860)! Learning rate decreased to 0.00120.
2024-12-02-12:53:38-root-INFO: grad norm: 274.025 264.437 71.853
2024-12-02-12:53:38-root-INFO: grad norm: 195.731 187.702 55.486
2024-12-02-12:53:38-root-INFO: grad norm: 158.359 152.233 43.621
2024-12-02-12:53:39-root-INFO: grad norm: 133.077 126.886 40.118
2024-12-02-12:53:40-root-INFO: grad norm: 118.854 113.787 34.335
2024-12-02-12:53:40-root-INFO: grad norm: 109.075 103.504 34.412
2024-12-02-12:53:40-root-INFO: Loss Change: 2447.172 -> 2303.255
2024-12-02-12:53:40-root-INFO: Regularization Change: 0.000 -> 0.981
2024-12-02-12:53:40-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-12:53:40-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:53:41-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-12:53:41-root-INFO: grad norm: 103.810 97.251 36.315
2024-12-02-12:53:41-root-INFO: grad norm: 88.076 83.000 29.467
2024-12-02-12:53:42-root-INFO: grad norm: 85.731 81.291 27.233
2024-12-02-12:53:42-root-INFO: grad norm: 85.327 81.042 26.700
2024-12-02-12:53:43-root-INFO: grad norm: 87.448 83.044 27.400
2024-12-02-12:53:43-root-INFO: grad norm: 94.621 90.875 26.357
2024-12-02-12:53:44-root-INFO: grad norm: 113.743 109.220 31.758
2024-12-02-12:53:44-root-INFO: grad norm: 152.132 148.327 33.812
2024-12-02-12:53:45-root-INFO: Loss Change: 2273.224 -> 2200.661
2024-12-02-12:53:45-root-INFO: Regularization Change: 0.000 -> 1.074
2024-12-02-12:53:45-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-12:53:45-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:53:45-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-12:53:45-root-INFO: grad norm: 321.288 310.780 81.498
2024-12-02-12:53:45-root-INFO: Loss too large (2196.093->2231.538)! Learning rate decreased to 0.00131.
2024-12-02-12:53:46-root-INFO: grad norm: 342.414 335.437 68.773
2024-12-02-12:53:46-root-INFO: grad norm: 388.349 380.053 79.844
2024-12-02-12:53:46-root-INFO: Loss too large (2189.507->2202.597)! Learning rate decreased to 0.00105.
2024-12-02-12:53:47-root-INFO: grad norm: 289.497 283.818 57.064
2024-12-02-12:53:47-root-INFO: grad norm: 202.401 197.660 43.552
2024-12-02-12:53:48-root-INFO: grad norm: 159.276 155.558 34.216
2024-12-02-12:53:48-root-INFO: grad norm: 126.795 123.092 30.420
2024-12-02-12:53:49-root-INFO: grad norm: 108.180 105.017 25.970
2024-12-02-12:53:49-root-INFO: Loss Change: 2196.093 -> 2116.055
2024-12-02-12:53:49-root-INFO: Regularization Change: 0.000 -> 0.482
2024-12-02-12:53:49-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-12:53:49-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:53:49-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-12:53:49-root-INFO: grad norm: 132.061 127.617 33.972
2024-12-02-12:53:50-root-INFO: grad norm: 198.472 194.403 39.984
2024-12-02-12:53:50-root-INFO: Loss too large (2102.353->2117.894)! Learning rate decreased to 0.00137.
2024-12-02-12:53:51-root-INFO: grad norm: 250.710 245.843 49.157
2024-12-02-12:53:51-root-INFO: Loss too large (2100.688->2107.185)! Learning rate decreased to 0.00109.
2024-12-02-12:53:51-root-INFO: grad norm: 213.204 209.248 40.878
2024-12-02-12:53:52-root-INFO: grad norm: 178.201 174.330 36.937
2024-12-02-12:53:52-root-INFO: grad norm: 156.246 153.013 31.620
2024-12-02-12:53:53-root-INFO: grad norm: 136.989 133.581 30.363
2024-12-02-12:53:53-root-INFO: grad norm: 124.044 121.128 26.736
2024-12-02-12:53:54-root-INFO: Loss Change: 2104.828 -> 2055.703
2024-12-02-12:53:54-root-INFO: Regularization Change: 0.000 -> 0.456
2024-12-02-12:53:54-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-12:53:54-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:53:54-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-12:53:54-root-INFO: grad norm: 185.027 177.819 51.139
2024-12-02-12:53:54-root-INFO: Loss too large (2046.603->2054.339)! Learning rate decreased to 0.00143.
2024-12-02-12:53:54-root-INFO: grad norm: 221.344 217.426 41.465
2024-12-02-12:53:55-root-INFO: Loss too large (2042.445->2046.768)! Learning rate decreased to 0.00114.
2024-12-02-12:53:55-root-INFO: grad norm: 204.740 200.951 39.205
2024-12-02-12:53:56-root-INFO: grad norm: 193.761 190.375 36.065
2024-12-02-12:53:56-root-INFO: grad norm: 183.080 179.659 35.226
2024-12-02-12:53:57-root-INFO: grad norm: 175.795 172.696 32.862
2024-12-02-12:53:57-root-INFO: grad norm: 168.843 165.618 32.841
2024-12-02-12:53:57-root-INFO: grad norm: 164.071 161.144 30.855
2024-12-02-12:53:58-root-INFO: Loss Change: 2046.603 -> 1999.182
2024-12-02-12:53:58-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-02-12:53:58-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-12:53:58-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-12:53:58-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-12:53:58-root-INFO: grad norm: 232.639 225.523 57.099
2024-12-02-12:53:58-root-INFO: Loss too large (1990.209->2015.322)! Learning rate decreased to 0.00149.
2024-12-02-12:53:58-root-INFO: Loss too large (1990.209->1992.055)! Learning rate decreased to 0.00119.
2024-12-02-12:53:59-root-INFO: grad norm: 203.735 200.285 37.337
2024-12-02-12:53:59-root-INFO: grad norm: 195.898 192.442 36.634
2024-12-02-12:54:00-root-INFO: grad norm: 191.076 187.955 34.390
2024-12-02-12:54:00-root-INFO: grad norm: 186.654 183.494 34.204
2024-12-02-12:54:01-root-INFO: grad norm: 183.745 180.817 32.673
2024-12-02-12:54:01-root-INFO: grad norm: 181.040 177.989 33.096
2024-12-02-12:54:02-root-INFO: grad norm: 179.258 176.432 31.705
2024-12-02-12:54:02-root-INFO: Loss Change: 1990.209 -> 1941.803
2024-12-02-12:54:02-root-INFO: Regularization Change: 0.000 -> 0.386
2024-12-02-12:54:02-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-12:54:02-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-12:54:02-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-12:54:02-root-INFO: grad norm: 244.288 239.849 46.359
2024-12-02-12:54:02-root-INFO: Loss too large (1930.357->1977.452)! Learning rate decreased to 0.00156.
2024-12-02-12:54:03-root-INFO: Loss too large (1930.357->1945.155)! Learning rate decreased to 0.00124.
2024-12-02-12:54:03-root-INFO: grad norm: 237.312 234.286 37.779
2024-12-02-12:54:04-root-INFO: grad norm: 235.261 231.747 40.509
2024-12-02-12:54:04-root-INFO: grad norm: 232.776 229.776 37.249
2024-12-02-12:54:04-root-INFO: grad norm: 229.131 225.783 39.028
2024-12-02-12:54:05-root-INFO: grad norm: 225.710 222.780 36.249
2024-12-02-12:54:05-root-INFO: grad norm: 220.985 217.763 37.596
2024-12-02-12:54:06-root-INFO: grad norm: 217.028 214.174 35.083
2024-12-02-12:54:06-root-INFO: Loss Change: 1930.357 -> 1890.170
2024-12-02-12:54:06-root-INFO: Regularization Change: 0.000 -> 0.358
2024-12-02-12:54:06-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-12:54:06-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-12:54:06-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-12:54:06-root-INFO: grad norm: 315.394 305.934 76.667
2024-12-02-12:54:07-root-INFO: Loss too large (1900.441->1952.903)! Learning rate decreased to 0.00162.
2024-12-02-12:54:07-root-INFO: Loss too large (1900.441->1908.554)! Learning rate decreased to 0.00130.
2024-12-02-12:54:07-root-INFO: grad norm: 256.812 253.234 42.717
2024-12-02-12:54:08-root-INFO: grad norm: 239.610 236.205 40.254
2024-12-02-12:54:08-root-INFO: grad norm: 232.807 229.404 39.665
2024-12-02-12:54:09-root-INFO: grad norm: 225.356 222.269 37.172
2024-12-02-12:54:09-root-INFO: grad norm: 220.544 217.431 36.921
2024-12-02-12:54:10-root-INFO: grad norm: 214.869 211.928 35.429
2024-12-02-12:54:10-root-INFO: grad norm: 211.006 208.067 35.098
2024-12-02-12:54:10-root-INFO: Loss Change: 1900.441 -> 1838.866
2024-12-02-12:54:10-root-INFO: Regularization Change: 0.000 -> 0.436
2024-12-02-12:54:10-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-12:54:10-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-12:54:11-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-12:54:11-root-INFO: grad norm: 279.267 274.120 53.372
2024-12-02-12:54:11-root-INFO: Loss too large (1838.271->1901.624)! Learning rate decreased to 0.00170.
2024-12-02-12:54:11-root-INFO: Loss too large (1838.271->1858.096)! Learning rate decreased to 0.00136.
2024-12-02-12:54:12-root-INFO: grad norm: 254.687 251.587 39.614
2024-12-02-12:54:12-root-INFO: grad norm: 241.960 238.934 38.145
2024-12-02-12:54:13-root-INFO: grad norm: 235.314 232.298 37.554
2024-12-02-12:54:13-root-INFO: grad norm: 227.716 224.803 36.301
2024-12-02-12:54:13-root-INFO: grad norm: 222.761 219.882 35.698
2024-12-02-12:54:14-root-INFO: grad norm: 216.631 213.809 34.853
2024-12-02-12:54:14-root-INFO: grad norm: 212.564 209.796 34.193
2024-12-02-12:54:15-root-INFO: Loss Change: 1838.271 -> 1791.450
2024-12-02-12:54:15-root-INFO: Regularization Change: 0.000 -> 0.385
2024-12-02-12:54:15-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-12:54:15-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-12:54:15-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-12:54:15-root-INFO: grad norm: 253.088 248.526 47.840
2024-12-02-12:54:15-root-INFO: Loss too large (1790.570->1841.465)! Learning rate decreased to 0.00177.
2024-12-02-12:54:15-root-INFO: Loss too large (1790.570->1805.031)! Learning rate decreased to 0.00142.
2024-12-02-12:54:16-root-INFO: grad norm: 224.603 221.861 34.989
2024-12-02-12:54:16-root-INFO: grad norm: 205.452 203.028 31.467
2024-12-02-12:54:17-root-INFO: grad norm: 196.688 194.055 32.073
2024-12-02-12:54:17-root-INFO: grad norm: 188.606 186.269 29.597
2024-12-02-12:54:18-root-INFO: grad norm: 184.016 181.485 30.412
2024-12-02-12:54:18-root-INFO: grad norm: 179.374 177.045 28.811
2024-12-02-12:54:19-root-INFO: grad norm: 176.687 174.219 29.431
2024-12-02-12:54:19-root-INFO: Loss Change: 1790.570 -> 1744.653
2024-12-02-12:54:19-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-02-12:54:19-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-12:54:19-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-12:54:19-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-12:54:19-root-INFO: grad norm: 253.811 249.350 47.381
2024-12-02-12:54:19-root-INFO: Loss too large (1742.703->1794.277)! Learning rate decreased to 0.00185.
2024-12-02-12:54:20-root-INFO: Loss too large (1742.703->1757.280)! Learning rate decreased to 0.00148.
2024-12-02-12:54:20-root-INFO: grad norm: 218.597 216.084 33.051
2024-12-02-12:54:21-root-INFO: grad norm: 187.660 185.431 28.839
2024-12-02-12:54:21-root-INFO: grad norm: 173.808 171.603 27.598
2024-12-02-12:54:21-root-INFO: grad norm: 161.018 159.012 25.337
2024-12-02-12:54:22-root-INFO: grad norm: 153.822 151.748 25.176
2024-12-02-12:54:22-root-INFO: grad norm: 147.422 145.478 23.859
2024-12-02-12:54:23-root-INFO: grad norm: 143.876 141.844 24.098
2024-12-02-12:54:23-root-INFO: Loss Change: 1742.703 -> 1693.001
2024-12-02-12:54:23-root-INFO: Regularization Change: 0.000 -> 0.405
2024-12-02-12:54:23-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-12:54:23-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-12:54:23-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-12:54:23-root-INFO: grad norm: 187.791 182.228 45.370
2024-12-02-12:54:24-root-INFO: Loss too large (1695.811->1718.453)! Learning rate decreased to 0.00193.
2024-12-02-12:54:24-root-INFO: Loss too large (1695.811->1699.020)! Learning rate decreased to 0.00154.
2024-12-02-12:54:24-root-INFO: grad norm: 169.961 167.511 28.755
2024-12-02-12:54:25-root-INFO: grad norm: 161.373 159.121 26.864
2024-12-02-12:54:25-root-INFO: grad norm: 158.048 155.831 26.377
2024-12-02-12:54:26-root-INFO: grad norm: 155.727 153.802 24.407
2024-12-02-12:54:26-root-INFO: grad norm: 155.499 153.383 25.564
2024-12-02-12:54:27-root-INFO: grad norm: 156.299 154.438 24.049
2024-12-02-12:54:27-root-INFO: grad norm: 157.826 155.717 25.715
2024-12-02-12:54:28-root-INFO: Loss Change: 1695.811 -> 1655.398
2024-12-02-12:54:28-root-INFO: Regularization Change: 0.000 -> 0.436
2024-12-02-12:54:28-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-12:54:28-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:54:28-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-12:54:28-root-INFO: grad norm: 312.476 307.076 57.842
2024-12-02-12:54:28-root-INFO: Loss too large (1660.939->1749.512)! Learning rate decreased to 0.00201.
2024-12-02-12:54:28-root-INFO: Loss too large (1660.939->1702.908)! Learning rate decreased to 0.00161.
2024-12-02-12:54:28-root-INFO: Loss too large (1660.939->1670.751)! Learning rate decreased to 0.00129.
2024-12-02-12:54:29-root-INFO: grad norm: 198.553 195.840 32.708
2024-12-02-12:54:29-root-INFO: grad norm: 84.080 82.042 18.400
2024-12-02-12:54:30-root-INFO: grad norm: 67.547 65.540 16.342
2024-12-02-12:54:30-root-INFO: grad norm: 59.024 57.102 14.940
2024-12-02-12:54:31-root-INFO: grad norm: 55.352 53.308 14.903
2024-12-02-12:54:31-root-INFO: grad norm: 53.372 51.458 14.162
2024-12-02-12:54:32-root-INFO: grad norm: 52.386 50.376 14.373
2024-12-02-12:54:32-root-INFO: Loss Change: 1660.939 -> 1604.391
2024-12-02-12:54:32-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-02-12:54:32-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-12:54:32-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:54:32-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-12:54:32-root-INFO: grad norm: 100.390 97.010 25.833
2024-12-02-12:54:33-root-INFO: Loss too large (1597.339->1602.270)! Learning rate decreased to 0.00209.
2024-12-02-12:54:33-root-INFO: grad norm: 145.093 142.513 27.242
2024-12-02-12:54:33-root-INFO: Loss too large (1597.274->1608.279)! Learning rate decreased to 0.00168.
2024-12-02-12:54:34-root-INFO: grad norm: 206.688 203.425 36.583
2024-12-02-12:54:34-root-INFO: Loss too large (1594.515->1607.062)! Learning rate decreased to 0.00134.
2024-12-02-12:54:34-root-INFO: Loss too large (1594.515->1595.681)! Learning rate decreased to 0.00107.
2024-12-02-12:54:34-root-INFO: grad norm: 135.233 132.729 25.904
2024-12-02-12:54:35-root-INFO: grad norm: 69.697 67.850 15.939
2024-12-02-12:54:35-root-INFO: grad norm: 60.569 58.622 15.234
2024-12-02-12:54:36-root-INFO: grad norm: 55.618 53.800 14.102
2024-12-02-12:54:36-root-INFO: grad norm: 54.019 52.097 14.280
2024-12-02-12:54:37-root-INFO: Loss Change: 1597.339 -> 1566.928
2024-12-02-12:54:37-root-INFO: Regularization Change: 0.000 -> 0.284
2024-12-02-12:54:37-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-12:54:37-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:54:37-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-12:54:37-root-INFO: grad norm: 306.068 298.138 69.221
2024-12-02-12:54:37-root-INFO: Loss too large (1586.736->1675.696)! Learning rate decreased to 0.00218.
2024-12-02-12:54:37-root-INFO: Loss too large (1586.736->1641.096)! Learning rate decreased to 0.00175.
2024-12-02-12:54:37-root-INFO: Loss too large (1586.736->1613.041)! Learning rate decreased to 0.00140.
2024-12-02-12:54:38-root-INFO: Loss too large (1586.736->1591.808)! Learning rate decreased to 0.00112.
2024-12-02-12:54:38-root-INFO: grad norm: 201.336 198.372 34.418
2024-12-02-12:54:39-root-INFO: grad norm: 85.398 81.535 25.394
2024-12-02-12:54:39-root-INFO: grad norm: 78.026 75.715 18.848
2024-12-02-12:54:40-root-INFO: grad norm: 73.687 71.194 19.004
2024-12-02-12:54:40-root-INFO: grad norm: 71.993 69.980 16.905
2024-12-02-12:54:40-root-INFO: grad norm: 71.223 69.182 16.926
2024-12-02-12:54:41-root-INFO: grad norm: 71.615 69.731 16.320
2024-12-02-12:54:41-root-INFO: Loss Change: 1586.736 -> 1526.725
2024-12-02-12:54:41-root-INFO: Regularization Change: 0.000 -> 0.346
2024-12-02-12:54:41-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-12:54:41-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:54:41-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-12:54:42-root-INFO: grad norm: 229.308 225.038 44.044
2024-12-02-12:54:42-root-INFO: Loss too large (1529.810->1612.481)! Learning rate decreased to 0.00228.
2024-12-02-12:54:42-root-INFO: Loss too large (1529.810->1583.350)! Learning rate decreased to 0.00182.
2024-12-02-12:54:42-root-INFO: Loss too large (1529.810->1559.870)! Learning rate decreased to 0.00146.
2024-12-02-12:54:42-root-INFO: Loss too large (1529.810->1542.264)! Learning rate decreased to 0.00117.
2024-12-02-12:54:42-root-INFO: Loss too large (1529.810->1530.136)! Learning rate decreased to 0.00093.
2024-12-02-12:54:43-root-INFO: grad norm: 164.844 162.091 30.002
2024-12-02-12:54:43-root-INFO: grad norm: 105.230 102.736 22.775
2024-12-02-12:54:44-root-INFO: grad norm: 95.600 93.213 21.229
2024-12-02-12:54:44-root-INFO: grad norm: 88.849 86.796 18.989
2024-12-02-12:54:45-root-INFO: grad norm: 87.024 84.751 19.762
2024-12-02-12:54:45-root-INFO: grad norm: 88.102 86.296 17.749
2024-12-02-12:54:46-root-INFO: grad norm: 91.871 89.650 20.078
2024-12-02-12:54:46-root-INFO: Loss Change: 1529.810 -> 1487.990
2024-12-02-12:54:46-root-INFO: Regularization Change: 0.000 -> 0.248
2024-12-02-12:54:46-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-12:54:46-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:54:46-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-12:54:47-root-INFO: grad norm: 344.919 340.785 53.245
2024-12-02-12:54:47-root-INFO: Loss too large (1498.349->1657.178)! Learning rate decreased to 0.00237.
2024-12-02-12:54:47-root-INFO: Loss too large (1498.349->1626.153)! Learning rate decreased to 0.00190.
2024-12-02-12:54:47-root-INFO: Loss too large (1498.349->1595.036)! Learning rate decreased to 0.00152.
2024-12-02-12:54:47-root-INFO: Loss too large (1498.349->1565.114)! Learning rate decreased to 0.00122.
2024-12-02-12:54:47-root-INFO: Loss too large (1498.349->1538.142)! Learning rate decreased to 0.00097.
2024-12-02-12:54:47-root-INFO: Loss too large (1498.349->1515.781)! Learning rate decreased to 0.00078.
2024-12-02-12:54:48-root-INFO: Loss too large (1498.349->1499.019)! Learning rate decreased to 0.00062.
2024-12-02-12:54:48-root-INFO: grad norm: 206.181 203.512 33.065
2024-12-02-12:54:48-root-INFO: grad norm: 90.876 88.528 20.524
2024-12-02-12:54:49-root-INFO: grad norm: 75.886 73.450 19.076
2024-12-02-12:54:49-root-INFO: grad norm: 66.462 64.282 16.880
2024-12-02-12:54:50-root-INFO: grad norm: 61.622 59.237 16.978
2024-12-02-12:54:50-root-INFO: grad norm: 58.309 56.217 15.479
2024-12-02-12:54:51-root-INFO: grad norm: 56.118 53.828 15.868
2024-12-02-12:54:51-root-INFO: Loss Change: 1498.349 -> 1462.341
2024-12-02-12:54:51-root-INFO: Regularization Change: 0.000 -> 0.098
2024-12-02-12:54:51-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-12:54:51-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-12:54:51-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-12:54:52-root-INFO: grad norm: 301.695 297.943 47.438
2024-12-02-12:54:52-root-INFO: Loss too large (1470.991->1635.399)! Learning rate decreased to 0.00247.
2024-12-02-12:54:52-root-INFO: Loss too large (1470.991->1606.123)! Learning rate decreased to 0.00198.
2024-12-02-12:54:52-root-INFO: Loss too large (1470.991->1576.723)! Learning rate decreased to 0.00158.
2024-12-02-12:54:52-root-INFO: Loss too large (1470.991->1548.228)! Learning rate decreased to 0.00127.
2024-12-02-12:54:52-root-INFO: Loss too large (1470.991->1522.134)! Learning rate decreased to 0.00101.
2024-12-02-12:54:53-root-INFO: Loss too large (1470.991->1499.971)! Learning rate decreased to 0.00081.
2024-12-02-12:54:53-root-INFO: Loss too large (1470.991->1482.765)! Learning rate decreased to 0.00065.
2024-12-02-12:54:53-root-INFO: grad norm: 251.222 248.477 37.042
2024-12-02-12:54:54-root-INFO: grad norm: 187.524 185.140 29.804
2024-12-02-12:54:54-root-INFO: grad norm: 183.042 180.666 29.400
2024-12-02-12:54:55-root-INFO: grad norm: 180.912 178.934 26.677
2024-12-02-12:54:55-root-INFO: grad norm: 182.484 180.137 29.175
2024-12-02-12:54:56-root-INFO: grad norm: 187.536 185.709 26.115
2024-12-02-12:54:56-root-INFO: Loss too large (1450.136->1450.670)! Learning rate decreased to 0.00052.
2024-12-02-12:54:56-root-INFO: grad norm: 139.055 136.926 24.243
2024-12-02-12:54:57-root-INFO: Loss Change: 1470.991 -> 1444.011
2024-12-02-12:54:57-root-INFO: Regularization Change: 0.000 -> 0.079
2024-12-02-12:54:57-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-12:54:57-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-12:54:57-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-12:54:57-root-INFO: grad norm: 441.558 436.378 67.438
2024-12-02-12:54:57-root-INFO: Loss too large (1471.299->1670.329)! Learning rate decreased to 0.00258.
2024-12-02-12:54:57-root-INFO: Loss too large (1471.299->1642.826)! Learning rate decreased to 0.00206.
2024-12-02-12:54:57-root-INFO: Loss too large (1471.299->1614.945)! Learning rate decreased to 0.00165.
2024-12-02-12:54:58-root-INFO: Loss too large (1471.299->1586.056)! Learning rate decreased to 0.00132.
2024-12-02-12:54:58-root-INFO: Loss too large (1471.299->1556.450)! Learning rate decreased to 0.00106.
2024-12-02-12:54:58-root-INFO: Loss too large (1471.299->1527.290)! Learning rate decreased to 0.00084.
2024-12-02-12:54:58-root-INFO: Loss too large (1471.299->1500.466)! Learning rate decreased to 0.00068.
2024-12-02-12:54:58-root-INFO: Loss too large (1471.299->1478.064)! Learning rate decreased to 0.00054.
2024-12-02-12:54:59-root-INFO: grad norm: 287.352 284.303 41.749
2024-12-02-12:54:59-root-INFO: grad norm: 131.808 128.666 28.606
2024-12-02-12:55:00-root-INFO: grad norm: 115.455 112.789 24.669
2024-12-02-12:55:00-root-INFO: grad norm: 101.448 98.863 22.758
2024-12-02-12:55:01-root-INFO: grad norm: 93.228 90.739 21.400
2024-12-02-12:55:01-root-INFO: grad norm: 85.973 83.711 19.592
2024-12-02-12:55:02-root-INFO: grad norm: 81.166 78.821 19.371
2024-12-02-12:55:02-root-INFO: Loss Change: 1471.299 -> 1428.205
2024-12-02-12:55:02-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-02-12:55:02-root-INFO: Undo step: 180
2024-12-02-12:55:02-root-INFO: Undo step: 181
2024-12-02-12:55:02-root-INFO: Undo step: 182
2024-12-02-12:55:02-root-INFO: Undo step: 183
2024-12-02-12:55:02-root-INFO: Undo step: 184
2024-12-02-12:55:02-root-INFO: Undo step: 185
2024-12-02-12:55:02-root-INFO: Undo step: 186
2024-12-02-12:55:02-root-INFO: Undo step: 187
2024-12-02-12:55:02-root-INFO: Undo step: 188
2024-12-02-12:55:02-root-INFO: Undo step: 189
2024-12-02-12:55:02-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-12:55:02-root-INFO: grad norm: 967.368 827.553 500.955
2024-12-02-12:55:03-root-INFO: grad norm: 469.223 418.872 211.462
2024-12-02-12:55:03-root-INFO: grad norm: 355.930 328.259 137.594
2024-12-02-12:55:04-root-INFO: grad norm: 346.398 321.689 128.482
2024-12-02-12:55:04-root-INFO: grad norm: 396.623 387.522 84.476
2024-12-02-12:55:05-root-INFO: Loss too large (2266.038->2291.838)! Learning rate decreased to 0.00170.
2024-12-02-12:55:05-root-INFO: grad norm: 472.612 463.567 92.022
2024-12-02-12:55:05-root-INFO: Loss too large (2226.319->2246.766)! Learning rate decreased to 0.00136.
2024-12-02-12:55:06-root-INFO: grad norm: 416.782 408.934 80.501
2024-12-02-12:55:06-root-INFO: grad norm: 374.941 366.794 77.736
2024-12-02-12:55:06-root-INFO: Loss Change: 3937.387 -> 2108.639
2024-12-02-12:55:07-root-INFO: Regularization Change: 0.000 -> 16.300
2024-12-02-12:55:07-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-12:55:07-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-12:55:07-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-12:55:07-root-INFO: grad norm: 407.587 397.696 89.249
2024-12-02-12:55:07-root-INFO: Loss too large (2106.434->2218.796)! Learning rate decreased to 0.00177.
2024-12-02-12:55:07-root-INFO: Loss too large (2106.434->2123.540)! Learning rate decreased to 0.00142.
2024-12-02-12:55:08-root-INFO: grad norm: 373.799 365.346 79.044
2024-12-02-12:55:08-root-INFO: grad norm: 358.443 350.792 73.663
2024-12-02-12:55:09-root-INFO: grad norm: 351.324 343.053 75.783
2024-12-02-12:55:09-root-INFO: grad norm: 363.275 355.558 74.481
2024-12-02-12:55:10-root-INFO: grad norm: 364.138 355.320 79.651
2024-12-02-12:55:10-root-INFO: grad norm: 381.009 373.080 77.330
2024-12-02-12:55:10-root-INFO: Loss too large (1967.294->1968.535)! Learning rate decreased to 0.00113.
2024-12-02-12:55:11-root-INFO: grad norm: 274.161 265.892 66.828
2024-12-02-12:55:11-root-INFO: Loss Change: 2106.434 -> 1908.693
2024-12-02-12:55:11-root-INFO: Regularization Change: 0.000 -> 1.608
2024-12-02-12:55:11-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-12:55:11-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-12:55:11-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-12:55:11-root-INFO: grad norm: 372.357 363.224 81.963
2024-12-02-12:55:12-root-INFO: Loss too large (1906.084->2006.110)! Learning rate decreased to 0.00185.
2024-12-02-12:55:12-root-INFO: Loss too large (1906.084->1959.570)! Learning rate decreased to 0.00148.
2024-12-02-12:55:12-root-INFO: Loss too large (1906.084->1925.551)! Learning rate decreased to 0.00118.
2024-12-02-12:55:12-root-INFO: grad norm: 267.366 258.230 69.295
2024-12-02-12:55:13-root-INFO: grad norm: 133.187 127.786 37.544
2024-12-02-12:55:13-root-INFO: grad norm: 137.960 131.390 42.068
2024-12-02-12:55:14-root-INFO: grad norm: 164.550 159.056 42.165
2024-12-02-12:55:14-root-INFO: grad norm: 190.516 183.142 52.489
2024-12-02-12:55:15-root-INFO: grad norm: 252.092 245.559 57.022
2024-12-02-12:55:15-root-INFO: Loss too large (1818.154->1823.602)! Learning rate decreased to 0.00095.
2024-12-02-12:55:15-root-INFO: grad norm: 217.806 209.889 58.190
2024-12-02-12:55:16-root-INFO: Loss Change: 1906.084 -> 1796.498
2024-12-02-12:55:16-root-INFO: Regularization Change: 0.000 -> 0.760
2024-12-02-12:55:16-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-12:55:16-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-12:55:16-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-12:55:16-root-INFO: grad norm: 369.930 359.979 85.225
2024-12-02-12:55:16-root-INFO: Loss too large (1808.435->1917.587)! Learning rate decreased to 0.00193.
2024-12-02-12:55:16-root-INFO: Loss too large (1808.435->1886.249)! Learning rate decreased to 0.00154.
2024-12-02-12:55:16-root-INFO: Loss too large (1808.435->1856.633)! Learning rate decreased to 0.00123.
2024-12-02-12:55:17-root-INFO: Loss too large (1808.435->1830.408)! Learning rate decreased to 0.00099.
2024-12-02-12:55:17-root-INFO: Loss too large (1808.435->1809.144)! Learning rate decreased to 0.00079.
2024-12-02-12:55:17-root-INFO: grad norm: 241.171 233.626 59.854
2024-12-02-12:55:18-root-INFO: grad norm: 124.576 118.808 37.467
2024-12-02-12:55:18-root-INFO: grad norm: 111.048 105.816 33.685
2024-12-02-12:55:19-root-INFO: grad norm: 101.480 96.295 32.024
2024-12-02-12:55:19-root-INFO: grad norm: 96.266 91.118 31.060
2024-12-02-12:55:20-root-INFO: grad norm: 92.318 87.341 29.904
2024-12-02-12:55:20-root-INFO: grad norm: 89.753 84.675 29.763
2024-12-02-12:55:21-root-INFO: Loss Change: 1808.435 -> 1734.983
2024-12-02-12:55:21-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-02-12:55:21-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-12:55:21-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:55:21-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-12:55:21-root-INFO: grad norm: 280.387 273.256 62.831
2024-12-02-12:55:21-root-INFO: Loss too large (1724.817->1826.840)! Learning rate decreased to 0.00201.
2024-12-02-12:55:21-root-INFO: Loss too large (1724.817->1797.588)! Learning rate decreased to 0.00161.
2024-12-02-12:55:21-root-INFO: Loss too large (1724.817->1771.419)! Learning rate decreased to 0.00129.
2024-12-02-12:55:21-root-INFO: Loss too large (1724.817->1749.381)! Learning rate decreased to 0.00103.
2024-12-02-12:55:22-root-INFO: Loss too large (1724.817->1732.367)! Learning rate decreased to 0.00082.
2024-12-02-12:55:22-root-INFO: grad norm: 239.587 232.233 58.907
2024-12-02-12:55:23-root-INFO: grad norm: 191.583 186.194 45.123
2024-12-02-12:55:23-root-INFO: grad norm: 185.017 178.869 47.300
2024-12-02-12:55:24-root-INFO: grad norm: 179.475 174.525 41.859
2024-12-02-12:55:24-root-INFO: grad norm: 179.052 172.990 46.198
2024-12-02-12:55:24-root-INFO: grad norm: 180.864 176.024 41.557
2024-12-02-12:55:25-root-INFO: grad norm: 183.413 177.277 47.048
2024-12-02-12:55:25-root-INFO: Loss Change: 1724.817 -> 1676.025
2024-12-02-12:55:25-root-INFO: Regularization Change: 0.000 -> 0.269
2024-12-02-12:55:25-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-12:55:25-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:55:25-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-12:55:26-root-INFO: grad norm: 312.425 305.198 66.811
2024-12-02-12:55:26-root-INFO: Loss too large (1674.825->1807.176)! Learning rate decreased to 0.00209.
2024-12-02-12:55:26-root-INFO: Loss too large (1674.825->1776.341)! Learning rate decreased to 0.00168.
2024-12-02-12:55:26-root-INFO: Loss too large (1674.825->1746.529)! Learning rate decreased to 0.00134.
2024-12-02-12:55:26-root-INFO: Loss too large (1674.825->1719.226)! Learning rate decreased to 0.00107.
2024-12-02-12:55:26-root-INFO: Loss too large (1674.825->1696.125)! Learning rate decreased to 0.00086.
2024-12-02-12:55:27-root-INFO: Loss too large (1674.825->1678.556)! Learning rate decreased to 0.00069.
2024-12-02-12:55:27-root-INFO: grad norm: 228.804 222.128 54.866
2024-12-02-12:55:27-root-INFO: grad norm: 150.369 145.931 36.262
2024-12-02-12:55:28-root-INFO: grad norm: 130.300 125.535 34.915
2024-12-02-12:55:28-root-INFO: grad norm: 112.920 109.087 29.171
2024-12-02-12:55:29-root-INFO: grad norm: 103.543 99.136 29.887
2024-12-02-12:55:29-root-INFO: grad norm: 95.575 91.930 26.145
2024-12-02-12:55:30-root-INFO: grad norm: 90.549 86.311 27.375
2024-12-02-12:55:30-root-INFO: Loss Change: 1674.825 -> 1630.234
2024-12-02-12:55:30-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-02-12:55:30-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-12:55:30-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:55:30-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-12:55:30-root-INFO: grad norm: 411.007 400.331 93.067
2024-12-02-12:55:31-root-INFO: Loss too large (1647.753->1808.717)! Learning rate decreased to 0.00218.
2024-12-02-12:55:31-root-INFO: Loss too large (1647.753->1776.011)! Learning rate decreased to 0.00175.
2024-12-02-12:55:31-root-INFO: Loss too large (1647.753->1743.264)! Learning rate decreased to 0.00140.
2024-12-02-12:55:31-root-INFO: Loss too large (1647.753->1711.359)! Learning rate decreased to 0.00112.
2024-12-02-12:55:31-root-INFO: Loss too large (1647.753->1681.767)! Learning rate decreased to 0.00089.
2024-12-02-12:55:31-root-INFO: Loss too large (1647.753->1656.495)! Learning rate decreased to 0.00072.
2024-12-02-12:55:32-root-INFO: grad norm: 297.928 290.014 68.211
2024-12-02-12:55:32-root-INFO: grad norm: 175.302 169.694 43.986
2024-12-02-12:55:33-root-INFO: grad norm: 162.715 158.077 38.569
2024-12-02-12:55:33-root-INFO: grad norm: 151.729 147.211 36.751
2024-12-02-12:55:34-root-INFO: grad norm: 146.491 142.120 35.519
2024-12-02-12:55:34-root-INFO: grad norm: 141.962 137.939 33.554
2024-12-02-12:55:35-root-INFO: grad norm: 139.892 135.633 34.256
2024-12-02-12:55:35-root-INFO: Loss Change: 1647.753 -> 1587.800
2024-12-02-12:55:35-root-INFO: Regularization Change: 0.000 -> 0.202
2024-12-02-12:55:35-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-12:55:35-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:55:35-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-12:55:35-root-INFO: grad norm: 355.406 347.482 74.633
2024-12-02-12:55:35-root-INFO: Loss too large (1592.489->1756.975)! Learning rate decreased to 0.00228.
2024-12-02-12:55:36-root-INFO: Loss too large (1592.489->1726.055)! Learning rate decreased to 0.00182.
2024-12-02-12:55:36-root-INFO: Loss too large (1592.489->1694.688)! Learning rate decreased to 0.00146.
2024-12-02-12:55:36-root-INFO: Loss too large (1592.489->1663.954)! Learning rate decreased to 0.00117.
2024-12-02-12:55:36-root-INFO: Loss too large (1592.489->1635.384)! Learning rate decreased to 0.00093.
2024-12-02-12:55:36-root-INFO: Loss too large (1592.489->1610.983)! Learning rate decreased to 0.00075.
2024-12-02-12:55:37-root-INFO: grad norm: 299.010 290.863 69.322
2024-12-02-12:55:37-root-INFO: grad norm: 225.572 220.307 48.453
2024-12-02-12:55:38-root-INFO: grad norm: 222.742 216.526 52.257
2024-12-02-12:55:38-root-INFO: grad norm: 219.965 215.098 46.016
2024-12-02-12:55:39-root-INFO: grad norm: 219.374 213.167 51.814
2024-12-02-12:55:39-root-INFO: grad norm: 218.584 213.859 45.201
2024-12-02-12:55:40-root-INFO: grad norm: 218.401 212.207 51.643
2024-12-02-12:55:40-root-INFO: Loss Change: 1592.489 -> 1553.029
2024-12-02-12:55:40-root-INFO: Regularization Change: 0.000 -> 0.162
2024-12-02-12:55:40-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-12:55:40-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-12:55:40-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-12:55:40-root-INFO: grad norm: 438.296 429.452 87.605
2024-12-02-12:55:41-root-INFO: Loss too large (1571.866->1759.758)! Learning rate decreased to 0.00237.
2024-12-02-12:55:41-root-INFO: Loss too large (1571.866->1731.020)! Learning rate decreased to 0.00190.
2024-12-02-12:55:41-root-INFO: Loss too large (1571.866->1699.496)! Learning rate decreased to 0.00152.
2024-12-02-12:55:41-root-INFO: Loss too large (1571.866->1666.010)! Learning rate decreased to 0.00122.
2024-12-02-12:55:41-root-INFO: Loss too large (1571.866->1632.061)! Learning rate decreased to 0.00097.
2024-12-02-12:55:41-root-INFO: Loss too large (1571.866->1599.997)! Learning rate decreased to 0.00078.
2024-12-02-12:55:41-root-INFO: Loss too large (1571.866->1572.985)! Learning rate decreased to 0.00062.
2024-12-02-12:55:42-root-INFO: grad norm: 279.018 271.553 64.112
2024-12-02-12:55:42-root-INFO: grad norm: 158.270 154.407 34.757
2024-12-02-12:55:43-root-INFO: grad norm: 130.912 127.085 31.420
2024-12-02-12:55:43-root-INFO: grad norm: 108.315 105.441 24.783
2024-12-02-12:55:44-root-INFO: grad norm: 95.144 91.991 24.292
2024-12-02-12:55:44-root-INFO: grad norm: 84.548 82.063 20.347
2024-12-02-12:55:45-root-INFO: grad norm: 77.615 74.765 20.840
2024-12-02-12:55:45-root-INFO: Loss Change: 1571.866 -> 1518.683
2024-12-02-12:55:45-root-INFO: Regularization Change: 0.000 -> 0.118
2024-12-02-12:55:45-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-12:55:45-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-12:55:45-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-12:55:46-root-INFO: grad norm: 280.650 274.071 60.413
2024-12-02-12:55:46-root-INFO: Loss too large (1524.395->1691.650)! Learning rate decreased to 0.00247.
2024-12-02-12:55:46-root-INFO: Loss too large (1524.395->1660.828)! Learning rate decreased to 0.00198.
2024-12-02-12:55:46-root-INFO: Loss too large (1524.395->1629.175)! Learning rate decreased to 0.00158.
2024-12-02-12:55:46-root-INFO: Loss too large (1524.395->1598.229)! Learning rate decreased to 0.00127.
2024-12-02-12:55:46-root-INFO: Loss too large (1524.395->1570.168)! Learning rate decreased to 0.00101.
2024-12-02-12:55:46-root-INFO: Loss too large (1524.395->1547.190)! Learning rate decreased to 0.00081.
2024-12-02-12:55:47-root-INFO: Loss too large (1524.395->1530.447)! Learning rate decreased to 0.00065.
2024-12-02-12:55:47-root-INFO: grad norm: 239.448 233.624 52.488
2024-12-02-12:55:48-root-INFO: grad norm: 198.471 193.761 42.981
2024-12-02-12:55:48-root-INFO: grad norm: 180.205 175.604 40.462
2024-12-02-12:55:49-root-INFO: grad norm: 162.032 158.223 34.925
2024-12-02-12:55:49-root-INFO: grad norm: 150.916 146.874 34.694
2024-12-02-12:55:50-root-INFO: grad norm: 139.889 136.603 30.144
2024-12-02-12:55:50-root-INFO: grad norm: 132.306 128.618 31.018
2024-12-02-12:55:50-root-INFO: Loss Change: 1524.395 -> 1493.725
2024-12-02-12:55:50-root-INFO: Regularization Change: 0.000 -> 0.109
2024-12-02-12:55:50-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-12:55:50-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-12:55:50-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-12:55:51-root-INFO: grad norm: 385.980 377.254 81.606
2024-12-02-12:55:51-root-INFO: Loss too large (1515.175->1712.478)! Learning rate decreased to 0.00258.
2024-12-02-12:55:51-root-INFO: Loss too large (1515.175->1684.035)! Learning rate decreased to 0.00206.
2024-12-02-12:55:51-root-INFO: Loss too large (1515.175->1652.962)! Learning rate decreased to 0.00165.
2024-12-02-12:55:51-root-INFO: Loss too large (1515.175->1619.844)! Learning rate decreased to 0.00132.
2024-12-02-12:55:51-root-INFO: Loss too large (1515.175->1586.073)! Learning rate decreased to 0.00106.
2024-12-02-12:55:52-root-INFO: Loss too large (1515.175->1554.116)! Learning rate decreased to 0.00084.
2024-12-02-12:55:52-root-INFO: Loss too large (1515.175->1527.167)! Learning rate decreased to 0.00068.
2024-12-02-12:55:52-root-INFO: grad norm: 310.664 303.132 67.993
2024-12-02-12:55:53-root-INFO: grad norm: 237.012 231.246 51.961
2024-12-02-12:55:53-root-INFO: grad norm: 218.651 213.345 47.876
2024-12-02-12:55:54-root-INFO: grad norm: 199.306 194.570 43.189
2024-12-02-12:55:54-root-INFO: grad norm: 188.272 183.599 41.684
2024-12-02-12:55:55-root-INFO: grad norm: 176.528 172.405 37.933
2024-12-02-12:55:55-root-INFO: grad norm: 168.717 164.453 37.693
2024-12-02-12:55:55-root-INFO: Loss Change: 1515.175 -> 1470.149
2024-12-02-12:55:55-root-INFO: Regularization Change: 0.000 -> 0.137
2024-12-02-12:55:55-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-12:55:55-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-12:55:56-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-12:55:56-root-INFO: grad norm: 368.951 361.478 73.884
2024-12-02-12:55:56-root-INFO: Loss too large (1486.637->1691.070)! Learning rate decreased to 0.00269.
2024-12-02-12:55:56-root-INFO: Loss too large (1486.637->1663.144)! Learning rate decreased to 0.00215.
2024-12-02-12:55:56-root-INFO: Loss too large (1486.637->1631.935)! Learning rate decreased to 0.00172.
2024-12-02-12:55:56-root-INFO: Loss too large (1486.637->1598.153)! Learning rate decreased to 0.00138.
2024-12-02-12:55:56-root-INFO: Loss too large (1486.637->1563.321)! Learning rate decreased to 0.00110.
2024-12-02-12:55:57-root-INFO: Loss too large (1486.637->1530.138)! Learning rate decreased to 0.00088.
2024-12-02-12:55:57-root-INFO: Loss too large (1486.637->1502.088)! Learning rate decreased to 0.00070.
2024-12-02-12:55:57-root-INFO: grad norm: 312.704 304.871 69.551
2024-12-02-12:55:58-root-INFO: grad norm: 257.083 251.608 52.775
2024-12-02-12:55:58-root-INFO: grad norm: 240.603 234.600 53.410
2024-12-02-12:55:59-root-INFO: grad norm: 222.509 217.775 45.652
2024-12-02-12:55:59-root-INFO: grad norm: 211.958 206.610 47.314
2024-12-02-12:56:00-root-INFO: grad norm: 200.295 196.024 41.141
2024-12-02-12:56:00-root-INFO: grad norm: 192.383 187.484 43.140
2024-12-02-12:56:00-root-INFO: Loss Change: 1486.637 -> 1447.784
2024-12-02-12:56:00-root-INFO: Regularization Change: 0.000 -> 0.123
2024-12-02-12:56:00-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-12:56:00-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-12:56:01-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-12:56:01-root-INFO: grad norm: 409.865 401.366 83.036
2024-12-02-12:56:01-root-INFO: Loss too large (1471.219->1679.059)! Learning rate decreased to 0.00280.
2024-12-02-12:56:01-root-INFO: Loss too large (1471.219->1651.742)! Learning rate decreased to 0.00224.
2024-12-02-12:56:01-root-INFO: Loss too large (1471.219->1621.186)! Learning rate decreased to 0.00179.
2024-12-02-12:56:01-root-INFO: Loss too large (1471.219->1587.483)! Learning rate decreased to 0.00143.
2024-12-02-12:56:02-root-INFO: Loss too large (1471.219->1551.731)! Learning rate decreased to 0.00115.
2024-12-02-12:56:02-root-INFO: Loss too large (1471.219->1516.224)! Learning rate decreased to 0.00092.
2024-12-02-12:56:02-root-INFO: Loss too large (1471.219->1484.717)! Learning rate decreased to 0.00073.
2024-12-02-12:56:02-root-INFO: grad norm: 325.798 317.508 73.028
2024-12-02-12:56:03-root-INFO: grad norm: 252.046 246.392 53.088
2024-12-02-12:56:03-root-INFO: grad norm: 234.725 228.850 52.187
2024-12-02-12:56:04-root-INFO: grad norm: 216.595 211.801 45.317
2024-12-02-12:56:04-root-INFO: grad norm: 205.789 200.575 46.032
2024-12-02-12:56:05-root-INFO: grad norm: 194.271 189.998 40.523
2024-12-02-12:56:05-root-INFO: grad norm: 186.295 181.530 41.866
2024-12-02-12:56:06-root-INFO: Loss Change: 1471.219 -> 1421.164
2024-12-02-12:56:06-root-INFO: Regularization Change: 0.000 -> 0.148
2024-12-02-12:56:06-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-12:56:06-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-12:56:06-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-12:56:06-root-INFO: grad norm: 360.969 353.550 72.811
2024-12-02-12:56:06-root-INFO: Loss too large (1439.071->1645.045)! Learning rate decreased to 0.00291.
2024-12-02-12:56:06-root-INFO: Loss too large (1439.071->1617.846)! Learning rate decreased to 0.00233.
2024-12-02-12:56:07-root-INFO: Loss too large (1439.071->1586.832)! Learning rate decreased to 0.00187.
2024-12-02-12:56:07-root-INFO: Loss too large (1439.071->1552.644)! Learning rate decreased to 0.00149.
2024-12-02-12:56:07-root-INFO: Loss too large (1439.071->1516.866)! Learning rate decreased to 0.00119.
2024-12-02-12:56:07-root-INFO: Loss too large (1439.071->1482.358)! Learning rate decreased to 0.00096.
2024-12-02-12:56:07-root-INFO: Loss too large (1439.071->1453.011)! Learning rate decreased to 0.00076.
2024-12-02-12:56:08-root-INFO: grad norm: 304.391 296.371 69.416
2024-12-02-12:56:08-root-INFO: grad norm: 254.295 248.743 52.849
2024-12-02-12:56:09-root-INFO: grad norm: 235.602 229.454 53.471
2024-12-02-12:56:09-root-INFO: grad norm: 216.780 212.012 45.218
2024-12-02-12:56:10-root-INFO: grad norm: 204.874 199.494 46.641
2024-12-02-12:56:10-root-INFO: grad norm: 192.793 188.525 40.342
2024-12-02-12:56:11-root-INFO: grad norm: 184.102 179.237 42.043
2024-12-02-12:56:11-root-INFO: Loss Change: 1439.071 -> 1397.997
2024-12-02-12:56:11-root-INFO: Regularization Change: 0.000 -> 0.136
2024-12-02-12:56:11-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-12:56:11-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-12:56:11-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-12:56:11-root-INFO: grad norm: 354.890 347.416 72.451
2024-12-02-12:56:11-root-INFO: Loss too large (1418.108->1626.964)! Learning rate decreased to 0.00304.
2024-12-02-12:56:12-root-INFO: Loss too large (1418.108->1598.851)! Learning rate decreased to 0.00243.
2024-12-02-12:56:12-root-INFO: Loss too large (1418.108->1566.545)! Learning rate decreased to 0.00194.
2024-12-02-12:56:12-root-INFO: Loss too large (1418.108->1530.679)! Learning rate decreased to 0.00155.
2024-12-02-12:56:12-root-INFO: Loss too large (1418.108->1493.066)! Learning rate decreased to 0.00124.
2024-12-02-12:56:12-root-INFO: Loss too large (1418.108->1457.115)! Learning rate decreased to 0.00099.
2024-12-02-12:56:12-root-INFO: Loss too large (1418.108->1427.217)! Learning rate decreased to 0.00080.
2024-12-02-12:56:13-root-INFO: grad norm: 289.098 281.815 64.480
2024-12-02-12:56:13-root-INFO: grad norm: 240.894 235.468 50.840
2024-12-02-12:56:14-root-INFO: grad norm: 218.307 212.816 48.656
2024-12-02-12:56:14-root-INFO: grad norm: 198.580 194.066 42.098
2024-12-02-12:56:15-root-INFO: grad norm: 184.875 180.170 41.445
2024-12-02-12:56:15-root-INFO: grad norm: 172.513 168.561 36.716
2024-12-02-12:56:16-root-INFO: grad norm: 162.940 158.745 36.734
2024-12-02-12:56:16-root-INFO: Loss Change: 1418.108 -> 1372.837
2024-12-02-12:56:16-root-INFO: Regularization Change: 0.000 -> 0.151
2024-12-02-12:56:16-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-12:56:16-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-12:56:16-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-12:56:16-root-INFO: grad norm: 328.617 321.854 66.324
2024-12-02-12:56:16-root-INFO: Loss too large (1390.189->1598.630)! Learning rate decreased to 0.00316.
2024-12-02-12:56:17-root-INFO: Loss too large (1390.189->1569.762)! Learning rate decreased to 0.00253.
2024-12-02-12:56:17-root-INFO: Loss too large (1390.189->1536.652)! Learning rate decreased to 0.00202.
2024-12-02-12:56:17-root-INFO: Loss too large (1390.189->1500.150)! Learning rate decreased to 0.00162.
2024-12-02-12:56:17-root-INFO: Loss too large (1390.189->1462.319)! Learning rate decreased to 0.00129.
2024-12-02-12:56:17-root-INFO: Loss too large (1390.189->1426.915)! Learning rate decreased to 0.00104.
2024-12-02-12:56:17-root-INFO: Loss too large (1390.189->1398.315)! Learning rate decreased to 0.00083.
2024-12-02-12:56:18-root-INFO: grad norm: 274.305 267.313 61.538
2024-12-02-12:56:18-root-INFO: grad norm: 235.776 230.611 49.082
2024-12-02-12:56:19-root-INFO: grad norm: 212.770 207.321 47.844
2024-12-02-12:56:19-root-INFO: grad norm: 193.775 189.452 40.704
2024-12-02-12:56:20-root-INFO: grad norm: 179.267 174.617 40.566
2024-12-02-12:56:20-root-INFO: grad norm: 166.860 163.083 35.301
2024-12-02-12:56:21-root-INFO: grad norm: 156.602 152.488 35.661
2024-12-02-12:56:21-root-INFO: Loss Change: 1390.189 -> 1347.791
2024-12-02-12:56:21-root-INFO: Regularization Change: 0.000 -> 0.154
2024-12-02-12:56:21-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-12:56:21-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-12:56:21-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-12:56:21-root-INFO: grad norm: 216.171 211.720 43.641
2024-12-02-12:56:21-root-INFO: Loss too large (1353.908->1533.861)! Learning rate decreased to 0.00329.
2024-12-02-12:56:22-root-INFO: Loss too large (1353.908->1501.517)! Learning rate decreased to 0.00263.
2024-12-02-12:56:22-root-INFO: Loss too large (1353.908->1466.715)! Learning rate decreased to 0.00211.
2024-12-02-12:56:22-root-INFO: Loss too large (1353.908->1432.086)! Learning rate decreased to 0.00168.
2024-12-02-12:56:22-root-INFO: Loss too large (1353.908->1401.184)! Learning rate decreased to 0.00135.
2024-12-02-12:56:22-root-INFO: Loss too large (1353.908->1376.929)! Learning rate decreased to 0.00108.
2024-12-02-12:56:22-root-INFO: Loss too large (1353.908->1360.235)! Learning rate decreased to 0.00086.
2024-12-02-12:56:23-root-INFO: grad norm: 209.370 203.913 47.491
2024-12-02-12:56:23-root-INFO: grad norm: 201.251 197.075 40.782
2024-12-02-12:56:24-root-INFO: grad norm: 192.783 187.736 43.824
2024-12-02-12:56:24-root-INFO: grad norm: 184.400 180.515 37.656
2024-12-02-12:56:25-root-INFO: grad norm: 176.245 171.611 40.150
2024-12-02-12:56:25-root-INFO: grad norm: 168.696 165.074 34.766
2024-12-02-12:56:26-root-INFO: grad norm: 161.526 157.257 36.889
2024-12-02-12:56:26-root-INFO: Loss Change: 1353.908 -> 1329.282
2024-12-02-12:56:26-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-02-12:56:26-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-12:56:26-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-12:56:26-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-12:56:26-root-INFO: grad norm: 333.361 326.450 67.528
2024-12-02-12:56:27-root-INFO: Loss too large (1356.126->1567.441)! Learning rate decreased to 0.00342.
2024-12-02-12:56:27-root-INFO: Loss too large (1356.126->1538.235)! Learning rate decreased to 0.00274.
2024-12-02-12:56:27-root-INFO: Loss too large (1356.126->1504.146)! Learning rate decreased to 0.00219.
2024-12-02-12:56:27-root-INFO: Loss too large (1356.126->1465.684)! Learning rate decreased to 0.00175.
2024-12-02-12:56:27-root-INFO: Loss too large (1356.126->1425.038)! Learning rate decreased to 0.00140.
2024-12-02-12:56:27-root-INFO: Loss too large (1356.126->1386.658)! Learning rate decreased to 0.00112.
2024-12-02-12:56:28-root-INFO: grad norm: 342.345 332.987 79.494
2024-12-02-12:56:28-root-INFO: grad norm: 324.315 317.837 64.497
2024-12-02-12:56:29-root-INFO: grad norm: 306.224 297.760 71.500
2024-12-02-12:56:29-root-INFO: grad norm: 285.794 279.877 57.857
2024-12-02-12:56:30-root-INFO: grad norm: 268.989 261.517 62.959
2024-12-02-12:56:30-root-INFO: grad norm: 253.831 248.394 52.256
2024-12-02-12:56:31-root-INFO: grad norm: 240.415 233.714 56.367
2024-12-02-12:56:31-root-INFO: Loss Change: 1356.126 -> 1313.216
2024-12-02-12:56:31-root-INFO: Regularization Change: 0.000 -> 0.284
2024-12-02-12:56:31-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-12:56:31-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-12:56:31-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-12:56:31-root-INFO: grad norm: 286.175 280.343 57.478
2024-12-02-12:56:31-root-INFO: Loss too large (1323.628->1523.487)! Learning rate decreased to 0.00356.
2024-12-02-12:56:32-root-INFO: Loss too large (1323.628->1490.145)! Learning rate decreased to 0.00285.
2024-12-02-12:56:32-root-INFO: Loss too large (1323.628->1450.965)! Learning rate decreased to 0.00228.
2024-12-02-12:56:32-root-INFO: Loss too large (1323.628->1408.402)! Learning rate decreased to 0.00182.
2024-12-02-12:56:32-root-INFO: Loss too large (1323.628->1367.147)! Learning rate decreased to 0.00146.
2024-12-02-12:56:32-root-INFO: Loss too large (1323.628->1333.250)! Learning rate decreased to 0.00117.
2024-12-02-12:56:33-root-INFO: grad norm: 253.201 246.099 59.550
2024-12-02-12:56:33-root-INFO: grad norm: 234.652 229.552 48.656
2024-12-02-12:56:34-root-INFO: grad norm: 218.236 212.099 51.387
2024-12-02-12:56:34-root-INFO: grad norm: 207.425 202.768 43.705
2024-12-02-12:56:35-root-INFO: grad norm: 197.205 191.645 46.497
2024-12-02-12:56:35-root-INFO: grad norm: 190.174 185.808 40.517
2024-12-02-12:56:36-root-INFO: grad norm: 183.464 178.283 43.289
2024-12-02-12:56:36-root-INFO: Loss Change: 1323.628 -> 1281.846
2024-12-02-12:56:36-root-INFO: Regularization Change: 0.000 -> 0.229
2024-12-02-12:56:36-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-12:56:36-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-12:56:36-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-12:56:36-root-INFO: grad norm: 267.089 261.638 53.684
2024-12-02-12:56:36-root-INFO: Loss too large (1295.172->1493.811)! Learning rate decreased to 0.00371.
2024-12-02-12:56:37-root-INFO: Loss too large (1295.172->1458.055)! Learning rate decreased to 0.00297.
2024-12-02-12:56:37-root-INFO: Loss too large (1295.172->1416.602)! Learning rate decreased to 0.00237.
2024-12-02-12:56:37-root-INFO: Loss too large (1295.172->1372.408)! Learning rate decreased to 0.00190.
2024-12-02-12:56:37-root-INFO: Loss too large (1295.172->1331.144)! Learning rate decreased to 0.00152.
2024-12-02-12:56:37-root-INFO: Loss too large (1295.172->1299.086)! Learning rate decreased to 0.00122.
2024-12-02-12:56:38-root-INFO: grad norm: 225.491 218.723 54.831
2024-12-02-12:56:38-root-INFO: grad norm: 208.190 203.541 43.750
2024-12-02-12:56:39-root-INFO: grad norm: 191.034 185.313 46.402
2024-12-02-12:56:39-root-INFO: grad norm: 181.659 177.446 38.894
2024-12-02-12:56:40-root-INFO: grad norm: 172.207 167.059 41.791
2024-12-02-12:56:40-root-INFO: grad norm: 166.624 162.667 36.095
2024-12-02-12:56:41-root-INFO: grad norm: 161.097 156.298 39.030
2024-12-02-12:56:41-root-INFO: Loss Change: 1295.172 -> 1252.817
2024-12-02-12:56:41-root-INFO: Regularization Change: 0.000 -> 0.235
2024-12-02-12:56:41-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-12:56:41-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-12:56:41-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-12:56:41-root-INFO: grad norm: 240.065 234.799 50.007
2024-12-02-12:56:41-root-INFO: Loss too large (1266.222->1462.104)! Learning rate decreased to 0.00386.
2024-12-02-12:56:41-root-INFO: Loss too large (1266.222->1423.999)! Learning rate decreased to 0.00309.
2024-12-02-12:56:42-root-INFO: Loss too large (1266.222->1381.326)! Learning rate decreased to 0.00247.
2024-12-02-12:56:42-root-INFO: Loss too large (1266.222->1337.707)! Learning rate decreased to 0.00198.
2024-12-02-12:56:42-root-INFO: Loss too large (1266.222->1299.057)! Learning rate decreased to 0.00158.
2024-12-02-12:56:42-root-INFO: Loss too large (1266.222->1270.507)! Learning rate decreased to 0.00126.
2024-12-02-12:56:43-root-INFO: grad norm: 214.895 208.669 51.353
2024-12-02-12:56:43-root-INFO: grad norm: 201.326 196.775 42.564
2024-12-02-12:56:43-root-INFO: grad norm: 186.250 180.846 44.537
2024-12-02-12:56:44-root-INFO: grad norm: 177.780 173.670 38.007
2024-12-02-12:56:44-root-INFO: grad norm: 168.329 163.457 40.202
2024-12-02-12:56:45-root-INFO: grad norm: 162.757 158.926 35.106
2024-12-02-12:56:45-root-INFO: grad norm: 156.683 152.169 37.341
2024-12-02-12:56:46-root-INFO: Loss Change: 1266.222 -> 1227.171
2024-12-02-12:56:46-root-INFO: Regularization Change: 0.000 -> 0.260
2024-12-02-12:56:46-root-INFO: Undo step: 170
2024-12-02-12:56:46-root-INFO: Undo step: 171
2024-12-02-12:56:46-root-INFO: Undo step: 172
2024-12-02-12:56:46-root-INFO: Undo step: 173
2024-12-02-12:56:46-root-INFO: Undo step: 174
2024-12-02-12:56:46-root-INFO: Undo step: 175
2024-12-02-12:56:46-root-INFO: Undo step: 176
2024-12-02-12:56:46-root-INFO: Undo step: 177
2024-12-02-12:56:46-root-INFO: Undo step: 178
2024-12-02-12:56:46-root-INFO: Undo step: 179
2024-12-02-12:56:46-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-12:56:46-root-INFO: grad norm: 825.740 788.101 246.462
2024-12-02-12:56:46-root-INFO: grad norm: 577.273 554.164 161.697
2024-12-02-12:56:47-root-INFO: grad norm: 540.607 515.930 161.471
2024-12-02-12:56:47-root-INFO: grad norm: 403.924 378.851 140.094
2024-12-02-12:56:48-root-INFO: grad norm: 304.489 286.918 101.939
2024-12-02-12:56:48-root-INFO: grad norm: 310.436 290.135 110.420
2024-12-02-12:56:49-root-INFO: grad norm: 394.602 376.590 117.856
2024-12-02-12:56:49-root-INFO: Loss too large (2001.739->2040.059)! Learning rate decreased to 0.00258.
2024-12-02-12:56:49-root-INFO: grad norm: 371.547 353.935 113.038
2024-12-02-12:56:50-root-INFO: Loss Change: 3400.440 -> 1905.444
2024-12-02-12:56:50-root-INFO: Regularization Change: 0.000 -> 19.279
2024-12-02-12:56:50-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-12:56:50-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-12:56:50-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-12:56:50-root-INFO: grad norm: 353.049 340.956 91.608
2024-12-02-12:56:50-root-INFO: Loss too large (1891.731->1912.770)! Learning rate decreased to 0.00269.
2024-12-02-12:56:51-root-INFO: grad norm: 338.259 323.534 98.715
2024-12-02-12:56:51-root-INFO: grad norm: 339.611 328.097 87.682
2024-12-02-12:56:52-root-INFO: grad norm: 372.428 357.656 103.847
2024-12-02-12:56:52-root-INFO: Loss too large (1755.720->1776.416)! Learning rate decreased to 0.00215.
2024-12-02-12:56:52-root-INFO: grad norm: 324.019 314.208 79.129
2024-12-02-12:56:53-root-INFO: grad norm: 296.068 285.403 78.748
2024-12-02-12:56:53-root-INFO: grad norm: 315.632 307.162 72.629
2024-12-02-12:56:54-root-INFO: Loss too large (1656.582->1662.464)! Learning rate decreased to 0.00172.
2024-12-02-12:56:54-root-INFO: grad norm: 252.004 244.364 61.580
2024-12-02-12:56:54-root-INFO: Loss Change: 1891.731 -> 1601.466
2024-12-02-12:56:54-root-INFO: Regularization Change: 0.000 -> 3.888
2024-12-02-12:56:54-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-12:56:54-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-12:56:55-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-12:56:55-root-INFO: grad norm: 310.778 303.582 66.490
2024-12-02-12:56:55-root-INFO: Loss too large (1605.830->1708.759)! Learning rate decreased to 0.00280.
2024-12-02-12:56:55-root-INFO: Loss too large (1605.830->1669.615)! Learning rate decreased to 0.00224.
2024-12-02-12:56:55-root-INFO: Loss too large (1605.830->1633.187)! Learning rate decreased to 0.00179.
2024-12-02-12:56:56-root-INFO: grad norm: 264.579 257.350 61.422
2024-12-02-12:56:56-root-INFO: grad norm: 224.946 219.323 49.982
2024-12-02-12:56:56-root-INFO: Loss too large (1559.783->1561.710)! Learning rate decreased to 0.00143.
2024-12-02-12:56:57-root-INFO: grad norm: 201.787 196.953 43.904
2024-12-02-12:56:57-root-INFO: grad norm: 196.279 191.634 42.447
2024-12-02-12:56:58-root-INFO: grad norm: 205.396 200.649 43.905
2024-12-02-12:56:58-root-INFO: grad norm: 223.354 218.510 46.261
2024-12-02-12:56:59-root-INFO: grad norm: 237.036 231.637 50.306
2024-12-02-12:56:59-root-INFO: Loss Change: 1605.830 -> 1512.906
2024-12-02-12:56:59-root-INFO: Regularization Change: 0.000 -> 0.893
2024-12-02-12:56:59-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-12:56:59-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-12:56:59-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-12:56:59-root-INFO: grad norm: 327.700 321.471 63.588
2024-12-02-12:57:00-root-INFO: Loss too large (1522.596->1661.870)! Learning rate decreased to 0.00291.
2024-12-02-12:57:00-root-INFO: Loss too large (1522.596->1625.120)! Learning rate decreased to 0.00233.
2024-12-02-12:57:00-root-INFO: Loss too large (1522.596->1587.037)! Learning rate decreased to 0.00187.
2024-12-02-12:57:00-root-INFO: Loss too large (1522.596->1550.165)! Learning rate decreased to 0.00149.
2024-12-02-12:57:01-root-INFO: grad norm: 287.175 280.315 62.398
2024-12-02-12:57:01-root-INFO: grad norm: 255.515 250.188 51.902
2024-12-02-12:57:01-root-INFO: Loss too large (1490.746->1492.215)! Learning rate decreased to 0.00119.
2024-12-02-12:57:02-root-INFO: grad norm: 202.886 198.410 42.385
2024-12-02-12:57:02-root-INFO: grad norm: 176.744 172.697 37.603
2024-12-02-12:57:03-root-INFO: grad norm: 168.624 164.921 35.143
2024-12-02-12:57:03-root-INFO: grad norm: 166.334 162.560 35.232
2024-12-02-12:57:04-root-INFO: grad norm: 169.203 165.547 34.985
2024-12-02-12:57:04-root-INFO: Loss Change: 1522.596 -> 1446.639
2024-12-02-12:57:04-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-02-12:57:04-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-12:57:04-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-12:57:04-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-12:57:04-root-INFO: grad norm: 288.954 283.318 56.795
2024-12-02-12:57:04-root-INFO: Loss too large (1457.360->1613.701)! Learning rate decreased to 0.00304.
2024-12-02-12:57:04-root-INFO: Loss too large (1457.360->1577.671)! Learning rate decreased to 0.00243.
2024-12-02-12:57:05-root-INFO: Loss too large (1457.360->1539.870)! Learning rate decreased to 0.00194.
2024-12-02-12:57:05-root-INFO: Loss too large (1457.360->1502.843)! Learning rate decreased to 0.00155.
2024-12-02-12:57:05-root-INFO: Loss too large (1457.360->1470.624)! Learning rate decreased to 0.00124.
2024-12-02-12:57:05-root-INFO: grad norm: 264.089 258.705 53.054
2024-12-02-12:57:06-root-INFO: grad norm: 248.804 243.981 48.751
2024-12-02-12:57:06-root-INFO: grad norm: 242.499 237.538 48.799
2024-12-02-12:57:07-root-INFO: grad norm: 237.793 233.250 46.263
2024-12-02-12:57:07-root-INFO: grad norm: 235.571 230.714 47.587
2024-12-02-12:57:08-root-INFO: grad norm: 233.729 229.289 45.339
2024-12-02-12:57:08-root-INFO: grad norm: 233.055 228.225 47.205
2024-12-02-12:57:08-root-INFO: Loss Change: 1457.360 -> 1411.234
2024-12-02-12:57:08-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-02-12:57:08-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-12:57:09-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-12:57:09-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-12:57:09-root-INFO: grad norm: 317.406 311.812 59.330
2024-12-02-12:57:09-root-INFO: Loss too large (1424.823->1591.244)! Learning rate decreased to 0.00316.
2024-12-02-12:57:09-root-INFO: Loss too large (1424.823->1554.458)! Learning rate decreased to 0.00253.
2024-12-02-12:57:09-root-INFO: Loss too large (1424.823->1514.302)! Learning rate decreased to 0.00202.
2024-12-02-12:57:10-root-INFO: Loss too large (1424.823->1472.998)! Learning rate decreased to 0.00162.
2024-12-02-12:57:10-root-INFO: Loss too large (1424.823->1435.065)! Learning rate decreased to 0.00129.
2024-12-02-12:57:10-root-INFO: grad norm: 275.161 269.422 55.903
2024-12-02-12:57:11-root-INFO: grad norm: 257.982 253.256 49.158
2024-12-02-12:57:11-root-INFO: grad norm: 248.759 243.525 50.764
2024-12-02-12:57:12-root-INFO: grad norm: 242.309 237.869 46.172
2024-12-02-12:57:12-root-INFO: grad norm: 237.616 232.579 48.665
2024-12-02-12:57:13-root-INFO: grad norm: 234.022 229.699 44.776
2024-12-02-12:57:13-root-INFO: grad norm: 231.305 226.384 47.457
2024-12-02-12:57:13-root-INFO: Loss Change: 1424.823 -> 1372.969
2024-12-02-12:57:13-root-INFO: Regularization Change: 0.000 -> 0.329
2024-12-02-12:57:13-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-12:57:13-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-12:57:14-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-12:57:14-root-INFO: grad norm: 263.659 259.053 49.069
2024-12-02-12:57:14-root-INFO: Loss too large (1376.404->1544.340)! Learning rate decreased to 0.00329.
2024-12-02-12:57:14-root-INFO: Loss too large (1376.404->1506.882)! Learning rate decreased to 0.00263.
2024-12-02-12:57:14-root-INFO: Loss too large (1376.404->1466.752)! Learning rate decreased to 0.00211.
2024-12-02-12:57:14-root-INFO: Loss too large (1376.404->1427.139)! Learning rate decreased to 0.00168.
2024-12-02-12:57:15-root-INFO: Loss too large (1376.404->1393.053)! Learning rate decreased to 0.00135.
2024-12-02-12:57:15-root-INFO: grad norm: 261.034 255.292 54.450
2024-12-02-12:57:15-root-INFO: grad norm: 255.260 250.781 47.604
2024-12-02-12:57:16-root-INFO: grad norm: 248.391 242.879 52.039
2024-12-02-12:57:17-root-INFO: grad norm: 243.165 238.780 45.972
2024-12-02-12:57:17-root-INFO: grad norm: 237.959 232.657 49.954
2024-12-02-12:57:17-root-INFO: grad norm: 234.343 230.009 44.863
2024-12-02-12:57:18-root-INFO: grad norm: 230.934 225.774 48.546
2024-12-02-12:57:18-root-INFO: Loss Change: 1376.404 -> 1343.594
2024-12-02-12:57:18-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-02-12:57:18-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-12:57:18-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-12:57:18-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-12:57:19-root-INFO: grad norm: 317.528 311.439 61.889
2024-12-02-12:57:19-root-INFO: Loss too large (1369.797->1536.505)! Learning rate decreased to 0.00342.
2024-12-02-12:57:19-root-INFO: Loss too large (1369.797->1497.093)! Learning rate decreased to 0.00274.
2024-12-02-12:57:19-root-INFO: Loss too large (1369.797->1452.621)! Learning rate decreased to 0.00219.
2024-12-02-12:57:19-root-INFO: Loss too large (1369.797->1406.029)! Learning rate decreased to 0.00175.
2024-12-02-12:57:20-root-INFO: grad norm: 323.958 315.278 74.490
2024-12-02-12:57:20-root-INFO: grad norm: 304.809 298.955 59.451
2024-12-02-12:57:21-root-INFO: grad norm: 291.866 283.755 68.330
2024-12-02-12:57:21-root-INFO: Loss too large (1343.651->1345.423)! Learning rate decreased to 0.00140.
2024-12-02-12:57:21-root-INFO: grad norm: 210.678 205.667 45.676
2024-12-02-12:57:22-root-INFO: grad norm: 178.696 174.425 38.836
2024-12-02-12:57:22-root-INFO: grad norm: 167.554 163.298 37.524
2024-12-02-12:57:23-root-INFO: grad norm: 160.642 156.916 34.401
2024-12-02-12:57:23-root-INFO: Loss Change: 1369.797 -> 1299.778
2024-12-02-12:57:23-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-02-12:57:23-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-12:57:23-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-12:57:23-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-12:57:23-root-INFO: grad norm: 200.129 195.332 43.554
2024-12-02-12:57:24-root-INFO: Loss too large (1302.167->1451.896)! Learning rate decreased to 0.00356.
2024-12-02-12:57:24-root-INFO: Loss too large (1302.167->1410.606)! Learning rate decreased to 0.00285.
2024-12-02-12:57:24-root-INFO: Loss too large (1302.167->1369.891)! Learning rate decreased to 0.00228.
2024-12-02-12:57:24-root-INFO: Loss too large (1302.167->1334.885)! Learning rate decreased to 0.00182.
2024-12-02-12:57:24-root-INFO: Loss too large (1302.167->1309.486)! Learning rate decreased to 0.00146.
2024-12-02-12:57:25-root-INFO: grad norm: 192.632 188.029 41.860
2024-12-02-12:57:25-root-INFO: grad norm: 189.376 184.932 40.784
2024-12-02-12:57:26-root-INFO: grad norm: 186.841 182.363 40.660
2024-12-02-12:57:26-root-INFO: grad norm: 185.885 181.549 39.916
2024-12-02-12:57:27-root-INFO: grad norm: 185.653 181.188 40.472
2024-12-02-12:57:27-root-INFO: Loss too large (1281.459->1281.756)! Learning rate decreased to 0.00117.
2024-12-02-12:57:27-root-INFO: grad norm: 131.243 127.716 30.223
2024-12-02-12:57:28-root-INFO: grad norm: 93.692 91.702 19.207
2024-12-02-12:57:28-root-INFO: Loss Change: 1302.167 -> 1265.328
2024-12-02-12:57:28-root-INFO: Regularization Change: 0.000 -> 0.244
2024-12-02-12:57:28-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-12:57:28-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-12:57:28-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-12:57:28-root-INFO: grad norm: 165.937 161.816 36.749
2024-12-02-12:57:29-root-INFO: Loss too large (1267.419->1406.116)! Learning rate decreased to 0.00371.
2024-12-02-12:57:29-root-INFO: Loss too large (1267.419->1365.529)! Learning rate decreased to 0.00297.
2024-12-02-12:57:29-root-INFO: Loss too large (1267.419->1327.693)! Learning rate decreased to 0.00237.
2024-12-02-12:57:29-root-INFO: Loss too large (1267.419->1297.242)! Learning rate decreased to 0.00190.
2024-12-02-12:57:29-root-INFO: Loss too large (1267.419->1276.308)! Learning rate decreased to 0.00152.
2024-12-02-12:57:30-root-INFO: grad norm: 187.788 183.072 41.820
2024-12-02-12:57:30-root-INFO: Loss too large (1263.947->1266.815)! Learning rate decreased to 0.00122.
2024-12-02-12:57:30-root-INFO: grad norm: 144.983 141.337 32.309
2024-12-02-12:57:31-root-INFO: grad norm: 109.145 106.635 23.274
2024-12-02-12:57:31-root-INFO: grad norm: 93.374 90.625 22.493
2024-12-02-12:57:32-root-INFO: grad norm: 80.006 78.174 17.025
2024-12-02-12:57:32-root-INFO: grad norm: 72.303 69.970 18.221
2024-12-02-12:57:33-root-INFO: grad norm: 65.889 64.334 14.228
2024-12-02-12:57:33-root-INFO: Loss Change: 1267.419 -> 1237.993
2024-12-02-12:57:33-root-INFO: Regularization Change: 0.000 -> 0.203
2024-12-02-12:57:33-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-12:57:33-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-12:57:33-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-12:57:33-root-INFO: grad norm: 150.729 146.714 34.557
2024-12-02-12:57:34-root-INFO: Loss too large (1242.962->1378.874)! Learning rate decreased to 0.00386.
2024-12-02-12:57:34-root-INFO: Loss too large (1242.962->1339.735)! Learning rate decreased to 0.00309.
2024-12-02-12:57:34-root-INFO: Loss too large (1242.962->1303.721)! Learning rate decreased to 0.00247.
2024-12-02-12:57:34-root-INFO: Loss too large (1242.962->1274.908)! Learning rate decreased to 0.00198.
2024-12-02-12:57:34-root-INFO: Loss too large (1242.962->1254.985)! Learning rate decreased to 0.00158.
2024-12-02-12:57:34-root-INFO: Loss too large (1242.962->1242.969)! Learning rate decreased to 0.00126.
2024-12-02-12:57:35-root-INFO: grad norm: 140.413 137.301 29.398
2024-12-02-12:57:35-root-INFO: grad norm: 134.483 131.058 30.155
2024-12-02-12:57:36-root-INFO: grad norm: 128.582 125.778 26.706
2024-12-02-12:57:36-root-INFO: grad norm: 125.156 122.039 27.759
2024-12-02-12:57:37-root-INFO: grad norm: 121.712 119.106 25.051
2024-12-02-12:57:37-root-INFO: grad norm: 119.858 116.909 26.427
2024-12-02-12:57:38-root-INFO: grad norm: 118.257 115.766 24.144
2024-12-02-12:57:38-root-INFO: Loss Change: 1242.962 -> 1218.542
2024-12-02-12:57:38-root-INFO: Regularization Change: 0.000 -> 0.211
2024-12-02-12:57:38-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-12:57:38-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-12:57:38-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-12:57:38-root-INFO: grad norm: 180.603 176.596 37.834
2024-12-02-12:57:38-root-INFO: Loss too large (1225.287->1399.425)! Learning rate decreased to 0.00401.
2024-12-02-12:57:39-root-INFO: Loss too large (1225.287->1358.886)! Learning rate decreased to 0.00321.
2024-12-02-12:57:39-root-INFO: Loss too large (1225.287->1316.229)! Learning rate decreased to 0.00257.
2024-12-02-12:57:39-root-INFO: Loss too large (1225.287->1277.058)! Learning rate decreased to 0.00206.
2024-12-02-12:57:39-root-INFO: Loss too large (1225.287->1246.766)! Learning rate decreased to 0.00164.
2024-12-02-12:57:39-root-INFO: Loss too large (1225.287->1227.163)! Learning rate decreased to 0.00132.
2024-12-02-12:57:40-root-INFO: grad norm: 162.380 158.713 34.316
2024-12-02-12:57:40-root-INFO: grad norm: 154.862 151.278 33.124
2024-12-02-12:57:41-root-INFO: grad norm: 145.522 142.320 30.361
2024-12-02-12:57:41-root-INFO: grad norm: 141.097 137.756 30.526
2024-12-02-12:57:42-root-INFO: grad norm: 135.793 132.864 28.050
2024-12-02-12:57:42-root-INFO: grad norm: 133.251 130.054 29.011
2024-12-02-12:57:42-root-INFO: grad norm: 130.491 127.717 26.762
2024-12-02-12:57:43-root-INFO: Loss Change: 1225.287 -> 1199.014
2024-12-02-12:57:43-root-INFO: Regularization Change: 0.000 -> 0.203
2024-12-02-12:57:43-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-12:57:43-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-12:57:43-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-12:57:43-root-INFO: grad norm: 195.922 191.855 39.712
2024-12-02-12:57:43-root-INFO: Loss too large (1210.054->1401.077)! Learning rate decreased to 0.00417.
2024-12-02-12:57:43-root-INFO: Loss too large (1210.054->1360.735)! Learning rate decreased to 0.00334.
2024-12-02-12:57:44-root-INFO: Loss too large (1210.054->1315.410)! Learning rate decreased to 0.00267.
2024-12-02-12:57:44-root-INFO: Loss too large (1210.054->1270.736)! Learning rate decreased to 0.00214.
2024-12-02-12:57:44-root-INFO: Loss too large (1210.054->1234.271)! Learning rate decreased to 0.00171.
2024-12-02-12:57:44-root-INFO: Loss too large (1210.054->1210.152)! Learning rate decreased to 0.00137.
2024-12-02-12:57:45-root-INFO: grad norm: 162.891 159.011 35.340
2024-12-02-12:57:45-root-INFO: grad norm: 154.772 151.171 33.190
2024-12-02-12:57:45-root-INFO: grad norm: 143.698 140.374 30.726
2024-12-02-12:57:46-root-INFO: grad norm: 139.326 135.937 30.544
2024-12-02-12:57:46-root-INFO: grad norm: 133.747 130.717 28.309
2024-12-02-12:57:47-root-INFO: grad norm: 131.438 128.172 29.118
2024-12-02-12:57:47-root-INFO: grad norm: 128.824 125.944 27.089
2024-12-02-12:57:48-root-INFO: Loss Change: 1210.054 -> 1180.645
2024-12-02-12:57:48-root-INFO: Regularization Change: 0.000 -> 0.200
2024-12-02-12:57:48-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-12:57:48-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-12:57:48-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-12:57:48-root-INFO: grad norm: 194.225 189.680 41.769
2024-12-02-12:57:48-root-INFO: Loss too large (1192.063->1385.898)! Learning rate decreased to 0.00434.
2024-12-02-12:57:48-root-INFO: Loss too large (1192.063->1345.318)! Learning rate decreased to 0.00347.
2024-12-02-12:57:49-root-INFO: Loss too large (1192.063->1299.459)! Learning rate decreased to 0.00278.
2024-12-02-12:57:49-root-INFO: Loss too large (1192.063->1254.031)! Learning rate decreased to 0.00222.
2024-12-02-12:57:49-root-INFO: Loss too large (1192.063->1216.826)! Learning rate decreased to 0.00178.
2024-12-02-12:57:49-root-INFO: Loss too large (1192.063->1192.171)! Learning rate decreased to 0.00142.
2024-12-02-12:57:49-root-INFO: grad norm: 168.619 164.546 36.840
2024-12-02-12:57:50-root-INFO: Loss too large (1178.939->1179.038)! Learning rate decreased to 0.00114.
2024-12-02-12:57:50-root-INFO: grad norm: 118.465 114.942 28.675
2024-12-02-12:57:51-root-INFO: grad norm: 76.118 74.342 16.349
2024-12-02-12:57:51-root-INFO: grad norm: 60.996 58.458 17.411
2024-12-02-12:57:52-root-INFO: grad norm: 50.506 49.036 12.096
2024-12-02-12:57:52-root-INFO: grad norm: 45.406 43.306 13.651
2024-12-02-12:57:52-root-INFO: grad norm: 42.259 40.783 11.072
2024-12-02-12:57:53-root-INFO: Loss Change: 1192.063 -> 1157.105
2024-12-02-12:57:53-root-INFO: Regularization Change: 0.000 -> 0.188
2024-12-02-12:57:53-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-12:57:53-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-12:57:53-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-12:57:53-root-INFO: grad norm: 122.037 118.437 29.425
2024-12-02-12:57:53-root-INFO: Loss too large (1159.953->1297.343)! Learning rate decreased to 0.00451.
2024-12-02-12:57:54-root-INFO: Loss too large (1159.953->1255.856)! Learning rate decreased to 0.00361.
2024-12-02-12:57:54-root-INFO: Loss too large (1159.953->1218.973)! Learning rate decreased to 0.00289.
2024-12-02-12:57:54-root-INFO: Loss too large (1159.953->1190.864)! Learning rate decreased to 0.00231.
2024-12-02-12:57:54-root-INFO: Loss too large (1159.953->1172.319)! Learning rate decreased to 0.00185.
2024-12-02-12:57:54-root-INFO: Loss too large (1159.953->1161.507)! Learning rate decreased to 0.00148.
2024-12-02-12:57:55-root-INFO: grad norm: 133.867 130.844 28.290
2024-12-02-12:57:55-root-INFO: Loss too large (1155.911->1155.927)! Learning rate decreased to 0.00118.
2024-12-02-12:57:55-root-INFO: grad norm: 103.804 100.698 25.202
2024-12-02-12:57:56-root-INFO: grad norm: 75.629 73.865 16.238
2024-12-02-12:57:56-root-INFO: grad norm: 63.474 61.034 17.429
2024-12-02-12:57:57-root-INFO: grad norm: 53.670 52.192 12.508
2024-12-02-12:57:57-root-INFO: grad norm: 48.339 46.245 14.074
2024-12-02-12:57:58-root-INFO: grad norm: 44.442 43.008 11.200
2024-12-02-12:57:58-root-INFO: Loss Change: 1159.953 -> 1137.180
2024-12-02-12:57:58-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-02-12:57:58-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-12:57:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-12:57:58-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-12:57:58-root-INFO: grad norm: 89.102 86.352 21.965
2024-12-02-12:57:59-root-INFO: Loss too large (1140.403->1227.509)! Learning rate decreased to 0.00469.
2024-12-02-12:57:59-root-INFO: Loss too large (1140.403->1194.973)! Learning rate decreased to 0.00375.
2024-12-02-12:57:59-root-INFO: Loss too large (1140.403->1170.808)! Learning rate decreased to 0.00300.
2024-12-02-12:57:59-root-INFO: Loss too large (1140.403->1154.885)! Learning rate decreased to 0.00240.
2024-12-02-12:57:59-root-INFO: Loss too large (1140.403->1145.362)! Learning rate decreased to 0.00192.
2024-12-02-12:58:00-root-INFO: grad norm: 150.606 147.189 31.901
2024-12-02-12:58:00-root-INFO: Loss too large (1140.135->1151.321)! Learning rate decreased to 0.00154.
2024-12-02-12:58:00-root-INFO: Loss too large (1140.135->1142.423)! Learning rate decreased to 0.00123.
2024-12-02-12:58:00-root-INFO: grad norm: 119.694 116.687 26.661
2024-12-02-12:58:01-root-INFO: grad norm: 86.288 84.424 17.838
2024-12-02-12:58:02-root-INFO: grad norm: 73.363 71.016 18.407
2024-12-02-12:58:02-root-INFO: grad norm: 61.287 59.875 13.079
2024-12-02-12:58:03-root-INFO: grad norm: 54.705 52.692 14.700
2024-12-02-12:58:03-root-INFO: grad norm: 49.133 47.860 11.115
2024-12-02-12:58:03-root-INFO: Loss Change: 1140.403 -> 1120.633
2024-12-02-12:58:03-root-INFO: Regularization Change: 0.000 -> 0.186
2024-12-02-12:58:03-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-12:58:03-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-12:58:04-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-12:58:04-root-INFO: grad norm: 130.353 126.788 30.279
2024-12-02-12:58:04-root-INFO: Loss too large (1126.709->1292.713)! Learning rate decreased to 0.00488.
2024-12-02-12:58:04-root-INFO: Loss too large (1126.709->1249.703)! Learning rate decreased to 0.00390.
2024-12-02-12:58:04-root-INFO: Loss too large (1126.709->1207.410)! Learning rate decreased to 0.00312.
2024-12-02-12:58:04-root-INFO: Loss too large (1126.709->1171.966)! Learning rate decreased to 0.00250.
2024-12-02-12:58:04-root-INFO: Loss too large (1126.709->1146.845)! Learning rate decreased to 0.00200.
2024-12-02-12:58:05-root-INFO: Loss too large (1126.709->1131.508)! Learning rate decreased to 0.00160.
2024-12-02-12:58:05-root-INFO: grad norm: 156.471 152.801 33.687
2024-12-02-12:58:05-root-INFO: Loss too large (1123.301->1126.569)! Learning rate decreased to 0.00128.
2024-12-02-12:58:06-root-INFO: grad norm: 126.121 122.828 28.633
2024-12-02-12:58:06-root-INFO: grad norm: 91.183 89.151 19.145
2024-12-02-12:58:07-root-INFO: grad norm: 78.614 76.025 20.008
2024-12-02-12:58:07-root-INFO: grad norm: 66.005 64.469 14.157
2024-12-02-12:58:08-root-INFO: grad norm: 59.195 56.980 16.040
2024-12-02-12:58:08-root-INFO: grad norm: 52.953 51.608 11.859
2024-12-02-12:58:09-root-INFO: Loss Change: 1126.709 -> 1102.652
2024-12-02-12:58:09-root-INFO: Regularization Change: 0.000 -> 0.193
2024-12-02-12:58:09-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-12:58:09-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-12:58:09-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-12:58:09-root-INFO: grad norm: 109.795 106.945 24.853
2024-12-02-12:58:09-root-INFO: Loss too large (1104.000->1250.940)! Learning rate decreased to 0.00507.
2024-12-02-12:58:09-root-INFO: Loss too large (1104.000->1208.651)! Learning rate decreased to 0.00405.
2024-12-02-12:58:09-root-INFO: Loss too large (1104.000->1169.943)! Learning rate decreased to 0.00324.
2024-12-02-12:58:10-root-INFO: Loss too large (1104.000->1139.879)! Learning rate decreased to 0.00259.
2024-12-02-12:58:10-root-INFO: Loss too large (1104.000->1119.825)! Learning rate decreased to 0.00208.
2024-12-02-12:58:10-root-INFO: Loss too large (1104.000->1108.024)! Learning rate decreased to 0.00166.
2024-12-02-12:58:10-root-INFO: grad norm: 137.866 134.927 28.315
2024-12-02-12:58:11-root-INFO: Loss too large (1101.802->1104.562)! Learning rate decreased to 0.00133.
2024-12-02-12:58:11-root-INFO: grad norm: 114.368 111.563 25.173
2024-12-02-12:58:12-root-INFO: grad norm: 86.392 84.614 17.437
2024-12-02-12:58:12-root-INFO: grad norm: 75.791 73.520 18.414
2024-12-02-12:58:13-root-INFO: grad norm: 64.717 63.322 13.361
2024-12-02-12:58:13-root-INFO: grad norm: 58.733 56.749 15.134
2024-12-02-12:58:14-root-INFO: grad norm: 52.988 51.746 11.405
2024-12-02-12:58:14-root-INFO: Loss Change: 1104.000 -> 1083.451
2024-12-02-12:58:14-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-02-12:58:14-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-12:58:14-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-12:58:14-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-12:58:14-root-INFO: grad norm: 131.680 127.908 31.291
2024-12-02-12:58:14-root-INFO: Loss too large (1096.345->1272.300)! Learning rate decreased to 0.00527.
2024-12-02-12:58:15-root-INFO: Loss too large (1096.345->1228.362)! Learning rate decreased to 0.00421.
2024-12-02-12:58:15-root-INFO: Loss too large (1096.345->1183.669)! Learning rate decreased to 0.00337.
2024-12-02-12:58:15-root-INFO: Loss too large (1096.345->1145.196)! Learning rate decreased to 0.00270.
2024-12-02-12:58:15-root-INFO: Loss too large (1096.345->1117.550)! Learning rate decreased to 0.00216.
2024-12-02-12:58:15-root-INFO: Loss too large (1096.345->1100.694)! Learning rate decreased to 0.00173.
2024-12-02-12:58:16-root-INFO: grad norm: 160.018 156.374 33.955
2024-12-02-12:58:16-root-INFO: Loss too large (1091.806->1095.812)! Learning rate decreased to 0.00138.
2024-12-02-12:58:16-root-INFO: grad norm: 129.261 125.799 29.714
2024-12-02-12:58:17-root-INFO: grad norm: 90.575 88.594 18.842
2024-12-02-12:58:17-root-INFO: grad norm: 78.757 76.023 20.570
2024-12-02-12:58:18-root-INFO: grad norm: 66.186 64.655 14.155
2024-12-02-12:58:18-root-INFO: grad norm: 59.645 57.300 16.559
2024-12-02-12:58:19-root-INFO: grad norm: 53.348 51.994 11.946
2024-12-02-12:58:19-root-INFO: Loss Change: 1096.345 -> 1068.825
2024-12-02-12:58:19-root-INFO: Regularization Change: 0.000 -> 0.240
2024-12-02-12:58:19-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-12:58:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-12:58:19-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-12:58:19-root-INFO: grad norm: 111.534 108.564 25.570
2024-12-02-12:58:20-root-INFO: Loss too large (1072.089->1229.907)! Learning rate decreased to 0.00547.
2024-12-02-12:58:20-root-INFO: Loss too large (1072.089->1185.315)! Learning rate decreased to 0.00438.
2024-12-02-12:58:20-root-INFO: Loss too large (1072.089->1143.302)! Learning rate decreased to 0.00350.
2024-12-02-12:58:20-root-INFO: Loss too large (1072.089->1110.147)! Learning rate decreased to 0.00280.
2024-12-02-12:58:20-root-INFO: Loss too large (1072.089->1088.031)! Learning rate decreased to 0.00224.
2024-12-02-12:58:20-root-INFO: Loss too large (1072.089->1075.201)! Learning rate decreased to 0.00179.
2024-12-02-12:58:21-root-INFO: grad norm: 132.708 129.801 27.624
2024-12-02-12:58:21-root-INFO: Loss too large (1068.620->1070.722)! Learning rate decreased to 0.00143.
2024-12-02-12:58:21-root-INFO: grad norm: 106.815 104.030 24.233
2024-12-02-12:58:22-root-INFO: grad norm: 76.257 74.620 15.718
2024-12-02-12:58:22-root-INFO: grad norm: 65.930 63.732 16.884
2024-12-02-12:58:23-root-INFO: grad norm: 55.785 54.465 12.063
2024-12-02-12:58:23-root-INFO: grad norm: 50.517 48.612 13.742
2024-12-02-12:58:24-root-INFO: grad norm: 45.875 44.643 10.560
2024-12-02-12:58:24-root-INFO: Loss Change: 1072.089 -> 1049.123
2024-12-02-12:58:24-root-INFO: Regularization Change: 0.000 -> 0.215
2024-12-02-12:58:24-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-12:58:24-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-12:58:24-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-12:58:25-root-INFO: grad norm: 105.621 102.613 25.024
2024-12-02-12:58:25-root-INFO: Loss too large (1056.751->1208.799)! Learning rate decreased to 0.00568.
2024-12-02-12:58:25-root-INFO: Loss too large (1056.751->1163.367)! Learning rate decreased to 0.00455.
2024-12-02-12:58:25-root-INFO: Loss too large (1056.751->1122.032)! Learning rate decreased to 0.00364.
2024-12-02-12:58:25-root-INFO: Loss too large (1056.751->1090.588)! Learning rate decreased to 0.00291.
2024-12-02-12:58:25-root-INFO: Loss too large (1056.751->1070.259)! Learning rate decreased to 0.00233.
2024-12-02-12:58:25-root-INFO: Loss too large (1056.751->1058.749)! Learning rate decreased to 0.00186.
2024-12-02-12:58:26-root-INFO: grad norm: 123.753 121.047 25.738
2024-12-02-12:58:26-root-INFO: Loss too large (1052.965->1054.325)! Learning rate decreased to 0.00149.
2024-12-02-12:58:27-root-INFO: grad norm: 99.732 96.966 23.329
2024-12-02-12:58:27-root-INFO: grad norm: 71.592 70.006 14.988
2024-12-02-12:58:28-root-INFO: grad norm: 61.955 59.721 16.487
2024-12-02-12:58:28-root-INFO: grad norm: 52.691 51.362 11.760
2024-12-02-12:58:29-root-INFO: grad norm: 47.895 45.941 13.539
2024-12-02-12:58:29-root-INFO: grad norm: 43.787 42.514 10.482
2024-12-02-12:58:29-root-INFO: Loss Change: 1056.751 -> 1033.396
2024-12-02-12:58:29-root-INFO: Regularization Change: 0.000 -> 0.233
2024-12-02-12:58:29-root-INFO: Undo step: 160
2024-12-02-12:58:29-root-INFO: Undo step: 161
2024-12-02-12:58:29-root-INFO: Undo step: 162
2024-12-02-12:58:29-root-INFO: Undo step: 163
2024-12-02-12:58:29-root-INFO: Undo step: 164
2024-12-02-12:58:29-root-INFO: Undo step: 165
2024-12-02-12:58:29-root-INFO: Undo step: 166
2024-12-02-12:58:29-root-INFO: Undo step: 167
2024-12-02-12:58:29-root-INFO: Undo step: 168
2024-12-02-12:58:29-root-INFO: Undo step: 169
2024-12-02-12:58:30-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-12:58:30-root-INFO: grad norm: 743.522 721.959 177.762
2024-12-02-12:58:30-root-INFO: grad norm: 805.319 779.943 200.571
2024-12-02-12:58:31-root-INFO: grad norm: 395.491 383.362 97.192
2024-12-02-12:58:31-root-INFO: grad norm: 399.094 391.494 77.514
2024-12-02-12:58:32-root-INFO: grad norm: 412.467 408.505 57.034
2024-12-02-12:58:32-root-INFO: grad norm: 542.592 534.403 93.910
2024-12-02-12:58:32-root-INFO: Loss too large (1882.435->1893.233)! Learning rate decreased to 0.00386.
2024-12-02-12:58:33-root-INFO: grad norm: 271.908 264.975 61.013
2024-12-02-12:58:33-root-INFO: grad norm: 217.539 214.252 37.674
2024-12-02-12:58:34-root-INFO: Loss Change: 3231.525 -> 1512.427
2024-12-02-12:58:34-root-INFO: Regularization Change: 0.000 -> 33.601
2024-12-02-12:58:34-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-12:58:34-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-12:58:34-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-12:58:34-root-INFO: grad norm: 194.114 190.012 39.696
2024-12-02-12:58:34-root-INFO: Loss too large (1511.095->1550.928)! Learning rate decreased to 0.00401.
2024-12-02-12:58:34-root-INFO: Loss too large (1511.095->1513.185)! Learning rate decreased to 0.00321.
2024-12-02-12:58:35-root-INFO: grad norm: 271.551 266.953 49.763
2024-12-02-12:58:35-root-INFO: Loss too large (1489.058->1558.864)! Learning rate decreased to 0.00257.
2024-12-02-12:58:35-root-INFO: Loss too large (1489.058->1505.425)! Learning rate decreased to 0.00206.
2024-12-02-12:58:35-root-INFO: grad norm: 268.790 265.515 41.830
2024-12-02-12:58:36-root-INFO: grad norm: 279.262 275.092 48.076
2024-12-02-12:58:36-root-INFO: grad norm: 289.174 285.900 43.388
2024-12-02-12:58:37-root-INFO: grad norm: 298.731 294.501 50.089
2024-12-02-12:58:37-root-INFO: grad norm: 301.305 298.106 43.788
2024-12-02-12:58:38-root-INFO: grad norm: 306.447 302.212 50.771
2024-12-02-12:58:38-root-INFO: Loss Change: 1511.095 -> 1417.853
2024-12-02-12:58:38-root-INFO: Regularization Change: 0.000 -> 2.153
2024-12-02-12:58:38-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-12:58:38-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-12:58:38-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-12:58:38-root-INFO: grad norm: 328.845 325.543 46.480
2024-12-02-12:58:39-root-INFO: Loss too large (1434.701->1593.476)! Learning rate decreased to 0.00417.
2024-12-02-12:58:39-root-INFO: Loss too large (1434.701->1551.672)! Learning rate decreased to 0.00334.
2024-12-02-12:58:39-root-INFO: Loss too large (1434.701->1500.802)! Learning rate decreased to 0.00267.
2024-12-02-12:58:39-root-INFO: Loss too large (1434.701->1443.959)! Learning rate decreased to 0.00214.
2024-12-02-12:58:40-root-INFO: grad norm: 315.952 311.944 50.166
2024-12-02-12:58:40-root-INFO: Loss too large (1391.605->1406.300)! Learning rate decreased to 0.00171.
2024-12-02-12:58:40-root-INFO: grad norm: 256.650 254.160 35.662
2024-12-02-12:58:41-root-INFO: grad norm: 243.373 240.189 39.242
2024-12-02-12:58:41-root-INFO: Loss too large (1347.908->1350.949)! Learning rate decreased to 0.00137.
2024-12-02-12:58:41-root-INFO: grad norm: 189.563 187.505 27.861
2024-12-02-12:58:42-root-INFO: grad norm: 154.905 152.369 27.912
2024-12-02-12:58:42-root-INFO: grad norm: 146.878 145.004 23.388
2024-12-02-12:58:43-root-INFO: grad norm: 144.340 141.924 26.294
2024-12-02-12:58:43-root-INFO: Loss Change: 1434.701 -> 1304.367
2024-12-02-12:58:43-root-INFO: Regularization Change: 0.000 -> 0.761
2024-12-02-12:58:43-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-12:58:43-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-12:58:43-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-12:58:43-root-INFO: grad norm: 208.454 206.127 31.061
2024-12-02-12:58:44-root-INFO: Loss too large (1307.914->1502.092)! Learning rate decreased to 0.00434.
2024-12-02-12:58:44-root-INFO: Loss too large (1307.914->1464.434)! Learning rate decreased to 0.00347.
2024-12-02-12:58:44-root-INFO: Loss too large (1307.914->1421.495)! Learning rate decreased to 0.00278.
2024-12-02-12:58:44-root-INFO: Loss too large (1307.914->1377.167)! Learning rate decreased to 0.00222.
2024-12-02-12:58:44-root-INFO: Loss too large (1307.914->1338.486)! Learning rate decreased to 0.00178.
2024-12-02-12:58:44-root-INFO: Loss too large (1307.914->1311.044)! Learning rate decreased to 0.00142.
2024-12-02-12:58:45-root-INFO: grad norm: 205.977 203.687 30.628
2024-12-02-12:58:45-root-INFO: Loss too large (1295.317->1295.714)! Learning rate decreased to 0.00114.
2024-12-02-12:58:45-root-INFO: grad norm: 155.049 153.266 23.444
2024-12-02-12:58:46-root-INFO: grad norm: 113.017 111.160 20.405
2024-12-02-12:58:46-root-INFO: grad norm: 98.121 96.496 17.784
2024-12-02-12:58:47-root-INFO: grad norm: 85.881 84.031 17.727
2024-12-02-12:58:47-root-INFO: grad norm: 79.385 77.732 16.111
2024-12-02-12:58:48-root-INFO: grad norm: 74.123 72.255 16.535
2024-12-02-12:58:48-root-INFO: Loss Change: 1307.914 -> 1256.825
2024-12-02-12:58:48-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-02-12:58:48-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-12:58:48-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-12:58:48-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-12:58:48-root-INFO: grad norm: 147.770 146.023 22.659
2024-12-02-12:58:49-root-INFO: Loss too large (1256.395->1427.930)! Learning rate decreased to 0.00451.
2024-12-02-12:58:49-root-INFO: Loss too large (1256.395->1387.990)! Learning rate decreased to 0.00361.
2024-12-02-12:58:49-root-INFO: Loss too large (1256.395->1346.491)! Learning rate decreased to 0.00289.
2024-12-02-12:58:49-root-INFO: Loss too large (1256.395->1309.283)! Learning rate decreased to 0.00231.
2024-12-02-12:58:49-root-INFO: Loss too large (1256.395->1281.243)! Learning rate decreased to 0.00185.
2024-12-02-12:58:49-root-INFO: Loss too large (1256.395->1263.278)! Learning rate decreased to 0.00148.
2024-12-02-12:58:50-root-INFO: grad norm: 193.241 191.224 27.851
2024-12-02-12:58:50-root-INFO: Loss too large (1253.289->1259.227)! Learning rate decreased to 0.00118.
2024-12-02-12:58:50-root-INFO: grad norm: 166.045 164.440 23.029
2024-12-02-12:58:51-root-INFO: grad norm: 135.019 133.301 21.472
2024-12-02-12:58:51-root-INFO: grad norm: 125.164 123.725 18.927
2024-12-02-12:58:52-root-INFO: grad norm: 114.716 113.070 19.362
2024-12-02-12:58:52-root-INFO: grad norm: 110.191 108.797 17.475
2024-12-02-12:58:53-root-INFO: grad norm: 105.993 104.379 18.430
2024-12-02-12:58:53-root-INFO: Loss Change: 1256.395 -> 1224.072
2024-12-02-12:58:53-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-12:58:53-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-12:58:53-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-12:58:53-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-12:58:53-root-INFO: grad norm: 161.435 159.756 23.222
2024-12-02-12:58:54-root-INFO: Loss too large (1227.903->1428.494)! Learning rate decreased to 0.00469.
2024-12-02-12:58:54-root-INFO: Loss too large (1227.903->1392.869)! Learning rate decreased to 0.00375.
2024-12-02-12:58:54-root-INFO: Loss too large (1227.903->1351.501)! Learning rate decreased to 0.00300.
2024-12-02-12:58:54-root-INFO: Loss too large (1227.903->1308.600)! Learning rate decreased to 0.00240.
2024-12-02-12:58:54-root-INFO: Loss too large (1227.903->1271.376)! Learning rate decreased to 0.00192.
2024-12-02-12:58:54-root-INFO: Loss too large (1227.903->1244.848)! Learning rate decreased to 0.00154.
2024-12-02-12:58:55-root-INFO: Loss too large (1227.903->1228.982)! Learning rate decreased to 0.00123.
2024-12-02-12:58:55-root-INFO: grad norm: 157.399 155.722 22.918
2024-12-02-12:58:55-root-INFO: grad norm: 155.160 153.660 21.523
2024-12-02-12:58:56-root-INFO: grad norm: 152.338 150.680 22.413
2024-12-02-12:58:56-root-INFO: grad norm: 151.128 149.691 20.791
2024-12-02-12:58:57-root-INFO: grad norm: 149.877 148.231 22.154
2024-12-02-12:58:57-root-INFO: grad norm: 149.537 148.126 20.496
2024-12-02-12:58:58-root-INFO: grad norm: 149.735 148.091 22.122
2024-12-02-12:58:58-root-INFO: Loss Change: 1227.903 -> 1202.670
2024-12-02-12:58:58-root-INFO: Regularization Change: 0.000 -> 0.236
2024-12-02-12:58:58-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-12:58:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-12:58:58-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-12:58:58-root-INFO: grad norm: 212.516 210.648 28.118
2024-12-02-12:58:59-root-INFO: Loss too large (1212.491->1444.013)! Learning rate decreased to 0.00488.
2024-12-02-12:58:59-root-INFO: Loss too large (1212.491->1414.330)! Learning rate decreased to 0.00390.
2024-12-02-12:58:59-root-INFO: Loss too large (1212.491->1376.479)! Learning rate decreased to 0.00312.
2024-12-02-12:58:59-root-INFO: Loss too large (1212.491->1330.360)! Learning rate decreased to 0.00250.
2024-12-02-12:58:59-root-INFO: Loss too large (1212.491->1280.608)! Learning rate decreased to 0.00200.
2024-12-02-12:58:59-root-INFO: Loss too large (1212.491->1237.821)! Learning rate decreased to 0.00160.
2024-12-02-12:59:00-root-INFO: grad norm: 300.225 297.593 39.668
2024-12-02-12:59:00-root-INFO: Loss too large (1209.690->1237.902)! Learning rate decreased to 0.00128.
2024-12-02-12:59:00-root-INFO: Loss too large (1209.690->1215.284)! Learning rate decreased to 0.00102.
2024-12-02-12:59:01-root-INFO: grad norm: 188.297 186.643 24.898
2024-12-02-12:59:01-root-INFO: grad norm: 80.158 78.747 14.975
2024-12-02-12:59:02-root-INFO: grad norm: 66.181 64.722 13.819
2024-12-02-12:59:02-root-INFO: grad norm: 55.896 54.357 13.028
2024-12-02-12:59:02-root-INFO: grad norm: 50.948 49.356 12.638
2024-12-02-12:59:03-root-INFO: grad norm: 47.681 46.049 12.366
2024-12-02-12:59:03-root-INFO: Loss Change: 1212.491 -> 1172.090
2024-12-02-12:59:03-root-INFO: Regularization Change: 0.000 -> 0.201
2024-12-02-12:59:03-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-12:59:03-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-12:59:03-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-12:59:04-root-INFO: grad norm: 101.230 99.776 17.091
2024-12-02-12:59:04-root-INFO: Loss too large (1170.626->1314.510)! Learning rate decreased to 0.00507.
2024-12-02-12:59:04-root-INFO: Loss too large (1170.626->1273.339)! Learning rate decreased to 0.00405.
2024-12-02-12:59:04-root-INFO: Loss too large (1170.626->1235.649)! Learning rate decreased to 0.00324.
2024-12-02-12:59:04-root-INFO: Loss too large (1170.626->1206.663)! Learning rate decreased to 0.00259.
2024-12-02-12:59:04-root-INFO: Loss too large (1170.626->1187.504)! Learning rate decreased to 0.00208.
2024-12-02-12:59:05-root-INFO: Loss too large (1170.626->1176.207)! Learning rate decreased to 0.00166.
2024-12-02-12:59:05-root-INFO: grad norm: 164.143 162.544 22.853
2024-12-02-12:59:05-root-INFO: Loss too large (1170.137->1179.923)! Learning rate decreased to 0.00133.
2024-12-02-12:59:05-root-INFO: Loss too large (1170.137->1171.518)! Learning rate decreased to 0.00106.
2024-12-02-12:59:06-root-INFO: grad norm: 121.807 120.463 18.042
2024-12-02-12:59:06-root-INFO: grad norm: 78.867 77.534 14.440
2024-12-02-12:59:07-root-INFO: grad norm: 65.724 64.351 13.366
2024-12-02-12:59:07-root-INFO: grad norm: 55.227 53.771 12.597
2024-12-02-12:59:08-root-INFO: grad norm: 50.117 48.602 12.231
2024-12-02-12:59:08-root-INFO: grad norm: 46.529 44.968 11.952
2024-12-02-12:59:09-root-INFO: Loss Change: 1170.626 -> 1149.439
2024-12-02-12:59:09-root-INFO: Regularization Change: 0.000 -> 0.169
2024-12-02-12:59:09-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-12:59:09-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-12:59:09-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-12:59:09-root-INFO: grad norm: 121.775 119.850 21.567
2024-12-02-12:59:09-root-INFO: Loss too large (1157.383->1339.725)! Learning rate decreased to 0.00527.
2024-12-02-12:59:09-root-INFO: Loss too large (1157.383->1298.642)! Learning rate decreased to 0.00421.
2024-12-02-12:59:09-root-INFO: Loss too large (1157.383->1254.902)! Learning rate decreased to 0.00337.
2024-12-02-12:59:09-root-INFO: Loss too large (1157.383->1215.595)! Learning rate decreased to 0.00270.
2024-12-02-12:59:10-root-INFO: Loss too large (1157.383->1186.411)! Learning rate decreased to 0.00216.
2024-12-02-12:59:10-root-INFO: Loss too large (1157.383->1168.019)! Learning rate decreased to 0.00173.
2024-12-02-12:59:10-root-INFO: Loss too large (1157.383->1157.812)! Learning rate decreased to 0.00138.
2024-12-02-12:59:10-root-INFO: grad norm: 139.948 138.536 19.834
2024-12-02-12:59:11-root-INFO: Loss too large (1152.791->1153.102)! Learning rate decreased to 0.00110.
2024-12-02-12:59:11-root-INFO: grad norm: 113.149 111.466 19.444
2024-12-02-12:59:12-root-INFO: grad norm: 83.348 82.002 14.916
2024-12-02-12:59:12-root-INFO: grad norm: 72.043 70.403 15.284
2024-12-02-12:59:12-root-INFO: grad norm: 61.405 59.974 13.179
2024-12-02-12:59:13-root-INFO: grad norm: 55.659 53.994 13.512
2024-12-02-12:59:13-root-INFO: grad norm: 50.797 49.283 12.311
2024-12-02-12:59:14-root-INFO: Loss Change: 1157.383 -> 1132.164
2024-12-02-12:59:14-root-INFO: Regularization Change: 0.000 -> 0.192
2024-12-02-12:59:14-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-12:59:14-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-12:59:14-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-12:59:14-root-INFO: grad norm: 112.722 111.228 18.292
2024-12-02-12:59:14-root-INFO: Loss too large (1134.219->1313.833)! Learning rate decreased to 0.00547.
2024-12-02-12:59:14-root-INFO: Loss too large (1134.219->1272.377)! Learning rate decreased to 0.00438.
2024-12-02-12:59:15-root-INFO: Loss too large (1134.219->1228.497)! Learning rate decreased to 0.00350.
2024-12-02-12:59:15-root-INFO: Loss too large (1134.219->1189.750)! Learning rate decreased to 0.00280.
2024-12-02-12:59:15-root-INFO: Loss too large (1134.219->1161.638)! Learning rate decreased to 0.00224.
2024-12-02-12:59:15-root-INFO: Loss too large (1134.219->1144.292)! Learning rate decreased to 0.00179.
2024-12-02-12:59:15-root-INFO: Loss too large (1134.219->1134.815)! Learning rate decreased to 0.00143.
2024-12-02-12:59:16-root-INFO: grad norm: 125.843 124.442 18.722
2024-12-02-12:59:16-root-INFO: grad norm: 130.118 128.763 18.727
2024-12-02-12:59:17-root-INFO: grad norm: 137.864 136.496 19.370
2024-12-02-12:59:17-root-INFO: Loss too large (1125.347->1126.393)! Learning rate decreased to 0.00115.
2024-12-02-12:59:17-root-INFO: grad norm: 106.042 104.769 16.383
2024-12-02-12:59:18-root-INFO: grad norm: 71.605 70.345 13.373
2024-12-02-12:59:18-root-INFO: grad norm: 60.806 59.445 12.795
2024-12-02-12:59:19-root-INFO: grad norm: 51.679 50.286 11.914
2024-12-02-12:59:19-root-INFO: Loss Change: 1134.219 -> 1111.078
2024-12-02-12:59:19-root-INFO: Regularization Change: 0.000 -> 0.194
2024-12-02-12:59:19-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-12:59:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-12:59:19-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-12:59:19-root-INFO: grad norm: 111.820 110.289 18.437
2024-12-02-12:59:19-root-INFO: Loss too large (1115.856->1303.455)! Learning rate decreased to 0.00568.
2024-12-02-12:59:20-root-INFO: Loss too large (1115.856->1262.514)! Learning rate decreased to 0.00455.
2024-12-02-12:59:20-root-INFO: Loss too large (1115.856->1217.424)! Learning rate decreased to 0.00364.
2024-12-02-12:59:20-root-INFO: Loss too large (1115.856->1176.281)! Learning rate decreased to 0.00291.
2024-12-02-12:59:20-root-INFO: Loss too large (1115.856->1145.890)! Learning rate decreased to 0.00233.
2024-12-02-12:59:20-root-INFO: Loss too large (1115.856->1127.067)! Learning rate decreased to 0.00186.
2024-12-02-12:59:20-root-INFO: Loss too large (1115.856->1116.823)! Learning rate decreased to 0.00149.
2024-12-02-12:59:21-root-INFO: grad norm: 126.912 125.629 18.001
2024-12-02-12:59:21-root-INFO: Loss too large (1111.860->1112.349)! Learning rate decreased to 0.00119.
2024-12-02-12:59:21-root-INFO: grad norm: 101.356 99.946 16.848
2024-12-02-12:59:22-root-INFO: grad norm: 72.245 70.981 13.454
2024-12-02-12:59:22-root-INFO: grad norm: 62.451 60.983 13.463
2024-12-02-12:59:23-root-INFO: grad norm: 53.476 52.092 12.086
2024-12-02-12:59:23-root-INFO: grad norm: 48.880 47.333 12.202
2024-12-02-12:59:24-root-INFO: grad norm: 45.173 43.697 11.452
2024-12-02-12:59:24-root-INFO: Loss Change: 1115.856 -> 1093.414
2024-12-02-12:59:24-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-12:59:24-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-12:59:24-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-12:59:24-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-12:59:24-root-INFO: grad norm: 95.549 94.212 15.927
2024-12-02-12:59:25-root-INFO: Loss too large (1094.760->1258.954)! Learning rate decreased to 0.00590.
2024-12-02-12:59:25-root-INFO: Loss too large (1094.760->1214.751)! Learning rate decreased to 0.00472.
2024-12-02-12:59:25-root-INFO: Loss too large (1094.760->1171.542)! Learning rate decreased to 0.00378.
2024-12-02-12:59:25-root-INFO: Loss too large (1094.760->1137.131)! Learning rate decreased to 0.00302.
2024-12-02-12:59:25-root-INFO: Loss too large (1094.760->1114.362)! Learning rate decreased to 0.00242.
2024-12-02-12:59:25-root-INFO: Loss too large (1094.760->1101.199)! Learning rate decreased to 0.00193.
2024-12-02-12:59:26-root-INFO: grad norm: 162.462 161.037 21.467
2024-12-02-12:59:26-root-INFO: Loss too large (1094.316->1106.312)! Learning rate decreased to 0.00155.
2024-12-02-12:59:26-root-INFO: Loss too large (1094.316->1097.350)! Learning rate decreased to 0.00124.
2024-12-02-12:59:27-root-INFO: grad norm: 118.837 117.612 17.021
2024-12-02-12:59:27-root-INFO: grad norm: 66.693 65.425 12.941
2024-12-02-12:59:27-root-INFO: grad norm: 56.508 55.137 12.370
2024-12-02-12:59:28-root-INFO: grad norm: 48.018 46.588 11.633
2024-12-02-12:59:28-root-INFO: grad norm: 44.093 42.576 11.464
2024-12-02-12:59:29-root-INFO: grad norm: 41.312 39.785 11.131
2024-12-02-12:59:29-root-INFO: Loss Change: 1094.760 -> 1073.845
2024-12-02-12:59:29-root-INFO: Regularization Change: 0.000 -> 0.191
2024-12-02-12:59:29-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-12:59:29-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-12:59:29-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-12:59:30-root-INFO: grad norm: 96.597 95.065 17.134
2024-12-02-12:59:30-root-INFO: Loss too large (1078.271->1245.282)! Learning rate decreased to 0.00613.
2024-12-02-12:59:30-root-INFO: Loss too large (1078.271->1200.078)! Learning rate decreased to 0.00490.
2024-12-02-12:59:30-root-INFO: Loss too large (1078.271->1155.617)! Learning rate decreased to 0.00392.
2024-12-02-12:59:30-root-INFO: Loss too large (1078.271->1120.200)! Learning rate decreased to 0.00314.
2024-12-02-12:59:30-root-INFO: Loss too large (1078.271->1096.946)! Learning rate decreased to 0.00251.
2024-12-02-12:59:30-root-INFO: Loss too large (1078.271->1083.686)! Learning rate decreased to 0.00201.
2024-12-02-12:59:31-root-INFO: grad norm: 160.233 158.869 20.863
2024-12-02-12:59:31-root-INFO: Loss too large (1076.887->1088.404)! Learning rate decreased to 0.00161.
2024-12-02-12:59:31-root-INFO: Loss too large (1076.887->1079.657)! Learning rate decreased to 0.00129.
2024-12-02-12:59:32-root-INFO: grad norm: 116.292 114.969 17.489
2024-12-02-12:59:32-root-INFO: grad norm: 63.454 62.138 12.854
2024-12-02-12:59:33-root-INFO: grad norm: 53.978 52.470 12.671
2024-12-02-12:59:33-root-INFO: grad norm: 46.322 44.830 11.665
2024-12-02-12:59:34-root-INFO: grad norm: 42.795 41.181 11.641
2024-12-02-12:59:34-root-INFO: grad norm: 40.367 38.796 11.153
2024-12-02-12:59:34-root-INFO: Loss Change: 1078.271 -> 1055.712
2024-12-02-12:59:34-root-INFO: Regularization Change: 0.000 -> 0.214
2024-12-02-12:59:34-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-12:59:34-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-12:59:34-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-12:59:35-root-INFO: grad norm: 95.261 93.638 17.511
2024-12-02-12:59:35-root-INFO: Loss too large (1060.518->1235.272)! Learning rate decreased to 0.00636.
2024-12-02-12:59:35-root-INFO: Loss too large (1060.518->1189.017)! Learning rate decreased to 0.00509.
2024-12-02-12:59:35-root-INFO: Loss too large (1060.518->1142.391)! Learning rate decreased to 0.00407.
2024-12-02-12:59:35-root-INFO: Loss too large (1060.518->1104.837)! Learning rate decreased to 0.00326.
2024-12-02-12:59:35-root-INFO: Loss too large (1060.518->1080.207)! Learning rate decreased to 0.00261.
2024-12-02-12:59:36-root-INFO: Loss too large (1060.518->1066.267)! Learning rate decreased to 0.00208.
2024-12-02-12:59:36-root-INFO: grad norm: 157.749 156.411 20.503
2024-12-02-12:59:36-root-INFO: Loss too large (1059.183->1070.576)! Learning rate decreased to 0.00167.
2024-12-02-12:59:36-root-INFO: Loss too large (1059.183->1061.949)! Learning rate decreased to 0.00133.
2024-12-02-12:59:37-root-INFO: grad norm: 113.158 111.784 17.578
2024-12-02-12:59:37-root-INFO: grad norm: 59.533 58.149 12.764
2024-12-02-12:59:38-root-INFO: grad norm: 50.958 49.344 12.726
2024-12-02-12:59:38-root-INFO: grad norm: 44.317 42.751 11.677
2024-12-02-12:59:39-root-INFO: grad norm: 41.286 39.597 11.689
2024-12-02-12:59:39-root-INFO: grad norm: 39.263 37.644 11.160
2024-12-02-12:59:39-root-INFO: Loss Change: 1060.518 -> 1037.881
2024-12-02-12:59:39-root-INFO: Regularization Change: 0.000 -> 0.223
2024-12-02-12:59:39-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-12:59:39-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-12:59:40-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-12:59:40-root-INFO: grad norm: 76.663 75.304 14.368
2024-12-02-12:59:40-root-INFO: Loss too large (1039.407->1175.764)! Learning rate decreased to 0.00660.
2024-12-02-12:59:40-root-INFO: Loss too large (1039.407->1129.445)! Learning rate decreased to 0.00528.
2024-12-02-12:59:40-root-INFO: Loss too large (1039.407->1090.869)! Learning rate decreased to 0.00423.
2024-12-02-12:59:40-root-INFO: Loss too large (1039.407->1064.767)! Learning rate decreased to 0.00338.
2024-12-02-12:59:41-root-INFO: Loss too large (1039.407->1049.518)! Learning rate decreased to 0.00270.
2024-12-02-12:59:41-root-INFO: Loss too large (1039.407->1041.427)! Learning rate decreased to 0.00216.
2024-12-02-12:59:41-root-INFO: grad norm: 117.306 116.159 16.362
2024-12-02-12:59:41-root-INFO: Loss too large (1037.483->1042.169)! Learning rate decreased to 0.00173.
2024-12-02-12:59:42-root-INFO: grad norm: 108.108 106.968 15.660
2024-12-02-12:59:42-root-INFO: grad norm: 90.128 89.045 13.928
2024-12-02-12:59:43-root-INFO: grad norm: 86.466 85.353 13.826
2024-12-02-12:59:43-root-INFO: grad norm: 80.073 79.019 12.950
2024-12-02-12:59:44-root-INFO: grad norm: 77.744 76.649 12.998
2024-12-02-12:59:44-root-INFO: grad norm: 73.981 72.947 12.327
2024-12-02-12:59:44-root-INFO: Loss Change: 1039.407 -> 1018.946
2024-12-02-12:59:44-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-12:59:44-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-12:59:44-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-12:59:45-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-12:59:45-root-INFO: grad norm: 116.647 115.408 16.956
2024-12-02-12:59:45-root-INFO: Loss too large (1025.535->1247.155)! Learning rate decreased to 0.00685.
2024-12-02-12:59:45-root-INFO: Loss too large (1025.535->1206.000)! Learning rate decreased to 0.00548.
2024-12-02-12:59:45-root-INFO: Loss too large (1025.535->1154.128)! Learning rate decreased to 0.00439.
2024-12-02-12:59:45-root-INFO: Loss too large (1025.535->1100.008)! Learning rate decreased to 0.00351.
2024-12-02-12:59:46-root-INFO: Loss too large (1025.535->1058.035)! Learning rate decreased to 0.00281.
2024-12-02-12:59:46-root-INFO: Loss too large (1025.535->1033.246)! Learning rate decreased to 0.00225.
2024-12-02-12:59:46-root-INFO: grad norm: 164.596 163.330 20.374
2024-12-02-12:59:46-root-INFO: Loss too large (1021.182->1032.845)! Learning rate decreased to 0.00180.
2024-12-02-12:59:47-root-INFO: Loss too large (1021.182->1023.758)! Learning rate decreased to 0.00144.
2024-12-02-12:59:47-root-INFO: grad norm: 105.593 104.458 15.443
2024-12-02-12:59:47-root-INFO: grad norm: 43.485 42.025 11.176
2024-12-02-12:59:48-root-INFO: grad norm: 39.291 37.713 11.024
2024-12-02-12:59:48-root-INFO: grad norm: 37.095 35.528 10.667
2024-12-02-12:59:49-root-INFO: grad norm: 36.152 34.570 10.575
2024-12-02-12:59:49-root-INFO: grad norm: 35.601 34.051 10.390
2024-12-02-12:59:50-root-INFO: Loss Change: 1025.535 -> 999.708
2024-12-02-12:59:50-root-INFO: Regularization Change: 0.000 -> 0.233
2024-12-02-12:59:50-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-12:59:50-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-12:59:50-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-12:59:50-root-INFO: grad norm: 76.296 74.947 14.281
2024-12-02-12:59:50-root-INFO: Loss too large (1002.666->1143.835)! Learning rate decreased to 0.00711.
2024-12-02-12:59:50-root-INFO: Loss too large (1002.666->1094.277)! Learning rate decreased to 0.00569.
2024-12-02-12:59:50-root-INFO: Loss too large (1002.666->1053.221)! Learning rate decreased to 0.00455.
2024-12-02-12:59:51-root-INFO: Loss too large (1002.666->1026.149)! Learning rate decreased to 0.00364.
2024-12-02-12:59:51-root-INFO: Loss too large (1002.666->1010.891)! Learning rate decreased to 0.00291.
2024-12-02-12:59:51-root-INFO: Loss too large (1002.666->1003.117)! Learning rate decreased to 0.00233.
2024-12-02-12:59:51-root-INFO: grad norm: 103.725 102.647 14.918
2024-12-02-12:59:52-root-INFO: Loss too large (999.519->1001.602)! Learning rate decreased to 0.00186.
2024-12-02-12:59:52-root-INFO: grad norm: 90.334 89.194 14.304
2024-12-02-12:59:52-root-INFO: grad norm: 68.105 67.015 12.137
2024-12-02-12:59:53-root-INFO: grad norm: 62.871 61.682 12.166
2024-12-02-12:59:53-root-INFO: grad norm: 55.741 54.618 11.136
2024-12-02-12:59:54-root-INFO: grad norm: 52.504 51.281 11.269
2024-12-02-12:59:54-root-INFO: grad norm: 48.544 47.386 10.539
2024-12-02-12:59:55-root-INFO: Loss Change: 1002.666 -> 979.628
2024-12-02-12:59:55-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-12:59:55-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-12:59:55-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-12:59:55-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-12:59:55-root-INFO: grad norm: 90.440 89.166 15.128
2024-12-02-12:59:55-root-INFO: Loss too large (983.406->1165.580)! Learning rate decreased to 0.00738.
2024-12-02-12:59:55-root-INFO: Loss too large (983.406->1112.157)! Learning rate decreased to 0.00590.
2024-12-02-12:59:55-root-INFO: Loss too large (983.406->1058.616)! Learning rate decreased to 0.00472.
2024-12-02-12:59:56-root-INFO: Loss too large (983.406->1018.471)! Learning rate decreased to 0.00378.
2024-12-02-12:59:56-root-INFO: Loss too large (983.406->994.923)! Learning rate decreased to 0.00302.
2024-12-02-12:59:56-root-INFO: grad norm: 171.912 170.708 20.317
2024-12-02-12:59:56-root-INFO: Loss too large (983.138->1005.774)! Learning rate decreased to 0.00242.
2024-12-02-12:59:57-root-INFO: Loss too large (983.138->992.626)! Learning rate decreased to 0.00193.
2024-12-02-12:59:57-root-INFO: Loss too large (983.138->983.550)! Learning rate decreased to 0.00155.
2024-12-02-12:59:57-root-INFO: grad norm: 93.683 92.600 14.205
2024-12-02-12:59:58-root-INFO: grad norm: 35.682 34.064 10.623
2024-12-02-12:59:58-root-INFO: grad norm: 35.125 33.540 10.433
2024-12-02-12:59:59-root-INFO: grad norm: 34.700 33.152 10.249
2024-12-02-12:59:59-root-INFO: grad norm: 34.349 32.830 10.099
2024-12-02-13:00:00-root-INFO: grad norm: 34.046 32.556 9.962
2024-12-02-13:00:00-root-INFO: Loss Change: 983.406 -> 960.012
2024-12-02-13:00:00-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-13:00:00-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-13:00:00-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-13:00:00-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-13:00:00-root-INFO: grad norm: 56.566 55.298 11.908
2024-12-02-13:00:00-root-INFO: Loss too large (960.569->1026.772)! Learning rate decreased to 0.00765.
2024-12-02-13:00:01-root-INFO: Loss too large (960.569->994.396)! Learning rate decreased to 0.00612.
2024-12-02-13:00:01-root-INFO: Loss too large (960.569->975.242)! Learning rate decreased to 0.00490.
2024-12-02-13:00:01-root-INFO: Loss too large (960.569->965.063)! Learning rate decreased to 0.00392.
2024-12-02-13:00:01-root-INFO: grad norm: 127.427 126.426 15.943
2024-12-02-13:00:02-root-INFO: Loss too large (960.037->982.176)! Learning rate decreased to 0.00314.
2024-12-02-13:00:02-root-INFO: Loss too large (960.037->970.853)! Learning rate decreased to 0.00251.
2024-12-02-13:00:02-root-INFO: Loss too large (960.037->963.055)! Learning rate decreased to 0.00201.
2024-12-02-13:00:02-root-INFO: grad norm: 88.567 87.605 13.018
2024-12-02-13:00:03-root-INFO: grad norm: 40.849 39.601 10.021
2024-12-02-13:00:03-root-INFO: grad norm: 37.185 35.842 9.904
2024-12-02-13:00:04-root-INFO: grad norm: 34.757 33.414 9.568
2024-12-02-13:00:04-root-INFO: grad norm: 33.658 32.287 9.507
2024-12-02-13:00:05-root-INFO: grad norm: 32.958 31.614 9.318
2024-12-02-13:00:05-root-INFO: Loss Change: 960.569 -> 938.376
2024-12-02-13:00:05-root-INFO: Regularization Change: 0.000 -> 0.377
2024-12-02-13:00:05-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-13:00:05-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-13:00:05-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-13:00:05-root-INFO: grad norm: 64.975 63.732 12.645
2024-12-02-13:00:05-root-INFO: Loss too large (940.653->1035.754)! Learning rate decreased to 0.00794.
2024-12-02-13:00:06-root-INFO: Loss too large (940.653->991.317)! Learning rate decreased to 0.00635.
2024-12-02-13:00:06-root-INFO: Loss too large (940.653->963.191)! Learning rate decreased to 0.00508.
2024-12-02-13:00:06-root-INFO: Loss too large (940.653->947.968)! Learning rate decreased to 0.00406.
2024-12-02-13:00:06-root-INFO: grad norm: 140.824 139.844 16.583
2024-12-02-13:00:07-root-INFO: Loss too large (940.457->965.384)! Learning rate decreased to 0.00325.
2024-12-02-13:00:07-root-INFO: Loss too large (940.457->952.495)! Learning rate decreased to 0.00260.
2024-12-02-13:00:07-root-INFO: Loss too large (940.457->943.552)! Learning rate decreased to 0.00208.
2024-12-02-13:00:07-root-INFO: grad norm: 88.463 87.511 12.945
2024-12-02-13:00:08-root-INFO: grad norm: 34.360 33.014 9.522
2024-12-02-13:00:08-root-INFO: grad norm: 33.144 31.775 9.428
2024-12-02-13:00:09-root-INFO: grad norm: 32.492 31.160 9.208
2024-12-02-13:00:09-root-INFO: grad norm: 32.100 30.778 9.115
2024-12-02-13:00:10-root-INFO: grad norm: 31.804 30.511 8.976
2024-12-02-13:00:10-root-INFO: Loss Change: 940.653 -> 917.108
2024-12-02-13:00:10-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-02-13:00:10-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-13:00:10-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:00:10-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-13:00:10-root-INFO: grad norm: 46.952 45.788 10.392
2024-12-02-13:00:11-root-INFO: Loss too large (917.685->945.833)! Learning rate decreased to 0.00823.
2024-12-02-13:00:11-root-INFO: Loss too large (917.685->929.243)! Learning rate decreased to 0.00659.
2024-12-02-13:00:11-root-INFO: Loss too large (917.685->920.666)! Learning rate decreased to 0.00527.
2024-12-02-13:00:11-root-INFO: grad norm: 114.048 113.215 13.761
2024-12-02-13:00:11-root-INFO: Loss too large (916.523->944.103)! Learning rate decreased to 0.00422.
2024-12-02-13:00:12-root-INFO: Loss too large (916.523->931.366)! Learning rate decreased to 0.00337.
2024-12-02-13:00:12-root-INFO: Loss too large (916.523->922.484)! Learning rate decreased to 0.00270.
2024-12-02-13:00:12-root-INFO: Loss too large (916.523->916.550)! Learning rate decreased to 0.00216.
2024-12-02-13:00:12-root-INFO: grad norm: 70.011 69.129 11.077
2024-12-02-13:00:13-root-INFO: grad norm: 32.772 31.561 8.825
2024-12-02-13:00:13-root-INFO: grad norm: 31.468 30.218 8.780
2024-12-02-13:00:14-root-INFO: grad norm: 30.921 29.695 8.622
2024-12-02-13:00:14-root-INFO: grad norm: 30.628 29.408 8.557
2024-12-02-13:00:15-root-INFO: grad norm: 30.402 29.200 8.462
2024-12-02-13:00:15-root-INFO: Loss Change: 917.685 -> 896.268
2024-12-02-13:00:15-root-INFO: Regularization Change: 0.000 -> 0.421
2024-12-02-13:00:15-root-INFO: Undo step: 150
2024-12-02-13:00:15-root-INFO: Undo step: 151
2024-12-02-13:00:15-root-INFO: Undo step: 152
2024-12-02-13:00:15-root-INFO: Undo step: 153
2024-12-02-13:00:15-root-INFO: Undo step: 154
2024-12-02-13:00:15-root-INFO: Undo step: 155
2024-12-02-13:00:15-root-INFO: Undo step: 156
2024-12-02-13:00:15-root-INFO: Undo step: 157
2024-12-02-13:00:15-root-INFO: Undo step: 158
2024-12-02-13:00:15-root-INFO: Undo step: 159
2024-12-02-13:00:15-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-13:00:15-root-INFO: grad norm: 411.840 405.869 69.875
2024-12-02-13:00:16-root-INFO: grad norm: 282.935 278.524 49.765
2024-12-02-13:00:16-root-INFO: grad norm: 281.157 275.797 54.638
2024-12-02-13:00:17-root-INFO: grad norm: 293.600 290.859 40.028
2024-12-02-13:00:17-root-INFO: grad norm: 257.390 253.429 44.981
2024-12-02-13:00:18-root-INFO: grad norm: 258.981 257.069 31.410
2024-12-02-13:00:18-root-INFO: Loss too large (1273.017->1360.322)! Learning rate decreased to 0.00568.
2024-12-02-13:00:18-root-INFO: grad norm: 323.679 319.863 49.553
2024-12-02-13:00:18-root-INFO: Loss too large (1268.404->1330.585)! Learning rate decreased to 0.00455.
2024-12-02-13:00:19-root-INFO: grad norm: 234.277 232.788 26.367
2024-12-02-13:00:19-root-INFO: Loss Change: 2249.492 -> 1122.881
2024-12-02-13:00:19-root-INFO: Regularization Change: 0.000 -> 30.557
2024-12-02-13:00:19-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-13:00:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:00:19-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-13:00:20-root-INFO: grad norm: 142.087 139.154 28.722
2024-12-02-13:00:20-root-INFO: Loss too large (1111.960->1182.220)! Learning rate decreased to 0.00590.
2024-12-02-13:00:20-root-INFO: Loss too large (1111.960->1149.202)! Learning rate decreased to 0.00472.
2024-12-02-13:00:20-root-INFO: Loss too large (1111.960->1126.585)! Learning rate decreased to 0.00378.
2024-12-02-13:00:20-root-INFO: grad norm: 145.830 144.396 20.398
2024-12-02-13:00:21-root-INFO: grad norm: 181.732 179.041 31.156
2024-12-02-13:00:21-root-INFO: Loss too large (1091.198->1113.458)! Learning rate decreased to 0.00302.
2024-12-02-13:00:21-root-INFO: Loss too large (1091.198->1094.932)! Learning rate decreased to 0.00242.
2024-12-02-13:00:22-root-INFO: grad norm: 123.426 121.981 18.835
2024-12-02-13:00:22-root-INFO: grad norm: 62.235 59.780 17.306
2024-12-02-13:00:23-root-INFO: grad norm: 56.953 54.878 15.234
2024-12-02-13:00:23-root-INFO: grad norm: 53.062 50.713 15.613
2024-12-02-13:00:24-root-INFO: grad norm: 50.819 48.742 14.379
2024-12-02-13:00:24-root-INFO: Loss Change: 1111.960 -> 1039.164
2024-12-02-13:00:24-root-INFO: Regularization Change: 0.000 -> 1.360
2024-12-02-13:00:24-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-13:00:24-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-13:00:24-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-13:00:24-root-INFO: grad norm: 55.392 53.694 13.608
2024-12-02-13:00:25-root-INFO: grad norm: 130.373 128.696 20.847
2024-12-02-13:00:25-root-INFO: Loss too large (1026.223->1124.054)! Learning rate decreased to 0.00613.
2024-12-02-13:00:25-root-INFO: Loss too large (1026.223->1087.941)! Learning rate decreased to 0.00490.
2024-12-02-13:00:25-root-INFO: Loss too large (1026.223->1062.183)! Learning rate decreased to 0.00392.
2024-12-02-13:00:25-root-INFO: Loss too large (1026.223->1044.198)! Learning rate decreased to 0.00314.
2024-12-02-13:00:25-root-INFO: Loss too large (1026.223->1032.092)! Learning rate decreased to 0.00251.
2024-12-02-13:00:26-root-INFO: grad norm: 110.411 109.186 16.400
2024-12-02-13:00:26-root-INFO: grad norm: 80.780 79.300 15.393
2024-12-02-13:00:27-root-INFO: grad norm: 77.530 76.313 13.684
2024-12-02-13:00:27-root-INFO: grad norm: 74.133 72.712 14.445
2024-12-02-13:00:28-root-INFO: grad norm: 73.592 72.405 13.160
2024-12-02-13:00:28-root-INFO: grad norm: 74.286 72.921 14.177
2024-12-02-13:00:29-root-INFO: Loss Change: 1035.977 -> 992.796
2024-12-02-13:00:29-root-INFO: Regularization Change: 0.000 -> 1.153
2024-12-02-13:00:29-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-13:00:29-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:00:29-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-13:00:29-root-INFO: grad norm: 105.344 104.089 16.216
2024-12-02-13:00:29-root-INFO: Loss too large (995.184->1142.411)! Learning rate decreased to 0.00636.
2024-12-02-13:00:29-root-INFO: Loss too large (995.184->1086.652)! Learning rate decreased to 0.00509.
2024-12-02-13:00:29-root-INFO: Loss too large (995.184->1037.687)! Learning rate decreased to 0.00407.
2024-12-02-13:00:29-root-INFO: Loss too large (995.184->1006.827)! Learning rate decreased to 0.00326.
2024-12-02-13:00:30-root-INFO: grad norm: 163.892 162.192 23.547
2024-12-02-13:00:30-root-INFO: Loss too large (991.483->1010.461)! Learning rate decreased to 0.00261.
2024-12-02-13:00:30-root-INFO: Loss too large (991.483->996.952)! Learning rate decreased to 0.00208.
2024-12-02-13:00:31-root-INFO: grad norm: 108.871 107.700 15.926
2024-12-02-13:00:31-root-INFO: grad norm: 45.119 43.760 10.993
2024-12-02-13:00:32-root-INFO: grad norm: 40.936 39.553 10.551
2024-12-02-13:00:32-root-INFO: grad norm: 38.347 36.942 10.283
2024-12-02-13:00:33-root-INFO: grad norm: 37.080 35.682 10.084
2024-12-02-13:00:33-root-INFO: grad norm: 36.233 34.843 9.939
2024-12-02-13:00:33-root-INFO: Loss Change: 995.184 -> 960.603
2024-12-02-13:00:33-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-13:00:33-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-13:00:33-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:00:34-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-13:00:34-root-INFO: grad norm: 53.575 52.442 10.963
2024-12-02-13:00:34-root-INFO: Loss too large (958.750->980.495)! Learning rate decreased to 0.00660.
2024-12-02-13:00:34-root-INFO: Loss too large (958.750->966.867)! Learning rate decreased to 0.00528.
2024-12-02-13:00:34-root-INFO: Loss too large (958.750->959.832)! Learning rate decreased to 0.00423.
2024-12-02-13:00:35-root-INFO: grad norm: 105.119 103.855 16.253
2024-12-02-13:00:35-root-INFO: Loss too large (956.474->972.470)! Learning rate decreased to 0.00338.
2024-12-02-13:00:35-root-INFO: Loss too large (956.474->963.079)! Learning rate decreased to 0.00270.
2024-12-02-13:00:35-root-INFO: Loss too large (956.474->956.987)! Learning rate decreased to 0.00216.
2024-12-02-13:00:36-root-INFO: grad norm: 77.505 76.537 12.211
2024-12-02-13:00:36-root-INFO: grad norm: 46.902 45.708 10.516
2024-12-02-13:00:37-root-INFO: grad norm: 41.200 40.045 9.688
2024-12-02-13:00:37-root-INFO: grad norm: 37.097 35.835 9.592
2024-12-02-13:00:37-root-INFO: grad norm: 35.298 34.069 9.231
2024-12-02-13:00:38-root-INFO: grad norm: 34.097 32.833 9.198
2024-12-02-13:00:38-root-INFO: Loss Change: 958.750 -> 934.211
2024-12-02-13:00:38-root-INFO: Regularization Change: 0.000 -> 0.455
2024-12-02-13:00:38-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-13:00:38-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:00:38-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-13:00:39-root-INFO: grad norm: 61.093 60.026 11.366
2024-12-02-13:00:39-root-INFO: Loss too large (934.651->984.879)! Learning rate decreased to 0.00685.
2024-12-02-13:00:39-root-INFO: Loss too large (934.651->957.941)! Learning rate decreased to 0.00548.
2024-12-02-13:00:39-root-INFO: Loss too large (934.651->943.044)! Learning rate decreased to 0.00439.
2024-12-02-13:00:39-root-INFO: Loss too large (934.651->935.479)! Learning rate decreased to 0.00351.
2024-12-02-13:00:40-root-INFO: grad norm: 95.990 94.865 14.656
2024-12-02-13:00:40-root-INFO: Loss too large (931.936->937.916)! Learning rate decreased to 0.00281.
2024-12-02-13:00:40-root-INFO: Loss too large (931.936->932.546)! Learning rate decreased to 0.00225.
2024-12-02-13:00:40-root-INFO: grad norm: 73.909 72.974 11.720
2024-12-02-13:00:41-root-INFO: grad norm: 47.236 46.188 9.894
2024-12-02-13:00:41-root-INFO: grad norm: 41.469 40.427 9.236
2024-12-02-13:00:42-root-INFO: grad norm: 36.705 35.596 8.953
2024-12-02-13:00:42-root-INFO: grad norm: 34.540 33.437 8.660
2024-12-02-13:00:43-root-INFO: grad norm: 32.963 31.834 8.551
2024-12-02-13:00:43-root-INFO: Loss Change: 934.651 -> 911.582
2024-12-02-13:00:43-root-INFO: Regularization Change: 0.000 -> 0.408
2024-12-02-13:00:43-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-13:00:43-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-13:00:43-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-13:00:43-root-INFO: grad norm: 59.353 58.306 11.096
2024-12-02-13:00:44-root-INFO: Loss too large (912.015->963.802)! Learning rate decreased to 0.00711.
2024-12-02-13:00:44-root-INFO: Loss too large (912.015->935.883)! Learning rate decreased to 0.00569.
2024-12-02-13:00:44-root-INFO: Loss too large (912.015->920.616)! Learning rate decreased to 0.00455.
2024-12-02-13:00:44-root-INFO: Loss too large (912.015->912.946)! Learning rate decreased to 0.00364.
2024-12-02-13:00:44-root-INFO: grad norm: 92.162 91.112 13.872
2024-12-02-13:00:45-root-INFO: Loss too large (909.383->914.663)! Learning rate decreased to 0.00291.
2024-12-02-13:00:45-root-INFO: Loss too large (909.383->909.681)! Learning rate decreased to 0.00233.
2024-12-02-13:00:45-root-INFO: grad norm: 69.286 68.391 11.103
2024-12-02-13:00:46-root-INFO: grad norm: 42.859 41.838 9.297
2024-12-02-13:00:46-root-INFO: grad norm: 37.593 36.559 8.760
2024-12-02-13:00:47-root-INFO: grad norm: 33.683 32.595 8.492
2024-12-02-13:00:47-root-INFO: grad norm: 31.973 30.886 8.263
2024-12-02-13:00:48-root-INFO: grad norm: 30.842 29.744 8.154
2024-12-02-13:00:48-root-INFO: Loss Change: 912.015 -> 889.921
2024-12-02-13:00:48-root-INFO: Regularization Change: 0.000 -> 0.404
2024-12-02-13:00:48-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-13:00:48-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-13:00:48-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-13:00:48-root-INFO: grad norm: 57.421 56.316 11.215
2024-12-02-13:00:48-root-INFO: Loss too large (889.916->931.696)! Learning rate decreased to 0.00738.
2024-12-02-13:00:49-root-INFO: Loss too large (889.916->907.953)! Learning rate decreased to 0.00590.
2024-12-02-13:00:49-root-INFO: Loss too large (889.916->895.427)! Learning rate decreased to 0.00472.
2024-12-02-13:00:49-root-INFO: grad norm: 120.076 118.924 16.593
2024-12-02-13:00:49-root-INFO: Loss too large (889.274->912.804)! Learning rate decreased to 0.00378.
2024-12-02-13:00:49-root-INFO: Loss too large (889.274->900.300)! Learning rate decreased to 0.00302.
2024-12-02-13:00:50-root-INFO: Loss too large (889.274->891.879)! Learning rate decreased to 0.00242.
2024-12-02-13:00:50-root-INFO: grad norm: 80.110 79.273 11.545
2024-12-02-13:00:51-root-INFO: grad norm: 35.305 34.282 8.437
2024-12-02-13:00:51-root-INFO: grad norm: 31.891 30.852 8.076
2024-12-02-13:00:52-root-INFO: grad norm: 30.012 28.958 7.884
2024-12-02-13:00:52-root-INFO: grad norm: 29.198 28.154 7.737
2024-12-02-13:00:52-root-INFO: grad norm: 28.709 27.674 7.637
2024-12-02-13:00:53-root-INFO: Loss Change: 889.916 -> 867.786
2024-12-02-13:00:53-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-02-13:00:53-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-13:00:53-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-13:00:53-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-13:00:53-root-INFO: grad norm: 43.673 42.762 8.873
2024-12-02-13:00:53-root-INFO: Loss too large (867.463->881.040)! Learning rate decreased to 0.00765.
2024-12-02-13:00:53-root-INFO: Loss too large (867.463->871.702)! Learning rate decreased to 0.00612.
2024-12-02-13:00:54-root-INFO: grad norm: 109.781 108.722 15.209
2024-12-02-13:00:54-root-INFO: Loss too large (867.098->901.732)! Learning rate decreased to 0.00490.
2024-12-02-13:00:54-root-INFO: Loss too large (867.098->886.192)! Learning rate decreased to 0.00392.
2024-12-02-13:00:54-root-INFO: Loss too large (867.098->875.466)! Learning rate decreased to 0.00314.
2024-12-02-13:00:54-root-INFO: Loss too large (867.098->868.349)! Learning rate decreased to 0.00251.
2024-12-02-13:00:55-root-INFO: grad norm: 70.435 69.709 10.088
2024-12-02-13:00:55-root-INFO: grad norm: 31.599 30.638 7.734
2024-12-02-13:00:56-root-INFO: grad norm: 29.036 28.074 7.413
2024-12-02-13:00:56-root-INFO: grad norm: 27.878 26.895 7.339
2024-12-02-13:00:57-root-INFO: grad norm: 27.404 26.433 7.228
2024-12-02-13:00:57-root-INFO: grad norm: 27.118 26.152 7.175
2024-12-02-13:00:58-root-INFO: Loss Change: 867.463 -> 847.509
2024-12-02-13:00:58-root-INFO: Regularization Change: 0.000 -> 0.452
2024-12-02-13:00:58-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-13:00:58-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-13:00:58-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-13:00:58-root-INFO: grad norm: 47.593 46.590 9.720
2024-12-02-13:00:58-root-INFO: Loss too large (848.121->867.981)! Learning rate decreased to 0.00794.
2024-12-02-13:00:58-root-INFO: Loss too large (848.121->855.047)! Learning rate decreased to 0.00635.
2024-12-02-13:00:58-root-INFO: Loss too large (848.121->848.624)! Learning rate decreased to 0.00508.
2024-12-02-13:00:59-root-INFO: grad norm: 80.833 79.948 11.923
2024-12-02-13:00:59-root-INFO: Loss too large (845.645->853.874)! Learning rate decreased to 0.00406.
2024-12-02-13:00:59-root-INFO: Loss too large (845.645->847.996)! Learning rate decreased to 0.00325.
2024-12-02-13:01:00-root-INFO: grad norm: 67.415 66.667 10.010
2024-12-02-13:01:00-root-INFO: grad norm: 46.142 45.364 8.438
2024-12-02-13:01:01-root-INFO: grad norm: 42.172 41.395 8.058
2024-12-02-13:01:01-root-INFO: grad norm: 37.329 36.539 7.639
2024-12-02-13:01:02-root-INFO: grad norm: 35.169 34.368 7.466
2024-12-02-13:01:02-root-INFO: grad norm: 32.799 31.995 7.218
2024-12-02-13:01:02-root-INFO: Loss Change: 848.121 -> 825.999
2024-12-02-13:01:02-root-INFO: Regularization Change: 0.000 -> 0.582
2024-12-02-13:01:02-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-13:01:02-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:01:02-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-13:01:03-root-INFO: grad norm: 46.507 45.738 8.420
2024-12-02-13:01:03-root-INFO: Loss too large (827.287->853.351)! Learning rate decreased to 0.00823.
2024-12-02-13:01:03-root-INFO: Loss too large (827.287->837.254)! Learning rate decreased to 0.00659.
2024-12-02-13:01:03-root-INFO: Loss too large (827.287->829.295)! Learning rate decreased to 0.00527.
2024-12-02-13:01:04-root-INFO: grad norm: 81.595 80.796 11.393
2024-12-02-13:01:04-root-INFO: Loss too large (825.585->834.035)! Learning rate decreased to 0.00422.
2024-12-02-13:01:04-root-INFO: Loss too large (825.585->827.994)! Learning rate decreased to 0.00337.
2024-12-02-13:01:04-root-INFO: grad norm: 64.170 63.490 9.311
2024-12-02-13:01:05-root-INFO: grad norm: 38.525 37.778 7.550
2024-12-02-13:01:05-root-INFO: grad norm: 34.456 33.689 7.230
2024-12-02-13:01:06-root-INFO: grad norm: 30.459 29.663 6.920
2024-12-02-13:01:06-root-INFO: grad norm: 28.635 27.817 6.794
2024-12-02-13:01:07-root-INFO: grad norm: 27.051 26.225 6.634
2024-12-02-13:01:07-root-INFO: Loss Change: 827.287 -> 806.703
2024-12-02-13:01:07-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-02-13:01:07-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-13:01:07-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:01:07-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-13:01:07-root-INFO: grad norm: 45.298 44.449 8.728
2024-12-02-13:01:08-root-INFO: Loss too large (807.420->829.260)! Learning rate decreased to 0.00854.
2024-12-02-13:01:08-root-INFO: Loss too large (807.420->815.184)! Learning rate decreased to 0.00683.
2024-12-02-13:01:08-root-INFO: Loss too large (807.420->808.318)! Learning rate decreased to 0.00546.
2024-12-02-13:01:08-root-INFO: grad norm: 72.407 71.620 10.651
2024-12-02-13:01:09-root-INFO: Loss too large (805.168->810.613)! Learning rate decreased to 0.00437.
2024-12-02-13:01:09-root-INFO: Loss too large (805.168->805.945)! Learning rate decreased to 0.00350.
2024-12-02-13:01:09-root-INFO: grad norm: 55.495 54.823 8.611
2024-12-02-13:01:10-root-INFO: grad norm: 33.988 33.235 7.111
2024-12-02-13:01:10-root-INFO: grad norm: 30.146 29.356 6.859
2024-12-02-13:01:11-root-INFO: grad norm: 26.966 26.154 6.565
2024-12-02-13:01:11-root-INFO: grad norm: 25.559 24.723 6.485
2024-12-02-13:01:11-root-INFO: grad norm: 24.537 23.704 6.338
2024-12-02-13:01:12-root-INFO: Loss Change: 807.420 -> 787.448
2024-12-02-13:01:12-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-02-13:01:12-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-13:01:12-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:01:12-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-13:01:12-root-INFO: grad norm: 42.196 41.285 8.721
2024-12-02-13:01:12-root-INFO: Loss too large (789.157->800.694)! Learning rate decreased to 0.00885.
2024-12-02-13:01:12-root-INFO: Loss too large (789.157->792.023)! Learning rate decreased to 0.00708.
2024-12-02-13:01:13-root-INFO: grad norm: 81.510 80.619 12.021
2024-12-02-13:01:13-root-INFO: Loss too large (787.844->802.995)! Learning rate decreased to 0.00566.
2024-12-02-13:01:13-root-INFO: Loss too large (787.844->794.124)! Learning rate decreased to 0.00453.
2024-12-02-13:01:13-root-INFO: Loss too large (787.844->788.407)! Learning rate decreased to 0.00363.
2024-12-02-13:01:14-root-INFO: grad norm: 55.028 54.375 8.451
2024-12-02-13:01:14-root-INFO: grad norm: 27.645 26.852 6.572
2024-12-02-13:01:15-root-INFO: grad norm: 25.050 24.205 6.451
2024-12-02-13:01:15-root-INFO: grad norm: 23.635 22.791 6.262
2024-12-02-13:01:16-root-INFO: grad norm: 23.050 22.194 6.221
2024-12-02-13:01:16-root-INFO: grad norm: 22.699 21.856 6.128
2024-12-02-13:01:17-root-INFO: Loss Change: 789.157 -> 769.293
2024-12-02-13:01:17-root-INFO: Regularization Change: 0.000 -> 0.599
2024-12-02-13:01:17-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-13:01:17-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-13:01:17-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-13:01:17-root-INFO: grad norm: 33.730 32.895 7.455
2024-12-02-13:01:17-root-INFO: Loss too large (769.035->769.865)! Learning rate decreased to 0.00917.
2024-12-02-13:01:18-root-INFO: grad norm: 72.616 71.885 10.278
2024-12-02-13:01:18-root-INFO: Loss too large (767.041->788.521)! Learning rate decreased to 0.00734.
2024-12-02-13:01:18-root-INFO: Loss too large (767.041->777.823)! Learning rate decreased to 0.00587.
2024-12-02-13:01:18-root-INFO: Loss too large (767.041->770.786)! Learning rate decreased to 0.00470.
2024-12-02-13:01:18-root-INFO: grad norm: 59.468 58.851 8.547
2024-12-02-13:01:19-root-INFO: grad norm: 37.573 36.935 6.893
2024-12-02-13:01:19-root-INFO: grad norm: 34.490 33.813 6.800
2024-12-02-13:01:20-root-INFO: grad norm: 30.704 30.048 6.313
2024-12-02-13:01:20-root-INFO: grad norm: 29.069 28.360 6.378
2024-12-02-13:01:21-root-INFO: grad norm: 27.221 26.546 6.025
2024-12-02-13:01:21-root-INFO: Loss Change: 769.035 -> 747.458
2024-12-02-13:01:21-root-INFO: Regularization Change: 0.000 -> 0.884
2024-12-02-13:01:21-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-13:01:21-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:01:21-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-13:01:21-root-INFO: grad norm: 40.285 39.637 7.196
2024-12-02-13:01:22-root-INFO: Loss too large (748.278->759.762)! Learning rate decreased to 0.00951.
2024-12-02-13:01:22-root-INFO: Loss too large (748.278->750.805)! Learning rate decreased to 0.00761.
2024-12-02-13:01:22-root-INFO: grad norm: 68.372 67.646 9.933
2024-12-02-13:01:22-root-INFO: Loss too large (746.651->754.769)! Learning rate decreased to 0.00608.
2024-12-02-13:01:23-root-INFO: Loss too large (746.651->748.704)! Learning rate decreased to 0.00487.
2024-12-02-13:01:23-root-INFO: grad norm: 51.860 51.309 7.535
2024-12-02-13:01:23-root-INFO: grad norm: 29.379 28.727 6.159
2024-12-02-13:01:24-root-INFO: grad norm: 26.109 25.431 5.913
2024-12-02-13:01:24-root-INFO: grad norm: 23.352 22.653 5.671
2024-12-02-13:01:25-root-INFO: grad norm: 22.130 21.398 5.642
2024-12-02-13:01:25-root-INFO: grad norm: 21.199 20.475 5.493
2024-12-02-13:01:26-root-INFO: Loss Change: 748.278 -> 728.006
2024-12-02-13:01:26-root-INFO: Regularization Change: 0.000 -> 0.773
2024-12-02-13:01:26-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-13:01:26-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:01:26-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-13:01:26-root-INFO: grad norm: 36.268 35.539 7.235
2024-12-02-13:01:26-root-INFO: Loss too large (729.573->732.351)! Learning rate decreased to 0.00985.
2024-12-02-13:01:26-root-INFO: grad norm: 69.281 68.470 10.569
2024-12-02-13:01:27-root-INFO: Loss too large (728.263->744.846)! Learning rate decreased to 0.00788.
2024-12-02-13:01:27-root-INFO: Loss too large (728.263->735.065)! Learning rate decreased to 0.00630.
2024-12-02-13:01:27-root-INFO: Loss too large (728.263->728.941)! Learning rate decreased to 0.00504.
2024-12-02-13:01:27-root-INFO: grad norm: 47.950 47.386 7.329
2024-12-02-13:01:28-root-INFO: grad norm: 25.234 24.542 5.870
2024-12-02-13:01:28-root-INFO: grad norm: 22.409 21.684 5.655
2024-12-02-13:01:29-root-INFO: grad norm: 20.668 19.934 5.460
2024-12-02-13:01:29-root-INFO: grad norm: 19.952 19.197 5.436
2024-12-02-13:01:30-root-INFO: grad norm: 19.515 18.773 5.329
2024-12-02-13:01:30-root-INFO: Loss Change: 729.573 -> 709.410
2024-12-02-13:01:30-root-INFO: Regularization Change: 0.000 -> 0.846
2024-12-02-13:01:30-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-13:01:30-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:01:30-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-13:01:30-root-INFO: grad norm: 27.649 26.884 6.458
2024-12-02-13:01:31-root-INFO: grad norm: 56.765 55.939 9.644
2024-12-02-13:01:31-root-INFO: Loss too large (707.305->726.673)! Learning rate decreased to 0.01021.
2024-12-02-13:01:31-root-INFO: Loss too large (707.305->716.190)! Learning rate decreased to 0.00816.
2024-12-02-13:01:31-root-INFO: Loss too large (707.305->709.675)! Learning rate decreased to 0.00653.
2024-12-02-13:01:32-root-INFO: grad norm: 48.370 47.762 7.645
2024-12-02-13:01:32-root-INFO: grad norm: 36.224 35.572 6.845
2024-12-02-13:01:33-root-INFO: grad norm: 33.204 32.615 6.229
2024-12-02-13:01:33-root-INFO: grad norm: 29.432 28.836 5.895
2024-12-02-13:01:34-root-INFO: grad norm: 27.839 27.243 5.730
2024-12-02-13:01:34-root-INFO: grad norm: 25.957 25.379 5.449
2024-12-02-13:01:34-root-INFO: Loss Change: 709.359 -> 688.063
2024-12-02-13:01:34-root-INFO: Regularization Change: 0.000 -> 1.239
2024-12-02-13:01:34-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-13:01:34-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-13:01:35-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-13:01:35-root-INFO: grad norm: 35.539 34.938 6.508
2024-12-02-13:01:35-root-INFO: Loss too large (688.219->692.871)! Learning rate decreased to 0.01057.
2024-12-02-13:01:35-root-INFO: grad norm: 65.899 65.177 9.733
2024-12-02-13:01:35-root-INFO: Loss too large (687.590->700.742)! Learning rate decreased to 0.00846.
2024-12-02-13:01:36-root-INFO: Loss too large (687.590->692.057)! Learning rate decreased to 0.00677.
2024-12-02-13:01:36-root-INFO: grad norm: 51.280 50.746 7.381
2024-12-02-13:01:37-root-INFO: grad norm: 28.737 28.068 6.165
2024-12-02-13:01:37-root-INFO: grad norm: 25.172 24.579 5.433
2024-12-02-13:01:38-root-INFO: grad norm: 22.030 21.381 5.307
2024-12-02-13:01:38-root-INFO: grad norm: 20.575 19.947 5.041
2024-12-02-13:01:38-root-INFO: grad norm: 19.352 18.706 4.959
2024-12-02-13:01:39-root-INFO: Loss Change: 688.219 -> 667.703
2024-12-02-13:01:39-root-INFO: Regularization Change: 0.000 -> 1.084
2024-12-02-13:01:39-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-13:01:39-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-13:01:39-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-13:01:39-root-INFO: grad norm: 32.365 31.542 7.251
2024-12-02-13:01:39-root-INFO: Loss too large (668.448->669.031)! Learning rate decreased to 0.01095.
2024-12-02-13:01:40-root-INFO: grad norm: 46.867 45.848 9.720
2024-12-02-13:01:40-root-INFO: Loss too large (666.348->669.837)! Learning rate decreased to 0.00876.
2024-12-02-13:01:40-root-INFO: grad norm: 47.958 47.090 9.080
2024-12-02-13:01:41-root-INFO: grad norm: 49.979 48.928 10.193
2024-12-02-13:01:41-root-INFO: grad norm: 50.155 49.278 9.337
2024-12-02-13:01:42-root-INFO: grad norm: 49.539 48.446 10.351
2024-12-02-13:01:42-root-INFO: grad norm: 49.089 48.204 9.279
2024-12-02-13:01:43-root-INFO: grad norm: 47.806 46.696 10.243
2024-12-02-13:01:43-root-INFO: Loss Change: 668.448 -> 650.914
2024-12-02-13:01:43-root-INFO: Regularization Change: 0.000 -> 1.551
2024-12-02-13:01:43-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-13:01:43-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-13:01:43-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-13:01:43-root-INFO: grad norm: 54.273 53.207 10.704
2024-12-02-13:01:43-root-INFO: Loss too large (652.574->664.642)! Learning rate decreased to 0.01134.
2024-12-02-13:01:44-root-INFO: grad norm: 73.185 71.722 14.563
2024-12-02-13:01:44-root-INFO: Loss too large (651.779->660.612)! Learning rate decreased to 0.00907.
2024-12-02-13:01:45-root-INFO: grad norm: 58.745 57.956 9.594
2024-12-02-13:01:45-root-INFO: grad norm: 38.601 37.376 9.645
2024-12-02-13:01:46-root-INFO: grad norm: 34.676 33.794 7.770
2024-12-02-13:01:46-root-INFO: grad norm: 31.402 30.201 8.603
2024-12-02-13:01:47-root-INFO: grad norm: 29.637 28.762 7.147
2024-12-02-13:01:47-root-INFO: grad norm: 28.204 27.033 8.044
2024-12-02-13:01:47-root-INFO: Loss Change: 652.574 -> 626.750
2024-12-02-13:01:47-root-INFO: Regularization Change: 0.000 -> 1.477
2024-12-02-13:01:47-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-13:01:47-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-13:01:48-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-13:01:48-root-INFO: grad norm: 39.258 38.166 9.197
2024-12-02-13:01:48-root-INFO: Loss too large (628.160->632.002)! Learning rate decreased to 0.01174.
2024-12-02-13:01:48-root-INFO: grad norm: 46.997 45.170 12.975
2024-12-02-13:01:49-root-INFO: grad norm: 57.723 55.926 14.292
2024-12-02-13:01:49-root-INFO: Loss too large (626.847->628.436)! Learning rate decreased to 0.00939.
2024-12-02-13:01:49-root-INFO: grad norm: 43.926 41.736 13.698
2024-12-02-13:01:50-root-INFO: grad norm: 32.112 31.211 7.556
2024-12-02-13:01:50-root-INFO: grad norm: 28.288 26.994 8.456
2024-12-02-13:01:51-root-INFO: grad norm: 25.814 24.969 6.554
2024-12-02-13:01:51-root-INFO: grad norm: 24.100 22.896 7.523
2024-12-02-13:01:52-root-INFO: Loss Change: 628.160 -> 606.956
2024-12-02-13:01:52-root-INFO: Regularization Change: 0.000 -> 1.502
2024-12-02-13:01:52-root-INFO: Undo step: 140
2024-12-02-13:01:52-root-INFO: Undo step: 141
2024-12-02-13:01:52-root-INFO: Undo step: 142
2024-12-02-13:01:52-root-INFO: Undo step: 143
2024-12-02-13:01:52-root-INFO: Undo step: 144
2024-12-02-13:01:52-root-INFO: Undo step: 145
2024-12-02-13:01:52-root-INFO: Undo step: 146
2024-12-02-13:01:52-root-INFO: Undo step: 147
2024-12-02-13:01:52-root-INFO: Undo step: 148
2024-12-02-13:01:52-root-INFO: Undo step: 149
2024-12-02-13:01:52-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-13:01:52-root-INFO: grad norm: 386.314 381.684 59.629
2024-12-02-13:01:52-root-INFO: grad norm: 426.252 423.421 49.051
2024-12-02-13:01:53-root-INFO: grad norm: 264.264 260.960 41.654
2024-12-02-13:01:54-root-INFO: grad norm: 372.616 369.828 45.492
2024-12-02-13:01:54-root-INFO: grad norm: 352.225 349.160 46.368
2024-12-02-13:01:54-root-INFO: Loss too large (1091.505->1442.863)! Learning rate decreased to 0.00823.
2024-12-02-13:01:54-root-INFO: Loss too large (1091.505->1272.544)! Learning rate decreased to 0.00659.
2024-12-02-13:01:55-root-INFO: Loss too large (1091.505->1161.735)! Learning rate decreased to 0.00527.
2024-12-02-13:01:55-root-INFO: grad norm: 202.701 200.865 27.218
2024-12-02-13:01:56-root-INFO: grad norm: 96.997 95.276 18.191
2024-12-02-13:01:56-root-INFO: grad norm: 69.934 67.870 16.865
2024-12-02-13:01:56-root-INFO: Loss Change: 1775.035 -> 877.048
2024-12-02-13:01:56-root-INFO: Regularization Change: 0.000 -> 40.792
2024-12-02-13:01:56-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-13:01:56-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:01:57-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-13:01:57-root-INFO: grad norm: 70.226 68.680 14.653
2024-12-02-13:01:57-root-INFO: grad norm: 183.534 182.081 23.042
2024-12-02-13:01:57-root-INFO: Loss too large (861.912->1001.004)! Learning rate decreased to 0.00854.
2024-12-02-13:01:58-root-INFO: Loss too large (861.912->939.535)! Learning rate decreased to 0.00683.
2024-12-02-13:01:58-root-INFO: Loss too large (861.912->899.093)! Learning rate decreased to 0.00546.
2024-12-02-13:01:58-root-INFO: Loss too large (861.912->872.603)! Learning rate decreased to 0.00437.
2024-12-02-13:01:58-root-INFO: grad norm: 101.483 100.525 13.907
2024-12-02-13:01:59-root-INFO: grad norm: 51.199 49.698 12.304
2024-12-02-13:01:59-root-INFO: grad norm: 43.870 42.217 11.928
2024-12-02-13:02:00-root-INFO: grad norm: 41.680 40.090 11.402
2024-12-02-13:02:00-root-INFO: grad norm: 40.090 38.532 11.070
2024-12-02-13:02:01-root-INFO: grad norm: 38.787 37.279 10.711
2024-12-02-13:02:01-root-INFO: Loss Change: 872.976 -> 783.739
2024-12-02-13:02:01-root-INFO: Regularization Change: 0.000 -> 3.546
2024-12-02-13:02:01-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-13:02:01-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:02:01-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-13:02:02-root-INFO: grad norm: 42.931 41.455 11.163
2024-12-02-13:02:02-root-INFO: grad norm: 60.510 59.333 11.877
2024-12-02-13:02:02-root-INFO: Loss too large (767.067->771.247)! Learning rate decreased to 0.00885.
2024-12-02-13:02:03-root-INFO: grad norm: 74.687 73.583 12.792
2024-12-02-13:02:03-root-INFO: grad norm: 164.808 163.678 19.268
2024-12-02-13:02:03-root-INFO: Loss too large (763.387->837.945)! Learning rate decreased to 0.00708.
2024-12-02-13:02:04-root-INFO: Loss too large (763.387->801.476)! Learning rate decreased to 0.00566.
2024-12-02-13:02:04-root-INFO: Loss too large (763.387->777.674)! Learning rate decreased to 0.00453.
2024-12-02-13:02:04-root-INFO: grad norm: 86.383 85.732 10.591
2024-12-02-13:02:05-root-INFO: grad norm: 39.592 38.656 8.556
2024-12-02-13:02:05-root-INFO: grad norm: 30.303 29.228 8.000
2024-12-02-13:02:06-root-INFO: grad norm: 28.906 27.824 7.835
2024-12-02-13:02:06-root-INFO: Loss Change: 779.710 -> 724.642
2024-12-02-13:02:06-root-INFO: Regularization Change: 0.000 -> 2.801
2024-12-02-13:02:06-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-13:02:06-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-13:02:06-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-13:02:06-root-INFO: grad norm: 32.144 31.131 8.004
2024-12-02-13:02:07-root-INFO: grad norm: 40.922 40.146 7.929
2024-12-02-13:02:07-root-INFO: grad norm: 61.710 60.974 9.505
2024-12-02-13:02:07-root-INFO: Loss too large (712.603->732.793)! Learning rate decreased to 0.00917.
2024-12-02-13:02:08-root-INFO: grad norm: 124.732 123.898 14.402
2024-12-02-13:02:08-root-INFO: Loss too large (711.809->754.933)! Learning rate decreased to 0.00734.
2024-12-02-13:02:08-root-INFO: Loss too large (711.809->732.143)! Learning rate decreased to 0.00587.
2024-12-02-13:02:08-root-INFO: Loss too large (711.809->717.367)! Learning rate decreased to 0.00470.
2024-12-02-13:02:09-root-INFO: grad norm: 66.109 65.549 8.588
2024-12-02-13:02:09-root-INFO: grad norm: 26.191 25.330 6.658
2024-12-02-13:02:10-root-INFO: grad norm: 23.300 22.409 6.381
2024-12-02-13:02:10-root-INFO: grad norm: 22.667 21.769 6.318
2024-12-02-13:02:10-root-INFO: Loss Change: 721.348 -> 686.301
2024-12-02-13:02:10-root-INFO: Regularization Change: 0.000 -> 1.926
2024-12-02-13:02:10-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-13:02:10-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:02:11-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-13:02:11-root-INFO: grad norm: 28.689 27.859 6.849
2024-12-02-13:02:11-root-INFO: grad norm: 40.861 40.087 7.915
2024-12-02-13:02:11-root-INFO: Loss too large (679.901->682.080)! Learning rate decreased to 0.00951.
2024-12-02-13:02:12-root-INFO: grad norm: 49.748 49.032 8.410
2024-12-02-13:02:12-root-INFO: grad norm: 86.835 86.081 11.417
2024-12-02-13:02:13-root-INFO: Loss too large (677.404->696.034)! Learning rate decreased to 0.00761.
2024-12-02-13:02:13-root-INFO: Loss too large (677.404->684.123)! Learning rate decreased to 0.00608.
2024-12-02-13:02:13-root-INFO: grad norm: 58.981 58.459 7.832
2024-12-02-13:02:14-root-INFO: grad norm: 22.374 21.613 5.783
2024-12-02-13:02:14-root-INFO: grad norm: 20.685 19.917 5.582
2024-12-02-13:02:15-root-INFO: grad norm: 19.841 19.076 5.457
2024-12-02-13:02:15-root-INFO: Loss Change: 684.768 -> 657.691
2024-12-02-13:02:15-root-INFO: Regularization Change: 0.000 -> 1.623
2024-12-02-13:02:15-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-13:02:15-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:02:15-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-13:02:15-root-INFO: grad norm: 31.907 31.136 6.971
2024-12-02-13:02:16-root-INFO: grad norm: 49.103 48.324 8.713
2024-12-02-13:02:16-root-INFO: Loss too large (654.676->661.737)! Learning rate decreased to 0.00985.
2024-12-02-13:02:16-root-INFO: Loss too large (654.676->655.725)! Learning rate decreased to 0.00788.
2024-12-02-13:02:17-root-INFO: grad norm: 43.529 42.900 7.378
2024-12-02-13:02:17-root-INFO: grad norm: 39.008 38.369 7.036
2024-12-02-13:02:18-root-INFO: grad norm: 37.355 36.776 6.547
2024-12-02-13:02:18-root-INFO: grad norm: 35.241 34.667 6.331
2024-12-02-13:02:18-root-INFO: grad norm: 34.477 33.925 6.144
2024-12-02-13:02:19-root-INFO: grad norm: 33.585 33.050 5.970
2024-12-02-13:02:19-root-INFO: Loss Change: 657.654 -> 635.268
2024-12-02-13:02:19-root-INFO: Regularization Change: 0.000 -> 1.550
2024-12-02-13:02:19-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-13:02:19-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:02:20-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-13:02:20-root-INFO: grad norm: 41.488 40.732 7.887
2024-12-02-13:02:20-root-INFO: Loss too large (635.548->638.590)! Learning rate decreased to 0.01021.
2024-12-02-13:02:20-root-INFO: grad norm: 55.320 54.463 9.697
2024-12-02-13:02:21-root-INFO: Loss too large (632.821->636.749)! Learning rate decreased to 0.00816.
2024-12-02-13:02:21-root-INFO: grad norm: 46.990 46.297 8.042
2024-12-02-13:02:21-root-INFO: grad norm: 33.466 32.641 7.384
2024-12-02-13:02:22-root-INFO: grad norm: 30.276 29.570 6.498
2024-12-02-13:02:22-root-INFO: grad norm: 26.750 25.987 6.343
2024-12-02-13:02:23-root-INFO: grad norm: 25.134 24.457 5.796
2024-12-02-13:02:23-root-INFO: grad norm: 23.458 22.742 5.750
2024-12-02-13:02:24-root-INFO: Loss Change: 635.548 -> 613.574
2024-12-02-13:02:24-root-INFO: Regularization Change: 0.000 -> 1.272
2024-12-02-13:02:24-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-13:02:24-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-13:02:24-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-13:02:24-root-INFO: grad norm: 33.153 32.335 7.321
2024-12-02-13:02:24-root-INFO: Loss too large (613.453->613.800)! Learning rate decreased to 0.01057.
2024-12-02-13:02:25-root-INFO: grad norm: 39.882 38.732 9.510
2024-12-02-13:02:25-root-INFO: grad norm: 46.804 45.469 11.100
2024-12-02-13:02:26-root-INFO: grad norm: 57.975 56.145 14.450
2024-12-02-13:02:26-root-INFO: Loss too large (609.666->612.723)! Learning rate decreased to 0.00846.
2024-12-02-13:02:27-root-INFO: grad norm: 46.221 45.058 10.302
2024-12-02-13:02:27-root-INFO: grad norm: 33.308 31.866 9.692
2024-12-02-13:02:27-root-INFO: grad norm: 28.942 27.852 7.868
2024-12-02-13:02:28-root-INFO: grad norm: 25.401 24.173 7.801
2024-12-02-13:02:28-root-INFO: Loss Change: 613.453 -> 594.608
2024-12-02-13:02:28-root-INFO: Regularization Change: 0.000 -> 1.291
2024-12-02-13:02:28-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-13:02:28-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-13:02:28-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-13:02:29-root-INFO: grad norm: 40.015 38.301 11.586
2024-12-02-13:02:29-root-INFO: Loss too large (596.257->598.435)! Learning rate decreased to 0.01095.
2024-12-02-13:02:29-root-INFO: grad norm: 44.053 41.812 13.871
2024-12-02-13:02:30-root-INFO: grad norm: 50.123 47.845 14.937
2024-12-02-13:02:30-root-INFO: grad norm: 54.589 51.925 16.844
2024-12-02-13:02:31-root-INFO: grad norm: 55.993 53.885 15.219
2024-12-02-13:02:31-root-INFO: grad norm: 59.310 56.980 16.459
2024-12-02-13:02:32-root-INFO: grad norm: 59.096 57.324 14.365
2024-12-02-13:02:32-root-INFO: grad norm: 60.222 58.195 15.494
2024-12-02-13:02:32-root-INFO: Loss too large (585.005->585.684)! Learning rate decreased to 0.00876.
2024-12-02-13:02:33-root-INFO: Loss Change: 596.257 -> 579.860
2024-12-02-13:02:33-root-INFO: Regularization Change: 0.000 -> 1.558
2024-12-02-13:02:33-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-13:02:33-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-13:02:33-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-13:02:33-root-INFO: grad norm: 48.062 46.944 10.307
2024-12-02-13:02:33-root-INFO: Loss too large (580.994->582.887)! Learning rate decreased to 0.01134.
2024-12-02-13:02:34-root-INFO: grad norm: 44.839 43.344 11.481
2024-12-02-13:02:34-root-INFO: grad norm: 45.683 44.228 11.439
2024-12-02-13:02:35-root-INFO: grad norm: 45.737 43.826 13.083
2024-12-02-13:02:35-root-INFO: grad norm: 46.581 44.879 12.477
2024-12-02-13:02:36-root-INFO: grad norm: 46.689 44.574 13.893
2024-12-02-13:02:36-root-INFO: grad norm: 46.667 44.927 12.624
2024-12-02-13:02:37-root-INFO: grad norm: 46.666 44.579 13.802
2024-12-02-13:02:37-root-INFO: Loss Change: 580.994 -> 563.313
2024-12-02-13:02:37-root-INFO: Regularization Change: 0.000 -> 1.488
2024-12-02-13:02:37-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-13:02:37-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-13:02:37-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-13:02:37-root-INFO: grad norm: 57.320 55.326 14.988
2024-12-02-13:02:37-root-INFO: Loss too large (566.176->571.675)! Learning rate decreased to 0.01174.
2024-12-02-13:02:38-root-INFO: grad norm: 51.664 49.300 15.447
2024-12-02-13:02:38-root-INFO: grad norm: 46.992 45.426 12.031
2024-12-02-13:02:39-root-INFO: grad norm: 44.683 42.813 12.790
2024-12-02-13:02:39-root-INFO: grad norm: 42.986 41.523 11.119
2024-12-02-13:02:40-root-INFO: grad norm: 41.821 40.033 12.098
2024-12-02-13:02:40-root-INFO: grad norm: 41.030 39.573 10.836
2024-12-02-13:02:41-root-INFO: grad norm: 40.463 38.666 11.924
2024-12-02-13:02:41-root-INFO: Loss Change: 566.176 -> 545.386
2024-12-02-13:02:41-root-INFO: Regularization Change: 0.000 -> 1.462
2024-12-02-13:02:41-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-13:02:41-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-13:02:41-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-13:02:41-root-INFO: grad norm: 49.748 48.096 12.717
2024-12-02-13:02:42-root-INFO: Loss too large (548.051->551.815)! Learning rate decreased to 0.01215.
2024-12-02-13:02:42-root-INFO: grad norm: 45.735 43.815 13.114
2024-12-02-13:02:43-root-INFO: grad norm: 42.712 41.264 11.025
2024-12-02-13:02:43-root-INFO: grad norm: 40.548 38.845 11.628
2024-12-02-13:02:44-root-INFO: grad norm: 38.828 37.470 10.179
2024-12-02-13:02:44-root-INFO: grad norm: 37.509 35.882 10.926
2024-12-02-13:02:44-root-INFO: grad norm: 36.506 35.175 9.769
2024-12-02-13:02:45-root-INFO: grad norm: 35.736 34.122 10.618
2024-12-02-13:02:45-root-INFO: Loss Change: 548.051 -> 529.354
2024-12-02-13:02:45-root-INFO: Regularization Change: 0.000 -> 1.426
2024-12-02-13:02:45-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-13:02:45-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-13:02:45-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-13:02:46-root-INFO: grad norm: 43.247 41.692 11.492
2024-12-02-13:02:46-root-INFO: Loss too large (530.683->533.378)! Learning rate decreased to 0.01258.
2024-12-02-13:02:46-root-INFO: grad norm: 39.923 38.111 11.890
2024-12-02-13:02:47-root-INFO: grad norm: 37.115 35.803 9.782
2024-12-02-13:02:47-root-INFO: grad norm: 35.171 33.654 10.218
2024-12-02-13:02:48-root-INFO: grad norm: 33.568 32.385 8.833
2024-12-02-13:02:48-root-INFO: grad norm: 32.356 30.967 9.380
2024-12-02-13:02:49-root-INFO: grad norm: 31.420 30.292 8.342
2024-12-02-13:02:49-root-INFO: grad norm: 30.702 29.357 8.987
2024-12-02-13:02:49-root-INFO: Loss Change: 530.683 -> 513.890
2024-12-02-13:02:49-root-INFO: Regularization Change: 0.000 -> 1.378
2024-12-02-13:02:49-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-13:02:49-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-13:02:50-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-13:02:50-root-INFO: grad norm: 45.310 43.487 12.722
2024-12-02-13:02:50-root-INFO: Loss too large (517.187->519.080)! Learning rate decreased to 0.01302.
2024-12-02-13:02:50-root-INFO: grad norm: 39.897 38.143 11.702
2024-12-02-13:02:51-root-INFO: grad norm: 35.943 34.726 9.274
2024-12-02-13:02:51-root-INFO: grad norm: 33.682 32.486 8.893
2024-12-02-13:02:52-root-INFO: grad norm: 31.783 30.845 7.665
2024-12-02-13:02:52-root-INFO: grad norm: 30.400 29.421 7.655
2024-12-02-13:02:53-root-INFO: grad norm: 29.253 28.446 6.822
2024-12-02-13:02:53-root-INFO: grad norm: 28.340 27.456 7.025
2024-12-02-13:02:54-root-INFO: Loss Change: 517.187 -> 498.882
2024-12-02-13:02:54-root-INFO: Regularization Change: 0.000 -> 1.460
2024-12-02-13:02:54-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-13:02:54-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-13:02:54-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-13:02:54-root-INFO: grad norm: 31.697 30.918 6.983
2024-12-02-13:02:54-root-INFO: Loss too large (499.406->499.799)! Learning rate decreased to 0.01347.
2024-12-02-13:02:55-root-INFO: grad norm: 29.882 29.044 7.025
2024-12-02-13:02:55-root-INFO: grad norm: 28.639 27.937 6.302
2024-12-02-13:02:56-root-INFO: grad norm: 27.516 26.731 6.528
2024-12-02-13:02:56-root-INFO: grad norm: 26.611 25.953 5.878
2024-12-02-13:02:57-root-INFO: grad norm: 25.763 25.017 6.155
2024-12-02-13:02:57-root-INFO: grad norm: 25.104 24.477 5.575
2024-12-02-13:02:58-root-INFO: grad norm: 24.499 23.780 5.890
2024-12-02-13:02:58-root-INFO: Loss Change: 499.406 -> 485.363
2024-12-02-13:02:58-root-INFO: Regularization Change: 0.000 -> 1.353
2024-12-02-13:02:58-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-13:02:58-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-13:02:58-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-13:02:58-root-INFO: grad norm: 31.745 30.948 7.069
2024-12-02-13:02:58-root-INFO: Loss too large (485.940->486.266)! Learning rate decreased to 0.01393.
2024-12-02-13:02:59-root-INFO: grad norm: 29.674 28.853 6.930
2024-12-02-13:02:59-root-INFO: grad norm: 28.405 27.714 6.228
2024-12-02-13:03:00-root-INFO: grad norm: 27.192 26.422 6.425
2024-12-02-13:03:00-root-INFO: grad norm: 26.261 25.620 5.768
2024-12-02-13:03:01-root-INFO: grad norm: 25.319 24.590 6.033
2024-12-02-13:03:01-root-INFO: grad norm: 24.622 24.013 5.442
2024-12-02-13:03:02-root-INFO: grad norm: 23.940 23.239 5.750
2024-12-02-13:03:02-root-INFO: Loss Change: 485.940 -> 472.051
2024-12-02-13:03:02-root-INFO: Regularization Change: 0.000 -> 1.357
2024-12-02-13:03:02-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-13:03:02-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-13:03:02-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-13:03:03-root-INFO: grad norm: 31.462 30.581 7.393
2024-12-02-13:03:03-root-INFO: Loss too large (472.884->473.308)! Learning rate decreased to 0.01441.
2024-12-02-13:03:03-root-INFO: grad norm: 28.977 28.102 7.067
2024-12-02-13:03:04-root-INFO: grad norm: 27.388 26.679 6.192
2024-12-02-13:03:04-root-INFO: grad norm: 26.013 25.278 6.137
2024-12-02-13:03:04-root-INFO: grad norm: 24.908 24.305 5.451
2024-12-02-13:03:05-root-INFO: grad norm: 23.937 23.299 5.490
2024-12-02-13:03:05-root-INFO: grad norm: 23.169 22.635 4.945
2024-12-02-13:03:06-root-INFO: grad norm: 22.512 21.937 5.053
2024-12-02-13:03:06-root-INFO: Loss Change: 472.884 -> 459.285
2024-12-02-13:03:06-root-INFO: Regularization Change: 0.000 -> 1.349
2024-12-02-13:03:06-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-13:03:06-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-13:03:06-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-13:03:07-root-INFO: grad norm: 30.530 29.755 6.835
2024-12-02-13:03:07-root-INFO: Loss too large (460.789->461.179)! Learning rate decreased to 0.01490.
2024-12-02-13:03:07-root-INFO: grad norm: 28.393 27.679 6.329
2024-12-02-13:03:08-root-INFO: grad norm: 26.991 26.396 5.632
2024-12-02-13:03:08-root-INFO: grad norm: 25.682 25.078 5.536
2024-12-02-13:03:09-root-INFO: grad norm: 24.666 24.161 4.964
2024-12-02-13:03:09-root-INFO: grad norm: 23.675 23.148 4.968
2024-12-02-13:03:10-root-INFO: grad norm: 22.937 22.489 4.511
2024-12-02-13:03:10-root-INFO: grad norm: 22.227 21.751 4.575
2024-12-02-13:03:10-root-INFO: Loss Change: 460.789 -> 447.646
2024-12-02-13:03:10-root-INFO: Regularization Change: 0.000 -> 1.352
2024-12-02-13:03:10-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-13:03:10-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-13:03:11-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-13:03:11-root-INFO: grad norm: 26.759 26.166 5.602
2024-12-02-13:03:11-root-INFO: grad norm: 36.972 36.248 7.285
2024-12-02-13:03:12-root-INFO: Loss too large (447.936->451.404)! Learning rate decreased to 0.01541.
2024-12-02-13:03:12-root-INFO: grad norm: 32.315 31.736 6.091
2024-12-02-13:03:13-root-INFO: grad norm: 26.690 26.111 5.529
2024-12-02-13:03:13-root-INFO: grad norm: 24.690 24.226 4.765
2024-12-02-13:03:14-root-INFO: grad norm: 22.708 22.231 4.630
2024-12-02-13:03:14-root-INFO: grad norm: 21.529 21.131 4.118
2024-12-02-13:03:15-root-INFO: grad norm: 20.433 20.016 4.108
2024-12-02-13:03:15-root-INFO: Loss Change: 448.059 -> 435.551
2024-12-02-13:03:15-root-INFO: Regularization Change: 0.000 -> 1.417
2024-12-02-13:03:15-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-13:03:15-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-13:03:15-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-13:03:15-root-INFO: grad norm: 26.189 25.566 5.682
2024-12-02-13:03:16-root-INFO: grad norm: 34.899 34.262 6.638
2024-12-02-13:03:16-root-INFO: Loss too large (436.251->439.230)! Learning rate decreased to 0.01593.
2024-12-02-13:03:16-root-INFO: grad norm: 31.068 30.571 5.534
2024-12-02-13:03:17-root-INFO: grad norm: 26.967 26.512 4.936
2024-12-02-13:03:18-root-INFO: grad norm: 24.808 24.434 4.289
2024-12-02-13:03:18-root-INFO: grad norm: 22.689 22.322 4.063
2024-12-02-13:03:19-root-INFO: grad norm: 21.419 21.104 3.663
2024-12-02-13:03:19-root-INFO: grad norm: 20.252 19.928 3.607
2024-12-02-13:03:19-root-INFO: Loss Change: 436.662 -> 424.389
2024-12-02-13:03:19-root-INFO: Regularization Change: 0.000 -> 1.445
2024-12-02-13:03:19-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-13:03:19-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-13:03:20-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-13:03:20-root-INFO: grad norm: 24.595 24.128 4.766
2024-12-02-13:03:20-root-INFO: grad norm: 33.858 33.402 5.540
2024-12-02-13:03:20-root-INFO: Loss too large (424.915->427.922)! Learning rate decreased to 0.01647.
2024-12-02-13:03:21-root-INFO: grad norm: 29.546 29.148 4.835
2024-12-02-13:03:21-root-INFO: grad norm: 24.502 24.145 4.167
2024-12-02-13:03:22-root-INFO: grad norm: 22.615 22.298 3.775
2024-12-02-13:03:22-root-INFO: grad norm: 20.756 20.449 3.556
2024-12-02-13:03:23-root-INFO: grad norm: 19.757 19.471 3.351
2024-12-02-13:03:23-root-INFO: grad norm: 18.854 18.571 3.253
2024-12-02-13:03:24-root-INFO: Loss Change: 425.044 -> 413.669
2024-12-02-13:03:24-root-INFO: Regularization Change: 0.000 -> 1.393
2024-12-02-13:03:24-root-INFO: Undo step: 130
2024-12-02-13:03:24-root-INFO: Undo step: 131
2024-12-02-13:03:24-root-INFO: Undo step: 132
2024-12-02-13:03:24-root-INFO: Undo step: 133
2024-12-02-13:03:24-root-INFO: Undo step: 134
2024-12-02-13:03:24-root-INFO: Undo step: 135
2024-12-02-13:03:24-root-INFO: Undo step: 136
2024-12-02-13:03:24-root-INFO: Undo step: 137
2024-12-02-13:03:24-root-INFO: Undo step: 138
2024-12-02-13:03:24-root-INFO: Undo step: 139
2024-12-02-13:03:24-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-13:03:24-root-INFO: grad norm: 367.860 362.960 59.841
2024-12-02-13:03:25-root-INFO: grad norm: 315.206 312.030 44.627
2024-12-02-13:03:25-root-INFO: grad norm: 160.212 156.851 32.645
2024-12-02-13:03:25-root-INFO: grad norm: 124.277 121.724 25.063
2024-12-02-13:03:26-root-INFO: grad norm: 97.490 95.162 21.175
2024-12-02-13:03:26-root-INFO: grad norm: 81.884 80.333 15.862
2024-12-02-13:03:27-root-INFO: grad norm: 66.661 64.945 15.027
2024-12-02-13:03:27-root-INFO: grad norm: 57.795 56.632 11.536
2024-12-02-13:03:28-root-INFO: Loss Change: 1662.419 -> 578.852
2024-12-02-13:03:28-root-INFO: Regularization Change: 0.000 -> 62.387
2024-12-02-13:03:28-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-13:03:28-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-13:03:28-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-13:03:28-root-INFO: grad norm: 46.178 44.459 12.481
2024-12-02-13:03:29-root-INFO: grad norm: 40.839 39.584 10.044
2024-12-02-13:03:29-root-INFO: grad norm: 36.907 35.537 9.964
2024-12-02-13:03:29-root-INFO: grad norm: 34.460 33.453 8.269
2024-12-02-13:03:30-root-INFO: grad norm: 32.896 31.771 8.530
2024-12-02-13:03:30-root-INFO: grad norm: 32.263 31.424 7.307
2024-12-02-13:03:31-root-INFO: grad norm: 32.568 31.641 7.719
2024-12-02-13:03:31-root-INFO: grad norm: 33.671 32.970 6.836
2024-12-02-13:03:32-root-INFO: Loss Change: 574.848 -> 504.637
2024-12-02-13:03:32-root-INFO: Regularization Change: 0.000 -> 8.245
2024-12-02-13:03:32-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-13:03:32-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-13:03:32-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-13:03:32-root-INFO: grad norm: 33.977 32.856 8.657
2024-12-02-13:03:33-root-INFO: grad norm: 34.613 33.743 7.712
2024-12-02-13:03:33-root-INFO: grad norm: 36.964 36.188 7.534
2024-12-02-13:03:34-root-INFO: grad norm: 40.007 39.411 6.881
2024-12-02-13:03:34-root-INFO: grad norm: 44.329 43.773 6.998
2024-12-02-13:03:35-root-INFO: grad norm: 48.061 47.612 6.553
2024-12-02-13:03:35-root-INFO: grad norm: 52.040 51.589 6.835
2024-12-02-13:03:36-root-INFO: grad norm: 54.420 54.030 6.506
2024-12-02-13:03:36-root-INFO: Loss Change: 502.769 -> 480.107
2024-12-02-13:03:36-root-INFO: Regularization Change: 0.000 -> 3.937
2024-12-02-13:03:36-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-13:03:36-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-13:03:36-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-13:03:36-root-INFO: grad norm: 51.958 51.114 9.331
2024-12-02-13:03:37-root-INFO: grad norm: 52.463 51.775 8.471
2024-12-02-13:03:37-root-INFO: grad norm: 53.625 53.093 7.533
2024-12-02-13:03:38-root-INFO: grad norm: 54.436 53.972 7.092
2024-12-02-13:03:38-root-INFO: grad norm: 54.619 54.189 6.841
2024-12-02-13:03:39-root-INFO: grad norm: 54.499 54.114 6.462
2024-12-02-13:03:39-root-INFO: grad norm: 53.982 53.590 6.496
2024-12-02-13:03:40-root-INFO: grad norm: 53.515 53.157 6.179
2024-12-02-13:03:40-root-INFO: Loss Change: 476.505 -> 456.850
2024-12-02-13:03:40-root-INFO: Regularization Change: 0.000 -> 2.767
2024-12-02-13:03:40-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-13:03:40-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-13:03:40-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-13:03:40-root-INFO: grad norm: 50.150 49.815 5.792
2024-12-02-13:03:41-root-INFO: grad norm: 50.694 50.269 6.547
2024-12-02-13:03:41-root-INFO: grad norm: 51.120 50.750 6.145
2024-12-02-13:03:42-root-INFO: grad norm: 51.477 51.063 6.514
2024-12-02-13:03:42-root-INFO: grad norm: 51.680 51.290 6.334
2024-12-02-13:03:43-root-INFO: grad norm: 51.730 51.301 6.648
2024-12-02-13:03:43-root-INFO: grad norm: 51.696 51.284 6.516
2024-12-02-13:03:44-root-INFO: grad norm: 51.583 51.130 6.814
2024-12-02-13:03:44-root-INFO: Loss Change: 455.167 -> 440.363
2024-12-02-13:03:44-root-INFO: Regularization Change: 0.000 -> 2.138
2024-12-02-13:03:44-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-13:03:44-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-13:03:44-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-13:03:44-root-INFO: grad norm: 47.143 46.753 6.054
2024-12-02-13:03:45-root-INFO: Loss too large (437.411->437.495)! Learning rate decreased to 0.01393.
2024-12-02-13:03:45-root-INFO: grad norm: 32.923 32.489 5.331
2024-12-02-13:03:45-root-INFO: grad norm: 22.727 22.433 3.644
2024-12-02-13:03:46-root-INFO: grad norm: 17.823 17.526 3.240
2024-12-02-13:03:47-root-INFO: grad norm: 14.529 14.238 2.895
2024-12-02-13:03:47-root-INFO: grad norm: 12.647 12.350 2.724
2024-12-02-13:03:48-root-INFO: grad norm: 11.410 11.102 2.636
2024-12-02-13:03:48-root-INFO: grad norm: 10.654 10.345 2.551
2024-12-02-13:03:48-root-INFO: Loss Change: 437.411 -> 417.786
2024-12-02-13:03:48-root-INFO: Regularization Change: 0.000 -> 1.356
2024-12-02-13:03:48-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-13:03:49-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-13:03:49-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-13:03:49-root-INFO: grad norm: 16.285 15.761 4.097
2024-12-02-13:03:49-root-INFO: grad norm: 15.145 14.795 3.238
2024-12-02-13:03:50-root-INFO: grad norm: 16.165 15.893 2.957
2024-12-02-13:03:50-root-INFO: grad norm: 17.995 17.719 3.141
2024-12-02-13:03:51-root-INFO: grad norm: 20.488 20.268 2.994
2024-12-02-13:03:51-root-INFO: grad norm: 23.606 23.350 3.466
2024-12-02-13:03:52-root-INFO: grad norm: 27.375 27.154 3.470
2024-12-02-13:03:52-root-INFO: grad norm: 31.377 31.096 4.191
2024-12-02-13:03:53-root-INFO: Loss Change: 417.547 -> 409.554
2024-12-02-13:03:53-root-INFO: Regularization Change: 0.000 -> 1.812
2024-12-02-13:03:53-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-13:03:53-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-13:03:53-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-13:03:53-root-INFO: grad norm: 47.186 46.707 6.707
2024-12-02-13:03:53-root-INFO: Loss too large (413.499->413.852)! Learning rate decreased to 0.01490.
2024-12-02-13:03:54-root-INFO: grad norm: 33.663 33.360 4.504
2024-12-02-13:03:54-root-INFO: grad norm: 24.921 24.696 3.347
2024-12-02-13:03:55-root-INFO: grad norm: 19.501 19.246 3.147
2024-12-02-13:03:55-root-INFO: grad norm: 15.849 15.631 2.620
2024-12-02-13:03:56-root-INFO: grad norm: 13.461 13.200 2.636
2024-12-02-13:03:56-root-INFO: grad norm: 11.808 11.569 2.366
2024-12-02-13:03:57-root-INFO: grad norm: 10.710 10.435 2.409
2024-12-02-13:03:57-root-INFO: Loss Change: 413.499 -> 394.993
2024-12-02-13:03:57-root-INFO: Regularization Change: 0.000 -> 1.207
2024-12-02-13:03:57-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-13:03:57-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-13:03:57-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-13:03:57-root-INFO: grad norm: 17.353 16.966 3.647
2024-12-02-13:03:58-root-INFO: grad norm: 18.359 18.085 3.159
2024-12-02-13:03:58-root-INFO: grad norm: 21.492 21.274 3.048
2024-12-02-13:03:59-root-INFO: grad norm: 25.485 25.213 3.713
2024-12-02-13:03:59-root-INFO: grad norm: 30.768 30.521 3.890
2024-12-02-13:03:59-root-INFO: Loss too large (392.505->393.240)! Learning rate decreased to 0.01541.
2024-12-02-13:04:00-root-INFO: grad norm: 24.401 24.112 3.747
2024-12-02-13:04:00-root-INFO: grad norm: 19.517 19.299 2.911
2024-12-02-13:04:01-root-INFO: grad norm: 16.455 16.189 2.946
2024-12-02-13:04:01-root-INFO: Loss Change: 395.303 -> 385.412
2024-12-02-13:04:01-root-INFO: Regularization Change: 0.000 -> 1.304
2024-12-02-13:04:01-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-13:04:01-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-13:04:01-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-13:04:01-root-INFO: grad norm: 24.526 24.095 4.582
2024-12-02-13:04:02-root-INFO: grad norm: 27.736 27.450 3.976
2024-12-02-13:04:02-root-INFO: Loss too large (386.150->386.178)! Learning rate decreased to 0.01593.
2024-12-02-13:04:03-root-INFO: grad norm: 22.008 21.773 3.205
2024-12-02-13:04:03-root-INFO: grad norm: 18.206 17.961 2.977
2024-12-02-13:04:04-root-INFO: grad norm: 15.364 15.143 2.595
2024-12-02-13:04:04-root-INFO: grad norm: 13.436 13.192 2.551
2024-12-02-13:04:05-root-INFO: grad norm: 11.976 11.747 2.327
2024-12-02-13:04:05-root-INFO: grad norm: 10.983 10.732 2.331
2024-12-02-13:04:05-root-INFO: Loss Change: 386.966 -> 376.269
2024-12-02-13:04:05-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-02-13:04:05-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-13:04:05-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-13:04:06-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-13:04:06-root-INFO: grad norm: 17.136 16.776 3.495
2024-12-02-13:04:06-root-INFO: grad norm: 18.975 18.691 3.269
2024-12-02-13:04:07-root-INFO: grad norm: 23.861 23.626 3.339
2024-12-02-13:04:07-root-INFO: Loss too large (375.488->376.059)! Learning rate decreased to 0.01647.
2024-12-02-13:04:07-root-INFO: grad norm: 20.550 20.273 3.367
2024-12-02-13:04:08-root-INFO: grad norm: 17.824 17.602 2.803
2024-12-02-13:04:08-root-INFO: grad norm: 16.022 15.758 2.895
2024-12-02-13:04:09-root-INFO: grad norm: 14.486 14.261 2.541
2024-12-02-13:04:09-root-INFO: grad norm: 13.468 13.211 2.621
2024-12-02-13:04:09-root-INFO: Loss Change: 377.025 -> 368.350
2024-12-02-13:04:09-root-INFO: Regularization Change: 0.000 -> 1.122
2024-12-02-13:04:09-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-13:04:09-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-13:04:10-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-13:04:10-root-INFO: grad norm: 22.385 21.989 4.192
2024-12-02-13:04:10-root-INFO: grad norm: 25.732 25.443 3.848
2024-12-02-13:04:11-root-INFO: Loss too large (368.970->369.302)! Learning rate decreased to 0.01702.
2024-12-02-13:04:11-root-INFO: grad norm: 21.405 21.170 3.164
2024-12-02-13:04:11-root-INFO: grad norm: 18.542 18.292 3.037
2024-12-02-13:04:12-root-INFO: grad norm: 16.255 16.032 2.685
2024-12-02-13:04:12-root-INFO: grad norm: 14.705 14.462 2.658
2024-12-02-13:04:13-root-INFO: grad norm: 13.435 13.212 2.440
2024-12-02-13:04:13-root-INFO: grad norm: 12.581 12.342 2.440
2024-12-02-13:04:14-root-INFO: Loss Change: 369.599 -> 360.200
2024-12-02-13:04:14-root-INFO: Regularization Change: 0.000 -> 1.085
2024-12-02-13:04:14-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-13:04:14-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-13:04:14-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-13:04:14-root-INFO: grad norm: 19.927 19.540 3.907
2024-12-02-13:04:15-root-INFO: grad norm: 22.879 22.627 3.385
2024-12-02-13:04:15-root-INFO: Loss too large (360.277->360.470)! Learning rate decreased to 0.01759.
2024-12-02-13:04:15-root-INFO: grad norm: 19.275 19.064 2.844
2024-12-02-13:04:16-root-INFO: grad norm: 16.817 16.591 2.747
2024-12-02-13:04:16-root-INFO: grad norm: 14.960 14.753 2.478
2024-12-02-13:04:17-root-INFO: grad norm: 13.652 13.425 2.478
2024-12-02-13:04:17-root-INFO: grad norm: 12.641 12.430 2.301
2024-12-02-13:04:17-root-INFO: grad norm: 11.948 11.719 2.325
2024-12-02-13:04:18-root-INFO: Loss Change: 361.035 -> 352.362
2024-12-02-13:04:18-root-INFO: Regularization Change: 0.000 -> 1.068
2024-12-02-13:04:18-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-13:04:18-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-13:04:18-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-13:04:18-root-INFO: grad norm: 18.514 18.221 3.282
2024-12-02-13:04:19-root-INFO: grad norm: 22.601 22.319 3.564
2024-12-02-13:04:19-root-INFO: Loss too large (353.086->353.520)! Learning rate decreased to 0.01817.
2024-12-02-13:04:19-root-INFO: grad norm: 19.856 19.634 2.963
2024-12-02-13:04:20-root-INFO: grad norm: 17.962 17.705 3.028
2024-12-02-13:04:20-root-INFO: grad norm: 16.259 16.045 2.626
2024-12-02-13:04:21-root-INFO: grad norm: 15.104 14.859 2.709
2024-12-02-13:04:21-root-INFO: grad norm: 14.072 13.861 2.427
2024-12-02-13:04:21-root-INFO: grad norm: 13.380 13.143 2.508
2024-12-02-13:04:22-root-INFO: Loss Change: 353.346 -> 345.506
2024-12-02-13:04:22-root-INFO: Regularization Change: 0.000 -> 1.047
2024-12-02-13:04:22-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-13:04:22-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-13:04:22-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-13:04:22-root-INFO: grad norm: 16.993 16.691 3.187
2024-12-02-13:04:23-root-INFO: grad norm: 20.369 20.098 3.314
2024-12-02-13:04:23-root-INFO: Loss too large (345.676->346.000)! Learning rate decreased to 0.01877.
2024-12-02-13:04:23-root-INFO: grad norm: 18.264 18.051 2.779
2024-12-02-13:04:24-root-INFO: grad norm: 16.844 16.597 2.873
2024-12-02-13:04:24-root-INFO: grad norm: 15.594 15.386 2.542
2024-12-02-13:04:25-root-INFO: grad norm: 14.722 14.485 2.634
2024-12-02-13:04:25-root-INFO: grad norm: 13.946 13.738 2.398
2024-12-02-13:04:26-root-INFO: grad norm: 13.415 13.184 2.479
2024-12-02-13:04:26-root-INFO: Loss Change: 346.163 -> 338.812
2024-12-02-13:04:26-root-INFO: Regularization Change: 0.000 -> 1.038
2024-12-02-13:04:26-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-13:04:26-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-13:04:26-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-13:04:26-root-INFO: grad norm: 19.552 19.215 3.618
2024-12-02-13:04:27-root-INFO: grad norm: 23.525 23.268 3.469
2024-12-02-13:04:27-root-INFO: Loss too large (339.329->340.103)! Learning rate decreased to 0.01939.
2024-12-02-13:04:27-root-INFO: grad norm: 20.538 20.323 2.965
2024-12-02-13:04:28-root-INFO: grad norm: 18.485 18.255 2.906
2024-12-02-13:04:28-root-INFO: grad norm: 16.743 16.538 2.611
2024-12-02-13:04:29-root-INFO: grad norm: 15.527 15.306 2.611
2024-12-02-13:04:29-root-INFO: grad norm: 14.499 14.296 2.415
2024-12-02-13:04:30-root-INFO: grad norm: 13.798 13.582 2.436
2024-12-02-13:04:30-root-INFO: Loss Change: 339.563 -> 331.801
2024-12-02-13:04:30-root-INFO: Regularization Change: 0.000 -> 1.061
2024-12-02-13:04:30-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-13:04:30-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-13:04:30-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-13:04:30-root-INFO: grad norm: 25.108 24.694 4.540
2024-12-02-13:04:30-root-INFO: Loss too large (334.054->334.446)! Learning rate decreased to 0.02013.
2024-12-02-13:04:31-root-INFO: grad norm: 21.215 20.977 3.170
2024-12-02-13:04:31-root-INFO: grad norm: 19.187 18.967 2.896
2024-12-02-13:04:32-root-INFO: grad norm: 17.678 17.453 2.806
2024-12-02-13:04:32-root-INFO: grad norm: 16.416 16.207 2.612
2024-12-02-13:04:33-root-INFO: grad norm: 15.489 15.272 2.588
2024-12-02-13:04:33-root-INFO: grad norm: 14.711 14.504 2.460
2024-12-02-13:04:34-root-INFO: grad norm: 14.164 13.950 2.451
2024-12-02-13:04:34-root-INFO: Loss Change: 334.054 -> 324.998
2024-12-02-13:04:34-root-INFO: Regularization Change: 0.000 -> 1.074
2024-12-02-13:04:34-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-13:04:34-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-13:04:34-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-13:04:34-root-INFO: grad norm: 18.956 18.673 3.263
2024-12-02-13:04:35-root-INFO: Loss too large (325.910->326.300)! Learning rate decreased to 0.02078.
2024-12-02-13:04:35-root-INFO: grad norm: 17.276 17.053 2.762
2024-12-02-13:04:36-root-INFO: grad norm: 16.290 16.085 2.578
2024-12-02-13:04:36-root-INFO: grad norm: 15.558 15.342 2.582
2024-12-02-13:04:37-root-INFO: grad norm: 14.987 14.785 2.446
2024-12-02-13:04:37-root-INFO: grad norm: 14.579 14.365 2.487
2024-12-02-13:04:38-root-INFO: grad norm: 14.276 14.075 2.385
2024-12-02-13:04:38-root-INFO: grad norm: 14.079 13.866 2.435
2024-12-02-13:04:38-root-INFO: Loss Change: 325.910 -> 318.913
2024-12-02-13:04:38-root-INFO: Regularization Change: 0.000 -> 1.015
2024-12-02-13:04:38-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-13:04:39-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-13:04:39-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-13:04:39-root-INFO: grad norm: 19.647 19.374 3.269
2024-12-02-13:04:39-root-INFO: Loss too large (319.869->320.502)! Learning rate decreased to 0.02145.
2024-12-02-13:04:40-root-INFO: grad norm: 17.929 17.704 2.827
2024-12-02-13:04:40-root-INFO: grad norm: 16.864 16.658 2.631
2024-12-02-13:04:41-root-INFO: grad norm: 16.063 15.847 2.621
2024-12-02-13:04:41-root-INFO: grad norm: 15.355 15.154 2.477
2024-12-02-13:04:42-root-INFO: grad norm: 14.850 14.640 2.485
2024-12-02-13:04:42-root-INFO: grad norm: 14.416 14.217 2.386
2024-12-02-13:04:43-root-INFO: grad norm: 14.125 13.920 2.399
2024-12-02-13:04:43-root-INFO: Loss Change: 319.869 -> 312.854
2024-12-02-13:04:43-root-INFO: Regularization Change: 0.000 -> 1.023
2024-12-02-13:04:43-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-13:04:43-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-13:04:43-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-13:04:43-root-INFO: grad norm: 19.175 18.901 3.231
2024-12-02-13:04:43-root-INFO: Loss too large (313.663->314.325)! Learning rate decreased to 0.02214.
2024-12-02-13:04:44-root-INFO: grad norm: 17.729 17.504 2.816
2024-12-02-13:04:44-root-INFO: grad norm: 17.038 16.832 2.643
2024-12-02-13:04:45-root-INFO: grad norm: 16.490 16.271 2.682
2024-12-02-13:04:45-root-INFO: grad norm: 16.006 15.802 2.547
2024-12-02-13:04:46-root-INFO: grad norm: 15.643 15.428 2.587
2024-12-02-13:04:46-root-INFO: grad norm: 15.323 15.120 2.486
2024-12-02-13:04:47-root-INFO: grad norm: 15.099 14.888 2.519
2024-12-02-13:04:47-root-INFO: Loss Change: 313.663 -> 307.005
2024-12-02-13:04:47-root-INFO: Regularization Change: 0.000 -> 1.032
2024-12-02-13:04:47-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-13:04:47-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-13:04:48-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-13:04:48-root-INFO: grad norm: 19.513 19.233 3.295
2024-12-02-13:04:48-root-INFO: Loss too large (308.022->308.830)! Learning rate decreased to 0.02285.
2024-12-02-13:04:48-root-INFO: grad norm: 18.083 17.860 2.832
2024-12-02-13:04:49-root-INFO: grad norm: 17.384 17.172 2.705
2024-12-02-13:04:49-root-INFO: grad norm: 16.821 16.603 2.699
2024-12-02-13:04:50-root-INFO: grad norm: 16.304 16.097 2.591
2024-12-02-13:04:50-root-INFO: grad norm: 15.910 15.696 2.597
2024-12-02-13:04:51-root-INFO: grad norm: 15.552 15.347 2.515
2024-12-02-13:04:51-root-INFO: grad norm: 15.296 15.087 2.521
2024-12-02-13:04:52-root-INFO: Loss Change: 308.022 -> 301.347
2024-12-02-13:04:52-root-INFO: Regularization Change: 0.000 -> 1.054
2024-12-02-13:04:52-root-INFO: Undo step: 120
2024-12-02-13:04:52-root-INFO: Undo step: 121
2024-12-02-13:04:52-root-INFO: Undo step: 122
2024-12-02-13:04:52-root-INFO: Undo step: 123
2024-12-02-13:04:52-root-INFO: Undo step: 124
2024-12-02-13:04:52-root-INFO: Undo step: 125
2024-12-02-13:04:52-root-INFO: Undo step: 126
2024-12-02-13:04:52-root-INFO: Undo step: 127
2024-12-02-13:04:52-root-INFO: Undo step: 128
2024-12-02-13:04:52-root-INFO: Undo step: 129
2024-12-02-13:04:52-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-13:04:52-root-INFO: grad norm: 231.886 228.008 42.236
2024-12-02-13:04:52-root-INFO: grad norm: 96.989 94.740 20.766
2024-12-02-13:04:53-root-INFO: grad norm: 74.568 72.414 17.796
2024-12-02-13:04:53-root-INFO: grad norm: 59.831 58.197 13.889
2024-12-02-13:04:54-root-INFO: grad norm: 50.416 48.489 13.806
2024-12-02-13:04:54-root-INFO: grad norm: 47.374 45.748 12.305
2024-12-02-13:04:55-root-INFO: grad norm: 47.540 45.763 12.877
2024-12-02-13:04:55-root-INFO: grad norm: 55.728 54.473 11.762
2024-12-02-13:04:56-root-INFO: Loss Change: 1049.957 -> 440.163
2024-12-02-13:04:56-root-INFO: Regularization Change: 0.000 -> 58.439
2024-12-02-13:04:56-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-13:04:56-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-13:04:56-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-13:04:56-root-INFO: grad norm: 51.447 50.303 10.791
2024-12-02-13:04:56-root-INFO: grad norm: 46.934 45.997 9.333
2024-12-02-13:04:57-root-INFO: grad norm: 48.110 47.056 10.013
2024-12-02-13:04:57-root-INFO: grad norm: 52.175 51.374 9.103
2024-12-02-13:04:58-root-INFO: Loss too large (406.808->408.436)! Learning rate decreased to 0.01702.
2024-12-02-13:04:58-root-INFO: grad norm: 39.327 38.623 7.407
2024-12-02-13:04:59-root-INFO: grad norm: 26.500 25.872 5.736
2024-12-02-13:04:59-root-INFO: grad norm: 25.299 24.746 5.260
2024-12-02-13:05:00-root-INFO: grad norm: 26.194 25.673 5.199
2024-12-02-13:05:00-root-INFO: Loss Change: 437.592 -> 379.493
2024-12-02-13:05:00-root-INFO: Regularization Change: 0.000 -> 6.498
2024-12-02-13:05:00-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-13:05:00-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-13:05:00-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-13:05:00-root-INFO: grad norm: 29.839 29.250 5.898
2024-12-02-13:05:01-root-INFO: grad norm: 41.803 41.332 6.257
2024-12-02-13:05:01-root-INFO: Loss too large (377.996->381.023)! Learning rate decreased to 0.01759.
2024-12-02-13:05:01-root-INFO: grad norm: 33.932 33.451 5.691
2024-12-02-13:05:02-root-INFO: grad norm: 24.902 24.459 4.677
2024-12-02-13:05:02-root-INFO: grad norm: 24.879 24.465 4.520
2024-12-02-13:05:03-root-INFO: grad norm: 25.929 25.528 4.541
2024-12-02-13:05:03-root-INFO: grad norm: 26.187 25.795 4.513
2024-12-02-13:05:04-root-INFO: grad norm: 26.765 26.386 4.486
2024-12-02-13:05:04-root-INFO: Loss Change: 379.529 -> 357.614
2024-12-02-13:05:04-root-INFO: Regularization Change: 0.000 -> 3.128
2024-12-02-13:05:04-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-13:05:04-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-13:05:04-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-13:05:04-root-INFO: grad norm: 30.011 29.575 5.098
2024-12-02-13:05:04-root-INFO: Loss too large (358.090->359.189)! Learning rate decreased to 0.01817.
2024-12-02-13:05:05-root-INFO: grad norm: 30.509 30.176 4.496
2024-12-02-13:05:05-root-INFO: grad norm: 30.295 29.931 4.683
2024-12-02-13:05:06-root-INFO: grad norm: 30.051 29.720 4.445
2024-12-02-13:05:06-root-INFO: grad norm: 29.949 29.603 4.540
2024-12-02-13:05:07-root-INFO: grad norm: 29.810 29.487 4.373
2024-12-02-13:05:07-root-INFO: grad norm: 29.776 29.442 4.447
2024-12-02-13:05:08-root-INFO: grad norm: 29.713 29.399 4.306
2024-12-02-13:05:08-root-INFO: Loss Change: 358.090 -> 343.994
2024-12-02-13:05:08-root-INFO: Regularization Change: 0.000 -> 2.098
2024-12-02-13:05:08-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-13:05:08-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-13:05:08-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-13:05:08-root-INFO: grad norm: 35.454 35.009 5.599
2024-12-02-13:05:08-root-INFO: Loss too large (345.890->349.133)! Learning rate decreased to 0.01877.
2024-12-02-13:05:09-root-INFO: grad norm: 34.560 34.268 4.482
2024-12-02-13:05:09-root-INFO: grad norm: 33.875 33.547 4.701
2024-12-02-13:05:10-root-INFO: grad norm: 33.099 32.813 4.336
2024-12-02-13:05:10-root-INFO: grad norm: 32.692 32.392 4.420
2024-12-02-13:05:11-root-INFO: grad norm: 32.250 31.970 4.240
2024-12-02-13:05:11-root-INFO: grad norm: 31.955 31.667 4.285
2024-12-02-13:05:12-root-INFO: grad norm: 31.637 31.362 4.166
2024-12-02-13:05:12-root-INFO: Loss Change: 345.890 -> 333.441
2024-12-02-13:05:12-root-INFO: Regularization Change: 0.000 -> 1.674
2024-12-02-13:05:12-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-13:05:12-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-13:05:12-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-13:05:12-root-INFO: grad norm: 40.334 39.884 6.008
2024-12-02-13:05:13-root-INFO: Loss too large (336.112->341.676)! Learning rate decreased to 0.01939.
2024-12-02-13:05:13-root-INFO: grad norm: 38.414 38.146 4.530
2024-12-02-13:05:14-root-INFO: grad norm: 36.692 36.400 4.617
2024-12-02-13:05:14-root-INFO: grad norm: 35.082 34.820 4.276
2024-12-02-13:05:15-root-INFO: grad norm: 33.838 33.573 4.222
2024-12-02-13:05:15-root-INFO: grad norm: 32.722 32.464 4.096
2024-12-02-13:05:15-root-INFO: grad norm: 31.843 31.587 4.028
2024-12-02-13:05:16-root-INFO: grad norm: 31.067 30.813 3.965
2024-12-02-13:05:16-root-INFO: Loss Change: 336.112 -> 323.169
2024-12-02-13:05:16-root-INFO: Regularization Change: 0.000 -> 1.471
2024-12-02-13:05:16-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-13:05:16-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-13:05:16-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-13:05:17-root-INFO: grad norm: 42.943 42.429 6.626
2024-12-02-13:05:17-root-INFO: Loss too large (327.465->334.246)! Learning rate decreased to 0.02013.
2024-12-02-13:05:17-root-INFO: grad norm: 39.917 39.654 4.577
2024-12-02-13:05:18-root-INFO: grad norm: 36.890 36.611 4.530
2024-12-02-13:05:18-root-INFO: grad norm: 34.651 34.406 4.113
2024-12-02-13:05:19-root-INFO: grad norm: 32.812 32.567 4.009
2024-12-02-13:05:19-root-INFO: grad norm: 31.307 31.069 3.859
2024-12-02-13:05:20-root-INFO: grad norm: 30.125 29.888 3.769
2024-12-02-13:05:20-root-INFO: grad norm: 29.125 28.890 3.690
2024-12-02-13:05:20-root-INFO: Loss Change: 327.465 -> 313.616
2024-12-02-13:05:20-root-INFO: Regularization Change: 0.000 -> 1.416
2024-12-02-13:05:20-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-13:05:20-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-13:05:20-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-13:05:21-root-INFO: grad norm: 34.333 33.974 4.956
2024-12-02-13:05:21-root-INFO: Loss too large (315.445->320.033)! Learning rate decreased to 0.02078.
2024-12-02-13:05:21-root-INFO: grad norm: 32.688 32.474 3.735
2024-12-02-13:05:22-root-INFO: grad norm: 31.204 30.956 3.923
2024-12-02-13:05:22-root-INFO: grad norm: 29.979 29.764 3.585
2024-12-02-13:05:23-root-INFO: grad norm: 28.968 28.739 3.633
2024-12-02-13:05:23-root-INFO: grad norm: 28.112 27.897 3.466
2024-12-02-13:05:24-root-INFO: grad norm: 27.419 27.197 3.479
2024-12-02-13:05:24-root-INFO: grad norm: 26.823 26.609 3.378
2024-12-02-13:05:24-root-INFO: Loss Change: 315.445 -> 305.393
2024-12-02-13:05:24-root-INFO: Regularization Change: 0.000 -> 1.251
2024-12-02-13:05:24-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-13:05:24-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-13:05:25-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-13:05:25-root-INFO: grad norm: 32.103 31.768 4.624
2024-12-02-13:05:25-root-INFO: Loss too large (307.073->311.170)! Learning rate decreased to 0.02145.
2024-12-02-13:05:25-root-INFO: grad norm: 30.377 30.173 3.517
2024-12-02-13:05:26-root-INFO: grad norm: 28.943 28.722 3.568
2024-12-02-13:05:26-root-INFO: grad norm: 27.787 27.585 3.345
2024-12-02-13:05:27-root-INFO: grad norm: 26.797 26.594 3.291
2024-12-02-13:05:27-root-INFO: grad norm: 25.995 25.794 3.223
2024-12-02-13:05:28-root-INFO: grad norm: 25.323 25.126 3.152
2024-12-02-13:05:28-root-INFO: grad norm: 24.779 24.579 3.138
2024-12-02-13:05:28-root-INFO: Loss Change: 307.073 -> 297.753
2024-12-02-13:05:28-root-INFO: Regularization Change: 0.000 -> 1.193
2024-12-02-13:05:28-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-13:05:28-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-13:05:29-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-13:05:29-root-INFO: grad norm: 30.160 29.842 4.368
2024-12-02-13:05:29-root-INFO: Loss too large (299.329->303.111)! Learning rate decreased to 0.02214.
2024-12-02-13:05:29-root-INFO: grad norm: 28.692 28.500 3.319
2024-12-02-13:05:30-root-INFO: grad norm: 27.513 27.305 3.379
2024-12-02-13:05:30-root-INFO: grad norm: 26.575 26.382 3.193
2024-12-02-13:05:31-root-INFO: grad norm: 25.721 25.527 3.151
2024-12-02-13:05:31-root-INFO: grad norm: 25.007 24.815 3.097
2024-12-02-13:05:32-root-INFO: grad norm: 24.383 24.195 3.028
2024-12-02-13:05:32-root-INFO: grad norm: 23.855 23.663 3.020
2024-12-02-13:05:33-root-INFO: Loss Change: 299.329 -> 290.862
2024-12-02-13:05:33-root-INFO: Regularization Change: 0.000 -> 1.151
2024-12-02-13:05:33-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-13:05:33-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-13:05:33-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-13:05:33-root-INFO: grad norm: 29.756 29.445 4.288
2024-12-02-13:05:33-root-INFO: Loss too large (292.982->296.774)! Learning rate decreased to 0.02285.
2024-12-02-13:05:34-root-INFO: grad norm: 28.193 28.008 3.227
2024-12-02-13:05:34-root-INFO: grad norm: 26.951 26.749 3.294
2024-12-02-13:05:35-root-INFO: grad norm: 25.929 25.743 3.094
2024-12-02-13:05:35-root-INFO: grad norm: 25.012 24.826 3.040
2024-12-02-13:05:36-root-INFO: grad norm: 24.235 24.051 2.985
2024-12-02-13:05:36-root-INFO: grad norm: 23.567 23.388 2.902
2024-12-02-13:05:37-root-INFO: grad norm: 23.000 22.816 2.898
2024-12-02-13:05:37-root-INFO: Loss Change: 292.982 -> 284.657
2024-12-02-13:05:37-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-02-13:05:37-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-13:05:37-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-13:05:37-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-13:05:37-root-INFO: grad norm: 30.317 29.949 4.709
2024-12-02-13:05:37-root-INFO: Loss too large (286.561->290.468)! Learning rate decreased to 0.02358.
2024-12-02-13:05:38-root-INFO: grad norm: 27.979 27.809 3.073
2024-12-02-13:05:39-root-INFO: grad norm: 26.138 25.944 3.177
2024-12-02-13:05:39-root-INFO: grad norm: 24.805 24.638 2.874
2024-12-02-13:05:39-root-INFO: grad norm: 23.632 23.461 2.837
2024-12-02-13:05:40-root-INFO: grad norm: 22.731 22.564 2.743
2024-12-02-13:05:40-root-INFO: grad norm: 21.961 21.798 2.674
2024-12-02-13:05:41-root-INFO: grad norm: 21.364 21.199 2.655
2024-12-02-13:05:41-root-INFO: Loss Change: 286.561 -> 277.721
2024-12-02-13:05:41-root-INFO: Regularization Change: 0.000 -> 1.123
2024-12-02-13:05:41-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-13:05:41-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-13:05:41-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-13:05:42-root-INFO: grad norm: 23.373 23.148 3.233
2024-12-02-13:05:42-root-INFO: Loss too large (279.302->281.612)! Learning rate decreased to 0.02432.
2024-12-02-13:05:42-root-INFO: grad norm: 22.198 22.041 2.633
2024-12-02-13:05:43-root-INFO: grad norm: 21.352 21.188 2.636
2024-12-02-13:05:43-root-INFO: grad norm: 20.668 20.511 2.547
2024-12-02-13:05:44-root-INFO: grad norm: 20.064 19.910 2.479
2024-12-02-13:05:44-root-INFO: grad norm: 19.572 19.414 2.480
2024-12-02-13:05:45-root-INFO: grad norm: 19.140 18.990 2.392
2024-12-02-13:05:45-root-INFO: grad norm: 18.795 18.638 2.429
2024-12-02-13:05:45-root-INFO: Loss Change: 279.302 -> 272.538
2024-12-02-13:05:45-root-INFO: Regularization Change: 0.000 -> 1.069
2024-12-02-13:05:45-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-13:05:45-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-13:05:45-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-13:05:46-root-INFO: grad norm: 27.149 26.836 4.109
2024-12-02-13:05:46-root-INFO: Loss too large (274.844->278.297)! Learning rate decreased to 0.02509.
2024-12-02-13:05:46-root-INFO: grad norm: 25.412 25.260 2.778
2024-12-02-13:05:47-root-INFO: grad norm: 24.201 24.031 2.863
2024-12-02-13:05:47-root-INFO: grad norm: 23.310 23.159 2.645
2024-12-02-13:05:48-root-INFO: grad norm: 22.426 22.275 2.602
2024-12-02-13:05:48-root-INFO: grad norm: 21.714 21.564 2.548
2024-12-02-13:05:49-root-INFO: grad norm: 21.051 20.906 2.465
2024-12-02-13:05:49-root-INFO: grad norm: 20.513 20.364 2.471
2024-12-02-13:05:49-root-INFO: Loss Change: 274.844 -> 267.347
2024-12-02-13:05:49-root-INFO: Regularization Change: 0.000 -> 1.084
2024-12-02-13:05:49-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-13:05:49-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-13:05:50-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-13:05:50-root-INFO: grad norm: 25.407 25.151 3.598
2024-12-02-13:05:50-root-INFO: Loss too large (268.777->271.927)! Learning rate decreased to 0.02587.
2024-12-02-13:05:50-root-INFO: grad norm: 23.686 23.542 2.601
2024-12-02-13:05:51-root-INFO: grad norm: 22.333 22.176 2.645
2024-12-02-13:05:51-root-INFO: grad norm: 21.273 21.131 2.460
2024-12-02-13:05:52-root-INFO: grad norm: 20.307 20.165 2.394
2024-12-02-13:05:52-root-INFO: grad norm: 19.531 19.388 2.352
2024-12-02-13:05:53-root-INFO: grad norm: 18.841 18.706 2.255
2024-12-02-13:05:53-root-INFO: grad norm: 18.289 18.147 2.271
2024-12-02-13:05:54-root-INFO: Loss Change: 268.777 -> 261.549
2024-12-02-13:05:54-root-INFO: Regularization Change: 0.000 -> 1.053
2024-12-02-13:05:54-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-13:05:54-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-13:05:54-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-13:05:54-root-INFO: grad norm: 23.821 23.590 3.312
2024-12-02-13:05:54-root-INFO: Loss too large (263.432->266.235)! Learning rate decreased to 0.02668.
2024-12-02-13:05:55-root-INFO: grad norm: 22.200 22.064 2.449
2024-12-02-13:05:55-root-INFO: grad norm: 20.889 20.746 2.441
2024-12-02-13:05:55-root-INFO: grad norm: 19.922 19.789 2.302
2024-12-02-13:05:56-root-INFO: grad norm: 19.023 18.894 2.215
2024-12-02-13:05:56-root-INFO: grad norm: 18.318 18.186 2.199
2024-12-02-13:05:57-root-INFO: grad norm: 17.688 17.564 2.094
2024-12-02-13:05:57-root-INFO: grad norm: 17.188 17.056 2.125
2024-12-02-13:05:58-root-INFO: Loss Change: 263.432 -> 256.608
2024-12-02-13:05:58-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-02-13:05:58-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-13:05:58-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-13:05:58-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-13:05:58-root-INFO: grad norm: 21.957 21.757 2.962
2024-12-02-13:05:58-root-INFO: Loss too large (258.239->260.480)! Learning rate decreased to 0.02751.
2024-12-02-13:05:59-root-INFO: grad norm: 20.228 20.106 2.218
2024-12-02-13:05:59-root-INFO: grad norm: 18.892 18.758 2.239
2024-12-02-13:05:59-root-INFO: grad norm: 17.806 17.684 2.074
2024-12-02-13:06:00-root-INFO: grad norm: 16.842 16.721 2.017
2024-12-02-13:06:00-root-INFO: grad norm: 16.066 15.946 1.960
2024-12-02-13:06:01-root-INFO: grad norm: 15.383 15.267 1.885
2024-12-02-13:06:01-root-INFO: grad norm: 14.842 14.723 1.876
2024-12-02-13:06:02-root-INFO: Loss Change: 258.239 -> 251.545
2024-12-02-13:06:02-root-INFO: Regularization Change: 0.000 -> 1.042
2024-12-02-13:06:02-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-13:06:02-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-13:06:02-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-13:06:02-root-INFO: grad norm: 20.085 19.864 2.970
2024-12-02-13:06:02-root-INFO: Loss too large (252.857->254.700)! Learning rate decreased to 0.02836.
2024-12-02-13:06:03-root-INFO: grad norm: 18.488 18.373 2.054
2024-12-02-13:06:03-root-INFO: grad norm: 17.289 17.163 2.085
2024-12-02-13:06:04-root-INFO: grad norm: 16.410 16.298 1.915
2024-12-02-13:06:04-root-INFO: grad norm: 15.577 15.464 1.873
2024-12-02-13:06:05-root-INFO: grad norm: 14.946 14.834 1.824
2024-12-02-13:06:05-root-INFO: grad norm: 14.377 14.268 1.761
2024-12-02-13:06:06-root-INFO: grad norm: 13.954 13.842 1.763
2024-12-02-13:06:06-root-INFO: Loss Change: 252.857 -> 246.725
2024-12-02-13:06:06-root-INFO: Regularization Change: 0.000 -> 1.038
2024-12-02-13:06:06-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-13:06:06-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-13:06:06-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-13:06:06-root-INFO: grad norm: 20.459 20.227 3.070
2024-12-02-13:06:06-root-INFO: Loss too large (248.284->250.187)! Learning rate decreased to 0.02923.
2024-12-02-13:06:07-root-INFO: grad norm: 18.618 18.506 2.035
2024-12-02-13:06:07-root-INFO: grad norm: 17.255 17.131 2.064
2024-12-02-13:06:08-root-INFO: grad norm: 16.264 16.156 1.873
2024-12-02-13:06:08-root-INFO: grad norm: 15.346 15.237 1.828
2024-12-02-13:06:09-root-INFO: grad norm: 14.660 14.552 1.770
2024-12-02-13:06:09-root-INFO: grad norm: 14.058 13.954 1.709
2024-12-02-13:06:10-root-INFO: grad norm: 13.619 13.512 1.706
2024-12-02-13:06:10-root-INFO: Loss Change: 248.284 -> 241.950
2024-12-02-13:06:10-root-INFO: Regularization Change: 0.000 -> 1.063
2024-12-02-13:06:10-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-13:06:10-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-13:06:10-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-13:06:10-root-INFO: grad norm: 17.307 17.142 2.385
2024-12-02-13:06:10-root-INFO: Loss too large (242.841->244.243)! Learning rate decreased to 0.03012.
2024-12-02-13:06:11-root-INFO: grad norm: 16.087 15.986 1.800
2024-12-02-13:06:11-root-INFO: grad norm: 15.135 15.026 1.821
2024-12-02-13:06:12-root-INFO: grad norm: 14.386 14.284 1.709
2024-12-02-13:06:12-root-INFO: grad norm: 13.731 13.628 1.672
2024-12-02-13:06:13-root-INFO: grad norm: 13.220 13.117 1.645
2024-12-02-13:06:13-root-INFO: grad norm: 12.785 12.685 1.598
2024-12-02-13:06:14-root-INFO: grad norm: 12.457 12.353 1.602
2024-12-02-13:06:14-root-INFO: Loss Change: 242.841 -> 237.406
2024-12-02-13:06:14-root-INFO: Regularization Change: 0.000 -> 1.035
2024-12-02-13:06:14-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-13:06:14-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-13:06:14-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-13:06:14-root-INFO: grad norm: 16.670 16.496 2.397
2024-12-02-13:06:14-root-INFO: Loss too large (238.500->239.815)! Learning rate decreased to 0.03103.
2024-12-02-13:06:15-root-INFO: grad norm: 15.598 15.499 1.752
2024-12-02-13:06:15-root-INFO: grad norm: 14.832 14.724 1.793
2024-12-02-13:06:16-root-INFO: grad norm: 14.216 14.117 1.673
2024-12-02-13:06:16-root-INFO: grad norm: 13.623 13.524 1.638
2024-12-02-13:06:17-root-INFO: grad norm: 13.139 13.041 1.607
2024-12-02-13:06:17-root-INFO: grad norm: 12.685 12.590 1.550
2024-12-02-13:06:18-root-INFO: grad norm: 12.317 12.219 1.551
2024-12-02-13:06:18-root-INFO: Loss Change: 238.500 -> 233.230
2024-12-02-13:06:18-root-INFO: Regularization Change: 0.000 -> 1.047
2024-12-02-13:06:18-root-INFO: Undo step: 110
2024-12-02-13:06:18-root-INFO: Undo step: 111
2024-12-02-13:06:18-root-INFO: Undo step: 112
2024-12-02-13:06:18-root-INFO: Undo step: 113
2024-12-02-13:06:18-root-INFO: Undo step: 114
2024-12-02-13:06:18-root-INFO: Undo step: 115
2024-12-02-13:06:18-root-INFO: Undo step: 116
2024-12-02-13:06:18-root-INFO: Undo step: 117
2024-12-02-13:06:18-root-INFO: Undo step: 118
2024-12-02-13:06:18-root-INFO: Undo step: 119
2024-12-02-13:06:18-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-13:06:18-root-INFO: grad norm: 134.237 130.916 29.674
2024-12-02-13:06:19-root-INFO: grad norm: 73.808 71.890 16.715
2024-12-02-13:06:19-root-INFO: grad norm: 52.342 50.825 12.509
2024-12-02-13:06:20-root-INFO: grad norm: 43.226 42.045 10.036
2024-12-02-13:06:20-root-INFO: grad norm: 39.651 38.693 8.665
2024-12-02-13:06:21-root-INFO: grad norm: 38.891 38.137 7.620
2024-12-02-13:06:21-root-INFO: grad norm: 38.134 37.471 7.083
2024-12-02-13:06:22-root-INFO: grad norm: 38.559 38.029 6.366
2024-12-02-13:06:22-root-INFO: Loss Change: 900.806 -> 341.769
2024-12-02-13:06:22-root-INFO: Regularization Change: 0.000 -> 75.328
2024-12-02-13:06:22-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-13:06:22-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-13:06:22-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-13:06:22-root-INFO: grad norm: 36.441 35.960 5.896
2024-12-02-13:06:23-root-INFO: grad norm: 36.370 35.976 5.335
2024-12-02-13:06:23-root-INFO: grad norm: 38.352 37.939 5.611
2024-12-02-13:06:24-root-INFO: grad norm: 40.665 40.334 5.177
2024-12-02-13:06:24-root-INFO: grad norm: 43.395 43.000 5.841
2024-12-02-13:06:25-root-INFO: grad norm: 46.523 46.213 5.363
2024-12-02-13:06:25-root-INFO: grad norm: 47.717 47.310 6.219
2024-12-02-13:06:26-root-INFO: grad norm: 48.188 47.880 5.438
2024-12-02-13:06:26-root-INFO: Loss Change: 338.334 -> 306.305
2024-12-02-13:06:26-root-INFO: Regularization Change: 0.000 -> 9.788
2024-12-02-13:06:26-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-13:06:26-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-13:06:26-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-13:06:27-root-INFO: grad norm: 45.131 44.781 5.610
2024-12-02-13:06:27-root-INFO: grad norm: 44.354 44.069 5.022
2024-12-02-13:06:28-root-INFO: grad norm: 44.008 43.651 5.594
2024-12-02-13:06:28-root-INFO: grad norm: 43.650 43.375 4.889
2024-12-02-13:06:28-root-INFO: grad norm: 43.052 42.703 5.468
2024-12-02-13:06:29-root-INFO: grad norm: 42.472 42.204 4.766
2024-12-02-13:06:29-root-INFO: grad norm: 41.902 41.564 5.308
2024-12-02-13:06:30-root-INFO: grad norm: 41.432 41.168 4.664
2024-12-02-13:06:30-root-INFO: Loss Change: 304.581 -> 283.768
2024-12-02-13:06:30-root-INFO: Regularization Change: 0.000 -> 4.436
2024-12-02-13:06:30-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-13:06:30-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-13:06:30-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-13:06:30-root-INFO: grad norm: 35.775 35.519 4.271
2024-12-02-13:06:31-root-INFO: grad norm: 36.502 36.251 4.270
2024-12-02-13:06:31-root-INFO: grad norm: 37.470 37.186 4.608
2024-12-02-13:06:32-root-INFO: grad norm: 38.667 38.416 4.402
2024-12-02-13:06:32-root-INFO: grad norm: 39.271 38.971 4.842
2024-12-02-13:06:33-root-INFO: grad norm: 39.700 39.444 4.495
2024-12-02-13:06:33-root-INFO: grad norm: 39.787 39.482 4.917
2024-12-02-13:06:34-root-INFO: grad norm: 39.748 39.491 4.511
2024-12-02-13:06:34-root-INFO: Loss Change: 279.564 -> 270.827
2024-12-02-13:06:34-root-INFO: Regularization Change: 0.000 -> 2.984
2024-12-02-13:06:34-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-13:06:34-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-13:06:34-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-13:06:34-root-INFO: grad norm: 35.776 35.537 4.125
2024-12-02-13:06:35-root-INFO: grad norm: 35.601 35.356 4.169
2024-12-02-13:06:35-root-INFO: grad norm: 35.699 35.433 4.348
2024-12-02-13:06:36-root-INFO: grad norm: 36.195 35.951 4.201
2024-12-02-13:06:36-root-INFO: grad norm: 36.379 36.101 4.483
2024-12-02-13:06:37-root-INFO: grad norm: 36.603 36.356 4.250
2024-12-02-13:06:37-root-INFO: grad norm: 36.667 36.385 4.542
2024-12-02-13:06:38-root-INFO: grad norm: 36.718 36.468 4.275
2024-12-02-13:06:38-root-INFO: Loss Change: 267.922 -> 259.848
2024-12-02-13:06:38-root-INFO: Regularization Change: 0.000 -> 2.311
2024-12-02-13:06:38-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-13:06:38-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-13:06:38-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-13:06:38-root-INFO: grad norm: 31.475 31.276 3.525
2024-12-02-13:06:39-root-INFO: grad norm: 31.888 31.660 3.808
2024-12-02-13:06:39-root-INFO: grad norm: 32.544 32.301 3.966
2024-12-02-13:06:40-root-INFO: grad norm: 33.441 33.208 3.945
2024-12-02-13:06:40-root-INFO: Loss too large (254.121->254.187)! Learning rate decreased to 0.02668.
2024-12-02-13:06:40-root-INFO: grad norm: 22.031 21.845 2.857
2024-12-02-13:06:41-root-INFO: grad norm: 15.121 14.979 2.062
2024-12-02-13:06:41-root-INFO: grad norm: 11.459 11.330 1.711
2024-12-02-13:06:42-root-INFO: grad norm: 9.144 9.017 1.518
2024-12-02-13:06:42-root-INFO: Loss Change: 256.580 -> 241.693
2024-12-02-13:06:42-root-INFO: Regularization Change: 0.000 -> 1.730
2024-12-02-13:06:42-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-13:06:42-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-13:06:42-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-13:06:42-root-INFO: grad norm: 6.896 6.697 1.644
2024-12-02-13:06:43-root-INFO: grad norm: 5.850 5.709 1.278
2024-12-02-13:06:43-root-INFO: grad norm: 5.746 5.603 1.272
2024-12-02-13:06:44-root-INFO: grad norm: 5.805 5.673 1.232
2024-12-02-13:06:44-root-INFO: grad norm: 5.956 5.828 1.228
2024-12-02-13:06:45-root-INFO: grad norm: 6.203 6.081 1.223
2024-12-02-13:06:45-root-INFO: grad norm: 6.566 6.451 1.220
2024-12-02-13:06:46-root-INFO: grad norm: 7.076 6.967 1.239
2024-12-02-13:06:46-root-INFO: Loss Change: 241.519 -> 234.880
2024-12-02-13:06:46-root-INFO: Regularization Change: 0.000 -> 1.852
2024-12-02-13:06:46-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-13:06:46-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-13:06:46-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-13:06:46-root-INFO: grad norm: 12.500 12.289 2.288
2024-12-02-13:06:47-root-INFO: grad norm: 14.442 14.341 1.708
2024-12-02-13:06:47-root-INFO: Loss too large (234.788->234.918)! Learning rate decreased to 0.02836.
2024-12-02-13:06:47-root-INFO: grad norm: 12.212 12.101 1.639
2024-12-02-13:06:48-root-INFO: grad norm: 10.666 10.568 1.438
2024-12-02-13:06:48-root-INFO: grad norm: 9.569 9.463 1.420
2024-12-02-13:06:49-root-INFO: grad norm: 8.769 8.668 1.324
2024-12-02-13:06:49-root-INFO: grad norm: 8.157 8.050 1.314
2024-12-02-13:06:50-root-INFO: grad norm: 7.704 7.601 1.252
2024-12-02-13:06:50-root-INFO: Loss Change: 235.291 -> 229.559
2024-12-02-13:06:50-root-INFO: Regularization Change: 0.000 -> 1.151
2024-12-02-13:06:50-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-13:06:50-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-13:06:50-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-13:06:50-root-INFO: grad norm: 15.570 15.322 2.767
2024-12-02-13:06:50-root-INFO: Loss too large (230.611->230.967)! Learning rate decreased to 0.02923.
2024-12-02-13:06:51-root-INFO: grad norm: 13.431 13.324 1.695
2024-12-02-13:06:51-root-INFO: grad norm: 12.035 11.911 1.722
2024-12-02-13:06:52-root-INFO: grad norm: 11.054 10.953 1.489
2024-12-02-13:06:52-root-INFO: grad norm: 10.154 10.044 1.489
2024-12-02-13:06:53-root-INFO: grad norm: 9.500 9.402 1.362
2024-12-02-13:06:53-root-INFO: grad norm: 8.903 8.798 1.362
2024-12-02-13:06:54-root-INFO: grad norm: 8.464 8.367 1.275
2024-12-02-13:06:54-root-INFO: Loss Change: 230.611 -> 224.779
2024-12-02-13:06:54-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-02-13:06:54-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-13:06:54-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-13:06:54-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-13:06:54-root-INFO: grad norm: 13.305 13.135 2.125
2024-12-02-13:06:55-root-INFO: Loss too large (225.429->225.816)! Learning rate decreased to 0.03012.
2024-12-02-13:06:55-root-INFO: grad norm: 12.011 11.914 1.525
2024-12-02-13:06:56-root-INFO: grad norm: 11.022 10.912 1.550
2024-12-02-13:06:56-root-INFO: grad norm: 10.281 10.186 1.391
2024-12-02-13:06:57-root-INFO: grad norm: 9.609 9.509 1.383
2024-12-02-13:06:57-root-INFO: grad norm: 9.101 9.008 1.297
2024-12-02-13:06:57-root-INFO: grad norm: 8.638 8.542 1.287
2024-12-02-13:06:58-root-INFO: grad norm: 8.288 8.196 1.230
2024-12-02-13:06:58-root-INFO: Loss Change: 225.429 -> 220.483
2024-12-02-13:06:58-root-INFO: Regularization Change: 0.000 -> 0.986
2024-12-02-13:06:58-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-13:06:58-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-13:06:59-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-13:06:59-root-INFO: grad norm: 13.778 13.594 2.246
2024-12-02-13:06:59-root-INFO: Loss too large (221.492->222.073)! Learning rate decreased to 0.03103.
2024-12-02-13:06:59-root-INFO: grad norm: 12.620 12.522 1.566
2024-12-02-13:07:00-root-INFO: grad norm: 11.793 11.682 1.615
2024-12-02-13:07:00-root-INFO: grad norm: 11.162 11.068 1.442
2024-12-02-13:07:01-root-INFO: grad norm: 10.554 10.454 1.446
2024-12-02-13:07:01-root-INFO: grad norm: 10.087 9.996 1.351
2024-12-02-13:07:02-root-INFO: grad norm: 9.645 9.550 1.347
2024-12-02-13:07:02-root-INFO: grad norm: 9.306 9.217 1.281
2024-12-02-13:07:03-root-INFO: Loss Change: 221.492 -> 216.723
2024-12-02-13:07:03-root-INFO: Regularization Change: 0.000 -> 0.972
2024-12-02-13:07:03-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-13:07:03-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-13:07:03-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-13:07:03-root-INFO: grad norm: 16.278 16.085 2.499
2024-12-02-13:07:03-root-INFO: Loss too large (218.322->219.535)! Learning rate decreased to 0.03197.
2024-12-02-13:07:04-root-INFO: grad norm: 14.775 14.670 1.756
2024-12-02-13:07:04-root-INFO: grad norm: 13.552 13.442 1.726
2024-12-02-13:07:04-root-INFO: grad norm: 12.647 12.551 1.552
2024-12-02-13:07:05-root-INFO: grad norm: 11.758 11.661 1.506
2024-12-02-13:07:05-root-INFO: grad norm: 11.080 10.990 1.411
2024-12-02-13:07:06-root-INFO: grad norm: 10.432 10.341 1.373
2024-12-02-13:07:06-root-INFO: grad norm: 9.931 9.844 1.307
2024-12-02-13:07:07-root-INFO: Loss Change: 218.322 -> 213.130
2024-12-02-13:07:07-root-INFO: Regularization Change: 0.000 -> 0.974
2024-12-02-13:07:07-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-13:07:07-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-13:07:07-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-13:07:07-root-INFO: grad norm: 14.612 14.455 2.140
2024-12-02-13:07:07-root-INFO: Loss too large (213.860->214.820)! Learning rate decreased to 0.03292.
2024-12-02-13:07:08-root-INFO: grad norm: 13.315 13.223 1.564
2024-12-02-13:07:08-root-INFO: grad norm: 12.234 12.133 1.575
2024-12-02-13:07:09-root-INFO: grad norm: 11.396 11.309 1.402
2024-12-02-13:07:09-root-INFO: grad norm: 10.606 10.514 1.393
2024-12-02-13:07:10-root-INFO: grad norm: 9.988 9.905 1.287
2024-12-02-13:07:10-root-INFO: grad norm: 9.414 9.327 1.280
2024-12-02-13:07:11-root-INFO: grad norm: 8.961 8.880 1.202
2024-12-02-13:07:11-root-INFO: Loss Change: 213.860 -> 209.114
2024-12-02-13:07:11-root-INFO: Regularization Change: 0.000 -> 0.948
2024-12-02-13:07:11-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-13:07:11-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-13:07:11-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-13:07:11-root-INFO: grad norm: 17.369 17.165 2.649
2024-12-02-13:07:11-root-INFO: Loss too large (211.107->212.702)! Learning rate decreased to 0.03391.
2024-12-02-13:07:12-root-INFO: grad norm: 15.674 15.573 1.779
2024-12-02-13:07:13-root-INFO: grad norm: 14.323 14.214 1.770
2024-12-02-13:07:13-root-INFO: grad norm: 13.325 13.234 1.553
2024-12-02-13:07:13-root-INFO: grad norm: 12.353 12.258 1.528
2024-12-02-13:07:14-root-INFO: grad norm: 11.592 11.507 1.402
2024-12-02-13:07:14-root-INFO: grad norm: 10.875 10.787 1.380
2024-12-02-13:07:15-root-INFO: grad norm: 10.301 10.220 1.288
2024-12-02-13:07:15-root-INFO: Loss Change: 211.107 -> 205.767
2024-12-02-13:07:15-root-INFO: Regularization Change: 0.000 -> 0.978
2024-12-02-13:07:15-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-13:07:15-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-13:07:15-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-13:07:16-root-INFO: grad norm: 17.033 16.842 2.543
2024-12-02-13:07:16-root-INFO: Loss too large (207.234->209.007)! Learning rate decreased to 0.03491.
2024-12-02-13:07:16-root-INFO: grad norm: 15.765 15.665 1.770
2024-12-02-13:07:17-root-INFO: grad norm: 14.781 14.666 1.835
2024-12-02-13:07:17-root-INFO: grad norm: 13.985 13.894 1.598
2024-12-02-13:07:18-root-INFO: grad norm: 13.216 13.116 1.621
2024-12-02-13:07:18-root-INFO: grad norm: 12.589 12.502 1.476
2024-12-02-13:07:19-root-INFO: grad norm: 11.997 11.904 1.486
2024-12-02-13:07:19-root-INFO: grad norm: 11.513 11.430 1.379
2024-12-02-13:07:19-root-INFO: Loss Change: 207.234 -> 202.277
2024-12-02-13:07:19-root-INFO: Regularization Change: 0.000 -> 0.975
2024-12-02-13:07:19-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-13:07:19-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-13:07:20-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-13:07:20-root-INFO: grad norm: 17.033 16.863 2.398
2024-12-02-13:07:20-root-INFO: Loss too large (204.060->206.020)! Learning rate decreased to 0.03594.
2024-12-02-13:07:20-root-INFO: grad norm: 15.820 15.724 1.741
2024-12-02-13:07:21-root-INFO: grad norm: 14.751 14.644 1.776
2024-12-02-13:07:21-root-INFO: grad norm: 13.881 13.791 1.571
2024-12-02-13:07:22-root-INFO: grad norm: 13.040 12.946 1.569
2024-12-02-13:07:22-root-INFO: grad norm: 12.346 12.262 1.439
2024-12-02-13:07:23-root-INFO: grad norm: 11.694 11.606 1.432
2024-12-02-13:07:23-root-INFO: grad norm: 11.153 11.073 1.332
2024-12-02-13:07:24-root-INFO: Loss Change: 204.060 -> 199.142
2024-12-02-13:07:24-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-13:07:24-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-13:07:24-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-13:07:24-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-13:07:24-root-INFO: grad norm: 17.673 17.487 2.561
2024-12-02-13:07:24-root-INFO: Loss too large (200.975->202.981)! Learning rate decreased to 0.03700.
2024-12-02-13:07:25-root-INFO: grad norm: 15.987 15.888 1.777
2024-12-02-13:07:25-root-INFO: grad norm: 14.620 14.512 1.772
2024-12-02-13:07:26-root-INFO: grad norm: 13.541 13.454 1.534
2024-12-02-13:07:26-root-INFO: grad norm: 12.529 12.436 1.527
2024-12-02-13:07:26-root-INFO: grad norm: 11.707 11.626 1.371
2024-12-02-13:07:27-root-INFO: grad norm: 10.950 10.864 1.369
2024-12-02-13:07:27-root-INFO: grad norm: 10.326 10.251 1.248
2024-12-02-13:07:28-root-INFO: Loss Change: 200.975 -> 195.651
2024-12-02-13:07:28-root-INFO: Regularization Change: 0.000 -> 0.983
2024-12-02-13:07:28-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-13:07:28-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-13:07:28-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-13:07:28-root-INFO: grad norm: 15.254 15.118 2.032
2024-12-02-13:07:28-root-INFO: Loss too large (196.979->198.475)! Learning rate decreased to 0.03808.
2024-12-02-13:07:29-root-INFO: grad norm: 13.908 13.819 1.579
2024-12-02-13:07:29-root-INFO: grad norm: 12.753 12.660 1.530
2024-12-02-13:07:30-root-INFO: grad norm: 11.817 11.736 1.378
2024-12-02-13:07:30-root-INFO: grad norm: 10.954 10.872 1.340
2024-12-02-13:07:31-root-INFO: grad norm: 10.246 10.171 1.239
2024-12-02-13:07:31-root-INFO: grad norm: 9.598 9.521 1.214
2024-12-02-13:07:31-root-INFO: grad norm: 9.062 8.990 1.133
2024-12-02-13:07:32-root-INFO: Loss Change: 196.979 -> 192.288
2024-12-02-13:07:32-root-INFO: Regularization Change: 0.000 -> 0.971
2024-12-02-13:07:32-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-13:07:32-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-13:07:32-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-13:07:32-root-INFO: grad norm: 13.739 13.582 2.072
2024-12-02-13:07:32-root-INFO: Loss too large (193.347->194.407)! Learning rate decreased to 0.03918.
2024-12-02-13:07:33-root-INFO: grad norm: 12.605 12.525 1.418
2024-12-02-13:07:33-root-INFO: grad norm: 11.795 11.698 1.512
2024-12-02-13:07:34-root-INFO: grad norm: 11.131 11.058 1.279
2024-12-02-13:07:34-root-INFO: grad norm: 10.514 10.429 1.336
2024-12-02-13:07:35-root-INFO: grad norm: 9.989 9.919 1.181
2024-12-02-13:07:35-root-INFO: grad norm: 9.505 9.426 1.226
2024-12-02-13:07:35-root-INFO: grad norm: 9.092 9.025 1.103
2024-12-02-13:07:36-root-INFO: Loss Change: 193.347 -> 189.048
2024-12-02-13:07:36-root-INFO: Regularization Change: 0.000 -> 0.992
2024-12-02-13:07:36-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-13:07:36-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-13:07:36-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-13:07:36-root-INFO: grad norm: 14.243 14.104 1.988
2024-12-02-13:07:36-root-INFO: Loss too large (190.401->191.787)! Learning rate decreased to 0.04031.
2024-12-02-13:07:37-root-INFO: grad norm: 13.147 13.066 1.457
2024-12-02-13:07:37-root-INFO: grad norm: 12.220 12.130 1.480
2024-12-02-13:07:38-root-INFO: grad norm: 11.463 11.389 1.300
2024-12-02-13:07:38-root-INFO: grad norm: 10.749 10.669 1.309
2024-12-02-13:07:39-root-INFO: grad norm: 10.147 10.077 1.185
2024-12-02-13:07:39-root-INFO: grad norm: 9.588 9.513 1.197
2024-12-02-13:07:39-root-INFO: grad norm: 9.112 9.046 1.095
2024-12-02-13:07:40-root-INFO: Loss Change: 190.401 -> 186.053
2024-12-02-13:07:40-root-INFO: Regularization Change: 0.000 -> 0.989
2024-12-02-13:07:40-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-13:07:40-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-13:07:40-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-13:07:40-root-INFO: grad norm: 13.914 13.785 1.890
2024-12-02-13:07:40-root-INFO: Loss too large (187.354->188.670)! Learning rate decreased to 0.04147.
2024-12-02-13:07:41-root-INFO: grad norm: 12.733 12.658 1.379
2024-12-02-13:07:41-root-INFO: grad norm: 11.742 11.656 1.421
2024-12-02-13:07:42-root-INFO: grad norm: 10.908 10.839 1.224
2024-12-02-13:07:42-root-INFO: grad norm: 10.142 10.065 1.245
2024-12-02-13:07:43-root-INFO: grad norm: 9.487 9.423 1.107
2024-12-02-13:07:43-root-INFO: grad norm: 8.891 8.819 1.126
2024-12-02-13:07:43-root-INFO: grad norm: 8.379 8.317 1.015
2024-12-02-13:07:44-root-INFO: Loss Change: 187.354 -> 182.990
2024-12-02-13:07:44-root-INFO: Regularization Change: 0.000 -> 0.998
2024-12-02-13:07:44-root-INFO: Undo step: 100
2024-12-02-13:07:44-root-INFO: Undo step: 101
2024-12-02-13:07:44-root-INFO: Undo step: 102
2024-12-02-13:07:44-root-INFO: Undo step: 103
2024-12-02-13:07:44-root-INFO: Undo step: 104
2024-12-02-13:07:44-root-INFO: Undo step: 105
2024-12-02-13:07:44-root-INFO: Undo step: 106
2024-12-02-13:07:44-root-INFO: Undo step: 107
2024-12-02-13:07:44-root-INFO: Undo step: 108
2024-12-02-13:07:44-root-INFO: Undo step: 109
2024-12-02-13:07:44-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-13:07:44-root-INFO: grad norm: 128.338 126.640 20.805
2024-12-02-13:07:45-root-INFO: grad norm: 67.702 66.955 10.023
2024-12-02-13:07:45-root-INFO: grad norm: 46.484 45.722 8.377
2024-12-02-13:07:46-root-INFO: grad norm: 46.545 46.093 6.472
2024-12-02-13:07:46-root-INFO: grad norm: 44.323 43.771 6.974
2024-12-02-13:07:46-root-INFO: grad norm: 40.330 39.984 5.273
2024-12-02-13:07:47-root-INFO: grad norm: 42.644 42.150 6.473
2024-12-02-13:07:47-root-INFO: grad norm: 49.608 49.271 5.771
2024-12-02-13:07:48-root-INFO: Loss too large (286.946->289.798)! Learning rate decreased to 0.03103.
2024-12-02-13:07:48-root-INFO: Loss Change: 714.242 -> 275.883
2024-12-02-13:07:48-root-INFO: Regularization Change: 0.000 -> 70.925
2024-12-02-13:07:48-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-13:07:48-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-13:07:48-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-13:07:48-root-INFO: grad norm: 32.889 32.542 4.762
2024-12-02-13:07:49-root-INFO: grad norm: 36.856 36.555 4.701
2024-12-02-13:07:49-root-INFO: Loss too large (266.360->267.596)! Learning rate decreased to 0.03197.
2024-12-02-13:07:49-root-INFO: grad norm: 29.362 29.028 4.419
2024-12-02-13:07:50-root-INFO: grad norm: 22.879 22.630 3.365
2024-12-02-13:07:50-root-INFO: grad norm: 19.734 19.461 3.273
2024-12-02-13:07:51-root-INFO: grad norm: 17.237 17.012 2.776
2024-12-02-13:07:51-root-INFO: grad norm: 15.611 15.367 2.746
2024-12-02-13:07:52-root-INFO: grad norm: 14.258 14.049 2.431
2024-12-02-13:07:52-root-INFO: Loss Change: 272.449 -> 235.529
2024-12-02-13:07:52-root-INFO: Regularization Change: 0.000 -> 7.365
2024-12-02-13:07:52-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-13:07:52-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-13:07:52-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-13:07:52-root-INFO: grad norm: 11.225 11.008 2.193
2024-12-02-13:07:53-root-INFO: grad norm: 12.477 12.272 2.253
2024-12-02-13:07:53-root-INFO: grad norm: 14.960 14.767 2.395
2024-12-02-13:07:54-root-INFO: grad norm: 19.071 18.876 2.722
2024-12-02-13:07:54-root-INFO: Loss too large (229.262->229.607)! Learning rate decreased to 0.03292.
2024-12-02-13:07:54-root-INFO: grad norm: 17.090 16.894 2.580
2024-12-02-13:07:55-root-INFO: grad norm: 15.526 15.355 2.301
2024-12-02-13:07:55-root-INFO: grad norm: 14.489 14.310 2.272
2024-12-02-13:07:56-root-INFO: grad norm: 13.640 13.483 2.064
2024-12-02-13:07:56-root-INFO: Loss Change: 234.377 -> 220.543
2024-12-02-13:07:56-root-INFO: Regularization Change: 0.000 -> 4.169
2024-12-02-13:07:56-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-13:07:56-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-13:07:56-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-13:07:56-root-INFO: grad norm: 9.811 9.621 1.920
2024-12-02-13:07:57-root-INFO: grad norm: 10.389 10.227 1.827
2024-12-02-13:07:57-root-INFO: grad norm: 12.668 12.518 1.942
2024-12-02-13:07:58-root-INFO: grad norm: 16.537 16.373 2.319
2024-12-02-13:07:58-root-INFO: Loss too large (216.660->217.290)! Learning rate decreased to 0.03391.
2024-12-02-13:07:59-root-INFO: grad norm: 15.154 14.996 2.183
2024-12-02-13:07:59-root-INFO: grad norm: 14.241 14.095 2.031
2024-12-02-13:08:00-root-INFO: grad norm: 13.673 13.525 2.002
2024-12-02-13:08:00-root-INFO: grad norm: 13.316 13.181 1.889
2024-12-02-13:08:00-root-INFO: Loss Change: 219.703 -> 210.927
2024-12-02-13:08:00-root-INFO: Regularization Change: 0.000 -> 2.840
2024-12-02-13:08:00-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-13:08:00-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-13:08:00-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-13:08:01-root-INFO: grad norm: 10.134 9.981 1.754
2024-12-02-13:08:01-root-INFO: grad norm: 12.091 11.960 1.769
2024-12-02-13:08:02-root-INFO: grad norm: 15.599 15.466 2.035
2024-12-02-13:08:02-root-INFO: Loss too large (208.865->209.482)! Learning rate decreased to 0.03491.
2024-12-02-13:08:02-root-INFO: grad norm: 14.310 14.178 1.936
2024-12-02-13:08:03-root-INFO: grad norm: 13.633 13.504 1.874
2024-12-02-13:08:03-root-INFO: grad norm: 13.260 13.136 1.804
2024-12-02-13:08:04-root-INFO: grad norm: 13.145 13.020 1.812
2024-12-02-13:08:04-root-INFO: grad norm: 13.209 13.091 1.759
2024-12-02-13:08:04-root-INFO: Loss Change: 209.843 -> 203.583
2024-12-02-13:08:04-root-INFO: Regularization Change: 0.000 -> 2.080
2024-12-02-13:08:04-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-13:08:04-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-13:08:04-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-13:08:05-root-INFO: grad norm: 10.197 10.086 1.507
2024-12-02-13:08:05-root-INFO: grad norm: 13.867 13.753 1.777
2024-12-02-13:08:05-root-INFO: Loss too large (202.717->203.738)! Learning rate decreased to 0.03594.
2024-12-02-13:08:06-root-INFO: grad norm: 13.748 13.633 1.775
2024-12-02-13:08:06-root-INFO: grad norm: 13.895 13.782 1.771
2024-12-02-13:08:07-root-INFO: grad norm: 14.125 14.007 1.823
2024-12-02-13:08:07-root-INFO: grad norm: 14.466 14.354 1.796
2024-12-02-13:08:07-root-INFO: grad norm: 14.816 14.696 1.881
2024-12-02-13:08:08-root-INFO: grad norm: 15.244 15.132 1.843
2024-12-02-13:08:08-root-INFO: Loss Change: 202.941 -> 198.808
2024-12-02-13:08:08-root-INFO: Regularization Change: 0.000 -> 1.646
2024-12-02-13:08:08-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-13:08:08-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-13:08:08-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-13:08:09-root-INFO: grad norm: 11.209 11.100 1.559
2024-12-02-13:08:09-root-INFO: Loss too large (197.495->197.548)! Learning rate decreased to 0.03700.
2024-12-02-13:08:09-root-INFO: grad norm: 10.701 10.603 1.442
2024-12-02-13:08:10-root-INFO: grad norm: 10.988 10.895 1.426
2024-12-02-13:08:10-root-INFO: grad norm: 11.467 11.370 1.485
2024-12-02-13:08:11-root-INFO: grad norm: 11.968 11.871 1.520
2024-12-02-13:08:11-root-INFO: grad norm: 12.545 12.448 1.558
2024-12-02-13:08:12-root-INFO: grad norm: 13.093 12.993 1.621
2024-12-02-13:08:12-root-INFO: grad norm: 13.695 13.595 1.646
2024-12-02-13:08:12-root-INFO: Loss Change: 197.495 -> 193.557
2024-12-02-13:08:12-root-INFO: Regularization Change: 0.000 -> 1.382
2024-12-02-13:08:12-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-13:08:12-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-13:08:12-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-13:08:13-root-INFO: grad norm: 10.613 10.525 1.363
2024-12-02-13:08:13-root-INFO: Loss too large (192.602->192.937)! Learning rate decreased to 0.03808.
2024-12-02-13:08:13-root-INFO: grad norm: 10.575 10.485 1.372
2024-12-02-13:08:14-root-INFO: grad norm: 10.896 10.806 1.398
2024-12-02-13:08:14-root-INFO: grad norm: 11.360 11.271 1.416
2024-12-02-13:08:15-root-INFO: grad norm: 11.844 11.750 1.484
2024-12-02-13:08:15-root-INFO: grad norm: 12.393 12.303 1.492
2024-12-02-13:08:16-root-INFO: grad norm: 12.920 12.823 1.580
2024-12-02-13:08:16-root-INFO: grad norm: 13.490 13.397 1.580
2024-12-02-13:08:16-root-INFO: Loss Change: 192.602 -> 189.303
2024-12-02-13:08:16-root-INFO: Regularization Change: 0.000 -> 1.265
2024-12-02-13:08:16-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-13:08:16-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-13:08:16-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-13:08:17-root-INFO: grad norm: 10.674 10.585 1.369
2024-12-02-13:08:17-root-INFO: Loss too large (188.332->188.763)! Learning rate decreased to 0.03918.
2024-12-02-13:08:17-root-INFO: grad norm: 10.827 10.737 1.393
2024-12-02-13:08:18-root-INFO: grad norm: 11.402 11.320 1.372
2024-12-02-13:08:18-root-INFO: grad norm: 12.097 12.007 1.476
2024-12-02-13:08:19-root-INFO: grad norm: 12.767 12.678 1.507
2024-12-02-13:08:19-root-INFO: grad norm: 13.477 13.384 1.582
2024-12-02-13:08:20-root-INFO: grad norm: 14.118 14.022 1.642
2024-12-02-13:08:20-root-INFO: grad norm: 14.776 14.679 1.691
2024-12-02-13:08:20-root-INFO: Loss Change: 188.332 -> 185.709
2024-12-02-13:08:20-root-INFO: Regularization Change: 0.000 -> 1.223
2024-12-02-13:08:20-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-13:08:20-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-13:08:21-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-13:08:21-root-INFO: grad norm: 11.649 11.565 1.396
2024-12-02-13:08:21-root-INFO: Loss too large (184.645->185.555)! Learning rate decreased to 0.04031.
2024-12-02-13:08:21-root-INFO: grad norm: 11.858 11.773 1.414
2024-12-02-13:08:22-root-INFO: grad norm: 12.310 12.225 1.447
2024-12-02-13:08:22-root-INFO: grad norm: 12.850 12.764 1.487
2024-12-02-13:08:23-root-INFO: grad norm: 13.360 13.269 1.555
2024-12-02-13:08:23-root-INFO: grad norm: 13.890 13.800 1.576
2024-12-02-13:08:24-root-INFO: grad norm: 14.359 14.263 1.655
2024-12-02-13:08:24-root-INFO: grad norm: 14.829 14.736 1.661
2024-12-02-13:08:25-root-INFO: Loss Change: 184.645 -> 182.143
2024-12-02-13:08:25-root-INFO: Regularization Change: 0.000 -> 1.147
2024-12-02-13:08:25-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-13:08:25-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-13:08:25-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-13:08:25-root-INFO: grad norm: 11.436 11.362 1.302
2024-12-02-13:08:25-root-INFO: Loss too large (181.055->181.960)! Learning rate decreased to 0.04147.
2024-12-02-13:08:25-root-INFO: grad norm: 11.455 11.372 1.373
2024-12-02-13:08:26-root-INFO: grad norm: 11.719 11.640 1.355
2024-12-02-13:08:26-root-INFO: grad norm: 12.068 11.986 1.410
2024-12-02-13:08:27-root-INFO: grad norm: 12.415 12.332 1.432
2024-12-02-13:08:27-root-INFO: grad norm: 12.788 12.704 1.462
2024-12-02-13:08:28-root-INFO: grad norm: 13.135 13.049 1.502
2024-12-02-13:08:28-root-INFO: grad norm: 13.490 13.405 1.517
2024-12-02-13:08:29-root-INFO: Loss Change: 181.055 -> 178.398
2024-12-02-13:08:29-root-INFO: Regularization Change: 0.000 -> 1.101
2024-12-02-13:08:29-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-13:08:29-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-13:08:29-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-13:08:29-root-INFO: grad norm: 10.644 10.571 1.247
2024-12-02-13:08:29-root-INFO: Loss too large (177.433->178.159)! Learning rate decreased to 0.04265.
2024-12-02-13:08:30-root-INFO: grad norm: 10.609 10.534 1.261
2024-12-02-13:08:30-root-INFO: grad norm: 10.860 10.789 1.242
2024-12-02-13:08:31-root-INFO: grad norm: 11.185 11.110 1.296
2024-12-02-13:08:31-root-INFO: grad norm: 11.513 11.438 1.313
2024-12-02-13:08:32-root-INFO: grad norm: 11.857 11.780 1.346
2024-12-02-13:08:32-root-INFO: grad norm: 12.183 12.104 1.379
2024-12-02-13:08:33-root-INFO: grad norm: 12.508 12.430 1.397
2024-12-02-13:08:33-root-INFO: Loss Change: 177.433 -> 174.819
2024-12-02-13:08:33-root-INFO: Regularization Change: 0.000 -> 1.080
2024-12-02-13:08:33-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-13:08:33-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-13:08:33-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-13:08:33-root-INFO: grad norm: 9.248 9.183 1.093
2024-12-02-13:08:33-root-INFO: Loss too large (173.941->174.348)! Learning rate decreased to 0.04386.
2024-12-02-13:08:34-root-INFO: grad norm: 9.197 9.130 1.112
2024-12-02-13:08:34-root-INFO: grad norm: 9.487 9.424 1.095
2024-12-02-13:08:35-root-INFO: grad norm: 9.871 9.803 1.155
2024-12-02-13:08:35-root-INFO: grad norm: 10.284 10.216 1.177
2024-12-02-13:08:36-root-INFO: grad norm: 10.725 10.655 1.221
2024-12-02-13:08:36-root-INFO: grad norm: 11.168 11.096 1.260
2024-12-02-13:08:37-root-INFO: grad norm: 11.617 11.545 1.293
2024-12-02-13:08:37-root-INFO: Loss Change: 173.941 -> 171.541
2024-12-02-13:08:37-root-INFO: Regularization Change: 0.000 -> 1.073
2024-12-02-13:08:37-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-13:08:37-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-13:08:37-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-13:08:37-root-INFO: grad norm: 8.831 8.756 1.153
2024-12-02-13:08:38-root-INFO: Loss too large (170.672->170.989)! Learning rate decreased to 0.04509.
2024-12-02-13:08:38-root-INFO: grad norm: 8.630 8.570 1.015
2024-12-02-13:08:39-root-INFO: grad norm: 8.860 8.801 1.022
2024-12-02-13:08:39-root-INFO: grad norm: 9.217 9.156 1.058
2024-12-02-13:08:39-root-INFO: grad norm: 9.627 9.565 1.090
2024-12-02-13:08:40-root-INFO: grad norm: 10.066 10.003 1.126
2024-12-02-13:08:40-root-INFO: grad norm: 10.524 10.458 1.172
2024-12-02-13:08:41-root-INFO: grad norm: 10.983 10.917 1.202
2024-12-02-13:08:41-root-INFO: Loss Change: 170.672 -> 168.309
2024-12-02-13:08:41-root-INFO: Regularization Change: 0.000 -> 1.066
2024-12-02-13:08:41-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-13:08:41-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-13:08:41-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-13:08:41-root-INFO: grad norm: 8.672 8.611 1.021
2024-12-02-13:08:42-root-INFO: Loss too large (167.706->168.134)! Learning rate decreased to 0.04636.
2024-12-02-13:08:42-root-INFO: grad norm: 8.593 8.535 1.001
2024-12-02-13:08:43-root-INFO: grad norm: 8.810 8.753 0.996
2024-12-02-13:08:43-root-INFO: grad norm: 9.119 9.059 1.039
2024-12-02-13:08:43-root-INFO: grad norm: 9.470 9.411 1.057
2024-12-02-13:08:44-root-INFO: grad norm: 9.848 9.787 1.094
2024-12-02-13:08:44-root-INFO: grad norm: 10.243 10.181 1.127
2024-12-02-13:08:45-root-INFO: grad norm: 10.641 10.578 1.156
2024-12-02-13:08:45-root-INFO: Loss Change: 167.706 -> 165.438
2024-12-02-13:08:45-root-INFO: Regularization Change: 0.000 -> 1.042
2024-12-02-13:08:45-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-13:08:45-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-13:08:45-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-13:08:45-root-INFO: grad norm: 7.843 7.783 0.969
2024-12-02-13:08:46-root-INFO: Loss too large (164.923->165.167)! Learning rate decreased to 0.04765.
2024-12-02-13:08:46-root-INFO: grad norm: 7.744 7.689 0.918
2024-12-02-13:08:46-root-INFO: grad norm: 8.014 7.962 0.915
2024-12-02-13:08:47-root-INFO: grad norm: 8.362 8.308 0.952
2024-12-02-13:08:47-root-INFO: grad norm: 8.762 8.707 0.975
2024-12-02-13:08:48-root-INFO: grad norm: 9.174 9.118 1.011
2024-12-02-13:08:48-root-INFO: grad norm: 9.615 9.558 1.047
2024-12-02-13:08:49-root-INFO: grad norm: 10.049 9.991 1.080
2024-12-02-13:08:49-root-INFO: Loss Change: 164.923 -> 162.748
2024-12-02-13:08:49-root-INFO: Regularization Change: 0.000 -> 1.047
2024-12-02-13:08:49-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-13:08:49-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-13:08:49-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-13:08:49-root-INFO: grad norm: 7.894 7.830 1.003
2024-12-02-13:08:49-root-INFO: Loss too large (162.070->162.373)! Learning rate decreased to 0.04897.
2024-12-02-13:08:50-root-INFO: grad norm: 7.758 7.708 0.883
2024-12-02-13:08:50-root-INFO: grad norm: 7.985 7.934 0.903
2024-12-02-13:08:51-root-INFO: grad norm: 8.303 8.252 0.915
2024-12-02-13:08:51-root-INFO: grad norm: 8.681 8.629 0.951
2024-12-02-13:08:52-root-INFO: grad norm: 9.067 9.015 0.973
2024-12-02-13:08:52-root-INFO: grad norm: 9.485 9.430 1.018
2024-12-02-13:08:53-root-INFO: grad norm: 9.890 9.835 1.041
2024-12-02-13:08:53-root-INFO: Loss Change: 162.070 -> 159.926
2024-12-02-13:08:53-root-INFO: Regularization Change: 0.000 -> 1.049
2024-12-02-13:08:53-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-13:08:53-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-13:08:53-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-13:08:53-root-INFO: grad norm: 7.491 7.430 0.953
2024-12-02-13:08:54-root-INFO: Loss too large (159.191->159.360)! Learning rate decreased to 0.05031.
2024-12-02-13:08:54-root-INFO: grad norm: 7.212 7.164 0.830
2024-12-02-13:08:54-root-INFO: grad norm: 7.369 7.323 0.817
2024-12-02-13:08:55-root-INFO: grad norm: 7.620 7.572 0.854
2024-12-02-13:08:55-root-INFO: grad norm: 7.929 7.883 0.858
2024-12-02-13:08:56-root-INFO: grad norm: 8.247 8.198 0.899
2024-12-02-13:08:56-root-INFO: grad norm: 8.605 8.556 0.912
2024-12-02-13:08:57-root-INFO: grad norm: 8.948 8.897 0.952
2024-12-02-13:08:57-root-INFO: Loss Change: 159.191 -> 156.909
2024-12-02-13:08:57-root-INFO: Regularization Change: 0.000 -> 1.065
2024-12-02-13:08:57-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-13:08:57-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-13:08:57-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-13:08:57-root-INFO: grad norm: 7.337 7.280 0.911
2024-12-02-13:08:58-root-INFO: Loss too large (156.564->156.743)! Learning rate decreased to 0.05169.
2024-12-02-13:08:58-root-INFO: grad norm: 7.012 6.965 0.808
2024-12-02-13:08:58-root-INFO: grad norm: 7.091 7.043 0.819
2024-12-02-13:08:59-root-INFO: grad norm: 7.306 7.260 0.818
2024-12-02-13:08:59-root-INFO: grad norm: 7.587 7.539 0.856
2024-12-02-13:09:00-root-INFO: grad norm: 7.931 7.884 0.866
2024-12-02-13:09:00-root-INFO: grad norm: 8.307 8.256 0.917
2024-12-02-13:09:01-root-INFO: grad norm: 8.734 8.684 0.936
2024-12-02-13:09:01-root-INFO: Loss Change: 156.564 -> 154.373
2024-12-02-13:09:01-root-INFO: Regularization Change: 0.000 -> 1.065
2024-12-02-13:09:01-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-13:09:01-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-13:09:01-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-13:09:01-root-INFO: grad norm: 6.693 6.650 0.761
2024-12-02-13:09:02-root-INFO: Loss too large (153.845->154.043)! Learning rate decreased to 0.05310.
2024-12-02-13:09:02-root-INFO: grad norm: 6.669 6.623 0.778
2024-12-02-13:09:03-root-INFO: grad norm: 6.922 6.878 0.785
2024-12-02-13:09:03-root-INFO: grad norm: 7.323 7.276 0.834
2024-12-02-13:09:03-root-INFO: grad norm: 7.681 7.633 0.858
2024-12-02-13:09:04-root-INFO: grad norm: 8.082 8.032 0.898
2024-12-02-13:09:04-root-INFO: grad norm: 8.447 8.397 0.924
2024-12-02-13:09:05-root-INFO: grad norm: 8.785 8.734 0.952
2024-12-02-13:09:05-root-INFO: Loss Change: 153.845 -> 151.932
2024-12-02-13:09:05-root-INFO: Regularization Change: 0.000 -> 1.046
2024-12-02-13:09:05-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-13:09:05-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-13:09:05-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-13:09:06-root-INFO: grad norm: 6.872 6.833 0.736
2024-12-02-13:09:06-root-INFO: Loss too large (151.549->151.740)! Learning rate decreased to 0.05453.
2024-12-02-13:09:06-root-INFO: grad norm: 6.785 6.736 0.813
2024-12-02-13:09:07-root-INFO: grad norm: 7.007 6.965 0.766
2024-12-02-13:09:07-root-INFO: grad norm: 7.359 7.310 0.848
2024-12-02-13:09:08-root-INFO: grad norm: 7.622 7.576 0.834
2024-12-02-13:09:08-root-INFO: grad norm: 7.873 7.823 0.884
2024-12-02-13:09:09-root-INFO: grad norm: 8.151 8.103 0.880
2024-12-02-13:09:09-root-INFO: grad norm: 8.402 8.351 0.920
2024-12-02-13:09:09-root-INFO: Loss Change: 151.549 -> 149.514
2024-12-02-13:09:09-root-INFO: Regularization Change: 0.000 -> 1.056
2024-12-02-13:09:09-root-INFO: Undo step: 90
2024-12-02-13:09:09-root-INFO: Undo step: 91
2024-12-02-13:09:09-root-INFO: Undo step: 92
2024-12-02-13:09:09-root-INFO: Undo step: 93
2024-12-02-13:09:09-root-INFO: Undo step: 94
2024-12-02-13:09:09-root-INFO: Undo step: 95
2024-12-02-13:09:09-root-INFO: Undo step: 96
2024-12-02-13:09:09-root-INFO: Undo step: 97
2024-12-02-13:09:09-root-INFO: Undo step: 98
2024-12-02-13:09:09-root-INFO: Undo step: 99
2024-12-02-13:09:09-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-13:09:10-root-INFO: grad norm: 87.907 86.696 14.540
2024-12-02-13:09:10-root-INFO: grad norm: 55.063 54.377 8.663
2024-12-02-13:09:11-root-INFO: grad norm: 34.374 33.747 6.536
2024-12-02-13:09:11-root-INFO: grad norm: 31.898 31.530 4.829
2024-12-02-13:09:11-root-INFO: grad norm: 35.015 34.655 5.012
2024-12-02-13:09:12-root-INFO: grad norm: 39.546 39.324 4.184
2024-12-02-13:09:12-root-INFO: grad norm: 42.246 41.937 5.106
2024-12-02-13:09:13-root-INFO: grad norm: 42.635 42.424 4.234
2024-12-02-13:09:13-root-INFO: Loss Change: 611.644 -> 242.371
2024-12-02-13:09:13-root-INFO: Regularization Change: 0.000 -> 94.941
2024-12-02-13:09:13-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-13:09:13-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-13:09:13-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-13:09:14-root-INFO: grad norm: 37.485 37.235 4.321
2024-12-02-13:09:14-root-INFO: grad norm: 34.959 34.771 3.619
2024-12-02-13:09:15-root-INFO: grad norm: 32.917 32.655 4.143
2024-12-02-13:09:15-root-INFO: grad norm: 31.536 31.352 3.401
2024-12-02-13:09:16-root-INFO: grad norm: 30.931 30.675 3.969
2024-12-02-13:09:16-root-INFO: grad norm: 30.980 30.789 3.441
2024-12-02-13:09:16-root-INFO: grad norm: 31.681 31.423 4.033
2024-12-02-13:09:17-root-INFO: grad norm: 32.660 32.451 3.688
2024-12-02-13:09:17-root-INFO: Loss Change: 239.076 -> 207.301
2024-12-02-13:09:17-root-INFO: Regularization Change: 0.000 -> 12.076
2024-12-02-13:09:17-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-13:09:17-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-13:09:17-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-13:09:18-root-INFO: grad norm: 29.862 29.655 3.512
2024-12-02-13:09:18-root-INFO: grad norm: 30.154 29.957 3.446
2024-12-02-13:09:19-root-INFO: grad norm: 30.118 29.883 3.758
2024-12-02-13:09:19-root-INFO: grad norm: 29.915 29.722 3.392
2024-12-02-13:09:20-root-INFO: grad norm: 29.599 29.360 3.754
2024-12-02-13:09:20-root-INFO: grad norm: 29.324 29.135 3.328
2024-12-02-13:09:20-root-INFO: grad norm: 29.056 28.820 3.696
2024-12-02-13:09:21-root-INFO: grad norm: 28.806 28.618 3.278
2024-12-02-13:09:21-root-INFO: Loss Change: 204.020 -> 190.669
2024-12-02-13:09:21-root-INFO: Regularization Change: 0.000 -> 5.654
2024-12-02-13:09:21-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-13:09:21-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-13:09:21-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-13:09:22-root-INFO: grad norm: 25.083 24.916 2.893
2024-12-02-13:09:22-root-INFO: grad norm: 25.126 24.962 2.865
2024-12-02-13:09:22-root-INFO: grad norm: 25.334 25.137 3.147
2024-12-02-13:09:23-root-INFO: grad norm: 25.677 25.507 2.948
2024-12-02-13:09:23-root-INFO: grad norm: 25.951 25.744 3.272
2024-12-02-13:09:24-root-INFO: grad norm: 26.215 26.040 3.018
2024-12-02-13:09:24-root-INFO: grad norm: 26.373 26.161 3.338
2024-12-02-13:09:25-root-INFO: grad norm: 26.476 26.299 3.056
2024-12-02-13:09:25-root-INFO: Loss Change: 187.468 -> 180.462
2024-12-02-13:09:25-root-INFO: Regularization Change: 0.000 -> 3.717
2024-12-02-13:09:25-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-13:09:25-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-13:09:25-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-13:09:25-root-INFO: grad norm: 23.539 23.378 2.745
2024-12-02-13:09:26-root-INFO: grad norm: 23.610 23.452 2.729
2024-12-02-13:09:26-root-INFO: grad norm: 23.773 23.589 2.954
2024-12-02-13:09:27-root-INFO: grad norm: 24.022 23.859 2.799
2024-12-02-13:09:27-root-INFO: grad norm: 24.201 24.007 3.059
2024-12-02-13:09:28-root-INFO: grad norm: 24.349 24.182 2.841
2024-12-02-13:09:28-root-INFO: grad norm: 24.438 24.241 3.096
2024-12-02-13:09:29-root-INFO: grad norm: 24.487 24.320 2.862
2024-12-02-13:09:29-root-INFO: Loss Change: 178.248 -> 172.938
2024-12-02-13:09:29-root-INFO: Regularization Change: 0.000 -> 2.827
2024-12-02-13:09:29-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-13:09:29-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-13:09:29-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-13:09:29-root-INFO: grad norm: 21.076 20.939 2.398
2024-12-02-13:09:30-root-INFO: grad norm: 21.323 21.178 2.487
2024-12-02-13:09:30-root-INFO: grad norm: 21.704 21.538 2.685
2024-12-02-13:09:31-root-INFO: grad norm: 22.160 22.006 2.607
2024-12-02-13:09:31-root-INFO: grad norm: 22.508 22.329 2.837
2024-12-02-13:09:31-root-INFO: grad norm: 22.805 22.647 2.684
2024-12-02-13:09:32-root-INFO: grad norm: 23.016 22.832 2.906
2024-12-02-13:09:32-root-INFO: grad norm: 23.177 23.016 2.732
2024-12-02-13:09:33-root-INFO: Loss Change: 170.636 -> 167.283
2024-12-02-13:09:33-root-INFO: Regularization Change: 0.000 -> 2.366
2024-12-02-13:09:33-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-13:09:33-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-13:09:33-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-13:09:33-root-INFO: grad norm: 20.270 20.137 2.313
2024-12-02-13:09:33-root-INFO: grad norm: 20.645 20.503 2.414
2024-12-02-13:09:34-root-INFO: grad norm: 21.042 20.881 2.601
2024-12-02-13:09:34-root-INFO: grad norm: 21.489 21.338 2.543
2024-12-02-13:09:35-root-INFO: grad norm: 21.784 21.610 2.747
2024-12-02-13:09:35-root-INFO: grad norm: 21.999 21.844 2.601
2024-12-02-13:09:36-root-INFO: grad norm: 22.143 21.966 2.794
2024-12-02-13:09:36-root-INFO: grad norm: 22.249 22.092 2.640
2024-12-02-13:09:37-root-INFO: Loss Change: 164.908 -> 162.234
2024-12-02-13:09:37-root-INFO: Regularization Change: 0.000 -> 2.071
2024-12-02-13:09:37-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-13:09:37-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-13:09:37-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-13:09:37-root-INFO: grad norm: 18.787 18.673 2.066
2024-12-02-13:09:37-root-INFO: grad norm: 18.949 18.814 2.256
2024-12-02-13:09:38-root-INFO: grad norm: 19.247 19.104 2.346
2024-12-02-13:09:38-root-INFO: grad norm: 19.634 19.491 2.361
2024-12-02-13:09:39-root-INFO: grad norm: 19.902 19.746 2.490
2024-12-02-13:09:39-root-INFO: grad norm: 20.102 19.958 2.403
2024-12-02-13:09:40-root-INFO: grad norm: 20.288 20.129 2.535
2024-12-02-13:09:40-root-INFO: grad norm: 20.460 20.313 2.450
2024-12-02-13:09:40-root-INFO: Loss Change: 159.652 -> 157.072
2024-12-02-13:09:40-root-INFO: Regularization Change: 0.000 -> 1.900
2024-12-02-13:09:40-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-13:09:40-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-13:09:41-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-13:09:41-root-INFO: grad norm: 17.997 17.880 2.048
2024-12-02-13:09:41-root-INFO: grad norm: 18.100 17.976 2.118
2024-12-02-13:09:42-root-INFO: grad norm: 18.403 18.263 2.266
2024-12-02-13:09:42-root-INFO: grad norm: 18.892 18.756 2.258
2024-12-02-13:09:43-root-INFO: grad norm: 19.171 19.016 2.434
2024-12-02-13:09:43-root-INFO: grad norm: 19.345 19.208 2.300
2024-12-02-13:09:44-root-INFO: grad norm: 19.555 19.400 2.459
2024-12-02-13:09:44-root-INFO: grad norm: 19.786 19.643 2.372
2024-12-02-13:09:44-root-INFO: Loss Change: 155.574 -> 153.372
2024-12-02-13:09:44-root-INFO: Regularization Change: 0.000 -> 1.789
2024-12-02-13:09:44-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-13:09:44-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-13:09:45-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-13:09:45-root-INFO: grad norm: 17.026 16.913 1.956
2024-12-02-13:09:45-root-INFO: grad norm: 17.199 17.075 2.062
2024-12-02-13:09:46-root-INFO: grad norm: 17.479 17.345 2.163
2024-12-02-13:09:46-root-INFO: grad norm: 17.869 17.736 2.181
2024-12-02-13:09:47-root-INFO: grad norm: 18.071 17.925 2.293
2024-12-02-13:09:47-root-INFO: grad norm: 18.158 18.024 2.197
2024-12-02-13:09:47-root-INFO: grad norm: 18.309 18.163 2.302
2024-12-02-13:09:48-root-INFO: grad norm: 18.504 18.366 2.259
2024-12-02-13:09:48-root-INFO: Loss Change: 151.520 -> 149.377
2024-12-02-13:09:48-root-INFO: Regularization Change: 0.000 -> 1.679
2024-12-02-13:09:48-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-13:09:48-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-13:09:48-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-13:09:48-root-INFO: grad norm: 15.695 15.598 1.735
2024-12-02-13:09:49-root-INFO: grad norm: 15.604 15.488 1.902
2024-12-02-13:09:49-root-INFO: grad norm: 15.731 15.614 1.918
2024-12-02-13:09:50-root-INFO: grad norm: 16.060 15.936 1.992
2024-12-02-13:09:50-root-INFO: grad norm: 16.158 16.028 2.043
2024-12-02-13:09:51-root-INFO: grad norm: 16.143 16.022 1.970
2024-12-02-13:09:51-root-INFO: grad norm: 16.285 16.158 2.029
2024-12-02-13:09:52-root-INFO: grad norm: 16.549 16.423 2.042
2024-12-02-13:09:52-root-INFO: Loss Change: 147.778 -> 145.322
2024-12-02-13:09:52-root-INFO: Regularization Change: 0.000 -> 1.631
2024-12-02-13:09:52-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-13:09:52-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-13:09:52-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-13:09:53-root-INFO: grad norm: 13.235 13.153 1.475
2024-12-02-13:09:53-root-INFO: grad norm: 13.400 13.297 1.655
2024-12-02-13:09:54-root-INFO: grad norm: 13.838 13.729 1.730
2024-12-02-13:09:54-root-INFO: grad norm: 14.508 14.392 1.832
2024-12-02-13:09:54-root-INFO: Loss too large (142.580->142.602)! Learning rate decreased to 0.05599.
2024-12-02-13:09:55-root-INFO: grad norm: 9.451 9.356 1.332
2024-12-02-13:09:55-root-INFO: grad norm: 6.321 6.259 0.878
2024-12-02-13:09:56-root-INFO: grad norm: 4.667 4.607 0.745
2024-12-02-13:09:56-root-INFO: grad norm: 3.739 3.686 0.630
2024-12-02-13:09:56-root-INFO: Loss Change: 143.267 -> 137.682
2024-12-02-13:09:56-root-INFO: Regularization Change: 0.000 -> 1.419
2024-12-02-13:09:56-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-13:09:56-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-13:09:57-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-13:09:57-root-INFO: grad norm: 3.027 2.955 0.656
2024-12-02-13:09:57-root-INFO: grad norm: 2.534 2.479 0.526
2024-12-02-13:09:58-root-INFO: grad norm: 2.482 2.426 0.526
2024-12-02-13:09:58-root-INFO: grad norm: 2.492 2.439 0.510
2024-12-02-13:09:59-root-INFO: grad norm: 2.530 2.476 0.520
2024-12-02-13:09:59-root-INFO: grad norm: 2.609 2.559 0.509
2024-12-02-13:10:00-root-INFO: grad norm: 2.724 2.672 0.534
2024-12-02-13:10:00-root-INFO: grad norm: 2.950 2.902 0.532
2024-12-02-13:10:00-root-INFO: Loss Change: 137.926 -> 135.089
2024-12-02-13:10:00-root-INFO: Regularization Change: 0.000 -> 1.639
2024-12-02-13:10:00-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-13:10:00-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-13:10:01-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-13:10:01-root-INFO: grad norm: 6.062 5.972 1.040
2024-12-02-13:10:01-root-INFO: grad norm: 6.142 6.094 0.771
2024-12-02-13:10:02-root-INFO: grad norm: 6.752 6.696 0.874
2024-12-02-13:10:02-root-INFO: grad norm: 7.737 7.679 0.945
2024-12-02-13:10:02-root-INFO: Loss too large (134.496->134.582)! Learning rate decreased to 0.05901.
2024-12-02-13:10:03-root-INFO: grad norm: 6.100 6.043 0.832
2024-12-02-13:10:03-root-INFO: grad norm: 4.961 4.903 0.759
2024-12-02-13:10:04-root-INFO: grad norm: 4.353 4.297 0.696
2024-12-02-13:10:04-root-INFO: grad norm: 3.914 3.853 0.687
2024-12-02-13:10:05-root-INFO: Loss Change: 135.290 -> 132.384
2024-12-02-13:10:05-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-02-13:10:05-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-13:10:05-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-13:10:05-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-13:10:05-root-INFO: grad norm: 7.018 6.903 1.262
2024-12-02-13:10:05-root-INFO: Loss too large (132.930->132.974)! Learning rate decreased to 0.06057.
2024-12-02-13:10:05-root-INFO: grad norm: 5.511 5.440 0.878
2024-12-02-13:10:06-root-INFO: grad norm: 4.250 4.187 0.727
2024-12-02-13:10:06-root-INFO: grad norm: 3.817 3.759 0.664
2024-12-02-13:10:07-root-INFO: grad norm: 3.561 3.506 0.625
2024-12-02-13:10:07-root-INFO: grad norm: 3.399 3.340 0.630
2024-12-02-13:10:08-root-INFO: grad norm: 3.280 3.226 0.591
2024-12-02-13:10:08-root-INFO: grad norm: 3.210 3.151 0.611
2024-12-02-13:10:09-root-INFO: Loss Change: 132.930 -> 130.142
2024-12-02-13:10:09-root-INFO: Regularization Change: 0.000 -> 1.068
2024-12-02-13:10:09-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-13:10:09-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-13:10:09-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-13:10:09-root-INFO: grad norm: 6.288 6.171 1.210
2024-12-02-13:10:09-root-INFO: Loss too large (130.523->130.532)! Learning rate decreased to 0.06216.
2024-12-02-13:10:09-root-INFO: grad norm: 5.060 4.997 0.799
2024-12-02-13:10:10-root-INFO: grad norm: 3.947 3.884 0.705
2024-12-02-13:10:10-root-INFO: grad norm: 3.681 3.626 0.632
2024-12-02-13:10:11-root-INFO: grad norm: 3.561 3.505 0.626
2024-12-02-13:10:11-root-INFO: grad norm: 3.468 3.411 0.624
2024-12-02-13:10:12-root-INFO: grad norm: 3.409 3.356 0.603
2024-12-02-13:10:12-root-INFO: grad norm: 3.369 3.312 0.618
2024-12-02-13:10:13-root-INFO: Loss Change: 130.523 -> 127.917
2024-12-02-13:10:13-root-INFO: Regularization Change: 0.000 -> 1.082
2024-12-02-13:10:13-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-13:10:13-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-13:10:13-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-13:10:13-root-INFO: grad norm: 5.168 5.082 0.939
2024-12-02-13:10:13-root-INFO: grad norm: 5.738 5.663 0.930
2024-12-02-13:10:14-root-INFO: grad norm: 6.988 6.905 1.069
2024-12-02-13:10:14-root-INFO: Loss too large (127.632->127.951)! Learning rate decreased to 0.06378.
2024-12-02-13:10:15-root-INFO: grad norm: 5.559 5.485 0.900
2024-12-02-13:10:15-root-INFO: grad norm: 3.913 3.860 0.641
2024-12-02-13:10:16-root-INFO: grad norm: 3.526 3.476 0.595
2024-12-02-13:10:16-root-INFO: grad norm: 3.428 3.379 0.575
2024-12-02-13:10:16-root-INFO: grad norm: 3.344 3.290 0.597
2024-12-02-13:10:17-root-INFO: Loss Change: 128.005 -> 125.551
2024-12-02-13:10:17-root-INFO: Regularization Change: 0.000 -> 1.198
2024-12-02-13:10:17-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-13:10:17-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-13:10:17-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-13:10:17-root-INFO: grad norm: 6.152 6.053 1.103
2024-12-02-13:10:17-root-INFO: Loss too large (125.930->125.955)! Learning rate decreased to 0.06543.
2024-12-02-13:10:18-root-INFO: grad norm: 5.045 4.984 0.782
2024-12-02-13:10:18-root-INFO: grad norm: 4.262 4.204 0.703
2024-12-02-13:10:19-root-INFO: grad norm: 3.984 3.928 0.667
2024-12-02-13:10:19-root-INFO: grad norm: 3.812 3.759 0.630
2024-12-02-13:10:20-root-INFO: grad norm: 3.698 3.642 0.644
2024-12-02-13:10:20-root-INFO: grad norm: 3.612 3.561 0.602
2024-12-02-13:10:21-root-INFO: grad norm: 3.563 3.506 0.630
2024-12-02-13:10:21-root-INFO: Loss Change: 125.930 -> 123.399
2024-12-02-13:10:21-root-INFO: Regularization Change: 0.000 -> 1.101
2024-12-02-13:10:21-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-13:10:21-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-13:10:21-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-13:10:21-root-INFO: grad norm: 6.054 5.963 1.042
2024-12-02-13:10:21-root-INFO: Loss too large (123.923->124.074)! Learning rate decreased to 0.06711.
2024-12-02-13:10:22-root-INFO: grad norm: 4.889 4.825 0.792
2024-12-02-13:10:23-root-INFO: grad norm: 3.565 3.511 0.615
2024-12-02-13:10:23-root-INFO: grad norm: 3.427 3.376 0.591
2024-12-02-13:10:24-root-INFO: grad norm: 3.514 3.465 0.583
2024-12-02-13:10:24-root-INFO: grad norm: 3.508 3.454 0.611
2024-12-02-13:10:25-root-INFO: grad norm: 3.565 3.517 0.588
2024-12-02-13:10:25-root-INFO: grad norm: 3.558 3.504 0.621
2024-12-02-13:10:25-root-INFO: Loss Change: 123.923 -> 121.506
2024-12-02-13:10:25-root-INFO: Regularization Change: 0.000 -> 1.101
2024-12-02-13:10:25-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-13:10:25-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-13:10:26-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-13:10:26-root-INFO: grad norm: 6.246 6.141 1.141
2024-12-02-13:10:26-root-INFO: Loss too large (121.784->121.914)! Learning rate decreased to 0.06882.
2024-12-02-13:10:26-root-INFO: grad norm: 5.036 4.976 0.773
2024-12-02-13:10:27-root-INFO: grad norm: 3.905 3.848 0.662
2024-12-02-13:10:27-root-INFO: grad norm: 3.763 3.712 0.614
2024-12-02-13:10:28-root-INFO: grad norm: 3.828 3.778 0.616
2024-12-02-13:10:28-root-INFO: grad norm: 3.777 3.724 0.631
2024-12-02-13:10:29-root-INFO: grad norm: 3.752 3.703 0.603
2024-12-02-13:10:29-root-INFO: grad norm: 3.724 3.670 0.629
2024-12-02-13:10:29-root-INFO: Loss Change: 121.784 -> 119.290
2024-12-02-13:10:29-root-INFO: Regularization Change: 0.000 -> 1.139
2024-12-02-13:10:29-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-13:10:29-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-13:10:30-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-13:10:30-root-INFO: grad norm: 6.790 6.685 1.191
2024-12-02-13:10:30-root-INFO: Loss too large (119.944->120.171)! Learning rate decreased to 0.07057.
2024-12-02-13:10:30-root-INFO: grad norm: 5.283 5.221 0.813
2024-12-02-13:10:31-root-INFO: grad norm: 3.710 3.658 0.618
2024-12-02-13:10:31-root-INFO: grad norm: 3.559 3.512 0.577
2024-12-02-13:10:32-root-INFO: grad norm: 3.761 3.716 0.584
2024-12-02-13:10:32-root-INFO: grad norm: 3.770 3.718 0.620
2024-12-02-13:10:33-root-INFO: grad norm: 3.865 3.819 0.595
2024-12-02-13:10:33-root-INFO: grad norm: 3.841 3.788 0.635
2024-12-02-13:10:34-root-INFO: Loss Change: 119.944 -> 117.344
2024-12-02-13:10:34-root-INFO: Regularization Change: 0.000 -> 1.169
2024-12-02-13:10:34-root-INFO: Undo step: 80
2024-12-02-13:10:34-root-INFO: Undo step: 81
2024-12-02-13:10:34-root-INFO: Undo step: 82
2024-12-02-13:10:34-root-INFO: Undo step: 83
2024-12-02-13:10:34-root-INFO: Undo step: 84
2024-12-02-13:10:34-root-INFO: Undo step: 85
2024-12-02-13:10:34-root-INFO: Undo step: 86
2024-12-02-13:10:34-root-INFO: Undo step: 87
2024-12-02-13:10:34-root-INFO: Undo step: 88
2024-12-02-13:10:34-root-INFO: Undo step: 89
2024-12-02-13:10:34-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-13:10:34-root-INFO: grad norm: 64.225 63.257 11.108
2024-12-02-13:10:34-root-INFO: grad norm: 40.313 39.825 6.248
2024-12-02-13:10:35-root-INFO: grad norm: 29.849 29.503 4.531
2024-12-02-13:10:35-root-INFO: grad norm: 20.474 20.172 3.506
2024-12-02-13:10:36-root-INFO: grad norm: 15.580 15.294 2.970
2024-12-02-13:10:36-root-INFO: grad norm: 13.972 13.735 2.561
2024-12-02-13:10:37-root-INFO: grad norm: 13.338 13.109 2.462
2024-12-02-13:10:37-root-INFO: grad norm: 13.013 12.825 2.206
2024-12-02-13:10:38-root-INFO: Loss Change: 477.623 -> 177.101
2024-12-02-13:10:38-root-INFO: Regularization Change: 0.000 -> 97.928
2024-12-02-13:10:38-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-13:10:38-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-13:10:38-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-13:10:38-root-INFO: grad norm: 11.297 11.121 1.985
2024-12-02-13:10:39-root-INFO: grad norm: 11.029 10.874 1.837
2024-12-02-13:10:39-root-INFO: grad norm: 11.339 11.184 1.871
2024-12-02-13:10:39-root-INFO: grad norm: 11.803 11.675 1.728
2024-12-02-13:10:40-root-INFO: grad norm: 12.622 12.480 1.886
2024-12-02-13:10:40-root-INFO: grad norm: 13.471 13.354 1.775
2024-12-02-13:10:41-root-INFO: grad norm: 14.554 14.412 2.031
2024-12-02-13:10:41-root-INFO: grad norm: 15.619 15.496 1.954
2024-12-02-13:10:42-root-INFO: Loss Change: 175.703 -> 156.194
2024-12-02-13:10:42-root-INFO: Regularization Change: 0.000 -> 12.642
2024-12-02-13:10:42-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-13:10:42-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-13:10:42-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-13:10:42-root-INFO: grad norm: 14.342 14.226 1.815
2024-12-02-13:10:42-root-INFO: grad norm: 15.020 14.903 1.870
2024-12-02-13:10:43-root-INFO: grad norm: 15.708 15.566 2.107
2024-12-02-13:10:43-root-INFO: grad norm: 16.362 16.235 2.037
2024-12-02-13:10:44-root-INFO: grad norm: 16.818 16.661 2.296
2024-12-02-13:10:44-root-INFO: grad norm: 17.223 17.086 2.168
2024-12-02-13:10:45-root-INFO: grad norm: 17.358 17.190 2.405
2024-12-02-13:10:45-root-INFO: grad norm: 17.424 17.284 2.208
2024-12-02-13:10:46-root-INFO: Loss Change: 155.176 -> 146.995
2024-12-02-13:10:46-root-INFO: Regularization Change: 0.000 -> 6.076
2024-12-02-13:10:46-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-13:10:46-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-13:10:46-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-13:10:46-root-INFO: grad norm: 14.155 14.046 1.751
2024-12-02-13:10:46-root-INFO: grad norm: 14.102 13.984 1.818
2024-12-02-13:10:47-root-INFO: grad norm: 14.102 13.962 1.983
2024-12-02-13:10:47-root-INFO: grad norm: 14.299 14.173 1.895
2024-12-02-13:10:48-root-INFO: grad norm: 14.268 14.118 2.064
2024-12-02-13:10:48-root-INFO: grad norm: 14.168 14.044 1.867
2024-12-02-13:10:49-root-INFO: grad norm: 14.133 13.987 2.024
2024-12-02-13:10:49-root-INFO: grad norm: 14.188 14.062 1.890
2024-12-02-13:10:50-root-INFO: Loss Change: 144.732 -> 138.335
2024-12-02-13:10:50-root-INFO: Regularization Change: 0.000 -> 3.913
2024-12-02-13:10:50-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-13:10:50-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-13:10:50-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-13:10:50-root-INFO: grad norm: 11.312 11.218 1.454
2024-12-02-13:10:50-root-INFO: grad norm: 11.157 11.054 1.514
2024-12-02-13:10:51-root-INFO: grad norm: 11.168 11.050 1.619
2024-12-02-13:10:51-root-INFO: grad norm: 11.371 11.262 1.566
2024-12-02-13:10:52-root-INFO: grad norm: 11.345 11.219 1.689
2024-12-02-13:10:52-root-INFO: grad norm: 11.221 11.117 1.528
2024-12-02-13:10:53-root-INFO: grad norm: 11.283 11.162 1.646
2024-12-02-13:10:53-root-INFO: grad norm: 11.489 11.381 1.575
2024-12-02-13:10:54-root-INFO: Loss Change: 136.892 -> 132.190
2024-12-02-13:10:54-root-INFO: Regularization Change: 0.000 -> 2.947
2024-12-02-13:10:54-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-13:10:54-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-13:10:54-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-13:10:54-root-INFO: grad norm: 9.033 8.958 1.161
2024-12-02-13:10:54-root-INFO: grad norm: 8.879 8.789 1.264
2024-12-02-13:10:55-root-INFO: grad norm: 9.012 8.914 1.326
2024-12-02-13:10:55-root-INFO: grad norm: 9.390 9.293 1.343
2024-12-02-13:10:56-root-INFO: grad norm: 9.413 9.303 1.432
2024-12-02-13:10:56-root-INFO: grad norm: 9.275 9.183 1.302
2024-12-02-13:10:57-root-INFO: grad norm: 9.446 9.344 1.388
2024-12-02-13:10:57-root-INFO: grad norm: 9.830 9.733 1.381
2024-12-02-13:10:58-root-INFO: Loss Change: 131.017 -> 127.439
2024-12-02-13:10:58-root-INFO: Regularization Change: 0.000 -> 2.453
2024-12-02-13:10:58-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-13:10:58-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-13:10:58-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-13:10:58-root-INFO: grad norm: 8.044 7.969 1.097
2024-12-02-13:10:59-root-INFO: grad norm: 7.790 7.713 1.098
2024-12-02-13:10:59-root-INFO: grad norm: 7.953 7.866 1.173
2024-12-02-13:10:59-root-INFO: grad norm: 8.437 8.351 1.199
2024-12-02-13:11:00-root-INFO: grad norm: 8.443 8.343 1.296
2024-12-02-13:11:00-root-INFO: grad norm: 8.269 8.186 1.164
2024-12-02-13:11:01-root-INFO: grad norm: 8.447 8.354 1.247
2024-12-02-13:11:01-root-INFO: grad norm: 8.858 8.769 1.246
2024-12-02-13:11:02-root-INFO: Loss Change: 126.596 -> 123.542
2024-12-02-13:11:02-root-INFO: Regularization Change: 0.000 -> 2.142
2024-12-02-13:11:02-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-13:11:02-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-13:11:02-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-13:11:02-root-INFO: grad norm: 6.628 6.572 0.863
2024-12-02-13:11:02-root-INFO: grad norm: 6.236 6.170 0.903
2024-12-02-13:11:03-root-INFO: grad norm: 6.433 6.360 0.967
2024-12-02-13:11:03-root-INFO: grad norm: 6.993 6.919 1.010
2024-12-02-13:11:04-root-INFO: grad norm: 7.028 6.940 1.112
2024-12-02-13:11:04-root-INFO: grad norm: 6.898 6.825 0.999
2024-12-02-13:11:05-root-INFO: grad norm: 7.116 7.033 1.086
2024-12-02-13:11:05-root-INFO: grad norm: 7.531 7.454 1.078
2024-12-02-13:11:06-root-INFO: Loss Change: 122.675 -> 119.932
2024-12-02-13:11:06-root-INFO: Regularization Change: 0.000 -> 1.980
2024-12-02-13:11:06-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-13:11:06-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-13:11:06-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-13:11:06-root-INFO: grad norm: 5.930 5.874 0.811
2024-12-02-13:11:06-root-INFO: grad norm: 5.811 5.748 0.852
2024-12-02-13:11:07-root-INFO: grad norm: 5.987 5.913 0.937
2024-12-02-13:11:07-root-INFO: grad norm: 6.356 6.287 0.934
2024-12-02-13:11:08-root-INFO: grad norm: 6.399 6.317 1.024
2024-12-02-13:11:08-root-INFO: grad norm: 6.325 6.256 0.931
2024-12-02-13:11:09-root-INFO: grad norm: 6.494 6.415 1.011
2024-12-02-13:11:09-root-INFO: grad norm: 6.787 6.715 0.988
2024-12-02-13:11:09-root-INFO: Loss Change: 119.509 -> 117.027
2024-12-02-13:11:09-root-INFO: Regularization Change: 0.000 -> 1.852
2024-12-02-13:11:09-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-13:11:09-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-13:11:10-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-13:11:10-root-INFO: grad norm: 4.984 4.942 0.645
2024-12-02-13:11:10-root-INFO: grad norm: 4.724 4.668 0.724
2024-12-02-13:11:11-root-INFO: grad norm: 4.948 4.887 0.778
2024-12-02-13:11:11-root-INFO: grad norm: 5.464 5.402 0.823
2024-12-02-13:11:12-root-INFO: grad norm: 5.509 5.434 0.903
2024-12-02-13:11:12-root-INFO: grad norm: 5.419 5.356 0.825
2024-12-02-13:11:13-root-INFO: grad norm: 5.637 5.564 0.899
2024-12-02-13:11:13-root-INFO: grad norm: 6.007 5.941 0.892
2024-12-02-13:11:13-root-INFO: Loss Change: 116.300 -> 113.992
2024-12-02-13:11:13-root-INFO: Regularization Change: 0.000 -> 1.797
2024-12-02-13:11:13-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-13:11:13-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-13:11:14-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-13:11:14-root-INFO: grad norm: 4.502 4.457 0.632
2024-12-02-13:11:14-root-INFO: grad norm: 4.230 4.182 0.639
2024-12-02-13:11:15-root-INFO: grad norm: 4.455 4.397 0.717
2024-12-02-13:11:15-root-INFO: grad norm: 4.960 4.906 0.733
2024-12-02-13:11:16-root-INFO: grad norm: 5.031 4.962 0.836
2024-12-02-13:11:16-root-INFO: grad norm: 4.985 4.929 0.749
2024-12-02-13:11:17-root-INFO: grad norm: 5.272 5.203 0.850
2024-12-02-13:11:17-root-INFO: grad norm: 5.726 5.665 0.833
2024-12-02-13:11:17-root-INFO: Loss too large (111.498->111.512)! Learning rate decreased to 0.07057.
2024-12-02-13:11:17-root-INFO: Loss Change: 113.650 -> 111.112
2024-12-02-13:11:17-root-INFO: Regularization Change: 0.000 -> 1.677
2024-12-02-13:11:17-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-13:11:17-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-13:11:18-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-13:11:18-root-INFO: grad norm: 4.009 3.964 0.602
2024-12-02-13:11:18-root-INFO: grad norm: 3.995 3.951 0.589
2024-12-02-13:11:19-root-INFO: grad norm: 4.195 4.144 0.655
2024-12-02-13:11:19-root-INFO: grad norm: 4.677 4.627 0.677
2024-12-02-13:11:20-root-INFO: grad norm: 4.735 4.672 0.772
2024-12-02-13:11:20-root-INFO: grad norm: 4.690 4.638 0.696
2024-12-02-13:11:21-root-INFO: grad norm: 4.990 4.926 0.797
2024-12-02-13:11:21-root-INFO: grad norm: 5.455 5.399 0.784
2024-12-02-13:11:21-root-INFO: Loss too large (108.876->108.905)! Learning rate decreased to 0.07235.
2024-12-02-13:11:22-root-INFO: Loss Change: 110.869 -> 108.533
2024-12-02-13:11:22-root-INFO: Regularization Change: 0.000 -> 1.669
2024-12-02-13:11:22-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-13:11:22-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-13:11:22-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-13:11:22-root-INFO: grad norm: 4.090 4.046 0.595
2024-12-02-13:11:22-root-INFO: grad norm: 4.390 4.351 0.585
2024-12-02-13:11:23-root-INFO: grad norm: 4.529 4.475 0.696
2024-12-02-13:11:23-root-INFO: grad norm: 4.761 4.715 0.662
2024-12-02-13:11:24-root-INFO: grad norm: 4.946 4.884 0.777
2024-12-02-13:11:24-root-INFO: grad norm: 5.131 5.080 0.721
2024-12-02-13:11:25-root-INFO: grad norm: 5.396 5.331 0.836
2024-12-02-13:11:25-root-INFO: grad norm: 5.679 5.624 0.790
2024-12-02-13:11:25-root-INFO: Loss too large (106.844->106.859)! Learning rate decreased to 0.07416.
2024-12-02-13:11:26-root-INFO: Loss Change: 108.648 -> 106.431
2024-12-02-13:11:26-root-INFO: Regularization Change: 0.000 -> 1.632
2024-12-02-13:11:26-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-13:11:26-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-13:11:26-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-13:11:26-root-INFO: grad norm: 4.198 4.159 0.569
2024-12-02-13:11:27-root-INFO: grad norm: 4.415 4.377 0.582
2024-12-02-13:11:27-root-INFO: grad norm: 4.578 4.529 0.667
2024-12-02-13:11:28-root-INFO: grad norm: 4.984 4.939 0.670
2024-12-02-13:11:28-root-INFO: Loss too large (105.226->105.248)! Learning rate decreased to 0.07600.
2024-12-02-13:11:28-root-INFO: grad norm: 3.905 3.854 0.626
2024-12-02-13:11:29-root-INFO: grad norm: 2.915 2.881 0.442
2024-12-02-13:11:29-root-INFO: grad norm: 2.721 2.680 0.468
2024-12-02-13:11:30-root-INFO: grad norm: 2.633 2.602 0.400
2024-12-02-13:11:30-root-INFO: Loss Change: 106.101 -> 103.795
2024-12-02-13:11:30-root-INFO: Regularization Change: 0.000 -> 1.347
2024-12-02-13:11:30-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-13:11:30-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-13:11:30-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-13:11:30-root-INFO: grad norm: 3.183 3.125 0.603
2024-12-02-13:11:31-root-INFO: grad norm: 2.678 2.650 0.382
2024-12-02-13:11:31-root-INFO: grad norm: 2.591 2.562 0.388
2024-12-02-13:11:32-root-INFO: grad norm: 2.626 2.601 0.357
2024-12-02-13:11:32-root-INFO: grad norm: 2.791 2.760 0.409
2024-12-02-13:11:33-root-INFO: grad norm: 3.340 3.313 0.422
2024-12-02-13:11:33-root-INFO: grad norm: 3.836 3.794 0.569
2024-12-02-13:11:34-root-INFO: grad norm: 5.083 5.040 0.657
2024-12-02-13:11:34-root-INFO: Loss too large (102.046->102.234)! Learning rate decreased to 0.07788.
2024-12-02-13:11:34-root-INFO: Loss Change: 103.836 -> 101.910
2024-12-02-13:11:34-root-INFO: Regularization Change: 0.000 -> 1.699
2024-12-02-13:11:34-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-13:11:34-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-13:11:34-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-13:11:35-root-INFO: grad norm: 4.426 4.386 0.596
2024-12-02-13:11:35-root-INFO: grad norm: 4.448 4.412 0.563
2024-12-02-13:11:35-root-INFO: Loss too large (101.419->101.425)! Learning rate decreased to 0.07980.
2024-12-02-13:11:36-root-INFO: grad norm: 3.643 3.601 0.548
2024-12-02-13:11:36-root-INFO: grad norm: 2.954 2.922 0.436
2024-12-02-13:11:37-root-INFO: grad norm: 2.888 2.849 0.472
2024-12-02-13:11:37-root-INFO: grad norm: 2.903 2.871 0.425
2024-12-02-13:11:38-root-INFO: grad norm: 2.915 2.875 0.483
2024-12-02-13:11:38-root-INFO: grad norm: 2.965 2.934 0.432
2024-12-02-13:11:39-root-INFO: Loss Change: 101.963 -> 99.747
2024-12-02-13:11:39-root-INFO: Regularization Change: 0.000 -> 1.216
2024-12-02-13:11:39-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-13:11:39-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-13:11:39-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-13:11:39-root-INFO: grad norm: 3.146 3.109 0.483
2024-12-02-13:11:39-root-INFO: grad norm: 3.703 3.675 0.457
2024-12-02-13:11:40-root-INFO: grad norm: 3.871 3.827 0.582
2024-12-02-13:11:40-root-INFO: grad norm: 4.452 4.411 0.601
2024-12-02-13:11:41-root-INFO: Loss too large (99.028->99.191)! Learning rate decreased to 0.08174.
2024-12-02-13:11:41-root-INFO: grad norm: 3.733 3.684 0.602
2024-12-02-13:11:42-root-INFO: grad norm: 2.697 2.664 0.417
2024-12-02-13:11:42-root-INFO: grad norm: 2.734 2.696 0.453
2024-12-02-13:11:43-root-INFO: grad norm: 2.869 2.839 0.416
2024-12-02-13:11:43-root-INFO: Loss Change: 99.790 -> 97.858
2024-12-02-13:11:43-root-INFO: Regularization Change: 0.000 -> 1.356
2024-12-02-13:11:43-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-13:11:43-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-13:11:43-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-13:11:43-root-INFO: grad norm: 3.305 3.263 0.522
2024-12-02-13:11:44-root-INFO: grad norm: 3.699 3.672 0.449
2024-12-02-13:11:44-root-INFO: grad norm: 3.935 3.895 0.558
2024-12-02-13:11:45-root-INFO: grad norm: 4.741 4.701 0.618
2024-12-02-13:11:45-root-INFO: Loss too large (97.126->97.350)! Learning rate decreased to 0.08372.
2024-12-02-13:11:45-root-INFO: grad norm: 3.885 3.837 0.608
2024-12-02-13:11:46-root-INFO: grad norm: 2.680 2.647 0.415
2024-12-02-13:11:47-root-INFO: grad norm: 2.723 2.687 0.443
2024-12-02-13:11:47-root-INFO: grad norm: 2.911 2.881 0.417
2024-12-02-13:11:48-root-INFO: Loss Change: 97.875 -> 95.925
2024-12-02-13:11:48-root-INFO: Regularization Change: 0.000 -> 1.388
2024-12-02-13:11:48-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-13:11:48-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-13:11:48-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-13:11:48-root-INFO: grad norm: 3.284 3.247 0.489
2024-12-02-13:11:48-root-INFO: grad norm: 4.065 4.036 0.486
2024-12-02-13:11:49-root-INFO: Loss too large (95.568->95.660)! Learning rate decreased to 0.08574.
2024-12-02-13:11:49-root-INFO: grad norm: 3.592 3.553 0.528
2024-12-02-13:11:50-root-INFO: grad norm: 3.109 3.078 0.440
2024-12-02-13:11:50-root-INFO: grad norm: 3.168 3.129 0.494
2024-12-02-13:11:51-root-INFO: grad norm: 3.302 3.270 0.455
2024-12-02-13:11:51-root-INFO: grad norm: 3.316 3.275 0.520
2024-12-02-13:11:52-root-INFO: grad norm: 3.341 3.309 0.463
2024-12-02-13:11:52-root-INFO: Loss Change: 95.855 -> 94.111
2024-12-02-13:11:52-root-INFO: Regularization Change: 0.000 -> 1.258
2024-12-02-13:11:52-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-13:11:52-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-13:11:52-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-13:11:52-root-INFO: grad norm: 3.391 3.359 0.460
2024-12-02-13:11:53-root-INFO: grad norm: 4.773 4.738 0.578
2024-12-02-13:11:53-root-INFO: Loss too large (94.003->94.273)! Learning rate decreased to 0.08779.
2024-12-02-13:11:54-root-INFO: grad norm: 3.932 3.887 0.597
2024-12-02-13:11:54-root-INFO: grad norm: 2.728 2.697 0.410
2024-12-02-13:11:55-root-INFO: grad norm: 2.816 2.780 0.447
2024-12-02-13:11:55-root-INFO: grad norm: 3.089 3.060 0.426
2024-12-02-13:11:56-root-INFO: grad norm: 3.167 3.128 0.493
2024-12-02-13:11:56-root-INFO: grad norm: 3.311 3.279 0.454
2024-12-02-13:11:56-root-INFO: Loss Change: 94.196 -> 92.477
2024-12-02-13:11:56-root-INFO: Regularization Change: 0.000 -> 1.245
2024-12-02-13:11:56-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-13:11:56-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-13:11:57-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-13:11:57-root-INFO: grad norm: 3.584 3.536 0.584
2024-12-02-13:11:57-root-INFO: grad norm: 3.979 3.951 0.471
2024-12-02-13:11:57-root-INFO: Loss too large (91.913->91.978)! Learning rate decreased to 0.08987.
2024-12-02-13:11:58-root-INFO: grad norm: 3.513 3.479 0.487
2024-12-02-13:11:58-root-INFO: grad norm: 3.186 3.155 0.448
2024-12-02-13:11:59-root-INFO: grad norm: 3.247 3.211 0.486
2024-12-02-13:11:59-root-INFO: grad norm: 3.370 3.338 0.466
2024-12-02-13:12:00-root-INFO: grad norm: 3.380 3.341 0.512
2024-12-02-13:12:00-root-INFO: grad norm: 3.391 3.359 0.470
2024-12-02-13:12:01-root-INFO: Loss Change: 92.304 -> 90.484
2024-12-02-13:12:01-root-INFO: Regularization Change: 0.000 -> 1.331
2024-12-02-13:12:01-root-INFO: Undo step: 70
2024-12-02-13:12:01-root-INFO: Undo step: 71
2024-12-02-13:12:01-root-INFO: Undo step: 72
2024-12-02-13:12:01-root-INFO: Undo step: 73
2024-12-02-13:12:01-root-INFO: Undo step: 74
2024-12-02-13:12:01-root-INFO: Undo step: 75
2024-12-02-13:12:01-root-INFO: Undo step: 76
2024-12-02-13:12:01-root-INFO: Undo step: 77
2024-12-02-13:12:01-root-INFO: Undo step: 78
2024-12-02-13:12:01-root-INFO: Undo step: 79
2024-12-02-13:12:01-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-13:12:01-root-INFO: grad norm: 50.187 49.549 7.976
2024-12-02-13:12:02-root-INFO: grad norm: 27.993 27.577 4.809
2024-12-02-13:12:02-root-INFO: grad norm: 19.375 19.022 3.682
2024-12-02-13:12:02-root-INFO: grad norm: 14.815 14.534 2.871
2024-12-02-13:12:03-root-INFO: grad norm: 12.155 11.908 2.438
2024-12-02-13:12:03-root-INFO: grad norm: 10.479 10.277 2.046
2024-12-02-13:12:04-root-INFO: grad norm: 9.366 9.188 1.816
2024-12-02-13:12:04-root-INFO: grad norm: 8.679 8.517 1.667
2024-12-02-13:12:05-root-INFO: Loss Change: 403.975 -> 141.219
2024-12-02-13:12:05-root-INFO: Regularization Change: 0.000 -> 114.153
2024-12-02-13:12:05-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-13:12:05-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-13:12:05-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-13:12:05-root-INFO: grad norm: 9.562 9.431 1.577
2024-12-02-13:12:06-root-INFO: grad norm: 8.209 8.065 1.531
2024-12-02-13:12:06-root-INFO: grad norm: 6.614 6.498 1.233
2024-12-02-13:12:07-root-INFO: grad norm: 6.042 5.929 1.160
2024-12-02-13:12:07-root-INFO: grad norm: 5.743 5.637 1.101
2024-12-02-13:12:07-root-INFO: grad norm: 5.528 5.425 1.064
2024-12-02-13:12:08-root-INFO: grad norm: 5.404 5.309 1.009
2024-12-02-13:12:08-root-INFO: grad norm: 5.394 5.296 1.025
2024-12-02-13:12:09-root-INFO: Loss Change: 140.766 -> 121.197
2024-12-02-13:12:09-root-INFO: Regularization Change: 0.000 -> 13.724
2024-12-02-13:12:09-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-13:12:09-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-13:12:09-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-13:12:09-root-INFO: grad norm: 7.337 7.233 1.236
2024-12-02-13:12:10-root-INFO: grad norm: 7.135 7.031 1.210
2024-12-02-13:12:10-root-INFO: grad norm: 6.658 6.573 1.060
2024-12-02-13:12:11-root-INFO: grad norm: 6.941 6.847 1.138
2024-12-02-13:12:11-root-INFO: grad norm: 7.970 7.886 1.150
2024-12-02-13:12:12-root-INFO: grad norm: 7.404 7.304 1.211
2024-12-02-13:12:12-root-INFO: grad norm: 6.628 6.559 0.960
2024-12-02-13:12:13-root-INFO: grad norm: 6.628 6.560 0.953
2024-12-02-13:12:13-root-INFO: Loss Change: 121.599 -> 113.100
2024-12-02-13:12:13-root-INFO: Regularization Change: 0.000 -> 6.474
2024-12-02-13:12:13-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-13:12:13-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-13:12:13-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-13:12:13-root-INFO: grad norm: 8.728 8.631 1.298
2024-12-02-13:12:14-root-INFO: grad norm: 8.758 8.666 1.267
2024-12-02-13:12:14-root-INFO: grad norm: 9.745 9.656 1.309
2024-12-02-13:12:15-root-INFO: grad norm: 8.686 8.586 1.313
2024-12-02-13:12:15-root-INFO: grad norm: 7.850 7.789 0.981
2024-12-02-13:12:16-root-INFO: grad norm: 7.496 7.440 0.914
2024-12-02-13:12:16-root-INFO: grad norm: 7.318 7.266 0.873
2024-12-02-13:12:16-root-INFO: grad norm: 7.504 7.459 0.823
2024-12-02-13:12:17-root-INFO: Loss Change: 113.411 -> 107.631
2024-12-02-13:12:17-root-INFO: Regularization Change: 0.000 -> 4.327
2024-12-02-13:12:17-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-13:12:17-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-13:12:17-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-13:12:17-root-INFO: grad norm: 9.025 8.953 1.137
2024-12-02-13:12:18-root-INFO: grad norm: 8.646 8.599 0.901
2024-12-02-13:12:18-root-INFO: grad norm: 7.983 7.933 0.891
2024-12-02-13:12:19-root-INFO: grad norm: 7.128 7.086 0.767
2024-12-02-13:12:19-root-INFO: grad norm: 6.895 6.849 0.796
2024-12-02-13:12:20-root-INFO: grad norm: 7.204 7.166 0.739
2024-12-02-13:12:20-root-INFO: grad norm: 6.421 6.375 0.765
2024-12-02-13:12:21-root-INFO: grad norm: 5.270 5.233 0.620
2024-12-02-13:12:21-root-INFO: Loss Change: 108.128 -> 102.482
2024-12-02-13:12:21-root-INFO: Regularization Change: 0.000 -> 3.293
2024-12-02-13:12:21-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-13:12:21-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-13:12:21-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-13:12:21-root-INFO: grad norm: 6.647 6.582 0.925
2024-12-02-13:12:22-root-INFO: grad norm: 6.331 6.291 0.712
2024-12-02-13:12:22-root-INFO: grad norm: 6.202 6.159 0.725
2024-12-02-13:12:23-root-INFO: grad norm: 6.952 6.916 0.703
2024-12-02-13:12:23-root-INFO: Loss too large (101.368->101.392)! Learning rate decreased to 0.07980.
2024-12-02-13:12:23-root-INFO: grad norm: 4.691 4.648 0.633
2024-12-02-13:12:24-root-INFO: grad norm: 2.668 2.631 0.444
2024-12-02-13:12:24-root-INFO: grad norm: 2.461 2.416 0.465
2024-12-02-13:12:25-root-INFO: grad norm: 2.604 2.567 0.437
2024-12-02-13:12:25-root-INFO: Loss Change: 102.903 -> 98.941
2024-12-02-13:12:25-root-INFO: Regularization Change: 0.000 -> 2.196
2024-12-02-13:12:25-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-13:12:25-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-13:12:25-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-13:12:25-root-INFO: grad norm: 3.348 3.292 0.608
2024-12-02-13:12:26-root-INFO: grad norm: 3.809 3.778 0.485
2024-12-02-13:12:26-root-INFO: grad norm: 4.134 4.094 0.571
2024-12-02-13:12:27-root-INFO: grad norm: 5.413 5.377 0.626
2024-12-02-13:12:27-root-INFO: Loss too large (98.004->98.233)! Learning rate decreased to 0.08174.
2024-12-02-13:12:27-root-INFO: grad norm: 3.978 3.934 0.589
2024-12-02-13:12:28-root-INFO: grad norm: 2.073 2.035 0.397
2024-12-02-13:12:28-root-INFO: grad norm: 2.054 2.013 0.408
2024-12-02-13:12:29-root-INFO: grad norm: 2.219 2.183 0.393
2024-12-02-13:12:29-root-INFO: Loss Change: 99.016 -> 96.234
2024-12-02-13:12:29-root-INFO: Regularization Change: 0.000 -> 1.902
2024-12-02-13:12:29-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-13:12:29-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-13:12:29-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-13:12:29-root-INFO: grad norm: 3.273 3.214 0.619
2024-12-02-13:12:30-root-INFO: grad norm: 3.293 3.265 0.435
2024-12-02-13:12:30-root-INFO: grad norm: 3.854 3.820 0.515
2024-12-02-13:12:31-root-INFO: grad norm: 6.286 6.250 0.678
2024-12-02-13:12:31-root-INFO: Loss too large (95.487->95.897)! Learning rate decreased to 0.08372.
2024-12-02-13:12:31-root-INFO: Loss too large (95.487->95.544)! Learning rate decreased to 0.06698.
2024-12-02-13:12:32-root-INFO: grad norm: 3.923 3.883 0.558
2024-12-02-13:12:32-root-INFO: grad norm: 1.839 1.802 0.366
2024-12-02-13:12:32-root-INFO: grad norm: 1.791 1.754 0.364
2024-12-02-13:12:33-root-INFO: grad norm: 1.777 1.741 0.357
2024-12-02-13:12:33-root-INFO: Loss Change: 96.262 -> 93.897
2024-12-02-13:12:33-root-INFO: Regularization Change: 0.000 -> 1.419
2024-12-02-13:12:33-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-13:12:33-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-13:12:33-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-13:12:34-root-INFO: grad norm: 2.951 2.889 0.603
2024-12-02-13:12:34-root-INFO: grad norm: 2.789 2.752 0.455
2024-12-02-13:12:34-root-INFO: grad norm: 3.818 3.780 0.540
2024-12-02-13:12:35-root-INFO: Loss too large (93.217->93.327)! Learning rate decreased to 0.08574.
2024-12-02-13:12:35-root-INFO: grad norm: 3.915 3.875 0.557
2024-12-02-13:12:36-root-INFO: grad norm: 4.519 4.483 0.565
2024-12-02-13:12:36-root-INFO: Loss too large (92.757->92.819)! Learning rate decreased to 0.06859.
2024-12-02-13:12:36-root-INFO: grad norm: 3.686 3.649 0.517
2024-12-02-13:12:37-root-INFO: grad norm: 2.619 2.590 0.389
2024-12-02-13:12:37-root-INFO: grad norm: 2.603 2.571 0.411
2024-12-02-13:12:37-root-INFO: Loss Change: 93.885 -> 91.772
2024-12-02-13:12:37-root-INFO: Regularization Change: 0.000 -> 1.344
2024-12-02-13:12:37-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-13:12:37-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-13:12:38-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-13:12:38-root-INFO: grad norm: 3.788 3.740 0.601
2024-12-02-13:12:38-root-INFO: Loss too large (91.984->92.032)! Learning rate decreased to 0.08779.
2024-12-02-13:12:38-root-INFO: grad norm: 3.813 3.776 0.524
2024-12-02-13:12:39-root-INFO: grad norm: 4.508 4.474 0.558
2024-12-02-13:12:39-root-INFO: Loss too large (91.500->91.577)! Learning rate decreased to 0.07023.
2024-12-02-13:12:40-root-INFO: grad norm: 3.674 3.640 0.503
2024-12-02-13:12:40-root-INFO: grad norm: 2.595 2.567 0.383
2024-12-02-13:12:40-root-INFO: grad norm: 2.591 2.560 0.402
2024-12-02-13:12:41-root-INFO: grad norm: 2.615 2.588 0.374
2024-12-02-13:12:41-root-INFO: grad norm: 2.635 2.604 0.404
2024-12-02-13:12:42-root-INFO: Loss Change: 91.984 -> 90.160
2024-12-02-13:12:42-root-INFO: Regularization Change: 0.000 -> 1.008
2024-12-02-13:12:42-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-13:12:42-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-13:12:42-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-13:12:42-root-INFO: grad norm: 5.042 4.956 0.924
2024-12-02-13:12:42-root-INFO: Loss too large (90.288->90.441)! Learning rate decreased to 0.08987.
2024-12-02-13:12:43-root-INFO: grad norm: 4.252 4.215 0.557
2024-12-02-13:12:43-root-INFO: grad norm: 3.824 3.787 0.531
2024-12-02-13:12:43-root-INFO: Loss too large (89.448->89.450)! Learning rate decreased to 0.07189.
2024-12-02-13:12:44-root-INFO: grad norm: 3.401 3.370 0.452
2024-12-02-13:12:44-root-INFO: grad norm: 2.941 2.914 0.401
2024-12-02-13:12:45-root-INFO: grad norm: 2.917 2.888 0.409
2024-12-02-13:12:45-root-INFO: grad norm: 2.905 2.879 0.386
2024-12-02-13:12:46-root-INFO: grad norm: 2.912 2.883 0.408
2024-12-02-13:12:46-root-INFO: Loss Change: 90.288 -> 88.225
2024-12-02-13:12:46-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-02-13:12:46-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-13:12:46-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-13:12:46-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-13:12:46-root-INFO: grad norm: 3.880 3.834 0.598
2024-12-02-13:12:46-root-INFO: Loss too large (88.441->88.611)! Learning rate decreased to 0.09199.
2024-12-02-13:12:47-root-INFO: grad norm: 3.951 3.918 0.506
2024-12-02-13:12:47-root-INFO: grad norm: 4.677 4.645 0.552
2024-12-02-13:12:47-root-INFO: Loss too large (88.008->88.173)! Learning rate decreased to 0.07359.
2024-12-02-13:12:48-root-INFO: grad norm: 3.798 3.767 0.485
2024-12-02-13:12:48-root-INFO: grad norm: 2.603 2.578 0.363
2024-12-02-13:12:49-root-INFO: grad norm: 2.686 2.659 0.382
2024-12-02-13:12:49-root-INFO: grad norm: 2.849 2.825 0.367
2024-12-02-13:12:50-root-INFO: grad norm: 2.925 2.897 0.401
2024-12-02-13:12:50-root-INFO: Loss Change: 88.441 -> 86.739
2024-12-02-13:12:50-root-INFO: Regularization Change: 0.000 -> 0.992
2024-12-02-13:12:50-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-13:12:50-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-13:12:50-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-13:12:50-root-INFO: grad norm: 5.637 5.555 0.959
2024-12-02-13:12:51-root-INFO: Loss too large (87.002->87.351)! Learning rate decreased to 0.09414.
2024-12-02-13:12:51-root-INFO: grad norm: 4.288 4.251 0.561
2024-12-02-13:12:51-root-INFO: grad norm: 2.595 2.561 0.422
2024-12-02-13:12:52-root-INFO: grad norm: 3.076 3.047 0.421
2024-12-02-13:12:52-root-INFO: grad norm: 4.577 4.547 0.521
2024-12-02-13:12:52-root-INFO: Loss too large (85.693->85.873)! Learning rate decreased to 0.07531.
2024-12-02-13:12:53-root-INFO: grad norm: 3.803 3.773 0.475
2024-12-02-13:12:53-root-INFO: grad norm: 2.742 2.717 0.367
2024-12-02-13:12:54-root-INFO: grad norm: 2.824 2.798 0.383
2024-12-02-13:12:54-root-INFO: Loss Change: 87.002 -> 84.807
2024-12-02-13:12:54-root-INFO: Regularization Change: 0.000 -> 1.181
2024-12-02-13:12:54-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-13:12:54-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-13:12:54-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-13:12:54-root-INFO: grad norm: 3.961 3.911 0.623
2024-12-02-13:12:55-root-INFO: Loss too large (84.837->85.085)! Learning rate decreased to 0.09633.
2024-12-02-13:12:55-root-INFO: grad norm: 4.045 4.014 0.501
2024-12-02-13:12:56-root-INFO: grad norm: 4.769 4.737 0.553
2024-12-02-13:12:56-root-INFO: Loss too large (84.422->84.642)! Learning rate decreased to 0.07706.
2024-12-02-13:12:56-root-INFO: grad norm: 3.870 3.841 0.473
2024-12-02-13:12:57-root-INFO: grad norm: 2.651 2.627 0.355
2024-12-02-13:12:57-root-INFO: grad norm: 2.799 2.774 0.372
2024-12-02-13:12:58-root-INFO: grad norm: 3.069 3.047 0.369
2024-12-02-13:12:58-root-INFO: grad norm: 3.168 3.142 0.402
2024-12-02-13:12:58-root-INFO: Loss Change: 84.837 -> 83.195
2024-12-02-13:12:58-root-INFO: Regularization Change: 0.000 -> 1.006
2024-12-02-13:12:58-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-13:12:58-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-13:12:58-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-13:12:59-root-INFO: grad norm: 4.644 4.601 0.633
2024-12-02-13:12:59-root-INFO: Loss too large (83.412->83.875)! Learning rate decreased to 0.09855.
2024-12-02-13:12:59-root-INFO: Loss too large (83.412->83.535)! Learning rate decreased to 0.07884.
2024-12-02-13:12:59-root-INFO: grad norm: 3.842 3.815 0.457
2024-12-02-13:13:00-root-INFO: grad norm: 3.024 3.000 0.380
2024-12-02-13:13:00-root-INFO: grad norm: 3.135 3.110 0.397
2024-12-02-13:13:01-root-INFO: grad norm: 3.328 3.306 0.385
2024-12-02-13:13:01-root-INFO: grad norm: 3.362 3.336 0.416
2024-12-02-13:13:02-root-INFO: grad norm: 3.415 3.392 0.389
2024-12-02-13:13:02-root-INFO: grad norm: 3.417 3.391 0.421
2024-12-02-13:13:03-root-INFO: Loss Change: 83.412 -> 81.824
2024-12-02-13:13:03-root-INFO: Regularization Change: 0.000 -> 0.886
2024-12-02-13:13:03-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-13:13:03-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-13:13:03-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-13:13:03-root-INFO: grad norm: 5.126 5.064 0.793
2024-12-02-13:13:03-root-INFO: Loss too large (82.081->82.555)! Learning rate decreased to 0.10081.
2024-12-02-13:13:03-root-INFO: Loss too large (82.081->82.150)! Learning rate decreased to 0.08065.
2024-12-02-13:13:04-root-INFO: grad norm: 3.974 3.947 0.466
2024-12-02-13:13:04-root-INFO: grad norm: 2.986 2.960 0.387
2024-12-02-13:13:05-root-INFO: grad norm: 3.148 3.124 0.387
2024-12-02-13:13:05-root-INFO: grad norm: 3.435 3.413 0.388
2024-12-02-13:13:06-root-INFO: grad norm: 3.475 3.450 0.416
2024-12-02-13:13:06-root-INFO: grad norm: 3.534 3.512 0.392
2024-12-02-13:13:07-root-INFO: grad norm: 3.530 3.504 0.422
2024-12-02-13:13:07-root-INFO: Loss Change: 82.081 -> 80.344
2024-12-02-13:13:07-root-INFO: Regularization Change: 0.000 -> 0.932
2024-12-02-13:13:07-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-13:13:07-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-13:13:07-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-13:13:07-root-INFO: grad norm: 4.543 4.500 0.623
2024-12-02-13:13:07-root-INFO: Loss too large (80.439->80.945)! Learning rate decreased to 0.10310.
2024-12-02-13:13:07-root-INFO: Loss too large (80.439->80.586)! Learning rate decreased to 0.08248.
2024-12-02-13:13:08-root-INFO: grad norm: 3.905 3.880 0.447
2024-12-02-13:13:08-root-INFO: grad norm: 3.357 3.334 0.394
2024-12-02-13:13:09-root-INFO: grad norm: 3.464 3.440 0.407
2024-12-02-13:13:09-root-INFO: grad norm: 3.635 3.613 0.397
2024-12-02-13:13:10-root-INFO: grad norm: 3.620 3.595 0.422
2024-12-02-13:13:10-root-INFO: grad norm: 3.593 3.572 0.392
2024-12-02-13:13:11-root-INFO: grad norm: 3.592 3.568 0.420
2024-12-02-13:13:11-root-INFO: Loss Change: 80.439 -> 78.893
2024-12-02-13:13:11-root-INFO: Regularization Change: 0.000 -> 0.915
2024-12-02-13:13:11-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-13:13:11-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-13:13:11-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-13:13:12-root-INFO: grad norm: 4.673 4.622 0.689
2024-12-02-13:13:12-root-INFO: Loss too large (79.167->79.645)! Learning rate decreased to 0.10543.
2024-12-02-13:13:12-root-INFO: Loss too large (79.167->79.270)! Learning rate decreased to 0.08434.
2024-12-02-13:13:12-root-INFO: grad norm: 3.888 3.863 0.445
2024-12-02-13:13:13-root-INFO: grad norm: 3.252 3.228 0.391
2024-12-02-13:13:13-root-INFO: grad norm: 3.354 3.330 0.395
2024-12-02-13:13:14-root-INFO: grad norm: 3.519 3.498 0.387
2024-12-02-13:13:14-root-INFO: grad norm: 3.521 3.498 0.410
2024-12-02-13:13:15-root-INFO: grad norm: 3.522 3.501 0.383
2024-12-02-13:13:15-root-INFO: grad norm: 3.519 3.495 0.410
2024-12-02-13:13:16-root-INFO: Loss Change: 79.167 -> 77.558
2024-12-02-13:13:16-root-INFO: Regularization Change: 0.000 -> 0.945
2024-12-02-13:13:16-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-13:13:16-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-13:13:16-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-13:13:16-root-INFO: grad norm: 5.005 4.948 0.753
2024-12-02-13:13:16-root-INFO: Loss too large (77.619->78.122)! Learning rate decreased to 0.10779.
2024-12-02-13:13:16-root-INFO: Loss too large (77.619->77.698)! Learning rate decreased to 0.08624.
2024-12-02-13:13:17-root-INFO: grad norm: 3.970 3.943 0.461
2024-12-02-13:13:17-root-INFO: grad norm: 3.145 3.121 0.386
2024-12-02-13:13:18-root-INFO: grad norm: 3.275 3.252 0.388
2024-12-02-13:13:18-root-INFO: grad norm: 3.485 3.464 0.379
2024-12-02-13:13:19-root-INFO: grad norm: 3.505 3.481 0.405
2024-12-02-13:13:19-root-INFO: grad norm: 3.530 3.510 0.377
2024-12-02-13:13:20-root-INFO: grad norm: 3.527 3.503 0.407
2024-12-02-13:13:20-root-INFO: Loss Change: 77.619 -> 75.895
2024-12-02-13:13:20-root-INFO: Regularization Change: 0.000 -> 0.989
2024-12-02-13:13:20-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-13:13:20-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-13:13:20-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-13:13:20-root-INFO: grad norm: 4.428 4.392 0.557
2024-12-02-13:13:21-root-INFO: Loss too large (76.073->76.599)! Learning rate decreased to 0.11019.
2024-12-02-13:13:21-root-INFO: Loss too large (76.073->76.223)! Learning rate decreased to 0.08816.
2024-12-02-13:13:21-root-INFO: grad norm: 3.820 3.796 0.431
2024-12-02-13:13:22-root-INFO: grad norm: 3.263 3.242 0.369
2024-12-02-13:13:22-root-INFO: grad norm: 3.316 3.293 0.385
2024-12-02-13:13:23-root-INFO: grad norm: 3.395 3.375 0.363
2024-12-02-13:13:23-root-INFO: grad norm: 3.402 3.380 0.391
2024-12-02-13:13:24-root-INFO: grad norm: 3.410 3.391 0.361
2024-12-02-13:13:24-root-INFO: grad norm: 3.409 3.386 0.391
2024-12-02-13:13:24-root-INFO: Loss Change: 76.073 -> 74.541
2024-12-02-13:13:24-root-INFO: Regularization Change: 0.000 -> 0.959
2024-12-02-13:13:24-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-13:13:24-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-13:13:25-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-13:13:25-root-INFO: grad norm: 4.519 4.471 0.651
2024-12-02-13:13:25-root-INFO: Loss too large (74.756->75.260)! Learning rate decreased to 0.11263.
2024-12-02-13:13:25-root-INFO: Loss too large (74.756->74.863)! Learning rate decreased to 0.09010.
2024-12-02-13:13:26-root-INFO: grad norm: 3.857 3.833 0.427
2024-12-02-13:13:26-root-INFO: grad norm: 3.393 3.371 0.383
2024-12-02-13:13:26-root-INFO: grad norm: 3.451 3.429 0.389
2024-12-02-13:13:27-root-INFO: grad norm: 3.536 3.517 0.370
2024-12-02-13:13:27-root-INFO: grad norm: 3.532 3.510 0.395
2024-12-02-13:13:28-root-INFO: grad norm: 3.523 3.504 0.364
2024-12-02-13:13:28-root-INFO: grad norm: 3.520 3.497 0.394
2024-12-02-13:13:29-root-INFO: Loss Change: 74.756 -> 73.179
2024-12-02-13:13:29-root-INFO: Regularization Change: 0.000 -> 0.995
2024-12-02-13:13:29-root-INFO: Undo step: 60
2024-12-02-13:13:29-root-INFO: Undo step: 61
2024-12-02-13:13:29-root-INFO: Undo step: 62
2024-12-02-13:13:29-root-INFO: Undo step: 63
2024-12-02-13:13:29-root-INFO: Undo step: 64
2024-12-02-13:13:29-root-INFO: Undo step: 65
2024-12-02-13:13:29-root-INFO: Undo step: 66
2024-12-02-13:13:29-root-INFO: Undo step: 67
2024-12-02-13:13:29-root-INFO: Undo step: 68
2024-12-02-13:13:29-root-INFO: Undo step: 69
2024-12-02-13:13:29-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-13:13:29-root-INFO: grad norm: 43.534 43.064 6.376
2024-12-02-13:13:29-root-INFO: grad norm: 23.243 22.887 4.053
2024-12-02-13:13:30-root-INFO: grad norm: 16.000 15.765 2.728
2024-12-02-13:13:30-root-INFO: grad norm: 11.916 11.745 2.008
2024-12-02-13:13:31-root-INFO: grad norm: 9.988 9.844 1.687
2024-12-02-13:13:31-root-INFO: grad norm: 8.818 8.702 1.426
2024-12-02-13:13:32-root-INFO: grad norm: 8.638 8.539 1.308
2024-12-02-13:13:32-root-INFO: grad norm: 7.549 7.460 1.155
2024-12-02-13:13:33-root-INFO: Loss Change: 336.207 -> 114.326
2024-12-02-13:13:33-root-INFO: Regularization Change: 0.000 -> 118.222
2024-12-02-13:13:33-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-13:13:33-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-13:13:33-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-13:13:33-root-INFO: grad norm: 6.524 6.427 1.120
2024-12-02-13:13:33-root-INFO: grad norm: 6.201 6.124 0.976
2024-12-02-13:13:34-root-INFO: grad norm: 7.807 7.732 1.080
2024-12-02-13:13:34-root-INFO: grad norm: 5.965 5.894 0.915
2024-12-02-13:13:35-root-INFO: grad norm: 5.737 5.662 0.923
2024-12-02-13:13:35-root-INFO: grad norm: 7.067 7.007 0.920
2024-12-02-13:13:36-root-INFO: grad norm: 5.706 5.630 0.930
2024-12-02-13:13:36-root-INFO: grad norm: 5.407 5.338 0.863
2024-12-02-13:13:37-root-INFO: Loss Change: 114.355 -> 98.878
2024-12-02-13:13:37-root-INFO: Regularization Change: 0.000 -> 14.408
2024-12-02-13:13:37-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-13:13:37-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-13:13:37-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-13:13:37-root-INFO: grad norm: 8.753 8.649 1.344
2024-12-02-13:13:38-root-INFO: grad norm: 6.174 6.100 0.949
2024-12-02-13:13:38-root-INFO: grad norm: 6.033 5.961 0.928
2024-12-02-13:13:38-root-INFO: grad norm: 5.347 5.285 0.810
2024-12-02-13:13:39-root-INFO: grad norm: 5.313 5.243 0.861
2024-12-02-13:13:39-root-INFO: grad norm: 6.162 6.112 0.786
2024-12-02-13:13:40-root-INFO: grad norm: 5.968 5.907 0.853
2024-12-02-13:13:40-root-INFO: grad norm: 5.469 5.420 0.729
2024-12-02-13:13:41-root-INFO: Loss Change: 98.999 -> 91.229
2024-12-02-13:13:41-root-INFO: Regularization Change: 0.000 -> 7.310
2024-12-02-13:13:41-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-13:13:41-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-13:13:41-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-13:13:41-root-INFO: grad norm: 5.928 5.874 0.792
2024-12-02-13:13:41-root-INFO: grad norm: 7.374 7.330 0.802
2024-12-02-13:13:42-root-INFO: Loss too large (90.102->90.402)! Learning rate decreased to 0.09633.
2024-12-02-13:13:42-root-INFO: grad norm: 4.630 4.586 0.640
2024-12-02-13:13:43-root-INFO: grad norm: 3.303 3.267 0.484
2024-12-02-13:13:43-root-INFO: grad norm: 3.911 3.876 0.518
2024-12-02-13:13:43-root-INFO: grad norm: 4.215 4.180 0.548
2024-12-02-13:13:44-root-INFO: grad norm: 5.129 5.095 0.594
2024-12-02-13:13:44-root-INFO: Loss too large (87.005->87.054)! Learning rate decreased to 0.07706.
2024-12-02-13:13:44-root-INFO: grad norm: 4.190 4.155 0.541
2024-12-02-13:13:45-root-INFO: Loss Change: 91.052 -> 86.040
2024-12-02-13:13:45-root-INFO: Regularization Change: 0.000 -> 3.133
2024-12-02-13:13:45-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-13:13:45-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-13:13:45-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-13:13:45-root-INFO: grad norm: 4.101 4.051 0.642
2024-12-02-13:13:45-root-INFO: Loss too large (86.183->86.186)! Learning rate decreased to 0.09855.
2024-12-02-13:13:46-root-INFO: grad norm: 4.276 4.240 0.553
2024-12-02-13:13:46-root-INFO: grad norm: 5.293 5.258 0.608
2024-12-02-13:13:46-root-INFO: Loss too large (85.433->85.562)! Learning rate decreased to 0.07884.
2024-12-02-13:13:47-root-INFO: grad norm: 4.211 4.177 0.532
2024-12-02-13:13:47-root-INFO: grad norm: 2.732 2.702 0.403
2024-12-02-13:13:48-root-INFO: grad norm: 2.930 2.898 0.427
2024-12-02-13:13:48-root-INFO: grad norm: 3.383 3.356 0.426
2024-12-02-13:13:49-root-INFO: grad norm: 3.615 3.584 0.472
2024-12-02-13:13:49-root-INFO: Loss Change: 86.183 -> 83.399
2024-12-02-13:13:49-root-INFO: Regularization Change: 0.000 -> 1.843
2024-12-02-13:13:49-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-13:13:49-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-13:13:49-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-13:13:49-root-INFO: grad norm: 5.779 5.714 0.867
2024-12-02-13:13:50-root-INFO: Loss too large (83.603->84.067)! Learning rate decreased to 0.10081.
2024-12-02-13:13:50-root-INFO: Loss too large (83.603->83.681)! Learning rate decreased to 0.08065.
2024-12-02-13:13:50-root-INFO: grad norm: 4.331 4.302 0.504
2024-12-02-13:13:51-root-INFO: grad norm: 2.706 2.677 0.394
2024-12-02-13:13:51-root-INFO: grad norm: 3.007 2.980 0.403
2024-12-02-13:13:52-root-INFO: grad norm: 3.694 3.668 0.432
2024-12-02-13:13:52-root-INFO: grad norm: 3.911 3.883 0.472
2024-12-02-13:13:53-root-INFO: grad norm: 4.292 4.266 0.475
2024-12-02-13:13:53-root-INFO: grad norm: 4.145 4.116 0.492
2024-12-02-13:13:53-root-INFO: Loss Change: 83.603 -> 80.978
2024-12-02-13:13:53-root-INFO: Regularization Change: 0.000 -> 1.476
2024-12-02-13:13:53-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-13:13:53-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-13:13:54-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-13:13:54-root-INFO: grad norm: 5.013 4.965 0.693
2024-12-02-13:13:54-root-INFO: Loss too large (81.033->81.571)! Learning rate decreased to 0.10310.
2024-12-02-13:13:54-root-INFO: Loss too large (81.033->81.211)! Learning rate decreased to 0.08248.
2024-12-02-13:13:55-root-INFO: grad norm: 4.260 4.233 0.483
2024-12-02-13:13:55-root-INFO: grad norm: 3.413 3.387 0.420
2024-12-02-13:13:56-root-INFO: grad norm: 3.723 3.697 0.440
2024-12-02-13:13:56-root-INFO: grad norm: 4.331 4.306 0.469
2024-12-02-13:13:56-root-INFO: grad norm: 4.152 4.125 0.477
2024-12-02-13:13:57-root-INFO: grad norm: 3.828 3.803 0.431
2024-12-02-13:13:57-root-INFO: grad norm: 3.951 3.925 0.460
2024-12-02-13:13:58-root-INFO: Loss Change: 81.033 -> 78.866
2024-12-02-13:13:58-root-INFO: Regularization Change: 0.000 -> 1.334
2024-12-02-13:13:58-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-13:13:58-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-13:13:58-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-13:13:58-root-INFO: grad norm: 5.292 5.238 0.759
2024-12-02-13:13:58-root-INFO: Loss too large (79.127->79.709)! Learning rate decreased to 0.10543.
2024-12-02-13:13:58-root-INFO: Loss too large (79.127->79.324)! Learning rate decreased to 0.08434.
2024-12-02-13:13:59-root-INFO: grad norm: 4.220 4.193 0.472
2024-12-02-13:13:59-root-INFO: grad norm: 2.951 2.925 0.388
2024-12-02-13:14:00-root-INFO: grad norm: 3.317 3.293 0.399
2024-12-02-13:14:00-root-INFO: grad norm: 4.056 4.032 0.441
2024-12-02-13:14:01-root-INFO: grad norm: 4.026 4.000 0.458
2024-12-02-13:14:01-root-INFO: grad norm: 3.952 3.928 0.434
2024-12-02-13:14:02-root-INFO: grad norm: 3.961 3.934 0.454
2024-12-02-13:14:02-root-INFO: Loss Change: 79.127 -> 76.999
2024-12-02-13:14:02-root-INFO: Regularization Change: 0.000 -> 1.277
2024-12-02-13:14:02-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-13:14:02-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-13:14:02-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-13:14:02-root-INFO: grad norm: 5.597 5.536 0.826
2024-12-02-13:14:02-root-INFO: Loss too large (77.029->77.630)! Learning rate decreased to 0.10779.
2024-12-02-13:14:02-root-INFO: Loss too large (77.029->77.202)! Learning rate decreased to 0.08624.
2024-12-02-13:14:03-root-INFO: grad norm: 4.193 4.167 0.472
2024-12-02-13:14:03-root-INFO: grad norm: 2.645 2.620 0.364
2024-12-02-13:14:04-root-INFO: grad norm: 3.002 2.979 0.368
2024-12-02-13:14:04-root-INFO: grad norm: 3.755 3.733 0.407
2024-12-02-13:14:05-root-INFO: grad norm: 3.900 3.876 0.437
2024-12-02-13:14:05-root-INFO: grad norm: 4.116 4.092 0.436
2024-12-02-13:14:05-root-INFO: Loss too large (75.315->75.319)! Learning rate decreased to 0.06899.
2024-12-02-13:14:06-root-INFO: grad norm: 3.417 3.394 0.396
2024-12-02-13:14:06-root-INFO: Loss Change: 77.029 -> 74.824
2024-12-02-13:14:06-root-INFO: Regularization Change: 0.000 -> 1.147
2024-12-02-13:14:06-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-13:14:06-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-13:14:06-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-13:14:07-root-INFO: grad norm: 3.679 3.642 0.522
2024-12-02-13:14:07-root-INFO: Loss too large (74.937->75.298)! Learning rate decreased to 0.11019.
2024-12-02-13:14:07-root-INFO: Loss too large (74.937->75.007)! Learning rate decreased to 0.08816.
2024-12-02-13:14:07-root-INFO: grad norm: 3.760 3.736 0.423
2024-12-02-13:14:08-root-INFO: grad norm: 4.207 4.182 0.451
2024-12-02-13:14:08-root-INFO: Loss too large (74.532->74.533)! Learning rate decreased to 0.07052.
2024-12-02-13:14:08-root-INFO: grad norm: 3.444 3.421 0.391
2024-12-02-13:14:09-root-INFO: grad norm: 2.717 2.698 0.325
2024-12-02-13:14:09-root-INFO: grad norm: 2.559 2.539 0.325
2024-12-02-13:14:10-root-INFO: grad norm: 2.413 2.394 0.299
2024-12-02-13:14:10-root-INFO: grad norm: 2.345 2.325 0.309
2024-12-02-13:14:11-root-INFO: Loss Change: 74.937 -> 73.333
2024-12-02-13:14:11-root-INFO: Regularization Change: 0.000 -> 0.867
2024-12-02-13:14:11-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-13:14:11-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-13:14:11-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-13:14:11-root-INFO: grad norm: 3.556 3.506 0.596
2024-12-02-13:14:11-root-INFO: Loss too large (73.482->73.739)! Learning rate decreased to 0.11263.
2024-12-02-13:14:11-root-INFO: grad norm: 4.221 4.196 0.462
2024-12-02-13:14:12-root-INFO: grad norm: 6.298 6.265 0.641
2024-12-02-13:14:12-root-INFO: Loss too large (73.397->73.823)! Learning rate decreased to 0.09010.
2024-12-02-13:14:12-root-INFO: Loss too large (73.397->73.409)! Learning rate decreased to 0.07208.
2024-12-02-13:14:13-root-INFO: grad norm: 4.030 4.007 0.432
2024-12-02-13:14:13-root-INFO: grad norm: 2.110 2.091 0.285
2024-12-02-13:14:14-root-INFO: grad norm: 2.068 2.049 0.284
2024-12-02-13:14:14-root-INFO: grad norm: 2.057 2.039 0.271
2024-12-02-13:14:15-root-INFO: grad norm: 2.065 2.046 0.283
2024-12-02-13:14:15-root-INFO: Loss Change: 73.482 -> 71.779
2024-12-02-13:14:15-root-INFO: Regularization Change: 0.000 -> 1.004
2024-12-02-13:14:15-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-13:14:15-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-13:14:15-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-13:14:15-root-INFO: grad norm: 3.489 3.441 0.574
2024-12-02-13:14:15-root-INFO: Loss too large (71.894->72.184)! Learning rate decreased to 0.11510.
2024-12-02-13:14:15-root-INFO: Loss too large (71.894->71.913)! Learning rate decreased to 0.09208.
2024-12-02-13:14:16-root-INFO: grad norm: 3.737 3.716 0.389
2024-12-02-13:14:16-root-INFO: grad norm: 4.659 4.634 0.480
2024-12-02-13:14:17-root-INFO: Loss too large (71.576->71.663)! Learning rate decreased to 0.07366.
2024-12-02-13:14:17-root-INFO: grad norm: 3.797 3.776 0.394
2024-12-02-13:14:17-root-INFO: grad norm: 2.970 2.951 0.335
2024-12-02-13:14:18-root-INFO: grad norm: 2.872 2.854 0.328
2024-12-02-13:14:18-root-INFO: grad norm: 2.785 2.767 0.313
2024-12-02-13:14:19-root-INFO: grad norm: 2.756 2.737 0.320
2024-12-02-13:14:19-root-INFO: Loss Change: 71.894 -> 70.385
2024-12-02-13:14:19-root-INFO: Regularization Change: 0.000 -> 0.872
2024-12-02-13:14:19-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-13:14:19-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-13:14:19-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-13:14:20-root-INFO: grad norm: 3.925 3.882 0.579
2024-12-02-13:14:20-root-INFO: Loss too large (70.404->70.927)! Learning rate decreased to 0.11760.
2024-12-02-13:14:20-root-INFO: Loss too large (70.404->70.585)! Learning rate decreased to 0.09408.
2024-12-02-13:14:20-root-INFO: grad norm: 4.074 4.053 0.418
2024-12-02-13:14:21-root-INFO: grad norm: 4.597 4.572 0.475
2024-12-02-13:14:21-root-INFO: Loss too large (70.048->70.120)! Learning rate decreased to 0.07527.
2024-12-02-13:14:22-root-INFO: grad norm: 3.745 3.725 0.389
2024-12-02-13:14:22-root-INFO: grad norm: 2.977 2.959 0.330
2024-12-02-13:14:23-root-INFO: grad norm: 2.865 2.847 0.324
2024-12-02-13:14:23-root-INFO: grad norm: 2.765 2.748 0.306
2024-12-02-13:14:24-root-INFO: grad norm: 2.728 2.710 0.314
2024-12-02-13:14:24-root-INFO: Loss Change: 70.404 -> 68.862
2024-12-02-13:14:24-root-INFO: Regularization Change: 0.000 -> 0.877
2024-12-02-13:14:24-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-13:14:24-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-13:14:24-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-13:14:24-root-INFO: grad norm: 4.168 4.124 0.604
2024-12-02-13:14:24-root-INFO: Loss too large (68.889->69.465)! Learning rate decreased to 0.12015.
2024-12-02-13:14:25-root-INFO: Loss too large (68.889->69.081)! Learning rate decreased to 0.09612.
2024-12-02-13:14:25-root-INFO: grad norm: 4.110 4.088 0.421
2024-12-02-13:14:25-root-INFO: grad norm: 4.347 4.323 0.450
2024-12-02-13:14:26-root-INFO: Loss too large (68.452->68.478)! Learning rate decreased to 0.07689.
2024-12-02-13:14:26-root-INFO: grad norm: 3.557 3.537 0.373
2024-12-02-13:14:27-root-INFO: grad norm: 2.919 2.902 0.318
2024-12-02-13:14:27-root-INFO: grad norm: 2.761 2.743 0.314
2024-12-02-13:14:27-root-INFO: grad norm: 2.626 2.610 0.291
2024-12-02-13:14:28-root-INFO: grad norm: 2.563 2.545 0.300
2024-12-02-13:14:28-root-INFO: Loss Change: 68.889 -> 67.282
2024-12-02-13:14:28-root-INFO: Regularization Change: 0.000 -> 0.891
2024-12-02-13:14:28-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-13:14:28-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-13:14:28-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-13:14:29-root-INFO: grad norm: 3.436 3.394 0.534
2024-12-02-13:14:29-root-INFO: Loss too large (67.406->67.766)! Learning rate decreased to 0.12272.
2024-12-02-13:14:29-root-INFO: Loss too large (67.406->67.465)! Learning rate decreased to 0.09818.
2024-12-02-13:14:29-root-INFO: grad norm: 3.737 3.718 0.381
2024-12-02-13:14:30-root-INFO: grad norm: 4.461 4.439 0.447
2024-12-02-13:14:30-root-INFO: Loss too large (67.121->67.146)! Learning rate decreased to 0.07854.
2024-12-02-13:14:30-root-INFO: grad norm: 3.610 3.591 0.370
2024-12-02-13:14:31-root-INFO: grad norm: 2.958 2.941 0.315
2024-12-02-13:14:31-root-INFO: grad norm: 2.786 2.769 0.311
2024-12-02-13:14:32-root-INFO: grad norm: 2.640 2.625 0.286
2024-12-02-13:14:32-root-INFO: grad norm: 2.566 2.549 0.296
2024-12-02-13:14:33-root-INFO: Loss Change: 67.406 -> 65.926
2024-12-02-13:14:33-root-INFO: Regularization Change: 0.000 -> 0.898
2024-12-02-13:14:33-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-13:14:33-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-13:14:33-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-13:14:33-root-INFO: grad norm: 3.684 3.641 0.559
2024-12-02-13:14:33-root-INFO: Loss too large (66.042->66.522)! Learning rate decreased to 0.12533.
2024-12-02-13:14:33-root-INFO: Loss too large (66.042->66.179)! Learning rate decreased to 0.10027.
2024-12-02-13:14:34-root-INFO: grad norm: 3.925 3.906 0.391
2024-12-02-13:14:34-root-INFO: grad norm: 4.435 4.412 0.444
2024-12-02-13:14:34-root-INFO: Loss too large (65.737->65.738)! Learning rate decreased to 0.08021.
2024-12-02-13:14:35-root-INFO: grad norm: 3.554 3.535 0.362
2024-12-02-13:14:35-root-INFO: grad norm: 2.939 2.922 0.310
2024-12-02-13:14:36-root-INFO: grad norm: 2.746 2.729 0.303
2024-12-02-13:14:36-root-INFO: grad norm: 2.591 2.575 0.280
2024-12-02-13:14:37-root-INFO: grad norm: 2.504 2.488 0.287
2024-12-02-13:14:37-root-INFO: Loss Change: 66.042 -> 64.530
2024-12-02-13:14:37-root-INFO: Regularization Change: 0.000 -> 0.908
2024-12-02-13:14:37-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-13:14:37-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-13:14:37-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-13:14:37-root-INFO: grad norm: 3.383 3.342 0.529
2024-12-02-13:14:37-root-INFO: Loss too large (64.531->64.894)! Learning rate decreased to 0.12798.
2024-12-02-13:14:38-root-INFO: Loss too large (64.531->64.587)! Learning rate decreased to 0.10238.
2024-12-02-13:14:38-root-INFO: grad norm: 3.694 3.676 0.362
2024-12-02-13:14:39-root-INFO: grad norm: 4.280 4.258 0.434
2024-12-02-13:14:39-root-INFO: grad norm: 4.125 4.106 0.403
2024-12-02-13:14:39-root-INFO: grad norm: 3.903 3.883 0.396
2024-12-02-13:14:40-root-INFO: grad norm: 3.941 3.921 0.393
2024-12-02-13:14:40-root-INFO: grad norm: 3.925 3.905 0.390
2024-12-02-13:14:41-root-INFO: grad norm: 3.894 3.874 0.392
2024-12-02-13:14:41-root-INFO: Loss Change: 64.531 -> 63.068
2024-12-02-13:14:41-root-INFO: Regularization Change: 0.000 -> 1.279
2024-12-02-13:14:41-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-13:14:41-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-13:14:41-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-13:14:42-root-INFO: grad norm: 4.725 4.686 0.613
2024-12-02-13:14:42-root-INFO: Loss too large (63.302->63.929)! Learning rate decreased to 0.13066.
2024-12-02-13:14:42-root-INFO: Loss too large (63.302->63.446)! Learning rate decreased to 0.10453.
2024-12-02-13:14:42-root-INFO: grad norm: 4.035 4.014 0.411
2024-12-02-13:14:43-root-INFO: grad norm: 3.562 3.542 0.377
2024-12-02-13:14:43-root-INFO: grad norm: 3.606 3.586 0.376
2024-12-02-13:14:44-root-INFO: grad norm: 3.627 3.608 0.363
2024-12-02-13:14:44-root-INFO: grad norm: 3.618 3.599 0.376
2024-12-02-13:14:45-root-INFO: grad norm: 3.589 3.571 0.356
2024-12-02-13:14:45-root-INFO: grad norm: 3.571 3.551 0.372
2024-12-02-13:14:46-root-INFO: Loss Change: 63.302 -> 61.523
2024-12-02-13:14:46-root-INFO: Regularization Change: 0.000 -> 1.264
2024-12-02-13:14:46-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-13:14:46-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-13:14:46-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-13:14:46-root-INFO: grad norm: 4.286 4.247 0.575
2024-12-02-13:14:46-root-INFO: Loss too large (61.630->62.151)! Learning rate decreased to 0.13338.
2024-12-02-13:14:46-root-INFO: Loss too large (61.630->61.689)! Learning rate decreased to 0.10670.
2024-12-02-13:14:47-root-INFO: grad norm: 3.783 3.762 0.394
2024-12-02-13:14:47-root-INFO: grad norm: 3.489 3.470 0.363
2024-12-02-13:14:48-root-INFO: grad norm: 3.432 3.412 0.364
2024-12-02-13:14:48-root-INFO: grad norm: 3.375 3.358 0.337
2024-12-02-13:14:48-root-INFO: grad norm: 3.351 3.332 0.355
2024-12-02-13:14:49-root-INFO: grad norm: 3.321 3.305 0.328
2024-12-02-13:14:49-root-INFO: grad norm: 3.305 3.287 0.350
2024-12-02-13:14:50-root-INFO: Loss Change: 61.630 -> 59.910
2024-12-02-13:14:50-root-INFO: Regularization Change: 0.000 -> 1.278
2024-12-02-13:14:50-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-13:14:50-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-13:14:50-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-13:14:50-root-INFO: grad norm: 4.196 4.150 0.619
2024-12-02-13:14:50-root-INFO: Loss too large (60.049->60.513)! Learning rate decreased to 0.13613.
2024-12-02-13:14:51-root-INFO: grad norm: 4.311 4.286 0.467
2024-12-02-13:14:51-root-INFO: grad norm: 4.541 4.511 0.515
2024-12-02-13:14:51-root-INFO: Loss too large (59.642->59.707)! Learning rate decreased to 0.10890.
2024-12-02-13:14:52-root-INFO: grad norm: 3.746 3.722 0.421
2024-12-02-13:14:52-root-INFO: grad norm: 3.223 3.204 0.347
2024-12-02-13:14:53-root-INFO: grad norm: 3.049 3.031 0.334
2024-12-02-13:14:53-root-INFO: grad norm: 2.930 2.914 0.308
2024-12-02-13:14:54-root-INFO: grad norm: 2.874 2.857 0.312
2024-12-02-13:14:54-root-INFO: Loss Change: 60.049 -> 58.167
2024-12-02-13:14:54-root-INFO: Regularization Change: 0.000 -> 1.472
2024-12-02-13:14:54-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-13:14:54-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-13:14:54-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-13:14:54-root-INFO: grad norm: 3.661 3.621 0.535
2024-12-02-13:14:54-root-INFO: Loss too large (58.057->58.370)! Learning rate decreased to 0.13891.
2024-12-02-13:14:55-root-INFO: grad norm: 4.071 4.047 0.436
2024-12-02-13:14:55-root-INFO: grad norm: 4.505 4.478 0.491
2024-12-02-13:14:56-root-INFO: grad norm: 4.408 4.378 0.508
2024-12-02-13:14:56-root-INFO: grad norm: 4.324 4.296 0.492
2024-12-02-13:14:57-root-INFO: grad norm: 4.385 4.354 0.527
2024-12-02-13:14:57-root-INFO: grad norm: 4.349 4.321 0.494
2024-12-02-13:14:58-root-INFO: grad norm: 4.357 4.325 0.525
2024-12-02-13:14:58-root-INFO: Loss Change: 58.057 -> 56.471
2024-12-02-13:14:58-root-INFO: Regularization Change: 0.000 -> 2.069
2024-12-02-13:14:58-root-INFO: Undo step: 50
2024-12-02-13:14:58-root-INFO: Undo step: 51
2024-12-02-13:14:58-root-INFO: Undo step: 52
2024-12-02-13:14:58-root-INFO: Undo step: 53
2024-12-02-13:14:58-root-INFO: Undo step: 54
2024-12-02-13:14:58-root-INFO: Undo step: 55
2024-12-02-13:14:58-root-INFO: Undo step: 56
2024-12-02-13:14:58-root-INFO: Undo step: 57
2024-12-02-13:14:58-root-INFO: Undo step: 58
2024-12-02-13:14:58-root-INFO: Undo step: 59
2024-12-02-13:14:58-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-13:14:58-root-INFO: grad norm: 31.759 31.490 4.125
2024-12-02-13:14:59-root-INFO: grad norm: 17.374 17.149 2.792
2024-12-02-13:14:59-root-INFO: grad norm: 11.728 11.549 2.042
2024-12-02-13:15:00-root-INFO: grad norm: 9.077 8.915 1.706
2024-12-02-13:15:00-root-INFO: grad norm: 7.554 7.416 1.436
2024-12-02-13:15:01-root-INFO: grad norm: 7.008 6.888 1.292
2024-12-02-13:15:01-root-INFO: grad norm: 6.400 6.287 1.194
2024-12-02-13:15:02-root-INFO: grad norm: 6.031 5.935 1.070
2024-12-02-13:15:02-root-INFO: Loss Change: 264.552 -> 95.132
2024-12-02-13:15:02-root-INFO: Regularization Change: 0.000 -> 124.036
2024-12-02-13:15:02-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-13:15:02-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-13:15:02-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-13:15:02-root-INFO: grad norm: 5.771 5.683 1.004
2024-12-02-13:15:03-root-INFO: grad norm: 5.692 5.616 0.926
2024-12-02-13:15:03-root-INFO: grad norm: 5.191 5.107 0.931
2024-12-02-13:15:04-root-INFO: grad norm: 4.487 4.417 0.787
2024-12-02-13:15:04-root-INFO: grad norm: 4.719 4.643 0.842
2024-12-02-13:15:04-root-INFO: grad norm: 5.571 5.511 0.814
2024-12-02-13:15:05-root-INFO: grad norm: 4.627 4.551 0.832
2024-12-02-13:15:05-root-INFO: grad norm: 3.391 3.333 0.627
2024-12-02-13:15:06-root-INFO: Loss Change: 94.776 -> 79.454
2024-12-02-13:15:06-root-INFO: Regularization Change: 0.000 -> 16.853
2024-12-02-13:15:06-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-13:15:06-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-13:15:06-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-13:15:06-root-INFO: grad norm: 3.371 3.310 0.638
2024-12-02-13:15:06-root-INFO: grad norm: 3.371 3.325 0.557
2024-12-02-13:15:07-root-INFO: grad norm: 3.863 3.808 0.650
2024-12-02-13:15:07-root-INFO: grad norm: 5.221 5.174 0.698
2024-12-02-13:15:08-root-INFO: grad norm: 4.412 4.346 0.760
2024-12-02-13:15:08-root-INFO: grad norm: 3.257 3.209 0.557
2024-12-02-13:15:09-root-INFO: grad norm: 3.447 3.387 0.642
2024-12-02-13:15:09-root-INFO: grad norm: 4.439 4.394 0.629
2024-12-02-13:15:10-root-INFO: Loss Change: 79.133 -> 72.824
2024-12-02-13:15:10-root-INFO: Regularization Change: 0.000 -> 8.422
2024-12-02-13:15:10-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-13:15:10-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-13:15:10-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-13:15:10-root-INFO: grad norm: 4.364 4.310 0.685
2024-12-02-13:15:10-root-INFO: grad norm: 4.436 4.394 0.613
2024-12-02-13:15:11-root-INFO: grad norm: 4.405 4.343 0.736
2024-12-02-13:15:11-root-INFO: grad norm: 4.323 4.279 0.612
2024-12-02-13:15:12-root-INFO: grad norm: 4.360 4.298 0.733
2024-12-02-13:15:12-root-INFO: grad norm: 4.398 4.354 0.624
2024-12-02-13:15:13-root-INFO: grad norm: 4.462 4.400 0.743
2024-12-02-13:15:13-root-INFO: grad norm: 4.509 4.464 0.635
2024-12-02-13:15:13-root-INFO: Loss Change: 72.333 -> 67.924
2024-12-02-13:15:13-root-INFO: Regularization Change: 0.000 -> 5.533
2024-12-02-13:15:13-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-13:15:13-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-13:15:14-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-13:15:14-root-INFO: grad norm: 4.545 4.497 0.662
2024-12-02-13:15:14-root-INFO: grad norm: 4.664 4.623 0.615
2024-12-02-13:15:15-root-INFO: grad norm: 4.745 4.689 0.725
2024-12-02-13:15:15-root-INFO: grad norm: 4.747 4.704 0.640
2024-12-02-13:15:16-root-INFO: grad norm: 4.740 4.680 0.747
2024-12-02-13:15:16-root-INFO: grad norm: 4.720 4.675 0.646
2024-12-02-13:15:17-root-INFO: grad norm: 4.655 4.595 0.745
2024-12-02-13:15:17-root-INFO: grad norm: 4.577 4.532 0.637
2024-12-02-13:15:17-root-INFO: Loss Change: 67.759 -> 64.369
2024-12-02-13:15:17-root-INFO: Regularization Change: 0.000 -> 4.283
2024-12-02-13:15:17-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-13:15:17-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-13:15:18-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-13:15:18-root-INFO: grad norm: 4.353 4.306 0.641
2024-12-02-13:15:18-root-INFO: grad norm: 4.470 4.430 0.598
2024-12-02-13:15:19-root-INFO: grad norm: 4.474 4.419 0.696
2024-12-02-13:15:19-root-INFO: grad norm: 4.478 4.435 0.618
2024-12-02-13:15:20-root-INFO: grad norm: 4.500 4.443 0.714
2024-12-02-13:15:20-root-INFO: grad norm: 4.493 4.450 0.624
2024-12-02-13:15:21-root-INFO: grad norm: 4.496 4.439 0.714
2024-12-02-13:15:21-root-INFO: grad norm: 4.475 4.431 0.626
2024-12-02-13:15:21-root-INFO: Loss Change: 64.089 -> 61.353
2024-12-02-13:15:21-root-INFO: Regularization Change: 0.000 -> 3.586
2024-12-02-13:15:21-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-13:15:21-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-13:15:22-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-13:15:22-root-INFO: grad norm: 4.315 4.271 0.618
2024-12-02-13:15:22-root-INFO: grad norm: 4.333 4.292 0.590
2024-12-02-13:15:23-root-INFO: grad norm: 4.357 4.307 0.663
2024-12-02-13:15:23-root-INFO: grad norm: 4.388 4.346 0.607
2024-12-02-13:15:24-root-INFO: grad norm: 4.397 4.344 0.683
2024-12-02-13:15:24-root-INFO: grad norm: 4.382 4.339 0.613
2024-12-02-13:15:24-root-INFO: grad norm: 4.392 4.338 0.686
2024-12-02-13:15:25-root-INFO: grad norm: 4.378 4.335 0.614
2024-12-02-13:15:25-root-INFO: Loss Change: 61.031 -> 58.592
2024-12-02-13:15:25-root-INFO: Regularization Change: 0.000 -> 3.198
2024-12-02-13:15:25-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-13:15:25-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-13:15:25-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-13:15:26-root-INFO: grad norm: 4.144 4.102 0.589
2024-12-02-13:15:26-root-INFO: grad norm: 4.219 4.181 0.563
2024-12-02-13:15:27-root-INFO: grad norm: 4.264 4.214 0.649
2024-12-02-13:15:27-root-INFO: grad norm: 4.300 4.261 0.583
2024-12-02-13:15:27-root-INFO: grad norm: 4.311 4.260 0.666
2024-12-02-13:15:28-root-INFO: grad norm: 4.305 4.264 0.593
2024-12-02-13:15:28-root-INFO: grad norm: 4.305 4.252 0.670
2024-12-02-13:15:29-root-INFO: grad norm: 4.280 4.239 0.593
2024-12-02-13:15:29-root-INFO: Loss Change: 58.405 -> 56.272
2024-12-02-13:15:29-root-INFO: Regularization Change: 0.000 -> 2.908
2024-12-02-13:15:29-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-13:15:29-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-13:15:29-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-13:15:29-root-INFO: grad norm: 3.950 3.911 0.557
2024-12-02-13:15:30-root-INFO: grad norm: 3.923 3.888 0.523
2024-12-02-13:15:30-root-INFO: grad norm: 3.988 3.943 0.597
2024-12-02-13:15:31-root-INFO: grad norm: 4.072 4.035 0.550
2024-12-02-13:15:31-root-INFO: grad norm: 4.104 4.056 0.625
2024-12-02-13:15:32-root-INFO: grad norm: 4.114 4.076 0.560
2024-12-02-13:15:32-root-INFO: grad norm: 4.128 4.080 0.629
2024-12-02-13:15:33-root-INFO: grad norm: 4.123 4.084 0.564
2024-12-02-13:15:33-root-INFO: Loss Change: 56.002 -> 54.030
2024-12-02-13:15:33-root-INFO: Regularization Change: 0.000 -> 2.734
2024-12-02-13:15:33-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-13:15:33-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-13:15:33-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-13:15:33-root-INFO: grad norm: 3.846 3.811 0.519
2024-12-02-13:15:34-root-INFO: grad norm: 3.863 3.828 0.518
2024-12-02-13:15:34-root-INFO: grad norm: 3.939 3.898 0.565
2024-12-02-13:15:35-root-INFO: grad norm: 4.028 3.991 0.545
2024-12-02-13:15:35-root-INFO: grad norm: 4.073 4.029 0.601
2024-12-02-13:15:36-root-INFO: grad norm: 4.100 4.062 0.559
2024-12-02-13:15:36-root-INFO: grad norm: 4.115 4.069 0.613
2024-12-02-13:15:37-root-INFO: grad norm: 4.110 4.071 0.563
2024-12-02-13:15:37-root-INFO: Loss Change: 53.746 -> 51.949
2024-12-02-13:15:37-root-INFO: Regularization Change: 0.000 -> 2.625
2024-12-02-13:15:37-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-13:15:37-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-13:15:37-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-13:15:37-root-INFO: grad norm: 3.829 3.795 0.510
2024-12-02-13:15:38-root-INFO: grad norm: 3.805 3.771 0.504
2024-12-02-13:15:38-root-INFO: grad norm: 3.856 3.816 0.554
2024-12-02-13:15:39-root-INFO: grad norm: 3.933 3.898 0.529
2024-12-02-13:15:39-root-INFO: grad norm: 3.961 3.918 0.585
2024-12-02-13:15:40-root-INFO: grad norm: 3.969 3.933 0.538
2024-12-02-13:15:40-root-INFO: grad norm: 3.978 3.934 0.592
2024-12-02-13:15:41-root-INFO: grad norm: 3.975 3.938 0.542
2024-12-02-13:15:41-root-INFO: Loss Change: 51.463 -> 49.708
2024-12-02-13:15:41-root-INFO: Regularization Change: 0.000 -> 2.543
2024-12-02-13:15:41-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-13:15:41-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-13:15:41-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-13:15:41-root-INFO: grad norm: 3.596 3.563 0.485
2024-12-02-13:15:42-root-INFO: grad norm: 3.546 3.515 0.471
2024-12-02-13:15:42-root-INFO: grad norm: 3.595 3.558 0.512
2024-12-02-13:15:43-root-INFO: grad norm: 3.663 3.630 0.490
2024-12-02-13:15:43-root-INFO: grad norm: 3.731 3.691 0.542
2024-12-02-13:15:44-root-INFO: grad norm: 3.797 3.762 0.511
2024-12-02-13:15:44-root-INFO: grad norm: 3.831 3.790 0.562
2024-12-02-13:15:45-root-INFO: grad norm: 3.846 3.811 0.519
2024-12-02-13:15:45-root-INFO: Loss Change: 49.481 -> 47.823
2024-12-02-13:15:45-root-INFO: Regularization Change: 0.000 -> 2.498
2024-12-02-13:15:45-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-13:15:45-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-13:15:45-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-13:15:45-root-INFO: grad norm: 3.632 3.599 0.489
2024-12-02-13:15:46-root-INFO: grad norm: 3.554 3.525 0.458
2024-12-02-13:15:46-root-INFO: grad norm: 3.612 3.575 0.514
2024-12-02-13:15:47-root-INFO: grad norm: 3.707 3.675 0.485
2024-12-02-13:15:47-root-INFO: grad norm: 3.740 3.700 0.545
2024-12-02-13:15:48-root-INFO: grad norm: 3.753 3.720 0.495
2024-12-02-13:15:48-root-INFO: grad norm: 3.770 3.730 0.550
2024-12-02-13:15:49-root-INFO: grad norm: 3.778 3.745 0.501
2024-12-02-13:15:49-root-INFO: Loss Change: 47.596 -> 45.974
2024-12-02-13:15:49-root-INFO: Regularization Change: 0.000 -> 2.421
2024-12-02-13:15:49-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-13:15:49-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-13:15:49-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-13:15:49-root-INFO: grad norm: 3.412 3.382 0.454
2024-12-02-13:15:50-root-INFO: grad norm: 3.337 3.308 0.435
2024-12-02-13:15:50-root-INFO: grad norm: 3.395 3.363 0.469
2024-12-02-13:15:51-root-INFO: grad norm: 3.481 3.450 0.461
2024-12-02-13:15:51-root-INFO: grad norm: 3.550 3.514 0.505
2024-12-02-13:15:52-root-INFO: grad norm: 3.610 3.578 0.480
2024-12-02-13:15:52-root-INFO: grad norm: 3.655 3.617 0.525
2024-12-02-13:15:53-root-INFO: grad norm: 3.682 3.649 0.491
2024-12-02-13:15:53-root-INFO: Loss Change: 45.733 -> 44.208
2024-12-02-13:15:53-root-INFO: Regularization Change: 0.000 -> 2.408
2024-12-02-13:15:53-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-13:15:53-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-13:15:53-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-13:15:53-root-INFO: grad norm: 3.400 3.371 0.445
2024-12-02-13:15:54-root-INFO: grad norm: 3.356 3.328 0.431
2024-12-02-13:15:54-root-INFO: grad norm: 3.402 3.369 0.473
2024-12-02-13:15:55-root-INFO: grad norm: 3.465 3.435 0.451
2024-12-02-13:15:55-root-INFO: grad norm: 3.520 3.484 0.500
2024-12-02-13:15:56-root-INFO: grad norm: 3.564 3.534 0.467
2024-12-02-13:15:56-root-INFO: grad norm: 3.597 3.560 0.515
2024-12-02-13:15:57-root-INFO: grad norm: 3.613 3.581 0.475
2024-12-02-13:15:57-root-INFO: Loss Change: 43.970 -> 42.487
2024-12-02-13:15:57-root-INFO: Regularization Change: 0.000 -> 2.357
2024-12-02-13:15:57-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-13:15:57-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-13:15:57-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-13:15:57-root-INFO: grad norm: 3.308 3.278 0.439
2024-12-02-13:15:58-root-INFO: grad norm: 3.243 3.217 0.415
2024-12-02-13:15:58-root-INFO: grad norm: 3.278 3.246 0.454
2024-12-02-13:15:59-root-INFO: grad norm: 3.327 3.299 0.434
2024-12-02-13:15:59-root-INFO: grad norm: 3.377 3.343 0.478
2024-12-02-13:16:00-root-INFO: grad norm: 3.420 3.390 0.448
2024-12-02-13:16:00-root-INFO: grad norm: 3.454 3.418 0.493
2024-12-02-13:16:01-root-INFO: grad norm: 3.470 3.440 0.457
2024-12-02-13:16:01-root-INFO: Loss Change: 42.166 -> 40.709
2024-12-02-13:16:01-root-INFO: Regularization Change: 0.000 -> 2.333
2024-12-02-13:16:01-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-13:16:01-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-13:16:01-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-13:16:01-root-INFO: grad norm: 3.207 3.179 0.417
2024-12-02-13:16:02-root-INFO: grad norm: 3.181 3.155 0.407
2024-12-02-13:16:02-root-INFO: grad norm: 3.231 3.200 0.444
2024-12-02-13:16:03-root-INFO: grad norm: 3.284 3.256 0.426
2024-12-02-13:16:03-root-INFO: grad norm: 3.331 3.298 0.467
2024-12-02-13:16:03-root-INFO: grad norm: 3.365 3.337 0.438
2024-12-02-13:16:04-root-INFO: grad norm: 3.394 3.360 0.479
2024-12-02-13:16:04-root-INFO: grad norm: 3.405 3.376 0.444
2024-12-02-13:16:05-root-INFO: Loss Change: 40.512 -> 39.117
2024-12-02-13:16:05-root-INFO: Regularization Change: 0.000 -> 2.303
2024-12-02-13:16:05-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-13:16:05-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-13:16:05-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-13:16:05-root-INFO: grad norm: 3.111 3.086 0.397
2024-12-02-13:16:06-root-INFO: grad norm: 3.028 3.004 0.382
2024-12-02-13:16:06-root-INFO: grad norm: 3.047 3.018 0.414
2024-12-02-13:16:07-root-INFO: grad norm: 3.089 3.063 0.400
2024-12-02-13:16:07-root-INFO: grad norm: 3.127 3.096 0.437
2024-12-02-13:16:08-root-INFO: grad norm: 3.161 3.135 0.412
2024-12-02-13:16:08-root-INFO: grad norm: 3.193 3.161 0.450
2024-12-02-13:16:09-root-INFO: grad norm: 3.209 3.182 0.420
2024-12-02-13:16:09-root-INFO: Loss Change: 38.727 -> 37.335
2024-12-02-13:16:09-root-INFO: Regularization Change: 0.000 -> 2.281
2024-12-02-13:16:09-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-13:16:09-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-13:16:09-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-13:16:09-root-INFO: grad norm: 3.023 2.997 0.391
2024-12-02-13:16:10-root-INFO: grad norm: 2.970 2.946 0.373
2024-12-02-13:16:10-root-INFO: grad norm: 2.983 2.954 0.414
2024-12-02-13:16:11-root-INFO: grad norm: 3.001 2.977 0.384
2024-12-02-13:16:11-root-INFO: grad norm: 3.029 2.999 0.429
2024-12-02-13:16:12-root-INFO: grad norm: 3.051 3.026 0.392
2024-12-02-13:16:12-root-INFO: grad norm: 3.072 3.040 0.436
2024-12-02-13:16:12-root-INFO: grad norm: 3.077 3.052 0.397
2024-12-02-13:16:13-root-INFO: Loss Change: 37.118 -> 35.752
2024-12-02-13:16:13-root-INFO: Regularization Change: 0.000 -> 2.247
2024-12-02-13:16:13-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-13:16:13-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-13:16:13-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-13:16:13-root-INFO: grad norm: 2.834 2.804 0.408
2024-12-02-13:16:14-root-INFO: grad norm: 2.602 2.581 0.328
2024-12-02-13:16:14-root-INFO: grad norm: 2.633 2.609 0.350
2024-12-02-13:16:15-root-INFO: grad norm: 2.725 2.702 0.347
2024-12-02-13:16:15-root-INFO: grad norm: 2.781 2.754 0.384
2024-12-02-13:16:16-root-INFO: grad norm: 2.833 2.810 0.362
2024-12-02-13:16:16-root-INFO: grad norm: 2.884 2.856 0.401
2024-12-02-13:16:16-root-INFO: grad norm: 2.919 2.895 0.373
2024-12-02-13:16:17-root-INFO: Loss Change: 35.482 -> 34.108
2024-12-02-13:16:17-root-INFO: Regularization Change: 0.000 -> 2.322
2024-12-02-13:16:17-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-13:16:17-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-13:16:17-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-13:16:17-root-INFO: grad norm: 2.745 2.720 0.369
2024-12-02-13:16:18-root-INFO: grad norm: 2.621 2.601 0.325
2024-12-02-13:16:18-root-INFO: grad norm: 2.634 2.610 0.352
2024-12-02-13:16:19-root-INFO: grad norm: 2.687 2.665 0.341
2024-12-02-13:16:19-root-INFO: grad norm: 2.719 2.692 0.378
2024-12-02-13:16:20-root-INFO: grad norm: 2.741 2.718 0.351
2024-12-02-13:16:20-root-INFO: grad norm: 2.774 2.747 0.388
2024-12-02-13:16:20-root-INFO: grad norm: 2.795 2.772 0.359
2024-12-02-13:16:21-root-INFO: Loss Change: 33.894 -> 32.572
2024-12-02-13:16:21-root-INFO: Regularization Change: 0.000 -> 2.276
2024-12-02-13:16:21-root-INFO: Undo step: 40
2024-12-02-13:16:21-root-INFO: Undo step: 41
2024-12-02-13:16:21-root-INFO: Undo step: 42
2024-12-02-13:16:21-root-INFO: Undo step: 43
2024-12-02-13:16:21-root-INFO: Undo step: 44
2024-12-02-13:16:21-root-INFO: Undo step: 45
2024-12-02-13:16:21-root-INFO: Undo step: 46
2024-12-02-13:16:21-root-INFO: Undo step: 47
2024-12-02-13:16:21-root-INFO: Undo step: 48
2024-12-02-13:16:21-root-INFO: Undo step: 49
2024-12-02-13:16:21-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-13:16:21-root-INFO: grad norm: 27.018 26.839 3.111
2024-12-02-13:16:22-root-INFO: grad norm: 14.666 14.512 2.121
2024-12-02-13:16:22-root-INFO: grad norm: 10.272 10.161 1.507
2024-12-02-13:16:22-root-INFO: grad norm: 8.441 8.344 1.276
2024-12-02-13:16:23-root-INFO: grad norm: 8.148 8.071 1.119
2024-12-02-13:16:23-root-INFO: grad norm: 6.736 6.662 0.993
2024-12-02-13:16:24-root-INFO: grad norm: 6.060 5.987 0.941
2024-12-02-13:16:24-root-INFO: grad norm: 5.805 5.747 0.822
2024-12-02-13:16:25-root-INFO: Loss Change: 232.217 -> 71.892
2024-12-02-13:16:25-root-INFO: Regularization Change: 0.000 -> 144.741
2024-12-02-13:16:25-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-13:16:25-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-13:16:25-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-13:16:25-root-INFO: grad norm: 5.641 5.590 0.757
2024-12-02-13:16:25-root-INFO: grad norm: 6.458 6.415 0.740
2024-12-02-13:16:26-root-INFO: grad norm: 5.285 5.241 0.682
2024-12-02-13:16:26-root-INFO: grad norm: 4.156 4.113 0.594
2024-12-02-13:16:27-root-INFO: grad norm: 3.795 3.756 0.541
2024-12-02-13:16:27-root-INFO: grad norm: 3.609 3.571 0.518
2024-12-02-13:16:28-root-INFO: grad norm: 3.477 3.442 0.489
2024-12-02-13:16:28-root-INFO: grad norm: 3.463 3.428 0.496
2024-12-02-13:16:28-root-INFO: Loss Change: 71.499 -> 57.354
2024-12-02-13:16:28-root-INFO: Regularization Change: 0.000 -> 19.297
2024-12-02-13:16:28-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-13:16:29-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-13:16:29-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-13:16:29-root-INFO: grad norm: 3.657 3.623 0.503
2024-12-02-13:16:29-root-INFO: grad norm: 4.385 4.349 0.556
2024-12-02-13:16:30-root-INFO: grad norm: 5.006 4.964 0.647
2024-12-02-13:16:30-root-INFO: grad norm: 6.098 6.048 0.778
2024-12-02-13:16:30-root-INFO: Loss too large (55.019->55.115)! Learning rate decreased to 0.14458.
2024-12-02-13:16:31-root-INFO: grad norm: 4.749 4.703 0.665
2024-12-02-13:16:31-root-INFO: grad norm: 3.358 3.328 0.448
2024-12-02-13:16:32-root-INFO: grad norm: 3.476 3.450 0.431
2024-12-02-13:16:32-root-INFO: grad norm: 3.708 3.680 0.451
2024-12-02-13:16:33-root-INFO: Loss Change: 57.115 -> 51.326
2024-12-02-13:16:33-root-INFO: Regularization Change: 0.000 -> 7.626
2024-12-02-13:16:33-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-13:16:33-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-13:16:33-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-13:16:33-root-INFO: grad norm: 3.645 3.620 0.427
2024-12-02-13:16:34-root-INFO: grad norm: 4.702 4.674 0.512
2024-12-02-13:16:34-root-INFO: grad norm: 4.333 4.297 0.557
2024-12-02-13:16:35-root-INFO: grad norm: 4.206 4.162 0.605
2024-12-02-13:16:35-root-INFO: Loss too large (49.256->49.269)! Learning rate decreased to 0.14747.
2024-12-02-13:16:35-root-INFO: grad norm: 3.734 3.696 0.527
2024-12-02-13:16:36-root-INFO: grad norm: 3.284 3.260 0.401
2024-12-02-13:16:36-root-INFO: grad norm: 3.228 3.206 0.374
2024-12-02-13:16:37-root-INFO: grad norm: 3.197 3.173 0.390
2024-12-02-13:16:37-root-INFO: Loss Change: 51.092 -> 46.990
2024-12-02-13:16:37-root-INFO: Regularization Change: 0.000 -> 5.214
2024-12-02-13:16:37-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-13:16:37-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-13:16:37-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-13:16:37-root-INFO: grad norm: 3.001 2.980 0.362
2024-12-02-13:16:38-root-INFO: grad norm: 3.815 3.790 0.434
2024-12-02-13:16:38-root-INFO: grad norm: 4.280 4.248 0.528
2024-12-02-13:16:39-root-INFO: grad norm: 4.968 4.926 0.642
2024-12-02-13:16:39-root-INFO: Loss too large (45.939->46.131)! Learning rate decreased to 0.15039.
2024-12-02-13:16:39-root-INFO: grad norm: 4.006 3.968 0.549
2024-12-02-13:16:40-root-INFO: grad norm: 3.114 3.092 0.373
2024-12-02-13:16:40-root-INFO: grad norm: 2.881 2.861 0.336
2024-12-02-13:16:41-root-INFO: grad norm: 2.747 2.726 0.342
2024-12-02-13:16:41-root-INFO: Loss Change: 46.743 -> 43.696
2024-12-02-13:16:41-root-INFO: Regularization Change: 0.000 -> 3.966
2024-12-02-13:16:41-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-13:16:41-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-13:16:41-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-13:16:41-root-INFO: grad norm: 2.556 2.532 0.349
2024-12-02-13:16:42-root-INFO: grad norm: 3.011 2.991 0.348
2024-12-02-13:16:42-root-INFO: grad norm: 3.764 3.740 0.432
2024-12-02-13:16:43-root-INFO: grad norm: 4.799 4.762 0.601
2024-12-02-13:16:43-root-INFO: Loss too large (42.957->43.147)! Learning rate decreased to 0.15335.
2024-12-02-13:16:43-root-INFO: grad norm: 3.821 3.786 0.518
2024-12-02-13:16:44-root-INFO: grad norm: 2.958 2.935 0.363
2024-12-02-13:16:44-root-INFO: grad norm: 2.675 2.656 0.322
2024-12-02-13:16:45-root-INFO: grad norm: 2.518 2.496 0.327
2024-12-02-13:16:45-root-INFO: Loss Change: 43.413 -> 40.976
2024-12-02-13:16:45-root-INFO: Regularization Change: 0.000 -> 3.256
2024-12-02-13:16:45-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-13:16:45-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-13:16:45-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-13:16:45-root-INFO: grad norm: 2.303 2.281 0.322
2024-12-02-13:16:46-root-INFO: grad norm: 2.732 2.710 0.339
2024-12-02-13:16:46-root-INFO: grad norm: 3.466 3.441 0.416
2024-12-02-13:16:47-root-INFO: grad norm: 4.477 4.441 0.568
2024-12-02-13:16:47-root-INFO: Loss too large (40.477->40.692)! Learning rate decreased to 0.15633.
2024-12-02-13:16:48-root-INFO: grad norm: 3.593 3.560 0.485
2024-12-02-13:16:48-root-INFO: grad norm: 2.780 2.760 0.340
2024-12-02-13:16:49-root-INFO: grad norm: 2.491 2.473 0.296
2024-12-02-13:16:49-root-INFO: grad norm: 2.327 2.307 0.301
2024-12-02-13:16:49-root-INFO: Loss Change: 40.820 -> 38.766
2024-12-02-13:16:49-root-INFO: Regularization Change: 0.000 -> 2.797
2024-12-02-13:16:49-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-13:16:49-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-13:16:49-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-13:16:50-root-INFO: grad norm: 2.113 2.090 0.313
2024-12-02-13:16:50-root-INFO: grad norm: 2.343 2.325 0.289
2024-12-02-13:16:51-root-INFO: grad norm: 2.975 2.954 0.350
2024-12-02-13:16:51-root-INFO: grad norm: 3.960 3.928 0.502
2024-12-02-13:16:51-root-INFO: Loss too large (38.051->38.266)! Learning rate decreased to 0.15935.
2024-12-02-13:16:52-root-INFO: grad norm: 3.301 3.271 0.443
2024-12-02-13:16:52-root-INFO: grad norm: 2.640 2.619 0.330
2024-12-02-13:16:53-root-INFO: grad norm: 2.387 2.369 0.291
2024-12-02-13:16:53-root-INFO: grad norm: 2.230 2.211 0.293
2024-12-02-13:16:53-root-INFO: Loss Change: 38.438 -> 36.644
2024-12-02-13:16:53-root-INFO: Regularization Change: 0.000 -> 2.501
2024-12-02-13:16:53-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-13:16:53-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-13:16:54-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-13:16:54-root-INFO: grad norm: 2.047 2.028 0.277
2024-12-02-13:16:54-root-INFO: grad norm: 2.514 2.494 0.320
2024-12-02-13:16:55-root-INFO: grad norm: 3.139 3.113 0.400
2024-12-02-13:16:55-root-INFO: grad norm: 4.001 3.969 0.505
2024-12-02-13:16:55-root-INFO: Loss too large (36.201->36.421)! Learning rate decreased to 0.16241.
2024-12-02-13:16:56-root-INFO: grad norm: 3.217 3.187 0.437
2024-12-02-13:16:56-root-INFO: grad norm: 2.454 2.436 0.299
2024-12-02-13:16:57-root-INFO: grad norm: 2.174 2.158 0.262
2024-12-02-13:16:57-root-INFO: grad norm: 2.010 1.994 0.259
2024-12-02-13:16:58-root-INFO: Loss Change: 36.447 -> 34.821
2024-12-02-13:16:58-root-INFO: Regularization Change: 0.000 -> 2.274
2024-12-02-13:16:58-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-13:16:58-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-13:16:58-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-13:16:58-root-INFO: grad norm: 2.060 2.021 0.400
2024-12-02-13:16:58-root-INFO: grad norm: 1.820 1.806 0.227
2024-12-02-13:16:59-root-INFO: grad norm: 2.160 2.146 0.246
2024-12-02-13:16:59-root-INFO: grad norm: 2.899 2.876 0.363
2024-12-02-13:17:00-root-INFO: Loss too large (34.066->34.172)! Learning rate decreased to 0.16577.
2024-12-02-13:17:00-root-INFO: grad norm: 2.603 2.581 0.340
2024-12-02-13:17:00-root-INFO: grad norm: 2.287 2.268 0.297
2024-12-02-13:17:01-root-INFO: grad norm: 2.146 2.129 0.275
2024-12-02-13:17:01-root-INFO: grad norm: 2.030 2.012 0.271
2024-12-02-13:17:02-root-INFO: Loss Change: 34.661 -> 33.091
2024-12-02-13:17:02-root-INFO: Regularization Change: 0.000 -> 2.217
2024-12-02-13:17:02-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-13:17:02-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-13:17:02-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-13:17:02-root-INFO: grad norm: 1.946 1.919 0.327
2024-12-02-13:17:03-root-INFO: grad norm: 2.083 2.066 0.267
2024-12-02-13:17:03-root-INFO: grad norm: 2.522 2.501 0.323
2024-12-02-13:17:04-root-INFO: grad norm: 3.333 3.305 0.427
2024-12-02-13:17:04-root-INFO: Loss too large (32.557->32.770)! Learning rate decreased to 0.16889.
2024-12-02-13:17:04-root-INFO: grad norm: 2.804 2.778 0.383
2024-12-02-13:17:05-root-INFO: grad norm: 2.180 2.163 0.273
2024-12-02-13:17:05-root-INFO: grad norm: 1.992 1.977 0.244
2024-12-02-13:17:06-root-INFO: grad norm: 1.869 1.853 0.243
2024-12-02-13:17:06-root-INFO: Loss Change: 32.944 -> 31.505
2024-12-02-13:17:06-root-INFO: Regularization Change: 0.000 -> 2.067
2024-12-02-13:17:06-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-13:17:06-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-13:17:06-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-13:17:06-root-INFO: grad norm: 1.755 1.732 0.284
2024-12-02-13:17:07-root-INFO: grad norm: 1.911 1.895 0.245
2024-12-02-13:17:07-root-INFO: grad norm: 2.346 2.327 0.302
2024-12-02-13:17:08-root-INFO: grad norm: 3.125 3.100 0.399
2024-12-02-13:17:08-root-INFO: Loss too large (30.946->31.124)! Learning rate decreased to 0.17203.
2024-12-02-13:17:08-root-INFO: grad norm: 2.627 2.602 0.360
2024-12-02-13:17:09-root-INFO: grad norm: 2.040 2.024 0.257
2024-12-02-13:17:09-root-INFO: grad norm: 1.856 1.842 0.231
2024-12-02-13:17:10-root-INFO: grad norm: 1.730 1.715 0.225
2024-12-02-13:17:10-root-INFO: Loss Change: 31.296 -> 29.966
2024-12-02-13:17:10-root-INFO: Regularization Change: 0.000 -> 1.962
2024-12-02-13:17:10-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-13:17:10-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-13:17:10-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-13:17:10-root-INFO: grad norm: 1.701 1.673 0.310
2024-12-02-13:17:11-root-INFO: grad norm: 1.651 1.637 0.210
2024-12-02-13:17:11-root-INFO: grad norm: 1.995 1.980 0.242
2024-12-02-13:17:12-root-INFO: grad norm: 2.686 2.665 0.340
2024-12-02-13:17:12-root-INFO: Loss too large (29.306->29.416)! Learning rate decreased to 0.17521.
2024-12-02-13:17:12-root-INFO: grad norm: 2.341 2.320 0.311
2024-12-02-13:17:13-root-INFO: grad norm: 1.932 1.916 0.249
2024-12-02-13:17:13-root-INFO: grad norm: 1.785 1.770 0.225
2024-12-02-13:17:14-root-INFO: grad norm: 1.663 1.648 0.219
2024-12-02-13:17:14-root-INFO: Loss Change: 29.733 -> 28.454
2024-12-02-13:17:14-root-INFO: Regularization Change: 0.000 -> 1.911
2024-12-02-13:17:14-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-13:17:14-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-13:17:14-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-13:17:14-root-INFO: grad norm: 1.679 1.650 0.311
2024-12-02-13:17:15-root-INFO: grad norm: 1.583 1.570 0.202
2024-12-02-13:17:15-root-INFO: grad norm: 1.864 1.849 0.233
2024-12-02-13:17:16-root-INFO: grad norm: 2.486 2.467 0.313
2024-12-02-13:17:16-root-INFO: Loss too large (27.833->27.915)! Learning rate decreased to 0.17841.
2024-12-02-13:17:16-root-INFO: grad norm: 2.153 2.133 0.291
2024-12-02-13:17:17-root-INFO: grad norm: 1.754 1.739 0.227
2024-12-02-13:17:17-root-INFO: grad norm: 1.635 1.622 0.210
2024-12-02-13:17:18-root-INFO: grad norm: 1.530 1.517 0.201
2024-12-02-13:17:18-root-INFO: Loss Change: 28.280 -> 27.039
2024-12-02-13:17:18-root-INFO: Regularization Change: 0.000 -> 1.863
2024-12-02-13:17:18-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-13:17:18-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-13:17:18-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-13:17:18-root-INFO: grad norm: 1.534 1.510 0.268
2024-12-02-13:17:19-root-INFO: grad norm: 1.561 1.548 0.200
2024-12-02-13:17:19-root-INFO: grad norm: 1.860 1.845 0.236
2024-12-02-13:17:20-root-INFO: grad norm: 2.468 2.448 0.312
2024-12-02-13:17:20-root-INFO: Loss too large (26.479->26.566)! Learning rate decreased to 0.18165.
2024-12-02-13:17:21-root-INFO: grad norm: 2.123 2.104 0.285
2024-12-02-13:17:21-root-INFO: grad norm: 1.709 1.694 0.220
2024-12-02-13:17:21-root-INFO: grad norm: 1.585 1.572 0.200
2024-12-02-13:17:22-root-INFO: grad norm: 1.476 1.463 0.191
2024-12-02-13:17:22-root-INFO: Loss Change: 26.869 -> 25.702
2024-12-02-13:17:22-root-INFO: Regularization Change: 0.000 -> 1.812
2024-12-02-13:17:22-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-13:17:22-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-13:17:22-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-13:17:23-root-INFO: grad norm: 1.501 1.474 0.283
2024-12-02-13:17:23-root-INFO: grad norm: 1.339 1.327 0.179
2024-12-02-13:17:23-root-INFO: grad norm: 1.535 1.523 0.193
2024-12-02-13:17:24-root-INFO: grad norm: 2.007 1.991 0.251
2024-12-02-13:17:24-root-INFO: Loss too large (25.105->25.114)! Learning rate decreased to 0.18491.
2024-12-02-13:17:25-root-INFO: grad norm: 1.791 1.775 0.238
2024-12-02-13:17:25-root-INFO: grad norm: 1.544 1.531 0.199
2024-12-02-13:17:25-root-INFO: grad norm: 1.451 1.438 0.187
2024-12-02-13:17:26-root-INFO: grad norm: 1.359 1.347 0.176
2024-12-02-13:17:26-root-INFO: Loss Change: 25.564 -> 24.419
2024-12-02-13:17:26-root-INFO: Regularization Change: 0.000 -> 1.785
2024-12-02-13:17:26-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-13:17:26-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-13:17:26-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-13:17:27-root-INFO: grad norm: 1.455 1.432 0.259
2024-12-02-13:17:27-root-INFO: grad norm: 1.360 1.349 0.170
2024-12-02-13:17:28-root-INFO: grad norm: 1.554 1.542 0.195
2024-12-02-13:17:28-root-INFO: grad norm: 1.983 1.968 0.241
2024-12-02-13:17:28-root-INFO: Loss too large (23.826->23.828)! Learning rate decreased to 0.18821.
2024-12-02-13:17:29-root-INFO: grad norm: 1.723 1.708 0.227
2024-12-02-13:17:29-root-INFO: grad norm: 1.439 1.427 0.181
2024-12-02-13:17:30-root-INFO: grad norm: 1.338 1.327 0.172
2024-12-02-13:17:30-root-INFO: grad norm: 1.243 1.233 0.157
2024-12-02-13:17:30-root-INFO: Loss Change: 24.260 -> 23.147
2024-12-02-13:17:30-root-INFO: Regularization Change: 0.000 -> 1.752
2024-12-02-13:17:30-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-13:17:30-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-13:17:31-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-13:17:31-root-INFO: grad norm: 1.396 1.369 0.271
2024-12-02-13:17:31-root-INFO: grad norm: 1.176 1.166 0.150
2024-12-02-13:17:32-root-INFO: grad norm: 1.287 1.278 0.156
2024-12-02-13:17:32-root-INFO: grad norm: 1.610 1.598 0.194
2024-12-02-13:17:33-root-INFO: grad norm: 1.839 1.823 0.240
2024-12-02-13:17:33-root-INFO: grad norm: 2.185 2.170 0.263
2024-12-02-13:17:33-root-INFO: Loss too large (22.215->22.238)! Learning rate decreased to 0.19153.
2024-12-02-13:17:34-root-INFO: grad norm: 1.800 1.785 0.232
2024-12-02-13:17:34-root-INFO: grad norm: 1.410 1.399 0.172
2024-12-02-13:17:35-root-INFO: Loss Change: 22.868 -> 21.747
2024-12-02-13:17:35-root-INFO: Regularization Change: 0.000 -> 1.940
2024-12-02-13:17:35-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-13:17:35-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-13:17:35-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-13:17:35-root-INFO: grad norm: 1.528 1.500 0.290
2024-12-02-13:17:35-root-INFO: grad norm: 1.202 1.192 0.153
2024-12-02-13:17:36-root-INFO: grad norm: 1.267 1.259 0.149
2024-12-02-13:17:36-root-INFO: grad norm: 1.528 1.517 0.179
2024-12-02-13:17:37-root-INFO: grad norm: 1.750 1.736 0.220
2024-12-02-13:17:37-root-INFO: grad norm: 2.078 2.063 0.243
2024-12-02-13:17:37-root-INFO: Loss too large (20.983->20.983)! Learning rate decreased to 0.19488.
2024-12-02-13:17:38-root-INFO: grad norm: 1.691 1.677 0.213
2024-12-02-13:17:38-root-INFO: grad norm: 1.323 1.313 0.160
2024-12-02-13:17:39-root-INFO: Loss Change: 21.662 -> 20.528
2024-12-02-13:17:39-root-INFO: Regularization Change: 0.000 -> 1.916
2024-12-02-13:17:39-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-13:17:39-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-13:17:39-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-13:17:39-root-INFO: grad norm: 1.328 1.307 0.234
2024-12-02-13:17:39-root-INFO: grad norm: 1.143 1.134 0.141
2024-12-02-13:17:40-root-INFO: grad norm: 1.250 1.241 0.146
2024-12-02-13:17:40-root-INFO: grad norm: 1.504 1.494 0.172
2024-12-02-13:17:41-root-INFO: grad norm: 1.718 1.705 0.211
2024-12-02-13:17:41-root-INFO: grad norm: 2.016 2.003 0.228
2024-12-02-13:17:42-root-INFO: grad norm: 2.121 2.104 0.266
2024-12-02-13:17:42-root-INFO: grad norm: 2.162 2.148 0.245
2024-12-02-13:17:43-root-INFO: Loss Change: 20.366 -> 19.514
2024-12-02-13:17:43-root-INFO: Regularization Change: 0.000 -> 2.204
2024-12-02-13:17:43-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-13:17:43-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-13:17:43-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-13:17:43-root-INFO: grad norm: 2.114 2.096 0.271
2024-12-02-13:17:43-root-INFO: grad norm: 1.842 1.831 0.201
2024-12-02-13:17:44-root-INFO: grad norm: 1.824 1.812 0.204
2024-12-02-13:17:44-root-INFO: grad norm: 1.846 1.836 0.198
2024-12-02-13:17:45-root-INFO: grad norm: 1.872 1.859 0.214
2024-12-02-13:17:45-root-INFO: grad norm: 1.895 1.885 0.203
2024-12-02-13:17:46-root-INFO: grad norm: 1.910 1.898 0.220
2024-12-02-13:17:46-root-INFO: grad norm: 1.916 1.905 0.206
2024-12-02-13:17:46-root-INFO: Loss Change: 19.306 -> 18.182
2024-12-02-13:17:46-root-INFO: Regularization Change: 0.000 -> 2.116
2024-12-02-13:17:46-root-INFO: Undo step: 30
2024-12-02-13:17:46-root-INFO: Undo step: 31
2024-12-02-13:17:46-root-INFO: Undo step: 32
2024-12-02-13:17:46-root-INFO: Undo step: 33
2024-12-02-13:17:46-root-INFO: Undo step: 34
2024-12-02-13:17:46-root-INFO: Undo step: 35
2024-12-02-13:17:46-root-INFO: Undo step: 36
2024-12-02-13:17:46-root-INFO: Undo step: 37
2024-12-02-13:17:46-root-INFO: Undo step: 38
2024-12-02-13:17:46-root-INFO: Undo step: 39
2024-12-02-13:17:47-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-13:17:47-root-INFO: grad norm: 23.006 22.875 2.452
2024-12-02-13:17:47-root-INFO: grad norm: 12.238 12.124 1.662
2024-12-02-13:17:48-root-INFO: grad norm: 8.636 8.543 1.261
2024-12-02-13:17:48-root-INFO: grad norm: 7.191 7.114 1.054
2024-12-02-13:17:49-root-INFO: grad norm: 6.624 6.562 0.901
2024-12-02-13:17:49-root-INFO: grad norm: 5.815 5.754 0.842
2024-12-02-13:17:50-root-INFO: grad norm: 5.121 5.062 0.775
2024-12-02-13:17:50-root-INFO: grad norm: 4.931 4.871 0.765
2024-12-02-13:17:50-root-INFO: Loss Change: 191.697 -> 53.589
2024-12-02-13:17:50-root-INFO: Regularization Change: 0.000 -> 153.844
2024-12-02-13:17:50-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-13:17:50-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-13:17:50-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-13:17:51-root-INFO: grad norm: 5.524 5.460 0.836
2024-12-02-13:17:51-root-INFO: grad norm: 5.476 5.423 0.764
2024-12-02-13:17:52-root-INFO: grad norm: 5.059 5.016 0.661
2024-12-02-13:17:52-root-INFO: grad norm: 4.639 4.597 0.625
2024-12-02-13:17:52-root-INFO: grad norm: 4.380 4.345 0.550
2024-12-02-13:17:53-root-INFO: grad norm: 4.193 4.156 0.562
2024-12-02-13:17:53-root-INFO: grad norm: 4.027 3.996 0.495
2024-12-02-13:17:54-root-INFO: grad norm: 3.892 3.856 0.527
2024-12-02-13:17:54-root-INFO: Loss Change: 53.270 -> 40.329
2024-12-02-13:17:54-root-INFO: Regularization Change: 0.000 -> 21.546
2024-12-02-13:17:54-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-13:17:54-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-13:17:54-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-13:17:55-root-INFO: grad norm: 4.252 4.206 0.622
2024-12-02-13:17:55-root-INFO: grad norm: 3.858 3.821 0.535
2024-12-02-13:17:55-root-INFO: grad norm: 3.483 3.457 0.422
2024-12-02-13:17:56-root-INFO: grad norm: 3.314 3.284 0.449
2024-12-02-13:17:56-root-INFO: grad norm: 3.250 3.226 0.396
2024-12-02-13:17:57-root-INFO: grad norm: 3.197 3.167 0.435
2024-12-02-13:17:57-root-INFO: grad norm: 3.164 3.140 0.386
2024-12-02-13:17:58-root-INFO: grad norm: 3.132 3.103 0.427
2024-12-02-13:17:58-root-INFO: Loss Change: 40.143 -> 33.937
2024-12-02-13:17:58-root-INFO: Regularization Change: 0.000 -> 10.289
2024-12-02-13:17:58-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-13:17:58-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-13:17:58-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-13:17:58-root-INFO: grad norm: 3.775 3.731 0.580
2024-12-02-13:17:59-root-INFO: grad norm: 3.379 3.345 0.477
2024-12-02-13:17:59-root-INFO: grad norm: 3.043 3.021 0.366
2024-12-02-13:18:00-root-INFO: grad norm: 2.883 2.856 0.389
2024-12-02-13:18:00-root-INFO: grad norm: 2.800 2.780 0.335
2024-12-02-13:18:01-root-INFO: grad norm: 2.745 2.720 0.368
2024-12-02-13:18:01-root-INFO: grad norm: 2.716 2.697 0.325
2024-12-02-13:18:02-root-INFO: grad norm: 2.693 2.669 0.360
2024-12-02-13:18:02-root-INFO: Loss Change: 33.891 -> 29.839
2024-12-02-13:18:02-root-INFO: Regularization Change: 0.000 -> 6.622
2024-12-02-13:18:02-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-13:18:02-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-13:18:02-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-13:18:02-root-INFO: grad norm: 3.188 3.148 0.500
2024-12-02-13:18:03-root-INFO: grad norm: 2.927 2.899 0.405
2024-12-02-13:18:03-root-INFO: grad norm: 2.709 2.688 0.332
2024-12-02-13:18:04-root-INFO: grad norm: 2.599 2.576 0.346
2024-12-02-13:18:04-root-INFO: grad norm: 2.517 2.498 0.302
2024-12-02-13:18:05-root-INFO: grad norm: 2.467 2.445 0.326
2024-12-02-13:18:05-root-INFO: grad norm: 2.434 2.417 0.291
2024-12-02-13:18:06-root-INFO: grad norm: 2.412 2.391 0.318
2024-12-02-13:18:06-root-INFO: Loss Change: 29.756 -> 26.864
2024-12-02-13:18:06-root-INFO: Regularization Change: 0.000 -> 4.867
2024-12-02-13:18:06-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-13:18:06-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-13:18:06-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-13:18:06-root-INFO: grad norm: 3.035 2.994 0.495
2024-12-02-13:18:07-root-INFO: grad norm: 2.705 2.679 0.379
2024-12-02-13:18:07-root-INFO: grad norm: 2.459 2.441 0.298
2024-12-02-13:18:08-root-INFO: grad norm: 2.324 2.304 0.306
2024-12-02-13:18:08-root-INFO: grad norm: 2.231 2.215 0.264
2024-12-02-13:18:09-root-INFO: grad norm: 2.173 2.155 0.282
2024-12-02-13:18:09-root-INFO: grad norm: 2.137 2.122 0.252
2024-12-02-13:18:10-root-INFO: grad norm: 2.114 2.096 0.272
2024-12-02-13:18:10-root-INFO: Loss Change: 26.846 -> 24.477
2024-12-02-13:18:10-root-INFO: Regularization Change: 0.000 -> 3.895
2024-12-02-13:18:10-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-13:18:10-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-13:18:10-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-13:18:10-root-INFO: grad norm: 2.666 2.631 0.430
2024-12-02-13:18:11-root-INFO: grad norm: 2.399 2.377 0.324
2024-12-02-13:18:11-root-INFO: grad norm: 2.233 2.217 0.264
2024-12-02-13:18:12-root-INFO: grad norm: 2.128 2.110 0.275
2024-12-02-13:18:12-root-INFO: grad norm: 2.044 2.030 0.237
2024-12-02-13:18:13-root-INFO: grad norm: 1.989 1.973 0.254
2024-12-02-13:18:13-root-INFO: grad norm: 1.948 1.935 0.225
2024-12-02-13:18:14-root-INFO: grad norm: 1.922 1.906 0.245
2024-12-02-13:18:14-root-INFO: Loss Change: 24.403 -> 22.473
2024-12-02-13:18:14-root-INFO: Regularization Change: 0.000 -> 3.256
2024-12-02-13:18:14-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-13:18:14-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-13:18:14-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-13:18:14-root-INFO: grad norm: 2.584 2.546 0.442
2024-12-02-13:18:15-root-INFO: grad norm: 2.283 2.263 0.305
2024-12-02-13:18:15-root-INFO: grad norm: 2.092 2.077 0.255
2024-12-02-13:18:16-root-INFO: grad norm: 1.978 1.962 0.252
2024-12-02-13:18:16-root-INFO: grad norm: 1.882 1.869 0.222
2024-12-02-13:18:17-root-INFO: grad norm: 1.821 1.807 0.230
2024-12-02-13:18:17-root-INFO: grad norm: 1.773 1.761 0.209
2024-12-02-13:18:18-root-INFO: grad norm: 1.743 1.729 0.220
2024-12-02-13:18:18-root-INFO: Loss Change: 22.292 -> 20.589
2024-12-02-13:18:18-root-INFO: Regularization Change: 0.000 -> 2.838
2024-12-02-13:18:18-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-13:18:18-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-13:18:18-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-13:18:18-root-INFO: grad norm: 2.559 2.520 0.446
2024-12-02-13:18:19-root-INFO: grad norm: 2.183 2.162 0.297
2024-12-02-13:18:19-root-INFO: grad norm: 1.973 1.958 0.238
2024-12-02-13:18:20-root-INFO: grad norm: 1.850 1.835 0.237
2024-12-02-13:18:20-root-INFO: grad norm: 1.752 1.740 0.206
2024-12-02-13:18:21-root-INFO: grad norm: 1.691 1.677 0.215
2024-12-02-13:18:21-root-INFO: grad norm: 1.643 1.632 0.193
2024-12-02-13:18:22-root-INFO: grad norm: 1.614 1.601 0.205
2024-12-02-13:18:22-root-INFO: Loss Change: 20.643 -> 19.084
2024-12-02-13:18:22-root-INFO: Regularization Change: 0.000 -> 2.545
2024-12-02-13:18:22-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-13:18:22-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-13:18:22-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-13:18:22-root-INFO: grad norm: 2.201 2.169 0.373
2024-12-02-13:18:23-root-INFO: grad norm: 1.945 1.927 0.260
2024-12-02-13:18:23-root-INFO: grad norm: 1.814 1.801 0.217
2024-12-02-13:18:24-root-INFO: grad norm: 1.739 1.725 0.220
2024-12-02-13:18:24-root-INFO: grad norm: 1.676 1.664 0.196
2024-12-02-13:18:25-root-INFO: grad norm: 1.639 1.626 0.205
2024-12-02-13:18:25-root-INFO: grad norm: 1.613 1.602 0.188
2024-12-02-13:18:26-root-INFO: grad norm: 1.601 1.589 0.200
2024-12-02-13:18:26-root-INFO: Loss Change: 19.008 -> 17.709
2024-12-02-13:18:26-root-INFO: Regularization Change: 0.000 -> 2.288
2024-12-02-13:18:26-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-13:18:26-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-13:18:26-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-13:18:26-root-INFO: grad norm: 2.369 2.335 0.401
2024-12-02-13:18:27-root-INFO: grad norm: 2.050 2.032 0.271
2024-12-02-13:18:27-root-INFO: grad norm: 1.854 1.841 0.218
2024-12-02-13:18:28-root-INFO: grad norm: 1.778 1.764 0.220
2024-12-02-13:18:28-root-INFO: grad norm: 1.723 1.712 0.196
2024-12-02-13:18:29-root-INFO: grad norm: 1.702 1.689 0.205
2024-12-02-13:18:29-root-INFO: grad norm: 1.695 1.685 0.189
2024-12-02-13:18:30-root-INFO: grad norm: 1.697 1.685 0.201
2024-12-02-13:18:30-root-INFO: Loss Change: 17.690 -> 16.466
2024-12-02-13:18:30-root-INFO: Regularization Change: 0.000 -> 2.125
2024-12-02-13:18:30-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-13:18:30-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-13:18:30-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-13:18:30-root-INFO: grad norm: 2.466 2.433 0.405
2024-12-02-13:18:31-root-INFO: grad norm: 2.128 2.111 0.265
2024-12-02-13:18:31-root-INFO: grad norm: 1.876 1.865 0.207
2024-12-02-13:18:32-root-INFO: grad norm: 1.788 1.776 0.202
2024-12-02-13:18:32-root-INFO: grad norm: 1.705 1.695 0.182
2024-12-02-13:18:32-root-INFO: grad norm: 1.651 1.641 0.187
2024-12-02-13:18:33-root-INFO: grad norm: 1.605 1.596 0.172
2024-12-02-13:18:33-root-INFO: grad norm: 1.573 1.563 0.180
2024-12-02-13:18:34-root-INFO: Loss Change: 16.480 -> 15.281
2024-12-02-13:18:34-root-INFO: Regularization Change: 0.000 -> 1.996
2024-12-02-13:18:34-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-13:18:34-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-13:18:34-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-13:18:34-root-INFO: grad norm: 2.331 2.298 0.391
2024-12-02-13:18:35-root-INFO: grad norm: 1.951 1.935 0.248
2024-12-02-13:18:36-root-INFO: grad norm: 1.676 1.665 0.187
2024-12-02-13:18:37-root-INFO: grad norm: 1.587 1.576 0.186
2024-12-02-13:18:38-root-INFO: grad norm: 1.515 1.506 0.164
2024-12-02-13:18:39-root-INFO: grad norm: 1.475 1.464 0.173
2024-12-02-13:18:40-root-INFO: grad norm: 1.447 1.438 0.157
2024-12-02-13:18:41-root-INFO: grad norm: 1.430 1.420 0.168
2024-12-02-13:18:41-root-INFO: Loss Change: 15.191 -> 14.068
2024-12-02-13:18:41-root-INFO: Regularization Change: 0.000 -> 1.883
2024-12-02-13:18:41-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-13:18:41-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-13:18:42-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-13:18:42-root-INFO: grad norm: 2.136 2.109 0.342
2024-12-02-13:18:43-root-INFO: grad norm: 1.818 1.803 0.230
2024-12-02-13:18:44-root-INFO: grad norm: 1.585 1.575 0.174
2024-12-02-13:18:45-root-INFO: grad norm: 1.514 1.503 0.178
2024-12-02-13:18:46-root-INFO: grad norm: 1.456 1.447 0.156
2024-12-02-13:18:47-root-INFO: grad norm: 1.425 1.415 0.168
2024-12-02-13:18:48-root-INFO: grad norm: 1.407 1.398 0.151
2024-12-02-13:18:49-root-INFO: grad norm: 1.395 1.386 0.164
2024-12-02-13:18:50-root-INFO: Loss Change: 14.085 -> 13.081
2024-12-02-13:18:50-root-INFO: Regularization Change: 0.000 -> 1.766
2024-12-02-13:18:50-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-13:18:50-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-13:18:50-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-13:18:50-root-INFO: grad norm: 2.115 2.086 0.352
2024-12-02-13:18:51-root-INFO: grad norm: 1.767 1.753 0.224
2024-12-02-13:18:52-root-INFO: grad norm: 1.541 1.531 0.170
2024-12-02-13:18:53-root-INFO: grad norm: 1.480 1.469 0.175
2024-12-02-13:18:54-root-INFO: grad norm: 1.432 1.424 0.153
2024-12-02-13:18:55-root-INFO: grad norm: 1.411 1.401 0.166
2024-12-02-13:18:56-root-INFO: grad norm: 1.402 1.394 0.149
2024-12-02-13:18:57-root-INFO: grad norm: 1.397 1.387 0.164
2024-12-02-13:18:58-root-INFO: Loss Change: 13.088 -> 12.127
2024-12-02-13:18:58-root-INFO: Regularization Change: 0.000 -> 1.703
2024-12-02-13:18:58-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-13:18:58-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-13:18:58-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-13:18:59-root-INFO: grad norm: 2.149 2.121 0.348
2024-12-02-13:19:00-root-INFO: grad norm: 1.759 1.746 0.213
2024-12-02-13:19:01-root-INFO: grad norm: 1.435 1.427 0.154
2024-12-02-13:19:02-root-INFO: grad norm: 1.356 1.348 0.153
2024-12-02-13:19:03-root-INFO: grad norm: 1.292 1.285 0.134
2024-12-02-13:19:04-root-INFO: grad norm: 1.265 1.257 0.142
2024-12-02-13:19:05-root-INFO: grad norm: 1.251 1.244 0.129
2024-12-02-13:19:06-root-INFO: grad norm: 1.245 1.237 0.140
2024-12-02-13:19:06-root-INFO: Loss Change: 12.056 -> 11.104
2024-12-02-13:19:06-root-INFO: Regularization Change: 0.000 -> 1.646
2024-12-02-13:19:06-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-13:19:06-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-13:19:07-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-13:19:07-root-INFO: grad norm: 2.118 2.091 0.342
2024-12-02-13:19:08-root-INFO: grad norm: 1.709 1.696 0.213
2024-12-02-13:19:09-root-INFO: grad norm: 1.387 1.379 0.148
2024-12-02-13:19:10-root-INFO: grad norm: 1.326 1.317 0.152
2024-12-02-13:19:11-root-INFO: grad norm: 1.283 1.277 0.132
2024-12-02-13:19:12-root-INFO: grad norm: 1.270 1.262 0.144
2024-12-02-13:19:13-root-INFO: grad norm: 1.271 1.264 0.130
2024-12-02-13:19:14-root-INFO: grad norm: 1.273 1.265 0.144
2024-12-02-13:19:14-root-INFO: Loss Change: 11.201 -> 10.290
2024-12-02-13:19:14-root-INFO: Regularization Change: 0.000 -> 1.599
2024-12-02-13:19:14-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-13:19:14-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-13:19:15-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-13:19:15-root-INFO: grad norm: 1.953 1.926 0.319
2024-12-02-13:19:16-root-INFO: grad norm: 1.587 1.576 0.192
2024-12-02-13:19:17-root-INFO: grad norm: 1.367 1.359 0.145
2024-12-02-13:19:18-root-INFO: grad norm: 1.299 1.290 0.148
2024-12-02-13:19:19-root-INFO: grad norm: 1.241 1.234 0.126
2024-12-02-13:19:20-root-INFO: grad norm: 1.219 1.211 0.136
2024-12-02-13:19:21-root-INFO: grad norm: 1.206 1.200 0.121
2024-12-02-13:19:22-root-INFO: grad norm: 1.201 1.194 0.134
2024-12-02-13:19:23-root-INFO: Loss Change: 10.229 -> 9.376
2024-12-02-13:19:23-root-INFO: Regularization Change: 0.000 -> 1.541
2024-12-02-13:19:23-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-13:19:23-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-13:19:23-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-13:19:23-root-INFO: grad norm: 1.977 1.952 0.314
2024-12-02-13:19:24-root-INFO: grad norm: 1.606 1.594 0.194
2024-12-02-13:19:25-root-INFO: grad norm: 1.339 1.332 0.141
2024-12-02-13:19:26-root-INFO: grad norm: 1.266 1.258 0.144
2024-12-02-13:19:27-root-INFO: grad norm: 1.204 1.198 0.121
2024-12-02-13:19:28-root-INFO: grad norm: 1.180 1.173 0.132
2024-12-02-13:19:29-root-INFO: grad norm: 1.164 1.158 0.116
2024-12-02-13:19:30-root-INFO: grad norm: 1.158 1.151 0.128
2024-12-02-13:19:31-root-INFO: Loss Change: 9.431 -> 8.602
2024-12-02-13:19:31-root-INFO: Regularization Change: 0.000 -> 1.487
2024-12-02-13:19:31-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-13:19:31-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-13:19:31-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-13:19:32-root-INFO: grad norm: 1.841 1.817 0.296
2024-12-02-13:19:33-root-INFO: grad norm: 1.493 1.482 0.181
2024-12-02-13:19:34-root-INFO: grad norm: 1.296 1.289 0.134
2024-12-02-13:19:35-root-INFO: grad norm: 1.230 1.222 0.140
2024-12-02-13:19:36-root-INFO: grad norm: 1.171 1.165 0.115
2024-12-02-13:19:37-root-INFO: grad norm: 1.146 1.139 0.128
2024-12-02-13:19:38-root-INFO: grad norm: 1.127 1.122 0.109
2024-12-02-13:19:39-root-INFO: grad norm: 1.118 1.111 0.123
2024-12-02-13:19:39-root-INFO: Loss Change: 8.605 -> 7.831
2024-12-02-13:19:39-root-INFO: Regularization Change: 0.000 -> 1.430
2024-12-02-13:19:39-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-13:19:39-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-13:19:40-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-13:19:40-root-INFO: grad norm: 1.919 1.893 0.318
2024-12-02-13:19:41-root-INFO: grad norm: 1.500 1.489 0.178
2024-12-02-13:19:42-root-INFO: grad norm: 1.230 1.224 0.127
2024-12-02-13:19:43-root-INFO: grad norm: 1.137 1.130 0.126
2024-12-02-13:19:44-root-INFO: grad norm: 1.060 1.055 0.101
2024-12-02-13:19:45-root-INFO: grad norm: 1.025 1.019 0.111
2024-12-02-13:19:46-root-INFO: grad norm: 1.010 1.006 0.094
2024-12-02-13:19:47-root-INFO: grad norm: 1.014 1.008 0.110
2024-12-02-13:19:48-root-INFO: Loss Change: 7.897 -> 7.104
2024-12-02-13:19:48-root-INFO: Regularization Change: 0.000 -> 1.401
2024-12-02-13:19:48-root-INFO: Undo step: 20
2024-12-02-13:19:48-root-INFO: Undo step: 21
2024-12-02-13:19:48-root-INFO: Undo step: 22
2024-12-02-13:19:48-root-INFO: Undo step: 23
2024-12-02-13:19:48-root-INFO: Undo step: 24
2024-12-02-13:19:48-root-INFO: Undo step: 25
2024-12-02-13:19:48-root-INFO: Undo step: 26
2024-12-02-13:19:48-root-INFO: Undo step: 27
2024-12-02-13:19:48-root-INFO: Undo step: 28
2024-12-02-13:19:48-root-INFO: Undo step: 29
2024-12-02-13:19:48-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-13:19:48-root-INFO: grad norm: 19.213 19.129 1.798
2024-12-02-13:19:49-root-INFO: grad norm: 10.312 10.238 1.228
2024-12-02-13:19:50-root-INFO: grad norm: 7.280 7.222 0.920
2024-12-02-13:19:51-root-INFO: grad norm: 5.723 5.673 0.757
2024-12-02-13:19:52-root-INFO: grad norm: 4.786 4.744 0.638
2024-12-02-13:19:53-root-INFO: grad norm: 4.151 4.114 0.559
2024-12-02-13:19:54-root-INFO: grad norm: 3.683 3.650 0.494
2024-12-02-13:19:55-root-INFO: grad norm: 3.317 3.287 0.447
2024-12-02-13:19:56-root-INFO: Loss Change: 158.086 -> 37.138
2024-12-02-13:19:56-root-INFO: Regularization Change: 0.000 -> 165.563
2024-12-02-13:19:56-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-13:19:56-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-13:19:56-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-13:19:56-root-INFO: grad norm: 3.274 3.244 0.441
2024-12-02-13:19:57-root-INFO: grad norm: 2.954 2.928 0.385
2024-12-02-13:19:58-root-INFO: grad norm: 2.852 2.831 0.348
2024-12-02-13:19:59-root-INFO: grad norm: 2.761 2.739 0.343
2024-12-02-13:20:00-root-INFO: grad norm: 2.680 2.662 0.314
2024-12-02-13:20:01-root-INFO: grad norm: 2.561 2.542 0.311
2024-12-02-13:20:02-root-INFO: grad norm: 2.371 2.354 0.281
2024-12-02-13:20:03-root-INFO: grad norm: 2.257 2.240 0.277
2024-12-02-13:20:04-root-INFO: Loss Change: 36.760 -> 25.286
2024-12-02-13:20:04-root-INFO: Regularization Change: 0.000 -> 23.315
2024-12-02-13:20:04-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-13:20:04-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-13:20:04-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-13:20:04-root-INFO: grad norm: 2.703 2.673 0.398
2024-12-02-13:20:05-root-INFO: grad norm: 2.336 2.318 0.293
2024-12-02-13:20:06-root-INFO: grad norm: 2.040 2.025 0.241
2024-12-02-13:20:07-root-INFO: grad norm: 1.897 1.883 0.237
2024-12-02-13:20:08-root-INFO: grad norm: 1.846 1.834 0.217
2024-12-02-13:20:09-root-INFO: grad norm: 1.794 1.781 0.219
2024-12-02-13:20:10-root-INFO: grad norm: 1.754 1.742 0.201
2024-12-02-13:20:11-root-INFO: grad norm: 1.711 1.698 0.206
2024-12-02-13:20:12-root-INFO: Loss Change: 24.882 -> 19.580
2024-12-02-13:20:12-root-INFO: Regularization Change: 0.000 -> 10.702
2024-12-02-13:20:12-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-13:20:12-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-13:20:12-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-13:20:13-root-INFO: grad norm: 2.158 2.132 0.334
2024-12-02-13:20:14-root-INFO: grad norm: 1.875 1.860 0.239
2024-12-02-13:20:15-root-INFO: grad norm: 1.697 1.686 0.195
2024-12-02-13:20:16-root-INFO: grad norm: 1.619 1.606 0.201
2024-12-02-13:20:17-root-INFO: grad norm: 1.582 1.572 0.177
2024-12-02-13:20:18-root-INFO: grad norm: 1.553 1.541 0.190
2024-12-02-13:20:19-root-INFO: grad norm: 1.537 1.527 0.169
2024-12-02-13:20:20-root-INFO: grad norm: 1.520 1.509 0.184
2024-12-02-13:20:20-root-INFO: Loss Change: 19.427 -> 16.243
2024-12-02-13:20:20-root-INFO: Regularization Change: 0.000 -> 6.532
2024-12-02-13:20:20-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-13:20:20-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-13:20:21-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-13:20:21-root-INFO: grad norm: 2.158 2.130 0.349
2024-12-02-13:20:22-root-INFO: grad norm: 1.864 1.848 0.247
2024-12-02-13:20:23-root-INFO: grad norm: 1.701 1.690 0.195
2024-12-02-13:20:24-root-INFO: grad norm: 1.629 1.616 0.211
2024-12-02-13:20:25-root-INFO: grad norm: 1.585 1.575 0.179
2024-12-02-13:20:26-root-INFO: grad norm: 1.554 1.541 0.202
2024-12-02-13:20:27-root-INFO: grad norm: 1.538 1.528 0.174
2024-12-02-13:20:28-root-INFO: grad norm: 1.528 1.515 0.200
2024-12-02-13:20:29-root-INFO: Loss Change: 16.102 -> 13.878
2024-12-02-13:20:29-root-INFO: Regularization Change: 0.000 -> 4.517
2024-12-02-13:20:29-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-13:20:29-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-13:20:29-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-13:20:29-root-INFO: grad norm: 2.256 2.225 0.373
2024-12-02-13:20:30-root-INFO: grad norm: 1.914 1.896 0.258
2024-12-02-13:20:31-root-INFO: grad norm: 1.613 1.602 0.191
2024-12-02-13:20:32-root-INFO: grad norm: 1.502 1.488 0.199
2024-12-02-13:20:33-root-INFO: grad norm: 1.513 1.504 0.171
2024-12-02-13:20:34-root-INFO: grad norm: 1.456 1.444 0.187
2024-12-02-13:20:35-root-INFO: grad norm: 1.372 1.363 0.154
2024-12-02-13:20:36-root-INFO: grad norm: 1.337 1.325 0.173
2024-12-02-13:20:37-root-INFO: Loss Change: 13.674 -> 11.962
2024-12-02-13:20:37-root-INFO: Regularization Change: 0.000 -> 3.337
2024-12-02-13:20:37-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-13:20:37-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-13:20:37-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-13:20:37-root-INFO: grad norm: 2.477 2.446 0.393
2024-12-02-13:20:38-root-INFO: grad norm: 1.938 1.920 0.264
2024-12-02-13:20:40-root-INFO: grad norm: 1.554 1.544 0.181
2024-12-02-13:20:40-root-INFO: grad norm: 1.417 1.404 0.192
2024-12-02-13:20:42-root-INFO: grad norm: 1.343 1.334 0.153
2024-12-02-13:20:42-root-INFO: grad norm: 1.306 1.294 0.174
2024-12-02-13:20:43-root-INFO: grad norm: 1.298 1.290 0.149
2024-12-02-13:20:44-root-INFO: grad norm: 1.303 1.291 0.173
2024-12-02-13:20:45-root-INFO: Loss Change: 12.013 -> 10.596
2024-12-02-13:20:45-root-INFO: Regularization Change: 0.000 -> 2.655
2024-12-02-13:20:45-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-13:20:45-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-13:20:46-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-13:20:46-root-INFO: grad norm: 2.018 1.987 0.354
2024-12-02-13:20:47-root-INFO: grad norm: 1.661 1.644 0.234
2024-12-02-13:20:48-root-INFO: grad norm: 1.410 1.399 0.174
2024-12-02-13:20:49-root-INFO: grad norm: 1.324 1.311 0.183
2024-12-02-13:20:50-root-INFO: grad norm: 1.426 1.417 0.162
2024-12-02-13:20:51-root-INFO: grad norm: 1.321 1.309 0.176
2024-12-02-13:20:52-root-INFO: grad norm: 1.224 1.216 0.142
2024-12-02-13:20:53-root-INFO: grad norm: 1.171 1.160 0.159
2024-12-02-13:20:53-root-INFO: Loss Change: 10.446 -> 9.316
2024-12-02-13:20:53-root-INFO: Regularization Change: 0.000 -> 2.156
2024-12-02-13:20:53-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-13:20:53-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-13:20:54-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-13:20:54-root-INFO: grad norm: 2.138 2.107 0.364
2024-12-02-13:20:55-root-INFO: grad norm: 1.695 1.677 0.245
2024-12-02-13:20:56-root-INFO: grad norm: 1.356 1.346 0.165
2024-12-02-13:20:57-root-INFO: grad norm: 1.241 1.229 0.175
2024-12-02-13:20:58-root-INFO: grad norm: 1.235 1.225 0.151
2024-12-02-13:20:59-root-INFO: grad norm: 1.222 1.210 0.169
2024-12-02-13:21:00-root-INFO: grad norm: 1.213 1.204 0.149
2024-12-02-13:21:01-root-INFO: grad norm: 1.219 1.207 0.170
2024-12-02-13:21:02-root-INFO: Loss Change: 9.332 -> 8.355
2024-12-02-13:21:02-root-INFO: Regularization Change: 0.000 -> 1.821
2024-12-02-13:21:02-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-13:21:02-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-13:21:02-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-13:21:02-root-INFO: grad norm: 2.049 2.018 0.353
2024-12-02-13:21:03-root-INFO: grad norm: 1.597 1.580 0.235
2024-12-02-13:21:04-root-INFO: grad norm: 1.304 1.295 0.159
2024-12-02-13:21:05-root-INFO: grad norm: 1.190 1.178 0.172
2024-12-02-13:21:06-root-INFO: grad norm: 1.216 1.207 0.149
2024-12-02-13:21:07-root-INFO: grad norm: 1.186 1.174 0.168
2024-12-02-13:21:08-root-INFO: grad norm: 1.127 1.118 0.143
2024-12-02-13:21:09-root-INFO: grad norm: 1.139 1.127 0.164
2024-12-02-13:21:10-root-INFO: Loss Change: 8.329 -> 7.482
2024-12-02-13:21:10-root-INFO: Regularization Change: 0.000 -> 1.584
2024-12-02-13:21:10-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-13:21:10-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-13:21:10-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-13:21:11-root-INFO: grad norm: 2.344 2.310 0.396
2024-12-02-13:21:12-root-INFO: grad norm: 1.629 1.611 0.242
2024-12-02-13:21:13-root-INFO: grad norm: 1.232 1.223 0.151
2024-12-02-13:21:14-root-INFO: grad norm: 1.109 1.098 0.159
2024-12-02-13:21:15-root-INFO: grad norm: 1.026 1.019 0.127
2024-12-02-13:21:16-root-INFO: grad norm: 0.982 0.972 0.137
2024-12-02-13:21:17-root-INFO: grad norm: 0.963 0.955 0.125
2024-12-02-13:21:18-root-INFO: grad norm: 0.974 0.964 0.139
2024-12-02-13:21:18-root-INFO: Loss Change: 7.543 -> 6.702
2024-12-02-13:21:18-root-INFO: Regularization Change: 0.000 -> 1.420
2024-12-02-13:21:18-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-13:21:18-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-13:21:19-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-13:21:19-root-INFO: grad norm: 1.963 1.933 0.340
2024-12-02-13:21:20-root-INFO: grad norm: 1.411 1.395 0.211
2024-12-02-13:21:21-root-INFO: grad norm: 1.104 1.096 0.131
2024-12-02-13:21:22-root-INFO: grad norm: 0.992 0.982 0.139
2024-12-02-13:21:23-root-INFO: grad norm: 0.926 0.919 0.118
2024-12-02-13:21:24-root-INFO: grad norm: 0.919 0.910 0.128
2024-12-02-13:21:25-root-INFO: grad norm: 1.081 1.075 0.118
2024-12-02-13:21:26-root-INFO: grad norm: 0.948 0.939 0.130
2024-12-02-13:21:26-root-INFO: Loss Change: 6.720 -> 6.002
2024-12-02-13:21:26-root-INFO: Regularization Change: 0.000 -> 1.282
2024-12-02-13:21:26-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-13:21:26-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-13:21:27-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-13:21:27-root-INFO: grad norm: 1.767 1.733 0.347
2024-12-02-13:21:28-root-INFO: grad norm: 1.285 1.270 0.200
2024-12-02-13:21:29-root-INFO: grad norm: 1.320 1.312 0.147
2024-12-02-13:21:30-root-INFO: grad norm: 1.025 1.016 0.140
2024-12-02-13:21:31-root-INFO: grad norm: 0.872 0.867 0.097
2024-12-02-13:21:32-root-INFO: grad norm: 0.771 0.765 0.101
2024-12-02-13:21:33-root-INFO: grad norm: 0.787 0.782 0.092
2024-12-02-13:21:34-root-INFO: grad norm: 0.813 0.806 0.105
2024-12-02-13:21:34-root-INFO: Loss Change: 6.083 -> 5.394
2024-12-02-13:21:34-root-INFO: Regularization Change: 0.000 -> 1.201
2024-12-02-13:21:34-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-13:21:34-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-13:21:35-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-13:21:35-root-INFO: grad norm: 1.853 1.824 0.326
2024-12-02-13:21:36-root-INFO: grad norm: 1.194 1.181 0.176
2024-12-02-13:21:37-root-INFO: grad norm: 0.906 0.900 0.100
2024-12-02-13:21:38-root-INFO: grad norm: 0.797 0.790 0.103
2024-12-02-13:21:39-root-INFO: grad norm: 0.714 0.709 0.083
2024-12-02-13:21:40-root-INFO: grad norm: 0.710 0.704 0.090
2024-12-02-13:21:41-root-INFO: grad norm: 0.970 0.966 0.082
2024-12-02-13:21:42-root-INFO: grad norm: 0.742 0.736 0.093
2024-12-02-13:21:42-root-INFO: Loss Change: 5.472 -> 4.832
2024-12-02-13:21:42-root-INFO: Regularization Change: 0.000 -> 1.135
2024-12-02-13:21:43-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-13:21:43-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-13:21:43-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-13:21:43-root-INFO: grad norm: 1.467 1.439 0.286
2024-12-02-13:21:44-root-INFO: grad norm: 0.965 0.953 0.150
2024-12-02-13:21:45-root-INFO: grad norm: 0.850 0.844 0.105
2024-12-02-13:21:46-root-INFO: grad norm: 0.831 0.823 0.112
2024-12-02-13:21:47-root-INFO: grad norm: 0.974 0.969 0.096
2024-12-02-13:21:48-root-INFO: grad norm: 0.756 0.749 0.103
2024-12-02-13:21:49-root-INFO: grad norm: 0.647 0.643 0.071
2024-12-02-13:21:50-root-INFO: grad norm: 0.596 0.592 0.067
2024-12-02-13:21:51-root-INFO: Loss Change: 4.912 -> 4.327
2024-12-02-13:21:51-root-INFO: Regularization Change: 0.000 -> 1.070
2024-12-02-13:21:51-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-13:21:51-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-13:21:51-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-13:21:51-root-INFO: grad norm: 1.438 1.408 0.292
2024-12-02-13:21:53-root-INFO: grad norm: 1.003 0.995 0.126
2024-12-02-13:21:54-root-INFO: grad norm: 0.788 0.783 0.080
2024-12-02-13:21:55-root-INFO: grad norm: 0.611 0.608 0.063
2024-12-02-13:21:56-root-INFO: grad norm: 0.616 0.614 0.054
2024-12-02-13:21:57-root-INFO: grad norm: 1.026 1.025 0.057
2024-12-02-13:21:57-root-INFO: Loss too large (4.005->4.013)! Learning rate decreased to 0.25535.
2024-12-02-13:21:58-root-INFO: grad norm: 0.556 0.554 0.046
2024-12-02-13:21:59-root-INFO: grad norm: 0.488 0.487 0.038
2024-12-02-13:21:59-root-INFO: Loss Change: 4.416 -> 3.882
2024-12-02-13:21:59-root-INFO: Regularization Change: 0.000 -> 0.874
2024-12-02-13:21:59-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-13:21:59-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-13:22:00-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-13:22:00-root-INFO: grad norm: 1.310 1.281 0.272
2024-12-02-13:22:01-root-INFO: grad norm: 0.850 0.842 0.117
2024-12-02-13:22:02-root-INFO: grad norm: 0.695 0.690 0.083
2024-12-02-13:22:03-root-INFO: grad norm: 0.679 0.674 0.077
2024-12-02-13:22:04-root-INFO: grad norm: 0.766 0.764 0.066
2024-12-02-13:22:05-root-INFO: grad norm: 0.604 0.601 0.067
2024-12-02-13:22:06-root-INFO: grad norm: 0.422 0.420 0.041
2024-12-02-13:22:07-root-INFO: grad norm: 0.392 0.391 0.034
2024-12-02-13:22:08-root-INFO: Loss Change: 3.977 -> 3.491
2024-12-02-13:22:08-root-INFO: Regularization Change: 0.000 -> 0.916
2024-12-02-13:22:08-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-13:22:08-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-13:22:08-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-13:22:08-root-INFO: grad norm: 1.239 1.212 0.258
2024-12-02-13:22:09-root-INFO: grad norm: 0.719 0.714 0.084
2024-12-02-13:22:10-root-INFO: grad norm: 0.662 0.660 0.053
2024-12-02-13:22:11-root-INFO: grad norm: 1.006 1.005 0.049
2024-12-02-13:22:12-root-INFO: Loss too large (3.344->3.351)! Learning rate decreased to 0.26280.
2024-12-02-13:22:13-root-INFO: grad norm: 0.550 0.548 0.048
2024-12-02-13:22:14-root-INFO: grad norm: 0.463 0.461 0.035
2024-12-02-13:22:15-root-INFO: grad norm: 0.469 0.468 0.033
2024-12-02-13:22:16-root-INFO: grad norm: 0.524 0.522 0.043
2024-12-02-13:22:16-root-INFO: Loss Change: 3.609 -> 3.187
2024-12-02-13:22:16-root-INFO: Regularization Change: 0.000 -> 0.706
2024-12-02-13:22:16-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-13:22:16-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-13:22:16-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-13:22:17-root-INFO: grad norm: 1.408 1.384 0.256
2024-12-02-13:22:18-root-INFO: grad norm: 0.716 0.710 0.091
2024-12-02-13:22:19-root-INFO: grad norm: 0.580 0.578 0.048
2024-12-02-13:22:20-root-INFO: grad norm: 0.558 0.557 0.034
2024-12-02-13:22:21-root-INFO: grad norm: 0.593 0.591 0.049
2024-12-02-13:22:22-root-INFO: grad norm: 0.735 0.734 0.044
2024-12-02-13:22:22-root-INFO: Loss too large (2.970->2.974)! Learning rate decreased to 0.26654.
2024-12-02-13:22:23-root-INFO: grad norm: 0.579 0.577 0.052
2024-12-02-13:22:24-root-INFO: grad norm: 0.382 0.381 0.030
2024-12-02-13:22:25-root-INFO: Loss Change: 3.322 -> 2.881
2024-12-02-13:22:25-root-INFO: Regularization Change: 0.000 -> 0.741
2024-12-02-13:22:25-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-13:22:25-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-13:22:25-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-13:22:25-root-INFO: grad norm: 1.124 1.099 0.236
2024-12-02-13:22:26-root-INFO: grad norm: 0.530 0.527 0.058
2024-12-02-13:22:27-root-INFO: grad norm: 0.445 0.444 0.038
2024-12-02-13:22:28-root-INFO: grad norm: 0.664 0.663 0.037
2024-12-02-13:22:29-root-INFO: Loss too large (2.757->2.764)! Learning rate decreased to 0.27030.
2024-12-02-13:22:30-root-INFO: grad norm: 0.628 0.626 0.054
2024-12-02-13:22:31-root-INFO: grad norm: 0.544 0.543 0.038
2024-12-02-13:22:32-root-INFO: grad norm: 0.582 0.580 0.049
2024-12-02-13:22:33-root-INFO: grad norm: 0.645 0.644 0.041
2024-12-02-13:22:33-root-INFO: Loss Change: 3.020 -> 2.657
2024-12-02-13:22:33-root-INFO: Regularization Change: 0.000 -> 0.655
2024-12-02-13:22:33-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-13:22:33-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-13:22:34-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-13:22:34-root-INFO: grad norm: 1.200 1.179 0.224
2024-12-02-13:22:35-root-INFO: grad norm: 0.768 0.766 0.049
2024-12-02-13:22:36-root-INFO: grad norm: 0.636 0.634 0.057
2024-12-02-13:22:37-root-INFO: grad norm: 0.513 0.511 0.039
2024-12-02-13:22:38-root-INFO: grad norm: 0.603 0.601 0.054
2024-12-02-13:22:39-root-INFO: grad norm: 0.791 0.789 0.049
2024-12-02-13:22:40-root-INFO: grad norm: 0.596 0.592 0.063
2024-12-02-13:22:41-root-INFO: grad norm: 0.372 0.370 0.034
2024-12-02-13:22:41-root-INFO: Loss Change: 2.821 -> 2.389
2024-12-02-13:22:41-root-INFO: Regularization Change: 0.000 -> 0.793
2024-12-02-13:22:41-root-INFO: Undo step: 10
2024-12-02-13:22:41-root-INFO: Undo step: 11
2024-12-02-13:22:41-root-INFO: Undo step: 12
2024-12-02-13:22:41-root-INFO: Undo step: 13
2024-12-02-13:22:41-root-INFO: Undo step: 14
2024-12-02-13:22:41-root-INFO: Undo step: 15
2024-12-02-13:22:41-root-INFO: Undo step: 16
2024-12-02-13:22:41-root-INFO: Undo step: 17
2024-12-02-13:22:41-root-INFO: Undo step: 18
2024-12-02-13:22:41-root-INFO: Undo step: 19
2024-12-02-13:22:42-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-13:22:42-root-INFO: grad norm: 16.743 16.701 1.182
2024-12-02-13:22:43-root-INFO: grad norm: 9.278 9.239 0.854
2024-12-02-13:22:44-root-INFO: grad norm: 6.527 6.495 0.641
2024-12-02-13:22:45-root-INFO: grad norm: 5.183 5.156 0.529
2024-12-02-13:22:46-root-INFO: grad norm: 4.209 4.186 0.443
2024-12-02-13:22:47-root-INFO: grad norm: 3.578 3.557 0.385
2024-12-02-13:22:48-root-INFO: grad norm: 3.132 3.113 0.340
2024-12-02-13:22:49-root-INFO: grad norm: 2.796 2.779 0.307
2024-12-02-13:22:50-root-INFO: Loss Change: 133.423 -> 23.983
2024-12-02-13:22:50-root-INFO: Regularization Change: 0.000 -> 170.154
2024-12-02-13:22:50-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-13:22:50-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-13:22:50-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-13:22:50-root-INFO: grad norm: 2.762 2.746 0.299
2024-12-02-13:22:51-root-INFO: grad norm: 2.422 2.408 0.258
2024-12-02-13:22:52-root-INFO: grad norm: 2.205 2.194 0.227
2024-12-02-13:22:53-root-INFO: grad norm: 2.036 2.025 0.217
2024-12-02-13:22:54-root-INFO: grad norm: 1.894 1.884 0.198
2024-12-02-13:22:55-root-INFO: grad norm: 1.775 1.765 0.191
2024-12-02-13:22:56-root-INFO: grad norm: 1.673 1.664 0.176
2024-12-02-13:22:57-root-INFO: grad norm: 1.585 1.576 0.172
2024-12-02-13:22:58-root-INFO: Loss Change: 23.346 -> 14.284
2024-12-02-13:22:58-root-INFO: Regularization Change: 0.000 -> 21.292
2024-12-02-13:22:58-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-13:22:58-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-13:22:58-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-13:22:58-root-INFO: grad norm: 1.890 1.873 0.249
2024-12-02-13:22:59-root-INFO: grad norm: 1.586 1.576 0.176
2024-12-02-13:23:00-root-INFO: grad norm: 1.477 1.470 0.143
2024-12-02-13:23:01-root-INFO: grad norm: 1.411 1.403 0.156
2024-12-02-13:23:02-root-INFO: grad norm: 1.366 1.360 0.135
2024-12-02-13:23:03-root-INFO: grad norm: 1.341 1.333 0.152
2024-12-02-13:23:04-root-INFO: grad norm: 1.374 1.367 0.139
2024-12-02-13:23:05-root-INFO: grad norm: 1.393 1.382 0.168
2024-12-02-13:23:06-root-INFO: Loss Change: 13.884 -> 10.070
2024-12-02-13:23:06-root-INFO: Regularization Change: 0.000 -> 9.217
2024-12-02-13:23:06-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-13:23:06-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-13:23:06-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-13:23:07-root-INFO: grad norm: 2.182 2.158 0.322
2024-12-02-13:23:08-root-INFO: grad norm: 1.782 1.765 0.245
2024-12-02-13:23:09-root-INFO: grad norm: 1.521 1.515 0.138
2024-12-02-13:23:10-root-INFO: grad norm: 1.358 1.348 0.161
2024-12-02-13:23:11-root-INFO: grad norm: 1.344 1.336 0.145
2024-12-02-13:23:12-root-INFO: grad norm: 1.369 1.357 0.176
2024-12-02-13:23:13-root-INFO: grad norm: 1.628 1.620 0.161
2024-12-02-13:23:14-root-INFO: grad norm: 1.393 1.380 0.187
2024-12-02-13:23:14-root-INFO: Loss Change: 9.875 -> 7.697
2024-12-02-13:23:14-root-INFO: Regularization Change: 0.000 -> 5.161
2024-12-02-13:23:14-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-13:23:14-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-13:23:15-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-13:23:15-root-INFO: grad norm: 1.974 1.949 0.310
2024-12-02-13:23:16-root-INFO: grad norm: 1.543 1.527 0.222
2024-12-02-13:23:17-root-INFO: grad norm: 1.448 1.442 0.136
2024-12-02-13:23:18-root-INFO: grad norm: 1.218 1.209 0.152
2024-12-02-13:23:19-root-INFO: grad norm: 1.144 1.138 0.117
2024-12-02-13:23:20-root-INFO: grad norm: 1.038 1.029 0.136
2024-12-02-13:23:21-root-INFO: grad norm: 0.988 0.982 0.108
2024-12-02-13:23:22-root-INFO: grad norm: 0.965 0.957 0.128
2024-12-02-13:23:23-root-INFO: Loss Change: 7.579 -> 6.034
2024-12-02-13:23:23-root-INFO: Regularization Change: 0.000 -> 3.300
2024-12-02-13:23:23-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-13:23:23-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-13:23:23-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-13:23:23-root-INFO: grad norm: 1.716 1.687 0.312
2024-12-02-13:23:24-root-INFO: grad norm: 1.258 1.244 0.186
2024-12-02-13:23:25-root-INFO: grad norm: 1.212 1.207 0.104
2024-12-02-13:23:26-root-INFO: grad norm: 0.973 0.966 0.114
2024-12-02-13:23:27-root-INFO: grad norm: 0.912 0.908 0.083
2024-12-02-13:23:28-root-INFO: grad norm: 0.905 0.901 0.088
2024-12-02-13:23:29-root-INFO: grad norm: 0.850 0.847 0.075
2024-12-02-13:23:30-root-INFO: grad norm: 0.786 0.782 0.079
2024-12-02-13:23:31-root-INFO: Loss Change: 5.955 -> 4.909
2024-12-02-13:23:31-root-INFO: Regularization Change: 0.000 -> 2.277
2024-12-02-13:23:31-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-13:23:31-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-13:23:31-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-13:23:31-root-INFO: grad norm: 1.467 1.444 0.256
2024-12-02-13:23:32-root-INFO: grad norm: 1.314 1.308 0.130
2024-12-02-13:23:33-root-INFO: grad norm: 0.833 0.830 0.070
2024-12-02-13:23:34-root-INFO: grad norm: 0.762 0.757 0.081
2024-12-02-13:23:35-root-INFO: grad norm: 0.663 0.660 0.067
2024-12-02-13:23:36-root-INFO: grad norm: 0.604 0.600 0.069
2024-12-02-13:23:37-root-INFO: grad norm: 0.613 0.610 0.055
2024-12-02-13:23:38-root-INFO: grad norm: 0.726 0.722 0.075
2024-12-02-13:23:39-root-INFO: Loss Change: 4.858 -> 4.124
2024-12-02-13:23:39-root-INFO: Regularization Change: 0.000 -> 1.671
2024-12-02-13:23:39-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-13:23:39-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-13:23:39-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-13:23:40-root-INFO: grad norm: 1.986 1.962 0.308
2024-12-02-13:23:41-root-INFO: grad norm: 0.974 0.965 0.134
2024-12-02-13:23:42-root-INFO: grad norm: 0.706 0.703 0.065
2024-12-02-13:23:43-root-INFO: grad norm: 0.627 0.624 0.067
2024-12-02-13:23:44-root-INFO: grad norm: 0.561 0.559 0.048
2024-12-02-13:23:45-root-INFO: grad norm: 0.505 0.503 0.047
2024-12-02-13:23:46-root-INFO: grad norm: 0.446 0.444 0.038
2024-12-02-13:23:47-root-INFO: grad norm: 0.446 0.444 0.044
2024-12-02-13:23:47-root-INFO: Loss Change: 4.195 -> 3.469
2024-12-02-13:23:47-root-INFO: Regularization Change: 0.000 -> 1.287
2024-12-02-13:23:47-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-13:23:47-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-13:23:48-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-13:23:48-root-INFO: grad norm: 1.364 1.342 0.248
2024-12-02-13:23:49-root-INFO: grad norm: 0.781 0.775 0.101
2024-12-02-13:23:50-root-INFO: grad norm: 0.613 0.611 0.050
2024-12-02-13:23:51-root-INFO: grad norm: 0.692 0.691 0.041
2024-12-02-13:23:52-root-INFO: grad norm: 0.616 0.614 0.051
2024-12-02-13:23:53-root-INFO: grad norm: 0.484 0.483 0.034
2024-12-02-13:23:54-root-INFO: grad norm: 0.602 0.600 0.051
2024-12-02-13:23:55-root-INFO: grad norm: 1.020 1.019 0.053
2024-12-02-13:23:55-root-INFO: Loss too large (3.081->3.086)! Learning rate decreased to 0.26654.
2024-12-02-13:23:56-root-INFO: Loss Change: 3.521 -> 3.066
2024-12-02-13:23:56-root-INFO: Regularization Change: 0.000 -> 1.033
2024-12-02-13:23:56-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-13:23:56-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-13:23:56-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-13:23:57-root-INFO: grad norm: 1.164 1.144 0.214
2024-12-02-13:23:58-root-INFO: grad norm: 0.552 0.549 0.057
2024-12-02-13:23:59-root-INFO: grad norm: 0.435 0.434 0.036
2024-12-02-13:24:00-root-INFO: grad norm: 0.589 0.588 0.034
2024-12-02-13:24:01-root-INFO: grad norm: 0.643 0.641 0.056
2024-12-02-13:24:02-root-INFO: grad norm: 0.702 0.700 0.043
2024-12-02-13:24:02-root-INFO: Loss too large (2.759->2.760)! Learning rate decreased to 0.27030.
2024-12-02-13:24:03-root-INFO: grad norm: 0.624 0.622 0.056
2024-12-02-13:24:04-root-INFO: grad norm: 0.532 0.531 0.036
2024-12-02-13:24:04-root-INFO: Loss Change: 3.131 -> 2.680
2024-12-02-13:24:04-root-INFO: Regularization Change: 0.000 -> 0.779
2024-12-02-13:24:04-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-13:24:04-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-13:24:05-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-13:24:05-root-INFO: grad norm: 1.138 1.116 0.224
2024-12-02-13:24:06-root-INFO: grad norm: 0.715 0.713 0.050
2024-12-02-13:24:07-root-INFO: grad norm: 0.671 0.669 0.056
2024-12-02-13:24:08-root-INFO: grad norm: 0.662 0.661 0.043
2024-12-02-13:24:09-root-INFO: grad norm: 0.659 0.656 0.062
2024-12-02-13:24:10-root-INFO: grad norm: 0.652 0.651 0.045
2024-12-02-13:24:11-root-INFO: grad norm: 0.650 0.647 0.064
2024-12-02-13:24:12-root-INFO: grad norm: 0.634 0.633 0.045
2024-12-02-13:24:13-root-INFO: Loss Change: 2.795 -> 2.417
2024-12-02-13:24:13-root-INFO: Regularization Change: 0.000 -> 0.806
2024-12-02-13:24:13-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-13:24:13-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-13:24:13-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-13:24:13-root-INFO: grad norm: 1.084 1.068 0.181
2024-12-02-13:24:14-root-INFO: grad norm: 0.679 0.677 0.040
2024-12-02-13:24:15-root-INFO: grad norm: 0.655 0.653 0.056
2024-12-02-13:24:16-root-INFO: grad norm: 0.655 0.654 0.045
2024-12-02-13:24:17-root-INFO: grad norm: 0.652 0.649 0.063
2024-12-02-13:24:18-root-INFO: grad norm: 0.641 0.639 0.047
2024-12-02-13:24:19-root-INFO: grad norm: 0.637 0.634 0.063
2024-12-02-13:24:20-root-INFO: grad norm: 0.627 0.625 0.046
2024-12-02-13:24:21-root-INFO: Loss Change: 2.520 -> 2.182
2024-12-02-13:24:21-root-INFO: Regularization Change: 0.000 -> 0.687
2024-12-02-13:24:21-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-13:24:21-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-13:24:21-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-13:24:22-root-INFO: grad norm: 1.048 1.032 0.181
2024-12-02-13:24:23-root-INFO: grad norm: 0.556 0.555 0.037
2024-12-02-13:24:24-root-INFO: grad norm: 0.532 0.530 0.044
2024-12-02-13:24:25-root-INFO: grad norm: 0.550 0.548 0.041
2024-12-02-13:24:26-root-INFO: grad norm: 0.565 0.562 0.053
2024-12-02-13:24:27-root-INFO: grad norm: 0.565 0.563 0.043
2024-12-02-13:24:28-root-INFO: grad norm: 0.559 0.557 0.055
2024-12-02-13:24:29-root-INFO: grad norm: 0.549 0.547 0.043
2024-12-02-13:24:29-root-INFO: Loss Change: 2.319 -> 1.991
2024-12-02-13:24:29-root-INFO: Regularization Change: 0.000 -> 0.638
2024-12-02-13:24:29-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-13:24:29-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-13:24:30-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-13:24:30-root-INFO: grad norm: 0.987 0.970 0.182
2024-12-02-13:24:31-root-INFO: grad norm: 0.430 0.429 0.035
2024-12-02-13:24:32-root-INFO: grad norm: 0.361 0.360 0.033
2024-12-02-13:24:33-root-INFO: grad norm: 0.350 0.348 0.034
2024-12-02-13:24:34-root-INFO: grad norm: 0.352 0.350 0.033
2024-12-02-13:24:35-root-INFO: grad norm: 0.355 0.353 0.033
2024-12-02-13:24:36-root-INFO: grad norm: 0.362 0.360 0.035
2024-12-02-13:24:37-root-INFO: grad norm: 0.365 0.363 0.033
2024-12-02-13:24:38-root-INFO: Loss Change: 2.145 -> 1.825
2024-12-02-13:24:38-root-INFO: Regularization Change: 0.000 -> 0.607
2024-12-02-13:24:38-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-13:24:38-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-13:24:38-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-13:24:38-root-INFO: grad norm: 0.895 0.879 0.170
2024-12-02-13:24:39-root-INFO: grad norm: 0.346 0.344 0.034
2024-12-02-13:24:40-root-INFO: grad norm: 0.265 0.264 0.031
2024-12-02-13:24:41-root-INFO: grad norm: 0.240 0.238 0.029
2024-12-02-13:24:42-root-INFO: grad norm: 0.225 0.223 0.028
2024-12-02-13:24:43-root-INFO: grad norm: 0.215 0.213 0.028
2024-12-02-13:24:44-root-INFO: grad norm: 0.207 0.205 0.027
2024-12-02-13:24:45-root-INFO: grad norm: 0.200 0.198 0.027
2024-12-02-13:24:46-root-INFO: Loss Change: 1.986 -> 1.688
2024-12-02-13:24:46-root-INFO: Regularization Change: 0.000 -> 0.572
2024-12-02-13:24:46-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-13:24:46-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-13:24:46-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-13:24:46-root-INFO: grad norm: 0.901 0.884 0.175
2024-12-02-13:24:47-root-INFO: grad norm: 0.363 0.361 0.041
2024-12-02-13:24:48-root-INFO: grad norm: 0.274 0.271 0.036
2024-12-02-13:24:49-root-INFO: grad norm: 0.239 0.236 0.033
2024-12-02-13:24:50-root-INFO: grad norm: 0.219 0.216 0.032
2024-12-02-13:24:51-root-INFO: grad norm: 0.205 0.203 0.031
2024-12-02-13:24:52-root-INFO: grad norm: 0.195 0.193 0.029
2024-12-02-13:24:53-root-INFO: grad norm: 0.187 0.185 0.029
2024-12-02-13:24:54-root-INFO: Loss Change: 1.872 -> 1.576
2024-12-02-13:24:54-root-INFO: Regularization Change: 0.000 -> 0.562
2024-12-02-13:24:54-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-13:24:54-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-13:24:54-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-13:24:55-root-INFO: grad norm: 0.827 0.814 0.147
2024-12-02-13:24:56-root-INFO: grad norm: 0.345 0.342 0.042
2024-12-02-13:24:57-root-INFO: grad norm: 0.254 0.252 0.035
2024-12-02-13:24:58-root-INFO: grad norm: 0.223 0.220 0.034
2024-12-02-13:24:59-root-INFO: grad norm: 0.205 0.203 0.033
2024-12-02-13:25:00-root-INFO: grad norm: 0.193 0.191 0.032
2024-12-02-13:25:01-root-INFO: grad norm: 0.184 0.182 0.031
2024-12-02-13:25:01-root-INFO: grad norm: 0.177 0.174 0.030
2024-12-02-13:25:02-root-INFO: Loss Change: 1.747 -> 1.476
2024-12-02-13:25:02-root-INFO: Regularization Change: 0.000 -> 0.536
2024-12-02-13:25:02-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-13:25:02-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-13:25:03-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-13:25:03-root-INFO: grad norm: 0.802 0.792 0.130
2024-12-02-13:25:04-root-INFO: grad norm: 0.342 0.339 0.043
2024-12-02-13:25:05-root-INFO: grad norm: 0.248 0.245 0.038
2024-12-02-13:25:06-root-INFO: grad norm: 0.219 0.216 0.037
2024-12-02-13:25:07-root-INFO: grad norm: 0.202 0.199 0.035
2024-12-02-13:25:08-root-INFO: grad norm: 0.190 0.187 0.034
2024-12-02-13:25:09-root-INFO: grad norm: 0.179 0.176 0.033
2024-12-02-13:25:10-root-INFO: grad norm: 0.171 0.168 0.032
2024-12-02-13:25:10-root-INFO: Loss Change: 1.653 -> 1.388
2024-12-02-13:25:10-root-INFO: Regularization Change: 0.000 -> 0.530
2024-12-02-13:25:10-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-13:25:10-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-13:25:11-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-13:25:11-root-INFO: grad norm: 0.758 0.750 0.113
2024-12-02-13:25:12-root-INFO: grad norm: 0.336 0.333 0.042
2024-12-02-13:25:13-root-INFO: grad norm: 0.302 0.300 0.039
2024-12-02-13:25:14-root-INFO: grad norm: 0.217 0.213 0.038
2024-12-02-13:25:15-root-INFO: grad norm: 0.199 0.196 0.037
2024-12-02-13:25:16-root-INFO: grad norm: 0.194 0.191 0.035
2024-12-02-13:25:17-root-INFO: grad norm: 0.179 0.176 0.034
2024-12-02-13:25:18-root-INFO: grad norm: 0.170 0.167 0.033
2024-12-02-13:25:19-root-INFO: Loss Change: 1.558 -> 1.312
2024-12-02-13:25:19-root-INFO: Regularization Change: 0.000 -> 0.505
2024-12-02-13:25:19-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-13:25:19-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-13:25:19-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-13:25:19-root-INFO: grad norm: 0.737 0.730 0.098
2024-12-02-13:25:20-root-INFO: grad norm: 0.411 0.409 0.037
2024-12-02-13:25:21-root-INFO: grad norm: 0.250 0.247 0.034
2024-12-02-13:25:22-root-INFO: grad norm: 0.225 0.223 0.033
2024-12-02-13:25:23-root-INFO: grad norm: 0.211 0.208 0.032
2024-12-02-13:25:24-root-INFO: grad norm: 0.194 0.192 0.031
2024-12-02-13:25:25-root-INFO: grad norm: 0.244 0.242 0.029
2024-12-02-13:25:26-root-INFO: grad norm: 0.174 0.172 0.028
2024-12-02-13:25:27-root-INFO: Loss Change: 1.458 -> 1.206
2024-12-02-13:25:27-root-INFO: Regularization Change: 0.000 -> 0.556
2024-12-02-13:25:27-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-13:25:27-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-13:25:27-root-INFO: step: 0 lr_xt 0.38992188
2024-12-02-13:25:27-root-INFO: grad norm: 0.783 0.778 0.081
2024-12-02-13:25:29-root-INFO: grad norm: 0.481 0.480 0.030
2024-12-02-13:25:29-root-INFO: grad norm: 0.390 0.388 0.032
2024-12-02-13:25:30-root-INFO: grad norm: 0.346 0.344 0.044
2024-12-02-13:25:31-root-INFO: grad norm: 0.323 0.319 0.050
2024-12-02-13:25:32-root-INFO: grad norm: 0.309 0.304 0.053
2024-12-02-13:25:33-root-INFO: grad norm: 0.300 0.295 0.055
2024-12-02-13:25:34-root-INFO: grad norm: 0.295 0.290 0.055
2024-12-02-13:25:35-root-INFO: Loss Change: 1.351 -> 0.841
2024-12-02-13:25:35-root-INFO: Regularization Change: 0.000 -> 1.332
2024-12-02-13:25:35-root-INFO: Undo step: 0
2024-12-02-13:25:35-root-INFO: Undo step: 1
2024-12-02-13:25:35-root-INFO: Undo step: 2
2024-12-02-13:25:35-root-INFO: Undo step: 3
2024-12-02-13:25:35-root-INFO: Undo step: 4
2024-12-02-13:25:35-root-INFO: Undo step: 5
2024-12-02-13:25:35-root-INFO: Undo step: 6
2024-12-02-13:25:35-root-INFO: Undo step: 7
2024-12-02-13:25:35-root-INFO: Undo step: 8
2024-12-02-13:25:35-root-INFO: Undo step: 9
2024-12-02-13:25:36-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-13:25:36-root-INFO: grad norm: 15.912 15.889 0.863
2024-12-02-13:25:37-root-INFO: grad norm: 7.560 7.541 0.535
2024-12-02-13:25:38-root-INFO: grad norm: 4.868 4.855 0.345
2024-12-02-13:25:39-root-INFO: grad norm: 3.694 3.683 0.289
2024-12-02-13:25:40-root-INFO: grad norm: 2.888 2.880 0.224
2024-12-02-13:25:41-root-INFO: grad norm: 2.716 2.709 0.203
2024-12-02-13:25:42-root-INFO: grad norm: 2.110 2.103 0.171
2024-12-02-13:25:43-root-INFO: grad norm: 1.873 1.867 0.156
2024-12-02-13:25:43-root-INFO: Loss Change: 93.118 -> 10.582
2024-12-02-13:25:43-root-INFO: Regularization Change: 0.000 -> 136.685
2024-12-02-13:25:43-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-13:25:43-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-13:25:44-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-13:25:44-root-INFO: grad norm: 1.941 1.934 0.170
2024-12-02-13:25:45-root-INFO: grad norm: 1.566 1.561 0.120
2024-12-02-13:25:46-root-INFO: grad norm: 1.396 1.392 0.102
2024-12-02-13:25:47-root-INFO: grad norm: 1.275 1.271 0.096
2024-12-02-13:25:48-root-INFO: grad norm: 1.289 1.286 0.090
2024-12-02-13:25:49-root-INFO: grad norm: 1.080 1.077 0.082
2024-12-02-13:25:50-root-INFO: grad norm: 0.995 0.992 0.077
2024-12-02-13:25:51-root-INFO: grad norm: 0.930 0.927 0.073
2024-12-02-13:25:52-root-INFO: Loss Change: 10.114 -> 5.492
2024-12-02-13:25:52-root-INFO: Regularization Change: 0.000 -> 12.082
2024-12-02-13:25:52-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-13:25:52-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-13:25:52-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-13:25:52-root-INFO: grad norm: 1.222 1.212 0.158
2024-12-02-13:25:53-root-INFO: grad norm: 0.899 0.897 0.064
2024-12-02-13:25:54-root-INFO: grad norm: 0.816 0.815 0.055
2024-12-02-13:25:55-root-INFO: grad norm: 0.763 0.761 0.054
2024-12-02-13:25:56-root-INFO: grad norm: 0.718 0.716 0.052
2024-12-02-13:25:57-root-INFO: grad norm: 0.679 0.677 0.050
2024-12-02-13:25:58-root-INFO: grad norm: 0.644 0.642 0.047
2024-12-02-13:25:59-root-INFO: grad norm: 0.608 0.607 0.045
2024-12-02-13:26:00-root-INFO: Loss Change: 5.280 -> 3.584
2024-12-02-13:26:00-root-INFO: Regularization Change: 0.000 -> 4.510
2024-12-02-13:26:00-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-13:26:00-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-13:26:00-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-13:26:01-root-INFO: grad norm: 1.038 1.024 0.171
2024-12-02-13:26:02-root-INFO: grad norm: 0.729 0.726 0.060
2024-12-02-13:26:02-root-INFO: grad norm: 0.759 0.758 0.051
2024-12-02-13:26:03-root-INFO: grad norm: 0.727 0.724 0.069
2024-12-02-13:26:04-root-INFO: grad norm: 0.694 0.693 0.047
2024-12-02-13:26:05-root-INFO: grad norm: 0.745 0.741 0.076
2024-12-02-13:26:06-root-INFO: grad norm: 0.764 0.762 0.055
2024-12-02-13:26:07-root-INFO: grad norm: 0.769 0.764 0.085
2024-12-02-13:26:08-root-INFO: Loss Change: 3.518 -> 2.678
2024-12-02-13:26:08-root-INFO: Regularization Change: 0.000 -> 2.283
2024-12-02-13:26:08-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-13:26:08-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-13:26:09-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-13:26:09-root-INFO: grad norm: 1.160 1.145 0.187
2024-12-02-13:26:10-root-INFO: grad norm: 0.873 0.869 0.078
2024-12-02-13:26:11-root-INFO: grad norm: 0.680 0.679 0.047
2024-12-02-13:26:12-root-INFO: grad norm: 0.568 0.565 0.051
2024-12-02-13:26:13-root-INFO: grad norm: 0.541 0.539 0.039
2024-12-02-13:26:14-root-INFO: grad norm: 0.525 0.523 0.049
2024-12-02-13:26:15-root-INFO: grad norm: 0.511 0.509 0.038
2024-12-02-13:26:16-root-INFO: grad norm: 0.490 0.488 0.047
2024-12-02-13:26:16-root-INFO: Loss Change: 2.727 -> 2.074
2024-12-02-13:26:16-root-INFO: Regularization Change: 0.000 -> 1.432
2024-12-02-13:26:16-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-13:26:16-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-13:26:17-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-13:26:17-root-INFO: grad norm: 0.974 0.958 0.176
2024-12-02-13:26:18-root-INFO: grad norm: 0.464 0.463 0.039
2024-12-02-13:26:19-root-INFO: grad norm: 0.378 0.377 0.032
2024-12-02-13:26:20-root-INFO: grad norm: 0.326 0.325 0.028
2024-12-02-13:26:21-root-INFO: grad norm: 0.304 0.302 0.028
2024-12-02-13:26:22-root-INFO: grad norm: 0.282 0.281 0.027
2024-12-02-13:26:23-root-INFO: grad norm: 0.267 0.266 0.026
2024-12-02-13:26:24-root-INFO: grad norm: 0.257 0.256 0.025
2024-12-02-13:26:25-root-INFO: Loss Change: 2.179 -> 1.753
2024-12-02-13:26:25-root-INFO: Regularization Change: 0.000 -> 0.920
2024-12-02-13:26:25-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-13:26:25-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-13:26:25-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-13:26:25-root-INFO: grad norm: 0.803 0.790 0.142
2024-12-02-13:26:26-root-INFO: grad norm: 0.374 0.372 0.038
2024-12-02-13:26:27-root-INFO: grad norm: 0.293 0.291 0.031
2024-12-02-13:26:28-root-INFO: grad norm: 0.272 0.271 0.029
2024-12-02-13:26:29-root-INFO: grad norm: 0.252 0.251 0.028
2024-12-02-13:26:30-root-INFO: grad norm: 0.235 0.234 0.028
2024-12-02-13:26:31-root-INFO: grad norm: 0.225 0.223 0.027
2024-12-02-13:26:32-root-INFO: grad norm: 0.216 0.214 0.026
2024-12-02-13:26:33-root-INFO: Loss Change: 1.869 -> 1.553
2024-12-02-13:26:33-root-INFO: Regularization Change: 0.000 -> 0.701
2024-12-02-13:26:33-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-13:26:33-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-13:26:33-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-13:26:33-root-INFO: grad norm: 0.783 0.772 0.131
2024-12-02-13:26:34-root-INFO: grad norm: 0.369 0.366 0.043
2024-12-02-13:26:34-root-INFO: grad norm: 0.279 0.277 0.034
2024-12-02-13:26:34-root-INFO: grad norm: 0.245 0.243 0.031
2024-12-02-13:26:35-root-INFO: grad norm: 0.226 0.224 0.030
2024-12-02-13:26:35-root-INFO: grad norm: 0.219 0.217 0.029
2024-12-02-13:26:36-root-INFO: grad norm: 0.205 0.203 0.028
2024-12-02-13:26:36-root-INFO: grad norm: 0.190 0.188 0.027
2024-12-02-13:26:37-root-INFO: Loss Change: 1.691 -> 1.407
2024-12-02-13:26:37-root-INFO: Regularization Change: 0.000 -> 0.611
2024-12-02-13:26:37-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-13:26:37-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-13:26:37-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-13:26:37-root-INFO: grad norm: 0.736 0.726 0.119
2024-12-02-13:26:37-root-INFO: grad norm: 0.342 0.339 0.042
2024-12-02-13:26:38-root-INFO: grad norm: 0.273 0.271 0.034
2024-12-02-13:26:38-root-INFO: grad norm: 0.220 0.218 0.032
2024-12-02-13:26:39-root-INFO: grad norm: 0.231 0.229 0.031
2024-12-02-13:26:39-root-INFO: grad norm: 0.198 0.196 0.030
2024-12-02-13:26:40-root-INFO: grad norm: 0.187 0.185 0.029
2024-12-02-13:26:40-root-INFO: grad norm: 0.179 0.177 0.027
2024-12-02-13:26:40-root-INFO: Loss Change: 1.545 -> 1.304
2024-12-02-13:26:40-root-INFO: Regularization Change: 0.000 -> 0.511
2024-12-02-13:26:40-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-13:26:40-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-13:26:41-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-13:26:41-root-INFO: grad norm: 0.707 0.698 0.114
2024-12-02-13:26:41-root-INFO: grad norm: 0.330 0.328 0.037
2024-12-02-13:26:42-root-INFO: grad norm: 0.247 0.245 0.031
2024-12-02-13:26:42-root-INFO: grad norm: 0.225 0.223 0.028
2024-12-02-13:26:43-root-INFO: grad norm: 0.224 0.222 0.028
2024-12-02-13:26:43-root-INFO: grad norm: 0.194 0.192 0.026
2024-12-02-13:26:44-root-INFO: grad norm: 0.185 0.183 0.025
2024-12-02-13:26:44-root-INFO: grad norm: 0.184 0.183 0.023
2024-12-02-13:26:44-root-INFO: Loss Change: 1.429 -> 1.193
2024-12-02-13:26:44-root-INFO: Regularization Change: 0.000 -> 0.518
2024-12-02-13:26:44-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-13:26:44-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-13:26:45-root-INFO: loss_sample0_0: 1.1929879188537598
2024-12-02-13:26:45-root-INFO: It takes 2436.222 seconds for image sample0
2024-12-02-13:26:45-root-INFO: lpips_score_sample0: 0.143
2024-12-02-13:26:45-root-INFO: psnr_score_sample0: 15.053
2024-12-02-13:26:45-root-INFO: ssim_score_sample0: 0.713
2024-12-02-13:26:45-root-INFO: mean_lpips: 0.14333118498325348
2024-12-02-13:26:45-root-INFO: best_mean_lpips: 0.14333118498325348
2024-12-02-13:26:45-root-INFO: mean_psnr: 15.053372383117676
2024-12-02-13:26:45-root-INFO: best_mean_psnr: 15.053372383117676
2024-12-02-13:26:45-root-INFO: mean_ssim: 0.7134689688682556
2024-12-02-13:26:45-root-INFO: best_mean_ssim: 0.7134689688682556
2024-12-02-13:26:45-root-INFO: final_loss: 1.1929879188537598
2024-12-02-13:26:45-root-INFO: mean time: 2436.221522092819
2024-12-02-13:26:45-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample2_iter8_lr0.02_10009 
 
Enjoy.
