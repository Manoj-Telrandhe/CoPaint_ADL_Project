2024-12-02-13:27:02-root-INFO: Prepare model...
2024-12-02-13:27:18-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-13:27:43-root-INFO: Start sampling
2024-12-02-13:27:48-root-INFO: step: 249 lr_xt 0.00019059
2024-12-02-13:27:48-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-13:27:49-root-INFO: Loss too large (77070.016->78612.734)! Learning rate decreased to 0.00015.
2024-12-02-13:27:49-root-INFO: grad norm: 15661.118 11248.847 10896.517
2024-12-02-13:27:50-root-INFO: grad norm: 14205.968 10378.396 9700.434
2024-12-02-13:27:50-root-INFO: grad norm: 15590.438 11542.587 10480.002
2024-12-02-13:27:50-root-INFO: Loss too large (28301.697->35851.426)! Learning rate decreased to 0.00012.
2024-12-02-13:27:51-root-INFO: grad norm: 16985.219 12431.798 11573.594
2024-12-02-13:27:51-root-INFO: Loss too large (28113.789->29014.088)! Learning rate decreased to 0.00010.
2024-12-02-13:27:51-root-INFO: grad norm: 13500.278 10439.770 8559.717
2024-12-02-13:27:52-root-INFO: grad norm: 13455.127 10629.282 8249.775
2024-12-02-13:27:52-root-INFO: Loss too large (22323.707->22381.811)! Learning rate decreased to 0.00008.
2024-12-02-13:27:52-root-INFO: grad norm: 9590.794 7721.844 5688.273
2024-12-02-13:27:53-root-INFO: Loss Change: 77070.016 -> 18637.451
2024-12-02-13:27:53-root-INFO: Regularization Change: 0.000 -> 18.364
2024-12-02-13:27:53-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-13:27:53-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-13:27:53-root-INFO: step: 248 lr_xt 0.00020082
2024-12-02-13:27:53-root-INFO: grad norm: 7521.752 5981.197 4560.925
2024-12-02-13:27:53-root-INFO: Loss too large (18708.244->30947.867)! Learning rate decreased to 0.00016.
2024-12-02-13:27:53-root-INFO: Loss too large (18708.244->24729.586)! Learning rate decreased to 0.00013.
2024-12-02-13:27:54-root-INFO: Loss too large (18708.244->21087.455)! Learning rate decreased to 0.00010.
2024-12-02-13:27:54-root-INFO: Loss too large (18708.244->19048.562)! Learning rate decreased to 0.00008.
2024-12-02-13:27:54-root-INFO: grad norm: 6005.454 4798.607 3610.934
2024-12-02-13:27:55-root-INFO: grad norm: 4839.267 3873.260 2901.098
2024-12-02-13:27:55-root-INFO: grad norm: 3853.265 3060.955 2340.557
2024-12-02-13:27:56-root-INFO: grad norm: 3089.067 2487.189 1832.002
2024-12-02-13:27:56-root-INFO: grad norm: 2467.746 1955.151 1505.707
2024-12-02-13:27:57-root-INFO: grad norm: 1998.166 1619.276 1170.731
2024-12-02-13:27:57-root-INFO: grad norm: 1631.587 1298.971 987.295
2024-12-02-13:27:57-root-INFO: Loss Change: 18708.244 -> 16490.879
2024-12-02-13:27:57-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-02-13:27:57-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-02-13:27:57-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-13:27:57-root-INFO: step: 247 lr_xt 0.00021156
2024-12-02-13:27:58-root-INFO: grad norm: 1318.054 1091.949 738.182
2024-12-02-13:27:58-root-INFO: Loss too large (16314.121->16376.119)! Learning rate decreased to 0.00017.
2024-12-02-13:27:58-root-INFO: grad norm: 2399.181 1883.211 1486.467
2024-12-02-13:27:58-root-INFO: Loss too large (16294.835->16719.383)! Learning rate decreased to 0.00014.
2024-12-02-13:27:59-root-INFO: Loss too large (16294.835->16416.529)! Learning rate decreased to 0.00011.
2024-12-02-13:27:59-root-INFO: grad norm: 2589.894 2072.370 1553.329
2024-12-02-13:28:00-root-INFO: grad norm: 2843.327 2268.301 1714.445
2024-12-02-13:28:00-root-INFO: grad norm: 3135.494 2504.604 1886.342
2024-12-02-13:28:01-root-INFO: grad norm: 3488.422 2803.708 2075.646
2024-12-02-13:28:01-root-INFO: Loss too large (16180.131->16200.811)! Learning rate decreased to 0.00009.
2024-12-02-13:28:01-root-INFO: grad norm: 2509.871 2006.840 1507.331
2024-12-02-13:28:02-root-INFO: grad norm: 1823.637 1496.704 1041.887
2024-12-02-13:28:02-root-INFO: Loss Change: 16314.121 -> 15773.620
2024-12-02-13:28:02-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-02-13:28:02-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-02-13:28:02-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-13:28:02-root-INFO: step: 246 lr_xt 0.00022285
2024-12-02-13:28:02-root-INFO: grad norm: 1388.456 1137.524 796.147
2024-12-02-13:28:03-root-INFO: Loss too large (15552.649->15703.746)! Learning rate decreased to 0.00018.
2024-12-02-13:28:03-root-INFO: Loss too large (15552.649->15579.586)! Learning rate decreased to 0.00014.
2024-12-02-13:28:03-root-INFO: grad norm: 1924.919 1584.868 1092.477
2024-12-02-13:28:03-root-INFO: Loss too large (15514.373->15563.380)! Learning rate decreased to 0.00011.
2024-12-02-13:28:04-root-INFO: grad norm: 2029.413 1629.900 1209.109
2024-12-02-13:28:04-root-INFO: grad norm: 2162.223 1787.878 1216.019
2024-12-02-13:28:05-root-INFO: grad norm: 2302.316 1840.731 1382.883
2024-12-02-13:28:05-root-INFO: grad norm: 2461.749 2033.644 1387.264
2024-12-02-13:28:06-root-INFO: grad norm: 2626.907 2096.269 1583.128
2024-12-02-13:28:06-root-INFO: grad norm: 2814.565 2319.586 1594.144
2024-12-02-13:28:06-root-INFO: Loss Change: 15552.649 -> 15230.260
2024-12-02-13:28:06-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-13:28:06-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-02-13:28:06-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-13:28:07-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-13:28:07-root-INFO: grad norm: 2959.061 2359.915 1785.173
2024-12-02-13:28:07-root-INFO: Loss too large (15122.979->16866.777)! Learning rate decreased to 0.00019.
2024-12-02-13:28:07-root-INFO: Loss too large (15122.979->15899.428)! Learning rate decreased to 0.00015.
2024-12-02-13:28:07-root-INFO: Loss too large (15122.979->15349.767)! Learning rate decreased to 0.00012.
2024-12-02-13:28:08-root-INFO: grad norm: 3004.631 2477.076 1700.560
2024-12-02-13:28:08-root-INFO: grad norm: 3064.877 2446.540 1846.054
2024-12-02-13:28:09-root-INFO: grad norm: 3138.072 2591.542 1769.579
2024-12-02-13:28:09-root-INFO: grad norm: 3209.723 2563.354 1931.719
2024-12-02-13:28:10-root-INFO: grad norm: 3286.984 2713.974 1854.348
2024-12-02-13:28:10-root-INFO: grad norm: 3361.275 2686.349 2020.321
2024-12-02-13:28:11-root-INFO: grad norm: 3441.012 2838.802 1944.677
2024-12-02-13:28:11-root-INFO: Loss Change: 15122.979 -> 14724.411
2024-12-02-13:28:11-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-13:28:11-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-13:28:11-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-13:28:11-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-13:28:11-root-INFO: grad norm: 3399.088 2754.388 1991.768
2024-12-02-13:28:11-root-INFO: Loss too large (14595.745->16830.020)! Learning rate decreased to 0.00020.
2024-12-02-13:28:12-root-INFO: Loss too large (14595.745->15549.359)! Learning rate decreased to 0.00016.
2024-12-02-13:28:12-root-INFO: Loss too large (14595.745->14826.998)! Learning rate decreased to 0.00013.
2024-12-02-13:28:12-root-INFO: grad norm: 3218.718 2668.235 1800.186
2024-12-02-13:28:13-root-INFO: grad norm: 3130.101 2532.115 1840.088
2024-12-02-13:28:13-root-INFO: grad norm: 3064.657 2557.218 1689.013
2024-12-02-13:28:14-root-INFO: grad norm: 3014.760 2434.404 1778.328
2024-12-02-13:28:14-root-INFO: grad norm: 2970.846 2483.766 1629.978
2024-12-02-13:28:14-root-INFO: grad norm: 2932.284 2367.286 1730.389
2024-12-02-13:28:15-root-INFO: grad norm: 2893.428 2420.755 1584.889
2024-12-02-13:28:15-root-INFO: Loss Change: 14595.745 -> 13887.887
2024-12-02-13:28:15-root-INFO: Regularization Change: 0.000 -> 0.588
2024-12-02-13:28:15-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-13:28:15-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:15-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-13:28:16-root-INFO: grad norm: 2946.892 2367.931 1754.158
2024-12-02-13:28:16-root-INFO: Loss too large (13796.268->15474.664)! Learning rate decreased to 0.00021.
2024-12-02-13:28:16-root-INFO: Loss too large (13796.268->14498.797)! Learning rate decreased to 0.00017.
2024-12-02-13:28:16-root-INFO: Loss too large (13796.268->13949.500)! Learning rate decreased to 0.00013.
2024-12-02-13:28:16-root-INFO: grad norm: 2748.398 2307.579 1492.907
2024-12-02-13:28:17-root-INFO: grad norm: 2609.311 2106.022 1540.511
2024-12-02-13:28:17-root-INFO: grad norm: 2484.766 2098.308 1330.852
2024-12-02-13:28:18-root-INFO: grad norm: 2377.532 1923.165 1397.890
2024-12-02-13:28:18-root-INFO: grad norm: 2276.127 1927.314 1210.874
2024-12-02-13:28:19-root-INFO: grad norm: 2187.712 1773.836 1280.465
2024-12-02-13:28:19-root-INFO: grad norm: 2102.320 1783.565 1112.946
2024-12-02-13:28:20-root-INFO: Loss Change: 13796.268 -> 13043.078
2024-12-02-13:28:20-root-INFO: Regularization Change: 0.000 -> 0.626
2024-12-02-13:28:20-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-13:28:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:20-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-13:28:20-root-INFO: grad norm: 1943.231 1606.599 1093.154
2024-12-02-13:28:20-root-INFO: Loss too large (12825.688->13417.695)! Learning rate decreased to 0.00022.
2024-12-02-13:28:20-root-INFO: Loss too large (12825.688->13036.254)! Learning rate decreased to 0.00018.
2024-12-02-13:28:20-root-INFO: Loss too large (12825.688->12826.354)! Learning rate decreased to 0.00014.
2024-12-02-13:28:21-root-INFO: grad norm: 1777.717 1511.388 935.941
2024-12-02-13:28:21-root-INFO: grad norm: 1650.298 1355.055 941.971
2024-12-02-13:28:22-root-INFO: grad norm: 1541.425 1327.147 784.011
2024-12-02-13:28:22-root-INFO: grad norm: 1447.182 1189.927 823.656
2024-12-02-13:28:23-root-INFO: grad norm: 1363.045 1181.775 679.191
2024-12-02-13:28:23-root-INFO: grad norm: 1287.511 1064.189 724.698
2024-12-02-13:28:24-root-INFO: grad norm: 1217.698 1061.924 595.906
2024-12-02-13:28:24-root-INFO: Loss Change: 12825.688 -> 12147.594
2024-12-02-13:28:24-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-13:28:24-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-13:28:24-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:24-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-13:28:24-root-INFO: grad norm: 1090.672 929.608 570.434
2024-12-02-13:28:25-root-INFO: Loss too large (12037.801->12048.189)! Learning rate decreased to 0.00023.
2024-12-02-13:28:25-root-INFO: grad norm: 1720.874 1473.445 889.026
2024-12-02-13:28:25-root-INFO: Loss too large (11989.456->12138.772)! Learning rate decreased to 0.00018.
2024-12-02-13:28:26-root-INFO: grad norm: 2223.025 1846.619 1237.675
2024-12-02-13:28:26-root-INFO: Loss too large (11976.810->12027.632)! Learning rate decreased to 0.00015.
2024-12-02-13:28:26-root-INFO: grad norm: 1987.389 1697.791 1033.062
2024-12-02-13:28:27-root-INFO: grad norm: 1788.385 1490.811 987.828
2024-12-02-13:28:27-root-INFO: grad norm: 1610.742 1387.899 817.451
2024-12-02-13:28:28-root-INFO: grad norm: 1458.024 1221.993 795.341
2024-12-02-13:28:28-root-INFO: grad norm: 1324.641 1153.034 652.062
2024-12-02-13:28:28-root-INFO: Loss Change: 12037.801 -> 11410.112
2024-12-02-13:28:28-root-INFO: Regularization Change: 0.000 -> 0.833
2024-12-02-13:28:28-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-13:28:28-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:29-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-13:28:29-root-INFO: grad norm: 1253.158 1079.984 635.640
2024-12-02-13:28:29-root-INFO: Loss too large (11225.465->11306.819)! Learning rate decreased to 0.00024.
2024-12-02-13:28:29-root-INFO: grad norm: 2000.108 1718.960 1022.549
2024-12-02-13:28:30-root-INFO: Loss too large (11200.538->11460.598)! Learning rate decreased to 0.00019.
2024-12-02-13:28:30-root-INFO: Loss too large (11200.538->11213.244)! Learning rate decreased to 0.00016.
2024-12-02-13:28:30-root-INFO: grad norm: 1722.693 1449.698 930.616
2024-12-02-13:28:31-root-INFO: grad norm: 1493.567 1299.469 736.289
2024-12-02-13:28:31-root-INFO: grad norm: 1310.993 1111.407 695.326
2024-12-02-13:28:32-root-INFO: grad norm: 1160.557 1022.899 548.242
2024-12-02-13:28:32-root-INFO: grad norm: 1039.711 892.424 533.459
2024-12-02-13:28:33-root-INFO: grad norm: 941.690 841.888 421.907
2024-12-02-13:28:33-root-INFO: Loss Change: 11225.465 -> 10576.677
2024-12-02-13:28:33-root-INFO: Regularization Change: 0.000 -> 0.818
2024-12-02-13:28:33-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-13:28:33-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:33-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-13:28:34-root-INFO: grad norm: 943.349 810.947 481.945
2024-12-02-13:28:34-root-INFO: grad norm: 1771.276 1506.086 932.267
2024-12-02-13:28:34-root-INFO: Loss too large (10456.389->10957.648)! Learning rate decreased to 0.00026.
2024-12-02-13:28:34-root-INFO: Loss too large (10456.389->10620.456)! Learning rate decreased to 0.00020.
2024-12-02-13:28:35-root-INFO: grad norm: 2117.401 1777.816 1150.112
2024-12-02-13:28:35-root-INFO: Loss too large (10434.579->10446.648)! Learning rate decreased to 0.00016.
2024-12-02-13:28:36-root-INFO: grad norm: 1713.319 1475.550 870.755
2024-12-02-13:28:36-root-INFO: grad norm: 1412.420 1200.781 743.677
2024-12-02-13:28:36-root-INFO: grad norm: 1178.341 1036.366 560.743
2024-12-02-13:28:37-root-INFO: grad norm: 1003.181 865.992 506.389
2024-12-02-13:28:37-root-INFO: grad norm: 871.039 784.135 379.264
2024-12-02-13:28:38-root-INFO: Loss Change: 10491.064 -> 9879.994
2024-12-02-13:28:38-root-INFO: Regularization Change: 0.000 -> 0.909
2024-12-02-13:28:38-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-13:28:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:38-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-13:28:38-root-INFO: grad norm: 1033.289 904.011 500.450
2024-12-02-13:28:38-root-INFO: grad norm: 1872.295 1623.511 932.577
2024-12-02-13:28:39-root-INFO: Loss too large (9677.945->10286.239)! Learning rate decreased to 0.00027.
2024-12-02-13:28:39-root-INFO: Loss too large (9677.945->9884.723)! Learning rate decreased to 0.00021.
2024-12-02-13:28:39-root-INFO: grad norm: 2133.438 1808.894 1131.132
2024-12-02-13:28:39-root-INFO: Loss too large (9661.705->9666.404)! Learning rate decreased to 0.00017.
2024-12-02-13:28:40-root-INFO: grad norm: 1612.673 1414.504 774.528
2024-12-02-13:28:40-root-INFO: grad norm: 1259.053 1081.641 644.414
2024-12-02-13:28:41-root-INFO: grad norm: 1005.152 902.474 442.572
2024-12-02-13:28:41-root-INFO: grad norm: 831.511 728.663 400.576
2024-12-02-13:28:42-root-INFO: grad norm: 712.506 657.063 275.559
2024-12-02-13:28:42-root-INFO: Loss Change: 9688.588 -> 9148.803
2024-12-02-13:28:42-root-INFO: Regularization Change: 0.000 -> 0.800
2024-12-02-13:28:42-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-13:28:42-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:42-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-13:28:42-root-INFO: grad norm: 834.649 726.555 410.800
2024-12-02-13:28:43-root-INFO: grad norm: 1542.230 1356.453 733.832
2024-12-02-13:28:43-root-INFO: Loss too large (9085.752->9480.840)! Learning rate decreased to 0.00028.
2024-12-02-13:28:43-root-INFO: Loss too large (9085.752->9207.262)! Learning rate decreased to 0.00023.
2024-12-02-13:28:44-root-INFO: grad norm: 1706.018 1463.329 877.021
2024-12-02-13:28:44-root-INFO: grad norm: 1901.398 1667.509 913.634
2024-12-02-13:28:45-root-INFO: grad norm: 2113.837 1817.410 1079.504
2024-12-02-13:28:45-root-INFO: grad norm: 2350.832 2054.674 1142.246
2024-12-02-13:28:45-root-INFO: Loss too large (9028.510->9049.390)! Learning rate decreased to 0.00018.
2024-12-02-13:28:46-root-INFO: grad norm: 1671.054 1439.216 849.163
2024-12-02-13:28:46-root-INFO: grad norm: 1202.354 1070.801 546.846
2024-12-02-13:28:46-root-INFO: Loss Change: 9095.499 -> 8655.808
2024-12-02-13:28:46-root-INFO: Regularization Change: 0.000 -> 0.810
2024-12-02-13:28:46-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-13:28:46-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:47-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-13:28:47-root-INFO: grad norm: 919.063 810.830 432.703
2024-12-02-13:28:47-root-INFO: Loss too large (8522.947->8578.404)! Learning rate decreased to 0.00030.
2024-12-02-13:28:47-root-INFO: grad norm: 1329.715 1174.381 623.676
2024-12-02-13:28:47-root-INFO: Loss too large (8508.289->8581.017)! Learning rate decreased to 0.00024.
2024-12-02-13:28:48-root-INFO: grad norm: 1413.071 1224.240 705.696
2024-12-02-13:28:48-root-INFO: grad norm: 1502.051 1324.488 708.441
2024-12-02-13:28:49-root-INFO: grad norm: 1593.280 1377.668 800.357
2024-12-02-13:28:49-root-INFO: grad norm: 1689.482 1485.838 804.136
2024-12-02-13:28:50-root-INFO: grad norm: 1782.778 1539.691 898.693
2024-12-02-13:28:50-root-INFO: grad norm: 1876.673 1646.901 899.788
2024-12-02-13:28:51-root-INFO: Loss Change: 8522.947 -> 8303.827
2024-12-02-13:28:51-root-INFO: Regularization Change: 0.000 -> 0.807
2024-12-02-13:28:51-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-13:28:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:28:51-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-13:28:51-root-INFO: grad norm: 2202.839 1913.259 1091.759
2024-12-02-13:28:51-root-INFO: Loss too large (8300.479->9254.140)! Learning rate decreased to 0.00031.
2024-12-02-13:28:51-root-INFO: Loss too large (8300.479->8603.167)! Learning rate decreased to 0.00025.
2024-12-02-13:28:52-root-INFO: grad norm: 2204.010 1944.348 1037.868
2024-12-02-13:28:52-root-INFO: grad norm: 2205.414 1915.383 1093.234
2024-12-02-13:28:53-root-INFO: grad norm: 2205.014 1945.201 1038.401
2024-12-02-13:28:53-root-INFO: grad norm: 2200.852 1912.387 1089.277
2024-12-02-13:28:53-root-INFO: grad norm: 2193.030 1933.805 1034.301
2024-12-02-13:28:54-root-INFO: grad norm: 2179.468 1894.505 1077.466
2024-12-02-13:28:54-root-INFO: grad norm: 2161.377 1905.668 1019.794
2024-12-02-13:28:55-root-INFO: Loss Change: 8300.479 -> 7960.499
2024-12-02-13:28:55-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-13:28:55-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-13:28:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-13:28:55-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-13:28:55-root-INFO: grad norm: 2122.475 1850.573 1039.365
2024-12-02-13:28:55-root-INFO: Loss too large (7912.303->8781.075)! Learning rate decreased to 0.00033.
2024-12-02-13:28:55-root-INFO: Loss too large (7912.303->8169.155)! Learning rate decreased to 0.00026.
2024-12-02-13:28:56-root-INFO: grad norm: 2019.030 1782.107 948.987
2024-12-02-13:28:56-root-INFO: grad norm: 1922.664 1676.688 940.932
2024-12-02-13:28:57-root-INFO: grad norm: 1825.996 1614.928 852.214
2024-12-02-13:28:57-root-INFO: grad norm: 1735.585 1513.502 849.449
2024-12-02-13:28:58-root-INFO: grad norm: 1647.709 1459.557 764.615
2024-12-02-13:28:58-root-INFO: grad norm: 1566.950 1366.693 766.475
2024-12-02-13:28:59-root-INFO: grad norm: 1488.328 1320.455 686.671
2024-12-02-13:28:59-root-INFO: Loss Change: 7912.303 -> 7455.523
2024-12-02-13:28:59-root-INFO: Regularization Change: 0.000 -> 0.619
2024-12-02-13:28:59-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-13:28:59-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:28:59-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-13:28:59-root-INFO: grad norm: 1552.240 1367.488 734.457
2024-12-02-13:28:59-root-INFO: Loss too large (7464.348->7859.245)! Learning rate decreased to 0.00035.
2024-12-02-13:29:00-root-INFO: Loss too large (7464.348->7547.761)! Learning rate decreased to 0.00028.
2024-12-02-13:29:00-root-INFO: grad norm: 1385.083 1230.663 635.549
2024-12-02-13:29:01-root-INFO: grad norm: 1253.167 1099.958 600.433
2024-12-02-13:29:01-root-INFO: grad norm: 1135.799 1014.900 509.918
2024-12-02-13:29:01-root-INFO: grad norm: 1035.984 910.059 495.031
2024-12-02-13:29:02-root-INFO: grad norm: 945.154 848.174 417.034
2024-12-02-13:29:02-root-INFO: grad norm: 865.775 761.911 411.167
2024-12-02-13:29:03-root-INFO: grad norm: 792.957 715.241 342.362
2024-12-02-13:29:03-root-INFO: Loss Change: 7464.348 -> 7060.476
2024-12-02-13:29:03-root-INFO: Regularization Change: 0.000 -> 0.601
2024-12-02-13:29:03-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-13:29:03-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:29:03-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-13:29:04-root-INFO: grad norm: 1169.282 1013.884 582.461
2024-12-02-13:29:04-root-INFO: Loss too large (7004.169->7190.037)! Learning rate decreased to 0.00036.
2024-12-02-13:29:04-root-INFO: Loss too large (7004.169->7022.106)! Learning rate decreased to 0.00029.
2024-12-02-13:29:04-root-INFO: grad norm: 1000.893 905.617 426.197
2024-12-02-13:29:05-root-INFO: grad norm: 887.636 780.185 423.332
2024-12-02-13:29:05-root-INFO: grad norm: 792.569 720.967 329.199
2024-12-02-13:29:06-root-INFO: grad norm: 714.174 632.390 331.855
2024-12-02-13:29:06-root-INFO: grad norm: 646.680 590.852 262.847
2024-12-02-13:29:07-root-INFO: grad norm: 589.147 524.840 267.652
2024-12-02-13:29:07-root-INFO: grad norm: 539.249 495.448 212.888
2024-12-02-13:29:07-root-INFO: Loss Change: 7004.169 -> 6685.037
2024-12-02-13:29:07-root-INFO: Regularization Change: 0.000 -> 0.545
2024-12-02-13:29:07-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-13:29:07-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:29:08-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-13:29:08-root-INFO: grad norm: 634.731 564.968 289.300
2024-12-02-13:29:08-root-INFO: Loss too large (6641.493->6669.161)! Learning rate decreased to 0.00038.
2024-12-02-13:29:08-root-INFO: grad norm: 798.310 723.442 337.536
2024-12-02-13:29:09-root-INFO: Loss too large (6628.470->6632.138)! Learning rate decreased to 0.00030.
2024-12-02-13:29:09-root-INFO: grad norm: 701.845 625.121 319.077
2024-12-02-13:29:09-root-INFO: grad norm: 620.171 566.651 252.029
2024-12-02-13:29:10-root-INFO: grad norm: 551.444 493.070 246.926
2024-12-02-13:29:10-root-INFO: grad norm: 493.810 455.016 191.857
2024-12-02-13:29:11-root-INFO: grad norm: 446.348 401.817 194.344
2024-12-02-13:29:11-root-INFO: grad norm: 406.620 377.978 149.910
2024-12-02-13:29:12-root-INFO: Loss Change: 6641.493 -> 6424.502
2024-12-02-13:29:12-root-INFO: Regularization Change: 0.000 -> 0.498
2024-12-02-13:29:12-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-13:29:12-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:29:12-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-13:29:12-root-INFO: grad norm: 593.055 524.338 277.098
2024-12-02-13:29:12-root-INFO: Loss too large (6363.682->6386.310)! Learning rate decreased to 0.00040.
2024-12-02-13:29:13-root-INFO: grad norm: 727.898 663.443 299.465
2024-12-02-13:29:13-root-INFO: grad norm: 911.160 812.519 412.342
2024-12-02-13:29:13-root-INFO: Loss too large (6348.764->6365.171)! Learning rate decreased to 0.00032.
2024-12-02-13:29:14-root-INFO: grad norm: 771.566 701.010 322.335
2024-12-02-13:29:14-root-INFO: grad norm: 661.028 591.453 295.198
2024-12-02-13:29:15-root-INFO: grad norm: 570.896 521.771 231.686
2024-12-02-13:29:15-root-INFO: grad norm: 498.177 448.079 217.728
2024-12-02-13:29:15-root-INFO: grad norm: 438.882 404.254 170.869
2024-12-02-13:29:16-root-INFO: Loss Change: 6363.682 -> 6163.527
2024-12-02-13:29:16-root-INFO: Regularization Change: 0.000 -> 0.502
2024-12-02-13:29:16-root-INFO: Undo step: 230
2024-12-02-13:29:16-root-INFO: Undo step: 231
2024-12-02-13:29:16-root-INFO: Undo step: 232
2024-12-02-13:29:16-root-INFO: Undo step: 233
2024-12-02-13:29:16-root-INFO: Undo step: 234
2024-12-02-13:29:16-root-INFO: Undo step: 235
2024-12-02-13:29:16-root-INFO: Undo step: 236
2024-12-02-13:29:16-root-INFO: Undo step: 237
2024-12-02-13:29:16-root-INFO: Undo step: 238
2024-12-02-13:29:16-root-INFO: Undo step: 239
2024-12-02-13:29:16-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-13:29:16-root-INFO: grad norm: 19745.178 16742.061 10467.831
2024-12-02-13:29:16-root-INFO: Loss too large (29438.129->54121.285)! Learning rate decreased to 0.00024.
2024-12-02-13:29:16-root-INFO: Loss too large (29438.129->39364.441)! Learning rate decreased to 0.00019.
2024-12-02-13:29:17-root-INFO: grad norm: 14908.328 12234.419 8519.229
2024-12-02-13:29:17-root-INFO: grad norm: 8724.165 6984.454 5227.663
2024-12-02-13:29:18-root-INFO: grad norm: 7151.512 6013.293 3870.973
2024-12-02-13:29:18-root-INFO: grad norm: 6391.683 5236.778 3664.665
2024-12-02-13:29:18-root-INFO: Loss too large (11480.754->11533.832)! Learning rate decreased to 0.00016.
2024-12-02-13:29:19-root-INFO: grad norm: 4293.198 3607.164 2328.071
2024-12-02-13:29:19-root-INFO: grad norm: 2938.380 2446.483 1627.513
2024-12-02-13:29:20-root-INFO: grad norm: 2211.119 1841.653 1223.667
2024-12-02-13:29:20-root-INFO: Loss Change: 29438.129 -> 9196.206
2024-12-02-13:29:20-root-INFO: Regularization Change: 0.000 -> 5.723
2024-12-02-13:29:20-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-13:29:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:29:20-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-13:29:20-root-INFO: grad norm: 1565.410 1338.223 812.198
2024-12-02-13:29:20-root-INFO: Loss too large (9056.979->9353.083)! Learning rate decreased to 0.00026.
2024-12-02-13:29:21-root-INFO: Loss too large (9056.979->9123.512)! Learning rate decreased to 0.00020.
2024-12-02-13:29:21-root-INFO: grad norm: 1764.396 1472.714 971.702
2024-12-02-13:29:22-root-INFO: grad norm: 2025.780 1727.652 1057.829
2024-12-02-13:29:22-root-INFO: grad norm: 2334.603 1950.660 1282.691
2024-12-02-13:29:22-root-INFO: Loss too large (8954.583->8966.574)! Learning rate decreased to 0.00016.
2024-12-02-13:29:23-root-INFO: grad norm: 1755.735 1501.838 909.443
2024-12-02-13:29:23-root-INFO: grad norm: 1353.606 1140.143 729.605
2024-12-02-13:29:24-root-INFO: grad norm: 1060.404 925.842 516.984
2024-12-02-13:29:24-root-INFO: grad norm: 859.917 738.625 440.329
2024-12-02-13:29:24-root-INFO: Loss Change: 9056.979 -> 8465.324
2024-12-02-13:29:24-root-INFO: Regularization Change: 0.000 -> 0.739
2024-12-02-13:29:24-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-13:29:24-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:29:25-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-13:29:25-root-INFO: grad norm: 796.906 660.842 445.362
2024-12-02-13:29:25-root-INFO: grad norm: 883.567 705.623 531.777
2024-12-02-13:29:25-root-INFO: Loss too large (8180.297->8187.513)! Learning rate decreased to 0.00027.
2024-12-02-13:29:26-root-INFO: grad norm: 1114.189 900.724 655.831
2024-12-02-13:29:26-root-INFO: Loss too large (8143.130->8146.969)! Learning rate decreased to 0.00021.
2024-12-02-13:29:26-root-INFO: grad norm: 1074.824 874.610 624.743
2024-12-02-13:29:27-root-INFO: grad norm: 1081.783 915.817 575.790
2024-12-02-13:29:27-root-INFO: grad norm: 1122.686 942.922 609.363
2024-12-02-13:29:28-root-INFO: grad norm: 1192.920 1028.704 604.008
2024-12-02-13:29:28-root-INFO: grad norm: 1280.795 1089.026 674.135
2024-12-02-13:29:29-root-INFO: Loss Change: 8270.983 -> 7929.989
2024-12-02-13:29:29-root-INFO: Regularization Change: 0.000 -> 0.770
2024-12-02-13:29:29-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-13:29:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:29:29-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-13:29:29-root-INFO: grad norm: 1003.398 874.645 491.734
2024-12-02-13:29:29-root-INFO: Loss too large (7811.326->7922.563)! Learning rate decreased to 0.00028.
2024-12-02-13:29:29-root-INFO: Loss too large (7811.326->7826.304)! Learning rate decreased to 0.00023.
2024-12-02-13:29:30-root-INFO: grad norm: 1035.789 895.553 520.426
2024-12-02-13:29:30-root-INFO: grad norm: 1092.027 961.997 516.803
2024-12-02-13:29:31-root-INFO: grad norm: 1153.128 996.285 580.619
2024-12-02-13:29:31-root-INFO: grad norm: 1222.263 1071.948 587.242
2024-12-02-13:29:32-root-INFO: grad norm: 1292.189 1115.970 651.432
2024-12-02-13:29:32-root-INFO: grad norm: 1367.783 1195.515 664.510
2024-12-02-13:29:32-root-INFO: grad norm: 1444.243 1247.217 728.209
2024-12-02-13:29:33-root-INFO: Loss Change: 7811.326 -> 7615.591
2024-12-02-13:29:33-root-INFO: Regularization Change: 0.000 -> 0.488
2024-12-02-13:29:33-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-13:29:33-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:29:33-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-13:29:33-root-INFO: grad norm: 1398.394 1210.061 700.897
2024-12-02-13:29:33-root-INFO: Loss too large (7462.372->7801.938)! Learning rate decreased to 0.00030.
2024-12-02-13:29:33-root-INFO: Loss too large (7462.372->7566.564)! Learning rate decreased to 0.00024.
2024-12-02-13:29:34-root-INFO: grad norm: 1430.844 1243.214 708.331
2024-12-02-13:29:34-root-INFO: grad norm: 1469.544 1280.680 720.706
2024-12-02-13:29:35-root-INFO: grad norm: 1506.642 1307.832 748.028
2024-12-02-13:29:35-root-INFO: grad norm: 1544.927 1346.576 757.321
2024-12-02-13:29:36-root-INFO: grad norm: 1579.288 1370.349 785.044
2024-12-02-13:29:36-root-INFO: grad norm: 1613.660 1406.587 790.831
2024-12-02-13:29:37-root-INFO: grad norm: 1644.644 1426.789 818.000
2024-12-02-13:29:37-root-INFO: Loss Change: 7462.372 -> 7280.340
2024-12-02-13:29:37-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-02-13:29:37-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-13:29:37-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-13:29:37-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-13:29:37-root-INFO: grad norm: 1188.565 1039.159 576.920
2024-12-02-13:29:37-root-INFO: Loss too large (7124.459->7356.239)! Learning rate decreased to 0.00031.
2024-12-02-13:29:38-root-INFO: Loss too large (7124.459->7186.693)! Learning rate decreased to 0.00025.
2024-12-02-13:29:38-root-INFO: grad norm: 1164.217 1018.697 563.611
2024-12-02-13:29:38-root-INFO: grad norm: 1151.890 1018.180 538.663
2024-12-02-13:29:39-root-INFO: grad norm: 1141.228 999.856 550.172
2024-12-02-13:29:39-root-INFO: grad norm: 1130.146 999.241 527.964
2024-12-02-13:29:40-root-INFO: grad norm: 1119.617 981.623 538.477
2024-12-02-13:29:40-root-INFO: grad norm: 1109.478 981.001 518.245
2024-12-02-13:29:41-root-INFO: grad norm: 1099.985 964.895 528.152
2024-12-02-13:29:41-root-INFO: Loss Change: 7124.459 -> 6909.239
2024-12-02-13:29:41-root-INFO: Regularization Change: 0.000 -> 0.404
2024-12-02-13:29:41-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-13:29:41-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-13:29:41-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-13:29:41-root-INFO: grad norm: 961.715 847.467 454.638
2024-12-02-13:29:42-root-INFO: Loss too large (6844.782->6982.283)! Learning rate decreased to 0.00033.
2024-12-02-13:29:42-root-INFO: Loss too large (6844.782->6873.391)! Learning rate decreased to 0.00026.
2024-12-02-13:29:42-root-INFO: grad norm: 923.222 814.937 433.840
2024-12-02-13:29:43-root-INFO: grad norm: 891.469 791.459 410.256
2024-12-02-13:29:43-root-INFO: grad norm: 861.772 761.786 402.904
2024-12-02-13:29:44-root-INFO: grad norm: 833.137 741.177 380.491
2024-12-02-13:29:44-root-INFO: grad norm: 805.800 713.113 375.212
2024-12-02-13:29:45-root-INFO: grad norm: 778.393 693.763 352.972
2024-12-02-13:29:45-root-INFO: grad norm: 752.953 667.140 349.088
2024-12-02-13:29:45-root-INFO: Loss Change: 6844.782 -> 6633.756
2024-12-02-13:29:45-root-INFO: Regularization Change: 0.000 -> 0.391
2024-12-02-13:29:45-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-13:29:45-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:29:46-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-13:29:46-root-INFO: grad norm: 611.732 516.848 327.238
2024-12-02-13:29:46-root-INFO: grad norm: 941.300 812.251 475.704
2024-12-02-13:29:46-root-INFO: Loss too large (6605.172->6728.263)! Learning rate decreased to 0.00035.
2024-12-02-13:29:47-root-INFO: Loss too large (6605.172->6622.137)! Learning rate decreased to 0.00028.
2024-12-02-13:29:47-root-INFO: grad norm: 848.697 748.972 399.159
2024-12-02-13:29:48-root-INFO: grad norm: 788.822 699.090 365.395
2024-12-02-13:29:48-root-INFO: grad norm: 737.985 657.677 334.787
2024-12-02-13:29:49-root-INFO: grad norm: 693.650 617.428 316.121
2024-12-02-13:29:49-root-INFO: grad norm: 652.858 584.503 290.826
2024-12-02-13:29:49-root-INFO: grad norm: 616.085 550.141 277.319
2024-12-02-13:29:50-root-INFO: Loss Change: 6615.194 -> 6409.825
2024-12-02-13:29:50-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-02-13:29:50-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-13:29:50-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:29:50-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-13:29:50-root-INFO: grad norm: 425.444 359.281 227.861
2024-12-02-13:29:51-root-INFO: grad norm: 383.239 320.479 210.156
2024-12-02-13:29:51-root-INFO: grad norm: 405.625 337.712 224.682
2024-12-02-13:29:51-root-INFO: grad norm: 489.265 395.329 288.262
2024-12-02-13:29:52-root-INFO: grad norm: 673.507 546.890 393.094
2024-12-02-13:29:52-root-INFO: Loss too large (6161.596->6186.819)! Learning rate decreased to 0.00036.
2024-12-02-13:29:53-root-INFO: grad norm: 749.499 631.909 403.038
2024-12-02-13:29:53-root-INFO: grad norm: 917.532 794.350 459.208
2024-12-02-13:29:53-root-INFO: Loss too large (6143.212->6165.077)! Learning rate decreased to 0.00029.
2024-12-02-13:29:54-root-INFO: grad norm: 806.610 715.515 372.367
2024-12-02-13:29:54-root-INFO: Loss Change: 6276.735 -> 6082.920
2024-12-02-13:29:54-root-INFO: Regularization Change: 0.000 -> 0.690
2024-12-02-13:29:54-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-13:29:54-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:29:54-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-13:29:54-root-INFO: grad norm: 869.769 778.350 388.162
2024-12-02-13:29:55-root-INFO: Loss too large (6054.664->6183.987)! Learning rate decreased to 0.00038.
2024-12-02-13:29:55-root-INFO: Loss too large (6054.664->6078.776)! Learning rate decreased to 0.00030.
2024-12-02-13:29:55-root-INFO: grad norm: 785.760 705.445 346.072
2024-12-02-13:29:56-root-INFO: grad norm: 716.972 642.341 318.508
2024-12-02-13:29:56-root-INFO: grad norm: 652.780 588.966 281.499
2024-12-02-13:29:57-root-INFO: grad norm: 597.312 536.132 263.333
2024-12-02-13:29:57-root-INFO: grad norm: 546.785 495.726 230.714
2024-12-02-13:29:57-root-INFO: grad norm: 502.405 452.387 218.532
2024-12-02-13:29:58-root-INFO: grad norm: 462.749 421.847 190.216
2024-12-02-13:29:58-root-INFO: Loss Change: 6054.664 -> 5868.761
2024-12-02-13:29:58-root-INFO: Regularization Change: 0.000 -> 0.347
2024-12-02-13:29:58-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-13:29:58-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:29:59-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-13:29:59-root-INFO: grad norm: 679.303 608.613 301.733
2024-12-02-13:29:59-root-INFO: Loss too large (5817.519->5883.295)! Learning rate decreased to 0.00040.
2024-12-02-13:29:59-root-INFO: Loss too large (5817.519->5822.127)! Learning rate decreased to 0.00032.
2024-12-02-13:29:59-root-INFO: grad norm: 602.241 546.196 253.699
2024-12-02-13:30:00-root-INFO: grad norm: 542.345 487.170 238.335
2024-12-02-13:30:00-root-INFO: grad norm: 489.895 446.300 202.023
2024-12-02-13:30:01-root-INFO: grad norm: 445.481 402.106 191.740
2024-12-02-13:30:01-root-INFO: grad norm: 406.510 372.371 163.067
2024-12-02-13:30:02-root-INFO: grad norm: 373.797 339.309 156.824
2024-12-02-13:30:02-root-INFO: grad norm: 345.963 318.957 134.003
2024-12-02-13:30:03-root-INFO: Loss Change: 5817.519 -> 5654.946
2024-12-02-13:30:03-root-INFO: Regularization Change: 0.000 -> 0.344
2024-12-02-13:30:03-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-13:30:03-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:30:03-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-13:30:03-root-INFO: grad norm: 655.773 590.945 284.293
2024-12-02-13:30:03-root-INFO: Loss too large (5645.678->5694.525)! Learning rate decreased to 0.00042.
2024-12-02-13:30:04-root-INFO: grad norm: 835.738 758.070 351.835
2024-12-02-13:30:04-root-INFO: Loss too large (5640.077->5663.847)! Learning rate decreased to 0.00034.
2024-12-02-13:30:04-root-INFO: grad norm: 744.381 669.978 324.396
2024-12-02-13:30:05-root-INFO: grad norm: 663.110 602.429 277.115
2024-12-02-13:30:05-root-INFO: grad norm: 595.134 536.581 257.422
2024-12-02-13:30:06-root-INFO: grad norm: 534.822 487.288 220.421
2024-12-02-13:30:06-root-INFO: grad norm: 483.570 437.255 206.513
2024-12-02-13:30:06-root-INFO: grad norm: 438.220 400.915 176.928
2024-12-02-13:30:07-root-INFO: Loss Change: 5645.678 -> 5474.498
2024-12-02-13:30:07-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-02-13:30:07-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-13:30:07-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:30:07-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-13:30:07-root-INFO: grad norm: 578.802 516.358 261.507
2024-12-02-13:30:07-root-INFO: Loss too large (5469.457->5508.763)! Learning rate decreased to 0.00044.
2024-12-02-13:30:08-root-INFO: grad norm: 716.064 647.099 306.612
2024-12-02-13:30:08-root-INFO: Loss too large (5464.677->5477.665)! Learning rate decreased to 0.00035.
2024-12-02-13:30:09-root-INFO: grad norm: 619.581 559.864 265.391
2024-12-02-13:30:09-root-INFO: grad norm: 542.216 494.074 223.359
2024-12-02-13:30:09-root-INFO: grad norm: 479.225 434.038 203.144
2024-12-02-13:30:10-root-INFO: grad norm: 425.152 389.511 170.397
2024-12-02-13:30:10-root-INFO: grad norm: 381.882 347.116 159.200
2024-12-02-13:30:11-root-INFO: grad norm: 345.759 318.799 133.853
2024-12-02-13:30:11-root-INFO: Loss Change: 5469.457 -> 5320.915
2024-12-02-13:30:11-root-INFO: Regularization Change: 0.000 -> 0.370
2024-12-02-13:30:11-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-13:30:11-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:30:12-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-13:30:12-root-INFO: grad norm: 740.828 661.666 333.204
2024-12-02-13:30:12-root-INFO: Loss too large (5372.999->5446.467)! Learning rate decreased to 0.00046.
2024-12-02-13:30:12-root-INFO: grad norm: 918.910 836.080 381.270
2024-12-02-13:30:13-root-INFO: Loss too large (5367.998->5401.069)! Learning rate decreased to 0.00037.
2024-12-02-13:30:13-root-INFO: grad norm: 787.952 711.502 338.575
2024-12-02-13:30:14-root-INFO: grad norm: 675.633 615.917 277.717
2024-12-02-13:30:14-root-INFO: grad norm: 587.215 531.285 250.116
2024-12-02-13:30:15-root-INFO: grad norm: 510.989 467.106 207.174
2024-12-02-13:30:15-root-INFO: grad norm: 449.602 408.150 188.561
2024-12-02-13:30:16-root-INFO: grad norm: 398.149 365.652 157.549
2024-12-02-13:30:16-root-INFO: Loss Change: 5372.999 -> 5181.613
2024-12-02-13:30:16-root-INFO: Regularization Change: 0.000 -> 0.456
2024-12-02-13:30:16-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-13:30:16-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-13:30:16-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-13:30:17-root-INFO: grad norm: 394.906 344.124 193.725
2024-12-02-13:30:17-root-INFO: grad norm: 568.768 507.999 255.802
2024-12-02-13:30:17-root-INFO: Loss too large (5121.341->5162.875)! Learning rate decreased to 0.00049.
2024-12-02-13:30:18-root-INFO: grad norm: 678.793 609.872 298.022
2024-12-02-13:30:18-root-INFO: Loss too large (5116.640->5124.905)! Learning rate decreased to 0.00039.
2024-12-02-13:30:18-root-INFO: grad norm: 563.186 513.676 230.902
2024-12-02-13:30:19-root-INFO: grad norm: 477.380 432.166 202.791
2024-12-02-13:30:20-root-INFO: grad norm: 408.103 375.112 160.746
2024-12-02-13:30:20-root-INFO: grad norm: 354.097 321.772 147.810
2024-12-02-13:30:20-root-INFO: grad norm: 310.879 287.985 117.091
2024-12-02-13:30:21-root-INFO: Loss Change: 5129.977 -> 4991.013
2024-12-02-13:30:21-root-INFO: Regularization Change: 0.000 -> 0.446
2024-12-02-13:30:21-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-13:30:21-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:30:21-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-13:30:21-root-INFO: grad norm: 686.053 611.592 310.844
2024-12-02-13:30:21-root-INFO: Loss too large (4993.901->5064.778)! Learning rate decreased to 0.00051.
2024-12-02-13:30:22-root-INFO: grad norm: 838.500 766.821 339.218
2024-12-02-13:30:22-root-INFO: Loss too large (4989.851->5019.084)! Learning rate decreased to 0.00041.
2024-12-02-13:30:22-root-INFO: grad norm: 707.218 637.604 305.971
2024-12-02-13:30:23-root-INFO: grad norm: 595.711 546.227 237.713
2024-12-02-13:30:24-root-INFO: grad norm: 508.599 459.580 217.850
2024-12-02-13:30:24-root-INFO: grad norm: 435.974 401.079 170.906
2024-12-02-13:30:25-root-INFO: grad norm: 378.688 343.319 159.802
2024-12-02-13:30:25-root-INFO: grad norm: 331.653 306.664 126.298
2024-12-02-13:30:25-root-INFO: Loss Change: 4993.901 -> 4820.323
2024-12-02-13:30:25-root-INFO: Regularization Change: 0.000 -> 0.441
2024-12-02-13:30:25-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-13:30:25-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:30:26-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-13:30:26-root-INFO: grad norm: 524.608 466.702 239.589
2024-12-02-13:30:26-root-INFO: Loss too large (4815.639->4845.782)! Learning rate decreased to 0.00054.
2024-12-02-13:30:26-root-INFO: grad norm: 637.066 583.846 254.904
2024-12-02-13:30:27-root-INFO: Loss too large (4805.095->4814.736)! Learning rate decreased to 0.00043.
2024-12-02-13:30:27-root-INFO: grad norm: 546.108 494.316 232.133
2024-12-02-13:30:28-root-INFO: grad norm: 469.529 432.188 183.496
2024-12-02-13:30:28-root-INFO: grad norm: 408.244 370.492 171.462
2024-12-02-13:30:28-root-INFO: grad norm: 357.167 330.057 136.496
2024-12-02-13:30:29-root-INFO: grad norm: 316.107 288.006 130.293
2024-12-02-13:30:29-root-INFO: grad norm: 282.196 262.109 104.563
2024-12-02-13:30:30-root-INFO: Loss Change: 4815.639 -> 4667.313
2024-12-02-13:30:30-root-INFO: Regularization Change: 0.000 -> 0.441
2024-12-02-13:30:30-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-13:30:30-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:30:30-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-13:30:30-root-INFO: grad norm: 512.010 459.082 226.710
2024-12-02-13:30:30-root-INFO: Loss too large (4670.282->4705.858)! Learning rate decreased to 0.00056.
2024-12-02-13:30:31-root-INFO: grad norm: 628.356 576.729 249.429
2024-12-02-13:30:31-root-INFO: Loss too large (4662.989->4674.095)! Learning rate decreased to 0.00045.
2024-12-02-13:30:32-root-INFO: grad norm: 533.784 484.105 224.872
2024-12-02-13:30:32-root-INFO: grad norm: 454.069 418.313 176.614
2024-12-02-13:30:33-root-INFO: grad norm: 391.890 356.389 162.987
2024-12-02-13:30:33-root-INFO: grad norm: 340.475 314.811 129.681
2024-12-02-13:30:34-root-INFO: grad norm: 300.051 274.032 122.217
2024-12-02-13:30:34-root-INFO: grad norm: 267.291 248.349 98.829
2024-12-02-13:30:35-root-INFO: Loss Change: 4670.282 -> 4527.996
2024-12-02-13:30:35-root-INFO: Regularization Change: 0.000 -> 0.439
2024-12-02-13:30:35-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-13:30:35-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:30:35-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-13:30:35-root-INFO: grad norm: 304.433 278.175 123.685
2024-12-02-13:30:35-root-INFO: grad norm: 491.657 452.485 192.310
2024-12-02-13:30:36-root-INFO: Loss too large (4468.918->4515.877)! Learning rate decreased to 0.00059.
2024-12-02-13:30:36-root-INFO: Loss too large (4468.918->4470.752)! Learning rate decreased to 0.00047.
2024-12-02-13:30:36-root-INFO: grad norm: 412.132 376.747 167.075
2024-12-02-13:30:37-root-INFO: grad norm: 348.545 322.091 133.194
2024-12-02-13:30:37-root-INFO: grad norm: 299.775 275.066 119.179
2024-12-02-13:30:38-root-INFO: grad norm: 261.307 242.768 96.669
2024-12-02-13:30:38-root-INFO: grad norm: 232.060 214.226 89.213
2024-12-02-13:30:39-root-INFO: grad norm: 209.760 196.047 74.596
2024-12-02-13:30:39-root-INFO: Loss Change: 4470.100 -> 4359.587
2024-12-02-13:30:39-root-INFO: Regularization Change: 0.000 -> 0.421
2024-12-02-13:30:39-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-13:30:39-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:30:39-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-13:30:39-root-INFO: grad norm: 307.476 278.725 129.822
2024-12-02-13:30:40-root-INFO: grad norm: 482.257 442.495 191.756
2024-12-02-13:30:40-root-INFO: Loss too large (4340.177->4384.630)! Learning rate decreased to 0.00062.
2024-12-02-13:30:40-root-INFO: Loss too large (4340.177->4340.200)! Learning rate decreased to 0.00050.
2024-12-02-13:30:41-root-INFO: grad norm: 387.760 354.218 157.758
2024-12-02-13:30:41-root-INFO: grad norm: 316.618 293.167 119.581
2024-12-02-13:30:42-root-INFO: grad norm: 265.043 243.471 104.735
2024-12-02-13:30:42-root-INFO: grad norm: 226.946 211.572 82.108
2024-12-02-13:30:43-root-INFO: grad norm: 199.789 185.161 75.040
2024-12-02-13:30:43-root-INFO: grad norm: 180.501 169.494 62.066
2024-12-02-13:30:44-root-INFO: Loss Change: 4340.391 -> 4234.384
2024-12-02-13:30:44-root-INFO: Regularization Change: 0.000 -> 0.415
2024-12-02-13:30:44-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-13:30:44-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:30:44-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-13:30:44-root-INFO: grad norm: 446.855 396.965 205.178
2024-12-02-13:30:44-root-INFO: Loss too large (4233.460->4252.293)! Learning rate decreased to 0.00065.
2024-12-02-13:30:45-root-INFO: grad norm: 511.468 471.223 198.869
2024-12-02-13:30:45-root-INFO: grad norm: 609.727 556.025 250.206
2024-12-02-13:30:45-root-INFO: Loss too large (4219.107->4227.749)! Learning rate decreased to 0.00052.
2024-12-02-13:30:46-root-INFO: grad norm: 481.929 444.366 186.532
2024-12-02-13:30:46-root-INFO: grad norm: 388.096 355.245 156.268
2024-12-02-13:30:47-root-INFO: grad norm: 315.487 292.002 119.444
2024-12-02-13:30:47-root-INFO: grad norm: 262.103 241.500 101.860
2024-12-02-13:30:48-root-INFO: grad norm: 222.412 207.210 80.816
2024-12-02-13:30:48-root-INFO: Loss Change: 4233.460 -> 4103.865
2024-12-02-13:30:48-root-INFO: Regularization Change: 0.000 -> 0.470
2024-12-02-13:30:48-root-INFO: Undo step: 220
2024-12-02-13:30:48-root-INFO: Undo step: 221
2024-12-02-13:30:48-root-INFO: Undo step: 222
2024-12-02-13:30:48-root-INFO: Undo step: 223
2024-12-02-13:30:48-root-INFO: Undo step: 224
2024-12-02-13:30:48-root-INFO: Undo step: 225
2024-12-02-13:30:48-root-INFO: Undo step: 226
2024-12-02-13:30:48-root-INFO: Undo step: 227
2024-12-02-13:30:48-root-INFO: Undo step: 228
2024-12-02-13:30:48-root-INFO: Undo step: 229
2024-12-02-13:30:49-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-13:30:49-root-INFO: grad norm: 6040.015 4501.091 4027.649
2024-12-02-13:30:49-root-INFO: grad norm: 6924.810 5210.617 4560.971
2024-12-02-13:30:49-root-INFO: Loss too large (13723.714->15109.399)! Learning rate decreased to 0.00040.
2024-12-02-13:30:50-root-INFO: grad norm: 5236.613 4191.465 3139.066
2024-12-02-13:30:50-root-INFO: grad norm: 4496.440 3733.054 2506.448
2024-12-02-13:30:51-root-INFO: Loss too large (9304.222->9457.897)! Learning rate decreased to 0.00032.
2024-12-02-13:30:51-root-INFO: grad norm: 3297.992 2838.633 1678.963
2024-12-02-13:30:52-root-INFO: grad norm: 2622.075 2300.241 1258.638
2024-12-02-13:30:52-root-INFO: grad norm: 2228.228 1948.959 1080.073
2024-12-02-13:30:53-root-INFO: grad norm: 1914.033 1702.729 874.205
2024-12-02-13:30:53-root-INFO: Loss Change: 14048.604 -> 6732.092
2024-12-02-13:30:53-root-INFO: Regularization Change: 0.000 -> 11.151
2024-12-02-13:30:53-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-13:30:53-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:30:53-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-13:30:53-root-INFO: grad norm: 1695.768 1517.224 757.406
2024-12-02-13:30:53-root-INFO: Loss too large (6579.105->6980.621)! Learning rate decreased to 0.00042.
2024-12-02-13:30:54-root-INFO: Loss too large (6579.105->6586.254)! Learning rate decreased to 0.00034.
2024-12-02-13:30:54-root-INFO: grad norm: 1402.200 1269.213 596.041
2024-12-02-13:30:55-root-INFO: grad norm: 1203.379 1072.782 545.215
2024-12-02-13:30:55-root-INFO: grad norm: 1036.866 941.065 435.302
2024-12-02-13:30:56-root-INFO: grad norm: 903.333 810.821 398.221
2024-12-02-13:30:56-root-INFO: grad norm: 791.718 719.774 329.760
2024-12-02-13:30:57-root-INFO: grad norm: 701.515 633.747 300.812
2024-12-02-13:30:57-root-INFO: grad norm: 625.416 569.089 259.388
2024-12-02-13:30:57-root-INFO: Loss Change: 6579.105 -> 5814.880
2024-12-02-13:30:57-root-INFO: Regularization Change: 0.000 -> 1.468
2024-12-02-13:30:57-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-13:30:57-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:30:58-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-13:30:58-root-INFO: grad norm: 674.460 609.757 288.258
2024-12-02-13:30:58-root-INFO: Loss too large (5765.304->5769.481)! Learning rate decreased to 0.00044.
2024-12-02-13:30:58-root-INFO: grad norm: 789.813 714.252 337.119
2024-12-02-13:30:59-root-INFO: grad norm: 965.019 876.681 403.351
2024-12-02-13:30:59-root-INFO: Loss too large (5710.417->5716.034)! Learning rate decreased to 0.00035.
2024-12-02-13:30:59-root-INFO: grad norm: 815.607 739.594 343.825
2024-12-02-13:31:00-root-INFO: grad norm: 700.487 638.557 287.972
2024-12-02-13:31:00-root-INFO: grad norm: 605.537 549.400 254.626
2024-12-02-13:31:01-root-INFO: grad norm: 531.299 485.864 214.976
2024-12-02-13:31:01-root-INFO: grad norm: 470.513 427.333 196.899
2024-12-02-13:31:02-root-INFO: Loss Change: 5765.304 -> 5447.721
2024-12-02-13:31:02-root-INFO: Regularization Change: 0.000 -> 0.886
2024-12-02-13:31:02-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-13:31:02-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-13:31:02-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-13:31:02-root-INFO: grad norm: 799.762 724.892 337.860
2024-12-02-13:31:02-root-INFO: Loss too large (5454.532->5520.791)! Learning rate decreased to 0.00046.
2024-12-02-13:31:02-root-INFO: grad norm: 956.935 870.039 398.442
2024-12-02-13:31:03-root-INFO: Loss too large (5436.183->5453.601)! Learning rate decreased to 0.00037.
2024-12-02-13:31:03-root-INFO: grad norm: 797.610 729.909 321.581
2024-12-02-13:31:04-root-INFO: grad norm: 668.454 607.266 279.390
2024-12-02-13:31:04-root-INFO: grad norm: 570.053 523.646 225.291
2024-12-02-13:31:05-root-INFO: grad norm: 490.187 445.259 205.005
2024-12-02-13:31:05-root-INFO: grad norm: 427.595 394.661 164.561
2024-12-02-13:31:05-root-INFO: grad norm: 377.405 343.520 156.298
2024-12-02-13:31:06-root-INFO: Loss Change: 5454.532 -> 5182.862
2024-12-02-13:31:06-root-INFO: Regularization Change: 0.000 -> 0.676
2024-12-02-13:31:06-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-13:31:06-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-13:31:06-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-13:31:06-root-INFO: grad norm: 384.926 336.099 187.630
2024-12-02-13:31:07-root-INFO: grad norm: 488.395 434.674 222.684
2024-12-02-13:31:07-root-INFO: Loss too large (5090.372->5094.465)! Learning rate decreased to 0.00049.
2024-12-02-13:31:07-root-INFO: grad norm: 547.430 496.681 230.190
2024-12-02-13:31:08-root-INFO: grad norm: 652.772 591.679 275.731
2024-12-02-13:31:08-root-INFO: grad norm: 794.253 725.231 323.851
2024-12-02-13:31:08-root-INFO: Loss too large (5057.141->5069.204)! Learning rate decreased to 0.00039.
2024-12-02-13:31:09-root-INFO: grad norm: 647.947 589.139 269.722
2024-12-02-13:31:09-root-INFO: grad norm: 536.564 492.416 213.136
2024-12-02-13:31:10-root-INFO: grad norm: 448.706 409.245 184.001
2024-12-02-13:31:10-root-INFO: Loss Change: 5118.022 -> 4934.508
2024-12-02-13:31:10-root-INFO: Regularization Change: 0.000 -> 0.668
2024-12-02-13:31:10-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-13:31:10-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:31:10-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-13:31:10-root-INFO: grad norm: 802.441 725.984 341.847
2024-12-02-13:31:10-root-INFO: Loss too large (4925.590->5025.826)! Learning rate decreased to 0.00051.
2024-12-02-13:31:11-root-INFO: grad norm: 954.531 871.866 388.562
2024-12-02-13:31:11-root-INFO: Loss too large (4921.925->4956.034)! Learning rate decreased to 0.00041.
2024-12-02-13:31:12-root-INFO: grad norm: 774.328 708.253 312.987
2024-12-02-13:31:12-root-INFO: grad norm: 627.663 573.935 254.086
2024-12-02-13:31:12-root-INFO: grad norm: 516.423 475.132 202.342
2024-12-02-13:31:13-root-INFO: grad norm: 427.728 392.084 170.942
2024-12-02-13:31:13-root-INFO: grad norm: 360.427 334.386 134.512
2024-12-02-13:31:14-root-INFO: grad norm: 307.551 283.672 118.818
2024-12-02-13:31:14-root-INFO: Loss Change: 4925.590 -> 4721.262
2024-12-02-13:31:14-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-02-13:31:14-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-13:31:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:31:14-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-13:31:15-root-INFO: grad norm: 506.790 458.277 216.374
2024-12-02-13:31:15-root-INFO: Loss too large (4709.391->4729.729)! Learning rate decreased to 0.00054.
2024-12-02-13:31:15-root-INFO: grad norm: 586.912 537.101 236.619
2024-12-02-13:31:15-root-INFO: Loss too large (4694.592->4695.335)! Learning rate decreased to 0.00043.
2024-12-02-13:31:16-root-INFO: grad norm: 482.260 443.994 188.266
2024-12-02-13:31:16-root-INFO: grad norm: 399.589 367.479 156.940
2024-12-02-13:31:17-root-INFO: grad norm: 337.461 313.303 125.385
2024-12-02-13:31:18-root-INFO: grad norm: 289.336 267.650 109.905
2024-12-02-13:31:19-root-INFO: grad norm: 253.124 237.578 87.341
2024-12-02-13:31:20-root-INFO: grad norm: 225.657 210.719 80.740
2024-12-02-13:31:21-root-INFO: Loss Change: 4709.391 -> 4565.555
2024-12-02-13:31:21-root-INFO: Regularization Change: 0.000 -> 0.424
2024-12-02-13:31:21-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-13:31:21-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:31:21-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-13:31:21-root-INFO: grad norm: 464.277 420.849 196.061
2024-12-02-13:31:22-root-INFO: Loss too large (4558.873->4577.877)! Learning rate decreased to 0.00056.
2024-12-02-13:31:23-root-INFO: grad norm: 537.336 493.382 212.849
2024-12-02-13:31:24-root-INFO: grad norm: 641.847 588.983 255.080
2024-12-02-13:31:24-root-INFO: Loss too large (4545.824->4553.165)! Learning rate decreased to 0.00045.
2024-12-02-13:31:25-root-INFO: grad norm: 509.982 468.329 201.866
2024-12-02-13:31:26-root-INFO: grad norm: 414.229 382.433 159.157
2024-12-02-13:31:27-root-INFO: grad norm: 339.390 312.937 131.363
2024-12-02-13:31:28-root-INFO: grad norm: 283.660 264.523 102.423
2024-12-02-13:31:29-root-INFO: grad norm: 241.908 224.929 89.031
2024-12-02-13:31:30-root-INFO: Loss Change: 4558.873 -> 4429.892
2024-12-02-13:31:30-root-INFO: Regularization Change: 0.000 -> 0.415
2024-12-02-13:31:30-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-13:31:30-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:31:30-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-13:31:30-root-INFO: grad norm: 284.428 264.092 105.616
2024-12-02-13:31:31-root-INFO: grad norm: 437.531 401.055 174.895
2024-12-02-13:31:32-root-INFO: Loss too large (4368.950->4399.068)! Learning rate decreased to 0.00059.
2024-12-02-13:31:33-root-INFO: grad norm: 515.058 475.194 198.686
2024-12-02-13:31:33-root-INFO: Loss too large (4365.832->4367.042)! Learning rate decreased to 0.00047.
2024-12-02-13:31:34-root-INFO: grad norm: 404.416 371.377 160.100
2024-12-02-13:31:35-root-INFO: grad norm: 325.107 302.515 119.077
2024-12-02-13:31:36-root-INFO: grad norm: 266.149 246.140 101.245
2024-12-02-13:31:37-root-INFO: grad norm: 224.428 211.622 74.726
2024-12-02-13:31:38-root-INFO: grad norm: 195.276 182.781 68.730
2024-12-02-13:31:39-root-INFO: Loss Change: 4372.127 -> 4273.833
2024-12-02-13:31:39-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-13:31:39-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-13:31:39-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:31:39-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-13:31:39-root-INFO: grad norm: 286.980 263.698 113.230
2024-12-02-13:31:40-root-INFO: grad norm: 432.357 395.370 174.973
2024-12-02-13:31:41-root-INFO: Loss too large (4243.460->4272.715)! Learning rate decreased to 0.00062.
2024-12-02-13:31:42-root-INFO: grad norm: 491.568 452.304 192.511
2024-12-02-13:31:43-root-INFO: grad norm: 566.219 518.124 228.365
2024-12-02-13:31:43-root-INFO: Loss too large (4237.778->4242.105)! Learning rate decreased to 0.00050.
2024-12-02-13:31:44-root-INFO: grad norm: 427.017 394.053 164.515
2024-12-02-13:31:45-root-INFO: grad norm: 325.744 299.583 127.902
2024-12-02-13:31:46-root-INFO: grad norm: 256.463 239.651 91.327
2024-12-02-13:31:47-root-INFO: grad norm: 209.230 194.650 76.737
2024-12-02-13:31:48-root-INFO: Loss Change: 4244.989 -> 4150.728
2024-12-02-13:31:48-root-INFO: Regularization Change: 0.000 -> 0.393
2024-12-02-13:31:48-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-13:31:48-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:31:48-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-13:31:49-root-INFO: grad norm: 481.790 430.889 215.537
2024-12-02-13:31:49-root-INFO: Loss too large (4155.701->4178.518)! Learning rate decreased to 0.00065.
2024-12-02-13:31:50-root-INFO: grad norm: 523.940 481.979 205.451
2024-12-02-13:31:51-root-INFO: grad norm: 591.121 540.924 238.379
2024-12-02-13:31:51-root-INFO: Loss too large (4136.531->4137.725)! Learning rate decreased to 0.00052.
2024-12-02-13:31:52-root-INFO: grad norm: 434.001 399.208 170.263
2024-12-02-13:31:53-root-INFO: grad norm: 327.841 302.567 126.225
2024-12-02-13:31:54-root-INFO: grad norm: 252.460 233.878 95.064
2024-12-02-13:31:55-root-INFO: grad norm: 202.747 190.222 70.157
2024-12-02-13:31:56-root-INFO: grad norm: 170.877 160.682 58.141
2024-12-02-13:31:57-root-INFO: Loss Change: 4155.701 -> 4029.614
2024-12-02-13:31:57-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-02-13:31:57-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-13:31:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:31:57-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-13:31:58-root-INFO: grad norm: 173.978 163.521 59.405
2024-12-02-13:31:59-root-INFO: grad norm: 210.151 195.610 76.812
2024-12-02-13:32:00-root-INFO: grad norm: 296.803 275.641 110.065
2024-12-02-13:32:00-root-INFO: Loss too large (3986.880->3992.837)! Learning rate decreased to 0.00068.
2024-12-02-13:32:01-root-INFO: grad norm: 322.440 296.780 126.052
2024-12-02-13:32:02-root-INFO: grad norm: 351.720 325.475 133.314
2024-12-02-13:32:03-root-INFO: grad norm: 385.768 353.871 153.597
2024-12-02-13:32:04-root-INFO: grad norm: 423.242 390.519 163.183
2024-12-02-13:32:05-root-INFO: grad norm: 466.775 427.306 187.853
2024-12-02-13:32:06-root-INFO: Loss Change: 4006.027 -> 3954.120
2024-12-02-13:32:06-root-INFO: Regularization Change: 0.000 -> 0.540
2024-12-02-13:32:06-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-13:32:06-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-13:32:06-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-13:32:06-root-INFO: grad norm: 701.547 640.436 286.373
2024-12-02-13:32:07-root-INFO: Loss too large (3970.412->4076.950)! Learning rate decreased to 0.00071.
2024-12-02-13:32:08-root-INFO: grad norm: 754.843 693.573 297.900
2024-12-02-13:32:08-root-INFO: Loss too large (3966.851->3974.450)! Learning rate decreased to 0.00057.
2024-12-02-13:32:09-root-INFO: grad norm: 515.598 472.657 206.003
2024-12-02-13:32:10-root-INFO: grad norm: 351.562 323.925 136.633
2024-12-02-13:32:11-root-INFO: grad norm: 249.643 231.612 93.154
2024-12-02-13:32:12-root-INFO: grad norm: 186.398 173.943 66.993
2024-12-02-13:32:13-root-INFO: grad norm: 150.996 143.237 47.778
2024-12-02-13:32:14-root-INFO: grad norm: 131.806 125.434 40.487
2024-12-02-13:32:15-root-INFO: Loss Change: 3970.412 -> 3816.451
2024-12-02-13:32:15-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-02-13:32:15-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-13:32:15-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:32:15-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-13:32:15-root-INFO: grad norm: 302.973 265.678 145.629
2024-12-02-13:32:16-root-INFO: grad norm: 398.669 365.229 159.829
2024-12-02-13:32:17-root-INFO: Loss too large (3812.782->3835.634)! Learning rate decreased to 0.00075.
2024-12-02-13:32:18-root-INFO: grad norm: 411.928 378.411 162.758
2024-12-02-13:32:19-root-INFO: grad norm: 431.613 396.935 169.508
2024-12-02-13:32:20-root-INFO: grad norm: 452.760 416.729 176.998
2024-12-02-13:32:21-root-INFO: grad norm: 475.932 436.984 188.564
2024-12-02-13:32:22-root-INFO: grad norm: 499.742 460.035 195.217
2024-12-02-13:32:23-root-INFO: grad norm: 525.781 482.279 209.410
2024-12-02-13:32:23-root-INFO: Loss Change: 3819.693 -> 3772.074
2024-12-02-13:32:23-root-INFO: Regularization Change: 0.000 -> 0.540
2024-12-02-13:32:23-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-13:32:23-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:32:24-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-13:32:24-root-INFO: grad norm: 738.354 668.261 313.995
2024-12-02-13:32:24-root-INFO: Loss too large (3796.789->3909.965)! Learning rate decreased to 0.00079.
2024-12-02-13:32:25-root-INFO: grad norm: 758.264 696.558 299.616
2024-12-02-13:32:26-root-INFO: Loss too large (3781.022->3782.644)! Learning rate decreased to 0.00063.
2024-12-02-13:32:27-root-INFO: grad norm: 495.706 453.820 199.429
2024-12-02-13:32:28-root-INFO: grad norm: 322.797 297.947 124.200
2024-12-02-13:32:29-root-INFO: grad norm: 222.659 206.497 83.282
2024-12-02-13:32:30-root-INFO: grad norm: 163.847 153.484 57.346
2024-12-02-13:32:31-root-INFO: grad norm: 133.089 126.442 41.534
2024-12-02-13:32:32-root-INFO: grad norm: 117.699 112.485 34.641
2024-12-02-13:32:33-root-INFO: Loss Change: 3796.789 -> 3619.405
2024-12-02-13:32:33-root-INFO: Regularization Change: 0.000 -> 0.443
2024-12-02-13:32:33-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-13:32:33-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:32:33-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-13:32:33-root-INFO: grad norm: 136.138 127.987 46.399
2024-12-02-13:32:34-root-INFO: grad norm: 158.035 148.068 55.235
2024-12-02-13:32:35-root-INFO: grad norm: 204.204 189.632 75.757
2024-12-02-13:32:36-root-INFO: grad norm: 284.989 262.731 110.413
2024-12-02-13:32:37-root-INFO: Loss too large (3585.340->3593.246)! Learning rate decreased to 0.00082.
2024-12-02-13:32:38-root-INFO: grad norm: 286.530 264.978 109.024
2024-12-02-13:32:39-root-INFO: grad norm: 288.409 266.047 111.351
2024-12-02-13:32:40-root-INFO: grad norm: 290.572 269.102 109.618
2024-12-02-13:32:41-root-INFO: grad norm: 292.998 270.252 113.188
2024-12-02-13:32:41-root-INFO: Loss Change: 3606.651 -> 3544.762
2024-12-02-13:32:41-root-INFO: Regularization Change: 0.000 -> 0.556
2024-12-02-13:32:41-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-13:32:41-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:32:42-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-13:32:42-root-INFO: grad norm: 429.486 387.006 186.238
2024-12-02-13:32:42-root-INFO: Loss too large (3543.064->3567.298)! Learning rate decreased to 0.00086.
2024-12-02-13:32:43-root-INFO: grad norm: 410.533 378.829 158.194
2024-12-02-13:32:44-root-INFO: grad norm: 403.324 371.124 157.915
2024-12-02-13:32:45-root-INFO: grad norm: 396.518 366.263 151.916
2024-12-02-13:32:46-root-INFO: grad norm: 390.815 360.923 149.904
2024-12-02-13:32:47-root-INFO: grad norm: 385.391 355.937 147.769
2024-12-02-13:32:48-root-INFO: grad norm: 380.614 352.044 144.678
2024-12-02-13:32:49-root-INFO: grad norm: 376.023 347.312 144.109
2024-12-02-13:32:50-root-INFO: Loss Change: 3543.064 -> 3460.986
2024-12-02-13:32:50-root-INFO: Regularization Change: 0.000 -> 0.486
2024-12-02-13:32:50-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-13:32:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:32:50-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-13:32:51-root-INFO: grad norm: 546.755 493.593 235.174
2024-12-02-13:32:51-root-INFO: Loss too large (3502.961->3547.522)! Learning rate decreased to 0.00090.
2024-12-02-13:32:52-root-INFO: grad norm: 514.722 476.596 194.411
2024-12-02-13:32:53-root-INFO: grad norm: 498.156 458.096 195.724
2024-12-02-13:32:54-root-INFO: grad norm: 482.902 447.950 180.375
2024-12-02-13:32:55-root-INFO: grad norm: 471.218 435.285 180.479
2024-12-02-13:32:56-root-INFO: grad norm: 460.060 426.951 171.370
2024-12-02-13:32:57-root-INFO: grad norm: 450.668 417.215 170.393
2024-12-02-13:32:58-root-INFO: grad norm: 442.048 410.426 164.184
2024-12-02-13:32:59-root-INFO: Loss Change: 3502.961 -> 3396.227
2024-12-02-13:32:59-root-INFO: Regularization Change: 0.000 -> 0.562
2024-12-02-13:32:59-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-13:32:59-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:32:59-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-13:33:00-root-INFO: grad norm: 435.715 404.089 162.970
2024-12-02-13:33:00-root-INFO: Loss too large (3384.185->3417.274)! Learning rate decreased to 0.00095.
2024-12-02-13:33:01-root-INFO: grad norm: 406.813 379.354 146.926
2024-12-02-13:33:02-root-INFO: grad norm: 381.897 354.969 140.862
2024-12-02-13:33:03-root-INFO: grad norm: 359.730 336.034 128.401
2024-12-02-13:33:04-root-INFO: grad norm: 340.399 317.141 123.665
2024-12-02-13:33:05-root-INFO: grad norm: 323.139 302.377 113.958
2024-12-02-13:33:06-root-INFO: grad norm: 307.984 287.550 110.314
2024-12-02-13:33:07-root-INFO: grad norm: 294.632 276.168 102.660
2024-12-02-13:33:08-root-INFO: Loss Change: 3384.185 -> 3290.922
2024-12-02-13:33:08-root-INFO: Regularization Change: 0.000 -> 0.504
2024-12-02-13:33:08-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-13:33:08-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-13:33:08-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-13:33:08-root-INFO: grad norm: 462.631 413.877 206.720
2024-12-02-13:33:09-root-INFO: Loss too large (3310.707->3325.955)! Learning rate decreased to 0.00099.
2024-12-02-13:33:10-root-INFO: grad norm: 399.418 372.131 145.095
2024-12-02-13:33:11-root-INFO: grad norm: 370.731 343.565 139.300
2024-12-02-13:33:12-root-INFO: grad norm: 352.161 331.239 119.575
2024-12-02-13:33:13-root-INFO: grad norm: 345.552 324.569 118.578
2024-12-02-13:33:14-root-INFO: grad norm: 342.775 324.215 111.263
2024-12-02-13:33:15-root-INFO: grad norm: 347.017 327.873 113.666
2024-12-02-13:33:16-root-INFO: grad norm: 352.747 334.392 112.306
2024-12-02-13:33:16-root-INFO: Loss Change: 3310.707 -> 3206.061
2024-12-02-13:33:16-root-INFO: Regularization Change: 0.000 -> 0.618
2024-12-02-13:33:16-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-13:33:16-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:33:17-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-13:33:17-root-INFO: grad norm: 398.548 374.846 135.394
2024-12-02-13:33:17-root-INFO: Loss too large (3203.987->3242.494)! Learning rate decreased to 0.00104.
2024-12-02-13:33:18-root-INFO: grad norm: 396.279 378.080 118.710
2024-12-02-13:33:19-root-INFO: grad norm: 407.563 386.008 130.788
2024-12-02-13:33:20-root-INFO: grad norm: 420.386 400.810 126.790
2024-12-02-13:33:21-root-INFO: grad norm: 441.988 419.542 139.060
2024-12-02-13:33:22-root-INFO: Loss too large (3175.964->3179.473)! Learning rate decreased to 0.00083.
2024-12-02-13:33:23-root-INFO: grad norm: 305.618 292.337 89.115
2024-12-02-13:33:24-root-INFO: grad norm: 207.497 195.604 69.237
2024-12-02-13:33:25-root-INFO: grad norm: 157.275 151.890 40.801
2024-12-02-13:33:25-root-INFO: Loss Change: 3203.987 -> 3106.954
2024-12-02-13:33:25-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-02-13:33:25-root-INFO: Undo step: 210
2024-12-02-13:33:25-root-INFO: Undo step: 211
2024-12-02-13:33:25-root-INFO: Undo step: 212
2024-12-02-13:33:25-root-INFO: Undo step: 213
2024-12-02-13:33:25-root-INFO: Undo step: 214
2024-12-02-13:33:25-root-INFO: Undo step: 215
2024-12-02-13:33:25-root-INFO: Undo step: 216
2024-12-02-13:33:25-root-INFO: Undo step: 217
2024-12-02-13:33:25-root-INFO: Undo step: 218
2024-12-02-13:33:25-root-INFO: Undo step: 219
2024-12-02-13:33:26-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-13:33:26-root-INFO: grad norm: 13580.961 12463.383 5395.051
2024-12-02-13:33:27-root-INFO: grad norm: 5181.416 4410.609 2719.117
2024-12-02-13:33:28-root-INFO: grad norm: 4240.817 3896.939 1672.840
2024-12-02-13:33:29-root-INFO: grad norm: 4020.253 3516.856 1947.860
2024-12-02-13:33:30-root-INFO: grad norm: 2440.941 2235.791 979.505
2024-12-02-13:33:31-root-INFO: grad norm: 1558.786 1385.558 714.172
2024-12-02-13:33:32-root-INFO: grad norm: 1104.601 972.491 523.837
2024-12-02-13:33:33-root-INFO: grad norm: 878.904 796.270 372.056
2024-12-02-13:33:34-root-INFO: Loss Change: 30152.801 -> 8781.146
2024-12-02-13:33:34-root-INFO: Regularization Change: 0.000 -> 96.207
2024-12-02-13:33:34-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-13:33:34-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-13:33:34-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-13:33:35-root-INFO: grad norm: 770.514 722.527 267.667
2024-12-02-13:33:36-root-INFO: grad norm: 945.700 822.325 467.045
2024-12-02-13:33:37-root-INFO: grad norm: 968.649 893.058 375.137
2024-12-02-13:33:38-root-INFO: grad norm: 1271.457 1166.239 506.446
2024-12-02-13:33:39-root-INFO: grad norm: 1812.848 1696.820 638.137
2024-12-02-13:33:40-root-INFO: grad norm: 2556.681 2329.891 1052.725
2024-12-02-13:33:41-root-INFO: grad norm: 2617.187 2412.865 1013.780
2024-12-02-13:33:41-root-INFO: Loss too large (6047.322->6223.133)! Learning rate decreased to 0.00068.
2024-12-02-13:33:42-root-INFO: grad norm: 2082.393 1893.751 866.064
2024-12-02-13:33:43-root-INFO: Loss Change: 8225.080 -> 4991.729
2024-12-02-13:33:43-root-INFO: Regularization Change: 0.000 -> 13.810
2024-12-02-13:33:43-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-13:33:43-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-13:33:43-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-13:33:43-root-INFO: grad norm: 2049.791 1896.707 777.268
2024-12-02-13:33:44-root-INFO: Loss too large (5039.001->5628.106)! Learning rate decreased to 0.00071.
2024-12-02-13:33:45-root-INFO: grad norm: 1903.760 1747.885 754.455
2024-12-02-13:33:46-root-INFO: grad norm: 1815.259 1680.929 685.306
2024-12-02-13:33:47-root-INFO: grad norm: 1728.302 1585.308 688.350
2024-12-02-13:33:48-root-INFO: grad norm: 1651.848 1528.016 627.510
2024-12-02-13:33:49-root-INFO: grad norm: 1575.294 1443.069 631.746
2024-12-02-13:33:50-root-INFO: grad norm: 1504.682 1390.202 575.678
2024-12-02-13:33:51-root-INFO: grad norm: 1434.647 1312.699 578.822
2024-12-02-13:33:51-root-INFO: Loss Change: 5039.001 -> 4192.184
2024-12-02-13:33:51-root-INFO: Regularization Change: 0.000 -> 2.321
2024-12-02-13:33:51-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-13:33:51-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:33:52-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-13:33:52-root-INFO: grad norm: 1471.490 1352.538 579.590
2024-12-02-13:33:52-root-INFO: Loss too large (4248.737->4587.492)! Learning rate decreased to 0.00075.
2024-12-02-13:33:53-root-INFO: grad norm: 1350.027 1238.415 537.495
2024-12-02-13:33:54-root-INFO: grad norm: 1255.794 1159.362 482.594
2024-12-02-13:33:55-root-INFO: grad norm: 1165.451 1068.010 466.508
2024-12-02-13:33:56-root-INFO: grad norm: 1085.971 1002.793 416.819
2024-12-02-13:33:57-root-INFO: grad norm: 1009.951 925.022 405.382
2024-12-02-13:33:58-root-INFO: grad norm: 942.713 870.602 361.607
2024-12-02-13:34:00-root-INFO: grad norm: 879.442 805.081 353.923
2024-12-02-13:34:00-root-INFO: Loss Change: 4248.737 -> 3775.686
2024-12-02-13:34:00-root-INFO: Regularization Change: 0.000 -> 1.113
2024-12-02-13:34:00-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-13:34:00-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:34:01-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-13:34:01-root-INFO: grad norm: 981.564 898.348 395.524
2024-12-02-13:34:01-root-INFO: Loss too large (3799.574->3942.754)! Learning rate decreased to 0.00079.
2024-12-02-13:34:02-root-INFO: grad norm: 892.022 816.024 360.290
2024-12-02-13:34:03-root-INFO: grad norm: 823.312 760.041 316.514
2024-12-02-13:34:04-root-INFO: grad norm: 758.513 694.559 304.846
2024-12-02-13:34:05-root-INFO: grad norm: 702.602 649.666 267.551
2024-12-02-13:34:06-root-INFO: grad norm: 650.349 595.609 261.158
2024-12-02-13:34:07-root-INFO: grad norm: 604.007 559.096 228.554
2024-12-02-13:34:08-root-INFO: grad norm: 561.062 513.831 225.318
2024-12-02-13:34:09-root-INFO: Loss Change: 3799.574 -> 3535.043
2024-12-02-13:34:09-root-INFO: Regularization Change: 0.000 -> 0.780
2024-12-02-13:34:09-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-13:34:09-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:34:09-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-13:34:10-root-INFO: grad norm: 536.282 495.576 204.945
2024-12-02-13:34:10-root-INFO: Loss too large (3527.971->3560.673)! Learning rate decreased to 0.00082.
2024-12-02-13:34:11-root-INFO: grad norm: 478.287 439.963 187.592
2024-12-02-13:34:12-root-INFO: grad norm: 429.846 398.258 161.735
2024-12-02-13:34:13-root-INFO: grad norm: 387.331 356.457 151.538
2024-12-02-13:34:14-root-INFO: grad norm: 350.846 326.222 129.120
2024-12-02-13:34:15-root-INFO: grad norm: 318.716 293.473 124.311
2024-12-02-13:34:16-root-INFO: grad norm: 290.784 271.438 104.290
2024-12-02-13:34:17-root-INFO: grad norm: 266.199 245.371 103.222
2024-12-02-13:34:18-root-INFO: Loss Change: 3527.971 -> 3385.100
2024-12-02-13:34:18-root-INFO: Regularization Change: 0.000 -> 0.632
2024-12-02-13:34:18-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-13:34:18-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:34:18-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-13:34:18-root-INFO: grad norm: 372.406 336.875 158.750
2024-12-02-13:34:19-root-INFO: Loss too large (3366.443->3367.862)! Learning rate decreased to 0.00086.
2024-12-02-13:34:20-root-INFO: grad norm: 316.643 290.785 125.328
2024-12-02-13:34:21-root-INFO: grad norm: 283.078 263.970 102.240
2024-12-02-13:34:22-root-INFO: grad norm: 254.851 235.539 97.315
2024-12-02-13:34:23-root-INFO: grad norm: 231.392 217.470 79.052
2024-12-02-13:34:24-root-INFO: grad norm: 211.313 195.986 79.010
2024-12-02-13:34:25-root-INFO: grad norm: 194.406 183.761 63.448
2024-12-02-13:34:26-root-INFO: grad norm: 180.001 167.564 65.747
2024-12-02-13:34:27-root-INFO: Loss Change: 3366.443 -> 3256.946
2024-12-02-13:34:27-root-INFO: Regularization Change: 0.000 -> 0.582
2024-12-02-13:34:27-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-13:34:27-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:34:27-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-13:34:27-root-INFO: grad norm: 345.018 306.070 159.243
2024-12-02-13:34:28-root-INFO: grad norm: 410.057 373.559 169.118
2024-12-02-13:34:29-root-INFO: Loss too large (3265.302->3279.755)! Learning rate decreased to 0.00090.
2024-12-02-13:34:30-root-INFO: grad norm: 359.750 332.321 137.776
2024-12-02-13:34:31-root-INFO: grad norm: 322.625 298.880 121.479
2024-12-02-13:34:32-root-INFO: grad norm: 294.470 275.547 103.857
2024-12-02-13:34:33-root-INFO: grad norm: 270.779 252.228 98.500
2024-12-02-13:34:34-root-INFO: grad norm: 251.031 236.254 84.855
2024-12-02-13:34:35-root-INFO: grad norm: 234.446 219.058 83.536
2024-12-02-13:34:35-root-INFO: Loss Change: 3274.893 -> 3167.933
2024-12-02-13:34:35-root-INFO: Regularization Change: 0.000 -> 0.653
2024-12-02-13:34:35-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-13:34:35-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-13:34:36-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-13:34:36-root-INFO: grad norm: 226.331 212.914 76.770
2024-12-02-13:34:37-root-INFO: grad norm: 292.401 274.429 100.930
2024-12-02-13:34:37-root-INFO: Loss too large (3144.649->3149.849)! Learning rate decreased to 0.00095.
2024-12-02-13:34:38-root-INFO: grad norm: 266.783 252.024 87.505
2024-12-02-13:34:39-root-INFO: grad norm: 246.103 231.968 82.204
2024-12-02-13:34:40-root-INFO: grad norm: 228.674 216.975 72.206
2024-12-02-13:34:41-root-INFO: grad norm: 214.361 202.542 70.196
2024-12-02-13:34:42-root-INFO: grad norm: 202.280 192.594 61.844
2024-12-02-13:34:43-root-INFO: grad norm: 192.425 182.140 62.067
2024-12-02-13:34:44-root-INFO: Loss Change: 3147.937 -> 3070.370
2024-12-02-13:34:44-root-INFO: Regularization Change: 0.000 -> 0.571
2024-12-02-13:34:44-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-13:34:44-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-13:34:44-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-13:34:45-root-INFO: grad norm: 370.671 327.725 173.186
2024-12-02-13:34:46-root-INFO: grad norm: 422.556 386.295 171.260
2024-12-02-13:34:46-root-INFO: Loss too large (3068.779->3087.303)! Learning rate decreased to 0.00099.
2024-12-02-13:34:47-root-INFO: grad norm: 368.398 343.576 132.939
2024-12-02-13:34:48-root-INFO: grad norm: 347.930 327.409 117.723
2024-12-02-13:34:49-root-INFO: grad norm: 350.645 334.344 105.667
2024-12-02-13:34:50-root-INFO: grad norm: 363.240 345.057 113.487
2024-12-02-13:34:51-root-INFO: grad norm: 382.053 366.357 108.386
2024-12-02-13:34:52-root-INFO: grad norm: 403.199 383.774 123.639
2024-12-02-13:34:53-root-INFO: Loss Change: 3080.623 -> 3001.650
2024-12-02-13:34:53-root-INFO: Regularization Change: 0.000 -> 0.690
2024-12-02-13:34:53-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-13:34:53-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:34:53-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-13:34:54-root-INFO: grad norm: 457.839 435.509 141.239
2024-12-02-13:34:54-root-INFO: Loss too large (2991.767->3053.004)! Learning rate decreased to 0.00104.
2024-12-02-13:34:55-root-INFO: grad norm: 477.079 456.117 139.864
2024-12-02-13:34:56-root-INFO: grad norm: 499.708 478.802 143.027
2024-12-02-13:34:57-root-INFO: grad norm: 519.286 495.736 154.610
2024-12-02-13:34:58-root-INFO: grad norm: 533.121 511.515 150.234
2024-12-02-13:34:59-root-INFO: grad norm: 539.079 514.084 162.245
2024-12-02-13:35:00-root-INFO: grad norm: 537.873 516.014 151.780
2024-12-02-13:35:01-root-INFO: grad norm: 529.908 505.125 160.160
2024-12-02-13:35:02-root-INFO: Loss Change: 2991.767 -> 2943.147
2024-12-02-13:35:02-root-INFO: Regularization Change: 0.000 -> 0.571
2024-12-02-13:35:02-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-13:35:02-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:35:02-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-13:35:02-root-INFO: grad norm: 565.512 533.363 187.957
2024-12-02-13:35:03-root-INFO: Loss too large (2934.574->3026.156)! Learning rate decreased to 0.00109.
2024-12-02-13:35:04-root-INFO: grad norm: 569.822 542.576 174.093
2024-12-02-13:35:05-root-INFO: grad norm: 587.931 563.618 167.326
2024-12-02-13:35:05-root-INFO: Loss too large (2919.058->2920.365)! Learning rate decreased to 0.00087.
2024-12-02-13:35:06-root-INFO: grad norm: 386.415 369.829 111.994
2024-12-02-13:35:07-root-INFO: grad norm: 262.168 250.535 77.229
2024-12-02-13:35:08-root-INFO: grad norm: 191.548 184.425 51.750
2024-12-02-13:35:09-root-INFO: grad norm: 149.136 142.501 43.990
2024-12-02-13:35:10-root-INFO: grad norm: 124.183 119.979 32.041
2024-12-02-13:35:10-root-INFO: Loss Change: 2934.574 -> 2788.599
2024-12-02-13:35:10-root-INFO: Regularization Change: 0.000 -> 0.527
2024-12-02-13:35:11-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-13:35:11-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:35:11-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-13:35:11-root-INFO: grad norm: 189.391 167.675 88.057
2024-12-02-13:35:12-root-INFO: grad norm: 189.046 174.970 71.582
2024-12-02-13:35:13-root-INFO: grad norm: 221.901 206.270 81.808
2024-12-02-13:35:14-root-INFO: Loss too large (2758.316->2758.345)! Learning rate decreased to 0.00114.
2024-12-02-13:35:15-root-INFO: grad norm: 216.750 206.425 66.100
2024-12-02-13:35:16-root-INFO: grad norm: 264.058 253.294 74.624
2024-12-02-13:35:16-root-INFO: Loss too large (2742.754->2748.761)! Learning rate decreased to 0.00091.
2024-12-02-13:35:17-root-INFO: grad norm: 250.527 240.589 69.863
2024-12-02-13:35:18-root-INFO: grad norm: 247.215 237.129 69.896
2024-12-02-13:35:19-root-INFO: grad norm: 248.417 238.183 70.570
2024-12-02-13:35:20-root-INFO: Loss Change: 2783.209 -> 2714.611
2024-12-02-13:35:20-root-INFO: Regularization Change: 0.000 -> 0.637
2024-12-02-13:35:20-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-13:35:20-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:35:20-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-13:35:20-root-INFO: grad norm: 241.699 226.648 83.957
2024-12-02-13:35:21-root-INFO: Loss too large (2691.130->2717.588)! Learning rate decreased to 0.00120.
2024-12-02-13:35:21-root-INFO: Loss too large (2691.130->2694.094)! Learning rate decreased to 0.00096.
2024-12-02-13:35:22-root-INFO: grad norm: 246.597 237.342 66.923
2024-12-02-13:35:23-root-INFO: grad norm: 278.234 265.853 82.075
2024-12-02-13:35:24-root-INFO: grad norm: 322.931 309.300 92.833
2024-12-02-13:35:24-root-INFO: Loss too large (2676.402->2678.519)! Learning rate decreased to 0.00077.
2024-12-02-13:35:25-root-INFO: grad norm: 249.589 238.482 73.627
2024-12-02-13:35:26-root-INFO: grad norm: 198.109 190.351 54.897
2024-12-02-13:35:27-root-INFO: grad norm: 163.824 156.528 48.347
2024-12-02-13:35:28-root-INFO: grad norm: 139.731 134.544 37.718
2024-12-02-13:35:29-root-INFO: Loss Change: 2691.130 -> 2629.990
2024-12-02-13:35:29-root-INFO: Regularization Change: 0.000 -> 0.339
2024-12-02-13:35:29-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-13:35:29-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:35:29-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-13:35:30-root-INFO: grad norm: 149.185 137.549 57.763
2024-12-02-13:35:31-root-INFO: grad norm: 225.300 215.812 64.694
2024-12-02-13:35:31-root-INFO: Loss too large (2602.340->2641.017)! Learning rate decreased to 0.00126.
2024-12-02-13:35:31-root-INFO: Loss too large (2602.340->2614.370)! Learning rate decreased to 0.00101.
2024-12-02-13:35:32-root-INFO: grad norm: 278.670 267.883 76.783
2024-12-02-13:35:33-root-INFO: Loss too large (2599.773->2607.726)! Learning rate decreased to 0.00080.
2024-12-02-13:35:34-root-INFO: grad norm: 262.661 251.613 75.376
2024-12-02-13:35:35-root-INFO: grad norm: 250.768 240.442 71.218
2024-12-02-13:35:36-root-INFO: grad norm: 241.431 231.348 69.044
2024-12-02-13:35:37-root-INFO: grad norm: 234.987 225.342 66.634
2024-12-02-13:35:38-root-INFO: grad norm: 230.500 220.924 65.750
2024-12-02-13:35:39-root-INFO: Loss Change: 2609.062 -> 2563.662
2024-12-02-13:35:39-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-13:35:39-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-13:35:39-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:35:39-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-13:35:39-root-INFO: grad norm: 226.478 212.556 78.180
2024-12-02-13:35:40-root-INFO: Loss too large (2547.004->2586.627)! Learning rate decreased to 0.00132.
2024-12-02-13:35:40-root-INFO: Loss too large (2547.004->2558.911)! Learning rate decreased to 0.00105.
2024-12-02-13:35:41-root-INFO: grad norm: 302.830 290.222 86.469
2024-12-02-13:35:41-root-INFO: Loss too large (2543.769->2564.855)! Learning rate decreased to 0.00084.
2024-12-02-13:35:42-root-INFO: grad norm: 331.291 318.069 92.660
2024-12-02-13:35:43-root-INFO: grad norm: 368.184 352.205 107.287
2024-12-02-13:35:44-root-INFO: Loss too large (2541.149->2543.406)! Learning rate decreased to 0.00067.
2024-12-02-13:35:45-root-INFO: grad norm: 265.663 254.788 75.232
2024-12-02-13:35:46-root-INFO: grad norm: 195.424 187.495 55.103
2024-12-02-13:35:47-root-INFO: grad norm: 152.000 145.637 43.517
2024-12-02-13:35:48-root-INFO: grad norm: 123.164 118.386 33.972
2024-12-02-13:35:48-root-INFO: Loss Change: 2547.004 -> 2496.542
2024-12-02-13:35:48-root-INFO: Regularization Change: 0.000 -> 0.249
2024-12-02-13:35:48-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-13:35:48-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-13:35:49-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-13:35:49-root-INFO: grad norm: 156.969 137.637 75.467
2024-12-02-13:35:50-root-INFO: grad norm: 144.110 135.330 49.531
2024-12-02-13:35:51-root-INFO: grad norm: 235.181 226.030 64.965
2024-12-02-13:35:51-root-INFO: Loss too large (2466.032->2544.885)! Learning rate decreased to 0.00138.
2024-12-02-13:35:52-root-INFO: Loss too large (2466.032->2501.860)! Learning rate decreased to 0.00110.
2024-12-02-13:35:52-root-INFO: Loss too large (2466.032->2476.932)! Learning rate decreased to 0.00088.
2024-12-02-13:35:53-root-INFO: grad norm: 291.321 279.149 83.329
2024-12-02-13:35:54-root-INFO: Loss too large (2463.275->2473.490)! Learning rate decreased to 0.00070.
2024-12-02-13:35:55-root-INFO: grad norm: 275.806 264.717 77.422
2024-12-02-13:35:56-root-INFO: grad norm: 262.290 251.329 75.031
2024-12-02-13:35:57-root-INFO: grad norm: 251.749 241.648 70.593
2024-12-02-13:35:58-root-INFO: grad norm: 242.948 232.848 69.321
2024-12-02-13:35:58-root-INFO: Loss Change: 2488.036 -> 2438.833
2024-12-02-13:35:58-root-INFO: Regularization Change: 0.000 -> 0.406
2024-12-02-13:35:58-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-13:35:58-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:35:59-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-13:35:59-root-INFO: grad norm: 213.872 205.184 60.339
2024-12-02-13:35:59-root-INFO: Loss too large (2421.189->2511.506)! Learning rate decreased to 0.00144.
2024-12-02-13:36:00-root-INFO: Loss too large (2421.189->2466.284)! Learning rate decreased to 0.00115.
2024-12-02-13:36:00-root-INFO: Loss too large (2421.189->2439.542)! Learning rate decreased to 0.00092.
2024-12-02-13:36:00-root-INFO: Loss too large (2421.189->2424.431)! Learning rate decreased to 0.00074.
2024-12-02-13:36:01-root-INFO: grad norm: 214.640 205.736 61.179
2024-12-02-13:36:02-root-INFO: grad norm: 219.674 211.181 60.492
2024-12-02-13:36:03-root-INFO: grad norm: 227.216 218.001 64.049
2024-12-02-13:36:04-root-INFO: grad norm: 236.385 227.367 64.667
2024-12-02-13:36:05-root-INFO: grad norm: 247.866 237.815 69.867
2024-12-02-13:36:06-root-INFO: grad norm: 260.548 250.701 70.950
2024-12-02-13:36:07-root-INFO: grad norm: 275.608 264.404 77.783
2024-12-02-13:36:08-root-INFO: Loss Change: 2421.189 -> 2397.063
2024-12-02-13:36:08-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-02-13:36:08-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-13:36:08-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:36:08-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-13:36:09-root-INFO: grad norm: 260.283 243.757 91.266
2024-12-02-13:36:09-root-INFO: Loss too large (2394.357->2465.362)! Learning rate decreased to 0.00150.
2024-12-02-13:36:09-root-INFO: Loss too large (2394.357->2419.443)! Learning rate decreased to 0.00120.
2024-12-02-13:36:10-root-INFO: grad norm: 443.160 424.272 128.001
2024-12-02-13:36:11-root-INFO: Loss too large (2393.908->2528.825)! Learning rate decreased to 0.00096.
2024-12-02-13:36:11-root-INFO: Loss too large (2393.908->2448.674)! Learning rate decreased to 0.00077.
2024-12-02-13:36:11-root-INFO: Loss too large (2393.908->2400.975)! Learning rate decreased to 0.00062.
2024-12-02-13:36:12-root-INFO: grad norm: 335.659 322.991 91.344
2024-12-02-13:36:13-root-INFO: grad norm: 255.971 244.719 75.058
2024-12-02-13:36:14-root-INFO: grad norm: 204.423 196.409 56.678
2024-12-02-13:36:15-root-INFO: grad norm: 165.933 158.584 48.836
2024-12-02-13:36:16-root-INFO: grad norm: 139.539 133.803 39.595
2024-12-02-13:36:17-root-INFO: grad norm: 119.893 114.566 35.340
2024-12-02-13:36:18-root-INFO: Loss Change: 2394.357 -> 2334.251
2024-12-02-13:36:18-root-INFO: Regularization Change: 0.000 -> 0.255
2024-12-02-13:36:18-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-13:36:18-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:36:18-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-13:36:19-root-INFO: grad norm: 125.361 118.417 41.143
2024-12-02-13:36:19-root-INFO: Loss too large (2328.052->2350.570)! Learning rate decreased to 0.00157.
2024-12-02-13:36:19-root-INFO: Loss too large (2328.052->2337.387)! Learning rate decreased to 0.00126.
2024-12-02-13:36:20-root-INFO: Loss too large (2328.052->2329.981)! Learning rate decreased to 0.00101.
2024-12-02-13:36:21-root-INFO: grad norm: 197.473 190.086 53.506
2024-12-02-13:36:21-root-INFO: Loss too large (2326.065->2336.108)! Learning rate decreased to 0.00081.
2024-12-02-13:36:21-root-INFO: Loss too large (2326.065->2326.316)! Learning rate decreased to 0.00064.
2024-12-02-13:36:22-root-INFO: grad norm: 173.851 166.532 49.915
2024-12-02-13:36:23-root-INFO: grad norm: 155.067 149.301 41.896
2024-12-02-13:36:24-root-INFO: grad norm: 140.900 135.072 40.105
2024-12-02-13:36:25-root-INFO: grad norm: 129.530 124.679 35.118
2024-12-02-13:36:26-root-INFO: grad norm: 120.773 115.865 34.080
2024-12-02-13:36:27-root-INFO: grad norm: 113.663 109.366 30.956
2024-12-02-13:36:28-root-INFO: Loss Change: 2328.052 -> 2299.429
2024-12-02-13:36:28-root-INFO: Regularization Change: 0.000 -> 0.153
2024-12-02-13:36:28-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-13:36:28-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:36:28-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-13:36:29-root-INFO: grad norm: 150.288 133.887 68.271
2024-12-02-13:36:29-root-INFO: Loss too large (2292.217->2298.422)! Learning rate decreased to 0.00165.
2024-12-02-13:36:30-root-INFO: grad norm: 303.245 290.902 85.638
2024-12-02-13:36:30-root-INFO: Loss too large (2288.980->2482.218)! Learning rate decreased to 0.00132.
2024-12-02-13:36:31-root-INFO: Loss too large (2288.980->2391.423)! Learning rate decreased to 0.00105.
2024-12-02-13:36:31-root-INFO: Loss too large (2288.980->2335.737)! Learning rate decreased to 0.00084.
2024-12-02-13:36:31-root-INFO: Loss too large (2288.980->2303.246)! Learning rate decreased to 0.00067.
2024-12-02-13:36:32-root-INFO: grad norm: 313.953 301.934 86.038
2024-12-02-13:36:33-root-INFO: grad norm: 328.027 316.412 86.519
2024-12-02-13:36:34-root-INFO: grad norm: 344.611 331.193 95.225
2024-12-02-13:36:35-root-INFO: grad norm: 360.285 347.761 94.168
2024-12-02-13:36:36-root-INFO: grad norm: 376.877 362.294 103.824
2024-12-02-13:36:37-root-INFO: Loss too large (2279.939->2280.236)! Learning rate decreased to 0.00054.
2024-12-02-13:36:38-root-INFO: grad norm: 251.193 242.178 66.693
2024-12-02-13:36:38-root-INFO: Loss Change: 2292.217 -> 2256.894
2024-12-02-13:36:38-root-INFO: Regularization Change: 0.000 -> 0.210
2024-12-02-13:36:38-root-INFO: Undo step: 200
2024-12-02-13:36:38-root-INFO: Undo step: 201
2024-12-02-13:36:38-root-INFO: Undo step: 202
2024-12-02-13:36:38-root-INFO: Undo step: 203
2024-12-02-13:36:38-root-INFO: Undo step: 204
2024-12-02-13:36:38-root-INFO: Undo step: 205
2024-12-02-13:36:38-root-INFO: Undo step: 206
2024-12-02-13:36:38-root-INFO: Undo step: 207
2024-12-02-13:36:38-root-INFO: Undo step: 208
2024-12-02-13:36:39-root-INFO: Undo step: 209
2024-12-02-13:36:39-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-13:36:39-root-INFO: grad norm: 4449.465 4032.833 1879.892
2024-12-02-13:36:40-root-INFO: grad norm: 2123.755 1943.938 855.242
2024-12-02-13:36:41-root-INFO: grad norm: 1209.298 1107.100 486.551
2024-12-02-13:36:42-root-INFO: grad norm: 882.913 829.372 302.783
2024-12-02-13:36:43-root-INFO: grad norm: 691.310 640.705 259.629
2024-12-02-13:36:44-root-INFO: grad norm: 750.234 707.273 250.233
2024-12-02-13:36:45-root-INFO: Loss too large (3503.576->3539.632)! Learning rate decreased to 0.00104.
2024-12-02-13:36:46-root-INFO: grad norm: 674.757 643.236 203.826
2024-12-02-13:36:47-root-INFO: grad norm: 672.650 628.806 238.874
2024-12-02-13:36:47-root-INFO: Loss Change: 9337.408 -> 3328.898
2024-12-02-13:36:47-root-INFO: Regularization Change: 0.000 -> 25.475
2024-12-02-13:36:47-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-13:36:47-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:36:48-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-13:36:48-root-INFO: grad norm: 610.788 588.742 162.619
2024-12-02-13:36:48-root-INFO: Loss too large (3271.724->3343.405)! Learning rate decreased to 0.00109.
2024-12-02-13:36:49-root-INFO: grad norm: 617.149 580.415 209.742
2024-12-02-13:36:50-root-INFO: grad norm: 633.358 605.122 187.002
2024-12-02-13:36:51-root-INFO: grad norm: 653.451 616.258 217.309
2024-12-02-13:36:52-root-INFO: grad norm: 675.576 643.281 206.381
2024-12-02-13:36:53-root-INFO: grad norm: 699.913 661.476 228.754
2024-12-02-13:36:54-root-INFO: grad norm: 723.411 687.350 225.553
2024-12-02-13:36:55-root-INFO: grad norm: 746.639 706.290 242.125
2024-12-02-13:36:56-root-INFO: Loss Change: 3271.724 -> 3130.904
2024-12-02-13:36:56-root-INFO: Regularization Change: 0.000 -> 1.767
2024-12-02-13:36:56-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-13:36:56-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:36:56-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-13:36:57-root-INFO: grad norm: 707.697 679.016 199.430
2024-12-02-13:36:57-root-INFO: Loss too large (3088.876->3234.026)! Learning rate decreased to 0.00114.
2024-12-02-13:36:58-root-INFO: grad norm: 716.519 682.126 219.324
2024-12-02-13:36:59-root-INFO: grad norm: 727.793 692.966 222.443
2024-12-02-13:37:00-root-INFO: grad norm: 735.928 699.113 229.851
2024-12-02-13:37:01-root-INFO: grad norm: 741.279 703.515 233.584
2024-12-02-13:37:02-root-INFO: grad norm: 742.721 704.319 235.732
2024-12-02-13:37:03-root-INFO: grad norm: 741.454 702.327 237.678
2024-12-02-13:37:04-root-INFO: grad norm: 736.570 697.414 236.959
2024-12-02-13:37:05-root-INFO: Loss Change: 3088.876 -> 2972.743
2024-12-02-13:37:05-root-INFO: Regularization Change: 0.000 -> 1.194
2024-12-02-13:37:05-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-13:37:05-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:37:05-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-13:37:05-root-INFO: grad norm: 680.032 651.789 193.945
2024-12-02-13:37:06-root-INFO: Loss too large (2928.876->3055.090)! Learning rate decreased to 0.00120.
2024-12-02-13:37:07-root-INFO: grad norm: 649.170 618.660 196.675
2024-12-02-13:37:08-root-INFO: grad norm: 623.708 595.465 185.562
2024-12-02-13:37:09-root-INFO: grad norm: 598.192 570.108 181.138
2024-12-02-13:37:10-root-INFO: grad norm: 575.586 548.863 173.347
2024-12-02-13:37:11-root-INFO: grad norm: 553.539 527.670 167.242
2024-12-02-13:37:12-root-INFO: grad norm: 534.032 509.104 161.256
2024-12-02-13:37:13-root-INFO: grad norm: 515.626 491.775 155.009
2024-12-02-13:37:13-root-INFO: Loss Change: 2928.876 -> 2763.611
2024-12-02-13:37:13-root-INFO: Regularization Change: 0.000 -> 0.957
2024-12-02-13:37:13-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-13:37:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:37:14-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-13:37:14-root-INFO: grad norm: 470.139 454.294 121.026
2024-12-02-13:37:14-root-INFO: Loss too large (2729.644->2788.269)! Learning rate decreased to 0.00126.
2024-12-02-13:37:15-root-INFO: grad norm: 454.465 437.821 121.867
2024-12-02-13:37:16-root-INFO: grad norm: 443.687 427.348 119.297
2024-12-02-13:37:17-root-INFO: grad norm: 433.982 418.068 116.446
2024-12-02-13:37:18-root-INFO: grad norm: 425.838 409.883 115.470
2024-12-02-13:37:19-root-INFO: grad norm: 418.495 403.174 112.199
2024-12-02-13:37:20-root-INFO: grad norm: 412.403 396.934 111.891
2024-12-02-13:37:21-root-INFO: grad norm: 406.975 392.158 108.814
2024-12-02-13:37:22-root-INFO: Loss Change: 2729.644 -> 2628.462
2024-12-02-13:37:22-root-INFO: Regularization Change: 0.000 -> 0.819
2024-12-02-13:37:22-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-13:37:22-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-13:37:22-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-13:37:23-root-INFO: grad norm: 427.355 412.626 111.231
2024-12-02-13:37:23-root-INFO: Loss too large (2613.407->2673.874)! Learning rate decreased to 0.00132.
2024-12-02-13:37:24-root-INFO: grad norm: 434.696 420.993 108.286
2024-12-02-13:37:25-root-INFO: grad norm: 444.380 429.718 113.208
2024-12-02-13:37:26-root-INFO: grad norm: 451.903 437.430 113.451
2024-12-02-13:37:27-root-INFO: grad norm: 455.413 440.117 117.037
2024-12-02-13:37:28-root-INFO: grad norm: 455.412 440.741 114.663
2024-12-02-13:37:29-root-INFO: grad norm: 451.320 436.101 116.215
2024-12-02-13:37:30-root-INFO: grad norm: 444.736 430.287 112.442
2024-12-02-13:37:31-root-INFO: Loss Change: 2613.407 -> 2546.539
2024-12-02-13:37:31-root-INFO: Regularization Change: 0.000 -> 0.769
2024-12-02-13:37:31-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-13:37:31-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-13:37:31-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-13:37:32-root-INFO: grad norm: 499.125 478.704 141.306
2024-12-02-13:37:32-root-INFO: Loss too large (2548.025->2636.154)! Learning rate decreased to 0.00138.
2024-12-02-13:37:33-root-INFO: grad norm: 495.125 478.152 128.527
2024-12-02-13:37:34-root-INFO: grad norm: 489.166 472.115 128.027
2024-12-02-13:37:35-root-INFO: grad norm: 480.150 463.694 124.630
2024-12-02-13:37:36-root-INFO: grad norm: 465.342 449.356 120.921
2024-12-02-13:37:37-root-INFO: grad norm: 450.191 434.845 116.542
2024-12-02-13:37:38-root-INFO: grad norm: 432.854 418.191 111.708
2024-12-02-13:37:39-root-INFO: grad norm: 417.723 403.572 107.806
2024-12-02-13:37:40-root-INFO: Loss Change: 2548.025 -> 2453.215
2024-12-02-13:37:40-root-INFO: Regularization Change: 0.000 -> 0.768
2024-12-02-13:37:40-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-13:37:40-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:37:40-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-13:37:40-root-INFO: grad norm: 419.826 406.451 105.125
2024-12-02-13:37:41-root-INFO: Loss too large (2443.824->2507.970)! Learning rate decreased to 0.00144.
2024-12-02-13:37:42-root-INFO: grad norm: 411.216 397.661 104.709
2024-12-02-13:37:43-root-INFO: grad norm: 402.849 389.812 101.655
2024-12-02-13:37:44-root-INFO: grad norm: 395.547 382.662 100.136
2024-12-02-13:37:45-root-INFO: grad norm: 388.859 376.197 98.421
2024-12-02-13:37:46-root-INFO: grad norm: 383.816 371.421 96.755
2024-12-02-13:37:47-root-INFO: grad norm: 379.951 367.565 96.221
2024-12-02-13:37:48-root-INFO: grad norm: 377.611 365.493 94.892
2024-12-02-13:37:48-root-INFO: Loss Change: 2443.824 -> 2372.325
2024-12-02-13:37:48-root-INFO: Regularization Change: 0.000 -> 0.677
2024-12-02-13:37:48-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-13:37:48-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:37:49-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-13:37:49-root-INFO: grad norm: 547.906 514.689 187.872
2024-12-02-13:37:49-root-INFO: Loss too large (2410.592->2532.906)! Learning rate decreased to 0.00150.
2024-12-02-13:37:50-root-INFO: Loss too large (2410.592->2413.453)! Learning rate decreased to 0.00120.
2024-12-02-13:37:51-root-INFO: grad norm: 368.365 355.040 98.182
2024-12-02-13:37:52-root-INFO: grad norm: 244.175 234.397 68.407
2024-12-02-13:37:53-root-INFO: grad norm: 183.571 176.894 49.060
2024-12-02-13:37:54-root-INFO: grad norm: 143.913 138.809 37.985
2024-12-02-13:37:55-root-INFO: grad norm: 121.066 116.639 32.443
2024-12-02-13:37:56-root-INFO: grad norm: 106.043 102.352 27.731
2024-12-02-13:37:57-root-INFO: grad norm: 96.682 93.124 25.986
2024-12-02-13:37:57-root-INFO: Loss Change: 2410.592 -> 2249.560
2024-12-02-13:37:57-root-INFO: Regularization Change: 0.000 -> 0.655
2024-12-02-13:37:57-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-13:37:57-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:37:58-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-13:37:58-root-INFO: grad norm: 96.192 91.956 28.231
2024-12-02-13:37:59-root-INFO: grad norm: 131.610 126.240 37.210
2024-12-02-13:37:59-root-INFO: Loss too large (2235.747->2239.587)! Learning rate decreased to 0.00157.
2024-12-02-13:38:00-root-INFO: grad norm: 178.299 172.450 45.296
2024-12-02-13:38:01-root-INFO: Loss too large (2232.554->2236.209)! Learning rate decreased to 0.00126.
2024-12-02-13:38:02-root-INFO: grad norm: 177.152 171.116 45.847
2024-12-02-13:38:03-root-INFO: grad norm: 179.049 173.500 44.231
2024-12-02-13:38:04-root-INFO: grad norm: 183.658 177.733 46.274
2024-12-02-13:38:05-root-INFO: grad norm: 191.645 185.743 47.196
2024-12-02-13:38:06-root-INFO: grad norm: 201.898 195.603 50.025
2024-12-02-13:38:06-root-INFO: Loss Change: 2243.705 -> 2203.950
2024-12-02-13:38:06-root-INFO: Regularization Change: 0.000 -> 0.562
2024-12-02-13:38:06-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-13:38:06-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:38:07-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-13:38:07-root-INFO: grad norm: 388.422 366.099 129.780
2024-12-02-13:38:07-root-INFO: Loss too large (2218.932->2373.658)! Learning rate decreased to 0.00165.
2024-12-02-13:38:08-root-INFO: Loss too large (2218.932->2279.626)! Learning rate decreased to 0.00132.
2024-12-02-13:38:08-root-INFO: Loss too large (2218.932->2223.083)! Learning rate decreased to 0.00105.
2024-12-02-13:38:09-root-INFO: grad norm: 284.597 276.374 67.921
2024-12-02-13:38:10-root-INFO: grad norm: 210.564 203.285 54.885
2024-12-02-13:38:11-root-INFO: grad norm: 166.888 161.941 40.330
2024-12-02-13:38:12-root-INFO: grad norm: 134.769 130.338 34.270
2024-12-02-13:38:13-root-INFO: grad norm: 114.322 110.739 28.398
2024-12-02-13:38:14-root-INFO: grad norm: 99.542 96.233 25.453
2024-12-02-13:38:15-root-INFO: grad norm: 89.709 86.778 22.744
2024-12-02-13:38:16-root-INFO: Loss Change: 2218.932 -> 2136.528
2024-12-02-13:38:16-root-INFO: Regularization Change: 0.000 -> 0.361
2024-12-02-13:38:16-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-13:38:16-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:38:16-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-13:38:16-root-INFO: grad norm: 159.973 149.230 57.635
2024-12-02-13:38:17-root-INFO: Loss too large (2125.871->2145.357)! Learning rate decreased to 0.00172.
2024-12-02-13:38:17-root-INFO: Loss too large (2125.871->2130.060)! Learning rate decreased to 0.00138.
2024-12-02-13:38:18-root-INFO: grad norm: 189.905 184.051 46.786
2024-12-02-13:38:18-root-INFO: Loss too large (2121.746->2123.969)! Learning rate decreased to 0.00110.
2024-12-02-13:38:19-root-INFO: grad norm: 174.099 168.596 43.426
2024-12-02-13:38:20-root-INFO: grad norm: 164.037 159.535 38.167
2024-12-02-13:38:21-root-INFO: grad norm: 155.976 151.264 38.049
2024-12-02-13:38:22-root-INFO: grad norm: 150.632 146.509 35.001
2024-12-02-13:38:23-root-INFO: grad norm: 146.758 142.365 35.640
2024-12-02-13:38:25-root-INFO: grad norm: 144.714 140.780 33.512
2024-12-02-13:38:25-root-INFO: Loss Change: 2125.871 -> 2086.969
2024-12-02-13:38:25-root-INFO: Regularization Change: 0.000 -> 0.330
2024-12-02-13:38:25-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-13:38:25-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-13:38:26-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-13:38:26-root-INFO: grad norm: 266.800 251.790 88.226
2024-12-02-13:38:26-root-INFO: Loss too large (2093.747->2200.438)! Learning rate decreased to 0.00180.
2024-12-02-13:38:27-root-INFO: Loss too large (2093.747->2141.903)! Learning rate decreased to 0.00144.
2024-12-02-13:38:27-root-INFO: Loss too large (2093.747->2106.608)! Learning rate decreased to 0.00115.
2024-12-02-13:38:28-root-INFO: grad norm: 265.048 258.021 60.629
2024-12-02-13:38:29-root-INFO: grad norm: 283.119 274.877 67.815
2024-12-02-13:38:29-root-INFO: Loss too large (2082.521->2083.408)! Learning rate decreased to 0.00092.
2024-12-02-13:38:30-root-INFO: grad norm: 198.940 193.661 45.527
2024-12-02-13:38:31-root-INFO: grad norm: 139.319 135.186 33.683
2024-12-02-13:38:32-root-INFO: grad norm: 106.716 103.636 25.453
2024-12-02-13:38:33-root-INFO: grad norm: 85.970 83.177 21.737
2024-12-02-13:38:34-root-INFO: grad norm: 74.533 72.168 18.624
2024-12-02-13:38:35-root-INFO: Loss Change: 2093.747 -> 2041.656
2024-12-02-13:38:35-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-13:38:35-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-13:38:35-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:38:35-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-13:38:36-root-INFO: grad norm: 162.626 153.465 53.812
2024-12-02-13:38:36-root-INFO: Loss too large (2029.920->2065.189)! Learning rate decreased to 0.00188.
2024-12-02-13:38:36-root-INFO: Loss too large (2029.920->2043.726)! Learning rate decreased to 0.00150.
2024-12-02-13:38:37-root-INFO: Loss too large (2029.920->2031.353)! Learning rate decreased to 0.00120.
2024-12-02-13:38:38-root-INFO: grad norm: 173.919 169.997 36.725
2024-12-02-13:38:39-root-INFO: grad norm: 206.860 201.003 48.876
2024-12-02-13:38:39-root-INFO: Loss too large (2022.881->2024.171)! Learning rate decreased to 0.00096.
2024-12-02-13:38:40-root-INFO: grad norm: 165.711 161.908 35.296
2024-12-02-13:38:41-root-INFO: grad norm: 132.553 128.675 31.829
2024-12-02-13:38:42-root-INFO: grad norm: 110.782 108.075 24.340
2024-12-02-13:38:43-root-INFO: grad norm: 94.255 91.287 23.468
2024-12-02-13:38:44-root-INFO: grad norm: 83.229 81.001 19.128
2024-12-02-13:38:45-root-INFO: Loss Change: 2029.920 -> 1992.689
2024-12-02-13:38:45-root-INFO: Regularization Change: 0.000 -> 0.255
2024-12-02-13:38:45-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-13:38:45-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:38:45-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-13:38:46-root-INFO: grad norm: 193.660 181.521 67.487
2024-12-02-13:38:46-root-INFO: Loss too large (1990.303->2042.063)! Learning rate decreased to 0.00196.
2024-12-02-13:38:46-root-INFO: Loss too large (1990.303->2011.290)! Learning rate decreased to 0.00157.
2024-12-02-13:38:47-root-INFO: Loss too large (1990.303->1993.222)! Learning rate decreased to 0.00126.
2024-12-02-13:38:48-root-INFO: grad norm: 208.260 203.235 45.474
2024-12-02-13:38:48-root-INFO: Loss too large (1983.370->1985.752)! Learning rate decreased to 0.00100.
2024-12-02-13:38:49-root-INFO: grad norm: 182.230 177.161 42.682
2024-12-02-13:38:50-root-INFO: grad norm: 163.307 159.404 35.487
2024-12-02-13:38:51-root-INFO: grad norm: 145.919 141.930 33.885
2024-12-02-13:38:52-root-INFO: grad norm: 132.855 129.653 28.992
2024-12-02-13:38:53-root-INFO: grad norm: 121.072 117.697 28.388
2024-12-02-13:38:54-root-INFO: grad norm: 111.944 109.194 24.663
2024-12-02-13:38:55-root-INFO: Loss Change: 1990.303 -> 1948.675
2024-12-02-13:38:55-root-INFO: Regularization Change: 0.000 -> 0.275
2024-12-02-13:38:55-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-13:38:55-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:38:55-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-13:38:55-root-INFO: grad norm: 159.796 154.273 41.647
2024-12-02-13:38:56-root-INFO: Loss too large (1945.137->2002.678)! Learning rate decreased to 0.00205.
2024-12-02-13:38:56-root-INFO: Loss too large (1945.137->1973.218)! Learning rate decreased to 0.00164.
2024-12-02-13:38:56-root-INFO: Loss too large (1945.137->1955.533)! Learning rate decreased to 0.00131.
2024-12-02-13:38:57-root-INFO: Loss too large (1945.137->1945.464)! Learning rate decreased to 0.00105.
2024-12-02-13:38:58-root-INFO: grad norm: 147.259 143.454 33.256
2024-12-02-13:38:59-root-INFO: grad norm: 137.345 133.690 31.475
2024-12-02-13:39:00-root-INFO: grad norm: 129.509 126.298 28.658
2024-12-02-13:39:01-root-INFO: grad norm: 122.110 118.906 27.789
2024-12-02-13:39:02-root-INFO: grad norm: 116.054 113.202 25.571
2024-12-02-13:39:03-root-INFO: grad norm: 110.323 107.369 25.362
2024-12-02-13:39:04-root-INFO: grad norm: 105.515 102.914 23.284
2024-12-02-13:39:04-root-INFO: Loss Change: 1945.137 -> 1912.227
2024-12-02-13:39:04-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-13:39:04-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-13:39:04-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:39:05-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-13:39:05-root-INFO: grad norm: 214.399 204.034 65.855
2024-12-02-13:39:05-root-INFO: Loss too large (1910.200->2017.721)! Learning rate decreased to 0.00214.
2024-12-02-13:39:06-root-INFO: Loss too large (1910.200->1964.802)! Learning rate decreased to 0.00171.
2024-12-02-13:39:06-root-INFO: Loss too large (1910.200->1932.034)! Learning rate decreased to 0.00137.
2024-12-02-13:39:06-root-INFO: Loss too large (1910.200->1912.836)! Learning rate decreased to 0.00110.
2024-12-02-13:39:07-root-INFO: grad norm: 195.005 191.142 38.621
2024-12-02-13:39:08-root-INFO: grad norm: 190.454 185.520 43.068
2024-12-02-13:39:09-root-INFO: grad norm: 186.273 182.312 38.207
2024-12-02-13:39:10-root-INFO: grad norm: 181.280 176.908 39.572
2024-12-02-13:39:11-root-INFO: grad norm: 176.559 172.804 36.221
2024-12-02-13:39:12-root-INFO: grad norm: 171.007 166.923 37.152
2024-12-02-13:39:14-root-INFO: grad norm: 165.872 162.346 34.020
2024-12-02-13:39:14-root-INFO: Loss Change: 1910.200 -> 1873.224
2024-12-02-13:39:14-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-13:39:14-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-13:39:14-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:39:14-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-13:39:15-root-INFO: grad norm: 275.992 265.430 75.619
2024-12-02-13:39:15-root-INFO: Loss too large (1876.051->2065.397)! Learning rate decreased to 0.00224.
2024-12-02-13:39:16-root-INFO: Loss too large (1876.051->1976.778)! Learning rate decreased to 0.00179.
2024-12-02-13:39:16-root-INFO: Loss too large (1876.051->1919.975)! Learning rate decreased to 0.00143.
2024-12-02-13:39:16-root-INFO: Loss too large (1876.051->1885.615)! Learning rate decreased to 0.00114.
2024-12-02-13:39:17-root-INFO: grad norm: 241.018 236.361 47.152
2024-12-02-13:39:18-root-INFO: grad norm: 224.401 218.905 49.359
2024-12-02-13:39:19-root-INFO: grad norm: 210.617 206.295 42.448
2024-12-02-13:39:20-root-INFO: grad norm: 194.708 190.057 42.303
2024-12-02-13:39:21-root-INFO: grad norm: 181.691 177.949 36.687
2024-12-02-13:39:22-root-INFO: grad norm: 167.504 163.461 36.580
2024-12-02-13:39:23-root-INFO: grad norm: 155.908 152.669 31.612
2024-12-02-13:39:24-root-INFO: Loss Change: 1876.051 -> 1828.789
2024-12-02-13:39:24-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-02-13:39:24-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-13:39:24-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-13:39:24-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-13:39:25-root-INFO: grad norm: 235.298 228.122 57.670
2024-12-02-13:39:25-root-INFO: Loss too large (1826.333->1984.166)! Learning rate decreased to 0.00233.
2024-12-02-13:39:25-root-INFO: Loss too large (1826.333->1910.709)! Learning rate decreased to 0.00187.
2024-12-02-13:39:26-root-INFO: Loss too large (1826.333->1864.078)! Learning rate decreased to 0.00149.
2024-12-02-13:39:26-root-INFO: Loss too large (1826.333->1836.034)! Learning rate decreased to 0.00119.
2024-12-02-13:39:27-root-INFO: grad norm: 212.377 208.503 40.375
2024-12-02-13:39:28-root-INFO: grad norm: 193.537 188.923 42.008
2024-12-02-13:39:29-root-INFO: grad norm: 178.001 174.547 34.897
2024-12-02-13:39:30-root-INFO: grad norm: 161.109 157.273 34.947
2024-12-02-13:39:31-root-INFO: grad norm: 147.441 144.504 29.283
2024-12-02-13:39:32-root-INFO: grad norm: 133.407 130.131 29.384
2024-12-02-13:39:33-root-INFO: grad norm: 122.075 119.558 24.658
2024-12-02-13:39:34-root-INFO: Loss Change: 1826.333 -> 1785.737
2024-12-02-13:39:34-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-13:39:34-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-13:39:34-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-13:39:34-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-13:39:34-root-INFO: grad norm: 290.292 278.689 81.249
2024-12-02-13:39:35-root-INFO: Loss too large (1801.809->1945.142)! Learning rate decreased to 0.00244.
2024-12-02-13:39:35-root-INFO: Loss too large (1801.809->1874.189)! Learning rate decreased to 0.00195.
2024-12-02-13:39:35-root-INFO: Loss too large (1801.809->1827.790)! Learning rate decreased to 0.00156.
2024-12-02-13:39:36-root-INFO: grad norm: 271.961 266.655 53.461
2024-12-02-13:39:37-root-INFO: grad norm: 334.410 327.591 67.186
2024-12-02-13:39:38-root-INFO: Loss too large (1796.079->1821.021)! Learning rate decreased to 0.00125.
2024-12-02-13:39:39-root-INFO: grad norm: 281.344 275.777 55.695
2024-12-02-13:39:40-root-INFO: grad norm: 222.684 217.770 46.523
2024-12-02-13:39:41-root-INFO: grad norm: 187.136 183.259 37.896
2024-12-02-13:39:42-root-INFO: grad norm: 152.968 149.396 32.867
2024-12-02-13:39:43-root-INFO: grad norm: 130.203 127.354 27.089
2024-12-02-13:39:43-root-INFO: Loss Change: 1801.809 -> 1745.278
2024-12-02-13:39:43-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-02-13:39:43-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-13:39:43-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-13:39:44-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-13:39:44-root-INFO: grad norm: 205.372 198.472 52.785
2024-12-02-13:39:44-root-INFO: Loss too large (1747.948->1845.653)! Learning rate decreased to 0.00254.
2024-12-02-13:39:45-root-INFO: Loss too large (1747.948->1796.348)! Learning rate decreased to 0.00203.
2024-12-02-13:39:45-root-INFO: Loss too large (1747.948->1765.660)! Learning rate decreased to 0.00163.
2024-12-02-13:39:46-root-INFO: grad norm: 215.070 211.022 41.534
2024-12-02-13:39:47-root-INFO: grad norm: 250.528 245.273 51.043
2024-12-02-13:39:47-root-INFO: Loss too large (1745.208->1754.450)! Learning rate decreased to 0.00130.
2024-12-02-13:39:48-root-INFO: grad norm: 200.566 196.645 39.468
2024-12-02-13:39:49-root-INFO: grad norm: 155.407 151.901 32.826
2024-12-02-13:39:50-root-INFO: grad norm: 127.727 124.938 26.542
2024-12-02-13:39:51-root-INFO: grad norm: 104.061 101.464 23.103
2024-12-02-13:39:52-root-INFO: grad norm: 88.437 86.244 19.570
2024-12-02-13:39:53-root-INFO: Loss Change: 1747.948 -> 1706.799
2024-12-02-13:39:53-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-02-13:39:53-root-INFO: Undo step: 190
2024-12-02-13:39:53-root-INFO: Undo step: 191
2024-12-02-13:39:53-root-INFO: Undo step: 192
2024-12-02-13:39:53-root-INFO: Undo step: 193
2024-12-02-13:39:53-root-INFO: Undo step: 194
2024-12-02-13:39:53-root-INFO: Undo step: 195
2024-12-02-13:39:53-root-INFO: Undo step: 196
2024-12-02-13:39:53-root-INFO: Undo step: 197
2024-12-02-13:39:53-root-INFO: Undo step: 198
2024-12-02-13:39:53-root-INFO: Undo step: 199
2024-12-02-13:39:53-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-13:39:54-root-INFO: grad norm: 1306.171 1119.851 672.322
2024-12-02-13:39:55-root-INFO: grad norm: 1036.274 948.848 416.595
2024-12-02-13:39:55-root-INFO: Loss too large (3343.650->3710.224)! Learning rate decreased to 0.00165.
2024-12-02-13:39:55-root-INFO: grad norm: 1018.789 961.190 337.705
2024-12-02-13:39:56-root-INFO: grad norm: 971.000 922.893 301.842
2024-12-02-13:39:56-root-INFO: grad norm: 939.570 891.070 297.970
2024-12-02-13:39:57-root-INFO: grad norm: 890.718 846.288 277.806
2024-12-02-13:39:58-root-INFO: grad norm: 853.820 810.228 269.332
2024-12-02-13:39:58-root-INFO: grad norm: 811.307 768.977 258.636
2024-12-02-13:39:59-root-INFO: Loss Change: 4873.662 -> 2774.235
2024-12-02-13:39:59-root-INFO: Regularization Change: 0.000 -> 15.838
2024-12-02-13:39:59-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-13:39:59-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-13:40:00-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-13:40:00-root-INFO: grad norm: 731.849 695.530 227.683
2024-12-02-13:40:00-root-INFO: Loss too large (2721.616->2888.329)! Learning rate decreased to 0.00172.
2024-12-02-13:40:01-root-INFO: grad norm: 703.296 665.480 227.510
2024-12-02-13:40:02-root-INFO: grad norm: 676.963 641.770 215.432
2024-12-02-13:40:03-root-INFO: grad norm: 646.611 611.398 210.471
2024-12-02-13:40:04-root-INFO: grad norm: 618.955 586.919 196.547
2024-12-02-13:40:05-root-INFO: grad norm: 584.856 552.765 191.069
2024-12-02-13:40:06-root-INFO: grad norm: 558.918 530.202 176.849
2024-12-02-13:40:07-root-INFO: grad norm: 528.660 499.699 172.576
2024-12-02-13:40:08-root-INFO: Loss Change: 2721.616 -> 2404.065
2024-12-02-13:40:08-root-INFO: Regularization Change: 0.000 -> 2.628
2024-12-02-13:40:08-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-13:40:08-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-13:40:08-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-13:40:08-root-INFO: grad norm: 459.752 437.937 139.940
2024-12-02-13:40:09-root-INFO: Loss too large (2369.595->2439.437)! Learning rate decreased to 0.00180.
2024-12-02-13:40:10-root-INFO: grad norm: 451.193 428.093 142.515
2024-12-02-13:40:10-root-INFO: Loss too large (2330.882->2330.907)! Learning rate decreased to 0.00144.
2024-12-02-13:40:11-root-INFO: grad norm: 304.381 288.615 96.691
2024-12-02-13:40:12-root-INFO: grad norm: 206.328 196.379 63.297
2024-12-02-13:40:13-root-INFO: grad norm: 161.041 152.742 51.031
2024-12-02-13:40:14-root-INFO: grad norm: 132.635 126.442 40.054
2024-12-02-13:40:15-root-INFO: grad norm: 117.239 111.338 36.726
2024-12-02-13:40:16-root-INFO: grad norm: 107.173 102.147 32.433
2024-12-02-13:40:17-root-INFO: Loss Change: 2369.595 -> 2169.737
2024-12-02-13:40:17-root-INFO: Regularization Change: 0.000 -> 1.305
2024-12-02-13:40:17-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-13:40:17-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:40:17-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-13:40:17-root-INFO: grad norm: 100.864 94.482 35.311
2024-12-02-13:40:18-root-INFO: grad norm: 85.010 80.104 28.463
2024-12-02-13:40:19-root-INFO: grad norm: 81.603 77.948 24.149
2024-12-02-13:40:20-root-INFO: grad norm: 82.869 78.406 26.827
2024-12-02-13:40:21-root-INFO: grad norm: 96.065 92.182 27.035
2024-12-02-13:40:22-root-INFO: grad norm: 154.349 148.031 43.710
2024-12-02-13:40:23-root-INFO: Loss too large (2075.097->2090.754)! Learning rate decreased to 0.00188.
2024-12-02-13:40:23-root-INFO: Loss too large (2075.097->2076.806)! Learning rate decreased to 0.00150.
2024-12-02-13:40:24-root-INFO: grad norm: 167.441 161.511 44.165
2024-12-02-13:40:25-root-INFO: grad norm: 190.252 183.665 49.629
2024-12-02-13:40:26-root-INFO: Loss Change: 2141.839 -> 2061.219
2024-12-02-13:40:26-root-INFO: Regularization Change: 0.000 -> 1.539
2024-12-02-13:40:26-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-13:40:26-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:40:26-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-13:40:27-root-INFO: grad norm: 202.090 195.088 52.736
2024-12-02-13:40:27-root-INFO: Loss too large (2041.903->2070.981)! Learning rate decreased to 0.00196.
2024-12-02-13:40:27-root-INFO: Loss too large (2041.903->2042.619)! Learning rate decreased to 0.00157.
2024-12-02-13:40:28-root-INFO: grad norm: 208.333 202.252 49.967
2024-12-02-13:40:29-root-INFO: Loss too large (2028.520->2028.752)! Learning rate decreased to 0.00126.
2024-12-02-13:40:30-root-INFO: grad norm: 172.624 166.576 45.293
2024-12-02-13:40:31-root-INFO: grad norm: 142.987 138.455 35.712
2024-12-02-13:40:32-root-INFO: grad norm: 124.737 120.116 33.639
2024-12-02-13:40:33-root-INFO: grad norm: 110.065 106.327 28.444
2024-12-02-13:40:34-root-INFO: grad norm: 100.305 96.407 27.689
2024-12-02-13:40:35-root-INFO: grad norm: 92.522 89.123 24.847
2024-12-02-13:40:35-root-INFO: Loss Change: 2041.903 -> 1975.150
2024-12-02-13:40:35-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-02-13:40:35-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-13:40:35-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:40:36-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-13:40:36-root-INFO: grad norm: 83.800 78.350 29.727
2024-12-02-13:40:37-root-INFO: grad norm: 116.427 112.605 29.588
2024-12-02-13:40:37-root-INFO: Loss too large (1952.487->1960.975)! Learning rate decreased to 0.00205.
2024-12-02-13:40:38-root-INFO: Loss too large (1952.487->1952.701)! Learning rate decreased to 0.00164.
2024-12-02-13:40:39-root-INFO: grad norm: 145.152 140.313 37.164
2024-12-02-13:40:40-root-INFO: grad norm: 198.144 192.481 47.033
2024-12-02-13:40:40-root-INFO: Loss too large (1946.799->1953.270)! Learning rate decreased to 0.00131.
2024-12-02-13:40:41-root-INFO: grad norm: 185.825 180.227 45.267
2024-12-02-13:40:42-root-INFO: grad norm: 172.819 167.833 41.216
2024-12-02-13:40:43-root-INFO: grad norm: 163.446 158.481 39.981
2024-12-02-13:40:44-root-INFO: grad norm: 154.023 149.489 37.097
2024-12-02-13:40:45-root-INFO: Loss Change: 1962.442 -> 1916.274
2024-12-02-13:40:45-root-INFO: Regularization Change: 0.000 -> 0.638
2024-12-02-13:40:45-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-13:40:45-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:40:45-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-13:40:45-root-INFO: grad norm: 149.430 142.667 44.447
2024-12-02-13:40:46-root-INFO: Loss too large (1902.594->1913.598)! Learning rate decreased to 0.00214.
2024-12-02-13:40:47-root-INFO: grad norm: 225.076 220.254 46.339
2024-12-02-13:40:47-root-INFO: Loss too large (1899.446->1940.628)! Learning rate decreased to 0.00171.
2024-12-02-13:40:47-root-INFO: Loss too large (1899.446->1911.392)! Learning rate decreased to 0.00137.
2024-12-02-13:40:48-root-INFO: grad norm: 216.382 210.266 51.081
2024-12-02-13:40:49-root-INFO: grad norm: 210.024 204.578 47.518
2024-12-02-13:40:50-root-INFO: grad norm: 205.065 199.394 47.891
2024-12-02-13:40:51-root-INFO: grad norm: 199.218 194.019 45.212
2024-12-02-13:40:52-root-INFO: grad norm: 194.677 189.349 45.231
2024-12-02-13:40:53-root-INFO: grad norm: 189.353 184.373 43.144
2024-12-02-13:40:54-root-INFO: Loss Change: 1902.594 -> 1860.738
2024-12-02-13:40:54-root-INFO: Regularization Change: 0.000 -> 0.529
2024-12-02-13:40:54-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-13:40:54-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-13:40:54-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-13:40:55-root-INFO: grad norm: 201.340 195.155 49.519
2024-12-02-13:40:55-root-INFO: Loss too large (1847.658->1899.367)! Learning rate decreased to 0.00224.
2024-12-02-13:40:55-root-INFO: Loss too large (1847.658->1861.321)! Learning rate decreased to 0.00179.
2024-12-02-13:40:56-root-INFO: grad norm: 234.210 229.230 48.038
2024-12-02-13:40:57-root-INFO: Loss too large (1841.154->1855.159)! Learning rate decreased to 0.00143.
2024-12-02-13:40:58-root-INFO: grad norm: 220.267 214.510 50.031
2024-12-02-13:40:59-root-INFO: grad norm: 209.009 203.727 46.690
2024-12-02-13:41:00-root-INFO: grad norm: 201.234 195.998 45.608
2024-12-02-13:41:01-root-INFO: grad norm: 192.091 187.199 43.077
2024-12-02-13:41:02-root-INFO: grad norm: 185.572 180.781 41.894
2024-12-02-13:41:03-root-INFO: grad norm: 178.069 173.474 40.193
2024-12-02-13:41:03-root-INFO: Loss Change: 1847.658 -> 1801.165
2024-12-02-13:41:03-root-INFO: Regularization Change: 0.000 -> 0.483
2024-12-02-13:41:03-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-13:41:03-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-13:41:04-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-13:41:04-root-INFO: grad norm: 154.919 150.758 35.665
2024-12-02-13:41:04-root-INFO: Loss too large (1781.370->1819.807)! Learning rate decreased to 0.00233.
2024-12-02-13:41:05-root-INFO: Loss too large (1781.370->1793.688)! Learning rate decreased to 0.00187.
2024-12-02-13:41:06-root-INFO: grad norm: 202.347 197.372 44.591
2024-12-02-13:41:06-root-INFO: Loss too large (1779.694->1790.737)! Learning rate decreased to 0.00149.
2024-12-02-13:41:07-root-INFO: grad norm: 192.510 187.989 41.476
2024-12-02-13:41:08-root-INFO: grad norm: 184.687 180.031 41.209
2024-12-02-13:41:09-root-INFO: grad norm: 179.288 175.040 38.798
2024-12-02-13:41:10-root-INFO: grad norm: 173.046 168.670 38.672
2024-12-02-13:41:11-root-INFO: grad norm: 168.608 164.607 36.515
2024-12-02-13:41:12-root-INFO: grad norm: 163.680 159.525 36.645
2024-12-02-13:41:13-root-INFO: Loss Change: 1781.370 -> 1745.523
2024-12-02-13:41:13-root-INFO: Regularization Change: 0.000 -> 0.447
2024-12-02-13:41:13-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-13:41:13-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-13:41:13-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-13:41:13-root-INFO: grad norm: 279.077 268.058 77.646
2024-12-02-13:41:14-root-INFO: Loss too large (1757.051->1845.783)! Learning rate decreased to 0.00244.
2024-12-02-13:41:14-root-INFO: Loss too large (1757.051->1788.204)! Learning rate decreased to 0.00195.
2024-12-02-13:41:15-root-INFO: grad norm: 262.150 256.768 52.848
2024-12-02-13:41:16-root-INFO: grad norm: 290.663 283.618 63.607
2024-12-02-13:41:17-root-INFO: Loss too large (1750.286->1764.983)! Learning rate decreased to 0.00156.
2024-12-02-13:41:18-root-INFO: grad norm: 259.030 253.096 55.123
2024-12-02-13:41:19-root-INFO: grad norm: 242.767 236.850 53.272
2024-12-02-13:41:20-root-INFO: grad norm: 220.981 215.841 47.387
2024-12-02-13:41:21-root-INFO: grad norm: 208.142 203.125 45.423
2024-12-02-13:41:22-root-INFO: grad norm: 192.274 187.716 41.619
2024-12-02-13:41:22-root-INFO: Loss Change: 1757.051 -> 1699.039
2024-12-02-13:41:22-root-INFO: Regularization Change: 0.000 -> 0.560
2024-12-02-13:41:22-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-13:41:22-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-13:41:23-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-13:41:23-root-INFO: grad norm: 222.444 216.079 52.830
2024-12-02-13:41:23-root-INFO: Loss too large (1692.982->1789.338)! Learning rate decreased to 0.00254.
2024-12-02-13:41:24-root-INFO: Loss too large (1692.982->1733.234)! Learning rate decreased to 0.00203.
2024-12-02-13:41:24-root-INFO: Loss too large (1692.982->1700.700)! Learning rate decreased to 0.00163.
2024-12-02-13:41:25-root-INFO: grad norm: 184.827 180.692 38.877
2024-12-02-13:41:26-root-INFO: grad norm: 159.564 155.424 36.108
2024-12-02-13:41:27-root-INFO: grad norm: 154.310 150.751 32.949
2024-12-02-13:41:28-root-INFO: grad norm: 151.488 147.809 33.181
2024-12-02-13:41:29-root-INFO: grad norm: 150.189 146.666 32.337
2024-12-02-13:41:30-root-INFO: grad norm: 149.867 146.348 32.286
2024-12-02-13:41:31-root-INFO: grad norm: 150.535 146.974 32.546
2024-12-02-13:41:32-root-INFO: Loss Change: 1692.982 -> 1648.444
2024-12-02-13:41:32-root-INFO: Regularization Change: 0.000 -> 0.442
2024-12-02-13:41:32-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-13:41:32-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-13:41:32-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-13:41:32-root-INFO: grad norm: 232.361 224.808 58.764
2024-12-02-13:41:33-root-INFO: Loss too large (1651.559->1759.527)! Learning rate decreased to 0.00265.
2024-12-02-13:41:33-root-INFO: Loss too large (1651.559->1702.338)! Learning rate decreased to 0.00212.
2024-12-02-13:41:33-root-INFO: Loss too large (1651.559->1667.240)! Learning rate decreased to 0.00170.
2024-12-02-13:41:34-root-INFO: grad norm: 197.721 192.957 43.141
2024-12-02-13:41:35-root-INFO: grad norm: 162.942 158.390 38.249
2024-12-02-13:41:36-root-INFO: grad norm: 161.995 158.238 34.687
2024-12-02-13:41:37-root-INFO: grad norm: 161.917 157.848 36.068
2024-12-02-13:41:38-root-INFO: grad norm: 164.222 160.355 35.429
2024-12-02-13:41:39-root-INFO: grad norm: 166.278 162.288 36.207
2024-12-02-13:41:40-root-INFO: grad norm: 169.858 165.834 36.754
2024-12-02-13:41:41-root-INFO: Loss Change: 1651.559 -> 1611.278
2024-12-02-13:41:41-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-02-13:41:41-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-13:41:41-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-13:41:41-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-13:41:42-root-INFO: grad norm: 305.552 296.133 75.281
2024-12-02-13:41:42-root-INFO: Loss too large (1616.234->1777.501)! Learning rate decreased to 0.00277.
2024-12-02-13:41:43-root-INFO: Loss too large (1616.234->1704.974)! Learning rate decreased to 0.00222.
2024-12-02-13:41:43-root-INFO: Loss too large (1616.234->1655.572)! Learning rate decreased to 0.00177.
2024-12-02-13:41:43-root-INFO: Loss too large (1616.234->1622.878)! Learning rate decreased to 0.00142.
2024-12-02-13:41:44-root-INFO: grad norm: 180.816 175.720 42.628
2024-12-02-13:41:45-root-INFO: grad norm: 76.571 73.680 20.841
2024-12-02-13:41:46-root-INFO: grad norm: 60.082 58.027 15.578
2024-12-02-13:41:47-root-INFO: grad norm: 53.642 51.595 14.675
2024-12-02-13:41:48-root-INFO: grad norm: 50.636 48.649 14.046
2024-12-02-13:41:49-root-INFO: grad norm: 49.248 47.312 13.675
2024-12-02-13:41:50-root-INFO: grad norm: 48.581 46.635 13.612
2024-12-02-13:41:51-root-INFO: Loss Change: 1616.234 -> 1557.762
2024-12-02-13:41:51-root-INFO: Regularization Change: 0.000 -> 0.356
2024-12-02-13:41:51-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-13:41:51-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-13:41:51-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-13:41:52-root-INFO: grad norm: 170.692 161.988 53.811
2024-12-02-13:41:52-root-INFO: Loss too large (1564.708->1616.100)! Learning rate decreased to 0.00289.
2024-12-02-13:41:52-root-INFO: Loss too large (1564.708->1590.479)! Learning rate decreased to 0.00231.
2024-12-02-13:41:53-root-INFO: Loss too large (1564.708->1573.350)! Learning rate decreased to 0.00185.
2024-12-02-13:41:54-root-INFO: grad norm: 163.915 159.476 37.888
2024-12-02-13:41:55-root-INFO: grad norm: 179.617 173.426 46.750
2024-12-02-13:41:55-root-INFO: Loss too large (1552.061->1558.372)! Learning rate decreased to 0.00148.
2024-12-02-13:41:56-root-INFO: grad norm: 146.655 142.661 33.995
2024-12-02-13:41:57-root-INFO: grad norm: 104.441 100.754 27.505
2024-12-02-13:41:58-root-INFO: grad norm: 94.754 92.177 21.947
2024-12-02-13:41:59-root-INFO: grad norm: 85.558 82.715 21.869
2024-12-02-13:42:00-root-INFO: grad norm: 81.401 79.112 19.166
2024-12-02-13:42:01-root-INFO: Loss Change: 1564.708 -> 1521.160
2024-12-02-13:42:01-root-INFO: Regularization Change: 0.000 -> 0.431
2024-12-02-13:42:01-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-13:42:01-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:42:01-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-13:42:02-root-INFO: grad norm: 271.367 262.430 69.071
2024-12-02-13:42:02-root-INFO: Loss too large (1527.015->1661.093)! Learning rate decreased to 0.00301.
2024-12-02-13:42:02-root-INFO: Loss too large (1527.015->1616.422)! Learning rate decreased to 0.00241.
2024-12-02-13:42:03-root-INFO: Loss too large (1527.015->1579.924)! Learning rate decreased to 0.00193.
2024-12-02-13:42:03-root-INFO: Loss too large (1527.015->1551.614)! Learning rate decreased to 0.00154.
2024-12-02-13:42:03-root-INFO: Loss too large (1527.015->1531.135)! Learning rate decreased to 0.00123.
2024-12-02-13:42:04-root-INFO: grad norm: 166.824 162.354 38.357
2024-12-02-13:42:05-root-INFO: grad norm: 69.263 66.446 19.552
2024-12-02-13:42:06-root-INFO: grad norm: 61.609 59.615 15.549
2024-12-02-13:42:07-root-INFO: grad norm: 58.282 56.217 15.379
2024-12-02-13:42:08-root-INFO: grad norm: 57.244 55.374 14.512
2024-12-02-13:42:09-root-INFO: grad norm: 56.826 54.917 14.606
2024-12-02-13:42:10-root-INFO: grad norm: 56.748 54.925 14.266
2024-12-02-13:42:11-root-INFO: Loss Change: 1527.015 -> 1476.320
2024-12-02-13:42:11-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-13:42:11-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-13:42:11-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:42:11-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-13:42:12-root-INFO: grad norm: 121.075 116.333 33.554
2024-12-02-13:42:12-root-INFO: Loss too large (1471.519->1512.933)! Learning rate decreased to 0.00314.
2024-12-02-13:42:12-root-INFO: Loss too large (1471.519->1494.961)! Learning rate decreased to 0.00251.
2024-12-02-13:42:13-root-INFO: Loss too large (1471.519->1482.543)! Learning rate decreased to 0.00201.
2024-12-02-13:42:13-root-INFO: Loss too large (1471.519->1474.489)! Learning rate decreased to 0.00161.
2024-12-02-13:42:14-root-INFO: grad norm: 145.619 142.399 30.453
2024-12-02-13:42:14-root-INFO: Loss too large (1469.645->1469.688)! Learning rate decreased to 0.00129.
2024-12-02-13:42:15-root-INFO: grad norm: 145.093 141.537 31.926
2024-12-02-13:42:16-root-INFO: grad norm: 150.950 147.793 30.708
2024-12-02-13:42:17-root-INFO: grad norm: 174.009 170.402 35.247
2024-12-02-13:42:18-root-INFO: Loss too large (1456.838->1460.583)! Learning rate decreased to 0.00103.
2024-12-02-13:42:19-root-INFO: grad norm: 145.084 142.172 28.921
2024-12-02-13:42:20-root-INFO: grad norm: 113.495 111.032 23.518
2024-12-02-13:42:21-root-INFO: grad norm: 108.395 106.105 22.160
2024-12-02-13:42:21-root-INFO: Loss Change: 1471.519 -> 1441.797
2024-12-02-13:42:21-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-13:42:21-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-13:42:21-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:42:22-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-13:42:22-root-INFO: grad norm: 511.436 498.807 112.955
2024-12-02-13:42:22-root-INFO: Loss too large (1492.966->1754.054)! Learning rate decreased to 0.00328.
2024-12-02-13:42:23-root-INFO: Loss too large (1492.966->1697.454)! Learning rate decreased to 0.00262.
2024-12-02-13:42:23-root-INFO: Loss too large (1492.966->1647.811)! Learning rate decreased to 0.00210.
2024-12-02-13:42:23-root-INFO: Loss too large (1492.966->1602.944)! Learning rate decreased to 0.00168.
2024-12-02-13:42:24-root-INFO: Loss too large (1492.966->1562.112)! Learning rate decreased to 0.00134.
2024-12-02-13:42:24-root-INFO: Loss too large (1492.966->1525.703)! Learning rate decreased to 0.00107.
2024-12-02-13:42:24-root-INFO: Loss too large (1492.966->1494.784)! Learning rate decreased to 0.00086.
2024-12-02-13:42:25-root-INFO: grad norm: 242.540 238.530 43.920
2024-12-02-13:42:26-root-INFO: grad norm: 91.990 88.868 23.762
2024-12-02-13:42:27-root-INFO: grad norm: 72.311 68.560 22.986
2024-12-02-13:42:28-root-INFO: grad norm: 64.299 61.793 17.777
2024-12-02-13:42:29-root-INFO: grad norm: 59.517 56.893 17.477
2024-12-02-13:42:30-root-INFO: grad norm: 56.695 54.539 15.484
2024-12-02-13:42:31-root-INFO: grad norm: 54.790 52.638 15.205
2024-12-02-13:42:32-root-INFO: Loss Change: 1492.966 -> 1416.372
2024-12-02-13:42:32-root-INFO: Regularization Change: 0.000 -> 0.213
2024-12-02-13:42:32-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-13:42:32-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:42:32-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-13:42:33-root-INFO: grad norm: 226.760 221.731 47.490
2024-12-02-13:42:33-root-INFO: Loss too large (1420.780->1581.192)! Learning rate decreased to 0.00342.
2024-12-02-13:42:33-root-INFO: Loss too large (1420.780->1545.781)! Learning rate decreased to 0.00273.
2024-12-02-13:42:34-root-INFO: Loss too large (1420.780->1513.714)! Learning rate decreased to 0.00219.
2024-12-02-13:42:34-root-INFO: Loss too large (1420.780->1485.486)! Learning rate decreased to 0.00175.
2024-12-02-13:42:34-root-INFO: Loss too large (1420.780->1461.766)! Learning rate decreased to 0.00140.
2024-12-02-13:42:35-root-INFO: Loss too large (1420.780->1443.030)! Learning rate decreased to 0.00112.
2024-12-02-13:42:35-root-INFO: Loss too large (1420.780->1429.290)! Learning rate decreased to 0.00090.
2024-12-02-13:42:36-root-INFO: grad norm: 197.257 194.264 34.235
2024-12-02-13:42:37-root-INFO: grad norm: 164.315 161.387 30.880
2024-12-02-13:42:37-root-INFO: Loss too large (1409.703->1409.904)! Learning rate decreased to 0.00072.
2024-12-02-13:42:38-root-INFO: grad norm: 123.509 121.174 23.899
2024-12-02-13:42:39-root-INFO: grad norm: 86.202 84.068 19.061
2024-12-02-13:42:40-root-INFO: grad norm: 73.422 71.291 17.559
2024-12-02-13:42:41-root-INFO: grad norm: 63.318 61.358 15.631
2024-12-02-13:42:42-root-INFO: grad norm: 58.005 55.912 15.444
2024-12-02-13:42:43-root-INFO: Loss Change: 1420.780 -> 1395.184
2024-12-02-13:42:43-root-INFO: Regularization Change: 0.000 -> 0.104
2024-12-02-13:42:43-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-13:42:43-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:42:43-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-13:42:44-root-INFO: grad norm: 298.882 293.999 53.805
2024-12-02-13:42:44-root-INFO: Loss too large (1405.578->1607.874)! Learning rate decreased to 0.00356.
2024-12-02-13:42:44-root-INFO: Loss too large (1405.578->1575.354)! Learning rate decreased to 0.00285.
2024-12-02-13:42:45-root-INFO: Loss too large (1405.578->1543.650)! Learning rate decreased to 0.00228.
2024-12-02-13:42:45-root-INFO: Loss too large (1405.578->1512.760)! Learning rate decreased to 0.00182.
2024-12-02-13:42:45-root-INFO: Loss too large (1405.578->1483.400)! Learning rate decreased to 0.00146.
2024-12-02-13:42:46-root-INFO: Loss too large (1405.578->1456.768)! Learning rate decreased to 0.00117.
2024-12-02-13:42:46-root-INFO: Loss too large (1405.578->1434.159)! Learning rate decreased to 0.00093.
2024-12-02-13:42:46-root-INFO: Loss too large (1405.578->1416.501)! Learning rate decreased to 0.00075.
2024-12-02-13:42:47-root-INFO: grad norm: 227.076 224.211 35.954
2024-12-02-13:42:48-root-INFO: grad norm: 136.293 133.659 26.667
2024-12-02-13:42:49-root-INFO: grad norm: 127.643 125.553 23.005
2024-12-02-13:42:50-root-INFO: grad norm: 119.675 117.627 22.047
2024-12-02-13:42:51-root-INFO: grad norm: 116.833 114.839 21.497
2024-12-02-13:42:52-root-INFO: grad norm: 114.716 112.902 20.316
2024-12-02-13:42:53-root-INFO: grad norm: 114.461 112.518 20.999
2024-12-02-13:42:54-root-INFO: Loss Change: 1405.578 -> 1378.240
2024-12-02-13:42:54-root-INFO: Regularization Change: 0.000 -> 0.091
2024-12-02-13:42:54-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-13:42:54-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-13:42:54-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-13:42:55-root-INFO: grad norm: 400.603 394.723 68.382
2024-12-02-13:42:55-root-INFO: Loss too large (1400.101->1637.861)! Learning rate decreased to 0.00371.
2024-12-02-13:42:55-root-INFO: Loss too large (1400.101->1605.367)! Learning rate decreased to 0.00297.
2024-12-02-13:42:56-root-INFO: Loss too large (1400.101->1574.119)! Learning rate decreased to 0.00238.
2024-12-02-13:42:56-root-INFO: Loss too large (1400.101->1542.958)! Learning rate decreased to 0.00190.
2024-12-02-13:42:56-root-INFO: Loss too large (1400.101->1511.539)! Learning rate decreased to 0.00152.
2024-12-02-13:42:57-root-INFO: Loss too large (1400.101->1480.371)! Learning rate decreased to 0.00122.
2024-12-02-13:42:57-root-INFO: Loss too large (1400.101->1450.752)! Learning rate decreased to 0.00097.
2024-12-02-13:42:57-root-INFO: Loss too large (1400.101->1424.495)! Learning rate decreased to 0.00078.
2024-12-02-13:42:58-root-INFO: Loss too large (1400.101->1403.332)! Learning rate decreased to 0.00062.
2024-12-02-13:42:59-root-INFO: grad norm: 238.794 236.102 35.755
2024-12-02-13:43:00-root-INFO: grad norm: 92.389 89.521 22.841
2024-12-02-13:43:01-root-INFO: grad norm: 75.620 73.506 17.755
2024-12-02-13:43:02-root-INFO: grad norm: 63.705 61.353 17.152
2024-12-02-13:43:03-root-INFO: grad norm: 57.232 55.081 15.545
2024-12-02-13:43:04-root-INFO: grad norm: 52.698 50.531 14.959
2024-12-02-13:43:05-root-INFO: grad norm: 49.915 47.790 14.407
2024-12-02-13:43:06-root-INFO: Loss Change: 1400.101 -> 1361.366
2024-12-02-13:43:06-root-INFO: Regularization Change: 0.000 -> 0.081
2024-12-02-13:43:06-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-13:43:06-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-13:43:06-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-13:43:06-root-INFO: grad norm: 347.383 340.838 67.113
2024-12-02-13:43:07-root-INFO: Loss too large (1386.173->1632.094)! Learning rate decreased to 0.00387.
2024-12-02-13:43:07-root-INFO: Loss too large (1386.173->1593.648)! Learning rate decreased to 0.00309.
2024-12-02-13:43:07-root-INFO: Loss too large (1386.173->1558.095)! Learning rate decreased to 0.00248.
2024-12-02-13:43:08-root-INFO: Loss too large (1386.173->1524.142)! Learning rate decreased to 0.00198.
2024-12-02-13:43:08-root-INFO: Loss too large (1386.173->1491.357)! Learning rate decreased to 0.00158.
2024-12-02-13:43:08-root-INFO: Loss too large (1386.173->1460.203)! Learning rate decreased to 0.00127.
2024-12-02-13:43:08-root-INFO: Loss too large (1386.173->1431.892)! Learning rate decreased to 0.00101.
2024-12-02-13:43:09-root-INFO: Loss too large (1386.173->1407.945)! Learning rate decreased to 0.00081.
2024-12-02-13:43:09-root-INFO: Loss too large (1386.173->1389.530)! Learning rate decreased to 0.00065.
2024-12-02-13:43:10-root-INFO: grad norm: 242.366 239.530 36.973
2024-12-02-13:43:11-root-INFO: grad norm: 138.175 134.495 31.679
2024-12-02-13:43:12-root-INFO: grad norm: 119.719 117.405 23.426
2024-12-02-13:43:13-root-INFO: grad norm: 103.179 100.386 23.844
2024-12-02-13:43:14-root-INFO: grad norm: 93.868 91.720 19.965
2024-12-02-13:43:15-root-INFO: grad norm: 85.114 82.738 19.974
2024-12-02-13:43:16-root-INFO: grad norm: 79.350 77.317 17.845
2024-12-02-13:43:17-root-INFO: Loss Change: 1386.173 -> 1348.152
2024-12-02-13:43:17-root-INFO: Regularization Change: 0.000 -> 0.107
2024-12-02-13:43:17-root-INFO: Undo step: 180
2024-12-02-13:43:17-root-INFO: Undo step: 181
2024-12-02-13:43:17-root-INFO: Undo step: 182
2024-12-02-13:43:17-root-INFO: Undo step: 183
2024-12-02-13:43:17-root-INFO: Undo step: 184
2024-12-02-13:43:17-root-INFO: Undo step: 185
2024-12-02-13:43:17-root-INFO: Undo step: 186
2024-12-02-13:43:17-root-INFO: Undo step: 187
2024-12-02-13:43:17-root-INFO: Undo step: 188
2024-12-02-13:43:17-root-INFO: Undo step: 189
2024-12-02-13:43:17-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-13:43:18-root-INFO: grad norm: 1013.694 868.697 522.437
2024-12-02-13:43:19-root-INFO: grad norm: 620.838 579.381 223.063
2024-12-02-13:43:19-root-INFO: grad norm: 729.071 682.615 256.090
2024-12-02-13:43:20-root-INFO: Loss too large (2646.430->2815.832)! Learning rate decreased to 0.00254.
2024-12-02-13:43:21-root-INFO: grad norm: 791.557 749.168 255.557
2024-12-02-13:43:21-root-INFO: Loss too large (2472.452->2715.673)! Learning rate decreased to 0.00203.
2024-12-02-13:43:22-root-INFO: grad norm: 802.585 771.927 219.707
2024-12-02-13:43:23-root-INFO: Loss too large (2385.444->2433.403)! Learning rate decreased to 0.00163.
2024-12-02-13:43:24-root-INFO: grad norm: 579.203 564.984 127.551
2024-12-02-13:43:25-root-INFO: grad norm: 416.604 405.895 93.851
2024-12-02-13:43:26-root-INFO: grad norm: 381.440 371.052 88.415
2024-12-02-13:43:26-root-INFO: Loss Change: 3981.314 -> 2011.309
2024-12-02-13:43:26-root-INFO: Regularization Change: 0.000 -> 19.516
2024-12-02-13:43:26-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-13:43:26-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-13:43:27-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-13:43:27-root-INFO: grad norm: 410.445 399.904 92.420
2024-12-02-13:43:27-root-INFO: Loss too large (2009.721->2268.785)! Learning rate decreased to 0.00265.
2024-12-02-13:43:28-root-INFO: Loss too large (2009.721->2143.882)! Learning rate decreased to 0.00212.
2024-12-02-13:43:28-root-INFO: Loss too large (2009.721->2056.448)! Learning rate decreased to 0.00170.
2024-12-02-13:43:29-root-INFO: grad norm: 364.785 351.794 96.485
2024-12-02-13:43:30-root-INFO: grad norm: 324.944 315.688 77.004
2024-12-02-13:43:31-root-INFO: grad norm: 343.989 332.992 86.281
2024-12-02-13:43:32-root-INFO: grad norm: 385.789 375.813 87.166
2024-12-02-13:43:32-root-INFO: Loss too large (1908.792->1914.902)! Learning rate decreased to 0.00136.
2024-12-02-13:43:33-root-INFO: grad norm: 297.539 286.500 80.295
2024-12-02-13:43:34-root-INFO: grad norm: 241.452 233.814 60.248
2024-12-02-13:43:35-root-INFO: grad norm: 219.828 210.219 64.284
2024-12-02-13:43:36-root-INFO: Loss Change: 2009.721 -> 1809.987
2024-12-02-13:43:36-root-INFO: Regularization Change: 0.000 -> 1.859
2024-12-02-13:43:36-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-13:43:36-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-13:43:36-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-13:43:37-root-INFO: grad norm: 388.270 374.887 101.059
2024-12-02-13:43:37-root-INFO: Loss too large (1809.734->1933.205)! Learning rate decreased to 0.00277.
2024-12-02-13:43:37-root-INFO: Loss too large (1809.734->1902.357)! Learning rate decreased to 0.00222.
2024-12-02-13:43:38-root-INFO: Loss too large (1809.734->1871.438)! Learning rate decreased to 0.00177.
2024-12-02-13:43:38-root-INFO: Loss too large (1809.734->1842.178)! Learning rate decreased to 0.00142.
2024-12-02-13:43:38-root-INFO: Loss too large (1809.734->1816.350)! Learning rate decreased to 0.00113.
2024-12-02-13:43:39-root-INFO: grad norm: 235.594 224.027 72.912
2024-12-02-13:43:40-root-INFO: grad norm: 99.256 93.570 33.112
2024-12-02-13:43:41-root-INFO: grad norm: 94.104 88.222 32.747
2024-12-02-13:43:42-root-INFO: grad norm: 91.442 85.773 31.694
2024-12-02-13:43:43-root-INFO: grad norm: 89.436 83.709 31.490
2024-12-02-13:43:45-root-INFO: grad norm: 87.800 82.259 30.697
2024-12-02-13:43:46-root-INFO: grad norm: 86.473 80.883 30.588
2024-12-02-13:43:46-root-INFO: Loss Change: 1809.734 -> 1699.370
2024-12-02-13:43:46-root-INFO: Regularization Change: 0.000 -> 0.709
2024-12-02-13:43:46-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-13:43:46-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-13:43:47-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-13:43:47-root-INFO: grad norm: 226.187 215.464 68.817
2024-12-02-13:43:47-root-INFO: Loss too large (1702.828->1799.147)! Learning rate decreased to 0.00289.
2024-12-02-13:43:48-root-INFO: Loss too large (1702.828->1771.907)! Learning rate decreased to 0.00231.
2024-12-02-13:43:48-root-INFO: Loss too large (1702.828->1747.484)! Learning rate decreased to 0.00185.
2024-12-02-13:43:48-root-INFO: Loss too large (1702.828->1727.005)! Learning rate decreased to 0.00148.
2024-12-02-13:43:49-root-INFO: Loss too large (1702.828->1711.245)! Learning rate decreased to 0.00118.
2024-12-02-13:43:50-root-INFO: grad norm: 222.269 212.943 63.710
2024-12-02-13:43:51-root-INFO: grad norm: 223.053 214.226 62.130
2024-12-02-13:43:51-root-INFO: Loss too large (1683.909->1684.823)! Learning rate decreased to 0.00095.
2024-12-02-13:43:52-root-INFO: grad norm: 180.415 172.490 52.883
2024-12-02-13:43:53-root-INFO: grad norm: 135.148 129.148 39.820
2024-12-02-13:43:54-root-INFO: grad norm: 123.726 117.886 37.563
2024-12-02-13:43:55-root-INFO: grad norm: 113.192 107.954 34.034
2024-12-02-13:43:56-root-INFO: grad norm: 108.309 102.994 33.514
2024-12-02-13:43:57-root-INFO: Loss Change: 1702.828 -> 1641.402
2024-12-02-13:43:57-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-02-13:43:57-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-13:43:57-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:43:57-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-13:43:57-root-INFO: grad norm: 320.685 308.794 86.517
2024-12-02-13:43:58-root-INFO: Loss too large (1638.800->1799.355)! Learning rate decreased to 0.00301.
2024-12-02-13:43:58-root-INFO: Loss too large (1638.800->1766.426)! Learning rate decreased to 0.00241.
2024-12-02-13:43:58-root-INFO: Loss too large (1638.800->1734.353)! Learning rate decreased to 0.00193.
2024-12-02-13:43:59-root-INFO: Loss too large (1638.800->1703.960)! Learning rate decreased to 0.00154.
2024-12-02-13:43:59-root-INFO: Loss too large (1638.800->1676.537)! Learning rate decreased to 0.00123.
2024-12-02-13:43:59-root-INFO: Loss too large (1638.800->1653.612)! Learning rate decreased to 0.00099.
2024-12-02-13:44:00-root-INFO: grad norm: 246.374 236.039 70.613
2024-12-02-13:44:01-root-INFO: grad norm: 148.093 142.253 41.175
2024-12-02-13:44:02-root-INFO: grad norm: 145.589 139.204 42.644
2024-12-02-13:44:03-root-INFO: grad norm: 146.246 140.730 39.786
2024-12-02-13:44:04-root-INFO: grad norm: 148.454 141.885 43.671
2024-12-02-13:44:05-root-INFO: grad norm: 153.879 148.268 41.173
2024-12-02-13:44:06-root-INFO: grad norm: 158.120 151.214 46.220
2024-12-02-13:44:07-root-INFO: Loss Change: 1638.800 -> 1582.830
2024-12-02-13:44:07-root-INFO: Regularization Change: 0.000 -> 0.320
2024-12-02-13:44:07-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-13:44:07-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:44:07-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-13:44:08-root-INFO: grad norm: 278.735 269.068 72.772
2024-12-02-13:44:08-root-INFO: Loss too large (1582.702->1744.637)! Learning rate decreased to 0.00314.
2024-12-02-13:44:08-root-INFO: Loss too large (1582.702->1713.692)! Learning rate decreased to 0.00251.
2024-12-02-13:44:09-root-INFO: Loss too large (1582.702->1682.683)! Learning rate decreased to 0.00201.
2024-12-02-13:44:09-root-INFO: Loss too large (1582.702->1652.839)! Learning rate decreased to 0.00161.
2024-12-02-13:44:09-root-INFO: Loss too large (1582.702->1625.822)! Learning rate decreased to 0.00129.
2024-12-02-13:44:10-root-INFO: Loss too large (1582.702->1603.369)! Learning rate decreased to 0.00103.
2024-12-02-13:44:10-root-INFO: Loss too large (1582.702->1586.573)! Learning rate decreased to 0.00082.
2024-12-02-13:44:11-root-INFO: grad norm: 203.689 195.477 57.256
2024-12-02-13:44:12-root-INFO: grad norm: 129.564 124.601 35.517
2024-12-02-13:44:13-root-INFO: grad norm: 111.853 106.888 32.953
2024-12-02-13:44:14-root-INFO: grad norm: 96.539 92.625 27.212
2024-12-02-13:44:15-root-INFO: grad norm: 88.430 84.201 27.019
2024-12-02-13:44:16-root-INFO: grad norm: 81.609 78.118 23.615
2024-12-02-13:44:17-root-INFO: grad norm: 77.351 73.499 24.103
2024-12-02-13:44:18-root-INFO: Loss Change: 1582.702 -> 1539.903
2024-12-02-13:44:18-root-INFO: Regularization Change: 0.000 -> 0.194
2024-12-02-13:44:18-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-13:44:18-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:44:18-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-13:44:19-root-INFO: grad norm: 387.545 372.290 107.661
2024-12-02-13:44:19-root-INFO: Loss too large (1562.088->1769.234)! Learning rate decreased to 0.00328.
2024-12-02-13:44:19-root-INFO: Loss too large (1562.088->1730.650)! Learning rate decreased to 0.00262.
2024-12-02-13:44:20-root-INFO: Loss too large (1562.088->1693.922)! Learning rate decreased to 0.00210.
2024-12-02-13:44:20-root-INFO: Loss too large (1562.088->1658.447)! Learning rate decreased to 0.00168.
2024-12-02-13:44:20-root-INFO: Loss too large (1562.088->1624.661)! Learning rate decreased to 0.00134.
2024-12-02-13:44:21-root-INFO: Loss too large (1562.088->1593.950)! Learning rate decreased to 0.00107.
2024-12-02-13:44:21-root-INFO: Loss too large (1562.088->1568.292)! Learning rate decreased to 0.00086.
2024-12-02-13:44:22-root-INFO: grad norm: 268.332 258.388 72.373
2024-12-02-13:44:23-root-INFO: grad norm: 139.083 133.108 40.329
2024-12-02-13:44:24-root-INFO: grad norm: 126.310 121.558 34.321
2024-12-02-13:44:25-root-INFO: grad norm: 116.064 111.525 32.140
2024-12-02-13:44:26-root-INFO: grad norm: 110.740 106.413 30.651
2024-12-02-13:44:27-root-INFO: grad norm: 106.097 102.161 28.630
2024-12-02-13:44:28-root-INFO: grad norm: 103.520 99.432 28.805
2024-12-02-13:44:29-root-INFO: Loss Change: 1562.088 -> 1498.726
2024-12-02-13:44:29-root-INFO: Regularization Change: 0.000 -> 0.245
2024-12-02-13:44:29-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-13:44:29-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:44:29-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-13:44:29-root-INFO: grad norm: 309.359 298.797 80.145
2024-12-02-13:44:30-root-INFO: Loss too large (1503.967->1700.002)! Learning rate decreased to 0.00342.
2024-12-02-13:44:30-root-INFO: Loss too large (1503.967->1667.141)! Learning rate decreased to 0.00273.
2024-12-02-13:44:30-root-INFO: Loss too large (1503.967->1634.231)! Learning rate decreased to 0.00219.
2024-12-02-13:44:31-root-INFO: Loss too large (1503.967->1601.624)! Learning rate decreased to 0.00175.
2024-12-02-13:44:31-root-INFO: Loss too large (1503.967->1570.419)! Learning rate decreased to 0.00140.
2024-12-02-13:44:31-root-INFO: Loss too large (1503.967->1542.393)! Learning rate decreased to 0.00112.
2024-12-02-13:44:32-root-INFO: Loss too large (1503.967->1519.464)! Learning rate decreased to 0.00090.
2024-12-02-13:44:33-root-INFO: grad norm: 257.832 247.986 70.569
2024-12-02-13:44:34-root-INFO: grad norm: 190.563 184.225 48.738
2024-12-02-13:44:35-root-INFO: grad norm: 184.260 177.055 51.023
2024-12-02-13:44:36-root-INFO: grad norm: 177.330 171.656 44.500
2024-12-02-13:44:36-root-INFO: grad norm: 175.000 168.043 48.853
2024-12-02-13:44:38-root-INFO: grad norm: 172.398 166.950 42.999
2024-12-02-13:44:39-root-INFO: grad norm: 171.505 164.665 47.952
2024-12-02-13:44:39-root-INFO: Loss Change: 1503.967 -> 1464.933
2024-12-02-13:44:39-root-INFO: Regularization Change: 0.000 -> 0.189
2024-12-02-13:44:39-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-13:44:39-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-13:44:40-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-13:44:40-root-INFO: grad norm: 372.871 361.209 92.524
2024-12-02-13:44:40-root-INFO: Loss too large (1481.271->1699.051)! Learning rate decreased to 0.00356.
2024-12-02-13:44:41-root-INFO: Loss too large (1481.271->1670.282)! Learning rate decreased to 0.00285.
2024-12-02-13:44:41-root-INFO: Loss too large (1481.271->1639.512)! Learning rate decreased to 0.00228.
2024-12-02-13:44:41-root-INFO: Loss too large (1481.271->1606.552)! Learning rate decreased to 0.00182.
2024-12-02-13:44:42-root-INFO: Loss too large (1481.271->1572.207)! Learning rate decreased to 0.00146.
2024-12-02-13:44:42-root-INFO: Loss too large (1481.271->1538.300)! Learning rate decreased to 0.00117.
2024-12-02-13:44:42-root-INFO: Loss too large (1481.271->1507.655)! Learning rate decreased to 0.00093.
2024-12-02-13:44:43-root-INFO: Loss too large (1481.271->1483.129)! Learning rate decreased to 0.00075.
2024-12-02-13:44:44-root-INFO: grad norm: 238.231 229.158 65.119
2024-12-02-13:44:45-root-INFO: grad norm: 128.302 123.900 33.321
2024-12-02-13:44:46-root-INFO: grad norm: 102.880 99.017 27.927
2024-12-02-13:44:47-root-INFO: grad norm: 83.403 80.550 21.627
2024-12-02-13:44:48-root-INFO: grad norm: 72.830 69.995 20.122
2024-12-02-13:44:49-root-INFO: grad norm: 65.270 62.984 17.123
2024-12-02-13:44:50-root-INFO: grad norm: 60.762 58.348 16.958
2024-12-02-13:44:50-root-INFO: Loss Change: 1481.271 -> 1433.740
2024-12-02-13:44:50-root-INFO: Regularization Change: 0.000 -> 0.137
2024-12-02-13:44:50-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-13:44:50-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-13:44:51-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-13:44:51-root-INFO: grad norm: 239.037 230.657 62.736
2024-12-02-13:44:51-root-INFO: Loss too large (1439.213->1631.981)! Learning rate decreased to 0.00371.
2024-12-02-13:44:52-root-INFO: Loss too large (1439.213->1600.081)! Learning rate decreased to 0.00297.
2024-12-02-13:44:52-root-INFO: Loss too large (1439.213->1567.064)! Learning rate decreased to 0.00238.
2024-12-02-13:44:52-root-INFO: Loss too large (1439.213->1534.073)! Learning rate decreased to 0.00190.
2024-12-02-13:44:53-root-INFO: Loss too large (1439.213->1503.172)! Learning rate decreased to 0.00152.
2024-12-02-13:44:53-root-INFO: Loss too large (1439.213->1476.728)! Learning rate decreased to 0.00122.
2024-12-02-13:44:53-root-INFO: Loss too large (1439.213->1456.329)! Learning rate decreased to 0.00097.
2024-12-02-13:44:54-root-INFO: Loss too large (1439.213->1442.219)! Learning rate decreased to 0.00078.
2024-12-02-13:44:55-root-INFO: grad norm: 194.161 187.482 50.486
2024-12-02-13:44:56-root-INFO: grad norm: 151.465 146.207 39.565
2024-12-02-13:44:57-root-INFO: grad norm: 132.031 127.344 34.868
2024-12-02-13:44:58-root-INFO: grad norm: 113.957 110.074 29.497
2024-12-02-13:44:59-root-INFO: grad norm: 102.764 98.994 27.581
2024-12-02-13:45:00-root-INFO: grad norm: 92.513 89.374 23.894
2024-12-02-13:45:01-root-INFO: grad norm: 85.422 82.204 23.226
2024-12-02-13:45:01-root-INFO: Loss Change: 1439.213 -> 1408.879
2024-12-02-13:45:01-root-INFO: Regularization Change: 0.000 -> 0.130
2024-12-02-13:45:01-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-13:45:01-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-13:45:02-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-13:45:02-root-INFO: grad norm: 315.544 304.472 82.853
2024-12-02-13:45:02-root-INFO: Loss too large (1427.708->1654.309)! Learning rate decreased to 0.00387.
2024-12-02-13:45:03-root-INFO: Loss too large (1427.708->1622.229)! Learning rate decreased to 0.00309.
2024-12-02-13:45:03-root-INFO: Loss too large (1427.708->1589.096)! Learning rate decreased to 0.00248.
2024-12-02-13:45:03-root-INFO: Loss too large (1427.708->1554.523)! Learning rate decreased to 0.00198.
2024-12-02-13:45:04-root-INFO: Loss too large (1427.708->1519.322)! Learning rate decreased to 0.00158.
2024-12-02-13:45:04-root-INFO: Loss too large (1427.708->1485.630)! Learning rate decreased to 0.00127.
2024-12-02-13:45:04-root-INFO: Loss too large (1427.708->1456.392)! Learning rate decreased to 0.00101.
2024-12-02-13:45:05-root-INFO: Loss too large (1427.708->1433.909)! Learning rate decreased to 0.00081.
2024-12-02-13:45:06-root-INFO: grad norm: 249.607 241.032 64.863
2024-12-02-13:45:07-root-INFO: grad norm: 185.183 178.486 49.351
2024-12-02-13:45:08-root-INFO: grad norm: 163.353 157.700 42.601
2024-12-02-13:45:09-root-INFO: grad norm: 141.972 136.946 37.440
2024-12-02-13:45:10-root-INFO: grad norm: 129.193 124.645 33.975
2024-12-02-13:45:11-root-INFO: grad norm: 116.698 112.631 30.542
2024-12-02-13:45:12-root-INFO: grad norm: 108.118 104.270 28.587
2024-12-02-13:45:12-root-INFO: Loss Change: 1427.708 -> 1384.748
2024-12-02-13:45:12-root-INFO: Regularization Change: 0.000 -> 0.166
2024-12-02-13:45:12-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-13:45:12-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-13:45:13-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-13:45:13-root-INFO: grad norm: 289.409 280.233 72.297
2024-12-02-13:45:13-root-INFO: Loss too large (1397.540->1625.432)! Learning rate decreased to 0.00403.
2024-12-02-13:45:14-root-INFO: Loss too large (1397.540->1596.131)! Learning rate decreased to 0.00322.
2024-12-02-13:45:14-root-INFO: Loss too large (1397.540->1564.277)! Learning rate decreased to 0.00258.
2024-12-02-13:45:14-root-INFO: Loss too large (1397.540->1529.932)! Learning rate decreased to 0.00206.
2024-12-02-13:45:15-root-INFO: Loss too large (1397.540->1494.349)! Learning rate decreased to 0.00165.
2024-12-02-13:45:15-root-INFO: Loss too large (1397.540->1460.128)! Learning rate decreased to 0.00132.
2024-12-02-13:45:15-root-INFO: Loss too large (1397.540->1430.524)! Learning rate decreased to 0.00106.
2024-12-02-13:45:16-root-INFO: Loss too large (1397.540->1407.878)! Learning rate decreased to 0.00085.
2024-12-02-13:45:17-root-INFO: grad norm: 247.206 238.254 65.922
2024-12-02-13:45:18-root-INFO: grad norm: 202.920 196.355 51.201
2024-12-02-13:45:19-root-INFO: grad norm: 184.487 177.749 49.404
2024-12-02-13:45:20-root-INFO: grad norm: 165.090 159.722 41.759
2024-12-02-13:45:21-root-INFO: grad norm: 153.161 147.524 41.168
2024-12-02-13:45:22-root-INFO: grad norm: 140.760 136.147 35.742
2024-12-02-13:45:23-root-INFO: grad norm: 132.122 127.240 35.582
2024-12-02-13:45:23-root-INFO: Loss Change: 1397.540 -> 1362.527
2024-12-02-13:45:24-root-INFO: Regularization Change: 0.000 -> 0.146
2024-12-02-13:45:24-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-13:45:24-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-13:45:24-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-13:45:24-root-INFO: grad norm: 341.707 330.608 86.386
2024-12-02-13:45:25-root-INFO: Loss too large (1383.178->1625.697)! Learning rate decreased to 0.00420.
2024-12-02-13:45:25-root-INFO: Loss too large (1383.178->1595.538)! Learning rate decreased to 0.00336.
2024-12-02-13:45:25-root-INFO: Loss too large (1383.178->1563.945)! Learning rate decreased to 0.00269.
2024-12-02-13:45:25-root-INFO: Loss too large (1383.178->1529.775)! Learning rate decreased to 0.00215.
2024-12-02-13:45:26-root-INFO: Loss too large (1383.178->1493.175)! Learning rate decreased to 0.00172.
2024-12-02-13:45:26-root-INFO: Loss too large (1383.178->1455.877)! Learning rate decreased to 0.00138.
2024-12-02-13:45:26-root-INFO: Loss too large (1383.178->1421.268)! Learning rate decreased to 0.00110.
2024-12-02-13:45:27-root-INFO: Loss too large (1383.178->1393.035)! Learning rate decreased to 0.00088.
2024-12-02-13:45:28-root-INFO: grad norm: 270.851 260.851 72.918
2024-12-02-13:45:29-root-INFO: grad norm: 204.042 197.040 52.994
2024-12-02-13:45:30-root-INFO: grad norm: 182.018 175.303 48.983
2024-12-02-13:45:31-root-INFO: grad norm: 160.337 154.863 41.535
2024-12-02-13:45:32-root-INFO: grad norm: 147.082 141.604 39.766
2024-12-02-13:45:33-root-INFO: grad norm: 134.035 129.471 34.677
2024-12-02-13:45:34-root-INFO: grad norm: 124.913 120.241 33.842
2024-12-02-13:45:35-root-INFO: Loss Change: 1383.178 -> 1335.763
2024-12-02-13:45:35-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-13:45:35-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-13:45:35-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-13:45:35-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-13:45:35-root-INFO: grad norm: 286.899 277.498 72.843
2024-12-02-13:45:36-root-INFO: Loss too large (1349.771->1581.966)! Learning rate decreased to 0.00437.
2024-12-02-13:45:36-root-INFO: Loss too large (1349.771->1554.098)! Learning rate decreased to 0.00350.
2024-12-02-13:45:36-root-INFO: Loss too large (1349.771->1523.091)! Learning rate decreased to 0.00280.
2024-12-02-13:45:37-root-INFO: Loss too large (1349.771->1488.689)! Learning rate decreased to 0.00224.
2024-12-02-13:45:37-root-INFO: Loss too large (1349.771->1452.047)! Learning rate decreased to 0.00179.
2024-12-02-13:45:37-root-INFO: Loss too large (1349.771->1415.960)! Learning rate decreased to 0.00143.
2024-12-02-13:45:38-root-INFO: Loss too large (1349.771->1384.240)! Learning rate decreased to 0.00115.
2024-12-02-13:45:38-root-INFO: Loss too large (1349.771->1359.828)! Learning rate decreased to 0.00092.
2024-12-02-13:45:39-root-INFO: grad norm: 243.682 234.292 66.993
2024-12-02-13:45:40-root-INFO: grad norm: 201.730 194.893 52.074
2024-12-02-13:45:41-root-INFO: grad norm: 181.699 174.668 50.059
2024-12-02-13:45:42-root-INFO: grad norm: 162.027 156.485 42.012
2024-12-02-13:45:43-root-INFO: grad norm: 149.165 143.374 41.161
2024-12-02-13:45:44-root-INFO: grad norm: 136.665 131.961 35.551
2024-12-02-13:45:45-root-INFO: grad norm: 127.609 122.652 35.222
2024-12-02-13:45:46-root-INFO: Loss Change: 1349.771 -> 1312.816
2024-12-02-13:45:46-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-02-13:45:46-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-13:45:46-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-13:45:46-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-13:45:46-root-INFO: grad norm: 290.825 281.162 74.345
2024-12-02-13:45:47-root-INFO: Loss too large (1329.971->1570.999)! Learning rate decreased to 0.00455.
2024-12-02-13:45:47-root-INFO: Loss too large (1329.971->1541.523)! Learning rate decreased to 0.00364.
2024-12-02-13:45:47-root-INFO: Loss too large (1329.971->1509.023)! Learning rate decreased to 0.00291.
2024-12-02-13:45:48-root-INFO: Loss too large (1329.971->1472.785)! Learning rate decreased to 0.00233.
2024-12-02-13:45:48-root-INFO: Loss too large (1329.971->1433.842)! Learning rate decreased to 0.00186.
2024-12-02-13:45:48-root-INFO: Loss too large (1329.971->1395.336)! Learning rate decreased to 0.00149.
2024-12-02-13:45:49-root-INFO: Loss too large (1329.971->1361.692)! Learning rate decreased to 0.00119.
2024-12-02-13:45:49-root-INFO: Loss too large (1329.971->1336.217)! Learning rate decreased to 0.00095.
2024-12-02-13:45:50-root-INFO: grad norm: 235.229 226.445 63.681
2024-12-02-13:45:51-root-INFO: grad norm: 189.935 183.292 49.791
2024-12-02-13:45:52-root-INFO: grad norm: 166.471 160.205 45.243
2024-12-02-13:45:53-root-INFO: grad norm: 146.142 140.985 38.483
2024-12-02-13:45:54-root-INFO: grad norm: 132.163 127.156 36.034
2024-12-02-13:45:55-root-INFO: grad norm: 119.867 115.616 31.636
2024-12-02-13:45:56-root-INFO: grad norm: 110.608 106.401 30.215
2024-12-02-13:45:57-root-INFO: Loss Change: 1329.971 -> 1288.054
2024-12-02-13:45:57-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-13:45:57-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-13:45:57-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-13:45:57-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-13:45:58-root-INFO: grad norm: 272.282 263.399 68.980
2024-12-02-13:45:58-root-INFO: Loss too large (1303.290->1543.719)! Learning rate decreased to 0.00474.
2024-12-02-13:45:58-root-INFO: Loss too large (1303.290->1514.666)! Learning rate decreased to 0.00379.
2024-12-02-13:45:59-root-INFO: Loss too large (1303.290->1482.015)! Learning rate decreased to 0.00303.
2024-12-02-13:45:59-root-INFO: Loss too large (1303.290->1445.211)! Learning rate decreased to 0.00243.
2024-12-02-13:45:59-root-INFO: Loss too large (1303.290->1405.622)! Learning rate decreased to 0.00194.
2024-12-02-13:46:00-root-INFO: Loss too large (1303.290->1366.865)! Learning rate decreased to 0.00155.
2024-12-02-13:46:00-root-INFO: Loss too large (1303.290->1333.624)! Learning rate decreased to 0.00124.
2024-12-02-13:46:00-root-INFO: Loss too large (1303.290->1309.015)! Learning rate decreased to 0.00099.
2024-12-02-13:46:01-root-INFO: grad norm: 224.185 215.643 61.292
2024-12-02-13:46:02-root-INFO: grad norm: 186.516 180.107 48.473
2024-12-02-13:46:03-root-INFO: grad norm: 163.357 157.041 44.982
2024-12-02-13:46:04-root-INFO: grad norm: 144.112 139.082 37.739
2024-12-02-13:46:05-root-INFO: grad norm: 129.908 124.840 35.930
2024-12-02-13:46:06-root-INFO: grad norm: 117.889 113.740 31.002
2024-12-02-13:46:07-root-INFO: grad norm: 108.411 104.159 30.064
2024-12-02-13:46:08-root-INFO: Loss Change: 1303.290 -> 1263.601
2024-12-02-13:46:08-root-INFO: Regularization Change: 0.000 -> 0.184
2024-12-02-13:46:08-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-13:46:08-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-13:46:08-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-13:46:09-root-INFO: grad norm: 166.073 160.564 42.421
2024-12-02-13:46:09-root-INFO: Loss too large (1268.667->1465.465)! Learning rate decreased to 0.00494.
2024-12-02-13:46:09-root-INFO: Loss too large (1268.667->1431.828)! Learning rate decreased to 0.00395.
2024-12-02-13:46:10-root-INFO: Loss too large (1268.667->1395.147)! Learning rate decreased to 0.00316.
2024-12-02-13:46:10-root-INFO: Loss too large (1268.667->1358.530)! Learning rate decreased to 0.00253.
2024-12-02-13:46:10-root-INFO: Loss too large (1268.667->1326.008)! Learning rate decreased to 0.00202.
2024-12-02-13:46:11-root-INFO: Loss too large (1268.667->1300.433)! Learning rate decreased to 0.00162.
2024-12-02-13:46:11-root-INFO: Loss too large (1268.667->1282.473)! Learning rate decreased to 0.00129.
2024-12-02-13:46:11-root-INFO: Loss too large (1268.667->1271.119)! Learning rate decreased to 0.00104.
2024-12-02-13:46:12-root-INFO: grad norm: 155.393 149.389 42.777
2024-12-02-13:46:13-root-INFO: grad norm: 145.590 140.758 37.199
2024-12-02-13:46:14-root-INFO: grad norm: 137.052 131.668 38.035
2024-12-02-13:46:15-root-INFO: grad norm: 129.240 124.915 33.155
2024-12-02-13:46:16-root-INFO: grad norm: 122.342 117.504 34.066
2024-12-02-13:46:17-root-INFO: grad norm: 116.114 112.188 29.937
2024-12-02-13:46:18-root-INFO: grad norm: 110.608 106.218 30.851
2024-12-02-13:46:19-root-INFO: Loss Change: 1268.667 -> 1244.561
2024-12-02-13:46:19-root-INFO: Regularization Change: 0.000 -> 0.165
2024-12-02-13:46:19-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-13:46:19-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-13:46:19-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-13:46:20-root-INFO: grad norm: 286.099 276.746 72.555
2024-12-02-13:46:20-root-INFO: Loss too large (1270.016->1521.986)! Learning rate decreased to 0.00514.
2024-12-02-13:46:20-root-INFO: Loss too large (1270.016->1492.156)! Learning rate decreased to 0.00411.
2024-12-02-13:46:21-root-INFO: Loss too large (1270.016->1459.180)! Learning rate decreased to 0.00329.
2024-12-02-13:46:21-root-INFO: Loss too large (1270.016->1421.314)! Learning rate decreased to 0.00263.
2024-12-02-13:46:22-root-INFO: Loss too large (1270.016->1379.027)! Learning rate decreased to 0.00210.
2024-12-02-13:46:22-root-INFO: Loss too large (1270.016->1336.004)! Learning rate decreased to 0.00168.
2024-12-02-13:46:22-root-INFO: Loss too large (1270.016->1298.223)! Learning rate decreased to 0.00135.
2024-12-02-13:46:23-root-INFO: Loss too large (1270.016->1270.223)! Learning rate decreased to 0.00108.
2024-12-02-13:46:24-root-INFO: grad norm: 219.743 211.169 60.784
2024-12-02-13:46:25-root-INFO: grad norm: 179.888 173.374 47.972
2024-12-02-13:46:26-root-INFO: grad norm: 152.807 146.813 42.378
2024-12-02-13:46:27-root-INFO: grad norm: 133.242 128.291 35.983
2024-12-02-13:46:28-root-INFO: grad norm: 117.851 113.225 32.693
2024-12-02-13:46:29-root-INFO: grad norm: 106.048 102.074 28.759
2024-12-02-13:46:30-root-INFO: grad norm: 96.321 92.546 26.703
2024-12-02-13:46:30-root-INFO: Loss Change: 1270.016 -> 1221.270
2024-12-02-13:46:30-root-INFO: Regularization Change: 0.000 -> 0.226
2024-12-02-13:46:30-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-13:46:30-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-13:46:31-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-13:46:31-root-INFO: grad norm: 165.295 159.494 43.405
2024-12-02-13:46:31-root-INFO: Loss too large (1225.854->1435.071)! Learning rate decreased to 0.00535.
2024-12-02-13:46:32-root-INFO: Loss too large (1225.854->1401.107)! Learning rate decreased to 0.00428.
2024-12-02-13:46:32-root-INFO: Loss too large (1225.854->1362.692)! Learning rate decreased to 0.00342.
2024-12-02-13:46:32-root-INFO: Loss too large (1225.854->1323.111)! Learning rate decreased to 0.00274.
2024-12-02-13:46:33-root-INFO: Loss too large (1225.854->1287.310)! Learning rate decreased to 0.00219.
2024-12-02-13:46:33-root-INFO: Loss too large (1225.854->1259.085)! Learning rate decreased to 0.00175.
2024-12-02-13:46:33-root-INFO: Loss too large (1225.854->1239.455)! Learning rate decreased to 0.00140.
2024-12-02-13:46:34-root-INFO: Loss too large (1225.854->1227.272)! Learning rate decreased to 0.00112.
2024-12-02-13:46:35-root-INFO: grad norm: 146.507 140.882 40.206
2024-12-02-13:46:36-root-INFO: grad norm: 132.569 127.856 35.034
2024-12-02-13:46:37-root-INFO: grad norm: 120.319 115.638 33.236
2024-12-02-13:46:38-root-INFO: grad norm: 110.808 106.839 29.392
2024-12-02-13:46:38-root-INFO: grad norm: 102.267 98.265 28.330
2024-12-02-13:46:40-root-INFO: grad norm: 95.465 92.025 25.395
2024-12-02-13:46:41-root-INFO: grad norm: 89.330 85.823 24.786
2024-12-02-13:46:41-root-INFO: Loss Change: 1225.854 -> 1199.894
2024-12-02-13:46:41-root-INFO: Regularization Change: 0.000 -> 0.179
2024-12-02-13:46:41-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-13:46:41-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-13:46:42-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-13:46:42-root-INFO: grad norm: 202.642 196.119 50.998
2024-12-02-13:46:42-root-INFO: Loss too large (1208.778->1447.286)! Learning rate decreased to 0.00556.
2024-12-02-13:46:43-root-INFO: Loss too large (1208.778->1416.906)! Learning rate decreased to 0.00445.
2024-12-02-13:46:43-root-INFO: Loss too large (1208.778->1380.689)! Learning rate decreased to 0.00356.
2024-12-02-13:46:43-root-INFO: Loss too large (1208.778->1339.193)! Learning rate decreased to 0.00285.
2024-12-02-13:46:44-root-INFO: Loss too large (1208.778->1296.314)! Learning rate decreased to 0.00228.
2024-12-02-13:46:44-root-INFO: Loss too large (1208.778->1258.220)! Learning rate decreased to 0.00182.
2024-12-02-13:46:44-root-INFO: Loss too large (1208.778->1229.365)! Learning rate decreased to 0.00146.
2024-12-02-13:46:45-root-INFO: Loss too large (1208.778->1210.520)! Learning rate decreased to 0.00117.
2024-12-02-13:46:46-root-INFO: grad norm: 169.716 162.696 48.306
2024-12-02-13:46:47-root-INFO: grad norm: 149.600 144.519 38.654
2024-12-02-13:46:48-root-INFO: grad norm: 131.185 125.701 37.535
2024-12-02-13:46:49-root-INFO: grad norm: 118.594 114.458 31.045
2024-12-02-13:46:50-root-INFO: grad norm: 106.909 102.429 30.623
2024-12-02-13:46:51-root-INFO: grad norm: 98.409 94.917 25.980
2024-12-02-13:46:52-root-INFO: grad norm: 90.562 86.767 25.942
2024-12-02-13:46:52-root-INFO: Loss Change: 1208.778 -> 1176.974
2024-12-02-13:46:52-root-INFO: Regularization Change: 0.000 -> 0.193
2024-12-02-13:46:52-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-13:46:52-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-13:46:53-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-13:46:53-root-INFO: grad norm: 194.120 187.685 49.565
2024-12-02-13:46:53-root-INFO: Loss too large (1189.932->1430.521)! Learning rate decreased to 0.00579.
2024-12-02-13:46:54-root-INFO: Loss too large (1189.932->1397.059)! Learning rate decreased to 0.00463.
2024-12-02-13:46:54-root-INFO: Loss too large (1189.932->1358.372)! Learning rate decreased to 0.00370.
2024-12-02-13:46:54-root-INFO: Loss too large (1189.932->1315.164)! Learning rate decreased to 0.00296.
2024-12-02-13:46:55-root-INFO: Loss too large (1189.932->1271.825)! Learning rate decreased to 0.00237.
2024-12-02-13:46:55-root-INFO: Loss too large (1189.932->1234.571)! Learning rate decreased to 0.00190.
2024-12-02-13:46:55-root-INFO: Loss too large (1189.932->1207.211)! Learning rate decreased to 0.00152.
2024-12-02-13:46:56-root-INFO: grad norm: 240.518 230.512 68.652
2024-12-02-13:46:57-root-INFO: Loss too large (1189.818->1196.546)! Learning rate decreased to 0.00121.
2024-12-02-13:46:58-root-INFO: grad norm: 192.433 186.307 48.168
2024-12-02-13:46:59-root-INFO: grad norm: 152.149 145.836 43.373
2024-12-02-13:47:00-root-INFO: grad norm: 131.333 126.816 34.149
2024-12-02-13:47:01-root-INFO: grad norm: 112.657 108.000 32.055
2024-12-02-13:47:02-root-INFO: grad norm: 100.709 97.157 26.510
2024-12-02-13:47:02-root-INFO: grad norm: 89.945 86.250 25.517
2024-12-02-13:47:03-root-INFO: Loss Change: 1189.932 -> 1155.288
2024-12-02-13:47:03-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-13:47:03-root-INFO: Undo step: 170
2024-12-02-13:47:03-root-INFO: Undo step: 171
2024-12-02-13:47:03-root-INFO: Undo step: 172
2024-12-02-13:47:03-root-INFO: Undo step: 173
2024-12-02-13:47:03-root-INFO: Undo step: 174
2024-12-02-13:47:03-root-INFO: Undo step: 175
2024-12-02-13:47:03-root-INFO: Undo step: 176
2024-12-02-13:47:03-root-INFO: Undo step: 177
2024-12-02-13:47:03-root-INFO: Undo step: 178
2024-12-02-13:47:03-root-INFO: Undo step: 179
2024-12-02-13:47:04-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-13:47:04-root-INFO: grad norm: 829.859 783.398 273.778
2024-12-02-13:47:05-root-INFO: grad norm: 808.851 777.432 223.248
2024-12-02-13:47:05-root-INFO: Loss too large (2842.143->2889.955)! Learning rate decreased to 0.00387.
2024-12-02-13:47:06-root-INFO: grad norm: 655.243 630.154 179.582
2024-12-02-13:47:07-root-INFO: grad norm: 434.031 410.193 141.860
2024-12-02-13:47:08-root-INFO: grad norm: 347.863 329.721 110.872
2024-12-02-13:47:09-root-INFO: grad norm: 284.578 265.525 102.378
2024-12-02-13:47:10-root-INFO: grad norm: 293.385 276.786 97.283
2024-12-02-13:47:11-root-INFO: grad norm: 396.552 371.267 139.335
2024-12-02-13:47:12-root-INFO: Loss too large (1840.584->1977.178)! Learning rate decreased to 0.00309.
2024-12-02-13:47:12-root-INFO: Loss Change: 3468.523 -> 1836.494
2024-12-02-13:47:12-root-INFO: Regularization Change: 0.000 -> 27.625
2024-12-02-13:47:12-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-13:47:12-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-13:47:13-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-13:47:13-root-INFO: grad norm: 476.982 460.413 124.628
2024-12-02-13:47:13-root-INFO: Loss too large (1835.230->2286.226)! Learning rate decreased to 0.00403.
2024-12-02-13:47:14-root-INFO: Loss too large (1835.230->2027.979)! Learning rate decreased to 0.00322.
2024-12-02-13:47:14-root-INFO: Loss too large (1835.230->1852.535)! Learning rate decreased to 0.00258.
2024-12-02-13:47:15-root-INFO: grad norm: 361.810 346.271 104.893
2024-12-02-13:47:16-root-INFO: grad norm: 309.667 300.158 76.150
2024-12-02-13:47:17-root-INFO: grad norm: 300.446 286.520 90.411
2024-12-02-13:47:18-root-INFO: grad norm: 315.780 305.524 79.823
2024-12-02-13:47:19-root-INFO: Loss too large (1618.285->1620.310)! Learning rate decreased to 0.00206.
2024-12-02-13:47:20-root-INFO: grad norm: 249.862 238.346 74.982
2024-12-02-13:47:20-root-INFO: grad norm: 223.605 215.864 58.326
2024-12-02-13:47:21-root-INFO: grad norm: 223.709 213.577 66.561
2024-12-02-13:47:22-root-INFO: Loss Change: 1835.230 -> 1525.891
2024-12-02-13:47:22-root-INFO: Regularization Change: 0.000 -> 3.419
2024-12-02-13:47:22-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-13:47:22-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-13:47:23-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-13:47:23-root-INFO: grad norm: 322.389 311.282 83.893
2024-12-02-13:47:23-root-INFO: Loss too large (1537.822->1702.852)! Learning rate decreased to 0.00420.
2024-12-02-13:47:24-root-INFO: Loss too large (1537.822->1658.412)! Learning rate decreased to 0.00336.
2024-12-02-13:47:24-root-INFO: Loss too large (1537.822->1612.885)! Learning rate decreased to 0.00269.
2024-12-02-13:47:24-root-INFO: Loss too large (1537.822->1568.545)! Learning rate decreased to 0.00215.
2024-12-02-13:47:25-root-INFO: grad norm: 262.661 250.059 80.381
2024-12-02-13:47:26-root-INFO: grad norm: 206.901 198.896 56.996
2024-12-02-13:47:27-root-INFO: grad norm: 226.704 215.838 69.347
2024-12-02-13:47:28-root-INFO: grad norm: 252.231 243.177 66.976
2024-12-02-13:47:29-root-INFO: Loss too large (1468.860->1471.096)! Learning rate decreased to 0.00172.
2024-12-02-13:47:30-root-INFO: grad norm: 199.172 189.714 60.646
2024-12-02-13:47:31-root-INFO: grad norm: 170.156 163.411 47.434
2024-12-02-13:47:32-root-INFO: grad norm: 162.745 155.452 48.175
2024-12-02-13:47:32-root-INFO: Loss Change: 1537.822 -> 1418.212
2024-12-02-13:47:32-root-INFO: Regularization Change: 0.000 -> 1.212
2024-12-02-13:47:32-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-13:47:32-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-13:47:33-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-13:47:33-root-INFO: grad norm: 238.121 228.929 65.523
2024-12-02-13:47:33-root-INFO: Loss too large (1423.988->1591.125)! Learning rate decreased to 0.00437.
2024-12-02-13:47:34-root-INFO: Loss too large (1423.988->1551.531)! Learning rate decreased to 0.00350.
2024-12-02-13:47:34-root-INFO: Loss too large (1423.988->1510.573)! Learning rate decreased to 0.00280.
2024-12-02-13:47:34-root-INFO: Loss too large (1423.988->1471.725)! Learning rate decreased to 0.00224.
2024-12-02-13:47:35-root-INFO: Loss too large (1423.988->1439.406)! Learning rate decreased to 0.00179.
2024-12-02-13:47:36-root-INFO: grad norm: 223.872 213.089 68.642
2024-12-02-13:47:37-root-INFO: grad norm: 210.619 202.486 57.963
2024-12-02-13:47:38-root-INFO: grad norm: 206.246 196.116 63.843
2024-12-02-13:47:39-root-INFO: grad norm: 202.273 194.342 56.084
2024-12-02-13:47:40-root-INFO: grad norm: 200.881 190.872 62.617
2024-12-02-13:47:41-root-INFO: grad norm: 199.294 191.320 55.810
2024-12-02-13:47:42-root-INFO: grad norm: 198.797 188.745 62.416
2024-12-02-13:47:43-root-INFO: Loss Change: 1423.988 -> 1368.459
2024-12-02-13:47:43-root-INFO: Regularization Change: 0.000 -> 0.695
2024-12-02-13:47:43-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-13:47:43-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-13:47:43-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-13:47:43-root-INFO: grad norm: 280.764 270.107 76.618
2024-12-02-13:47:44-root-INFO: Loss too large (1384.724->1580.391)! Learning rate decreased to 0.00455.
2024-12-02-13:47:44-root-INFO: Loss too large (1384.724->1536.714)! Learning rate decreased to 0.00364.
2024-12-02-13:47:44-root-INFO: Loss too large (1384.724->1489.420)! Learning rate decreased to 0.00291.
2024-12-02-13:47:45-root-INFO: Loss too large (1384.724->1441.028)! Learning rate decreased to 0.00233.
2024-12-02-13:47:45-root-INFO: Loss too large (1384.724->1397.747)! Learning rate decreased to 0.00186.
2024-12-02-13:47:46-root-INFO: grad norm: 241.225 228.722 76.653
2024-12-02-13:47:47-root-INFO: grad norm: 215.352 206.451 61.274
2024-12-02-13:47:48-root-INFO: grad norm: 200.586 190.008 64.277
2024-12-02-13:47:49-root-INFO: grad norm: 189.868 181.603 55.409
2024-12-02-13:47:50-root-INFO: grad norm: 181.986 172.378 58.351
2024-12-02-13:47:51-root-INFO: grad norm: 176.335 168.375 52.385
2024-12-02-13:47:52-root-INFO: grad norm: 172.164 163.078 55.189
2024-12-02-13:47:53-root-INFO: Loss Change: 1384.724 -> 1317.747
2024-12-02-13:47:53-root-INFO: Regularization Change: 0.000 -> 0.605
2024-12-02-13:47:53-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-13:47:53-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-13:47:53-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-13:47:53-root-INFO: grad norm: 244.035 233.687 70.310
2024-12-02-13:47:54-root-INFO: Loss too large (1329.518->1522.524)! Learning rate decreased to 0.00474.
2024-12-02-13:47:54-root-INFO: Loss too large (1329.518->1476.779)! Learning rate decreased to 0.00379.
2024-12-02-13:47:54-root-INFO: Loss too large (1329.518->1427.323)! Learning rate decreased to 0.00303.
2024-12-02-13:47:55-root-INFO: Loss too large (1329.518->1378.734)! Learning rate decreased to 0.00243.
2024-12-02-13:47:55-root-INFO: Loss too large (1329.518->1338.715)! Learning rate decreased to 0.00194.
2024-12-02-13:47:56-root-INFO: grad norm: 212.874 201.375 69.019
2024-12-02-13:47:57-root-INFO: grad norm: 193.848 185.131 57.475
2024-12-02-13:47:58-root-INFO: grad norm: 176.951 167.349 57.496
2024-12-02-13:47:59-root-INFO: grad norm: 166.626 158.689 50.813
2024-12-02-13:48:00-root-INFO: grad norm: 157.169 148.765 50.703
2024-12-02-13:48:01-root-INFO: grad norm: 151.448 143.972 46.993
2024-12-02-13:48:02-root-INFO: grad norm: 146.539 138.811 46.960
2024-12-02-13:48:03-root-INFO: Loss Change: 1329.518 -> 1273.426
2024-12-02-13:48:03-root-INFO: Regularization Change: 0.000 -> 0.525
2024-12-02-13:48:03-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-13:48:03-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-13:48:03-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-13:48:03-root-INFO: grad norm: 172.830 164.344 53.488
2024-12-02-13:48:04-root-INFO: Loss too large (1275.358->1439.825)! Learning rate decreased to 0.00494.
2024-12-02-13:48:04-root-INFO: Loss too large (1275.358->1392.718)! Learning rate decreased to 0.00395.
2024-12-02-13:48:04-root-INFO: Loss too large (1275.358->1346.756)! Learning rate decreased to 0.00316.
2024-12-02-13:48:05-root-INFO: Loss too large (1275.358->1308.866)! Learning rate decreased to 0.00253.
2024-12-02-13:48:05-root-INFO: Loss too large (1275.358->1282.673)! Learning rate decreased to 0.00202.
2024-12-02-13:48:06-root-INFO: grad norm: 165.188 156.335 53.353
2024-12-02-13:48:07-root-INFO: grad norm: 159.378 151.549 49.337
2024-12-02-13:48:08-root-INFO: grad norm: 153.035 144.824 49.456
2024-12-02-13:48:09-root-INFO: grad norm: 148.969 141.465 46.685
2024-12-02-13:48:10-root-INFO: grad norm: 145.024 137.305 46.684
2024-12-02-13:48:11-root-INFO: grad norm: 142.807 135.487 45.132
2024-12-02-13:48:12-root-INFO: grad norm: 141.062 133.590 45.302
2024-12-02-13:48:13-root-INFO: Loss Change: 1275.358 -> 1240.827
2024-12-02-13:48:13-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-02-13:48:13-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-13:48:13-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-13:48:13-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-13:48:13-root-INFO: grad norm: 232.787 221.550 71.451
2024-12-02-13:48:14-root-INFO: Loss too large (1262.221->1466.377)! Learning rate decreased to 0.00514.
2024-12-02-13:48:14-root-INFO: Loss too large (1262.221->1415.367)! Learning rate decreased to 0.00411.
2024-12-02-13:48:15-root-INFO: Loss too large (1262.221->1359.716)! Learning rate decreased to 0.00329.
2024-12-02-13:48:15-root-INFO: Loss too large (1262.221->1305.839)! Learning rate decreased to 0.00263.
2024-12-02-13:48:15-root-INFO: Loss too large (1262.221->1263.949)! Learning rate decreased to 0.00210.
2024-12-02-13:48:16-root-INFO: grad norm: 192.005 181.732 61.962
2024-12-02-13:48:17-root-INFO: grad norm: 174.352 165.267 55.546
2024-12-02-13:48:18-root-INFO: grad norm: 154.547 146.419 49.459
2024-12-02-13:48:19-root-INFO: grad norm: 144.971 137.014 47.367
2024-12-02-13:48:20-root-INFO: grad norm: 134.990 128.059 42.699
2024-12-02-13:48:21-root-INFO: grad norm: 129.985 122.705 42.892
2024-12-02-13:48:22-root-INFO: grad norm: 125.412 119.073 39.367
2024-12-02-13:48:23-root-INFO: Loss Change: 1262.221 -> 1205.138
2024-12-02-13:48:23-root-INFO: Regularization Change: 0.000 -> 0.513
2024-12-02-13:48:23-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-13:48:23-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-13:48:23-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-13:48:24-root-INFO: grad norm: 157.650 149.185 50.963
2024-12-02-13:48:24-root-INFO: Loss too large (1207.695->1375.674)! Learning rate decreased to 0.00535.
2024-12-02-13:48:24-root-INFO: Loss too large (1207.695->1324.596)! Learning rate decreased to 0.00428.
2024-12-02-13:48:25-root-INFO: Loss too large (1207.695->1275.822)! Learning rate decreased to 0.00342.
2024-12-02-13:48:25-root-INFO: Loss too large (1207.695->1237.556)! Learning rate decreased to 0.00274.
2024-12-02-13:48:25-root-INFO: Loss too large (1207.695->1212.594)! Learning rate decreased to 0.00219.
2024-12-02-13:48:26-root-INFO: grad norm: 146.184 138.889 45.603
2024-12-02-13:48:27-root-INFO: grad norm: 140.199 132.733 45.142
2024-12-02-13:48:28-root-INFO: grad norm: 133.617 126.943 41.702
2024-12-02-13:48:29-root-INFO: grad norm: 130.517 123.523 42.151
2024-12-02-13:48:30-root-INFO: grad norm: 127.750 121.395 39.792
2024-12-02-13:48:31-root-INFO: grad norm: 126.903 120.120 40.934
2024-12-02-13:48:32-root-INFO: grad norm: 126.942 120.612 39.585
2024-12-02-13:48:33-root-INFO: Loss Change: 1207.695 -> 1176.530
2024-12-02-13:48:33-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-02-13:48:33-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-13:48:33-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-13:48:33-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-13:48:34-root-INFO: grad norm: 188.056 179.325 56.633
2024-12-02-13:48:34-root-INFO: Loss too large (1185.365->1388.068)! Learning rate decreased to 0.00556.
2024-12-02-13:48:34-root-INFO: Loss too large (1185.365->1335.225)! Learning rate decreased to 0.00445.
2024-12-02-13:48:35-root-INFO: Loss too large (1185.365->1277.944)! Learning rate decreased to 0.00356.
2024-12-02-13:48:35-root-INFO: Loss too large (1185.365->1226.702)! Learning rate decreased to 0.00285.
2024-12-02-13:48:35-root-INFO: Loss too large (1185.365->1190.596)! Learning rate decreased to 0.00228.
2024-12-02-13:48:36-root-INFO: grad norm: 167.152 158.483 53.132
2024-12-02-13:48:37-root-INFO: Loss too large (1170.069->1170.330)! Learning rate decreased to 0.00182.
2024-12-02-13:48:38-root-INFO: grad norm: 112.368 106.192 36.741
2024-12-02-13:48:39-root-INFO: grad norm: 71.047 68.231 19.803
2024-12-02-13:48:40-root-INFO: grad norm: 55.908 52.463 19.319
2024-12-02-13:48:41-root-INFO: grad norm: 46.216 44.655 11.910
2024-12-02-13:48:42-root-INFO: grad norm: 41.601 39.227 13.854
2024-12-02-13:48:43-root-INFO: grad norm: 38.868 37.527 10.120
2024-12-02-13:48:43-root-INFO: Loss Change: 1185.365 -> 1139.668
2024-12-02-13:48:43-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-13:48:44-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-13:48:44-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-13:48:44-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-13:48:44-root-INFO: grad norm: 107.523 101.658 35.027
2024-12-02-13:48:45-root-INFO: Loss too large (1143.344->1255.223)! Learning rate decreased to 0.00579.
2024-12-02-13:48:45-root-INFO: Loss too large (1143.344->1210.760)! Learning rate decreased to 0.00463.
2024-12-02-13:48:45-root-INFO: Loss too large (1143.344->1178.228)! Learning rate decreased to 0.00370.
2024-12-02-13:48:46-root-INFO: Loss too large (1143.344->1157.221)! Learning rate decreased to 0.00296.
2024-12-02-13:48:46-root-INFO: Loss too large (1143.344->1144.979)! Learning rate decreased to 0.00237.
2024-12-02-13:48:47-root-INFO: grad norm: 123.684 118.515 35.384
2024-12-02-13:48:47-root-INFO: Loss too large (1138.547->1140.297)! Learning rate decreased to 0.00190.
2024-12-02-13:48:48-root-INFO: grad norm: 104.112 98.728 33.049
2024-12-02-13:48:49-root-INFO: grad norm: 85.055 81.843 23.153
2024-12-02-13:48:50-root-INFO: grad norm: 75.130 71.038 24.457
2024-12-02-13:48:51-root-INFO: grad norm: 66.144 63.860 17.233
2024-12-02-13:48:52-root-INFO: grad norm: 60.796 57.455 19.877
2024-12-02-13:48:53-root-INFO: grad norm: 56.185 54.360 14.201
2024-12-02-13:48:54-root-INFO: Loss Change: 1143.344 -> 1114.823
2024-12-02-13:48:54-root-INFO: Regularization Change: 0.000 -> 0.362
2024-12-02-13:48:54-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-13:48:54-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-13:48:54-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-13:48:54-root-INFO: grad norm: 107.714 102.377 33.485
2024-12-02-13:48:55-root-INFO: Loss too large (1118.061->1255.609)! Learning rate decreased to 0.00602.
2024-12-02-13:48:55-root-INFO: Loss too large (1118.061->1207.074)! Learning rate decreased to 0.00482.
2024-12-02-13:48:56-root-INFO: Loss too large (1118.061->1168.201)! Learning rate decreased to 0.00385.
2024-12-02-13:48:56-root-INFO: Loss too large (1118.061->1141.542)! Learning rate decreased to 0.00308.
2024-12-02-13:48:56-root-INFO: Loss too large (1118.061->1125.310)! Learning rate decreased to 0.00247.
2024-12-02-13:48:57-root-INFO: grad norm: 141.108 135.189 40.441
2024-12-02-13:48:57-root-INFO: Loss too large (1116.374->1122.490)! Learning rate decreased to 0.00197.
2024-12-02-13:48:59-root-INFO: grad norm: 123.808 118.214 36.794
2024-12-02-13:49:00-root-INFO: grad norm: 104.084 100.070 28.626
2024-12-02-13:49:00-root-INFO: grad norm: 94.861 90.253 29.206
2024-12-02-13:49:01-root-INFO: grad norm: 85.252 82.177 22.690
2024-12-02-13:49:02-root-INFO: grad norm: 80.079 76.080 24.990
2024-12-02-13:49:04-root-INFO: grad norm: 75.093 72.505 19.545
2024-12-02-13:49:04-root-INFO: Loss Change: 1118.061 -> 1092.916
2024-12-02-13:49:04-root-INFO: Regularization Change: 0.000 -> 0.344
2024-12-02-13:49:04-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-13:49:04-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-13:49:05-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-13:49:05-root-INFO: grad norm: 130.006 124.281 38.152
2024-12-02-13:49:05-root-INFO: Loss too large (1099.850->1289.812)! Learning rate decreased to 0.00626.
2024-12-02-13:49:06-root-INFO: Loss too large (1099.850->1237.228)! Learning rate decreased to 0.00501.
2024-12-02-13:49:06-root-INFO: Loss too large (1099.850->1185.068)! Learning rate decreased to 0.00401.
2024-12-02-13:49:06-root-INFO: Loss too large (1099.850->1143.404)! Learning rate decreased to 0.00321.
2024-12-02-13:49:07-root-INFO: Loss too large (1099.850->1115.787)! Learning rate decreased to 0.00256.
2024-12-02-13:49:07-root-INFO: Loss too large (1099.850->1099.917)! Learning rate decreased to 0.00205.
2024-12-02-13:49:08-root-INFO: grad norm: 109.962 105.765 30.087
2024-12-02-13:49:09-root-INFO: grad norm: 100.941 96.162 30.692
2024-12-02-13:49:10-root-INFO: grad norm: 90.715 87.443 24.142
2024-12-02-13:49:11-root-INFO: grad norm: 85.552 81.358 26.456
2024-12-02-13:49:12-root-INFO: grad norm: 80.201 77.421 20.935
2024-12-02-13:49:13-root-INFO: grad norm: 77.445 73.618 24.047
2024-12-02-13:49:14-root-INFO: grad norm: 74.954 72.411 19.359
2024-12-02-13:49:15-root-INFO: Loss Change: 1099.850 -> 1072.490
2024-12-02-13:49:15-root-INFO: Regularization Change: 0.000 -> 0.326
2024-12-02-13:49:15-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-13:49:15-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-13:49:15-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-13:49:15-root-INFO: grad norm: 140.193 134.105 40.865
2024-12-02-13:49:16-root-INFO: Loss too large (1081.135->1291.131)! Learning rate decreased to 0.00651.
2024-12-02-13:49:16-root-INFO: Loss too large (1081.135->1236.204)! Learning rate decreased to 0.00521.
2024-12-02-13:49:16-root-INFO: Loss too large (1081.135->1180.070)! Learning rate decreased to 0.00417.
2024-12-02-13:49:17-root-INFO: Loss too large (1081.135->1133.115)! Learning rate decreased to 0.00333.
2024-12-02-13:49:17-root-INFO: Loss too large (1081.135->1100.807)! Learning rate decreased to 0.00267.
2024-12-02-13:49:17-root-INFO: Loss too large (1081.135->1081.761)! Learning rate decreased to 0.00213.
2024-12-02-13:49:18-root-INFO: grad norm: 127.885 123.121 34.577
2024-12-02-13:49:19-root-INFO: grad norm: 121.875 116.549 35.637
2024-12-02-13:49:20-root-INFO: grad norm: 113.896 109.728 30.528
2024-12-02-13:49:21-root-INFO: grad norm: 109.918 105.058 32.323
2024-12-02-13:49:22-root-INFO: grad norm: 104.993 101.208 27.939
2024-12-02-13:49:23-root-INFO: grad norm: 102.661 98.108 30.234
2024-12-02-13:49:24-root-INFO: grad norm: 100.136 96.548 26.564
2024-12-02-13:49:25-root-INFO: Loss Change: 1081.135 -> 1052.466
2024-12-02-13:49:25-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-02-13:49:25-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-13:49:25-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-13:49:25-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-13:49:26-root-INFO: grad norm: 162.301 156.454 43.173
2024-12-02-13:49:26-root-INFO: Loss too large (1062.003->1303.818)! Learning rate decreased to 0.00677.
2024-12-02-13:49:26-root-INFO: Loss too large (1062.003->1253.833)! Learning rate decreased to 0.00542.
2024-12-02-13:49:27-root-INFO: Loss too large (1062.003->1195.198)! Learning rate decreased to 0.00433.
2024-12-02-13:49:27-root-INFO: Loss too large (1062.003->1136.925)! Learning rate decreased to 0.00347.
2024-12-02-13:49:27-root-INFO: Loss too large (1062.003->1091.768)! Learning rate decreased to 0.00277.
2024-12-02-13:49:28-root-INFO: Loss too large (1062.003->1063.612)! Learning rate decreased to 0.00222.
2024-12-02-13:49:29-root-INFO: grad norm: 142.435 137.165 38.386
2024-12-02-13:49:29-root-INFO: Loss too large (1048.966->1049.125)! Learning rate decreased to 0.00177.
2024-12-02-13:49:30-root-INFO: grad norm: 97.116 92.786 28.674
2024-12-02-13:49:31-root-INFO: grad norm: 59.628 57.839 14.497
2024-12-02-13:49:32-root-INFO: grad norm: 47.152 44.510 15.561
2024-12-02-13:49:33-root-INFO: grad norm: 39.473 38.183 10.008
2024-12-02-13:49:34-root-INFO: grad norm: 36.107 34.165 11.683
2024-12-02-13:49:35-root-INFO: grad norm: 34.268 32.978 9.316
2024-12-02-13:49:36-root-INFO: Loss Change: 1062.003 -> 1025.582
2024-12-02-13:49:36-root-INFO: Regularization Change: 0.000 -> 0.304
2024-12-02-13:49:36-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-13:49:36-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-13:49:36-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-13:49:37-root-INFO: grad norm: 67.772 64.471 20.893
2024-12-02-13:49:37-root-INFO: Loss too large (1027.820->1088.425)! Learning rate decreased to 0.00704.
2024-12-02-13:49:37-root-INFO: Loss too large (1027.820->1060.722)! Learning rate decreased to 0.00563.
2024-12-02-13:49:38-root-INFO: Loss too large (1027.820->1043.367)! Learning rate decreased to 0.00450.
2024-12-02-13:49:38-root-INFO: Loss too large (1027.820->1033.228)! Learning rate decreased to 0.00360.
2024-12-02-13:49:39-root-INFO: grad norm: 127.867 123.348 33.691
2024-12-02-13:49:39-root-INFO: Loss too large (1027.686->1052.493)! Learning rate decreased to 0.00288.
2024-12-02-13:49:40-root-INFO: Loss too large (1027.686->1037.641)! Learning rate decreased to 0.00231.
2024-12-02-13:49:40-root-INFO: Loss too large (1027.686->1028.303)! Learning rate decreased to 0.00184.
2024-12-02-13:49:41-root-INFO: grad norm: 91.920 88.112 26.183
2024-12-02-13:49:42-root-INFO: grad norm: 60.459 58.699 14.481
2024-12-02-13:49:43-root-INFO: grad norm: 48.805 46.317 15.383
2024-12-02-13:49:44-root-INFO: grad norm: 40.714 39.505 9.849
2024-12-02-13:49:45-root-INFO: grad norm: 36.836 34.960 11.607
2024-12-02-13:49:46-root-INFO: grad norm: 34.434 33.279 8.843
2024-12-02-13:49:47-root-INFO: Loss Change: 1027.820 -> 1006.473
2024-12-02-13:49:47-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-02-13:49:47-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-13:49:47-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-13:49:47-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-13:49:47-root-INFO: grad norm: 96.503 92.367 27.948
2024-12-02-13:49:48-root-INFO: Loss too large (1010.196->1171.301)! Learning rate decreased to 0.00732.
2024-12-02-13:49:48-root-INFO: Loss too large (1010.196->1116.553)! Learning rate decreased to 0.00585.
2024-12-02-13:49:48-root-INFO: Loss too large (1010.196->1072.024)! Learning rate decreased to 0.00468.
2024-12-02-13:49:49-root-INFO: Loss too large (1010.196->1041.179)! Learning rate decreased to 0.00375.
2024-12-02-13:49:49-root-INFO: Loss too large (1010.196->1022.190)! Learning rate decreased to 0.00300.
2024-12-02-13:49:49-root-INFO: Loss too large (1010.196->1011.533)! Learning rate decreased to 0.00240.
2024-12-02-13:49:50-root-INFO: grad norm: 106.863 103.378 27.068
2024-12-02-13:49:51-root-INFO: Loss too large (1006.098->1006.598)! Learning rate decreased to 0.00192.
2024-12-02-13:49:52-root-INFO: grad norm: 85.060 81.437 24.560
2024-12-02-13:49:53-root-INFO: grad norm: 63.743 61.883 15.289
2024-12-02-13:49:54-root-INFO: grad norm: 53.965 51.282 16.804
2024-12-02-13:49:55-root-INFO: grad norm: 45.771 44.443 10.947
2024-12-02-13:49:56-root-INFO: grad norm: 41.207 39.074 13.086
2024-12-02-13:49:57-root-INFO: grad norm: 37.711 36.540 9.323
2024-12-02-13:49:58-root-INFO: Loss Change: 1010.196 -> 986.783
2024-12-02-13:49:58-root-INFO: Regularization Change: 0.000 -> 0.296
2024-12-02-13:49:58-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-13:49:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-13:49:58-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-13:49:58-root-INFO: grad norm: 82.927 79.714 22.861
2024-12-02-13:49:59-root-INFO: Loss too large (987.285->1131.104)! Learning rate decreased to 0.00760.
2024-12-02-13:49:59-root-INFO: Loss too large (987.285->1080.351)! Learning rate decreased to 0.00608.
2024-12-02-13:49:59-root-INFO: Loss too large (987.285->1040.897)! Learning rate decreased to 0.00487.
2024-12-02-13:50:00-root-INFO: Loss too large (987.285->1014.414)! Learning rate decreased to 0.00389.
2024-12-02-13:50:00-root-INFO: Loss too large (987.285->998.378)! Learning rate decreased to 0.00311.
2024-12-02-13:50:00-root-INFO: Loss too large (987.285->989.411)! Learning rate decreased to 0.00249.
2024-12-02-13:50:01-root-INFO: grad norm: 99.631 96.795 23.599
2024-12-02-13:50:02-root-INFO: Loss too large (984.798->985.943)! Learning rate decreased to 0.00199.
2024-12-02-13:50:03-root-INFO: grad norm: 83.043 79.985 22.328
2024-12-02-13:50:04-root-INFO: grad norm: 65.441 63.721 14.906
2024-12-02-13:50:05-root-INFO: grad norm: 57.036 54.614 16.445
2024-12-02-13:50:06-root-INFO: grad norm: 49.188 47.896 11.200
2024-12-02-13:50:07-root-INFO: grad norm: 44.663 42.633 13.313
2024-12-02-13:50:08-root-INFO: grad norm: 40.737 39.609 9.523
2024-12-02-13:50:08-root-INFO: Loss Change: 987.285 -> 967.161
2024-12-02-13:50:08-root-INFO: Regularization Change: 0.000 -> 0.278
2024-12-02-13:50:08-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-13:50:08-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:50:09-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-13:50:09-root-INFO: grad norm: 116.064 111.969 30.560
2024-12-02-13:50:09-root-INFO: Loss too large (979.740->1198.535)! Learning rate decreased to 0.00790.
2024-12-02-13:50:10-root-INFO: Loss too large (979.740->1138.441)! Learning rate decreased to 0.00632.
2024-12-02-13:50:10-root-INFO: Loss too large (979.740->1080.352)! Learning rate decreased to 0.00506.
2024-12-02-13:50:10-root-INFO: Loss too large (979.740->1034.189)! Learning rate decreased to 0.00404.
2024-12-02-13:50:11-root-INFO: Loss too large (979.740->1003.148)! Learning rate decreased to 0.00324.
2024-12-02-13:50:11-root-INFO: Loss too large (979.740->984.771)! Learning rate decreased to 0.00259.
2024-12-02-13:50:12-root-INFO: grad norm: 140.929 136.557 34.832
2024-12-02-13:50:12-root-INFO: Loss too large (975.044->980.595)! Learning rate decreased to 0.00207.
2024-12-02-13:50:13-root-INFO: grad norm: 115.314 111.546 29.236
2024-12-02-13:50:14-root-INFO: grad norm: 83.160 80.858 19.431
2024-12-02-13:50:15-root-INFO: grad norm: 71.599 68.656 20.316
2024-12-02-13:50:16-root-INFO: grad norm: 59.495 57.911 13.637
2024-12-02-13:50:17-root-INFO: grad norm: 53.089 50.672 15.837
2024-12-02-13:50:18-root-INFO: grad norm: 47.029 45.754 10.880
2024-12-02-13:50:19-root-INFO: Loss Change: 979.740 -> 951.196
2024-12-02-13:50:19-root-INFO: Regularization Change: 0.000 -> 0.361
2024-12-02-13:50:19-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-13:50:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:50:19-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-13:50:20-root-INFO: grad norm: 95.078 91.970 24.112
2024-12-02-13:50:20-root-INFO: Loss too large (954.489->1146.436)! Learning rate decreased to 0.00821.
2024-12-02-13:50:20-root-INFO: Loss too large (954.489->1087.895)! Learning rate decreased to 0.00656.
2024-12-02-13:50:21-root-INFO: Loss too large (954.489->1035.331)! Learning rate decreased to 0.00525.
2024-12-02-13:50:21-root-INFO: Loss too large (954.489->996.816)! Learning rate decreased to 0.00420.
2024-12-02-13:50:21-root-INFO: Loss too large (954.489->972.463)! Learning rate decreased to 0.00336.
2024-12-02-13:50:22-root-INFO: Loss too large (954.489->958.613)! Learning rate decreased to 0.00269.
2024-12-02-13:50:23-root-INFO: grad norm: 113.516 110.212 27.187
2024-12-02-13:50:23-root-INFO: Loss too large (951.448->954.159)! Learning rate decreased to 0.00215.
2024-12-02-13:50:24-root-INFO: grad norm: 91.596 88.692 22.881
2024-12-02-13:50:25-root-INFO: grad norm: 66.567 64.794 15.265
2024-12-02-13:50:26-root-INFO: grad norm: 56.829 54.577 15.838
2024-12-02-13:50:27-root-INFO: grad norm: 47.602 46.309 11.016
2024-12-02-13:50:28-root-INFO: grad norm: 42.675 40.794 12.529
2024-12-02-13:50:29-root-INFO: grad norm: 38.447 37.317 9.251
2024-12-02-13:50:30-root-INFO: Loss Change: 954.489 -> 931.878
2024-12-02-13:50:30-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-02-13:50:30-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-13:50:30-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:50:30-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-13:50:31-root-INFO: grad norm: 91.318 88.292 23.310
2024-12-02-13:50:31-root-INFO: Loss too large (939.094->1123.884)! Learning rate decreased to 0.00852.
2024-12-02-13:50:31-root-INFO: Loss too large (939.094->1063.685)! Learning rate decreased to 0.00682.
2024-12-02-13:50:32-root-INFO: Loss too large (939.094->1012.544)! Learning rate decreased to 0.00545.
2024-12-02-13:50:32-root-INFO: Loss too large (939.094->976.539)! Learning rate decreased to 0.00436.
2024-12-02-13:50:32-root-INFO: Loss too large (939.094->954.320)! Learning rate decreased to 0.00349.
2024-12-02-13:50:33-root-INFO: Loss too large (939.094->941.873)! Learning rate decreased to 0.00279.
2024-12-02-13:50:34-root-INFO: grad norm: 107.288 104.252 25.344
2024-12-02-13:50:34-root-INFO: Loss too large (935.517->937.659)! Learning rate decreased to 0.00223.
2024-12-02-13:50:35-root-INFO: grad norm: 86.853 84.038 21.932
2024-12-02-13:50:36-root-INFO: grad norm: 63.383 61.724 14.406
2024-12-02-13:50:37-root-INFO: grad norm: 54.135 51.910 15.361
2024-12-02-13:50:38-root-INFO: grad norm: 45.398 44.159 10.536
2024-12-02-13:50:39-root-INFO: grad norm: 40.736 38.861 12.217
2024-12-02-13:50:40-root-INFO: grad norm: 36.773 35.659 8.984
2024-12-02-13:50:41-root-INFO: Loss Change: 939.094 -> 916.099
2024-12-02-13:50:41-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-02-13:50:41-root-INFO: Undo step: 160
2024-12-02-13:50:41-root-INFO: Undo step: 161
2024-12-02-13:50:41-root-INFO: Undo step: 162
2024-12-02-13:50:41-root-INFO: Undo step: 163
2024-12-02-13:50:41-root-INFO: Undo step: 164
2024-12-02-13:50:41-root-INFO: Undo step: 165
2024-12-02-13:50:41-root-INFO: Undo step: 166
2024-12-02-13:50:41-root-INFO: Undo step: 167
2024-12-02-13:50:41-root-INFO: Undo step: 168
2024-12-02-13:50:41-root-INFO: Undo step: 169
2024-12-02-13:50:41-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-13:50:41-root-INFO: grad norm: 739.672 718.163 177.077
2024-12-02-13:50:42-root-INFO: grad norm: 990.711 966.022 219.798
2024-12-02-13:50:43-root-INFO: grad norm: 577.224 556.116 154.671
2024-12-02-13:50:44-root-INFO: grad norm: 347.105 339.572 71.922
2024-12-02-13:50:45-root-INFO: grad norm: 520.647 511.105 99.222
2024-12-02-13:50:46-root-INFO: Loss too large (1824.219->1958.465)! Learning rate decreased to 0.00579.
2024-12-02-13:50:46-root-INFO: Loss too large (1824.219->1875.810)! Learning rate decreased to 0.00463.
2024-12-02-13:50:47-root-INFO: grad norm: 202.971 198.837 40.758
2024-12-02-13:50:48-root-INFO: grad norm: 147.680 143.863 33.362
2024-12-02-13:50:49-root-INFO: grad norm: 229.684 225.734 42.410
2024-12-02-13:50:50-root-INFO: Loss Change: 3213.018 -> 1515.385
2024-12-02-13:50:50-root-INFO: Regularization Change: 0.000 -> 56.795
2024-12-02-13:50:50-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-13:50:50-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-13:50:50-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-13:50:51-root-INFO: grad norm: 374.812 369.883 60.586
2024-12-02-13:50:51-root-INFO: Loss too large (1532.195->1687.292)! Learning rate decreased to 0.00602.
2024-12-02-13:50:51-root-INFO: Loss too large (1532.195->1649.209)! Learning rate decreased to 0.00482.
2024-12-02-13:50:52-root-INFO: Loss too large (1532.195->1610.003)! Learning rate decreased to 0.00385.
2024-12-02-13:50:52-root-INFO: Loss too large (1532.195->1563.050)! Learning rate decreased to 0.00308.
2024-12-02-13:50:53-root-INFO: grad norm: 254.055 250.048 44.944
2024-12-02-13:50:54-root-INFO: grad norm: 163.358 159.819 33.816
2024-12-02-13:50:55-root-INFO: grad norm: 249.969 246.077 43.941
2024-12-02-13:50:55-root-INFO: Loss too large (1384.672->1443.617)! Learning rate decreased to 0.00247.
2024-12-02-13:50:56-root-INFO: Loss too large (1384.672->1398.985)! Learning rate decreased to 0.00197.
2024-12-02-13:50:56-root-INFO: grad norm: 247.114 244.309 37.130
2024-12-02-13:50:57-root-INFO: grad norm: 259.649 255.917 43.863
2024-12-02-13:50:58-root-INFO: grad norm: 270.968 268.167 38.856
2024-12-02-13:50:59-root-INFO: grad norm: 282.123 278.296 46.309
2024-12-02-13:51:00-root-INFO: Loss Change: 1532.195 -> 1344.728
2024-12-02-13:51:00-root-INFO: Regularization Change: 0.000 -> 2.260
2024-12-02-13:51:00-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-13:51:00-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-13:51:00-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-13:51:01-root-INFO: grad norm: 311.802 308.950 42.075
2024-12-02-13:51:01-root-INFO: Loss too large (1360.350->1557.978)! Learning rate decreased to 0.00626.
2024-12-02-13:51:01-root-INFO: Loss too large (1360.350->1536.994)! Learning rate decreased to 0.00501.
2024-12-02-13:51:02-root-INFO: Loss too large (1360.350->1509.746)! Learning rate decreased to 0.00401.
2024-12-02-13:51:02-root-INFO: Loss too large (1360.350->1472.078)! Learning rate decreased to 0.00321.
2024-12-02-13:51:02-root-INFO: Loss too large (1360.350->1423.869)! Learning rate decreased to 0.00256.
2024-12-02-13:51:03-root-INFO: Loss too large (1360.350->1371.080)! Learning rate decreased to 0.00205.
2024-12-02-13:51:04-root-INFO: grad norm: 293.426 289.755 46.267
2024-12-02-13:51:04-root-INFO: Loss too large (1325.591->1331.469)! Learning rate decreased to 0.00164.
2024-12-02-13:51:05-root-INFO: grad norm: 232.425 230.070 33.004
2024-12-02-13:51:06-root-INFO: grad norm: 218.141 215.113 36.222
2024-12-02-13:51:07-root-INFO: grad norm: 220.663 218.488 30.904
2024-12-02-13:51:08-root-INFO: grad norm: 233.889 230.787 37.967
2024-12-02-13:51:09-root-INFO: Loss too large (1273.691->1276.395)! Learning rate decreased to 0.00131.
2024-12-02-13:51:10-root-INFO: grad norm: 182.344 180.361 26.818
2024-12-02-13:51:11-root-INFO: grad norm: 149.308 146.800 27.252
2024-12-02-13:51:11-root-INFO: Loss Change: 1360.350 -> 1245.339
2024-12-02-13:51:11-root-INFO: Regularization Change: 0.000 -> 0.718
2024-12-02-13:51:11-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-13:51:11-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-13:51:12-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-13:51:12-root-INFO: grad norm: 201.580 199.175 31.045
2024-12-02-13:51:12-root-INFO: Loss too large (1248.871->1484.536)! Learning rate decreased to 0.00651.
2024-12-02-13:51:13-root-INFO: Loss too large (1248.871->1459.760)! Learning rate decreased to 0.00521.
2024-12-02-13:51:13-root-INFO: Loss too large (1248.871->1428.033)! Learning rate decreased to 0.00417.
2024-12-02-13:51:13-root-INFO: Loss too large (1248.871->1388.997)! Learning rate decreased to 0.00333.
2024-12-02-13:51:14-root-INFO: Loss too large (1248.871->1345.326)! Learning rate decreased to 0.00267.
2024-12-02-13:51:14-root-INFO: Loss too large (1248.871->1303.283)! Learning rate decreased to 0.00213.
2024-12-02-13:51:14-root-INFO: Loss too large (1248.871->1269.616)! Learning rate decreased to 0.00171.
2024-12-02-13:51:15-root-INFO: grad norm: 263.882 260.940 39.297
2024-12-02-13:51:16-root-INFO: Loss too large (1247.308->1261.153)! Learning rate decreased to 0.00137.
2024-12-02-13:51:17-root-INFO: grad norm: 220.333 218.203 30.561
2024-12-02-13:51:18-root-INFO: grad norm: 182.416 180.046 29.309
2024-12-02-13:51:19-root-INFO: grad norm: 175.907 174.062 25.413
2024-12-02-13:51:20-root-INFO: grad norm: 172.431 170.137 28.036
2024-12-02-13:51:21-root-INFO: grad norm: 173.025 171.256 24.673
2024-12-02-13:51:22-root-INFO: grad norm: 177.493 175.191 28.488
2024-12-02-13:51:22-root-INFO: Loss Change: 1248.871 -> 1204.788
2024-12-02-13:51:22-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-02-13:51:22-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-13:51:22-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-13:51:23-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-13:51:23-root-INFO: grad norm: 233.479 231.320 31.684
2024-12-02-13:51:23-root-INFO: Loss too large (1213.147->1464.357)! Learning rate decreased to 0.00677.
2024-12-02-13:51:24-root-INFO: Loss too large (1213.147->1443.912)! Learning rate decreased to 0.00542.
2024-12-02-13:51:24-root-INFO: Loss too large (1213.147->1417.397)! Learning rate decreased to 0.00433.
2024-12-02-13:51:24-root-INFO: Loss too large (1213.147->1382.121)! Learning rate decreased to 0.00347.
2024-12-02-13:51:25-root-INFO: Loss too large (1213.147->1337.851)! Learning rate decreased to 0.00277.
2024-12-02-13:51:25-root-INFO: Loss too large (1213.147->1288.372)! Learning rate decreased to 0.00222.
2024-12-02-13:51:25-root-INFO: Loss too large (1213.147->1242.801)! Learning rate decreased to 0.00177.
2024-12-02-13:51:26-root-INFO: grad norm: 306.813 303.705 43.561
2024-12-02-13:51:27-root-INFO: Loss too large (1209.820->1235.039)! Learning rate decreased to 0.00142.
2024-12-02-13:51:28-root-INFO: grad norm: 242.146 240.161 30.944
2024-12-02-13:51:29-root-INFO: grad norm: 168.227 166.135 26.445
2024-12-02-13:51:30-root-INFO: grad norm: 166.788 165.135 23.424
2024-12-02-13:51:31-root-INFO: grad norm: 167.713 165.631 26.342
2024-12-02-13:51:31-root-INFO: Loss too large (1171.254->1171.335)! Learning rate decreased to 0.00114.
2024-12-02-13:51:32-root-INFO: grad norm: 125.706 124.176 19.554
2024-12-02-13:51:33-root-INFO: grad norm: 90.042 88.212 18.060
2024-12-02-13:51:34-root-INFO: Loss Change: 1213.147 -> 1155.794
2024-12-02-13:51:34-root-INFO: Regularization Change: 0.000 -> 0.369
2024-12-02-13:51:34-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-13:51:34-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-13:51:34-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-13:51:34-root-INFO: grad norm: 125.459 123.788 20.405
2024-12-02-13:51:35-root-INFO: Loss too large (1157.661->1374.295)! Learning rate decreased to 0.00704.
2024-12-02-13:51:35-root-INFO: Loss too large (1157.661->1341.814)! Learning rate decreased to 0.00563.
2024-12-02-13:51:35-root-INFO: Loss too large (1157.661->1303.002)! Learning rate decreased to 0.00450.
2024-12-02-13:51:36-root-INFO: Loss too large (1157.661->1261.193)! Learning rate decreased to 0.00360.
2024-12-02-13:51:36-root-INFO: Loss too large (1157.661->1222.554)! Learning rate decreased to 0.00288.
2024-12-02-13:51:36-root-INFO: Loss too large (1157.661->1192.354)! Learning rate decreased to 0.00231.
2024-12-02-13:51:37-root-INFO: Loss too large (1157.661->1172.105)! Learning rate decreased to 0.00184.
2024-12-02-13:51:37-root-INFO: Loss too large (1157.661->1160.162)! Learning rate decreased to 0.00148.
2024-12-02-13:51:38-root-INFO: grad norm: 151.763 149.895 23.738
2024-12-02-13:51:38-root-INFO: Loss too large (1153.905->1154.915)! Learning rate decreased to 0.00118.
2024-12-02-13:51:39-root-INFO: grad norm: 126.129 124.599 19.586
2024-12-02-13:51:40-root-INFO: grad norm: 99.342 97.637 18.326
2024-12-02-13:51:41-root-INFO: grad norm: 88.360 86.855 16.240
2024-12-02-13:51:42-root-INFO: grad norm: 77.818 76.096 16.282
2024-12-02-13:51:43-root-INFO: grad norm: 72.018 70.452 14.939
2024-12-02-13:51:44-root-INFO: grad norm: 66.712 64.947 15.246
2024-12-02-13:51:45-root-INFO: Loss Change: 1157.661 -> 1127.738
2024-12-02-13:51:45-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-02-13:51:45-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-13:51:45-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-13:51:46-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-13:51:46-root-INFO: grad norm: 140.631 138.786 22.702
2024-12-02-13:51:46-root-INFO: Loss too large (1131.594->1374.319)! Learning rate decreased to 0.00732.
2024-12-02-13:51:46-root-INFO: Loss too large (1131.594->1345.551)! Learning rate decreased to 0.00585.
2024-12-02-13:51:47-root-INFO: Loss too large (1131.594->1309.799)! Learning rate decreased to 0.00468.
2024-12-02-13:51:47-root-INFO: Loss too large (1131.594->1267.697)! Learning rate decreased to 0.00375.
2024-12-02-13:51:48-root-INFO: Loss too large (1131.594->1223.582)! Learning rate decreased to 0.00300.
2024-12-02-13:51:48-root-INFO: Loss too large (1131.594->1184.673)! Learning rate decreased to 0.00240.
2024-12-02-13:51:48-root-INFO: Loss too large (1131.594->1156.090)! Learning rate decreased to 0.00192.
2024-12-02-13:51:48-root-INFO: Loss too large (1131.594->1138.238)! Learning rate decreased to 0.00153.
2024-12-02-13:51:50-root-INFO: grad norm: 184.979 183.003 26.972
2024-12-02-13:51:50-root-INFO: Loss too large (1128.529->1134.583)! Learning rate decreased to 0.00123.
2024-12-02-13:51:51-root-INFO: grad norm: 156.192 154.536 22.683
2024-12-02-13:51:52-root-INFO: grad norm: 119.363 117.726 19.700
2024-12-02-13:51:53-root-INFO: grad norm: 108.684 107.187 17.975
2024-12-02-13:51:54-root-INFO: grad norm: 96.266 94.705 17.270
2024-12-02-13:51:55-root-INFO: grad norm: 90.286 88.824 16.183
2024-12-02-13:51:56-root-INFO: grad norm: 83.744 82.205 15.979
2024-12-02-13:51:56-root-INFO: Loss Change: 1131.594 -> 1101.207
2024-12-02-13:51:56-root-INFO: Regularization Change: 0.000 -> 0.255
2024-12-02-13:51:56-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-13:51:56-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-13:51:57-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-13:51:57-root-INFO: grad norm: 137.420 135.867 20.599
2024-12-02-13:51:58-root-INFO: Loss too large (1102.641->1350.834)! Learning rate decreased to 0.00760.
2024-12-02-13:51:58-root-INFO: Loss too large (1102.641->1323.752)! Learning rate decreased to 0.00608.
2024-12-02-13:51:58-root-INFO: Loss too large (1102.641->1289.131)! Learning rate decreased to 0.00487.
2024-12-02-13:51:59-root-INFO: Loss too large (1102.641->1247.235)! Learning rate decreased to 0.00389.
2024-12-02-13:51:59-root-INFO: Loss too large (1102.641->1201.855)! Learning rate decreased to 0.00311.
2024-12-02-13:51:59-root-INFO: Loss too large (1102.641->1160.663)! Learning rate decreased to 0.00249.
2024-12-02-13:52:00-root-INFO: Loss too large (1102.641->1129.927)! Learning rate decreased to 0.00199.
2024-12-02-13:52:00-root-INFO: Loss too large (1102.641->1110.670)! Learning rate decreased to 0.00159.
2024-12-02-13:52:01-root-INFO: grad norm: 185.052 183.243 25.809
2024-12-02-13:52:01-root-INFO: Loss too large (1100.225->1107.824)! Learning rate decreased to 0.00128.
2024-12-02-13:52:02-root-INFO: grad norm: 155.270 153.784 21.430
2024-12-02-13:52:03-root-INFO: grad norm: 113.942 112.461 18.312
2024-12-02-13:52:04-root-INFO: grad norm: 104.173 102.806 16.822
2024-12-02-13:52:05-root-INFO: grad norm: 92.148 90.706 16.240
2024-12-02-13:52:06-root-INFO: grad norm: 86.601 85.236 15.316
2024-12-02-13:52:07-root-INFO: grad norm: 80.231 78.788 15.148
2024-12-02-13:52:08-root-INFO: Loss Change: 1102.641 -> 1074.429
2024-12-02-13:52:08-root-INFO: Regularization Change: 0.000 -> 0.240
2024-12-02-13:52:08-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-13:52:08-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:52:08-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-13:52:09-root-INFO: grad norm: 148.004 145.982 24.383
2024-12-02-13:52:09-root-INFO: Loss too large (1084.503->1346.029)! Learning rate decreased to 0.00790.
2024-12-02-13:52:09-root-INFO: Loss too large (1084.503->1318.579)! Learning rate decreased to 0.00632.
2024-12-02-13:52:10-root-INFO: Loss too large (1084.503->1284.545)! Learning rate decreased to 0.00506.
2024-12-02-13:52:10-root-INFO: Loss too large (1084.503->1242.631)! Learning rate decreased to 0.00404.
2024-12-02-13:52:10-root-INFO: Loss too large (1084.503->1195.048)! Learning rate decreased to 0.00324.
2024-12-02-13:52:11-root-INFO: Loss too large (1084.503->1149.573)! Learning rate decreased to 0.00259.
2024-12-02-13:52:11-root-INFO: Loss too large (1084.503->1114.393)! Learning rate decreased to 0.00207.
2024-12-02-13:52:11-root-INFO: Loss too large (1084.503->1092.073)! Learning rate decreased to 0.00166.
2024-12-02-13:52:12-root-INFO: grad norm: 196.462 194.623 26.817
2024-12-02-13:52:13-root-INFO: Loss too large (1080.080->1088.901)! Learning rate decreased to 0.00133.
2024-12-02-13:52:14-root-INFO: grad norm: 161.824 160.097 23.574
2024-12-02-13:52:15-root-INFO: grad norm: 111.122 109.684 17.815
2024-12-02-13:52:16-root-INFO: grad norm: 101.528 100.009 17.499
2024-12-02-13:52:17-root-INFO: grad norm: 89.426 88.061 15.564
2024-12-02-13:52:18-root-INFO: grad norm: 83.792 82.335 15.556
2024-12-02-13:52:19-root-INFO: grad norm: 77.175 75.835 14.316
2024-12-02-13:52:19-root-INFO: Loss Change: 1084.503 -> 1051.153
2024-12-02-13:52:19-root-INFO: Regularization Change: 0.000 -> 0.286
2024-12-02-13:52:19-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-13:52:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:52:20-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-13:52:20-root-INFO: grad norm: 131.368 129.852 19.900
2024-12-02-13:52:20-root-INFO: Loss too large (1055.294->1310.112)! Learning rate decreased to 0.00821.
2024-12-02-13:52:21-root-INFO: Loss too large (1055.294->1282.110)! Learning rate decreased to 0.00656.
2024-12-02-13:52:21-root-INFO: Loss too large (1055.294->1246.255)! Learning rate decreased to 0.00525.
2024-12-02-13:52:21-root-INFO: Loss too large (1055.294->1202.218)! Learning rate decreased to 0.00420.
2024-12-02-13:52:22-root-INFO: Loss too large (1055.294->1154.110)! Learning rate decreased to 0.00336.
2024-12-02-13:52:22-root-INFO: Loss too large (1055.294->1110.898)! Learning rate decreased to 0.00269.
2024-12-02-13:52:22-root-INFO: Loss too large (1055.294->1079.617)! Learning rate decreased to 0.00215.
2024-12-02-13:52:23-root-INFO: Loss too large (1055.294->1060.823)! Learning rate decreased to 0.00172.
2024-12-02-13:52:24-root-INFO: grad norm: 166.537 164.934 23.050
2024-12-02-13:52:24-root-INFO: Loss too large (1051.113->1056.714)! Learning rate decreased to 0.00138.
2024-12-02-13:52:25-root-INFO: grad norm: 135.959 134.578 19.330
2024-12-02-13:52:26-root-INFO: grad norm: 93.097 91.757 15.742
2024-12-02-13:52:27-root-INFO: grad norm: 83.407 82.081 14.814
2024-12-02-13:52:28-root-INFO: grad norm: 72.097 70.741 13.917
2024-12-02-13:52:29-root-INFO: grad norm: 66.645 65.270 13.468
2024-12-02-13:52:30-root-INFO: grad norm: 60.845 59.439 13.003
2024-12-02-13:52:31-root-INFO: Loss Change: 1055.294 -> 1025.983
2024-12-02-13:52:31-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-02-13:52:31-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-13:52:31-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:52:31-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-13:52:31-root-INFO: grad norm: 114.577 113.062 18.567
2024-12-02-13:52:32-root-INFO: Loss too large (1030.659->1275.936)! Learning rate decreased to 0.00852.
2024-12-02-13:52:32-root-INFO: Loss too large (1030.659->1245.070)! Learning rate decreased to 0.00682.
2024-12-02-13:52:32-root-INFO: Loss too large (1030.659->1205.900)! Learning rate decreased to 0.00545.
2024-12-02-13:52:33-root-INFO: Loss too large (1030.659->1159.513)! Learning rate decreased to 0.00436.
2024-12-02-13:52:33-root-INFO: Loss too large (1030.659->1112.718)! Learning rate decreased to 0.00349.
2024-12-02-13:52:34-root-INFO: Loss too large (1030.659->1074.497)! Learning rate decreased to 0.00279.
2024-12-02-13:52:34-root-INFO: Loss too large (1030.659->1048.962)! Learning rate decreased to 0.00223.
2024-12-02-13:52:34-root-INFO: Loss too large (1030.659->1034.376)! Learning rate decreased to 0.00179.
2024-12-02-13:52:35-root-INFO: grad norm: 149.035 147.572 20.831
2024-12-02-13:52:35-root-INFO: Loss too large (1027.036->1031.256)! Learning rate decreased to 0.00143.
2024-12-02-13:52:37-root-INFO: grad norm: 124.539 123.178 18.364
2024-12-02-13:52:38-root-INFO: grad norm: 89.352 88.079 15.029
2024-12-02-13:52:39-root-INFO: grad norm: 80.662 79.348 14.502
2024-12-02-13:52:40-root-INFO: grad norm: 70.190 68.913 13.329
2024-12-02-13:52:41-root-INFO: grad norm: 65.151 63.807 13.162
2024-12-02-13:52:42-root-INFO: grad norm: 59.613 58.299 12.445
2024-12-02-13:52:42-root-INFO: Loss Change: 1030.659 -> 1003.194
2024-12-02-13:52:42-root-INFO: Regularization Change: 0.000 -> 0.271
2024-12-02-13:52:42-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-13:52:42-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:52:43-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-13:52:43-root-INFO: grad norm: 105.160 103.808 16.809
2024-12-02-13:52:43-root-INFO: Loss too large (1005.731->1247.431)! Learning rate decreased to 0.00885.
2024-12-02-13:52:44-root-INFO: Loss too large (1005.731->1214.119)! Learning rate decreased to 0.00708.
2024-12-02-13:52:44-root-INFO: Loss too large (1005.731->1172.426)! Learning rate decreased to 0.00567.
2024-12-02-13:52:44-root-INFO: Loss too large (1005.731->1124.517)! Learning rate decreased to 0.00453.
2024-12-02-13:52:45-root-INFO: Loss too large (1005.731->1078.567)! Learning rate decreased to 0.00363.
2024-12-02-13:52:45-root-INFO: Loss too large (1005.731->1043.122)! Learning rate decreased to 0.00290.
2024-12-02-13:52:45-root-INFO: Loss too large (1005.731->1020.563)! Learning rate decreased to 0.00232.
2024-12-02-13:52:46-root-INFO: Loss too large (1005.731->1008.115)! Learning rate decreased to 0.00186.
2024-12-02-13:52:47-root-INFO: grad norm: 132.453 131.114 18.790
2024-12-02-13:52:47-root-INFO: Loss too large (1002.013->1004.702)! Learning rate decreased to 0.00149.
2024-12-02-13:52:48-root-INFO: grad norm: 109.798 108.550 16.504
2024-12-02-13:52:49-root-INFO: grad norm: 78.317 77.043 14.073
2024-12-02-13:52:50-root-INFO: grad norm: 70.017 68.711 13.456
2024-12-02-13:52:51-root-INFO: grad norm: 60.588 59.232 12.747
2024-12-02-13:52:52-root-INFO: grad norm: 55.956 54.552 12.453
2024-12-02-13:52:53-root-INFO: grad norm: 51.225 49.784 12.064
2024-12-02-13:52:54-root-INFO: Loss Change: 1005.731 -> 979.994
2024-12-02-13:52:54-root-INFO: Regularization Change: 0.000 -> 0.265
2024-12-02-13:52:54-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-13:52:54-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-13:52:54-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-13:52:54-root-INFO: grad norm: 105.385 103.823 18.082
2024-12-02-13:52:55-root-INFO: Loss too large (985.701->1232.834)! Learning rate decreased to 0.00919.
2024-12-02-13:52:55-root-INFO: Loss too large (985.701->1197.439)! Learning rate decreased to 0.00735.
2024-12-02-13:52:55-root-INFO: Loss too large (985.701->1154.320)! Learning rate decreased to 0.00588.
2024-12-02-13:52:56-root-INFO: Loss too large (985.701->1105.139)! Learning rate decreased to 0.00471.
2024-12-02-13:52:56-root-INFO: Loss too large (985.701->1058.081)! Learning rate decreased to 0.00376.
2024-12-02-13:52:56-root-INFO: Loss too large (985.701->1022.013)! Learning rate decreased to 0.00301.
2024-12-02-13:52:57-root-INFO: Loss too large (985.701->999.328)! Learning rate decreased to 0.00241.
2024-12-02-13:52:57-root-INFO: Loss too large (985.701->987.005)! Learning rate decreased to 0.00193.
2024-12-02-13:52:58-root-INFO: grad norm: 130.778 129.479 18.385
2024-12-02-13:52:58-root-INFO: Loss too large (981.091->983.580)! Learning rate decreased to 0.00154.
2024-12-02-13:53:00-root-INFO: grad norm: 107.771 106.450 16.826
2024-12-02-13:53:00-root-INFO: grad norm: 75.133 73.903 13.542
2024-12-02-13:53:02-root-INFO: grad norm: 66.744 65.403 13.311
2024-12-02-13:53:03-root-INFO: grad norm: 57.210 55.917 12.096
2024-12-02-13:53:04-root-INFO: grad norm: 52.456 51.054 12.050
2024-12-02-13:53:05-root-INFO: grad norm: 47.663 46.298 11.324
2024-12-02-13:53:05-root-INFO: Loss Change: 985.701 -> 959.354
2024-12-02-13:53:05-root-INFO: Regularization Change: 0.000 -> 0.280
2024-12-02-13:53:05-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-13:53:05-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:53:06-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-13:53:06-root-INFO: grad norm: 98.133 96.568 17.454
2024-12-02-13:53:06-root-INFO: Loss too large (964.398->1209.869)! Learning rate decreased to 0.00954.
2024-12-02-13:53:07-root-INFO: Loss too large (964.398->1174.743)! Learning rate decreased to 0.00763.
2024-12-02-13:53:07-root-INFO: Loss too large (964.398->1130.323)! Learning rate decreased to 0.00611.
2024-12-02-13:53:07-root-INFO: Loss too large (964.398->1079.609)! Learning rate decreased to 0.00489.
2024-12-02-13:53:08-root-INFO: Loss too large (964.398->1032.478)! Learning rate decreased to 0.00391.
2024-12-02-13:53:08-root-INFO: Loss too large (964.398->997.762)! Learning rate decreased to 0.00313.
2024-12-02-13:53:08-root-INFO: Loss too large (964.398->976.654)! Learning rate decreased to 0.00250.
2024-12-02-13:53:09-root-INFO: Loss too large (964.398->965.443)! Learning rate decreased to 0.00200.
2024-12-02-13:53:10-root-INFO: grad norm: 118.875 117.622 17.216
2024-12-02-13:53:10-root-INFO: Loss too large (960.140->961.760)! Learning rate decreased to 0.00160.
2024-12-02-13:53:11-root-INFO: grad norm: 97.064 95.735 16.010
2024-12-02-13:53:12-root-INFO: grad norm: 67.319 66.091 12.801
2024-12-02-13:53:13-root-INFO: grad norm: 59.151 57.767 12.722
2024-12-02-13:53:14-root-INFO: grad norm: 50.470 49.162 11.415
2024-12-02-13:53:15-root-INFO: grad norm: 46.120 44.668 11.482
2024-12-02-13:53:16-root-INFO: grad norm: 42.094 40.711 10.700
2024-12-02-13:53:17-root-INFO: Loss Change: 964.398 -> 940.214
2024-12-02-13:53:17-root-INFO: Regularization Change: 0.000 -> 0.268
2024-12-02-13:53:17-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-13:53:17-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:53:17-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-13:53:17-root-INFO: grad norm: 77.901 76.629 14.019
2024-12-02-13:53:18-root-INFO: Loss too large (942.292->1159.436)! Learning rate decreased to 0.00991.
2024-12-02-13:53:18-root-INFO: Loss too large (942.292->1116.824)! Learning rate decreased to 0.00792.
2024-12-02-13:53:18-root-INFO: Loss too large (942.292->1067.237)! Learning rate decreased to 0.00634.
2024-12-02-13:53:19-root-INFO: Loss too large (942.292->1019.718)! Learning rate decreased to 0.00507.
2024-12-02-13:53:19-root-INFO: Loss too large (942.292->983.527)! Learning rate decreased to 0.00406.
2024-12-02-13:53:19-root-INFO: Loss too large (942.292->960.768)! Learning rate decreased to 0.00325.
2024-12-02-13:53:20-root-INFO: Loss too large (942.292->948.173)! Learning rate decreased to 0.00260.
2024-12-02-13:53:21-root-INFO: grad norm: 137.458 136.276 17.986
2024-12-02-13:53:21-root-INFO: Loss too large (941.802->953.725)! Learning rate decreased to 0.00208.
2024-12-02-13:53:21-root-INFO: Loss too large (941.802->945.310)! Learning rate decreased to 0.00166.
2024-12-02-13:53:22-root-INFO: grad norm: 101.197 100.094 14.901
2024-12-02-13:53:23-root-INFO: grad norm: 53.961 52.746 11.385
2024-12-02-13:53:24-root-INFO: grad norm: 46.620 45.272 11.132
2024-12-02-13:53:25-root-INFO: grad norm: 40.356 38.959 10.528
2024-12-02-13:53:26-root-INFO: grad norm: 37.453 35.966 10.447
2024-12-02-13:53:27-root-INFO: grad norm: 35.332 33.844 10.148
2024-12-02-13:53:28-root-INFO: Loss Change: 942.292 -> 921.977
2024-12-02-13:53:28-root-INFO: Regularization Change: 0.000 -> 0.255
2024-12-02-13:53:28-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-13:53:28-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:53:28-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-13:53:29-root-INFO: grad norm: 79.455 78.099 14.619
2024-12-02-13:53:29-root-INFO: Loss too large (925.949->1150.970)! Learning rate decreased to 0.01028.
2024-12-02-13:53:29-root-INFO: Loss too large (925.949->1106.057)! Learning rate decreased to 0.00822.
2024-12-02-13:53:30-root-INFO: Loss too large (925.949->1054.340)! Learning rate decreased to 0.00658.
2024-12-02-13:53:30-root-INFO: Loss too large (925.949->1004.805)! Learning rate decreased to 0.00526.
2024-12-02-13:53:30-root-INFO: Loss too large (925.949->967.218)! Learning rate decreased to 0.00421.
2024-12-02-13:53:31-root-INFO: Loss too large (925.949->943.796)! Learning rate decreased to 0.00337.
2024-12-02-13:53:31-root-INFO: Loss too large (925.949->930.997)! Learning rate decreased to 0.00269.
2024-12-02-13:53:32-root-INFO: grad norm: 135.305 134.157 17.588
2024-12-02-13:53:32-root-INFO: Loss too large (924.632->936.196)! Learning rate decreased to 0.00216.
2024-12-02-13:53:33-root-INFO: Loss too large (924.632->927.972)! Learning rate decreased to 0.00172.
2024-12-02-13:53:34-root-INFO: grad norm: 98.066 96.937 14.836
2024-12-02-13:53:35-root-INFO: grad norm: 49.859 48.627 11.014
2024-12-02-13:53:36-root-INFO: grad norm: 43.135 41.731 10.914
2024-12-02-13:53:37-root-INFO: grad norm: 37.735 36.319 10.242
2024-12-02-13:53:38-root-INFO: grad norm: 35.256 33.741 10.222
2024-12-02-13:53:39-root-INFO: grad norm: 33.545 32.055 9.886
2024-12-02-13:53:40-root-INFO: Loss Change: 925.949 -> 904.958
2024-12-02-13:53:40-root-INFO: Regularization Change: 0.000 -> 0.269
2024-12-02-13:53:40-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-13:53:40-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-13:53:40-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-13:53:40-root-INFO: grad norm: 75.872 74.512 14.298
2024-12-02-13:53:41-root-INFO: Loss too large (908.962->1130.437)! Learning rate decreased to 0.01067.
2024-12-02-13:53:41-root-INFO: Loss too large (908.962->1083.519)! Learning rate decreased to 0.00853.
2024-12-02-13:53:41-root-INFO: Loss too large (908.962->1030.043)! Learning rate decreased to 0.00683.
2024-12-02-13:53:42-root-INFO: Loss too large (908.962->980.683)! Learning rate decreased to 0.00546.
2024-12-02-13:53:42-root-INFO: Loss too large (908.962->944.956)! Learning rate decreased to 0.00437.
2024-12-02-13:53:42-root-INFO: Loss too large (908.962->923.602)! Learning rate decreased to 0.00350.
2024-12-02-13:53:43-root-INFO: Loss too large (908.962->912.299)! Learning rate decreased to 0.00280.
2024-12-02-13:53:44-root-INFO: grad norm: 119.342 118.282 15.876
2024-12-02-13:53:44-root-INFO: Loss too large (906.837->914.516)! Learning rate decreased to 0.00224.
2024-12-02-13:53:44-root-INFO: Loss too large (906.837->908.153)! Learning rate decreased to 0.00179.
2024-12-02-13:53:45-root-INFO: grad norm: 84.057 82.931 13.711
2024-12-02-13:53:46-root-INFO: grad norm: 43.924 42.618 10.632
2024-12-02-13:53:47-root-INFO: grad norm: 38.051 36.562 10.541
2024-12-02-13:53:48-root-INFO: grad norm: 34.216 32.719 10.011
2024-12-02-13:53:50-root-INFO: grad norm: 32.588 31.030 9.957
2024-12-02-13:53:51-root-INFO: grad norm: 31.612 30.085 9.705
2024-12-02-13:53:51-root-INFO: Loss Change: 908.962 -> 888.438
2024-12-02-13:53:51-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-13:53:51-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-13:53:51-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-13:53:52-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-13:53:52-root-INFO: grad norm: 71.081 69.654 14.176
2024-12-02-13:53:52-root-INFO: Loss too large (891.472->1097.592)! Learning rate decreased to 0.01107.
2024-12-02-13:53:53-root-INFO: Loss too large (891.472->1044.360)! Learning rate decreased to 0.00885.
2024-12-02-13:53:53-root-INFO: Loss too large (891.472->989.332)! Learning rate decreased to 0.00708.
2024-12-02-13:53:53-root-INFO: Loss too large (891.472->944.626)! Learning rate decreased to 0.00567.
2024-12-02-13:53:54-root-INFO: Loss too large (891.472->915.696)! Learning rate decreased to 0.00453.
2024-12-02-13:53:54-root-INFO: Loss too large (891.472->899.648)! Learning rate decreased to 0.00363.
2024-12-02-13:53:54-root-INFO: Loss too large (891.472->891.568)! Learning rate decreased to 0.00290.
2024-12-02-13:53:55-root-INFO: grad norm: 95.020 94.017 13.772
2024-12-02-13:53:56-root-INFO: Loss too large (887.871->890.510)! Learning rate decreased to 0.00232.
2024-12-02-13:53:57-root-INFO: grad norm: 82.394 81.282 13.495
2024-12-02-13:53:58-root-INFO: grad norm: 60.455 59.432 11.073
2024-12-02-13:53:59-root-INFO: grad norm: 55.590 54.443 11.234
2024-12-02-13:54:00-root-INFO: grad norm: 48.857 47.799 10.108
2024-12-02-13:54:01-root-INFO: grad norm: 45.812 44.636 10.317
2024-12-02-13:54:02-root-INFO: grad norm: 42.062 40.968 9.531
2024-12-02-13:54:02-root-INFO: Loss Change: 891.472 -> 869.023
2024-12-02-13:54:02-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-13:54:02-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-13:54:02-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-13:54:03-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-13:54:03-root-INFO: grad norm: 68.744 67.627 12.343
2024-12-02-13:54:03-root-INFO: Loss too large (871.344->1086.432)! Learning rate decreased to 0.01148.
2024-12-02-13:54:04-root-INFO: Loss too large (871.344->1036.162)! Learning rate decreased to 0.00919.
2024-12-02-13:54:04-root-INFO: Loss too large (871.344->979.534)! Learning rate decreased to 0.00735.
2024-12-02-13:54:04-root-INFO: Loss too large (871.344->930.606)! Learning rate decreased to 0.00588.
2024-12-02-13:54:05-root-INFO: Loss too large (871.344->898.307)! Learning rate decreased to 0.00470.
2024-12-02-13:54:05-root-INFO: Loss too large (871.344->880.584)! Learning rate decreased to 0.00376.
2024-12-02-13:54:05-root-INFO: Loss too large (871.344->871.847)! Learning rate decreased to 0.00301.
2024-12-02-13:54:06-root-INFO: grad norm: 83.832 82.941 12.190
2024-12-02-13:54:07-root-INFO: Loss too large (867.926->869.069)! Learning rate decreased to 0.00241.
2024-12-02-13:54:08-root-INFO: grad norm: 68.930 67.930 11.696
2024-12-02-13:54:09-root-INFO: grad norm: 47.380 46.358 9.788
2024-12-02-13:54:10-root-INFO: grad norm: 42.284 41.124 9.835
2024-12-02-13:54:11-root-INFO: grad norm: 36.832 35.683 9.125
2024-12-02-13:54:12-root-INFO: grad norm: 34.243 32.984 9.203
2024-12-02-13:54:13-root-INFO: grad norm: 31.848 30.617 8.770
2024-12-02-13:54:13-root-INFO: Loss Change: 871.344 -> 850.923
2024-12-02-13:54:13-root-INFO: Regularization Change: 0.000 -> 0.349
2024-12-02-13:54:13-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-13:54:13-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-13:54:14-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-13:54:14-root-INFO: grad norm: 66.270 65.013 12.846
2024-12-02-13:54:14-root-INFO: Loss too large (855.035->1053.988)! Learning rate decreased to 0.01191.
2024-12-02-13:54:15-root-INFO: Loss too large (855.035->995.561)! Learning rate decreased to 0.00953.
2024-12-02-13:54:15-root-INFO: Loss too large (855.035->938.730)! Learning rate decreased to 0.00762.
2024-12-02-13:54:15-root-INFO: Loss too large (855.035->896.669)! Learning rate decreased to 0.00610.
2024-12-02-13:54:16-root-INFO: Loss too large (855.035->871.826)! Learning rate decreased to 0.00488.
2024-12-02-13:54:16-root-INFO: Loss too large (855.035->858.988)! Learning rate decreased to 0.00390.
2024-12-02-13:54:17-root-INFO: grad norm: 110.153 109.256 14.027
2024-12-02-13:54:17-root-INFO: Loss too large (852.907->863.711)! Learning rate decreased to 0.00312.
2024-12-02-13:54:18-root-INFO: Loss too large (852.907->856.192)! Learning rate decreased to 0.00250.
2024-12-02-13:54:19-root-INFO: grad norm: 78.453 77.470 12.377
2024-12-02-13:54:20-root-INFO: grad norm: 36.506 35.350 9.111
2024-12-02-13:54:21-root-INFO: grad norm: 32.537 31.227 9.140
2024-12-02-13:54:22-root-INFO: grad norm: 29.677 28.396 8.623
2024-12-02-13:54:23-root-INFO: grad norm: 28.348 27.005 8.624
2024-12-02-13:54:24-root-INFO: grad norm: 27.468 26.167 8.353
2024-12-02-13:54:24-root-INFO: Loss Change: 855.035 -> 833.580
2024-12-02-13:54:24-root-INFO: Regularization Change: 0.000 -> 0.397
2024-12-02-13:54:24-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-13:54:25-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:54:25-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-13:54:25-root-INFO: grad norm: 43.793 42.615 10.090
2024-12-02-13:54:26-root-INFO: Loss too large (834.867->915.245)! Learning rate decreased to 0.01235.
2024-12-02-13:54:26-root-INFO: Loss too large (834.867->877.276)! Learning rate decreased to 0.00988.
2024-12-02-13:54:26-root-INFO: Loss too large (834.867->854.556)! Learning rate decreased to 0.00790.
2024-12-02-13:54:27-root-INFO: Loss too large (834.867->842.362)! Learning rate decreased to 0.00632.
2024-12-02-13:54:27-root-INFO: Loss too large (834.867->836.202)! Learning rate decreased to 0.00506.
2024-12-02-13:54:28-root-INFO: grad norm: 85.147 84.375 11.440
2024-12-02-13:54:28-root-INFO: Loss too large (833.281->844.672)! Learning rate decreased to 0.00405.
2024-12-02-13:54:29-root-INFO: Loss too large (833.281->837.832)! Learning rate decreased to 0.00324.
2024-12-02-13:54:29-root-INFO: Loss too large (833.281->833.390)! Learning rate decreased to 0.00259.
2024-12-02-13:54:30-root-INFO: grad norm: 58.660 57.745 10.323
2024-12-02-13:54:31-root-INFO: grad norm: 31.750 30.646 8.302
2024-12-02-13:54:32-root-INFO: grad norm: 28.410 27.160 8.333
2024-12-02-13:54:33-root-INFO: grad norm: 26.644 25.410 8.013
2024-12-02-13:54:34-root-INFO: grad norm: 25.939 24.675 7.998
2024-12-02-13:54:35-root-INFO: grad norm: 25.529 24.293 7.848
2024-12-02-13:54:36-root-INFO: Loss Change: 834.867 -> 817.096
2024-12-02-13:54:36-root-INFO: Regularization Change: 0.000 -> 0.391
2024-12-02-13:54:36-root-INFO: Undo step: 150
2024-12-02-13:54:36-root-INFO: Undo step: 151
2024-12-02-13:54:36-root-INFO: Undo step: 152
2024-12-02-13:54:36-root-INFO: Undo step: 153
2024-12-02-13:54:36-root-INFO: Undo step: 154
2024-12-02-13:54:36-root-INFO: Undo step: 155
2024-12-02-13:54:36-root-INFO: Undo step: 156
2024-12-02-13:54:36-root-INFO: Undo step: 157
2024-12-02-13:54:36-root-INFO: Undo step: 158
2024-12-02-13:54:36-root-INFO: Undo step: 159
2024-12-02-13:54:36-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-13:54:36-root-INFO: grad norm: 438.309 431.418 77.415
2024-12-02-13:54:37-root-INFO: grad norm: 386.381 379.814 70.935
2024-12-02-13:54:38-root-INFO: grad norm: 160.154 156.041 36.060
2024-12-02-13:54:39-root-INFO: grad norm: 276.132 272.170 46.608
2024-12-02-13:54:40-root-INFO: Loss too large (1336.334->1563.132)! Learning rate decreased to 0.00852.
2024-12-02-13:54:40-root-INFO: Loss too large (1336.334->1391.481)! Learning rate decreased to 0.00682.
2024-12-02-13:54:41-root-INFO: grad norm: 282.903 281.431 28.826
2024-12-02-13:54:42-root-INFO: grad norm: 242.826 240.411 34.165
2024-12-02-13:54:43-root-INFO: grad norm: 233.915 232.487 25.809
2024-12-02-13:54:43-root-INFO: Loss too large (1174.640->1183.573)! Learning rate decreased to 0.00545.
2024-12-02-13:54:44-root-INFO: grad norm: 304.212 300.529 47.194
2024-12-02-13:54:45-root-INFO: Loss too large (1102.126->1230.918)! Learning rate decreased to 0.00436.
2024-12-02-13:54:45-root-INFO: Loss too large (1102.126->1157.292)! Learning rate decreased to 0.00349.
2024-12-02-13:54:45-root-INFO: Loss too large (1102.126->1105.017)! Learning rate decreased to 0.00279.
2024-12-02-13:54:46-root-INFO: Loss Change: 2213.796 -> 1068.933
2024-12-02-13:54:46-root-INFO: Regularization Change: 0.000 -> 39.585
2024-12-02-13:54:46-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-13:54:46-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-13:54:46-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-13:54:47-root-INFO: grad norm: 178.460 177.153 21.558
2024-12-02-13:54:47-root-INFO: Loss too large (1073.167->1269.079)! Learning rate decreased to 0.00885.
2024-12-02-13:54:47-root-INFO: Loss too large (1073.167->1222.369)! Learning rate decreased to 0.00708.
2024-12-02-13:54:48-root-INFO: Loss too large (1073.167->1160.185)! Learning rate decreased to 0.00567.
2024-12-02-13:54:48-root-INFO: Loss too large (1073.167->1090.269)! Learning rate decreased to 0.00453.
2024-12-02-13:54:49-root-INFO: grad norm: 236.137 233.177 37.269
2024-12-02-13:54:49-root-INFO: Loss too large (1037.101->1099.833)! Learning rate decreased to 0.00363.
2024-12-02-13:54:50-root-INFO: Loss too large (1037.101->1063.076)! Learning rate decreased to 0.00290.
2024-12-02-13:54:50-root-INFO: Loss too large (1037.101->1037.543)! Learning rate decreased to 0.00232.
2024-12-02-13:54:51-root-INFO: grad norm: 129.431 128.192 17.866
2024-12-02-13:54:52-root-INFO: grad norm: 50.552 48.321 14.852
2024-12-02-13:54:53-root-INFO: grad norm: 48.232 46.108 14.156
2024-12-02-13:54:54-root-INFO: grad norm: 46.636 44.528 13.861
2024-12-02-13:54:55-root-INFO: grad norm: 45.359 43.329 13.417
2024-12-02-13:54:56-root-INFO: grad norm: 44.268 42.277 13.127
2024-12-02-13:54:57-root-INFO: Loss Change: 1073.167 -> 971.129
2024-12-02-13:54:57-root-INFO: Regularization Change: 0.000 -> 1.256
2024-12-02-13:54:57-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-13:54:57-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-13:54:57-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-13:54:57-root-INFO: grad norm: 61.576 60.080 13.491
2024-12-02-13:54:58-root-INFO: Loss too large (969.073->1015.961)! Learning rate decreased to 0.00919.
2024-12-02-13:54:58-root-INFO: Loss too large (969.073->989.482)! Learning rate decreased to 0.00735.
2024-12-02-13:54:58-root-INFO: Loss too large (969.073->974.551)! Learning rate decreased to 0.00588.
2024-12-02-13:54:59-root-INFO: grad norm: 159.230 157.452 23.725
2024-12-02-13:55:00-root-INFO: Loss too large (966.985->1044.886)! Learning rate decreased to 0.00471.
2024-12-02-13:55:00-root-INFO: Loss too large (966.985->1013.376)! Learning rate decreased to 0.00376.
2024-12-02-13:55:00-root-INFO: Loss too large (966.985->990.872)! Learning rate decreased to 0.00301.
2024-12-02-13:55:01-root-INFO: Loss too large (966.985->975.286)! Learning rate decreased to 0.00241.
2024-12-02-13:55:02-root-INFO: grad norm: 115.359 114.258 15.898
2024-12-02-13:55:03-root-INFO: grad norm: 55.283 53.863 12.450
2024-12-02-13:55:04-root-INFO: grad norm: 50.466 49.204 11.215
2024-12-02-13:55:05-root-INFO: grad norm: 46.218 44.799 11.367
2024-12-02-13:55:06-root-INFO: grad norm: 44.033 42.729 10.636
2024-12-02-13:55:07-root-INFO: grad norm: 42.127 40.727 10.768
2024-12-02-13:55:07-root-INFO: Loss Change: 969.073 -> 930.406
2024-12-02-13:55:07-root-INFO: Regularization Change: 0.000 -> 0.846
2024-12-02-13:55:07-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-13:55:07-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:55:08-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-13:55:08-root-INFO: grad norm: 72.194 70.859 13.821
2024-12-02-13:55:08-root-INFO: Loss too large (930.203->1076.081)! Learning rate decreased to 0.00954.
2024-12-02-13:55:09-root-INFO: Loss too large (930.203->1026.469)! Learning rate decreased to 0.00763.
2024-12-02-13:55:09-root-INFO: Loss too large (930.203->983.232)! Learning rate decreased to 0.00611.
2024-12-02-13:55:09-root-INFO: Loss too large (930.203->953.990)! Learning rate decreased to 0.00489.
2024-12-02-13:55:10-root-INFO: Loss too large (930.203->937.558)! Learning rate decreased to 0.00391.
2024-12-02-13:55:11-root-INFO: grad norm: 141.265 139.831 20.073
2024-12-02-13:55:11-root-INFO: Loss too large (929.322->952.353)! Learning rate decreased to 0.00313.
2024-12-02-13:55:11-root-INFO: Loss too large (929.322->938.676)! Learning rate decreased to 0.00250.
2024-12-02-13:55:12-root-INFO: Loss too large (929.322->929.595)! Learning rate decreased to 0.00200.
2024-12-02-13:55:13-root-INFO: grad norm: 87.386 86.324 13.584
2024-12-02-13:55:14-root-INFO: grad norm: 40.286 38.997 10.111
2024-12-02-13:55:15-root-INFO: grad norm: 36.974 35.675 9.714
2024-12-02-13:55:16-root-INFO: grad norm: 35.487 34.185 9.526
2024-12-02-13:55:17-root-INFO: grad norm: 34.776 33.493 9.359
2024-12-02-13:55:18-root-INFO: grad norm: 34.281 33.014 9.236
2024-12-02-13:55:19-root-INFO: Loss Change: 930.203 -> 903.058
2024-12-02-13:55:19-root-INFO: Regularization Change: 0.000 -> 0.439
2024-12-02-13:55:19-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-13:55:19-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:55:19-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-13:55:19-root-INFO: grad norm: 50.804 49.710 10.488
2024-12-02-13:55:20-root-INFO: Loss too large (901.427->960.995)! Learning rate decreased to 0.00991.
2024-12-02-13:55:20-root-INFO: Loss too large (901.427->931.464)! Learning rate decreased to 0.00792.
2024-12-02-13:55:20-root-INFO: Loss too large (901.427->914.168)! Learning rate decreased to 0.00634.
2024-12-02-13:55:21-root-INFO: Loss too large (901.427->904.951)! Learning rate decreased to 0.00507.
2024-12-02-13:55:22-root-INFO: grad norm: 119.556 118.316 17.175
2024-12-02-13:55:22-root-INFO: Loss too large (900.370->933.214)! Learning rate decreased to 0.00406.
2024-12-02-13:55:22-root-INFO: Loss too large (900.370->917.813)! Learning rate decreased to 0.00325.
2024-12-02-13:55:23-root-INFO: Loss too large (900.370->907.221)! Learning rate decreased to 0.00260.
2024-12-02-13:55:24-root-INFO: grad norm: 97.071 96.184 13.088
2024-12-02-13:55:25-root-INFO: grad norm: 59.315 58.290 10.976
2024-12-02-13:55:26-root-INFO: grad norm: 54.901 54.012 9.840
2024-12-02-13:55:27-root-INFO: grad norm: 49.480 48.468 9.955
2024-12-02-13:55:28-root-INFO: grad norm: 47.144 46.238 9.195
2024-12-02-13:55:29-root-INFO: grad norm: 44.503 43.499 9.397
2024-12-02-13:55:29-root-INFO: Loss Change: 901.427 -> 875.952
2024-12-02-13:55:29-root-INFO: Regularization Change: 0.000 -> 0.582
2024-12-02-13:55:29-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-13:55:29-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-13:55:30-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-13:55:30-root-INFO: grad norm: 75.273 74.193 12.707
2024-12-02-13:55:30-root-INFO: Loss too large (878.223->1068.158)! Learning rate decreased to 0.01028.
2024-12-02-13:55:31-root-INFO: Loss too large (878.223->1015.827)! Learning rate decreased to 0.00822.
2024-12-02-13:55:31-root-INFO: Loss too large (878.223->962.031)! Learning rate decreased to 0.00658.
2024-12-02-13:55:31-root-INFO: Loss too large (878.223->919.684)! Learning rate decreased to 0.00526.
2024-12-02-13:55:32-root-INFO: Loss too large (878.223->894.094)! Learning rate decreased to 0.00421.
2024-12-02-13:55:32-root-INFO: Loss too large (878.223->880.928)! Learning rate decreased to 0.00337.
2024-12-02-13:55:33-root-INFO: grad norm: 104.230 103.168 14.839
2024-12-02-13:55:33-root-INFO: Loss too large (874.816->880.178)! Learning rate decreased to 0.00269.
2024-12-02-13:55:34-root-INFO: grad norm: 88.605 87.732 12.406
2024-12-02-13:55:35-root-INFO: grad norm: 60.629 59.722 10.448
2024-12-02-13:55:36-root-INFO: grad norm: 56.448 55.616 9.661
2024-12-02-13:55:37-root-INFO: grad norm: 50.717 49.833 9.427
2024-12-02-13:55:38-root-INFO: grad norm: 48.381 47.553 8.912
2024-12-02-13:55:39-root-INFO: grad norm: 45.471 44.598 8.868
2024-12-02-13:55:40-root-INFO: Loss Change: 878.223 -> 853.572
2024-12-02-13:55:40-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-02-13:55:40-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-13:55:40-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-13:55:41-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-13:55:41-root-INFO: grad norm: 74.001 72.972 12.294
2024-12-02-13:55:41-root-INFO: Loss too large (856.020->1051.936)! Learning rate decreased to 0.01067.
2024-12-02-13:55:42-root-INFO: Loss too large (856.020->998.479)! Learning rate decreased to 0.00853.
2024-12-02-13:55:42-root-INFO: Loss too large (856.020->942.290)! Learning rate decreased to 0.00683.
2024-12-02-13:55:42-root-INFO: Loss too large (856.020->897.902)! Learning rate decreased to 0.00546.
2024-12-02-13:55:43-root-INFO: Loss too large (856.020->871.517)! Learning rate decreased to 0.00437.
2024-12-02-13:55:43-root-INFO: Loss too large (856.020->858.245)! Learning rate decreased to 0.00350.
2024-12-02-13:55:44-root-INFO: grad norm: 96.919 95.944 13.711
2024-12-02-13:55:44-root-INFO: Loss too large (852.222->856.328)! Learning rate decreased to 0.00280.
2024-12-02-13:55:45-root-INFO: grad norm: 80.460 79.643 11.438
2024-12-02-13:55:46-root-INFO: grad norm: 52.771 51.907 9.509
2024-12-02-13:55:47-root-INFO: grad norm: 48.211 47.397 8.825
2024-12-02-13:55:48-root-INFO: grad norm: 42.512 41.646 8.535
2024-12-02-13:55:49-root-INFO: grad norm: 39.968 39.135 8.116
2024-12-02-13:55:50-root-INFO: grad norm: 37.098 36.222 8.014
2024-12-02-13:55:51-root-INFO: Loss Change: 856.020 -> 832.080
2024-12-02-13:55:51-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-13:55:51-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-13:55:51-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-13:55:51-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-13:55:52-root-INFO: grad norm: 65.456 64.379 11.827
2024-12-02-13:55:52-root-INFO: Loss too large (833.565->1000.591)! Learning rate decreased to 0.01107.
2024-12-02-13:55:52-root-INFO: Loss too large (833.565->942.629)! Learning rate decreased to 0.00885.
2024-12-02-13:55:53-root-INFO: Loss too large (833.565->892.058)! Learning rate decreased to 0.00708.
2024-12-02-13:55:53-root-INFO: Loss too large (833.565->859.163)! Learning rate decreased to 0.00567.
2024-12-02-13:55:53-root-INFO: Loss too large (833.565->841.576)! Learning rate decreased to 0.00453.
2024-12-02-13:55:54-root-INFO: grad norm: 124.312 123.223 16.414
2024-12-02-13:55:55-root-INFO: Loss too large (833.073->852.968)! Learning rate decreased to 0.00363.
2024-12-02-13:55:55-root-INFO: Loss too large (833.073->841.310)! Learning rate decreased to 0.00290.
2024-12-02-13:55:55-root-INFO: Loss too large (833.073->833.544)! Learning rate decreased to 0.00232.
2024-12-02-13:55:56-root-INFO: grad norm: 71.439 70.670 10.458
2024-12-02-13:55:57-root-INFO: grad norm: 28.652 27.618 7.630
2024-12-02-13:55:58-root-INFO: grad norm: 27.511 26.497 7.400
2024-12-02-13:55:59-root-INFO: grad norm: 26.986 25.991 7.259
2024-12-02-13:56:00-root-INFO: grad norm: 26.647 25.671 7.145
2024-12-02-13:56:01-root-INFO: grad norm: 26.372 25.412 7.052
2024-12-02-13:56:02-root-INFO: Loss Change: 833.565 -> 813.309
2024-12-02-13:56:02-root-INFO: Regularization Change: 0.000 -> 0.359
2024-12-02-13:56:02-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-13:56:02-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-13:56:02-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-13:56:03-root-INFO: grad norm: 41.234 40.329 8.595
2024-12-02-13:56:03-root-INFO: Loss too large (813.262->855.832)! Learning rate decreased to 0.01148.
2024-12-02-13:56:03-root-INFO: Loss too large (813.262->833.035)! Learning rate decreased to 0.00919.
2024-12-02-13:56:04-root-INFO: Loss too large (813.262->820.818)! Learning rate decreased to 0.00735.
2024-12-02-13:56:04-root-INFO: Loss too large (813.262->814.627)! Learning rate decreased to 0.00588.
2024-12-02-13:56:05-root-INFO: grad norm: 85.024 84.144 12.202
2024-12-02-13:56:05-root-INFO: Loss too large (811.672->828.120)! Learning rate decreased to 0.00470.
2024-12-02-13:56:06-root-INFO: Loss too large (811.672->819.301)! Learning rate decreased to 0.00376.
2024-12-02-13:56:06-root-INFO: Loss too large (811.672->813.495)! Learning rate decreased to 0.00301.
2024-12-02-13:56:07-root-INFO: grad norm: 64.688 64.015 9.307
2024-12-02-13:56:08-root-INFO: grad norm: 37.795 36.996 7.732
2024-12-02-13:56:09-root-INFO: grad norm: 33.285 32.506 7.160
2024-12-02-13:56:10-root-INFO: grad norm: 29.380 28.530 7.016
2024-12-02-13:56:11-root-INFO: grad norm: 27.612 26.770 6.765
2024-12-02-13:56:12-root-INFO: grad norm: 26.264 25.392 6.713
2024-12-02-13:56:13-root-INFO: Loss Change: 813.262 -> 793.960
2024-12-02-13:56:13-root-INFO: Regularization Change: 0.000 -> 0.496
2024-12-02-13:56:13-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-13:56:13-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-13:56:13-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-13:56:13-root-INFO: grad norm: 49.023 48.063 9.652
2024-12-02-13:56:14-root-INFO: Loss too large (795.392->882.969)! Learning rate decreased to 0.01191.
2024-12-02-13:56:14-root-INFO: Loss too large (795.392->840.162)! Learning rate decreased to 0.00953.
2024-12-02-13:56:14-root-INFO: Loss too large (795.392->815.005)! Learning rate decreased to 0.00762.
2024-12-02-13:56:15-root-INFO: Loss too large (795.392->801.923)! Learning rate decreased to 0.00610.
2024-12-02-13:56:15-root-INFO: Loss too large (795.392->795.521)! Learning rate decreased to 0.00488.
2024-12-02-13:56:16-root-INFO: grad norm: 72.548 71.752 10.713
2024-12-02-13:56:16-root-INFO: Loss too large (792.608->796.762)! Learning rate decreased to 0.00390.
2024-12-02-13:56:17-root-INFO: Loss too large (792.608->792.695)! Learning rate decreased to 0.00312.
2024-12-02-13:56:18-root-INFO: grad norm: 54.492 53.791 8.711
2024-12-02-13:56:19-root-INFO: grad norm: 33.600 32.807 7.259
2024-12-02-13:56:20-root-INFO: grad norm: 29.511 28.696 6.885
2024-12-02-13:56:21-root-INFO: grad norm: 26.530 25.686 6.637
2024-12-02-13:56:22-root-INFO: grad norm: 25.228 24.381 6.485
2024-12-02-13:56:23-root-INFO: grad norm: 24.379 23.530 6.377
2024-12-02-13:56:24-root-INFO: Loss Change: 795.392 -> 776.084
2024-12-02-13:56:24-root-INFO: Regularization Change: 0.000 -> 0.469
2024-12-02-13:56:24-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-13:56:24-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:56:24-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-13:56:24-root-INFO: grad norm: 36.333 35.541 7.544
2024-12-02-13:56:25-root-INFO: Loss too large (777.116->803.405)! Learning rate decreased to 0.01235.
2024-12-02-13:56:25-root-INFO: Loss too large (777.116->788.153)! Learning rate decreased to 0.00988.
2024-12-02-13:56:25-root-INFO: Loss too large (777.116->780.332)! Learning rate decreased to 0.00790.
2024-12-02-13:56:26-root-INFO: grad norm: 91.026 90.222 12.075
2024-12-02-13:56:27-root-INFO: Loss too large (776.502->809.383)! Learning rate decreased to 0.00632.
2024-12-02-13:56:27-root-INFO: Loss too large (776.502->794.805)! Learning rate decreased to 0.00506.
2024-12-02-13:56:27-root-INFO: Loss too large (776.502->784.848)! Learning rate decreased to 0.00405.
2024-12-02-13:56:28-root-INFO: Loss too large (776.502->778.273)! Learning rate decreased to 0.00324.
2024-12-02-13:56:29-root-INFO: grad norm: 60.902 60.295 8.576
2024-12-02-13:56:30-root-INFO: grad norm: 27.657 26.879 6.516
2024-12-02-13:56:31-root-INFO: grad norm: 24.946 24.148 6.257
2024-12-02-13:56:32-root-INFO: grad norm: 23.498 22.680 6.146
2024-12-02-13:56:33-root-INFO: grad norm: 22.901 22.084 6.062
2024-12-02-13:56:34-root-INFO: grad norm: 22.551 21.736 6.007
2024-12-02-13:56:34-root-INFO: Loss Change: 777.116 -> 759.206
2024-12-02-13:56:34-root-INFO: Regularization Change: 0.000 -> 0.524
2024-12-02-13:56:34-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-13:56:34-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:56:35-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-13:56:35-root-INFO: grad norm: 38.841 37.995 8.064
2024-12-02-13:56:35-root-INFO: Loss too large (759.596->799.876)! Learning rate decreased to 0.01281.
2024-12-02-13:56:36-root-INFO: Loss too large (759.596->777.594)! Learning rate decreased to 0.01024.
2024-12-02-13:56:36-root-INFO: Loss too large (759.596->766.054)! Learning rate decreased to 0.00820.
2024-12-02-13:56:36-root-INFO: Loss too large (759.596->760.336)! Learning rate decreased to 0.00656.
2024-12-02-13:56:37-root-INFO: grad norm: 69.047 68.280 10.267
2024-12-02-13:56:38-root-INFO: Loss too large (757.665->766.617)! Learning rate decreased to 0.00525.
2024-12-02-13:56:38-root-INFO: Loss too large (757.665->760.769)! Learning rate decreased to 0.00420.
2024-12-02-13:56:39-root-INFO: grad norm: 59.514 58.923 8.370
2024-12-02-13:56:40-root-INFO: grad norm: 42.678 42.042 7.342
2024-12-02-13:56:41-root-INFO: grad norm: 39.751 39.152 6.878
2024-12-02-13:56:42-root-INFO: grad norm: 35.590 34.960 6.666
2024-12-02-13:56:43-root-INFO: grad norm: 33.911 33.297 6.422
2024-12-02-13:56:44-root-INFO: grad norm: 31.744 31.112 6.302
2024-12-02-13:56:45-root-INFO: Loss Change: 759.596 -> 740.701
2024-12-02-13:56:45-root-INFO: Regularization Change: 0.000 -> 0.656
2024-12-02-13:56:45-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-13:56:45-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:56:45-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-13:56:46-root-INFO: grad norm: 49.786 48.986 8.892
2024-12-02-13:56:46-root-INFO: Loss too large (743.431->854.449)! Learning rate decreased to 0.01328.
2024-12-02-13:56:46-root-INFO: Loss too large (743.431->800.357)! Learning rate decreased to 0.01062.
2024-12-02-13:56:47-root-INFO: Loss too large (743.431->767.451)! Learning rate decreased to 0.00850.
2024-12-02-13:56:47-root-INFO: Loss too large (743.431->750.890)! Learning rate decreased to 0.00680.
2024-12-02-13:56:48-root-INFO: grad norm: 95.842 94.994 12.719
2024-12-02-13:56:48-root-INFO: Loss too large (743.165->761.225)! Learning rate decreased to 0.00544.
2024-12-02-13:56:49-root-INFO: Loss too large (743.165->750.711)! Learning rate decreased to 0.00435.
2024-12-02-13:56:49-root-INFO: Loss too large (743.165->743.817)! Learning rate decreased to 0.00348.
2024-12-02-13:56:50-root-INFO: grad norm: 54.906 54.315 8.033
2024-12-02-13:56:51-root-INFO: grad norm: 21.840 21.011 5.961
2024-12-02-13:56:52-root-INFO: grad norm: 21.271 20.458 5.825
2024-12-02-13:56:53-root-INFO: grad norm: 21.001 20.199 5.745
2024-12-02-13:56:54-root-INFO: grad norm: 20.804 20.012 5.688
2024-12-02-13:56:55-root-INFO: grad norm: 20.638 19.854 5.636
2024-12-02-13:56:56-root-INFO: Loss Change: 743.431 -> 725.574
2024-12-02-13:56:56-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-02-13:56:56-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-13:56:56-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-13:56:56-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-13:56:56-root-INFO: grad norm: 31.115 30.310 7.035
2024-12-02-13:56:57-root-INFO: Loss too large (725.608->734.775)! Learning rate decreased to 0.01376.
2024-12-02-13:56:57-root-INFO: Loss too large (725.608->727.753)! Learning rate decreased to 0.01101.
2024-12-02-13:56:58-root-INFO: grad norm: 76.357 75.594 10.768
2024-12-02-13:56:58-root-INFO: Loss too large (724.361->759.895)! Learning rate decreased to 0.00881.
2024-12-02-13:56:59-root-INFO: Loss too large (724.361->744.167)! Learning rate decreased to 0.00705.
2024-12-02-13:56:59-root-INFO: Loss too large (724.361->733.664)! Learning rate decreased to 0.00564.
2024-12-02-13:56:59-root-INFO: Loss too large (724.361->726.828)! Learning rate decreased to 0.00451.
2024-12-02-13:57:00-root-INFO: grad norm: 54.358 53.807 7.725
2024-12-02-13:57:01-root-INFO: grad norm: 26.579 25.904 5.953
2024-12-02-13:57:02-root-INFO: grad norm: 23.632 22.923 5.742
2024-12-02-13:57:03-root-INFO: grad norm: 21.526 20.798 5.551
2024-12-02-13:57:04-root-INFO: grad norm: 20.604 19.859 5.493
2024-12-02-13:57:05-root-INFO: grad norm: 19.986 19.244 5.395
2024-12-02-13:57:06-root-INFO: Loss Change: 725.608 -> 706.713
2024-12-02-13:57:06-root-INFO: Regularization Change: 0.000 -> 0.772
2024-12-02-13:57:06-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-13:57:06-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:57:06-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-13:57:07-root-INFO: grad norm: 31.711 31.064 6.374
2024-12-02-13:57:07-root-INFO: Loss too large (707.067->723.179)! Learning rate decreased to 0.01426.
2024-12-02-13:57:07-root-INFO: Loss too large (707.067->712.808)! Learning rate decreased to 0.01141.
2024-12-02-13:57:08-root-INFO: Loss too large (707.067->707.674)! Learning rate decreased to 0.00913.
2024-12-02-13:57:09-root-INFO: grad norm: 56.939 56.226 8.986
2024-12-02-13:57:09-root-INFO: Loss too large (705.266->714.108)! Learning rate decreased to 0.00730.
2024-12-02-13:57:09-root-INFO: Loss too large (705.266->708.296)! Learning rate decreased to 0.00584.
2024-12-02-13:57:10-root-INFO: grad norm: 50.686 50.190 7.071
2024-12-02-13:57:11-root-INFO: grad norm: 39.793 39.234 6.651
2024-12-02-13:57:12-root-INFO: grad norm: 37.624 37.117 6.153
2024-12-02-13:57:13-root-INFO: grad norm: 34.225 33.687 6.044
2024-12-02-13:57:14-root-INFO: grad norm: 33.029 32.509 5.833
2024-12-02-13:57:15-root-INFO: grad norm: 31.321 30.792 5.729
2024-12-02-13:57:16-root-INFO: Loss Change: 707.067 -> 688.623
2024-12-02-13:57:16-root-INFO: Regularization Change: 0.000 -> 0.924
2024-12-02-13:57:16-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-13:57:16-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:57:16-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-13:57:17-root-INFO: grad norm: 46.451 45.771 7.917
2024-12-02-13:57:17-root-INFO: Loss too large (691.347->772.012)! Learning rate decreased to 0.01478.
2024-12-02-13:57:17-root-INFO: Loss too large (691.347->726.952)! Learning rate decreased to 0.01182.
2024-12-02-13:57:18-root-INFO: Loss too large (691.347->703.761)! Learning rate decreased to 0.00946.
2024-12-02-13:57:18-root-INFO: Loss too large (691.347->692.891)! Learning rate decreased to 0.00757.
2024-12-02-13:57:19-root-INFO: grad norm: 58.786 58.101 8.947
2024-12-02-13:57:19-root-INFO: Loss too large (688.072->690.780)! Learning rate decreased to 0.00605.
2024-12-02-13:57:20-root-INFO: grad norm: 48.443 47.932 7.016
2024-12-02-13:57:21-root-INFO: grad norm: 32.064 31.498 6.001
2024-12-02-13:57:23-root-INFO: grad norm: 28.999 28.448 5.624
2024-12-02-13:57:24-root-INFO: grad norm: 25.422 24.851 5.359
2024-12-02-13:57:25-root-INFO: grad norm: 23.880 23.292 5.265
2024-12-02-13:57:26-root-INFO: grad norm: 22.232 21.645 5.077
2024-12-02-13:57:26-root-INFO: Loss Change: 691.347 -> 670.650
2024-12-02-13:57:26-root-INFO: Regularization Change: 0.000 -> 0.872
2024-12-02-13:57:26-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-13:57:26-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:57:27-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-13:57:27-root-INFO: grad norm: 32.071 31.397 6.541
2024-12-02-13:57:27-root-INFO: Loss too large (671.223->691.178)! Learning rate decreased to 0.01531.
2024-12-02-13:57:28-root-INFO: Loss too large (671.223->678.420)! Learning rate decreased to 0.01225.
2024-12-02-13:57:28-root-INFO: Loss too large (671.223->672.239)! Learning rate decreased to 0.00980.
2024-12-02-13:57:29-root-INFO: grad norm: 51.464 50.804 8.213
2024-12-02-13:57:29-root-INFO: Loss too large (669.380->674.588)! Learning rate decreased to 0.00784.
2024-12-02-13:57:30-root-INFO: Loss too large (669.380->670.073)! Learning rate decreased to 0.00627.
2024-12-02-13:57:31-root-INFO: grad norm: 40.077 39.568 6.365
2024-12-02-13:57:32-root-INFO: grad norm: 25.855 25.285 5.398
2024-12-02-13:57:33-root-INFO: grad norm: 22.761 22.169 5.154
2024-12-02-13:57:34-root-INFO: grad norm: 20.090 19.480 4.911
2024-12-02-13:57:35-root-INFO: grad norm: 18.910 18.271 4.872
2024-12-02-13:57:36-root-INFO: grad norm: 17.997 17.364 4.730
2024-12-02-13:57:36-root-INFO: Loss Change: 671.223 -> 653.111
2024-12-02-13:57:36-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-02-13:57:36-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-13:57:36-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-13:57:37-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-13:57:37-root-INFO: grad norm: 27.360 26.749 5.748
2024-12-02-13:57:37-root-INFO: Loss too large (652.950->660.540)! Learning rate decreased to 0.01586.
2024-12-02-13:57:38-root-INFO: Loss too large (652.950->654.612)! Learning rate decreased to 0.01269.
2024-12-02-13:57:39-root-INFO: grad norm: 53.273 52.592 8.487
2024-12-02-13:57:39-root-INFO: Loss too large (651.741->665.077)! Learning rate decreased to 0.01015.
2024-12-02-13:57:39-root-INFO: Loss too large (651.741->657.066)! Learning rate decreased to 0.00812.
2024-12-02-13:57:40-root-INFO: Loss too large (651.741->652.131)! Learning rate decreased to 0.00650.
2024-12-02-13:57:41-root-INFO: grad norm: 38.483 38.014 5.986
2024-12-02-13:57:42-root-INFO: grad norm: 22.699 22.131 5.047
2024-12-02-13:57:43-root-INFO: grad norm: 19.745 19.162 4.761
2024-12-02-13:57:44-root-INFO: grad norm: 17.750 17.140 4.615
2024-12-02-13:57:45-root-INFO: grad norm: 16.938 16.309 4.572
2024-12-02-13:57:46-root-INFO: grad norm: 16.429 15.804 4.490
2024-12-02-13:57:46-root-INFO: Loss Change: 652.950 -> 635.702
2024-12-02-13:57:46-root-INFO: Regularization Change: 0.000 -> 0.950
2024-12-02-13:57:47-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-13:57:47-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-13:57:47-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-13:57:47-root-INFO: grad norm: 28.698 27.860 6.888
2024-12-02-13:57:48-root-INFO: Loss too large (636.595->642.826)! Learning rate decreased to 0.01642.
2024-12-02-13:57:48-root-INFO: Loss too large (636.595->637.380)! Learning rate decreased to 0.01314.
2024-12-02-13:57:49-root-INFO: grad norm: 48.067 47.092 9.629
2024-12-02-13:57:49-root-INFO: Loss too large (634.708->643.933)! Learning rate decreased to 0.01051.
2024-12-02-13:57:50-root-INFO: Loss too large (634.708->637.326)! Learning rate decreased to 0.00841.
2024-12-02-13:57:50-root-INFO: grad norm: 43.269 42.682 7.102
2024-12-02-13:57:51-root-INFO: grad norm: 37.329 36.721 6.705
2024-12-02-13:57:52-root-INFO: grad norm: 35.195 34.709 5.831
2024-12-02-13:57:53-root-INFO: grad norm: 32.166 31.665 5.652
2024-12-02-13:57:54-root-INFO: grad norm: 30.974 30.512 5.327
2024-12-02-13:57:55-root-INFO: grad norm: 29.317 28.859 5.165
2024-12-02-13:57:56-root-INFO: Loss Change: 636.595 -> 618.001
2024-12-02-13:57:56-root-INFO: Regularization Change: 0.000 -> 1.351
2024-12-02-13:57:56-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-13:57:56-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-13:57:57-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-13:57:57-root-INFO: grad norm: 36.113 35.578 6.188
2024-12-02-13:57:57-root-INFO: Loss too large (618.869->651.582)! Learning rate decreased to 0.01701.
2024-12-02-13:57:58-root-INFO: Loss too large (618.869->630.523)! Learning rate decreased to 0.01361.
2024-12-02-13:57:58-root-INFO: Loss too large (618.869->620.711)! Learning rate decreased to 0.01088.
2024-12-02-13:57:59-root-INFO: grad norm: 46.245 45.656 7.358
2024-12-02-13:57:59-root-INFO: Loss too large (616.331->618.462)! Learning rate decreased to 0.00871.
2024-12-02-13:58:00-root-INFO: grad norm: 39.011 38.552 5.965
2024-12-02-13:58:01-root-INFO: grad norm: 28.226 27.700 5.426
2024-12-02-13:58:02-root-INFO: grad norm: 25.169 24.708 4.796
2024-12-02-13:58:03-root-INFO: grad norm: 21.938 21.428 4.705
2024-12-02-13:58:04-root-INFO: grad norm: 20.429 19.943 4.427
2024-12-02-13:58:05-root-INFO: grad norm: 18.941 18.430 4.369
2024-12-02-13:58:06-root-INFO: Loss Change: 618.869 -> 599.646
2024-12-02-13:58:06-root-INFO: Regularization Change: 0.000 -> 1.190
2024-12-02-13:58:06-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-13:58:06-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-13:58:06-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-13:58:07-root-INFO: grad norm: 28.318 27.740 5.693
2024-12-02-13:58:07-root-INFO: Loss too large (600.505->609.832)! Learning rate decreased to 0.01761.
2024-12-02-13:58:07-root-INFO: Loss too large (600.505->602.863)! Learning rate decreased to 0.01409.
2024-12-02-13:58:08-root-INFO: grad norm: 45.443 44.544 8.996
2024-12-02-13:58:09-root-INFO: Loss too large (599.399->606.184)! Learning rate decreased to 0.01127.
2024-12-02-13:58:09-root-INFO: Loss too large (599.399->600.237)! Learning rate decreased to 0.00902.
2024-12-02-13:58:10-root-INFO: grad norm: 35.808 35.280 6.126
2024-12-02-13:58:11-root-INFO: grad norm: 25.290 24.585 5.927
2024-12-02-13:58:12-root-INFO: grad norm: 21.576 21.087 4.565
2024-12-02-13:58:13-root-INFO: grad norm: 18.567 17.945 4.767
2024-12-02-13:58:14-root-INFO: grad norm: 17.046 16.547 4.095
2024-12-02-13:58:15-root-INFO: grad norm: 15.885 15.298 4.279
2024-12-02-13:58:16-root-INFO: Loss Change: 600.505 -> 582.694
2024-12-02-13:58:16-root-INFO: Regularization Change: 0.000 -> 1.260
2024-12-02-13:58:16-root-INFO: Undo step: 140
2024-12-02-13:58:16-root-INFO: Undo step: 141
2024-12-02-13:58:16-root-INFO: Undo step: 142
2024-12-02-13:58:16-root-INFO: Undo step: 143
2024-12-02-13:58:16-root-INFO: Undo step: 144
2024-12-02-13:58:16-root-INFO: Undo step: 145
2024-12-02-13:58:16-root-INFO: Undo step: 146
2024-12-02-13:58:16-root-INFO: Undo step: 147
2024-12-02-13:58:16-root-INFO: Undo step: 148
2024-12-02-13:58:16-root-INFO: Undo step: 149
2024-12-02-13:58:16-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-13:58:16-root-INFO: grad norm: 374.383 368.790 64.470
2024-12-02-13:58:17-root-INFO: Loss too large (1752.757->1807.051)! Learning rate decreased to 0.01235.
2024-12-02-13:58:18-root-INFO: grad norm: 410.719 407.354 52.473
2024-12-02-13:58:19-root-INFO: grad norm: 216.122 212.438 39.737
2024-12-02-13:58:20-root-INFO: grad norm: 294.213 292.369 32.889
2024-12-02-13:58:21-root-INFO: grad norm: 195.093 192.996 28.527
2024-12-02-13:58:22-root-INFO: grad norm: 177.218 175.110 27.255
2024-12-02-13:58:23-root-INFO: grad norm: 109.455 107.175 22.226
2024-12-02-13:58:23-root-INFO: Loss too large (852.740->882.143)! Learning rate decreased to 0.00988.
2024-12-02-13:58:23-root-INFO: Loss too large (852.740->858.532)! Learning rate decreased to 0.00790.
2024-12-02-13:58:24-root-INFO: grad norm: 113.971 112.753 16.620
2024-12-02-13:58:25-root-INFO: Loss Change: 1752.757 -> 826.517
2024-12-02-13:58:25-root-INFO: Regularization Change: 0.000 -> 51.466
2024-12-02-13:58:25-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-13:58:25-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:58:25-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-13:58:26-root-INFO: grad norm: 166.766 165.467 20.774
2024-12-02-13:58:26-root-INFO: Loss too large (813.781->1091.574)! Learning rate decreased to 0.01281.
2024-12-02-13:58:26-root-INFO: Loss too large (813.781->984.598)! Learning rate decreased to 0.01024.
2024-12-02-13:58:27-root-INFO: Loss too large (813.781->913.695)! Learning rate decreased to 0.00820.
2024-12-02-13:58:27-root-INFO: Loss too large (813.781->866.925)! Learning rate decreased to 0.00656.
2024-12-02-13:58:27-root-INFO: Loss too large (813.781->836.101)! Learning rate decreased to 0.00525.
2024-12-02-13:58:28-root-INFO: Loss too large (813.781->815.917)! Learning rate decreased to 0.00420.
2024-12-02-13:58:29-root-INFO: grad norm: 89.178 88.331 12.266
2024-12-02-13:58:30-root-INFO: grad norm: 43.821 42.476 10.775
2024-12-02-13:58:31-root-INFO: grad norm: 39.535 38.095 10.574
2024-12-02-13:58:32-root-INFO: grad norm: 37.993 36.614 10.145
2024-12-02-13:58:33-root-INFO: grad norm: 36.822 35.472 9.879
2024-12-02-13:58:34-root-INFO: grad norm: 35.792 34.482 9.593
2024-12-02-13:58:35-root-INFO: grad norm: 34.842 33.565 9.348
2024-12-02-13:58:35-root-INFO: Loss Change: 813.781 -> 743.344
2024-12-02-13:58:35-root-INFO: Regularization Change: 0.000 -> 1.755
2024-12-02-13:58:35-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-13:58:35-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-13:58:36-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-13:58:36-root-INFO: grad norm: 40.338 39.041 10.148
2024-12-02-13:58:37-root-INFO: grad norm: 104.363 103.339 14.584
2024-12-02-13:58:37-root-INFO: Loss too large (731.429->854.336)! Learning rate decreased to 0.01328.
2024-12-02-13:58:38-root-INFO: Loss too large (731.429->804.053)! Learning rate decreased to 0.01062.
2024-12-02-13:58:38-root-INFO: Loss too large (731.429->771.299)! Learning rate decreased to 0.00850.
2024-12-02-13:58:38-root-INFO: Loss too large (731.429->750.035)! Learning rate decreased to 0.00680.
2024-12-02-13:58:39-root-INFO: Loss too large (731.429->736.397)! Learning rate decreased to 0.00544.
2024-12-02-13:58:40-root-INFO: grad norm: 70.896 70.263 9.453
2024-12-02-13:58:41-root-INFO: grad norm: 30.536 29.536 7.747
2024-12-02-13:58:42-root-INFO: grad norm: 28.869 27.874 7.512
2024-12-02-13:58:43-root-INFO: grad norm: 27.708 26.717 7.343
2024-12-02-13:58:44-root-INFO: grad norm: 26.826 25.844 7.194
2024-12-02-13:58:45-root-INFO: grad norm: 26.058 25.087 7.048
2024-12-02-13:58:45-root-INFO: Loss Change: 740.100 -> 691.410
2024-12-02-13:58:45-root-INFO: Regularization Change: 0.000 -> 2.634
2024-12-02-13:58:45-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-13:58:45-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-13:58:46-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-13:58:46-root-INFO: grad norm: 31.673 30.712 7.743
2024-12-02-13:58:47-root-INFO: grad norm: 89.755 89.067 11.087
2024-12-02-13:58:47-root-INFO: Loss too large (685.559->785.444)! Learning rate decreased to 0.01376.
2024-12-02-13:58:48-root-INFO: Loss too large (685.559->744.986)! Learning rate decreased to 0.01101.
2024-12-02-13:58:48-root-INFO: Loss too large (685.559->718.478)! Learning rate decreased to 0.00881.
2024-12-02-13:58:48-root-INFO: Loss too large (685.559->701.174)! Learning rate decreased to 0.00705.
2024-12-02-13:58:49-root-INFO: Loss too large (685.559->690.039)! Learning rate decreased to 0.00564.
2024-12-02-13:58:50-root-INFO: grad norm: 60.459 59.916 8.090
2024-12-02-13:58:51-root-INFO: grad norm: 24.985 24.188 6.263
2024-12-02-13:58:52-root-INFO: grad norm: 23.242 22.411 6.162
2024-12-02-13:58:53-root-INFO: grad norm: 22.175 21.343 6.017
2024-12-02-13:58:54-root-INFO: grad norm: 21.542 20.700 5.963
2024-12-02-13:58:55-root-INFO: grad norm: 21.054 20.221 5.861
2024-12-02-13:58:55-root-INFO: Loss Change: 688.763 -> 658.217
2024-12-02-13:58:55-root-INFO: Regularization Change: 0.000 -> 1.701
2024-12-02-13:58:55-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-13:58:55-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:58:56-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-13:58:56-root-INFO: grad norm: 28.266 27.433 6.812
2024-12-02-13:58:57-root-INFO: grad norm: 81.787 81.048 10.972
2024-12-02-13:58:57-root-INFO: Loss too large (656.287->743.007)! Learning rate decreased to 0.01426.
2024-12-02-13:58:58-root-INFO: Loss too large (656.287->706.940)! Learning rate decreased to 0.01141.
2024-12-02-13:58:58-root-INFO: Loss too large (656.287->683.473)! Learning rate decreased to 0.00913.
2024-12-02-13:58:58-root-INFO: Loss too large (656.287->668.325)! Learning rate decreased to 0.00730.
2024-12-02-13:58:59-root-INFO: Loss too large (656.287->658.738)! Learning rate decreased to 0.00584.
2024-12-02-13:59:00-root-INFO: grad norm: 52.584 52.100 7.117
2024-12-02-13:59:01-root-INFO: grad norm: 21.567 20.869 5.442
2024-12-02-13:59:02-root-INFO: grad norm: 19.860 19.122 5.362
2024-12-02-13:59:03-root-INFO: grad norm: 19.002 18.262 5.251
2024-12-02-13:59:04-root-INFO: grad norm: 18.573 17.825 5.221
2024-12-02-13:59:05-root-INFO: grad norm: 18.273 17.532 5.151
2024-12-02-13:59:05-root-INFO: Loss Change: 656.906 -> 633.745
2024-12-02-13:59:05-root-INFO: Regularization Change: 0.000 -> 1.322
2024-12-02-13:59:05-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-13:59:05-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:59:06-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-13:59:06-root-INFO: grad norm: 30.161 29.314 7.096
2024-12-02-13:59:07-root-INFO: Loss too large (633.792->635.515)! Learning rate decreased to 0.01478.
2024-12-02-13:59:08-root-INFO: grad norm: 57.078 56.369 8.967
2024-12-02-13:59:08-root-INFO: Loss too large (631.964->654.105)! Learning rate decreased to 0.01182.
2024-12-02-13:59:08-root-INFO: Loss too large (631.964->641.706)! Learning rate decreased to 0.00946.
2024-12-02-13:59:09-root-INFO: Loss too large (631.964->634.100)! Learning rate decreased to 0.00757.
2024-12-02-13:59:10-root-INFO: grad norm: 46.531 46.016 6.906
2024-12-02-13:59:11-root-INFO: grad norm: 35.377 34.898 5.802
2024-12-02-13:59:12-root-INFO: grad norm: 32.675 32.176 5.686
2024-12-02-13:59:13-root-INFO: grad norm: 29.045 28.574 5.211
2024-12-02-13:59:14-root-INFO: grad norm: 27.656 27.143 5.306
2024-12-02-13:59:15-root-INFO: grad norm: 25.904 25.427 4.949
2024-12-02-13:59:15-root-INFO: Loss Change: 633.792 -> 612.423
2024-12-02-13:59:15-root-INFO: Regularization Change: 0.000 -> 1.410
2024-12-02-13:59:15-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-13:59:15-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-13:59:16-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-13:59:16-root-INFO: grad norm: 34.365 33.662 6.917
2024-12-02-13:59:16-root-INFO: Loss too large (612.506->631.696)! Learning rate decreased to 0.01531.
2024-12-02-13:59:17-root-INFO: Loss too large (612.506->618.578)! Learning rate decreased to 0.01225.
2024-12-02-13:59:18-root-INFO: grad norm: 65.240 64.622 8.960
2024-12-02-13:59:18-root-INFO: Loss too large (612.373->628.919)! Learning rate decreased to 0.00980.
2024-12-02-13:59:18-root-INFO: Loss too large (612.373->618.634)! Learning rate decreased to 0.00784.
2024-12-02-13:59:19-root-INFO: grad norm: 51.237 50.747 7.069
2024-12-02-13:59:20-root-INFO: grad norm: 28.162 27.667 5.257
2024-12-02-13:59:21-root-INFO: grad norm: 24.553 24.036 5.009
2024-12-02-13:59:22-root-INFO: grad norm: 21.077 20.570 4.596
2024-12-02-13:59:23-root-INFO: grad norm: 19.557 19.004 4.615
2024-12-02-13:59:24-root-INFO: grad norm: 18.147 17.614 4.368
2024-12-02-13:59:25-root-INFO: Loss Change: 612.506 -> 593.299
2024-12-02-13:59:25-root-INFO: Regularization Change: 0.000 -> 1.151
2024-12-02-13:59:25-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-13:59:25-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-13:59:26-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-13:59:26-root-INFO: grad norm: 27.368 26.736 5.850
2024-12-02-13:59:26-root-INFO: Loss too large (592.958->598.829)! Learning rate decreased to 0.01586.
2024-12-02-13:59:27-root-INFO: Loss too large (592.958->593.701)! Learning rate decreased to 0.01269.
2024-12-02-13:59:28-root-INFO: grad norm: 41.788 41.200 6.988
2024-12-02-13:59:28-root-INFO: Loss too large (591.241->595.472)! Learning rate decreased to 0.01015.
2024-12-02-13:59:28-root-INFO: Loss too large (591.241->591.350)! Learning rate decreased to 0.00812.
2024-12-02-13:59:29-root-INFO: grad norm: 33.002 32.526 5.586
2024-12-02-13:59:30-root-INFO: grad norm: 23.598 23.114 4.756
2024-12-02-13:59:31-root-INFO: grad norm: 20.599 20.098 4.515
2024-12-02-13:59:32-root-INFO: grad norm: 17.952 17.450 4.218
2024-12-02-13:59:33-root-INFO: grad norm: 16.713 16.175 4.207
2024-12-02-13:59:34-root-INFO: grad norm: 15.706 15.180 4.033
2024-12-02-13:59:35-root-INFO: Loss Change: 592.958 -> 576.441
2024-12-02-13:59:35-root-INFO: Regularization Change: 0.000 -> 1.062
2024-12-02-13:59:35-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-13:59:35-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-13:59:35-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-13:59:36-root-INFO: grad norm: 29.180 28.158 7.657
2024-12-02-13:59:36-root-INFO: Loss too large (577.417->582.562)! Learning rate decreased to 0.01642.
2024-12-02-13:59:36-root-INFO: Loss too large (577.417->577.593)! Learning rate decreased to 0.01314.
2024-12-02-13:59:37-root-INFO: grad norm: 39.221 38.237 8.730
2024-12-02-13:59:38-root-INFO: Loss too large (575.141->577.866)! Learning rate decreased to 0.01051.
2024-12-02-13:59:39-root-INFO: grad norm: 39.920 39.053 8.275
2024-12-02-13:59:40-root-INFO: grad norm: 43.356 42.558 8.281
2024-12-02-13:59:40-root-INFO: Loss too large (571.302->571.693)! Learning rate decreased to 0.00841.
2024-12-02-13:59:41-root-INFO: grad norm: 32.751 32.193 6.016
2024-12-02-13:59:42-root-INFO: grad norm: 21.886 21.381 4.676
2024-12-02-13:59:43-root-INFO: grad norm: 18.587 18.072 4.343
2024-12-02-13:59:44-root-INFO: grad norm: 16.092 15.595 3.968
2024-12-02-13:59:45-root-INFO: Loss Change: 577.417 -> 560.232
2024-12-02-13:59:45-root-INFO: Regularization Change: 0.000 -> 1.178
2024-12-02-13:59:45-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-13:59:45-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-13:59:45-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-13:59:45-root-INFO: grad norm: 22.998 22.353 5.407
2024-12-02-13:59:46-root-INFO: Loss too large (560.042->562.383)! Learning rate decreased to 0.01701.
2024-12-02-13:59:47-root-INFO: grad norm: 43.553 42.842 7.840
2024-12-02-13:59:47-root-INFO: Loss too large (559.447->572.138)! Learning rate decreased to 0.01361.
2024-12-02-13:59:47-root-INFO: Loss too large (559.447->564.070)! Learning rate decreased to 0.01088.
2024-12-02-13:59:48-root-INFO: grad norm: 42.016 41.366 7.363
2024-12-02-13:59:49-root-INFO: grad norm: 39.333 38.663 7.228
2024-12-02-13:59:50-root-INFO: grad norm: 38.081 37.456 6.868
2024-12-02-13:59:51-root-INFO: grad norm: 36.111 35.478 6.733
2024-12-02-13:59:52-root-INFO: grad norm: 35.210 34.614 6.447
2024-12-02-13:59:53-root-INFO: grad norm: 33.980 33.385 6.328
2024-12-02-13:59:54-root-INFO: Loss Change: 560.042 -> 546.278
2024-12-02-13:59:54-root-INFO: Regularization Change: 0.000 -> 1.493
2024-12-02-13:59:54-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-13:59:54-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-13:59:54-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-13:59:55-root-INFO: grad norm: 42.906 41.999 8.775
2024-12-02-13:59:55-root-INFO: Loss too large (548.102->578.547)! Learning rate decreased to 0.01761.
2024-12-02-13:59:55-root-INFO: Loss too large (548.102->557.914)! Learning rate decreased to 0.01409.
2024-12-02-13:59:56-root-INFO: grad norm: 59.960 58.745 12.008
2024-12-02-13:59:57-root-INFO: Loss too large (547.832->557.049)! Learning rate decreased to 0.01127.
2024-12-02-13:59:58-root-INFO: grad norm: 52.004 51.108 9.611
2024-12-02-13:59:59-root-INFO: grad norm: 39.474 38.442 8.970
2024-12-02-14:00:00-root-INFO: grad norm: 35.072 34.253 7.531
2024-12-02-14:00:01-root-INFO: grad norm: 30.546 29.677 7.235
2024-12-02-14:00:02-root-INFO: grad norm: 28.115 27.397 6.317
2024-12-02-14:00:03-root-INFO: grad norm: 25.787 25.040 6.162
2024-12-02-14:00:03-root-INFO: Loss Change: 548.102 -> 529.151
2024-12-02-14:00:03-root-INFO: Regularization Change: 0.000 -> 1.363
2024-12-02-14:00:03-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-14:00:03-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-14:00:04-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-14:00:04-root-INFO: grad norm: 34.293 33.415 7.708
2024-12-02-14:00:04-root-INFO: Loss too large (530.899->545.417)! Learning rate decreased to 0.01823.
2024-12-02-14:00:05-root-INFO: Loss too large (530.899->535.171)! Learning rate decreased to 0.01458.
2024-12-02-14:00:06-root-INFO: grad norm: 43.685 42.598 9.687
2024-12-02-14:00:06-root-INFO: Loss too large (529.844->532.571)! Learning rate decreased to 0.01167.
2024-12-02-14:00:07-root-INFO: grad norm: 37.795 36.963 7.887
2024-12-02-14:00:08-root-INFO: grad norm: 31.469 30.600 7.343
2024-12-02-14:00:09-root-INFO: grad norm: 27.932 27.229 6.227
2024-12-02-14:00:10-root-INFO: grad norm: 24.605 23.878 5.935
2024-12-02-14:00:11-root-INFO: grad norm: 22.491 21.882 5.198
2024-12-02-14:00:12-root-INFO: grad norm: 20.567 19.934 5.063
2024-12-02-14:00:13-root-INFO: Loss Change: 530.899 -> 514.350
2024-12-02-14:00:13-root-INFO: Regularization Change: 0.000 -> 1.329
2024-12-02-14:00:13-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-14:00:13-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-14:00:13-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-14:00:14-root-INFO: grad norm: 27.175 26.378 6.534
2024-12-02-14:00:14-root-INFO: Loss too large (514.766->522.157)! Learning rate decreased to 0.01887.
2024-12-02-14:00:14-root-INFO: Loss too large (514.766->516.525)! Learning rate decreased to 0.01509.
2024-12-02-14:00:15-root-INFO: grad norm: 33.602 32.536 8.398
2024-12-02-14:00:16-root-INFO: Loss too large (513.555->514.122)! Learning rate decreased to 0.01207.
2024-12-02-14:00:17-root-INFO: grad norm: 29.074 28.335 6.513
2024-12-02-14:00:18-root-INFO: grad norm: 25.030 24.271 6.118
2024-12-02-14:00:19-root-INFO: grad norm: 22.410 21.829 5.067
2024-12-02-14:00:20-root-INFO: grad norm: 20.113 19.511 4.886
2024-12-02-14:00:21-root-INFO: grad norm: 18.574 18.082 4.245
2024-12-02-14:00:22-root-INFO: grad norm: 17.226 16.707 4.196
2024-12-02-14:00:22-root-INFO: Loss Change: 514.766 -> 500.353
2024-12-02-14:00:22-root-INFO: Regularization Change: 0.000 -> 1.280
2024-12-02-14:00:22-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-14:00:22-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-14:00:23-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-14:00:23-root-INFO: grad norm: 31.498 30.300 8.603
2024-12-02-14:00:23-root-INFO: Loss too large (502.533->510.946)! Learning rate decreased to 0.01952.
2024-12-02-14:00:24-root-INFO: Loss too large (502.533->504.064)! Learning rate decreased to 0.01562.
2024-12-02-14:00:25-root-INFO: grad norm: 35.194 33.921 9.381
2024-12-02-14:00:25-root-INFO: Loss too large (500.386->500.425)! Learning rate decreased to 0.01250.
2024-12-02-14:00:26-root-INFO: grad norm: 28.656 27.842 6.784
2024-12-02-14:00:27-root-INFO: grad norm: 23.874 23.172 5.745
2024-12-02-14:00:28-root-INFO: grad norm: 20.829 20.287 4.722
2024-12-02-14:00:29-root-INFO: grad norm: 18.494 17.982 4.323
2024-12-02-14:00:30-root-INFO: grad norm: 16.951 16.515 3.818
2024-12-02-14:00:31-root-INFO: grad norm: 15.740 15.295 3.719
2024-12-02-14:00:32-root-INFO: Loss Change: 502.533 -> 486.415
2024-12-02-14:00:32-root-INFO: Regularization Change: 0.000 -> 1.376
2024-12-02-14:00:32-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-14:00:32-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-14:00:32-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-14:00:32-root-INFO: grad norm: 19.190 18.760 4.038
2024-12-02-14:00:33-root-INFO: Loss too large (486.624->487.883)! Learning rate decreased to 0.02020.
2024-12-02-14:00:34-root-INFO: grad norm: 30.707 30.250 5.277
2024-12-02-14:00:34-root-INFO: Loss too large (485.862->490.606)! Learning rate decreased to 0.01616.
2024-12-02-14:00:34-root-INFO: Loss too large (485.862->486.334)! Learning rate decreased to 0.01293.
2024-12-02-14:00:35-root-INFO: grad norm: 26.340 25.976 4.366
2024-12-02-14:00:36-root-INFO: grad norm: 22.382 21.993 4.154
2024-12-02-14:00:37-root-INFO: grad norm: 20.070 19.731 3.669
2024-12-02-14:00:38-root-INFO: grad norm: 18.007 17.633 3.653
2024-12-02-14:00:39-root-INFO: grad norm: 16.754 16.415 3.353
2024-12-02-14:00:40-root-INFO: grad norm: 15.631 15.256 3.404
2024-12-02-14:00:41-root-INFO: Loss Change: 486.624 -> 473.874
2024-12-02-14:00:41-root-INFO: Regularization Change: 0.000 -> 1.355
2024-12-02-14:00:41-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-14:00:41-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-14:00:41-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-14:00:42-root-INFO: grad norm: 22.718 22.223 4.717
2024-12-02-14:00:42-root-INFO: Loss too large (474.049->478.088)! Learning rate decreased to 0.02090.
2024-12-02-14:00:42-root-INFO: Loss too large (474.049->474.342)! Learning rate decreased to 0.01672.
2024-12-02-14:00:43-root-INFO: grad norm: 26.947 26.493 4.924
2024-12-02-14:00:44-root-INFO: Loss too large (472.426->472.427)! Learning rate decreased to 0.01338.
2024-12-02-14:00:45-root-INFO: grad norm: 23.372 23.007 4.113
2024-12-02-14:00:46-root-INFO: grad norm: 20.196 19.811 3.921
2024-12-02-14:00:47-root-INFO: grad norm: 18.332 18.000 3.474
2024-12-02-14:00:48-root-INFO: grad norm: 16.653 16.285 3.485
2024-12-02-14:00:49-root-INFO: grad norm: 15.635 15.304 3.196
2024-12-02-14:00:50-root-INFO: grad norm: 14.714 14.346 3.268
2024-12-02-14:00:50-root-INFO: Loss Change: 474.049 -> 461.315
2024-12-02-14:00:50-root-INFO: Regularization Change: 0.000 -> 1.272
2024-12-02-14:00:50-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-14:00:50-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-14:00:51-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-14:00:51-root-INFO: grad norm: 21.197 20.668 4.704
2024-12-02-14:00:51-root-INFO: Loss too large (461.521->464.555)! Learning rate decreased to 0.02162.
2024-12-02-14:00:52-root-INFO: grad norm: 34.225 33.617 6.419
2024-12-02-14:00:53-root-INFO: Loss too large (461.424->468.321)! Learning rate decreased to 0.01729.
2024-12-02-14:00:53-root-INFO: Loss too large (461.424->462.586)! Learning rate decreased to 0.01384.
2024-12-02-14:00:54-root-INFO: grad norm: 28.251 27.832 4.847
2024-12-02-14:00:55-root-INFO: grad norm: 22.903 22.501 4.275
2024-12-02-14:00:56-root-INFO: grad norm: 20.120 19.792 3.619
2024-12-02-14:00:57-root-INFO: grad norm: 17.683 17.332 3.504
2024-12-02-14:00:58-root-INFO: grad norm: 16.304 15.993 3.169
2024-12-02-14:00:59-root-INFO: grad norm: 15.097 14.755 3.194
2024-12-02-14:01:00-root-INFO: Loss Change: 461.521 -> 449.112
2024-12-02-14:01:00-root-INFO: Regularization Change: 0.000 -> 1.356
2024-12-02-14:01:00-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-14:01:00-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-14:01:00-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-14:01:00-root-INFO: grad norm: 22.487 21.953 4.871
2024-12-02-14:01:01-root-INFO: Loss too large (450.138->454.890)! Learning rate decreased to 0.02236.
2024-12-02-14:01:01-root-INFO: Loss too large (450.138->450.787)! Learning rate decreased to 0.01789.
2024-12-02-14:01:02-root-INFO: grad norm: 26.798 26.320 5.043
2024-12-02-14:01:02-root-INFO: Loss too large (448.656->448.932)! Learning rate decreased to 0.01431.
2024-12-02-14:01:03-root-INFO: grad norm: 23.149 22.789 4.065
2024-12-02-14:01:04-root-INFO: grad norm: 19.966 19.609 3.761
2024-12-02-14:01:05-root-INFO: grad norm: 18.198 17.892 3.324
2024-12-02-14:01:06-root-INFO: grad norm: 16.623 16.294 3.292
2024-12-02-14:01:07-root-INFO: grad norm: 15.718 15.419 3.051
2024-12-02-14:01:08-root-INFO: grad norm: 14.895 14.572 3.089
2024-12-02-14:01:09-root-INFO: Loss Change: 450.138 -> 438.153
2024-12-02-14:01:09-root-INFO: Regularization Change: 0.000 -> 1.271
2024-12-02-14:01:09-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-14:01:09-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-14:01:09-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-14:01:10-root-INFO: grad norm: 18.846 18.442 3.885
2024-12-02-14:01:10-root-INFO: Loss too large (438.193->440.609)! Learning rate decreased to 0.02312.
2024-12-02-14:01:11-root-INFO: grad norm: 31.204 30.755 5.269
2024-12-02-14:01:11-root-INFO: Loss too large (438.007->443.830)! Learning rate decreased to 0.01849.
2024-12-02-14:01:12-root-INFO: Loss too large (438.007->439.169)! Learning rate decreased to 0.01479.
2024-12-02-14:01:13-root-INFO: grad norm: 25.853 25.520 4.138
2024-12-02-14:01:14-root-INFO: grad norm: 20.558 20.228 3.671
2024-12-02-14:01:15-root-INFO: grad norm: 18.392 18.106 3.228
2024-12-02-14:01:16-root-INFO: grad norm: 16.372 16.064 3.162
2024-12-02-14:01:17-root-INFO: grad norm: 15.316 15.033 2.935
2024-12-02-14:01:18-root-INFO: grad norm: 14.329 14.023 2.945
2024-12-02-14:01:19-root-INFO: Loss Change: 438.193 -> 426.852
2024-12-02-14:01:19-root-INFO: Regularization Change: 0.000 -> 1.349
2024-12-02-14:01:19-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-14:01:19-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-14:01:19-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-14:01:19-root-INFO: grad norm: 19.047 18.538 4.376
2024-12-02-14:01:19-root-INFO: Loss too large (427.526->429.375)! Learning rate decreased to 0.02390.
2024-12-02-14:01:20-root-INFO: grad norm: 29.374 28.915 5.172
2024-12-02-14:01:21-root-INFO: Loss too large (426.906->432.076)! Learning rate decreased to 0.01912.
2024-12-02-14:01:21-root-INFO: Loss too large (426.906->427.616)! Learning rate decreased to 0.01530.
2024-12-02-14:01:22-root-INFO: grad norm: 24.296 23.970 3.969
2024-12-02-14:01:23-root-INFO: grad norm: 19.984 19.680 3.471
2024-12-02-14:01:24-root-INFO: grad norm: 17.866 17.596 3.093
2024-12-02-14:01:25-root-INFO: grad norm: 16.003 15.718 3.007
2024-12-02-14:01:26-root-INFO: grad norm: 14.993 14.724 2.823
2024-12-02-14:01:27-root-INFO: grad norm: 14.087 13.802 2.821
2024-12-02-14:01:28-root-INFO: Loss Change: 427.526 -> 416.142
2024-12-02-14:01:28-root-INFO: Regularization Change: 0.000 -> 1.381
2024-12-02-14:01:28-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-14:01:28-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-14:01:28-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-14:01:28-root-INFO: grad norm: 17.829 17.429 3.755
2024-12-02-14:01:29-root-INFO: Loss too large (416.435->418.999)! Learning rate decreased to 0.02471.
2024-12-02-14:01:30-root-INFO: grad norm: 29.859 29.450 4.925
2024-12-02-14:01:30-root-INFO: Loss too large (416.429->422.057)! Learning rate decreased to 0.01976.
2024-12-02-14:01:30-root-INFO: Loss too large (416.429->417.636)! Learning rate decreased to 0.01581.
2024-12-02-14:01:31-root-INFO: grad norm: 24.604 24.293 3.905
2024-12-02-14:01:32-root-INFO: grad norm: 19.502 19.212 3.351
2024-12-02-14:01:33-root-INFO: grad norm: 17.624 17.358 3.051
2024-12-02-14:01:34-root-INFO: grad norm: 15.866 15.592 2.938
2024-12-02-14:01:35-root-INFO: grad norm: 15.007 14.743 2.804
2024-12-02-14:01:36-root-INFO: grad norm: 14.178 13.905 2.768
2024-12-02-14:01:37-root-INFO: Loss Change: 416.435 -> 406.063
2024-12-02-14:01:37-root-INFO: Regularization Change: 0.000 -> 1.329
2024-12-02-14:01:37-root-INFO: Undo step: 130
2024-12-02-14:01:37-root-INFO: Undo step: 131
2024-12-02-14:01:37-root-INFO: Undo step: 132
2024-12-02-14:01:37-root-INFO: Undo step: 133
2024-12-02-14:01:37-root-INFO: Undo step: 134
2024-12-02-14:01:37-root-INFO: Undo step: 135
2024-12-02-14:01:37-root-INFO: Undo step: 136
2024-12-02-14:01:37-root-INFO: Undo step: 137
2024-12-02-14:01:37-root-INFO: Undo step: 138
2024-12-02-14:01:37-root-INFO: Undo step: 139
2024-12-02-14:01:38-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-14:01:38-root-INFO: grad norm: 364.950 359.562 62.481
2024-12-02-14:01:38-root-INFO: Loss too large (1670.845->1675.267)! Learning rate decreased to 0.01761.
2024-12-02-14:01:39-root-INFO: grad norm: 324.741 321.197 47.843
2024-12-02-14:01:40-root-INFO: grad norm: 190.453 186.217 39.945
2024-12-02-14:01:41-root-INFO: grad norm: 143.337 140.703 27.355
2024-12-02-14:01:42-root-INFO: grad norm: 105.577 103.375 21.449
2024-12-02-14:01:43-root-INFO: grad norm: 93.825 92.724 14.331
2024-12-02-14:01:44-root-INFO: grad norm: 82.986 81.498 15.644
2024-12-02-14:01:45-root-INFO: grad norm: 78.372 77.454 11.964
2024-12-02-14:01:46-root-INFO: Loss Change: 1670.845 -> 574.562
2024-12-02-14:01:46-root-INFO: Regularization Change: 0.000 -> 72.916
2024-12-02-14:01:46-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-14:01:46-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-14:01:46-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-14:01:47-root-INFO: grad norm: 67.536 66.275 12.986
2024-12-02-14:01:47-root-INFO: Loss too large (568.708->571.661)! Learning rate decreased to 0.01823.
2024-12-02-14:01:48-root-INFO: grad norm: 65.132 64.181 11.085
2024-12-02-14:01:49-root-INFO: grad norm: 64.796 63.821 11.199
2024-12-02-14:01:50-root-INFO: grad norm: 65.943 65.139 10.263
2024-12-02-14:01:51-root-INFO: grad norm: 67.856 67.032 10.538
2024-12-02-14:01:52-root-INFO: grad norm: 69.169 68.424 10.129
2024-12-02-14:01:53-root-INFO: grad norm: 70.172 69.409 10.322
2024-12-02-14:01:54-root-INFO: grad norm: 70.344 69.611 10.129
2024-12-02-14:01:55-root-INFO: Loss Change: 568.708 -> 506.457
2024-12-02-14:01:55-root-INFO: Regularization Change: 0.000 -> 9.316
2024-12-02-14:01:55-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-14:01:55-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-14:01:55-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-14:01:55-root-INFO: grad norm: 66.918 66.028 10.880
2024-12-02-14:01:56-root-INFO: Loss too large (502.898->523.442)! Learning rate decreased to 0.01887.
2024-12-02-14:01:57-root-INFO: grad norm: 66.612 65.760 10.621
2024-12-02-14:01:58-root-INFO: grad norm: 66.280 65.574 9.643
2024-12-02-14:01:59-root-INFO: grad norm: 66.154 65.501 9.275
2024-12-02-14:02:00-root-INFO: grad norm: 65.954 65.358 8.848
2024-12-02-14:02:01-root-INFO: grad norm: 65.609 65.054 8.517
2024-12-02-14:02:02-root-INFO: grad norm: 65.337 64.799 8.373
2024-12-02-14:02:03-root-INFO: grad norm: 64.991 64.484 8.102
2024-12-02-14:02:03-root-INFO: Loss Change: 502.898 -> 472.950
2024-12-02-14:02:03-root-INFO: Regularization Change: 0.000 -> 4.424
2024-12-02-14:02:03-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-14:02:03-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-14:02:04-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-14:02:04-root-INFO: grad norm: 58.458 57.600 9.983
2024-12-02-14:02:04-root-INFO: Loss too large (467.949->484.313)! Learning rate decreased to 0.01952.
2024-12-02-14:02:05-root-INFO: grad norm: 58.020 57.258 9.372
2024-12-02-14:02:06-root-INFO: grad norm: 58.104 57.572 7.848
2024-12-02-14:02:07-root-INFO: grad norm: 58.942 58.444 7.651
2024-12-02-14:02:08-root-INFO: grad norm: 59.330 58.901 7.116
2024-12-02-14:02:09-root-INFO: grad norm: 59.570 59.165 6.931
2024-12-02-14:02:10-root-INFO: grad norm: 59.561 59.171 6.807
2024-12-02-14:02:11-root-INFO: grad norm: 59.455 59.086 6.615
2024-12-02-14:02:12-root-INFO: Loss Change: 467.949 -> 448.362
2024-12-02-14:02:12-root-INFO: Regularization Change: 0.000 -> 3.316
2024-12-02-14:02:12-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-14:02:12-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-14:02:12-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-14:02:13-root-INFO: grad norm: 55.913 55.583 6.060
2024-12-02-14:02:13-root-INFO: Loss too large (446.175->463.589)! Learning rate decreased to 0.02020.
2024-12-02-14:02:14-root-INFO: grad norm: 56.132 55.758 6.465
2024-12-02-14:02:15-root-INFO: grad norm: 55.834 55.497 6.127
2024-12-02-14:02:16-root-INFO: grad norm: 55.963 55.618 6.213
2024-12-02-14:02:17-root-INFO: grad norm: 55.830 55.494 6.122
2024-12-02-14:02:18-root-INFO: grad norm: 55.652 55.311 6.154
2024-12-02-14:02:19-root-INFO: grad norm: 55.402 55.059 6.152
2024-12-02-14:02:20-root-INFO: grad norm: 55.106 54.761 6.152
2024-12-02-14:02:21-root-INFO: Loss Change: 446.175 -> 430.222
2024-12-02-14:02:21-root-INFO: Regularization Change: 0.000 -> 2.609
2024-12-02-14:02:21-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-14:02:21-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-14:02:21-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-14:02:21-root-INFO: grad norm: 49.181 48.887 5.366
2024-12-02-14:02:22-root-INFO: Loss too large (426.132->440.081)! Learning rate decreased to 0.02090.
2024-12-02-14:02:23-root-INFO: grad norm: 50.241 49.869 6.106
2024-12-02-14:02:24-root-INFO: grad norm: 51.354 51.030 5.760
2024-12-02-14:02:25-root-INFO: grad norm: 52.357 51.996 6.136
2024-12-02-14:02:26-root-INFO: grad norm: 52.913 52.563 6.071
2024-12-02-14:02:27-root-INFO: grad norm: 53.219 52.847 6.281
2024-12-02-14:02:28-root-INFO: grad norm: 53.240 52.867 6.293
2024-12-02-14:02:29-root-INFO: grad norm: 53.122 52.735 6.402
2024-12-02-14:02:30-root-INFO: Loss Change: 426.132 -> 414.954
2024-12-02-14:02:30-root-INFO: Regularization Change: 0.000 -> 2.332
2024-12-02-14:02:30-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-14:02:30-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-14:02:30-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-14:02:30-root-INFO: grad norm: 47.313 47.000 5.430
2024-12-02-14:02:31-root-INFO: Loss too large (410.965->424.055)! Learning rate decreased to 0.02162.
2024-12-02-14:02:32-root-INFO: grad norm: 47.285 46.870 6.249
2024-12-02-14:02:33-root-INFO: grad norm: 47.262 46.917 5.700
2024-12-02-14:02:34-root-INFO: grad norm: 47.750 47.363 6.068
2024-12-02-14:02:35-root-INFO: grad norm: 48.182 47.820 5.896
2024-12-02-14:02:36-root-INFO: grad norm: 48.526 48.135 6.148
2024-12-02-14:02:37-root-INFO: grad norm: 48.750 48.368 6.090
2024-12-02-14:02:38-root-INFO: grad norm: 48.861 48.458 6.259
2024-12-02-14:02:38-root-INFO: Loss Change: 410.965 -> 399.957
2024-12-02-14:02:38-root-INFO: Regularization Change: 0.000 -> 2.110
2024-12-02-14:02:38-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-14:02:38-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-14:02:39-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-14:02:39-root-INFO: grad norm: 43.520 43.209 5.193
2024-12-02-14:02:39-root-INFO: Loss too large (396.709->408.598)! Learning rate decreased to 0.02236.
2024-12-02-14:02:40-root-INFO: grad norm: 44.442 44.026 6.068
2024-12-02-14:02:41-root-INFO: grad norm: 45.643 45.293 5.641
2024-12-02-14:02:42-root-INFO: Loss too large (394.687->395.085)! Learning rate decreased to 0.01789.
2024-12-02-14:02:43-root-INFO: grad norm: 30.769 30.486 4.163
2024-12-02-14:02:44-root-INFO: grad norm: 21.317 21.079 3.179
2024-12-02-14:02:45-root-INFO: grad norm: 16.422 16.211 2.621
2024-12-02-14:02:45-root-INFO: grad norm: 13.310 13.087 2.426
2024-12-02-14:02:46-root-INFO: grad norm: 11.488 11.273 2.210
2024-12-02-14:02:47-root-INFO: Loss Change: 396.709 -> 377.287
2024-12-02-14:02:47-root-INFO: Regularization Change: 0.000 -> 1.626
2024-12-02-14:02:47-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-14:02:47-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-14:02:47-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-14:02:48-root-INFO: grad norm: 12.929 12.532 3.179
2024-12-02-14:02:49-root-INFO: grad norm: 16.008 15.680 3.220
2024-12-02-14:02:49-root-INFO: Loss too large (375.590->376.043)! Learning rate decreased to 0.02312.
2024-12-02-14:02:50-root-INFO: grad norm: 19.748 19.469 3.306
2024-12-02-14:02:50-root-INFO: Loss too large (374.576->374.774)! Learning rate decreased to 0.01849.
2024-12-02-14:02:51-root-INFO: grad norm: 17.487 17.220 3.045
2024-12-02-14:02:53-root-INFO: grad norm: 15.338 15.097 2.711
2024-12-02-14:02:53-root-INFO: grad norm: 14.371 14.137 2.583
2024-12-02-14:02:54-root-INFO: grad norm: 13.417 13.184 2.493
2024-12-02-14:02:56-root-INFO: grad norm: 12.914 12.691 2.389
2024-12-02-14:02:56-root-INFO: Loss Change: 377.018 -> 367.451
2024-12-02-14:02:56-root-INFO: Regularization Change: 0.000 -> 1.530
2024-12-02-14:02:56-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-14:02:56-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-14:02:57-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-14:02:57-root-INFO: grad norm: 20.073 19.571 4.460
2024-12-02-14:02:57-root-INFO: Loss too large (368.459->369.879)! Learning rate decreased to 0.02390.
2024-12-02-14:02:58-root-INFO: grad norm: 23.057 22.765 3.654
2024-12-02-14:02:59-root-INFO: Loss too large (367.643->367.792)! Learning rate decreased to 0.01912.
2024-12-02-14:03:00-root-INFO: grad norm: 20.160 19.902 3.214
2024-12-02-14:03:01-root-INFO: grad norm: 18.499 18.265 2.936
2024-12-02-14:03:02-root-INFO: grad norm: 16.992 16.759 2.809
2024-12-02-14:03:03-root-INFO: grad norm: 16.050 15.830 2.650
2024-12-02-14:03:04-root-INFO: grad norm: 15.213 14.986 2.616
2024-12-02-14:03:05-root-INFO: grad norm: 14.696 14.482 2.499
2024-12-02-14:03:05-root-INFO: Loss Change: 368.459 -> 358.742
2024-12-02-14:03:05-root-INFO: Regularization Change: 0.000 -> 1.354
2024-12-02-14:03:05-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-14:03:05-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-14:03:06-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-14:03:06-root-INFO: grad norm: 20.180 19.797 3.911
2024-12-02-14:03:06-root-INFO: Loss too large (359.717->362.538)! Learning rate decreased to 0.02471.
2024-12-02-14:03:07-root-INFO: Loss too large (359.717->359.806)! Learning rate decreased to 0.01976.
2024-12-02-14:03:08-root-INFO: grad norm: 18.255 18.002 3.025
2024-12-02-14:03:09-root-INFO: grad norm: 17.577 17.344 2.853
2024-12-02-14:03:10-root-INFO: grad norm: 17.124 16.893 2.803
2024-12-02-14:03:11-root-INFO: grad norm: 16.775 16.547 2.759
2024-12-02-14:03:12-root-INFO: grad norm: 16.503 16.277 2.726
2024-12-02-14:03:13-root-INFO: grad norm: 16.272 16.044 2.713
2024-12-02-14:03:14-root-INFO: grad norm: 16.101 15.877 2.679
2024-12-02-14:03:14-root-INFO: Loss Change: 359.717 -> 351.140
2024-12-02-14:03:14-root-INFO: Regularization Change: 0.000 -> 1.228
2024-12-02-14:03:14-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-14:03:14-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-14:03:15-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-14:03:15-root-INFO: grad norm: 23.887 23.434 4.627
2024-12-02-14:03:15-root-INFO: Loss too large (352.432->357.353)! Learning rate decreased to 0.02553.
2024-12-02-14:03:16-root-INFO: Loss too large (352.432->353.144)! Learning rate decreased to 0.02043.
2024-12-02-14:03:17-root-INFO: grad norm: 21.285 21.036 3.244
2024-12-02-14:03:18-root-INFO: grad norm: 20.025 19.775 3.153
2024-12-02-14:03:19-root-INFO: grad norm: 19.120 18.892 2.943
2024-12-02-14:03:20-root-INFO: grad norm: 18.288 18.055 2.908
2024-12-02-14:03:21-root-INFO: grad norm: 17.692 17.472 2.785
2024-12-02-14:03:22-root-INFO: grad norm: 17.140 16.913 2.780
2024-12-02-14:03:23-root-INFO: grad norm: 16.765 16.548 2.687
2024-12-02-14:03:23-root-INFO: Loss Change: 352.432 -> 343.132
2024-12-02-14:03:23-root-INFO: Regularization Change: 0.000 -> 1.258
2024-12-02-14:03:23-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-14:03:23-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-14:03:24-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-14:03:24-root-INFO: grad norm: 23.554 23.133 4.435
2024-12-02-14:03:24-root-INFO: Loss too large (344.252->349.668)! Learning rate decreased to 0.02639.
2024-12-02-14:03:25-root-INFO: Loss too large (344.252->345.232)! Learning rate decreased to 0.02111.
2024-12-02-14:03:26-root-INFO: grad norm: 21.249 21.009 3.186
2024-12-02-14:03:26-root-INFO: grad norm: 20.023 19.786 3.077
2024-12-02-14:03:28-root-INFO: grad norm: 19.135 18.914 2.905
2024-12-02-14:03:29-root-INFO: grad norm: 18.391 18.167 2.857
2024-12-02-14:03:30-root-INFO: grad norm: 17.835 17.619 2.772
2024-12-02-14:03:31-root-INFO: grad norm: 17.373 17.153 2.751
2024-12-02-14:03:31-root-INFO: grad norm: 17.035 16.820 2.697
2024-12-02-14:03:32-root-INFO: Loss Change: 344.252 -> 335.367
2024-12-02-14:03:32-root-INFO: Regularization Change: 0.000 -> 1.244
2024-12-02-14:03:32-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-14:03:32-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-14:03:33-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-14:03:33-root-INFO: grad norm: 23.152 22.806 3.987
2024-12-02-14:03:33-root-INFO: Loss too large (336.749->342.870)! Learning rate decreased to 0.02726.
2024-12-02-14:03:34-root-INFO: Loss too large (336.749->338.391)! Learning rate decreased to 0.02181.
2024-12-02-14:03:35-root-INFO: grad norm: 21.679 21.431 3.271
2024-12-02-14:03:36-root-INFO: grad norm: 20.487 20.250 3.107
2024-12-02-14:03:36-root-INFO: grad norm: 19.625 19.393 3.005
2024-12-02-14:03:37-root-INFO: grad norm: 18.712 18.490 2.874
2024-12-02-14:03:39-root-INFO: grad norm: 18.105 17.884 2.821
2024-12-02-14:03:40-root-INFO: grad norm: 17.512 17.296 2.739
2024-12-02-14:03:41-root-INFO: grad norm: 17.127 16.912 2.708
2024-12-02-14:03:41-root-INFO: Loss Change: 336.749 -> 328.400
2024-12-02-14:03:41-root-INFO: Regularization Change: 0.000 -> 1.225
2024-12-02-14:03:41-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-14:03:41-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-14:03:42-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-14:03:42-root-INFO: grad norm: 21.016 20.670 3.798
2024-12-02-14:03:42-root-INFO: Loss too large (329.335->334.405)! Learning rate decreased to 0.02816.
2024-12-02-14:03:43-root-INFO: Loss too large (329.335->330.470)! Learning rate decreased to 0.02253.
2024-12-02-14:03:44-root-INFO: grad norm: 19.815 19.584 3.018
2024-12-02-14:03:45-root-INFO: grad norm: 19.389 19.164 2.946
2024-12-02-14:03:46-root-INFO: grad norm: 19.054 18.832 2.901
2024-12-02-14:03:47-root-INFO: grad norm: 18.783 18.566 2.842
2024-12-02-14:03:48-root-INFO: grad norm: 18.530 18.311 2.845
2024-12-02-14:03:49-root-INFO: grad norm: 18.303 18.089 2.794
2024-12-02-14:03:50-root-INFO: grad norm: 18.103 17.885 2.804
2024-12-02-14:03:50-root-INFO: Loss Change: 329.335 -> 321.825
2024-12-02-14:03:50-root-INFO: Regularization Change: 0.000 -> 1.222
2024-12-02-14:03:50-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-14:03:50-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-14:03:51-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-14:03:51-root-INFO: grad norm: 24.553 24.168 4.328
2024-12-02-14:03:51-root-INFO: Loss too large (323.143->330.917)! Learning rate decreased to 0.02909.
2024-12-02-14:03:52-root-INFO: Loss too large (323.143->325.184)! Learning rate decreased to 0.02327.
2024-12-02-14:03:53-root-INFO: grad norm: 22.889 22.654 3.271
2024-12-02-14:03:54-root-INFO: grad norm: 21.939 21.706 3.191
2024-12-02-14:03:55-root-INFO: grad norm: 21.186 20.963 3.064
2024-12-02-14:03:56-root-INFO: grad norm: 20.477 20.258 2.987
2024-12-02-14:03:57-root-INFO: grad norm: 19.922 19.705 2.933
2024-12-02-14:03:58-root-INFO: grad norm: 19.417 19.203 2.875
2024-12-02-14:03:59-root-INFO: grad norm: 19.029 18.814 2.848
2024-12-02-14:04:00-root-INFO: Loss Change: 323.143 -> 314.876
2024-12-02-14:04:00-root-INFO: Regularization Change: 0.000 -> 1.255
2024-12-02-14:04:00-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-14:04:00-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-14:04:00-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-14:04:00-root-INFO: grad norm: 30.318 29.841 5.359
2024-12-02-14:04:01-root-INFO: Loss too large (318.133->330.921)! Learning rate decreased to 0.03019.
2024-12-02-14:04:01-root-INFO: Loss too large (318.133->321.814)! Learning rate decreased to 0.02415.
2024-12-02-14:04:02-root-INFO: grad norm: 27.848 27.580 3.860
2024-12-02-14:04:03-root-INFO: grad norm: 26.134 25.874 3.679
2024-12-02-14:04:04-root-INFO: grad norm: 24.750 24.506 3.465
2024-12-02-14:04:05-root-INFO: grad norm: 23.359 23.126 3.295
2024-12-02-14:04:06-root-INFO: grad norm: 22.328 22.101 3.172
2024-12-02-14:04:07-root-INFO: grad norm: 21.355 21.134 3.065
2024-12-02-14:04:08-root-INFO: grad norm: 20.649 20.432 2.982
2024-12-02-14:04:09-root-INFO: Loss Change: 318.133 -> 308.056
2024-12-02-14:04:09-root-INFO: Regularization Change: 0.000 -> 1.345
2024-12-02-14:04:09-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-14:04:09-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-14:04:09-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-14:04:09-root-INFO: grad norm: 25.986 25.652 4.156
2024-12-02-14:04:10-root-INFO: Loss too large (309.735->320.511)! Learning rate decreased to 0.03117.
2024-12-02-14:04:10-root-INFO: Loss too large (309.735->313.069)! Learning rate decreased to 0.02494.
2024-12-02-14:04:11-root-INFO: grad norm: 24.820 24.590 3.375
2024-12-02-14:04:12-root-INFO: grad norm: 23.831 23.599 3.320
2024-12-02-14:04:13-root-INFO: grad norm: 23.017 22.795 3.193
2024-12-02-14:04:14-root-INFO: grad norm: 22.257 22.039 3.106
2024-12-02-14:04:15-root-INFO: grad norm: 21.657 21.440 3.059
2024-12-02-14:04:16-root-INFO: grad norm: 21.125 20.913 2.984
2024-12-02-14:04:17-root-INFO: grad norm: 20.712 20.497 2.972
2024-12-02-14:04:18-root-INFO: Loss Change: 309.735 -> 301.662
2024-12-02-14:04:18-root-INFO: Regularization Change: 0.000 -> 1.288
2024-12-02-14:04:18-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-14:04:18-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-14:04:18-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-14:04:18-root-INFO: grad norm: 26.921 26.591 4.203
2024-12-02-14:04:19-root-INFO: Loss too large (303.551->315.550)! Learning rate decreased to 0.03218.
2024-12-02-14:04:19-root-INFO: Loss too large (303.551->307.360)! Learning rate decreased to 0.02574.
2024-12-02-14:04:20-root-INFO: grad norm: 25.575 25.339 3.471
2024-12-02-14:04:21-root-INFO: grad norm: 24.342 24.109 3.355
2024-12-02-14:04:22-root-INFO: grad norm: 23.359 23.136 3.219
2024-12-02-14:04:23-root-INFO: grad norm: 22.399 22.183 3.100
2024-12-02-14:04:24-root-INFO: grad norm: 21.673 21.460 3.030
2024-12-02-14:04:25-root-INFO: grad norm: 21.013 20.805 2.945
2024-12-02-14:04:26-root-INFO: grad norm: 20.522 20.315 2.909
2024-12-02-14:04:27-root-INFO: Loss Change: 303.551 -> 295.233
2024-12-02-14:04:27-root-INFO: Regularization Change: 0.000 -> 1.300
2024-12-02-14:04:27-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-14:04:27-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-14:04:27-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-14:04:27-root-INFO: grad norm: 26.508 26.181 4.150
2024-12-02-14:04:28-root-INFO: Loss too large (297.026->309.294)! Learning rate decreased to 0.03321.
2024-12-02-14:04:28-root-INFO: Loss too large (297.026->300.945)! Learning rate decreased to 0.02657.
2024-12-02-14:04:29-root-INFO: grad norm: 25.432 25.198 3.444
2024-12-02-14:04:30-root-INFO: grad norm: 24.561 24.332 3.351
2024-12-02-14:04:31-root-INFO: grad norm: 23.800 23.573 3.278
2024-12-02-14:04:32-root-INFO: grad norm: 23.043 22.825 3.161
2024-12-02-14:04:33-root-INFO: grad norm: 22.430 22.210 3.131
2024-12-02-14:04:34-root-INFO: grad norm: 21.855 21.642 3.039
2024-12-02-14:04:35-root-INFO: grad norm: 21.409 21.194 3.024
2024-12-02-14:04:36-root-INFO: Loss Change: 297.026 -> 289.244
2024-12-02-14:04:36-root-INFO: Regularization Change: 0.000 -> 1.296
2024-12-02-14:04:36-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-14:04:36-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-14:04:36-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-14:04:36-root-INFO: grad norm: 26.954 26.616 4.254
2024-12-02-14:04:37-root-INFO: Loss too large (291.233->304.481)! Learning rate decreased to 0.03427.
2024-12-02-14:04:37-root-INFO: Loss too large (291.233->295.518)! Learning rate decreased to 0.02742.
2024-12-02-14:04:38-root-INFO: grad norm: 25.849 25.618 3.448
2024-12-02-14:04:39-root-INFO: grad norm: 24.934 24.699 3.414
2024-12-02-14:04:40-root-INFO: grad norm: 24.161 23.937 3.284
2024-12-02-14:04:41-root-INFO: grad norm: 23.392 23.172 3.198
2024-12-02-14:04:42-root-INFO: grad norm: 22.777 22.560 3.136
2024-12-02-14:04:43-root-INFO: grad norm: 22.198 21.986 3.062
2024-12-02-14:04:44-root-INFO: grad norm: 21.752 21.540 3.028
2024-12-02-14:04:45-root-INFO: Loss Change: 291.233 -> 283.522
2024-12-02-14:04:45-root-INFO: Regularization Change: 0.000 -> 1.297
2024-12-02-14:04:45-root-INFO: Undo step: 120
2024-12-02-14:04:45-root-INFO: Undo step: 121
2024-12-02-14:04:45-root-INFO: Undo step: 122
2024-12-02-14:04:45-root-INFO: Undo step: 123
2024-12-02-14:04:45-root-INFO: Undo step: 124
2024-12-02-14:04:45-root-INFO: Undo step: 125
2024-12-02-14:04:45-root-INFO: Undo step: 126
2024-12-02-14:04:45-root-INFO: Undo step: 127
2024-12-02-14:04:45-root-INFO: Undo step: 128
2024-12-02-14:04:45-root-INFO: Undo step: 129
2024-12-02-14:04:45-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-14:04:45-root-INFO: grad norm: 241.761 237.766 43.769
2024-12-02-14:04:46-root-INFO: grad norm: 109.563 107.620 20.538
2024-12-02-14:04:47-root-INFO: grad norm: 88.008 85.137 22.296
2024-12-02-14:04:48-root-INFO: grad norm: 80.254 78.173 18.158
2024-12-02-14:04:49-root-INFO: grad norm: 74.769 71.840 20.722
2024-12-02-14:04:50-root-INFO: grad norm: 79.557 78.049 15.413
2024-12-02-14:04:51-root-INFO: Loss too large (457.998->465.838)! Learning rate decreased to 0.02471.
2024-12-02-14:04:52-root-INFO: grad norm: 61.021 59.581 13.180
2024-12-02-14:04:53-root-INFO: grad norm: 46.607 45.820 8.532
2024-12-02-14:04:53-root-INFO: Loss Change: 1029.205 -> 401.570
2024-12-02-14:04:53-root-INFO: Regularization Change: 0.000 -> 83.443
2024-12-02-14:04:53-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-14:04:53-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-14:04:54-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-14:04:54-root-INFO: grad norm: 42.549 41.889 7.463
2024-12-02-14:04:55-root-INFO: Loss too large (398.450->403.023)! Learning rate decreased to 0.02553.
2024-12-02-14:04:56-root-INFO: grad norm: 50.598 50.035 7.526
2024-12-02-14:04:56-root-INFO: Loss too large (390.728->392.861)! Learning rate decreased to 0.02043.
2024-12-02-14:04:57-root-INFO: grad norm: 37.702 37.094 6.748
2024-12-02-14:04:58-root-INFO: grad norm: 24.129 23.589 5.078
2024-12-02-14:04:59-root-INFO: grad norm: 23.221 22.656 5.089
2024-12-02-14:05:00-root-INFO: grad norm: 25.462 24.985 4.906
2024-12-02-14:05:01-root-INFO: grad norm: 26.704 26.231 4.999
2024-12-02-14:05:02-root-INFO: grad norm: 29.285 28.855 5.003
2024-12-02-14:05:03-root-INFO: Loss Change: 398.450 -> 355.625
2024-12-02-14:05:03-root-INFO: Regularization Change: 0.000 -> 6.288
2024-12-02-14:05:03-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-14:05:03-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-14:05:03-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-14:05:03-root-INFO: grad norm: 33.635 33.122 5.852
2024-12-02-14:05:04-root-INFO: Loss too large (356.381->367.111)! Learning rate decreased to 0.02639.
2024-12-02-14:05:05-root-INFO: grad norm: 49.674 49.223 6.677
2024-12-02-14:05:05-root-INFO: Loss too large (356.321->364.956)! Learning rate decreased to 0.02111.
2024-12-02-14:05:06-root-INFO: grad norm: 37.994 37.600 5.457
2024-12-02-14:05:07-root-INFO: grad norm: 24.444 24.029 4.486
2024-12-02-14:05:08-root-INFO: grad norm: 24.459 24.150 3.873
2024-12-02-14:05:09-root-INFO: grad norm: 27.020 26.656 4.415
2024-12-02-14:05:10-root-INFO: grad norm: 29.054 28.757 4.144
2024-12-02-14:05:11-root-INFO: grad norm: 32.434 32.086 4.737
2024-12-02-14:05:11-root-INFO: Loss too large (335.867->336.144)! Learning rate decreased to 0.01689.
2024-12-02-14:05:12-root-INFO: Loss Change: 356.381 -> 332.926
2024-12-02-14:05:12-root-INFO: Regularization Change: 0.000 -> 3.377
2024-12-02-14:05:12-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-14:05:12-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-14:05:12-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-14:05:13-root-INFO: grad norm: 29.448 29.070 4.703
2024-12-02-14:05:13-root-INFO: Loss too large (334.110->347.698)! Learning rate decreased to 0.02726.
2024-12-02-14:05:13-root-INFO: Loss too large (334.110->337.886)! Learning rate decreased to 0.02181.
2024-12-02-14:05:14-root-INFO: grad norm: 33.293 32.980 4.558
2024-12-02-14:05:15-root-INFO: Loss too large (332.525->333.025)! Learning rate decreased to 0.01745.
2024-12-02-14:05:16-root-INFO: grad norm: 25.505 25.221 3.794
2024-12-02-14:05:17-root-INFO: grad norm: 19.399 19.100 3.395
2024-12-02-14:05:18-root-INFO: grad norm: 16.659 16.377 3.049
2024-12-02-14:05:19-root-INFO: grad norm: 14.532 14.230 2.949
2024-12-02-14:05:20-root-INFO: grad norm: 13.289 12.998 2.763
2024-12-02-14:05:21-root-INFO: grad norm: 12.261 11.957 2.713
2024-12-02-14:05:21-root-INFO: Loss Change: 334.110 -> 318.551
2024-12-02-14:05:21-root-INFO: Regularization Change: 0.000 -> 1.718
2024-12-02-14:05:21-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-14:05:21-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-14:05:22-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-14:05:22-root-INFO: grad norm: 16.794 16.386 3.681
2024-12-02-14:05:22-root-INFO: Loss too large (319.346->321.476)! Learning rate decreased to 0.02816.
2024-12-02-14:05:23-root-INFO: grad norm: 26.386 26.108 3.818
2024-12-02-14:05:24-root-INFO: Loss too large (319.104->323.957)! Learning rate decreased to 0.02253.
2024-12-02-14:05:24-root-INFO: Loss too large (319.104->319.655)! Learning rate decreased to 0.01802.
2024-12-02-14:05:25-root-INFO: grad norm: 21.912 21.665 3.279
2024-12-02-14:05:26-root-INFO: grad norm: 18.413 18.170 2.982
2024-12-02-14:05:27-root-INFO: grad norm: 16.501 16.265 2.779
2024-12-02-14:05:28-root-INFO: grad norm: 14.859 14.618 2.669
2024-12-02-14:05:29-root-INFO: grad norm: 13.832 13.598 2.537
2024-12-02-14:05:30-root-INFO: grad norm: 12.913 12.672 2.486
2024-12-02-14:05:31-root-INFO: Loss Change: 319.346 -> 309.005
2024-12-02-14:05:31-root-INFO: Regularization Change: 0.000 -> 1.497
2024-12-02-14:05:31-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-14:05:31-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-14:05:31-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-14:05:31-root-INFO: grad norm: 19.601 19.178 4.048
2024-12-02-14:05:32-root-INFO: Loss too large (309.705->315.044)! Learning rate decreased to 0.02909.
2024-12-02-14:05:32-root-INFO: Loss too large (309.705->310.880)! Learning rate decreased to 0.02327.
2024-12-02-14:05:33-root-INFO: grad norm: 22.792 22.569 3.175
2024-12-02-14:05:33-root-INFO: Loss too large (308.649->309.002)! Learning rate decreased to 0.01862.
2024-12-02-14:05:34-root-INFO: grad norm: 19.408 19.185 2.937
2024-12-02-14:05:35-root-INFO: grad norm: 16.898 16.685 2.675
2024-12-02-14:05:36-root-INFO: grad norm: 15.334 15.126 2.517
2024-12-02-14:05:37-root-INFO: grad norm: 14.046 13.831 2.449
2024-12-02-14:05:38-root-INFO: grad norm: 13.192 12.987 2.315
2024-12-02-14:05:40-root-INFO: grad norm: 12.445 12.229 2.309
2024-12-02-14:05:40-root-INFO: Loss Change: 309.705 -> 300.287
2024-12-02-14:05:40-root-INFO: Regularization Change: 0.000 -> 1.252
2024-12-02-14:05:40-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-14:05:40-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-14:05:41-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-14:05:41-root-INFO: grad norm: 22.735 22.206 4.876
2024-12-02-14:05:41-root-INFO: Loss too large (302.142->309.721)! Learning rate decreased to 0.03019.
2024-12-02-14:05:42-root-INFO: Loss too large (302.142->303.983)! Learning rate decreased to 0.02415.
2024-12-02-14:05:43-root-INFO: grad norm: 24.807 24.590 3.272
2024-12-02-14:05:43-root-INFO: Loss too large (300.871->301.314)! Learning rate decreased to 0.01932.
2024-12-02-14:05:44-root-INFO: grad norm: 20.313 20.096 2.964
2024-12-02-14:05:45-root-INFO: grad norm: 17.636 17.441 2.612
2024-12-02-14:05:46-root-INFO: grad norm: 15.935 15.744 2.462
2024-12-02-14:05:47-root-INFO: grad norm: 14.722 14.526 2.393
2024-12-02-14:05:48-root-INFO: grad norm: 13.906 13.721 2.261
2024-12-02-14:05:49-root-INFO: grad norm: 13.249 13.052 2.272
2024-12-02-14:05:50-root-INFO: Loss Change: 302.142 -> 292.509
2024-12-02-14:05:50-root-INFO: Regularization Change: 0.000 -> 1.222
2024-12-02-14:05:50-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-14:05:50-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-14:05:50-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-14:05:50-root-INFO: grad norm: 17.214 16.882 3.366
2024-12-02-14:05:51-root-INFO: Loss too large (293.071->298.520)! Learning rate decreased to 0.03117.
2024-12-02-14:05:51-root-INFO: Loss too large (293.071->294.725)! Learning rate decreased to 0.02494.
2024-12-02-14:05:52-root-INFO: grad norm: 21.395 21.217 2.756
2024-12-02-14:05:52-root-INFO: Loss too large (292.645->293.451)! Learning rate decreased to 0.01995.
2024-12-02-14:05:53-root-INFO: grad norm: 18.982 18.787 2.719
2024-12-02-14:05:54-root-INFO: grad norm: 17.167 16.990 2.457
2024-12-02-14:05:55-root-INFO: grad norm: 15.923 15.746 2.373
2024-12-02-14:05:56-root-INFO: grad norm: 14.895 14.717 2.297
2024-12-02-14:05:57-root-INFO: grad norm: 14.167 13.997 2.193
2024-12-02-14:05:58-root-INFO: grad norm: 13.533 13.355 2.192
2024-12-02-14:05:59-root-INFO: Loss Change: 293.071 -> 285.906
2024-12-02-14:05:59-root-INFO: Regularization Change: 0.000 -> 1.067
2024-12-02-14:05:59-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-14:05:59-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-14:05:59-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-14:06:00-root-INFO: grad norm: 17.321 16.998 3.330
2024-12-02-14:06:00-root-INFO: Loss too large (286.590->292.306)! Learning rate decreased to 0.03218.
2024-12-02-14:06:00-root-INFO: Loss too large (286.590->288.344)! Learning rate decreased to 0.02574.
2024-12-02-14:06:01-root-INFO: grad norm: 21.038 20.865 2.693
2024-12-02-14:06:02-root-INFO: Loss too large (286.170->286.950)! Learning rate decreased to 0.02059.
2024-12-02-14:06:03-root-INFO: grad norm: 18.394 18.212 2.579
2024-12-02-14:06:04-root-INFO: grad norm: 16.450 16.280 2.356
2024-12-02-14:06:05-root-INFO: grad norm: 15.090 14.926 2.218
2024-12-02-14:06:06-root-INFO: grad norm: 13.998 13.827 2.179
2024-12-02-14:06:07-root-INFO: grad norm: 13.212 13.054 2.035
2024-12-02-14:06:08-root-INFO: grad norm: 12.550 12.379 2.066
2024-12-02-14:06:08-root-INFO: Loss Change: 286.590 -> 279.686
2024-12-02-14:06:08-root-INFO: Regularization Change: 0.000 -> 1.022
2024-12-02-14:06:08-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-14:06:08-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-14:06:09-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-14:06:09-root-INFO: grad norm: 16.697 16.392 3.176
2024-12-02-14:06:09-root-INFO: Loss too large (280.309->285.899)! Learning rate decreased to 0.03321.
2024-12-02-14:06:10-root-INFO: Loss too large (280.309->282.070)! Learning rate decreased to 0.02657.
2024-12-02-14:06:11-root-INFO: grad norm: 20.497 20.334 2.582
2024-12-02-14:06:11-root-INFO: Loss too large (279.961->280.873)! Learning rate decreased to 0.02126.
2024-12-02-14:06:12-root-INFO: grad norm: 18.237 18.064 2.507
2024-12-02-14:06:13-root-INFO: grad norm: 16.583 16.420 2.318
2024-12-02-14:06:14-root-INFO: grad norm: 15.392 15.235 2.195
2024-12-02-14:06:15-root-INFO: grad norm: 14.424 14.260 2.169
2024-12-02-14:06:16-root-INFO: grad norm: 13.705 13.554 2.027
2024-12-02-14:06:17-root-INFO: grad norm: 13.092 12.928 2.067
2024-12-02-14:06:18-root-INFO: Loss Change: 280.309 -> 273.957
2024-12-02-14:06:18-root-INFO: Regularization Change: 0.000 -> 0.988
2024-12-02-14:06:18-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-14:06:18-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-14:06:18-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-14:06:18-root-INFO: grad norm: 17.785 17.488 3.235
2024-12-02-14:06:19-root-INFO: Loss too large (275.062->282.305)! Learning rate decreased to 0.03427.
2024-12-02-14:06:19-root-INFO: Loss too large (275.062->277.606)! Learning rate decreased to 0.02742.
2024-12-02-14:06:20-root-INFO: grad norm: 22.211 22.048 2.681
2024-12-02-14:06:20-root-INFO: Loss too large (274.976->276.309)! Learning rate decreased to 0.02194.
2024-12-02-14:06:21-root-INFO: grad norm: 19.610 19.438 2.592
2024-12-02-14:06:22-root-INFO: grad norm: 17.601 17.442 2.361
2024-12-02-14:06:23-root-INFO: grad norm: 16.158 16.006 2.213
2024-12-02-14:06:24-root-INFO: grad norm: 14.966 14.807 2.171
2024-12-02-14:06:25-root-INFO: grad norm: 14.088 13.944 2.008
2024-12-02-14:06:26-root-INFO: grad norm: 13.340 13.183 2.044
2024-12-02-14:06:27-root-INFO: Loss Change: 275.062 -> 268.745
2024-12-02-14:06:27-root-INFO: Regularization Change: 0.000 -> 0.974
2024-12-02-14:06:27-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-14:06:27-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-14:06:27-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-14:06:28-root-INFO: grad norm: 19.245 18.874 3.758
2024-12-02-14:06:28-root-INFO: Loss too large (269.607->278.204)! Learning rate decreased to 0.03536.
2024-12-02-14:06:29-root-INFO: Loss too large (269.607->272.603)! Learning rate decreased to 0.02829.
2024-12-02-14:06:30-root-INFO: grad norm: 23.033 22.877 2.675
2024-12-02-14:06:30-root-INFO: Loss too large (269.475->270.715)! Learning rate decreased to 0.02263.
2024-12-02-14:06:31-root-INFO: grad norm: 19.584 19.411 2.601
2024-12-02-14:06:32-root-INFO: grad norm: 17.147 17.002 2.224
2024-12-02-14:06:33-root-INFO: grad norm: 15.362 15.216 2.114
2024-12-02-14:06:34-root-INFO: grad norm: 14.005 13.861 2.004
2024-12-02-14:06:35-root-INFO: grad norm: 12.990 12.854 1.874
2024-12-02-14:06:36-root-INFO: grad norm: 12.181 12.036 1.870
2024-12-02-14:06:37-root-INFO: Loss Change: 269.607 -> 262.832
2024-12-02-14:06:37-root-INFO: Regularization Change: 0.000 -> 0.985
2024-12-02-14:06:37-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-14:06:37-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-14:06:37-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-14:06:37-root-INFO: grad norm: 13.164 12.948 2.376
2024-12-02-14:06:38-root-INFO: Loss too large (263.923->267.491)! Learning rate decreased to 0.03648.
2024-12-02-14:06:38-root-INFO: Loss too large (263.923->264.967)! Learning rate decreased to 0.02919.
2024-12-02-14:06:39-root-INFO: grad norm: 16.284 16.145 2.128
2024-12-02-14:06:39-root-INFO: Loss too large (263.587->264.037)! Learning rate decreased to 0.02335.
2024-12-02-14:06:40-root-INFO: grad norm: 14.568 14.424 2.041
2024-12-02-14:06:41-root-INFO: grad norm: 13.243 13.100 1.943
2024-12-02-14:06:42-root-INFO: grad norm: 12.211 12.077 1.805
2024-12-02-14:06:43-root-INFO: grad norm: 11.375 11.230 1.810
2024-12-02-14:06:44-root-INFO: grad norm: 10.702 10.571 1.666
2024-12-02-14:06:45-root-INFO: grad norm: 10.142 9.996 1.715
2024-12-02-14:06:46-root-INFO: Loss Change: 263.923 -> 258.554
2024-12-02-14:06:46-root-INFO: Regularization Change: 0.000 -> 0.934
2024-12-02-14:06:46-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-14:06:46-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-14:06:46-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-14:06:47-root-INFO: grad norm: 17.164 16.836 3.336
2024-12-02-14:06:47-root-INFO: Loss too large (259.820->266.383)! Learning rate decreased to 0.03763.
2024-12-02-14:06:47-root-INFO: Loss too large (259.820->261.917)! Learning rate decreased to 0.03011.
2024-12-02-14:06:48-root-INFO: grad norm: 19.837 19.698 2.343
2024-12-02-14:06:49-root-INFO: Loss too large (259.443->260.194)! Learning rate decreased to 0.02409.
2024-12-02-14:06:50-root-INFO: grad norm: 16.744 16.590 2.271
2024-12-02-14:06:51-root-INFO: grad norm: 14.690 14.561 1.943
2024-12-02-14:06:52-root-INFO: grad norm: 13.111 12.980 1.855
2024-12-02-14:06:53-root-INFO: grad norm: 11.936 11.805 1.764
2024-12-02-14:06:54-root-INFO: grad norm: 11.013 10.888 1.652
2024-12-02-14:06:55-root-INFO: grad norm: 10.299 10.165 1.655
2024-12-02-14:06:55-root-INFO: Loss Change: 259.820 -> 253.569
2024-12-02-14:06:55-root-INFO: Regularization Change: 0.000 -> 0.972
2024-12-02-14:06:55-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-14:06:56-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-14:06:56-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-14:06:56-root-INFO: grad norm: 14.622 14.370 2.705
2024-12-02-14:06:57-root-INFO: Loss too large (254.205->259.389)! Learning rate decreased to 0.03881.
2024-12-02-14:06:57-root-INFO: Loss too large (254.205->255.911)! Learning rate decreased to 0.03105.
2024-12-02-14:06:58-root-INFO: grad norm: 17.680 17.552 2.122
2024-12-02-14:06:58-root-INFO: Loss too large (253.990->254.671)! Learning rate decreased to 0.02484.
2024-12-02-14:06:59-root-INFO: grad norm: 15.363 15.221 2.080
2024-12-02-14:07:00-root-INFO: grad norm: 13.673 13.548 1.844
2024-12-02-14:07:01-root-INFO: grad norm: 12.343 12.218 1.751
2024-12-02-14:07:02-root-INFO: grad norm: 11.323 11.196 1.690
2024-12-02-14:07:03-root-INFO: grad norm: 10.501 10.382 1.575
2024-12-02-14:07:04-root-INFO: grad norm: 9.849 9.721 1.587
2024-12-02-14:07:05-root-INFO: Loss Change: 254.205 -> 248.756
2024-12-02-14:07:05-root-INFO: Regularization Change: 0.000 -> 0.937
2024-12-02-14:07:05-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-14:07:05-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-14:07:05-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-14:07:06-root-INFO: grad norm: 14.290 14.057 2.572
2024-12-02-14:07:06-root-INFO: Loss too large (249.790->254.707)! Learning rate decreased to 0.04002.
2024-12-02-14:07:06-root-INFO: Loss too large (249.790->251.372)! Learning rate decreased to 0.03202.
2024-12-02-14:07:07-root-INFO: grad norm: 16.964 16.841 2.038
2024-12-02-14:07:08-root-INFO: Loss too large (249.528->250.055)! Learning rate decreased to 0.02561.
2024-12-02-14:07:09-root-INFO: grad norm: 14.501 14.370 1.944
2024-12-02-14:07:10-root-INFO: grad norm: 12.803 12.687 1.726
2024-12-02-14:07:11-root-INFO: grad norm: 11.476 11.360 1.627
2024-12-02-14:07:12-root-INFO: grad norm: 10.487 10.367 1.576
2024-12-02-14:07:13-root-INFO: grad norm: 9.695 9.583 1.469
2024-12-02-14:07:14-root-INFO: grad norm: 9.083 8.961 1.482
2024-12-02-14:07:14-root-INFO: Loss Change: 249.790 -> 244.414
2024-12-02-14:07:14-root-INFO: Regularization Change: 0.000 -> 0.941
2024-12-02-14:07:14-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-14:07:14-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-14:07:15-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-14:07:15-root-INFO: grad norm: 13.175 12.980 2.259
2024-12-02-14:07:15-root-INFO: Loss too large (245.260->249.663)! Learning rate decreased to 0.04126.
2024-12-02-14:07:16-root-INFO: Loss too large (245.260->246.675)! Learning rate decreased to 0.03301.
2024-12-02-14:07:17-root-INFO: grad norm: 15.840 15.727 1.888
2024-12-02-14:07:17-root-INFO: Loss too large (245.034->245.481)! Learning rate decreased to 0.02641.
2024-12-02-14:07:18-root-INFO: grad norm: 13.532 13.408 1.829
2024-12-02-14:07:19-root-INFO: grad norm: 11.802 11.694 1.598
2024-12-02-14:07:20-root-INFO: grad norm: 10.423 10.310 1.532
2024-12-02-14:07:21-root-INFO: grad norm: 9.377 9.266 1.442
2024-12-02-14:07:22-root-INFO: grad norm: 8.530 8.419 1.371
2024-12-02-14:07:23-root-INFO: grad norm: 7.874 7.758 1.343
2024-12-02-14:07:24-root-INFO: Loss Change: 245.260 -> 240.133
2024-12-02-14:07:24-root-INFO: Regularization Change: 0.000 -> 0.928
2024-12-02-14:07:24-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-14:07:24-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-14:07:24-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-14:07:24-root-INFO: grad norm: 12.252 12.020 2.376
2024-12-02-14:07:25-root-INFO: Loss too large (240.797->244.105)! Learning rate decreased to 0.04253.
2024-12-02-14:07:25-root-INFO: Loss too large (240.797->241.702)! Learning rate decreased to 0.03403.
2024-12-02-14:07:26-root-INFO: grad norm: 14.083 13.976 1.734
2024-12-02-14:07:26-root-INFO: Loss too large (240.386->240.535)! Learning rate decreased to 0.02722.
2024-12-02-14:07:27-root-INFO: grad norm: 11.839 11.716 1.698
2024-12-02-14:07:28-root-INFO: grad norm: 10.302 10.201 1.440
2024-12-02-14:07:29-root-INFO: grad norm: 9.064 8.954 1.410
2024-12-02-14:07:30-root-INFO: grad norm: 8.159 8.052 1.314
2024-12-02-14:07:31-root-INFO: grad norm: 7.432 7.323 1.269
2024-12-02-14:07:32-root-INFO: grad norm: 6.889 6.776 1.239
2024-12-02-14:07:33-root-INFO: Loss Change: 240.797 -> 235.815
2024-12-02-14:07:33-root-INFO: Regularization Change: 0.000 -> 0.935
2024-12-02-14:07:33-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-14:07:33-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-14:07:33-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-14:07:34-root-INFO: grad norm: 13.140 12.894 2.532
2024-12-02-14:07:34-root-INFO: Loss too large (236.681->240.759)! Learning rate decreased to 0.04384.
2024-12-02-14:07:34-root-INFO: Loss too large (236.681->237.860)! Learning rate decreased to 0.03507.
2024-12-02-14:07:35-root-INFO: grad norm: 14.880 14.770 1.808
2024-12-02-14:07:36-root-INFO: Loss too large (236.264->236.466)! Learning rate decreased to 0.02806.
2024-12-02-14:07:37-root-INFO: grad norm: 12.241 12.118 1.727
2024-12-02-14:07:38-root-INFO: grad norm: 10.460 10.363 1.423
2024-12-02-14:07:39-root-INFO: grad norm: 9.049 8.941 1.393
2024-12-02-14:07:40-root-INFO: grad norm: 8.033 7.931 1.271
2024-12-02-14:07:41-root-INFO: grad norm: 7.232 7.126 1.235
2024-12-02-14:07:42-root-INFO: grad norm: 6.646 6.539 1.190
2024-12-02-14:07:42-root-INFO: Loss Change: 236.681 -> 231.479
2024-12-02-14:07:42-root-INFO: Regularization Change: 0.000 -> 0.962
2024-12-02-14:07:42-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-14:07:42-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-14:07:43-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-14:07:43-root-INFO: grad norm: 10.044 9.867 1.875
2024-12-02-14:07:43-root-INFO: Loss too large (231.890->234.073)! Learning rate decreased to 0.04517.
2024-12-02-14:07:44-root-INFO: Loss too large (231.890->232.401)! Learning rate decreased to 0.03614.
2024-12-02-14:07:45-root-INFO: grad norm: 11.727 11.633 1.485
2024-12-02-14:07:46-root-INFO: grad norm: 14.665 14.550 1.837
2024-12-02-14:07:46-root-INFO: Loss too large (231.473->231.958)! Learning rate decreased to 0.02891.
2024-12-02-14:07:47-root-INFO: grad norm: 12.401 12.307 1.528
2024-12-02-14:07:48-root-INFO: grad norm: 10.582 10.483 1.441
2024-12-02-14:07:49-root-INFO: grad norm: 9.253 9.160 1.312
2024-12-02-14:07:50-root-INFO: grad norm: 8.188 8.092 1.250
2024-12-02-14:07:51-root-INFO: grad norm: 7.397 7.300 1.197
2024-12-02-14:07:52-root-INFO: Loss Change: 231.890 -> 227.447
2024-12-02-14:07:52-root-INFO: Regularization Change: 0.000 -> 0.983
2024-12-02-14:07:52-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-14:07:52-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-14:07:52-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-14:07:52-root-INFO: grad norm: 11.334 11.152 2.024
2024-12-02-14:07:53-root-INFO: Loss too large (228.203->231.659)! Learning rate decreased to 0.04654.
2024-12-02-14:07:53-root-INFO: Loss too large (228.203->229.270)! Learning rate decreased to 0.03724.
2024-12-02-14:07:54-root-INFO: grad norm: 13.463 13.368 1.598
2024-12-02-14:07:55-root-INFO: Loss too large (227.953->228.190)! Learning rate decreased to 0.02979.
2024-12-02-14:07:56-root-INFO: grad norm: 11.368 11.261 1.557
2024-12-02-14:07:57-root-INFO: grad norm: 9.827 9.738 1.320
2024-12-02-14:07:57-root-INFO: grad norm: 8.554 8.456 1.288
2024-12-02-14:07:58-root-INFO: grad norm: 7.590 7.497 1.184
2024-12-02-14:07:59-root-INFO: grad norm: 6.799 6.701 1.149
2024-12-02-14:08:00-root-INFO: grad norm: 6.194 6.095 1.102
2024-12-02-14:08:01-root-INFO: Loss Change: 228.203 -> 223.582
2024-12-02-14:08:01-root-INFO: Regularization Change: 0.000 -> 0.947
2024-12-02-14:08:01-root-INFO: Undo step: 110
2024-12-02-14:08:01-root-INFO: Undo step: 111
2024-12-02-14:08:01-root-INFO: Undo step: 112
2024-12-02-14:08:01-root-INFO: Undo step: 113
2024-12-02-14:08:01-root-INFO: Undo step: 114
2024-12-02-14:08:01-root-INFO: Undo step: 115
2024-12-02-14:08:01-root-INFO: Undo step: 116
2024-12-02-14:08:01-root-INFO: Undo step: 117
2024-12-02-14:08:01-root-INFO: Undo step: 118
2024-12-02-14:08:01-root-INFO: Undo step: 119
2024-12-02-14:08:02-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-14:08:02-root-INFO: grad norm: 131.296 128.692 26.019
2024-12-02-14:08:03-root-INFO: grad norm: 89.800 88.495 15.253
2024-12-02-14:08:04-root-INFO: grad norm: 60.504 58.976 13.511
2024-12-02-14:08:05-root-INFO: grad norm: 54.809 53.998 9.391
2024-12-02-14:08:06-root-INFO: grad norm: 56.926 56.014 10.149
2024-12-02-14:08:07-root-INFO: grad norm: 59.729 59.134 8.414
2024-12-02-14:08:08-root-INFO: grad norm: 65.051 64.218 10.378
2024-12-02-14:08:08-root-INFO: Loss too large (360.688->367.571)! Learning rate decreased to 0.03427.
2024-12-02-14:08:09-root-INFO: grad norm: 53.699 53.247 6.951
2024-12-02-14:08:10-root-INFO: Loss Change: 880.856 -> 323.898
2024-12-02-14:08:10-root-INFO: Regularization Change: 0.000 -> 93.732
2024-12-02-14:08:10-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-14:08:10-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-14:08:10-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-14:08:11-root-INFO: grad norm: 43.652 43.180 6.403
2024-12-02-14:08:11-root-INFO: Loss too large (319.001->326.269)! Learning rate decreased to 0.03536.
2024-12-02-14:08:12-root-INFO: grad norm: 42.463 42.100 5.542
2024-12-02-14:08:13-root-INFO: grad norm: 43.975 43.479 6.583
2024-12-02-14:08:14-root-INFO: grad norm: 46.191 45.805 5.961
2024-12-02-14:08:15-root-INFO: grad norm: 47.224 46.694 7.057
2024-12-02-14:08:16-root-INFO: grad norm: 48.190 47.780 6.273
2024-12-02-14:08:16-root-INFO: Loss too large (297.694->297.877)! Learning rate decreased to 0.02829.
2024-12-02-14:08:17-root-INFO: grad norm: 33.267 32.839 5.319
2024-12-02-14:08:18-root-INFO: grad norm: 24.374 24.125 3.477
2024-12-02-14:08:19-root-INFO: Loss Change: 319.001 -> 273.695
2024-12-02-14:08:19-root-INFO: Regularization Change: 0.000 -> 7.910
2024-12-02-14:08:19-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-14:08:19-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-14:08:19-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-14:08:20-root-INFO: grad norm: 17.929 17.669 3.041
2024-12-02-14:08:20-root-INFO: Loss too large (273.024->274.512)! Learning rate decreased to 0.03648.
2024-12-02-14:08:21-root-INFO: grad norm: 22.445 22.222 3.156
2024-12-02-14:08:21-root-INFO: Loss too large (271.570->272.126)! Learning rate decreased to 0.02919.
2024-12-02-14:08:22-root-INFO: grad norm: 20.302 20.030 3.311
2024-12-02-14:08:23-root-INFO: grad norm: 18.684 18.480 2.749
2024-12-02-14:08:24-root-INFO: grad norm: 17.652 17.404 2.952
2024-12-02-14:08:25-root-INFO: grad norm: 16.980 16.792 2.522
2024-12-02-14:08:26-root-INFO: grad norm: 16.581 16.347 2.775
2024-12-02-14:08:27-root-INFO: grad norm: 16.465 16.286 2.421
2024-12-02-14:08:28-root-INFO: Loss Change: 273.024 -> 258.496
2024-12-02-14:08:28-root-INFO: Regularization Change: 0.000 -> 3.396
2024-12-02-14:08:28-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-14:08:28-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-14:08:29-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-14:08:29-root-INFO: grad norm: 13.127 12.860 2.633
2024-12-02-14:08:30-root-INFO: grad norm: 21.405 21.255 2.529
2024-12-02-14:08:30-root-INFO: Loss too large (256.888->264.024)! Learning rate decreased to 0.03763.
2024-12-02-14:08:31-root-INFO: Loss too large (256.888->258.622)! Learning rate decreased to 0.03011.
2024-12-02-14:08:32-root-INFO: grad norm: 19.988 19.784 2.845
2024-12-02-14:08:33-root-INFO: grad norm: 19.173 19.006 2.521
2024-12-02-14:08:34-root-INFO: grad norm: 18.796 18.593 2.752
2024-12-02-14:08:35-root-INFO: grad norm: 18.663 18.494 2.503
2024-12-02-14:08:36-root-INFO: grad norm: 18.753 18.549 2.756
2024-12-02-14:08:37-root-INFO: grad norm: 19.068 18.894 2.567
2024-12-02-14:08:37-root-INFO: Loss Change: 257.190 -> 248.645
2024-12-02-14:08:37-root-INFO: Regularization Change: 0.000 -> 2.674
2024-12-02-14:08:37-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-14:08:37-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-14:08:38-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-14:08:38-root-INFO: grad norm: 16.667 16.491 2.416
2024-12-02-14:08:38-root-INFO: Loss too large (247.385->251.548)! Learning rate decreased to 0.03881.
2024-12-02-14:08:39-root-INFO: Loss too large (247.385->247.770)! Learning rate decreased to 0.03105.
2024-12-02-14:08:40-root-INFO: grad norm: 16.137 15.973 2.293
2024-12-02-14:08:41-root-INFO: grad norm: 16.414 16.244 2.361
2024-12-02-14:08:42-root-INFO: grad norm: 17.068 16.903 2.367
2024-12-02-14:08:43-root-INFO: grad norm: 17.776 17.596 2.523
2024-12-02-14:08:44-root-INFO: grad norm: 18.783 18.610 2.548
2024-12-02-14:08:45-root-INFO: grad norm: 19.782 19.590 2.747
2024-12-02-14:08:46-root-INFO: grad norm: 21.033 20.846 2.798
2024-12-02-14:08:46-root-INFO: Loss too large (241.553->241.579)! Learning rate decreased to 0.02484.
2024-12-02-14:08:47-root-INFO: Loss Change: 247.385 -> 239.481
2024-12-02-14:08:47-root-INFO: Regularization Change: 0.000 -> 1.793
2024-12-02-14:08:47-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-14:08:47-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-14:08:47-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-14:08:47-root-INFO: grad norm: 10.613 10.456 1.816
2024-12-02-14:08:48-root-INFO: Loss too large (238.554->238.970)! Learning rate decreased to 0.04002.
2024-12-02-14:08:49-root-INFO: grad norm: 14.236 14.109 1.898
2024-12-02-14:08:49-root-INFO: Loss too large (237.876->238.939)! Learning rate decreased to 0.03202.
2024-12-02-14:08:50-root-INFO: grad norm: 14.820 14.678 2.046
2024-12-02-14:08:51-root-INFO: grad norm: 15.840 15.689 2.182
2024-12-02-14:08:52-root-INFO: grad norm: 16.999 16.840 2.318
2024-12-02-14:08:53-root-INFO: grad norm: 18.516 18.348 2.484
2024-12-02-14:08:53-root-INFO: Loss too large (235.709->235.804)! Learning rate decreased to 0.02561.
2024-12-02-14:08:54-root-INFO: grad norm: 13.633 13.492 1.953
2024-12-02-14:08:55-root-INFO: grad norm: 10.056 9.930 1.588
2024-12-02-14:08:56-root-INFO: Loss Change: 238.554 -> 231.964
2024-12-02-14:08:56-root-INFO: Regularization Change: 0.000 -> 1.507
2024-12-02-14:08:56-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-14:08:56-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-14:08:56-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-14:08:57-root-INFO: grad norm: 6.640 6.434 1.641
2024-12-02-14:08:58-root-INFO: grad norm: 7.191 7.084 1.237
2024-12-02-14:08:59-root-INFO: grad norm: 11.074 10.974 1.488
2024-12-02-14:08:59-root-INFO: Loss too large (229.983->231.792)! Learning rate decreased to 0.04126.
2024-12-02-14:08:59-root-INFO: Loss too large (229.983->230.078)! Learning rate decreased to 0.03301.
2024-12-02-14:09:00-root-INFO: grad norm: 11.202 11.114 1.396
2024-12-02-14:09:02-root-INFO: grad norm: 11.731 11.634 1.499
2024-12-02-14:09:03-root-INFO: grad norm: 12.542 12.452 1.504
2024-12-02-14:09:04-root-INFO: grad norm: 13.189 13.091 1.605
2024-12-02-14:09:05-root-INFO: grad norm: 14.016 13.921 1.626
2024-12-02-14:09:05-root-INFO: Loss Change: 231.735 -> 226.888
2024-12-02-14:09:05-root-INFO: Regularization Change: 0.000 -> 1.947
2024-12-02-14:09:05-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-14:09:05-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-14:09:06-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-14:09:06-root-INFO: grad norm: 14.833 14.684 2.102
2024-12-02-14:09:06-root-INFO: Loss too large (226.669->231.252)! Learning rate decreased to 0.04253.
2024-12-02-14:09:07-root-INFO: Loss too large (226.669->227.646)! Learning rate decreased to 0.03403.
2024-12-02-14:09:08-root-INFO: grad norm: 14.960 14.886 1.493
2024-12-02-14:09:09-root-INFO: grad norm: 15.812 15.725 1.652
2024-12-02-14:09:10-root-INFO: grad norm: 16.858 16.788 1.540
2024-12-02-14:09:11-root-INFO: grad norm: 18.022 17.937 1.746
2024-12-02-14:09:12-root-INFO: grad norm: 19.358 19.286 1.666
2024-12-02-14:09:12-root-INFO: Loss too large (224.497->224.573)! Learning rate decreased to 0.02722.
2024-12-02-14:09:13-root-INFO: grad norm: 13.540 13.462 1.457
2024-12-02-14:09:14-root-INFO: grad norm: 9.668 9.596 1.176
2024-12-02-14:09:15-root-INFO: Loss Change: 226.669 -> 220.549
2024-12-02-14:09:15-root-INFO: Regularization Change: 0.000 -> 1.184
2024-12-02-14:09:15-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-14:09:15-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-14:09:15-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-14:09:15-root-INFO: grad norm: 14.646 14.374 2.814
2024-12-02-14:09:16-root-INFO: Loss too large (221.503->226.959)! Learning rate decreased to 0.04384.
2024-12-02-14:09:16-root-INFO: Loss too large (221.503->223.209)! Learning rate decreased to 0.03507.
2024-12-02-14:09:17-root-INFO: grad norm: 16.370 16.258 1.912
2024-12-02-14:09:17-root-INFO: Loss too large (221.126->221.385)! Learning rate decreased to 0.02806.
2024-12-02-14:09:18-root-INFO: grad norm: 12.878 12.749 1.821
2024-12-02-14:09:19-root-INFO: grad norm: 10.490 10.399 1.378
2024-12-02-14:09:20-root-INFO: grad norm: 8.734 8.622 1.394
2024-12-02-14:09:21-root-INFO: grad norm: 7.478 7.388 1.155
2024-12-02-14:09:22-root-INFO: grad norm: 6.532 6.424 1.186
2024-12-02-14:09:23-root-INFO: grad norm: 5.849 5.755 1.042
2024-12-02-14:09:24-root-INFO: Loss Change: 221.503 -> 215.999
2024-12-02-14:09:24-root-INFO: Regularization Change: 0.000 -> 0.942
2024-12-02-14:09:24-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-14:09:24-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-14:09:24-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-14:09:25-root-INFO: grad norm: 10.470 10.270 2.035
2024-12-02-14:09:25-root-INFO: Loss too large (216.426->219.066)! Learning rate decreased to 0.04517.
2024-12-02-14:09:25-root-INFO: Loss too large (216.426->217.176)! Learning rate decreased to 0.03614.
2024-12-02-14:09:26-root-INFO: grad norm: 12.308 12.207 1.577
2024-12-02-14:09:27-root-INFO: Loss too large (216.137->216.185)! Learning rate decreased to 0.02891.
2024-12-02-14:09:28-root-INFO: grad norm: 10.262 10.142 1.562
2024-12-02-14:09:29-root-INFO: grad norm: 8.790 8.702 1.243
2024-12-02-14:09:30-root-INFO: grad norm: 7.596 7.490 1.263
2024-12-02-14:09:31-root-INFO: grad norm: 6.720 6.632 1.082
2024-12-02-14:09:32-root-INFO: grad norm: 6.011 5.908 1.106
2024-12-02-14:09:33-root-INFO: grad norm: 5.487 5.397 0.993
2024-12-02-14:09:33-root-INFO: Loss Change: 216.426 -> 212.116
2024-12-02-14:09:33-root-INFO: Regularization Change: 0.000 -> 0.874
2024-12-02-14:09:33-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-14:09:33-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-14:09:34-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-14:09:34-root-INFO: grad norm: 10.740 10.522 2.155
2024-12-02-14:09:34-root-INFO: Loss too large (212.895->215.938)! Learning rate decreased to 0.04654.
2024-12-02-14:09:35-root-INFO: Loss too large (212.895->213.839)! Learning rate decreased to 0.03724.
2024-12-02-14:09:36-root-INFO: grad norm: 12.826 12.724 1.612
2024-12-02-14:09:36-root-INFO: Loss too large (212.673->212.901)! Learning rate decreased to 0.02979.
2024-12-02-14:09:37-root-INFO: grad norm: 10.965 10.842 1.635
2024-12-02-14:09:38-root-INFO: grad norm: 9.597 9.510 1.286
2024-12-02-14:09:39-root-INFO: grad norm: 8.408 8.302 1.332
2024-12-02-14:09:40-root-INFO: grad norm: 7.509 7.425 1.117
2024-12-02-14:09:41-root-INFO: grad norm: 6.736 6.636 1.159
2024-12-02-14:09:42-root-INFO: grad norm: 6.144 6.060 1.013
2024-12-02-14:09:43-root-INFO: Loss Change: 212.895 -> 208.684
2024-12-02-14:09:43-root-INFO: Regularization Change: 0.000 -> 0.870
2024-12-02-14:09:43-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-14:09:43-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-14:09:43-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-14:09:43-root-INFO: grad norm: 12.841 12.608 2.437
2024-12-02-14:09:44-root-INFO: Loss too large (209.904->215.219)! Learning rate decreased to 0.04795.
2024-12-02-14:09:44-root-INFO: Loss too large (209.904->211.888)! Learning rate decreased to 0.03836.
2024-12-02-14:09:44-root-INFO: Loss too large (209.904->209.984)! Learning rate decreased to 0.03069.
2024-12-02-14:09:45-root-INFO: grad norm: 10.747 10.660 1.369
2024-12-02-14:09:46-root-INFO: grad norm: 9.359 9.242 1.474
2024-12-02-14:09:47-root-INFO: grad norm: 8.325 8.244 1.162
2024-12-02-14:09:48-root-INFO: grad norm: 7.419 7.317 1.222
2024-12-02-14:09:49-root-INFO: grad norm: 6.720 6.640 1.037
2024-12-02-14:09:50-root-INFO: grad norm: 6.113 6.016 1.084
2024-12-02-14:09:51-root-INFO: grad norm: 5.639 5.558 0.958
2024-12-02-14:09:52-root-INFO: Loss Change: 209.904 -> 205.416
2024-12-02-14:09:52-root-INFO: Regularization Change: 0.000 -> 0.835
2024-12-02-14:09:52-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-14:09:52-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-14:09:52-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-14:09:53-root-INFO: grad norm: 10.122 9.936 1.928
2024-12-02-14:09:53-root-INFO: Loss too large (205.759->208.829)! Learning rate decreased to 0.04939.
2024-12-02-14:09:53-root-INFO: Loss too large (205.759->206.779)! Learning rate decreased to 0.03951.
2024-12-02-14:09:54-root-INFO: grad norm: 12.304 12.209 1.519
2024-12-02-14:09:55-root-INFO: Loss too large (205.634->205.914)! Learning rate decreased to 0.03161.
2024-12-02-14:09:56-root-INFO: grad norm: 10.586 10.474 1.536
2024-12-02-14:09:57-root-INFO: grad norm: 9.308 9.230 1.209
2024-12-02-14:09:58-root-INFO: grad norm: 8.186 8.088 1.263
2024-12-02-14:09:59-root-INFO: grad norm: 7.327 7.251 1.051
2024-12-02-14:10:00-root-INFO: grad norm: 6.579 6.486 1.103
2024-12-02-14:10:01-root-INFO: grad norm: 5.999 5.923 0.953
2024-12-02-14:10:02-root-INFO: Loss Change: 205.759 -> 201.878
2024-12-02-14:10:02-root-INFO: Regularization Change: 0.000 -> 0.855
2024-12-02-14:10:02-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-14:10:02-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-14:10:02-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-14:10:02-root-INFO: grad norm: 14.440 14.204 2.602
2024-12-02-14:10:03-root-INFO: Loss too large (203.497->211.176)! Learning rate decreased to 0.05086.
2024-12-02-14:10:03-root-INFO: Loss too large (203.497->206.573)! Learning rate decreased to 0.04069.
2024-12-02-14:10:03-root-INFO: Loss too large (203.497->203.896)! Learning rate decreased to 0.03255.
2024-12-02-14:10:04-root-INFO: grad norm: 12.163 12.078 1.427
2024-12-02-14:10:05-root-INFO: grad norm: 10.658 10.547 1.536
2024-12-02-14:10:06-root-INFO: grad norm: 9.527 9.453 1.186
2024-12-02-14:10:07-root-INFO: grad norm: 8.502 8.408 1.263
2024-12-02-14:10:08-root-INFO: grad norm: 7.693 7.622 1.042
2024-12-02-14:10:09-root-INFO: grad norm: 6.971 6.882 1.107
2024-12-02-14:10:10-root-INFO: grad norm: 6.393 6.322 0.946
2024-12-02-14:10:11-root-INFO: Loss Change: 203.497 -> 198.761
2024-12-02-14:10:11-root-INFO: Regularization Change: 0.000 -> 0.853
2024-12-02-14:10:11-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-14:10:11-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-14:10:11-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-14:10:11-root-INFO: grad norm: 12.730 12.514 2.332
2024-12-02-14:10:12-root-INFO: Loss too large (199.673->205.954)! Learning rate decreased to 0.05237.
2024-12-02-14:10:12-root-INFO: Loss too large (199.673->202.161)! Learning rate decreased to 0.04190.
2024-12-02-14:10:12-root-INFO: Loss too large (199.673->199.980)! Learning rate decreased to 0.03352.
2024-12-02-14:10:13-root-INFO: grad norm: 11.065 10.983 1.341
2024-12-02-14:10:14-root-INFO: grad norm: 10.032 9.919 1.501
2024-12-02-14:10:15-root-INFO: grad norm: 9.217 9.144 1.151
2024-12-02-14:10:16-root-INFO: grad norm: 8.473 8.377 1.271
2024-12-02-14:10:17-root-INFO: grad norm: 7.861 7.793 1.038
2024-12-02-14:10:18-root-INFO: grad norm: 7.301 7.212 1.135
2024-12-02-14:10:19-root-INFO: grad norm: 6.835 6.767 0.957
2024-12-02-14:10:20-root-INFO: Loss Change: 199.673 -> 195.454
2024-12-02-14:10:20-root-INFO: Regularization Change: 0.000 -> 0.848
2024-12-02-14:10:20-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-14:10:20-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-14:10:20-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-14:10:21-root-INFO: grad norm: 12.356 12.170 2.140
2024-12-02-14:10:21-root-INFO: Loss too large (196.725->203.114)! Learning rate decreased to 0.05391.
2024-12-02-14:10:21-root-INFO: Loss too large (196.725->199.344)! Learning rate decreased to 0.04313.
2024-12-02-14:10:22-root-INFO: Loss too large (196.725->197.159)! Learning rate decreased to 0.03450.
2024-12-02-14:10:23-root-INFO: grad norm: 10.985 10.913 1.259
2024-12-02-14:10:24-root-INFO: grad norm: 10.051 9.948 1.437
2024-12-02-14:10:25-root-INFO: grad norm: 9.292 9.225 1.114
2024-12-02-14:10:26-root-INFO: grad norm: 8.587 8.497 1.237
2024-12-02-14:10:27-root-INFO: grad norm: 7.994 7.930 1.015
2024-12-02-14:10:28-root-INFO: grad norm: 7.446 7.362 1.114
2024-12-02-14:10:29-root-INFO: grad norm: 6.982 6.918 0.940
2024-12-02-14:10:29-root-INFO: Loss Change: 196.725 -> 192.721
2024-12-02-14:10:29-root-INFO: Regularization Change: 0.000 -> 0.838
2024-12-02-14:10:29-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-14:10:29-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-14:10:30-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-14:10:30-root-INFO: grad norm: 13.558 13.353 2.348
2024-12-02-14:10:30-root-INFO: Loss too large (194.010->201.941)! Learning rate decreased to 0.05550.
2024-12-02-14:10:31-root-INFO: Loss too large (194.010->197.302)! Learning rate decreased to 0.04440.
2024-12-02-14:10:31-root-INFO: Loss too large (194.010->194.589)! Learning rate decreased to 0.03552.
2024-12-02-14:10:32-root-INFO: grad norm: 11.795 11.720 1.330
2024-12-02-14:10:33-root-INFO: grad norm: 10.667 10.565 1.472
2024-12-02-14:10:34-root-INFO: grad norm: 9.770 9.705 1.128
2024-12-02-14:10:35-root-INFO: grad norm: 8.947 8.860 1.247
2024-12-02-14:10:36-root-INFO: grad norm: 8.263 8.201 1.008
2024-12-02-14:10:37-root-INFO: grad norm: 7.638 7.557 1.112
2024-12-02-14:10:38-root-INFO: grad norm: 7.113 7.053 0.923
2024-12-02-14:10:39-root-INFO: Loss Change: 194.010 -> 189.704
2024-12-02-14:10:39-root-INFO: Regularization Change: 0.000 -> 0.860
2024-12-02-14:10:39-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-14:10:39-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-14:10:39-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-14:10:39-root-INFO: grad norm: 12.080 11.930 1.896
2024-12-02-14:10:40-root-INFO: Loss too large (190.684->197.335)! Learning rate decreased to 0.05711.
2024-12-02-14:10:40-root-INFO: Loss too large (190.684->193.446)! Learning rate decreased to 0.04569.
2024-12-02-14:10:40-root-INFO: Loss too large (190.684->191.187)! Learning rate decreased to 0.03655.
2024-12-02-14:10:41-root-INFO: grad norm: 10.670 10.597 1.243
2024-12-02-14:10:42-root-INFO: grad norm: 9.655 9.565 1.316
2024-12-02-14:10:43-root-INFO: grad norm: 8.833 8.769 1.063
2024-12-02-14:10:44-root-INFO: grad norm: 8.094 8.014 1.132
2024-12-02-14:10:45-root-INFO: grad norm: 7.477 7.416 0.952
2024-12-02-14:10:46-root-INFO: grad norm: 6.921 6.846 1.018
2024-12-02-14:10:47-root-INFO: grad norm: 6.453 6.393 0.875
2024-12-02-14:10:48-root-INFO: Loss Change: 190.684 -> 186.760
2024-12-02-14:10:48-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-14:10:48-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-14:10:48-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-14:10:48-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-14:10:49-root-INFO: grad norm: 10.978 10.810 1.914
2024-12-02-14:10:49-root-INFO: Loss too large (187.514->192.818)! Learning rate decreased to 0.05877.
2024-12-02-14:10:49-root-INFO: Loss too large (187.514->189.622)! Learning rate decreased to 0.04702.
2024-12-02-14:10:50-root-INFO: Loss too large (187.514->187.779)! Learning rate decreased to 0.03761.
2024-12-02-14:10:51-root-INFO: grad norm: 9.777 9.715 1.106
2024-12-02-14:10:52-root-INFO: grad norm: 9.050 8.957 1.292
2024-12-02-14:10:53-root-INFO: grad norm: 8.452 8.395 0.978
2024-12-02-14:10:54-root-INFO: grad norm: 7.905 7.824 1.124
2024-12-02-14:10:55-root-INFO: grad norm: 7.431 7.376 0.904
2024-12-02-14:10:56-root-INFO: grad norm: 6.996 6.920 1.024
2024-12-02-14:10:57-root-INFO: grad norm: 6.617 6.562 0.848
2024-12-02-14:10:57-root-INFO: Loss Change: 187.514 -> 183.797
2024-12-02-14:10:57-root-INFO: Regularization Change: 0.000 -> 0.877
2024-12-02-14:10:57-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-14:10:57-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-14:10:58-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-14:10:58-root-INFO: grad norm: 11.533 11.383 1.856
2024-12-02-14:10:58-root-INFO: Loss too large (184.770->191.280)! Learning rate decreased to 0.06047.
2024-12-02-14:10:59-root-INFO: Loss too large (184.770->187.503)! Learning rate decreased to 0.04837.
2024-12-02-14:10:59-root-INFO: Loss too large (184.770->185.299)! Learning rate decreased to 0.03870.
2024-12-02-14:11:00-root-INFO: grad norm: 10.409 10.346 1.148
2024-12-02-14:11:01-root-INFO: grad norm: 9.610 9.523 1.291
2024-12-02-14:11:02-root-INFO: grad norm: 8.954 8.897 1.009
2024-12-02-14:11:03-root-INFO: grad norm: 8.350 8.274 1.124
2024-12-02-14:11:04-root-INFO: grad norm: 7.831 7.777 0.920
2024-12-02-14:11:05-root-INFO: grad norm: 7.355 7.284 1.023
2024-12-02-14:11:06-root-INFO: grad norm: 6.942 6.889 0.855
2024-12-02-14:11:07-root-INFO: Loss Change: 184.770 -> 181.041
2024-12-02-14:11:07-root-INFO: Regularization Change: 0.000 -> 0.880
2024-12-02-14:11:07-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-14:11:07-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-14:11:07-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-14:11:07-root-INFO: grad norm: 11.673 11.535 1.790
2024-12-02-14:11:08-root-INFO: Loss too large (182.072->188.999)! Learning rate decreased to 0.06220.
2024-12-02-14:11:08-root-INFO: Loss too large (182.072->184.994)! Learning rate decreased to 0.04976.
2024-12-02-14:11:08-root-INFO: Loss too large (182.072->182.652)! Learning rate decreased to 0.03981.
2024-12-02-14:11:09-root-INFO: grad norm: 10.433 10.374 1.111
2024-12-02-14:11:10-root-INFO: grad norm: 9.518 9.435 1.255
2024-12-02-14:11:11-root-INFO: grad norm: 8.754 8.700 0.971
2024-12-02-14:11:12-root-INFO: grad norm: 8.067 7.994 1.084
2024-12-02-14:11:13-root-INFO: grad norm: 7.477 7.425 0.879
2024-12-02-14:11:14-root-INFO: grad norm: 6.948 6.879 0.977
2024-12-02-14:11:15-root-INFO: grad norm: 6.491 6.440 0.814
2024-12-02-14:11:16-root-INFO: Loss Change: 182.072 -> 178.258
2024-12-02-14:11:16-root-INFO: Regularization Change: 0.000 -> 0.896
2024-12-02-14:11:16-root-INFO: Undo step: 100
2024-12-02-14:11:16-root-INFO: Undo step: 101
2024-12-02-14:11:16-root-INFO: Undo step: 102
2024-12-02-14:11:16-root-INFO: Undo step: 103
2024-12-02-14:11:16-root-INFO: Undo step: 104
2024-12-02-14:11:16-root-INFO: Undo step: 105
2024-12-02-14:11:16-root-INFO: Undo step: 106
2024-12-02-14:11:16-root-INFO: Undo step: 107
2024-12-02-14:11:16-root-INFO: Undo step: 108
2024-12-02-14:11:16-root-INFO: Undo step: 109
2024-12-02-14:11:16-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-14:11:17-root-INFO: grad norm: 126.026 124.465 19.774
2024-12-02-14:11:18-root-INFO: grad norm: 107.992 106.952 14.951
2024-12-02-14:11:19-root-INFO: grad norm: 78.514 77.275 13.893
2024-12-02-14:11:20-root-INFO: grad norm: 61.395 60.883 7.918
2024-12-02-14:11:21-root-INFO: grad norm: 56.901 56.180 9.030
2024-12-02-14:11:22-root-INFO: grad norm: 60.912 60.358 8.203
2024-12-02-14:11:22-root-INFO: Loss too large (314.018->323.001)! Learning rate decreased to 0.04654.
2024-12-02-14:11:23-root-INFO: grad norm: 44.601 43.940 7.647
2024-12-02-14:11:24-root-INFO: grad norm: 33.719 33.379 4.778
2024-12-02-14:11:25-root-INFO: Loss Change: 703.252 -> 259.341
2024-12-02-14:11:25-root-INFO: Regularization Change: 0.000 -> 97.476
2024-12-02-14:11:25-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-14:11:25-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-14:11:25-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-14:11:25-root-INFO: grad norm: 29.636 29.315 4.347
2024-12-02-14:11:26-root-INFO: Loss too large (256.360->261.836)! Learning rate decreased to 0.04795.
2024-12-02-14:11:27-root-INFO: grad norm: 31.843 31.532 4.441
2024-12-02-14:11:27-root-INFO: Loss too large (250.144->251.329)! Learning rate decreased to 0.03836.
2024-12-02-14:11:28-root-INFO: grad norm: 24.560 24.253 3.868
2024-12-02-14:11:29-root-INFO: grad norm: 18.456 18.239 2.826
2024-12-02-14:11:30-root-INFO: grad norm: 16.014 15.776 2.747
2024-12-02-14:11:31-root-INFO: grad norm: 14.248 14.060 2.311
2024-12-02-14:11:32-root-INFO: grad norm: 13.375 13.171 2.325
2024-12-02-14:11:33-root-INFO: grad norm: 12.868 12.697 2.092
2024-12-02-14:11:34-root-INFO: Loss Change: 256.360 -> 223.913
2024-12-02-14:11:34-root-INFO: Regularization Change: 0.000 -> 7.411
2024-12-02-14:11:34-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-14:11:34-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-14:11:34-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-14:11:35-root-INFO: grad norm: 11.175 11.018 1.871
2024-12-02-14:11:36-root-INFO: grad norm: 19.347 19.216 2.244
2024-12-02-14:11:36-root-INFO: Loss too large (222.406->229.256)! Learning rate decreased to 0.04939.
2024-12-02-14:11:36-root-INFO: Loss too large (222.406->224.032)! Learning rate decreased to 0.03951.
2024-12-02-14:11:37-root-INFO: grad norm: 18.144 18.006 2.230
2024-12-02-14:11:38-root-INFO: grad norm: 17.182 17.044 2.174
2024-12-02-14:11:39-root-INFO: grad norm: 16.869 16.723 2.213
2024-12-02-14:11:40-root-INFO: grad norm: 16.797 16.656 2.172
2024-12-02-14:11:41-root-INFO: grad norm: 16.944 16.793 2.255
2024-12-02-14:11:42-root-INFO: grad norm: 17.307 17.162 2.231
2024-12-02-14:11:43-root-INFO: Loss Change: 222.889 -> 212.557
2024-12-02-14:11:43-root-INFO: Regularization Change: 0.000 -> 4.292
2024-12-02-14:11:43-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-14:11:43-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-14:11:43-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-14:11:44-root-INFO: grad norm: 14.885 14.750 1.999
2024-12-02-14:11:44-root-INFO: Loss too large (211.276->215.883)! Learning rate decreased to 0.05086.
2024-12-02-14:11:44-root-INFO: Loss too large (211.276->212.018)! Learning rate decreased to 0.04069.
2024-12-02-14:11:45-root-INFO: grad norm: 15.326 15.208 1.896
2024-12-02-14:11:46-root-INFO: grad norm: 16.195 16.076 1.961
2024-12-02-14:11:47-root-INFO: grad norm: 17.355 17.230 2.078
2024-12-02-14:11:48-root-INFO: grad norm: 18.220 18.084 2.220
2024-12-02-14:11:49-root-INFO: grad norm: 19.184 19.047 2.288
2024-12-02-14:11:50-root-INFO: grad norm: 19.807 19.653 2.461
2024-12-02-14:11:51-root-INFO: grad norm: 20.449 20.299 2.475
2024-12-02-14:11:52-root-INFO: Loss Change: 211.276 -> 205.762
2024-12-02-14:11:52-root-INFO: Regularization Change: 0.000 -> 2.701
2024-12-02-14:11:52-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-14:11:52-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-14:11:52-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-14:11:53-root-INFO: grad norm: 18.104 17.978 2.136
2024-12-02-14:11:53-root-INFO: Loss too large (204.092->212.914)! Learning rate decreased to 0.05237.
2024-12-02-14:11:53-root-INFO: Loss too large (204.092->206.394)! Learning rate decreased to 0.04190.
2024-12-02-14:11:54-root-INFO: grad norm: 18.193 18.062 2.185
2024-12-02-14:11:55-root-INFO: grad norm: 18.555 18.420 2.229
2024-12-02-14:11:56-root-INFO: grad norm: 19.152 19.009 2.337
2024-12-02-14:11:57-root-INFO: grad norm: 19.634 19.480 2.459
2024-12-02-14:11:58-root-INFO: grad norm: 20.301 20.142 2.534
2024-12-02-14:11:59-root-INFO: Loss too large (200.251->200.448)! Learning rate decreased to 0.03352.
2024-12-02-14:12:00-root-INFO: grad norm: 14.250 14.122 1.909
2024-12-02-14:12:01-root-INFO: grad norm: 10.222 10.111 1.505
2024-12-02-14:12:01-root-INFO: Loss Change: 204.092 -> 195.093
2024-12-02-14:12:01-root-INFO: Regularization Change: 0.000 -> 1.909
2024-12-02-14:12:01-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-14:12:01-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-14:12:02-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-14:12:02-root-INFO: grad norm: 6.618 6.478 1.354
2024-12-02-14:12:03-root-INFO: grad norm: 10.834 10.748 1.361
2024-12-02-14:12:03-root-INFO: Loss too large (194.434->197.349)! Learning rate decreased to 0.05391.
2024-12-02-14:12:04-root-INFO: Loss too large (194.434->195.199)! Learning rate decreased to 0.04313.
2024-12-02-14:12:05-root-INFO: grad norm: 11.713 11.640 1.307
2024-12-02-14:12:06-root-INFO: grad norm: 13.353 13.274 1.454
2024-12-02-14:12:06-root-INFO: Loss too large (193.606->193.628)! Learning rate decreased to 0.03450.
2024-12-02-14:12:07-root-INFO: grad norm: 10.430 10.354 1.259
2024-12-02-14:12:08-root-INFO: grad norm: 8.477 8.392 1.198
2024-12-02-14:12:09-root-INFO: grad norm: 7.335 7.254 1.086
2024-12-02-14:12:10-root-INFO: grad norm: 6.480 6.390 1.076
2024-12-02-14:12:11-root-INFO: Loss Change: 194.923 -> 189.655
2024-12-02-14:12:11-root-INFO: Regularization Change: 0.000 -> 1.679
2024-12-02-14:12:11-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-14:12:11-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-14:12:11-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-14:12:11-root-INFO: grad norm: 7.341 7.123 1.774
2024-12-02-14:12:12-root-INFO: grad norm: 11.288 11.123 1.928
2024-12-02-14:12:13-root-INFO: Loss too large (189.535->192.434)! Learning rate decreased to 0.05550.
2024-12-02-14:12:13-root-INFO: Loss too large (189.535->189.888)! Learning rate decreased to 0.04440.
2024-12-02-14:12:14-root-INFO: grad norm: 10.555 10.425 1.656
2024-12-02-14:12:15-root-INFO: grad norm: 10.526 10.410 1.562
2024-12-02-14:12:16-root-INFO: grad norm: 10.706 10.588 1.587
2024-12-02-14:12:17-root-INFO: grad norm: 11.137 11.023 1.589
2024-12-02-14:12:18-root-INFO: grad norm: 11.948 11.828 1.692
2024-12-02-14:12:19-root-INFO: grad norm: 13.300 13.180 1.780
2024-12-02-14:12:19-root-INFO: Loss too large (186.453->186.463)! Learning rate decreased to 0.03552.
2024-12-02-14:12:20-root-INFO: Loss Change: 189.694 -> 185.174
2024-12-02-14:12:20-root-INFO: Regularization Change: 0.000 -> 1.853
2024-12-02-14:12:20-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-14:12:20-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-14:12:21-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-14:12:21-root-INFO: grad norm: 16.018 15.823 2.493
2024-12-02-14:12:21-root-INFO: Loss too large (186.628->197.408)! Learning rate decreased to 0.05711.
2024-12-02-14:12:22-root-INFO: Loss too large (186.628->191.077)! Learning rate decreased to 0.04569.
2024-12-02-14:12:22-root-INFO: Loss too large (186.628->187.325)! Learning rate decreased to 0.03655.
2024-12-02-14:12:23-root-INFO: grad norm: 12.922 12.820 1.615
2024-12-02-14:12:24-root-INFO: grad norm: 10.970 10.872 1.464
2024-12-02-14:12:25-root-INFO: grad norm: 9.799 9.723 1.216
2024-12-02-14:12:26-root-INFO: grad norm: 8.897 8.818 1.178
2024-12-02-14:12:27-root-INFO: grad norm: 8.239 8.171 1.055
2024-12-02-14:12:28-root-INFO: grad norm: 7.684 7.612 1.052
2024-12-02-14:12:29-root-INFO: grad norm: 7.246 7.181 0.970
2024-12-02-14:12:30-root-INFO: Loss Change: 186.628 -> 181.095
2024-12-02-14:12:30-root-INFO: Regularization Change: 0.000 -> 1.055
2024-12-02-14:12:30-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-14:12:30-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-14:12:30-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-14:12:30-root-INFO: grad norm: 11.479 11.315 1.932
2024-12-02-14:12:31-root-INFO: Loss too large (181.764->187.267)! Learning rate decreased to 0.05877.
2024-12-02-14:12:31-root-INFO: Loss too large (181.764->184.017)! Learning rate decreased to 0.04702.
2024-12-02-14:12:31-root-INFO: Loss too large (181.764->182.068)! Learning rate decreased to 0.03761.
2024-12-02-14:12:32-root-INFO: grad norm: 10.299 10.225 1.227
2024-12-02-14:12:33-root-INFO: grad norm: 9.588 9.501 1.285
2024-12-02-14:12:34-root-INFO: grad norm: 9.059 8.994 1.080
2024-12-02-14:12:35-root-INFO: grad norm: 8.588 8.514 1.126
2024-12-02-14:12:36-root-INFO: grad norm: 8.200 8.139 1.001
2024-12-02-14:12:37-root-INFO: grad norm: 7.852 7.782 1.044
2024-12-02-14:12:38-root-INFO: grad norm: 7.561 7.501 0.948
2024-12-02-14:12:39-root-INFO: Loss Change: 181.764 -> 177.698
2024-12-02-14:12:39-root-INFO: Regularization Change: 0.000 -> 0.993
2024-12-02-14:12:39-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-14:12:39-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-14:12:39-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-14:12:40-root-INFO: grad norm: 12.036 11.879 1.940
2024-12-02-14:12:40-root-INFO: Loss too large (178.604->185.311)! Learning rate decreased to 0.06047.
2024-12-02-14:12:40-root-INFO: Loss too large (178.604->181.498)! Learning rate decreased to 0.04837.
2024-12-02-14:12:41-root-INFO: Loss too large (178.604->179.182)! Learning rate decreased to 0.03870.
2024-12-02-14:12:42-root-INFO: grad norm: 10.877 10.805 1.246
2024-12-02-14:12:43-root-INFO: grad norm: 10.108 10.024 1.295
2024-12-02-14:12:44-root-INFO: grad norm: 9.545 9.483 1.086
2024-12-02-14:12:45-root-INFO: grad norm: 9.049 8.979 1.126
2024-12-02-14:12:46-root-INFO: grad norm: 8.642 8.584 1.000
2024-12-02-14:12:47-root-INFO: grad norm: 8.279 8.213 1.040
2024-12-02-14:12:48-root-INFO: grad norm: 7.973 7.917 0.943
2024-12-02-14:12:48-root-INFO: Loss Change: 178.604 -> 174.677
2024-12-02-14:12:48-root-INFO: Regularization Change: 0.000 -> 0.956
2024-12-02-14:12:48-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-14:12:48-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-14:12:49-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-14:12:49-root-INFO: grad norm: 12.401 12.253 1.908
2024-12-02-14:12:49-root-INFO: Loss too large (175.676->183.179)! Learning rate decreased to 0.06220.
2024-12-02-14:12:50-root-INFO: Loss too large (175.676->178.973)! Learning rate decreased to 0.04976.
2024-12-02-14:12:50-root-INFO: Loss too large (175.676->176.395)! Learning rate decreased to 0.03981.
2024-12-02-14:12:51-root-INFO: grad norm: 11.161 11.093 1.226
2024-12-02-14:12:52-root-INFO: grad norm: 10.263 10.182 1.291
2024-12-02-14:12:53-root-INFO: grad norm: 9.570 9.511 1.060
2024-12-02-14:12:54-root-INFO: grad norm: 8.965 8.896 1.108
2024-12-02-14:12:55-root-INFO: grad norm: 8.460 8.405 0.964
2024-12-02-14:12:56-root-INFO: grad norm: 8.019 7.955 1.008
2024-12-02-14:12:57-root-INFO: grad norm: 7.643 7.590 0.900
2024-12-02-14:12:58-root-INFO: Loss Change: 175.676 -> 171.726
2024-12-02-14:12:58-root-INFO: Regularization Change: 0.000 -> 0.936
2024-12-02-14:12:58-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-14:12:58-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-14:12:58-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-14:12:59-root-INFO: grad norm: 11.340 11.205 1.745
2024-12-02-14:12:59-root-INFO: Loss too large (172.433->178.758)! Learning rate decreased to 0.06397.
2024-12-02-14:12:59-root-INFO: Loss too large (172.433->175.131)! Learning rate decreased to 0.05118.
2024-12-02-14:13:00-root-INFO: Loss too large (172.433->172.935)! Learning rate decreased to 0.04094.
2024-12-02-14:13:01-root-INFO: grad norm: 10.186 10.123 1.131
2024-12-02-14:13:02-root-INFO: grad norm: 9.408 9.335 1.170
2024-12-02-14:13:03-root-INFO: grad norm: 8.795 8.739 0.984
2024-12-02-14:13:04-root-INFO: grad norm: 8.268 8.205 1.019
2024-12-02-14:13:05-root-INFO: grad norm: 7.817 7.765 0.901
2024-12-02-14:13:06-root-INFO: grad norm: 7.429 7.370 0.937
2024-12-02-14:13:07-root-INFO: grad norm: 7.090 7.040 0.845
2024-12-02-14:13:08-root-INFO: Loss Change: 172.433 -> 168.728
2024-12-02-14:13:08-root-INFO: Regularization Change: 0.000 -> 0.927
2024-12-02-14:13:08-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-14:13:08-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-14:13:08-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-14:13:08-root-INFO: grad norm: 11.263 11.126 1.752
2024-12-02-14:13:09-root-INFO: Loss too large (169.609->176.206)! Learning rate decreased to 0.06579.
2024-12-02-14:13:09-root-INFO: Loss too large (169.609->172.438)! Learning rate decreased to 0.05263.
2024-12-02-14:13:09-root-INFO: Loss too large (169.609->170.170)! Learning rate decreased to 0.04210.
2024-12-02-14:13:10-root-INFO: grad norm: 10.189 10.125 1.137
2024-12-02-14:13:11-root-INFO: grad norm: 9.467 9.392 1.185
2024-12-02-14:13:12-root-INFO: grad norm: 8.890 8.835 0.994
2024-12-02-14:13:13-root-INFO: grad norm: 8.389 8.326 1.024
2024-12-02-14:13:14-root-INFO: grad norm: 7.952 7.900 0.907
2024-12-02-14:13:15-root-INFO: grad norm: 7.574 7.516 0.937
2024-12-02-14:13:16-root-INFO: grad norm: 7.239 7.190 0.847
2024-12-02-14:13:17-root-INFO: Loss Change: 169.609 -> 166.013
2024-12-02-14:13:17-root-INFO: Regularization Change: 0.000 -> 0.924
2024-12-02-14:13:17-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-14:13:17-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-14:13:17-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-14:13:18-root-INFO: grad norm: 11.982 11.824 1.939
2024-12-02-14:13:18-root-INFO: Loss too large (167.016->174.753)! Learning rate decreased to 0.06764.
2024-12-02-14:13:18-root-INFO: Loss too large (167.016->170.381)! Learning rate decreased to 0.05411.
2024-12-02-14:13:19-root-INFO: Loss too large (167.016->167.731)! Learning rate decreased to 0.04329.
2024-12-02-14:13:20-root-INFO: grad norm: 10.719 10.653 1.188
2024-12-02-14:13:21-root-INFO: grad norm: 9.867 9.793 1.212
2024-12-02-14:13:22-root-INFO: grad norm: 9.248 9.193 1.005
2024-12-02-14:13:23-root-INFO: grad norm: 8.716 8.656 1.027
2024-12-02-14:13:24-root-INFO: grad norm: 8.253 8.203 0.909
2024-12-02-14:13:25-root-INFO: grad norm: 7.858 7.803 0.932
2024-12-02-14:13:26-root-INFO: grad norm: 7.503 7.455 0.844
2024-12-02-14:13:26-root-INFO: Loss Change: 167.016 -> 163.312
2024-12-02-14:13:26-root-INFO: Regularization Change: 0.000 -> 0.931
2024-12-02-14:13:26-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-14:13:26-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-14:13:27-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-14:13:27-root-INFO: grad norm: 10.777 10.658 1.595
2024-12-02-14:13:27-root-INFO: Loss too large (164.125->170.687)! Learning rate decreased to 0.06953.
2024-12-02-14:13:28-root-INFO: Loss too large (164.125->166.950)! Learning rate decreased to 0.05563.
2024-12-02-14:13:28-root-INFO: Loss too large (164.125->164.704)! Learning rate decreased to 0.04450.
2024-12-02-14:13:29-root-INFO: grad norm: 9.746 9.690 1.051
2024-12-02-14:13:30-root-INFO: grad norm: 9.028 8.960 1.106
2024-12-02-14:13:31-root-INFO: grad norm: 8.438 8.388 0.920
2024-12-02-14:13:32-root-INFO: grad norm: 7.941 7.883 0.956
2024-12-02-14:13:33-root-INFO: grad norm: 7.499 7.452 0.838
2024-12-02-14:13:34-root-INFO: grad norm: 7.129 7.075 0.871
2024-12-02-14:13:35-root-INFO: grad norm: 6.793 6.748 0.781
2024-12-02-14:13:36-root-INFO: Loss Change: 164.125 -> 160.723
2024-12-02-14:13:36-root-INFO: Regularization Change: 0.000 -> 0.909
2024-12-02-14:13:36-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-14:13:36-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-14:13:36-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-14:13:36-root-INFO: grad norm: 10.978 10.849 1.677
2024-12-02-14:13:37-root-INFO: Loss too large (161.870->168.897)! Learning rate decreased to 0.07147.
2024-12-02-14:13:37-root-INFO: Loss too large (161.870->164.927)! Learning rate decreased to 0.05718.
2024-12-02-14:13:37-root-INFO: Loss too large (161.870->162.529)! Learning rate decreased to 0.04574.
2024-12-02-14:13:38-root-INFO: grad norm: 9.942 9.884 1.065
2024-12-02-14:13:39-root-INFO: grad norm: 9.207 9.139 1.117
2024-12-02-14:13:40-root-INFO: grad norm: 8.632 8.583 0.925
2024-12-02-14:13:41-root-INFO: grad norm: 8.140 8.084 0.959
2024-12-02-14:13:42-root-INFO: grad norm: 7.696 7.650 0.841
2024-12-02-14:13:43-root-INFO: grad norm: 7.323 7.271 0.872
2024-12-02-14:13:44-root-INFO: grad norm: 6.978 6.934 0.783
2024-12-02-14:13:45-root-INFO: Loss Change: 161.870 -> 158.474
2024-12-02-14:13:45-root-INFO: Regularization Change: 0.000 -> 0.918
2024-12-02-14:13:45-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-14:13:45-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-14:13:45-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-14:13:46-root-INFO: grad norm: 10.809 10.683 1.644
2024-12-02-14:13:46-root-INFO: Loss too large (159.293->166.314)! Learning rate decreased to 0.07345.
2024-12-02-14:13:46-root-INFO: Loss too large (159.293->162.329)! Learning rate decreased to 0.05876.
2024-12-02-14:13:47-root-INFO: Loss too large (159.293->159.927)! Learning rate decreased to 0.04701.
2024-12-02-14:13:48-root-INFO: grad norm: 9.756 9.700 1.046
2024-12-02-14:13:49-root-INFO: grad norm: 9.029 8.964 1.077
2024-12-02-14:13:50-root-INFO: grad norm: 8.449 8.400 0.904
2024-12-02-14:13:51-root-INFO: grad norm: 7.962 7.909 0.919
2024-12-02-14:13:52-root-INFO: grad norm: 7.520 7.475 0.819
2024-12-02-14:13:53-root-INFO: grad norm: 7.154 7.105 0.835
2024-12-02-14:13:54-root-INFO: grad norm: 6.811 6.768 0.760
2024-12-02-14:13:55-root-INFO: Loss Change: 159.293 -> 155.931
2024-12-02-14:13:55-root-INFO: Regularization Change: 0.000 -> 0.926
2024-12-02-14:13:55-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-14:13:55-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-14:13:55-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-14:13:55-root-INFO: grad norm: 10.929 10.801 1.672
2024-12-02-14:13:56-root-INFO: Loss too large (156.810->164.047)! Learning rate decreased to 0.07547.
2024-12-02-14:13:56-root-INFO: Loss too large (156.810->159.924)! Learning rate decreased to 0.06038.
2024-12-02-14:13:56-root-INFO: Loss too large (156.810->157.435)! Learning rate decreased to 0.04830.
2024-12-02-14:13:57-root-INFO: grad norm: 9.720 9.664 1.036
2024-12-02-14:13:58-root-INFO: grad norm: 8.868 8.804 1.063
2024-12-02-14:13:59-root-INFO: grad norm: 8.231 8.184 0.878
2024-12-02-14:14:00-root-INFO: grad norm: 7.712 7.659 0.899
2024-12-02-14:14:01-root-INFO: grad norm: 7.236 7.192 0.792
2024-12-02-14:14:02-root-INFO: grad norm: 6.855 6.807 0.812
2024-12-02-14:14:03-root-INFO: grad norm: 6.495 6.454 0.733
2024-12-02-14:14:04-root-INFO: Loss Change: 156.810 -> 153.343
2024-12-02-14:14:04-root-INFO: Regularization Change: 0.000 -> 0.948
2024-12-02-14:14:04-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-14:14:04-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-14:14:04-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-14:14:05-root-INFO: grad norm: 9.349 9.245 1.392
2024-12-02-14:14:05-root-INFO: Loss too large (154.060->159.390)! Learning rate decreased to 0.07753.
2024-12-02-14:14:05-root-INFO: Loss too large (154.060->156.224)! Learning rate decreased to 0.06203.
2024-12-02-14:14:06-root-INFO: Loss too large (154.060->154.367)! Learning rate decreased to 0.04962.
2024-12-02-14:14:07-root-INFO: grad norm: 8.266 8.216 0.909
2024-12-02-14:14:08-root-INFO: grad norm: 7.594 7.536 0.931
2024-12-02-14:14:09-root-INFO: grad norm: 7.012 6.966 0.800
2024-12-02-14:14:10-root-INFO: grad norm: 6.556 6.507 0.803
2024-12-02-14:14:11-root-INFO: grad norm: 6.134 6.090 0.729
2024-12-02-14:14:12-root-INFO: grad norm: 5.800 5.754 0.733
2024-12-02-14:14:13-root-INFO: grad norm: 5.487 5.445 0.680
2024-12-02-14:14:14-root-INFO: Loss Change: 154.060 -> 150.881
2024-12-02-14:14:14-root-INFO: Regularization Change: 0.000 -> 0.945
2024-12-02-14:14:14-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-14:14:14-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-14:14:14-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-14:14:14-root-INFO: grad norm: 8.572 8.476 1.283
2024-12-02-14:14:15-root-INFO: Loss too large (151.483->156.162)! Learning rate decreased to 0.07964.
2024-12-02-14:14:15-root-INFO: Loss too large (151.483->153.406)! Learning rate decreased to 0.06371.
2024-12-02-14:14:15-root-INFO: Loss too large (151.483->151.801)! Learning rate decreased to 0.05097.
2024-12-02-14:14:16-root-INFO: grad norm: 7.679 7.629 0.877
2024-12-02-14:14:17-root-INFO: grad norm: 7.076 7.020 0.891
2024-12-02-14:14:18-root-INFO: grad norm: 6.614 6.569 0.772
2024-12-02-14:14:19-root-INFO: grad norm: 6.234 6.185 0.781
2024-12-02-14:14:20-root-INFO: grad norm: 5.895 5.852 0.711
2024-12-02-14:14:21-root-INFO: grad norm: 5.620 5.573 0.722
2024-12-02-14:14:22-root-INFO: grad norm: 5.364 5.321 0.671
2024-12-02-14:14:23-root-INFO: Loss Change: 151.483 -> 148.588
2024-12-02-14:14:23-root-INFO: Regularization Change: 0.000 -> 0.929
2024-12-02-14:14:23-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-14:14:23-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-14:14:24-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-14:14:24-root-INFO: grad norm: 8.258 8.154 1.304
2024-12-02-14:14:24-root-INFO: Loss too large (149.269->153.520)! Learning rate decreased to 0.08180.
2024-12-02-14:14:25-root-INFO: Loss too large (149.269->150.981)! Learning rate decreased to 0.06544.
2024-12-02-14:14:25-root-INFO: Loss too large (149.269->149.503)! Learning rate decreased to 0.05235.
2024-12-02-14:14:26-root-INFO: grad norm: 7.316 7.271 0.810
2024-12-02-14:14:27-root-INFO: grad norm: 6.701 6.644 0.870
2024-12-02-14:14:28-root-INFO: grad norm: 6.244 6.202 0.725
2024-12-02-14:14:29-root-INFO: grad norm: 5.880 5.831 0.761
2024-12-02-14:14:30-root-INFO: grad norm: 5.552 5.510 0.678
2024-12-02-14:14:31-root-INFO: grad norm: 5.293 5.245 0.705
2024-12-02-14:14:32-root-INFO: grad norm: 5.050 5.008 0.646
2024-12-02-14:14:33-root-INFO: Loss Change: 149.269 -> 146.390
2024-12-02-14:14:33-root-INFO: Regularization Change: 0.000 -> 0.947
2024-12-02-14:14:33-root-INFO: Undo step: 90
2024-12-02-14:14:33-root-INFO: Undo step: 91
2024-12-02-14:14:33-root-INFO: Undo step: 92
2024-12-02-14:14:33-root-INFO: Undo step: 93
2024-12-02-14:14:33-root-INFO: Undo step: 94
2024-12-02-14:14:33-root-INFO: Undo step: 95
2024-12-02-14:14:33-root-INFO: Undo step: 96
2024-12-02-14:14:33-root-INFO: Undo step: 97
2024-12-02-14:14:33-root-INFO: Undo step: 98
2024-12-02-14:14:33-root-INFO: Undo step: 99
2024-12-02-14:14:33-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-14:14:33-root-INFO: grad norm: 84.699 83.622 13.468
2024-12-02-14:14:35-root-INFO: grad norm: 51.123 50.498 7.973
2024-12-02-14:14:36-root-INFO: grad norm: 37.656 37.179 5.972
2024-12-02-14:14:37-root-INFO: grad norm: 41.526 41.194 5.244
2024-12-02-14:14:37-root-INFO: Loss too large (271.138->284.590)! Learning rate decreased to 0.06220.
2024-12-02-14:14:38-root-INFO: grad norm: 41.607 41.145 6.186
2024-12-02-14:14:39-root-INFO: grad norm: 43.975 43.695 4.948
2024-12-02-14:14:39-root-INFO: Loss too large (252.321->252.793)! Learning rate decreased to 0.04976.
2024-12-02-14:14:40-root-INFO: grad norm: 33.003 32.646 4.840
2024-12-02-14:14:41-root-INFO: grad norm: 25.839 25.656 3.069
2024-12-02-14:14:42-root-INFO: Loss Change: 594.462 -> 215.232
2024-12-02-14:14:42-root-INFO: Regularization Change: 0.000 -> 112.754
2024-12-02-14:14:42-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-14:14:42-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-14:14:42-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-14:14:43-root-INFO: grad norm: 19.543 19.340 2.812
2024-12-02-14:14:43-root-INFO: Loss too large (213.358->217.776)! Learning rate decreased to 0.06397.
2024-12-02-14:14:44-root-INFO: grad norm: 24.065 23.904 2.784
2024-12-02-14:14:44-root-INFO: Loss too large (211.406->212.107)! Learning rate decreased to 0.05118.
2024-12-02-14:14:45-root-INFO: grad norm: 20.755 20.547 2.932
2024-12-02-14:14:46-root-INFO: grad norm: 18.318 18.166 2.355
2024-12-02-14:14:47-root-INFO: grad norm: 16.513 16.322 2.505
2024-12-02-14:14:48-root-INFO: grad norm: 15.208 15.062 2.103
2024-12-02-14:14:49-root-INFO: grad norm: 14.253 14.075 2.246
2024-12-02-14:14:50-root-INFO: grad norm: 13.584 13.443 1.951
2024-12-02-14:14:51-root-INFO: Loss Change: 213.358 -> 188.452
2024-12-02-14:14:51-root-INFO: Regularization Change: 0.000 -> 9.394
2024-12-02-14:14:51-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-14:14:51-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-14:14:51-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-14:14:52-root-INFO: grad norm: 10.216 10.080 1.662
2024-12-02-14:14:52-root-INFO: Loss too large (187.330->187.398)! Learning rate decreased to 0.06579.
2024-12-02-14:14:53-root-INFO: grad norm: 13.080 12.955 1.802
2024-12-02-14:14:53-root-INFO: Loss too large (186.016->186.163)! Learning rate decreased to 0.05263.
2024-12-02-14:14:54-root-INFO: grad norm: 12.840 12.700 1.895
2024-12-02-14:14:55-root-INFO: grad norm: 12.828 12.703 1.786
2024-12-02-14:14:56-root-INFO: grad norm: 12.947 12.806 1.909
2024-12-02-14:14:57-root-INFO: grad norm: 13.201 13.078 1.803
2024-12-02-14:14:58-root-INFO: grad norm: 13.565 13.424 1.951
2024-12-02-14:14:59-root-INFO: grad norm: 14.037 13.912 1.869
2024-12-02-14:15:00-root-INFO: Loss Change: 187.330 -> 177.255
2024-12-02-14:15:00-root-INFO: Regularization Change: 0.000 -> 5.049
2024-12-02-14:15:00-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-14:15:00-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-14:15:01-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-14:15:01-root-INFO: grad norm: 11.054 10.950 1.516
2024-12-02-14:15:01-root-INFO: Loss too large (175.687->178.659)! Learning rate decreased to 0.06764.
2024-12-02-14:15:02-root-INFO: Loss too large (175.687->176.019)! Learning rate decreased to 0.05411.
2024-12-02-14:15:03-root-INFO: grad norm: 11.216 11.107 1.558
2024-12-02-14:15:04-root-INFO: grad norm: 11.784 11.669 1.645
2024-12-02-14:15:05-root-INFO: grad norm: 12.510 12.397 1.676
2024-12-02-14:15:06-root-INFO: grad norm: 13.351 13.227 1.818
2024-12-02-14:15:07-root-INFO: grad norm: 14.264 14.144 1.847
2024-12-02-14:15:08-root-INFO: grad norm: 15.235 15.100 2.017
2024-12-02-14:15:09-root-INFO: grad norm: 16.219 16.088 2.055
2024-12-02-14:15:09-root-INFO: Loss Change: 175.687 -> 170.551
2024-12-02-14:15:09-root-INFO: Regularization Change: 0.000 -> 3.278
2024-12-02-14:15:09-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-14:15:09-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-14:15:10-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-14:15:10-root-INFO: grad norm: 14.276 14.173 1.714
2024-12-02-14:15:10-root-INFO: Loss too large (169.279->177.552)! Learning rate decreased to 0.06953.
2024-12-02-14:15:11-root-INFO: Loss too large (169.279->171.995)! Learning rate decreased to 0.05563.
2024-12-02-14:15:12-root-INFO: grad norm: 14.994 14.872 1.913
2024-12-02-14:15:13-root-INFO: grad norm: 15.831 15.704 2.006
2024-12-02-14:15:14-root-INFO: grad norm: 16.680 16.547 2.102
2024-12-02-14:15:15-root-INFO: grad norm: 17.462 17.319 2.227
2024-12-02-14:15:16-root-INFO: grad norm: 18.184 18.041 2.277
2024-12-02-14:15:17-root-INFO: grad norm: 18.774 18.619 2.403
2024-12-02-14:15:18-root-INFO: grad norm: 19.275 19.124 2.409
2024-12-02-14:15:19-root-INFO: Loss Change: 169.279 -> 166.388
2024-12-02-14:15:19-root-INFO: Regularization Change: 0.000 -> 2.542
2024-12-02-14:15:19-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-14:15:19-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-14:15:19-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-14:15:19-root-INFO: grad norm: 16.280 16.176 1.838
2024-12-02-14:15:20-root-INFO: Loss too large (164.735->176.413)! Learning rate decreased to 0.07147.
2024-12-02-14:15:20-root-INFO: Loss too large (164.735->168.783)! Learning rate decreased to 0.05718.
2024-12-02-14:15:21-root-INFO: grad norm: 16.759 16.627 2.095
2024-12-02-14:15:22-root-INFO: grad norm: 17.305 17.172 2.143
2024-12-02-14:15:23-root-INFO: grad norm: 17.836 17.696 2.227
2024-12-02-14:15:24-root-INFO: grad norm: 18.258 18.112 2.305
2024-12-02-14:15:25-root-INFO: grad norm: 18.617 18.472 2.324
2024-12-02-14:15:26-root-INFO: grad norm: 18.858 18.705 2.399
2024-12-02-14:15:27-root-INFO: grad norm: 19.033 18.884 2.376
2024-12-02-14:15:28-root-INFO: Loss Change: 164.735 -> 161.739
2024-12-02-14:15:28-root-INFO: Regularization Change: 0.000 -> 2.134
2024-12-02-14:15:28-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-14:15:28-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-14:15:28-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-14:15:28-root-INFO: grad norm: 16.055 15.951 1.828
2024-12-02-14:15:29-root-INFO: Loss too large (159.852->171.549)! Learning rate decreased to 0.07345.
2024-12-02-14:15:29-root-INFO: Loss too large (159.852->163.916)! Learning rate decreased to 0.05876.
2024-12-02-14:15:30-root-INFO: grad norm: 16.310 16.183 2.032
2024-12-02-14:15:31-root-INFO: grad norm: 16.646 16.516 2.069
2024-12-02-14:15:32-root-INFO: grad norm: 17.014 16.881 2.125
2024-12-02-14:15:33-root-INFO: grad norm: 17.291 17.151 2.194
2024-12-02-14:15:34-root-INFO: grad norm: 17.527 17.390 2.189
2024-12-02-14:15:35-root-INFO: grad norm: 17.689 17.544 2.255
2024-12-02-14:15:36-root-INFO: grad norm: 17.808 17.669 2.223
2024-12-02-14:15:37-root-INFO: Loss Change: 159.852 -> 156.905
2024-12-02-14:15:37-root-INFO: Regularization Change: 0.000 -> 1.880
2024-12-02-14:15:37-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-14:15:37-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-14:15:37-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-14:15:37-root-INFO: grad norm: 14.437 14.349 1.593
2024-12-02-14:15:38-root-INFO: Loss too large (154.951->164.455)! Learning rate decreased to 0.07547.
2024-12-02-14:15:38-root-INFO: Loss too large (154.951->158.188)! Learning rate decreased to 0.06038.
2024-12-02-14:15:39-root-INFO: grad norm: 14.551 14.436 1.831
2024-12-02-14:15:40-root-INFO: grad norm: 14.830 14.719 1.812
2024-12-02-14:15:41-root-INFO: grad norm: 15.170 15.050 1.910
2024-12-02-14:15:42-root-INFO: grad norm: 15.446 15.324 1.936
2024-12-02-14:15:43-root-INFO: grad norm: 15.697 15.573 1.970
2024-12-02-14:15:44-root-INFO: grad norm: 15.907 15.780 2.003
2024-12-02-14:15:45-root-INFO: grad norm: 16.086 15.959 2.012
2024-12-02-14:15:46-root-INFO: Loss Change: 154.951 -> 152.243
2024-12-02-14:15:46-root-INFO: Regularization Change: 0.000 -> 1.736
2024-12-02-14:15:46-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-14:15:46-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-14:15:46-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-14:15:47-root-INFO: grad norm: 13.726 13.637 1.558
2024-12-02-14:15:47-root-INFO: Loss too large (151.174->160.071)! Learning rate decreased to 0.07753.
2024-12-02-14:15:47-root-INFO: Loss too large (151.174->154.182)! Learning rate decreased to 0.06203.
2024-12-02-14:15:48-root-INFO: grad norm: 13.830 13.722 1.730
2024-12-02-14:15:49-root-INFO: grad norm: 14.128 14.018 1.756
2024-12-02-14:15:50-root-INFO: grad norm: 14.528 14.414 1.820
2024-12-02-14:15:51-root-INFO: grad norm: 14.852 14.731 1.890
2024-12-02-14:15:52-root-INFO: grad norm: 15.152 15.033 1.896
2024-12-02-14:15:53-root-INFO: grad norm: 15.412 15.287 1.964
2024-12-02-14:15:54-root-INFO: grad norm: 15.630 15.508 1.950
2024-12-02-14:15:55-root-INFO: Loss Change: 151.174 -> 148.874
2024-12-02-14:15:55-root-INFO: Regularization Change: 0.000 -> 1.638
2024-12-02-14:15:55-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-14:15:55-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-14:15:55-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-14:15:56-root-INFO: grad norm: 12.978 12.894 1.475
2024-12-02-14:15:56-root-INFO: Loss too large (147.507->155.890)! Learning rate decreased to 0.07964.
2024-12-02-14:15:56-root-INFO: Loss too large (147.507->150.392)! Learning rate decreased to 0.06371.
2024-12-02-14:15:57-root-INFO: grad norm: 13.181 13.076 1.661
2024-12-02-14:15:58-root-INFO: grad norm: 13.472 13.366 1.688
2024-12-02-14:15:59-root-INFO: grad norm: 13.822 13.710 1.753
2024-12-02-14:16:00-root-INFO: grad norm: 14.079 13.963 1.798
2024-12-02-14:16:01-root-INFO: grad norm: 14.286 14.172 1.804
2024-12-02-14:16:02-root-INFO: grad norm: 14.483 14.364 1.847
2024-12-02-14:16:03-root-INFO: grad norm: 14.648 14.531 1.847
2024-12-02-14:16:04-root-INFO: Loss Change: 147.507 -> 145.346
2024-12-02-14:16:04-root-INFO: Regularization Change: 0.000 -> 1.542
2024-12-02-14:16:04-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-14:16:04-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-14:16:05-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-14:16:05-root-INFO: grad norm: 11.996 11.925 1.304
2024-12-02-14:16:05-root-INFO: Loss too large (144.211->151.258)! Learning rate decreased to 0.08180.
2024-12-02-14:16:06-root-INFO: Loss too large (144.211->146.520)! Learning rate decreased to 0.06544.
2024-12-02-14:16:07-root-INFO: grad norm: 11.966 11.866 1.540
2024-12-02-14:16:08-root-INFO: grad norm: 12.086 11.994 1.488
2024-12-02-14:16:09-root-INFO: grad norm: 12.315 12.213 1.584
2024-12-02-14:16:10-root-INFO: grad norm: 12.439 12.338 1.577
2024-12-02-14:16:11-root-INFO: grad norm: 12.535 12.432 1.601
2024-12-02-14:16:12-root-INFO: grad norm: 12.656 12.554 1.604
2024-12-02-14:16:13-root-INFO: grad norm: 12.776 12.672 1.626
2024-12-02-14:16:13-root-INFO: Loss Change: 144.211 -> 141.712
2024-12-02-14:16:13-root-INFO: Regularization Change: 0.000 -> 1.502
2024-12-02-14:16:13-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-14:16:13-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-14:16:14-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-14:16:14-root-INFO: grad norm: 9.470 9.408 1.080
2024-12-02-14:16:15-root-INFO: Loss too large (140.216->144.334)! Learning rate decreased to 0.08399.
2024-12-02-14:16:15-root-INFO: Loss too large (140.216->141.428)! Learning rate decreased to 0.06719.
2024-12-02-14:16:16-root-INFO: grad norm: 9.488 9.406 1.246
2024-12-02-14:16:17-root-INFO: grad norm: 9.796 9.715 1.252
2024-12-02-14:16:18-root-INFO: grad norm: 10.292 10.203 1.357
2024-12-02-14:16:19-root-INFO: grad norm: 10.641 10.547 1.407
2024-12-02-14:16:20-root-INFO: grad norm: 10.944 10.850 1.428
2024-12-02-14:16:21-root-INFO: grad norm: 11.316 11.219 1.479
2024-12-02-14:16:22-root-INFO: grad norm: 11.667 11.569 1.509
2024-12-02-14:16:23-root-INFO: Loss Change: 140.216 -> 138.324
2024-12-02-14:16:23-root-INFO: Regularization Change: 0.000 -> 1.518
2024-12-02-14:16:23-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-14:16:23-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-14:16:23-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-14:16:23-root-INFO: grad norm: 9.587 9.526 1.081
2024-12-02-14:16:24-root-INFO: Loss too large (137.762->142.357)! Learning rate decreased to 0.08623.
2024-12-02-14:16:24-root-INFO: Loss too large (137.762->139.165)! Learning rate decreased to 0.06899.
2024-12-02-14:16:25-root-INFO: grad norm: 9.660 9.575 1.279
2024-12-02-14:16:26-root-INFO: grad norm: 9.857 9.777 1.253
2024-12-02-14:16:27-root-INFO: grad norm: 10.111 10.024 1.325
2024-12-02-14:16:28-root-INFO: grad norm: 10.260 10.173 1.329
2024-12-02-14:16:29-root-INFO: grad norm: 10.350 10.263 1.340
2024-12-02-14:16:30-root-INFO: grad norm: 10.485 10.398 1.350
2024-12-02-14:16:31-root-INFO: grad norm: 10.614 10.526 1.367
2024-12-02-14:16:32-root-INFO: Loss Change: 137.762 -> 135.524
2024-12-02-14:16:32-root-INFO: Regularization Change: 0.000 -> 1.460
2024-12-02-14:16:32-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-14:16:32-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-14:16:32-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-14:16:32-root-INFO: grad norm: 7.800 7.752 0.863
2024-12-02-14:16:33-root-INFO: Loss too large (134.348->136.883)! Learning rate decreased to 0.08852.
2024-12-02-14:16:33-root-INFO: Loss too large (134.348->134.929)! Learning rate decreased to 0.07082.
2024-12-02-14:16:34-root-INFO: grad norm: 7.583 7.515 1.015
2024-12-02-14:16:35-root-INFO: grad norm: 7.743 7.680 0.989
2024-12-02-14:16:36-root-INFO: grad norm: 8.158 8.083 1.099
2024-12-02-14:16:37-root-INFO: grad norm: 8.305 8.229 1.121
2024-12-02-14:16:38-root-INFO: grad norm: 8.401 8.325 1.126
2024-12-02-14:16:39-root-INFO: grad norm: 8.608 8.531 1.151
2024-12-02-14:16:40-root-INFO: grad norm: 8.840 8.762 1.174
2024-12-02-14:16:41-root-INFO: Loss Change: 134.348 -> 132.115
2024-12-02-14:16:41-root-INFO: Regularization Change: 0.000 -> 1.482
2024-12-02-14:16:41-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-14:16:41-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-14:16:41-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-14:16:42-root-INFO: grad norm: 6.634 6.588 0.779
2024-12-02-14:16:42-root-INFO: Loss too large (131.441->133.222)! Learning rate decreased to 0.09086.
2024-12-02-14:16:42-root-INFO: Loss too large (131.441->131.757)! Learning rate decreased to 0.07268.
2024-12-02-14:16:43-root-INFO: grad norm: 6.533 6.470 0.909
2024-12-02-14:16:44-root-INFO: grad norm: 6.705 6.643 0.909
2024-12-02-14:16:45-root-INFO: grad norm: 7.053 6.985 0.977
2024-12-02-14:16:47-root-INFO: grad norm: 7.184 7.113 1.005
2024-12-02-14:16:48-root-INFO: grad norm: 7.260 7.191 0.998
2024-12-02-14:16:49-root-INFO: grad norm: 7.457 7.386 1.029
2024-12-02-14:16:50-root-INFO: grad norm: 7.681 7.610 1.042
2024-12-02-14:16:50-root-INFO: Loss Change: 131.441 -> 129.271
2024-12-02-14:16:50-root-INFO: Regularization Change: 0.000 -> 1.459
2024-12-02-14:16:50-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-14:16:50-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-14:16:51-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-14:16:51-root-INFO: grad norm: 5.716 5.673 0.697
2024-12-02-14:16:51-root-INFO: Loss too large (128.664->129.819)! Learning rate decreased to 0.09324.
2024-12-02-14:16:52-root-INFO: Loss too large (128.664->128.740)! Learning rate decreased to 0.07459.
2024-12-02-14:16:53-root-INFO: grad norm: 5.675 5.613 0.836
2024-12-02-14:16:54-root-INFO: grad norm: 5.908 5.851 0.823
2024-12-02-14:16:55-root-INFO: grad norm: 6.323 6.258 0.904
2024-12-02-14:16:56-root-INFO: grad norm: 6.462 6.396 0.926
2024-12-02-14:16:57-root-INFO: grad norm: 6.517 6.452 0.919
2024-12-02-14:16:58-root-INFO: grad norm: 6.758 6.691 0.948
2024-12-02-14:16:59-root-INFO: grad norm: 7.049 6.981 0.974
2024-12-02-14:16:59-root-INFO: Loss Change: 128.664 -> 126.586
2024-12-02-14:16:59-root-INFO: Regularization Change: 0.000 -> 1.475
2024-12-02-14:16:59-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-14:16:59-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-14:17:00-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-14:17:00-root-INFO: grad norm: 5.696 5.650 0.719
2024-12-02-14:17:00-root-INFO: Loss too large (126.070->127.330)! Learning rate decreased to 0.09566.
2024-12-02-14:17:01-root-INFO: Loss too large (126.070->126.170)! Learning rate decreased to 0.07653.
2024-12-02-14:17:02-root-INFO: grad norm: 5.561 5.504 0.796
2024-12-02-14:17:03-root-INFO: grad norm: 5.796 5.738 0.820
2024-12-02-14:17:04-root-INFO: grad norm: 6.231 6.170 0.873
2024-12-02-14:17:05-root-INFO: grad norm: 6.341 6.274 0.916
2024-12-02-14:17:06-root-INFO: grad norm: 6.355 6.293 0.885
2024-12-02-14:17:07-root-INFO: grad norm: 6.567 6.501 0.928
2024-12-02-14:17:08-root-INFO: grad norm: 6.837 6.773 0.937
2024-12-02-14:17:08-root-INFO: Loss Change: 126.070 -> 124.003
2024-12-02-14:17:08-root-INFO: Regularization Change: 0.000 -> 1.457
2024-12-02-14:17:08-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-14:17:08-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-14:17:09-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-14:17:09-root-INFO: grad norm: 5.212 5.170 0.656
2024-12-02-14:17:09-root-INFO: Loss too large (123.527->124.327)! Learning rate decreased to 0.09814.
2024-12-02-14:17:10-root-INFO: grad norm: 7.465 7.401 0.977
2024-12-02-14:17:11-root-INFO: Loss too large (123.418->124.325)! Learning rate decreased to 0.07851.
2024-12-02-14:17:12-root-INFO: grad norm: 6.645 6.577 0.950
2024-12-02-14:17:13-root-INFO: grad norm: 5.620 5.571 0.736
2024-12-02-14:17:14-root-INFO: grad norm: 5.605 5.559 0.718
2024-12-02-14:17:15-root-INFO: grad norm: 5.978 5.925 0.788
2024-12-02-14:17:16-root-INFO: grad norm: 6.207 6.147 0.859
2024-12-02-14:17:17-root-INFO: grad norm: 6.717 6.654 0.913
2024-12-02-14:17:18-root-INFO: Loss Change: 123.527 -> 121.479
2024-12-02-14:17:18-root-INFO: Regularization Change: 0.000 -> 1.582
2024-12-02-14:17:18-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-14:17:18-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-14:17:18-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-14:17:18-root-INFO: grad norm: 5.319 5.275 0.687
2024-12-02-14:17:19-root-INFO: Loss too large (121.249->122.317)! Learning rate decreased to 0.10066.
2024-12-02-14:17:19-root-INFO: Loss too large (121.249->121.264)! Learning rate decreased to 0.08053.
2024-12-02-14:17:20-root-INFO: grad norm: 5.176 5.121 0.756
2024-12-02-14:17:21-root-INFO: grad norm: 5.333 5.275 0.785
2024-12-02-14:17:22-root-INFO: grad norm: 5.687 5.628 0.814
2024-12-02-14:17:23-root-INFO: grad norm: 5.686 5.621 0.854
2024-12-02-14:17:24-root-INFO: grad norm: 5.569 5.511 0.802
2024-12-02-14:17:25-root-INFO: grad norm: 5.716 5.654 0.844
2024-12-02-14:17:26-root-INFO: grad norm: 5.963 5.903 0.843
2024-12-02-14:17:27-root-INFO: Loss Change: 121.249 -> 119.129
2024-12-02-14:17:27-root-INFO: Regularization Change: 0.000 -> 1.482
2024-12-02-14:17:27-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-14:17:27-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-14:17:27-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-14:17:28-root-INFO: grad norm: 4.791 4.750 0.626
2024-12-02-14:17:28-root-INFO: Loss too large (118.668->119.269)! Learning rate decreased to 0.10323.
2024-12-02-14:17:29-root-INFO: grad norm: 6.871 6.813 0.892
2024-12-02-14:17:29-root-INFO: Loss too large (118.480->119.256)! Learning rate decreased to 0.08259.
2024-12-02-14:17:30-root-INFO: Loss too large (118.480->118.486)! Learning rate decreased to 0.06607.
2024-12-02-14:17:31-root-INFO: grad norm: 4.589 4.535 0.706
2024-12-02-14:17:32-root-INFO: grad norm: 2.673 2.630 0.475
2024-12-02-14:17:33-root-INFO: grad norm: 2.328 2.288 0.427
2024-12-02-14:17:34-root-INFO: grad norm: 2.192 2.151 0.418
2024-12-02-14:17:35-root-INFO: grad norm: 2.127 2.086 0.414
2024-12-02-14:17:36-root-INFO: grad norm: 2.092 2.052 0.406
2024-12-02-14:17:36-root-INFO: Loss Change: 118.668 -> 116.165
2024-12-02-14:17:36-root-INFO: Regularization Change: 0.000 -> 1.153
2024-12-02-14:17:37-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-14:17:37-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-14:17:37-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-14:17:37-root-INFO: grad norm: 3.770 3.690 0.773
2024-12-02-14:17:38-root-INFO: grad norm: 5.910 5.847 0.861
2024-12-02-14:17:39-root-INFO: Loss too large (116.395->118.120)! Learning rate decreased to 0.10585.
2024-12-02-14:17:39-root-INFO: Loss too large (116.395->116.774)! Learning rate decreased to 0.08468.
2024-12-02-14:17:40-root-INFO: grad norm: 5.947 5.890 0.819
2024-12-02-14:17:41-root-INFO: grad norm: 6.214 6.151 0.878
2024-12-02-14:17:42-root-INFO: grad norm: 7.009 6.945 0.948
2024-12-02-14:17:42-root-INFO: Loss too large (115.672->115.783)! Learning rate decreased to 0.06774.
2024-12-02-14:17:43-root-INFO: grad norm: 4.969 4.910 0.764
2024-12-02-14:17:44-root-INFO: grad norm: 3.196 3.155 0.512
2024-12-02-14:17:45-root-INFO: grad norm: 2.834 2.793 0.479
2024-12-02-14:17:46-root-INFO: Loss Change: 116.479 -> 114.041
2024-12-02-14:17:46-root-INFO: Regularization Change: 0.000 -> 1.479
2024-12-02-14:17:46-root-INFO: Undo step: 80
2024-12-02-14:17:46-root-INFO: Undo step: 81
2024-12-02-14:17:46-root-INFO: Undo step: 82
2024-12-02-14:17:46-root-INFO: Undo step: 83
2024-12-02-14:17:46-root-INFO: Undo step: 84
2024-12-02-14:17:46-root-INFO: Undo step: 85
2024-12-02-14:17:46-root-INFO: Undo step: 86
2024-12-02-14:17:46-root-INFO: Undo step: 87
2024-12-02-14:17:46-root-INFO: Undo step: 88
2024-12-02-14:17:46-root-INFO: Undo step: 89
2024-12-02-14:17:46-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-14:17:47-root-INFO: grad norm: 63.513 62.518 11.201
2024-12-02-14:17:48-root-INFO: grad norm: 50.171 49.855 5.625
2024-12-02-14:17:49-root-INFO: grad norm: 41.289 41.074 4.213
2024-12-02-14:17:50-root-INFO: grad norm: 29.090 28.935 2.998
2024-12-02-14:17:51-root-INFO: grad norm: 22.203 22.058 2.525
2024-12-02-14:17:52-root-INFO: grad norm: 22.737 22.609 2.409
2024-12-02-14:17:53-root-INFO: grad norm: 24.896 24.761 2.597
2024-12-02-14:17:54-root-INFO: grad norm: 26.961 26.788 3.055
2024-12-02-14:17:54-root-INFO: Loss Change: 470.207 -> 181.372
2024-12-02-14:17:54-root-INFO: Regularization Change: 0.000 -> 131.762
2024-12-02-14:17:54-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-14:17:54-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-14:17:55-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-14:17:55-root-INFO: grad norm: 33.159 32.914 4.022
2024-12-02-14:17:55-root-INFO: Loss too large (184.538->190.081)! Learning rate decreased to 0.08399.
2024-12-02-14:17:56-root-INFO: grad norm: 24.031 23.744 3.704
2024-12-02-14:17:57-root-INFO: grad norm: 17.992 17.819 2.488
2024-12-02-14:17:58-root-INFO: grad norm: 17.250 17.004 2.902
2024-12-02-14:17:59-root-INFO: grad norm: 18.087 17.897 2.617
2024-12-02-14:18:00-root-INFO: grad norm: 19.686 19.424 3.201
2024-12-02-14:18:01-root-INFO: grad norm: 21.690 21.471 3.075
2024-12-02-14:18:02-root-INFO: Loss too large (154.299->155.460)! Learning rate decreased to 0.06719.
2024-12-02-14:18:03-root-INFO: grad norm: 15.834 15.618 2.606
2024-12-02-14:18:04-root-INFO: Loss Change: 184.538 -> 145.367
2024-12-02-14:18:04-root-INFO: Regularization Change: 0.000 -> 12.465
2024-12-02-14:18:04-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-14:18:04-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-14:18:04-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-14:18:04-root-INFO: grad norm: 14.009 13.834 2.207
2024-12-02-14:18:05-root-INFO: Loss too large (146.706->151.554)! Learning rate decreased to 0.08623.
2024-12-02-14:18:06-root-INFO: grad norm: 16.089 15.898 2.469
2024-12-02-14:18:06-root-INFO: Loss too large (146.448->146.972)! Learning rate decreased to 0.06899.
2024-12-02-14:18:07-root-INFO: grad norm: 12.121 11.999 1.717
2024-12-02-14:18:08-root-INFO: grad norm: 9.336 9.207 1.548
2024-12-02-14:18:09-root-INFO: grad norm: 7.620 7.526 1.195
2024-12-02-14:18:10-root-INFO: grad norm: 6.409 6.300 1.177
2024-12-02-14:18:11-root-INFO: grad norm: 5.581 5.496 0.969
2024-12-02-14:18:12-root-INFO: grad norm: 4.984 4.884 0.995
2024-12-02-14:18:12-root-INFO: Loss Change: 146.706 -> 135.480
2024-12-02-14:18:12-root-INFO: Regularization Change: 0.000 -> 4.613
2024-12-02-14:18:12-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-14:18:12-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-14:18:13-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-14:18:13-root-INFO: grad norm: 8.319 8.138 1.723
2024-12-02-14:18:13-root-INFO: Loss too large (135.906->137.151)! Learning rate decreased to 0.08852.
2024-12-02-14:18:14-root-INFO: grad norm: 9.571 9.416 1.715
2024-12-02-14:18:15-root-INFO: grad norm: 11.537 11.393 1.816
2024-12-02-14:18:16-root-INFO: Loss too large (135.286->136.032)! Learning rate decreased to 0.07082.
2024-12-02-14:18:17-root-INFO: grad norm: 9.526 9.392 1.595
2024-12-02-14:18:18-root-INFO: grad norm: 7.802 7.709 1.199
2024-12-02-14:18:19-root-INFO: grad norm: 6.725 6.626 1.151
2024-12-02-14:18:20-root-INFO: grad norm: 5.982 5.903 0.967
2024-12-02-14:18:21-root-INFO: grad norm: 5.399 5.306 0.997
2024-12-02-14:18:21-root-INFO: Loss Change: 135.906 -> 129.575
2024-12-02-14:18:21-root-INFO: Regularization Change: 0.000 -> 3.362
2024-12-02-14:18:21-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-14:18:21-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-14:18:22-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-14:18:22-root-INFO: grad norm: 8.254 8.095 1.612
2024-12-02-14:18:22-root-INFO: Loss too large (130.239->132.091)! Learning rate decreased to 0.09086.
2024-12-02-14:18:23-root-INFO: Loss too large (130.239->130.402)! Learning rate decreased to 0.07268.
2024-12-02-14:18:24-root-INFO: grad norm: 6.904 6.796 1.212
2024-12-02-14:18:25-root-INFO: grad norm: 5.624 5.545 0.938
2024-12-02-14:18:26-root-INFO: grad norm: 5.066 4.986 0.895
2024-12-02-14:18:27-root-INFO: grad norm: 4.786 4.718 0.803
2024-12-02-14:18:28-root-INFO: grad norm: 4.515 4.433 0.857
2024-12-02-14:18:29-root-INFO: grad norm: 4.349 4.283 0.753
2024-12-02-14:18:30-root-INFO: grad norm: 4.184 4.102 0.822
2024-12-02-14:18:31-root-INFO: Loss Change: 130.239 -> 125.234
2024-12-02-14:18:31-root-INFO: Regularization Change: 0.000 -> 2.405
2024-12-02-14:18:31-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-14:18:31-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-14:18:31-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-14:18:31-root-INFO: grad norm: 7.077 6.929 1.441
2024-12-02-14:18:32-root-INFO: Loss too large (125.620->126.899)! Learning rate decreased to 0.09324.
2024-12-02-14:18:32-root-INFO: Loss too large (125.620->125.771)! Learning rate decreased to 0.07459.
2024-12-02-14:18:33-root-INFO: grad norm: 5.921 5.825 1.062
2024-12-02-14:18:34-root-INFO: grad norm: 4.666 4.596 0.805
2024-12-02-14:18:35-root-INFO: grad norm: 4.321 4.254 0.760
2024-12-02-14:18:36-root-INFO: grad norm: 4.282 4.222 0.719
2024-12-02-14:18:37-root-INFO: grad norm: 4.166 4.092 0.778
2024-12-02-14:18:38-root-INFO: grad norm: 4.167 4.106 0.711
2024-12-02-14:18:39-root-INFO: grad norm: 4.068 3.992 0.781
2024-12-02-14:18:40-root-INFO: Loss Change: 125.620 -> 121.591
2024-12-02-14:18:40-root-INFO: Regularization Change: 0.000 -> 2.069
2024-12-02-14:18:40-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-14:18:40-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-14:18:40-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-14:18:40-root-INFO: grad norm: 5.906 5.793 1.150
2024-12-02-14:18:41-root-INFO: Loss too large (121.728->122.649)! Learning rate decreased to 0.09566.
2024-12-02-14:18:41-root-INFO: Loss too large (121.728->121.819)! Learning rate decreased to 0.07653.
2024-12-02-14:18:42-root-INFO: grad norm: 5.241 5.156 0.941
2024-12-02-14:18:43-root-INFO: grad norm: 4.522 4.456 0.772
2024-12-02-14:18:44-root-INFO: grad norm: 4.343 4.271 0.786
2024-12-02-14:18:45-root-INFO: grad norm: 4.345 4.285 0.720
2024-12-02-14:18:46-root-INFO: grad norm: 4.214 4.140 0.785
2024-12-02-14:18:47-root-INFO: grad norm: 4.078 4.020 0.685
2024-12-02-14:18:48-root-INFO: grad norm: 3.999 3.927 0.754
2024-12-02-14:18:49-root-INFO: Loss Change: 121.728 -> 118.382
2024-12-02-14:18:49-root-INFO: Regularization Change: 0.000 -> 1.844
2024-12-02-14:18:49-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-14:18:49-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-14:18:49-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-14:18:49-root-INFO: grad norm: 6.745 6.606 1.364
2024-12-02-14:18:50-root-INFO: Loss too large (118.797->120.258)! Learning rate decreased to 0.09814.
2024-12-02-14:18:50-root-INFO: Loss too large (118.797->119.084)! Learning rate decreased to 0.07851.
2024-12-02-14:18:51-root-INFO: grad norm: 5.735 5.648 0.995
2024-12-02-14:18:52-root-INFO: grad norm: 4.664 4.599 0.776
2024-12-02-14:18:53-root-INFO: grad norm: 4.428 4.363 0.756
2024-12-02-14:18:54-root-INFO: grad norm: 4.514 4.456 0.718
2024-12-02-14:18:55-root-INFO: grad norm: 4.369 4.299 0.783
2024-12-02-14:18:56-root-INFO: grad norm: 4.237 4.181 0.689
2024-12-02-14:18:57-root-INFO: grad norm: 4.142 4.072 0.755
2024-12-02-14:18:58-root-INFO: Loss Change: 118.797 -> 115.496
2024-12-02-14:18:58-root-INFO: Regularization Change: 0.000 -> 1.741
2024-12-02-14:18:58-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-14:18:58-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-14:18:58-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-14:18:58-root-INFO: grad norm: 6.387 6.273 1.203
2024-12-02-14:18:59-root-INFO: Loss too large (116.012->117.363)! Learning rate decreased to 0.10066.
2024-12-02-14:18:59-root-INFO: Loss too large (116.012->116.356)! Learning rate decreased to 0.08053.
2024-12-02-14:19:00-root-INFO: grad norm: 5.356 5.273 0.936
2024-12-02-14:19:01-root-INFO: grad norm: 4.102 4.048 0.667
2024-12-02-14:19:02-root-INFO: grad norm: 3.874 3.820 0.646
2024-12-02-14:19:03-root-INFO: grad norm: 3.992 3.944 0.622
2024-12-02-14:19:04-root-INFO: grad norm: 3.962 3.901 0.696
2024-12-02-14:19:05-root-INFO: grad norm: 4.079 4.028 0.646
2024-12-02-14:19:06-root-INFO: grad norm: 4.001 3.936 0.719
2024-12-02-14:19:07-root-INFO: Loss Change: 116.012 -> 113.019
2024-12-02-14:19:07-root-INFO: Regularization Change: 0.000 -> 1.642
2024-12-02-14:19:07-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-14:19:07-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-14:19:07-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-14:19:07-root-INFO: grad norm: 6.735 6.598 1.354
2024-12-02-14:19:08-root-INFO: Loss too large (113.370->115.015)! Learning rate decreased to 0.10323.
2024-12-02-14:19:08-root-INFO: Loss too large (113.370->113.809)! Learning rate decreased to 0.08259.
2024-12-02-14:19:09-root-INFO: grad norm: 5.619 5.540 0.942
2024-12-02-14:19:10-root-INFO: grad norm: 4.338 4.280 0.707
2024-12-02-14:19:11-root-INFO: grad norm: 4.113 4.060 0.655
2024-12-02-14:19:12-root-INFO: grad norm: 4.298 4.247 0.659
2024-12-02-14:19:13-root-INFO: grad norm: 4.232 4.172 0.716
2024-12-02-14:19:14-root-INFO: grad norm: 4.254 4.202 0.663
2024-12-02-14:19:15-root-INFO: grad norm: 4.162 4.099 0.720
2024-12-02-14:19:16-root-INFO: Loss Change: 113.370 -> 110.401
2024-12-02-14:19:16-root-INFO: Regularization Change: 0.000 -> 1.606
2024-12-02-14:19:16-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-14:19:16-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-14:19:16-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-14:19:16-root-INFO: grad norm: 7.340 7.196 1.449
2024-12-02-14:19:17-root-INFO: Loss too large (111.196->113.300)! Learning rate decreased to 0.10585.
2024-12-02-14:19:17-root-INFO: Loss too large (111.196->111.823)! Learning rate decreased to 0.08468.
2024-12-02-14:19:18-root-INFO: grad norm: 5.976 5.895 0.980
2024-12-02-14:19:19-root-INFO: grad norm: 4.484 4.432 0.684
2024-12-02-14:19:20-root-INFO: grad norm: 4.098 4.052 0.608
2024-12-02-14:19:21-root-INFO: grad norm: 4.141 4.097 0.602
2024-12-02-14:19:22-root-INFO: grad norm: 4.155 4.101 0.668
2024-12-02-14:19:23-root-INFO: grad norm: 4.508 4.459 0.667
2024-12-02-14:19:24-root-INFO: grad norm: 4.377 4.315 0.737
2024-12-02-14:19:25-root-INFO: Loss Change: 111.196 -> 108.159
2024-12-02-14:19:25-root-INFO: Regularization Change: 0.000 -> 1.591
2024-12-02-14:19:25-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-14:19:25-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-14:19:25-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-14:19:26-root-INFO: grad norm: 6.594 6.464 1.301
2024-12-02-14:19:26-root-INFO: Loss too large (108.516->110.288)! Learning rate decreased to 0.10852.
2024-12-02-14:19:26-root-INFO: Loss too large (108.516->109.024)! Learning rate decreased to 0.08682.
2024-12-02-14:19:27-root-INFO: grad norm: 5.621 5.547 0.905
2024-12-02-14:19:29-root-INFO: grad norm: 4.507 4.450 0.715
2024-12-02-14:19:30-root-INFO: grad norm: 4.394 4.340 0.686
2024-12-02-14:19:31-root-INFO: grad norm: 4.794 4.741 0.714
2024-12-02-14:19:32-root-INFO: grad norm: 4.596 4.533 0.756
2024-12-02-14:19:33-root-INFO: grad norm: 4.242 4.192 0.649
2024-12-02-14:19:34-root-INFO: grad norm: 4.244 4.186 0.698
2024-12-02-14:19:34-root-INFO: Loss Change: 108.516 -> 105.748
2024-12-02-14:19:34-root-INFO: Regularization Change: 0.000 -> 1.576
2024-12-02-14:19:34-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-14:19:34-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-14:19:35-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-14:19:35-root-INFO: grad norm: 6.263 6.159 1.138
2024-12-02-14:19:35-root-INFO: Loss too large (106.296->108.041)! Learning rate decreased to 0.11124.
2024-12-02-14:19:36-root-INFO: Loss too large (106.296->106.875)! Learning rate decreased to 0.08899.
2024-12-02-14:19:37-root-INFO: grad norm: 5.457 5.387 0.871
2024-12-02-14:19:38-root-INFO: grad norm: 4.390 4.338 0.672
2024-12-02-14:19:39-root-INFO: grad norm: 4.328 4.277 0.663
2024-12-02-14:19:40-root-INFO: grad norm: 4.795 4.745 0.691
2024-12-02-14:19:41-root-INFO: grad norm: 4.623 4.562 0.746
2024-12-02-14:19:42-root-INFO: grad norm: 4.298 4.250 0.639
2024-12-02-14:19:43-root-INFO: grad norm: 4.318 4.262 0.696
2024-12-02-14:19:43-root-INFO: Loss Change: 106.296 -> 103.756
2024-12-02-14:19:43-root-INFO: Regularization Change: 0.000 -> 1.537
2024-12-02-14:19:43-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-14:19:43-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-14:19:44-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-14:19:44-root-INFO: grad norm: 6.819 6.705 1.245
2024-12-02-14:19:44-root-INFO: Loss too large (103.986->106.011)! Learning rate decreased to 0.11401.
2024-12-02-14:19:45-root-INFO: Loss too large (103.986->104.711)! Learning rate decreased to 0.09121.
2024-12-02-14:19:46-root-INFO: grad norm: 5.540 5.465 0.908
2024-12-02-14:19:47-root-INFO: grad norm: 3.977 3.934 0.584
2024-12-02-14:19:48-root-INFO: grad norm: 3.653 3.611 0.548
2024-12-02-14:19:49-root-INFO: grad norm: 3.822 3.785 0.534
2024-12-02-14:19:50-root-INFO: grad norm: 3.969 3.919 0.629
2024-12-02-14:19:51-root-INFO: grad norm: 4.566 4.520 0.642
2024-12-02-14:19:52-root-INFO: grad norm: 4.381 4.321 0.723
2024-12-02-14:19:52-root-INFO: Loss Change: 103.986 -> 101.274
2024-12-02-14:19:52-root-INFO: Regularization Change: 0.000 -> 1.556
2024-12-02-14:19:52-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-14:19:52-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-14:19:53-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-14:19:53-root-INFO: grad norm: 6.812 6.695 1.260
2024-12-02-14:19:53-root-INFO: Loss too large (101.829->103.748)! Learning rate decreased to 0.11682.
2024-12-02-14:19:54-root-INFO: Loss too large (101.829->102.512)! Learning rate decreased to 0.09346.
2024-12-02-14:19:55-root-INFO: grad norm: 5.315 5.243 0.877
2024-12-02-14:19:56-root-INFO: grad norm: 3.614 3.574 0.534
2024-12-02-14:19:57-root-INFO: grad norm: 3.215 3.180 0.474
2024-12-02-14:19:58-root-INFO: grad norm: 3.209 3.177 0.456
2024-12-02-14:19:59-root-INFO: grad norm: 3.344 3.303 0.522
2024-12-02-14:20:00-root-INFO: grad norm: 3.949 3.909 0.556
2024-12-02-14:20:01-root-INFO: grad norm: 3.996 3.943 0.646
2024-12-02-14:20:02-root-INFO: Loss Change: 101.829 -> 99.146
2024-12-02-14:20:02-root-INFO: Regularization Change: 0.000 -> 1.566
2024-12-02-14:20:02-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-14:20:02-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-14:20:02-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-14:20:02-root-INFO: grad norm: 5.742 5.646 1.046
2024-12-02-14:20:03-root-INFO: Loss too large (99.475->100.978)! Learning rate decreased to 0.11969.
2024-12-02-14:20:03-root-INFO: Loss too large (99.475->99.985)! Learning rate decreased to 0.09575.
2024-12-02-14:20:04-root-INFO: grad norm: 4.960 4.897 0.789
2024-12-02-14:20:05-root-INFO: grad norm: 3.957 3.910 0.608
2024-12-02-14:20:06-root-INFO: grad norm: 3.990 3.941 0.620
2024-12-02-14:20:07-root-INFO: grad norm: 4.524 4.478 0.638
2024-12-02-14:20:08-root-INFO: grad norm: 4.323 4.267 0.692
2024-12-02-14:20:09-root-INFO: grad norm: 3.899 3.857 0.569
2024-12-02-14:20:10-root-INFO: grad norm: 3.994 3.943 0.634
2024-12-02-14:20:11-root-INFO: Loss Change: 99.475 -> 97.130
2024-12-02-14:20:11-root-INFO: Regularization Change: 0.000 -> 1.552
2024-12-02-14:20:11-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-14:20:11-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-14:20:11-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-14:20:11-root-INFO: grad norm: 6.092 5.999 1.062
2024-12-02-14:20:12-root-INFO: Loss too large (97.554->99.274)! Learning rate decreased to 0.12261.
2024-12-02-14:20:12-root-INFO: Loss too large (97.554->98.197)! Learning rate decreased to 0.09809.
2024-12-02-14:20:13-root-INFO: grad norm: 5.016 4.951 0.810
2024-12-02-14:20:14-root-INFO: grad norm: 3.596 3.554 0.548
2024-12-02-14:20:15-root-INFO: grad norm: 3.497 3.455 0.540
2024-12-02-14:20:16-root-INFO: grad norm: 3.945 3.906 0.556
2024-12-02-14:20:17-root-INFO: grad norm: 4.058 4.007 0.636
2024-12-02-14:20:19-root-INFO: grad norm: 4.346 4.303 0.614
2024-12-02-14:20:20-root-INFO: grad norm: 4.257 4.203 0.677
2024-12-02-14:20:20-root-INFO: Loss Change: 97.554 -> 95.161
2024-12-02-14:20:20-root-INFO: Regularization Change: 0.000 -> 1.555
2024-12-02-14:20:20-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-14:20:20-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-14:20:21-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-14:20:21-root-INFO: grad norm: 5.765 5.667 1.058
2024-12-02-14:20:21-root-INFO: Loss too large (95.493->97.068)! Learning rate decreased to 0.12558.
2024-12-02-14:20:22-root-INFO: Loss too large (95.493->96.030)! Learning rate decreased to 0.10047.
2024-12-02-14:20:23-root-INFO: grad norm: 4.897 4.836 0.769
2024-12-02-14:20:24-root-INFO: grad norm: 3.831 3.785 0.594
2024-12-02-14:20:25-root-INFO: grad norm: 3.890 3.843 0.602
2024-12-02-14:20:26-root-INFO: grad norm: 4.456 4.412 0.630
2024-12-02-14:20:26-root-INFO: Loss too large (94.141->94.152)! Learning rate decreased to 0.08037.
2024-12-02-14:20:27-root-INFO: grad norm: 3.431 3.388 0.542
2024-12-02-14:20:28-root-INFO: grad norm: 2.443 2.413 0.378
2024-12-02-14:20:29-root-INFO: grad norm: 2.256 2.224 0.377
2024-12-02-14:20:30-root-INFO: Loss Change: 95.493 -> 93.086
2024-12-02-14:20:30-root-INFO: Regularization Change: 0.000 -> 1.317
2024-12-02-14:20:30-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-14:20:30-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-14:20:30-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-14:20:30-root-INFO: grad norm: 3.671 3.598 0.728
2024-12-02-14:20:31-root-INFO: Loss too large (93.178->93.600)! Learning rate decreased to 0.12860.
2024-12-02-14:20:31-root-INFO: Loss too large (93.178->93.183)! Learning rate decreased to 0.10288.
2024-12-02-14:20:32-root-INFO: grad norm: 3.622 3.576 0.574
2024-12-02-14:20:33-root-INFO: grad norm: 4.405 4.359 0.633
2024-12-02-14:20:33-root-INFO: Loss too large (92.685->92.766)! Learning rate decreased to 0.08231.
2024-12-02-14:20:34-root-INFO: grad norm: 3.484 3.441 0.547
2024-12-02-14:20:35-root-INFO: grad norm: 2.385 2.356 0.369
2024-12-02-14:20:36-root-INFO: grad norm: 2.290 2.259 0.375
2024-12-02-14:20:37-root-INFO: grad norm: 2.231 2.205 0.335
2024-12-02-14:20:38-root-INFO: grad norm: 2.211 2.181 0.363
2024-12-02-14:20:39-root-INFO: Loss Change: 93.178 -> 91.287
2024-12-02-14:20:39-root-INFO: Regularization Change: 0.000 -> 1.179
2024-12-02-14:20:39-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-14:20:39-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-14:20:39-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-14:20:40-root-INFO: grad norm: 3.410 3.358 0.595
2024-12-02-14:20:40-root-INFO: Loss too large (91.523->92.003)! Learning rate decreased to 0.13168.
2024-12-02-14:20:40-root-INFO: Loss too large (91.523->91.631)! Learning rate decreased to 0.10534.
2024-12-02-14:20:41-root-INFO: grad norm: 3.672 3.624 0.590
2024-12-02-14:20:42-root-INFO: grad norm: 4.624 4.576 0.667
2024-12-02-14:20:43-root-INFO: Loss too large (91.164->91.324)! Learning rate decreased to 0.08427.
2024-12-02-14:20:44-root-INFO: grad norm: 3.691 3.643 0.590
2024-12-02-14:20:45-root-INFO: grad norm: 2.503 2.471 0.395
2024-12-02-14:20:46-root-INFO: grad norm: 2.393 2.360 0.394
2024-12-02-14:20:47-root-INFO: grad norm: 2.339 2.313 0.351
2024-12-02-14:20:48-root-INFO: grad norm: 2.321 2.290 0.378
2024-12-02-14:20:48-root-INFO: Loss Change: 91.523 -> 89.761
2024-12-02-14:20:48-root-INFO: Regularization Change: 0.000 -> 1.163
2024-12-02-14:20:48-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-14:20:48-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-14:20:49-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-14:20:49-root-INFO: grad norm: 4.793 4.692 0.978
2024-12-02-14:20:49-root-INFO: Loss too large (89.947->90.981)! Learning rate decreased to 0.13480.
2024-12-02-14:20:50-root-INFO: Loss too large (89.947->90.269)! Learning rate decreased to 0.10784.
2024-12-02-14:20:51-root-INFO: grad norm: 4.346 4.292 0.683
2024-12-02-14:20:52-root-INFO: grad norm: 4.248 4.197 0.658
2024-12-02-14:20:52-root-INFO: Loss too large (89.220->89.286)! Learning rate decreased to 0.08627.
2024-12-02-14:20:53-root-INFO: grad norm: 3.465 3.424 0.534
2024-12-02-14:20:54-root-INFO: grad norm: 2.646 2.615 0.403
2024-12-02-14:20:55-root-INFO: grad norm: 2.531 2.500 0.395
2024-12-02-14:20:56-root-INFO: grad norm: 2.435 2.409 0.357
2024-12-02-14:20:57-root-INFO: grad norm: 2.402 2.372 0.376
2024-12-02-14:20:58-root-INFO: Loss Change: 89.947 -> 87.846
2024-12-02-14:20:58-root-INFO: Regularization Change: 0.000 -> 1.250
2024-12-02-14:20:58-root-INFO: Undo step: 70
2024-12-02-14:20:58-root-INFO: Undo step: 71
2024-12-02-14:20:58-root-INFO: Undo step: 72
2024-12-02-14:20:58-root-INFO: Undo step: 73
2024-12-02-14:20:58-root-INFO: Undo step: 74
2024-12-02-14:20:58-root-INFO: Undo step: 75
2024-12-02-14:20:58-root-INFO: Undo step: 76
2024-12-02-14:20:58-root-INFO: Undo step: 77
2024-12-02-14:20:58-root-INFO: Undo step: 78
2024-12-02-14:20:58-root-INFO: Undo step: 79
2024-12-02-14:20:58-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-14:20:58-root-INFO: grad norm: 49.908 49.267 7.976
2024-12-02-14:20:59-root-INFO: grad norm: 34.974 34.632 4.877
2024-12-02-14:21:00-root-INFO: grad norm: 30.085 29.772 4.328
2024-12-02-14:21:01-root-INFO: grad norm: 25.533 25.317 3.317
2024-12-02-14:21:02-root-INFO: grad norm: 26.193 25.851 4.216
2024-12-02-14:21:03-root-INFO: grad norm: 33.152 32.834 4.583
2024-12-02-14:21:04-root-INFO: Loss too large (167.226->170.904)! Learning rate decreased to 0.10585.
2024-12-02-14:21:05-root-INFO: grad norm: 20.815 20.499 3.614
2024-12-02-14:21:06-root-INFO: grad norm: 14.913 14.788 1.931
2024-12-02-14:21:06-root-INFO: Loss Change: 400.878 -> 135.295
2024-12-02-14:21:06-root-INFO: Regularization Change: 0.000 -> 143.041
2024-12-02-14:21:06-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-14:21:06-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-14:21:07-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-14:21:07-root-INFO: grad norm: 11.860 11.737 1.703
2024-12-02-14:21:08-root-INFO: grad norm: 15.156 15.025 1.987
2024-12-02-14:21:08-root-INFO: Loss too large (132.913->134.546)! Learning rate decreased to 0.10852.
2024-12-02-14:21:09-root-INFO: grad norm: 13.660 13.507 2.035
2024-12-02-14:21:10-root-INFO: grad norm: 12.595 12.472 1.751
2024-12-02-14:21:12-root-INFO: grad norm: 11.963 11.827 1.800
2024-12-02-14:21:13-root-INFO: grad norm: 11.560 11.445 1.626
2024-12-02-14:21:14-root-INFO: grad norm: 11.338 11.212 1.686
2024-12-02-14:21:15-root-INFO: grad norm: 11.231 11.120 1.574
2024-12-02-14:21:15-root-INFO: Loss Change: 133.977 -> 116.960
2024-12-02-14:21:15-root-INFO: Regularization Change: 0.000 -> 14.609
2024-12-02-14:21:15-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-14:21:15-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-14:21:16-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-14:21:16-root-INFO: grad norm: 9.894 9.813 1.262
2024-12-02-14:21:16-root-INFO: Loss too large (116.226->117.652)! Learning rate decreased to 0.11124.
2024-12-02-14:21:17-root-INFO: grad norm: 9.959 9.863 1.376
2024-12-02-14:21:18-root-INFO: grad norm: 10.098 10.002 1.383
2024-12-02-14:21:19-root-INFO: grad norm: 10.390 10.290 1.439
2024-12-02-14:21:20-root-INFO: grad norm: 10.794 10.682 1.546
2024-12-02-14:21:21-root-INFO: grad norm: 11.568 11.452 1.637
2024-12-02-14:21:22-root-INFO: grad norm: 11.873 11.740 1.772
2024-12-02-14:21:23-root-INFO: grad norm: 11.993 11.873 1.693
2024-12-02-14:21:24-root-INFO: Loss Change: 116.226 -> 110.178
2024-12-02-14:21:24-root-INFO: Regularization Change: 0.000 -> 6.703
2024-12-02-14:21:24-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-14:21:24-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-14:21:24-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-14:21:25-root-INFO: grad norm: 11.040 10.950 1.405
2024-12-02-14:21:25-root-INFO: Loss too large (108.880->112.695)! Learning rate decreased to 0.11401.
2024-12-02-14:21:26-root-INFO: grad norm: 11.842 11.727 1.642
2024-12-02-14:21:27-root-INFO: grad norm: 11.726 11.600 1.710
2024-12-02-14:21:28-root-INFO: grad norm: 11.474 11.368 1.553
2024-12-02-14:21:29-root-INFO: grad norm: 11.456 11.347 1.572
2024-12-02-14:21:30-root-INFO: grad norm: 11.774 11.666 1.594
2024-12-02-14:21:31-root-INFO: grad norm: 11.975 11.852 1.710
2024-12-02-14:21:32-root-INFO: grad norm: 12.436 12.316 1.726
2024-12-02-14:21:33-root-INFO: Loss Change: 108.880 -> 105.319
2024-12-02-14:21:33-root-INFO: Regularization Change: 0.000 -> 4.627
2024-12-02-14:21:33-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-14:21:33-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-14:21:33-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-14:21:33-root-INFO: grad norm: 11.309 11.223 1.390
2024-12-02-14:21:34-root-INFO: Loss too large (104.417->107.600)! Learning rate decreased to 0.11682.
2024-12-02-14:21:35-root-INFO: grad norm: 10.818 10.728 1.397
2024-12-02-14:21:36-root-INFO: grad norm: 10.716 10.623 1.412
2024-12-02-14:21:37-root-INFO: grad norm: 11.739 11.637 1.544
2024-12-02-14:21:37-root-INFO: Loss too large (102.562->102.636)! Learning rate decreased to 0.09346.
2024-12-02-14:21:38-root-INFO: grad norm: 7.184 7.103 1.074
2024-12-02-14:21:39-root-INFO: grad norm: 4.627 4.582 0.646
2024-12-02-14:21:40-root-INFO: grad norm: 3.329 3.293 0.492
2024-12-02-14:21:41-root-INFO: grad norm: 2.777 2.738 0.469
2024-12-02-14:21:42-root-INFO: Loss Change: 104.417 -> 97.082
2024-12-02-14:21:42-root-INFO: Regularization Change: 0.000 -> 3.256
2024-12-02-14:21:42-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-14:21:42-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-14:21:42-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-14:21:43-root-INFO: grad norm: 3.330 3.259 0.683
2024-12-02-14:21:44-root-INFO: grad norm: 4.393 4.329 0.748
2024-12-02-14:21:44-root-INFO: Loss too large (96.891->97.133)! Learning rate decreased to 0.11969.
2024-12-02-14:21:45-root-INFO: grad norm: 5.837 5.777 0.837
2024-12-02-14:21:45-root-INFO: Loss too large (96.412->96.720)! Learning rate decreased to 0.09575.
2024-12-02-14:21:46-root-INFO: grad norm: 4.077 4.020 0.676
2024-12-02-14:21:47-root-INFO: grad norm: 2.412 2.372 0.440
2024-12-02-14:21:48-root-INFO: grad norm: 2.087 2.051 0.388
2024-12-02-14:21:49-root-INFO: grad norm: 2.041 2.002 0.398
2024-12-02-14:21:50-root-INFO: grad norm: 2.164 2.131 0.380
2024-12-02-14:21:51-root-INFO: Loss Change: 97.097 -> 93.951
2024-12-02-14:21:51-root-INFO: Regularization Change: 0.000 -> 2.574
2024-12-02-14:21:51-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-14:21:51-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-14:21:51-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-14:21:52-root-INFO: grad norm: 3.027 2.962 0.622
2024-12-02-14:21:53-root-INFO: grad norm: 4.578 4.543 0.560
2024-12-02-14:21:53-root-INFO: Loss too large (93.748->94.280)! Learning rate decreased to 0.12261.
2024-12-02-14:21:53-root-INFO: Loss too large (93.748->93.792)! Learning rate decreased to 0.09809.
2024-12-02-14:21:54-root-INFO: grad norm: 3.839 3.803 0.521
2024-12-02-14:21:55-root-INFO: grad norm: 3.374 3.346 0.434
2024-12-02-14:21:56-root-INFO: grad norm: 3.387 3.350 0.498
2024-12-02-14:21:57-root-INFO: grad norm: 3.662 3.631 0.474
2024-12-02-14:21:58-root-INFO: grad norm: 3.504 3.464 0.526
2024-12-02-14:21:59-root-INFO: grad norm: 3.134 3.102 0.447
2024-12-02-14:22:00-root-INFO: Loss Change: 94.056 -> 91.568
2024-12-02-14:22:00-root-INFO: Regularization Change: 0.000 -> 2.209
2024-12-02-14:22:00-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-14:22:00-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-14:22:00-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-14:22:01-root-INFO: grad norm: 3.749 3.700 0.604
2024-12-02-14:22:01-root-INFO: Loss too large (91.541->91.872)! Learning rate decreased to 0.12558.
2024-12-02-14:22:02-root-INFO: grad norm: 5.923 5.891 0.612
2024-12-02-14:22:02-root-INFO: Loss too large (91.299->91.775)! Learning rate decreased to 0.10047.
2024-12-02-14:22:03-root-INFO: Loss too large (91.299->91.397)! Learning rate decreased to 0.08037.
2024-12-02-14:22:04-root-INFO: grad norm: 3.575 3.540 0.506
2024-12-02-14:22:05-root-INFO: grad norm: 1.800 1.768 0.338
2024-12-02-14:22:06-root-INFO: grad norm: 1.715 1.683 0.332
2024-12-02-14:22:07-root-INFO: grad norm: 1.709 1.676 0.334
2024-12-02-14:22:08-root-INFO: grad norm: 1.719 1.687 0.326
2024-12-02-14:22:09-root-INFO: grad norm: 1.736 1.704 0.334
2024-12-02-14:22:10-root-INFO: Loss Change: 91.541 -> 89.228
2024-12-02-14:22:10-root-INFO: Regularization Change: 0.000 -> 1.380
2024-12-02-14:22:10-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-14:22:10-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-14:22:10-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-14:22:10-root-INFO: grad norm: 3.189 3.115 0.685
2024-12-02-14:22:11-root-INFO: Loss too large (89.249->89.285)! Learning rate decreased to 0.12860.
2024-12-02-14:22:12-root-INFO: grad norm: 3.924 3.876 0.613
2024-12-02-14:22:12-root-INFO: Loss too large (89.047->89.255)! Learning rate decreased to 0.10288.
2024-12-02-14:22:13-root-INFO: grad norm: 5.160 5.117 0.666
2024-12-02-14:22:13-root-INFO: Loss too large (88.783->88.986)! Learning rate decreased to 0.08231.
2024-12-02-14:22:14-root-INFO: grad norm: 3.677 3.638 0.533
2024-12-02-14:22:15-root-INFO: grad norm: 1.829 1.799 0.330
2024-12-02-14:22:16-root-INFO: grad norm: 1.803 1.773 0.326
2024-12-02-14:22:17-root-INFO: grad norm: 1.896 1.869 0.317
2024-12-02-14:22:18-root-INFO: grad norm: 2.007 1.978 0.341
2024-12-02-14:22:19-root-INFO: Loss Change: 89.249 -> 87.190
2024-12-02-14:22:19-root-INFO: Regularization Change: 0.000 -> 1.390
2024-12-02-14:22:19-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-14:22:19-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-14:22:19-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-14:22:19-root-INFO: grad norm: 3.279 3.233 0.542
2024-12-02-14:22:20-root-INFO: Loss too large (87.372->87.712)! Learning rate decreased to 0.13168.
2024-12-02-14:22:20-root-INFO: Loss too large (87.372->87.434)! Learning rate decreased to 0.10534.
2024-12-02-14:22:21-root-INFO: grad norm: 3.538 3.501 0.511
2024-12-02-14:22:22-root-INFO: grad norm: 4.849 4.814 0.584
2024-12-02-14:22:23-root-INFO: Loss too large (86.993->87.232)! Learning rate decreased to 0.08427.
2024-12-02-14:22:24-root-INFO: grad norm: 3.603 3.567 0.510
2024-12-02-14:22:25-root-INFO: grad norm: 1.853 1.825 0.320
2024-12-02-14:22:26-root-INFO: grad norm: 1.888 1.860 0.327
2024-12-02-14:22:27-root-INFO: grad norm: 2.069 2.044 0.318
2024-12-02-14:22:28-root-INFO: grad norm: 2.239 2.211 0.354
2024-12-02-14:22:28-root-INFO: Loss Change: 87.372 -> 85.547
2024-12-02-14:22:28-root-INFO: Regularization Change: 0.000 -> 1.238
2024-12-02-14:22:28-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-14:22:28-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-14:22:29-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-14:22:29-root-INFO: grad norm: 4.861 4.764 0.966
2024-12-02-14:22:29-root-INFO: Loss too large (85.687->86.618)! Learning rate decreased to 0.13480.
2024-12-02-14:22:30-root-INFO: Loss too large (85.687->86.019)! Learning rate decreased to 0.10784.
2024-12-02-14:22:31-root-INFO: grad norm: 4.110 4.067 0.591
2024-12-02-14:22:32-root-INFO: grad norm: 3.319 3.280 0.505
2024-12-02-14:22:33-root-INFO: grad norm: 3.610 3.573 0.513
2024-12-02-14:22:34-root-INFO: grad norm: 4.700 4.666 0.569
2024-12-02-14:22:34-root-INFO: Loss too large (84.470->84.717)! Learning rate decreased to 0.08627.
2024-12-02-14:22:35-root-INFO: grad norm: 3.562 3.528 0.491
2024-12-02-14:22:36-root-INFO: grad norm: 1.928 1.903 0.312
2024-12-02-14:22:37-root-INFO: grad norm: 2.006 1.980 0.321
2024-12-02-14:22:38-root-INFO: Loss Change: 85.687 -> 83.440
2024-12-02-14:22:38-root-INFO: Regularization Change: 0.000 -> 1.434
2024-12-02-14:22:38-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-14:22:38-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-14:22:38-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-14:22:38-root-INFO: grad norm: 3.102 3.058 0.524
2024-12-02-14:22:39-root-INFO: Loss too large (83.618->83.966)! Learning rate decreased to 0.13798.
2024-12-02-14:22:39-root-INFO: Loss too large (83.618->83.695)! Learning rate decreased to 0.11038.
2024-12-02-14:22:40-root-INFO: grad norm: 3.459 3.426 0.475
2024-12-02-14:22:41-root-INFO: grad norm: 5.108 5.074 0.587
2024-12-02-14:22:41-root-INFO: Loss too large (83.340->83.675)! Learning rate decreased to 0.08831.
2024-12-02-14:22:42-root-INFO: Loss too large (83.340->83.370)! Learning rate decreased to 0.07065.
2024-12-02-14:22:43-root-INFO: grad norm: 3.348 3.319 0.444
2024-12-02-14:22:44-root-INFO: grad norm: 1.682 1.658 0.281
2024-12-02-14:22:45-root-INFO: grad norm: 1.613 1.588 0.281
2024-12-02-14:22:46-root-INFO: grad norm: 1.567 1.544 0.268
2024-12-02-14:22:47-root-INFO: grad norm: 1.538 1.514 0.274
2024-12-02-14:22:47-root-INFO: Loss Change: 83.618 -> 82.062
2024-12-02-14:22:47-root-INFO: Regularization Change: 0.000 -> 0.910
2024-12-02-14:22:47-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-14:22:47-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-14:22:48-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-14:22:48-root-INFO: grad norm: 3.862 3.767 0.851
2024-12-02-14:22:48-root-INFO: Loss too large (82.198->82.664)! Learning rate decreased to 0.14121.
2024-12-02-14:22:49-root-INFO: Loss too large (82.198->82.200)! Learning rate decreased to 0.11297.
2024-12-02-14:22:50-root-INFO: grad norm: 3.802 3.764 0.540
2024-12-02-14:22:51-root-INFO: grad norm: 5.603 5.563 0.667
2024-12-02-14:22:51-root-INFO: Loss too large (81.777->82.173)! Learning rate decreased to 0.09037.
2024-12-02-14:22:51-root-INFO: Loss too large (81.777->81.812)! Learning rate decreased to 0.07230.
2024-12-02-14:22:52-root-INFO: grad norm: 3.514 3.485 0.456
2024-12-02-14:22:54-root-INFO: grad norm: 1.628 1.604 0.279
2024-12-02-14:22:55-root-INFO: grad norm: 1.568 1.544 0.272
2024-12-02-14:22:56-root-INFO: grad norm: 1.534 1.511 0.264
2024-12-02-14:22:57-root-INFO: grad norm: 1.512 1.488 0.267
2024-12-02-14:22:57-root-INFO: Loss Change: 82.198 -> 80.385
2024-12-02-14:22:57-root-INFO: Regularization Change: 0.000 -> 0.992
2024-12-02-14:22:57-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-14:22:57-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-14:22:58-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-14:22:58-root-INFO: grad norm: 2.365 2.314 0.490
2024-12-02-14:22:59-root-INFO: grad norm: 3.880 3.832 0.603
2024-12-02-14:22:59-root-INFO: Loss too large (80.239->82.024)! Learning rate decreased to 0.14449.
2024-12-02-14:23:00-root-INFO: Loss too large (80.239->80.894)! Learning rate decreased to 0.11559.
2024-12-02-14:23:00-root-INFO: Loss too large (80.239->80.251)! Learning rate decreased to 0.09247.
2024-12-02-14:23:01-root-INFO: grad norm: 4.141 4.112 0.484
2024-12-02-14:23:01-root-INFO: Loss too large (79.921->79.970)! Learning rate decreased to 0.07398.
2024-12-02-14:23:02-root-INFO: grad norm: 3.268 3.241 0.419
2024-12-02-14:23:03-root-INFO: grad norm: 2.309 2.289 0.308
2024-12-02-14:23:04-root-INFO: grad norm: 2.215 2.192 0.314
2024-12-02-14:23:05-root-INFO: grad norm: 2.118 2.098 0.289
2024-12-02-14:23:06-root-INFO: grad norm: 2.074 2.052 0.302
2024-12-02-14:23:07-root-INFO: Loss Change: 80.364 -> 78.832
2024-12-02-14:23:07-root-INFO: Regularization Change: 0.000 -> 1.056
2024-12-02-14:23:07-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-14:23:07-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-14:23:08-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-14:23:08-root-INFO: grad norm: 3.220 3.179 0.509
2024-12-02-14:23:08-root-INFO: Loss too large (78.978->79.583)! Learning rate decreased to 0.14783.
2024-12-02-14:23:09-root-INFO: Loss too large (78.978->79.254)! Learning rate decreased to 0.11826.
2024-12-02-14:23:09-root-INFO: Loss too large (78.978->79.013)! Learning rate decreased to 0.09461.
2024-12-02-14:23:10-root-INFO: grad norm: 3.295 3.269 0.417
2024-12-02-14:23:11-root-INFO: grad norm: 3.936 3.911 0.441
2024-12-02-14:23:11-root-INFO: Loss too large (78.638->78.698)! Learning rate decreased to 0.07569.
2024-12-02-14:23:12-root-INFO: grad norm: 3.231 3.205 0.406
2024-12-02-14:23:13-root-INFO: grad norm: 2.412 2.392 0.310
2024-12-02-14:23:14-root-INFO: grad norm: 2.317 2.294 0.320
2024-12-02-14:23:15-root-INFO: grad norm: 2.215 2.196 0.290
2024-12-02-14:23:17-root-INFO: grad norm: 2.171 2.150 0.307
2024-12-02-14:23:17-root-INFO: Loss Change: 78.978 -> 77.596
2024-12-02-14:23:17-root-INFO: Regularization Change: 0.000 -> 0.810
2024-12-02-14:23:17-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-14:23:17-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-14:23:18-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-14:23:18-root-INFO: grad norm: 3.790 3.725 0.696
2024-12-02-14:23:18-root-INFO: Loss too large (77.779->78.601)! Learning rate decreased to 0.15121.
2024-12-02-14:23:19-root-INFO: Loss too large (77.779->78.132)! Learning rate decreased to 0.12097.
2024-12-02-14:23:19-root-INFO: Loss too large (77.779->77.803)! Learning rate decreased to 0.09678.
2024-12-02-14:23:20-root-INFO: grad norm: 3.557 3.529 0.447
2024-12-02-14:23:21-root-INFO: grad norm: 4.091 4.064 0.464
2024-12-02-14:23:21-root-INFO: Loss too large (77.313->77.399)! Learning rate decreased to 0.07742.
2024-12-02-14:23:22-root-INFO: grad norm: 3.341 3.316 0.410
2024-12-02-14:23:23-root-INFO: grad norm: 2.478 2.458 0.313
2024-12-02-14:23:24-root-INFO: grad norm: 2.410 2.388 0.321
2024-12-02-14:23:25-root-INFO: grad norm: 2.343 2.324 0.294
2024-12-02-14:23:26-root-INFO: grad norm: 2.318 2.297 0.313
2024-12-02-14:23:27-root-INFO: Loss Change: 77.779 -> 76.252
2024-12-02-14:23:27-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-02-14:23:27-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-14:23:27-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-14:23:27-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-14:23:27-root-INFO: grad norm: 3.278 3.236 0.522
2024-12-02-14:23:28-root-INFO: Loss too large (76.298->77.007)! Learning rate decreased to 0.15465.
2024-12-02-14:23:28-root-INFO: Loss too large (76.298->76.644)! Learning rate decreased to 0.12372.
2024-12-02-14:23:28-root-INFO: Loss too large (76.298->76.374)! Learning rate decreased to 0.09898.
2024-12-02-14:23:30-root-INFO: grad norm: 3.442 3.416 0.422
2024-12-02-14:23:31-root-INFO: grad norm: 4.301 4.276 0.463
2024-12-02-14:23:31-root-INFO: Loss too large (76.001->76.128)! Learning rate decreased to 0.07918.
2024-12-02-14:23:32-root-INFO: grad norm: 3.473 3.448 0.418
2024-12-02-14:23:33-root-INFO: grad norm: 2.504 2.485 0.309
2024-12-02-14:23:34-root-INFO: grad norm: 2.472 2.451 0.322
2024-12-02-14:23:35-root-INFO: grad norm: 2.449 2.431 0.297
2024-12-02-14:23:36-root-INFO: grad norm: 2.445 2.425 0.319
2024-12-02-14:23:37-root-INFO: Loss Change: 76.298 -> 74.923
2024-12-02-14:23:37-root-INFO: Regularization Change: 0.000 -> 0.851
2024-12-02-14:23:37-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-14:23:37-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-14:23:37-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-14:23:37-root-INFO: grad norm: 3.489 3.436 0.601
2024-12-02-14:23:38-root-INFO: Loss too large (75.148->75.918)! Learning rate decreased to 0.15815.
2024-12-02-14:23:38-root-INFO: Loss too large (75.148->75.522)! Learning rate decreased to 0.12652.
2024-12-02-14:23:38-root-INFO: Loss too large (75.148->75.225)! Learning rate decreased to 0.10121.
2024-12-02-14:23:39-root-INFO: grad norm: 3.512 3.487 0.426
2024-12-02-14:23:40-root-INFO: grad norm: 4.186 4.161 0.459
2024-12-02-14:23:41-root-INFO: Loss too large (74.784->74.895)! Learning rate decreased to 0.08097.
2024-12-02-14:23:42-root-INFO: grad norm: 3.407 3.382 0.408
2024-12-02-14:23:43-root-INFO: grad norm: 2.522 2.502 0.310
2024-12-02-14:23:44-root-INFO: grad norm: 2.465 2.444 0.318
2024-12-02-14:23:45-root-INFO: grad norm: 2.412 2.394 0.293
2024-12-02-14:23:46-root-INFO: grad norm: 2.393 2.373 0.312
2024-12-02-14:23:46-root-INFO: Loss Change: 75.148 -> 73.709
2024-12-02-14:23:46-root-INFO: Regularization Change: 0.000 -> 0.882
2024-12-02-14:23:46-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-14:23:46-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-14:23:47-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-14:23:47-root-INFO: grad norm: 3.942 3.878 0.705
2024-12-02-14:23:47-root-INFO: Loss too large (73.730->74.717)! Learning rate decreased to 0.16169.
2024-12-02-14:23:48-root-INFO: Loss too large (73.730->74.205)! Learning rate decreased to 0.12935.
2024-12-02-14:23:48-root-INFO: Loss too large (73.730->73.828)! Learning rate decreased to 0.10348.
2024-12-02-14:23:49-root-INFO: grad norm: 3.701 3.673 0.461
2024-12-02-14:23:50-root-INFO: grad norm: 4.068 4.042 0.459
2024-12-02-14:23:50-root-INFO: Loss too large (73.237->73.324)! Learning rate decreased to 0.08279.
2024-12-02-14:23:51-root-INFO: grad norm: 3.356 3.332 0.404
2024-12-02-14:23:53-root-INFO: grad norm: 2.599 2.580 0.315
2024-12-02-14:23:53-root-INFO: grad norm: 2.517 2.497 0.321
2024-12-02-14:23:54-root-INFO: grad norm: 2.438 2.421 0.293
2024-12-02-14:23:56-root-INFO: grad norm: 2.408 2.388 0.310
2024-12-02-14:23:56-root-INFO: Loss Change: 73.730 -> 72.155
2024-12-02-14:23:56-root-INFO: Regularization Change: 0.000 -> 0.936
2024-12-02-14:23:56-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-14:23:56-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-14:23:57-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-14:23:57-root-INFO: grad norm: 3.237 3.205 0.455
2024-12-02-14:23:57-root-INFO: Loss too large (72.276->73.065)! Learning rate decreased to 0.16529.
2024-12-02-14:23:58-root-INFO: Loss too large (72.276->72.702)! Learning rate decreased to 0.13223.
2024-12-02-14:23:58-root-INFO: Loss too large (72.276->72.414)! Learning rate decreased to 0.10579.
2024-12-02-14:23:59-root-INFO: grad norm: 3.459 3.434 0.414
2024-12-02-14:24:00-root-INFO: grad norm: 4.069 4.046 0.430
2024-12-02-14:24:00-root-INFO: Loss too large (71.989->72.069)! Learning rate decreased to 0.08463.
2024-12-02-14:24:01-root-INFO: grad norm: 3.321 3.297 0.395
2024-12-02-14:24:02-root-INFO: grad norm: 2.554 2.536 0.301
2024-12-02-14:24:03-root-INFO: grad norm: 2.446 2.426 0.311
2024-12-02-14:24:04-root-INFO: grad norm: 2.338 2.321 0.278
2024-12-02-14:24:05-root-INFO: grad norm: 2.288 2.269 0.296
2024-12-02-14:24:06-root-INFO: Loss Change: 72.276 -> 70.895
2024-12-02-14:24:06-root-INFO: Regularization Change: 0.000 -> 0.906
2024-12-02-14:24:06-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-14:24:06-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-14:24:06-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-14:24:07-root-INFO: grad norm: 3.340 3.291 0.571
2024-12-02-14:24:07-root-INFO: Loss too large (71.068->71.868)! Learning rate decreased to 0.16894.
2024-12-02-14:24:07-root-INFO: Loss too large (71.068->71.451)! Learning rate decreased to 0.13515.
2024-12-02-14:24:08-root-INFO: Loss too large (71.068->71.141)! Learning rate decreased to 0.10812.
2024-12-02-14:24:09-root-INFO: grad norm: 3.474 3.450 0.410
2024-12-02-14:24:10-root-INFO: grad norm: 4.266 4.242 0.450
2024-12-02-14:24:10-root-INFO: Loss too large (70.755->70.869)! Learning rate decreased to 0.08650.
2024-12-02-14:24:11-root-INFO: grad norm: 3.455 3.432 0.401
2024-12-02-14:24:12-root-INFO: grad norm: 2.624 2.606 0.307
2024-12-02-14:24:13-root-INFO: grad norm: 2.556 2.536 0.315
2024-12-02-14:24:14-root-INFO: grad norm: 2.492 2.476 0.287
2024-12-02-14:24:15-root-INFO: grad norm: 2.467 2.448 0.306
2024-12-02-14:24:16-root-INFO: Loss Change: 71.068 -> 69.638
2024-12-02-14:24:16-root-INFO: Regularization Change: 0.000 -> 0.947
2024-12-02-14:24:16-root-INFO: Undo step: 60
2024-12-02-14:24:16-root-INFO: Undo step: 61
2024-12-02-14:24:16-root-INFO: Undo step: 62
2024-12-02-14:24:16-root-INFO: Undo step: 63
2024-12-02-14:24:16-root-INFO: Undo step: 64
2024-12-02-14:24:16-root-INFO: Undo step: 65
2024-12-02-14:24:16-root-INFO: Undo step: 66
2024-12-02-14:24:16-root-INFO: Undo step: 67
2024-12-02-14:24:16-root-INFO: Undo step: 68
2024-12-02-14:24:16-root-INFO: Undo step: 69
2024-12-02-14:24:16-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-14:24:16-root-INFO: grad norm: 42.762 42.290 6.337
2024-12-02-14:24:17-root-INFO: grad norm: 21.725 21.429 3.571
2024-12-02-14:24:18-root-INFO: grad norm: 15.896 15.724 2.332
2024-12-02-14:24:19-root-INFO: grad norm: 15.145 15.043 1.755
2024-12-02-14:24:20-root-INFO: grad norm: 15.530 15.404 1.979
2024-12-02-14:24:21-root-INFO: grad norm: 16.951 16.806 2.211
2024-12-02-14:24:22-root-INFO: grad norm: 17.366 17.180 2.538
2024-12-02-14:24:23-root-INFO: grad norm: 18.236 18.048 2.618
2024-12-02-14:24:24-root-INFO: Loss Change: 330.685 -> 117.044
2024-12-02-14:24:24-root-INFO: Regularization Change: 0.000 -> 153.754
2024-12-02-14:24:24-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-14:24:24-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-14:24:24-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-14:24:25-root-INFO: grad norm: 18.717 18.520 2.706
2024-12-02-14:24:25-root-INFO: Loss too large (116.025->117.148)! Learning rate decreased to 0.13798.
2024-12-02-14:24:26-root-INFO: grad norm: 14.490 14.338 2.088
2024-12-02-14:24:27-root-INFO: grad norm: 10.042 9.909 1.627
2024-12-02-14:24:28-root-INFO: grad norm: 8.405 8.308 1.272
2024-12-02-14:24:29-root-INFO: grad norm: 7.752 7.654 1.229
2024-12-02-14:24:30-root-INFO: grad norm: 8.458 8.354 1.321
2024-12-02-14:24:31-root-INFO: grad norm: 7.448 7.349 1.205
2024-12-02-14:24:32-root-INFO: grad norm: 7.106 7.014 1.143
2024-12-02-14:24:33-root-INFO: Loss Change: 116.025 -> 91.429
2024-12-02-14:24:33-root-INFO: Regularization Change: 0.000 -> 14.281
2024-12-02-14:24:33-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-14:24:33-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-14:24:33-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-14:24:34-root-INFO: grad norm: 8.602 8.510 1.254
2024-12-02-14:24:34-root-INFO: Loss too large (91.219->92.731)! Learning rate decreased to 0.14121.
2024-12-02-14:24:34-root-INFO: Loss too large (91.219->91.403)! Learning rate decreased to 0.11297.
2024-12-02-14:24:35-root-INFO: grad norm: 5.229 5.153 0.886
2024-12-02-14:24:36-root-INFO: grad norm: 4.117 4.072 0.610
2024-12-02-14:24:37-root-INFO: grad norm: 3.807 3.756 0.618
2024-12-02-14:24:38-root-INFO: grad norm: 4.002 3.961 0.569
2024-12-02-14:24:39-root-INFO: grad norm: 5.340 5.299 0.660
2024-12-02-14:24:40-root-INFO: Loss too large (86.447->86.455)! Learning rate decreased to 0.09037.
2024-12-02-14:24:41-root-INFO: grad norm: 3.992 3.954 0.550
2024-12-02-14:24:42-root-INFO: grad norm: 2.356 2.318 0.424
2024-12-02-14:24:42-root-INFO: Loss Change: 91.219 -> 84.690
2024-12-02-14:24:42-root-INFO: Regularization Change: 0.000 -> 4.316
2024-12-02-14:24:42-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-14:24:42-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-14:24:43-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-14:24:43-root-INFO: grad norm: 2.758 2.700 0.562
2024-12-02-14:24:44-root-INFO: grad norm: 3.222 3.175 0.553
2024-12-02-14:24:45-root-INFO: grad norm: 5.072 5.015 0.755
2024-12-02-14:24:45-root-INFO: Loss too large (83.533->85.548)! Learning rate decreased to 0.14449.
2024-12-02-14:24:46-root-INFO: Loss too large (83.533->83.947)! Learning rate decreased to 0.11559.
2024-12-02-14:24:47-root-INFO: grad norm: 6.988 6.937 0.838
2024-12-02-14:24:47-root-INFO: Loss too large (83.046->83.388)! Learning rate decreased to 0.09247.
2024-12-02-14:24:48-root-INFO: grad norm: 3.896 3.861 0.526
2024-12-02-14:24:49-root-INFO: grad norm: 3.084 3.049 0.460
2024-12-02-14:24:50-root-INFO: grad norm: 3.615 3.587 0.450
2024-12-02-14:24:51-root-INFO: grad norm: 3.766 3.733 0.498
2024-12-02-14:24:52-root-INFO: Loss Change: 84.555 -> 80.758
2024-12-02-14:24:52-root-INFO: Regularization Change: 0.000 -> 3.702
2024-12-02-14:24:52-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-14:24:52-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-14:24:52-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-14:24:52-root-INFO: grad norm: 5.459 5.407 0.750
2024-12-02-14:24:53-root-INFO: Loss too large (80.960->82.031)! Learning rate decreased to 0.14783.
2024-12-02-14:24:53-root-INFO: Loss too large (80.960->81.619)! Learning rate decreased to 0.11826.
2024-12-02-14:24:53-root-INFO: Loss too large (80.960->81.265)! Learning rate decreased to 0.09461.
2024-12-02-14:24:54-root-INFO: grad norm: 3.932 3.898 0.517
2024-12-02-14:24:55-root-INFO: grad norm: 2.000 1.969 0.354
2024-12-02-14:24:57-root-INFO: grad norm: 1.881 1.849 0.347
2024-12-02-14:24:57-root-INFO: grad norm: 1.885 1.856 0.327
2024-12-02-14:24:58-root-INFO: grad norm: 1.988 1.959 0.341
2024-12-02-14:24:59-root-INFO: grad norm: 2.432 2.407 0.345
2024-12-02-14:25:01-root-INFO: grad norm: 3.136 3.108 0.416
2024-12-02-14:25:01-root-INFO: Loss Change: 80.960 -> 78.333
2024-12-02-14:25:01-root-INFO: Regularization Change: 0.000 -> 1.905
2024-12-02-14:25:01-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-14:25:01-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-14:25:02-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-14:25:02-root-INFO: grad norm: 6.761 6.692 0.960
2024-12-02-14:25:02-root-INFO: Loss too large (78.590->80.078)! Learning rate decreased to 0.15121.
2024-12-02-14:25:03-root-INFO: Loss too large (78.590->79.525)! Learning rate decreased to 0.12097.
2024-12-02-14:25:03-root-INFO: Loss too large (78.590->79.089)! Learning rate decreased to 0.09678.
2024-12-02-14:25:03-root-INFO: Loss too large (78.590->78.712)! Learning rate decreased to 0.07742.
2024-12-02-14:25:04-root-INFO: grad norm: 3.998 3.970 0.471
2024-12-02-14:25:05-root-INFO: grad norm: 1.926 1.899 0.320
2024-12-02-14:25:06-root-INFO: grad norm: 1.914 1.888 0.314
2024-12-02-14:25:07-root-INFO: grad norm: 2.001 1.976 0.315
2024-12-02-14:25:08-root-INFO: grad norm: 2.227 2.204 0.318
2024-12-02-14:25:09-root-INFO: grad norm: 2.503 2.479 0.346
2024-12-02-14:25:10-root-INFO: grad norm: 3.066 3.045 0.363
2024-12-02-14:25:11-root-INFO: Loss Change: 78.590 -> 76.242
2024-12-02-14:25:11-root-INFO: Regularization Change: 0.000 -> 1.240
2024-12-02-14:25:11-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-14:25:11-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-14:25:11-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-14:25:12-root-INFO: grad norm: 3.441 3.412 0.444
2024-12-02-14:25:12-root-INFO: Loss too large (76.126->77.988)! Learning rate decreased to 0.15465.
2024-12-02-14:25:12-root-INFO: Loss too large (76.126->76.984)! Learning rate decreased to 0.12372.
2024-12-02-14:25:13-root-INFO: Loss too large (76.126->76.361)! Learning rate decreased to 0.09898.
2024-12-02-14:25:14-root-INFO: grad norm: 5.505 5.478 0.552
2024-12-02-14:25:14-root-INFO: Loss too large (76.010->76.375)! Learning rate decreased to 0.07918.
2024-12-02-14:25:14-root-INFO: Loss too large (76.010->76.065)! Learning rate decreased to 0.06335.
2024-12-02-14:25:15-root-INFO: grad norm: 3.881 3.855 0.441
2024-12-02-14:25:16-root-INFO: grad norm: 2.208 2.185 0.314
2024-12-02-14:25:17-root-INFO: grad norm: 2.129 2.106 0.310
2024-12-02-14:25:18-root-INFO: grad norm: 2.065 2.044 0.298
2024-12-02-14:25:19-root-INFO: grad norm: 2.034 2.011 0.302
2024-12-02-14:25:20-root-INFO: grad norm: 2.011 1.990 0.291
2024-12-02-14:25:21-root-INFO: Loss Change: 76.126 -> 74.477
2024-12-02-14:25:21-root-INFO: Regularization Change: 0.000 -> 0.796
2024-12-02-14:25:21-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-14:25:21-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-14:25:21-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-14:25:22-root-INFO: grad norm: 2.390 2.337 0.501
2024-12-02-14:25:23-root-INFO: grad norm: 4.333 4.309 0.462
2024-12-02-14:25:23-root-INFO: Loss too large (74.266->75.419)! Learning rate decreased to 0.15815.
2024-12-02-14:25:23-root-INFO: Loss too large (74.266->75.010)! Learning rate decreased to 0.12652.
2024-12-02-14:25:24-root-INFO: Loss too large (74.266->74.664)! Learning rate decreased to 0.10121.
2024-12-02-14:25:24-root-INFO: Loss too large (74.266->74.371)! Learning rate decreased to 0.08097.
2024-12-02-14:25:25-root-INFO: grad norm: 4.002 3.976 0.458
2024-12-02-14:25:26-root-INFO: grad norm: 4.105 4.083 0.425
2024-12-02-14:25:26-root-INFO: Loss too large (73.751->73.788)! Learning rate decreased to 0.06478.
2024-12-02-14:25:27-root-INFO: grad norm: 3.461 3.437 0.400
2024-12-02-14:25:28-root-INFO: grad norm: 2.726 2.706 0.327
2024-12-02-14:25:29-root-INFO: grad norm: 2.613 2.592 0.332
2024-12-02-14:25:30-root-INFO: grad norm: 2.498 2.479 0.309
2024-12-02-14:25:31-root-INFO: Loss Change: 74.589 -> 72.851
2024-12-02-14:25:31-root-INFO: Regularization Change: 0.000 -> 1.174
2024-12-02-14:25:31-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-14:25:31-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-14:25:31-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-14:25:32-root-INFO: grad norm: 2.733 2.669 0.588
2024-12-02-14:25:33-root-INFO: grad norm: 5.831 5.797 0.631
2024-12-02-14:25:33-root-INFO: Loss too large (72.578->74.253)! Learning rate decreased to 0.16169.
2024-12-02-14:25:33-root-INFO: Loss too large (72.578->73.646)! Learning rate decreased to 0.12935.
2024-12-02-14:25:34-root-INFO: Loss too large (72.578->73.180)! Learning rate decreased to 0.10348.
2024-12-02-14:25:34-root-INFO: Loss too large (72.578->72.787)! Learning rate decreased to 0.08279.
2024-12-02-14:25:35-root-INFO: grad norm: 4.170 4.142 0.485
2024-12-02-14:25:36-root-INFO: grad norm: 2.299 2.279 0.304
2024-12-02-14:25:37-root-INFO: grad norm: 2.730 2.710 0.333
2024-12-02-14:25:38-root-INFO: grad norm: 3.642 3.623 0.378
2024-12-02-14:25:38-root-INFO: Loss too large (71.352->71.371)! Learning rate decreased to 0.06623.
2024-12-02-14:25:39-root-INFO: grad norm: 3.243 3.221 0.371
2024-12-02-14:25:41-root-INFO: grad norm: 2.791 2.773 0.319
2024-12-02-14:25:41-root-INFO: Loss Change: 72.648 -> 70.856
2024-12-02-14:25:41-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-02-14:25:41-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-14:25:41-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-14:25:42-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-14:25:42-root-INFO: grad norm: 2.584 2.558 0.368
2024-12-02-14:25:42-root-INFO: Loss too large (70.842->71.679)! Learning rate decreased to 0.16529.
2024-12-02-14:25:43-root-INFO: Loss too large (70.842->71.172)! Learning rate decreased to 0.13223.
2024-12-02-14:25:43-root-INFO: Loss too large (70.842->70.874)! Learning rate decreased to 0.10579.
2024-12-02-14:25:44-root-INFO: grad norm: 4.083 4.061 0.428
2024-12-02-14:25:44-root-INFO: Loss too large (70.714->71.007)! Learning rate decreased to 0.08463.
2024-12-02-14:25:45-root-INFO: Loss too large (70.714->70.752)! Learning rate decreased to 0.06770.
2024-12-02-14:25:46-root-INFO: grad norm: 3.453 3.433 0.379
2024-12-02-14:25:47-root-INFO: grad norm: 2.795 2.776 0.321
2024-12-02-14:25:48-root-INFO: grad norm: 2.668 2.649 0.319
2024-12-02-14:25:49-root-INFO: grad norm: 2.537 2.519 0.299
2024-12-02-14:25:50-root-INFO: grad norm: 2.482 2.463 0.306
2024-12-02-14:25:51-root-INFO: grad norm: 2.428 2.410 0.289
2024-12-02-14:25:52-root-INFO: Loss Change: 70.842 -> 69.555
2024-12-02-14:25:52-root-INFO: Regularization Change: 0.000 -> 0.729
2024-12-02-14:25:52-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-14:25:52-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-14:25:52-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-14:25:52-root-INFO: grad norm: 2.546 2.501 0.475
2024-12-02-14:25:53-root-INFO: Loss too large (69.599->69.950)! Learning rate decreased to 0.16894.
2024-12-02-14:25:53-root-INFO: Loss too large (69.599->69.612)! Learning rate decreased to 0.13515.
2024-12-02-14:25:54-root-INFO: grad norm: 4.470 4.452 0.407
2024-12-02-14:25:54-root-INFO: Loss too large (69.431->70.136)! Learning rate decreased to 0.10812.
2024-12-02-14:25:55-root-INFO: Loss too large (69.431->69.824)! Learning rate decreased to 0.08650.
2024-12-02-14:25:55-root-INFO: Loss too large (69.431->69.533)! Learning rate decreased to 0.06920.
2024-12-02-14:25:56-root-INFO: grad norm: 3.762 3.740 0.403
2024-12-02-14:25:57-root-INFO: grad norm: 2.998 2.980 0.328
2024-12-02-14:25:58-root-INFO: grad norm: 2.934 2.915 0.332
2024-12-02-14:25:59-root-INFO: grad norm: 2.866 2.849 0.313
2024-12-02-14:26:00-root-INFO: grad norm: 2.845 2.827 0.324
2024-12-02-14:26:01-root-INFO: grad norm: 2.825 2.808 0.307
2024-12-02-14:26:02-root-INFO: Loss Change: 69.599 -> 68.282
2024-12-02-14:26:02-root-INFO: Regularization Change: 0.000 -> 0.821
2024-12-02-14:26:02-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-14:26:02-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-14:26:02-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-14:26:02-root-INFO: grad norm: 2.686 2.646 0.460
2024-12-02-14:26:03-root-INFO: Loss too large (68.244->69.033)! Learning rate decreased to 0.17265.
2024-12-02-14:26:03-root-INFO: Loss too large (68.244->68.530)! Learning rate decreased to 0.13812.
2024-12-02-14:26:04-root-INFO: grad norm: 5.314 5.293 0.472
2024-12-02-14:26:04-root-INFO: Loss too large (68.234->69.056)! Learning rate decreased to 0.11050.
2024-12-02-14:26:05-root-INFO: Loss too large (68.234->68.712)! Learning rate decreased to 0.08840.
2024-12-02-14:26:05-root-INFO: Loss too large (68.234->68.365)! Learning rate decreased to 0.07072.
2024-12-02-14:26:06-root-INFO: grad norm: 4.020 3.999 0.415
2024-12-02-14:26:07-root-INFO: grad norm: 2.732 2.715 0.311
2024-12-02-14:26:08-root-INFO: grad norm: 2.731 2.714 0.309
2024-12-02-14:26:09-root-INFO: grad norm: 2.747 2.730 0.303
2024-12-02-14:26:10-root-INFO: grad norm: 2.766 2.749 0.311
2024-12-02-14:26:11-root-INFO: grad norm: 2.796 2.780 0.302
2024-12-02-14:26:12-root-INFO: Loss Change: 68.244 -> 66.935
2024-12-02-14:26:12-root-INFO: Regularization Change: 0.000 -> 0.825
2024-12-02-14:26:12-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-14:26:12-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-14:26:12-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-14:26:13-root-INFO: grad norm: 2.752 2.718 0.431
2024-12-02-14:26:13-root-INFO: Loss too large (66.802->67.864)! Learning rate decreased to 0.17641.
2024-12-02-14:26:13-root-INFO: Loss too large (66.802->67.265)! Learning rate decreased to 0.14113.
2024-12-02-14:26:14-root-INFO: Loss too large (66.802->66.900)! Learning rate decreased to 0.11290.
2024-12-02-14:26:15-root-INFO: grad norm: 4.376 4.354 0.434
2024-12-02-14:26:15-root-INFO: Loss too large (66.695->67.108)! Learning rate decreased to 0.09032.
2024-12-02-14:26:15-root-INFO: Loss too large (66.695->66.803)! Learning rate decreased to 0.07226.
2024-12-02-14:26:16-root-INFO: grad norm: 3.750 3.730 0.383
2024-12-02-14:26:17-root-INFO: grad norm: 3.120 3.102 0.334
2024-12-02-14:26:18-root-INFO: grad norm: 3.044 3.026 0.328
2024-12-02-14:26:19-root-INFO: grad norm: 2.962 2.946 0.313
2024-12-02-14:26:20-root-INFO: grad norm: 2.933 2.915 0.319
2024-12-02-14:26:21-root-INFO: grad norm: 2.901 2.885 0.305
2024-12-02-14:26:22-root-INFO: Loss Change: 66.802 -> 65.571
2024-12-02-14:26:22-root-INFO: Regularization Change: 0.000 -> 0.763
2024-12-02-14:26:22-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-14:26:22-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-14:26:22-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-14:26:23-root-INFO: grad norm: 2.645 2.606 0.454
2024-12-02-14:26:23-root-INFO: Loss too large (65.365->66.081)! Learning rate decreased to 0.18022.
2024-12-02-14:26:23-root-INFO: Loss too large (65.365->65.603)! Learning rate decreased to 0.14417.
2024-12-02-14:26:24-root-INFO: grad norm: 4.809 4.785 0.483
2024-12-02-14:26:25-root-INFO: Loss too large (65.325->66.079)! Learning rate decreased to 0.11534.
2024-12-02-14:26:25-root-INFO: Loss too large (65.325->65.715)! Learning rate decreased to 0.09227.
2024-12-02-14:26:25-root-INFO: Loss too large (65.325->65.368)! Learning rate decreased to 0.07382.
2024-12-02-14:26:26-root-INFO: grad norm: 3.782 3.761 0.392
2024-12-02-14:26:27-root-INFO: grad norm: 2.982 2.966 0.311
2024-12-02-14:26:28-root-INFO: grad norm: 2.864 2.846 0.314
2024-12-02-14:26:29-root-INFO: grad norm: 2.752 2.737 0.291
2024-12-02-14:26:30-root-INFO: grad norm: 2.702 2.685 0.301
2024-12-02-14:26:31-root-INFO: grad norm: 2.657 2.642 0.282
2024-12-02-14:26:32-root-INFO: Loss Change: 65.365 -> 64.058
2024-12-02-14:26:32-root-INFO: Regularization Change: 0.000 -> 0.846
2024-12-02-14:26:32-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-14:26:32-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-14:26:33-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-14:26:33-root-INFO: grad norm: 2.726 2.696 0.407
2024-12-02-14:26:33-root-INFO: Loss too large (64.085->65.142)! Learning rate decreased to 0.18408.
2024-12-02-14:26:34-root-INFO: Loss too large (64.085->64.555)! Learning rate decreased to 0.14727.
2024-12-02-14:26:34-root-INFO: Loss too large (64.085->64.192)! Learning rate decreased to 0.11781.
2024-12-02-14:26:35-root-INFO: grad norm: 4.200 4.180 0.402
2024-12-02-14:26:35-root-INFO: Loss too large (63.985->64.345)! Learning rate decreased to 0.09425.
2024-12-02-14:26:36-root-INFO: Loss too large (63.985->64.038)! Learning rate decreased to 0.07540.
2024-12-02-14:26:37-root-INFO: grad norm: 3.563 3.545 0.360
2024-12-02-14:26:37-root-INFO: grad norm: 3.024 3.008 0.313
2024-12-02-14:26:38-root-INFO: grad norm: 2.885 2.868 0.310
2024-12-02-14:26:39-root-INFO: grad norm: 2.755 2.740 0.287
2024-12-02-14:26:40-root-INFO: grad norm: 2.690 2.674 0.296
2024-12-02-14:26:41-root-INFO: grad norm: 2.630 2.616 0.275
2024-12-02-14:26:42-root-INFO: Loss Change: 64.085 -> 62.832
2024-12-02-14:26:42-root-INFO: Regularization Change: 0.000 -> 0.787
2024-12-02-14:26:42-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-14:26:42-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-14:26:42-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-14:26:43-root-INFO: grad norm: 2.440 2.402 0.425
2024-12-02-14:26:43-root-INFO: Loss too large (62.786->63.289)! Learning rate decreased to 0.18800.
2024-12-02-14:26:43-root-INFO: Loss too large (62.786->62.917)! Learning rate decreased to 0.15040.
2024-12-02-14:26:44-root-INFO: grad norm: 4.312 4.295 0.378
2024-12-02-14:26:45-root-INFO: Loss too large (62.703->63.398)! Learning rate decreased to 0.12032.
2024-12-02-14:26:45-root-INFO: Loss too large (62.703->63.055)! Learning rate decreased to 0.09626.
2024-12-02-14:26:46-root-INFO: Loss too large (62.703->62.731)! Learning rate decreased to 0.07700.
2024-12-02-14:26:47-root-INFO: grad norm: 3.574 3.555 0.364
2024-12-02-14:26:48-root-INFO: grad norm: 3.033 3.018 0.305
2024-12-02-14:26:49-root-INFO: grad norm: 2.872 2.855 0.306
2024-12-02-14:26:50-root-INFO: grad norm: 2.727 2.712 0.282
2024-12-02-14:26:51-root-INFO: grad norm: 2.648 2.632 0.289
2024-12-02-14:26:52-root-INFO: grad norm: 2.576 2.562 0.269
2024-12-02-14:26:52-root-INFO: Loss Change: 62.786 -> 61.509
2024-12-02-14:26:52-root-INFO: Regularization Change: 0.000 -> 0.871
2024-12-02-14:26:52-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-14:26:52-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-14:26:53-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-14:26:53-root-INFO: grad norm: 2.497 2.463 0.406
2024-12-02-14:26:53-root-INFO: Loss too large (61.392->62.075)! Learning rate decreased to 0.19197.
2024-12-02-14:26:54-root-INFO: Loss too large (61.392->61.640)! Learning rate decreased to 0.15357.
2024-12-02-14:26:55-root-INFO: grad norm: 4.541 4.522 0.411
2024-12-02-14:26:55-root-INFO: Loss too large (61.379->62.071)! Learning rate decreased to 0.12286.
2024-12-02-14:26:55-root-INFO: Loss too large (61.379->61.706)! Learning rate decreased to 0.09829.
2024-12-02-14:26:56-root-INFO: grad norm: 4.099 4.079 0.405
2024-12-02-14:26:57-root-INFO: grad norm: 3.598 3.580 0.354
2024-12-02-14:26:58-root-INFO: grad norm: 3.771 3.753 0.377
2024-12-02-14:26:59-root-INFO: grad norm: 3.895 3.877 0.367
2024-12-02-14:27:00-root-INFO: grad norm: 3.825 3.806 0.387
2024-12-02-14:27:01-root-INFO: grad norm: 3.698 3.681 0.353
2024-12-02-14:27:02-root-INFO: Loss Change: 61.392 -> 60.162
2024-12-02-14:27:02-root-INFO: Regularization Change: 0.000 -> 1.275
2024-12-02-14:27:02-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-14:27:02-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-14:27:02-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-14:27:03-root-INFO: grad norm: 3.543 3.519 0.413
2024-12-02-14:27:03-root-INFO: Loss too large (60.105->62.267)! Learning rate decreased to 0.19599.
2024-12-02-14:27:03-root-INFO: Loss too large (60.105->61.231)! Learning rate decreased to 0.15679.
2024-12-02-14:27:04-root-INFO: Loss too large (60.105->60.525)! Learning rate decreased to 0.12543.
2024-12-02-14:27:05-root-INFO: grad norm: 4.696 4.675 0.441
2024-12-02-14:27:05-root-INFO: Loss too large (60.080->60.228)! Learning rate decreased to 0.10035.
2024-12-02-14:27:06-root-INFO: grad norm: 3.869 3.849 0.396
2024-12-02-14:27:07-root-INFO: grad norm: 3.226 3.210 0.325
2024-12-02-14:27:08-root-INFO: grad norm: 3.271 3.253 0.344
2024-12-02-14:27:09-root-INFO: grad norm: 3.308 3.293 0.321
2024-12-02-14:27:10-root-INFO: grad norm: 3.317 3.299 0.349
2024-12-02-14:27:11-root-INFO: grad norm: 3.311 3.296 0.319
2024-12-02-14:27:12-root-INFO: Loss Change: 60.105 -> 58.659
2024-12-02-14:27:12-root-INFO: Regularization Change: 0.000 -> 1.158
2024-12-02-14:27:12-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-14:27:12-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-14:27:12-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-14:27:13-root-INFO: grad norm: 3.198 3.177 0.373
2024-12-02-14:27:13-root-INFO: Loss too large (58.559->60.161)! Learning rate decreased to 0.20006.
2024-12-02-14:27:13-root-INFO: Loss too large (58.559->59.348)! Learning rate decreased to 0.16005.
2024-12-02-14:27:14-root-INFO: Loss too large (58.559->58.809)! Learning rate decreased to 0.12804.
2024-12-02-14:27:15-root-INFO: grad norm: 4.082 4.064 0.380
2024-12-02-14:27:15-root-INFO: Loss too large (58.478->58.552)! Learning rate decreased to 0.10243.
2024-12-02-14:27:16-root-INFO: grad norm: 3.556 3.537 0.369
2024-12-02-14:27:17-root-INFO: grad norm: 3.182 3.167 0.312
2024-12-02-14:27:18-root-INFO: grad norm: 3.100 3.083 0.333
2024-12-02-14:27:19-root-INFO: grad norm: 3.035 3.020 0.296
2024-12-02-14:27:20-root-INFO: grad norm: 3.007 2.989 0.324
2024-12-02-14:27:21-root-INFO: grad norm: 2.983 2.968 0.290
2024-12-02-14:27:22-root-INFO: Loss Change: 58.559 -> 57.120
2024-12-02-14:27:22-root-INFO: Regularization Change: 0.000 -> 1.173
2024-12-02-14:27:22-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-14:27:22-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-14:27:22-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-14:27:23-root-INFO: grad norm: 2.908 2.877 0.422
2024-12-02-14:27:23-root-INFO: Loss too large (57.044->58.118)! Learning rate decreased to 0.20419.
2024-12-02-14:27:23-root-INFO: Loss too large (57.044->57.497)! Learning rate decreased to 0.16335.
2024-12-02-14:27:24-root-INFO: Loss too large (57.044->57.105)! Learning rate decreased to 0.13068.
2024-12-02-14:27:25-root-INFO: grad norm: 3.496 3.478 0.358
2024-12-02-14:27:25-root-INFO: Loss too large (56.879->56.904)! Learning rate decreased to 0.10454.
2024-12-02-14:27:26-root-INFO: grad norm: 3.228 3.211 0.323
2024-12-02-14:27:27-root-INFO: grad norm: 3.049 3.034 0.306
2024-12-02-14:27:28-root-INFO: grad norm: 2.966 2.951 0.306
2024-12-02-14:27:29-root-INFO: grad norm: 2.904 2.890 0.290
2024-12-02-14:27:30-root-INFO: grad norm: 2.867 2.851 0.302
2024-12-02-14:27:31-root-INFO: grad norm: 2.837 2.823 0.283
2024-12-02-14:27:32-root-INFO: Loss Change: 57.044 -> 55.617
2024-12-02-14:27:32-root-INFO: Regularization Change: 0.000 -> 1.202
2024-12-02-14:27:32-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-14:27:32-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-14:27:32-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-14:27:32-root-INFO: grad norm: 2.660 2.633 0.376
2024-12-02-14:27:33-root-INFO: Loss too large (55.314->56.136)! Learning rate decreased to 0.20837.
2024-12-02-14:27:33-root-INFO: Loss too large (55.314->55.631)! Learning rate decreased to 0.16669.
2024-12-02-14:27:33-root-INFO: Loss too large (55.314->55.320)! Learning rate decreased to 0.13335.
2024-12-02-14:27:35-root-INFO: grad norm: 3.167 3.150 0.335
2024-12-02-14:27:36-root-INFO: grad norm: 3.750 3.731 0.378
2024-12-02-14:27:37-root-INFO: grad norm: 4.163 4.145 0.394
2024-12-02-14:27:38-root-INFO: grad norm: 4.032 4.008 0.435
2024-12-02-14:27:39-root-INFO: grad norm: 3.947 3.925 0.409
2024-12-02-14:27:40-root-INFO: grad norm: 4.050 4.023 0.470
2024-12-02-14:27:41-root-INFO: grad norm: 4.109 4.085 0.437
2024-12-02-14:27:41-root-INFO: Loss Change: 55.314 -> 54.120
2024-12-02-14:27:41-root-INFO: Regularization Change: 0.000 -> 1.823
2024-12-02-14:27:41-root-INFO: Undo step: 50
2024-12-02-14:27:41-root-INFO: Undo step: 51
2024-12-02-14:27:41-root-INFO: Undo step: 52
2024-12-02-14:27:42-root-INFO: Undo step: 53
2024-12-02-14:27:42-root-INFO: Undo step: 54
2024-12-02-14:27:42-root-INFO: Undo step: 55
2024-12-02-14:27:42-root-INFO: Undo step: 56
2024-12-02-14:27:42-root-INFO: Undo step: 57
2024-12-02-14:27:42-root-INFO: Undo step: 58
2024-12-02-14:27:42-root-INFO: Undo step: 59
2024-12-02-14:27:42-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-14:27:42-root-INFO: grad norm: 31.769 31.480 4.277
2024-12-02-14:27:43-root-INFO: grad norm: 16.485 16.297 2.485
2024-12-02-14:27:44-root-INFO: grad norm: 11.682 11.509 2.007
2024-12-02-14:27:45-root-INFO: grad norm: 10.188 10.057 1.628
2024-12-02-14:27:46-root-INFO: grad norm: 9.431 9.307 1.525
2024-12-02-14:27:47-root-INFO: grad norm: 10.768 10.676 1.411
2024-12-02-14:27:48-root-INFO: grad norm: 9.023 8.931 1.286
2024-12-02-14:27:49-root-INFO: grad norm: 9.055 8.990 1.079
2024-12-02-14:27:50-root-INFO: Loss Change: 263.957 -> 87.887
2024-12-02-14:27:50-root-INFO: Regularization Change: 0.000 -> 167.638
2024-12-02-14:27:50-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-14:27:50-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-14:27:50-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-14:27:51-root-INFO: grad norm: 8.460 8.396 1.037
2024-12-02-14:27:52-root-INFO: grad norm: 8.475 8.413 1.027
2024-12-02-14:27:53-root-INFO: grad norm: 8.703 8.618 1.208
2024-12-02-14:27:54-root-INFO: grad norm: 9.787 9.709 1.228
2024-12-02-14:27:54-root-INFO: Loss too large (81.450->81.685)! Learning rate decreased to 0.17265.
2024-12-02-14:27:55-root-INFO: grad norm: 6.589 6.506 1.043
2024-12-02-14:27:56-root-INFO: grad norm: 4.875 4.829 0.668
2024-12-02-14:27:57-root-INFO: grad norm: 4.177 4.135 0.589
2024-12-02-14:27:58-root-INFO: grad norm: 4.055 4.015 0.568
2024-12-02-14:27:59-root-INFO: Loss Change: 87.409 -> 72.559
2024-12-02-14:27:59-root-INFO: Regularization Change: 0.000 -> 18.042
2024-12-02-14:27:59-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-14:27:59-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-14:27:59-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-14:27:59-root-INFO: grad norm: 5.003 4.954 0.697
2024-12-02-14:28:00-root-INFO: grad norm: 4.434 4.387 0.644
2024-12-02-14:28:01-root-INFO: grad norm: 4.513 4.479 0.551
2024-12-02-14:28:02-root-INFO: grad norm: 5.820 5.785 0.638
2024-12-02-14:28:03-root-INFO: Loss too large (69.810->70.653)! Learning rate decreased to 0.17641.
2024-12-02-14:28:04-root-INFO: grad norm: 5.268 5.211 0.777
2024-12-02-14:28:05-root-INFO: grad norm: 5.028 4.981 0.686
2024-12-02-14:28:06-root-INFO: grad norm: 4.618 4.551 0.785
2024-12-02-14:28:07-root-INFO: grad norm: 4.044 3.995 0.626
2024-12-02-14:28:07-root-INFO: Loss Change: 72.388 -> 66.461
2024-12-02-14:28:07-root-INFO: Regularization Change: 0.000 -> 9.601
2024-12-02-14:28:07-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-14:28:07-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-14:28:08-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-14:28:08-root-INFO: grad norm: 4.063 4.011 0.648
2024-12-02-14:28:08-root-INFO: Loss too large (65.979->66.124)! Learning rate decreased to 0.18022.
2024-12-02-14:28:09-root-INFO: grad norm: 4.815 4.772 0.640
2024-12-02-14:28:10-root-INFO: Loss too large (65.359->65.511)! Learning rate decreased to 0.14417.
2024-12-02-14:28:11-root-INFO: grad norm: 3.935 3.881 0.650
2024-12-02-14:28:12-root-INFO: grad norm: 2.988 2.954 0.450
2024-12-02-14:28:13-root-INFO: grad norm: 2.972 2.932 0.486
2024-12-02-14:28:14-root-INFO: grad norm: 3.079 3.046 0.452
2024-12-02-14:28:15-root-INFO: grad norm: 3.133 3.091 0.513
2024-12-02-14:28:16-root-INFO: grad norm: 3.249 3.214 0.479
2024-12-02-14:28:17-root-INFO: Loss Change: 65.979 -> 62.376
2024-12-02-14:28:17-root-INFO: Regularization Change: 0.000 -> 3.914
2024-12-02-14:28:17-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-14:28:17-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-14:28:17-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-14:28:17-root-INFO: grad norm: 3.270 3.232 0.495
2024-12-02-14:28:18-root-INFO: Loss too large (62.296->62.589)! Learning rate decreased to 0.18408.
2024-12-02-14:28:19-root-INFO: grad norm: 4.623 4.581 0.622
2024-12-02-14:28:19-root-INFO: Loss too large (62.038->62.406)! Learning rate decreased to 0.14727.
2024-12-02-14:28:20-root-INFO: grad norm: 3.895 3.840 0.652
2024-12-02-14:28:21-root-INFO: grad norm: 2.897 2.864 0.433
2024-12-02-14:28:22-root-INFO: grad norm: 2.955 2.917 0.474
2024-12-02-14:28:23-root-INFO: grad norm: 3.112 3.079 0.452
2024-12-02-14:28:24-root-INFO: grad norm: 3.177 3.136 0.513
2024-12-02-14:28:25-root-INFO: grad norm: 3.287 3.252 0.480
2024-12-02-14:28:26-root-INFO: Loss Change: 62.296 -> 59.639
2024-12-02-14:28:26-root-INFO: Regularization Change: 0.000 -> 3.289
2024-12-02-14:28:26-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-14:28:26-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-14:28:26-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-14:28:27-root-INFO: grad norm: 3.204 3.163 0.515
2024-12-02-14:28:27-root-INFO: Loss too large (59.453->59.872)! Learning rate decreased to 0.18800.
2024-12-02-14:28:28-root-INFO: grad norm: 4.651 4.608 0.635
2024-12-02-14:28:28-root-INFO: Loss too large (59.278->59.787)! Learning rate decreased to 0.15040.
2024-12-02-14:28:29-root-INFO: grad norm: 3.947 3.891 0.660
2024-12-02-14:28:30-root-INFO: grad norm: 2.930 2.901 0.412
2024-12-02-14:28:31-root-INFO: grad norm: 2.948 2.913 0.452
2024-12-02-14:28:32-root-INFO: grad norm: 3.075 3.044 0.440
2024-12-02-14:28:33-root-INFO: grad norm: 3.149 3.109 0.500
2024-12-02-14:28:34-root-INFO: grad norm: 3.289 3.254 0.480
2024-12-02-14:28:35-root-INFO: Loss Change: 59.453 -> 57.166
2024-12-02-14:28:35-root-INFO: Regularization Change: 0.000 -> 2.924
2024-12-02-14:28:35-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-14:28:35-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-14:28:35-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-14:28:36-root-INFO: grad norm: 3.283 3.244 0.505
2024-12-02-14:28:36-root-INFO: Loss too large (56.920->57.519)! Learning rate decreased to 0.19197.
2024-12-02-14:28:37-root-INFO: grad norm: 4.884 4.838 0.670
2024-12-02-14:28:37-root-INFO: Loss too large (56.819->57.455)! Learning rate decreased to 0.15357.
2024-12-02-14:28:38-root-INFO: grad norm: 4.016 3.960 0.669
2024-12-02-14:28:39-root-INFO: grad norm: 2.838 2.812 0.383
2024-12-02-14:28:41-root-INFO: grad norm: 2.791 2.761 0.404
2024-12-02-14:28:42-root-INFO: grad norm: 2.860 2.832 0.399
2024-12-02-14:28:43-root-INFO: grad norm: 2.958 2.924 0.449
2024-12-02-14:28:44-root-INFO: grad norm: 3.182 3.149 0.461
2024-12-02-14:28:44-root-INFO: Loss Change: 56.920 -> 54.791
2024-12-02-14:28:44-root-INFO: Regularization Change: 0.000 -> 2.700
2024-12-02-14:28:44-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-14:28:44-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-14:28:45-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-14:28:45-root-INFO: grad norm: 3.136 3.100 0.474
2024-12-02-14:28:45-root-INFO: Loss too large (54.695->55.359)! Learning rate decreased to 0.19599.
2024-12-02-14:28:46-root-INFO: grad norm: 4.841 4.797 0.654
2024-12-02-14:28:47-root-INFO: Loss too large (54.668->55.387)! Learning rate decreased to 0.15679.
2024-12-02-14:28:47-root-INFO: Loss too large (54.668->54.717)! Learning rate decreased to 0.12543.
2024-12-02-14:28:48-root-INFO: grad norm: 3.225 3.181 0.532
2024-12-02-14:28:49-root-INFO: grad norm: 1.910 1.892 0.261
2024-12-02-14:28:50-root-INFO: grad norm: 1.610 1.593 0.232
2024-12-02-14:28:51-root-INFO: grad norm: 1.446 1.431 0.213
2024-12-02-14:28:52-root-INFO: grad norm: 1.361 1.345 0.204
2024-12-02-14:28:53-root-INFO: grad norm: 1.308 1.293 0.199
2024-12-02-14:28:54-root-INFO: Loss Change: 54.695 -> 52.675
2024-12-02-14:28:54-root-INFO: Regularization Change: 0.000 -> 1.799
2024-12-02-14:28:54-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-14:28:54-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-14:28:54-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-14:28:55-root-INFO: grad norm: 1.710 1.671 0.366
2024-12-02-14:28:56-root-INFO: grad norm: 1.780 1.747 0.344
2024-12-02-14:28:57-root-INFO: grad norm: 2.309 2.275 0.395
2024-12-02-14:28:57-root-INFO: Loss too large (51.957->51.980)! Learning rate decreased to 0.20006.
2024-12-02-14:28:58-root-INFO: grad norm: 2.765 2.725 0.466
2024-12-02-14:28:58-root-INFO: Loss too large (51.764->51.773)! Learning rate decreased to 0.16005.
2024-12-02-14:28:59-root-INFO: grad norm: 2.791 2.766 0.378
2024-12-02-14:29:00-root-INFO: grad norm: 2.964 2.934 0.416
2024-12-02-14:29:01-root-INFO: grad norm: 3.172 3.148 0.388
2024-12-02-14:29:02-root-INFO: grad norm: 3.273 3.242 0.450
2024-12-02-14:29:03-root-INFO: Loss Change: 52.604 -> 50.834
2024-12-02-14:29:03-root-INFO: Regularization Change: 0.000 -> 3.182
2024-12-02-14:29:03-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-14:29:03-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-14:29:03-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-14:29:04-root-INFO: grad norm: 4.286 4.217 0.766
2024-12-02-14:29:04-root-INFO: Loss too large (50.978->52.507)! Learning rate decreased to 0.20419.
2024-12-02-14:29:04-root-INFO: Loss too large (50.978->51.486)! Learning rate decreased to 0.16335.
2024-12-02-14:29:05-root-INFO: grad norm: 4.205 4.154 0.650
2024-12-02-14:29:06-root-INFO: grad norm: 4.830 4.780 0.696
2024-12-02-14:29:07-root-INFO: Loss too large (50.556->50.766)! Learning rate decreased to 0.13068.
2024-12-02-14:29:08-root-INFO: grad norm: 3.428 3.383 0.554
2024-12-02-14:29:09-root-INFO: grad norm: 2.072 2.052 0.288
2024-12-02-14:29:10-root-INFO: grad norm: 1.762 1.744 0.245
2024-12-02-14:29:11-root-INFO: grad norm: 1.614 1.597 0.234
2024-12-02-14:29:12-root-INFO: grad norm: 1.542 1.525 0.223
2024-12-02-14:29:13-root-INFO: Loss Change: 50.978 -> 48.880
2024-12-02-14:29:13-root-INFO: Regularization Change: 0.000 -> 1.682
2024-12-02-14:29:13-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-14:29:13-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-14:29:13-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-14:29:13-root-INFO: grad norm: 2.416 2.364 0.500
2024-12-02-14:29:14-root-INFO: Loss too large (48.656->48.939)! Learning rate decreased to 0.20837.
2024-12-02-14:29:15-root-INFO: grad norm: 3.350 3.305 0.551
2024-12-02-14:29:15-root-INFO: Loss too large (48.638->49.102)! Learning rate decreased to 0.16669.
2024-12-02-14:29:16-root-INFO: grad norm: 4.257 4.215 0.597
2024-12-02-14:29:16-root-INFO: Loss too large (48.491->48.753)! Learning rate decreased to 0.13335.
2024-12-02-14:29:17-root-INFO: grad norm: 3.245 3.204 0.512
2024-12-02-14:29:18-root-INFO: grad norm: 2.082 2.064 0.274
2024-12-02-14:29:19-root-INFO: grad norm: 1.833 1.816 0.251
2024-12-02-14:29:20-root-INFO: grad norm: 1.704 1.688 0.237
2024-12-02-14:29:21-root-INFO: grad norm: 1.636 1.619 0.233
2024-12-02-14:29:22-root-INFO: Loss Change: 48.656 -> 47.068
2024-12-02-14:29:22-root-INFO: Regularization Change: 0.000 -> 1.719
2024-12-02-14:29:22-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-14:29:22-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-14:29:22-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-14:29:23-root-INFO: grad norm: 2.728 2.676 0.529
2024-12-02-14:29:23-root-INFO: Loss too large (47.189->47.715)! Learning rate decreased to 0.21260.
2024-12-02-14:29:23-root-INFO: Loss too large (47.189->47.283)! Learning rate decreased to 0.17008.
2024-12-02-14:29:24-root-INFO: grad norm: 2.935 2.901 0.448
2024-12-02-14:29:25-root-INFO: grad norm: 3.946 3.909 0.542
2024-12-02-14:29:26-root-INFO: Loss too large (46.928->47.176)! Learning rate decreased to 0.13606.
2024-12-02-14:29:27-root-INFO: grad norm: 3.048 3.010 0.481
2024-12-02-14:29:28-root-INFO: grad norm: 1.930 1.913 0.257
2024-12-02-14:29:29-root-INFO: grad norm: 1.726 1.709 0.240
2024-12-02-14:29:30-root-INFO: grad norm: 1.637 1.621 0.230
2024-12-02-14:29:31-root-INFO: grad norm: 1.596 1.579 0.231
2024-12-02-14:29:32-root-INFO: Loss Change: 47.189 -> 45.613
2024-12-02-14:29:32-root-INFO: Regularization Change: 0.000 -> 1.605
2024-12-02-14:29:32-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-14:29:32-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-14:29:32-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-14:29:32-root-INFO: grad norm: 2.248 2.205 0.436
2024-12-02-14:29:33-root-INFO: Loss too large (45.567->45.835)! Learning rate decreased to 0.21688.
2024-12-02-14:29:34-root-INFO: grad norm: 3.151 3.110 0.510
2024-12-02-14:29:34-root-INFO: Loss too large (45.565->45.987)! Learning rate decreased to 0.17350.
2024-12-02-14:29:35-root-INFO: grad norm: 4.015 3.979 0.535
2024-12-02-14:29:35-root-INFO: Loss too large (45.419->45.679)! Learning rate decreased to 0.13880.
2024-12-02-14:29:36-root-INFO: grad norm: 3.103 3.066 0.477
2024-12-02-14:29:37-root-INFO: grad norm: 2.026 2.010 0.254
2024-12-02-14:29:38-root-INFO: grad norm: 1.798 1.781 0.244
2024-12-02-14:29:39-root-INFO: grad norm: 1.688 1.673 0.226
2024-12-02-14:29:40-root-INFO: grad norm: 1.630 1.613 0.230
2024-12-02-14:29:41-root-INFO: Loss Change: 45.567 -> 44.099
2024-12-02-14:29:41-root-INFO: Regularization Change: 0.000 -> 1.677
2024-12-02-14:29:41-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-14:29:41-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-14:29:41-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-14:29:42-root-INFO: grad norm: 2.679 2.622 0.550
2024-12-02-14:29:42-root-INFO: Loss too large (44.185->44.762)! Learning rate decreased to 0.22121.
2024-12-02-14:29:42-root-INFO: Loss too large (44.185->44.317)! Learning rate decreased to 0.17697.
2024-12-02-14:29:43-root-INFO: grad norm: 2.901 2.868 0.434
2024-12-02-14:29:44-root-INFO: grad norm: 3.880 3.845 0.523
2024-12-02-14:29:45-root-INFO: Loss too large (43.962->44.230)! Learning rate decreased to 0.14157.
2024-12-02-14:29:46-root-INFO: grad norm: 3.036 3.002 0.456
2024-12-02-14:29:47-root-INFO: grad norm: 1.999 1.983 0.258
2024-12-02-14:29:48-root-INFO: grad norm: 1.798 1.782 0.237
2024-12-02-14:29:49-root-INFO: grad norm: 1.705 1.689 0.230
2024-12-02-14:29:50-root-INFO: grad norm: 1.653 1.638 0.228
2024-12-02-14:29:51-root-INFO: Loss Change: 44.185 -> 42.698
2024-12-02-14:29:51-root-INFO: Regularization Change: 0.000 -> 1.577
2024-12-02-14:29:51-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-14:29:51-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-14:29:51-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-14:29:51-root-INFO: grad norm: 2.472 2.427 0.471
2024-12-02-14:29:52-root-INFO: Loss too large (42.707->43.237)! Learning rate decreased to 0.22559.
2024-12-02-14:29:52-root-INFO: Loss too large (42.707->42.839)! Learning rate decreased to 0.18047.
2024-12-02-14:29:53-root-INFO: grad norm: 2.793 2.763 0.410
2024-12-02-14:29:54-root-INFO: grad norm: 3.872 3.840 0.502
2024-12-02-14:29:54-root-INFO: Loss too large (42.545->42.841)! Learning rate decreased to 0.14438.
2024-12-02-14:29:55-root-INFO: grad norm: 3.016 2.982 0.453
2024-12-02-14:29:56-root-INFO: grad norm: 1.939 1.924 0.242
2024-12-02-14:29:57-root-INFO: grad norm: 1.752 1.737 0.230
2024-12-02-14:29:58-root-INFO: grad norm: 1.684 1.669 0.223
2024-12-02-14:29:59-root-INFO: grad norm: 1.652 1.637 0.227
2024-12-02-14:30:00-root-INFO: Loss Change: 42.707 -> 41.306
2024-12-02-14:30:00-root-INFO: Regularization Change: 0.000 -> 1.557
2024-12-02-14:30:00-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-14:30:00-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-14:30:00-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-14:30:01-root-INFO: grad norm: 2.582 2.533 0.500
2024-12-02-14:30:01-root-INFO: Loss too large (41.254->41.891)! Learning rate decreased to 0.23002.
2024-12-02-14:30:01-root-INFO: Loss too large (41.254->41.445)! Learning rate decreased to 0.18402.
2024-12-02-14:30:02-root-INFO: grad norm: 2.898 2.866 0.428
2024-12-02-14:30:03-root-INFO: grad norm: 3.937 3.904 0.506
2024-12-02-14:30:04-root-INFO: Loss too large (41.102->41.426)! Learning rate decreased to 0.14721.
2024-12-02-14:30:05-root-INFO: grad norm: 3.075 3.042 0.452
2024-12-02-14:30:06-root-INFO: grad norm: 2.019 2.003 0.250
2024-12-02-14:30:07-root-INFO: grad norm: 1.818 1.802 0.234
2024-12-02-14:30:08-root-INFO: grad norm: 1.738 1.723 0.226
2024-12-02-14:30:09-root-INFO: grad norm: 1.693 1.678 0.228
2024-12-02-14:30:09-root-INFO: Loss Change: 41.254 -> 39.854
2024-12-02-14:30:09-root-INFO: Regularization Change: 0.000 -> 1.568
2024-12-02-14:30:09-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-14:30:09-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-14:30:10-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-14:30:10-root-INFO: grad norm: 2.510 2.465 0.475
2024-12-02-14:30:10-root-INFO: Loss too large (39.901->40.553)! Learning rate decreased to 0.23450.
2024-12-02-14:30:11-root-INFO: Loss too large (39.901->40.110)! Learning rate decreased to 0.18760.
2024-12-02-14:30:12-root-INFO: grad norm: 2.888 2.858 0.416
2024-12-02-14:30:13-root-INFO: grad norm: 3.934 3.902 0.500
2024-12-02-14:30:13-root-INFO: Loss too large (39.788->40.116)! Learning rate decreased to 0.15008.
2024-12-02-14:30:14-root-INFO: grad norm: 3.081 3.049 0.444
2024-12-02-14:30:15-root-INFO: grad norm: 2.045 2.029 0.253
2024-12-02-14:30:16-root-INFO: grad norm: 1.829 1.814 0.233
2024-12-02-14:30:17-root-INFO: grad norm: 1.742 1.728 0.225
2024-12-02-14:30:18-root-INFO: grad norm: 1.690 1.675 0.225
2024-12-02-14:30:19-root-INFO: Loss Change: 39.901 -> 38.537
2024-12-02-14:30:19-root-INFO: Regularization Change: 0.000 -> 1.570
2024-12-02-14:30:19-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-14:30:19-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-14:30:19-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-14:30:19-root-INFO: grad norm: 2.560 2.515 0.477
2024-12-02-14:30:20-root-INFO: Loss too large (38.414->39.087)! Learning rate decreased to 0.23903.
2024-12-02-14:30:20-root-INFO: Loss too large (38.414->38.627)! Learning rate decreased to 0.19122.
2024-12-02-14:30:21-root-INFO: grad norm: 2.859 2.829 0.408
2024-12-02-14:30:22-root-INFO: grad norm: 3.770 3.741 0.470
2024-12-02-14:30:22-root-INFO: Loss too large (38.258->38.538)! Learning rate decreased to 0.15298.
2024-12-02-14:30:23-root-INFO: grad norm: 2.927 2.898 0.416
2024-12-02-14:30:24-root-INFO: grad norm: 1.920 1.906 0.235
2024-12-02-14:30:25-root-INFO: grad norm: 1.725 1.711 0.218
2024-12-02-14:30:27-root-INFO: grad norm: 1.652 1.638 0.211
2024-12-02-14:30:28-root-INFO: grad norm: 1.612 1.598 0.212
2024-12-02-14:30:28-root-INFO: Loss Change: 38.414 -> 37.039
2024-12-02-14:30:28-root-INFO: Regularization Change: 0.000 -> 1.582
2024-12-02-14:30:28-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-14:30:28-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-14:30:29-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-14:30:29-root-INFO: grad norm: 2.142 2.108 0.378
2024-12-02-14:30:29-root-INFO: Loss too large (36.967->37.382)! Learning rate decreased to 0.24361.
2024-12-02-14:30:30-root-INFO: Loss too large (36.967->37.071)! Learning rate decreased to 0.19489.
2024-12-02-14:30:31-root-INFO: grad norm: 2.496 2.471 0.351
2024-12-02-14:30:32-root-INFO: grad norm: 3.534 3.508 0.429
2024-12-02-14:30:32-root-INFO: Loss too large (36.831->37.113)! Learning rate decreased to 0.15591.
2024-12-02-14:30:33-root-INFO: grad norm: 2.794 2.765 0.398
2024-12-02-14:30:34-root-INFO: grad norm: 1.817 1.804 0.219
2024-12-02-14:30:35-root-INFO: grad norm: 1.673 1.660 0.213
2024-12-02-14:30:36-root-INFO: grad norm: 1.633 1.621 0.204
2024-12-02-14:30:37-root-INFO: grad norm: 1.616 1.602 0.214
2024-12-02-14:30:38-root-INFO: Loss Change: 36.967 -> 35.702
2024-12-02-14:30:38-root-INFO: Regularization Change: 0.000 -> 1.570
2024-12-02-14:30:38-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-14:30:38-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-14:30:38-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-14:30:38-root-INFO: grad norm: 2.873 2.815 0.574
2024-12-02-14:30:39-root-INFO: Loss too large (35.775->36.583)! Learning rate decreased to 0.24866.
2024-12-02-14:30:39-root-INFO: Loss too large (35.775->36.016)! Learning rate decreased to 0.19893.
2024-12-02-14:30:40-root-INFO: grad norm: 2.953 2.923 0.420
2024-12-02-14:30:41-root-INFO: grad norm: 3.561 3.535 0.437
2024-12-02-14:30:41-root-INFO: Loss too large (35.493->35.695)! Learning rate decreased to 0.15914.
2024-12-02-14:30:42-root-INFO: grad norm: 2.759 2.733 0.378
2024-12-02-14:30:43-root-INFO: grad norm: 1.881 1.867 0.229
2024-12-02-14:30:44-root-INFO: grad norm: 1.697 1.684 0.215
2024-12-02-14:30:45-root-INFO: grad norm: 1.620 1.607 0.203
2024-12-02-14:30:46-root-INFO: grad norm: 1.578 1.564 0.206
2024-12-02-14:30:47-root-INFO: Loss Change: 35.775 -> 34.290
2024-12-02-14:30:47-root-INFO: Regularization Change: 0.000 -> 1.669
2024-12-02-14:30:47-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-14:30:47-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-14:30:47-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-14:30:48-root-INFO: grad norm: 2.505 2.461 0.468
2024-12-02-14:30:48-root-INFO: Loss too large (34.318->34.928)! Learning rate decreased to 0.25333.
2024-12-02-14:30:48-root-INFO: Loss too large (34.318->34.496)! Learning rate decreased to 0.20266.
2024-12-02-14:30:49-root-INFO: grad norm: 2.664 2.637 0.376
2024-12-02-14:30:50-root-INFO: grad norm: 3.353 3.328 0.408
2024-12-02-14:30:51-root-INFO: Loss too large (34.084->34.280)! Learning rate decreased to 0.16213.
2024-12-02-14:30:52-root-INFO: grad norm: 2.606 2.581 0.360
2024-12-02-14:30:53-root-INFO: grad norm: 1.724 1.711 0.212
2024-12-02-14:30:54-root-INFO: grad norm: 1.585 1.572 0.202
2024-12-02-14:30:55-root-INFO: grad norm: 1.536 1.523 0.192
2024-12-02-14:30:56-root-INFO: grad norm: 1.516 1.503 0.198
2024-12-02-14:30:56-root-INFO: Loss Change: 34.318 -> 32.956
2024-12-02-14:30:56-root-INFO: Regularization Change: 0.000 -> 1.641
2024-12-02-14:30:56-root-INFO: Undo step: 40
2024-12-02-14:30:56-root-INFO: Undo step: 41
2024-12-02-14:30:56-root-INFO: Undo step: 42
2024-12-02-14:30:56-root-INFO: Undo step: 43
2024-12-02-14:30:56-root-INFO: Undo step: 44
2024-12-02-14:30:56-root-INFO: Undo step: 45
2024-12-02-14:30:56-root-INFO: Undo step: 46
2024-12-02-14:30:56-root-INFO: Undo step: 47
2024-12-02-14:30:56-root-INFO: Undo step: 48
2024-12-02-14:30:56-root-INFO: Undo step: 49
2024-12-02-14:30:57-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-14:30:57-root-INFO: grad norm: 27.172 26.960 3.388
2024-12-02-14:30:58-root-INFO: grad norm: 13.470 13.349 1.799
2024-12-02-14:30:59-root-INFO: grad norm: 11.915 11.726 2.112
2024-12-02-14:31:00-root-INFO: grad norm: 11.287 11.211 1.303
2024-12-02-14:31:01-root-INFO: grad norm: 7.289 7.206 1.097
2024-12-02-14:31:02-root-INFO: grad norm: 6.134 6.073 0.864
2024-12-02-14:31:03-root-INFO: grad norm: 5.761 5.703 0.817
2024-12-02-14:31:04-root-INFO: grad norm: 6.514 6.450 0.910
2024-12-02-14:31:05-root-INFO: Loss Change: 232.013 -> 65.548
2024-12-02-14:31:05-root-INFO: Regularization Change: 0.000 -> 192.226
2024-12-02-14:31:05-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-14:31:05-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-14:31:05-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-14:31:05-root-INFO: grad norm: 5.585 5.534 0.754
2024-12-02-14:31:06-root-INFO: grad norm: 5.451 5.409 0.680
2024-12-02-14:31:07-root-INFO: grad norm: 7.638 7.595 0.808
2024-12-02-14:31:08-root-INFO: Loss too large (61.283->61.343)! Learning rate decreased to 0.21260.
2024-12-02-14:31:09-root-INFO: grad norm: 4.906 4.857 0.692
2024-12-02-14:31:10-root-INFO: grad norm: 4.635 4.594 0.615
2024-12-02-14:31:11-root-INFO: grad norm: 3.720 3.676 0.569
2024-12-02-14:31:12-root-INFO: grad norm: 3.230 3.202 0.424
2024-12-02-14:31:13-root-INFO: grad norm: 3.645 3.618 0.442
2024-12-02-14:31:13-root-INFO: Loss Change: 65.360 -> 53.235
2024-12-02-14:31:13-root-INFO: Regularization Change: 0.000 -> 20.080
2024-12-02-14:31:13-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-14:31:14-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-14:31:14-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-14:31:14-root-INFO: grad norm: 4.626 4.587 0.594
2024-12-02-14:31:15-root-INFO: Loss too large (53.139->53.734)! Learning rate decreased to 0.21688.
2024-12-02-14:31:16-root-INFO: grad norm: 5.330 5.304 0.524
2024-12-02-14:31:17-root-INFO: grad norm: 4.129 4.106 0.430
2024-12-02-14:31:18-root-INFO: grad norm: 3.253 3.225 0.420
2024-12-02-14:31:19-root-INFO: grad norm: 3.470 3.436 0.487
2024-12-02-14:31:20-root-INFO: grad norm: 3.518 3.475 0.544
2024-12-02-14:31:21-root-INFO: grad norm: 3.976 3.938 0.549
2024-12-02-14:31:21-root-INFO: Loss too large (48.522->48.753)! Learning rate decreased to 0.17350.
2024-12-02-14:31:22-root-INFO: grad norm: 3.907 3.868 0.550
2024-12-02-14:31:23-root-INFO: Loss Change: 53.139 -> 47.658
2024-12-02-14:31:23-root-INFO: Regularization Change: 0.000 -> 8.736
2024-12-02-14:31:23-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-14:31:23-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-14:31:23-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-14:31:23-root-INFO: grad norm: 4.991 4.926 0.807
2024-12-02-14:31:24-root-INFO: Loss too large (47.853->49.242)! Learning rate decreased to 0.22121.
2024-12-02-14:31:24-root-INFO: Loss too large (47.853->48.317)! Learning rate decreased to 0.17697.
2024-12-02-14:31:25-root-INFO: grad norm: 4.181 4.137 0.600
2024-12-02-14:31:26-root-INFO: grad norm: 3.303 3.269 0.468
2024-12-02-14:31:27-root-INFO: grad norm: 3.467 3.436 0.465
2024-12-02-14:31:28-root-INFO: grad norm: 3.766 3.736 0.478
2024-12-02-14:31:29-root-INFO: grad norm: 3.734 3.699 0.504
2024-12-02-14:31:30-root-INFO: grad norm: 3.670 3.639 0.477
2024-12-02-14:31:31-root-INFO: grad norm: 3.666 3.631 0.500
2024-12-02-14:31:32-root-INFO: Loss Change: 47.853 -> 44.326
2024-12-02-14:31:32-root-INFO: Regularization Change: 0.000 -> 4.412
2024-12-02-14:31:32-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-14:31:32-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-14:31:32-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-14:31:32-root-INFO: grad norm: 4.420 4.363 0.708
2024-12-02-14:31:33-root-INFO: Loss too large (44.431->45.788)! Learning rate decreased to 0.22559.
2024-12-02-14:31:33-root-INFO: Loss too large (44.431->44.890)! Learning rate decreased to 0.18047.
2024-12-02-14:31:34-root-INFO: grad norm: 3.971 3.930 0.566
2024-12-02-14:31:35-root-INFO: grad norm: 3.503 3.474 0.457
2024-12-02-14:31:36-root-INFO: grad norm: 3.512 3.481 0.467
2024-12-02-14:31:37-root-INFO: grad norm: 3.568 3.539 0.452
2024-12-02-14:31:38-root-INFO: grad norm: 3.580 3.548 0.477
2024-12-02-14:31:39-root-INFO: grad norm: 3.585 3.556 0.450
2024-12-02-14:31:40-root-INFO: grad norm: 3.583 3.552 0.474
2024-12-02-14:31:41-root-INFO: Loss Change: 44.431 -> 41.707
2024-12-02-14:31:41-root-INFO: Regularization Change: 0.000 -> 3.578
2024-12-02-14:31:41-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-14:31:41-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-14:31:41-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-14:31:42-root-INFO: grad norm: 4.353 4.293 0.715
2024-12-02-14:31:42-root-INFO: Loss too large (41.733->43.263)! Learning rate decreased to 0.23002.
2024-12-02-14:31:42-root-INFO: Loss too large (41.733->42.249)! Learning rate decreased to 0.18402.
2024-12-02-14:31:43-root-INFO: grad norm: 3.918 3.879 0.545
2024-12-02-14:31:44-root-INFO: grad norm: 3.436 3.408 0.435
2024-12-02-14:31:45-root-INFO: grad norm: 3.361 3.333 0.429
2024-12-02-14:31:46-root-INFO: grad norm: 3.350 3.323 0.426
2024-12-02-14:31:47-root-INFO: grad norm: 3.321 3.292 0.439
2024-12-02-14:31:48-root-INFO: grad norm: 3.297 3.270 0.422
2024-12-02-14:31:49-root-INFO: grad norm: 3.282 3.253 0.436
2024-12-02-14:31:50-root-INFO: Loss Change: 41.733 -> 39.303
2024-12-02-14:31:50-root-INFO: Regularization Change: 0.000 -> 3.089
2024-12-02-14:31:50-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-14:31:50-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-14:31:50-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-14:31:51-root-INFO: grad norm: 4.004 3.949 0.665
2024-12-02-14:31:51-root-INFO: Loss too large (39.423->40.808)! Learning rate decreased to 0.23450.
2024-12-02-14:31:51-root-INFO: Loss too large (39.423->39.875)! Learning rate decreased to 0.18760.
2024-12-02-14:31:52-root-INFO: grad norm: 3.650 3.616 0.502
2024-12-02-14:31:53-root-INFO: grad norm: 3.264 3.237 0.416
2024-12-02-14:31:54-root-INFO: grad norm: 3.187 3.161 0.405
2024-12-02-14:31:55-root-INFO: grad norm: 3.167 3.141 0.403
2024-12-02-14:31:56-root-INFO: grad norm: 3.134 3.108 0.409
2024-12-02-14:31:57-root-INFO: grad norm: 3.113 3.088 0.398
2024-12-02-14:31:58-root-INFO: grad norm: 3.099 3.072 0.407
2024-12-02-14:31:59-root-INFO: Loss Change: 39.423 -> 37.314
2024-12-02-14:31:59-root-INFO: Regularization Change: 0.000 -> 2.759
2024-12-02-14:31:59-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-14:31:59-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-14:31:59-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-14:32:00-root-INFO: grad norm: 3.895 3.840 0.655
2024-12-02-14:32:00-root-INFO: Loss too large (37.276->38.626)! Learning rate decreased to 0.23903.
2024-12-02-14:32:00-root-INFO: Loss too large (37.276->37.690)! Learning rate decreased to 0.19122.
2024-12-02-14:32:02-root-INFO: grad norm: 3.495 3.461 0.483
2024-12-02-14:32:03-root-INFO: grad norm: 3.090 3.065 0.395
2024-12-02-14:32:04-root-INFO: grad norm: 2.993 2.968 0.380
2024-12-02-14:32:05-root-INFO: grad norm: 2.955 2.931 0.376
2024-12-02-14:32:06-root-INFO: grad norm: 2.916 2.891 0.379
2024-12-02-14:32:07-root-INFO: grad norm: 2.898 2.874 0.372
2024-12-02-14:32:08-root-INFO: grad norm: 2.884 2.859 0.378
2024-12-02-14:32:08-root-INFO: Loss Change: 37.276 -> 35.310
2024-12-02-14:32:08-root-INFO: Regularization Change: 0.000 -> 2.534
2024-12-02-14:32:08-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-14:32:08-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-14:32:09-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-14:32:09-root-INFO: grad norm: 3.398 3.355 0.539
2024-12-02-14:32:09-root-INFO: Loss too large (35.285->36.391)! Learning rate decreased to 0.24361.
2024-12-02-14:32:10-root-INFO: Loss too large (35.285->35.618)! Learning rate decreased to 0.19489.
2024-12-02-14:32:11-root-INFO: grad norm: 3.163 3.134 0.430
2024-12-02-14:32:12-root-INFO: grad norm: 2.941 2.918 0.373
2024-12-02-14:32:13-root-INFO: grad norm: 2.873 2.849 0.374
2024-12-02-14:32:14-root-INFO: grad norm: 2.826 2.803 0.356
2024-12-02-14:32:15-root-INFO: grad norm: 2.793 2.769 0.367
2024-12-02-14:32:16-root-INFO: grad norm: 2.769 2.747 0.350
2024-12-02-14:32:17-root-INFO: grad norm: 2.756 2.732 0.363
2024-12-02-14:32:18-root-INFO: Loss Change: 35.285 -> 33.598
2024-12-02-14:32:18-root-INFO: Regularization Change: 0.000 -> 2.347
2024-12-02-14:32:18-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-14:32:18-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-14:32:18-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-14:32:18-root-INFO: grad norm: 3.860 3.793 0.714
2024-12-02-14:32:19-root-INFO: Loss too large (33.765->35.063)! Learning rate decreased to 0.24866.
2024-12-02-14:32:19-root-INFO: Loss too large (33.765->34.099)! Learning rate decreased to 0.19893.
2024-12-02-14:32:20-root-INFO: grad norm: 3.283 3.251 0.460
2024-12-02-14:32:21-root-INFO: grad norm: 2.776 2.753 0.356
2024-12-02-14:32:22-root-INFO: grad norm: 2.648 2.627 0.332
2024-12-02-14:32:23-root-INFO: grad norm: 2.600 2.579 0.327
2024-12-02-14:32:24-root-INFO: grad norm: 2.556 2.534 0.329
2024-12-02-14:32:25-root-INFO: grad norm: 2.546 2.525 0.323
2024-12-02-14:32:26-root-INFO: grad norm: 2.531 2.510 0.330
2024-12-02-14:32:27-root-INFO: Loss Change: 33.765 -> 31.885
2024-12-02-14:32:27-root-INFO: Regularization Change: 0.000 -> 2.326
2024-12-02-14:32:27-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-14:32:27-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-14:32:27-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-14:32:27-root-INFO: grad norm: 3.434 3.383 0.588
2024-12-02-14:32:28-root-INFO: Loss too large (32.006->33.123)! Learning rate decreased to 0.25333.
2024-12-02-14:32:28-root-INFO: Loss too large (32.006->32.311)! Learning rate decreased to 0.20266.
2024-12-02-14:32:29-root-INFO: grad norm: 3.026 2.997 0.417
2024-12-02-14:32:30-root-INFO: grad norm: 2.654 2.632 0.337
2024-12-02-14:32:31-root-INFO: grad norm: 2.548 2.527 0.326
2024-12-02-14:32:32-root-INFO: grad norm: 2.493 2.473 0.313
2024-12-02-14:32:33-root-INFO: grad norm: 2.447 2.426 0.319
2024-12-02-14:32:34-root-INFO: grad norm: 2.424 2.405 0.308
2024-12-02-14:32:35-root-INFO: grad norm: 2.407 2.386 0.317
2024-12-02-14:32:36-root-INFO: Loss Change: 32.006 -> 30.358
2024-12-02-14:32:36-root-INFO: Regularization Change: 0.000 -> 2.190
2024-12-02-14:32:36-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-14:32:36-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-14:32:36-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-14:32:37-root-INFO: grad norm: 3.055 3.011 0.516
2024-12-02-14:32:37-root-INFO: Loss too large (30.336->31.232)! Learning rate decreased to 0.25805.
2024-12-02-14:32:37-root-INFO: Loss too large (30.336->30.565)! Learning rate decreased to 0.20644.
2024-12-02-14:32:38-root-INFO: grad norm: 2.745 2.719 0.377
2024-12-02-14:32:39-root-INFO: grad norm: 2.484 2.464 0.316
2024-12-02-14:32:40-root-INFO: grad norm: 2.405 2.384 0.311
2024-12-02-14:32:41-root-INFO: grad norm: 2.352 2.333 0.293
2024-12-02-14:32:42-root-INFO: grad norm: 2.318 2.298 0.300
2024-12-02-14:32:43-root-INFO: grad norm: 2.300 2.282 0.287
2024-12-02-14:32:44-root-INFO: grad norm: 2.287 2.267 0.298
2024-12-02-14:32:45-root-INFO: Loss Change: 30.336 -> 28.858
2024-12-02-14:32:45-root-INFO: Regularization Change: 0.000 -> 2.096
2024-12-02-14:32:45-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-14:32:45-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-14:32:45-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-14:32:46-root-INFO: grad norm: 3.050 3.000 0.550
2024-12-02-14:32:46-root-INFO: Loss too large (28.829->29.691)! Learning rate decreased to 0.26281.
2024-12-02-14:32:46-root-INFO: Loss too large (28.829->29.027)! Learning rate decreased to 0.21025.
2024-12-02-14:32:47-root-INFO: grad norm: 2.668 2.643 0.363
2024-12-02-14:32:48-root-INFO: grad norm: 2.359 2.340 0.302
2024-12-02-14:32:50-root-INFO: grad norm: 2.265 2.247 0.285
2024-12-02-14:32:50-root-INFO: grad norm: 2.200 2.183 0.274
2024-12-02-14:32:51-root-INFO: grad norm: 2.160 2.143 0.273
2024-12-02-14:32:53-root-INFO: grad norm: 2.141 2.124 0.267
2024-12-02-14:32:54-root-INFO: grad norm: 2.126 2.108 0.272
2024-12-02-14:32:54-root-INFO: Loss Change: 28.829 -> 27.356
2024-12-02-14:32:54-root-INFO: Regularization Change: 0.000 -> 2.056
2024-12-02-14:32:54-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-14:32:54-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-14:32:55-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-14:32:55-root-INFO: grad norm: 2.977 2.931 0.519
2024-12-02-14:32:55-root-INFO: Loss too large (27.399->28.216)! Learning rate decreased to 0.26762.
2024-12-02-14:32:56-root-INFO: Loss too large (27.399->27.571)! Learning rate decreased to 0.21410.
2024-12-02-14:32:57-root-INFO: grad norm: 2.575 2.551 0.351
2024-12-02-14:32:58-root-INFO: grad norm: 2.273 2.255 0.283
2024-12-02-14:32:59-root-INFO: grad norm: 2.162 2.144 0.275
2024-12-02-14:33:00-root-INFO: grad norm: 2.078 2.063 0.254
2024-12-02-14:33:01-root-INFO: grad norm: 2.030 2.014 0.258
2024-12-02-14:33:02-root-INFO: grad norm: 2.000 1.985 0.246
2024-12-02-14:33:03-root-INFO: grad norm: 1.981 1.964 0.254
2024-12-02-14:33:03-root-INFO: Loss Change: 27.399 -> 25.954
2024-12-02-14:33:03-root-INFO: Regularization Change: 0.000 -> 2.016
2024-12-02-14:33:03-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-14:33:03-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-14:33:04-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-14:33:04-root-INFO: grad norm: 2.585 2.546 0.449
2024-12-02-14:33:05-root-INFO: Loss too large (25.921->26.526)! Learning rate decreased to 0.27247.
2024-12-02-14:33:05-root-INFO: Loss too large (25.921->26.041)! Learning rate decreased to 0.21798.
2024-12-02-14:33:06-root-INFO: grad norm: 2.304 2.284 0.308
2024-12-02-14:33:07-root-INFO: grad norm: 2.100 2.083 0.268
2024-12-02-14:33:08-root-INFO: grad norm: 2.030 2.013 0.258
2024-12-02-14:33:09-root-INFO: grad norm: 1.967 1.952 0.243
2024-12-02-14:33:10-root-INFO: grad norm: 1.936 1.921 0.246
2024-12-02-14:33:11-root-INFO: grad norm: 1.915 1.900 0.235
2024-12-02-14:33:12-root-INFO: grad norm: 1.902 1.887 0.243
2024-12-02-14:33:13-root-INFO: Loss Change: 25.921 -> 24.630
2024-12-02-14:33:13-root-INFO: Regularization Change: 0.000 -> 1.961
2024-12-02-14:33:13-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-14:33:13-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-14:33:13-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-14:33:13-root-INFO: grad norm: 2.643 2.602 0.465
2024-12-02-14:33:14-root-INFO: Loss too large (24.671->25.284)! Learning rate decreased to 0.27737.
2024-12-02-14:33:14-root-INFO: Loss too large (24.671->24.771)! Learning rate decreased to 0.22190.
2024-12-02-14:33:15-root-INFO: grad norm: 2.276 2.255 0.314
2024-12-02-14:33:16-root-INFO: grad norm: 2.017 2.002 0.251
2024-12-02-14:33:17-root-INFO: grad norm: 1.920 1.904 0.244
2024-12-02-14:33:18-root-INFO: grad norm: 1.840 1.827 0.222
2024-12-02-14:33:19-root-INFO: grad norm: 1.800 1.786 0.226
2024-12-02-14:33:20-root-INFO: grad norm: 1.774 1.761 0.214
2024-12-02-14:33:21-root-INFO: grad norm: 1.759 1.745 0.220
2024-12-02-14:33:22-root-INFO: Loss Change: 24.671 -> 23.358
2024-12-02-14:33:22-root-INFO: Regularization Change: 0.000 -> 1.941
2024-12-02-14:33:22-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-14:33:22-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-14:33:22-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-14:33:22-root-INFO: grad norm: 2.357 2.323 0.399
2024-12-02-14:33:23-root-INFO: Loss too large (23.320->23.779)! Learning rate decreased to 0.28231.
2024-12-02-14:33:23-root-INFO: Loss too large (23.320->23.364)! Learning rate decreased to 0.22585.
2024-12-02-14:33:24-root-INFO: grad norm: 2.063 2.045 0.272
2024-12-02-14:33:25-root-INFO: grad norm: 1.894 1.881 0.226
2024-12-02-14:33:26-root-INFO: grad norm: 1.808 1.794 0.227
2024-12-02-14:33:27-root-INFO: grad norm: 1.731 1.719 0.203
2024-12-02-14:33:28-root-INFO: grad norm: 1.688 1.675 0.211
2024-12-02-14:33:29-root-INFO: grad norm: 1.655 1.643 0.195
2024-12-02-14:33:30-root-INFO: grad norm: 1.635 1.622 0.206
2024-12-02-14:33:31-root-INFO: Loss Change: 23.320 -> 22.096
2024-12-02-14:33:31-root-INFO: Regularization Change: 0.000 -> 1.900
2024-12-02-14:33:31-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-14:33:31-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-14:33:31-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-14:33:31-root-INFO: grad norm: 2.278 2.241 0.407
2024-12-02-14:33:32-root-INFO: Loss too large (21.939->22.332)! Learning rate decreased to 0.28730.
2024-12-02-14:33:32-root-INFO: Loss too large (21.939->21.954)! Learning rate decreased to 0.22984.
2024-12-02-14:33:33-root-INFO: grad norm: 1.962 1.945 0.257
2024-12-02-14:33:34-root-INFO: grad norm: 1.785 1.772 0.217
2024-12-02-14:33:35-root-INFO: grad norm: 1.700 1.687 0.211
2024-12-02-14:33:36-root-INFO: grad norm: 1.622 1.611 0.191
2024-12-02-14:33:37-root-INFO: grad norm: 1.581 1.569 0.196
2024-12-02-14:33:38-root-INFO: grad norm: 1.546 1.536 0.182
2024-12-02-14:33:39-root-INFO: grad norm: 1.528 1.516 0.190
2024-12-02-14:33:40-root-INFO: Loss Change: 21.939 -> 20.737
2024-12-02-14:33:40-root-INFO: Regularization Change: 0.000 -> 1.891
2024-12-02-14:33:40-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-14:33:40-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-14:33:40-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-14:33:40-root-INFO: grad norm: 2.330 2.292 0.419
2024-12-02-14:33:41-root-INFO: Loss too large (20.803->21.183)! Learning rate decreased to 0.29232.
2024-12-02-14:33:42-root-INFO: grad norm: 2.576 2.548 0.377
2024-12-02-14:33:43-root-INFO: grad norm: 3.079 3.057 0.372
2024-12-02-14:33:43-root-INFO: Loss too large (20.698->20.884)! Learning rate decreased to 0.23386.
2024-12-02-14:33:44-root-INFO: grad norm: 2.358 2.338 0.308
2024-12-02-14:33:45-root-INFO: grad norm: 1.688 1.677 0.193
2024-12-02-14:33:46-root-INFO: grad norm: 1.475 1.464 0.179
2024-12-02-14:33:47-root-INFO: grad norm: 1.345 1.336 0.154
2024-12-02-14:33:48-root-INFO: grad norm: 1.277 1.268 0.157
2024-12-02-14:33:49-root-INFO: Loss Change: 20.803 -> 19.507
2024-12-02-14:33:49-root-INFO: Regularization Change: 0.000 -> 2.113
2024-12-02-14:33:49-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-14:33:49-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-14:33:49-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-14:33:50-root-INFO: grad norm: 1.810 1.780 0.327
2024-12-02-14:33:50-root-INFO: Loss too large (19.430->19.569)! Learning rate decreased to 0.29738.
2024-12-02-14:33:51-root-INFO: grad norm: 2.046 2.025 0.292
2024-12-02-14:33:52-root-INFO: grad norm: 2.642 2.624 0.313
2024-12-02-14:33:52-root-INFO: Loss too large (19.308->19.476)! Learning rate decreased to 0.23791.
2024-12-02-14:33:53-root-INFO: grad norm: 2.146 2.128 0.279
2024-12-02-14:33:54-root-INFO: grad norm: 1.626 1.615 0.188
2024-12-02-14:33:55-root-INFO: grad norm: 1.454 1.443 0.181
2024-12-02-14:33:56-root-INFO: grad norm: 1.325 1.316 0.153
2024-12-02-14:33:57-root-INFO: grad norm: 1.260 1.250 0.158
2024-12-02-14:33:58-root-INFO: Loss Change: 19.430 -> 18.306
2024-12-02-14:33:58-root-INFO: Regularization Change: 0.000 -> 2.034
2024-12-02-14:33:58-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-14:33:58-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-14:33:58-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-14:33:59-root-INFO: grad norm: 1.939 1.906 0.353
2024-12-02-14:33:59-root-INFO: Loss too large (18.272->18.464)! Learning rate decreased to 0.30249.
2024-12-02-14:34:00-root-INFO: grad norm: 2.131 2.108 0.310
2024-12-02-14:34:01-root-INFO: grad norm: 2.672 2.653 0.318
2024-12-02-14:34:01-root-INFO: Loss too large (18.138->18.308)! Learning rate decreased to 0.24199.
2024-12-02-14:34:02-root-INFO: grad norm: 2.117 2.098 0.281
2024-12-02-14:34:03-root-INFO: grad norm: 1.539 1.529 0.179
2024-12-02-14:34:04-root-INFO: grad norm: 1.358 1.347 0.175
2024-12-02-14:34:05-root-INFO: grad norm: 1.223 1.215 0.142
2024-12-02-14:34:06-root-INFO: grad norm: 1.155 1.145 0.150
2024-12-02-14:34:07-root-INFO: Loss Change: 18.272 -> 17.131
2024-12-02-14:34:07-root-INFO: Regularization Change: 0.000 -> 2.004
2024-12-02-14:34:07-root-INFO: Undo step: 30
2024-12-02-14:34:07-root-INFO: Undo step: 31
2024-12-02-14:34:07-root-INFO: Undo step: 32
2024-12-02-14:34:07-root-INFO: Undo step: 33
2024-12-02-14:34:07-root-INFO: Undo step: 34
2024-12-02-14:34:07-root-INFO: Undo step: 35
2024-12-02-14:34:07-root-INFO: Undo step: 36
2024-12-02-14:34:07-root-INFO: Undo step: 37
2024-12-02-14:34:07-root-INFO: Undo step: 38
2024-12-02-14:34:07-root-INFO: Undo step: 39
2024-12-02-14:34:07-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-14:34:08-root-INFO: grad norm: 22.912 22.783 2.432
2024-12-02-14:34:09-root-INFO: grad norm: 10.416 10.319 1.422
2024-12-02-14:34:10-root-INFO: grad norm: 8.204 8.104 1.274
2024-12-02-14:34:11-root-INFO: grad norm: 8.516 8.459 0.977
2024-12-02-14:34:12-root-INFO: grad norm: 7.490 7.386 1.247
2024-12-02-14:34:13-root-INFO: grad norm: 6.584 6.543 0.730
2024-12-02-14:34:14-root-INFO: grad norm: 5.557 5.470 0.978
2024-12-02-14:34:15-root-INFO: grad norm: 5.268 5.222 0.695
2024-12-02-14:34:15-root-INFO: Loss Change: 190.385 -> 46.002
2024-12-02-14:34:16-root-INFO: Regularization Change: 0.000 -> 203.328
2024-12-02-14:34:16-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-14:34:16-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-14:34:16-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-14:34:16-root-INFO: grad norm: 5.085 5.029 0.753
2024-12-02-14:34:17-root-INFO: grad norm: 6.029 5.975 0.805
2024-12-02-14:34:18-root-INFO: grad norm: 5.735 5.658 0.931
2024-12-02-14:34:19-root-INFO: grad norm: 5.595 5.544 0.751
2024-12-02-14:34:20-root-INFO: grad norm: 5.659 5.592 0.864
2024-12-02-14:34:21-root-INFO: grad norm: 5.627 5.568 0.813
2024-12-02-14:34:22-root-INFO: grad norm: 5.498 5.417 0.941
2024-12-02-14:34:23-root-INFO: grad norm: 6.014 5.941 0.938
2024-12-02-14:34:24-root-INFO: Loss too large (36.829->37.011)! Learning rate decreased to 0.25805.
2024-12-02-14:34:24-root-INFO: Loss Change: 45.376 -> 35.353
2024-12-02-14:34:24-root-INFO: Regularization Change: 0.000 -> 25.321
2024-12-02-14:34:24-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-14:34:24-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-14:34:25-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-14:34:25-root-INFO: grad norm: 3.966 3.926 0.568
2024-12-02-14:34:26-root-INFO: grad norm: 4.390 4.345 0.628
2024-12-02-14:34:26-root-INFO: Loss too large (33.666->33.778)! Learning rate decreased to 0.26281.
2024-12-02-14:34:27-root-INFO: grad norm: 3.641 3.596 0.570
2024-12-02-14:34:28-root-INFO: grad norm: 3.017 2.986 0.433
2024-12-02-14:34:29-root-INFO: grad norm: 2.832 2.800 0.424
2024-12-02-14:34:30-root-INFO: grad norm: 2.837 2.808 0.398
2024-12-02-14:34:31-root-INFO: grad norm: 2.842 2.811 0.419
2024-12-02-14:34:32-root-INFO: grad norm: 2.915 2.887 0.400
2024-12-02-14:34:33-root-INFO: Loss Change: 34.658 -> 29.505
2024-12-02-14:34:33-root-INFO: Regularization Change: 0.000 -> 9.541
2024-12-02-14:34:33-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-14:34:33-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-14:34:33-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-14:34:34-root-INFO: grad norm: 2.768 2.744 0.363
2024-12-02-14:34:35-root-INFO: grad norm: 4.010 3.982 0.471
2024-12-02-14:34:35-root-INFO: Loss too large (28.948->29.575)! Learning rate decreased to 0.26762.
2024-12-02-14:34:36-root-INFO: grad norm: 3.434 3.402 0.464
2024-12-02-14:34:37-root-INFO: grad norm: 2.651 2.632 0.315
2024-12-02-14:34:38-root-INFO: grad norm: 2.553 2.532 0.324
2024-12-02-14:34:39-root-INFO: grad norm: 2.599 2.578 0.324
2024-12-02-14:34:40-root-INFO: grad norm: 2.650 2.626 0.356
2024-12-02-14:34:41-root-INFO: grad norm: 2.804 2.781 0.360
2024-12-02-14:34:42-root-INFO: Loss Change: 29.115 -> 26.250
2024-12-02-14:34:42-root-INFO: Regularization Change: 0.000 -> 6.507
2024-12-02-14:34:42-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-14:34:42-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-14:34:42-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-14:34:43-root-INFO: grad norm: 2.691 2.671 0.332
2024-12-02-14:34:43-root-INFO: Loss too large (25.934->25.946)! Learning rate decreased to 0.27247.
2024-12-02-14:34:44-root-INFO: grad norm: 2.710 2.690 0.323
2024-12-02-14:34:45-root-INFO: grad norm: 2.732 2.709 0.353
2024-12-02-14:34:46-root-INFO: grad norm: 2.774 2.753 0.344
2024-12-02-14:34:47-root-INFO: grad norm: 2.800 2.775 0.375
2024-12-02-14:34:48-root-INFO: grad norm: 2.812 2.789 0.354
2024-12-02-14:34:49-root-INFO: grad norm: 2.831 2.805 0.382
2024-12-02-14:34:50-root-INFO: grad norm: 2.839 2.816 0.360
2024-12-02-14:34:51-root-INFO: Loss Change: 25.934 -> 23.883
2024-12-02-14:34:51-root-INFO: Regularization Change: 0.000 -> 4.714
2024-12-02-14:34:51-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-14:34:51-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-14:34:51-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-14:34:51-root-INFO: grad norm: 2.661 2.640 0.331
2024-12-02-14:34:52-root-INFO: Loss too large (23.590->23.631)! Learning rate decreased to 0.27737.
2024-12-02-14:34:53-root-INFO: grad norm: 2.575 2.558 0.296
2024-12-02-14:34:54-root-INFO: grad norm: 2.580 2.560 0.321
2024-12-02-14:34:55-root-INFO: grad norm: 2.621 2.602 0.313
2024-12-02-14:34:56-root-INFO: grad norm: 2.653 2.630 0.343
2024-12-02-14:34:57-root-INFO: grad norm: 2.692 2.671 0.331
2024-12-02-14:34:58-root-INFO: grad norm: 2.706 2.682 0.358
2024-12-02-14:34:59-root-INFO: grad norm: 2.706 2.685 0.338
2024-12-02-14:34:59-root-INFO: Loss Change: 23.590 -> 21.882
2024-12-02-14:34:59-root-INFO: Regularization Change: 0.000 -> 3.885
2024-12-02-14:34:59-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-14:34:59-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-14:35:00-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-14:35:00-root-INFO: grad norm: 2.551 2.533 0.300
2024-12-02-14:35:00-root-INFO: Loss too large (21.586->21.665)! Learning rate decreased to 0.28231.
2024-12-02-14:35:01-root-INFO: grad norm: 2.476 2.461 0.275
2024-12-02-14:35:02-root-INFO: grad norm: 2.462 2.443 0.303
2024-12-02-14:35:03-root-INFO: grad norm: 2.464 2.448 0.287
2024-12-02-14:35:04-root-INFO: grad norm: 2.491 2.471 0.319
2024-12-02-14:35:05-root-INFO: grad norm: 2.534 2.515 0.305
2024-12-02-14:35:06-root-INFO: grad norm: 2.545 2.522 0.334
2024-12-02-14:35:07-root-INFO: grad norm: 2.542 2.523 0.311
2024-12-02-14:35:08-root-INFO: Loss Change: 21.586 -> 20.129
2024-12-02-14:35:08-root-INFO: Regularization Change: 0.000 -> 3.317
2024-12-02-14:35:08-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-14:35:08-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-14:35:08-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-14:35:09-root-INFO: grad norm: 2.379 2.361 0.289
2024-12-02-14:35:09-root-INFO: Loss too large (19.698->19.779)! Learning rate decreased to 0.28730.
2024-12-02-14:35:10-root-INFO: grad norm: 2.332 2.317 0.260
2024-12-02-14:35:11-root-INFO: grad norm: 2.322 2.305 0.279
2024-12-02-14:35:12-root-INFO: grad norm: 2.324 2.309 0.268
2024-12-02-14:35:13-root-INFO: grad norm: 2.359 2.341 0.294
2024-12-02-14:35:14-root-INFO: grad norm: 2.413 2.396 0.287
2024-12-02-14:35:15-root-INFO: grad norm: 2.425 2.405 0.311
2024-12-02-14:35:16-root-INFO: grad norm: 2.421 2.403 0.292
2024-12-02-14:35:17-root-INFO: Loss Change: 19.698 -> 18.459
2024-12-02-14:35:17-root-INFO: Regularization Change: 0.000 -> 2.936
2024-12-02-14:35:17-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-14:35:17-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-14:35:17-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-14:35:18-root-INFO: grad norm: 2.280 2.261 0.287
2024-12-02-14:35:18-root-INFO: Loss too large (18.236->18.285)! Learning rate decreased to 0.29232.
2024-12-02-14:35:19-root-INFO: grad norm: 2.194 2.181 0.240
2024-12-02-14:35:20-root-INFO: grad norm: 2.173 2.157 0.262
2024-12-02-14:35:21-root-INFO: grad norm: 2.173 2.159 0.244
2024-12-02-14:35:22-root-INFO: grad norm: 2.212 2.195 0.275
2024-12-02-14:35:23-root-INFO: grad norm: 2.278 2.263 0.266
2024-12-02-14:35:23-root-INFO: Loss too large (17.374->17.383)! Learning rate decreased to 0.23386.
2024-12-02-14:35:24-root-INFO: grad norm: 1.692 1.678 0.219
2024-12-02-14:35:25-root-INFO: grad norm: 1.212 1.202 0.150
2024-12-02-14:35:26-root-INFO: Loss Change: 18.236 -> 16.806
2024-12-02-14:35:26-root-INFO: Regularization Change: 0.000 -> 2.341
2024-12-02-14:35:26-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-14:35:26-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-14:35:27-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-14:35:27-root-INFO: grad norm: 1.221 1.198 0.236
2024-12-02-14:35:28-root-INFO: grad norm: 1.528 1.513 0.217
2024-12-02-14:35:28-root-INFO: Loss too large (16.468->16.510)! Learning rate decreased to 0.29738.
2024-12-02-14:35:29-root-INFO: grad norm: 1.517 1.503 0.202
2024-12-02-14:35:30-root-INFO: grad norm: 1.663 1.651 0.195
2024-12-02-14:35:31-root-INFO: grad norm: 1.732 1.718 0.220
2024-12-02-14:35:32-root-INFO: grad norm: 1.844 1.833 0.204
2024-12-02-14:35:33-root-INFO: Loss too large (16.009->16.035)! Learning rate decreased to 0.23791.
2024-12-02-14:35:33-root-INFO: grad norm: 1.516 1.505 0.187
2024-12-02-14:35:35-root-INFO: grad norm: 1.190 1.181 0.141
2024-12-02-14:35:35-root-INFO: Loss Change: 16.624 -> 15.601
2024-12-02-14:35:35-root-INFO: Regularization Change: 0.000 -> 2.259
2024-12-02-14:35:35-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-14:35:35-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-14:35:36-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-14:35:36-root-INFO: grad norm: 1.307 1.280 0.262
2024-12-02-14:35:37-root-INFO: grad norm: 1.619 1.602 0.234
2024-12-02-14:35:37-root-INFO: Loss too large (15.322->15.414)! Learning rate decreased to 0.30249.
2024-12-02-14:35:38-root-INFO: grad norm: 1.661 1.649 0.206
2024-12-02-14:35:39-root-INFO: grad norm: 1.983 1.971 0.217
2024-12-02-14:35:40-root-INFO: Loss too large (15.127->15.200)! Learning rate decreased to 0.24199.
2024-12-02-14:35:41-root-INFO: grad norm: 1.594 1.582 0.195
2024-12-02-14:35:42-root-INFO: grad norm: 1.161 1.154 0.128
2024-12-02-14:35:43-root-INFO: grad norm: 1.057 1.050 0.125
2024-12-02-14:35:44-root-INFO: grad norm: 0.981 0.974 0.115
2024-12-02-14:35:44-root-INFO: Loss Change: 15.454 -> 14.515
2024-12-02-14:35:44-root-INFO: Regularization Change: 0.000 -> 1.912
2024-12-02-14:35:44-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-14:35:44-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-14:35:45-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-14:35:45-root-INFO: grad norm: 1.297 1.267 0.281
2024-12-02-14:35:46-root-INFO: grad norm: 1.536 1.516 0.248
2024-12-02-14:35:46-root-INFO: Loss too large (14.275->14.342)! Learning rate decreased to 0.30763.
2024-12-02-14:35:47-root-INFO: grad norm: 1.598 1.585 0.201
2024-12-02-14:35:48-root-INFO: grad norm: 1.916 1.904 0.215
2024-12-02-14:35:49-root-INFO: Loss too large (14.105->14.146)! Learning rate decreased to 0.24610.
2024-12-02-14:35:50-root-INFO: grad norm: 1.486 1.476 0.180
2024-12-02-14:35:51-root-INFO: grad norm: 1.074 1.068 0.118
2024-12-02-14:35:52-root-INFO: grad norm: 0.992 0.985 0.114
2024-12-02-14:35:53-root-INFO: grad norm: 0.935 0.928 0.108
2024-12-02-14:35:54-root-INFO: Loss Change: 14.408 -> 13.517
2024-12-02-14:35:54-root-INFO: Regularization Change: 0.000 -> 1.830
2024-12-02-14:35:54-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-14:35:54-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-14:35:54-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-14:35:54-root-INFO: grad norm: 1.374 1.343 0.292
2024-12-02-14:35:55-root-INFO: grad norm: 1.733 1.712 0.269
2024-12-02-14:35:56-root-INFO: Loss too large (13.249->13.421)! Learning rate decreased to 0.31281.
2024-12-02-14:35:57-root-INFO: grad norm: 1.840 1.826 0.225
2024-12-02-14:35:58-root-INFO: grad norm: 2.184 2.170 0.244
2024-12-02-14:35:58-root-INFO: Loss too large (13.126->13.203)! Learning rate decreased to 0.25025.
2024-12-02-14:35:59-root-INFO: grad norm: 1.631 1.619 0.196
2024-12-02-14:36:00-root-INFO: grad norm: 1.104 1.097 0.123
2024-12-02-14:36:01-root-INFO: grad norm: 1.001 0.995 0.114
2024-12-02-14:36:02-root-INFO: grad norm: 0.940 0.934 0.109
2024-12-02-14:36:03-root-INFO: Loss Change: 13.343 -> 12.486
2024-12-02-14:36:03-root-INFO: Regularization Change: 0.000 -> 1.750
2024-12-02-14:36:03-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-14:36:03-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-14:36:03-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-14:36:03-root-INFO: grad norm: 1.297 1.270 0.263
2024-12-02-14:36:04-root-INFO: grad norm: 1.730 1.710 0.261
2024-12-02-14:36:05-root-INFO: Loss too large (12.355->12.575)! Learning rate decreased to 0.31802.
2024-12-02-14:36:06-root-INFO: grad norm: 1.893 1.878 0.233
2024-12-02-14:36:07-root-INFO: grad norm: 2.264 2.250 0.253
2024-12-02-14:36:07-root-INFO: Loss too large (12.278->12.375)! Learning rate decreased to 0.25442.
2024-12-02-14:36:08-root-INFO: grad norm: 1.675 1.663 0.204
2024-12-02-14:36:09-root-INFO: grad norm: 1.117 1.110 0.124
2024-12-02-14:36:10-root-INFO: grad norm: 0.996 0.990 0.114
2024-12-02-14:36:11-root-INFO: grad norm: 0.935 0.929 0.108
2024-12-02-14:36:12-root-INFO: Loss Change: 12.421 -> 11.632
2024-12-02-14:36:12-root-INFO: Regularization Change: 0.000 -> 1.659
2024-12-02-14:36:12-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-14:36:12-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-14:36:12-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-14:36:13-root-INFO: grad norm: 1.392 1.362 0.284
2024-12-02-14:36:14-root-INFO: grad norm: 1.901 1.879 0.284
2024-12-02-14:36:14-root-INFO: Loss too large (11.535->11.828)! Learning rate decreased to 0.32327.
2024-12-02-14:36:15-root-INFO: grad norm: 1.980 1.964 0.250
2024-12-02-14:36:16-root-INFO: grad norm: 2.175 2.161 0.239
2024-12-02-14:36:16-root-INFO: Loss too large (11.416->11.483)! Learning rate decreased to 0.25862.
2024-12-02-14:36:18-root-INFO: grad norm: 1.603 1.592 0.190
2024-12-02-14:36:19-root-INFO: grad norm: 1.117 1.110 0.125
2024-12-02-14:36:20-root-INFO: grad norm: 1.007 1.000 0.118
2024-12-02-14:36:21-root-INFO: grad norm: 0.949 0.943 0.110
2024-12-02-14:36:21-root-INFO: Loss Change: 11.578 -> 10.800
2024-12-02-14:36:21-root-INFO: Regularization Change: 0.000 -> 1.618
2024-12-02-14:36:21-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-14:36:21-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-14:36:22-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-14:36:22-root-INFO: grad norm: 1.309 1.281 0.269
2024-12-02-14:36:23-root-INFO: grad norm: 1.693 1.674 0.250
2024-12-02-14:36:23-root-INFO: Loss too large (10.577->10.783)! Learning rate decreased to 0.32856.
2024-12-02-14:36:24-root-INFO: grad norm: 1.785 1.772 0.216
2024-12-02-14:36:25-root-INFO: grad norm: 2.026 2.014 0.220
2024-12-02-14:36:26-root-INFO: Loss too large (10.469->10.525)! Learning rate decreased to 0.26285.
2024-12-02-14:36:27-root-INFO: grad norm: 1.490 1.480 0.179
2024-12-02-14:36:28-root-INFO: grad norm: 1.020 1.013 0.114
2024-12-02-14:36:29-root-INFO: grad norm: 0.921 0.915 0.107
2024-12-02-14:36:30-root-INFO: grad norm: 0.863 0.857 0.099
2024-12-02-14:36:30-root-INFO: Loss Change: 10.648 -> 9.902
2024-12-02-14:36:30-root-INFO: Regularization Change: 0.000 -> 1.569
2024-12-02-14:36:30-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-14:36:30-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-14:36:31-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-14:36:31-root-INFO: grad norm: 1.284 1.257 0.265
2024-12-02-14:36:32-root-INFO: grad norm: 1.469 1.451 0.233
2024-12-02-14:36:32-root-INFO: Loss too large (9.802->9.877)! Learning rate decreased to 0.33388.
2024-12-02-14:36:33-root-INFO: grad norm: 1.520 1.510 0.173
2024-12-02-14:36:34-root-INFO: grad norm: 1.803 1.793 0.192
2024-12-02-14:36:35-root-INFO: Loss too large (9.685->9.715)! Learning rate decreased to 0.26710.
2024-12-02-14:36:36-root-INFO: grad norm: 1.392 1.383 0.164
2024-12-02-14:36:37-root-INFO: grad norm: 1.111 1.104 0.125
2024-12-02-14:36:38-root-INFO: grad norm: 1.042 1.034 0.127
2024-12-02-14:36:39-root-INFO: grad norm: 0.994 0.988 0.114
2024-12-02-14:36:40-root-INFO: Loss Change: 9.899 -> 9.203
2024-12-02-14:36:40-root-INFO: Regularization Change: 0.000 -> 1.537
2024-12-02-14:36:40-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-14:36:40-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-14:36:40-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-14:36:40-root-INFO: grad norm: 1.338 1.312 0.262
2024-12-02-14:36:41-root-INFO: grad norm: 1.873 1.855 0.258
2024-12-02-14:36:42-root-INFO: Loss too large (9.039->9.354)! Learning rate decreased to 0.33923.
2024-12-02-14:36:42-root-INFO: Loss too large (9.039->9.081)! Learning rate decreased to 0.27138.
2024-12-02-14:36:43-root-INFO: grad norm: 1.423 1.413 0.174
2024-12-02-14:36:44-root-INFO: grad norm: 1.062 1.056 0.114
2024-12-02-14:36:45-root-INFO: grad norm: 0.980 0.974 0.109
2024-12-02-14:36:46-root-INFO: grad norm: 0.925 0.920 0.100
2024-12-02-14:36:47-root-INFO: grad norm: 0.898 0.892 0.102
2024-12-02-14:36:48-root-INFO: grad norm: 0.882 0.877 0.095
2024-12-02-14:36:49-root-INFO: Loss Change: 9.082 -> 8.416
2024-12-02-14:36:49-root-INFO: Regularization Change: 0.000 -> 1.328
2024-12-02-14:36:49-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-14:36:49-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-14:36:49-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-14:36:49-root-INFO: grad norm: 1.246 1.222 0.244
2024-12-02-14:36:50-root-INFO: grad norm: 1.538 1.523 0.218
2024-12-02-14:36:51-root-INFO: Loss too large (8.322->8.463)! Learning rate decreased to 0.34461.
2024-12-02-14:36:52-root-INFO: grad norm: 1.595 1.586 0.174
2024-12-02-14:36:53-root-INFO: grad norm: 1.768 1.758 0.192
2024-12-02-14:36:53-root-INFO: Loss too large (8.217->8.223)! Learning rate decreased to 0.27569.
2024-12-02-14:36:54-root-INFO: grad norm: 1.298 1.288 0.163
2024-12-02-14:36:55-root-INFO: grad norm: 0.978 0.972 0.110
2024-12-02-14:36:56-root-INFO: grad norm: 0.915 0.908 0.110
2024-12-02-14:36:57-root-INFO: grad norm: 0.874 0.869 0.098
2024-12-02-14:36:58-root-INFO: Loss Change: 8.383 -> 7.741
2024-12-02-14:36:58-root-INFO: Regularization Change: 0.000 -> 1.423
2024-12-02-14:36:58-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-14:36:58-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-14:36:58-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-14:36:58-root-INFO: grad norm: 1.214 1.191 0.234
2024-12-02-14:36:59-root-INFO: grad norm: 1.539 1.523 0.218
2024-12-02-14:37:00-root-INFO: Loss too large (7.610->7.801)! Learning rate decreased to 0.35002.
2024-12-02-14:37:01-root-INFO: grad norm: 1.624 1.613 0.190
2024-12-02-14:37:02-root-INFO: grad norm: 1.840 1.830 0.193
2024-12-02-14:37:02-root-INFO: Loss too large (7.530->7.592)! Learning rate decreased to 0.28002.
2024-12-02-14:37:03-root-INFO: grad norm: 1.393 1.382 0.170
2024-12-02-14:37:04-root-INFO: grad norm: 1.042 1.036 0.109
2024-12-02-14:37:05-root-INFO: grad norm: 0.950 0.944 0.110
2024-12-02-14:37:06-root-INFO: grad norm: 0.880 0.875 0.093
2024-12-02-14:37:07-root-INFO: Loss Change: 7.678 -> 7.065
2024-12-02-14:37:07-root-INFO: Regularization Change: 0.000 -> 1.376
2024-12-02-14:37:07-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-14:37:07-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-14:37:07-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-14:37:08-root-INFO: grad norm: 1.236 1.211 0.246
2024-12-02-14:37:09-root-INFO: grad norm: 1.440 1.426 0.201
2024-12-02-14:37:09-root-INFO: Loss too large (6.951->7.073)! Learning rate decreased to 0.35546.
2024-12-02-14:37:10-root-INFO: grad norm: 1.481 1.474 0.152
2024-12-02-14:37:11-root-INFO: grad norm: 1.677 1.668 0.165
2024-12-02-14:37:11-root-INFO: Loss too large (6.859->6.878)! Learning rate decreased to 0.28437.
2024-12-02-14:37:12-root-INFO: grad norm: 1.257 1.249 0.145
2024-12-02-14:37:13-root-INFO: grad norm: 1.023 1.018 0.100
2024-12-02-14:37:14-root-INFO: grad norm: 0.929 0.923 0.105
2024-12-02-14:37:15-root-INFO: grad norm: 0.816 0.812 0.083
2024-12-02-14:37:16-root-INFO: Loss Change: 7.036 -> 6.428
2024-12-02-14:37:16-root-INFO: Regularization Change: 0.000 -> 1.339
2024-12-02-14:37:16-root-INFO: Undo step: 20
2024-12-02-14:37:16-root-INFO: Undo step: 21
2024-12-02-14:37:16-root-INFO: Undo step: 22
2024-12-02-14:37:16-root-INFO: Undo step: 23
2024-12-02-14:37:16-root-INFO: Undo step: 24
2024-12-02-14:37:16-root-INFO: Undo step: 25
2024-12-02-14:37:16-root-INFO: Undo step: 26
2024-12-02-14:37:16-root-INFO: Undo step: 27
2024-12-02-14:37:16-root-INFO: Undo step: 28
2024-12-02-14:37:16-root-INFO: Undo step: 29
2024-12-02-14:37:16-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-14:37:17-root-INFO: grad norm: 19.214 19.124 1.865
2024-12-02-14:37:18-root-INFO: grad norm: 8.626 8.552 1.131
2024-12-02-14:37:19-root-INFO: grad norm: 6.123 6.079 0.736
2024-12-02-14:37:20-root-INFO: grad norm: 5.250 5.209 0.657
2024-12-02-14:37:21-root-INFO: grad norm: 5.039 5.011 0.525
2024-12-02-14:37:22-root-INFO: grad norm: 4.786 4.754 0.552
2024-12-02-14:37:23-root-INFO: grad norm: 4.574 4.554 0.437
2024-12-02-14:37:24-root-INFO: grad norm: 4.452 4.425 0.488
2024-12-02-14:37:25-root-INFO: Loss Change: 157.866 -> 30.736
2024-12-02-14:37:25-root-INFO: Regularization Change: 0.000 -> 224.534
2024-12-02-14:37:25-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-14:37:25-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-14:37:25-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-14:37:25-root-INFO: grad norm: 4.690 4.661 0.525
2024-12-02-14:37:26-root-INFO: grad norm: 4.502 4.467 0.561
2024-12-02-14:37:27-root-INFO: grad norm: 4.285 4.261 0.453
2024-12-02-14:37:28-root-INFO: grad norm: 4.195 4.165 0.504
2024-12-02-14:37:29-root-INFO: grad norm: 4.152 4.127 0.452
2024-12-02-14:37:30-root-INFO: grad norm: 4.135 4.103 0.510
2024-12-02-14:37:31-root-INFO: grad norm: 4.113 4.084 0.484
2024-12-02-14:37:32-root-INFO: grad norm: 4.088 4.052 0.536
2024-12-02-14:37:33-root-INFO: Loss Change: 30.571 -> 21.173
2024-12-02-14:37:33-root-INFO: Regularization Change: 0.000 -> 28.661
2024-12-02-14:37:33-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-14:37:33-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-14:37:33-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-14:37:34-root-INFO: grad norm: 4.691 4.634 0.731
2024-12-02-14:37:35-root-INFO: grad norm: 4.510 4.451 0.725
2024-12-02-14:37:36-root-INFO: grad norm: 4.260 4.214 0.627
2024-12-02-14:37:37-root-INFO: grad norm: 4.169 4.117 0.652
2024-12-02-14:37:38-root-INFO: grad norm: 4.272 4.228 0.611
2024-12-02-14:37:39-root-INFO: grad norm: 4.207 4.158 0.637
2024-12-02-14:37:40-root-INFO: grad norm: 4.038 3.997 0.571
2024-12-02-14:37:41-root-INFO: grad norm: 3.938 3.894 0.590
2024-12-02-14:37:42-root-INFO: Loss Change: 21.147 -> 16.757
2024-12-02-14:37:42-root-INFO: Regularization Change: 0.000 -> 13.319
2024-12-02-14:37:42-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-14:37:42-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-14:37:42-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-14:37:42-root-INFO: grad norm: 4.680 4.616 0.772
2024-12-02-14:37:43-root-INFO: grad norm: 4.323 4.263 0.719
2024-12-02-14:37:44-root-INFO: grad norm: 3.987 3.941 0.603
2024-12-02-14:37:45-root-INFO: grad norm: 3.668 3.623 0.569
2024-12-02-14:37:46-root-INFO: grad norm: 3.566 3.526 0.530
2024-12-02-14:37:47-root-INFO: grad norm: 3.487 3.445 0.538
2024-12-02-14:37:48-root-INFO: grad norm: 3.369 3.329 0.520
2024-12-02-14:37:49-root-INFO: grad norm: 3.294 3.254 0.515
2024-12-02-14:37:50-root-INFO: Loss Change: 17.009 -> 13.624
2024-12-02-14:37:50-root-INFO: Regularization Change: 0.000 -> 7.837
2024-12-02-14:37:50-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-14:37:50-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-14:37:50-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-14:37:51-root-INFO: grad norm: 4.000 3.934 0.724
2024-12-02-14:37:52-root-INFO: grad norm: 3.749 3.692 0.653
2024-12-02-14:37:53-root-INFO: grad norm: 3.447 3.402 0.553
2024-12-02-14:37:54-root-INFO: grad norm: 3.293 3.250 0.526
2024-12-02-14:37:55-root-INFO: grad norm: 3.278 3.241 0.489
2024-12-02-14:37:56-root-INFO: grad norm: 3.308 3.275 0.468
2024-12-02-14:37:57-root-INFO: grad norm: 3.073 3.040 0.451
2024-12-02-14:37:58-root-INFO: grad norm: 3.004 2.969 0.456
2024-12-02-14:37:58-root-INFO: Loss Change: 13.862 -> 11.615
2024-12-02-14:37:58-root-INFO: Regularization Change: 0.000 -> 5.338
2024-12-02-14:37:58-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-14:37:59-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-14:37:59-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-14:37:59-root-INFO: grad norm: 3.645 3.585 0.661
2024-12-02-14:38:00-root-INFO: grad norm: 3.364 3.315 0.574
2024-12-02-14:38:01-root-INFO: grad norm: 3.050 3.010 0.493
2024-12-02-14:38:02-root-INFO: grad norm: 2.900 2.861 0.473
2024-12-02-14:38:03-root-INFO: grad norm: 3.026 2.989 0.472
2024-12-02-14:38:04-root-INFO: Loss too large (10.568->10.587)! Learning rate decreased to 0.32856.
2024-12-02-14:38:05-root-INFO: grad norm: 1.964 1.939 0.309
2024-12-02-14:38:06-root-INFO: grad norm: 1.305 1.292 0.189
2024-12-02-14:38:07-root-INFO: grad norm: 1.010 0.999 0.145
2024-12-02-14:38:08-root-INFO: Loss Change: 11.707 -> 9.342
2024-12-02-14:38:08-root-INFO: Regularization Change: 0.000 -> 3.609
2024-12-02-14:38:08-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-14:38:08-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-14:38:08-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-14:38:08-root-INFO: grad norm: 1.851 1.813 0.375
2024-12-02-14:38:09-root-INFO: Loss too large (9.407->9.442)! Learning rate decreased to 0.33388.
2024-12-02-14:38:10-root-INFO: grad norm: 1.533 1.512 0.253
2024-12-02-14:38:11-root-INFO: grad norm: 1.376 1.360 0.210
2024-12-02-14:38:12-root-INFO: grad norm: 1.357 1.337 0.230
2024-12-02-14:38:13-root-INFO: grad norm: 1.866 1.850 0.245
2024-12-02-14:38:13-root-INFO: Loss too large (8.863->8.919)! Learning rate decreased to 0.26710.
2024-12-02-14:38:14-root-INFO: grad norm: 1.230 1.215 0.197
2024-12-02-14:38:15-root-INFO: grad norm: 0.826 0.819 0.110
2024-12-02-14:38:16-root-INFO: grad norm: 0.700 0.692 0.108
2024-12-02-14:38:17-root-INFO: Loss Change: 9.407 -> 8.430
2024-12-02-14:38:17-root-INFO: Regularization Change: 0.000 -> 1.823
2024-12-02-14:38:17-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-14:38:17-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-14:38:17-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-14:38:17-root-INFO: grad norm: 1.344 1.311 0.299
2024-12-02-14:38:18-root-INFO: grad norm: 1.475 1.445 0.295
2024-12-02-14:38:19-root-INFO: Loss too large (8.206->8.303)! Learning rate decreased to 0.33923.
2024-12-02-14:38:20-root-INFO: grad norm: 1.852 1.833 0.265
2024-12-02-14:38:20-root-INFO: Loss too large (8.097->8.162)! Learning rate decreased to 0.27138.
2024-12-02-14:38:21-root-INFO: grad norm: 1.243 1.227 0.197
2024-12-02-14:38:22-root-INFO: grad norm: 0.826 0.819 0.110
2024-12-02-14:38:23-root-INFO: grad norm: 0.691 0.683 0.103
2024-12-02-14:38:24-root-INFO: grad norm: 0.602 0.597 0.077
2024-12-02-14:38:25-root-INFO: grad norm: 0.556 0.551 0.075
2024-12-02-14:38:26-root-INFO: Loss Change: 8.310 -> 7.551
2024-12-02-14:38:26-root-INFO: Regularization Change: 0.000 -> 1.496
2024-12-02-14:38:26-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-14:38:26-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-14:38:26-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-14:38:27-root-INFO: grad norm: 1.410 1.377 0.303
2024-12-02-14:38:27-root-INFO: Loss too large (7.546->7.569)! Learning rate decreased to 0.34461.
2024-12-02-14:38:28-root-INFO: grad norm: 1.276 1.257 0.224
2024-12-02-14:38:29-root-INFO: grad norm: 1.388 1.371 0.219
2024-12-02-14:38:29-root-INFO: Loss too large (7.338->7.352)! Learning rate decreased to 0.27569.
2024-12-02-14:38:30-root-INFO: grad norm: 1.106 1.091 0.181
2024-12-02-14:38:31-root-INFO: grad norm: 0.853 0.845 0.118
2024-12-02-14:38:32-root-INFO: grad norm: 0.788 0.779 0.115
2024-12-02-14:38:33-root-INFO: grad norm: 0.821 0.815 0.095
2024-12-02-14:38:34-root-INFO: grad norm: 0.749 0.743 0.099
2024-12-02-14:38:35-root-INFO: Loss Change: 7.546 -> 6.893
2024-12-02-14:38:35-root-INFO: Regularization Change: 0.000 -> 1.239
2024-12-02-14:38:35-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-14:38:35-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-14:38:36-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-14:38:36-root-INFO: grad norm: 1.417 1.385 0.302
2024-12-02-14:38:36-root-INFO: Loss too large (6.870->6.906)! Learning rate decreased to 0.35002.
2024-12-02-14:38:37-root-INFO: grad norm: 1.289 1.269 0.226
2024-12-02-14:38:38-root-INFO: grad norm: 1.504 1.487 0.225
2024-12-02-14:38:39-root-INFO: Loss too large (6.683->6.739)! Learning rate decreased to 0.28002.
2024-12-02-14:38:40-root-INFO: grad norm: 1.143 1.128 0.187
2024-12-02-14:38:41-root-INFO: grad norm: 0.801 0.794 0.110
2024-12-02-14:38:42-root-INFO: grad norm: 0.647 0.639 0.101
2024-12-02-14:38:43-root-INFO: grad norm: 0.572 0.567 0.075
2024-12-02-14:38:44-root-INFO: grad norm: 0.566 0.561 0.078
2024-12-02-14:38:44-root-INFO: Loss Change: 6.870 -> 6.261
2024-12-02-14:38:44-root-INFO: Regularization Change: 0.000 -> 1.141
2024-12-02-14:38:44-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-14:38:44-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-14:38:45-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-14:38:45-root-INFO: grad norm: 1.716 1.681 0.343
2024-12-02-14:38:45-root-INFO: Loss too large (6.306->6.432)! Learning rate decreased to 0.35546.
2024-12-02-14:38:46-root-INFO: grad norm: 1.364 1.343 0.237
2024-12-02-14:38:47-root-INFO: grad norm: 1.346 1.330 0.209
2024-12-02-14:38:48-root-INFO: grad norm: 1.345 1.324 0.236
2024-12-02-14:38:49-root-INFO: grad norm: 1.457 1.439 0.227
2024-12-02-14:38:50-root-INFO: Loss too large (5.986->6.028)! Learning rate decreased to 0.28437.
2024-12-02-14:38:51-root-INFO: grad norm: 1.133 1.117 0.191
2024-12-02-14:38:52-root-INFO: grad norm: 0.931 0.924 0.117
2024-12-02-14:38:53-root-INFO: grad norm: 0.786 0.778 0.113
2024-12-02-14:38:53-root-INFO: Loss Change: 6.306 -> 5.687
2024-12-02-14:38:53-root-INFO: Regularization Change: 0.000 -> 1.172
2024-12-02-14:38:53-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-14:38:53-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-14:38:54-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-14:38:54-root-INFO: grad norm: 1.431 1.398 0.306
2024-12-02-14:38:54-root-INFO: Loss too large (5.709->5.760)! Learning rate decreased to 0.36092.
2024-12-02-14:38:55-root-INFO: grad norm: 1.283 1.263 0.229
2024-12-02-14:38:56-root-INFO: grad norm: 1.649 1.634 0.222
2024-12-02-14:38:57-root-INFO: Loss too large (5.543->5.609)! Learning rate decreased to 0.28874.
2024-12-02-14:38:58-root-INFO: grad norm: 1.034 1.021 0.166
2024-12-02-14:38:59-root-INFO: grad norm: 0.703 0.698 0.086
2024-12-02-14:39:00-root-INFO: grad norm: 0.585 0.579 0.081
2024-12-02-14:39:01-root-INFO: grad norm: 0.496 0.492 0.061
2024-12-02-14:39:02-root-INFO: grad norm: 0.465 0.461 0.060
2024-12-02-14:39:02-root-INFO: Loss Change: 5.709 -> 5.158
2024-12-02-14:39:02-root-INFO: Regularization Change: 0.000 -> 0.985
2024-12-02-14:39:02-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-14:39:02-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-14:39:03-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-14:39:03-root-INFO: grad norm: 1.521 1.485 0.328
2024-12-02-14:39:03-root-INFO: Loss too large (5.241->5.305)! Learning rate decreased to 0.36641.
2024-12-02-14:39:04-root-INFO: grad norm: 1.225 1.205 0.218
2024-12-02-14:39:05-root-INFO: grad norm: 1.145 1.131 0.178
2024-12-02-14:39:06-root-INFO: grad norm: 1.083 1.067 0.187
2024-12-02-14:39:07-root-INFO: grad norm: 1.155 1.142 0.174
2024-12-02-14:39:08-root-INFO: Loss too large (4.907->4.930)! Learning rate decreased to 0.29313.
2024-12-02-14:39:09-root-INFO: grad norm: 0.937 0.925 0.146
2024-12-02-14:39:10-root-INFO: grad norm: 0.921 0.916 0.099
2024-12-02-14:39:11-root-INFO: grad norm: 0.700 0.694 0.092
2024-12-02-14:39:12-root-INFO: Loss Change: 5.241 -> 4.689
2024-12-02-14:39:12-root-INFO: Regularization Change: 0.000 -> 1.041
2024-12-02-14:39:12-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-14:39:12-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-14:39:12-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-14:39:12-root-INFO: grad norm: 1.344 1.312 0.293
2024-12-02-14:39:13-root-INFO: grad norm: 1.506 1.478 0.292
2024-12-02-14:39:14-root-INFO: Loss too large (4.732->4.955)! Learning rate decreased to 0.37193.
2024-12-02-14:39:15-root-INFO: grad norm: 2.268 2.255 0.246
2024-12-02-14:39:15-root-INFO: Loss too large (4.692->4.736)! Learning rate decreased to 0.29754.
2024-12-02-14:39:16-root-INFO: grad norm: 1.004 0.992 0.151
2024-12-02-14:39:17-root-INFO: grad norm: 0.602 0.599 0.068
2024-12-02-14:39:18-root-INFO: grad norm: 0.486 0.482 0.063
2024-12-02-14:39:19-root-INFO: grad norm: 0.432 0.430 0.042
2024-12-02-14:39:20-root-INFO: grad norm: 0.402 0.400 0.042
2024-12-02-14:39:21-root-INFO: Loss Change: 4.761 -> 4.281
2024-12-02-14:39:21-root-INFO: Regularization Change: 0.000 -> 0.970
2024-12-02-14:39:21-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-14:39:21-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-14:39:21-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-14:39:21-root-INFO: grad norm: 1.227 1.198 0.269
2024-12-02-14:39:22-root-INFO: grad norm: 1.287 1.261 0.254
2024-12-02-14:39:23-root-INFO: Loss too large (4.290->4.337)! Learning rate decreased to 0.37747.
2024-12-02-14:39:24-root-INFO: grad norm: 1.248 1.234 0.186
2024-12-02-14:39:25-root-INFO: grad norm: 1.123 1.108 0.187
2024-12-02-14:39:26-root-INFO: grad norm: 0.963 0.954 0.136
2024-12-02-14:39:27-root-INFO: grad norm: 0.896 0.884 0.147
2024-12-02-14:39:28-root-INFO: grad norm: 1.128 1.121 0.132
2024-12-02-14:39:28-root-INFO: Loss too large (3.973->4.001)! Learning rate decreased to 0.30197.
2024-12-02-14:39:29-root-INFO: grad norm: 0.735 0.727 0.107
2024-12-02-14:39:30-root-INFO: Loss Change: 4.365 -> 3.855
2024-12-02-14:39:30-root-INFO: Regularization Change: 0.000 -> 1.081
2024-12-02-14:39:30-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-14:39:30-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-14:39:30-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-14:39:31-root-INFO: grad norm: 1.428 1.394 0.312
2024-12-02-14:39:32-root-INFO: grad norm: 1.434 1.406 0.278
2024-12-02-14:39:33-root-INFO: grad norm: 1.774 1.747 0.305
2024-12-02-14:39:33-root-INFO: Loss too large (3.906->4.108)! Learning rate decreased to 0.38303.
2024-12-02-14:39:34-root-INFO: grad norm: 1.520 1.495 0.272
2024-12-02-14:39:35-root-INFO: grad norm: 2.057 2.046 0.215
2024-12-02-14:39:36-root-INFO: grad norm: 1.129 1.114 0.183
2024-12-02-14:39:37-root-INFO: grad norm: 0.879 0.870 0.126
2024-12-02-14:39:38-root-INFO: grad norm: 0.776 0.765 0.133
2024-12-02-14:39:39-root-INFO: Loss Change: 3.988 -> 3.508
2024-12-02-14:39:39-root-INFO: Regularization Change: 0.000 -> 1.320
2024-12-02-14:39:39-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-14:39:39-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-14:39:39-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-14:39:39-root-INFO: grad norm: 1.612 1.574 0.347
2024-12-02-14:39:40-root-INFO: grad norm: 1.645 1.614 0.319
2024-12-02-14:39:41-root-INFO: grad norm: 1.860 1.833 0.317
2024-12-02-14:39:42-root-INFO: Loss too large (3.615->3.749)! Learning rate decreased to 0.38861.
2024-12-02-14:39:43-root-INFO: grad norm: 1.406 1.384 0.252
2024-12-02-14:39:44-root-INFO: grad norm: 1.032 1.020 0.158
2024-12-02-14:39:45-root-INFO: grad norm: 0.902 0.890 0.148
2024-12-02-14:39:46-root-INFO: grad norm: 1.007 1.000 0.119
2024-12-02-14:39:46-root-INFO: Loss too large (3.182->3.192)! Learning rate decreased to 0.31088.
2024-12-02-14:39:47-root-INFO: grad norm: 0.601 0.595 0.083
2024-12-02-14:39:48-root-INFO: Loss Change: 3.692 -> 3.078
2024-12-02-14:39:48-root-INFO: Regularization Change: 0.000 -> 1.082
2024-12-02-14:39:48-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-14:39:48-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-14:39:48-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-14:39:49-root-INFO: grad norm: 1.289 1.256 0.288
2024-12-02-14:39:50-root-INFO: grad norm: 1.212 1.188 0.238
2024-12-02-14:39:50-root-INFO: Loss too large (3.149->3.159)! Learning rate decreased to 0.39420.
2024-12-02-14:39:51-root-INFO: grad norm: 1.141 1.131 0.151
2024-12-02-14:39:51-root-INFO: Loss too large (3.034->3.040)! Learning rate decreased to 0.31536.
2024-12-02-14:39:52-root-INFO: grad norm: 0.637 0.631 0.088
2024-12-02-14:39:53-root-INFO: grad norm: 0.427 0.426 0.034
2024-12-02-14:39:54-root-INFO: grad norm: 0.422 0.421 0.028
2024-12-02-14:39:55-root-INFO: grad norm: 0.496 0.494 0.043
2024-12-02-14:39:56-root-INFO: grad norm: 0.710 0.709 0.043
2024-12-02-14:39:57-root-INFO: Loss too large (2.846->2.854)! Learning rate decreased to 0.25229.
2024-12-02-14:39:57-root-INFO: Loss Change: 3.238 -> 2.837
2024-12-02-14:39:57-root-INFO: Regularization Change: 0.000 -> 0.732
2024-12-02-14:39:57-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-14:39:57-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-14:39:58-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-14:39:58-root-INFO: grad norm: 1.112 1.092 0.215
2024-12-02-14:39:59-root-INFO: grad norm: 1.307 1.300 0.142
2024-12-02-14:39:59-root-INFO: Loss too large (2.878->2.891)! Learning rate decreased to 0.39982.
2024-12-02-14:40:00-root-INFO: grad norm: 0.685 0.681 0.081
2024-12-02-14:40:01-root-INFO: grad norm: 0.552 0.548 0.068
2024-12-02-14:40:02-root-INFO: grad norm: 0.430 0.427 0.047
2024-12-02-14:40:03-root-INFO: grad norm: 0.343 0.341 0.038
2024-12-02-14:40:04-root-INFO: grad norm: 0.319 0.318 0.028
2024-12-02-14:40:05-root-INFO: grad norm: 0.406 0.405 0.027
2024-12-02-14:40:06-root-INFO: Loss Change: 2.949 -> 2.569
2024-12-02-14:40:06-root-INFO: Regularization Change: 0.000 -> 0.876
2024-12-02-14:40:06-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-14:40:06-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-14:40:06-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-14:40:07-root-INFO: grad norm: 1.214 1.189 0.244
2024-12-02-14:40:08-root-INFO: grad norm: 1.368 1.359 0.156
2024-12-02-14:40:08-root-INFO: Loss too large (2.669->2.671)! Learning rate decreased to 0.40545.
2024-12-02-14:40:09-root-INFO: grad norm: 0.720 0.714 0.087
2024-12-02-14:40:10-root-INFO: grad norm: 0.577 0.573 0.067
2024-12-02-14:40:11-root-INFO: grad norm: 0.444 0.440 0.054
2024-12-02-14:40:12-root-INFO: grad norm: 0.412 0.409 0.050
2024-12-02-14:40:13-root-INFO: grad norm: 0.630 0.627 0.054
2024-12-02-14:40:13-root-INFO: Loss too large (2.381->2.401)! Learning rate decreased to 0.32436.
2024-12-02-14:40:14-root-INFO: grad norm: 0.579 0.575 0.064
2024-12-02-14:40:15-root-INFO: Loss Change: 2.729 -> 2.339
2024-12-02-14:40:15-root-INFO: Regularization Change: 0.000 -> 0.786
2024-12-02-14:40:15-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-14:40:15-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-14:40:15-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-14:40:16-root-INFO: grad norm: 1.378 1.345 0.297
2024-12-02-14:40:17-root-INFO: grad norm: 1.135 1.116 0.203
2024-12-02-14:40:18-root-INFO: grad norm: 1.064 1.051 0.167
2024-12-02-14:40:19-root-INFO: grad norm: 1.076 1.065 0.152
2024-12-02-14:40:20-root-INFO: grad norm: 1.077 1.068 0.141
2024-12-02-14:40:21-root-INFO: grad norm: 1.118 1.111 0.130
2024-12-02-14:40:21-root-INFO: Loss too large (2.325->2.327)! Learning rate decreased to 0.41109.
2024-12-02-14:40:22-root-INFO: grad norm: 0.752 0.747 0.082
2024-12-02-14:40:23-root-INFO: grad norm: 0.572 0.570 0.047
2024-12-02-14:40:24-root-INFO: Loss Change: 2.572 -> 2.153
2024-12-02-14:40:24-root-INFO: Regularization Change: 0.000 -> 0.996
2024-12-02-14:40:24-root-INFO: Undo step: 10
2024-12-02-14:40:24-root-INFO: Undo step: 11
2024-12-02-14:40:24-root-INFO: Undo step: 12
2024-12-02-14:40:24-root-INFO: Undo step: 13
2024-12-02-14:40:24-root-INFO: Undo step: 14
2024-12-02-14:40:24-root-INFO: Undo step: 15
2024-12-02-14:40:24-root-INFO: Undo step: 16
2024-12-02-14:40:24-root-INFO: Undo step: 17
2024-12-02-14:40:24-root-INFO: Undo step: 18
2024-12-02-14:40:24-root-INFO: Undo step: 19
2024-12-02-14:40:24-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-14:40:25-root-INFO: grad norm: 16.848 16.806 1.195
2024-12-02-14:40:26-root-INFO: grad norm: 8.011 7.975 0.755
2024-12-02-14:40:26-root-INFO: grad norm: 5.935 5.911 0.535
2024-12-02-14:40:27-root-INFO: grad norm: 4.805 4.785 0.440
2024-12-02-14:40:28-root-INFO: grad norm: 4.280 4.265 0.354
2024-12-02-14:40:29-root-INFO: grad norm: 3.853 3.838 0.346
2024-12-02-14:40:31-root-INFO: grad norm: 3.374 3.362 0.293
2024-12-02-14:40:32-root-INFO: grad norm: 3.124 3.111 0.281
2024-12-02-14:40:32-root-INFO: Loss Change: 133.442 -> 18.698
2024-12-02-14:40:32-root-INFO: Regularization Change: 0.000 -> 225.145
2024-12-02-14:40:32-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-14:40:32-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-14:40:33-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-14:40:33-root-INFO: grad norm: 3.238 3.224 0.297
2024-12-02-14:40:34-root-INFO: grad norm: 3.004 2.990 0.293
2024-12-02-14:40:35-root-INFO: grad norm: 2.792 2.780 0.259
2024-12-02-14:40:36-root-INFO: grad norm: 2.716 2.703 0.261
2024-12-02-14:40:37-root-INFO: grad norm: 2.739 2.724 0.287
2024-12-02-14:40:38-root-INFO: grad norm: 2.790 2.776 0.279
2024-12-02-14:40:39-root-INFO: grad norm: 2.708 2.688 0.333
2024-12-02-14:40:40-root-INFO: grad norm: 2.642 2.629 0.264
2024-12-02-14:40:41-root-INFO: Loss Change: 18.216 -> 11.235
2024-12-02-14:40:41-root-INFO: Regularization Change: 0.000 -> 24.588
2024-12-02-14:40:41-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-14:40:41-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-14:40:41-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-14:40:42-root-INFO: grad norm: 2.866 2.843 0.367
2024-12-02-14:40:43-root-INFO: grad norm: 2.886 2.869 0.315
2024-12-02-14:40:44-root-INFO: grad norm: 2.572 2.550 0.338
2024-12-02-14:40:45-root-INFO: grad norm: 2.388 2.377 0.239
2024-12-02-14:40:46-root-INFO: grad norm: 2.357 2.338 0.296
2024-12-02-14:40:47-root-INFO: grad norm: 2.532 2.514 0.305
2024-12-02-14:40:48-root-INFO: grad norm: 2.270 2.248 0.314
2024-12-02-14:40:49-root-INFO: grad norm: 2.426 2.419 0.184
2024-12-02-14:40:49-root-INFO: Loss Change: 10.948 -> 7.899
2024-12-02-14:40:49-root-INFO: Regularization Change: 0.000 -> 10.403
2024-12-02-14:40:49-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-14:40:49-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-14:40:50-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-14:40:50-root-INFO: grad norm: 2.311 2.292 0.295
2024-12-02-14:40:51-root-INFO: grad norm: 2.105 2.091 0.246
2024-12-02-14:40:52-root-INFO: grad norm: 1.983 1.967 0.251
2024-12-02-14:40:53-root-INFO: grad norm: 2.035 2.019 0.249
2024-12-02-14:40:54-root-INFO: grad norm: 1.996 1.977 0.276
2024-12-02-14:40:55-root-INFO: grad norm: 1.901 1.889 0.210
2024-12-02-14:40:56-root-INFO: grad norm: 1.863 1.847 0.247
2024-12-02-14:40:57-root-INFO: grad norm: 1.950 1.936 0.233
2024-12-02-14:40:58-root-INFO: Loss Change: 7.701 -> 6.025
2024-12-02-14:40:58-root-INFO: Regularization Change: 0.000 -> 5.678
2024-12-02-14:40:58-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-14:40:58-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-14:40:58-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-14:40:58-root-INFO: grad norm: 2.191 2.164 0.341
2024-12-02-14:40:59-root-INFO: grad norm: 1.972 1.955 0.260
2024-12-02-14:41:00-root-INFO: grad norm: 1.851 1.834 0.249
2024-12-02-14:41:01-root-INFO: grad norm: 1.948 1.936 0.219
2024-12-02-14:41:02-root-INFO: grad norm: 1.789 1.773 0.240
2024-12-02-14:41:03-root-INFO: grad norm: 1.688 1.677 0.184
2024-12-02-14:41:04-root-INFO: grad norm: 1.631 1.617 0.213
2024-12-02-14:41:05-root-INFO: grad norm: 1.720 1.709 0.191
2024-12-02-14:41:06-root-INFO: Loss Change: 5.917 -> 4.798
2024-12-02-14:41:06-root-INFO: Regularization Change: 0.000 -> 3.459
2024-12-02-14:41:06-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-14:41:06-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-14:41:06-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-14:41:07-root-INFO: grad norm: 2.001 1.973 0.333
2024-12-02-14:41:08-root-INFO: grad norm: 1.756 1.736 0.263
2024-12-02-14:41:09-root-INFO: grad norm: 1.608 1.592 0.225
2024-12-02-14:41:10-root-INFO: grad norm: 1.510 1.495 0.214
2024-12-02-14:41:11-root-INFO: grad norm: 1.439 1.425 0.203
2024-12-02-14:41:12-root-INFO: grad norm: 1.411 1.396 0.203
2024-12-02-14:41:13-root-INFO: grad norm: 1.485 1.471 0.207
2024-12-02-14:41:13-root-INFO: Loss too large (3.964->3.990)! Learning rate decreased to 0.38303.
2024-12-02-14:41:14-root-INFO: grad norm: 0.979 0.969 0.138
2024-12-02-14:41:15-root-INFO: Loss Change: 4.753 -> 3.702
2024-12-02-14:41:15-root-INFO: Regularization Change: 0.000 -> 2.250
2024-12-02-14:41:15-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-14:41:15-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-14:41:15-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-14:41:15-root-INFO: grad norm: 1.453 1.419 0.314
2024-12-02-14:41:16-root-INFO: grad norm: 1.333 1.308 0.254
2024-12-02-14:41:17-root-INFO: grad norm: 1.385 1.367 0.221
2024-12-02-14:41:18-root-INFO: Loss too large (3.580->3.607)! Learning rate decreased to 0.38861.
2024-12-02-14:41:19-root-INFO: grad norm: 1.175 1.167 0.140
2024-12-02-14:41:20-root-INFO: grad norm: 0.737 0.731 0.088
2024-12-02-14:41:21-root-INFO: grad norm: 0.612 0.607 0.074
2024-12-02-14:41:22-root-INFO: grad norm: 0.594 0.591 0.061
2024-12-02-14:41:23-root-INFO: grad norm: 0.639 0.634 0.076
2024-12-02-14:41:23-root-INFO: Loss Change: 3.769 -> 3.179
2024-12-02-14:41:23-root-INFO: Regularization Change: 0.000 -> 1.358
2024-12-02-14:41:23-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-14:41:23-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-14:41:24-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-14:41:24-root-INFO: grad norm: 1.768 1.737 0.330
2024-12-02-14:41:24-root-INFO: Loss too large (3.319->3.352)! Learning rate decreased to 0.39420.
2024-12-02-14:41:25-root-INFO: grad norm: 1.007 0.993 0.169
2024-12-02-14:41:26-root-INFO: grad norm: 0.707 0.700 0.094
2024-12-02-14:41:27-root-INFO: grad norm: 0.602 0.596 0.081
2024-12-02-14:41:29-root-INFO: grad norm: 0.510 0.507 0.055
2024-12-02-14:41:30-root-INFO: grad norm: 0.415 0.412 0.051
2024-12-02-14:41:31-root-INFO: grad norm: 0.411 0.409 0.039
2024-12-02-14:41:32-root-INFO: grad norm: 0.528 0.524 0.058
2024-12-02-14:41:32-root-INFO: Loss too large (2.810->2.823)! Learning rate decreased to 0.31536.
2024-12-02-14:41:33-root-INFO: Loss Change: 3.319 -> 2.796
2024-12-02-14:41:33-root-INFO: Regularization Change: 0.000 -> 0.950
2024-12-02-14:41:33-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-14:41:33-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-14:41:33-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-14:41:33-root-INFO: grad norm: 1.501 1.476 0.273
2024-12-02-14:41:34-root-INFO: Loss too large (2.916->2.926)! Learning rate decreased to 0.39982.
2024-12-02-14:41:35-root-INFO: grad norm: 0.835 0.824 0.136
2024-12-02-14:41:36-root-INFO: grad norm: 0.616 0.611 0.078
2024-12-02-14:41:37-root-INFO: grad norm: 0.510 0.506 0.063
2024-12-02-14:41:38-root-INFO: grad norm: 0.511 0.508 0.055
2024-12-02-14:41:39-root-INFO: grad norm: 0.598 0.594 0.072
2024-12-02-14:41:40-root-INFO: grad norm: 0.976 0.973 0.071
2024-12-02-14:41:40-root-INFO: Loss too large (2.574->2.600)! Learning rate decreased to 0.31985.
2024-12-02-14:41:41-root-INFO: grad norm: 0.537 0.533 0.069
2024-12-02-14:41:42-root-INFO: Loss Change: 2.916 -> 2.499
2024-12-02-14:41:42-root-INFO: Regularization Change: 0.000 -> 0.797
2024-12-02-14:41:42-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-14:41:42-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-14:41:42-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-14:41:43-root-INFO: grad norm: 1.183 1.153 0.265
2024-12-02-14:41:44-root-INFO: grad norm: 0.985 0.967 0.187
2024-12-02-14:41:45-root-INFO: grad norm: 0.985 0.970 0.167
2024-12-02-14:41:46-root-INFO: grad norm: 1.028 1.013 0.173
2024-12-02-14:41:47-root-INFO: grad norm: 1.187 1.173 0.184
2024-12-02-14:41:47-root-INFO: Loss too large (2.447->2.538)! Learning rate decreased to 0.40545.
2024-12-02-14:41:48-root-INFO: grad norm: 0.952 0.940 0.149
2024-12-02-14:41:49-root-INFO: grad norm: 0.793 0.786 0.104
2024-12-02-14:41:49-root-INFO: Loss too large (2.317->2.328)! Learning rate decreased to 0.32436.
2024-12-02-14:41:50-root-INFO: grad norm: 0.626 0.620 0.082
2024-12-02-14:41:51-root-INFO: Loss Change: 2.642 -> 2.253
2024-12-02-14:41:51-root-INFO: Regularization Change: 0.000 -> 0.858
2024-12-02-14:41:51-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-14:41:51-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-14:41:51-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-14:41:52-root-INFO: grad norm: 1.427 1.394 0.304
2024-12-02-14:41:53-root-INFO: grad norm: 1.118 1.099 0.205
2024-12-02-14:41:54-root-INFO: grad norm: 1.042 1.030 0.161
2024-12-02-14:41:55-root-INFO: grad norm: 0.961 0.950 0.146
2024-12-02-14:41:56-root-INFO: grad norm: 0.978 0.969 0.132
2024-12-02-14:41:56-root-INFO: Loss too large (2.216->2.233)! Learning rate decreased to 0.41109.
2024-12-02-14:41:57-root-INFO: grad norm: 0.816 0.812 0.077
2024-12-02-14:41:58-root-INFO: grad norm: 0.622 0.617 0.078
2024-12-02-14:41:59-root-INFO: grad norm: 0.460 0.458 0.046
2024-12-02-14:42:00-root-INFO: Loss Change: 2.470 -> 2.059
2024-12-02-14:42:00-root-INFO: Regularization Change: 0.000 -> 0.870
2024-12-02-14:42:00-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-14:42:00-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-14:42:00-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-14:42:00-root-INFO: grad norm: 1.062 1.040 0.214
2024-12-02-14:42:01-root-INFO: grad norm: 0.998 0.991 0.120
2024-12-02-14:42:02-root-INFO: Loss too large (2.118->2.119)! Learning rate decreased to 0.41675.
2024-12-02-14:42:03-root-INFO: grad norm: 0.665 0.659 0.088
2024-12-02-14:42:04-root-INFO: grad norm: 0.467 0.464 0.049
2024-12-02-14:42:05-root-INFO: grad norm: 0.500 0.496 0.060
2024-12-02-14:42:05-root-INFO: Loss too large (1.964->1.971)! Learning rate decreased to 0.33340.
2024-12-02-14:42:06-root-INFO: grad norm: 0.536 0.534 0.045
2024-12-02-14:42:07-root-INFO: grad norm: 0.517 0.513 0.067
2024-12-02-14:42:08-root-INFO: grad norm: 0.482 0.480 0.048
2024-12-02-14:42:09-root-INFO: Loss Change: 2.209 -> 1.915
2024-12-02-14:42:09-root-INFO: Regularization Change: 0.000 -> 0.566
2024-12-02-14:42:09-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-14:42:09-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-14:42:09-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-14:42:09-root-INFO: grad norm: 0.992 0.971 0.204
2024-12-02-14:42:10-root-INFO: grad norm: 0.779 0.772 0.103
2024-12-02-14:42:12-root-INFO: grad norm: 0.735 0.728 0.100
2024-12-02-14:42:13-root-INFO: grad norm: 0.789 0.786 0.071
2024-12-02-14:42:13-root-INFO: Loss too large (1.909->1.923)! Learning rate decreased to 0.42241.
2024-12-02-14:42:14-root-INFO: grad norm: 0.647 0.640 0.094
2024-12-02-14:42:15-root-INFO: grad norm: 0.580 0.577 0.059
2024-12-02-14:42:16-root-INFO: grad norm: 0.575 0.569 0.086
2024-12-02-14:42:17-root-INFO: grad norm: 0.572 0.568 0.063
2024-12-02-14:42:18-root-INFO: Loss Change: 2.083 -> 1.804
2024-12-02-14:42:18-root-INFO: Regularization Change: 0.000 -> 0.710
2024-12-02-14:42:18-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-14:42:18-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-14:42:18-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-14:42:18-root-INFO: grad norm: 0.998 0.980 0.187
2024-12-02-14:42:19-root-INFO: grad norm: 0.740 0.736 0.081
2024-12-02-14:42:20-root-INFO: grad norm: 0.674 0.669 0.086
2024-12-02-14:42:21-root-INFO: grad norm: 0.684 0.681 0.063
2024-12-02-14:42:22-root-INFO: grad norm: 0.686 0.679 0.101
2024-12-02-14:42:23-root-INFO: grad norm: 0.689 0.685 0.070
2024-12-02-14:42:24-root-INFO: grad norm: 0.679 0.671 0.108
2024-12-02-14:42:25-root-INFO: grad norm: 0.668 0.663 0.081
2024-12-02-14:42:26-root-INFO: Loss Change: 1.968 -> 1.707
2024-12-02-14:42:26-root-INFO: Regularization Change: 0.000 -> 0.824
2024-12-02-14:42:26-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-14:42:26-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-14:42:26-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-14:42:27-root-INFO: grad norm: 1.001 0.988 0.157
2024-12-02-14:42:28-root-INFO: grad norm: 0.681 0.678 0.067
2024-12-02-14:42:29-root-INFO: grad norm: 0.576 0.572 0.065
2024-12-02-14:42:30-root-INFO: grad norm: 0.536 0.533 0.051
2024-12-02-14:42:31-root-INFO: grad norm: 0.524 0.519 0.067
2024-12-02-14:42:32-root-INFO: grad norm: 0.521 0.518 0.058
2024-12-02-14:42:33-root-INFO: grad norm: 0.524 0.519 0.075
2024-12-02-14:42:34-root-INFO: grad norm: 0.534 0.530 0.067
2024-12-02-14:42:34-root-INFO: Loss Change: 1.853 -> 1.570
2024-12-02-14:42:34-root-INFO: Regularization Change: 0.000 -> 0.731
2024-12-02-14:42:34-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-14:42:34-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-14:42:35-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-14:42:35-root-INFO: grad norm: 0.885 0.873 0.141
2024-12-02-14:42:36-root-INFO: grad norm: 0.485 0.482 0.052
2024-12-02-14:42:37-root-INFO: grad norm: 0.366 0.364 0.040
2024-12-02-14:42:38-root-INFO: grad norm: 0.333 0.331 0.035
2024-12-02-14:42:39-root-INFO: grad norm: 0.324 0.321 0.039
2024-12-02-14:42:40-root-INFO: grad norm: 0.330 0.328 0.038
2024-12-02-14:42:41-root-INFO: grad norm: 0.345 0.342 0.046
2024-12-02-14:42:42-root-INFO: grad norm: 0.377 0.374 0.047
2024-12-02-14:42:43-root-INFO: Loss Change: 1.727 -> 1.462
2024-12-02-14:42:43-root-INFO: Regularization Change: 0.000 -> 0.683
2024-12-02-14:42:43-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-14:42:43-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-14:42:43-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-14:42:44-root-INFO: grad norm: 0.770 0.761 0.115
2024-12-02-14:42:45-root-INFO: grad norm: 0.358 0.355 0.045
2024-12-02-14:42:46-root-INFO: grad norm: 0.261 0.259 0.032
2024-12-02-14:42:47-root-INFO: grad norm: 0.236 0.234 0.031
2024-12-02-14:42:48-root-INFO: grad norm: 0.232 0.230 0.033
2024-12-02-14:42:49-root-INFO: grad norm: 0.252 0.250 0.033
2024-12-02-14:42:50-root-INFO: grad norm: 0.270 0.268 0.037
2024-12-02-14:42:51-root-INFO: grad norm: 0.304 0.302 0.038
2024-12-02-14:42:51-root-INFO: Loss Change: 1.617 -> 1.379
2024-12-02-14:42:51-root-INFO: Regularization Change: 0.000 -> 0.651
2024-12-02-14:42:51-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-14:42:51-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-14:42:52-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-14:42:52-root-INFO: grad norm: 0.737 0.731 0.098
2024-12-02-14:42:53-root-INFO: grad norm: 0.312 0.309 0.045
2024-12-02-14:42:54-root-INFO: grad norm: 0.232 0.230 0.031
2024-12-02-14:42:55-root-INFO: grad norm: 0.214 0.211 0.032
2024-12-02-14:42:56-root-INFO: grad norm: 0.206 0.203 0.031
2024-12-02-14:42:57-root-INFO: grad norm: 0.221 0.219 0.030
2024-12-02-14:42:58-root-INFO: grad norm: 0.234 0.232 0.032
2024-12-02-14:42:59-root-INFO: grad norm: 0.254 0.253 0.030
2024-12-02-14:43:00-root-INFO: Loss Change: 1.546 -> 1.310
2024-12-02-14:43:00-root-INFO: Regularization Change: 0.000 -> 0.639
2024-12-02-14:43:00-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-14:43:00-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-14:43:00-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-14:43:00-root-INFO: grad norm: 0.688 0.684 0.079
2024-12-02-14:43:01-root-INFO: grad norm: 0.378 0.376 0.042
2024-12-02-14:43:02-root-INFO: grad norm: 0.415 0.414 0.030
2024-12-02-14:43:03-root-INFO: grad norm: 0.538 0.537 0.030
2024-12-02-14:43:04-root-INFO: Loss too large (1.304->1.322)! Learning rate decreased to 0.45653.
2024-12-02-14:43:04-root-INFO: Loss too large (1.304->1.315)! Learning rate decreased to 0.36522.
2024-12-02-14:43:05-root-INFO: grad norm: 0.167 0.165 0.029
2024-12-02-14:43:06-root-INFO: grad norm: 0.156 0.154 0.028
2024-12-02-14:43:07-root-INFO: grad norm: 0.161 0.158 0.027
2024-12-02-14:43:08-root-INFO: grad norm: 0.154 0.152 0.026
2024-12-02-14:43:09-root-INFO: Loss Change: 1.475 -> 1.259
2024-12-02-14:43:09-root-INFO: Regularization Change: 0.000 -> 0.495
2024-12-02-14:43:09-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-14:43:09-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-14:43:09-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-14:43:10-root-INFO: grad norm: 0.734 0.730 0.075
2024-12-02-14:43:11-root-INFO: grad norm: 0.365 0.363 0.035
2024-12-02-14:43:12-root-INFO: grad norm: 0.338 0.337 0.028
2024-12-02-14:43:13-root-INFO: grad norm: 0.250 0.248 0.028
2024-12-02-14:43:14-root-INFO: grad norm: 0.241 0.240 0.025
2024-12-02-14:43:15-root-INFO: grad norm: 0.212 0.211 0.024
2024-12-02-14:43:16-root-INFO: grad norm: 0.201 0.200 0.022
2024-12-02-14:43:17-root-INFO: grad norm: 0.186 0.184 0.021
2024-12-02-14:43:17-root-INFO: Loss Change: 1.417 -> 1.149
2024-12-02-14:43:17-root-INFO: Regularization Change: 0.000 -> 0.827
2024-12-02-14:43:17-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-14:43:17-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-14:43:18-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-14:43:18-root-INFO: grad norm: 0.751 0.747 0.076
2024-12-02-14:43:19-root-INFO: grad norm: 0.418 0.417 0.025
2024-12-02-14:43:20-root-INFO: grad norm: 0.344 0.341 0.045
2024-12-02-14:43:21-root-INFO: grad norm: 0.318 0.313 0.053
2024-12-02-14:43:22-root-INFO: grad norm: 0.305 0.300 0.053
2024-12-02-14:43:23-root-INFO: grad norm: 0.298 0.293 0.052
2024-12-02-14:43:24-root-INFO: grad norm: 0.293 0.289 0.050
2024-12-02-14:43:25-root-INFO: grad norm: 0.288 0.284 0.048
2024-12-02-14:43:26-root-INFO: Loss Change: 1.293 -> 0.695
2024-12-02-14:43:26-root-INFO: Regularization Change: 0.000 -> 2.282
2024-12-02-14:43:26-root-INFO: Undo step: 0
2024-12-02-14:43:26-root-INFO: Undo step: 1
2024-12-02-14:43:26-root-INFO: Undo step: 2
2024-12-02-14:43:26-root-INFO: Undo step: 3
2024-12-02-14:43:26-root-INFO: Undo step: 4
2024-12-02-14:43:26-root-INFO: Undo step: 5
2024-12-02-14:43:26-root-INFO: Undo step: 6
2024-12-02-14:43:26-root-INFO: Undo step: 7
2024-12-02-14:43:26-root-INFO: Undo step: 8
2024-12-02-14:43:26-root-INFO: Undo step: 9
2024-12-02-14:43:26-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-14:43:27-root-INFO: grad norm: 16.012 15.988 0.879
2024-12-02-14:43:28-root-INFO: grad norm: 6.751 6.735 0.455
2024-12-02-14:43:29-root-INFO: grad norm: 4.602 4.593 0.286
2024-12-02-14:43:30-root-INFO: grad norm: 2.776 2.765 0.239
2024-12-02-14:43:31-root-INFO: grad norm: 2.569 2.563 0.174
2024-12-02-14:43:32-root-INFO: grad norm: 2.051 2.045 0.164
2024-12-02-14:43:33-root-INFO: grad norm: 1.859 1.854 0.137
2024-12-02-14:43:34-root-INFO: grad norm: 2.089 2.084 0.133
2024-12-02-14:43:34-root-INFO: Loss Change: 93.285 -> 7.816
2024-12-02-14:43:34-root-INFO: Regularization Change: 0.000 -> 178.817
2024-12-02-14:43:34-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-14:43:34-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-14:43:35-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-14:43:35-root-INFO: grad norm: 1.919 1.911 0.176
2024-12-02-14:43:36-root-INFO: grad norm: 1.700 1.694 0.149
2024-12-02-14:43:37-root-INFO: grad norm: 1.533 1.529 0.111
2024-12-02-14:43:38-root-INFO: grad norm: 1.415 1.411 0.112
2024-12-02-14:43:39-root-INFO: grad norm: 1.320 1.317 0.091
2024-12-02-14:43:40-root-INFO: grad norm: 1.242 1.239 0.093
2024-12-02-14:43:41-root-INFO: grad norm: 1.177 1.174 0.080
2024-12-02-14:43:42-root-INFO: grad norm: 1.126 1.123 0.079
2024-12-02-14:43:43-root-INFO: Loss Change: 7.529 -> 4.101
2024-12-02-14:43:43-root-INFO: Regularization Change: 0.000 -> 12.778
2024-12-02-14:43:43-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-14:43:43-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-14:43:43-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-14:43:44-root-INFO: grad norm: 1.464 1.451 0.194
2024-12-02-14:43:45-root-INFO: grad norm: 1.302 1.296 0.129
2024-12-02-14:43:46-root-INFO: grad norm: 1.060 1.056 0.092
2024-12-02-14:43:47-root-INFO: grad norm: 0.969 0.964 0.095
2024-12-02-14:43:48-root-INFO: grad norm: 1.030 1.027 0.087
2024-12-02-14:43:49-root-INFO: grad norm: 0.844 0.839 0.092
2024-12-02-14:43:50-root-INFO: grad norm: 0.775 0.772 0.058
2024-12-02-14:43:51-root-INFO: grad norm: 0.954 0.952 0.069
2024-12-02-14:43:51-root-INFO: Loss Change: 4.040 -> 2.783
2024-12-02-14:43:51-root-INFO: Regularization Change: 0.000 -> 4.615
2024-12-02-14:43:51-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-14:43:51-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-14:43:52-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-14:43:52-root-INFO: grad norm: 1.232 1.218 0.189
2024-12-02-14:43:53-root-INFO: grad norm: 0.853 0.847 0.100
2024-12-02-14:43:54-root-INFO: grad norm: 0.797 0.794 0.069
2024-12-02-14:43:55-root-INFO: grad norm: 0.986 0.984 0.058
2024-12-02-14:43:56-root-INFO: grad norm: 0.656 0.653 0.062
2024-12-02-14:43:57-root-INFO: grad norm: 0.544 0.543 0.037
2024-12-02-14:43:58-root-INFO: grad norm: 0.532 0.531 0.033
2024-12-02-14:43:59-root-INFO: grad norm: 0.666 0.662 0.075
2024-12-02-14:44:00-root-INFO: Loss too large (2.143->2.150)! Learning rate decreased to 0.42809.
2024-12-02-14:44:00-root-INFO: Loss Change: 2.829 -> 2.112
2024-12-02-14:44:00-root-INFO: Regularization Change: 0.000 -> 2.247
2024-12-02-14:44:00-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-14:44:00-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-14:44:01-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-14:44:01-root-INFO: grad norm: 1.150 1.130 0.213
2024-12-02-14:44:02-root-INFO: grad norm: 0.899 0.889 0.139
2024-12-02-14:44:03-root-INFO: grad norm: 0.805 0.800 0.092
2024-12-02-14:44:04-root-INFO: grad norm: 0.837 0.828 0.121
2024-12-02-14:44:04-root-INFO: Loss too large (1.968->1.970)! Learning rate decreased to 0.43377.
2024-12-02-14:44:05-root-INFO: grad norm: 0.661 0.658 0.067
2024-12-02-14:44:06-root-INFO: grad norm: 0.489 0.485 0.064
2024-12-02-14:44:08-root-INFO: grad norm: 0.442 0.440 0.037
2024-12-02-14:44:09-root-INFO: grad norm: 0.369 0.367 0.041
2024-12-02-14:44:09-root-INFO: Loss Change: 2.231 -> 1.775
2024-12-02-14:44:09-root-INFO: Regularization Change: 0.000 -> 1.130
2024-12-02-14:44:09-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-14:44:09-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-14:44:10-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-14:44:10-root-INFO: grad norm: 0.963 0.944 0.190
2024-12-02-14:44:11-root-INFO: grad norm: 0.684 0.677 0.101
2024-12-02-14:44:12-root-INFO: grad norm: 0.583 0.580 0.062
2024-12-02-14:44:13-root-INFO: grad norm: 0.543 0.539 0.069
2024-12-02-14:44:14-root-INFO: grad norm: 0.534 0.531 0.053
2024-12-02-14:44:15-root-INFO: grad norm: 0.504 0.500 0.064
2024-12-02-14:44:16-root-INFO: grad norm: 0.523 0.520 0.058
2024-12-02-14:44:17-root-INFO: grad norm: 0.506 0.502 0.065
2024-12-02-14:44:18-root-INFO: Loss Change: 1.927 -> 1.583
2024-12-02-14:44:18-root-INFO: Regularization Change: 0.000 -> 1.038
2024-12-02-14:44:18-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-14:44:18-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-14:44:18-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-14:44:18-root-INFO: grad norm: 1.045 1.027 0.192
2024-12-02-14:44:19-root-INFO: grad norm: 0.609 0.604 0.081
2024-12-02-14:44:20-root-INFO: grad norm: 0.418 0.417 0.037
2024-12-02-14:44:21-root-INFO: grad norm: 0.339 0.337 0.039
2024-12-02-14:44:22-root-INFO: grad norm: 0.312 0.311 0.026
2024-12-02-14:44:23-root-INFO: grad norm: 0.331 0.329 0.032
2024-12-02-14:44:24-root-INFO: grad norm: 0.285 0.284 0.024
2024-12-02-14:44:25-root-INFO: grad norm: 0.287 0.286 0.029
2024-12-02-14:44:26-root-INFO: Loss Change: 1.757 -> 1.418
2024-12-02-14:44:26-root-INFO: Regularization Change: 0.000 -> 0.866
2024-12-02-14:44:26-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-14:44:26-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-14:44:26-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-14:44:27-root-INFO: grad norm: 0.794 0.786 0.117
2024-12-02-14:44:28-root-INFO: grad norm: 0.412 0.408 0.055
2024-12-02-14:44:29-root-INFO: grad norm: 0.307 0.305 0.035
2024-12-02-14:44:30-root-INFO: grad norm: 0.286 0.283 0.041
2024-12-02-14:44:31-root-INFO: grad norm: 0.302 0.299 0.039
2024-12-02-14:44:32-root-INFO: grad norm: 0.303 0.300 0.045
2024-12-02-14:44:33-root-INFO: grad norm: 0.313 0.310 0.041
2024-12-02-14:44:34-root-INFO: grad norm: 0.306 0.303 0.045
2024-12-02-14:44:34-root-INFO: Loss Change: 1.574 -> 1.314
2024-12-02-14:44:34-root-INFO: Regularization Change: 0.000 -> 0.720
2024-12-02-14:44:34-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-14:44:34-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-14:44:35-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-14:44:35-root-INFO: grad norm: 0.900 0.886 0.162
2024-12-02-14:44:36-root-INFO: grad norm: 0.502 0.500 0.048
2024-12-02-14:44:37-root-INFO: grad norm: 0.289 0.287 0.028
2024-12-02-14:44:38-root-INFO: grad norm: 0.219 0.217 0.032
2024-12-02-14:44:39-root-INFO: grad norm: 0.222 0.221 0.026
2024-12-02-14:44:40-root-INFO: grad norm: 0.179 0.177 0.029
2024-12-02-14:44:41-root-INFO: grad norm: 0.180 0.178 0.023
2024-12-02-14:44:42-root-INFO: grad norm: 0.171 0.169 0.027
2024-12-02-14:44:43-root-INFO: Loss Change: 1.494 -> 1.234
2024-12-02-14:44:43-root-INFO: Regularization Change: 0.000 -> 0.660
2024-12-02-14:44:43-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-14:44:43-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-14:44:43-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-14:44:44-root-INFO: grad norm: 0.740 0.735 0.088
2024-12-02-14:44:45-root-INFO: grad norm: 0.373 0.371 0.034
2024-12-02-14:44:46-root-INFO: grad norm: 0.276 0.275 0.024
2024-12-02-14:44:47-root-INFO: grad norm: 0.239 0.238 0.026
2024-12-02-14:44:48-root-INFO: grad norm: 0.215 0.214 0.021
2024-12-02-14:44:49-root-INFO: grad norm: 0.212 0.211 0.022
2024-12-02-14:44:50-root-INFO: grad norm: 0.184 0.184 0.019
2024-12-02-14:44:51-root-INFO: grad norm: 0.178 0.177 0.020
2024-12-02-14:44:52-root-INFO: Loss Change: 1.379 -> 1.129
2024-12-02-14:44:52-root-INFO: Regularization Change: 0.000 -> 0.736
2024-12-02-14:44:52-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-14:44:52-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-14:44:52-root-INFO: loss_sample0_0: 1.1285048723220825
2024-12-02-14:44:52-root-INFO: It takes 4628.462 seconds for image sample0
2024-12-02-14:44:52-root-INFO: lpips_score_sample0: 0.148
2024-12-02-14:44:52-root-INFO: psnr_score_sample0: 17.078
2024-12-02-14:44:52-root-INFO: ssim_score_sample0: 0.715
2024-12-02-14:44:52-root-INFO: mean_lpips: 0.14750446379184723
2024-12-02-14:44:52-root-INFO: best_mean_lpips: 0.14750446379184723
2024-12-02-14:44:52-root-INFO: mean_psnr: 17.077945709228516
2024-12-02-14:44:52-root-INFO: best_mean_psnr: 17.077945709228516
2024-12-02-14:44:52-root-INFO: mean_ssim: 0.715366005897522
2024-12-02-14:44:52-root-INFO: best_mean_ssim: 0.715366005897522
2024-12-02-14:44:52-root-INFO: final_loss: 1.1285048723220825
2024-12-02-14:44:52-root-INFO: mean time: 4628.4618508815765
2024-12-02-14:44:52-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample2_iter8_lr0.03_10009 
 
Enjoy.
