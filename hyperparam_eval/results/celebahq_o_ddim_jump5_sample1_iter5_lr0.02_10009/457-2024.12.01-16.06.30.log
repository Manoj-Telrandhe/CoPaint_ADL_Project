2024-12-01-16:06:33-root-INFO: Prepare model...
2024-12-01-16:06:50-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-01-16:07:16-root-INFO: Start sampling
2024-12-01-16:07:21-root-INFO: step: 249 lr_xt 0.00012706
2024-12-01-16:07:21-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-01-16:07:22-root-INFO: grad norm: 23712.414 17715.260 15762.239
2024-12-01-16:07:22-root-INFO: grad norm: 26791.451 21193.887 16389.051
2024-12-01-16:07:22-root-INFO: Loss too large (43809.938->60939.746)! Learning rate decreased to 0.00010.
2024-12-01-16:07:23-root-INFO: Loss too large (43809.938->45474.273)! Learning rate decreased to 0.00008.
2024-12-01-16:07:23-root-INFO: grad norm: 21161.568 16197.647 13617.937
2024-12-01-16:07:24-root-INFO: grad norm: 19143.291 15678.843 10983.599
2024-12-01-16:07:24-root-INFO: Loss Change: 77070.016 -> 26877.148
2024-12-01-16:07:24-root-INFO: Regularization Change: 0.000 -> 12.886
2024-12-01-16:07:24-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-01-16:07:24-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-16:07:24-root-INFO: step: 248 lr_xt 0.00013388
2024-12-01-16:07:24-root-INFO: grad norm: 16833.188 13086.392 10587.851
2024-12-01-16:07:25-root-INFO: Loss too large (26922.422->41978.324)! Learning rate decreased to 0.00011.
2024-12-01-16:07:25-root-INFO: Loss too large (26922.422->30477.258)! Learning rate decreased to 0.00009.
2024-12-01-16:07:25-root-INFO: grad norm: 14962.307 12111.881 8784.815
2024-12-01-16:07:26-root-INFO: grad norm: 13330.779 10541.792 8159.674
2024-12-01-16:07:26-root-INFO: grad norm: 11796.043 9477.737 7022.757
2024-12-01-16:07:27-root-INFO: grad norm: 10413.207 8319.823 6262.222
2024-12-01-16:07:27-root-INFO: Loss Change: 26922.422 -> 19160.537
2024-12-01-16:07:27-root-INFO: Regularization Change: 0.000 -> 2.794
2024-12-01-16:07:27-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-01-16:07:27-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-16:07:27-root-INFO: step: 247 lr_xt 0.00014104
2024-12-01-16:07:27-root-INFO: grad norm: 8803.503 7050.845 5271.361
2024-12-01-16:07:27-root-INFO: Loss too large (18953.281->23036.594)! Learning rate decreased to 0.00011.
2024-12-01-16:07:28-root-INFO: Loss too large (18953.281->19964.223)! Learning rate decreased to 0.00009.
2024-12-01-16:07:28-root-INFO: grad norm: 7322.107 5942.632 4277.661
2024-12-01-16:07:29-root-INFO: grad norm: 5995.245 4786.363 3610.221
2024-12-01-16:07:29-root-INFO: grad norm: 4943.557 4020.369 2876.698
2024-12-01-16:07:29-root-INFO: grad norm: 4022.235 3207.296 2427.268
2024-12-01-16:07:30-root-INFO: Loss Change: 18953.281 -> 16716.287
2024-12-01-16:07:30-root-INFO: Regularization Change: 0.000 -> 0.649
2024-12-01-16:07:30-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-01-16:07:30-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-16:07:30-root-INFO: step: 246 lr_xt 0.00014856
2024-12-01-16:07:30-root-INFO: grad norm: 3162.669 2611.130 1784.510
2024-12-01-16:07:30-root-INFO: Loss too large (16469.129->16841.117)! Learning rate decreased to 0.00012.
2024-12-01-16:07:31-root-INFO: grad norm: 3732.916 2998.605 2223.292
2024-12-01-16:07:31-root-INFO: Loss too large (16466.869->16537.178)! Learning rate decreased to 0.00010.
2024-12-01-16:07:31-root-INFO: grad norm: 2893.745 2369.964 1660.430
2024-12-01-16:07:32-root-INFO: grad norm: 2242.270 1817.479 1313.220
2024-12-01-16:07:32-root-INFO: grad norm: 1775.768 1456.297 1016.146
2024-12-01-16:07:33-root-INFO: Loss Change: 16469.129 -> 15881.556
2024-12-01-16:07:33-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-01-16:07:33-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-01-16:07:33-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-16:07:33-root-INFO: step: 245 lr_xt 0.00015646
2024-12-01-16:07:33-root-INFO: grad norm: 1396.718 1149.037 794.062
2024-12-01-16:07:34-root-INFO: grad norm: 2001.182 1614.650 1182.215
2024-12-01-16:07:34-root-INFO: Loss too large (15714.607->15791.719)! Learning rate decreased to 0.00013.
2024-12-01-16:07:34-root-INFO: grad norm: 2205.379 1820.328 1245.031
2024-12-01-16:07:35-root-INFO: grad norm: 2437.174 1958.179 1450.983
2024-12-01-16:07:35-root-INFO: grad norm: 2714.300 2240.203 1532.617
2024-12-01-16:07:36-root-INFO: Loss Change: 15753.134 -> 15577.865
2024-12-01-16:07:36-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-01-16:07:36-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-01-16:07:36-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-16:07:36-root-INFO: step: 244 lr_xt 0.00016475
2024-12-01-16:07:36-root-INFO: grad norm: 3027.029 2437.065 1795.445
2024-12-01-16:07:36-root-INFO: Loss too large (15495.254->15710.410)! Learning rate decreased to 0.00013.
2024-12-01-16:07:37-root-INFO: grad norm: 3078.810 2533.486 1749.434
2024-12-01-16:07:37-root-INFO: grad norm: 3222.856 2600.189 1904.158
2024-12-01-16:07:38-root-INFO: grad norm: 3421.746 2836.915 1913.180
2024-12-01-16:07:38-root-INFO: grad norm: 3641.459 2938.594 2150.556
2024-12-01-16:07:38-root-INFO: Loss Change: 15495.254 -> 15238.794
2024-12-01-16:07:38-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-01-16:07:38-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-01-16:07:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:07:39-root-INFO: step: 243 lr_xt 0.00017345
2024-12-01-16:07:39-root-INFO: grad norm: 3728.776 3092.847 2082.803
2024-12-01-16:07:39-root-INFO: Loss too large (15090.453->15566.497)! Learning rate decreased to 0.00014.
2024-12-01-16:07:39-root-INFO: grad norm: 3748.711 3020.538 2220.176
2024-12-01-16:07:40-root-INFO: grad norm: 3818.832 3188.424 2101.768
2024-12-01-16:07:40-root-INFO: grad norm: 3909.819 3160.748 2301.381
2024-12-01-16:07:41-root-INFO: grad norm: 4022.587 3357.806 2215.028
2024-12-01-16:07:41-root-INFO: Loss Change: 15090.453 -> 14810.240
2024-12-01-16:07:41-root-INFO: Regularization Change: 0.000 -> 0.561
2024-12-01-16:07:41-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-01-16:07:41-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:07:41-root-INFO: step: 242 lr_xt 0.00018258
2024-12-01-16:07:41-root-INFO: grad norm: 3847.180 3156.692 2199.111
2024-12-01-16:07:42-root-INFO: Loss too large (14551.131->15039.519)! Learning rate decreased to 0.00015.
2024-12-01-16:07:42-root-INFO: grad norm: 3787.115 3176.390 2062.229
2024-12-01-16:07:43-root-INFO: grad norm: 3760.371 3069.540 2172.168
2024-12-01-16:07:43-root-INFO: grad norm: 3743.169 3146.768 2027.109
2024-12-01-16:07:44-root-INFO: grad norm: 3736.450 3048.869 2159.967
2024-12-01-16:07:44-root-INFO: Loss Change: 14551.131 -> 14081.598
2024-12-01-16:07:44-root-INFO: Regularization Change: 0.000 -> 0.597
2024-12-01-16:07:44-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-01-16:07:44-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:07:44-root-INFO: step: 241 lr_xt 0.00019216
2024-12-01-16:07:44-root-INFO: grad norm: 3713.485 3124.741 2006.480
2024-12-01-16:07:44-root-INFO: Loss too large (14017.855->14494.987)! Learning rate decreased to 0.00015.
2024-12-01-16:07:45-root-INFO: grad norm: 3580.057 2948.352 2030.771
2024-12-01-16:07:45-root-INFO: grad norm: 3457.101 2920.033 1850.663
2024-12-01-16:07:46-root-INFO: grad norm: 3350.526 2756.009 1905.371
2024-12-01-16:07:46-root-INFO: grad norm: 3244.564 2744.452 1730.660
2024-12-01-16:07:47-root-INFO: Loss Change: 14017.855 -> 13458.083
2024-12-01-16:07:47-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-01-16:07:47-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-01-16:07:47-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:07:47-root-INFO: step: 240 lr_xt 0.00020221
2024-12-01-16:07:47-root-INFO: grad norm: 2988.908 2492.028 1650.264
2024-12-01-16:07:47-root-INFO: Loss too large (13258.799->13466.858)! Learning rate decreased to 0.00016.
2024-12-01-16:07:48-root-INFO: grad norm: 2773.089 2367.260 1444.336
2024-12-01-16:07:48-root-INFO: grad norm: 2604.329 2157.608 1458.511
2024-12-01-16:07:49-root-INFO: grad norm: 2453.020 2104.762 1259.874
2024-12-01-16:07:49-root-INFO: grad norm: 2320.874 1923.609 1298.532
2024-12-01-16:07:50-root-INFO: Loss Change: 13258.799 -> 12626.716
2024-12-01-16:07:50-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-01-16:07:50-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-01-16:07:50-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:07:50-root-INFO: step: 239 lr_xt 0.00021275
2024-12-01-16:07:50-root-INFO: grad norm: 2090.839 1800.851 1062.328
2024-12-01-16:07:50-root-INFO: Loss too large (12550.245->12593.204)! Learning rate decreased to 0.00017.
2024-12-01-16:07:51-root-INFO: grad norm: 1926.947 1605.890 1065.009
2024-12-01-16:07:51-root-INFO: grad norm: 1779.542 1540.139 891.482
2024-12-01-16:07:52-root-INFO: grad norm: 1650.419 1379.898 905.408
2024-12-01-16:07:52-root-INFO: grad norm: 1532.310 1332.149 757.202
2024-12-01-16:07:52-root-INFO: Loss Change: 12550.245 -> 11990.064
2024-12-01-16:07:52-root-INFO: Regularization Change: 0.000 -> 0.470
2024-12-01-16:07:52-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-01-16:07:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:07:53-root-INFO: step: 238 lr_xt 0.00022380
2024-12-01-16:07:53-root-INFO: grad norm: 1543.357 1323.460 793.979
2024-12-01-16:07:53-root-INFO: grad norm: 1861.078 1612.261 929.637
2024-12-01-16:07:54-root-INFO: grad norm: 2384.709 2016.269 1273.381
2024-12-01-16:07:54-root-INFO: Loss too large (11678.701->11753.084)! Learning rate decreased to 0.00018.
2024-12-01-16:07:55-root-INFO: grad norm: 2107.980 1831.590 1043.484
2024-12-01-16:07:55-root-INFO: grad norm: 1879.491 1594.288 995.354
2024-12-01-16:07:55-root-INFO: Loss Change: 11781.879 -> 11259.322
2024-12-01-16:07:55-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-01-16:07:55-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-01-16:07:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:07:56-root-INFO: step: 237 lr_xt 0.00023539
2024-12-01-16:07:56-root-INFO: grad norm: 1520.228 1334.689 727.803
2024-12-01-16:07:56-root-INFO: grad norm: 1860.733 1589.151 967.951
2024-12-01-16:07:57-root-INFO: grad norm: 2349.396 2052.830 1142.608
2024-12-01-16:07:57-root-INFO: Loss too large (11078.853->11142.084)! Learning rate decreased to 0.00019.
2024-12-01-16:07:57-root-INFO: grad norm: 2018.848 1731.238 1038.537
2024-12-01-16:07:58-root-INFO: grad norm: 1743.745 1533.211 830.609
2024-12-01-16:07:58-root-INFO: Loss Change: 11163.605 -> 10640.326
2024-12-01-16:07:58-root-INFO: Regularization Change: 0.000 -> 0.621
2024-12-01-16:07:58-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-01-16:07:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:07:58-root-INFO: step: 236 lr_xt 0.00024753
2024-12-01-16:07:59-root-INFO: grad norm: 1446.355 1265.016 701.197
2024-12-01-16:07:59-root-INFO: grad norm: 1731.342 1516.125 836.008
2024-12-01-16:08:00-root-INFO: grad norm: 2102.600 1815.442 1060.706
2024-12-01-16:08:00-root-INFO: Loss too large (10407.873->10417.879)! Learning rate decreased to 0.00020.
2024-12-01-16:08:00-root-INFO: grad norm: 1725.235 1516.045 823.434
2024-12-01-16:08:01-root-INFO: grad norm: 1432.186 1239.298 717.843
2024-12-01-16:08:01-root-INFO: Loss Change: 10513.177 -> 9985.134
2024-12-01-16:08:01-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-01-16:08:01-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-01-16:08:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:08:01-root-INFO: step: 235 lr_xt 0.00026027
2024-12-01-16:08:01-root-INFO: grad norm: 1011.641 902.479 457.108
2024-12-01-16:08:02-root-INFO: grad norm: 1115.473 974.075 543.560
2024-12-01-16:08:02-root-INFO: grad norm: 1268.119 1134.007 567.587
2024-12-01-16:08:03-root-INFO: grad norm: 1464.395 1274.496 721.188
2024-12-01-16:08:03-root-INFO: grad norm: 1705.328 1513.200 786.365
2024-12-01-16:08:03-root-INFO: Loss Change: 9844.682 -> 9526.325
2024-12-01-16:08:03-root-INFO: Regularization Change: 0.000 -> 0.754
2024-12-01-16:08:03-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-01-16:08:03-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-16:08:04-root-INFO: step: 234 lr_xt 0.00027361
2024-12-01-16:08:04-root-INFO: grad norm: 1907.926 1671.024 920.793
2024-12-01-16:08:04-root-INFO: grad norm: 2149.788 1900.109 1005.571
2024-12-01-16:08:05-root-INFO: grad norm: 2415.393 2107.289 1180.446
2024-12-01-16:08:05-root-INFO: Loss too large (9417.777->9428.930)! Learning rate decreased to 0.00022.
2024-12-01-16:08:05-root-INFO: grad norm: 1741.586 1546.067 801.746
2024-12-01-16:08:06-root-INFO: grad norm: 1283.695 1122.037 623.622
2024-12-01-16:08:06-root-INFO: Loss Change: 9452.771 -> 8909.186
2024-12-01-16:08:06-root-INFO: Regularization Change: 0.000 -> 0.572
2024-12-01-16:08:06-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-01-16:08:06-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:08:06-root-INFO: step: 233 lr_xt 0.00028759
2024-12-01-16:08:07-root-INFO: grad norm: 920.538 806.527 443.740
2024-12-01-16:08:07-root-INFO: grad norm: 924.444 814.671 436.928
2024-12-01-16:08:07-root-INFO: grad norm: 958.933 865.030 413.856
2024-12-01-16:08:08-root-INFO: grad norm: 1001.597 882.663 473.394
2024-12-01-16:08:08-root-INFO: grad norm: 1048.496 945.156 453.899
2024-12-01-16:08:09-root-INFO: Loss Change: 8848.558 -> 8495.885
2024-12-01-16:08:09-root-INFO: Regularization Change: 0.000 -> 0.607
2024-12-01-16:08:09-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-01-16:08:09-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:08:09-root-INFO: step: 232 lr_xt 0.00030224
2024-12-01-16:08:09-root-INFO: grad norm: 1381.255 1206.611 672.276
2024-12-01-16:08:09-root-INFO: grad norm: 1363.660 1230.395 587.960
2024-12-01-16:08:10-root-INFO: grad norm: 1375.047 1213.097 647.417
2024-12-01-16:08:10-root-INFO: grad norm: 1389.966 1254.769 597.964
2024-12-01-16:08:11-root-INFO: grad norm: 1403.161 1241.230 654.377
2024-12-01-16:08:11-root-INFO: Loss Change: 8380.835 -> 8054.046
2024-12-01-16:08:11-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-01-16:08:11-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-01-16:08:11-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:08:11-root-INFO: step: 231 lr_xt 0.00031758
2024-12-01-16:08:12-root-INFO: grad norm: 1252.042 1134.629 529.364
2024-12-01-16:08:12-root-INFO: grad norm: 1227.396 1088.307 567.528
2024-12-01-16:08:12-root-INFO: grad norm: 1198.713 1085.750 507.996
2024-12-01-16:08:13-root-INFO: grad norm: 1168.140 1035.864 539.941
2024-12-01-16:08:14-root-INFO: grad norm: 1134.999 1029.120 478.680
2024-12-01-16:08:14-root-INFO: Loss Change: 7957.583 -> 7642.458
2024-12-01-16:08:14-root-INFO: Regularization Change: 0.000 -> 0.589
2024-12-01-16:08:14-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-01-16:08:14-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:08:14-root-INFO: step: 230 lr_xt 0.00033364
2024-12-01-16:08:14-root-INFO: grad norm: 1236.842 1095.874 573.445
2024-12-01-16:08:15-root-INFO: grad norm: 1151.001 1048.316 475.225
2024-12-01-16:08:15-root-INFO: grad norm: 1075.503 956.965 490.839
2024-12-01-16:08:16-root-INFO: grad norm: 1004.488 915.958 412.332
2024-12-01-16:08:16-root-INFO: grad norm: 939.834 838.557 424.393
2024-12-01-16:08:16-root-INFO: Loss Change: 7578.823 -> 7255.425
2024-12-01-16:08:16-root-INFO: Regularization Change: 0.000 -> 0.551
2024-12-01-16:08:16-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-01-16:08:16-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:08:17-root-INFO: step: 229 lr_xt 0.00035047
2024-12-01-16:08:17-root-INFO: grad norm: 655.775 607.028 248.108
2024-12-01-16:08:17-root-INFO: grad norm: 593.347 533.222 260.260
2024-12-01-16:08:18-root-INFO: grad norm: 551.694 514.541 199.030
2024-12-01-16:08:18-root-INFO: grad norm: 516.204 468.270 217.231
2024-12-01-16:08:19-root-INFO: grad norm: 485.576 453.459 173.663
2024-12-01-16:08:19-root-INFO: Loss Change: 7137.035 -> 6892.322
2024-12-01-16:08:19-root-INFO: Regularization Change: 0.000 -> 0.423
2024-12-01-16:08:19-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-01-16:08:19-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:08:19-root-INFO: step: 228 lr_xt 0.00036807
2024-12-01-16:08:19-root-INFO: grad norm: 590.152 524.246 271.008
2024-12-01-16:08:20-root-INFO: grad norm: 519.487 481.851 194.129
2024-12-01-16:08:20-root-INFO: grad norm: 474.961 432.730 195.786
2024-12-01-16:08:21-root-INFO: grad norm: 440.078 411.177 156.850
2024-12-01-16:08:22-root-INFO: grad norm: 411.456 377.251 164.248
2024-12-01-16:08:22-root-INFO: Loss Change: 6855.381 -> 6643.425
2024-12-01-16:08:22-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-01-16:08:22-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-01-16:08:22-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:08:22-root-INFO: step: 227 lr_xt 0.00038651
2024-12-01-16:08:22-root-INFO: grad norm: 406.001 362.094 183.643
2024-12-01-16:08:23-root-INFO: grad norm: 344.920 318.771 131.738
2024-12-01-16:08:23-root-INFO: grad norm: 324.231 298.561 126.438
2024-12-01-16:08:24-root-INFO: grad norm: 312.250 291.293 112.465
2024-12-01-16:08:24-root-INFO: grad norm: 303.684 282.151 112.315
2024-12-01-16:08:24-root-INFO: Loss Change: 6627.518 -> 6431.988
2024-12-01-16:08:24-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-01-16:08:24-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-01-16:08:24-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-16:08:25-root-INFO: step: 226 lr_xt 0.00040579
2024-12-01-16:08:25-root-INFO: grad norm: 365.876 320.167 177.083
2024-12-01-16:08:25-root-INFO: grad norm: 295.304 274.951 107.733
2024-12-01-16:08:26-root-INFO: grad norm: 283.995 266.097 99.225
2024-12-01-16:08:26-root-INFO: grad norm: 278.678 260.521 98.944
2024-12-01-16:08:27-root-INFO: grad norm: 274.534 257.312 95.706
2024-12-01-16:08:27-root-INFO: Loss Change: 6374.374 -> 6205.503
2024-12-01-16:08:27-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-01-16:08:27-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-01-16:08:27-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:08:27-root-INFO: step: 225 lr_xt 0.00042598
2024-12-01-16:08:27-root-INFO: grad norm: 493.586 447.954 207.278
2024-12-01-16:08:28-root-INFO: grad norm: 403.988 374.495 151.525
2024-12-01-16:08:28-root-INFO: grad norm: 363.007 333.053 144.395
2024-12-01-16:08:28-root-INFO: grad norm: 333.707 309.177 125.579
2024-12-01-16:08:29-root-INFO: grad norm: 311.808 287.951 119.619
2024-12-01-16:08:29-root-INFO: Loss Change: 6155.996 -> 5978.444
2024-12-01-16:08:29-root-INFO: Regularization Change: 0.000 -> 0.358
2024-12-01-16:08:29-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-01-16:08:29-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:08:30-root-INFO: step: 224 lr_xt 0.00044709
2024-12-01-16:08:30-root-INFO: grad norm: 317.093 283.298 142.444
2024-12-01-16:08:30-root-INFO: grad norm: 272.423 249.925 108.404
2024-12-01-16:08:31-root-INFO: grad norm: 260.366 239.918 101.143
2024-12-01-16:08:31-root-INFO: grad norm: 253.198 233.737 97.346
2024-12-01-16:08:32-root-INFO: grad norm: 248.379 229.902 94.005
2024-12-01-16:08:32-root-INFO: Loss Change: 5944.329 -> 5789.340
2024-12-01-16:08:32-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-01-16:08:32-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-01-16:08:32-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:08:32-root-INFO: step: 223 lr_xt 0.00046917
2024-12-01-16:08:33-root-INFO: grad norm: 356.564 326.380 143.575
2024-12-01-16:08:33-root-INFO: grad norm: 313.853 287.995 124.749
2024-12-01-16:08:33-root-INFO: grad norm: 292.652 271.190 110.006
2024-12-01-16:08:34-root-INFO: grad norm: 277.515 255.257 108.895
2024-12-01-16:08:34-root-INFO: grad norm: 266.158 247.532 97.815
2024-12-01-16:08:35-root-INFO: Loss Change: 5755.140 -> 5604.754
2024-12-01-16:08:35-root-INFO: Regularization Change: 0.000 -> 0.347
2024-12-01-16:08:35-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-01-16:08:35-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:08:35-root-INFO: step: 222 lr_xt 0.00049227
2024-12-01-16:08:35-root-INFO: grad norm: 245.020 223.702 99.962
2024-12-01-16:08:36-root-INFO: grad norm: 233.041 216.764 85.565
2024-12-01-16:08:36-root-INFO: grad norm: 229.739 212.810 86.555
2024-12-01-16:08:36-root-INFO: grad norm: 227.039 211.028 83.749
2024-12-01-16:08:37-root-INFO: grad norm: 224.624 208.350 83.942
2024-12-01-16:08:37-root-INFO: Loss Change: 5538.899 -> 5409.786
2024-12-01-16:08:37-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-01-16:08:37-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-01-16:08:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:08:37-root-INFO: step: 221 lr_xt 0.00051641
2024-12-01-16:08:38-root-INFO: grad norm: 270.603 249.702 104.282
2024-12-01-16:08:38-root-INFO: grad norm: 247.729 229.226 93.941
2024-12-01-16:08:39-root-INFO: grad norm: 237.988 222.030 85.681
2024-12-01-16:08:39-root-INFO: grad norm: 230.693 213.692 86.920
2024-12-01-16:08:40-root-INFO: grad norm: 224.860 209.815 80.868
2024-12-01-16:08:40-root-INFO: Loss Change: 5361.817 -> 5236.466
2024-12-01-16:08:40-root-INFO: Regularization Change: 0.000 -> 0.321
2024-12-01-16:08:40-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-01-16:08:40-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:08:40-root-INFO: step: 220 lr_xt 0.00054166
2024-12-01-16:08:40-root-INFO: grad norm: 375.791 347.404 143.280
2024-12-01-16:08:41-root-INFO: grad norm: 317.402 294.198 119.128
2024-12-01-16:08:41-root-INFO: grad norm: 291.170 272.246 103.258
2024-12-01-16:08:42-root-INFO: grad norm: 273.503 253.044 103.793
2024-12-01-16:08:42-root-INFO: grad norm: 260.175 243.967 90.394
2024-12-01-16:08:42-root-INFO: Loss Change: 5221.195 -> 5075.812
2024-12-01-16:08:42-root-INFO: Regularization Change: 0.000 -> 0.381
2024-12-01-16:08:42-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-01-16:08:42-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:08:43-root-INFO: step: 219 lr_xt 0.00056804
2024-12-01-16:08:43-root-INFO: grad norm: 244.382 222.998 99.972
2024-12-01-16:08:43-root-INFO: grad norm: 228.295 213.351 81.239
2024-12-01-16:08:44-root-INFO: grad norm: 220.735 204.103 84.060
2024-12-01-16:08:44-root-INFO: grad norm: 214.658 200.330 77.110
2024-12-01-16:08:45-root-INFO: grad norm: 209.600 193.816 79.797
2024-12-01-16:08:45-root-INFO: Loss Change: 5049.426 -> 4933.919
2024-12-01-16:08:45-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-01-16:08:45-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-01-16:08:45-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-16:08:45-root-INFO: step: 218 lr_xt 0.00059561
2024-12-01-16:08:45-root-INFO: grad norm: 340.398 316.766 124.620
2024-12-01-16:08:46-root-INFO: grad norm: 307.720 286.498 112.297
2024-12-01-16:08:46-root-INFO: grad norm: 286.684 269.416 97.994
2024-12-01-16:08:47-root-INFO: grad norm: 270.447 251.176 100.260
2024-12-01-16:08:47-root-INFO: grad norm: 257.263 242.136 86.915
2024-12-01-16:08:48-root-INFO: Loss Change: 4914.380 -> 4790.510
2024-12-01-16:08:48-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-01-16:08:48-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-01-16:08:48-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:08:48-root-INFO: step: 217 lr_xt 0.00062443
2024-12-01-16:08:48-root-INFO: grad norm: 247.355 217.785 117.279
2024-12-01-16:08:48-root-INFO: grad norm: 200.831 184.991 78.177
2024-12-01-16:08:49-root-INFO: grad norm: 191.040 176.661 72.712
2024-12-01-16:08:49-root-INFO: grad norm: 186.382 172.397 70.836
2024-12-01-16:08:50-root-INFO: grad norm: 183.372 170.194 68.258
2024-12-01-16:08:50-root-INFO: Loss Change: 4766.234 -> 4645.311
2024-12-01-16:08:50-root-INFO: Regularization Change: 0.000 -> 0.368
2024-12-01-16:08:50-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-01-16:08:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:08:50-root-INFO: step: 216 lr_xt 0.00065452
2024-12-01-16:08:50-root-INFO: grad norm: 314.044 288.119 124.946
2024-12-01-16:08:51-root-INFO: grad norm: 276.337 256.316 103.266
2024-12-01-16:08:52-root-INFO: grad norm: 265.700 249.990 90.008
2024-12-01-16:08:52-root-INFO: grad norm: 259.190 240.583 96.432
2024-12-01-16:08:53-root-INFO: grad norm: 254.623 240.506 83.605
2024-12-01-16:08:53-root-INFO: Loss Change: 4629.154 -> 4507.255
2024-12-01-16:08:53-root-INFO: Regularization Change: 0.000 -> 0.401
2024-12-01-16:08:53-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-01-16:08:53-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:08:53-root-INFO: step: 215 lr_xt 0.00068596
2024-12-01-16:08:53-root-INFO: grad norm: 227.749 211.797 83.736
2024-12-01-16:08:54-root-INFO: grad norm: 220.660 207.789 74.259
2024-12-01-16:08:54-root-INFO: grad norm: 215.201 199.912 79.666
2024-12-01-16:08:55-root-INFO: grad norm: 210.121 197.975 70.402
2024-12-01-16:08:55-root-INFO: grad norm: 205.301 190.589 76.317
2024-12-01-16:08:56-root-INFO: Loss Change: 4489.579 -> 4387.451
2024-12-01-16:08:56-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-01-16:08:56-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-01-16:08:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:08:56-root-INFO: step: 214 lr_xt 0.00071879
2024-12-01-16:08:56-root-INFO: grad norm: 330.651 306.984 122.845
2024-12-01-16:08:57-root-INFO: grad norm: 302.901 282.930 108.165
2024-12-01-16:08:57-root-INFO: grad norm: 289.233 273.816 93.171
2024-12-01-16:08:57-root-INFO: grad norm: 277.598 259.151 99.505
2024-12-01-16:08:58-root-INFO: grad norm: 266.913 253.345 84.016
2024-12-01-16:08:58-root-INFO: Loss Change: 4365.186 -> 4258.323
2024-12-01-16:08:58-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-01-16:08:58-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-01-16:08:58-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:08:58-root-INFO: step: 213 lr_xt 0.00075308
2024-12-01-16:08:59-root-INFO: grad norm: 209.312 187.717 92.596
2024-12-01-16:08:59-root-INFO: grad norm: 170.130 156.881 65.825
2024-12-01-16:09:00-root-INFO: grad norm: 160.083 148.184 60.565
2024-12-01-16:09:00-root-INFO: grad norm: 154.966 143.910 57.483
2024-12-01-16:09:01-root-INFO: grad norm: 151.423 140.944 55.353
2024-12-01-16:09:01-root-INFO: Loss Change: 4236.107 -> 4132.947
2024-12-01-16:09:01-root-INFO: Regularization Change: 0.000 -> 0.380
2024-12-01-16:09:01-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-01-16:09:01-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:09:01-root-INFO: step: 212 lr_xt 0.00078886
2024-12-01-16:09:01-root-INFO: grad norm: 150.232 141.245 51.182
2024-12-01-16:09:02-root-INFO: grad norm: 146.497 137.487 50.586
2024-12-01-16:09:02-root-INFO: grad norm: 143.905 135.062 49.668
2024-12-01-16:09:03-root-INFO: grad norm: 141.661 132.735 49.489
2024-12-01-16:09:03-root-INFO: grad norm: 139.540 130.773 48.680
2024-12-01-16:09:04-root-INFO: Loss Change: 4108.805 -> 4027.381
2024-12-01-16:09:04-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-01-16:09:04-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-01-16:09:04-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-16:09:04-root-INFO: step: 211 lr_xt 0.00082622
2024-12-01-16:09:04-root-INFO: grad norm: 262.585 236.101 114.921
2024-12-01-16:09:04-root-INFO: grad norm: 205.357 189.759 78.505
2024-12-01-16:09:05-root-INFO: grad norm: 193.244 182.475 63.609
2024-12-01-16:09:05-root-INFO: grad norm: 186.993 173.771 69.066
2024-12-01-16:09:06-root-INFO: grad norm: 182.400 173.307 56.871
2024-12-01-16:09:06-root-INFO: Loss Change: 4007.523 -> 3914.104
2024-12-01-16:09:06-root-INFO: Regularization Change: 0.000 -> 0.371
2024-12-01-16:09:06-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-01-16:09:06-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:09:06-root-INFO: step: 210 lr_xt 0.00086520
2024-12-01-16:09:07-root-INFO: grad norm: 137.826 127.196 53.077
2024-12-01-16:09:07-root-INFO: grad norm: 130.068 121.974 45.166
2024-12-01-16:09:08-root-INFO: grad norm: 127.418 119.102 45.278
2024-12-01-16:09:08-root-INFO: grad norm: 125.245 117.502 43.354
2024-12-01-16:09:08-root-INFO: grad norm: 123.331 115.262 43.877
2024-12-01-16:09:09-root-INFO: Loss Change: 3892.621 -> 3823.451
2024-12-01-16:09:09-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-01-16:09:09-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-01-16:09:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:09:09-root-INFO: step: 209 lr_xt 0.00090588
2024-12-01-16:09:09-root-INFO: grad norm: 185.101 168.121 77.446
2024-12-01-16:09:10-root-INFO: grad norm: 153.874 143.078 56.620
2024-12-01-16:09:10-root-INFO: grad norm: 146.815 138.160 49.663
2024-12-01-16:09:11-root-INFO: grad norm: 143.332 134.044 50.757
2024-12-01-16:09:11-root-INFO: grad norm: 140.943 133.323 45.717
2024-12-01-16:09:11-root-INFO: Loss Change: 3797.020 -> 3726.891
2024-12-01-16:09:11-root-INFO: Regularization Change: 0.000 -> 0.312
2024-12-01-16:09:11-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-01-16:09:11-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:09:12-root-INFO: step: 208 lr_xt 0.00094831
2024-12-01-16:09:12-root-INFO: grad norm: 157.065 141.548 68.070
2024-12-01-16:09:12-root-INFO: grad norm: 122.373 113.245 46.375
2024-12-01-16:09:13-root-INFO: grad norm: 114.208 106.451 41.371
2024-12-01-16:09:13-root-INFO: grad norm: 110.930 103.640 39.549
2024-12-01-16:09:14-root-INFO: grad norm: 108.865 101.986 38.084
2024-12-01-16:09:14-root-INFO: Loss Change: 3715.576 -> 3648.555
2024-12-01-16:09:14-root-INFO: Regularization Change: 0.000 -> 0.307
2024-12-01-16:09:14-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-01-16:09:14-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:09:14-root-INFO: step: 207 lr_xt 0.00100094
2024-12-01-16:09:14-root-INFO: grad norm: 134.540 122.934 54.665
2024-12-01-16:09:15-root-INFO: grad norm: 118.398 111.240 40.543
2024-12-01-16:09:15-root-INFO: grad norm: 115.701 108.463 40.280
2024-12-01-16:09:16-root-INFO: grad norm: 114.780 108.095 38.601
2024-12-01-16:09:16-root-INFO: grad norm: 114.621 107.965 38.491
2024-12-01-16:09:17-root-INFO: Loss Change: 3621.984 -> 3564.504
2024-12-01-16:09:17-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-01-16:09:17-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-01-16:09:17-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:09:17-root-INFO: step: 206 lr_xt 0.00104745
2024-12-01-16:09:17-root-INFO: grad norm: 122.906 113.583 46.955
2024-12-01-16:09:17-root-INFO: grad norm: 105.949 99.169 37.294
2024-12-01-16:09:18-root-INFO: grad norm: 103.006 96.904 34.927
2024-12-01-16:09:18-root-INFO: grad norm: 101.977 95.942 34.560
2024-12-01-16:09:19-root-INFO: grad norm: 101.529 95.805 33.607
2024-12-01-16:09:19-root-INFO: Loss Change: 3539.960 -> 3484.901
2024-12-01-16:09:19-root-INFO: Regularization Change: 0.000 -> 0.285
2024-12-01-16:09:19-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-16:09:19-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:09:19-root-INFO: step: 205 lr_xt 0.00109594
2024-12-01-16:09:19-root-INFO: grad norm: 152.745 140.547 59.813
2024-12-01-16:09:20-root-INFO: grad norm: 138.792 131.277 45.049
2024-12-01-16:09:20-root-INFO: grad norm: 146.949 139.114 47.342
2024-12-01-16:09:21-root-INFO: grad norm: 160.799 153.472 47.985
2024-12-01-16:09:21-root-INFO: grad norm: 180.992 172.341 55.289
2024-12-01-16:09:22-root-INFO: Loss Change: 3470.672 -> 3421.515
2024-12-01-16:09:22-root-INFO: Regularization Change: 0.000 -> 0.315
2024-12-01-16:09:22-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-16:09:22-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-16:09:22-root-INFO: step: 204 lr_xt 0.00114648
2024-12-01-16:09:22-root-INFO: grad norm: 200.798 187.817 71.027
2024-12-01-16:09:22-root-INFO: grad norm: 208.906 197.772 67.290
2024-12-01-16:09:23-root-INFO: grad norm: 242.535 232.314 69.666
2024-12-01-16:09:23-root-INFO: grad norm: 293.629 280.164 87.897
2024-12-01-16:09:24-root-INFO: Loss too large (3386.355->3388.108)! Learning rate decreased to 0.00092.
2024-12-01-16:09:24-root-INFO: grad norm: 242.251 232.092 69.418
2024-12-01-16:09:24-root-INFO: Loss Change: 3411.845 -> 3358.893
2024-12-01-16:09:24-root-INFO: Regularization Change: 0.000 -> 0.305
2024-12-01-16:09:24-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-16:09:24-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:09:25-root-INFO: step: 203 lr_xt 0.00119917
2024-12-01-16:09:25-root-INFO: grad norm: 189.983 180.228 60.094
2024-12-01-16:09:25-root-INFO: grad norm: 217.601 208.568 62.046
2024-12-01-16:09:26-root-INFO: grad norm: 261.227 250.167 75.205
2024-12-01-16:09:26-root-INFO: grad norm: 312.527 299.673 88.710
2024-12-01-16:09:26-root-INFO: Loss too large (3329.722->3330.571)! Learning rate decreased to 0.00096.
2024-12-01-16:09:27-root-INFO: grad norm: 248.874 238.358 71.582
2024-12-01-16:09:27-root-INFO: Loss Change: 3342.961 -> 3298.505
2024-12-01-16:09:27-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-01-16:09:27-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-01-16:09:27-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:09:27-root-INFO: step: 202 lr_xt 0.00125407
2024-12-01-16:09:28-root-INFO: grad norm: 249.035 231.304 92.286
2024-12-01-16:09:28-root-INFO: grad norm: 275.886 263.022 83.263
2024-12-01-16:09:28-root-INFO: Loss too large (3272.592->3274.527)! Learning rate decreased to 0.00100.
2024-12-01-16:09:29-root-INFO: grad norm: 244.923 234.724 69.940
2024-12-01-16:09:29-root-INFO: grad norm: 220.725 210.694 65.785
2024-12-01-16:09:30-root-INFO: grad norm: 203.837 195.538 57.571
2024-12-01-16:09:30-root-INFO: Loss Change: 3288.708 -> 3225.660
2024-12-01-16:09:30-root-INFO: Regularization Change: 0.000 -> 0.329
2024-12-01-16:09:30-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-01-16:09:30-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:09:30-root-INFO: step: 201 lr_xt 0.00131127
2024-12-01-16:09:30-root-INFO: grad norm: 224.674 213.351 70.426
2024-12-01-16:09:31-root-INFO: grad norm: 286.906 276.187 77.689
2024-12-01-16:09:31-root-INFO: Loss too large (3225.418->3230.987)! Learning rate decreased to 0.00105.
2024-12-01-16:09:31-root-INFO: grad norm: 256.857 245.656 75.025
2024-12-01-16:09:32-root-INFO: grad norm: 235.222 226.239 64.384
2024-12-01-16:09:32-root-INFO: grad norm: 217.273 207.780 63.520
2024-12-01-16:09:33-root-INFO: Loss Change: 3227.002 -> 3179.836
2024-12-01-16:09:33-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-01-16:09:33-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-01-16:09:33-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:09:33-root-INFO: step: 200 lr_xt 0.00137086
2024-12-01-16:09:33-root-INFO: grad norm: 266.130 251.868 85.953
2024-12-01-16:09:33-root-INFO: Loss too large (3174.967->3175.461)! Learning rate decreased to 0.00110.
2024-12-01-16:09:34-root-INFO: grad norm: 247.448 237.163 70.597
2024-12-01-16:09:34-root-INFO: grad norm: 244.988 235.212 68.518
2024-12-01-16:09:35-root-INFO: grad norm: 247.638 237.494 70.154
2024-12-01-16:09:35-root-INFO: grad norm: 254.090 244.168 70.308
2024-12-01-16:09:35-root-INFO: Loss Change: 3174.967 -> 3124.895
2024-12-01-16:09:35-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-01-16:09:35-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-16:09:35-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:09:36-root-INFO: step: 199 lr_xt 0.00143293
2024-12-01-16:09:36-root-INFO: grad norm: 256.134 246.183 70.699
2024-12-01-16:09:36-root-INFO: Loss too large (3106.415->3113.175)! Learning rate decreased to 0.00115.
2024-12-01-16:09:37-root-INFO: grad norm: 257.972 248.234 70.211
2024-12-01-16:09:37-root-INFO: grad norm: 270.789 260.101 75.326
2024-12-01-16:09:38-root-INFO: grad norm: 288.054 277.472 77.359
2024-12-01-16:09:38-root-INFO: grad norm: 310.448 298.579 85.019
2024-12-01-16:09:38-root-INFO: Loss Change: 3106.415 -> 3071.475
2024-12-01-16:09:38-root-INFO: Regularization Change: 0.000 -> 0.370
2024-12-01-16:09:38-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-01-16:09:38-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-16:09:39-root-INFO: step: 198 lr_xt 0.00149757
2024-12-01-16:09:39-root-INFO: grad norm: 350.113 335.651 99.586
2024-12-01-16:09:39-root-INFO: Loss too large (3069.747->3104.333)! Learning rate decreased to 0.00120.
2024-12-01-16:09:39-root-INFO: grad norm: 380.672 366.089 104.354
2024-12-01-16:09:40-root-INFO: Loss too large (3061.657->3062.744)! Learning rate decreased to 0.00096.
2024-12-01-16:09:40-root-INFO: grad norm: 277.561 267.386 74.465
2024-12-01-16:09:40-root-INFO: grad norm: 207.820 199.035 59.786
2024-12-01-16:09:41-root-INFO: grad norm: 167.032 160.517 46.195
2024-12-01-16:09:41-root-INFO: Loss Change: 3069.747 -> 2994.178
2024-12-01-16:09:41-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-01-16:09:41-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-01-16:09:41-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:09:41-root-INFO: step: 197 lr_xt 0.00156486
2024-12-01-16:09:42-root-INFO: grad norm: 118.224 112.484 36.392
2024-12-01-16:09:42-root-INFO: grad norm: 141.931 135.998 40.607
2024-12-01-16:09:43-root-INFO: grad norm: 225.796 217.983 58.880
2024-12-01-16:09:43-root-INFO: Loss too large (2957.293->2976.019)! Learning rate decreased to 0.00125.
2024-12-01-16:09:43-root-INFO: grad norm: 293.204 282.874 77.142
2024-12-01-16:09:43-root-INFO: Loss too large (2956.776->2963.708)! Learning rate decreased to 0.00100.
2024-12-01-16:09:44-root-INFO: grad norm: 265.282 256.482 67.759
2024-12-01-16:09:44-root-INFO: Loss Change: 2976.041 -> 2933.120
2024-12-01-16:09:44-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-01-16:09:44-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-01-16:09:44-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:09:44-root-INFO: step: 196 lr_xt 0.00163492
2024-12-01-16:09:44-root-INFO: grad norm: 262.906 252.034 74.824
2024-12-01-16:09:45-root-INFO: Loss too large (2925.749->2955.573)! Learning rate decreased to 0.00131.
2024-12-01-16:09:45-root-INFO: Loss too large (2925.749->2926.159)! Learning rate decreased to 0.00105.
2024-12-01-16:09:45-root-INFO: grad norm: 244.800 235.926 65.314
2024-12-01-16:09:46-root-INFO: grad norm: 239.387 231.067 62.563
2024-12-01-16:09:46-root-INFO: grad norm: 240.205 231.653 63.525
2024-12-01-16:09:47-root-INFO: grad norm: 246.125 237.673 63.944
2024-12-01-16:09:47-root-INFO: Loss Change: 2925.749 -> 2874.483
2024-12-01-16:09:47-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-01-16:09:47-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-01-16:09:47-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:09:47-root-INFO: step: 195 lr_xt 0.00170783
2024-12-01-16:09:47-root-INFO: grad norm: 247.165 237.890 67.073
2024-12-01-16:09:48-root-INFO: Loss too large (2863.591->2896.629)! Learning rate decreased to 0.00137.
2024-12-01-16:09:48-root-INFO: Loss too large (2863.591->2868.491)! Learning rate decreased to 0.00109.
2024-12-01-16:09:48-root-INFO: grad norm: 248.089 239.603 64.331
2024-12-01-16:09:49-root-INFO: grad norm: 256.168 247.677 65.406
2024-12-01-16:09:49-root-INFO: grad norm: 269.679 260.513 69.711
2024-12-01-16:09:50-root-INFO: grad norm: 289.452 280.355 71.998
2024-12-01-16:09:50-root-INFO: Loss Change: 2863.591 -> 2824.125
2024-12-01-16:09:50-root-INFO: Regularization Change: 0.000 -> 0.344
2024-12-01-16:09:50-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-01-16:09:50-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:09:50-root-INFO: step: 194 lr_xt 0.00178371
2024-12-01-16:09:50-root-INFO: grad norm: 326.576 315.084 85.873
2024-12-01-16:09:50-root-INFO: Loss too large (2816.089->2905.349)! Learning rate decreased to 0.00143.
2024-12-01-16:09:51-root-INFO: Loss too large (2816.089->2842.468)! Learning rate decreased to 0.00114.
2024-12-01-16:09:51-root-INFO: grad norm: 356.941 346.099 87.308
2024-12-01-16:09:52-root-INFO: grad norm: 395.152 382.577 98.892
2024-12-01-16:09:52-root-INFO: grad norm: 438.831 426.005 105.319
2024-12-01-16:09:52-root-INFO: Loss too large (2804.775->2808.569)! Learning rate decreased to 0.00091.
2024-12-01-16:09:53-root-INFO: grad norm: 315.496 305.040 80.549
2024-12-01-16:09:53-root-INFO: Loss Change: 2816.089 -> 2751.448
2024-12-01-16:09:53-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-01-16:09:53-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-01-16:09:53-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:09:53-root-INFO: step: 193 lr_xt 0.00186266
2024-12-01-16:09:53-root-INFO: grad norm: 200.938 194.845 49.111
2024-12-01-16:09:54-root-INFO: Loss too large (2733.478->2748.474)! Learning rate decreased to 0.00149.
2024-12-01-16:09:54-root-INFO: grad norm: 301.339 291.717 75.540
2024-12-01-16:09:54-root-INFO: Loss too large (2730.889->2762.115)! Learning rate decreased to 0.00119.
2024-12-01-16:09:55-root-INFO: grad norm: 359.793 349.656 84.804
2024-12-01-16:09:55-root-INFO: Loss too large (2728.775->2734.889)! Learning rate decreased to 0.00095.
2024-12-01-16:09:55-root-INFO: grad norm: 289.512 280.138 73.077
2024-12-01-16:09:56-root-INFO: grad norm: 238.257 231.362 56.907
2024-12-01-16:09:56-root-INFO: Loss Change: 2733.478 -> 2679.061
2024-12-01-16:09:56-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-01-16:09:56-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-01-16:09:56-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-16:09:56-root-INFO: step: 192 lr_xt 0.00194479
2024-12-01-16:09:57-root-INFO: grad norm: 243.988 235.550 63.610
2024-12-01-16:09:57-root-INFO: Loss too large (2664.815->2718.758)! Learning rate decreased to 0.00156.
2024-12-01-16:09:57-root-INFO: Loss too large (2664.815->2680.248)! Learning rate decreased to 0.00124.
2024-12-01-16:09:57-root-INFO: grad norm: 298.019 290.540 66.348
2024-12-01-16:09:58-root-INFO: Loss too large (2659.326->2663.001)! Learning rate decreased to 0.00100.
2024-12-01-16:09:58-root-INFO: grad norm: 256.647 248.243 65.140
2024-12-01-16:09:58-root-INFO: grad norm: 226.725 220.918 50.985
2024-12-01-16:09:59-root-INFO: grad norm: 208.272 201.061 54.327
2024-12-01-16:09:59-root-INFO: Loss Change: 2664.815 -> 2608.574
2024-12-01-16:09:59-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-01-16:09:59-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-01-16:09:59-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-16:09:59-root-INFO: step: 191 lr_xt 0.00203021
2024-12-01-16:10:00-root-INFO: grad norm: 190.249 183.918 48.672
2024-12-01-16:10:00-root-INFO: Loss too large (2592.784->2609.454)! Learning rate decreased to 0.00162.
2024-12-01-16:10:00-root-INFO: grad norm: 324.415 315.472 75.644
2024-12-01-16:10:00-root-INFO: Loss too large (2591.481->2665.615)! Learning rate decreased to 0.00130.
2024-12-01-16:10:01-root-INFO: Loss too large (2591.481->2611.605)! Learning rate decreased to 0.00104.
2024-12-01-16:10:01-root-INFO: grad norm: 334.212 325.610 75.340
2024-12-01-16:10:02-root-INFO: grad norm: 347.682 338.211 80.595
2024-12-01-16:10:02-root-INFO: grad norm: 364.670 355.791 79.979
2024-12-01-16:10:02-root-INFO: Loss Change: 2592.784 -> 2565.084
2024-12-01-16:10:02-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-01-16:10:02-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-01-16:10:02-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-16:10:03-root-INFO: step: 190 lr_xt 0.00211904
2024-12-01-16:10:03-root-INFO: grad norm: 422.820 411.607 96.729
2024-12-01-16:10:03-root-INFO: Loss too large (2565.628->2860.388)! Learning rate decreased to 0.00170.
2024-12-01-16:10:03-root-INFO: Loss too large (2565.628->2704.854)! Learning rate decreased to 0.00136.
2024-12-01-16:10:03-root-INFO: Loss too large (2565.628->2608.099)! Learning rate decreased to 0.00108.
2024-12-01-16:10:04-root-INFO: grad norm: 428.888 419.314 90.115
2024-12-01-16:10:04-root-INFO: grad norm: 435.993 425.166 96.557
2024-12-01-16:10:05-root-INFO: grad norm: 439.239 429.586 91.580
2024-12-01-16:10:05-root-INFO: grad norm: 439.029 428.120 97.263
2024-12-01-16:10:06-root-INFO: Loss Change: 2565.628 -> 2521.403
2024-12-01-16:10:06-root-INFO: Regularization Change: 0.000 -> 0.455
2024-12-01-16:10:06-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-01-16:10:06-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-16:10:06-root-INFO: step: 189 lr_xt 0.00221139
2024-12-01-16:10:06-root-INFO: grad norm: 417.204 408.355 85.471
2024-12-01-16:10:06-root-INFO: Loss too large (2504.756->2791.331)! Learning rate decreased to 0.00177.
2024-12-01-16:10:06-root-INFO: Loss too large (2504.756->2644.214)! Learning rate decreased to 0.00142.
2024-12-01-16:10:07-root-INFO: Loss too large (2504.756->2551.351)! Learning rate decreased to 0.00113.
2024-12-01-16:10:07-root-INFO: grad norm: 415.558 405.927 88.948
2024-12-01-16:10:08-root-INFO: grad norm: 416.876 408.169 84.754
2024-12-01-16:10:08-root-INFO: grad norm: 421.997 412.318 89.865
2024-12-01-16:10:09-root-INFO: grad norm: 427.158 418.470 85.713
2024-12-01-16:10:09-root-INFO: Loss Change: 2504.756 -> 2467.115
2024-12-01-16:10:09-root-INFO: Regularization Change: 0.000 -> 0.468
2024-12-01-16:10:09-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-16:10:09-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-16:10:09-root-INFO: step: 188 lr_xt 0.00230740
2024-12-01-16:10:09-root-INFO: grad norm: 491.717 480.869 102.714
2024-12-01-16:10:09-root-INFO: Loss too large (2475.155->2865.698)! Learning rate decreased to 0.00185.
2024-12-01-16:10:09-root-INFO: Loss too large (2475.155->2663.205)! Learning rate decreased to 0.00148.
2024-12-01-16:10:10-root-INFO: Loss too large (2475.155->2530.770)! Learning rate decreased to 0.00118.
2024-12-01-16:10:10-root-INFO: grad norm: 466.730 458.343 88.080
2024-12-01-16:10:11-root-INFO: grad norm: 448.303 438.827 91.688
2024-12-01-16:10:11-root-INFO: grad norm: 427.825 420.066 81.110
2024-12-01-16:10:12-root-INFO: grad norm: 413.748 404.982 84.713
2024-12-01-16:10:12-root-INFO: Loss Change: 2475.155 -> 2404.974
2024-12-01-16:10:12-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-01-16:10:12-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-16:10:12-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-16:10:12-root-INFO: step: 187 lr_xt 0.00240719
2024-12-01-16:10:12-root-INFO: grad norm: 410.804 402.985 79.765
2024-12-01-16:10:12-root-INFO: Loss too large (2404.641->2709.236)! Learning rate decreased to 0.00193.
2024-12-01-16:10:13-root-INFO: Loss too large (2404.641->2553.018)! Learning rate decreased to 0.00154.
2024-12-01-16:10:13-root-INFO: Loss too large (2404.641->2454.177)! Learning rate decreased to 0.00123.
2024-12-01-16:10:13-root-INFO: grad norm: 410.069 402.149 80.204
2024-12-01-16:10:14-root-INFO: grad norm: 415.603 407.919 79.546
2024-12-01-16:10:14-root-INFO: grad norm: 427.108 419.100 82.318
2024-12-01-16:10:15-root-INFO: grad norm: 439.659 431.734 83.100
2024-12-01-16:10:15-root-INFO: Loss Change: 2404.641 -> 2375.643
2024-12-01-16:10:15-root-INFO: Regularization Change: 0.000 -> 0.521
2024-12-01-16:10:15-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-16:10:15-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:10:15-root-INFO: step: 186 lr_xt 0.00251089
2024-12-01-16:10:15-root-INFO: grad norm: 498.036 488.868 95.117
2024-12-01-16:10:16-root-INFO: Loss too large (2381.252->2811.584)! Learning rate decreased to 0.00201.
2024-12-01-16:10:16-root-INFO: Loss too large (2381.252->2596.872)! Learning rate decreased to 0.00161.
2024-12-01-16:10:16-root-INFO: Loss too large (2381.252->2451.481)! Learning rate decreased to 0.00129.
2024-12-01-16:10:16-root-INFO: grad norm: 480.748 472.906 86.481
2024-12-01-16:10:17-root-INFO: grad norm: 464.788 456.432 87.738
2024-12-01-16:10:17-root-INFO: grad norm: 444.458 437.180 80.104
2024-12-01-16:10:18-root-INFO: grad norm: 428.408 420.660 81.110
2024-12-01-16:10:18-root-INFO: Loss Change: 2381.252 -> 2316.552
2024-12-01-16:10:18-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-01-16:10:18-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-16:10:18-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:10:18-root-INFO: step: 185 lr_xt 0.00261863
2024-12-01-16:10:19-root-INFO: grad norm: 397.334 390.985 70.751
2024-12-01-16:10:19-root-INFO: Loss too large (2300.043->2610.500)! Learning rate decreased to 0.00209.
2024-12-01-16:10:19-root-INFO: Loss too large (2300.043->2450.885)! Learning rate decreased to 0.00168.
2024-12-01-16:10:19-root-INFO: Loss too large (2300.043->2349.834)! Learning rate decreased to 0.00134.
2024-12-01-16:10:20-root-INFO: grad norm: 390.559 384.166 70.377
2024-12-01-16:10:20-root-INFO: grad norm: 391.916 385.712 69.455
2024-12-01-16:10:21-root-INFO: grad norm: 402.116 395.576 72.230
2024-12-01-16:10:21-root-INFO: grad norm: 416.725 410.227 73.303
2024-12-01-16:10:21-root-INFO: Loss Change: 2300.043 -> 2272.357
2024-12-01-16:10:21-root-INFO: Regularization Change: 0.000 -> 0.537
2024-12-01-16:10:21-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-01-16:10:21-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:10:22-root-INFO: step: 184 lr_xt 0.00273055
2024-12-01-16:10:22-root-INFO: grad norm: 504.217 495.877 91.330
2024-12-01-16:10:22-root-INFO: Loss too large (2291.273->2763.315)! Learning rate decreased to 0.00218.
2024-12-01-16:10:22-root-INFO: Loss too large (2291.273->2539.201)! Learning rate decreased to 0.00175.
2024-12-01-16:10:22-root-INFO: Loss too large (2291.273->2380.943)! Learning rate decreased to 0.00140.
2024-12-01-16:10:23-root-INFO: grad norm: 507.137 499.726 86.386
2024-12-01-16:10:23-root-INFO: grad norm: 504.560 496.923 87.454
2024-12-01-16:10:24-root-INFO: grad norm: 485.651 478.447 83.340
2024-12-01-16:10:24-root-INFO: grad norm: 461.985 454.958 80.269
2024-12-01-16:10:25-root-INFO: Loss Change: 2291.273 -> 2235.725
2024-12-01-16:10:25-root-INFO: Regularization Change: 0.000 -> 0.705
2024-12-01-16:10:25-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-01-16:10:25-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:10:25-root-INFO: step: 183 lr_xt 0.00284680
2024-12-01-16:10:25-root-INFO: grad norm: 396.025 390.693 64.765
2024-12-01-16:10:25-root-INFO: Loss too large (2213.086->2539.788)! Learning rate decreased to 0.00228.
2024-12-01-16:10:25-root-INFO: Loss too large (2213.086->2370.424)! Learning rate decreased to 0.00182.
2024-12-01-16:10:26-root-INFO: Loss too large (2213.086->2262.847)! Learning rate decreased to 0.00146.
2024-12-01-16:10:26-root-INFO: grad norm: 384.075 378.295 66.379
2024-12-01-16:10:27-root-INFO: grad norm: 384.009 378.535 64.607
2024-12-01-16:10:27-root-INFO: grad norm: 394.595 388.949 66.513
2024-12-01-16:10:28-root-INFO: grad norm: 410.434 404.573 69.110
2024-12-01-16:10:28-root-INFO: Loss Change: 2213.086 -> 2182.481
2024-12-01-16:10:28-root-INFO: Regularization Change: 0.000 -> 0.611
2024-12-01-16:10:28-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-01-16:10:28-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:10:28-root-INFO: step: 182 lr_xt 0.00296752
2024-12-01-16:10:28-root-INFO: grad norm: 464.065 457.957 75.048
2024-12-01-16:10:28-root-INFO: Loss too large (2190.888->2623.938)! Learning rate decreased to 0.00237.
2024-12-01-16:10:29-root-INFO: Loss too large (2190.888->2418.383)! Learning rate decreased to 0.00190.
2024-12-01-16:10:29-root-INFO: Loss too large (2190.888->2273.143)! Learning rate decreased to 0.00152.
2024-12-01-16:10:29-root-INFO: grad norm: 457.760 451.735 74.026
2024-12-01-16:10:30-root-INFO: grad norm: 450.652 444.844 72.116
2024-12-01-16:10:30-root-INFO: grad norm: 440.667 434.789 71.729
2024-12-01-16:10:31-root-INFO: grad norm: 431.346 425.747 69.276
2024-12-01-16:10:31-root-INFO: Loss Change: 2190.888 -> 2145.220
2024-12-01-16:10:31-root-INFO: Regularization Change: 0.000 -> 0.701
2024-12-01-16:10:31-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-01-16:10:31-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-16:10:31-root-INFO: step: 181 lr_xt 0.00309285
2024-12-01-16:10:31-root-INFO: grad norm: 396.498 391.121 65.082
2024-12-01-16:10:31-root-INFO: Loss too large (2127.665->2494.314)! Learning rate decreased to 0.00247.
2024-12-01-16:10:32-root-INFO: Loss too large (2127.665->2310.200)! Learning rate decreased to 0.00198.
2024-12-01-16:10:32-root-INFO: Loss too large (2127.665->2191.207)! Learning rate decreased to 0.00158.
2024-12-01-16:10:32-root-INFO: grad norm: 396.533 391.709 61.665
2024-12-01-16:10:33-root-INFO: grad norm: 402.891 397.474 65.843
2024-12-01-16:10:33-root-INFO: grad norm: 414.256 409.291 63.946
2024-12-01-16:10:34-root-INFO: grad norm: 425.244 419.605 69.018
2024-12-01-16:10:34-root-INFO: Loss Change: 2127.665 -> 2106.849
2024-12-01-16:10:34-root-INFO: Regularization Change: 0.000 -> 0.676
2024-12-01-16:10:34-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-01-16:10:34-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-16:10:34-root-INFO: step: 180 lr_xt 0.00322295
2024-12-01-16:10:34-root-INFO: grad norm: 479.644 473.674 75.438
2024-12-01-16:10:34-root-INFO: Loss too large (2123.893->2587.320)! Learning rate decreased to 0.00258.
2024-12-01-16:10:35-root-INFO: Loss too large (2123.893->2371.291)! Learning rate decreased to 0.00206.
2024-12-01-16:10:35-root-INFO: Loss too large (2123.893->2212.723)! Learning rate decreased to 0.00165.
2024-12-01-16:10:35-root-INFO: grad norm: 463.717 457.977 72.733
2024-12-01-16:10:36-root-INFO: grad norm: 448.517 443.248 68.547
2024-12-01-16:10:36-root-INFO: grad norm: 432.916 427.383 68.989
2024-12-01-16:10:37-root-INFO: grad norm: 419.417 414.483 64.146
2024-12-01-16:10:37-root-INFO: Loss Change: 2123.893 -> 2063.883
2024-12-01-16:10:37-root-INFO: Regularization Change: 0.000 -> 0.814
2024-12-01-16:10:37-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-01-16:10:37-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-16:10:37-root-INFO: step: 179 lr_xt 0.00335799
2024-12-01-16:10:37-root-INFO: grad norm: 379.020 374.369 59.198
2024-12-01-16:10:38-root-INFO: Loss too large (2043.131->2411.177)! Learning rate decreased to 0.00269.
2024-12-01-16:10:38-root-INFO: Loss too large (2043.131->2228.678)! Learning rate decreased to 0.00215.
2024-12-01-16:10:38-root-INFO: Loss too large (2043.131->2110.174)! Learning rate decreased to 0.00172.
2024-12-01-16:10:38-root-INFO: grad norm: 383.258 378.975 57.135
2024-12-01-16:10:39-root-INFO: grad norm: 392.581 387.568 62.541
2024-12-01-16:10:39-root-INFO: Loss too large (2032.726->2032.964)! Learning rate decreased to 0.00138.
2024-12-01-16:10:40-root-INFO: grad norm: 264.401 261.238 40.776
2024-12-01-16:10:40-root-INFO: grad norm: 195.565 192.106 36.620
2024-12-01-16:10:40-root-INFO: Loss Change: 2043.131 -> 1958.646
2024-12-01-16:10:40-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-01-16:10:40-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-01-16:10:40-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-16:10:40-root-INFO: step: 178 lr_xt 0.00349812
2024-12-01-16:10:41-root-INFO: grad norm: 219.585 216.289 37.904
2024-12-01-16:10:41-root-INFO: Loss too large (1959.264->2108.764)! Learning rate decreased to 0.00280.
2024-12-01-16:10:41-root-INFO: Loss too large (1959.264->2032.379)! Learning rate decreased to 0.00224.
2024-12-01-16:10:41-root-INFO: Loss too large (1959.264->1985.362)! Learning rate decreased to 0.00179.
2024-12-01-16:10:42-root-INFO: grad norm: 262.993 259.263 44.140
2024-12-01-16:10:42-root-INFO: Loss too large (1958.576->1967.056)! Learning rate decreased to 0.00143.
2024-12-01-16:10:42-root-INFO: grad norm: 217.989 215.392 33.551
2024-12-01-16:10:43-root-INFO: grad norm: 187.583 184.118 35.891
2024-12-01-16:10:43-root-INFO: grad norm: 168.027 165.841 27.016
2024-12-01-16:10:44-root-INFO: Loss Change: 1959.264 -> 1918.235
2024-12-01-16:10:44-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-01-16:10:44-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-01-16:10:44-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-16:10:44-root-INFO: step: 177 lr_xt 0.00364350
2024-12-01-16:10:44-root-INFO: grad norm: 119.898 116.478 28.432
2024-12-01-16:10:44-root-INFO: Loss too large (1904.904->1939.684)! Learning rate decreased to 0.00291.
2024-12-01-16:10:44-root-INFO: Loss too large (1904.904->1918.966)! Learning rate decreased to 0.00233.
2024-12-01-16:10:44-root-INFO: Loss too large (1904.904->1907.235)! Learning rate decreased to 0.00187.
2024-12-01-16:10:45-root-INFO: grad norm: 150.940 148.928 24.565
2024-12-01-16:10:45-root-INFO: Loss too large (1901.025->1901.665)! Learning rate decreased to 0.00149.
2024-12-01-16:10:46-root-INFO: grad norm: 142.499 139.162 30.662
2024-12-01-16:10:46-root-INFO: grad norm: 137.993 136.104 22.752
2024-12-01-16:10:47-root-INFO: grad norm: 135.564 132.279 29.661
2024-12-01-16:10:47-root-INFO: Loss Change: 1904.904 -> 1879.040
2024-12-01-16:10:47-root-INFO: Regularization Change: 0.000 -> 0.243
2024-12-01-16:10:47-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-01-16:10:47-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-16:10:47-root-INFO: step: 176 lr_xt 0.00379432
2024-12-01-16:10:47-root-INFO: grad norm: 197.905 195.168 32.806
2024-12-01-16:10:47-root-INFO: Loss too large (1880.859->2029.736)! Learning rate decreased to 0.00304.
2024-12-01-16:10:48-root-INFO: Loss too large (1880.859->1958.905)! Learning rate decreased to 0.00243.
2024-12-01-16:10:48-root-INFO: Loss too large (1880.859->1913.997)! Learning rate decreased to 0.00194.
2024-12-01-16:10:48-root-INFO: Loss too large (1880.859->1887.523)! Learning rate decreased to 0.00155.
2024-12-01-16:10:48-root-INFO: grad norm: 184.275 181.016 34.505
2024-12-01-16:10:49-root-INFO: grad norm: 174.635 172.637 26.339
2024-12-01-16:10:49-root-INFO: grad norm: 169.492 166.213 33.173
2024-12-01-16:10:50-root-INFO: grad norm: 165.806 163.977 24.555
2024-12-01-16:10:50-root-INFO: Loss Change: 1880.859 -> 1850.222
2024-12-01-16:10:50-root-INFO: Regularization Change: 0.000 -> 0.269
2024-12-01-16:10:50-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-01-16:10:50-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-16:10:50-root-INFO: step: 175 lr_xt 0.00395074
2024-12-01-16:10:50-root-INFO: grad norm: 109.695 106.314 27.027
2024-12-01-16:10:51-root-INFO: Loss too large (1836.782->1865.720)! Learning rate decreased to 0.00316.
2024-12-01-16:10:51-root-INFO: Loss too large (1836.782->1847.907)! Learning rate decreased to 0.00253.
2024-12-01-16:10:51-root-INFO: Loss too large (1836.782->1837.887)! Learning rate decreased to 0.00202.
2024-12-01-16:10:51-root-INFO: grad norm: 135.139 133.464 21.213
2024-12-01-16:10:52-root-INFO: grad norm: 179.009 175.701 34.257
2024-12-01-16:10:52-root-INFO: Loss too large (1831.925->1837.845)! Learning rate decreased to 0.00162.
2024-12-01-16:10:53-root-INFO: grad norm: 166.344 164.669 23.547
2024-12-01-16:10:53-root-INFO: grad norm: 157.220 154.065 31.343
2024-12-01-16:10:54-root-INFO: Loss Change: 1836.782 -> 1814.060
2024-12-01-16:10:54-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-01-16:10:54-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-01-16:10:54-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-16:10:54-root-INFO: step: 174 lr_xt 0.00411294
2024-12-01-16:10:54-root-INFO: grad norm: 166.143 164.457 23.612
2024-12-01-16:10:54-root-INFO: Loss too large (1813.201->1931.208)! Learning rate decreased to 0.00329.
2024-12-01-16:10:54-root-INFO: Loss too large (1813.201->1872.784)! Learning rate decreased to 0.00263.
2024-12-01-16:10:54-root-INFO: Loss too large (1813.201->1836.999)! Learning rate decreased to 0.00211.
2024-12-01-16:10:55-root-INFO: Loss too large (1813.201->1816.565)! Learning rate decreased to 0.00168.
2024-12-01-16:10:55-root-INFO: grad norm: 154.260 151.181 30.666
2024-12-01-16:10:56-root-INFO: grad norm: 147.474 145.831 21.949
2024-12-01-16:10:56-root-INFO: grad norm: 141.785 138.792 28.980
2024-12-01-16:10:57-root-INFO: grad norm: 139.177 137.514 21.454
2024-12-01-16:10:57-root-INFO: Loss Change: 1813.201 -> 1784.788
2024-12-01-16:10:57-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-01-16:10:57-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-01-16:10:57-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-16:10:57-root-INFO: step: 173 lr_xt 0.00428111
2024-12-01-16:10:57-root-INFO: grad norm: 81.662 78.018 24.121
2024-12-01-16:10:58-root-INFO: grad norm: 177.779 176.140 24.086
2024-12-01-16:10:58-root-INFO: Loss too large (1774.452->1901.427)! Learning rate decreased to 0.00342.
2024-12-01-16:10:58-root-INFO: Loss too large (1774.452->1837.417)! Learning rate decreased to 0.00274.
2024-12-01-16:10:58-root-INFO: Loss too large (1774.452->1798.274)! Learning rate decreased to 0.00219.
2024-12-01-16:10:58-root-INFO: Loss too large (1774.452->1775.941)! Learning rate decreased to 0.00175.
2024-12-01-16:10:59-root-INFO: grad norm: 148.206 145.225 29.578
2024-12-01-16:10:59-root-INFO: grad norm: 125.385 123.945 18.952
2024-12-01-16:11:00-root-INFO: grad norm: 107.002 104.076 24.849
2024-12-01-16:11:00-root-INFO: Loss Change: 1776.882 -> 1742.833
2024-12-01-16:11:00-root-INFO: Regularization Change: 0.000 -> 0.413
2024-12-01-16:11:00-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-01-16:11:00-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-16:11:00-root-INFO: step: 172 lr_xt 0.00445543
2024-12-01-16:11:00-root-INFO: grad norm: 112.630 111.007 19.053
2024-12-01-16:11:01-root-INFO: Loss too large (1737.361->1777.488)! Learning rate decreased to 0.00356.
2024-12-01-16:11:01-root-INFO: Loss too large (1737.361->1754.044)! Learning rate decreased to 0.00285.
2024-12-01-16:11:01-root-INFO: Loss too large (1737.361->1740.690)! Learning rate decreased to 0.00228.
2024-12-01-16:11:01-root-INFO: grad norm: 141.310 138.375 28.649
2024-12-01-16:11:02-root-INFO: Loss too large (1733.599->1734.808)! Learning rate decreased to 0.00182.
2024-12-01-16:11:02-root-INFO: grad norm: 136.042 134.308 21.651
2024-12-01-16:11:03-root-INFO: grad norm: 134.957 132.113 27.556
2024-12-01-16:11:03-root-INFO: grad norm: 149.652 147.649 24.399
2024-12-01-16:11:03-root-INFO: Loss Change: 1737.361 -> 1715.823
2024-12-01-16:11:03-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-01-16:11:03-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-16:11:03-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-16:11:04-root-INFO: step: 171 lr_xt 0.00463611
2024-12-01-16:11:04-root-INFO: grad norm: 121.806 119.335 24.411
2024-12-01-16:11:04-root-INFO: Loss too large (1700.906->1793.724)! Learning rate decreased to 0.00371.
2024-12-01-16:11:04-root-INFO: Loss too large (1700.906->1746.922)! Learning rate decreased to 0.00297.
2024-12-01-16:11:04-root-INFO: Loss too large (1700.906->1718.603)! Learning rate decreased to 0.00237.
2024-12-01-16:11:04-root-INFO: Loss too large (1700.906->1702.975)! Learning rate decreased to 0.00190.
2024-12-01-16:11:05-root-INFO: grad norm: 153.231 150.977 26.186
2024-12-01-16:11:05-root-INFO: Loss too large (1695.223->1697.390)! Learning rate decreased to 0.00152.
2024-12-01-16:11:06-root-INFO: grad norm: 130.389 127.817 25.769
2024-12-01-16:11:06-root-INFO: grad norm: 101.532 99.398 20.705
2024-12-01-16:11:07-root-INFO: grad norm: 94.715 92.202 21.674
2024-12-01-16:11:07-root-INFO: Loss Change: 1700.906 -> 1670.826
2024-12-01-16:11:07-root-INFO: Regularization Change: 0.000 -> 0.226
2024-12-01-16:11:07-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-01-16:11:07-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-16:11:07-root-INFO: step: 170 lr_xt 0.00482333
2024-12-01-16:11:07-root-INFO: grad norm: 167.717 165.115 29.428
2024-12-01-16:11:07-root-INFO: Loss too large (1672.362->1752.285)! Learning rate decreased to 0.00386.
2024-12-01-16:11:08-root-INFO: Loss too large (1672.362->1726.558)! Learning rate decreased to 0.00309.
2024-12-01-16:11:08-root-INFO: Loss too large (1672.362->1704.922)! Learning rate decreased to 0.00247.
2024-12-01-16:11:08-root-INFO: Loss too large (1672.362->1688.026)! Learning rate decreased to 0.00198.
2024-12-01-16:11:08-root-INFO: Loss too large (1672.362->1675.961)! Learning rate decreased to 0.00158.
2024-12-01-16:11:09-root-INFO: grad norm: 147.031 144.525 27.027
2024-12-01-16:11:09-root-INFO: grad norm: 121.702 119.419 23.467
2024-12-01-16:11:10-root-INFO: grad norm: 117.055 114.687 23.427
2024-12-01-16:11:10-root-INFO: grad norm: 111.210 109.039 21.869
2024-12-01-16:11:11-root-INFO: Loss Change: 1672.362 -> 1639.483
2024-12-01-16:11:11-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-01-16:11:11-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-01-16:11:11-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-16:11:11-root-INFO: step: 169 lr_xt 0.00501730
2024-12-01-16:11:11-root-INFO: grad norm: 84.864 82.492 19.925
2024-12-01-16:11:11-root-INFO: Loss too large (1632.947->1652.003)! Learning rate decreased to 0.00401.
2024-12-01-16:11:11-root-INFO: Loss too large (1632.947->1638.796)! Learning rate decreased to 0.00321.
2024-12-01-16:11:12-root-INFO: grad norm: 205.034 202.494 32.174
2024-12-01-16:11:12-root-INFO: Loss too large (1631.547->1683.553)! Learning rate decreased to 0.00257.
2024-12-01-16:11:12-root-INFO: Loss too large (1631.547->1663.015)! Learning rate decreased to 0.00206.
2024-12-01-16:11:12-root-INFO: Loss too large (1631.547->1645.441)! Learning rate decreased to 0.00164.
2024-12-01-16:11:12-root-INFO: Loss too large (1631.547->1632.272)! Learning rate decreased to 0.00132.
2024-12-01-16:11:13-root-INFO: grad norm: 145.270 142.862 26.336
2024-12-01-16:11:13-root-INFO: grad norm: 88.812 86.546 19.936
2024-12-01-16:11:14-root-INFO: grad norm: 77.240 74.900 18.867
2024-12-01-16:11:14-root-INFO: Loss Change: 1632.947 -> 1601.357
2024-12-01-16:11:14-root-INFO: Regularization Change: 0.000 -> 0.266
2024-12-01-16:11:14-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-01-16:11:14-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-16:11:14-root-INFO: step: 168 lr_xt 0.00521823
2024-12-01-16:11:15-root-INFO: grad norm: 101.365 99.213 20.778
2024-12-01-16:11:15-root-INFO: Loss too large (1598.686->1633.987)! Learning rate decreased to 0.00417.
2024-12-01-16:11:15-root-INFO: Loss too large (1598.686->1619.546)! Learning rate decreased to 0.00334.
2024-12-01-16:11:15-root-INFO: Loss too large (1598.686->1608.847)! Learning rate decreased to 0.00267.
2024-12-01-16:11:15-root-INFO: Loss too large (1598.686->1601.583)! Learning rate decreased to 0.00214.
2024-12-01-16:11:16-root-INFO: grad norm: 139.059 136.838 24.755
2024-12-01-16:11:16-root-INFO: Loss too large (1597.083->1599.677)! Learning rate decreased to 0.00171.
2024-12-01-16:11:16-root-INFO: grad norm: 157.733 155.597 25.871
2024-12-01-16:11:16-root-INFO: Loss too large (1590.764->1591.314)! Learning rate decreased to 0.00137.
2024-12-01-16:11:17-root-INFO: grad norm: 126.866 124.777 22.928
2024-12-01-16:11:17-root-INFO: grad norm: 95.000 93.009 19.344
2024-12-01-16:11:18-root-INFO: Loss Change: 1598.686 -> 1572.980
2024-12-01-16:11:18-root-INFO: Regularization Change: 0.000 -> 0.212
2024-12-01-16:11:18-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-01-16:11:18-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-16:11:18-root-INFO: step: 167 lr_xt 0.00542633
2024-12-01-16:11:18-root-INFO: grad norm: 61.518 58.927 17.664
2024-12-01-16:11:18-root-INFO: grad norm: 95.816 93.800 19.551
2024-12-01-16:11:19-root-INFO: Loss too large (1554.903->1592.249)! Learning rate decreased to 0.00434.
2024-12-01-16:11:19-root-INFO: Loss too large (1554.903->1577.077)! Learning rate decreased to 0.00347.
2024-12-01-16:11:19-root-INFO: Loss too large (1554.903->1565.936)! Learning rate decreased to 0.00278.
2024-12-01-16:11:19-root-INFO: Loss too large (1554.903->1558.443)! Learning rate decreased to 0.00222.
2024-12-01-16:11:20-root-INFO: grad norm: 137.080 135.239 22.386
2024-12-01-16:11:20-root-INFO: Loss too large (1553.836->1561.084)! Learning rate decreased to 0.00178.
2024-12-01-16:11:20-root-INFO: grad norm: 172.794 170.640 27.198
2024-12-01-16:11:20-root-INFO: Loss too large (1550.658->1554.879)! Learning rate decreased to 0.00142.
2024-12-01-16:11:21-root-INFO: grad norm: 143.049 141.167 23.132
2024-12-01-16:11:21-root-INFO: Loss Change: 1569.385 -> 1537.618
2024-12-01-16:11:21-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-01-16:11:21-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-01-16:11:21-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-16:11:21-root-INFO: step: 166 lr_xt 0.00564182
2024-12-01-16:11:22-root-INFO: grad norm: 190.632 188.187 30.434
2024-12-01-16:11:22-root-INFO: Loss too large (1540.479->1649.021)! Learning rate decreased to 0.00451.
2024-12-01-16:11:22-root-INFO: Loss too large (1540.479->1628.014)! Learning rate decreased to 0.00361.
2024-12-01-16:11:22-root-INFO: Loss too large (1540.479->1605.090)! Learning rate decreased to 0.00289.
2024-12-01-16:11:22-root-INFO: Loss too large (1540.479->1582.257)! Learning rate decreased to 0.00231.
2024-12-01-16:11:22-root-INFO: Loss too large (1540.479->1562.185)! Learning rate decreased to 0.00185.
2024-12-01-16:11:22-root-INFO: Loss too large (1540.479->1546.749)! Learning rate decreased to 0.00148.
2024-12-01-16:11:23-root-INFO: grad norm: 157.595 155.738 24.126
2024-12-01-16:11:23-root-INFO: grad norm: 120.648 118.615 22.055
2024-12-01-16:11:24-root-INFO: grad norm: 110.676 109.094 18.645
2024-12-01-16:11:24-root-INFO: grad norm: 99.597 97.691 19.392
2024-12-01-16:11:25-root-INFO: Loss Change: 1540.479 -> 1512.644
2024-12-01-16:11:25-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-01-16:11:25-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-01-16:11:25-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-16:11:25-root-INFO: step: 165 lr_xt 0.00586491
2024-12-01-16:11:25-root-INFO: grad norm: 81.235 79.656 15.941
2024-12-01-16:11:25-root-INFO: Loss too large (1510.022->1555.720)! Learning rate decreased to 0.00469.
2024-12-01-16:11:25-root-INFO: Loss too large (1510.022->1534.661)! Learning rate decreased to 0.00375.
2024-12-01-16:11:25-root-INFO: Loss too large (1510.022->1521.247)! Learning rate decreased to 0.00300.
2024-12-01-16:11:26-root-INFO: Loss too large (1510.022->1513.234)! Learning rate decreased to 0.00240.
2024-12-01-16:11:26-root-INFO: grad norm: 146.539 144.486 24.443
2024-12-01-16:11:26-root-INFO: Loss too large (1508.798->1524.178)! Learning rate decreased to 0.00192.
2024-12-01-16:11:26-root-INFO: Loss too large (1508.798->1513.026)! Learning rate decreased to 0.00154.
2024-12-01-16:11:27-root-INFO: grad norm: 138.041 136.404 21.197
2024-12-01-16:11:27-root-INFO: grad norm: 127.237 125.332 21.937
2024-12-01-16:11:28-root-INFO: grad norm: 121.879 120.337 19.330
2024-12-01-16:11:28-root-INFO: Loss Change: 1510.022 -> 1491.426
2024-12-01-16:11:28-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-01-16:11:28-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-01-16:11:28-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-16:11:28-root-INFO: step: 164 lr_xt 0.00609585
2024-12-01-16:11:28-root-INFO: grad norm: 171.597 169.267 28.178
2024-12-01-16:11:29-root-INFO: Loss too large (1497.233->1607.493)! Learning rate decreased to 0.00488.
2024-12-01-16:11:29-root-INFO: Loss too large (1497.233->1585.887)! Learning rate decreased to 0.00390.
2024-12-01-16:11:29-root-INFO: Loss too large (1497.233->1562.078)! Learning rate decreased to 0.00312.
2024-12-01-16:11:29-root-INFO: Loss too large (1497.233->1538.611)! Learning rate decreased to 0.00250.
2024-12-01-16:11:29-root-INFO: Loss too large (1497.233->1518.466)! Learning rate decreased to 0.00200.
2024-12-01-16:11:29-root-INFO: Loss too large (1497.233->1503.403)! Learning rate decreased to 0.00160.
2024-12-01-16:11:30-root-INFO: grad norm: 156.533 154.637 24.288
2024-12-01-16:11:30-root-INFO: grad norm: 138.762 136.725 23.689
2024-12-01-16:11:31-root-INFO: grad norm: 131.009 129.370 20.656
2024-12-01-16:11:31-root-INFO: grad norm: 121.548 119.659 21.345
2024-12-01-16:11:32-root-INFO: Loss Change: 1497.233 -> 1471.742
2024-12-01-16:11:32-root-INFO: Regularization Change: 0.000 -> 0.212
2024-12-01-16:11:32-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-01-16:11:32-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-16:11:32-root-INFO: step: 163 lr_xt 0.00633485
2024-12-01-16:11:32-root-INFO: grad norm: 82.056 80.568 15.556
2024-12-01-16:11:32-root-INFO: Loss too large (1463.451->1515.501)! Learning rate decreased to 0.00507.
2024-12-01-16:11:32-root-INFO: Loss too large (1463.451->1492.726)! Learning rate decreased to 0.00405.
2024-12-01-16:11:32-root-INFO: Loss too large (1463.451->1477.776)! Learning rate decreased to 0.00324.
2024-12-01-16:11:32-root-INFO: Loss too large (1463.451->1468.543)! Learning rate decreased to 0.00259.
2024-12-01-16:11:33-root-INFO: grad norm: 149.845 147.952 23.744
2024-12-01-16:11:33-root-INFO: Loss too large (1463.235->1480.533)! Learning rate decreased to 0.00208.
2024-12-01-16:11:33-root-INFO: Loss too large (1463.235->1467.703)! Learning rate decreased to 0.00166.
2024-12-01-16:11:34-root-INFO: grad norm: 139.990 138.443 20.750
2024-12-01-16:11:34-root-INFO: grad norm: 128.238 126.472 21.213
2024-12-01-16:11:35-root-INFO: grad norm: 121.308 119.866 18.649
2024-12-01-16:11:35-root-INFO: Loss Change: 1463.451 -> 1444.067
2024-12-01-16:11:35-root-INFO: Regularization Change: 0.000 -> 0.223
2024-12-01-16:11:35-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-01-16:11:35-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-16:11:35-root-INFO: step: 162 lr_xt 0.00658217
2024-12-01-16:11:35-root-INFO: grad norm: 166.884 164.354 28.951
2024-12-01-16:11:36-root-INFO: Loss too large (1456.600->1568.844)! Learning rate decreased to 0.00527.
2024-12-01-16:11:36-root-INFO: Loss too large (1456.600->1546.497)! Learning rate decreased to 0.00421.
2024-12-01-16:11:36-root-INFO: Loss too large (1456.600->1521.686)! Learning rate decreased to 0.00337.
2024-12-01-16:11:36-root-INFO: Loss too large (1456.600->1497.174)! Learning rate decreased to 0.00270.
2024-12-01-16:11:36-root-INFO: Loss too large (1456.600->1476.272)! Learning rate decreased to 0.00216.
2024-12-01-16:11:36-root-INFO: Loss too large (1456.600->1460.881)! Learning rate decreased to 0.00173.
2024-12-01-16:11:37-root-INFO: grad norm: 153.070 151.309 23.156
2024-12-01-16:11:37-root-INFO: grad norm: 137.733 135.568 24.326
2024-12-01-16:11:38-root-INFO: grad norm: 128.927 127.360 20.037
2024-12-01-16:11:38-root-INFO: grad norm: 118.632 116.667 21.502
2024-12-01-16:11:39-root-INFO: Loss Change: 1456.600 -> 1426.103
2024-12-01-16:11:39-root-INFO: Regularization Change: 0.000 -> 0.265
2024-12-01-16:11:39-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-01-16:11:39-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-16:11:39-root-INFO: step: 161 lr_xt 0.00683803
2024-12-01-16:11:39-root-INFO: grad norm: 83.204 81.584 16.339
2024-12-01-16:11:39-root-INFO: Loss too large (1420.294->1462.398)! Learning rate decreased to 0.00547.
2024-12-01-16:11:39-root-INFO: Loss too large (1420.294->1443.135)! Learning rate decreased to 0.00438.
2024-12-01-16:11:39-root-INFO: Loss too large (1420.294->1430.448)! Learning rate decreased to 0.00350.
2024-12-01-16:11:39-root-INFO: Loss too large (1420.294->1422.598)! Learning rate decreased to 0.00280.
2024-12-01-16:11:40-root-INFO: grad norm: 137.065 135.250 22.229
2024-12-01-16:11:40-root-INFO: Loss too large (1418.123->1428.509)! Learning rate decreased to 0.00224.
2024-12-01-16:11:41-root-INFO: grad norm: 164.145 162.412 23.793
2024-12-01-16:11:41-root-INFO: grad norm: 184.136 182.208 26.578
2024-12-01-16:11:42-root-INFO: grad norm: 188.637 186.703 26.941
2024-12-01-16:11:42-root-INFO: Loss Change: 1420.294 -> 1400.798
2024-12-01-16:11:42-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-01-16:11:42-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-01-16:11:42-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-16:11:42-root-INFO: step: 160 lr_xt 0.00710269
2024-12-01-16:11:42-root-INFO: grad norm: 210.119 207.874 30.634
2024-12-01-16:11:42-root-INFO: Loss too large (1411.222->1540.114)! Learning rate decreased to 0.00568.
2024-12-01-16:11:43-root-INFO: Loss too large (1411.222->1511.954)! Learning rate decreased to 0.00455.
2024-12-01-16:11:43-root-INFO: Loss too large (1411.222->1478.951)! Learning rate decreased to 0.00364.
2024-12-01-16:11:43-root-INFO: Loss too large (1411.222->1444.721)! Learning rate decreased to 0.00291.
2024-12-01-16:11:43-root-INFO: Loss too large (1411.222->1415.303)! Learning rate decreased to 0.00233.
2024-12-01-16:11:43-root-INFO: grad norm: 191.119 189.110 27.638
2024-12-01-16:11:44-root-INFO: grad norm: 170.004 167.995 26.059
2024-12-01-16:11:44-root-INFO: grad norm: 154.880 152.775 25.451
2024-12-01-16:11:45-root-INFO: grad norm: 137.236 135.369 22.562
2024-12-01-16:11:45-root-INFO: Loss Change: 1411.222 -> 1327.926
2024-12-01-16:11:45-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-01-16:11:45-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-01-16:11:45-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-16:11:45-root-INFO: step: 159 lr_xt 0.00737641
2024-12-01-16:11:46-root-INFO: grad norm: 95.341 92.892 21.470
2024-12-01-16:11:46-root-INFO: Loss too large (1322.637->1364.367)! Learning rate decreased to 0.00590.
2024-12-01-16:11:46-root-INFO: Loss too large (1322.637->1342.109)! Learning rate decreased to 0.00472.
2024-12-01-16:11:46-root-INFO: Loss too large (1322.637->1328.037)! Learning rate decreased to 0.00378.
2024-12-01-16:11:46-root-INFO: grad norm: 149.998 148.535 20.900
2024-12-01-16:11:47-root-INFO: Loss too large (1319.790->1345.005)! Learning rate decreased to 0.00302.
2024-12-01-16:11:47-root-INFO: Loss too large (1319.790->1322.032)! Learning rate decreased to 0.00242.
2024-12-01-16:11:47-root-INFO: grad norm: 151.365 149.095 26.118
2024-12-01-16:11:48-root-INFO: grad norm: 150.827 149.435 20.444
2024-12-01-16:11:48-root-INFO: grad norm: 148.717 146.553 25.279
2024-12-01-16:11:49-root-INFO: Loss Change: 1322.637 -> 1292.815
2024-12-01-16:11:49-root-INFO: Regularization Change: 0.000 -> 0.614
2024-12-01-16:11:49-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-01-16:11:49-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-16:11:49-root-INFO: step: 158 lr_xt 0.00765943
2024-12-01-16:11:49-root-INFO: grad norm: 165.399 163.839 22.664
2024-12-01-16:11:49-root-INFO: Loss too large (1299.838->1445.368)! Learning rate decreased to 0.00613.
2024-12-01-16:11:49-root-INFO: Loss too large (1299.838->1403.897)! Learning rate decreased to 0.00490.
2024-12-01-16:11:49-root-INFO: Loss too large (1299.838->1359.912)! Learning rate decreased to 0.00392.
2024-12-01-16:11:50-root-INFO: Loss too large (1299.838->1321.526)! Learning rate decreased to 0.00314.
2024-12-01-16:11:50-root-INFO: grad norm: 212.931 210.629 31.220
2024-12-01-16:11:50-root-INFO: Loss too large (1294.843->1308.909)! Learning rate decreased to 0.00251.
2024-12-01-16:11:51-root-INFO: grad norm: 166.032 164.620 21.608
2024-12-01-16:11:51-root-INFO: grad norm: 111.806 109.924 20.430
2024-12-01-16:11:52-root-INFO: grad norm: 100.942 99.764 15.379
2024-12-01-16:11:52-root-INFO: Loss Change: 1299.838 -> 1247.835
2024-12-01-16:11:52-root-INFO: Regularization Change: 0.000 -> 0.583
2024-12-01-16:11:52-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-01-16:11:52-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-16:11:52-root-INFO: step: 157 lr_xt 0.00795203
2024-12-01-16:11:52-root-INFO: grad norm: 64.489 62.259 16.810
2024-12-01-16:11:53-root-INFO: grad norm: 149.932 148.646 19.594
2024-12-01-16:11:53-root-INFO: Loss too large (1244.702->1379.264)! Learning rate decreased to 0.00636.
2024-12-01-16:11:53-root-INFO: Loss too large (1244.702->1337.531)! Learning rate decreased to 0.00509.
2024-12-01-16:11:53-root-INFO: Loss too large (1244.702->1295.744)! Learning rate decreased to 0.00407.
2024-12-01-16:11:53-root-INFO: Loss too large (1244.702->1260.919)! Learning rate decreased to 0.00326.
2024-12-01-16:11:54-root-INFO: grad norm: 190.330 188.361 27.306
2024-12-01-16:11:54-root-INFO: Loss too large (1238.000->1250.355)! Learning rate decreased to 0.00261.
2024-12-01-16:11:54-root-INFO: grad norm: 143.455 142.181 19.075
2024-12-01-16:11:55-root-INFO: grad norm: 83.002 81.414 16.161
2024-12-01-16:11:55-root-INFO: Loss Change: 1246.813 -> 1205.086
2024-12-01-16:11:55-root-INFO: Regularization Change: 0.000 -> 0.865
2024-12-01-16:11:55-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-01-16:11:55-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-16:11:55-root-INFO: step: 156 lr_xt 0.00825448
2024-12-01-16:11:55-root-INFO: grad norm: 90.204 89.001 14.686
2024-12-01-16:11:56-root-INFO: Loss too large (1204.449->1276.859)! Learning rate decreased to 0.00660.
2024-12-01-16:11:56-root-INFO: Loss too large (1204.449->1241.715)! Learning rate decreased to 0.00528.
2024-12-01-16:11:56-root-INFO: Loss too large (1204.449->1217.869)! Learning rate decreased to 0.00423.
2024-12-01-16:11:56-root-INFO: grad norm: 163.759 161.974 24.117
2024-12-01-16:11:57-root-INFO: Loss too large (1204.265->1231.775)! Learning rate decreased to 0.00338.
2024-12-01-16:11:57-root-INFO: Loss too large (1204.265->1212.346)! Learning rate decreased to 0.00270.
2024-12-01-16:11:57-root-INFO: grad norm: 122.293 121.182 16.442
2024-12-01-16:11:58-root-INFO: grad norm: 68.718 67.191 14.406
2024-12-01-16:11:58-root-INFO: grad norm: 60.706 59.534 11.869
2024-12-01-16:11:59-root-INFO: Loss Change: 1204.449 -> 1170.383
2024-12-01-16:11:59-root-INFO: Regularization Change: 0.000 -> 0.475
2024-12-01-16:11:59-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-01-16:11:59-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-16:11:59-root-INFO: step: 155 lr_xt 0.00856705
2024-12-01-16:11:59-root-INFO: grad norm: 47.259 45.489 12.810
2024-12-01-16:11:59-root-INFO: grad norm: 48.767 47.546 10.842
2024-12-01-16:12:00-root-INFO: grad norm: 108.108 106.650 17.694
2024-12-01-16:12:00-root-INFO: Loss too large (1144.285->1217.891)! Learning rate decreased to 0.00685.
2024-12-01-16:12:00-root-INFO: Loss too large (1144.285->1187.655)! Learning rate decreased to 0.00548.
2024-12-01-16:12:00-root-INFO: Loss too large (1144.285->1167.083)! Learning rate decreased to 0.00439.
2024-12-01-16:12:00-root-INFO: Loss too large (1144.285->1153.378)! Learning rate decreased to 0.00351.
2024-12-01-16:12:01-root-INFO: Loss too large (1144.285->1144.622)! Learning rate decreased to 0.00281.
2024-12-01-16:12:01-root-INFO: grad norm: 89.817 88.787 13.563
2024-12-01-16:12:02-root-INFO: grad norm: 69.975 68.652 13.540
2024-12-01-16:12:02-root-INFO: Loss Change: 1169.116 -> 1126.270
2024-12-01-16:12:02-root-INFO: Regularization Change: 0.000 -> 1.164
2024-12-01-16:12:02-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-01-16:12:02-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-16:12:02-root-INFO: step: 154 lr_xt 0.00889002
2024-12-01-16:12:02-root-INFO: grad norm: 82.768 81.623 13.721
2024-12-01-16:12:02-root-INFO: Loss too large (1128.147->1201.079)! Learning rate decreased to 0.00711.
2024-12-01-16:12:03-root-INFO: Loss too large (1128.147->1164.678)! Learning rate decreased to 0.00569.
2024-12-01-16:12:03-root-INFO: Loss too large (1128.147->1140.764)! Learning rate decreased to 0.00455.
2024-12-01-16:12:03-root-INFO: grad norm: 154.907 153.322 22.103
2024-12-01-16:12:03-root-INFO: Loss too large (1127.657->1154.599)! Learning rate decreased to 0.00364.
2024-12-01-16:12:04-root-INFO: Loss too large (1127.657->1136.899)! Learning rate decreased to 0.00291.
2024-12-01-16:12:04-root-INFO: grad norm: 111.816 110.805 15.001
2024-12-01-16:12:04-root-INFO: grad norm: 52.403 51.002 12.037
2024-12-01-16:12:05-root-INFO: grad norm: 47.306 46.085 10.680
2024-12-01-16:12:05-root-INFO: Loss Change: 1128.147 -> 1097.389
2024-12-01-16:12:05-root-INFO: Regularization Change: 0.000 -> 0.450
2024-12-01-16:12:05-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-01-16:12:05-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-16:12:05-root-INFO: step: 153 lr_xt 0.00922367
2024-12-01-16:12:05-root-INFO: grad norm: 45.319 43.685 12.061
2024-12-01-16:12:06-root-INFO: grad norm: 69.395 68.264 12.478
2024-12-01-16:12:06-root-INFO: Loss too large (1083.116->1102.287)! Learning rate decreased to 0.00738.
2024-12-01-16:12:06-root-INFO: Loss too large (1083.116->1091.627)! Learning rate decreased to 0.00590.
2024-12-01-16:12:06-root-INFO: Loss too large (1083.116->1084.935)! Learning rate decreased to 0.00472.
2024-12-01-16:12:07-root-INFO: grad norm: 85.030 84.053 12.849
2024-12-01-16:12:07-root-INFO: grad norm: 139.779 138.334 20.049
2024-12-01-16:12:08-root-INFO: Loss too large (1078.243->1097.177)! Learning rate decreased to 0.00378.
2024-12-01-16:12:08-root-INFO: Loss too large (1078.243->1082.930)! Learning rate decreased to 0.00302.
2024-12-01-16:12:08-root-INFO: grad norm: 94.749 93.824 13.210
2024-12-01-16:12:08-root-INFO: Loss Change: 1095.866 -> 1059.300
2024-12-01-16:12:08-root-INFO: Regularization Change: 0.000 -> 0.919
2024-12-01-16:12:08-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-01-16:12:08-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-16:12:09-root-INFO: step: 152 lr_xt 0.00956831
2024-12-01-16:12:09-root-INFO: grad norm: 39.810 38.304 10.846
2024-12-01-16:12:09-root-INFO: grad norm: 40.842 39.680 9.674
2024-12-01-16:12:10-root-INFO: grad norm: 50.394 48.965 11.918
2024-12-01-16:12:10-root-INFO: grad norm: 55.653 54.719 10.154
2024-12-01-16:12:11-root-INFO: grad norm: 75.160 73.909 13.654
2024-12-01-16:12:11-root-INFO: Loss too large (1014.448->1048.067)! Learning rate decreased to 0.00765.
2024-12-01-16:12:11-root-INFO: Loss too large (1014.448->1025.249)! Learning rate decreased to 0.00612.
2024-12-01-16:12:11-root-INFO: Loss Change: 1057.461 -> 1013.387
2024-12-01-16:12:11-root-INFO: Regularization Change: 0.000 -> 2.567
2024-12-01-16:12:11-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-01-16:12:11-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-16:12:12-root-INFO: step: 151 lr_xt 0.00992422
2024-12-01-16:12:12-root-INFO: grad norm: 94.026 92.982 13.973
2024-12-01-16:12:12-root-INFO: Loss too large (1005.837->1054.396)! Learning rate decreased to 0.00794.
2024-12-01-16:12:12-root-INFO: Loss too large (1005.837->1032.577)! Learning rate decreased to 0.00635.
2024-12-01-16:12:12-root-INFO: Loss too large (1005.837->1018.261)! Learning rate decreased to 0.00508.
2024-12-01-16:12:12-root-INFO: Loss too large (1005.837->1009.027)! Learning rate decreased to 0.00406.
2024-12-01-16:12:13-root-INFO: grad norm: 84.253 83.375 12.132
2024-12-01-16:12:13-root-INFO: grad norm: 66.079 65.013 11.819
2024-12-01-16:12:14-root-INFO: grad norm: 62.611 61.720 10.526
2024-12-01-16:12:14-root-INFO: grad norm: 57.044 55.979 10.973
2024-12-01-16:12:15-root-INFO: Loss Change: 1005.837 -> 977.662
2024-12-01-16:12:15-root-INFO: Regularization Change: 0.000 -> 0.565
2024-12-01-16:12:15-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-01-16:12:15-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-16:12:15-root-INFO: step: 150 lr_xt 0.01029171
2024-12-01-16:12:15-root-INFO: grad norm: 64.875 63.945 10.948
2024-12-01-16:12:15-root-INFO: Loss too large (978.143->1017.781)! Learning rate decreased to 0.00823.
2024-12-01-16:12:15-root-INFO: Loss too large (978.143->991.953)! Learning rate decreased to 0.00659.
2024-12-01-16:12:16-root-INFO: Loss too large (978.143->978.471)! Learning rate decreased to 0.00527.
2024-12-01-16:12:16-root-INFO: grad norm: 86.809 85.755 13.488
2024-12-01-16:12:16-root-INFO: Loss too large (972.400->975.097)! Learning rate decreased to 0.00422.
2024-12-01-16:12:17-root-INFO: grad norm: 74.099 73.282 10.973
2024-12-01-16:12:17-root-INFO: grad norm: 52.145 51.100 10.389
2024-12-01-16:12:18-root-INFO: grad norm: 48.350 47.430 9.390
2024-12-01-16:12:18-root-INFO: Loss Change: 978.143 -> 949.656
2024-12-01-16:12:18-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-01-16:12:18-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-01-16:12:18-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-16:12:18-root-INFO: step: 149 lr_xt 0.01067108
2024-12-01-16:12:18-root-INFO: grad norm: 37.465 36.172 9.758
2024-12-01-16:12:19-root-INFO: grad norm: 39.329 38.339 8.772
2024-12-01-16:12:19-root-INFO: grad norm: 67.152 66.062 12.049
2024-12-01-16:12:19-root-INFO: Loss too large (927.640->943.984)! Learning rate decreased to 0.00854.
2024-12-01-16:12:20-root-INFO: Loss too large (927.640->933.919)! Learning rate decreased to 0.00683.
2024-12-01-16:12:20-root-INFO: grad norm: 80.801 80.063 10.899
2024-12-01-16:12:20-root-INFO: grad norm: 153.156 151.718 20.939
2024-12-01-16:12:21-root-INFO: Loss too large (926.076->968.491)! Learning rate decreased to 0.00546.
2024-12-01-16:12:21-root-INFO: Loss too large (926.076->944.387)! Learning rate decreased to 0.00437.
2024-12-01-16:12:21-root-INFO: Loss too large (926.076->928.563)! Learning rate decreased to 0.00350.
2024-12-01-16:12:21-root-INFO: Loss Change: 947.647 -> 918.498
2024-12-01-16:12:21-root-INFO: Regularization Change: 0.000 -> 1.666
2024-12-01-16:12:21-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-01-16:12:21-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-16:12:22-root-INFO: step: 148 lr_xt 0.01106266
2024-12-01-16:12:22-root-INFO: grad norm: 87.514 86.689 11.987
2024-12-01-16:12:22-root-INFO: Loss too large (921.205->972.602)! Learning rate decreased to 0.00885.
2024-12-01-16:12:22-root-INFO: Loss too large (921.205->927.805)! Learning rate decreased to 0.00708.
2024-12-01-16:12:22-root-INFO: grad norm: 110.200 109.091 15.594
2024-12-01-16:12:23-root-INFO: Loss too large (907.636->921.855)! Learning rate decreased to 0.00566.
2024-12-01-16:12:23-root-INFO: Loss too large (907.636->909.790)! Learning rate decreased to 0.00453.
2024-12-01-16:12:23-root-INFO: grad norm: 69.443 68.684 10.241
2024-12-01-16:12:24-root-INFO: grad norm: 31.581 30.413 8.510
2024-12-01-16:12:24-root-INFO: grad norm: 30.384 29.239 8.264
2024-12-01-16:12:25-root-INFO: Loss Change: 921.205 -> 880.577
2024-12-01-16:12:25-root-INFO: Regularization Change: 0.000 -> 0.752
2024-12-01-16:12:25-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-01-16:12:25-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-16:12:25-root-INFO: step: 147 lr_xt 0.01146675
2024-12-01-16:12:25-root-INFO: grad norm: 34.564 33.404 8.879
2024-12-01-16:12:25-root-INFO: grad norm: 44.393 43.522 8.752
2024-12-01-16:12:26-root-INFO: grad norm: 70.794 70.012 10.498
2024-12-01-16:12:26-root-INFO: Loss too large (867.566->897.417)! Learning rate decreased to 0.00917.
2024-12-01-16:12:26-root-INFO: Loss too large (867.566->870.111)! Learning rate decreased to 0.00734.
2024-12-01-16:12:27-root-INFO: grad norm: 81.404 80.546 11.792
2024-12-01-16:12:27-root-INFO: Loss too large (858.635->862.638)! Learning rate decreased to 0.00587.
2024-12-01-16:12:27-root-INFO: grad norm: 63.624 62.922 9.425
2024-12-01-16:12:28-root-INFO: Loss Change: 877.769 -> 844.684
2024-12-01-16:12:28-root-INFO: Regularization Change: 0.000 -> 1.440
2024-12-01-16:12:28-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-01-16:12:28-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-16:12:28-root-INFO: step: 146 lr_xt 0.01188369
2024-12-01-16:12:28-root-INFO: grad norm: 29.737 28.638 8.008
2024-12-01-16:12:28-root-INFO: grad norm: 31.392 30.491 7.468
2024-12-01-16:12:29-root-INFO: grad norm: 49.054 48.130 9.480
2024-12-01-16:12:29-root-INFO: Loss too large (828.695->832.525)! Learning rate decreased to 0.00951.
2024-12-01-16:12:29-root-INFO: grad norm: 57.805 57.139 8.748
2024-12-01-16:12:30-root-INFO: grad norm: 93.983 93.019 13.421
2024-12-01-16:12:30-root-INFO: Loss too large (824.374->844.186)! Learning rate decreased to 0.00761.
2024-12-01-16:12:30-root-INFO: Loss too large (824.374->830.781)! Learning rate decreased to 0.00608.
2024-12-01-16:12:31-root-INFO: Loss Change: 842.213 -> 822.351
2024-12-01-16:12:31-root-INFO: Regularization Change: 0.000 -> 1.613
2024-12-01-16:12:31-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-01-16:12:31-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-16:12:31-root-INFO: step: 145 lr_xt 0.01231381
2024-12-01-16:12:31-root-INFO: grad norm: 71.070 70.306 10.395
2024-12-01-16:12:31-root-INFO: Loss too large (825.345->834.370)! Learning rate decreased to 0.00985.
2024-12-01-16:12:32-root-INFO: grad norm: 89.265 88.372 12.597
2024-12-01-16:12:32-root-INFO: Loss too large (815.246->828.257)! Learning rate decreased to 0.00788.
2024-12-01-16:12:32-root-INFO: Loss too large (815.246->816.490)! Learning rate decreased to 0.00630.
2024-12-01-16:12:33-root-INFO: grad norm: 54.105 53.439 8.468
2024-12-01-16:12:33-root-INFO: grad norm: 24.226 23.169 7.079
2024-12-01-16:12:34-root-INFO: grad norm: 22.611 21.575 6.768
2024-12-01-16:12:34-root-INFO: Loss Change: 825.345 -> 792.322
2024-12-01-16:12:34-root-INFO: Regularization Change: 0.000 -> 0.824
2024-12-01-16:12:34-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-01-16:12:34-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-16:12:34-root-INFO: step: 144 lr_xt 0.01275743
2024-12-01-16:12:34-root-INFO: grad norm: 28.348 27.257 7.788
2024-12-01-16:12:35-root-INFO: grad norm: 31.010 30.205 7.020
2024-12-01-16:12:35-root-INFO: grad norm: 40.002 39.197 7.984
2024-12-01-16:12:36-root-INFO: grad norm: 62.141 61.494 8.939
2024-12-01-16:12:36-root-INFO: Loss too large (780.125->788.526)! Learning rate decreased to 0.01021.
2024-12-01-16:12:36-root-INFO: grad norm: 61.561 60.861 9.262
2024-12-01-16:12:37-root-INFO: Loss Change: 792.152 -> 771.001
2024-12-01-16:12:37-root-INFO: Regularization Change: 0.000 -> 1.700
2024-12-01-16:12:37-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-01-16:12:37-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-16:12:37-root-INFO: step: 143 lr_xt 0.01321490
2024-12-01-16:12:37-root-INFO: grad norm: 46.123 45.459 7.796
2024-12-01-16:12:37-root-INFO: Loss too large (766.071->766.593)! Learning rate decreased to 0.01057.
2024-12-01-16:12:38-root-INFO: grad norm: 45.052 44.303 8.176
2024-12-01-16:12:38-root-INFO: grad norm: 42.556 41.936 7.233
2024-12-01-16:12:39-root-INFO: grad norm: 42.031 41.261 8.008
2024-12-01-16:12:39-root-INFO: grad norm: 40.140 39.529 6.979
2024-12-01-16:12:39-root-INFO: Loss Change: 766.071 -> 744.070
2024-12-01-16:12:39-root-INFO: Regularization Change: 0.000 -> 1.268
2024-12-01-16:12:39-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-01-16:12:39-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-16:12:40-root-INFO: step: 142 lr_xt 0.01368658
2024-12-01-16:12:40-root-INFO: grad norm: 53.571 52.696 9.639
2024-12-01-16:12:40-root-INFO: grad norm: 67.298 66.705 8.914
2024-12-01-16:12:40-root-INFO: Loss too large (747.440->749.722)! Learning rate decreased to 0.01095.
2024-12-01-16:12:41-root-INFO: grad norm: 54.879 54.216 8.504
2024-12-01-16:12:41-root-INFO: grad norm: 45.903 45.333 7.217
2024-12-01-16:12:42-root-INFO: grad norm: 42.000 41.336 7.437
2024-12-01-16:12:42-root-INFO: Loss Change: 747.488 -> 721.680
2024-12-01-16:12:42-root-INFO: Regularization Change: 0.000 -> 1.473
2024-12-01-16:12:42-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-01-16:12:42-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-16:12:42-root-INFO: step: 141 lr_xt 0.01417280
2024-12-01-16:12:42-root-INFO: grad norm: 29.385 28.709 6.264
2024-12-01-16:12:43-root-INFO: grad norm: 38.218 37.444 7.656
2024-12-01-16:12:43-root-INFO: grad norm: 45.443 44.879 7.137
2024-12-01-16:12:44-root-INFO: grad norm: 58.630 57.805 9.801
2024-12-01-16:12:44-root-INFO: Loss too large (710.443->714.235)! Learning rate decreased to 0.01134.
2024-12-01-16:12:44-root-INFO: grad norm: 45.950 45.386 7.178
2024-12-01-16:12:45-root-INFO: Loss Change: 718.835 -> 697.567
2024-12-01-16:12:45-root-INFO: Regularization Change: 0.000 -> 1.538
2024-12-01-16:12:45-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-01-16:12:45-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-16:12:45-root-INFO: step: 140 lr_xt 0.01467393
2024-12-01-16:12:45-root-INFO: grad norm: 43.340 42.730 7.245
2024-12-01-16:12:46-root-INFO: grad norm: 54.004 53.486 7.465
2024-12-01-16:12:46-root-INFO: Loss too large (696.695->698.527)! Learning rate decreased to 0.01174.
2024-12-01-16:12:46-root-INFO: grad norm: 46.525 45.955 7.264
2024-12-01-16:12:47-root-INFO: grad norm: 39.323 38.789 6.457
2024-12-01-16:12:47-root-INFO: grad norm: 37.674 37.011 7.038
2024-12-01-16:12:48-root-INFO: Loss Change: 699.218 -> 677.926
2024-12-01-16:12:48-root-INFO: Regularization Change: 0.000 -> 1.391
2024-12-01-16:12:48-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-01-16:12:48-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-16:12:48-root-INFO: step: 139 lr_xt 0.01519033
2024-12-01-16:12:48-root-INFO: grad norm: 30.920 30.232 6.487
2024-12-01-16:12:48-root-INFO: grad norm: 51.454 50.607 9.295
2024-12-01-16:12:48-root-INFO: Loss too large (673.308->677.502)! Learning rate decreased to 0.01215.
2024-12-01-16:12:49-root-INFO: Loss too large (673.308->673.786)! Learning rate decreased to 0.00972.
2024-12-01-16:12:49-root-INFO: grad norm: 33.318 32.703 6.371
2024-12-01-16:12:50-root-INFO: grad norm: 17.640 16.863 5.178
2024-12-01-16:12:50-root-INFO: grad norm: 16.870 16.054 5.184
2024-12-01-16:12:50-root-INFO: Loss Change: 676.276 -> 657.760
2024-12-01-16:12:50-root-INFO: Regularization Change: 0.000 -> 0.902
2024-12-01-16:12:50-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-01-16:12:50-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-16:12:51-root-INFO: step: 138 lr_xt 0.01572237
2024-12-01-16:12:51-root-INFO: grad norm: 21.758 21.070 5.426
2024-12-01-16:12:51-root-INFO: grad norm: 23.015 22.389 5.331
2024-12-01-16:12:52-root-INFO: grad norm: 27.297 26.753 5.419
2024-12-01-16:12:52-root-INFO: grad norm: 36.156 35.701 5.716
2024-12-01-16:12:52-root-INFO: Loss too large (646.804->646.952)! Learning rate decreased to 0.01258.
2024-12-01-16:12:53-root-INFO: grad norm: 35.477 34.947 6.111
2024-12-01-16:12:53-root-INFO: Loss Change: 657.338 -> 640.420
2024-12-01-16:12:53-root-INFO: Regularization Change: 0.000 -> 1.471
2024-12-01-16:12:53-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-01-16:12:53-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-16:12:53-root-INFO: step: 137 lr_xt 0.01627042
2024-12-01-16:12:54-root-INFO: grad norm: 28.362 27.645 6.334
2024-12-01-16:12:54-root-INFO: grad norm: 44.640 43.886 8.172
2024-12-01-16:12:54-root-INFO: Loss too large (635.545->639.712)! Learning rate decreased to 0.01302.
2024-12-01-16:12:54-root-INFO: Loss too large (635.545->635.909)! Learning rate decreased to 0.01041.
2024-12-01-16:12:55-root-INFO: grad norm: 32.392 31.794 6.195
2024-12-01-16:12:55-root-INFO: grad norm: 18.175 17.422 5.175
2024-12-01-16:12:56-root-INFO: grad norm: 17.305 16.610 4.857
2024-12-01-16:12:56-root-INFO: Loss Change: 638.414 -> 621.610
2024-12-01-16:12:56-root-INFO: Regularization Change: 0.000 -> 0.911
2024-12-01-16:12:56-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-01-16:12:56-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-16:12:56-root-INFO: step: 136 lr_xt 0.01683487
2024-12-01-16:12:56-root-INFO: grad norm: 22.408 21.747 5.405
2024-12-01-16:12:57-root-INFO: grad norm: 28.803 28.242 5.659
2024-12-01-16:12:57-root-INFO: grad norm: 56.214 55.396 9.555
2024-12-01-16:12:58-root-INFO: Loss too large (618.840->626.434)! Learning rate decreased to 0.01347.
2024-12-01-16:12:58-root-INFO: Loss too large (618.840->621.320)! Learning rate decreased to 0.01077.
2024-12-01-16:12:58-root-INFO: grad norm: 33.805 33.244 6.135
2024-12-01-16:12:59-root-INFO: grad norm: 17.928 17.330 4.593
2024-12-01-16:12:59-root-INFO: Loss Change: 621.613 -> 605.911
2024-12-01-16:12:59-root-INFO: Regularization Change: 0.000 -> 1.020
2024-12-01-16:12:59-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-01-16:12:59-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-16:12:59-root-INFO: step: 135 lr_xt 0.01741608
2024-12-01-16:13:00-root-INFO: grad norm: 19.509 18.858 4.998
2024-12-01-16:13:00-root-INFO: grad norm: 22.171 21.593 5.029
2024-12-01-16:13:01-root-INFO: grad norm: 36.947 36.295 6.909
2024-12-01-16:13:01-root-INFO: Loss too large (599.884->603.177)! Learning rate decreased to 0.01393.
2024-12-01-16:13:01-root-INFO: grad norm: 34.846 34.299 6.150
2024-12-01-16:13:02-root-INFO: grad norm: 28.815 28.237 5.743
2024-12-01-16:13:02-root-INFO: Loss Change: 605.584 -> 591.326
2024-12-01-16:13:02-root-INFO: Regularization Change: 0.000 -> 1.342
2024-12-01-16:13:02-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-01-16:13:02-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-16:13:02-root-INFO: step: 134 lr_xt 0.01801447
2024-12-01-16:13:02-root-INFO: grad norm: 26.153 25.569 5.498
2024-12-01-16:13:03-root-INFO: grad norm: 46.257 45.524 8.201
2024-12-01-16:13:03-root-INFO: Loss too large (587.533->592.506)! Learning rate decreased to 0.01441.
2024-12-01-16:13:03-root-INFO: Loss too large (587.533->588.832)! Learning rate decreased to 0.01153.
2024-12-01-16:13:04-root-INFO: grad norm: 29.995 29.417 5.856
2024-12-01-16:13:04-root-INFO: grad norm: 15.172 14.518 4.407
2024-12-01-16:13:05-root-INFO: grad norm: 14.340 13.651 4.392
2024-12-01-16:13:05-root-INFO: Loss Change: 589.281 -> 574.121
2024-12-01-16:13:05-root-INFO: Regularization Change: 0.000 -> 0.874
2024-12-01-16:13:05-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-01-16:13:05-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-16:13:05-root-INFO: step: 133 lr_xt 0.01863041
2024-12-01-16:13:05-root-INFO: grad norm: 20.727 20.175 4.755
2024-12-01-16:13:06-root-INFO: grad norm: 25.421 24.984 4.691
2024-12-01-16:13:06-root-INFO: grad norm: 33.492 33.122 4.968
2024-12-01-16:13:06-root-INFO: Loss too large (569.867->569.975)! Learning rate decreased to 0.01490.
2024-12-01-16:13:07-root-INFO: grad norm: 33.631 33.246 5.071
2024-12-01-16:13:07-root-INFO: grad norm: 35.879 35.434 5.631
2024-12-01-16:13:08-root-INFO: Loss Change: 574.446 -> 562.273
2024-12-01-16:13:08-root-INFO: Regularization Change: 0.000 -> 1.425
2024-12-01-16:13:08-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-01-16:13:08-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-16:13:08-root-INFO: step: 132 lr_xt 0.01926430
2024-12-01-16:13:08-root-INFO: grad norm: 32.851 32.372 5.588
2024-12-01-16:13:08-root-INFO: Loss too large (560.230->562.642)! Learning rate decreased to 0.01541.
2024-12-01-16:13:09-root-INFO: grad norm: 41.663 41.052 7.108
2024-12-01-16:13:09-root-INFO: Loss too large (557.992->559.014)! Learning rate decreased to 0.01233.
2024-12-01-16:13:09-root-INFO: grad norm: 30.923 30.359 5.876
2024-12-01-16:13:10-root-INFO: grad norm: 18.474 17.903 4.560
2024-12-01-16:13:10-root-INFO: grad norm: 17.246 16.678 4.389
2024-12-01-16:13:11-root-INFO: Loss Change: 560.230 -> 545.132
2024-12-01-16:13:11-root-INFO: Regularization Change: 0.000 -> 0.853
2024-12-01-16:13:11-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-01-16:13:11-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-16:13:11-root-INFO: step: 131 lr_xt 0.01991656
2024-12-01-16:13:11-root-INFO: grad norm: 25.749 25.198 5.298
2024-12-01-16:13:11-root-INFO: grad norm: 34.884 34.322 6.233
2024-12-01-16:13:12-root-INFO: Loss too large (545.440->547.094)! Learning rate decreased to 0.01593.
2024-12-01-16:13:12-root-INFO: grad norm: 39.120 38.546 6.678
2024-12-01-16:13:12-root-INFO: Loss too large (541.770->542.044)! Learning rate decreased to 0.01275.
2024-12-01-16:13:13-root-INFO: grad norm: 29.447 28.868 5.812
2024-12-01-16:13:13-root-INFO: grad norm: 19.464 18.932 4.520
2024-12-01-16:13:14-root-INFO: Loss Change: 546.001 -> 531.503
2024-12-01-16:13:14-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-01-16:13:14-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-01-16:13:14-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-16:13:14-root-INFO: step: 130 lr_xt 0.02058758
2024-12-01-16:13:14-root-INFO: grad norm: 17.476 16.900 4.451
2024-12-01-16:13:14-root-INFO: grad norm: 26.827 26.285 5.368
2024-12-01-16:13:15-root-INFO: Loss too large (527.780->528.865)! Learning rate decreased to 0.01647.
2024-12-01-16:13:15-root-INFO: grad norm: 27.196 26.594 5.692
2024-12-01-16:13:15-root-INFO: grad norm: 27.456 26.895 5.520
2024-12-01-16:13:16-root-INFO: grad norm: 27.016 26.400 5.739
2024-12-01-16:13:16-root-INFO: Loss Change: 530.716 -> 516.305
2024-12-01-16:13:16-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-01-16:13:16-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-01-16:13:16-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-16:13:16-root-INFO: step: 129 lr_xt 0.02127779
2024-12-01-16:13:17-root-INFO: grad norm: 33.076 32.530 5.981
2024-12-01-16:13:17-root-INFO: Loss too large (517.212->519.336)! Learning rate decreased to 0.01702.
2024-12-01-16:13:17-root-INFO: grad norm: 31.295 30.666 6.242
2024-12-01-16:13:18-root-INFO: grad norm: 27.476 27.017 5.001
2024-12-01-16:13:18-root-INFO: grad norm: 28.954 28.385 5.710
2024-12-01-16:13:19-root-INFO: grad norm: 31.650 31.171 5.488
2024-12-01-16:13:19-root-INFO: Loss Change: 517.212 -> 504.136
2024-12-01-16:13:19-root-INFO: Regularization Change: 0.000 -> 1.507
2024-12-01-16:13:19-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-01-16:13:19-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-16:13:19-root-INFO: step: 128 lr_xt 0.02198759
2024-12-01-16:13:19-root-INFO: grad norm: 27.998 27.346 6.007
2024-12-01-16:13:20-root-INFO: grad norm: 42.291 41.670 7.220
2024-12-01-16:13:20-root-INFO: Loss too large (499.686->504.982)! Learning rate decreased to 0.01759.
2024-12-01-16:13:20-root-INFO: Loss too large (499.686->500.244)! Learning rate decreased to 0.01407.
2024-12-01-16:13:21-root-INFO: grad norm: 27.483 26.829 5.961
2024-12-01-16:13:21-root-INFO: grad norm: 14.786 14.334 3.630
2024-12-01-16:13:22-root-INFO: grad norm: 13.113 12.593 3.654
2024-12-01-16:13:22-root-INFO: Loss Change: 502.282 -> 486.254
2024-12-01-16:13:22-root-INFO: Regularization Change: 0.000 -> 1.042
2024-12-01-16:13:22-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-01-16:13:22-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-16:13:22-root-INFO: step: 127 lr_xt 0.02271741
2024-12-01-16:13:22-root-INFO: grad norm: 17.690 17.262 3.868
2024-12-01-16:13:23-root-INFO: grad norm: 23.576 23.080 4.815
2024-12-01-16:13:23-root-INFO: Loss too large (483.776->484.159)! Learning rate decreased to 0.01817.
2024-12-01-16:13:24-root-INFO: grad norm: 27.956 27.513 4.960
2024-12-01-16:13:24-root-INFO: grad norm: 30.033 29.441 5.934
2024-12-01-16:13:25-root-INFO: grad norm: 32.700 32.215 5.614
2024-12-01-16:13:25-root-INFO: Loss Change: 486.139 -> 477.846
2024-12-01-16:13:25-root-INFO: Regularization Change: 0.000 -> 1.428
2024-12-01-16:13:25-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-01-16:13:25-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-16:13:25-root-INFO: step: 126 lr_xt 0.02346768
2024-12-01-16:13:25-root-INFO: grad norm: 30.234 29.610 6.115
2024-12-01-16:13:26-root-INFO: grad norm: 45.249 44.597 7.651
2024-12-01-16:13:26-root-INFO: Loss too large (475.997->483.906)! Learning rate decreased to 0.01877.
2024-12-01-16:13:26-root-INFO: Loss too large (475.997->476.474)! Learning rate decreased to 0.01502.
2024-12-01-16:13:27-root-INFO: grad norm: 28.918 28.256 6.150
2024-12-01-16:13:27-root-INFO: grad norm: 15.888 15.527 3.367
2024-12-01-16:13:28-root-INFO: grad norm: 13.140 12.684 3.433
2024-12-01-16:13:28-root-INFO: Loss Change: 476.786 -> 461.417
2024-12-01-16:13:28-root-INFO: Regularization Change: 0.000 -> 1.007
2024-12-01-16:13:28-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-01-16:13:28-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-16:13:28-root-INFO: step: 125 lr_xt 0.02423882
2024-12-01-16:13:28-root-INFO: grad norm: 18.270 17.877 3.767
2024-12-01-16:13:29-root-INFO: grad norm: 24.421 23.994 4.548
2024-12-01-16:13:29-root-INFO: Loss too large (459.544->460.681)! Learning rate decreased to 0.01939.
2024-12-01-16:13:29-root-INFO: grad norm: 27.708 27.295 4.763
2024-12-01-16:13:30-root-INFO: grad norm: 30.236 29.633 6.006
2024-12-01-16:13:30-root-INFO: grad norm: 35.058 34.491 6.280
2024-12-01-16:13:30-root-INFO: Loss too large (455.327->455.735)! Learning rate decreased to 0.01551.
2024-12-01-16:13:31-root-INFO: Loss Change: 461.667 -> 452.479
2024-12-01-16:13:31-root-INFO: Regularization Change: 0.000 -> 1.258
2024-12-01-16:13:31-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-01-16:13:31-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-16:13:31-root-INFO: step: 124 lr_xt 0.02515763
2024-12-01-16:13:31-root-INFO: grad norm: 23.099 22.436 5.495
2024-12-01-16:13:32-root-INFO: grad norm: 38.276 37.568 7.329
2024-12-01-16:13:32-root-INFO: Loss too large (449.989->455.711)! Learning rate decreased to 0.02013.
2024-12-01-16:13:32-root-INFO: Loss too large (449.989->451.396)! Learning rate decreased to 0.01610.
2024-12-01-16:13:33-root-INFO: grad norm: 26.166 25.426 6.179
2024-12-01-16:13:33-root-INFO: grad norm: 13.909 13.497 3.360
2024-12-01-16:13:33-root-INFO: grad norm: 12.522 12.043 3.432
2024-12-01-16:13:34-root-INFO: Loss Change: 450.981 -> 438.533
2024-12-01-16:13:34-root-INFO: Regularization Change: 0.000 -> 0.974
2024-12-01-16:13:34-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-01-16:13:34-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-16:13:34-root-INFO: step: 123 lr_xt 0.02597490
2024-12-01-16:13:34-root-INFO: grad norm: 16.799 16.383 3.716
2024-12-01-16:13:35-root-INFO: grad norm: 24.332 23.731 5.374
2024-12-01-16:13:35-root-INFO: Loss too large (437.617->440.339)! Learning rate decreased to 0.02078.
2024-12-01-16:13:35-root-INFO: grad norm: 31.637 31.010 6.269
2024-12-01-16:13:35-root-INFO: Loss too large (436.649->437.917)! Learning rate decreased to 0.01662.
2024-12-01-16:13:36-root-INFO: grad norm: 25.729 25.020 6.000
2024-12-01-16:13:36-root-INFO: grad norm: 19.369 18.904 4.215
2024-12-01-16:13:37-root-INFO: Loss Change: 438.774 -> 429.451
2024-12-01-16:13:37-root-INFO: Regularization Change: 0.000 -> 0.997
2024-12-01-16:13:37-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-01-16:13:37-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-16:13:37-root-INFO: step: 122 lr_xt 0.02681440
2024-12-01-16:13:37-root-INFO: grad norm: 15.961 15.457 3.978
2024-12-01-16:13:37-root-INFO: grad norm: 26.941 26.353 5.600
2024-12-01-16:13:38-root-INFO: Loss too large (427.742->431.529)! Learning rate decreased to 0.02145.
2024-12-01-16:13:38-root-INFO: Loss too large (427.742->428.457)! Learning rate decreased to 0.01716.
2024-12-01-16:13:38-root-INFO: grad norm: 22.715 22.041 5.494
2024-12-01-16:13:39-root-INFO: grad norm: 17.762 17.294 4.048
2024-12-01-16:13:39-root-INFO: grad norm: 16.806 16.258 4.257
2024-12-01-16:13:40-root-INFO: Loss Change: 428.819 -> 419.783
2024-12-01-16:13:40-root-INFO: Regularization Change: 0.000 -> 0.897
2024-12-01-16:13:40-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-01-16:13:40-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-16:13:40-root-INFO: step: 121 lr_xt 0.02767658
2024-12-01-16:13:40-root-INFO: grad norm: 21.737 21.225 4.689
2024-12-01-16:13:40-root-INFO: Loss too large (420.161->422.092)! Learning rate decreased to 0.02214.
2024-12-01-16:13:41-root-INFO: grad norm: 25.385 24.702 5.849
2024-12-01-16:13:41-root-INFO: grad norm: 32.596 31.904 6.681
2024-12-01-16:13:41-root-INFO: Loss too large (418.539->420.526)! Learning rate decreased to 0.01771.
2024-12-01-16:13:42-root-INFO: grad norm: 26.085 25.378 6.033
2024-12-01-16:13:42-root-INFO: grad norm: 18.494 18.048 4.035
2024-12-01-16:13:42-root-INFO: Loss Change: 420.161 -> 411.238
2024-12-01-16:13:42-root-INFO: Regularization Change: 0.000 -> 0.920
2024-12-01-16:13:42-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-01-16:13:42-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-16:13:43-root-INFO: step: 120 lr_xt 0.02856188
2024-12-01-16:13:43-root-INFO: grad norm: 16.293 15.817 3.912
2024-12-01-16:13:43-root-INFO: grad norm: 28.370 27.784 5.737
2024-12-01-16:13:43-root-INFO: Loss too large (410.568->415.347)! Learning rate decreased to 0.02285.
2024-12-01-16:13:44-root-INFO: Loss too large (410.568->411.721)! Learning rate decreased to 0.01828.
2024-12-01-16:13:44-root-INFO: grad norm: 23.211 22.573 5.407
2024-12-01-16:13:44-root-INFO: grad norm: 17.450 17.014 3.876
2024-12-01-16:13:45-root-INFO: grad norm: 16.470 15.962 4.057
2024-12-01-16:13:45-root-INFO: Loss Change: 411.025 -> 402.491
2024-12-01-16:13:45-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-01-16:13:45-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-01-16:13:45-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-16:13:46-root-INFO: step: 119 lr_xt 0.02947075
2024-12-01-16:13:46-root-INFO: grad norm: 21.486 21.014 4.479
2024-12-01-16:13:46-root-INFO: Loss too large (403.094->404.855)! Learning rate decreased to 0.02358.
2024-12-01-16:13:46-root-INFO: grad norm: 24.590 23.970 5.487
2024-12-01-16:13:47-root-INFO: grad norm: 30.792 30.198 6.022
2024-12-01-16:13:47-root-INFO: Loss too large (401.339->402.833)! Learning rate decreased to 0.01886.
2024-12-01-16:13:47-root-INFO: grad norm: 24.453 23.817 5.540
2024-12-01-16:13:48-root-INFO: grad norm: 17.775 17.374 3.752
2024-12-01-16:13:48-root-INFO: Loss Change: 403.094 -> 394.218
2024-12-01-16:13:48-root-INFO: Regularization Change: 0.000 -> 0.954
2024-12-01-16:13:48-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-01-16:13:48-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-16:13:49-root-INFO: step: 118 lr_xt 0.03040366
2024-12-01-16:13:49-root-INFO: grad norm: 13.844 13.424 3.386
2024-12-01-16:13:49-root-INFO: grad norm: 23.386 22.915 4.668
2024-12-01-16:13:49-root-INFO: Loss too large (392.967->396.283)! Learning rate decreased to 0.02432.
2024-12-01-16:13:50-root-INFO: Loss too large (392.967->393.416)! Learning rate decreased to 0.01946.
2024-12-01-16:13:50-root-INFO: grad norm: 20.064 19.534 4.582
2024-12-01-16:13:51-root-INFO: grad norm: 16.523 16.138 3.545
2024-12-01-16:13:51-root-INFO: grad norm: 15.369 14.923 3.675
2024-12-01-16:13:51-root-INFO: Loss Change: 393.717 -> 386.026
2024-12-01-16:13:51-root-INFO: Regularization Change: 0.000 -> 0.883
2024-12-01-16:13:51-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-01-16:13:51-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-16:13:51-root-INFO: step: 117 lr_xt 0.03136105
2024-12-01-16:13:52-root-INFO: grad norm: 19.669 19.278 3.903
2024-12-01-16:13:52-root-INFO: Loss too large (386.784->388.033)! Learning rate decreased to 0.02509.
2024-12-01-16:13:52-root-INFO: grad norm: 22.487 21.993 4.686
2024-12-01-16:13:53-root-INFO: grad norm: 27.964 27.471 5.227
2024-12-01-16:13:53-root-INFO: Loss too large (385.067->386.028)! Learning rate decreased to 0.02007.
2024-12-01-16:13:53-root-INFO: grad norm: 22.369 21.839 4.839
2024-12-01-16:13:54-root-INFO: grad norm: 17.034 16.670 3.502
2024-12-01-16:13:54-root-INFO: Loss Change: 386.784 -> 378.516
2024-12-01-16:13:54-root-INFO: Regularization Change: 0.000 -> 0.950
2024-12-01-16:13:54-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-01-16:13:54-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-16:13:55-root-INFO: step: 116 lr_xt 0.03234339
2024-12-01-16:13:55-root-INFO: grad norm: 14.135 13.728 3.365
2024-12-01-16:13:55-root-INFO: grad norm: 23.215 22.799 4.374
2024-12-01-16:13:55-root-INFO: Loss too large (377.265->380.424)! Learning rate decreased to 0.02587.
2024-12-01-16:13:56-root-INFO: Loss too large (377.265->377.538)! Learning rate decreased to 0.02070.
2024-12-01-16:13:56-root-INFO: grad norm: 19.160 18.673 4.292
2024-12-01-16:13:57-root-INFO: grad norm: 15.294 14.948 3.234
2024-12-01-16:13:57-root-INFO: grad norm: 14.046 13.641 3.350
2024-12-01-16:13:57-root-INFO: Loss Change: 377.895 -> 370.206
2024-12-01-16:13:57-root-INFO: Regularization Change: 0.000 -> 0.912
2024-12-01-16:13:57-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-01-16:13:57-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-16:13:58-root-INFO: step: 115 lr_xt 0.03335113
2024-12-01-16:13:58-root-INFO: grad norm: 17.507 17.159 3.475
2024-12-01-16:13:58-root-INFO: Loss too large (371.072->371.994)! Learning rate decreased to 0.02668.
2024-12-01-16:13:59-root-INFO: grad norm: 20.208 19.753 4.266
2024-12-01-16:13:59-root-INFO: grad norm: 24.791 24.364 4.581
2024-12-01-16:13:59-root-INFO: Loss too large (369.355->369.845)! Learning rate decreased to 0.02134.
2024-12-01-16:14:00-root-INFO: grad norm: 19.963 19.487 4.329
2024-12-01-16:14:00-root-INFO: grad norm: 15.569 15.238 3.196
2024-12-01-16:14:01-root-INFO: Loss Change: 371.072 -> 363.411
2024-12-01-16:14:01-root-INFO: Regularization Change: 0.000 -> 0.940
2024-12-01-16:14:01-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-01-16:14:01-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-16:14:01-root-INFO: step: 114 lr_xt 0.03438473
2024-12-01-16:14:01-root-INFO: grad norm: 13.190 12.821 3.096
2024-12-01-16:14:01-root-INFO: grad norm: 20.309 19.957 3.767
2024-12-01-16:14:02-root-INFO: Loss too large (362.244->364.215)! Learning rate decreased to 0.02751.
2024-12-01-16:14:02-root-INFO: grad norm: 20.909 20.404 4.565
2024-12-01-16:14:03-root-INFO: grad norm: 21.542 21.160 4.036
2024-12-01-16:14:03-root-INFO: grad norm: 21.571 21.042 4.749
2024-12-01-16:14:03-root-INFO: Loss Change: 363.045 -> 356.541
2024-12-01-16:14:03-root-INFO: Regularization Change: 0.000 -> 1.330
2024-12-01-16:14:03-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-01-16:14:03-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-16:14:04-root-INFO: step: 113 lr_xt 0.03544467
2024-12-01-16:14:04-root-INFO: grad norm: 25.340 24.919 4.602
2024-12-01-16:14:04-root-INFO: Loss too large (357.362->360.877)! Learning rate decreased to 0.02836.
2024-12-01-16:14:05-root-INFO: grad norm: 24.438 23.896 5.122
2024-12-01-16:14:05-root-INFO: grad norm: 22.978 22.595 4.178
2024-12-01-16:14:05-root-INFO: grad norm: 22.704 22.188 4.814
2024-12-01-16:14:06-root-INFO: grad norm: 22.127 21.754 4.046
2024-12-01-16:14:06-root-INFO: Loss Change: 357.362 -> 349.594
2024-12-01-16:14:06-root-INFO: Regularization Change: 0.000 -> 1.477
2024-12-01-16:14:06-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-01-16:14:06-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-16:14:07-root-INFO: step: 112 lr_xt 0.03653141
2024-12-01-16:14:07-root-INFO: grad norm: 19.440 18.940 4.380
2024-12-01-16:14:07-root-INFO: Loss too large (348.437->348.597)! Learning rate decreased to 0.02923.
2024-12-01-16:14:07-root-INFO: grad norm: 19.020 18.670 3.631
2024-12-01-16:14:08-root-INFO: grad norm: 18.969 18.484 4.265
2024-12-01-16:14:08-root-INFO: grad norm: 18.817 18.470 3.598
2024-12-01-16:14:09-root-INFO: grad norm: 18.750 18.269 4.222
2024-12-01-16:14:09-root-INFO: Loss Change: 348.437 -> 340.319
2024-12-01-16:14:09-root-INFO: Regularization Change: 0.000 -> 1.357
2024-12-01-16:14:09-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-01-16:14:09-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-16:14:09-root-INFO: step: 111 lr_xt 0.03764541
2024-12-01-16:14:10-root-INFO: grad norm: 22.504 22.143 4.016
2024-12-01-16:14:10-root-INFO: Loss too large (341.279->343.408)! Learning rate decreased to 0.03012.
2024-12-01-16:14:10-root-INFO: grad norm: 21.516 21.017 4.603
2024-12-01-16:14:11-root-INFO: grad norm: 20.329 19.993 3.682
2024-12-01-16:14:11-root-INFO: grad norm: 19.605 19.137 4.260
2024-12-01-16:14:12-root-INFO: grad norm: 18.789 18.464 3.480
2024-12-01-16:14:12-root-INFO: Loss Change: 341.279 -> 333.208
2024-12-01-16:14:12-root-INFO: Regularization Change: 0.000 -> 1.413
2024-12-01-16:14:12-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-01-16:14:12-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-16:14:12-root-INFO: step: 110 lr_xt 0.03878715
2024-12-01-16:14:12-root-INFO: grad norm: 16.136 15.707 3.698
2024-12-01-16:14:13-root-INFO: grad norm: 22.081 21.714 4.008
2024-12-01-16:14:13-root-INFO: Loss too large (332.218->333.974)! Learning rate decreased to 0.03103.
2024-12-01-16:14:14-root-INFO: grad norm: 20.257 19.767 4.428
2024-12-01-16:14:14-root-INFO: grad norm: 18.425 18.103 3.429
2024-12-01-16:14:15-root-INFO: grad norm: 17.485 17.064 3.818
2024-12-01-16:14:15-root-INFO: Loss Change: 332.395 -> 325.219
2024-12-01-16:14:15-root-INFO: Regularization Change: 0.000 -> 1.406
2024-12-01-16:14:15-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-01-16:14:15-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-16:14:15-root-INFO: step: 109 lr_xt 0.03995709
2024-12-01-16:14:15-root-INFO: grad norm: 20.781 20.471 3.576
2024-12-01-16:14:15-root-INFO: Loss too large (326.497->327.905)! Learning rate decreased to 0.03197.
2024-12-01-16:14:16-root-INFO: grad norm: 19.621 19.235 3.872
2024-12-01-16:14:16-root-INFO: grad norm: 18.698 18.416 3.233
2024-12-01-16:14:17-root-INFO: grad norm: 17.984 17.623 3.586
2024-12-01-16:14:17-root-INFO: grad norm: 17.336 17.069 3.027
2024-12-01-16:14:18-root-INFO: Loss Change: 326.497 -> 318.681
2024-12-01-16:14:18-root-INFO: Regularization Change: 0.000 -> 1.397
2024-12-01-16:14:18-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-01-16:14:18-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-16:14:18-root-INFO: step: 108 lr_xt 0.04115569
2024-12-01-16:14:18-root-INFO: grad norm: 14.882 14.544 3.155
2024-12-01-16:14:18-root-INFO: grad norm: 20.047 19.760 3.377
2024-12-01-16:14:19-root-INFO: Loss too large (317.568->318.996)! Learning rate decreased to 0.03292.
2024-12-01-16:14:19-root-INFO: grad norm: 18.761 18.402 3.650
2024-12-01-16:14:20-root-INFO: grad norm: 17.628 17.370 3.003
2024-12-01-16:14:20-root-INFO: grad norm: 16.826 16.515 3.221
2024-12-01-16:14:20-root-INFO: Loss Change: 317.750 -> 311.361
2024-12-01-16:14:20-root-INFO: Regularization Change: 0.000 -> 1.389
2024-12-01-16:14:20-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-01-16:14:20-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-16:14:21-root-INFO: step: 107 lr_xt 0.04238344
2024-12-01-16:14:21-root-INFO: grad norm: 21.351 21.108 3.212
2024-12-01-16:14:21-root-INFO: Loss too large (313.015->315.025)! Learning rate decreased to 0.03391.
2024-12-01-16:14:21-root-INFO: grad norm: 20.361 20.073 3.413
2024-12-01-16:14:22-root-INFO: grad norm: 19.458 19.229 2.978
2024-12-01-16:14:22-root-INFO: grad norm: 18.706 18.425 3.229
2024-12-01-16:14:23-root-INFO: grad norm: 18.028 17.807 2.811
2024-12-01-16:14:23-root-INFO: Loss Change: 313.015 -> 305.518
2024-12-01-16:14:23-root-INFO: Regularization Change: 0.000 -> 1.466
2024-12-01-16:14:23-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-01-16:14:23-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-16:14:23-root-INFO: step: 106 lr_xt 0.04364080
2024-12-01-16:14:24-root-INFO: grad norm: 14.688 14.457 2.597
2024-12-01-16:14:24-root-INFO: Loss too large (304.225->304.316)! Learning rate decreased to 0.03491.
2024-12-01-16:14:24-root-INFO: grad norm: 14.476 14.282 2.363
2024-12-01-16:14:25-root-INFO: grad norm: 14.607 14.400 2.451
2024-12-01-16:14:25-root-INFO: grad norm: 14.780 14.604 2.275
2024-12-01-16:14:26-root-INFO: grad norm: 14.949 14.756 2.391
2024-12-01-16:14:26-root-INFO: Loss Change: 304.225 -> 298.236
2024-12-01-16:14:26-root-INFO: Regularization Change: 0.000 -> 1.292
2024-12-01-16:14:26-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-01-16:14:26-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-16:14:26-root-INFO: step: 105 lr_xt 0.04492824
2024-12-01-16:14:27-root-INFO: grad norm: 18.760 18.572 2.652
2024-12-01-16:14:27-root-INFO: Loss too large (299.496->301.177)! Learning rate decreased to 0.03594.
2024-12-01-16:14:27-root-INFO: grad norm: 18.518 18.336 2.595
2024-12-01-16:14:28-root-INFO: grad norm: 18.162 17.997 2.440
2024-12-01-16:14:28-root-INFO: grad norm: 17.749 17.568 2.534
2024-12-01-16:14:29-root-INFO: grad norm: 17.348 17.189 2.342
2024-12-01-16:14:29-root-INFO: Loss Change: 299.496 -> 293.110
2024-12-01-16:14:29-root-INFO: Regularization Change: 0.000 -> 1.434
2024-12-01-16:14:29-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-01-16:14:29-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-16:14:29-root-INFO: step: 104 lr_xt 0.04624623
2024-12-01-16:14:29-root-INFO: grad norm: 14.427 14.257 2.207
2024-12-01-16:14:30-root-INFO: Loss too large (292.163->292.525)! Learning rate decreased to 0.03700.
2024-12-01-16:14:30-root-INFO: grad norm: 14.336 14.189 2.049
2024-12-01-16:14:30-root-INFO: grad norm: 14.527 14.376 2.088
2024-12-01-16:14:31-root-INFO: grad norm: 14.680 14.543 1.998
2024-12-01-16:14:31-root-INFO: grad norm: 14.793 14.649 2.061
2024-12-01-16:14:32-root-INFO: Loss Change: 292.163 -> 286.633
2024-12-01-16:14:32-root-INFO: Regularization Change: 0.000 -> 1.311
2024-12-01-16:14:32-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-01-16:14:32-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-16:14:32-root-INFO: step: 103 lr_xt 0.04759523
2024-12-01-16:14:32-root-INFO: grad norm: 17.787 17.650 2.199
2024-12-01-16:14:32-root-INFO: Loss too large (287.519->288.767)! Learning rate decreased to 0.03808.
2024-12-01-16:14:33-root-INFO: grad norm: 17.063 16.924 2.170
2024-12-01-16:14:33-root-INFO: grad norm: 16.393 16.265 2.039
2024-12-01-16:14:34-root-INFO: grad norm: 15.783 15.641 2.112
2024-12-01-16:14:34-root-INFO: grad norm: 15.308 15.180 1.974
2024-12-01-16:14:34-root-INFO: Loss Change: 287.519 -> 280.940
2024-12-01-16:14:34-root-INFO: Regularization Change: 0.000 -> 1.413
2024-12-01-16:14:34-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-01-16:14:34-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-16:14:35-root-INFO: step: 102 lr_xt 0.04897571
2024-12-01-16:14:35-root-INFO: grad norm: 13.411 13.283 1.849
2024-12-01-16:14:35-root-INFO: Loss too large (280.141->280.537)! Learning rate decreased to 0.03918.
2024-12-01-16:14:35-root-INFO: grad norm: 13.540 13.416 1.826
2024-12-01-16:14:36-root-INFO: grad norm: 13.887 13.770 1.792
2024-12-01-16:14:36-root-INFO: grad norm: 14.114 13.999 1.794
2024-12-01-16:14:37-root-INFO: grad norm: 14.291 14.178 1.794
2024-12-01-16:14:37-root-INFO: Loss Change: 280.141 -> 275.107
2024-12-01-16:14:37-root-INFO: Regularization Change: 0.000 -> 1.324
2024-12-01-16:14:37-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-01-16:14:37-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-16:14:37-root-INFO: step: 101 lr_xt 0.05038813
2024-12-01-16:14:38-root-INFO: grad norm: 17.215 17.082 2.131
2024-12-01-16:14:38-root-INFO: Loss too large (276.102->277.287)! Learning rate decreased to 0.04031.
2024-12-01-16:14:38-root-INFO: grad norm: 16.530 16.413 1.968
2024-12-01-16:14:39-root-INFO: grad norm: 15.896 15.780 1.915
2024-12-01-16:14:39-root-INFO: grad norm: 15.280 15.158 1.927
2024-12-01-16:14:40-root-INFO: grad norm: 14.823 14.707 1.851
2024-12-01-16:14:40-root-INFO: Loss Change: 276.102 -> 269.677
2024-12-01-16:14:40-root-INFO: Regularization Change: 0.000 -> 1.449
2024-12-01-16:14:40-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-01-16:14:40-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-16:14:40-root-INFO: step: 100 lr_xt 0.05183295
2024-12-01-16:14:40-root-INFO: grad norm: 12.648 12.528 1.739
2024-12-01-16:14:41-root-INFO: Loss too large (269.119->269.481)! Learning rate decreased to 0.04147.
2024-12-01-16:14:41-root-INFO: grad norm: 12.549 12.432 1.711
2024-12-01-16:14:42-root-INFO: grad norm: 12.668 12.554 1.692
2024-12-01-16:14:42-root-INFO: grad norm: 12.769 12.657 1.688
2024-12-01-16:14:43-root-INFO: grad norm: 12.874 12.763 1.682
2024-12-01-16:14:43-root-INFO: Loss Change: 269.119 -> 264.206
2024-12-01-16:14:43-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-01-16:14:43-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-01-16:14:43-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-16:14:43-root-INFO: step: 99 lr_xt 0.05331064
2024-12-01-16:14:43-root-INFO: grad norm: 15.271 15.147 1.944
2024-12-01-16:14:43-root-INFO: Loss too large (264.800->265.545)! Learning rate decreased to 0.04265.
2024-12-01-16:14:44-root-INFO: grad norm: 14.747 14.634 1.818
2024-12-01-16:14:44-root-INFO: grad norm: 14.267 14.157 1.771
2024-12-01-16:14:45-root-INFO: grad norm: 13.768 13.655 1.760
2024-12-01-16:14:45-root-INFO: grad norm: 13.409 13.300 1.707
2024-12-01-16:14:46-root-INFO: Loss Change: 264.800 -> 258.776
2024-12-01-16:14:46-root-INFO: Regularization Change: 0.000 -> 1.418
2024-12-01-16:14:46-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-01-16:14:46-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-16:14:46-root-INFO: step: 98 lr_xt 0.05482165
2024-12-01-16:14:46-root-INFO: grad norm: 11.544 11.433 1.599
2024-12-01-16:14:46-root-INFO: Loss too large (258.307->258.526)! Learning rate decreased to 0.04386.
2024-12-01-16:14:46-root-INFO: grad norm: 11.515 11.404 1.596
2024-12-01-16:14:47-root-INFO: grad norm: 11.733 11.631 1.543
2024-12-01-16:14:47-root-INFO: grad norm: 11.892 11.788 1.571
2024-12-01-16:14:48-root-INFO: grad norm: 12.059 11.961 1.538
2024-12-01-16:14:48-root-INFO: Loss Change: 258.307 -> 253.721
2024-12-01-16:14:48-root-INFO: Regularization Change: 0.000 -> 1.318
2024-12-01-16:14:48-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-01-16:14:48-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-16:14:48-root-INFO: step: 97 lr_xt 0.05636643
2024-12-01-16:14:49-root-INFO: grad norm: 14.704 14.587 1.852
2024-12-01-16:14:49-root-INFO: Loss too large (254.329->255.039)! Learning rate decreased to 0.04509.
2024-12-01-16:14:49-root-INFO: grad norm: 14.306 14.212 1.645
2024-12-01-16:14:50-root-INFO: grad norm: 13.865 13.768 1.639
2024-12-01-16:14:50-root-INFO: grad norm: 13.336 13.241 1.593
2024-12-01-16:14:51-root-INFO: grad norm: 12.983 12.888 1.573
2024-12-01-16:14:51-root-INFO: Loss Change: 254.329 -> 248.479
2024-12-01-16:14:51-root-INFO: Regularization Change: 0.000 -> 1.443
2024-12-01-16:14:51-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-01-16:14:51-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-16:14:51-root-INFO: step: 96 lr_xt 0.05794543
2024-12-01-16:14:51-root-INFO: grad norm: 11.091 10.991 1.486
2024-12-01-16:14:52-root-INFO: Loss too large (247.966->248.218)! Learning rate decreased to 0.04636.
2024-12-01-16:14:52-root-INFO: grad norm: 10.850 10.747 1.498
2024-12-01-16:14:52-root-INFO: grad norm: 10.795 10.699 1.433
2024-12-01-16:14:53-root-INFO: grad norm: 10.808 10.709 1.463
2024-12-01-16:14:53-root-INFO: grad norm: 10.852 10.758 1.420
2024-12-01-16:14:54-root-INFO: Loss Change: 247.966 -> 243.446
2024-12-01-16:14:54-root-INFO: Regularization Change: 0.000 -> 1.307
2024-12-01-16:14:54-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-01-16:14:54-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-16:14:54-root-INFO: step: 95 lr_xt 0.05955910
2024-12-01-16:14:54-root-INFO: grad norm: 13.235 13.117 1.765
2024-12-01-16:14:54-root-INFO: Loss too large (244.309->244.751)! Learning rate decreased to 0.04765.
2024-12-01-16:14:55-root-INFO: grad norm: 12.868 12.779 1.510
2024-12-01-16:14:55-root-INFO: grad norm: 12.507 12.414 1.522
2024-12-01-16:14:56-root-INFO: grad norm: 12.069 11.983 1.441
2024-12-01-16:14:56-root-INFO: grad norm: 11.789 11.699 1.449
2024-12-01-16:14:56-root-INFO: Loss Change: 244.309 -> 238.882
2024-12-01-16:14:56-root-INFO: Regularization Change: 0.000 -> 1.400
2024-12-01-16:14:56-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-01-16:14:56-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-16:14:57-root-INFO: step: 94 lr_xt 0.06120788
2024-12-01-16:14:57-root-INFO: grad norm: 9.984 9.887 1.385
2024-12-01-16:14:57-root-INFO: Loss too large (238.241->238.415)! Learning rate decreased to 0.04897.
2024-12-01-16:14:57-root-INFO: grad norm: 9.983 9.890 1.362
2024-12-01-16:14:58-root-INFO: grad norm: 10.248 10.163 1.315
2024-12-01-16:14:58-root-INFO: grad norm: 10.444 10.357 1.347
2024-12-01-16:14:59-root-INFO: grad norm: 10.692 10.612 1.303
2024-12-01-16:14:59-root-INFO: Loss Change: 238.241 -> 234.320
2024-12-01-16:14:59-root-INFO: Regularization Change: 0.000 -> 1.300
2024-12-01-16:14:59-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-01-16:14:59-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-16:14:59-root-INFO: step: 93 lr_xt 0.06289219
2024-12-01-16:15:00-root-INFO: grad norm: 13.068 12.964 1.646
2024-12-01-16:15:00-root-INFO: Loss too large (234.872->235.418)! Learning rate decreased to 0.05031.
2024-12-01-16:15:00-root-INFO: grad norm: 12.840 12.766 1.382
2024-12-01-16:15:01-root-INFO: grad norm: 12.464 12.384 1.409
2024-12-01-16:15:01-root-INFO: grad norm: 11.928 11.856 1.311
2024-12-01-16:15:02-root-INFO: grad norm: 11.612 11.535 1.335
2024-12-01-16:15:02-root-INFO: Loss Change: 234.872 -> 229.553
2024-12-01-16:15:02-root-INFO: Regularization Change: 0.000 -> 1.430
2024-12-01-16:15:02-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-01-16:15:02-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-16:15:02-root-INFO: step: 92 lr_xt 0.06461248
2024-12-01-16:15:02-root-INFO: grad norm: 10.037 9.953 1.299
2024-12-01-16:15:03-root-INFO: Loss too large (229.187->229.483)! Learning rate decreased to 0.05169.
2024-12-01-16:15:03-root-INFO: grad norm: 9.777 9.696 1.257
2024-12-01-16:15:04-root-INFO: grad norm: 9.719 9.643 1.213
2024-12-01-16:15:04-root-INFO: grad norm: 9.740 9.663 1.225
2024-12-01-16:15:05-root-INFO: grad norm: 9.788 9.715 1.191
2024-12-01-16:15:05-root-INFO: Loss Change: 229.187 -> 225.240
2024-12-01-16:15:05-root-INFO: Regularization Change: 0.000 -> 1.301
2024-12-01-16:15:05-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-01-16:15:05-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-16:15:05-root-INFO: step: 91 lr_xt 0.06636917
2024-12-01-16:15:05-root-INFO: grad norm: 11.589 11.503 1.416
2024-12-01-16:15:05-root-INFO: Loss too large (225.760->226.032)! Learning rate decreased to 0.05310.
2024-12-01-16:15:06-root-INFO: grad norm: 11.194 11.127 1.223
2024-12-01-16:15:06-root-INFO: grad norm: 10.850 10.779 1.242
2024-12-01-16:15:07-root-INFO: grad norm: 10.416 10.351 1.170
2024-12-01-16:15:07-root-INFO: grad norm: 10.176 10.107 1.189
2024-12-01-16:15:08-root-INFO: Loss Change: 225.760 -> 220.869
2024-12-01-16:15:08-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-01-16:15:08-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-01-16:15:08-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-16:15:08-root-INFO: step: 90 lr_xt 0.06816268
2024-12-01-16:15:08-root-INFO: grad norm: 8.755 8.678 1.163
2024-12-01-16:15:08-root-INFO: Loss too large (220.714->220.771)! Learning rate decreased to 0.05453.
2024-12-01-16:15:09-root-INFO: grad norm: 8.537 8.461 1.133
2024-12-01-16:15:09-root-INFO: grad norm: 8.541 8.472 1.082
2024-12-01-16:15:10-root-INFO: grad norm: 8.602 8.530 1.112
2024-12-01-16:15:10-root-INFO: grad norm: 8.718 8.652 1.067
2024-12-01-16:15:10-root-INFO: Loss Change: 220.714 -> 217.020
2024-12-01-16:15:10-root-INFO: Regularization Change: 0.000 -> 1.260
2024-12-01-16:15:10-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-01-16:15:10-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-16:15:11-root-INFO: step: 89 lr_xt 0.06999342
2024-12-01-16:15:11-root-INFO: grad norm: 11.070 10.983 1.385
2024-12-01-16:15:11-root-INFO: Loss too large (217.507->217.660)! Learning rate decreased to 0.05599.
2024-12-01-16:15:11-root-INFO: grad norm: 10.588 10.528 1.129
2024-12-01-16:15:12-root-INFO: grad norm: 10.193 10.128 1.145
2024-12-01-16:15:12-root-INFO: grad norm: 9.700 9.641 1.069
2024-12-01-16:15:13-root-INFO: grad norm: 9.437 9.374 1.087
2024-12-01-16:15:13-root-INFO: Loss Change: 217.507 -> 212.661
2024-12-01-16:15:13-root-INFO: Regularization Change: 0.000 -> 1.381
2024-12-01-16:15:13-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-01-16:15:13-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-16:15:13-root-INFO: step: 88 lr_xt 0.07186179
2024-12-01-16:15:13-root-INFO: grad norm: 7.971 7.894 1.104
2024-12-01-16:15:14-root-INFO: grad norm: 9.904 9.843 1.096
2024-12-01-16:15:14-root-INFO: grad norm: 13.962 13.918 1.110
2024-12-01-16:15:14-root-INFO: Loss too large (212.322->214.237)! Learning rate decreased to 0.05749.
2024-12-01-16:15:15-root-INFO: grad norm: 11.755 11.700 1.137
2024-12-01-16:15:16-root-INFO: grad norm: 8.699 8.640 1.008
2024-12-01-16:15:16-root-INFO: Loss Change: 212.631 -> 208.795
2024-12-01-16:15:16-root-INFO: Regularization Change: 0.000 -> 1.499
2024-12-01-16:15:16-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-01-16:15:16-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-16:15:16-root-INFO: step: 87 lr_xt 0.07376819
2024-12-01-16:15:16-root-INFO: grad norm: 10.033 9.955 1.243
2024-12-01-16:15:17-root-INFO: grad norm: 13.265 13.215 1.155
2024-12-01-16:15:17-root-INFO: Loss too large (208.712->210.115)! Learning rate decreased to 0.05901.
2024-12-01-16:15:17-root-INFO: grad norm: 11.010 10.952 1.121
2024-12-01-16:15:18-root-INFO: grad norm: 8.295 8.237 0.978
2024-12-01-16:15:18-root-INFO: grad norm: 7.504 7.441 0.969
2024-12-01-16:15:19-root-INFO: Loss Change: 209.088 -> 204.057
2024-12-01-16:15:19-root-INFO: Regularization Change: 0.000 -> 1.450
2024-12-01-16:15:19-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-01-16:15:19-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-16:15:19-root-INFO: step: 86 lr_xt 0.07571301
2024-12-01-16:15:19-root-INFO: grad norm: 5.803 5.722 0.968
2024-12-01-16:15:20-root-INFO: grad norm: 6.649 6.584 0.927
2024-12-01-16:15:20-root-INFO: grad norm: 9.014 8.966 0.931
2024-12-01-16:15:20-root-INFO: Loss too large (202.609->203.004)! Learning rate decreased to 0.06057.
2024-12-01-16:15:21-root-INFO: grad norm: 8.208 8.152 0.951
2024-12-01-16:15:21-root-INFO: grad norm: 7.137 7.081 0.893
2024-12-01-16:15:22-root-INFO: Loss Change: 203.891 -> 200.227
2024-12-01-16:15:22-root-INFO: Regularization Change: 0.000 -> 1.434
2024-12-01-16:15:22-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-01-16:15:22-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-16:15:22-root-INFO: step: 85 lr_xt 0.07769664
2024-12-01-16:15:22-root-INFO: grad norm: 8.295 8.222 1.098
2024-12-01-16:15:23-root-INFO: grad norm: 11.334 11.288 1.017
2024-12-01-16:15:23-root-INFO: Loss too large (200.078->201.171)! Learning rate decreased to 0.06216.
2024-12-01-16:15:23-root-INFO: grad norm: 9.668 9.615 1.011
2024-12-01-16:15:24-root-INFO: grad norm: 7.411 7.357 0.897
2024-12-01-16:15:24-root-INFO: grad norm: 6.797 6.737 0.896
2024-12-01-16:15:25-root-INFO: Loss Change: 200.507 -> 196.108
2024-12-01-16:15:25-root-INFO: Regularization Change: 0.000 -> 1.395
2024-12-01-16:15:25-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-01-16:15:25-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-16:15:25-root-INFO: step: 84 lr_xt 0.07971945
2024-12-01-16:15:25-root-INFO: grad norm: 5.453 5.376 0.911
2024-12-01-16:15:25-root-INFO: grad norm: 6.314 6.255 0.859
2024-12-01-16:15:26-root-INFO: grad norm: 8.633 8.588 0.875
2024-12-01-16:15:26-root-INFO: Loss too large (194.683->195.088)! Learning rate decreased to 0.06378.
2024-12-01-16:15:27-root-INFO: grad norm: 7.783 7.733 0.884
2024-12-01-16:15:27-root-INFO: grad norm: 6.573 6.520 0.835
2024-12-01-16:15:27-root-INFO: Loss Change: 195.879 -> 192.364
2024-12-01-16:15:27-root-INFO: Regularization Change: 0.000 -> 1.431
2024-12-01-16:15:27-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-01-16:15:27-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-16:15:27-root-INFO: step: 83 lr_xt 0.08178179
2024-12-01-16:15:28-root-INFO: grad norm: 7.825 7.757 1.031
2024-12-01-16:15:28-root-INFO: grad norm: 10.439 10.396 0.945
2024-12-01-16:15:28-root-INFO: Loss too large (192.221->193.078)! Learning rate decreased to 0.06543.
2024-12-01-16:15:29-root-INFO: grad norm: 8.794 8.744 0.932
2024-12-01-16:15:29-root-INFO: grad norm: 6.595 6.543 0.830
2024-12-01-16:15:30-root-INFO: grad norm: 6.013 5.956 0.823
2024-12-01-16:15:30-root-INFO: Loss Change: 192.731 -> 188.457
2024-12-01-16:15:30-root-INFO: Regularization Change: 0.000 -> 1.390
2024-12-01-16:15:30-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-01-16:15:30-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-16:15:30-root-INFO: step: 82 lr_xt 0.08388403
2024-12-01-16:15:30-root-INFO: grad norm: 4.935 4.855 0.882
2024-12-01-16:15:31-root-INFO: grad norm: 5.490 5.432 0.798
2024-12-01-16:15:31-root-INFO: grad norm: 7.257 7.212 0.810
2024-12-01-16:15:32-root-INFO: Loss too large (187.077->187.140)! Learning rate decreased to 0.06711.
2024-12-01-16:15:32-root-INFO: grad norm: 6.599 6.549 0.809
2024-12-01-16:15:33-root-INFO: grad norm: 5.712 5.659 0.776
2024-12-01-16:15:33-root-INFO: Loss Change: 188.404 -> 184.937
2024-12-01-16:15:33-root-INFO: Regularization Change: 0.000 -> 1.423
2024-12-01-16:15:33-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-01-16:15:33-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-16:15:33-root-INFO: step: 81 lr_xt 0.08602650
2024-12-01-16:15:33-root-INFO: grad norm: 6.875 6.805 0.975
2024-12-01-16:15:34-root-INFO: grad norm: 9.042 9.002 0.852
2024-12-01-16:15:34-root-INFO: Loss too large (184.537->185.045)! Learning rate decreased to 0.06882.
2024-12-01-16:15:34-root-INFO: grad norm: 7.702 7.654 0.857
2024-12-01-16:15:35-root-INFO: grad norm: 5.940 5.891 0.766
2024-12-01-16:15:35-root-INFO: grad norm: 5.441 5.386 0.769
2024-12-01-16:15:36-root-INFO: Loss Change: 185.107 -> 181.138
2024-12-01-16:15:36-root-INFO: Regularization Change: 0.000 -> 1.377
2024-12-01-16:15:36-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-01-16:15:36-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-16:15:36-root-INFO: step: 80 lr_xt 0.08820955
2024-12-01-16:15:36-root-INFO: grad norm: 4.440 4.360 0.841
2024-12-01-16:15:37-root-INFO: grad norm: 4.501 4.440 0.736
2024-12-01-16:15:37-root-INFO: grad norm: 5.457 5.406 0.742
2024-12-01-16:15:38-root-INFO: grad norm: 6.321 6.275 0.754
2024-12-01-16:15:38-root-INFO: grad norm: 8.340 8.305 0.767
2024-12-01-16:15:38-root-INFO: Loss too large (178.565->179.010)! Learning rate decreased to 0.07057.
2024-12-01-16:15:39-root-INFO: Loss Change: 181.108 -> 178.166
2024-12-01-16:15:39-root-INFO: Regularization Change: 0.000 -> 1.788
2024-12-01-16:15:39-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-01-16:15:39-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-16:15:39-root-INFO: step: 79 lr_xt 0.09043348
2024-12-01-16:15:39-root-INFO: grad norm: 7.893 7.839 0.918
2024-12-01-16:15:39-root-INFO: grad norm: 8.991 8.953 0.827
2024-12-01-16:15:40-root-INFO: Loss too large (177.251->177.600)! Learning rate decreased to 0.07235.
2024-12-01-16:15:40-root-INFO: grad norm: 7.065 7.020 0.797
2024-12-01-16:15:41-root-INFO: grad norm: 4.730 4.676 0.714
2024-12-01-16:15:41-root-INFO: grad norm: 4.216 4.156 0.704
2024-12-01-16:15:42-root-INFO: Loss Change: 178.396 -> 173.720
2024-12-01-16:15:42-root-INFO: Regularization Change: 0.000 -> 1.472
2024-12-01-16:15:42-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-01-16:15:42-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-16:15:42-root-INFO: step: 78 lr_xt 0.09269861
2024-12-01-16:15:42-root-INFO: grad norm: 3.627 3.548 0.752
2024-12-01-16:15:42-root-INFO: grad norm: 3.604 3.539 0.680
2024-12-01-16:15:43-root-INFO: grad norm: 4.004 3.945 0.687
2024-12-01-16:15:44-root-INFO: grad norm: 4.525 4.473 0.683
2024-12-01-16:15:44-root-INFO: grad norm: 5.771 5.730 0.691
2024-12-01-16:15:44-root-INFO: Loss Change: 173.786 -> 170.804
2024-12-01-16:15:44-root-INFO: Regularization Change: 0.000 -> 1.910
2024-12-01-16:15:44-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-01-16:15:44-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-16:15:45-root-INFO: step: 77 lr_xt 0.09500525
2024-12-01-16:15:45-root-INFO: grad norm: 7.127 7.077 0.840
2024-12-01-16:15:45-root-INFO: grad norm: 7.761 7.722 0.772
2024-12-01-16:15:46-root-INFO: Loss too large (169.753->169.767)! Learning rate decreased to 0.07600.
2024-12-01-16:15:46-root-INFO: grad norm: 6.023 5.980 0.720
2024-12-01-16:15:47-root-INFO: grad norm: 4.104 4.049 0.668
2024-12-01-16:15:47-root-INFO: grad norm: 3.650 3.591 0.651
2024-12-01-16:15:47-root-INFO: Loss Change: 170.876 -> 166.527
2024-12-01-16:15:47-root-INFO: Regularization Change: 0.000 -> 1.446
2024-12-01-16:15:47-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-01-16:15:47-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-16:15:48-root-INFO: step: 76 lr_xt 0.09735366
2024-12-01-16:15:48-root-INFO: grad norm: 3.607 3.526 0.760
2024-12-01-16:15:48-root-INFO: grad norm: 3.254 3.189 0.647
2024-12-01-16:15:49-root-INFO: grad norm: 3.197 3.130 0.649
2024-12-01-16:15:49-root-INFO: grad norm: 3.254 3.192 0.632
2024-12-01-16:15:50-root-INFO: grad norm: 3.484 3.427 0.631
2024-12-01-16:15:50-root-INFO: Loss Change: 166.481 -> 162.816
2024-12-01-16:15:50-root-INFO: Regularization Change: 0.000 -> 1.849
2024-12-01-16:15:50-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-01-16:15:50-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-16:15:50-root-INFO: step: 75 lr_xt 0.09974414
2024-12-01-16:15:50-root-INFO: grad norm: 4.633 4.574 0.740
2024-12-01-16:15:51-root-INFO: grad norm: 5.387 5.347 0.657
2024-12-01-16:15:51-root-INFO: grad norm: 5.795 5.755 0.687
2024-12-01-16:15:52-root-INFO: grad norm: 6.525 6.491 0.671
2024-12-01-16:15:52-root-INFO: grad norm: 6.535 6.497 0.708
2024-12-01-16:15:53-root-INFO: Loss Change: 162.939 -> 159.639
2024-12-01-16:15:53-root-INFO: Regularization Change: 0.000 -> 1.974
2024-12-01-16:15:53-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-01-16:15:53-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-16:15:53-root-INFO: step: 74 lr_xt 0.10217692
2024-12-01-16:15:53-root-INFO: grad norm: 5.272 5.231 0.658
2024-12-01-16:15:53-root-INFO: grad norm: 5.082 5.041 0.642
2024-12-01-16:15:54-root-INFO: grad norm: 4.905 4.866 0.620
2024-12-01-16:15:54-root-INFO: grad norm: 4.891 4.849 0.635
2024-12-01-16:15:55-root-INFO: grad norm: 4.863 4.824 0.614
2024-12-01-16:15:55-root-INFO: Loss Change: 159.408 -> 156.037
2024-12-01-16:15:55-root-INFO: Regularization Change: 0.000 -> 1.976
2024-12-01-16:15:55-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-01-16:15:55-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-16:15:55-root-INFO: step: 73 lr_xt 0.10465226
2024-12-01-16:15:55-root-INFO: grad norm: 6.003 5.947 0.822
2024-12-01-16:15:56-root-INFO: grad norm: 5.625 5.585 0.664
2024-12-01-16:15:56-root-INFO: grad norm: 5.448 5.404 0.692
2024-12-01-16:15:57-root-INFO: grad norm: 5.224 5.185 0.644
2024-12-01-16:15:57-root-INFO: grad norm: 5.105 5.061 0.662
2024-12-01-16:15:58-root-INFO: Loss Change: 156.325 -> 152.409
2024-12-01-16:15:58-root-INFO: Regularization Change: 0.000 -> 2.070
2024-12-01-16:15:58-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-01-16:15:58-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-16:15:58-root-INFO: step: 72 lr_xt 0.10717038
2024-12-01-16:15:58-root-INFO: grad norm: 3.990 3.944 0.606
2024-12-01-16:15:59-root-INFO: grad norm: 3.803 3.758 0.581
2024-12-01-16:15:59-root-INFO: grad norm: 3.735 3.692 0.565
2024-12-01-16:16:00-root-INFO: grad norm: 3.732 3.688 0.571
2024-12-01-16:16:00-root-INFO: grad norm: 3.752 3.710 0.560
2024-12-01-16:16:01-root-INFO: Loss Change: 152.067 -> 148.721
2024-12-01-16:16:01-root-INFO: Regularization Change: 0.000 -> 1.904
2024-12-01-16:16:01-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-01-16:16:01-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-16:16:01-root-INFO: step: 71 lr_xt 0.10973151
2024-12-01-16:16:01-root-INFO: grad norm: 4.581 4.530 0.682
2024-12-01-16:16:02-root-INFO: grad norm: 4.354 4.314 0.594
2024-12-01-16:16:02-root-INFO: grad norm: 4.214 4.171 0.605
2024-12-01-16:16:03-root-INFO: grad norm: 4.103 4.062 0.578
2024-12-01-16:16:03-root-INFO: grad norm: 4.029 3.986 0.587
2024-12-01-16:16:03-root-INFO: Loss Change: 148.933 -> 145.472
2024-12-01-16:16:03-root-INFO: Regularization Change: 0.000 -> 1.944
2024-12-01-16:16:03-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-01-16:16:03-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-16:16:04-root-INFO: step: 70 lr_xt 0.11233583
2024-12-01-16:16:04-root-INFO: grad norm: 3.569 3.518 0.605
2024-12-01-16:16:04-root-INFO: grad norm: 3.189 3.143 0.538
2024-12-01-16:16:05-root-INFO: grad norm: 3.033 2.989 0.517
2024-12-01-16:16:05-root-INFO: grad norm: 2.971 2.925 0.523
2024-12-01-16:16:06-root-INFO: grad norm: 2.927 2.882 0.510
2024-12-01-16:16:06-root-INFO: Loss Change: 145.178 -> 141.803
2024-12-01-16:16:06-root-INFO: Regularization Change: 0.000 -> 1.894
2024-12-01-16:16:06-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-01-16:16:06-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-16:16:06-root-INFO: step: 69 lr_xt 0.11498353
2024-12-01-16:16:06-root-INFO: grad norm: 3.735 3.680 0.634
2024-12-01-16:16:07-root-INFO: grad norm: 3.572 3.530 0.542
2024-12-01-16:16:08-root-INFO: grad norm: 3.532 3.488 0.561
2024-12-01-16:16:08-root-INFO: grad norm: 3.481 3.440 0.536
2024-12-01-16:16:09-root-INFO: grad norm: 3.445 3.402 0.544
2024-12-01-16:16:09-root-INFO: Loss Change: 142.000 -> 138.771
2024-12-01-16:16:09-root-INFO: Regularization Change: 0.000 -> 1.920
2024-12-01-16:16:09-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-01-16:16:09-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-16:16:09-root-INFO: step: 68 lr_xt 0.11767478
2024-12-01-16:16:09-root-INFO: grad norm: 3.342 3.290 0.588
2024-12-01-16:16:10-root-INFO: grad norm: 2.897 2.852 0.510
2024-12-01-16:16:10-root-INFO: grad norm: 2.751 2.707 0.488
2024-12-01-16:16:11-root-INFO: grad norm: 2.702 2.657 0.494
2024-12-01-16:16:11-root-INFO: grad norm: 2.674 2.629 0.484
2024-12-01-16:16:12-root-INFO: Loss Change: 138.555 -> 135.230
2024-12-01-16:16:12-root-INFO: Regularization Change: 0.000 -> 1.928
2024-12-01-16:16:12-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-01-16:16:12-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-16:16:12-root-INFO: step: 67 lr_xt 0.12040972
2024-12-01-16:16:12-root-INFO: grad norm: 3.438 3.384 0.610
2024-12-01-16:16:12-root-INFO: grad norm: 3.240 3.198 0.518
2024-12-01-16:16:13-root-INFO: grad norm: 3.178 3.134 0.526
2024-12-01-16:16:13-root-INFO: grad norm: 3.114 3.072 0.509
2024-12-01-16:16:14-root-INFO: grad norm: 3.052 3.010 0.508
2024-12-01-16:16:14-root-INFO: Loss Change: 135.229 -> 132.071
2024-12-01-16:16:14-root-INFO: Regularization Change: 0.000 -> 1.955
2024-12-01-16:16:14-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-01-16:16:14-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-16:16:14-root-INFO: step: 66 lr_xt 0.12318848
2024-12-01-16:16:15-root-INFO: grad norm: 2.801 2.756 0.505
2024-12-01-16:16:15-root-INFO: grad norm: 2.666 2.623 0.474
2024-12-01-16:16:16-root-INFO: grad norm: 2.666 2.623 0.474
2024-12-01-16:16:16-root-INFO: grad norm: 2.733 2.692 0.472
2024-12-01-16:16:17-root-INFO: grad norm: 2.764 2.723 0.477
2024-12-01-16:16:17-root-INFO: Loss Change: 132.006 -> 128.931
2024-12-01-16:16:17-root-INFO: Regularization Change: 0.000 -> 1.920
2024-12-01-16:16:17-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-01-16:16:17-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-16:16:17-root-INFO: step: 65 lr_xt 0.12601118
2024-12-01-16:16:17-root-INFO: grad norm: 4.084 4.028 0.677
2024-12-01-16:16:18-root-INFO: grad norm: 3.630 3.589 0.545
2024-12-01-16:16:18-root-INFO: grad norm: 3.137 3.096 0.505
2024-12-01-16:16:19-root-INFO: grad norm: 3.019 2.978 0.495
2024-12-01-16:16:19-root-INFO: grad norm: 2.956 2.917 0.478
2024-12-01-16:16:20-root-INFO: Loss Change: 129.094 -> 125.871
2024-12-01-16:16:20-root-INFO: Regularization Change: 0.000 -> 2.028
2024-12-01-16:16:20-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-01-16:16:20-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-16:16:20-root-INFO: step: 64 lr_xt 0.12887791
2024-12-01-16:16:20-root-INFO: grad norm: 2.831 2.791 0.476
2024-12-01-16:16:21-root-INFO: grad norm: 2.713 2.676 0.446
2024-12-01-16:16:21-root-INFO: grad norm: 2.723 2.685 0.453
2024-12-01-16:16:21-root-INFO: grad norm: 2.810 2.774 0.448
2024-12-01-16:16:22-root-INFO: grad norm: 2.860 2.823 0.460
2024-12-01-16:16:22-root-INFO: Loss Change: 125.782 -> 122.781
2024-12-01-16:16:22-root-INFO: Regularization Change: 0.000 -> 1.977
2024-12-01-16:16:22-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-01-16:16:22-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-16:16:22-root-INFO: step: 63 lr_xt 0.13178874
2024-12-01-16:16:23-root-INFO: grad norm: 3.816 3.764 0.625
2024-12-01-16:16:23-root-INFO: grad norm: 3.509 3.471 0.514
2024-12-01-16:16:24-root-INFO: grad norm: 3.201 3.163 0.489
2024-12-01-16:16:24-root-INFO: grad norm: 3.125 3.087 0.482
2024-12-01-16:16:25-root-INFO: grad norm: 3.074 3.039 0.462
2024-12-01-16:16:25-root-INFO: Loss Change: 122.966 -> 119.920
2024-12-01-16:16:25-root-INFO: Regularization Change: 0.000 -> 2.073
2024-12-01-16:16:25-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-01-16:16:25-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-16:16:25-root-INFO: step: 62 lr_xt 0.13474373
2024-12-01-16:16:25-root-INFO: grad norm: 3.035 2.994 0.497
2024-12-01-16:16:26-root-INFO: grad norm: 2.809 2.775 0.433
2024-12-01-16:16:26-root-INFO: grad norm: 2.804 2.770 0.437
2024-12-01-16:16:27-root-INFO: grad norm: 2.918 2.886 0.429
2024-12-01-16:16:27-root-INFO: grad norm: 2.991 2.957 0.450
2024-12-01-16:16:28-root-INFO: Loss Change: 119.653 -> 116.638
2024-12-01-16:16:28-root-INFO: Regularization Change: 0.000 -> 2.083
2024-12-01-16:16:28-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-01-16:16:28-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-16:16:28-root-INFO: step: 61 lr_xt 0.13774291
2024-12-01-16:16:28-root-INFO: grad norm: 3.790 3.750 0.552
2024-12-01-16:16:28-root-INFO: grad norm: 3.555 3.519 0.502
2024-12-01-16:16:29-root-INFO: grad norm: 3.260 3.227 0.461
2024-12-01-16:16:29-root-INFO: grad norm: 3.200 3.165 0.471
2024-12-01-16:16:30-root-INFO: grad norm: 3.156 3.125 0.442
2024-12-01-16:16:30-root-INFO: Loss Change: 116.737 -> 113.781
2024-12-01-16:16:30-root-INFO: Regularization Change: 0.000 -> 2.145
2024-12-01-16:16:30-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-01-16:16:30-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-16:16:30-root-INFO: step: 60 lr_xt 0.14078630
2024-12-01-16:16:30-root-INFO: grad norm: 3.085 3.052 0.451
2024-12-01-16:16:31-root-INFO: grad norm: 2.908 2.879 0.409
2024-12-01-16:16:31-root-INFO: grad norm: 2.932 2.901 0.422
2024-12-01-16:16:32-root-INFO: grad norm: 3.068 3.040 0.414
2024-12-01-16:16:32-root-INFO: grad norm: 3.153 3.121 0.444
2024-12-01-16:16:33-root-INFO: Loss Change: 113.689 -> 110.764
2024-12-01-16:16:33-root-INFO: Regularization Change: 0.000 -> 2.145
2024-12-01-16:16:33-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-01-16:16:33-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-16:16:33-root-INFO: step: 59 lr_xt 0.14387389
2024-12-01-16:16:33-root-INFO: grad norm: 4.325 4.279 0.625
2024-12-01-16:16:33-root-INFO: grad norm: 3.968 3.933 0.525
2024-12-01-16:16:34-root-INFO: grad norm: 3.533 3.502 0.470
2024-12-01-16:16:34-root-INFO: grad norm: 3.473 3.440 0.478
2024-12-01-16:16:35-root-INFO: grad norm: 3.450 3.422 0.444
2024-12-01-16:16:35-root-INFO: Loss Change: 110.948 -> 107.979
2024-12-01-16:16:35-root-INFO: Regularization Change: 0.000 -> 2.283
2024-12-01-16:16:35-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-01-16:16:35-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-16:16:36-root-INFO: step: 58 lr_xt 0.14700566
2024-12-01-16:16:36-root-INFO: grad norm: 3.289 3.261 0.431
2024-12-01-16:16:36-root-INFO: grad norm: 3.212 3.186 0.404
2024-12-01-16:16:37-root-INFO: grad norm: 3.258 3.229 0.433
2024-12-01-16:16:37-root-INFO: grad norm: 3.385 3.359 0.415
2024-12-01-16:16:38-root-INFO: grad norm: 3.446 3.415 0.457
2024-12-01-16:16:38-root-INFO: Loss Change: 107.728 -> 104.841
2024-12-01-16:16:38-root-INFO: Regularization Change: 0.000 -> 2.252
2024-12-01-16:16:38-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-01-16:16:38-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-16:16:38-root-INFO: step: 57 lr_xt 0.15018154
2024-12-01-16:16:38-root-INFO: grad norm: 4.609 4.568 0.613
2024-12-01-16:16:39-root-INFO: grad norm: 4.175 4.136 0.563
2024-12-01-16:16:39-root-INFO: grad norm: 3.653 3.624 0.459
2024-12-01-16:16:40-root-INFO: grad norm: 3.577 3.543 0.490
2024-12-01-16:16:40-root-INFO: grad norm: 3.544 3.518 0.432
2024-12-01-16:16:41-root-INFO: Loss Change: 104.922 -> 101.910
2024-12-01-16:16:41-root-INFO: Regularization Change: 0.000 -> 2.409
2024-12-01-16:16:41-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-01-16:16:41-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-16:16:41-root-INFO: step: 56 lr_xt 0.15340147
2024-12-01-16:16:41-root-INFO: grad norm: 3.390 3.364 0.422
2024-12-01-16:16:41-root-INFO: grad norm: 3.280 3.257 0.389
2024-12-01-16:16:42-root-INFO: grad norm: 3.328 3.300 0.429
2024-12-01-16:16:42-root-INFO: grad norm: 3.461 3.437 0.402
2024-12-01-16:16:43-root-INFO: grad norm: 3.528 3.499 0.457
2024-12-01-16:16:43-root-INFO: Loss Change: 101.798 -> 98.921
2024-12-01-16:16:43-root-INFO: Regularization Change: 0.000 -> 2.361
2024-12-01-16:16:43-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-01-16:16:43-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-16:16:43-root-INFO: step: 55 lr_xt 0.15666536
2024-12-01-16:16:44-root-INFO: grad norm: 4.554 4.512 0.612
2024-12-01-16:16:44-root-INFO: grad norm: 4.205 4.168 0.555
2024-12-01-16:16:45-root-INFO: grad norm: 3.806 3.778 0.459
2024-12-01-16:16:45-root-INFO: grad norm: 3.748 3.715 0.496
2024-12-01-16:16:46-root-INFO: grad norm: 3.722 3.697 0.434
2024-12-01-16:16:46-root-INFO: Loss Change: 99.085 -> 96.188
2024-12-01-16:16:46-root-INFO: Regularization Change: 0.000 -> 2.515
2024-12-01-16:16:46-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-01-16:16:46-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-16:16:46-root-INFO: step: 54 lr_xt 0.15997308
2024-12-01-16:16:46-root-INFO: grad norm: 3.539 3.514 0.423
2024-12-01-16:16:47-root-INFO: grad norm: 3.535 3.512 0.400
2024-12-01-16:16:47-root-INFO: grad norm: 3.607 3.579 0.444
2024-12-01-16:16:48-root-INFO: grad norm: 3.741 3.718 0.420
2024-12-01-16:16:48-root-INFO: grad norm: 3.793 3.763 0.476
2024-12-01-16:16:48-root-INFO: Loss Change: 95.941 -> 93.118
2024-12-01-16:16:48-root-INFO: Regularization Change: 0.000 -> 2.480
2024-12-01-16:16:48-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-01-16:16:48-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-16:16:49-root-INFO: step: 53 lr_xt 0.16332449
2024-12-01-16:16:49-root-INFO: grad norm: 4.732 4.692 0.611
2024-12-01-16:16:49-root-INFO: grad norm: 4.356 4.318 0.577
2024-12-01-16:16:50-root-INFO: grad norm: 3.927 3.899 0.469
2024-12-01-16:16:50-root-INFO: grad norm: 3.870 3.836 0.517
2024-12-01-16:16:51-root-INFO: grad norm: 3.855 3.829 0.449
2024-12-01-16:16:51-root-INFO: Loss Change: 93.319 -> 90.447
2024-12-01-16:16:51-root-INFO: Regularization Change: 0.000 -> 2.631
2024-12-01-16:16:51-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-01-16:16:51-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-16:16:51-root-INFO: step: 52 lr_xt 0.16671942
2024-12-01-16:16:51-root-INFO: grad norm: 3.720 3.691 0.463
2024-12-01-16:16:52-root-INFO: grad norm: 3.743 3.718 0.431
2024-12-01-16:16:52-root-INFO: grad norm: 3.856 3.823 0.511
2024-12-01-16:16:53-root-INFO: grad norm: 4.018 3.989 0.476
2024-12-01-16:16:53-root-INFO: grad norm: 4.100 4.062 0.554
2024-12-01-16:16:54-root-INFO: Loss Change: 90.275 -> 87.582
2024-12-01-16:16:54-root-INFO: Regularization Change: 0.000 -> 2.598
2024-12-01-16:16:54-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-01-16:16:54-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-16:16:54-root-INFO: step: 51 lr_xt 0.17015769
2024-12-01-16:16:54-root-INFO: grad norm: 4.971 4.923 0.690
2024-12-01-16:16:54-root-INFO: grad norm: 4.624 4.582 0.626
2024-12-01-16:16:55-root-INFO: grad norm: 4.255 4.221 0.537
2024-12-01-16:16:55-root-INFO: grad norm: 4.163 4.124 0.564
2024-12-01-16:16:56-root-INFO: grad norm: 4.088 4.057 0.498
2024-12-01-16:16:56-root-INFO: Loss Change: 87.772 -> 84.904
2024-12-01-16:16:56-root-INFO: Regularization Change: 0.000 -> 2.784
2024-12-01-16:16:56-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-01-16:16:56-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-16:16:56-root-INFO: step: 50 lr_xt 0.17363908
2024-12-01-16:16:57-root-INFO: grad norm: 3.837 3.805 0.488
2024-12-01-16:16:57-root-INFO: grad norm: 3.812 3.786 0.449
2024-12-01-16:16:58-root-INFO: grad norm: 3.868 3.835 0.500
2024-12-01-16:16:58-root-INFO: grad norm: 3.970 3.943 0.461
2024-12-01-16:16:59-root-INFO: grad norm: 4.010 3.976 0.521
2024-12-01-16:16:59-root-INFO: Loss Change: 84.480 -> 81.817
2024-12-01-16:16:59-root-INFO: Regularization Change: 0.000 -> 2.651
2024-12-01-16:16:59-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-01-16:16:59-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-16:16:59-root-INFO: step: 49 lr_xt 0.17716334
2024-12-01-16:16:59-root-INFO: grad norm: 4.907 4.867 0.631
2024-12-01-16:17:00-root-INFO: grad norm: 4.464 4.423 0.597
2024-12-01-16:17:00-root-INFO: grad norm: 4.032 4.004 0.481
2024-12-01-16:17:01-root-INFO: grad norm: 3.932 3.896 0.528
2024-12-01-16:17:01-root-INFO: grad norm: 3.863 3.836 0.452
2024-12-01-16:17:02-root-INFO: Loss Change: 82.066 -> 79.213
2024-12-01-16:17:02-root-INFO: Regularization Change: 0.000 -> 2.780
2024-12-01-16:17:02-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-01-16:17:02-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-16:17:02-root-INFO: step: 48 lr_xt 0.18073022
2024-12-01-16:17:02-root-INFO: grad norm: 3.678 3.652 0.440
2024-12-01-16:17:03-root-INFO: grad norm: 3.623 3.600 0.411
2024-12-01-16:17:03-root-INFO: grad norm: 3.688 3.657 0.475
2024-12-01-16:17:04-root-INFO: grad norm: 3.794 3.770 0.429
2024-12-01-16:17:04-root-INFO: grad norm: 3.860 3.827 0.501
2024-12-01-16:17:04-root-INFO: Loss Change: 79.030 -> 76.535
2024-12-01-16:17:04-root-INFO: Regularization Change: 0.000 -> 2.622
2024-12-01-16:17:04-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-01-16:17:04-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-16:17:05-root-INFO: step: 47 lr_xt 0.18433941
2024-12-01-16:17:05-root-INFO: grad norm: 4.737 4.691 0.653
2024-12-01-16:17:05-root-INFO: grad norm: 4.430 4.391 0.588
2024-12-01-16:17:06-root-INFO: grad norm: 4.151 4.121 0.497
2024-12-01-16:17:06-root-INFO: grad norm: 4.062 4.026 0.538
2024-12-01-16:17:07-root-INFO: grad norm: 3.987 3.959 0.465
2024-12-01-16:17:07-root-INFO: Loss Change: 76.755 -> 74.125
2024-12-01-16:17:07-root-INFO: Regularization Change: 0.000 -> 2.802
2024-12-01-16:17:07-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-01-16:17:07-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-16:17:07-root-INFO: step: 46 lr_xt 0.18799060
2024-12-01-16:17:07-root-INFO: grad norm: 3.666 3.638 0.453
2024-12-01-16:17:08-root-INFO: grad norm: 3.629 3.606 0.407
2024-12-01-16:17:08-root-INFO: grad norm: 3.710 3.679 0.477
2024-12-01-16:17:09-root-INFO: grad norm: 3.817 3.792 0.432
2024-12-01-16:17:09-root-INFO: grad norm: 3.889 3.856 0.505
2024-12-01-16:17:10-root-INFO: Loss Change: 73.836 -> 71.479
2024-12-01-16:17:10-root-INFO: Regularization Change: 0.000 -> 2.664
2024-12-01-16:17:10-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-01-16:17:10-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-16:17:10-root-INFO: step: 45 lr_xt 0.19168344
2024-12-01-16:17:10-root-INFO: grad norm: 4.665 4.623 0.622
2024-12-01-16:17:11-root-INFO: grad norm: 4.378 4.340 0.581
2024-12-01-16:17:11-root-INFO: grad norm: 4.135 4.107 0.479
2024-12-01-16:17:12-root-INFO: grad norm: 4.055 4.021 0.529
2024-12-01-16:17:12-root-INFO: grad norm: 4.020 3.995 0.451
2024-12-01-16:17:12-root-INFO: Loss Change: 71.574 -> 69.024
2024-12-01-16:17:12-root-INFO: Regularization Change: 0.000 -> 2.863
2024-12-01-16:17:12-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-01-16:17:12-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-16:17:13-root-INFO: step: 44 lr_xt 0.19541757
2024-12-01-16:17:13-root-INFO: grad norm: 3.691 3.667 0.424
2024-12-01-16:17:13-root-INFO: grad norm: 3.725 3.704 0.400
2024-12-01-16:17:14-root-INFO: grad norm: 3.852 3.823 0.465
2024-12-01-16:17:14-root-INFO: grad norm: 4.015 3.992 0.425
2024-12-01-16:17:15-root-INFO: grad norm: 4.112 4.082 0.497
2024-12-01-16:17:15-root-INFO: Loss Change: 68.821 -> 66.636
2024-12-01-16:17:15-root-INFO: Regularization Change: 0.000 -> 2.756
2024-12-01-16:17:15-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-01-16:17:15-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-16:17:15-root-INFO: step: 43 lr_xt 0.19919257
2024-12-01-16:17:15-root-INFO: grad norm: 4.924 4.883 0.633
2024-12-01-16:17:16-root-INFO: grad norm: 4.626 4.589 0.586
2024-12-01-16:17:16-root-INFO: grad norm: 4.339 4.313 0.477
2024-12-01-16:17:17-root-INFO: grad norm: 4.201 4.168 0.522
2024-12-01-16:17:17-root-INFO: grad norm: 4.083 4.059 0.443
2024-12-01-16:17:18-root-INFO: Loss Change: 66.692 -> 64.068
2024-12-01-16:17:18-root-INFO: Regularization Change: 0.000 -> 3.000
2024-12-01-16:17:18-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-01-16:17:18-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-16:17:18-root-INFO: step: 42 lr_xt 0.20300803
2024-12-01-16:17:18-root-INFO: grad norm: 3.762 3.739 0.412
2024-12-01-16:17:18-root-INFO: grad norm: 3.725 3.706 0.381
2024-12-01-16:17:19-root-INFO: grad norm: 3.783 3.756 0.448
2024-12-01-16:17:19-root-INFO: grad norm: 3.849 3.828 0.399
2024-12-01-16:17:20-root-INFO: grad norm: 3.914 3.886 0.469
2024-12-01-16:17:20-root-INFO: Loss Change: 63.765 -> 61.553
2024-12-01-16:17:20-root-INFO: Regularization Change: 0.000 -> 2.816
2024-12-01-16:17:20-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-01-16:17:20-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-16:17:20-root-INFO: step: 41 lr_xt 0.20721469
2024-12-01-16:17:20-root-INFO: grad norm: 4.953 4.903 0.702
2024-12-01-16:17:21-root-INFO: grad norm: 4.551 4.513 0.583
2024-12-01-16:17:21-root-INFO: grad norm: 4.221 4.195 0.463
2024-12-01-16:17:22-root-INFO: grad norm: 4.029 3.997 0.500
2024-12-01-16:17:22-root-INFO: grad norm: 3.882 3.860 0.416
2024-12-01-16:17:23-root-INFO: Loss Change: 61.867 -> 59.056
2024-12-01-16:17:23-root-INFO: Regularization Change: 0.000 -> 3.111
2024-12-01-16:17:23-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-01-16:17:23-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-16:17:23-root-INFO: step: 40 lr_xt 0.21110784
2024-12-01-16:17:23-root-INFO: grad norm: 3.301 3.283 0.343
2024-12-01-16:17:23-root-INFO: grad norm: 3.187 3.171 0.320
2024-12-01-16:17:24-root-INFO: grad norm: 3.236 3.214 0.375
2024-12-01-16:17:24-root-INFO: grad norm: 3.307 3.290 0.339
2024-12-01-16:17:25-root-INFO: grad norm: 3.412 3.389 0.400
2024-12-01-16:17:25-root-INFO: Loss Change: 58.674 -> 56.501
2024-12-01-16:17:25-root-INFO: Regularization Change: 0.000 -> 2.787
2024-12-01-16:17:25-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-01-16:17:25-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-16:17:25-root-INFO: step: 39 lr_xt 0.21503976
2024-12-01-16:17:26-root-INFO: grad norm: 4.082 4.046 0.546
2024-12-01-16:17:26-root-INFO: grad norm: 3.929 3.898 0.487
2024-12-01-16:17:27-root-INFO: grad norm: 3.817 3.795 0.408
2024-12-01-16:17:27-root-INFO: grad norm: 3.716 3.689 0.450
2024-12-01-16:17:28-root-INFO: grad norm: 3.640 3.620 0.384
2024-12-01-16:17:28-root-INFO: Loss Change: 56.561 -> 54.122
2024-12-01-16:17:28-root-INFO: Regularization Change: 0.000 -> 2.957
2024-12-01-16:17:28-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-01-16:17:28-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-16:17:28-root-INFO: step: 38 lr_xt 0.21900989
2024-12-01-16:17:28-root-INFO: grad norm: 3.197 3.182 0.317
2024-12-01-16:17:29-root-INFO: grad norm: 3.063 3.048 0.307
2024-12-01-16:17:29-root-INFO: grad norm: 3.067 3.047 0.348
2024-12-01-16:17:30-root-INFO: grad norm: 3.093 3.076 0.323
2024-12-01-16:17:30-root-INFO: grad norm: 3.150 3.128 0.364
2024-12-01-16:17:31-root-INFO: Loss Change: 53.772 -> 51.618
2024-12-01-16:17:31-root-INFO: Regularization Change: 0.000 -> 2.780
2024-12-01-16:17:31-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-01-16:17:31-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-16:17:31-root-INFO: step: 37 lr_xt 0.22301766
2024-12-01-16:17:31-root-INFO: grad norm: 3.967 3.928 0.560
2024-12-01-16:17:31-root-INFO: grad norm: 3.679 3.650 0.464
2024-12-01-16:17:32-root-INFO: grad norm: 3.495 3.474 0.379
2024-12-01-16:17:32-root-INFO: grad norm: 3.333 3.308 0.405
2024-12-01-16:17:33-root-INFO: grad norm: 3.229 3.211 0.348
2024-12-01-16:17:33-root-INFO: Loss Change: 51.742 -> 49.200
2024-12-01-16:17:33-root-INFO: Regularization Change: 0.000 -> 2.951
2024-12-01-16:17:33-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-01-16:17:33-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-16:17:33-root-INFO: step: 36 lr_xt 0.22706247
2024-12-01-16:17:34-root-INFO: grad norm: 2.817 2.803 0.281
2024-12-01-16:17:34-root-INFO: grad norm: 2.689 2.675 0.275
2024-12-01-16:17:34-root-INFO: grad norm: 2.676 2.659 0.303
2024-12-01-16:17:35-root-INFO: grad norm: 2.695 2.680 0.285
2024-12-01-16:17:35-root-INFO: grad norm: 2.746 2.727 0.318
2024-12-01-16:17:36-root-INFO: Loss Change: 48.892 -> 46.793
2024-12-01-16:17:36-root-INFO: Regularization Change: 0.000 -> 2.706
2024-12-01-16:17:36-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-01-16:17:36-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-16:17:36-root-INFO: step: 35 lr_xt 0.23114370
2024-12-01-16:17:36-root-INFO: grad norm: 3.474 3.439 0.490
2024-12-01-16:17:37-root-INFO: grad norm: 3.267 3.241 0.415
2024-12-01-16:17:37-root-INFO: grad norm: 3.131 3.113 0.337
2024-12-01-16:17:37-root-INFO: grad norm: 3.005 2.983 0.362
2024-12-01-16:17:38-root-INFO: grad norm: 2.924 2.908 0.312
2024-12-01-16:17:38-root-INFO: Loss Change: 46.889 -> 44.529
2024-12-01-16:17:38-root-INFO: Regularization Change: 0.000 -> 2.858
2024-12-01-16:17:38-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-01-16:17:38-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-16:17:39-root-INFO: step: 34 lr_xt 0.23526068
2024-12-01-16:17:39-root-INFO: grad norm: 2.513 2.499 0.263
2024-12-01-16:17:39-root-INFO: grad norm: 2.353 2.342 0.230
2024-12-01-16:17:40-root-INFO: grad norm: 2.323 2.307 0.268
2024-12-01-16:17:40-root-INFO: grad norm: 2.332 2.320 0.240
2024-12-01-16:17:41-root-INFO: grad norm: 2.367 2.351 0.278
2024-12-01-16:17:41-root-INFO: Loss Change: 44.198 -> 42.109
2024-12-01-16:17:41-root-INFO: Regularization Change: 0.000 -> 2.667
2024-12-01-16:17:41-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-01-16:17:41-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-16:17:41-root-INFO: step: 33 lr_xt 0.23941272
2024-12-01-16:17:42-root-INFO: grad norm: 3.050 3.017 0.450
2024-12-01-16:17:42-root-INFO: grad norm: 2.868 2.846 0.352
2024-12-01-16:17:43-root-INFO: grad norm: 2.763 2.747 0.293
2024-12-01-16:17:43-root-INFO: grad norm: 2.662 2.644 0.312
2024-12-01-16:17:44-root-INFO: grad norm: 2.595 2.580 0.271
2024-12-01-16:17:44-root-INFO: Loss Change: 41.978 -> 39.738
2024-12-01-16:17:44-root-INFO: Regularization Change: 0.000 -> 2.804
2024-12-01-16:17:44-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-01-16:17:44-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-16:17:44-root-INFO: step: 32 lr_xt 0.24359912
2024-12-01-16:17:44-root-INFO: grad norm: 2.226 2.210 0.268
2024-12-01-16:17:45-root-INFO: grad norm: 1.984 1.973 0.203
2024-12-01-16:17:45-root-INFO: grad norm: 1.933 1.919 0.235
2024-12-01-16:17:46-root-INFO: grad norm: 1.931 1.919 0.206
2024-12-01-16:17:46-root-INFO: grad norm: 1.948 1.933 0.245
2024-12-01-16:17:47-root-INFO: Loss Change: 39.452 -> 37.423
2024-12-01-16:17:47-root-INFO: Regularization Change: 0.000 -> 2.551
2024-12-01-16:17:47-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-01-16:17:47-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-16:17:47-root-INFO: step: 31 lr_xt 0.24781911
2024-12-01-16:17:47-root-INFO: grad norm: 2.568 2.539 0.387
2024-12-01-16:17:48-root-INFO: grad norm: 2.418 2.398 0.312
2024-12-01-16:17:48-root-INFO: grad norm: 2.356 2.343 0.249
2024-12-01-16:17:49-root-INFO: grad norm: 2.309 2.292 0.279
2024-12-01-16:17:49-root-INFO: grad norm: 2.274 2.262 0.234
2024-12-01-16:17:49-root-INFO: Loss Change: 37.383 -> 35.385
2024-12-01-16:17:49-root-INFO: Regularization Change: 0.000 -> 2.613
2024-12-01-16:17:49-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-01-16:17:49-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-16:17:50-root-INFO: step: 30 lr_xt 0.25207194
2024-12-01-16:17:50-root-INFO: grad norm: 2.021 2.005 0.258
2024-12-01-16:17:50-root-INFO: grad norm: 1.807 1.796 0.196
2024-12-01-16:17:51-root-INFO: grad norm: 1.755 1.740 0.225
2024-12-01-16:17:51-root-INFO: grad norm: 1.746 1.735 0.192
2024-12-01-16:17:52-root-INFO: grad norm: 1.752 1.737 0.229
2024-12-01-16:17:52-root-INFO: Loss Change: 35.058 -> 33.143
2024-12-01-16:17:52-root-INFO: Regularization Change: 0.000 -> 2.468
2024-12-01-16:17:52-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-01-16:17:52-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-16:17:52-root-INFO: step: 29 lr_xt 0.25635679
2024-12-01-16:17:52-root-INFO: grad norm: 2.405 2.374 0.384
2024-12-01-16:17:53-root-INFO: grad norm: 2.169 2.150 0.287
2024-12-01-16:17:53-root-INFO: grad norm: 2.057 2.045 0.223
2024-12-01-16:17:54-root-INFO: grad norm: 1.982 1.967 0.243
2024-12-01-16:17:54-root-INFO: grad norm: 1.931 1.920 0.204
2024-12-01-16:17:54-root-INFO: Loss Change: 33.116 -> 31.175
2024-12-01-16:17:54-root-INFO: Regularization Change: 0.000 -> 2.519
2024-12-01-16:17:54-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-01-16:17:54-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-16:17:55-root-INFO: step: 28 lr_xt 0.26067283
2024-12-01-16:17:55-root-INFO: grad norm: 1.778 1.761 0.242
2024-12-01-16:17:55-root-INFO: grad norm: 1.518 1.508 0.170
2024-12-01-16:17:56-root-INFO: grad norm: 1.466 1.455 0.179
2024-12-01-16:17:56-root-INFO: grad norm: 1.456 1.448 0.160
2024-12-01-16:17:57-root-INFO: grad norm: 1.458 1.447 0.183
2024-12-01-16:17:57-root-INFO: Loss Change: 30.764 -> 28.937
2024-12-01-16:17:57-root-INFO: Regularization Change: 0.000 -> 2.372
2024-12-01-16:17:57-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-01-16:17:57-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-16:17:57-root-INFO: step: 27 lr_xt 0.26501920
2024-12-01-16:17:57-root-INFO: grad norm: 2.024 1.999 0.323
2024-12-01-16:17:58-root-INFO: grad norm: 1.814 1.799 0.238
2024-12-01-16:17:58-root-INFO: grad norm: 1.716 1.705 0.189
2024-12-01-16:17:59-root-INFO: grad norm: 1.656 1.644 0.201
2024-12-01-16:17:59-root-INFO: grad norm: 1.615 1.606 0.175
2024-12-01-16:18:00-root-INFO: Loss Change: 28.855 -> 27.051
2024-12-01-16:18:00-root-INFO: Regularization Change: 0.000 -> 2.396
2024-12-01-16:18:00-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-01-16:18:00-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-16:18:00-root-INFO: step: 26 lr_xt 0.26939500
2024-12-01-16:18:00-root-INFO: grad norm: 1.590 1.572 0.234
2024-12-01-16:18:00-root-INFO: grad norm: 1.314 1.305 0.153
2024-12-01-16:18:01-root-INFO: grad norm: 1.258 1.249 0.152
2024-12-01-16:18:01-root-INFO: grad norm: 1.240 1.233 0.137
2024-12-01-16:18:02-root-INFO: grad norm: 1.231 1.221 0.152
2024-12-01-16:18:02-root-INFO: Loss Change: 26.798 -> 25.075
2024-12-01-16:18:02-root-INFO: Regularization Change: 0.000 -> 2.279
2024-12-01-16:18:02-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-01-16:18:02-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-16:18:02-root-INFO: step: 25 lr_xt 0.27379933
2024-12-01-16:18:03-root-INFO: grad norm: 1.806 1.780 0.308
2024-12-01-16:18:03-root-INFO: grad norm: 1.528 1.515 0.200
2024-12-01-16:18:03-root-INFO: grad norm: 1.420 1.411 0.159
2024-12-01-16:18:04-root-INFO: grad norm: 1.373 1.363 0.163
2024-12-01-16:18:04-root-INFO: grad norm: 1.362 1.354 0.148
2024-12-01-16:18:05-root-INFO: Loss Change: 24.827 -> 23.170
2024-12-01-16:18:05-root-INFO: Regularization Change: 0.000 -> 2.248
2024-12-01-16:18:05-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-01-16:18:05-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-16:18:05-root-INFO: step: 24 lr_xt 0.27823123
2024-12-01-16:18:05-root-INFO: grad norm: 1.504 1.485 0.239
2024-12-01-16:18:06-root-INFO: grad norm: 1.194 1.186 0.141
2024-12-01-16:18:06-root-INFO: grad norm: 1.148 1.140 0.135
2024-12-01-16:18:06-root-INFO: grad norm: 1.158 1.152 0.124
2024-12-01-16:18:07-root-INFO: grad norm: 1.163 1.154 0.137
2024-12-01-16:18:07-root-INFO: Loss Change: 23.004 -> 21.438
2024-12-01-16:18:07-root-INFO: Regularization Change: 0.000 -> 2.128
2024-12-01-16:18:07-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-01-16:18:07-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-16:18:08-root-INFO: step: 23 lr_xt 0.28268972
2024-12-01-16:18:08-root-INFO: grad norm: 1.815 1.790 0.301
2024-12-01-16:18:08-root-INFO: grad norm: 1.448 1.435 0.194
2024-12-01-16:18:09-root-INFO: grad norm: 1.224 1.216 0.139
2024-12-01-16:18:09-root-INFO: grad norm: 1.188 1.179 0.144
2024-12-01-16:18:10-root-INFO: grad norm: 1.186 1.179 0.127
2024-12-01-16:18:10-root-INFO: Loss Change: 21.209 -> 19.674
2024-12-01-16:18:10-root-INFO: Regularization Change: 0.000 -> 2.101
2024-12-01-16:18:10-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-01-16:18:10-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-16:18:10-root-INFO: step: 22 lr_xt 0.28717380
2024-12-01-16:18:10-root-INFO: grad norm: 1.421 1.402 0.235
2024-12-01-16:18:11-root-INFO: grad norm: 1.122 1.114 0.134
2024-12-01-16:18:11-root-INFO: grad norm: 1.082 1.075 0.128
2024-12-01-16:18:12-root-INFO: grad norm: 1.103 1.097 0.116
2024-12-01-16:18:12-root-INFO: grad norm: 1.112 1.104 0.129
2024-12-01-16:18:13-root-INFO: Loss Change: 19.479 -> 18.060
2024-12-01-16:18:13-root-INFO: Regularization Change: 0.000 -> 1.992
2024-12-01-16:18:13-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-01-16:18:13-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-16:18:13-root-INFO: step: 21 lr_xt 0.29168243
2024-12-01-16:18:13-root-INFO: grad norm: 1.747 1.725 0.277
2024-12-01-16:18:14-root-INFO: grad norm: 1.373 1.361 0.180
2024-12-01-16:18:14-root-INFO: grad norm: 1.122 1.115 0.125
2024-12-01-16:18:14-root-INFO: grad norm: 1.094 1.086 0.132
2024-12-01-16:18:15-root-INFO: grad norm: 1.102 1.096 0.116
2024-12-01-16:18:15-root-INFO: Loss Change: 17.902 -> 16.485
2024-12-01-16:18:15-root-INFO: Regularization Change: 0.000 -> 1.990
2024-12-01-16:18:15-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-01-16:18:15-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-16:18:15-root-INFO: step: 20 lr_xt 0.29621455
2024-12-01-16:18:16-root-INFO: grad norm: 1.412 1.393 0.234
2024-12-01-16:18:16-root-INFO: grad norm: 1.065 1.057 0.125
2024-12-01-16:18:16-root-INFO: grad norm: 1.024 1.018 0.115
2024-12-01-16:18:17-root-INFO: grad norm: 1.050 1.044 0.109
2024-12-01-16:18:17-root-INFO: grad norm: 1.064 1.058 0.120
2024-12-01-16:18:18-root-INFO: Loss Change: 16.299 -> 14.976
2024-12-01-16:18:18-root-INFO: Regularization Change: 0.000 -> 1.899
2024-12-01-16:18:18-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-01-16:18:18-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-16:18:18-root-INFO: step: 19 lr_xt 0.30076908
2024-12-01-16:18:18-root-INFO: grad norm: 1.673 1.652 0.259
2024-12-01-16:18:19-root-INFO: grad norm: 1.299 1.289 0.161
2024-12-01-16:18:19-root-INFO: grad norm: 1.058 1.052 0.112
2024-12-01-16:18:19-root-INFO: grad norm: 1.030 1.024 0.118
2024-12-01-16:18:20-root-INFO: grad norm: 1.032 1.027 0.105
2024-12-01-16:18:20-root-INFO: Loss Change: 14.825 -> 13.520
2024-12-01-16:18:20-root-INFO: Regularization Change: 0.000 -> 1.879
2024-12-01-16:18:20-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-01-16:18:20-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-16:18:20-root-INFO: step: 18 lr_xt 0.30534490
2024-12-01-16:18:21-root-INFO: grad norm: 1.386 1.368 0.225
2024-12-01-16:18:21-root-INFO: grad norm: 1.009 1.003 0.115
2024-12-01-16:18:21-root-INFO: grad norm: 0.970 0.965 0.104
2024-12-01-16:18:22-root-INFO: grad norm: 0.996 0.991 0.100
2024-12-01-16:18:22-root-INFO: grad norm: 1.004 0.998 0.109
2024-12-01-16:18:23-root-INFO: Loss Change: 13.361 -> 12.122
2024-12-01-16:18:23-root-INFO: Regularization Change: 0.000 -> 1.815
2024-12-01-16:18:23-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-01-16:18:23-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-16:18:23-root-INFO: step: 17 lr_xt 0.30994086
2024-12-01-16:18:23-root-INFO: grad norm: 1.612 1.594 0.245
2024-12-01-16:18:23-root-INFO: grad norm: 1.220 1.212 0.142
2024-12-01-16:18:24-root-INFO: grad norm: 1.001 0.996 0.096
2024-12-01-16:18:24-root-INFO: grad norm: 0.966 0.960 0.104
2024-12-01-16:18:25-root-INFO: grad norm: 0.951 0.947 0.089
2024-12-01-16:18:25-root-INFO: Loss Change: 12.032 -> 10.834
2024-12-01-16:18:25-root-INFO: Regularization Change: 0.000 -> 1.756
2024-12-01-16:18:25-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-01-16:18:25-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-16:18:25-root-INFO: step: 16 lr_xt 0.31455579
2024-12-01-16:18:25-root-INFO: grad norm: 1.289 1.273 0.200
2024-12-01-16:18:26-root-INFO: grad norm: 0.917 0.911 0.100
2024-12-01-16:18:26-root-INFO: grad norm: 0.876 0.871 0.086
2024-12-01-16:18:27-root-INFO: grad norm: 0.883 0.879 0.083
2024-12-01-16:18:27-root-INFO: grad norm: 0.886 0.882 0.089
2024-12-01-16:18:28-root-INFO: Loss Change: 10.710 -> 9.616
2024-12-01-16:18:28-root-INFO: Regularization Change: 0.000 -> 1.642
2024-12-01-16:18:28-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-01-16:18:28-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-16:18:28-root-INFO: step: 15 lr_xt 0.31918850
2024-12-01-16:18:28-root-INFO: grad norm: 1.528 1.508 0.243
2024-12-01-16:18:28-root-INFO: grad norm: 1.110 1.103 0.119
2024-12-01-16:18:29-root-INFO: grad norm: 0.953 0.950 0.080
2024-12-01-16:18:29-root-INFO: grad norm: 0.910 0.905 0.088
2024-12-01-16:18:30-root-INFO: grad norm: 0.871 0.868 0.072
2024-12-01-16:18:30-root-INFO: Loss Change: 9.554 -> 8.470
2024-12-01-16:18:30-root-INFO: Regularization Change: 0.000 -> 1.606
2024-12-01-16:18:30-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-01-16:18:30-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-16:18:30-root-INFO: step: 14 lr_xt 0.32383775
2024-12-01-16:18:30-root-INFO: grad norm: 1.237 1.221 0.194
2024-12-01-16:18:31-root-INFO: grad norm: 0.820 0.816 0.080
2024-12-01-16:18:31-root-INFO: grad norm: 0.759 0.756 0.067
2024-12-01-16:18:32-root-INFO: grad norm: 0.735 0.732 0.065
2024-12-01-16:18:32-root-INFO: grad norm: 0.717 0.714 0.066
2024-12-01-16:18:33-root-INFO: Loss Change: 8.381 -> 7.405
2024-12-01-16:18:33-root-INFO: Regularization Change: 0.000 -> 1.469
2024-12-01-16:18:33-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-01-16:18:33-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-16:18:33-root-INFO: step: 13 lr_xt 0.32850231
2024-12-01-16:18:33-root-INFO: grad norm: 1.301 1.284 0.209
2024-12-01-16:18:33-root-INFO: grad norm: 0.892 0.888 0.090
2024-12-01-16:18:34-root-INFO: grad norm: 0.795 0.792 0.062
2024-12-01-16:18:34-root-INFO: grad norm: 0.744 0.741 0.066
2024-12-01-16:18:35-root-INFO: grad norm: 0.704 0.702 0.056
2024-12-01-16:18:35-root-INFO: Loss Change: 7.380 -> 6.439
2024-12-01-16:18:35-root-INFO: Regularization Change: 0.000 -> 1.416
2024-12-01-16:18:35-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-01-16:18:35-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-16:18:35-root-INFO: step: 12 lr_xt 0.33318090
2024-12-01-16:18:35-root-INFO: grad norm: 1.138 1.123 0.180
2024-12-01-16:18:36-root-INFO: grad norm: 0.715 0.711 0.070
2024-12-01-16:18:36-root-INFO: grad norm: 0.652 0.650 0.052
2024-12-01-16:18:37-root-INFO: grad norm: 0.624 0.622 0.052
2024-12-01-16:18:37-root-INFO: grad norm: 0.604 0.602 0.049
2024-12-01-16:18:38-root-INFO: Loss Change: 6.400 -> 5.585
2024-12-01-16:18:38-root-INFO: Regularization Change: 0.000 -> 1.248
2024-12-01-16:18:38-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-01-16:18:38-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-16:18:38-root-INFO: step: 11 lr_xt 0.33787222
2024-12-01-16:18:38-root-INFO: grad norm: 1.171 1.155 0.194
2024-12-01-16:18:38-root-INFO: grad norm: 0.699 0.696 0.066
2024-12-01-16:18:39-root-INFO: grad norm: 0.621 0.620 0.046
2024-12-01-16:18:40-root-INFO: grad norm: 0.588 0.586 0.046
2024-12-01-16:18:40-root-INFO: grad norm: 0.564 0.563 0.042
2024-12-01-16:18:40-root-INFO: Loss Change: 5.596 -> 4.842
2024-12-01-16:18:40-root-INFO: Regularization Change: 0.000 -> 1.143
2024-12-01-16:18:40-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-01-16:18:40-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-16:18:41-root-INFO: step: 10 lr_xt 0.34257494
2024-12-01-16:18:41-root-INFO: grad norm: 1.170 1.153 0.198
2024-12-01-16:18:41-root-INFO: grad norm: 0.659 0.656 0.061
2024-12-01-16:18:42-root-INFO: grad norm: 0.576 0.575 0.041
2024-12-01-16:18:42-root-INFO: grad norm: 0.544 0.542 0.040
2024-12-01-16:18:43-root-INFO: grad norm: 0.521 0.520 0.037
2024-12-01-16:18:43-root-INFO: Loss Change: 4.885 -> 4.187
2024-12-01-16:18:43-root-INFO: Regularization Change: 0.000 -> 1.051
2024-12-01-16:18:43-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-01-16:18:43-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-16:18:43-root-INFO: step: 9 lr_xt 0.34728771
2024-12-01-16:18:43-root-INFO: grad norm: 1.082 1.067 0.177
2024-12-01-16:18:44-root-INFO: grad norm: 0.614 0.611 0.054
2024-12-01-16:18:44-root-INFO: grad norm: 0.536 0.535 0.037
2024-12-01-16:18:45-root-INFO: grad norm: 0.506 0.505 0.035
2024-12-01-16:18:45-root-INFO: grad norm: 0.486 0.485 0.033
2024-12-01-16:18:46-root-INFO: Loss Change: 4.241 -> 3.624
2024-12-01-16:18:46-root-INFO: Regularization Change: 0.000 -> 0.943
2024-12-01-16:18:46-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-01-16:18:46-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-16:18:46-root-INFO: step: 8 lr_xt 0.35200918
2024-12-01-16:18:46-root-INFO: grad norm: 1.077 1.062 0.176
2024-12-01-16:18:46-root-INFO: grad norm: 0.601 0.599 0.051
2024-12-01-16:18:47-root-INFO: grad norm: 0.545 0.543 0.038
2024-12-01-16:18:47-root-INFO: grad norm: 0.546 0.545 0.037
2024-12-01-16:18:48-root-INFO: grad norm: 0.607 0.606 0.036
2024-12-01-16:18:48-root-INFO: Loss Change: 3.714 -> 3.162
2024-12-01-16:18:48-root-INFO: Regularization Change: 0.000 -> 0.885
2024-12-01-16:18:48-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-01-16:18:48-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-16:18:48-root-INFO: step: 7 lr_xt 0.35673794
2024-12-01-16:18:49-root-INFO: grad norm: 1.101 1.089 0.164
2024-12-01-16:18:49-root-INFO: grad norm: 0.681 0.680 0.046
2024-12-01-16:18:50-root-INFO: grad norm: 0.674 0.673 0.043
2024-12-01-16:18:50-root-INFO: grad norm: 0.680 0.679 0.039
2024-12-01-16:18:50-root-INFO: grad norm: 0.685 0.684 0.046
2024-12-01-16:18:51-root-INFO: Loss Change: 3.255 -> 2.757
2024-12-01-16:18:51-root-INFO: Regularization Change: 0.000 -> 0.812
2024-12-01-16:18:51-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-01-16:18:51-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-16:18:51-root-INFO: step: 6 lr_xt 0.36147257
2024-12-01-16:18:51-root-INFO: grad norm: 1.141 1.129 0.168
2024-12-01-16:18:52-root-INFO: grad norm: 0.718 0.716 0.050
2024-12-01-16:18:52-root-INFO: grad norm: 0.655 0.653 0.042
2024-12-01-16:18:53-root-INFO: grad norm: 0.629 0.628 0.043
2024-12-01-16:18:53-root-INFO: grad norm: 0.609 0.608 0.038
2024-12-01-16:18:54-root-INFO: Loss Change: 2.890 -> 2.400
2024-12-01-16:18:54-root-INFO: Regularization Change: 0.000 -> 0.743
2024-12-01-16:18:54-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-01-16:18:54-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-16:18:54-root-INFO: step: 5 lr_xt 0.36621164
2024-12-01-16:18:54-root-INFO: grad norm: 0.958 0.949 0.135
2024-12-01-16:18:55-root-INFO: grad norm: 0.529 0.527 0.044
2024-12-01-16:18:55-root-INFO: grad norm: 0.469 0.468 0.036
2024-12-01-16:18:56-root-INFO: grad norm: 0.442 0.441 0.036
2024-12-01-16:18:56-root-INFO: grad norm: 0.413 0.412 0.034
2024-12-01-16:18:57-root-INFO: Loss Change: 2.519 -> 2.108
2024-12-01-16:18:57-root-INFO: Regularization Change: 0.000 -> 0.645
2024-12-01-16:18:57-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-01-16:18:57-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-16:18:57-root-INFO: step: 4 lr_xt 0.37095370
2024-12-01-16:18:57-root-INFO: grad norm: 0.889 0.879 0.130
2024-12-01-16:18:57-root-INFO: grad norm: 0.451 0.449 0.038
2024-12-01-16:18:58-root-INFO: grad norm: 0.379 0.377 0.035
2024-12-01-16:18:58-root-INFO: grad norm: 0.341 0.340 0.033
2024-12-01-16:18:59-root-INFO: grad norm: 0.318 0.317 0.032
2024-12-01-16:18:59-root-INFO: Loss Change: 2.238 -> 1.867
2024-12-01-16:18:59-root-INFO: Regularization Change: 0.000 -> 0.574
2024-12-01-16:18:59-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-01-16:18:59-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-16:19:00-root-INFO: step: 3 lr_xt 0.37569726
2024-12-01-16:19:00-root-INFO: grad norm: 0.806 0.799 0.106
2024-12-01-16:19:00-root-INFO: grad norm: 0.415 0.413 0.040
2024-12-01-16:19:01-root-INFO: grad norm: 0.348 0.345 0.038
2024-12-01-16:19:01-root-INFO: grad norm: 0.313 0.311 0.036
2024-12-01-16:19:02-root-INFO: grad norm: 0.292 0.290 0.036
2024-12-01-16:19:02-root-INFO: Loss Change: 2.004 -> 1.674
2024-12-01-16:19:02-root-INFO: Regularization Change: 0.000 -> 0.526
2024-12-01-16:19:02-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-01-16:19:02-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-16:19:02-root-INFO: step: 2 lr_xt 0.38044082
2024-12-01-16:19:02-root-INFO: grad norm: 0.769 0.763 0.093
2024-12-01-16:19:03-root-INFO: grad norm: 0.411 0.409 0.041
2024-12-01-16:19:03-root-INFO: grad norm: 0.354 0.352 0.040
2024-12-01-16:19:04-root-INFO: grad norm: 0.307 0.304 0.039
2024-12-01-16:19:04-root-INFO: grad norm: 0.278 0.275 0.037
2024-12-01-16:19:05-root-INFO: Loss Change: 1.808 -> 1.498
2024-12-01-16:19:05-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-01-16:19:05-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-01-16:19:05-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-16:19:05-root-INFO: step: 1 lr_xt 0.38518288
2024-12-01-16:19:05-root-INFO: grad norm: 0.745 0.741 0.081
2024-12-01-16:19:06-root-INFO: grad norm: 0.437 0.436 0.037
2024-12-01-16:19:06-root-INFO: grad norm: 0.410 0.409 0.034
2024-12-01-16:19:07-root-INFO: grad norm: 0.508 0.507 0.032
2024-12-01-16:19:07-root-INFO: grad norm: 0.294 0.293 0.034
2024-12-01-16:19:07-root-INFO: Loss Change: 1.617 -> 1.290
2024-12-01-16:19:07-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-01-16:19:07-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-01-16:19:07-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-16:19:07-root-INFO: loss_sample0_0: 1.2896103858947754
2024-12-01-16:19:08-root-INFO: It takes 711.680 seconds for image sample0
2024-12-01-16:19:08-root-INFO: lpips_score_sample0: 0.141
2024-12-01-16:19:08-root-INFO: psnr_score_sample0: 17.623
2024-12-01-16:19:08-root-INFO: ssim_score_sample0: 0.728
2024-12-01-16:19:08-root-INFO: mean_lpips: 0.14128926396369934
2024-12-01-16:19:08-root-INFO: best_mean_lpips: 0.14128926396369934
2024-12-01-16:19:08-root-INFO: mean_psnr: 17.62327003479004
2024-12-01-16:19:08-root-INFO: best_mean_psnr: 17.62327003479004
2024-12-01-16:19:08-root-INFO: mean_ssim: 0.7281657457351685
2024-12-01-16:19:08-root-INFO: best_mean_ssim: 0.7281657457351685
2024-12-01-16:19:08-root-INFO: final_loss: 1.2896103858947754
2024-12-01-16:19:08-root-INFO: mean time: 711.679744720459
2024-12-01-16:19:08-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample1_iter5_lr0.02_10009 
 
Enjoy.
