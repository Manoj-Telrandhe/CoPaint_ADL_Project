2024-12-02-10:44:16-root-INFO: Prepare model...
2024-12-02-10:44:31-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-10:44:56-root-INFO: Start sampling
2024-12-02-10:45:02-root-INFO: step: 249 lr_xt 0.00019059
2024-12-02-10:45:02-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-10:45:03-root-INFO: Loss too large (77070.016->78612.719)! Learning rate decreased to 0.00015.
2024-12-02-10:45:03-root-INFO: grad norm: 15661.111 11248.843 10896.512
2024-12-02-10:45:03-root-INFO: Loss Change: 77070.016 -> 41352.125
2024-12-02-10:45:03-root-INFO: Regularization Change: 0.000 -> 13.469
2024-12-02-10:45:03-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-10:45:03-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-10:45:03-root-INFO: step: 248 lr_xt 0.00020082
2024-12-02-10:45:04-root-INFO: grad norm: 15701.263 11446.210 10747.741
2024-12-02-10:45:04-root-INFO: Loss too large (37235.805->50510.305)! Learning rate decreased to 0.00016.
2024-12-02-10:45:04-root-INFO: grad norm: 22338.301 16896.531 14611.877
2024-12-02-10:45:04-root-INFO: Loss too large (35233.348->52901.355)! Learning rate decreased to 0.00013.
2024-12-02-10:45:05-root-INFO: Loss too large (35233.348->40696.738)! Learning rate decreased to 0.00010.
2024-12-02-10:45:05-root-INFO: Loss Change: 37235.805 -> 30680.480
2024-12-02-10:45:05-root-INFO: Regularization Change: 0.000 -> 1.727
2024-12-02-10:45:05-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-02-10:45:05-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-10:45:05-root-INFO: step: 247 lr_xt 0.00021156
2024-12-02-10:45:05-root-INFO: grad norm: 17054.857 13055.636 10973.539
2024-12-02-10:45:05-root-INFO: Loss too large (29564.895->72512.297)! Learning rate decreased to 0.00017.
2024-12-02-10:45:05-root-INFO: Loss too large (29564.895->47837.398)! Learning rate decreased to 0.00014.
2024-12-02-10:45:06-root-INFO: Loss too large (29564.895->32723.227)! Learning rate decreased to 0.00011.
2024-12-02-10:45:06-root-INFO: grad norm: 15444.916 12359.620 9262.032
2024-12-02-10:45:06-root-INFO: Loss too large (24640.793->25551.723)! Learning rate decreased to 0.00009.
2024-12-02-10:45:07-root-INFO: Loss Change: 29564.895 -> 21262.205
2024-12-02-10:45:07-root-INFO: Regularization Change: 0.000 -> 0.904
2024-12-02-10:45:07-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-02-10:45:07-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-10:45:07-root-INFO: step: 246 lr_xt 0.00022285
2024-12-02-10:45:07-root-INFO: grad norm: 10880.780 8797.516 6402.741
2024-12-02-10:45:07-root-INFO: Loss too large (20900.270->47111.633)! Learning rate decreased to 0.00018.
2024-12-02-10:45:07-root-INFO: Loss too large (20900.270->33637.566)! Learning rate decreased to 0.00014.
2024-12-02-10:45:07-root-INFO: Loss too large (20900.270->25659.352)! Learning rate decreased to 0.00011.
2024-12-02-10:45:07-root-INFO: Loss too large (20900.270->21201.797)! Learning rate decreased to 0.00009.
2024-12-02-10:45:08-root-INFO: grad norm: 7895.763 6382.424 4648.412
2024-12-02-10:45:08-root-INFO: Loss Change: 20900.270 -> 17923.332
2024-12-02-10:45:08-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-02-10:45:08-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-02-10:45:08-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-10:45:08-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-10:45:09-root-INFO: grad norm: 5732.625 4690.701 3295.499
2024-12-02-10:45:09-root-INFO: Loss too large (17760.211->24841.160)! Learning rate decreased to 0.00019.
2024-12-02-10:45:09-root-INFO: Loss too large (17760.211->21058.322)! Learning rate decreased to 0.00015.
2024-12-02-10:45:09-root-INFO: Loss too large (17760.211->18876.246)! Learning rate decreased to 0.00012.
2024-12-02-10:45:10-root-INFO: grad norm: 6265.739 5056.777 3699.797
2024-12-02-10:45:10-root-INFO: Loss too large (17678.156->17738.873)! Learning rate decreased to 0.00010.
2024-12-02-10:45:10-root-INFO: Loss Change: 17760.211 -> 16976.742
2024-12-02-10:45:10-root-INFO: Regularization Change: 0.000 -> 0.104
2024-12-02-10:45:10-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-10:45:10-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-10:45:10-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-10:45:10-root-INFO: grad norm: 4212.089 3513.177 2323.636
2024-12-02-10:45:10-root-INFO: Loss too large (16852.160->20500.754)! Learning rate decreased to 0.00020.
2024-12-02-10:45:11-root-INFO: Loss too large (16852.160->18477.848)! Learning rate decreased to 0.00016.
2024-12-02-10:45:11-root-INFO: Loss too large (16852.160->17324.457)! Learning rate decreased to 0.00013.
2024-12-02-10:45:11-root-INFO: grad norm: 4315.080 3500.835 2522.710
2024-12-02-10:45:11-root-INFO: Loss Change: 16852.160 -> 16621.758
2024-12-02-10:45:11-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-02-10:45:11-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-10:45:11-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:12-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-10:45:12-root-INFO: grad norm: 4314.291 3570.161 2422.200
2024-12-02-10:45:12-root-INFO: Loss too large (16416.221->20529.586)! Learning rate decreased to 0.00021.
2024-12-02-10:45:12-root-INFO: Loss too large (16416.221->18258.184)! Learning rate decreased to 0.00017.
2024-12-02-10:45:12-root-INFO: Loss too large (16416.221->16962.555)! Learning rate decreased to 0.00013.
2024-12-02-10:45:13-root-INFO: grad norm: 4224.114 3476.154 2399.895
2024-12-02-10:45:13-root-INFO: Loss Change: 16416.221 -> 16161.559
2024-12-02-10:45:13-root-INFO: Regularization Change: 0.000 -> 0.059
2024-12-02-10:45:13-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-10:45:13-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:13-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-10:45:13-root-INFO: grad norm: 3836.921 3191.158 2130.369
2024-12-02-10:45:14-root-INFO: Loss too large (15875.479->19102.705)! Learning rate decreased to 0.00022.
2024-12-02-10:45:14-root-INFO: Loss too large (15875.479->17279.027)! Learning rate decreased to 0.00018.
2024-12-02-10:45:14-root-INFO: Loss too large (15875.479->16245.924)! Learning rate decreased to 0.00014.
2024-12-02-10:45:14-root-INFO: grad norm: 3574.374 2970.721 1987.704
2024-12-02-10:45:15-root-INFO: Loss Change: 15875.479 -> 15558.342
2024-12-02-10:45:15-root-INFO: Regularization Change: 0.000 -> 0.055
2024-12-02-10:45:15-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-10:45:15-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:15-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-10:45:15-root-INFO: grad norm: 3108.360 2577.042 1738.032
2024-12-02-10:45:15-root-INFO: Loss too large (15461.096->17478.445)! Learning rate decreased to 0.00023.
2024-12-02-10:45:15-root-INFO: Loss too large (15461.096->16297.443)! Learning rate decreased to 0.00018.
2024-12-02-10:45:15-root-INFO: Loss too large (15461.096->15633.641)! Learning rate decreased to 0.00015.
2024-12-02-10:45:16-root-INFO: grad norm: 2800.809 2360.917 1506.851
2024-12-02-10:45:16-root-INFO: Loss Change: 15461.096 -> 15139.590
2024-12-02-10:45:16-root-INFO: Regularization Change: 0.000 -> 0.061
2024-12-02-10:45:16-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-10:45:16-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:16-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-10:45:16-root-INFO: grad norm: 2353.747 1951.569 1315.865
2024-12-02-10:45:17-root-INFO: Loss too large (14933.971->15914.293)! Learning rate decreased to 0.00024.
2024-12-02-10:45:17-root-INFO: Loss too large (14933.971->15287.112)! Learning rate decreased to 0.00019.
2024-12-02-10:45:17-root-INFO: Loss too large (14933.971->14941.643)! Learning rate decreased to 0.00016.
2024-12-02-10:45:17-root-INFO: grad norm: 2051.972 1762.906 1050.118
2024-12-02-10:45:18-root-INFO: Loss Change: 14933.971 -> 14626.941
2024-12-02-10:45:18-root-INFO: Regularization Change: 0.000 -> 0.070
2024-12-02-10:45:18-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-10:45:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:18-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-10:45:18-root-INFO: grad norm: 1838.159 1505.173 1055.122
2024-12-02-10:45:18-root-INFO: Loss too large (14607.664->15086.423)! Learning rate decreased to 0.00026.
2024-12-02-10:45:18-root-INFO: Loss too large (14607.664->14739.992)! Learning rate decreased to 0.00020.
2024-12-02-10:45:19-root-INFO: grad norm: 2276.240 1958.894 1159.310
2024-12-02-10:45:19-root-INFO: Loss too large (14553.420->14563.490)! Learning rate decreased to 0.00016.
2024-12-02-10:45:19-root-INFO: Loss Change: 14607.664 -> 14391.525
2024-12-02-10:45:19-root-INFO: Regularization Change: 0.000 -> 0.088
2024-12-02-10:45:19-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-10:45:19-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:19-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-10:45:20-root-INFO: grad norm: 1884.457 1577.900 1030.248
2024-12-02-10:45:20-root-INFO: Loss too large (14195.121->14645.701)! Learning rate decreased to 0.00027.
2024-12-02-10:45:20-root-INFO: Loss too large (14195.121->14289.858)! Learning rate decreased to 0.00021.
2024-12-02-10:45:20-root-INFO: grad norm: 2237.232 1947.728 1100.711
2024-12-02-10:45:21-root-INFO: Loss Change: 14195.121 -> 14083.777
2024-12-02-10:45:21-root-INFO: Regularization Change: 0.000 -> 0.144
2024-12-02-10:45:21-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-10:45:21-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:21-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-10:45:21-root-INFO: grad norm: 2730.000 2284.100 1495.255
2024-12-02-10:45:21-root-INFO: Loss too large (14062.092->15572.732)! Learning rate decreased to 0.00028.
2024-12-02-10:45:21-root-INFO: Loss too large (14062.092->14602.338)! Learning rate decreased to 0.00023.
2024-12-02-10:45:21-root-INFO: Loss too large (14062.092->14067.102)! Learning rate decreased to 0.00018.
2024-12-02-10:45:22-root-INFO: grad norm: 2185.268 1929.379 1026.105
2024-12-02-10:45:22-root-INFO: Loss Change: 14062.092 -> 13595.483
2024-12-02-10:45:22-root-INFO: Regularization Change: 0.000 -> 0.103
2024-12-02-10:45:22-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-10:45:22-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:22-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-10:45:23-root-INFO: grad norm: 1686.623 1418.440 912.537
2024-12-02-10:45:23-root-INFO: Loss too large (13501.086->13840.228)! Learning rate decreased to 0.00030.
2024-12-02-10:45:23-root-INFO: Loss too large (13501.086->13548.486)! Learning rate decreased to 0.00024.
2024-12-02-10:45:23-root-INFO: grad norm: 1947.835 1715.937 921.750
2024-12-02-10:45:24-root-INFO: Loss Change: 13501.086 -> 13321.589
2024-12-02-10:45:24-root-INFO: Regularization Change: 0.000 -> 0.153
2024-12-02-10:45:24-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-10:45:24-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:24-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-10:45:24-root-INFO: grad norm: 2334.190 1985.874 1226.682
2024-12-02-10:45:24-root-INFO: Loss too large (13288.548->14310.625)! Learning rate decreased to 0.00031.
2024-12-02-10:45:24-root-INFO: Loss too large (13288.548->13600.680)! Learning rate decreased to 0.00025.
2024-12-02-10:45:25-root-INFO: grad norm: 2686.997 2380.789 1245.711
2024-12-02-10:45:25-root-INFO: Loss Change: 13288.548 -> 13201.547
2024-12-02-10:45:25-root-INFO: Regularization Change: 0.000 -> 0.181
2024-12-02-10:45:25-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-10:45:25-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-10:45:25-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-10:45:26-root-INFO: grad norm: 3000.527 2600.819 1496.295
2024-12-02-10:45:26-root-INFO: Loss too large (13105.468->15146.868)! Learning rate decreased to 0.00033.
2024-12-02-10:45:26-root-INFO: Loss too large (13105.468->13825.992)! Learning rate decreased to 0.00026.
2024-12-02-10:45:26-root-INFO: grad norm: 3403.656 3013.261 1582.760
2024-12-02-10:45:26-root-INFO: Loss too large (13095.054->13137.024)! Learning rate decreased to 0.00021.
2024-12-02-10:45:27-root-INFO: Loss Change: 13105.468 -> 12634.621
2024-12-02-10:45:27-root-INFO: Regularization Change: 0.000 -> 0.149
2024-12-02-10:45:27-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-10:45:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:27-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-10:45:27-root-INFO: grad norm: 2430.046 2128.621 1172.219
2024-12-02-10:45:27-root-INFO: Loss too large (12561.304->13722.398)! Learning rate decreased to 0.00035.
2024-12-02-10:45:27-root-INFO: Loss too large (12561.304->12900.523)! Learning rate decreased to 0.00028.
2024-12-02-10:45:28-root-INFO: grad norm: 2670.641 2381.805 1208.028
2024-12-02-10:45:28-root-INFO: Loss Change: 12561.304 -> 12378.233
2024-12-02-10:45:28-root-INFO: Regularization Change: 0.000 -> 0.215
2024-12-02-10:45:28-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-10:45:28-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:29-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-10:45:29-root-INFO: grad norm: 3034.024 2683.168 1416.303
2024-12-02-10:45:29-root-INFO: Loss too large (12231.965->14355.846)! Learning rate decreased to 0.00036.
2024-12-02-10:45:29-root-INFO: Loss too large (12231.965->12933.149)! Learning rate decreased to 0.00029.
2024-12-02-10:45:29-root-INFO: grad norm: 3312.529 2993.070 1419.288
2024-12-02-10:45:30-root-INFO: Loss Change: 12231.965 -> 12118.352
2024-12-02-10:45:30-root-INFO: Regularization Change: 0.000 -> 0.250
2024-12-02-10:45:30-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-10:45:30-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:30-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-10:45:30-root-INFO: grad norm: 3545.115 3175.119 1576.851
2024-12-02-10:45:30-root-INFO: Loss too large (12055.086->15270.179)! Learning rate decreased to 0.00038.
2024-12-02-10:45:30-root-INFO: Loss too large (12055.086->13194.843)! Learning rate decreased to 0.00030.
2024-12-02-10:45:31-root-INFO: grad norm: 3822.175 3470.083 1602.355
2024-12-02-10:45:31-root-INFO: Loss Change: 12055.086 -> 11984.874
2024-12-02-10:45:31-root-INFO: Regularization Change: 0.000 -> 0.254
2024-12-02-10:45:31-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-10:45:31-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:31-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-10:45:31-root-INFO: grad norm: 4048.978 3664.543 1722.019
2024-12-02-10:45:32-root-INFO: Loss too large (11904.279->16234.072)! Learning rate decreased to 0.00040.
2024-12-02-10:45:32-root-INFO: Loss too large (11904.279->13438.406)! Learning rate decreased to 0.00032.
2024-12-02-10:45:32-root-INFO: grad norm: 4224.055 3859.857 1715.849
2024-12-02-10:45:33-root-INFO: Loss Change: 11904.279 -> 11730.268
2024-12-02-10:45:33-root-INFO: Regularization Change: 0.000 -> 0.274
2024-12-02-10:45:33-root-INFO: Undo step: 230
2024-12-02-10:45:33-root-INFO: Undo step: 231
2024-12-02-10:45:33-root-INFO: Undo step: 232
2024-12-02-10:45:33-root-INFO: Undo step: 233
2024-12-02-10:45:33-root-INFO: Undo step: 234
2024-12-02-10:45:33-root-INFO: Undo step: 235
2024-12-02-10:45:33-root-INFO: Undo step: 236
2024-12-02-10:45:33-root-INFO: Undo step: 237
2024-12-02-10:45:33-root-INFO: Undo step: 238
2024-12-02-10:45:33-root-INFO: Undo step: 239
2024-12-02-10:45:33-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-10:45:33-root-INFO: grad norm: 14658.817 12204.444 8119.881
2024-12-02-10:45:33-root-INFO: Loss too large (24118.225->43381.477)! Learning rate decreased to 0.00024.
2024-12-02-10:45:33-root-INFO: Loss too large (24118.225->32274.984)! Learning rate decreased to 0.00019.
2024-12-02-10:45:34-root-INFO: grad norm: 13176.042 10722.382 7657.585
2024-12-02-10:45:34-root-INFO: Loss Change: 24118.225 -> 18915.000
2024-12-02-10:45:34-root-INFO: Regularization Change: 0.000 -> 2.405
2024-12-02-10:45:34-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-10:45:34-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:34-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-10:45:35-root-INFO: grad norm: 11408.240 9397.026 6468.682
2024-12-02-10:45:35-root-INFO: Loss too large (18495.521->37785.516)! Learning rate decreased to 0.00026.
2024-12-02-10:45:35-root-INFO: Loss too large (18495.521->26825.820)! Learning rate decreased to 0.00020.
2024-12-02-10:45:35-root-INFO: Loss too large (18495.521->19791.998)! Learning rate decreased to 0.00016.
2024-12-02-10:45:36-root-INFO: grad norm: 7766.030 6524.978 4211.399
2024-12-02-10:45:36-root-INFO: Loss Change: 18495.521 -> 13258.252
2024-12-02-10:45:36-root-INFO: Regularization Change: 0.000 -> 0.488
2024-12-02-10:45:36-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-10:45:36-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:36-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-10:45:36-root-INFO: grad norm: 4891.367 4006.516 2805.940
2024-12-02-10:45:36-root-INFO: Loss too large (13064.569->17430.559)! Learning rate decreased to 0.00027.
2024-12-02-10:45:37-root-INFO: Loss too large (13064.569->14732.508)! Learning rate decreased to 0.00021.
2024-12-02-10:45:37-root-INFO: Loss too large (13064.569->13157.551)! Learning rate decreased to 0.00017.
2024-12-02-10:45:37-root-INFO: grad norm: 3533.714 2994.752 1875.792
2024-12-02-10:45:38-root-INFO: Loss Change: 13064.569 -> 11884.250
2024-12-02-10:45:38-root-INFO: Regularization Change: 0.000 -> 0.190
2024-12-02-10:45:38-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-10:45:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:38-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-10:45:38-root-INFO: grad norm: 2369.032 1992.768 1281.090
2024-12-02-10:45:38-root-INFO: Loss too large (11725.568->12699.947)! Learning rate decreased to 0.00028.
2024-12-02-10:45:38-root-INFO: Loss too large (11725.568->12042.006)! Learning rate decreased to 0.00023.
2024-12-02-10:45:39-root-INFO: grad norm: 2658.538 2231.342 1445.316
2024-12-02-10:45:39-root-INFO: Loss Change: 11725.568 -> 11676.340
2024-12-02-10:45:39-root-INFO: Regularization Change: 0.000 -> 0.123
2024-12-02-10:45:39-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-10:45:39-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:39-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-10:45:39-root-INFO: grad norm: 3009.681 2561.783 1579.700
2024-12-02-10:45:39-root-INFO: Loss too large (11561.422->13429.879)! Learning rate decreased to 0.00030.
2024-12-02-10:45:40-root-INFO: Loss too large (11561.422->12251.260)! Learning rate decreased to 0.00024.
2024-12-02-10:45:40-root-INFO: Loss too large (11561.422->11582.176)! Learning rate decreased to 0.00019.
2024-12-02-10:45:40-root-INFO: grad norm: 2175.826 1835.997 1167.619
2024-12-02-10:45:41-root-INFO: Loss Change: 11561.422 -> 11022.020
2024-12-02-10:45:41-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-10:45:41-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-10:45:41-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-10:45:41-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-10:45:41-root-INFO: grad norm: 1396.475 1198.352 717.004
2024-12-02-10:45:41-root-INFO: Loss too large (10866.996->11053.461)! Learning rate decreased to 0.00031.
2024-12-02-10:45:41-root-INFO: grad norm: 2055.038 1739.252 1094.615
2024-12-02-10:45:42-root-INFO: Loss too large (10865.182->11102.249)! Learning rate decreased to 0.00025.
2024-12-02-10:45:42-root-INFO: Loss Change: 10866.996 -> 10804.896
2024-12-02-10:45:42-root-INFO: Regularization Change: 0.000 -> 0.152
2024-12-02-10:45:42-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-10:45:42-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-10:45:42-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-10:45:42-root-INFO: grad norm: 2168.281 1866.043 1104.231
2024-12-02-10:45:42-root-INFO: Loss too large (10739.502->11634.688)! Learning rate decreased to 0.00033.
2024-12-02-10:45:43-root-INFO: Loss too large (10739.502->11010.808)! Learning rate decreased to 0.00026.
2024-12-02-10:45:43-root-INFO: grad norm: 2260.412 1927.822 1180.239
2024-12-02-10:45:43-root-INFO: Loss Change: 10739.502 -> 10598.835
2024-12-02-10:45:43-root-INFO: Regularization Change: 0.000 -> 0.107
2024-12-02-10:45:43-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-10:45:43-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:43-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-10:45:44-root-INFO: grad norm: 2310.657 1971.575 1205.001
2024-12-02-10:45:44-root-INFO: Loss too large (10580.420->11587.027)! Learning rate decreased to 0.00035.
2024-12-02-10:45:44-root-INFO: Loss too large (10580.420->10863.911)! Learning rate decreased to 0.00028.
2024-12-02-10:45:44-root-INFO: grad norm: 2295.570 1974.673 1170.601
2024-12-02-10:45:45-root-INFO: Loss Change: 10580.420 -> 10371.256
2024-12-02-10:45:45-root-INFO: Regularization Change: 0.000 -> 0.120
2024-12-02-10:45:45-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-10:45:45-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:45-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-10:45:45-root-INFO: grad norm: 2005.218 1731.107 1012.013
2024-12-02-10:45:45-root-INFO: Loss too large (10074.697->10785.447)! Learning rate decreased to 0.00036.
2024-12-02-10:45:45-root-INFO: Loss too large (10074.697->10241.285)! Learning rate decreased to 0.00029.
2024-12-02-10:45:46-root-INFO: grad norm: 1921.133 1669.120 951.204
2024-12-02-10:45:46-root-INFO: Loss Change: 10074.697 -> 9842.990
2024-12-02-10:45:46-root-INFO: Regularization Change: 0.000 -> 0.119
2024-12-02-10:45:46-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-10:45:46-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:46-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-10:45:46-root-INFO: grad norm: 1778.367 1564.729 845.113
2024-12-02-10:45:47-root-INFO: Loss too large (9764.169->10326.061)! Learning rate decreased to 0.00038.
2024-12-02-10:45:47-root-INFO: Loss too large (9764.169->9885.982)! Learning rate decreased to 0.00030.
2024-12-02-10:45:47-root-INFO: grad norm: 1681.794 1465.603 824.887
2024-12-02-10:45:47-root-INFO: Loss Change: 9764.169 -> 9540.670
2024-12-02-10:45:47-root-INFO: Regularization Change: 0.000 -> 0.109
2024-12-02-10:45:47-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-10:45:47-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:48-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-10:45:48-root-INFO: grad norm: 1418.795 1249.086 672.878
2024-12-02-10:45:48-root-INFO: Loss too large (9395.713->9672.977)! Learning rate decreased to 0.00040.
2024-12-02-10:45:48-root-INFO: Loss too large (9395.713->9412.643)! Learning rate decreased to 0.00032.
2024-12-02-10:45:49-root-INFO: grad norm: 1288.812 1131.208 617.581
2024-12-02-10:45:49-root-INFO: Loss Change: 9395.713 -> 9170.394
2024-12-02-10:45:49-root-INFO: Regularization Change: 0.000 -> 0.115
2024-12-02-10:45:49-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-10:45:49-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:49-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-10:45:49-root-INFO: grad norm: 1083.573 956.012 510.071
2024-12-02-10:45:50-root-INFO: Loss too large (9053.649->9119.191)! Learning rate decreased to 0.00042.
2024-12-02-10:45:50-root-INFO: grad norm: 1338.685 1182.021 628.413
2024-12-02-10:45:50-root-INFO: Loss too large (8996.606->9000.388)! Learning rate decreased to 0.00034.
2024-12-02-10:45:51-root-INFO: Loss Change: 9053.649 -> 8878.471
2024-12-02-10:45:51-root-INFO: Regularization Change: 0.000 -> 0.154
2024-12-02-10:45:51-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-10:45:51-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:51-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-10:45:51-root-INFO: grad norm: 1094.268 976.640 493.556
2024-12-02-10:45:51-root-INFO: Loss too large (8829.434->8917.998)! Learning rate decreased to 0.00044.
2024-12-02-10:45:51-root-INFO: grad norm: 1338.871 1187.723 617.973
2024-12-02-10:45:52-root-INFO: Loss Change: 8829.434 -> 8773.764
2024-12-02-10:45:52-root-INFO: Regularization Change: 0.000 -> 0.203
2024-12-02-10:45:52-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-10:45:52-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:45:52-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-10:45:52-root-INFO: grad norm: 1440.846 1281.074 659.459
2024-12-02-10:45:52-root-INFO: Loss too large (8683.203->8939.242)! Learning rate decreased to 0.00046.
2024-12-02-10:45:53-root-INFO: grad norm: 1710.544 1523.193 778.359
2024-12-02-10:45:53-root-INFO: Loss too large (8655.076->8695.029)! Learning rate decreased to 0.00037.
2024-12-02-10:45:53-root-INFO: Loss Change: 8683.203 -> 8462.671
2024-12-02-10:45:53-root-INFO: Regularization Change: 0.000 -> 0.177
2024-12-02-10:45:53-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-10:45:53-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-10:45:53-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-10:45:53-root-INFO: grad norm: 1348.695 1226.476 561.012
2024-12-02-10:45:54-root-INFO: Loss too large (8401.793->8631.799)! Learning rate decreased to 0.00049.
2024-12-02-10:45:54-root-INFO: grad norm: 1583.472 1421.024 698.622
2024-12-02-10:45:54-root-INFO: Loss too large (8374.467->8380.220)! Learning rate decreased to 0.00039.
2024-12-02-10:45:55-root-INFO: Loss Change: 8401.793 -> 8182.012
2024-12-02-10:45:55-root-INFO: Regularization Change: 0.000 -> 0.172
2024-12-02-10:45:55-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-10:45:55-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:45:55-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-10:45:55-root-INFO: grad norm: 1055.761 948.045 464.587
2024-12-02-10:45:55-root-INFO: Loss too large (8031.077->8067.649)! Learning rate decreased to 0.00051.
2024-12-02-10:45:56-root-INFO: grad norm: 1147.783 1032.878 500.567
2024-12-02-10:45:56-root-INFO: Loss Change: 8031.077 -> 7884.677
2024-12-02-10:45:56-root-INFO: Regularization Change: 0.000 -> 0.237
2024-12-02-10:45:56-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-10:45:56-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:45:56-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-10:45:56-root-INFO: grad norm: 1192.065 1093.781 473.985
2024-12-02-10:45:56-root-INFO: Loss too large (7788.746->7934.270)! Learning rate decreased to 0.00054.
2024-12-02-10:45:57-root-INFO: grad norm: 1333.808 1211.098 558.825
2024-12-02-10:45:57-root-INFO: Loss Change: 7788.746 -> 7698.128
2024-12-02-10:45:57-root-INFO: Regularization Change: 0.000 -> 0.231
2024-12-02-10:45:57-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-10:45:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:45:57-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-10:45:57-root-INFO: grad norm: 1361.977 1254.935 529.261
2024-12-02-10:45:58-root-INFO: Loss too large (7596.146->7856.952)! Learning rate decreased to 0.00056.
2024-12-02-10:45:58-root-INFO: grad norm: 1507.479 1377.270 612.879
2024-12-02-10:45:58-root-INFO: Loss Change: 7596.146 -> 7537.691
2024-12-02-10:45:58-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-02-10:45:58-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-10:45:58-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:45:58-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-10:45:59-root-INFO: grad norm: 1619.327 1495.220 621.722
2024-12-02-10:45:59-root-INFO: Loss too large (7459.923->7942.394)! Learning rate decreased to 0.00059.
2024-12-02-10:45:59-root-INFO: Loss too large (7459.923->7469.428)! Learning rate decreased to 0.00047.
2024-12-02-10:45:59-root-INFO: grad norm: 1166.406 1072.570 458.361
2024-12-02-10:46:00-root-INFO: Loss Change: 7459.923 -> 7061.512
2024-12-02-10:46:00-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-02-10:46:00-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-10:46:00-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:00-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-10:46:00-root-INFO: grad norm: 772.666 713.622 296.237
2024-12-02-10:46:01-root-INFO: grad norm: 1129.532 1036.247 449.481
2024-12-02-10:46:01-root-INFO: Loss too large (6983.148->7151.567)! Learning rate decreased to 0.00062.
2024-12-02-10:46:01-root-INFO: Loss Change: 6986.162 -> 6930.411
2024-12-02-10:46:01-root-INFO: Regularization Change: 0.000 -> 0.285
2024-12-02-10:46:01-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-10:46:01-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:01-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-10:46:01-root-INFO: grad norm: 1028.763 954.009 384.994
2024-12-02-10:46:02-root-INFO: Loss too large (6816.959->6921.804)! Learning rate decreased to 0.00065.
2024-12-02-10:46:02-root-INFO: grad norm: 1077.852 992.768 419.734
2024-12-02-10:46:02-root-INFO: Loss Change: 6816.959 -> 6695.157
2024-12-02-10:46:02-root-INFO: Regularization Change: 0.000 -> 0.230
2024-12-02-10:46:02-root-INFO: Undo step: 220
2024-12-02-10:46:02-root-INFO: Undo step: 221
2024-12-02-10:46:02-root-INFO: Undo step: 222
2024-12-02-10:46:02-root-INFO: Undo step: 223
2024-12-02-10:46:02-root-INFO: Undo step: 224
2024-12-02-10:46:02-root-INFO: Undo step: 225
2024-12-02-10:46:02-root-INFO: Undo step: 226
2024-12-02-10:46:02-root-INFO: Undo step: 227
2024-12-02-10:46:02-root-INFO: Undo step: 228
2024-12-02-10:46:02-root-INFO: Undo step: 229
2024-12-02-10:46:02-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-10:46:03-root-INFO: grad norm: 5593.890 4099.897 3805.582
2024-12-02-10:46:03-root-INFO: grad norm: 6352.789 4684.289 4291.312
2024-12-02-10:46:03-root-INFO: Loss too large (14264.578->15507.311)! Learning rate decreased to 0.00040.
2024-12-02-10:46:04-root-INFO: Loss Change: 14440.472 -> 12228.377
2024-12-02-10:46:04-root-INFO: Regularization Change: 0.000 -> 2.720
2024-12-02-10:46:04-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-10:46:04-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:46:04-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-10:46:04-root-INFO: grad norm: 4473.266 3488.728 2799.801
2024-12-02-10:46:04-root-INFO: Loss too large (11768.758->12251.907)! Learning rate decreased to 0.00042.
2024-12-02-10:46:05-root-INFO: grad norm: 3738.443 3012.908 2213.219
2024-12-02-10:46:05-root-INFO: Loss Change: 11768.758 -> 10169.004
2024-12-02-10:46:05-root-INFO: Regularization Change: 0.000 -> 0.724
2024-12-02-10:46:05-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-10:46:05-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:46:05-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-10:46:05-root-INFO: grad norm: 3779.277 3166.921 2062.412
2024-12-02-10:46:05-root-INFO: Loss too large (10142.652->11908.655)! Learning rate decreased to 0.00044.
2024-12-02-10:46:06-root-INFO: grad norm: 4017.032 3451.666 2054.885
2024-12-02-10:46:06-root-INFO: Loss too large (9971.115->10344.404)! Learning rate decreased to 0.00035.
2024-12-02-10:46:06-root-INFO: Loss Change: 10142.652 -> 9098.606
2024-12-02-10:46:06-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-10:46:06-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-10:46:06-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-10:46:06-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-10:46:07-root-INFO: grad norm: 2924.560 2584.844 1368.076
2024-12-02-10:46:07-root-INFO: Loss too large (9073.574->10406.277)! Learning rate decreased to 0.00046.
2024-12-02-10:46:07-root-INFO: Loss too large (9073.574->9105.975)! Learning rate decreased to 0.00037.
2024-12-02-10:46:07-root-INFO: grad norm: 2075.093 1843.757 952.138
2024-12-02-10:46:08-root-INFO: Loss Change: 9073.574 -> 8155.029
2024-12-02-10:46:08-root-INFO: Regularization Change: 0.000 -> 0.280
2024-12-02-10:46:08-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-10:46:08-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-10:46:08-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-10:46:08-root-INFO: grad norm: 1553.319 1367.129 737.399
2024-12-02-10:46:08-root-INFO: Loss too large (8102.608->8417.238)! Learning rate decreased to 0.00049.
2024-12-02-10:46:09-root-INFO: grad norm: 1779.711 1583.046 813.226
2024-12-02-10:46:09-root-INFO: Loss too large (8070.395->8082.888)! Learning rate decreased to 0.00039.
2024-12-02-10:46:09-root-INFO: Loss Change: 8102.608 -> 7832.567
2024-12-02-10:46:09-root-INFO: Regularization Change: 0.000 -> 0.191
2024-12-02-10:46:09-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-10:46:09-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:09-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-10:46:09-root-INFO: grad norm: 1416.604 1277.527 612.121
2024-12-02-10:46:10-root-INFO: Loss too large (7715.147->7945.269)! Learning rate decreased to 0.00051.
2024-12-02-10:46:10-root-INFO: grad norm: 1545.487 1400.153 654.294
2024-12-02-10:46:10-root-INFO: Loss Change: 7715.147 -> 7644.927
2024-12-02-10:46:10-root-INFO: Regularization Change: 0.000 -> 0.206
2024-12-02-10:46:10-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-10:46:10-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:11-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-10:46:11-root-INFO: grad norm: 1743.713 1581.711 733.978
2024-12-02-10:46:11-root-INFO: Loss too large (7609.896->8091.004)! Learning rate decreased to 0.00054.
2024-12-02-10:46:11-root-INFO: grad norm: 1906.562 1737.227 785.508
2024-12-02-10:46:11-root-INFO: Loss too large (7583.229->7619.439)! Learning rate decreased to 0.00043.
2024-12-02-10:46:12-root-INFO: Loss Change: 7609.896 -> 7299.315
2024-12-02-10:46:12-root-INFO: Regularization Change: 0.000 -> 0.158
2024-12-02-10:46:12-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-10:46:12-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:12-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-10:46:12-root-INFO: grad norm: 1421.013 1296.501 581.689
2024-12-02-10:46:12-root-INFO: Loss too large (7282.018->7556.599)! Learning rate decreased to 0.00056.
2024-12-02-10:46:13-root-INFO: grad norm: 1514.525 1388.805 604.157
2024-12-02-10:46:13-root-INFO: Loss Change: 7282.018 -> 7212.414
2024-12-02-10:46:13-root-INFO: Regularization Change: 0.000 -> 0.185
2024-12-02-10:46:13-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-10:46:13-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:13-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-10:46:13-root-INFO: grad norm: 1604.167 1470.009 642.205
2024-12-02-10:46:14-root-INFO: Loss too large (7165.987->7574.150)! Learning rate decreased to 0.00059.
2024-12-02-10:46:14-root-INFO: grad norm: 1683.536 1540.838 678.314
2024-12-02-10:46:14-root-INFO: Loss Change: 7165.987 -> 7105.304
2024-12-02-10:46:14-root-INFO: Regularization Change: 0.000 -> 0.173
2024-12-02-10:46:14-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-10:46:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:15-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-10:46:15-root-INFO: grad norm: 1750.556 1602.778 703.953
2024-12-02-10:46:15-root-INFO: Loss too large (7065.717->7546.529)! Learning rate decreased to 0.00062.
2024-12-02-10:46:15-root-INFO: grad norm: 1763.757 1614.688 709.664
2024-12-02-10:46:16-root-INFO: Loss Change: 7065.717 -> 6949.134
2024-12-02-10:46:16-root-INFO: Regularization Change: 0.000 -> 0.171
2024-12-02-10:46:16-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-10:46:16-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:16-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-10:46:16-root-INFO: grad norm: 1906.297 1743.687 770.405
2024-12-02-10:46:16-root-INFO: Loss too large (7017.004->7572.581)! Learning rate decreased to 0.00065.
2024-12-02-10:46:17-root-INFO: grad norm: 1874.881 1725.373 733.667
2024-12-02-10:46:17-root-INFO: Loss Change: 7017.004 -> 6841.195
2024-12-02-10:46:17-root-INFO: Regularization Change: 0.000 -> 0.198
2024-12-02-10:46:17-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-10:46:17-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:17-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-10:46:17-root-INFO: grad norm: 1794.574 1646.331 714.207
2024-12-02-10:46:17-root-INFO: Loss too large (6791.803->7280.005)! Learning rate decreased to 0.00068.
2024-12-02-10:46:18-root-INFO: grad norm: 1708.244 1567.891 678.096
2024-12-02-10:46:18-root-INFO: Loss Change: 6791.803 -> 6572.333
2024-12-02-10:46:18-root-INFO: Regularization Change: 0.000 -> 0.172
2024-12-02-10:46:18-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-10:46:18-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-10:46:18-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-10:46:19-root-INFO: grad norm: 1715.057 1571.521 686.834
2024-12-02-10:46:19-root-INFO: Loss too large (6617.022->7040.142)! Learning rate decreased to 0.00071.
2024-12-02-10:46:19-root-INFO: grad norm: 1596.182 1470.629 620.524
2024-12-02-10:46:20-root-INFO: Loss Change: 6617.022 -> 6360.130
2024-12-02-10:46:20-root-INFO: Regularization Change: 0.000 -> 0.191
2024-12-02-10:46:20-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-10:46:20-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:20-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-10:46:20-root-INFO: grad norm: 1583.153 1448.103 639.822
2024-12-02-10:46:20-root-INFO: Loss too large (6413.979->6746.514)! Learning rate decreased to 0.00075.
2024-12-02-10:46:20-root-INFO: grad norm: 1440.771 1332.326 548.387
2024-12-02-10:46:21-root-INFO: Loss Change: 6413.979 -> 6139.982
2024-12-02-10:46:21-root-INFO: Regularization Change: 0.000 -> 0.210
2024-12-02-10:46:21-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-10:46:21-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:21-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-10:46:21-root-INFO: grad norm: 1437.661 1317.718 574.882
2024-12-02-10:46:21-root-INFO: Loss too large (6190.335->6453.960)! Learning rate decreased to 0.00079.
2024-12-02-10:46:22-root-INFO: grad norm: 1301.954 1204.323 494.662
2024-12-02-10:46:22-root-INFO: Loss Change: 6190.335 -> 5930.941
2024-12-02-10:46:22-root-INFO: Regularization Change: 0.000 -> 0.225
2024-12-02-10:46:22-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-10:46:22-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:22-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-10:46:23-root-INFO: grad norm: 1183.763 1091.075 459.184
2024-12-02-10:46:23-root-INFO: Loss too large (5918.871->6082.230)! Learning rate decreased to 0.00082.
2024-12-02-10:46:23-root-INFO: grad norm: 1047.155 968.369 398.491
2024-12-02-10:46:24-root-INFO: Loss Change: 5918.871 -> 5694.801
2024-12-02-10:46:24-root-INFO: Regularization Change: 0.000 -> 0.192
2024-12-02-10:46:24-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-10:46:24-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:24-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-10:46:24-root-INFO: grad norm: 1012.390 928.314 403.939
2024-12-02-10:46:24-root-INFO: Loss too large (5707.395->5800.191)! Learning rate decreased to 0.00086.
2024-12-02-10:46:24-root-INFO: grad norm: 882.371 818.068 330.672
2024-12-02-10:46:25-root-INFO: Loss Change: 5707.395 -> 5502.046
2024-12-02-10:46:25-root-INFO: Regularization Change: 0.000 -> 0.207
2024-12-02-10:46:25-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-10:46:25-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:25-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-10:46:25-root-INFO: grad norm: 914.215 833.698 375.149
2024-12-02-10:46:25-root-INFO: Loss too large (5545.959->5599.792)! Learning rate decreased to 0.00090.
2024-12-02-10:46:26-root-INFO: grad norm: 791.538 734.328 295.455
2024-12-02-10:46:26-root-INFO: Loss Change: 5545.959 -> 5347.599
2024-12-02-10:46:26-root-INFO: Regularization Change: 0.000 -> 0.236
2024-12-02-10:46:26-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-10:46:26-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:26-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-10:46:26-root-INFO: grad norm: 688.645 633.827 269.249
2024-12-02-10:46:26-root-INFO: Loss too large (5327.364->5335.625)! Learning rate decreased to 0.00095.
2024-12-02-10:46:27-root-INFO: grad norm: 590.035 546.599 222.197
2024-12-02-10:46:27-root-INFO: Loss Change: 5327.364 -> 5173.045
2024-12-02-10:46:27-root-INFO: Regularization Change: 0.000 -> 0.202
2024-12-02-10:46:27-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-10:46:27-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-10:46:27-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-10:46:28-root-INFO: grad norm: 652.830 591.155 276.989
2024-12-02-10:46:28-root-INFO: grad norm: 801.933 742.447 303.100
2024-12-02-10:46:28-root-INFO: Loss too large (5176.998->5232.226)! Learning rate decreased to 0.00099.
2024-12-02-10:46:29-root-INFO: Loss Change: 5197.543 -> 5087.553
2024-12-02-10:46:29-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-02-10:46:29-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-10:46:29-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:29-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-10:46:29-root-INFO: grad norm: 719.647 662.600 280.806
2024-12-02-10:46:29-root-INFO: Loss too large (5086.387->5095.317)! Learning rate decreased to 0.00104.
2024-12-02-10:46:30-root-INFO: grad norm: 585.746 543.580 218.218
2024-12-02-10:46:30-root-INFO: Loss Change: 5086.387 -> 4908.149
2024-12-02-10:46:30-root-INFO: Regularization Change: 0.000 -> 0.226
2024-12-02-10:46:30-root-INFO: Undo step: 210
2024-12-02-10:46:30-root-INFO: Undo step: 211
2024-12-02-10:46:30-root-INFO: Undo step: 212
2024-12-02-10:46:30-root-INFO: Undo step: 213
2024-12-02-10:46:30-root-INFO: Undo step: 214
2024-12-02-10:46:30-root-INFO: Undo step: 215
2024-12-02-10:46:30-root-INFO: Undo step: 216
2024-12-02-10:46:30-root-INFO: Undo step: 217
2024-12-02-10:46:30-root-INFO: Undo step: 218
2024-12-02-10:46:30-root-INFO: Undo step: 219
2024-12-02-10:46:30-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-10:46:30-root-INFO: grad norm: 14113.658 12796.569 5953.417
2024-12-02-10:46:31-root-INFO: grad norm: 5686.883 4886.766 2908.635
2024-12-02-10:46:31-root-INFO: Loss Change: 24718.545 -> 14559.764
2024-12-02-10:46:31-root-INFO: Regularization Change: 0.000 -> 104.209
2024-12-02-10:46:31-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-10:46:31-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-10:46:31-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-10:46:31-root-INFO: grad norm: 2853.870 2630.406 1107.041
2024-12-02-10:46:32-root-INFO: grad norm: 2545.235 2207.735 1266.542
2024-12-02-10:46:32-root-INFO: Loss Change: 12580.182 -> 10131.102
2024-12-02-10:46:32-root-INFO: Regularization Change: 0.000 -> 4.196
2024-12-02-10:46:32-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-10:46:32-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-10:46:32-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-10:46:32-root-INFO: grad norm: 2390.360 2250.768 804.901
2024-12-02-10:46:33-root-INFO: grad norm: 2883.879 2636.445 1168.722
2024-12-02-10:46:33-root-INFO: Loss Change: 9603.688 -> 9067.364
2024-12-02-10:46:33-root-INFO: Regularization Change: 0.000 -> 2.592
2024-12-02-10:46:33-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-10:46:33-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:33-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-10:46:33-root-INFO: grad norm: 2749.452 2547.476 1034.336
2024-12-02-10:46:34-root-INFO: grad norm: 2327.978 2116.088 970.390
2024-12-02-10:46:34-root-INFO: Loss Change: 8986.623 -> 7020.668
2024-12-02-10:46:34-root-INFO: Regularization Change: 0.000 -> 1.959
2024-12-02-10:46:34-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-10:46:34-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:34-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-10:46:35-root-INFO: grad norm: 2538.891 2343.097 977.683
2024-12-02-10:46:35-root-INFO: Loss too large (7085.170->7454.387)! Learning rate decreased to 0.00079.
2024-12-02-10:46:35-root-INFO: grad norm: 1948.936 1789.219 772.687
2024-12-02-10:46:36-root-INFO: Loss Change: 7085.170 -> 5944.625
2024-12-02-10:46:36-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-10:46:36-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-10:46:36-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:36-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-10:46:36-root-INFO: grad norm: 1566.902 1441.979 613.089
2024-12-02-10:46:36-root-INFO: Loss too large (5928.743->6121.227)! Learning rate decreased to 0.00082.
2024-12-02-10:46:36-root-INFO: grad norm: 1247.447 1147.159 490.051
2024-12-02-10:46:37-root-INFO: Loss Change: 5928.743 -> 5457.339
2024-12-02-10:46:37-root-INFO: Regularization Change: 0.000 -> 0.307
2024-12-02-10:46:37-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-10:46:37-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:37-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-10:46:37-root-INFO: grad norm: 1043.288 958.705 411.502
2024-12-02-10:46:37-root-INFO: Loss too large (5444.279->5471.717)! Learning rate decreased to 0.00086.
2024-12-02-10:46:38-root-INFO: grad norm: 811.313 750.357 308.534
2024-12-02-10:46:38-root-INFO: Loss Change: 5444.279 -> 5158.464
2024-12-02-10:46:38-root-INFO: Regularization Change: 0.000 -> 0.260
2024-12-02-10:46:38-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-10:46:38-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:38-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-10:46:38-root-INFO: grad norm: 755.250 690.685 305.543
2024-12-02-10:46:39-root-INFO: grad norm: 866.943 802.526 327.935
2024-12-02-10:46:39-root-INFO: Loss too large (5142.964->5158.328)! Learning rate decreased to 0.00090.
2024-12-02-10:46:39-root-INFO: Loss Change: 5179.705 -> 5017.640
2024-12-02-10:46:39-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-02-10:46:39-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-10:46:39-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-10:46:39-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-10:46:40-root-INFO: grad norm: 653.475 600.226 258.375
2024-12-02-10:46:40-root-INFO: grad norm: 743.869 691.457 274.277
2024-12-02-10:46:40-root-INFO: Loss Change: 4987.146 -> 4950.721
2024-12-02-10:46:40-root-INFO: Regularization Change: 0.000 -> 0.309
2024-12-02-10:46:40-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-10:46:40-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-10:46:41-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-10:46:41-root-INFO: grad norm: 949.210 870.686 378.030
2024-12-02-10:46:41-root-INFO: grad norm: 1056.812 982.131 390.221
2024-12-02-10:46:41-root-INFO: Loss too large (4968.848->5013.706)! Learning rate decreased to 0.00099.
2024-12-02-10:46:42-root-INFO: Loss Change: 4979.452 -> 4776.968
2024-12-02-10:46:42-root-INFO: Regularization Change: 0.000 -> 0.278
2024-12-02-10:46:42-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-10:46:42-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:42-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-10:46:42-root-INFO: grad norm: 792.656 729.748 309.470
2024-12-02-10:46:42-root-INFO: grad norm: 871.795 813.646 313.059
2024-12-02-10:46:43-root-INFO: Loss too large (4764.207->4770.267)! Learning rate decreased to 0.00104.
2024-12-02-10:46:43-root-INFO: Loss Change: 4780.984 -> 4610.168
2024-12-02-10:46:43-root-INFO: Regularization Change: 0.000 -> 0.243
2024-12-02-10:46:43-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-10:46:43-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:43-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-10:46:43-root-INFO: grad norm: 658.330 604.270 261.260
2024-12-02-10:46:44-root-INFO: grad norm: 704.736 657.974 252.435
2024-12-02-10:46:44-root-INFO: Loss Change: 4595.101 -> 4543.851
2024-12-02-10:46:44-root-INFO: Regularization Change: 0.000 -> 0.304
2024-12-02-10:46:44-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-10:46:44-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:44-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-10:46:44-root-INFO: grad norm: 821.650 759.672 313.063
2024-12-02-10:46:45-root-INFO: grad norm: 884.271 827.065 312.886
2024-12-02-10:46:45-root-INFO: Loss too large (4540.688->4552.680)! Learning rate decreased to 0.00114.
2024-12-02-10:46:45-root-INFO: Loss Change: 4560.131 -> 4370.114
2024-12-02-10:46:45-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-10:46:45-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-10:46:45-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:45-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-10:46:46-root-INFO: grad norm: 617.248 571.319 233.645
2024-12-02-10:46:46-root-INFO: grad norm: 645.465 605.785 222.820
2024-12-02-10:46:46-root-INFO: Loss Change: 4354.981 -> 4303.478
2024-12-02-10:46:46-root-INFO: Regularization Change: 0.000 -> 0.279
2024-12-02-10:46:46-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-10:46:46-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:47-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-10:46:47-root-INFO: grad norm: 714.202 670.070 247.166
2024-12-02-10:46:47-root-INFO: grad norm: 758.105 715.073 251.781
2024-12-02-10:46:48-root-INFO: Loss Change: 4294.570 -> 4279.042
2024-12-02-10:46:48-root-INFO: Regularization Change: 0.000 -> 0.264
2024-12-02-10:46:48-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-10:46:48-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:48-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-10:46:48-root-INFO: grad norm: 831.502 787.439 267.084
2024-12-02-10:46:48-root-INFO: Loss too large (4282.317->4285.464)! Learning rate decreased to 0.00132.
2024-12-02-10:46:49-root-INFO: grad norm: 564.243 534.632 180.388
2024-12-02-10:46:49-root-INFO: Loss Change: 4282.317 -> 4013.404
2024-12-02-10:46:49-root-INFO: Regularization Change: 0.000 -> 0.306
2024-12-02-10:46:49-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-10:46:49-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-10:46:49-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-10:46:49-root-INFO: grad norm: 431.186 406.098 144.932
2024-12-02-10:46:50-root-INFO: grad norm: 474.111 448.527 153.638
2024-12-02-10:46:50-root-INFO: Loss Change: 4016.535 -> 3990.082
2024-12-02-10:46:50-root-INFO: Regularization Change: 0.000 -> 0.288
2024-12-02-10:46:50-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-10:46:50-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:46:50-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-10:46:50-root-INFO: grad norm: 540.816 517.813 156.052
2024-12-02-10:46:50-root-INFO: Loss too large (3975.142->3998.212)! Learning rate decreased to 0.00144.
2024-12-02-10:46:51-root-INFO: grad norm: 432.725 409.296 140.453
2024-12-02-10:46:51-root-INFO: Loss Change: 3975.142 -> 3859.425
2024-12-02-10:46:51-root-INFO: Regularization Change: 0.000 -> 0.184
2024-12-02-10:46:51-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-10:46:51-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:46:51-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-10:46:51-root-INFO: grad norm: 353.055 334.982 111.513
2024-12-02-10:46:52-root-INFO: grad norm: 428.501 404.810 140.505
2024-12-02-10:46:52-root-INFO: Loss too large (3831.542->3854.663)! Learning rate decreased to 0.00150.
2024-12-02-10:46:52-root-INFO: Loss Change: 3848.577 -> 3791.078
2024-12-02-10:46:52-root-INFO: Regularization Change: 0.000 -> 0.253
2024-12-02-10:46:52-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-10:46:52-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:46:53-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-10:46:53-root-INFO: grad norm: 403.420 385.824 117.845
2024-12-02-10:46:53-root-INFO: Loss too large (3792.494->3822.184)! Learning rate decreased to 0.00157.
2024-12-02-10:46:53-root-INFO: grad norm: 384.609 362.576 128.307
2024-12-02-10:46:54-root-INFO: Loss Change: 3792.494 -> 3733.815
2024-12-02-10:46:54-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-02-10:46:54-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-10:46:54-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:46:54-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-10:46:54-root-INFO: grad norm: 326.024 311.429 96.454
2024-12-02-10:46:54-root-INFO: Loss too large (3712.105->3725.062)! Learning rate decreased to 0.00165.
2024-12-02-10:46:55-root-INFO: grad norm: 335.289 315.205 114.301
2024-12-02-10:46:55-root-INFO: Loss Change: 3712.105 -> 3667.494
2024-12-02-10:46:55-root-INFO: Regularization Change: 0.000 -> 0.187
2024-12-02-10:46:55-root-INFO: Undo step: 200
2024-12-02-10:46:55-root-INFO: Undo step: 201
2024-12-02-10:46:55-root-INFO: Undo step: 202
2024-12-02-10:46:55-root-INFO: Undo step: 203
2024-12-02-10:46:55-root-INFO: Undo step: 204
2024-12-02-10:46:55-root-INFO: Undo step: 205
2024-12-02-10:46:55-root-INFO: Undo step: 206
2024-12-02-10:46:55-root-INFO: Undo step: 207
2024-12-02-10:46:55-root-INFO: Undo step: 208
2024-12-02-10:46:55-root-INFO: Undo step: 209
2024-12-02-10:46:55-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-10:46:55-root-INFO: grad norm: 3788.158 3386.447 1697.680
2024-12-02-10:46:56-root-INFO: grad norm: 1901.934 1725.400 800.217
2024-12-02-10:46:56-root-INFO: Loss Change: 9296.836 -> 5035.182
2024-12-02-10:46:56-root-INFO: Regularization Change: 0.000 -> 16.866
2024-12-02-10:46:56-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-10:46:56-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:56-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-10:46:56-root-INFO: grad norm: 1110.046 1015.515 448.252
2024-12-02-10:46:57-root-INFO: grad norm: 832.845 782.909 284.050
2024-12-02-10:46:57-root-INFO: Loss Change: 4949.588 -> 4402.167
2024-12-02-10:46:57-root-INFO: Regularization Change: 0.000 -> 0.915
2024-12-02-10:46:57-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-10:46:57-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:57-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-10:46:58-root-INFO: grad norm: 594.023 540.766 245.836
2024-12-02-10:46:58-root-INFO: grad norm: 478.234 452.025 156.145
2024-12-02-10:46:58-root-INFO: Loss Change: 4378.136 -> 4153.416
2024-12-02-10:46:58-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-10:46:58-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-10:46:58-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:46:59-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-10:46:59-root-INFO: grad norm: 373.000 340.634 151.980
2024-12-02-10:46:59-root-INFO: grad norm: 337.506 318.166 112.608
2024-12-02-10:46:59-root-INFO: Loss Change: 4109.526 -> 3992.193
2024-12-02-10:46:59-root-INFO: Regularization Change: 0.000 -> 0.346
2024-12-02-10:46:59-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-10:46:59-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:47:00-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-10:47:00-root-INFO: grad norm: 319.813 296.783 119.166
2024-12-02-10:47:00-root-INFO: grad norm: 325.221 308.546 102.800
2024-12-02-10:47:01-root-INFO: Loss Change: 3962.219 -> 3888.383
2024-12-02-10:47:01-root-INFO: Regularization Change: 0.000 -> 0.284
2024-12-02-10:47:01-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-10:47:01-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-10:47:01-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-10:47:01-root-INFO: grad norm: 359.397 337.938 122.330
2024-12-02-10:47:01-root-INFO: grad norm: 416.831 398.211 123.192
2024-12-02-10:47:02-root-INFO: Loss Change: 3869.028 -> 3846.791
2024-12-02-10:47:02-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-10:47:02-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-10:47:02-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-10:47:02-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-10:47:02-root-INFO: grad norm: 507.012 480.788 160.948
2024-12-02-10:47:02-root-INFO: Loss too large (3826.520->3847.766)! Learning rate decreased to 0.00138.
2024-12-02-10:47:03-root-INFO: grad norm: 424.553 405.806 124.767
2024-12-02-10:47:03-root-INFO: Loss Change: 3826.520 -> 3726.297
2024-12-02-10:47:03-root-INFO: Regularization Change: 0.000 -> 0.181
2024-12-02-10:47:03-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-10:47:03-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:47:03-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-10:47:03-root-INFO: grad norm: 348.142 331.576 106.113
2024-12-02-10:47:04-root-INFO: grad norm: 437.232 420.009 121.509
2024-12-02-10:47:04-root-INFO: Loss too large (3707.933->3725.607)! Learning rate decreased to 0.00144.
2024-12-02-10:47:04-root-INFO: Loss Change: 3708.099 -> 3663.590
2024-12-02-10:47:04-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-02-10:47:04-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-10:47:04-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:47:04-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-10:47:05-root-INFO: grad norm: 401.897 380.573 129.172
2024-12-02-10:47:05-root-INFO: grad norm: 488.539 471.593 127.557
2024-12-02-10:47:05-root-INFO: Loss too large (3659.528->3685.470)! Learning rate decreased to 0.00150.
2024-12-02-10:47:05-root-INFO: Loss Change: 3659.908 -> 3603.518
2024-12-02-10:47:05-root-INFO: Regularization Change: 0.000 -> 0.237
2024-12-02-10:47:05-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-10:47:05-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:47:06-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-10:47:06-root-INFO: grad norm: 384.011 367.348 111.892
2024-12-02-10:47:06-root-INFO: Loss too large (3588.636->3600.748)! Learning rate decreased to 0.00157.
2024-12-02-10:47:06-root-INFO: grad norm: 331.430 319.125 89.469
2024-12-02-10:47:07-root-INFO: Loss Change: 3588.636 -> 3516.975
2024-12-02-10:47:07-root-INFO: Regularization Change: 0.000 -> 0.150
2024-12-02-10:47:07-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-10:47:07-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:47:07-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-10:47:07-root-INFO: grad norm: 336.441 319.926 104.117
2024-12-02-10:47:07-root-INFO: Loss too large (3512.245->3517.212)! Learning rate decreased to 0.00165.
2024-12-02-10:47:08-root-INFO: grad norm: 305.129 293.025 85.089
2024-12-02-10:47:08-root-INFO: Loss Change: 3512.245 -> 3451.039
2024-12-02-10:47:08-root-INFO: Regularization Change: 0.000 -> 0.170
2024-12-02-10:47:08-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-10:47:08-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:47:08-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-10:47:08-root-INFO: grad norm: 318.802 303.726 96.876
2024-12-02-10:47:08-root-INFO: Loss too large (3440.531->3453.658)! Learning rate decreased to 0.00172.
2024-12-02-10:47:09-root-INFO: grad norm: 310.477 296.633 91.678
2024-12-02-10:47:09-root-INFO: Loss Change: 3440.531 -> 3391.263
2024-12-02-10:47:09-root-INFO: Regularization Change: 0.000 -> 0.168
2024-12-02-10:47:09-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-10:47:09-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-10:47:09-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-10:47:10-root-INFO: grad norm: 355.953 338.755 109.306
2024-12-02-10:47:10-root-INFO: Loss too large (3395.562->3436.735)! Learning rate decreased to 0.00180.
2024-12-02-10:47:10-root-INFO: grad norm: 376.024 356.745 118.857
2024-12-02-10:47:11-root-INFO: Loss Change: 3395.562 -> 3364.062
2024-12-02-10:47:11-root-INFO: Regularization Change: 0.000 -> 0.191
2024-12-02-10:47:11-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-10:47:11-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:11-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-10:47:11-root-INFO: grad norm: 448.379 428.470 132.126
2024-12-02-10:47:11-root-INFO: Loss too large (3362.419->3467.470)! Learning rate decreased to 0.00188.
2024-12-02-10:47:12-root-INFO: grad norm: 479.351 455.033 150.740
2024-12-02-10:47:12-root-INFO: Loss Change: 3362.419 -> 3343.862
2024-12-02-10:47:12-root-INFO: Regularization Change: 0.000 -> 0.189
2024-12-02-10:47:12-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-10:47:12-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:12-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-10:47:12-root-INFO: grad norm: 543.903 518.481 164.343
2024-12-02-10:47:12-root-INFO: Loss too large (3353.446->3519.104)! Learning rate decreased to 0.00196.
2024-12-02-10:47:13-root-INFO: Loss too large (3353.446->3356.952)! Learning rate decreased to 0.00157.
2024-12-02-10:47:13-root-INFO: grad norm: 382.937 361.848 125.327
2024-12-02-10:47:13-root-INFO: Loss Change: 3353.446 -> 3210.147
2024-12-02-10:47:13-root-INFO: Regularization Change: 0.000 -> 0.207
2024-12-02-10:47:13-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-10:47:13-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:13-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-10:47:14-root-INFO: grad norm: 285.548 272.501 85.328
2024-12-02-10:47:14-root-INFO: Loss too large (3203.379->3230.636)! Learning rate decreased to 0.00205.
2024-12-02-10:47:14-root-INFO: grad norm: 321.637 303.803 105.613
2024-12-02-10:47:15-root-INFO: Loss Change: 3203.379 -> 3178.848
2024-12-02-10:47:15-root-INFO: Regularization Change: 0.000 -> 0.222
2024-12-02-10:47:15-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-10:47:15-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:15-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-10:47:15-root-INFO: grad norm: 413.424 396.825 115.969
2024-12-02-10:47:15-root-INFO: Loss too large (3180.339->3309.578)! Learning rate decreased to 0.00214.
2024-12-02-10:47:15-root-INFO: Loss too large (3180.339->3195.709)! Learning rate decreased to 0.00171.
2024-12-02-10:47:16-root-INFO: grad norm: 340.018 323.590 104.412
2024-12-02-10:47:16-root-INFO: Loss Change: 3180.339 -> 3094.377
2024-12-02-10:47:16-root-INFO: Regularization Change: 0.000 -> 0.171
2024-12-02-10:47:16-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-10:47:16-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:16-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-10:47:16-root-INFO: grad norm: 330.668 318.287 89.637
2024-12-02-10:47:16-root-INFO: Loss too large (3090.356->3170.936)! Learning rate decreased to 0.00224.
2024-12-02-10:47:17-root-INFO: Loss too large (3090.356->3095.020)! Learning rate decreased to 0.00179.
2024-12-02-10:47:17-root-INFO: grad norm: 292.215 278.587 88.199
2024-12-02-10:47:18-root-INFO: Loss Change: 3090.356 -> 3024.197
2024-12-02-10:47:18-root-INFO: Regularization Change: 0.000 -> 0.179
2024-12-02-10:47:18-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-10:47:18-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-10:47:18-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-10:47:18-root-INFO: grad norm: 296.969 287.857 72.998
2024-12-02-10:47:18-root-INFO: Loss too large (3014.168->3084.979)! Learning rate decreased to 0.00233.
2024-12-02-10:47:18-root-INFO: Loss too large (3014.168->3019.725)! Learning rate decreased to 0.00187.
2024-12-02-10:47:19-root-INFO: grad norm: 280.551 268.952 79.836
2024-12-02-10:47:19-root-INFO: Loss Change: 3014.168 -> 2958.536
2024-12-02-10:47:19-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-02-10:47:19-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-10:47:19-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:47:19-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-10:47:19-root-INFO: grad norm: 331.416 321.301 81.253
2024-12-02-10:47:19-root-INFO: Loss too large (2956.900->3073.921)! Learning rate decreased to 0.00244.
2024-12-02-10:47:20-root-INFO: Loss too large (2956.900->2982.462)! Learning rate decreased to 0.00195.
2024-12-02-10:47:20-root-INFO: grad norm: 338.325 325.770 91.309
2024-12-02-10:47:21-root-INFO: Loss Change: 2956.900 -> 2911.179
2024-12-02-10:47:21-root-INFO: Regularization Change: 0.000 -> 0.222
2024-12-02-10:47:21-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-10:47:21-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:47:21-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-10:47:21-root-INFO: grad norm: 398.505 388.367 89.316
2024-12-02-10:47:21-root-INFO: Loss too large (2915.461->3135.528)! Learning rate decreased to 0.00254.
2024-12-02-10:47:21-root-INFO: Loss too large (2915.461->2987.700)! Learning rate decreased to 0.00203.
2024-12-02-10:47:22-root-INFO: grad norm: 418.141 404.272 106.800
2024-12-02-10:47:22-root-INFO: Loss Change: 2915.461 -> 2882.546
2024-12-02-10:47:22-root-INFO: Regularization Change: 0.000 -> 0.220
2024-12-02-10:47:22-root-INFO: Undo step: 190
2024-12-02-10:47:22-root-INFO: Undo step: 191
2024-12-02-10:47:22-root-INFO: Undo step: 192
2024-12-02-10:47:22-root-INFO: Undo step: 193
2024-12-02-10:47:22-root-INFO: Undo step: 194
2024-12-02-10:47:22-root-INFO: Undo step: 195
2024-12-02-10:47:22-root-INFO: Undo step: 196
2024-12-02-10:47:22-root-INFO: Undo step: 197
2024-12-02-10:47:22-root-INFO: Undo step: 198
2024-12-02-10:47:22-root-INFO: Undo step: 199
2024-12-02-10:47:22-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-10:47:22-root-INFO: grad norm: 1017.163 825.683 594.028
2024-12-02-10:47:23-root-INFO: grad norm: 455.411 420.631 174.552
2024-12-02-10:47:23-root-INFO: Loss Change: 4983.876 -> 3716.044
2024-12-02-10:47:23-root-INFO: Regularization Change: 0.000 -> 5.914
2024-12-02-10:47:23-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-10:47:23-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-10:47:23-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-10:47:23-root-INFO: grad norm: 552.800 517.316 194.866
2024-12-02-10:47:24-root-INFO: Loss too large (3687.676->3767.809)! Learning rate decreased to 0.00172.
2024-12-02-10:47:24-root-INFO: grad norm: 562.319 530.891 185.355
2024-12-02-10:47:24-root-INFO: Loss Change: 3687.676 -> 3593.192
2024-12-02-10:47:24-root-INFO: Regularization Change: 0.000 -> 0.465
2024-12-02-10:47:24-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-10:47:24-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-10:47:25-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-10:47:25-root-INFO: grad norm: 622.571 583.828 216.195
2024-12-02-10:47:25-root-INFO: Loss too large (3583.178->3783.350)! Learning rate decreased to 0.00180.
2024-12-02-10:47:25-root-INFO: Loss too large (3583.178->3598.873)! Learning rate decreased to 0.00144.
2024-12-02-10:47:25-root-INFO: grad norm: 458.266 432.845 150.509
2024-12-02-10:47:26-root-INFO: Loss Change: 3583.178 -> 3406.275
2024-12-02-10:47:26-root-INFO: Regularization Change: 0.000 -> 0.278
2024-12-02-10:47:26-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-10:47:26-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:26-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-10:47:26-root-INFO: grad norm: 350.135 329.312 118.947
2024-12-02-10:47:26-root-INFO: Loss too large (3384.522->3427.128)! Learning rate decreased to 0.00188.
2024-12-02-10:47:27-root-INFO: grad norm: 397.860 378.249 123.371
2024-12-02-10:47:27-root-INFO: Loss Change: 3384.522 -> 3356.000
2024-12-02-10:47:27-root-INFO: Regularization Change: 0.000 -> 0.295
2024-12-02-10:47:27-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-10:47:27-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:27-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-10:47:28-root-INFO: grad norm: 512.370 487.416 157.953
2024-12-02-10:47:28-root-INFO: Loss too large (3348.760->3534.582)! Learning rate decreased to 0.00196.
2024-12-02-10:47:28-root-INFO: Loss too large (3348.760->3389.999)! Learning rate decreased to 0.00157.
2024-12-02-10:47:28-root-INFO: grad norm: 415.439 396.881 122.781
2024-12-02-10:47:29-root-INFO: Loss Change: 3348.760 -> 3236.121
2024-12-02-10:47:29-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-02-10:47:29-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-10:47:29-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:29-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-10:47:29-root-INFO: grad norm: 321.795 307.515 94.798
2024-12-02-10:47:29-root-INFO: Loss too large (3223.216->3277.970)! Learning rate decreased to 0.00205.
2024-12-02-10:47:30-root-INFO: grad norm: 381.972 365.249 111.785
2024-12-02-10:47:30-root-INFO: Loss Change: 3223.216 -> 3218.049
2024-12-02-10:47:30-root-INFO: Regularization Change: 0.000 -> 0.268
2024-12-02-10:47:30-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-10:47:30-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:30-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-10:47:30-root-INFO: grad norm: 518.076 498.784 140.062
2024-12-02-10:47:31-root-INFO: Loss too large (3217.674->3444.903)! Learning rate decreased to 0.00214.
2024-12-02-10:47:31-root-INFO: Loss too large (3217.674->3281.281)! Learning rate decreased to 0.00171.
2024-12-02-10:47:31-root-INFO: grad norm: 430.141 412.517 121.863
2024-12-02-10:47:32-root-INFO: Loss Change: 3217.674 -> 3109.329
2024-12-02-10:47:32-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-02-10:47:32-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-10:47:32-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-10:47:32-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-10:47:32-root-INFO: grad norm: 371.981 356.834 105.066
2024-12-02-10:47:32-root-INFO: Loss too large (3101.505->3212.699)! Learning rate decreased to 0.00224.
2024-12-02-10:47:32-root-INFO: Loss too large (3101.505->3118.209)! Learning rate decreased to 0.00179.
2024-12-02-10:47:33-root-INFO: grad norm: 328.804 313.255 99.918
2024-12-02-10:47:33-root-INFO: Loss Change: 3101.505 -> 3028.110
2024-12-02-10:47:33-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-02-10:47:33-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-10:47:33-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-10:47:33-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-10:47:33-root-INFO: grad norm: 326.968 314.842 88.217
2024-12-02-10:47:34-root-INFO: Loss too large (3014.892->3118.256)! Learning rate decreased to 0.00233.
2024-12-02-10:47:34-root-INFO: Loss too large (3014.892->3037.214)! Learning rate decreased to 0.00187.
2024-12-02-10:47:34-root-INFO: grad norm: 317.665 302.265 97.710
2024-12-02-10:47:35-root-INFO: Loss Change: 3014.892 -> 2963.087
2024-12-02-10:47:35-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-10:47:35-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-10:47:35-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:47:35-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-10:47:35-root-INFO: grad norm: 370.149 355.913 101.664
2024-12-02-10:47:35-root-INFO: Loss too large (2963.199->3137.168)! Learning rate decreased to 0.00244.
2024-12-02-10:47:35-root-INFO: Loss too large (2963.199->3020.246)! Learning rate decreased to 0.00195.
2024-12-02-10:47:36-root-INFO: grad norm: 392.163 372.509 122.591
2024-12-02-10:47:36-root-INFO: Loss Change: 2963.199 -> 2933.406
2024-12-02-10:47:36-root-INFO: Regularization Change: 0.000 -> 0.218
2024-12-02-10:47:36-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-10:47:36-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:47:36-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-10:47:36-root-INFO: grad norm: 470.059 452.472 127.376
2024-12-02-10:47:37-root-INFO: Loss too large (2936.856->3260.342)! Learning rate decreased to 0.00254.
2024-12-02-10:47:37-root-INFO: Loss too large (2936.856->3069.307)! Learning rate decreased to 0.00203.
2024-12-02-10:47:37-root-INFO: Loss too large (2936.856->2946.620)! Learning rate decreased to 0.00163.
2024-12-02-10:47:37-root-INFO: grad norm: 348.466 331.178 108.398
2024-12-02-10:47:38-root-INFO: Loss Change: 2936.856 -> 2826.814
2024-12-02-10:47:38-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-02-10:47:38-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-10:47:38-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:47:38-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-10:47:38-root-INFO: grad norm: 286.804 276.327 76.810
2024-12-02-10:47:38-root-INFO: Loss too large (2818.183->2941.381)! Learning rate decreased to 0.00265.
2024-12-02-10:47:38-root-INFO: Loss too large (2818.183->2860.516)! Learning rate decreased to 0.00212.
2024-12-02-10:47:39-root-INFO: grad norm: 342.903 324.830 109.856
2024-12-02-10:47:39-root-INFO: Loss too large (2812.786->2814.804)! Learning rate decreased to 0.00170.
2024-12-02-10:47:39-root-INFO: Loss Change: 2818.183 -> 2772.389
2024-12-02-10:47:39-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-10:47:39-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-10:47:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:47:39-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-10:47:40-root-INFO: grad norm: 326.724 315.439 85.128
2024-12-02-10:47:40-root-INFO: Loss too large (2765.146->2959.221)! Learning rate decreased to 0.00277.
2024-12-02-10:47:40-root-INFO: Loss too large (2765.146->2844.524)! Learning rate decreased to 0.00222.
2024-12-02-10:47:40-root-INFO: Loss too large (2765.146->2774.022)! Learning rate decreased to 0.00177.
2024-12-02-10:47:41-root-INFO: grad norm: 281.995 268.371 86.592
2024-12-02-10:47:41-root-INFO: Loss Change: 2765.146 -> 2704.843
2024-12-02-10:47:41-root-INFO: Regularization Change: 0.000 -> 0.146
2024-12-02-10:47:41-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-10:47:41-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-10:47:41-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-10:47:41-root-INFO: grad norm: 265.336 256.325 68.563
2024-12-02-10:47:41-root-INFO: Loss too large (2701.521->2838.467)! Learning rate decreased to 0.00289.
2024-12-02-10:47:42-root-INFO: Loss too large (2701.521->2756.239)! Learning rate decreased to 0.00231.
2024-12-02-10:47:42-root-INFO: Loss too large (2701.521->2706.886)! Learning rate decreased to 0.00185.
2024-12-02-10:47:42-root-INFO: grad norm: 256.337 243.416 80.356
2024-12-02-10:47:42-root-INFO: Loss Change: 2701.521 -> 2658.764
2024-12-02-10:47:42-root-INFO: Regularization Change: 0.000 -> 0.154
2024-12-02-10:47:42-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-10:47:42-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:47:43-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-10:47:43-root-INFO: grad norm: 287.872 279.197 70.139
2024-12-02-10:47:43-root-INFO: Loss too large (2648.216->2844.408)! Learning rate decreased to 0.00301.
2024-12-02-10:47:43-root-INFO: Loss too large (2648.216->2736.705)! Learning rate decreased to 0.00241.
2024-12-02-10:47:43-root-INFO: Loss too large (2648.216->2669.985)! Learning rate decreased to 0.00193.
2024-12-02-10:47:44-root-INFO: grad norm: 289.107 276.253 85.247
2024-12-02-10:47:44-root-INFO: Loss Change: 2648.216 -> 2612.548
2024-12-02-10:47:44-root-INFO: Regularization Change: 0.000 -> 0.149
2024-12-02-10:47:44-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-10:47:44-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:47:44-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-10:47:44-root-INFO: grad norm: 309.764 300.435 75.449
2024-12-02-10:47:44-root-INFO: Loss too large (2602.197->2851.741)! Learning rate decreased to 0.00314.
2024-12-02-10:47:45-root-INFO: Loss too large (2602.197->2721.538)! Learning rate decreased to 0.00251.
2024-12-02-10:47:45-root-INFO: Loss too large (2602.197->2638.646)! Learning rate decreased to 0.00201.
2024-12-02-10:47:45-root-INFO: grad norm: 314.115 300.604 91.132
2024-12-02-10:47:46-root-INFO: Loss Change: 2602.197 -> 2569.733
2024-12-02-10:47:46-root-INFO: Regularization Change: 0.000 -> 0.148
2024-12-02-10:47:46-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-10:47:46-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:47:46-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-10:47:46-root-INFO: grad norm: 373.135 362.703 87.615
2024-12-02-10:47:46-root-INFO: Loss too large (2573.521->2964.563)! Learning rate decreased to 0.00328.
2024-12-02-10:47:46-root-INFO: Loss too large (2573.521->2773.447)! Learning rate decreased to 0.00262.
2024-12-02-10:47:46-root-INFO: Loss too large (2573.521->2646.861)! Learning rate decreased to 0.00210.
2024-12-02-10:47:47-root-INFO: grad norm: 385.345 369.877 108.083
2024-12-02-10:47:47-root-INFO: Loss Change: 2573.521 -> 2545.833
2024-12-02-10:47:47-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-10:47:47-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-10:47:47-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:47:47-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-10:47:47-root-INFO: grad norm: 422.041 410.926 96.218
2024-12-02-10:47:48-root-INFO: Loss too large (2544.378->3026.316)! Learning rate decreased to 0.00342.
2024-12-02-10:47:48-root-INFO: Loss too large (2544.378->2790.981)! Learning rate decreased to 0.00273.
2024-12-02-10:47:48-root-INFO: Loss too large (2544.378->2632.563)! Learning rate decreased to 0.00219.
2024-12-02-10:47:48-root-INFO: grad norm: 401.545 386.258 109.742
2024-12-02-10:47:49-root-INFO: Loss Change: 2544.378 -> 2489.789
2024-12-02-10:47:49-root-INFO: Regularization Change: 0.000 -> 0.175
2024-12-02-10:47:49-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-10:47:49-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:47:49-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-10:47:49-root-INFO: grad norm: 397.677 387.203 90.669
2024-12-02-10:47:49-root-INFO: Loss too large (2489.345->2926.537)! Learning rate decreased to 0.00356.
2024-12-02-10:47:49-root-INFO: Loss too large (2489.345->2706.089)! Learning rate decreased to 0.00285.
2024-12-02-10:47:49-root-INFO: Loss too large (2489.345->2560.545)! Learning rate decreased to 0.00228.
2024-12-02-10:47:50-root-INFO: grad norm: 368.244 353.661 102.604
2024-12-02-10:47:50-root-INFO: Loss Change: 2489.345 -> 2427.901
2024-12-02-10:47:50-root-INFO: Regularization Change: 0.000 -> 0.180
2024-12-02-10:47:50-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-10:47:50-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-10:47:50-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-10:47:50-root-INFO: grad norm: 351.734 343.007 77.868
2024-12-02-10:47:51-root-INFO: Loss too large (2423.690->2778.481)! Learning rate decreased to 0.00371.
2024-12-02-10:47:51-root-INFO: Loss too large (2423.690->2592.623)! Learning rate decreased to 0.00297.
2024-12-02-10:47:51-root-INFO: Loss too large (2423.690->2473.414)! Learning rate decreased to 0.00238.
2024-12-02-10:47:51-root-INFO: grad norm: 323.501 310.584 90.503
2024-12-02-10:47:52-root-INFO: Loss Change: 2423.690 -> 2365.143
2024-12-02-10:47:52-root-INFO: Regularization Change: 0.000 -> 0.188
2024-12-02-10:47:52-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-10:47:52-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:47:52-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-10:47:52-root-INFO: grad norm: 319.817 312.195 69.407
2024-12-02-10:47:52-root-INFO: Loss too large (2365.068->2680.572)! Learning rate decreased to 0.00387.
2024-12-02-10:47:52-root-INFO: Loss too large (2365.068->2512.310)! Learning rate decreased to 0.00309.
2024-12-02-10:47:53-root-INFO: Loss too large (2365.068->2406.597)! Learning rate decreased to 0.00248.
2024-12-02-10:47:53-root-INFO: grad norm: 303.419 291.416 84.495
2024-12-02-10:47:53-root-INFO: Loss Change: 2365.068 -> 2314.784
2024-12-02-10:47:53-root-INFO: Regularization Change: 0.000 -> 0.205
2024-12-02-10:47:53-root-INFO: Undo step: 180
2024-12-02-10:47:53-root-INFO: Undo step: 181
2024-12-02-10:47:53-root-INFO: Undo step: 182
2024-12-02-10:47:53-root-INFO: Undo step: 183
2024-12-02-10:47:53-root-INFO: Undo step: 184
2024-12-02-10:47:53-root-INFO: Undo step: 185
2024-12-02-10:47:53-root-INFO: Undo step: 186
2024-12-02-10:47:53-root-INFO: Undo step: 187
2024-12-02-10:47:53-root-INFO: Undo step: 188
2024-12-02-10:47:53-root-INFO: Undo step: 189
2024-12-02-10:47:54-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-10:47:54-root-INFO: grad norm: 886.603 744.343 481.683
2024-12-02-10:47:54-root-INFO: grad norm: 469.504 414.853 219.844
2024-12-02-10:47:55-root-INFO: Loss Change: 4341.045 -> 3088.451
2024-12-02-10:47:55-root-INFO: Regularization Change: 0.000 -> 8.887
2024-12-02-10:47:55-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-10:47:55-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:47:55-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-10:47:55-root-INFO: grad norm: 432.869 419.781 105.638
2024-12-02-10:47:55-root-INFO: Loss too large (3073.347->3290.925)! Learning rate decreased to 0.00265.
2024-12-02-10:47:55-root-INFO: Loss too large (3073.347->3105.331)! Learning rate decreased to 0.00212.
2024-12-02-10:47:56-root-INFO: grad norm: 535.102 516.445 140.066
2024-12-02-10:47:56-root-INFO: Loss too large (3004.057->3069.772)! Learning rate decreased to 0.00170.
2024-12-02-10:47:56-root-INFO: Loss Change: 3073.347 -> 2975.651
2024-12-02-10:47:56-root-INFO: Regularization Change: 0.000 -> 0.613
2024-12-02-10:47:56-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-10:47:56-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-10:47:56-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-10:47:57-root-INFO: grad norm: 464.651 451.707 108.908
2024-12-02-10:47:57-root-INFO: Loss too large (2965.532->3280.317)! Learning rate decreased to 0.00277.
2024-12-02-10:47:57-root-INFO: Loss too large (2965.532->3042.974)! Learning rate decreased to 0.00222.
2024-12-02-10:47:57-root-INFO: grad norm: 543.128 527.712 128.484
2024-12-02-10:47:58-root-INFO: Loss too large (2900.610->2956.023)! Learning rate decreased to 0.00177.
2024-12-02-10:47:58-root-INFO: Loss Change: 2965.532 -> 2860.420
2024-12-02-10:47:58-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-02-10:47:58-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-10:47:58-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-10:47:58-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-10:47:58-root-INFO: grad norm: 426.925 412.008 111.869
2024-12-02-10:47:58-root-INFO: Loss too large (2855.688->3116.579)! Learning rate decreased to 0.00289.
2024-12-02-10:47:58-root-INFO: Loss too large (2855.688->2912.181)! Learning rate decreased to 0.00231.
2024-12-02-10:47:59-root-INFO: grad norm: 468.923 454.340 116.037
2024-12-02-10:47:59-root-INFO: Loss too large (2792.823->2820.854)! Learning rate decreased to 0.00185.
2024-12-02-10:47:59-root-INFO: Loss Change: 2855.688 -> 2745.935
2024-12-02-10:47:59-root-INFO: Regularization Change: 0.000 -> 0.329
2024-12-02-10:47:59-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-10:47:59-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:48:00-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-10:48:00-root-INFO: grad norm: 374.563 359.980 103.496
2024-12-02-10:48:00-root-INFO: Loss too large (2738.677->2936.732)! Learning rate decreased to 0.00301.
2024-12-02-10:48:00-root-INFO: Loss too large (2738.677->2773.090)! Learning rate decreased to 0.00241.
2024-12-02-10:48:01-root-INFO: grad norm: 393.252 379.806 101.953
2024-12-02-10:48:01-root-INFO: Loss too large (2681.523->2690.400)! Learning rate decreased to 0.00193.
2024-12-02-10:48:01-root-INFO: Loss Change: 2738.677 -> 2636.268
2024-12-02-10:48:01-root-INFO: Regularization Change: 0.000 -> 0.297
2024-12-02-10:48:01-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-10:48:01-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:48:01-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-10:48:01-root-INFO: grad norm: 305.035 291.397 90.189
2024-12-02-10:48:02-root-INFO: Loss too large (2627.014->2764.388)! Learning rate decreased to 0.00314.
2024-12-02-10:48:02-root-INFO: Loss too large (2627.014->2650.240)! Learning rate decreased to 0.00251.
2024-12-02-10:48:02-root-INFO: grad norm: 330.617 317.617 91.798
2024-12-02-10:48:02-root-INFO: Loss too large (2588.459->2589.578)! Learning rate decreased to 0.00201.
2024-12-02-10:48:03-root-INFO: Loss Change: 2627.014 -> 2549.429
2024-12-02-10:48:03-root-INFO: Regularization Change: 0.000 -> 0.269
2024-12-02-10:48:03-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-10:48:03-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:48:03-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-10:48:03-root-INFO: grad norm: 299.305 284.382 93.330
2024-12-02-10:48:03-root-INFO: Loss too large (2543.190->2712.669)! Learning rate decreased to 0.00328.
2024-12-02-10:48:03-root-INFO: Loss too large (2543.190->2588.963)! Learning rate decreased to 0.00262.
2024-12-02-10:48:04-root-INFO: grad norm: 356.006 340.313 104.535
2024-12-02-10:48:04-root-INFO: Loss too large (2520.177->2542.500)! Learning rate decreased to 0.00210.
2024-12-02-10:48:04-root-INFO: Loss Change: 2543.190 -> 2487.165
2024-12-02-10:48:04-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-10:48:04-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-10:48:04-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:48:04-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-10:48:05-root-INFO: grad norm: 321.034 305.149 99.735
2024-12-02-10:48:05-root-INFO: Loss too large (2482.122->2704.354)! Learning rate decreased to 0.00342.
2024-12-02-10:48:05-root-INFO: Loss too large (2482.122->2553.051)! Learning rate decreased to 0.00273.
2024-12-02-10:48:05-root-INFO: grad norm: 380.175 360.713 120.081
2024-12-02-10:48:05-root-INFO: Loss too large (2466.269->2493.476)! Learning rate decreased to 0.00219.
2024-12-02-10:48:06-root-INFO: Loss Change: 2482.122 -> 2426.220
2024-12-02-10:48:06-root-INFO: Regularization Change: 0.000 -> 0.246
2024-12-02-10:48:06-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-10:48:06-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-10:48:06-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-10:48:06-root-INFO: grad norm: 333.175 316.002 105.586
2024-12-02-10:48:06-root-INFO: Loss too large (2426.522->2688.284)! Learning rate decreased to 0.00356.
2024-12-02-10:48:06-root-INFO: Loss too large (2426.522->2519.233)! Learning rate decreased to 0.00285.
2024-12-02-10:48:07-root-INFO: grad norm: 393.557 370.005 134.103
2024-12-02-10:48:07-root-INFO: Loss too large (2420.170->2447.608)! Learning rate decreased to 0.00228.
2024-12-02-10:48:07-root-INFO: Loss Change: 2426.522 -> 2370.783
2024-12-02-10:48:07-root-INFO: Regularization Change: 0.000 -> 0.238
2024-12-02-10:48:07-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-10:48:07-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-10:48:08-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-10:48:08-root-INFO: grad norm: 328.184 311.258 104.035
2024-12-02-10:48:08-root-INFO: Loss too large (2365.572->2631.823)! Learning rate decreased to 0.00371.
2024-12-02-10:48:08-root-INFO: Loss too large (2365.572->2463.511)! Learning rate decreased to 0.00297.
2024-12-02-10:48:08-root-INFO: grad norm: 383.841 359.371 134.856
2024-12-02-10:48:09-root-INFO: Loss too large (2364.217->2383.562)! Learning rate decreased to 0.00238.
2024-12-02-10:48:09-root-INFO: Loss Change: 2365.572 -> 2308.330
2024-12-02-10:48:09-root-INFO: Regularization Change: 0.000 -> 0.238
2024-12-02-10:48:09-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-10:48:09-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:48:09-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-10:48:09-root-INFO: grad norm: 328.566 312.472 101.571
2024-12-02-10:48:10-root-INFO: Loss too large (2310.935->2599.199)! Learning rate decreased to 0.00387.
2024-12-02-10:48:10-root-INFO: Loss too large (2310.935->2420.569)! Learning rate decreased to 0.00309.
2024-12-02-10:48:10-root-INFO: Loss too large (2310.935->2314.565)! Learning rate decreased to 0.00248.
2024-12-02-10:48:10-root-INFO: grad norm: 262.569 244.483 95.761
2024-12-02-10:48:11-root-INFO: Loss Change: 2310.935 -> 2226.673
2024-12-02-10:48:11-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-02-10:48:11-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-10:48:11-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:48:11-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-10:48:11-root-INFO: grad norm: 255.015 242.040 80.307
2024-12-02-10:48:11-root-INFO: Loss too large (2222.675->2409.396)! Learning rate decreased to 0.00403.
2024-12-02-10:48:11-root-INFO: Loss too large (2222.675->2295.687)! Learning rate decreased to 0.00322.
2024-12-02-10:48:11-root-INFO: Loss too large (2222.675->2229.164)! Learning rate decreased to 0.00258.
2024-12-02-10:48:12-root-INFO: grad norm: 227.683 211.657 83.910
2024-12-02-10:48:12-root-INFO: Loss Change: 2222.675 -> 2167.581
2024-12-02-10:48:12-root-INFO: Regularization Change: 0.000 -> 0.212
2024-12-02-10:48:12-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-10:48:12-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:48:12-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-10:48:13-root-INFO: grad norm: 252.017 240.247 76.116
2024-12-02-10:48:13-root-INFO: Loss too large (2166.854->2368.413)! Learning rate decreased to 0.00420.
2024-12-02-10:48:13-root-INFO: Loss too large (2166.854->2249.816)! Learning rate decreased to 0.00336.
2024-12-02-10:48:13-root-INFO: Loss too large (2166.854->2179.706)! Learning rate decreased to 0.00269.
2024-12-02-10:48:13-root-INFO: grad norm: 235.471 220.167 83.506
2024-12-02-10:48:14-root-INFO: Loss Change: 2166.854 -> 2118.071
2024-12-02-10:48:14-root-INFO: Regularization Change: 0.000 -> 0.221
2024-12-02-10:48:14-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-10:48:14-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:48:14-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-10:48:14-root-INFO: grad norm: 255.846 244.768 74.470
2024-12-02-10:48:14-root-INFO: Loss too large (2118.000->2345.234)! Learning rate decreased to 0.00437.
2024-12-02-10:48:14-root-INFO: Loss too large (2118.000->2214.684)! Learning rate decreased to 0.00350.
2024-12-02-10:48:15-root-INFO: Loss too large (2118.000->2136.729)! Learning rate decreased to 0.00280.
2024-12-02-10:48:15-root-INFO: grad norm: 243.362 228.536 83.645
2024-12-02-10:48:15-root-INFO: Loss Change: 2118.000 -> 2070.981
2024-12-02-10:48:15-root-INFO: Regularization Change: 0.000 -> 0.226
2024-12-02-10:48:15-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-10:48:15-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-10:48:16-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-10:48:16-root-INFO: grad norm: 268.011 257.460 74.459
2024-12-02-10:48:16-root-INFO: Loss too large (2072.094->2330.292)! Learning rate decreased to 0.00455.
2024-12-02-10:48:16-root-INFO: Loss too large (2072.094->2185.064)! Learning rate decreased to 0.00364.
2024-12-02-10:48:16-root-INFO: Loss too large (2072.094->2096.707)! Learning rate decreased to 0.00291.
2024-12-02-10:48:17-root-INFO: grad norm: 252.128 237.782 83.834
2024-12-02-10:48:17-root-INFO: Loss Change: 2072.094 -> 2021.631
2024-12-02-10:48:17-root-INFO: Regularization Change: 0.000 -> 0.241
2024-12-02-10:48:17-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-10:48:17-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:48:17-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-10:48:17-root-INFO: grad norm: 290.924 279.901 79.323
2024-12-02-10:48:17-root-INFO: Loss too large (2028.139->2337.639)! Learning rate decreased to 0.00474.
2024-12-02-10:48:18-root-INFO: Loss too large (2028.139->2170.256)! Learning rate decreased to 0.00379.
2024-12-02-10:48:18-root-INFO: Loss too large (2028.139->2064.483)! Learning rate decreased to 0.00303.
2024-12-02-10:48:18-root-INFO: grad norm: 268.576 254.350 86.251
2024-12-02-10:48:18-root-INFO: Loss Change: 2028.139 -> 1972.115
2024-12-02-10:48:18-root-INFO: Regularization Change: 0.000 -> 0.247
2024-12-02-10:48:18-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-10:48:18-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:48:19-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-10:48:19-root-INFO: grad norm: 267.024 256.567 73.995
2024-12-02-10:48:19-root-INFO: Loss too large (1977.982->2251.874)! Learning rate decreased to 0.00494.
2024-12-02-10:48:19-root-INFO: Loss too large (1977.982->2103.434)! Learning rate decreased to 0.00395.
2024-12-02-10:48:19-root-INFO: Loss too large (1977.982->2009.873)! Learning rate decreased to 0.00316.
2024-12-02-10:48:20-root-INFO: grad norm: 251.930 238.746 80.428
2024-12-02-10:48:20-root-INFO: Loss Change: 1977.982 -> 1929.212
2024-12-02-10:48:20-root-INFO: Regularization Change: 0.000 -> 0.245
2024-12-02-10:48:20-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-10:48:20-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:48:20-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-10:48:20-root-INFO: grad norm: 300.775 286.839 90.492
2024-12-02-10:48:20-root-INFO: Loss too large (1945.321->2236.245)! Learning rate decreased to 0.00514.
2024-12-02-10:48:21-root-INFO: Loss too large (1945.321->2087.887)! Learning rate decreased to 0.00411.
2024-12-02-10:48:21-root-INFO: Loss too large (1945.321->1986.312)! Learning rate decreased to 0.00329.
2024-12-02-10:48:21-root-INFO: grad norm: 261.570 248.129 82.771
2024-12-02-10:48:22-root-INFO: Loss Change: 1945.321 -> 1875.300
2024-12-02-10:48:22-root-INFO: Regularization Change: 0.000 -> 0.271
2024-12-02-10:48:22-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-10:48:22-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-10:48:22-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-10:48:22-root-INFO: grad norm: 246.539 234.923 74.783
2024-12-02-10:48:22-root-INFO: Loss too large (1879.262->2086.998)! Learning rate decreased to 0.00535.
2024-12-02-10:48:22-root-INFO: Loss too large (1879.262->1977.629)! Learning rate decreased to 0.00428.
2024-12-02-10:48:22-root-INFO: Loss too large (1879.262->1903.854)! Learning rate decreased to 0.00342.
2024-12-02-10:48:23-root-INFO: grad norm: 224.514 212.442 72.627
2024-12-02-10:48:23-root-INFO: Loss Change: 1879.262 -> 1827.100
2024-12-02-10:48:23-root-INFO: Regularization Change: 0.000 -> 0.259
2024-12-02-10:48:23-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-10:48:23-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:48:23-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-10:48:23-root-INFO: grad norm: 246.919 234.091 78.553
2024-12-02-10:48:24-root-INFO: Loss too large (1835.824->2014.752)! Learning rate decreased to 0.00556.
2024-12-02-10:48:24-root-INFO: Loss too large (1835.824->1923.862)! Learning rate decreased to 0.00445.
2024-12-02-10:48:24-root-INFO: Loss too large (1835.824->1857.022)! Learning rate decreased to 0.00356.
2024-12-02-10:48:24-root-INFO: grad norm: 217.614 205.312 72.129
2024-12-02-10:48:25-root-INFO: Loss Change: 1835.824 -> 1778.845
2024-12-02-10:48:25-root-INFO: Regularization Change: 0.000 -> 0.268
2024-12-02-10:48:25-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-10:48:25-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:48:25-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-10:48:25-root-INFO: grad norm: 232.007 219.745 74.427
2024-12-02-10:48:25-root-INFO: Loss too large (1789.769->1930.664)! Learning rate decreased to 0.00579.
2024-12-02-10:48:25-root-INFO: Loss too large (1789.769->1859.072)! Learning rate decreased to 0.00463.
2024-12-02-10:48:25-root-INFO: Loss too large (1789.769->1802.752)! Learning rate decreased to 0.00370.
2024-12-02-10:48:26-root-INFO: grad norm: 203.309 191.532 68.191
2024-12-02-10:48:26-root-INFO: Loss Change: 1789.769 -> 1733.465
2024-12-02-10:48:26-root-INFO: Regularization Change: 0.000 -> 0.290
2024-12-02-10:48:26-root-INFO: Undo step: 170
2024-12-02-10:48:26-root-INFO: Undo step: 171
2024-12-02-10:48:26-root-INFO: Undo step: 172
2024-12-02-10:48:26-root-INFO: Undo step: 173
2024-12-02-10:48:26-root-INFO: Undo step: 174
2024-12-02-10:48:26-root-INFO: Undo step: 175
2024-12-02-10:48:26-root-INFO: Undo step: 176
2024-12-02-10:48:26-root-INFO: Undo step: 177
2024-12-02-10:48:26-root-INFO: Undo step: 178
2024-12-02-10:48:26-root-INFO: Undo step: 179
2024-12-02-10:48:26-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-10:48:26-root-INFO: grad norm: 893.242 850.906 271.735
2024-12-02-10:48:27-root-INFO: grad norm: 655.661 629.648 182.851
2024-12-02-10:48:27-root-INFO: Loss Change: 3792.405 -> 2700.555
2024-12-02-10:48:27-root-INFO: Regularization Change: 0.000 -> 11.607
2024-12-02-10:48:27-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-10:48:27-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:48:27-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-10:48:28-root-INFO: grad norm: 434.394 418.018 118.149
2024-12-02-10:48:28-root-INFO: grad norm: 507.941 476.605 175.647
2024-12-02-10:48:28-root-INFO: Loss too large (2622.915->2649.538)! Learning rate decreased to 0.00403.
2024-12-02-10:48:28-root-INFO: Loss Change: 2693.708 -> 2444.219
2024-12-02-10:48:28-root-INFO: Regularization Change: 0.000 -> 3.027
2024-12-02-10:48:28-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-10:48:28-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:48:29-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-10:48:29-root-INFO: grad norm: 436.314 417.662 126.206
2024-12-02-10:48:29-root-INFO: Loss too large (2437.567->2456.724)! Learning rate decreased to 0.00420.
2024-12-02-10:48:29-root-INFO: grad norm: 335.945 318.535 106.745
2024-12-02-10:48:30-root-INFO: Loss Change: 2437.567 -> 2166.829
2024-12-02-10:48:30-root-INFO: Regularization Change: 0.000 -> 1.698
2024-12-02-10:48:30-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-10:48:30-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-10:48:30-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-10:48:30-root-INFO: grad norm: 254.033 243.016 74.000
2024-12-02-10:48:30-root-INFO: Loss too large (2154.203->2154.621)! Learning rate decreased to 0.00437.
2024-12-02-10:48:31-root-INFO: grad norm: 257.836 243.298 85.357
2024-12-02-10:48:31-root-INFO: Loss Change: 2154.203 -> 2064.553
2024-12-02-10:48:31-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-02-10:48:31-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-10:48:31-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-10:48:31-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-10:48:31-root-INFO: grad norm: 309.038 297.581 83.368
2024-12-02-10:48:31-root-INFO: Loss too large (2059.769->2167.039)! Learning rate decreased to 0.00455.
2024-12-02-10:48:31-root-INFO: Loss too large (2059.769->2061.373)! Learning rate decreased to 0.00364.
2024-12-02-10:48:32-root-INFO: grad norm: 257.135 243.873 81.512
2024-12-02-10:48:32-root-INFO: Loss Change: 2059.769 -> 1940.752
2024-12-02-10:48:32-root-INFO: Regularization Change: 0.000 -> 0.655
2024-12-02-10:48:32-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-10:48:32-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:48:32-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-10:48:32-root-INFO: grad norm: 229.622 220.055 65.591
2024-12-02-10:48:33-root-INFO: Loss too large (1930.495->1997.963)! Learning rate decreased to 0.00474.
2024-12-02-10:48:33-root-INFO: grad norm: 290.767 275.316 93.522
2024-12-02-10:48:33-root-INFO: Loss too large (1929.329->1951.648)! Learning rate decreased to 0.00379.
2024-12-02-10:48:34-root-INFO: Loss Change: 1930.495 -> 1879.708
2024-12-02-10:48:34-root-INFO: Regularization Change: 0.000 -> 0.620
2024-12-02-10:48:34-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-10:48:34-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:48:34-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-10:48:34-root-INFO: grad norm: 256.202 245.049 74.767
2024-12-02-10:48:34-root-INFO: Loss too large (1878.464->1998.071)! Learning rate decreased to 0.00494.
2024-12-02-10:48:34-root-INFO: Loss too large (1878.464->1896.526)! Learning rate decreased to 0.00395.
2024-12-02-10:48:35-root-INFO: grad norm: 235.628 224.034 73.003
2024-12-02-10:48:35-root-INFO: Loss Change: 1878.464 -> 1811.317
2024-12-02-10:48:35-root-INFO: Regularization Change: 0.000 -> 0.416
2024-12-02-10:48:35-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-10:48:35-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-10:48:35-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-10:48:35-root-INFO: grad norm: 237.994 224.540 78.885
2024-12-02-10:48:35-root-INFO: Loss too large (1810.339->1908.016)! Learning rate decreased to 0.00514.
2024-12-02-10:48:35-root-INFO: Loss too large (1810.339->1820.184)! Learning rate decreased to 0.00411.
2024-12-02-10:48:36-root-INFO: grad norm: 212.749 201.342 68.729
2024-12-02-10:48:36-root-INFO: Loss Change: 1810.339 -> 1745.580
2024-12-02-10:48:36-root-INFO: Regularization Change: 0.000 -> 0.402
2024-12-02-10:48:36-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-10:48:36-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-10:48:36-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-10:48:37-root-INFO: grad norm: 205.037 192.162 71.512
2024-12-02-10:48:37-root-INFO: Loss too large (1744.924->1827.216)! Learning rate decreased to 0.00535.
2024-12-02-10:48:37-root-INFO: Loss too large (1744.924->1752.512)! Learning rate decreased to 0.00428.
2024-12-02-10:48:37-root-INFO: grad norm: 188.938 178.848 60.919
2024-12-02-10:48:38-root-INFO: Loss Change: 1744.924 -> 1695.592
2024-12-02-10:48:38-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-02-10:48:38-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-10:48:38-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:48:38-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-10:48:38-root-INFO: grad norm: 195.064 181.227 72.159
2024-12-02-10:48:38-root-INFO: Loss too large (1693.747->1779.054)! Learning rate decreased to 0.00556.
2024-12-02-10:48:38-root-INFO: Loss too large (1693.747->1703.833)! Learning rate decreased to 0.00445.
2024-12-02-10:48:39-root-INFO: grad norm: 185.161 174.549 61.786
2024-12-02-10:48:39-root-INFO: Loss Change: 1693.747 -> 1652.779
2024-12-02-10:48:39-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-02-10:48:39-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-10:48:39-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:48:39-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-10:48:39-root-INFO: grad norm: 199.507 184.202 76.634
2024-12-02-10:48:40-root-INFO: Loss too large (1657.705->1753.777)! Learning rate decreased to 0.00579.
2024-12-02-10:48:40-root-INFO: Loss too large (1657.705->1670.117)! Learning rate decreased to 0.00463.
2024-12-02-10:48:40-root-INFO: grad norm: 188.714 177.729 63.448
2024-12-02-10:48:40-root-INFO: Loss Change: 1657.705 -> 1616.369
2024-12-02-10:48:40-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-02-10:48:40-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-10:48:40-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:48:41-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-10:48:41-root-INFO: grad norm: 193.982 179.513 73.513
2024-12-02-10:48:41-root-INFO: Loss too large (1622.007->1709.665)! Learning rate decreased to 0.00602.
2024-12-02-10:48:41-root-INFO: Loss too large (1622.007->1628.235)! Learning rate decreased to 0.00482.
2024-12-02-10:48:42-root-INFO: grad norm: 174.081 164.342 57.410
2024-12-02-10:48:42-root-INFO: Loss Change: 1622.007 -> 1574.833
2024-12-02-10:48:42-root-INFO: Regularization Change: 0.000 -> 0.324
2024-12-02-10:48:42-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-10:48:42-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:48:42-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-10:48:42-root-INFO: grad norm: 178.983 166.595 65.430
2024-12-02-10:48:42-root-INFO: Loss too large (1582.369->1656.726)! Learning rate decreased to 0.00626.
2024-12-02-10:48:42-root-INFO: Loss too large (1582.369->1585.200)! Learning rate decreased to 0.00501.
2024-12-02-10:48:43-root-INFO: grad norm: 157.457 149.131 50.524
2024-12-02-10:48:43-root-INFO: Loss Change: 1582.369 -> 1536.617
2024-12-02-10:48:43-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-02-10:48:43-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-10:48:43-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-10:48:43-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-10:48:44-root-INFO: grad norm: 167.073 156.211 59.258
2024-12-02-10:48:44-root-INFO: Loss too large (1543.692->1608.525)! Learning rate decreased to 0.00651.
2024-12-02-10:48:44-root-INFO: Loss too large (1543.692->1545.633)! Learning rate decreased to 0.00521.
2024-12-02-10:48:44-root-INFO: grad norm: 148.370 140.817 46.736
2024-12-02-10:48:45-root-INFO: Loss Change: 1543.692 -> 1500.934
2024-12-02-10:48:45-root-INFO: Regularization Change: 0.000 -> 0.329
2024-12-02-10:48:45-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-10:48:45-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:48:45-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-10:48:45-root-INFO: grad norm: 156.532 147.996 50.987
2024-12-02-10:48:45-root-INFO: Loss too large (1504.976->1560.854)! Learning rate decreased to 0.00677.
2024-12-02-10:48:46-root-INFO: grad norm: 211.222 200.276 67.115
2024-12-02-10:48:46-root-INFO: Loss too large (1504.076->1543.265)! Learning rate decreased to 0.00542.
2024-12-02-10:48:46-root-INFO: Loss Change: 1504.976 -> 1492.046
2024-12-02-10:48:46-root-INFO: Regularization Change: 0.000 -> 0.423
2024-12-02-10:48:46-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-10:48:46-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:48:46-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-10:48:46-root-INFO: grad norm: 183.795 175.462 54.714
2024-12-02-10:48:47-root-INFO: Loss too large (1499.161->1556.971)! Learning rate decreased to 0.00704.
2024-12-02-10:48:47-root-INFO: grad norm: 216.032 205.633 66.217
2024-12-02-10:48:47-root-INFO: Loss too large (1480.776->1513.395)! Learning rate decreased to 0.00563.
2024-12-02-10:48:47-root-INFO: Loss Change: 1499.161 -> 1460.050
2024-12-02-10:48:47-root-INFO: Regularization Change: 0.000 -> 0.404
2024-12-02-10:48:47-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-10:48:47-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:48:48-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-10:48:48-root-INFO: grad norm: 178.994 171.363 51.704
2024-12-02-10:48:48-root-INFO: Loss too large (1468.606->1512.038)! Learning rate decreased to 0.00732.
2024-12-02-10:48:48-root-INFO: grad norm: 197.654 188.971 57.941
2024-12-02-10:48:49-root-INFO: Loss too large (1439.958->1463.532)! Learning rate decreased to 0.00585.
2024-12-02-10:48:49-root-INFO: Loss Change: 1468.606 -> 1418.217
2024-12-02-10:48:49-root-INFO: Regularization Change: 0.000 -> 0.429
2024-12-02-10:48:49-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-10:48:49-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-10:48:49-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-10:48:49-root-INFO: grad norm: 158.161 152.000 43.714
2024-12-02-10:48:49-root-INFO: Loss too large (1420.209->1452.205)! Learning rate decreased to 0.00760.
2024-12-02-10:48:50-root-INFO: grad norm: 173.732 166.834 48.467
2024-12-02-10:48:50-root-INFO: Loss too large (1394.083->1410.663)! Learning rate decreased to 0.00608.
2024-12-02-10:48:50-root-INFO: Loss Change: 1420.209 -> 1374.844
2024-12-02-10:48:50-root-INFO: Regularization Change: 0.000 -> 0.408
2024-12-02-10:48:50-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-10:48:50-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:48:50-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-10:48:51-root-INFO: grad norm: 147.239 141.446 40.894
2024-12-02-10:48:51-root-INFO: Loss too large (1387.201->1412.769)! Learning rate decreased to 0.00790.
2024-12-02-10:48:51-root-INFO: grad norm: 162.620 156.473 44.289
2024-12-02-10:48:51-root-INFO: Loss too large (1361.908->1375.641)! Learning rate decreased to 0.00632.
2024-12-02-10:48:52-root-INFO: Loss Change: 1387.201 -> 1343.843
2024-12-02-10:48:52-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-10:48:52-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-10:48:52-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:48:52-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-10:48:52-root-INFO: grad norm: 134.006 129.449 34.652
2024-12-02-10:48:52-root-INFO: Loss too large (1347.113->1363.532)! Learning rate decreased to 0.00821.
2024-12-02-10:48:53-root-INFO: grad norm: 143.570 138.634 37.320
2024-12-02-10:48:53-root-INFO: Loss too large (1320.628->1328.898)! Learning rate decreased to 0.00656.
2024-12-02-10:48:53-root-INFO: Loss Change: 1347.113 -> 1303.784
2024-12-02-10:48:53-root-INFO: Regularization Change: 0.000 -> 0.440
2024-12-02-10:48:53-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-10:48:53-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:48:53-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-10:48:53-root-INFO: grad norm: 119.479 115.329 31.218
2024-12-02-10:48:54-root-INFO: Loss too large (1310.009->1322.269)! Learning rate decreased to 0.00852.
2024-12-02-10:48:54-root-INFO: grad norm: 131.464 127.184 33.272
2024-12-02-10:48:54-root-INFO: Loss too large (1287.534->1293.668)! Learning rate decreased to 0.00682.
2024-12-02-10:48:54-root-INFO: Loss Change: 1310.009 -> 1272.216
2024-12-02-10:48:54-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-02-10:48:54-root-INFO: Undo step: 160
2024-12-02-10:48:54-root-INFO: Undo step: 161
2024-12-02-10:48:54-root-INFO: Undo step: 162
2024-12-02-10:48:54-root-INFO: Undo step: 163
2024-12-02-10:48:55-root-INFO: Undo step: 164
2024-12-02-10:48:55-root-INFO: Undo step: 165
2024-12-02-10:48:55-root-INFO: Undo step: 166
2024-12-02-10:48:55-root-INFO: Undo step: 167
2024-12-02-10:48:55-root-INFO: Undo step: 168
2024-12-02-10:48:55-root-INFO: Undo step: 169
2024-12-02-10:48:55-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-10:48:55-root-INFO: grad norm: 700.813 680.555 167.282
2024-12-02-10:48:55-root-INFO: grad norm: 1045.528 1025.202 205.159
2024-12-02-10:48:55-root-INFO: Loss too large (3087.437->3361.263)! Learning rate decreased to 0.00579.
2024-12-02-10:48:56-root-INFO: Loss Change: 3181.275 -> 2856.854
2024-12-02-10:48:56-root-INFO: Regularization Change: 0.000 -> 26.887
2024-12-02-10:48:56-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-10:48:56-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:48:56-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-10:48:56-root-INFO: grad norm: 553.074 538.117 127.755
2024-12-02-10:48:56-root-INFO: grad norm: 540.180 527.811 114.936
2024-12-02-10:48:57-root-INFO: Loss Change: 2873.075 -> 2247.273
2024-12-02-10:48:57-root-INFO: Regularization Change: 0.000 -> 18.124
2024-12-02-10:48:57-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-10:48:57-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-10:48:57-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-10:48:57-root-INFO: grad norm: 308.004 302.844 56.143
2024-12-02-10:48:58-root-INFO: grad norm: 296.576 286.133 78.009
2024-12-02-10:48:58-root-INFO: Loss Change: 2233.058 -> 1808.489
2024-12-02-10:48:58-root-INFO: Regularization Change: 0.000 -> 9.685
2024-12-02-10:48:58-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-10:48:58-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-10:48:58-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-10:48:58-root-INFO: grad norm: 305.032 297.106 69.085
2024-12-02-10:48:59-root-INFO: grad norm: 258.525 247.442 74.884
2024-12-02-10:48:59-root-INFO: Loss Change: 1812.252 -> 1639.629
2024-12-02-10:48:59-root-INFO: Regularization Change: 0.000 -> 2.148
2024-12-02-10:48:59-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-10:48:59-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:48:59-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-10:48:59-root-INFO: grad norm: 240.375 233.400 57.487
2024-12-02-10:49:00-root-INFO: grad norm: 317.380 300.523 102.060
2024-12-02-10:49:00-root-INFO: Loss too large (1628.297->1742.097)! Learning rate decreased to 0.00677.
2024-12-02-10:49:00-root-INFO: Loss Change: 1638.606 -> 1615.118
2024-12-02-10:49:00-root-INFO: Regularization Change: 0.000 -> 1.615
2024-12-02-10:49:00-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-10:49:00-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:49:00-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-10:49:01-root-INFO: grad norm: 238.804 231.259 59.553
2024-12-02-10:49:01-root-INFO: grad norm: 260.601 247.658 81.107
2024-12-02-10:49:01-root-INFO: Loss too large (1507.718->1608.740)! Learning rate decreased to 0.00704.
2024-12-02-10:49:01-root-INFO: Loss too large (1507.718->1521.985)! Learning rate decreased to 0.00563.
2024-12-02-10:49:02-root-INFO: Loss Change: 1621.006 -> 1467.717
2024-12-02-10:49:02-root-INFO: Regularization Change: 0.000 -> 1.272
2024-12-02-10:49:02-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-10:49:02-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-10:49:02-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-10:49:02-root-INFO: grad norm: 169.131 162.792 45.869
2024-12-02-10:49:02-root-INFO: grad norm: 211.986 202.919 61.335
2024-12-02-10:49:03-root-INFO: Loss too large (1427.625->1510.411)! Learning rate decreased to 0.00732.
2024-12-02-10:49:03-root-INFO: Loss too large (1427.625->1443.029)! Learning rate decreased to 0.00585.
2024-12-02-10:49:03-root-INFO: Loss Change: 1476.068 -> 1401.746
2024-12-02-10:49:03-root-INFO: Regularization Change: 0.000 -> 0.798
2024-12-02-10:49:03-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-10:49:03-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-10:49:03-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-10:49:03-root-INFO: grad norm: 143.254 137.586 39.896
2024-12-02-10:49:04-root-INFO: grad norm: 197.445 190.079 53.430
2024-12-02-10:49:04-root-INFO: Loss too large (1377.648->1451.669)! Learning rate decreased to 0.00760.
2024-12-02-10:49:04-root-INFO: Loss too large (1377.648->1391.975)! Learning rate decreased to 0.00608.
2024-12-02-10:49:04-root-INFO: Loss Change: 1402.990 -> 1355.227
2024-12-02-10:49:04-root-INFO: Regularization Change: 0.000 -> 0.577
2024-12-02-10:49:04-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-10:49:04-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:49:05-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-10:49:05-root-INFO: grad norm: 140.473 134.959 38.970
2024-12-02-10:49:05-root-INFO: grad norm: 189.289 183.003 48.375
2024-12-02-10:49:05-root-INFO: Loss too large (1341.729->1413.580)! Learning rate decreased to 0.00790.
2024-12-02-10:49:05-root-INFO: Loss too large (1341.729->1353.771)! Learning rate decreased to 0.00632.
2024-12-02-10:49:06-root-INFO: Loss Change: 1365.564 -> 1317.847
2024-12-02-10:49:06-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-10:49:06-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-10:49:06-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:49:06-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-10:49:06-root-INFO: grad norm: 129.429 124.601 35.021
2024-12-02-10:49:07-root-INFO: grad norm: 174.621 169.314 42.723
2024-12-02-10:49:07-root-INFO: Loss too large (1300.575->1364.210)! Learning rate decreased to 0.00821.
2024-12-02-10:49:07-root-INFO: Loss too large (1300.575->1310.571)! Learning rate decreased to 0.00656.
2024-12-02-10:49:07-root-INFO: Loss Change: 1322.096 -> 1278.481
2024-12-02-10:49:07-root-INFO: Regularization Change: 0.000 -> 0.548
2024-12-02-10:49:07-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-10:49:07-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:49:07-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-10:49:07-root-INFO: grad norm: 122.560 117.843 33.676
2024-12-02-10:49:08-root-INFO: grad norm: 185.207 180.229 42.650
2024-12-02-10:49:08-root-INFO: Loss too large (1272.513->1359.484)! Learning rate decreased to 0.00852.
2024-12-02-10:49:08-root-INFO: Loss too large (1272.513->1296.336)! Learning rate decreased to 0.00682.
2024-12-02-10:49:09-root-INFO: Loss Change: 1283.076 -> 1257.084
2024-12-02-10:49:09-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-10:49:09-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-10:49:09-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:49:09-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-10:49:09-root-INFO: grad norm: 130.850 126.225 34.481
2024-12-02-10:49:09-root-INFO: grad norm: 188.291 183.582 41.847
2024-12-02-10:49:09-root-INFO: Loss too large (1244.363->1327.339)! Learning rate decreased to 0.00885.
2024-12-02-10:49:10-root-INFO: Loss too large (1244.363->1263.686)! Learning rate decreased to 0.00708.
2024-12-02-10:49:10-root-INFO: Loss Change: 1260.721 -> 1224.152
2024-12-02-10:49:10-root-INFO: Regularization Change: 0.000 -> 0.542
2024-12-02-10:49:10-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-10:49:10-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-10:49:10-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-10:49:10-root-INFO: grad norm: 128.181 123.642 33.809
2024-12-02-10:49:11-root-INFO: grad norm: 166.761 162.750 36.356
2024-12-02-10:49:11-root-INFO: Loss too large (1206.943->1267.958)! Learning rate decreased to 0.00919.
2024-12-02-10:49:11-root-INFO: Loss too large (1206.943->1213.671)! Learning rate decreased to 0.00735.
2024-12-02-10:49:11-root-INFO: Loss Change: 1230.406 -> 1181.601
2024-12-02-10:49:11-root-INFO: Regularization Change: 0.000 -> 0.602
2024-12-02-10:49:11-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-10:49:11-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:49:11-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-10:49:12-root-INFO: grad norm: 113.345 109.423 29.559
2024-12-02-10:49:12-root-INFO: grad norm: 162.250 158.377 35.237
2024-12-02-10:49:12-root-INFO: Loss too large (1173.863->1240.782)! Learning rate decreased to 0.00954.
2024-12-02-10:49:12-root-INFO: Loss too large (1173.863->1186.170)! Learning rate decreased to 0.00763.
2024-12-02-10:49:13-root-INFO: Loss Change: 1186.102 -> 1153.601
2024-12-02-10:49:13-root-INFO: Regularization Change: 0.000 -> 0.553
2024-12-02-10:49:13-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-10:49:13-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:49:13-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-10:49:13-root-INFO: grad norm: 108.112 104.841 26.390
2024-12-02-10:49:13-root-INFO: grad norm: 142.832 139.631 30.068
2024-12-02-10:49:14-root-INFO: Loss too large (1139.131->1186.588)! Learning rate decreased to 0.00991.
2024-12-02-10:49:14-root-INFO: Loss too large (1139.131->1142.111)! Learning rate decreased to 0.00792.
2024-12-02-10:49:14-root-INFO: Loss Change: 1156.568 -> 1116.546
2024-12-02-10:49:14-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-02-10:49:14-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-10:49:14-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:49:14-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-10:49:14-root-INFO: grad norm: 100.787 97.807 24.331
2024-12-02-10:49:15-root-INFO: grad norm: 142.440 139.394 29.298
2024-12-02-10:49:15-root-INFO: Loss too large (1112.011->1164.680)! Learning rate decreased to 0.01028.
2024-12-02-10:49:15-root-INFO: Loss too large (1112.011->1117.140)! Learning rate decreased to 0.00822.
2024-12-02-10:49:16-root-INFO: Loss Change: 1120.856 -> 1089.916
2024-12-02-10:49:16-root-INFO: Regularization Change: 0.000 -> 0.551
2024-12-02-10:49:16-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-10:49:16-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-10:49:16-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-10:49:16-root-INFO: grad norm: 100.612 97.889 23.252
2024-12-02-10:49:16-root-INFO: grad norm: 141.599 138.797 28.034
2024-12-02-10:49:17-root-INFO: Loss too large (1086.664->1140.335)! Learning rate decreased to 0.01067.
2024-12-02-10:49:17-root-INFO: Loss too large (1086.664->1090.331)! Learning rate decreased to 0.00853.
2024-12-02-10:49:17-root-INFO: Loss Change: 1093.652 -> 1062.144
2024-12-02-10:49:17-root-INFO: Regularization Change: 0.000 -> 0.561
2024-12-02-10:49:17-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-10:49:17-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-10:49:17-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-10:49:17-root-INFO: grad norm: 101.911 99.379 22.575
2024-12-02-10:49:18-root-INFO: grad norm: 142.675 140.053 27.225
2024-12-02-10:49:18-root-INFO: Loss too large (1063.508->1118.661)! Learning rate decreased to 0.01107.
2024-12-02-10:49:18-root-INFO: Loss too large (1063.508->1064.106)! Learning rate decreased to 0.00885.
2024-12-02-10:49:18-root-INFO: Loss Change: 1066.284 -> 1034.008
2024-12-02-10:49:18-root-INFO: Regularization Change: 0.000 -> 0.583
2024-12-02-10:49:18-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-10:49:18-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-10:49:19-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-10:49:19-root-INFO: grad norm: 96.392 94.090 20.941
2024-12-02-10:49:19-root-INFO: grad norm: 137.205 134.775 25.708
2024-12-02-10:49:19-root-INFO: Loss too large (1035.668->1090.455)! Learning rate decreased to 0.01148.
2024-12-02-10:49:19-root-INFO: Loss too large (1035.668->1036.749)! Learning rate decreased to 0.00919.
2024-12-02-10:49:20-root-INFO: Loss Change: 1036.402 -> 1007.152
2024-12-02-10:49:20-root-INFO: Regularization Change: 0.000 -> 0.560
2024-12-02-10:49:20-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-10:49:20-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-10:49:20-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-10:49:20-root-INFO: grad norm: 100.505 98.280 21.029
2024-12-02-10:49:20-root-INFO: Loss too large (1011.892->1020.156)! Learning rate decreased to 0.01191.
2024-12-02-10:49:21-root-INFO: grad norm: 92.905 91.629 15.345
2024-12-02-10:49:21-root-INFO: Loss Change: 1011.892 -> 981.358
2024-12-02-10:49:21-root-INFO: Regularization Change: 0.000 -> 0.609
2024-12-02-10:49:21-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-10:49:21-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:49:21-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-10:49:21-root-INFO: grad norm: 105.754 104.064 18.832
2024-12-02-10:49:22-root-INFO: Loss too large (985.582->1015.192)! Learning rate decreased to 0.01235.
2024-12-02-10:49:22-root-INFO: grad norm: 107.008 105.652 16.979
2024-12-02-10:49:22-root-INFO: Loss Change: 985.582 -> 966.262
2024-12-02-10:49:22-root-INFO: Regularization Change: 0.000 -> 0.557
2024-12-02-10:49:22-root-INFO: Undo step: 150
2024-12-02-10:49:22-root-INFO: Undo step: 151
2024-12-02-10:49:22-root-INFO: Undo step: 152
2024-12-02-10:49:22-root-INFO: Undo step: 153
2024-12-02-10:49:22-root-INFO: Undo step: 154
2024-12-02-10:49:22-root-INFO: Undo step: 155
2024-12-02-10:49:22-root-INFO: Undo step: 156
2024-12-02-10:49:22-root-INFO: Undo step: 157
2024-12-02-10:49:22-root-INFO: Undo step: 158
2024-12-02-10:49:22-root-INFO: Undo step: 159
2024-12-02-10:49:23-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-10:49:23-root-INFO: grad norm: 397.885 389.310 82.158
2024-12-02-10:49:23-root-INFO: grad norm: 212.000 206.062 49.826
2024-12-02-10:49:23-root-INFO: Loss Change: 2155.696 -> 1341.235
2024-12-02-10:49:23-root-INFO: Regularization Change: 0.000 -> 19.008
2024-12-02-10:49:23-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-10:49:23-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-10:49:24-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-10:49:24-root-INFO: grad norm: 266.432 253.868 80.851
2024-12-02-10:49:24-root-INFO: Loss too large (1326.609->1489.625)! Learning rate decreased to 0.00885.
2024-12-02-10:49:24-root-INFO: Loss too large (1326.609->1391.298)! Learning rate decreased to 0.00708.
2024-12-02-10:49:24-root-INFO: Loss too large (1326.609->1327.985)! Learning rate decreased to 0.00567.
2024-12-02-10:49:25-root-INFO: grad norm: 165.558 162.450 31.928
2024-12-02-10:49:25-root-INFO: Loss Change: 1326.609 -> 1181.341
2024-12-02-10:49:25-root-INFO: Regularization Change: 0.000 -> 1.587
2024-12-02-10:49:25-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-10:49:25-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-10:49:25-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-10:49:25-root-INFO: grad norm: 77.321 74.719 19.890
2024-12-02-10:49:26-root-INFO: grad norm: 114.576 109.556 33.542
2024-12-02-10:49:26-root-INFO: Loss too large (1135.501->1163.449)! Learning rate decreased to 0.00919.
2024-12-02-10:49:26-root-INFO: Loss too large (1135.501->1141.968)! Learning rate decreased to 0.00735.
2024-12-02-10:49:27-root-INFO: Loss Change: 1173.515 -> 1128.429
2024-12-02-10:49:27-root-INFO: Regularization Change: 0.000 -> 1.486
2024-12-02-10:49:27-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-10:49:27-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:49:27-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-10:49:27-root-INFO: grad norm: 110.574 107.473 26.004
2024-12-02-10:49:27-root-INFO: Loss too large (1129.103->1149.265)! Learning rate decreased to 0.00954.
2024-12-02-10:49:28-root-INFO: grad norm: 166.736 161.093 43.010
2024-12-02-10:49:28-root-INFO: Loss too large (1100.310->1162.711)! Learning rate decreased to 0.00763.
2024-12-02-10:49:28-root-INFO: Loss too large (1100.310->1122.981)! Learning rate decreased to 0.00611.
2024-12-02-10:49:28-root-INFO: Loss Change: 1129.103 -> 1097.593
2024-12-02-10:49:28-root-INFO: Regularization Change: 0.000 -> 0.615
2024-12-02-10:49:28-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-10:49:28-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:49:28-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-10:49:28-root-INFO: grad norm: 116.825 113.933 25.832
2024-12-02-10:49:29-root-INFO: Loss too large (1098.939->1117.603)! Learning rate decreased to 0.00991.
2024-12-02-10:49:29-root-INFO: grad norm: 145.640 141.773 33.340
2024-12-02-10:49:29-root-INFO: Loss too large (1065.714->1106.761)! Learning rate decreased to 0.00792.
2024-12-02-10:49:29-root-INFO: Loss too large (1065.714->1071.887)! Learning rate decreased to 0.00634.
2024-12-02-10:49:30-root-INFO: Loss Change: 1098.939 -> 1051.456
2024-12-02-10:49:30-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-10:49:30-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-10:49:30-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-10:49:30-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-10:49:30-root-INFO: grad norm: 107.357 104.639 24.004
2024-12-02-10:49:30-root-INFO: Loss too large (1055.374->1087.371)! Learning rate decreased to 0.01028.
2024-12-02-10:49:31-root-INFO: grad norm: 149.326 145.886 31.868
2024-12-02-10:49:31-root-INFO: Loss too large (1039.220->1085.632)! Learning rate decreased to 0.00822.
2024-12-02-10:49:31-root-INFO: Loss too large (1039.220->1046.102)! Learning rate decreased to 0.00658.
2024-12-02-10:49:31-root-INFO: Loss Change: 1055.374 -> 1023.228
2024-12-02-10:49:31-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-02-10:49:31-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-10:49:31-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-10:49:31-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-10:49:32-root-INFO: grad norm: 105.038 102.530 22.815
2024-12-02-10:49:32-root-INFO: Loss too large (1027.144->1056.866)! Learning rate decreased to 0.01067.
2024-12-02-10:49:32-root-INFO: grad norm: 138.264 135.453 27.741
2024-12-02-10:49:32-root-INFO: Loss too large (1011.239->1048.665)! Learning rate decreased to 0.00853.
2024-12-02-10:49:33-root-INFO: Loss too large (1011.239->1012.456)! Learning rate decreased to 0.00683.
2024-12-02-10:49:33-root-INFO: Loss Change: 1027.144 -> 992.142
2024-12-02-10:49:33-root-INFO: Regularization Change: 0.000 -> 0.449
2024-12-02-10:49:33-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-10:49:33-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-10:49:33-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-10:49:33-root-INFO: grad norm: 99.474 97.205 21.127
2024-12-02-10:49:33-root-INFO: Loss too large (995.794->1025.225)! Learning rate decreased to 0.01107.
2024-12-02-10:49:34-root-INFO: grad norm: 129.687 127.310 24.717
2024-12-02-10:49:34-root-INFO: Loss too large (984.924->1014.905)! Learning rate decreased to 0.00885.
2024-12-02-10:49:34-root-INFO: Loss Change: 995.794 -> 981.099
2024-12-02-10:49:34-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-02-10:49:34-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-10:49:34-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-10:49:34-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-10:49:35-root-INFO: grad norm: 121.293 119.227 22.292
2024-12-02-10:49:35-root-INFO: Loss too large (985.580->1036.403)! Learning rate decreased to 0.01148.
2024-12-02-10:49:35-root-INFO: grad norm: 142.096 139.953 24.588
2024-12-02-10:49:35-root-INFO: Loss too large (978.524->995.494)! Learning rate decreased to 0.00919.
2024-12-02-10:49:36-root-INFO: Loss Change: 985.580 -> 957.485
2024-12-02-10:49:36-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-02-10:49:36-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-10:49:36-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-10:49:36-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-10:49:36-root-INFO: grad norm: 117.787 115.693 22.111
2024-12-02-10:49:36-root-INFO: Loss too large (964.614->1003.645)! Learning rate decreased to 0.01191.
2024-12-02-10:49:37-root-INFO: grad norm: 130.612 128.583 22.929
2024-12-02-10:49:37-root-INFO: Loss too large (949.588->963.332)! Learning rate decreased to 0.00953.
2024-12-02-10:49:37-root-INFO: Loss Change: 964.614 -> 929.979
2024-12-02-10:49:37-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-10:49:37-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-10:49:37-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:49:37-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-10:49:37-root-INFO: grad norm: 106.447 104.726 19.063
2024-12-02-10:49:38-root-INFO: Loss too large (934.831->967.823)! Learning rate decreased to 0.01235.
2024-12-02-10:49:38-root-INFO: grad norm: 117.498 115.801 19.893
2024-12-02-10:49:38-root-INFO: Loss too large (924.011->930.775)! Learning rate decreased to 0.00988.
2024-12-02-10:49:38-root-INFO: Loss Change: 934.831 -> 903.357
2024-12-02-10:49:38-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-02-10:49:38-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-10:49:38-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:49:39-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-10:49:39-root-INFO: grad norm: 97.431 95.774 17.896
2024-12-02-10:49:39-root-INFO: Loss too large (907.862->936.591)! Learning rate decreased to 0.01281.
2024-12-02-10:49:39-root-INFO: grad norm: 108.854 107.137 19.253
2024-12-02-10:49:40-root-INFO: Loss too large (899.330->904.101)! Learning rate decreased to 0.01024.
2024-12-02-10:49:40-root-INFO: Loss Change: 907.862 -> 879.983
2024-12-02-10:49:40-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-02-10:49:40-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-10:49:40-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:49:40-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-10:49:40-root-INFO: grad norm: 91.695 90.083 17.118
2024-12-02-10:49:40-root-INFO: Loss too large (886.236->909.802)! Learning rate decreased to 0.01328.
2024-12-02-10:49:41-root-INFO: grad norm: 100.026 98.370 18.126
2024-12-02-10:49:41-root-INFO: Loss too large (877.185->878.129)! Learning rate decreased to 0.01062.
2024-12-02-10:49:41-root-INFO: Loss Change: 886.236 -> 857.689
2024-12-02-10:49:41-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-02-10:49:41-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-10:49:41-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-10:49:42-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-10:49:42-root-INFO: grad norm: 82.142 80.761 14.996
2024-12-02-10:49:42-root-INFO: Loss too large (860.597->878.423)! Learning rate decreased to 0.01376.
2024-12-02-10:49:42-root-INFO: grad norm: 89.642 88.229 15.855
2024-12-02-10:49:43-root-INFO: Loss Change: 860.597 -> 851.557
2024-12-02-10:49:43-root-INFO: Regularization Change: 0.000 -> 0.573
2024-12-02-10:49:43-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-10:49:43-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:49:43-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-10:49:43-root-INFO: grad norm: 106.003 104.539 17.553
2024-12-02-10:49:43-root-INFO: Loss too large (857.051->889.952)! Learning rate decreased to 0.01426.
2024-12-02-10:49:44-root-INFO: grad norm: 103.020 101.360 18.420
2024-12-02-10:49:44-root-INFO: Loss Change: 857.051 -> 835.263
2024-12-02-10:49:44-root-INFO: Regularization Change: 0.000 -> 0.581
2024-12-02-10:49:44-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-10:49:44-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:49:44-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-10:49:44-root-INFO: grad norm: 103.241 101.483 18.971
2024-12-02-10:49:44-root-INFO: Loss too large (843.676->861.282)! Learning rate decreased to 0.01478.
2024-12-02-10:49:45-root-INFO: grad norm: 91.136 89.173 18.810
2024-12-02-10:49:45-root-INFO: Loss Change: 843.676 -> 809.925
2024-12-02-10:49:45-root-INFO: Regularization Change: 0.000 -> 0.603
2024-12-02-10:49:45-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-10:49:45-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:49:45-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-10:49:45-root-INFO: grad norm: 88.255 86.585 17.086
2024-12-02-10:49:46-root-INFO: Loss too large (814.678->824.416)! Learning rate decreased to 0.01531.
2024-12-02-10:49:46-root-INFO: grad norm: 77.114 75.361 16.352
2024-12-02-10:49:46-root-INFO: Loss Change: 814.678 -> 785.016
2024-12-02-10:49:46-root-INFO: Regularization Change: 0.000 -> 0.574
2024-12-02-10:49:46-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-10:49:46-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-10:49:47-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-10:49:47-root-INFO: grad norm: 76.775 75.430 14.305
2024-12-02-10:49:47-root-INFO: Loss too large (788.604->795.931)! Learning rate decreased to 0.01586.
2024-12-02-10:49:47-root-INFO: grad norm: 68.756 67.340 13.880
2024-12-02-10:49:48-root-INFO: Loss Change: 788.604 -> 764.559
2024-12-02-10:49:48-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-10:49:48-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-10:49:48-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-10:49:48-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-10:49:48-root-INFO: grad norm: 72.223 70.861 13.962
2024-12-02-10:49:48-root-INFO: Loss too large (769.654->776.225)! Learning rate decreased to 0.01642.
2024-12-02-10:49:49-root-INFO: grad norm: 64.983 63.670 13.001
2024-12-02-10:49:49-root-INFO: Loss Change: 769.654 -> 747.162
2024-12-02-10:49:49-root-INFO: Regularization Change: 0.000 -> 0.549
2024-12-02-10:49:49-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-10:49:49-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-10:49:49-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-10:49:49-root-INFO: grad norm: 64.401 63.349 11.592
2024-12-02-10:49:49-root-INFO: Loss too large (749.551->754.080)! Learning rate decreased to 0.01701.
2024-12-02-10:49:50-root-INFO: grad norm: 59.016 58.006 10.873
2024-12-02-10:49:50-root-INFO: Loss Change: 749.551 -> 730.254
2024-12-02-10:49:50-root-INFO: Regularization Change: 0.000 -> 0.523
2024-12-02-10:49:50-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-10:49:50-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-10:49:50-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-10:49:51-root-INFO: grad norm: 61.725 60.791 10.698
2024-12-02-10:49:51-root-INFO: Loss too large (733.769->738.089)! Learning rate decreased to 0.01761.
2024-12-02-10:49:51-root-INFO: grad norm: 57.291 56.380 10.176
2024-12-02-10:49:52-root-INFO: Loss Change: 733.769 -> 715.967
2024-12-02-10:49:52-root-INFO: Regularization Change: 0.000 -> 0.517
2024-12-02-10:49:52-root-INFO: Undo step: 140
2024-12-02-10:49:52-root-INFO: Undo step: 141
2024-12-02-10:49:52-root-INFO: Undo step: 142
2024-12-02-10:49:52-root-INFO: Undo step: 143
2024-12-02-10:49:52-root-INFO: Undo step: 144
2024-12-02-10:49:52-root-INFO: Undo step: 145
2024-12-02-10:49:52-root-INFO: Undo step: 146
2024-12-02-10:49:52-root-INFO: Undo step: 147
2024-12-02-10:49:52-root-INFO: Undo step: 148
2024-12-02-10:49:52-root-INFO: Undo step: 149
2024-12-02-10:49:52-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-10:49:52-root-INFO: grad norm: 361.256 355.239 65.657
2024-12-02-10:49:52-root-INFO: grad norm: 335.507 332.508 44.755
2024-12-02-10:49:53-root-INFO: Loss Change: 1716.320 -> 1161.385
2024-12-02-10:49:53-root-INFO: Regularization Change: 0.000 -> 35.182
2024-12-02-10:49:53-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-10:49:53-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:49:53-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-10:49:53-root-INFO: grad norm: 307.226 304.598 40.099
2024-12-02-10:49:53-root-INFO: Loss too large (1146.020->1557.245)! Learning rate decreased to 0.01281.
2024-12-02-10:49:53-root-INFO: Loss too large (1146.020->1325.574)! Learning rate decreased to 0.01024.
2024-12-02-10:49:53-root-INFO: Loss too large (1146.020->1190.583)! Learning rate decreased to 0.00820.
2024-12-02-10:49:54-root-INFO: grad norm: 151.648 149.111 27.623
2024-12-02-10:49:54-root-INFO: Loss Change: 1146.020 -> 978.077
2024-12-02-10:49:54-root-INFO: Regularization Change: 0.000 -> 3.781
2024-12-02-10:49:54-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-10:49:54-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-10:49:54-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-10:49:55-root-INFO: grad norm: 78.373 76.153 18.519
2024-12-02-10:49:55-root-INFO: grad norm: 63.883 61.834 16.050
2024-12-02-10:49:55-root-INFO: Loss Change: 973.707 -> 894.128
2024-12-02-10:49:55-root-INFO: Regularization Change: 0.000 -> 2.764
2024-12-02-10:49:55-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-10:49:55-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-10:49:56-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-10:49:56-root-INFO: grad norm: 72.739 71.134 15.196
2024-12-02-10:49:56-root-INFO: grad norm: 92.789 90.968 18.296
2024-12-02-10:49:56-root-INFO: Loss too large (870.852->886.829)! Learning rate decreased to 0.01376.
2024-12-02-10:49:57-root-INFO: Loss Change: 891.766 -> 860.537
2024-12-02-10:49:57-root-INFO: Regularization Change: 0.000 -> 1.436
2024-12-02-10:49:57-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-10:49:57-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:49:57-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-10:49:57-root-INFO: grad norm: 98.403 96.847 17.430
2024-12-02-10:49:57-root-INFO: Loss too large (862.663->867.415)! Learning rate decreased to 0.01426.
2024-12-02-10:49:58-root-INFO: grad norm: 82.299 81.088 14.068
2024-12-02-10:49:58-root-INFO: Loss Change: 862.663 -> 814.845
2024-12-02-10:49:58-root-INFO: Regularization Change: 0.000 -> 1.049
2024-12-02-10:49:58-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-10:49:58-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:49:58-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-10:49:58-root-INFO: grad norm: 78.220 76.985 13.846
2024-12-02-10:49:59-root-INFO: Loss too large (817.574->820.045)! Learning rate decreased to 0.01478.
2024-12-02-10:49:59-root-INFO: grad norm: 69.396 68.333 12.104
2024-12-02-10:49:59-root-INFO: Loss Change: 817.574 -> 785.215
2024-12-02-10:49:59-root-INFO: Regularization Change: 0.000 -> 0.801
2024-12-02-10:49:59-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-10:49:59-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-10:50:00-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-10:50:00-root-INFO: grad norm: 69.041 67.992 11.993
2024-12-02-10:50:00-root-INFO: Loss too large (786.459->789.663)! Learning rate decreased to 0.01531.
2024-12-02-10:50:00-root-INFO: grad norm: 61.797 60.890 10.547
2024-12-02-10:50:01-root-INFO: Loss Change: 786.459 -> 760.135
2024-12-02-10:50:01-root-INFO: Regularization Change: 0.000 -> 0.676
2024-12-02-10:50:01-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-10:50:01-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-10:50:01-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-10:50:01-root-INFO: grad norm: 62.226 61.297 10.712
2024-12-02-10:50:01-root-INFO: Loss too large (761.371->764.493)! Learning rate decreased to 0.01586.
2024-12-02-10:50:02-root-INFO: grad norm: 56.455 55.656 9.463
2024-12-02-10:50:02-root-INFO: Loss Change: 761.371 -> 739.338
2024-12-02-10:50:02-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-10:50:02-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-10:50:02-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-10:50:02-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-10:50:02-root-INFO: grad norm: 60.292 59.413 10.257
2024-12-02-10:50:02-root-INFO: Loss too large (741.996->746.243)! Learning rate decreased to 0.01642.
2024-12-02-10:50:03-root-INFO: grad norm: 54.777 54.086 8.669
2024-12-02-10:50:03-root-INFO: Loss Change: 741.996 -> 721.532
2024-12-02-10:50:03-root-INFO: Regularization Change: 0.000 -> 0.582
2024-12-02-10:50:03-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-10:50:03-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-10:50:03-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-10:50:03-root-INFO: grad norm: 56.472 55.735 9.092
2024-12-02-10:50:04-root-INFO: Loss too large (722.491->728.317)! Learning rate decreased to 0.01701.
2024-12-02-10:50:04-root-INFO: grad norm: 52.270 51.627 8.173
2024-12-02-10:50:04-root-INFO: Loss Change: 722.491 -> 704.723
2024-12-02-10:50:04-root-INFO: Regularization Change: 0.000 -> 0.527
2024-12-02-10:50:04-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-10:50:04-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-10:50:05-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-10:50:05-root-INFO: grad norm: 54.138 53.500 8.288
2024-12-02-10:50:05-root-INFO: Loss too large (706.438->711.959)! Learning rate decreased to 0.01761.
2024-12-02-10:50:05-root-INFO: grad norm: 50.314 49.736 7.609
2024-12-02-10:50:06-root-INFO: Loss Change: 706.438 -> 689.827
2024-12-02-10:50:06-root-INFO: Regularization Change: 0.000 -> 0.515
2024-12-02-10:50:06-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-10:50:06-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:50:06-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-10:50:06-root-INFO: grad norm: 51.905 51.299 7.905
2024-12-02-10:50:06-root-INFO: Loss too large (691.596->697.059)! Learning rate decreased to 0.01823.
2024-12-02-10:50:06-root-INFO: grad norm: 48.804 48.258 7.280
2024-12-02-10:50:07-root-INFO: Loss Change: 691.596 -> 676.299
2024-12-02-10:50:07-root-INFO: Regularization Change: 0.000 -> 0.504
2024-12-02-10:50:07-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-10:50:07-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:50:07-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-10:50:07-root-INFO: grad norm: 50.431 49.913 7.210
2024-12-02-10:50:07-root-INFO: Loss too large (677.003->683.268)! Learning rate decreased to 0.01887.
2024-12-02-10:50:08-root-INFO: grad norm: 47.566 47.056 6.948
2024-12-02-10:50:08-root-INFO: Loss Change: 677.003 -> 662.669
2024-12-02-10:50:08-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-02-10:50:08-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-10:50:08-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:50:08-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-10:50:08-root-INFO: grad norm: 54.556 53.954 8.087
2024-12-02-10:50:09-root-INFO: Loss too large (666.503->675.205)! Learning rate decreased to 0.01952.
2024-12-02-10:50:09-root-INFO: grad norm: 50.091 49.597 7.012
2024-12-02-10:50:09-root-INFO: Loss Change: 666.503 -> 650.404
2024-12-02-10:50:09-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-02-10:50:09-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-10:50:09-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-10:50:09-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-10:50:10-root-INFO: grad norm: 47.031 46.590 6.424
2024-12-02-10:50:10-root-INFO: Loss too large (651.376->656.518)! Learning rate decreased to 0.02020.
2024-12-02-10:50:10-root-INFO: grad norm: 45.113 44.660 6.377
2024-12-02-10:50:11-root-INFO: Loss Change: 651.376 -> 638.780
2024-12-02-10:50:11-root-INFO: Regularization Change: 0.000 -> 0.479
2024-12-02-10:50:11-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-10:50:11-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-10:50:11-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-10:50:11-root-INFO: grad norm: 50.377 49.919 6.777
2024-12-02-10:50:11-root-INFO: Loss too large (639.884->648.810)! Learning rate decreased to 0.02090.
2024-12-02-10:50:12-root-INFO: grad norm: 47.241 46.787 6.531
2024-12-02-10:50:12-root-INFO: Loss Change: 639.884 -> 626.284
2024-12-02-10:50:12-root-INFO: Regularization Change: 0.000 -> 0.490
2024-12-02-10:50:12-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-10:50:12-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-10:50:12-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-10:50:12-root-INFO: grad norm: 47.869 47.441 6.387
2024-12-02-10:50:13-root-INFO: Loss too large (628.144->635.069)! Learning rate decreased to 0.02162.
2024-12-02-10:50:13-root-INFO: grad norm: 45.441 45.003 6.296
2024-12-02-10:50:13-root-INFO: Loss Change: 628.144 -> 615.580
2024-12-02-10:50:13-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-10:50:13-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-10:50:13-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-10:50:13-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-10:50:14-root-INFO: grad norm: 49.124 48.649 6.817
2024-12-02-10:50:14-root-INFO: Loss too large (617.997->627.056)! Learning rate decreased to 0.02236.
2024-12-02-10:50:14-root-INFO: grad norm: 46.186 45.720 6.543
2024-12-02-10:50:15-root-INFO: Loss Change: 617.997 -> 605.048
2024-12-02-10:50:15-root-INFO: Regularization Change: 0.000 -> 0.487
2024-12-02-10:50:15-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-10:50:15-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-10:50:15-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-10:50:15-root-INFO: grad norm: 47.236 46.766 6.649
2024-12-02-10:50:15-root-INFO: Loss too large (606.791->615.244)! Learning rate decreased to 0.02312.
2024-12-02-10:50:15-root-INFO: grad norm: 45.258 44.765 6.659
2024-12-02-10:50:16-root-INFO: Loss Change: 606.791 -> 595.094
2024-12-02-10:50:16-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-02-10:50:16-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-10:50:16-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-10:50:16-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-10:50:16-root-INFO: grad norm: 50.220 49.657 7.499
2024-12-02-10:50:16-root-INFO: Loss too large (598.383->609.531)! Learning rate decreased to 0.02390.
2024-12-02-10:50:17-root-INFO: grad norm: 46.134 45.615 6.899
2024-12-02-10:50:17-root-INFO: Loss Change: 598.383 -> 584.565
2024-12-02-10:50:17-root-INFO: Regularization Change: 0.000 -> 0.519
2024-12-02-10:50:17-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-10:50:17-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-10:50:17-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-10:50:17-root-INFO: grad norm: 43.812 43.277 6.830
2024-12-02-10:50:17-root-INFO: Loss too large (586.197->592.373)! Learning rate decreased to 0.02471.
2024-12-02-10:50:18-root-INFO: grad norm: 42.603 42.071 6.710
2024-12-02-10:50:18-root-INFO: Loss Change: 586.197 -> 576.237
2024-12-02-10:50:18-root-INFO: Regularization Change: 0.000 -> 0.516
2024-12-02-10:50:18-root-INFO: Undo step: 130
2024-12-02-10:50:18-root-INFO: Undo step: 131
2024-12-02-10:50:18-root-INFO: Undo step: 132
2024-12-02-10:50:18-root-INFO: Undo step: 133
2024-12-02-10:50:18-root-INFO: Undo step: 134
2024-12-02-10:50:18-root-INFO: Undo step: 135
2024-12-02-10:50:18-root-INFO: Undo step: 136
2024-12-02-10:50:18-root-INFO: Undo step: 137
2024-12-02-10:50:18-root-INFO: Undo step: 138
2024-12-02-10:50:18-root-INFO: Undo step: 139
2024-12-02-10:50:18-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-10:50:18-root-INFO: grad norm: 428.860 423.667 66.540
2024-12-02-10:50:19-root-INFO: Loss too large (1604.602->2183.369)! Learning rate decreased to 0.01761.
2024-12-02-10:50:19-root-INFO: Loss too large (1604.602->1684.878)! Learning rate decreased to 0.01409.
2024-12-02-10:50:19-root-INFO: grad norm: 241.714 238.260 40.715
2024-12-02-10:50:20-root-INFO: Loss Change: 1604.602 -> 960.450
2024-12-02-10:50:20-root-INFO: Regularization Change: 0.000 -> 24.238
2024-12-02-10:50:20-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-10:50:20-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:50:20-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-10:50:20-root-INFO: grad norm: 123.240 119.477 30.223
2024-12-02-10:50:20-root-INFO: grad norm: 159.467 157.474 25.135
2024-12-02-10:50:21-root-INFO: Loss Change: 956.261 -> 858.231
2024-12-02-10:50:21-root-INFO: Regularization Change: 0.000 -> 14.015
2024-12-02-10:50:21-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-10:50:21-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:50:21-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-10:50:21-root-INFO: grad norm: 110.988 109.540 17.869
2024-12-02-10:50:21-root-INFO: grad norm: 100.060 99.148 13.474
2024-12-02-10:50:22-root-INFO: Loss Change: 852.719 -> 759.495
2024-12-02-10:50:22-root-INFO: Regularization Change: 0.000 -> 4.706
2024-12-02-10:50:22-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-10:50:22-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-10:50:22-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-10:50:22-root-INFO: grad norm: 92.014 90.691 15.545
2024-12-02-10:50:23-root-INFO: grad norm: 95.802 94.600 15.130
2024-12-02-10:50:23-root-INFO: Loss Change: 750.892 -> 715.398
2024-12-02-10:50:23-root-INFO: Regularization Change: 0.000 -> 2.628
2024-12-02-10:50:23-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-10:50:23-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-10:50:23-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-10:50:23-root-INFO: grad norm: 90.561 89.335 14.856
2024-12-02-10:50:24-root-INFO: grad norm: 79.713 78.481 13.957
2024-12-02-10:50:24-root-INFO: Loss Change: 713.152 -> 664.675
2024-12-02-10:50:24-root-INFO: Regularization Change: 0.000 -> 1.644
2024-12-02-10:50:24-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-10:50:24-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-10:50:24-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-10:50:24-root-INFO: grad norm: 68.296 67.380 11.149
2024-12-02-10:50:25-root-INFO: grad norm: 62.969 62.036 10.802
2024-12-02-10:50:25-root-INFO: Loss Change: 661.116 -> 632.593
2024-12-02-10:50:25-root-INFO: Regularization Change: 0.000 -> 1.160
2024-12-02-10:50:25-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-10:50:25-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-10:50:25-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-10:50:25-root-INFO: grad norm: 58.695 57.804 10.192
2024-12-02-10:50:26-root-INFO: grad norm: 58.867 57.969 10.240
2024-12-02-10:50:26-root-INFO: Loss Change: 629.167 -> 613.456
2024-12-02-10:50:26-root-INFO: Regularization Change: 0.000 -> 0.961
2024-12-02-10:50:26-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-10:50:26-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-10:50:26-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-10:50:26-root-INFO: grad norm: 57.357 56.547 9.606
2024-12-02-10:50:27-root-INFO: grad norm: 59.438 58.641 9.704
2024-12-02-10:50:27-root-INFO: Loss Change: 610.966 -> 600.296
2024-12-02-10:50:27-root-INFO: Regularization Change: 0.000 -> 0.880
2024-12-02-10:50:27-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-10:50:27-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-10:50:27-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-10:50:27-root-INFO: grad norm: 58.279 57.592 8.921
2024-12-02-10:50:28-root-INFO: grad norm: 58.396 57.699 8.993
2024-12-02-10:50:28-root-INFO: Loss Change: 597.726 -> 585.116
2024-12-02-10:50:28-root-INFO: Regularization Change: 0.000 -> 0.809
2024-12-02-10:50:28-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-10:50:28-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-10:50:28-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-10:50:29-root-INFO: grad norm: 55.628 54.970 8.531
2024-12-02-10:50:29-root-INFO: grad norm: 56.377 55.747 8.405
2024-12-02-10:50:29-root-INFO: Loss Change: 582.799 -> 572.091
2024-12-02-10:50:29-root-INFO: Regularization Change: 0.000 -> 0.798
2024-12-02-10:50:29-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-10:50:29-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-10:50:29-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-10:50:30-root-INFO: grad norm: 55.256 54.670 8.022
2024-12-02-10:50:30-root-INFO: grad norm: 56.129 55.565 7.936
2024-12-02-10:50:30-root-INFO: Loss Change: 569.939 -> 560.330
2024-12-02-10:50:30-root-INFO: Regularization Change: 0.000 -> 0.749
2024-12-02-10:50:30-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-10:50:30-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-10:50:31-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-10:50:31-root-INFO: grad norm: 53.268 52.685 7.858
2024-12-02-10:50:31-root-INFO: grad norm: 53.976 53.432 7.639
2024-12-02-10:50:32-root-INFO: Loss Change: 557.261 -> 547.889
2024-12-02-10:50:32-root-INFO: Regularization Change: 0.000 -> 0.755
2024-12-02-10:50:32-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-10:50:32-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-10:50:32-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-10:50:32-root-INFO: grad norm: 52.068 51.477 7.824
2024-12-02-10:50:32-root-INFO: grad norm: 53.435 52.931 7.323
2024-12-02-10:50:33-root-INFO: Loss too large (536.382->537.085)! Learning rate decreased to 0.02639.
2024-12-02-10:50:33-root-INFO: Loss Change: 544.912 -> 522.913
2024-12-02-10:50:33-root-INFO: Regularization Change: 0.000 -> 0.687
2024-12-02-10:50:33-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-10:50:33-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-10:50:33-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-10:50:33-root-INFO: grad norm: 36.165 35.717 5.678
2024-12-02-10:50:34-root-INFO: grad norm: 43.871 43.489 5.779
2024-12-02-10:50:34-root-INFO: Loss too large (517.362->522.065)! Learning rate decreased to 0.02726.
2024-12-02-10:50:34-root-INFO: Loss Change: 520.778 -> 511.353
2024-12-02-10:50:34-root-INFO: Regularization Change: 0.000 -> 0.617
2024-12-02-10:50:34-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-10:50:34-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-10:50:34-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-10:50:34-root-INFO: grad norm: 35.452 35.005 5.610
2024-12-02-10:50:35-root-INFO: grad norm: 44.512 44.161 5.584
2024-12-02-10:50:35-root-INFO: Loss too large (507.850->514.252)! Learning rate decreased to 0.02816.
2024-12-02-10:50:35-root-INFO: Loss Change: 510.387 -> 502.791
2024-12-02-10:50:35-root-INFO: Regularization Change: 0.000 -> 0.602
2024-12-02-10:50:35-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-10:50:35-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-10:50:36-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-10:50:36-root-INFO: grad norm: 35.271 34.810 5.683
2024-12-02-10:50:36-root-INFO: grad norm: 43.499 43.165 5.380
2024-12-02-10:50:36-root-INFO: Loss too large (498.305->504.402)! Learning rate decreased to 0.02909.
2024-12-02-10:50:37-root-INFO: Loss Change: 501.217 -> 493.014
2024-12-02-10:50:37-root-INFO: Regularization Change: 0.000 -> 0.613
2024-12-02-10:50:37-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-10:50:37-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-10:50:37-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-10:50:37-root-INFO: grad norm: 34.472 33.993 5.726
2024-12-02-10:50:37-root-INFO: grad norm: 42.800 42.465 5.343
2024-12-02-10:50:37-root-INFO: Loss too large (488.531->494.480)! Learning rate decreased to 0.03019.
2024-12-02-10:50:38-root-INFO: Loss Change: 490.984 -> 483.114
2024-12-02-10:50:38-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-10:50:38-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-10:50:38-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-10:50:38-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-10:50:38-root-INFO: grad norm: 34.389 33.970 5.350
2024-12-02-10:50:39-root-INFO: grad norm: 42.397 42.082 5.156
2024-12-02-10:50:39-root-INFO: Loss too large (480.322->486.005)! Learning rate decreased to 0.03117.
2024-12-02-10:50:39-root-INFO: Loss Change: 482.366 -> 474.349
2024-12-02-10:50:39-root-INFO: Regularization Change: 0.000 -> 0.628
2024-12-02-10:50:39-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-10:50:39-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-10:50:39-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-10:50:39-root-INFO: grad norm: 33.845 33.447 5.175
2024-12-02-10:50:40-root-INFO: grad norm: 41.677 41.376 5.002
2024-12-02-10:50:40-root-INFO: Loss too large (471.827->476.901)! Learning rate decreased to 0.03218.
2024-12-02-10:50:40-root-INFO: Loss Change: 473.200 -> 465.244
2024-12-02-10:50:40-root-INFO: Regularization Change: 0.000 -> 0.633
2024-12-02-10:50:40-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-10:50:40-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-10:50:41-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-10:50:41-root-INFO: grad norm: 33.068 32.678 5.063
2024-12-02-10:50:41-root-INFO: grad norm: 40.747 40.452 4.901
2024-12-02-10:50:41-root-INFO: Loss too large (462.990->467.810)! Learning rate decreased to 0.03321.
2024-12-02-10:50:42-root-INFO: Loss Change: 464.005 -> 456.218
2024-12-02-10:50:42-root-INFO: Regularization Change: 0.000 -> 0.645
2024-12-02-10:50:42-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-10:50:42-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-10:50:42-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-10:50:42-root-INFO: grad norm: 32.618 32.232 5.001
2024-12-02-10:50:42-root-INFO: grad norm: 39.972 39.682 4.807
2024-12-02-10:50:43-root-INFO: Loss too large (454.646->459.125)! Learning rate decreased to 0.03427.
2024-12-02-10:50:43-root-INFO: Loss Change: 455.474 -> 447.530
2024-12-02-10:50:43-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-10:50:43-root-INFO: Undo step: 120
2024-12-02-10:50:43-root-INFO: Undo step: 121
2024-12-02-10:50:43-root-INFO: Undo step: 122
2024-12-02-10:50:43-root-INFO: Undo step: 123
2024-12-02-10:50:43-root-INFO: Undo step: 124
2024-12-02-10:50:43-root-INFO: Undo step: 125
2024-12-02-10:50:43-root-INFO: Undo step: 126
2024-12-02-10:50:43-root-INFO: Undo step: 127
2024-12-02-10:50:43-root-INFO: Undo step: 128
2024-12-02-10:50:43-root-INFO: Undo step: 129
2024-12-02-10:50:43-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-10:50:43-root-INFO: grad norm: 211.605 207.788 40.011
2024-12-02-10:50:44-root-INFO: grad norm: 97.470 95.534 19.326
2024-12-02-10:50:44-root-INFO: Loss Change: 1108.510 -> 689.996
2024-12-02-10:50:44-root-INFO: Regularization Change: 0.000 -> 44.063
2024-12-02-10:50:44-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-10:50:44-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-10:50:44-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-10:50:44-root-INFO: grad norm: 67.164 65.217 16.058
2024-12-02-10:50:45-root-INFO: grad norm: 55.456 54.392 10.810
2024-12-02-10:50:45-root-INFO: Loss Change: 685.354 -> 575.990
2024-12-02-10:50:45-root-INFO: Regularization Change: 0.000 -> 7.664
2024-12-02-10:50:45-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-10:50:45-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-10:50:45-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-10:50:46-root-INFO: grad norm: 52.931 52.164 8.983
2024-12-02-10:50:46-root-INFO: grad norm: 53.157 52.396 8.967
2024-12-02-10:50:46-root-INFO: Loss Change: 571.077 -> 539.768
2024-12-02-10:50:46-root-INFO: Regularization Change: 0.000 -> 2.852
2024-12-02-10:50:46-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-10:50:46-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-10:50:47-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-10:50:47-root-INFO: grad norm: 58.993 58.431 8.129
2024-12-02-10:50:47-root-INFO: Loss too large (538.221->545.698)! Learning rate decreased to 0.02726.
2024-12-02-10:50:47-root-INFO: grad norm: 51.035 50.291 8.679
2024-12-02-10:50:48-root-INFO: Loss Change: 538.221 -> 510.423
2024-12-02-10:50:48-root-INFO: Regularization Change: 0.000 -> 1.211
2024-12-02-10:50:48-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-10:50:48-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-10:50:48-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-10:50:48-root-INFO: grad norm: 45.541 45.066 6.558
2024-12-02-10:50:48-root-INFO: Loss too large (508.241->515.167)! Learning rate decreased to 0.02816.
2024-12-02-10:50:49-root-INFO: grad norm: 40.114 39.492 7.034
2024-12-02-10:50:49-root-INFO: Loss Change: 508.241 -> 488.374
2024-12-02-10:50:49-root-INFO: Regularization Change: 0.000 -> 0.930
2024-12-02-10:50:49-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-10:50:49-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-10:50:49-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-10:50:49-root-INFO: grad norm: 30.541 30.035 5.536
2024-12-02-10:50:50-root-INFO: grad norm: 40.947 40.353 6.948
2024-12-02-10:50:50-root-INFO: Loss too large (485.825->492.796)! Learning rate decreased to 0.02909.
2024-12-02-10:50:50-root-INFO: Loss Change: 486.433 -> 479.171
2024-12-02-10:50:50-root-INFO: Regularization Change: 0.000 -> 1.040
2024-12-02-10:50:50-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-10:50:50-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-10:50:50-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-10:50:51-root-INFO: grad norm: 44.066 43.610 6.326
2024-12-02-10:50:51-root-INFO: Loss too large (477.775->487.024)! Learning rate decreased to 0.03019.
2024-12-02-10:50:51-root-INFO: grad norm: 38.218 37.686 6.351
2024-12-02-10:50:52-root-INFO: Loss Change: 477.775 -> 461.834
2024-12-02-10:50:52-root-INFO: Regularization Change: 0.000 -> 0.832
2024-12-02-10:50:52-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-10:50:52-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-10:50:52-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-10:50:52-root-INFO: grad norm: 30.992 30.581 5.033
2024-12-02-10:50:52-root-INFO: Loss too large (460.884->464.386)! Learning rate decreased to 0.03117.
2024-12-02-10:50:52-root-INFO: grad norm: 33.857 33.457 5.190
2024-12-02-10:50:53-root-INFO: Loss Change: 460.884 -> 455.440
2024-12-02-10:50:53-root-INFO: Regularization Change: 0.000 -> 0.713
2024-12-02-10:50:53-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-10:50:53-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-10:50:53-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-10:50:53-root-INFO: grad norm: 39.632 39.294 5.164
2024-12-02-10:50:53-root-INFO: Loss too large (454.782->467.025)! Learning rate decreased to 0.03218.
2024-12-02-10:50:53-root-INFO: Loss too large (454.782->455.691)! Learning rate decreased to 0.02574.
2024-12-02-10:50:54-root-INFO: grad norm: 31.351 30.982 4.796
2024-12-02-10:50:54-root-INFO: Loss Change: 454.782 -> 440.879
2024-12-02-10:50:54-root-INFO: Regularization Change: 0.000 -> 0.448
2024-12-02-10:50:54-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-10:50:54-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-10:50:54-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-10:50:55-root-INFO: grad norm: 24.033 23.642 4.316
2024-12-02-10:50:55-root-INFO: Loss too large (440.270->442.083)! Learning rate decreased to 0.03321.
2024-12-02-10:50:55-root-INFO: grad norm: 27.557 27.181 4.538
2024-12-02-10:50:56-root-INFO: Loss Change: 440.270 -> 435.505
2024-12-02-10:50:56-root-INFO: Regularization Change: 0.000 -> 0.623
2024-12-02-10:50:56-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-10:50:56-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-10:50:56-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-10:50:56-root-INFO: grad norm: 31.871 31.529 4.654
2024-12-02-10:50:56-root-INFO: Loss too large (435.120->441.502)! Learning rate decreased to 0.03427.
2024-12-02-10:50:56-root-INFO: grad norm: 31.396 31.018 4.858
2024-12-02-10:50:57-root-INFO: Loss Change: 435.120 -> 427.133
2024-12-02-10:50:57-root-INFO: Regularization Change: 0.000 -> 0.616
2024-12-02-10:50:57-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-10:50:57-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-10:50:57-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-10:50:57-root-INFO: grad norm: 30.886 30.523 4.720
2024-12-02-10:50:57-root-INFO: Loss too large (426.656->433.559)! Learning rate decreased to 0.03536.
2024-12-02-10:50:58-root-INFO: grad norm: 32.706 32.347 4.829
2024-12-02-10:50:58-root-INFO: Loss Change: 426.656 -> 421.582
2024-12-02-10:50:58-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-10:50:58-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-10:50:58-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-10:50:58-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-10:50:58-root-INFO: grad norm: 35.391 35.106 4.483
2024-12-02-10:50:59-root-INFO: Loss too large (422.207->434.526)! Learning rate decreased to 0.03648.
2024-12-02-10:50:59-root-INFO: Loss too large (422.207->422.835)! Learning rate decreased to 0.02919.
2024-12-02-10:50:59-root-INFO: grad norm: 27.905 27.607 4.066
2024-12-02-10:50:59-root-INFO: Loss Change: 422.207 -> 410.224
2024-12-02-10:50:59-root-INFO: Regularization Change: 0.000 -> 0.419
2024-12-02-10:50:59-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-10:50:59-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-10:51:00-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-10:51:00-root-INFO: grad norm: 21.620 21.281 3.813
2024-12-02-10:51:00-root-INFO: Loss too large (409.694->411.036)! Learning rate decreased to 0.03763.
2024-12-02-10:51:00-root-INFO: grad norm: 24.731 24.433 3.827
2024-12-02-10:51:01-root-INFO: Loss Change: 409.694 -> 405.967
2024-12-02-10:51:01-root-INFO: Regularization Change: 0.000 -> 0.618
2024-12-02-10:51:01-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-10:51:01-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-10:51:01-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-10:51:01-root-INFO: grad norm: 29.043 28.767 3.990
2024-12-02-10:51:01-root-INFO: Loss too large (405.646->413.056)! Learning rate decreased to 0.03881.
2024-12-02-10:51:02-root-INFO: grad norm: 31.984 31.699 4.263
2024-12-02-10:51:02-root-INFO: Loss Change: 405.646 -> 402.962
2024-12-02-10:51:02-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-10:51:02-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-10:51:02-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-10:51:02-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-10:51:02-root-INFO: grad norm: 35.109 34.874 4.048
2024-12-02-10:51:02-root-INFO: Loss too large (402.591->416.394)! Learning rate decreased to 0.04002.
2024-12-02-10:51:03-root-INFO: Loss too large (402.591->403.029)! Learning rate decreased to 0.03202.
2024-12-02-10:51:03-root-INFO: grad norm: 27.097 26.853 3.633
2024-12-02-10:51:03-root-INFO: Loss Change: 402.591 -> 390.028
2024-12-02-10:51:03-root-INFO: Regularization Change: 0.000 -> 0.452
2024-12-02-10:51:03-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-10:51:03-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-10:51:03-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-10:51:04-root-INFO: grad norm: 20.803 20.562 3.155
2024-12-02-10:51:04-root-INFO: Loss too large (389.852->392.061)! Learning rate decreased to 0.04126.
2024-12-02-10:51:04-root-INFO: grad norm: 24.028 23.791 3.368
2024-12-02-10:51:05-root-INFO: Loss Change: 389.852 -> 386.863
2024-12-02-10:51:05-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-10:51:05-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-10:51:05-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-10:51:05-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-10:51:05-root-INFO: grad norm: 28.117 27.899 3.492
2024-12-02-10:51:05-root-INFO: Loss too large (386.657->395.362)! Learning rate decreased to 0.04253.
2024-12-02-10:51:05-root-INFO: grad norm: 31.887 31.661 3.790
2024-12-02-10:51:06-root-INFO: Loss Change: 386.657 -> 385.701
2024-12-02-10:51:06-root-INFO: Regularization Change: 0.000 -> 0.615
2024-12-02-10:51:06-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-10:51:06-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-10:51:06-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-10:51:06-root-INFO: grad norm: 35.330 35.131 3.747
2024-12-02-10:51:06-root-INFO: Loss too large (385.379->399.735)! Learning rate decreased to 0.04384.
2024-12-02-10:51:07-root-INFO: grad norm: 37.456 37.231 4.103
2024-12-02-10:51:07-root-INFO: Loss Change: 385.379 -> 382.653
2024-12-02-10:51:07-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-02-10:51:07-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-10:51:07-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-10:51:07-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-10:51:07-root-INFO: grad norm: 38.012 37.836 3.654
2024-12-02-10:51:07-root-INFO: Loss too large (382.050->397.351)! Learning rate decreased to 0.04517.
2024-12-02-10:51:08-root-INFO: grad norm: 37.925 37.713 4.005
2024-12-02-10:51:08-root-INFO: Loss Change: 382.050 -> 375.771
2024-12-02-10:51:08-root-INFO: Regularization Change: 0.000 -> 0.599
2024-12-02-10:51:08-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-10:51:08-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-10:51:08-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-10:51:09-root-INFO: grad norm: 36.129 35.950 3.594
2024-12-02-10:51:09-root-INFO: Loss too large (375.402->387.575)! Learning rate decreased to 0.04654.
2024-12-02-10:51:09-root-INFO: grad norm: 34.982 34.778 3.770
2024-12-02-10:51:09-root-INFO: Loss Change: 375.402 -> 367.649
2024-12-02-10:51:09-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-02-10:51:09-root-INFO: Undo step: 110
2024-12-02-10:51:09-root-INFO: Undo step: 111
2024-12-02-10:51:09-root-INFO: Undo step: 112
2024-12-02-10:51:09-root-INFO: Undo step: 113
2024-12-02-10:51:09-root-INFO: Undo step: 114
2024-12-02-10:51:09-root-INFO: Undo step: 115
2024-12-02-10:51:09-root-INFO: Undo step: 116
2024-12-02-10:51:09-root-INFO: Undo step: 117
2024-12-02-10:51:09-root-INFO: Undo step: 118
2024-12-02-10:51:09-root-INFO: Undo step: 119
2024-12-02-10:51:10-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-10:51:10-root-INFO: grad norm: 126.876 124.315 25.365
2024-12-02-10:51:10-root-INFO: grad norm: 88.022 86.556 15.995
2024-12-02-10:51:11-root-INFO: Loss Change: 943.414 -> 555.543
2024-12-02-10:51:11-root-INFO: Regularization Change: 0.000 -> 34.877
2024-12-02-10:51:11-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-10:51:11-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-10:51:11-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-10:51:11-root-INFO: grad norm: 58.707 57.210 13.174
2024-12-02-10:51:11-root-INFO: grad norm: 48.832 47.962 9.173
2024-12-02-10:51:12-root-INFO: Loss Change: 550.009 -> 473.312
2024-12-02-10:51:12-root-INFO: Regularization Change: 0.000 -> 6.953
2024-12-02-10:51:12-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-10:51:12-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-10:51:12-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-10:51:12-root-INFO: grad norm: 49.909 49.227 8.219
2024-12-02-10:51:12-root-INFO: grad norm: 53.821 53.328 7.271
2024-12-02-10:51:13-root-INFO: Loss Change: 472.758 -> 456.846
2024-12-02-10:51:13-root-INFO: Regularization Change: 0.000 -> 3.156
2024-12-02-10:51:13-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-10:51:13-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-10:51:13-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-10:51:13-root-INFO: grad norm: 61.831 61.424 7.077
2024-12-02-10:51:13-root-INFO: Loss too large (456.641->459.803)! Learning rate decreased to 0.03763.
2024-12-02-10:51:14-root-INFO: grad norm: 46.530 46.225 5.322
2024-12-02-10:51:14-root-INFO: Loss Change: 456.641 -> 412.475
2024-12-02-10:51:14-root-INFO: Regularization Change: 0.000 -> 1.766
2024-12-02-10:51:14-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-10:51:14-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-10:51:14-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-10:51:14-root-INFO: grad norm: 36.398 36.087 4.747
2024-12-02-10:51:15-root-INFO: Loss too large (411.648->414.802)! Learning rate decreased to 0.03881.
2024-12-02-10:51:15-root-INFO: grad norm: 33.520 33.260 4.169
2024-12-02-10:51:15-root-INFO: Loss Change: 411.648 -> 396.783
2024-12-02-10:51:15-root-INFO: Regularization Change: 0.000 -> 1.042
2024-12-02-10:51:15-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-10:51:15-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-10:51:16-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-10:51:16-root-INFO: grad norm: 31.978 31.664 4.473
2024-12-02-10:51:16-root-INFO: Loss too large (395.821->400.994)! Learning rate decreased to 0.04002.
2024-12-02-10:51:16-root-INFO: grad norm: 31.988 31.746 3.926
2024-12-02-10:51:17-root-INFO: Loss Change: 395.821 -> 386.569
2024-12-02-10:51:17-root-INFO: Regularization Change: 0.000 -> 0.841
2024-12-02-10:51:17-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-10:51:17-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-10:51:17-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-10:51:17-root-INFO: grad norm: 30.752 30.464 4.196
2024-12-02-10:51:17-root-INFO: Loss too large (385.698->390.859)! Learning rate decreased to 0.04126.
2024-12-02-10:51:18-root-INFO: grad norm: 30.386 30.159 3.709
2024-12-02-10:51:18-root-INFO: Loss Change: 385.698 -> 377.226
2024-12-02-10:51:18-root-INFO: Regularization Change: 0.000 -> 0.729
2024-12-02-10:51:18-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-10:51:18-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-10:51:18-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-10:51:18-root-INFO: grad norm: 29.386 29.110 4.017
2024-12-02-10:51:18-root-INFO: Loss too large (376.155->381.777)! Learning rate decreased to 0.04253.
2024-12-02-10:51:19-root-INFO: grad norm: 29.348 29.133 3.542
2024-12-02-10:51:19-root-INFO: Loss Change: 376.155 -> 368.954
2024-12-02-10:51:19-root-INFO: Regularization Change: 0.000 -> 0.670
2024-12-02-10:51:19-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-10:51:19-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-10:51:19-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-10:51:19-root-INFO: grad norm: 28.227 27.962 3.857
2024-12-02-10:51:20-root-INFO: Loss too large (367.616->373.305)! Learning rate decreased to 0.04384.
2024-12-02-10:51:20-root-INFO: grad norm: 28.319 28.115 3.387
2024-12-02-10:51:20-root-INFO: Loss Change: 367.616 -> 361.175
2024-12-02-10:51:20-root-INFO: Regularization Change: 0.000 -> 0.634
2024-12-02-10:51:20-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-10:51:20-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-10:51:20-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-10:51:21-root-INFO: grad norm: 27.335 27.084 3.695
2024-12-02-10:51:21-root-INFO: Loss too large (360.335->365.868)! Learning rate decreased to 0.04517.
2024-12-02-10:51:21-root-INFO: grad norm: 27.250 27.053 3.271
2024-12-02-10:51:22-root-INFO: Loss Change: 360.335 -> 354.136
2024-12-02-10:51:22-root-INFO: Regularization Change: 0.000 -> 0.591
2024-12-02-10:51:22-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-10:51:22-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-10:51:22-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-10:51:22-root-INFO: grad norm: 26.038 25.804 3.488
2024-12-02-10:51:22-root-INFO: Loss too large (353.264->358.374)! Learning rate decreased to 0.04654.
2024-12-02-10:51:22-root-INFO: grad norm: 25.936 25.746 3.133
2024-12-02-10:51:23-root-INFO: Loss Change: 353.264 -> 347.400
2024-12-02-10:51:23-root-INFO: Regularization Change: 0.000 -> 0.575
2024-12-02-10:51:23-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-10:51:23-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-10:51:23-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-10:51:23-root-INFO: grad norm: 24.835 24.607 3.353
2024-12-02-10:51:23-root-INFO: Loss too large (346.736->351.286)! Learning rate decreased to 0.04795.
2024-12-02-10:51:24-root-INFO: grad norm: 24.616 24.430 3.020
2024-12-02-10:51:24-root-INFO: Loss Change: 346.736 -> 340.988
2024-12-02-10:51:24-root-INFO: Regularization Change: 0.000 -> 0.571
2024-12-02-10:51:24-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-10:51:24-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-10:51:24-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-10:51:24-root-INFO: grad norm: 23.483 23.265 3.194
2024-12-02-10:51:25-root-INFO: Loss too large (340.145->344.007)! Learning rate decreased to 0.04939.
2024-12-02-10:51:25-root-INFO: grad norm: 23.037 22.857 2.872
2024-12-02-10:51:25-root-INFO: Loss Change: 340.145 -> 334.369
2024-12-02-10:51:25-root-INFO: Regularization Change: 0.000 -> 0.556
2024-12-02-10:51:25-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-10:51:25-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-10:51:26-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-10:51:26-root-INFO: grad norm: 22.145 21.927 3.098
2024-12-02-10:51:26-root-INFO: Loss too large (333.893->337.266)! Learning rate decreased to 0.05086.
2024-12-02-10:51:26-root-INFO: grad norm: 21.819 21.642 2.774
2024-12-02-10:51:27-root-INFO: Loss Change: 333.893 -> 328.423
2024-12-02-10:51:27-root-INFO: Regularization Change: 0.000 -> 0.566
2024-12-02-10:51:27-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-10:51:27-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-10:51:27-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-10:51:27-root-INFO: grad norm: 20.528 20.321 2.909
2024-12-02-10:51:27-root-INFO: Loss too large (327.355->329.972)! Learning rate decreased to 0.05237.
2024-12-02-10:51:28-root-INFO: grad norm: 20.316 20.140 2.668
2024-12-02-10:51:28-root-INFO: Loss Change: 327.355 -> 322.150
2024-12-02-10:51:28-root-INFO: Regularization Change: 0.000 -> 0.567
2024-12-02-10:51:28-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-10:51:28-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-10:51:28-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-10:51:28-root-INFO: grad norm: 20.060 19.859 2.832
2024-12-02-10:51:28-root-INFO: Loss too large (322.077->324.840)! Learning rate decreased to 0.05391.
2024-12-02-10:51:29-root-INFO: grad norm: 20.022 19.850 2.619
2024-12-02-10:51:29-root-INFO: Loss Change: 322.077 -> 317.183
2024-12-02-10:51:29-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-02-10:51:29-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-10:51:29-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-10:51:29-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-10:51:30-root-INFO: grad norm: 19.666 19.467 2.787
2024-12-02-10:51:30-root-INFO: Loss too large (316.979->319.292)! Learning rate decreased to 0.05550.
2024-12-02-10:51:30-root-INFO: grad norm: 19.200 19.033 2.522
2024-12-02-10:51:30-root-INFO: Loss Change: 316.979 -> 311.715
2024-12-02-10:51:30-root-INFO: Regularization Change: 0.000 -> 0.572
2024-12-02-10:51:30-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-10:51:30-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-10:51:31-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-10:51:31-root-INFO: grad norm: 18.929 18.743 2.647
2024-12-02-10:51:31-root-INFO: Loss too large (311.566->313.782)! Learning rate decreased to 0.05711.
2024-12-02-10:51:31-root-INFO: grad norm: 18.588 18.425 2.463
2024-12-02-10:51:32-root-INFO: Loss Change: 311.566 -> 306.541
2024-12-02-10:51:32-root-INFO: Regularization Change: 0.000 -> 0.564
2024-12-02-10:51:32-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-10:51:32-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-10:51:32-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-10:51:32-root-INFO: grad norm: 18.609 18.425 2.609
2024-12-02-10:51:32-root-INFO: Loss too large (306.538->308.676)! Learning rate decreased to 0.05877.
2024-12-02-10:51:33-root-INFO: grad norm: 18.305 18.141 2.441
2024-12-02-10:51:33-root-INFO: Loss Change: 306.538 -> 301.597
2024-12-02-10:51:33-root-INFO: Regularization Change: 0.000 -> 0.580
2024-12-02-10:51:33-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-10:51:33-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-10:51:33-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-10:51:33-root-INFO: grad norm: 18.391 18.206 2.602
2024-12-02-10:51:33-root-INFO: Loss too large (301.751->303.894)! Learning rate decreased to 0.06047.
2024-12-02-10:51:34-root-INFO: grad norm: 17.954 17.795 2.389
2024-12-02-10:51:34-root-INFO: Loss Change: 301.751 -> 296.750
2024-12-02-10:51:34-root-INFO: Regularization Change: 0.000 -> 0.583
2024-12-02-10:51:34-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-10:51:34-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-10:51:34-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-10:51:34-root-INFO: grad norm: 18.190 18.009 2.562
2024-12-02-10:51:35-root-INFO: Loss too large (297.092->299.193)! Learning rate decreased to 0.06220.
2024-12-02-10:51:35-root-INFO: grad norm: 17.612 17.455 2.343
2024-12-02-10:51:35-root-INFO: Loss Change: 297.092 -> 292.009
2024-12-02-10:51:35-root-INFO: Regularization Change: 0.000 -> 0.588
2024-12-02-10:51:36-root-INFO: Undo step: 100
2024-12-02-10:51:36-root-INFO: Undo step: 101
2024-12-02-10:51:36-root-INFO: Undo step: 102
2024-12-02-10:51:36-root-INFO: Undo step: 103
2024-12-02-10:51:36-root-INFO: Undo step: 104
2024-12-02-10:51:36-root-INFO: Undo step: 105
2024-12-02-10:51:36-root-INFO: Undo step: 106
2024-12-02-10:51:36-root-INFO: Undo step: 107
2024-12-02-10:51:36-root-INFO: Undo step: 108
2024-12-02-10:51:36-root-INFO: Undo step: 109
2024-12-02-10:51:36-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-10:51:36-root-INFO: grad norm: 118.092 116.509 19.271
2024-12-02-10:51:36-root-INFO: grad norm: 80.475 79.744 10.817
2024-12-02-10:51:37-root-INFO: Loss Change: 755.706 -> 471.922
2024-12-02-10:51:37-root-INFO: Regularization Change: 0.000 -> 41.534
2024-12-02-10:51:37-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-10:51:37-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-10:51:37-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-10:51:37-root-INFO: grad norm: 62.955 62.232 9.512
2024-12-02-10:51:38-root-INFO: grad norm: 50.185 49.838 5.891
2024-12-02-10:51:38-root-INFO: Loss Change: 468.272 -> 385.391
2024-12-02-10:51:38-root-INFO: Regularization Change: 0.000 -> 8.424
2024-12-02-10:51:38-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-10:51:38-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-10:51:38-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-10:51:38-root-INFO: grad norm: 44.566 44.258 5.234
2024-12-02-10:51:39-root-INFO: grad norm: 45.894 45.638 4.837
2024-12-02-10:51:39-root-INFO: Loss Change: 382.211 -> 368.816
2024-12-02-10:51:39-root-INFO: Regularization Change: 0.000 -> 3.618
2024-12-02-10:51:39-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-10:51:39-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-10:51:39-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-10:51:39-root-INFO: grad norm: 42.989 42.728 4.725
2024-12-02-10:51:40-root-INFO: grad norm: 43.826 43.592 4.518
2024-12-02-10:51:40-root-INFO: Loss Change: 365.424 -> 355.038
2024-12-02-10:51:40-root-INFO: Regularization Change: 0.000 -> 1.953
2024-12-02-10:51:40-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-10:51:40-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-10:51:40-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-10:51:41-root-INFO: grad norm: 42.259 42.021 4.481
2024-12-02-10:51:41-root-INFO: grad norm: 42.277 42.015 4.701
2024-12-02-10:51:41-root-INFO: Loss Change: 351.890 -> 342.547
2024-12-02-10:51:41-root-INFO: Regularization Change: 0.000 -> 1.693
2024-12-02-10:51:41-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-10:51:41-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-10:51:41-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-10:51:42-root-INFO: grad norm: 41.219 40.980 4.438
2024-12-02-10:51:42-root-INFO: grad norm: 44.571 44.268 5.187
2024-12-02-10:51:42-root-INFO: Loss too large (337.129->342.309)! Learning rate decreased to 0.05391.
2024-12-02-10:51:43-root-INFO: Loss Change: 340.082 -> 321.219
2024-12-02-10:51:43-root-INFO: Regularization Change: 0.000 -> 1.268
2024-12-02-10:51:43-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-10:51:43-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-10:51:43-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-10:51:43-root-INFO: grad norm: 29.863 29.638 3.655
2024-12-02-10:51:43-root-INFO: grad norm: 37.518 37.246 4.512
2024-12-02-10:51:44-root-INFO: Loss too large (317.126->330.458)! Learning rate decreased to 0.05550.
2024-12-02-10:51:44-root-INFO: Loss Change: 318.465 -> 313.205
2024-12-02-10:51:44-root-INFO: Regularization Change: 0.000 -> 1.099
2024-12-02-10:51:44-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-10:51:44-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-10:51:44-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-10:51:44-root-INFO: grad norm: 30.048 29.837 3.549
2024-12-02-10:51:44-root-INFO: Loss too large (311.095->312.349)! Learning rate decreased to 0.05711.
2024-12-02-10:51:45-root-INFO: grad norm: 24.484 24.278 3.174
2024-12-02-10:51:45-root-INFO: Loss Change: 311.095 -> 296.991
2024-12-02-10:51:45-root-INFO: Regularization Change: 0.000 -> 0.841
2024-12-02-10:51:45-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-10:51:45-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-10:51:45-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-10:51:46-root-INFO: grad norm: 20.798 20.651 2.467
2024-12-02-10:51:46-root-INFO: Loss too large (295.570->296.841)! Learning rate decreased to 0.05877.
2024-12-02-10:51:46-root-INFO: grad norm: 19.089 18.916 2.562
2024-12-02-10:51:47-root-INFO: Loss Change: 295.570 -> 288.370
2024-12-02-10:51:47-root-INFO: Regularization Change: 0.000 -> 0.669
2024-12-02-10:51:47-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-10:51:47-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-10:51:47-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-10:51:47-root-INFO: grad norm: 16.806 16.680 2.054
2024-12-02-10:51:47-root-INFO: Loss too large (287.288->287.875)! Learning rate decreased to 0.06047.
2024-12-02-10:51:47-root-INFO: grad norm: 16.207 16.053 2.229
2024-12-02-10:51:48-root-INFO: Loss Change: 287.288 -> 281.871
2024-12-02-10:51:48-root-INFO: Regularization Change: 0.000 -> 0.622
2024-12-02-10:51:48-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-10:51:48-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-10:51:48-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-10:51:48-root-INFO: grad norm: 14.832 14.716 1.853
2024-12-02-10:51:48-root-INFO: Loss too large (281.107->281.327)! Learning rate decreased to 0.06220.
2024-12-02-10:51:49-root-INFO: grad norm: 14.605 14.464 2.029
2024-12-02-10:51:49-root-INFO: Loss Change: 281.107 -> 276.404
2024-12-02-10:51:49-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-10:51:49-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-10:51:49-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-10:51:49-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-10:51:49-root-INFO: grad norm: 13.635 13.522 1.757
2024-12-02-10:51:49-root-INFO: Loss too large (275.427->275.513)! Learning rate decreased to 0.06397.
2024-12-02-10:51:50-root-INFO: grad norm: 13.701 13.570 1.893
2024-12-02-10:51:50-root-INFO: Loss Change: 275.427 -> 271.244
2024-12-02-10:51:50-root-INFO: Regularization Change: 0.000 -> 0.589
2024-12-02-10:51:50-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-10:51:50-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-10:51:50-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-10:51:51-root-INFO: grad norm: 13.052 12.944 1.680
2024-12-02-10:51:51-root-INFO: Loss too large (270.492->270.671)! Learning rate decreased to 0.06579.
2024-12-02-10:51:51-root-INFO: grad norm: 13.350 13.226 1.815
2024-12-02-10:51:51-root-INFO: Loss Change: 270.492 -> 266.714
2024-12-02-10:51:51-root-INFO: Regularization Change: 0.000 -> 0.587
2024-12-02-10:51:51-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-10:51:51-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-10:51:52-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-10:51:52-root-INFO: grad norm: 13.134 13.031 1.640
2024-12-02-10:51:52-root-INFO: Loss too large (265.885->266.728)! Learning rate decreased to 0.06764.
2024-12-02-10:51:52-root-INFO: grad norm: 13.929 13.816 1.772
2024-12-02-10:51:53-root-INFO: Loss Change: 265.885 -> 262.793
2024-12-02-10:51:53-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-02-10:51:53-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-10:51:53-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-10:51:53-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-10:51:53-root-INFO: grad norm: 14.613 14.519 1.660
2024-12-02-10:51:53-root-INFO: Loss too large (262.177->264.619)! Learning rate decreased to 0.06953.
2024-12-02-10:51:54-root-INFO: grad norm: 15.786 15.677 1.852
2024-12-02-10:51:54-root-INFO: Loss Change: 262.177 -> 259.778
2024-12-02-10:51:54-root-INFO: Regularization Change: 0.000 -> 0.587
2024-12-02-10:51:54-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-10:51:54-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-10:51:54-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-10:51:54-root-INFO: grad norm: 16.452 16.357 1.771
2024-12-02-10:51:54-root-INFO: Loss too large (259.289->263.687)! Learning rate decreased to 0.07147.
2024-12-02-10:51:55-root-INFO: grad norm: 17.845 17.734 1.988
2024-12-02-10:51:55-root-INFO: Loss Change: 259.289 -> 257.431
2024-12-02-10:51:55-root-INFO: Regularization Change: 0.000 -> 0.595
2024-12-02-10:51:55-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-10:51:55-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-10:51:55-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-10:51:55-root-INFO: grad norm: 18.695 18.598 1.904
2024-12-02-10:51:56-root-INFO: Loss too large (256.644->263.234)! Learning rate decreased to 0.07345.
2024-12-02-10:51:56-root-INFO: grad norm: 19.751 19.635 2.135
2024-12-02-10:51:57-root-INFO: Loss Change: 256.644 -> 254.556
2024-12-02-10:51:57-root-INFO: Regularization Change: 0.000 -> 0.592
2024-12-02-10:51:57-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-10:51:57-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-10:51:57-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-10:51:57-root-INFO: grad norm: 19.432 19.330 1.991
2024-12-02-10:51:57-root-INFO: Loss too large (253.442->260.325)! Learning rate decreased to 0.07547.
2024-12-02-10:51:58-root-INFO: grad norm: 19.749 19.628 2.181
2024-12-02-10:51:58-root-INFO: Loss Change: 253.442 -> 250.377
2024-12-02-10:51:58-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-02-10:51:58-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-10:51:58-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-10:51:58-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-10:51:58-root-INFO: grad norm: 19.047 18.945 1.968
2024-12-02-10:51:58-root-INFO: Loss too large (249.641->255.982)! Learning rate decreased to 0.07753.
2024-12-02-10:51:59-root-INFO: grad norm: 19.048 18.925 2.158
2024-12-02-10:51:59-root-INFO: Loss Change: 249.641 -> 246.197
2024-12-02-10:51:59-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-10:51:59-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-10:51:59-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-10:51:59-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-10:51:59-root-INFO: grad norm: 18.168 18.067 1.909
2024-12-02-10:52:00-root-INFO: Loss too large (245.364->251.444)! Learning rate decreased to 0.07964.
2024-12-02-10:52:00-root-INFO: grad norm: 18.389 18.267 2.113
2024-12-02-10:52:00-root-INFO: Loss Change: 245.364 -> 242.333
2024-12-02-10:52:00-root-INFO: Regularization Change: 0.000 -> 0.583
2024-12-02-10:52:00-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-10:52:00-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-10:52:01-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-10:52:01-root-INFO: grad norm: 17.469 17.371 1.850
2024-12-02-10:52:01-root-INFO: Loss too large (241.424->247.230)! Learning rate decreased to 0.08180.
2024-12-02-10:52:01-root-INFO: grad norm: 17.742 17.618 2.096
2024-12-02-10:52:02-root-INFO: Loss Change: 241.424 -> 238.498
2024-12-02-10:52:02-root-INFO: Regularization Change: 0.000 -> 0.591
2024-12-02-10:52:02-root-INFO: Undo step: 90
2024-12-02-10:52:02-root-INFO: Undo step: 91
2024-12-02-10:52:02-root-INFO: Undo step: 92
2024-12-02-10:52:02-root-INFO: Undo step: 93
2024-12-02-10:52:02-root-INFO: Undo step: 94
2024-12-02-10:52:02-root-INFO: Undo step: 95
2024-12-02-10:52:02-root-INFO: Undo step: 96
2024-12-02-10:52:02-root-INFO: Undo step: 97
2024-12-02-10:52:02-root-INFO: Undo step: 98
2024-12-02-10:52:02-root-INFO: Undo step: 99
2024-12-02-10:52:02-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-10:52:02-root-INFO: grad norm: 80.451 79.567 11.890
2024-12-02-10:52:02-root-INFO: grad norm: 52.783 52.230 7.623
2024-12-02-10:52:03-root-INFO: Loss Change: 636.087 -> 370.204
2024-12-02-10:52:03-root-INFO: Regularization Change: 0.000 -> 52.417
2024-12-02-10:52:03-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-10:52:03-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-10:52:03-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-10:52:03-root-INFO: grad norm: 37.790 37.422 5.260
2024-12-02-10:52:04-root-INFO: grad norm: 35.449 35.193 4.254
2024-12-02-10:52:04-root-INFO: Loss Change: 369.069 -> 327.239
2024-12-02-10:52:04-root-INFO: Regularization Change: 0.000 -> 8.213
2024-12-02-10:52:04-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-10:52:04-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-10:52:04-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-10:52:04-root-INFO: grad norm: 33.960 33.723 4.011
2024-12-02-10:52:05-root-INFO: grad norm: 34.033 33.843 3.598
2024-12-02-10:52:05-root-INFO: Loss too large (309.329->309.603)! Learning rate decreased to 0.06579.
2024-12-02-10:52:05-root-INFO: Loss Change: 325.603 -> 295.641
2024-12-02-10:52:05-root-INFO: Regularization Change: 0.000 -> 3.358
2024-12-02-10:52:05-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-10:52:05-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-10:52:05-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-10:52:05-root-INFO: grad norm: 25.433 25.252 3.030
2024-12-02-10:52:06-root-INFO: grad norm: 31.956 31.782 3.335
2024-12-02-10:52:06-root-INFO: Loss too large (292.240->299.341)! Learning rate decreased to 0.06764.
2024-12-02-10:52:06-root-INFO: Loss Change: 293.803 -> 284.586
2024-12-02-10:52:06-root-INFO: Regularization Change: 0.000 -> 1.994
2024-12-02-10:52:06-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-10:52:06-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-10:52:06-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-10:52:07-root-INFO: grad norm: 26.192 26.029 2.921
2024-12-02-10:52:07-root-INFO: Loss too large (283.299->284.322)! Learning rate decreased to 0.06953.
2024-12-02-10:52:07-root-INFO: grad norm: 21.617 21.434 2.807
2024-12-02-10:52:08-root-INFO: Loss Change: 283.299 -> 268.235
2024-12-02-10:52:08-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-02-10:52:08-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-10:52:08-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-10:52:08-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-10:52:08-root-INFO: grad norm: 18.004 17.853 2.321
2024-12-02-10:52:08-root-INFO: Loss too large (267.316->268.094)! Learning rate decreased to 0.07147.
2024-12-02-10:52:08-root-INFO: grad norm: 17.078 16.905 2.424
2024-12-02-10:52:09-root-INFO: Loss Change: 267.316 -> 259.626
2024-12-02-10:52:09-root-INFO: Regularization Change: 0.000 -> 1.008
2024-12-02-10:52:09-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-10:52:09-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-10:52:09-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-10:52:09-root-INFO: grad norm: 15.621 15.482 2.076
2024-12-02-10:52:09-root-INFO: Loss too large (258.416->259.275)! Learning rate decreased to 0.07345.
2024-12-02-10:52:10-root-INFO: grad norm: 15.573 15.412 2.233
2024-12-02-10:52:10-root-INFO: Loss Change: 258.416 -> 252.686
2024-12-02-10:52:10-root-INFO: Regularization Change: 0.000 -> 0.891
2024-12-02-10:52:10-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-10:52:10-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-10:52:10-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-10:52:10-root-INFO: grad norm: 14.482 14.348 1.964
2024-12-02-10:52:10-root-INFO: Loss too large (251.584->252.433)! Learning rate decreased to 0.07547.
2024-12-02-10:52:11-root-INFO: grad norm: 14.625 14.472 2.114
2024-12-02-10:52:11-root-INFO: Loss Change: 251.584 -> 246.650
2024-12-02-10:52:11-root-INFO: Regularization Change: 0.000 -> 0.826
2024-12-02-10:52:11-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-10:52:11-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-10:52:11-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-10:52:12-root-INFO: grad norm: 14.209 14.087 1.858
2024-12-02-10:52:12-root-INFO: Loss too large (246.130->247.386)! Learning rate decreased to 0.07753.
2024-12-02-10:52:12-root-INFO: grad norm: 14.501 14.360 2.017
2024-12-02-10:52:13-root-INFO: Loss Change: 246.130 -> 241.834
2024-12-02-10:52:13-root-INFO: Regularization Change: 0.000 -> 0.779
2024-12-02-10:52:13-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-10:52:13-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-10:52:13-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-10:52:13-root-INFO: grad norm: 14.166 14.052 1.790
2024-12-02-10:52:13-root-INFO: Loss too large (241.130->243.075)! Learning rate decreased to 0.07964.
2024-12-02-10:52:14-root-INFO: grad norm: 14.684 14.548 1.992
2024-12-02-10:52:14-root-INFO: Loss Change: 241.130 -> 237.519
2024-12-02-10:52:14-root-INFO: Regularization Change: 0.000 -> 0.728
2024-12-02-10:52:14-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-10:52:14-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-10:52:14-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-10:52:14-root-INFO: grad norm: 14.264 14.156 1.752
2024-12-02-10:52:14-root-INFO: Loss too large (236.814->239.268)! Learning rate decreased to 0.08180.
2024-12-02-10:52:15-root-INFO: grad norm: 14.839 14.704 1.995
2024-12-02-10:52:15-root-INFO: Loss Change: 236.814 -> 233.590
2024-12-02-10:52:15-root-INFO: Regularization Change: 0.000 -> 0.708
2024-12-02-10:52:15-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-10:52:15-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-10:52:15-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-10:52:15-root-INFO: grad norm: 14.308 14.201 1.741
2024-12-02-10:52:16-root-INFO: Loss too large (232.325->235.198)! Learning rate decreased to 0.08399.
2024-12-02-10:52:16-root-INFO: grad norm: 14.988 14.858 1.974
2024-12-02-10:52:16-root-INFO: Loss Change: 232.325 -> 229.547
2024-12-02-10:52:16-root-INFO: Regularization Change: 0.000 -> 0.706
2024-12-02-10:52:16-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-10:52:16-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-10:52:17-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-10:52:17-root-INFO: grad norm: 15.096 14.995 1.745
2024-12-02-10:52:17-root-INFO: Loss too large (229.001->233.509)! Learning rate decreased to 0.08623.
2024-12-02-10:52:17-root-INFO: grad norm: 16.231 16.102 2.040
2024-12-02-10:52:18-root-INFO: Loss Change: 229.001 -> 227.211
2024-12-02-10:52:18-root-INFO: Regularization Change: 0.000 -> 0.690
2024-12-02-10:52:18-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-10:52:18-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-10:52:18-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-10:52:18-root-INFO: grad norm: 16.161 16.067 1.734
2024-12-02-10:52:18-root-INFO: Loss too large (225.466->231.702)! Learning rate decreased to 0.08852.
2024-12-02-10:52:19-root-INFO: grad norm: 17.447 17.327 2.045
2024-12-02-10:52:19-root-INFO: Loss Change: 225.466 -> 224.201
2024-12-02-10:52:19-root-INFO: Regularization Change: 0.000 -> 0.698
2024-12-02-10:52:19-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-10:52:19-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-10:52:19-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-10:52:19-root-INFO: grad norm: 17.755 17.665 1.787
2024-12-02-10:52:19-root-INFO: Loss too large (222.928->231.549)! Learning rate decreased to 0.09086.
2024-12-02-10:52:20-root-INFO: Loss too large (222.928->223.605)! Learning rate decreased to 0.07268.
2024-12-02-10:52:20-root-INFO: grad norm: 13.170 13.064 1.672
2024-12-02-10:52:20-root-INFO: Loss Change: 222.928 -> 215.717
2024-12-02-10:52:20-root-INFO: Regularization Change: 0.000 -> 0.546
2024-12-02-10:52:20-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-10:52:20-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-10:52:21-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-10:52:21-root-INFO: grad norm: 8.172 8.079 1.229
2024-12-02-10:52:21-root-INFO: grad norm: 12.258 12.159 1.553
2024-12-02-10:52:21-root-INFO: Loss too large (214.799->218.313)! Learning rate decreased to 0.09324.
2024-12-02-10:52:22-root-INFO: Loss Change: 214.937 -> 214.057
2024-12-02-10:52:22-root-INFO: Regularization Change: 0.000 -> 0.905
2024-12-02-10:52:22-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-10:52:22-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-10:52:22-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-10:52:22-root-INFO: grad norm: 13.496 13.420 1.432
2024-12-02-10:52:22-root-INFO: Loss too large (213.204->218.515)! Learning rate decreased to 0.09566.
2024-12-02-10:52:22-root-INFO: Loss too large (213.204->213.693)! Learning rate decreased to 0.07653.
2024-12-02-10:52:23-root-INFO: grad norm: 10.828 10.733 1.434
2024-12-02-10:52:23-root-INFO: Loss Change: 213.204 -> 208.456
2024-12-02-10:52:23-root-INFO: Regularization Change: 0.000 -> 0.450
2024-12-02-10:52:23-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-10:52:23-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-10:52:23-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-10:52:23-root-INFO: grad norm: 7.040 6.952 1.110
2024-12-02-10:52:24-root-INFO: grad norm: 10.379 10.283 1.407
2024-12-02-10:52:24-root-INFO: Loss too large (207.261->209.553)! Learning rate decreased to 0.09814.
2024-12-02-10:52:24-root-INFO: Loss Change: 207.767 -> 206.409
2024-12-02-10:52:24-root-INFO: Regularization Change: 0.000 -> 0.878
2024-12-02-10:52:24-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-10:52:24-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-10:52:24-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-10:52:25-root-INFO: grad norm: 11.565 11.485 1.357
2024-12-02-10:52:25-root-INFO: Loss too large (205.963->209.730)! Learning rate decreased to 0.10066.
2024-12-02-10:52:25-root-INFO: Loss too large (205.963->206.188)! Learning rate decreased to 0.08053.
2024-12-02-10:52:25-root-INFO: grad norm: 9.512 9.407 1.409
2024-12-02-10:52:26-root-INFO: Loss Change: 205.963 -> 201.987
2024-12-02-10:52:26-root-INFO: Regularization Change: 0.000 -> 0.439
2024-12-02-10:52:26-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-10:52:26-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-10:52:26-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-10:52:26-root-INFO: grad norm: 6.660 6.562 1.141
2024-12-02-10:52:26-root-INFO: grad norm: 9.526 9.405 1.515
2024-12-02-10:52:27-root-INFO: Loss too large (200.851->202.347)! Learning rate decreased to 0.10323.
2024-12-02-10:52:27-root-INFO: Loss Change: 201.358 -> 199.681
2024-12-02-10:52:27-root-INFO: Regularization Change: 0.000 -> 0.869
2024-12-02-10:52:27-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-10:52:27-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-10:52:27-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-10:52:27-root-INFO: grad norm: 10.535 10.429 1.489
2024-12-02-10:52:27-root-INFO: Loss too large (199.271->201.839)! Learning rate decreased to 0.10585.
2024-12-02-10:52:28-root-INFO: grad norm: 10.966 10.824 1.760
2024-12-02-10:52:28-root-INFO: Loss Change: 199.271 -> 196.970
2024-12-02-10:52:28-root-INFO: Regularization Change: 0.000 -> 0.685
2024-12-02-10:52:28-root-INFO: Undo step: 80
2024-12-02-10:52:28-root-INFO: Undo step: 81
2024-12-02-10:52:28-root-INFO: Undo step: 82
2024-12-02-10:52:28-root-INFO: Undo step: 83
2024-12-02-10:52:28-root-INFO: Undo step: 84
2024-12-02-10:52:28-root-INFO: Undo step: 85
2024-12-02-10:52:28-root-INFO: Undo step: 86
2024-12-02-10:52:28-root-INFO: Undo step: 87
2024-12-02-10:52:28-root-INFO: Undo step: 88
2024-12-02-10:52:28-root-INFO: Undo step: 89
2024-12-02-10:52:28-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-10:52:29-root-INFO: grad norm: 62.216 61.332 10.451
2024-12-02-10:52:29-root-INFO: grad norm: 53.424 53.072 6.125
2024-12-02-10:52:29-root-INFO: Loss Change: 521.729 -> 326.970
2024-12-02-10:52:29-root-INFO: Regularization Change: 0.000 -> 58.339
2024-12-02-10:52:29-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-10:52:29-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-10:52:30-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-10:52:30-root-INFO: grad norm: 38.622 38.376 4.354
2024-12-02-10:52:30-root-INFO: grad norm: 26.019 25.814 3.256
2024-12-02-10:52:30-root-INFO: Loss Change: 326.706 -> 254.655
2024-12-02-10:52:30-root-INFO: Regularization Change: 0.000 -> 12.267
2024-12-02-10:52:30-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-10:52:30-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-10:52:31-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-10:52:31-root-INFO: grad norm: 19.285 19.122 2.504
2024-12-02-10:52:31-root-INFO: grad norm: 18.428 18.285 2.291
2024-12-02-10:52:31-root-INFO: Loss Change: 254.542 -> 237.842
2024-12-02-10:52:31-root-INFO: Regularization Change: 0.000 -> 4.228
2024-12-02-10:52:31-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-10:52:31-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-10:52:32-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-10:52:32-root-INFO: grad norm: 19.351 19.222 2.223
2024-12-02-10:52:32-root-INFO: grad norm: 20.112 20.005 2.070
2024-12-02-10:52:33-root-INFO: Loss Change: 237.543 -> 229.912
2024-12-02-10:52:33-root-INFO: Regularization Change: 0.000 -> 2.539
2024-12-02-10:52:33-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-10:52:33-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-10:52:33-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-10:52:33-root-INFO: grad norm: 21.791 21.676 2.240
2024-12-02-10:52:33-root-INFO: grad norm: 22.033 21.918 2.248
2024-12-02-10:52:34-root-INFO: Loss Change: 230.355 -> 224.005
2024-12-02-10:52:34-root-INFO: Regularization Change: 0.000 -> 1.932
2024-12-02-10:52:34-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-10:52:34-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-10:52:34-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-10:52:34-root-INFO: grad norm: 22.704 22.554 2.603
2024-12-02-10:52:34-root-INFO: grad norm: 23.354 23.195 2.721
2024-12-02-10:52:35-root-INFO: Loss Change: 224.484 -> 220.599
2024-12-02-10:52:35-root-INFO: Regularization Change: 0.000 -> 1.786
2024-12-02-10:52:35-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-10:52:35-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-10:52:35-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-10:52:35-root-INFO: grad norm: 23.001 22.802 3.022
2024-12-02-10:52:36-root-INFO: grad norm: 21.590 21.443 2.515
2024-12-02-10:52:36-root-INFO: Loss Change: 221.186 -> 212.401
2024-12-02-10:52:36-root-INFO: Regularization Change: 0.000 -> 1.377
2024-12-02-10:52:36-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-10:52:36-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-10:52:36-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-10:52:36-root-INFO: grad norm: 21.081 20.887 2.853
2024-12-02-10:52:37-root-INFO: grad norm: 19.807 19.652 2.472
2024-12-02-10:52:37-root-INFO: Loss Change: 213.457 -> 206.108
2024-12-02-10:52:37-root-INFO: Regularization Change: 0.000 -> 1.246
2024-12-02-10:52:37-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-10:52:37-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-10:52:37-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-10:52:37-root-INFO: grad norm: 20.112 19.915 2.805
2024-12-02-10:52:38-root-INFO: grad norm: 20.738 20.546 2.819
2024-12-02-10:52:38-root-INFO: Loss Change: 207.371 -> 206.668
2024-12-02-10:52:38-root-INFO: Regularization Change: 0.000 -> 1.480
2024-12-02-10:52:38-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-10:52:38-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-10:52:38-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-10:52:39-root-INFO: grad norm: 25.032 24.794 3.443
2024-12-02-10:52:39-root-INFO: Loss too large (209.105->214.610)! Learning rate decreased to 0.10323.
2024-12-02-10:52:39-root-INFO: grad norm: 19.922 19.731 2.756
2024-12-02-10:52:40-root-INFO: Loss Change: 209.105 -> 195.390
2024-12-02-10:52:40-root-INFO: Regularization Change: 0.000 -> 1.358
2024-12-02-10:52:40-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-10:52:40-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-10:52:40-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-10:52:40-root-INFO: grad norm: 18.549 18.379 2.503
2024-12-02-10:52:40-root-INFO: Loss too large (197.573->201.871)! Learning rate decreased to 0.10585.
2024-12-02-10:52:41-root-INFO: grad norm: 16.352 16.197 2.243
2024-12-02-10:52:41-root-INFO: Loss Change: 197.573 -> 190.738
2024-12-02-10:52:41-root-INFO: Regularization Change: 0.000 -> 0.800
2024-12-02-10:52:41-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-10:52:41-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-10:52:41-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-10:52:41-root-INFO: grad norm: 16.678 16.523 2.270
2024-12-02-10:52:41-root-INFO: Loss too large (192.083->196.648)! Learning rate decreased to 0.10852.
2024-12-02-10:52:42-root-INFO: grad norm: 15.520 15.380 2.077
2024-12-02-10:52:42-root-INFO: Loss Change: 192.083 -> 187.268
2024-12-02-10:52:42-root-INFO: Regularization Change: 0.000 -> 0.697
2024-12-02-10:52:42-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-10:52:42-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-10:52:42-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-10:52:42-root-INFO: grad norm: 16.008 15.864 2.139
2024-12-02-10:52:43-root-INFO: Loss too large (188.839->192.993)! Learning rate decreased to 0.11124.
2024-12-02-10:52:43-root-INFO: grad norm: 14.817 14.690 1.937
2024-12-02-10:52:43-root-INFO: Loss Change: 188.839 -> 184.110
2024-12-02-10:52:43-root-INFO: Regularization Change: 0.000 -> 0.662
2024-12-02-10:52:43-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-10:52:43-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-10:52:44-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-10:52:44-root-INFO: grad norm: 15.269 15.128 2.069
2024-12-02-10:52:44-root-INFO: Loss too large (185.194->188.546)! Learning rate decreased to 0.11401.
2024-12-02-10:52:44-root-INFO: grad norm: 13.779 13.655 1.844
2024-12-02-10:52:45-root-INFO: Loss Change: 185.194 -> 180.121
2024-12-02-10:52:45-root-INFO: Regularization Change: 0.000 -> 0.677
2024-12-02-10:52:45-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-10:52:45-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-10:52:45-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-10:52:45-root-INFO: grad norm: 14.210 14.075 1.954
2024-12-02-10:52:45-root-INFO: Loss too large (181.440->184.259)! Learning rate decreased to 0.11682.
2024-12-02-10:52:46-root-INFO: grad norm: 12.844 12.729 1.711
2024-12-02-10:52:46-root-INFO: Loss Change: 181.440 -> 176.720
2024-12-02-10:52:46-root-INFO: Regularization Change: 0.000 -> 0.680
2024-12-02-10:52:46-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-10:52:46-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-10:52:46-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-10:52:46-root-INFO: grad norm: 13.371 13.239 1.870
2024-12-02-10:52:46-root-INFO: Loss too large (177.915->180.440)! Learning rate decreased to 0.11969.
2024-12-02-10:52:47-root-INFO: grad norm: 12.163 12.055 1.614
2024-12-02-10:52:47-root-INFO: Loss Change: 177.915 -> 173.545
2024-12-02-10:52:47-root-INFO: Regularization Change: 0.000 -> 0.682
2024-12-02-10:52:47-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-10:52:47-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-10:52:47-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-10:52:48-root-INFO: grad norm: 12.501 12.376 1.765
2024-12-02-10:52:48-root-INFO: Loss too large (174.624->176.741)! Learning rate decreased to 0.12261.
2024-12-02-10:52:48-root-INFO: grad norm: 11.391 11.289 1.525
2024-12-02-10:52:48-root-INFO: Loss Change: 174.624 -> 170.527
2024-12-02-10:52:48-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-02-10:52:49-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-10:52:49-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-10:52:49-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-10:52:49-root-INFO: grad norm: 11.931 11.802 1.746
2024-12-02-10:52:49-root-INFO: Loss too large (171.469->173.411)! Learning rate decreased to 0.12558.
2024-12-02-10:52:49-root-INFO: grad norm: 10.935 10.835 1.477
2024-12-02-10:52:50-root-INFO: Loss Change: 171.469 -> 167.614
2024-12-02-10:52:50-root-INFO: Regularization Change: 0.000 -> 0.695
2024-12-02-10:52:50-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-10:52:50-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-10:52:50-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-10:52:50-root-INFO: grad norm: 11.495 11.366 1.716
2024-12-02-10:52:50-root-INFO: Loss too large (168.489->170.339)! Learning rate decreased to 0.12860.
2024-12-02-10:52:51-root-INFO: grad norm: 10.582 10.483 1.449
2024-12-02-10:52:51-root-INFO: Loss Change: 168.489 -> 164.802
2024-12-02-10:52:51-root-INFO: Regularization Change: 0.000 -> 0.692
2024-12-02-10:52:51-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-10:52:51-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-10:52:51-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-10:52:51-root-INFO: grad norm: 10.775 10.649 1.644
2024-12-02-10:52:51-root-INFO: Loss too large (165.487->167.132)! Learning rate decreased to 0.13168.
2024-12-02-10:52:52-root-INFO: grad norm: 10.006 9.908 1.396
2024-12-02-10:52:52-root-INFO: Loss Change: 165.487 -> 162.103
2024-12-02-10:52:52-root-INFO: Regularization Change: 0.000 -> 0.675
2024-12-02-10:52:52-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-10:52:52-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-10:52:52-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-10:52:53-root-INFO: grad norm: 10.974 10.832 1.763
2024-12-02-10:52:53-root-INFO: Loss too large (162.990->164.666)! Learning rate decreased to 0.13480.
2024-12-02-10:52:53-root-INFO: grad norm: 9.885 9.787 1.392
2024-12-02-10:52:54-root-INFO: Loss Change: 162.990 -> 159.181
2024-12-02-10:52:54-root-INFO: Regularization Change: 0.000 -> 0.708
2024-12-02-10:52:54-root-INFO: Undo step: 70
2024-12-02-10:52:54-root-INFO: Undo step: 71
2024-12-02-10:52:54-root-INFO: Undo step: 72
2024-12-02-10:52:54-root-INFO: Undo step: 73
2024-12-02-10:52:54-root-INFO: Undo step: 74
2024-12-02-10:52:54-root-INFO: Undo step: 75
2024-12-02-10:52:54-root-INFO: Undo step: 76
2024-12-02-10:52:54-root-INFO: Undo step: 77
2024-12-02-10:52:54-root-INFO: Undo step: 78
2024-12-02-10:52:54-root-INFO: Undo step: 79
2024-12-02-10:52:54-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-10:52:54-root-INFO: grad norm: 44.494 43.927 7.085
2024-12-02-10:52:54-root-INFO: grad norm: 26.782 26.365 4.708
2024-12-02-10:52:55-root-INFO: Loss Change: 415.597 -> 246.471
2024-12-02-10:52:55-root-INFO: Regularization Change: 0.000 -> 50.529
2024-12-02-10:52:55-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-10:52:55-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-10:52:55-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-10:52:55-root-INFO: grad norm: 23.688 23.433 3.467
2024-12-02-10:52:55-root-INFO: grad norm: 21.252 21.002 3.249
2024-12-02-10:52:56-root-INFO: Loss Change: 245.005 -> 216.679
2024-12-02-10:52:56-root-INFO: Regularization Change: 0.000 -> 9.119
2024-12-02-10:52:56-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-10:52:56-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-10:52:56-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-10:52:56-root-INFO: grad norm: 20.862 20.633 3.079
2024-12-02-10:52:57-root-INFO: grad norm: 21.859 21.645 3.052
2024-12-02-10:52:57-root-INFO: Loss Change: 215.474 -> 207.567
2024-12-02-10:52:57-root-INFO: Regularization Change: 0.000 -> 4.354
2024-12-02-10:52:57-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-10:52:57-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-10:52:57-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-10:52:57-root-INFO: grad norm: 22.070 21.862 3.023
2024-12-02-10:52:58-root-INFO: grad norm: 22.565 22.354 3.075
2024-12-02-10:52:58-root-INFO: Loss Change: 205.664 -> 199.662
2024-12-02-10:52:58-root-INFO: Regularization Change: 0.000 -> 2.524
2024-12-02-10:52:58-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-10:52:58-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-10:52:58-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-10:52:58-root-INFO: grad norm: 21.827 21.632 2.912
2024-12-02-10:52:59-root-INFO: grad norm: 21.451 21.259 2.867
2024-12-02-10:52:59-root-INFO: Loss Change: 197.789 -> 190.848
2024-12-02-10:52:59-root-INFO: Regularization Change: 0.000 -> 1.840
2024-12-02-10:52:59-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-10:52:59-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-10:52:59-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-10:52:59-root-INFO: grad norm: 20.125 19.943 2.694
2024-12-02-10:53:00-root-INFO: grad norm: 19.797 19.613 2.691
2024-12-02-10:53:00-root-INFO: Loss Change: 189.180 -> 183.490
2024-12-02-10:53:00-root-INFO: Regularization Change: 0.000 -> 1.514
2024-12-02-10:53:00-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-10:53:00-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-10:53:00-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-10:53:00-root-INFO: grad norm: 18.603 18.431 2.524
2024-12-02-10:53:01-root-INFO: grad norm: 18.201 18.028 2.507
2024-12-02-10:53:01-root-INFO: Loss Change: 182.264 -> 177.077
2024-12-02-10:53:01-root-INFO: Regularization Change: 0.000 -> 1.313
2024-12-02-10:53:01-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-10:53:01-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-10:53:01-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-10:53:02-root-INFO: grad norm: 16.910 16.751 2.309
2024-12-02-10:53:02-root-INFO: grad norm: 16.628 16.469 2.296
2024-12-02-10:53:02-root-INFO: Loss Change: 175.605 -> 171.234
2024-12-02-10:53:02-root-INFO: Regularization Change: 0.000 -> 1.199
2024-12-02-10:53:02-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-10:53:02-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-10:53:03-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-10:53:03-root-INFO: grad norm: 15.410 15.264 2.112
2024-12-02-10:53:03-root-INFO: grad norm: 15.152 15.008 2.083
2024-12-02-10:53:03-root-INFO: Loss Change: 169.835 -> 165.863
2024-12-02-10:53:03-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-02-10:53:04-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-10:53:04-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-10:53:04-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-10:53:04-root-INFO: grad norm: 14.385 14.253 1.942
2024-12-02-10:53:04-root-INFO: grad norm: 14.147 14.018 1.908
2024-12-02-10:53:05-root-INFO: Loss Change: 165.201 -> 161.552
2024-12-02-10:53:05-root-INFO: Regularization Change: 0.000 -> 1.061
2024-12-02-10:53:05-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-10:53:05-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-10:53:05-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-10:53:05-root-INFO: grad norm: 13.048 12.933 1.725
2024-12-02-10:53:05-root-INFO: grad norm: 12.672 12.556 1.707
2024-12-02-10:53:06-root-INFO: Loss Change: 160.151 -> 156.451
2024-12-02-10:53:06-root-INFO: Regularization Change: 0.000 -> 1.067
2024-12-02-10:53:06-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-10:53:06-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-10:53:06-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-10:53:06-root-INFO: grad norm: 11.653 11.547 1.567
2024-12-02-10:53:06-root-INFO: grad norm: 11.584 11.475 1.582
2024-12-02-10:53:07-root-INFO: Loss Change: 155.856 -> 152.931
2024-12-02-10:53:07-root-INFO: Regularization Change: 0.000 -> 1.032
2024-12-02-10:53:07-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-10:53:07-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-10:53:07-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-10:53:07-root-INFO: grad norm: 11.361 11.260 1.507
2024-12-02-10:53:08-root-INFO: grad norm: 10.761 10.662 1.454
2024-12-02-10:53:08-root-INFO: Loss Change: 152.086 -> 148.892
2024-12-02-10:53:08-root-INFO: Regularization Change: 0.000 -> 1.186
2024-12-02-10:53:08-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-10:53:08-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-10:53:08-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-10:53:08-root-INFO: grad norm: 9.789 9.691 1.383
2024-12-02-10:53:09-root-INFO: grad norm: 9.708 9.616 1.334
2024-12-02-10:53:09-root-INFO: Loss Change: 148.246 -> 145.357
2024-12-02-10:53:09-root-INFO: Regularization Change: 0.000 -> 1.129
2024-12-02-10:53:09-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-10:53:09-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-10:53:09-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-10:53:09-root-INFO: grad norm: 9.385 9.303 1.238
2024-12-02-10:53:10-root-INFO: grad norm: 9.547 9.468 1.228
2024-12-02-10:53:10-root-INFO: Loss Change: 144.935 -> 142.722
2024-12-02-10:53:10-root-INFO: Regularization Change: 0.000 -> 0.995
2024-12-02-10:53:10-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-10:53:10-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-10:53:10-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-10:53:11-root-INFO: grad norm: 9.931 9.860 1.187
2024-12-02-10:53:11-root-INFO: grad norm: 9.454 9.381 1.171
2024-12-02-10:53:11-root-INFO: Loss Change: 142.336 -> 139.866
2024-12-02-10:53:11-root-INFO: Regularization Change: 0.000 -> 1.201
2024-12-02-10:53:11-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-10:53:11-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-10:53:12-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-10:53:12-root-INFO: grad norm: 8.858 8.793 1.071
2024-12-02-10:53:12-root-INFO: grad norm: 8.660 8.598 1.040
2024-12-02-10:53:12-root-INFO: Loss Change: 139.561 -> 136.633
2024-12-02-10:53:12-root-INFO: Regularization Change: 0.000 -> 1.174
2024-12-02-10:53:12-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-10:53:12-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-10:53:13-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-10:53:13-root-INFO: grad norm: 8.538 8.481 0.988
2024-12-02-10:53:13-root-INFO: grad norm: 8.348 8.288 0.997
2024-12-02-10:53:14-root-INFO: Loss Change: 136.471 -> 133.766
2024-12-02-10:53:14-root-INFO: Regularization Change: 0.000 -> 1.027
2024-12-02-10:53:14-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-10:53:14-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-10:53:14-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-10:53:14-root-INFO: grad norm: 8.352 8.294 0.980
2024-12-02-10:53:14-root-INFO: grad norm: 7.785 7.725 0.968
2024-12-02-10:53:15-root-INFO: Loss Change: 133.482 -> 130.349
2024-12-02-10:53:15-root-INFO: Regularization Change: 0.000 -> 1.063
2024-12-02-10:53:15-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-10:53:15-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-10:53:15-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-10:53:15-root-INFO: grad norm: 7.137 7.083 0.877
2024-12-02-10:53:15-root-INFO: grad norm: 6.806 6.752 0.860
2024-12-02-10:53:16-root-INFO: Loss Change: 130.329 -> 127.794
2024-12-02-10:53:16-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-02-10:53:16-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-10:53:16-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-10:53:16-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-10:53:16-root-INFO: grad norm: 8.709 8.656 0.961
2024-12-02-10:53:17-root-INFO: grad norm: 6.934 6.872 0.926
2024-12-02-10:53:17-root-INFO: Loss Change: 127.875 -> 125.505
2024-12-02-10:53:17-root-INFO: Regularization Change: 0.000 -> 1.931
2024-12-02-10:53:17-root-INFO: Undo step: 60
2024-12-02-10:53:17-root-INFO: Undo step: 61
2024-12-02-10:53:17-root-INFO: Undo step: 62
2024-12-02-10:53:17-root-INFO: Undo step: 63
2024-12-02-10:53:17-root-INFO: Undo step: 64
2024-12-02-10:53:17-root-INFO: Undo step: 65
2024-12-02-10:53:17-root-INFO: Undo step: 66
2024-12-02-10:53:17-root-INFO: Undo step: 67
2024-12-02-10:53:17-root-INFO: Undo step: 68
2024-12-02-10:53:17-root-INFO: Undo step: 69
2024-12-02-10:53:17-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-10:53:17-root-INFO: grad norm: 40.332 39.945 5.577
2024-12-02-10:53:18-root-INFO: grad norm: 17.889 17.631 3.027
2024-12-02-10:53:18-root-INFO: Loss Change: 355.668 -> 185.576
2024-12-02-10:53:18-root-INFO: Regularization Change: 0.000 -> 62.273
2024-12-02-10:53:18-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-10:53:18-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-10:53:18-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-10:53:18-root-INFO: grad norm: 11.516 11.327 2.080
2024-12-02-10:53:19-root-INFO: grad norm: 9.830 9.686 1.672
2024-12-02-10:53:19-root-INFO: Loss Change: 185.241 -> 163.449
2024-12-02-10:53:19-root-INFO: Regularization Change: 0.000 -> 8.449
2024-12-02-10:53:19-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-10:53:19-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-10:53:19-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-10:53:19-root-INFO: grad norm: 9.283 9.188 1.324
2024-12-02-10:53:20-root-INFO: grad norm: 9.280 9.203 1.193
2024-12-02-10:53:20-root-INFO: Loss Change: 162.718 -> 153.055
2024-12-02-10:53:20-root-INFO: Regularization Change: 0.000 -> 4.051
2024-12-02-10:53:20-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-10:53:20-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-10:53:20-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-10:53:20-root-INFO: grad norm: 9.539 9.476 1.089
2024-12-02-10:53:21-root-INFO: grad norm: 9.572 9.519 1.010
2024-12-02-10:53:21-root-INFO: Loss Change: 152.698 -> 146.305
2024-12-02-10:53:21-root-INFO: Regularization Change: 0.000 -> 2.618
2024-12-02-10:53:21-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-10:53:21-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-10:53:21-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-10:53:22-root-INFO: grad norm: 9.461 9.411 0.968
2024-12-02-10:53:22-root-INFO: grad norm: 9.173 9.126 0.923
2024-12-02-10:53:22-root-INFO: Loss Change: 146.188 -> 140.769
2024-12-02-10:53:22-root-INFO: Regularization Change: 0.000 -> 2.000
2024-12-02-10:53:22-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-10:53:22-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-10:53:23-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-10:53:23-root-INFO: grad norm: 8.846 8.796 0.935
2024-12-02-10:53:23-root-INFO: grad norm: 8.450 8.402 0.894
2024-12-02-10:53:23-root-INFO: Loss Change: 140.498 -> 135.743
2024-12-02-10:53:23-root-INFO: Regularization Change: 0.000 -> 1.705
2024-12-02-10:53:23-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-10:53:23-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-10:53:24-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-10:53:24-root-INFO: grad norm: 8.247 8.197 0.910
2024-12-02-10:53:24-root-INFO: grad norm: 8.021 7.969 0.906
2024-12-02-10:53:25-root-INFO: Loss Change: 135.614 -> 131.619
2024-12-02-10:53:25-root-INFO: Regularization Change: 0.000 -> 1.521
2024-12-02-10:53:25-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-10:53:25-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-10:53:25-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-10:53:25-root-INFO: grad norm: 7.946 7.885 0.985
2024-12-02-10:53:25-root-INFO: grad norm: 7.755 7.700 0.916
2024-12-02-10:53:26-root-INFO: Loss Change: 131.730 -> 128.272
2024-12-02-10:53:26-root-INFO: Regularization Change: 0.000 -> 1.380
2024-12-02-10:53:26-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-10:53:26-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-10:53:26-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-10:53:26-root-INFO: grad norm: 8.120 8.042 1.124
2024-12-02-10:53:26-root-INFO: grad norm: 8.038 7.974 1.009
2024-12-02-10:53:27-root-INFO: Loss Change: 128.245 -> 125.394
2024-12-02-10:53:27-root-INFO: Regularization Change: 0.000 -> 1.279
2024-12-02-10:53:27-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-10:53:27-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-10:53:27-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-10:53:27-root-INFO: grad norm: 8.955 8.866 1.260
2024-12-02-10:53:27-root-INFO: grad norm: 8.381 8.309 1.099
2024-12-02-10:53:28-root-INFO: Loss Change: 125.641 -> 122.669
2024-12-02-10:53:28-root-INFO: Regularization Change: 0.000 -> 1.389
2024-12-02-10:53:28-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-10:53:28-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-10:53:28-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-10:53:28-root-INFO: grad norm: 8.392 8.301 1.233
2024-12-02-10:53:29-root-INFO: grad norm: 8.094 8.027 1.036
2024-12-02-10:53:29-root-INFO: Loss Change: 123.071 -> 119.962
2024-12-02-10:53:29-root-INFO: Regularization Change: 0.000 -> 1.313
2024-12-02-10:53:29-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-10:53:29-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-10:53:29-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-10:53:29-root-INFO: grad norm: 8.864 8.771 1.283
2024-12-02-10:53:30-root-INFO: grad norm: 8.653 8.578 1.132
2024-12-02-10:53:30-root-INFO: Loss Change: 120.538 -> 117.693
2024-12-02-10:53:30-root-INFO: Regularization Change: 0.000 -> 1.148
2024-12-02-10:53:30-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-10:53:30-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-10:53:30-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-10:53:30-root-INFO: grad norm: 9.066 8.972 1.298
2024-12-02-10:53:31-root-INFO: grad norm: 8.562 8.483 1.158
2024-12-02-10:53:31-root-INFO: Loss Change: 118.203 -> 114.875
2024-12-02-10:53:31-root-INFO: Regularization Change: 0.000 -> 1.127
2024-12-02-10:53:31-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-10:53:31-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-10:53:31-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-10:53:31-root-INFO: grad norm: 8.857 8.771 1.234
2024-12-02-10:53:32-root-INFO: grad norm: 8.144 8.066 1.127
2024-12-02-10:53:32-root-INFO: Loss Change: 115.459 -> 111.823
2024-12-02-10:53:32-root-INFO: Regularization Change: 0.000 -> 1.128
2024-12-02-10:53:32-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-10:53:32-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-10:53:32-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-10:53:33-root-INFO: grad norm: 8.175 8.093 1.158
2024-12-02-10:53:33-root-INFO: grad norm: 7.595 7.520 1.067
2024-12-02-10:53:33-root-INFO: Loss Change: 112.337 -> 109.057
2024-12-02-10:53:33-root-INFO: Regularization Change: 0.000 -> 1.136
2024-12-02-10:53:33-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-10:53:33-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-10:53:34-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-10:53:34-root-INFO: grad norm: 8.195 8.114 1.147
2024-12-02-10:53:34-root-INFO: grad norm: 7.201 7.128 1.022
2024-12-02-10:53:34-root-INFO: Loss Change: 109.675 -> 106.338
2024-12-02-10:53:34-root-INFO: Regularization Change: 0.000 -> 1.256
2024-12-02-10:53:34-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-10:53:34-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-10:53:35-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-10:53:35-root-INFO: grad norm: 7.053 6.985 0.977
2024-12-02-10:53:35-root-INFO: grad norm: 6.334 6.275 0.860
2024-12-02-10:53:36-root-INFO: Loss Change: 106.700 -> 103.340
2024-12-02-10:53:36-root-INFO: Regularization Change: 0.000 -> 1.286
2024-12-02-10:53:36-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-10:53:36-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-10:53:36-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-10:53:36-root-INFO: grad norm: 6.640 6.585 0.858
2024-12-02-10:53:36-root-INFO: grad norm: 7.218 7.169 0.843
2024-12-02-10:53:37-root-INFO: Loss Change: 103.722 -> 102.132
2024-12-02-10:53:37-root-INFO: Regularization Change: 0.000 -> 1.562
2024-12-02-10:53:37-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-10:53:37-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-10:53:37-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-10:53:37-root-INFO: grad norm: 6.352 6.304 0.773
2024-12-02-10:53:37-root-INFO: grad norm: 5.764 5.711 0.782
2024-12-02-10:53:38-root-INFO: Loss Change: 102.365 -> 98.221
2024-12-02-10:53:38-root-INFO: Regularization Change: 0.000 -> 1.806
2024-12-02-10:53:38-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-10:53:38-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-10:53:38-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-10:53:38-root-INFO: grad norm: 6.007 5.947 0.842
2024-12-02-10:53:39-root-INFO: grad norm: 5.941 5.883 0.826
2024-12-02-10:53:39-root-INFO: Loss Change: 98.618 -> 96.944
2024-12-02-10:53:39-root-INFO: Regularization Change: 0.000 -> 1.292
2024-12-02-10:53:39-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-10:53:39-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-10:53:39-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-10:53:39-root-INFO: grad norm: 8.064 7.998 1.030
2024-12-02-10:53:40-root-INFO: grad norm: 6.277 6.213 0.896
2024-12-02-10:53:40-root-INFO: Loss Change: 97.278 -> 94.541
2024-12-02-10:53:40-root-INFO: Regularization Change: 0.000 -> 2.317
2024-12-02-10:53:40-root-INFO: Undo step: 50
2024-12-02-10:53:40-root-INFO: Undo step: 51
2024-12-02-10:53:40-root-INFO: Undo step: 52
2024-12-02-10:53:40-root-INFO: Undo step: 53
2024-12-02-10:53:40-root-INFO: Undo step: 54
2024-12-02-10:53:40-root-INFO: Undo step: 55
2024-12-02-10:53:40-root-INFO: Undo step: 56
2024-12-02-10:53:40-root-INFO: Undo step: 57
2024-12-02-10:53:40-root-INFO: Undo step: 58
2024-12-02-10:53:40-root-INFO: Undo step: 59
2024-12-02-10:53:40-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-10:53:40-root-INFO: grad norm: 31.327 31.052 4.141
2024-12-02-10:53:41-root-INFO: grad norm: 15.639 15.407 2.682
2024-12-02-10:53:41-root-INFO: Loss Change: 281.757 -> 152.739
2024-12-02-10:53:41-root-INFO: Regularization Change: 0.000 -> 61.884
2024-12-02-10:53:41-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-10:53:41-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-10:53:41-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-10:53:41-root-INFO: grad norm: 10.643 10.481 1.854
2024-12-02-10:53:42-root-INFO: grad norm: 8.150 8.002 1.547
2024-12-02-10:53:42-root-INFO: Loss Change: 152.444 -> 130.746
2024-12-02-10:53:42-root-INFO: Regularization Change: 0.000 -> 9.668
2024-12-02-10:53:42-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-10:53:42-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-10:53:42-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-10:53:42-root-INFO: grad norm: 6.771 6.669 1.173
2024-12-02-10:53:43-root-INFO: grad norm: 5.805 5.714 1.024
2024-12-02-10:53:43-root-INFO: Loss Change: 130.480 -> 120.205
2024-12-02-10:53:43-root-INFO: Regularization Change: 0.000 -> 4.605
2024-12-02-10:53:43-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-10:53:43-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-10:53:43-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-10:53:44-root-INFO: grad norm: 5.481 5.406 0.903
2024-12-02-10:53:44-root-INFO: grad norm: 4.918 4.846 0.836
2024-12-02-10:53:44-root-INFO: Loss Change: 119.939 -> 113.169
2024-12-02-10:53:44-root-INFO: Regularization Change: 0.000 -> 3.059
2024-12-02-10:53:44-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-10:53:44-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-10:53:45-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-10:53:45-root-INFO: grad norm: 5.077 5.007 0.837
2024-12-02-10:53:45-root-INFO: grad norm: 4.726 4.662 0.772
2024-12-02-10:53:45-root-INFO: Loss Change: 113.279 -> 108.003
2024-12-02-10:53:45-root-INFO: Regularization Change: 0.000 -> 2.430
2024-12-02-10:53:45-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-10:53:45-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-10:53:46-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-10:53:46-root-INFO: grad norm: 5.194 5.131 0.807
2024-12-02-10:53:46-root-INFO: grad norm: 5.202 5.143 0.783
2024-12-02-10:53:46-root-INFO: Loss Change: 108.076 -> 103.709
2024-12-02-10:53:46-root-INFO: Regularization Change: 0.000 -> 2.357
2024-12-02-10:53:46-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-10:53:46-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-10:53:47-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-10:53:47-root-INFO: grad norm: 6.672 6.603 0.961
2024-12-02-10:53:47-root-INFO: grad norm: 5.841 5.778 0.857
2024-12-02-10:53:48-root-INFO: Loss Change: 103.875 -> 100.423
2024-12-02-10:53:48-root-INFO: Regularization Change: 0.000 -> 1.956
2024-12-02-10:53:48-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-10:53:48-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-10:53:48-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-10:53:48-root-INFO: grad norm: 6.022 5.961 0.850
2024-12-02-10:53:48-root-INFO: grad norm: 5.685 5.629 0.793
2024-12-02-10:53:49-root-INFO: Loss Change: 100.627 -> 97.241
2024-12-02-10:53:49-root-INFO: Regularization Change: 0.000 -> 1.626
2024-12-02-10:53:49-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-10:53:49-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-10:53:49-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-10:53:49-root-INFO: grad norm: 6.205 6.152 0.808
2024-12-02-10:53:49-root-INFO: grad norm: 7.070 7.025 0.797
2024-12-02-10:53:50-root-INFO: Loss Change: 97.436 -> 95.415
2024-12-02-10:53:50-root-INFO: Regularization Change: 0.000 -> 1.801
2024-12-02-10:53:50-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-10:53:50-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-10:53:50-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-10:53:50-root-INFO: grad norm: 6.089 6.038 0.782
2024-12-02-10:53:50-root-INFO: grad norm: 5.574 5.520 0.772
2024-12-02-10:53:51-root-INFO: Loss Change: 95.600 -> 91.473
2024-12-02-10:53:51-root-INFO: Regularization Change: 0.000 -> 1.836
2024-12-02-10:53:51-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-10:53:51-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-10:53:51-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-10:53:51-root-INFO: grad norm: 5.876 5.817 0.831
2024-12-02-10:53:52-root-INFO: grad norm: 5.879 5.827 0.779
2024-12-02-10:53:52-root-INFO: Loss Change: 91.502 -> 89.306
2024-12-02-10:53:52-root-INFO: Regularization Change: 0.000 -> 1.467
2024-12-02-10:53:52-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-10:53:52-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-10:53:52-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-10:53:52-root-INFO: grad norm: 6.259 6.204 0.832
2024-12-02-10:53:53-root-INFO: grad norm: 6.059 6.005 0.812
2024-12-02-10:53:53-root-INFO: Loss Change: 89.591 -> 87.091
2024-12-02-10:53:53-root-INFO: Regularization Change: 0.000 -> 1.282
2024-12-02-10:53:53-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-10:53:53-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-10:53:53-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-10:53:53-root-INFO: grad norm: 6.365 6.310 0.837
2024-12-02-10:53:54-root-INFO: grad norm: 6.306 6.253 0.811
2024-12-02-10:53:54-root-INFO: Loss Change: 87.325 -> 85.113
2024-12-02-10:53:54-root-INFO: Regularization Change: 0.000 -> 1.244
2024-12-02-10:53:54-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-10:53:54-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-10:53:54-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-10:53:54-root-INFO: grad norm: 6.808 6.749 0.894
2024-12-02-10:53:55-root-INFO: grad norm: 6.538 6.483 0.845
2024-12-02-10:53:55-root-INFO: Loss Change: 85.496 -> 82.926
2024-12-02-10:53:55-root-INFO: Regularization Change: 0.000 -> 1.232
2024-12-02-10:53:55-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-10:53:55-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-10:53:55-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-10:53:56-root-INFO: grad norm: 7.002 6.945 0.891
2024-12-02-10:53:56-root-INFO: grad norm: 7.133 7.082 0.855
2024-12-02-10:53:56-root-INFO: Loss Change: 83.207 -> 81.254
2024-12-02-10:53:56-root-INFO: Regularization Change: 0.000 -> 1.313
2024-12-02-10:53:56-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-10:53:56-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-10:53:56-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-10:53:57-root-INFO: grad norm: 6.778 6.724 0.850
2024-12-02-10:53:57-root-INFO: grad norm: 6.261 6.200 0.871
2024-12-02-10:53:57-root-INFO: Loss Change: 81.459 -> 78.042
2024-12-02-10:53:57-root-INFO: Regularization Change: 0.000 -> 1.761
2024-12-02-10:53:57-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-10:53:57-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-10:53:58-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-10:53:58-root-INFO: grad norm: 7.049 6.972 1.038
2024-12-02-10:53:58-root-INFO: Loss too large (78.582->78.764)! Learning rate decreased to 0.23450.
2024-12-02-10:53:58-root-INFO: grad norm: 5.311 5.244 0.839
2024-12-02-10:53:59-root-INFO: Loss Change: 78.582 -> 74.835
2024-12-02-10:53:59-root-INFO: Regularization Change: 0.000 -> 1.142
2024-12-02-10:53:59-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-10:53:59-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-10:53:59-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-10:53:59-root-INFO: grad norm: 4.956 4.903 0.725
2024-12-02-10:53:59-root-INFO: grad norm: 5.098 5.030 0.833
2024-12-02-10:54:00-root-INFO: Loss Change: 74.926 -> 73.288
2024-12-02-10:54:00-root-INFO: Regularization Change: 0.000 -> 1.354
2024-12-02-10:54:00-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-10:54:00-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-10:54:00-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-10:54:00-root-INFO: grad norm: 6.144 6.074 0.921
2024-12-02-10:54:00-root-INFO: Loss too large (73.563->73.927)! Learning rate decreased to 0.24361.
2024-12-02-10:54:01-root-INFO: grad norm: 4.982 4.918 0.796
2024-12-02-10:54:01-root-INFO: Loss Change: 73.563 -> 70.638
2024-12-02-10:54:01-root-INFO: Regularization Change: 0.000 -> 1.009
2024-12-02-10:54:01-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-10:54:01-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-10:54:01-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-10:54:01-root-INFO: grad norm: 4.946 4.890 0.744
2024-12-02-10:54:02-root-INFO: grad norm: 5.303 5.236 0.837
2024-12-02-10:54:02-root-INFO: Loss Change: 70.985 -> 69.703
2024-12-02-10:54:02-root-INFO: Regularization Change: 0.000 -> 1.437
2024-12-02-10:54:02-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-10:54:02-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-10:54:02-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-10:54:02-root-INFO: grad norm: 6.776 6.703 0.992
2024-12-02-10:54:03-root-INFO: Loss too large (70.230->70.482)! Learning rate decreased to 0.25333.
2024-12-02-10:54:03-root-INFO: grad norm: 5.156 5.093 0.803
2024-12-02-10:54:03-root-INFO: Loss Change: 70.230 -> 66.481
2024-12-02-10:54:03-root-INFO: Regularization Change: 0.000 -> 1.115
2024-12-02-10:54:03-root-INFO: Undo step: 40
2024-12-02-10:54:03-root-INFO: Undo step: 41
2024-12-02-10:54:03-root-INFO: Undo step: 42
2024-12-02-10:54:03-root-INFO: Undo step: 43
2024-12-02-10:54:03-root-INFO: Undo step: 44
2024-12-02-10:54:03-root-INFO: Undo step: 45
2024-12-02-10:54:03-root-INFO: Undo step: 46
2024-12-02-10:54:03-root-INFO: Undo step: 47
2024-12-02-10:54:03-root-INFO: Undo step: 48
2024-12-02-10:54:03-root-INFO: Undo step: 49
2024-12-02-10:54:03-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-10:54:04-root-INFO: grad norm: 26.754 26.585 3.002
2024-12-02-10:54:04-root-INFO: grad norm: 13.698 13.567 1.894
2024-12-02-10:54:04-root-INFO: Loss Change: 246.452 -> 123.173
2024-12-02-10:54:04-root-INFO: Regularization Change: 0.000 -> 72.135
2024-12-02-10:54:04-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-10:54:04-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-10:54:05-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-10:54:05-root-INFO: grad norm: 8.931 8.801 1.516
2024-12-02-10:54:05-root-INFO: grad norm: 6.937 6.843 1.138
2024-12-02-10:54:06-root-INFO: Loss Change: 122.625 -> 102.068
2024-12-02-10:54:06-root-INFO: Regularization Change: 0.000 -> 11.459
2024-12-02-10:54:06-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-10:54:06-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-10:54:06-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-10:54:06-root-INFO: grad norm: 6.196 6.096 1.108
2024-12-02-10:54:06-root-INFO: grad norm: 5.602 5.523 0.940
2024-12-02-10:54:07-root-INFO: Loss Change: 101.886 -> 92.407
2024-12-02-10:54:07-root-INFO: Regularization Change: 0.000 -> 5.743
2024-12-02-10:54:07-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-10:54:07-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-10:54:07-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-10:54:07-root-INFO: grad norm: 6.747 6.659 1.082
2024-12-02-10:54:08-root-INFO: grad norm: 5.721 5.638 0.969
2024-12-02-10:54:08-root-INFO: Loss Change: 92.463 -> 86.734
2024-12-02-10:54:08-root-INFO: Regularization Change: 0.000 -> 3.811
2024-12-02-10:54:08-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-10:54:08-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-10:54:08-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-10:54:08-root-INFO: grad norm: 5.776 5.699 0.936
2024-12-02-10:54:09-root-INFO: grad norm: 5.373 5.314 0.794
2024-12-02-10:54:09-root-INFO: Loss Change: 86.768 -> 82.132
2024-12-02-10:54:09-root-INFO: Regularization Change: 0.000 -> 2.732
2024-12-02-10:54:09-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-10:54:09-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-10:54:09-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-10:54:09-root-INFO: grad norm: 6.031 5.959 0.931
2024-12-02-10:54:10-root-INFO: grad norm: 6.421 6.370 0.806
2024-12-02-10:54:10-root-INFO: Loss Change: 82.096 -> 79.301
2024-12-02-10:54:10-root-INFO: Regularization Change: 0.000 -> 2.414
2024-12-02-10:54:10-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-10:54:10-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-10:54:11-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-10:54:11-root-INFO: grad norm: 5.705 5.655 0.754
2024-12-02-10:54:11-root-INFO: grad norm: 5.300 5.250 0.728
2024-12-02-10:54:11-root-INFO: Loss Change: 79.451 -> 75.033
2024-12-02-10:54:11-root-INFO: Regularization Change: 0.000 -> 2.685
2024-12-02-10:54:11-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-10:54:11-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-10:54:12-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-10:54:12-root-INFO: grad norm: 6.180 6.107 0.944
2024-12-02-10:54:12-root-INFO: grad norm: 6.082 5.999 0.999
2024-12-02-10:54:13-root-INFO: Loss Change: 75.136 -> 72.535
2024-12-02-10:54:13-root-INFO: Regularization Change: 0.000 -> 1.845
2024-12-02-10:54:13-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-10:54:13-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-10:54:13-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-10:54:13-root-INFO: grad norm: 6.641 6.563 1.015
2024-12-02-10:54:13-root-INFO: Loss too large (72.776->72.840)! Learning rate decreased to 0.24361.
2024-12-02-10:54:14-root-INFO: grad norm: 5.066 5.004 0.791
2024-12-02-10:54:14-root-INFO: Loss Change: 72.776 -> 68.844
2024-12-02-10:54:14-root-INFO: Regularization Change: 0.000 -> 1.251
2024-12-02-10:54:14-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-10:54:14-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-10:54:14-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-10:54:14-root-INFO: grad norm: 4.852 4.782 0.818
2024-12-02-10:54:15-root-INFO: grad norm: 5.239 5.167 0.862
2024-12-02-10:54:15-root-INFO: Loss Change: 69.105 -> 67.456
2024-12-02-10:54:15-root-INFO: Regularization Change: 0.000 -> 1.605
2024-12-02-10:54:15-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-10:54:15-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-10:54:15-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-10:54:15-root-INFO: grad norm: 6.613 6.533 1.024
2024-12-02-10:54:16-root-INFO: Loss too large (67.951->68.167)! Learning rate decreased to 0.25333.
2024-12-02-10:54:16-root-INFO: grad norm: 5.022 4.960 0.784
2024-12-02-10:54:16-root-INFO: Loss Change: 67.951 -> 64.159
2024-12-02-10:54:16-root-INFO: Regularization Change: 0.000 -> 1.172
2024-12-02-10:54:16-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-10:54:16-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-10:54:17-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-10:54:17-root-INFO: grad norm: 4.247 4.197 0.651
2024-12-02-10:54:17-root-INFO: grad norm: 4.854 4.797 0.744
2024-12-02-10:54:18-root-INFO: Loss Change: 64.238 -> 63.185
2024-12-02-10:54:18-root-INFO: Regularization Change: 0.000 -> 1.547
2024-12-02-10:54:18-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-10:54:18-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-10:54:18-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-10:54:18-root-INFO: grad norm: 6.436 6.363 0.966
2024-12-02-10:54:19-root-INFO: grad norm: 6.038 5.961 0.965
2024-12-02-10:54:19-root-INFO: Loss Change: 63.517 -> 60.994
2024-12-02-10:54:19-root-INFO: Regularization Change: 0.000 -> 1.689
2024-12-02-10:54:19-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-10:54:19-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-10:54:19-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-10:54:19-root-INFO: grad norm: 6.181 6.113 0.915
2024-12-02-10:54:20-root-INFO: grad norm: 5.954 5.884 0.912
2024-12-02-10:54:20-root-INFO: Loss Change: 61.443 -> 59.036
2024-12-02-10:54:20-root-INFO: Regularization Change: 0.000 -> 1.583
2024-12-02-10:54:20-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-10:54:20-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-10:54:20-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-10:54:20-root-INFO: grad norm: 6.375 6.302 0.958
2024-12-02-10:54:21-root-INFO: grad norm: 6.126 6.051 0.958
2024-12-02-10:54:21-root-INFO: Loss Change: 59.447 -> 57.003
2024-12-02-10:54:21-root-INFO: Regularization Change: 0.000 -> 1.456
2024-12-02-10:54:21-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-10:54:21-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-10:54:21-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-10:54:21-root-INFO: grad norm: 6.477 6.406 0.960
2024-12-02-10:54:22-root-INFO: grad norm: 5.957 5.884 0.933
2024-12-02-10:54:22-root-INFO: Loss Change: 57.496 -> 54.558
2024-12-02-10:54:22-root-INFO: Regularization Change: 0.000 -> 1.472
2024-12-02-10:54:22-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-10:54:22-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-10:54:22-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-10:54:22-root-INFO: grad norm: 5.978 5.913 0.880
2024-12-02-10:54:23-root-INFO: grad norm: 5.563 5.500 0.837
2024-12-02-10:54:23-root-INFO: Loss Change: 54.920 -> 52.275
2024-12-02-10:54:23-root-INFO: Regularization Change: 0.000 -> 1.480
2024-12-02-10:54:23-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-10:54:23-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-10:54:23-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-10:54:24-root-INFO: grad norm: 5.841 5.778 0.852
2024-12-02-10:54:24-root-INFO: grad norm: 5.304 5.244 0.796
2024-12-02-10:54:24-root-INFO: Loss Change: 52.455 -> 49.586
2024-12-02-10:54:24-root-INFO: Regularization Change: 0.000 -> 1.491
2024-12-02-10:54:24-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-10:54:24-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-10:54:24-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-10:54:25-root-INFO: grad norm: 5.462 5.403 0.805
2024-12-02-10:54:25-root-INFO: grad norm: 5.010 4.955 0.737
2024-12-02-10:54:25-root-INFO: Loss Change: 50.000 -> 47.360
2024-12-02-10:54:25-root-INFO: Regularization Change: 0.000 -> 1.494
2024-12-02-10:54:25-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-10:54:25-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-10:54:26-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-10:54:26-root-INFO: grad norm: 5.220 5.166 0.750
2024-12-02-10:54:26-root-INFO: grad norm: 4.779 4.728 0.698
2024-12-02-10:54:27-root-INFO: Loss Change: 47.630 -> 45.033
2024-12-02-10:54:27-root-INFO: Regularization Change: 0.000 -> 1.481
2024-12-02-10:54:27-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-10:54:27-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-10:54:27-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-10:54:27-root-INFO: grad norm: 4.958 4.907 0.707
2024-12-02-10:54:27-root-INFO: grad norm: 4.572 4.525 0.655
2024-12-02-10:54:28-root-INFO: Loss Change: 45.250 -> 42.791
2024-12-02-10:54:28-root-INFO: Regularization Change: 0.000 -> 1.487
2024-12-02-10:54:28-root-INFO: Undo step: 30
2024-12-02-10:54:28-root-INFO: Undo step: 31
2024-12-02-10:54:28-root-INFO: Undo step: 32
2024-12-02-10:54:28-root-INFO: Undo step: 33
2024-12-02-10:54:28-root-INFO: Undo step: 34
2024-12-02-10:54:28-root-INFO: Undo step: 35
2024-12-02-10:54:28-root-INFO: Undo step: 36
2024-12-02-10:54:28-root-INFO: Undo step: 37
2024-12-02-10:54:28-root-INFO: Undo step: 38
2024-12-02-10:54:28-root-INFO: Undo step: 39
2024-12-02-10:54:28-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-10:54:28-root-INFO: grad norm: 23.303 23.147 2.695
2024-12-02-10:54:29-root-INFO: grad norm: 10.452 10.341 1.523
2024-12-02-10:54:29-root-INFO: Loss Change: 202.138 -> 93.002
2024-12-02-10:54:29-root-INFO: Regularization Change: 0.000 -> 76.794
2024-12-02-10:54:29-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-10:54:29-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-10:54:29-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-10:54:29-root-INFO: grad norm: 8.069 7.983 1.173
2024-12-02-10:54:30-root-INFO: grad norm: 7.455 7.380 1.055
2024-12-02-10:54:30-root-INFO: Loss Change: 92.666 -> 76.076
2024-12-02-10:54:30-root-INFO: Regularization Change: 0.000 -> 12.484
2024-12-02-10:54:30-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-10:54:30-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-10:54:30-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-10:54:30-root-INFO: grad norm: 6.424 6.359 0.910
2024-12-02-10:54:31-root-INFO: grad norm: 5.609 5.565 0.701
2024-12-02-10:54:31-root-INFO: Loss Change: 75.807 -> 66.620
2024-12-02-10:54:31-root-INFO: Regularization Change: 0.000 -> 6.223
2024-12-02-10:54:31-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-10:54:31-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-10:54:31-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-10:54:31-root-INFO: grad norm: 5.343 5.288 0.770
2024-12-02-10:54:32-root-INFO: grad norm: 5.290 5.253 0.623
2024-12-02-10:54:32-root-INFO: Loss Change: 66.398 -> 61.303
2024-12-02-10:54:32-root-INFO: Regularization Change: 0.000 -> 3.859
2024-12-02-10:54:32-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-10:54:32-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-10:54:32-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-10:54:33-root-INFO: grad norm: 5.069 5.017 0.728
2024-12-02-10:54:33-root-INFO: grad norm: 4.571 4.541 0.523
2024-12-02-10:54:33-root-INFO: Loss Change: 61.196 -> 56.555
2024-12-02-10:54:33-root-INFO: Regularization Change: 0.000 -> 2.954
2024-12-02-10:54:33-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-10:54:33-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-10:54:34-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-10:54:34-root-INFO: grad norm: 4.694 4.650 0.643
2024-12-02-10:54:34-root-INFO: grad norm: 4.736 4.706 0.536
2024-12-02-10:54:35-root-INFO: Loss Change: 56.439 -> 53.464
2024-12-02-10:54:35-root-INFO: Regularization Change: 0.000 -> 2.400
2024-12-02-10:54:35-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-10:54:35-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-10:54:35-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-10:54:35-root-INFO: grad norm: 4.713 4.671 0.627
2024-12-02-10:54:35-root-INFO: grad norm: 4.325 4.299 0.479
2024-12-02-10:54:36-root-INFO: Loss Change: 53.309 -> 50.055
2024-12-02-10:54:36-root-INFO: Regularization Change: 0.000 -> 2.052
2024-12-02-10:54:36-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-10:54:36-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-10:54:36-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-10:54:36-root-INFO: grad norm: 4.446 4.408 0.583
2024-12-02-10:54:37-root-INFO: grad norm: 4.328 4.304 0.464
2024-12-02-10:54:37-root-INFO: Loss Change: 49.776 -> 47.238
2024-12-02-10:54:37-root-INFO: Regularization Change: 0.000 -> 1.855
2024-12-02-10:54:37-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-10:54:37-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-10:54:37-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-10:54:37-root-INFO: grad norm: 4.385 4.350 0.559
2024-12-02-10:54:38-root-INFO: grad norm: 4.034 4.010 0.440
2024-12-02-10:54:38-root-INFO: Loss Change: 47.148 -> 44.470
2024-12-02-10:54:38-root-INFO: Regularization Change: 0.000 -> 1.722
2024-12-02-10:54:38-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-10:54:38-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-10:54:38-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-10:54:38-root-INFO: grad norm: 4.142 4.110 0.517
2024-12-02-10:54:39-root-INFO: grad norm: 4.026 4.003 0.424
2024-12-02-10:54:39-root-INFO: Loss Change: 44.366 -> 42.188
2024-12-02-10:54:39-root-INFO: Regularization Change: 0.000 -> 1.620
2024-12-02-10:54:39-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-10:54:39-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-10:54:39-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-10:54:39-root-INFO: grad norm: 4.100 4.069 0.509
2024-12-02-10:54:40-root-INFO: grad norm: 3.737 3.715 0.403
2024-12-02-10:54:40-root-INFO: Loss Change: 42.043 -> 39.602
2024-12-02-10:54:40-root-INFO: Regularization Change: 0.000 -> 1.572
2024-12-02-10:54:40-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-10:54:40-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-10:54:40-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-10:54:41-root-INFO: grad norm: 3.906 3.876 0.484
2024-12-02-10:54:41-root-INFO: grad norm: 3.893 3.872 0.407
2024-12-02-10:54:41-root-INFO: Loss Change: 39.547 -> 37.687
2024-12-02-10:54:41-root-INFO: Regularization Change: 0.000 -> 1.575
2024-12-02-10:54:41-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-10:54:41-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-10:54:42-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-10:54:42-root-INFO: grad norm: 3.974 3.944 0.488
2024-12-02-10:54:42-root-INFO: grad norm: 3.567 3.546 0.388
2024-12-02-10:54:43-root-INFO: Loss Change: 37.471 -> 35.049
2024-12-02-10:54:43-root-INFO: Regularization Change: 0.000 -> 1.596
2024-12-02-10:54:43-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-10:54:43-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-10:54:43-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-10:54:43-root-INFO: grad norm: 3.697 3.669 0.455
2024-12-02-10:54:43-root-INFO: grad norm: 3.538 3.518 0.381
2024-12-02-10:54:44-root-INFO: Loss Change: 35.025 -> 33.175
2024-12-02-10:54:44-root-INFO: Regularization Change: 0.000 -> 1.476
2024-12-02-10:54:44-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-10:54:44-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-10:54:44-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-10:54:44-root-INFO: grad norm: 3.826 3.797 0.468
2024-12-02-10:54:44-root-INFO: grad norm: 3.647 3.625 0.399
2024-12-02-10:54:45-root-INFO: Loss Change: 33.135 -> 31.295
2024-12-02-10:54:45-root-INFO: Regularization Change: 0.000 -> 1.415
2024-12-02-10:54:45-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-10:54:45-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-10:54:45-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-10:54:45-root-INFO: grad norm: 3.832 3.803 0.476
2024-12-02-10:54:45-root-INFO: grad norm: 3.493 3.471 0.391
2024-12-02-10:54:46-root-INFO: Loss Change: 31.116 -> 29.094
2024-12-02-10:54:46-root-INFO: Regularization Change: 0.000 -> 1.389
2024-12-02-10:54:46-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-10:54:46-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-10:54:46-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-10:54:46-root-INFO: grad norm: 3.787 3.757 0.478
2024-12-02-10:54:47-root-INFO: grad norm: 3.533 3.510 0.400
2024-12-02-10:54:47-root-INFO: Loss Change: 29.178 -> 27.368
2024-12-02-10:54:47-root-INFO: Regularization Change: 0.000 -> 1.357
2024-12-02-10:54:47-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-10:54:47-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-10:54:47-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-10:54:47-root-INFO: grad norm: 3.676 3.647 0.462
2024-12-02-10:54:48-root-INFO: grad norm: 3.338 3.316 0.383
2024-12-02-10:54:48-root-INFO: Loss Change: 27.208 -> 25.331
2024-12-02-10:54:48-root-INFO: Regularization Change: 0.000 -> 1.330
2024-12-02-10:54:48-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-10:54:48-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-10:54:48-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-10:54:48-root-INFO: grad norm: 3.641 3.613 0.448
2024-12-02-10:54:49-root-INFO: grad norm: 3.286 3.264 0.383
2024-12-02-10:54:49-root-INFO: Loss Change: 25.348 -> 23.577
2024-12-02-10:54:49-root-INFO: Regularization Change: 0.000 -> 1.307
2024-12-02-10:54:49-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-10:54:49-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-10:54:49-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-10:54:49-root-INFO: grad norm: 3.413 3.388 0.415
2024-12-02-10:54:50-root-INFO: grad norm: 3.085 3.066 0.344
2024-12-02-10:54:50-root-INFO: Loss Change: 23.501 -> 21.778
2024-12-02-10:54:50-root-INFO: Regularization Change: 0.000 -> 1.283
2024-12-02-10:54:50-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-10:54:50-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-10:54:50-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-10:54:50-root-INFO: grad norm: 3.347 3.323 0.406
2024-12-02-10:54:51-root-INFO: grad norm: 3.017 2.997 0.349
2024-12-02-10:54:51-root-INFO: Loss Change: 21.755 -> 20.082
2024-12-02-10:54:51-root-INFO: Regularization Change: 0.000 -> 1.235
2024-12-02-10:54:51-root-INFO: Undo step: 20
2024-12-02-10:54:51-root-INFO: Undo step: 21
2024-12-02-10:54:51-root-INFO: Undo step: 22
2024-12-02-10:54:51-root-INFO: Undo step: 23
2024-12-02-10:54:51-root-INFO: Undo step: 24
2024-12-02-10:54:51-root-INFO: Undo step: 25
2024-12-02-10:54:51-root-INFO: Undo step: 26
2024-12-02-10:54:51-root-INFO: Undo step: 27
2024-12-02-10:54:51-root-INFO: Undo step: 28
2024-12-02-10:54:51-root-INFO: Undo step: 29
2024-12-02-10:54:51-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-10:54:52-root-INFO: grad norm: 19.226 19.134 1.882
2024-12-02-10:54:52-root-INFO: grad norm: 8.927 8.841 1.241
2024-12-02-10:54:52-root-INFO: Loss Change: 163.277 -> 67.733
2024-12-02-10:54:52-root-INFO: Regularization Change: 0.000 -> 81.033
2024-12-02-10:54:52-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-10:54:52-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-10:54:53-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-10:54:53-root-INFO: grad norm: 6.549 6.481 0.937
2024-12-02-10:54:53-root-INFO: grad norm: 5.266 5.212 0.750
2024-12-02-10:54:53-root-INFO: Loss Change: 67.380 -> 51.119
2024-12-02-10:54:53-root-INFO: Regularization Change: 0.000 -> 13.449
2024-12-02-10:54:53-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-10:54:53-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-10:54:54-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-10:54:54-root-INFO: grad norm: 5.085 5.034 0.717
2024-12-02-10:54:54-root-INFO: grad norm: 4.746 4.694 0.701
2024-12-02-10:54:55-root-INFO: Loss Change: 50.757 -> 42.949
2024-12-02-10:54:55-root-INFO: Regularization Change: 0.000 -> 6.874
2024-12-02-10:54:55-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-10:54:55-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-10:54:55-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-10:54:55-root-INFO: grad norm: 5.027 4.969 0.764
2024-12-02-10:54:55-root-INFO: grad norm: 4.838 4.785 0.714
2024-12-02-10:54:56-root-INFO: Loss Change: 42.952 -> 37.955
2024-12-02-10:54:56-root-INFO: Regularization Change: 0.000 -> 4.382
2024-12-02-10:54:56-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-10:54:56-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-10:54:56-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-10:54:56-root-INFO: grad norm: 5.326 5.263 0.816
2024-12-02-10:54:56-root-INFO: grad norm: 4.834 4.776 0.750
2024-12-02-10:54:57-root-INFO: Loss Change: 38.093 -> 33.921
2024-12-02-10:54:57-root-INFO: Regularization Change: 0.000 -> 3.218
2024-12-02-10:54:57-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-10:54:57-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-10:54:57-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-10:54:57-root-INFO: grad norm: 4.871 4.809 0.775
2024-12-02-10:54:58-root-INFO: grad norm: 4.270 4.220 0.648
2024-12-02-10:54:58-root-INFO: Loss Change: 33.877 -> 30.220
2024-12-02-10:54:58-root-INFO: Regularization Change: 0.000 -> 2.545
2024-12-02-10:54:58-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-10:54:58-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-10:54:58-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-10:54:58-root-INFO: grad norm: 4.647 4.592 0.717
2024-12-02-10:54:59-root-INFO: grad norm: 4.035 3.988 0.615
2024-12-02-10:54:59-root-INFO: Loss Change: 30.501 -> 27.392
2024-12-02-10:54:59-root-INFO: Regularization Change: 0.000 -> 2.092
2024-12-02-10:54:59-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-10:54:59-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-10:54:59-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-10:54:59-root-INFO: grad norm: 4.041 3.988 0.655
2024-12-02-10:55:00-root-INFO: grad norm: 3.493 3.452 0.538
2024-12-02-10:55:00-root-INFO: Loss Change: 27.330 -> 24.702
2024-12-02-10:55:00-root-INFO: Regularization Change: 0.000 -> 1.828
2024-12-02-10:55:00-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-10:55:00-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-10:55:00-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-10:55:00-root-INFO: grad norm: 3.933 3.886 0.608
2024-12-02-10:55:01-root-INFO: grad norm: 3.387 3.347 0.519
2024-12-02-10:55:01-root-INFO: Loss Change: 24.852 -> 22.535
2024-12-02-10:55:01-root-INFO: Regularization Change: 0.000 -> 1.618
2024-12-02-10:55:01-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-10:55:01-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-10:55:01-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-10:55:01-root-INFO: grad norm: 3.503 3.459 0.553
2024-12-02-10:55:02-root-INFO: grad norm: 3.059 3.024 0.461
2024-12-02-10:55:02-root-INFO: Loss Change: 22.546 -> 20.521
2024-12-02-10:55:02-root-INFO: Regularization Change: 0.000 -> 1.484
2024-12-02-10:55:02-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-10:55:02-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-10:55:02-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-10:55:03-root-INFO: grad norm: 3.557 3.518 0.531
2024-12-02-10:55:03-root-INFO: grad norm: 3.067 3.034 0.453
2024-12-02-10:55:03-root-INFO: Loss Change: 20.622 -> 18.665
2024-12-02-10:55:03-root-INFO: Regularization Change: 0.000 -> 1.387
2024-12-02-10:55:03-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-10:55:03-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-10:55:03-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-10:55:04-root-INFO: grad norm: 3.203 3.167 0.479
2024-12-02-10:55:04-root-INFO: grad norm: 2.783 2.753 0.408
2024-12-02-10:55:04-root-INFO: Loss Change: 18.688 -> 16.946
2024-12-02-10:55:04-root-INFO: Regularization Change: 0.000 -> 1.307
2024-12-02-10:55:04-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-10:55:04-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-10:55:05-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-10:55:05-root-INFO: grad norm: 3.245 3.213 0.458
2024-12-02-10:55:05-root-INFO: grad norm: 2.796 2.768 0.396
2024-12-02-10:55:06-root-INFO: Loss Change: 17.021 -> 15.325
2024-12-02-10:55:06-root-INFO: Regularization Change: 0.000 -> 1.235
2024-12-02-10:55:06-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-10:55:06-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-10:55:06-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-10:55:06-root-INFO: grad norm: 2.958 2.929 0.414
2024-12-02-10:55:06-root-INFO: grad norm: 2.591 2.567 0.357
2024-12-02-10:55:07-root-INFO: Loss Change: 15.394 -> 13.907
2024-12-02-10:55:07-root-INFO: Regularization Change: 0.000 -> 1.179
2024-12-02-10:55:07-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-10:55:07-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-10:55:07-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-10:55:07-root-INFO: grad norm: 3.056 3.031 0.388
2024-12-02-10:55:07-root-INFO: grad norm: 2.561 2.538 0.346
2024-12-02-10:55:08-root-INFO: Loss Change: 13.992 -> 12.445
2024-12-02-10:55:08-root-INFO: Regularization Change: 0.000 -> 1.153
2024-12-02-10:55:08-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-10:55:08-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-10:55:08-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-10:55:08-root-INFO: grad norm: 2.710 2.685 0.368
2024-12-02-10:55:08-root-INFO: grad norm: 2.311 2.290 0.312
2024-12-02-10:55:09-root-INFO: Loss Change: 12.505 -> 11.119
2024-12-02-10:55:09-root-INFO: Regularization Change: 0.000 -> 1.130
2024-12-02-10:55:09-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-10:55:09-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-10:55:09-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-10:55:09-root-INFO: grad norm: 2.749 2.728 0.344
2024-12-02-10:55:10-root-INFO: grad norm: 2.339 2.320 0.302
2024-12-02-10:55:10-root-INFO: Loss Change: 11.207 -> 9.855
2024-12-02-10:55:10-root-INFO: Regularization Change: 0.000 -> 1.054
2024-12-02-10:55:10-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-10:55:10-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-10:55:10-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-10:55:10-root-INFO: grad norm: 2.560 2.538 0.336
2024-12-02-10:55:11-root-INFO: grad norm: 2.179 2.161 0.283
2024-12-02-10:55:11-root-INFO: Loss Change: 9.970 -> 8.738
2024-12-02-10:55:11-root-INFO: Regularization Change: 0.000 -> 0.993
2024-12-02-10:55:11-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-10:55:11-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-10:55:11-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-10:55:11-root-INFO: grad norm: 2.543 2.523 0.316
2024-12-02-10:55:12-root-INFO: grad norm: 2.116 2.098 0.271
2024-12-02-10:55:12-root-INFO: Loss Change: 8.860 -> 7.601
2024-12-02-10:55:12-root-INFO: Regularization Change: 0.000 -> 0.983
2024-12-02-10:55:12-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-10:55:12-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-10:55:12-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-10:55:12-root-INFO: grad norm: 2.368 2.346 0.317
2024-12-02-10:55:13-root-INFO: grad norm: 2.004 1.988 0.254
2024-12-02-10:55:13-root-INFO: Loss Change: 7.758 -> 6.680
2024-12-02-10:55:13-root-INFO: Regularization Change: 0.000 -> 0.927
2024-12-02-10:55:13-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-10:55:13-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-10:55:13-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-10:55:14-root-INFO: grad norm: 2.511 2.490 0.325
2024-12-02-10:55:14-root-INFO: grad norm: 2.011 1.996 0.244
2024-12-02-10:55:14-root-INFO: Loss Change: 6.883 -> 5.806
2024-12-02-10:55:14-root-INFO: Regularization Change: 0.000 -> 0.924
2024-12-02-10:55:14-root-INFO: Undo step: 10
2024-12-02-10:55:14-root-INFO: Undo step: 11
2024-12-02-10:55:14-root-INFO: Undo step: 12
2024-12-02-10:55:15-root-INFO: Undo step: 13
2024-12-02-10:55:15-root-INFO: Undo step: 14
2024-12-02-10:55:15-root-INFO: Undo step: 15
2024-12-02-10:55:15-root-INFO: Undo step: 16
2024-12-02-10:55:15-root-INFO: Undo step: 17
2024-12-02-10:55:15-root-INFO: Undo step: 18
2024-12-02-10:55:15-root-INFO: Undo step: 19
2024-12-02-10:55:15-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-10:55:15-root-INFO: grad norm: 16.708 16.660 1.273
2024-12-02-10:55:15-root-INFO: grad norm: 7.702 7.656 0.843
2024-12-02-10:55:16-root-INFO: Loss Change: 132.953 -> 44.956
2024-12-02-10:55:16-root-INFO: Regularization Change: 0.000 -> 86.958
2024-12-02-10:55:16-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-10:55:16-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-10:55:16-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-10:55:16-root-INFO: grad norm: 5.739 5.711 0.573
2024-12-02-10:55:16-root-INFO: grad norm: 4.556 4.529 0.495
2024-12-02-10:55:17-root-INFO: Loss Change: 44.228 -> 29.707
2024-12-02-10:55:17-root-INFO: Regularization Change: 0.000 -> 14.455
2024-12-02-10:55:17-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-10:55:17-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-10:55:17-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-10:55:17-root-INFO: grad norm: 4.260 4.237 0.440
2024-12-02-10:55:17-root-INFO: grad norm: 3.714 3.692 0.404
2024-12-02-10:55:18-root-INFO: Loss Change: 29.180 -> 22.429
2024-12-02-10:55:18-root-INFO: Regularization Change: 0.000 -> 6.552
2024-12-02-10:55:18-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-10:55:18-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-10:55:18-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-10:55:18-root-INFO: grad norm: 3.597 3.577 0.377
2024-12-02-10:55:19-root-INFO: grad norm: 3.111 3.093 0.333
2024-12-02-10:55:19-root-INFO: Loss Change: 22.060 -> 17.828
2024-12-02-10:55:19-root-INFO: Regularization Change: 0.000 -> 3.907
2024-12-02-10:55:19-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-10:55:19-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-10:55:19-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-10:55:19-root-INFO: grad norm: 3.168 3.149 0.341
2024-12-02-10:55:20-root-INFO: grad norm: 2.694 2.677 0.302
2024-12-02-10:55:20-root-INFO: Loss Change: 17.526 -> 14.546
2024-12-02-10:55:20-root-INFO: Regularization Change: 0.000 -> 2.675
2024-12-02-10:55:20-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-10:55:20-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-10:55:20-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-10:55:20-root-INFO: grad norm: 2.790 2.770 0.335
2024-12-02-10:55:21-root-INFO: grad norm: 2.383 2.366 0.282
2024-12-02-10:55:21-root-INFO: Loss Change: 14.293 -> 12.087
2024-12-02-10:55:21-root-INFO: Regularization Change: 0.000 -> 1.979
2024-12-02-10:55:21-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-10:55:21-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-10:55:21-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-10:55:21-root-INFO: grad norm: 2.596 2.575 0.329
2024-12-02-10:55:22-root-INFO: grad norm: 2.217 2.200 0.273
2024-12-02-10:55:22-root-INFO: Loss Change: 11.915 -> 10.169
2024-12-02-10:55:22-root-INFO: Regularization Change: 0.000 -> 1.547
2024-12-02-10:55:22-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-10:55:22-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-10:55:22-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-10:55:23-root-INFO: grad norm: 2.436 2.414 0.330
2024-12-02-10:55:23-root-INFO: grad norm: 2.047 2.030 0.264
2024-12-02-10:55:23-root-INFO: Loss Change: 10.078 -> 8.619
2024-12-02-10:55:23-root-INFO: Regularization Change: 0.000 -> 1.274
2024-12-02-10:55:23-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-10:55:23-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-10:55:24-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-10:55:24-root-INFO: grad norm: 2.280 2.257 0.316
2024-12-02-10:55:24-root-INFO: grad norm: 1.924 1.907 0.254
2024-12-02-10:55:24-root-INFO: Loss Change: 8.568 -> 7.351
2024-12-02-10:55:24-root-INFO: Regularization Change: 0.000 -> 1.075
2024-12-02-10:55:24-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-10:55:24-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-10:55:25-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-10:55:25-root-INFO: grad norm: 2.311 2.287 0.336
2024-12-02-10:55:25-root-INFO: grad norm: 1.890 1.873 0.251
2024-12-02-10:55:26-root-INFO: Loss Change: 7.400 -> 6.299
2024-12-02-10:55:26-root-INFO: Regularization Change: 0.000 -> 0.943
2024-12-02-10:55:26-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-10:55:26-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-10:55:26-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-10:55:26-root-INFO: grad norm: 2.190 2.167 0.322
2024-12-02-10:55:26-root-INFO: grad norm: 1.755 1.739 0.236
2024-12-02-10:55:27-root-INFO: Loss Change: 6.393 -> 5.375
2024-12-02-10:55:27-root-INFO: Regularization Change: 0.000 -> 0.849
2024-12-02-10:55:27-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-10:55:27-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-10:55:27-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-10:55:27-root-INFO: grad norm: 2.052 2.031 0.296
2024-12-02-10:55:27-root-INFO: grad norm: 1.798 1.786 0.207
2024-12-02-10:55:28-root-INFO: Loss Change: 5.493 -> 4.671
2024-12-02-10:55:28-root-INFO: Regularization Change: 0.000 -> 0.800
2024-12-02-10:55:28-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-10:55:28-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-10:55:28-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-10:55:28-root-INFO: grad norm: 1.901 1.880 0.283
2024-12-02-10:55:29-root-INFO: grad norm: 1.464 1.450 0.201
2024-12-02-10:55:29-root-INFO: Loss Change: 4.829 -> 3.962
2024-12-02-10:55:29-root-INFO: Regularization Change: 0.000 -> 0.726
2024-12-02-10:55:29-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-10:55:29-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-10:55:29-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-10:55:29-root-INFO: grad norm: 1.762 1.742 0.261
2024-12-02-10:55:30-root-INFO: grad norm: 1.256 1.245 0.166
2024-12-02-10:55:30-root-INFO: Loss Change: 4.136 -> 3.364
2024-12-02-10:55:30-root-INFO: Regularization Change: 0.000 -> 0.639
2024-12-02-10:55:30-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-10:55:30-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-10:55:30-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-10:55:30-root-INFO: grad norm: 1.509 1.495 0.210
2024-12-02-10:55:31-root-INFO: grad norm: 1.100 1.094 0.120
2024-12-02-10:55:31-root-INFO: Loss Change: 3.525 -> 2.946
2024-12-02-10:55:31-root-INFO: Regularization Change: 0.000 -> 0.595
2024-12-02-10:55:31-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-10:55:31-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-10:55:31-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-10:55:31-root-INFO: grad norm: 1.350 1.338 0.177
2024-12-02-10:55:32-root-INFO: grad norm: 0.875 0.871 0.085
2024-12-02-10:55:32-root-INFO: Loss Change: 3.099 -> 2.532
2024-12-02-10:55:32-root-INFO: Regularization Change: 0.000 -> 0.526
2024-12-02-10:55:32-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-10:55:32-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-10:55:32-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-10:55:33-root-INFO: grad norm: 1.222 1.216 0.123
2024-12-02-10:55:33-root-INFO: grad norm: 0.893 0.891 0.060
2024-12-02-10:55:33-root-INFO: Loss Change: 2.655 -> 2.204
2024-12-02-10:55:33-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-02-10:55:33-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-10:55:33-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-10:55:33-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-10:55:34-root-INFO: grad norm: 1.073 1.069 0.093
2024-12-02-10:55:34-root-INFO: grad norm: 0.695 0.693 0.058
2024-12-02-10:55:34-root-INFO: Loss Change: 2.324 -> 1.911
2024-12-02-10:55:35-root-INFO: Regularization Change: 0.000 -> 0.451
2024-12-02-10:55:35-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-10:55:35-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-10:55:35-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-10:55:35-root-INFO: grad norm: 0.915 0.911 0.084
2024-12-02-10:55:35-root-INFO: grad norm: 0.998 0.996 0.063
2024-12-02-10:55:36-root-INFO: Loss Change: 2.033 -> 1.723
2024-12-02-10:55:36-root-INFO: Regularization Change: 0.000 -> 0.696
2024-12-02-10:55:36-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-10:55:36-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-10:55:36-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-10:55:36-root-INFO: grad norm: 1.148 1.144 0.088
2024-12-02-10:55:36-root-INFO: grad norm: 0.482 0.479 0.058
2024-12-02-10:55:37-root-INFO: Loss Change: 1.844 -> 1.403
2024-12-02-10:55:37-root-INFO: Regularization Change: 0.000 -> 0.534
2024-12-02-10:55:37-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-10:55:37-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-10:55:37-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-10:55:37-root-INFO: grad norm: 0.980 0.977 0.081
2024-12-02-10:55:37-root-INFO: grad norm: 0.504 0.502 0.044
2024-12-02-10:55:38-root-INFO: Loss Change: 1.560 -> 1.079
2024-12-02-10:55:38-root-INFO: Regularization Change: 0.000 -> 0.620
2024-12-02-10:55:38-root-INFO: Undo step: 0
2024-12-02-10:55:38-root-INFO: Undo step: 1
2024-12-02-10:55:38-root-INFO: Undo step: 2
2024-12-02-10:55:38-root-INFO: Undo step: 3
2024-12-02-10:55:38-root-INFO: Undo step: 4
2024-12-02-10:55:38-root-INFO: Undo step: 5
2024-12-02-10:55:38-root-INFO: Undo step: 6
2024-12-02-10:55:38-root-INFO: Undo step: 7
2024-12-02-10:55:38-root-INFO: Undo step: 8
2024-12-02-10:55:38-root-INFO: Undo step: 9
2024-12-02-10:55:38-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-10:55:38-root-INFO: grad norm: 15.883 15.856 0.916
2024-12-02-10:55:39-root-INFO: grad norm: 6.588 6.571 0.465
2024-12-02-10:55:39-root-INFO: Loss Change: 92.590 -> 22.167
2024-12-02-10:55:39-root-INFO: Regularization Change: 0.000 -> 90.894
2024-12-02-10:55:39-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-10:55:39-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-10:55:39-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-10:55:39-root-INFO: grad norm: 4.311 4.301 0.292
2024-12-02-10:55:40-root-INFO: grad norm: 2.873 2.861 0.255
2024-12-02-10:55:40-root-INFO: Loss Change: 21.586 -> 11.793
2024-12-02-10:55:40-root-INFO: Regularization Change: 0.000 -> 11.165
2024-12-02-10:55:40-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-10:55:40-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-10:55:40-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-10:55:40-root-INFO: grad norm: 2.644 2.636 0.206
2024-12-02-10:55:41-root-INFO: grad norm: 2.057 2.048 0.188
2024-12-02-10:55:41-root-INFO: Loss Change: 11.408 -> 7.753
2024-12-02-10:55:41-root-INFO: Regularization Change: 0.000 -> 4.318
2024-12-02-10:55:41-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-10:55:41-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-10:55:41-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-10:55:41-root-INFO: grad norm: 2.064 2.055 0.190
2024-12-02-10:55:42-root-INFO: grad norm: 1.658 1.652 0.151
2024-12-02-10:55:42-root-INFO: Loss Change: 7.528 -> 5.406
2024-12-02-10:55:42-root-INFO: Regularization Change: 0.000 -> 2.339
2024-12-02-10:55:42-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-10:55:42-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-10:55:42-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-10:55:43-root-INFO: grad norm: 1.679 1.670 0.174
2024-12-02-10:55:43-root-INFO: grad norm: 1.791 1.786 0.128
2024-12-02-10:55:43-root-INFO: Loss Change: 5.313 -> 4.049
2024-12-02-10:55:43-root-INFO: Regularization Change: 0.000 -> 1.996
2024-12-02-10:55:43-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-10:55:43-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-10:55:44-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-10:55:44-root-INFO: grad norm: 1.819 1.810 0.176
2024-12-02-10:55:44-root-INFO: grad norm: 1.016 1.010 0.103
2024-12-02-10:55:44-root-INFO: Loss Change: 4.068 -> 2.965
2024-12-02-10:55:44-root-INFO: Regularization Change: 0.000 -> 1.344
2024-12-02-10:55:44-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-10:55:45-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-10:55:45-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-10:55:45-root-INFO: grad norm: 1.251 1.243 0.143
2024-12-02-10:55:45-root-INFO: grad norm: 0.958 0.954 0.088
2024-12-02-10:55:46-root-INFO: Loss Change: 3.011 -> 2.410
2024-12-02-10:55:46-root-INFO: Regularization Change: 0.000 -> 0.748
2024-12-02-10:55:46-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-10:55:46-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-10:55:46-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-10:55:46-root-INFO: grad norm: 1.217 1.211 0.123
2024-12-02-10:55:46-root-INFO: grad norm: 0.748 0.745 0.071
2024-12-02-10:55:47-root-INFO: Loss Change: 2.500 -> 1.969
2024-12-02-10:55:47-root-INFO: Regularization Change: 0.000 -> 0.617
2024-12-02-10:55:47-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-10:55:47-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-10:55:47-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-10:55:47-root-INFO: grad norm: 1.526 1.521 0.121
2024-12-02-10:55:47-root-INFO: grad norm: 0.918 0.916 0.065
2024-12-02-10:55:48-root-INFO: Loss Change: 2.082 -> 1.629
2024-12-02-10:55:48-root-INFO: Regularization Change: 0.000 -> 0.663
2024-12-02-10:55:48-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-10:55:48-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-10:55:48-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-10:55:48-root-INFO: grad norm: 0.841 0.836 0.094
2024-12-02-10:55:49-root-INFO: grad norm: 0.474 0.471 0.053
2024-12-02-10:55:49-root-INFO: Loss Change: 1.714 -> 1.413
2024-12-02-10:55:49-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-10:55:49-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-10:55:49-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-10:55:49-root-INFO: loss_sample0_0: 1.4125005006790161
2024-12-02-10:55:49-root-INFO: It takes 652.546 seconds for image sample0
2024-12-02-10:55:49-root-INFO: lpips_score_sample0: 0.142
2024-12-02-10:55:49-root-INFO: psnr_score_sample0: 17.370
2024-12-02-10:55:49-root-INFO: ssim_score_sample0: 0.729
2024-12-02-10:55:49-root-INFO: mean_lpips: 0.14224369823932648
2024-12-02-10:55:49-root-INFO: best_mean_lpips: 0.14224369823932648
2024-12-02-10:55:49-root-INFO: mean_psnr: 17.369522094726562
2024-12-02-10:55:49-root-INFO: best_mean_psnr: 17.369522094726562
2024-12-02-10:55:49-root-INFO: mean_ssim: 0.7285795211791992
2024-12-02-10:55:49-root-INFO: best_mean_ssim: 0.7285795211791992
2024-12-02-10:55:49-root-INFO: final_loss: 1.4125005006790161
2024-12-02-10:55:49-root-INFO: mean time: 652.5459158420563
2024-12-02-10:55:49-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample2_iter2_lr0.03_10009 
 
Enjoy.
