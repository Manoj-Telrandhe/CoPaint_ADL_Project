2024-12-02-09:13:35-root-INFO: Prepare model...
2024-12-02-09:13:51-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-09:14:14-root-INFO: Start sampling
2024-12-02-09:14:19-root-INFO: step: 249 lr_xt 0.00019059
2024-12-02-09:14:19-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-09:14:19-root-INFO: Loss too large (77070.016->78612.750)! Learning rate decreased to 0.00015.
2024-12-02-09:14:20-root-INFO: grad norm: 15661.459 11249.089 10896.756
2024-12-02-09:14:20-root-INFO: grad norm: 14206.317 10378.599 9700.729
2024-12-02-09:14:21-root-INFO: grad norm: 15590.810 11542.585 10480.557
2024-12-02-09:14:21-root-INFO: Loss too large (28302.252->35851.637)! Learning rate decreased to 0.00012.
2024-12-02-09:14:21-root-INFO: grad norm: 16984.281 12430.691 11573.406
2024-12-02-09:14:22-root-INFO: Loss too large (28113.418->29010.053)! Learning rate decreased to 0.00010.
2024-12-02-09:14:22-root-INFO: Loss Change: 77070.016 -> 22715.430
2024-12-02-09:14:22-root-INFO: Regularization Change: 0.000 -> 14.513
2024-12-02-09:14:22-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-09:14:22-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:14:22-root-INFO: step: 248 lr_xt 0.00020082
2024-12-02-09:14:22-root-INFO: grad norm: 13142.824 10133.295 8369.597
2024-12-02-09:14:22-root-INFO: Loss too large (23082.484->43997.363)! Learning rate decreased to 0.00016.
2024-12-02-09:14:23-root-INFO: Loss too large (23082.484->34061.781)! Learning rate decreased to 0.00013.
2024-12-02-09:14:23-root-INFO: Loss too large (23082.484->26817.477)! Learning rate decreased to 0.00010.
2024-12-02-09:14:23-root-INFO: grad norm: 12861.765 10063.230 8009.768
2024-12-02-09:14:24-root-INFO: grad norm: 14410.701 11497.181 8688.104
2024-12-02-09:14:24-root-INFO: Loss too large (22202.750->24117.023)! Learning rate decreased to 0.00008.
2024-12-02-09:14:24-root-INFO: grad norm: 11108.503 8907.727 6637.111
2024-12-02-09:14:25-root-INFO: grad norm: 8510.373 6789.841 5130.742
2024-12-02-09:14:25-root-INFO: Loss Change: 23082.484 -> 17986.518
2024-12-02-09:14:25-root-INFO: Regularization Change: 0.000 -> 1.691
2024-12-02-09:14:25-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-02-09:14:25-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:14:25-root-INFO: step: 247 lr_xt 0.00021156
2024-12-02-09:14:26-root-INFO: grad norm: 6313.963 5134.512 3674.633
2024-12-02-09:14:26-root-INFO: Loss too large (17728.002->26590.205)! Learning rate decreased to 0.00017.
2024-12-02-09:14:26-root-INFO: Loss too large (17728.002->22034.688)! Learning rate decreased to 0.00014.
2024-12-02-09:14:26-root-INFO: Loss too large (17728.002->19380.715)! Learning rate decreased to 0.00011.
2024-12-02-09:14:26-root-INFO: Loss too large (17728.002->17903.371)! Learning rate decreased to 0.00009.
2024-12-02-09:14:27-root-INFO: grad norm: 4645.627 3700.167 2809.022
2024-12-02-09:14:27-root-INFO: grad norm: 3492.412 2848.976 2019.970
2024-12-02-09:14:28-root-INFO: grad norm: 2609.823 2079.627 1576.810
2024-12-02-09:14:28-root-INFO: grad norm: 1994.186 1634.514 1142.429
2024-12-02-09:14:28-root-INFO: Loss Change: 17728.002 -> 16355.934
2024-12-02-09:14:28-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-02-09:14:28-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-02-09:14:28-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:14:29-root-INFO: step: 246 lr_xt 0.00022285
2024-12-02-09:14:29-root-INFO: grad norm: 1555.665 1252.998 922.003
2024-12-02-09:14:29-root-INFO: Loss too large (16122.067->16329.197)! Learning rate decreased to 0.00018.
2024-12-02-09:14:29-root-INFO: Loss too large (16122.067->16170.068)! Learning rate decreased to 0.00014.
2024-12-02-09:14:29-root-INFO: grad norm: 2202.247 1807.557 1258.027
2024-12-02-09:14:30-root-INFO: Loss too large (16084.648->16168.909)! Learning rate decreased to 0.00011.
2024-12-02-09:14:30-root-INFO: grad norm: 2373.746 1930.109 1381.792
2024-12-02-09:14:30-root-INFO: grad norm: 2564.886 2081.673 1498.426
2024-12-02-09:14:31-root-INFO: grad norm: 2784.212 2272.085 1609.182
2024-12-02-09:14:31-root-INFO: Loss Change: 16122.067 -> 15912.993
2024-12-02-09:14:31-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-09:14:31-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-02-09:14:31-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:14:31-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-09:14:32-root-INFO: grad norm: 2944.446 2375.436 1739.846
2024-12-02-09:14:32-root-INFO: Loss too large (15795.152->17528.469)! Learning rate decreased to 0.00019.
2024-12-02-09:14:32-root-INFO: Loss too large (15795.152->16569.773)! Learning rate decreased to 0.00015.
2024-12-02-09:14:32-root-INFO: Loss too large (15795.152->16024.693)! Learning rate decreased to 0.00012.
2024-12-02-09:14:32-root-INFO: grad norm: 3038.541 2490.576 1740.621
2024-12-02-09:14:33-root-INFO: grad norm: 3137.439 2523.057 1864.861
2024-12-02-09:14:33-root-INFO: grad norm: 3244.667 2667.163 1847.729
2024-12-02-09:14:34-root-INFO: grad norm: 3348.835 2687.122 1998.518
2024-12-02-09:14:34-root-INFO: Loss Change: 15795.152 -> 15539.326
2024-12-02-09:14:34-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-02-09:14:34-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-09:14:34-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-09:14:34-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-09:14:35-root-INFO: grad norm: 3646.006 2953.586 2137.683
2024-12-02-09:14:35-root-INFO: Loss too large (15494.031->17981.855)! Learning rate decreased to 0.00020.
2024-12-02-09:14:35-root-INFO: Loss too large (15494.031->16615.686)! Learning rate decreased to 0.00016.
2024-12-02-09:14:35-root-INFO: Loss too large (15494.031->15814.037)! Learning rate decreased to 0.00013.
2024-12-02-09:14:36-root-INFO: grad norm: 3483.784 2816.232 2050.753
2024-12-02-09:14:36-root-INFO: grad norm: 3390.262 2796.378 1916.805
2024-12-02-09:14:36-root-INFO: grad norm: 3335.538 2693.321 1967.699
2024-12-02-09:14:37-root-INFO: grad norm: 3291.281 2732.106 1835.246
2024-12-02-09:14:37-root-INFO: Loss Change: 15494.031 -> 15012.624
2024-12-02-09:14:37-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-02-09:14:37-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-09:14:37-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:14:37-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-09:14:38-root-INFO: grad norm: 3309.368 2649.205 1983.337
2024-12-02-09:14:38-root-INFO: Loss too large (14913.710->17104.891)! Learning rate decreased to 0.00021.
2024-12-02-09:14:38-root-INFO: Loss too large (14913.710->15843.975)! Learning rate decreased to 0.00017.
2024-12-02-09:14:38-root-INFO: Loss too large (14913.710->15133.027)! Learning rate decreased to 0.00013.
2024-12-02-09:14:38-root-INFO: grad norm: 3078.885 2572.146 1692.216
2024-12-02-09:14:39-root-INFO: grad norm: 2924.762 2351.960 1738.538
2024-12-02-09:14:39-root-INFO: grad norm: 2787.036 2344.483 1506.973
2024-12-02-09:14:40-root-INFO: grad norm: 2672.440 2152.213 1584.271
2024-12-02-09:14:40-root-INFO: Loss Change: 14913.710 -> 14353.294
2024-12-02-09:14:40-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-02-09:14:40-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-09:14:40-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:14:40-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-09:14:40-root-INFO: grad norm: 2583.572 2172.711 1397.917
2024-12-02-09:14:41-root-INFO: Loss too large (14156.782->15400.889)! Learning rate decreased to 0.00022.
2024-12-02-09:14:41-root-INFO: Loss too large (14156.782->14672.547)! Learning rate decreased to 0.00018.
2024-12-02-09:14:41-root-INFO: Loss too large (14156.782->14257.943)! Learning rate decreased to 0.00014.
2024-12-02-09:14:41-root-INFO: grad norm: 2379.077 1938.541 1379.155
2024-12-02-09:14:42-root-INFO: grad norm: 2207.917 1880.018 1157.769
2024-12-02-09:14:42-root-INFO: grad norm: 2064.352 1674.969 1206.661
2024-12-02-09:14:43-root-INFO: grad norm: 1934.017 1656.431 998.328
2024-12-02-09:14:43-root-INFO: Loss Change: 14156.782 -> 13641.001
2024-12-02-09:14:43-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-02-09:14:43-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-09:14:43-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:14:43-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-09:14:43-root-INFO: grad norm: 1676.260 1381.486 949.392
2024-12-02-09:14:44-root-INFO: Loss too large (13532.029->13918.896)! Learning rate decreased to 0.00023.
2024-12-02-09:14:44-root-INFO: Loss too large (13532.029->13648.625)! Learning rate decreased to 0.00018.
2024-12-02-09:14:44-root-INFO: grad norm: 2173.385 1856.499 1130.051
2024-12-02-09:14:44-root-INFO: Loss too large (13502.215->13545.515)! Learning rate decreased to 0.00015.
2024-12-02-09:14:45-root-INFO: grad norm: 1972.859 1620.828 1124.761
2024-12-02-09:14:45-root-INFO: grad norm: 1796.804 1546.816 914.257
2024-12-02-09:14:46-root-INFO: grad norm: 1645.780 1354.582 934.719
2024-12-02-09:14:46-root-INFO: Loss Change: 13532.029 -> 13090.648
2024-12-02-09:14:46-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-09:14:46-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-09:14:46-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:14:46-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-09:14:46-root-INFO: grad norm: 1539.800 1325.164 784.171
2024-12-02-09:14:47-root-INFO: Loss too large (12911.068->13151.513)! Learning rate decreased to 0.00024.
2024-12-02-09:14:47-root-INFO: Loss too large (12911.068->12950.585)! Learning rate decreased to 0.00019.
2024-12-02-09:14:47-root-INFO: grad norm: 1884.311 1566.651 1047.011
2024-12-02-09:14:48-root-INFO: grad norm: 2416.627 2072.869 1242.296
2024-12-02-09:14:48-root-INFO: Loss too large (12830.389->12899.901)! Learning rate decreased to 0.00016.
2024-12-02-09:14:48-root-INFO: grad norm: 2125.404 1765.094 1183.970
2024-12-02-09:14:49-root-INFO: grad norm: 1875.118 1619.171 945.703
2024-12-02-09:14:49-root-INFO: Loss Change: 12911.068 -> 12457.693
2024-12-02-09:14:49-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-02-09:14:49-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-09:14:49-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:14:49-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-09:14:49-root-INFO: grad norm: 1696.810 1412.464 940.272
2024-12-02-09:14:49-root-INFO: Loss too large (12398.306->12811.792)! Learning rate decreased to 0.00026.
2024-12-02-09:14:50-root-INFO: Loss too large (12398.306->12515.643)! Learning rate decreased to 0.00020.
2024-12-02-09:14:50-root-INFO: grad norm: 2105.818 1806.950 1081.389
2024-12-02-09:14:50-root-INFO: Loss too large (12355.472->12363.764)! Learning rate decreased to 0.00016.
2024-12-02-09:14:51-root-INFO: grad norm: 1806.061 1511.180 989.036
2024-12-02-09:14:51-root-INFO: grad norm: 1560.905 1353.870 776.826
2024-12-02-09:14:52-root-INFO: grad norm: 1364.780 1148.619 737.088
2024-12-02-09:14:52-root-INFO: Loss Change: 12398.306 -> 11875.730
2024-12-02-09:14:52-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-02-09:14:52-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-09:14:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:14:52-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-09:14:52-root-INFO: grad norm: 1245.086 1053.750 663.212
2024-12-02-09:14:53-root-INFO: grad norm: 2342.257 1951.828 1294.809
2024-12-02-09:14:53-root-INFO: Loss too large (11643.613->12690.107)! Learning rate decreased to 0.00027.
2024-12-02-09:14:53-root-INFO: Loss too large (11643.613->12018.354)! Learning rate decreased to 0.00021.
2024-12-02-09:14:53-root-INFO: Loss too large (11643.613->11646.822)! Learning rate decreased to 0.00017.
2024-12-02-09:14:54-root-INFO: grad norm: 1885.665 1633.836 941.442
2024-12-02-09:14:54-root-INFO: grad norm: 1563.750 1325.822 829.163
2024-12-02-09:14:55-root-INFO: grad norm: 1320.471 1162.062 627.102
2024-12-02-09:14:55-root-INFO: Loss Change: 11652.668 -> 11084.258
2024-12-02-09:14:55-root-INFO: Regularization Change: 0.000 -> 0.599
2024-12-02-09:14:55-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-09:14:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:14:55-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-09:14:55-root-INFO: grad norm: 1277.823 1091.982 663.631
2024-12-02-09:14:55-root-INFO: Loss too large (11024.975->11110.390)! Learning rate decreased to 0.00028.
2024-12-02-09:14:56-root-INFO: grad norm: 1957.503 1711.384 950.254
2024-12-02-09:14:56-root-INFO: Loss too large (10987.291->11204.156)! Learning rate decreased to 0.00023.
2024-12-02-09:14:57-root-INFO: grad norm: 2310.938 1980.281 1191.184
2024-12-02-09:14:57-root-INFO: Loss too large (10954.085->10956.350)! Learning rate decreased to 0.00018.
2024-12-02-09:14:57-root-INFO: grad norm: 1825.514 1602.257 874.800
2024-12-02-09:14:58-root-INFO: grad norm: 1471.539 1267.013 748.401
2024-12-02-09:14:58-root-INFO: Loss Change: 11024.975 -> 10497.371
2024-12-02-09:14:58-root-INFO: Regularization Change: 0.000 -> 0.593
2024-12-02-09:14:58-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-09:14:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:14:58-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-09:14:58-root-INFO: grad norm: 1217.096 1063.518 591.821
2024-12-02-09:14:58-root-INFO: Loss too large (10379.547->10459.265)! Learning rate decreased to 0.00030.
2024-12-02-09:14:59-root-INFO: grad norm: 1830.807 1584.264 917.584
2024-12-02-09:14:59-root-INFO: Loss too large (10343.053->10510.791)! Learning rate decreased to 0.00024.
2024-12-02-09:15:00-root-INFO: grad norm: 2087.044 1825.170 1012.178
2024-12-02-09:15:00-root-INFO: grad norm: 2376.895 2051.774 1199.940
2024-12-02-09:15:00-root-INFO: grad norm: 2707.952 2361.776 1324.772
2024-12-02-09:15:01-root-INFO: Loss too large (10254.022->10280.080)! Learning rate decreased to 0.00019.
2024-12-02-09:15:01-root-INFO: Loss Change: 10379.547 -> 9993.547
2024-12-02-09:15:01-root-INFO: Regularization Change: 0.000 -> 0.744
2024-12-02-09:15:01-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-09:15:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:15:01-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-09:15:01-root-INFO: grad norm: 2137.563 1859.807 1053.706
2024-12-02-09:15:01-root-INFO: Loss too large (9925.225->10810.632)! Learning rate decreased to 0.00031.
2024-12-02-09:15:01-root-INFO: Loss too large (9925.225->10206.730)! Learning rate decreased to 0.00025.
2024-12-02-09:15:02-root-INFO: grad norm: 2339.310 2065.329 1098.540
2024-12-02-09:15:02-root-INFO: grad norm: 2560.032 2225.663 1264.986
2024-12-02-09:15:03-root-INFO: grad norm: 2798.139 2464.276 1325.491
2024-12-02-09:15:03-root-INFO: Loss too large (9833.137->9838.982)! Learning rate decreased to 0.00020.
2024-12-02-09:15:04-root-INFO: grad norm: 1949.209 1694.236 963.836
2024-12-02-09:15:04-root-INFO: Loss Change: 9925.225 -> 9345.403
2024-12-02-09:15:04-root-INFO: Regularization Change: 0.000 -> 0.648
2024-12-02-09:15:04-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-09:15:04-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-09:15:04-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-09:15:04-root-INFO: grad norm: 1327.067 1179.421 608.336
2024-12-02-09:15:04-root-INFO: Loss too large (9276.199->9495.654)! Learning rate decreased to 0.00033.
2024-12-02-09:15:04-root-INFO: Loss too large (9276.199->9302.016)! Learning rate decreased to 0.00026.
2024-12-02-09:15:05-root-INFO: grad norm: 1380.591 1207.271 669.722
2024-12-02-09:15:05-root-INFO: grad norm: 1437.112 1281.475 650.472
2024-12-02-09:15:06-root-INFO: grad norm: 1491.795 1302.630 727.054
2024-12-02-09:15:06-root-INFO: grad norm: 1543.928 1374.565 703.055
2024-12-02-09:15:07-root-INFO: Loss Change: 9276.199 -> 8935.686
2024-12-02-09:15:07-root-INFO: Regularization Change: 0.000 -> 0.645
2024-12-02-09:15:07-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-09:15:07-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:15:07-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-09:15:07-root-INFO: grad norm: 1618.352 1432.792 752.444
2024-12-02-09:15:07-root-INFO: Loss too large (8897.591->9309.715)! Learning rate decreased to 0.00035.
2024-12-02-09:15:07-root-INFO: Loss too large (8897.591->8980.188)! Learning rate decreased to 0.00028.
2024-12-02-09:15:08-root-INFO: grad norm: 1574.738 1401.658 717.744
2024-12-02-09:15:08-root-INFO: grad norm: 1544.594 1356.945 737.882
2024-12-02-09:15:09-root-INFO: grad norm: 1514.564 1352.632 681.387
2024-12-02-09:15:09-root-INFO: grad norm: 1480.379 1299.559 708.990
2024-12-02-09:15:09-root-INFO: Loss Change: 8897.591 -> 8505.337
2024-12-02-09:15:09-root-INFO: Regularization Change: 0.000 -> 0.658
2024-12-02-09:15:09-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-09:15:09-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:15:10-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-09:15:10-root-INFO: grad norm: 1109.857 993.959 493.789
2024-12-02-09:15:10-root-INFO: Loss too large (8280.650->8377.847)! Learning rate decreased to 0.00036.
2024-12-02-09:15:10-root-INFO: grad norm: 1431.102 1259.616 679.279
2024-12-02-09:15:10-root-INFO: Loss too large (8253.785->8307.815)! Learning rate decreased to 0.00029.
2024-12-02-09:15:11-root-INFO: grad norm: 1333.846 1207.052 567.602
2024-12-02-09:15:11-root-INFO: grad norm: 1249.740 1106.280 581.372
2024-12-02-09:15:12-root-INFO: grad norm: 1170.456 1061.241 493.696
2024-12-02-09:15:12-root-INFO: Loss Change: 8280.650 -> 7951.441
2024-12-02-09:15:12-root-INFO: Regularization Change: 0.000 -> 0.592
2024-12-02-09:15:12-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-09:15:12-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:15:12-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-09:15:13-root-INFO: grad norm: 1143.106 1014.742 526.299
2024-12-02-09:15:13-root-INFO: Loss too large (7892.605->8057.095)! Learning rate decreased to 0.00038.
2024-12-02-09:15:13-root-INFO: Loss too large (7892.605->7898.393)! Learning rate decreased to 0.00030.
2024-12-02-09:15:13-root-INFO: grad norm: 1039.962 945.774 432.472
2024-12-02-09:15:14-root-INFO: grad norm: 947.852 842.708 433.898
2024-12-02-09:15:14-root-INFO: grad norm: 863.436 789.632 349.289
2024-12-02-09:15:15-root-INFO: grad norm: 787.925 702.869 356.092
2024-12-02-09:15:15-root-INFO: Loss Change: 7892.605 -> 7559.095
2024-12-02-09:15:15-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-09:15:15-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-09:15:15-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:15:15-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-09:15:15-root-INFO: grad norm: 542.323 507.701 190.669
2024-12-02-09:15:16-root-INFO: grad norm: 777.105 693.646 350.354
2024-12-02-09:15:16-root-INFO: Loss too large (7390.077->7412.262)! Learning rate decreased to 0.00040.
2024-12-02-09:15:16-root-INFO: grad norm: 959.776 876.433 391.198
2024-12-02-09:15:17-root-INFO: grad norm: 1209.951 1079.316 546.861
2024-12-02-09:15:17-root-INFO: Loss too large (7345.125->7368.701)! Learning rate decreased to 0.00032.
2024-12-02-09:15:17-root-INFO: grad norm: 1041.093 947.744 430.878
2024-12-02-09:15:18-root-INFO: Loss Change: 7444.280 -> 7199.524
2024-12-02-09:15:18-root-INFO: Regularization Change: 0.000 -> 0.615
2024-12-02-09:15:18-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-09:15:18-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:15:18-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-09:15:18-root-INFO: grad norm: 1092.350 980.421 481.668
2024-12-02-09:15:18-root-INFO: Loss too large (7150.934->7308.469)! Learning rate decreased to 0.00042.
2024-12-02-09:15:18-root-INFO: grad norm: 1355.258 1239.558 547.924
2024-12-02-09:15:19-root-INFO: Loss too large (7149.869->7200.714)! Learning rate decreased to 0.00034.
2024-12-02-09:15:19-root-INFO: grad norm: 1137.028 1021.623 499.119
2024-12-02-09:15:20-root-INFO: grad norm: 956.062 876.538 381.753
2024-12-02-09:15:20-root-INFO: grad norm: 813.979 733.166 353.595
2024-12-02-09:15:20-root-INFO: Loss Change: 7150.934 -> 6867.462
2024-12-02-09:15:20-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-09:15:20-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-09:15:20-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:15:21-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-09:15:21-root-INFO: grad norm: 571.561 517.819 241.961
2024-12-02-09:15:21-root-INFO: grad norm: 862.571 770.721 387.321
2024-12-02-09:15:21-root-INFO: Loss too large (6785.961->6866.360)! Learning rate decreased to 0.00044.
2024-12-02-09:15:22-root-INFO: grad norm: 1031.941 938.789 428.460
2024-12-02-09:15:22-root-INFO: Loss too large (6771.335->6780.937)! Learning rate decreased to 0.00035.
2024-12-02-09:15:22-root-INFO: grad norm: 843.689 761.708 362.784
2024-12-02-09:15:23-root-INFO: grad norm: 700.113 643.712 275.307
2024-12-02-09:15:23-root-INFO: Loss Change: 6803.661 -> 6601.152
2024-12-02-09:15:23-root-INFO: Regularization Change: 0.000 -> 0.461
2024-12-02-09:15:23-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-09:15:23-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:15:23-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-09:15:23-root-INFO: grad norm: 914.145 824.767 394.233
2024-12-02-09:15:24-root-INFO: Loss too large (6638.871->6730.228)! Learning rate decreased to 0.00046.
2024-12-02-09:15:24-root-INFO: grad norm: 1074.596 987.152 424.604
2024-12-02-09:15:24-root-INFO: Loss too large (6618.748->6632.225)! Learning rate decreased to 0.00037.
2024-12-02-09:15:25-root-INFO: grad norm: 863.132 784.437 360.077
2024-12-02-09:15:25-root-INFO: grad norm: 700.919 644.995 274.352
2024-12-02-09:15:26-root-INFO: grad norm: 581.282 530.058 238.594
2024-12-02-09:15:26-root-INFO: Loss Change: 6638.871 -> 6393.909
2024-12-02-09:15:26-root-INFO: Regularization Change: 0.000 -> 0.427
2024-12-02-09:15:26-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-09:15:26-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-09:15:26-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-09:15:26-root-INFO: grad norm: 515.738 465.456 222.117
2024-12-02-09:15:27-root-INFO: grad norm: 723.676 657.899 301.456
2024-12-02-09:15:27-root-INFO: Loss too large (6314.322->6361.558)! Learning rate decreased to 0.00049.
2024-12-02-09:15:27-root-INFO: grad norm: 827.914 759.384 329.813
2024-12-02-09:15:28-root-INFO: grad norm: 973.084 885.310 403.880
2024-12-02-09:15:28-root-INFO: Loss too large (6286.935->6294.044)! Learning rate decreased to 0.00039.
2024-12-02-09:15:28-root-INFO: grad norm: 760.088 697.736 301.493
2024-12-02-09:15:29-root-INFO: Loss Change: 6335.594 -> 6167.308
2024-12-02-09:15:29-root-INFO: Regularization Change: 0.000 -> 0.469
2024-12-02-09:15:29-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-09:15:29-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:15:29-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-09:15:29-root-INFO: grad norm: 923.178 839.618 383.798
2024-12-02-09:15:29-root-INFO: Loss too large (6160.254->6275.759)! Learning rate decreased to 0.00051.
2024-12-02-09:15:30-root-INFO: grad norm: 1066.328 982.266 414.982
2024-12-02-09:15:30-root-INFO: Loss too large (6145.215->6166.277)! Learning rate decreased to 0.00041.
2024-12-02-09:15:30-root-INFO: grad norm: 833.390 763.196 334.770
2024-12-02-09:15:31-root-INFO: grad norm: 656.524 604.744 255.556
2024-12-02-09:15:31-root-INFO: grad norm: 529.888 486.634 209.688
2024-12-02-09:15:32-root-INFO: Loss Change: 6160.254 -> 5927.676
2024-12-02-09:15:32-root-INFO: Regularization Change: 0.000 -> 0.416
2024-12-02-09:15:32-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-09:15:32-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:15:32-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-09:15:32-root-INFO: grad norm: 346.512 313.555 147.492
2024-12-02-09:15:33-root-INFO: grad norm: 386.212 352.579 157.632
2024-12-02-09:15:33-root-INFO: grad norm: 556.356 512.497 216.515
2024-12-02-09:15:33-root-INFO: Loss too large (5815.123->5840.518)! Learning rate decreased to 0.00054.
2024-12-02-09:15:34-root-INFO: grad norm: 657.510 608.548 248.975
2024-12-02-09:15:34-root-INFO: grad norm: 794.603 732.904 306.996
2024-12-02-09:15:34-root-INFO: Loss too large (5790.688->5798.659)! Learning rate decreased to 0.00043.
2024-12-02-09:15:35-root-INFO: Loss Change: 5884.827 -> 5741.875
2024-12-02-09:15:35-root-INFO: Regularization Change: 0.000 -> 0.548
2024-12-02-09:15:35-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-09:15:35-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:15:35-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-09:15:35-root-INFO: grad norm: 862.644 796.629 330.965
2024-12-02-09:15:35-root-INFO: Loss too large (5743.268->5882.052)! Learning rate decreased to 0.00056.
2024-12-02-09:15:35-root-INFO: Loss too large (5743.268->5746.988)! Learning rate decreased to 0.00045.
2024-12-02-09:15:36-root-INFO: grad norm: 689.115 637.459 261.774
2024-12-02-09:15:36-root-INFO: grad norm: 565.595 525.366 209.495
2024-12-02-09:15:37-root-INFO: grad norm: 470.856 434.534 181.343
2024-12-02-09:15:37-root-INFO: grad norm: 401.271 373.741 146.068
2024-12-02-09:15:37-root-INFO: Loss Change: 5743.268 -> 5535.210
2024-12-02-09:15:37-root-INFO: Regularization Change: 0.000 -> 0.369
2024-12-02-09:15:37-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-09:15:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:15:38-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-09:15:38-root-INFO: grad norm: 299.685 273.791 121.858
2024-12-02-09:15:38-root-INFO: grad norm: 398.698 372.407 142.384
2024-12-02-09:15:39-root-INFO: grad norm: 630.794 583.664 239.244
2024-12-02-09:15:39-root-INFO: Loss too large (5432.133->5500.107)! Learning rate decreased to 0.00059.
2024-12-02-09:15:39-root-INFO: grad norm: 763.778 712.864 274.191
2024-12-02-09:15:39-root-INFO: Loss too large (5428.921->5438.774)! Learning rate decreased to 0.00047.
2024-12-02-09:15:40-root-INFO: grad norm: 619.405 573.824 233.213
2024-12-02-09:15:40-root-INFO: Loss Change: 5466.222 -> 5342.021
2024-12-02-09:15:40-root-INFO: Regularization Change: 0.000 -> 0.475
2024-12-02-09:15:40-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-09:15:40-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:15:40-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-09:15:41-root-INFO: grad norm: 607.612 565.252 222.896
2024-12-02-09:15:41-root-INFO: Loss too large (5308.884->5366.543)! Learning rate decreased to 0.00062.
2024-12-02-09:15:41-root-INFO: grad norm: 718.047 666.596 266.911
2024-12-02-09:15:41-root-INFO: Loss too large (5299.657->5304.567)! Learning rate decreased to 0.00050.
2024-12-02-09:15:42-root-INFO: grad norm: 573.124 534.879 205.855
2024-12-02-09:15:42-root-INFO: grad norm: 464.144 430.618 173.200
2024-12-02-09:15:43-root-INFO: grad norm: 384.786 359.698 136.667
2024-12-02-09:15:43-root-INFO: Loss Change: 5308.884 -> 5156.962
2024-12-02-09:15:43-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-02-09:15:43-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-09:15:43-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:15:43-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-09:15:43-root-INFO: grad norm: 286.919 265.107 109.731
2024-12-02-09:15:44-root-INFO: grad norm: 243.945 221.872 101.401
2024-12-02-09:15:44-root-INFO: grad norm: 274.517 257.591 94.902
2024-12-02-09:15:45-root-INFO: grad norm: 388.312 359.644 146.431
2024-12-02-09:15:45-root-INFO: Loss too large (5018.451->5024.946)! Learning rate decreased to 0.00065.
2024-12-02-09:15:46-root-INFO: grad norm: 469.939 441.857 160.015
2024-12-02-09:15:46-root-INFO: Loss Change: 5125.861 -> 4994.967
2024-12-02-09:15:46-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-02-09:15:46-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-09:15:46-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:15:46-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-09:15:46-root-INFO: grad norm: 553.712 513.105 208.135
2024-12-02-09:15:46-root-INFO: Loss too large (4967.406->5025.518)! Learning rate decreased to 0.00068.
2024-12-02-09:15:47-root-INFO: grad norm: 669.734 628.272 231.987
2024-12-02-09:15:47-root-INFO: Loss too large (4963.202->4971.968)! Learning rate decreased to 0.00055.
2024-12-02-09:15:47-root-INFO: grad norm: 545.682 508.830 197.130
2024-12-02-09:15:48-root-INFO: grad norm: 451.377 423.572 155.975
2024-12-02-09:15:48-root-INFO: grad norm: 378.746 352.525 138.472
2024-12-02-09:15:49-root-INFO: Loss Change: 4967.406 -> 4832.438
2024-12-02-09:15:49-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-02-09:15:49-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-09:15:49-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-09:15:49-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-09:15:49-root-INFO: grad norm: 499.811 465.882 181.011
2024-12-02-09:15:49-root-INFO: Loss too large (4824.207->4861.047)! Learning rate decreased to 0.00071.
2024-12-02-09:15:50-root-INFO: grad norm: 598.820 559.316 213.894
2024-12-02-09:15:50-root-INFO: Loss too large (4811.867->4816.078)! Learning rate decreased to 0.00057.
2024-12-02-09:15:50-root-INFO: grad norm: 494.448 464.361 169.848
2024-12-02-09:15:51-root-INFO: grad norm: 412.630 385.113 148.161
2024-12-02-09:15:51-root-INFO: grad norm: 350.597 329.856 118.800
2024-12-02-09:15:52-root-INFO: Loss Change: 4824.207 -> 4692.021
2024-12-02-09:15:52-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-09:15:52-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-09:15:52-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:15:52-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-09:15:52-root-INFO: grad norm: 247.952 219.631 115.074
2024-12-02-09:15:53-root-INFO: grad norm: 229.065 210.582 90.143
2024-12-02-09:15:53-root-INFO: grad norm: 283.980 263.064 106.968
2024-12-02-09:15:54-root-INFO: grad norm: 440.178 415.254 146.015
2024-12-02-09:15:54-root-INFO: Loss too large (4586.037->4623.092)! Learning rate decreased to 0.00075.
2024-12-02-09:15:54-root-INFO: grad norm: 547.091 511.430 194.289
2024-12-02-09:15:54-root-INFO: Loss too large (4580.759->4587.721)! Learning rate decreased to 0.00060.
2024-12-02-09:15:55-root-INFO: Loss Change: 4663.039 -> 4549.417
2024-12-02-09:15:55-root-INFO: Regularization Change: 0.000 -> 0.605
2024-12-02-09:15:55-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-09:15:55-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:15:55-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-09:15:55-root-INFO: grad norm: 669.998 624.982 241.444
2024-12-02-09:15:55-root-INFO: Loss too large (4563.025->4689.824)! Learning rate decreased to 0.00079.
2024-12-02-09:15:55-root-INFO: Loss too large (4563.025->4573.415)! Learning rate decreased to 0.00063.
2024-12-02-09:15:56-root-INFO: grad norm: 555.001 519.525 195.244
2024-12-02-09:15:56-root-INFO: grad norm: 473.205 445.997 158.145
2024-12-02-09:15:57-root-INFO: grad norm: 407.141 380.788 144.099
2024-12-02-09:15:57-root-INFO: grad norm: 354.080 334.363 116.507
2024-12-02-09:15:57-root-INFO: Loss Change: 4563.025 -> 4401.912
2024-12-02-09:15:57-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-02-09:15:57-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-09:15:57-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:15:58-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-09:15:58-root-INFO: grad norm: 278.602 260.723 98.195
2024-12-02-09:15:58-root-INFO: grad norm: 437.949 413.789 143.450
2024-12-02-09:15:58-root-INFO: Loss too large (4374.518->4417.491)! Learning rate decreased to 0.00082.
2024-12-02-09:15:59-root-INFO: grad norm: 537.669 504.008 187.256
2024-12-02-09:15:59-root-INFO: Loss too large (4370.675->4377.284)! Learning rate decreased to 0.00066.
2024-12-02-09:16:00-root-INFO: grad norm: 443.387 418.923 145.243
2024-12-02-09:16:00-root-INFO: grad norm: 368.534 345.184 129.095
2024-12-02-09:16:00-root-INFO: Loss Change: 4382.283 -> 4285.926
2024-12-02-09:16:00-root-INFO: Regularization Change: 0.000 -> 0.419
2024-12-02-09:16:00-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-09:16:00-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:16:00-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-09:16:01-root-INFO: grad norm: 449.954 419.229 163.419
2024-12-02-09:16:01-root-INFO: Loss too large (4272.594->4311.767)! Learning rate decreased to 0.00086.
2024-12-02-09:16:01-root-INFO: grad norm: 528.234 494.656 185.329
2024-12-02-09:16:01-root-INFO: Loss too large (4262.605->4266.897)! Learning rate decreased to 0.00069.
2024-12-02-09:16:02-root-INFO: grad norm: 425.609 402.187 139.243
2024-12-02-09:16:02-root-INFO: grad norm: 346.920 325.198 120.828
2024-12-02-09:16:03-root-INFO: grad norm: 287.493 272.158 92.639
2024-12-02-09:16:03-root-INFO: Loss Change: 4272.594 -> 4156.396
2024-12-02-09:16:03-root-INFO: Regularization Change: 0.000 -> 0.379
2024-12-02-09:16:03-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-09:16:03-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:16:03-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-09:16:04-root-INFO: grad norm: 200.003 178.909 89.404
2024-12-02-09:16:04-root-INFO: grad norm: 160.737 148.931 60.463
2024-12-02-09:16:04-root-INFO: grad norm: 151.811 140.612 57.227
2024-12-02-09:16:05-root-INFO: grad norm: 150.178 139.908 54.583
2024-12-02-09:16:05-root-INFO: grad norm: 159.594 149.335 56.297
2024-12-02-09:16:06-root-INFO: Loss Change: 4136.374 -> 4012.137
2024-12-02-09:16:06-root-INFO: Regularization Change: 0.000 -> 0.708
2024-12-02-09:16:06-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-09:16:06-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:16:06-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-09:16:06-root-INFO: grad norm: 189.459 177.369 66.594
2024-12-02-09:16:06-root-INFO: grad norm: 263.604 249.526 84.994
2024-12-02-09:16:07-root-INFO: Loss too large (3974.819->3977.991)! Learning rate decreased to 0.00095.
2024-12-02-09:16:07-root-INFO: grad norm: 304.950 285.947 105.965
2024-12-02-09:16:08-root-INFO: grad norm: 360.341 342.701 111.364
2024-12-02-09:16:08-root-INFO: grad norm: 426.607 400.686 146.440
2024-12-02-09:16:08-root-INFO: Loss too large (3952.512->3953.294)! Learning rate decreased to 0.00076.
2024-12-02-09:16:08-root-INFO: Loss Change: 3988.804 -> 3925.273
2024-12-02-09:16:08-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-02-09:16:08-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-09:16:08-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-09:16:09-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-09:16:09-root-INFO: grad norm: 495.355 460.811 181.741
2024-12-02-09:16:09-root-INFO: Loss too large (3927.740->3988.396)! Learning rate decreased to 0.00099.
2024-12-02-09:16:09-root-INFO: grad norm: 553.055 517.924 193.969
2024-12-02-09:16:10-root-INFO: Loss too large (3917.858->3927.980)! Learning rate decreased to 0.00079.
2024-12-02-09:16:10-root-INFO: grad norm: 426.875 405.490 133.418
2024-12-02-09:16:11-root-INFO: grad norm: 336.979 316.688 115.168
2024-12-02-09:16:11-root-INFO: grad norm: 268.966 255.876 82.885
2024-12-02-09:16:11-root-INFO: Loss Change: 3927.740 -> 3810.365
2024-12-02-09:16:11-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-02-09:16:11-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-09:16:11-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:16:11-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-09:16:12-root-INFO: grad norm: 153.850 143.740 54.852
2024-12-02-09:16:12-root-INFO: grad norm: 193.421 183.148 62.198
2024-12-02-09:16:13-root-INFO: grad norm: 287.180 270.997 95.043
2024-12-02-09:16:13-root-INFO: Loss too large (3766.447->3782.351)! Learning rate decreased to 0.00104.
2024-12-02-09:16:13-root-INFO: grad norm: 331.738 315.977 101.039
2024-12-02-09:16:14-root-INFO: grad norm: 382.367 360.988 126.065
2024-12-02-09:16:14-root-INFO: Loss Change: 3787.856 -> 3754.415
2024-12-02-09:16:14-root-INFO: Regularization Change: 0.000 -> 0.487
2024-12-02-09:16:14-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-09:16:14-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:16:14-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-09:16:14-root-INFO: grad norm: 512.289 483.703 168.736
2024-12-02-09:16:15-root-INFO: Loss too large (3745.979->3834.052)! Learning rate decreased to 0.00109.
2024-12-02-09:16:15-root-INFO: Loss too large (3745.979->3747.182)! Learning rate decreased to 0.00087.
2024-12-02-09:16:15-root-INFO: grad norm: 370.559 350.570 120.059
2024-12-02-09:16:16-root-INFO: grad norm: 276.422 262.503 86.610
2024-12-02-09:16:16-root-INFO: grad norm: 217.080 205.437 70.138
2024-12-02-09:16:17-root-INFO: grad norm: 176.021 166.971 55.716
2024-12-02-09:16:17-root-INFO: Loss Change: 3745.979 -> 3630.734
2024-12-02-09:16:17-root-INFO: Regularization Change: 0.000 -> 0.330
2024-12-02-09:16:17-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-09:16:17-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:16:17-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-09:16:17-root-INFO: grad norm: 155.810 141.071 66.150
2024-12-02-09:16:18-root-INFO: grad norm: 140.750 131.632 49.836
2024-12-02-09:16:18-root-INFO: grad norm: 164.998 157.005 50.731
2024-12-02-09:16:19-root-INFO: grad norm: 237.197 225.736 72.841
2024-12-02-09:16:19-root-INFO: Loss too large (3577.421->3587.828)! Learning rate decreased to 0.00114.
2024-12-02-09:16:19-root-INFO: grad norm: 276.793 263.482 84.803
2024-12-02-09:16:20-root-INFO: Loss Change: 3621.384 -> 3568.701
2024-12-02-09:16:20-root-INFO: Regularization Change: 0.000 -> 0.519
2024-12-02-09:16:20-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-09:16:20-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:16:20-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-09:16:20-root-INFO: grad norm: 368.637 349.264 117.932
2024-12-02-09:16:20-root-INFO: Loss too large (3551.100->3603.373)! Learning rate decreased to 0.00120.
2024-12-02-09:16:20-root-INFO: Loss too large (3551.100->3553.754)! Learning rate decreased to 0.00096.
2024-12-02-09:16:21-root-INFO: grad norm: 287.126 273.980 85.888
2024-12-02-09:16:21-root-INFO: grad norm: 227.282 215.865 71.130
2024-12-02-09:16:22-root-INFO: grad norm: 189.176 180.540 56.504
2024-12-02-09:16:22-root-INFO: grad norm: 161.029 152.690 51.148
2024-12-02-09:16:22-root-INFO: Loss Change: 3551.100 -> 3473.324
2024-12-02-09:16:22-root-INFO: Regularization Change: 0.000 -> 0.285
2024-12-02-09:16:22-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-09:16:22-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:16:23-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-09:16:23-root-INFO: grad norm: 139.111 130.726 47.565
2024-12-02-09:16:23-root-INFO: grad norm: 178.060 169.277 55.231
2024-12-02-09:16:24-root-INFO: grad norm: 284.437 272.479 81.605
2024-12-02-09:16:24-root-INFO: Loss too large (3436.644->3471.790)! Learning rate decreased to 0.00126.
2024-12-02-09:16:24-root-INFO: Loss too large (3436.644->3438.098)! Learning rate decreased to 0.00101.
2024-12-02-09:16:25-root-INFO: grad norm: 244.519 233.210 73.504
2024-12-02-09:16:25-root-INFO: grad norm: 218.351 208.520 64.782
2024-12-02-09:16:25-root-INFO: Loss Change: 3449.323 -> 3397.972
2024-12-02-09:16:25-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-09:16:25-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-09:16:25-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:16:26-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-09:16:26-root-INFO: grad norm: 249.487 235.366 82.743
2024-12-02-09:16:26-root-INFO: Loss too large (3388.857->3406.991)! Learning rate decreased to 0.00132.
2024-12-02-09:16:26-root-INFO: grad norm: 305.843 292.003 90.961
2024-12-02-09:16:26-root-INFO: Loss too large (3384.331->3391.809)! Learning rate decreased to 0.00105.
2024-12-02-09:16:27-root-INFO: grad norm: 275.824 263.143 82.673
2024-12-02-09:16:27-root-INFO: grad norm: 254.345 243.405 73.794
2024-12-02-09:16:28-root-INFO: grad norm: 235.675 224.663 71.200
2024-12-02-09:16:28-root-INFO: Loss Change: 3388.857 -> 3335.771
2024-12-02-09:16:28-root-INFO: Regularization Change: 0.000 -> 0.326
2024-12-02-09:16:28-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-09:16:28-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-09:16:28-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-09:16:29-root-INFO: grad norm: 223.479 209.554 77.654
2024-12-02-09:16:29-root-INFO: Loss too large (3327.959->3337.078)! Learning rate decreased to 0.00138.
2024-12-02-09:16:29-root-INFO: grad norm: 282.158 268.028 88.171
2024-12-02-09:16:29-root-INFO: Loss too large (3319.517->3329.756)! Learning rate decreased to 0.00110.
2024-12-02-09:16:30-root-INFO: grad norm: 272.408 260.723 78.930
2024-12-02-09:16:30-root-INFO: grad norm: 267.049 254.248 81.690
2024-12-02-09:16:31-root-INFO: grad norm: 265.252 253.871 76.862
2024-12-02-09:16:31-root-INFO: Loss Change: 3327.959 -> 3279.622
2024-12-02-09:16:31-root-INFO: Regularization Change: 0.000 -> 0.362
2024-12-02-09:16:31-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-09:16:31-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:16:31-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-09:16:31-root-INFO: grad norm: 251.649 239.610 76.903
2024-12-02-09:16:32-root-INFO: Loss too large (3263.979->3301.622)! Learning rate decreased to 0.00144.
2024-12-02-09:16:32-root-INFO: Loss too large (3263.979->3270.701)! Learning rate decreased to 0.00115.
2024-12-02-09:16:32-root-INFO: grad norm: 244.985 234.661 70.371
2024-12-02-09:16:33-root-INFO: grad norm: 243.422 232.736 71.332
2024-12-02-09:16:33-root-INFO: grad norm: 244.862 234.521 70.408
2024-12-02-09:16:34-root-INFO: grad norm: 248.977 238.138 72.662
2024-12-02-09:16:34-root-INFO: Loss Change: 3263.979 -> 3219.265
2024-12-02-09:16:34-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-02-09:16:34-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-09:16:34-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:16:34-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-09:16:34-root-INFO: grad norm: 301.228 281.685 106.731
2024-12-02-09:16:34-root-INFO: Loss too large (3211.865->3264.427)! Learning rate decreased to 0.00150.
2024-12-02-09:16:35-root-INFO: Loss too large (3211.865->3217.251)! Learning rate decreased to 0.00120.
2024-12-02-09:16:35-root-INFO: grad norm: 300.693 287.058 89.522
2024-12-02-09:16:35-root-INFO: grad norm: 328.020 314.161 94.339
2024-12-02-09:16:36-root-INFO: grad norm: 365.117 349.617 105.254
2024-12-02-09:16:36-root-INFO: Loss too large (3182.933->3184.486)! Learning rate decreased to 0.00096.
2024-12-02-09:16:37-root-INFO: grad norm: 267.209 256.120 76.178
2024-12-02-09:16:37-root-INFO: Loss Change: 3211.865 -> 3141.521
2024-12-02-09:16:37-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-09:16:37-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-09:16:37-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:16:37-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-09:16:37-root-INFO: grad norm: 230.083 218.415 72.341
2024-12-02-09:16:37-root-INFO: Loss too large (3143.921->3182.917)! Learning rate decreased to 0.00157.
2024-12-02-09:16:37-root-INFO: Loss too large (3143.921->3152.818)! Learning rate decreased to 0.00126.
2024-12-02-09:16:38-root-INFO: grad norm: 248.745 239.090 68.629
2024-12-02-09:16:38-root-INFO: grad norm: 277.454 265.188 81.582
2024-12-02-09:16:39-root-INFO: grad norm: 311.343 298.863 87.268
2024-12-02-09:16:39-root-INFO: grad norm: 354.013 339.066 101.784
2024-12-02-09:16:39-root-INFO: Loss too large (3124.632->3127.815)! Learning rate decreased to 0.00101.
2024-12-02-09:16:40-root-INFO: Loss Change: 3143.921 -> 3102.096
2024-12-02-09:16:40-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-02-09:16:40-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-09:16:40-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:16:40-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-09:16:40-root-INFO: grad norm: 326.843 309.859 103.991
2024-12-02-09:16:40-root-INFO: Loss too large (3101.252->3212.871)! Learning rate decreased to 0.00165.
2024-12-02-09:16:40-root-INFO: Loss too large (3101.252->3136.869)! Learning rate decreased to 0.00132.
2024-12-02-09:16:41-root-INFO: grad norm: 372.610 357.507 105.008
2024-12-02-09:16:41-root-INFO: Loss too large (3094.903->3102.929)! Learning rate decreased to 0.00105.
2024-12-02-09:16:41-root-INFO: grad norm: 292.150 280.097 83.050
2024-12-02-09:16:42-root-INFO: grad norm: 232.444 222.667 66.708
2024-12-02-09:16:42-root-INFO: grad norm: 195.329 187.146 55.945
2024-12-02-09:16:43-root-INFO: Loss Change: 3101.252 -> 3028.448
2024-12-02-09:16:43-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-02-09:16:43-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-09:16:43-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:16:43-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-09:16:43-root-INFO: grad norm: 167.061 159.307 50.305
2024-12-02-09:16:43-root-INFO: Loss too large (3010.272->3017.878)! Learning rate decreased to 0.00172.
2024-12-02-09:16:44-root-INFO: grad norm: 256.949 247.374 69.491
2024-12-02-09:16:44-root-INFO: Loss too large (3005.354->3037.770)! Learning rate decreased to 0.00138.
2024-12-02-09:16:44-root-INFO: Loss too large (3005.354->3007.510)! Learning rate decreased to 0.00110.
2024-12-02-09:16:44-root-INFO: grad norm: 230.373 221.002 65.037
2024-12-02-09:16:45-root-INFO: grad norm: 213.237 204.941 58.899
2024-12-02-09:16:45-root-INFO: grad norm: 201.406 193.259 56.704
2024-12-02-09:16:46-root-INFO: Loss Change: 3010.272 -> 2961.202
2024-12-02-09:16:46-root-INFO: Regularization Change: 0.000 -> 0.343
2024-12-02-09:16:46-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-09:16:46-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-09:16:46-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-09:16:46-root-INFO: grad norm: 218.254 207.312 68.240
2024-12-02-09:16:46-root-INFO: Loss too large (2957.959->3011.888)! Learning rate decreased to 0.00180.
2024-12-02-09:16:46-root-INFO: Loss too large (2957.959->2974.716)! Learning rate decreased to 0.00144.
2024-12-02-09:16:47-root-INFO: grad norm: 300.309 288.022 85.023
2024-12-02-09:16:47-root-INFO: Loss too large (2954.488->2974.670)! Learning rate decreased to 0.00115.
2024-12-02-09:16:47-root-INFO: grad norm: 309.454 297.961 83.551
2024-12-02-09:16:48-root-INFO: grad norm: 322.989 310.582 88.661
2024-12-02-09:16:48-root-INFO: grad norm: 338.110 325.422 91.753
2024-12-02-09:16:49-root-INFO: Loss Change: 2957.959 -> 2926.982
2024-12-02-09:16:49-root-INFO: Regularization Change: 0.000 -> 0.398
2024-12-02-09:16:49-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-09:16:49-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:16:49-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-09:16:49-root-INFO: grad norm: 310.461 300.299 78.781
2024-12-02-09:16:49-root-INFO: Loss too large (2902.718->3074.329)! Learning rate decreased to 0.00188.
2024-12-02-09:16:49-root-INFO: Loss too large (2902.718->2982.076)! Learning rate decreased to 0.00150.
2024-12-02-09:16:49-root-INFO: Loss too large (2902.718->2926.505)! Learning rate decreased to 0.00120.
2024-12-02-09:16:50-root-INFO: grad norm: 325.486 313.498 87.522
2024-12-02-09:16:50-root-INFO: grad norm: 349.630 338.162 88.814
2024-12-02-09:16:51-root-INFO: grad norm: 375.785 362.315 99.713
2024-12-02-09:16:51-root-INFO: grad norm: 403.557 390.528 101.718
2024-12-02-09:16:52-root-INFO: Loss Change: 2902.718 -> 2880.110
2024-12-02-09:16:52-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-02-09:16:52-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-09:16:52-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:16:52-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-09:16:52-root-INFO: grad norm: 441.434 424.826 119.942
2024-12-02-09:16:52-root-INFO: Loss too large (2875.826->3259.282)! Learning rate decreased to 0.00196.
2024-12-02-09:16:52-root-INFO: Loss too large (2875.826->3060.130)! Learning rate decreased to 0.00157.
2024-12-02-09:16:52-root-INFO: Loss too large (2875.826->2935.949)! Learning rate decreased to 0.00126.
2024-12-02-09:16:53-root-INFO: grad norm: 460.041 445.145 116.120
2024-12-02-09:16:53-root-INFO: grad norm: 474.900 458.840 122.461
2024-12-02-09:16:54-root-INFO: grad norm: 477.712 462.561 119.358
2024-12-02-09:16:54-root-INFO: grad norm: 468.253 452.133 121.805
2024-12-02-09:16:55-root-INFO: Loss Change: 2875.826 -> 2824.478
2024-12-02-09:16:55-root-INFO: Regularization Change: 0.000 -> 0.652
2024-12-02-09:16:55-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-09:16:55-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:16:55-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-09:16:55-root-INFO: grad norm: 431.742 417.898 108.454
2024-12-02-09:16:55-root-INFO: Loss too large (2814.047->3129.228)! Learning rate decreased to 0.00205.
2024-12-02-09:16:55-root-INFO: Loss too large (2814.047->2960.223)! Learning rate decreased to 0.00164.
2024-12-02-09:16:56-root-INFO: Loss too large (2814.047->2853.838)! Learning rate decreased to 0.00131.
2024-12-02-09:16:56-root-INFO: grad norm: 390.390 376.961 101.512
2024-12-02-09:16:57-root-INFO: grad norm: 356.744 345.872 87.400
2024-12-02-09:16:57-root-INFO: grad norm: 337.216 325.284 88.910
2024-12-02-09:16:58-root-INFO: grad norm: 326.770 317.096 78.920
2024-12-02-09:16:58-root-INFO: Loss Change: 2814.047 -> 2727.552
2024-12-02-09:16:58-root-INFO: Regularization Change: 0.000 -> 0.612
2024-12-02-09:16:58-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-09:16:58-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:16:58-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-09:16:58-root-INFO: grad norm: 340.704 327.914 92.472
2024-12-02-09:16:58-root-INFO: Loss too large (2720.109->2956.967)! Learning rate decreased to 0.00214.
2024-12-02-09:16:58-root-INFO: Loss too large (2720.109->2826.053)! Learning rate decreased to 0.00171.
2024-12-02-09:16:59-root-INFO: Loss too large (2720.109->2748.047)! Learning rate decreased to 0.00137.
2024-12-02-09:16:59-root-INFO: grad norm: 348.184 338.230 82.657
2024-12-02-09:17:00-root-INFO: grad norm: 370.694 358.318 94.983
2024-12-02-09:17:00-root-INFO: grad norm: 401.106 390.052 93.518
2024-12-02-09:17:01-root-INFO: grad norm: 431.322 417.172 109.573
2024-12-02-09:17:01-root-INFO: Loss Change: 2720.109 -> 2688.976
2024-12-02-09:17:01-root-INFO: Regularization Change: 0.000 -> 0.673
2024-12-02-09:17:01-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-09:17:01-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:17:01-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-09:17:01-root-INFO: grad norm: 420.570 410.765 90.285
2024-12-02-09:17:01-root-INFO: Loss too large (2665.664->3040.354)! Learning rate decreased to 0.00224.
2024-12-02-09:17:01-root-INFO: Loss too large (2665.664->2851.541)! Learning rate decreased to 0.00179.
2024-12-02-09:17:02-root-INFO: Loss too large (2665.664->2730.247)! Learning rate decreased to 0.00143.
2024-12-02-09:17:02-root-INFO: grad norm: 434.578 421.108 107.357
2024-12-02-09:17:03-root-INFO: grad norm: 448.892 437.437 100.760
2024-12-02-09:17:03-root-INFO: grad norm: 456.874 442.926 112.026
2024-12-02-09:17:04-root-INFO: grad norm: 454.074 442.736 100.840
2024-12-02-09:17:04-root-INFO: Loss Change: 2665.664 -> 2620.235
2024-12-02-09:17:04-root-INFO: Regularization Change: 0.000 -> 0.789
2024-12-02-09:17:04-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-09:17:04-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-09:17:04-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-09:17:04-root-INFO: grad norm: 461.235 447.737 110.766
2024-12-02-09:17:04-root-INFO: Loss too large (2615.731->3031.037)! Learning rate decreased to 0.00233.
2024-12-02-09:17:04-root-INFO: Loss too large (2615.731->2809.846)! Learning rate decreased to 0.00187.
2024-12-02-09:17:05-root-INFO: Loss too large (2615.731->2668.081)! Learning rate decreased to 0.00149.
2024-12-02-09:17:05-root-INFO: grad norm: 419.934 410.376 89.083
2024-12-02-09:17:06-root-INFO: grad norm: 391.665 380.162 94.224
2024-12-02-09:17:06-root-INFO: grad norm: 370.792 362.525 77.861
2024-12-02-09:17:07-root-INFO: grad norm: 361.695 351.070 87.024
2024-12-02-09:17:07-root-INFO: Loss Change: 2615.731 -> 2521.253
2024-12-02-09:17:07-root-INFO: Regularization Change: 0.000 -> 0.794
2024-12-02-09:17:07-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-09:17:07-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:17:07-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-09:17:07-root-INFO: grad norm: 342.607 335.513 69.359
2024-12-02-09:17:07-root-INFO: Loss too large (2503.752->2775.138)! Learning rate decreased to 0.00244.
2024-12-02-09:17:08-root-INFO: Loss too large (2503.752->2629.741)! Learning rate decreased to 0.00195.
2024-12-02-09:17:08-root-INFO: Loss too large (2503.752->2540.700)! Learning rate decreased to 0.00156.
2024-12-02-09:17:08-root-INFO: grad norm: 357.820 348.178 82.506
2024-12-02-09:17:09-root-INFO: grad norm: 396.163 387.423 82.756
2024-12-02-09:17:09-root-INFO: Loss too large (2487.495->2493.453)! Learning rate decreased to 0.00125.
2024-12-02-09:17:09-root-INFO: grad norm: 294.344 286.433 67.784
2024-12-02-09:17:10-root-INFO: grad norm: 234.271 229.082 49.034
2024-12-02-09:17:10-root-INFO: Loss Change: 2503.752 -> 2415.514
2024-12-02-09:17:10-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-02-09:17:10-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-09:17:10-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:17:10-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-09:17:10-root-INFO: grad norm: 245.987 238.565 59.970
2024-12-02-09:17:11-root-INFO: Loss too large (2413.049->2592.019)! Learning rate decreased to 0.00254.
2024-12-02-09:17:11-root-INFO: Loss too large (2413.049->2499.102)! Learning rate decreased to 0.00203.
2024-12-02-09:17:11-root-INFO: Loss too large (2413.049->2443.432)! Learning rate decreased to 0.00163.
2024-12-02-09:17:11-root-INFO: grad norm: 318.185 311.538 64.696
2024-12-02-09:17:12-root-INFO: Loss too large (2412.205->2433.458)! Learning rate decreased to 0.00130.
2024-12-02-09:17:12-root-INFO: grad norm: 300.796 293.718 64.873
2024-12-02-09:17:13-root-INFO: grad norm: 294.835 288.910 58.812
2024-12-02-09:17:13-root-INFO: grad norm: 299.137 292.143 64.305
2024-12-02-09:17:13-root-INFO: Loss Change: 2413.049 -> 2374.677
2024-12-02-09:17:13-root-INFO: Regularization Change: 0.000 -> 0.435
2024-12-02-09:17:13-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-09:17:13-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:17:14-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-09:17:14-root-INFO: grad norm: 294.199 288.293 58.657
2024-12-02-09:17:14-root-INFO: Loss too large (2358.959->2701.553)! Learning rate decreased to 0.00265.
2024-12-02-09:17:14-root-INFO: Loss too large (2358.959->2547.284)! Learning rate decreased to 0.00212.
2024-12-02-09:17:14-root-INFO: Loss too large (2358.959->2447.902)! Learning rate decreased to 0.00170.
2024-12-02-09:17:15-root-INFO: Loss too large (2358.959->2387.787)! Learning rate decreased to 0.00136.
2024-12-02-09:17:15-root-INFO: grad norm: 319.416 312.758 64.879
2024-12-02-09:17:16-root-INFO: grad norm: 360.162 353.358 69.677
2024-12-02-09:17:16-root-INFO: Loss too large (2353.183->2359.043)! Learning rate decreased to 0.00109.
2024-12-02-09:17:16-root-INFO: grad norm: 271.249 265.473 55.678
2024-12-02-09:17:17-root-INFO: grad norm: 215.697 211.350 43.085
2024-12-02-09:17:17-root-INFO: Loss Change: 2358.959 -> 2302.218
2024-12-02-09:17:17-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-02-09:17:17-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-09:17:17-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:17:17-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-09:17:17-root-INFO: grad norm: 261.263 255.221 55.861
2024-12-02-09:17:17-root-INFO: Loss too large (2300.497->2617.693)! Learning rate decreased to 0.00277.
2024-12-02-09:17:18-root-INFO: Loss too large (2300.497->2478.031)! Learning rate decreased to 0.00222.
2024-12-02-09:17:18-root-INFO: Loss too large (2300.497->2387.588)! Learning rate decreased to 0.00177.
2024-12-02-09:17:18-root-INFO: Loss too large (2300.497->2332.584)! Learning rate decreased to 0.00142.
2024-12-02-09:17:18-root-INFO: Loss too large (2300.497->2301.327)! Learning rate decreased to 0.00113.
2024-12-02-09:17:18-root-INFO: grad norm: 221.789 217.769 42.036
2024-12-02-09:17:19-root-INFO: grad norm: 202.370 197.925 42.182
2024-12-02-09:17:19-root-INFO: grad norm: 192.637 188.830 38.111
2024-12-02-09:17:20-root-INFO: grad norm: 190.814 186.658 39.605
2024-12-02-09:17:20-root-INFO: Loss Change: 2300.497 -> 2253.776
2024-12-02-09:17:20-root-INFO: Regularization Change: 0.000 -> 0.263
2024-12-02-09:17:20-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-09:17:20-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-09:17:20-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-09:17:21-root-INFO: grad norm: 197.579 192.445 44.751
2024-12-02-09:17:21-root-INFO: Loss too large (2250.829->2460.733)! Learning rate decreased to 0.00289.
2024-12-02-09:17:21-root-INFO: Loss too large (2250.829->2367.208)! Learning rate decreased to 0.00231.
2024-12-02-09:17:21-root-INFO: Loss too large (2250.829->2308.705)! Learning rate decreased to 0.00185.
2024-12-02-09:17:21-root-INFO: Loss too large (2250.829->2273.735)! Learning rate decreased to 0.00148.
2024-12-02-09:17:21-root-INFO: Loss too large (2250.829->2253.903)! Learning rate decreased to 0.00118.
2024-12-02-09:17:22-root-INFO: grad norm: 209.319 205.071 41.958
2024-12-02-09:17:22-root-INFO: grad norm: 234.280 229.571 46.735
2024-12-02-09:17:23-root-INFO: grad norm: 270.353 265.863 49.069
2024-12-02-09:17:23-root-INFO: Loss too large (2237.557->2238.711)! Learning rate decreased to 0.00095.
2024-12-02-09:17:23-root-INFO: grad norm: 210.061 205.749 42.342
2024-12-02-09:17:24-root-INFO: Loss Change: 2250.829 -> 2215.339
2024-12-02-09:17:24-root-INFO: Regularization Change: 0.000 -> 0.207
2024-12-02-09:17:24-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-09:17:24-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:17:24-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-09:17:24-root-INFO: grad norm: 246.900 242.084 48.530
2024-12-02-09:17:24-root-INFO: Loss too large (2210.309->2606.935)! Learning rate decreased to 0.00301.
2024-12-02-09:17:24-root-INFO: Loss too large (2210.309->2449.797)! Learning rate decreased to 0.00241.
2024-12-02-09:17:24-root-INFO: Loss too large (2210.309->2342.672)! Learning rate decreased to 0.00193.
2024-12-02-09:17:25-root-INFO: Loss too large (2210.309->2274.094)! Learning rate decreased to 0.00154.
2024-12-02-09:17:25-root-INFO: Loss too large (2210.309->2232.765)! Learning rate decreased to 0.00123.
2024-12-02-09:17:25-root-INFO: grad norm: 298.408 293.494 53.928
2024-12-02-09:17:25-root-INFO: Loss too large (2209.465->2217.141)! Learning rate decreased to 0.00099.
2024-12-02-09:17:26-root-INFO: grad norm: 248.300 244.124 45.349
2024-12-02-09:17:26-root-INFO: grad norm: 211.913 207.919 40.947
2024-12-02-09:17:27-root-INFO: grad norm: 187.503 184.071 35.710
2024-12-02-09:17:27-root-INFO: Loss Change: 2210.309 -> 2172.649
2024-12-02-09:17:27-root-INFO: Regularization Change: 0.000 -> 0.187
2024-12-02-09:17:27-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-09:17:27-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:17:27-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-09:17:28-root-INFO: grad norm: 142.976 138.894 33.920
2024-12-02-09:17:28-root-INFO: Loss too large (2156.906->2284.586)! Learning rate decreased to 0.00314.
2024-12-02-09:17:28-root-INFO: Loss too large (2156.906->2227.336)! Learning rate decreased to 0.00251.
2024-12-02-09:17:28-root-INFO: Loss too large (2156.906->2192.159)! Learning rate decreased to 0.00201.
2024-12-02-09:17:28-root-INFO: Loss too large (2156.906->2171.322)! Learning rate decreased to 0.00161.
2024-12-02-09:17:28-root-INFO: Loss too large (2156.906->2159.523)! Learning rate decreased to 0.00129.
2024-12-02-09:17:29-root-INFO: grad norm: 183.390 180.224 33.929
2024-12-02-09:17:29-root-INFO: Loss too large (2153.274->2154.668)! Learning rate decreased to 0.00103.
2024-12-02-09:17:29-root-INFO: grad norm: 175.436 171.658 36.213
2024-12-02-09:17:30-root-INFO: grad norm: 172.019 169.038 31.881
2024-12-02-09:17:30-root-INFO: grad norm: 172.055 168.367 35.433
2024-12-02-09:17:31-root-INFO: Loss Change: 2156.906 -> 2132.421
2024-12-02-09:17:31-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-02-09:17:31-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-09:17:31-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:17:31-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-09:17:31-root-INFO: grad norm: 283.668 278.224 55.311
2024-12-02-09:17:31-root-INFO: Loss too large (2139.301->2716.691)! Learning rate decreased to 0.00328.
2024-12-02-09:17:31-root-INFO: Loss too large (2139.301->2514.977)! Learning rate decreased to 0.00262.
2024-12-02-09:17:32-root-INFO: Loss too large (2139.301->2364.396)! Learning rate decreased to 0.00210.
2024-12-02-09:17:32-root-INFO: Loss too large (2139.301->2260.278)! Learning rate decreased to 0.00168.
2024-12-02-09:17:32-root-INFO: Loss too large (2139.301->2193.266)! Learning rate decreased to 0.00134.
2024-12-02-09:17:32-root-INFO: Loss too large (2139.301->2153.073)! Learning rate decreased to 0.00107.
2024-12-02-09:17:32-root-INFO: grad norm: 288.909 284.234 51.765
2024-12-02-09:17:33-root-INFO: grad norm: 306.405 302.116 51.089
2024-12-02-09:17:33-root-INFO: grad norm: 325.933 320.863 57.265
2024-12-02-09:17:34-root-INFO: grad norm: 345.568 341.101 55.386
2024-12-02-09:17:34-root-INFO: Loss Change: 2139.301 -> 2123.502
2024-12-02-09:17:34-root-INFO: Regularization Change: 0.000 -> 0.263
2024-12-02-09:17:34-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-09:17:34-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:17:34-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-09:17:35-root-INFO: grad norm: 300.549 295.939 52.439
2024-12-02-09:17:35-root-INFO: Loss too large (2103.151->2837.642)! Learning rate decreased to 0.00342.
2024-12-02-09:17:35-root-INFO: Loss too large (2103.151->2576.973)! Learning rate decreased to 0.00273.
2024-12-02-09:17:35-root-INFO: Loss too large (2103.151->2389.251)! Learning rate decreased to 0.00219.
2024-12-02-09:17:35-root-INFO: Loss too large (2103.151->2261.286)! Learning rate decreased to 0.00175.
2024-12-02-09:17:35-root-INFO: Loss too large (2103.151->2178.646)! Learning rate decreased to 0.00140.
2024-12-02-09:17:35-root-INFO: Loss too large (2103.151->2128.249)! Learning rate decreased to 0.00112.
2024-12-02-09:17:36-root-INFO: grad norm: 313.959 309.702 51.522
2024-12-02-09:17:36-root-INFO: grad norm: 331.281 326.543 55.830
2024-12-02-09:17:37-root-INFO: grad norm: 349.195 344.854 54.891
2024-12-02-09:17:37-root-INFO: grad norm: 366.147 361.072 60.751
2024-12-02-09:17:38-root-INFO: Loss Change: 2103.151 -> 2093.419
2024-12-02-09:17:38-root-INFO: Regularization Change: 0.000 -> 0.280
2024-12-02-09:17:38-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-09:17:38-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:17:38-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-09:17:38-root-INFO: grad norm: 438.168 433.031 66.893
2024-12-02-09:17:38-root-INFO: Loss too large (2102.077->3190.078)! Learning rate decreased to 0.00356.
2024-12-02-09:17:38-root-INFO: Loss too large (2102.077->2890.398)! Learning rate decreased to 0.00285.
2024-12-02-09:17:38-root-INFO: Loss too large (2102.077->2631.138)! Learning rate decreased to 0.00228.
2024-12-02-09:17:39-root-INFO: Loss too large (2102.077->2420.145)! Learning rate decreased to 0.00182.
2024-12-02-09:17:39-root-INFO: Loss too large (2102.077->2262.924)! Learning rate decreased to 0.00146.
2024-12-02-09:17:39-root-INFO: Loss too large (2102.077->2157.174)! Learning rate decreased to 0.00117.
2024-12-02-09:17:39-root-INFO: grad norm: 431.902 426.594 67.501
2024-12-02-09:17:40-root-INFO: grad norm: 423.482 418.678 63.612
2024-12-02-09:17:40-root-INFO: grad norm: 410.923 405.767 64.895
2024-12-02-09:17:41-root-INFO: grad norm: 398.560 394.039 59.863
2024-12-02-09:17:41-root-INFO: Loss Change: 2102.077 -> 2065.068
2024-12-02-09:17:41-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-09:17:41-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-09:17:41-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-09:17:41-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-09:17:41-root-INFO: grad norm: 342.693 337.952 56.807
2024-12-02-09:17:42-root-INFO: Loss too large (2047.607->2994.716)! Learning rate decreased to 0.00371.
2024-12-02-09:17:42-root-INFO: Loss too large (2047.607->2668.956)! Learning rate decreased to 0.00297.
2024-12-02-09:17:42-root-INFO: Loss too large (2047.607->2428.481)! Learning rate decreased to 0.00238.
2024-12-02-09:17:42-root-INFO: Loss too large (2047.607->2260.674)! Learning rate decreased to 0.00190.
2024-12-02-09:17:42-root-INFO: Loss too large (2047.607->2149.930)! Learning rate decreased to 0.00152.
2024-12-02-09:17:43-root-INFO: Loss too large (2047.607->2081.156)! Learning rate decreased to 0.00122.
2024-12-02-09:17:43-root-INFO: grad norm: 336.625 332.893 49.987
2024-12-02-09:17:43-root-INFO: grad norm: 335.743 331.269 54.628
2024-12-02-09:17:44-root-INFO: grad norm: 338.824 335.170 49.629
2024-12-02-09:17:44-root-INFO: grad norm: 343.797 339.264 55.645
2024-12-02-09:17:45-root-INFO: Loss Change: 2047.607 -> 2024.810
2024-12-02-09:17:45-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-02-09:17:45-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-09:17:45-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:17:45-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-09:17:45-root-INFO: grad norm: 419.250 414.344 63.951
2024-12-02-09:17:45-root-INFO: Loss too large (2039.863->3076.400)! Learning rate decreased to 0.00387.
2024-12-02-09:17:46-root-INFO: Loss too large (2039.863->2795.380)! Learning rate decreased to 0.00309.
2024-12-02-09:17:46-root-INFO: Loss too large (2039.863->2549.893)! Learning rate decreased to 0.00248.
2024-12-02-09:17:46-root-INFO: Loss too large (2039.863->2347.957)! Learning rate decreased to 0.00198.
2024-12-02-09:17:46-root-INFO: Loss too large (2039.863->2195.258)! Learning rate decreased to 0.00158.
2024-12-02-09:17:46-root-INFO: Loss too large (2039.863->2091.530)! Learning rate decreased to 0.00127.
2024-12-02-09:17:47-root-INFO: grad norm: 411.337 406.443 63.262
2024-12-02-09:17:47-root-INFO: grad norm: 405.954 401.642 59.011
2024-12-02-09:17:48-root-INFO: grad norm: 397.906 393.016 62.191
2024-12-02-09:17:48-root-INFO: grad norm: 389.769 385.692 56.227
2024-12-02-09:17:48-root-INFO: Loss Change: 2039.863 -> 2003.056
2024-12-02-09:17:48-root-INFO: Regularization Change: 0.000 -> 0.418
2024-12-02-09:17:48-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-09:17:48-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:17:49-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-09:17:49-root-INFO: grad norm: 333.044 328.883 52.484
2024-12-02-09:17:49-root-INFO: Loss too large (1983.228->2954.730)! Learning rate decreased to 0.00403.
2024-12-02-09:17:49-root-INFO: Loss too large (1983.228->2622.442)! Learning rate decreased to 0.00322.
2024-12-02-09:17:49-root-INFO: Loss too large (1983.228->2377.133)! Learning rate decreased to 0.00258.
2024-12-02-09:17:49-root-INFO: Loss too large (1983.228->2205.506)! Learning rate decreased to 0.00206.
2024-12-02-09:17:49-root-INFO: Loss too large (1983.228->2091.722)! Learning rate decreased to 0.00165.
2024-12-02-09:17:50-root-INFO: Loss too large (1983.228->2020.616)! Learning rate decreased to 0.00132.
2024-12-02-09:17:50-root-INFO: grad norm: 331.652 328.232 47.506
2024-12-02-09:17:51-root-INFO: grad norm: 333.186 328.992 52.700
2024-12-02-09:17:51-root-INFO: grad norm: 337.043 333.683 47.472
2024-12-02-09:17:51-root-INFO: grad norm: 341.541 337.250 53.973
2024-12-02-09:17:52-root-INFO: Loss Change: 1983.228 -> 1963.729
2024-12-02-09:17:52-root-INFO: Regularization Change: 0.000 -> 0.351
2024-12-02-09:17:52-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-09:17:52-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:17:52-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-09:17:52-root-INFO: grad norm: 406.441 402.012 59.844
2024-12-02-09:17:52-root-INFO: Loss too large (1976.545->2979.739)! Learning rate decreased to 0.00420.
2024-12-02-09:17:52-root-INFO: Loss too large (1976.545->2713.000)! Learning rate decreased to 0.00336.
2024-12-02-09:17:53-root-INFO: Loss too large (1976.545->2476.461)! Learning rate decreased to 0.00269.
2024-12-02-09:17:53-root-INFO: Loss too large (1976.545->2278.573)! Learning rate decreased to 0.00215.
2024-12-02-09:17:53-root-INFO: Loss too large (1976.545->2126.541)! Learning rate decreased to 0.00172.
2024-12-02-09:17:53-root-INFO: Loss too large (1976.545->2022.709)! Learning rate decreased to 0.00138.
2024-12-02-09:17:54-root-INFO: grad norm: 383.792 379.527 57.063
2024-12-02-09:17:54-root-INFO: grad norm: 366.992 363.357 51.529
2024-12-02-09:17:54-root-INFO: grad norm: 350.305 346.135 53.888
2024-12-02-09:17:55-root-INFO: grad norm: 337.061 333.792 46.833
2024-12-02-09:17:55-root-INFO: Loss Change: 1976.545 -> 1927.162
2024-12-02-09:17:55-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-02-09:17:55-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-09:17:55-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:17:55-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-09:17:56-root-INFO: grad norm: 284.640 281.028 45.204
2024-12-02-09:17:56-root-INFO: Loss too large (1910.520->2709.377)! Learning rate decreased to 0.00437.
2024-12-02-09:17:56-root-INFO: Loss too large (1910.520->2424.239)! Learning rate decreased to 0.00350.
2024-12-02-09:17:56-root-INFO: Loss too large (1910.520->2220.227)! Learning rate decreased to 0.00280.
2024-12-02-09:17:56-root-INFO: Loss too large (1910.520->2081.333)! Learning rate decreased to 0.00224.
2024-12-02-09:17:56-root-INFO: Loss too large (1910.520->1991.429)! Learning rate decreased to 0.00179.
2024-12-02-09:17:57-root-INFO: Loss too large (1910.520->1936.391)! Learning rate decreased to 0.00143.
2024-12-02-09:17:57-root-INFO: grad norm: 275.483 272.730 38.850
2024-12-02-09:17:57-root-INFO: grad norm: 269.791 266.207 43.830
2024-12-02-09:17:58-root-INFO: grad norm: 267.565 265.009 36.892
2024-12-02-09:17:58-root-INFO: grad norm: 266.514 262.909 43.688
2024-12-02-09:17:59-root-INFO: Loss Change: 1910.520 -> 1884.289
2024-12-02-09:17:59-root-INFO: Regularization Change: 0.000 -> 0.320
2024-12-02-09:17:59-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-09:17:59-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-09:17:59-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-09:17:59-root-INFO: grad norm: 327.047 323.658 46.959
2024-12-02-09:17:59-root-INFO: Loss too large (1894.585->2672.501)! Learning rate decreased to 0.00455.
2024-12-02-09:17:59-root-INFO: Loss too large (1894.585->2455.904)! Learning rate decreased to 0.00364.
2024-12-02-09:17:59-root-INFO: Loss too large (1894.585->2266.102)! Learning rate decreased to 0.00291.
2024-12-02-09:18:00-root-INFO: Loss too large (1894.585->2111.501)! Learning rate decreased to 0.00233.
2024-12-02-09:18:00-root-INFO: Loss too large (1894.585->1998.087)! Learning rate decreased to 0.00186.
2024-12-02-09:18:00-root-INFO: Loss too large (1894.585->1924.360)! Learning rate decreased to 0.00149.
2024-12-02-09:18:00-root-INFO: grad norm: 302.698 298.998 47.178
2024-12-02-09:18:01-root-INFO: grad norm: 283.481 280.899 38.175
2024-12-02-09:18:01-root-INFO: grad norm: 268.225 264.658 43.596
2024-12-02-09:18:02-root-INFO: grad norm: 256.064 253.752 34.332
2024-12-02-09:18:02-root-INFO: Loss Change: 1894.585 -> 1850.686
2024-12-02-09:18:02-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-09:18:02-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-09:18:02-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:18:02-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-09:18:02-root-INFO: grad norm: 179.620 176.497 33.352
2024-12-02-09:18:03-root-INFO: Loss too large (1834.082->2196.000)! Learning rate decreased to 0.00474.
2024-12-02-09:18:03-root-INFO: Loss too large (1834.082->2050.572)! Learning rate decreased to 0.00379.
2024-12-02-09:18:03-root-INFO: Loss too large (1834.082->1955.227)! Learning rate decreased to 0.00303.
2024-12-02-09:18:03-root-INFO: Loss too large (1834.082->1895.116)! Learning rate decreased to 0.00243.
2024-12-02-09:18:03-root-INFO: Loss too large (1834.082->1858.759)! Learning rate decreased to 0.00194.
2024-12-02-09:18:03-root-INFO: Loss too large (1834.082->1837.847)! Learning rate decreased to 0.00155.
2024-12-02-09:18:04-root-INFO: grad norm: 162.923 161.108 24.255
2024-12-02-09:18:04-root-INFO: grad norm: 151.680 148.757 29.635
2024-12-02-09:18:05-root-INFO: grad norm: 144.465 142.783 21.986
2024-12-02-09:18:05-root-INFO: grad norm: 138.710 135.854 28.002
2024-12-02-09:18:06-root-INFO: Loss Change: 1834.082 -> 1804.431
2024-12-02-09:18:06-root-INFO: Regularization Change: 0.000 -> 0.241
2024-12-02-09:18:06-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-09:18:06-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:18:06-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-09:18:06-root-INFO: grad norm: 151.614 149.896 22.761
2024-12-02-09:18:06-root-INFO: Loss too large (1803.916->2070.659)! Learning rate decreased to 0.00494.
2024-12-02-09:18:06-root-INFO: Loss too large (1803.916->1967.771)! Learning rate decreased to 0.00395.
2024-12-02-09:18:06-root-INFO: Loss too large (1803.916->1895.753)! Learning rate decreased to 0.00316.
2024-12-02-09:18:06-root-INFO: Loss too large (1803.916->1849.346)! Learning rate decreased to 0.00253.
2024-12-02-09:18:07-root-INFO: Loss too large (1803.916->1821.412)! Learning rate decreased to 0.00202.
2024-12-02-09:18:07-root-INFO: Loss too large (1803.916->1805.640)! Learning rate decreased to 0.00162.
2024-12-02-09:18:07-root-INFO: grad norm: 141.967 139.149 28.148
2024-12-02-09:18:08-root-INFO: grad norm: 136.044 134.419 20.965
2024-12-02-09:18:08-root-INFO: grad norm: 130.791 128.038 26.693
2024-12-02-09:18:09-root-INFO: grad norm: 127.283 125.681 20.128
2024-12-02-09:18:09-root-INFO: Loss Change: 1803.916 -> 1777.444
2024-12-02-09:18:09-root-INFO: Regularization Change: 0.000 -> 0.231
2024-12-02-09:18:09-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-09:18:09-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:18:09-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-09:18:09-root-INFO: grad norm: 80.583 76.881 24.147
2024-12-02-09:18:10-root-INFO: Loss too large (1771.874->1784.985)! Learning rate decreased to 0.00514.
2024-12-02-09:18:10-root-INFO: Loss too large (1771.874->1773.951)! Learning rate decreased to 0.00411.
2024-12-02-09:18:10-root-INFO: grad norm: 164.584 162.757 24.452
2024-12-02-09:18:10-root-INFO: Loss too large (1768.023->1873.905)! Learning rate decreased to 0.00329.
2024-12-02-09:18:10-root-INFO: Loss too large (1768.023->1818.158)! Learning rate decreased to 0.00263.
2024-12-02-09:18:11-root-INFO: Loss too large (1768.023->1785.230)! Learning rate decreased to 0.00210.
2024-12-02-09:18:11-root-INFO: grad norm: 204.643 201.705 34.551
2024-12-02-09:18:11-root-INFO: Loss too large (1767.022->1772.744)! Learning rate decreased to 0.00168.
2024-12-02-09:18:12-root-INFO: grad norm: 166.569 164.766 24.442
2024-12-02-09:18:12-root-INFO: grad norm: 132.414 129.667 26.833
2024-12-02-09:18:13-root-INFO: Loss Change: 1771.874 -> 1740.303
2024-12-02-09:18:13-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-02-09:18:13-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-09:18:13-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-09:18:13-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-09:18:13-root-INFO: grad norm: 118.397 117.007 18.086
2024-12-02-09:18:13-root-INFO: Loss too large (1733.935->1883.849)! Learning rate decreased to 0.00535.
2024-12-02-09:18:13-root-INFO: Loss too large (1733.935->1817.740)! Learning rate decreased to 0.00428.
2024-12-02-09:18:13-root-INFO: Loss too large (1733.935->1776.218)! Learning rate decreased to 0.00342.
2024-12-02-09:18:13-root-INFO: Loss too large (1733.935->1751.389)! Learning rate decreased to 0.00274.
2024-12-02-09:18:14-root-INFO: Loss too large (1733.935->1737.289)! Learning rate decreased to 0.00219.
2024-12-02-09:18:14-root-INFO: grad norm: 140.894 138.120 27.818
2024-12-02-09:18:15-root-INFO: grad norm: 168.217 166.571 23.475
2024-12-02-09:18:15-root-INFO: grad norm: 210.539 207.727 34.296
2024-12-02-09:18:15-root-INFO: Loss too large (1727.964->1734.796)! Learning rate decreased to 0.00175.
2024-12-02-09:18:16-root-INFO: grad norm: 169.730 167.798 25.536
2024-12-02-09:18:16-root-INFO: Loss Change: 1733.935 -> 1705.807
2024-12-02-09:18:16-root-INFO: Regularization Change: 0.000 -> 0.340
2024-12-02-09:18:16-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-09:18:16-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:18:16-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-09:18:16-root-INFO: grad norm: 188.104 185.527 31.031
2024-12-02-09:18:16-root-INFO: Loss too large (1701.363->1818.027)! Learning rate decreased to 0.00556.
2024-12-02-09:18:17-root-INFO: Loss too large (1701.363->1795.155)! Learning rate decreased to 0.00445.
2024-12-02-09:18:17-root-INFO: Loss too large (1701.363->1772.547)! Learning rate decreased to 0.00356.
2024-12-02-09:18:17-root-INFO: Loss too large (1701.363->1750.471)! Learning rate decreased to 0.00285.
2024-12-02-09:18:17-root-INFO: Loss too large (1701.363->1730.481)! Learning rate decreased to 0.00228.
2024-12-02-09:18:17-root-INFO: Loss too large (1701.363->1714.131)! Learning rate decreased to 0.00182.
2024-12-02-09:18:17-root-INFO: Loss too large (1701.363->1702.151)! Learning rate decreased to 0.00146.
2024-12-02-09:18:18-root-INFO: grad norm: 135.277 132.878 25.365
2024-12-02-09:18:18-root-INFO: grad norm: 83.887 81.621 19.364
2024-12-02-09:18:19-root-INFO: grad norm: 75.725 73.223 19.302
2024-12-02-09:18:19-root-INFO: grad norm: 68.655 66.276 17.917
2024-12-02-09:18:19-root-INFO: Loss Change: 1701.363 -> 1667.277
2024-12-02-09:18:19-root-INFO: Regularization Change: 0.000 -> 0.194
2024-12-02-09:18:19-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-09:18:19-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:18:20-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-09:18:20-root-INFO: grad norm: 77.041 74.602 19.233
2024-12-02-09:18:20-root-INFO: Loss too large (1663.066->1672.486)! Learning rate decreased to 0.00579.
2024-12-02-09:18:20-root-INFO: Loss too large (1663.066->1663.180)! Learning rate decreased to 0.00463.
2024-12-02-09:18:20-root-INFO: grad norm: 169.724 166.749 31.641
2024-12-02-09:18:21-root-INFO: Loss too large (1658.111->1829.389)! Learning rate decreased to 0.00370.
2024-12-02-09:18:21-root-INFO: Loss too large (1658.111->1747.069)! Learning rate decreased to 0.00296.
2024-12-02-09:18:21-root-INFO: Loss too large (1658.111->1695.665)! Learning rate decreased to 0.00237.
2024-12-02-09:18:21-root-INFO: Loss too large (1658.111->1666.073)! Learning rate decreased to 0.00190.
2024-12-02-09:18:22-root-INFO: grad norm: 195.889 193.738 28.948
2024-12-02-09:18:22-root-INFO: Loss too large (1650.580->1654.432)! Learning rate decreased to 0.00152.
2024-12-02-09:18:22-root-INFO: grad norm: 152.902 150.239 28.414
2024-12-02-09:18:23-root-INFO: grad norm: 107.657 105.653 20.675
2024-12-02-09:18:23-root-INFO: Loss Change: 1663.066 -> 1625.363
2024-12-02-09:18:23-root-INFO: Regularization Change: 0.000 -> 0.450
2024-12-02-09:18:23-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-09:18:23-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:18:23-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-09:18:23-root-INFO: grad norm: 75.106 72.687 18.905
2024-12-02-09:18:24-root-INFO: Loss too large (1619.053->1643.105)! Learning rate decreased to 0.00602.
2024-12-02-09:18:24-root-INFO: Loss too large (1619.053->1627.351)! Learning rate decreased to 0.00482.
2024-12-02-09:18:24-root-INFO: grad norm: 240.585 238.088 34.573
2024-12-02-09:18:24-root-INFO: Loss too large (1618.475->1716.481)! Learning rate decreased to 0.00385.
2024-12-02-09:18:24-root-INFO: Loss too large (1618.475->1697.325)! Learning rate decreased to 0.00308.
2024-12-02-09:18:25-root-INFO: Loss too large (1618.475->1675.411)! Learning rate decreased to 0.00247.
2024-12-02-09:18:25-root-INFO: Loss too large (1618.475->1652.908)! Learning rate decreased to 0.00197.
2024-12-02-09:18:25-root-INFO: Loss too large (1618.475->1632.729)! Learning rate decreased to 0.00158.
2024-12-02-09:18:25-root-INFO: grad norm: 184.757 182.012 31.729
2024-12-02-09:18:26-root-INFO: grad norm: 109.402 107.345 21.115
2024-12-02-09:18:26-root-INFO: grad norm: 107.594 105.434 21.455
2024-12-02-09:18:27-root-INFO: Loss Change: 1619.053 -> 1582.660
2024-12-02-09:18:27-root-INFO: Regularization Change: 0.000 -> 0.439
2024-12-02-09:18:27-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-09:18:27-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:18:27-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-09:18:27-root-INFO: grad norm: 152.268 150.118 25.497
2024-12-02-09:18:27-root-INFO: Loss too large (1582.222->1700.853)! Learning rate decreased to 0.00626.
2024-12-02-09:18:27-root-INFO: Loss too large (1582.222->1679.344)! Learning rate decreased to 0.00501.
2024-12-02-09:18:27-root-INFO: Loss too large (1582.222->1657.038)! Learning rate decreased to 0.00401.
2024-12-02-09:18:28-root-INFO: Loss too large (1582.222->1635.067)! Learning rate decreased to 0.00321.
2024-12-02-09:18:28-root-INFO: Loss too large (1582.222->1615.307)! Learning rate decreased to 0.00256.
2024-12-02-09:18:28-root-INFO: Loss too large (1582.222->1599.316)! Learning rate decreased to 0.00205.
2024-12-02-09:18:28-root-INFO: Loss too large (1582.222->1587.676)! Learning rate decreased to 0.00164.
2024-12-02-09:18:28-root-INFO: grad norm: 146.074 143.916 25.020
2024-12-02-09:18:29-root-INFO: grad norm: 138.103 136.089 23.501
2024-12-02-09:18:29-root-INFO: grad norm: 136.447 134.369 23.723
2024-12-02-09:18:30-root-INFO: grad norm: 134.005 132.057 22.767
2024-12-02-09:18:30-root-INFO: Loss Change: 1582.222 -> 1559.218
2024-12-02-09:18:30-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-09:18:30-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-09:18:30-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-09:18:30-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-09:18:30-root-INFO: grad norm: 104.367 102.553 19.374
2024-12-02-09:18:31-root-INFO: Loss too large (1553.033->1745.633)! Learning rate decreased to 0.00651.
2024-12-02-09:18:31-root-INFO: Loss too large (1553.033->1677.280)! Learning rate decreased to 0.00521.
2024-12-02-09:18:31-root-INFO: Loss too large (1553.033->1627.694)! Learning rate decreased to 0.00417.
2024-12-02-09:18:31-root-INFO: Loss too large (1553.033->1593.603)! Learning rate decreased to 0.00333.
2024-12-02-09:18:31-root-INFO: Loss too large (1553.033->1571.678)! Learning rate decreased to 0.00267.
2024-12-02-09:18:31-root-INFO: Loss too large (1553.033->1558.582)! Learning rate decreased to 0.00213.
2024-12-02-09:18:32-root-INFO: grad norm: 165.044 162.719 27.609
2024-12-02-09:18:32-root-INFO: Loss too large (1551.394->1561.731)! Learning rate decreased to 0.00171.
2024-12-02-09:18:32-root-INFO: Loss too large (1551.394->1551.528)! Learning rate decreased to 0.00137.
2024-12-02-09:18:33-root-INFO: grad norm: 125.113 123.304 21.200
2024-12-02-09:18:33-root-INFO: grad norm: 88.474 86.435 18.883
2024-12-02-09:18:34-root-INFO: grad norm: 75.776 74.022 16.209
2024-12-02-09:18:34-root-INFO: Loss Change: 1553.033 -> 1528.941
2024-12-02-09:18:34-root-INFO: Regularization Change: 0.000 -> 0.173
2024-12-02-09:18:34-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-09:18:34-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:18:34-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-09:18:34-root-INFO: grad norm: 134.916 132.646 24.644
2024-12-02-09:18:34-root-INFO: Loss too large (1528.895->1644.618)! Learning rate decreased to 0.00677.
2024-12-02-09:18:34-root-INFO: Loss too large (1528.895->1624.145)! Learning rate decreased to 0.00542.
2024-12-02-09:18:35-root-INFO: Loss too large (1528.895->1602.307)! Learning rate decreased to 0.00433.
2024-12-02-09:18:35-root-INFO: Loss too large (1528.895->1580.647)! Learning rate decreased to 0.00347.
2024-12-02-09:18:35-root-INFO: Loss too large (1528.895->1561.304)! Learning rate decreased to 0.00277.
2024-12-02-09:18:35-root-INFO: Loss too large (1528.895->1545.869)! Learning rate decreased to 0.00222.
2024-12-02-09:18:35-root-INFO: Loss too large (1528.895->1534.808)! Learning rate decreased to 0.00177.
2024-12-02-09:18:36-root-INFO: grad norm: 145.127 143.335 22.740
2024-12-02-09:18:36-root-INFO: grad norm: 161.125 158.949 26.390
2024-12-02-09:18:36-root-INFO: Loss too large (1521.957->1522.851)! Learning rate decreased to 0.00142.
2024-12-02-09:18:37-root-INFO: grad norm: 126.130 124.488 20.287
2024-12-02-09:18:37-root-INFO: grad norm: 93.459 91.542 18.832
2024-12-02-09:18:38-root-INFO: Loss Change: 1528.895 -> 1504.588
2024-12-02-09:18:38-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-09:18:38-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-09:18:38-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:18:38-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-09:18:38-root-INFO: grad norm: 68.669 66.922 15.392
2024-12-02-09:18:38-root-INFO: Loss too large (1502.481->1549.827)! Learning rate decreased to 0.00704.
2024-12-02-09:18:38-root-INFO: Loss too large (1502.481->1528.330)! Learning rate decreased to 0.00563.
2024-12-02-09:18:38-root-INFO: Loss too large (1502.481->1514.492)! Learning rate decreased to 0.00450.
2024-12-02-09:18:39-root-INFO: Loss too large (1502.481->1506.113)! Learning rate decreased to 0.00360.
2024-12-02-09:18:39-root-INFO: grad norm: 165.351 163.295 25.997
2024-12-02-09:18:39-root-INFO: Loss too large (1501.401->1555.377)! Learning rate decreased to 0.00288.
2024-12-02-09:18:39-root-INFO: Loss too large (1501.401->1533.686)! Learning rate decreased to 0.00231.
2024-12-02-09:18:39-root-INFO: Loss too large (1501.401->1516.201)! Learning rate decreased to 0.00184.
2024-12-02-09:18:40-root-INFO: Loss too large (1501.401->1503.819)! Learning rate decreased to 0.00148.
2024-12-02-09:18:40-root-INFO: grad norm: 135.528 133.898 20.956
2024-12-02-09:18:41-root-INFO: grad norm: 106.875 105.060 19.611
2024-12-02-09:18:41-root-INFO: grad norm: 94.362 92.895 16.575
2024-12-02-09:18:42-root-INFO: Loss Change: 1502.481 -> 1479.679
2024-12-02-09:18:42-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-02-09:18:42-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-09:18:42-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:18:42-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-09:18:42-root-INFO: grad norm: 135.373 133.192 24.205
2024-12-02-09:18:42-root-INFO: Loss too large (1483.808->1609.562)! Learning rate decreased to 0.00732.
2024-12-02-09:18:42-root-INFO: Loss too large (1483.808->1589.939)! Learning rate decreased to 0.00585.
2024-12-02-09:18:42-root-INFO: Loss too large (1483.808->1567.723)! Learning rate decreased to 0.00468.
2024-12-02-09:18:43-root-INFO: Loss too large (1483.808->1544.442)! Learning rate decreased to 0.00375.
2024-12-02-09:18:43-root-INFO: Loss too large (1483.808->1522.727)! Learning rate decreased to 0.00300.
2024-12-02-09:18:43-root-INFO: Loss too large (1483.808->1504.885)! Learning rate decreased to 0.00240.
2024-12-02-09:18:43-root-INFO: Loss too large (1483.808->1491.870)! Learning rate decreased to 0.00192.
2024-12-02-09:18:43-root-INFO: grad norm: 155.357 153.494 23.989
2024-12-02-09:18:44-root-INFO: grad norm: 180.303 178.155 27.751
2024-12-02-09:18:44-root-INFO: Loss too large (1479.472->1481.489)! Learning rate decreased to 0.00153.
2024-12-02-09:18:45-root-INFO: grad norm: 141.363 139.686 21.711
2024-12-02-09:18:45-root-INFO: grad norm: 107.807 105.974 19.793
2024-12-02-09:18:45-root-INFO: Loss Change: 1483.808 -> 1458.607
2024-12-02-09:18:45-root-INFO: Regularization Change: 0.000 -> 0.216
2024-12-02-09:18:45-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-09:18:45-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-09:18:46-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-09:18:46-root-INFO: grad norm: 61.281 59.577 14.350
2024-12-02-09:18:46-root-INFO: Loss too large (1451.431->1472.833)! Learning rate decreased to 0.00760.
2024-12-02-09:18:46-root-INFO: Loss too large (1451.431->1460.782)! Learning rate decreased to 0.00608.
2024-12-02-09:18:46-root-INFO: Loss too large (1451.431->1453.397)! Learning rate decreased to 0.00487.
2024-12-02-09:18:47-root-INFO: grad norm: 162.996 161.155 24.427
2024-12-02-09:18:47-root-INFO: Loss too large (1449.229->1531.164)! Learning rate decreased to 0.00389.
2024-12-02-09:18:47-root-INFO: Loss too large (1449.229->1505.410)! Learning rate decreased to 0.00311.
2024-12-02-09:18:47-root-INFO: Loss too large (1449.229->1481.630)! Learning rate decreased to 0.00249.
2024-12-02-09:18:47-root-INFO: Loss too large (1449.229->1462.729)! Learning rate decreased to 0.00199.
2024-12-02-09:18:47-root-INFO: Loss too large (1449.229->1449.660)! Learning rate decreased to 0.00159.
2024-12-02-09:18:48-root-INFO: grad norm: 128.954 127.457 19.589
2024-12-02-09:18:48-root-INFO: grad norm: 101.525 99.868 18.271
2024-12-02-09:18:49-root-INFO: grad norm: 86.951 85.541 15.595
2024-12-02-09:18:49-root-INFO: Loss Change: 1451.431 -> 1424.415
2024-12-02-09:18:49-root-INFO: Regularization Change: 0.000 -> 0.320
2024-12-02-09:18:49-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-09:18:49-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:18:49-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-09:18:50-root-INFO: grad norm: 122.546 120.114 24.291
2024-12-02-09:18:50-root-INFO: Loss too large (1433.960->1555.667)! Learning rate decreased to 0.00790.
2024-12-02-09:18:50-root-INFO: Loss too large (1433.960->1532.977)! Learning rate decreased to 0.00632.
2024-12-02-09:18:50-root-INFO: Loss too large (1433.960->1508.341)! Learning rate decreased to 0.00506.
2024-12-02-09:18:50-root-INFO: Loss too large (1433.960->1484.175)! Learning rate decreased to 0.00404.
2024-12-02-09:18:50-root-INFO: Loss too large (1433.960->1463.303)! Learning rate decreased to 0.00324.
2024-12-02-09:18:50-root-INFO: Loss too large (1433.960->1447.403)! Learning rate decreased to 0.00259.
2024-12-02-09:18:51-root-INFO: Loss too large (1433.960->1436.627)! Learning rate decreased to 0.00207.
2024-12-02-09:18:51-root-INFO: grad norm: 140.390 138.707 21.676
2024-12-02-09:18:52-root-INFO: grad norm: 157.539 155.427 25.709
2024-12-02-09:18:52-root-INFO: grad norm: 165.565 163.810 24.044
2024-12-02-09:18:52-root-INFO: grad norm: 165.711 163.694 25.772
2024-12-02-09:18:53-root-INFO: Loss Change: 1433.960 -> 1405.628
2024-12-02-09:18:53-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-02-09:18:53-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-09:18:53-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:18:53-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-09:18:53-root-INFO: grad norm: 137.614 135.856 21.928
2024-12-02-09:18:53-root-INFO: Loss too large (1397.985->1660.381)! Learning rate decreased to 0.00821.
2024-12-02-09:18:53-root-INFO: Loss too large (1397.985->1565.813)! Learning rate decreased to 0.00656.
2024-12-02-09:18:54-root-INFO: Loss too large (1397.985->1500.929)! Learning rate decreased to 0.00525.
2024-12-02-09:18:54-root-INFO: Loss too large (1397.985->1456.477)! Learning rate decreased to 0.00420.
2024-12-02-09:18:54-root-INFO: Loss too large (1397.985->1426.462)! Learning rate decreased to 0.00336.
2024-12-02-09:18:54-root-INFO: Loss too large (1397.985->1406.903)! Learning rate decreased to 0.00269.
2024-12-02-09:18:55-root-INFO: grad norm: 179.940 177.972 26.539
2024-12-02-09:18:55-root-INFO: grad norm: 212.695 210.343 31.544
2024-12-02-09:18:55-root-INFO: grad norm: 212.301 210.336 28.819
2024-12-02-09:18:56-root-INFO: grad norm: 215.121 212.416 34.007
2024-12-02-09:18:56-root-INFO: Loss too large (1356.819->1361.531)! Learning rate decreased to 0.00215.
2024-12-02-09:18:56-root-INFO: Loss Change: 1397.985 -> 1342.361
2024-12-02-09:18:56-root-INFO: Regularization Change: 0.000 -> 1.024
2024-12-02-09:18:56-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-09:18:56-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:18:57-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-09:18:57-root-INFO: grad norm: 189.382 187.438 27.062
2024-12-02-09:18:57-root-INFO: Loss too large (1351.345->1550.723)! Learning rate decreased to 0.00852.
2024-12-02-09:18:57-root-INFO: Loss too large (1351.345->1525.123)! Learning rate decreased to 0.00682.
2024-12-02-09:18:57-root-INFO: Loss too large (1351.345->1492.351)! Learning rate decreased to 0.00545.
2024-12-02-09:18:57-root-INFO: Loss too large (1351.345->1451.513)! Learning rate decreased to 0.00436.
2024-12-02-09:18:57-root-INFO: Loss too large (1351.345->1407.119)! Learning rate decreased to 0.00349.
2024-12-02-09:18:58-root-INFO: Loss too large (1351.345->1367.695)! Learning rate decreased to 0.00279.
2024-12-02-09:18:58-root-INFO: grad norm: 213.708 211.109 33.226
2024-12-02-09:18:58-root-INFO: Loss too large (1340.133->1347.124)! Learning rate decreased to 0.00223.
2024-12-02-09:18:59-root-INFO: grad norm: 164.371 162.758 22.974
2024-12-02-09:18:59-root-INFO: grad norm: 121.316 119.153 22.805
2024-12-02-09:19:00-root-INFO: grad norm: 108.966 107.598 17.211
2024-12-02-09:19:00-root-INFO: Loss Change: 1351.345 -> 1293.924
2024-12-02-09:19:00-root-INFO: Regularization Change: 0.000 -> 0.542
2024-12-02-09:19:00-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-09:19:00-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:19:00-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-09:19:00-root-INFO: grad norm: 73.490 71.352 17.596
2024-12-02-09:19:00-root-INFO: Loss too large (1288.906->1337.612)! Learning rate decreased to 0.00885.
2024-12-02-09:19:01-root-INFO: Loss too large (1288.906->1315.712)! Learning rate decreased to 0.00708.
2024-12-02-09:19:01-root-INFO: Loss too large (1288.906->1301.221)! Learning rate decreased to 0.00567.
2024-12-02-09:19:01-root-INFO: Loss too large (1288.906->1292.096)! Learning rate decreased to 0.00453.
2024-12-02-09:19:01-root-INFO: grad norm: 133.472 132.204 18.356
2024-12-02-09:19:01-root-INFO: Loss too large (1286.737->1326.086)! Learning rate decreased to 0.00363.
2024-12-02-09:19:02-root-INFO: Loss too large (1286.737->1300.010)! Learning rate decreased to 0.00290.
2024-12-02-09:19:02-root-INFO: grad norm: 175.260 173.144 27.150
2024-12-02-09:19:02-root-INFO: Loss too large (1283.836->1291.611)! Learning rate decreased to 0.00232.
2024-12-02-09:19:03-root-INFO: grad norm: 142.342 141.049 19.146
2024-12-02-09:19:03-root-INFO: grad norm: 104.280 102.468 19.351
2024-12-02-09:19:03-root-INFO: Loss Change: 1288.906 -> 1256.911
2024-12-02-09:19:03-root-INFO: Regularization Change: 0.000 -> 0.537
2024-12-02-09:19:03-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-09:19:03-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-09:19:04-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-09:19:04-root-INFO: grad norm: 116.378 114.977 18.001
2024-12-02-09:19:04-root-INFO: Loss too large (1261.059->1437.281)! Learning rate decreased to 0.00919.
2024-12-02-09:19:04-root-INFO: Loss too large (1261.059->1398.847)! Learning rate decreased to 0.00735.
2024-12-02-09:19:04-root-INFO: Loss too large (1261.059->1356.254)! Learning rate decreased to 0.00588.
2024-12-02-09:19:04-root-INFO: Loss too large (1261.059->1316.049)! Learning rate decreased to 0.00471.
2024-12-02-09:19:04-root-INFO: Loss too large (1261.059->1284.628)! Learning rate decreased to 0.00376.
2024-12-02-09:19:05-root-INFO: Loss too large (1261.059->1264.407)! Learning rate decreased to 0.00301.
2024-12-02-09:19:05-root-INFO: grad norm: 139.661 137.837 22.496
2024-12-02-09:19:05-root-INFO: Loss too large (1253.525->1255.905)! Learning rate decreased to 0.00241.
2024-12-02-09:19:06-root-INFO: grad norm: 115.510 114.321 16.529
2024-12-02-09:19:06-root-INFO: grad norm: 87.701 86.086 16.756
2024-12-02-09:19:07-root-INFO: grad norm: 78.331 77.178 13.386
2024-12-02-09:19:07-root-INFO: Loss Change: 1261.059 -> 1223.954
2024-12-02-09:19:07-root-INFO: Regularization Change: 0.000 -> 0.427
2024-12-02-09:19:07-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-09:19:07-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:19:07-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-09:19:07-root-INFO: grad norm: 52.910 50.765 14.914
2024-12-02-09:19:08-root-INFO: grad norm: 85.365 84.273 13.609
2024-12-02-09:19:08-root-INFO: Loss too large (1201.717->1316.310)! Learning rate decreased to 0.00954.
2024-12-02-09:19:08-root-INFO: Loss too large (1201.717->1282.491)! Learning rate decreased to 0.00763.
2024-12-02-09:19:08-root-INFO: Loss too large (1201.717->1251.681)! Learning rate decreased to 0.00611.
2024-12-02-09:19:08-root-INFO: Loss too large (1201.717->1227.295)! Learning rate decreased to 0.00489.
2024-12-02-09:19:09-root-INFO: Loss too large (1201.717->1210.846)! Learning rate decreased to 0.00391.
2024-12-02-09:19:09-root-INFO: grad norm: 157.273 155.610 22.812
2024-12-02-09:19:09-root-INFO: Loss too large (1201.226->1227.620)! Learning rate decreased to 0.00313.
2024-12-02-09:19:09-root-INFO: Loss too large (1201.226->1209.757)! Learning rate decreased to 0.00250.
2024-12-02-09:19:10-root-INFO: grad norm: 125.549 124.370 17.162
2024-12-02-09:19:10-root-INFO: grad norm: 80.766 79.357 15.023
2024-12-02-09:19:11-root-INFO: Loss Change: 1224.401 -> 1178.013
2024-12-02-09:19:11-root-INFO: Regularization Change: 0.000 -> 1.168
2024-12-02-09:19:11-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-09:19:11-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:19:11-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-09:19:11-root-INFO: grad norm: 90.057 88.890 14.448
2024-12-02-09:19:11-root-INFO: Loss too large (1177.632->1339.588)! Learning rate decreased to 0.00991.
2024-12-02-09:19:11-root-INFO: Loss too large (1177.632->1295.603)! Learning rate decreased to 0.00792.
2024-12-02-09:19:11-root-INFO: Loss too large (1177.632->1251.968)! Learning rate decreased to 0.00634.
2024-12-02-09:19:11-root-INFO: Loss too large (1177.632->1215.958)! Learning rate decreased to 0.00507.
2024-12-02-09:19:12-root-INFO: Loss too large (1177.632->1191.698)! Learning rate decreased to 0.00406.
2024-12-02-09:19:12-root-INFO: Loss too large (1177.632->1177.978)! Learning rate decreased to 0.00325.
2024-12-02-09:19:12-root-INFO: grad norm: 107.176 105.733 17.524
2024-12-02-09:19:12-root-INFO: Loss too large (1171.312->1171.638)! Learning rate decreased to 0.00260.
2024-12-02-09:19:13-root-INFO: grad norm: 89.071 88.012 13.700
2024-12-02-09:19:13-root-INFO: grad norm: 66.722 65.332 13.548
2024-12-02-09:19:14-root-INFO: grad norm: 60.163 59.027 11.634
2024-12-02-09:19:14-root-INFO: Loss Change: 1177.632 -> 1148.501
2024-12-02-09:19:14-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-09:19:14-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-09:19:14-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:19:14-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-09:19:14-root-INFO: grad norm: 45.879 44.196 12.311
2024-12-02-09:19:15-root-INFO: grad norm: 55.399 54.323 10.868
2024-12-02-09:19:15-root-INFO: Loss too large (1126.891->1137.217)! Learning rate decreased to 0.01028.
2024-12-02-09:19:16-root-INFO: grad norm: 158.147 156.511 22.689
2024-12-02-09:19:16-root-INFO: Loss too large (1126.737->1336.802)! Learning rate decreased to 0.00822.
2024-12-02-09:19:16-root-INFO: Loss too large (1126.737->1257.524)! Learning rate decreased to 0.00658.
2024-12-02-09:19:16-root-INFO: Loss too large (1126.737->1203.410)! Learning rate decreased to 0.00526.
2024-12-02-09:19:16-root-INFO: Loss too large (1126.737->1166.699)! Learning rate decreased to 0.00421.
2024-12-02-09:19:16-root-INFO: Loss too large (1126.737->1142.099)! Learning rate decreased to 0.00337.
2024-12-02-09:19:17-root-INFO: grad norm: 135.746 134.724 16.623
2024-12-02-09:19:17-root-INFO: grad norm: 117.825 116.420 18.141
2024-12-02-09:19:17-root-INFO: Loss too large (1106.189->1108.753)! Learning rate decreased to 0.00269.
2024-12-02-09:19:18-root-INFO: Loss Change: 1147.334 -> 1102.479
2024-12-02-09:19:18-root-INFO: Regularization Change: 0.000 -> 1.743
2024-12-02-09:19:18-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-09:19:18-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-09:19:18-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-09:19:18-root-INFO: grad norm: 105.482 104.396 15.095
2024-12-02-09:19:18-root-INFO: Loss too large (1105.747->1305.776)! Learning rate decreased to 0.01067.
2024-12-02-09:19:18-root-INFO: Loss too large (1105.747->1260.299)! Learning rate decreased to 0.00853.
2024-12-02-09:19:19-root-INFO: Loss too large (1105.747->1208.517)! Learning rate decreased to 0.00683.
2024-12-02-09:19:19-root-INFO: Loss too large (1105.747->1159.033)! Learning rate decreased to 0.00546.
2024-12-02-09:19:19-root-INFO: Loss too large (1105.747->1122.459)! Learning rate decreased to 0.00437.
2024-12-02-09:19:19-root-INFO: grad norm: 167.775 166.123 23.485
2024-12-02-09:19:20-root-INFO: Loss too large (1101.889->1128.303)! Learning rate decreased to 0.00350.
2024-12-02-09:19:20-root-INFO: Loss too large (1101.889->1109.689)! Learning rate decreased to 0.00280.
2024-12-02-09:19:20-root-INFO: grad norm: 109.052 108.075 14.563
2024-12-02-09:19:21-root-INFO: grad norm: 42.824 41.408 10.922
2024-12-02-09:19:21-root-INFO: grad norm: 39.682 38.357 10.169
2024-12-02-09:19:22-root-INFO: Loss Change: 1105.747 -> 1071.024
2024-12-02-09:19:22-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-02-09:19:22-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-09:19:22-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-09:19:22-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-09:19:22-root-INFO: grad norm: 47.599 46.109 11.814
2024-12-02-09:19:22-root-INFO: Loss too large (1070.016->1074.072)! Learning rate decreased to 0.01107.
2024-12-02-09:19:23-root-INFO: grad norm: 154.355 152.891 21.205
2024-12-02-09:19:23-root-INFO: Loss too large (1066.571->1280.775)! Learning rate decreased to 0.00885.
2024-12-02-09:19:23-root-INFO: Loss too large (1066.571->1199.977)! Learning rate decreased to 0.00708.
2024-12-02-09:19:23-root-INFO: Loss too large (1066.571->1146.142)! Learning rate decreased to 0.00567.
2024-12-02-09:19:23-root-INFO: Loss too large (1066.571->1110.308)! Learning rate decreased to 0.00453.
2024-12-02-09:19:23-root-INFO: Loss too large (1066.571->1086.501)! Learning rate decreased to 0.00363.
2024-12-02-09:19:24-root-INFO: Loss too large (1066.571->1070.882)! Learning rate decreased to 0.00290.
2024-12-02-09:19:24-root-INFO: grad norm: 97.964 97.046 13.383
2024-12-02-09:19:25-root-INFO: grad norm: 39.285 37.916 10.280
2024-12-02-09:19:25-root-INFO: grad norm: 37.013 35.695 9.789
2024-12-02-09:19:25-root-INFO: Loss Change: 1070.016 -> 1037.536
2024-12-02-09:19:25-root-INFO: Regularization Change: 0.000 -> 0.732
2024-12-02-09:19:25-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-09:19:25-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-09:19:26-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-09:19:26-root-INFO: grad norm: 41.550 40.224 10.411
2024-12-02-09:19:26-root-INFO: grad norm: 162.242 160.721 22.166
2024-12-02-09:19:26-root-INFO: Loss too large (1032.987->1402.464)! Learning rate decreased to 0.01148.
2024-12-02-09:19:27-root-INFO: Loss too large (1032.987->1269.598)! Learning rate decreased to 0.00919.
2024-12-02-09:19:27-root-INFO: Loss too large (1032.987->1180.632)! Learning rate decreased to 0.00735.
2024-12-02-09:19:27-root-INFO: Loss too large (1032.987->1121.313)! Learning rate decreased to 0.00588.
2024-12-02-09:19:27-root-INFO: Loss too large (1032.987->1081.850)! Learning rate decreased to 0.00470.
2024-12-02-09:19:27-root-INFO: Loss too large (1032.987->1055.650)! Learning rate decreased to 0.00376.
2024-12-02-09:19:27-root-INFO: Loss too large (1032.987->1038.425)! Learning rate decreased to 0.00301.
2024-12-02-09:19:28-root-INFO: grad norm: 97.798 96.951 12.843
2024-12-02-09:19:28-root-INFO: grad norm: 35.376 33.978 9.847
2024-12-02-09:19:29-root-INFO: grad norm: 34.499 33.153 9.541
2024-12-02-09:19:29-root-INFO: Loss Change: 1036.338 -> 1003.046
2024-12-02-09:19:29-root-INFO: Regularization Change: 0.000 -> 0.880
2024-12-02-09:19:29-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-09:19:29-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-09:19:29-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-09:19:29-root-INFO: grad norm: 45.135 43.868 10.619
2024-12-02-09:19:29-root-INFO: Loss too large (1002.465->1012.743)! Learning rate decreased to 0.01191.
2024-12-02-09:19:30-root-INFO: Loss too large (1002.465->1002.625)! Learning rate decreased to 0.00953.
2024-12-02-09:19:30-root-INFO: grad norm: 114.004 112.847 16.201
2024-12-02-09:19:30-root-INFO: Loss too large (997.869->1069.133)! Learning rate decreased to 0.00762.
2024-12-02-09:19:30-root-INFO: Loss too large (997.869->1038.612)! Learning rate decreased to 0.00610.
2024-12-02-09:19:31-root-INFO: Loss too large (997.869->1018.341)! Learning rate decreased to 0.00488.
2024-12-02-09:19:31-root-INFO: Loss too large (997.869->1005.038)! Learning rate decreased to 0.00390.
2024-12-02-09:19:31-root-INFO: grad norm: 89.057 88.206 12.283
2024-12-02-09:19:32-root-INFO: grad norm: 48.701 47.562 10.472
2024-12-02-09:19:32-root-INFO: grad norm: 44.605 43.573 9.540
2024-12-02-09:19:33-root-INFO: Loss Change: 1002.465 -> 972.003
2024-12-02-09:19:33-root-INFO: Regularization Change: 0.000 -> 0.771
2024-12-02-09:19:33-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-09:19:33-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:19:33-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-09:19:33-root-INFO: grad norm: 36.076 34.802 9.501
2024-12-02-09:19:33-root-INFO: grad norm: 49.055 48.157 9.345
2024-12-02-09:19:34-root-INFO: Loss too large (954.426->988.794)! Learning rate decreased to 0.01235.
2024-12-02-09:19:34-root-INFO: Loss too large (954.426->968.400)! Learning rate decreased to 0.00988.
2024-12-02-09:19:34-root-INFO: Loss too large (954.426->957.045)! Learning rate decreased to 0.00790.
2024-12-02-09:19:34-root-INFO: grad norm: 100.867 99.736 15.059
2024-12-02-09:19:34-root-INFO: Loss too large (951.402->980.939)! Learning rate decreased to 0.00632.
2024-12-02-09:19:35-root-INFO: Loss too large (951.402->965.094)! Learning rate decreased to 0.00506.
2024-12-02-09:19:35-root-INFO: Loss too large (951.402->954.749)! Learning rate decreased to 0.00405.
2024-12-02-09:19:35-root-INFO: grad norm: 77.177 76.394 10.967
2024-12-02-09:19:36-root-INFO: grad norm: 44.810 43.734 9.759
2024-12-02-09:19:36-root-INFO: Loss Change: 970.168 -> 932.220
2024-12-02-09:19:36-root-INFO: Regularization Change: 0.000 -> 1.383
2024-12-02-09:19:36-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-09:19:36-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:19:36-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-09:19:36-root-INFO: grad norm: 55.634 54.669 10.319
2024-12-02-09:19:37-root-INFO: Loss too large (932.574->1014.696)! Learning rate decreased to 0.01281.
2024-12-02-09:19:37-root-INFO: Loss too large (932.574->972.940)! Learning rate decreased to 0.01024.
2024-12-02-09:19:37-root-INFO: Loss too large (932.574->947.401)! Learning rate decreased to 0.00820.
2024-12-02-09:19:37-root-INFO: Loss too large (932.574->934.246)! Learning rate decreased to 0.00656.
2024-12-02-09:19:37-root-INFO: grad norm: 91.336 90.286 13.810
2024-12-02-09:19:38-root-INFO: Loss too large (928.231->938.015)! Learning rate decreased to 0.00525.
2024-12-02-09:19:38-root-INFO: Loss too large (928.231->929.696)! Learning rate decreased to 0.00420.
2024-12-02-09:19:38-root-INFO: grad norm: 68.499 67.699 10.439
2024-12-02-09:19:39-root-INFO: grad norm: 39.171 38.069 9.227
2024-12-02-09:19:39-root-INFO: grad norm: 35.698 34.636 8.644
2024-12-02-09:19:39-root-INFO: Loss Change: 932.574 -> 905.383
2024-12-02-09:19:39-root-INFO: Regularization Change: 0.000 -> 0.593
2024-12-02-09:19:39-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-09:19:39-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:19:40-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-09:19:40-root-INFO: grad norm: 37.141 35.929 9.410
2024-12-02-09:19:40-root-INFO: grad norm: 80.101 79.155 12.275
2024-12-02-09:19:40-root-INFO: Loss too large (893.454->981.233)! Learning rate decreased to 0.01328.
2024-12-02-09:19:41-root-INFO: Loss too large (893.454->943.470)! Learning rate decreased to 0.01062.
2024-12-02-09:19:41-root-INFO: Loss too large (893.454->919.339)! Learning rate decreased to 0.00850.
2024-12-02-09:19:41-root-INFO: Loss too large (893.454->904.130)! Learning rate decreased to 0.00680.
2024-12-02-09:19:41-root-INFO: Loss too large (893.454->894.796)! Learning rate decreased to 0.00544.
2024-12-02-09:19:41-root-INFO: grad norm: 67.081 66.286 10.296
2024-12-02-09:19:42-root-INFO: grad norm: 51.803 50.867 9.806
2024-12-02-09:19:42-root-INFO: grad norm: 48.690 47.848 9.017
2024-12-02-09:19:43-root-INFO: Loss Change: 904.345 -> 869.127
2024-12-02-09:19:43-root-INFO: Regularization Change: 0.000 -> 1.408
2024-12-02-09:19:43-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-09:19:43-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-09:19:43-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-09:19:43-root-INFO: grad norm: 36.191 35.097 8.829
2024-12-02-09:19:43-root-INFO: grad norm: 69.255 68.546 9.885
2024-12-02-09:19:44-root-INFO: Loss too large (860.590->980.045)! Learning rate decreased to 0.01376.
2024-12-02-09:19:44-root-INFO: Loss too large (860.590->924.381)! Learning rate decreased to 0.01101.
2024-12-02-09:19:44-root-INFO: Loss too large (860.590->882.395)! Learning rate decreased to 0.00881.
2024-12-02-09:19:44-root-INFO: grad norm: 136.283 135.047 18.315
2024-12-02-09:19:44-root-INFO: Loss too large (860.099->907.148)! Learning rate decreased to 0.00705.
2024-12-02-09:19:45-root-INFO: Loss too large (860.099->881.411)! Learning rate decreased to 0.00564.
2024-12-02-09:19:45-root-INFO: Loss too large (860.099->864.660)! Learning rate decreased to 0.00451.
2024-12-02-09:19:45-root-INFO: grad norm: 72.534 71.882 9.706
2024-12-02-09:19:46-root-INFO: grad norm: 27.535 26.442 7.681
2024-12-02-09:19:46-root-INFO: Loss Change: 865.332 -> 835.349
2024-12-02-09:19:46-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-02-09:19:46-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-09:19:46-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:19:46-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-09:19:46-root-INFO: grad norm: 32.112 31.104 7.981
2024-12-02-09:19:47-root-INFO: grad norm: 80.963 80.122 11.635
2024-12-02-09:19:47-root-INFO: Loss too large (830.053->916.658)! Learning rate decreased to 0.01426.
2024-12-02-09:19:47-root-INFO: Loss too large (830.053->878.679)! Learning rate decreased to 0.01141.
2024-12-02-09:19:47-root-INFO: Loss too large (830.053->854.489)! Learning rate decreased to 0.00913.
2024-12-02-09:19:47-root-INFO: Loss too large (830.053->839.324)! Learning rate decreased to 0.00730.
2024-12-02-09:19:48-root-INFO: Loss too large (830.053->830.082)! Learning rate decreased to 0.00584.
2024-12-02-09:19:48-root-INFO: grad norm: 53.842 53.139 8.673
2024-12-02-09:19:48-root-INFO: grad norm: 27.425 26.423 7.345
2024-12-02-09:19:49-root-INFO: grad norm: 24.915 23.887 7.083
2024-12-02-09:19:49-root-INFO: Loss Change: 834.340 -> 808.906
2024-12-02-09:19:49-root-INFO: Regularization Change: 0.000 -> 1.039
2024-12-02-09:19:49-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-09:19:49-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:19:49-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-09:19:50-root-INFO: grad norm: 31.654 30.567 8.221
2024-12-02-09:19:50-root-INFO: grad norm: 61.534 60.816 9.369
2024-12-02-09:19:50-root-INFO: Loss too large (804.860->848.508)! Learning rate decreased to 0.01478.
2024-12-02-09:19:50-root-INFO: Loss too large (804.860->824.605)! Learning rate decreased to 0.01182.
2024-12-02-09:19:50-root-INFO: Loss too large (804.860->810.339)! Learning rate decreased to 0.00946.
2024-12-02-09:19:51-root-INFO: grad norm: 62.435 61.701 9.547
2024-12-02-09:19:51-root-INFO: grad norm: 76.610 75.878 10.565
2024-12-02-09:19:52-root-INFO: Loss too large (796.972->802.715)! Learning rate decreased to 0.00757.
2024-12-02-09:19:52-root-INFO: grad norm: 59.370 58.713 8.809
2024-12-02-09:19:52-root-INFO: Loss Change: 809.263 -> 783.020
2024-12-02-09:19:52-root-INFO: Regularization Change: 0.000 -> 1.454
2024-12-02-09:19:52-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-09:19:52-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:19:53-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-09:19:53-root-INFO: grad norm: 27.348 26.351 7.317
2024-12-02-09:19:53-root-INFO: grad norm: 41.065 40.321 7.779
2024-12-02-09:19:53-root-INFO: Loss too large (775.432->802.210)! Learning rate decreased to 0.01531.
2024-12-02-09:19:53-root-INFO: Loss too large (775.432->781.760)! Learning rate decreased to 0.01225.
2024-12-02-09:19:54-root-INFO: grad norm: 72.963 72.218 10.398
2024-12-02-09:19:54-root-INFO: Loss too large (773.026->789.886)! Learning rate decreased to 0.00980.
2024-12-02-09:19:54-root-INFO: Loss too large (773.026->778.694)! Learning rate decreased to 0.00784.
2024-12-02-09:19:55-root-INFO: grad norm: 53.584 52.992 7.946
2024-12-02-09:19:55-root-INFO: grad norm: 26.421 25.535 6.782
2024-12-02-09:19:55-root-INFO: Loss Change: 781.242 -> 756.941
2024-12-02-09:19:55-root-INFO: Regularization Change: 0.000 -> 1.347
2024-12-02-09:19:55-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-09:19:55-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-09:19:56-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-09:19:56-root-INFO: grad norm: 31.365 30.584 6.958
2024-12-02-09:19:56-root-INFO: Loss too large (756.050->759.106)! Learning rate decreased to 0.01586.
2024-12-02-09:19:57-root-INFO: grad norm: 61.341 60.605 9.473
2024-12-02-09:19:57-root-INFO: Loss too large (754.296->775.953)! Learning rate decreased to 0.01269.
2024-12-02-09:19:57-root-INFO: Loss too large (754.296->762.868)! Learning rate decreased to 0.01015.
2024-12-02-09:19:57-root-INFO: Loss too large (754.296->754.672)! Learning rate decreased to 0.00812.
2024-12-02-09:19:58-root-INFO: grad norm: 44.979 44.378 7.327
2024-12-02-09:19:58-root-INFO: grad norm: 29.070 28.255 6.834
2024-12-02-09:19:59-root-INFO: grad norm: 25.816 25.089 6.085
2024-12-02-09:19:59-root-INFO: Loss Change: 756.050 -> 735.862
2024-12-02-09:19:59-root-INFO: Regularization Change: 0.000 -> 0.950
2024-12-02-09:19:59-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-09:19:59-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-09:19:59-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-09:19:59-root-INFO: grad norm: 31.948 30.994 7.751
2024-12-02-09:20:00-root-INFO: grad norm: 50.981 50.350 7.993
2024-12-02-09:20:00-root-INFO: Loss too large (735.981->760.967)! Learning rate decreased to 0.01642.
2024-12-02-09:20:00-root-INFO: Loss too large (735.981->739.819)! Learning rate decreased to 0.01314.
2024-12-02-09:20:00-root-INFO: grad norm: 59.982 59.181 9.771
2024-12-02-09:20:01-root-INFO: Loss too large (729.537->733.045)! Learning rate decreased to 0.01051.
2024-12-02-09:20:01-root-INFO: grad norm: 47.221 46.642 7.371
2024-12-02-09:20:02-root-INFO: grad norm: 32.061 31.402 6.468
2024-12-02-09:20:02-root-INFO: Loss Change: 736.752 -> 712.367
2024-12-02-09:20:02-root-INFO: Regularization Change: 0.000 -> 1.642
2024-12-02-09:20:02-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-09:20:02-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-09:20:02-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-09:20:02-root-INFO: grad norm: 22.931 22.168 5.867
2024-12-02-09:20:03-root-INFO: grad norm: 35.313 34.598 7.070
2024-12-02-09:20:03-root-INFO: Loss too large (704.889->712.547)! Learning rate decreased to 0.01701.
2024-12-02-09:20:03-root-INFO: Loss too large (704.889->706.148)! Learning rate decreased to 0.01361.
2024-12-02-09:20:04-root-INFO: grad norm: 42.718 42.181 6.750
2024-12-02-09:20:04-root-INFO: grad norm: 60.141 59.344 9.760
2024-12-02-09:20:04-root-INFO: Loss too large (701.233->705.043)! Learning rate decreased to 0.01088.
2024-12-02-09:20:05-root-INFO: grad norm: 44.712 44.163 6.980
2024-12-02-09:20:05-root-INFO: Loss Change: 710.226 -> 688.373
2024-12-02-09:20:05-root-INFO: Regularization Change: 0.000 -> 1.674
2024-12-02-09:20:05-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-09:20:05-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-09:20:05-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-09:20:05-root-INFO: grad norm: 40.166 39.585 6.803
2024-12-02-09:20:05-root-INFO: Loss too large (689.690->702.638)! Learning rate decreased to 0.01761.
2024-12-02-09:20:06-root-INFO: Loss too large (689.690->692.120)! Learning rate decreased to 0.01409.
2024-12-02-09:20:06-root-INFO: grad norm: 48.212 47.722 6.857
2024-12-02-09:20:06-root-INFO: Loss too large (686.480->686.587)! Learning rate decreased to 0.01127.
2024-12-02-09:20:07-root-INFO: grad norm: 40.552 40.023 6.527
2024-12-02-09:20:07-root-INFO: grad norm: 33.710 33.191 5.894
2024-12-02-09:20:08-root-INFO: grad norm: 31.643 31.012 6.290
2024-12-02-09:20:08-root-INFO: Loss Change: 689.690 -> 669.195
2024-12-02-09:20:08-root-INFO: Regularization Change: 0.000 -> 1.204
2024-12-02-09:20:08-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-09:20:08-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:20:08-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-09:20:08-root-INFO: grad norm: 26.147 25.450 5.998
2024-12-02-09:20:08-root-INFO: Loss too large (667.828->671.689)! Learning rate decreased to 0.01823.
2024-12-02-09:20:09-root-INFO: grad norm: 62.656 61.756 10.586
2024-12-02-09:20:09-root-INFO: Loss too large (667.042->678.012)! Learning rate decreased to 0.01458.
2024-12-02-09:20:09-root-INFO: Loss too large (667.042->673.112)! Learning rate decreased to 0.01167.
2024-12-02-09:20:09-root-INFO: Loss too large (667.042->668.895)! Learning rate decreased to 0.00933.
2024-12-02-09:20:10-root-INFO: grad norm: 34.849 34.261 6.378
2024-12-02-09:20:10-root-INFO: grad norm: 20.626 19.981 5.117
2024-12-02-09:20:11-root-INFO: grad norm: 17.979 17.175 5.316
2024-12-02-09:20:11-root-INFO: Loss Change: 667.828 -> 650.826
2024-12-02-09:20:11-root-INFO: Regularization Change: 0.000 -> 0.900
2024-12-02-09:20:11-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-09:20:11-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:20:11-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-09:20:11-root-INFO: grad norm: 22.020 21.365 5.330
2024-12-02-09:20:12-root-INFO: grad norm: 41.217 40.620 6.992
2024-12-02-09:20:12-root-INFO: Loss too large (647.081->665.090)! Learning rate decreased to 0.01887.
2024-12-02-09:20:12-root-INFO: Loss too large (647.081->654.992)! Learning rate decreased to 0.01509.
2024-12-02-09:20:12-root-INFO: Loss too large (647.081->648.555)! Learning rate decreased to 0.01207.
2024-12-02-09:20:13-root-INFO: grad norm: 38.520 38.082 5.793
2024-12-02-09:20:13-root-INFO: grad norm: 38.782 38.197 6.707
2024-12-02-09:20:14-root-INFO: grad norm: 36.733 36.261 5.875
2024-12-02-09:20:14-root-INFO: Loss Change: 650.385 -> 633.199
2024-12-02-09:20:14-root-INFO: Regularization Change: 0.000 -> 1.388
2024-12-02-09:20:14-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-09:20:14-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:20:14-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-09:20:15-root-INFO: grad norm: 39.047 38.371 7.238
2024-12-02-09:20:15-root-INFO: Loss too large (633.320->641.347)! Learning rate decreased to 0.01952.
2024-12-02-09:20:15-root-INFO: Loss too large (633.320->636.992)! Learning rate decreased to 0.01562.
2024-12-02-09:20:15-root-INFO: Loss too large (633.320->633.754)! Learning rate decreased to 0.01250.
2024-12-02-09:20:16-root-INFO: grad norm: 32.660 32.027 6.401
2024-12-02-09:20:16-root-INFO: grad norm: 23.178 22.483 5.634
2024-12-02-09:20:16-root-INFO: grad norm: 24.031 23.434 5.325
2024-12-02-09:20:17-root-INFO: grad norm: 27.437 26.759 6.059
2024-12-02-09:20:17-root-INFO: Loss Change: 633.320 -> 617.111
2024-12-02-09:20:17-root-INFO: Regularization Change: 0.000 -> 1.099
2024-12-02-09:20:17-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-09:20:17-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-09:20:17-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-09:20:18-root-INFO: grad norm: 29.296 28.724 5.761
2024-12-02-09:20:18-root-INFO: Loss too large (616.717->628.729)! Learning rate decreased to 0.02020.
2024-12-02-09:20:18-root-INFO: Loss too large (616.717->619.551)! Learning rate decreased to 0.01616.
2024-12-02-09:20:18-root-INFO: grad norm: 48.871 48.199 8.074
2024-12-02-09:20:19-root-INFO: Loss too large (614.761->619.640)! Learning rate decreased to 0.01293.
2024-12-02-09:20:19-root-INFO: Loss too large (614.761->615.612)! Learning rate decreased to 0.01034.
2024-12-02-09:20:19-root-INFO: grad norm: 31.614 31.073 5.823
2024-12-02-09:20:20-root-INFO: grad norm: 15.906 15.215 4.638
2024-12-02-09:20:20-root-INFO: grad norm: 14.996 14.290 4.547
2024-12-02-09:20:20-root-INFO: Loss Change: 616.717 -> 601.182
2024-12-02-09:20:20-root-INFO: Regularization Change: 0.000 -> 0.785
2024-12-02-09:20:20-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-09:20:20-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-09:20:21-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-09:20:21-root-INFO: grad norm: 19.394 18.761 4.915
2024-12-02-09:20:21-root-INFO: grad norm: 31.837 31.357 5.507
2024-12-02-09:20:21-root-INFO: Loss too large (597.131->607.303)! Learning rate decreased to 0.02090.
2024-12-02-09:20:22-root-INFO: Loss too large (597.131->600.343)! Learning rate decreased to 0.01672.
2024-12-02-09:20:22-root-INFO: grad norm: 39.109 38.728 5.450
2024-12-02-09:20:22-root-INFO: grad norm: 53.120 52.746 6.292
2024-12-02-09:20:23-root-INFO: Loss too large (595.778->601.802)! Learning rate decreased to 0.01338.
2024-12-02-09:20:23-root-INFO: grad norm: 47.974 47.522 6.570
2024-12-02-09:20:23-root-INFO: Loss Change: 601.058 -> 587.767
2024-12-02-09:20:23-root-INFO: Regularization Change: 0.000 -> 1.705
2024-12-02-09:20:23-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-09:20:23-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-09:20:24-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-09:20:24-root-INFO: grad norm: 33.434 32.929 5.792
2024-12-02-09:20:24-root-INFO: Loss too large (584.556->605.787)! Learning rate decreased to 0.02162.
2024-12-02-09:20:24-root-INFO: Loss too large (584.556->592.351)! Learning rate decreased to 0.01729.
2024-12-02-09:20:24-root-INFO: Loss too large (584.556->584.815)! Learning rate decreased to 0.01384.
2024-12-02-09:20:25-root-INFO: grad norm: 37.729 37.113 6.785
2024-12-02-09:20:25-root-INFO: grad norm: 34.209 33.662 6.091
2024-12-02-09:20:25-root-INFO: grad norm: 26.619 26.061 5.425
2024-12-02-09:20:26-root-INFO: grad norm: 27.381 26.857 5.330
2024-12-02-09:20:26-root-INFO: Loss Change: 584.556 -> 569.191
2024-12-02-09:20:26-root-INFO: Regularization Change: 0.000 -> 1.097
2024-12-02-09:20:26-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-09:20:26-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-09:20:26-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-09:20:27-root-INFO: grad norm: 40.128 39.558 6.739
2024-12-02-09:20:27-root-INFO: Loss too large (570.868->592.324)! Learning rate decreased to 0.02236.
2024-12-02-09:20:27-root-INFO: Loss too large (570.868->581.201)! Learning rate decreased to 0.01789.
2024-12-02-09:20:27-root-INFO: Loss too large (570.868->574.015)! Learning rate decreased to 0.01431.
2024-12-02-09:20:28-root-INFO: grad norm: 36.975 36.432 6.313
2024-12-02-09:20:28-root-INFO: grad norm: 31.723 31.219 5.634
2024-12-02-09:20:29-root-INFO: grad norm: 31.530 31.015 5.676
2024-12-02-09:20:29-root-INFO: grad norm: 33.251 32.703 6.011
2024-12-02-09:20:29-root-INFO: Loss Change: 570.868 -> 557.037
2024-12-02-09:20:29-root-INFO: Regularization Change: 0.000 -> 1.213
2024-12-02-09:20:29-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-09:20:29-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-09:20:30-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-09:20:30-root-INFO: grad norm: 29.477 28.891 5.849
2024-12-02-09:20:30-root-INFO: Loss too large (555.503->569.681)! Learning rate decreased to 0.02312.
2024-12-02-09:20:30-root-INFO: Loss too large (555.503->558.952)! Learning rate decreased to 0.01849.
2024-12-02-09:20:30-root-INFO: grad norm: 46.762 46.080 7.960
2024-12-02-09:20:31-root-INFO: Loss too large (553.250->558.392)! Learning rate decreased to 0.01479.
2024-12-02-09:20:31-root-INFO: Loss too large (553.250->554.060)! Learning rate decreased to 0.01184.
2024-12-02-09:20:31-root-INFO: grad norm: 29.465 28.867 5.907
2024-12-02-09:20:32-root-INFO: grad norm: 15.046 14.469 4.126
2024-12-02-09:20:32-root-INFO: grad norm: 13.920 13.303 4.098
2024-12-02-09:20:33-root-INFO: Loss Change: 555.503 -> 539.639
2024-12-02-09:20:33-root-INFO: Regularization Change: 0.000 -> 0.874
2024-12-02-09:20:33-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-09:20:33-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-09:20:33-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-09:20:33-root-INFO: grad norm: 19.954 19.429 4.549
2024-12-02-09:20:33-root-INFO: grad norm: 39.868 39.420 5.963
2024-12-02-09:20:34-root-INFO: Loss too large (538.271->567.172)! Learning rate decreased to 0.02390.
2024-12-02-09:20:34-root-INFO: Loss too large (538.271->551.144)! Learning rate decreased to 0.01912.
2024-12-02-09:20:34-root-INFO: Loss too large (538.271->541.447)! Learning rate decreased to 0.01530.
2024-12-02-09:20:34-root-INFO: grad norm: 39.705 39.275 5.823
2024-12-02-09:20:35-root-INFO: grad norm: 37.504 36.974 6.280
2024-12-02-09:20:35-root-INFO: grad norm: 37.946 37.455 6.084
2024-12-02-09:20:36-root-INFO: Loss Change: 540.037 -> 527.491
2024-12-02-09:20:36-root-INFO: Regularization Change: 0.000 -> 1.714
2024-12-02-09:20:36-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-09:20:36-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-09:20:36-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-09:20:36-root-INFO: grad norm: 31.909 31.307 6.170
2024-12-02-09:20:36-root-INFO: Loss too large (525.685->544.064)! Learning rate decreased to 0.02471.
2024-12-02-09:20:36-root-INFO: Loss too large (525.685->530.948)! Learning rate decreased to 0.01976.
2024-12-02-09:20:37-root-INFO: grad norm: 48.466 47.833 7.810
2024-12-02-09:20:37-root-INFO: Loss too large (523.775->529.562)! Learning rate decreased to 0.01581.
2024-12-02-09:20:37-root-INFO: Loss too large (523.775->523.932)! Learning rate decreased to 0.01265.
2024-12-02-09:20:38-root-INFO: grad norm: 29.700 29.064 6.114
2024-12-02-09:20:38-root-INFO: grad norm: 15.233 14.745 3.824
2024-12-02-09:20:39-root-INFO: grad norm: 13.544 12.996 3.816
2024-12-02-09:20:39-root-INFO: Loss Change: 525.685 -> 508.716
2024-12-02-09:20:39-root-INFO: Regularization Change: 0.000 -> 0.965
2024-12-02-09:20:39-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-09:20:39-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-09:20:39-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-09:20:39-root-INFO: grad norm: 19.772 19.340 4.109
2024-12-02-09:20:40-root-INFO: grad norm: 41.561 41.136 5.932
2024-12-02-09:20:40-root-INFO: Loss too large (508.591->542.001)! Learning rate decreased to 0.02553.
2024-12-02-09:20:40-root-INFO: Loss too large (508.591->523.843)! Learning rate decreased to 0.02043.
2024-12-02-09:20:40-root-INFO: Loss too large (508.591->512.730)! Learning rate decreased to 0.01634.
2024-12-02-09:20:41-root-INFO: grad norm: 39.482 39.091 5.546
2024-12-02-09:20:41-root-INFO: grad norm: 35.922 35.380 6.216
2024-12-02-09:20:42-root-INFO: grad norm: 36.156 35.672 5.898
2024-12-02-09:20:42-root-INFO: Loss Change: 509.007 -> 498.021
2024-12-02-09:20:42-root-INFO: Regularization Change: 0.000 -> 1.661
2024-12-02-09:20:42-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-09:20:42-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-09:20:42-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-09:20:42-root-INFO: grad norm: 29.217 28.550 6.208
2024-12-02-09:20:42-root-INFO: Loss too large (496.024->511.525)! Learning rate decreased to 0.02639.
2024-12-02-09:20:43-root-INFO: Loss too large (496.024->499.846)! Learning rate decreased to 0.02111.
2024-12-02-09:20:43-root-INFO: grad norm: 42.829 42.166 7.509
2024-12-02-09:20:43-root-INFO: Loss too large (493.583->498.916)! Learning rate decreased to 0.01689.
2024-12-02-09:20:43-root-INFO: Loss too large (493.583->494.068)! Learning rate decreased to 0.01351.
2024-12-02-09:20:44-root-INFO: grad norm: 27.342 26.659 6.072
2024-12-02-09:20:44-root-INFO: grad norm: 13.904 13.441 3.559
2024-12-02-09:20:45-root-INFO: grad norm: 12.405 11.883 3.562
2024-12-02-09:20:45-root-INFO: Loss Change: 496.024 -> 480.856
2024-12-02-09:20:45-root-INFO: Regularization Change: 0.000 -> 0.912
2024-12-02-09:20:45-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-09:20:45-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-09:20:45-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-09:20:45-root-INFO: grad norm: 17.284 16.846 3.866
2024-12-02-09:20:46-root-INFO: grad norm: 35.054 34.443 6.516
2024-12-02-09:20:46-root-INFO: Loss too large (480.670->512.225)! Learning rate decreased to 0.02726.
2024-12-02-09:20:46-root-INFO: Loss too large (480.670->493.947)! Learning rate decreased to 0.02181.
2024-12-02-09:20:46-root-INFO: Loss too large (480.670->483.305)! Learning rate decreased to 0.01745.
2024-12-02-09:20:47-root-INFO: grad norm: 36.150 35.613 6.205
2024-12-02-09:20:47-root-INFO: grad norm: 34.138 33.477 6.685
2024-12-02-09:20:48-root-INFO: grad norm: 30.962 30.474 5.473
2024-12-02-09:20:48-root-INFO: Loss Change: 480.746 -> 470.833
2024-12-02-09:20:48-root-INFO: Regularization Change: 0.000 -> 1.539
2024-12-02-09:20:48-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-09:20:48-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-09:20:48-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-09:20:48-root-INFO: grad norm: 28.195 27.544 6.024
2024-12-02-09:20:49-root-INFO: Loss too large (469.836->489.556)! Learning rate decreased to 0.02816.
2024-12-02-09:20:49-root-INFO: Loss too large (469.836->477.017)! Learning rate decreased to 0.02253.
2024-12-02-09:20:49-root-INFO: Loss too large (469.836->469.965)! Learning rate decreased to 0.01802.
2024-12-02-09:20:49-root-INFO: grad norm: 29.371 28.802 5.751
2024-12-02-09:20:50-root-INFO: grad norm: 29.240 28.546 6.334
2024-12-02-09:20:50-root-INFO: grad norm: 28.865 28.288 5.740
2024-12-02-09:20:51-root-INFO: grad norm: 29.034 28.338 6.318
2024-12-02-09:20:51-root-INFO: Loss Change: 469.836 -> 458.193
2024-12-02-09:20:51-root-INFO: Regularization Change: 0.000 -> 1.179
2024-12-02-09:20:51-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-09:20:51-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-09:20:51-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-09:20:52-root-INFO: grad norm: 36.362 35.714 6.830
2024-12-02-09:20:52-root-INFO: Loss too large (459.648->486.554)! Learning rate decreased to 0.02909.
2024-12-02-09:20:52-root-INFO: Loss too large (459.648->473.336)! Learning rate decreased to 0.02327.
2024-12-02-09:20:52-root-INFO: Loss too large (459.648->464.581)! Learning rate decreased to 0.01862.
2024-12-02-09:20:52-root-INFO: grad norm: 34.556 33.818 7.104
2024-12-02-09:20:53-root-INFO: grad norm: 31.997 31.435 5.974
2024-12-02-09:20:53-root-INFO: grad norm: 31.359 30.657 6.597
2024-12-02-09:20:54-root-INFO: grad norm: 31.170 30.583 6.021
2024-12-02-09:20:54-root-INFO: Loss Change: 459.648 -> 449.452
2024-12-02-09:20:54-root-INFO: Regularization Change: 0.000 -> 1.313
2024-12-02-09:20:54-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-09:20:54-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-09:20:55-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-09:20:55-root-INFO: grad norm: 26.655 25.916 6.233
2024-12-02-09:20:55-root-INFO: Loss too large (447.539->464.436)! Learning rate decreased to 0.03019.
2024-12-02-09:20:55-root-INFO: Loss too large (447.539->453.099)! Learning rate decreased to 0.02415.
2024-12-02-09:20:56-root-INFO: grad norm: 41.882 41.030 8.400
2024-12-02-09:20:56-root-INFO: Loss too large (446.728->453.811)! Learning rate decreased to 0.01932.
2024-12-02-09:20:56-root-INFO: Loss too large (446.728->448.172)! Learning rate decreased to 0.01546.
2024-12-02-09:20:56-root-INFO: grad norm: 27.835 27.075 6.457
2024-12-02-09:20:57-root-INFO: grad norm: 13.999 13.593 3.347
2024-12-02-09:20:57-root-INFO: grad norm: 12.275 11.802 3.375
2024-12-02-09:20:58-root-INFO: Loss Change: 447.539 -> 434.615
2024-12-02-09:20:58-root-INFO: Regularization Change: 0.000 -> 0.902
2024-12-02-09:20:58-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-09:20:58-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-09:20:58-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-09:20:58-root-INFO: grad norm: 16.164 15.759 3.593
2024-12-02-09:20:58-root-INFO: Loss too large (434.840->436.593)! Learning rate decreased to 0.03117.
2024-12-02-09:20:59-root-INFO: grad norm: 28.331 27.707 5.913
2024-12-02-09:20:59-root-INFO: Loss too large (434.486->446.181)! Learning rate decreased to 0.02494.
2024-12-02-09:20:59-root-INFO: Loss too large (434.486->437.687)! Learning rate decreased to 0.01995.
2024-12-02-09:21:00-root-INFO: grad norm: 32.975 32.343 6.424
2024-12-02-09:21:00-root-INFO: Loss too large (433.047->433.591)! Learning rate decreased to 0.01596.
2024-12-02-09:21:00-root-INFO: grad norm: 24.949 24.253 5.853
2024-12-02-09:21:01-root-INFO: grad norm: 17.833 17.386 3.968
2024-12-02-09:21:01-root-INFO: Loss Change: 434.840 -> 425.354
2024-12-02-09:21:01-root-INFO: Regularization Change: 0.000 -> 0.995
2024-12-02-09:21:01-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-09:21:01-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-09:21:01-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-09:21:01-root-INFO: grad norm: 14.527 14.063 3.641
2024-12-02-09:21:01-root-INFO: Loss too large (424.856->425.896)! Learning rate decreased to 0.03218.
2024-12-02-09:21:02-root-INFO: grad norm: 28.774 28.171 5.859
2024-12-02-09:21:02-root-INFO: Loss too large (424.073->432.237)! Learning rate decreased to 0.02574.
2024-12-02-09:21:02-root-INFO: Loss too large (424.073->427.777)! Learning rate decreased to 0.02059.
2024-12-02-09:21:02-root-INFO: Loss too large (424.073->424.480)! Learning rate decreased to 0.01647.
2024-12-02-09:21:03-root-INFO: grad norm: 22.516 21.847 5.447
2024-12-02-09:21:03-root-INFO: grad norm: 16.253 15.808 3.776
2024-12-02-09:21:04-root-INFO: grad norm: 14.897 14.389 3.858
2024-12-02-09:21:04-root-INFO: Loss Change: 424.856 -> 415.867
2024-12-02-09:21:04-root-INFO: Regularization Change: 0.000 -> 0.897
2024-12-02-09:21:04-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-09:21:04-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-09:21:04-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-09:21:04-root-INFO: grad norm: 19.236 18.770 4.209
2024-12-02-09:21:04-root-INFO: Loss too large (416.102->423.209)! Learning rate decreased to 0.03321.
2024-12-02-09:21:05-root-INFO: Loss too large (416.102->419.175)! Learning rate decreased to 0.02657.
2024-12-02-09:21:05-root-INFO: Loss too large (416.102->416.596)! Learning rate decreased to 0.02126.
2024-12-02-09:21:05-root-INFO: grad norm: 21.952 21.328 5.198
2024-12-02-09:21:06-root-INFO: grad norm: 27.918 27.300 5.843
2024-12-02-09:21:06-root-INFO: Loss too large (413.938->414.802)! Learning rate decreased to 0.01700.
2024-12-02-09:21:06-root-INFO: grad norm: 22.881 22.214 5.482
2024-12-02-09:21:07-root-INFO: grad norm: 17.341 16.899 3.889
2024-12-02-09:21:07-root-INFO: Loss Change: 416.102 -> 407.921
2024-12-02-09:21:07-root-INFO: Regularization Change: 0.000 -> 0.807
2024-12-02-09:21:07-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-09:21:07-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-09:21:07-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-09:21:07-root-INFO: grad norm: 15.641 15.184 3.753
2024-12-02-09:21:08-root-INFO: Loss too large (407.840->411.906)! Learning rate decreased to 0.03427.
2024-12-02-09:21:08-root-INFO: Loss too large (407.840->408.714)! Learning rate decreased to 0.02742.
2024-12-02-09:21:08-root-INFO: grad norm: 25.395 24.870 5.138
2024-12-02-09:21:08-root-INFO: Loss too large (406.968->410.163)! Learning rate decreased to 0.02194.
2024-12-02-09:21:08-root-INFO: Loss too large (406.968->407.313)! Learning rate decreased to 0.01755.
2024-12-02-09:21:09-root-INFO: grad norm: 20.866 20.262 4.985
2024-12-02-09:21:09-root-INFO: grad norm: 16.319 15.897 3.686
2024-12-02-09:21:10-root-INFO: grad norm: 15.177 14.686 3.829
2024-12-02-09:21:10-root-INFO: Loss Change: 407.840 -> 399.741
2024-12-02-09:21:10-root-INFO: Regularization Change: 0.000 -> 0.804
2024-12-02-09:21:10-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-09:21:10-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-09:21:10-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-09:21:11-root-INFO: grad norm: 19.944 19.492 4.223
2024-12-02-09:21:11-root-INFO: Loss too large (400.268->408.793)! Learning rate decreased to 0.03536.
2024-12-02-09:21:11-root-INFO: Loss too large (400.268->404.023)! Learning rate decreased to 0.02829.
2024-12-02-09:21:11-root-INFO: Loss too large (400.268->400.974)! Learning rate decreased to 0.02263.
2024-12-02-09:21:11-root-INFO: grad norm: 22.215 21.617 5.122
2024-12-02-09:21:12-root-INFO: grad norm: 27.459 26.900 5.510
2024-12-02-09:21:12-root-INFO: Loss too large (398.035->398.756)! Learning rate decreased to 0.01811.
2024-12-02-09:21:13-root-INFO: grad norm: 22.066 21.444 5.204
2024-12-02-09:21:13-root-INFO: grad norm: 16.621 16.220 3.629
2024-12-02-09:21:13-root-INFO: Loss Change: 400.268 -> 391.905
2024-12-02-09:21:13-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-09:21:13-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-09:21:13-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-09:21:14-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-09:21:14-root-INFO: grad norm: 12.976 12.564 3.244
2024-12-02-09:21:14-root-INFO: Loss too large (391.512->393.287)! Learning rate decreased to 0.03648.
2024-12-02-09:21:14-root-INFO: grad norm: 27.112 26.593 5.279
2024-12-02-09:21:15-root-INFO: Loss too large (391.396->399.597)! Learning rate decreased to 0.02919.
2024-12-02-09:21:15-root-INFO: Loss too large (391.396->395.249)! Learning rate decreased to 0.02335.
2024-12-02-09:21:15-root-INFO: Loss too large (391.396->391.888)! Learning rate decreased to 0.01868.
2024-12-02-09:21:16-root-INFO: grad norm: 21.156 20.585 4.881
2024-12-02-09:21:16-root-INFO: grad norm: 15.459 15.080 3.402
2024-12-02-09:21:17-root-INFO: grad norm: 13.987 13.553 3.455
2024-12-02-09:21:17-root-INFO: Loss Change: 391.512 -> 383.693
2024-12-02-09:21:17-root-INFO: Regularization Change: 0.000 -> 0.900
2024-12-02-09:21:17-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-09:21:17-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-09:21:17-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-09:21:17-root-INFO: grad norm: 18.053 17.678 3.658
2024-12-02-09:21:17-root-INFO: Loss too large (384.360->391.675)! Learning rate decreased to 0.03763.
2024-12-02-09:21:17-root-INFO: Loss too large (384.360->387.226)! Learning rate decreased to 0.03011.
2024-12-02-09:21:18-root-INFO: Loss too large (384.360->384.531)! Learning rate decreased to 0.02409.
2024-12-02-09:21:18-root-INFO: grad norm: 19.766 19.296 4.287
2024-12-02-09:21:19-root-INFO: grad norm: 23.980 23.520 4.674
2024-12-02-09:21:19-root-INFO: Loss too large (382.012->382.178)! Learning rate decreased to 0.01927.
2024-12-02-09:21:19-root-INFO: grad norm: 19.432 18.922 4.423
2024-12-02-09:21:20-root-INFO: grad norm: 15.350 14.989 3.309
2024-12-02-09:21:20-root-INFO: Loss Change: 384.360 -> 376.493
2024-12-02-09:21:20-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-02-09:21:20-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-09:21:20-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-09:21:20-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-09:21:20-root-INFO: grad norm: 13.029 12.637 3.169
2024-12-02-09:21:21-root-INFO: Loss too large (375.997->377.946)! Learning rate decreased to 0.03881.
2024-12-02-09:21:21-root-INFO: grad norm: 26.007 25.567 4.762
2024-12-02-09:21:21-root-INFO: Loss too large (375.926->383.390)! Learning rate decreased to 0.03105.
2024-12-02-09:21:21-root-INFO: Loss too large (375.926->379.195)! Learning rate decreased to 0.02484.
2024-12-02-09:21:21-root-INFO: Loss too large (375.926->375.964)! Learning rate decreased to 0.01987.
2024-12-02-09:21:22-root-INFO: grad norm: 19.633 19.121 4.455
2024-12-02-09:21:22-root-INFO: grad norm: 14.322 13.977 3.124
2024-12-02-09:21:23-root-INFO: grad norm: 12.826 12.428 3.171
2024-12-02-09:21:23-root-INFO: Loss Change: 375.997 -> 368.234
2024-12-02-09:21:23-root-INFO: Regularization Change: 0.000 -> 0.931
2024-12-02-09:21:23-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-09:21:23-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-09:21:23-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-09:21:23-root-INFO: grad norm: 16.090 15.750 3.291
2024-12-02-09:21:24-root-INFO: Loss too large (369.014->374.845)! Learning rate decreased to 0.04002.
2024-12-02-09:21:24-root-INFO: Loss too large (369.014->371.291)! Learning rate decreased to 0.03202.
2024-12-02-09:21:24-root-INFO: Loss too large (369.014->369.096)! Learning rate decreased to 0.02561.
2024-12-02-09:21:24-root-INFO: grad norm: 17.938 17.497 3.954
2024-12-02-09:21:25-root-INFO: grad norm: 21.506 21.099 4.162
2024-12-02-09:21:25-root-INFO: grad norm: 22.867 22.315 4.991
2024-12-02-09:21:26-root-INFO: grad norm: 24.188 23.754 4.564
2024-12-02-09:21:26-root-INFO: Loss Change: 369.014 -> 364.572
2024-12-02-09:21:26-root-INFO: Regularization Change: 0.000 -> 1.218
2024-12-02-09:21:26-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-09:21:26-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-09:21:26-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-09:21:26-root-INFO: grad norm: 22.551 21.979 5.047
2024-12-02-09:21:27-root-INFO: Loss too large (363.805->379.157)! Learning rate decreased to 0.04126.
2024-12-02-09:21:27-root-INFO: Loss too large (363.805->369.625)! Learning rate decreased to 0.03301.
2024-12-02-09:21:27-root-INFO: Loss too large (363.805->363.858)! Learning rate decreased to 0.02641.
2024-12-02-09:21:27-root-INFO: grad norm: 21.275 20.881 4.076
2024-12-02-09:21:28-root-INFO: grad norm: 20.949 20.401 4.759
2024-12-02-09:21:28-root-INFO: grad norm: 20.372 19.985 3.952
2024-12-02-09:21:29-root-INFO: grad norm: 20.145 19.607 4.624
2024-12-02-09:21:29-root-INFO: Loss Change: 363.805 -> 354.675
2024-12-02-09:21:29-root-INFO: Regularization Change: 0.000 -> 1.278
2024-12-02-09:21:29-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-09:21:29-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-09:21:29-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-09:21:29-root-INFO: grad norm: 24.191 23.775 4.467
2024-12-02-09:21:30-root-INFO: Loss too large (355.574->371.437)! Learning rate decreased to 0.04253.
2024-12-02-09:21:30-root-INFO: Loss too large (355.574->363.440)! Learning rate decreased to 0.03403.
2024-12-02-09:21:30-root-INFO: Loss too large (355.574->357.928)! Learning rate decreased to 0.02722.
2024-12-02-09:21:30-root-INFO: grad norm: 22.923 22.378 4.969
2024-12-02-09:21:31-root-INFO: grad norm: 21.405 21.023 4.026
2024-12-02-09:21:31-root-INFO: grad norm: 20.610 20.091 4.596
2024-12-02-09:21:32-root-INFO: grad norm: 19.654 19.285 3.790
2024-12-02-09:21:32-root-INFO: Loss Change: 355.574 -> 347.388
2024-12-02-09:21:32-root-INFO: Regularization Change: 0.000 -> 1.296
2024-12-02-09:21:32-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-09:21:32-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-09:21:32-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-09:21:32-root-INFO: grad norm: 16.814 16.325 4.026
2024-12-02-09:21:33-root-INFO: Loss too large (346.403->353.751)! Learning rate decreased to 0.04384.
2024-12-02-09:21:33-root-INFO: Loss too large (346.403->348.782)! Learning rate decreased to 0.03507.
2024-12-02-09:21:33-root-INFO: grad norm: 22.901 22.484 4.351
2024-12-02-09:21:33-root-INFO: Loss too large (345.890->347.459)! Learning rate decreased to 0.02806.
2024-12-02-09:21:34-root-INFO: grad norm: 20.430 19.865 4.772
2024-12-02-09:21:34-root-INFO: grad norm: 17.828 17.470 3.557
2024-12-02-09:21:35-root-INFO: grad norm: 16.920 16.434 4.028
2024-12-02-09:21:35-root-INFO: Loss Change: 346.403 -> 338.587
2024-12-02-09:21:35-root-INFO: Regularization Change: 0.000 -> 1.277
2024-12-02-09:21:35-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-09:21:35-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-09:21:35-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-09:21:35-root-INFO: grad norm: 19.889 19.527 3.777
2024-12-02-09:21:36-root-INFO: Loss too large (339.347->350.261)! Learning rate decreased to 0.04517.
2024-12-02-09:21:36-root-INFO: Loss too large (339.347->344.145)! Learning rate decreased to 0.03614.
2024-12-02-09:21:36-root-INFO: Loss too large (339.347->340.136)! Learning rate decreased to 0.02891.
2024-12-02-09:21:36-root-INFO: grad norm: 18.461 17.969 4.237
2024-12-02-09:21:37-root-INFO: grad norm: 17.084 16.749 3.369
2024-12-02-09:21:37-root-INFO: grad norm: 16.153 15.696 3.815
2024-12-02-09:21:38-root-INFO: grad norm: 15.232 14.911 3.112
2024-12-02-09:21:38-root-INFO: Loss Change: 339.347 -> 331.380
2024-12-02-09:21:38-root-INFO: Regularization Change: 0.000 -> 1.199
2024-12-02-09:21:38-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-09:21:38-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-09:21:38-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-09:21:38-root-INFO: grad norm: 12.817 12.408 3.212
2024-12-02-09:21:39-root-INFO: Loss too large (330.783->333.647)! Learning rate decreased to 0.04654.
2024-12-02-09:21:39-root-INFO: Loss too large (330.783->331.143)! Learning rate decreased to 0.03724.
2024-12-02-09:21:39-root-INFO: grad norm: 16.579 16.231 3.375
2024-12-02-09:21:39-root-INFO: Loss too large (329.783->329.889)! Learning rate decreased to 0.02979.
2024-12-02-09:21:40-root-INFO: grad norm: 15.382 14.921 3.739
2024-12-02-09:21:40-root-INFO: grad norm: 14.210 13.888 3.008
2024-12-02-09:21:41-root-INFO: grad norm: 13.444 13.027 3.324
2024-12-02-09:21:41-root-INFO: Loss Change: 330.783 -> 323.811
2024-12-02-09:21:41-root-INFO: Regularization Change: 0.000 -> 1.196
2024-12-02-09:21:41-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-09:21:41-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-09:21:41-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-09:21:41-root-INFO: grad norm: 16.510 16.196 3.207
2024-12-02-09:21:42-root-INFO: Loss too large (324.662->332.429)! Learning rate decreased to 0.04795.
2024-12-02-09:21:42-root-INFO: Loss too large (324.662->327.555)! Learning rate decreased to 0.03836.
2024-12-02-09:21:42-root-INFO: grad norm: 21.220 20.732 4.526
2024-12-02-09:21:42-root-INFO: Loss too large (324.616->325.512)! Learning rate decreased to 0.03069.
2024-12-02-09:21:43-root-INFO: grad norm: 18.975 18.654 3.477
2024-12-02-09:21:43-root-INFO: grad norm: 17.441 17.028 3.771
2024-12-02-09:21:44-root-INFO: grad norm: 16.094 15.804 3.044
2024-12-02-09:21:44-root-INFO: Loss Change: 324.662 -> 317.673
2024-12-02-09:21:44-root-INFO: Regularization Change: 0.000 -> 1.294
2024-12-02-09:21:44-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-09:21:44-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-09:21:44-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-09:21:44-root-INFO: grad norm: 13.321 12.957 3.092
2024-12-02-09:21:45-root-INFO: Loss too large (316.858->321.039)! Learning rate decreased to 0.04939.
2024-12-02-09:21:45-root-INFO: Loss too large (316.858->317.881)! Learning rate decreased to 0.03951.
2024-12-02-09:21:45-root-INFO: grad norm: 17.069 16.766 3.199
2024-12-02-09:21:45-root-INFO: Loss too large (316.128->316.459)! Learning rate decreased to 0.03161.
2024-12-02-09:21:46-root-INFO: grad norm: 15.657 15.286 3.388
2024-12-02-09:21:46-root-INFO: grad norm: 14.503 14.237 2.764
2024-12-02-09:21:47-root-INFO: grad norm: 13.696 13.382 2.919
2024-12-02-09:21:47-root-INFO: Loss Change: 316.858 -> 310.312
2024-12-02-09:21:47-root-INFO: Regularization Change: 0.000 -> 1.216
2024-12-02-09:21:47-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-09:21:47-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-09:21:47-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-09:21:47-root-INFO: grad norm: 18.240 18.001 2.945
2024-12-02-09:21:48-root-INFO: Loss too large (311.615->323.136)! Learning rate decreased to 0.05086.
2024-12-02-09:21:48-root-INFO: Loss too large (311.615->316.555)! Learning rate decreased to 0.04069.
2024-12-02-09:21:48-root-INFO: Loss too large (311.615->312.401)! Learning rate decreased to 0.03255.
2024-12-02-09:21:48-root-INFO: grad norm: 17.357 17.080 3.088
2024-12-02-09:21:49-root-INFO: grad norm: 16.680 16.461 2.697
2024-12-02-09:21:49-root-INFO: grad norm: 16.092 15.829 2.895
2024-12-02-09:21:50-root-INFO: grad norm: 15.551 15.345 2.526
2024-12-02-09:21:50-root-INFO: Loss Change: 311.615 -> 304.725
2024-12-02-09:21:50-root-INFO: Regularization Change: 0.000 -> 1.246
2024-12-02-09:21:50-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-09:21:50-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-09:21:50-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-09:21:51-root-INFO: grad norm: 12.544 12.328 2.316
2024-12-02-09:21:51-root-INFO: Loss too large (303.666->307.466)! Learning rate decreased to 0.05237.
2024-12-02-09:21:51-root-INFO: Loss too large (303.666->304.716)! Learning rate decreased to 0.04190.
2024-12-02-09:21:51-root-INFO: grad norm: 16.399 16.224 2.395
2024-12-02-09:21:51-root-INFO: Loss too large (303.117->303.578)! Learning rate decreased to 0.03352.
2024-12-02-09:21:52-root-INFO: grad norm: 15.580 15.383 2.471
2024-12-02-09:21:52-root-INFO: grad norm: 15.018 14.850 2.241
2024-12-02-09:21:53-root-INFO: grad norm: 14.572 14.385 2.330
2024-12-02-09:21:53-root-INFO: Loss Change: 303.666 -> 297.935
2024-12-02-09:21:53-root-INFO: Regularization Change: 0.000 -> 1.251
2024-12-02-09:21:53-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-09:21:53-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-09:21:53-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-09:21:53-root-INFO: grad norm: 17.905 17.721 2.557
2024-12-02-09:21:54-root-INFO: Loss too large (299.119->310.367)! Learning rate decreased to 0.05391.
2024-12-02-09:21:54-root-INFO: Loss too large (299.119->304.262)! Learning rate decreased to 0.04313.
2024-12-02-09:21:54-root-INFO: Loss too large (299.119->300.127)! Learning rate decreased to 0.03450.
2024-12-02-09:21:54-root-INFO: grad norm: 17.194 17.016 2.467
2024-12-02-09:21:55-root-INFO: grad norm: 16.536 16.376 2.291
2024-12-02-09:21:55-root-INFO: grad norm: 15.947 15.773 2.350
2024-12-02-09:21:56-root-INFO: grad norm: 15.452 15.300 2.159
2024-12-02-09:21:56-root-INFO: Loss Change: 299.119 -> 292.591
2024-12-02-09:21:56-root-INFO: Regularization Change: 0.000 -> 1.271
2024-12-02-09:21:56-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-09:21:56-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-09:21:56-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-09:21:57-root-INFO: grad norm: 12.563 12.394 2.052
2024-12-02-09:21:57-root-INFO: Loss too large (291.814->296.294)! Learning rate decreased to 0.05550.
2024-12-02-09:21:57-root-INFO: Loss too large (291.814->293.349)! Learning rate decreased to 0.04440.
2024-12-02-09:21:57-root-INFO: grad norm: 16.422 16.294 2.046
2024-12-02-09:21:57-root-INFO: Loss too large (291.567->292.059)! Learning rate decreased to 0.03552.
2024-12-02-09:21:58-root-INFO: grad norm: 15.402 15.263 2.065
2024-12-02-09:21:58-root-INFO: grad norm: 14.804 14.675 1.951
2024-12-02-09:21:59-root-INFO: grad norm: 14.293 14.154 1.991
2024-12-02-09:21:59-root-INFO: Loss Change: 291.814 -> 286.459
2024-12-02-09:21:59-root-INFO: Regularization Change: 0.000 -> 1.268
2024-12-02-09:21:59-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-09:21:59-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-09:21:59-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-09:22:00-root-INFO: grad norm: 16.898 16.765 2.117
2024-12-02-09:22:00-root-INFO: Loss too large (287.273->297.458)! Learning rate decreased to 0.05711.
2024-12-02-09:22:00-root-INFO: Loss too large (287.273->291.814)! Learning rate decreased to 0.04569.
2024-12-02-09:22:00-root-INFO: Loss too large (287.273->287.946)! Learning rate decreased to 0.03655.
2024-12-02-09:22:00-root-INFO: grad norm: 15.816 15.681 2.061
2024-12-02-09:22:01-root-INFO: grad norm: 14.997 14.872 1.934
2024-12-02-09:22:01-root-INFO: grad norm: 14.296 14.158 1.983
2024-12-02-09:22:02-root-INFO: grad norm: 13.796 13.670 1.859
2024-12-02-09:22:02-root-INFO: Loss Change: 287.273 -> 280.760
2024-12-02-09:22:02-root-INFO: Regularization Change: 0.000 -> 1.259
2024-12-02-09:22:02-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-09:22:02-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-09:22:02-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-09:22:03-root-INFO: grad norm: 11.998 11.863 1.793
2024-12-02-09:22:03-root-INFO: Loss too large (280.058->284.658)! Learning rate decreased to 0.05877.
2024-12-02-09:22:03-root-INFO: Loss too large (280.058->281.738)! Learning rate decreased to 0.04702.
2024-12-02-09:22:03-root-INFO: grad norm: 15.949 15.843 1.832
2024-12-02-09:22:03-root-INFO: Loss too large (279.960->280.575)! Learning rate decreased to 0.03761.
2024-12-02-09:22:04-root-INFO: grad norm: 15.146 15.042 1.768
2024-12-02-09:22:04-root-INFO: grad norm: 14.626 14.519 1.773
2024-12-02-09:22:05-root-INFO: grad norm: 14.144 14.036 1.748
2024-12-02-09:22:05-root-INFO: Loss Change: 280.058 -> 275.150
2024-12-02-09:22:05-root-INFO: Regularization Change: 0.000 -> 1.294
2024-12-02-09:22:05-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-09:22:05-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-09:22:05-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-09:22:06-root-INFO: grad norm: 16.670 16.541 2.068
2024-12-02-09:22:06-root-INFO: Loss too large (276.091->286.545)! Learning rate decreased to 0.06047.
2024-12-02-09:22:06-root-INFO: Loss too large (276.091->280.827)! Learning rate decreased to 0.04837.
2024-12-02-09:22:06-root-INFO: Loss too large (276.091->276.832)! Learning rate decreased to 0.03870.
2024-12-02-09:22:07-root-INFO: grad norm: 15.631 15.517 1.884
2024-12-02-09:22:07-root-INFO: grad norm: 14.846 14.732 1.832
2024-12-02-09:22:07-root-INFO: grad norm: 14.139 14.020 1.830
2024-12-02-09:22:08-root-INFO: grad norm: 13.650 13.535 1.763
2024-12-02-09:22:08-root-INFO: Loss Change: 276.091 -> 269.705
2024-12-02-09:22:08-root-INFO: Regularization Change: 0.000 -> 1.305
2024-12-02-09:22:08-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-09:22:08-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-09:22:08-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-09:22:09-root-INFO: grad norm: 11.518 11.394 1.690
2024-12-02-09:22:09-root-INFO: Loss too large (269.238->273.670)! Learning rate decreased to 0.06220.
2024-12-02-09:22:09-root-INFO: Loss too large (269.238->270.895)! Learning rate decreased to 0.04976.
2024-12-02-09:22:09-root-INFO: grad norm: 14.955 14.855 1.724
2024-12-02-09:22:09-root-INFO: Loss too large (269.187->269.485)! Learning rate decreased to 0.03981.
2024-12-02-09:22:10-root-INFO: grad norm: 13.876 13.774 1.681
2024-12-02-09:22:10-root-INFO: grad norm: 13.322 13.216 1.677
2024-12-02-09:22:11-root-INFO: grad norm: 12.803 12.695 1.656
2024-12-02-09:22:11-root-INFO: Loss Change: 269.238 -> 264.414
2024-12-02-09:22:11-root-INFO: Regularization Change: 0.000 -> 1.291
2024-12-02-09:22:11-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-09:22:11-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-09:22:11-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-09:22:12-root-INFO: grad norm: 14.848 14.726 1.901
2024-12-02-09:22:12-root-INFO: Loss too large (264.971->273.829)! Learning rate decreased to 0.06397.
2024-12-02-09:22:12-root-INFO: Loss too large (264.971->268.786)! Learning rate decreased to 0.05118.
2024-12-02-09:22:12-root-INFO: Loss too large (264.971->265.355)! Learning rate decreased to 0.04094.
2024-12-02-09:22:13-root-INFO: grad norm: 13.943 13.831 1.762
2024-12-02-09:22:13-root-INFO: grad norm: 13.301 13.191 1.710
2024-12-02-09:22:13-root-INFO: grad norm: 12.695 12.581 1.694
2024-12-02-09:22:14-root-INFO: grad norm: 12.300 12.189 1.644
2024-12-02-09:22:14-root-INFO: Loss Change: 264.971 -> 258.967
2024-12-02-09:22:14-root-INFO: Regularization Change: 0.000 -> 1.283
2024-12-02-09:22:14-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-09:22:14-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-09:22:14-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-09:22:15-root-INFO: grad norm: 10.565 10.441 1.609
2024-12-02-09:22:15-root-INFO: Loss too large (258.597->262.201)! Learning rate decreased to 0.06579.
2024-12-02-09:22:15-root-INFO: Loss too large (258.597->259.862)! Learning rate decreased to 0.05263.
2024-12-02-09:22:15-root-INFO: grad norm: 13.609 13.509 1.647
2024-12-02-09:22:16-root-INFO: Loss too large (258.428->258.586)! Learning rate decreased to 0.04210.
2024-12-02-09:22:16-root-INFO: grad norm: 12.795 12.700 1.553
2024-12-02-09:22:16-root-INFO: grad norm: 12.390 12.289 1.579
2024-12-02-09:22:17-root-INFO: grad norm: 11.974 11.876 1.528
2024-12-02-09:22:17-root-INFO: Loss Change: 258.597 -> 254.031
2024-12-02-09:22:17-root-INFO: Regularization Change: 0.000 -> 1.304
2024-12-02-09:22:17-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-09:22:17-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-09:22:17-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-09:22:17-root-INFO: grad norm: 14.314 14.198 1.816
2024-12-02-09:22:18-root-INFO: Loss too large (254.596->263.357)! Learning rate decreased to 0.06764.
2024-12-02-09:22:18-root-INFO: Loss too large (254.596->258.392)! Learning rate decreased to 0.05411.
2024-12-02-09:22:18-root-INFO: Loss too large (254.596->254.966)! Learning rate decreased to 0.04329.
2024-12-02-09:22:18-root-INFO: grad norm: 13.562 13.466 1.608
2024-12-02-09:22:19-root-INFO: grad norm: 13.002 12.905 1.593
2024-12-02-09:22:19-root-INFO: grad norm: 12.402 12.305 1.551
2024-12-02-09:22:20-root-INFO: grad norm: 12.038 11.941 1.529
2024-12-02-09:22:20-root-INFO: Loss Change: 254.596 -> 248.790
2024-12-02-09:22:20-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-02-09:22:20-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-09:22:20-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-09:22:20-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-09:22:20-root-INFO: grad norm: 10.228 10.118 1.493
2024-12-02-09:22:21-root-INFO: Loss too large (248.350->251.972)! Learning rate decreased to 0.06953.
2024-12-02-09:22:21-root-INFO: Loss too large (248.350->249.684)! Learning rate decreased to 0.05563.
2024-12-02-09:22:21-root-INFO: grad norm: 12.919 12.827 1.545
2024-12-02-09:22:22-root-INFO: grad norm: 16.898 16.829 1.519
2024-12-02-09:22:22-root-INFO: Loss too large (248.188->249.478)! Learning rate decreased to 0.04450.
2024-12-02-09:22:22-root-INFO: grad norm: 14.180 14.097 1.533
2024-12-02-09:22:23-root-INFO: grad norm: 11.857 11.769 1.440
2024-12-02-09:22:23-root-INFO: Loss Change: 248.350 -> 243.830
2024-12-02-09:22:23-root-INFO: Regularization Change: 0.000 -> 1.424
2024-12-02-09:22:23-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-09:22:23-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-09:22:23-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-09:22:23-root-INFO: grad norm: 13.340 13.224 1.760
2024-12-02-09:22:24-root-INFO: Loss too large (244.705->252.463)! Learning rate decreased to 0.07147.
2024-12-02-09:22:24-root-INFO: Loss too large (244.705->247.769)! Learning rate decreased to 0.05718.
2024-12-02-09:22:24-root-INFO: grad norm: 17.209 17.126 1.695
2024-12-02-09:22:24-root-INFO: Loss too large (244.656->246.025)! Learning rate decreased to 0.04574.
2024-12-02-09:22:25-root-INFO: grad norm: 14.227 14.137 1.599
2024-12-02-09:22:25-root-INFO: grad norm: 11.673 11.581 1.461
2024-12-02-09:22:26-root-INFO: grad norm: 10.668 10.573 1.427
2024-12-02-09:22:26-root-INFO: Loss Change: 244.705 -> 238.770
2024-12-02-09:22:26-root-INFO: Regularization Change: 0.000 -> 1.386
2024-12-02-09:22:26-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-09:22:26-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-09:22:26-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-09:22:26-root-INFO: grad norm: 8.324 8.208 1.381
2024-12-02-09:22:27-root-INFO: Loss too large (238.199->239.911)! Learning rate decreased to 0.07345.
2024-12-02-09:22:27-root-INFO: Loss too large (238.199->238.508)! Learning rate decreased to 0.05876.
2024-12-02-09:22:28-root-INFO: grad norm: 10.294 10.205 1.350
2024-12-02-09:22:28-root-INFO: grad norm: 14.144 14.077 1.372
2024-12-02-09:22:28-root-INFO: Loss too large (237.447->238.667)! Learning rate decreased to 0.04701.
2024-12-02-09:22:29-root-INFO: grad norm: 12.582 12.505 1.385
2024-12-02-09:22:29-root-INFO: grad norm: 10.908 10.829 1.313
2024-12-02-09:22:30-root-INFO: Loss Change: 238.199 -> 234.167
2024-12-02-09:22:30-root-INFO: Regularization Change: 0.000 -> 1.377
2024-12-02-09:22:30-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-09:22:30-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-09:22:30-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-09:22:30-root-INFO: grad norm: 12.586 12.480 1.629
2024-12-02-09:22:30-root-INFO: Loss too large (234.691->242.393)! Learning rate decreased to 0.07547.
2024-12-02-09:22:30-root-INFO: Loss too large (234.691->237.715)! Learning rate decreased to 0.06038.
2024-12-02-09:22:31-root-INFO: grad norm: 16.716 16.644 1.549
2024-12-02-09:22:31-root-INFO: Loss too large (234.665->236.404)! Learning rate decreased to 0.04830.
2024-12-02-09:22:32-root-INFO: grad norm: 13.922 13.844 1.473
2024-12-02-09:22:32-root-INFO: grad norm: 11.123 11.045 1.313
2024-12-02-09:22:33-root-INFO: grad norm: 10.163 10.080 1.296
2024-12-02-09:22:33-root-INFO: Loss Change: 234.691 -> 229.095
2024-12-02-09:22:33-root-INFO: Regularization Change: 0.000 -> 1.380
2024-12-02-09:22:33-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-09:22:33-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-09:22:33-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-09:22:33-root-INFO: grad norm: 8.177 8.075 1.286
2024-12-02-09:22:33-root-INFO: Loss too large (228.795->230.609)! Learning rate decreased to 0.07753.
2024-12-02-09:22:34-root-INFO: Loss too large (228.795->229.179)! Learning rate decreased to 0.06203.
2024-12-02-09:22:34-root-INFO: grad norm: 9.864 9.785 1.248
2024-12-02-09:22:35-root-INFO: grad norm: 13.342 13.282 1.264
2024-12-02-09:22:35-root-INFO: Loss too large (227.982->229.102)! Learning rate decreased to 0.04962.
2024-12-02-09:22:35-root-INFO: grad norm: 11.673 11.606 1.256
2024-12-02-09:22:36-root-INFO: grad norm: 9.788 9.715 1.192
2024-12-02-09:22:36-root-INFO: Loss Change: 228.795 -> 224.793
2024-12-02-09:22:36-root-INFO: Regularization Change: 0.000 -> 1.363
2024-12-02-09:22:36-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-09:22:36-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-09:22:36-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-09:22:36-root-INFO: grad norm: 10.954 10.866 1.387
2024-12-02-09:22:36-root-INFO: Loss too large (225.274->231.457)! Learning rate decreased to 0.07964.
2024-12-02-09:22:37-root-INFO: Loss too large (225.274->227.488)! Learning rate decreased to 0.06371.
2024-12-02-09:22:37-root-INFO: grad norm: 14.553 14.492 1.335
2024-12-02-09:22:37-root-INFO: Loss too large (225.041->226.442)! Learning rate decreased to 0.05097.
2024-12-02-09:22:38-root-INFO: grad norm: 12.261 12.194 1.283
2024-12-02-09:22:38-root-INFO: grad norm: 9.756 9.686 1.164
2024-12-02-09:22:39-root-INFO: grad norm: 8.891 8.816 1.150
2024-12-02-09:22:39-root-INFO: Loss Change: 225.274 -> 220.235
2024-12-02-09:22:39-root-INFO: Regularization Change: 0.000 -> 1.306
2024-12-02-09:22:39-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-09:22:39-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-09:22:39-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-09:22:40-root-INFO: grad norm: 7.049 6.953 1.155
2024-12-02-09:22:40-root-INFO: Loss too large (220.143->221.117)! Learning rate decreased to 0.08180.
2024-12-02-09:22:40-root-INFO: grad norm: 10.503 10.436 1.184
2024-12-02-09:22:40-root-INFO: Loss too large (220.120->221.864)! Learning rate decreased to 0.06544.
2024-12-02-09:22:41-root-INFO: grad norm: 13.582 13.532 1.169
2024-12-02-09:22:41-root-INFO: Loss too large (219.583->220.802)! Learning rate decreased to 0.05235.
2024-12-02-09:22:41-root-INFO: grad norm: 11.334 11.274 1.164
2024-12-02-09:22:42-root-INFO: grad norm: 8.727 8.661 1.066
2024-12-02-09:22:42-root-INFO: Loss Change: 220.143 -> 216.137
2024-12-02-09:22:42-root-INFO: Regularization Change: 0.000 -> 1.458
2024-12-02-09:22:42-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-09:22:42-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-09:22:43-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-09:22:43-root-INFO: grad norm: 10.386 10.297 1.354
2024-12-02-09:22:43-root-INFO: Loss too large (216.583->222.260)! Learning rate decreased to 0.08399.
2024-12-02-09:22:43-root-INFO: Loss too large (216.583->218.409)! Learning rate decreased to 0.06719.
2024-12-02-09:22:43-root-INFO: grad norm: 13.419 13.362 1.237
2024-12-02-09:22:44-root-INFO: Loss too large (216.152->217.289)! Learning rate decreased to 0.05375.
2024-12-02-09:22:44-root-INFO: grad norm: 11.167 11.105 1.172
2024-12-02-09:22:45-root-INFO: grad norm: 8.667 8.603 1.050
2024-12-02-09:22:45-root-INFO: grad norm: 7.824 7.755 1.035
2024-12-02-09:22:46-root-INFO: Loss Change: 216.583 -> 211.596
2024-12-02-09:22:46-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-02-09:22:46-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-09:22:46-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-09:22:46-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-09:22:46-root-INFO: grad norm: 6.128 6.030 1.091
2024-12-02-09:22:46-root-INFO: Loss too large (211.644->211.957)! Learning rate decreased to 0.08623.
2024-12-02-09:22:46-root-INFO: grad norm: 8.927 8.863 1.067
2024-12-02-09:22:47-root-INFO: Loss too large (211.292->212.480)! Learning rate decreased to 0.06899.
2024-12-02-09:22:47-root-INFO: grad norm: 11.692 11.644 1.059
2024-12-02-09:22:47-root-INFO: Loss too large (210.782->211.630)! Learning rate decreased to 0.05519.
2024-12-02-09:22:48-root-INFO: grad norm: 9.947 9.892 1.049
2024-12-02-09:22:48-root-INFO: grad norm: 7.849 7.789 0.972
2024-12-02-09:22:48-root-INFO: Loss Change: 211.644 -> 207.797
2024-12-02-09:22:48-root-INFO: Regularization Change: 0.000 -> 1.447
2024-12-02-09:22:48-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-09:22:48-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-09:22:49-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-09:22:49-root-INFO: grad norm: 9.180 9.102 1.190
2024-12-02-09:22:49-root-INFO: Loss too large (208.007->212.437)! Learning rate decreased to 0.08852.
2024-12-02-09:22:49-root-INFO: Loss too large (208.007->209.252)! Learning rate decreased to 0.07082.
2024-12-02-09:22:50-root-INFO: grad norm: 11.713 11.664 1.075
2024-12-02-09:22:50-root-INFO: Loss too large (207.468->208.222)! Learning rate decreased to 0.05665.
2024-12-02-09:22:50-root-INFO: grad norm: 9.767 9.711 1.045
2024-12-02-09:22:51-root-INFO: grad norm: 7.584 7.526 0.940
2024-12-02-09:22:51-root-INFO: grad norm: 6.828 6.764 0.935
2024-12-02-09:22:51-root-INFO: Loss Change: 208.007 -> 203.387
2024-12-02-09:22:51-root-INFO: Regularization Change: 0.000 -> 1.281
2024-12-02-09:22:51-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-09:22:51-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-09:22:52-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-09:22:52-root-INFO: grad norm: 5.367 5.278 0.972
2024-12-02-09:22:52-root-INFO: grad norm: 8.905 8.851 0.983
2024-12-02-09:22:52-root-INFO: Loss too large (203.011->206.969)! Learning rate decreased to 0.09086.
2024-12-02-09:22:53-root-INFO: Loss too large (203.011->203.861)! Learning rate decreased to 0.07268.
2024-12-02-09:22:53-root-INFO: grad norm: 10.885 10.843 0.960
2024-12-02-09:22:53-root-INFO: Loss too large (202.148->202.826)! Learning rate decreased to 0.05815.
2024-12-02-09:22:54-root-INFO: grad norm: 9.000 8.950 0.950
2024-12-02-09:22:54-root-INFO: grad norm: 6.689 6.631 0.879
2024-12-02-09:22:55-root-INFO: Loss Change: 203.280 -> 199.247
2024-12-02-09:22:55-root-INFO: Regularization Change: 0.000 -> 1.614
2024-12-02-09:22:55-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-09:22:55-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-09:22:55-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-09:22:55-root-INFO: grad norm: 7.667 7.593 1.065
2024-12-02-09:22:55-root-INFO: Loss too large (199.478->202.406)! Learning rate decreased to 0.09324.
2024-12-02-09:22:55-root-INFO: Loss too large (199.478->200.097)! Learning rate decreased to 0.07459.
2024-12-02-09:22:56-root-INFO: grad norm: 9.820 9.773 0.960
2024-12-02-09:22:56-root-INFO: Loss too large (198.873->199.303)! Learning rate decreased to 0.05967.
2024-12-02-09:22:57-root-INFO: grad norm: 8.382 8.329 0.945
2024-12-02-09:22:57-root-INFO: grad norm: 6.689 6.632 0.867
2024-12-02-09:22:58-root-INFO: grad norm: 6.102 6.041 0.864
2024-12-02-09:22:58-root-INFO: Loss Change: 199.478 -> 195.351
2024-12-02-09:22:58-root-INFO: Regularization Change: 0.000 -> 1.242
2024-12-02-09:22:58-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-09:22:58-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-09:22:58-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-09:22:58-root-INFO: grad norm: 4.963 4.878 0.911
2024-12-02-09:22:59-root-INFO: grad norm: 8.218 8.168 0.906
2024-12-02-09:22:59-root-INFO: Loss too large (194.873->198.464)! Learning rate decreased to 0.09566.
2024-12-02-09:22:59-root-INFO: Loss too large (194.873->195.580)! Learning rate decreased to 0.07653.
2024-12-02-09:22:59-root-INFO: grad norm: 10.140 10.100 0.897
2024-12-02-09:23:00-root-INFO: Loss too large (194.034->194.659)! Learning rate decreased to 0.06122.
2024-12-02-09:23:00-root-INFO: grad norm: 8.366 8.320 0.878
2024-12-02-09:23:01-root-INFO: grad norm: 6.065 6.009 0.820
2024-12-02-09:23:01-root-INFO: Loss Change: 195.164 -> 191.302
2024-12-02-09:23:01-root-INFO: Regularization Change: 0.000 -> 1.609
2024-12-02-09:23:01-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-09:23:01-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-09:23:01-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-09:23:01-root-INFO: grad norm: 7.214 7.144 1.000
2024-12-02-09:23:02-root-INFO: Loss too large (191.620->194.182)! Learning rate decreased to 0.09814.
2024-12-02-09:23:02-root-INFO: Loss too large (191.620->192.067)! Learning rate decreased to 0.07851.
2024-12-02-09:23:02-root-INFO: grad norm: 9.010 8.966 0.892
2024-12-02-09:23:02-root-INFO: Loss too large (190.974->191.245)! Learning rate decreased to 0.06281.
2024-12-02-09:23:03-root-INFO: grad norm: 7.612 7.562 0.871
2024-12-02-09:23:03-root-INFO: grad norm: 6.006 5.952 0.803
2024-12-02-09:23:04-root-INFO: grad norm: 5.460 5.402 0.796
2024-12-02-09:23:04-root-INFO: Loss Change: 191.620 -> 187.645
2024-12-02-09:23:04-root-INFO: Regularization Change: 0.000 -> 1.238
2024-12-02-09:23:04-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-09:23:04-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-09:23:04-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-09:23:04-root-INFO: grad norm: 4.575 4.488 0.885
2024-12-02-09:23:05-root-INFO: grad norm: 7.213 7.161 0.859
2024-12-02-09:23:05-root-INFO: Loss too large (187.121->189.768)! Learning rate decreased to 0.10066.
2024-12-02-09:23:05-root-INFO: Loss too large (187.121->187.481)! Learning rate decreased to 0.08053.
2024-12-02-09:23:06-root-INFO: grad norm: 8.744 8.703 0.837
2024-12-02-09:23:06-root-INFO: Loss too large (186.319->186.628)! Learning rate decreased to 0.06442.
2024-12-02-09:23:06-root-INFO: grad norm: 7.281 7.236 0.808
2024-12-02-09:23:07-root-INFO: grad norm: 5.446 5.392 0.767
2024-12-02-09:23:07-root-INFO: Loss Change: 187.623 -> 183.868
2024-12-02-09:23:07-root-INFO: Regularization Change: 0.000 -> 1.612
2024-12-02-09:23:07-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-09:23:07-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-09:23:07-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-09:23:08-root-INFO: grad norm: 6.490 6.419 0.956
2024-12-02-09:23:08-root-INFO: Loss too large (184.012->185.902)! Learning rate decreased to 0.10323.
2024-12-02-09:23:08-root-INFO: Loss too large (184.012->184.196)! Learning rate decreased to 0.08259.
2024-12-02-09:23:08-root-INFO: grad norm: 7.971 7.929 0.815
2024-12-02-09:23:09-root-INFO: Loss too large (183.340->183.416)! Learning rate decreased to 0.06607.
2024-12-02-09:23:09-root-INFO: grad norm: 6.752 6.703 0.810
2024-12-02-09:23:09-root-INFO: grad norm: 5.386 5.334 0.745
2024-12-02-09:23:10-root-INFO: grad norm: 4.911 4.854 0.746
2024-12-02-09:23:10-root-INFO: Loss Change: 184.012 -> 180.260
2024-12-02-09:23:10-root-INFO: Regularization Change: 0.000 -> 1.234
2024-12-02-09:23:10-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-09:23:10-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-09:23:10-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-09:23:11-root-INFO: grad norm: 4.253 4.168 0.847
2024-12-02-09:23:11-root-INFO: grad norm: 6.010 5.957 0.794
2024-12-02-09:23:11-root-INFO: Loss too large (179.472->180.492)! Learning rate decreased to 0.10585.
2024-12-02-09:23:12-root-INFO: grad norm: 9.633 9.594 0.858
2024-12-02-09:23:12-root-INFO: Loss too large (179.225->181.037)! Learning rate decreased to 0.08468.
2024-12-02-09:23:12-root-INFO: Loss too large (179.225->179.472)! Learning rate decreased to 0.06774.
2024-12-02-09:23:12-root-INFO: grad norm: 7.226 7.186 0.757
2024-12-02-09:23:13-root-INFO: grad norm: 4.753 4.699 0.713
2024-12-02-09:23:13-root-INFO: Loss Change: 180.281 -> 176.361
2024-12-02-09:23:13-root-INFO: Regularization Change: 0.000 -> 1.793
2024-12-02-09:23:13-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-09:23:13-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-09:23:13-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-09:23:14-root-INFO: grad norm: 5.476 5.410 0.847
2024-12-02-09:23:14-root-INFO: Loss too large (176.466->177.260)! Learning rate decreased to 0.10852.
2024-12-02-09:23:14-root-INFO: grad norm: 9.276 9.239 0.827
2024-12-02-09:23:14-root-INFO: Loss too large (176.210->178.226)! Learning rate decreased to 0.08682.
2024-12-02-09:23:15-root-INFO: Loss too large (176.210->176.599)! Learning rate decreased to 0.06945.
2024-12-02-09:23:15-root-INFO: grad norm: 7.221 7.178 0.781
2024-12-02-09:23:15-root-INFO: grad norm: 4.735 4.682 0.705
2024-12-02-09:23:16-root-INFO: grad norm: 4.182 4.124 0.692
2024-12-02-09:23:16-root-INFO: Loss Change: 176.466 -> 172.769
2024-12-02-09:23:16-root-INFO: Regularization Change: 0.000 -> 1.365
2024-12-02-09:23:16-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-09:23:16-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-09:23:16-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-09:23:17-root-INFO: grad norm: 3.602 3.523 0.749
2024-12-02-09:23:17-root-INFO: grad norm: 4.819 4.769 0.694
2024-12-02-09:23:17-root-INFO: Loss too large (171.913->172.189)! Learning rate decreased to 0.11124.
2024-12-02-09:23:18-root-INFO: grad norm: 7.697 7.661 0.747
2024-12-02-09:23:18-root-INFO: Loss too large (171.482->172.517)! Learning rate decreased to 0.08899.
2024-12-02-09:23:18-root-INFO: Loss too large (171.482->171.496)! Learning rate decreased to 0.07119.
2024-12-02-09:23:19-root-INFO: grad norm: 6.064 6.024 0.695
2024-12-02-09:23:19-root-INFO: grad norm: 4.304 4.252 0.668
2024-12-02-09:23:19-root-INFO: Loss Change: 172.842 -> 169.134
2024-12-02-09:23:19-root-INFO: Regularization Change: 0.000 -> 1.790
2024-12-02-09:23:19-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-09:23:19-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-09:23:20-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-09:23:20-root-INFO: grad norm: 4.936 4.874 0.779
2024-12-02-09:23:20-root-INFO: Loss too large (169.131->169.470)! Learning rate decreased to 0.11401.
2024-12-02-09:23:20-root-INFO: grad norm: 7.744 7.705 0.777
2024-12-02-09:23:20-root-INFO: Loss too large (168.713->169.806)! Learning rate decreased to 0.09121.
2024-12-02-09:23:21-root-INFO: grad norm: 7.635 7.594 0.787
2024-12-02-09:23:21-root-INFO: grad norm: 7.163 7.124 0.751
2024-12-02-09:23:22-root-INFO: grad norm: 6.969 6.928 0.753
2024-12-02-09:23:22-root-INFO: Loss Change: 169.131 -> 165.730
2024-12-02-09:23:22-root-INFO: Regularization Change: 0.000 -> 1.963
2024-12-02-09:23:22-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-09:23:22-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-09:23:22-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-09:23:23-root-INFO: grad norm: 5.424 5.378 0.710
2024-12-02-09:23:23-root-INFO: Loss too large (165.364->165.777)! Learning rate decreased to 0.11682.
2024-12-02-09:23:23-root-INFO: grad norm: 6.439 6.401 0.695
2024-12-02-09:23:24-root-INFO: grad norm: 9.417 9.388 0.740
2024-12-02-09:23:24-root-INFO: Loss too large (164.560->166.141)! Learning rate decreased to 0.09346.
2024-12-02-09:23:24-root-INFO: Loss too large (164.560->164.613)! Learning rate decreased to 0.07477.
2024-12-02-09:23:24-root-INFO: grad norm: 6.149 6.110 0.690
2024-12-02-09:23:25-root-INFO: grad norm: 3.213 3.152 0.621
2024-12-02-09:23:25-root-INFO: Loss Change: 165.364 -> 161.572
2024-12-02-09:23:25-root-INFO: Regularization Change: 0.000 -> 1.619
2024-12-02-09:23:25-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-09:23:25-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-09:23:25-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-09:23:25-root-INFO: grad norm: 4.112 4.039 0.771
2024-12-02-09:23:26-root-INFO: grad norm: 6.573 6.524 0.803
2024-12-02-09:23:26-root-INFO: Loss too large (161.194->162.816)! Learning rate decreased to 0.11969.
2024-12-02-09:23:26-root-INFO: Loss too large (161.194->161.338)! Learning rate decreased to 0.09575.
2024-12-02-09:23:27-root-INFO: grad norm: 6.181 6.133 0.765
2024-12-02-09:23:27-root-INFO: grad norm: 5.991 5.949 0.709
2024-12-02-09:23:28-root-INFO: grad norm: 5.780 5.737 0.705
2024-12-02-09:23:28-root-INFO: Loss Change: 161.732 -> 158.242
2024-12-02-09:23:28-root-INFO: Regularization Change: 0.000 -> 2.132
2024-12-02-09:23:28-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-09:23:28-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-09:23:28-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-09:23:29-root-INFO: grad norm: 4.501 4.457 0.631
2024-12-02-09:23:29-root-INFO: grad norm: 6.515 6.480 0.682
2024-12-02-09:23:29-root-INFO: Loss too large (157.891->158.918)! Learning rate decreased to 0.12261.
2024-12-02-09:23:30-root-INFO: grad norm: 8.575 8.544 0.731
2024-12-02-09:23:30-root-INFO: Loss too large (157.168->158.195)! Learning rate decreased to 0.09809.
2024-12-02-09:23:30-root-INFO: grad norm: 7.051 7.012 0.738
2024-12-02-09:23:31-root-INFO: grad norm: 4.816 4.771 0.659
2024-12-02-09:23:31-root-INFO: Loss Change: 157.989 -> 154.282
2024-12-02-09:23:31-root-INFO: Regularization Change: 0.000 -> 2.292
2024-12-02-09:23:31-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-09:23:31-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-09:23:31-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-09:23:31-root-INFO: grad norm: 5.962 5.897 0.883
2024-12-02-09:23:32-root-INFO: Loss too large (154.665->155.464)! Learning rate decreased to 0.12558.
2024-12-02-09:23:32-root-INFO: grad norm: 7.412 7.366 0.826
2024-12-02-09:23:32-root-INFO: Loss too large (154.248->154.454)! Learning rate decreased to 0.10047.
2024-12-02-09:23:33-root-INFO: grad norm: 6.310 6.264 0.761
2024-12-02-09:23:33-root-INFO: grad norm: 5.336 5.293 0.674
2024-12-02-09:23:34-root-INFO: grad norm: 4.800 4.755 0.656
2024-12-02-09:23:34-root-INFO: Loss Change: 154.665 -> 150.795
2024-12-02-09:23:34-root-INFO: Regularization Change: 0.000 -> 2.004
2024-12-02-09:23:34-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-09:23:34-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-09:23:34-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-09:23:34-root-INFO: grad norm: 3.376 3.324 0.592
2024-12-02-09:23:35-root-INFO: grad norm: 4.491 4.453 0.582
2024-12-02-09:23:35-root-INFO: grad norm: 8.320 8.292 0.680
2024-12-02-09:23:36-root-INFO: Loss too large (149.727->152.559)! Learning rate decreased to 0.12860.
2024-12-02-09:23:36-root-INFO: Loss too large (149.727->150.461)! Learning rate decreased to 0.10288.
2024-12-02-09:23:36-root-INFO: grad norm: 6.479 6.445 0.662
2024-12-02-09:23:37-root-INFO: grad norm: 4.163 4.120 0.598
2024-12-02-09:23:37-root-INFO: Loss Change: 150.464 -> 146.744
2024-12-02-09:23:37-root-INFO: Regularization Change: 0.000 -> 2.485
2024-12-02-09:23:37-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-09:23:37-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-09:23:37-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-09:23:38-root-INFO: grad norm: 4.664 4.609 0.716
2024-12-02-09:23:38-root-INFO: Loss too large (146.998->147.022)! Learning rate decreased to 0.13168.
2024-12-02-09:23:38-root-INFO: grad norm: 5.496 5.452 0.695
2024-12-02-09:23:39-root-INFO: grad norm: 6.587 6.538 0.799
2024-12-02-09:23:39-root-INFO: grad norm: 7.970 7.923 0.864
2024-12-02-09:23:39-root-INFO: Loss too large (145.913->146.089)! Learning rate decreased to 0.10534.
2024-12-02-09:23:40-root-INFO: grad norm: 6.296 6.253 0.728
2024-12-02-09:23:40-root-INFO: Loss Change: 146.998 -> 143.587
2024-12-02-09:23:40-root-INFO: Regularization Change: 0.000 -> 2.322
2024-12-02-09:23:40-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-09:23:40-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-09:23:40-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-09:23:41-root-INFO: grad norm: 3.654 3.609 0.571
2024-12-02-09:23:41-root-INFO: grad norm: 4.755 4.720 0.573
2024-12-02-09:23:41-root-INFO: Loss too large (142.403->142.426)! Learning rate decreased to 0.13480.
2024-12-02-09:23:42-root-INFO: grad norm: 5.464 5.432 0.587
2024-12-02-09:23:42-root-INFO: grad norm: 6.078 6.042 0.664
2024-12-02-09:23:43-root-INFO: grad norm: 6.895 6.858 0.706
2024-12-02-09:23:43-root-INFO: Loss Change: 143.063 -> 140.821
2024-12-02-09:23:43-root-INFO: Regularization Change: 0.000 -> 3.198
2024-12-02-09:23:43-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-09:23:43-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-09:23:43-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-09:23:43-root-INFO: grad norm: 8.984 8.921 1.060
2024-12-02-09:23:43-root-INFO: Loss too large (141.555->144.423)! Learning rate decreased to 0.13798.
2024-12-02-09:23:44-root-INFO: grad norm: 9.648 9.591 1.049
2024-12-02-09:23:44-root-INFO: grad norm: 10.239 10.175 1.139
2024-12-02-09:23:45-root-INFO: grad norm: 10.949 10.880 1.229
2024-12-02-09:23:45-root-INFO: Loss too large (140.552->140.684)! Learning rate decreased to 0.11038.
2024-12-02-09:23:46-root-INFO: grad norm: 7.612 7.560 0.890
2024-12-02-09:23:46-root-INFO: Loss Change: 141.555 -> 136.446
2024-12-02-09:23:46-root-INFO: Regularization Change: 0.000 -> 2.918
2024-12-02-09:23:46-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-09:23:46-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-09:23:46-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-09:23:46-root-INFO: grad norm: 3.992 3.949 0.581
2024-12-02-09:23:47-root-INFO: grad norm: 5.514 5.471 0.685
2024-12-02-09:23:47-root-INFO: Loss too large (135.280->135.822)! Learning rate decreased to 0.14121.
2024-12-02-09:23:47-root-INFO: grad norm: 5.987 5.940 0.746
2024-12-02-09:23:48-root-INFO: grad norm: 6.585 6.535 0.815
2024-12-02-09:23:48-root-INFO: grad norm: 7.126 7.071 0.880
2024-12-02-09:23:49-root-INFO: Loss Change: 135.808 -> 133.459
2024-12-02-09:23:49-root-INFO: Regularization Change: 0.000 -> 3.295
2024-12-02-09:23:49-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-09:23:49-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-09:23:49-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-09:23:49-root-INFO: grad norm: 8.840 8.765 1.155
2024-12-02-09:23:49-root-INFO: Loss too large (134.092->136.900)! Learning rate decreased to 0.14449.
2024-12-02-09:23:50-root-INFO: grad norm: 9.216 9.144 1.143
2024-12-02-09:23:50-root-INFO: grad norm: 9.470 9.399 1.151
2024-12-02-09:23:51-root-INFO: grad norm: 9.736 9.659 1.219
2024-12-02-09:23:51-root-INFO: grad norm: 9.941 9.867 1.206
2024-12-02-09:23:51-root-INFO: Loss Change: 134.092 -> 131.585
2024-12-02-09:23:51-root-INFO: Regularization Change: 0.000 -> 4.220
2024-12-02-09:23:51-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-09:23:51-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-09:23:51-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-09:23:52-root-INFO: grad norm: 8.997 8.930 1.093
2024-12-02-09:23:52-root-INFO: Loss too large (130.920->133.445)! Learning rate decreased to 0.14783.
2024-12-02-09:23:52-root-INFO: grad norm: 8.918 8.852 1.079
2024-12-02-09:23:53-root-INFO: grad norm: 8.809 8.736 1.131
2024-12-02-09:23:53-root-INFO: grad norm: 8.742 8.676 1.069
2024-12-02-09:23:54-root-INFO: grad norm: 8.664 8.590 1.129
2024-12-02-09:23:54-root-INFO: Loss Change: 130.920 -> 127.254
2024-12-02-09:23:54-root-INFO: Regularization Change: 0.000 -> 4.088
2024-12-02-09:23:54-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-09:23:54-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-09:23:54-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-09:23:54-root-INFO: grad norm: 10.132 10.042 1.348
2024-12-02-09:23:55-root-INFO: Loss too large (128.304->131.520)! Learning rate decreased to 0.15121.
2024-12-02-09:23:55-root-INFO: grad norm: 9.496 9.410 1.275
2024-12-02-09:23:56-root-INFO: grad norm: 8.764 8.695 1.097
2024-12-02-09:23:56-root-INFO: grad norm: 8.394 8.317 1.132
2024-12-02-09:23:56-root-INFO: grad norm: 8.296 8.230 1.045
2024-12-02-09:23:57-root-INFO: Loss Change: 128.304 -> 123.699
2024-12-02-09:23:57-root-INFO: Regularization Change: 0.000 -> 4.368
2024-12-02-09:23:57-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-09:23:57-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-09:23:57-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-09:23:57-root-INFO: grad norm: 7.139 7.076 0.948
2024-12-02-09:23:57-root-INFO: Loss too large (123.103->124.215)! Learning rate decreased to 0.15465.
2024-12-02-09:23:58-root-INFO: grad norm: 6.999 6.939 0.913
2024-12-02-09:23:58-root-INFO: grad norm: 6.943 6.876 0.965
2024-12-02-09:23:59-root-INFO: grad norm: 6.976 6.915 0.920
2024-12-02-09:23:59-root-INFO: grad norm: 6.944 6.875 0.977
2024-12-02-09:24:00-root-INFO: Loss Change: 123.103 -> 119.496
2024-12-02-09:24:00-root-INFO: Regularization Change: 0.000 -> 3.604
2024-12-02-09:24:00-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-09:24:00-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-09:24:00-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-09:24:00-root-INFO: grad norm: 8.052 7.970 1.144
2024-12-02-09:24:00-root-INFO: Loss too large (120.201->122.291)! Learning rate decreased to 0.15815.
2024-12-02-09:24:01-root-INFO: grad norm: 7.742 7.666 1.083
2024-12-02-09:24:01-root-INFO: grad norm: 7.357 7.289 0.998
2024-12-02-09:24:02-root-INFO: grad norm: 7.206 7.134 1.020
2024-12-02-09:24:02-root-INFO: grad norm: 7.174 7.109 0.964
2024-12-02-09:24:02-root-INFO: Loss Change: 120.201 -> 116.649
2024-12-02-09:24:02-root-INFO: Regularization Change: 0.000 -> 3.887
2024-12-02-09:24:02-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-09:24:02-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-09:24:03-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-09:24:03-root-INFO: grad norm: 6.116 6.055 0.861
2024-12-02-09:24:03-root-INFO: Loss too large (115.889->116.388)! Learning rate decreased to 0.16169.
2024-12-02-09:24:03-root-INFO: grad norm: 5.981 5.925 0.819
2024-12-02-09:24:04-root-INFO: grad norm: 5.874 5.811 0.855
2024-12-02-09:24:04-root-INFO: grad norm: 5.811 5.758 0.783
2024-12-02-09:24:05-root-INFO: grad norm: 5.717 5.655 0.839
2024-12-02-09:24:05-root-INFO: Loss Change: 115.889 -> 112.195
2024-12-02-09:24:05-root-INFO: Regularization Change: 0.000 -> 3.401
2024-12-02-09:24:05-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-09:24:05-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-09:24:05-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-09:24:05-root-INFO: grad norm: 6.366 6.304 0.883
2024-12-02-09:24:06-root-INFO: Loss too large (112.511->113.604)! Learning rate decreased to 0.16529.
2024-12-02-09:24:06-root-INFO: grad norm: 6.026 5.961 0.883
2024-12-02-09:24:06-root-INFO: grad norm: 5.580 5.528 0.762
2024-12-02-09:24:07-root-INFO: grad norm: 5.472 5.412 0.812
2024-12-02-09:24:07-root-INFO: grad norm: 5.495 5.446 0.730
2024-12-02-09:24:08-root-INFO: Loss Change: 112.511 -> 109.207
2024-12-02-09:24:08-root-INFO: Regularization Change: 0.000 -> 3.444
2024-12-02-09:24:08-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-09:24:08-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-09:24:08-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-09:24:08-root-INFO: grad norm: 4.762 4.717 0.658
2024-12-02-09:24:09-root-INFO: grad norm: 7.102 7.048 0.875
2024-12-02-09:24:09-root-INFO: Loss too large (108.775->110.342)! Learning rate decreased to 0.16894.
2024-12-02-09:24:09-root-INFO: grad norm: 6.202 6.137 0.895
2024-12-02-09:24:10-root-INFO: grad norm: 4.981 4.930 0.711
2024-12-02-09:24:10-root-INFO: grad norm: 4.762 4.705 0.732
2024-12-02-09:24:10-root-INFO: Loss Change: 108.827 -> 105.420
2024-12-02-09:24:10-root-INFO: Regularization Change: 0.000 -> 3.443
2024-12-02-09:24:10-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-09:24:10-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-09:24:11-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-09:24:11-root-INFO: grad norm: 5.928 5.864 0.872
2024-12-02-09:24:11-root-INFO: Loss too large (105.816->106.796)! Learning rate decreased to 0.17265.
2024-12-02-09:24:11-root-INFO: grad norm: 5.776 5.712 0.858
2024-12-02-09:24:12-root-INFO: grad norm: 5.727 5.674 0.777
2024-12-02-09:24:12-root-INFO: grad norm: 5.640 5.575 0.853
2024-12-02-09:24:13-root-INFO: grad norm: 5.558 5.505 0.768
2024-12-02-09:24:13-root-INFO: Loss Change: 105.816 -> 102.884
2024-12-02-09:24:13-root-INFO: Regularization Change: 0.000 -> 3.509
2024-12-02-09:24:13-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-09:24:13-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-09:24:13-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-09:24:13-root-INFO: grad norm: 5.081 5.023 0.763
2024-12-02-09:24:14-root-INFO: Loss too large (102.409->102.702)! Learning rate decreased to 0.17641.
2024-12-02-09:24:14-root-INFO: grad norm: 5.247 5.196 0.728
2024-12-02-09:24:15-root-INFO: grad norm: 5.323 5.259 0.823
2024-12-02-09:24:15-root-INFO: grad norm: 5.370 5.316 0.753
2024-12-02-09:24:15-root-INFO: grad norm: 5.356 5.292 0.827
2024-12-02-09:24:16-root-INFO: Loss Change: 102.409 -> 99.245
2024-12-02-09:24:16-root-INFO: Regularization Change: 0.000 -> 3.391
2024-12-02-09:24:16-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-09:24:16-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-09:24:16-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-09:24:16-root-INFO: grad norm: 6.271 6.210 0.874
2024-12-02-09:24:16-root-INFO: Loss too large (99.462->100.756)! Learning rate decreased to 0.18022.
2024-12-02-09:24:17-root-INFO: grad norm: 5.849 5.777 0.915
2024-12-02-09:24:17-root-INFO: grad norm: 5.239 5.186 0.741
2024-12-02-09:24:18-root-INFO: grad norm: 5.170 5.106 0.810
2024-12-02-09:24:18-root-INFO: grad norm: 5.278 5.230 0.712
2024-12-02-09:24:18-root-INFO: Loss Change: 99.462 -> 96.324
2024-12-02-09:24:18-root-INFO: Regularization Change: 0.000 -> 3.658
2024-12-02-09:24:18-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-09:24:18-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-09:24:19-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-09:24:19-root-INFO: grad norm: 4.902 4.853 0.691
2024-12-02-09:24:19-root-INFO: Loss too large (96.088->96.171)! Learning rate decreased to 0.18408.
2024-12-02-09:24:19-root-INFO: grad norm: 4.830 4.788 0.634
2024-12-02-09:24:20-root-INFO: grad norm: 4.849 4.797 0.711
2024-12-02-09:24:20-root-INFO: grad norm: 4.923 4.882 0.634
2024-12-02-09:24:21-root-INFO: grad norm: 4.909 4.855 0.723
2024-12-02-09:24:21-root-INFO: Loss Change: 96.088 -> 92.818
2024-12-02-09:24:21-root-INFO: Regularization Change: 0.000 -> 3.426
2024-12-02-09:24:21-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-09:24:21-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-09:24:21-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-09:24:21-root-INFO: grad norm: 5.784 5.726 0.823
2024-12-02-09:24:22-root-INFO: Loss too large (93.107->94.247)! Learning rate decreased to 0.18800.
2024-12-02-09:24:22-root-INFO: grad norm: 5.456 5.391 0.838
2024-12-02-09:24:23-root-INFO: grad norm: 4.983 4.934 0.696
2024-12-02-09:24:23-root-INFO: grad norm: 4.966 4.907 0.766
2024-12-02-09:24:23-root-INFO: grad norm: 5.095 5.049 0.683
2024-12-02-09:24:24-root-INFO: Loss Change: 93.107 -> 90.247
2024-12-02-09:24:24-root-INFO: Regularization Change: 0.000 -> 3.659
2024-12-02-09:24:24-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-09:24:24-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-09:24:24-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-09:24:24-root-INFO: grad norm: 4.706 4.661 0.650
2024-12-02-09:24:24-root-INFO: Loss too large (89.858->89.966)! Learning rate decreased to 0.19197.
2024-12-02-09:24:25-root-INFO: grad norm: 4.749 4.708 0.620
2024-12-02-09:24:25-root-INFO: grad norm: 4.800 4.749 0.696
2024-12-02-09:24:26-root-INFO: grad norm: 4.868 4.825 0.641
2024-12-02-09:24:26-root-INFO: grad norm: 4.887 4.834 0.723
2024-12-02-09:24:26-root-INFO: Loss Change: 89.858 -> 86.758
2024-12-02-09:24:26-root-INFO: Regularization Change: 0.000 -> 3.488
2024-12-02-09:24:26-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-09:24:26-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-09:24:27-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-09:24:27-root-INFO: grad norm: 5.745 5.688 0.807
2024-12-02-09:24:27-root-INFO: Loss too large (87.074->88.326)! Learning rate decreased to 0.19599.
2024-12-02-09:24:27-root-INFO: grad norm: 5.444 5.378 0.843
2024-12-02-09:24:28-root-INFO: grad norm: 5.004 4.954 0.704
2024-12-02-09:24:28-root-INFO: grad norm: 5.011 4.950 0.780
2024-12-02-09:24:29-root-INFO: grad norm: 5.144 5.096 0.697
2024-12-02-09:24:29-root-INFO: Loss Change: 87.074 -> 84.408
2024-12-02-09:24:29-root-INFO: Regularization Change: 0.000 -> 3.730
2024-12-02-09:24:29-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-09:24:29-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-09:24:29-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-09:24:29-root-INFO: grad norm: 4.791 4.742 0.684
2024-12-02-09:24:30-root-INFO: Loss too large (84.105->84.332)! Learning rate decreased to 0.20006.
2024-12-02-09:24:30-root-INFO: grad norm: 4.767 4.726 0.622
2024-12-02-09:24:31-root-INFO: grad norm: 4.816 4.763 0.715
2024-12-02-09:24:31-root-INFO: grad norm: 4.919 4.877 0.636
2024-12-02-09:24:31-root-INFO: grad norm: 4.949 4.894 0.735
2024-12-02-09:24:32-root-INFO: Loss Change: 84.105 -> 81.189
2024-12-02-09:24:32-root-INFO: Regularization Change: 0.000 -> 3.514
2024-12-02-09:24:32-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-09:24:32-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-09:24:32-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-09:24:32-root-INFO: grad norm: 5.969 5.905 0.876
2024-12-02-09:24:32-root-INFO: Loss too large (81.566->83.162)! Learning rate decreased to 0.20419.
2024-12-02-09:24:33-root-INFO: grad norm: 5.683 5.618 0.859
2024-12-02-09:24:33-root-INFO: grad norm: 5.304 5.255 0.725
2024-12-02-09:24:34-root-INFO: grad norm: 5.278 5.218 0.790
2024-12-02-09:24:34-root-INFO: grad norm: 5.338 5.291 0.702
2024-12-02-09:24:34-root-INFO: Loss Change: 81.566 -> 79.109
2024-12-02-09:24:34-root-INFO: Regularization Change: 0.000 -> 3.862
2024-12-02-09:24:34-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-09:24:34-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-09:24:35-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-09:24:35-root-INFO: grad norm: 4.835 4.792 0.646
2024-12-02-09:24:35-root-INFO: Loss too large (78.469->78.843)! Learning rate decreased to 0.20837.
2024-12-02-09:24:35-root-INFO: grad norm: 4.913 4.875 0.605
2024-12-02-09:24:36-root-INFO: grad norm: 4.973 4.923 0.705
2024-12-02-09:24:36-root-INFO: grad norm: 5.068 5.029 0.630
2024-12-02-09:24:37-root-INFO: grad norm: 5.084 5.032 0.732
2024-12-02-09:24:37-root-INFO: Loss Change: 78.469 -> 75.670
2024-12-02-09:24:37-root-INFO: Regularization Change: 0.000 -> 3.583
2024-12-02-09:24:37-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-09:24:37-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-09:24:37-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-09:24:37-root-INFO: grad norm: 6.113 6.052 0.864
2024-12-02-09:24:38-root-INFO: Loss too large (76.143->77.883)! Learning rate decreased to 0.21260.
2024-12-02-09:24:38-root-INFO: grad norm: 5.712 5.645 0.870
2024-12-02-09:24:38-root-INFO: grad norm: 5.196 5.149 0.700
2024-12-02-09:24:39-root-INFO: grad norm: 5.182 5.123 0.777
2024-12-02-09:24:39-root-INFO: grad norm: 5.270 5.225 0.685
2024-12-02-09:24:40-root-INFO: Loss Change: 76.143 -> 73.638
2024-12-02-09:24:40-root-INFO: Regularization Change: 0.000 -> 3.992
2024-12-02-09:24:40-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-09:24:40-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-09:24:40-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-09:24:40-root-INFO: grad norm: 4.869 4.827 0.635
2024-12-02-09:24:40-root-INFO: Loss too large (73.277->73.706)! Learning rate decreased to 0.21688.
2024-12-02-09:24:41-root-INFO: grad norm: 4.878 4.843 0.587
2024-12-02-09:24:41-root-INFO: grad norm: 4.934 4.885 0.697
2024-12-02-09:24:42-root-INFO: grad norm: 5.016 4.978 0.617
2024-12-02-09:24:42-root-INFO: grad norm: 5.047 4.995 0.724
2024-12-02-09:24:42-root-INFO: Loss Change: 73.277 -> 70.552
2024-12-02-09:24:42-root-INFO: Regularization Change: 0.000 -> 3.681
2024-12-02-09:24:42-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-09:24:42-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-09:24:43-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-09:24:43-root-INFO: grad norm: 6.121 6.053 0.914
2024-12-02-09:24:43-root-INFO: Loss too large (71.045->73.004)! Learning rate decreased to 0.22121.
2024-12-02-09:24:43-root-INFO: Loss too large (71.045->71.060)! Learning rate decreased to 0.17697.
2024-12-02-09:24:43-root-INFO: grad norm: 4.344 4.300 0.612
2024-12-02-09:24:44-root-INFO: grad norm: 3.088 3.060 0.411
2024-12-02-09:24:44-root-INFO: grad norm: 2.671 2.646 0.363
2024-12-02-09:24:45-root-INFO: grad norm: 2.427 2.407 0.315
2024-12-02-09:24:45-root-INFO: Loss Change: 71.045 -> 67.449
2024-12-02-09:24:45-root-INFO: Regularization Change: 0.000 -> 2.303
2024-12-02-09:24:45-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-09:24:45-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-09:24:45-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-09:24:46-root-INFO: grad norm: 2.083 2.063 0.289
2024-12-02-09:24:46-root-INFO: grad norm: 2.907 2.894 0.281
2024-12-02-09:24:46-root-INFO: Loss too large (66.789->66.964)! Learning rate decreased to 0.22559.
2024-12-02-09:24:47-root-INFO: grad norm: 3.530 3.510 0.373
2024-12-02-09:24:47-root-INFO: grad norm: 4.438 4.417 0.433
2024-12-02-09:24:47-root-INFO: Loss too large (66.395->66.509)! Learning rate decreased to 0.18047.
2024-12-02-09:24:48-root-INFO: grad norm: 3.745 3.718 0.445
2024-12-02-09:24:48-root-INFO: Loss Change: 67.251 -> 65.195
2024-12-02-09:24:48-root-INFO: Regularization Change: 0.000 -> 2.693
2024-12-02-09:24:48-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-09:24:48-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-09:24:48-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-09:24:48-root-INFO: grad norm: 4.015 3.971 0.594
2024-12-02-09:24:49-root-INFO: Loss too large (65.268->66.190)! Learning rate decreased to 0.23002.
2024-12-02-09:24:49-root-INFO: Loss too large (65.268->65.274)! Learning rate decreased to 0.18402.
2024-12-02-09:24:49-root-INFO: grad norm: 3.527 3.498 0.456
2024-12-02-09:24:50-root-INFO: grad norm: 3.194 3.170 0.391
2024-12-02-09:24:50-root-INFO: grad norm: 3.044 3.019 0.387
2024-12-02-09:24:51-root-INFO: grad norm: 2.946 2.924 0.355
2024-12-02-09:24:51-root-INFO: Loss Change: 65.268 -> 62.981
2024-12-02-09:24:51-root-INFO: Regularization Change: 0.000 -> 2.108
2024-12-02-09:24:51-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-09:24:51-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-09:24:51-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-09:24:51-root-INFO: grad norm: 2.584 2.568 0.291
2024-12-02-09:24:52-root-INFO: grad norm: 4.281 4.262 0.398
2024-12-02-09:24:52-root-INFO: Loss too large (62.821->63.938)! Learning rate decreased to 0.23450.
2024-12-02-09:24:52-root-INFO: Loss too large (62.821->62.942)! Learning rate decreased to 0.18760.
2024-12-02-09:24:53-root-INFO: grad norm: 3.794 3.767 0.450
2024-12-02-09:24:53-root-INFO: grad norm: 3.485 3.461 0.411
2024-12-02-09:24:54-root-INFO: grad norm: 3.343 3.316 0.418
2024-12-02-09:24:54-root-INFO: Loss Change: 62.838 -> 60.919
2024-12-02-09:24:54-root-INFO: Regularization Change: 0.000 -> 2.411
2024-12-02-09:24:54-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-09:24:54-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-09:24:54-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-09:24:54-root-INFO: grad norm: 3.991 3.947 0.591
2024-12-02-09:24:54-root-INFO: Loss too large (60.912->62.005)! Learning rate decreased to 0.23903.
2024-12-02-09:24:55-root-INFO: Loss too large (60.912->61.002)! Learning rate decreased to 0.19122.
2024-12-02-09:24:55-root-INFO: grad norm: 3.653 3.621 0.481
2024-12-02-09:24:55-root-INFO: grad norm: 3.433 3.406 0.425
2024-12-02-09:24:56-root-INFO: grad norm: 3.309 3.282 0.426
2024-12-02-09:24:56-root-INFO: grad norm: 3.224 3.199 0.396
2024-12-02-09:24:57-root-INFO: Loss Change: 60.912 -> 58.731
2024-12-02-09:24:57-root-INFO: Regularization Change: 0.000 -> 2.205
2024-12-02-09:24:57-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-09:24:57-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-09:24:57-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-09:24:57-root-INFO: grad norm: 2.876 2.860 0.302
2024-12-02-09:24:57-root-INFO: Loss too large (58.464->58.792)! Learning rate decreased to 0.24361.
2024-12-02-09:24:58-root-INFO: grad norm: 3.768 3.748 0.390
2024-12-02-09:24:58-root-INFO: Loss too large (58.295->58.402)! Learning rate decreased to 0.19489.
2024-12-02-09:24:58-root-INFO: grad norm: 3.552 3.526 0.427
2024-12-02-09:24:59-root-INFO: grad norm: 3.406 3.383 0.398
2024-12-02-09:24:59-root-INFO: grad norm: 3.319 3.293 0.412
2024-12-02-09:24:59-root-INFO: Loss Change: 58.464 -> 56.643
2024-12-02-09:24:59-root-INFO: Regularization Change: 0.000 -> 2.280
2024-12-02-09:24:59-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-09:24:59-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-09:25:00-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-09:25:00-root-INFO: grad norm: 4.264 4.210 0.679
2024-12-02-09:25:00-root-INFO: Loss too large (56.888->58.212)! Learning rate decreased to 0.24866.
2024-12-02-09:25:00-root-INFO: Loss too large (56.888->56.992)! Learning rate decreased to 0.19893.
2024-12-02-09:25:01-root-INFO: grad norm: 3.811 3.777 0.508
2024-12-02-09:25:01-root-INFO: grad norm: 3.537 3.510 0.438
2024-12-02-09:25:02-root-INFO: grad norm: 3.357 3.329 0.436
2024-12-02-09:25:02-root-INFO: grad norm: 3.231 3.206 0.398
2024-12-02-09:25:02-root-INFO: Loss Change: 56.888 -> 54.517
2024-12-02-09:25:02-root-INFO: Regularization Change: 0.000 -> 2.378
2024-12-02-09:25:02-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-09:25:02-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-09:25:03-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-09:25:03-root-INFO: grad norm: 2.714 2.697 0.309
2024-12-02-09:25:03-root-INFO: Loss too large (54.221->54.348)! Learning rate decreased to 0.25333.
2024-12-02-09:25:03-root-INFO: grad norm: 3.381 3.360 0.378
2024-12-02-09:25:04-root-INFO: grad norm: 4.372 4.336 0.555
2024-12-02-09:25:04-root-INFO: Loss too large (53.941->54.153)! Learning rate decreased to 0.20266.
2024-12-02-09:25:04-root-INFO: grad norm: 3.812 3.781 0.484
2024-12-02-09:25:05-root-INFO: grad norm: 3.460 3.428 0.466
2024-12-02-09:25:05-root-INFO: Loss Change: 54.221 -> 52.361
2024-12-02-09:25:05-root-INFO: Regularization Change: 0.000 -> 2.583
2024-12-02-09:25:05-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-09:25:05-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-09:25:05-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-09:25:06-root-INFO: grad norm: 3.859 3.811 0.606
2024-12-02-09:25:06-root-INFO: Loss too large (52.427->53.528)! Learning rate decreased to 0.25805.
2024-12-02-09:25:06-root-INFO: Loss too large (52.427->52.432)! Learning rate decreased to 0.20644.
2024-12-02-09:25:06-root-INFO: grad norm: 3.430 3.397 0.474
2024-12-02-09:25:07-root-INFO: grad norm: 3.177 3.150 0.416
2024-12-02-09:25:07-root-INFO: grad norm: 2.994 2.968 0.397
2024-12-02-09:25:08-root-INFO: grad norm: 2.874 2.851 0.366
2024-12-02-09:25:08-root-INFO: Loss Change: 52.427 -> 50.136
2024-12-02-09:25:08-root-INFO: Regularization Change: 0.000 -> 2.331
2024-12-02-09:25:08-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-09:25:08-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-09:25:08-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-09:25:08-root-INFO: grad norm: 2.509 2.490 0.309
2024-12-02-09:25:09-root-INFO: grad norm: 3.982 3.957 0.444
2024-12-02-09:25:09-root-INFO: Loss too large (49.857->51.023)! Learning rate decreased to 0.26281.
2024-12-02-09:25:09-root-INFO: Loss too large (49.857->49.883)! Learning rate decreased to 0.21025.
2024-12-02-09:25:10-root-INFO: grad norm: 3.463 3.434 0.449
2024-12-02-09:25:10-root-INFO: grad norm: 3.163 3.136 0.411
2024-12-02-09:25:11-root-INFO: grad norm: 2.959 2.934 0.389
2024-12-02-09:25:11-root-INFO: Loss Change: 49.858 -> 47.932
2024-12-02-09:25:11-root-INFO: Regularization Change: 0.000 -> 2.628
2024-12-02-09:25:11-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-09:25:11-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-09:25:11-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-09:25:11-root-INFO: grad norm: 3.606 3.559 0.581
2024-12-02-09:25:11-root-INFO: Loss too large (48.028->48.918)! Learning rate decreased to 0.26762.
2024-12-02-09:25:12-root-INFO: grad norm: 4.482 4.435 0.648
2024-12-02-09:25:12-root-INFO: Loss too large (47.936->48.171)! Learning rate decreased to 0.21410.
2024-12-02-09:25:12-root-INFO: grad norm: 3.724 3.693 0.475
2024-12-02-09:25:13-root-INFO: grad norm: 3.138 3.109 0.425
2024-12-02-09:25:13-root-INFO: grad norm: 2.817 2.793 0.365
2024-12-02-09:25:14-root-INFO: Loss Change: 48.028 -> 45.719
2024-12-02-09:25:14-root-INFO: Regularization Change: 0.000 -> 2.548
2024-12-02-09:25:14-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-09:25:14-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-09:25:14-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-09:25:14-root-INFO: grad norm: 2.304 2.289 0.266
2024-12-02-09:25:15-root-INFO: grad norm: 3.596 3.575 0.384
2024-12-02-09:25:15-root-INFO: Loss too large (45.341->46.249)! Learning rate decreased to 0.27247.
2024-12-02-09:25:15-root-INFO: grad norm: 4.326 4.291 0.552
2024-12-02-09:25:15-root-INFO: Loss too large (45.295->45.319)! Learning rate decreased to 0.21798.
2024-12-02-09:25:16-root-INFO: grad norm: 3.431 3.403 0.443
2024-12-02-09:25:16-root-INFO: grad norm: 2.883 2.856 0.388
2024-12-02-09:25:17-root-INFO: Loss Change: 45.453 -> 43.470
2024-12-02-09:25:17-root-INFO: Regularization Change: 0.000 -> 2.866
2024-12-02-09:25:17-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-09:25:17-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-09:25:17-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-09:25:17-root-INFO: grad norm: 3.276 3.234 0.525
2024-12-02-09:25:17-root-INFO: Loss too large (43.560->44.150)! Learning rate decreased to 0.27737.
2024-12-02-09:25:17-root-INFO: grad norm: 3.950 3.905 0.593
2024-12-02-09:25:18-root-INFO: Loss too large (43.348->43.419)! Learning rate decreased to 0.22190.
2024-12-02-09:25:18-root-INFO: grad norm: 3.254 3.226 0.429
2024-12-02-09:25:19-root-INFO: grad norm: 2.714 2.688 0.375
2024-12-02-09:25:19-root-INFO: grad norm: 2.411 2.389 0.322
2024-12-02-09:25:19-root-INFO: Loss Change: 43.560 -> 41.286
2024-12-02-09:25:19-root-INFO: Regularization Change: 0.000 -> 2.524
2024-12-02-09:25:19-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-09:25:19-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-09:25:19-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-09:25:20-root-INFO: grad norm: 1.962 1.948 0.238
2024-12-02-09:25:20-root-INFO: grad norm: 2.782 2.764 0.312
2024-12-02-09:25:20-root-INFO: Loss too large (40.678->41.015)! Learning rate decreased to 0.28231.
2024-12-02-09:25:21-root-INFO: grad norm: 3.369 3.343 0.420
2024-12-02-09:25:21-root-INFO: grad norm: 4.013 3.983 0.495
2024-12-02-09:25:22-root-INFO: grad norm: 4.526 4.479 0.655
2024-12-02-09:25:22-root-INFO: Loss Change: 41.013 -> 40.257
2024-12-02-09:25:22-root-INFO: Regularization Change: 0.000 -> 3.986
2024-12-02-09:25:22-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-09:25:22-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-09:25:22-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-09:25:22-root-INFO: grad norm: 5.287 5.224 0.816
2024-12-02-09:25:22-root-INFO: Loss too large (40.348->41.735)! Learning rate decreased to 0.28730.
2024-12-02-09:25:23-root-INFO: grad norm: 5.068 4.998 0.840
2024-12-02-09:25:23-root-INFO: grad norm: 4.774 4.721 0.710
2024-12-02-09:25:24-root-INFO: grad norm: 4.524 4.464 0.730
2024-12-02-09:25:24-root-INFO: grad norm: 4.382 4.334 0.650
2024-12-02-09:25:25-root-INFO: Loss Change: 40.348 -> 37.542
2024-12-02-09:25:25-root-INFO: Regularization Change: 0.000 -> 4.788
2024-12-02-09:25:25-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-09:25:25-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-09:25:25-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-09:25:25-root-INFO: grad norm: 3.639 3.609 0.465
2024-12-02-09:25:25-root-INFO: Loss too large (36.990->37.363)! Learning rate decreased to 0.29232.
2024-12-02-09:25:26-root-INFO: grad norm: 3.488 3.455 0.477
2024-12-02-09:25:26-root-INFO: grad norm: 3.447 3.410 0.506
2024-12-02-09:25:26-root-INFO: grad norm: 3.462 3.427 0.493
2024-12-02-09:25:27-root-INFO: grad norm: 3.499 3.459 0.526
2024-12-02-09:25:27-root-INFO: Loss Change: 36.990 -> 34.925
2024-12-02-09:25:27-root-INFO: Regularization Change: 0.000 -> 3.895
2024-12-02-09:25:27-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-09:25:27-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-09:25:27-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-09:25:28-root-INFO: grad norm: 4.137 4.081 0.680
2024-12-02-09:25:28-root-INFO: Loss too large (35.078->35.840)! Learning rate decreased to 0.29738.
2024-12-02-09:25:28-root-INFO: grad norm: 3.972 3.918 0.649
2024-12-02-09:25:29-root-INFO: grad norm: 3.779 3.737 0.562
2024-12-02-09:25:29-root-INFO: grad norm: 3.630 3.584 0.572
2024-12-02-09:25:30-root-INFO: grad norm: 3.536 3.498 0.522
2024-12-02-09:25:30-root-INFO: Loss Change: 35.078 -> 32.798
2024-12-02-09:25:30-root-INFO: Regularization Change: 0.000 -> 4.003
2024-12-02-09:25:30-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-09:25:30-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-09:25:30-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-09:25:30-root-INFO: grad norm: 2.936 2.913 0.364
2024-12-02-09:25:30-root-INFO: Loss too large (32.277->32.368)! Learning rate decreased to 0.30249.
2024-12-02-09:25:31-root-INFO: grad norm: 2.808 2.782 0.379
2024-12-02-09:25:31-root-INFO: grad norm: 2.775 2.746 0.396
2024-12-02-09:25:32-root-INFO: grad norm: 2.799 2.771 0.396
2024-12-02-09:25:32-root-INFO: grad norm: 2.840 2.809 0.420
2024-12-02-09:25:33-root-INFO: Loss Change: 32.277 -> 30.405
2024-12-02-09:25:33-root-INFO: Regularization Change: 0.000 -> 3.436
2024-12-02-09:25:33-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-09:25:33-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-09:25:33-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-09:25:33-root-INFO: grad norm: 3.600 3.545 0.623
2024-12-02-09:25:33-root-INFO: Loss too large (30.568->31.034)! Learning rate decreased to 0.30763.
2024-12-02-09:25:34-root-INFO: grad norm: 3.387 3.340 0.564
2024-12-02-09:25:34-root-INFO: grad norm: 3.178 3.141 0.487
2024-12-02-09:25:35-root-INFO: grad norm: 3.048 3.010 0.479
2024-12-02-09:25:35-root-INFO: grad norm: 2.982 2.948 0.446
2024-12-02-09:25:35-root-INFO: Loss Change: 30.568 -> 28.457
2024-12-02-09:25:35-root-INFO: Regularization Change: 0.000 -> 3.654
2024-12-02-09:25:35-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-09:25:35-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-09:25:36-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-09:25:36-root-INFO: grad norm: 2.449 2.431 0.297
2024-12-02-09:25:36-root-INFO: grad norm: 3.241 3.214 0.417
2024-12-02-09:25:36-root-INFO: Loss too large (27.756->28.049)! Learning rate decreased to 0.31281.
2024-12-02-09:25:37-root-INFO: grad norm: 2.961 2.933 0.409
2024-12-02-09:25:37-root-INFO: grad norm: 2.690 2.662 0.387
2024-12-02-09:25:38-root-INFO: grad norm: 2.544 2.516 0.370
2024-12-02-09:25:38-root-INFO: Loss Change: 27.874 -> 26.086
2024-12-02-09:25:38-root-INFO: Regularization Change: 0.000 -> 3.434
2024-12-02-09:25:38-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-09:25:38-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-09:25:38-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-09:25:38-root-INFO: grad norm: 3.155 3.106 0.554
2024-12-02-09:25:39-root-INFO: Loss too large (26.187->26.440)! Learning rate decreased to 0.31802.
2024-12-02-09:25:39-root-INFO: grad norm: 2.857 2.818 0.470
2024-12-02-09:25:39-root-INFO: grad norm: 2.579 2.548 0.394
2024-12-02-09:25:40-root-INFO: grad norm: 2.425 2.397 0.368
2024-12-02-09:25:40-root-INFO: grad norm: 2.352 2.329 0.333
2024-12-02-09:25:41-root-INFO: Loss Change: 26.187 -> 24.234
2024-12-02-09:25:41-root-INFO: Regularization Change: 0.000 -> 3.205
2024-12-02-09:25:41-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-09:25:41-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-09:25:41-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-09:25:41-root-INFO: grad norm: 1.896 1.886 0.197
2024-12-02-09:25:41-root-INFO: grad norm: 2.412 2.398 0.256
2024-12-02-09:25:42-root-INFO: Loss too large (23.589->23.613)! Learning rate decreased to 0.32327.
2024-12-02-09:25:42-root-INFO: grad norm: 2.201 2.185 0.263
2024-12-02-09:25:43-root-INFO: grad norm: 1.984 1.969 0.243
2024-12-02-09:25:43-root-INFO: grad norm: 1.898 1.883 0.244
2024-12-02-09:25:43-root-INFO: Loss Change: 23.847 -> 22.243
2024-12-02-09:25:43-root-INFO: Regularization Change: 0.000 -> 2.967
2024-12-02-09:25:43-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-09:25:43-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-09:25:43-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-09:25:44-root-INFO: grad norm: 2.580 2.542 0.440
2024-12-02-09:25:44-root-INFO: Loss too large (22.142->22.176)! Learning rate decreased to 0.32856.
2024-12-02-09:25:44-root-INFO: grad norm: 2.275 2.247 0.353
2024-12-02-09:25:45-root-INFO: grad norm: 2.002 1.983 0.280
2024-12-02-09:25:45-root-INFO: grad norm: 1.878 1.859 0.264
2024-12-02-09:25:46-root-INFO: grad norm: 1.837 1.823 0.231
2024-12-02-09:25:46-root-INFO: Loss Change: 22.142 -> 20.449
2024-12-02-09:25:46-root-INFO: Regularization Change: 0.000 -> 2.788
2024-12-02-09:25:46-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-09:25:46-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-09:25:46-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-09:25:46-root-INFO: grad norm: 1.574 1.565 0.171
2024-12-02-09:25:47-root-INFO: grad norm: 1.869 1.861 0.169
2024-12-02-09:25:47-root-INFO: grad norm: 2.161 2.150 0.218
2024-12-02-09:25:48-root-INFO: grad norm: 2.509 2.497 0.241
2024-12-02-09:25:48-root-INFO: grad norm: 2.857 2.838 0.325
2024-12-02-09:25:49-root-INFO: Loss Change: 20.195 -> 19.312
2024-12-02-09:25:49-root-INFO: Regularization Change: 0.000 -> 4.135
2024-12-02-09:25:49-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-09:25:49-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-09:25:49-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-09:25:49-root-INFO: grad norm: 3.760 3.722 0.534
2024-12-02-09:25:49-root-INFO: Loss too large (19.332->19.354)! Learning rate decreased to 0.33923.
2024-12-02-09:25:50-root-INFO: grad norm: 2.618 2.588 0.396
2024-12-02-09:25:50-root-INFO: grad norm: 1.840 1.822 0.250
2024-12-02-09:25:51-root-INFO: grad norm: 1.477 1.462 0.210
2024-12-02-09:25:51-root-INFO: grad norm: 1.278 1.268 0.158
2024-12-02-09:25:51-root-INFO: Loss Change: 19.332 -> 16.827
2024-12-02-09:25:51-root-INFO: Regularization Change: 0.000 -> 2.892
2024-12-02-09:25:51-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-09:25:51-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-09:25:51-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-09:25:52-root-INFO: grad norm: 1.225 1.212 0.181
2024-12-02-09:25:52-root-INFO: grad norm: 1.214 1.203 0.161
2024-12-02-09:25:53-root-INFO: grad norm: 1.433 1.424 0.152
2024-12-02-09:25:53-root-INFO: grad norm: 1.635 1.625 0.177
2024-12-02-09:25:54-root-INFO: grad norm: 1.662 1.654 0.169
2024-12-02-09:25:54-root-INFO: Loss Change: 16.611 -> 15.319
2024-12-02-09:25:54-root-INFO: Regularization Change: 0.000 -> 3.391
2024-12-02-09:25:54-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-09:25:54-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-09:25:54-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-09:25:54-root-INFO: grad norm: 2.037 2.024 0.229
2024-12-02-09:25:55-root-INFO: grad norm: 2.223 2.209 0.245
2024-12-02-09:25:55-root-INFO: grad norm: 2.443 2.432 0.229
2024-12-02-09:25:56-root-INFO: grad norm: 2.542 2.526 0.283
2024-12-02-09:25:56-root-INFO: grad norm: 2.571 2.558 0.256
2024-12-02-09:25:56-root-INFO: Loss Change: 15.136 -> 14.158
2024-12-02-09:25:56-root-INFO: Regularization Change: 0.000 -> 3.810
2024-12-02-09:25:56-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-09:25:56-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-09:25:57-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-09:25:57-root-INFO: grad norm: 2.448 2.439 0.213
2024-12-02-09:25:57-root-INFO: grad norm: 2.369 2.361 0.196
2024-12-02-09:25:58-root-INFO: grad norm: 2.250 2.240 0.215
2024-12-02-09:25:58-root-INFO: grad norm: 2.182 2.174 0.188
2024-12-02-09:25:59-root-INFO: grad norm: 2.209 2.198 0.222
2024-12-02-09:25:59-root-INFO: Loss Change: 13.832 -> 12.461
2024-12-02-09:25:59-root-INFO: Regularization Change: 0.000 -> 3.740
2024-12-02-09:25:59-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-09:25:59-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-09:25:59-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-09:25:59-root-INFO: grad norm: 2.749 2.727 0.353
2024-12-02-09:26:00-root-INFO: grad norm: 2.613 2.587 0.365
2024-12-02-09:26:00-root-INFO: grad norm: 2.541 2.523 0.303
2024-12-02-09:26:01-root-INFO: grad norm: 2.513 2.487 0.362
2024-12-02-09:26:01-root-INFO: grad norm: 2.559 2.541 0.304
2024-12-02-09:26:01-root-INFO: Loss Change: 12.459 -> 11.352
2024-12-02-09:26:01-root-INFO: Regularization Change: 0.000 -> 3.877
2024-12-02-09:26:01-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-09:26:01-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-09:26:02-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-09:26:02-root-INFO: grad norm: 2.309 2.297 0.238
2024-12-02-09:26:02-root-INFO: grad norm: 2.184 2.175 0.205
2024-12-02-09:26:03-root-INFO: grad norm: 2.096 2.081 0.246
2024-12-02-09:26:03-root-INFO: grad norm: 2.070 2.061 0.199
2024-12-02-09:26:04-root-INFO: grad norm: 2.065 2.050 0.250
2024-12-02-09:26:04-root-INFO: Loss Change: 10.978 -> 9.722
2024-12-02-09:26:04-root-INFO: Regularization Change: 0.000 -> 3.298
2024-12-02-09:26:04-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-09:26:04-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-09:26:04-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-09:26:04-root-INFO: grad norm: 2.570 2.544 0.367
2024-12-02-09:26:05-root-INFO: grad norm: 2.358 2.331 0.356
2024-12-02-09:26:05-root-INFO: grad norm: 2.144 2.127 0.270
2024-12-02-09:26:06-root-INFO: grad norm: 2.067 2.044 0.304
2024-12-02-09:26:06-root-INFO: grad norm: 2.036 2.021 0.244
2024-12-02-09:26:06-root-INFO: Loss Change: 9.814 -> 8.652
2024-12-02-09:26:06-root-INFO: Regularization Change: 0.000 -> 3.247
2024-12-02-09:26:06-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-09:26:06-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-09:26:07-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-09:26:07-root-INFO: grad norm: 1.792 1.784 0.174
2024-12-02-09:26:07-root-INFO: grad norm: 1.659 1.652 0.144
2024-12-02-09:26:08-root-INFO: grad norm: 1.608 1.599 0.171
2024-12-02-09:26:08-root-INFO: grad norm: 1.608 1.602 0.139
2024-12-02-09:26:09-root-INFO: grad norm: 1.598 1.588 0.177
2024-12-02-09:26:09-root-INFO: Loss Change: 8.368 -> 7.407
2024-12-02-09:26:09-root-INFO: Regularization Change: 0.000 -> 2.568
2024-12-02-09:26:09-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-09:26:09-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-09:26:09-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-09:26:09-root-INFO: grad norm: 2.162 2.136 0.331
2024-12-02-09:26:10-root-INFO: grad norm: 1.951 1.929 0.292
2024-12-02-09:26:10-root-INFO: grad norm: 1.767 1.753 0.218
2024-12-02-09:26:11-root-INFO: grad norm: 1.670 1.653 0.237
2024-12-02-09:26:11-root-INFO: grad norm: 1.598 1.588 0.184
2024-12-02-09:26:12-root-INFO: Loss Change: 7.488 -> 6.481
2024-12-02-09:26:12-root-INFO: Regularization Change: 0.000 -> 2.584
2024-12-02-09:26:12-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-09:26:12-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-09:26:12-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-09:26:12-root-INFO: grad norm: 1.417 1.412 0.122
2024-12-02-09:26:12-root-INFO: grad norm: 1.278 1.274 0.098
2024-12-02-09:26:13-root-INFO: grad norm: 1.248 1.244 0.100
2024-12-02-09:26:13-root-INFO: grad norm: 1.224 1.220 0.092
2024-12-02-09:26:14-root-INFO: grad norm: 1.193 1.189 0.101
2024-12-02-09:26:14-root-INFO: Loss Change: 6.280 -> 5.529
2024-12-02-09:26:14-root-INFO: Regularization Change: 0.000 -> 1.967
2024-12-02-09:26:14-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-09:26:14-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-09:26:14-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-09:26:15-root-INFO: grad norm: 1.712 1.693 0.259
2024-12-02-09:26:15-root-INFO: grad norm: 1.521 1.506 0.216
2024-12-02-09:26:15-root-INFO: grad norm: 1.395 1.386 0.160
2024-12-02-09:26:16-root-INFO: grad norm: 1.323 1.311 0.175
2024-12-02-09:26:16-root-INFO: grad norm: 1.268 1.261 0.134
2024-12-02-09:26:17-root-INFO: Loss Change: 5.606 -> 4.863
2024-12-02-09:26:17-root-INFO: Regularization Change: 0.000 -> 1.917
2024-12-02-09:26:17-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-09:26:17-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-09:26:17-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-09:26:17-root-INFO: grad norm: 1.214 1.210 0.101
2024-12-02-09:26:18-root-INFO: grad norm: 1.069 1.067 0.077
2024-12-02-09:26:18-root-INFO: grad norm: 1.051 1.048 0.067
2024-12-02-09:26:19-root-INFO: grad norm: 1.043 1.041 0.071
2024-12-02-09:26:19-root-INFO: grad norm: 1.024 1.022 0.071
2024-12-02-09:26:19-root-INFO: Loss Change: 4.763 -> 4.192
2024-12-02-09:26:19-root-INFO: Regularization Change: 0.000 -> 1.540
2024-12-02-09:26:19-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-09:26:19-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-09:26:20-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-09:26:20-root-INFO: grad norm: 1.538 1.521 0.234
2024-12-02-09:26:20-root-INFO: grad norm: 1.293 1.281 0.173
2024-12-02-09:26:21-root-INFO: grad norm: 1.134 1.127 0.126
2024-12-02-09:26:21-root-INFO: grad norm: 1.040 1.032 0.133
2024-12-02-09:26:22-root-INFO: grad norm: 1.070 1.064 0.108
2024-12-02-09:26:22-root-INFO: Loss Change: 4.293 -> 3.707
2024-12-02-09:26:22-root-INFO: Regularization Change: 0.000 -> 1.542
2024-12-02-09:26:22-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-09:26:22-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-09:26:22-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-09:26:22-root-INFO: grad norm: 1.137 1.130 0.123
2024-12-02-09:26:23-root-INFO: grad norm: 0.889 0.886 0.074
2024-12-02-09:26:23-root-INFO: grad norm: 0.894 0.893 0.058
2024-12-02-09:26:24-root-INFO: grad norm: 1.218 1.217 0.067
2024-12-02-09:26:24-root-INFO: grad norm: 0.856 0.854 0.054
2024-12-02-09:26:24-root-INFO: Loss Change: 3.702 -> 3.233
2024-12-02-09:26:24-root-INFO: Regularization Change: 0.000 -> 1.217
2024-12-02-09:26:24-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-09:26:24-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-09:26:24-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-09:26:25-root-INFO: grad norm: 1.199 1.184 0.184
2024-12-02-09:26:25-root-INFO: grad norm: 0.946 0.938 0.122
2024-12-02-09:26:26-root-INFO: grad norm: 1.025 1.019 0.107
2024-12-02-09:26:26-root-INFO: grad norm: 0.822 0.817 0.097
2024-12-02-09:26:27-root-INFO: grad norm: 0.752 0.749 0.067
2024-12-02-09:26:27-root-INFO: Loss Change: 3.332 -> 2.805
2024-12-02-09:26:27-root-INFO: Regularization Change: 0.000 -> 1.172
2024-12-02-09:26:27-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-09:26:27-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-09:26:27-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-09:26:27-root-INFO: grad norm: 1.107 1.097 0.150
2024-12-02-09:26:28-root-INFO: grad norm: 0.738 0.733 0.085
2024-12-02-09:26:28-root-INFO: grad norm: 0.589 0.586 0.055
2024-12-02-09:26:29-root-INFO: grad norm: 0.591 0.589 0.047
2024-12-02-09:26:29-root-INFO: grad norm: 0.690 0.688 0.058
2024-12-02-09:26:29-root-INFO: Loss Change: 2.902 -> 2.525
2024-12-02-09:26:29-root-INFO: Regularization Change: 0.000 -> 0.911
2024-12-02-09:26:29-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-09:26:29-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-09:26:30-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-09:26:30-root-INFO: grad norm: 1.277 1.261 0.203
2024-12-02-09:26:30-root-INFO: grad norm: 0.763 0.757 0.100
2024-12-02-09:26:31-root-INFO: grad norm: 0.600 0.597 0.062
2024-12-02-09:26:31-root-INFO: grad norm: 0.584 0.583 0.043
2024-12-02-09:26:32-root-INFO: grad norm: 0.653 0.650 0.063
2024-12-02-09:26:32-root-INFO: Loss Change: 2.679 -> 2.249
2024-12-02-09:26:32-root-INFO: Regularization Change: 0.000 -> 0.841
2024-12-02-09:26:32-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-09:26:32-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-09:26:32-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-09:26:32-root-INFO: grad norm: 1.152 1.137 0.185
2024-12-02-09:26:33-root-INFO: grad norm: 0.749 0.743 0.100
2024-12-02-09:26:33-root-INFO: grad norm: 0.567 0.563 0.061
2024-12-02-09:26:34-root-INFO: grad norm: 0.645 0.640 0.078
2024-12-02-09:26:34-root-INFO: grad norm: 0.700 0.697 0.059
2024-12-02-09:26:34-root-INFO: Loss Change: 2.407 -> 2.019
2024-12-02-09:26:34-root-INFO: Regularization Change: 0.000 -> 0.897
2024-12-02-09:26:34-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-09:26:34-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-09:26:35-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-09:26:35-root-INFO: grad norm: 0.975 0.966 0.129
2024-12-02-09:26:35-root-INFO: grad norm: 0.648 0.646 0.050
2024-12-02-09:26:36-root-INFO: grad norm: 0.533 0.531 0.055
2024-12-02-09:26:36-root-INFO: grad norm: 0.497 0.495 0.040
2024-12-02-09:26:37-root-INFO: grad norm: 0.463 0.460 0.047
2024-12-02-09:26:37-root-INFO: Loss Change: 2.144 -> 1.796
2024-12-02-09:26:37-root-INFO: Regularization Change: 0.000 -> 0.771
2024-12-02-09:26:37-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-09:26:37-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-09:26:37-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-09:26:37-root-INFO: grad norm: 1.483 1.474 0.156
2024-12-02-09:26:38-root-INFO: grad norm: 1.893 1.891 0.087
2024-12-02-09:26:38-root-INFO: grad norm: 0.743 0.741 0.044
2024-12-02-09:26:39-root-INFO: grad norm: 0.331 0.329 0.035
2024-12-02-09:26:39-root-INFO: grad norm: 0.277 0.276 0.030
2024-12-02-09:26:40-root-INFO: Loss Change: 1.959 -> 1.575
2024-12-02-09:26:40-root-INFO: Regularization Change: 0.000 -> 1.300
2024-12-02-09:26:40-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-09:26:40-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-09:26:40-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-09:26:40-root-INFO: grad norm: 0.797 0.789 0.117
2024-12-02-09:26:40-root-INFO: grad norm: 0.397 0.394 0.050
2024-12-02-09:26:41-root-INFO: grad norm: 0.276 0.274 0.035
2024-12-02-09:26:41-root-INFO: grad norm: 0.244 0.241 0.034
2024-12-02-09:26:42-root-INFO: grad norm: 0.238 0.237 0.030
2024-12-02-09:26:42-root-INFO: Loss Change: 1.731 -> 1.441
2024-12-02-09:26:42-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-02-09:26:42-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-09:26:42-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-09:26:42-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-09:26:42-root-INFO: grad norm: 0.712 0.706 0.092
2024-12-02-09:26:43-root-INFO: grad norm: 0.505 0.503 0.047
2024-12-02-09:26:43-root-INFO: grad norm: 0.442 0.440 0.034
2024-12-02-09:26:44-root-INFO: grad norm: 0.483 0.482 0.035
2024-12-02-09:26:44-root-INFO: Loss too large (1.380->1.385)! Learning rate decreased to 0.45653.
2024-12-02-09:26:44-root-INFO: grad norm: 0.646 0.645 0.029
2024-12-02-09:26:45-root-INFO: Loss Change: 1.584 -> 1.361
2024-12-02-09:26:45-root-INFO: Regularization Change: 0.000 -> 0.506
2024-12-02-09:26:45-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-09:26:45-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-09:26:45-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-09:26:45-root-INFO: grad norm: 0.716 0.711 0.083
2024-12-02-09:26:46-root-INFO: grad norm: 0.499 0.498 0.039
2024-12-02-09:26:46-root-INFO: grad norm: 0.444 0.443 0.031
2024-12-02-09:26:46-root-INFO: grad norm: 0.405 0.404 0.030
2024-12-02-09:26:47-root-INFO: grad norm: 0.406 0.405 0.026
2024-12-02-09:26:47-root-INFO: Loss Change: 1.493 -> 1.225
2024-12-02-09:26:47-root-INFO: Regularization Change: 0.000 -> 0.575
2024-12-02-09:26:47-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-09:26:47-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-09:26:47-root-INFO: loss_sample0_0: 1.2248053550720215
2024-12-02-09:26:48-root-INFO: It takes 753.614 seconds for image sample0
2024-12-02-09:26:48-root-INFO: lpips_score_sample0: 0.143
2024-12-02-09:26:48-root-INFO: psnr_score_sample0: 17.690
2024-12-02-09:26:48-root-INFO: ssim_score_sample0: 0.726
2024-12-02-09:26:48-root-INFO: mean_lpips: 0.14250211417675018
2024-12-02-09:26:48-root-INFO: best_mean_lpips: 0.14250211417675018
2024-12-02-09:26:48-root-INFO: mean_psnr: 17.689817428588867
2024-12-02-09:26:48-root-INFO: best_mean_psnr: 17.689817428588867
2024-12-02-09:26:48-root-INFO: mean_ssim: 0.7264518737792969
2024-12-02-09:26:48-root-INFO: best_mean_ssim: 0.7264518737792969
2024-12-02-09:26:48-root-INFO: final_loss: 1.2248053550720215
2024-12-02-09:26:48-root-INFO: mean time: 753.6144263744354
2024-12-02-09:26:48-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample1_iter5_lr0.03_10009 
 
Enjoy.
