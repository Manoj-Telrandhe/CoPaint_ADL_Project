2024-12-02-01:00:05-root-INFO: Prepare model...
2024-12-02-01:00:20-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-01:00:46-root-INFO: Start sampling
2024-12-02-01:00:55-root-INFO: step: 249 lr_xt 0.00019059
2024-12-02-01:00:55-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-01:00:56-root-INFO: Loss too large (77070.016->78612.719)! Learning rate decreased to 0.00015.
2024-12-02-01:00:57-root-INFO: grad norm: 15661.126 11248.851 10896.524
2024-12-02-01:00:57-root-INFO: Loss Change: 77070.016 -> 41352.211
2024-12-02-01:00:57-root-INFO: Regularization Change: 0.000 -> 13.469
2024-12-02-01:00:57-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-01:00:57-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-01:00:58-root-INFO: step: 248 lr_xt 0.00020082
2024-12-02-01:00:58-root-INFO: grad norm: 15701.229 11446.181 10747.723
2024-12-02-01:00:58-root-INFO: Loss too large (37235.867->50510.512)! Learning rate decreased to 0.00016.
2024-12-02-01:00:59-root-INFO: grad norm: 22338.281 16896.535 14611.840
2024-12-02-01:01:00-root-INFO: Loss too large (35233.355->52901.566)! Learning rate decreased to 0.00013.
2024-12-02-01:01:00-root-INFO: Loss too large (35233.355->40696.824)! Learning rate decreased to 0.00010.
2024-12-02-01:01:01-root-INFO: Loss Change: 37235.867 -> 30680.496
2024-12-02-01:01:01-root-INFO: Regularization Change: 0.000 -> 1.727
2024-12-02-01:01:01-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-02-01:01:01-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-01:01:01-root-INFO: step: 247 lr_xt 0.00021156
2024-12-02-01:01:01-root-INFO: grad norm: 17054.883 13055.664 10973.543
2024-12-02-01:01:02-root-INFO: Loss too large (29564.922->72512.719)! Learning rate decreased to 0.00017.
2024-12-02-01:01:02-root-INFO: Loss too large (29564.922->47837.781)! Learning rate decreased to 0.00014.
2024-12-02-01:01:02-root-INFO: Loss too large (29564.922->32723.391)! Learning rate decreased to 0.00011.
2024-12-02-01:01:03-root-INFO: grad norm: 15445.050 12359.732 9262.105
2024-12-02-01:01:04-root-INFO: Loss too large (24640.902->25551.943)! Learning rate decreased to 0.00009.
2024-12-02-01:01:04-root-INFO: Loss Change: 29564.922 -> 21262.279
2024-12-02-01:01:04-root-INFO: Regularization Change: 0.000 -> 0.904
2024-12-02-01:01:04-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-02-01:01:04-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-01:01:05-root-INFO: step: 246 lr_xt 0.00022285
2024-12-02-01:01:05-root-INFO: grad norm: 10880.894 8797.607 6402.808
2024-12-02-01:01:05-root-INFO: Loss too large (20900.348->47112.172)! Learning rate decreased to 0.00018.
2024-12-02-01:01:06-root-INFO: Loss too large (20900.348->33637.898)! Learning rate decreased to 0.00014.
2024-12-02-01:01:06-root-INFO: Loss too large (20900.348->25659.504)! Learning rate decreased to 0.00011.
2024-12-02-01:01:06-root-INFO: Loss too large (20900.348->21201.904)! Learning rate decreased to 0.00009.
2024-12-02-01:01:07-root-INFO: grad norm: 7895.841 6382.487 4648.458
2024-12-02-01:01:08-root-INFO: Loss Change: 20900.348 -> 17923.344
2024-12-02-01:01:08-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-02-01:01:08-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-02-01:01:08-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-01:01:08-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-01:01:09-root-INFO: grad norm: 5732.622 4690.697 3295.498
2024-12-02-01:01:09-root-INFO: Loss too large (17760.223->24841.090)! Learning rate decreased to 0.00019.
2024-12-02-01:01:09-root-INFO: Loss too large (17760.223->21058.297)! Learning rate decreased to 0.00015.
2024-12-02-01:01:10-root-INFO: Loss too large (17760.223->18876.215)! Learning rate decreased to 0.00012.
2024-12-02-01:01:11-root-INFO: grad norm: 6265.687 5056.732 3699.770
2024-12-02-01:01:11-root-INFO: Loss too large (17678.137->17738.859)! Learning rate decreased to 0.00010.
2024-12-02-01:01:12-root-INFO: Loss Change: 17760.223 -> 16976.738
2024-12-02-01:01:12-root-INFO: Regularization Change: 0.000 -> 0.104
2024-12-02-01:01:12-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-01:01:12-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-01:01:12-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-01:01:12-root-INFO: grad norm: 4212.016 3513.121 2323.588
2024-12-02-01:01:13-root-INFO: Loss too large (16852.141->20500.537)! Learning rate decreased to 0.00020.
2024-12-02-01:01:13-root-INFO: Loss too large (16852.141->18477.746)! Learning rate decreased to 0.00016.
2024-12-02-01:01:13-root-INFO: Loss too large (16852.141->17324.408)! Learning rate decreased to 0.00013.
2024-12-02-01:01:14-root-INFO: grad norm: 4315.023 3500.785 2522.684
2024-12-02-01:01:15-root-INFO: Loss Change: 16852.141 -> 16621.770
2024-12-02-01:01:15-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-02-01:01:15-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-01:01:15-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:15-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-01:01:16-root-INFO: grad norm: 4314.125 3570.027 2422.103
2024-12-02-01:01:16-root-INFO: Loss too large (16416.225->20529.062)! Learning rate decreased to 0.00021.
2024-12-02-01:01:16-root-INFO: Loss too large (16416.225->18257.848)! Learning rate decreased to 0.00017.
2024-12-02-01:01:17-root-INFO: Loss too large (16416.225->16962.398)! Learning rate decreased to 0.00013.
2024-12-02-01:01:18-root-INFO: grad norm: 4223.731 3475.842 2399.674
2024-12-02-01:01:18-root-INFO: Loss Change: 16416.225 -> 16161.454
2024-12-02-01:01:18-root-INFO: Regularization Change: 0.000 -> 0.059
2024-12-02-01:01:18-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-01:01:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:19-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-01:01:19-root-INFO: grad norm: 3836.514 3190.826 2130.133
2024-12-02-01:01:19-root-INFO: Loss too large (15875.368->19101.832)! Learning rate decreased to 0.00022.
2024-12-02-01:01:20-root-INFO: Loss too large (15875.368->17278.574)! Learning rate decreased to 0.00018.
2024-12-02-01:01:20-root-INFO: Loss too large (15875.368->16245.701)! Learning rate decreased to 0.00014.
2024-12-02-01:01:21-root-INFO: grad norm: 3573.970 2970.391 1987.471
2024-12-02-01:01:22-root-INFO: Loss Change: 15875.368 -> 15558.244
2024-12-02-01:01:22-root-INFO: Regularization Change: 0.000 -> 0.055
2024-12-02-01:01:22-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-01:01:22-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:22-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-01:01:22-root-INFO: grad norm: 3107.927 2576.686 1737.786
2024-12-02-01:01:23-root-INFO: Loss too large (15460.994->17477.629)! Learning rate decreased to 0.00023.
2024-12-02-01:01:23-root-INFO: Loss too large (15460.994->16296.986)! Learning rate decreased to 0.00018.
2024-12-02-01:01:23-root-INFO: Loss too large (15460.994->15633.424)! Learning rate decreased to 0.00015.
2024-12-02-01:01:24-root-INFO: grad norm: 2800.498 2360.662 1506.672
2024-12-02-01:01:25-root-INFO: Loss Change: 15460.994 -> 15139.561
2024-12-02-01:01:25-root-INFO: Regularization Change: 0.000 -> 0.061
2024-12-02-01:01:25-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-01:01:25-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:25-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-01:01:26-root-INFO: grad norm: 2353.586 1951.435 1315.777
2024-12-02-01:01:26-root-INFO: Loss too large (14933.938->15914.114)! Learning rate decreased to 0.00024.
2024-12-02-01:01:26-root-INFO: Loss too large (14933.938->15286.988)! Learning rate decreased to 0.00019.
2024-12-02-01:01:27-root-INFO: Loss too large (14933.938->14941.584)! Learning rate decreased to 0.00016.
2024-12-02-01:01:28-root-INFO: grad norm: 2051.845 1762.803 1050.044
2024-12-02-01:01:28-root-INFO: Loss Change: 14933.938 -> 14626.931
2024-12-02-01:01:28-root-INFO: Regularization Change: 0.000 -> 0.070
2024-12-02-01:01:28-root-INFO: Undo step: 240
2024-12-02-01:01:28-root-INFO: Undo step: 241
2024-12-02-01:01:28-root-INFO: Undo step: 242
2024-12-02-01:01:28-root-INFO: Undo step: 243
2024-12-02-01:01:28-root-INFO: Undo step: 244
2024-12-02-01:01:29-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-01:01:29-root-INFO: grad norm: 10579.234 7614.191 7344.677
2024-12-02-01:01:29-root-INFO: Loss too large (21291.344->32775.707)! Learning rate decreased to 0.00019.
2024-12-02-01:01:30-root-INFO: Loss too large (21291.344->25301.449)! Learning rate decreased to 0.00015.
2024-12-02-01:01:31-root-INFO: grad norm: 10210.643 7295.411 7143.822
2024-12-02-01:01:31-root-INFO: Loss Change: 21291.344 -> 20301.066
2024-12-02-01:01:31-root-INFO: Regularization Change: 0.000 -> 0.324
2024-12-02-01:01:31-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-01:01:31-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-01:01:32-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-01:01:32-root-INFO: grad norm: 9479.533 6982.750 6411.143
2024-12-02-01:01:32-root-INFO: Loss too large (19755.660->30489.582)! Learning rate decreased to 0.00020.
2024-12-02-01:01:33-root-INFO: Loss too large (19755.660->24060.336)! Learning rate decreased to 0.00016.
2024-12-02-01:01:33-root-INFO: Loss too large (19755.660->20041.471)! Learning rate decreased to 0.00013.
2024-12-02-01:01:34-root-INFO: grad norm: 6394.118 4751.785 4278.468
2024-12-02-01:01:35-root-INFO: Loss Change: 19755.660 -> 16699.609
2024-12-02-01:01:35-root-INFO: Regularization Change: 0.000 -> 0.297
2024-12-02-01:01:35-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-01:01:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:35-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-01:01:35-root-INFO: grad norm: 4616.547 3558.803 2940.651
2024-12-02-01:01:36-root-INFO: Loss too large (16485.395->19670.055)! Learning rate decreased to 0.00021.
2024-12-02-01:01:36-root-INFO: Loss too large (16485.395->17777.855)! Learning rate decreased to 0.00017.
2024-12-02-01:01:36-root-INFO: Loss too large (16485.395->16665.408)! Learning rate decreased to 0.00013.
2024-12-02-01:01:37-root-INFO: grad norm: 3572.892 2701.026 2338.807
2024-12-02-01:01:38-root-INFO: Loss Change: 16485.395 -> 15723.190
2024-12-02-01:01:38-root-INFO: Regularization Change: 0.000 -> 0.100
2024-12-02-01:01:38-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-01:01:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:38-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-01:01:39-root-INFO: grad norm: 3084.935 2500.889 1806.205
2024-12-02-01:01:39-root-INFO: Loss too large (15604.561->16993.316)! Learning rate decreased to 0.00022.
2024-12-02-01:01:39-root-INFO: Loss too large (15604.561->16109.558)! Learning rate decreased to 0.00018.
2024-12-02-01:01:40-root-INFO: Loss too large (15604.561->15608.707)! Learning rate decreased to 0.00014.
2024-12-02-01:01:41-root-INFO: grad norm: 2504.008 1927.977 1597.798
2024-12-02-01:01:42-root-INFO: Loss Change: 15604.561 -> 15143.054
2024-12-02-01:01:42-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-02-01:01:42-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-01:01:42-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:42-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-01:01:42-root-INFO: grad norm: 2219.775 1879.536 1180.992
2024-12-02-01:01:43-root-INFO: Loss too large (15134.571->15755.225)! Learning rate decreased to 0.00023.
2024-12-02-01:01:43-root-INFO: Loss too large (15134.571->15312.150)! Learning rate decreased to 0.00018.
2024-12-02-01:01:44-root-INFO: grad norm: 2678.176 2104.434 1656.497
2024-12-02-01:01:44-root-INFO: Loss too large (15069.533->15082.000)! Learning rate decreased to 0.00015.
2024-12-02-01:01:45-root-INFO: Loss Change: 15134.571 -> 14861.545
2024-12-02-01:01:45-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-01:01:45-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-01:01:45-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:45-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-01:01:46-root-INFO: grad norm: 2366.650 2022.713 1228.683
2024-12-02-01:01:46-root-INFO: Loss too large (14748.040->15537.279)! Learning rate decreased to 0.00024.
2024-12-02-01:01:46-root-INFO: Loss too large (14748.040->14989.836)! Learning rate decreased to 0.00019.
2024-12-02-01:01:47-root-INFO: grad norm: 2820.626 2246.852 1705.165
2024-12-02-01:01:48-root-INFO: Loss too large (14689.060->14709.589)! Learning rate decreased to 0.00016.
2024-12-02-01:01:48-root-INFO: Loss Change: 14748.040 -> 14449.672
2024-12-02-01:01:48-root-INFO: Regularization Change: 0.000 -> 0.106
2024-12-02-01:01:48-root-INFO: Undo step: 240
2024-12-02-01:01:48-root-INFO: Undo step: 241
2024-12-02-01:01:48-root-INFO: Undo step: 242
2024-12-02-01:01:48-root-INFO: Undo step: 243
2024-12-02-01:01:48-root-INFO: Undo step: 244
2024-12-02-01:01:49-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-01:01:49-root-INFO: grad norm: 15640.460 12471.152 9438.980
2024-12-02-01:01:49-root-INFO: Loss too large (25954.707->46061.227)! Learning rate decreased to 0.00019.
2024-12-02-01:01:50-root-INFO: Loss too large (25954.707->35889.414)! Learning rate decreased to 0.00015.
2024-12-02-01:01:50-root-INFO: Loss too large (25954.707->27673.928)! Learning rate decreased to 0.00012.
2024-12-02-01:01:51-root-INFO: grad norm: 12612.272 9743.268 8008.630
2024-12-02-01:01:52-root-INFO: Loss Change: 25954.707 -> 19310.250
2024-12-02-01:01:52-root-INFO: Regularization Change: 0.000 -> 1.224
2024-12-02-01:01:52-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-01:01:52-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-01:01:52-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-01:01:52-root-INFO: grad norm: 10709.708 8823.767 6069.512
2024-12-02-01:01:53-root-INFO: Loss too large (19242.957->38072.328)! Learning rate decreased to 0.00020.
2024-12-02-01:01:53-root-INFO: Loss too large (19242.957->28721.441)! Learning rate decreased to 0.00016.
2024-12-02-01:01:53-root-INFO: Loss too large (19242.957->22551.535)! Learning rate decreased to 0.00013.
2024-12-02-01:01:54-root-INFO: grad norm: 9814.189 7831.991 5914.240
2024-12-02-01:01:55-root-INFO: Loss Change: 19242.957 -> 17594.984
2024-12-02-01:01:55-root-INFO: Regularization Change: 0.000 -> 0.210
2024-12-02-01:01:55-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-01:01:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:55-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-01:01:56-root-INFO: grad norm: 8275.582 6777.732 4748.432
2024-12-02-01:01:56-root-INFO: Loss too large (17090.535->29458.090)! Learning rate decreased to 0.00021.
2024-12-02-01:01:56-root-INFO: Loss too large (17090.535->23078.463)! Learning rate decreased to 0.00017.
2024-12-02-01:01:57-root-INFO: Loss too large (17090.535->19044.924)! Learning rate decreased to 0.00013.
2024-12-02-01:01:58-root-INFO: grad norm: 7267.112 5838.670 4326.760
2024-12-02-01:01:58-root-INFO: Loss Change: 17090.535 -> 15886.547
2024-12-02-01:01:58-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-02-01:01:58-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-01:01:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:01:59-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-01:01:59-root-INFO: grad norm: 6156.353 5040.521 3534.661
2024-12-02-01:01:59-root-INFO: Loss too large (15736.569->22854.510)! Learning rate decreased to 0.00022.
2024-12-02-01:02:00-root-INFO: Loss too large (15736.569->18977.521)! Learning rate decreased to 0.00018.
2024-12-02-01:02:00-root-INFO: Loss too large (15736.569->16627.158)! Learning rate decreased to 0.00014.
2024-12-02-01:02:01-root-INFO: grad norm: 5212.153 4214.625 3066.507
2024-12-02-01:02:02-root-INFO: Loss Change: 15736.569 -> 14810.669
2024-12-02-01:02:02-root-INFO: Regularization Change: 0.000 -> 0.086
2024-12-02-01:02:02-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-01:02:02-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:02-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-01:02:02-root-INFO: grad norm: 4366.488 3605.941 2462.398
2024-12-02-01:02:03-root-INFO: Loss too large (14813.041->18469.057)! Learning rate decreased to 0.00023.
2024-12-02-01:02:03-root-INFO: Loss too large (14813.041->16382.043)! Learning rate decreased to 0.00018.
2024-12-02-01:02:03-root-INFO: Loss too large (14813.041->15156.824)! Learning rate decreased to 0.00015.
2024-12-02-01:02:04-root-INFO: grad norm: 3645.774 2952.476 2138.820
2024-12-02-01:02:05-root-INFO: Loss Change: 14813.041 -> 14187.639
2024-12-02-01:02:05-root-INFO: Regularization Change: 0.000 -> 0.084
2024-12-02-01:02:05-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-01:02:05-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:05-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-01:02:06-root-INFO: grad norm: 3006.048 2501.960 1666.290
2024-12-02-01:02:06-root-INFO: Loss too large (14057.506->15603.110)! Learning rate decreased to 0.00024.
2024-12-02-01:02:06-root-INFO: Loss too large (14057.506->14631.226)! Learning rate decreased to 0.00019.
2024-12-02-01:02:07-root-INFO: Loss too large (14057.506->14082.908)! Learning rate decreased to 0.00016.
2024-12-02-01:02:08-root-INFO: grad norm: 2437.084 1979.485 1421.626
2024-12-02-01:02:08-root-INFO: Loss Change: 14057.506 -> 13592.172
2024-12-02-01:02:08-root-INFO: Regularization Change: 0.000 -> 0.091
2024-12-02-01:02:08-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-01:02:08-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:09-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-01:02:09-root-INFO: grad norm: 1904.760 1624.889 993.906
2024-12-02-01:02:09-root-INFO: Loss too large (13525.859->13982.523)! Learning rate decreased to 0.00026.
2024-12-02-01:02:10-root-INFO: Loss too large (13525.859->13636.872)! Learning rate decreased to 0.00020.
2024-12-02-01:02:11-root-INFO: grad norm: 2231.844 1802.913 1315.535
2024-12-02-01:02:11-root-INFO: Loss Change: 13525.859 -> 13412.571
2024-12-02-01:02:11-root-INFO: Regularization Change: 0.000 -> 0.121
2024-12-02-01:02:11-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-01:02:11-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:12-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-01:02:12-root-INFO: grad norm: 2617.425 2194.822 1426.068
2024-12-02-01:02:12-root-INFO: Loss too large (13235.734->14363.871)! Learning rate decreased to 0.00027.
2024-12-02-01:02:13-root-INFO: Loss too large (13235.734->13601.455)! Learning rate decreased to 0.00021.
2024-12-02-01:02:14-root-INFO: grad norm: 2985.246 2449.434 1706.449
2024-12-02-01:02:14-root-INFO: Loss too large (13180.151->13194.548)! Learning rate decreased to 0.00017.
2024-12-02-01:02:15-root-INFO: Loss Change: 13235.734 -> 12877.328
2024-12-02-01:02:15-root-INFO: Regularization Change: 0.000 -> 0.117
2024-12-02-01:02:15-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-01:02:15-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:15-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-01:02:15-root-INFO: grad norm: 2089.579 1789.337 1079.172
2024-12-02-01:02:16-root-INFO: Loss too large (12783.265->13413.888)! Learning rate decreased to 0.00028.
2024-12-02-01:02:16-root-INFO: Loss too large (12783.265->12945.135)! Learning rate decreased to 0.00023.
2024-12-02-01:02:17-root-INFO: grad norm: 2332.575 1915.678 1330.821
2024-12-02-01:02:18-root-INFO: Loss Change: 12783.265 -> 12635.312
2024-12-02-01:02:18-root-INFO: Regularization Change: 0.000 -> 0.141
2024-12-02-01:02:18-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-01:02:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:18-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-01:02:18-root-INFO: grad norm: 2632.624 2250.084 1366.685
2024-12-02-01:02:18-root-INFO: Loss too large (12543.545->13853.742)! Learning rate decreased to 0.00030.
2024-12-02-01:02:19-root-INFO: Loss too large (12543.545->12985.754)! Learning rate decreased to 0.00024.
2024-12-02-01:02:20-root-INFO: grad norm: 2920.663 2423.595 1629.865
2024-12-02-01:02:20-root-INFO: Loss Change: 12543.545 -> 12476.225
2024-12-02-01:02:20-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-02-01:02:20-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-01:02:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:21-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-01:02:21-root-INFO: grad norm: 2980.935 2549.226 1545.128
2024-12-02-01:02:21-root-INFO: Loss too large (12335.613->14102.799)! Learning rate decreased to 0.00031.
2024-12-02-01:02:22-root-INFO: Loss too large (12335.613->12937.742)! Learning rate decreased to 0.00025.
2024-12-02-01:02:23-root-INFO: grad norm: 3205.758 2681.964 1756.120
2024-12-02-01:02:23-root-INFO: Loss Change: 12335.613 -> 12238.627
2024-12-02-01:02:23-root-INFO: Regularization Change: 0.000 -> 0.150
2024-12-02-01:02:23-root-INFO: Undo step: 235
2024-12-02-01:02:24-root-INFO: Undo step: 236
2024-12-02-01:02:24-root-INFO: Undo step: 237
2024-12-02-01:02:24-root-INFO: Undo step: 238
2024-12-02-01:02:24-root-INFO: Undo step: 239
2024-12-02-01:02:24-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-01:02:24-root-INFO: grad norm: 3214.506 2492.375 2030.052
2024-12-02-01:02:25-root-INFO: grad norm: 3648.807 2651.723 2506.424
2024-12-02-01:02:25-root-INFO: Loss too large (13455.979->14378.404)! Learning rate decreased to 0.00024.
2024-12-02-01:02:26-root-INFO: Loss Change: 13997.514 -> 13405.949
2024-12-02-01:02:26-root-INFO: Regularization Change: 0.000 -> 0.786
2024-12-02-01:02:26-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-01:02:26-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:26-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-01:02:27-root-INFO: grad norm: 3808.378 2786.078 2596.442
2024-12-02-01:02:27-root-INFO: Loss too large (13309.597->14431.501)! Learning rate decreased to 0.00026.
2024-12-02-01:02:28-root-INFO: grad norm: 4220.342 3066.570 2899.557
2024-12-02-01:02:28-root-INFO: Loss too large (13286.152->13408.906)! Learning rate decreased to 0.00020.
2024-12-02-01:02:29-root-INFO: Loss Change: 13309.597 -> 12651.893
2024-12-02-01:02:29-root-INFO: Regularization Change: 0.000 -> 0.185
2024-12-02-01:02:29-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-01:02:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:29-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-01:02:30-root-INFO: grad norm: 3150.206 2390.635 2051.502
2024-12-02-01:02:30-root-INFO: Loss too large (12486.078->13253.178)! Learning rate decreased to 0.00027.
2024-12-02-01:02:31-root-INFO: grad norm: 3686.470 2828.208 2364.593
2024-12-02-01:02:31-root-INFO: Loss too large (12423.083->12791.299)! Learning rate decreased to 0.00021.
2024-12-02-01:02:32-root-INFO: Loss Change: 12486.078 -> 12103.391
2024-12-02-01:02:32-root-INFO: Regularization Change: 0.000 -> 0.208
2024-12-02-01:02:32-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-01:02:32-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:32-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-01:02:33-root-INFO: grad norm: 3156.632 2577.000 1823.018
2024-12-02-01:02:33-root-INFO: Loss too large (12052.662->13581.256)! Learning rate decreased to 0.00028.
2024-12-02-01:02:33-root-INFO: Loss too large (12052.662->12448.064)! Learning rate decreased to 0.00023.
2024-12-02-01:02:34-root-INFO: grad norm: 3166.754 2665.392 1709.976
2024-12-02-01:02:35-root-INFO: Loss Change: 12052.662 -> 11834.162
2024-12-02-01:02:35-root-INFO: Regularization Change: 0.000 -> 0.163
2024-12-02-01:02:35-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-01:02:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:35-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-01:02:36-root-INFO: grad norm: 3319.587 2812.925 1762.700
2024-12-02-01:02:36-root-INFO: Loss too large (11673.225->13953.801)! Learning rate decreased to 0.00030.
2024-12-02-01:02:37-root-INFO: Loss too large (11673.225->12467.676)! Learning rate decreased to 0.00024.
2024-12-02-01:02:38-root-INFO: grad norm: 3668.964 3131.909 1911.136
2024-12-02-01:02:38-root-INFO: Loss too large (11648.348->11734.613)! Learning rate decreased to 0.00019.
2024-12-02-01:02:38-root-INFO: Loss Change: 11673.225 -> 11199.679
2024-12-02-01:02:38-root-INFO: Regularization Change: 0.000 -> 0.114
2024-12-02-01:02:38-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-01:02:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:39-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-01:02:39-root-INFO: grad norm: 2771.430 2370.727 1435.437
2024-12-02-01:02:39-root-INFO: Loss too large (11165.034->12715.664)! Learning rate decreased to 0.00031.
2024-12-02-01:02:40-root-INFO: Loss too large (11165.034->11668.732)! Learning rate decreased to 0.00025.
2024-12-02-01:02:41-root-INFO: grad norm: 3005.974 2605.106 1499.767
2024-12-02-01:02:42-root-INFO: Loss Change: 11165.034 -> 11093.366
2024-12-02-01:02:42-root-INFO: Regularization Change: 0.000 -> 0.137
2024-12-02-01:02:42-root-INFO: Undo step: 235
2024-12-02-01:02:42-root-INFO: Undo step: 236
2024-12-02-01:02:42-root-INFO: Undo step: 237
2024-12-02-01:02:42-root-INFO: Undo step: 238
2024-12-02-01:02:42-root-INFO: Undo step: 239
2024-12-02-01:02:42-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-01:02:42-root-INFO: grad norm: 10821.190 7925.985 7367.288
2024-12-02-01:02:43-root-INFO: Loss too large (20548.344->29128.590)! Learning rate decreased to 0.00024.
2024-12-02-01:02:43-root-INFO: Loss too large (20548.344->22158.109)! Learning rate decreased to 0.00019.
2024-12-02-01:02:44-root-INFO: grad norm: 8925.063 6857.758 5712.085
2024-12-02-01:02:45-root-INFO: Loss Change: 20548.344 -> 15921.341
2024-12-02-01:02:45-root-INFO: Regularization Change: 0.000 -> 1.427
2024-12-02-01:02:45-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-01:02:45-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:45-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-01:02:45-root-INFO: grad norm: 8113.157 6324.112 5082.217
2024-12-02-01:02:46-root-INFO: Loss too large (15640.834->25454.457)! Learning rate decreased to 0.00026.
2024-12-02-01:02:46-root-INFO: Loss too large (15640.834->19747.258)! Learning rate decreased to 0.00020.
2024-12-02-01:02:46-root-INFO: Loss too large (15640.834->16050.167)! Learning rate decreased to 0.00016.
2024-12-02-01:02:47-root-INFO: grad norm: 5776.259 4741.511 3298.976
2024-12-02-01:02:48-root-INFO: Loss Change: 15640.834 -> 12854.778
2024-12-02-01:02:48-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-02-01:02:48-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-01:02:48-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:48-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-01:02:49-root-INFO: grad norm: 4265.145 3468.255 2482.471
2024-12-02-01:02:49-root-INFO: Loss too large (12621.804->16014.122)! Learning rate decreased to 0.00027.
2024-12-02-01:02:49-root-INFO: Loss too large (12621.804->13945.537)! Learning rate decreased to 0.00021.
2024-12-02-01:02:50-root-INFO: Loss too large (12621.804->12729.240)! Learning rate decreased to 0.00017.
2024-12-02-01:02:51-root-INFO: grad norm: 3221.743 2724.863 1718.939
2024-12-02-01:02:51-root-INFO: Loss Change: 12621.804 -> 11725.333
2024-12-02-01:02:51-root-INFO: Regularization Change: 0.000 -> 0.157
2024-12-02-01:02:51-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-01:02:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:52-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-01:02:52-root-INFO: grad norm: 2226.345 1890.008 1176.640
2024-12-02-01:02:52-root-INFO: Loss too large (11587.496->12406.026)! Learning rate decreased to 0.00028.
2024-12-02-01:02:53-root-INFO: Loss too large (11587.496->11840.555)! Learning rate decreased to 0.00023.
2024-12-02-01:02:54-root-INFO: grad norm: 2543.332 2150.357 1358.124
2024-12-02-01:02:54-root-INFO: Loss Change: 11587.496 -> 11510.727
2024-12-02-01:02:54-root-INFO: Regularization Change: 0.000 -> 0.136
2024-12-02-01:02:54-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-01:02:54-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:55-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-01:02:55-root-INFO: grad norm: 2945.075 2523.167 1518.912
2024-12-02-01:02:55-root-INFO: Loss too large (11425.794->13185.979)! Learning rate decreased to 0.00030.
2024-12-02-01:02:56-root-INFO: Loss too large (11425.794->12066.000)! Learning rate decreased to 0.00024.
2024-12-02-01:02:56-root-INFO: Loss too large (11425.794->11433.723)! Learning rate decreased to 0.00019.
2024-12-02-01:02:57-root-INFO: grad norm: 2167.408 1833.609 1155.654
2024-12-02-01:02:58-root-INFO: Loss Change: 11425.794 -> 10884.932
2024-12-02-01:02:58-root-INFO: Regularization Change: 0.000 -> 0.105
2024-12-02-01:02:58-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-01:02:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-01:02:58-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-01:02:58-root-INFO: grad norm: 1442.852 1262.896 697.793
2024-12-02-01:02:59-root-INFO: Loss too large (10741.284->10918.779)! Learning rate decreased to 0.00031.
2024-12-02-01:03:00-root-INFO: grad norm: 2115.188 1790.358 1126.340
2024-12-02-01:03:00-root-INFO: Loss too large (10726.470->10962.038)! Learning rate decreased to 0.00025.
2024-12-02-01:03:01-root-INFO: Loss Change: 10741.284 -> 10651.694
2024-12-02-01:03:01-root-INFO: Regularization Change: 0.000 -> 0.175
2024-12-02-01:03:01-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-01:03:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-01:03:01-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-01:03:01-root-INFO: grad norm: 2252.018 1963.930 1102.074
2024-12-02-01:03:02-root-INFO: Loss too large (10612.424->11559.991)! Learning rate decreased to 0.00033.
2024-12-02-01:03:02-root-INFO: Loss too large (10612.424->10896.496)! Learning rate decreased to 0.00026.
2024-12-02-01:03:03-root-INFO: grad norm: 2361.503 2015.167 1231.177
2024-12-02-01:03:04-root-INFO: Loss Change: 10612.424 -> 10449.007
2024-12-02-01:03:04-root-INFO: Regularization Change: 0.000 -> 0.127
2024-12-02-01:03:04-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-01:03:04-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:04-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-01:03:04-root-INFO: grad norm: 2500.281 2168.011 1245.446
2024-12-02-01:03:05-root-INFO: Loss too large (10423.935->11621.903)! Learning rate decreased to 0.00035.
2024-12-02-01:03:05-root-INFO: Loss too large (10423.935->10775.387)! Learning rate decreased to 0.00028.
2024-12-02-01:03:06-root-INFO: grad norm: 2510.145 2163.542 1272.757
2024-12-02-01:03:07-root-INFO: Loss Change: 10423.935 -> 10197.994
2024-12-02-01:03:07-root-INFO: Regularization Change: 0.000 -> 0.140
2024-12-02-01:03:07-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-01:03:07-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:07-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-01:03:07-root-INFO: grad norm: 2278.876 2008.532 1076.604
2024-12-02-01:03:08-root-INFO: Loss too large (9929.135->10904.266)! Learning rate decreased to 0.00036.
2024-12-02-01:03:08-root-INFO: Loss too large (9929.135->10187.604)! Learning rate decreased to 0.00029.
2024-12-02-01:03:09-root-INFO: grad norm: 2223.432 1933.385 1098.032
2024-12-02-01:03:10-root-INFO: Loss Change: 9929.135 -> 9678.961
2024-12-02-01:03:10-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-02-01:03:10-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-01:03:10-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:10-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-01:03:10-root-INFO: grad norm: 2110.681 1880.194 959.084
2024-12-02-01:03:11-root-INFO: Loss too large (9605.299->10477.256)! Learning rate decreased to 0.00038.
2024-12-02-01:03:11-root-INFO: Loss too large (9605.299->9833.699)! Learning rate decreased to 0.00030.
2024-12-02-01:03:12-root-INFO: grad norm: 2033.415 1775.299 991.509
2024-12-02-01:03:13-root-INFO: Loss Change: 9605.299 -> 9357.925
2024-12-02-01:03:13-root-INFO: Regularization Change: 0.000 -> 0.126
2024-12-02-01:03:13-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-01:03:13-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:13-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-01:03:13-root-INFO: grad norm: 1788.792 1601.934 795.980
2024-12-02-01:03:14-root-INFO: Loss too large (9210.879->9783.916)! Learning rate decreased to 0.00040.
2024-12-02-01:03:14-root-INFO: Loss too large (9210.879->9326.038)! Learning rate decreased to 0.00032.
2024-12-02-01:03:15-root-INFO: grad norm: 1674.292 1470.886 799.841
2024-12-02-01:03:16-root-INFO: Loss Change: 9210.879 -> 8960.445
2024-12-02-01:03:16-root-INFO: Regularization Change: 0.000 -> 0.129
2024-12-02-01:03:16-root-INFO: Undo step: 230
2024-12-02-01:03:16-root-INFO: Undo step: 231
2024-12-02-01:03:16-root-INFO: Undo step: 232
2024-12-02-01:03:16-root-INFO: Undo step: 233
2024-12-02-01:03:16-root-INFO: Undo step: 234
2024-12-02-01:03:16-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-01:03:16-root-INFO: grad norm: 9229.427 7445.098 5454.616
2024-12-02-01:03:17-root-INFO: Loss too large (18933.367->25324.871)! Learning rate decreased to 0.00031.
2024-12-02-01:03:18-root-INFO: grad norm: 10425.955 8609.057 5880.874
2024-12-02-01:03:18-root-INFO: Loss too large (17337.773->22564.535)! Learning rate decreased to 0.00025.
2024-12-02-01:03:19-root-INFO: Loss Change: 18933.367 -> 16371.727
2024-12-02-01:03:19-root-INFO: Regularization Change: 0.000 -> 2.089
2024-12-02-01:03:19-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-01:03:19-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-01:03:19-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-01:03:19-root-INFO: grad norm: 8850.505 7819.606 4145.503
2024-12-02-01:03:20-root-INFO: Loss too large (16023.390->29489.961)! Learning rate decreased to 0.00033.
2024-12-02-01:03:20-root-INFO: Loss too large (16023.390->19199.812)! Learning rate decreased to 0.00026.
2024-12-02-01:03:21-root-INFO: grad norm: 7964.772 6837.572 4084.754
2024-12-02-01:03:22-root-INFO: Loss Change: 16023.390 -> 13472.329
2024-12-02-01:03:22-root-INFO: Regularization Change: 0.000 -> 0.551
2024-12-02-01:03:22-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-01:03:22-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:22-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-01:03:22-root-INFO: grad norm: 7058.285 6257.278 3265.862
2024-12-02-01:03:23-root-INFO: Loss too large (13280.496->22355.533)! Learning rate decreased to 0.00035.
2024-12-02-01:03:23-root-INFO: Loss too large (13280.496->15465.599)! Learning rate decreased to 0.00028.
2024-12-02-01:03:24-root-INFO: grad norm: 6200.374 5379.249 3083.557
2024-12-02-01:03:25-root-INFO: Loss Change: 13280.496 -> 11401.988
2024-12-02-01:03:25-root-INFO: Regularization Change: 0.000 -> 0.301
2024-12-02-01:03:25-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-01:03:25-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:25-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-01:03:25-root-INFO: grad norm: 5591.967 4949.100 2603.172
2024-12-02-01:03:26-root-INFO: Loss too large (11304.282->17286.820)! Learning rate decreased to 0.00036.
2024-12-02-01:03:26-root-INFO: Loss too large (11304.282->12746.154)! Learning rate decreased to 0.00029.
2024-12-02-01:03:27-root-INFO: grad norm: 4835.627 4266.397 2276.213
2024-12-02-01:03:28-root-INFO: Loss Change: 11304.282 -> 9990.068
2024-12-02-01:03:28-root-INFO: Regularization Change: 0.000 -> 0.190
2024-12-02-01:03:28-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-01:03:28-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:28-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-01:03:28-root-INFO: grad norm: 4239.524 3747.498 1982.378
2024-12-02-01:03:29-root-INFO: Loss too large (9858.173->13556.623)! Learning rate decreased to 0.00038.
2024-12-02-01:03:29-root-INFO: Loss too large (9858.173->10813.982)! Learning rate decreased to 0.00030.
2024-12-02-01:03:30-root-INFO: grad norm: 3712.280 3298.873 1702.486
2024-12-02-01:03:31-root-INFO: Loss Change: 9858.173 -> 9075.240
2024-12-02-01:03:31-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-02-01:03:31-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-01:03:31-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:31-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-01:03:31-root-INFO: grad norm: 3320.722 2943.242 1537.701
2024-12-02-01:03:32-root-INFO: Loss too large (9023.230->11285.602)! Learning rate decreased to 0.00040.
2024-12-02-01:03:32-root-INFO: Loss too large (9023.230->9563.093)! Learning rate decreased to 0.00032.
2024-12-02-01:03:33-root-INFO: grad norm: 2838.273 2545.978 1254.508
2024-12-02-01:03:34-root-INFO: Loss Change: 9023.230 -> 8423.245
2024-12-02-01:03:34-root-INFO: Regularization Change: 0.000 -> 0.125
2024-12-02-01:03:34-root-INFO: Undo step: 230
2024-12-02-01:03:34-root-INFO: Undo step: 231
2024-12-02-01:03:34-root-INFO: Undo step: 232
2024-12-02-01:03:34-root-INFO: Undo step: 233
2024-12-02-01:03:34-root-INFO: Undo step: 234
2024-12-02-01:03:34-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-01:03:34-root-INFO: grad norm: 3478.130 2932.294 1870.573
2024-12-02-01:03:35-root-INFO: Loss too large (9989.309->11165.020)! Learning rate decreased to 0.00031.
2024-12-02-01:03:36-root-INFO: grad norm: 4213.287 3486.653 2365.382
2024-12-02-01:03:36-root-INFO: Loss too large (9938.776->10993.307)! Learning rate decreased to 0.00025.
2024-12-02-01:03:37-root-INFO: Loss Change: 9989.309 -> 9774.977
2024-12-02-01:03:37-root-INFO: Regularization Change: 0.000 -> 0.338
2024-12-02-01:03:37-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-01:03:37-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-01:03:37-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-01:03:37-root-INFO: grad norm: 3863.153 3322.205 1971.524
2024-12-02-01:03:38-root-INFO: Loss too large (9683.214->12635.228)! Learning rate decreased to 0.00033.
2024-12-02-01:03:38-root-INFO: Loss too large (9683.214->10577.673)! Learning rate decreased to 0.00026.
2024-12-02-01:03:39-root-INFO: grad norm: 3682.648 3153.070 1902.642
2024-12-02-01:03:40-root-INFO: Loss Change: 9683.214 -> 9351.368
2024-12-02-01:03:40-root-INFO: Regularization Change: 0.000 -> 0.103
2024-12-02-01:03:40-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-01:03:40-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:40-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-01:03:40-root-INFO: grad norm: 3428.608 3007.041 1647.137
2024-12-02-01:03:41-root-INFO: Loss too large (9283.238->11595.428)! Learning rate decreased to 0.00035.
2024-12-02-01:03:41-root-INFO: Loss too large (9283.238->9929.649)! Learning rate decreased to 0.00028.
2024-12-02-01:03:42-root-INFO: grad norm: 3156.221 2740.828 1565.118
2024-12-02-01:03:43-root-INFO: Loss Change: 9283.238 -> 8892.679
2024-12-02-01:03:43-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-01:03:43-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-01:03:43-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:43-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-01:03:43-root-INFO: grad norm: 3162.865 2771.255 1524.422
2024-12-02-01:03:44-root-INFO: Loss too large (8866.424->10811.589)! Learning rate decreased to 0.00036.
2024-12-02-01:03:44-root-INFO: Loss too large (8866.424->9357.707)! Learning rate decreased to 0.00029.
2024-12-02-01:03:45-root-INFO: grad norm: 2796.108 2468.338 1313.594
2024-12-02-01:03:46-root-INFO: Loss Change: 8866.424 -> 8430.775
2024-12-02-01:03:46-root-INFO: Regularization Change: 0.000 -> 0.094
2024-12-02-01:03:46-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-01:03:46-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:46-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-01:03:46-root-INFO: grad norm: 2526.367 2219.747 1206.339
2024-12-02-01:03:47-root-INFO: Loss too large (8354.022->9618.958)! Learning rate decreased to 0.00038.
2024-12-02-01:03:47-root-INFO: Loss too large (8354.022->8665.896)! Learning rate decreased to 0.00030.
2024-12-02-01:03:48-root-INFO: grad norm: 2219.645 1972.552 1017.774
2024-12-02-01:03:49-root-INFO: Loss Change: 8354.022 -> 8023.533
2024-12-02-01:03:49-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-02-01:03:49-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-01:03:49-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:49-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-01:03:49-root-INFO: grad norm: 2101.675 1850.390 996.542
2024-12-02-01:03:50-root-INFO: Loss too large (7984.242->8820.230)! Learning rate decreased to 0.00040.
2024-12-02-01:03:50-root-INFO: Loss too large (7984.242->8156.604)! Learning rate decreased to 0.00032.
2024-12-02-01:03:51-root-INFO: grad norm: 1788.625 1603.914 791.604
2024-12-02-01:03:52-root-INFO: Loss Change: 7984.242 -> 7685.254
2024-12-02-01:03:52-root-INFO: Regularization Change: 0.000 -> 0.088
2024-12-02-01:03:52-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-01:03:52-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:52-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-01:03:52-root-INFO: grad norm: 1706.528 1513.145 789.068
2024-12-02-01:03:53-root-INFO: Loss too large (7668.939->8179.094)! Learning rate decreased to 0.00042.
2024-12-02-01:03:53-root-INFO: Loss too large (7668.939->7741.985)! Learning rate decreased to 0.00034.
2024-12-02-01:03:54-root-INFO: grad norm: 1420.576 1288.537 598.089
2024-12-02-01:03:55-root-INFO: Loss Change: 7668.939 -> 7412.812
2024-12-02-01:03:55-root-INFO: Regularization Change: 0.000 -> 0.092
2024-12-02-01:03:55-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-01:03:55-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:55-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-01:03:55-root-INFO: grad norm: 1316.863 1171.049 602.305
2024-12-02-01:03:56-root-INFO: Loss too large (7384.885->7650.746)! Learning rate decreased to 0.00044.
2024-12-02-01:03:56-root-INFO: Loss too large (7384.885->7398.667)! Learning rate decreased to 0.00035.
2024-12-02-01:03:57-root-INFO: grad norm: 1080.151 982.534 448.725
2024-12-02-01:03:58-root-INFO: Loss Change: 7384.885 -> 7184.747
2024-12-02-01:03:58-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-02-01:03:58-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-01:03:58-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:03:58-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-01:03:58-root-INFO: grad norm: 1194.610 1067.189 536.845
2024-12-02-01:03:59-root-INFO: Loss too large (7220.402->7401.205)! Learning rate decreased to 0.00046.
2024-12-02-01:04:00-root-INFO: grad norm: 1432.730 1304.585 592.262
2024-12-02-01:04:00-root-INFO: Loss too large (7199.990->7244.631)! Learning rate decreased to 0.00037.
2024-12-02-01:04:01-root-INFO: Loss Change: 7220.402 -> 7079.413
2024-12-02-01:04:01-root-INFO: Regularization Change: 0.000 -> 0.117
2024-12-02-01:04:01-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-01:04:01-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-01:04:01-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-01:04:01-root-INFO: grad norm: 1147.931 1025.234 516.372
2024-12-02-01:04:02-root-INFO: Loss too large (6998.240->7181.429)! Learning rate decreased to 0.00049.
2024-12-02-01:04:03-root-INFO: grad norm: 1358.180 1232.084 571.509
2024-12-02-01:04:03-root-INFO: Loss too large (6985.717->7012.989)! Learning rate decreased to 0.00039.
2024-12-02-01:04:04-root-INFO: Loss Change: 6998.240 -> 6861.625
2024-12-02-01:04:04-root-INFO: Regularization Change: 0.000 -> 0.107
2024-12-02-01:04:04-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-01:04:04-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:04-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-01:04:04-root-INFO: grad norm: 1343.902 1206.649 591.668
2024-12-02-01:04:05-root-INFO: Loss too large (6851.544->7135.130)! Learning rate decreased to 0.00051.
2024-12-02-01:04:06-root-INFO: grad norm: 1568.048 1428.730 646.146
2024-12-02-01:04:06-root-INFO: Loss too large (6841.483->6904.702)! Learning rate decreased to 0.00041.
2024-12-02-01:04:07-root-INFO: Loss Change: 6851.544 -> 6686.080
2024-12-02-01:04:07-root-INFO: Regularization Change: 0.000 -> 0.121
2024-12-02-01:04:07-root-INFO: Undo step: 225
2024-12-02-01:04:07-root-INFO: Undo step: 226
2024-12-02-01:04:07-root-INFO: Undo step: 227
2024-12-02-01:04:07-root-INFO: Undo step: 228
2024-12-02-01:04:07-root-INFO: Undo step: 229
2024-12-02-01:04:07-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-01:04:07-root-INFO: grad norm: 5949.283 4797.417 3518.346
2024-12-02-01:04:08-root-INFO: Loss too large (12196.557->12414.977)! Learning rate decreased to 0.00040.
2024-12-02-01:04:09-root-INFO: grad norm: 3966.680 3462.933 1934.592
2024-12-02-01:04:09-root-INFO: Loss Change: 12196.557 -> 8937.002
2024-12-02-01:04:09-root-INFO: Regularization Change: 0.000 -> 1.821
2024-12-02-01:04:09-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-01:04:09-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:04:10-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-01:04:10-root-INFO: grad norm: 3209.802 2893.295 1389.847
2024-12-02-01:04:10-root-INFO: Loss too large (8795.735->10380.932)! Learning rate decreased to 0.00042.
2024-12-02-01:04:11-root-INFO: Loss too large (8795.735->9024.467)! Learning rate decreased to 0.00034.
2024-12-02-01:04:12-root-INFO: grad norm: 2424.962 2167.406 1087.564
2024-12-02-01:04:12-root-INFO: Loss Change: 8795.735 -> 7909.807
2024-12-02-01:04:12-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-01:04:13-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-01:04:13-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:04:13-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-01:04:13-root-INFO: grad norm: 1715.412 1559.509 714.542
2024-12-02-01:04:14-root-INFO: Loss too large (7792.664->8211.885)! Learning rate decreased to 0.00044.
2024-12-02-01:04:14-root-INFO: Loss too large (7792.664->7811.380)! Learning rate decreased to 0.00035.
2024-12-02-01:04:15-root-INFO: grad norm: 1374.070 1226.474 619.541
2024-12-02-01:04:16-root-INFO: Loss Change: 7792.664 -> 7445.522
2024-12-02-01:04:16-root-INFO: Regularization Change: 0.000 -> 0.151
2024-12-02-01:04:16-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-01:04:16-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:04:16-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-01:04:16-root-INFO: grad norm: 959.503 872.703 398.792
2024-12-02-01:04:17-root-INFO: grad norm: 1413.918 1257.545 646.333
2024-12-02-01:04:18-root-INFO: Loss too large (7366.541->7632.919)! Learning rate decreased to 0.00046.
2024-12-02-01:04:18-root-INFO: Loss Change: 7393.979 -> 7346.944
2024-12-02-01:04:18-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-02-01:04:18-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-01:04:18-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-01:04:19-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-01:04:19-root-INFO: grad norm: 1584.735 1451.106 636.927
2024-12-02-01:04:19-root-INFO: Loss too large (7230.113->7655.637)! Learning rate decreased to 0.00049.
2024-12-02-01:04:20-root-INFO: Loss too large (7230.113->7269.721)! Learning rate decreased to 0.00039.
2024-12-02-01:04:21-root-INFO: grad norm: 1238.229 1116.314 535.773
2024-12-02-01:04:21-root-INFO: Loss Change: 7230.113 -> 6931.685
2024-12-02-01:04:21-root-INFO: Regularization Change: 0.000 -> 0.124
2024-12-02-01:04:21-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-01:04:21-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:22-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-01:04:22-root-INFO: grad norm: 785.169 723.912 304.042
2024-12-02-01:04:23-root-INFO: grad norm: 1121.528 1000.153 507.462
2024-12-02-01:04:23-root-INFO: Loss too large (6779.275->6939.235)! Learning rate decreased to 0.00051.
2024-12-02-01:04:24-root-INFO: Loss Change: 6806.064 -> 6751.266
2024-12-02-01:04:24-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-02-01:04:24-root-INFO: Undo step: 225
2024-12-02-01:04:24-root-INFO: Undo step: 226
2024-12-02-01:04:24-root-INFO: Undo step: 227
2024-12-02-01:04:24-root-INFO: Undo step: 228
2024-12-02-01:04:24-root-INFO: Undo step: 229
2024-12-02-01:04:24-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-01:04:25-root-INFO: grad norm: 4824.905 4109.146 2528.760
2024-12-02-01:04:25-root-INFO: Loss too large (10558.717->12363.089)! Learning rate decreased to 0.00040.
2024-12-02-01:04:26-root-INFO: grad norm: 4619.079 4077.641 2169.961
2024-12-02-01:04:27-root-INFO: Loss Change: 10558.717 -> 9945.803
2024-12-02-01:04:27-root-INFO: Regularization Change: 0.000 -> 1.235
2024-12-02-01:04:27-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-01:04:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:04:27-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-01:04:27-root-INFO: grad norm: 4670.785 4041.688 2341.151
2024-12-02-01:04:28-root-INFO: Loss too large (9564.562->12928.370)! Learning rate decreased to 0.00042.
2024-12-02-01:04:28-root-INFO: Loss too large (9564.562->10183.371)! Learning rate decreased to 0.00034.
2024-12-02-01:04:29-root-INFO: grad norm: 3527.917 3139.729 1608.821
2024-12-02-01:04:30-root-INFO: Loss Change: 9564.562 -> 7850.710
2024-12-02-01:04:30-root-INFO: Regularization Change: 0.000 -> 0.407
2024-12-02-01:04:30-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-01:04:30-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:04:30-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-01:04:30-root-INFO: grad norm: 2471.311 2150.885 1216.993
2024-12-02-01:04:31-root-INFO: Loss too large (7707.951->8696.355)! Learning rate decreased to 0.00044.
2024-12-02-01:04:31-root-INFO: Loss too large (7707.951->7827.743)! Learning rate decreased to 0.00035.
2024-12-02-01:04:32-root-INFO: grad norm: 1916.677 1714.105 857.611
2024-12-02-01:04:33-root-INFO: Loss Change: 7707.951 -> 7112.482
2024-12-02-01:04:33-root-INFO: Regularization Change: 0.000 -> 0.204
2024-12-02-01:04:33-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-01:04:33-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-01:04:33-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-01:04:33-root-INFO: grad norm: 1205.228 1042.551 604.701
2024-12-02-01:04:34-root-INFO: Loss too large (6929.901->7036.634)! Learning rate decreased to 0.00046.
2024-12-02-01:04:35-root-INFO: grad norm: 1323.240 1184.456 589.940
2024-12-02-01:04:35-root-INFO: Loss Change: 6929.901 -> 6845.516
2024-12-02-01:04:35-root-INFO: Regularization Change: 0.000 -> 0.190
2024-12-02-01:04:35-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-01:04:35-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-01:04:36-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-01:04:36-root-INFO: grad norm: 1462.621 1303.278 663.873
2024-12-02-01:04:36-root-INFO: Loss too large (6784.368->7112.637)! Learning rate decreased to 0.00049.
2024-12-02-01:04:37-root-INFO: Loss too large (6784.368->6793.539)! Learning rate decreased to 0.00039.
2024-12-02-01:04:38-root-INFO: grad norm: 1099.020 992.397 472.222
2024-12-02-01:04:38-root-INFO: Loss Change: 6784.368 -> 6514.362
2024-12-02-01:04:38-root-INFO: Regularization Change: 0.000 -> 0.113
2024-12-02-01:04:38-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-01:04:38-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:39-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-01:04:39-root-INFO: grad norm: 600.497 527.889 286.235
2024-12-02-01:04:40-root-INFO: grad norm: 779.141 693.725 354.691
2024-12-02-01:04:40-root-INFO: Loss too large (6332.755->6380.934)! Learning rate decreased to 0.00051.
2024-12-02-01:04:41-root-INFO: Loss Change: 6371.836 -> 6301.066
2024-12-02-01:04:41-root-INFO: Regularization Change: 0.000 -> 0.150
2024-12-02-01:04:41-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-01:04:41-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:41-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-01:04:42-root-INFO: grad norm: 672.807 607.567 289.018
2024-12-02-01:04:42-root-INFO: Loss too large (6230.627->6243.563)! Learning rate decreased to 0.00054.
2024-12-02-01:04:43-root-INFO: grad norm: 720.629 650.851 309.351
2024-12-02-01:04:44-root-INFO: Loss Change: 6230.627 -> 6164.514
2024-12-02-01:04:44-root-INFO: Regularization Change: 0.000 -> 0.106
2024-12-02-01:04:44-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-01:04:44-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:44-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-01:04:44-root-INFO: grad norm: 627.240 569.167 263.587
2024-12-02-01:04:45-root-INFO: grad norm: 917.131 829.810 390.571
2024-12-02-01:04:46-root-INFO: Loss too large (6096.893->6224.149)! Learning rate decreased to 0.00056.
2024-12-02-01:04:46-root-INFO: Loss Change: 6099.748 -> 6081.531
2024-12-02-01:04:46-root-INFO: Regularization Change: 0.000 -> 0.157
2024-12-02-01:04:46-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-01:04:46-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:47-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-01:04:47-root-INFO: grad norm: 951.541 870.270 384.787
2024-12-02-01:04:47-root-INFO: Loss too large (5994.685->6148.625)! Learning rate decreased to 0.00059.
2024-12-02-01:04:48-root-INFO: grad norm: 1057.842 964.860 433.676
2024-12-02-01:04:49-root-INFO: Loss Change: 5994.685 -> 5984.521
2024-12-02-01:04:49-root-INFO: Regularization Change: 0.000 -> 0.100
2024-12-02-01:04:49-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-01:04:49-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:49-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-01:04:50-root-INFO: grad norm: 1040.686 950.120 424.617
2024-12-02-01:04:50-root-INFO: Loss too large (5887.808->6079.019)! Learning rate decreased to 0.00062.
2024-12-02-01:04:51-root-INFO: grad norm: 1117.910 1018.775 460.240
2024-12-02-01:04:52-root-INFO: Loss Change: 5887.808 -> 5867.405
2024-12-02-01:04:52-root-INFO: Regularization Change: 0.000 -> 0.101
2024-12-02-01:04:52-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-01:04:52-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:52-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-01:04:52-root-INFO: grad norm: 960.949 885.047 374.320
2024-12-02-01:04:53-root-INFO: Loss too large (5775.095->5900.689)! Learning rate decreased to 0.00065.
2024-12-02-01:04:54-root-INFO: grad norm: 1004.955 916.353 412.591
2024-12-02-01:04:54-root-INFO: Loss Change: 5775.095 -> 5714.289
2024-12-02-01:04:54-root-INFO: Regularization Change: 0.000 -> 0.145
2024-12-02-01:04:54-root-INFO: Undo step: 220
2024-12-02-01:04:54-root-INFO: Undo step: 221
2024-12-02-01:04:54-root-INFO: Undo step: 222
2024-12-02-01:04:54-root-INFO: Undo step: 223
2024-12-02-01:04:54-root-INFO: Undo step: 224
2024-12-02-01:04:55-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-01:04:55-root-INFO: grad norm: 3993.112 3037.569 2591.932
2024-12-02-01:04:56-root-INFO: grad norm: 2823.911 2297.635 1641.752
2024-12-02-01:04:57-root-INFO: Loss Change: 9959.725 -> 7077.042
2024-12-02-01:04:57-root-INFO: Regularization Change: 0.000 -> 2.547
2024-12-02-01:04:57-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-01:04:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:04:57-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-01:04:57-root-INFO: grad norm: 2270.388 1911.469 1225.133
2024-12-02-01:04:58-root-INFO: Loss too large (6885.146->7142.188)! Learning rate decreased to 0.00054.
2024-12-02-01:04:59-root-INFO: grad norm: 1866.390 1661.368 850.451
2024-12-02-01:04:59-root-INFO: Loss Change: 6885.146 -> 6397.307
2024-12-02-01:04:59-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-01:04:59-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-01:04:59-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:00-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-01:05:00-root-INFO: grad norm: 1662.701 1523.367 666.281
2024-12-02-01:05:00-root-INFO: Loss too large (6238.442->6846.337)! Learning rate decreased to 0.00056.
2024-12-02-01:05:01-root-INFO: Loss too large (6238.442->6344.969)! Learning rate decreased to 0.00045.
2024-12-02-01:05:02-root-INFO: grad norm: 1298.391 1165.606 571.998
2024-12-02-01:05:02-root-INFO: Loss Change: 6238.442 -> 5922.158
2024-12-02-01:05:02-root-INFO: Regularization Change: 0.000 -> 0.125
2024-12-02-01:05:02-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-01:05:02-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:03-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-01:05:03-root-INFO: grad norm: 923.514 851.722 356.999
2024-12-02-01:05:03-root-INFO: Loss too large (5844.264->5980.401)! Learning rate decreased to 0.00059.
2024-12-02-01:05:04-root-INFO: grad norm: 1077.791 969.695 470.452
2024-12-02-01:05:05-root-INFO: Loss too large (5833.503->5840.015)! Learning rate decreased to 0.00047.
2024-12-02-01:05:05-root-INFO: Loss Change: 5844.264 -> 5724.730
2024-12-02-01:05:05-root-INFO: Regularization Change: 0.000 -> 0.112
2024-12-02-01:05:05-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-01:05:05-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:06-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-01:05:06-root-INFO: grad norm: 712.419 658.072 272.913
2024-12-02-01:05:06-root-INFO: Loss too large (5640.209->5694.074)! Learning rate decreased to 0.00062.
2024-12-02-01:05:07-root-INFO: grad norm: 811.387 726.538 361.235
2024-12-02-01:05:08-root-INFO: Loss Change: 5640.209 -> 5599.231
2024-12-02-01:05:08-root-INFO: Regularization Change: 0.000 -> 0.131
2024-12-02-01:05:08-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-01:05:08-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:08-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-01:05:09-root-INFO: grad norm: 724.635 675.757 261.626
2024-12-02-01:05:09-root-INFO: Loss too large (5515.036->5557.233)! Learning rate decreased to 0.00065.
2024-12-02-01:05:10-root-INFO: grad norm: 806.801 723.097 357.852
2024-12-02-01:05:11-root-INFO: Loss Change: 5515.036 -> 5464.529
2024-12-02-01:05:11-root-INFO: Regularization Change: 0.000 -> 0.163
2024-12-02-01:05:11-root-INFO: Undo step: 220
2024-12-02-01:05:11-root-INFO: Undo step: 221
2024-12-02-01:05:11-root-INFO: Undo step: 222
2024-12-02-01:05:11-root-INFO: Undo step: 223
2024-12-02-01:05:11-root-INFO: Undo step: 224
2024-12-02-01:05:11-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-01:05:11-root-INFO: grad norm: 3941.596 3532.643 1748.317
2024-12-02-01:05:12-root-INFO: Loss too large (9004.922->12079.324)! Learning rate decreased to 0.00051.
2024-12-02-01:05:12-root-INFO: Loss too large (9004.922->9688.427)! Learning rate decreased to 0.00041.
2024-12-02-01:05:13-root-INFO: grad norm: 3208.144 2923.467 1321.185
2024-12-02-01:05:14-root-INFO: Loss Change: 9004.922 -> 7542.646
2024-12-02-01:05:14-root-INFO: Regularization Change: 0.000 -> 0.588
2024-12-02-01:05:14-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-01:05:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:14-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-01:05:14-root-INFO: grad norm: 2355.646 2108.526 1050.326
2024-12-02-01:05:15-root-INFO: Loss too large (7303.354->8434.737)! Learning rate decreased to 0.00054.
2024-12-02-01:05:15-root-INFO: Loss too large (7303.354->7464.138)! Learning rate decreased to 0.00043.
2024-12-02-01:05:16-root-INFO: grad norm: 1934.628 1767.789 785.944
2024-12-02-01:05:17-root-INFO: Loss Change: 7303.354 -> 6654.615
2024-12-02-01:05:17-root-INFO: Regularization Change: 0.000 -> 0.325
2024-12-02-01:05:17-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-01:05:17-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:17-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-01:05:17-root-INFO: grad norm: 1358.526 1216.188 605.378
2024-12-02-01:05:18-root-INFO: Loss too large (6471.540->6787.788)! Learning rate decreased to 0.00056.
2024-12-02-01:05:18-root-INFO: Loss too large (6471.540->6472.380)! Learning rate decreased to 0.00045.
2024-12-02-01:05:19-root-INFO: grad norm: 1141.328 1050.051 447.241
2024-12-02-01:05:20-root-INFO: Loss Change: 6471.540 -> 6182.326
2024-12-02-01:05:20-root-INFO: Regularization Change: 0.000 -> 0.193
2024-12-02-01:05:20-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-01:05:20-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:20-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-01:05:20-root-INFO: grad norm: 917.610 817.266 417.234
2024-12-02-01:05:21-root-INFO: Loss too large (6096.322->6204.989)! Learning rate decreased to 0.00059.
2024-12-02-01:05:22-root-INFO: grad norm: 1134.548 1045.213 441.282
2024-12-02-01:05:22-root-INFO: Loss too large (6069.396->6089.845)! Learning rate decreased to 0.00047.
2024-12-02-01:05:23-root-INFO: Loss Change: 6096.322 -> 5956.028
2024-12-02-01:05:23-root-INFO: Regularization Change: 0.000 -> 0.173
2024-12-02-01:05:23-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-01:05:23-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:23-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-01:05:23-root-INFO: grad norm: 800.792 713.174 364.212
2024-12-02-01:05:24-root-INFO: Loss too large (5847.231->5926.556)! Learning rate decreased to 0.00062.
2024-12-02-01:05:25-root-INFO: grad norm: 975.410 898.481 379.681
2024-12-02-01:05:25-root-INFO: Loss too large (5820.624->5828.331)! Learning rate decreased to 0.00050.
2024-12-02-01:05:26-root-INFO: Loss Change: 5847.231 -> 5727.953
2024-12-02-01:05:26-root-INFO: Regularization Change: 0.000 -> 0.147
2024-12-02-01:05:26-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-01:05:26-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:26-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-01:05:26-root-INFO: grad norm: 561.076 512.725 227.858
2024-12-02-01:05:27-root-INFO: grad norm: 829.270 762.257 326.578
2024-12-02-01:05:28-root-INFO: Loss too large (5595.766->5737.020)! Learning rate decreased to 0.00065.
2024-12-02-01:05:28-root-INFO: Loss Change: 5630.812 -> 5593.370
2024-12-02-01:05:28-root-INFO: Regularization Change: 0.000 -> 0.260
2024-12-02-01:05:28-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-01:05:28-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:29-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-01:05:29-root-INFO: grad norm: 984.328 887.779 425.148
2024-12-02-01:05:29-root-INFO: Loss too large (5536.707->5790.086)! Learning rate decreased to 0.00068.
2024-12-02-01:05:30-root-INFO: Loss too large (5536.707->5576.253)! Learning rate decreased to 0.00055.
2024-12-02-01:05:31-root-INFO: grad norm: 837.227 774.944 316.878
2024-12-02-01:05:31-root-INFO: Loss Change: 5536.707 -> 5382.143
2024-12-02-01:05:31-root-INFO: Regularization Change: 0.000 -> 0.106
2024-12-02-01:05:31-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-01:05:31-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-01:05:32-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-01:05:32-root-INFO: grad norm: 501.801 460.277 199.874
2024-12-02-01:05:33-root-INFO: grad norm: 769.268 711.146 293.333
2024-12-02-01:05:33-root-INFO: Loss too large (5292.809->5421.998)! Learning rate decreased to 0.00071.
2024-12-02-01:05:34-root-INFO: Loss Change: 5308.362 -> 5286.833
2024-12-02-01:05:34-root-INFO: Regularization Change: 0.000 -> 0.233
2024-12-02-01:05:34-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-01:05:34-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:05:34-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-01:05:34-root-INFO: grad norm: 765.277 703.517 301.186
2024-12-02-01:05:35-root-INFO: Loss too large (5226.202->5333.812)! Learning rate decreased to 0.00075.
2024-12-02-01:05:36-root-INFO: grad norm: 898.167 830.209 342.721
2024-12-02-01:05:36-root-INFO: Loss too large (5211.592->5217.559)! Learning rate decreased to 0.00060.
2024-12-02-01:05:37-root-INFO: Loss Change: 5226.202 -> 5114.229
2024-12-02-01:05:37-root-INFO: Regularization Change: 0.000 -> 0.148
2024-12-02-01:05:37-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-01:05:37-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:05:37-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-01:05:38-root-INFO: grad norm: 542.225 505.655 195.758
2024-12-02-01:05:39-root-INFO: grad norm: 797.666 739.216 299.719
2024-12-02-01:05:39-root-INFO: Loss too large (5035.209->5198.287)! Learning rate decreased to 0.00079.
2024-12-02-01:05:39-root-INFO: Loss Change: 5051.911 -> 5033.704
2024-12-02-01:05:40-root-INFO: Regularization Change: 0.000 -> 0.284
2024-12-02-01:05:40-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-01:05:40-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:05:40-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-01:05:40-root-INFO: grad norm: 907.650 839.032 346.199
2024-12-02-01:05:40-root-INFO: Loss too large (4993.856->5240.832)! Learning rate decreased to 0.00082.
2024-12-02-01:05:41-root-INFO: Loss too large (4993.856->5028.482)! Learning rate decreased to 0.00066.
2024-12-02-01:05:42-root-INFO: grad norm: 727.742 680.938 256.772
2024-12-02-01:05:42-root-INFO: Loss Change: 4993.856 -> 4833.100
2024-12-02-01:05:42-root-INFO: Regularization Change: 0.000 -> 0.117
2024-12-02-01:05:42-root-INFO: Undo step: 215
2024-12-02-01:05:42-root-INFO: Undo step: 216
2024-12-02-01:05:42-root-INFO: Undo step: 217
2024-12-02-01:05:42-root-INFO: Undo step: 218
2024-12-02-01:05:42-root-INFO: Undo step: 219
2024-12-02-01:05:43-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-01:05:43-root-INFO: grad norm: 2024.707 1544.497 1309.184
2024-12-02-01:05:44-root-INFO: grad norm: 1684.452 1277.416 1097.991
2024-12-02-01:05:45-root-INFO: Loss Change: 7253.327 -> 6062.007
2024-12-02-01:05:45-root-INFO: Regularization Change: 0.000 -> 1.835
2024-12-02-01:05:45-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-01:05:45-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:05:45-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-01:05:45-root-INFO: grad norm: 1521.623 1221.092 907.893
2024-12-02-01:05:46-root-INFO: grad norm: 1706.117 1433.090 925.791
2024-12-02-01:05:47-root-INFO: Loss too large (5818.732->6132.939)! Learning rate decreased to 0.00068.
2024-12-02-01:05:47-root-INFO: Loss Change: 6018.073 -> 5665.250
2024-12-02-01:05:47-root-INFO: Regularization Change: 0.000 -> 0.763
2024-12-02-01:05:48-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-01:05:48-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-01:05:48-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-01:05:48-root-INFO: grad norm: 1707.428 1527.372 763.180
2024-12-02-01:05:48-root-INFO: Loss too large (5703.518->6221.421)! Learning rate decreased to 0.00071.
2024-12-02-01:05:49-root-INFO: grad norm: 1813.685 1643.464 767.125
2024-12-02-01:05:50-root-INFO: Loss too large (5564.827->5724.355)! Learning rate decreased to 0.00057.
2024-12-02-01:05:50-root-INFO: Loss Change: 5703.518 -> 5324.499
2024-12-02-01:05:50-root-INFO: Regularization Change: 0.000 -> 0.246
2024-12-02-01:05:50-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-01:05:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:05:51-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-01:05:51-root-INFO: grad norm: 1455.326 1313.791 626.040
2024-12-02-01:05:51-root-INFO: Loss too large (5364.423->5820.071)! Learning rate decreased to 0.00075.
2024-12-02-01:05:52-root-INFO: grad norm: 1584.852 1441.085 659.567
2024-12-02-01:05:53-root-INFO: Loss too large (5304.014->5409.863)! Learning rate decreased to 0.00060.
2024-12-02-01:05:53-root-INFO: Loss Change: 5364.423 -> 5094.694
2024-12-02-01:05:53-root-INFO: Regularization Change: 0.000 -> 0.164
2024-12-02-01:05:53-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-01:05:53-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:05:54-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-01:05:54-root-INFO: grad norm: 1296.333 1172.262 553.429
2024-12-02-01:05:54-root-INFO: Loss too large (5146.980->5500.901)! Learning rate decreased to 0.00079.
2024-12-02-01:05:55-root-INFO: grad norm: 1392.882 1267.055 578.526
2024-12-02-01:05:56-root-INFO: Loss too large (5081.667->5151.124)! Learning rate decreased to 0.00063.
2024-12-02-01:05:56-root-INFO: Loss Change: 5146.980 -> 4900.394
2024-12-02-01:05:56-root-INFO: Regularization Change: 0.000 -> 0.166
2024-12-02-01:05:56-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-01:05:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:05:57-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-01:05:57-root-INFO: grad norm: 995.929 906.400 412.692
2024-12-02-01:05:57-root-INFO: Loss too large (4883.188->5071.216)! Learning rate decreased to 0.00082.
2024-12-02-01:05:58-root-INFO: grad norm: 1031.297 941.741 420.354
2024-12-02-01:05:58-root-INFO: Loss too large (4827.680->4828.888)! Learning rate decreased to 0.00066.
2024-12-02-01:05:59-root-INFO: Loss Change: 4883.188 -> 4695.472
2024-12-02-01:05:59-root-INFO: Regularization Change: 0.000 -> 0.137
2024-12-02-01:05:59-root-INFO: Undo step: 215
2024-12-02-01:05:59-root-INFO: Undo step: 216
2024-12-02-01:05:59-root-INFO: Undo step: 217
2024-12-02-01:05:59-root-INFO: Undo step: 218
2024-12-02-01:05:59-root-INFO: Undo step: 219
2024-12-02-01:06:00-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-01:06:00-root-INFO: grad norm: 3470.755 3041.248 1672.410
2024-12-02-01:06:01-root-INFO: grad norm: 3169.694 2813.288 1460.264
2024-12-02-01:06:01-root-INFO: Loss too large (8051.260->8225.929)! Learning rate decreased to 0.00065.
2024-12-02-01:06:02-root-INFO: Loss Change: 8555.055 -> 6501.863
2024-12-02-01:06:02-root-INFO: Regularization Change: 0.000 -> 4.370
2024-12-02-01:06:02-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-01:06:02-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-01:06:02-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-01:06:03-root-INFO: grad norm: 2644.926 2453.703 987.407
2024-12-02-01:06:03-root-INFO: Loss too large (6403.494->7936.756)! Learning rate decreased to 0.00068.
2024-12-02-01:06:03-root-INFO: Loss too large (6403.494->6649.962)! Learning rate decreased to 0.00055.
2024-12-02-01:06:04-root-INFO: grad norm: 1858.508 1671.254 812.996
2024-12-02-01:06:05-root-INFO: Loss Change: 6403.494 -> 5372.808
2024-12-02-01:06:05-root-INFO: Regularization Change: 0.000 -> 0.494
2024-12-02-01:06:05-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-01:06:05-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-01:06:05-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-01:06:05-root-INFO: grad norm: 962.680 893.097 359.346
2024-12-02-01:06:06-root-INFO: Loss too large (5262.671->5356.001)! Learning rate decreased to 0.00071.
2024-12-02-01:06:07-root-INFO: grad norm: 988.457 894.041 421.591
2024-12-02-01:06:07-root-INFO: Loss Change: 5262.671 -> 5123.867
2024-12-02-01:06:07-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-02-01:06:07-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-01:06:07-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:08-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-01:06:08-root-INFO: grad norm: 896.729 830.005 339.433
2024-12-02-01:06:08-root-INFO: Loss too large (5039.535->5141.231)! Learning rate decreased to 0.00075.
2024-12-02-01:06:09-root-INFO: grad norm: 902.706 815.488 387.114
2024-12-02-01:06:10-root-INFO: Loss Change: 5039.535 -> 4925.567
2024-12-02-01:06:10-root-INFO: Regularization Change: 0.000 -> 0.201
2024-12-02-01:06:10-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-01:06:10-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:10-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-01:06:11-root-INFO: grad norm: 771.255 716.492 285.434
2024-12-02-01:06:11-root-INFO: Loss too large (4849.352->4913.558)! Learning rate decreased to 0.00079.
2024-12-02-01:06:12-root-INFO: grad norm: 766.349 693.230 326.685
2024-12-02-01:06:13-root-INFO: Loss Change: 4849.352 -> 4746.089
2024-12-02-01:06:13-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-01:06:13-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-01:06:13-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:13-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-01:06:13-root-INFO: grad norm: 693.589 640.568 265.968
2024-12-02-01:06:14-root-INFO: Loss too large (4684.205->4740.855)! Learning rate decreased to 0.00082.
2024-12-02-01:06:15-root-INFO: grad norm: 667.176 605.515 280.134
2024-12-02-01:06:15-root-INFO: Loss Change: 4684.205 -> 4593.321
2024-12-02-01:06:15-root-INFO: Regularization Change: 0.000 -> 0.134
2024-12-02-01:06:15-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-01:06:15-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:16-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-01:06:16-root-INFO: grad norm: 534.725 499.031 192.091
2024-12-02-01:06:16-root-INFO: Loss too large (4528.314->4534.843)! Learning rate decreased to 0.00086.
2024-12-02-01:06:17-root-INFO: grad norm: 494.533 449.245 206.741
2024-12-02-01:06:18-root-INFO: Loss Change: 4528.314 -> 4442.099
2024-12-02-01:06:18-root-INFO: Regularization Change: 0.000 -> 0.133
2024-12-02-01:06:18-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-01:06:18-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:18-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-01:06:19-root-INFO: grad norm: 380.467 356.032 134.150
2024-12-02-01:06:20-root-INFO: grad norm: 451.458 410.086 188.796
2024-12-02-01:06:20-root-INFO: Loss Change: 4406.125 -> 4368.993
2024-12-02-01:06:20-root-INFO: Regularization Change: 0.000 -> 0.236
2024-12-02-01:06:20-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-01:06:20-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:21-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-01:06:21-root-INFO: grad norm: 558.652 517.839 209.607
2024-12-02-01:06:21-root-INFO: Loss too large (4337.050->4359.148)! Learning rate decreased to 0.00095.
2024-12-02-01:06:22-root-INFO: grad norm: 482.567 439.670 198.900
2024-12-02-01:06:23-root-INFO: Loss Change: 4337.050 -> 4248.384
2024-12-02-01:06:23-root-INFO: Regularization Change: 0.000 -> 0.114
2024-12-02-01:06:23-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-01:06:23-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-01:06:23-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-01:06:24-root-INFO: grad norm: 321.141 299.124 116.862
2024-12-02-01:06:25-root-INFO: grad norm: 346.113 317.717 137.296
2024-12-02-01:06:25-root-INFO: Loss Change: 4191.621 -> 4137.716
2024-12-02-01:06:25-root-INFO: Regularization Change: 0.000 -> 0.205
2024-12-02-01:06:25-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-01:06:25-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:06:26-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-01:06:26-root-INFO: grad norm: 372.909 348.240 133.377
2024-12-02-01:06:27-root-INFO: grad norm: 433.725 399.669 168.471
2024-12-02-01:06:28-root-INFO: Loss Change: 4105.884 -> 4083.622
2024-12-02-01:06:28-root-INFO: Regularization Change: 0.000 -> 0.170
2024-12-02-01:06:28-root-INFO: Undo step: 210
2024-12-02-01:06:28-root-INFO: Undo step: 211
2024-12-02-01:06:28-root-INFO: Undo step: 212
2024-12-02-01:06:28-root-INFO: Undo step: 213
2024-12-02-01:06:28-root-INFO: Undo step: 214
2024-12-02-01:06:28-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-01:06:28-root-INFO: grad norm: 3261.970 2826.945 1627.522
2024-12-02-01:06:29-root-INFO: grad norm: 2848.091 2512.674 1340.931
2024-12-02-01:06:30-root-INFO: Loss Change: 8356.869 -> 6256.446
2024-12-02-01:06:30-root-INFO: Regularization Change: 0.000 -> 6.038
2024-12-02-01:06:30-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-01:06:30-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:30-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-01:06:31-root-INFO: grad norm: 2486.304 2216.077 1127.258
2024-12-02-01:06:31-root-INFO: Loss too large (6107.702->6505.441)! Learning rate decreased to 0.00086.
2024-12-02-01:06:32-root-INFO: grad norm: 1686.661 1566.932 624.139
2024-12-02-01:06:33-root-INFO: Loss Change: 6107.702 -> 4880.921
2024-12-02-01:06:33-root-INFO: Regularization Change: 0.000 -> 1.090
2024-12-02-01:06:33-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-01:06:33-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:33-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-01:06:33-root-INFO: grad norm: 1183.433 1081.743 479.943
2024-12-02-01:06:34-root-INFO: Loss too large (4904.736->5032.535)! Learning rate decreased to 0.00090.
2024-12-02-01:06:35-root-INFO: grad norm: 1015.661 957.429 338.963
2024-12-02-01:06:35-root-INFO: Loss Change: 4904.736 -> 4634.411
2024-12-02-01:06:35-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-01:06:35-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-01:06:35-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:36-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-01:06:36-root-INFO: grad norm: 877.242 799.839 360.294
2024-12-02-01:06:36-root-INFO: Loss too large (4613.272->4667.160)! Learning rate decreased to 0.00095.
2024-12-02-01:06:37-root-INFO: grad norm: 734.953 687.278 260.395
2024-12-02-01:06:38-root-INFO: Loss Change: 4613.272 -> 4427.215
2024-12-02-01:06:38-root-INFO: Regularization Change: 0.000 -> 0.192
2024-12-02-01:06:38-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-01:06:38-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-01:06:38-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-01:06:39-root-INFO: grad norm: 756.921 685.492 320.981
2024-12-02-01:06:39-root-INFO: Loss too large (4440.208->4454.662)! Learning rate decreased to 0.00099.
2024-12-02-01:06:40-root-INFO: grad norm: 611.316 567.334 227.683
2024-12-02-01:06:41-root-INFO: Loss Change: 4440.208 -> 4269.734
2024-12-02-01:06:41-root-INFO: Regularization Change: 0.000 -> 0.200
2024-12-02-01:06:41-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-01:06:41-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:06:41-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-01:06:41-root-INFO: grad norm: 554.167 505.923 226.150
2024-12-02-01:06:42-root-INFO: grad norm: 669.076 619.763 252.103
2024-12-02-01:06:43-root-INFO: Loss too large (4262.063->4294.414)! Learning rate decreased to 0.00104.
2024-12-02-01:06:43-root-INFO: Loss Change: 4271.073 -> 4192.305
2024-12-02-01:06:43-root-INFO: Regularization Change: 0.000 -> 0.187
2024-12-02-01:06:43-root-INFO: Undo step: 210
2024-12-02-01:06:43-root-INFO: Undo step: 211
2024-12-02-01:06:43-root-INFO: Undo step: 212
2024-12-02-01:06:43-root-INFO: Undo step: 213
2024-12-02-01:06:43-root-INFO: Undo step: 214
2024-12-02-01:06:44-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-01:06:44-root-INFO: grad norm: 2420.559 1869.661 1537.359
2024-12-02-01:06:45-root-INFO: grad norm: 1373.383 1210.723 648.328
2024-12-02-01:06:46-root-INFO: Loss Change: 7327.052 -> 4970.395
2024-12-02-01:06:46-root-INFO: Regularization Change: 0.000 -> 4.082
2024-12-02-01:06:46-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-01:06:46-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:46-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-01:06:46-root-INFO: grad norm: 994.643 870.028 482.044
2024-12-02-01:06:47-root-INFO: grad norm: 967.995 894.440 370.125
2024-12-02-01:06:48-root-INFO: Loss Change: 4863.800 -> 4682.826
2024-12-02-01:06:48-root-INFO: Regularization Change: 0.000 -> 0.565
2024-12-02-01:06:48-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-01:06:48-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:48-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-01:06:49-root-INFO: grad norm: 1099.719 1023.636 401.934
2024-12-02-01:06:49-root-INFO: Loss too large (4656.856->4873.202)! Learning rate decreased to 0.00090.
2024-12-02-01:06:50-root-INFO: grad norm: 1063.945 1000.261 362.569
2024-12-02-01:06:51-root-INFO: Loss Change: 4656.856 -> 4572.893
2024-12-02-01:06:51-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-02-01:06:51-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-01:06:51-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-01:06:51-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-01:06:51-root-INFO: grad norm: 1153.122 1067.594 435.814
2024-12-02-01:06:52-root-INFO: Loss too large (4555.756->5045.006)! Learning rate decreased to 0.00095.
2024-12-02-01:06:52-root-INFO: Loss too large (4555.756->4621.402)! Learning rate decreased to 0.00076.
2024-12-02-01:06:53-root-INFO: grad norm: 873.687 821.830 296.522
2024-12-02-01:06:54-root-INFO: Loss Change: 4555.756 -> 4302.688
2024-12-02-01:06:54-root-INFO: Regularization Change: 0.000 -> 0.152
2024-12-02-01:06:54-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-01:06:54-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-01:06:54-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-01:06:54-root-INFO: grad norm: 808.040 736.843 331.650
2024-12-02-01:06:55-root-INFO: Loss too large (4313.587->4521.934)! Learning rate decreased to 0.00099.
2024-12-02-01:06:55-root-INFO: Loss too large (4313.587->4315.151)! Learning rate decreased to 0.00079.
2024-12-02-01:06:56-root-INFO: grad norm: 609.497 569.684 216.672
2024-12-02-01:06:57-root-INFO: Loss Change: 4313.587 -> 4148.627
2024-12-02-01:06:57-root-INFO: Regularization Change: 0.000 -> 0.141
2024-12-02-01:06:57-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-01:06:57-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:06:57-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-01:06:57-root-INFO: grad norm: 529.759 484.320 214.659
2024-12-02-01:06:58-root-INFO: Loss too large (4143.663->4213.212)! Learning rate decreased to 0.00104.
2024-12-02-01:06:59-root-INFO: grad norm: 608.211 565.235 224.566
2024-12-02-01:06:59-root-INFO: Loss too large (4129.381->4130.451)! Learning rate decreased to 0.00083.
2024-12-02-01:07:00-root-INFO: Loss Change: 4143.663 -> 4069.580
2024-12-02-01:07:00-root-INFO: Regularization Change: 0.000 -> 0.117
2024-12-02-01:07:00-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-01:07:00-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:00-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-01:07:00-root-INFO: grad norm: 537.314 487.039 226.934
2024-12-02-01:07:01-root-INFO: Loss too large (4061.961->4136.167)! Learning rate decreased to 0.00109.
2024-12-02-01:07:02-root-INFO: grad norm: 608.510 560.708 236.411
2024-12-02-01:07:02-root-INFO: Loss too large (4046.211->4047.388)! Learning rate decreased to 0.00087.
2024-12-02-01:07:03-root-INFO: Loss Change: 4061.961 -> 3983.661
2024-12-02-01:07:03-root-INFO: Regularization Change: 0.000 -> 0.125
2024-12-02-01:07:03-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-01:07:03-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:03-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-01:07:03-root-INFO: grad norm: 541.260 488.914 232.218
2024-12-02-01:07:04-root-INFO: Loss too large (3994.158->4075.228)! Learning rate decreased to 0.00114.
2024-12-02-01:07:05-root-INFO: grad norm: 613.396 562.000 245.785
2024-12-02-01:07:05-root-INFO: Loss too large (3979.222->3982.666)! Learning rate decreased to 0.00091.
2024-12-02-01:07:06-root-INFO: Loss Change: 3994.158 -> 3914.027
2024-12-02-01:07:06-root-INFO: Regularization Change: 0.000 -> 0.135
2024-12-02-01:07:06-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-01:07:06-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:06-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-01:07:06-root-INFO: grad norm: 503.248 454.123 216.868
2024-12-02-01:07:07-root-INFO: Loss too large (3896.843->3967.882)! Learning rate decreased to 0.00120.
2024-12-02-01:07:08-root-INFO: grad norm: 557.408 505.951 233.917
2024-12-02-01:07:08-root-INFO: Loss Change: 3896.843 -> 3879.103
2024-12-02-01:07:08-root-INFO: Regularization Change: 0.000 -> 0.151
2024-12-02-01:07:08-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-01:07:08-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:09-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-01:07:09-root-INFO: grad norm: 672.729 606.549 290.968
2024-12-02-01:07:09-root-INFO: Loss too large (3877.555->4040.677)! Learning rate decreased to 0.00126.
2024-12-02-01:07:10-root-INFO: grad norm: 725.805 658.218 305.847
2024-12-02-01:07:11-root-INFO: Loss too large (3867.685->3874.225)! Learning rate decreased to 0.00101.
2024-12-02-01:07:11-root-INFO: Loss Change: 3877.555 -> 3767.976
2024-12-02-01:07:11-root-INFO: Regularization Change: 0.000 -> 0.127
2024-12-02-01:07:11-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-01:07:11-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:12-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-01:07:12-root-INFO: grad norm: 561.454 503.765 247.894
2024-12-02-01:07:12-root-INFO: Loss too large (3767.452->3875.050)! Learning rate decreased to 0.00132.
2024-12-02-01:07:13-root-INFO: grad norm: 603.900 544.259 261.681
2024-12-02-01:07:14-root-INFO: Loss Change: 3767.452 -> 3751.506
2024-12-02-01:07:14-root-INFO: Regularization Change: 0.000 -> 0.149
2024-12-02-01:07:14-root-INFO: Undo step: 205
2024-12-02-01:07:14-root-INFO: Undo step: 206
2024-12-02-01:07:14-root-INFO: Undo step: 207
2024-12-02-01:07:14-root-INFO: Undo step: 208
2024-12-02-01:07:14-root-INFO: Undo step: 209
2024-12-02-01:07:14-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-01:07:15-root-INFO: grad norm: 1067.331 908.733 559.820
2024-12-02-01:07:16-root-INFO: grad norm: 505.382 447.007 235.788
2024-12-02-01:07:16-root-INFO: Loss Change: 5405.683 -> 4246.677
2024-12-02-01:07:16-root-INFO: Regularization Change: 0.000 -> 3.272
2024-12-02-01:07:16-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-01:07:16-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:17-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-01:07:17-root-INFO: grad norm: 347.871 310.943 155.977
2024-12-02-01:07:18-root-INFO: grad norm: 307.678 286.991 110.915
2024-12-02-01:07:19-root-INFO: Loss Change: 4186.256 -> 4008.898
2024-12-02-01:07:19-root-INFO: Regularization Change: 0.000 -> 0.557
2024-12-02-01:07:19-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-01:07:19-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:19-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-01:07:19-root-INFO: grad norm: 372.966 342.486 147.672
2024-12-02-01:07:20-root-INFO: grad norm: 432.262 408.061 142.608
2024-12-02-01:07:21-root-INFO: Loss Change: 3998.166 -> 3943.716
2024-12-02-01:07:21-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-02-01:07:21-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-01:07:21-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:21-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-01:07:22-root-INFO: grad norm: 546.439 510.048 196.078
2024-12-02-01:07:22-root-INFO: Loss too large (3922.089->3931.160)! Learning rate decreased to 0.00120.
2024-12-02-01:07:23-root-INFO: grad norm: 452.987 427.662 149.339
2024-12-02-01:07:24-root-INFO: Loss Change: 3922.089 -> 3805.066
2024-12-02-01:07:24-root-INFO: Regularization Change: 0.000 -> 0.187
2024-12-02-01:07:24-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-01:07:24-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:24-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-01:07:24-root-INFO: grad norm: 405.634 379.883 142.224
2024-12-02-01:07:25-root-INFO: grad norm: 488.208 458.690 167.185
2024-12-02-01:07:26-root-INFO: Loss too large (3772.402->3779.608)! Learning rate decreased to 0.00126.
2024-12-02-01:07:26-root-INFO: Loss Change: 3784.456 -> 3715.561
2024-12-02-01:07:26-root-INFO: Regularization Change: 0.000 -> 0.205
2024-12-02-01:07:26-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-01:07:26-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:27-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-01:07:27-root-INFO: grad norm: 435.602 406.722 155.969
2024-12-02-01:07:28-root-INFO: grad norm: 520.821 486.863 184.984
2024-12-02-01:07:28-root-INFO: Loss too large (3701.389->3714.514)! Learning rate decreased to 0.00132.
2024-12-02-01:07:29-root-INFO: Loss Change: 3705.557 -> 3635.880
2024-12-02-01:07:29-root-INFO: Regularization Change: 0.000 -> 0.208
2024-12-02-01:07:29-root-INFO: Undo step: 205
2024-12-02-01:07:29-root-INFO: Undo step: 206
2024-12-02-01:07:29-root-INFO: Undo step: 207
2024-12-02-01:07:29-root-INFO: Undo step: 208
2024-12-02-01:07:29-root-INFO: Undo step: 209
2024-12-02-01:07:29-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-01:07:30-root-INFO: grad norm: 2468.924 2107.318 1286.388
2024-12-02-01:07:31-root-INFO: grad norm: 2671.660 2516.885 896.136
2024-12-02-01:07:31-root-INFO: Loss too large (6622.493->7533.992)! Learning rate decreased to 0.00104.
2024-12-02-01:07:32-root-INFO: Loss Change: 6886.185 -> 5600.521
2024-12-02-01:07:32-root-INFO: Regularization Change: 0.000 -> 5.450
2024-12-02-01:07:32-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-01:07:32-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:32-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-01:07:32-root-INFO: grad norm: 2392.532 2323.129 572.084
2024-12-02-01:07:33-root-INFO: Loss too large (5479.966->6669.048)! Learning rate decreased to 0.00109.
2024-12-02-01:07:34-root-INFO: grad norm: 1993.980 1884.354 652.047
2024-12-02-01:07:34-root-INFO: Loss Change: 5479.966 -> 4508.082
2024-12-02-01:07:34-root-INFO: Regularization Change: 0.000 -> 1.596
2024-12-02-01:07:34-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-01:07:34-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:35-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-01:07:35-root-INFO: grad norm: 1336.977 1249.592 475.422
2024-12-02-01:07:35-root-INFO: Loss too large (4420.301->4531.179)! Learning rate decreased to 0.00114.
2024-12-02-01:07:36-root-INFO: grad norm: 986.037 921.992 349.570
2024-12-02-01:07:37-root-INFO: Loss Change: 4420.301 -> 3886.001
2024-12-02-01:07:37-root-INFO: Regularization Change: 0.000 -> 0.498
2024-12-02-01:07:37-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-01:07:37-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:37-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-01:07:38-root-INFO: grad norm: 662.998 620.961 232.324
2024-12-02-01:07:39-root-INFO: grad norm: 727.646 682.699 251.775
2024-12-02-01:07:39-root-INFO: Loss too large (3807.232->3809.202)! Learning rate decreased to 0.00120.
2024-12-02-01:07:39-root-INFO: Loss Change: 3822.289 -> 3680.249
2024-12-02-01:07:39-root-INFO: Regularization Change: 0.000 -> 0.254
2024-12-02-01:07:39-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-01:07:39-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:40-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-01:07:40-root-INFO: grad norm: 464.487 439.541 150.173
2024-12-02-01:07:41-root-INFO: grad norm: 492.763 464.882 163.402
2024-12-02-01:07:42-root-INFO: Loss Change: 3638.231 -> 3593.469
2024-12-02-01:07:42-root-INFO: Regularization Change: 0.000 -> 0.235
2024-12-02-01:07:42-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-01:07:42-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-01:07:42-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-01:07:43-root-INFO: grad norm: 480.977 458.882 144.105
2024-12-02-01:07:44-root-INFO: grad norm: 496.730 471.294 156.914
2024-12-02-01:07:44-root-INFO: Loss Change: 3555.199 -> 3510.408
2024-12-02-01:07:44-root-INFO: Regularization Change: 0.000 -> 0.211
2024-12-02-01:07:44-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-01:07:44-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-01:07:45-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-01:07:45-root-INFO: grad norm: 479.740 460.380 134.910
2024-12-02-01:07:46-root-INFO: grad norm: 483.713 462.038 143.174
2024-12-02-01:07:46-root-INFO: Loss Change: 3475.116 -> 3424.800
2024-12-02-01:07:46-root-INFO: Regularization Change: 0.000 -> 0.207
2024-12-02-01:07:46-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-01:07:47-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:07:47-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-01:07:47-root-INFO: grad norm: 478.766 461.369 127.890
2024-12-02-01:07:48-root-INFO: grad norm: 478.595 456.888 142.503
2024-12-02-01:07:49-root-INFO: Loss Change: 3404.468 -> 3355.753
2024-12-02-01:07:49-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-01:07:49-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-01:07:49-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:07:49-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-01:07:50-root-INFO: grad norm: 482.672 466.830 122.644
2024-12-02-01:07:51-root-INFO: grad norm: 462.675 449.442 109.865
2024-12-02-01:07:51-root-INFO: Loss Change: 3347.049 -> 3277.103
2024-12-02-01:07:51-root-INFO: Regularization Change: 0.000 -> 0.233
2024-12-02-01:07:51-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-01:07:51-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:07:52-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-01:07:52-root-INFO: grad norm: 432.619 420.513 101.626
2024-12-02-01:07:53-root-INFO: grad norm: 416.506 403.852 101.884
2024-12-02-01:07:53-root-INFO: Loss Change: 3259.854 -> 3199.587
2024-12-02-01:07:53-root-INFO: Regularization Change: 0.000 -> 0.194
2024-12-02-01:07:54-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-01:07:54-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:07:54-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-01:07:54-root-INFO: grad norm: 457.319 443.879 110.057
2024-12-02-01:07:55-root-INFO: grad norm: 445.488 435.439 94.090
2024-12-02-01:07:56-root-INFO: Loss Change: 3204.480 -> 3147.379
2024-12-02-01:07:56-root-INFO: Regularization Change: 0.000 -> 0.218
2024-12-02-01:07:56-root-INFO: Undo step: 200
2024-12-02-01:07:56-root-INFO: Undo step: 201
2024-12-02-01:07:56-root-INFO: Undo step: 202
2024-12-02-01:07:56-root-INFO: Undo step: 203
2024-12-02-01:07:56-root-INFO: Undo step: 204
2024-12-02-01:07:56-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-01:07:57-root-INFO: grad norm: 1491.005 1362.525 605.492
2024-12-02-01:07:58-root-INFO: grad norm: 1026.159 966.957 343.506
2024-12-02-01:07:58-root-INFO: Loss Change: 4770.463 -> 3664.824
2024-12-02-01:07:58-root-INFO: Regularization Change: 0.000 -> 3.571
2024-12-02-01:07:58-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-01:07:58-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-01:07:59-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-01:07:59-root-INFO: grad norm: 629.146 598.664 193.458
2024-12-02-01:08:00-root-INFO: grad norm: 630.901 604.258 181.406
2024-12-02-01:08:01-root-INFO: Loss Change: 3625.483 -> 3509.319
2024-12-02-01:08:01-root-INFO: Regularization Change: 0.000 -> 0.532
2024-12-02-01:08:01-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-01:08:01-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:01-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-01:08:01-root-INFO: grad norm: 636.007 607.694 187.652
2024-12-02-01:08:02-root-INFO: grad norm: 659.759 635.117 178.631
2024-12-02-01:08:03-root-INFO: Loss Change: 3492.188 -> 3438.431
2024-12-02-01:08:03-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-02-01:08:03-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-01:08:03-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:03-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-01:08:04-root-INFO: grad norm: 693.127 658.240 217.129
2024-12-02-01:08:05-root-INFO: grad norm: 704.701 681.270 180.208
2024-12-02-01:08:05-root-INFO: Loss Change: 3430.229 -> 3377.153
2024-12-02-01:08:05-root-INFO: Regularization Change: 0.000 -> 0.377
2024-12-02-01:08:05-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-01:08:05-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:06-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-01:08:06-root-INFO: grad norm: 704.087 676.159 196.333
2024-12-02-01:08:07-root-INFO: grad norm: 714.999 694.465 170.126
2024-12-02-01:08:08-root-INFO: Loss Change: 3362.566 -> 3316.994
2024-12-02-01:08:08-root-INFO: Regularization Change: 0.000 -> 0.263
2024-12-02-01:08:08-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-01:08:08-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:08-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-01:08:08-root-INFO: grad norm: 703.978 674.989 199.935
2024-12-02-01:08:09-root-INFO: grad norm: 718.065 700.222 159.078
2024-12-02-01:08:10-root-INFO: Loss too large (3242.224->3250.757)! Learning rate decreased to 0.00165.
2024-12-02-01:08:10-root-INFO: Loss Change: 3286.941 -> 3089.679
2024-12-02-01:08:10-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-02-01:08:10-root-INFO: Undo step: 200
2024-12-02-01:08:10-root-INFO: Undo step: 201
2024-12-02-01:08:10-root-INFO: Undo step: 202
2024-12-02-01:08:10-root-INFO: Undo step: 203
2024-12-02-01:08:10-root-INFO: Undo step: 204
2024-12-02-01:08:11-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-01:08:11-root-INFO: grad norm: 1470.154 1360.469 557.203
2024-12-02-01:08:12-root-INFO: grad norm: 1226.265 1173.415 356.124
2024-12-02-01:08:13-root-INFO: Loss Change: 4947.861 -> 4028.444
2024-12-02-01:08:13-root-INFO: Regularization Change: 0.000 -> 4.833
2024-12-02-01:08:13-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-01:08:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-01:08:13-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-01:08:13-root-INFO: grad norm: 887.727 834.716 302.172
2024-12-02-01:08:14-root-INFO: grad norm: 802.004 753.141 275.663
2024-12-02-01:08:15-root-INFO: Loss too large (3640.489->3754.657)! Learning rate decreased to 0.00138.
2024-12-02-01:08:15-root-INFO: Loss Change: 4047.515 -> 3569.531
2024-12-02-01:08:15-root-INFO: Regularization Change: 0.000 -> 1.381
2024-12-02-01:08:15-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-01:08:15-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:16-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-01:08:16-root-INFO: grad norm: 675.380 642.444 208.336
2024-12-02-01:08:17-root-INFO: grad norm: 810.969 766.755 264.117
2024-12-02-01:08:17-root-INFO: Loss too large (3527.404->3650.745)! Learning rate decreased to 0.00144.
2024-12-02-01:08:18-root-INFO: Loss Change: 3574.987 -> 3452.079
2024-12-02-01:08:18-root-INFO: Regularization Change: 0.000 -> 0.366
2024-12-02-01:08:18-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-01:08:18-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:18-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-01:08:18-root-INFO: grad norm: 706.994 664.766 240.679
2024-12-02-01:08:19-root-INFO: grad norm: 843.894 805.764 250.800
2024-12-02-01:08:20-root-INFO: Loss too large (3439.407->3568.437)! Learning rate decreased to 0.00150.
2024-12-02-01:08:20-root-INFO: Loss Change: 3472.725 -> 3347.146
2024-12-02-01:08:20-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-01:08:20-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-01:08:20-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:21-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-01:08:21-root-INFO: grad norm: 646.452 612.302 207.329
2024-12-02-01:08:22-root-INFO: grad norm: 699.498 667.001 210.729
2024-12-02-01:08:22-root-INFO: Loss too large (3282.169->3331.413)! Learning rate decreased to 0.00157.
2024-12-02-01:08:23-root-INFO: Loss Change: 3345.269 -> 3181.417
2024-12-02-01:08:23-root-INFO: Regularization Change: 0.000 -> 0.307
2024-12-02-01:08:23-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-01:08:23-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:23-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-01:08:24-root-INFO: grad norm: 538.679 506.053 184.624
2024-12-02-01:08:25-root-INFO: grad norm: 583.556 557.467 172.534
2024-12-02-01:08:25-root-INFO: Loss too large (3133.444->3156.625)! Learning rate decreased to 0.00165.
2024-12-02-01:08:26-root-INFO: Loss Change: 3176.222 -> 3049.030
2024-12-02-01:08:26-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-02-01:08:26-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-01:08:26-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:26-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-01:08:26-root-INFO: grad norm: 437.529 412.989 144.470
2024-12-02-01:08:27-root-INFO: grad norm: 471.504 450.191 140.157
2024-12-02-01:08:28-root-INFO: Loss too large (3003.240->3013.217)! Learning rate decreased to 0.00172.
2024-12-02-01:08:28-root-INFO: Loss Change: 3037.103 -> 2940.030
2024-12-02-01:08:28-root-INFO: Regularization Change: 0.000 -> 0.249
2024-12-02-01:08:28-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-01:08:28-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-01:08:29-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-01:08:29-root-INFO: grad norm: 386.719 363.272 132.607
2024-12-02-01:08:30-root-INFO: grad norm: 459.211 441.459 126.448
2024-12-02-01:08:30-root-INFO: Loss too large (2919.880->2963.679)! Learning rate decreased to 0.00180.
2024-12-02-01:08:31-root-INFO: Loss Change: 2937.451 -> 2881.992
2024-12-02-01:08:31-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-01:08:31-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-01:08:31-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:08:31-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-01:08:32-root-INFO: grad norm: 398.389 381.088 116.126
2024-12-02-01:08:33-root-INFO: grad norm: 504.656 490.271 119.635
2024-12-02-01:08:33-root-INFO: Loss too large (2874.378->2954.767)! Learning rate decreased to 0.00188.
2024-12-02-01:08:34-root-INFO: Loss Change: 2877.904 -> 2848.109
2024-12-02-01:08:34-root-INFO: Regularization Change: 0.000 -> 0.206
2024-12-02-01:08:34-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-01:08:34-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:08:34-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-01:08:34-root-INFO: grad norm: 460.460 438.388 140.852
2024-12-02-01:08:35-root-INFO: Loss too large (2855.903->2859.007)! Learning rate decreased to 0.00196.
2024-12-02-01:08:36-root-INFO: grad norm: 349.320 339.655 81.604
2024-12-02-01:08:36-root-INFO: Loss Change: 2855.903 -> 2738.192
2024-12-02-01:08:36-root-INFO: Regularization Change: 0.000 -> 0.215
2024-12-02-01:08:36-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-01:08:36-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:08:37-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-01:08:37-root-INFO: grad norm: 316.032 305.616 80.471
2024-12-02-01:08:37-root-INFO: Loss too large (2729.854->2734.003)! Learning rate decreased to 0.00205.
2024-12-02-01:08:38-root-INFO: grad norm: 267.020 258.942 65.179
2024-12-02-01:08:39-root-INFO: Loss Change: 2729.854 -> 2666.022
2024-12-02-01:08:39-root-INFO: Regularization Change: 0.000 -> 0.164
2024-12-02-01:08:39-root-INFO: Undo step: 195
2024-12-02-01:08:39-root-INFO: Undo step: 196
2024-12-02-01:08:39-root-INFO: Undo step: 197
2024-12-02-01:08:39-root-INFO: Undo step: 198
2024-12-02-01:08:39-root-INFO: Undo step: 199
2024-12-02-01:08:39-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-01:08:40-root-INFO: grad norm: 1056.280 920.496 518.089
2024-12-02-01:08:41-root-INFO: grad norm: 811.307 791.382 178.700
2024-12-02-01:08:41-root-INFO: Loss too large (3407.527->3429.151)! Learning rate decreased to 0.00165.
2024-12-02-01:08:42-root-INFO: Loss Change: 4188.467 -> 3239.594
2024-12-02-01:08:42-root-INFO: Regularization Change: 0.000 -> 4.405
2024-12-02-01:08:42-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-01:08:42-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:42-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-01:08:42-root-INFO: grad norm: 599.234 581.560 144.461
2024-12-02-01:08:43-root-INFO: grad norm: 611.665 598.863 124.484
2024-12-02-01:08:44-root-INFO: Loss too large (3120.170->3131.264)! Learning rate decreased to 0.00172.
2024-12-02-01:08:44-root-INFO: Loss Change: 3228.285 -> 3021.438
2024-12-02-01:08:44-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-02-01:08:44-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-01:08:44-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-01:08:45-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-01:08:45-root-INFO: grad norm: 467.686 446.303 139.799
2024-12-02-01:08:46-root-INFO: grad norm: 493.010 479.089 116.331
2024-12-02-01:08:46-root-INFO: Loss too large (2948.016->2979.911)! Learning rate decreased to 0.00180.
2024-12-02-01:08:47-root-INFO: Loss Change: 3028.993 -> 2895.294
2024-12-02-01:08:47-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-01:08:47-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-01:08:47-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:08:47-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-01:08:48-root-INFO: grad norm: 403.922 387.568 113.771
2024-12-02-01:08:49-root-INFO: grad norm: 461.315 447.653 111.437
2024-12-02-01:08:49-root-INFO: Loss too large (2848.960->2896.196)! Learning rate decreased to 0.00188.
2024-12-02-01:08:50-root-INFO: Loss Change: 2893.909 -> 2813.143
2024-12-02-01:08:50-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-01:08:50-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-01:08:50-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:08:50-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-01:08:50-root-INFO: grad norm: 425.845 402.952 137.744
2024-12-02-01:08:51-root-INFO: grad norm: 525.878 510.550 126.040
2024-12-02-01:08:52-root-INFO: Loss too large (2807.036->2895.216)! Learning rate decreased to 0.00196.
2024-12-02-01:08:52-root-INFO: Loss Change: 2824.971 -> 2780.246
2024-12-02-01:08:52-root-INFO: Regularization Change: 0.000 -> 0.264
2024-12-02-01:08:52-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-01:08:52-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:08:53-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-01:08:53-root-INFO: grad norm: 416.250 402.014 107.931
2024-12-02-01:08:54-root-INFO: grad norm: 464.133 450.635 111.120
2024-12-02-01:08:54-root-INFO: Loss too large (2728.160->2791.522)! Learning rate decreased to 0.00205.
2024-12-02-01:08:55-root-INFO: Loss Change: 2776.440 -> 2696.308
2024-12-02-01:08:55-root-INFO: Regularization Change: 0.000 -> 0.214
2024-12-02-01:08:55-root-INFO: Undo step: 195
2024-12-02-01:08:55-root-INFO: Undo step: 196
2024-12-02-01:08:55-root-INFO: Undo step: 197
2024-12-02-01:08:55-root-INFO: Undo step: 198
2024-12-02-01:08:55-root-INFO: Undo step: 199
2024-12-02-01:08:55-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-01:08:56-root-INFO: grad norm: 861.711 714.276 482.033
2024-12-02-01:08:57-root-INFO: grad norm: 483.012 461.591 142.247
2024-12-02-01:08:58-root-INFO: Loss Change: 4108.473 -> 3177.223
2024-12-02-01:08:58-root-INFO: Regularization Change: 0.000 -> 4.159
2024-12-02-01:08:58-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-01:08:58-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-01:08:58-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-01:08:58-root-INFO: grad norm: 529.991 517.630 113.797
2024-12-02-01:08:59-root-INFO: Loss too large (3155.295->3173.134)! Learning rate decreased to 0.00172.
2024-12-02-01:09:00-root-INFO: grad norm: 441.818 430.055 101.274
2024-12-02-01:09:00-root-INFO: Loss Change: 3155.295 -> 2972.968
2024-12-02-01:09:00-root-INFO: Regularization Change: 0.000 -> 0.511
2024-12-02-01:09:00-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-01:09:00-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-01:09:01-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-01:09:01-root-INFO: grad norm: 404.338 394.043 90.658
2024-12-02-01:09:01-root-INFO: Loss too large (2962.686->2981.918)! Learning rate decreased to 0.00180.
2024-12-02-01:09:02-root-INFO: grad norm: 355.425 346.852 77.593
2024-12-02-01:09:03-root-INFO: Loss Change: 2962.686 -> 2850.138
2024-12-02-01:09:03-root-INFO: Regularization Change: 0.000 -> 0.349
2024-12-02-01:09:03-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-01:09:03-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:03-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-01:09:04-root-INFO: grad norm: 333.640 325.540 73.072
2024-12-02-01:09:04-root-INFO: Loss too large (2844.451->2860.268)! Learning rate decreased to 0.00188.
2024-12-02-01:09:05-root-INFO: grad norm: 313.049 305.148 69.887
2024-12-02-01:09:06-root-INFO: Loss Change: 2844.451 -> 2765.913
2024-12-02-01:09:06-root-INFO: Regularization Change: 0.000 -> 0.274
2024-12-02-01:09:06-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-01:09:06-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:06-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-01:09:06-root-INFO: grad norm: 428.641 416.689 100.516
2024-12-02-01:09:07-root-INFO: Loss too large (2766.224->2857.008)! Learning rate decreased to 0.00196.
2024-12-02-01:09:07-root-INFO: Loss too large (2766.224->2790.004)! Learning rate decreased to 0.00157.
2024-12-02-01:09:08-root-INFO: grad norm: 302.724 296.058 63.180
2024-12-02-01:09:09-root-INFO: Loss Change: 2766.224 -> 2674.236
2024-12-02-01:09:09-root-INFO: Regularization Change: 0.000 -> 0.206
2024-12-02-01:09:09-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-01:09:09-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:09-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-01:09:09-root-INFO: grad norm: 190.465 183.787 49.994
2024-12-02-01:09:10-root-INFO: grad norm: 271.639 264.892 60.165
2024-12-02-01:09:11-root-INFO: Loss too large (2647.952->2696.757)! Learning rate decreased to 0.00205.
2024-12-02-01:09:11-root-INFO: Loss Change: 2664.269 -> 2645.870
2024-12-02-01:09:11-root-INFO: Regularization Change: 0.000 -> 0.290
2024-12-02-01:09:11-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-01:09:11-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:12-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-01:09:12-root-INFO: grad norm: 487.775 472.071 122.774
2024-12-02-01:09:12-root-INFO: Loss too large (2657.232->2863.059)! Learning rate decreased to 0.00214.
2024-12-02-01:09:13-root-INFO: Loss too large (2657.232->2738.551)! Learning rate decreased to 0.00171.
2024-12-02-01:09:13-root-INFO: Loss too large (2657.232->2659.443)! Learning rate decreased to 0.00137.
2024-12-02-01:09:14-root-INFO: grad norm: 285.773 279.552 59.304
2024-12-02-01:09:15-root-INFO: Loss Change: 2657.232 -> 2554.641
2024-12-02-01:09:15-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-02-01:09:15-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-01:09:15-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:15-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-01:09:15-root-INFO: grad norm: 202.676 193.352 60.767
2024-12-02-01:09:16-root-INFO: Loss too large (2544.317->2560.146)! Learning rate decreased to 0.00224.
2024-12-02-01:09:17-root-INFO: grad norm: 276.537 270.266 58.559
2024-12-02-01:09:17-root-INFO: Loss too large (2536.375->2564.986)! Learning rate decreased to 0.00179.
2024-12-02-01:09:18-root-INFO: Loss Change: 2544.317 -> 2524.137
2024-12-02-01:09:18-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-01:09:18-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-01:09:18-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-01:09:18-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-01:09:18-root-INFO: grad norm: 381.988 370.857 91.542
2024-12-02-01:09:19-root-INFO: Loss too large (2525.991->2722.314)! Learning rate decreased to 0.00233.
2024-12-02-01:09:19-root-INFO: Loss too large (2525.991->2610.537)! Learning rate decreased to 0.00187.
2024-12-02-01:09:19-root-INFO: Loss too large (2525.991->2541.529)! Learning rate decreased to 0.00149.
2024-12-02-01:09:20-root-INFO: grad norm: 278.003 271.607 59.289
2024-12-02-01:09:21-root-INFO: Loss Change: 2525.991 -> 2458.132
2024-12-02-01:09:21-root-INFO: Regularization Change: 0.000 -> 0.109
2024-12-02-01:09:21-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-01:09:21-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:09:21-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-01:09:21-root-INFO: grad norm: 318.161 306.020 87.055
2024-12-02-01:09:22-root-INFO: Loss too large (2468.392->2616.047)! Learning rate decreased to 0.00244.
2024-12-02-01:09:22-root-INFO: Loss too large (2468.392->2534.250)! Learning rate decreased to 0.00195.
2024-12-02-01:09:22-root-INFO: Loss too large (2468.392->2483.250)! Learning rate decreased to 0.00156.
2024-12-02-01:09:24-root-INFO: grad norm: 266.772 261.191 54.281
2024-12-02-01:09:24-root-INFO: Loss Change: 2468.392 -> 2418.062
2024-12-02-01:09:24-root-INFO: Regularization Change: 0.000 -> 0.113
2024-12-02-01:09:24-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-01:09:24-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:09:25-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-01:09:25-root-INFO: grad norm: 326.376 316.279 80.556
2024-12-02-01:09:25-root-INFO: Loss too large (2426.153->2613.295)! Learning rate decreased to 0.00254.
2024-12-02-01:09:25-root-INFO: Loss too large (2426.153->2518.632)! Learning rate decreased to 0.00203.
2024-12-02-01:09:26-root-INFO: Loss too large (2426.153->2457.940)! Learning rate decreased to 0.00163.
2024-12-02-01:09:27-root-INFO: grad norm: 292.269 286.595 57.315
2024-12-02-01:09:28-root-INFO: Loss Change: 2426.153 -> 2382.279
2024-12-02-01:09:28-root-INFO: Regularization Change: 0.000 -> 0.104
2024-12-02-01:09:28-root-INFO: Undo step: 190
2024-12-02-01:09:28-root-INFO: Undo step: 191
2024-12-02-01:09:28-root-INFO: Undo step: 192
2024-12-02-01:09:28-root-INFO: Undo step: 193
2024-12-02-01:09:28-root-INFO: Undo step: 194
2024-12-02-01:09:28-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-01:09:28-root-INFO: grad norm: 1113.525 1016.223 455.224
2024-12-02-01:09:29-root-INFO: Loss too large (3915.138->4082.758)! Learning rate decreased to 0.00205.
2024-12-02-01:09:30-root-INFO: grad norm: 1219.589 1176.378 321.763
2024-12-02-01:09:30-root-INFO: Loss Change: 3915.138 -> 3366.496
2024-12-02-01:09:30-root-INFO: Regularization Change: 0.000 -> 5.486
2024-12-02-01:09:30-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-01:09:30-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:31-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-01:09:31-root-INFO: grad norm: 954.028 935.854 185.328
2024-12-02-01:09:32-root-INFO: grad norm: 656.721 633.899 171.627
2024-12-02-01:09:33-root-INFO: Loss Change: 3323.416 -> 2763.192
2024-12-02-01:09:33-root-INFO: Regularization Change: 0.000 -> 2.149
2024-12-02-01:09:33-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-01:09:33-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:33-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-01:09:33-root-INFO: grad norm: 368.311 355.920 94.733
2024-12-02-01:09:34-root-INFO: grad norm: 457.691 451.748 73.516
2024-12-02-01:09:35-root-INFO: Loss too large (2636.932->2714.027)! Learning rate decreased to 0.00224.
2024-12-02-01:09:35-root-INFO: Loss too large (2636.932->2644.934)! Learning rate decreased to 0.00179.
2024-12-02-01:09:36-root-INFO: Loss Change: 2728.870 -> 2600.188
2024-12-02-01:09:36-root-INFO: Regularization Change: 0.000 -> 0.584
2024-12-02-01:09:36-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-01:09:36-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-01:09:36-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-01:09:36-root-INFO: grad norm: 265.006 258.300 59.235
2024-12-02-01:09:37-root-INFO: grad norm: 342.389 339.036 47.796
2024-12-02-01:09:38-root-INFO: Loss too large (2520.252->2594.991)! Learning rate decreased to 0.00233.
2024-12-02-01:09:38-root-INFO: Loss too large (2520.252->2543.383)! Learning rate decreased to 0.00187.
2024-12-02-01:09:39-root-INFO: Loss Change: 2581.146 -> 2509.138
2024-12-02-01:09:39-root-INFO: Regularization Change: 0.000 -> 0.279
2024-12-02-01:09:39-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-01:09:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:09:39-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-01:09:39-root-INFO: grad norm: 242.514 236.341 54.370
2024-12-02-01:09:40-root-INFO: Loss too large (2481.499->2506.781)! Learning rate decreased to 0.00244.
2024-12-02-01:09:41-root-INFO: grad norm: 342.152 337.609 55.572
2024-12-02-01:09:41-root-INFO: Loss too large (2453.027->2504.029)! Learning rate decreased to 0.00195.
2024-12-02-01:09:41-root-INFO: Loss too large (2453.027->2467.223)! Learning rate decreased to 0.00156.
2024-12-02-01:09:42-root-INFO: Loss Change: 2481.499 -> 2441.670
2024-12-02-01:09:42-root-INFO: Regularization Change: 0.000 -> 0.169
2024-12-02-01:09:42-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-01:09:42-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:09:42-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-01:09:43-root-INFO: grad norm: 206.101 201.572 42.969
2024-12-02-01:09:43-root-INFO: Loss too large (2418.189->2458.171)! Learning rate decreased to 0.00254.
2024-12-02-01:09:44-root-INFO: grad norm: 362.316 357.185 60.761
2024-12-02-01:09:44-root-INFO: Loss too large (2407.478->2481.130)! Learning rate decreased to 0.00203.
2024-12-02-01:09:45-root-INFO: Loss too large (2407.478->2439.084)! Learning rate decreased to 0.00163.
2024-12-02-01:09:45-root-INFO: Loss too large (2407.478->2407.916)! Learning rate decreased to 0.00130.
2024-12-02-01:09:45-root-INFO: Loss Change: 2418.189 -> 2386.820
2024-12-02-01:09:45-root-INFO: Regularization Change: 0.000 -> 0.120
2024-12-02-01:09:46-root-INFO: Undo step: 190
2024-12-02-01:09:46-root-INFO: Undo step: 191
2024-12-02-01:09:46-root-INFO: Undo step: 192
2024-12-02-01:09:46-root-INFO: Undo step: 193
2024-12-02-01:09:46-root-INFO: Undo step: 194
2024-12-02-01:09:46-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-01:09:46-root-INFO: grad norm: 984.916 926.195 334.995
2024-12-02-01:09:47-root-INFO: grad norm: 898.598 868.660 230.019
2024-12-02-01:09:48-root-INFO: Loss too large (3495.709->3797.403)! Learning rate decreased to 0.00205.
2024-12-02-01:09:48-root-INFO: Loss Change: 4147.387 -> 3398.942
2024-12-02-01:09:48-root-INFO: Regularization Change: 0.000 -> 5.658
2024-12-02-01:09:48-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-01:09:48-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:49-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-01:09:49-root-INFO: grad norm: 1046.825 1012.306 266.608
2024-12-02-01:09:49-root-INFO: Loss too large (3360.052->3798.159)! Learning rate decreased to 0.00214.
2024-12-02-01:09:50-root-INFO: Loss too large (3360.052->3444.607)! Learning rate decreased to 0.00171.
2024-12-02-01:09:50-root-INFO: grad norm: 644.219 606.191 218.061
2024-12-02-01:09:51-root-INFO: Loss Change: 3360.052 -> 2822.356
2024-12-02-01:09:51-root-INFO: Regularization Change: 0.000 -> 1.283
2024-12-02-01:09:51-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-01:09:51-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-01:09:52-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-01:09:52-root-INFO: grad norm: 191.399 177.916 70.564
2024-12-02-01:09:53-root-INFO: grad norm: 186.270 175.465 62.519
2024-12-02-01:09:53-root-INFO: Loss Change: 2786.805 -> 2704.190
2024-12-02-01:09:53-root-INFO: Regularization Change: 0.000 -> 0.645
2024-12-02-01:09:53-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-01:09:53-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-01:09:54-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-01:09:54-root-INFO: grad norm: 245.475 233.687 75.155
2024-12-02-01:09:54-root-INFO: Loss too large (2676.263->2705.592)! Learning rate decreased to 0.00233.
2024-12-02-01:09:55-root-INFO: grad norm: 314.173 301.969 86.715
2024-12-02-01:09:56-root-INFO: Loss too large (2667.725->2687.331)! Learning rate decreased to 0.00187.
2024-12-02-01:09:56-root-INFO: Loss Change: 2676.263 -> 2641.191
2024-12-02-01:09:56-root-INFO: Regularization Change: 0.000 -> 0.222
2024-12-02-01:09:56-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-01:09:56-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:09:57-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-01:09:57-root-INFO: grad norm: 253.588 244.054 68.880
2024-12-02-01:09:57-root-INFO: Loss too large (2611.375->2672.126)! Learning rate decreased to 0.00244.
2024-12-02-01:09:58-root-INFO: Loss too large (2611.375->2623.246)! Learning rate decreased to 0.00195.
2024-12-02-01:09:59-root-INFO: grad norm: 269.058 259.234 72.043
2024-12-02-01:09:59-root-INFO: Loss Change: 2611.375 -> 2580.401
2024-12-02-01:09:59-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-01:09:59-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-01:09:59-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:10:00-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-01:10:00-root-INFO: grad norm: 239.581 231.271 62.551
2024-12-02-01:10:00-root-INFO: Loss too large (2551.108->2624.547)! Learning rate decreased to 0.00254.
2024-12-02-01:10:01-root-INFO: Loss too large (2551.108->2573.133)! Learning rate decreased to 0.00203.
2024-12-02-01:10:02-root-INFO: grad norm: 273.408 264.807 68.039
2024-12-02-01:10:02-root-INFO: Loss Change: 2551.108 -> 2535.204
2024-12-02-01:10:02-root-INFO: Regularization Change: 0.000 -> 0.163
2024-12-02-01:10:02-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-01:10:02-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:10:03-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-01:10:03-root-INFO: grad norm: 279.517 271.068 68.205
2024-12-02-01:10:03-root-INFO: Loss too large (2510.847->2649.411)! Learning rate decreased to 0.00265.
2024-12-02-01:10:04-root-INFO: Loss too large (2510.847->2564.729)! Learning rate decreased to 0.00212.
2024-12-02-01:10:04-root-INFO: Loss too large (2510.847->2515.520)! Learning rate decreased to 0.00170.
2024-12-02-01:10:05-root-INFO: grad norm: 235.173 228.024 57.543
2024-12-02-01:10:06-root-INFO: Loss Change: 2510.847 -> 2465.641
2024-12-02-01:10:06-root-INFO: Regularization Change: 0.000 -> 0.104
2024-12-02-01:10:06-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-01:10:06-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:10:06-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-01:10:06-root-INFO: grad norm: 134.785 129.691 36.702
2024-12-02-01:10:07-root-INFO: Loss too large (2442.894->2444.312)! Learning rate decreased to 0.00277.
2024-12-02-01:10:08-root-INFO: grad norm: 226.627 221.358 48.585
2024-12-02-01:10:08-root-INFO: Loss too large (2432.937->2471.449)! Learning rate decreased to 0.00222.
2024-12-02-01:10:08-root-INFO: Loss too large (2432.937->2439.753)! Learning rate decreased to 0.00177.
2024-12-02-01:10:09-root-INFO: Loss Change: 2442.894 -> 2421.752
2024-12-02-01:10:09-root-INFO: Regularization Change: 0.000 -> 0.181
2024-12-02-01:10:09-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-01:10:09-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-01:10:09-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-01:10:10-root-INFO: grad norm: 186.247 180.946 44.117
2024-12-02-01:10:10-root-INFO: Loss too large (2411.885->2485.097)! Learning rate decreased to 0.00289.
2024-12-02-01:10:10-root-INFO: Loss too large (2411.885->2432.477)! Learning rate decreased to 0.00231.
2024-12-02-01:10:11-root-INFO: grad norm: 285.022 278.787 59.290
2024-12-02-01:10:11-root-INFO: Loss too large (2405.712->2436.353)! Learning rate decreased to 0.00185.
2024-12-02-01:10:12-root-INFO: Loss Change: 2411.885 -> 2405.092
2024-12-02-01:10:12-root-INFO: Regularization Change: 0.000 -> 0.163
2024-12-02-01:10:12-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-01:10:12-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:10:12-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-01:10:13-root-INFO: grad norm: 229.000 223.468 50.028
2024-12-02-01:10:13-root-INFO: Loss too large (2380.026->2527.810)! Learning rate decreased to 0.00301.
2024-12-02-01:10:13-root-INFO: Loss too large (2380.026->2422.122)! Learning rate decreased to 0.00241.
2024-12-02-01:10:14-root-INFO: grad norm: 345.699 338.979 67.830
2024-12-02-01:10:15-root-INFO: Loss too large (2370.149->2426.301)! Learning rate decreased to 0.00193.
2024-12-02-01:10:15-root-INFO: Loss too large (2370.149->2381.691)! Learning rate decreased to 0.00154.
2024-12-02-01:10:16-root-INFO: Loss Change: 2380.026 -> 2352.887
2024-12-02-01:10:16-root-INFO: Regularization Change: 0.000 -> 0.116
2024-12-02-01:10:16-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-01:10:16-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:10:16-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-01:10:16-root-INFO: grad norm: 215.394 210.066 47.614
2024-12-02-01:10:17-root-INFO: Loss too large (2335.432->2483.390)! Learning rate decreased to 0.00314.
2024-12-02-01:10:17-root-INFO: Loss too large (2335.432->2378.393)! Learning rate decreased to 0.00251.
2024-12-02-01:10:18-root-INFO: grad norm: 350.490 343.700 68.652
2024-12-02-01:10:18-root-INFO: Loss too large (2326.708->2390.881)! Learning rate decreased to 0.00201.
2024-12-02-01:10:19-root-INFO: Loss too large (2326.708->2347.526)! Learning rate decreased to 0.00161.
2024-12-02-01:10:19-root-INFO: Loss Change: 2335.432 -> 2317.095
2024-12-02-01:10:19-root-INFO: Regularization Change: 0.000 -> 0.122
2024-12-02-01:10:19-root-INFO: Undo step: 185
2024-12-02-01:10:19-root-INFO: Undo step: 186
2024-12-02-01:10:19-root-INFO: Undo step: 187
2024-12-02-01:10:19-root-INFO: Undo step: 188
2024-12-02-01:10:19-root-INFO: Undo step: 189
2024-12-02-01:10:20-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-01:10:20-root-INFO: grad norm: 857.749 787.995 338.817
2024-12-02-01:10:21-root-INFO: grad norm: 593.558 561.163 193.407
2024-12-02-01:10:22-root-INFO: Loss Change: 3747.442 -> 2822.447
2024-12-02-01:10:22-root-INFO: Regularization Change: 0.000 -> 6.343
2024-12-02-01:10:22-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-01:10:22-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:10:22-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-01:10:22-root-INFO: grad norm: 539.059 514.637 160.417
2024-12-02-01:10:23-root-INFO: Loss too large (2804.304->2857.334)! Learning rate decreased to 0.00265.
2024-12-02-01:10:24-root-INFO: grad norm: 407.421 394.312 102.519
2024-12-02-01:10:24-root-INFO: Loss Change: 2804.304 -> 2546.440
2024-12-02-01:10:24-root-INFO: Regularization Change: 0.000 -> 0.978
2024-12-02-01:10:24-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-01:10:24-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:10:25-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-01:10:25-root-INFO: grad norm: 254.046 240.790 80.991
2024-12-02-01:10:26-root-INFO: grad norm: 296.382 287.489 72.058
2024-12-02-01:10:26-root-INFO: Loss too large (2514.443->2525.146)! Learning rate decreased to 0.00277.
2024-12-02-01:10:27-root-INFO: Loss Change: 2535.053 -> 2460.208
2024-12-02-01:10:27-root-INFO: Regularization Change: 0.000 -> 0.616
2024-12-02-01:10:27-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-01:10:27-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-01:10:27-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-01:10:28-root-INFO: grad norm: 364.221 355.902 77.399
2024-12-02-01:10:28-root-INFO: Loss too large (2458.895->2594.703)! Learning rate decreased to 0.00289.
2024-12-02-01:10:28-root-INFO: Loss too large (2458.895->2508.179)! Learning rate decreased to 0.00231.
2024-12-02-01:10:29-root-INFO: grad norm: 296.624 289.549 64.400
2024-12-02-01:10:30-root-INFO: Loss Change: 2458.895 -> 2384.679
2024-12-02-01:10:30-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-01:10:30-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-01:10:30-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:10:30-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-01:10:31-root-INFO: grad norm: 256.870 249.188 62.350
2024-12-02-01:10:31-root-INFO: Loss too large (2381.057->2482.323)! Learning rate decreased to 0.00301.
2024-12-02-01:10:31-root-INFO: Loss too large (2381.057->2398.817)! Learning rate decreased to 0.00241.
2024-12-02-01:10:32-root-INFO: grad norm: 273.979 265.934 65.907
2024-12-02-01:10:33-root-INFO: Loss Change: 2381.057 -> 2354.171
2024-12-02-01:10:33-root-INFO: Regularization Change: 0.000 -> 0.199
2024-12-02-01:10:33-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-01:10:33-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:10:33-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-01:10:34-root-INFO: grad norm: 309.697 301.905 69.036
2024-12-02-01:10:34-root-INFO: Loss too large (2346.455->2513.262)! Learning rate decreased to 0.00314.
2024-12-02-01:10:34-root-INFO: Loss too large (2346.455->2387.372)! Learning rate decreased to 0.00251.
2024-12-02-01:10:35-root-INFO: grad norm: 316.938 308.994 70.513
2024-12-02-01:10:36-root-INFO: Loss Change: 2346.455 -> 2317.925
2024-12-02-01:10:36-root-INFO: Regularization Change: 0.000 -> 0.197
2024-12-02-01:10:36-root-INFO: Undo step: 185
2024-12-02-01:10:36-root-INFO: Undo step: 186
2024-12-02-01:10:36-root-INFO: Undo step: 187
2024-12-02-01:10:36-root-INFO: Undo step: 188
2024-12-02-01:10:36-root-INFO: Undo step: 189
2024-12-02-01:10:36-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-01:10:37-root-INFO: grad norm: 766.245 715.173 275.062
2024-12-02-01:10:37-root-INFO: Loss too large (3466.608->3602.742)! Learning rate decreased to 0.00254.
2024-12-02-01:10:38-root-INFO: grad norm: 1056.881 1026.870 250.073
2024-12-02-01:10:38-root-INFO: Loss too large (3287.636->3564.266)! Learning rate decreased to 0.00203.
2024-12-02-01:10:39-root-INFO: Loss Change: 3466.608 -> 3286.634
2024-12-02-01:10:39-root-INFO: Regularization Change: 0.000 -> 3.798
2024-12-02-01:10:39-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-01:10:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:10:39-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-01:10:40-root-INFO: grad norm: 705.389 677.147 197.599
2024-12-02-01:10:41-root-INFO: grad norm: 585.256 564.339 155.068
2024-12-02-01:10:41-root-INFO: Loss too large (2823.102->2873.289)! Learning rate decreased to 0.00265.
2024-12-02-01:10:42-root-INFO: Loss Change: 3287.967 -> 2738.295
2024-12-02-01:10:42-root-INFO: Regularization Change: 0.000 -> 2.274
2024-12-02-01:10:42-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-01:10:42-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-01:10:42-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-01:10:42-root-INFO: grad norm: 396.661 378.324 119.211
2024-12-02-01:10:43-root-INFO: grad norm: 613.425 603.023 112.489
2024-12-02-01:10:44-root-INFO: Loss too large (2623.980->2776.490)! Learning rate decreased to 0.00277.
2024-12-02-01:10:44-root-INFO: Loss too large (2623.980->2676.956)! Learning rate decreased to 0.00222.
2024-12-02-01:10:45-root-INFO: Loss Change: 2729.973 -> 2600.341
2024-12-02-01:10:45-root-INFO: Regularization Change: 0.000 -> 1.482
2024-12-02-01:10:45-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-01:10:45-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-01:10:45-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-01:10:45-root-INFO: grad norm: 333.059 314.334 110.103
2024-12-02-01:10:46-root-INFO: grad norm: 329.353 311.349 107.400
2024-12-02-01:10:47-root-INFO: Loss too large (2432.204->2497.983)! Learning rate decreased to 0.00289.
2024-12-02-01:10:47-root-INFO: Loss Change: 2593.338 -> 2421.897
2024-12-02-01:10:47-root-INFO: Regularization Change: 0.000 -> 1.152
2024-12-02-01:10:47-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-01:10:47-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:10:48-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-01:10:48-root-INFO: grad norm: 366.219 342.473 129.724
2024-12-02-01:10:48-root-INFO: Loss too large (2418.985->2540.174)! Learning rate decreased to 0.00301.
2024-12-02-01:10:49-root-INFO: grad norm: 456.731 440.356 121.199
2024-12-02-01:10:50-root-INFO: Loss too large (2404.826->2487.351)! Learning rate decreased to 0.00241.
2024-12-02-01:10:50-root-INFO: Loss Change: 2418.985 -> 2382.329
2024-12-02-01:10:50-root-INFO: Regularization Change: 0.000 -> 0.310
2024-12-02-01:10:50-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-01:10:50-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:10:51-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-01:10:51-root-INFO: grad norm: 372.975 352.991 120.448
2024-12-02-01:10:51-root-INFO: Loss too large (2372.890->2503.905)! Learning rate decreased to 0.00314.
2024-12-02-01:10:52-root-INFO: grad norm: 463.374 448.860 115.066
2024-12-02-01:10:53-root-INFO: Loss too large (2349.809->2437.876)! Learning rate decreased to 0.00251.
2024-12-02-01:10:53-root-INFO: Loss Change: 2372.890 -> 2334.137
2024-12-02-01:10:53-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-02-01:10:53-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-01:10:53-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:10:54-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-01:10:54-root-INFO: grad norm: 385.469 366.414 119.695
2024-12-02-01:10:54-root-INFO: Loss too large (2337.421->2478.547)! Learning rate decreased to 0.00328.
2024-12-02-01:10:55-root-INFO: grad norm: 469.774 455.104 116.481
2024-12-02-01:10:56-root-INFO: Loss too large (2308.243->2405.432)! Learning rate decreased to 0.00262.
2024-12-02-01:10:56-root-INFO: Loss Change: 2337.421 -> 2291.349
2024-12-02-01:10:56-root-INFO: Regularization Change: 0.000 -> 0.249
2024-12-02-01:10:56-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-01:10:56-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:10:57-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-01:10:57-root-INFO: grad norm: 373.592 357.141 109.641
2024-12-02-01:10:57-root-INFO: Loss too large (2296.880->2405.274)! Learning rate decreased to 0.00342.
2024-12-02-01:10:58-root-INFO: grad norm: 420.155 407.787 101.192
2024-12-02-01:10:59-root-INFO: Loss too large (2248.960->2322.446)! Learning rate decreased to 0.00273.
2024-12-02-01:10:59-root-INFO: Loss Change: 2296.880 -> 2219.711
2024-12-02-01:10:59-root-INFO: Regularization Change: 0.000 -> 0.239
2024-12-02-01:10:59-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-01:10:59-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:11:00-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-01:11:00-root-INFO: grad norm: 358.737 346.326 93.543
2024-12-02-01:11:00-root-INFO: Loss too large (2217.403->2414.111)! Learning rate decreased to 0.00356.
2024-12-02-01:11:01-root-INFO: Loss too large (2217.403->2236.074)! Learning rate decreased to 0.00285.
2024-12-02-01:11:02-root-INFO: grad norm: 327.921 320.572 69.036
2024-12-02-01:11:02-root-INFO: Loss too large (2139.857->2154.250)! Learning rate decreased to 0.00228.
2024-12-02-01:11:03-root-INFO: Loss Change: 2217.403 -> 2112.741
2024-12-02-01:11:03-root-INFO: Regularization Change: 0.000 -> 0.274
2024-12-02-01:11:03-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-01:11:03-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-01:11:03-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-01:11:03-root-INFO: grad norm: 254.470 249.294 51.063
2024-12-02-01:11:04-root-INFO: Loss too large (2100.554->2306.012)! Learning rate decreased to 0.00371.
2024-12-02-01:11:04-root-INFO: Loss too large (2100.554->2174.939)! Learning rate decreased to 0.00297.
2024-12-02-01:11:05-root-INFO: grad norm: 406.839 402.005 62.531
2024-12-02-01:11:05-root-INFO: Loss too large (2095.899->2172.054)! Learning rate decreased to 0.00238.
2024-12-02-01:11:06-root-INFO: Loss too large (2095.899->2124.479)! Learning rate decreased to 0.00190.
2024-12-02-01:11:06-root-INFO: Loss Change: 2100.554 -> 2083.584
2024-12-02-01:11:06-root-INFO: Regularization Change: 0.000 -> 0.190
2024-12-02-01:11:06-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-01:11:06-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:11:07-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-01:11:07-root-INFO: grad norm: 260.930 256.941 45.446
2024-12-02-01:11:07-root-INFO: Loss too large (2071.892->2314.943)! Learning rate decreased to 0.00387.
2024-12-02-01:11:08-root-INFO: Loss too large (2071.892->2167.479)! Learning rate decreased to 0.00309.
2024-12-02-01:11:09-root-INFO: grad norm: 466.250 461.917 63.420
2024-12-02-01:11:09-root-INFO: Loss too large (2069.689->2169.672)! Learning rate decreased to 0.00248.
2024-12-02-01:11:09-root-INFO: Loss too large (2069.689->2124.822)! Learning rate decreased to 0.00198.
2024-12-02-01:11:10-root-INFO: Loss too large (2069.689->2078.924)! Learning rate decreased to 0.00158.
2024-12-02-01:11:10-root-INFO: Loss Change: 2071.892 -> 2037.808
2024-12-02-01:11:10-root-INFO: Regularization Change: 0.000 -> 0.137
2024-12-02-01:11:10-root-INFO: Undo step: 180
2024-12-02-01:11:10-root-INFO: Undo step: 181
2024-12-02-01:11:10-root-INFO: Undo step: 182
2024-12-02-01:11:10-root-INFO: Undo step: 183
2024-12-02-01:11:10-root-INFO: Undo step: 184
2024-12-02-01:11:11-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-01:11:11-root-INFO: grad norm: 613.908 585.638 184.148
2024-12-02-01:11:12-root-INFO: grad norm: 540.950 518.043 155.751
2024-12-02-01:11:12-root-INFO: Loss too large (2668.768->2695.268)! Learning rate decreased to 0.00314.
2024-12-02-01:11:13-root-INFO: Loss Change: 3357.778 -> 2551.936
2024-12-02-01:11:13-root-INFO: Regularization Change: 0.000 -> 8.012
2024-12-02-01:11:13-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-01:11:13-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:11:13-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-01:11:14-root-INFO: grad norm: 483.980 470.343 114.080
2024-12-02-01:11:15-root-INFO: grad norm: 403.098 390.449 100.189
2024-12-02-01:11:15-root-INFO: Loss Change: 2579.773 -> 2411.340
2024-12-02-01:11:15-root-INFO: Regularization Change: 0.000 -> 1.285
2024-12-02-01:11:15-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-01:11:15-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:11:16-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-01:11:16-root-INFO: grad norm: 319.451 310.419 75.426
2024-12-02-01:11:17-root-INFO: grad norm: 270.791 262.490 66.533
2024-12-02-01:11:17-root-INFO: Loss too large (2261.725->2284.547)! Learning rate decreased to 0.00342.
2024-12-02-01:11:18-root-INFO: Loss Change: 2413.363 -> 2238.803
2024-12-02-01:11:18-root-INFO: Regularization Change: 0.000 -> 0.989
2024-12-02-01:11:18-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-01:11:18-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:11:18-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-01:11:18-root-INFO: grad norm: 263.777 256.861 60.007
2024-12-02-01:11:19-root-INFO: Loss too large (2231.486->2268.786)! Learning rate decreased to 0.00356.
2024-12-02-01:11:20-root-INFO: grad norm: 344.492 336.823 72.282
2024-12-02-01:11:20-root-INFO: Loss too large (2186.841->2241.764)! Learning rate decreased to 0.00285.
2024-12-02-01:11:20-root-INFO: Loss too large (2186.841->2193.977)! Learning rate decreased to 0.00228.
2024-12-02-01:11:21-root-INFO: Loss Change: 2231.486 -> 2159.820
2024-12-02-01:11:21-root-INFO: Regularization Change: 0.000 -> 0.392
2024-12-02-01:11:21-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-01:11:21-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-01:11:21-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-01:11:22-root-INFO: grad norm: 219.298 213.953 48.121
2024-12-02-01:11:22-root-INFO: Loss too large (2145.720->2203.245)! Learning rate decreased to 0.00371.
2024-12-02-01:11:23-root-INFO: grad norm: 369.979 363.126 70.882
2024-12-02-01:11:23-root-INFO: Loss too large (2124.781->2212.369)! Learning rate decreased to 0.00297.
2024-12-02-01:11:24-root-INFO: Loss too large (2124.781->2163.214)! Learning rate decreased to 0.00238.
2024-12-02-01:11:24-root-INFO: Loss Change: 2145.720 -> 2122.090
2024-12-02-01:11:24-root-INFO: Regularization Change: 0.000 -> 0.321
2024-12-02-01:11:24-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-01:11:24-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:11:25-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-01:11:25-root-INFO: grad norm: 236.598 230.572 53.058
2024-12-02-01:11:25-root-INFO: Loss too large (2106.350->2159.475)! Learning rate decreased to 0.00387.
2024-12-02-01:11:26-root-INFO: grad norm: 373.929 367.297 70.114
2024-12-02-01:11:27-root-INFO: Loss too large (2065.729->2170.931)! Learning rate decreased to 0.00309.
2024-12-02-01:11:27-root-INFO: Loss too large (2065.729->2118.796)! Learning rate decreased to 0.00248.
2024-12-02-01:11:27-root-INFO: Loss too large (2065.729->2072.911)! Learning rate decreased to 0.00198.
2024-12-02-01:11:28-root-INFO: Loss Change: 2106.350 -> 2037.396
2024-12-02-01:11:28-root-INFO: Regularization Change: 0.000 -> 0.272
2024-12-02-01:11:28-root-INFO: Undo step: 180
2024-12-02-01:11:28-root-INFO: Undo step: 181
2024-12-02-01:11:28-root-INFO: Undo step: 182
2024-12-02-01:11:28-root-INFO: Undo step: 183
2024-12-02-01:11:28-root-INFO: Undo step: 184
2024-12-02-01:11:28-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-01:11:29-root-INFO: grad norm: 742.822 687.979 280.126
2024-12-02-01:11:30-root-INFO: grad norm: 895.899 872.286 204.332
2024-12-02-01:11:30-root-INFO: Loss too large (2863.813->3221.625)! Learning rate decreased to 0.00314.
2024-12-02-01:11:30-root-INFO: Loss too large (2863.813->2890.778)! Learning rate decreased to 0.00251.
2024-12-02-01:11:31-root-INFO: Loss Change: 3222.155 -> 2670.107
2024-12-02-01:11:31-root-INFO: Regularization Change: 0.000 -> 7.108
2024-12-02-01:11:31-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-01:11:31-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:11:31-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-01:11:32-root-INFO: grad norm: 593.509 571.941 158.548
2024-12-02-01:11:33-root-INFO: grad norm: 313.659 298.039 97.750
2024-12-02-01:11:33-root-INFO: Loss Change: 2715.760 -> 2379.844
2024-12-02-01:11:33-root-INFO: Regularization Change: 0.000 -> 2.689
2024-12-02-01:11:33-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-01:11:33-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:11:34-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-01:11:34-root-INFO: grad norm: 315.534 306.341 75.608
2024-12-02-01:11:35-root-INFO: grad norm: 317.303 308.747 73.188
2024-12-02-01:11:35-root-INFO: Loss too large (2214.723->2293.933)! Learning rate decreased to 0.00342.
2024-12-02-01:11:35-root-INFO: Loss too large (2214.723->2220.848)! Learning rate decreased to 0.00273.
2024-12-02-01:11:36-root-INFO: Loss Change: 2368.340 -> 2174.987
2024-12-02-01:11:36-root-INFO: Regularization Change: 0.000 -> 0.723
2024-12-02-01:11:36-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-01:11:36-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-01:11:36-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-01:11:37-root-INFO: grad norm: 266.029 257.120 68.270
2024-12-02-01:11:37-root-INFO: Loss too large (2158.371->2286.347)! Learning rate decreased to 0.00356.
2024-12-02-01:11:38-root-INFO: Loss too large (2158.371->2179.048)! Learning rate decreased to 0.00285.
2024-12-02-01:11:39-root-INFO: grad norm: 344.796 336.296 76.084
2024-12-02-01:11:39-root-INFO: Loss too large (2119.940->2159.258)! Learning rate decreased to 0.00228.
2024-12-02-01:11:39-root-INFO: Loss too large (2119.940->2123.206)! Learning rate decreased to 0.00182.
2024-12-02-01:11:40-root-INFO: Loss Change: 2158.371 -> 2095.204
2024-12-02-01:11:40-root-INFO: Regularization Change: 0.000 -> 0.307
2024-12-02-01:11:40-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-01:11:40-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-01:11:40-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-01:11:41-root-INFO: grad norm: 218.191 209.523 60.887
2024-12-02-01:11:41-root-INFO: Loss too large (2071.066->2228.753)! Learning rate decreased to 0.00371.
2024-12-02-01:11:41-root-INFO: Loss too large (2071.066->2134.699)! Learning rate decreased to 0.00297.
2024-12-02-01:11:42-root-INFO: Loss too large (2071.066->2075.169)! Learning rate decreased to 0.00238.
2024-12-02-01:11:43-root-INFO: grad norm: 266.258 258.182 65.082
2024-12-02-01:11:43-root-INFO: Loss too large (2043.045->2060.135)! Learning rate decreased to 0.00190.
2024-12-02-01:11:44-root-INFO: Loss Change: 2071.066 -> 2037.362
2024-12-02-01:11:44-root-INFO: Regularization Change: 0.000 -> 0.160
2024-12-02-01:11:44-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-01:11:44-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:11:44-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-01:11:44-root-INFO: grad norm: 209.655 202.162 55.549
2024-12-02-01:11:45-root-INFO: Loss too large (2014.354->2211.469)! Learning rate decreased to 0.00387.
2024-12-02-01:11:45-root-INFO: Loss too large (2014.354->2114.228)! Learning rate decreased to 0.00309.
2024-12-02-01:11:45-root-INFO: Loss too large (2014.354->2048.260)! Learning rate decreased to 0.00248.
2024-12-02-01:11:46-root-INFO: grad norm: 330.004 320.790 77.435
2024-12-02-01:11:46-root-INFO: Loss too large (2008.759->2050.510)! Learning rate decreased to 0.00198.
2024-12-02-01:11:47-root-INFO: Loss too large (2008.759->2016.962)! Learning rate decreased to 0.00158.
2024-12-02-01:11:47-root-INFO: Loss Change: 2014.354 -> 1991.137
2024-12-02-01:11:47-root-INFO: Regularization Change: 0.000 -> 0.137
2024-12-02-01:11:47-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-01:11:47-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:11:48-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-01:11:48-root-INFO: grad norm: 211.169 203.867 55.050
2024-12-02-01:11:48-root-INFO: Loss too large (1969.720->2214.174)! Learning rate decreased to 0.00403.
2024-12-02-01:11:49-root-INFO: Loss too large (1969.720->2108.200)! Learning rate decreased to 0.00322.
2024-12-02-01:11:49-root-INFO: Loss too large (1969.720->2032.203)! Learning rate decreased to 0.00258.
2024-12-02-01:11:49-root-INFO: Loss too large (1969.720->1983.184)! Learning rate decreased to 0.00206.
2024-12-02-01:11:50-root-INFO: grad norm: 265.445 257.433 64.722
2024-12-02-01:11:51-root-INFO: Loss too large (1955.506->1969.919)! Learning rate decreased to 0.00165.
2024-12-02-01:11:51-root-INFO: Loss Change: 1969.720 -> 1948.461
2024-12-02-01:11:51-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-01:11:51-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-01:11:51-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:11:52-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-01:11:52-root-INFO: grad norm: 197.797 191.608 49.094
2024-12-02-01:11:52-root-INFO: Loss too large (1925.801->2170.730)! Learning rate decreased to 0.00420.
2024-12-02-01:11:53-root-INFO: Loss too large (1925.801->2072.046)! Learning rate decreased to 0.00336.
2024-12-02-01:11:53-root-INFO: Loss too large (1925.801->1999.747)! Learning rate decreased to 0.00269.
2024-12-02-01:11:53-root-INFO: Loss too large (1925.801->1951.534)! Learning rate decreased to 0.00215.
2024-12-02-01:11:54-root-INFO: grad norm: 292.108 284.158 67.684
2024-12-02-01:11:55-root-INFO: Loss too large (1922.828->1950.018)! Learning rate decreased to 0.00172.
2024-12-02-01:11:55-root-INFO: Loss Change: 1925.801 -> 1922.279
2024-12-02-01:11:55-root-INFO: Regularization Change: 0.000 -> 0.112
2024-12-02-01:11:55-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-01:11:55-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:11:56-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-01:11:56-root-INFO: grad norm: 237.023 229.045 60.978
2024-12-02-01:11:56-root-INFO: Loss too large (1904.848->2257.569)! Learning rate decreased to 0.00437.
2024-12-02-01:11:57-root-INFO: Loss too large (1904.848->2128.221)! Learning rate decreased to 0.00350.
2024-12-02-01:11:57-root-INFO: Loss too large (1904.848->2027.543)! Learning rate decreased to 0.00280.
2024-12-02-01:11:57-root-INFO: Loss too large (1904.848->1954.977)! Learning rate decreased to 0.00224.
2024-12-02-01:11:58-root-INFO: Loss too large (1904.848->1908.204)! Learning rate decreased to 0.00179.
2024-12-02-01:11:59-root-INFO: grad norm: 238.485 231.139 58.735
2024-12-02-01:11:59-root-INFO: Loss too large (1882.185->1885.065)! Learning rate decreased to 0.00143.
2024-12-02-01:11:59-root-INFO: Loss Change: 1904.848 -> 1870.404
2024-12-02-01:12:00-root-INFO: Regularization Change: 0.000 -> 0.069
2024-12-02-01:12:00-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-01:12:00-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-01:12:00-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-01:12:00-root-INFO: grad norm: 141.125 137.253 32.831
2024-12-02-01:12:01-root-INFO: Loss too large (1851.911->1994.833)! Learning rate decreased to 0.00455.
2024-12-02-01:12:01-root-INFO: Loss too large (1851.911->1934.640)! Learning rate decreased to 0.00364.
2024-12-02-01:12:01-root-INFO: Loss too large (1851.911->1893.077)! Learning rate decreased to 0.00291.
2024-12-02-01:12:01-root-INFO: Loss too large (1851.911->1866.699)! Learning rate decreased to 0.00233.
2024-12-02-01:12:02-root-INFO: grad norm: 251.831 244.930 58.549
2024-12-02-01:12:03-root-INFO: Loss too large (1851.414->1891.644)! Learning rate decreased to 0.00186.
2024-12-02-01:12:03-root-INFO: Loss too large (1851.414->1863.617)! Learning rate decreased to 0.00149.
2024-12-02-01:12:04-root-INFO: Loss Change: 1851.911 -> 1844.260
2024-12-02-01:12:04-root-INFO: Regularization Change: 0.000 -> 0.102
2024-12-02-01:12:04-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-01:12:04-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:12:04-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-01:12:05-root-INFO: grad norm: 174.634 170.070 39.662
2024-12-02-01:12:05-root-INFO: Loss too large (1822.656->2077.225)! Learning rate decreased to 0.00474.
2024-12-02-01:12:05-root-INFO: Loss too large (1822.656->1989.133)! Learning rate decreased to 0.00379.
2024-12-02-01:12:06-root-INFO: Loss too large (1822.656->1921.591)! Learning rate decreased to 0.00303.
2024-12-02-01:12:06-root-INFO: Loss too large (1822.656->1873.251)! Learning rate decreased to 0.00243.
2024-12-02-01:12:06-root-INFO: Loss too large (1822.656->1841.435)! Learning rate decreased to 0.00194.
2024-12-02-01:12:07-root-INFO: grad norm: 256.130 249.595 57.491
2024-12-02-01:12:08-root-INFO: Loss too large (1822.399->1840.459)! Learning rate decreased to 0.00155.
2024-12-02-01:12:08-root-INFO: Loss Change: 1822.656 -> 1818.101
2024-12-02-01:12:08-root-INFO: Regularization Change: 0.000 -> 0.079
2024-12-02-01:12:08-root-INFO: Undo step: 175
2024-12-02-01:12:08-root-INFO: Undo step: 176
2024-12-02-01:12:08-root-INFO: Undo step: 177
2024-12-02-01:12:08-root-INFO: Undo step: 178
2024-12-02-01:12:08-root-INFO: Undo step: 179
2024-12-02-01:12:09-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-01:12:09-root-INFO: grad norm: 823.107 764.858 304.135
2024-12-02-01:12:09-root-INFO: Loss too large (3178.222->3484.438)! Learning rate decreased to 0.00387.
2024-12-02-01:12:10-root-INFO: grad norm: 746.758 729.034 161.731
2024-12-02-01:12:11-root-INFO: Loss Change: 3178.222 -> 2426.301
2024-12-02-01:12:11-root-INFO: Regularization Change: 0.000 -> 7.508
2024-12-02-01:12:11-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-01:12:11-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:12:11-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-01:12:11-root-INFO: grad norm: 621.710 608.362 128.135
2024-12-02-01:12:12-root-INFO: Loss too large (2396.123->2903.344)! Learning rate decreased to 0.00403.
2024-12-02-01:12:12-root-INFO: Loss too large (2396.123->2581.692)! Learning rate decreased to 0.00322.
2024-12-02-01:12:13-root-INFO: grad norm: 666.578 649.293 150.814
2024-12-02-01:12:14-root-INFO: Loss Change: 2396.123 -> 2261.798
2024-12-02-01:12:14-root-INFO: Regularization Change: 0.000 -> 2.972
2024-12-02-01:12:14-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-01:12:14-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:12:14-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-01:12:14-root-INFO: grad norm: 293.490 282.853 78.298
2024-12-02-01:12:15-root-INFO: grad norm: 424.128 418.730 67.449
2024-12-02-01:12:16-root-INFO: Loss too large (2207.223->2248.820)! Learning rate decreased to 0.00420.
2024-12-02-01:12:16-root-INFO: Loss Change: 2226.000 -> 2076.081
2024-12-02-01:12:16-root-INFO: Regularization Change: 0.000 -> 2.587
2024-12-02-01:12:16-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-01:12:17-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:12:17-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-01:12:17-root-INFO: grad norm: 379.018 373.157 66.393
2024-12-02-01:12:18-root-INFO: Loss too large (2062.660->2136.535)! Learning rate decreased to 0.00437.
2024-12-02-01:12:19-root-INFO: grad norm: 353.125 347.433 63.145
2024-12-02-01:12:19-root-INFO: Loss too large (2011.072->2021.639)! Learning rate decreased to 0.00350.
2024-12-02-01:12:20-root-INFO: Loss Change: 2062.660 -> 1924.159
2024-12-02-01:12:20-root-INFO: Regularization Change: 0.000 -> 1.169
2024-12-02-01:12:20-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-01:12:20-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-01:12:20-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-01:12:20-root-INFO: grad norm: 500.001 488.863 104.950
2024-12-02-01:12:21-root-INFO: Loss too large (1957.324->2106.748)! Learning rate decreased to 0.00455.
2024-12-02-01:12:21-root-INFO: Loss too large (1957.324->2063.633)! Learning rate decreased to 0.00364.
2024-12-02-01:12:21-root-INFO: Loss too large (1957.324->2020.765)! Learning rate decreased to 0.00291.
2024-12-02-01:12:22-root-INFO: Loss too large (1957.324->1974.882)! Learning rate decreased to 0.00233.
2024-12-02-01:12:23-root-INFO: grad norm: 233.584 225.203 62.010
2024-12-02-01:12:23-root-INFO: Loss Change: 1957.324 -> 1819.769
2024-12-02-01:12:23-root-INFO: Regularization Change: 0.000 -> 0.692
2024-12-02-01:12:23-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-01:12:23-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:12:23-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-01:12:24-root-INFO: grad norm: 138.431 132.357 40.555
2024-12-02-01:12:25-root-INFO: grad norm: 389.911 380.849 83.573
2024-12-02-01:12:25-root-INFO: Loss too large (1804.703->1979.369)! Learning rate decreased to 0.00474.
2024-12-02-01:12:25-root-INFO: Loss too large (1804.703->1953.819)! Learning rate decreased to 0.00379.
2024-12-02-01:12:26-root-INFO: Loss too large (1804.703->1922.404)! Learning rate decreased to 0.00303.
2024-12-02-01:12:26-root-INFO: Loss too large (1804.703->1885.291)! Learning rate decreased to 0.00243.
2024-12-02-01:12:26-root-INFO: Loss too large (1804.703->1845.135)! Learning rate decreased to 0.00194.
2024-12-02-01:12:27-root-INFO: Loss too large (1804.703->1806.503)! Learning rate decreased to 0.00155.
2024-12-02-01:12:28-root-INFO: Loss Change: 1806.381 -> 1774.649
2024-12-02-01:12:28-root-INFO: Regularization Change: 0.000 -> 0.544
2024-12-02-01:12:28-root-INFO: Undo step: 175
2024-12-02-01:12:28-root-INFO: Undo step: 176
2024-12-02-01:12:28-root-INFO: Undo step: 177
2024-12-02-01:12:28-root-INFO: Undo step: 178
2024-12-02-01:12:28-root-INFO: Undo step: 179
2024-12-02-01:12:28-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-01:12:28-root-INFO: grad norm: 584.162 562.979 155.882
2024-12-02-01:12:29-root-INFO: grad norm: 680.722 660.444 164.913
2024-12-02-01:12:30-root-INFO: Loss too large (2545.001->2750.209)! Learning rate decreased to 0.00387.
2024-12-02-01:12:30-root-INFO: Loss Change: 2771.695 -> 2398.758
2024-12-02-01:12:30-root-INFO: Regularization Change: 0.000 -> 4.417
2024-12-02-01:12:30-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-01:12:30-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:12:31-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-01:12:31-root-INFO: grad norm: 562.573 551.673 110.206
2024-12-02-01:12:31-root-INFO: Loss too large (2376.579->2538.229)! Learning rate decreased to 0.00403.
2024-12-02-01:12:32-root-INFO: grad norm: 564.107 543.309 151.763
2024-12-02-01:12:33-root-INFO: Loss Change: 2376.579 -> 2199.147
2024-12-02-01:12:33-root-INFO: Regularization Change: 0.000 -> 2.369
2024-12-02-01:12:33-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-01:12:33-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:12:33-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-01:12:33-root-INFO: grad norm: 340.383 328.341 89.735
2024-12-02-01:12:34-root-INFO: grad norm: 420.212 401.036 125.490
2024-12-02-01:12:35-root-INFO: Loss too large (2142.934->2171.720)! Learning rate decreased to 0.00420.
2024-12-02-01:12:35-root-INFO: Loss Change: 2174.967 -> 1988.057
2024-12-02-01:12:35-root-INFO: Regularization Change: 0.000 -> 2.586
2024-12-02-01:12:35-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-01:12:35-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-01:12:36-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-01:12:36-root-INFO: grad norm: 444.424 431.212 107.558
2024-12-02-01:12:36-root-INFO: Loss too large (2004.695->2275.226)! Learning rate decreased to 0.00437.
2024-12-02-01:12:37-root-INFO: Loss too large (2004.695->2104.074)! Learning rate decreased to 0.00350.
2024-12-02-01:12:38-root-INFO: grad norm: 352.458 334.107 112.244
2024-12-02-01:12:38-root-INFO: Loss Change: 2004.695 -> 1847.447
2024-12-02-01:12:38-root-INFO: Regularization Change: 0.000 -> 0.698
2024-12-02-01:12:38-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-01:12:38-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-01:12:39-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-01:12:39-root-INFO: grad norm: 408.346 386.053 133.078
2024-12-02-01:12:39-root-INFO: Loss too large (1874.430->2119.306)! Learning rate decreased to 0.00455.
2024-12-02-01:12:40-root-INFO: Loss too large (1874.430->2030.860)! Learning rate decreased to 0.00364.
2024-12-02-01:12:40-root-INFO: Loss too large (1874.430->1951.125)! Learning rate decreased to 0.00291.
2024-12-02-01:12:40-root-INFO: Loss too large (1874.430->1880.624)! Learning rate decreased to 0.00233.
2024-12-02-01:12:41-root-INFO: grad norm: 268.440 252.801 90.286
2024-12-02-01:12:42-root-INFO: Loss Change: 1874.430 -> 1750.484
2024-12-02-01:12:42-root-INFO: Regularization Change: 0.000 -> 0.302
2024-12-02-01:12:42-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-01:12:42-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:12:42-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-01:12:43-root-INFO: grad norm: 242.507 229.388 78.682
2024-12-02-01:12:43-root-INFO: Loss too large (1756.337->1933.446)! Learning rate decreased to 0.00474.
2024-12-02-01:12:43-root-INFO: Loss too large (1756.337->1873.278)! Learning rate decreased to 0.00379.
2024-12-02-01:12:44-root-INFO: Loss too large (1756.337->1818.179)! Learning rate decreased to 0.00303.
2024-12-02-01:12:44-root-INFO: Loss too large (1756.337->1774.042)! Learning rate decreased to 0.00243.
2024-12-02-01:12:45-root-INFO: grad norm: 238.578 224.098 81.850
2024-12-02-01:12:46-root-INFO: Loss Change: 1756.337 -> 1726.462
2024-12-02-01:12:46-root-INFO: Regularization Change: 0.000 -> 0.173
2024-12-02-01:12:46-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-01:12:46-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:12:46-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-01:12:46-root-INFO: grad norm: 295.296 278.848 97.177
2024-12-02-01:12:47-root-INFO: Loss too large (1735.210->1942.423)! Learning rate decreased to 0.00494.
2024-12-02-01:12:47-root-INFO: Loss too large (1735.210->1892.154)! Learning rate decreased to 0.00395.
2024-12-02-01:12:47-root-INFO: Loss too large (1735.210->1838.651)! Learning rate decreased to 0.00316.
2024-12-02-01:12:48-root-INFO: Loss too large (1735.210->1785.550)! Learning rate decreased to 0.00253.
2024-12-02-01:12:48-root-INFO: Loss too large (1735.210->1740.026)! Learning rate decreased to 0.00202.
2024-12-02-01:12:49-root-INFO: grad norm: 228.130 215.306 75.411
2024-12-02-01:12:50-root-INFO: Loss Change: 1735.210 -> 1680.566
2024-12-02-01:12:50-root-INFO: Regularization Change: 0.000 -> 0.113
2024-12-02-01:12:50-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-01:12:50-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:12:50-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-01:12:50-root-INFO: grad norm: 275.417 260.138 90.459
2024-12-02-01:12:51-root-INFO: Loss too large (1697.649->1910.409)! Learning rate decreased to 0.00514.
2024-12-02-01:12:51-root-INFO: Loss too large (1697.649->1858.254)! Learning rate decreased to 0.00411.
2024-12-02-01:12:51-root-INFO: Loss too large (1697.649->1803.670)! Learning rate decreased to 0.00329.
2024-12-02-01:12:52-root-INFO: Loss too large (1697.649->1750.696)! Learning rate decreased to 0.00263.
2024-12-02-01:12:52-root-INFO: Loss too large (1697.649->1706.324)! Learning rate decreased to 0.00210.
2024-12-02-01:12:53-root-INFO: grad norm: 232.857 220.925 73.583
2024-12-02-01:12:53-root-INFO: Loss Change: 1697.649 -> 1652.978
2024-12-02-01:12:53-root-INFO: Regularization Change: 0.000 -> 0.124
2024-12-02-01:12:53-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-01:12:54-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-01:12:54-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-01:12:54-root-INFO: grad norm: 250.055 236.796 80.343
2024-12-02-01:12:54-root-INFO: Loss too large (1658.024->1864.462)! Learning rate decreased to 0.00535.
2024-12-02-01:12:55-root-INFO: Loss too large (1658.024->1817.836)! Learning rate decreased to 0.00428.
2024-12-02-01:12:55-root-INFO: Loss too large (1658.024->1767.338)! Learning rate decreased to 0.00342.
2024-12-02-01:12:56-root-INFO: Loss too large (1658.024->1717.300)! Learning rate decreased to 0.00274.
2024-12-02-01:12:56-root-INFO: Loss too large (1658.024->1674.903)! Learning rate decreased to 0.00219.
2024-12-02-01:12:57-root-INFO: grad norm: 233.329 222.353 70.720
2024-12-02-01:12:58-root-INFO: Loss Change: 1658.024 -> 1628.668
2024-12-02-01:12:58-root-INFO: Regularization Change: 0.000 -> 0.102
2024-12-02-01:12:58-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-01:12:58-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:12:58-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-01:12:58-root-INFO: grad norm: 279.405 265.693 86.454
2024-12-02-01:12:59-root-INFO: Loss too large (1641.676->1859.074)! Learning rate decreased to 0.00556.
2024-12-02-01:12:59-root-INFO: Loss too large (1641.676->1815.264)! Learning rate decreased to 0.00445.
2024-12-02-01:12:59-root-INFO: Loss too large (1641.676->1765.721)! Learning rate decreased to 0.00356.
2024-12-02-01:13:00-root-INFO: Loss too large (1641.676->1712.129)! Learning rate decreased to 0.00285.
2024-12-02-01:13:00-root-INFO: Loss too large (1641.676->1661.956)! Learning rate decreased to 0.00228.
2024-12-02-01:13:01-root-INFO: grad norm: 252.984 241.989 73.771
2024-12-02-01:13:01-root-INFO: Loss Change: 1641.676 -> 1605.995
2024-12-02-01:13:01-root-INFO: Regularization Change: 0.000 -> 0.108
2024-12-02-01:13:01-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-01:13:02-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:13:02-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-01:13:02-root-INFO: grad norm: 282.102 268.376 86.924
2024-12-02-01:13:02-root-INFO: Loss too large (1622.830->1841.922)! Learning rate decreased to 0.00579.
2024-12-02-01:13:03-root-INFO: Loss too large (1622.830->1795.930)! Learning rate decreased to 0.00463.
2024-12-02-01:13:03-root-INFO: Loss too large (1622.830->1744.230)! Learning rate decreased to 0.00370.
2024-12-02-01:13:03-root-INFO: Loss too large (1622.830->1687.969)! Learning rate decreased to 0.00296.
2024-12-02-01:13:04-root-INFO: Loss too large (1622.830->1635.294)! Learning rate decreased to 0.00237.
2024-12-02-01:13:05-root-INFO: grad norm: 247.128 237.158 69.488
2024-12-02-01:13:05-root-INFO: Loss Change: 1622.830 -> 1580.541
2024-12-02-01:13:05-root-INFO: Regularization Change: 0.000 -> 0.128
2024-12-02-01:13:06-root-INFO: Undo step: 170
2024-12-02-01:13:06-root-INFO: Undo step: 171
2024-12-02-01:13:06-root-INFO: Undo step: 172
2024-12-02-01:13:06-root-INFO: Undo step: 173
2024-12-02-01:13:06-root-INFO: Undo step: 174
2024-12-02-01:13:06-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-01:13:06-root-INFO: grad norm: 616.862 595.813 159.768
2024-12-02-01:13:07-root-INFO: grad norm: 473.652 464.177 94.266
2024-12-02-01:13:08-root-INFO: Loss too large (2262.077->2426.414)! Learning rate decreased to 0.00474.
2024-12-02-01:13:08-root-INFO: Loss Change: 2643.811 -> 2168.823
2024-12-02-01:13:08-root-INFO: Regularization Change: 0.000 -> 8.395
2024-12-02-01:13:08-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-01:13:08-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:13:09-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-01:13:09-root-INFO: grad norm: 692.390 674.827 154.958
2024-12-02-01:13:09-root-INFO: Loss too large (2203.397->2270.893)! Learning rate decreased to 0.00494.
2024-12-02-01:13:10-root-INFO: grad norm: 263.175 255.368 63.626
2024-12-02-01:13:11-root-INFO: Loss Change: 2203.397 -> 1991.185
2024-12-02-01:13:11-root-INFO: Regularization Change: 0.000 -> 9.629
2024-12-02-01:13:11-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-01:13:11-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:13:11-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-01:13:11-root-INFO: grad norm: 158.420 152.552 42.719
2024-12-02-01:13:12-root-INFO: grad norm: 210.693 205.460 46.665
2024-12-02-01:13:13-root-INFO: Loss Change: 1964.591 -> 1818.358
2024-12-02-01:13:13-root-INFO: Regularization Change: 0.000 -> 2.578
2024-12-02-01:13:13-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-01:13:13-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-01:13:13-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-01:13:14-root-INFO: grad norm: 248.493 240.271 63.396
2024-12-02-01:13:15-root-INFO: grad norm: 289.307 281.854 65.246
2024-12-02-01:13:15-root-INFO: Loss Change: 1808.761 -> 1719.925
2024-12-02-01:13:15-root-INFO: Regularization Change: 0.000 -> 2.064
2024-12-02-01:13:15-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-01:13:15-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:13:16-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-01:13:16-root-INFO: grad norm: 272.050 264.363 64.214
2024-12-02-01:13:17-root-INFO: grad norm: 285.637 275.156 76.666
2024-12-02-01:13:17-root-INFO: Loss too large (1649.438->1781.515)! Learning rate decreased to 0.00556.
2024-12-02-01:13:18-root-INFO: Loss too large (1649.438->1676.075)! Learning rate decreased to 0.00445.
2024-12-02-01:13:18-root-INFO: Loss Change: 1695.031 -> 1611.508
2024-12-02-01:13:18-root-INFO: Regularization Change: 0.000 -> 1.240
2024-12-02-01:13:18-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-01:13:18-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:13:19-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-01:13:19-root-INFO: grad norm: 336.273 325.055 86.129
2024-12-02-01:13:19-root-INFO: Loss too large (1596.723->2209.785)! Learning rate decreased to 0.00579.
2024-12-02-01:13:20-root-INFO: Loss too large (1596.723->2033.514)! Learning rate decreased to 0.00463.
2024-12-02-01:13:20-root-INFO: Loss too large (1596.723->1896.215)! Learning rate decreased to 0.00370.
2024-12-02-01:13:20-root-INFO: Loss too large (1596.723->1784.440)! Learning rate decreased to 0.00296.
2024-12-02-01:13:21-root-INFO: Loss too large (1596.723->1692.551)! Learning rate decreased to 0.00237.
2024-12-02-01:13:21-root-INFO: Loss too large (1596.723->1619.986)! Learning rate decreased to 0.00190.
2024-12-02-01:13:22-root-INFO: grad norm: 365.453 354.443 89.028
2024-12-02-01:13:22-root-INFO: Loss too large (1568.150->1581.405)! Learning rate decreased to 0.00152.
2024-12-02-01:13:23-root-INFO: Loss Change: 1596.723 -> 1542.599
2024-12-02-01:13:23-root-INFO: Regularization Change: 0.000 -> 0.153
2024-12-02-01:13:23-root-INFO: Undo step: 170
2024-12-02-01:13:23-root-INFO: Undo step: 171
2024-12-02-01:13:23-root-INFO: Undo step: 172
2024-12-02-01:13:23-root-INFO: Undo step: 173
2024-12-02-01:13:23-root-INFO: Undo step: 174
2024-12-02-01:13:23-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-01:13:24-root-INFO: grad norm: 572.922 542.981 182.787
2024-12-02-01:13:25-root-INFO: grad norm: 720.106 684.123 224.785
2024-12-02-01:13:25-root-INFO: Loss too large (2443.765->2786.341)! Learning rate decreased to 0.00474.
2024-12-02-01:13:26-root-INFO: Loss Change: 2724.510 -> 2362.039
2024-12-02-01:13:26-root-INFO: Regularization Change: 0.000 -> 10.267
2024-12-02-01:13:26-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-01:13:26-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:13:26-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-01:13:26-root-INFO: grad norm: 613.148 595.486 146.106
2024-12-02-01:13:27-root-INFO: Loss too large (2337.362->2598.622)! Learning rate decreased to 0.00494.
2024-12-02-01:13:27-root-INFO: Loss too large (2337.362->2347.845)! Learning rate decreased to 0.00395.
2024-12-02-01:13:28-root-INFO: grad norm: 360.879 354.319 68.495
2024-12-02-01:13:29-root-INFO: Loss Change: 2337.362 -> 1886.227
2024-12-02-01:13:29-root-INFO: Regularization Change: 0.000 -> 2.939
2024-12-02-01:13:29-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-01:13:29-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-01:13:29-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-01:13:29-root-INFO: grad norm: 226.837 223.162 40.669
2024-12-02-01:13:30-root-INFO: grad norm: 218.188 210.517 57.347
2024-12-02-01:13:31-root-INFO: Loss too large (1743.071->1847.802)! Learning rate decreased to 0.00514.
2024-12-02-01:13:31-root-INFO: Loss too large (1743.071->1755.947)! Learning rate decreased to 0.00411.
2024-12-02-01:13:32-root-INFO: Loss Change: 1887.844 -> 1706.460
2024-12-02-01:13:32-root-INFO: Regularization Change: 0.000 -> 2.614
2024-12-02-01:13:32-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-01:13:32-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-01:13:32-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-01:13:32-root-INFO: grad norm: 322.903 309.708 91.364
2024-12-02-01:13:33-root-INFO: Loss too large (1716.052->1956.200)! Learning rate decreased to 0.00535.
2024-12-02-01:13:33-root-INFO: Loss too large (1716.052->1864.573)! Learning rate decreased to 0.00428.
2024-12-02-01:13:33-root-INFO: Loss too large (1716.052->1784.450)! Learning rate decreased to 0.00342.
2024-12-02-01:13:34-root-INFO: Loss too large (1716.052->1719.577)! Learning rate decreased to 0.00274.
2024-12-02-01:13:35-root-INFO: grad norm: 246.712 234.037 78.060
2024-12-02-01:13:35-root-INFO: Loss Change: 1716.052 -> 1622.307
2024-12-02-01:13:35-root-INFO: Regularization Change: 0.000 -> 0.362
2024-12-02-01:13:35-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-01:13:35-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:13:36-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-01:13:36-root-INFO: grad norm: 247.162 236.632 71.372
2024-12-02-01:13:36-root-INFO: Loss too large (1632.838->1806.377)! Learning rate decreased to 0.00556.
2024-12-02-01:13:37-root-INFO: Loss too large (1632.838->1753.171)! Learning rate decreased to 0.00445.
2024-12-02-01:13:37-root-INFO: Loss too large (1632.838->1698.147)! Learning rate decreased to 0.00356.
2024-12-02-01:13:37-root-INFO: Loss too large (1632.838->1648.505)! Learning rate decreased to 0.00285.
2024-12-02-01:13:38-root-INFO: grad norm: 238.536 225.989 76.343
2024-12-02-01:13:39-root-INFO: Loss Change: 1632.838 -> 1598.321
2024-12-02-01:13:39-root-INFO: Regularization Change: 0.000 -> 0.222
2024-12-02-01:13:39-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-01:13:39-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:13:39-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-01:13:40-root-INFO: grad norm: 306.471 294.677 84.202
2024-12-02-01:13:40-root-INFO: Loss too large (1620.853->1808.616)! Learning rate decreased to 0.00579.
2024-12-02-01:13:40-root-INFO: Loss too large (1620.853->1764.343)! Learning rate decreased to 0.00463.
2024-12-02-01:13:41-root-INFO: Loss too large (1620.853->1713.937)! Learning rate decreased to 0.00370.
2024-12-02-01:13:41-root-INFO: Loss too large (1620.853->1659.158)! Learning rate decreased to 0.00296.
2024-12-02-01:13:42-root-INFO: grad norm: 261.669 249.235 79.703
2024-12-02-01:13:43-root-INFO: Loss Change: 1620.853 -> 1559.588
2024-12-02-01:13:43-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-02-01:13:43-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-01:13:43-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:13:43-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-01:13:43-root-INFO: grad norm: 270.211 259.088 76.728
2024-12-02-01:13:44-root-INFO: Loss too large (1575.119->1784.791)! Learning rate decreased to 0.00602.
2024-12-02-01:13:44-root-INFO: Loss too large (1575.119->1732.466)! Learning rate decreased to 0.00482.
2024-12-02-01:13:44-root-INFO: Loss too large (1575.119->1674.072)! Learning rate decreased to 0.00385.
2024-12-02-01:13:45-root-INFO: Loss too large (1575.119->1613.823)! Learning rate decreased to 0.00308.
2024-12-02-01:13:46-root-INFO: grad norm: 267.062 254.468 81.043
2024-12-02-01:13:46-root-INFO: Loss Change: 1575.119 -> 1548.810
2024-12-02-01:13:46-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-02-01:13:46-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-01:13:46-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:13:47-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-01:13:47-root-INFO: grad norm: 285.518 275.006 76.762
2024-12-02-01:13:47-root-INFO: Loss too large (1566.193->1752.110)! Learning rate decreased to 0.00626.
2024-12-02-01:13:47-root-INFO: Loss too large (1566.193->1707.444)! Learning rate decreased to 0.00501.
2024-12-02-01:13:48-root-INFO: Loss too large (1566.193->1652.740)! Learning rate decreased to 0.00401.
2024-12-02-01:13:48-root-INFO: Loss too large (1566.193->1591.168)! Learning rate decreased to 0.00321.
2024-12-02-01:13:49-root-INFO: grad norm: 261.353 249.053 79.234
2024-12-02-01:13:50-root-INFO: Loss Change: 1566.193 -> 1522.932
2024-12-02-01:13:50-root-INFO: Regularization Change: 0.000 -> 0.172
2024-12-02-01:13:50-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-01:13:50-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-01:13:50-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-01:13:50-root-INFO: grad norm: 277.741 266.572 77.969
2024-12-02-01:13:51-root-INFO: Loss too large (1545.917->1744.460)! Learning rate decreased to 0.00651.
2024-12-02-01:13:51-root-INFO: Loss too large (1545.917->1690.015)! Learning rate decreased to 0.00521.
2024-12-02-01:13:51-root-INFO: Loss too large (1545.917->1625.236)! Learning rate decreased to 0.00417.
2024-12-02-01:13:52-root-INFO: Loss too large (1545.917->1555.488)! Learning rate decreased to 0.00333.
2024-12-02-01:13:53-root-INFO: grad norm: 249.960 238.216 75.718
2024-12-02-01:13:53-root-INFO: Loss too large (1498.179->1499.211)! Learning rate decreased to 0.00267.
2024-12-02-01:13:54-root-INFO: Loss Change: 1545.917 -> 1468.492
2024-12-02-01:13:54-root-INFO: Regularization Change: 0.000 -> 0.214
2024-12-02-01:13:54-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-01:13:54-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:13:54-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-01:13:54-root-INFO: grad norm: 213.544 204.762 60.611
2024-12-02-01:13:55-root-INFO: Loss too large (1482.617->1675.022)! Learning rate decreased to 0.00677.
2024-12-02-01:13:55-root-INFO: Loss too large (1482.617->1623.968)! Learning rate decreased to 0.00542.
2024-12-02-01:13:55-root-INFO: Loss too large (1482.617->1563.477)! Learning rate decreased to 0.00433.
2024-12-02-01:13:56-root-INFO: Loss too large (1482.617->1504.691)! Learning rate decreased to 0.00347.
2024-12-02-01:13:57-root-INFO: grad norm: 231.159 220.472 69.476
2024-12-02-01:13:57-root-INFO: Loss too large (1461.902->1473.971)! Learning rate decreased to 0.00277.
2024-12-02-01:13:58-root-INFO: Loss Change: 1482.617 -> 1445.084
2024-12-02-01:13:58-root-INFO: Regularization Change: 0.000 -> 0.153
2024-12-02-01:13:58-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-01:13:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:13:58-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-01:13:58-root-INFO: grad norm: 202.995 194.591 57.802
2024-12-02-01:13:59-root-INFO: Loss too large (1455.182->1646.844)! Learning rate decreased to 0.00704.
2024-12-02-01:13:59-root-INFO: Loss too large (1455.182->1598.058)! Learning rate decreased to 0.00563.
2024-12-02-01:13:59-root-INFO: Loss too large (1455.182->1538.675)! Learning rate decreased to 0.00450.
2024-12-02-01:14:00-root-INFO: Loss too large (1455.182->1480.420)! Learning rate decreased to 0.00360.
2024-12-02-01:14:01-root-INFO: grad norm: 229.862 219.450 68.396
2024-12-02-01:14:01-root-INFO: Loss too large (1437.957->1454.047)! Learning rate decreased to 0.00288.
2024-12-02-01:14:02-root-INFO: Loss Change: 1455.182 -> 1424.721
2024-12-02-01:14:02-root-INFO: Regularization Change: 0.000 -> 0.143
2024-12-02-01:14:02-root-INFO: Undo step: 165
2024-12-02-01:14:02-root-INFO: Undo step: 166
2024-12-02-01:14:02-root-INFO: Undo step: 167
2024-12-02-01:14:02-root-INFO: Undo step: 168
2024-12-02-01:14:02-root-INFO: Undo step: 169
2024-12-02-01:14:02-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-01:14:02-root-INFO: grad norm: 580.719 567.055 125.232
2024-12-02-01:14:03-root-INFO: grad norm: 512.277 499.656 113.012
2024-12-02-01:14:04-root-INFO: Loss too large (2215.573->2537.590)! Learning rate decreased to 0.00579.
2024-12-02-01:14:04-root-INFO: Loss too large (2215.573->2229.397)! Learning rate decreased to 0.00463.
2024-12-02-01:14:05-root-INFO: Loss Change: 2486.440 -> 2024.624
2024-12-02-01:14:05-root-INFO: Regularization Change: 0.000 -> 14.003
2024-12-02-01:14:05-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-01:14:05-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:14:05-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-01:14:05-root-INFO: grad norm: 415.030 405.868 86.724
2024-12-02-01:14:06-root-INFO: Loss too large (2009.311->2057.443)! Learning rate decreased to 0.00602.
2024-12-02-01:14:07-root-INFO: grad norm: 375.911 370.685 62.462
2024-12-02-01:14:07-root-INFO: Loss Change: 2009.311 -> 1675.317
2024-12-02-01:14:07-root-INFO: Regularization Change: 0.000 -> 3.419
2024-12-02-01:14:07-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-01:14:07-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:14:08-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-01:14:08-root-INFO: grad norm: 255.743 250.037 53.720
2024-12-02-01:14:09-root-INFO: grad norm: 246.988 243.470 41.535
2024-12-02-01:14:10-root-INFO: Loss Change: 1668.344 -> 1580.465
2024-12-02-01:14:10-root-INFO: Regularization Change: 0.000 -> 1.887
2024-12-02-01:14:10-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-01:14:10-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-01:14:10-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-01:14:10-root-INFO: grad norm: 279.467 273.511 57.392
2024-12-02-01:14:11-root-INFO: Loss too large (1577.409->1705.385)! Learning rate decreased to 0.00651.
2024-12-02-01:14:11-root-INFO: Loss too large (1577.409->1580.843)! Learning rate decreased to 0.00521.
2024-12-02-01:14:12-root-INFO: grad norm: 280.438 273.470 62.125
2024-12-02-01:14:12-root-INFO: Loss too large (1508.511->1594.477)! Learning rate decreased to 0.00417.
2024-12-02-01:14:12-root-INFO: Loss too large (1508.511->1536.199)! Learning rate decreased to 0.00333.
2024-12-02-01:14:13-root-INFO: Loss Change: 1577.409 -> 1478.338
2024-12-02-01:14:13-root-INFO: Regularization Change: 0.000 -> 1.433
2024-12-02-01:14:13-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-01:14:13-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:14:13-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-01:14:14-root-INFO: grad norm: 290.551 282.966 65.955
2024-12-02-01:14:14-root-INFO: Loss too large (1459.131->1892.110)! Learning rate decreased to 0.00677.
2024-12-02-01:14:14-root-INFO: Loss too large (1459.131->1747.051)! Learning rate decreased to 0.00542.
2024-12-02-01:14:15-root-INFO: Loss too large (1459.131->1636.883)! Learning rate decreased to 0.00433.
2024-12-02-01:14:15-root-INFO: Loss too large (1459.131->1553.861)! Learning rate decreased to 0.00347.
2024-12-02-01:14:15-root-INFO: Loss too large (1459.131->1492.644)! Learning rate decreased to 0.00277.
2024-12-02-01:14:16-root-INFO: grad norm: 223.893 217.760 52.047
2024-12-02-01:14:17-root-INFO: Loss Change: 1459.131 -> 1397.727
2024-12-02-01:14:17-root-INFO: Regularization Change: 0.000 -> 0.168
2024-12-02-01:14:17-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-01:14:17-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:14:17-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-01:14:18-root-INFO: grad norm: 115.715 111.241 31.865
2024-12-02-01:14:18-root-INFO: Loss too large (1388.358->1479.303)! Learning rate decreased to 0.00704.
2024-12-02-01:14:18-root-INFO: Loss too large (1388.358->1441.645)! Learning rate decreased to 0.00563.
2024-12-02-01:14:19-root-INFO: Loss too large (1388.358->1415.292)! Learning rate decreased to 0.00450.
2024-12-02-01:14:19-root-INFO: Loss too large (1388.358->1397.779)! Learning rate decreased to 0.00360.
2024-12-02-01:14:20-root-INFO: grad norm: 148.303 144.127 34.946
2024-12-02-01:14:20-root-INFO: Loss too large (1386.899->1387.941)! Learning rate decreased to 0.00288.
2024-12-02-01:14:21-root-INFO: Loss Change: 1388.358 -> 1372.091
2024-12-02-01:14:21-root-INFO: Regularization Change: 0.000 -> 0.144
2024-12-02-01:14:21-root-INFO: Undo step: 165
2024-12-02-01:14:21-root-INFO: Undo step: 166
2024-12-02-01:14:21-root-INFO: Undo step: 167
2024-12-02-01:14:21-root-INFO: Undo step: 168
2024-12-02-01:14:21-root-INFO: Undo step: 169
2024-12-02-01:14:21-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-01:14:22-root-INFO: grad norm: 433.601 417.942 115.475
2024-12-02-01:14:23-root-INFO: grad norm: 386.206 378.694 75.802
2024-12-02-01:14:23-root-INFO: Loss Change: 2243.142 -> 1863.738
2024-12-02-01:14:23-root-INFO: Regularization Change: 0.000 -> 8.951
2024-12-02-01:14:23-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-01:14:23-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:14:24-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-01:14:24-root-INFO: grad norm: 412.562 405.972 73.445
2024-12-02-01:14:24-root-INFO: Loss too large (1850.427->2117.160)! Learning rate decreased to 0.00602.
2024-12-02-01:14:25-root-INFO: Loss too large (1850.427->1902.896)! Learning rate decreased to 0.00482.
2024-12-02-01:14:26-root-INFO: grad norm: 372.119 365.104 71.914
2024-12-02-01:14:26-root-INFO: Loss Change: 1850.427 -> 1660.047
2024-12-02-01:14:26-root-INFO: Regularization Change: 0.000 -> 2.367
2024-12-02-01:14:26-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-01:14:26-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-01:14:27-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-01:14:27-root-INFO: grad norm: 339.008 330.018 77.553
2024-12-02-01:14:27-root-INFO: Loss too large (1643.382->2034.540)! Learning rate decreased to 0.00626.
2024-12-02-01:14:28-root-INFO: Loss too large (1643.382->1810.539)! Learning rate decreased to 0.00501.
2024-12-02-01:14:28-root-INFO: Loss too large (1643.382->1662.571)! Learning rate decreased to 0.00401.
2024-12-02-01:14:29-root-INFO: grad norm: 275.548 269.428 57.750
2024-12-02-01:14:30-root-INFO: Loss Change: 1643.382 -> 1514.417
2024-12-02-01:14:30-root-INFO: Regularization Change: 0.000 -> 0.492
2024-12-02-01:14:30-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-01:14:30-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-01:14:30-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-01:14:30-root-INFO: grad norm: 243.608 236.215 59.560
2024-12-02-01:14:31-root-INFO: Loss too large (1487.821->1784.016)! Learning rate decreased to 0.00651.
2024-12-02-01:14:31-root-INFO: Loss too large (1487.821->1650.807)! Learning rate decreased to 0.00521.
2024-12-02-01:14:31-root-INFO: Loss too large (1487.821->1560.804)! Learning rate decreased to 0.00417.
2024-12-02-01:14:32-root-INFO: Loss too large (1487.821->1502.237)! Learning rate decreased to 0.00333.
2024-12-02-01:14:33-root-INFO: grad norm: 189.489 184.078 44.959
2024-12-02-01:14:33-root-INFO: Loss Change: 1487.821 -> 1422.519
2024-12-02-01:14:33-root-INFO: Regularization Change: 0.000 -> 0.257
2024-12-02-01:14:33-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-01:14:33-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:14:34-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-01:14:34-root-INFO: grad norm: 103.972 99.895 28.828
2024-12-02-01:14:34-root-INFO: Loss too large (1406.948->1427.911)! Learning rate decreased to 0.00677.
2024-12-02-01:14:35-root-INFO: Loss too large (1406.948->1409.902)! Learning rate decreased to 0.00542.
2024-12-02-01:14:36-root-INFO: grad norm: 147.490 143.129 35.601
2024-12-02-01:14:36-root-INFO: Loss too large (1399.363->1419.121)! Learning rate decreased to 0.00433.
2024-12-02-01:14:37-root-INFO: Loss Change: 1406.948 -> 1388.165
2024-12-02-01:14:37-root-INFO: Regularization Change: 0.000 -> 0.428
2024-12-02-01:14:37-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-01:14:37-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:14:37-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-01:14:37-root-INFO: grad norm: 175.164 170.648 39.518
2024-12-02-01:14:38-root-INFO: Loss too large (1374.867->1541.859)! Learning rate decreased to 0.00704.
2024-12-02-01:14:38-root-INFO: Loss too large (1374.867->1471.608)! Learning rate decreased to 0.00563.
2024-12-02-01:14:38-root-INFO: Loss too large (1374.867->1423.072)! Learning rate decreased to 0.00450.
2024-12-02-01:14:39-root-INFO: Loss too large (1374.867->1390.652)! Learning rate decreased to 0.00360.
2024-12-02-01:14:40-root-INFO: grad norm: 152.813 148.667 35.356
2024-12-02-01:14:40-root-INFO: Loss Change: 1374.867 -> 1339.360
2024-12-02-01:14:40-root-INFO: Regularization Change: 0.000 -> 0.193
2024-12-02-01:14:40-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-01:14:40-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:14:41-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-01:14:41-root-INFO: grad norm: 86.490 83.713 21.742
2024-12-02-01:14:41-root-INFO: Loss too large (1328.061->1337.484)! Learning rate decreased to 0.00732.
2024-12-02-01:14:42-root-INFO: grad norm: 144.940 141.315 32.211
2024-12-02-01:14:43-root-INFO: Loss too large (1325.972->1411.235)! Learning rate decreased to 0.00585.
2024-12-02-01:14:43-root-INFO: Loss too large (1325.972->1347.903)! Learning rate decreased to 0.00468.
2024-12-02-01:14:44-root-INFO: Loss Change: 1328.061 -> 1312.678
2024-12-02-01:14:44-root-INFO: Regularization Change: 0.000 -> 0.499
2024-12-02-01:14:44-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-01:14:44-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-01:14:44-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-01:14:44-root-INFO: grad norm: 166.929 163.507 33.626
2024-12-02-01:14:45-root-INFO: Loss too large (1298.641->1466.072)! Learning rate decreased to 0.00760.
2024-12-02-01:14:45-root-INFO: Loss too large (1298.641->1395.997)! Learning rate decreased to 0.00608.
2024-12-02-01:14:45-root-INFO: Loss too large (1298.641->1347.955)! Learning rate decreased to 0.00487.
2024-12-02-01:14:46-root-INFO: Loss too large (1298.641->1316.026)! Learning rate decreased to 0.00389.
2024-12-02-01:14:47-root-INFO: grad norm: 139.740 136.429 30.237
2024-12-02-01:14:47-root-INFO: Loss Change: 1298.641 -> 1265.452
2024-12-02-01:14:47-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-02-01:14:47-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-01:14:47-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:14:48-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-01:14:48-root-INFO: grad norm: 70.258 68.180 16.963
2024-12-02-01:14:49-root-INFO: grad norm: 122.586 119.982 25.130
2024-12-02-01:14:49-root-INFO: Loss too large (1250.872->1394.543)! Learning rate decreased to 0.00790.
2024-12-02-01:14:50-root-INFO: Loss too large (1250.872->1318.782)! Learning rate decreased to 0.00632.
2024-12-02-01:14:50-root-INFO: Loss too large (1250.872->1265.338)! Learning rate decreased to 0.00506.
2024-12-02-01:14:51-root-INFO: Loss Change: 1260.517 -> 1238.574
2024-12-02-01:14:51-root-INFO: Regularization Change: 0.000 -> 0.639
2024-12-02-01:14:51-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-01:14:51-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:14:51-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-01:14:51-root-INFO: grad norm: 131.002 128.655 24.686
2024-12-02-01:14:52-root-INFO: Loss too large (1227.035->1328.729)! Learning rate decreased to 0.00821.
2024-12-02-01:14:52-root-INFO: Loss too large (1227.035->1283.717)! Learning rate decreased to 0.00656.
2024-12-02-01:14:52-root-INFO: Loss too large (1227.035->1253.182)! Learning rate decreased to 0.00525.
2024-12-02-01:14:53-root-INFO: Loss too large (1227.035->1233.277)! Learning rate decreased to 0.00420.
2024-12-02-01:14:54-root-INFO: grad norm: 108.328 105.953 22.561
2024-12-02-01:14:54-root-INFO: Loss Change: 1227.035 -> 1200.508
2024-12-02-01:14:54-root-INFO: Regularization Change: 0.000 -> 0.164
2024-12-02-01:14:54-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-01:14:54-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:14:55-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-01:14:55-root-INFO: grad norm: 59.096 57.394 14.083
2024-12-02-01:14:56-root-INFO: grad norm: 103.354 101.400 20.004
2024-12-02-01:14:56-root-INFO: Loss too large (1185.940->1305.093)! Learning rate decreased to 0.00852.
2024-12-02-01:14:57-root-INFO: Loss too large (1185.940->1235.798)! Learning rate decreased to 0.00682.
2024-12-02-01:14:57-root-INFO: Loss too large (1185.940->1195.158)! Learning rate decreased to 0.00545.
2024-12-02-01:14:58-root-INFO: Loss Change: 1195.851 -> 1176.071
2024-12-02-01:14:58-root-INFO: Regularization Change: 0.000 -> 0.565
2024-12-02-01:14:58-root-INFO: Undo step: 160
2024-12-02-01:14:58-root-INFO: Undo step: 161
2024-12-02-01:14:58-root-INFO: Undo step: 162
2024-12-02-01:14:58-root-INFO: Undo step: 163
2024-12-02-01:14:58-root-INFO: Undo step: 164
2024-12-02-01:14:58-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-01:14:58-root-INFO: grad norm: 283.087 259.308 113.567
2024-12-02-01:14:59-root-INFO: grad norm: 202.374 196.153 49.791
2024-12-02-01:15:00-root-INFO: Loss too large (1445.067->1488.160)! Learning rate decreased to 0.00704.
2024-12-02-01:15:00-root-INFO: Loss Change: 1826.023 -> 1414.512
2024-12-02-01:15:00-root-INFO: Regularization Change: 0.000 -> 8.971
2024-12-02-01:15:00-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-01:15:00-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:15:01-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-01:15:01-root-INFO: grad norm: 359.193 347.948 89.174
2024-12-02-01:15:01-root-INFO: Loss too large (1390.384->1855.231)! Learning rate decreased to 0.00732.
2024-12-02-01:15:02-root-INFO: Loss too large (1390.384->1650.993)! Learning rate decreased to 0.00585.
2024-12-02-01:15:02-root-INFO: Loss too large (1390.384->1512.998)! Learning rate decreased to 0.00468.
2024-12-02-01:15:02-root-INFO: Loss too large (1390.384->1421.069)! Learning rate decreased to 0.00375.
2024-12-02-01:15:03-root-INFO: grad norm: 205.899 199.693 50.169
2024-12-02-01:15:04-root-INFO: Loss Change: 1390.384 -> 1256.096
2024-12-02-01:15:04-root-INFO: Regularization Change: 0.000 -> 0.806
2024-12-02-01:15:04-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-01:15:04-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-01:15:04-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-01:15:05-root-INFO: grad norm: 93.787 90.078 26.116
2024-12-02-01:15:06-root-INFO: grad norm: 191.262 184.546 50.237
2024-12-02-01:15:06-root-INFO: Loss too large (1226.925->1424.186)! Learning rate decreased to 0.00760.
2024-12-02-01:15:06-root-INFO: Loss too large (1226.925->1344.912)! Learning rate decreased to 0.00608.
2024-12-02-01:15:07-root-INFO: Loss too large (1226.925->1289.347)! Learning rate decreased to 0.00487.
2024-12-02-01:15:07-root-INFO: Loss too large (1226.925->1251.148)! Learning rate decreased to 0.00389.
2024-12-02-01:15:08-root-INFO: Loss Change: 1248.417 -> 1225.890
2024-12-02-01:15:08-root-INFO: Regularization Change: 0.000 -> 0.978
2024-12-02-01:15:08-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-01:15:08-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:15:08-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-01:15:08-root-INFO: grad norm: 199.436 194.936 42.130
2024-12-02-01:15:09-root-INFO: Loss too large (1239.387->1443.371)! Learning rate decreased to 0.00790.
2024-12-02-01:15:09-root-INFO: Loss too large (1239.387->1348.223)! Learning rate decreased to 0.00632.
2024-12-02-01:15:09-root-INFO: Loss too large (1239.387->1253.503)! Learning rate decreased to 0.00506.
2024-12-02-01:15:10-root-INFO: grad norm: 189.394 183.620 46.409
2024-12-02-01:15:11-root-INFO: Loss too large (1202.206->1211.025)! Learning rate decreased to 0.00404.
2024-12-02-01:15:11-root-INFO: Loss Change: 1239.387 -> 1189.544
2024-12-02-01:15:11-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-02-01:15:11-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-01:15:11-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:15:12-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-01:15:12-root-INFO: grad norm: 135.223 130.403 35.781
2024-12-02-01:15:12-root-INFO: Loss too large (1189.657->1280.066)! Learning rate decreased to 0.00821.
2024-12-02-01:15:13-root-INFO: Loss too large (1189.657->1199.793)! Learning rate decreased to 0.00656.
2024-12-02-01:15:14-root-INFO: grad norm: 164.049 159.013 40.338
2024-12-02-01:15:14-root-INFO: Loss too large (1160.406->1204.365)! Learning rate decreased to 0.00525.
2024-12-02-01:15:14-root-INFO: Loss too large (1160.406->1171.146)! Learning rate decreased to 0.00420.
2024-12-02-01:15:15-root-INFO: Loss Change: 1189.657 -> 1151.072
2024-12-02-01:15:15-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-01:15:15-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-01:15:15-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:15:15-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-01:15:16-root-INFO: grad norm: 148.536 144.857 32.856
2024-12-02-01:15:16-root-INFO: Loss too large (1159.450->1328.893)! Learning rate decreased to 0.00852.
2024-12-02-01:15:16-root-INFO: Loss too large (1159.450->1223.523)! Learning rate decreased to 0.00682.
2024-12-02-01:15:17-root-INFO: Loss too large (1159.450->1161.441)! Learning rate decreased to 0.00545.
2024-12-02-01:15:18-root-INFO: grad norm: 127.115 124.074 27.638
2024-12-02-01:15:18-root-INFO: Loss Change: 1159.450 -> 1124.691
2024-12-02-01:15:18-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-01:15:18-root-INFO: Undo step: 160
2024-12-02-01:15:18-root-INFO: Undo step: 161
2024-12-02-01:15:18-root-INFO: Undo step: 162
2024-12-02-01:15:18-root-INFO: Undo step: 163
2024-12-02-01:15:18-root-INFO: Undo step: 164
2024-12-02-01:15:19-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-01:15:19-root-INFO: grad norm: 499.932 486.095 116.804
2024-12-02-01:15:19-root-INFO: Loss too large (1930.098->2055.645)! Learning rate decreased to 0.00704.
2024-12-02-01:15:20-root-INFO: grad norm: 418.709 412.402 72.398
2024-12-02-01:15:21-root-INFO: Loss Change: 1930.098 -> 1545.734
2024-12-02-01:15:21-root-INFO: Regularization Change: 0.000 -> 10.353
2024-12-02-01:15:21-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-01:15:21-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-01:15:21-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-01:15:22-root-INFO: grad norm: 323.197 319.206 50.632
2024-12-02-01:15:22-root-INFO: Loss too large (1551.817->1582.265)! Learning rate decreased to 0.00732.
2024-12-02-01:15:23-root-INFO: grad norm: 407.484 394.801 100.870
2024-12-02-01:15:23-root-INFO: Loss too large (1405.574->1752.824)! Learning rate decreased to 0.00585.
2024-12-02-01:15:24-root-INFO: Loss too large (1405.574->1558.880)! Learning rate decreased to 0.00468.
2024-12-02-01:15:24-root-INFO: Loss too large (1405.574->1432.102)! Learning rate decreased to 0.00375.
2024-12-02-01:15:25-root-INFO: Loss Change: 1551.817 -> 1350.876
2024-12-02-01:15:25-root-INFO: Regularization Change: 0.000 -> 3.613
2024-12-02-01:15:25-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-01:15:25-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-01:15:25-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-01:15:25-root-INFO: grad norm: 251.911 245.240 57.589
2024-12-02-01:15:26-root-INFO: Loss too large (1353.512->1467.301)! Learning rate decreased to 0.00760.
2024-12-02-01:15:26-root-INFO: Loss too large (1353.512->1384.357)! Learning rate decreased to 0.00608.
2024-12-02-01:15:27-root-INFO: grad norm: 305.428 296.223 74.418
2024-12-02-01:15:27-root-INFO: Loss too large (1276.784->1382.851)! Learning rate decreased to 0.00487.
2024-12-02-01:15:28-root-INFO: Loss too large (1276.784->1306.400)! Learning rate decreased to 0.00389.
2024-12-02-01:15:28-root-INFO: Loss Change: 1353.512 -> 1256.675
2024-12-02-01:15:28-root-INFO: Regularization Change: 0.000 -> 0.679
2024-12-02-01:15:28-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-01:15:28-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:15:29-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-01:15:29-root-INFO: grad norm: 188.665 181.667 50.908
2024-12-02-01:15:29-root-INFO: Loss too large (1270.454->1363.039)! Learning rate decreased to 0.00790.
2024-12-02-01:15:30-root-INFO: grad norm: 322.742 313.283 77.565
2024-12-02-01:15:31-root-INFO: Loss too large (1253.361->1520.029)! Learning rate decreased to 0.00632.
2024-12-02-01:15:31-root-INFO: Loss too large (1253.361->1376.926)! Learning rate decreased to 0.00506.
2024-12-02-01:15:31-root-INFO: Loss too large (1253.361->1283.272)! Learning rate decreased to 0.00404.
2024-12-02-01:15:32-root-INFO: Loss Change: 1270.454 -> 1223.649
2024-12-02-01:15:32-root-INFO: Regularization Change: 0.000 -> 0.666
2024-12-02-01:15:32-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-01:15:32-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:15:32-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-01:15:33-root-INFO: grad norm: 187.689 182.728 42.866
2024-12-02-01:15:33-root-INFO: Loss too large (1226.936->1332.665)! Learning rate decreased to 0.00821.
2024-12-02-01:15:34-root-INFO: grad norm: 297.352 289.557 67.641
2024-12-02-01:15:34-root-INFO: Loss too large (1223.237->1391.009)! Learning rate decreased to 0.00656.
2024-12-02-01:15:34-root-INFO: Loss too large (1223.237->1284.432)! Learning rate decreased to 0.00525.
2024-12-02-01:15:35-root-INFO: Loss Change: 1226.936 -> 1217.692
2024-12-02-01:15:35-root-INFO: Regularization Change: 0.000 -> 0.706
2024-12-02-01:15:35-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-01:15:35-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:15:35-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-01:15:36-root-INFO: grad norm: 183.806 178.108 45.414
2024-12-02-01:15:37-root-INFO: grad norm: 298.201 290.214 68.553
2024-12-02-01:15:37-root-INFO: Loss too large (1210.887->1598.732)! Learning rate decreased to 0.00852.
2024-12-02-01:15:37-root-INFO: Loss too large (1210.887->1394.238)! Learning rate decreased to 0.00682.
2024-12-02-01:15:38-root-INFO: Loss too large (1210.887->1265.128)! Learning rate decreased to 0.00545.
2024-12-02-01:15:38-root-INFO: Loss Change: 1226.970 -> 1186.923
2024-12-02-01:15:38-root-INFO: Regularization Change: 0.000 -> 1.210
2024-12-02-01:15:38-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-01:15:38-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:15:39-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-01:15:39-root-INFO: grad norm: 200.545 195.938 42.736
2024-12-02-01:15:39-root-INFO: Loss too large (1192.574->1232.551)! Learning rate decreased to 0.00885.
2024-12-02-01:15:40-root-INFO: grad norm: 187.936 183.321 41.393
2024-12-02-01:15:41-root-INFO: Loss too large (1141.365->1165.433)! Learning rate decreased to 0.00708.
2024-12-02-01:15:41-root-INFO: Loss Change: 1192.574 -> 1119.263
2024-12-02-01:15:41-root-INFO: Regularization Change: 0.000 -> 1.139
2024-12-02-01:15:41-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-01:15:41-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-01:15:42-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-01:15:42-root-INFO: grad norm: 162.974 158.828 36.527
2024-12-02-01:15:42-root-INFO: Loss too large (1128.894->1155.061)! Learning rate decreased to 0.00919.
2024-12-02-01:15:43-root-INFO: grad norm: 150.570 148.070 27.324
2024-12-02-01:15:44-root-INFO: Loss Change: 1128.894 -> 1089.242
2024-12-02-01:15:44-root-INFO: Regularization Change: 0.000 -> 0.980
2024-12-02-01:15:44-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-01:15:44-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:15:45-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-01:15:45-root-INFO: grad norm: 200.266 196.503 38.642
2024-12-02-01:15:45-root-INFO: Loss too large (1108.826->1229.694)! Learning rate decreased to 0.00954.
2024-12-02-01:15:45-root-INFO: Loss too large (1108.826->1131.809)! Learning rate decreased to 0.00763.
2024-12-02-01:15:47-root-INFO: grad norm: 138.964 136.882 23.964
2024-12-02-01:15:47-root-INFO: Loss Change: 1108.826 -> 1025.725
2024-12-02-01:15:47-root-INFO: Regularization Change: 0.000 -> 0.601
2024-12-02-01:15:47-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-01:15:47-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:15:48-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-01:15:48-root-INFO: grad norm: 98.167 95.896 20.992
2024-12-02-01:15:48-root-INFO: Loss too large (1027.778->1047.093)! Learning rate decreased to 0.00991.
2024-12-02-01:15:49-root-INFO: grad norm: 117.845 115.584 22.972
2024-12-02-01:15:50-root-INFO: Loss too large (1019.108->1030.094)! Learning rate decreased to 0.00792.
2024-12-02-01:15:50-root-INFO: Loss Change: 1027.778 -> 1005.537
2024-12-02-01:15:50-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-02-01:15:50-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-01:15:50-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:15:51-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-01:15:51-root-INFO: grad norm: 126.725 124.104 25.638
2024-12-02-01:15:51-root-INFO: Loss too large (1013.982->1078.488)! Learning rate decreased to 0.01028.
2024-12-02-01:15:51-root-INFO: Loss too large (1013.982->1023.668)! Learning rate decreased to 0.00822.
2024-12-02-01:15:52-root-INFO: grad norm: 107.443 105.964 17.766
2024-12-02-01:15:53-root-INFO: Loss Change: 1013.982 -> 977.850
2024-12-02-01:15:53-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-01:15:53-root-INFO: Undo step: 155
2024-12-02-01:15:53-root-INFO: Undo step: 156
2024-12-02-01:15:53-root-INFO: Undo step: 157
2024-12-02-01:15:53-root-INFO: Undo step: 158
2024-12-02-01:15:53-root-INFO: Undo step: 159
2024-12-02-01:15:54-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-01:15:54-root-INFO: grad norm: 250.113 240.327 69.279
2024-12-02-01:15:55-root-INFO: grad norm: 233.125 227.317 51.712
2024-12-02-01:15:56-root-INFO: Loss Change: 1581.317 -> 1362.004
2024-12-02-01:15:56-root-INFO: Regularization Change: 0.000 -> 8.498
2024-12-02-01:15:56-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-01:15:56-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:15:56-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-01:15:56-root-INFO: grad norm: 325.108 320.071 57.008
2024-12-02-01:15:57-root-INFO: Loss too large (1344.737->1577.378)! Learning rate decreased to 0.00885.
2024-12-02-01:15:57-root-INFO: Loss too large (1344.737->1399.388)! Learning rate decreased to 0.00708.
2024-12-02-01:15:58-root-INFO: grad norm: 205.546 202.714 34.006
2024-12-02-01:15:59-root-INFO: Loss Change: 1344.737 -> 1120.014
2024-12-02-01:15:59-root-INFO: Regularization Change: 0.000 -> 2.406
2024-12-02-01:15:59-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-01:15:59-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-01:15:59-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-01:15:59-root-INFO: grad norm: 76.735 74.111 19.899
2024-12-02-01:16:00-root-INFO: grad norm: 91.166 89.023 19.650
2024-12-02-01:16:01-root-INFO: Loss too large (1084.860->1088.191)! Learning rate decreased to 0.00919.
2024-12-02-01:16:01-root-INFO: Loss Change: 1112.778 -> 1070.369
2024-12-02-01:16:01-root-INFO: Regularization Change: 0.000 -> 1.352
2024-12-02-01:16:01-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-01:16:01-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:16:02-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-01:16:02-root-INFO: grad norm: 142.860 140.274 27.057
2024-12-02-01:16:02-root-INFO: Loss too large (1075.010->1130.941)! Learning rate decreased to 0.00954.
2024-12-02-01:16:02-root-INFO: Loss too large (1075.010->1091.058)! Learning rate decreased to 0.00763.
2024-12-02-01:16:03-root-INFO: grad norm: 118.537 116.535 21.690
2024-12-02-01:16:04-root-INFO: Loss Change: 1075.010 -> 1030.040
2024-12-02-01:16:04-root-INFO: Regularization Change: 0.000 -> 0.559
2024-12-02-01:16:04-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-01:16:04-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:16:04-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-01:16:05-root-INFO: grad norm: 103.747 101.819 19.908
2024-12-02-01:16:05-root-INFO: Loss too large (1030.793->1074.994)! Learning rate decreased to 0.00991.
2024-12-02-01:16:05-root-INFO: Loss too large (1030.793->1035.770)! Learning rate decreased to 0.00792.
2024-12-02-01:16:06-root-INFO: grad norm: 104.643 102.680 20.170
2024-12-02-01:16:07-root-INFO: Loss Change: 1030.793 -> 1008.492
2024-12-02-01:16:07-root-INFO: Regularization Change: 0.000 -> 0.418
2024-12-02-01:16:07-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-01:16:07-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:16:07-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-01:16:08-root-INFO: grad norm: 124.987 122.831 23.115
2024-12-02-01:16:08-root-INFO: Loss too large (1014.216->1094.064)! Learning rate decreased to 0.01028.
2024-12-02-01:16:08-root-INFO: Loss too large (1014.216->1030.462)! Learning rate decreased to 0.00822.
2024-12-02-01:16:09-root-INFO: grad norm: 120.274 118.378 21.270
2024-12-02-01:16:10-root-INFO: Loss Change: 1014.216 -> 988.238
2024-12-02-01:16:10-root-INFO: Regularization Change: 0.000 -> 0.398
2024-12-02-01:16:10-root-INFO: Undo step: 155
2024-12-02-01:16:10-root-INFO: Undo step: 156
2024-12-02-01:16:10-root-INFO: Undo step: 157
2024-12-02-01:16:10-root-INFO: Undo step: 158
2024-12-02-01:16:10-root-INFO: Undo step: 159
2024-12-02-01:16:10-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-01:16:11-root-INFO: grad norm: 379.344 367.338 94.682
2024-12-02-01:16:12-root-INFO: grad norm: 492.280 477.428 120.010
2024-12-02-01:16:12-root-INFO: Loss too large (1630.310->2148.007)! Learning rate decreased to 0.00852.
2024-12-02-01:16:12-root-INFO: Loss too large (1630.310->1786.747)! Learning rate decreased to 0.00682.
2024-12-02-01:16:13-root-INFO: Loss Change: 1746.684 -> 1555.194
2024-12-02-01:16:13-root-INFO: Regularization Change: 0.000 -> 13.936
2024-12-02-01:16:13-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-01:16:13-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-01:16:13-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-01:16:14-root-INFO: grad norm: 463.813 458.911 67.252
2024-12-02-01:16:15-root-INFO: grad norm: 224.308 215.939 60.701
2024-12-02-01:16:15-root-INFO: Loss Change: 1575.091 -> 1261.360
2024-12-02-01:16:15-root-INFO: Regularization Change: 0.000 -> 19.712
2024-12-02-01:16:15-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-01:16:15-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-01:16:16-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-01:16:16-root-INFO: grad norm: 186.557 180.943 45.422
2024-12-02-01:16:17-root-INFO: grad norm: 177.097 170.182 49.002
2024-12-02-01:16:17-root-INFO: Loss too large (1122.089->1171.200)! Learning rate decreased to 0.00919.
2024-12-02-01:16:18-root-INFO: Loss Change: 1269.454 -> 1120.014
2024-12-02-01:16:18-root-INFO: Regularization Change: 0.000 -> 2.697
2024-12-02-01:16:18-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-01:16:18-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:16:18-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-01:16:19-root-INFO: grad norm: 149.657 144.603 38.567
2024-12-02-01:16:20-root-INFO: grad norm: 159.158 154.890 36.611
2024-12-02-01:16:20-root-INFO: Loss too large (1065.891->1123.893)! Learning rate decreased to 0.00954.
2024-12-02-01:16:21-root-INFO: Loss Change: 1128.289 -> 1062.611
2024-12-02-01:16:21-root-INFO: Regularization Change: 0.000 -> 1.157
2024-12-02-01:16:21-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-01:16:21-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:16:21-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-01:16:21-root-INFO: grad norm: 160.894 156.831 35.930
2024-12-02-01:16:22-root-INFO: Loss too large (1068.651->1083.858)! Learning rate decreased to 0.00991.
2024-12-02-01:16:23-root-INFO: grad norm: 125.410 123.268 23.084
2024-12-02-01:16:23-root-INFO: Loss Change: 1068.651 -> 998.933
2024-12-02-01:16:23-root-INFO: Regularization Change: 0.000 -> 0.904
2024-12-02-01:16:23-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-01:16:23-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-01:16:24-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-01:16:24-root-INFO: grad norm: 120.972 119.375 19.587
2024-12-02-01:16:24-root-INFO: Loss too large (1005.624->1018.843)! Learning rate decreased to 0.01028.
2024-12-02-01:16:25-root-INFO: grad norm: 112.020 110.822 16.336
2024-12-02-01:16:26-root-INFO: Loss Change: 1005.624 -> 971.913
2024-12-02-01:16:26-root-INFO: Regularization Change: 0.000 -> 0.540
2024-12-02-01:16:26-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-01:16:26-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-01:16:26-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-01:16:27-root-INFO: grad norm: 123.650 121.996 20.157
2024-12-02-01:16:27-root-INFO: Loss too large (979.324->1004.029)! Learning rate decreased to 0.01067.
2024-12-02-01:16:28-root-INFO: grad norm: 119.040 117.672 17.994
2024-12-02-01:16:29-root-INFO: Loss Change: 979.324 -> 954.465
2024-12-02-01:16:29-root-INFO: Regularization Change: 0.000 -> 0.515
2024-12-02-01:16:29-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-01:16:29-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-01:16:29-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-01:16:29-root-INFO: grad norm: 147.297 143.930 31.311
2024-12-02-01:16:30-root-INFO: Loss too large (967.307->1011.342)! Learning rate decreased to 0.01107.
2024-12-02-01:16:31-root-INFO: grad norm: 124.317 122.469 21.354
2024-12-02-01:16:31-root-INFO: Loss Change: 967.307 -> 931.828
2024-12-02-01:16:31-root-INFO: Regularization Change: 0.000 -> 0.769
2024-12-02-01:16:31-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-01:16:31-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-01:16:32-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-01:16:32-root-INFO: grad norm: 117.590 116.277 17.528
2024-12-02-01:16:32-root-INFO: Loss too large (937.670->948.877)! Learning rate decreased to 0.01148.
2024-12-02-01:16:33-root-INFO: grad norm: 103.520 102.403 15.160
2024-12-02-01:16:34-root-INFO: Loss Change: 937.670 -> 900.816
2024-12-02-01:16:34-root-INFO: Regularization Change: 0.000 -> 0.527
2024-12-02-01:16:34-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-01:16:34-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-01:16:34-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-01:16:35-root-INFO: grad norm: 110.172 108.660 18.190
2024-12-02-01:16:35-root-INFO: Loss too large (907.146->925.531)! Learning rate decreased to 0.01191.
2024-12-02-01:16:36-root-INFO: grad norm: 105.020 103.507 17.765
2024-12-02-01:16:37-root-INFO: Loss Change: 907.146 -> 882.890
2024-12-02-01:16:37-root-INFO: Regularization Change: 0.000 -> 0.554
2024-12-02-01:16:37-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-01:16:37-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:16:37-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-01:16:37-root-INFO: grad norm: 105.973 104.136 19.648
2024-12-02-01:16:38-root-INFO: Loss too large (888.760->900.115)! Learning rate decreased to 0.01235.
2024-12-02-01:16:39-root-INFO: grad norm: 95.290 94.078 15.148
2024-12-02-01:16:39-root-INFO: Loss Change: 888.760 -> 858.455
2024-12-02-01:16:39-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-02-01:16:39-root-INFO: Undo step: 150
2024-12-02-01:16:39-root-INFO: Undo step: 151
2024-12-02-01:16:39-root-INFO: Undo step: 152
2024-12-02-01:16:39-root-INFO: Undo step: 153
2024-12-02-01:16:39-root-INFO: Undo step: 154
2024-12-02-01:16:40-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-01:16:40-root-INFO: grad norm: 340.080 328.744 87.072
2024-12-02-01:16:41-root-INFO: grad norm: 166.648 160.939 43.248
2024-12-02-01:16:42-root-INFO: Loss Change: 1510.600 -> 1005.412
2024-12-02-01:16:42-root-INFO: Regularization Change: 0.000 -> 19.382
2024-12-02-01:16:42-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-01:16:42-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-01:16:42-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-01:16:42-root-INFO: grad norm: 133.168 129.536 30.887
2024-12-02-01:16:43-root-INFO: grad norm: 198.876 193.928 44.085
2024-12-02-01:16:44-root-INFO: Loss too large (1008.246->1154.256)! Learning rate decreased to 0.01067.
2024-12-02-01:16:44-root-INFO: Loss too large (1008.246->1054.572)! Learning rate decreased to 0.00853.
2024-12-02-01:16:45-root-INFO: Loss Change: 1009.382 -> 995.646
2024-12-02-01:16:45-root-INFO: Regularization Change: 0.000 -> 2.733
2024-12-02-01:16:45-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-01:16:45-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-01:16:45-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-01:16:45-root-INFO: grad norm: 163.931 161.751 26.648
2024-12-02-01:16:46-root-INFO: Loss too large (1002.059->1036.027)! Learning rate decreased to 0.01107.
2024-12-02-01:16:47-root-INFO: grad norm: 170.047 166.492 34.592
2024-12-02-01:16:47-root-INFO: Loss too large (936.295->982.622)! Learning rate decreased to 0.00885.
2024-12-02-01:16:48-root-INFO: Loss Change: 1002.059 -> 935.968
2024-12-02-01:16:48-root-INFO: Regularization Change: 0.000 -> 1.069
2024-12-02-01:16:48-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-01:16:48-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-01:16:48-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-01:16:48-root-INFO: grad norm: 130.804 128.941 21.999
2024-12-02-01:16:49-root-INFO: grad norm: 205.305 202.179 35.691
2024-12-02-01:16:50-root-INFO: Loss too large (936.019->1115.443)! Learning rate decreased to 0.01148.
2024-12-02-01:16:50-root-INFO: Loss too large (936.019->996.966)! Learning rate decreased to 0.00919.
2024-12-02-01:16:51-root-INFO: Loss Change: 940.252 -> 923.664
2024-12-02-01:16:51-root-INFO: Regularization Change: 0.000 -> 1.022
2024-12-02-01:16:51-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-01:16:51-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-01:16:51-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-01:16:51-root-INFO: grad norm: 153.862 151.909 24.437
2024-12-02-01:16:52-root-INFO: Loss too large (932.425->939.542)! Learning rate decreased to 0.01191.
2024-12-02-01:16:52-root-INFO: grad norm: 117.862 116.359 18.764
2024-12-02-01:16:53-root-INFO: Loss Change: 932.425 -> 857.643
2024-12-02-01:16:53-root-INFO: Regularization Change: 0.000 -> 1.001
2024-12-02-01:16:53-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-01:16:53-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:16:53-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-01:16:54-root-INFO: grad norm: 113.552 111.739 20.214
2024-12-02-01:16:54-root-INFO: Loss too large (862.814->877.227)! Learning rate decreased to 0.01235.
2024-12-02-01:16:55-root-INFO: grad norm: 99.920 98.239 18.252
2024-12-02-01:16:56-root-INFO: Loss Change: 862.814 -> 826.329
2024-12-02-01:16:56-root-INFO: Regularization Change: 0.000 -> 0.619
2024-12-02-01:16:56-root-INFO: Undo step: 150
2024-12-02-01:16:56-root-INFO: Undo step: 151
2024-12-02-01:16:56-root-INFO: Undo step: 152
2024-12-02-01:16:56-root-INFO: Undo step: 153
2024-12-02-01:16:56-root-INFO: Undo step: 154
2024-12-02-01:16:56-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-01:16:56-root-INFO: grad norm: 394.157 380.873 101.469
2024-12-02-01:16:57-root-INFO: Loss too large (1395.010->1749.654)! Learning rate decreased to 0.01028.
2024-12-02-01:16:57-root-INFO: Loss too large (1395.010->1514.495)! Learning rate decreased to 0.00822.
2024-12-02-01:16:58-root-INFO: grad norm: 256.418 252.475 44.796
2024-12-02-01:16:59-root-INFO: Loss Change: 1395.010 -> 1016.150
2024-12-02-01:16:59-root-INFO: Regularization Change: 0.000 -> 7.319
2024-12-02-01:16:59-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-01:16:59-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-01:16:59-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-01:16:59-root-INFO: grad norm: 116.498 112.594 29.904
2024-12-02-01:17:00-root-INFO: grad norm: 121.591 118.811 25.852
2024-12-02-01:17:01-root-INFO: Loss too large (933.382->984.547)! Learning rate decreased to 0.01067.
2024-12-02-01:17:01-root-INFO: Loss too large (933.382->947.668)! Learning rate decreased to 0.00853.
2024-12-02-01:17:02-root-INFO: Loss Change: 1009.595 -> 925.465
2024-12-02-01:17:02-root-INFO: Regularization Change: 0.000 -> 2.630
2024-12-02-01:17:02-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-01:17:02-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-01:17:02-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-01:17:02-root-INFO: grad norm: 122.316 119.575 25.752
2024-12-02-01:17:03-root-INFO: Loss too large (924.264->950.810)! Learning rate decreased to 0.01107.
2024-12-02-01:17:04-root-INFO: grad norm: 143.462 140.819 27.411
2024-12-02-01:17:04-root-INFO: Loss too large (904.565->908.758)! Learning rate decreased to 0.00885.
2024-12-02-01:17:05-root-INFO: Loss Change: 924.264 -> 883.233
2024-12-02-01:17:05-root-INFO: Regularization Change: 0.000 -> 1.086
2024-12-02-01:17:05-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-01:17:05-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-01:17:05-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-01:17:05-root-INFO: grad norm: 90.286 87.637 21.713
2024-12-02-01:17:06-root-INFO: grad norm: 102.485 100.391 20.611
2024-12-02-01:17:07-root-INFO: Loss too large (857.983->878.868)! Learning rate decreased to 0.01148.
2024-12-02-01:17:07-root-INFO: Loss Change: 883.509 -> 850.768
2024-12-02-01:17:07-root-INFO: Regularization Change: 0.000 -> 1.002
2024-12-02-01:17:07-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-01:17:07-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-01:17:08-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-01:17:08-root-INFO: grad norm: 114.232 110.649 28.385
2024-12-02-01:17:08-root-INFO: Loss too large (856.940->876.487)! Learning rate decreased to 0.01191.
2024-12-02-01:17:09-root-INFO: grad norm: 107.828 106.104 19.206
2024-12-02-01:17:10-root-INFO: Loss Change: 856.940 -> 830.757
2024-12-02-01:17:10-root-INFO: Regularization Change: 0.000 -> 0.853
2024-12-02-01:17:10-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-01:17:10-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:17:10-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-01:17:11-root-INFO: grad norm: 111.842 110.466 17.491
2024-12-02-01:17:11-root-INFO: Loss too large (837.550->858.566)! Learning rate decreased to 0.01235.
2024-12-02-01:17:12-root-INFO: grad norm: 110.217 109.017 16.217
2024-12-02-01:17:13-root-INFO: Loss Change: 837.550 -> 817.458
2024-12-02-01:17:13-root-INFO: Regularization Change: 0.000 -> 0.542
2024-12-02-01:17:13-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-01:17:13-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:17:13-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-01:17:13-root-INFO: grad norm: 122.060 120.128 21.631
2024-12-02-01:17:14-root-INFO: Loss too large (827.635->850.916)! Learning rate decreased to 0.01281.
2024-12-02-01:17:15-root-INFO: grad norm: 106.597 104.995 18.412
2024-12-02-01:17:15-root-INFO: Loss Change: 827.635 -> 790.099
2024-12-02-01:17:15-root-INFO: Regularization Change: 0.000 -> 0.531
2024-12-02-01:17:15-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-01:17:15-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:17:16-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-01:17:16-root-INFO: grad norm: 110.508 108.825 19.212
2024-12-02-01:17:16-root-INFO: Loss too large (800.857->821.354)! Learning rate decreased to 0.01328.
2024-12-02-01:17:17-root-INFO: grad norm: 95.449 93.936 16.926
2024-12-02-01:17:18-root-INFO: Loss Change: 800.857 -> 766.890
2024-12-02-01:17:18-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-02-01:17:18-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-01:17:18-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-01:17:18-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-01:17:19-root-INFO: grad norm: 95.240 93.832 16.315
2024-12-02-01:17:19-root-INFO: Loss too large (773.090->789.712)! Learning rate decreased to 0.01376.
2024-12-02-01:17:20-root-INFO: grad norm: 84.464 83.047 15.409
2024-12-02-01:17:20-root-INFO: Loss Change: 773.090 -> 747.103
2024-12-02-01:17:20-root-INFO: Regularization Change: 0.000 -> 0.433
2024-12-02-01:17:21-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-01:17:21-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:17:21-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-01:17:21-root-INFO: grad norm: 88.081 86.716 15.448
2024-12-02-01:17:22-root-INFO: Loss too large (752.973->770.512)! Learning rate decreased to 0.01426.
2024-12-02-01:17:23-root-INFO: grad norm: 79.899 78.415 15.328
2024-12-02-01:17:23-root-INFO: Loss Change: 752.973 -> 731.224
2024-12-02-01:17:23-root-INFO: Regularization Change: 0.000 -> 0.402
2024-12-02-01:17:23-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-01:17:23-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:17:24-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-01:17:24-root-INFO: grad norm: 89.022 87.553 16.105
2024-12-02-01:17:24-root-INFO: Loss too large (739.794->759.768)! Learning rate decreased to 0.01478.
2024-12-02-01:17:25-root-INFO: grad norm: 80.582 79.114 15.314
2024-12-02-01:17:26-root-INFO: Loss Change: 739.794 -> 717.836
2024-12-02-01:17:26-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-02-01:17:26-root-INFO: Undo step: 145
2024-12-02-01:17:26-root-INFO: Undo step: 146
2024-12-02-01:17:26-root-INFO: Undo step: 147
2024-12-02-01:17:26-root-INFO: Undo step: 148
2024-12-02-01:17:26-root-INFO: Undo step: 149
2024-12-02-01:17:26-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-01:17:26-root-INFO: grad norm: 230.351 225.621 46.443
2024-12-02-01:17:27-root-INFO: grad norm: 168.926 166.727 27.164
2024-12-02-01:17:28-root-INFO: Loss too large (1008.661->1015.827)! Learning rate decreased to 0.01235.
2024-12-02-01:17:28-root-INFO: Loss Change: 1192.417 -> 946.714
2024-12-02-01:17:28-root-INFO: Regularization Change: 0.000 -> 12.082
2024-12-02-01:17:28-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-01:17:28-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:17:29-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-01:17:29-root-INFO: grad norm: 180.860 176.873 37.768
2024-12-02-01:17:29-root-INFO: Loss too large (958.613->1025.285)! Learning rate decreased to 0.01281.
2024-12-02-01:17:30-root-INFO: grad norm: 171.574 169.559 26.223
2024-12-02-01:17:31-root-INFO: Loss Change: 958.613 -> 879.507
2024-12-02-01:17:31-root-INFO: Regularization Change: 0.000 -> 2.144
2024-12-02-01:17:31-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-01:17:31-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:17:31-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-01:17:32-root-INFO: grad norm: 154.208 152.150 25.111
2024-12-02-01:17:32-root-INFO: Loss too large (892.411->905.979)! Learning rate decreased to 0.01328.
2024-12-02-01:17:33-root-INFO: grad norm: 123.355 121.862 19.136
2024-12-02-01:17:34-root-INFO: Loss Change: 892.411 -> 807.602
2024-12-02-01:17:34-root-INFO: Regularization Change: 0.000 -> 1.262
2024-12-02-01:17:34-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-01:17:34-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-01:17:34-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-01:17:34-root-INFO: grad norm: 109.331 107.509 19.879
2024-12-02-01:17:35-root-INFO: Loss too large (814.475->818.357)! Learning rate decreased to 0.01376.
2024-12-02-01:17:36-root-INFO: grad norm: 87.925 86.550 15.490
2024-12-02-01:17:36-root-INFO: Loss Change: 814.475 -> 765.634
2024-12-02-01:17:36-root-INFO: Regularization Change: 0.000 -> 0.781
2024-12-02-01:17:36-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-01:17:36-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:17:37-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-01:17:37-root-INFO: grad norm: 85.584 83.941 16.691
2024-12-02-01:17:37-root-INFO: Loss too large (770.922->777.249)! Learning rate decreased to 0.01426.
2024-12-02-01:17:38-root-INFO: grad norm: 74.340 73.018 13.958
2024-12-02-01:17:39-root-INFO: Loss Change: 770.922 -> 741.473
2024-12-02-01:17:39-root-INFO: Regularization Change: 0.000 -> 0.569
2024-12-02-01:17:39-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-01:17:39-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:17:39-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-01:17:40-root-INFO: grad norm: 82.666 80.926 16.868
2024-12-02-01:17:40-root-INFO: Loss too large (749.279->760.004)! Learning rate decreased to 0.01478.
2024-12-02-01:17:41-root-INFO: grad norm: 74.253 72.942 13.892
2024-12-02-01:17:42-root-INFO: Loss Change: 749.279 -> 725.019
2024-12-02-01:17:42-root-INFO: Regularization Change: 0.000 -> 0.506
2024-12-02-01:17:42-root-INFO: Undo step: 145
2024-12-02-01:17:42-root-INFO: Undo step: 146
2024-12-02-01:17:42-root-INFO: Undo step: 147
2024-12-02-01:17:42-root-INFO: Undo step: 148
2024-12-02-01:17:42-root-INFO: Undo step: 149
2024-12-02-01:17:42-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-01:17:42-root-INFO: grad norm: 268.544 263.861 49.932
2024-12-02-01:17:43-root-INFO: grad norm: 181.937 178.878 33.218
2024-12-02-01:17:44-root-INFO: Loss Change: 1302.497 -> 957.467
2024-12-02-01:17:44-root-INFO: Regularization Change: 0.000 -> 17.339
2024-12-02-01:17:44-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-01:17:44-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:17:44-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-01:17:45-root-INFO: grad norm: 174.470 171.655 31.211
2024-12-02-01:17:46-root-INFO: grad norm: 145.809 143.653 24.985
2024-12-02-01:17:46-root-INFO: Loss Change: 966.698 -> 846.327
2024-12-02-01:17:46-root-INFO: Regularization Change: 0.000 -> 3.541
2024-12-02-01:17:46-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-01:17:46-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-01:17:47-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-01:17:47-root-INFO: grad norm: 126.953 125.497 19.174
2024-12-02-01:17:48-root-INFO: grad norm: 121.978 120.726 17.431
2024-12-02-01:17:49-root-INFO: Loss Change: 854.850 -> 815.517
2024-12-02-01:17:49-root-INFO: Regularization Change: 0.000 -> 1.652
2024-12-02-01:17:49-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-01:17:49-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-01:17:49-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-01:17:49-root-INFO: grad norm: 127.127 125.609 19.587
2024-12-02-01:17:50-root-INFO: grad norm: 119.440 117.918 19.004
2024-12-02-01:17:51-root-INFO: Loss Change: 823.542 -> 784.899
2024-12-02-01:17:51-root-INFO: Regularization Change: 0.000 -> 1.035
2024-12-02-01:17:51-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-01:17:51-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:17:51-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-01:17:52-root-INFO: grad norm: 116.597 115.040 18.989
2024-12-02-01:17:53-root-INFO: grad norm: 103.132 101.610 17.653
2024-12-02-01:17:53-root-INFO: Loss Change: 793.126 -> 749.738
2024-12-02-01:17:53-root-INFO: Regularization Change: 0.000 -> 0.834
2024-12-02-01:17:53-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-01:17:53-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:17:54-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-01:17:54-root-INFO: grad norm: 107.956 106.534 17.465
2024-12-02-01:17:55-root-INFO: grad norm: 99.033 97.622 16.658
2024-12-02-01:17:56-root-INFO: Loss Change: 761.415 -> 729.510
2024-12-02-01:17:56-root-INFO: Regularization Change: 0.000 -> 0.717
2024-12-02-01:17:56-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-01:17:56-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:17:56-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-01:17:56-root-INFO: grad norm: 102.913 101.418 17.478
2024-12-02-01:17:57-root-INFO: grad norm: 93.730 92.328 16.153
2024-12-02-01:17:58-root-INFO: Loss Change: 738.003 -> 707.280
2024-12-02-01:17:58-root-INFO: Regularization Change: 0.000 -> 0.639
2024-12-02-01:17:58-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-01:17:58-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-01:17:58-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-01:17:59-root-INFO: grad norm: 94.664 93.325 15.865
2024-12-02-01:18:00-root-INFO: grad norm: 88.804 87.410 15.675
2024-12-02-01:18:00-root-INFO: Loss Change: 714.291 -> 691.123
2024-12-02-01:18:00-root-INFO: Regularization Change: 0.000 -> 0.575
2024-12-02-01:18:00-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-01:18:00-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-01:18:01-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-01:18:01-root-INFO: grad norm: 97.902 96.376 17.218
2024-12-02-01:18:02-root-INFO: grad norm: 89.916 88.534 15.701
2024-12-02-01:18:03-root-INFO: Loss Change: 703.333 -> 676.558
2024-12-02-01:18:03-root-INFO: Regularization Change: 0.000 -> 0.603
2024-12-02-01:18:03-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-01:18:03-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-01:18:03-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-01:18:03-root-INFO: grad norm: 90.387 89.159 14.845
2024-12-02-01:18:04-root-INFO: grad norm: 82.326 81.001 14.709
2024-12-02-01:18:05-root-INFO: Loss Change: 683.136 -> 657.580
2024-12-02-01:18:05-root-INFO: Regularization Change: 0.000 -> 0.585
2024-12-02-01:18:05-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-01:18:05-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-01:18:05-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-01:18:06-root-INFO: grad norm: 85.397 84.075 14.964
2024-12-02-01:18:07-root-INFO: grad norm: 79.768 78.374 14.845
2024-12-02-01:18:07-root-INFO: Loss Change: 665.846 -> 645.032
2024-12-02-01:18:07-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-02-01:18:07-root-INFO: Undo step: 140
2024-12-02-01:18:07-root-INFO: Undo step: 141
2024-12-02-01:18:07-root-INFO: Undo step: 142
2024-12-02-01:18:07-root-INFO: Undo step: 143
2024-12-02-01:18:07-root-INFO: Undo step: 144
2024-12-02-01:18:08-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-01:18:08-root-INFO: grad norm: 310.999 305.353 58.988
2024-12-02-01:18:09-root-INFO: grad norm: 140.109 137.608 26.356
2024-12-02-01:18:10-root-INFO: Loss Change: 1245.336 -> 807.530
2024-12-02-01:18:10-root-INFO: Regularization Change: 0.000 -> 30.346
2024-12-02-01:18:10-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-01:18:10-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:18:10-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-01:18:10-root-INFO: grad norm: 85.455 83.909 16.185
2024-12-02-01:18:11-root-INFO: grad norm: 61.745 60.738 11.102
2024-12-02-01:18:12-root-INFO: Loss Change: 803.655 -> 710.316
2024-12-02-01:18:12-root-INFO: Regularization Change: 0.000 -> 3.516
2024-12-02-01:18:12-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-01:18:12-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-01:18:12-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-01:18:13-root-INFO: grad norm: 51.895 51.151 8.759
2024-12-02-01:18:13-root-INFO: grad norm: 50.666 50.013 8.109
2024-12-02-01:18:14-root-INFO: Loss Change: 706.005 -> 670.659
2024-12-02-01:18:14-root-INFO: Regularization Change: 0.000 -> 1.570
2024-12-02-01:18:14-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-01:18:14-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-01:18:14-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-01:18:15-root-INFO: grad norm: 49.529 48.815 8.383
2024-12-02-01:18:16-root-INFO: grad norm: 50.283 49.673 7.807
2024-12-02-01:18:17-root-INFO: Loss Change: 667.597 -> 645.379
2024-12-02-01:18:17-root-INFO: Regularization Change: 0.000 -> 1.212
2024-12-02-01:18:17-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-01:18:17-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-01:18:17-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-01:18:17-root-INFO: grad norm: 52.008 51.427 7.754
2024-12-02-01:18:18-root-INFO: grad norm: 57.240 56.661 8.123
2024-12-02-01:18:19-root-INFO: Loss Change: 642.168 -> 634.281
2024-12-02-01:18:19-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-02-01:18:19-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-01:18:19-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-01:18:19-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-01:18:20-root-INFO: grad norm: 61.540 60.858 9.134
2024-12-02-01:18:20-root-INFO: Loss too large (628.921->634.582)! Learning rate decreased to 0.01761.
2024-12-02-01:18:21-root-INFO: grad norm: 51.701 51.070 8.054
2024-12-02-01:18:22-root-INFO: Loss Change: 628.921 -> 609.982
2024-12-02-01:18:22-root-INFO: Regularization Change: 0.000 -> 0.468
2024-12-02-01:18:22-root-INFO: Undo step: 140
2024-12-02-01:18:22-root-INFO: Undo step: 141
2024-12-02-01:18:22-root-INFO: Undo step: 142
2024-12-02-01:18:22-root-INFO: Undo step: 143
2024-12-02-01:18:22-root-INFO: Undo step: 144
2024-12-02-01:18:22-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-01:18:22-root-INFO: grad norm: 322.467 317.331 57.326
2024-12-02-01:18:23-root-INFO: grad norm: 215.617 213.173 32.375
2024-12-02-01:18:24-root-INFO: Loss Change: 1328.325 -> 902.902
2024-12-02-01:18:24-root-INFO: Regularization Change: 0.000 -> 29.372
2024-12-02-01:18:24-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-01:18:24-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-01:18:24-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-01:18:25-root-INFO: grad norm: 150.319 148.209 25.097
2024-12-02-01:18:25-root-INFO: grad norm: 144.987 143.317 21.944
2024-12-02-01:18:26-root-INFO: Loss too large (784.669->800.499)! Learning rate decreased to 0.01531.
2024-12-02-01:18:27-root-INFO: Loss Change: 893.578 -> 751.202
2024-12-02-01:18:27-root-INFO: Regularization Change: 0.000 -> 5.745
2024-12-02-01:18:27-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-01:18:27-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-01:18:27-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-01:18:27-root-INFO: grad norm: 107.175 105.819 16.995
2024-12-02-01:18:28-root-INFO: grad norm: 147.600 145.214 26.435
2024-12-02-01:18:29-root-INFO: Loss too large (703.357->804.330)! Learning rate decreased to 0.01586.
2024-12-02-01:18:29-root-INFO: Loss too large (703.357->740.271)! Learning rate decreased to 0.01269.
2024-12-02-01:18:30-root-INFO: Loss Change: 750.072 -> 700.342
2024-12-02-01:18:30-root-INFO: Regularization Change: 0.000 -> 1.455
2024-12-02-01:18:30-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-01:18:30-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-01:18:30-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-01:18:30-root-INFO: grad norm: 88.008 86.762 14.758
2024-12-02-01:18:31-root-INFO: grad norm: 80.643 79.118 15.609
2024-12-02-01:18:31-root-INFO: Loss too large (654.270->667.506)! Learning rate decreased to 0.01642.
2024-12-02-01:18:32-root-INFO: Loss Change: 701.947 -> 648.898
2024-12-02-01:18:32-root-INFO: Regularization Change: 0.000 -> 1.290
2024-12-02-01:18:32-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-01:18:32-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-01:18:32-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-01:18:33-root-INFO: grad norm: 65.205 64.139 11.742
2024-12-02-01:18:34-root-INFO: grad norm: 67.578 66.604 11.435
2024-12-02-01:18:34-root-INFO: Loss too large (633.168->641.033)! Learning rate decreased to 0.01701.
2024-12-02-01:18:35-root-INFO: Loss Change: 651.152 -> 624.749
2024-12-02-01:18:35-root-INFO: Regularization Change: 0.000 -> 0.798
2024-12-02-01:18:35-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-01:18:35-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-01:18:35-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-01:18:35-root-INFO: grad norm: 68.961 68.031 11.285
2024-12-02-01:18:36-root-INFO: Loss too large (630.543->634.781)! Learning rate decreased to 0.01761.
2024-12-02-01:18:37-root-INFO: grad norm: 58.856 58.133 9.202
2024-12-02-01:18:37-root-INFO: Loss Change: 630.543 -> 608.214
2024-12-02-01:18:37-root-INFO: Regularization Change: 0.000 -> 0.527
2024-12-02-01:18:37-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-01:18:37-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:18:38-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-01:18:38-root-INFO: grad norm: 66.789 65.900 10.863
2024-12-02-01:18:38-root-INFO: Loss too large (614.509->625.915)! Learning rate decreased to 0.01823.
2024-12-02-01:18:39-root-INFO: grad norm: 62.078 61.378 9.297
2024-12-02-01:18:40-root-INFO: Loss Change: 614.509 -> 599.120
2024-12-02-01:18:40-root-INFO: Regularization Change: 0.000 -> 0.448
2024-12-02-01:18:40-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-01:18:40-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:18:40-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-01:18:41-root-INFO: grad norm: 64.372 63.598 9.952
2024-12-02-01:18:41-root-INFO: Loss too large (603.306->611.699)! Learning rate decreased to 0.01887.
2024-12-02-01:18:42-root-INFO: grad norm: 57.285 56.615 8.734
2024-12-02-01:18:43-root-INFO: Loss Change: 603.306 -> 586.103
2024-12-02-01:18:43-root-INFO: Regularization Change: 0.000 -> 0.408
2024-12-02-01:18:43-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-01:18:43-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:18:43-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-01:18:43-root-INFO: grad norm: 67.260 66.298 11.333
2024-12-02-01:18:44-root-INFO: Loss too large (594.522->607.342)! Learning rate decreased to 0.01952.
2024-12-02-01:18:45-root-INFO: grad norm: 62.787 62.135 9.022
2024-12-02-01:18:45-root-INFO: Loss Change: 594.522 -> 579.786
2024-12-02-01:18:45-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-02-01:18:45-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-01:18:45-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-01:18:46-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-01:18:46-root-INFO: grad norm: 59.195 58.507 9.001
2024-12-02-01:18:46-root-INFO: Loss too large (583.392->585.037)! Learning rate decreased to 0.02020.
2024-12-02-01:18:47-root-INFO: grad norm: 48.561 47.983 7.470
2024-12-02-01:18:48-root-INFO: Loss Change: 583.392 -> 562.301
2024-12-02-01:18:48-root-INFO: Regularization Change: 0.000 -> 0.486
2024-12-02-01:18:48-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-01:18:48-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-01:18:48-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-01:18:49-root-INFO: grad norm: 53.491 52.825 8.417
2024-12-02-01:18:49-root-INFO: Loss too large (565.955->574.370)! Learning rate decreased to 0.02090.
2024-12-02-01:18:50-root-INFO: grad norm: 51.148 50.601 7.465
2024-12-02-01:18:51-root-INFO: Loss Change: 565.955 -> 555.545
2024-12-02-01:18:51-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-02-01:18:51-root-INFO: Undo step: 135
2024-12-02-01:18:51-root-INFO: Undo step: 136
2024-12-02-01:18:51-root-INFO: Undo step: 137
2024-12-02-01:18:51-root-INFO: Undo step: 138
2024-12-02-01:18:51-root-INFO: Undo step: 139
2024-12-02-01:18:51-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-01:18:51-root-INFO: grad norm: 179.825 175.355 39.846
2024-12-02-01:18:52-root-INFO: grad norm: 117.277 114.798 23.986
2024-12-02-01:18:53-root-INFO: Loss Change: 1015.061 -> 711.214
2024-12-02-01:18:53-root-INFO: Regularization Change: 0.000 -> 19.136
2024-12-02-01:18:53-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-01:18:53-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:18:53-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-01:18:54-root-INFO: grad norm: 68.450 66.730 15.246
2024-12-02-01:18:55-root-INFO: grad norm: 65.590 64.115 13.832
2024-12-02-01:18:55-root-INFO: Loss Change: 709.794 -> 651.456
2024-12-02-01:18:55-root-INFO: Regularization Change: 0.000 -> 3.457
2024-12-02-01:18:55-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-01:18:55-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:18:56-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-01:18:56-root-INFO: grad norm: 78.441 77.427 12.574
2024-12-02-01:18:57-root-INFO: grad norm: 89.070 87.995 13.798
2024-12-02-01:18:58-root-INFO: Loss Change: 654.865 -> 638.978
2024-12-02-01:18:58-root-INFO: Regularization Change: 0.000 -> 2.385
2024-12-02-01:18:58-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-01:18:58-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:18:58-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-01:18:58-root-INFO: grad norm: 106.055 105.047 14.593
2024-12-02-01:18:59-root-INFO: grad norm: 105.356 104.299 14.881
2024-12-02-01:19:00-root-INFO: Loss Change: 651.653 -> 630.402
2024-12-02-01:19:00-root-INFO: Regularization Change: 0.000 -> 1.828
2024-12-02-01:19:00-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-01:19:00-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-01:19:00-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-01:19:01-root-INFO: grad norm: 107.806 107.002 13.137
2024-12-02-01:19:02-root-INFO: grad norm: 94.694 93.660 13.955
2024-12-02-01:19:02-root-INFO: Loss Change: 637.916 -> 589.946
2024-12-02-01:19:02-root-INFO: Regularization Change: 0.000 -> 1.174
2024-12-02-01:19:02-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-01:19:02-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-01:19:03-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-01:19:03-root-INFO: grad norm: 88.471 87.679 11.812
2024-12-02-01:19:04-root-INFO: grad norm: 80.034 79.134 11.973
2024-12-02-01:19:05-root-INFO: Loss Change: 597.436 -> 571.947
2024-12-02-01:19:05-root-INFO: Regularization Change: 0.000 -> 0.955
2024-12-02-01:19:05-root-INFO: Undo step: 135
2024-12-02-01:19:05-root-INFO: Undo step: 136
2024-12-02-01:19:05-root-INFO: Undo step: 137
2024-12-02-01:19:05-root-INFO: Undo step: 138
2024-12-02-01:19:05-root-INFO: Undo step: 139
2024-12-02-01:19:05-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-01:19:05-root-INFO: grad norm: 205.678 202.065 38.378
2024-12-02-01:19:06-root-INFO: grad norm: 104.162 101.511 23.351
2024-12-02-01:19:07-root-INFO: Loss Change: 1095.666 -> 700.171
2024-12-02-01:19:07-root-INFO: Regularization Change: 0.000 -> 21.088
2024-12-02-01:19:07-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-01:19:07-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:19:07-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-01:19:08-root-INFO: grad norm: 88.122 86.733 15.589
2024-12-02-01:19:09-root-INFO: grad norm: 100.979 99.606 16.591
2024-12-02-01:19:09-root-INFO: Loss too large (670.905->679.747)! Learning rate decreased to 0.01823.
2024-12-02-01:19:10-root-INFO: Loss Change: 705.623 -> 645.822
2024-12-02-01:19:10-root-INFO: Regularization Change: 0.000 -> 3.105
2024-12-02-01:19:10-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-01:19:10-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:19:10-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-01:19:10-root-INFO: grad norm: 87.272 86.247 13.334
2024-12-02-01:19:11-root-INFO: grad norm: 101.683 100.325 16.563
2024-12-02-01:19:12-root-INFO: Loss too large (629.432->650.995)! Learning rate decreased to 0.01887.
2024-12-02-01:19:12-root-INFO: Loss Change: 650.598 -> 612.968
2024-12-02-01:19:12-root-INFO: Regularization Change: 0.000 -> 1.526
2024-12-02-01:19:12-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-01:19:12-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-01:19:13-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-01:19:13-root-INFO: grad norm: 86.469 85.375 13.711
2024-12-02-01:19:14-root-INFO: grad norm: 92.400 91.201 14.833
2024-12-02-01:19:14-root-INFO: Loss too large (602.109->617.290)! Learning rate decreased to 0.01952.
2024-12-02-01:19:15-root-INFO: Loss Change: 622.657 -> 584.621
2024-12-02-01:19:15-root-INFO: Regularization Change: 0.000 -> 1.118
2024-12-02-01:19:15-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-01:19:15-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-01:19:15-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-01:19:16-root-INFO: grad norm: 71.761 70.866 11.301
2024-12-02-01:19:17-root-INFO: grad norm: 81.170 80.159 12.773
2024-12-02-01:19:17-root-INFO: Loss too large (575.888->588.815)! Learning rate decreased to 0.02020.
2024-12-02-01:19:18-root-INFO: Loss Change: 588.341 -> 564.697
2024-12-02-01:19:18-root-INFO: Regularization Change: 0.000 -> 0.840
2024-12-02-01:19:18-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-01:19:18-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-01:19:18-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-01:19:18-root-INFO: grad norm: 63.781 62.978 10.091
2024-12-02-01:19:19-root-INFO: grad norm: 66.301 65.562 9.875
2024-12-02-01:19:20-root-INFO: Loss too large (552.085->560.204)! Learning rate decreased to 0.02090.
2024-12-02-01:19:20-root-INFO: Loss Change: 569.203 -> 541.372
2024-12-02-01:19:20-root-INFO: Regularization Change: 0.000 -> 0.764
2024-12-02-01:19:20-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-01:19:20-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-01:19:21-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-01:19:21-root-INFO: grad norm: 59.616 58.863 9.444
2024-12-02-01:19:22-root-INFO: grad norm: 71.143 70.403 10.229
2024-12-02-01:19:22-root-INFO: Loss too large (543.391->554.934)! Learning rate decreased to 0.02162.
2024-12-02-01:19:23-root-INFO: Loss Change: 546.601 -> 533.642
2024-12-02-01:19:23-root-INFO: Regularization Change: 0.000 -> 0.645
2024-12-02-01:19:23-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-01:19:23-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-01:19:23-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-01:19:24-root-INFO: grad norm: 59.428 58.694 9.312
2024-12-02-01:19:25-root-INFO: grad norm: 66.126 65.447 9.449
2024-12-02-01:19:25-root-INFO: Loss too large (529.621->540.550)! Learning rate decreased to 0.02236.
2024-12-02-01:19:26-root-INFO: Loss Change: 538.914 -> 520.331
2024-12-02-01:19:26-root-INFO: Regularization Change: 0.000 -> 0.576
2024-12-02-01:19:26-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-01:19:26-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-01:19:26-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-01:19:26-root-INFO: grad norm: 56.034 55.364 8.638
2024-12-02-01:19:27-root-INFO: grad norm: 65.511 64.881 9.063
2024-12-02-01:19:28-root-INFO: Loss too large (519.284->529.981)! Learning rate decreased to 0.02312.
2024-12-02-01:19:28-root-INFO: Loss Change: 523.776 -> 510.430
2024-12-02-01:19:28-root-INFO: Regularization Change: 0.000 -> 0.543
2024-12-02-01:19:28-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-01:19:28-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-01:19:29-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-01:19:29-root-INFO: grad norm: 54.761 54.107 8.435
2024-12-02-01:19:30-root-INFO: grad norm: 61.452 60.876 8.395
2024-12-02-01:19:30-root-INFO: Loss too large (507.810->517.512)! Learning rate decreased to 0.02390.
2024-12-02-01:19:31-root-INFO: Loss Change: 514.962 -> 498.783
2024-12-02-01:19:31-root-INFO: Regularization Change: 0.000 -> 0.550
2024-12-02-01:19:31-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-01:19:31-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-01:19:31-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-01:19:32-root-INFO: grad norm: 50.462 49.895 7.542
2024-12-02-01:19:33-root-INFO: grad norm: 60.124 59.581 8.060
2024-12-02-01:19:33-root-INFO: Loss too large (497.471->508.049)! Learning rate decreased to 0.02471.
2024-12-02-01:19:34-root-INFO: Loss Change: 501.263 -> 490.608
2024-12-02-01:19:34-root-INFO: Regularization Change: 0.000 -> 0.523
2024-12-02-01:19:34-root-INFO: Undo step: 130
2024-12-02-01:19:34-root-INFO: Undo step: 131
2024-12-02-01:19:34-root-INFO: Undo step: 132
2024-12-02-01:19:34-root-INFO: Undo step: 133
2024-12-02-01:19:34-root-INFO: Undo step: 134
2024-12-02-01:19:34-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-01:19:34-root-INFO: grad norm: 160.099 157.210 30.279
2024-12-02-01:19:35-root-INFO: grad norm: 84.335 82.325 18.304
2024-12-02-01:19:36-root-INFO: Loss Change: 889.416 -> 597.648
2024-12-02-01:19:36-root-INFO: Regularization Change: 0.000 -> 15.670
2024-12-02-01:19:36-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-01:19:36-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-01:19:36-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-01:19:37-root-INFO: grad norm: 53.267 52.175 10.728
2024-12-02-01:19:38-root-INFO: grad norm: 42.361 41.443 8.772
2024-12-02-01:19:38-root-INFO: Loss Change: 595.313 -> 545.038
2024-12-02-01:19:38-root-INFO: Regularization Change: 0.000 -> 2.655
2024-12-02-01:19:38-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-01:19:38-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-01:19:39-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-01:19:39-root-INFO: grad norm: 38.959 38.028 8.467
2024-12-02-01:19:40-root-INFO: grad norm: 36.542 35.897 6.837
2024-12-02-01:19:41-root-INFO: Loss Change: 544.876 -> 520.555
2024-12-02-01:19:41-root-INFO: Regularization Change: 0.000 -> 1.428
2024-12-02-01:19:41-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-01:19:41-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-01:19:41-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-01:19:41-root-INFO: grad norm: 39.047 38.273 7.736
2024-12-02-01:19:42-root-INFO: grad norm: 39.051 38.504 6.508
2024-12-02-01:19:43-root-INFO: Loss Change: 521.254 -> 504.746
2024-12-02-01:19:43-root-INFO: Regularization Change: 0.000 -> 1.108
2024-12-02-01:19:43-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-01:19:43-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-01:19:43-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-01:19:44-root-INFO: grad norm: 47.719 46.881 8.905
2024-12-02-01:19:45-root-INFO: grad norm: 55.138 54.523 8.214
2024-12-02-01:19:45-root-INFO: Loss too large (501.281->507.212)! Learning rate decreased to 0.02390.
2024-12-02-01:19:46-root-INFO: Loss Change: 507.735 -> 492.316
2024-12-02-01:19:46-root-INFO: Regularization Change: 0.000 -> 0.836
2024-12-02-01:19:46-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-01:19:46-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-01:19:46-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-01:19:46-root-INFO: grad norm: 49.624 48.920 8.330
2024-12-02-01:19:47-root-INFO: grad norm: 62.053 61.468 8.501
2024-12-02-01:19:48-root-INFO: Loss too large (494.191->505.954)! Learning rate decreased to 0.02471.
2024-12-02-01:19:48-root-INFO: Loss Change: 495.277 -> 486.307
2024-12-02-01:19:48-root-INFO: Regularization Change: 0.000 -> 0.710
2024-12-02-01:19:48-root-INFO: Undo step: 130
2024-12-02-01:19:48-root-INFO: Undo step: 131
2024-12-02-01:19:48-root-INFO: Undo step: 132
2024-12-02-01:19:48-root-INFO: Undo step: 133
2024-12-02-01:19:48-root-INFO: Undo step: 134
2024-12-02-01:19:49-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-01:19:49-root-INFO: grad norm: 138.918 135.116 32.279
2024-12-02-01:19:50-root-INFO: grad norm: 83.584 81.744 17.439
2024-12-02-01:19:51-root-INFO: Loss Change: 905.626 -> 602.995
2024-12-02-01:19:51-root-INFO: Regularization Change: 0.000 -> 19.171
2024-12-02-01:19:51-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-01:19:51-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-01:19:51-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-01:19:51-root-INFO: grad norm: 53.774 52.252 12.702
2024-12-02-01:19:53-root-INFO: grad norm: 44.727 43.710 9.483
2024-12-02-01:19:53-root-INFO: Loss Change: 600.709 -> 537.388
2024-12-02-01:19:53-root-INFO: Regularization Change: 0.000 -> 3.961
2024-12-02-01:19:53-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-01:19:53-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-01:19:54-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-01:19:54-root-INFO: grad norm: 45.334 44.556 8.362
2024-12-02-01:19:55-root-INFO: grad norm: 43.529 42.900 7.373
2024-12-02-01:19:56-root-INFO: Loss Change: 538.491 -> 514.260
2024-12-02-01:19:56-root-INFO: Regularization Change: 0.000 -> 1.481
2024-12-02-01:19:56-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-01:19:56-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-01:19:56-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-01:19:56-root-INFO: grad norm: 46.177 45.560 7.524
2024-12-02-01:19:57-root-INFO: grad norm: 45.906 45.356 7.084
2024-12-02-01:19:58-root-INFO: Loss Change: 514.619 -> 500.883
2024-12-02-01:19:58-root-INFO: Regularization Change: 0.000 -> 1.049
2024-12-02-01:19:58-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-01:19:58-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-01:19:58-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-01:19:59-root-INFO: grad norm: 59.053 58.415 8.656
2024-12-02-01:19:59-root-INFO: Loss too large (503.876->506.312)! Learning rate decreased to 0.02390.
2024-12-02-01:20:00-root-INFO: grad norm: 40.187 39.629 6.678
2024-12-02-01:20:01-root-INFO: Loss Change: 503.876 -> 476.983
2024-12-02-01:20:01-root-INFO: Regularization Change: 0.000 -> 0.835
2024-12-02-01:20:01-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-01:20:01-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-01:20:01-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-01:20:01-root-INFO: grad norm: 30.716 30.124 6.005
2024-12-02-01:20:02-root-INFO: grad norm: 33.344 32.825 5.859
2024-12-02-01:20:03-root-INFO: Loss Change: 477.632 -> 469.348
2024-12-02-01:20:03-root-INFO: Regularization Change: 0.000 -> 0.795
2024-12-02-01:20:03-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-01:20:03-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-01:20:03-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-01:20:04-root-INFO: grad norm: 46.062 45.269 8.512
2024-12-02-01:20:05-root-INFO: grad norm: 54.011 53.236 9.112
2024-12-02-01:20:05-root-INFO: Loss too large (472.988->477.997)! Learning rate decreased to 0.02553.
2024-12-02-01:20:06-root-INFO: Loss Change: 473.927 -> 462.994
2024-12-02-01:20:06-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-02-01:20:06-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-01:20:06-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-01:20:06-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-01:20:06-root-INFO: grad norm: 49.708 48.848 9.206
2024-12-02-01:20:07-root-INFO: Loss too large (467.241->468.461)! Learning rate decreased to 0.02639.
2024-12-02-01:20:08-root-INFO: grad norm: 39.808 39.171 7.095
2024-12-02-01:20:08-root-INFO: Loss Change: 467.241 -> 449.095
2024-12-02-01:20:08-root-INFO: Regularization Change: 0.000 -> 0.576
2024-12-02-01:20:08-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-01:20:08-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-01:20:09-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-01:20:09-root-INFO: grad norm: 42.377 41.658 7.772
2024-12-02-01:20:09-root-INFO: Loss too large (451.106->458.880)! Learning rate decreased to 0.02726.
2024-12-02-01:20:10-root-INFO: grad norm: 41.808 41.257 6.767
2024-12-02-01:20:11-root-INFO: Loss Change: 451.106 -> 443.583
2024-12-02-01:20:11-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-02-01:20:11-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-01:20:11-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-01:20:11-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-01:20:12-root-INFO: grad norm: 43.411 42.738 7.614
2024-12-02-01:20:12-root-INFO: Loss too large (446.200->450.956)! Learning rate decreased to 0.02816.
2024-12-02-01:20:13-root-INFO: grad norm: 38.772 38.269 6.226
2024-12-02-01:20:14-root-INFO: Loss Change: 446.200 -> 434.046
2024-12-02-01:20:14-root-INFO: Regularization Change: 0.000 -> 0.448
2024-12-02-01:20:14-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-01:20:14-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-01:20:14-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-01:20:14-root-INFO: grad norm: 43.065 42.374 7.687
2024-12-02-01:20:15-root-INFO: Loss too large (437.053->446.581)! Learning rate decreased to 0.02909.
2024-12-02-01:20:16-root-INFO: grad norm: 42.482 41.974 6.548
2024-12-02-01:20:16-root-INFO: Loss Change: 437.053 -> 429.924
2024-12-02-01:20:16-root-INFO: Regularization Change: 0.000 -> 0.453
2024-12-02-01:20:16-root-INFO: Undo step: 125
2024-12-02-01:20:16-root-INFO: Undo step: 126
2024-12-02-01:20:16-root-INFO: Undo step: 127
2024-12-02-01:20:16-root-INFO: Undo step: 128
2024-12-02-01:20:16-root-INFO: Undo step: 129
2024-12-02-01:20:17-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-01:20:17-root-INFO: grad norm: 161.583 159.195 27.674
2024-12-02-01:20:18-root-INFO: grad norm: 83.107 81.320 17.140
2024-12-02-01:20:19-root-INFO: Loss Change: 796.345 -> 548.068
2024-12-02-01:20:19-root-INFO: Regularization Change: 0.000 -> 24.305
2024-12-02-01:20:19-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-01:20:19-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-01:20:19-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-01:20:19-root-INFO: grad norm: 46.673 45.394 10.853
2024-12-02-01:20:20-root-INFO: grad norm: 35.702 34.742 8.224
2024-12-02-01:20:21-root-INFO: Loss Change: 545.456 -> 484.418
2024-12-02-01:20:21-root-INFO: Regularization Change: 0.000 -> 4.278
2024-12-02-01:20:21-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-01:20:21-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-01:20:21-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-01:20:22-root-INFO: grad norm: 35.035 34.362 6.836
2024-12-02-01:20:23-root-INFO: grad norm: 36.750 36.243 6.082
2024-12-02-01:20:23-root-INFO: Loss Change: 482.595 -> 463.964
2024-12-02-01:20:23-root-INFO: Regularization Change: 0.000 -> 1.759
2024-12-02-01:20:23-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-01:20:23-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-01:20:24-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-01:20:24-root-INFO: grad norm: 42.854 42.452 5.856
2024-12-02-01:20:25-root-INFO: grad norm: 47.421 47.041 5.997
2024-12-02-01:20:26-root-INFO: Loss Change: 463.168 -> 458.477
2024-12-02-01:20:26-root-INFO: Regularization Change: 0.000 -> 1.183
2024-12-02-01:20:26-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-01:20:26-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-01:20:26-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-01:20:26-root-INFO: grad norm: 59.952 59.548 6.944
2024-12-02-01:20:27-root-INFO: Loss too large (458.516->466.956)! Learning rate decreased to 0.02816.
2024-12-02-01:20:27-root-INFO: grad norm: 38.093 37.739 5.177
2024-12-02-01:20:28-root-INFO: Loss Change: 458.516 -> 432.954
2024-12-02-01:20:28-root-INFO: Regularization Change: 0.000 -> 1.078
2024-12-02-01:20:28-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-01:20:28-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-01:20:28-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-01:20:28-root-INFO: grad norm: 25.099 24.611 4.926
2024-12-02-01:20:28-root-INFO: grad norm: 28.177 27.848 4.292
2024-12-02-01:20:29-root-INFO: Loss Change: 432.324 -> 426.235
2024-12-02-01:20:29-root-INFO: Regularization Change: 0.000 -> 0.877
2024-12-02-01:20:29-root-INFO: Undo step: 125
2024-12-02-01:20:29-root-INFO: Undo step: 126
2024-12-02-01:20:29-root-INFO: Undo step: 127
2024-12-02-01:20:29-root-INFO: Undo step: 128
2024-12-02-01:20:29-root-INFO: Undo step: 129
2024-12-02-01:20:29-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-01:20:29-root-INFO: grad norm: 116.427 113.300 26.802
2024-12-02-01:20:30-root-INFO: grad norm: 74.716 73.313 14.412
2024-12-02-01:20:31-root-INFO: Loss Change: 786.738 -> 539.159
2024-12-02-01:20:31-root-INFO: Regularization Change: 0.000 -> 15.690
2024-12-02-01:20:31-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-01:20:31-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-01:20:31-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-01:20:31-root-INFO: grad norm: 58.566 56.870 13.989
2024-12-02-01:20:32-root-INFO: grad norm: 47.472 46.573 9.195
2024-12-02-01:20:33-root-INFO: Loss Change: 535.520 -> 483.294
2024-12-02-01:20:33-root-INFO: Regularization Change: 0.000 -> 3.262
2024-12-02-01:20:33-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-01:20:33-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-01:20:33-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-01:20:34-root-INFO: grad norm: 50.019 49.165 9.203
2024-12-02-01:20:35-root-INFO: grad norm: 48.258 47.667 7.529
2024-12-02-01:20:35-root-INFO: Loss Change: 480.245 -> 461.221
2024-12-02-01:20:35-root-INFO: Regularization Change: 0.000 -> 1.706
2024-12-02-01:20:35-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-01:20:35-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-01:20:36-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-01:20:36-root-INFO: grad norm: 45.886 45.207 7.865
2024-12-02-01:20:37-root-INFO: grad norm: 47.192 46.650 7.128
2024-12-02-01:20:38-root-INFO: Loss Change: 460.078 -> 449.293
2024-12-02-01:20:38-root-INFO: Regularization Change: 0.000 -> 1.175
2024-12-02-01:20:38-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-01:20:38-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-01:20:38-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-01:20:38-root-INFO: grad norm: 49.395 48.840 7.384
2024-12-02-01:20:39-root-INFO: Loss too large (448.844->449.244)! Learning rate decreased to 0.02816.
2024-12-02-01:20:40-root-INFO: grad norm: 44.183 43.829 5.587
2024-12-02-01:20:40-root-INFO: Loss Change: 448.844 -> 431.936
2024-12-02-01:20:40-root-INFO: Regularization Change: 0.000 -> 0.972
2024-12-02-01:20:40-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-01:20:40-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-01:20:41-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-01:20:41-root-INFO: grad norm: 30.667 30.271 4.914
2024-12-02-01:20:42-root-INFO: grad norm: 29.383 29.096 4.094
2024-12-02-01:20:43-root-INFO: Loss Change: 430.837 -> 416.341
2024-12-02-01:20:43-root-INFO: Regularization Change: 0.000 -> 1.177
2024-12-02-01:20:43-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-01:20:43-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-01:20:43-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-01:20:43-root-INFO: grad norm: 32.571 32.255 4.525
2024-12-02-01:20:44-root-INFO: grad norm: 40.030 39.778 4.482
2024-12-02-01:20:45-root-INFO: Loss too large (414.568->418.977)! Learning rate decreased to 0.03019.
2024-12-02-01:20:45-root-INFO: Loss Change: 415.209 -> 409.214
2024-12-02-01:20:45-root-INFO: Regularization Change: 0.000 -> 0.751
2024-12-02-01:20:45-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-01:20:45-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-01:20:46-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-01:20:46-root-INFO: grad norm: 33.018 32.744 4.243
2024-12-02-01:20:47-root-INFO: grad norm: 46.661 46.412 4.819
2024-12-02-01:20:47-root-INFO: Loss too large (408.419->417.280)! Learning rate decreased to 0.03117.
2024-12-02-01:20:48-root-INFO: Loss Change: 408.682 -> 406.389
2024-12-02-01:20:48-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-02-01:20:48-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-01:20:48-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-01:20:48-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-01:20:49-root-INFO: grad norm: 33.094 32.839 4.099
2024-12-02-01:20:50-root-INFO: grad norm: 35.710 35.511 3.770
2024-12-02-01:20:50-root-INFO: Loss Change: 406.210 -> 398.777
2024-12-02-01:20:50-root-INFO: Regularization Change: 0.000 -> 1.215
2024-12-02-01:20:50-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-01:20:50-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-01:20:51-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-01:20:51-root-INFO: grad norm: 42.417 42.214 4.146
2024-12-02-01:20:51-root-INFO: Loss too large (398.397->403.969)! Learning rate decreased to 0.03321.
2024-12-02-01:20:52-root-INFO: grad norm: 35.171 34.970 3.755
2024-12-02-01:20:53-root-INFO: Loss Change: 398.397 -> 384.567
2024-12-02-01:20:53-root-INFO: Regularization Change: 0.000 -> 0.561
2024-12-02-01:20:53-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-01:20:53-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-01:20:53-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-01:20:54-root-INFO: grad norm: 32.011 31.796 3.703
2024-12-02-01:20:54-root-INFO: Loss too large (384.798->392.270)! Learning rate decreased to 0.03427.
2024-12-02-01:20:55-root-INFO: grad norm: 36.049 35.852 3.760
2024-12-02-01:20:55-root-INFO: Loss too large (382.568->382.810)! Learning rate decreased to 0.02742.
2024-12-02-01:20:56-root-INFO: Loss Change: 384.798 -> 377.772
2024-12-02-01:20:56-root-INFO: Regularization Change: 0.000 -> 0.461
2024-12-02-01:20:56-root-INFO: Undo step: 120
2024-12-02-01:20:56-root-INFO: Undo step: 121
2024-12-02-01:20:56-root-INFO: Undo step: 122
2024-12-02-01:20:56-root-INFO: Undo step: 123
2024-12-02-01:20:56-root-INFO: Undo step: 124
2024-12-02-01:20:57-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-01:20:57-root-INFO: grad norm: 102.879 100.721 20.962
2024-12-02-01:20:58-root-INFO: grad norm: 66.594 65.455 12.260
2024-12-02-01:20:59-root-INFO: Loss Change: 700.407 -> 491.230
2024-12-02-01:20:59-root-INFO: Regularization Change: 0.000 -> 16.136
2024-12-02-01:20:59-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-01:20:59-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-01:20:59-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-01:20:59-root-INFO: grad norm: 58.232 57.424 9.662
2024-12-02-01:21:00-root-INFO: grad norm: 53.858 53.381 7.153
2024-12-02-01:21:01-root-INFO: Loss Change: 487.734 -> 445.448
2024-12-02-01:21:01-root-INFO: Regularization Change: 0.000 -> 3.788
2024-12-02-01:21:01-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-01:21:01-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-01:21:01-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-01:21:02-root-INFO: grad norm: 54.714 54.315 6.590
2024-12-02-01:21:02-root-INFO: grad norm: 42.380 42.005 5.628
2024-12-02-01:21:03-root-INFO: Loss Change: 445.781 -> 413.052
2024-12-02-01:21:03-root-INFO: Regularization Change: 0.000 -> 2.215
2024-12-02-01:21:03-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-01:21:03-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-01:21:04-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-01:21:04-root-INFO: grad norm: 36.408 36.077 4.892
2024-12-02-01:21:05-root-INFO: grad norm: 35.729 35.410 4.770
2024-12-02-01:21:06-root-INFO: Loss Change: 413.584 -> 401.254
2024-12-02-01:21:06-root-INFO: Regularization Change: 0.000 -> 1.270
2024-12-02-01:21:06-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-01:21:06-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-01:21:06-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-01:21:06-root-INFO: grad norm: 41.826 41.453 5.578
2024-12-02-01:21:07-root-INFO: Loss too large (402.716->403.872)! Learning rate decreased to 0.03321.
2024-12-02-01:21:08-root-INFO: grad norm: 32.973 32.678 4.400
2024-12-02-01:21:08-root-INFO: Loss Change: 402.716 -> 385.186
2024-12-02-01:21:08-root-INFO: Regularization Change: 0.000 -> 0.731
2024-12-02-01:21:08-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-01:21:08-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-01:21:09-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-01:21:09-root-INFO: grad norm: 29.966 29.633 4.457
2024-12-02-01:21:10-root-INFO: grad norm: 37.958 37.684 4.549
2024-12-02-01:21:10-root-INFO: Loss too large (386.212->388.972)! Learning rate decreased to 0.03427.
2024-12-02-01:21:11-root-INFO: Loss Change: 386.551 -> 380.029
2024-12-02-01:21:11-root-INFO: Regularization Change: 0.000 -> 0.690
2024-12-02-01:21:11-root-INFO: Undo step: 120
2024-12-02-01:21:11-root-INFO: Undo step: 121
2024-12-02-01:21:11-root-INFO: Undo step: 122
2024-12-02-01:21:11-root-INFO: Undo step: 123
2024-12-02-01:21:11-root-INFO: Undo step: 124
2024-12-02-01:21:11-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-01:21:12-root-INFO: grad norm: 127.658 126.351 18.221
2024-12-02-01:21:13-root-INFO: grad norm: 67.133 66.202 11.144
2024-12-02-01:21:13-root-INFO: Loss Change: 765.901 -> 493.801
2024-12-02-01:21:13-root-INFO: Regularization Change: 0.000 -> 22.428
2024-12-02-01:21:13-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-01:21:13-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-01:21:14-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-01:21:14-root-INFO: grad norm: 64.372 63.735 9.029
2024-12-02-01:21:15-root-INFO: grad norm: 54.774 54.110 8.503
2024-12-02-01:21:15-root-INFO: Loss Change: 497.741 -> 448.477
2024-12-02-01:21:15-root-INFO: Regularization Change: 0.000 -> 4.035
2024-12-02-01:21:16-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-01:21:16-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-01:21:16-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-01:21:16-root-INFO: grad norm: 53.476 53.032 6.873
2024-12-02-01:21:16-root-INFO: Loss too large (450.090->451.384)! Learning rate decreased to 0.03117.
2024-12-02-01:21:17-root-INFO: grad norm: 38.584 38.089 6.160
2024-12-02-01:21:18-root-INFO: Loss Change: 450.090 -> 417.241
2024-12-02-01:21:18-root-INFO: Regularization Change: 0.000 -> 1.603
2024-12-02-01:21:18-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-01:21:18-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-01:21:18-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-01:21:19-root-INFO: grad norm: 30.788 30.383 4.982
2024-12-02-01:21:20-root-INFO: grad norm: 34.452 34.067 5.134
2024-12-02-01:21:20-root-INFO: Loss Change: 418.059 -> 409.768
2024-12-02-01:21:20-root-INFO: Regularization Change: 0.000 -> 1.435
2024-12-02-01:21:20-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-01:21:20-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-01:21:21-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-01:21:21-root-INFO: grad norm: 47.082 46.714 5.879
2024-12-02-01:21:21-root-INFO: Loss too large (411.855->422.306)! Learning rate decreased to 0.03321.
2024-12-02-01:21:22-root-INFO: grad norm: 39.572 39.234 5.166
2024-12-02-01:21:23-root-INFO: Loss Change: 411.855 -> 394.717
2024-12-02-01:21:23-root-INFO: Regularization Change: 0.000 -> 0.846
2024-12-02-01:21:23-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-01:21:23-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-01:21:23-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-01:21:24-root-INFO: grad norm: 38.418 38.115 4.815
2024-12-02-01:21:24-root-INFO: Loss too large (396.076->404.008)! Learning rate decreased to 0.03427.
2024-12-02-01:21:25-root-INFO: grad norm: 33.083 32.788 4.409
2024-12-02-01:21:26-root-INFO: Loss Change: 396.076 -> 382.743
2024-12-02-01:21:26-root-INFO: Regularization Change: 0.000 -> 0.717
2024-12-02-01:21:26-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-01:21:26-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-01:21:26-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-01:21:26-root-INFO: grad norm: 32.113 31.783 4.589
2024-12-02-01:21:27-root-INFO: Loss too large (384.475->389.602)! Learning rate decreased to 0.03536.
2024-12-02-01:21:28-root-INFO: grad norm: 30.779 30.527 3.932
2024-12-02-01:21:29-root-INFO: Loss Change: 384.475 -> 376.402
2024-12-02-01:21:29-root-INFO: Regularization Change: 0.000 -> 0.603
2024-12-02-01:21:29-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-01:21:29-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-01:21:29-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-01:21:29-root-INFO: grad norm: 33.560 33.316 4.038
2024-12-02-01:21:30-root-INFO: Loss too large (377.288->386.853)! Learning rate decreased to 0.03648.
2024-12-02-01:21:30-root-INFO: Loss too large (377.288->377.340)! Learning rate decreased to 0.02919.
2024-12-02-01:21:31-root-INFO: grad norm: 24.783 24.544 3.432
2024-12-02-01:21:32-root-INFO: Loss Change: 377.288 -> 365.511
2024-12-02-01:21:32-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-02-01:21:32-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-01:21:32-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-01:21:32-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-01:21:32-root-INFO: grad norm: 21.712 21.400 3.664
2024-12-02-01:21:33-root-INFO: Loss too large (366.067->368.096)! Learning rate decreased to 0.03763.
2024-12-02-01:21:34-root-INFO: grad norm: 24.099 23.878 3.260
2024-12-02-01:21:34-root-INFO: Loss Change: 366.067 -> 362.460
2024-12-02-01:21:34-root-INFO: Regularization Change: 0.000 -> 0.543
2024-12-02-01:21:34-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-01:21:34-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-01:21:35-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-01:21:35-root-INFO: grad norm: 32.273 32.026 3.984
2024-12-02-01:21:35-root-INFO: Loss too large (363.398->375.254)! Learning rate decreased to 0.03881.
2024-12-02-01:21:36-root-INFO: Loss too large (363.398->365.477)! Learning rate decreased to 0.03105.
2024-12-02-01:21:37-root-INFO: grad norm: 25.491 25.286 3.229
2024-12-02-01:21:37-root-INFO: Loss Change: 363.398 -> 353.341
2024-12-02-01:21:37-root-INFO: Regularization Change: 0.000 -> 0.371
2024-12-02-01:21:37-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-01:21:37-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-01:21:38-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-01:21:38-root-INFO: grad norm: 22.459 22.206 3.359
2024-12-02-01:21:38-root-INFO: Loss too large (354.333->358.948)! Learning rate decreased to 0.04002.
2024-12-02-01:21:39-root-INFO: grad norm: 25.693 25.501 3.136
2024-12-02-01:21:40-root-INFO: Loss Change: 354.333 -> 352.366
2024-12-02-01:21:40-root-INFO: Regularization Change: 0.000 -> 0.515
2024-12-02-01:21:40-root-INFO: Undo step: 115
2024-12-02-01:21:40-root-INFO: Undo step: 116
2024-12-02-01:21:40-root-INFO: Undo step: 117
2024-12-02-01:21:40-root-INFO: Undo step: 118
2024-12-02-01:21:40-root-INFO: Undo step: 119
2024-12-02-01:21:40-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-01:21:41-root-INFO: grad norm: 102.903 101.262 18.305
2024-12-02-01:21:42-root-INFO: grad norm: 56.582 55.779 9.496
2024-12-02-01:21:42-root-INFO: Loss Change: 677.125 -> 427.407
2024-12-02-01:21:42-root-INFO: Regularization Change: 0.000 -> 22.430
2024-12-02-01:21:42-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-01:21:42-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-01:21:43-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-01:21:43-root-INFO: grad norm: 34.013 32.985 8.297
2024-12-02-01:21:44-root-INFO: grad norm: 36.000 35.349 6.819
2024-12-02-01:21:45-root-INFO: Loss Change: 425.493 -> 403.051
2024-12-02-01:21:45-root-INFO: Regularization Change: 0.000 -> 3.572
2024-12-02-01:21:45-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-01:21:45-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-01:21:45-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-01:21:45-root-INFO: grad norm: 46.082 45.300 8.456
2024-12-02-01:21:46-root-INFO: Loss too large (402.893->407.403)! Learning rate decreased to 0.03648.
2024-12-02-01:21:47-root-INFO: grad norm: 38.632 38.293 5.103
2024-12-02-01:21:47-root-INFO: Loss Change: 402.893 -> 381.083
2024-12-02-01:21:47-root-INFO: Regularization Change: 0.000 -> 1.465
2024-12-02-01:21:47-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-01:21:47-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-01:21:48-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-01:21:48-root-INFO: grad norm: 35.762 35.365 5.313
2024-12-02-01:21:48-root-INFO: Loss too large (382.716->387.702)! Learning rate decreased to 0.03763.
2024-12-02-01:21:49-root-INFO: grad norm: 36.565 36.315 4.270
2024-12-02-01:21:50-root-INFO: Loss Change: 382.716 -> 374.420
2024-12-02-01:21:50-root-INFO: Regularization Change: 0.000 -> 0.902
2024-12-02-01:21:50-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-01:21:50-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-01:21:50-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-01:21:51-root-INFO: grad norm: 40.611 40.311 4.925
2024-12-02-01:21:51-root-INFO: Loss too large (375.626->387.063)! Learning rate decreased to 0.03881.
2024-12-02-01:21:52-root-INFO: grad norm: 39.885 39.652 4.306
2024-12-02-01:21:53-root-INFO: Loss Change: 375.626 -> 366.468
2024-12-02-01:21:53-root-INFO: Regularization Change: 0.000 -> 0.736
2024-12-02-01:21:53-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-01:21:53-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-01:21:53-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-01:21:53-root-INFO: grad norm: 42.333 42.045 4.933
2024-12-02-01:21:54-root-INFO: Loss too large (368.733->382.546)! Learning rate decreased to 0.04002.
2024-12-02-01:21:55-root-INFO: grad norm: 39.795 39.573 4.193
2024-12-02-01:21:55-root-INFO: Loss Change: 368.733 -> 357.433
2024-12-02-01:21:55-root-INFO: Regularization Change: 0.000 -> 0.667
2024-12-02-01:21:55-root-INFO: Undo step: 115
2024-12-02-01:21:55-root-INFO: Undo step: 116
2024-12-02-01:21:55-root-INFO: Undo step: 117
2024-12-02-01:21:55-root-INFO: Undo step: 118
2024-12-02-01:21:55-root-INFO: Undo step: 119
2024-12-02-01:21:56-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-01:21:56-root-INFO: grad norm: 102.092 100.144 19.850
2024-12-02-01:21:57-root-INFO: grad norm: 56.962 56.073 10.022
2024-12-02-01:21:58-root-INFO: Loss Change: 655.245 -> 441.956
2024-12-02-01:21:58-root-INFO: Regularization Change: 0.000 -> 19.103
2024-12-02-01:21:58-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-01:21:58-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-01:21:58-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-01:21:58-root-INFO: grad norm: 38.652 38.084 6.602
2024-12-02-01:21:59-root-INFO: grad norm: 42.880 42.559 5.238
2024-12-02-01:22:00-root-INFO: Loss too large (415.143->418.105)! Learning rate decreased to 0.03536.
2024-12-02-01:22:00-root-INFO: Loss Change: 438.627 -> 407.427
2024-12-02-01:22:00-root-INFO: Regularization Change: 0.000 -> 3.564
2024-12-02-01:22:00-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-01:22:00-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-01:22:01-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-01:22:01-root-INFO: grad norm: 41.396 41.087 5.048
2024-12-02-01:22:01-root-INFO: Loss too large (407.228->409.689)! Learning rate decreased to 0.03648.
2024-12-02-01:22:02-root-INFO: grad norm: 43.206 42.963 4.569
2024-12-02-01:22:03-root-INFO: Loss Change: 407.228 -> 393.896
2024-12-02-01:22:03-root-INFO: Regularization Change: 0.000 -> 1.451
2024-12-02-01:22:03-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-01:22:03-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-01:22:03-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-01:22:04-root-INFO: grad norm: 44.417 44.114 5.175
2024-12-02-01:22:04-root-INFO: Loss too large (395.249->397.912)! Learning rate decreased to 0.03763.
2024-12-02-01:22:05-root-INFO: grad norm: 40.088 39.847 4.388
2024-12-02-01:22:06-root-INFO: Loss Change: 395.249 -> 376.795
2024-12-02-01:22:06-root-INFO: Regularization Change: 0.000 -> 1.092
2024-12-02-01:22:06-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-01:22:06-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-01:22:06-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-01:22:06-root-INFO: grad norm: 39.433 39.165 4.587
2024-12-02-01:22:07-root-INFO: Loss too large (377.134->381.590)! Learning rate decreased to 0.03881.
2024-12-02-01:22:08-root-INFO: grad norm: 36.104 35.876 4.047
2024-12-02-01:22:08-root-INFO: Loss Change: 377.134 -> 362.851
2024-12-02-01:22:08-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-02-01:22:08-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-01:22:08-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-01:22:09-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-01:22:09-root-INFO: grad norm: 35.923 35.671 4.245
2024-12-02-01:22:09-root-INFO: Loss too large (363.958->369.245)! Learning rate decreased to 0.04002.
2024-12-02-01:22:10-root-INFO: grad norm: 33.653 33.442 3.767
2024-12-02-01:22:11-root-INFO: Loss Change: 363.958 -> 352.871
2024-12-02-01:22:11-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-02-01:22:11-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-01:22:11-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-01:22:11-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-01:22:11-root-INFO: grad norm: 34.807 34.562 4.123
2024-12-02-01:22:12-root-INFO: Loss too large (354.772->361.149)! Learning rate decreased to 0.04126.
2024-12-02-01:22:13-root-INFO: grad norm: 32.962 32.766 3.593
2024-12-02-01:22:13-root-INFO: Loss Change: 354.772 -> 345.309
2024-12-02-01:22:13-root-INFO: Regularization Change: 0.000 -> 0.648
2024-12-02-01:22:14-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-01:22:14-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-01:22:14-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-01:22:14-root-INFO: grad norm: 34.115 33.882 3.985
2024-12-02-01:22:15-root-INFO: Loss too large (346.105->354.761)! Learning rate decreased to 0.04253.
2024-12-02-01:22:16-root-INFO: grad norm: 33.060 32.874 3.499
2024-12-02-01:22:16-root-INFO: Loss Change: 346.105 -> 338.398
2024-12-02-01:22:16-root-INFO: Regularization Change: 0.000 -> 0.585
2024-12-02-01:22:16-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-01:22:16-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-01:22:16-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-01:22:17-root-INFO: grad norm: 34.902 34.659 4.108
2024-12-02-01:22:17-root-INFO: Loss too large (340.240->349.676)! Learning rate decreased to 0.04384.
2024-12-02-01:22:18-root-INFO: grad norm: 32.945 32.762 3.463
2024-12-02-01:22:19-root-INFO: Loss Change: 340.240 -> 331.674
2024-12-02-01:22:19-root-INFO: Regularization Change: 0.000 -> 0.567
2024-12-02-01:22:19-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-01:22:19-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-01:22:19-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-01:22:19-root-INFO: grad norm: 32.756 32.538 3.774
2024-12-02-01:22:20-root-INFO: Loss too large (332.961->340.957)! Learning rate decreased to 0.04517.
2024-12-02-01:22:21-root-INFO: grad norm: 30.754 30.578 3.288
2024-12-02-01:22:21-root-INFO: Loss Change: 332.961 -> 324.819
2024-12-02-01:22:22-root-INFO: Regularization Change: 0.000 -> 0.527
2024-12-02-01:22:22-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-01:22:22-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-01:22:22-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-01:22:22-root-INFO: grad norm: 31.364 31.145 3.705
2024-12-02-01:22:23-root-INFO: Loss too large (326.301->334.442)! Learning rate decreased to 0.04654.
2024-12-02-01:22:24-root-INFO: grad norm: 29.793 29.624 3.173
2024-12-02-01:22:24-root-INFO: Loss Change: 326.301 -> 319.105
2024-12-02-01:22:24-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-02-01:22:24-root-INFO: Undo step: 110
2024-12-02-01:22:24-root-INFO: Undo step: 111
2024-12-02-01:22:24-root-INFO: Undo step: 112
2024-12-02-01:22:24-root-INFO: Undo step: 113
2024-12-02-01:22:24-root-INFO: Undo step: 114
2024-12-02-01:22:25-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-01:22:25-root-INFO: grad norm: 84.361 83.658 10.868
2024-12-02-01:22:26-root-INFO: grad norm: 56.702 56.279 6.914
2024-12-02-01:22:27-root-INFO: Loss Change: 555.031 -> 403.053
2024-12-02-01:22:27-root-INFO: Regularization Change: 0.000 -> 18.475
2024-12-02-01:22:27-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-01:22:27-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-01:22:27-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-01:22:27-root-INFO: grad norm: 54.551 54.257 5.652
2024-12-02-01:22:28-root-INFO: grad norm: 44.510 44.239 4.903
2024-12-02-01:22:29-root-INFO: Loss Change: 403.521 -> 362.699
2024-12-02-01:22:29-root-INFO: Regularization Change: 0.000 -> 4.004
2024-12-02-01:22:29-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-01:22:29-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-01:22:29-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-01:22:30-root-INFO: grad norm: 41.086 40.824 4.628
2024-12-02-01:22:31-root-INFO: grad norm: 41.275 41.022 4.570
2024-12-02-01:22:31-root-INFO: Loss Change: 363.503 -> 353.906
2024-12-02-01:22:31-root-INFO: Regularization Change: 0.000 -> 1.874
2024-12-02-01:22:31-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-01:22:31-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-01:22:32-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-01:22:32-root-INFO: grad norm: 46.115 45.812 5.271
2024-12-02-01:22:33-root-INFO: grad norm: 48.408 48.125 5.222
2024-12-02-01:22:34-root-INFO: Loss Change: 356.288 -> 353.910
2024-12-02-01:22:34-root-INFO: Regularization Change: 0.000 -> 1.267
2024-12-02-01:22:34-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-01:22:34-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-01:22:34-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-01:22:34-root-INFO: grad norm: 49.207 48.915 5.357
2024-12-02-01:22:35-root-INFO: grad norm: 43.945 43.670 4.908
2024-12-02-01:22:36-root-INFO: Loss Change: 355.951 -> 335.454
2024-12-02-01:22:36-root-INFO: Regularization Change: 0.000 -> 1.174
2024-12-02-01:22:36-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-01:22:36-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-01:22:36-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-01:22:37-root-INFO: grad norm: 42.664 42.369 5.010
2024-12-02-01:22:38-root-INFO: grad norm: 41.913 41.639 4.788
2024-12-02-01:22:38-root-INFO: Loss Change: 337.877 -> 330.286
2024-12-02-01:22:38-root-INFO: Regularization Change: 0.000 -> 0.936
2024-12-02-01:22:38-root-INFO: Undo step: 110
2024-12-02-01:22:38-root-INFO: Undo step: 111
2024-12-02-01:22:38-root-INFO: Undo step: 112
2024-12-02-01:22:38-root-INFO: Undo step: 113
2024-12-02-01:22:38-root-INFO: Undo step: 114
2024-12-02-01:22:39-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-01:22:39-root-INFO: grad norm: 72.375 70.920 14.442
2024-12-02-01:22:40-root-INFO: grad norm: 42.028 41.483 6.749
2024-12-02-01:22:41-root-INFO: Loss Change: 507.138 -> 366.588
2024-12-02-01:22:41-root-INFO: Regularization Change: 0.000 -> 14.619
2024-12-02-01:22:41-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-01:22:41-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-01:22:41-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-01:22:41-root-INFO: grad norm: 31.937 31.487 5.342
2024-12-02-01:22:42-root-INFO: grad norm: 31.375 31.068 4.375
2024-12-02-01:22:43-root-INFO: Loss Change: 366.472 -> 347.575
2024-12-02-01:22:43-root-INFO: Regularization Change: 0.000 -> 2.715
2024-12-02-01:22:43-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-01:22:43-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-01:22:43-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-01:22:44-root-INFO: grad norm: 35.498 35.219 4.436
2024-12-02-01:22:45-root-INFO: grad norm: 33.789 33.532 4.160
2024-12-02-01:22:45-root-INFO: Loss Change: 347.620 -> 332.995
2024-12-02-01:22:45-root-INFO: Regularization Change: 0.000 -> 1.569
2024-12-02-01:22:45-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-01:22:45-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-01:22:46-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-01:22:46-root-INFO: grad norm: 34.840 34.582 4.235
2024-12-02-01:22:47-root-INFO: grad norm: 34.397 34.132 4.261
2024-12-02-01:22:48-root-INFO: Loss Change: 334.246 -> 324.646
2024-12-02-01:22:48-root-INFO: Regularization Change: 0.000 -> 1.257
2024-12-02-01:22:48-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-01:22:48-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-01:22:48-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-01:22:48-root-INFO: grad norm: 36.380 36.114 4.393
2024-12-02-01:22:49-root-INFO: Loss too large (325.850->327.940)! Learning rate decreased to 0.04517.
2024-12-02-01:22:50-root-INFO: grad norm: 27.562 27.323 3.623
2024-12-02-01:22:50-root-INFO: Loss Change: 325.850 -> 308.342
2024-12-02-01:22:50-root-INFO: Regularization Change: 0.000 -> 0.858
2024-12-02-01:22:50-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-01:22:50-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-01:22:51-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-01:22:51-root-INFO: grad norm: 23.399 23.149 3.408
2024-12-02-01:22:51-root-INFO: Loss too large (309.039->309.965)! Learning rate decreased to 0.04654.
2024-12-02-01:22:52-root-INFO: grad norm: 20.615 20.404 2.937
2024-12-02-01:22:53-root-INFO: Loss Change: 309.039 -> 300.818
2024-12-02-01:22:53-root-INFO: Regularization Change: 0.000 -> 0.587
2024-12-02-01:22:53-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-01:22:53-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-01:22:53-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-01:22:54-root-INFO: grad norm: 21.474 21.228 3.240
2024-12-02-01:22:54-root-INFO: Loss too large (302.019->303.504)! Learning rate decreased to 0.04795.
2024-12-02-01:22:55-root-INFO: grad norm: 19.802 19.609 2.754
2024-12-02-01:22:56-root-INFO: Loss Change: 302.019 -> 295.425
2024-12-02-01:22:56-root-INFO: Regularization Change: 0.000 -> 0.533
2024-12-02-01:22:56-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-01:22:56-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-01:22:56-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-01:22:56-root-INFO: grad norm: 20.112 19.887 3.000
2024-12-02-01:22:57-root-INFO: Loss too large (295.642->297.016)! Learning rate decreased to 0.04939.
2024-12-02-01:22:58-root-INFO: grad norm: 18.662 18.482 2.589
2024-12-02-01:22:58-root-INFO: Loss Change: 295.642 -> 289.670
2024-12-02-01:22:58-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-02-01:22:58-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-01:22:58-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-01:22:59-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-01:22:59-root-INFO: grad norm: 20.665 20.434 3.085
2024-12-02-01:22:59-root-INFO: Loss too large (291.161->293.305)! Learning rate decreased to 0.05086.
2024-12-02-01:23:00-root-INFO: grad norm: 19.344 19.172 2.573
2024-12-02-01:23:01-root-INFO: Loss Change: 291.161 -> 285.506
2024-12-02-01:23:01-root-INFO: Regularization Change: 0.000 -> 0.496
2024-12-02-01:23:01-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-01:23:01-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-01:23:01-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-01:23:02-root-INFO: grad norm: 20.689 20.462 3.060
2024-12-02-01:23:02-root-INFO: Loss too large (286.245->288.589)! Learning rate decreased to 0.05237.
2024-12-02-01:23:03-root-INFO: grad norm: 19.211 19.047 2.505
2024-12-02-01:23:04-root-INFO: Loss Change: 286.245 -> 280.589
2024-12-02-01:23:04-root-INFO: Regularization Change: 0.000 -> 0.482
2024-12-02-01:23:04-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-01:23:04-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-01:23:04-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-01:23:04-root-INFO: grad norm: 20.651 20.429 3.018
2024-12-02-01:23:05-root-INFO: Loss too large (282.105->285.020)! Learning rate decreased to 0.05391.
2024-12-02-01:23:06-root-INFO: grad norm: 19.441 19.286 2.454
2024-12-02-01:23:06-root-INFO: Loss Change: 282.105 -> 276.900
2024-12-02-01:23:06-root-INFO: Regularization Change: 0.000 -> 0.466
2024-12-02-01:23:06-root-INFO: Undo step: 105
2024-12-02-01:23:06-root-INFO: Undo step: 106
2024-12-02-01:23:06-root-INFO: Undo step: 107
2024-12-02-01:23:06-root-INFO: Undo step: 108
2024-12-02-01:23:06-root-INFO: Undo step: 109
2024-12-02-01:23:07-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-01:23:07-root-INFO: grad norm: 82.018 81.252 11.182
2024-12-02-01:23:08-root-INFO: grad norm: 57.549 57.030 7.712
2024-12-02-01:23:09-root-INFO: Loss Change: 504.671 -> 362.531
2024-12-02-01:23:09-root-INFO: Regularization Change: 0.000 -> 22.463
2024-12-02-01:23:09-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-01:23:09-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-01:23:09-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-01:23:09-root-INFO: grad norm: 41.736 41.388 5.377
2024-12-02-01:23:10-root-INFO: grad norm: 34.679 34.378 4.559
2024-12-02-01:23:11-root-INFO: Loss Change: 361.325 -> 325.665
2024-12-02-01:23:11-root-INFO: Regularization Change: 0.000 -> 4.366
2024-12-02-01:23:11-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-01:23:11-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-01:23:11-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-01:23:12-root-INFO: grad norm: 36.102 35.892 3.890
2024-12-02-01:23:13-root-INFO: grad norm: 38.718 38.477 4.316
2024-12-02-01:23:13-root-INFO: Loss Change: 323.577 -> 318.935
2024-12-02-01:23:13-root-INFO: Regularization Change: 0.000 -> 1.816
2024-12-02-01:23:13-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-01:23:13-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-01:23:14-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-01:23:14-root-INFO: grad norm: 40.267 40.114 3.501
2024-12-02-01:23:14-root-INFO: Loss too large (317.495->319.419)! Learning rate decreased to 0.05086.
2024-12-02-01:23:15-root-INFO: grad norm: 28.513 28.334 3.189
2024-12-02-01:23:16-root-INFO: Loss Change: 317.495 -> 292.634
2024-12-02-01:23:16-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-02-01:23:16-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-01:23:16-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-01:23:16-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-01:23:17-root-INFO: grad norm: 18.999 18.855 2.338
2024-12-02-01:23:18-root-INFO: grad norm: 22.240 22.090 2.576
2024-12-02-01:23:18-root-INFO: Loss too large (289.404->289.775)! Learning rate decreased to 0.05237.
2024-12-02-01:23:19-root-INFO: Loss Change: 291.300 -> 284.112
2024-12-02-01:23:19-root-INFO: Regularization Change: 0.000 -> 0.875
2024-12-02-01:23:19-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-01:23:19-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-01:23:19-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-01:23:19-root-INFO: grad norm: 18.467 18.345 2.120
2024-12-02-01:23:20-root-INFO: Loss too large (283.786->284.299)! Learning rate decreased to 0.05391.
2024-12-02-01:23:21-root-INFO: grad norm: 16.937 16.804 2.113
2024-12-02-01:23:21-root-INFO: Loss Change: 283.786 -> 277.337
2024-12-02-01:23:21-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-02-01:23:21-root-INFO: Undo step: 105
2024-12-02-01:23:21-root-INFO: Undo step: 106
2024-12-02-01:23:21-root-INFO: Undo step: 107
2024-12-02-01:23:21-root-INFO: Undo step: 108
2024-12-02-01:23:21-root-INFO: Undo step: 109
2024-12-02-01:23:22-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-01:23:22-root-INFO: grad norm: 72.716 71.573 12.839
2024-12-02-01:23:23-root-INFO: grad norm: 42.006 41.522 6.359
2024-12-02-01:23:24-root-INFO: Loss Change: 502.365 -> 348.750
2024-12-02-01:23:24-root-INFO: Regularization Change: 0.000 -> 17.917
2024-12-02-01:23:24-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-01:23:24-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-01:23:24-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-01:23:24-root-INFO: grad norm: 40.401 40.076 5.113
2024-12-02-01:23:25-root-INFO: grad norm: 42.142 41.864 4.836
2024-12-02-01:23:26-root-INFO: Loss Change: 346.814 -> 330.789
2024-12-02-01:23:26-root-INFO: Regularization Change: 0.000 -> 3.393
2024-12-02-01:23:26-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-01:23:26-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-01:23:27-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-01:23:27-root-INFO: grad norm: 41.557 41.339 4.247
2024-12-02-01:23:28-root-INFO: grad norm: 40.939 40.679 4.601
2024-12-02-01:23:29-root-INFO: Loss Change: 328.675 -> 315.078
2024-12-02-01:23:29-root-INFO: Regularization Change: 0.000 -> 1.864
2024-12-02-01:23:29-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-01:23:29-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-01:23:29-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-01:23:29-root-INFO: grad norm: 37.093 36.904 3.738
2024-12-02-01:23:30-root-INFO: grad norm: 36.840 36.601 4.195
2024-12-02-01:23:31-root-INFO: Loss Change: 312.829 -> 303.961
2024-12-02-01:23:31-root-INFO: Regularization Change: 0.000 -> 1.323
2024-12-02-01:23:31-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-01:23:31-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-01:23:31-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-01:23:31-root-INFO: grad norm: 34.366 34.186 3.512
2024-12-02-01:23:32-root-INFO: grad norm: 36.879 36.628 4.298
2024-12-02-01:23:33-root-INFO: Loss too large (300.613->300.728)! Learning rate decreased to 0.05237.
2024-12-02-01:23:33-root-INFO: Loss Change: 300.998 -> 286.604
2024-12-02-01:23:33-root-INFO: Regularization Change: 0.000 -> 1.043
2024-12-02-01:23:33-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-01:23:33-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-01:23:34-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-01:23:34-root-INFO: grad norm: 24.417 24.264 2.729
2024-12-02-01:23:34-root-INFO: Loss too large (284.867->285.300)! Learning rate decreased to 0.05391.
2024-12-02-01:23:35-root-INFO: grad norm: 20.392 20.210 2.720
2024-12-02-01:23:36-root-INFO: Loss Change: 284.867 -> 274.870
2024-12-02-01:23:36-root-INFO: Regularization Change: 0.000 -> 0.691
2024-12-02-01:23:36-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-01:23:36-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-01:23:36-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-01:23:37-root-INFO: grad norm: 15.918 15.781 2.082
2024-12-02-01:23:38-root-INFO: grad norm: 21.818 21.651 2.692
2024-12-02-01:23:38-root-INFO: Loss too large (272.612->276.992)! Learning rate decreased to 0.05550.
2024-12-02-01:23:39-root-INFO: Loss Change: 272.974 -> 270.502
2024-12-02-01:23:39-root-INFO: Regularization Change: 0.000 -> 0.732
2024-12-02-01:23:39-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-01:23:39-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-01:23:39-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-01:23:39-root-INFO: grad norm: 18.648 18.521 2.173
2024-12-02-01:23:40-root-INFO: Loss too large (268.972->270.936)! Learning rate decreased to 0.05711.
2024-12-02-01:23:41-root-INFO: grad norm: 18.244 18.085 2.408
2024-12-02-01:23:41-root-INFO: Loss Change: 268.972 -> 264.500
2024-12-02-01:23:41-root-INFO: Regularization Change: 0.000 -> 0.521
2024-12-02-01:23:41-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-01:23:41-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-01:23:42-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-01:23:42-root-INFO: grad norm: 16.881 16.760 2.016
2024-12-02-01:23:42-root-INFO: Loss too large (263.007->265.660)! Learning rate decreased to 0.05877.
2024-12-02-01:23:43-root-INFO: grad norm: 18.005 17.847 2.384
2024-12-02-01:23:44-root-INFO: Loss Change: 263.007 -> 260.380
2024-12-02-01:23:44-root-INFO: Regularization Change: 0.000 -> 0.502
2024-12-02-01:23:44-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-01:23:44-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-01:23:44-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-01:23:45-root-INFO: grad norm: 17.237 17.123 1.976
2024-12-02-01:23:45-root-INFO: Loss too large (258.909->262.900)! Learning rate decreased to 0.06047.
2024-12-02-01:23:46-root-INFO: grad norm: 19.112 18.963 2.389
2024-12-02-01:23:47-root-INFO: Loss Change: 258.909 -> 257.475
2024-12-02-01:23:47-root-INFO: Regularization Change: 0.000 -> 0.491
2024-12-02-01:23:47-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-01:23:47-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-01:23:47-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-01:23:47-root-INFO: grad norm: 19.138 19.035 1.976
2024-12-02-01:23:48-root-INFO: Loss too large (256.102->262.263)! Learning rate decreased to 0.06220.
2024-12-02-01:23:49-root-INFO: grad norm: 21.297 21.150 2.506
2024-12-02-01:23:49-root-INFO: Loss Change: 256.102 -> 255.472
2024-12-02-01:23:49-root-INFO: Regularization Change: 0.000 -> 0.488
2024-12-02-01:23:49-root-INFO: Undo step: 100
2024-12-02-01:23:49-root-INFO: Undo step: 101
2024-12-02-01:23:49-root-INFO: Undo step: 102
2024-12-02-01:23:49-root-INFO: Undo step: 103
2024-12-02-01:23:49-root-INFO: Undo step: 104
2024-12-02-01:23:50-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-01:23:50-root-INFO: grad norm: 60.944 60.404 8.098
2024-12-02-01:23:51-root-INFO: grad norm: 38.814 38.351 5.978
2024-12-02-01:23:52-root-INFO: Loss Change: 477.542 -> 317.982
2024-12-02-01:23:52-root-INFO: Regularization Change: 0.000 -> 23.068
2024-12-02-01:23:52-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-01:23:52-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-01:23:52-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-01:23:52-root-INFO: grad norm: 33.235 32.910 4.632
2024-12-02-01:23:53-root-INFO: grad norm: 38.046 37.734 4.859
2024-12-02-01:23:54-root-INFO: Loss too large (305.103->308.493)! Learning rate decreased to 0.05550.
2024-12-02-01:23:54-root-INFO: Loss Change: 315.391 -> 292.781
2024-12-02-01:23:54-root-INFO: Regularization Change: 0.000 -> 3.173
2024-12-02-01:23:54-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-01:23:54-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-01:23:55-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-01:23:55-root-INFO: grad norm: 30.361 30.110 3.900
2024-12-02-01:23:55-root-INFO: Loss too large (291.294->291.545)! Learning rate decreased to 0.05711.
2024-12-02-01:23:56-root-INFO: grad norm: 25.298 25.057 3.483
2024-12-02-01:23:57-root-INFO: Loss Change: 291.294 -> 274.316
2024-12-02-01:23:57-root-INFO: Regularization Change: 0.000 -> 1.285
2024-12-02-01:23:57-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-01:23:57-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-01:23:57-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-01:23:58-root-INFO: grad norm: 21.253 21.054 2.903
2024-12-02-01:23:58-root-INFO: Loss too large (272.957->273.645)! Learning rate decreased to 0.05877.
2024-12-02-01:23:59-root-INFO: grad norm: 19.573 19.363 2.862
2024-12-02-01:24:00-root-INFO: Loss Change: 272.957 -> 264.282
2024-12-02-01:24:00-root-INFO: Regularization Change: 0.000 -> 0.873
2024-12-02-01:24:00-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-01:24:00-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-01:24:00-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-01:24:00-root-INFO: grad norm: 17.013 16.844 2.397
2024-12-02-01:24:01-root-INFO: Loss too large (263.239->263.563)! Learning rate decreased to 0.06047.
2024-12-02-01:24:02-root-INFO: grad norm: 16.370 16.185 2.452
2024-12-02-01:24:02-root-INFO: Loss Change: 263.239 -> 257.196
2024-12-02-01:24:02-root-INFO: Regularization Change: 0.000 -> 0.707
2024-12-02-01:24:02-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-01:24:02-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-01:24:03-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-01:24:03-root-INFO: grad norm: 14.533 14.389 2.042
2024-12-02-01:24:04-root-INFO: grad norm: 19.932 19.773 2.513
2024-12-02-01:24:04-root-INFO: Loss too large (256.258->259.709)! Learning rate decreased to 0.06220.
2024-12-02-01:24:05-root-INFO: Loss Change: 256.290 -> 253.681
2024-12-02-01:24:05-root-INFO: Regularization Change: 0.000 -> 0.811
2024-12-02-01:24:05-root-INFO: Undo step: 100
2024-12-02-01:24:05-root-INFO: Undo step: 101
2024-12-02-01:24:05-root-INFO: Undo step: 102
2024-12-02-01:24:05-root-INFO: Undo step: 103
2024-12-02-01:24:05-root-INFO: Undo step: 104
2024-12-02-01:24:06-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-01:24:06-root-INFO: grad norm: 65.619 65.094 8.286
2024-12-02-01:24:07-root-INFO: grad norm: 40.186 39.769 5.780
2024-12-02-01:24:07-root-INFO: Loss Change: 472.731 -> 320.653
2024-12-02-01:24:07-root-INFO: Regularization Change: 0.000 -> 22.005
2024-12-02-01:24:07-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-01:24:07-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-01:24:08-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-01:24:08-root-INFO: grad norm: 32.985 32.715 4.216
2024-12-02-01:24:09-root-INFO: grad norm: 35.398 35.179 3.933
2024-12-02-01:24:10-root-INFO: Loss Change: 319.263 -> 304.407
2024-12-02-01:24:10-root-INFO: Regularization Change: 0.000 -> 3.933
2024-12-02-01:24:10-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-01:24:10-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-01:24:10-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-01:24:10-root-INFO: grad norm: 35.783 35.620 3.410
2024-12-02-01:24:11-root-INFO: grad norm: 35.114 34.975 3.125
2024-12-02-01:24:12-root-INFO: Loss Change: 304.283 -> 292.135
2024-12-02-01:24:12-root-INFO: Regularization Change: 0.000 -> 2.284
2024-12-02-01:24:12-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-01:24:12-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-01:24:12-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-01:24:13-root-INFO: grad norm: 36.049 35.910 3.156
2024-12-02-01:24:14-root-INFO: grad norm: 37.040 36.909 3.108
2024-12-02-01:24:14-root-INFO: Loss Change: 292.741 -> 287.181
2024-12-02-01:24:14-root-INFO: Regularization Change: 0.000 -> 1.582
2024-12-02-01:24:14-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-01:24:14-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-01:24:15-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-01:24:15-root-INFO: grad norm: 38.743 38.573 3.627
2024-12-02-01:24:16-root-INFO: grad norm: 37.146 37.004 3.242
2024-12-02-01:24:17-root-INFO: Loss Change: 288.995 -> 277.636
2024-12-02-01:24:17-root-INFO: Regularization Change: 0.000 -> 1.224
2024-12-02-01:24:17-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-01:24:17-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-01:24:17-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-01:24:17-root-INFO: grad norm: 36.086 35.904 3.614
2024-12-02-01:24:18-root-INFO: grad norm: 33.564 33.430 2.998
2024-12-02-01:24:19-root-INFO: Loss Change: 279.597 -> 267.512
2024-12-02-01:24:19-root-INFO: Regularization Change: 0.000 -> 1.027
2024-12-02-01:24:19-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-01:24:19-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-01:24:19-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-01:24:20-root-INFO: grad norm: 31.884 31.724 3.192
2024-12-02-01:24:21-root-INFO: grad norm: 29.968 29.852 2.636
2024-12-02-01:24:21-root-INFO: Loss Change: 268.581 -> 259.386
2024-12-02-01:24:21-root-INFO: Regularization Change: 0.000 -> 0.888
2024-12-02-01:24:21-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-01:24:21-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-01:24:22-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-01:24:22-root-INFO: grad norm: 29.330 29.180 2.969
2024-12-02-01:24:23-root-INFO: grad norm: 28.402 28.296 2.447
2024-12-02-01:24:24-root-INFO: Loss Change: 260.769 -> 254.608
2024-12-02-01:24:24-root-INFO: Regularization Change: 0.000 -> 0.814
2024-12-02-01:24:24-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-01:24:24-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-01:24:24-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-01:24:24-root-INFO: grad norm: 28.182 28.041 2.819
2024-12-02-01:24:25-root-INFO: grad norm: 27.143 27.055 2.182
2024-12-02-01:24:26-root-INFO: Loss Change: 255.679 -> 249.663
2024-12-02-01:24:26-root-INFO: Regularization Change: 0.000 -> 0.778
2024-12-02-01:24:26-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-01:24:26-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-01:24:27-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-01:24:27-root-INFO: grad norm: 26.881 26.765 2.503
2024-12-02-01:24:28-root-INFO: grad norm: 25.881 25.815 1.855
2024-12-02-01:24:28-root-INFO: Loss Change: 250.614 -> 244.908
2024-12-02-01:24:28-root-INFO: Regularization Change: 0.000 -> 0.749
2024-12-02-01:24:28-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-01:24:28-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-01:24:29-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-01:24:29-root-INFO: grad norm: 25.838 25.739 2.265
2024-12-02-01:24:30-root-INFO: grad norm: 24.873 24.816 1.673
2024-12-02-01:24:31-root-INFO: Loss Change: 245.896 -> 240.412
2024-12-02-01:24:31-root-INFO: Regularization Change: 0.000 -> 0.727
2024-12-02-01:24:31-root-INFO: Undo step: 95
2024-12-02-01:24:31-root-INFO: Undo step: 96
2024-12-02-01:24:31-root-INFO: Undo step: 97
2024-12-02-01:24:31-root-INFO: Undo step: 98
2024-12-02-01:24:31-root-INFO: Undo step: 99
2024-12-02-01:24:31-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-01:24:31-root-INFO: grad norm: 56.427 55.793 8.433
2024-12-02-01:24:32-root-INFO: grad norm: 27.685 27.308 4.550
2024-12-02-01:24:33-root-INFO: Loss Change: 431.599 -> 279.671
2024-12-02-01:24:33-root-INFO: Regularization Change: 0.000 -> 23.344
2024-12-02-01:24:33-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-01:24:33-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-01:24:33-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-01:24:34-root-INFO: grad norm: 19.299 19.007 3.345
2024-12-02-01:24:35-root-INFO: grad norm: 17.272 17.043 2.798
2024-12-02-01:24:35-root-INFO: Loss Change: 278.302 -> 258.893
2024-12-02-01:24:35-root-INFO: Regularization Change: 0.000 -> 3.489
2024-12-02-01:24:35-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-01:24:35-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-01:24:36-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-01:24:36-root-INFO: grad norm: 17.082 16.902 2.479
2024-12-02-01:24:37-root-INFO: grad norm: 19.793 19.634 2.508
2024-12-02-01:24:38-root-INFO: Loss Change: 257.845 -> 252.995
2024-12-02-01:24:38-root-INFO: Regularization Change: 0.000 -> 1.938
2024-12-02-01:24:38-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-01:24:38-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-01:24:38-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-01:24:39-root-INFO: grad norm: 21.730 21.598 2.391
2024-12-02-01:24:40-root-INFO: grad norm: 23.384 23.235 2.640
2024-12-02-01:24:40-root-INFO: Loss Change: 251.969 -> 248.881
2024-12-02-01:24:40-root-INFO: Regularization Change: 0.000 -> 1.398
2024-12-02-01:24:40-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-01:24:40-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-01:24:41-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-01:24:41-root-INFO: grad norm: 23.795 23.648 2.649
2024-12-02-01:24:42-root-INFO: grad norm: 25.762 25.568 3.158
2024-12-02-01:24:42-root-INFO: Loss too large (246.758->247.693)! Learning rate decreased to 0.06953.
2024-12-02-01:24:43-root-INFO: Loss Change: 248.037 -> 238.355
2024-12-02-01:24:43-root-INFO: Regularization Change: 0.000 -> 1.026
2024-12-02-01:24:43-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-01:24:43-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-01:24:43-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-01:24:44-root-INFO: grad norm: 17.636 17.500 2.182
2024-12-02-01:24:44-root-INFO: Loss too large (237.195->237.268)! Learning rate decreased to 0.07147.
2024-12-02-01:24:45-root-INFO: grad norm: 14.492 14.341 2.083
2024-12-02-01:24:46-root-INFO: Loss Change: 237.195 -> 229.800
2024-12-02-01:24:46-root-INFO: Regularization Change: 0.000 -> 0.682
2024-12-02-01:24:46-root-INFO: Undo step: 95
2024-12-02-01:24:46-root-INFO: Undo step: 96
2024-12-02-01:24:46-root-INFO: Undo step: 97
2024-12-02-01:24:46-root-INFO: Undo step: 98
2024-12-02-01:24:46-root-INFO: Undo step: 99
2024-12-02-01:24:46-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-01:24:46-root-INFO: grad norm: 68.058 67.644 7.496
2024-12-02-01:24:47-root-INFO: grad norm: 46.370 46.020 5.682
2024-12-02-01:24:48-root-INFO: Loss Change: 437.797 -> 297.837
2024-12-02-01:24:48-root-INFO: Regularization Change: 0.000 -> 23.474
2024-12-02-01:24:48-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-01:24:48-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-01:24:48-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-01:24:49-root-INFO: grad norm: 32.537 32.364 3.349
2024-12-02-01:24:50-root-INFO: grad norm: 24.338 24.164 2.910
2024-12-02-01:24:50-root-INFO: Loss Change: 296.537 -> 261.453
2024-12-02-01:24:50-root-INFO: Regularization Change: 0.000 -> 4.401
2024-12-02-01:24:50-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-01:24:50-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-01:24:51-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-01:24:51-root-INFO: grad norm: 21.674 21.542 2.390
2024-12-02-01:24:52-root-INFO: grad norm: 21.266 21.119 2.493
2024-12-02-01:24:53-root-INFO: Loss Change: 260.483 -> 250.637
2024-12-02-01:24:53-root-INFO: Regularization Change: 0.000 -> 1.833
2024-12-02-01:24:53-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-01:24:53-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-01:24:53-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-01:24:54-root-INFO: grad norm: 20.996 20.879 2.214
2024-12-02-01:24:55-root-INFO: grad norm: 22.322 22.163 2.658
2024-12-02-01:24:55-root-INFO: Loss Change: 249.270 -> 245.314
2024-12-02-01:24:55-root-INFO: Regularization Change: 0.000 -> 1.332
2024-12-02-01:24:55-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-01:24:55-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-01:24:56-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-01:24:56-root-INFO: grad norm: 22.692 22.564 2.406
2024-12-02-01:24:57-root-INFO: grad norm: 24.262 24.068 3.063
2024-12-02-01:24:58-root-INFO: Loss Change: 243.983 -> 242.026
2024-12-02-01:24:58-root-INFO: Regularization Change: 0.000 -> 1.095
2024-12-02-01:24:58-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-01:24:58-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-01:24:58-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-01:24:58-root-INFO: grad norm: 23.969 23.829 2.592
2024-12-02-01:24:59-root-INFO: grad norm: 25.110 24.896 3.268
2024-12-02-01:25:00-root-INFO: Loss Change: 240.422 -> 238.457
2024-12-02-01:25:00-root-INFO: Regularization Change: 0.000 -> 0.935
2024-12-02-01:25:00-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-01:25:00-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-01:25:00-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-01:25:01-root-INFO: grad norm: 24.577 24.414 2.822
2024-12-02-01:25:02-root-INFO: grad norm: 25.571 25.344 3.397
2024-12-02-01:25:02-root-INFO: Loss Change: 236.747 -> 235.122
2024-12-02-01:25:02-root-INFO: Regularization Change: 0.000 -> 0.850
2024-12-02-01:25:02-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-01:25:02-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-01:25:03-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-01:25:03-root-INFO: grad norm: 24.675 24.491 3.005
2024-12-02-01:25:04-root-INFO: grad norm: 25.504 25.272 3.433
2024-12-02-01:25:05-root-INFO: Loss Change: 233.130 -> 231.493
2024-12-02-01:25:05-root-INFO: Regularization Change: 0.000 -> 0.793
2024-12-02-01:25:05-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-01:25:05-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-01:25:05-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-01:25:05-root-INFO: grad norm: 24.475 24.275 3.119
2024-12-02-01:25:06-root-INFO: grad norm: 25.327 25.079 3.534
2024-12-02-01:25:07-root-INFO: Loss too large (227.645->228.513)! Learning rate decreased to 0.07753.
2024-12-02-01:25:07-root-INFO: Loss Change: 229.799 -> 218.628
2024-12-02-01:25:07-root-INFO: Regularization Change: 0.000 -> 0.774
2024-12-02-01:25:07-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-01:25:07-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-01:25:08-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-01:25:08-root-INFO: grad norm: 16.091 15.930 2.269
2024-12-02-01:25:09-root-INFO: grad norm: 18.721 18.532 2.651
2024-12-02-01:25:09-root-INFO: Loss too large (216.847->219.002)! Learning rate decreased to 0.07964.
2024-12-02-01:25:10-root-INFO: Loss Change: 217.248 -> 212.905
2024-12-02-01:25:10-root-INFO: Regularization Change: 0.000 -> 0.605
2024-12-02-01:25:10-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-01:25:10-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-01:25:10-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-01:25:11-root-INFO: grad norm: 13.369 13.242 1.839
2024-12-02-01:25:11-root-INFO: Loss too large (211.851->211.981)! Learning rate decreased to 0.08180.
2024-12-02-01:25:12-root-INFO: grad norm: 11.103 10.980 1.652
2024-12-02-01:25:13-root-INFO: Loss Change: 211.851 -> 207.050
2024-12-02-01:25:13-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-02-01:25:13-root-INFO: Undo step: 90
2024-12-02-01:25:13-root-INFO: Undo step: 91
2024-12-02-01:25:13-root-INFO: Undo step: 92
2024-12-02-01:25:13-root-INFO: Undo step: 93
2024-12-02-01:25:13-root-INFO: Undo step: 94
2024-12-02-01:25:13-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-01:25:13-root-INFO: grad norm: 46.865 46.263 7.487
2024-12-02-01:25:14-root-INFO: grad norm: 37.613 37.257 5.163
2024-12-02-01:25:15-root-INFO: Loss Change: 371.081 -> 285.244
2024-12-02-01:25:15-root-INFO: Regularization Change: 0.000 -> 20.838
2024-12-02-01:25:15-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-01:25:15-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-01:25:15-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-01:25:16-root-INFO: grad norm: 37.077 36.714 5.176
2024-12-02-01:25:17-root-INFO: grad norm: 37.926 37.654 4.538
2024-12-02-01:25:17-root-INFO: Loss Change: 282.929 -> 268.178
2024-12-02-01:25:17-root-INFO: Regularization Change: 0.000 -> 4.189
2024-12-02-01:25:17-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-01:25:17-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-01:25:18-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-01:25:18-root-INFO: grad norm: 35.694 35.396 4.597
2024-12-02-01:25:19-root-INFO: grad norm: 33.787 33.551 3.987
2024-12-02-01:25:20-root-INFO: Loss Change: 265.404 -> 249.492
2024-12-02-01:25:20-root-INFO: Regularization Change: 0.000 -> 2.162
2024-12-02-01:25:20-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-01:25:20-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-01:25:20-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-01:25:20-root-INFO: grad norm: 31.037 30.782 3.970
2024-12-02-01:25:21-root-INFO: grad norm: 29.943 29.729 3.575
2024-12-02-01:25:22-root-INFO: Loss Change: 248.072 -> 238.258
2024-12-02-01:25:22-root-INFO: Regularization Change: 0.000 -> 1.429
2024-12-02-01:25:22-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-01:25:22-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-01:25:22-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-01:25:23-root-INFO: grad norm: 28.154 27.928 3.563
2024-12-02-01:25:24-root-INFO: grad norm: 27.906 27.704 3.354
2024-12-02-01:25:25-root-INFO: Loss Change: 236.639 -> 230.903
2024-12-02-01:25:25-root-INFO: Regularization Change: 0.000 -> 1.087
2024-12-02-01:25:25-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-01:25:25-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-01:25:25-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-01:25:25-root-INFO: grad norm: 26.206 26.008 3.219
2024-12-02-01:25:26-root-INFO: grad norm: 25.942 25.752 3.131
2024-12-02-01:25:27-root-INFO: Loss Change: 229.110 -> 224.132
2024-12-02-01:25:27-root-INFO: Regularization Change: 0.000 -> 0.932
2024-12-02-01:25:27-root-INFO: Undo step: 90
2024-12-02-01:25:27-root-INFO: Undo step: 91
2024-12-02-01:25:27-root-INFO: Undo step: 92
2024-12-02-01:25:27-root-INFO: Undo step: 93
2024-12-02-01:25:27-root-INFO: Undo step: 94
2024-12-02-01:25:27-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-01:25:28-root-INFO: grad norm: 60.650 59.914 9.417
2024-12-02-01:25:29-root-INFO: grad norm: 36.245 35.779 5.793
2024-12-02-01:25:29-root-INFO: Loss Change: 405.892 -> 264.638
2024-12-02-01:25:29-root-INFO: Regularization Change: 0.000 -> 29.258
2024-12-02-01:25:29-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-01:25:29-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-01:25:30-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-01:25:30-root-INFO: grad norm: 27.325 27.066 3.752
2024-12-02-01:25:31-root-INFO: grad norm: 29.126 28.947 3.223
2024-12-02-01:25:32-root-INFO: Loss Change: 262.603 -> 251.554
2024-12-02-01:25:32-root-INFO: Regularization Change: 0.000 -> 4.709
2024-12-02-01:25:32-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-01:25:32-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-01:25:32-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-01:25:32-root-INFO: grad norm: 31.645 31.523 2.777
2024-12-02-01:25:33-root-INFO: Loss too large (249.804->250.910)! Learning rate decreased to 0.07547.
2024-12-02-01:25:34-root-INFO: grad norm: 23.168 23.035 2.479
2024-12-02-01:25:34-root-INFO: Loss Change: 249.804 -> 226.525
2024-12-02-01:25:34-root-INFO: Regularization Change: 0.000 -> 1.826
2024-12-02-01:25:34-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-01:25:34-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-01:25:35-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-01:25:35-root-INFO: grad norm: 16.175 16.070 1.839
2024-12-02-01:25:36-root-INFO: grad norm: 18.905 18.806 1.937
2024-12-02-01:25:36-root-INFO: Loss too large (224.036->224.251)! Learning rate decreased to 0.07753.
2024-12-02-01:25:37-root-INFO: Loss Change: 225.744 -> 218.497
2024-12-02-01:25:37-root-INFO: Regularization Change: 0.000 -> 1.250
2024-12-02-01:25:37-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-01:25:37-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-01:25:37-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-01:25:38-root-INFO: grad norm: 14.838 14.754 1.578
2024-12-02-01:25:39-root-INFO: grad norm: 18.037 17.955 1.715
2024-12-02-01:25:39-root-INFO: Loss too large (217.450->218.404)! Learning rate decreased to 0.07964.
2024-12-02-01:25:40-root-INFO: Loss Change: 217.871 -> 212.829
2024-12-02-01:25:40-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-01:25:40-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-01:25:40-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-01:25:40-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-01:25:40-root-INFO: grad norm: 14.634 14.559 1.478
2024-12-02-01:25:41-root-INFO: Loss too large (212.653->212.669)! Learning rate decreased to 0.08180.
2024-12-02-01:25:42-root-INFO: grad norm: 12.344 12.263 1.410
2024-12-02-01:25:42-root-INFO: Loss Change: 212.653 -> 206.428
2024-12-02-01:25:42-root-INFO: Regularization Change: 0.000 -> 0.709
2024-12-02-01:25:42-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-01:25:42-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-01:25:43-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-01:25:43-root-INFO: grad norm: 10.599 10.497 1.464
2024-12-02-01:25:44-root-INFO: grad norm: 13.181 13.102 1.440
2024-12-02-01:25:44-root-INFO: Loss too large (205.066->205.636)! Learning rate decreased to 0.08399.
2024-12-02-01:25:45-root-INFO: Loss Change: 206.027 -> 202.550
2024-12-02-01:25:45-root-INFO: Regularization Change: 0.000 -> 0.799
2024-12-02-01:25:45-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-01:25:45-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-01:25:45-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-01:25:46-root-INFO: grad norm: 12.182 12.093 1.476
2024-12-02-01:25:46-root-INFO: Loss too large (203.035->203.192)! Learning rate decreased to 0.08623.
2024-12-02-01:25:47-root-INFO: grad norm: 10.848 10.766 1.336
2024-12-02-01:25:48-root-INFO: Loss Change: 203.035 -> 198.642
2024-12-02-01:25:48-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-02-01:25:48-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-01:25:48-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-01:25:48-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-01:25:48-root-INFO: grad norm: 11.163 11.042 1.641
2024-12-02-01:25:49-root-INFO: Loss too large (198.883->198.983)! Learning rate decreased to 0.08852.
2024-12-02-01:25:50-root-INFO: grad norm: 10.249 10.154 1.392
2024-12-02-01:25:50-root-INFO: Loss Change: 198.883 -> 195.096
2024-12-02-01:25:50-root-INFO: Regularization Change: 0.000 -> 0.578
2024-12-02-01:25:50-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-01:25:50-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-01:25:51-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-01:25:51-root-INFO: grad norm: 11.185 11.054 1.704
2024-12-02-01:25:51-root-INFO: Loss too large (195.826->196.504)! Learning rate decreased to 0.09086.
2024-12-02-01:25:52-root-INFO: grad norm: 10.727 10.624 1.486
2024-12-02-01:25:53-root-INFO: Loss Change: 195.826 -> 192.613
2024-12-02-01:25:53-root-INFO: Regularization Change: 0.000 -> 0.553
2024-12-02-01:25:53-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-01:25:53-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-01:25:53-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-01:25:54-root-INFO: grad norm: 12.239 12.101 1.833
2024-12-02-01:25:54-root-INFO: Loss too large (193.350->195.160)! Learning rate decreased to 0.09324.
2024-12-02-01:25:55-root-INFO: grad norm: 12.189 12.079 1.632
2024-12-02-01:25:56-root-INFO: Loss Change: 193.350 -> 190.696
2024-12-02-01:25:56-root-INFO: Regularization Change: 0.000 -> 0.551
2024-12-02-01:25:56-root-INFO: Undo step: 85
2024-12-02-01:25:56-root-INFO: Undo step: 86
2024-12-02-01:25:56-root-INFO: Undo step: 87
2024-12-02-01:25:56-root-INFO: Undo step: 88
2024-12-02-01:25:56-root-INFO: Undo step: 89
2024-12-02-01:25:56-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-01:25:56-root-INFO: grad norm: 48.603 48.159 6.549
2024-12-02-01:25:57-root-INFO: grad norm: 32.639 32.338 4.422
2024-12-02-01:25:58-root-INFO: Loss Change: 354.090 -> 246.304
2024-12-02-01:25:58-root-INFO: Regularization Change: 0.000 -> 27.006
2024-12-02-01:25:58-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-01:25:58-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-01:25:58-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-01:25:59-root-INFO: grad norm: 26.393 26.211 3.096
2024-12-02-01:26:00-root-INFO: grad norm: 23.001 22.776 3.208
2024-12-02-01:26:00-root-INFO: Loss Change: 246.865 -> 223.909
2024-12-02-01:26:00-root-INFO: Regularization Change: 0.000 -> 4.943
2024-12-02-01:26:00-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-01:26:00-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-01:26:01-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-01:26:01-root-INFO: grad norm: 24.902 24.735 2.880
2024-12-02-01:26:02-root-INFO: grad norm: 27.134 26.884 3.676
2024-12-02-01:26:02-root-INFO: Loss too large (224.757->225.942)! Learning rate decreased to 0.08623.
2024-12-02-01:26:03-root-INFO: Loss Change: 225.875 -> 212.678
2024-12-02-01:26:03-root-INFO: Regularization Change: 0.000 -> 2.097
2024-12-02-01:26:03-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-01:26:03-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-01:26:03-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-01:26:04-root-INFO: grad norm: 22.758 22.552 3.056
2024-12-02-01:26:04-root-INFO: Loss too large (214.773->218.397)! Learning rate decreased to 0.08852.
2024-12-02-01:26:05-root-INFO: grad norm: 19.301 19.099 2.788
2024-12-02-01:26:06-root-INFO: Loss Change: 214.773 -> 203.842
2024-12-02-01:26:06-root-INFO: Regularization Change: 0.000 -> 1.161
2024-12-02-01:26:06-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-01:26:06-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-01:26:06-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-01:26:06-root-INFO: grad norm: 18.875 18.693 2.618
2024-12-02-01:26:07-root-INFO: Loss too large (205.632->208.851)! Learning rate decreased to 0.09086.
2024-12-02-01:26:08-root-INFO: grad norm: 16.721 16.552 2.368
2024-12-02-01:26:08-root-INFO: Loss Change: 205.632 -> 198.287
2024-12-02-01:26:08-root-INFO: Regularization Change: 0.000 -> 0.842
2024-12-02-01:26:08-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-01:26:08-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-01:26:09-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-01:26:09-root-INFO: grad norm: 16.958 16.791 2.379
2024-12-02-01:26:09-root-INFO: Loss too large (199.653->202.573)! Learning rate decreased to 0.09324.
2024-12-02-01:26:10-root-INFO: grad norm: 15.304 15.155 2.131
2024-12-02-01:26:11-root-INFO: Loss Change: 199.653 -> 193.752
2024-12-02-01:26:11-root-INFO: Regularization Change: 0.000 -> 0.730
2024-12-02-01:26:11-root-INFO: Undo step: 85
2024-12-02-01:26:11-root-INFO: Undo step: 86
2024-12-02-01:26:11-root-INFO: Undo step: 87
2024-12-02-01:26:11-root-INFO: Undo step: 88
2024-12-02-01:26:11-root-INFO: Undo step: 89
2024-12-02-01:26:11-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-01:26:12-root-INFO: grad norm: 45.674 45.339 5.522
2024-12-02-01:26:13-root-INFO: grad norm: 27.012 26.749 3.761
2024-12-02-01:26:13-root-INFO: Loss Change: 372.510 -> 244.730
2024-12-02-01:26:13-root-INFO: Regularization Change: 0.000 -> 29.508
2024-12-02-01:26:13-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-01:26:13-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-01:26:14-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-01:26:14-root-INFO: grad norm: 25.286 25.067 3.318
2024-12-02-01:26:15-root-INFO: grad norm: 25.479 25.189 3.833
2024-12-02-01:26:16-root-INFO: Loss Change: 246.299 -> 230.767
2024-12-02-01:26:16-root-INFO: Regularization Change: 0.000 -> 4.965
2024-12-02-01:26:16-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-01:26:16-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-01:26:16-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-01:26:16-root-INFO: grad norm: 28.856 28.564 4.090
2024-12-02-01:26:17-root-INFO: grad norm: 29.605 29.249 4.579
2024-12-02-01:26:18-root-INFO: Loss Change: 233.233 -> 226.887
2024-12-02-01:26:18-root-INFO: Regularization Change: 0.000 -> 2.533
2024-12-02-01:26:18-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-01:26:18-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-01:26:18-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-01:26:19-root-INFO: grad norm: 32.446 32.080 4.857
2024-12-02-01:26:19-root-INFO: Loss too large (230.664->231.260)! Learning rate decreased to 0.08852.
2024-12-02-01:26:20-root-INFO: grad norm: 21.989 21.693 3.600
2024-12-02-01:26:21-root-INFO: Loss Change: 230.664 -> 203.311
2024-12-02-01:26:21-root-INFO: Regularization Change: 0.000 -> 1.979
2024-12-02-01:26:21-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-01:26:21-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-01:26:21-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-01:26:21-root-INFO: grad norm: 17.215 16.996 2.735
2024-12-02-01:26:22-root-INFO: grad norm: 19.662 19.418 3.088
2024-12-02-01:26:23-root-INFO: Loss too large (204.521->205.277)! Learning rate decreased to 0.09086.
2024-12-02-01:26:23-root-INFO: Loss Change: 204.825 -> 197.861
2024-12-02-01:26:23-root-INFO: Regularization Change: 0.000 -> 1.097
2024-12-02-01:26:24-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-01:26:24-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-01:26:24-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-01:26:24-root-INFO: grad norm: 16.584 16.374 2.630
2024-12-02-01:26:24-root-INFO: Loss too large (199.068->199.617)! Learning rate decreased to 0.09324.
2024-12-02-01:26:25-root-INFO: grad norm: 13.133 12.962 2.110
2024-12-02-01:26:26-root-INFO: Loss Change: 199.068 -> 190.964
2024-12-02-01:26:26-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-01:26:26-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-01:26:26-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-01:26:26-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-01:26:27-root-INFO: grad norm: 12.135 11.975 1.962
2024-12-02-01:26:28-root-INFO: grad norm: 14.912 14.739 2.265
2024-12-02-01:26:28-root-INFO: Loss too large (191.396->192.726)! Learning rate decreased to 0.09566.
2024-12-02-01:26:29-root-INFO: Loss Change: 191.599 -> 188.020
2024-12-02-01:26:29-root-INFO: Regularization Change: 0.000 -> 0.847
2024-12-02-01:26:29-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-01:26:29-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-01:26:29-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-01:26:29-root-INFO: grad norm: 14.348 14.165 2.284
2024-12-02-01:26:30-root-INFO: Loss too large (189.301->190.342)! Learning rate decreased to 0.09814.
2024-12-02-01:26:31-root-INFO: grad norm: 11.965 11.821 1.852
2024-12-02-01:26:31-root-INFO: Loss Change: 189.301 -> 183.422
2024-12-02-01:26:31-root-INFO: Regularization Change: 0.000 -> 0.687
2024-12-02-01:26:31-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-01:26:31-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-01:26:32-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-01:26:32-root-INFO: grad norm: 11.638 11.494 1.823
2024-12-02-01:26:32-root-INFO: Loss too large (184.262->184.657)! Learning rate decreased to 0.10066.
2024-12-02-01:26:33-root-INFO: grad norm: 10.048 9.929 1.544
2024-12-02-01:26:34-root-INFO: Loss Change: 184.262 -> 179.787
2024-12-02-01:26:34-root-INFO: Regularization Change: 0.000 -> 0.629
2024-12-02-01:26:34-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-01:26:34-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-01:26:34-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-01:26:35-root-INFO: grad norm: 10.715 10.570 1.754
2024-12-02-01:26:35-root-INFO: Loss too large (180.549->180.836)! Learning rate decreased to 0.10323.
2024-12-02-01:26:36-root-INFO: grad norm: 9.408 9.301 1.415
2024-12-02-01:26:37-root-INFO: Loss Change: 180.549 -> 176.553
2024-12-02-01:26:37-root-INFO: Regularization Change: 0.000 -> 0.612
2024-12-02-01:26:37-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-01:26:37-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-01:26:37-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-01:26:37-root-INFO: grad norm: 10.694 10.552 1.734
2024-12-02-01:26:38-root-INFO: Loss too large (177.830->178.343)! Learning rate decreased to 0.10585.
2024-12-02-01:26:39-root-INFO: grad norm: 9.415 9.311 1.390
2024-12-02-01:26:39-root-INFO: Loss Change: 177.830 -> 173.942
2024-12-02-01:26:39-root-INFO: Regularization Change: 0.000 -> 0.609
2024-12-02-01:26:40-root-INFO: Undo step: 80
2024-12-02-01:26:40-root-INFO: Undo step: 81
2024-12-02-01:26:40-root-INFO: Undo step: 82
2024-12-02-01:26:40-root-INFO: Undo step: 83
2024-12-02-01:26:40-root-INFO: Undo step: 84
2024-12-02-01:26:40-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-01:26:40-root-INFO: grad norm: 49.476 48.992 6.903
2024-12-02-01:26:41-root-INFO: grad norm: 29.960 29.723 3.757
2024-12-02-01:26:42-root-INFO: Loss Change: 331.498 -> 226.311
2024-12-02-01:26:42-root-INFO: Regularization Change: 0.000 -> 33.420
2024-12-02-01:26:42-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-01:26:42-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-01:26:42-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-01:26:43-root-INFO: grad norm: 23.375 23.151 3.226
2024-12-02-01:26:44-root-INFO: grad norm: 19.770 19.603 2.564
2024-12-02-01:26:44-root-INFO: Loss Change: 226.003 -> 203.712
2024-12-02-01:26:44-root-INFO: Regularization Change: 0.000 -> 4.637
2024-12-02-01:26:44-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-01:26:44-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-01:26:45-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-01:26:45-root-INFO: grad norm: 17.572 17.449 2.075
2024-12-02-01:26:46-root-INFO: grad norm: 16.481 16.365 1.951
2024-12-02-01:26:47-root-INFO: Loss Change: 203.670 -> 194.028
2024-12-02-01:26:47-root-INFO: Regularization Change: 0.000 -> 2.298
2024-12-02-01:26:47-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-01:26:47-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-01:26:47-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-01:26:47-root-INFO: grad norm: 16.742 16.642 1.828
2024-12-02-01:26:48-root-INFO: grad norm: 17.399 17.290 1.944
2024-12-02-01:26:49-root-INFO: Loss Change: 194.826 -> 191.201
2024-12-02-01:26:49-root-INFO: Regularization Change: 0.000 -> 1.680
2024-12-02-01:26:49-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-01:26:49-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-01:26:50-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-01:26:50-root-INFO: grad norm: 19.488 19.341 2.385
2024-12-02-01:26:51-root-INFO: grad norm: 20.908 20.739 2.651
2024-12-02-01:26:51-root-INFO: Loss too large (190.649->192.109)! Learning rate decreased to 0.10323.
2024-12-02-01:26:52-root-INFO: Loss Change: 192.439 -> 183.140
2024-12-02-01:26:52-root-INFO: Regularization Change: 0.000 -> 1.314
2024-12-02-01:26:52-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-01:26:52-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-01:26:52-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-01:26:53-root-INFO: grad norm: 16.806 16.644 2.327
2024-12-02-01:26:54-root-INFO: grad norm: 19.021 18.841 2.610
2024-12-02-01:26:54-root-INFO: Loss too large (184.286->186.741)! Learning rate decreased to 0.10585.
2024-12-02-01:26:55-root-INFO: Loss Change: 185.104 -> 178.628
2024-12-02-01:26:55-root-INFO: Regularization Change: 0.000 -> 1.020
2024-12-02-01:26:55-root-INFO: Undo step: 80
2024-12-02-01:26:55-root-INFO: Undo step: 81
2024-12-02-01:26:55-root-INFO: Undo step: 82
2024-12-02-01:26:55-root-INFO: Undo step: 83
2024-12-02-01:26:55-root-INFO: Undo step: 84
2024-12-02-01:26:55-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-01:26:55-root-INFO: grad norm: 39.942 39.583 5.345
2024-12-02-01:26:56-root-INFO: grad norm: 20.218 19.948 3.293
2024-12-02-01:26:57-root-INFO: Loss Change: 329.992 -> 215.709
2024-12-02-01:26:57-root-INFO: Regularization Change: 0.000 -> 27.717
2024-12-02-01:26:57-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-01:26:57-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-01:26:57-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-01:26:58-root-INFO: grad norm: 14.819 14.616 2.445
2024-12-02-01:26:59-root-INFO: grad norm: 12.920 12.766 1.984
2024-12-02-01:26:59-root-INFO: Loss Change: 214.834 -> 197.406
2024-12-02-01:26:59-root-INFO: Regularization Change: 0.000 -> 4.507
2024-12-02-01:26:59-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-01:26:59-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-01:27:00-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-01:27:00-root-INFO: grad norm: 11.914 11.777 1.796
2024-12-02-01:27:01-root-INFO: grad norm: 12.682 12.571 1.668
2024-12-02-01:27:02-root-INFO: Loss Change: 196.442 -> 190.292
2024-12-02-01:27:02-root-INFO: Regularization Change: 0.000 -> 2.351
2024-12-02-01:27:02-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-01:27:02-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-01:27:02-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-01:27:02-root-INFO: grad norm: 14.360 14.258 1.705
2024-12-02-01:27:03-root-INFO: Loss too large (189.510->190.240)! Learning rate decreased to 0.10066.
2024-12-02-01:27:04-root-INFO: grad norm: 12.445 12.342 1.594
2024-12-02-01:27:04-root-INFO: Loss Change: 189.510 -> 182.583
2024-12-02-01:27:04-root-INFO: Regularization Change: 0.000 -> 1.099
2024-12-02-01:27:04-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-01:27:04-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-01:27:05-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-01:27:05-root-INFO: grad norm: 9.729 9.642 1.296
2024-12-02-01:27:06-root-INFO: grad norm: 12.130 12.034 1.525
2024-12-02-01:27:06-root-INFO: Loss too large (181.048->181.065)! Learning rate decreased to 0.10323.
2024-12-02-01:27:07-root-INFO: Loss Change: 181.546 -> 177.782
2024-12-02-01:27:07-root-INFO: Regularization Change: 0.000 -> 1.086
2024-12-02-01:27:07-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-01:27:07-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-01:27:07-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-01:27:08-root-INFO: grad norm: 9.074 8.993 1.216
2024-12-02-01:27:09-root-INFO: grad norm: 11.421 11.329 1.451
2024-12-02-01:27:09-root-INFO: Loss too large (176.558->176.742)! Learning rate decreased to 0.10585.
2024-12-02-01:27:10-root-INFO: Loss Change: 176.873 -> 173.699
2024-12-02-01:27:10-root-INFO: Regularization Change: 0.000 -> 0.958
2024-12-02-01:27:10-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-01:27:10-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-01:27:10-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-01:27:10-root-INFO: grad norm: 9.196 9.114 1.224
2024-12-02-01:27:11-root-INFO: Loss too large (172.877->172.928)! Learning rate decreased to 0.10852.
2024-12-02-01:27:12-root-INFO: grad norm: 8.547 8.464 1.194
2024-12-02-01:27:12-root-INFO: Loss Change: 172.877 -> 169.286
2024-12-02-01:27:12-root-INFO: Regularization Change: 0.000 -> 0.699
2024-12-02-01:27:12-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-01:27:12-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-01:27:13-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-01:27:13-root-INFO: grad norm: 6.867 6.796 0.988
2024-12-02-01:27:14-root-INFO: grad norm: 8.712 8.636 1.154
2024-12-02-01:27:15-root-INFO: Loss Change: 168.943 -> 168.055
2024-12-02-01:27:15-root-INFO: Regularization Change: 0.000 -> 1.109
2024-12-02-01:27:15-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-01:27:15-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-01:27:15-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-01:27:15-root-INFO: grad norm: 10.862 10.793 1.227
2024-12-02-01:27:16-root-INFO: Loss too large (167.097->168.378)! Learning rate decreased to 0.11401.
2024-12-02-01:27:17-root-INFO: grad norm: 9.578 9.500 1.215
2024-12-02-01:27:17-root-INFO: Loss Change: 167.097 -> 163.167
2024-12-02-01:27:17-root-INFO: Regularization Change: 0.000 -> 0.659
2024-12-02-01:27:17-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-01:27:17-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-01:27:18-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-01:27:18-root-INFO: grad norm: 7.085 7.020 0.957
2024-12-02-01:27:19-root-INFO: grad norm: 8.862 8.791 1.114
2024-12-02-01:27:19-root-INFO: Loss too large (161.889->161.914)! Learning rate decreased to 0.11682.
2024-12-02-01:27:20-root-INFO: Loss Change: 162.402 -> 159.958
2024-12-02-01:27:20-root-INFO: Regularization Change: 0.000 -> 0.776
2024-12-02-01:27:20-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-01:27:20-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-01:27:20-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-01:27:21-root-INFO: grad norm: 6.968 6.908 0.913
2024-12-02-01:27:22-root-INFO: grad norm: 8.755 8.691 1.061
2024-12-02-01:27:22-root-INFO: Loss too large (159.033->159.147)! Learning rate decreased to 0.11969.
2024-12-02-01:27:23-root-INFO: Loss Change: 159.451 -> 157.158
2024-12-02-01:27:23-root-INFO: Regularization Change: 0.000 -> 0.760
2024-12-02-01:27:23-root-INFO: Undo step: 75
2024-12-02-01:27:23-root-INFO: Undo step: 76
2024-12-02-01:27:23-root-INFO: Undo step: 77
2024-12-02-01:27:23-root-INFO: Undo step: 78
2024-12-02-01:27:23-root-INFO: Undo step: 79
2024-12-02-01:27:23-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-01:27:24-root-INFO: grad norm: 39.156 38.808 5.206
2024-12-02-01:27:25-root-INFO: grad norm: 27.076 26.859 3.420
2024-12-02-01:27:25-root-INFO: Loss Change: 315.387 -> 208.476
2024-12-02-01:27:25-root-INFO: Regularization Change: 0.000 -> 29.658
2024-12-02-01:27:25-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-01:27:25-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-01:27:26-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-01:27:26-root-INFO: grad norm: 21.519 21.282 3.184
2024-12-02-01:27:27-root-INFO: grad norm: 20.188 19.983 2.873
2024-12-02-01:27:27-root-INFO: Loss Change: 206.411 -> 188.995
2024-12-02-01:27:27-root-INFO: Regularization Change: 0.000 -> 5.100
2024-12-02-01:27:27-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-01:27:27-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-01:27:28-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-01:27:28-root-INFO: grad norm: 19.068 18.899 2.533
2024-12-02-01:27:29-root-INFO: grad norm: 19.147 18.966 2.627
2024-12-02-01:27:30-root-INFO: Loss Change: 187.562 -> 180.311
2024-12-02-01:27:30-root-INFO: Regularization Change: 0.000 -> 2.446
2024-12-02-01:27:30-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-01:27:30-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-01:27:30-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-01:27:30-root-INFO: grad norm: 18.000 17.845 2.355
2024-12-02-01:27:31-root-INFO: grad norm: 17.866 17.702 2.419
2024-12-02-01:27:32-root-INFO: Loss Change: 178.459 -> 172.831
2024-12-02-01:27:32-root-INFO: Regularization Change: 0.000 -> 1.705
2024-12-02-01:27:32-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-01:27:32-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-01:27:32-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-01:27:33-root-INFO: grad norm: 16.371 16.228 2.158
2024-12-02-01:27:34-root-INFO: grad norm: 16.584 16.432 2.244
2024-12-02-01:27:34-root-INFO: Loss Change: 170.826 -> 167.156
2024-12-02-01:27:34-root-INFO: Regularization Change: 0.000 -> 1.355
2024-12-02-01:27:34-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-01:27:34-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-01:27:35-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-01:27:35-root-INFO: grad norm: 15.526 15.389 2.054
2024-12-02-01:27:36-root-INFO: grad norm: 15.913 15.765 2.172
2024-12-02-01:27:37-root-INFO: Loss Change: 165.646 -> 163.149
2024-12-02-01:27:37-root-INFO: Regularization Change: 0.000 -> 1.201
2024-12-02-01:27:37-root-INFO: Undo step: 75
2024-12-02-01:27:37-root-INFO: Undo step: 76
2024-12-02-01:27:37-root-INFO: Undo step: 77
2024-12-02-01:27:37-root-INFO: Undo step: 78
2024-12-02-01:27:37-root-INFO: Undo step: 79
2024-12-02-01:27:37-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-01:27:37-root-INFO: grad norm: 38.495 38.075 5.668
2024-12-02-01:27:38-root-INFO: grad norm: 23.440 23.200 3.347
2024-12-02-01:27:39-root-INFO: Loss Change: 310.605 -> 200.313
2024-12-02-01:27:39-root-INFO: Regularization Change: 0.000 -> 28.396
2024-12-02-01:27:39-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-01:27:39-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-01:27:40-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-01:27:40-root-INFO: grad norm: 19.599 19.402 2.771
2024-12-02-01:27:41-root-INFO: grad norm: 19.512 19.316 2.763
2024-12-02-01:27:41-root-INFO: Loss Change: 198.482 -> 185.151
2024-12-02-01:27:41-root-INFO: Regularization Change: 0.000 -> 4.805
2024-12-02-01:27:42-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-01:27:42-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-01:27:42-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-01:27:42-root-INFO: grad norm: 19.324 19.168 2.450
2024-12-02-01:27:43-root-INFO: grad norm: 20.588 20.401 2.766
2024-12-02-01:27:44-root-INFO: Loss Change: 183.774 -> 180.153
2024-12-02-01:27:44-root-INFO: Regularization Change: 0.000 -> 2.562
2024-12-02-01:27:44-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-01:27:44-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-01:27:44-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-01:27:44-root-INFO: grad norm: 20.271 20.102 2.605
2024-12-02-01:27:45-root-INFO: grad norm: 20.955 20.754 2.894
2024-12-02-01:27:46-root-INFO: Loss Change: 178.293 -> 174.993
2024-12-02-01:27:46-root-INFO: Regularization Change: 0.000 -> 1.778
2024-12-02-01:27:46-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-01:27:46-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-01:27:47-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-01:27:47-root-INFO: grad norm: 20.102 19.922 2.682
2024-12-02-01:27:48-root-INFO: grad norm: 20.553 20.339 2.958
2024-12-02-01:27:49-root-INFO: Loss Change: 172.978 -> 170.219
2024-12-02-01:27:49-root-INFO: Regularization Change: 0.000 -> 1.479
2024-12-02-01:27:49-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-01:27:49-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-01:27:49-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-01:27:49-root-INFO: grad norm: 19.594 19.410 2.682
2024-12-02-01:27:50-root-INFO: grad norm: 19.673 19.441 3.013
2024-12-02-01:27:51-root-INFO: Loss Change: 168.580 -> 165.031
2024-12-02-01:27:51-root-INFO: Regularization Change: 0.000 -> 1.249
2024-12-02-01:27:51-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-01:27:51-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-01:27:51-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-01:27:52-root-INFO: grad norm: 18.696 18.498 2.711
2024-12-02-01:27:53-root-INFO: grad norm: 20.152 19.906 3.141
2024-12-02-01:27:53-root-INFO: Loss Change: 163.510 -> 161.837
2024-12-02-01:27:53-root-INFO: Regularization Change: 0.000 -> 2.040
2024-12-02-01:27:53-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-01:27:53-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-01:27:54-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-01:27:54-root-INFO: grad norm: 17.572 17.376 2.617
2024-12-02-01:27:55-root-INFO: grad norm: 17.007 16.776 2.794
2024-12-02-01:27:55-root-INFO: Loss Change: 160.590 -> 155.547
2024-12-02-01:27:55-root-INFO: Regularization Change: 0.000 -> 1.257
2024-12-02-01:27:55-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-01:27:55-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-01:27:56-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-01:27:56-root-INFO: grad norm: 15.406 15.240 2.255
2024-12-02-01:27:57-root-INFO: grad norm: 15.125 14.918 2.491
2024-12-02-01:27:58-root-INFO: Loss Change: 154.076 -> 150.182
2024-12-02-01:27:58-root-INFO: Regularization Change: 0.000 -> 1.086
2024-12-02-01:27:58-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-01:27:58-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-01:27:58-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-01:27:58-root-INFO: grad norm: 14.342 14.187 2.097
2024-12-02-01:27:59-root-INFO: grad norm: 14.242 14.054 2.302
2024-12-02-01:28:00-root-INFO: Loss Change: 149.602 -> 146.486
2024-12-02-01:28:00-root-INFO: Regularization Change: 0.000 -> 0.989
2024-12-02-01:28:00-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-01:28:00-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-01:28:00-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-01:28:01-root-INFO: grad norm: 13.028 12.894 1.863
2024-12-02-01:28:02-root-INFO: grad norm: 13.936 13.763 2.190
2024-12-02-01:28:02-root-INFO: Loss Change: 144.941 -> 143.712
2024-12-02-01:28:02-root-INFO: Regularization Change: 0.000 -> 1.428
2024-12-02-01:28:02-root-INFO: Undo step: 70
2024-12-02-01:28:02-root-INFO: Undo step: 71
2024-12-02-01:28:02-root-INFO: Undo step: 72
2024-12-02-01:28:02-root-INFO: Undo step: 73
2024-12-02-01:28:02-root-INFO: Undo step: 74
2024-12-02-01:28:03-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-01:28:03-root-INFO: grad norm: 34.164 33.967 3.671
2024-12-02-01:28:04-root-INFO: grad norm: 20.119 19.969 2.448
2024-12-02-01:28:05-root-INFO: Loss Change: 268.321 -> 171.854
2024-12-02-01:28:05-root-INFO: Regularization Change: 0.000 -> 29.073
2024-12-02-01:28:05-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-01:28:05-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-01:28:05-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-01:28:05-root-INFO: grad norm: 13.405 13.251 2.025
2024-12-02-01:28:06-root-INFO: grad norm: 13.526 13.406 1.798
2024-12-02-01:28:07-root-INFO: Loss Change: 170.736 -> 159.356
2024-12-02-01:28:07-root-INFO: Regularization Change: 0.000 -> 4.620
2024-12-02-01:28:07-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-01:28:07-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-01:28:07-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-01:28:08-root-INFO: grad norm: 12.474 12.379 1.537
2024-12-02-01:28:09-root-INFO: grad norm: 12.552 12.463 1.494
2024-12-02-01:28:09-root-INFO: Loss Change: 158.068 -> 151.467
2024-12-02-01:28:09-root-INFO: Regularization Change: 0.000 -> 2.681
2024-12-02-01:28:09-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-01:28:09-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-01:28:10-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-01:28:10-root-INFO: grad norm: 13.866 13.809 1.257
2024-12-02-01:28:11-root-INFO: grad norm: 11.777 11.698 1.363
2024-12-02-01:28:12-root-INFO: Loss Change: 150.544 -> 146.767
2024-12-02-01:28:12-root-INFO: Regularization Change: 0.000 -> 2.628
2024-12-02-01:28:12-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-01:28:12-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-01:28:12-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-01:28:12-root-INFO: grad norm: 10.956 10.882 1.271
2024-12-02-01:28:13-root-INFO: grad norm: 11.033 10.945 1.397
2024-12-02-01:28:14-root-INFO: Loss Change: 146.023 -> 142.275
2024-12-02-01:28:14-root-INFO: Regularization Change: 0.000 -> 1.397
2024-12-02-01:28:14-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-01:28:14-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-01:28:14-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-01:28:15-root-INFO: grad norm: 9.929 9.865 1.129
2024-12-02-01:28:16-root-INFO: grad norm: 10.151 10.063 1.337
2024-12-02-01:28:16-root-INFO: Loss Change: 140.798 -> 137.539
2024-12-02-01:28:16-root-INFO: Regularization Change: 0.000 -> 1.347
2024-12-02-01:28:17-root-INFO: Undo step: 70
2024-12-02-01:28:17-root-INFO: Undo step: 71
2024-12-02-01:28:17-root-INFO: Undo step: 72
2024-12-02-01:28:17-root-INFO: Undo step: 73
2024-12-02-01:28:17-root-INFO: Undo step: 74
2024-12-02-01:28:17-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-01:28:17-root-INFO: grad norm: 33.140 32.819 4.602
2024-12-02-01:28:18-root-INFO: grad norm: 21.953 21.747 3.004
2024-12-02-01:28:19-root-INFO: Loss Change: 269.161 -> 178.281
2024-12-02-01:28:19-root-INFO: Regularization Change: 0.000 -> 27.992
2024-12-02-01:28:19-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-01:28:19-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-01:28:19-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-01:28:19-root-INFO: grad norm: 15.972 15.773 2.514
2024-12-02-01:28:20-root-INFO: grad norm: 14.929 14.777 2.126
2024-12-02-01:28:21-root-INFO: Loss Change: 176.980 -> 161.065
2024-12-02-01:28:21-root-INFO: Regularization Change: 0.000 -> 5.571
2024-12-02-01:28:21-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-01:28:21-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-01:28:21-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-01:28:22-root-INFO: grad norm: 13.185 13.058 1.824
2024-12-02-01:28:23-root-INFO: grad norm: 12.584 12.484 1.581
2024-12-02-01:28:23-root-INFO: Loss Change: 159.831 -> 151.080
2024-12-02-01:28:23-root-INFO: Regularization Change: 0.000 -> 2.926
2024-12-02-01:28:23-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-01:28:23-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-01:28:24-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-01:28:24-root-INFO: grad norm: 11.862 11.779 1.396
2024-12-02-01:28:25-root-INFO: grad norm: 12.018 11.940 1.367
2024-12-02-01:28:26-root-INFO: Loss Change: 149.924 -> 145.401
2024-12-02-01:28:26-root-INFO: Regularization Change: 0.000 -> 1.720
2024-12-02-01:28:26-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-01:28:26-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-01:28:26-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-01:28:26-root-INFO: grad norm: 11.869 11.799 1.287
2024-12-02-01:28:27-root-INFO: grad norm: 11.890 11.816 1.322
2024-12-02-01:28:28-root-INFO: Loss Change: 144.802 -> 141.263
2024-12-02-01:28:28-root-INFO: Regularization Change: 0.000 -> 1.372
2024-12-02-01:28:28-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-01:28:28-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-01:28:28-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-01:28:29-root-INFO: grad norm: 10.728 10.665 1.160
2024-12-02-01:28:30-root-INFO: grad norm: 10.932 10.855 1.290
2024-12-02-01:28:30-root-INFO: Loss Change: 139.898 -> 137.125
2024-12-02-01:28:30-root-INFO: Regularization Change: 0.000 -> 1.267
2024-12-02-01:28:30-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-01:28:30-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-01:28:31-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-01:28:31-root-INFO: grad norm: 10.713 10.651 1.154
2024-12-02-01:28:32-root-INFO: grad norm: 10.983 10.911 1.255
2024-12-02-01:28:33-root-INFO: Loss Change: 136.573 -> 134.224
2024-12-02-01:28:33-root-INFO: Regularization Change: 0.000 -> 1.064
2024-12-02-01:28:33-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-01:28:33-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-01:28:33-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-01:28:33-root-INFO: grad norm: 10.203 10.144 1.099
2024-12-02-01:28:34-root-INFO: grad norm: 9.851 9.788 1.113
2024-12-02-01:28:35-root-INFO: Loss Change: 133.152 -> 129.991
2024-12-02-01:28:35-root-INFO: Regularization Change: 0.000 -> 1.024
2024-12-02-01:28:35-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-01:28:35-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-01:28:35-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-01:28:36-root-INFO: grad norm: 9.057 9.001 1.005
2024-12-02-01:28:37-root-INFO: grad norm: 9.534 9.468 1.118
2024-12-02-01:28:37-root-INFO: Loss Change: 129.331 -> 128.082
2024-12-02-01:28:37-root-INFO: Regularization Change: 0.000 -> 1.000
2024-12-02-01:28:37-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-01:28:37-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-01:28:38-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-01:28:38-root-INFO: grad norm: 11.584 11.542 0.982
2024-12-02-01:28:39-root-INFO: grad norm: 8.692 8.637 0.969
2024-12-02-01:28:40-root-INFO: Loss Change: 127.854 -> 125.703
2024-12-02-01:28:40-root-INFO: Regularization Change: 0.000 -> 2.407
2024-12-02-01:28:40-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-01:28:40-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-01:28:40-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-01:28:40-root-INFO: grad norm: 7.814 7.764 0.877
2024-12-02-01:28:41-root-INFO: grad norm: 8.187 8.127 0.987
2024-12-02-01:28:42-root-INFO: Loss Change: 124.859 -> 122.753
2024-12-02-01:28:42-root-INFO: Regularization Change: 0.000 -> 1.148
2024-12-02-01:28:42-root-INFO: Undo step: 65
2024-12-02-01:28:42-root-INFO: Undo step: 66
2024-12-02-01:28:42-root-INFO: Undo step: 67
2024-12-02-01:28:42-root-INFO: Undo step: 68
2024-12-02-01:28:42-root-INFO: Undo step: 69
2024-12-02-01:28:42-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-01:28:43-root-INFO: grad norm: 32.169 31.884 4.274
2024-12-02-01:28:44-root-INFO: grad norm: 20.622 20.433 2.789
2024-12-02-01:28:44-root-INFO: Loss Change: 243.912 -> 156.872
2024-12-02-01:28:44-root-INFO: Regularization Change: 0.000 -> 30.841
2024-12-02-01:28:44-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-01:28:44-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-01:28:45-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-01:28:45-root-INFO: grad norm: 12.750 12.598 1.965
2024-12-02-01:28:46-root-INFO: grad norm: 11.608 11.453 1.892
2024-12-02-01:28:47-root-INFO: Loss Change: 156.159 -> 143.040
2024-12-02-01:28:47-root-INFO: Regularization Change: 0.000 -> 5.122
2024-12-02-01:28:47-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-01:28:47-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-01:28:47-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-01:28:47-root-INFO: grad norm: 10.488 10.367 1.589
2024-12-02-01:28:48-root-INFO: grad norm: 11.023 10.871 1.822
2024-12-02-01:28:49-root-INFO: Loss Change: 141.323 -> 136.458
2024-12-02-01:28:49-root-INFO: Regularization Change: 0.000 -> 2.444
2024-12-02-01:28:49-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-01:28:49-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-01:28:49-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-01:28:50-root-INFO: grad norm: 10.888 10.759 1.667
2024-12-02-01:28:51-root-INFO: grad norm: 11.759 11.594 1.960
2024-12-02-01:28:51-root-INFO: Loss Change: 135.406 -> 133.120
2024-12-02-01:28:51-root-INFO: Regularization Change: 0.000 -> 1.730
2024-12-02-01:28:51-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-01:28:51-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-01:28:52-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-01:28:52-root-INFO: grad norm: 11.655 11.507 1.850
2024-12-02-01:28:53-root-INFO: grad norm: 12.441 12.273 2.036
2024-12-02-01:28:54-root-INFO: Loss Change: 132.253 -> 130.843
2024-12-02-01:28:54-root-INFO: Regularization Change: 0.000 -> 1.445
2024-12-02-01:28:54-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-01:28:54-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-01:28:54-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-01:28:54-root-INFO: grad norm: 11.731 11.582 1.866
2024-12-02-01:28:55-root-INFO: grad norm: 12.118 11.956 1.974
2024-12-02-01:28:56-root-INFO: Loss Change: 129.344 -> 127.294
2024-12-02-01:28:56-root-INFO: Regularization Change: 0.000 -> 1.194
2024-12-02-01:28:56-root-INFO: Undo step: 65
2024-12-02-01:28:56-root-INFO: Undo step: 66
2024-12-02-01:28:56-root-INFO: Undo step: 67
2024-12-02-01:28:56-root-INFO: Undo step: 68
2024-12-02-01:28:56-root-INFO: Undo step: 69
2024-12-02-01:28:56-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-01:28:57-root-INFO: grad norm: 29.133 28.912 3.582
2024-12-02-01:28:58-root-INFO: grad norm: 14.562 14.389 2.236
2024-12-02-01:28:58-root-INFO: Loss Change: 250.929 -> 154.492
2024-12-02-01:28:58-root-INFO: Regularization Change: 0.000 -> 34.053
2024-12-02-01:28:58-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-01:28:58-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-01:28:59-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-01:28:59-root-INFO: grad norm: 11.660 11.532 1.724
2024-12-02-01:29:00-root-INFO: grad norm: 11.843 11.706 1.794
2024-12-02-01:29:01-root-INFO: Loss Change: 153.875 -> 142.600
2024-12-02-01:29:01-root-INFO: Regularization Change: 0.000 -> 5.345
2024-12-02-01:29:01-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-01:29:01-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-01:29:01-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-01:29:01-root-INFO: grad norm: 11.751 11.644 1.582
2024-12-02-01:29:02-root-INFO: grad norm: 12.687 12.553 1.840
2024-12-02-01:29:03-root-INFO: Loss Change: 140.958 -> 136.631
2024-12-02-01:29:03-root-INFO: Regularization Change: 0.000 -> 2.819
2024-12-02-01:29:03-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-01:29:03-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-01:29:03-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-01:29:04-root-INFO: grad norm: 12.839 12.708 1.829
2024-12-02-01:29:05-root-INFO: grad norm: 14.203 14.044 2.120
2024-12-02-01:29:05-root-INFO: Loss Change: 135.579 -> 134.716
2024-12-02-01:29:05-root-INFO: Regularization Change: 0.000 -> 2.103
2024-12-02-01:29:05-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-01:29:05-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-01:29:06-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-01:29:06-root-INFO: grad norm: 14.233 14.083 2.060
2024-12-02-01:29:07-root-INFO: grad norm: 14.416 14.264 2.086
2024-12-02-01:29:08-root-INFO: Loss Change: 133.757 -> 130.594
2024-12-02-01:29:08-root-INFO: Regularization Change: 0.000 -> 1.496
2024-12-02-01:29:08-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-01:29:08-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-01:29:08-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-01:29:08-root-INFO: grad norm: 13.495 13.362 1.896
2024-12-02-01:29:09-root-INFO: grad norm: 14.507 14.366 2.018
2024-12-02-01:29:10-root-INFO: Loss Change: 129.050 -> 127.437
2024-12-02-01:29:10-root-INFO: Regularization Change: 0.000 -> 1.594
2024-12-02-01:29:10-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-01:29:10-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-01:29:10-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-01:29:11-root-INFO: grad norm: 12.658 12.533 1.777
2024-12-02-01:29:12-root-INFO: grad norm: 12.340 12.224 1.681
2024-12-02-01:29:12-root-INFO: Loss Change: 126.377 -> 121.790
2024-12-02-01:29:12-root-INFO: Regularization Change: 0.000 -> 1.544
2024-12-02-01:29:12-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-01:29:12-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-01:29:13-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-01:29:13-root-INFO: grad norm: 11.241 11.142 1.494
2024-12-02-01:29:14-root-INFO: grad norm: 11.215 11.105 1.566
2024-12-02-01:29:15-root-INFO: Loss Change: 120.886 -> 118.329
2024-12-02-01:29:15-root-INFO: Regularization Change: 0.000 -> 1.103
2024-12-02-01:29:15-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-01:29:15-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-01:29:15-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-01:29:15-root-INFO: grad norm: 10.402 10.311 1.371
2024-12-02-01:29:16-root-INFO: grad norm: 10.846 10.744 1.486
2024-12-02-01:29:17-root-INFO: Loss Change: 117.056 -> 115.537
2024-12-02-01:29:17-root-INFO: Regularization Change: 0.000 -> 1.077
2024-12-02-01:29:17-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-01:29:17-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-01:29:17-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-01:29:18-root-INFO: grad norm: 9.904 9.813 1.339
2024-12-02-01:29:19-root-INFO: grad norm: 9.546 9.459 1.290
2024-12-02-01:29:19-root-INFO: Loss Change: 115.013 -> 111.413
2024-12-02-01:29:19-root-INFO: Regularization Change: 0.000 -> 1.372
2024-12-02-01:29:19-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-01:29:19-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-01:29:20-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-01:29:20-root-INFO: grad norm: 8.769 8.705 1.059
2024-12-02-01:29:21-root-INFO: grad norm: 8.759 8.689 1.102
2024-12-02-01:29:22-root-INFO: Loss Change: 110.674 -> 108.736
2024-12-02-01:29:22-root-INFO: Regularization Change: 0.000 -> 0.949
2024-12-02-01:29:22-root-INFO: Undo step: 60
2024-12-02-01:29:22-root-INFO: Undo step: 61
2024-12-02-01:29:22-root-INFO: Undo step: 62
2024-12-02-01:29:22-root-INFO: Undo step: 63
2024-12-02-01:29:22-root-INFO: Undo step: 64
2024-12-02-01:29:22-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-01:29:22-root-INFO: grad norm: 28.317 28.139 3.172
2024-12-02-01:29:23-root-INFO: grad norm: 14.164 14.044 1.841
2024-12-02-01:29:24-root-INFO: Loss Change: 230.608 -> 140.145
2024-12-02-01:29:24-root-INFO: Regularization Change: 0.000 -> 39.438
2024-12-02-01:29:24-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-01:29:24-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-01:29:24-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-01:29:25-root-INFO: grad norm: 12.315 12.208 1.621
2024-12-02-01:29:26-root-INFO: grad norm: 12.668 12.579 1.498
2024-12-02-01:29:26-root-INFO: Loss Change: 139.280 -> 129.293
2024-12-02-01:29:26-root-INFO: Regularization Change: 0.000 -> 5.501
2024-12-02-01:29:26-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-01:29:26-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-01:29:27-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-01:29:27-root-INFO: grad norm: 12.530 12.449 1.430
2024-12-02-01:29:28-root-INFO: grad norm: 12.546 12.457 1.493
2024-12-02-01:29:29-root-INFO: Loss Change: 128.454 -> 122.494
2024-12-02-01:29:29-root-INFO: Regularization Change: 0.000 -> 2.854
2024-12-02-01:29:29-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-01:29:29-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-01:29:29-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-01:29:29-root-INFO: grad norm: 11.748 11.667 1.383
2024-12-02-01:29:30-root-INFO: grad norm: 11.690 11.596 1.476
2024-12-02-01:29:31-root-INFO: Loss Change: 121.199 -> 117.246
2024-12-02-01:29:31-root-INFO: Regularization Change: 0.000 -> 1.834
2024-12-02-01:29:31-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-01:29:31-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-01:29:31-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-01:29:32-root-INFO: grad norm: 11.186 11.094 1.433
2024-12-02-01:29:33-root-INFO: grad norm: 11.192 11.098 1.447
2024-12-02-01:29:33-root-INFO: Loss Change: 116.714 -> 113.892
2024-12-02-01:29:33-root-INFO: Regularization Change: 0.000 -> 1.376
2024-12-02-01:29:33-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-01:29:33-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-01:29:34-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-01:29:34-root-INFO: grad norm: 11.238 11.166 1.265
2024-12-02-01:29:35-root-INFO: grad norm: 10.504 10.421 1.311
2024-12-02-01:29:36-root-INFO: Loss Change: 113.073 -> 110.599
2024-12-02-01:29:36-root-INFO: Regularization Change: 0.000 -> 1.532
2024-12-02-01:29:36-root-INFO: Undo step: 60
2024-12-02-01:29:36-root-INFO: Undo step: 61
2024-12-02-01:29:36-root-INFO: Undo step: 62
2024-12-02-01:29:36-root-INFO: Undo step: 63
2024-12-02-01:29:36-root-INFO: Undo step: 64
2024-12-02-01:29:36-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-01:29:36-root-INFO: grad norm: 28.346 28.146 3.357
2024-12-02-01:29:37-root-INFO: grad norm: 15.363 15.217 2.116
2024-12-02-01:29:38-root-INFO: Loss Change: 228.092 -> 139.692
2024-12-02-01:29:38-root-INFO: Regularization Change: 0.000 -> 34.426
2024-12-02-01:29:38-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-01:29:38-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-01:29:38-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-01:29:39-root-INFO: grad norm: 11.305 11.157 1.822
2024-12-02-01:29:40-root-INFO: grad norm: 10.596 10.445 1.779
2024-12-02-01:29:40-root-INFO: Loss Change: 138.636 -> 126.149
2024-12-02-01:29:40-root-INFO: Regularization Change: 0.000 -> 5.439
2024-12-02-01:29:40-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-01:29:40-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-01:29:41-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-01:29:41-root-INFO: grad norm: 9.495 9.346 1.672
2024-12-02-01:29:42-root-INFO: grad norm: 9.927 9.769 1.766
2024-12-02-01:29:43-root-INFO: Loss Change: 125.185 -> 120.525
2024-12-02-01:29:43-root-INFO: Regularization Change: 0.000 -> 2.739
2024-12-02-01:29:43-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-01:29:43-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-01:29:43-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-01:29:43-root-INFO: grad norm: 9.165 9.008 1.690
2024-12-02-01:29:44-root-INFO: grad norm: 9.571 9.416 1.715
2024-12-02-01:29:45-root-INFO: Loss Change: 119.164 -> 115.979
2024-12-02-01:29:45-root-INFO: Regularization Change: 0.000 -> 1.852
2024-12-02-01:29:45-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-01:29:45-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-01:29:45-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-01:29:46-root-INFO: grad norm: 9.139 8.991 1.642
2024-12-02-01:29:47-root-INFO: grad norm: 9.393 9.246 1.658
2024-12-02-01:29:47-root-INFO: Loss Change: 115.283 -> 112.563
2024-12-02-01:29:47-root-INFO: Regularization Change: 0.000 -> 1.451
2024-12-02-01:29:47-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-01:29:47-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-01:29:48-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-01:29:48-root-INFO: grad norm: 8.742 8.618 1.464
2024-12-02-01:29:49-root-INFO: grad norm: 9.350 9.218 1.568
2024-12-02-01:29:49-root-INFO: Loss too large (109.427->109.768)! Learning rate decreased to 0.16894.
2024-12-02-01:29:50-root-INFO: Loss Change: 111.581 -> 107.300
2024-12-02-01:29:50-root-INFO: Regularization Change: 0.000 -> 1.120
2024-12-02-01:29:50-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-01:29:50-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-01:29:50-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-01:29:51-root-INFO: grad norm: 5.781 5.712 0.891
2024-12-02-01:29:52-root-INFO: grad norm: 5.839 5.752 1.003
2024-12-02-01:29:52-root-INFO: Loss Change: 106.691 -> 103.889
2024-12-02-01:29:52-root-INFO: Regularization Change: 0.000 -> 1.307
2024-12-02-01:29:52-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-01:29:52-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-01:29:53-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-01:29:53-root-INFO: grad norm: 5.672 5.607 0.856
2024-12-02-01:29:54-root-INFO: grad norm: 8.770 8.696 1.144
2024-12-02-01:29:54-root-INFO: Loss too large (102.697->103.878)! Learning rate decreased to 0.17641.
2024-12-02-01:29:55-root-INFO: Loss Change: 103.252 -> 102.374
2024-12-02-01:29:55-root-INFO: Regularization Change: 0.000 -> 1.491
2024-12-02-01:29:55-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-01:29:55-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-01:29:55-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-01:29:56-root-INFO: grad norm: 4.847 4.800 0.676
2024-12-02-01:29:57-root-INFO: grad norm: 5.002 4.951 0.712
2024-12-02-01:29:57-root-INFO: Loss Change: 101.760 -> 98.491
2024-12-02-01:29:57-root-INFO: Regularization Change: 0.000 -> 1.646
2024-12-02-01:29:57-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-01:29:57-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-01:29:58-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-01:29:58-root-INFO: grad norm: 4.561 4.524 0.585
2024-12-02-01:29:59-root-INFO: grad norm: 5.363 5.316 0.704
2024-12-02-01:29:59-root-INFO: Loss too large (96.883->97.164)! Learning rate decreased to 0.18408.
2024-12-02-01:30:00-root-INFO: Loss Change: 98.175 -> 96.313
2024-12-02-01:30:00-root-INFO: Regularization Change: 0.000 -> 0.981
2024-12-02-01:30:00-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-01:30:00-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-01:30:00-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-01:30:01-root-INFO: grad norm: 4.381 4.350 0.524
2024-12-02-01:30:02-root-INFO: grad norm: 6.504 6.469 0.666
2024-12-02-01:30:02-root-INFO: Loss too large (95.079->95.831)! Learning rate decreased to 0.18800.
2024-12-02-01:30:03-root-INFO: Loss Change: 95.963 -> 94.941
2024-12-02-01:30:03-root-INFO: Regularization Change: 0.000 -> 0.937
2024-12-02-01:30:03-root-INFO: Undo step: 55
2024-12-02-01:30:03-root-INFO: Undo step: 56
2024-12-02-01:30:03-root-INFO: Undo step: 57
2024-12-02-01:30:03-root-INFO: Undo step: 58
2024-12-02-01:30:03-root-INFO: Undo step: 59
2024-12-02-01:30:03-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-01:30:03-root-INFO: grad norm: 23.290 23.105 2.927
2024-12-02-01:30:04-root-INFO: grad norm: 12.240 12.117 1.729
2024-12-02-01:30:05-root-INFO: Loss Change: 199.944 -> 122.479
2024-12-02-01:30:05-root-INFO: Regularization Change: 0.000 -> 34.344
2024-12-02-01:30:05-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-01:30:05-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-01:30:05-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-01:30:06-root-INFO: grad norm: 9.191 9.124 1.107
2024-12-02-01:30:07-root-INFO: grad norm: 7.865 7.796 1.041
2024-12-02-01:30:07-root-INFO: Loss Change: 121.894 -> 109.877
2024-12-02-01:30:07-root-INFO: Regularization Change: 0.000 -> 5.330
2024-12-02-01:30:07-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-01:30:07-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-01:30:08-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-01:30:08-root-INFO: grad norm: 6.953 6.910 0.771
2024-12-02-01:30:09-root-INFO: grad norm: 5.998 5.950 0.751
2024-12-02-01:30:10-root-INFO: Loss Change: 109.442 -> 103.439
2024-12-02-01:30:10-root-INFO: Regularization Change: 0.000 -> 2.606
2024-12-02-01:30:10-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-01:30:10-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-01:30:10-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-01:30:10-root-INFO: grad norm: 4.862 4.823 0.617
2024-12-02-01:30:11-root-INFO: grad norm: 4.461 4.421 0.592
2024-12-02-01:30:12-root-INFO: Loss Change: 102.977 -> 98.998
2024-12-02-01:30:12-root-INFO: Regularization Change: 0.000 -> 1.830
2024-12-02-01:30:12-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-01:30:12-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-01:30:12-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-01:30:13-root-INFO: grad norm: 4.582 4.537 0.640
2024-12-02-01:30:14-root-INFO: grad norm: 5.339 5.298 0.659
2024-12-02-01:30:14-root-INFO: Loss too large (97.808->98.014)! Learning rate decreased to 0.18408.
2024-12-02-01:30:15-root-INFO: Loss Change: 98.916 -> 96.796
2024-12-02-01:30:15-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-02-01:30:15-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-01:30:15-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-01:30:15-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-01:30:15-root-INFO: grad norm: 7.533 7.478 0.906
2024-12-02-01:30:16-root-INFO: Loss too large (97.056->97.219)! Learning rate decreased to 0.18800.
2024-12-02-01:30:17-root-INFO: grad norm: 4.387 4.339 0.650
2024-12-02-01:30:17-root-INFO: Loss Change: 97.056 -> 94.379
2024-12-02-01:30:17-root-INFO: Regularization Change: 0.000 -> 1.632
2024-12-02-01:30:17-root-INFO: Undo step: 55
2024-12-02-01:30:17-root-INFO: Undo step: 56
2024-12-02-01:30:17-root-INFO: Undo step: 57
2024-12-02-01:30:17-root-INFO: Undo step: 58
2024-12-02-01:30:17-root-INFO: Undo step: 59
2024-12-02-01:30:18-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-01:30:18-root-INFO: grad norm: 22.711 22.459 3.372
2024-12-02-01:30:19-root-INFO: grad norm: 12.566 12.378 2.164
2024-12-02-01:30:20-root-INFO: Loss Change: 200.156 -> 124.643
2024-12-02-01:30:20-root-INFO: Regularization Change: 0.000 -> 34.938
2024-12-02-01:30:20-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-01:30:20-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-01:30:20-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-01:30:20-root-INFO: grad norm: 9.495 9.255 2.121
2024-12-02-01:30:21-root-INFO: grad norm: 8.717 8.557 1.659
2024-12-02-01:30:22-root-INFO: Loss Change: 124.197 -> 112.699
2024-12-02-01:30:22-root-INFO: Regularization Change: 0.000 -> 5.671
2024-12-02-01:30:22-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-01:30:22-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-01:30:22-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-01:30:23-root-INFO: grad norm: 7.607 7.437 1.598
2024-12-02-01:30:24-root-INFO: grad norm: 6.733 6.618 1.242
2024-12-02-01:30:24-root-INFO: Loss Change: 112.233 -> 105.710
2024-12-02-01:30:24-root-INFO: Regularization Change: 0.000 -> 3.012
2024-12-02-01:30:24-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-01:30:24-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-01:30:25-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-01:30:25-root-INFO: grad norm: 8.302 8.215 1.202
2024-12-02-01:30:26-root-INFO: grad norm: 6.442 6.363 1.008
2024-12-02-01:30:27-root-INFO: Loss Change: 105.529 -> 101.570
2024-12-02-01:30:27-root-INFO: Regularization Change: 0.000 -> 3.023
2024-12-02-01:30:27-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-01:30:27-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-01:30:27-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-01:30:27-root-INFO: grad norm: 5.976 5.909 0.893
2024-12-02-01:30:28-root-INFO: grad norm: 6.142 6.082 0.854
2024-12-02-01:30:29-root-INFO: Loss Change: 101.401 -> 97.943
2024-12-02-01:30:29-root-INFO: Regularization Change: 0.000 -> 2.059
2024-12-02-01:30:29-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-01:30:29-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-01:30:29-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-01:30:30-root-INFO: grad norm: 6.295 6.240 0.830
2024-12-02-01:30:31-root-INFO: grad norm: 6.271 6.221 0.793
2024-12-02-01:30:31-root-INFO: Loss Change: 97.918 -> 95.296
2024-12-02-01:30:31-root-INFO: Regularization Change: 0.000 -> 1.481
2024-12-02-01:30:31-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-01:30:31-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-01:30:32-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-01:30:32-root-INFO: grad norm: 7.741 7.679 0.977
2024-12-02-01:30:33-root-INFO: grad norm: 7.318 7.260 0.923
2024-12-02-01:30:34-root-INFO: Loss Change: 95.487 -> 93.211
2024-12-02-01:30:34-root-INFO: Regularization Change: 0.000 -> 1.573
2024-12-02-01:30:34-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-01:30:34-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-01:30:34-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-01:30:34-root-INFO: grad norm: 7.592 7.526 0.998
2024-12-02-01:30:35-root-INFO: grad norm: 7.463 7.405 0.926
2024-12-02-01:30:36-root-INFO: Loss Change: 93.564 -> 91.060
2024-12-02-01:30:36-root-INFO: Regularization Change: 0.000 -> 1.338
2024-12-02-01:30:36-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-01:30:36-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-01:30:36-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-01:30:37-root-INFO: grad norm: 8.030 7.960 1.054
2024-12-02-01:30:38-root-INFO: grad norm: 8.142 8.073 1.062
2024-12-02-01:30:38-root-INFO: Loss Change: 91.471 -> 89.956
2024-12-02-01:30:38-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-02-01:30:38-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-01:30:38-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-01:30:39-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-01:30:39-root-INFO: grad norm: 9.892 9.802 1.333
2024-12-02-01:30:40-root-INFO: grad norm: 8.413 8.330 1.178
2024-12-02-01:30:41-root-INFO: Loss Change: 90.650 -> 87.727
2024-12-02-01:30:41-root-INFO: Regularization Change: 0.000 -> 2.092
2024-12-02-01:30:41-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-01:30:41-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-01:30:41-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-01:30:41-root-INFO: grad norm: 8.155 8.079 1.106
2024-12-02-01:30:42-root-INFO: grad norm: 7.456 7.388 1.007
2024-12-02-01:30:43-root-INFO: Loss Change: 87.943 -> 84.219
2024-12-02-01:30:43-root-INFO: Regularization Change: 0.000 -> 1.669
2024-12-02-01:30:43-root-INFO: Undo step: 50
2024-12-02-01:30:43-root-INFO: Undo step: 51
2024-12-02-01:30:43-root-INFO: Undo step: 52
2024-12-02-01:30:43-root-INFO: Undo step: 53
2024-12-02-01:30:43-root-INFO: Undo step: 54
2024-12-02-01:30:43-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-01:30:44-root-INFO: grad norm: 21.269 21.089 2.760
2024-12-02-01:30:45-root-INFO: grad norm: 10.441 10.312 1.631
2024-12-02-01:30:45-root-INFO: Loss Change: 174.758 -> 107.260
2024-12-02-01:30:45-root-INFO: Regularization Change: 0.000 -> 35.154
2024-12-02-01:30:45-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-01:30:45-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-01:30:46-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-01:30:46-root-INFO: grad norm: 8.813 8.686 1.489
2024-12-02-01:30:47-root-INFO: grad norm: 7.915 7.808 1.294
2024-12-02-01:30:48-root-INFO: Loss Change: 107.469 -> 97.668
2024-12-02-01:30:48-root-INFO: Regularization Change: 0.000 -> 5.619
2024-12-02-01:30:48-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-01:30:48-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-01:30:48-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-01:30:48-root-INFO: grad norm: 8.997 8.868 1.520
2024-12-02-01:30:49-root-INFO: grad norm: 8.622 8.498 1.456
2024-12-02-01:30:50-root-INFO: Loss Change: 98.325 -> 93.827
2024-12-02-01:30:50-root-INFO: Regularization Change: 0.000 -> 3.095
2024-12-02-01:30:50-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-01:30:50-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-01:30:50-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-01:30:51-root-INFO: grad norm: 9.634 9.506 1.566
2024-12-02-01:30:52-root-INFO: grad norm: 8.586 8.467 1.424
2024-12-02-01:30:53-root-INFO: Loss Change: 94.479 -> 89.964
2024-12-02-01:30:53-root-INFO: Regularization Change: 0.000 -> 2.608
2024-12-02-01:30:53-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-01:30:53-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-01:30:53-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-01:30:53-root-INFO: grad norm: 9.152 9.021 1.546
2024-12-02-01:30:54-root-INFO: grad norm: 8.645 8.522 1.455
2024-12-02-01:30:55-root-INFO: Loss Change: 90.761 -> 87.057
2024-12-02-01:30:55-root-INFO: Regularization Change: 0.000 -> 1.917
2024-12-02-01:30:55-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-01:30:55-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-01:30:55-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-01:30:56-root-INFO: grad norm: 9.138 9.003 1.565
2024-12-02-01:30:57-root-INFO: grad norm: 8.708 8.585 1.460
2024-12-02-01:30:57-root-INFO: Loss Change: 87.575 -> 84.461
2024-12-02-01:30:57-root-INFO: Regularization Change: 0.000 -> 1.480
2024-12-02-01:30:57-root-INFO: Undo step: 50
2024-12-02-01:30:57-root-INFO: Undo step: 51
2024-12-02-01:30:57-root-INFO: Undo step: 52
2024-12-02-01:30:57-root-INFO: Undo step: 53
2024-12-02-01:30:57-root-INFO: Undo step: 54
2024-12-02-01:30:58-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-01:30:58-root-INFO: grad norm: 23.382 23.213 2.803
2024-12-02-01:30:59-root-INFO: grad norm: 11.674 11.543 1.743
2024-12-02-01:31:00-root-INFO: Loss Change: 182.506 -> 108.406
2024-12-02-01:31:00-root-INFO: Regularization Change: 0.000 -> 38.926
2024-12-02-01:31:00-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-01:31:00-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-01:31:00-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-01:31:00-root-INFO: grad norm: 7.928 7.841 1.172
2024-12-02-01:31:01-root-INFO: grad norm: 6.665 6.608 0.867
2024-12-02-01:31:02-root-INFO: Loss Change: 108.085 -> 95.855
2024-12-02-01:31:02-root-INFO: Regularization Change: 0.000 -> 6.292
2024-12-02-01:31:02-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-01:31:02-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-01:31:02-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-01:31:03-root-INFO: grad norm: 6.588 6.513 0.990
2024-12-02-01:31:04-root-INFO: grad norm: 6.165 6.119 0.755
2024-12-02-01:31:04-root-INFO: Loss Change: 95.893 -> 90.064
2024-12-02-01:31:04-root-INFO: Regularization Change: 0.000 -> 3.088
2024-12-02-01:31:04-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-01:31:04-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-01:31:04-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-01:31:05-root-INFO: grad norm: 6.579 6.501 1.013
2024-12-02-01:31:06-root-INFO: grad norm: 6.259 6.203 0.834
2024-12-02-01:31:06-root-INFO: Loss Change: 90.190 -> 86.154
2024-12-02-01:31:06-root-INFO: Regularization Change: 0.000 -> 2.064
2024-12-02-01:31:06-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-01:31:06-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-01:31:07-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-01:31:07-root-INFO: grad norm: 6.670 6.565 1.175
2024-12-02-01:31:08-root-INFO: grad norm: 6.682 6.609 0.988
2024-12-02-01:31:09-root-INFO: Loss Change: 86.487 -> 83.835
2024-12-02-01:31:09-root-INFO: Regularization Change: 0.000 -> 1.656
2024-12-02-01:31:09-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-01:31:09-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-01:31:09-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-01:31:09-root-INFO: grad norm: 8.381 8.270 1.358
2024-12-02-01:31:10-root-INFO: Loss too large (84.135->84.265)! Learning rate decreased to 0.20837.
2024-12-02-01:31:11-root-INFO: grad norm: 5.246 5.185 0.794
2024-12-02-01:31:11-root-INFO: Loss Change: 84.135 -> 79.555
2024-12-02-01:31:11-root-INFO: Regularization Change: 0.000 -> 1.506
2024-12-02-01:31:11-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-01:31:11-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-01:31:12-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-01:31:12-root-INFO: grad norm: 4.266 4.200 0.749
2024-12-02-01:31:13-root-INFO: grad norm: 4.100 4.060 0.577
2024-12-02-01:31:14-root-INFO: Loss Change: 79.647 -> 77.220
2024-12-02-01:31:14-root-INFO: Regularization Change: 0.000 -> 1.411
2024-12-02-01:31:14-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-01:31:14-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-01:31:14-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-01:31:14-root-INFO: grad norm: 5.130 5.064 0.819
2024-12-02-01:31:15-root-INFO: grad norm: 5.498 5.454 0.696
2024-12-02-01:31:16-root-INFO: Loss Change: 77.382 -> 75.835
2024-12-02-01:31:16-root-INFO: Regularization Change: 0.000 -> 1.342
2024-12-02-01:31:16-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-01:31:16-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-01:31:16-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-01:31:17-root-INFO: grad norm: 5.517 5.446 0.883
2024-12-02-01:31:18-root-INFO: grad norm: 4.911 4.854 0.743
2024-12-02-01:31:18-root-INFO: Loss Change: 76.094 -> 73.049
2024-12-02-01:31:18-root-INFO: Regularization Change: 0.000 -> 1.345
2024-12-02-01:31:18-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-01:31:18-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-01:31:19-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-01:31:19-root-INFO: grad norm: 5.494 5.407 0.973
2024-12-02-01:31:20-root-INFO: grad norm: 5.268 5.199 0.852
2024-12-02-01:31:21-root-INFO: Loss Change: 73.336 -> 71.201
2024-12-02-01:31:21-root-INFO: Regularization Change: 0.000 -> 1.150
2024-12-02-01:31:21-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-01:31:21-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-01:31:21-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-01:31:22-root-INFO: grad norm: 5.736 5.641 1.040
2024-12-02-01:31:23-root-INFO: grad norm: 5.480 5.400 0.937
2024-12-02-01:31:23-root-INFO: Loss Change: 71.514 -> 69.394
2024-12-02-01:31:23-root-INFO: Regularization Change: 0.000 -> 1.136
2024-12-02-01:31:23-root-INFO: Undo step: 45
2024-12-02-01:31:23-root-INFO: Undo step: 46
2024-12-02-01:31:23-root-INFO: Undo step: 47
2024-12-02-01:31:23-root-INFO: Undo step: 48
2024-12-02-01:31:23-root-INFO: Undo step: 49
2024-12-02-01:31:24-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-01:31:24-root-INFO: grad norm: 19.428 19.312 2.127
2024-12-02-01:31:25-root-INFO: grad norm: 10.798 10.717 1.321
2024-12-02-01:31:26-root-INFO: Loss Change: 158.803 -> 96.592
2024-12-02-01:31:26-root-INFO: Regularization Change: 0.000 -> 35.971
2024-12-02-01:31:26-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-01:31:26-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-01:31:26-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-01:31:26-root-INFO: grad norm: 8.089 8.023 1.028
2024-12-02-01:31:27-root-INFO: grad norm: 6.877 6.829 0.817
2024-12-02-01:31:28-root-INFO: Loss Change: 95.976 -> 84.372
2024-12-02-01:31:28-root-INFO: Regularization Change: 0.000 -> 6.672
2024-12-02-01:31:28-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-01:31:28-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-01:31:28-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-01:31:29-root-INFO: grad norm: 5.814 5.763 0.766
2024-12-02-01:31:30-root-INFO: grad norm: 5.875 5.832 0.710
2024-12-02-01:31:30-root-INFO: Loss Change: 83.991 -> 79.532
2024-12-02-01:31:30-root-INFO: Regularization Change: 0.000 -> 3.108
2024-12-02-01:31:30-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-01:31:30-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-01:31:31-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-01:31:31-root-INFO: grad norm: 6.765 6.717 0.804
2024-12-02-01:31:32-root-INFO: grad norm: 5.903 5.862 0.695
2024-12-02-01:31:33-root-INFO: Loss Change: 79.397 -> 75.428
2024-12-02-01:31:33-root-INFO: Regularization Change: 0.000 -> 2.503
2024-12-02-01:31:33-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-01:31:33-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-01:31:33-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-01:31:33-root-INFO: grad norm: 5.998 5.946 0.794
2024-12-02-01:31:34-root-INFO: grad norm: 5.319 5.259 0.791
2024-12-02-01:31:35-root-INFO: Loss Change: 75.394 -> 71.976
2024-12-02-01:31:35-root-INFO: Regularization Change: 0.000 -> 1.926
2024-12-02-01:31:35-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-01:31:35-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-01:31:35-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-01:31:36-root-INFO: grad norm: 5.009 4.933 0.870
2024-12-02-01:31:37-root-INFO: grad norm: 5.082 5.010 0.851
2024-12-02-01:31:37-root-INFO: Loss Change: 71.962 -> 70.075
2024-12-02-01:31:37-root-INFO: Regularization Change: 0.000 -> 1.635
2024-12-02-01:31:37-root-INFO: Undo step: 45
2024-12-02-01:31:37-root-INFO: Undo step: 46
2024-12-02-01:31:37-root-INFO: Undo step: 47
2024-12-02-01:31:37-root-INFO: Undo step: 48
2024-12-02-01:31:37-root-INFO: Undo step: 49
2024-12-02-01:31:38-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-01:31:38-root-INFO: grad norm: 19.656 19.489 2.553
2024-12-02-01:31:39-root-INFO: grad norm: 10.555 10.424 1.659
2024-12-02-01:31:40-root-INFO: Loss Change: 158.648 -> 95.180
2024-12-02-01:31:40-root-INFO: Regularization Change: 0.000 -> 36.428
2024-12-02-01:31:40-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-01:31:40-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-01:31:40-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-01:31:40-root-INFO: grad norm: 8.884 8.784 1.324
2024-12-02-01:31:41-root-INFO: grad norm: 6.088 5.993 1.074
2024-12-02-01:31:42-root-INFO: Loss Change: 95.292 -> 83.971
2024-12-02-01:31:42-root-INFO: Regularization Change: 0.000 -> 7.175
2024-12-02-01:31:42-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-01:31:42-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-01:31:42-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-01:31:42-root-INFO: grad norm: 5.650 5.574 0.925
2024-12-02-01:31:43-root-INFO: grad norm: 4.985 4.920 0.797
2024-12-02-01:31:44-root-INFO: Loss Change: 83.881 -> 77.739
2024-12-02-01:31:44-root-INFO: Regularization Change: 0.000 -> 3.427
2024-12-02-01:31:44-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-01:31:44-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-01:31:44-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-01:31:45-root-INFO: grad norm: 5.965 5.889 0.953
2024-12-02-01:31:46-root-INFO: grad norm: 6.453 6.385 0.935
2024-12-02-01:31:46-root-INFO: Loss Change: 77.983 -> 76.559
2024-12-02-01:31:46-root-INFO: Regularization Change: 0.000 -> 2.402
2024-12-02-01:31:46-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-01:31:46-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-01:31:47-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-01:31:47-root-INFO: grad norm: 8.999 8.912 1.246
2024-12-02-01:31:47-root-INFO: Loss too large (77.177->77.417)! Learning rate decreased to 0.22559.
2024-12-02-01:31:48-root-INFO: grad norm: 5.906 5.845 0.848
2024-12-02-01:31:49-root-INFO: Loss Change: 77.177 -> 71.173
2024-12-02-01:31:49-root-INFO: Regularization Change: 0.000 -> 1.965
2024-12-02-01:31:49-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-01:31:49-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-01:31:49-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-01:31:50-root-INFO: grad norm: 4.505 4.454 0.675
2024-12-02-01:31:51-root-INFO: grad norm: 4.805 4.766 0.613
2024-12-02-01:31:51-root-INFO: Loss Change: 71.206 -> 69.439
2024-12-02-01:31:51-root-INFO: Regularization Change: 0.000 -> 1.699
2024-12-02-01:31:51-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-01:31:51-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-01:31:52-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-01:31:52-root-INFO: grad norm: 6.363 6.315 0.782
2024-12-02-01:31:53-root-INFO: grad norm: 6.275 6.232 0.739
2024-12-02-01:31:54-root-INFO: Loss Change: 69.712 -> 67.795
2024-12-02-01:31:54-root-INFO: Regularization Change: 0.000 -> 1.693
2024-12-02-01:31:54-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-01:31:54-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-01:31:54-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-01:31:54-root-INFO: grad norm: 5.975 5.926 0.762
2024-12-02-01:31:55-root-INFO: grad norm: 5.472 5.424 0.724
2024-12-02-01:31:56-root-INFO: Loss Change: 67.845 -> 64.808
2024-12-02-01:31:56-root-INFO: Regularization Change: 0.000 -> 1.505
2024-12-02-01:31:56-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-01:31:56-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-01:31:56-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-01:31:57-root-INFO: grad norm: 6.082 6.032 0.774
2024-12-02-01:31:58-root-INFO: grad norm: 6.317 6.274 0.737
2024-12-02-01:31:58-root-INFO: Loss Change: 65.031 -> 63.503
2024-12-02-01:31:58-root-INFO: Regularization Change: 0.000 -> 1.356
2024-12-02-01:31:58-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-01:31:59-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-01:31:59-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-01:31:59-root-INFO: grad norm: 6.506 6.451 0.845
2024-12-02-01:32:00-root-INFO: grad norm: 5.833 5.779 0.786
2024-12-02-01:32:01-root-INFO: Loss Change: 63.873 -> 60.642
2024-12-02-01:32:01-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-02-01:32:01-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-01:32:01-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-01:32:01-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-01:32:01-root-INFO: grad norm: 6.193 6.137 0.834
2024-12-02-01:32:02-root-INFO: grad norm: 6.019 5.970 0.766
2024-12-02-01:32:03-root-INFO: Loss Change: 60.990 -> 58.922
2024-12-02-01:32:03-root-INFO: Regularization Change: 0.000 -> 1.309
2024-12-02-01:32:03-root-INFO: Undo step: 40
2024-12-02-01:32:03-root-INFO: Undo step: 41
2024-12-02-01:32:03-root-INFO: Undo step: 42
2024-12-02-01:32:03-root-INFO: Undo step: 43
2024-12-02-01:32:03-root-INFO: Undo step: 44
2024-12-02-01:32:04-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-01:32:04-root-INFO: grad norm: 18.325 18.171 2.371
2024-12-02-01:32:05-root-INFO: grad norm: 10.251 10.180 1.205
2024-12-02-01:32:06-root-INFO: Loss Change: 141.501 -> 82.409
2024-12-02-01:32:06-root-INFO: Regularization Change: 0.000 -> 36.633
2024-12-02-01:32:06-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-01:32:06-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-01:32:06-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-01:32:06-root-INFO: grad norm: 6.540 6.474 0.926
2024-12-02-01:32:07-root-INFO: grad norm: 5.037 4.977 0.774
2024-12-02-01:32:08-root-INFO: Loss Change: 82.054 -> 71.071
2024-12-02-01:32:08-root-INFO: Regularization Change: 0.000 -> 6.510
2024-12-02-01:32:08-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-01:32:08-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-01:32:08-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-01:32:09-root-INFO: grad norm: 4.980 4.927 0.723
2024-12-02-01:32:10-root-INFO: grad norm: 5.129 5.068 0.786
2024-12-02-01:32:10-root-INFO: Loss Change: 70.739 -> 66.854
2024-12-02-01:32:10-root-INFO: Regularization Change: 0.000 -> 3.197
2024-12-02-01:32:10-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-01:32:10-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-01:32:11-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-01:32:11-root-INFO: grad norm: 6.965 6.899 0.955
2024-12-02-01:32:11-root-INFO: Loss too large (67.080->67.197)! Learning rate decreased to 0.24361.
2024-12-02-01:32:12-root-INFO: grad norm: 5.247 5.190 0.770
2024-12-02-01:32:13-root-INFO: Loss Change: 67.080 -> 62.675
2024-12-02-01:32:13-root-INFO: Regularization Change: 0.000 -> 1.816
2024-12-02-01:32:13-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-01:32:13-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-01:32:13-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-01:32:14-root-INFO: grad norm: 4.514 4.450 0.757
2024-12-02-01:32:14-root-INFO: grad norm: 4.972 4.910 0.785
2024-12-02-01:32:15-root-INFO: Loss Change: 62.828 -> 60.949
2024-12-02-01:32:15-root-INFO: Regularization Change: 0.000 -> 2.126
2024-12-02-01:32:15-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-01:32:15-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-01:32:16-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-01:32:16-root-INFO: grad norm: 7.206 7.116 1.139
2024-12-02-01:32:16-root-INFO: Loss too large (61.545->61.931)! Learning rate decreased to 0.25333.
2024-12-02-01:32:17-root-INFO: grad norm: 5.218 5.152 0.824
2024-12-02-01:32:18-root-INFO: Loss Change: 61.545 -> 57.308
2024-12-02-01:32:18-root-INFO: Regularization Change: 0.000 -> 1.455
2024-12-02-01:32:18-root-INFO: Undo step: 40
2024-12-02-01:32:18-root-INFO: Undo step: 41
2024-12-02-01:32:18-root-INFO: Undo step: 42
2024-12-02-01:32:18-root-INFO: Undo step: 43
2024-12-02-01:32:18-root-INFO: Undo step: 44
2024-12-02-01:32:18-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-01:32:18-root-INFO: grad norm: 19.260 19.135 2.188
2024-12-02-01:32:19-root-INFO: grad norm: 9.166 9.081 1.244
2024-12-02-01:32:20-root-INFO: Loss Change: 147.201 -> 82.772
2024-12-02-01:32:20-root-INFO: Regularization Change: 0.000 -> 43.984
2024-12-02-01:32:20-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-01:32:20-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-01:32:20-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-01:32:21-root-INFO: grad norm: 7.740 7.667 1.059
2024-12-02-01:32:22-root-INFO: grad norm: 6.747 6.681 0.946
2024-12-02-01:32:22-root-INFO: Loss Change: 82.817 -> 72.762
2024-12-02-01:32:22-root-INFO: Regularization Change: 0.000 -> 6.585
2024-12-02-01:32:22-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-01:32:22-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-01:32:23-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-01:32:23-root-INFO: grad norm: 6.717 6.640 1.014
2024-12-02-01:32:24-root-INFO: grad norm: 6.101 6.035 0.895
2024-12-02-01:32:25-root-INFO: Loss Change: 72.723 -> 67.020
2024-12-02-01:32:25-root-INFO: Regularization Change: 0.000 -> 3.638
2024-12-02-01:32:25-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-01:32:25-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-01:32:25-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-01:32:25-root-INFO: grad norm: 6.360 6.274 1.039
2024-12-02-01:32:26-root-INFO: grad norm: 5.951 5.862 1.024
2024-12-02-01:32:27-root-INFO: Loss Change: 67.231 -> 63.178
2024-12-02-01:32:27-root-INFO: Regularization Change: 0.000 -> 2.478
2024-12-02-01:32:27-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-01:32:27-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-01:32:27-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-01:32:28-root-INFO: grad norm: 6.736 6.611 1.292
2024-12-02-01:32:29-root-INFO: grad norm: 6.541 6.427 1.215
2024-12-02-01:32:29-root-INFO: Loss Change: 63.776 -> 60.687
2024-12-02-01:32:29-root-INFO: Regularization Change: 0.000 -> 1.996
2024-12-02-01:32:29-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-01:32:29-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-01:32:30-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-01:32:30-root-INFO: grad norm: 7.358 7.222 1.411
2024-12-02-01:32:31-root-INFO: grad norm: 6.989 6.860 1.336
2024-12-02-01:32:32-root-INFO: Loss Change: 61.417 -> 58.456
2024-12-02-01:32:32-root-INFO: Regularization Change: 0.000 -> 1.794
2024-12-02-01:32:32-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-01:32:32-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-01:32:32-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-01:32:32-root-INFO: grad norm: 7.585 7.452 1.416
2024-12-02-01:32:33-root-INFO: grad norm: 7.200 7.074 1.339
2024-12-02-01:32:34-root-INFO: Loss Change: 59.071 -> 56.153
2024-12-02-01:32:34-root-INFO: Regularization Change: 0.000 -> 1.675
2024-12-02-01:32:34-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-01:32:34-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-01:32:34-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-01:32:35-root-INFO: grad norm: 7.451 7.320 1.394
2024-12-02-01:32:36-root-INFO: grad norm: 6.852 6.726 1.307
2024-12-02-01:32:36-root-INFO: Loss Change: 56.784 -> 53.580
2024-12-02-01:32:36-root-INFO: Regularization Change: 0.000 -> 1.534
2024-12-02-01:32:36-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-01:32:36-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-01:32:37-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-01:32:37-root-INFO: grad norm: 7.241 7.112 1.364
2024-12-02-01:32:38-root-INFO: grad norm: 6.669 6.549 1.259
2024-12-02-01:32:39-root-INFO: Loss Change: 54.293 -> 51.317
2024-12-02-01:32:39-root-INFO: Regularization Change: 0.000 -> 1.490
2024-12-02-01:32:39-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-01:32:39-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-01:32:39-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-01:32:39-root-INFO: grad norm: 6.782 6.665 1.252
2024-12-02-01:32:40-root-INFO: grad norm: 6.243 6.136 1.149
2024-12-02-01:32:41-root-INFO: Loss Change: 51.894 -> 49.069
2024-12-02-01:32:41-root-INFO: Regularization Change: 0.000 -> 1.420
2024-12-02-01:32:41-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-01:32:41-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-01:32:41-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-01:32:42-root-INFO: grad norm: 6.699 6.587 1.224
2024-12-02-01:32:43-root-INFO: grad norm: 6.142 6.035 1.142
2024-12-02-01:32:43-root-INFO: Loss Change: 49.702 -> 47.057
2024-12-02-01:32:43-root-INFO: Regularization Change: 0.000 -> 1.426
2024-12-02-01:32:43-root-INFO: Undo step: 35
2024-12-02-01:32:43-root-INFO: Undo step: 36
2024-12-02-01:32:43-root-INFO: Undo step: 37
2024-12-02-01:32:43-root-INFO: Undo step: 38
2024-12-02-01:32:43-root-INFO: Undo step: 39
2024-12-02-01:32:44-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-01:32:44-root-INFO: grad norm: 16.873 16.779 1.779
2024-12-02-01:32:45-root-INFO: grad norm: 9.365 9.261 1.395
2024-12-02-01:32:46-root-INFO: Loss Change: 126.539 -> 70.803
2024-12-02-01:32:46-root-INFO: Regularization Change: 0.000 -> 40.086
2024-12-02-01:32:46-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-01:32:46-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-01:32:46-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-01:32:46-root-INFO: grad norm: 7.962 7.842 1.379
2024-12-02-01:32:47-root-INFO: grad norm: 6.456 6.380 0.989
2024-12-02-01:32:48-root-INFO: Loss Change: 70.774 -> 60.173
2024-12-02-01:32:48-root-INFO: Regularization Change: 0.000 -> 6.969
2024-12-02-01:32:48-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-01:32:48-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-01:32:48-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-01:32:49-root-INFO: grad norm: 6.312 6.219 1.076
2024-12-02-01:32:50-root-INFO: grad norm: 5.879 5.795 0.994
2024-12-02-01:32:50-root-INFO: Loss Change: 60.363 -> 55.224
2024-12-02-01:32:50-root-INFO: Regularization Change: 0.000 -> 3.407
2024-12-02-01:32:50-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-01:32:50-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-01:32:51-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-01:32:51-root-INFO: grad norm: 6.298 6.192 1.152
2024-12-02-01:32:52-root-INFO: grad norm: 5.941 5.845 1.062
2024-12-02-01:32:53-root-INFO: Loss Change: 55.614 -> 51.854
2024-12-02-01:32:53-root-INFO: Regularization Change: 0.000 -> 2.348
2024-12-02-01:32:53-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-01:32:53-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-01:32:53-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-01:32:53-root-INFO: grad norm: 6.316 6.204 1.184
2024-12-02-01:32:54-root-INFO: grad norm: 5.987 5.890 1.073
2024-12-02-01:32:55-root-INFO: Loss Change: 52.282 -> 49.166
2024-12-02-01:32:55-root-INFO: Regularization Change: 0.000 -> 1.880
2024-12-02-01:32:55-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-01:32:55-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-01:32:55-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-01:32:56-root-INFO: grad norm: 6.408 6.297 1.191
2024-12-02-01:32:57-root-INFO: grad norm: 5.981 5.881 1.091
2024-12-02-01:32:57-root-INFO: Loss Change: 49.699 -> 46.714
2024-12-02-01:32:57-root-INFO: Regularization Change: 0.000 -> 1.634
2024-12-02-01:32:57-root-INFO: Undo step: 35
2024-12-02-01:32:57-root-INFO: Undo step: 36
2024-12-02-01:32:57-root-INFO: Undo step: 37
2024-12-02-01:32:57-root-INFO: Undo step: 38
2024-12-02-01:32:57-root-INFO: Undo step: 39
2024-12-02-01:32:58-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-01:32:58-root-INFO: grad norm: 17.336 17.249 1.735
2024-12-02-01:32:59-root-INFO: grad norm: 8.529 8.441 1.228
2024-12-02-01:33:00-root-INFO: Loss Change: 130.246 -> 69.217
2024-12-02-01:33:00-root-INFO: Regularization Change: 0.000 -> 43.283
2024-12-02-01:33:00-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-01:33:00-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-01:33:00-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-01:33:00-root-INFO: grad norm: 7.157 7.076 1.075
2024-12-02-01:33:01-root-INFO: grad norm: 6.271 6.206 0.899
2024-12-02-01:33:02-root-INFO: Loss Change: 69.226 -> 59.649
2024-12-02-01:33:02-root-INFO: Regularization Change: 0.000 -> 6.748
2024-12-02-01:33:02-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-01:33:02-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-01:33:02-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-01:33:03-root-INFO: grad norm: 6.453 6.365 1.061
2024-12-02-01:33:04-root-INFO: grad norm: 6.109 6.033 0.965
2024-12-02-01:33:04-root-INFO: Loss Change: 59.881 -> 54.810
2024-12-02-01:33:04-root-INFO: Regularization Change: 0.000 -> 3.561
2024-12-02-01:33:04-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-01:33:04-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-01:33:05-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-01:33:05-root-INFO: grad norm: 6.808 6.702 1.196
2024-12-02-01:33:06-root-INFO: grad norm: 6.254 6.158 1.089
2024-12-02-01:33:07-root-INFO: Loss Change: 55.311 -> 51.383
2024-12-02-01:33:07-root-INFO: Regularization Change: 0.000 -> 2.604
2024-12-02-01:33:07-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-01:33:07-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-01:33:07-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-01:33:07-root-INFO: grad norm: 6.515 6.412 1.152
2024-12-02-01:33:08-root-INFO: grad norm: 6.119 6.033 1.025
2024-12-02-01:33:09-root-INFO: Loss Change: 51.837 -> 48.467
2024-12-02-01:33:09-root-INFO: Regularization Change: 0.000 -> 2.025
2024-12-02-01:33:09-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-01:33:09-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-01:33:09-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-01:33:10-root-INFO: grad norm: 6.518 6.416 1.149
2024-12-02-01:33:11-root-INFO: grad norm: 5.985 5.893 1.045
2024-12-02-01:33:11-root-INFO: Loss Change: 49.009 -> 45.829
2024-12-02-01:33:11-root-INFO: Regularization Change: 0.000 -> 1.716
2024-12-02-01:33:11-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-01:33:11-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-01:33:12-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-01:33:12-root-INFO: grad norm: 6.195 6.093 1.118
2024-12-02-01:33:13-root-INFO: grad norm: 5.723 5.632 1.017
2024-12-02-01:33:14-root-INFO: Loss Change: 46.307 -> 43.586
2024-12-02-01:33:14-root-INFO: Regularization Change: 0.000 -> 1.576
2024-12-02-01:33:14-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-01:33:14-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-01:33:14-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-01:33:14-root-INFO: grad norm: 6.168 6.068 1.108
2024-12-02-01:33:15-root-INFO: grad norm: 5.571 5.485 0.978
2024-12-02-01:33:16-root-INFO: Loss Change: 43.907 -> 41.182
2024-12-02-01:33:16-root-INFO: Regularization Change: 0.000 -> 1.548
2024-12-02-01:33:16-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-01:33:16-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-01:33:16-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-01:33:17-root-INFO: grad norm: 5.945 5.848 1.067
2024-12-02-01:33:18-root-INFO: grad norm: 5.266 5.182 0.940
2024-12-02-01:33:18-root-INFO: Loss Change: 41.751 -> 39.136
2024-12-02-01:33:18-root-INFO: Regularization Change: 0.000 -> 1.558
2024-12-02-01:33:18-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-01:33:18-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-01:33:19-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-01:33:19-root-INFO: grad norm: 5.606 5.518 0.993
2024-12-02-01:33:20-root-INFO: grad norm: 5.033 4.957 0.874
2024-12-02-01:33:21-root-INFO: Loss Change: 39.560 -> 37.084
2024-12-02-01:33:21-root-INFO: Regularization Change: 0.000 -> 1.446
2024-12-02-01:33:21-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-01:33:21-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-01:33:21-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-01:33:21-root-INFO: grad norm: 5.320 5.238 0.933
2024-12-02-01:33:22-root-INFO: grad norm: 4.790 4.719 0.820
2024-12-02-01:33:23-root-INFO: Loss Change: 37.453 -> 35.175
2024-12-02-01:33:23-root-INFO: Regularization Change: 0.000 -> 1.344
2024-12-02-01:33:23-root-INFO: Undo step: 30
2024-12-02-01:33:23-root-INFO: Undo step: 31
2024-12-02-01:33:23-root-INFO: Undo step: 32
2024-12-02-01:33:23-root-INFO: Undo step: 33
2024-12-02-01:33:23-root-INFO: Undo step: 34
2024-12-02-01:33:23-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-01:33:24-root-INFO: grad norm: 16.056 15.973 1.631
2024-12-02-01:33:25-root-INFO: grad norm: 7.546 7.463 1.117
2024-12-02-01:33:25-root-INFO: Loss Change: 115.915 -> 57.526
2024-12-02-01:33:25-root-INFO: Regularization Change: 0.000 -> 45.269
2024-12-02-01:33:25-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-01:33:25-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-01:33:26-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-01:33:26-root-INFO: grad norm: 6.132 6.058 0.951
2024-12-02-01:33:27-root-INFO: grad norm: 5.336 5.266 0.860
2024-12-02-01:33:28-root-INFO: Loss Change: 57.540 -> 49.002
2024-12-02-01:33:28-root-INFO: Regularization Change: 0.000 -> 6.932
2024-12-02-01:33:28-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-01:33:28-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-01:33:28-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-01:33:28-root-INFO: grad norm: 5.626 5.549 0.925
2024-12-02-01:33:29-root-INFO: grad norm: 5.379 5.312 0.847
2024-12-02-01:33:30-root-INFO: Loss Change: 49.023 -> 44.567
2024-12-02-01:33:30-root-INFO: Regularization Change: 0.000 -> 3.624
2024-12-02-01:33:30-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-01:33:30-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-01:33:30-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-01:33:31-root-INFO: grad norm: 5.970 5.882 1.022
2024-12-02-01:33:32-root-INFO: grad norm: 5.347 5.266 0.927
2024-12-02-01:33:32-root-INFO: Loss Change: 44.949 -> 41.078
2024-12-02-01:33:32-root-INFO: Regularization Change: 0.000 -> 2.577
2024-12-02-01:33:32-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-01:33:32-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-01:33:33-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-01:33:33-root-INFO: grad norm: 5.481 5.395 0.963
2024-12-02-01:33:34-root-INFO: grad norm: 4.962 4.888 0.857
2024-12-02-01:33:35-root-INFO: Loss Change: 41.395 -> 38.280
2024-12-02-01:33:35-root-INFO: Regularization Change: 0.000 -> 2.049
2024-12-02-01:33:35-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-01:33:35-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-01:33:35-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-01:33:35-root-INFO: grad norm: 5.482 5.395 0.976
2024-12-02-01:33:36-root-INFO: grad norm: 4.863 4.783 0.879
2024-12-02-01:33:37-root-INFO: Loss Change: 38.589 -> 35.740
2024-12-02-01:33:37-root-INFO: Regularization Change: 0.000 -> 1.775
2024-12-02-01:33:37-root-INFO: Undo step: 30
2024-12-02-01:33:37-root-INFO: Undo step: 31
2024-12-02-01:33:37-root-INFO: Undo step: 32
2024-12-02-01:33:37-root-INFO: Undo step: 33
2024-12-02-01:33:37-root-INFO: Undo step: 34
2024-12-02-01:33:37-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-01:33:38-root-INFO: grad norm: 15.371 15.268 1.772
2024-12-02-01:33:39-root-INFO: grad norm: 7.300 7.211 1.139
2024-12-02-01:33:39-root-INFO: Loss Change: 112.229 -> 57.058
2024-12-02-01:33:39-root-INFO: Regularization Change: 0.000 -> 42.372
2024-12-02-01:33:39-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-01:33:39-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-01:33:40-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-01:33:40-root-INFO: grad norm: 5.802 5.729 0.919
2024-12-02-01:33:41-root-INFO: grad norm: 5.238 5.167 0.857
2024-12-02-01:33:42-root-INFO: Loss Change: 56.978 -> 48.732
2024-12-02-01:33:42-root-INFO: Regularization Change: 0.000 -> 6.928
2024-12-02-01:33:42-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-01:33:42-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-01:33:42-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-01:33:42-root-INFO: grad norm: 6.098 6.012 1.017
2024-12-02-01:33:43-root-INFO: grad norm: 5.659 5.579 0.948
2024-12-02-01:33:44-root-INFO: Loss Change: 48.750 -> 44.152
2024-12-02-01:33:44-root-INFO: Regularization Change: 0.000 -> 3.786
2024-12-02-01:33:44-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-01:33:44-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-01:33:44-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-01:33:45-root-INFO: grad norm: 5.914 5.826 1.015
2024-12-02-01:33:46-root-INFO: grad norm: 5.334 5.251 0.940
2024-12-02-01:33:46-root-INFO: Loss Change: 44.537 -> 40.545
2024-12-02-01:33:46-root-INFO: Regularization Change: 0.000 -> 2.700
2024-12-02-01:33:46-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-01:33:46-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-01:33:47-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-01:33:47-root-INFO: grad norm: 5.871 5.778 1.044
2024-12-02-01:33:48-root-INFO: grad norm: 5.181 5.096 0.938
2024-12-02-01:33:49-root-INFO: Loss Change: 40.901 -> 37.551
2024-12-02-01:33:49-root-INFO: Regularization Change: 0.000 -> 2.151
2024-12-02-01:33:49-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-01:33:49-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-01:33:49-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-01:33:49-root-INFO: grad norm: 5.279 5.193 0.950
2024-12-02-01:33:50-root-INFO: grad norm: 4.757 4.678 0.866
2024-12-02-01:33:51-root-INFO: Loss Change: 37.839 -> 35.051
2024-12-02-01:33:51-root-INFO: Regularization Change: 0.000 -> 1.840
2024-12-02-01:33:51-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-01:33:51-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-01:33:51-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-01:33:52-root-INFO: grad norm: 5.365 5.275 0.983
2024-12-02-01:33:53-root-INFO: grad norm: 4.797 4.714 0.891
2024-12-02-01:33:53-root-INFO: Loss Change: 35.458 -> 32.898
2024-12-02-01:33:53-root-INFO: Regularization Change: 0.000 -> 1.626
2024-12-02-01:33:53-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-01:33:53-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-01:33:54-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-01:33:54-root-INFO: grad norm: 5.050 4.965 0.923
2024-12-02-01:33:55-root-INFO: grad norm: 4.494 4.416 0.834
2024-12-02-01:33:56-root-INFO: Loss Change: 33.140 -> 30.726
2024-12-02-01:33:56-root-INFO: Regularization Change: 0.000 -> 1.512
2024-12-02-01:33:56-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-01:33:56-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-01:33:56-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-01:33:56-root-INFO: grad norm: 4.915 4.834 0.891
2024-12-02-01:33:57-root-INFO: grad norm: 4.358 4.282 0.812
2024-12-02-01:33:58-root-INFO: Loss Change: 31.059 -> 28.821
2024-12-02-01:33:58-root-INFO: Regularization Change: 0.000 -> 1.398
2024-12-02-01:33:58-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-01:33:58-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-01:33:58-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-01:33:59-root-INFO: grad norm: 4.621 4.543 0.845
2024-12-02-01:34:00-root-INFO: grad norm: 4.073 4.003 0.751
2024-12-02-01:34:00-root-INFO: Loss Change: 29.132 -> 27.020
2024-12-02-01:34:00-root-INFO: Regularization Change: 0.000 -> 1.324
2024-12-02-01:34:00-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-01:34:00-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-01:34:01-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-01:34:01-root-INFO: grad norm: 4.466 4.392 0.810
2024-12-02-01:34:02-root-INFO: grad norm: 3.909 3.844 0.706
2024-12-02-01:34:03-root-INFO: Loss Change: 27.182 -> 25.110
2024-12-02-01:34:03-root-INFO: Regularization Change: 0.000 -> 1.263
2024-12-02-01:34:03-root-INFO: Undo step: 25
2024-12-02-01:34:03-root-INFO: Undo step: 26
2024-12-02-01:34:03-root-INFO: Undo step: 27
2024-12-02-01:34:03-root-INFO: Undo step: 28
2024-12-02-01:34:03-root-INFO: Undo step: 29
2024-12-02-01:34:03-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-01:34:03-root-INFO: grad norm: 14.442 14.377 1.371
2024-12-02-01:34:04-root-INFO: grad norm: 7.582 7.522 0.954
2024-12-02-01:34:05-root-INFO: Loss Change: 100.797 -> 48.603
2024-12-02-01:34:05-root-INFO: Regularization Change: 0.000 -> 46.197
2024-12-02-01:34:05-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-01:34:05-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-01:34:05-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-01:34:06-root-INFO: grad norm: 7.132 7.073 0.917
2024-12-02-01:34:07-root-INFO: grad norm: 6.222 6.163 0.859
2024-12-02-01:34:07-root-INFO: Loss Change: 48.672 -> 40.026
2024-12-02-01:34:07-root-INFO: Regularization Change: 0.000 -> 7.545
2024-12-02-01:34:07-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-01:34:07-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-01:34:08-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-01:34:08-root-INFO: grad norm: 5.983 5.919 0.877
2024-12-02-01:34:09-root-INFO: grad norm: 4.978 4.917 0.776
2024-12-02-01:34:10-root-INFO: Loss Change: 40.032 -> 34.175
2024-12-02-01:34:10-root-INFO: Regularization Change: 0.000 -> 3.948
2024-12-02-01:34:10-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-01:34:10-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-01:34:10-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-01:34:10-root-INFO: grad norm: 5.225 5.159 0.829
2024-12-02-01:34:11-root-INFO: grad norm: 4.664 4.604 0.749
2024-12-02-01:34:12-root-INFO: Loss Change: 34.388 -> 30.835
2024-12-02-01:34:12-root-INFO: Regularization Change: 0.000 -> 2.505
2024-12-02-01:34:12-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-01:34:12-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-01:34:12-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-01:34:13-root-INFO: grad norm: 4.885 4.819 0.801
2024-12-02-01:34:14-root-INFO: grad norm: 4.326 4.269 0.695
2024-12-02-01:34:15-root-INFO: Loss Change: 31.046 -> 28.121
2024-12-02-01:34:15-root-INFO: Regularization Change: 0.000 -> 1.978
2024-12-02-01:34:15-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-01:34:15-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-01:34:15-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-01:34:15-root-INFO: grad norm: 4.577 4.514 0.759
2024-12-02-01:34:16-root-INFO: grad norm: 4.007 3.952 0.664
2024-12-02-01:34:17-root-INFO: Loss Change: 28.186 -> 25.631
2024-12-02-01:34:17-root-INFO: Regularization Change: 0.000 -> 1.660
2024-12-02-01:34:17-root-INFO: Undo step: 25
2024-12-02-01:34:17-root-INFO: Undo step: 26
2024-12-02-01:34:17-root-INFO: Undo step: 27
2024-12-02-01:34:17-root-INFO: Undo step: 28
2024-12-02-01:34:17-root-INFO: Undo step: 29
2024-12-02-01:34:17-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-01:34:18-root-INFO: grad norm: 14.076 14.028 1.168
2024-12-02-01:34:19-root-INFO: grad norm: 6.921 6.879 0.763
2024-12-02-01:34:19-root-INFO: Loss Change: 97.496 -> 46.759
2024-12-02-01:34:19-root-INFO: Regularization Change: 0.000 -> 43.736
2024-12-02-01:34:19-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-01:34:19-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-01:34:20-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-01:34:20-root-INFO: grad norm: 5.718 5.679 0.666
2024-12-02-01:34:21-root-INFO: grad norm: 5.178 5.144 0.590
2024-12-02-01:34:21-root-INFO: Loss Change: 46.499 -> 38.706
2024-12-02-01:34:21-root-INFO: Regularization Change: 0.000 -> 6.849
2024-12-02-01:34:22-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-01:34:22-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-01:34:22-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-01:34:22-root-INFO: grad norm: 5.250 5.201 0.713
2024-12-02-01:34:23-root-INFO: grad norm: 4.636 4.591 0.644
2024-12-02-01:34:24-root-INFO: Loss Change: 38.527 -> 33.545
2024-12-02-01:34:24-root-INFO: Regularization Change: 0.000 -> 3.691
2024-12-02-01:34:24-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-01:34:24-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-01:34:24-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-01:34:25-root-INFO: grad norm: 4.894 4.831 0.781
2024-12-02-01:34:26-root-INFO: grad norm: 4.410 4.355 0.698
2024-12-02-01:34:26-root-INFO: Loss Change: 33.674 -> 30.172
2024-12-02-01:34:26-root-INFO: Regularization Change: 0.000 -> 2.472
2024-12-02-01:34:26-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-01:34:26-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-01:34:27-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-01:34:27-root-INFO: grad norm: 4.624 4.559 0.772
2024-12-02-01:34:28-root-INFO: grad norm: 4.129 4.073 0.675
2024-12-02-01:34:29-root-INFO: Loss Change: 30.352 -> 27.485
2024-12-02-01:34:29-root-INFO: Regularization Change: 0.000 -> 1.910
2024-12-02-01:34:29-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-01:34:29-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-01:34:29-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-01:34:29-root-INFO: grad norm: 4.400 4.335 0.750
2024-12-02-01:34:30-root-INFO: grad norm: 3.889 3.836 0.639
2024-12-02-01:34:31-root-INFO: Loss Change: 27.539 -> 25.043
2024-12-02-01:34:31-root-INFO: Regularization Change: 0.000 -> 1.600
2024-12-02-01:34:31-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-01:34:31-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-01:34:31-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-01:34:32-root-INFO: grad norm: 4.169 4.110 0.699
2024-12-02-01:34:33-root-INFO: grad norm: 3.658 3.609 0.596
2024-12-02-01:34:33-root-INFO: Loss Change: 25.345 -> 23.129
2024-12-02-01:34:33-root-INFO: Regularization Change: 0.000 -> 1.409
2024-12-02-01:34:33-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-01:34:33-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-01:34:34-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-01:34:34-root-INFO: grad norm: 3.869 3.814 0.649
2024-12-02-01:34:35-root-INFO: grad norm: 3.399 3.355 0.548
2024-12-02-01:34:36-root-INFO: Loss Change: 23.171 -> 21.214
2024-12-02-01:34:36-root-INFO: Regularization Change: 0.000 -> 1.271
2024-12-02-01:34:36-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-01:34:36-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-01:34:36-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-01:34:36-root-INFO: grad norm: 3.748 3.698 0.606
2024-12-02-01:34:37-root-INFO: grad norm: 3.262 3.222 0.514
2024-12-02-01:34:38-root-INFO: Loss Change: 21.419 -> 19.596
2024-12-02-01:34:38-root-INFO: Regularization Change: 0.000 -> 1.169
2024-12-02-01:34:38-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-01:34:38-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-01:34:38-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-01:34:39-root-INFO: grad norm: 3.459 3.414 0.556
2024-12-02-01:34:40-root-INFO: grad norm: 3.030 2.994 0.464
2024-12-02-01:34:40-root-INFO: Loss Change: 19.699 -> 18.061
2024-12-02-01:34:40-root-INFO: Regularization Change: 0.000 -> 1.103
2024-12-02-01:34:40-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-01:34:40-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-01:34:41-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-01:34:41-root-INFO: grad norm: 3.447 3.404 0.538
2024-12-02-01:34:42-root-INFO: grad norm: 2.972 2.938 0.446
2024-12-02-01:34:43-root-INFO: Loss Change: 18.232 -> 16.614
2024-12-02-01:34:43-root-INFO: Regularization Change: 0.000 -> 1.049
2024-12-02-01:34:43-root-INFO: Undo step: 20
2024-12-02-01:34:43-root-INFO: Undo step: 21
2024-12-02-01:34:43-root-INFO: Undo step: 22
2024-12-02-01:34:43-root-INFO: Undo step: 23
2024-12-02-01:34:43-root-INFO: Undo step: 24
2024-12-02-01:34:43-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-01:34:43-root-INFO: grad norm: 13.486 13.438 1.128
2024-12-02-01:34:44-root-INFO: grad norm: 5.713 5.672 0.683
2024-12-02-01:34:45-root-INFO: Loss Change: 87.274 -> 35.541
2024-12-02-01:34:45-root-INFO: Regularization Change: 0.000 -> 46.842
2024-12-02-01:34:45-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-01:34:45-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-01:34:45-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-01:34:46-root-INFO: grad norm: 4.332 4.295 0.566
2024-12-02-01:34:47-root-INFO: grad norm: 3.712 3.677 0.506
2024-12-02-01:34:47-root-INFO: Loss Change: 35.263 -> 28.064
2024-12-02-01:34:47-root-INFO: Regularization Change: 0.000 -> 6.814
2024-12-02-01:34:47-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-01:34:47-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-01:34:48-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-01:34:48-root-INFO: grad norm: 4.024 3.977 0.607
2024-12-02-01:34:49-root-INFO: grad norm: 3.728 3.684 0.574
2024-12-02-01:34:50-root-INFO: Loss Change: 27.788 -> 24.015
2024-12-02-01:34:50-root-INFO: Regularization Change: 0.000 -> 3.388
2024-12-02-01:34:50-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-01:34:50-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-01:34:50-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-01:34:50-root-INFO: grad norm: 4.069 4.018 0.645
2024-12-02-01:34:51-root-INFO: grad norm: 3.726 3.677 0.600
2024-12-02-01:34:52-root-INFO: Loss Change: 24.083 -> 21.341
2024-12-02-01:34:52-root-INFO: Regularization Change: 0.000 -> 2.254
2024-12-02-01:34:52-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-01:34:52-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-01:34:52-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-01:34:53-root-INFO: grad norm: 3.974 3.924 0.629
2024-12-02-01:34:54-root-INFO: grad norm: 3.563 3.518 0.570
2024-12-02-01:34:54-root-INFO: Loss Change: 21.398 -> 19.099
2024-12-02-01:34:54-root-INFO: Regularization Change: 0.000 -> 1.724
2024-12-02-01:34:54-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-01:34:54-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-01:34:55-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-01:34:55-root-INFO: grad norm: 3.921 3.871 0.621
2024-12-02-01:34:56-root-INFO: grad norm: 3.420 3.376 0.547
2024-12-02-01:34:57-root-INFO: Loss Change: 19.278 -> 17.194
2024-12-02-01:34:57-root-INFO: Regularization Change: 0.000 -> 1.436
2024-12-02-01:34:57-root-INFO: Undo step: 20
2024-12-02-01:34:57-root-INFO: Undo step: 21
2024-12-02-01:34:57-root-INFO: Undo step: 22
2024-12-02-01:34:57-root-INFO: Undo step: 23
2024-12-02-01:34:57-root-INFO: Undo step: 24
2024-12-02-01:34:57-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-01:34:57-root-INFO: grad norm: 13.763 13.716 1.138
2024-12-02-01:34:58-root-INFO: grad norm: 6.363 6.319 0.740
2024-12-02-01:34:59-root-INFO: Loss Change: 88.154 -> 37.484
2024-12-02-01:34:59-root-INFO: Regularization Change: 0.000 -> 47.264
2024-12-02-01:34:59-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-01:34:59-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-01:34:59-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-01:35:00-root-INFO: grad norm: 5.290 5.252 0.632
2024-12-02-01:35:01-root-INFO: grad norm: 4.623 4.588 0.570
2024-12-02-01:35:01-root-INFO: Loss Change: 37.314 -> 29.629
2024-12-02-01:35:01-root-INFO: Regularization Change: 0.000 -> 7.508
2024-12-02-01:35:01-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-01:35:01-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-01:35:02-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-01:35:02-root-INFO: grad norm: 4.635 4.596 0.596
2024-12-02-01:35:03-root-INFO: grad norm: 4.071 4.024 0.623
2024-12-02-01:35:04-root-INFO: Loss Change: 29.346 -> 24.833
2024-12-02-01:35:04-root-INFO: Regularization Change: 0.000 -> 3.766
2024-12-02-01:35:04-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-01:35:04-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-01:35:04-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-01:35:04-root-INFO: grad norm: 4.621 4.570 0.688
2024-12-02-01:35:05-root-INFO: grad norm: 3.913 3.858 0.657
2024-12-02-01:35:06-root-INFO: Loss Change: 24.958 -> 21.717
2024-12-02-01:35:06-root-INFO: Regularization Change: 0.000 -> 2.594
2024-12-02-01:35:06-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-01:35:06-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-01:35:06-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-01:35:07-root-INFO: grad norm: 3.970 3.918 0.639
2024-12-02-01:35:08-root-INFO: grad norm: 3.437 3.386 0.587
2024-12-02-01:35:08-root-INFO: Loss Change: 21.741 -> 19.135
2024-12-02-01:35:08-root-INFO: Regularization Change: 0.000 -> 1.950
2024-12-02-01:35:08-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-01:35:08-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-01:35:09-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-01:35:09-root-INFO: grad norm: 3.889 3.838 0.624
2024-12-02-01:35:10-root-INFO: grad norm: 3.328 3.280 0.561
2024-12-02-01:35:11-root-INFO: Loss Change: 19.279 -> 17.063
2024-12-02-01:35:11-root-INFO: Regularization Change: 0.000 -> 1.522
2024-12-02-01:35:11-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-01:35:11-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-01:35:11-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-01:35:11-root-INFO: grad norm: 3.415 3.369 0.559
2024-12-02-01:35:12-root-INFO: grad norm: 2.960 2.919 0.489
2024-12-02-01:35:13-root-INFO: Loss Change: 17.120 -> 15.339
2024-12-02-01:35:13-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-02-01:35:13-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-01:35:13-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-01:35:13-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-01:35:14-root-INFO: grad norm: 3.528 3.488 0.534
2024-12-02-01:35:15-root-INFO: grad norm: 2.899 2.861 0.464
2024-12-02-01:35:15-root-INFO: Loss Change: 15.485 -> 13.748
2024-12-02-01:35:15-root-INFO: Regularization Change: 0.000 -> 1.252
2024-12-02-01:35:15-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-01:35:15-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-01:35:16-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-01:35:16-root-INFO: grad norm: 3.023 2.987 0.467
2024-12-02-01:35:17-root-INFO: grad norm: 2.562 2.534 0.383
2024-12-02-01:35:18-root-INFO: Loss Change: 13.862 -> 12.370
2024-12-02-01:35:18-root-INFO: Regularization Change: 0.000 -> 1.096
2024-12-02-01:35:18-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-01:35:18-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-01:35:18-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-01:35:18-root-INFO: grad norm: 2.942 2.910 0.427
2024-12-02-01:35:19-root-INFO: grad norm: 2.537 2.511 0.358
2024-12-02-01:35:20-root-INFO: Loss Change: 12.491 -> 11.172
2024-12-02-01:35:20-root-INFO: Regularization Change: 0.000 -> 0.985
2024-12-02-01:35:20-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-01:35:20-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-01:35:20-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-01:35:21-root-INFO: grad norm: 2.948 2.917 0.426
2024-12-02-01:35:22-root-INFO: grad norm: 2.468 2.441 0.360
2024-12-02-01:35:22-root-INFO: Loss Change: 11.326 -> 9.993
2024-12-02-01:35:22-root-INFO: Regularization Change: 0.000 -> 0.950
2024-12-02-01:35:22-root-INFO: Undo step: 15
2024-12-02-01:35:22-root-INFO: Undo step: 16
2024-12-02-01:35:22-root-INFO: Undo step: 17
2024-12-02-01:35:22-root-INFO: Undo step: 18
2024-12-02-01:35:22-root-INFO: Undo step: 19
2024-12-02-01:35:23-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-01:35:23-root-INFO: grad norm: 12.601 12.561 1.002
2024-12-02-01:35:24-root-INFO: grad norm: 5.505 5.471 0.613
2024-12-02-01:35:25-root-INFO: Loss Change: 77.718 -> 27.714
2024-12-02-01:35:25-root-INFO: Regularization Change: 0.000 -> 48.852
2024-12-02-01:35:25-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-01:35:25-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-01:35:25-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-01:35:25-root-INFO: grad norm: 4.141 4.120 0.414
2024-12-02-01:35:26-root-INFO: grad norm: 3.432 3.410 0.382
2024-12-02-01:35:27-root-INFO: Loss Change: 27.065 -> 20.001
2024-12-02-01:35:27-root-INFO: Regularization Change: 0.000 -> 7.112
2024-12-02-01:35:27-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-01:35:27-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-01:35:27-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-01:35:28-root-INFO: grad norm: 3.323 3.298 0.411
2024-12-02-01:35:29-root-INFO: grad norm: 2.761 2.737 0.366
2024-12-02-01:35:29-root-INFO: Loss Change: 19.664 -> 16.101
2024-12-02-01:35:29-root-INFO: Regularization Change: 0.000 -> 3.346
2024-12-02-01:35:29-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-01:35:29-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-01:35:30-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-01:35:30-root-INFO: grad norm: 2.910 2.877 0.434
2024-12-02-01:35:31-root-INFO: grad norm: 2.505 2.475 0.386
2024-12-02-01:35:32-root-INFO: Loss Change: 15.960 -> 13.651
2024-12-02-01:35:32-root-INFO: Regularization Change: 0.000 -> 2.086
2024-12-02-01:35:32-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-01:35:32-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-01:35:32-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-01:35:32-root-INFO: grad norm: 3.006 2.969 0.470
2024-12-02-01:35:33-root-INFO: grad norm: 2.691 2.660 0.409
2024-12-02-01:35:34-root-INFO: Loss Change: 13.657 -> 11.969
2024-12-02-01:35:34-root-INFO: Regularization Change: 0.000 -> 1.522
2024-12-02-01:35:34-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-01:35:34-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-01:35:34-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-01:35:35-root-INFO: grad norm: 3.082 3.044 0.484
2024-12-02-01:35:36-root-INFO: grad norm: 2.666 2.634 0.410
2024-12-02-01:35:36-root-INFO: Loss Change: 12.064 -> 10.537
2024-12-02-01:35:36-root-INFO: Regularization Change: 0.000 -> 1.249
2024-12-02-01:35:36-root-INFO: Undo step: 15
2024-12-02-01:35:36-root-INFO: Undo step: 16
2024-12-02-01:35:36-root-INFO: Undo step: 17
2024-12-02-01:35:36-root-INFO: Undo step: 18
2024-12-02-01:35:36-root-INFO: Undo step: 19
2024-12-02-01:35:37-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-01:35:37-root-INFO: grad norm: 13.666 13.624 1.071
2024-12-02-01:35:38-root-INFO: grad norm: 6.051 6.014 0.675
2024-12-02-01:35:39-root-INFO: Loss Change: 82.765 -> 28.990
2024-12-02-01:35:39-root-INFO: Regularization Change: 0.000 -> 54.176
2024-12-02-01:35:39-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-01:35:39-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-01:35:39-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-01:35:39-root-INFO: grad norm: 4.770 4.737 0.559
2024-12-02-01:35:40-root-INFO: grad norm: 3.684 3.650 0.497
2024-12-02-01:35:41-root-INFO: Loss Change: 28.533 -> 20.474
2024-12-02-01:35:41-root-INFO: Regularization Change: 0.000 -> 7.936
2024-12-02-01:35:41-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-01:35:41-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-01:35:41-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-01:35:42-root-INFO: grad norm: 3.632 3.595 0.518
2024-12-02-01:35:43-root-INFO: grad norm: 3.052 3.020 0.437
2024-12-02-01:35:43-root-INFO: Loss Change: 20.258 -> 16.373
2024-12-02-01:35:43-root-INFO: Regularization Change: 0.000 -> 3.608
2024-12-02-01:35:43-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-01:35:43-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-01:35:44-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-01:35:44-root-INFO: grad norm: 3.443 3.409 0.486
2024-12-02-01:35:45-root-INFO: grad norm: 2.930 2.899 0.421
2024-12-02-01:35:46-root-INFO: Loss Change: 16.321 -> 13.797
2024-12-02-01:35:46-root-INFO: Regularization Change: 0.000 -> 2.283
2024-12-02-01:35:46-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-01:35:46-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-01:35:46-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-01:35:46-root-INFO: grad norm: 3.189 3.155 0.466
2024-12-02-01:35:47-root-INFO: grad norm: 2.658 2.626 0.408
2024-12-02-01:35:48-root-INFO: Loss Change: 13.808 -> 11.786
2024-12-02-01:35:48-root-INFO: Regularization Change: 0.000 -> 1.696
2024-12-02-01:35:48-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-01:35:48-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-01:35:48-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-01:35:49-root-INFO: grad norm: 3.096 3.059 0.476
2024-12-02-01:35:50-root-INFO: grad norm: 2.581 2.554 0.374
2024-12-02-01:35:50-root-INFO: Loss Change: 11.860 -> 10.256
2024-12-02-01:35:50-root-INFO: Regularization Change: 0.000 -> 1.355
2024-12-02-01:35:50-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-01:35:50-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-01:35:51-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-01:35:51-root-INFO: grad norm: 2.846 2.814 0.423
2024-12-02-01:35:52-root-INFO: grad norm: 2.391 2.365 0.356
2024-12-02-01:35:53-root-INFO: Loss Change: 10.360 -> 8.936
2024-12-02-01:35:53-root-INFO: Regularization Change: 0.000 -> 1.105
2024-12-02-01:35:53-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-01:35:53-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-01:35:53-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-01:35:53-root-INFO: grad norm: 2.778 2.746 0.421
2024-12-02-01:35:54-root-INFO: grad norm: 2.295 2.269 0.341
2024-12-02-01:35:55-root-INFO: Loss Change: 9.113 -> 7.811
2024-12-02-01:35:55-root-INFO: Regularization Change: 0.000 -> 0.947
2024-12-02-01:35:55-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-01:35:55-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-01:35:55-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-01:35:56-root-INFO: grad norm: 2.531 2.502 0.380
2024-12-02-01:35:57-root-INFO: grad norm: 2.082 2.059 0.313
2024-12-02-01:35:57-root-INFO: Loss Change: 7.981 -> 6.839
2024-12-02-01:35:57-root-INFO: Regularization Change: 0.000 -> 0.859
2024-12-02-01:35:57-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-01:35:57-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-01:35:58-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-01:35:58-root-INFO: grad norm: 2.474 2.445 0.372
2024-12-02-01:35:59-root-INFO: grad norm: 1.969 1.946 0.296
2024-12-02-01:36:00-root-INFO: Loss Change: 7.062 -> 5.957
2024-12-02-01:36:00-root-INFO: Regularization Change: 0.000 -> 0.808
2024-12-02-01:36:00-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-01:36:00-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-01:36:00-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-01:36:00-root-INFO: grad norm: 2.272 2.245 0.350
2024-12-02-01:36:01-root-INFO: grad norm: 1.777 1.755 0.273
2024-12-02-01:36:02-root-INFO: Loss Change: 6.197 -> 5.214
2024-12-02-01:36:02-root-INFO: Regularization Change: 0.000 -> 0.806
2024-12-02-01:36:02-root-INFO: Undo step: 10
2024-12-02-01:36:02-root-INFO: Undo step: 11
2024-12-02-01:36:02-root-INFO: Undo step: 12
2024-12-02-01:36:02-root-INFO: Undo step: 13
2024-12-02-01:36:02-root-INFO: Undo step: 14
2024-12-02-01:36:02-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-01:36:03-root-INFO: grad norm: 12.743 12.717 0.813
2024-12-02-01:36:04-root-INFO: grad norm: 4.980 4.958 0.462
2024-12-02-01:36:04-root-INFO: Loss Change: 68.321 -> 19.306
2024-12-02-01:36:04-root-INFO: Regularization Change: 0.000 -> 53.857
2024-12-02-01:36:04-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-01:36:04-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-01:36:05-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-01:36:05-root-INFO: grad norm: 3.564 3.546 0.352
2024-12-02-01:36:06-root-INFO: grad norm: 2.817 2.797 0.335
2024-12-02-01:36:07-root-INFO: Loss Change: 18.709 -> 12.745
2024-12-02-01:36:07-root-INFO: Regularization Change: 0.000 -> 6.454
2024-12-02-01:36:07-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-01:36:07-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-01:36:07-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-01:36:07-root-INFO: grad norm: 2.869 2.847 0.357
2024-12-02-01:36:08-root-INFO: grad norm: 2.349 2.325 0.334
2024-12-02-01:36:09-root-INFO: Loss Change: 12.457 -> 9.586
2024-12-02-01:36:09-root-INFO: Regularization Change: 0.000 -> 2.878
2024-12-02-01:36:09-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-01:36:09-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-01:36:09-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-01:36:10-root-INFO: grad norm: 2.449 2.422 0.363
2024-12-02-01:36:11-root-INFO: grad norm: 2.062 2.038 0.314
2024-12-02-01:36:11-root-INFO: Loss Change: 9.448 -> 7.701
2024-12-02-01:36:11-root-INFO: Regularization Change: 0.000 -> 1.718
2024-12-02-01:36:12-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-01:36:12-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-01:36:12-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-01:36:12-root-INFO: grad norm: 2.546 2.517 0.384
2024-12-02-01:36:13-root-INFO: grad norm: 2.033 2.010 0.307
2024-12-02-01:36:14-root-INFO: Loss Change: 7.740 -> 6.404
2024-12-02-01:36:14-root-INFO: Regularization Change: 0.000 -> 1.284
2024-12-02-01:36:14-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-01:36:14-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-01:36:14-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-01:36:15-root-INFO: grad norm: 2.319 2.292 0.351
2024-12-02-01:36:16-root-INFO: grad norm: 1.852 1.830 0.282
2024-12-02-01:36:16-root-INFO: Loss Change: 6.512 -> 5.332
2024-12-02-01:36:16-root-INFO: Regularization Change: 0.000 -> 1.032
2024-12-02-01:36:16-root-INFO: Undo step: 10
2024-12-02-01:36:16-root-INFO: Undo step: 11
2024-12-02-01:36:16-root-INFO: Undo step: 12
2024-12-02-01:36:16-root-INFO: Undo step: 13
2024-12-02-01:36:16-root-INFO: Undo step: 14
2024-12-02-01:36:17-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-01:36:17-root-INFO: grad norm: 13.174 13.135 1.013
2024-12-02-01:36:18-root-INFO: grad norm: 5.606 5.575 0.590
2024-12-02-01:36:19-root-INFO: Loss Change: 72.032 -> 20.947
2024-12-02-01:36:19-root-INFO: Regularization Change: 0.000 -> 56.707
2024-12-02-01:36:19-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-01:36:19-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-01:36:19-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-01:36:19-root-INFO: grad norm: 4.188 4.171 0.375
2024-12-02-01:36:20-root-INFO: grad norm: 2.970 2.950 0.344
2024-12-02-01:36:21-root-INFO: Loss Change: 20.326 -> 13.156
2024-12-02-01:36:21-root-INFO: Regularization Change: 0.000 -> 7.399
2024-12-02-01:36:21-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-01:36:21-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-01:36:21-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-01:36:22-root-INFO: grad norm: 2.578 2.562 0.287
2024-12-02-01:36:23-root-INFO: grad norm: 2.069 2.056 0.234
2024-12-02-01:36:23-root-INFO: Loss Change: 12.737 -> 9.746
2024-12-02-01:36:23-root-INFO: Regularization Change: 0.000 -> 3.120
2024-12-02-01:36:23-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-01:36:23-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-01:36:24-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-01:36:24-root-INFO: grad norm: 2.361 2.344 0.281
2024-12-02-01:36:25-root-INFO: grad norm: 1.891 1.876 0.236
2024-12-02-01:36:26-root-INFO: Loss Change: 9.484 -> 7.738
2024-12-02-01:36:26-root-INFO: Regularization Change: 0.000 -> 1.928
2024-12-02-01:36:26-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-01:36:26-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-01:36:26-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-01:36:26-root-INFO: grad norm: 2.166 2.144 0.306
2024-12-02-01:36:27-root-INFO: grad norm: 1.829 1.810 0.263
2024-12-02-01:36:28-root-INFO: Loss Change: 7.630 -> 6.309
2024-12-02-01:36:28-root-INFO: Regularization Change: 0.000 -> 1.305
2024-12-02-01:36:28-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-01:36:28-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-01:36:28-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-01:36:29-root-INFO: grad norm: 2.242 2.214 0.354
2024-12-02-01:36:30-root-INFO: grad norm: 1.854 1.834 0.272
2024-12-02-01:36:30-root-INFO: Loss Change: 6.339 -> 5.285
2024-12-02-01:36:30-root-INFO: Regularization Change: 0.000 -> 1.005
2024-12-02-01:36:30-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-01:36:30-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-01:36:31-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-01:36:31-root-INFO: grad norm: 2.317 2.291 0.346
2024-12-02-01:36:32-root-INFO: grad norm: 1.724 1.707 0.245
2024-12-02-01:36:32-root-INFO: Loss Change: 5.420 -> 4.480
2024-12-02-01:36:32-root-INFO: Regularization Change: 0.000 -> 0.946
2024-12-02-01:36:33-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-01:36:33-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-01:36:33-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-01:36:33-root-INFO: grad norm: 2.009 1.987 0.295
2024-12-02-01:36:34-root-INFO: grad norm: 1.547 1.532 0.211
2024-12-02-01:36:35-root-INFO: Loss Change: 4.658 -> 3.771
2024-12-02-01:36:35-root-INFO: Regularization Change: 0.000 -> 0.746
2024-12-02-01:36:35-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-01:36:35-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-01:36:35-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-01:36:36-root-INFO: grad norm: 1.863 1.843 0.273
2024-12-02-01:36:37-root-INFO: grad norm: 1.440 1.429 0.180
2024-12-02-01:36:37-root-INFO: Loss Change: 3.968 -> 3.287
2024-12-02-01:36:37-root-INFO: Regularization Change: 0.000 -> 0.626
2024-12-02-01:36:37-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-01:36:37-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-01:36:38-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-01:36:38-root-INFO: grad norm: 1.828 1.812 0.239
2024-12-02-01:36:39-root-INFO: grad norm: 1.145 1.138 0.128
2024-12-02-01:36:40-root-INFO: Loss Change: 3.498 -> 2.788
2024-12-02-01:36:40-root-INFO: Regularization Change: 0.000 -> 0.643
2024-12-02-01:36:40-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-01:36:40-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-01:36:40-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-01:36:40-root-INFO: grad norm: 1.360 1.350 0.168
2024-12-02-01:36:41-root-INFO: grad norm: 0.796 0.792 0.080
2024-12-02-01:36:42-root-INFO: Loss Change: 2.968 -> 2.396
2024-12-02-01:36:42-root-INFO: Regularization Change: 0.000 -> 0.532
2024-12-02-01:36:42-root-INFO: Undo step: 5
2024-12-02-01:36:42-root-INFO: Undo step: 6
2024-12-02-01:36:42-root-INFO: Undo step: 7
2024-12-02-01:36:42-root-INFO: Undo step: 8
2024-12-02-01:36:42-root-INFO: Undo step: 9
2024-12-02-01:36:42-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-01:36:43-root-INFO: grad norm: 12.267 12.252 0.607
2024-12-02-01:36:43-root-INFO: grad norm: 4.617 4.604 0.342
2024-12-02-01:36:44-root-INFO: Loss Change: 55.638 -> 12.553
2024-12-02-01:36:44-root-INFO: Regularization Change: 0.000 -> 54.385
2024-12-02-01:36:44-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-01:36:44-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-01:36:45-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-01:36:45-root-INFO: grad norm: 3.206 3.200 0.195
2024-12-02-01:36:46-root-INFO: grad norm: 2.165 2.156 0.200
2024-12-02-01:36:47-root-INFO: Loss Change: 12.067 -> 7.021
2024-12-02-01:36:47-root-INFO: Regularization Change: 0.000 -> 5.943
2024-12-02-01:36:47-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-01:36:47-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-01:36:47-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-01:36:47-root-INFO: grad norm: 2.114 2.101 0.229
2024-12-02-01:36:48-root-INFO: grad norm: 1.811 1.797 0.227
2024-12-02-01:36:49-root-INFO: Loss Change: 6.823 -> 5.117
2024-12-02-01:36:49-root-INFO: Regularization Change: 0.000 -> 2.285
2024-12-02-01:36:49-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-01:36:49-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-01:36:49-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-01:36:49-root-INFO: grad norm: 2.247 2.231 0.270
2024-12-02-01:36:51-root-INFO: grad norm: 1.495 1.482 0.197
2024-12-02-01:36:51-root-INFO: Loss Change: 5.123 -> 3.720
2024-12-02-01:36:51-root-INFO: Regularization Change: 0.000 -> 1.418
2024-12-02-01:36:51-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-01:36:51-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-01:36:52-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-01:36:52-root-INFO: grad norm: 1.677 1.666 0.198
2024-12-02-01:36:53-root-INFO: grad norm: 1.217 1.210 0.121
2024-12-02-01:36:54-root-INFO: Loss Change: 3.782 -> 2.963
2024-12-02-01:36:54-root-INFO: Regularization Change: 0.000 -> 0.820
2024-12-02-01:36:54-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-01:36:54-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-01:36:54-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-01:36:54-root-INFO: grad norm: 1.409 1.401 0.150
2024-12-02-01:36:55-root-INFO: grad norm: 0.953 0.950 0.073
2024-12-02-01:36:56-root-INFO: Loss Change: 3.046 -> 2.440
2024-12-02-01:36:56-root-INFO: Regularization Change: 0.000 -> 0.597
2024-12-02-01:36:56-root-INFO: Undo step: 5
2024-12-02-01:36:56-root-INFO: Undo step: 6
2024-12-02-01:36:56-root-INFO: Undo step: 7
2024-12-02-01:36:56-root-INFO: Undo step: 8
2024-12-02-01:36:56-root-INFO: Undo step: 9
2024-12-02-01:36:56-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-01:36:57-root-INFO: grad norm: 12.851 12.831 0.725
2024-12-02-01:36:58-root-INFO: grad norm: 4.864 4.839 0.494
2024-12-02-01:36:58-root-INFO: Loss Change: 58.594 -> 13.378
2024-12-02-01:36:58-root-INFO: Regularization Change: 0.000 -> 58.859
2024-12-02-01:36:58-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-01:36:58-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-01:36:59-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-01:36:59-root-INFO: grad norm: 3.966 3.956 0.283
2024-12-02-01:37:00-root-INFO: grad norm: 2.504 2.490 0.270
2024-12-02-01:37:01-root-INFO: Loss Change: 12.973 -> 7.360
2024-12-02-01:37:01-root-INFO: Regularization Change: 0.000 -> 7.144
2024-12-02-01:37:01-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-01:37:01-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-01:37:01-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-01:37:01-root-INFO: grad norm: 2.279 2.269 0.208
2024-12-02-01:37:02-root-INFO: grad norm: 1.747 1.737 0.185
2024-12-02-01:37:03-root-INFO: Loss Change: 7.116 -> 4.928
2024-12-02-01:37:03-root-INFO: Regularization Change: 0.000 -> 2.394
2024-12-02-01:37:03-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-01:37:03-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-01:37:03-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-01:37:04-root-INFO: grad norm: 1.881 1.865 0.239
2024-12-02-01:37:05-root-INFO: grad norm: 1.556 1.545 0.183
2024-12-02-01:37:05-root-INFO: Loss Change: 4.835 -> 3.700
2024-12-02-01:37:05-root-INFO: Regularization Change: 0.000 -> 1.377
2024-12-02-01:37:05-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-01:37:05-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-01:37:06-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-01:37:06-root-INFO: grad norm: 1.838 1.825 0.218
2024-12-02-01:37:07-root-INFO: grad norm: 1.169 1.162 0.132
2024-12-02-01:37:08-root-INFO: Loss Change: 3.752 -> 2.832
2024-12-02-01:37:08-root-INFO: Regularization Change: 0.000 -> 0.906
2024-12-02-01:37:08-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-01:37:08-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-01:37:08-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-01:37:08-root-INFO: grad norm: 1.364 1.353 0.175
2024-12-02-01:37:09-root-INFO: grad norm: 0.837 0.832 0.089
2024-12-02-01:37:10-root-INFO: Loss Change: 2.923 -> 2.314
2024-12-02-01:37:10-root-INFO: Regularization Change: 0.000 -> 0.611
2024-12-02-01:37:10-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-01:37:10-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-01:37:11-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-01:37:11-root-INFO: grad norm: 1.106 1.098 0.134
2024-12-02-01:37:12-root-INFO: grad norm: 0.738 0.735 0.066
2024-12-02-01:37:12-root-INFO: Loss Change: 2.418 -> 2.004
2024-12-02-01:37:12-root-INFO: Regularization Change: 0.000 -> 0.451
2024-12-02-01:37:13-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-01:37:13-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-01:37:13-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-01:37:13-root-INFO: grad norm: 1.068 1.061 0.119
2024-12-02-01:37:14-root-INFO: grad norm: 0.641 0.639 0.055
2024-12-02-01:37:15-root-INFO: Loss Change: 2.136 -> 1.751
2024-12-02-01:37:15-root-INFO: Regularization Change: 0.000 -> 0.406
2024-12-02-01:37:15-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-01:37:15-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-01:37:15-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-01:37:15-root-INFO: grad norm: 0.940 0.934 0.104
2024-12-02-01:37:16-root-INFO: grad norm: 0.543 0.541 0.051
2024-12-02-01:37:17-root-INFO: Loss Change: 1.882 -> 1.559
2024-12-02-01:37:17-root-INFO: Regularization Change: 0.000 -> 0.360
2024-12-02-01:37:17-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-01:37:17-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-01:37:17-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-01:37:18-root-INFO: grad norm: 0.892 0.886 0.103
2024-12-02-01:37:19-root-INFO: grad norm: 0.561 0.559 0.041
2024-12-02-01:37:19-root-INFO: Loss Change: 1.678 -> 1.390
2024-12-02-01:37:19-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-01:37:19-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-01:37:19-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-01:37:20-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-01:37:20-root-INFO: grad norm: 0.995 0.990 0.093
2024-12-02-01:37:21-root-INFO: grad norm: 0.499 0.498 0.031
2024-12-02-01:37:22-root-INFO: Loss Change: 1.545 -> 1.076
2024-12-02-01:37:22-root-INFO: Regularization Change: 0.000 -> 0.591
2024-12-02-01:37:22-root-INFO: Undo step: 0
2024-12-02-01:37:22-root-INFO: Undo step: 1
2024-12-02-01:37:22-root-INFO: Undo step: 2
2024-12-02-01:37:22-root-INFO: Undo step: 3
2024-12-02-01:37:22-root-INFO: Undo step: 4
2024-12-02-01:37:22-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-01:37:23-root-INFO: grad norm: 11.701 11.693 0.412
2024-12-02-01:37:24-root-INFO: grad norm: 5.862 5.856 0.276
2024-12-02-01:37:24-root-INFO: Loss Change: 35.738 -> 6.580
2024-12-02-01:37:24-root-INFO: Regularization Change: 0.000 -> 46.036
2024-12-02-01:37:24-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-01:37:24-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-01:37:25-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-01:37:25-root-INFO: grad norm: 3.099 3.097 0.103
2024-12-02-01:37:26-root-INFO: grad norm: 1.540 1.538 0.080
2024-12-02-01:37:27-root-INFO: Loss Change: 6.332 -> 3.578
2024-12-02-01:37:27-root-INFO: Regularization Change: 0.000 -> 4.046
2024-12-02-01:37:27-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-01:37:27-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-01:37:27-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-01:37:27-root-INFO: grad norm: 1.880 1.879 0.085
2024-12-02-01:37:28-root-INFO: grad norm: 1.605 1.604 0.048
2024-12-02-01:37:29-root-INFO: Loss Change: 3.567 -> 2.623
2024-12-02-01:37:29-root-INFO: Regularization Change: 0.000 -> 1.167
2024-12-02-01:37:29-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-01:37:29-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-01:37:29-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-01:37:30-root-INFO: grad norm: 1.265 1.263 0.078
2024-12-02-01:37:31-root-INFO: grad norm: 0.814 0.812 0.052
2024-12-02-01:37:31-root-INFO: Loss Change: 2.659 -> 2.003
2024-12-02-01:37:31-root-INFO: Regularization Change: 0.000 -> 0.907
2024-12-02-01:37:31-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-01:37:31-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-01:37:32-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-01:37:32-root-INFO: grad norm: 0.941 0.937 0.077
2024-12-02-01:37:33-root-INFO: grad norm: 0.845 0.843 0.046
2024-12-02-01:37:34-root-INFO: Loss Change: 2.095 -> 1.538
2024-12-02-01:37:34-root-INFO: Regularization Change: 0.000 -> 0.622
2024-12-02-01:37:34-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-01:37:34-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-01:37:34-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-01:37:34-root-INFO: grad norm: 1.249 1.246 0.084
2024-12-02-01:37:35-root-INFO: grad norm: 0.980 0.979 0.035
2024-12-02-01:37:36-root-INFO: Loss Change: 1.684 -> 1.136
2024-12-02-01:37:36-root-INFO: Regularization Change: 0.000 -> 0.751
2024-12-02-01:37:36-root-INFO: Undo step: 0
2024-12-02-01:37:36-root-INFO: Undo step: 1
2024-12-02-01:37:36-root-INFO: Undo step: 2
2024-12-02-01:37:36-root-INFO: Undo step: 3
2024-12-02-01:37:36-root-INFO: Undo step: 4
2024-12-02-01:37:36-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-01:37:37-root-INFO: grad norm: 13.286 13.278 0.467
2024-12-02-01:37:38-root-INFO: grad norm: 6.956 6.949 0.310
2024-12-02-01:37:39-root-INFO: Loss Change: 37.988 -> 7.965
2024-12-02-01:37:39-root-INFO: Regularization Change: 0.000 -> 49.989
2024-12-02-01:37:39-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-01:37:39-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-01:37:39-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-01:37:39-root-INFO: grad norm: 3.544 3.543 0.104
2024-12-02-01:37:40-root-INFO: grad norm: 3.018 3.017 0.104
2024-12-02-01:37:41-root-INFO: Loss Change: 7.820 -> 4.868
2024-12-02-01:37:41-root-INFO: Regularization Change: 0.000 -> 6.240
2024-12-02-01:37:41-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-01:37:41-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-01:37:41-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-01:37:42-root-INFO: grad norm: 2.142 2.138 0.127
2024-12-02-01:37:42-root-INFO: grad norm: 1.861 1.859 0.080
2024-12-02-01:37:43-root-INFO: Loss Change: 4.932 -> 3.314
2024-12-02-01:37:43-root-INFO: Regularization Change: 0.000 -> 2.633
2024-12-02-01:37:43-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-01:37:43-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-01:37:44-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-01:37:44-root-INFO: grad norm: 1.923 1.921 0.100
2024-12-02-01:37:45-root-INFO: grad norm: 1.089 1.087 0.067
2024-12-02-01:37:46-root-INFO: Loss Change: 3.445 -> 2.658
2024-12-02-01:37:46-root-INFO: Regularization Change: 0.000 -> 0.955
2024-12-02-01:37:46-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-01:37:46-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-01:37:46-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-01:37:46-root-INFO: grad norm: 1.009 1.006 0.069
2024-12-02-01:37:47-root-INFO: grad norm: 2.152 2.152 0.055
2024-12-02-01:37:48-root-INFO: Loss Change: 2.756 -> 2.133
2024-12-02-01:37:48-root-INFO: Regularization Change: 0.000 -> 2.562
2024-12-02-01:37:48-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-01:37:48-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-01:37:48-root-INFO: loss_sample0_0: 2.133110523223877
2024-12-02-01:37:48-root-INFO: It takes 2221.781 seconds for image sample0
2024-12-02-01:37:48-root-INFO: lpips_score_sample0: 0.139
2024-12-02-01:37:48-root-INFO: psnr_score_sample0: 18.723
2024-12-02-01:37:48-root-INFO: ssim_score_sample0: 0.742
2024-12-02-01:37:48-root-INFO: mean_lpips: 0.1389942467212677
2024-12-02-01:37:48-root-INFO: best_mean_lpips: 0.1389942467212677
2024-12-02-01:37:48-root-INFO: mean_psnr: 18.723424911499023
2024-12-02-01:37:48-root-INFO: best_mean_psnr: 18.723424911499023
2024-12-02-01:37:48-root-INFO: mean_ssim: 0.7422500848770142
2024-12-02-01:37:48-root-INFO: best_mean_ssim: 0.7422500848770142
2024-12-02-01:37:48-root-INFO: final_loss: 2.133110523223877
2024-12-02-01:37:48-root-INFO: mean time: 2221.7812733650208
2024-12-02-01:37:48-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample3_iter2_lr0.03_10009 
 
Enjoy.
