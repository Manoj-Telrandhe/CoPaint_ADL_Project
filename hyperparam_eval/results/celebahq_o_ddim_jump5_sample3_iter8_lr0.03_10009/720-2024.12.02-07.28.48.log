2024-12-02-07:28:51-root-INFO: Prepare model...
2024-12-02-07:29:06-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-07:29:30-root-INFO: Start sampling
2024-12-02-07:29:34-root-INFO: step: 249 lr_xt 0.00019059
2024-12-02-07:29:34-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-07:29:34-root-INFO: Loss too large (77070.016->78612.750)! Learning rate decreased to 0.00015.
2024-12-02-07:29:35-root-INFO: grad norm: 15661.117 11248.846 10896.517
2024-12-02-07:29:35-root-INFO: grad norm: 14205.964 10378.394 9700.431
2024-12-02-07:29:35-root-INFO: grad norm: 15590.468 11542.606 10480.025
2024-12-02-07:29:36-root-INFO: Loss too large (28301.721->35851.566)! Learning rate decreased to 0.00012.
2024-12-02-07:29:36-root-INFO: grad norm: 16985.301 12431.854 11573.654
2024-12-02-07:29:36-root-INFO: Loss too large (28113.877->29014.209)! Learning rate decreased to 0.00010.
2024-12-02-07:29:37-root-INFO: grad norm: 13500.367 10439.840 8559.770
2024-12-02-07:29:37-root-INFO: grad norm: 13455.274 10629.398 8249.866
2024-12-02-07:29:37-root-INFO: Loss too large (22323.791->22381.936)! Learning rate decreased to 0.00008.
2024-12-02-07:29:38-root-INFO: grad norm: 9591.013 7722.024 5688.397
2024-12-02-07:29:38-root-INFO: Loss Change: 77070.016 -> 18637.553
2024-12-02-07:29:38-root-INFO: Regularization Change: 0.000 -> 18.364
2024-12-02-07:29:38-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-07:29:38-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-07:29:38-root-INFO: step: 248 lr_xt 0.00020082
2024-12-02-07:29:38-root-INFO: grad norm: 7522.279 5981.621 4561.239
2024-12-02-07:29:39-root-INFO: Loss too large (18708.328->30950.424)! Learning rate decreased to 0.00016.
2024-12-02-07:29:39-root-INFO: Loss too large (18708.328->24731.029)! Learning rate decreased to 0.00013.
2024-12-02-07:29:39-root-INFO: Loss too large (18708.328->21088.293)! Learning rate decreased to 0.00010.
2024-12-02-07:29:39-root-INFO: Loss too large (18708.328->19049.004)! Learning rate decreased to 0.00008.
2024-12-02-07:29:39-root-INFO: grad norm: 6006.247 4799.242 3611.410
2024-12-02-07:29:40-root-INFO: grad norm: 4839.783 3873.669 2901.411
2024-12-02-07:29:40-root-INFO: grad norm: 3853.131 3060.852 2340.470
2024-12-02-07:29:41-root-INFO: grad norm: 3088.558 2486.781 1831.697
2024-12-02-07:29:41-root-INFO: grad norm: 2467.133 1954.663 1505.337
2024-12-02-07:29:42-root-INFO: grad norm: 1997.609 1618.831 1170.396
2024-12-02-07:29:42-root-INFO: grad norm: 1630.859 1298.401 986.842
2024-12-02-07:29:42-root-INFO: Loss Change: 18708.328 -> 16490.844
2024-12-02-07:29:42-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-02-07:29:42-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-02-07:29:42-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-07:29:43-root-INFO: step: 247 lr_xt 0.00021156
2024-12-02-07:29:43-root-INFO: grad norm: 1317.771 1091.657 738.110
2024-12-02-07:29:43-root-INFO: Loss too large (16314.109->16375.895)! Learning rate decreased to 0.00017.
2024-12-02-07:29:43-root-INFO: grad norm: 2397.187 1881.560 1485.340
2024-12-02-07:29:43-root-INFO: Loss too large (16294.695->16717.855)! Learning rate decreased to 0.00014.
2024-12-02-07:29:44-root-INFO: Loss too large (16294.695->16415.688)! Learning rate decreased to 0.00011.
2024-12-02-07:29:44-root-INFO: grad norm: 2587.392 2070.380 1551.812
2024-12-02-07:29:45-root-INFO: grad norm: 2841.159 2266.546 1713.170
2024-12-02-07:29:45-root-INFO: grad norm: 3133.605 2503.056 1885.256
2024-12-02-07:29:45-root-INFO: grad norm: 3486.773 2802.359 2074.698
2024-12-02-07:29:46-root-INFO: Loss too large (16179.855->16200.448)! Learning rate decreased to 0.00009.
2024-12-02-07:29:46-root-INFO: grad norm: 2508.562 2005.776 1506.567
2024-12-02-07:29:46-root-INFO: grad norm: 1822.620 1495.878 1041.294
2024-12-02-07:29:47-root-INFO: Loss Change: 16314.109 -> 15773.575
2024-12-02-07:29:47-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-02-07:29:47-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-02-07:29:47-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-07:29:47-root-INFO: step: 246 lr_xt 0.00022285
2024-12-02-07:29:47-root-INFO: grad norm: 1387.820 1137.020 795.758
2024-12-02-07:29:47-root-INFO: Loss too large (15552.600->15703.281)! Learning rate decreased to 0.00018.
2024-12-02-07:29:47-root-INFO: Loss too large (15552.600->15579.354)! Learning rate decreased to 0.00014.
2024-12-02-07:29:48-root-INFO: grad norm: 1923.865 1584.011 1091.863
2024-12-02-07:29:48-root-INFO: Loss too large (15514.247->15563.113)! Learning rate decreased to 0.00011.
2024-12-02-07:29:48-root-INFO: grad norm: 2028.313 1629.030 1208.436
2024-12-02-07:29:49-root-INFO: grad norm: 2160.977 1786.862 1215.297
2024-12-02-07:29:49-root-INFO: grad norm: 2300.922 1839.621 1382.042
2024-12-02-07:29:50-root-INFO: grad norm: 2460.130 2032.338 1386.305
2024-12-02-07:29:50-root-INFO: grad norm: 2625.229 2094.931 1582.116
2024-12-02-07:29:51-root-INFO: grad norm: 2812.903 2318.248 1593.157
2024-12-02-07:29:51-root-INFO: Loss Change: 15552.600 -> 15229.972
2024-12-02-07:29:51-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-07:29:51-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-02-07:29:51-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-07:29:51-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-07:29:51-root-INFO: grad norm: 2957.373 2358.557 1784.170
2024-12-02-07:29:52-root-INFO: Loss too large (15122.697->16864.148)! Learning rate decreased to 0.00019.
2024-12-02-07:29:52-root-INFO: Loss too large (15122.697->15898.022)! Learning rate decreased to 0.00015.
2024-12-02-07:29:52-root-INFO: Loss too large (15122.697->15349.080)! Learning rate decreased to 0.00012.
2024-12-02-07:29:52-root-INFO: grad norm: 3002.844 2475.627 1699.513
2024-12-02-07:29:53-root-INFO: grad norm: 3062.967 2445.000 1844.923
2024-12-02-07:29:53-root-INFO: grad norm: 3135.930 2589.807 1768.320
2024-12-02-07:29:54-root-INFO: grad norm: 3207.286 2561.394 1930.271
2024-12-02-07:29:54-root-INFO: grad norm: 3284.396 2711.891 1852.810
2024-12-02-07:29:55-root-INFO: grad norm: 3358.972 2684.499 2018.950
2024-12-02-07:29:55-root-INFO: grad norm: 3439.016 2837.210 1943.469
2024-12-02-07:29:55-root-INFO: Loss Change: 15122.697 -> 14723.989
2024-12-02-07:29:55-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-07:29:55-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-07:29:55-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-07:29:56-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-07:29:56-root-INFO: grad norm: 3397.324 2752.918 1990.791
2024-12-02-07:29:56-root-INFO: Loss too large (14595.376->16826.703)! Learning rate decreased to 0.00020.
2024-12-02-07:29:56-root-INFO: Loss too large (14595.376->15547.572)! Learning rate decreased to 0.00016.
2024-12-02-07:29:56-root-INFO: Loss too large (14595.376->14826.122)! Learning rate decreased to 0.00013.
2024-12-02-07:29:57-root-INFO: grad norm: 3216.758 2666.628 1799.063
2024-12-02-07:29:57-root-INFO: grad norm: 3128.094 2530.478 1838.927
2024-12-02-07:29:58-root-INFO: grad norm: 3062.520 2555.479 1687.767
2024-12-02-07:29:58-root-INFO: grad norm: 3012.640 2432.683 1777.091
2024-12-02-07:29:59-root-INFO: grad norm: 2968.734 2482.053 1628.739
2024-12-02-07:29:59-root-INFO: grad norm: 2930.259 2365.639 1729.211
2024-12-02-07:29:59-root-INFO: grad norm: 2891.489 2419.182 1583.751
2024-12-02-07:30:00-root-INFO: Loss Change: 14595.376 -> 13887.530
2024-12-02-07:30:00-root-INFO: Regularization Change: 0.000 -> 0.588
2024-12-02-07:30:00-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-07:30:00-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:00-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-07:30:00-root-INFO: grad norm: 2944.914 2366.319 1753.012
2024-12-02-07:30:00-root-INFO: Loss too large (13795.885->15471.473)! Learning rate decreased to 0.00021.
2024-12-02-07:30:00-root-INFO: Loss too large (13795.885->14497.118)! Learning rate decreased to 0.00017.
2024-12-02-07:30:00-root-INFO: Loss too large (13795.885->13948.680)! Learning rate decreased to 0.00013.
2024-12-02-07:30:01-root-INFO: grad norm: 2746.593 2306.098 1491.874
2024-12-02-07:30:01-root-INFO: grad norm: 2607.846 2104.850 1539.633
2024-12-02-07:30:02-root-INFO: grad norm: 2483.475 2097.255 1330.102
2024-12-02-07:30:02-root-INFO: grad norm: 2376.288 1922.153 1397.166
2024-12-02-07:30:03-root-INFO: grad norm: 2274.842 1926.273 1210.115
2024-12-02-07:30:03-root-INFO: grad norm: 2186.468 1772.823 1279.741
2024-12-02-07:30:04-root-INFO: grad norm: 2101.128 1782.591 1112.253
2024-12-02-07:30:04-root-INFO: Loss Change: 13795.885 -> 13042.900
2024-12-02-07:30:04-root-INFO: Regularization Change: 0.000 -> 0.626
2024-12-02-07:30:04-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-07:30:04-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:04-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-07:30:04-root-INFO: grad norm: 1942.069 1605.651 1092.482
2024-12-02-07:30:04-root-INFO: Loss too large (12825.530->13416.403)! Learning rate decreased to 0.00022.
2024-12-02-07:30:05-root-INFO: Loss too large (12825.530->13035.573)! Learning rate decreased to 0.00018.
2024-12-02-07:30:05-root-INFO: Loss too large (12825.530->12826.002)! Learning rate decreased to 0.00014.
2024-12-02-07:30:05-root-INFO: grad norm: 1776.536 1510.413 935.270
2024-12-02-07:30:06-root-INFO: grad norm: 1649.168 1354.140 941.308
2024-12-02-07:30:06-root-INFO: grad norm: 1540.327 1326.244 783.381
2024-12-02-07:30:07-root-INFO: grad norm: 1446.141 1189.090 823.037
2024-12-02-07:30:07-root-INFO: grad norm: 1362.134 1181.024 678.668
2024-12-02-07:30:08-root-INFO: grad norm: 1286.700 1063.540 724.210
2024-12-02-07:30:08-root-INFO: grad norm: 1216.944 1061.309 595.464
2024-12-02-07:30:09-root-INFO: Loss Change: 12825.530 -> 12147.533
2024-12-02-07:30:09-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-07:30:09-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-07:30:09-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:09-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-07:30:09-root-INFO: grad norm: 1090.017 929.087 570.030
2024-12-02-07:30:09-root-INFO: Loss too large (12037.754->12047.783)! Learning rate decreased to 0.00023.
2024-12-02-07:30:10-root-INFO: grad norm: 1719.187 1472.052 888.069
2024-12-02-07:30:10-root-INFO: Loss too large (11989.252->12137.900)! Learning rate decreased to 0.00018.
2024-12-02-07:30:10-root-INFO: grad norm: 2220.427 1844.475 1236.207
2024-12-02-07:30:10-root-INFO: Loss too large (11976.379->12026.762)! Learning rate decreased to 0.00015.
2024-12-02-07:30:11-root-INFO: grad norm: 1984.881 1695.723 1031.639
2024-12-02-07:30:11-root-INFO: grad norm: 1786.021 1488.875 986.470
2024-12-02-07:30:12-root-INFO: grad norm: 1608.561 1386.106 816.197
2024-12-02-07:30:12-root-INFO: grad norm: 1456.067 1220.398 794.203
2024-12-02-07:30:13-root-INFO: grad norm: 1322.889 1151.599 651.042
2024-12-02-07:30:13-root-INFO: Loss Change: 12037.754 -> 11409.975
2024-12-02-07:30:13-root-INFO: Regularization Change: 0.000 -> 0.833
2024-12-02-07:30:13-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-07:30:13-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:13-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-07:30:13-root-INFO: grad norm: 1251.650 1078.754 634.759
2024-12-02-07:30:14-root-INFO: Loss too large (11225.317->11305.736)! Learning rate decreased to 0.00024.
2024-12-02-07:30:14-root-INFO: grad norm: 1996.702 1716.110 1020.678
2024-12-02-07:30:14-root-INFO: Loss too large (11199.982->11458.615)! Learning rate decreased to 0.00019.
2024-12-02-07:30:14-root-INFO: Loss too large (11199.982->11212.284)! Learning rate decreased to 0.00016.
2024-12-02-07:30:15-root-INFO: grad norm: 1720.106 1447.550 929.174
2024-12-02-07:30:15-root-INFO: grad norm: 1491.606 1297.852 735.166
2024-12-02-07:30:16-root-INFO: grad norm: 1309.327 1110.047 694.358
2024-12-02-07:30:16-root-INFO: grad norm: 1159.136 1021.727 547.422
2024-12-02-07:30:17-root-INFO: grad norm: 1038.474 891.435 532.702
2024-12-02-07:30:17-root-INFO: grad norm: 940.606 840.997 421.264
2024-12-02-07:30:17-root-INFO: Loss Change: 11225.317 -> 10576.618
2024-12-02-07:30:17-root-INFO: Regularization Change: 0.000 -> 0.818
2024-12-02-07:30:17-root-INFO: Undo step: 240
2024-12-02-07:30:17-root-INFO: Undo step: 241
2024-12-02-07:30:17-root-INFO: Undo step: 242
2024-12-02-07:30:17-root-INFO: Undo step: 243
2024-12-02-07:30:17-root-INFO: Undo step: 244
2024-12-02-07:30:18-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-07:30:18-root-INFO: grad norm: 10652.583 7752.672 7305.724
2024-12-02-07:30:18-root-INFO: Loss too large (18129.812->29307.580)! Learning rate decreased to 0.00019.
2024-12-02-07:30:18-root-INFO: Loss too large (18129.812->22131.750)! Learning rate decreased to 0.00015.
2024-12-02-07:30:18-root-INFO: grad norm: 10004.068 7274.157 6867.898
2024-12-02-07:30:19-root-INFO: grad norm: 10209.204 7606.818 6809.124
2024-12-02-07:30:19-root-INFO: Loss too large (16859.014->17806.121)! Learning rate decreased to 0.00012.
2024-12-02-07:30:20-root-INFO: grad norm: 7252.062 5466.477 4765.505
2024-12-02-07:30:20-root-INFO: grad norm: 5378.968 4194.083 3367.932
2024-12-02-07:30:20-root-INFO: grad norm: 4639.613 3594.087 2934.033
2024-12-02-07:30:21-root-INFO: grad norm: 4082.127 3290.856 2415.373
2024-12-02-07:30:21-root-INFO: grad norm: 3795.781 2981.028 2349.772
2024-12-02-07:30:22-root-INFO: Loss Change: 18129.812 -> 13098.708
2024-12-02-07:30:22-root-INFO: Regularization Change: 0.000 -> 1.447
2024-12-02-07:30:22-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-07:30:22-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-07:30:22-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-07:30:22-root-INFO: grad norm: 3529.912 2897.395 2016.279
2024-12-02-07:30:22-root-INFO: Loss too large (12995.621->15109.520)! Learning rate decreased to 0.00020.
2024-12-02-07:30:22-root-INFO: Loss too large (12995.621->13896.202)! Learning rate decreased to 0.00016.
2024-12-02-07:30:22-root-INFO: Loss too large (12995.621->13194.812)! Learning rate decreased to 0.00013.
2024-12-02-07:30:23-root-INFO: grad norm: 3161.355 2543.721 1877.139
2024-12-02-07:30:23-root-INFO: grad norm: 2893.942 2416.063 1592.965
2024-12-02-07:30:24-root-INFO: grad norm: 2706.747 2168.575 1619.804
2024-12-02-07:30:24-root-INFO: grad norm: 2538.986 2136.076 1372.453
2024-12-02-07:30:25-root-INFO: grad norm: 2404.120 1922.983 1442.890
2024-12-02-07:30:25-root-INFO: grad norm: 2277.252 1925.187 1216.360
2024-12-02-07:30:26-root-INFO: grad norm: 2163.352 1731.299 1297.188
2024-12-02-07:30:26-root-INFO: Loss Change: 12995.621 -> 12094.700
2024-12-02-07:30:26-root-INFO: Regularization Change: 0.000 -> 0.639
2024-12-02-07:30:26-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-07:30:26-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:26-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-07:30:26-root-INFO: grad norm: 1887.827 1583.630 1027.622
2024-12-02-07:30:27-root-INFO: Loss too large (11850.270->12293.613)! Learning rate decreased to 0.00021.
2024-12-02-07:30:27-root-INFO: Loss too large (11850.270->11989.793)! Learning rate decreased to 0.00017.
2024-12-02-07:30:27-root-INFO: grad norm: 2377.832 1902.152 1426.850
2024-12-02-07:30:27-root-INFO: Loss too large (11822.604->11886.827)! Learning rate decreased to 0.00013.
2024-12-02-07:30:28-root-INFO: grad norm: 2142.738 1816.721 1136.155
2024-12-02-07:30:28-root-INFO: grad norm: 1959.055 1572.641 1168.202
2024-12-02-07:30:29-root-INFO: grad norm: 1798.483 1538.817 930.904
2024-12-02-07:30:29-root-INFO: grad norm: 1662.821 1339.302 985.516
2024-12-02-07:30:30-root-INFO: grad norm: 1538.570 1326.118 780.133
2024-12-02-07:30:30-root-INFO: grad norm: 1430.902 1158.051 840.476
2024-12-02-07:30:30-root-INFO: Loss Change: 11850.270 -> 11248.800
2024-12-02-07:30:30-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-02-07:30:30-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-07:30:30-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:31-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-07:30:31-root-INFO: grad norm: 1378.393 1170.588 727.799
2024-12-02-07:30:31-root-INFO: Loss too large (11039.910->11206.719)! Learning rate decreased to 0.00022.
2024-12-02-07:30:31-root-INFO: Loss too large (11039.910->11063.938)! Learning rate decreased to 0.00018.
2024-12-02-07:30:32-root-INFO: grad norm: 1664.100 1352.846 969.040
2024-12-02-07:30:32-root-INFO: grad norm: 2128.209 1805.689 1126.393
2024-12-02-07:30:32-root-INFO: Loss too large (10979.520->11030.662)! Learning rate decreased to 0.00014.
2024-12-02-07:30:33-root-INFO: grad norm: 1868.514 1518.208 1089.215
2024-12-02-07:30:33-root-INFO: grad norm: 1645.782 1413.501 842.978
2024-12-02-07:30:33-root-INFO: grad norm: 1464.836 1194.912 847.307
2024-12-02-07:30:34-root-INFO: grad norm: 1305.698 1135.094 645.296
2024-12-02-07:30:34-root-INFO: grad norm: 1169.103 961.027 665.754
2024-12-02-07:30:35-root-INFO: Loss Change: 11039.910 -> 10539.029
2024-12-02-07:30:35-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-02-07:30:35-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-07:30:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:35-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-07:30:35-root-INFO: grad norm: 1107.838 959.305 554.112
2024-12-02-07:30:35-root-INFO: Loss too large (10438.262->10529.746)! Learning rate decreased to 0.00023.
2024-12-02-07:30:35-root-INFO: Loss too large (10438.262->10440.770)! Learning rate decreased to 0.00018.
2024-12-02-07:30:36-root-INFO: grad norm: 1312.960 1091.163 730.223
2024-12-02-07:30:36-root-INFO: grad norm: 1615.386 1384.984 831.439
2024-12-02-07:30:36-root-INFO: Loss too large (10368.800->10369.801)! Learning rate decreased to 0.00015.
2024-12-02-07:30:37-root-INFO: grad norm: 1371.312 1135.857 768.327
2024-12-02-07:30:37-root-INFO: grad norm: 1170.723 1021.895 571.247
2024-12-02-07:30:38-root-INFO: grad norm: 1012.925 848.816 552.745
2024-12-02-07:30:38-root-INFO: grad norm: 885.735 790.595 399.358
2024-12-02-07:30:39-root-INFO: grad norm: 787.652 673.776 407.949
2024-12-02-07:30:39-root-INFO: Loss Change: 10438.262 -> 10013.896
2024-12-02-07:30:39-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-07:30:39-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-07:30:39-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:39-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-07:30:39-root-INFO: grad norm: 703.488 626.521 319.949
2024-12-02-07:30:40-root-INFO: grad norm: 991.718 832.010 539.689
2024-12-02-07:30:40-root-INFO: Loss too large (9765.921->9839.648)! Learning rate decreased to 0.00024.
2024-12-02-07:30:40-root-INFO: grad norm: 1569.422 1357.013 788.416
2024-12-02-07:30:40-root-INFO: Loss too large (9765.223->9902.724)! Learning rate decreased to 0.00019.
2024-12-02-07:30:41-root-INFO: grad norm: 1849.810 1536.969 1029.330
2024-12-02-07:30:41-root-INFO: Loss too large (9758.902->9768.357)! Learning rate decreased to 0.00016.
2024-12-02-07:30:41-root-INFO: grad norm: 1440.043 1248.086 718.335
2024-12-02-07:30:42-root-INFO: grad norm: 1143.678 957.720 625.117
2024-12-02-07:30:42-root-INFO: grad norm: 925.735 823.782 422.337
2024-12-02-07:30:43-root-INFO: grad norm: 771.678 661.004 398.197
2024-12-02-07:30:43-root-INFO: Loss Change: 9838.945 -> 9434.327
2024-12-02-07:30:43-root-INFO: Regularization Change: 0.000 -> 0.609
2024-12-02-07:30:43-root-INFO: Undo step: 240
2024-12-02-07:30:43-root-INFO: Undo step: 241
2024-12-02-07:30:43-root-INFO: Undo step: 242
2024-12-02-07:30:43-root-INFO: Undo step: 243
2024-12-02-07:30:43-root-INFO: Undo step: 244
2024-12-02-07:30:43-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-07:30:43-root-INFO: grad norm: 15503.285 12588.821 9048.394
2024-12-02-07:30:44-root-INFO: Loss too large (22438.795->36644.836)! Learning rate decreased to 0.00019.
2024-12-02-07:30:44-root-INFO: Loss too large (22438.795->28170.105)! Learning rate decreased to 0.00015.
2024-12-02-07:30:44-root-INFO: grad norm: 14151.278 11103.639 8773.136
2024-12-02-07:30:45-root-INFO: grad norm: 14561.219 11845.105 8468.919
2024-12-02-07:30:45-root-INFO: Loss too large (18135.006->21929.109)! Learning rate decreased to 0.00012.
2024-12-02-07:30:45-root-INFO: grad norm: 11196.288 8996.555 6664.747
2024-12-02-07:30:46-root-INFO: grad norm: 8035.305 6504.312 4718.056
2024-12-02-07:30:46-root-INFO: grad norm: 6603.458 5335.209 3891.170
2024-12-02-07:30:47-root-INFO: grad norm: 5370.809 4364.337 3130.199
2024-12-02-07:30:47-root-INFO: grad norm: 4650.212 3749.094 2751.139
2024-12-02-07:30:48-root-INFO: Loss Change: 22438.795 -> 11445.570
2024-12-02-07:30:48-root-INFO: Regularization Change: 0.000 -> 3.672
2024-12-02-07:30:48-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-07:30:48-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-07:30:48-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-07:30:48-root-INFO: grad norm: 4002.836 3239.171 2351.695
2024-12-02-07:30:48-root-INFO: Loss too large (11404.196->13876.887)! Learning rate decreased to 0.00020.
2024-12-02-07:30:48-root-INFO: Loss too large (11404.196->12409.514)! Learning rate decreased to 0.00016.
2024-12-02-07:30:48-root-INFO: Loss too large (11404.196->11563.503)! Learning rate decreased to 0.00013.
2024-12-02-07:30:49-root-INFO: grad norm: 3226.201 2612.977 1892.280
2024-12-02-07:30:49-root-INFO: grad norm: 2657.979 2201.622 1489.198
2024-12-02-07:30:50-root-INFO: grad norm: 2283.329 1844.834 1345.429
2024-12-02-07:30:50-root-INFO: grad norm: 1982.951 1670.885 1067.819
2024-12-02-07:30:51-root-INFO: grad norm: 1753.661 1416.122 1034.372
2024-12-02-07:30:51-root-INFO: grad norm: 1556.692 1330.675 807.833
2024-12-02-07:30:52-root-INFO: grad norm: 1393.813 1129.949 816.046
2024-12-02-07:30:52-root-INFO: Loss Change: 11404.196 -> 10340.940
2024-12-02-07:30:52-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-07:30:52-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-07:30:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:52-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-07:30:52-root-INFO: grad norm: 1143.538 959.364 622.333
2024-12-02-07:30:52-root-INFO: Loss too large (10107.975->10113.048)! Learning rate decreased to 0.00021.
2024-12-02-07:30:53-root-INFO: grad norm: 1600.848 1275.091 967.915
2024-12-02-07:30:53-root-INFO: Loss too large (10057.250->10161.688)! Learning rate decreased to 0.00017.
2024-12-02-07:30:53-root-INFO: grad norm: 1948.880 1643.224 1047.831
2024-12-02-07:30:54-root-INFO: Loss too large (10037.354->10063.209)! Learning rate decreased to 0.00013.
2024-12-02-07:30:54-root-INFO: grad norm: 1642.366 1324.803 970.701
2024-12-02-07:30:54-root-INFO: grad norm: 1402.932 1211.824 706.894
2024-12-02-07:30:55-root-INFO: grad norm: 1226.132 1000.469 708.844
2024-12-02-07:30:55-root-INFO: grad norm: 1077.027 948.962 509.370
2024-12-02-07:30:56-root-INFO: grad norm: 957.053 792.249 536.928
2024-12-02-07:30:56-root-INFO: Loss Change: 10107.975 -> 9674.604
2024-12-02-07:30:56-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-02-07:30:56-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-07:30:56-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:30:56-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-07:30:57-root-INFO: grad norm: 900.173 775.847 456.479
2024-12-02-07:30:57-root-INFO: grad norm: 1649.524 1317.241 992.877
2024-12-02-07:30:57-root-INFO: Loss too large (9479.578->9899.940)! Learning rate decreased to 0.00022.
2024-12-02-07:30:57-root-INFO: Loss too large (9479.578->9628.156)! Learning rate decreased to 0.00018.
2024-12-02-07:30:58-root-INFO: grad norm: 1994.525 1690.354 1058.694
2024-12-02-07:30:58-root-INFO: Loss too large (9478.406->9513.801)! Learning rate decreased to 0.00014.
2024-12-02-07:30:58-root-INFO: grad norm: 1617.997 1317.898 938.647
2024-12-02-07:30:59-root-INFO: grad norm: 1327.301 1153.603 656.451
2024-12-02-07:30:59-root-INFO: grad norm: 1108.361 913.787 627.262
2024-12-02-07:31:00-root-INFO: grad norm: 931.792 830.987 421.542
2024-12-02-07:31:00-root-INFO: grad norm: 801.398 673.976 433.584
2024-12-02-07:31:01-root-INFO: Loss Change: 9490.534 -> 9137.831
2024-12-02-07:31:01-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-02-07:31:01-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-07:31:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:01-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-07:31:01-root-INFO: grad norm: 735.491 653.390 337.680
2024-12-02-07:31:01-root-INFO: grad norm: 1348.126 1118.293 752.903
2024-12-02-07:31:01-root-INFO: Loss too large (9032.796->9294.963)! Learning rate decreased to 0.00023.
2024-12-02-07:31:02-root-INFO: Loss too large (9032.796->9115.453)! Learning rate decreased to 0.00018.
2024-12-02-07:31:02-root-INFO: grad norm: 1578.712 1360.422 800.990
2024-12-02-07:31:02-root-INFO: Loss too large (9017.932->9023.474)! Learning rate decreased to 0.00015.
2024-12-02-07:31:03-root-INFO: grad norm: 1248.512 1042.064 687.665
2024-12-02-07:31:03-root-INFO: grad norm: 998.794 880.622 471.269
2024-12-02-07:31:04-root-INFO: grad norm: 823.741 699.641 434.800
2024-12-02-07:31:04-root-INFO: grad norm: 696.650 634.144 288.414
2024-12-02-07:31:05-root-INFO: grad norm: 608.276 533.326 292.512
2024-12-02-07:31:05-root-INFO: Loss Change: 9044.738 -> 8744.219
2024-12-02-07:31:05-root-INFO: Regularization Change: 0.000 -> 0.397
2024-12-02-07:31:05-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-07:31:05-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:05-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-07:31:05-root-INFO: grad norm: 555.883 504.458 233.512
2024-12-02-07:31:06-root-INFO: grad norm: 489.208 442.713 208.156
2024-12-02-07:31:06-root-INFO: grad norm: 637.705 585.216 253.359
2024-12-02-07:31:07-root-INFO: grad norm: 1155.478 973.403 622.587
2024-12-02-07:31:07-root-INFO: Loss too large (8458.516->8638.462)! Learning rate decreased to 0.00024.
2024-12-02-07:31:07-root-INFO: Loss too large (8458.516->8507.765)! Learning rate decreased to 0.00019.
2024-12-02-07:31:07-root-INFO: grad norm: 1295.825 1133.123 628.645
2024-12-02-07:31:08-root-INFO: grad norm: 1466.925 1234.004 793.159
2024-12-02-07:31:08-root-INFO: grad norm: 1675.055 1448.736 840.817
2024-12-02-07:31:09-root-INFO: Loss too large (8420.190->8429.264)! Learning rate decreased to 0.00016.
2024-12-02-07:31:09-root-INFO: grad norm: 1247.428 1051.801 670.666
2024-12-02-07:31:09-root-INFO: Loss Change: 8590.109 -> 8282.323
2024-12-02-07:31:09-root-INFO: Regularization Change: 0.000 -> 0.600
2024-12-02-07:31:09-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-07:31:09-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:10-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-07:31:10-root-INFO: grad norm: 808.257 716.186 374.644
2024-12-02-07:31:10-root-INFO: Loss too large (8172.238->8215.830)! Learning rate decreased to 0.00026.
2024-12-02-07:31:10-root-INFO: grad norm: 1189.613 1005.104 636.352
2024-12-02-07:31:11-root-INFO: Loss too large (8167.281->8225.811)! Learning rate decreased to 0.00020.
2024-12-02-07:31:11-root-INFO: grad norm: 1304.862 1133.072 647.157
2024-12-02-07:31:12-root-INFO: grad norm: 1438.406 1216.897 766.924
2024-12-02-07:31:12-root-INFO: grad norm: 1594.307 1378.502 800.967
2024-12-02-07:31:13-root-INFO: grad norm: 1758.547 1487.373 938.194
2024-12-02-07:31:13-root-INFO: grad norm: 1946.316 1676.289 989.041
2024-12-02-07:31:13-root-INFO: Loss too large (8118.386->8133.517)! Learning rate decreased to 0.00016.
2024-12-02-07:31:14-root-INFO: grad norm: 1379.854 1170.606 730.533
2024-12-02-07:31:14-root-INFO: Loss Change: 8172.238 -> 7935.303
2024-12-02-07:31:14-root-INFO: Regularization Change: 0.000 -> 0.408
2024-12-02-07:31:14-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-07:31:14-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:14-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-07:31:14-root-INFO: grad norm: 782.864 662.322 417.380
2024-12-02-07:31:15-root-INFO: grad norm: 1166.359 950.999 675.275
2024-12-02-07:31:15-root-INFO: Loss too large (7756.435->7933.025)! Learning rate decreased to 0.00027.
2024-12-02-07:31:15-root-INFO: Loss too large (7756.435->7795.275)! Learning rate decreased to 0.00021.
2024-12-02-07:31:15-root-INFO: grad norm: 1169.679 1016.813 578.135
2024-12-02-07:31:16-root-INFO: grad norm: 1212.651 1026.572 645.501
2024-12-02-07:31:16-root-INFO: grad norm: 1278.628 1121.317 614.442
2024-12-02-07:31:17-root-INFO: grad norm: 1355.607 1156.260 707.626
2024-12-02-07:31:17-root-INFO: grad norm: 1446.980 1267.795 697.458
2024-12-02-07:31:18-root-INFO: grad norm: 1543.910 1319.564 801.504
2024-12-02-07:31:18-root-INFO: Loss Change: 7778.063 -> 7618.909
2024-12-02-07:31:18-root-INFO: Regularization Change: 0.000 -> 0.492
2024-12-02-07:31:18-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-07:31:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:18-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-07:31:19-root-INFO: grad norm: 1188.901 1049.992 557.675
2024-12-02-07:31:19-root-INFO: Loss too large (7504.309->7712.449)! Learning rate decreased to 0.00028.
2024-12-02-07:31:19-root-INFO: Loss too large (7504.309->7559.747)! Learning rate decreased to 0.00023.
2024-12-02-07:31:19-root-INFO: grad norm: 1211.511 1044.116 614.475
2024-12-02-07:31:20-root-INFO: grad norm: 1251.465 1109.513 578.917
2024-12-02-07:31:20-root-INFO: grad norm: 1293.289 1115.204 654.918
2024-12-02-07:31:21-root-INFO: grad norm: 1339.752 1183.863 627.218
2024-12-02-07:31:21-root-INFO: grad norm: 1387.396 1197.042 701.398
2024-12-02-07:31:22-root-INFO: grad norm: 1439.999 1269.193 680.254
2024-12-02-07:31:22-root-INFO: grad norm: 1493.595 1289.224 754.141
2024-12-02-07:31:23-root-INFO: Loss Change: 7504.309 -> 7335.619
2024-12-02-07:31:23-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-07:31:23-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-07:31:23-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:23-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-07:31:23-root-INFO: grad norm: 1391.896 1218.917 672.022
2024-12-02-07:31:23-root-INFO: Loss too large (7180.046->7523.941)! Learning rate decreased to 0.00030.
2024-12-02-07:31:23-root-INFO: Loss too large (7180.046->7288.977)! Learning rate decreased to 0.00024.
2024-12-02-07:31:24-root-INFO: grad norm: 1416.763 1230.496 702.208
2024-12-02-07:31:24-root-INFO: grad norm: 1448.999 1274.441 689.491
2024-12-02-07:31:25-root-INFO: grad norm: 1480.398 1283.932 736.950
2024-12-02-07:31:25-root-INFO: grad norm: 1512.776 1330.234 720.395
2024-12-02-07:31:26-root-INFO: grad norm: 1542.028 1336.707 768.808
2024-12-02-07:31:26-root-INFO: grad norm: 1572.648 1382.434 749.732
2024-12-02-07:31:26-root-INFO: grad norm: 1599.404 1386.207 797.825
2024-12-02-07:31:27-root-INFO: Loss Change: 7180.046 -> 7021.634
2024-12-02-07:31:27-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-07:31:27-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-07:31:27-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:27-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-07:31:27-root-INFO: grad norm: 1111.279 986.237 512.131
2024-12-02-07:31:27-root-INFO: Loss too large (6877.402->7079.127)! Learning rate decreased to 0.00031.
2024-12-02-07:31:27-root-INFO: Loss too large (6877.402->6931.418)! Learning rate decreased to 0.00025.
2024-12-02-07:31:28-root-INFO: grad norm: 1090.862 952.824 531.137
2024-12-02-07:31:28-root-INFO: grad norm: 1082.370 967.128 485.991
2024-12-02-07:31:29-root-INFO: grad norm: 1079.075 943.552 523.558
2024-12-02-07:31:29-root-INFO: grad norm: 1077.601 962.221 485.134
2024-12-02-07:31:30-root-INFO: grad norm: 1076.956 942.258 521.521
2024-12-02-07:31:30-root-INFO: grad norm: 1076.224 960.349 485.786
2024-12-02-07:31:31-root-INFO: grad norm: 1075.727 941.558 520.246
2024-12-02-07:31:31-root-INFO: Loss Change: 6877.402 -> 6688.747
2024-12-02-07:31:31-root-INFO: Regularization Change: 0.000 -> 0.368
2024-12-02-07:31:31-root-INFO: Undo step: 235
2024-12-02-07:31:31-root-INFO: Undo step: 236
2024-12-02-07:31:31-root-INFO: Undo step: 237
2024-12-02-07:31:31-root-INFO: Undo step: 238
2024-12-02-07:31:31-root-INFO: Undo step: 239
2024-12-02-07:31:31-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-07:31:31-root-INFO: grad norm: 4574.228 3713.825 2670.406
2024-12-02-07:31:32-root-INFO: grad norm: 3581.770 2768.392 2272.682
2024-12-02-07:31:32-root-INFO: Loss too large (9353.621->10066.648)! Learning rate decreased to 0.00024.
2024-12-02-07:31:32-root-INFO: grad norm: 3951.648 3262.891 2229.139
2024-12-02-07:31:33-root-INFO: Loss too large (9205.587->9726.352)! Learning rate decreased to 0.00019.
2024-12-02-07:31:33-root-INFO: grad norm: 3739.935 3146.227 2021.972
2024-12-02-07:31:33-root-INFO: Loss too large (8898.188->8929.959)! Learning rate decreased to 0.00016.
2024-12-02-07:31:34-root-INFO: grad norm: 2555.306 2185.579 1323.947
2024-12-02-07:31:34-root-INFO: grad norm: 1777.307 1517.793 924.728
2024-12-02-07:31:35-root-INFO: grad norm: 1304.020 1121.662 665.087
2024-12-02-07:31:35-root-INFO: grad norm: 985.305 857.653 485.034
2024-12-02-07:31:35-root-INFO: Loss Change: 10843.311 -> 8003.187
2024-12-02-07:31:35-root-INFO: Regularization Change: 0.000 -> 2.121
2024-12-02-07:31:35-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-07:31:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:36-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-07:31:36-root-INFO: grad norm: 885.628 760.644 453.604
2024-12-02-07:31:36-root-INFO: Loss too large (7920.596->7925.767)! Learning rate decreased to 0.00026.
2024-12-02-07:31:36-root-INFO: grad norm: 1227.868 1049.122 637.966
2024-12-02-07:31:36-root-INFO: Loss too large (7883.676->7915.746)! Learning rate decreased to 0.00020.
2024-12-02-07:31:37-root-INFO: grad norm: 1311.337 1119.101 683.533
2024-12-02-07:31:37-root-INFO: grad norm: 1417.017 1217.519 724.973
2024-12-02-07:31:38-root-INFO: grad norm: 1536.135 1311.193 800.302
2024-12-02-07:31:38-root-INFO: grad norm: 1673.767 1438.884 855.049
2024-12-02-07:31:39-root-INFO: grad norm: 1819.451 1552.572 948.642
2024-12-02-07:31:39-root-INFO: grad norm: 1984.426 1705.182 1015.036
2024-12-02-07:31:39-root-INFO: Loss too large (7733.646->7738.005)! Learning rate decreased to 0.00016.
2024-12-02-07:31:40-root-INFO: Loss Change: 7920.596 -> 7609.581
2024-12-02-07:31:40-root-INFO: Regularization Change: 0.000 -> 0.620
2024-12-02-07:31:40-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-07:31:40-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:40-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-07:31:40-root-INFO: grad norm: 1611.432 1413.746 773.327
2024-12-02-07:31:40-root-INFO: Loss too large (7477.277->7844.267)! Learning rate decreased to 0.00027.
2024-12-02-07:31:40-root-INFO: Loss too large (7477.277->7565.464)! Learning rate decreased to 0.00021.
2024-12-02-07:31:41-root-INFO: grad norm: 1576.093 1378.371 764.305
2024-12-02-07:31:41-root-INFO: grad norm: 1626.554 1403.573 821.986
2024-12-02-07:31:42-root-INFO: grad norm: 1690.571 1478.759 819.330
2024-12-02-07:31:42-root-INFO: grad norm: 1758.865 1520.166 884.705
2024-12-02-07:31:42-root-INFO: grad norm: 1838.253 1605.174 895.873
2024-12-02-07:31:43-root-INFO: grad norm: 1918.676 1660.280 961.658
2024-12-02-07:31:43-root-INFO: grad norm: 2010.043 1752.839 983.783
2024-12-02-07:31:44-root-INFO: Loss too large (7324.402->7327.885)! Learning rate decreased to 0.00017.
2024-12-02-07:31:44-root-INFO: Loss Change: 7477.277 -> 7190.001
2024-12-02-07:31:44-root-INFO: Regularization Change: 0.000 -> 0.391
2024-12-02-07:31:44-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-07:31:44-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:44-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-07:31:44-root-INFO: grad norm: 1586.393 1381.489 779.829
2024-12-02-07:31:44-root-INFO: Loss too large (7205.305->7632.444)! Learning rate decreased to 0.00028.
2024-12-02-07:31:45-root-INFO: Loss too large (7205.305->7331.108)! Learning rate decreased to 0.00023.
2024-12-02-07:31:45-root-INFO: grad norm: 1580.617 1389.076 754.200
2024-12-02-07:31:45-root-INFO: grad norm: 1591.776 1388.340 778.629
2024-12-02-07:31:46-root-INFO: grad norm: 1611.426 1417.137 767.084
2024-12-02-07:31:46-root-INFO: grad norm: 1632.472 1426.559 793.658
2024-12-02-07:31:47-root-INFO: grad norm: 1659.260 1458.685 790.811
2024-12-02-07:31:47-root-INFO: grad norm: 1686.118 1474.812 817.264
2024-12-02-07:31:48-root-INFO: grad norm: 1715.966 1508.070 818.696
2024-12-02-07:31:48-root-INFO: Loss Change: 7205.305 -> 7041.486
2024-12-02-07:31:48-root-INFO: Regularization Change: 0.000 -> 0.349
2024-12-02-07:31:48-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-07:31:48-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:48-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-07:31:48-root-INFO: grad norm: 1714.743 1512.824 807.284
2024-12-02-07:31:49-root-INFO: Loss too large (6901.693->7457.507)! Learning rate decreased to 0.00030.
2024-12-02-07:31:49-root-INFO: Loss too large (6901.693->7075.751)! Learning rate decreased to 0.00024.
2024-12-02-07:31:49-root-INFO: grad norm: 1710.229 1502.960 816.083
2024-12-02-07:31:50-root-INFO: grad norm: 1712.761 1505.519 816.678
2024-12-02-07:31:50-root-INFO: grad norm: 1715.096 1508.712 815.685
2024-12-02-07:31:51-root-INFO: grad norm: 1714.982 1507.323 818.009
2024-12-02-07:31:51-root-INFO: grad norm: 1715.775 1510.025 814.684
2024-12-02-07:31:52-root-INFO: grad norm: 1715.970 1507.981 818.868
2024-12-02-07:31:52-root-INFO: grad norm: 1717.075 1511.624 814.456
2024-12-02-07:31:52-root-INFO: Loss Change: 6901.693 -> 6738.570
2024-12-02-07:31:52-root-INFO: Regularization Change: 0.000 -> 0.313
2024-12-02-07:31:52-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-07:31:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:31:52-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-07:31:53-root-INFO: grad norm: 2006.823 1776.250 933.956
2024-12-02-07:31:53-root-INFO: Loss too large (6762.093->7556.993)! Learning rate decreased to 0.00031.
2024-12-02-07:31:53-root-INFO: Loss too large (6762.093->7005.145)! Learning rate decreased to 0.00025.
2024-12-02-07:31:53-root-INFO: grad norm: 1936.423 1719.635 890.276
2024-12-02-07:31:54-root-INFO: grad norm: 1881.534 1667.110 872.304
2024-12-02-07:31:54-root-INFO: grad norm: 1829.232 1624.694 840.511
2024-12-02-07:31:55-root-INFO: grad norm: 1787.116 1583.859 827.753
2024-12-02-07:31:55-root-INFO: grad norm: 1746.546 1551.138 802.742
2024-12-02-07:31:56-root-INFO: grad norm: 1714.723 1519.850 793.935
2024-12-02-07:31:56-root-INFO: grad norm: 1684.960 1496.284 774.740
2024-12-02-07:31:57-root-INFO: Loss Change: 6762.093 -> 6528.308
2024-12-02-07:31:57-root-INFO: Regularization Change: 0.000 -> 0.295
2024-12-02-07:31:57-root-INFO: Undo step: 235
2024-12-02-07:31:57-root-INFO: Undo step: 236
2024-12-02-07:31:57-root-INFO: Undo step: 237
2024-12-02-07:31:57-root-INFO: Undo step: 238
2024-12-02-07:31:57-root-INFO: Undo step: 239
2024-12-02-07:31:57-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-07:31:57-root-INFO: grad norm: 11952.581 9072.692 7781.417
2024-12-02-07:31:57-root-INFO: Loss too large (17900.236->28527.381)! Learning rate decreased to 0.00024.
2024-12-02-07:31:57-root-INFO: Loss too large (17900.236->20741.150)! Learning rate decreased to 0.00019.
2024-12-02-07:31:58-root-INFO: grad norm: 9758.417 7825.583 5829.832
2024-12-02-07:31:58-root-INFO: grad norm: 9084.747 7324.490 5374.428
2024-12-02-07:31:58-root-INFO: Loss too large (12638.186->13396.794)! Learning rate decreased to 0.00016.
2024-12-02-07:31:59-root-INFO: grad norm: 6531.898 5504.088 3517.201
2024-12-02-07:31:59-root-INFO: grad norm: 4709.475 3879.202 2670.382
2024-12-02-07:32:00-root-INFO: grad norm: 3671.255 3117.527 1938.849
2024-12-02-07:32:00-root-INFO: grad norm: 2825.577 2348.002 1571.868
2024-12-02-07:32:01-root-INFO: grad norm: 2220.083 1885.370 1172.241
2024-12-02-07:32:01-root-INFO: Loss Change: 17900.236 -> 8212.450
2024-12-02-07:32:01-root-INFO: Regularization Change: 0.000 -> 4.435
2024-12-02-07:32:01-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-07:32:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:32:01-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-07:32:01-root-INFO: grad norm: 1606.958 1348.414 874.125
2024-12-02-07:32:02-root-INFO: Loss too large (8112.670->8410.812)! Learning rate decreased to 0.00026.
2024-12-02-07:32:02-root-INFO: Loss too large (8112.670->8176.242)! Learning rate decreased to 0.00020.
2024-12-02-07:32:02-root-INFO: grad norm: 1795.890 1525.462 947.728
2024-12-02-07:32:03-root-INFO: grad norm: 2023.228 1709.046 1082.874
2024-12-02-07:32:03-root-INFO: grad norm: 2266.657 1914.838 1212.902
2024-12-02-07:32:04-root-INFO: grad norm: 2539.649 2151.382 1349.581
2024-12-02-07:32:04-root-INFO: Loss too large (7970.694->7992.203)! Learning rate decreased to 0.00016.
2024-12-02-07:32:04-root-INFO: grad norm: 1827.769 1545.576 975.672
2024-12-02-07:32:05-root-INFO: grad norm: 1314.901 1127.515 676.516
2024-12-02-07:32:05-root-INFO: grad norm: 980.698 835.817 513.009
2024-12-02-07:32:06-root-INFO: Loss Change: 8112.670 -> 7510.936
2024-12-02-07:32:06-root-INFO: Regularization Change: 0.000 -> 0.778
2024-12-02-07:32:06-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-07:32:06-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:32:06-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-07:32:06-root-INFO: grad norm: 773.559 661.973 400.231
2024-12-02-07:32:06-root-INFO: grad norm: 970.091 779.203 577.857
2024-12-02-07:32:06-root-INFO: Loss too large (7233.083->7297.237)! Learning rate decreased to 0.00027.
2024-12-02-07:32:07-root-INFO: grad norm: 1306.873 1098.513 707.944
2024-12-02-07:32:07-root-INFO: Loss too large (7223.354->7275.514)! Learning rate decreased to 0.00021.
2024-12-02-07:32:08-root-INFO: grad norm: 1282.775 1072.385 703.919
2024-12-02-07:32:08-root-INFO: grad norm: 1287.695 1111.887 649.512
2024-12-02-07:32:09-root-INFO: grad norm: 1310.930 1112.970 692.701
2024-12-02-07:32:09-root-INFO: grad norm: 1344.068 1168.873 663.517
2024-12-02-07:32:09-root-INFO: grad norm: 1380.321 1177.337 720.530
2024-12-02-07:32:10-root-INFO: Loss Change: 7296.600 -> 7045.367
2024-12-02-07:32:10-root-INFO: Regularization Change: 0.000 -> 0.620
2024-12-02-07:32:10-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-07:32:10-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:32:10-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-07:32:10-root-INFO: grad norm: 1016.539 893.663 484.478
2024-12-02-07:32:10-root-INFO: Loss too large (6937.821->7059.027)! Learning rate decreased to 0.00028.
2024-12-02-07:32:10-root-INFO: Loss too large (6937.821->6957.876)! Learning rate decreased to 0.00023.
2024-12-02-07:32:11-root-INFO: grad norm: 992.583 856.418 501.766
2024-12-02-07:32:11-root-INFO: grad norm: 987.775 874.635 459.035
2024-12-02-07:32:12-root-INFO: grad norm: 986.901 852.094 497.906
2024-12-02-07:32:12-root-INFO: grad norm: 985.899 872.207 459.621
2024-12-02-07:32:13-root-INFO: grad norm: 984.914 851.716 494.606
2024-12-02-07:32:13-root-INFO: grad norm: 986.212 871.907 460.860
2024-12-02-07:32:14-root-INFO: grad norm: 987.269 854.877 493.847
2024-12-02-07:32:14-root-INFO: Loss Change: 6937.821 -> 6732.575
2024-12-02-07:32:14-root-INFO: Regularization Change: 0.000 -> 0.365
2024-12-02-07:32:14-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-07:32:14-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:32:14-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-07:32:14-root-INFO: grad norm: 875.223 770.635 414.894
2024-12-02-07:32:14-root-INFO: Loss too large (6586.689->6679.707)! Learning rate decreased to 0.00030.
2024-12-02-07:32:15-root-INFO: Loss too large (6586.689->6601.224)! Learning rate decreased to 0.00024.
2024-12-02-07:32:15-root-INFO: grad norm: 847.177 740.682 411.216
2024-12-02-07:32:15-root-INFO: grad norm: 828.229 734.494 382.729
2024-12-02-07:32:16-root-INFO: grad norm: 812.031 709.331 395.277
2024-12-02-07:32:16-root-INFO: grad norm: 796.507 707.726 365.441
2024-12-02-07:32:17-root-INFO: grad norm: 780.881 682.546 379.348
2024-12-02-07:32:17-root-INFO: grad norm: 764.393 680.358 348.439
2024-12-02-07:32:18-root-INFO: grad norm: 749.072 655.452 362.618
2024-12-02-07:32:18-root-INFO: Loss Change: 6586.689 -> 6401.279
2024-12-02-07:32:18-root-INFO: Regularization Change: 0.000 -> 0.329
2024-12-02-07:32:18-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-07:32:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-07:32:18-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-07:32:18-root-INFO: grad norm: 360.760 333.123 138.482
2024-12-02-07:32:19-root-INFO: grad norm: 477.700 418.738 229.905
2024-12-02-07:32:19-root-INFO: grad norm: 816.114 721.954 380.559
2024-12-02-07:32:20-root-INFO: Loss too large (6280.241->6369.637)! Learning rate decreased to 0.00031.
2024-12-02-07:32:20-root-INFO: Loss too large (6280.241->6296.593)! Learning rate decreased to 0.00025.
2024-12-02-07:32:20-root-INFO: grad norm: 767.909 676.023 364.250
2024-12-02-07:32:21-root-INFO: grad norm: 733.165 656.510 326.383
2024-12-02-07:32:21-root-INFO: grad norm: 704.938 623.183 329.517
2024-12-02-07:32:22-root-INFO: grad norm: 679.302 609.945 299.030
2024-12-02-07:32:22-root-INFO: grad norm: 658.042 582.932 305.303
2024-12-02-07:32:22-root-INFO: Loss Change: 6305.106 -> 6159.984
2024-12-02-07:32:22-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-07:32:22-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-07:32:22-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-07:32:23-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-07:32:23-root-INFO: grad norm: 509.421 456.117 226.864
2024-12-02-07:32:23-root-INFO: Loss too large (6100.663->6108.996)! Learning rate decreased to 0.00033.
2024-12-02-07:32:23-root-INFO: grad norm: 653.364 579.787 301.217
2024-12-02-07:32:24-root-INFO: Loss too large (6089.122->6092.563)! Learning rate decreased to 0.00026.
2024-12-02-07:32:24-root-INFO: grad norm: 620.830 558.173 271.795
2024-12-02-07:32:24-root-INFO: grad norm: 594.097 529.795 268.827
2024-12-02-07:32:25-root-INFO: grad norm: 569.476 513.831 245.519
2024-12-02-07:32:25-root-INFO: grad norm: 547.537 489.383 245.562
2024-12-02-07:32:26-root-INFO: grad norm: 527.130 476.896 224.578
2024-12-02-07:32:26-root-INFO: grad norm: 508.420 455.588 225.678
2024-12-02-07:32:27-root-INFO: Loss Change: 6100.663 -> 5962.839
2024-12-02-07:32:27-root-INFO: Regularization Change: 0.000 -> 0.296
2024-12-02-07:32:27-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-07:32:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:32:27-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-07:32:27-root-INFO: grad norm: 430.083 362.998 230.660
2024-12-02-07:32:27-root-INFO: grad norm: 489.393 416.931 256.268
2024-12-02-07:32:28-root-INFO: grad norm: 786.542 679.076 396.868
2024-12-02-07:32:28-root-INFO: Loss too large (5897.335->5980.278)! Learning rate decreased to 0.00035.
2024-12-02-07:32:28-root-INFO: Loss too large (5897.335->5908.625)! Learning rate decreased to 0.00028.
2024-12-02-07:32:29-root-INFO: grad norm: 695.891 616.173 323.411
2024-12-02-07:32:29-root-INFO: grad norm: 640.420 572.856 286.311
2024-12-02-07:32:30-root-INFO: grad norm: 602.393 538.791 269.410
2024-12-02-07:32:30-root-INFO: grad norm: 569.157 512.178 248.220
2024-12-02-07:32:31-root-INFO: grad norm: 539.669 484.115 238.486
2024-12-02-07:32:31-root-INFO: Loss Change: 5931.323 -> 5776.273
2024-12-02-07:32:31-root-INFO: Regularization Change: 0.000 -> 0.398
2024-12-02-07:32:31-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-07:32:31-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:32:31-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-07:32:31-root-INFO: grad norm: 423.770 351.333 236.952
2024-12-02-07:32:32-root-INFO: grad norm: 465.737 394.451 247.627
2024-12-02-07:32:32-root-INFO: grad norm: 700.828 604.918 353.884
2024-12-02-07:32:32-root-INFO: Loss too large (5644.936->5707.670)! Learning rate decreased to 0.00036.
2024-12-02-07:32:33-root-INFO: Loss too large (5644.936->5648.872)! Learning rate decreased to 0.00029.
2024-12-02-07:32:33-root-INFO: grad norm: 619.362 554.250 276.433
2024-12-02-07:32:34-root-INFO: grad norm: 588.111 528.094 258.828
2024-12-02-07:32:34-root-INFO: grad norm: 567.472 513.467 241.611
2024-12-02-07:32:35-root-INFO: grad norm: 551.706 497.393 238.704
2024-12-02-07:32:35-root-INFO: grad norm: 536.940 486.340 227.549
2024-12-02-07:32:35-root-INFO: Loss Change: 5682.117 -> 5540.170
2024-12-02-07:32:35-root-INFO: Regularization Change: 0.000 -> 0.388
2024-12-02-07:32:35-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-07:32:35-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:32:36-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-07:32:36-root-INFO: grad norm: 710.389 639.932 308.446
2024-12-02-07:32:36-root-INFO: Loss too large (5510.697->5601.383)! Learning rate decreased to 0.00038.
2024-12-02-07:32:36-root-INFO: Loss too large (5510.697->5529.723)! Learning rate decreased to 0.00030.
2024-12-02-07:32:36-root-INFO: grad norm: 675.692 610.029 290.559
2024-12-02-07:32:37-root-INFO: grad norm: 647.455 583.065 281.485
2024-12-02-07:32:38-root-INFO: grad norm: 620.201 560.887 264.680
2024-12-02-07:32:38-root-INFO: grad norm: 595.114 536.212 258.142
2024-12-02-07:32:38-root-INFO: grad norm: 570.814 517.003 241.944
2024-12-02-07:32:39-root-INFO: grad norm: 548.310 494.661 236.548
2024-12-02-07:32:40-root-INFO: grad norm: 526.520 477.641 221.546
2024-12-02-07:32:40-root-INFO: Loss Change: 5510.697 -> 5383.770
2024-12-02-07:32:40-root-INFO: Regularization Change: 0.000 -> 0.263
2024-12-02-07:32:40-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-07:32:40-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:32:40-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-07:32:40-root-INFO: grad norm: 827.466 741.080 368.105
2024-12-02-07:32:40-root-INFO: Loss too large (5350.780->5493.037)! Learning rate decreased to 0.00040.
2024-12-02-07:32:41-root-INFO: Loss too large (5350.780->5385.841)! Learning rate decreased to 0.00032.
2024-12-02-07:32:41-root-INFO: grad norm: 777.511 703.469 331.142
2024-12-02-07:32:42-root-INFO: grad norm: 736.727 662.668 321.929
2024-12-02-07:32:42-root-INFO: grad norm: 697.971 631.808 296.620
2024-12-02-07:32:43-root-INFO: grad norm: 662.763 597.248 287.313
2024-12-02-07:32:43-root-INFO: grad norm: 629.032 569.804 266.467
2024-12-02-07:32:43-root-INFO: grad norm: 598.404 540.144 257.551
2024-12-02-07:32:44-root-INFO: grad norm: 569.469 516.379 240.099
2024-12-02-07:32:44-root-INFO: Loss Change: 5350.780 -> 5213.873
2024-12-02-07:32:44-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-07:32:44-root-INFO: Undo step: 230
2024-12-02-07:32:44-root-INFO: Undo step: 231
2024-12-02-07:32:44-root-INFO: Undo step: 232
2024-12-02-07:32:44-root-INFO: Undo step: 233
2024-12-02-07:32:44-root-INFO: Undo step: 234
2024-12-02-07:32:44-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-07:32:45-root-INFO: grad norm: 9499.234 7734.234 5515.168
2024-12-02-07:32:45-root-INFO: Loss too large (16972.426->22013.592)! Learning rate decreased to 0.00031.
2024-12-02-07:32:45-root-INFO: grad norm: 10080.329 8469.691 5466.018
2024-12-02-07:32:45-root-INFO: Loss too large (14263.612->18228.566)! Learning rate decreased to 0.00025.
2024-12-02-07:32:46-root-INFO: grad norm: 8256.207 7433.893 3591.962
2024-12-02-07:32:46-root-INFO: grad norm: 6780.299 5897.648 3345.176
2024-12-02-07:32:46-root-INFO: Loss too large (9527.873->9617.399)! Learning rate decreased to 0.00020.
2024-12-02-07:32:47-root-INFO: grad norm: 4376.579 3958.061 1867.671
2024-12-02-07:32:47-root-INFO: grad norm: 2696.931 2359.313 1306.553
2024-12-02-07:32:48-root-INFO: grad norm: 1853.163 1674.675 793.521
2024-12-02-07:32:48-root-INFO: grad norm: 1265.908 1109.835 608.923
2024-12-02-07:32:49-root-INFO: Loss Change: 16972.426 -> 6321.794
2024-12-02-07:32:49-root-INFO: Regularization Change: 0.000 -> 7.056
2024-12-02-07:32:49-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-07:32:49-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-07:32:49-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-07:32:49-root-INFO: grad norm: 1041.574 941.738 444.978
2024-12-02-07:32:49-root-INFO: Loss too large (6278.526->6363.254)! Learning rate decreased to 0.00033.
2024-12-02-07:32:50-root-INFO: grad norm: 1448.848 1276.231 685.854
2024-12-02-07:32:50-root-INFO: Loss too large (6261.295->6356.668)! Learning rate decreased to 0.00026.
2024-12-02-07:32:50-root-INFO: grad norm: 1454.418 1303.231 645.694
2024-12-02-07:32:51-root-INFO: grad norm: 1459.351 1289.647 683.018
2024-12-02-07:32:51-root-INFO: grad norm: 1456.595 1299.923 657.167
2024-12-02-07:32:52-root-INFO: grad norm: 1448.801 1282.627 673.716
2024-12-02-07:32:52-root-INFO: grad norm: 1436.144 1278.398 654.377
2024-12-02-07:32:53-root-INFO: grad norm: 1419.800 1258.258 657.738
2024-12-02-07:32:53-root-INFO: Loss Change: 6278.526 -> 6000.406
2024-12-02-07:32:53-root-INFO: Regularization Change: 0.000 -> 0.757
2024-12-02-07:32:53-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-07:32:53-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:32:53-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-07:32:53-root-INFO: grad norm: 1644.534 1472.354 732.575
2024-12-02-07:32:53-root-INFO: Loss too large (6037.594->6551.067)! Learning rate decreased to 0.00035.
2024-12-02-07:32:54-root-INFO: Loss too large (6037.594->6172.243)! Learning rate decreased to 0.00028.
2024-12-02-07:32:54-root-INFO: grad norm: 1543.031 1368.680 712.502
2024-12-02-07:32:55-root-INFO: grad norm: 1471.131 1312.475 664.555
2024-12-02-07:32:55-root-INFO: grad norm: 1402.218 1245.856 643.474
2024-12-02-07:32:55-root-INFO: grad norm: 1341.920 1195.945 608.658
2024-12-02-07:32:56-root-INFO: grad norm: 1281.730 1139.938 585.981
2024-12-02-07:32:57-root-INFO: grad norm: 1227.020 1092.722 558.154
2024-12-02-07:32:57-root-INFO: grad norm: 1172.883 1043.977 534.573
2024-12-02-07:32:57-root-INFO: Loss Change: 6037.594 -> 5728.903
2024-12-02-07:32:57-root-INFO: Regularization Change: 0.000 -> 0.454
2024-12-02-07:32:57-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-07:32:57-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:32:58-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-07:32:58-root-INFO: grad norm: 1700.218 1508.996 783.372
2024-12-02-07:32:58-root-INFO: Loss too large (5749.447->6357.988)! Learning rate decreased to 0.00036.
2024-12-02-07:32:58-root-INFO: Loss too large (5749.447->5919.374)! Learning rate decreased to 0.00029.
2024-12-02-07:32:59-root-INFO: grad norm: 1585.825 1421.893 702.183
2024-12-02-07:32:59-root-INFO: grad norm: 1504.051 1344.513 674.132
2024-12-02-07:32:59-root-INFO: grad norm: 1425.021 1279.910 626.509
2024-12-02-07:33:00-root-INFO: grad norm: 1360.276 1216.758 608.153
2024-12-02-07:33:00-root-INFO: grad norm: 1296.123 1164.529 569.041
2024-12-02-07:33:01-root-INFO: grad norm: 1242.214 1111.359 554.957
2024-12-02-07:33:01-root-INFO: grad norm: 1189.264 1068.734 521.688
2024-12-02-07:33:02-root-INFO: Loss Change: 5749.447 -> 5485.446
2024-12-02-07:33:02-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-02-07:33:02-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-07:33:02-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:02-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-07:33:02-root-INFO: grad norm: 1324.979 1187.754 587.206
2024-12-02-07:33:02-root-INFO: Loss too large (5450.093->5846.695)! Learning rate decreased to 0.00038.
2024-12-02-07:33:02-root-INFO: Loss too large (5450.093->5565.909)! Learning rate decreased to 0.00030.
2024-12-02-07:33:03-root-INFO: grad norm: 1245.312 1120.802 542.776
2024-12-02-07:33:03-root-INFO: grad norm: 1175.766 1055.013 519.012
2024-12-02-07:33:04-root-INFO: grad norm: 1106.533 996.866 480.285
2024-12-02-07:33:04-root-INFO: grad norm: 1045.690 938.074 462.045
2024-12-02-07:33:05-root-INFO: grad norm: 986.298 889.380 426.365
2024-12-02-07:33:05-root-INFO: grad norm: 934.145 838.052 412.668
2024-12-02-07:33:06-root-INFO: grad norm: 882.671 796.770 379.822
2024-12-02-07:33:06-root-INFO: Loss Change: 5450.093 -> 5261.155
2024-12-02-07:33:06-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-02-07:33:06-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-07:33:06-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:06-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-07:33:06-root-INFO: grad norm: 1159.803 1039.266 514.849
2024-12-02-07:33:07-root-INFO: Loss too large (5248.302->5555.713)! Learning rate decreased to 0.00040.
2024-12-02-07:33:07-root-INFO: Loss too large (5248.302->5334.009)! Learning rate decreased to 0.00032.
2024-12-02-07:33:07-root-INFO: grad norm: 1071.077 967.502 459.506
2024-12-02-07:33:08-root-INFO: grad norm: 995.827 895.043 436.543
2024-12-02-07:33:08-root-INFO: grad norm: 924.030 835.904 393.823
2024-12-02-07:33:08-root-INFO: grad norm: 861.764 774.829 377.196
2024-12-02-07:33:09-root-INFO: grad norm: 802.532 726.975 339.948
2024-12-02-07:33:09-root-INFO: grad norm: 751.170 675.769 328.012
2024-12-02-07:33:10-root-INFO: grad norm: 702.202 637.046 295.398
2024-12-02-07:33:10-root-INFO: Loss Change: 5248.302 -> 5075.228
2024-12-02-07:33:10-root-INFO: Regularization Change: 0.000 -> 0.253
2024-12-02-07:33:10-root-INFO: Undo step: 230
2024-12-02-07:33:10-root-INFO: Undo step: 231
2024-12-02-07:33:10-root-INFO: Undo step: 232
2024-12-02-07:33:10-root-INFO: Undo step: 233
2024-12-02-07:33:10-root-INFO: Undo step: 234
2024-12-02-07:33:10-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-07:33:10-root-INFO: grad norm: 2356.206 1749.533 1578.240
2024-12-02-07:33:11-root-INFO: Loss too large (7019.645->7091.366)! Learning rate decreased to 0.00031.
2024-12-02-07:33:11-root-INFO: grad norm: 2034.938 1501.016 1374.018
2024-12-02-07:33:12-root-INFO: grad norm: 2098.925 1559.035 1405.310
2024-12-02-07:33:12-root-INFO: grad norm: 2285.844 1744.633 1476.934
2024-12-02-07:33:12-root-INFO: Loss too large (6617.311->6660.526)! Learning rate decreased to 0.00025.
2024-12-02-07:33:13-root-INFO: grad norm: 1708.200 1358.052 1036.167
2024-12-02-07:33:13-root-INFO: grad norm: 1434.931 1204.032 780.599
2024-12-02-07:33:14-root-INFO: grad norm: 1317.341 1127.097 681.939
2024-12-02-07:33:14-root-INFO: grad norm: 1280.742 1120.323 620.625
2024-12-02-07:33:14-root-INFO: Loss Change: 7019.645 -> 6119.220
2024-12-02-07:33:14-root-INFO: Regularization Change: 0.000 -> 1.286
2024-12-02-07:33:14-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-07:33:14-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-07:33:15-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-07:33:15-root-INFO: grad norm: 1081.739 943.492 529.132
2024-12-02-07:33:15-root-INFO: Loss too large (6039.380->6207.270)! Learning rate decreased to 0.00033.
2024-12-02-07:33:15-root-INFO: Loss too large (6039.380->6071.564)! Learning rate decreased to 0.00026.
2024-12-02-07:33:16-root-INFO: grad norm: 1054.558 934.781 488.138
2024-12-02-07:33:16-root-INFO: grad norm: 1032.770 904.540 498.419
2024-12-02-07:33:17-root-INFO: grad norm: 1011.375 895.184 470.666
2024-12-02-07:33:17-root-INFO: grad norm: 987.366 865.810 474.622
2024-12-02-07:33:18-root-INFO: grad norm: 961.253 850.441 448.059
2024-12-02-07:33:18-root-INFO: grad norm: 933.724 819.644 447.242
2024-12-02-07:33:18-root-INFO: grad norm: 906.428 801.923 422.530
2024-12-02-07:33:19-root-INFO: Loss Change: 6039.380 -> 5781.979
2024-12-02-07:33:19-root-INFO: Regularization Change: 0.000 -> 0.496
2024-12-02-07:33:19-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-07:33:19-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:19-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-07:33:19-root-INFO: grad norm: 650.835 550.242 347.590
2024-12-02-07:33:19-root-INFO: Loss too large (5701.294->5712.211)! Learning rate decreased to 0.00035.
2024-12-02-07:33:20-root-INFO: grad norm: 780.931 686.229 372.751
2024-12-02-07:33:20-root-INFO: Loss too large (5679.079->5687.597)! Learning rate decreased to 0.00028.
2024-12-02-07:33:20-root-INFO: grad norm: 724.971 635.383 349.100
2024-12-02-07:33:21-root-INFO: grad norm: 682.270 607.950 309.659
2024-12-02-07:33:21-root-INFO: grad norm: 644.375 568.944 302.526
2024-12-02-07:33:22-root-INFO: grad norm: 610.797 545.585 274.608
2024-12-02-07:33:22-root-INFO: grad norm: 579.803 513.909 268.457
2024-12-02-07:33:23-root-INFO: grad norm: 550.709 492.840 245.743
2024-12-02-07:33:23-root-INFO: Loss Change: 5701.294 -> 5525.571
2024-12-02-07:33:23-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-07:33:23-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-07:33:23-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:23-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-07:33:23-root-INFO: grad norm: 450.605 381.774 239.362
2024-12-02-07:33:24-root-INFO: grad norm: 577.028 496.176 294.568
2024-12-02-07:33:24-root-INFO: Loss too large (5384.450->5409.427)! Learning rate decreased to 0.00036.
2024-12-02-07:33:24-root-INFO: grad norm: 714.298 627.273 341.686
2024-12-02-07:33:25-root-INFO: Loss too large (5376.657->5387.394)! Learning rate decreased to 0.00029.
2024-12-02-07:33:25-root-INFO: grad norm: 652.177 579.794 298.618
2024-12-02-07:33:25-root-INFO: grad norm: 611.504 545.227 276.883
2024-12-02-07:33:26-root-INFO: grad norm: 575.623 514.944 257.246
2024-12-02-07:33:26-root-INFO: grad norm: 544.145 486.950 242.844
2024-12-02-07:33:27-root-INFO: grad norm: 514.511 461.622 227.216
2024-12-02-07:33:27-root-INFO: Loss Change: 5408.947 -> 5271.302
2024-12-02-07:33:27-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-02-07:33:27-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-07:33:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:27-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-07:33:28-root-INFO: grad norm: 699.719 625.522 313.575
2024-12-02-07:33:28-root-INFO: Loss too large (5233.723->5316.177)! Learning rate decreased to 0.00038.
2024-12-02-07:33:28-root-INFO: Loss too large (5233.723->5248.757)! Learning rate decreased to 0.00030.
2024-12-02-07:33:28-root-INFO: grad norm: 643.406 575.182 288.335
2024-12-02-07:33:29-root-INFO: grad norm: 595.426 532.631 266.150
2024-12-02-07:33:29-root-INFO: grad norm: 550.265 493.913 242.573
2024-12-02-07:33:30-root-INFO: grad norm: 509.330 456.605 225.676
2024-12-02-07:33:30-root-INFO: grad norm: 471.804 425.390 204.064
2024-12-02-07:33:31-root-INFO: grad norm: 438.545 394.551 191.446
2024-12-02-07:33:31-root-INFO: grad norm: 408.417 370.169 172.565
2024-12-02-07:33:32-root-INFO: Loss Change: 5233.723 -> 5105.532
2024-12-02-07:33:32-root-INFO: Regularization Change: 0.000 -> 0.250
2024-12-02-07:33:32-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-07:33:32-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:32-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-07:33:32-root-INFO: grad norm: 729.717 647.659 336.191
2024-12-02-07:33:32-root-INFO: Loss too large (5066.389->5163.098)! Learning rate decreased to 0.00040.
2024-12-02-07:33:32-root-INFO: Loss too large (5066.389->5084.946)! Learning rate decreased to 0.00032.
2024-12-02-07:33:33-root-INFO: grad norm: 650.477 583.171 288.153
2024-12-02-07:33:33-root-INFO: grad norm: 584.869 521.992 263.811
2024-12-02-07:33:34-root-INFO: grad norm: 526.497 474.140 228.891
2024-12-02-07:33:34-root-INFO: grad norm: 477.208 427.622 211.820
2024-12-02-07:33:35-root-INFO: grad norm: 433.556 392.446 184.274
2024-12-02-07:33:35-root-INFO: grad norm: 396.322 357.065 171.976
2024-12-02-07:33:36-root-INFO: grad norm: 363.320 330.929 149.957
2024-12-02-07:33:36-root-INFO: Loss Change: 5066.389 -> 4937.817
2024-12-02-07:33:36-root-INFO: Regularization Change: 0.000 -> 0.244
2024-12-02-07:33:36-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-07:33:36-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:36-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-07:33:36-root-INFO: grad norm: 799.512 713.123 361.490
2024-12-02-07:33:36-root-INFO: Loss too large (4957.835->5075.920)! Learning rate decreased to 0.00042.
2024-12-02-07:33:36-root-INFO: Loss too large (4957.835->4978.184)! Learning rate decreased to 0.00034.
2024-12-02-07:33:37-root-INFO: grad norm: 706.102 633.833 311.184
2024-12-02-07:33:38-root-INFO: grad norm: 634.746 567.951 283.432
2024-12-02-07:33:38-root-INFO: grad norm: 570.586 513.530 248.707
2024-12-02-07:33:39-root-INFO: grad norm: 517.779 464.666 228.430
2024-12-02-07:33:39-root-INFO: grad norm: 470.413 424.878 201.910
2024-12-02-07:33:39-root-INFO: grad norm: 429.969 387.457 186.414
2024-12-02-07:33:40-root-INFO: grad norm: 393.931 357.375 165.725
2024-12-02-07:33:40-root-INFO: Loss Change: 4957.835 -> 4811.771
2024-12-02-07:33:40-root-INFO: Regularization Change: 0.000 -> 0.276
2024-12-02-07:33:40-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-07:33:40-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:40-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-07:33:41-root-INFO: grad norm: 606.095 534.974 284.875
2024-12-02-07:33:41-root-INFO: Loss too large (4802.220->4859.708)! Learning rate decreased to 0.00044.
2024-12-02-07:33:41-root-INFO: Loss too large (4802.220->4806.192)! Learning rate decreased to 0.00035.
2024-12-02-07:33:41-root-INFO: grad norm: 511.713 458.957 226.293
2024-12-02-07:33:42-root-INFO: grad norm: 453.824 409.238 196.164
2024-12-02-07:33:42-root-INFO: grad norm: 404.907 366.310 172.529
2024-12-02-07:33:43-root-INFO: grad norm: 364.348 330.031 154.366
2024-12-02-07:33:43-root-INFO: grad norm: 329.521 300.376 135.492
2024-12-02-07:33:44-root-INFO: grad norm: 300.868 274.314 123.584
2024-12-02-07:33:44-root-INFO: grad norm: 276.859 254.481 109.044
2024-12-02-07:33:44-root-INFO: Loss Change: 4802.220 -> 4688.787
2024-12-02-07:33:44-root-INFO: Regularization Change: 0.000 -> 0.243
2024-12-02-07:33:44-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-07:33:44-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:33:44-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-07:33:45-root-INFO: grad norm: 756.177 671.399 347.890
2024-12-02-07:33:45-root-INFO: Loss too large (4729.610->4832.127)! Learning rate decreased to 0.00046.
2024-12-02-07:33:45-root-INFO: Loss too large (4729.610->4740.908)! Learning rate decreased to 0.00037.
2024-12-02-07:33:45-root-INFO: grad norm: 638.671 573.740 280.577
2024-12-02-07:33:46-root-INFO: grad norm: 554.969 498.132 244.653
2024-12-02-07:33:46-root-INFO: grad norm: 483.893 436.126 209.633
2024-12-02-07:33:47-root-INFO: grad norm: 427.490 385.893 183.940
2024-12-02-07:33:47-root-INFO: grad norm: 379.140 343.272 160.971
2024-12-02-07:33:48-root-INFO: grad norm: 339.041 308.255 141.166
2024-12-02-07:33:48-root-INFO: grad norm: 305.115 278.137 125.440
2024-12-02-07:33:49-root-INFO: Loss Change: 4729.610 -> 4581.877
2024-12-02-07:33:49-root-INFO: Regularization Change: 0.000 -> 0.297
2024-12-02-07:33:49-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-07:33:49-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-07:33:49-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-07:33:49-root-INFO: grad norm: 355.958 306.037 181.789
2024-12-02-07:33:49-root-INFO: grad norm: 474.821 418.873 223.609
2024-12-02-07:33:49-root-INFO: Loss too large (4518.739->4545.322)! Learning rate decreased to 0.00049.
2024-12-02-07:33:50-root-INFO: grad norm: 557.886 498.283 250.900
2024-12-02-07:33:50-root-INFO: Loss too large (4513.946->4519.631)! Learning rate decreased to 0.00039.
2024-12-02-07:33:51-root-INFO: grad norm: 465.172 418.653 202.768
2024-12-02-07:33:51-root-INFO: grad norm: 396.825 358.349 170.458
2024-12-02-07:33:51-root-INFO: grad norm: 341.030 309.487 143.245
2024-12-02-07:33:52-root-INFO: grad norm: 296.619 270.214 122.341
2024-12-02-07:33:52-root-INFO: grad norm: 260.724 239.085 103.997
2024-12-02-07:33:53-root-INFO: Loss Change: 4528.522 -> 4429.404
2024-12-02-07:33:53-root-INFO: Regularization Change: 0.000 -> 0.309
2024-12-02-07:33:53-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-07:33:53-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:33:53-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-07:33:53-root-INFO: grad norm: 727.364 643.050 339.920
2024-12-02-07:33:53-root-INFO: Loss too large (4442.616->4543.680)! Learning rate decreased to 0.00051.
2024-12-02-07:33:53-root-INFO: Loss too large (4442.616->4451.508)! Learning rate decreased to 0.00041.
2024-12-02-07:33:54-root-INFO: grad norm: 589.147 531.174 254.851
2024-12-02-07:33:54-root-INFO: grad norm: 496.210 444.034 221.492
2024-12-02-07:33:55-root-INFO: grad norm: 419.411 379.375 178.832
2024-12-02-07:33:55-root-INFO: grad norm: 359.634 324.451 155.138
2024-12-02-07:33:56-root-INFO: grad norm: 310.598 282.679 128.701
2024-12-02-07:33:56-root-INFO: grad norm: 272.314 248.318 111.773
2024-12-02-07:33:57-root-INFO: grad norm: 241.251 221.615 95.336
2024-12-02-07:33:57-root-INFO: Loss Change: 4442.616 -> 4300.423
2024-12-02-07:33:57-root-INFO: Regularization Change: 0.000 -> 0.296
2024-12-02-07:33:57-root-INFO: Undo step: 225
2024-12-02-07:33:57-root-INFO: Undo step: 226
2024-12-02-07:33:57-root-INFO: Undo step: 227
2024-12-02-07:33:57-root-INFO: Undo step: 228
2024-12-02-07:33:57-root-INFO: Undo step: 229
2024-12-02-07:33:57-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-07:33:57-root-INFO: grad norm: 6651.730 5434.562 3835.498
2024-12-02-07:33:57-root-INFO: Loss too large (11182.234->11467.093)! Learning rate decreased to 0.00040.
2024-12-02-07:33:58-root-INFO: grad norm: 4272.684 3699.336 2137.930
2024-12-02-07:33:58-root-INFO: grad norm: 3307.430 2903.462 1583.983
2024-12-02-07:33:59-root-INFO: Loss too large (7043.730->7110.735)! Learning rate decreased to 0.00032.
2024-12-02-07:33:59-root-INFO: grad norm: 2409.013 2103.207 1174.676
2024-12-02-07:34:00-root-INFO: grad norm: 1833.679 1632.804 834.464
2024-12-02-07:34:00-root-INFO: grad norm: 1503.826 1318.138 723.882
2024-12-02-07:34:00-root-INFO: grad norm: 1243.947 1113.816 553.911
2024-12-02-07:34:01-root-INFO: grad norm: 1056.652 929.977 501.653
2024-12-02-07:34:01-root-INFO: Loss Change: 11182.234 -> 5516.543
2024-12-02-07:34:01-root-INFO: Regularization Change: 0.000 -> 4.319
2024-12-02-07:34:01-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-07:34:01-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:34:01-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-07:34:02-root-INFO: grad norm: 697.140 633.020 292.045
2024-12-02-07:34:02-root-INFO: grad norm: 957.666 833.217 472.095
2024-12-02-07:34:02-root-INFO: Loss too large (5389.514->5496.848)! Learning rate decreased to 0.00042.
2024-12-02-07:34:03-root-INFO: grad norm: 1191.340 1072.306 519.088
2024-12-02-07:34:03-root-INFO: Loss too large (5379.579->5426.930)! Learning rate decreased to 0.00034.
2024-12-02-07:34:03-root-INFO: grad norm: 1031.379 915.202 475.551
2024-12-02-07:34:04-root-INFO: grad norm: 896.983 810.838 383.562
2024-12-02-07:34:04-root-INFO: grad norm: 795.364 705.913 366.456
2024-12-02-07:34:05-root-INFO: grad norm: 707.500 641.377 298.650
2024-12-02-07:34:05-root-INFO: grad norm: 637.911 566.478 293.314
2024-12-02-07:34:05-root-INFO: Loss Change: 5430.833 -> 5108.805
2024-12-02-07:34:05-root-INFO: Regularization Change: 0.000 -> 0.914
2024-12-02-07:34:06-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-07:34:06-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:34:06-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-07:34:06-root-INFO: grad norm: 444.533 396.214 201.554
2024-12-02-07:34:06-root-INFO: grad norm: 655.950 571.211 322.472
2024-12-02-07:34:06-root-INFO: Loss too large (5040.646->5090.317)! Learning rate decreased to 0.00044.
2024-12-02-07:34:07-root-INFO: grad norm: 815.376 732.444 358.278
2024-12-02-07:34:07-root-INFO: Loss too large (5034.001->5052.020)! Learning rate decreased to 0.00035.
2024-12-02-07:34:08-root-INFO: grad norm: 711.111 631.246 327.424
2024-12-02-07:34:08-root-INFO: grad norm: 626.905 568.864 263.447
2024-12-02-07:34:09-root-INFO: grad norm: 559.029 497.157 255.634
2024-12-02-07:34:09-root-INFO: grad norm: 500.437 455.898 206.383
2024-12-02-07:34:10-root-INFO: grad norm: 451.803 402.592 205.051
2024-12-02-07:34:10-root-INFO: Loss Change: 5055.180 -> 4875.589
2024-12-02-07:34:10-root-INFO: Regularization Change: 0.000 -> 0.555
2024-12-02-07:34:10-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-07:34:10-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:34:10-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-07:34:10-root-INFO: grad norm: 410.197 355.139 205.275
2024-12-02-07:34:11-root-INFO: grad norm: 417.941 369.463 195.377
2024-12-02-07:34:11-root-INFO: grad norm: 599.355 531.123 277.732
2024-12-02-07:34:12-root-INFO: Loss too large (4806.195->4851.515)! Learning rate decreased to 0.00046.
2024-12-02-07:34:12-root-INFO: grad norm: 745.972 674.011 319.661
2024-12-02-07:34:12-root-INFO: Loss too large (4801.145->4820.553)! Learning rate decreased to 0.00037.
2024-12-02-07:34:13-root-INFO: grad norm: 654.191 585.107 292.601
2024-12-02-07:34:13-root-INFO: grad norm: 577.780 524.208 242.973
2024-12-02-07:34:14-root-INFO: grad norm: 516.405 462.629 229.453
2024-12-02-07:34:14-root-INFO: grad norm: 462.937 420.872 192.814
2024-12-02-07:34:15-root-INFO: Loss Change: 4863.052 -> 4687.636
2024-12-02-07:34:15-root-INFO: Regularization Change: 0.000 -> 0.604
2024-12-02-07:34:15-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-07:34:15-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-07:34:15-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-07:34:15-root-INFO: grad norm: 456.964 395.585 228.753
2024-12-02-07:34:15-root-INFO: Loss too large (4645.431->4652.762)! Learning rate decreased to 0.00049.
2024-12-02-07:34:16-root-INFO: grad norm: 518.270 463.422 232.043
2024-12-02-07:34:16-root-INFO: grad norm: 639.747 571.030 288.445
2024-12-02-07:34:17-root-INFO: Loss too large (4626.795->4635.924)! Learning rate decreased to 0.00039.
2024-12-02-07:34:17-root-INFO: grad norm: 544.553 492.516 232.307
2024-12-02-07:34:18-root-INFO: grad norm: 470.025 421.154 208.694
2024-12-02-07:34:18-root-INFO: grad norm: 408.315 371.257 169.967
2024-12-02-07:34:19-root-INFO: grad norm: 358.851 322.473 157.432
2024-12-02-07:34:19-root-INFO: grad norm: 317.871 290.538 128.955
2024-12-02-07:34:19-root-INFO: Loss Change: 4645.431 -> 4514.898
2024-12-02-07:34:19-root-INFO: Regularization Change: 0.000 -> 0.387
2024-12-02-07:34:19-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-07:34:19-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:34:20-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-07:34:20-root-INFO: grad norm: 783.578 694.054 363.708
2024-12-02-07:34:20-root-INFO: Loss too large (4535.010->4640.834)! Learning rate decreased to 0.00051.
2024-12-02-07:34:20-root-INFO: Loss too large (4535.010->4537.348)! Learning rate decreased to 0.00041.
2024-12-02-07:34:21-root-INFO: grad norm: 630.951 575.442 258.779
2024-12-02-07:34:21-root-INFO: grad norm: 541.613 482.887 245.286
2024-12-02-07:34:22-root-INFO: grad norm: 466.845 425.738 191.551
2024-12-02-07:34:22-root-INFO: grad norm: 407.672 365.275 181.027
2024-12-02-07:34:23-root-INFO: grad norm: 357.717 326.893 145.267
2024-12-02-07:34:23-root-INFO: grad norm: 317.079 285.607 137.723
2024-12-02-07:34:24-root-INFO: grad norm: 283.387 259.909 112.940
2024-12-02-07:34:24-root-INFO: Loss Change: 4535.010 -> 4360.118
2024-12-02-07:34:24-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-07:34:24-root-INFO: Undo step: 225
2024-12-02-07:34:24-root-INFO: Undo step: 226
2024-12-02-07:34:24-root-INFO: Undo step: 227
2024-12-02-07:34:24-root-INFO: Undo step: 228
2024-12-02-07:34:24-root-INFO: Undo step: 229
2024-12-02-07:34:24-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-07:34:24-root-INFO: grad norm: 5256.306 4525.133 2674.307
2024-12-02-07:34:24-root-INFO: Loss too large (9284.143->10456.748)! Learning rate decreased to 0.00040.
2024-12-02-07:34:25-root-INFO: grad norm: 3984.236 3556.909 1795.142
2024-12-02-07:34:25-root-INFO: grad norm: 3590.916 3115.020 1786.429
2024-12-02-07:34:25-root-INFO: Loss too large (7149.083->7479.938)! Learning rate decreased to 0.00032.
2024-12-02-07:34:26-root-INFO: grad norm: 2908.041 2588.796 1324.704
2024-12-02-07:34:26-root-INFO: grad norm: 2440.431 2140.939 1171.359
2024-12-02-07:34:27-root-INFO: grad norm: 2133.845 1905.182 961.030
2024-12-02-07:34:27-root-INFO: grad norm: 1867.404 1643.437 886.741
2024-12-02-07:34:28-root-INFO: grad norm: 1664.221 1488.047 745.216
2024-12-02-07:34:28-root-INFO: Loss Change: 9284.143 -> 5447.079
2024-12-02-07:34:28-root-INFO: Regularization Change: 0.000 -> 4.558
2024-12-02-07:34:28-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-07:34:28-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:34:28-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-07:34:29-root-INFO: grad norm: 1046.657 920.672 497.849
2024-12-02-07:34:29-root-INFO: Loss too large (5224.927->5347.466)! Learning rate decreased to 0.00042.
2024-12-02-07:34:29-root-INFO: grad norm: 1289.723 1157.555 568.728
2024-12-02-07:34:29-root-INFO: Loss too large (5212.326->5274.964)! Learning rate decreased to 0.00034.
2024-12-02-07:34:30-root-INFO: grad norm: 1159.361 1031.581 529.112
2024-12-02-07:34:30-root-INFO: grad norm: 1055.673 950.713 458.903
2024-12-02-07:34:31-root-INFO: grad norm: 960.417 855.013 437.439
2024-12-02-07:34:31-root-INFO: grad norm: 881.067 793.822 382.264
2024-12-02-07:34:32-root-INFO: grad norm: 807.477 719.387 366.745
2024-12-02-07:34:32-root-INFO: grad norm: 745.334 671.899 322.606
2024-12-02-07:34:33-root-INFO: Loss Change: 5224.927 -> 4896.918
2024-12-02-07:34:33-root-INFO: Regularization Change: 0.000 -> 0.763
2024-12-02-07:34:33-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-07:34:33-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:34:33-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-07:34:33-root-INFO: grad norm: 493.473 425.732 249.536
2024-12-02-07:34:33-root-INFO: Loss too large (4834.188->4839.130)! Learning rate decreased to 0.00044.
2024-12-02-07:34:33-root-INFO: grad norm: 576.139 516.192 255.894
2024-12-02-07:34:34-root-INFO: grad norm: 739.428 658.794 335.773
2024-12-02-07:34:34-root-INFO: Loss too large (4811.308->4829.401)! Learning rate decreased to 0.00035.
2024-12-02-07:34:35-root-INFO: grad norm: 663.250 597.846 287.194
2024-12-02-07:34:35-root-INFO: grad norm: 597.435 535.620 264.650
2024-12-02-07:34:36-root-INFO: grad norm: 542.582 489.877 233.272
2024-12-02-07:34:36-root-INFO: grad norm: 493.179 443.451 215.818
2024-12-02-07:34:37-root-INFO: grad norm: 451.517 408.465 192.416
2024-12-02-07:34:37-root-INFO: Loss Change: 4834.188 -> 4682.613
2024-12-02-07:34:37-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-02-07:34:37-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-07:34:37-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-07:34:37-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-07:34:37-root-INFO: grad norm: 372.681 320.273 190.568
2024-12-02-07:34:38-root-INFO: grad norm: 443.817 393.968 204.358
2024-12-02-07:34:38-root-INFO: Loss too large (4616.167->4624.633)! Learning rate decreased to 0.00046.
2024-12-02-07:34:38-root-INFO: grad norm: 523.322 470.490 229.139
2024-12-02-07:34:39-root-INFO: grad norm: 681.474 614.406 294.808
2024-12-02-07:34:39-root-INFO: Loss too large (4603.377->4623.988)! Learning rate decreased to 0.00037.
2024-12-02-07:34:39-root-INFO: grad norm: 615.145 556.064 263.052
2024-12-02-07:34:40-root-INFO: grad norm: 556.966 503.057 239.050
2024-12-02-07:34:40-root-INFO: grad norm: 508.443 460.797 214.896
2024-12-02-07:34:41-root-INFO: grad norm: 464.162 419.933 197.744
2024-12-02-07:34:41-root-INFO: Loss Change: 4644.435 -> 4513.986
2024-12-02-07:34:41-root-INFO: Regularization Change: 0.000 -> 0.446
2024-12-02-07:34:41-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-07:34:41-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-07:34:41-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-07:34:41-root-INFO: grad norm: 470.427 412.954 225.324
2024-12-02-07:34:42-root-INFO: Loss too large (4482.354->4497.000)! Learning rate decreased to 0.00049.
2024-12-02-07:34:42-root-INFO: grad norm: 545.359 486.311 246.815
2024-12-02-07:34:42-root-INFO: Loss too large (4470.109->4472.835)! Learning rate decreased to 0.00039.
2024-12-02-07:34:43-root-INFO: grad norm: 468.017 424.295 197.520
2024-12-02-07:34:43-root-INFO: grad norm: 409.039 370.309 173.736
2024-12-02-07:34:44-root-INFO: grad norm: 361.797 329.431 149.573
2024-12-02-07:34:44-root-INFO: grad norm: 321.546 293.164 132.085
2024-12-02-07:34:44-root-INFO: grad norm: 289.241 264.833 116.292
2024-12-02-07:34:45-root-INFO: grad norm: 262.464 241.105 103.710
2024-12-02-07:34:45-root-INFO: Loss Change: 4482.354 -> 4361.895
2024-12-02-07:34:45-root-INFO: Regularization Change: 0.000 -> 0.331
2024-12-02-07:34:45-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-07:34:45-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:34:45-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-07:34:46-root-INFO: grad norm: 743.565 662.340 337.926
2024-12-02-07:34:46-root-INFO: Loss too large (4370.648->4472.518)! Learning rate decreased to 0.00051.
2024-12-02-07:34:46-root-INFO: Loss too large (4370.648->4376.720)! Learning rate decreased to 0.00041.
2024-12-02-07:34:46-root-INFO: grad norm: 610.940 556.535 252.023
2024-12-02-07:34:47-root-INFO: grad norm: 531.997 479.400 230.643
2024-12-02-07:34:47-root-INFO: grad norm: 464.261 423.257 190.765
2024-12-02-07:34:48-root-INFO: grad norm: 409.779 371.233 173.507
2024-12-02-07:34:48-root-INFO: grad norm: 362.367 331.282 146.840
2024-12-02-07:34:49-root-INFO: grad norm: 323.225 294.577 133.037
2024-12-02-07:34:49-root-INFO: grad norm: 289.727 266.149 114.484
2024-12-02-07:34:49-root-INFO: Loss Change: 4370.648 -> 4213.550
2024-12-02-07:34:50-root-INFO: Regularization Change: 0.000 -> 0.339
2024-12-02-07:34:50-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-07:34:50-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:34:50-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-07:34:50-root-INFO: grad norm: 568.755 507.323 257.111
2024-12-02-07:34:50-root-INFO: Loss too large (4223.043->4276.731)! Learning rate decreased to 0.00054.
2024-12-02-07:34:50-root-INFO: grad norm: 721.440 654.526 303.432
2024-12-02-07:34:51-root-INFO: Loss too large (4221.708->4251.234)! Learning rate decreased to 0.00043.
2024-12-02-07:34:51-root-INFO: grad norm: 637.396 576.635 271.597
2024-12-02-07:34:52-root-INFO: grad norm: 561.386 510.673 233.169
2024-12-02-07:34:52-root-INFO: grad norm: 498.909 452.749 209.592
2024-12-02-07:34:52-root-INFO: grad norm: 442.603 403.336 182.258
2024-12-02-07:34:53-root-INFO: grad norm: 396.253 360.906 163.594
2024-12-02-07:34:53-root-INFO: grad norm: 354.767 324.180 144.107
2024-12-02-07:34:54-root-INFO: Loss Change: 4223.043 -> 4096.718
2024-12-02-07:34:54-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-02-07:34:54-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-07:34:54-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:34:54-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-07:34:54-root-INFO: grad norm: 652.592 584.955 289.315
2024-12-02-07:34:54-root-INFO: Loss too large (4118.738->4213.025)! Learning rate decreased to 0.00056.
2024-12-02-07:34:54-root-INFO: Loss too large (4118.738->4129.003)! Learning rate decreased to 0.00045.
2024-12-02-07:34:55-root-INFO: grad norm: 549.786 500.776 226.911
2024-12-02-07:34:55-root-INFO: grad norm: 481.023 436.181 202.802
2024-12-02-07:34:56-root-INFO: grad norm: 420.006 383.389 171.517
2024-12-02-07:34:56-root-INFO: grad norm: 370.548 337.709 152.508
2024-12-02-07:34:57-root-INFO: grad norm: 327.254 299.722 131.386
2024-12-02-07:34:57-root-INFO: grad norm: 292.109 267.847 116.556
2024-12-02-07:34:58-root-INFO: grad norm: 262.187 241.300 102.549
2024-12-02-07:34:58-root-INFO: Loss Change: 4118.738 -> 3983.092
2024-12-02-07:34:58-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-02-07:34:58-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-07:34:58-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:34:58-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-07:34:58-root-INFO: grad norm: 322.275 293.643 132.795
2024-12-02-07:34:58-root-INFO: Loss too large (3918.422->3928.583)! Learning rate decreased to 0.00059.
2024-12-02-07:34:59-root-INFO: grad norm: 397.263 362.091 163.425
2024-12-02-07:34:59-root-INFO: Loss too large (3912.512->3913.421)! Learning rate decreased to 0.00047.
2024-12-02-07:34:59-root-INFO: grad norm: 343.509 314.387 138.415
2024-12-02-07:35:00-root-INFO: grad norm: 298.072 273.089 119.454
2024-12-02-07:35:00-root-INFO: grad norm: 262.254 241.572 102.079
2024-12-02-07:35:01-root-INFO: grad norm: 232.624 214.509 90.001
2024-12-02-07:35:01-root-INFO: grad norm: 209.104 194.147 77.660
2024-12-02-07:35:02-root-INFO: grad norm: 190.232 176.818 70.169
2024-12-02-07:35:02-root-INFO: Loss Change: 3918.422 -> 3834.600
2024-12-02-07:35:02-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-02-07:35:02-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-07:35:02-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:02-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-07:35:02-root-INFO: grad norm: 332.738 299.957 144.015
2024-12-02-07:35:03-root-INFO: Loss too large (3811.238->3821.557)! Learning rate decreased to 0.00062.
2024-12-02-07:35:03-root-INFO: grad norm: 384.970 350.593 159.017
2024-12-02-07:35:03-root-INFO: grad norm: 463.469 421.556 192.597
2024-12-02-07:35:04-root-INFO: Loss too large (3802.844->3805.888)! Learning rate decreased to 0.00050.
2024-12-02-07:35:04-root-INFO: grad norm: 373.521 341.171 152.055
2024-12-02-07:35:05-root-INFO: grad norm: 307.808 281.704 124.051
2024-12-02-07:35:05-root-INFO: grad norm: 256.035 235.394 100.715
2024-12-02-07:35:06-root-INFO: grad norm: 218.173 201.579 83.459
2024-12-02-07:35:06-root-INFO: grad norm: 189.347 175.834 70.246
2024-12-02-07:35:06-root-INFO: Loss Change: 3811.238 -> 3727.474
2024-12-02-07:35:06-root-INFO: Regularization Change: 0.000 -> 0.307
2024-12-02-07:35:06-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-07:35:06-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:07-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-07:35:07-root-INFO: grad norm: 517.266 457.279 241.785
2024-12-02-07:35:07-root-INFO: Loss too large (3745.954->3788.389)! Learning rate decreased to 0.00065.
2024-12-02-07:35:07-root-INFO: grad norm: 595.797 543.518 244.055
2024-12-02-07:35:07-root-INFO: Loss too large (3737.085->3749.271)! Learning rate decreased to 0.00052.
2024-12-02-07:35:08-root-INFO: grad norm: 474.219 429.961 200.042
2024-12-02-07:35:08-root-INFO: grad norm: 375.152 343.966 149.755
2024-12-02-07:35:09-root-INFO: grad norm: 303.264 277.111 123.202
2024-12-02-07:35:09-root-INFO: grad norm: 247.132 228.131 95.027
2024-12-02-07:35:10-root-INFO: grad norm: 207.332 191.680 79.028
2024-12-02-07:35:10-root-INFO: grad norm: 178.082 166.181 64.009
2024-12-02-07:35:11-root-INFO: Loss Change: 3745.954 -> 3628.805
2024-12-02-07:35:11-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-02-07:35:11-root-INFO: Undo step: 220
2024-12-02-07:35:11-root-INFO: Undo step: 221
2024-12-02-07:35:11-root-INFO: Undo step: 222
2024-12-02-07:35:11-root-INFO: Undo step: 223
2024-12-02-07:35:11-root-INFO: Undo step: 224
2024-12-02-07:35:11-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-07:35:11-root-INFO: grad norm: 3798.731 2879.559 2477.599
2024-12-02-07:35:12-root-INFO: grad norm: 2539.131 2024.416 1532.621
2024-12-02-07:35:12-root-INFO: grad norm: 2048.231 1626.002 1245.539
2024-12-02-07:35:13-root-INFO: grad norm: 1748.025 1452.218 972.961
2024-12-02-07:35:13-root-INFO: grad norm: 1452.700 1170.701 860.115
2024-12-02-07:35:14-root-INFO: grad norm: 1248.355 1065.997 649.646
2024-12-02-07:35:14-root-INFO: grad norm: 1126.234 936.141 626.134
2024-12-02-07:35:14-root-INFO: Loss too large (4512.843->4517.896)! Learning rate decreased to 0.00051.
2024-12-02-07:35:15-root-INFO: grad norm: 896.785 812.327 379.932
2024-12-02-07:35:15-root-INFO: Loss Change: 8462.795 -> 4387.283
2024-12-02-07:35:15-root-INFO: Regularization Change: 0.000 -> 7.592
2024-12-02-07:35:15-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-07:35:15-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:15-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-07:35:15-root-INFO: grad norm: 1284.286 1149.576 572.597
2024-12-02-07:35:15-root-INFO: Loss too large (4409.528->4893.573)! Learning rate decreased to 0.00054.
2024-12-02-07:35:16-root-INFO: Loss too large (4409.528->4533.852)! Learning rate decreased to 0.00043.
2024-12-02-07:35:16-root-INFO: grad norm: 1160.969 1056.995 480.218
2024-12-02-07:35:17-root-INFO: grad norm: 1069.125 960.768 468.991
2024-12-02-07:35:17-root-INFO: grad norm: 981.076 892.866 406.571
2024-12-02-07:35:17-root-INFO: grad norm: 910.657 819.341 397.463
2024-12-02-07:35:18-root-INFO: grad norm: 843.090 767.318 349.320
2024-12-02-07:35:18-root-INFO: grad norm: 787.169 708.948 342.093
2024-12-02-07:35:19-root-INFO: grad norm: 733.687 667.999 303.437
2024-12-02-07:35:19-root-INFO: Loss Change: 4409.528 -> 4150.847
2024-12-02-07:35:19-root-INFO: Regularization Change: 0.000 -> 0.433
2024-12-02-07:35:19-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-07:35:19-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:19-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-07:35:20-root-INFO: grad norm: 1018.693 912.851 452.149
2024-12-02-07:35:20-root-INFO: Loss too large (4192.868->4515.501)! Learning rate decreased to 0.00056.
2024-12-02-07:35:20-root-INFO: Loss too large (4192.868->4275.877)! Learning rate decreased to 0.00045.
2024-12-02-07:35:20-root-INFO: grad norm: 929.325 846.689 383.095
2024-12-02-07:35:21-root-INFO: grad norm: 866.770 780.413 377.155
2024-12-02-07:35:21-root-INFO: grad norm: 804.984 733.633 331.331
2024-12-02-07:35:22-root-INFO: grad norm: 752.704 678.936 324.975
2024-12-02-07:35:22-root-INFO: grad norm: 701.696 639.716 288.342
2024-12-02-07:35:23-root-INFO: grad norm: 657.614 594.060 282.042
2024-12-02-07:35:23-root-INFO: grad norm: 615.082 561.076 252.031
2024-12-02-07:35:23-root-INFO: Loss Change: 4192.868 -> 4015.820
2024-12-02-07:35:23-root-INFO: Regularization Change: 0.000 -> 0.336
2024-12-02-07:35:23-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-07:35:23-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:24-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-07:35:24-root-INFO: grad norm: 665.635 602.307 283.365
2024-12-02-07:35:24-root-INFO: Loss too large (3960.963->4099.731)! Learning rate decreased to 0.00059.
2024-12-02-07:35:24-root-INFO: Loss too large (3960.963->3995.948)! Learning rate decreased to 0.00047.
2024-12-02-07:35:24-root-INFO: grad norm: 610.373 557.000 249.613
2024-12-02-07:35:25-root-INFO: grad norm: 564.682 512.101 237.946
2024-12-02-07:35:25-root-INFO: grad norm: 521.691 476.766 211.792
2024-12-02-07:35:26-root-INFO: grad norm: 484.536 440.020 202.875
2024-12-02-07:35:26-root-INFO: grad norm: 449.673 411.556 181.183
2024-12-02-07:35:27-root-INFO: grad norm: 419.200 381.276 174.233
2024-12-02-07:35:27-root-INFO: grad norm: 390.598 358.167 155.831
2024-12-02-07:35:28-root-INFO: Loss Change: 3960.963 -> 3851.742
2024-12-02-07:35:28-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-07:35:28-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-07:35:28-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:28-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-07:35:28-root-INFO: grad norm: 541.277 488.661 232.791
2024-12-02-07:35:28-root-INFO: Loss too large (3838.614->3921.297)! Learning rate decreased to 0.00062.
2024-12-02-07:35:28-root-INFO: Loss too large (3838.614->3854.210)! Learning rate decreased to 0.00050.
2024-12-02-07:35:29-root-INFO: grad norm: 472.793 432.959 189.945
2024-12-02-07:35:29-root-INFO: grad norm: 422.933 383.556 178.206
2024-12-02-07:35:30-root-INFO: grad norm: 377.895 347.359 148.816
2024-12-02-07:35:30-root-INFO: grad norm: 340.003 309.164 141.492
2024-12-02-07:35:30-root-INFO: grad norm: 306.070 282.347 118.147
2024-12-02-07:35:31-root-INFO: grad norm: 277.280 253.081 113.290
2024-12-02-07:35:31-root-INFO: grad norm: 251.750 233.235 94.761
2024-12-02-07:35:32-root-INFO: Loss Change: 3838.614 -> 3742.713
2024-12-02-07:35:32-root-INFO: Regularization Change: 0.000 -> 0.260
2024-12-02-07:35:32-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-07:35:32-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:32-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-07:35:32-root-INFO: grad norm: 602.117 534.711 276.820
2024-12-02-07:35:32-root-INFO: Loss too large (3770.454->3866.869)! Learning rate decreased to 0.00065.
2024-12-02-07:35:32-root-INFO: Loss too large (3770.454->3783.209)! Learning rate decreased to 0.00052.
2024-12-02-07:35:33-root-INFO: grad norm: 520.314 479.807 201.276
2024-12-02-07:35:33-root-INFO: grad norm: 463.528 418.244 199.824
2024-12-02-07:35:34-root-INFO: grad norm: 412.716 380.460 159.951
2024-12-02-07:35:34-root-INFO: grad norm: 371.201 337.106 155.401
2024-12-02-07:35:35-root-INFO: grad norm: 333.797 308.141 128.334
2024-12-02-07:35:35-root-INFO: grad norm: 302.606 276.365 123.259
2024-12-02-07:35:36-root-INFO: grad norm: 274.471 254.105 103.756
2024-12-02-07:35:36-root-INFO: Loss Change: 3770.454 -> 3652.797
2024-12-02-07:35:36-root-INFO: Regularization Change: 0.000 -> 0.317
2024-12-02-07:35:36-root-INFO: Undo step: 220
2024-12-02-07:35:36-root-INFO: Undo step: 221
2024-12-02-07:35:36-root-INFO: Undo step: 222
2024-12-02-07:35:36-root-INFO: Undo step: 223
2024-12-02-07:35:36-root-INFO: Undo step: 224
2024-12-02-07:35:36-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-07:35:36-root-INFO: grad norm: 3232.549 2834.355 1554.285
2024-12-02-07:35:36-root-INFO: Loss too large (7231.498->9079.872)! Learning rate decreased to 0.00051.
2024-12-02-07:35:37-root-INFO: Loss too large (7231.498->7462.092)! Learning rate decreased to 0.00041.
2024-12-02-07:35:37-root-INFO: grad norm: 2780.183 2570.794 1058.508
2024-12-02-07:35:38-root-INFO: grad norm: 2514.854 2279.131 1063.040
2024-12-02-07:35:38-root-INFO: grad norm: 2310.200 2126.349 903.140
2024-12-02-07:35:38-root-INFO: grad norm: 2120.267 1927.258 883.859
2024-12-02-07:35:39-root-INFO: grad norm: 1985.074 1820.315 791.818
2024-12-02-07:35:39-root-INFO: grad norm: 1857.403 1688.455 773.992
2024-12-02-07:35:40-root-INFO: grad norm: 1763.276 1612.686 713.012
2024-12-02-07:35:40-root-INFO: Loss Change: 7231.498 -> 4892.709
2024-12-02-07:35:40-root-INFO: Regularization Change: 0.000 -> 4.879
2024-12-02-07:35:40-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-07:35:40-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:40-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-07:35:40-root-INFO: grad norm: 1344.900 1221.607 562.523
2024-12-02-07:35:41-root-INFO: Loss too large (4707.235->5227.989)! Learning rate decreased to 0.00054.
2024-12-02-07:35:41-root-INFO: Loss too large (4707.235->4860.046)! Learning rate decreased to 0.00043.
2024-12-02-07:35:41-root-INFO: grad norm: 1307.091 1198.357 521.947
2024-12-02-07:35:42-root-INFO: grad norm: 1289.238 1170.650 540.107
2024-12-02-07:35:42-root-INFO: grad norm: 1270.226 1162.609 511.678
2024-12-02-07:35:43-root-INFO: grad norm: 1244.549 1128.562 524.644
2024-12-02-07:35:43-root-INFO: grad norm: 1218.556 1113.182 495.686
2024-12-02-07:35:44-root-INFO: grad norm: 1184.766 1073.082 502.161
2024-12-02-07:35:44-root-INFO: grad norm: 1150.273 1049.133 471.644
2024-12-02-07:35:44-root-INFO: Loss Change: 4707.235 -> 4360.134
2024-12-02-07:35:44-root-INFO: Regularization Change: 0.000 -> 1.005
2024-12-02-07:35:44-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-07:35:44-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:44-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-07:35:45-root-INFO: grad norm: 715.578 651.911 295.065
2024-12-02-07:35:45-root-INFO: Loss too large (4249.150->4361.823)! Learning rate decreased to 0.00056.
2024-12-02-07:35:45-root-INFO: Loss too large (4249.150->4265.228)! Learning rate decreased to 0.00045.
2024-12-02-07:35:45-root-INFO: grad norm: 664.160 606.602 270.449
2024-12-02-07:35:46-root-INFO: grad norm: 635.210 575.034 269.868
2024-12-02-07:35:46-root-INFO: grad norm: 606.425 554.697 245.077
2024-12-02-07:35:47-root-INFO: grad norm: 574.643 519.887 244.810
2024-12-02-07:35:47-root-INFO: grad norm: 544.334 498.092 219.554
2024-12-02-07:35:48-root-INFO: grad norm: 512.087 463.364 218.006
2024-12-02-07:35:48-root-INFO: grad norm: 481.483 440.897 193.484
2024-12-02-07:35:48-root-INFO: Loss Change: 4249.150 -> 4055.248
2024-12-02-07:35:48-root-INFO: Regularization Change: 0.000 -> 0.581
2024-12-02-07:35:48-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-07:35:48-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:49-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-07:35:49-root-INFO: grad norm: 356.656 323.727 149.679
2024-12-02-07:35:49-root-INFO: Loss too large (3974.108->3982.029)! Learning rate decreased to 0.00059.
2024-12-02-07:35:49-root-INFO: grad norm: 450.826 413.476 179.671
2024-12-02-07:35:49-root-INFO: Loss too large (3964.374->3964.664)! Learning rate decreased to 0.00047.
2024-12-02-07:35:50-root-INFO: grad norm: 410.445 372.594 172.159
2024-12-02-07:35:50-root-INFO: grad norm: 374.835 344.653 147.360
2024-12-02-07:35:51-root-INFO: grad norm: 341.530 310.878 141.413
2024-12-02-07:35:51-root-INFO: grad norm: 312.527 288.378 120.463
2024-12-02-07:35:52-root-INFO: grad norm: 285.890 261.162 116.308
2024-12-02-07:35:52-root-INFO: grad norm: 262.663 243.510 98.462
2024-12-02-07:35:53-root-INFO: Loss Change: 3974.108 -> 3857.240
2024-12-02-07:35:53-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-02-07:35:53-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-07:35:53-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:53-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-07:35:53-root-INFO: grad norm: 185.464 172.299 68.629
2024-12-02-07:35:53-root-INFO: grad norm: 174.646 158.835 72.613
2024-12-02-07:35:54-root-INFO: grad norm: 192.991 180.512 68.271
2024-12-02-07:35:54-root-INFO: grad norm: 263.892 240.242 109.191
2024-12-02-07:35:55-root-INFO: Loss too large (3772.264->3772.943)! Learning rate decreased to 0.00062.
2024-12-02-07:35:55-root-INFO: grad norm: 313.232 287.823 123.581
2024-12-02-07:35:56-root-INFO: grad norm: 384.169 349.115 160.326
2024-12-02-07:35:56-root-INFO: Loss too large (3758.952->3758.958)! Learning rate decreased to 0.00050.
2024-12-02-07:35:56-root-INFO: grad norm: 324.412 297.719 128.866
2024-12-02-07:35:57-root-INFO: grad norm: 275.038 251.481 111.371
2024-12-02-07:35:57-root-INFO: Loss Change: 3818.447 -> 3720.357
2024-12-02-07:35:57-root-INFO: Regularization Change: 0.000 -> 0.516
2024-12-02-07:35:57-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-07:35:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:35:57-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-07:35:57-root-INFO: grad norm: 622.567 552.588 286.768
2024-12-02-07:35:57-root-INFO: Loss too large (3753.885->3830.292)! Learning rate decreased to 0.00065.
2024-12-02-07:35:58-root-INFO: grad norm: 742.627 677.062 305.093
2024-12-02-07:35:58-root-INFO: Loss too large (3750.478->3782.016)! Learning rate decreased to 0.00052.
2024-12-02-07:35:58-root-INFO: grad norm: 606.259 549.240 256.680
2024-12-02-07:35:59-root-INFO: grad norm: 490.040 447.469 199.778
2024-12-02-07:36:00-root-INFO: grad norm: 403.133 367.462 165.795
2024-12-02-07:36:00-root-INFO: grad norm: 331.548 303.284 133.951
2024-12-02-07:36:00-root-INFO: grad norm: 277.155 254.971 108.650
2024-12-02-07:36:01-root-INFO: grad norm: 233.701 215.078 91.421
2024-12-02-07:36:01-root-INFO: Loss Change: 3753.885 -> 3614.562
2024-12-02-07:36:01-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-07:36:01-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-07:36:01-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:36:01-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-07:36:02-root-INFO: grad norm: 234.215 216.631 89.039
2024-12-02-07:36:02-root-INFO: grad norm: 356.836 326.349 144.320
2024-12-02-07:36:02-root-INFO: Loss too large (3584.673->3607.120)! Learning rate decreased to 0.00068.
2024-12-02-07:36:03-root-INFO: grad norm: 413.368 378.650 165.823
2024-12-02-07:36:03-root-INFO: grad norm: 484.199 441.484 198.849
2024-12-02-07:36:03-root-INFO: Loss too large (3581.524->3586.715)! Learning rate decreased to 0.00055.
2024-12-02-07:36:04-root-INFO: grad norm: 371.093 340.272 148.072
2024-12-02-07:36:04-root-INFO: grad norm: 285.881 262.317 113.657
2024-12-02-07:36:05-root-INFO: grad norm: 226.651 209.980 85.317
2024-12-02-07:36:05-root-INFO: grad norm: 184.466 171.310 68.415
2024-12-02-07:36:06-root-INFO: Loss Change: 3586.928 -> 3513.285
2024-12-02-07:36:06-root-INFO: Regularization Change: 0.000 -> 0.343
2024-12-02-07:36:06-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-07:36:06-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-07:36:06-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-07:36:06-root-INFO: grad norm: 375.880 337.048 166.387
2024-12-02-07:36:06-root-INFO: Loss too large (3510.506->3527.407)! Learning rate decreased to 0.00071.
2024-12-02-07:36:07-root-INFO: grad norm: 410.162 377.436 160.547
2024-12-02-07:36:07-root-INFO: grad norm: 460.667 421.021 186.963
2024-12-02-07:36:07-root-INFO: Loss too large (3498.613->3498.662)! Learning rate decreased to 0.00057.
2024-12-02-07:36:08-root-INFO: grad norm: 336.339 309.798 130.954
2024-12-02-07:36:08-root-INFO: grad norm: 253.786 234.230 97.691
2024-12-02-07:36:09-root-INFO: grad norm: 196.318 182.661 71.944
2024-12-02-07:36:09-root-INFO: grad norm: 159.356 149.747 54.498
2024-12-02-07:36:10-root-INFO: grad norm: 135.834 128.555 43.868
2024-12-02-07:36:10-root-INFO: Loss Change: 3510.506 -> 3424.572
2024-12-02-07:36:10-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-02-07:36:10-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-07:36:10-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:36:10-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-07:36:10-root-INFO: grad norm: 322.546 279.158 161.576
2024-12-02-07:36:11-root-INFO: grad norm: 434.059 396.914 175.690
2024-12-02-07:36:11-root-INFO: Loss too large (3422.423->3458.972)! Learning rate decreased to 0.00075.
2024-12-02-07:36:11-root-INFO: grad norm: 467.092 427.853 187.395
2024-12-02-07:36:12-root-INFO: grad norm: 517.449 476.269 202.289
2024-12-02-07:36:12-root-INFO: Loss too large (3416.553->3420.655)! Learning rate decreased to 0.00060.
2024-12-02-07:36:12-root-INFO: grad norm: 370.212 341.646 142.599
2024-12-02-07:36:13-root-INFO: grad norm: 266.840 247.330 100.158
2024-12-02-07:36:13-root-INFO: grad norm: 200.903 187.579 71.945
2024-12-02-07:36:14-root-INFO: grad norm: 158.101 148.575 54.051
2024-12-02-07:36:14-root-INFO: Loss Change: 3426.403 -> 3340.109
2024-12-02-07:36:14-root-INFO: Regularization Change: 0.000 -> 0.376
2024-12-02-07:36:14-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-07:36:14-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:36:14-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-07:36:14-root-INFO: grad norm: 356.493 312.545 171.473
2024-12-02-07:36:15-root-INFO: Loss too large (3344.333->3345.892)! Learning rate decreased to 0.00079.
2024-12-02-07:36:15-root-INFO: grad norm: 343.186 316.957 131.586
2024-12-02-07:36:15-root-INFO: grad norm: 369.610 340.402 144.007
2024-12-02-07:36:16-root-INFO: grad norm: 410.149 379.428 155.746
2024-12-02-07:36:16-root-INFO: Loss too large (3314.089->3314.234)! Learning rate decreased to 0.00063.
2024-12-02-07:36:17-root-INFO: grad norm: 298.665 277.670 110.000
2024-12-02-07:36:17-root-INFO: grad norm: 221.054 206.253 79.528
2024-12-02-07:36:17-root-INFO: grad norm: 171.655 161.559 57.999
2024-12-02-07:36:18-root-INFO: grad norm: 139.782 132.223 45.345
2024-12-02-07:36:18-root-INFO: Loss Change: 3344.333 -> 3254.104
2024-12-02-07:36:18-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-07:36:18-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-07:36:18-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:36:18-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-07:36:19-root-INFO: grad norm: 151.052 141.981 51.557
2024-12-02-07:36:19-root-INFO: grad norm: 198.723 186.912 67.487
2024-12-02-07:36:20-root-INFO: grad norm: 292.171 272.613 105.099
2024-12-02-07:36:20-root-INFO: Loss too large (3231.894->3245.930)! Learning rate decreased to 0.00082.
2024-12-02-07:36:20-root-INFO: grad norm: 317.416 297.053 111.858
2024-12-02-07:36:21-root-INFO: grad norm: 344.549 322.009 122.574
2024-12-02-07:36:21-root-INFO: grad norm: 375.741 351.414 133.003
2024-12-02-07:36:22-root-INFO: grad norm: 407.624 381.107 144.620
2024-12-02-07:36:22-root-INFO: grad norm: 443.020 413.998 157.710
2024-12-02-07:36:22-root-INFO: Loss too large (3215.079->3215.889)! Learning rate decreased to 0.00066.
2024-12-02-07:36:23-root-INFO: Loss Change: 3240.744 -> 3190.206
2024-12-02-07:36:23-root-INFO: Regularization Change: 0.000 -> 0.450
2024-12-02-07:36:23-root-INFO: Undo step: 215
2024-12-02-07:36:23-root-INFO: Undo step: 216
2024-12-02-07:36:23-root-INFO: Undo step: 217
2024-12-02-07:36:23-root-INFO: Undo step: 218
2024-12-02-07:36:23-root-INFO: Undo step: 219
2024-12-02-07:36:23-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-07:36:23-root-INFO: grad norm: 2222.486 1759.627 1357.629
2024-12-02-07:36:23-root-INFO: grad norm: 1904.939 1514.137 1155.932
2024-12-02-07:36:24-root-INFO: grad norm: 2013.939 1709.002 1065.487
2024-12-02-07:36:24-root-INFO: Loss too large (4842.568->5091.015)! Learning rate decreased to 0.00065.
2024-12-02-07:36:24-root-INFO: grad norm: 1856.301 1652.864 844.921
2024-12-02-07:36:25-root-INFO: Loss too large (4445.516->4515.417)! Learning rate decreased to 0.00052.
2024-12-02-07:36:25-root-INFO: grad norm: 1354.769 1227.993 572.216
2024-12-02-07:36:26-root-INFO: grad norm: 998.630 903.303 425.798
2024-12-02-07:36:26-root-INFO: grad norm: 767.152 697.722 318.914
2024-12-02-07:36:26-root-INFO: grad norm: 583.407 526.292 251.756
2024-12-02-07:36:27-root-INFO: Loss Change: 6055.798 -> 3748.869
2024-12-02-07:36:27-root-INFO: Regularization Change: 0.000 -> 6.684
2024-12-02-07:36:27-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-07:36:27-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:36:27-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-07:36:27-root-INFO: grad norm: 481.896 440.156 196.179
2024-12-02-07:36:27-root-INFO: Loss too large (3704.700->3718.491)! Learning rate decreased to 0.00068.
2024-12-02-07:36:28-root-INFO: grad norm: 522.165 470.988 225.450
2024-12-02-07:36:28-root-INFO: grad norm: 575.075 524.861 235.015
2024-12-02-07:36:29-root-INFO: grad norm: 642.771 580.297 276.424
2024-12-02-07:36:29-root-INFO: Loss too large (3662.825->3666.101)! Learning rate decreased to 0.00055.
2024-12-02-07:36:29-root-INFO: grad norm: 468.205 428.192 189.389
2024-12-02-07:36:30-root-INFO: grad norm: 342.133 309.036 146.805
2024-12-02-07:36:30-root-INFO: grad norm: 260.143 240.907 98.174
2024-12-02-07:36:31-root-INFO: grad norm: 205.199 186.719 85.104
2024-12-02-07:36:31-root-INFO: Loss Change: 3704.700 -> 3547.363
2024-12-02-07:36:31-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-07:36:31-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-07:36:31-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-07:36:31-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-07:36:31-root-INFO: grad norm: 384.645 345.536 168.986
2024-12-02-07:36:32-root-INFO: Loss too large (3539.864->3554.023)! Learning rate decreased to 0.00071.
2024-12-02-07:36:32-root-INFO: grad norm: 408.986 371.201 171.697
2024-12-02-07:36:33-root-INFO: grad norm: 441.647 402.512 181.759
2024-12-02-07:36:33-root-INFO: grad norm: 481.490 435.975 204.349
2024-12-02-07:36:34-root-INFO: grad norm: 524.528 478.788 214.223
2024-12-02-07:36:34-root-INFO: grad norm: 574.954 520.431 244.385
2024-12-02-07:36:34-root-INFO: Loss too large (3509.970->3513.314)! Learning rate decreased to 0.00057.
2024-12-02-07:36:35-root-INFO: grad norm: 404.226 370.171 162.394
2024-12-02-07:36:35-root-INFO: grad norm: 285.728 258.994 120.676
2024-12-02-07:36:35-root-INFO: Loss Change: 3539.864 -> 3439.189
2024-12-02-07:36:35-root-INFO: Regularization Change: 0.000 -> 0.441
2024-12-02-07:36:35-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-07:36:35-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:36:36-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-07:36:36-root-INFO: grad norm: 423.387 374.994 196.560
2024-12-02-07:36:36-root-INFO: Loss too large (3437.117->3452.407)! Learning rate decreased to 0.00075.
2024-12-02-07:36:37-root-INFO: grad norm: 420.077 382.863 172.860
2024-12-02-07:36:37-root-INFO: grad norm: 441.626 403.418 179.686
2024-12-02-07:36:37-root-INFO: grad norm: 470.485 427.856 195.692
2024-12-02-07:36:38-root-INFO: grad norm: 501.564 459.164 201.830
2024-12-02-07:36:39-root-INFO: grad norm: 537.599 488.500 224.456
2024-12-02-07:36:39-root-INFO: Loss too large (3400.702->3401.722)! Learning rate decreased to 0.00060.
2024-12-02-07:36:39-root-INFO: grad norm: 367.643 337.828 145.030
2024-12-02-07:36:40-root-INFO: grad norm: 254.043 231.850 103.844
2024-12-02-07:36:40-root-INFO: Loss Change: 3437.117 -> 3335.464
2024-12-02-07:36:40-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-02-07:36:40-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-07:36:40-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:36:40-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-07:36:40-root-INFO: grad norm: 422.457 372.850 198.628
2024-12-02-07:36:40-root-INFO: Loss too large (3343.734->3358.664)! Learning rate decreased to 0.00079.
2024-12-02-07:36:41-root-INFO: grad norm: 416.510 380.032 170.459
2024-12-02-07:36:41-root-INFO: grad norm: 435.499 397.316 178.323
2024-12-02-07:36:42-root-INFO: grad norm: 463.964 422.722 191.229
2024-12-02-07:36:42-root-INFO: grad norm: 496.236 454.745 198.638
2024-12-02-07:36:43-root-INFO: grad norm: 535.146 487.525 220.684
2024-12-02-07:36:43-root-INFO: Loss too large (3307.639->3310.678)! Learning rate decreased to 0.00063.
2024-12-02-07:36:43-root-INFO: grad norm: 368.267 338.484 145.084
2024-12-02-07:36:44-root-INFO: grad norm: 254.485 233.012 102.313
2024-12-02-07:36:44-root-INFO: Loss Change: 3343.734 -> 3242.395
2024-12-02-07:36:44-root-INFO: Regularization Change: 0.000 -> 0.417
2024-12-02-07:36:44-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-07:36:44-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:36:44-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-07:36:44-root-INFO: grad norm: 222.659 205.562 85.565
2024-12-02-07:36:45-root-INFO: grad norm: 320.646 294.888 125.917
2024-12-02-07:36:45-root-INFO: Loss too large (3224.742->3241.438)! Learning rate decreased to 0.00082.
2024-12-02-07:36:45-root-INFO: grad norm: 331.242 305.340 128.409
2024-12-02-07:36:46-root-INFO: grad norm: 344.550 317.154 134.638
2024-12-02-07:36:46-root-INFO: grad norm: 358.781 331.114 138.156
2024-12-02-07:36:47-root-INFO: grad norm: 375.639 345.643 147.090
2024-12-02-07:36:47-root-INFO: grad norm: 392.794 362.563 151.113
2024-12-02-07:36:48-root-INFO: grad norm: 412.682 379.554 162.004
2024-12-02-07:36:48-root-INFO: Loss Change: 3224.851 -> 3190.898
2024-12-02-07:36:48-root-INFO: Regularization Change: 0.000 -> 0.431
2024-12-02-07:36:48-root-INFO: Undo step: 215
2024-12-02-07:36:48-root-INFO: Undo step: 216
2024-12-02-07:36:48-root-INFO: Undo step: 217
2024-12-02-07:36:48-root-INFO: Undo step: 218
2024-12-02-07:36:48-root-INFO: Undo step: 219
2024-12-02-07:36:48-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-07:36:48-root-INFO: grad norm: 3394.707 2959.680 1662.626
2024-12-02-07:36:49-root-INFO: grad norm: 3337.538 2989.326 1484.281
2024-12-02-07:36:49-root-INFO: Loss too large (6869.205->7356.770)! Learning rate decreased to 0.00065.
2024-12-02-07:36:50-root-INFO: grad norm: 3006.694 2771.166 1166.552
2024-12-02-07:36:50-root-INFO: Loss too large (5421.596->5796.543)! Learning rate decreased to 0.00052.
2024-12-02-07:36:50-root-INFO: grad norm: 2126.145 1935.935 879.003
2024-12-02-07:36:51-root-INFO: grad norm: 1345.410 1227.877 549.949
2024-12-02-07:36:51-root-INFO: grad norm: 974.020 893.132 388.625
2024-12-02-07:36:52-root-INFO: grad norm: 689.192 625.779 288.767
2024-12-02-07:36:52-root-INFO: grad norm: 519.036 478.976 199.950
2024-12-02-07:36:52-root-INFO: Loss Change: 7175.352 -> 3830.453
2024-12-02-07:36:52-root-INFO: Regularization Change: 0.000 -> 6.768
2024-12-02-07:36:52-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-07:36:52-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-07:36:52-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-07:36:53-root-INFO: grad norm: 368.274 334.620 153.804
2024-12-02-07:36:53-root-INFO: grad norm: 473.614 435.847 185.331
2024-12-02-07:36:53-root-INFO: Loss too large (3740.005->3741.499)! Learning rate decreased to 0.00068.
2024-12-02-07:36:54-root-INFO: grad norm: 496.345 453.416 201.920
2024-12-02-07:36:54-root-INFO: grad norm: 523.297 480.141 208.095
2024-12-02-07:36:55-root-INFO: grad norm: 557.915 510.683 224.659
2024-12-02-07:36:55-root-INFO: grad norm: 593.959 543.547 239.465
2024-12-02-07:36:56-root-INFO: grad norm: 638.258 584.763 255.783
2024-12-02-07:36:56-root-INFO: grad norm: 683.285 624.056 278.268
2024-12-02-07:36:56-root-INFO: Loss Change: 3774.772 -> 3626.132
2024-12-02-07:36:56-root-INFO: Regularization Change: 0.000 -> 1.233
2024-12-02-07:36:56-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-07:36:56-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-07:36:57-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-07:36:57-root-INFO: grad norm: 521.901 484.425 194.199
2024-12-02-07:36:57-root-INFO: Loss too large (3557.062->3596.937)! Learning rate decreased to 0.00071.
2024-12-02-07:36:57-root-INFO: grad norm: 542.065 493.107 225.122
2024-12-02-07:36:58-root-INFO: grad norm: 570.176 526.351 219.216
2024-12-02-07:36:58-root-INFO: grad norm: 598.161 545.471 245.474
2024-12-02-07:36:59-root-INFO: grad norm: 630.422 580.561 245.723
2024-12-02-07:36:59-root-INFO: grad norm: 661.596 603.593 270.895
2024-12-02-07:37:00-root-INFO: grad norm: 697.433 641.636 273.341
2024-12-02-07:37:00-root-INFO: Loss too large (3506.406->3509.400)! Learning rate decreased to 0.00057.
2024-12-02-07:37:00-root-INFO: grad norm: 469.199 428.597 190.926
2024-12-02-07:37:01-root-INFO: Loss Change: 3557.062 -> 3422.842
2024-12-02-07:37:01-root-INFO: Regularization Change: 0.000 -> 0.580
2024-12-02-07:37:01-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-07:37:01-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:01-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-07:37:01-root-INFO: grad norm: 219.078 194.783 100.273
2024-12-02-07:37:02-root-INFO: grad norm: 211.708 190.357 92.651
2024-12-02-07:37:02-root-INFO: grad norm: 270.626 251.124 100.871
2024-12-02-07:37:02-root-INFO: Loss too large (3352.847->3353.111)! Learning rate decreased to 0.00075.
2024-12-02-07:37:03-root-INFO: grad norm: 274.086 251.676 108.546
2024-12-02-07:37:03-root-INFO: grad norm: 281.170 261.199 104.076
2024-12-02-07:37:04-root-INFO: grad norm: 288.897 265.559 113.755
2024-12-02-07:37:04-root-INFO: grad norm: 297.591 275.934 111.448
2024-12-02-07:37:04-root-INFO: grad norm: 306.406 281.567 120.851
2024-12-02-07:37:05-root-INFO: Loss Change: 3384.333 -> 3296.491
2024-12-02-07:37:05-root-INFO: Regularization Change: 0.000 -> 0.614
2024-12-02-07:37:05-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-07:37:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:05-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-07:37:05-root-INFO: grad norm: 207.371 185.479 92.738
2024-12-02-07:37:06-root-INFO: grad norm: 194.381 179.869 73.696
2024-12-02-07:37:06-root-INFO: grad norm: 233.191 219.423 78.942
2024-12-02-07:37:07-root-INFO: grad norm: 315.743 293.094 117.430
2024-12-02-07:37:07-root-INFO: Loss too large (3231.689->3241.172)! Learning rate decreased to 0.00079.
2024-12-02-07:37:07-root-INFO: grad norm: 315.298 294.075 113.723
2024-12-02-07:37:08-root-INFO: grad norm: 318.329 293.365 123.575
2024-12-02-07:37:08-root-INFO: grad norm: 323.666 300.452 120.366
2024-12-02-07:37:08-root-INFO: grad norm: 329.642 302.958 129.924
2024-12-02-07:37:09-root-INFO: Loss Change: 3266.095 -> 3189.781
2024-12-02-07:37:09-root-INFO: Regularization Change: 0.000 -> 0.609
2024-12-02-07:37:09-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-07:37:09-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:09-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-07:37:09-root-INFO: grad norm: 280.534 261.041 102.748
2024-12-02-07:37:09-root-INFO: Loss too large (3160.230->3165.142)! Learning rate decreased to 0.00082.
2024-12-02-07:37:10-root-INFO: grad norm: 273.057 251.301 106.808
2024-12-02-07:37:10-root-INFO: grad norm: 266.930 249.154 95.779
2024-12-02-07:37:11-root-INFO: grad norm: 261.327 240.647 101.887
2024-12-02-07:37:11-root-INFO: grad norm: 255.862 238.771 91.943
2024-12-02-07:37:12-root-INFO: grad norm: 250.693 230.991 97.419
2024-12-02-07:37:12-root-INFO: grad norm: 245.722 229.273 88.392
2024-12-02-07:37:13-root-INFO: grad norm: 241.047 222.286 93.234
2024-12-02-07:37:13-root-INFO: Loss Change: 3160.230 -> 3090.376
2024-12-02-07:37:13-root-INFO: Regularization Change: 0.000 -> 0.431
2024-12-02-07:37:13-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-07:37:13-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:13-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-07:37:13-root-INFO: grad norm: 167.386 154.240 65.024
2024-12-02-07:37:14-root-INFO: grad norm: 160.814 149.131 60.176
2024-12-02-07:37:14-root-INFO: grad norm: 192.803 183.059 60.518
2024-12-02-07:37:15-root-INFO: grad norm: 252.489 234.150 94.469
2024-12-02-07:37:15-root-INFO: Loss too large (3026.147->3029.318)! Learning rate decreased to 0.00086.
2024-12-02-07:37:15-root-INFO: grad norm: 239.653 224.823 82.994
2024-12-02-07:37:16-root-INFO: grad norm: 229.310 212.014 87.367
2024-12-02-07:37:16-root-INFO: grad norm: 220.227 206.164 77.438
2024-12-02-07:37:17-root-INFO: grad norm: 212.215 196.221 80.822
2024-12-02-07:37:17-root-INFO: Loss Change: 3051.919 -> 2985.061
2024-12-02-07:37:17-root-INFO: Regularization Change: 0.000 -> 0.519
2024-12-02-07:37:17-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-07:37:17-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:17-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-07:37:17-root-INFO: grad norm: 196.108 174.243 89.988
2024-12-02-07:37:18-root-INFO: grad norm: 172.164 155.570 73.746
2024-12-02-07:37:18-root-INFO: grad norm: 199.695 184.357 76.751
2024-12-02-07:37:19-root-INFO: grad norm: 249.462 226.414 104.729
2024-12-02-07:37:19-root-INFO: Loss too large (2946.086->2947.459)! Learning rate decreased to 0.00090.
2024-12-02-07:37:19-root-INFO: grad norm: 232.157 212.101 94.392
2024-12-02-07:37:20-root-INFO: grad norm: 228.532 207.619 95.504
2024-12-02-07:37:20-root-INFO: grad norm: 228.615 208.888 92.903
2024-12-02-07:37:21-root-INFO: grad norm: 233.412 212.469 96.635
2024-12-02-07:37:21-root-INFO: Loss Change: 2978.170 -> 2907.822
2024-12-02-07:37:21-root-INFO: Regularization Change: 0.000 -> 0.556
2024-12-02-07:37:21-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-07:37:21-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:21-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-07:37:21-root-INFO: grad norm: 211.527 195.234 81.407
2024-12-02-07:37:22-root-INFO: Loss too large (2888.771->2891.034)! Learning rate decreased to 0.00095.
2024-12-02-07:37:22-root-INFO: grad norm: 212.442 193.805 87.012
2024-12-02-07:37:22-root-INFO: grad norm: 217.879 200.672 84.866
2024-12-02-07:37:23-root-INFO: grad norm: 225.752 206.253 91.780
2024-12-02-07:37:23-root-INFO: grad norm: 238.739 218.851 95.397
2024-12-02-07:37:24-root-INFO: grad norm: 252.696 231.020 102.396
2024-12-02-07:37:24-root-INFO: grad norm: 272.799 248.879 111.707
2024-12-02-07:37:25-root-INFO: grad norm: 291.910 267.043 117.896
2024-12-02-07:37:25-root-INFO: Loss Change: 2888.771 -> 2845.914
2024-12-02-07:37:25-root-INFO: Regularization Change: 0.000 -> 0.409
2024-12-02-07:37:25-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-07:37:25-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-07:37:25-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-07:37:25-root-INFO: grad norm: 420.240 376.751 186.173
2024-12-02-07:37:25-root-INFO: Loss too large (2837.188->2867.524)! Learning rate decreased to 0.00099.
2024-12-02-07:37:26-root-INFO: grad norm: 404.846 370.531 163.116
2024-12-02-07:37:26-root-INFO: grad norm: 444.450 400.948 191.770
2024-12-02-07:37:26-root-INFO: Loss too large (2812.418->2829.372)! Learning rate decreased to 0.00079.
2024-12-02-07:37:27-root-INFO: grad norm: 338.944 309.934 137.199
2024-12-02-07:37:27-root-INFO: grad norm: 239.086 219.623 94.488
2024-12-02-07:37:28-root-INFO: grad norm: 193.385 177.015 77.867
2024-12-02-07:37:28-root-INFO: grad norm: 156.822 145.716 57.964
2024-12-02-07:37:29-root-INFO: grad norm: 135.740 125.628 51.411
2024-12-02-07:37:29-root-INFO: Loss Change: 2837.188 -> 2743.172
2024-12-02-07:37:29-root-INFO: Regularization Change: 0.000 -> 0.386
2024-12-02-07:37:29-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-07:37:29-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:37:29-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-07:37:30-root-INFO: grad norm: 136.427 126.461 51.187
2024-12-02-07:37:30-root-INFO: grad norm: 184.628 167.884 76.827
2024-12-02-07:37:30-root-INFO: Loss too large (2727.061->2731.924)! Learning rate decreased to 0.00104.
2024-12-02-07:37:31-root-INFO: grad norm: 222.544 203.771 89.461
2024-12-02-07:37:31-root-INFO: Loss too large (2722.515->2723.975)! Learning rate decreased to 0.00083.
2024-12-02-07:37:31-root-INFO: grad norm: 195.715 178.781 79.636
2024-12-02-07:37:32-root-INFO: grad norm: 174.615 161.805 65.649
2024-12-02-07:37:32-root-INFO: grad norm: 162.393 149.200 64.117
2024-12-02-07:37:33-root-INFO: grad norm: 152.118 141.543 55.727
2024-12-02-07:37:33-root-INFO: grad norm: 145.720 134.423 56.257
2024-12-02-07:37:34-root-INFO: Loss Change: 2732.297 -> 2686.140
2024-12-02-07:37:34-root-INFO: Regularization Change: 0.000 -> 0.335
2024-12-02-07:37:34-root-INFO: Undo step: 210
2024-12-02-07:37:34-root-INFO: Undo step: 211
2024-12-02-07:37:34-root-INFO: Undo step: 212
2024-12-02-07:37:34-root-INFO: Undo step: 213
2024-12-02-07:37:34-root-INFO: Undo step: 214
2024-12-02-07:37:34-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-07:37:34-root-INFO: grad norm: 2784.079 2313.812 1548.344
2024-12-02-07:37:34-root-INFO: grad norm: 2497.456 2223.066 1138.097
2024-12-02-07:37:35-root-INFO: grad norm: 2898.402 2520.050 1431.811
2024-12-02-07:37:35-root-INFO: Loss too large (5097.945->5134.132)! Learning rate decreased to 0.00082.
2024-12-02-07:37:35-root-INFO: grad norm: 1513.064 1404.977 561.607
2024-12-02-07:37:36-root-INFO: grad norm: 756.401 706.085 271.269
2024-12-02-07:37:36-root-INFO: grad norm: 645.965 606.451 222.459
2024-12-02-07:37:37-root-INFO: grad norm: 619.740 571.591 239.503
2024-12-02-07:37:37-root-INFO: grad norm: 618.107 572.861 232.135
2024-12-02-07:37:38-root-INFO: Loss Change: 6876.738 -> 3328.299
2024-12-02-07:37:38-root-INFO: Regularization Change: 0.000 -> 11.682
2024-12-02-07:37:38-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-07:37:38-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:38-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-07:37:38-root-INFO: grad norm: 754.552 687.173 311.676
2024-12-02-07:37:38-root-INFO: Loss too large (3330.480->3430.095)! Learning rate decreased to 0.00086.
2024-12-02-07:37:39-root-INFO: grad norm: 741.020 683.779 285.583
2024-12-02-07:37:39-root-INFO: grad norm: 729.542 669.277 290.344
2024-12-02-07:37:40-root-INFO: grad norm: 716.507 660.984 276.556
2024-12-02-07:37:40-root-INFO: grad norm: 704.806 647.827 277.617
2024-12-02-07:37:41-root-INFO: grad norm: 691.546 637.862 267.150
2024-12-02-07:37:41-root-INFO: grad norm: 679.768 625.337 266.530
2024-12-02-07:37:41-root-INFO: grad norm: 666.674 614.881 257.635
2024-12-02-07:37:42-root-INFO: Loss Change: 3330.480 -> 3152.564
2024-12-02-07:37:42-root-INFO: Regularization Change: 0.000 -> 1.028
2024-12-02-07:37:42-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-07:37:42-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:42-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-07:37:42-root-INFO: grad norm: 871.595 791.466 365.047
2024-12-02-07:37:42-root-INFO: Loss too large (3220.471->3379.083)! Learning rate decreased to 0.00090.
2024-12-02-07:37:43-root-INFO: grad norm: 847.737 779.286 333.725
2024-12-02-07:37:43-root-INFO: grad norm: 825.221 753.703 336.036
2024-12-02-07:37:44-root-INFO: grad norm: 799.501 734.036 316.846
2024-12-02-07:37:44-root-INFO: grad norm: 776.450 710.282 313.648
2024-12-02-07:37:45-root-INFO: grad norm: 750.163 688.638 297.527
2024-12-02-07:37:45-root-INFO: grad norm: 726.731 665.236 292.572
2024-12-02-07:37:46-root-INFO: grad norm: 700.912 643.456 277.923
2024-12-02-07:37:46-root-INFO: Loss Change: 3220.471 -> 3041.409
2024-12-02-07:37:46-root-INFO: Regularization Change: 0.000 -> 0.795
2024-12-02-07:37:46-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-07:37:46-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:37:46-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-07:37:46-root-INFO: grad norm: 681.066 622.578 276.130
2024-12-02-07:37:47-root-INFO: Loss too large (3030.240->3118.845)! Learning rate decreased to 0.00095.
2024-12-02-07:37:47-root-INFO: grad norm: 627.507 576.116 248.706
2024-12-02-07:37:48-root-INFO: grad norm: 584.581 534.707 236.267
2024-12-02-07:37:48-root-INFO: grad norm: 541.200 496.180 216.108
2024-12-02-07:37:49-root-INFO: grad norm: 503.898 460.855 203.778
2024-12-02-07:37:49-root-INFO: grad norm: 467.562 428.098 188.006
2024-12-02-07:37:49-root-INFO: grad norm: 436.006 398.550 176.804
2024-12-02-07:37:50-root-INFO: grad norm: 406.208 371.485 164.329
2024-12-02-07:37:50-root-INFO: Loss Change: 3030.240 -> 2871.206
2024-12-02-07:37:50-root-INFO: Regularization Change: 0.000 -> 0.621
2024-12-02-07:37:50-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-07:37:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-07:37:50-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-07:37:51-root-INFO: grad norm: 624.494 559.750 276.899
2024-12-02-07:37:51-root-INFO: Loss too large (2900.719->2975.321)! Learning rate decreased to 0.00099.
2024-12-02-07:37:51-root-INFO: grad norm: 578.019 527.448 236.440
2024-12-02-07:37:52-root-INFO: grad norm: 551.219 497.007 238.385
2024-12-02-07:37:52-root-INFO: grad norm: 524.866 477.694 217.470
2024-12-02-07:37:53-root-INFO: grad norm: 505.584 455.615 219.158
2024-12-02-07:37:53-root-INFO: grad norm: 484.597 441.371 200.066
2024-12-02-07:37:54-root-INFO: grad norm: 464.119 418.336 201.002
2024-12-02-07:37:54-root-INFO: grad norm: 443.780 404.774 181.930
2024-12-02-07:37:54-root-INFO: Loss Change: 2900.719 -> 2769.974
2024-12-02-07:37:54-root-INFO: Regularization Change: 0.000 -> 0.603
2024-12-02-07:37:54-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-07:37:54-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:37:54-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-07:37:55-root-INFO: grad norm: 486.155 440.135 206.465
2024-12-02-07:37:55-root-INFO: Loss too large (2768.793->2827.945)! Learning rate decreased to 0.00104.
2024-12-02-07:37:55-root-INFO: grad norm: 459.745 418.529 190.261
2024-12-02-07:37:56-root-INFO: grad norm: 435.693 395.117 183.607
2024-12-02-07:37:56-root-INFO: grad norm: 416.357 380.117 169.895
2024-12-02-07:37:57-root-INFO: grad norm: 396.608 360.115 166.179
2024-12-02-07:37:57-root-INFO: grad norm: 381.137 348.998 153.185
2024-12-02-07:37:58-root-INFO: grad norm: 364.347 331.396 151.411
2024-12-02-07:37:58-root-INFO: grad norm: 351.530 322.832 139.114
2024-12-02-07:37:58-root-INFO: Loss Change: 2768.793 -> 2679.241
2024-12-02-07:37:58-root-INFO: Regularization Change: 0.000 -> 0.502
2024-12-02-07:37:58-root-INFO: Undo step: 210
2024-12-02-07:37:58-root-INFO: Undo step: 211
2024-12-02-07:37:58-root-INFO: Undo step: 212
2024-12-02-07:37:58-root-INFO: Undo step: 213
2024-12-02-07:37:58-root-INFO: Undo step: 214
2024-12-02-07:37:58-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-07:37:59-root-INFO: grad norm: 2778.605 2319.274 1530.234
2024-12-02-07:37:59-root-INFO: grad norm: 2159.789 1988.352 843.295
2024-12-02-07:37:59-root-INFO: Loss too large (4823.978->5019.111)! Learning rate decreased to 0.00082.
2024-12-02-07:38:00-root-INFO: grad norm: 1916.202 1798.663 660.789
2024-12-02-07:38:00-root-INFO: Loss too large (4099.763->4236.881)! Learning rate decreased to 0.00066.
2024-12-02-07:38:00-root-INFO: grad norm: 1357.594 1255.560 516.365
2024-12-02-07:38:01-root-INFO: grad norm: 915.351 859.264 315.489
2024-12-02-07:38:01-root-INFO: grad norm: 654.241 602.978 253.867
2024-12-02-07:38:02-root-INFO: grad norm: 468.138 439.406 161.479
2024-12-02-07:38:02-root-INFO: grad norm: 349.312 320.367 139.227
2024-12-02-07:38:03-root-INFO: Loss Change: 6288.991 -> 3207.024
2024-12-02-07:38:03-root-INFO: Regularization Change: 0.000 -> 7.843
2024-12-02-07:38:03-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-07:38:03-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:38:03-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-07:38:03-root-INFO: grad norm: 207.255 190.823 80.876
2024-12-02-07:38:03-root-INFO: grad norm: 194.558 179.592 74.830
2024-12-02-07:38:04-root-INFO: grad norm: 223.023 211.631 70.367
2024-12-02-07:38:04-root-INFO: grad norm: 302.120 280.045 113.363
2024-12-02-07:38:05-root-INFO: Loss too large (3086.353->3091.229)! Learning rate decreased to 0.00086.
2024-12-02-07:38:05-root-INFO: grad norm: 325.142 306.666 108.043
2024-12-02-07:38:05-root-INFO: grad norm: 353.289 326.905 133.962
2024-12-02-07:38:06-root-INFO: grad norm: 387.291 363.936 132.458
2024-12-02-07:38:06-root-INFO: grad norm: 425.100 393.163 161.655
2024-12-02-07:38:07-root-INFO: Loss Change: 3155.343 -> 3045.045
2024-12-02-07:38:07-root-INFO: Regularization Change: 0.000 -> 1.136
2024-12-02-07:38:07-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-07:38:07-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:38:07-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-07:38:07-root-INFO: grad norm: 298.503 279.849 103.867
2024-12-02-07:38:08-root-INFO: grad norm: 403.235 376.555 144.238
2024-12-02-07:38:08-root-INFO: Loss too large (2995.392->3030.954)! Learning rate decreased to 0.00090.
2024-12-02-07:38:08-root-INFO: grad norm: 439.439 411.095 155.266
2024-12-02-07:38:09-root-INFO: grad norm: 483.904 447.559 183.995
2024-12-02-07:38:09-root-INFO: Loss too large (2987.137->2987.325)! Learning rate decreased to 0.00072.
2024-12-02-07:38:09-root-INFO: grad norm: 340.930 318.680 121.146
2024-12-02-07:38:10-root-INFO: grad norm: 247.852 229.489 93.623
2024-12-02-07:38:10-root-INFO: grad norm: 185.761 175.197 61.749
2024-12-02-07:38:11-root-INFO: grad norm: 146.995 137.398 52.240
2024-12-02-07:38:11-root-INFO: Loss Change: 3003.485 -> 2901.980
2024-12-02-07:38:11-root-INFO: Regularization Change: 0.000 -> 0.554
2024-12-02-07:38:11-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-07:38:11-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-07:38:11-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-07:38:11-root-INFO: grad norm: 111.888 106.581 34.051
2024-12-02-07:38:12-root-INFO: grad norm: 120.343 113.033 41.304
2024-12-02-07:38:12-root-INFO: grad norm: 145.885 138.499 45.830
2024-12-02-07:38:13-root-INFO: grad norm: 198.814 183.967 75.387
2024-12-02-07:38:13-root-INFO: Loss too large (2855.823->2856.319)! Learning rate decreased to 0.00095.
2024-12-02-07:38:13-root-INFO: grad norm: 210.946 198.068 72.574
2024-12-02-07:38:14-root-INFO: grad norm: 224.504 207.217 86.390
2024-12-02-07:38:14-root-INFO: grad norm: 239.575 224.134 84.619
2024-12-02-07:38:15-root-INFO: grad norm: 255.940 235.761 99.610
2024-12-02-07:38:15-root-INFO: Loss Change: 2883.180 -> 2824.544
2024-12-02-07:38:15-root-INFO: Regularization Change: 0.000 -> 0.618
2024-12-02-07:38:15-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-07:38:15-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-07:38:15-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-07:38:15-root-INFO: grad norm: 197.379 175.784 89.769
2024-12-02-07:38:16-root-INFO: grad norm: 151.501 144.816 44.508
2024-12-02-07:38:16-root-INFO: grad norm: 150.180 144.006 42.618
2024-12-02-07:38:17-root-INFO: grad norm: 179.199 170.100 56.378
2024-12-02-07:38:17-root-INFO: grad norm: 247.593 233.496 82.352
2024-12-02-07:38:17-root-INFO: Loss too large (2742.912->2752.520)! Learning rate decreased to 0.00099.
2024-12-02-07:38:18-root-INFO: grad norm: 257.744 237.779 99.466
2024-12-02-07:38:18-root-INFO: grad norm: 273.965 254.288 101.953
2024-12-02-07:38:19-root-INFO: grad norm: 292.384 267.145 118.836
2024-12-02-07:38:19-root-INFO: Loss Change: 2788.701 -> 2724.209
2024-12-02-07:38:19-root-INFO: Regularization Change: 0.000 -> 0.672
2024-12-02-07:38:19-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-07:38:19-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:19-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-07:38:19-root-INFO: grad norm: 229.849 211.444 90.120
2024-12-02-07:38:20-root-INFO: Loss too large (2697.656->2705.159)! Learning rate decreased to 0.00104.
2024-12-02-07:38:20-root-INFO: grad norm: 240.925 219.890 98.454
2024-12-02-07:38:20-root-INFO: grad norm: 255.577 234.207 102.306
2024-12-02-07:38:21-root-INFO: grad norm: 272.046 246.799 114.451
2024-12-02-07:38:21-root-INFO: grad norm: 289.901 264.631 118.378
2024-12-02-07:38:22-root-INFO: grad norm: 309.521 279.664 132.632
2024-12-02-07:38:22-root-INFO: grad norm: 330.351 300.575 137.062
2024-12-02-07:38:23-root-INFO: grad norm: 353.406 318.440 153.271
2024-12-02-07:38:23-root-INFO: Loss Change: 2697.656 -> 2667.366
2024-12-02-07:38:23-root-INFO: Regularization Change: 0.000 -> 0.447
2024-12-02-07:38:23-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-07:38:23-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:23-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-07:38:24-root-INFO: grad norm: 272.958 248.930 111.983
2024-12-02-07:38:24-root-INFO: Loss too large (2628.282->2636.258)! Learning rate decreased to 0.00109.
2024-12-02-07:38:24-root-INFO: grad norm: 275.181 250.159 114.653
2024-12-02-07:38:25-root-INFO: grad norm: 297.238 267.459 129.675
2024-12-02-07:38:25-root-INFO: grad norm: 329.181 293.871 148.325
2024-12-02-07:38:25-root-INFO: Loss too large (2610.670->2611.592)! Learning rate decreased to 0.00087.
2024-12-02-07:38:26-root-INFO: grad norm: 237.352 212.752 105.227
2024-12-02-07:38:26-root-INFO: grad norm: 174.858 156.385 78.225
2024-12-02-07:38:27-root-INFO: grad norm: 135.331 123.722 54.840
2024-12-02-07:38:27-root-INFO: grad norm: 109.602 100.151 44.523
2024-12-02-07:38:27-root-INFO: Loss Change: 2628.282 -> 2560.901
2024-12-02-07:38:27-root-INFO: Regularization Change: 0.000 -> 0.380
2024-12-02-07:38:27-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-07:38:27-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:28-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-07:38:28-root-INFO: grad norm: 162.308 142.641 77.443
2024-12-02-07:38:28-root-INFO: grad norm: 194.449 174.010 86.781
2024-12-02-07:38:28-root-INFO: Loss too large (2542.658->2549.432)! Learning rate decreased to 0.00114.
2024-12-02-07:38:29-root-INFO: grad norm: 224.560 198.003 105.933
2024-12-02-07:38:29-root-INFO: grad norm: 270.516 239.621 125.542
2024-12-02-07:38:29-root-INFO: Loss too large (2538.072->2540.851)! Learning rate decreased to 0.00091.
2024-12-02-07:38:30-root-INFO: grad norm: 220.810 194.502 104.528
2024-12-02-07:38:30-root-INFO: grad norm: 186.231 166.741 82.943
2024-12-02-07:38:31-root-INFO: grad norm: 158.478 140.636 73.054
2024-12-02-07:38:31-root-INFO: grad norm: 138.119 125.631 57.391
2024-12-02-07:38:32-root-INFO: Loss Change: 2553.355 -> 2498.777
2024-12-02-07:38:32-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-02-07:38:32-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-07:38:32-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:32-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-07:38:32-root-INFO: grad norm: 225.881 198.634 107.549
2024-12-02-07:38:32-root-INFO: Loss too large (2484.809->2503.027)! Learning rate decreased to 0.00120.
2024-12-02-07:38:33-root-INFO: grad norm: 283.841 250.356 133.746
2024-12-02-07:38:33-root-INFO: Loss too large (2484.670->2494.344)! Learning rate decreased to 0.00096.
2024-12-02-07:38:33-root-INFO: grad norm: 259.724 229.222 122.121
2024-12-02-07:38:34-root-INFO: grad norm: 243.788 216.662 111.760
2024-12-02-07:38:34-root-INFO: grad norm: 229.840 203.336 107.149
2024-12-02-07:38:35-root-INFO: grad norm: 219.104 195.800 98.332
2024-12-02-07:38:35-root-INFO: grad norm: 209.757 185.810 97.326
2024-12-02-07:38:36-root-INFO: grad norm: 202.550 181.728 89.451
2024-12-02-07:38:36-root-INFO: Loss Change: 2484.809 -> 2438.586
2024-12-02-07:38:36-root-INFO: Regularization Change: 0.000 -> 0.335
2024-12-02-07:38:36-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-07:38:36-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:36-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-07:38:36-root-INFO: grad norm: 307.118 269.290 147.663
2024-12-02-07:38:37-root-INFO: Loss too large (2433.747->2493.560)! Learning rate decreased to 0.00126.
2024-12-02-07:38:37-root-INFO: Loss too large (2433.747->2450.573)! Learning rate decreased to 0.00101.
2024-12-02-07:38:37-root-INFO: grad norm: 299.633 266.846 136.284
2024-12-02-07:38:38-root-INFO: grad norm: 301.960 267.281 140.503
2024-12-02-07:38:38-root-INFO: grad norm: 305.623 273.584 136.226
2024-12-02-07:38:39-root-INFO: grad norm: 309.611 274.368 143.462
2024-12-02-07:38:39-root-INFO: grad norm: 312.613 280.380 138.253
2024-12-02-07:38:40-root-INFO: grad norm: 315.311 279.537 145.877
2024-12-02-07:38:40-root-INFO: grad norm: 316.795 284.446 139.461
2024-12-02-07:38:40-root-INFO: Loss Change: 2433.747 -> 2393.864
2024-12-02-07:38:40-root-INFO: Regularization Change: 0.000 -> 0.329
2024-12-02-07:38:40-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-07:38:40-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:41-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-07:38:41-root-INFO: grad norm: 435.262 384.011 204.912
2024-12-02-07:38:41-root-INFO: Loss too large (2398.804->2547.783)! Learning rate decreased to 0.00132.
2024-12-02-07:38:41-root-INFO: Loss too large (2398.804->2454.718)! Learning rate decreased to 0.00105.
2024-12-02-07:38:42-root-INFO: grad norm: 446.460 401.287 195.692
2024-12-02-07:38:42-root-INFO: grad norm: 460.922 410.686 209.252
2024-12-02-07:38:42-root-INFO: Loss too large (2391.416->2394.616)! Learning rate decreased to 0.00084.
2024-12-02-07:38:43-root-INFO: grad norm: 306.324 275.693 133.519
2024-12-02-07:38:43-root-INFO: grad norm: 203.017 182.611 88.708
2024-12-02-07:38:44-root-INFO: grad norm: 147.303 134.485 60.099
2024-12-02-07:38:44-root-INFO: grad norm: 112.233 102.837 44.953
2024-12-02-07:38:45-root-INFO: grad norm: 92.453 86.874 31.629
2024-12-02-07:38:45-root-INFO: Loss Change: 2398.804 -> 2316.009
2024-12-02-07:38:45-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-02-07:38:45-root-INFO: Undo step: 205
2024-12-02-07:38:45-root-INFO: Undo step: 206
2024-12-02-07:38:45-root-INFO: Undo step: 207
2024-12-02-07:38:45-root-INFO: Undo step: 208
2024-12-02-07:38:45-root-INFO: Undo step: 209
2024-12-02-07:38:45-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-07:38:45-root-INFO: grad norm: 1685.807 1503.023 763.459
2024-12-02-07:38:46-root-INFO: grad norm: 1199.680 1061.700 558.594
2024-12-02-07:38:46-root-INFO: grad norm: 1231.548 1141.212 462.976
2024-12-02-07:38:46-root-INFO: Loss too large (3472.260->3563.403)! Learning rate decreased to 0.00104.
2024-12-02-07:38:47-root-INFO: grad norm: 933.077 854.239 375.377
2024-12-02-07:38:47-root-INFO: grad norm: 703.641 661.719 239.244
2024-12-02-07:38:48-root-INFO: grad norm: 564.624 526.230 204.651
2024-12-02-07:38:48-root-INFO: grad norm: 457.574 432.574 149.176
2024-12-02-07:38:49-root-INFO: grad norm: 377.814 354.269 131.287
2024-12-02-07:38:49-root-INFO: Loss Change: 4906.257 -> 2806.328
2024-12-02-07:38:49-root-INFO: Regularization Change: 0.000 -> 9.295
2024-12-02-07:38:49-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-07:38:49-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:49-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-07:38:49-root-INFO: grad norm: 273.600 260.987 82.116
2024-12-02-07:38:50-root-INFO: grad norm: 293.452 280.955 84.723
2024-12-02-07:38:50-root-INFO: grad norm: 339.087 324.126 99.612
2024-12-02-07:38:51-root-INFO: grad norm: 408.381 389.343 123.237
2024-12-02-07:38:51-root-INFO: Loss too large (2715.254->2723.403)! Learning rate decreased to 0.00109.
2024-12-02-07:38:51-root-INFO: grad norm: 331.679 315.883 101.139
2024-12-02-07:38:52-root-INFO: grad norm: 267.874 253.654 86.117
2024-12-02-07:38:52-root-INFO: grad norm: 220.887 209.874 68.876
2024-12-02-07:38:53-root-INFO: grad norm: 183.866 173.481 60.916
2024-12-02-07:38:53-root-INFO: Loss Change: 2764.768 -> 2610.928
2024-12-02-07:38:53-root-INFO: Regularization Change: 0.000 -> 1.330
2024-12-02-07:38:53-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-07:38:53-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:53-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-07:38:53-root-INFO: grad norm: 170.161 154.254 71.838
2024-12-02-07:38:54-root-INFO: grad norm: 131.888 124.600 43.234
2024-12-02-07:38:54-root-INFO: grad norm: 117.947 111.652 38.017
2024-12-02-07:38:55-root-INFO: grad norm: 113.608 108.020 35.193
2024-12-02-07:38:55-root-INFO: grad norm: 115.365 109.978 34.839
2024-12-02-07:38:56-root-INFO: grad norm: 121.964 116.092 37.387
2024-12-02-07:38:56-root-INFO: grad norm: 133.258 126.912 40.634
2024-12-02-07:38:57-root-INFO: grad norm: 150.116 142.667 46.698
2024-12-02-07:38:57-root-INFO: Loss Change: 2596.581 -> 2500.622
2024-12-02-07:38:57-root-INFO: Regularization Change: 0.000 -> 1.087
2024-12-02-07:38:57-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-07:38:57-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:38:57-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-07:38:57-root-INFO: grad norm: 165.978 157.248 53.120
2024-12-02-07:38:58-root-INFO: grad norm: 167.306 161.181 44.855
2024-12-02-07:38:58-root-INFO: grad norm: 183.645 175.796 53.114
2024-12-02-07:38:59-root-INFO: grad norm: 208.631 199.580 60.785
2024-12-02-07:38:59-root-INFO: grad norm: 240.719 229.074 73.962
2024-12-02-07:39:00-root-INFO: grad norm: 282.233 268.220 87.824
2024-12-02-07:39:00-root-INFO: Loss too large (2437.005->2439.729)! Learning rate decreased to 0.00120.
2024-12-02-07:39:00-root-INFO: grad norm: 218.779 207.253 70.073
2024-12-02-07:39:01-root-INFO: grad norm: 170.844 161.478 55.792
2024-12-02-07:39:01-root-INFO: Loss Change: 2468.100 -> 2396.750
2024-12-02-07:39:01-root-INFO: Regularization Change: 0.000 -> 0.761
2024-12-02-07:39:01-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-07:39:01-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:39:01-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-07:39:01-root-INFO: grad norm: 137.863 128.636 49.589
2024-12-02-07:39:02-root-INFO: grad norm: 119.364 114.861 32.477
2024-12-02-07:39:02-root-INFO: grad norm: 120.573 115.981 32.955
2024-12-02-07:39:03-root-INFO: grad norm: 127.785 122.516 36.316
2024-12-02-07:39:03-root-INFO: grad norm: 139.665 133.905 39.696
2024-12-02-07:39:04-root-INFO: grad norm: 155.831 148.531 47.138
2024-12-02-07:39:04-root-INFO: grad norm: 175.827 167.585 53.200
2024-12-02-07:39:05-root-INFO: grad norm: 201.050 190.458 64.396
2024-12-02-07:39:05-root-INFO: Loss Change: 2375.642 -> 2324.158
2024-12-02-07:39:05-root-INFO: Regularization Change: 0.000 -> 0.812
2024-12-02-07:39:05-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-07:39:05-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:39:05-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-07:39:05-root-INFO: grad norm: 221.784 210.357 70.269
2024-12-02-07:39:06-root-INFO: grad norm: 221.636 213.824 58.324
2024-12-02-07:39:06-root-INFO: grad norm: 242.834 232.463 70.209
2024-12-02-07:39:07-root-INFO: grad norm: 272.721 260.589 80.436
2024-12-02-07:39:07-root-INFO: Loss too large (2287.393->2289.582)! Learning rate decreased to 0.00132.
2024-12-02-07:39:07-root-INFO: grad norm: 202.172 192.186 62.753
2024-12-02-07:39:08-root-INFO: grad norm: 153.083 144.536 50.437
2024-12-02-07:39:08-root-INFO: grad norm: 124.165 117.022 41.507
2024-12-02-07:39:09-root-INFO: grad norm: 109.840 102.725 38.891
2024-12-02-07:39:09-root-INFO: Loss Change: 2303.976 -> 2236.438
2024-12-02-07:39:09-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-02-07:39:09-root-INFO: Undo step: 205
2024-12-02-07:39:09-root-INFO: Undo step: 206
2024-12-02-07:39:09-root-INFO: Undo step: 207
2024-12-02-07:39:09-root-INFO: Undo step: 208
2024-12-02-07:39:09-root-INFO: Undo step: 209
2024-12-02-07:39:09-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-07:39:09-root-INFO: grad norm: 2840.195 2427.705 1474.096
2024-12-02-07:39:10-root-INFO: grad norm: 2553.232 2394.937 885.027
2024-12-02-07:39:10-root-INFO: grad norm: 3239.531 3114.828 890.176
2024-12-02-07:39:10-root-INFO: Loss too large (5510.393->6961.669)! Learning rate decreased to 0.00104.
2024-12-02-07:39:11-root-INFO: grad norm: 2417.397 2266.751 840.030
2024-12-02-07:39:11-root-INFO: grad norm: 1596.711 1469.391 624.802
2024-12-02-07:39:12-root-INFO: grad norm: 1156.922 1072.208 434.554
2024-12-02-07:39:12-root-INFO: grad norm: 825.502 761.330 319.107
2024-12-02-07:39:13-root-INFO: grad norm: 610.937 565.785 230.503
2024-12-02-07:39:13-root-INFO: Loss Change: 6413.015 -> 2725.584
2024-12-02-07:39:13-root-INFO: Regularization Change: 0.000 -> 13.030
2024-12-02-07:39:13-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-07:39:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:39:13-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-07:39:13-root-INFO: grad norm: 386.683 365.292 126.827
2024-12-02-07:39:14-root-INFO: grad norm: 396.867 372.108 137.983
2024-12-02-07:39:14-root-INFO: grad norm: 438.633 411.009 153.201
2024-12-02-07:39:15-root-INFO: grad norm: 490.492 455.382 182.236
2024-12-02-07:39:15-root-INFO: Loss too large (2639.980->2642.710)! Learning rate decreased to 0.00109.
2024-12-02-07:39:15-root-INFO: grad norm: 354.715 332.100 124.631
2024-12-02-07:39:16-root-INFO: grad norm: 263.017 243.338 99.823
2024-12-02-07:39:16-root-INFO: grad norm: 199.419 187.558 67.749
2024-12-02-07:39:17-root-INFO: grad norm: 157.717 145.970 59.726
2024-12-02-07:39:17-root-INFO: Loss Change: 2680.481 -> 2508.729
2024-12-02-07:39:17-root-INFO: Regularization Change: 0.000 -> 1.260
2024-12-02-07:39:17-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-07:39:17-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:39:17-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-07:39:17-root-INFO: grad norm: 201.640 181.066 88.736
2024-12-02-07:39:18-root-INFO: grad norm: 131.661 122.765 47.576
2024-12-02-07:39:18-root-INFO: grad norm: 106.578 99.306 38.695
2024-12-02-07:39:19-root-INFO: grad norm: 94.801 89.420 31.483
2024-12-02-07:39:19-root-INFO: grad norm: 88.425 83.424 29.315
2024-12-02-07:39:20-root-INFO: grad norm: 84.623 80.343 26.571
2024-12-02-07:39:20-root-INFO: grad norm: 81.984 77.724 26.083
2024-12-02-07:39:21-root-INFO: grad norm: 79.952 76.205 24.189
2024-12-02-07:39:21-root-INFO: Loss Change: 2496.749 -> 2393.083
2024-12-02-07:39:21-root-INFO: Regularization Change: 0.000 -> 1.030
2024-12-02-07:39:21-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-07:39:21-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:39:21-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-07:39:21-root-INFO: grad norm: 147.703 131.592 67.081
2024-12-02-07:39:22-root-INFO: grad norm: 117.066 108.801 43.207
2024-12-02-07:39:22-root-INFO: grad norm: 105.660 97.030 41.822
2024-12-02-07:39:23-root-INFO: grad norm: 99.623 94.086 32.752
2024-12-02-07:39:23-root-INFO: grad norm: 96.251 89.173 36.227
2024-12-02-07:39:24-root-INFO: grad norm: 94.650 90.116 28.944
2024-12-02-07:39:24-root-INFO: grad norm: 94.196 87.714 34.340
2024-12-02-07:39:25-root-INFO: grad norm: 94.705 90.553 27.734
2024-12-02-07:39:25-root-INFO: Loss Change: 2366.230 -> 2297.587
2024-12-02-07:39:25-root-INFO: Regularization Change: 0.000 -> 0.768
2024-12-02-07:39:25-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-07:39:25-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:39:25-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-07:39:25-root-INFO: grad norm: 213.687 192.004 93.790
2024-12-02-07:39:26-root-INFO: grad norm: 182.553 167.806 71.879
2024-12-02-07:39:26-root-INFO: grad norm: 171.962 156.520 71.221
2024-12-02-07:39:27-root-INFO: grad norm: 174.304 162.242 63.714
2024-12-02-07:39:27-root-INFO: grad norm: 180.474 165.658 71.612
2024-12-02-07:39:28-root-INFO: grad norm: 188.336 175.999 67.043
2024-12-02-07:39:28-root-INFO: grad norm: 200.655 185.106 77.449
2024-12-02-07:39:29-root-INFO: grad norm: 214.789 201.342 74.806
2024-12-02-07:39:29-root-INFO: Loss Change: 2288.537 -> 2235.926
2024-12-02-07:39:29-root-INFO: Regularization Change: 0.000 -> 0.706
2024-12-02-07:39:29-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-07:39:29-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-07:39:29-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-07:39:29-root-INFO: grad norm: 414.592 382.084 160.928
2024-12-02-07:39:30-root-INFO: Loss too large (2248.919->2275.998)! Learning rate decreased to 0.00132.
2024-12-02-07:39:30-root-INFO: grad norm: 303.642 286.934 99.335
2024-12-02-07:39:31-root-INFO: grad norm: 229.001 216.977 73.231
2024-12-02-07:39:31-root-INFO: grad norm: 190.067 181.829 55.351
2024-12-02-07:39:31-root-INFO: grad norm: 163.753 156.201 49.158
2024-12-02-07:39:32-root-INFO: grad norm: 147.753 142.055 40.637
2024-12-02-07:39:32-root-INFO: grad norm: 136.150 130.574 38.562
2024-12-02-07:39:33-root-INFO: grad norm: 128.289 123.810 33.603
2024-12-02-07:39:33-root-INFO: Loss Change: 2248.919 -> 2157.270
2024-12-02-07:39:33-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-07:39:33-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-07:39:33-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-07:39:34-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-07:39:34-root-INFO: grad norm: 453.121 431.303 138.911
2024-12-02-07:39:34-root-INFO: Loss too large (2178.938->2230.574)! Learning rate decreased to 0.00138.
2024-12-02-07:39:34-root-INFO: Loss too large (2178.938->2190.397)! Learning rate decreased to 0.00110.
2024-12-02-07:39:35-root-INFO: grad norm: 223.508 215.366 59.778
2024-12-02-07:39:35-root-INFO: grad norm: 151.117 146.711 36.225
2024-12-02-07:39:36-root-INFO: grad norm: 98.967 95.390 26.365
2024-12-02-07:39:36-root-INFO: grad norm: 76.919 73.652 22.182
2024-12-02-07:39:37-root-INFO: grad norm: 68.322 65.850 18.212
2024-12-02-07:39:37-root-INFO: grad norm: 63.949 61.556 17.331
2024-12-02-07:39:37-root-INFO: grad norm: 61.648 59.542 15.976
2024-12-02-07:39:38-root-INFO: Loss Change: 2178.938 -> 2101.779
2024-12-02-07:39:38-root-INFO: Regularization Change: 0.000 -> 0.352
2024-12-02-07:39:38-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-07:39:38-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:39:38-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-07:39:38-root-INFO: grad norm: 101.565 95.521 34.512
2024-12-02-07:39:39-root-INFO: grad norm: 136.734 131.486 37.519
2024-12-02-07:39:39-root-INFO: Loss too large (2086.501->2090.437)! Learning rate decreased to 0.00144.
2024-12-02-07:39:39-root-INFO: grad norm: 172.493 166.869 43.690
2024-12-02-07:39:39-root-INFO: Loss too large (2082.306->2085.386)! Learning rate decreased to 0.00115.
2024-12-02-07:39:40-root-INFO: grad norm: 146.186 141.930 35.018
2024-12-02-07:39:40-root-INFO: grad norm: 112.856 109.153 28.673
2024-12-02-07:39:41-root-INFO: grad norm: 103.016 99.927 25.040
2024-12-02-07:39:41-root-INFO: grad norm: 96.784 93.713 24.184
2024-12-02-07:39:42-root-INFO: grad norm: 92.624 90.039 21.730
2024-12-02-07:39:42-root-INFO: Loss Change: 2089.405 -> 2053.820
2024-12-02-07:39:42-root-INFO: Regularization Change: 0.000 -> 0.351
2024-12-02-07:39:42-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-07:39:42-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:39:43-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-07:39:43-root-INFO: grad norm: 977.407 946.346 244.446
2024-12-02-07:39:43-root-INFO: Loss too large (2157.867->2295.462)! Learning rate decreased to 0.00150.
2024-12-02-07:39:43-root-INFO: Loss too large (2157.867->2218.364)! Learning rate decreased to 0.00120.
2024-12-02-07:39:43-root-INFO: Loss too large (2157.867->2162.788)! Learning rate decreased to 0.00096.
2024-12-02-07:39:44-root-INFO: grad norm: 264.882 257.333 62.787
2024-12-02-07:39:44-root-INFO: grad norm: 182.745 177.555 43.243
2024-12-02-07:39:45-root-INFO: grad norm: 154.529 150.925 33.182
2024-12-02-07:39:45-root-INFO: grad norm: 137.538 134.273 29.789
2024-12-02-07:39:46-root-INFO: grad norm: 119.063 116.015 26.767
2024-12-02-07:39:46-root-INFO: grad norm: 95.495 92.501 23.726
2024-12-02-07:39:47-root-INFO: grad norm: 72.120 69.018 20.922
2024-12-02-07:39:47-root-INFO: Loss Change: 2157.867 -> 2004.671
2024-12-02-07:39:47-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-07:39:47-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-07:39:47-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:39:47-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-07:39:48-root-INFO: grad norm: 69.899 65.446 24.550
2024-12-02-07:39:48-root-INFO: grad norm: 65.208 62.400 18.928
2024-12-02-07:39:48-root-INFO: grad norm: 78.033 76.032 17.556
2024-12-02-07:39:49-root-INFO: grad norm: 188.189 184.281 38.150
2024-12-02-07:39:49-root-INFO: Loss too large (1985.982->2011.634)! Learning rate decreased to 0.00157.
2024-12-02-07:39:49-root-INFO: Loss too large (1985.982->1999.662)! Learning rate decreased to 0.00126.
2024-12-02-07:39:49-root-INFO: Loss too large (1985.982->1991.238)! Learning rate decreased to 0.00101.
2024-12-02-07:39:50-root-INFO: grad norm: 141.970 139.042 28.686
2024-12-02-07:39:50-root-INFO: grad norm: 76.361 74.028 18.731
2024-12-02-07:39:51-root-INFO: grad norm: 70.963 68.840 17.231
2024-12-02-07:39:51-root-INFO: grad norm: 69.315 67.176 17.089
2024-12-02-07:39:52-root-INFO: Loss Change: 2001.630 -> 1965.278
2024-12-02-07:39:52-root-INFO: Regularization Change: 0.000 -> 0.394
2024-12-02-07:39:52-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-07:39:52-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:39:52-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-07:39:52-root-INFO: grad norm: 503.195 487.184 125.928
2024-12-02-07:39:52-root-INFO: Loss too large (1986.923->2106.591)! Learning rate decreased to 0.00165.
2024-12-02-07:39:52-root-INFO: Loss too large (1986.923->2059.847)! Learning rate decreased to 0.00132.
2024-12-02-07:39:53-root-INFO: Loss too large (1986.923->2025.188)! Learning rate decreased to 0.00105.
2024-12-02-07:39:53-root-INFO: Loss too large (1986.923->1999.724)! Learning rate decreased to 0.00084.
2024-12-02-07:39:53-root-INFO: grad norm: 204.279 200.534 38.939
2024-12-02-07:39:54-root-INFO: grad norm: 154.660 151.285 32.131
2024-12-02-07:39:54-root-INFO: grad norm: 71.128 67.692 21.842
2024-12-02-07:39:55-root-INFO: grad norm: 64.036 60.910 19.763
2024-12-02-07:39:55-root-INFO: grad norm: 60.606 58.258 16.704
2024-12-02-07:39:55-root-INFO: grad norm: 58.615 56.227 16.560
2024-12-02-07:39:56-root-INFO: grad norm: 57.516 55.550 14.910
2024-12-02-07:39:56-root-INFO: Loss Change: 1986.923 -> 1925.636
2024-12-02-07:39:56-root-INFO: Regularization Change: 0.000 -> 0.190
2024-12-02-07:39:56-root-INFO: Undo step: 200
2024-12-02-07:39:56-root-INFO: Undo step: 201
2024-12-02-07:39:56-root-INFO: Undo step: 202
2024-12-02-07:39:56-root-INFO: Undo step: 203
2024-12-02-07:39:56-root-INFO: Undo step: 204
2024-12-02-07:39:56-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-07:39:57-root-INFO: grad norm: 1776.779 1641.546 679.904
2024-12-02-07:39:57-root-INFO: grad norm: 1625.749 1558.846 461.582
2024-12-02-07:39:57-root-INFO: grad norm: 1430.412 1381.798 369.747
2024-12-02-07:39:58-root-INFO: grad norm: 1466.736 1422.451 357.696
2024-12-02-07:39:58-root-INFO: grad norm: 1422.629 1352.865 440.033
2024-12-02-07:39:59-root-INFO: grad norm: 1418.200 1336.228 475.169
2024-12-02-07:39:59-root-INFO: grad norm: 1389.122 1319.261 434.983
2024-12-02-07:40:00-root-INFO: grad norm: 1374.814 1306.026 429.431
2024-12-02-07:40:00-root-INFO: Loss Change: 4397.947 -> 3043.636
2024-12-02-07:40:00-root-INFO: Regularization Change: 0.000 -> 12.084
2024-12-02-07:40:00-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-07:40:00-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-07:40:01-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-07:40:01-root-INFO: grad norm: 1333.709 1258.716 440.923
2024-12-02-07:40:01-root-INFO: grad norm: 1285.960 1227.380 383.709
2024-12-02-07:40:02-root-INFO: grad norm: 1222.124 1158.409 389.456
2024-12-02-07:40:02-root-INFO: grad norm: 1150.263 1093.069 358.199
2024-12-02-07:40:03-root-INFO: grad norm: 1091.477 1036.275 342.719
2024-12-02-07:40:03-root-INFO: grad norm: 1056.210 1012.404 301.027
2024-12-02-07:40:04-root-INFO: grad norm: 1005.362 958.802 302.408
2024-12-02-07:40:04-root-INFO: grad norm: 946.309 903.093 282.708
2024-12-02-07:40:04-root-INFO: Loss Change: 3063.071 -> 2514.283
2024-12-02-07:40:04-root-INFO: Regularization Change: 0.000 -> 1.904
2024-12-02-07:40:04-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-07:40:04-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:05-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-07:40:05-root-INFO: grad norm: 861.122 819.102 265.714
2024-12-02-07:40:05-root-INFO: grad norm: 842.292 813.737 217.460
2024-12-02-07:40:06-root-INFO: grad norm: 779.606 748.826 216.897
2024-12-02-07:40:06-root-INFO: grad norm: 714.433 684.715 203.911
2024-12-02-07:40:07-root-INFO: grad norm: 661.321 635.775 182.030
2024-12-02-07:40:07-root-INFO: grad norm: 666.198 646.087 162.452
2024-12-02-07:40:08-root-INFO: grad norm: 630.659 609.188 163.155
2024-12-02-07:40:08-root-INFO: grad norm: 585.408 564.611 154.649
2024-12-02-07:40:09-root-INFO: Loss Change: 2492.484 -> 2213.938
2024-12-02-07:40:09-root-INFO: Regularization Change: 0.000 -> 1.193
2024-12-02-07:40:09-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-07:40:09-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:09-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-07:40:09-root-INFO: grad norm: 606.967 574.248 196.592
2024-12-02-07:40:09-root-INFO: grad norm: 496.121 472.919 149.946
2024-12-02-07:40:10-root-INFO: grad norm: 450.400 435.189 116.064
2024-12-02-07:40:10-root-INFO: Loss too large (2128.262->2128.354)! Learning rate decreased to 0.00150.
2024-12-02-07:40:10-root-INFO: grad norm: 317.765 309.622 71.477
2024-12-02-07:40:11-root-INFO: grad norm: 239.359 233.409 53.037
2024-12-02-07:40:11-root-INFO: grad norm: 177.982 173.136 41.249
2024-12-02-07:40:12-root-INFO: grad norm: 161.549 157.129 37.530
2024-12-02-07:40:12-root-INFO: grad norm: 170.958 165.828 41.566
2024-12-02-07:40:13-root-INFO: Loss Change: 2232.885 -> 2005.490
2024-12-02-07:40:13-root-INFO: Regularization Change: 0.000 -> 1.148
2024-12-02-07:40:13-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-07:40:13-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:13-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-07:40:13-root-INFO: grad norm: 182.738 177.311 44.206
2024-12-02-07:40:13-root-INFO: Loss too large (1999.090->2009.132)! Learning rate decreased to 0.00157.
2024-12-02-07:40:14-root-INFO: grad norm: 245.443 239.317 54.496
2024-12-02-07:40:14-root-INFO: Loss too large (1990.245->2004.184)! Learning rate decreased to 0.00126.
2024-12-02-07:40:14-root-INFO: Loss too large (1990.245->1991.830)! Learning rate decreased to 0.00101.
2024-12-02-07:40:14-root-INFO: grad norm: 152.005 147.623 36.236
2024-12-02-07:40:15-root-INFO: grad norm: 68.305 65.849 18.152
2024-12-02-07:40:15-root-INFO: grad norm: 64.697 62.081 18.211
2024-12-02-07:40:16-root-INFO: grad norm: 63.254 60.822 17.369
2024-12-02-07:40:16-root-INFO: grad norm: 62.288 59.834 17.311
2024-12-02-07:40:17-root-INFO: grad norm: 61.503 59.151 16.848
2024-12-02-07:40:17-root-INFO: Loss Change: 1999.090 -> 1949.718
2024-12-02-07:40:17-root-INFO: Regularization Change: 0.000 -> 0.320
2024-12-02-07:40:17-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-07:40:17-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:17-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-07:40:17-root-INFO: grad norm: 454.676 435.044 132.163
2024-12-02-07:40:17-root-INFO: Loss too large (1972.253->2061.810)! Learning rate decreased to 0.00165.
2024-12-02-07:40:18-root-INFO: Loss too large (1972.253->2022.146)! Learning rate decreased to 0.00132.
2024-12-02-07:40:18-root-INFO: Loss too large (1972.253->1992.439)! Learning rate decreased to 0.00105.
2024-12-02-07:40:18-root-INFO: grad norm: 211.505 207.067 43.100
2024-12-02-07:40:19-root-INFO: grad norm: 150.376 146.256 34.961
2024-12-02-07:40:19-root-INFO: grad norm: 97.732 94.463 25.066
2024-12-02-07:40:20-root-INFO: grad norm: 95.984 93.168 23.075
2024-12-02-07:40:20-root-INFO: grad norm: 98.658 95.600 24.374
2024-12-02-07:40:21-root-INFO: grad norm: 101.662 98.888 23.587
2024-12-02-07:40:21-root-INFO: grad norm: 108.506 105.353 25.969
2024-12-02-07:40:21-root-INFO: Loss Change: 1972.253 -> 1902.571
2024-12-02-07:40:21-root-INFO: Regularization Change: 0.000 -> 0.336
2024-12-02-07:40:21-root-INFO: Undo step: 200
2024-12-02-07:40:21-root-INFO: Undo step: 201
2024-12-02-07:40:21-root-INFO: Undo step: 202
2024-12-02-07:40:21-root-INFO: Undo step: 203
2024-12-02-07:40:21-root-INFO: Undo step: 204
2024-12-02-07:40:22-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-07:40:22-root-INFO: grad norm: 2177.037 2028.306 790.864
2024-12-02-07:40:22-root-INFO: grad norm: 1630.645 1553.418 495.877
2024-12-02-07:40:23-root-INFO: grad norm: 1014.906 967.133 307.713
2024-12-02-07:40:23-root-INFO: grad norm: 772.017 735.075 235.957
2024-12-02-07:40:24-root-INFO: grad norm: 759.610 718.908 245.312
2024-12-02-07:40:24-root-INFO: grad norm: 753.115 715.984 233.556
2024-12-02-07:40:25-root-INFO: grad norm: 757.636 720.410 234.568
2024-12-02-07:40:25-root-INFO: grad norm: 765.684 724.383 248.074
2024-12-02-07:40:25-root-INFO: Loss too large (2555.797->2558.192)! Learning rate decreased to 0.00132.
2024-12-02-07:40:26-root-INFO: Loss Change: 5259.615 -> 2405.541
2024-12-02-07:40:26-root-INFO: Regularization Change: 0.000 -> 13.982
2024-12-02-07:40:26-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-07:40:26-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-07:40:26-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-07:40:26-root-INFO: grad norm: 527.798 508.371 141.880
2024-12-02-07:40:26-root-INFO: grad norm: 509.218 488.351 144.279
2024-12-02-07:40:27-root-INFO: grad norm: 507.721 487.675 141.257
2024-12-02-07:40:27-root-INFO: grad norm: 525.648 501.382 157.865
2024-12-02-07:40:28-root-INFO: Loss too large (2327.924->2330.131)! Learning rate decreased to 0.00138.
2024-12-02-07:40:28-root-INFO: grad norm: 355.668 343.488 92.278
2024-12-02-07:40:28-root-INFO: grad norm: 242.173 230.512 74.242
2024-12-02-07:40:29-root-INFO: grad norm: 174.800 168.804 45.392
2024-12-02-07:40:29-root-INFO: grad norm: 134.628 127.264 43.914
2024-12-02-07:40:30-root-INFO: Loss Change: 2393.813 -> 2160.315
2024-12-02-07:40:30-root-INFO: Regularization Change: 0.000 -> 1.676
2024-12-02-07:40:30-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-07:40:30-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:30-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-07:40:30-root-INFO: grad norm: 134.488 127.865 41.685
2024-12-02-07:40:30-root-INFO: grad norm: 143.813 137.259 42.921
2024-12-02-07:40:31-root-INFO: grad norm: 183.236 176.926 47.670
2024-12-02-07:40:31-root-INFO: grad norm: 197.815 191.009 51.443
2024-12-02-07:40:32-root-INFO: grad norm: 222.077 215.594 53.271
2024-12-02-07:40:32-root-INFO: Loss too large (2108.938->2114.403)! Learning rate decreased to 0.00144.
2024-12-02-07:40:32-root-INFO: grad norm: 181.983 176.284 45.185
2024-12-02-07:40:33-root-INFO: grad norm: 135.819 131.375 34.460
2024-12-02-07:40:33-root-INFO: grad norm: 125.556 121.347 32.239
2024-12-02-07:40:34-root-INFO: Loss Change: 2145.133 -> 2067.162
2024-12-02-07:40:34-root-INFO: Regularization Change: 0.000 -> 0.992
2024-12-02-07:40:34-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-07:40:34-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:34-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-07:40:34-root-INFO: grad norm: 863.215 829.730 238.093
2024-12-02-07:40:34-root-INFO: Loss too large (2156.454->2229.283)! Learning rate decreased to 0.00150.
2024-12-02-07:40:34-root-INFO: Loss too large (2156.454->2179.245)! Learning rate decreased to 0.00120.
2024-12-02-07:40:35-root-INFO: grad norm: 214.651 205.563 61.797
2024-12-02-07:40:35-root-INFO: grad norm: 171.004 166.354 39.606
2024-12-02-07:40:36-root-INFO: grad norm: 158.565 153.094 41.295
2024-12-02-07:40:36-root-INFO: grad norm: 140.532 136.114 34.959
2024-12-02-07:40:37-root-INFO: grad norm: 107.508 103.435 29.311
2024-12-02-07:40:37-root-INFO: grad norm: 78.739 74.037 26.802
2024-12-02-07:40:38-root-INFO: grad norm: 72.262 67.618 25.486
2024-12-02-07:40:38-root-INFO: Loss Change: 2156.454 -> 1997.098
2024-12-02-07:40:38-root-INFO: Regularization Change: 0.000 -> 0.613
2024-12-02-07:40:38-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-07:40:38-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:38-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-07:40:38-root-INFO: grad norm: 77.810 72.095 29.269
2024-12-02-07:40:39-root-INFO: grad norm: 73.964 70.613 22.012
2024-12-02-07:40:39-root-INFO: grad norm: 103.467 99.028 29.979
2024-12-02-07:40:39-root-INFO: Loss too large (1967.861->1968.607)! Learning rate decreased to 0.00157.
2024-12-02-07:40:40-root-INFO: grad norm: 132.906 129.515 29.831
2024-12-02-07:40:40-root-INFO: grad norm: 275.212 268.437 60.692
2024-12-02-07:40:40-root-INFO: Loss too large (1965.429->1991.511)! Learning rate decreased to 0.00126.
2024-12-02-07:40:41-root-INFO: Loss too large (1965.429->1978.239)! Learning rate decreased to 0.00101.
2024-12-02-07:40:41-root-INFO: Loss too large (1965.429->1968.290)! Learning rate decreased to 0.00081.
2024-12-02-07:40:41-root-INFO: grad norm: 147.880 144.400 31.890
2024-12-02-07:40:42-root-INFO: grad norm: 67.658 65.032 18.667
2024-12-02-07:40:42-root-INFO: grad norm: 63.169 60.230 19.044
2024-12-02-07:40:42-root-INFO: Loss Change: 1985.363 -> 1941.968
2024-12-02-07:40:42-root-INFO: Regularization Change: 0.000 -> 0.436
2024-12-02-07:40:42-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-07:40:42-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:43-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-07:40:43-root-INFO: grad norm: 527.217 506.838 145.166
2024-12-02-07:40:43-root-INFO: Loss too large (1965.279->2067.173)! Learning rate decreased to 0.00165.
2024-12-02-07:40:43-root-INFO: Loss too large (1965.279->2028.900)! Learning rate decreased to 0.00132.
2024-12-02-07:40:43-root-INFO: Loss too large (1965.279->1998.809)! Learning rate decreased to 0.00105.
2024-12-02-07:40:43-root-INFO: Loss too large (1965.279->1975.376)! Learning rate decreased to 0.00084.
2024-12-02-07:40:44-root-INFO: grad norm: 196.810 192.336 41.724
2024-12-02-07:40:44-root-INFO: grad norm: 163.242 159.324 35.550
2024-12-02-07:40:45-root-INFO: grad norm: 70.646 66.645 23.437
2024-12-02-07:40:45-root-INFO: grad norm: 64.865 61.234 21.399
2024-12-02-07:40:46-root-INFO: grad norm: 62.248 59.338 18.810
2024-12-02-07:40:46-root-INFO: grad norm: 60.899 57.983 18.620
2024-12-02-07:40:47-root-INFO: grad norm: 60.122 57.638 17.104
2024-12-02-07:40:47-root-INFO: Loss Change: 1965.279 -> 1895.300
2024-12-02-07:40:47-root-INFO: Regularization Change: 0.000 -> 0.220
2024-12-02-07:40:47-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-07:40:47-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:40:47-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-07:40:47-root-INFO: grad norm: 243.857 232.106 74.788
2024-12-02-07:40:48-root-INFO: Loss too large (1891.130->1941.927)! Learning rate decreased to 0.00172.
2024-12-02-07:40:48-root-INFO: Loss too large (1891.130->1922.447)! Learning rate decreased to 0.00138.
2024-12-02-07:40:48-root-INFO: Loss too large (1891.130->1907.900)! Learning rate decreased to 0.00110.
2024-12-02-07:40:48-root-INFO: Loss too large (1891.130->1897.186)! Learning rate decreased to 0.00088.
2024-12-02-07:40:49-root-INFO: grad norm: 182.889 179.236 36.375
2024-12-02-07:40:49-root-INFO: grad norm: 96.292 92.271 27.537
2024-12-02-07:40:50-root-INFO: grad norm: 101.601 99.145 22.208
2024-12-02-07:40:50-root-INFO: grad norm: 117.838 114.301 28.656
2024-12-02-07:40:51-root-INFO: grad norm: 126.985 124.335 25.805
2024-12-02-07:40:51-root-INFO: grad norm: 148.544 144.744 33.384
2024-12-02-07:40:51-root-INFO: Loss too large (1862.972->1863.432)! Learning rate decreased to 0.00070.
2024-12-02-07:40:52-root-INFO: grad norm: 123.552 120.943 25.258
2024-12-02-07:40:52-root-INFO: Loss Change: 1891.130 -> 1856.425
2024-12-02-07:40:52-root-INFO: Regularization Change: 0.000 -> 0.172
2024-12-02-07:40:52-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-07:40:52-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-07:40:52-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-07:40:52-root-INFO: grad norm: 562.424 544.046 142.603
2024-12-02-07:40:52-root-INFO: Loss too large (1889.441->2022.973)! Learning rate decreased to 0.00180.
2024-12-02-07:40:53-root-INFO: Loss too large (1889.441->1984.748)! Learning rate decreased to 0.00144.
2024-12-02-07:40:53-root-INFO: Loss too large (1889.441->1952.806)! Learning rate decreased to 0.00115.
2024-12-02-07:40:53-root-INFO: Loss too large (1889.441->1926.025)! Learning rate decreased to 0.00092.
2024-12-02-07:40:53-root-INFO: Loss too large (1889.441->1903.778)! Learning rate decreased to 0.00074.
2024-12-02-07:40:54-root-INFO: grad norm: 217.514 212.818 44.953
2024-12-02-07:40:54-root-INFO: grad norm: 187.815 183.802 38.620
2024-12-02-07:40:55-root-INFO: grad norm: 79.016 74.562 26.156
2024-12-02-07:40:55-root-INFO: grad norm: 74.704 71.816 20.572
2024-12-02-07:40:55-root-INFO: grad norm: 72.918 69.594 21.763
2024-12-02-07:40:56-root-INFO: grad norm: 72.520 70.208 18.164
2024-12-02-07:40:56-root-INFO: grad norm: 73.478 70.601 20.358
2024-12-02-07:40:57-root-INFO: Loss Change: 1889.441 -> 1823.509
2024-12-02-07:40:57-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-07:40:57-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-07:40:57-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:40:57-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-07:40:57-root-INFO: grad norm: 303.801 293.935 76.793
2024-12-02-07:40:57-root-INFO: Loss too large (1823.132->1917.995)! Learning rate decreased to 0.00188.
2024-12-02-07:40:57-root-INFO: Loss too large (1823.132->1891.505)! Learning rate decreased to 0.00150.
2024-12-02-07:40:58-root-INFO: Loss too large (1823.132->1869.906)! Learning rate decreased to 0.00120.
2024-12-02-07:40:58-root-INFO: Loss too large (1823.132->1852.345)! Learning rate decreased to 0.00096.
2024-12-02-07:40:58-root-INFO: Loss too large (1823.132->1838.196)! Learning rate decreased to 0.00077.
2024-12-02-07:40:58-root-INFO: Loss too large (1823.132->1827.161)! Learning rate decreased to 0.00062.
2024-12-02-07:40:59-root-INFO: grad norm: 200.378 196.382 39.819
2024-12-02-07:40:59-root-INFO: grad norm: 79.854 75.950 24.662
2024-12-02-07:41:00-root-INFO: grad norm: 73.637 70.796 20.260
2024-12-02-07:41:00-root-INFO: grad norm: 68.742 65.882 19.624
2024-12-02-07:41:01-root-INFO: grad norm: 65.741 63.288 17.789
2024-12-02-07:41:01-root-INFO: grad norm: 63.278 60.825 17.446
2024-12-02-07:41:01-root-INFO: grad norm: 61.595 59.341 16.509
2024-12-02-07:41:02-root-INFO: Loss Change: 1823.132 -> 1792.354
2024-12-02-07:41:02-root-INFO: Regularization Change: 0.000 -> 0.098
2024-12-02-07:41:02-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-07:41:02-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:41:02-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-07:41:02-root-INFO: grad norm: 439.227 424.525 112.690
2024-12-02-07:41:02-root-INFO: Loss too large (1809.064->1942.303)! Learning rate decreased to 0.00196.
2024-12-02-07:41:02-root-INFO: Loss too large (1809.064->1908.659)! Learning rate decreased to 0.00157.
2024-12-02-07:41:03-root-INFO: Loss too large (1809.064->1880.281)! Learning rate decreased to 0.00126.
2024-12-02-07:41:03-root-INFO: Loss too large (1809.064->1856.293)! Learning rate decreased to 0.00100.
2024-12-02-07:41:03-root-INFO: Loss too large (1809.064->1836.084)! Learning rate decreased to 0.00080.
2024-12-02-07:41:03-root-INFO: Loss too large (1809.064->1819.178)! Learning rate decreased to 0.00064.
2024-12-02-07:41:03-root-INFO: grad norm: 243.055 238.204 48.318
2024-12-02-07:41:04-root-INFO: grad norm: 94.097 89.606 28.721
2024-12-02-07:41:04-root-INFO: grad norm: 86.598 81.693 28.731
2024-12-02-07:41:05-root-INFO: grad norm: 82.477 79.484 22.017
2024-12-02-07:41:05-root-INFO: grad norm: 80.129 76.606 23.501
2024-12-02-07:41:06-root-INFO: grad norm: 78.969 76.590 19.240
2024-12-02-07:41:06-root-INFO: grad norm: 78.678 75.746 21.280
2024-12-02-07:41:07-root-INFO: Loss Change: 1809.064 -> 1760.598
2024-12-02-07:41:07-root-INFO: Regularization Change: 0.000 -> 0.143
2024-12-02-07:41:07-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-07:41:07-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:41:07-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-07:41:07-root-INFO: grad norm: 137.452 132.485 36.617
2024-12-02-07:41:07-root-INFO: Loss too large (1753.454->1798.515)! Learning rate decreased to 0.00205.
2024-12-02-07:41:07-root-INFO: Loss too large (1753.454->1784.284)! Learning rate decreased to 0.00164.
2024-12-02-07:41:07-root-INFO: Loss too large (1753.454->1772.910)! Learning rate decreased to 0.00131.
2024-12-02-07:41:08-root-INFO: Loss too large (1753.454->1764.318)! Learning rate decreased to 0.00105.
2024-12-02-07:41:08-root-INFO: Loss too large (1753.454->1758.260)! Learning rate decreased to 0.00084.
2024-12-02-07:41:08-root-INFO: Loss too large (1753.454->1754.301)! Learning rate decreased to 0.00067.
2024-12-02-07:41:08-root-INFO: grad norm: 142.952 139.989 28.951
2024-12-02-07:41:09-root-INFO: grad norm: 158.815 155.147 33.937
2024-12-02-07:41:09-root-INFO: grad norm: 165.026 161.950 31.710
2024-12-02-07:41:10-root-INFO: grad norm: 176.614 173.096 35.076
2024-12-02-07:41:10-root-INFO: Loss too large (1745.625->1746.341)! Learning rate decreased to 0.00054.
2024-12-02-07:41:10-root-INFO: grad norm: 137.555 134.850 27.149
2024-12-02-07:41:11-root-INFO: grad norm: 96.866 94.490 21.323
2024-12-02-07:41:11-root-INFO: grad norm: 83.424 81.313 18.648
2024-12-02-07:41:12-root-INFO: Loss Change: 1753.454 -> 1735.852
2024-12-02-07:41:12-root-INFO: Regularization Change: 0.000 -> 0.076
2024-12-02-07:41:12-root-INFO: Undo step: 195
2024-12-02-07:41:12-root-INFO: Undo step: 196
2024-12-02-07:41:12-root-INFO: Undo step: 197
2024-12-02-07:41:12-root-INFO: Undo step: 198
2024-12-02-07:41:12-root-INFO: Undo step: 199
2024-12-02-07:41:12-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-07:41:12-root-INFO: grad norm: 1204.688 1041.176 605.992
2024-12-02-07:41:12-root-INFO: grad norm: 784.582 760.980 190.995
2024-12-02-07:41:13-root-INFO: grad norm: 927.005 908.014 186.677
2024-12-02-07:41:13-root-INFO: Loss too large (2652.070->2873.839)! Learning rate decreased to 0.00165.
2024-12-02-07:41:14-root-INFO: grad norm: 899.222 883.224 168.865
2024-12-02-07:41:14-root-INFO: grad norm: 797.279 769.009 210.428
2024-12-02-07:41:15-root-INFO: grad norm: 609.269 589.802 152.784
2024-12-02-07:41:15-root-INFO: grad norm: 492.269 474.339 131.648
2024-12-02-07:41:15-root-INFO: grad norm: 404.772 390.340 107.124
2024-12-02-07:41:16-root-INFO: Loss Change: 3690.105 -> 2108.062
2024-12-02-07:41:16-root-INFO: Regularization Change: 0.000 -> 11.642
2024-12-02-07:41:16-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-07:41:16-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:41:16-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-07:41:16-root-INFO: grad norm: 444.978 420.598 145.268
2024-12-02-07:41:16-root-INFO: Loss too large (2113.275->2166.041)! Learning rate decreased to 0.00172.
2024-12-02-07:41:17-root-INFO: grad norm: 412.366 397.693 109.022
2024-12-02-07:41:17-root-INFO: grad norm: 401.488 386.079 110.163
2024-12-02-07:41:18-root-INFO: grad norm: 407.216 394.301 101.741
2024-12-02-07:41:18-root-INFO: grad norm: 540.163 521.276 141.587
2024-12-02-07:41:18-root-INFO: Loss too large (2033.218->2098.086)! Learning rate decreased to 0.00138.
2024-12-02-07:41:19-root-INFO: grad norm: 330.940 319.146 87.561
2024-12-02-07:41:19-root-INFO: grad norm: 224.931 221.214 40.723
2024-12-02-07:41:20-root-INFO: grad norm: 170.308 163.509 47.642
2024-12-02-07:41:20-root-INFO: Loss Change: 2113.275 -> 1923.781
2024-12-02-07:41:20-root-INFO: Regularization Change: 0.000 -> 1.414
2024-12-02-07:41:20-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-07:41:20-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-07:41:20-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-07:41:20-root-INFO: grad norm: 200.932 182.953 83.078
2024-12-02-07:41:20-root-INFO: Loss too large (1913.343->1914.584)! Learning rate decreased to 0.00180.
2024-12-02-07:41:21-root-INFO: grad norm: 243.743 235.011 64.659
2024-12-02-07:41:21-root-INFO: Loss too large (1899.956->1988.399)! Learning rate decreased to 0.00144.
2024-12-02-07:41:21-root-INFO: Loss too large (1899.956->1931.572)! Learning rate decreased to 0.00115.
2024-12-02-07:41:21-root-INFO: Loss too large (1899.956->1901.483)! Learning rate decreased to 0.00092.
2024-12-02-07:41:22-root-INFO: grad norm: 320.307 312.646 69.635
2024-12-02-07:41:22-root-INFO: Loss too large (1887.155->1909.632)! Learning rate decreased to 0.00074.
2024-12-02-07:41:22-root-INFO: Loss too large (1887.155->1896.590)! Learning rate decreased to 0.00059.
2024-12-02-07:41:23-root-INFO: grad norm: 226.796 222.249 45.188
2024-12-02-07:41:23-root-INFO: grad norm: 100.436 95.705 30.461
2024-12-02-07:41:24-root-INFO: grad norm: 99.130 95.635 26.091
2024-12-02-07:41:24-root-INFO: grad norm: 100.599 96.067 29.854
2024-12-02-07:41:24-root-INFO: grad norm: 103.746 100.454 25.927
2024-12-02-07:41:25-root-INFO: Loss Change: 1913.343 -> 1857.196
2024-12-02-07:41:25-root-INFO: Regularization Change: 0.000 -> 0.289
2024-12-02-07:41:25-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-07:41:25-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:41:25-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-07:41:25-root-INFO: grad norm: 548.851 535.751 119.199
2024-12-02-07:41:25-root-INFO: Loss too large (1867.549->2041.246)! Learning rate decreased to 0.00188.
2024-12-02-07:41:26-root-INFO: Loss too large (1867.549->2009.248)! Learning rate decreased to 0.00150.
2024-12-02-07:41:26-root-INFO: Loss too large (1867.549->1979.326)! Learning rate decreased to 0.00120.
2024-12-02-07:41:26-root-INFO: Loss too large (1867.549->1951.381)! Learning rate decreased to 0.00096.
2024-12-02-07:41:26-root-INFO: Loss too large (1867.549->1925.807)! Learning rate decreased to 0.00077.
2024-12-02-07:41:26-root-INFO: Loss too large (1867.549->1902.986)! Learning rate decreased to 0.00062.
2024-12-02-07:41:26-root-INFO: Loss too large (1867.549->1883.114)! Learning rate decreased to 0.00049.
2024-12-02-07:41:27-root-INFO: grad norm: 278.651 273.603 52.799
2024-12-02-07:41:27-root-INFO: grad norm: 127.002 123.010 31.592
2024-12-02-07:41:28-root-INFO: grad norm: 111.323 106.349 32.905
2024-12-02-07:41:28-root-INFO: grad norm: 105.092 101.451 27.422
2024-12-02-07:41:29-root-INFO: grad norm: 100.136 95.937 28.692
2024-12-02-07:41:29-root-INFO: grad norm: 98.147 94.743 25.622
2024-12-02-07:41:30-root-INFO: grad norm: 97.359 93.567 26.908
2024-12-02-07:41:30-root-INFO: Loss Change: 1867.549 -> 1819.346
2024-12-02-07:41:30-root-INFO: Regularization Change: 0.000 -> 0.098
2024-12-02-07:41:30-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-07:41:30-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:41:30-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-07:41:30-root-INFO: grad norm: 412.173 398.186 106.464
2024-12-02-07:41:30-root-INFO: Loss too large (1824.471->1980.816)! Learning rate decreased to 0.00196.
2024-12-02-07:41:30-root-INFO: Loss too large (1824.471->1949.623)! Learning rate decreased to 0.00157.
2024-12-02-07:41:31-root-INFO: Loss too large (1824.471->1921.687)! Learning rate decreased to 0.00126.
2024-12-02-07:41:31-root-INFO: Loss too large (1824.471->1896.682)! Learning rate decreased to 0.00100.
2024-12-02-07:41:31-root-INFO: Loss too large (1824.471->1874.474)! Learning rate decreased to 0.00080.
2024-12-02-07:41:31-root-INFO: Loss too large (1824.471->1854.976)! Learning rate decreased to 0.00064.
2024-12-02-07:41:31-root-INFO: Loss too large (1824.471->1838.400)! Learning rate decreased to 0.00051.
2024-12-02-07:41:31-root-INFO: Loss too large (1824.471->1825.297)! Learning rate decreased to 0.00041.
2024-12-02-07:41:32-root-INFO: grad norm: 263.042 257.509 53.667
2024-12-02-07:41:32-root-INFO: grad norm: 111.232 102.151 44.019
2024-12-02-07:41:33-root-INFO: grad norm: 95.911 90.016 33.108
2024-12-02-07:41:33-root-INFO: grad norm: 85.267 78.898 32.337
2024-12-02-07:41:34-root-INFO: grad norm: 78.814 74.067 26.939
2024-12-02-07:41:34-root-INFO: grad norm: 74.244 69.426 26.309
2024-12-02-07:41:35-root-INFO: grad norm: 71.147 67.209 23.343
2024-12-02-07:41:35-root-INFO: Loss Change: 1824.471 -> 1785.004
2024-12-02-07:41:35-root-INFO: Regularization Change: 0.000 -> 0.083
2024-12-02-07:41:35-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-07:41:35-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:41:35-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-07:41:35-root-INFO: grad norm: 283.795 276.719 62.979
2024-12-02-07:41:36-root-INFO: Loss too large (1781.488->1921.459)! Learning rate decreased to 0.00205.
2024-12-02-07:41:36-root-INFO: Loss too large (1781.488->1895.734)! Learning rate decreased to 0.00164.
2024-12-02-07:41:36-root-INFO: Loss too large (1781.488->1871.611)! Learning rate decreased to 0.00131.
2024-12-02-07:41:36-root-INFO: Loss too large (1781.488->1849.326)! Learning rate decreased to 0.00105.
2024-12-02-07:41:36-root-INFO: Loss too large (1781.488->1829.120)! Learning rate decreased to 0.00084.
2024-12-02-07:41:36-root-INFO: Loss too large (1781.488->1811.553)! Learning rate decreased to 0.00067.
2024-12-02-07:41:37-root-INFO: Loss too large (1781.488->1797.376)! Learning rate decreased to 0.00054.
2024-12-02-07:41:37-root-INFO: Loss too large (1781.488->1786.966)! Learning rate decreased to 0.00043.
2024-12-02-07:41:37-root-INFO: grad norm: 234.854 231.015 42.292
2024-12-02-07:41:38-root-INFO: grad norm: 171.381 166.618 40.123
2024-12-02-07:41:38-root-INFO: grad norm: 157.478 154.358 31.191
2024-12-02-07:41:38-root-INFO: grad norm: 141.690 137.779 33.060
2024-12-02-07:41:39-root-INFO: grad norm: 134.670 131.787 27.717
2024-12-02-07:41:39-root-INFO: grad norm: 127.263 123.807 29.457
2024-12-02-07:41:40-root-INFO: grad norm: 123.623 120.875 25.921
2024-12-02-07:41:40-root-INFO: Loss Change: 1781.488 -> 1760.140
2024-12-02-07:41:40-root-INFO: Regularization Change: 0.000 -> 0.053
2024-12-02-07:41:40-root-INFO: Undo step: 195
2024-12-02-07:41:40-root-INFO: Undo step: 196
2024-12-02-07:41:40-root-INFO: Undo step: 197
2024-12-02-07:41:40-root-INFO: Undo step: 198
2024-12-02-07:41:40-root-INFO: Undo step: 199
2024-12-02-07:41:40-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-07:41:40-root-INFO: grad norm: 1101.853 944.266 567.840
2024-12-02-07:41:41-root-INFO: grad norm: 637.838 613.756 173.609
2024-12-02-07:41:41-root-INFO: grad norm: 744.459 731.729 137.083
2024-12-02-07:41:42-root-INFO: Loss too large (2474.860->2574.604)! Learning rate decreased to 0.00165.
2024-12-02-07:41:42-root-INFO: Loss too large (2474.860->2475.726)! Learning rate decreased to 0.00132.
2024-12-02-07:41:42-root-INFO: grad norm: 358.902 348.454 85.970
2024-12-02-07:41:43-root-INFO: grad norm: 289.373 277.716 81.304
2024-12-02-07:41:43-root-INFO: grad norm: 185.110 174.514 61.730
2024-12-02-07:41:44-root-INFO: grad norm: 228.238 220.252 59.845
2024-12-02-07:41:44-root-INFO: Loss too large (2184.116->2184.223)! Learning rate decreased to 0.00105.
2024-12-02-07:41:44-root-INFO: grad norm: 358.715 349.806 79.449
2024-12-02-07:41:45-root-INFO: Loss too large (2168.131->2183.671)! Learning rate decreased to 0.00084.
2024-12-02-07:41:45-root-INFO: Loss too large (2168.131->2169.572)! Learning rate decreased to 0.00067.
2024-12-02-07:41:45-root-INFO: Loss Change: 3870.305 -> 2159.553
2024-12-02-07:41:45-root-INFO: Regularization Change: 0.000 -> 9.154
2024-12-02-07:41:45-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-07:41:45-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-07:41:45-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-07:41:45-root-INFO: grad norm: 220.459 211.689 61.561
2024-12-02-07:41:46-root-INFO: Loss too large (2125.434->2232.438)! Learning rate decreased to 0.00172.
2024-12-02-07:41:46-root-INFO: Loss too large (2125.434->2171.725)! Learning rate decreased to 0.00138.
2024-12-02-07:41:46-root-INFO: Loss too large (2125.434->2136.170)! Learning rate decreased to 0.00110.
2024-12-02-07:41:46-root-INFO: grad norm: 431.479 422.445 87.835
2024-12-02-07:41:46-root-INFO: Loss too large (2117.665->2154.963)! Learning rate decreased to 0.00088.
2024-12-02-07:41:47-root-INFO: Loss too large (2117.665->2134.637)! Learning rate decreased to 0.00070.
2024-12-02-07:41:47-root-INFO: Loss too large (2117.665->2118.643)! Learning rate decreased to 0.00056.
2024-12-02-07:41:47-root-INFO: grad norm: 254.598 247.970 57.715
2024-12-02-07:41:48-root-INFO: grad norm: 121.319 113.450 42.980
2024-12-02-07:41:48-root-INFO: grad norm: 118.120 110.346 42.144
2024-12-02-07:41:49-root-INFO: grad norm: 115.632 107.981 41.362
2024-12-02-07:41:49-root-INFO: grad norm: 113.414 105.882 40.642
2024-12-02-07:41:50-root-INFO: grad norm: 111.389 103.981 39.943
2024-12-02-07:41:50-root-INFO: Loss Change: 2125.434 -> 2043.352
2024-12-02-07:41:50-root-INFO: Regularization Change: 0.000 -> 0.376
2024-12-02-07:41:50-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-07:41:50-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-07:41:50-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-07:41:50-root-INFO: grad norm: 553.182 537.237 131.858
2024-12-02-07:41:50-root-INFO: Loss too large (2059.192->2239.111)! Learning rate decreased to 0.00180.
2024-12-02-07:41:51-root-INFO: Loss too large (2059.192->2199.214)! Learning rate decreased to 0.00144.
2024-12-02-07:41:51-root-INFO: Loss too large (2059.192->2163.223)! Learning rate decreased to 0.00115.
2024-12-02-07:41:51-root-INFO: Loss too large (2059.192->2130.724)! Learning rate decreased to 0.00092.
2024-12-02-07:41:51-root-INFO: Loss too large (2059.192->2102.262)! Learning rate decreased to 0.00074.
2024-12-02-07:41:51-root-INFO: Loss too large (2059.192->2078.195)! Learning rate decreased to 0.00059.
2024-12-02-07:41:52-root-INFO: grad norm: 309.515 303.494 60.753
2024-12-02-07:41:52-root-INFO: grad norm: 150.908 144.465 43.622
2024-12-02-07:41:53-root-INFO: grad norm: 155.529 149.002 44.584
2024-12-02-07:41:53-root-INFO: grad norm: 161.610 156.065 41.972
2024-12-02-07:41:54-root-INFO: grad norm: 176.862 171.133 44.650
2024-12-02-07:41:54-root-INFO: grad norm: 187.868 182.665 43.909
2024-12-02-07:41:55-root-INFO: grad norm: 211.619 206.103 48.004
2024-12-02-07:41:55-root-INFO: Loss Change: 2059.192 -> 1979.404
2024-12-02-07:41:55-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-02-07:41:55-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-07:41:55-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:41:55-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-07:41:55-root-INFO: grad norm: 133.601 123.564 50.805
2024-12-02-07:41:56-root-INFO: grad norm: 300.932 292.482 70.814
2024-12-02-07:41:56-root-INFO: Loss too large (1948.115->2440.538)! Learning rate decreased to 0.00188.
2024-12-02-07:41:56-root-INFO: Loss too large (1948.115->2257.949)! Learning rate decreased to 0.00150.
2024-12-02-07:41:56-root-INFO: Loss too large (1948.115->2125.559)! Learning rate decreased to 0.00120.
2024-12-02-07:41:56-root-INFO: Loss too large (1948.115->2036.697)! Learning rate decreased to 0.00096.
2024-12-02-07:41:56-root-INFO: Loss too large (1948.115->1981.961)! Learning rate decreased to 0.00077.
2024-12-02-07:41:57-root-INFO: Loss too large (1948.115->1951.249)! Learning rate decreased to 0.00062.
2024-12-02-07:41:57-root-INFO: grad norm: 362.087 356.372 64.075
2024-12-02-07:41:57-root-INFO: Loss too large (1935.884->1948.220)! Learning rate decreased to 0.00049.
2024-12-02-07:41:57-root-INFO: Loss too large (1935.884->1936.383)! Learning rate decreased to 0.00039.
2024-12-02-07:41:58-root-INFO: grad norm: 236.890 231.440 50.523
2024-12-02-07:41:58-root-INFO: grad norm: 126.281 121.613 34.020
2024-12-02-07:41:59-root-INFO: grad norm: 108.795 103.343 34.008
2024-12-02-07:41:59-root-INFO: grad norm: 97.455 92.074 31.935
2024-12-02-07:42:00-root-INFO: grad norm: 92.253 86.557 31.914
2024-12-02-07:42:00-root-INFO: Loss Change: 1954.449 -> 1905.083
2024-12-02-07:42:00-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-02-07:42:00-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-07:42:00-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:42:00-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-07:42:01-root-INFO: grad norm: 562.039 547.929 125.149
2024-12-02-07:42:01-root-INFO: Loss too large (1916.315->2133.068)! Learning rate decreased to 0.00196.
2024-12-02-07:42:01-root-INFO: Loss too large (1916.315->2094.961)! Learning rate decreased to 0.00157.
2024-12-02-07:42:01-root-INFO: Loss too large (1916.315->2060.380)! Learning rate decreased to 0.00126.
2024-12-02-07:42:01-root-INFO: Loss too large (1916.315->2027.922)! Learning rate decreased to 0.00100.
2024-12-02-07:42:01-root-INFO: Loss too large (1916.315->1997.695)! Learning rate decreased to 0.00080.
2024-12-02-07:42:02-root-INFO: Loss too large (1916.315->1970.171)! Learning rate decreased to 0.00064.
2024-12-02-07:42:02-root-INFO: Loss too large (1916.315->1945.590)! Learning rate decreased to 0.00051.
2024-12-02-07:42:02-root-INFO: Loss too large (1916.315->1924.240)! Learning rate decreased to 0.00041.
2024-12-02-07:42:02-root-INFO: grad norm: 339.937 334.770 59.047
2024-12-02-07:42:03-root-INFO: grad norm: 108.788 100.474 41.712
2024-12-02-07:42:03-root-INFO: grad norm: 101.515 95.044 35.664
2024-12-02-07:42:04-root-INFO: grad norm: 96.353 89.745 35.066
2024-12-02-07:42:04-root-INFO: grad norm: 92.697 86.981 32.049
2024-12-02-07:42:05-root-INFO: grad norm: 89.767 83.946 31.799
2024-12-02-07:42:05-root-INFO: grad norm: 87.498 82.183 30.030
2024-12-02-07:42:05-root-INFO: Loss Change: 1916.315 -> 1858.335
2024-12-02-07:42:05-root-INFO: Regularization Change: 0.000 -> 0.106
2024-12-02-07:42:05-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-07:42:05-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:42:06-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-07:42:06-root-INFO: grad norm: 296.952 289.876 64.438
2024-12-02-07:42:06-root-INFO: Loss too large (1853.567->2011.070)! Learning rate decreased to 0.00205.
2024-12-02-07:42:06-root-INFO: Loss too large (1853.567->1982.017)! Learning rate decreased to 0.00164.
2024-12-02-07:42:06-root-INFO: Loss too large (1853.567->1954.793)! Learning rate decreased to 0.00131.
2024-12-02-07:42:06-root-INFO: Loss too large (1853.567->1929.795)! Learning rate decreased to 0.00105.
2024-12-02-07:42:07-root-INFO: Loss too large (1853.567->1907.216)! Learning rate decreased to 0.00084.
2024-12-02-07:42:07-root-INFO: Loss too large (1853.567->1887.453)! Learning rate decreased to 0.00067.
2024-12-02-07:42:07-root-INFO: Loss too large (1853.567->1871.325)! Learning rate decreased to 0.00054.
2024-12-02-07:42:07-root-INFO: Loss too large (1853.567->1859.438)! Learning rate decreased to 0.00043.
2024-12-02-07:42:07-root-INFO: grad norm: 264.744 260.581 46.761
2024-12-02-07:42:08-root-INFO: grad norm: 224.755 219.493 48.349
2024-12-02-07:42:08-root-INFO: grad norm: 214.053 210.052 41.194
2024-12-02-07:42:09-root-INFO: grad norm: 201.382 196.735 43.010
2024-12-02-07:42:09-root-INFO: grad norm: 196.332 192.419 39.001
2024-12-02-07:42:10-root-INFO: grad norm: 190.944 186.597 40.514
2024-12-02-07:42:10-root-INFO: grad norm: 188.924 185.069 37.967
2024-12-02-07:42:11-root-INFO: Loss Change: 1853.567 -> 1825.785
2024-12-02-07:42:11-root-INFO: Regularization Change: 0.000 -> 0.079
2024-12-02-07:42:11-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-07:42:11-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:42:11-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-07:42:11-root-INFO: grad norm: 595.513 582.364 124.449
2024-12-02-07:42:11-root-INFO: Loss too large (1843.542->2085.570)! Learning rate decreased to 0.00214.
2024-12-02-07:42:11-root-INFO: Loss too large (1843.542->2050.260)! Learning rate decreased to 0.00171.
2024-12-02-07:42:11-root-INFO: Loss too large (1843.542->2018.268)! Learning rate decreased to 0.00137.
2024-12-02-07:42:12-root-INFO: Loss too large (1843.542->1987.206)! Learning rate decreased to 0.00110.
2024-12-02-07:42:12-root-INFO: Loss too large (1843.542->1956.592)! Learning rate decreased to 0.00088.
2024-12-02-07:42:12-root-INFO: Loss too large (1843.542->1926.901)! Learning rate decreased to 0.00070.
2024-12-02-07:42:12-root-INFO: Loss too large (1843.542->1898.633)! Learning rate decreased to 0.00056.
2024-12-02-07:42:12-root-INFO: Loss too large (1843.542->1872.175)! Learning rate decreased to 0.00045.
2024-12-02-07:42:12-root-INFO: Loss too large (1843.542->1848.711)! Learning rate decreased to 0.00036.
2024-12-02-07:42:13-root-INFO: grad norm: 364.823 360.138 58.280
2024-12-02-07:42:13-root-INFO: grad norm: 140.028 132.813 44.368
2024-12-02-07:42:14-root-INFO: grad norm: 120.659 115.637 34.449
2024-12-02-07:42:14-root-INFO: grad norm: 105.344 99.289 35.200
2024-12-02-07:42:15-root-INFO: grad norm: 96.004 91.052 30.436
2024-12-02-07:42:15-root-INFO: grad norm: 88.958 83.557 30.522
2024-12-02-07:42:16-root-INFO: grad norm: 84.343 79.532 28.078
2024-12-02-07:42:16-root-INFO: Loss Change: 1843.542 -> 1789.930
2024-12-02-07:42:16-root-INFO: Regularization Change: 0.000 -> 0.075
2024-12-02-07:42:16-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-07:42:16-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:42:16-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-07:42:16-root-INFO: grad norm: 645.250 632.249 128.873
2024-12-02-07:42:16-root-INFO: Loss too large (1814.155->2056.840)! Learning rate decreased to 0.00224.
2024-12-02-07:42:17-root-INFO: Loss too large (1814.155->2028.190)! Learning rate decreased to 0.00179.
2024-12-02-07:42:17-root-INFO: Loss too large (1814.155->2001.569)! Learning rate decreased to 0.00143.
2024-12-02-07:42:17-root-INFO: Loss too large (1814.155->1974.599)! Learning rate decreased to 0.00114.
2024-12-02-07:42:17-root-INFO: Loss too large (1814.155->1946.423)! Learning rate decreased to 0.00092.
2024-12-02-07:42:17-root-INFO: Loss too large (1814.155->1917.471)! Learning rate decreased to 0.00073.
2024-12-02-07:42:17-root-INFO: Loss too large (1814.155->1888.532)! Learning rate decreased to 0.00059.
2024-12-02-07:42:18-root-INFO: Loss too large (1814.155->1860.163)! Learning rate decreased to 0.00047.
2024-12-02-07:42:18-root-INFO: Loss too large (1814.155->1833.212)! Learning rate decreased to 0.00038.
2024-12-02-07:42:18-root-INFO: grad norm: 419.756 414.486 66.305
2024-12-02-07:42:19-root-INFO: grad norm: 148.494 141.360 45.472
2024-12-02-07:42:19-root-INFO: grad norm: 144.894 140.137 36.822
2024-12-02-07:42:20-root-INFO: grad norm: 142.584 136.929 39.758
2024-12-02-07:42:20-root-INFO: grad norm: 141.190 137.161 33.489
2024-12-02-07:42:20-root-INFO: grad norm: 140.580 135.792 36.376
2024-12-02-07:42:21-root-INFO: grad norm: 140.482 136.882 31.598
2024-12-02-07:42:21-root-INFO: Loss Change: 1814.155 -> 1757.639
2024-12-02-07:42:21-root-INFO: Regularization Change: 0.000 -> 0.078
2024-12-02-07:42:21-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-07:42:21-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-07:42:21-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-07:42:22-root-INFO: grad norm: 428.608 420.070 85.123
2024-12-02-07:42:22-root-INFO: Loss too large (1756.301->1987.743)! Learning rate decreased to 0.00233.
2024-12-02-07:42:22-root-INFO: Loss too large (1756.301->1959.278)! Learning rate decreased to 0.00187.
2024-12-02-07:42:22-root-INFO: Loss too large (1756.301->1931.281)! Learning rate decreased to 0.00149.
2024-12-02-07:42:22-root-INFO: Loss too large (1756.301->1903.148)! Learning rate decreased to 0.00119.
2024-12-02-07:42:22-root-INFO: Loss too large (1756.301->1875.182)! Learning rate decreased to 0.00096.
2024-12-02-07:42:23-root-INFO: Loss too large (1756.301->1847.793)! Learning rate decreased to 0.00076.
2024-12-02-07:42:23-root-INFO: Loss too large (1756.301->1821.292)! Learning rate decreased to 0.00061.
2024-12-02-07:42:23-root-INFO: Loss too large (1756.301->1796.516)! Learning rate decreased to 0.00049.
2024-12-02-07:42:23-root-INFO: Loss too large (1756.301->1775.333)! Learning rate decreased to 0.00039.
2024-12-02-07:42:23-root-INFO: Loss too large (1756.301->1759.441)! Learning rate decreased to 0.00031.
2024-12-02-07:42:24-root-INFO: grad norm: 305.323 301.457 48.430
2024-12-02-07:42:24-root-INFO: grad norm: 190.394 185.027 44.888
2024-12-02-07:42:25-root-INFO: grad norm: 154.084 150.599 32.590
2024-12-02-07:42:25-root-INFO: grad norm: 123.607 118.961 33.568
2024-12-02-07:42:26-root-INFO: grad norm: 106.632 102.922 27.884
2024-12-02-07:42:26-root-INFO: grad norm: 93.679 89.260 28.432
2024-12-02-07:42:27-root-INFO: grad norm: 85.798 81.864 25.682
2024-12-02-07:42:27-root-INFO: Loss Change: 1756.301 -> 1725.894
2024-12-02-07:42:27-root-INFO: Regularization Change: 0.000 -> 0.044
2024-12-02-07:42:27-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-07:42:27-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:42:27-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-07:42:27-root-INFO: grad norm: 763.735 748.602 151.286
2024-12-02-07:42:27-root-INFO: Loss too large (1779.206->2022.792)! Learning rate decreased to 0.00244.
2024-12-02-07:42:28-root-INFO: Loss too large (1779.206->1994.983)! Learning rate decreased to 0.00195.
2024-12-02-07:42:28-root-INFO: Loss too large (1779.206->1971.745)! Learning rate decreased to 0.00156.
2024-12-02-07:42:28-root-INFO: Loss too large (1779.206->1949.855)! Learning rate decreased to 0.00125.
2024-12-02-07:42:28-root-INFO: Loss too large (1779.206->1926.887)! Learning rate decreased to 0.00100.
2024-12-02-07:42:28-root-INFO: Loss too large (1779.206->1901.775)! Learning rate decreased to 0.00080.
2024-12-02-07:42:28-root-INFO: Loss too large (1779.206->1874.738)! Learning rate decreased to 0.00064.
2024-12-02-07:42:28-root-INFO: Loss too large (1779.206->1846.396)! Learning rate decreased to 0.00051.
2024-12-02-07:42:29-root-INFO: Loss too large (1779.206->1817.218)! Learning rate decreased to 0.00041.
2024-12-02-07:42:29-root-INFO: Loss too large (1779.206->1788.212)! Learning rate decreased to 0.00033.
2024-12-02-07:42:29-root-INFO: grad norm: 460.129 454.395 72.418
2024-12-02-07:42:30-root-INFO: grad norm: 158.071 148.023 55.458
2024-12-02-07:42:30-root-INFO: grad norm: 145.683 138.984 43.670
2024-12-02-07:42:31-root-INFO: grad norm: 134.863 126.832 45.845
2024-12-02-07:42:31-root-INFO: grad norm: 126.342 120.704 37.320
2024-12-02-07:42:32-root-INFO: grad norm: 118.540 111.899 39.119
2024-12-02-07:42:32-root-INFO: grad norm: 112.367 107.468 32.819
2024-12-02-07:42:32-root-INFO: Loss Change: 1779.206 -> 1704.514
2024-12-02-07:42:32-root-INFO: Regularization Change: 0.000 -> 0.088
2024-12-02-07:42:32-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-07:42:32-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:42:33-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-07:42:33-root-INFO: grad norm: 668.847 656.844 126.145
2024-12-02-07:42:33-root-INFO: Loss too large (1736.944->1982.718)! Learning rate decreased to 0.00254.
2024-12-02-07:42:33-root-INFO: Loss too large (1736.944->1961.353)! Learning rate decreased to 0.00203.
2024-12-02-07:42:33-root-INFO: Loss too large (1736.944->1941.724)! Learning rate decreased to 0.00163.
2024-12-02-07:42:33-root-INFO: Loss too large (1736.944->1921.479)! Learning rate decreased to 0.00130.
2024-12-02-07:42:34-root-INFO: Loss too large (1736.944->1898.992)! Learning rate decreased to 0.00104.
2024-12-02-07:42:34-root-INFO: Loss too large (1736.944->1873.842)! Learning rate decreased to 0.00083.
2024-12-02-07:42:34-root-INFO: Loss too large (1736.944->1846.511)! Learning rate decreased to 0.00067.
2024-12-02-07:42:34-root-INFO: Loss too large (1736.944->1817.597)! Learning rate decreased to 0.00053.
2024-12-02-07:42:34-root-INFO: Loss too large (1736.944->1787.586)! Learning rate decreased to 0.00043.
2024-12-02-07:42:34-root-INFO: Loss too large (1736.944->1758.038)! Learning rate decreased to 0.00034.
2024-12-02-07:42:35-root-INFO: grad norm: 472.531 467.676 67.565
2024-12-02-07:42:35-root-INFO: grad norm: 246.429 239.580 57.698
2024-12-02-07:42:36-root-INFO: grad norm: 245.976 242.246 42.676
2024-12-02-07:42:36-root-INFO: grad norm: 245.994 240.179 53.170
2024-12-02-07:42:37-root-INFO: grad norm: 245.744 242.320 40.874
2024-12-02-07:42:37-root-INFO: grad norm: 245.608 240.444 50.098
2024-12-02-07:42:38-root-INFO: grad norm: 245.402 242.148 39.833
2024-12-02-07:42:38-root-INFO: Loss Change: 1736.944 -> 1684.076
2024-12-02-07:42:38-root-INFO: Regularization Change: 0.000 -> 0.066
2024-12-02-07:42:38-root-INFO: Undo step: 190
2024-12-02-07:42:38-root-INFO: Undo step: 191
2024-12-02-07:42:38-root-INFO: Undo step: 192
2024-12-02-07:42:38-root-INFO: Undo step: 193
2024-12-02-07:42:38-root-INFO: Undo step: 194
2024-12-02-07:42:38-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-07:42:38-root-INFO: grad norm: 1334.484 1238.899 495.959
2024-12-02-07:42:39-root-INFO: grad norm: 1499.201 1445.969 395.954
2024-12-02-07:42:39-root-INFO: Loss too large (3477.438->3549.926)! Learning rate decreased to 0.00205.
2024-12-02-07:42:39-root-INFO: grad norm: 844.743 824.867 182.164
2024-12-02-07:42:40-root-INFO: grad norm: 308.607 296.297 86.292
2024-12-02-07:42:40-root-INFO: grad norm: 250.889 238.743 77.119
2024-12-02-07:42:41-root-INFO: grad norm: 219.692 210.037 64.414
2024-12-02-07:42:41-root-INFO: grad norm: 228.808 219.800 63.571
2024-12-02-07:42:42-root-INFO: grad norm: 340.965 331.163 81.170
2024-12-02-07:42:42-root-INFO: Loss too large (2020.226->2055.535)! Learning rate decreased to 0.00164.
2024-12-02-07:42:42-root-INFO: Loss too large (2020.226->2020.990)! Learning rate decreased to 0.00131.
2024-12-02-07:42:42-root-INFO: Loss Change: 3868.929 -> 1999.700
2024-12-02-07:42:42-root-INFO: Regularization Change: 0.000 -> 14.279
2024-12-02-07:42:42-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-07:42:42-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:42:42-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-07:42:43-root-INFO: grad norm: 214.431 209.203 47.060
2024-12-02-07:42:43-root-INFO: Loss too large (1967.951->2044.518)! Learning rate decreased to 0.00214.
2024-12-02-07:42:43-root-INFO: Loss too large (1967.951->1979.126)! Learning rate decreased to 0.00171.
2024-12-02-07:42:43-root-INFO: grad norm: 344.489 337.526 68.910
2024-12-02-07:42:44-root-INFO: Loss too large (1948.380->1992.914)! Learning rate decreased to 0.00137.
2024-12-02-07:42:44-root-INFO: Loss too large (1948.380->1967.549)! Learning rate decreased to 0.00110.
2024-12-02-07:42:44-root-INFO: Loss too large (1948.380->1948.839)! Learning rate decreased to 0.00088.
2024-12-02-07:42:44-root-INFO: grad norm: 208.847 204.860 40.614
2024-12-02-07:42:45-root-INFO: grad norm: 104.491 98.574 34.663
2024-12-02-07:42:45-root-INFO: grad norm: 98.047 92.324 33.009
2024-12-02-07:42:46-root-INFO: grad norm: 93.988 88.307 32.180
2024-12-02-07:42:46-root-INFO: grad norm: 90.735 85.387 30.691
2024-12-02-07:42:47-root-INFO: grad norm: 88.388 83.175 29.906
2024-12-02-07:42:47-root-INFO: Loss Change: 1967.951 -> 1871.958
2024-12-02-07:42:47-root-INFO: Regularization Change: 0.000 -> 0.587
2024-12-02-07:42:47-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-07:42:47-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:42:47-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-07:42:47-root-INFO: grad norm: 364.632 357.158 73.452
2024-12-02-07:42:48-root-INFO: Loss too large (1871.239->2036.305)! Learning rate decreased to 0.00224.
2024-12-02-07:42:48-root-INFO: Loss too large (1871.239->1999.616)! Learning rate decreased to 0.00179.
2024-12-02-07:42:48-root-INFO: Loss too large (1871.239->1965.808)! Learning rate decreased to 0.00143.
2024-12-02-07:42:48-root-INFO: Loss too large (1871.239->1935.819)! Learning rate decreased to 0.00114.
2024-12-02-07:42:48-root-INFO: Loss too large (1871.239->1910.131)! Learning rate decreased to 0.00092.
2024-12-02-07:42:48-root-INFO: Loss too large (1871.239->1888.997)! Learning rate decreased to 0.00073.
2024-12-02-07:42:48-root-INFO: Loss too large (1871.239->1872.777)! Learning rate decreased to 0.00059.
2024-12-02-07:42:49-root-INFO: grad norm: 238.759 234.991 42.248
2024-12-02-07:42:49-root-INFO: grad norm: 117.251 111.562 36.080
2024-12-02-07:42:50-root-INFO: grad norm: 104.124 99.360 31.133
2024-12-02-07:42:50-root-INFO: grad norm: 94.598 89.701 30.042
2024-12-02-07:42:51-root-INFO: grad norm: 89.291 84.732 28.166
2024-12-02-07:42:51-root-INFO: grad norm: 85.206 80.734 27.242
2024-12-02-07:42:52-root-INFO: grad norm: 82.428 78.098 26.365
2024-12-02-07:42:52-root-INFO: Loss Change: 1871.239 -> 1820.693
2024-12-02-07:42:52-root-INFO: Regularization Change: 0.000 -> 0.163
2024-12-02-07:42:52-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-07:42:52-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-07:42:52-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-07:42:52-root-INFO: grad norm: 211.024 206.599 42.989
2024-12-02-07:42:53-root-INFO: Loss too large (1804.659->1923.764)! Learning rate decreased to 0.00233.
2024-12-02-07:42:53-root-INFO: Loss too large (1804.659->1894.004)! Learning rate decreased to 0.00187.
2024-12-02-07:42:53-root-INFO: Loss too large (1804.659->1868.121)! Learning rate decreased to 0.00149.
2024-12-02-07:42:53-root-INFO: Loss too large (1804.659->1846.214)! Learning rate decreased to 0.00119.
2024-12-02-07:42:53-root-INFO: Loss too large (1804.659->1828.680)! Learning rate decreased to 0.00096.
2024-12-02-07:42:53-root-INFO: Loss too large (1804.659->1815.737)! Learning rate decreased to 0.00076.
2024-12-02-07:42:53-root-INFO: Loss too large (1804.659->1807.022)! Learning rate decreased to 0.00061.
2024-12-02-07:42:54-root-INFO: grad norm: 202.982 199.967 34.855
2024-12-02-07:42:54-root-INFO: grad norm: 199.489 196.250 35.803
2024-12-02-07:42:55-root-INFO: grad norm: 201.040 198.047 34.562
2024-12-02-07:42:55-root-INFO: grad norm: 206.316 203.403 34.548
2024-12-02-07:42:56-root-INFO: grad norm: 211.246 208.248 35.461
2024-12-02-07:42:56-root-INFO: grad norm: 221.087 218.264 35.219
2024-12-02-07:42:56-root-INFO: Loss too large (1784.494->1784.552)! Learning rate decreased to 0.00049.
2024-12-02-07:42:57-root-INFO: grad norm: 167.743 164.791 31.328
2024-12-02-07:42:57-root-INFO: Loss Change: 1804.659 -> 1775.532
2024-12-02-07:42:57-root-INFO: Regularization Change: 0.000 -> 0.116
2024-12-02-07:42:57-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-07:42:57-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:42:57-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-07:42:58-root-INFO: grad norm: 609.592 599.123 112.490
2024-12-02-07:42:58-root-INFO: Loss too large (1815.564->2050.371)! Learning rate decreased to 0.00244.
2024-12-02-07:42:58-root-INFO: Loss too large (1815.564->2015.140)! Learning rate decreased to 0.00195.
2024-12-02-07:42:58-root-INFO: Loss too large (1815.564->1982.099)! Learning rate decreased to 0.00156.
2024-12-02-07:42:58-root-INFO: Loss too large (1815.564->1948.900)! Learning rate decreased to 0.00125.
2024-12-02-07:42:58-root-INFO: Loss too large (1815.564->1915.267)! Learning rate decreased to 0.00100.
2024-12-02-07:42:58-root-INFO: Loss too large (1815.564->1882.182)! Learning rate decreased to 0.00080.
2024-12-02-07:42:59-root-INFO: Loss too large (1815.564->1850.554)! Learning rate decreased to 0.00064.
2024-12-02-07:42:59-root-INFO: Loss too large (1815.564->1821.340)! Learning rate decreased to 0.00051.
2024-12-02-07:42:59-root-INFO: grad norm: 351.579 347.593 52.791
2024-12-02-07:43:00-root-INFO: grad norm: 108.762 101.098 40.105
2024-12-02-07:43:00-root-INFO: grad norm: 97.946 91.932 33.790
2024-12-02-07:43:01-root-INFO: grad norm: 90.591 84.739 32.031
2024-12-02-07:43:01-root-INFO: grad norm: 85.393 80.455 28.617
2024-12-02-07:43:02-root-INFO: grad norm: 81.369 76.639 27.339
2024-12-02-07:43:02-root-INFO: grad norm: 78.385 74.141 25.445
2024-12-02-07:43:02-root-INFO: Loss Change: 1815.564 -> 1739.893
2024-12-02-07:43:02-root-INFO: Regularization Change: 0.000 -> 0.141
2024-12-02-07:43:02-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-07:43:02-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:43:03-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-07:43:03-root-INFO: grad norm: 443.799 437.087 76.890
2024-12-02-07:43:03-root-INFO: Loss too large (1751.445->1979.545)! Learning rate decreased to 0.00254.
2024-12-02-07:43:03-root-INFO: Loss too large (1751.445->1951.055)! Learning rate decreased to 0.00203.
2024-12-02-07:43:03-root-INFO: Loss too large (1751.445->1921.258)! Learning rate decreased to 0.00163.
2024-12-02-07:43:03-root-INFO: Loss too large (1751.445->1890.086)! Learning rate decreased to 0.00130.
2024-12-02-07:43:03-root-INFO: Loss too large (1751.445->1858.499)! Learning rate decreased to 0.00104.
2024-12-02-07:43:04-root-INFO: Loss too large (1751.445->1827.385)! Learning rate decreased to 0.00083.
2024-12-02-07:43:04-root-INFO: Loss too large (1751.445->1797.638)! Learning rate decreased to 0.00067.
2024-12-02-07:43:04-root-INFO: Loss too large (1751.445->1771.290)! Learning rate decreased to 0.00053.
2024-12-02-07:43:04-root-INFO: grad norm: 355.894 352.730 47.349
2024-12-02-07:43:05-root-INFO: grad norm: 247.988 243.855 45.086
2024-12-02-07:43:05-root-INFO: grad norm: 244.554 241.774 36.772
2024-12-02-07:43:06-root-INFO: grad norm: 241.330 237.838 40.904
2024-12-02-07:43:06-root-INFO: grad norm: 240.967 238.300 35.749
2024-12-02-07:43:07-root-INFO: grad norm: 241.299 238.118 39.051
2024-12-02-07:43:07-root-INFO: grad norm: 242.409 239.799 35.480
2024-12-02-07:43:08-root-INFO: Loss Change: 1751.445 -> 1712.813
2024-12-02-07:43:08-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-07:43:08-root-INFO: Undo step: 190
2024-12-02-07:43:08-root-INFO: Undo step: 191
2024-12-02-07:43:08-root-INFO: Undo step: 192
2024-12-02-07:43:08-root-INFO: Undo step: 193
2024-12-02-07:43:08-root-INFO: Undo step: 194
2024-12-02-07:43:08-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-07:43:08-root-INFO: grad norm: 1027.035 966.804 346.541
2024-12-02-07:43:08-root-INFO: grad norm: 560.269 518.880 211.341
2024-12-02-07:43:09-root-INFO: Loss too large (2703.307->2882.238)! Learning rate decreased to 0.00205.
2024-12-02-07:43:09-root-INFO: Loss too large (2703.307->2714.281)! Learning rate decreased to 0.00164.
2024-12-02-07:43:09-root-INFO: grad norm: 700.857 667.816 212.656
2024-12-02-07:43:09-root-INFO: Loss too large (2618.289->2698.129)! Learning rate decreased to 0.00131.
2024-12-02-07:43:10-root-INFO: grad norm: 772.554 740.866 218.990
2024-12-02-07:43:10-root-INFO: grad norm: 728.621 697.983 209.064
2024-12-02-07:43:11-root-INFO: grad norm: 608.909 583.740 173.256
2024-12-02-07:43:11-root-INFO: grad norm: 549.482 528.232 151.332
2024-12-02-07:43:12-root-INFO: grad norm: 561.308 543.970 138.431
2024-12-02-07:43:12-root-INFO: Loss too large (2275.231->2285.259)! Learning rate decreased to 0.00105.
2024-12-02-07:43:12-root-INFO: Loss Change: 3931.047 -> 2236.619
2024-12-02-07:43:12-root-INFO: Regularization Change: 0.000 -> 11.276
2024-12-02-07:43:12-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-07:43:12-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:43:12-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-07:43:13-root-INFO: grad norm: 398.434 386.267 97.714
2024-12-02-07:43:13-root-INFO: Loss too large (2217.152->2587.270)! Learning rate decreased to 0.00214.
2024-12-02-07:43:13-root-INFO: Loss too large (2217.152->2391.647)! Learning rate decreased to 0.00171.
2024-12-02-07:43:13-root-INFO: Loss too large (2217.152->2263.706)! Learning rate decreased to 0.00137.
2024-12-02-07:43:14-root-INFO: grad norm: 527.928 517.408 104.865
2024-12-02-07:43:14-root-INFO: Loss too large (2190.420->2235.854)! Learning rate decreased to 0.00110.
2024-12-02-07:43:14-root-INFO: Loss too large (2190.420->2198.915)! Learning rate decreased to 0.00088.
2024-12-02-07:43:15-root-INFO: grad norm: 292.579 286.749 58.115
2024-12-02-07:43:15-root-INFO: grad norm: 114.026 105.639 42.922
2024-12-02-07:43:15-root-INFO: grad norm: 111.203 103.391 40.945
2024-12-02-07:43:16-root-INFO: grad norm: 112.478 104.936 40.494
2024-12-02-07:43:16-root-INFO: grad norm: 120.933 114.559 38.741
2024-12-02-07:43:17-root-INFO: grad norm: 136.676 130.456 40.763
2024-12-02-07:43:17-root-INFO: Loss Change: 2217.152 -> 2070.582
2024-12-02-07:43:17-root-INFO: Regularization Change: 0.000 -> 0.740
2024-12-02-07:43:17-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-07:43:17-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-07:43:17-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-07:43:18-root-INFO: grad norm: 566.140 557.907 96.199
2024-12-02-07:43:18-root-INFO: Loss too large (2090.538->2308.808)! Learning rate decreased to 0.00224.
2024-12-02-07:43:18-root-INFO: Loss too large (2090.538->2267.607)! Learning rate decreased to 0.00179.
2024-12-02-07:43:18-root-INFO: Loss too large (2090.538->2229.327)! Learning rate decreased to 0.00143.
2024-12-02-07:43:18-root-INFO: Loss too large (2090.538->2192.481)! Learning rate decreased to 0.00114.
2024-12-02-07:43:18-root-INFO: Loss too large (2090.538->2157.434)! Learning rate decreased to 0.00092.
2024-12-02-07:43:18-root-INFO: Loss too large (2090.538->2124.844)! Learning rate decreased to 0.00073.
2024-12-02-07:43:19-root-INFO: Loss too large (2090.538->2095.514)! Learning rate decreased to 0.00059.
2024-12-02-07:43:19-root-INFO: grad norm: 320.359 315.315 56.625
2024-12-02-07:43:20-root-INFO: grad norm: 104.897 98.164 36.976
2024-12-02-07:43:20-root-INFO: grad norm: 99.735 93.066 35.859
2024-12-02-07:43:21-root-INFO: grad norm: 96.105 89.708 34.478
2024-12-02-07:43:21-root-INFO: grad norm: 93.579 87.170 34.034
2024-12-02-07:43:22-root-INFO: grad norm: 91.624 85.451 33.062
2024-12-02-07:43:22-root-INFO: grad norm: 90.080 83.871 32.864
2024-12-02-07:43:22-root-INFO: Loss Change: 2090.538 -> 2004.169
2024-12-02-07:43:22-root-INFO: Regularization Change: 0.000 -> 0.223
2024-12-02-07:43:22-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-07:43:22-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-07:43:23-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-07:43:23-root-INFO: grad norm: 189.563 184.878 41.886
2024-12-02-07:43:23-root-INFO: Loss too large (1984.052->2083.978)! Learning rate decreased to 0.00233.
2024-12-02-07:43:23-root-INFO: Loss too large (1984.052->2052.967)! Learning rate decreased to 0.00187.
2024-12-02-07:43:23-root-INFO: Loss too large (1984.052->2027.850)! Learning rate decreased to 0.00149.
2024-12-02-07:43:23-root-INFO: Loss too large (1984.052->2008.525)! Learning rate decreased to 0.00119.
2024-12-02-07:43:24-root-INFO: Loss too large (1984.052->1994.783)! Learning rate decreased to 0.00096.
2024-12-02-07:43:24-root-INFO: Loss too large (1984.052->1985.830)! Learning rate decreased to 0.00076.
2024-12-02-07:43:24-root-INFO: grad norm: 221.364 217.013 43.677
2024-12-02-07:43:25-root-INFO: grad norm: 306.876 303.048 48.319
2024-12-02-07:43:25-root-INFO: Loss too large (1978.600->1987.666)! Learning rate decreased to 0.00061.
2024-12-02-07:43:25-root-INFO: grad norm: 268.309 263.842 48.758
2024-12-02-07:43:26-root-INFO: grad norm: 221.540 218.210 38.269
2024-12-02-07:43:26-root-INFO: grad norm: 210.349 205.985 42.625
2024-12-02-07:43:27-root-INFO: grad norm: 197.903 194.678 35.581
2024-12-02-07:43:27-root-INFO: grad norm: 193.167 188.840 40.656
2024-12-02-07:43:28-root-INFO: Loss Change: 1984.052 -> 1944.738
2024-12-02-07:43:28-root-INFO: Regularization Change: 0.000 -> 0.201
2024-12-02-07:43:28-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-07:43:28-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:43:28-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-07:43:28-root-INFO: grad norm: 620.240 611.152 105.790
2024-12-02-07:43:28-root-INFO: Loss too large (1985.440->2226.250)! Learning rate decreased to 0.00244.
2024-12-02-07:43:28-root-INFO: Loss too large (1985.440->2185.112)! Learning rate decreased to 0.00195.
2024-12-02-07:43:28-root-INFO: Loss too large (1985.440->2149.219)! Learning rate decreased to 0.00156.
2024-12-02-07:43:28-root-INFO: Loss too large (1985.440->2114.635)! Learning rate decreased to 0.00125.
2024-12-02-07:43:29-root-INFO: Loss too large (1985.440->2080.052)! Learning rate decreased to 0.00100.
2024-12-02-07:43:29-root-INFO: Loss too large (1985.440->2045.749)! Learning rate decreased to 0.00080.
2024-12-02-07:43:29-root-INFO: Loss too large (1985.440->2012.097)! Learning rate decreased to 0.00064.
2024-12-02-07:43:29-root-INFO: grad norm: 375.424 370.428 61.044
2024-12-02-07:43:30-root-INFO: grad norm: 100.623 94.082 35.689
2024-12-02-07:43:30-root-INFO: grad norm: 96.680 90.691 33.501
2024-12-02-07:43:31-root-INFO: grad norm: 95.135 89.784 31.455
2024-12-02-07:43:31-root-INFO: grad norm: 95.267 90.081 31.005
2024-12-02-07:43:32-root-INFO: grad norm: 97.485 92.974 29.314
2024-12-02-07:43:32-root-INFO: grad norm: 101.368 96.754 30.234
2024-12-02-07:43:33-root-INFO: Loss Change: 1985.440 -> 1890.428
2024-12-02-07:43:33-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-07:43:33-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-07:43:33-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:43:33-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-07:43:33-root-INFO: grad norm: 428.466 422.742 69.803
2024-12-02-07:43:33-root-INFO: Loss too large (1898.900->2122.179)! Learning rate decreased to 0.00254.
2024-12-02-07:43:33-root-INFO: Loss too large (1898.900->2090.987)! Learning rate decreased to 0.00203.
2024-12-02-07:43:33-root-INFO: Loss too large (1898.900->2059.488)! Learning rate decreased to 0.00163.
2024-12-02-07:43:34-root-INFO: Loss too large (1898.900->2027.232)! Learning rate decreased to 0.00130.
2024-12-02-07:43:34-root-INFO: Loss too large (1898.900->1994.712)! Learning rate decreased to 0.00104.
2024-12-02-07:43:34-root-INFO: Loss too large (1898.900->1962.420)! Learning rate decreased to 0.00083.
2024-12-02-07:43:34-root-INFO: Loss too large (1898.900->1932.102)! Learning rate decreased to 0.00067.
2024-12-02-07:43:34-root-INFO: Loss too large (1898.900->1907.057)! Learning rate decreased to 0.00053.
2024-12-02-07:43:35-root-INFO: grad norm: 315.335 311.414 49.577
2024-12-02-07:43:35-root-INFO: grad norm: 204.541 200.984 37.977
2024-12-02-07:43:36-root-INFO: grad norm: 179.323 175.734 35.698
2024-12-02-07:43:36-root-INFO: grad norm: 154.918 151.643 31.690
2024-12-02-07:43:37-root-INFO: grad norm: 141.176 137.534 31.861
2024-12-02-07:43:37-root-INFO: grad norm: 128.344 125.115 28.607
2024-12-02-07:43:38-root-INFO: grad norm: 120.046 116.323 29.666
2024-12-02-07:43:38-root-INFO: Loss Change: 1898.900 -> 1846.063
2024-12-02-07:43:38-root-INFO: Regularization Change: 0.000 -> 0.136
2024-12-02-07:43:38-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-07:43:38-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:43:38-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-07:43:38-root-INFO: grad norm: 367.496 362.625 59.637
2024-12-02-07:43:38-root-INFO: Loss too large (1850.660->2062.217)! Learning rate decreased to 0.00265.
2024-12-02-07:43:39-root-INFO: Loss too large (1850.660->2035.355)! Learning rate decreased to 0.00212.
2024-12-02-07:43:39-root-INFO: Loss too large (1850.660->2006.533)! Learning rate decreased to 0.00170.
2024-12-02-07:43:39-root-INFO: Loss too large (1850.660->1976.194)! Learning rate decreased to 0.00136.
2024-12-02-07:43:39-root-INFO: Loss too large (1850.660->1944.982)! Learning rate decreased to 0.00109.
2024-12-02-07:43:39-root-INFO: Loss too large (1850.660->1913.766)! Learning rate decreased to 0.00087.
2024-12-02-07:43:39-root-INFO: Loss too large (1850.660->1885.159)! Learning rate decreased to 0.00070.
2024-12-02-07:43:40-root-INFO: Loss too large (1850.660->1862.361)! Learning rate decreased to 0.00056.
2024-12-02-07:43:40-root-INFO: grad norm: 313.723 310.034 47.971
2024-12-02-07:43:40-root-INFO: grad norm: 254.848 251.309 42.322
2024-12-02-07:43:41-root-INFO: grad norm: 239.119 235.686 40.373
2024-12-02-07:43:41-root-INFO: grad norm: 221.917 218.797 37.081
2024-12-02-07:43:42-root-INFO: grad norm: 213.814 210.456 37.746
2024-12-02-07:43:42-root-INFO: grad norm: 205.365 202.476 34.326
2024-12-02-07:43:43-root-INFO: grad norm: 201.118 197.800 36.382
2024-12-02-07:43:43-root-INFO: Loss Change: 1850.660 -> 1808.119
2024-12-02-07:43:43-root-INFO: Regularization Change: 0.000 -> 0.135
2024-12-02-07:43:43-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-07:43:43-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:43:43-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-07:43:44-root-INFO: grad norm: 508.301 502.545 76.280
2024-12-02-07:43:44-root-INFO: Loss too large (1821.770->2065.710)! Learning rate decreased to 0.00277.
2024-12-02-07:43:44-root-INFO: Loss too large (1821.770->2037.117)! Learning rate decreased to 0.00222.
2024-12-02-07:43:44-root-INFO: Loss too large (1821.770->2009.835)! Learning rate decreased to 0.00177.
2024-12-02-07:43:44-root-INFO: Loss too large (1821.770->1980.966)! Learning rate decreased to 0.00142.
2024-12-02-07:43:44-root-INFO: Loss too large (1821.770->1949.784)! Learning rate decreased to 0.00113.
2024-12-02-07:43:44-root-INFO: Loss too large (1821.770->1916.706)! Learning rate decreased to 0.00091.
2024-12-02-07:43:45-root-INFO: Loss too large (1821.770->1882.159)! Learning rate decreased to 0.00073.
2024-12-02-07:43:45-root-INFO: Loss too large (1821.770->1848.404)! Learning rate decreased to 0.00058.
2024-12-02-07:43:45-root-INFO: grad norm: 403.244 399.142 57.373
2024-12-02-07:43:46-root-INFO: grad norm: 285.755 282.430 43.468
2024-12-02-07:43:46-root-INFO: grad norm: 291.113 287.559 45.346
2024-12-02-07:43:47-root-INFO: grad norm: 297.972 294.762 43.622
2024-12-02-07:43:47-root-INFO: Loss too large (1781.684->1782.175)! Learning rate decreased to 0.00046.
2024-12-02-07:43:47-root-INFO: grad norm: 217.809 214.527 37.669
2024-12-02-07:43:48-root-INFO: grad norm: 155.921 153.309 28.423
2024-12-02-07:43:48-root-INFO: grad norm: 128.001 124.705 28.862
2024-12-02-07:43:48-root-INFO: Loss Change: 1821.770 -> 1760.717
2024-12-02-07:43:48-root-INFO: Regularization Change: 0.000 -> 0.128
2024-12-02-07:43:48-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-07:43:48-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-07:43:49-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-07:43:49-root-INFO: grad norm: 367.248 361.938 62.224
2024-12-02-07:43:49-root-INFO: Loss too large (1778.118->1996.048)! Learning rate decreased to 0.00289.
2024-12-02-07:43:49-root-INFO: Loss too large (1778.118->1973.775)! Learning rate decreased to 0.00231.
2024-12-02-07:43:49-root-INFO: Loss too large (1778.118->1948.470)! Learning rate decreased to 0.00185.
2024-12-02-07:43:49-root-INFO: Loss too large (1778.118->1920.116)! Learning rate decreased to 0.00148.
2024-12-02-07:43:50-root-INFO: Loss too large (1778.118->1889.390)! Learning rate decreased to 0.00118.
2024-12-02-07:43:50-root-INFO: Loss too large (1778.118->1857.118)! Learning rate decreased to 0.00095.
2024-12-02-07:43:50-root-INFO: Loss too large (1778.118->1825.849)! Learning rate decreased to 0.00076.
2024-12-02-07:43:50-root-INFO: Loss too large (1778.118->1799.462)! Learning rate decreased to 0.00061.
2024-12-02-07:43:50-root-INFO: Loss too large (1778.118->1780.274)! Learning rate decreased to 0.00048.
2024-12-02-07:43:51-root-INFO: grad norm: 277.608 273.895 45.253
2024-12-02-07:43:51-root-INFO: grad norm: 201.729 197.774 39.746
2024-12-02-07:43:52-root-INFO: grad norm: 169.721 166.227 34.260
2024-12-02-07:43:52-root-INFO: grad norm: 142.262 138.676 31.742
2024-12-02-07:43:52-root-INFO: grad norm: 125.330 121.802 29.527
2024-12-02-07:43:53-root-INFO: grad norm: 111.291 107.806 27.632
2024-12-02-07:43:53-root-INFO: grad norm: 101.780 98.198 26.763
2024-12-02-07:43:54-root-INFO: Loss Change: 1778.118 -> 1733.598
2024-12-02-07:43:54-root-INFO: Regularization Change: 0.000 -> 0.116
2024-12-02-07:43:54-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-07:43:54-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:43:54-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-07:43:54-root-INFO: grad norm: 356.006 351.658 55.470
2024-12-02-07:43:54-root-INFO: Loss too large (1730.299->1957.387)! Learning rate decreased to 0.00301.
2024-12-02-07:43:54-root-INFO: Loss too large (1730.299->1935.461)! Learning rate decreased to 0.00241.
2024-12-02-07:43:54-root-INFO: Loss too large (1730.299->1910.484)! Learning rate decreased to 0.00193.
2024-12-02-07:43:55-root-INFO: Loss too large (1730.299->1882.242)! Learning rate decreased to 0.00154.
2024-12-02-07:43:55-root-INFO: Loss too large (1730.299->1851.334)! Learning rate decreased to 0.00123.
2024-12-02-07:43:55-root-INFO: Loss too large (1730.299->1818.433)! Learning rate decreased to 0.00099.
2024-12-02-07:43:55-root-INFO: Loss too large (1730.299->1785.889)! Learning rate decreased to 0.00079.
2024-12-02-07:43:55-root-INFO: Loss too large (1730.299->1757.818)! Learning rate decreased to 0.00063.
2024-12-02-07:43:55-root-INFO: Loss too large (1730.299->1736.984)! Learning rate decreased to 0.00051.
2024-12-02-07:43:56-root-INFO: grad norm: 294.954 291.550 44.678
2024-12-02-07:43:56-root-INFO: grad norm: 239.576 236.347 39.200
2024-12-02-07:43:57-root-INFO: grad norm: 214.727 211.677 36.068
2024-12-02-07:43:57-root-INFO: grad norm: 191.070 188.271 32.585
2024-12-02-07:43:58-root-INFO: grad norm: 176.233 173.329 31.860
2024-12-02-07:43:58-root-INFO: grad norm: 162.115 159.540 28.779
2024-12-02-07:43:59-root-INFO: grad norm: 152.253 149.427 29.197
2024-12-02-07:43:59-root-INFO: Loss Change: 1730.299 -> 1692.154
2024-12-02-07:43:59-root-INFO: Regularization Change: 0.000 -> 0.102
2024-12-02-07:43:59-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-07:43:59-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:43:59-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-07:43:59-root-INFO: grad norm: 307.589 304.287 44.950
2024-12-02-07:43:59-root-INFO: Loss too large (1689.729->1915.124)! Learning rate decreased to 0.00314.
2024-12-02-07:43:59-root-INFO: Loss too large (1689.729->1894.498)! Learning rate decreased to 0.00251.
2024-12-02-07:44:00-root-INFO: Loss too large (1689.729->1869.835)! Learning rate decreased to 0.00201.
2024-12-02-07:44:00-root-INFO: Loss too large (1689.729->1841.543)! Learning rate decreased to 0.00161.
2024-12-02-07:44:00-root-INFO: Loss too large (1689.729->1810.293)! Learning rate decreased to 0.00129.
2024-12-02-07:44:00-root-INFO: Loss too large (1689.729->1777.123)! Learning rate decreased to 0.00103.
2024-12-02-07:44:00-root-INFO: Loss too large (1689.729->1745.266)! Learning rate decreased to 0.00082.
2024-12-02-07:44:00-root-INFO: Loss too large (1689.729->1718.748)! Learning rate decreased to 0.00066.
2024-12-02-07:44:00-root-INFO: Loss too large (1689.729->1699.545)! Learning rate decreased to 0.00053.
2024-12-02-07:44:01-root-INFO: grad norm: 291.823 288.866 41.439
2024-12-02-07:44:01-root-INFO: grad norm: 276.000 273.141 39.618
2024-12-02-07:44:02-root-INFO: grad norm: 267.604 264.768 38.856
2024-12-02-07:44:02-root-INFO: grad norm: 259.196 256.576 36.758
2024-12-02-07:44:03-root-INFO: grad norm: 254.406 251.636 37.441
2024-12-02-07:44:03-root-INFO: grad norm: 249.785 247.310 35.075
2024-12-02-07:44:04-root-INFO: grad norm: 247.210 244.479 36.641
2024-12-02-07:44:04-root-INFO: Loss Change: 1689.729 -> 1664.512
2024-12-02-07:44:04-root-INFO: Regularization Change: 0.000 -> 0.088
2024-12-02-07:44:04-root-INFO: Undo step: 185
2024-12-02-07:44:04-root-INFO: Undo step: 186
2024-12-02-07:44:04-root-INFO: Undo step: 187
2024-12-02-07:44:04-root-INFO: Undo step: 188
2024-12-02-07:44:04-root-INFO: Undo step: 189
2024-12-02-07:44:04-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-07:44:04-root-INFO: grad norm: 796.122 719.249 341.308
2024-12-02-07:44:05-root-INFO: grad norm: 695.598 671.656 180.929
2024-12-02-07:44:05-root-INFO: grad norm: 431.782 406.067 146.785
2024-12-02-07:44:06-root-INFO: grad norm: 490.422 473.034 129.434
2024-12-02-07:44:06-root-INFO: grad norm: 623.066 601.438 162.740
2024-12-02-07:44:06-root-INFO: Loss too large (2371.013->2427.020)! Learning rate decreased to 0.00254.
2024-12-02-07:44:07-root-INFO: grad norm: 467.625 451.981 119.944
2024-12-02-07:44:07-root-INFO: grad norm: 267.285 255.213 79.418
2024-12-02-07:44:08-root-INFO: grad norm: 250.853 241.425 68.124
2024-12-02-07:44:08-root-INFO: Loss Change: 3229.278 -> 1984.494
2024-12-02-07:44:08-root-INFO: Regularization Change: 0.000 -> 13.958
2024-12-02-07:44:08-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-07:44:08-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:44:08-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-07:44:08-root-INFO: grad norm: 274.129 266.783 63.035
2024-12-02-07:44:09-root-INFO: Loss too large (1952.572->2220.363)! Learning rate decreased to 0.00265.
2024-12-02-07:44:09-root-INFO: Loss too large (1952.572->2082.476)! Learning rate decreased to 0.00212.
2024-12-02-07:44:09-root-INFO: Loss too large (1952.572->1996.446)! Learning rate decreased to 0.00170.
2024-12-02-07:44:09-root-INFO: grad norm: 503.560 497.273 79.320
2024-12-02-07:44:10-root-INFO: Loss too large (1948.226->2030.020)! Learning rate decreased to 0.00136.
2024-12-02-07:44:10-root-INFO: Loss too large (1948.226->1992.728)! Learning rate decreased to 0.00109.
2024-12-02-07:44:10-root-INFO: Loss too large (1948.226->1959.999)! Learning rate decreased to 0.00087.
2024-12-02-07:44:10-root-INFO: grad norm: 281.612 277.483 48.046
2024-12-02-07:44:11-root-INFO: grad norm: 109.448 104.429 32.764
2024-12-02-07:44:11-root-INFO: grad norm: 103.751 98.587 32.324
2024-12-02-07:44:12-root-INFO: grad norm: 100.111 95.112 31.240
2024-12-02-07:44:12-root-INFO: grad norm: 97.904 92.819 31.143
2024-12-02-07:44:13-root-INFO: grad norm: 97.111 92.287 30.229
2024-12-02-07:44:13-root-INFO: Loss Change: 1952.572 -> 1843.327
2024-12-02-07:44:13-root-INFO: Regularization Change: 0.000 -> 0.646
2024-12-02-07:44:13-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-07:44:13-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:44:13-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-07:44:13-root-INFO: grad norm: 293.748 289.244 51.243
2024-12-02-07:44:13-root-INFO: Loss too large (1837.700->2014.453)! Learning rate decreased to 0.00277.
2024-12-02-07:44:14-root-INFO: Loss too large (1837.700->1979.379)! Learning rate decreased to 0.00222.
2024-12-02-07:44:14-root-INFO: Loss too large (1837.700->1945.678)! Learning rate decreased to 0.00177.
2024-12-02-07:44:14-root-INFO: Loss too large (1837.700->1914.390)! Learning rate decreased to 0.00142.
2024-12-02-07:44:14-root-INFO: Loss too large (1837.700->1886.647)! Learning rate decreased to 0.00113.
2024-12-02-07:44:14-root-INFO: Loss too large (1837.700->1863.666)! Learning rate decreased to 0.00091.
2024-12-02-07:44:14-root-INFO: Loss too large (1837.700->1846.262)! Learning rate decreased to 0.00073.
2024-12-02-07:44:15-root-INFO: grad norm: 257.455 253.919 42.522
2024-12-02-07:44:15-root-INFO: grad norm: 218.207 214.485 40.133
2024-12-02-07:44:16-root-INFO: grad norm: 220.000 216.689 38.023
2024-12-02-07:44:16-root-INFO: grad norm: 232.365 228.889 40.041
2024-12-02-07:44:17-root-INFO: grad norm: 242.542 239.265 39.736
2024-12-02-07:44:17-root-INFO: grad norm: 268.136 264.753 42.462
2024-12-02-07:44:17-root-INFO: Loss too large (1801.333->1804.461)! Learning rate decreased to 0.00058.
2024-12-02-07:44:18-root-INFO: grad norm: 217.403 214.224 37.042
2024-12-02-07:44:18-root-INFO: Loss Change: 1837.700 -> 1787.570
2024-12-02-07:44:18-root-INFO: Regularization Change: 0.000 -> 0.221
2024-12-02-07:44:18-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-07:44:18-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-07:44:18-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-07:44:18-root-INFO: grad norm: 435.526 430.159 68.162
2024-12-02-07:44:19-root-INFO: Loss too large (1803.638->2028.994)! Learning rate decreased to 0.00289.
2024-12-02-07:44:19-root-INFO: Loss too large (1803.638->2004.757)! Learning rate decreased to 0.00231.
2024-12-02-07:44:19-root-INFO: Loss too large (1803.638->1976.484)! Learning rate decreased to 0.00185.
2024-12-02-07:44:19-root-INFO: Loss too large (1803.638->1944.821)! Learning rate decreased to 0.00148.
2024-12-02-07:44:19-root-INFO: Loss too large (1803.638->1911.391)! Learning rate decreased to 0.00118.
2024-12-02-07:44:19-root-INFO: Loss too large (1803.638->1877.931)! Learning rate decreased to 0.00095.
2024-12-02-07:44:20-root-INFO: Loss too large (1803.638->1846.543)! Learning rate decreased to 0.00076.
2024-12-02-07:44:20-root-INFO: Loss too large (1803.638->1819.736)! Learning rate decreased to 0.00061.
2024-12-02-07:44:20-root-INFO: grad norm: 321.846 318.377 47.128
2024-12-02-07:44:21-root-INFO: grad norm: 188.525 184.354 39.435
2024-12-02-07:44:21-root-INFO: grad norm: 182.287 179.269 33.032
2024-12-02-07:44:22-root-INFO: grad norm: 179.294 175.615 36.134
2024-12-02-07:44:22-root-INFO: grad norm: 181.082 178.221 32.061
2024-12-02-07:44:22-root-INFO: grad norm: 187.622 184.277 35.270
2024-12-02-07:44:23-root-INFO: grad norm: 194.842 192.078 32.703
2024-12-02-07:44:23-root-INFO: Loss Change: 1803.638 -> 1749.764
2024-12-02-07:44:23-root-INFO: Regularization Change: 0.000 -> 0.165
2024-12-02-07:44:23-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-07:44:23-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:44:23-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-07:44:24-root-INFO: grad norm: 478.961 474.383 66.066
2024-12-02-07:44:24-root-INFO: Loss too large (1756.276->1997.489)! Learning rate decreased to 0.00301.
2024-12-02-07:44:24-root-INFO: Loss too large (1756.276->1975.348)! Learning rate decreased to 0.00241.
2024-12-02-07:44:24-root-INFO: Loss too large (1756.276->1950.014)! Learning rate decreased to 0.00193.
2024-12-02-07:44:24-root-INFO: Loss too large (1756.276->1920.600)! Learning rate decreased to 0.00154.
2024-12-02-07:44:24-root-INFO: Loss too large (1756.276->1887.995)! Learning rate decreased to 0.00123.
2024-12-02-07:44:25-root-INFO: Loss too large (1756.276->1853.673)! Learning rate decreased to 0.00099.
2024-12-02-07:44:25-root-INFO: Loss too large (1756.276->1819.328)! Learning rate decreased to 0.00079.
2024-12-02-07:44:25-root-INFO: Loss too large (1756.276->1787.452)! Learning rate decreased to 0.00063.
2024-12-02-07:44:25-root-INFO: Loss too large (1756.276->1761.017)! Learning rate decreased to 0.00051.
2024-12-02-07:44:25-root-INFO: grad norm: 309.956 306.688 44.895
2024-12-02-07:44:26-root-INFO: grad norm: 164.189 160.841 32.988
2024-12-02-07:44:26-root-INFO: grad norm: 140.916 137.882 29.083
2024-12-02-07:44:27-root-INFO: grad norm: 121.638 118.254 28.492
2024-12-02-07:44:27-root-INFO: grad norm: 111.165 107.928 26.629
2024-12-02-07:44:28-root-INFO: grad norm: 102.652 99.203 26.386
2024-12-02-07:44:28-root-INFO: grad norm: 97.321 93.953 25.380
2024-12-02-07:44:29-root-INFO: Loss Change: 1756.276 -> 1701.589
2024-12-02-07:44:29-root-INFO: Regularization Change: 0.000 -> 0.113
2024-12-02-07:44:29-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-07:44:29-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:44:29-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-07:44:29-root-INFO: grad norm: 232.714 229.637 37.718
2024-12-02-07:44:29-root-INFO: Loss too large (1693.168->1888.058)! Learning rate decreased to 0.00314.
2024-12-02-07:44:29-root-INFO: Loss too large (1693.168->1860.209)! Learning rate decreased to 0.00251.
2024-12-02-07:44:30-root-INFO: Loss too large (1693.168->1830.373)! Learning rate decreased to 0.00201.
2024-12-02-07:44:30-root-INFO: Loss too large (1693.168->1799.904)! Learning rate decreased to 0.00161.
2024-12-02-07:44:30-root-INFO: Loss too large (1693.168->1770.516)! Learning rate decreased to 0.00129.
2024-12-02-07:44:30-root-INFO: Loss too large (1693.168->1744.321)! Learning rate decreased to 0.00103.
2024-12-02-07:44:30-root-INFO: Loss too large (1693.168->1723.062)! Learning rate decreased to 0.00082.
2024-12-02-07:44:30-root-INFO: Loss too large (1693.168->1707.417)! Learning rate decreased to 0.00066.
2024-12-02-07:44:31-root-INFO: Loss too large (1693.168->1696.966)! Learning rate decreased to 0.00053.
2024-12-02-07:44:31-root-INFO: grad norm: 221.335 218.697 34.074
2024-12-02-07:44:32-root-INFO: grad norm: 210.714 207.920 34.204
2024-12-02-07:44:32-root-INFO: grad norm: 206.958 204.410 32.375
2024-12-02-07:44:33-root-INFO: grad norm: 204.714 202.075 32.763
2024-12-02-07:44:33-root-INFO: grad norm: 205.286 202.785 31.947
2024-12-02-07:44:34-root-INFO: grad norm: 208.029 205.487 32.424
2024-12-02-07:44:34-root-INFO: grad norm: 211.574 209.094 32.300
2024-12-02-07:44:34-root-INFO: Loss Change: 1693.168 -> 1669.552
2024-12-02-07:44:34-root-INFO: Regularization Change: 0.000 -> 0.095
2024-12-02-07:44:34-root-INFO: Undo step: 185
2024-12-02-07:44:34-root-INFO: Undo step: 186
2024-12-02-07:44:34-root-INFO: Undo step: 187
2024-12-02-07:44:34-root-INFO: Undo step: 188
2024-12-02-07:44:34-root-INFO: Undo step: 189
2024-12-02-07:44:34-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-07:44:35-root-INFO: grad norm: 690.159 636.711 266.306
2024-12-02-07:44:35-root-INFO: grad norm: 603.538 586.108 144.000
2024-12-02-07:44:35-root-INFO: Loss too large (2605.775->3050.729)! Learning rate decreased to 0.00254.
2024-12-02-07:44:35-root-INFO: Loss too large (2605.775->2766.391)! Learning rate decreased to 0.00203.
2024-12-02-07:44:36-root-INFO: grad norm: 838.823 812.187 209.702
2024-12-02-07:44:36-root-INFO: Loss too large (2578.448->2706.028)! Learning rate decreased to 0.00163.
2024-12-02-07:44:37-root-INFO: grad norm: 678.705 656.865 170.788
2024-12-02-07:44:37-root-INFO: grad norm: 466.208 448.134 128.553
2024-12-02-07:44:38-root-INFO: grad norm: 417.972 404.872 103.823
2024-12-02-07:44:38-root-INFO: grad norm: 362.484 348.481 99.778
2024-12-02-07:44:38-root-INFO: grad norm: 334.739 324.706 81.340
2024-12-02-07:44:39-root-INFO: Loss Change: 3058.545 -> 2049.575
2024-12-02-07:44:39-root-INFO: Regularization Change: 0.000 -> 10.492
2024-12-02-07:44:39-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-07:44:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:44:39-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-07:44:39-root-INFO: grad norm: 318.956 305.253 92.485
2024-12-02-07:44:39-root-INFO: Loss too large (2031.475->2218.380)! Learning rate decreased to 0.00265.
2024-12-02-07:44:39-root-INFO: Loss too large (2031.475->2110.804)! Learning rate decreased to 0.00212.
2024-12-02-07:44:40-root-INFO: Loss too large (2031.475->2044.580)! Learning rate decreased to 0.00170.
2024-12-02-07:44:40-root-INFO: grad norm: 316.915 308.481 72.624
2024-12-02-07:44:41-root-INFO: grad norm: 303.466 292.078 82.353
2024-12-02-07:44:41-root-INFO: grad norm: 294.884 287.057 67.487
2024-12-02-07:44:41-root-INFO: grad norm: 290.413 280.289 76.015
2024-12-02-07:44:42-root-INFO: grad norm: 297.959 290.600 65.811
2024-12-02-07:44:42-root-INFO: grad norm: 309.861 300.827 74.276
2024-12-02-07:44:43-root-INFO: grad norm: 397.204 390.982 70.029
2024-12-02-07:44:43-root-INFO: Loss too large (1880.114->1913.183)! Learning rate decreased to 0.00136.
2024-12-02-07:44:43-root-INFO: Loss Change: 2031.475 -> 1874.841
2024-12-02-07:44:43-root-INFO: Regularization Change: 0.000 -> 2.100
2024-12-02-07:44:43-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-07:44:43-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-07:44:44-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-07:44:44-root-INFO: grad norm: 322.999 315.543 68.997
2024-12-02-07:44:44-root-INFO: Loss too large (1845.008->2294.369)! Learning rate decreased to 0.00277.
2024-12-02-07:44:44-root-INFO: Loss too large (1845.008->2093.463)! Learning rate decreased to 0.00222.
2024-12-02-07:44:44-root-INFO: Loss too large (1845.008->1950.704)! Learning rate decreased to 0.00177.
2024-12-02-07:44:44-root-INFO: Loss too large (1845.008->1860.314)! Learning rate decreased to 0.00142.
2024-12-02-07:44:45-root-INFO: grad norm: 374.745 370.471 56.436
2024-12-02-07:44:45-root-INFO: Loss too large (1812.115->1846.833)! Learning rate decreased to 0.00113.
2024-12-02-07:44:45-root-INFO: Loss too large (1812.115->1820.420)! Learning rate decreased to 0.00091.
2024-12-02-07:44:46-root-INFO: grad norm: 251.505 248.745 37.153
2024-12-02-07:44:46-root-INFO: grad norm: 131.713 128.089 30.683
2024-12-02-07:44:47-root-INFO: grad norm: 139.059 135.783 30.008
2024-12-02-07:44:47-root-INFO: grad norm: 177.021 173.726 33.996
2024-12-02-07:44:47-root-INFO: grad norm: 215.376 212.583 34.575
2024-12-02-07:44:48-root-INFO: grad norm: 323.135 319.587 47.757
2024-12-02-07:44:48-root-INFO: Loss too large (1752.517->1770.429)! Learning rate decreased to 0.00073.
2024-12-02-07:44:48-root-INFO: Loss too large (1752.517->1755.395)! Learning rate decreased to 0.00058.
2024-12-02-07:44:49-root-INFO: Loss Change: 1845.008 -> 1745.381
2024-12-02-07:44:49-root-INFO: Regularization Change: 0.000 -> 0.447
2024-12-02-07:44:49-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-07:44:49-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-07:44:49-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-07:44:49-root-INFO: grad norm: 111.703 106.244 34.495
2024-12-02-07:44:49-root-INFO: Loss too large (1723.964->1775.526)! Learning rate decreased to 0.00289.
2024-12-02-07:44:49-root-INFO: Loss too large (1723.964->1749.423)! Learning rate decreased to 0.00231.
2024-12-02-07:44:49-root-INFO: Loss too large (1723.964->1733.698)! Learning rate decreased to 0.00185.
2024-12-02-07:44:49-root-INFO: Loss too large (1723.964->1724.905)! Learning rate decreased to 0.00148.
2024-12-02-07:44:50-root-INFO: grad norm: 271.103 266.945 47.299
2024-12-02-07:44:50-root-INFO: Loss too large (1720.423->1783.331)! Learning rate decreased to 0.00118.
2024-12-02-07:44:50-root-INFO: Loss too large (1720.423->1760.090)! Learning rate decreased to 0.00095.
2024-12-02-07:44:50-root-INFO: Loss too large (1720.423->1741.242)! Learning rate decreased to 0.00076.
2024-12-02-07:44:51-root-INFO: Loss too large (1720.423->1727.378)! Learning rate decreased to 0.00061.
2024-12-02-07:44:51-root-INFO: grad norm: 238.460 235.865 35.079
2024-12-02-07:44:52-root-INFO: grad norm: 200.719 197.159 37.633
2024-12-02-07:44:52-root-INFO: grad norm: 195.005 192.408 31.717
2024-12-02-07:44:53-root-INFO: grad norm: 191.860 188.577 35.342
2024-12-02-07:44:53-root-INFO: grad norm: 194.045 191.490 31.382
2024-12-02-07:44:54-root-INFO: grad norm: 201.159 198.044 35.267
2024-12-02-07:44:54-root-INFO: Loss Change: 1723.964 -> 1693.652
2024-12-02-07:44:54-root-INFO: Regularization Change: 0.000 -> 0.205
2024-12-02-07:44:54-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-07:44:54-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:44:54-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-07:44:55-root-INFO: grad norm: 86.390 80.533 31.268
2024-12-02-07:44:55-root-INFO: grad norm: 93.356 89.993 24.832
2024-12-02-07:44:55-root-INFO: grad norm: 203.816 197.475 50.446
2024-12-02-07:44:56-root-INFO: Loss too large (1645.766->1954.774)! Learning rate decreased to 0.00301.
2024-12-02-07:44:56-root-INFO: Loss too large (1645.766->1848.274)! Learning rate decreased to 0.00241.
2024-12-02-07:44:56-root-INFO: Loss too large (1645.766->1770.444)! Learning rate decreased to 0.00193.
2024-12-02-07:44:56-root-INFO: Loss too large (1645.766->1716.295)! Learning rate decreased to 0.00154.
2024-12-02-07:44:56-root-INFO: Loss too large (1645.766->1680.667)! Learning rate decreased to 0.00123.
2024-12-02-07:44:57-root-INFO: Loss too large (1645.766->1658.611)! Learning rate decreased to 0.00099.
2024-12-02-07:44:57-root-INFO: Loss too large (1645.766->1645.878)! Learning rate decreased to 0.00079.
2024-12-02-07:44:57-root-INFO: grad norm: 297.960 294.524 45.115
2024-12-02-07:44:57-root-INFO: Loss too large (1639.184->1665.976)! Learning rate decreased to 0.00063.
2024-12-02-07:44:58-root-INFO: Loss too large (1639.184->1649.909)! Learning rate decreased to 0.00051.
2024-12-02-07:44:58-root-INFO: grad norm: 278.532 275.892 38.258
2024-12-02-07:44:59-root-INFO: grad norm: 256.147 253.357 37.706
2024-12-02-07:44:59-root-INFO: Loss too large (1632.284->1632.386)! Learning rate decreased to 0.00040.
2024-12-02-07:44:59-root-INFO: grad norm: 177.527 175.194 28.688
2024-12-02-07:45:00-root-INFO: grad norm: 118.705 116.335 23.604
2024-12-02-07:45:00-root-INFO: Loss Change: 1668.681 -> 1621.435
2024-12-02-07:45:00-root-INFO: Regularization Change: 0.000 -> 0.497
2024-12-02-07:45:00-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-07:45:00-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:45:00-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-07:45:00-root-INFO: grad norm: 133.793 130.530 29.368
2024-12-02-07:45:00-root-INFO: Loss too large (1607.748->1733.395)! Learning rate decreased to 0.00314.
2024-12-02-07:45:01-root-INFO: Loss too large (1607.748->1706.915)! Learning rate decreased to 0.00251.
2024-12-02-07:45:01-root-INFO: Loss too large (1607.748->1681.825)! Learning rate decreased to 0.00201.
2024-12-02-07:45:01-root-INFO: Loss too large (1607.748->1659.600)! Learning rate decreased to 0.00161.
2024-12-02-07:45:01-root-INFO: Loss too large (1607.748->1641.480)! Learning rate decreased to 0.00129.
2024-12-02-07:45:01-root-INFO: Loss too large (1607.748->1627.892)! Learning rate decreased to 0.00103.
2024-12-02-07:45:01-root-INFO: Loss too large (1607.748->1618.452)! Learning rate decreased to 0.00082.
2024-12-02-07:45:02-root-INFO: Loss too large (1607.748->1612.331)! Learning rate decreased to 0.00066.
2024-12-02-07:45:02-root-INFO: Loss too large (1607.748->1608.625)! Learning rate decreased to 0.00053.
2024-12-02-07:45:02-root-INFO: grad norm: 156.123 153.717 27.304
2024-12-02-07:45:03-root-INFO: grad norm: 191.722 189.008 32.145
2024-12-02-07:45:03-root-INFO: Loss too large (1605.562->1606.322)! Learning rate decreased to 0.00042.
2024-12-02-07:45:03-root-INFO: grad norm: 160.604 158.353 26.796
2024-12-02-07:45:04-root-INFO: grad norm: 133.113 130.630 25.588
2024-12-02-07:45:04-root-INFO: grad norm: 116.878 114.569 23.118
2024-12-02-07:45:05-root-INFO: grad norm: 102.843 100.369 22.424
2024-12-02-07:45:05-root-INFO: grad norm: 93.484 91.052 21.186
2024-12-02-07:45:06-root-INFO: Loss Change: 1607.748 -> 1593.626
2024-12-02-07:45:06-root-INFO: Regularization Change: 0.000 -> 0.046
2024-12-02-07:45:06-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-07:45:06-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:45:06-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-07:45:06-root-INFO: grad norm: 566.384 559.035 90.944
2024-12-02-07:45:06-root-INFO: Loss too large (1630.918->1885.795)! Learning rate decreased to 0.00328.
2024-12-02-07:45:06-root-INFO: Loss too large (1630.918->1863.250)! Learning rate decreased to 0.00262.
2024-12-02-07:45:07-root-INFO: Loss too large (1630.918->1842.620)! Learning rate decreased to 0.00210.
2024-12-02-07:45:07-root-INFO: Loss too large (1630.918->1821.116)! Learning rate decreased to 0.00168.
2024-12-02-07:45:07-root-INFO: Loss too large (1630.918->1796.770)! Learning rate decreased to 0.00134.
2024-12-02-07:45:07-root-INFO: Loss too large (1630.918->1769.113)! Learning rate decreased to 0.00107.
2024-12-02-07:45:07-root-INFO: Loss too large (1630.918->1738.606)! Learning rate decreased to 0.00086.
2024-12-02-07:45:07-root-INFO: Loss too large (1630.918->1706.056)! Learning rate decreased to 0.00069.
2024-12-02-07:45:07-root-INFO: Loss too large (1630.918->1673.013)! Learning rate decreased to 0.00055.
2024-12-02-07:45:08-root-INFO: Loss too large (1630.918->1642.597)! Learning rate decreased to 0.00044.
2024-12-02-07:45:08-root-INFO: grad norm: 406.004 402.572 52.679
2024-12-02-07:45:09-root-INFO: grad norm: 257.544 252.816 49.120
2024-12-02-07:45:09-root-INFO: grad norm: 245.943 243.042 37.663
2024-12-02-07:45:10-root-INFO: grad norm: 235.387 231.576 42.189
2024-12-02-07:45:10-root-INFO: grad norm: 230.265 227.753 33.920
2024-12-02-07:45:10-root-INFO: grad norm: 225.423 222.166 38.179
2024-12-02-07:45:11-root-INFO: grad norm: 222.968 220.673 31.911
2024-12-02-07:45:11-root-INFO: Loss Change: 1630.918 -> 1575.564
2024-12-02-07:45:11-root-INFO: Regularization Change: 0.000 -> 0.092
2024-12-02-07:45:11-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-07:45:11-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:45:11-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-07:45:12-root-INFO: grad norm: 503.676 498.406 72.668
2024-12-02-07:45:12-root-INFO: Loss too large (1595.150->1842.510)! Learning rate decreased to 0.00342.
2024-12-02-07:45:12-root-INFO: Loss too large (1595.150->1825.673)! Learning rate decreased to 0.00273.
2024-12-02-07:45:12-root-INFO: Loss too large (1595.150->1808.891)! Learning rate decreased to 0.00219.
2024-12-02-07:45:12-root-INFO: Loss too large (1595.150->1789.853)! Learning rate decreased to 0.00175.
2024-12-02-07:45:12-root-INFO: Loss too large (1595.150->1767.245)! Learning rate decreased to 0.00140.
2024-12-02-07:45:13-root-INFO: Loss too large (1595.150->1741.039)! Learning rate decreased to 0.00112.
2024-12-02-07:45:13-root-INFO: Loss too large (1595.150->1711.842)! Learning rate decreased to 0.00090.
2024-12-02-07:45:13-root-INFO: Loss too large (1595.150->1680.425)! Learning rate decreased to 0.00072.
2024-12-02-07:45:13-root-INFO: Loss too large (1595.150->1648.256)! Learning rate decreased to 0.00057.
2024-12-02-07:45:13-root-INFO: Loss too large (1595.150->1618.390)! Learning rate decreased to 0.00046.
2024-12-02-07:45:14-root-INFO: grad norm: 420.560 417.248 52.676
2024-12-02-07:45:14-root-INFO: grad norm: 333.850 330.183 49.349
2024-12-02-07:45:14-root-INFO: Loss too large (1572.632->1573.926)! Learning rate decreased to 0.00037.
2024-12-02-07:45:15-root-INFO: grad norm: 241.485 238.760 36.176
2024-12-02-07:45:15-root-INFO: grad norm: 171.327 168.483 31.084
2024-12-02-07:45:16-root-INFO: grad norm: 137.545 134.906 26.810
2024-12-02-07:45:16-root-INFO: grad norm: 111.240 108.466 24.691
2024-12-02-07:45:17-root-INFO: grad norm: 94.882 92.077 22.897
2024-12-02-07:45:17-root-INFO: Loss Change: 1595.150 -> 1551.872
2024-12-02-07:45:17-root-INFO: Regularization Change: 0.000 -> 0.061
2024-12-02-07:45:17-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-07:45:17-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:45:17-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-07:45:17-root-INFO: grad norm: 405.023 400.837 58.081
2024-12-02-07:45:18-root-INFO: Loss too large (1564.015->1812.940)! Learning rate decreased to 0.00356.
2024-12-02-07:45:18-root-INFO: Loss too large (1564.015->1797.893)! Learning rate decreased to 0.00285.
2024-12-02-07:45:18-root-INFO: Loss too large (1564.015->1780.990)! Learning rate decreased to 0.00228.
2024-12-02-07:45:18-root-INFO: Loss too large (1564.015->1760.585)! Learning rate decreased to 0.00182.
2024-12-02-07:45:18-root-INFO: Loss too large (1564.015->1736.281)! Learning rate decreased to 0.00146.
2024-12-02-07:45:18-root-INFO: Loss too large (1564.015->1708.547)! Learning rate decreased to 0.00117.
2024-12-02-07:45:19-root-INFO: Loss too large (1564.015->1678.126)! Learning rate decreased to 0.00093.
2024-12-02-07:45:19-root-INFO: Loss too large (1564.015->1646.180)! Learning rate decreased to 0.00075.
2024-12-02-07:45:19-root-INFO: Loss too large (1564.015->1615.099)! Learning rate decreased to 0.00060.
2024-12-02-07:45:19-root-INFO: Loss too large (1564.015->1588.297)! Learning rate decreased to 0.00048.
2024-12-02-07:45:19-root-INFO: Loss too large (1564.015->1568.318)! Learning rate decreased to 0.00038.
2024-12-02-07:45:20-root-INFO: grad norm: 307.509 304.576 42.370
2024-12-02-07:45:20-root-INFO: grad norm: 233.189 230.199 37.220
2024-12-02-07:45:21-root-INFO: grad norm: 197.908 195.395 31.439
2024-12-02-07:45:21-root-INFO: grad norm: 167.386 164.831 29.136
2024-12-02-07:45:22-root-INFO: grad norm: 146.875 144.538 26.097
2024-12-02-07:45:22-root-INFO: grad norm: 128.923 126.577 24.480
2024-12-02-07:45:23-root-INFO: grad norm: 115.643 113.378 22.780
2024-12-02-07:45:23-root-INFO: Loss Change: 1564.015 -> 1533.084
2024-12-02-07:45:23-root-INFO: Regularization Change: 0.000 -> 0.050
2024-12-02-07:45:23-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-07:45:23-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-07:45:23-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-07:45:23-root-INFO: grad norm: 456.275 451.790 63.817
2024-12-02-07:45:23-root-INFO: Loss too large (1552.502->1811.078)! Learning rate decreased to 0.00371.
2024-12-02-07:45:24-root-INFO: Loss too large (1552.502->1797.316)! Learning rate decreased to 0.00297.
2024-12-02-07:45:24-root-INFO: Loss too large (1552.502->1782.738)! Learning rate decreased to 0.00238.
2024-12-02-07:45:24-root-INFO: Loss too large (1552.502->1765.312)! Learning rate decreased to 0.00190.
2024-12-02-07:45:24-root-INFO: Loss too large (1552.502->1743.706)! Learning rate decreased to 0.00152.
2024-12-02-07:45:24-root-INFO: Loss too large (1552.502->1717.728)! Learning rate decreased to 0.00122.
2024-12-02-07:45:24-root-INFO: Loss too large (1552.502->1687.856)! Learning rate decreased to 0.00097.
2024-12-02-07:45:25-root-INFO: Loss too large (1552.502->1654.824)! Learning rate decreased to 0.00078.
2024-12-02-07:45:25-root-INFO: Loss too large (1552.502->1620.306)! Learning rate decreased to 0.00062.
2024-12-02-07:45:25-root-INFO: Loss too large (1552.502->1587.829)! Learning rate decreased to 0.00050.
2024-12-02-07:45:25-root-INFO: Loss too large (1552.502->1561.555)! Learning rate decreased to 0.00040.
2024-12-02-07:45:25-root-INFO: grad norm: 366.591 363.643 46.397
2024-12-02-07:45:26-root-INFO: grad norm: 301.509 298.274 44.049
2024-12-02-07:45:26-root-INFO: grad norm: 273.402 270.904 36.871
2024-12-02-07:45:27-root-INFO: grad norm: 248.253 245.521 36.732
2024-12-02-07:45:27-root-INFO: grad norm: 231.765 229.512 32.238
2024-12-02-07:45:28-root-INFO: grad norm: 216.328 213.913 32.238
2024-12-02-07:45:28-root-INFO: grad norm: 204.919 202.827 29.203
2024-12-02-07:45:29-root-INFO: Loss Change: 1552.502 -> 1518.306
2024-12-02-07:45:29-root-INFO: Regularization Change: 0.000 -> 0.052
2024-12-02-07:45:29-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-07:45:29-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:45:29-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-07:45:29-root-INFO: grad norm: 537.137 531.513 77.524
2024-12-02-07:45:29-root-INFO: Loss too large (1555.251->1816.138)! Learning rate decreased to 0.00387.
2024-12-02-07:45:29-root-INFO: Loss too large (1555.251->1800.374)! Learning rate decreased to 0.00309.
2024-12-02-07:45:30-root-INFO: Loss too large (1555.251->1786.002)! Learning rate decreased to 0.00248.
2024-12-02-07:45:30-root-INFO: Loss too large (1555.251->1770.901)! Learning rate decreased to 0.00198.
2024-12-02-07:45:30-root-INFO: Loss too large (1555.251->1752.841)! Learning rate decreased to 0.00158.
2024-12-02-07:45:30-root-INFO: Loss too large (1555.251->1730.379)! Learning rate decreased to 0.00127.
2024-12-02-07:45:30-root-INFO: Loss too large (1555.251->1703.195)! Learning rate decreased to 0.00101.
2024-12-02-07:45:30-root-INFO: Loss too large (1555.251->1671.584)! Learning rate decreased to 0.00081.
2024-12-02-07:45:30-root-INFO: Loss too large (1555.251->1636.276)! Learning rate decreased to 0.00065.
2024-12-02-07:45:31-root-INFO: Loss too large (1555.251->1599.660)! Learning rate decreased to 0.00052.
2024-12-02-07:45:31-root-INFO: Loss too large (1555.251->1566.631)! Learning rate decreased to 0.00042.
2024-12-02-07:45:31-root-INFO: grad norm: 433.775 430.171 55.804
2024-12-02-07:45:32-root-INFO: grad norm: 367.084 362.982 54.719
2024-12-02-07:45:32-root-INFO: grad norm: 347.641 344.592 45.938
2024-12-02-07:45:33-root-INFO: grad norm: 329.479 325.958 48.037
2024-12-02-07:45:33-root-INFO: grad norm: 318.523 315.789 41.643
2024-12-02-07:45:34-root-INFO: grad norm: 307.362 304.215 43.873
2024-12-02-07:45:34-root-INFO: grad norm: 299.324 296.801 38.785
2024-12-02-07:45:34-root-INFO: Loss Change: 1555.251 -> 1508.411
2024-12-02-07:45:34-root-INFO: Regularization Change: 0.000 -> 0.078
2024-12-02-07:45:34-root-INFO: Undo step: 180
2024-12-02-07:45:34-root-INFO: Undo step: 181
2024-12-02-07:45:34-root-INFO: Undo step: 182
2024-12-02-07:45:34-root-INFO: Undo step: 183
2024-12-02-07:45:34-root-INFO: Undo step: 184
2024-12-02-07:45:35-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-07:45:35-root-INFO: grad norm: 767.599 742.033 196.459
2024-12-02-07:45:35-root-INFO: grad norm: 585.825 569.267 138.295
2024-12-02-07:45:36-root-INFO: grad norm: 466.926 455.096 104.441
2024-12-02-07:45:36-root-INFO: grad norm: 680.740 670.063 120.094
2024-12-02-07:45:36-root-INFO: Loss too large (2089.269->2151.039)! Learning rate decreased to 0.00314.
2024-12-02-07:45:37-root-INFO: grad norm: 241.903 232.963 65.155
2024-12-02-07:45:37-root-INFO: grad norm: 159.416 151.815 48.637
2024-12-02-07:45:38-root-INFO: grad norm: 152.980 147.956 38.882
2024-12-02-07:45:38-root-INFO: grad norm: 176.340 171.307 41.830
2024-12-02-07:45:39-root-INFO: Loss Change: 3146.142 -> 1756.845
2024-12-02-07:45:39-root-INFO: Regularization Change: 0.000 -> 16.568
2024-12-02-07:45:39-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-07:45:39-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:45:39-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-07:45:39-root-INFO: grad norm: 240.271 230.715 67.088
2024-12-02-07:45:39-root-INFO: grad norm: 172.293 167.448 40.573
2024-12-02-07:45:40-root-INFO: grad norm: 163.715 158.738 40.061
2024-12-02-07:45:40-root-INFO: Loss too large (1655.462->1839.173)! Learning rate decreased to 0.00328.
2024-12-02-07:45:40-root-INFO: Loss too large (1655.462->1762.458)! Learning rate decreased to 0.00262.
2024-12-02-07:45:40-root-INFO: Loss too large (1655.462->1710.490)! Learning rate decreased to 0.00210.
2024-12-02-07:45:41-root-INFO: Loss too large (1655.462->1678.027)! Learning rate decreased to 0.00168.
2024-12-02-07:45:41-root-INFO: Loss too large (1655.462->1659.386)! Learning rate decreased to 0.00134.
2024-12-02-07:45:41-root-INFO: grad norm: 303.934 300.095 48.153
2024-12-02-07:45:41-root-INFO: Loss too large (1649.673->1701.403)! Learning rate decreased to 0.00107.
2024-12-02-07:45:42-root-INFO: Loss too large (1649.673->1677.983)! Learning rate decreased to 0.00086.
2024-12-02-07:45:42-root-INFO: Loss too large (1649.673->1659.880)! Learning rate decreased to 0.00069.
2024-12-02-07:45:42-root-INFO: grad norm: 243.685 240.555 38.929
2024-12-02-07:45:43-root-INFO: grad norm: 174.035 170.484 34.975
2024-12-02-07:45:43-root-INFO: grad norm: 167.820 164.769 31.856
2024-12-02-07:45:44-root-INFO: grad norm: 165.783 162.303 33.789
2024-12-02-07:45:44-root-INFO: Loss Change: 1765.851 -> 1624.814
2024-12-02-07:45:44-root-INFO: Regularization Change: 0.000 -> 1.320
2024-12-02-07:45:44-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-07:45:44-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:45:44-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-07:45:44-root-INFO: grad norm: 86.931 81.131 31.222
2024-12-02-07:45:45-root-INFO: grad norm: 256.641 253.876 37.574
2024-12-02-07:45:45-root-INFO: Loss too large (1599.122->1789.184)! Learning rate decreased to 0.00342.
2024-12-02-07:45:45-root-INFO: Loss too large (1599.122->1766.765)! Learning rate decreased to 0.00273.
2024-12-02-07:45:45-root-INFO: Loss too large (1599.122->1741.935)! Learning rate decreased to 0.00219.
2024-12-02-07:45:45-root-INFO: Loss too large (1599.122->1715.487)! Learning rate decreased to 0.00175.
2024-12-02-07:45:46-root-INFO: Loss too large (1599.122->1688.358)! Learning rate decreased to 0.00140.
2024-12-02-07:45:46-root-INFO: Loss too large (1599.122->1662.038)! Learning rate decreased to 0.00112.
2024-12-02-07:45:46-root-INFO: Loss too large (1599.122->1638.657)! Learning rate decreased to 0.00090.
2024-12-02-07:45:46-root-INFO: Loss too large (1599.122->1619.966)! Learning rate decreased to 0.00072.
2024-12-02-07:45:46-root-INFO: Loss too large (1599.122->1606.539)! Learning rate decreased to 0.00057.
2024-12-02-07:45:47-root-INFO: grad norm: 240.338 237.149 39.020
2024-12-02-07:45:47-root-INFO: grad norm: 222.076 219.382 34.487
2024-12-02-07:45:48-root-INFO: grad norm: 216.335 213.316 36.017
2024-12-02-07:45:48-root-INFO: grad norm: 211.159 208.459 33.657
2024-12-02-07:45:49-root-INFO: grad norm: 210.350 207.430 34.927
2024-12-02-07:45:49-root-INFO: grad norm: 211.367 208.651 33.776
2024-12-02-07:45:49-root-INFO: Loss Change: 1611.463 -> 1581.957
2024-12-02-07:45:49-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-02-07:45:49-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-07:45:49-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:45:50-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-07:45:50-root-INFO: grad norm: 70.677 65.919 25.494
2024-12-02-07:45:50-root-INFO: grad norm: 324.435 321.447 43.927
2024-12-02-07:45:50-root-INFO: Loss too large (1563.202->1789.264)! Learning rate decreased to 0.00356.
2024-12-02-07:45:51-root-INFO: Loss too large (1563.202->1771.375)! Learning rate decreased to 0.00285.
2024-12-02-07:45:51-root-INFO: Loss too large (1563.202->1750.347)! Learning rate decreased to 0.00228.
2024-12-02-07:45:51-root-INFO: Loss too large (1563.202->1725.907)! Learning rate decreased to 0.00182.
2024-12-02-07:45:51-root-INFO: Loss too large (1563.202->1698.624)! Learning rate decreased to 0.00146.
2024-12-02-07:45:51-root-INFO: Loss too large (1563.202->1669.278)! Learning rate decreased to 0.00117.
2024-12-02-07:45:51-root-INFO: Loss too large (1563.202->1639.268)! Learning rate decreased to 0.00093.
2024-12-02-07:45:51-root-INFO: Loss too large (1563.202->1611.214)! Learning rate decreased to 0.00075.
2024-12-02-07:45:52-root-INFO: Loss too large (1563.202->1587.903)! Learning rate decreased to 0.00060.
2024-12-02-07:45:52-root-INFO: Loss too large (1563.202->1570.789)! Learning rate decreased to 0.00048.
2024-12-02-07:45:52-root-INFO: grad norm: 266.396 263.553 38.812
2024-12-02-07:45:53-root-INFO: grad norm: 210.353 207.853 32.333
2024-12-02-07:45:53-root-INFO: grad norm: 187.982 185.385 31.143
2024-12-02-07:45:54-root-INFO: grad norm: 166.092 163.656 28.342
2024-12-02-07:45:54-root-INFO: grad norm: 153.447 150.879 27.956
2024-12-02-07:45:55-root-INFO: grad norm: 141.253 138.785 26.291
2024-12-02-07:45:55-root-INFO: Loss Change: 1567.101 -> 1541.791
2024-12-02-07:45:55-root-INFO: Regularization Change: 0.000 -> 0.214
2024-12-02-07:45:55-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-07:45:55-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-07:45:55-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-07:45:55-root-INFO: grad norm: 189.964 186.996 33.450
2024-12-02-07:45:55-root-INFO: Loss too large (1537.269->1734.616)! Learning rate decreased to 0.00371.
2024-12-02-07:45:55-root-INFO: Loss too large (1537.269->1709.948)! Learning rate decreased to 0.00297.
2024-12-02-07:45:56-root-INFO: Loss too large (1537.269->1683.034)! Learning rate decreased to 0.00238.
2024-12-02-07:45:56-root-INFO: Loss too large (1537.269->1654.606)! Learning rate decreased to 0.00190.
2024-12-02-07:45:56-root-INFO: Loss too large (1537.269->1625.992)! Learning rate decreased to 0.00152.
2024-12-02-07:45:56-root-INFO: Loss too large (1537.269->1599.430)! Learning rate decreased to 0.00122.
2024-12-02-07:45:56-root-INFO: Loss too large (1537.269->1577.098)! Learning rate decreased to 0.00097.
2024-12-02-07:45:56-root-INFO: Loss too large (1537.269->1560.073)! Learning rate decreased to 0.00078.
2024-12-02-07:45:57-root-INFO: Loss too large (1537.269->1548.214)! Learning rate decreased to 0.00062.
2024-12-02-07:45:57-root-INFO: Loss too large (1537.269->1540.618)! Learning rate decreased to 0.00050.
2024-12-02-07:45:57-root-INFO: grad norm: 198.694 196.217 31.275
2024-12-02-07:45:58-root-INFO: grad norm: 210.802 208.071 33.824
2024-12-02-07:45:58-root-INFO: grad norm: 220.357 217.894 32.858
2024-12-02-07:45:59-root-INFO: grad norm: 232.328 229.669 35.055
2024-12-02-07:45:59-root-INFO: Loss too large (1531.836->1531.924)! Learning rate decreased to 0.00040.
2024-12-02-07:45:59-root-INFO: grad norm: 165.834 163.457 27.979
2024-12-02-07:46:00-root-INFO: grad norm: 117.871 115.262 24.665
2024-12-02-07:46:00-root-INFO: grad norm: 93.102 90.293 22.698
2024-12-02-07:46:01-root-INFO: Loss Change: 1537.269 -> 1521.343
2024-12-02-07:46:01-root-INFO: Regularization Change: 0.000 -> 0.046
2024-12-02-07:46:01-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-07:46:01-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:46:01-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-07:46:01-root-INFO: grad norm: 396.112 391.403 60.896
2024-12-02-07:46:01-root-INFO: Loss too large (1541.633->1791.872)! Learning rate decreased to 0.00387.
2024-12-02-07:46:01-root-INFO: Loss too large (1541.633->1773.900)! Learning rate decreased to 0.00309.
2024-12-02-07:46:01-root-INFO: Loss too large (1541.633->1755.372)! Learning rate decreased to 0.00248.
2024-12-02-07:46:02-root-INFO: Loss too large (1541.633->1734.413)! Learning rate decreased to 0.00198.
2024-12-02-07:46:02-root-INFO: Loss too large (1541.633->1710.184)! Learning rate decreased to 0.00158.
2024-12-02-07:46:02-root-INFO: Loss too large (1541.633->1682.766)! Learning rate decreased to 0.00127.
2024-12-02-07:46:02-root-INFO: Loss too large (1541.633->1652.542)! Learning rate decreased to 0.00101.
2024-12-02-07:46:02-root-INFO: Loss too large (1541.633->1620.589)! Learning rate decreased to 0.00081.
2024-12-02-07:46:02-root-INFO: Loss too large (1541.633->1589.693)! Learning rate decreased to 0.00065.
2024-12-02-07:46:03-root-INFO: Loss too large (1541.633->1563.428)! Learning rate decreased to 0.00052.
2024-12-02-07:46:03-root-INFO: Loss too large (1541.633->1544.058)! Learning rate decreased to 0.00042.
2024-12-02-07:46:03-root-INFO: grad norm: 295.952 292.662 44.008
2024-12-02-07:46:04-root-INFO: grad norm: 216.059 212.494 39.087
2024-12-02-07:46:04-root-INFO: grad norm: 181.002 178.082 32.377
2024-12-02-07:46:05-root-INFO: grad norm: 150.686 147.479 30.921
2024-12-02-07:46:05-root-INFO: grad norm: 130.855 127.990 27.231
2024-12-02-07:46:05-root-INFO: grad norm: 113.751 110.643 26.409
2024-12-02-07:46:06-root-INFO: grad norm: 101.435 98.487 24.278
2024-12-02-07:46:06-root-INFO: Loss Change: 1541.633 -> 1506.218
2024-12-02-07:46:06-root-INFO: Regularization Change: 0.000 -> 0.067
2024-12-02-07:46:06-root-INFO: Undo step: 180
2024-12-02-07:46:06-root-INFO: Undo step: 181
2024-12-02-07:46:06-root-INFO: Undo step: 182
2024-12-02-07:46:06-root-INFO: Undo step: 183
2024-12-02-07:46:06-root-INFO: Undo step: 184
2024-12-02-07:46:06-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-07:46:07-root-INFO: grad norm: 942.744 889.649 311.914
2024-12-02-07:46:07-root-INFO: grad norm: 1202.359 1186.355 195.519
2024-12-02-07:46:08-root-INFO: grad norm: 695.166 675.407 164.566
2024-12-02-07:46:08-root-INFO: grad norm: 399.438 380.818 120.533
2024-12-02-07:46:09-root-INFO: grad norm: 277.843 266.665 78.014
2024-12-02-07:46:09-root-INFO: grad norm: 248.199 236.073 76.632
2024-12-02-07:46:10-root-INFO: grad norm: 236.040 225.543 69.609
2024-12-02-07:46:10-root-INFO: grad norm: 242.281 231.935 70.045
2024-12-02-07:46:10-root-INFO: Loss Change: 3104.285 -> 1876.508
2024-12-02-07:46:10-root-INFO: Regularization Change: 0.000 -> 22.246
2024-12-02-07:46:10-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-07:46:10-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:46:11-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-07:46:11-root-INFO: grad norm: 254.802 242.964 76.762
2024-12-02-07:46:11-root-INFO: grad norm: 280.878 269.370 79.578
2024-12-02-07:46:12-root-INFO: grad norm: 307.871 298.271 76.283
2024-12-02-07:46:12-root-INFO: Loss too large (1797.768->1845.633)! Learning rate decreased to 0.00328.
2024-12-02-07:46:12-root-INFO: grad norm: 334.873 325.871 77.126
2024-12-02-07:46:12-root-INFO: Loss too large (1752.051->1782.957)! Learning rate decreased to 0.00262.
2024-12-02-07:46:13-root-INFO: grad norm: 228.766 223.024 50.934
2024-12-02-07:46:13-root-INFO: grad norm: 113.709 109.416 30.950
2024-12-02-07:46:14-root-INFO: grad norm: 97.703 93.922 26.919
2024-12-02-07:46:14-root-INFO: grad norm: 99.753 96.113 26.701
2024-12-02-07:46:15-root-INFO: Loss Change: 1863.215 -> 1619.377
2024-12-02-07:46:15-root-INFO: Regularization Change: 0.000 -> 3.927
2024-12-02-07:46:15-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-07:46:15-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:46:15-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-07:46:15-root-INFO: grad norm: 91.609 86.933 28.893
2024-12-02-07:46:15-root-INFO: grad norm: 121.513 117.825 29.709
2024-12-02-07:46:16-root-INFO: grad norm: 204.643 198.724 48.866
2024-12-02-07:46:16-root-INFO: Loss too large (1580.260->1704.492)! Learning rate decreased to 0.00342.
2024-12-02-07:46:16-root-INFO: Loss too large (1580.260->1615.217)! Learning rate decreased to 0.00273.
2024-12-02-07:46:17-root-INFO: grad norm: 302.558 297.191 56.735
2024-12-02-07:46:17-root-INFO: Loss too large (1568.622->1618.118)! Learning rate decreased to 0.00219.
2024-12-02-07:46:17-root-INFO: Loss too large (1568.622->1583.357)! Learning rate decreased to 0.00175.
2024-12-02-07:46:18-root-INFO: grad norm: 204.569 198.719 48.574
2024-12-02-07:46:18-root-INFO: grad norm: 103.554 100.140 26.374
2024-12-02-07:46:19-root-INFO: grad norm: 105.855 102.613 25.994
2024-12-02-07:46:19-root-INFO: grad norm: 136.024 132.036 32.696
2024-12-02-07:46:19-root-INFO: Loss too large (1504.174->1504.458)! Learning rate decreased to 0.00140.
2024-12-02-07:46:19-root-INFO: Loss Change: 1609.158 -> 1499.963
2024-12-02-07:46:19-root-INFO: Regularization Change: 0.000 -> 1.977
2024-12-02-07:46:19-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-07:46:19-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-07:46:20-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-07:46:20-root-INFO: grad norm: 85.807 82.637 23.110
2024-12-02-07:46:20-root-INFO: Loss too large (1483.353->1503.954)! Learning rate decreased to 0.00356.
2024-12-02-07:46:20-root-INFO: Loss too large (1483.353->1489.452)! Learning rate decreased to 0.00285.
2024-12-02-07:46:21-root-INFO: grad norm: 229.653 225.091 45.549
2024-12-02-07:46:21-root-INFO: Loss too large (1481.949->1568.852)! Learning rate decreased to 0.00228.
2024-12-02-07:46:21-root-INFO: Loss too large (1481.949->1540.034)! Learning rate decreased to 0.00182.
2024-12-02-07:46:21-root-INFO: Loss too large (1481.949->1515.493)! Learning rate decreased to 0.00146.
2024-12-02-07:46:21-root-INFO: Loss too large (1481.949->1496.527)! Learning rate decreased to 0.00117.
2024-12-02-07:46:21-root-INFO: Loss too large (1481.949->1483.289)! Learning rate decreased to 0.00093.
2024-12-02-07:46:22-root-INFO: grad norm: 156.690 153.512 31.399
2024-12-02-07:46:22-root-INFO: grad norm: 93.381 89.793 25.636
2024-12-02-07:46:23-root-INFO: grad norm: 79.689 77.232 19.634
2024-12-02-07:46:23-root-INFO: grad norm: 70.324 66.785 22.027
2024-12-02-07:46:24-root-INFO: grad norm: 65.969 63.321 18.503
2024-12-02-07:46:24-root-INFO: grad norm: 62.999 59.487 20.743
2024-12-02-07:46:24-root-INFO: Loss Change: 1483.353 -> 1449.690
2024-12-02-07:46:24-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-02-07:46:24-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-07:46:24-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-07:46:25-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-07:46:25-root-INFO: grad norm: 154.538 150.211 36.314
2024-12-02-07:46:25-root-INFO: Loss too large (1446.234->1569.440)! Learning rate decreased to 0.00371.
2024-12-02-07:46:25-root-INFO: Loss too large (1446.234->1538.547)! Learning rate decreased to 0.00297.
2024-12-02-07:46:25-root-INFO: Loss too large (1446.234->1510.633)! Learning rate decreased to 0.00238.
2024-12-02-07:46:25-root-INFO: Loss too large (1446.234->1487.388)! Learning rate decreased to 0.00190.
2024-12-02-07:46:25-root-INFO: Loss too large (1446.234->1469.591)! Learning rate decreased to 0.00152.
2024-12-02-07:46:26-root-INFO: Loss too large (1446.234->1457.028)! Learning rate decreased to 0.00122.
2024-12-02-07:46:26-root-INFO: Loss too large (1446.234->1448.848)! Learning rate decreased to 0.00097.
2024-12-02-07:46:26-root-INFO: grad norm: 154.784 152.273 27.766
2024-12-02-07:46:27-root-INFO: grad norm: 164.037 160.279 34.911
2024-12-02-07:46:27-root-INFO: grad norm: 172.876 170.224 30.168
2024-12-02-07:46:28-root-INFO: grad norm: 191.948 188.304 37.223
2024-12-02-07:46:28-root-INFO: Loss too large (1435.882->1438.029)! Learning rate decreased to 0.00078.
2024-12-02-07:46:28-root-INFO: grad norm: 151.256 148.942 26.360
2024-12-02-07:46:29-root-INFO: grad norm: 112.979 109.805 26.591
2024-12-02-07:46:29-root-INFO: grad norm: 98.822 96.880 19.494
2024-12-02-07:46:30-root-INFO: Loss Change: 1446.234 -> 1421.850
2024-12-02-07:46:30-root-INFO: Regularization Change: 0.000 -> 0.144
2024-12-02-07:46:30-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-07:46:30-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:46:30-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-07:46:30-root-INFO: grad norm: 319.931 314.430 59.074
2024-12-02-07:46:30-root-INFO: Loss too large (1438.547->1654.278)! Learning rate decreased to 0.00387.
2024-12-02-07:46:30-root-INFO: Loss too large (1438.547->1626.806)! Learning rate decreased to 0.00309.
2024-12-02-07:46:30-root-INFO: Loss too large (1438.547->1597.042)! Learning rate decreased to 0.00248.
2024-12-02-07:46:30-root-INFO: Loss too large (1438.547->1565.143)! Learning rate decreased to 0.00198.
2024-12-02-07:46:31-root-INFO: Loss too large (1438.547->1532.241)! Learning rate decreased to 0.00158.
2024-12-02-07:46:31-root-INFO: Loss too large (1438.547->1500.388)! Learning rate decreased to 0.00127.
2024-12-02-07:46:31-root-INFO: Loss too large (1438.547->1472.256)! Learning rate decreased to 0.00101.
2024-12-02-07:46:31-root-INFO: Loss too large (1438.547->1449.994)! Learning rate decreased to 0.00081.
2024-12-02-07:46:31-root-INFO: grad norm: 253.414 249.452 44.637
2024-12-02-07:46:32-root-INFO: grad norm: 176.869 172.843 37.519
2024-12-02-07:46:32-root-INFO: grad norm: 167.693 165.252 28.505
2024-12-02-07:46:33-root-INFO: grad norm: 160.582 157.147 33.034
2024-12-02-07:46:33-root-INFO: grad norm: 159.557 157.347 26.465
2024-12-02-07:46:34-root-INFO: grad norm: 161.169 158.006 31.771
2024-12-02-07:46:34-root-INFO: grad norm: 164.196 162.022 26.634
2024-12-02-07:46:35-root-INFO: Loss Change: 1438.547 -> 1401.214
2024-12-02-07:46:35-root-INFO: Regularization Change: 0.000 -> 0.152
2024-12-02-07:46:35-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-07:46:35-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:46:35-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-07:46:35-root-INFO: grad norm: 397.436 392.927 59.697
2024-12-02-07:46:35-root-INFO: Loss too large (1420.303->1652.911)! Learning rate decreased to 0.00403.
2024-12-02-07:46:35-root-INFO: Loss too large (1420.303->1633.429)! Learning rate decreased to 0.00322.
2024-12-02-07:46:35-root-INFO: Loss too large (1420.303->1611.066)! Learning rate decreased to 0.00258.
2024-12-02-07:46:35-root-INFO: Loss too large (1420.303->1584.609)! Learning rate decreased to 0.00206.
2024-12-02-07:46:36-root-INFO: Loss too large (1420.303->1554.029)! Learning rate decreased to 0.00165.
2024-12-02-07:46:36-root-INFO: Loss too large (1420.303->1520.348)! Learning rate decreased to 0.00132.
2024-12-02-07:46:36-root-INFO: Loss too large (1420.303->1485.746)! Learning rate decreased to 0.00106.
2024-12-02-07:46:36-root-INFO: Loss too large (1420.303->1453.651)! Learning rate decreased to 0.00085.
2024-12-02-07:46:36-root-INFO: Loss too large (1420.303->1427.427)! Learning rate decreased to 0.00068.
2024-12-02-07:46:37-root-INFO: grad norm: 264.831 261.319 42.986
2024-12-02-07:46:37-root-INFO: grad norm: 145.787 142.607 30.283
2024-12-02-07:46:38-root-INFO: grad norm: 122.982 121.122 21.305
2024-12-02-07:46:38-root-INFO: grad norm: 103.328 100.428 24.310
2024-12-02-07:46:39-root-INFO: grad norm: 92.450 90.634 18.236
2024-12-02-07:46:39-root-INFO: grad norm: 83.296 80.494 21.422
2024-12-02-07:46:40-root-INFO: grad norm: 77.516 75.629 16.999
2024-12-02-07:46:40-root-INFO: Loss Change: 1420.303 -> 1377.203
2024-12-02-07:46:40-root-INFO: Regularization Change: 0.000 -> 0.100
2024-12-02-07:46:40-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-07:46:40-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:46:40-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-07:46:40-root-INFO: grad norm: 302.117 298.086 49.182
2024-12-02-07:46:41-root-INFO: Loss too large (1388.559->1617.475)! Learning rate decreased to 0.00420.
2024-12-02-07:46:41-root-INFO: Loss too large (1388.559->1594.589)! Learning rate decreased to 0.00336.
2024-12-02-07:46:41-root-INFO: Loss too large (1388.559->1568.935)! Learning rate decreased to 0.00269.
2024-12-02-07:46:41-root-INFO: Loss too large (1388.559->1540.109)! Learning rate decreased to 0.00215.
2024-12-02-07:46:41-root-INFO: Loss too large (1388.559->1508.675)! Learning rate decreased to 0.00172.
2024-12-02-07:46:41-root-INFO: Loss too large (1388.559->1476.149)! Learning rate decreased to 0.00138.
2024-12-02-07:46:41-root-INFO: Loss too large (1388.559->1445.183)! Learning rate decreased to 0.00110.
2024-12-02-07:46:42-root-INFO: Loss too large (1388.559->1418.712)! Learning rate decreased to 0.00088.
2024-12-02-07:46:42-root-INFO: Loss too large (1388.559->1398.585)! Learning rate decreased to 0.00070.
2024-12-02-07:46:42-root-INFO: grad norm: 261.363 258.132 40.967
2024-12-02-07:46:43-root-INFO: grad norm: 217.602 214.416 37.099
2024-12-02-07:46:43-root-INFO: grad norm: 205.945 203.540 31.381
2024-12-02-07:46:44-root-INFO: grad norm: 193.974 191.190 32.744
2024-12-02-07:46:44-root-INFO: grad norm: 189.024 186.883 28.366
2024-12-02-07:46:45-root-INFO: grad norm: 184.223 181.647 30.702
2024-12-02-07:46:45-root-INFO: grad norm: 182.379 180.361 27.062
2024-12-02-07:46:45-root-INFO: Loss Change: 1388.559 -> 1358.332
2024-12-02-07:46:45-root-INFO: Regularization Change: 0.000 -> 0.116
2024-12-02-07:46:45-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-07:46:45-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:46:46-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-07:46:46-root-INFO: grad norm: 383.938 380.242 53.150
2024-12-02-07:46:46-root-INFO: Loss too large (1376.162->1617.045)! Learning rate decreased to 0.00437.
2024-12-02-07:46:46-root-INFO: Loss too large (1376.162->1600.075)! Learning rate decreased to 0.00350.
2024-12-02-07:46:46-root-INFO: Loss too large (1376.162->1580.825)! Learning rate decreased to 0.00280.
2024-12-02-07:46:46-root-INFO: Loss too large (1376.162->1557.768)! Learning rate decreased to 0.00224.
2024-12-02-07:46:47-root-INFO: Loss too large (1376.162->1530.300)! Learning rate decreased to 0.00179.
2024-12-02-07:46:47-root-INFO: Loss too large (1376.162->1498.811)! Learning rate decreased to 0.00143.
2024-12-02-07:46:47-root-INFO: Loss too large (1376.162->1464.644)! Learning rate decreased to 0.00115.
2024-12-02-07:46:47-root-INFO: Loss too large (1376.162->1430.643)! Learning rate decreased to 0.00092.
2024-12-02-07:46:47-root-INFO: Loss too large (1376.162->1400.645)! Learning rate decreased to 0.00073.
2024-12-02-07:46:47-root-INFO: Loss too large (1376.162->1377.604)! Learning rate decreased to 0.00059.
2024-12-02-07:46:48-root-INFO: grad norm: 252.282 249.366 38.241
2024-12-02-07:46:48-root-INFO: grad norm: 154.795 152.145 28.519
2024-12-02-07:46:49-root-INFO: grad norm: 123.519 121.718 21.016
2024-12-02-07:46:49-root-INFO: grad norm: 98.781 96.367 21.703
2024-12-02-07:46:50-root-INFO: grad norm: 84.321 82.547 17.209
2024-12-02-07:46:50-root-INFO: grad norm: 73.255 70.884 18.486
2024-12-02-07:46:51-root-INFO: grad norm: 66.102 64.208 15.710
2024-12-02-07:46:51-root-INFO: Loss Change: 1376.162 -> 1336.931
2024-12-02-07:46:51-root-INFO: Regularization Change: 0.000 -> 0.084
2024-12-02-07:46:51-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-07:46:51-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-07:46:51-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-07:46:51-root-INFO: grad norm: 290.389 287.028 44.053
2024-12-02-07:46:52-root-INFO: Loss too large (1350.227->1591.554)! Learning rate decreased to 0.00455.
2024-12-02-07:46:52-root-INFO: Loss too large (1350.227->1571.194)! Learning rate decreased to 0.00364.
2024-12-02-07:46:52-root-INFO: Loss too large (1350.227->1547.924)! Learning rate decreased to 0.00291.
2024-12-02-07:46:52-root-INFO: Loss too large (1350.227->1520.929)! Learning rate decreased to 0.00233.
2024-12-02-07:46:52-root-INFO: Loss too large (1350.227->1490.414)! Learning rate decreased to 0.00186.
2024-12-02-07:46:52-root-INFO: Loss too large (1350.227->1457.482)! Learning rate decreased to 0.00149.
2024-12-02-07:46:52-root-INFO: Loss too large (1350.227->1424.516)! Learning rate decreased to 0.00119.
2024-12-02-07:46:53-root-INFO: Loss too large (1350.227->1394.811)! Learning rate decreased to 0.00095.
2024-12-02-07:46:53-root-INFO: Loss too large (1350.227->1371.045)! Learning rate decreased to 0.00076.
2024-12-02-07:46:53-root-INFO: Loss too large (1350.227->1354.221)! Learning rate decreased to 0.00061.
2024-12-02-07:46:53-root-INFO: grad norm: 234.668 232.177 34.106
2024-12-02-07:46:54-root-INFO: grad norm: 186.567 183.885 31.521
2024-12-02-07:46:54-root-INFO: grad norm: 164.158 162.314 24.535
2024-12-02-07:46:55-root-INFO: grad norm: 144.027 141.691 25.832
2024-12-02-07:46:55-root-INFO: grad norm: 131.262 129.656 20.472
2024-12-02-07:46:56-root-INFO: grad norm: 119.707 117.564 22.550
2024-12-02-07:46:56-root-INFO: grad norm: 111.556 110.058 18.220
2024-12-02-07:46:56-root-INFO: Loss Change: 1350.227 -> 1320.686
2024-12-02-07:46:56-root-INFO: Regularization Change: 0.000 -> 0.091
2024-12-02-07:46:56-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-07:46:56-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:46:57-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-07:46:57-root-INFO: grad norm: 335.481 332.316 45.968
2024-12-02-07:46:57-root-INFO: Loss too large (1338.194->1586.511)! Learning rate decreased to 0.00474.
2024-12-02-07:46:57-root-INFO: Loss too large (1338.194->1570.221)! Learning rate decreased to 0.00379.
2024-12-02-07:46:57-root-INFO: Loss too large (1338.194->1551.550)! Learning rate decreased to 0.00303.
2024-12-02-07:46:57-root-INFO: Loss too large (1338.194->1528.932)! Learning rate decreased to 0.00243.
2024-12-02-07:46:57-root-INFO: Loss too large (1338.194->1501.723)! Learning rate decreased to 0.00194.
2024-12-02-07:46:58-root-INFO: Loss too large (1338.194->1470.201)! Learning rate decreased to 0.00155.
2024-12-02-07:46:58-root-INFO: Loss too large (1338.194->1435.655)! Learning rate decreased to 0.00124.
2024-12-02-07:46:58-root-INFO: Loss too large (1338.194->1401.084)! Learning rate decreased to 0.00099.
2024-12-02-07:46:58-root-INFO: Loss too large (1338.194->1370.551)! Learning rate decreased to 0.00080.
2024-12-02-07:46:58-root-INFO: Loss too large (1338.194->1347.094)! Learning rate decreased to 0.00064.
2024-12-02-07:46:59-root-INFO: grad norm: 280.770 277.902 40.027
2024-12-02-07:46:59-root-INFO: grad norm: 234.786 232.215 34.654
2024-12-02-07:47:00-root-INFO: grad norm: 214.747 212.609 30.224
2024-12-02-07:47:00-root-INFO: grad norm: 196.189 193.923 29.727
2024-12-02-07:47:01-root-INFO: grad norm: 184.686 182.843 26.026
2024-12-02-07:47:01-root-INFO: grad norm: 173.821 171.736 26.841
2024-12-02-07:47:02-root-INFO: grad norm: 166.306 164.629 23.553
2024-12-02-07:47:02-root-INFO: Loss Change: 1338.194 -> 1305.594
2024-12-02-07:47:02-root-INFO: Regularization Change: 0.000 -> 0.095
2024-12-02-07:47:02-root-INFO: Undo step: 175
2024-12-02-07:47:02-root-INFO: Undo step: 176
2024-12-02-07:47:02-root-INFO: Undo step: 177
2024-12-02-07:47:02-root-INFO: Undo step: 178
2024-12-02-07:47:02-root-INFO: Undo step: 179
2024-12-02-07:47:02-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-07:47:02-root-INFO: grad norm: 753.592 677.135 330.739
2024-12-02-07:47:03-root-INFO: grad norm: 594.990 575.388 151.466
2024-12-02-07:47:03-root-INFO: grad norm: 828.332 818.581 126.726
2024-12-02-07:47:04-root-INFO: grad norm: 292.971 285.773 64.544
2024-12-02-07:47:04-root-INFO: grad norm: 209.309 204.607 44.117
2024-12-02-07:47:05-root-INFO: grad norm: 257.872 253.982 44.624
2024-12-02-07:47:05-root-INFO: Loss too large (1790.421->1798.996)! Learning rate decreased to 0.00387.
2024-12-02-07:47:05-root-INFO: grad norm: 216.062 209.719 51.971
2024-12-02-07:47:06-root-INFO: grad norm: 178.765 175.135 35.845
2024-12-02-07:47:06-root-INFO: Loss Change: 2761.313 -> 1655.412
2024-12-02-07:47:06-root-INFO: Regularization Change: 0.000 -> 22.944
2024-12-02-07:47:06-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-07:47:06-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:47:06-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-07:47:06-root-INFO: grad norm: 177.498 170.740 48.512
2024-12-02-07:47:07-root-INFO: grad norm: 241.409 232.804 63.879
2024-12-02-07:47:07-root-INFO: Loss too large (1607.123->1719.979)! Learning rate decreased to 0.00403.
2024-12-02-07:47:07-root-INFO: Loss too large (1607.123->1636.857)! Learning rate decreased to 0.00322.
2024-12-02-07:47:08-root-INFO: grad norm: 301.199 293.617 67.156
2024-12-02-07:47:08-root-INFO: Loss too large (1590.407->1608.381)! Learning rate decreased to 0.00258.
2024-12-02-07:47:08-root-INFO: grad norm: 193.274 188.518 42.613
2024-12-02-07:47:09-root-INFO: grad norm: 83.516 80.293 22.979
2024-12-02-07:47:09-root-INFO: grad norm: 68.743 65.967 19.337
2024-12-02-07:47:10-root-INFO: grad norm: 67.473 64.403 20.122
2024-12-02-07:47:10-root-INFO: grad norm: 67.205 64.087 20.232
2024-12-02-07:47:10-root-INFO: Loss Change: 1649.929 -> 1467.022
2024-12-02-07:47:10-root-INFO: Regularization Change: 0.000 -> 3.202
2024-12-02-07:47:10-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-07:47:10-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:47:11-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-07:47:11-root-INFO: grad norm: 146.810 142.933 33.518
2024-12-02-07:47:11-root-INFO: Loss too large (1465.496->1504.580)! Learning rate decreased to 0.00420.
2024-12-02-07:47:11-root-INFO: Loss too large (1465.496->1482.100)! Learning rate decreased to 0.00336.
2024-12-02-07:47:11-root-INFO: Loss too large (1465.496->1467.214)! Learning rate decreased to 0.00269.
2024-12-02-07:47:12-root-INFO: grad norm: 152.719 148.404 36.047
2024-12-02-07:47:12-root-INFO: grad norm: 179.573 175.270 39.076
2024-12-02-07:47:12-root-INFO: Loss too large (1442.597->1444.074)! Learning rate decreased to 0.00215.
2024-12-02-07:47:13-root-INFO: grad norm: 150.426 145.419 38.488
2024-12-02-07:47:13-root-INFO: grad norm: 112.969 108.670 30.866
2024-12-02-07:47:14-root-INFO: grad norm: 106.809 101.110 34.423
2024-12-02-07:47:14-root-INFO: grad norm: 101.506 96.094 32.700
2024-12-02-07:47:15-root-INFO: grad norm: 102.099 94.886 37.693
2024-12-02-07:47:15-root-INFO: Loss Change: 1465.496 -> 1353.513
2024-12-02-07:47:15-root-INFO: Regularization Change: 0.000 -> 1.852
2024-12-02-07:47:15-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-07:47:15-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:47:15-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-07:47:15-root-INFO: grad norm: 156.461 151.305 39.832
2024-12-02-07:47:15-root-INFO: Loss too large (1351.069->1397.487)! Learning rate decreased to 0.00437.
2024-12-02-07:47:16-root-INFO: Loss too large (1351.069->1368.993)! Learning rate decreased to 0.00350.
2024-12-02-07:47:16-root-INFO: grad norm: 243.918 233.760 69.658
2024-12-02-07:47:16-root-INFO: Loss too large (1350.526->1491.744)! Learning rate decreased to 0.00280.
2024-12-02-07:47:16-root-INFO: Loss too large (1350.526->1405.494)! Learning rate decreased to 0.00224.
2024-12-02-07:47:16-root-INFO: Loss too large (1350.526->1352.793)! Learning rate decreased to 0.00179.
2024-12-02-07:47:17-root-INFO: grad norm: 228.491 220.698 59.165
2024-12-02-07:47:17-root-INFO: Loss too large (1324.844->1331.573)! Learning rate decreased to 0.00143.
2024-12-02-07:47:18-root-INFO: grad norm: 177.108 168.659 54.048
2024-12-02-07:47:18-root-INFO: grad norm: 132.784 127.981 35.390
2024-12-02-07:47:18-root-INFO: grad norm: 127.317 121.190 39.021
2024-12-02-07:47:19-root-INFO: grad norm: 133.043 127.961 36.421
2024-12-02-07:47:19-root-INFO: grad norm: 145.179 138.634 43.097
2024-12-02-07:47:20-root-INFO: Loss Change: 1351.069 -> 1289.136
2024-12-02-07:47:20-root-INFO: Regularization Change: 0.000 -> 0.804
2024-12-02-07:47:20-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-07:47:20-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-07:47:20-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-07:47:20-root-INFO: grad norm: 343.565 331.142 91.552
2024-12-02-07:47:20-root-INFO: Loss too large (1309.525->1565.101)! Learning rate decreased to 0.00455.
2024-12-02-07:47:20-root-INFO: Loss too large (1309.525->1527.385)! Learning rate decreased to 0.00364.
2024-12-02-07:47:21-root-INFO: Loss too large (1309.525->1486.778)! Learning rate decreased to 0.00291.
2024-12-02-07:47:21-root-INFO: Loss too large (1309.525->1443.375)! Learning rate decreased to 0.00233.
2024-12-02-07:47:21-root-INFO: Loss too large (1309.525->1399.229)! Learning rate decreased to 0.00186.
2024-12-02-07:47:21-root-INFO: Loss too large (1309.525->1358.000)! Learning rate decreased to 0.00149.
2024-12-02-07:47:21-root-INFO: Loss too large (1309.525->1323.681)! Learning rate decreased to 0.00119.
2024-12-02-07:47:22-root-INFO: grad norm: 239.697 230.250 66.631
2024-12-02-07:47:22-root-INFO: grad norm: 139.145 133.553 39.051
2024-12-02-07:47:23-root-INFO: grad norm: 128.303 123.104 36.152
2024-12-02-07:47:23-root-INFO: grad norm: 124.665 119.689 34.870
2024-12-02-07:47:24-root-INFO: grad norm: 127.571 122.493 35.633
2024-12-02-07:47:24-root-INFO: grad norm: 136.668 131.370 37.682
2024-12-02-07:47:25-root-INFO: grad norm: 147.403 141.633 40.835
2024-12-02-07:47:25-root-INFO: Loss Change: 1309.525 -> 1257.427
2024-12-02-07:47:25-root-INFO: Regularization Change: 0.000 -> 0.231
2024-12-02-07:47:25-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-07:47:25-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:47:25-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-07:47:25-root-INFO: grad norm: 338.053 326.350 88.179
2024-12-02-07:47:26-root-INFO: Loss too large (1276.474->1535.557)! Learning rate decreased to 0.00474.
2024-12-02-07:47:26-root-INFO: Loss too large (1276.474->1504.400)! Learning rate decreased to 0.00379.
2024-12-02-07:47:26-root-INFO: Loss too large (1276.474->1469.979)! Learning rate decreased to 0.00303.
2024-12-02-07:47:26-root-INFO: Loss too large (1276.474->1431.667)! Learning rate decreased to 0.00243.
2024-12-02-07:47:26-root-INFO: Loss too large (1276.474->1390.417)! Learning rate decreased to 0.00194.
2024-12-02-07:47:26-root-INFO: Loss too large (1276.474->1348.955)! Learning rate decreased to 0.00155.
2024-12-02-07:47:26-root-INFO: Loss too large (1276.474->1311.491)! Learning rate decreased to 0.00124.
2024-12-02-07:47:27-root-INFO: Loss too large (1276.474->1281.791)! Learning rate decreased to 0.00099.
2024-12-02-07:47:27-root-INFO: grad norm: 229.483 220.942 62.025
2024-12-02-07:47:27-root-INFO: grad norm: 143.432 137.642 40.341
2024-12-02-07:47:28-root-INFO: grad norm: 121.626 117.170 32.622
2024-12-02-07:47:29-root-INFO: grad norm: 105.188 100.807 30.040
2024-12-02-07:47:29-root-INFO: grad norm: 96.989 93.440 25.996
2024-12-02-07:47:29-root-INFO: grad norm: 91.143 87.306 26.168
2024-12-02-07:47:30-root-INFO: grad norm: 88.434 85.221 23.621
2024-12-02-07:47:30-root-INFO: Loss Change: 1276.474 -> 1228.276
2024-12-02-07:47:30-root-INFO: Regularization Change: 0.000 -> 0.164
2024-12-02-07:47:30-root-INFO: Undo step: 175
2024-12-02-07:47:30-root-INFO: Undo step: 176
2024-12-02-07:47:30-root-INFO: Undo step: 177
2024-12-02-07:47:30-root-INFO: Undo step: 178
2024-12-02-07:47:30-root-INFO: Undo step: 179
2024-12-02-07:47:31-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-07:47:31-root-INFO: grad norm: 708.377 681.859 192.005
2024-12-02-07:47:31-root-INFO: grad norm: 882.547 861.041 193.642
2024-12-02-07:47:31-root-INFO: Loss too large (2440.389->2611.226)! Learning rate decreased to 0.00387.
2024-12-02-07:47:32-root-INFO: grad norm: 616.097 604.086 121.060
2024-12-02-07:47:32-root-INFO: grad norm: 536.063 516.668 142.892
2024-12-02-07:47:33-root-INFO: grad norm: 632.793 614.045 152.889
2024-12-02-07:47:33-root-INFO: Loss too large (1788.126->1809.595)! Learning rate decreased to 0.00309.
2024-12-02-07:47:34-root-INFO: grad norm: 274.502 259.626 89.137
2024-12-02-07:47:34-root-INFO: grad norm: 255.615 241.483 83.816
2024-12-02-07:47:34-root-INFO: Loss too large (1492.436->1505.134)! Learning rate decreased to 0.00248.
2024-12-02-07:47:35-root-INFO: grad norm: 284.473 268.591 93.723
2024-12-02-07:47:35-root-INFO: Loss too large (1458.387->1487.684)! Learning rate decreased to 0.00198.
2024-12-02-07:47:35-root-INFO: Loss Change: 2503.898 -> 1454.683
2024-12-02-07:47:35-root-INFO: Regularization Change: 0.000 -> 14.896
2024-12-02-07:47:35-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-07:47:35-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:47:35-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-07:47:36-root-INFO: grad norm: 222.685 208.475 78.274
2024-12-02-07:47:36-root-INFO: Loss too large (1427.651->1686.034)! Learning rate decreased to 0.00403.
2024-12-02-07:47:36-root-INFO: Loss too large (1427.651->1574.157)! Learning rate decreased to 0.00322.
2024-12-02-07:47:36-root-INFO: Loss too large (1427.651->1495.075)! Learning rate decreased to 0.00258.
2024-12-02-07:47:36-root-INFO: Loss too large (1427.651->1443.882)! Learning rate decreased to 0.00206.
2024-12-02-07:47:37-root-INFO: grad norm: 245.598 232.068 80.389
2024-12-02-07:47:37-root-INFO: Loss too large (1414.324->1422.491)! Learning rate decreased to 0.00165.
2024-12-02-07:47:37-root-INFO: grad norm: 199.113 185.372 72.686
2024-12-02-07:47:38-root-INFO: grad norm: 162.428 154.059 51.466
2024-12-02-07:47:38-root-INFO: grad norm: 152.370 141.411 56.742
2024-12-02-07:47:39-root-INFO: grad norm: 148.063 140.421 46.955
2024-12-02-07:47:39-root-INFO: grad norm: 150.039 139.574 55.051
2024-12-02-07:47:40-root-INFO: grad norm: 157.160 148.917 50.230
2024-12-02-07:47:40-root-INFO: Loss Change: 1427.651 -> 1358.742
2024-12-02-07:47:40-root-INFO: Regularization Change: 0.000 -> 0.684
2024-12-02-07:47:40-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-07:47:40-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:47:40-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-07:47:40-root-INFO: grad norm: 111.426 106.313 33.368
2024-12-02-07:47:41-root-INFO: Loss too large (1343.581->1379.383)! Learning rate decreased to 0.00420.
2024-12-02-07:47:41-root-INFO: Loss too large (1343.581->1356.399)! Learning rate decreased to 0.00336.
2024-12-02-07:47:41-root-INFO: grad norm: 219.676 208.903 67.949
2024-12-02-07:47:41-root-INFO: Loss too large (1343.313->1438.815)! Learning rate decreased to 0.00269.
2024-12-02-07:47:42-root-INFO: Loss too large (1343.313->1400.008)! Learning rate decreased to 0.00215.
2024-12-02-07:47:42-root-INFO: Loss too large (1343.313->1368.834)! Learning rate decreased to 0.00172.
2024-12-02-07:47:42-root-INFO: Loss too large (1343.313->1346.985)! Learning rate decreased to 0.00138.
2024-12-02-07:47:42-root-INFO: grad norm: 179.163 168.186 61.749
2024-12-02-07:47:43-root-INFO: grad norm: 147.551 140.106 46.279
2024-12-02-07:47:43-root-INFO: grad norm: 130.421 122.193 45.593
2024-12-02-07:47:44-root-INFO: grad norm: 116.905 111.112 36.346
2024-12-02-07:47:44-root-INFO: grad norm: 108.740 101.853 38.084
2024-12-02-07:47:45-root-INFO: grad norm: 102.648 97.599 31.797
2024-12-02-07:47:45-root-INFO: Loss Change: 1343.581 -> 1302.762
2024-12-02-07:47:45-root-INFO: Regularization Change: 0.000 -> 0.493
2024-12-02-07:47:45-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-07:47:45-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-07:47:45-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-07:47:45-root-INFO: grad norm: 71.921 68.526 21.837
2024-12-02-07:47:46-root-INFO: grad norm: 116.311 110.450 36.454
2024-12-02-07:47:46-root-INFO: Loss too large (1285.864->1379.760)! Learning rate decreased to 0.00437.
2024-12-02-07:47:46-root-INFO: Loss too large (1285.864->1335.713)! Learning rate decreased to 0.00350.
2024-12-02-07:47:46-root-INFO: Loss too large (1285.864->1308.809)! Learning rate decreased to 0.00280.
2024-12-02-07:47:46-root-INFO: Loss too large (1285.864->1293.064)! Learning rate decreased to 0.00224.
2024-12-02-07:47:47-root-INFO: grad norm: 192.000 182.561 59.460
2024-12-02-07:47:47-root-INFO: Loss too large (1284.371->1323.327)! Learning rate decreased to 0.00179.
2024-12-02-07:47:47-root-INFO: Loss too large (1284.371->1299.643)! Learning rate decreased to 0.00143.
2024-12-02-07:47:47-root-INFO: Loss too large (1284.371->1284.662)! Learning rate decreased to 0.00115.
2024-12-02-07:47:48-root-INFO: grad norm: 140.580 132.939 45.715
2024-12-02-07:47:48-root-INFO: grad norm: 106.475 101.352 32.630
2024-12-02-07:47:49-root-INFO: grad norm: 85.717 80.862 28.438
2024-12-02-07:47:49-root-INFO: grad norm: 71.583 68.147 21.910
2024-12-02-07:47:50-root-INFO: grad norm: 62.441 58.849 20.873
2024-12-02-07:47:50-root-INFO: Loss Change: 1293.820 -> 1258.914
2024-12-02-07:47:50-root-INFO: Regularization Change: 0.000 -> 0.491
2024-12-02-07:47:50-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-07:47:50-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-07:47:50-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-07:47:50-root-INFO: grad norm: 198.795 188.573 62.928
2024-12-02-07:47:51-root-INFO: Loss too large (1266.315->1496.933)! Learning rate decreased to 0.00455.
2024-12-02-07:47:51-root-INFO: Loss too large (1266.315->1444.999)! Learning rate decreased to 0.00364.
2024-12-02-07:47:51-root-INFO: Loss too large (1266.315->1393.053)! Learning rate decreased to 0.00291.
2024-12-02-07:47:51-root-INFO: Loss too large (1266.315->1346.012)! Learning rate decreased to 0.00233.
2024-12-02-07:47:51-root-INFO: Loss too large (1266.315->1308.500)! Learning rate decreased to 0.00186.
2024-12-02-07:47:51-root-INFO: Loss too large (1266.315->1282.131)! Learning rate decreased to 0.00149.
2024-12-02-07:47:52-root-INFO: grad norm: 227.904 217.940 66.651
2024-12-02-07:47:52-root-INFO: Loss too large (1265.694->1268.611)! Learning rate decreased to 0.00119.
2024-12-02-07:47:53-root-INFO: grad norm: 176.844 168.076 54.996
2024-12-02-07:47:53-root-INFO: grad norm: 144.326 137.491 43.889
2024-12-02-07:47:54-root-INFO: grad norm: 120.483 114.654 37.023
2024-12-02-07:47:54-root-INFO: grad norm: 102.910 97.837 31.913
2024-12-02-07:47:55-root-INFO: grad norm: 89.692 85.417 27.359
2024-12-02-07:47:55-root-INFO: grad norm: 79.626 75.607 24.977
2024-12-02-07:47:55-root-INFO: Loss Change: 1266.315 -> 1232.156
2024-12-02-07:47:55-root-INFO: Regularization Change: 0.000 -> 0.224
2024-12-02-07:47:55-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-07:47:55-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:47:56-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-07:47:56-root-INFO: grad norm: 203.538 193.584 62.871
2024-12-02-07:47:56-root-INFO: Loss too large (1239.310->1481.010)! Learning rate decreased to 0.00474.
2024-12-02-07:47:56-root-INFO: Loss too large (1239.310->1434.014)! Learning rate decreased to 0.00379.
2024-12-02-07:47:56-root-INFO: Loss too large (1239.310->1383.779)! Learning rate decreased to 0.00303.
2024-12-02-07:47:56-root-INFO: Loss too large (1239.310->1334.589)! Learning rate decreased to 0.00243.
2024-12-02-07:47:57-root-INFO: Loss too large (1239.310->1292.662)! Learning rate decreased to 0.00194.
2024-12-02-07:47:57-root-INFO: Loss too large (1239.310->1261.764)! Learning rate decreased to 0.00155.
2024-12-02-07:47:57-root-INFO: Loss too large (1239.310->1241.882)! Learning rate decreased to 0.00124.
2024-12-02-07:47:57-root-INFO: grad norm: 175.967 168.652 50.211
2024-12-02-07:47:58-root-INFO: grad norm: 157.443 150.071 47.613
2024-12-02-07:47:58-root-INFO: grad norm: 141.350 135.112 41.527
2024-12-02-07:47:59-root-INFO: grad norm: 128.611 122.794 38.244
2024-12-02-07:47:59-root-INFO: grad norm: 117.318 112.002 34.917
2024-12-02-07:48:00-root-INFO: grad norm: 108.402 103.588 31.947
2024-12-02-07:48:00-root-INFO: grad norm: 100.625 96.002 30.150
2024-12-02-07:48:00-root-INFO: Loss Change: 1239.310 -> 1206.471
2024-12-02-07:48:00-root-INFO: Regularization Change: 0.000 -> 0.216
2024-12-02-07:48:00-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-07:48:00-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:48:01-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-07:48:01-root-INFO: grad norm: 156.031 149.057 46.127
2024-12-02-07:48:01-root-INFO: Loss too large (1210.588->1419.291)! Learning rate decreased to 0.00494.
2024-12-02-07:48:01-root-INFO: Loss too large (1210.588->1374.256)! Learning rate decreased to 0.00395.
2024-12-02-07:48:01-root-INFO: Loss too large (1210.588->1327.219)! Learning rate decreased to 0.00316.
2024-12-02-07:48:01-root-INFO: Loss too large (1210.588->1284.594)! Learning rate decreased to 0.00253.
2024-12-02-07:48:02-root-INFO: Loss too large (1210.588->1251.265)! Learning rate decreased to 0.00202.
2024-12-02-07:48:02-root-INFO: Loss too large (1210.588->1228.395)! Learning rate decreased to 0.00162.
2024-12-02-07:48:02-root-INFO: Loss too large (1210.588->1214.341)! Learning rate decreased to 0.00129.
2024-12-02-07:48:02-root-INFO: grad norm: 149.671 143.603 42.184
2024-12-02-07:48:03-root-INFO: grad norm: 144.435 138.241 41.845
2024-12-02-07:48:03-root-INFO: grad norm: 138.794 132.979 39.753
2024-12-02-07:48:04-root-INFO: grad norm: 133.807 128.183 38.385
2024-12-02-07:48:04-root-INFO: grad norm: 128.700 123.228 37.127
2024-12-02-07:48:05-root-INFO: grad norm: 124.473 119.298 35.516
2024-12-02-07:48:05-root-INFO: grad norm: 120.404 115.248 34.858
2024-12-02-07:48:06-root-INFO: Loss Change: 1210.588 -> 1188.311
2024-12-02-07:48:06-root-INFO: Regularization Change: 0.000 -> 0.194
2024-12-02-07:48:06-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-07:48:06-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:48:06-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-07:48:06-root-INFO: grad norm: 270.124 258.208 79.344
2024-12-02-07:48:06-root-INFO: Loss too large (1212.338->1505.339)! Learning rate decreased to 0.00514.
2024-12-02-07:48:06-root-INFO: Loss too large (1212.338->1462.519)! Learning rate decreased to 0.00411.
2024-12-02-07:48:06-root-INFO: Loss too large (1212.338->1415.868)! Learning rate decreased to 0.00329.
2024-12-02-07:48:07-root-INFO: Loss too large (1212.338->1363.199)! Learning rate decreased to 0.00263.
2024-12-02-07:48:07-root-INFO: Loss too large (1212.338->1307.419)! Learning rate decreased to 0.00210.
2024-12-02-07:48:07-root-INFO: Loss too large (1212.338->1257.068)! Learning rate decreased to 0.00168.
2024-12-02-07:48:07-root-INFO: Loss too large (1212.338->1219.761)! Learning rate decreased to 0.00135.
2024-12-02-07:48:08-root-INFO: grad norm: 241.112 232.656 63.296
2024-12-02-07:48:08-root-INFO: grad norm: 221.105 211.754 63.624
2024-12-02-07:48:09-root-INFO: grad norm: 198.201 190.790 53.694
2024-12-02-07:48:09-root-INFO: grad norm: 181.911 174.415 51.680
2024-12-02-07:48:09-root-INFO: grad norm: 164.794 158.423 45.378
2024-12-02-07:48:10-root-INFO: grad norm: 152.973 146.761 43.149
2024-12-02-07:48:10-root-INFO: grad norm: 141.395 135.828 39.282
2024-12-02-07:48:11-root-INFO: Loss Change: 1212.338 -> 1166.236
2024-12-02-07:48:11-root-INFO: Regularization Change: 0.000 -> 0.271
2024-12-02-07:48:11-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-07:48:11-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-07:48:11-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-07:48:11-root-INFO: grad norm: 196.153 188.224 55.208
2024-12-02-07:48:11-root-INFO: Loss too large (1171.451->1429.357)! Learning rate decreased to 0.00535.
2024-12-02-07:48:11-root-INFO: Loss too large (1171.451->1389.932)! Learning rate decreased to 0.00428.
2024-12-02-07:48:11-root-INFO: Loss too large (1171.451->1342.658)! Learning rate decreased to 0.00342.
2024-12-02-07:48:12-root-INFO: Loss too large (1171.451->1290.517)! Learning rate decreased to 0.00274.
2024-12-02-07:48:12-root-INFO: Loss too large (1171.451->1241.631)! Learning rate decreased to 0.00219.
2024-12-02-07:48:12-root-INFO: Loss too large (1171.451->1203.641)! Learning rate decreased to 0.00175.
2024-12-02-07:48:12-root-INFO: Loss too large (1171.451->1178.739)! Learning rate decreased to 0.00140.
2024-12-02-07:48:12-root-INFO: grad norm: 184.247 177.972 47.673
2024-12-02-07:48:13-root-INFO: grad norm: 176.038 169.190 48.623
2024-12-02-07:48:13-root-INFO: grad norm: 166.491 160.564 44.026
2024-12-02-07:48:14-root-INFO: grad norm: 159.715 153.615 43.717
2024-12-02-07:48:14-root-INFO: grad norm: 152.355 146.818 40.701
2024-12-02-07:48:15-root-INFO: grad norm: 147.401 141.830 40.140
2024-12-02-07:48:15-root-INFO: grad norm: 142.439 137.207 38.249
2024-12-02-07:48:16-root-INFO: Loss Change: 1171.451 -> 1145.367
2024-12-02-07:48:16-root-INFO: Regularization Change: 0.000 -> 0.207
2024-12-02-07:48:16-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-07:48:16-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:48:16-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-07:48:16-root-INFO: grad norm: 234.580 226.091 62.536
2024-12-02-07:48:16-root-INFO: Loss too large (1157.551->1441.222)! Learning rate decreased to 0.00556.
2024-12-02-07:48:16-root-INFO: Loss too large (1157.551->1405.869)! Learning rate decreased to 0.00445.
2024-12-02-07:48:16-root-INFO: Loss too large (1157.551->1363.155)! Learning rate decreased to 0.00356.
2024-12-02-07:48:17-root-INFO: Loss too large (1157.551->1311.272)! Learning rate decreased to 0.00285.
2024-12-02-07:48:17-root-INFO: Loss too large (1157.551->1254.487)! Learning rate decreased to 0.00228.
2024-12-02-07:48:17-root-INFO: Loss too large (1157.551->1203.670)! Learning rate decreased to 0.00182.
2024-12-02-07:48:17-root-INFO: Loss too large (1157.551->1167.359)! Learning rate decreased to 0.00146.
2024-12-02-07:48:18-root-INFO: grad norm: 218.524 211.545 54.787
2024-12-02-07:48:18-root-INFO: Loss too large (1146.122->1146.343)! Learning rate decreased to 0.00117.
2024-12-02-07:48:18-root-INFO: grad norm: 143.139 138.050 37.828
2024-12-02-07:48:19-root-INFO: grad norm: 88.916 85.782 23.396
2024-12-02-07:48:19-root-INFO: grad norm: 66.216 63.737 17.949
2024-12-02-07:48:19-root-INFO: grad norm: 51.953 49.938 14.330
2024-12-02-07:48:20-root-INFO: grad norm: 45.005 43.168 12.725
2024-12-02-07:48:20-root-INFO: grad norm: 41.129 39.415 11.747
2024-12-02-07:48:21-root-INFO: Loss Change: 1157.551 -> 1118.150
2024-12-02-07:48:21-root-INFO: Regularization Change: 0.000 -> 0.185
2024-12-02-07:48:21-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-07:48:21-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:48:21-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-07:48:21-root-INFO: grad norm: 132.939 127.570 37.400
2024-12-02-07:48:21-root-INFO: Loss too large (1123.979->1337.140)! Learning rate decreased to 0.00579.
2024-12-02-07:48:21-root-INFO: Loss too large (1123.979->1283.717)! Learning rate decreased to 0.00463.
2024-12-02-07:48:22-root-INFO: Loss too large (1123.979->1231.417)! Learning rate decreased to 0.00370.
2024-12-02-07:48:22-root-INFO: Loss too large (1123.979->1187.526)! Learning rate decreased to 0.00296.
2024-12-02-07:48:22-root-INFO: Loss too large (1123.979->1156.017)! Learning rate decreased to 0.00237.
2024-12-02-07:48:22-root-INFO: Loss too large (1123.979->1136.071)! Learning rate decreased to 0.00190.
2024-12-02-07:48:22-root-INFO: Loss too large (1123.979->1124.640)! Learning rate decreased to 0.00152.
2024-12-02-07:48:23-root-INFO: grad norm: 143.779 139.532 34.687
2024-12-02-07:48:23-root-INFO: Loss too large (1118.727->1118.830)! Learning rate decreased to 0.00121.
2024-12-02-07:48:23-root-INFO: grad norm: 113.547 109.256 30.919
2024-12-02-07:48:24-root-INFO: grad norm: 87.075 84.299 21.811
2024-12-02-07:48:24-root-INFO: grad norm: 72.237 69.396 20.057
2024-12-02-07:48:25-root-INFO: grad norm: 60.342 58.270 15.673
2024-12-02-07:48:25-root-INFO: grad norm: 53.000 50.824 15.030
2024-12-02-07:48:26-root-INFO: grad norm: 47.510 45.757 12.788
2024-12-02-07:48:26-root-INFO: Loss Change: 1123.979 -> 1098.265
2024-12-02-07:48:26-root-INFO: Regularization Change: 0.000 -> 0.199
2024-12-02-07:48:26-root-INFO: Undo step: 170
2024-12-02-07:48:26-root-INFO: Undo step: 171
2024-12-02-07:48:26-root-INFO: Undo step: 172
2024-12-02-07:48:26-root-INFO: Undo step: 173
2024-12-02-07:48:26-root-INFO: Undo step: 174
2024-12-02-07:48:26-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-07:48:26-root-INFO: grad norm: 664.980 639.775 181.346
2024-12-02-07:48:27-root-INFO: grad norm: 612.649 590.747 162.347
2024-12-02-07:48:27-root-INFO: grad norm: 296.871 282.237 92.058
2024-12-02-07:48:28-root-INFO: grad norm: 187.667 179.255 55.558
2024-12-02-07:48:28-root-INFO: grad norm: 183.491 177.870 45.070
2024-12-02-07:48:29-root-INFO: grad norm: 273.570 269.048 49.533
2024-12-02-07:48:29-root-INFO: Loss too large (1631.310->1705.960)! Learning rate decreased to 0.00474.
2024-12-02-07:48:29-root-INFO: grad norm: 297.509 289.701 67.714
2024-12-02-07:48:30-root-INFO: grad norm: 317.286 309.381 70.383
2024-12-02-07:48:30-root-INFO: Loss Change: 2360.904 -> 1426.721
2024-12-02-07:48:30-root-INFO: Regularization Change: 0.000 -> 21.280
2024-12-02-07:48:30-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-07:48:30-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:48:30-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-07:48:30-root-INFO: grad norm: 324.808 312.517 88.508
2024-12-02-07:48:30-root-INFO: Loss too large (1427.115->1621.445)! Learning rate decreased to 0.00494.
2024-12-02-07:48:31-root-INFO: Loss too large (1427.115->1436.860)! Learning rate decreased to 0.00395.
2024-12-02-07:48:31-root-INFO: grad norm: 386.226 373.096 99.851
2024-12-02-07:48:31-root-INFO: Loss too large (1329.348->1447.940)! Learning rate decreased to 0.00316.
2024-12-02-07:48:31-root-INFO: Loss too large (1329.348->1408.827)! Learning rate decreased to 0.00253.
2024-12-02-07:48:31-root-INFO: Loss too large (1329.348->1367.203)! Learning rate decreased to 0.00202.
2024-12-02-07:48:32-root-INFO: grad norm: 278.751 263.173 91.882
2024-12-02-07:48:32-root-INFO: grad norm: 158.707 152.835 42.771
2024-12-02-07:48:33-root-INFO: grad norm: 196.744 184.178 69.186
2024-12-02-07:48:33-root-INFO: Loss too large (1246.752->1254.814)! Learning rate decreased to 0.00162.
2024-12-02-07:48:33-root-INFO: grad norm: 200.480 191.849 58.190
2024-12-02-07:48:34-root-INFO: grad norm: 214.849 201.482 74.598
2024-12-02-07:48:34-root-INFO: grad norm: 240.868 230.124 71.136
2024-12-02-07:48:35-root-INFO: Loss too large (1233.966->1239.508)! Learning rate decreased to 0.00129.
2024-12-02-07:48:35-root-INFO: Loss Change: 1427.115 -> 1223.548
2024-12-02-07:48:35-root-INFO: Regularization Change: 0.000 -> 1.487
2024-12-02-07:48:35-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-07:48:35-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:48:35-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-07:48:35-root-INFO: grad norm: 113.875 109.362 31.741
2024-12-02-07:48:35-root-INFO: Loss too large (1209.636->1289.208)! Learning rate decreased to 0.00514.
2024-12-02-07:48:36-root-INFO: Loss too large (1209.636->1250.201)! Learning rate decreased to 0.00411.
2024-12-02-07:48:36-root-INFO: Loss too large (1209.636->1225.767)! Learning rate decreased to 0.00329.
2024-12-02-07:48:36-root-INFO: Loss too large (1209.636->1211.477)! Learning rate decreased to 0.00263.
2024-12-02-07:48:36-root-INFO: grad norm: 186.548 178.492 54.226
2024-12-02-07:48:37-root-INFO: Loss too large (1203.828->1261.302)! Learning rate decreased to 0.00210.
2024-12-02-07:48:37-root-INFO: Loss too large (1203.828->1232.957)! Learning rate decreased to 0.00168.
2024-12-02-07:48:37-root-INFO: Loss too large (1203.828->1213.151)! Learning rate decreased to 0.00135.
2024-12-02-07:48:37-root-INFO: grad norm: 191.329 180.540 63.344
2024-12-02-07:48:38-root-INFO: grad norm: 198.933 190.109 58.591
2024-12-02-07:48:38-root-INFO: grad norm: 207.404 195.752 68.539
2024-12-02-07:48:39-root-INFO: grad norm: 216.503 206.922 63.692
2024-12-02-07:48:39-root-INFO: grad norm: 224.606 211.979 74.249
2024-12-02-07:48:40-root-INFO: grad norm: 231.532 221.296 68.083
2024-12-02-07:48:40-root-INFO: Loss Change: 1209.636 -> 1190.358
2024-12-02-07:48:40-root-INFO: Regularization Change: 0.000 -> 0.401
2024-12-02-07:48:40-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-07:48:40-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-07:48:40-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-07:48:40-root-INFO: grad norm: 193.627 183.959 60.419
2024-12-02-07:48:40-root-INFO: Loss too large (1177.303->1600.614)! Learning rate decreased to 0.00535.
2024-12-02-07:48:41-root-INFO: Loss too large (1177.303->1485.989)! Learning rate decreased to 0.00428.
2024-12-02-07:48:41-root-INFO: Loss too large (1177.303->1391.110)! Learning rate decreased to 0.00342.
2024-12-02-07:48:41-root-INFO: Loss too large (1177.303->1315.796)! Learning rate decreased to 0.00274.
2024-12-02-07:48:41-root-INFO: Loss too large (1177.303->1259.286)! Learning rate decreased to 0.00219.
2024-12-02-07:48:41-root-INFO: Loss too large (1177.303->1219.540)! Learning rate decreased to 0.00175.
2024-12-02-07:48:41-root-INFO: Loss too large (1177.303->1193.490)! Learning rate decreased to 0.00140.
2024-12-02-07:48:42-root-INFO: Loss too large (1177.303->1177.730)! Learning rate decreased to 0.00112.
2024-12-02-07:48:42-root-INFO: grad norm: 149.339 142.417 44.937
2024-12-02-07:48:43-root-INFO: grad norm: 123.326 117.022 38.927
2024-12-02-07:48:43-root-INFO: grad norm: 106.877 102.008 31.892
2024-12-02-07:48:43-root-INFO: grad norm: 95.237 90.374 30.042
2024-12-02-07:48:44-root-INFO: grad norm: 87.329 83.401 25.896
2024-12-02-07:48:44-root-INFO: grad norm: 81.553 77.420 25.634
2024-12-02-07:48:45-root-INFO: grad norm: 77.618 74.144 22.962
2024-12-02-07:48:45-root-INFO: Loss Change: 1177.303 -> 1148.001
2024-12-02-07:48:45-root-INFO: Regularization Change: 0.000 -> 0.170
2024-12-02-07:48:45-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-07:48:45-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:48:45-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-07:48:46-root-INFO: grad norm: 103.427 98.941 30.131
2024-12-02-07:48:46-root-INFO: Loss too large (1142.486->1288.949)! Learning rate decreased to 0.00556.
2024-12-02-07:48:46-root-INFO: Loss too large (1142.486->1244.999)! Learning rate decreased to 0.00445.
2024-12-02-07:48:46-root-INFO: Loss too large (1142.486->1208.360)! Learning rate decreased to 0.00356.
2024-12-02-07:48:46-root-INFO: Loss too large (1142.486->1180.909)! Learning rate decreased to 0.00285.
2024-12-02-07:48:46-root-INFO: Loss too large (1142.486->1162.242)! Learning rate decreased to 0.00228.
2024-12-02-07:48:46-root-INFO: Loss too large (1142.486->1150.554)! Learning rate decreased to 0.00182.
2024-12-02-07:48:47-root-INFO: Loss too large (1142.486->1143.762)! Learning rate decreased to 0.00146.
2024-12-02-07:48:47-root-INFO: grad norm: 135.020 128.793 40.531
2024-12-02-07:48:47-root-INFO: Loss too large (1140.137->1143.473)! Learning rate decreased to 0.00117.
2024-12-02-07:48:48-root-INFO: grad norm: 136.283 130.395 39.624
2024-12-02-07:48:48-root-INFO: grad norm: 138.899 132.324 42.226
2024-12-02-07:48:49-root-INFO: grad norm: 141.486 135.515 40.668
2024-12-02-07:48:49-root-INFO: grad norm: 144.973 138.067 44.212
2024-12-02-07:48:50-root-INFO: grad norm: 148.048 141.874 42.308
2024-12-02-07:48:50-root-INFO: grad norm: 151.889 144.642 46.355
2024-12-02-07:48:50-root-INFO: Loss Change: 1142.486 -> 1127.972
2024-12-02-07:48:50-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-07:48:50-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-07:48:50-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:48:50-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-07:48:51-root-INFO: grad norm: 269.545 258.500 76.368
2024-12-02-07:48:51-root-INFO: Loss too large (1143.398->1438.576)! Learning rate decreased to 0.00579.
2024-12-02-07:48:51-root-INFO: Loss too large (1143.398->1405.559)! Learning rate decreased to 0.00463.
2024-12-02-07:48:51-root-INFO: Loss too large (1143.398->1370.381)! Learning rate decreased to 0.00370.
2024-12-02-07:48:51-root-INFO: Loss too large (1143.398->1330.674)! Learning rate decreased to 0.00296.
2024-12-02-07:48:51-root-INFO: Loss too large (1143.398->1285.314)! Learning rate decreased to 0.00237.
2024-12-02-07:48:52-root-INFO: Loss too large (1143.398->1236.310)! Learning rate decreased to 0.00190.
2024-12-02-07:48:52-root-INFO: Loss too large (1143.398->1190.340)! Learning rate decreased to 0.00152.
2024-12-02-07:48:52-root-INFO: Loss too large (1143.398->1154.726)! Learning rate decreased to 0.00121.
2024-12-02-07:48:52-root-INFO: grad norm: 255.659 244.387 75.074
2024-12-02-07:48:53-root-INFO: grad norm: 243.735 234.094 67.872
2024-12-02-07:48:53-root-INFO: grad norm: 229.036 218.569 68.448
2024-12-02-07:48:54-root-INFO: grad norm: 218.373 209.759 60.729
2024-12-02-07:48:54-root-INFO: grad norm: 205.443 195.924 61.810
2024-12-02-07:48:55-root-INFO: grad norm: 196.661 188.855 54.856
2024-12-02-07:48:55-root-INFO: grad norm: 186.503 177.813 56.264
2024-12-02-07:48:56-root-INFO: Loss Change: 1143.398 -> 1108.685
2024-12-02-07:48:56-root-INFO: Regularization Change: 0.000 -> 0.205
2024-12-02-07:48:56-root-INFO: Undo step: 170
2024-12-02-07:48:56-root-INFO: Undo step: 171
2024-12-02-07:48:56-root-INFO: Undo step: 172
2024-12-02-07:48:56-root-INFO: Undo step: 173
2024-12-02-07:48:56-root-INFO: Undo step: 174
2024-12-02-07:48:56-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-07:48:56-root-INFO: grad norm: 599.511 560.012 214.009
2024-12-02-07:48:56-root-INFO: Loss too large (2302.905->2599.734)! Learning rate decreased to 0.00474.
2024-12-02-07:48:57-root-INFO: grad norm: 802.744 762.827 249.985
2024-12-02-07:48:57-root-INFO: Loss too large (2236.417->2499.916)! Learning rate decreased to 0.00379.
2024-12-02-07:48:57-root-INFO: grad norm: 739.936 719.325 173.428
2024-12-02-07:48:58-root-INFO: grad norm: 547.072 534.243 117.783
2024-12-02-07:48:58-root-INFO: grad norm: 574.793 554.430 151.637
2024-12-02-07:48:58-root-INFO: Loss too large (1614.191->1684.679)! Learning rate decreased to 0.00303.
2024-12-02-07:48:59-root-INFO: grad norm: 367.252 349.979 111.306
2024-12-02-07:48:59-root-INFO: grad norm: 331.341 314.952 102.917
2024-12-02-07:48:59-root-INFO: Loss too large (1409.823->1426.500)! Learning rate decreased to 0.00243.
2024-12-02-07:49:00-root-INFO: grad norm: 267.372 248.354 99.034
2024-12-02-07:49:00-root-INFO: Loss Change: 2302.905 -> 1337.468
2024-12-02-07:49:00-root-INFO: Regularization Change: 0.000 -> 16.591
2024-12-02-07:49:00-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-07:49:00-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:49:00-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-07:49:00-root-INFO: grad norm: 263.298 248.815 86.119
2024-12-02-07:49:01-root-INFO: Loss too large (1345.125->1541.772)! Learning rate decreased to 0.00494.
2024-12-02-07:49:01-root-INFO: Loss too large (1345.125->1490.471)! Learning rate decreased to 0.00395.
2024-12-02-07:49:01-root-INFO: Loss too large (1345.125->1436.252)! Learning rate decreased to 0.00316.
2024-12-02-07:49:01-root-INFO: Loss too large (1345.125->1384.469)! Learning rate decreased to 0.00253.
2024-12-02-07:49:02-root-INFO: grad norm: 266.919 248.600 97.180
2024-12-02-07:49:02-root-INFO: grad norm: 268.881 253.666 89.166
2024-12-02-07:49:03-root-INFO: grad norm: 269.896 251.179 98.755
2024-12-02-07:49:03-root-INFO: grad norm: 267.327 252.074 89.007
2024-12-02-07:49:03-root-INFO: grad norm: 269.306 250.474 98.937
2024-12-02-07:49:04-root-INFO: grad norm: 267.323 252.004 89.194
2024-12-02-07:49:04-root-INFO: grad norm: 268.016 249.200 98.652
2024-12-02-07:49:05-root-INFO: Loss Change: 1345.125 -> 1282.004
2024-12-02-07:49:05-root-INFO: Regularization Change: 0.000 -> 1.424
2024-12-02-07:49:05-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-07:49:05-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-07:49:05-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-07:49:05-root-INFO: grad norm: 349.417 329.550 116.143
2024-12-02-07:49:05-root-INFO: Loss too large (1323.143->1565.293)! Learning rate decreased to 0.00514.
2024-12-02-07:49:05-root-INFO: Loss too large (1323.143->1496.770)! Learning rate decreased to 0.00411.
2024-12-02-07:49:06-root-INFO: Loss too large (1323.143->1424.766)! Learning rate decreased to 0.00329.
2024-12-02-07:49:06-root-INFO: Loss too large (1323.143->1350.446)! Learning rate decreased to 0.00263.
2024-12-02-07:49:06-root-INFO: grad norm: 291.835 274.250 99.770
2024-12-02-07:49:07-root-INFO: grad norm: 254.938 241.046 83.010
2024-12-02-07:49:07-root-INFO: grad norm: 244.885 228.452 88.194
2024-12-02-07:49:08-root-INFO: grad norm: 240.891 227.776 78.399
2024-12-02-07:49:08-root-INFO: grad norm: 241.080 224.939 86.729
2024-12-02-07:49:08-root-INFO: Loss too large (1231.439->1232.154)! Learning rate decreased to 0.00210.
2024-12-02-07:49:09-root-INFO: grad norm: 169.482 160.620 54.087
2024-12-02-07:49:09-root-INFO: grad norm: 132.083 122.334 49.801
2024-12-02-07:49:09-root-INFO: Loss Change: 1323.143 -> 1187.464
2024-12-02-07:49:09-root-INFO: Regularization Change: 0.000 -> 1.075
2024-12-02-07:49:09-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-07:49:09-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-07:49:10-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-07:49:10-root-INFO: grad norm: 164.431 155.299 54.036
2024-12-02-07:49:10-root-INFO: Loss too large (1190.132->1365.561)! Learning rate decreased to 0.00535.
2024-12-02-07:49:10-root-INFO: Loss too large (1190.132->1310.560)! Learning rate decreased to 0.00428.
2024-12-02-07:49:10-root-INFO: Loss too large (1190.132->1259.204)! Learning rate decreased to 0.00342.
2024-12-02-07:49:10-root-INFO: Loss too large (1190.132->1219.436)! Learning rate decreased to 0.00274.
2024-12-02-07:49:10-root-INFO: Loss too large (1190.132->1193.754)! Learning rate decreased to 0.00219.
2024-12-02-07:49:11-root-INFO: grad norm: 148.090 139.004 51.075
2024-12-02-07:49:11-root-INFO: grad norm: 141.996 134.146 46.560
2024-12-02-07:49:12-root-INFO: grad norm: 139.217 130.618 48.171
2024-12-02-07:49:12-root-INFO: grad norm: 139.961 132.223 45.896
2024-12-02-07:49:13-root-INFO: grad norm: 143.814 135.083 49.347
2024-12-02-07:49:13-root-INFO: grad norm: 148.305 140.086 48.686
2024-12-02-07:49:14-root-INFO: grad norm: 155.587 146.303 52.941
2024-12-02-07:49:14-root-INFO: Loss too large (1156.117->1156.146)! Learning rate decreased to 0.00175.
2024-12-02-07:49:14-root-INFO: Loss Change: 1190.132 -> 1148.204
2024-12-02-07:49:14-root-INFO: Regularization Change: 0.000 -> 0.541
2024-12-02-07:49:14-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-07:49:14-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:49:14-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-07:49:15-root-INFO: grad norm: 186.941 177.107 59.833
2024-12-02-07:49:15-root-INFO: Loss too large (1156.245->1374.286)! Learning rate decreased to 0.00556.
2024-12-02-07:49:15-root-INFO: Loss too large (1156.245->1319.303)! Learning rate decreased to 0.00445.
2024-12-02-07:49:15-root-INFO: Loss too large (1156.245->1260.861)! Learning rate decreased to 0.00356.
2024-12-02-07:49:15-root-INFO: Loss too large (1156.245->1208.132)! Learning rate decreased to 0.00285.
2024-12-02-07:49:15-root-INFO: Loss too large (1156.245->1169.967)! Learning rate decreased to 0.00228.
2024-12-02-07:49:16-root-INFO: grad norm: 191.471 181.695 60.400
2024-12-02-07:49:16-root-INFO: Loss too large (1147.399->1150.314)! Learning rate decreased to 0.00182.
2024-12-02-07:49:16-root-INFO: grad norm: 136.172 129.133 43.214
2024-12-02-07:49:17-root-INFO: grad norm: 91.742 87.052 28.957
2024-12-02-07:49:17-root-INFO: grad norm: 73.576 69.885 23.010
2024-12-02-07:49:18-root-INFO: grad norm: 60.224 57.336 18.424
2024-12-02-07:49:18-root-INFO: grad norm: 53.029 50.437 16.375
2024-12-02-07:49:19-root-INFO: grad norm: 47.915 45.760 14.209
2024-12-02-07:49:19-root-INFO: Loss Change: 1156.245 -> 1110.442
2024-12-02-07:49:19-root-INFO: Regularization Change: 0.000 -> 0.400
2024-12-02-07:49:19-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-07:49:19-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:49:19-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-07:49:19-root-INFO: grad norm: 126.330 119.730 40.301
2024-12-02-07:49:20-root-INFO: Loss too large (1114.028->1271.342)! Learning rate decreased to 0.00579.
2024-12-02-07:49:20-root-INFO: Loss too large (1114.028->1215.239)! Learning rate decreased to 0.00463.
2024-12-02-07:49:20-root-INFO: Loss too large (1114.028->1169.838)! Learning rate decreased to 0.00370.
2024-12-02-07:49:20-root-INFO: Loss too large (1114.028->1138.602)! Learning rate decreased to 0.00296.
2024-12-02-07:49:20-root-INFO: Loss too large (1114.028->1119.748)! Learning rate decreased to 0.00237.
2024-12-02-07:49:21-root-INFO: grad norm: 151.452 144.972 43.826
2024-12-02-07:49:21-root-INFO: Loss too large (1109.551->1114.831)! Learning rate decreased to 0.00190.
2024-12-02-07:49:21-root-INFO: grad norm: 129.759 123.097 41.042
2024-12-02-07:49:22-root-INFO: grad norm: 106.705 102.023 31.259
2024-12-02-07:49:22-root-INFO: grad norm: 95.096 90.281 29.874
2024-12-02-07:49:23-root-INFO: grad norm: 83.853 80.193 24.502
2024-12-02-07:49:23-root-INFO: grad norm: 77.330 73.447 24.197
2024-12-02-07:49:24-root-INFO: grad norm: 71.325 68.253 20.707
2024-12-02-07:49:24-root-INFO: Loss Change: 1114.028 -> 1081.814
2024-12-02-07:49:24-root-INFO: Regularization Change: 0.000 -> 0.397
2024-12-02-07:49:24-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-07:49:24-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:49:24-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-07:49:24-root-INFO: grad norm: 137.693 130.895 42.732
2024-12-02-07:49:25-root-INFO: Loss too large (1088.271->1289.266)! Learning rate decreased to 0.00602.
2024-12-02-07:49:25-root-INFO: Loss too large (1088.271->1232.999)! Learning rate decreased to 0.00482.
2024-12-02-07:49:25-root-INFO: Loss too large (1088.271->1178.124)! Learning rate decreased to 0.00385.
2024-12-02-07:49:25-root-INFO: Loss too large (1088.271->1134.201)! Learning rate decreased to 0.00308.
2024-12-02-07:49:25-root-INFO: Loss too large (1088.271->1105.113)! Learning rate decreased to 0.00247.
2024-12-02-07:49:25-root-INFO: Loss too large (1088.271->1088.441)! Learning rate decreased to 0.00197.
2024-12-02-07:49:26-root-INFO: grad norm: 121.193 116.834 32.214
2024-12-02-07:49:26-root-INFO: grad norm: 113.527 107.961 35.110
2024-12-02-07:49:27-root-INFO: grad norm: 104.622 100.689 28.414
2024-12-02-07:49:27-root-INFO: grad norm: 99.631 94.840 30.526
2024-12-02-07:49:28-root-INFO: grad norm: 94.238 90.619 25.864
2024-12-02-07:49:28-root-INFO: grad norm: 91.228 86.890 27.795
2024-12-02-07:49:29-root-INFO: grad norm: 88.291 84.863 24.364
2024-12-02-07:49:29-root-INFO: Loss Change: 1088.271 -> 1059.316
2024-12-02-07:49:29-root-INFO: Regularization Change: 0.000 -> 0.347
2024-12-02-07:49:29-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-07:49:29-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:49:29-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-07:49:29-root-INFO: grad norm: 141.570 135.277 41.740
2024-12-02-07:49:30-root-INFO: Loss too large (1065.326->1283.965)! Learning rate decreased to 0.00626.
2024-12-02-07:49:30-root-INFO: Loss too large (1065.326->1232.396)! Learning rate decreased to 0.00501.
2024-12-02-07:49:30-root-INFO: Loss too large (1065.326->1175.554)! Learning rate decreased to 0.00401.
2024-12-02-07:49:30-root-INFO: Loss too large (1065.326->1124.936)! Learning rate decreased to 0.00321.
2024-12-02-07:49:30-root-INFO: Loss too large (1065.326->1089.332)! Learning rate decreased to 0.00256.
2024-12-02-07:49:30-root-INFO: Loss too large (1065.326->1068.417)! Learning rate decreased to 0.00205.
2024-12-02-07:49:31-root-INFO: grad norm: 130.306 125.791 34.004
2024-12-02-07:49:31-root-INFO: grad norm: 124.288 118.831 36.425
2024-12-02-07:49:32-root-INFO: grad norm: 115.908 111.779 30.662
2024-12-02-07:49:32-root-INFO: grad norm: 111.655 106.785 32.615
2024-12-02-07:49:33-root-INFO: grad norm: 106.405 102.559 28.349
2024-12-02-07:49:33-root-INFO: grad norm: 103.798 99.297 30.235
2024-12-02-07:49:34-root-INFO: grad norm: 100.973 97.294 27.007
2024-12-02-07:49:34-root-INFO: Loss Change: 1065.326 -> 1038.931
2024-12-02-07:49:34-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-02-07:49:34-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-07:49:34-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-07:49:34-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-07:49:34-root-INFO: grad norm: 173.053 165.774 49.660
2024-12-02-07:49:35-root-INFO: Loss too large (1052.426->1315.647)! Learning rate decreased to 0.00651.
2024-12-02-07:49:35-root-INFO: Loss too large (1052.426->1261.341)! Learning rate decreased to 0.00521.
2024-12-02-07:49:35-root-INFO: Loss too large (1052.426->1198.658)! Learning rate decreased to 0.00417.
2024-12-02-07:49:35-root-INFO: Loss too large (1052.426->1134.844)! Learning rate decreased to 0.00333.
2024-12-02-07:49:35-root-INFO: Loss too large (1052.426->1084.547)! Learning rate decreased to 0.00267.
2024-12-02-07:49:35-root-INFO: Loss too large (1052.426->1053.683)! Learning rate decreased to 0.00213.
2024-12-02-07:49:36-root-INFO: grad norm: 152.095 147.413 37.448
2024-12-02-07:49:36-root-INFO: Loss too large (1038.012->1038.116)! Learning rate decreased to 0.00171.
2024-12-02-07:49:36-root-INFO: grad norm: 103.309 98.666 30.624
2024-12-02-07:49:37-root-INFO: grad norm: 62.610 60.771 15.066
2024-12-02-07:49:37-root-INFO: grad norm: 48.889 46.424 15.326
2024-12-02-07:49:38-root-INFO: grad norm: 40.543 39.185 10.404
2024-12-02-07:49:38-root-INFO: grad norm: 36.850 35.022 11.463
2024-12-02-07:49:39-root-INFO: grad norm: 34.827 33.468 9.635
2024-12-02-07:49:39-root-INFO: Loss Change: 1052.426 -> 1013.125
2024-12-02-07:49:39-root-INFO: Regularization Change: 0.000 -> 0.315
2024-12-02-07:49:39-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-07:49:39-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:49:39-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-07:49:39-root-INFO: grad norm: 100.642 96.362 29.037
2024-12-02-07:49:40-root-INFO: Loss too large (1015.663->1176.269)! Learning rate decreased to 0.00677.
2024-12-02-07:49:40-root-INFO: Loss too large (1015.663->1117.431)! Learning rate decreased to 0.00542.
2024-12-02-07:49:40-root-INFO: Loss too large (1015.663->1071.370)! Learning rate decreased to 0.00433.
2024-12-02-07:49:40-root-INFO: Loss too large (1015.663->1041.200)! Learning rate decreased to 0.00347.
2024-12-02-07:49:40-root-INFO: Loss too large (1015.663->1023.678)! Learning rate decreased to 0.00277.
2024-12-02-07:49:41-root-INFO: grad norm: 148.861 144.466 35.906
2024-12-02-07:49:41-root-INFO: Loss too large (1014.352->1028.879)! Learning rate decreased to 0.00222.
2024-12-02-07:49:41-root-INFO: Loss too large (1014.352->1016.505)! Learning rate decreased to 0.00177.
2024-12-02-07:49:41-root-INFO: grad norm: 107.339 102.927 30.460
2024-12-02-07:49:42-root-INFO: grad norm: 67.495 65.529 16.173
2024-12-02-07:49:42-root-INFO: grad norm: 53.667 51.112 16.361
2024-12-02-07:49:43-root-INFO: grad norm: 43.627 42.197 11.079
2024-12-02-07:49:43-root-INFO: grad norm: 38.819 36.852 12.199
2024-12-02-07:49:44-root-INFO: grad norm: 35.764 34.389 9.823
2024-12-02-07:49:44-root-INFO: Loss Change: 1015.663 -> 990.672
2024-12-02-07:49:44-root-INFO: Regularization Change: 0.000 -> 0.301
2024-12-02-07:49:44-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-07:49:44-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:49:44-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-07:49:44-root-INFO: grad norm: 82.629 79.096 23.903
2024-12-02-07:49:45-root-INFO: Loss too large (994.074->1123.513)! Learning rate decreased to 0.00704.
2024-12-02-07:49:45-root-INFO: Loss too large (994.074->1072.615)! Learning rate decreased to 0.00563.
2024-12-02-07:49:45-root-INFO: Loss too large (994.074->1036.054)! Learning rate decreased to 0.00450.
2024-12-02-07:49:45-root-INFO: Loss too large (994.074->1013.345)! Learning rate decreased to 0.00360.
2024-12-02-07:49:45-root-INFO: Loss too large (994.074->1000.445)! Learning rate decreased to 0.00288.
2024-12-02-07:49:46-root-INFO: grad norm: 132.541 128.867 30.989
2024-12-02-07:49:46-root-INFO: Loss too large (993.609->1007.079)! Learning rate decreased to 0.00231.
2024-12-02-07:49:46-root-INFO: Loss too large (993.609->996.416)! Learning rate decreased to 0.00184.
2024-12-02-07:49:46-root-INFO: grad norm: 102.423 98.352 28.590
2024-12-02-07:49:47-root-INFO: grad norm: 70.131 68.217 16.271
2024-12-02-07:49:47-root-INFO: grad norm: 57.844 55.242 17.151
2024-12-02-07:49:48-root-INFO: grad norm: 47.340 45.954 11.372
2024-12-02-07:49:48-root-INFO: grad norm: 41.893 39.864 12.879
2024-12-02-07:49:49-root-INFO: grad norm: 37.770 36.510 9.672
2024-12-02-07:49:49-root-INFO: Loss Change: 994.074 -> 972.672
2024-12-02-07:49:49-root-INFO: Regularization Change: 0.000 -> 0.288
2024-12-02-07:49:49-root-INFO: Undo step: 165
2024-12-02-07:49:49-root-INFO: Undo step: 166
2024-12-02-07:49:49-root-INFO: Undo step: 167
2024-12-02-07:49:49-root-INFO: Undo step: 168
2024-12-02-07:49:49-root-INFO: Undo step: 169
2024-12-02-07:49:49-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-07:49:50-root-INFO: grad norm: 612.940 597.248 137.808
2024-12-02-07:49:50-root-INFO: Loss too large (2021.563->2097.097)! Learning rate decreased to 0.00579.
2024-12-02-07:49:50-root-INFO: grad norm: 510.017 499.666 102.228
2024-12-02-07:49:51-root-INFO: grad norm: 512.089 501.550 103.354
2024-12-02-07:49:51-root-INFO: grad norm: 385.205 380.659 59.007
2024-12-02-07:49:52-root-INFO: grad norm: 251.179 242.820 64.258
2024-12-02-07:49:52-root-INFO: Loss too large (1256.613->1269.330)! Learning rate decreased to 0.00463.
2024-12-02-07:49:52-root-INFO: grad norm: 287.564 279.105 69.236
2024-12-02-07:49:52-root-INFO: Loss too large (1205.915->1310.441)! Learning rate decreased to 0.00370.
2024-12-02-07:49:53-root-INFO: Loss too large (1205.915->1241.546)! Learning rate decreased to 0.00296.
2024-12-02-07:49:53-root-INFO: grad norm: 307.586 295.621 84.955
2024-12-02-07:49:53-root-INFO: Loss too large (1182.636->1193.002)! Learning rate decreased to 0.00237.
2024-12-02-07:49:54-root-INFO: grad norm: 219.430 213.540 50.496
2024-12-02-07:49:54-root-INFO: Loss Change: 2021.563 -> 1115.777
2024-12-02-07:49:54-root-INFO: Regularization Change: 0.000 -> 18.551
2024-12-02-07:49:54-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-07:49:54-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:49:54-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-07:49:54-root-INFO: grad norm: 104.913 100.181 31.155
2024-12-02-07:49:54-root-INFO: Loss too large (1103.574->1159.058)! Learning rate decreased to 0.00602.
2024-12-02-07:49:55-root-INFO: Loss too large (1103.574->1130.594)! Learning rate decreased to 0.00482.
2024-12-02-07:49:55-root-INFO: Loss too large (1103.574->1112.529)! Learning rate decreased to 0.00385.
2024-12-02-07:49:55-root-INFO: grad norm: 160.057 155.763 36.826
2024-12-02-07:49:55-root-INFO: Loss too large (1101.842->1135.556)! Learning rate decreased to 0.00308.
2024-12-02-07:49:56-root-INFO: Loss too large (1101.842->1105.349)! Learning rate decreased to 0.00247.
2024-12-02-07:49:56-root-INFO: grad norm: 156.806 150.420 44.294
2024-12-02-07:49:57-root-INFO: grad norm: 157.095 152.793 36.512
2024-12-02-07:49:57-root-INFO: grad norm: 160.858 154.509 44.748
2024-12-02-07:49:57-root-INFO: Loss too large (1077.675->1078.517)! Learning rate decreased to 0.00197.
2024-12-02-07:49:58-root-INFO: grad norm: 119.675 116.328 28.105
2024-12-02-07:49:58-root-INFO: grad norm: 84.462 80.557 25.384
2024-12-02-07:49:59-root-INFO: grad norm: 70.967 68.701 17.790
2024-12-02-07:49:59-root-INFO: Loss Change: 1103.574 -> 1049.768
2024-12-02-07:49:59-root-INFO: Regularization Change: 0.000 -> 0.934
2024-12-02-07:49:59-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-07:49:59-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:49:59-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-07:49:59-root-INFO: grad norm: 53.962 51.889 14.812
2024-12-02-07:50:00-root-INFO: grad norm: 168.401 162.739 43.298
2024-12-02-07:50:00-root-INFO: Loss too large (1043.480->1294.828)! Learning rate decreased to 0.00626.
2024-12-02-07:50:00-root-INFO: Loss too large (1043.480->1209.307)! Learning rate decreased to 0.00501.
2024-12-02-07:50:00-root-INFO: Loss too large (1043.480->1146.625)! Learning rate decreased to 0.00401.
2024-12-02-07:50:00-root-INFO: Loss too large (1043.480->1101.869)! Learning rate decreased to 0.00321.
2024-12-02-07:50:00-root-INFO: Loss too large (1043.480->1071.172)! Learning rate decreased to 0.00256.
2024-12-02-07:50:01-root-INFO: Loss too large (1043.480->1051.226)! Learning rate decreased to 0.00205.
2024-12-02-07:50:01-root-INFO: grad norm: 140.881 137.108 32.385
2024-12-02-07:50:02-root-INFO: grad norm: 110.791 106.894 29.128
2024-12-02-07:50:02-root-INFO: grad norm: 99.680 96.824 23.689
2024-12-02-07:50:03-root-INFO: grad norm: 87.505 84.299 23.467
2024-12-02-07:50:03-root-INFO: grad norm: 81.337 78.898 19.771
2024-12-02-07:50:03-root-INFO: grad norm: 75.035 72.212 20.390
2024-12-02-07:50:04-root-INFO: Loss Change: 1045.471 -> 1010.240
2024-12-02-07:50:04-root-INFO: Regularization Change: 0.000 -> 0.818
2024-12-02-07:50:04-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-07:50:04-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-07:50:04-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-07:50:04-root-INFO: grad norm: 144.677 140.264 35.459
2024-12-02-07:50:04-root-INFO: Loss too large (1017.828->1256.127)! Learning rate decreased to 0.00651.
2024-12-02-07:50:05-root-INFO: Loss too large (1017.828->1191.498)! Learning rate decreased to 0.00521.
2024-12-02-07:50:05-root-INFO: Loss too large (1017.828->1125.545)! Learning rate decreased to 0.00417.
2024-12-02-07:50:05-root-INFO: Loss too large (1017.828->1071.390)! Learning rate decreased to 0.00333.
2024-12-02-07:50:05-root-INFO: Loss too large (1017.828->1035.902)! Learning rate decreased to 0.00267.
2024-12-02-07:50:05-root-INFO: grad norm: 202.058 196.443 47.300
2024-12-02-07:50:06-root-INFO: Loss too large (1016.163->1037.720)! Learning rate decreased to 0.00213.
2024-12-02-07:50:06-root-INFO: Loss too large (1016.163->1018.079)! Learning rate decreased to 0.00171.
2024-12-02-07:50:06-root-INFO: grad norm: 129.078 125.446 30.403
2024-12-02-07:50:07-root-INFO: grad norm: 65.487 63.189 17.194
2024-12-02-07:50:07-root-INFO: grad norm: 50.087 48.136 13.845
2024-12-02-07:50:08-root-INFO: grad norm: 41.561 39.704 12.285
2024-12-02-07:50:08-root-INFO: grad norm: 38.255 36.476 11.530
2024-12-02-07:50:09-root-INFO: grad norm: 36.611 34.840 11.248
2024-12-02-07:50:09-root-INFO: Loss Change: 1017.828 -> 981.963
2024-12-02-07:50:09-root-INFO: Regularization Change: 0.000 -> 0.360
2024-12-02-07:50:09-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-07:50:09-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:50:09-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-07:50:09-root-INFO: grad norm: 95.466 92.498 23.620
2024-12-02-07:50:10-root-INFO: Loss too large (981.999->1115.218)! Learning rate decreased to 0.00677.
2024-12-02-07:50:10-root-INFO: Loss too large (981.999->1058.925)! Learning rate decreased to 0.00542.
2024-12-02-07:50:10-root-INFO: Loss too large (981.999->1020.483)! Learning rate decreased to 0.00433.
2024-12-02-07:50:10-root-INFO: Loss too large (981.999->997.379)! Learning rate decreased to 0.00347.
2024-12-02-07:50:10-root-INFO: Loss too large (981.999->984.608)! Learning rate decreased to 0.00277.
2024-12-02-07:50:11-root-INFO: grad norm: 126.048 122.733 28.721
2024-12-02-07:50:11-root-INFO: Loss too large (978.093->986.149)! Learning rate decreased to 0.00222.
2024-12-02-07:50:11-root-INFO: grad norm: 123.420 120.144 28.248
2024-12-02-07:50:12-root-INFO: grad norm: 119.256 116.051 27.463
2024-12-02-07:50:12-root-INFO: grad norm: 117.110 114.020 26.727
2024-12-02-07:50:13-root-INFO: grad norm: 113.880 110.757 26.485
2024-12-02-07:50:13-root-INFO: grad norm: 112.334 109.370 25.632
2024-12-02-07:50:14-root-INFO: grad norm: 110.240 107.180 25.797
2024-12-02-07:50:14-root-INFO: Loss Change: 981.999 -> 959.591
2024-12-02-07:50:14-root-INFO: Regularization Change: 0.000 -> 0.448
2024-12-02-07:50:14-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-07:50:14-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:50:14-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-07:50:14-root-INFO: grad norm: 152.624 148.719 34.304
2024-12-02-07:50:14-root-INFO: Loss too large (967.546->1236.840)! Learning rate decreased to 0.00704.
2024-12-02-07:50:15-root-INFO: Loss too large (967.546->1184.847)! Learning rate decreased to 0.00563.
2024-12-02-07:50:15-root-INFO: Loss too large (967.546->1120.255)! Learning rate decreased to 0.00450.
2024-12-02-07:50:15-root-INFO: Loss too large (967.546->1052.871)! Learning rate decreased to 0.00360.
2024-12-02-07:50:15-root-INFO: Loss too large (967.546->1001.345)! Learning rate decreased to 0.00288.
2024-12-02-07:50:15-root-INFO: Loss too large (967.546->971.143)! Learning rate decreased to 0.00231.
2024-12-02-07:50:16-root-INFO: grad norm: 143.369 139.899 31.350
2024-12-02-07:50:16-root-INFO: Loss too large (956.291->958.341)! Learning rate decreased to 0.00184.
2024-12-02-07:50:16-root-INFO: grad norm: 102.095 99.374 23.412
2024-12-02-07:50:17-root-INFO: grad norm: 61.674 59.863 14.837
2024-12-02-07:50:17-root-INFO: grad norm: 48.899 47.171 12.883
2024-12-02-07:50:18-root-INFO: grad norm: 40.033 38.513 10.925
2024-12-02-07:50:18-root-INFO: grad norm: 36.050 34.492 10.485
2024-12-02-07:50:19-root-INFO: grad norm: 33.695 32.205 9.910
2024-12-02-07:50:19-root-INFO: Loss Change: 967.546 -> 933.323
2024-12-02-07:50:19-root-INFO: Regularization Change: 0.000 -> 0.305
2024-12-02-07:50:19-root-INFO: Undo step: 165
2024-12-02-07:50:19-root-INFO: Undo step: 166
2024-12-02-07:50:19-root-INFO: Undo step: 167
2024-12-02-07:50:19-root-INFO: Undo step: 168
2024-12-02-07:50:19-root-INFO: Undo step: 169
2024-12-02-07:50:19-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-07:50:19-root-INFO: grad norm: 497.560 479.597 132.487
2024-12-02-07:50:20-root-INFO: grad norm: 574.727 567.919 88.200
2024-12-02-07:50:20-root-INFO: grad norm: 280.146 273.860 59.012
2024-12-02-07:50:21-root-INFO: grad norm: 413.991 401.498 100.935
2024-12-02-07:50:21-root-INFO: Loss too large (1372.141->1904.869)! Learning rate decreased to 0.00579.
2024-12-02-07:50:21-root-INFO: Loss too large (1372.141->1712.168)! Learning rate decreased to 0.00463.
2024-12-02-07:50:21-root-INFO: Loss too large (1372.141->1566.210)! Learning rate decreased to 0.00370.
2024-12-02-07:50:21-root-INFO: Loss too large (1372.141->1454.202)! Learning rate decreased to 0.00296.
2024-12-02-07:50:22-root-INFO: grad norm: 356.851 349.146 73.752
2024-12-02-07:50:22-root-INFO: grad norm: 334.829 321.524 93.449
2024-12-02-07:50:22-root-INFO: Loss too large (1231.056->1287.023)! Learning rate decreased to 0.00237.
2024-12-02-07:50:22-root-INFO: Loss too large (1231.056->1235.060)! Learning rate decreased to 0.00190.
2024-12-02-07:50:23-root-INFO: grad norm: 233.410 225.250 61.179
2024-12-02-07:50:23-root-INFO: grad norm: 135.627 129.830 39.228
2024-12-02-07:50:24-root-INFO: Loss Change: 2001.810 -> 1142.559
2024-12-02-07:50:24-root-INFO: Regularization Change: 0.000 -> 18.490
2024-12-02-07:50:24-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-07:50:24-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:50:24-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-07:50:24-root-INFO: grad norm: 162.315 156.256 43.936
2024-12-02-07:50:24-root-INFO: Loss too large (1140.926->1332.737)! Learning rate decreased to 0.00602.
2024-12-02-07:50:24-root-INFO: Loss too large (1140.926->1272.452)! Learning rate decreased to 0.00482.
2024-12-02-07:50:25-root-INFO: Loss too large (1140.926->1214.170)! Learning rate decreased to 0.00385.
2024-12-02-07:50:25-root-INFO: Loss too large (1140.926->1169.523)! Learning rate decreased to 0.00308.
2024-12-02-07:50:25-root-INFO: Loss too large (1140.926->1141.590)! Learning rate decreased to 0.00247.
2024-12-02-07:50:25-root-INFO: grad norm: 184.612 178.289 47.903
2024-12-02-07:50:26-root-INFO: grad norm: 186.409 179.760 49.344
2024-12-02-07:50:26-root-INFO: grad norm: 181.804 175.370 47.938
2024-12-02-07:50:27-root-INFO: grad norm: 174.621 168.658 45.244
2024-12-02-07:50:27-root-INFO: grad norm: 159.612 153.893 42.344
2024-12-02-07:50:28-root-INFO: grad norm: 152.874 147.807 39.033
2024-12-02-07:50:28-root-INFO: grad norm: 143.291 138.175 37.945
2024-12-02-07:50:28-root-INFO: Loss Change: 1140.926 -> 1056.688
2024-12-02-07:50:28-root-INFO: Regularization Change: 0.000 -> 1.670
2024-12-02-07:50:28-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-07:50:28-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-07:50:29-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-07:50:29-root-INFO: grad norm: 180.285 175.034 43.196
2024-12-02-07:50:29-root-INFO: Loss too large (1063.632->1321.798)! Learning rate decreased to 0.00626.
2024-12-02-07:50:29-root-INFO: Loss too large (1063.632->1254.187)! Learning rate decreased to 0.00501.
2024-12-02-07:50:29-root-INFO: Loss too large (1063.632->1175.693)! Learning rate decreased to 0.00401.
2024-12-02-07:50:29-root-INFO: Loss too large (1063.632->1108.510)! Learning rate decreased to 0.00321.
2024-12-02-07:50:29-root-INFO: Loss too large (1063.632->1066.505)! Learning rate decreased to 0.00256.
2024-12-02-07:50:30-root-INFO: grad norm: 164.699 159.799 39.878
2024-12-02-07:50:30-root-INFO: grad norm: 157.136 152.589 37.526
2024-12-02-07:50:31-root-INFO: grad norm: 148.375 143.891 36.200
2024-12-02-07:50:31-root-INFO: grad norm: 145.074 140.930 34.428
2024-12-02-07:50:32-root-INFO: grad norm: 141.418 137.183 34.349
2024-12-02-07:50:32-root-INFO: grad norm: 140.317 136.361 33.085
2024-12-02-07:50:33-root-INFO: grad norm: 139.797 135.684 33.662
2024-12-02-07:50:33-root-INFO: Loss Change: 1063.632 -> 1006.591
2024-12-02-07:50:33-root-INFO: Regularization Change: 0.000 -> 0.967
2024-12-02-07:50:33-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-07:50:33-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-07:50:33-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-07:50:33-root-INFO: grad norm: 200.133 194.713 46.259
2024-12-02-07:50:34-root-INFO: Loss too large (1018.007->1326.966)! Learning rate decreased to 0.00651.
2024-12-02-07:50:34-root-INFO: Loss too large (1018.007->1255.533)! Learning rate decreased to 0.00521.
2024-12-02-07:50:34-root-INFO: Loss too large (1018.007->1169.726)! Learning rate decreased to 0.00417.
2024-12-02-07:50:34-root-INFO: Loss too large (1018.007->1086.816)! Learning rate decreased to 0.00333.
2024-12-02-07:50:34-root-INFO: Loss too large (1018.007->1030.422)! Learning rate decreased to 0.00267.
2024-12-02-07:50:35-root-INFO: grad norm: 195.768 191.165 42.200
2024-12-02-07:50:35-root-INFO: Loss too large (1000.514->1004.980)! Learning rate decreased to 0.00213.
2024-12-02-07:50:35-root-INFO: grad norm: 136.834 132.919 32.496
2024-12-02-07:50:36-root-INFO: grad norm: 80.357 78.188 18.543
2024-12-02-07:50:36-root-INFO: grad norm: 63.501 61.376 16.290
2024-12-02-07:50:37-root-INFO: grad norm: 51.278 49.573 13.113
2024-12-02-07:50:37-root-INFO: grad norm: 45.494 43.755 12.459
2024-12-02-07:50:38-root-INFO: grad norm: 41.715 40.089 11.535
2024-12-02-07:50:38-root-INFO: Loss Change: 1018.007 -> 956.382
2024-12-02-07:50:38-root-INFO: Regularization Change: 0.000 -> 0.620
2024-12-02-07:50:38-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-07:50:38-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:50:38-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-07:50:38-root-INFO: grad norm: 95.088 92.428 22.334
2024-12-02-07:50:38-root-INFO: Loss too large (955.027->1055.965)! Learning rate decreased to 0.00677.
2024-12-02-07:50:39-root-INFO: Loss too large (955.027->1007.661)! Learning rate decreased to 0.00542.
2024-12-02-07:50:39-root-INFO: Loss too large (955.027->978.306)! Learning rate decreased to 0.00433.
2024-12-02-07:50:39-root-INFO: Loss too large (955.027->961.565)! Learning rate decreased to 0.00347.
2024-12-02-07:50:39-root-INFO: grad norm: 133.998 131.119 27.628
2024-12-02-07:50:39-root-INFO: Loss too large (952.610->966.267)! Learning rate decreased to 0.00277.
2024-12-02-07:50:40-root-INFO: Loss too large (952.610->953.425)! Learning rate decreased to 0.00222.
2024-12-02-07:50:40-root-INFO: grad norm: 101.530 98.786 23.444
2024-12-02-07:50:41-root-INFO: grad norm: 73.443 71.565 16.502
2024-12-02-07:50:41-root-INFO: grad norm: 61.467 59.508 15.394
2024-12-02-07:50:42-root-INFO: grad norm: 51.222 49.620 12.712
2024-12-02-07:50:42-root-INFO: grad norm: 45.682 43.975 12.370
2024-12-02-07:50:42-root-INFO: grad norm: 41.371 39.833 11.176
2024-12-02-07:50:43-root-INFO: Loss Change: 955.027 -> 923.024
2024-12-02-07:50:43-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-07:50:43-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-07:50:43-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:50:43-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-07:50:43-root-INFO: grad norm: 79.500 77.258 18.748
2024-12-02-07:50:43-root-INFO: Loss too large (924.341->1013.457)! Learning rate decreased to 0.00704.
2024-12-02-07:50:43-root-INFO: Loss too large (924.341->971.243)! Learning rate decreased to 0.00563.
2024-12-02-07:50:44-root-INFO: Loss too large (924.341->945.934)! Learning rate decreased to 0.00450.
2024-12-02-07:50:44-root-INFO: Loss too large (924.341->931.633)! Learning rate decreased to 0.00360.
2024-12-02-07:50:44-root-INFO: grad norm: 128.828 126.233 25.726
2024-12-02-07:50:44-root-INFO: Loss too large (923.986->941.382)! Learning rate decreased to 0.00288.
2024-12-02-07:50:45-root-INFO: Loss too large (923.986->928.015)! Learning rate decreased to 0.00231.
2024-12-02-07:50:45-root-INFO: grad norm: 105.533 102.968 23.127
2024-12-02-07:50:45-root-INFO: grad norm: 80.749 78.944 16.979
2024-12-02-07:50:46-root-INFO: grad norm: 70.017 68.117 16.201
2024-12-02-07:50:46-root-INFO: grad norm: 59.118 57.610 13.269
2024-12-02-07:50:47-root-INFO: grad norm: 53.123 51.504 13.013
2024-12-02-07:50:47-root-INFO: grad norm: 47.528 46.140 11.400
2024-12-02-07:50:48-root-INFO: Loss Change: 924.341 -> 898.796
2024-12-02-07:50:48-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-02-07:50:48-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-07:50:48-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:50:48-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-07:50:48-root-INFO: grad norm: 100.145 97.272 23.817
2024-12-02-07:50:48-root-INFO: Loss too large (900.802->1076.424)! Learning rate decreased to 0.00732.
2024-12-02-07:50:48-root-INFO: Loss too large (900.802->1003.091)! Learning rate decreased to 0.00585.
2024-12-02-07:50:48-root-INFO: Loss too large (900.802->952.511)! Learning rate decreased to 0.00468.
2024-12-02-07:50:48-root-INFO: Loss too large (900.802->922.392)! Learning rate decreased to 0.00375.
2024-12-02-07:50:49-root-INFO: Loss too large (900.802->905.797)! Learning rate decreased to 0.00300.
2024-12-02-07:50:49-root-INFO: grad norm: 122.510 120.248 23.436
2024-12-02-07:50:49-root-INFO: Loss too large (897.274->902.296)! Learning rate decreased to 0.00240.
2024-12-02-07:50:50-root-INFO: grad norm: 104.819 102.233 23.140
2024-12-02-07:50:50-root-INFO: grad norm: 83.463 81.808 16.538
2024-12-02-07:50:51-root-INFO: grad norm: 74.055 72.063 17.061
2024-12-02-07:50:51-root-INFO: grad norm: 63.532 62.132 13.262
2024-12-02-07:50:52-root-INFO: grad norm: 57.774 56.077 13.901
2024-12-02-07:50:52-root-INFO: grad norm: 51.854 50.570 11.466
2024-12-02-07:50:53-root-INFO: Loss Change: 900.802 -> 874.551
2024-12-02-07:50:53-root-INFO: Regularization Change: 0.000 -> 0.409
2024-12-02-07:50:53-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-07:50:53-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-07:50:53-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-07:50:53-root-INFO: grad norm: 81.830 79.772 18.236
2024-12-02-07:50:53-root-INFO: Loss too large (873.269->1010.395)! Learning rate decreased to 0.00760.
2024-12-02-07:50:53-root-INFO: Loss too large (873.269->949.677)! Learning rate decreased to 0.00608.
2024-12-02-07:50:53-root-INFO: Loss too large (873.269->911.123)! Learning rate decreased to 0.00487.
2024-12-02-07:50:53-root-INFO: Loss too large (873.269->888.996)! Learning rate decreased to 0.00389.
2024-12-02-07:50:54-root-INFO: Loss too large (873.269->877.017)! Learning rate decreased to 0.00311.
2024-12-02-07:50:54-root-INFO: grad norm: 102.114 100.307 19.121
2024-12-02-07:50:54-root-INFO: Loss too large (870.921->874.311)! Learning rate decreased to 0.00249.
2024-12-02-07:50:55-root-INFO: grad norm: 88.930 86.856 19.093
2024-12-02-07:50:55-root-INFO: grad norm: 73.356 71.928 14.405
2024-12-02-07:50:56-root-INFO: grad norm: 66.159 64.463 14.888
2024-12-02-07:50:56-root-INFO: grad norm: 58.130 56.864 12.068
2024-12-02-07:50:57-root-INFO: grad norm: 53.651 52.143 12.632
2024-12-02-07:50:57-root-INFO: grad norm: 48.973 47.787 10.710
2024-12-02-07:50:58-root-INFO: Loss Change: 873.269 -> 852.245
2024-12-02-07:50:58-root-INFO: Regularization Change: 0.000 -> 0.362
2024-12-02-07:50:58-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-07:50:58-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:50:58-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-07:50:58-root-INFO: grad norm: 119.671 116.783 26.132
2024-12-02-07:50:58-root-INFO: Loss too large (863.450->1120.061)! Learning rate decreased to 0.00790.
2024-12-02-07:50:58-root-INFO: Loss too large (863.450->1030.537)! Learning rate decreased to 0.00632.
2024-12-02-07:50:58-root-INFO: Loss too large (863.450->953.714)! Learning rate decreased to 0.00506.
2024-12-02-07:50:58-root-INFO: Loss too large (863.450->903.644)! Learning rate decreased to 0.00404.
2024-12-02-07:50:59-root-INFO: Loss too large (863.450->875.211)! Learning rate decreased to 0.00324.
2024-12-02-07:50:59-root-INFO: grad norm: 157.264 154.809 27.680
2024-12-02-07:50:59-root-INFO: Loss too large (860.343->874.390)! Learning rate decreased to 0.00259.
2024-12-02-07:50:59-root-INFO: Loss too large (860.343->860.399)! Learning rate decreased to 0.00207.
2024-12-02-07:51:00-root-INFO: grad norm: 97.123 94.795 21.137
2024-12-02-07:51:00-root-INFO: grad norm: 46.374 45.206 10.343
2024-12-02-07:51:01-root-INFO: grad norm: 35.414 33.881 10.308
2024-12-02-07:51:01-root-INFO: grad norm: 30.065 28.838 8.504
2024-12-02-07:51:02-root-INFO: grad norm: 28.080 26.714 8.652
2024-12-02-07:51:02-root-INFO: grad norm: 27.115 25.865 8.138
2024-12-02-07:51:03-root-INFO: Loss Change: 863.450 -> 835.030
2024-12-02-07:51:03-root-INFO: Regularization Change: 0.000 -> 0.340
2024-12-02-07:51:03-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-07:51:03-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:51:03-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-07:51:03-root-INFO: grad norm: 58.777 57.298 13.106
2024-12-02-07:51:03-root-INFO: Loss too large (834.476->878.915)! Learning rate decreased to 0.00821.
2024-12-02-07:51:03-root-INFO: Loss too large (834.476->856.336)! Learning rate decreased to 0.00656.
2024-12-02-07:51:03-root-INFO: Loss too large (834.476->843.385)! Learning rate decreased to 0.00525.
2024-12-02-07:51:04-root-INFO: Loss too large (834.476->836.247)! Learning rate decreased to 0.00420.
2024-12-02-07:51:04-root-INFO: grad norm: 84.997 83.540 15.671
2024-12-02-07:51:04-root-INFO: Loss too large (832.546->840.270)! Learning rate decreased to 0.00336.
2024-12-02-07:51:04-root-INFO: Loss too large (832.546->833.630)! Learning rate decreased to 0.00269.
2024-12-02-07:51:05-root-INFO: grad norm: 72.193 70.592 15.117
2024-12-02-07:51:05-root-INFO: grad norm: 61.395 60.189 12.109
2024-12-02-07:51:06-root-INFO: grad norm: 55.960 54.575 12.373
2024-12-02-07:51:06-root-INFO: grad norm: 49.833 48.737 10.396
2024-12-02-07:51:07-root-INFO: grad norm: 46.311 45.036 10.793
2024-12-02-07:51:07-root-INFO: grad norm: 42.579 41.536 9.370
2024-12-02-07:51:08-root-INFO: Loss Change: 834.476 -> 816.201
2024-12-02-07:51:08-root-INFO: Regularization Change: 0.000 -> 0.385
2024-12-02-07:51:08-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-07:51:08-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:51:08-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-07:51:08-root-INFO: grad norm: 89.053 87.087 18.607
2024-12-02-07:51:08-root-INFO: Loss too large (821.373->1010.711)! Learning rate decreased to 0.00852.
2024-12-02-07:51:08-root-INFO: Loss too large (821.373->931.332)! Learning rate decreased to 0.00682.
2024-12-02-07:51:09-root-INFO: Loss too large (821.373->876.670)! Learning rate decreased to 0.00545.
2024-12-02-07:51:09-root-INFO: Loss too large (821.373->844.815)! Learning rate decreased to 0.00436.
2024-12-02-07:51:09-root-INFO: Loss too large (821.373->827.598)! Learning rate decreased to 0.00349.
2024-12-02-07:51:09-root-INFO: grad norm: 114.303 112.544 19.976
2024-12-02-07:51:09-root-INFO: Loss too large (818.859->825.892)! Learning rate decreased to 0.00279.
2024-12-02-07:51:10-root-INFO: grad norm: 97.733 95.805 19.315
2024-12-02-07:51:10-root-INFO: grad norm: 74.606 73.372 13.513
2024-12-02-07:51:11-root-INFO: grad norm: 66.733 65.233 14.069
2024-12-02-07:51:11-root-INFO: grad norm: 56.987 55.940 10.875
2024-12-02-07:51:12-root-INFO: grad norm: 52.212 50.899 11.636
2024-12-02-07:51:12-root-INFO: grad norm: 46.795 45.826 9.470
2024-12-02-07:51:13-root-INFO: Loss Change: 821.373 -> 800.200
2024-12-02-07:51:13-root-INFO: Regularization Change: 0.000 -> 0.369
2024-12-02-07:51:13-root-INFO: Undo step: 160
2024-12-02-07:51:13-root-INFO: Undo step: 161
2024-12-02-07:51:13-root-INFO: Undo step: 162
2024-12-02-07:51:13-root-INFO: Undo step: 163
2024-12-02-07:51:13-root-INFO: Undo step: 164
2024-12-02-07:51:13-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-07:51:13-root-INFO: grad norm: 348.919 330.381 112.219
2024-12-02-07:51:14-root-INFO: grad norm: 471.058 456.493 116.231
2024-12-02-07:51:14-root-INFO: Loss too large (1264.972->1855.610)! Learning rate decreased to 0.00704.
2024-12-02-07:51:14-root-INFO: Loss too large (1264.972->1584.775)! Learning rate decreased to 0.00563.
2024-12-02-07:51:14-root-INFO: Loss too large (1264.972->1401.278)! Learning rate decreased to 0.00450.
2024-12-02-07:51:14-root-INFO: Loss too large (1264.972->1276.485)! Learning rate decreased to 0.00360.
2024-12-02-07:51:15-root-INFO: grad norm: 269.858 262.195 63.853
2024-12-02-07:51:15-root-INFO: grad norm: 106.178 101.466 31.281
2024-12-02-07:51:16-root-INFO: grad norm: 107.925 102.852 32.701
2024-12-02-07:51:16-root-INFO: grad norm: 122.921 118.359 33.176
2024-12-02-07:51:17-root-INFO: grad norm: 183.253 175.933 51.276
2024-12-02-07:51:17-root-INFO: Loss too large (950.919->973.002)! Learning rate decreased to 0.00288.
2024-12-02-07:51:17-root-INFO: Loss too large (950.919->952.869)! Learning rate decreased to 0.00231.
2024-12-02-07:51:18-root-INFO: grad norm: 130.710 126.246 33.868
2024-12-02-07:51:18-root-INFO: Loss Change: 1585.302 -> 922.790
2024-12-02-07:51:18-root-INFO: Regularization Change: 0.000 -> 14.032
2024-12-02-07:51:18-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-07:51:18-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:51:18-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-07:51:18-root-INFO: grad norm: 68.195 65.550 18.807
2024-12-02-07:51:19-root-INFO: grad norm: 111.783 110.015 19.802
2024-12-02-07:51:19-root-INFO: Loss too large (906.489->962.721)! Learning rate decreased to 0.00732.
2024-12-02-07:51:19-root-INFO: Loss too large (906.489->925.688)! Learning rate decreased to 0.00585.
2024-12-02-07:51:19-root-INFO: grad norm: 147.128 145.039 24.704
2024-12-02-07:51:20-root-INFO: Loss too large (904.313->937.157)! Learning rate decreased to 0.00468.
2024-12-02-07:51:20-root-INFO: Loss too large (904.313->909.585)! Learning rate decreased to 0.00375.
2024-12-02-07:51:20-root-INFO: grad norm: 134.658 130.900 31.590
2024-12-02-07:51:20-root-INFO: Loss too large (893.683->898.426)! Learning rate decreased to 0.00300.
2024-12-02-07:51:21-root-INFO: grad norm: 137.817 132.897 36.495
2024-12-02-07:51:21-root-INFO: Loss too large (882.691->887.867)! Learning rate decreased to 0.00240.
2024-12-02-07:51:22-root-INFO: grad norm: 107.601 104.064 27.364
2024-12-02-07:51:22-root-INFO: grad norm: 72.335 69.522 19.979
2024-12-02-07:51:22-root-INFO: grad norm: 61.262 59.065 16.260
2024-12-02-07:51:23-root-INFO: Loss Change: 914.519 -> 862.629
2024-12-02-07:51:23-root-INFO: Regularization Change: 0.000 -> 1.574
2024-12-02-07:51:23-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-07:51:23-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-07:51:23-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-07:51:23-root-INFO: grad norm: 47.311 45.808 11.831
2024-12-02-07:51:24-root-INFO: grad norm: 86.150 84.801 15.183
2024-12-02-07:51:24-root-INFO: Loss too large (853.989->904.478)! Learning rate decreased to 0.00760.
2024-12-02-07:51:24-root-INFO: Loss too large (853.989->875.388)! Learning rate decreased to 0.00608.
2024-12-02-07:51:24-root-INFO: Loss too large (853.989->858.566)! Learning rate decreased to 0.00487.
2024-12-02-07:51:25-root-INFO: grad norm: 103.029 100.914 20.767
2024-12-02-07:51:25-root-INFO: Loss too large (849.503->863.268)! Learning rate decreased to 0.00389.
2024-12-02-07:51:25-root-INFO: Loss too large (849.503->851.974)! Learning rate decreased to 0.00311.
2024-12-02-07:51:26-root-INFO: grad norm: 96.374 93.386 23.811
2024-12-02-07:51:26-root-INFO: grad norm: 121.800 117.686 31.386
2024-12-02-07:51:26-root-INFO: Loss too large (843.377->849.554)! Learning rate decreased to 0.00249.
2024-12-02-07:51:27-root-INFO: grad norm: 100.722 97.515 25.213
2024-12-02-07:51:27-root-INFO: grad norm: 73.433 70.834 19.365
2024-12-02-07:51:28-root-INFO: grad norm: 63.943 61.769 16.530
2024-12-02-07:51:28-root-INFO: Loss Change: 857.418 -> 829.000
2024-12-02-07:51:28-root-INFO: Regularization Change: 0.000 -> 0.873
2024-12-02-07:51:28-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-07:51:28-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:51:28-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-07:51:28-root-INFO: grad norm: 76.726 74.176 19.618
2024-12-02-07:51:29-root-INFO: Loss too large (833.139->863.766)! Learning rate decreased to 0.00790.
2024-12-02-07:51:29-root-INFO: Loss too large (833.139->845.086)! Learning rate decreased to 0.00632.
2024-12-02-07:51:29-root-INFO: Loss too large (833.139->834.082)! Learning rate decreased to 0.00506.
2024-12-02-07:51:29-root-INFO: grad norm: 95.413 93.149 20.666
2024-12-02-07:51:30-root-INFO: Loss too large (828.115->841.790)! Learning rate decreased to 0.00404.
2024-12-02-07:51:30-root-INFO: Loss too large (828.115->830.706)! Learning rate decreased to 0.00324.
2024-12-02-07:51:30-root-INFO: grad norm: 91.520 88.542 23.157
2024-12-02-07:51:31-root-INFO: grad norm: 118.011 114.195 29.767
2024-12-02-07:51:31-root-INFO: Loss too large (822.976->829.223)! Learning rate decreased to 0.00259.
2024-12-02-07:51:31-root-INFO: grad norm: 97.652 94.472 24.716
2024-12-02-07:51:32-root-INFO: grad norm: 70.911 68.551 18.144
2024-12-02-07:51:32-root-INFO: grad norm: 61.584 59.429 16.148
2024-12-02-07:51:33-root-INFO: grad norm: 51.720 49.885 13.655
2024-12-02-07:51:33-root-INFO: Loss Change: 833.139 -> 807.705
2024-12-02-07:51:33-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-02-07:51:33-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-07:51:33-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:51:33-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-07:51:34-root-INFO: grad norm: 82.208 79.838 19.597
2024-12-02-07:51:34-root-INFO: Loss too large (808.788->951.563)! Learning rate decreased to 0.00821.
2024-12-02-07:51:34-root-INFO: Loss too large (808.788->886.846)! Learning rate decreased to 0.00656.
2024-12-02-07:51:34-root-INFO: Loss too large (808.788->846.272)! Learning rate decreased to 0.00525.
2024-12-02-07:51:34-root-INFO: Loss too large (808.788->823.346)! Learning rate decreased to 0.00420.
2024-12-02-07:51:34-root-INFO: Loss too large (808.788->811.154)! Learning rate decreased to 0.00336.
2024-12-02-07:51:35-root-INFO: grad norm: 90.016 87.105 22.709
2024-12-02-07:51:35-root-INFO: Loss too large (805.102->806.681)! Learning rate decreased to 0.00269.
2024-12-02-07:51:35-root-INFO: grad norm: 72.011 69.738 17.948
2024-12-02-07:51:36-root-INFO: grad norm: 53.190 51.323 13.968
2024-12-02-07:51:36-root-INFO: grad norm: 45.446 43.880 11.828
2024-12-02-07:51:37-root-INFO: grad norm: 38.304 36.814 10.578
2024-12-02-07:51:37-root-INFO: grad norm: 34.426 33.122 9.382
2024-12-02-07:51:38-root-INFO: grad norm: 31.099 29.777 8.971
2024-12-02-07:51:38-root-INFO: Loss Change: 808.788 -> 788.829
2024-12-02-07:51:38-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-02-07:51:38-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-07:51:38-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:51:38-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-07:51:38-root-INFO: grad norm: 81.461 79.080 19.551
2024-12-02-07:51:39-root-INFO: Loss too large (793.809->925.700)! Learning rate decreased to 0.00852.
2024-12-02-07:51:39-root-INFO: Loss too large (793.809->864.682)! Learning rate decreased to 0.00682.
2024-12-02-07:51:39-root-INFO: Loss too large (793.809->827.374)! Learning rate decreased to 0.00545.
2024-12-02-07:51:39-root-INFO: Loss too large (793.809->806.309)! Learning rate decreased to 0.00436.
2024-12-02-07:51:39-root-INFO: Loss too large (793.809->795.053)! Learning rate decreased to 0.00349.
2024-12-02-07:51:40-root-INFO: grad norm: 87.575 84.935 21.341
2024-12-02-07:51:40-root-INFO: Loss too large (789.464->790.808)! Learning rate decreased to 0.00279.
2024-12-02-07:51:40-root-INFO: grad norm: 69.792 67.568 17.476
2024-12-02-07:51:41-root-INFO: grad norm: 51.108 49.453 12.900
2024-12-02-07:51:41-root-INFO: grad norm: 43.329 41.789 11.449
2024-12-02-07:51:42-root-INFO: grad norm: 36.113 34.815 9.593
2024-12-02-07:51:42-root-INFO: grad norm: 32.214 30.948 8.943
2024-12-02-07:51:43-root-INFO: grad norm: 28.926 27.766 8.109
2024-12-02-07:51:43-root-INFO: Loss Change: 793.809 -> 773.677
2024-12-02-07:51:43-root-INFO: Regularization Change: 0.000 -> 0.345
2024-12-02-07:51:43-root-INFO: Undo step: 160
2024-12-02-07:51:43-root-INFO: Undo step: 161
2024-12-02-07:51:43-root-INFO: Undo step: 162
2024-12-02-07:51:43-root-INFO: Undo step: 163
2024-12-02-07:51:43-root-INFO: Undo step: 164
2024-12-02-07:51:43-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-07:51:43-root-INFO: grad norm: 435.522 418.657 120.026
2024-12-02-07:51:44-root-INFO: grad norm: 446.541 434.022 104.996
2024-12-02-07:51:44-root-INFO: Loss too large (1420.546->1625.299)! Learning rate decreased to 0.00704.
2024-12-02-07:51:44-root-INFO: Loss too large (1420.546->1500.318)! Learning rate decreased to 0.00563.
2024-12-02-07:51:45-root-INFO: grad norm: 274.797 269.282 54.778
2024-12-02-07:51:45-root-INFO: grad norm: 327.805 314.900 91.071
2024-12-02-07:51:45-root-INFO: Loss too large (1071.466->1307.006)! Learning rate decreased to 0.00450.
2024-12-02-07:51:45-root-INFO: Loss too large (1071.466->1201.250)! Learning rate decreased to 0.00360.
2024-12-02-07:51:46-root-INFO: Loss too large (1071.466->1128.281)! Learning rate decreased to 0.00288.
2024-12-02-07:51:46-root-INFO: Loss too large (1071.466->1078.444)! Learning rate decreased to 0.00231.
2024-12-02-07:51:46-root-INFO: grad norm: 244.851 239.321 51.744
2024-12-02-07:51:47-root-INFO: grad norm: 124.498 119.445 35.108
2024-12-02-07:51:47-root-INFO: grad norm: 115.569 112.592 26.064
2024-12-02-07:51:48-root-INFO: grad norm: 108.839 104.861 29.156
2024-12-02-07:51:48-root-INFO: Loss Change: 1659.364 -> 947.576
2024-12-02-07:51:48-root-INFO: Regularization Change: 0.000 -> 14.195
2024-12-02-07:51:48-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-07:51:48-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-07:51:48-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-07:51:48-root-INFO: grad norm: 149.341 144.516 37.654
2024-12-02-07:51:48-root-INFO: Loss too large (950.152->1197.399)! Learning rate decreased to 0.00732.
2024-12-02-07:51:49-root-INFO: Loss too large (950.152->1146.646)! Learning rate decreased to 0.00585.
2024-12-02-07:51:49-root-INFO: Loss too large (950.152->1076.536)! Learning rate decreased to 0.00468.
2024-12-02-07:51:49-root-INFO: Loss too large (950.152->1009.981)! Learning rate decreased to 0.00375.
2024-12-02-07:51:49-root-INFO: Loss too large (950.152->965.727)! Learning rate decreased to 0.00300.
2024-12-02-07:51:50-root-INFO: grad norm: 212.332 206.393 49.868
2024-12-02-07:51:50-root-INFO: Loss too large (942.408->967.573)! Learning rate decreased to 0.00240.
2024-12-02-07:51:50-root-INFO: Loss too large (942.408->945.643)! Learning rate decreased to 0.00192.
2024-12-02-07:51:50-root-INFO: grad norm: 140.318 136.214 33.689
2024-12-02-07:51:51-root-INFO: grad norm: 68.930 66.626 17.671
2024-12-02-07:51:51-root-INFO: grad norm: 57.605 55.399 15.789
2024-12-02-07:51:52-root-INFO: grad norm: 50.634 48.636 14.083
2024-12-02-07:51:52-root-INFO: grad norm: 47.170 45.162 13.617
2024-12-02-07:51:53-root-INFO: grad norm: 44.816 42.888 13.004
2024-12-02-07:51:53-root-INFO: Loss Change: 950.152 -> 891.795
2024-12-02-07:51:53-root-INFO: Regularization Change: 0.000 -> 0.742
2024-12-02-07:51:53-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-07:51:53-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-07:51:53-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-07:51:53-root-INFO: grad norm: 68.291 65.729 18.529
2024-12-02-07:51:54-root-INFO: Loss too large (887.634->943.766)! Learning rate decreased to 0.00760.
2024-12-02-07:51:54-root-INFO: Loss too large (887.634->914.030)! Learning rate decreased to 0.00608.
2024-12-02-07:51:54-root-INFO: Loss too large (887.634->897.252)! Learning rate decreased to 0.00487.
2024-12-02-07:51:54-root-INFO: Loss too large (887.634->888.365)! Learning rate decreased to 0.00389.
2024-12-02-07:51:55-root-INFO: grad norm: 117.436 114.443 26.342
2024-12-02-07:51:55-root-INFO: Loss too large (884.011->903.686)! Learning rate decreased to 0.00311.
2024-12-02-07:51:55-root-INFO: Loss too large (884.011->891.225)! Learning rate decreased to 0.00249.
2024-12-02-07:51:55-root-INFO: grad norm: 116.069 112.717 27.693
2024-12-02-07:51:56-root-INFO: grad norm: 114.074 111.198 25.454
2024-12-02-07:51:56-root-INFO: grad norm: 112.947 109.689 26.932
2024-12-02-07:51:57-root-INFO: grad norm: 111.048 108.239 24.823
2024-12-02-07:51:57-root-INFO: grad norm: 110.127 106.953 26.248
2024-12-02-07:51:58-root-INFO: grad norm: 108.725 105.970 24.320
2024-12-02-07:51:58-root-INFO: Loss too large (862.673->862.728)! Learning rate decreased to 0.00199.
2024-12-02-07:51:58-root-INFO: Loss Change: 887.634 -> 858.779
2024-12-02-07:51:58-root-INFO: Regularization Change: 0.000 -> 0.636
2024-12-02-07:51:58-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-07:51:58-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:51:58-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-07:51:58-root-INFO: grad norm: 141.813 136.941 36.854
2024-12-02-07:51:59-root-INFO: Loss too large (872.970->1155.607)! Learning rate decreased to 0.00790.
2024-12-02-07:51:59-root-INFO: Loss too large (872.970->1093.792)! Learning rate decreased to 0.00632.
2024-12-02-07:51:59-root-INFO: Loss too large (872.970->1014.613)! Learning rate decreased to 0.00506.
2024-12-02-07:51:59-root-INFO: Loss too large (872.970->941.673)! Learning rate decreased to 0.00404.
2024-12-02-07:51:59-root-INFO: Loss too large (872.970->894.344)! Learning rate decreased to 0.00324.
2024-12-02-07:52:00-root-INFO: grad norm: 208.872 204.327 43.337
2024-12-02-07:52:00-root-INFO: Loss too large (869.395->901.713)! Learning rate decreased to 0.00259.
2024-12-02-07:52:00-root-INFO: Loss too large (869.395->878.259)! Learning rate decreased to 0.00207.
2024-12-02-07:52:00-root-INFO: grad norm: 134.256 130.160 32.912
2024-12-02-07:52:01-root-INFO: grad norm: 50.757 49.129 12.751
2024-12-02-07:52:01-root-INFO: grad norm: 41.079 38.867 13.297
2024-12-02-07:52:02-root-INFO: grad norm: 35.590 33.931 10.741
2024-12-02-07:52:02-root-INFO: grad norm: 33.228 31.309 11.129
2024-12-02-07:52:03-root-INFO: grad norm: 31.853 30.172 10.213
2024-12-02-07:52:03-root-INFO: Loss Change: 872.970 -> 833.662
2024-12-02-07:52:03-root-INFO: Regularization Change: 0.000 -> 0.449
2024-12-02-07:52:03-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-07:52:03-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:52:03-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-07:52:03-root-INFO: grad norm: 65.838 63.501 17.384
2024-12-02-07:52:04-root-INFO: Loss too large (833.635->901.237)! Learning rate decreased to 0.00821.
2024-12-02-07:52:04-root-INFO: Loss too large (833.635->867.008)! Learning rate decreased to 0.00656.
2024-12-02-07:52:04-root-INFO: Loss too large (833.635->847.605)! Learning rate decreased to 0.00525.
2024-12-02-07:52:04-root-INFO: Loss too large (833.635->837.118)! Learning rate decreased to 0.00420.
2024-12-02-07:52:05-root-INFO: grad norm: 108.242 105.847 22.642
2024-12-02-07:52:05-root-INFO: Loss too large (831.760->849.513)! Learning rate decreased to 0.00336.
2024-12-02-07:52:05-root-INFO: Loss too large (831.760->838.199)! Learning rate decreased to 0.00269.
2024-12-02-07:52:05-root-INFO: grad norm: 99.806 96.985 23.561
2024-12-02-07:52:06-root-INFO: grad norm: 87.776 85.790 18.567
2024-12-02-07:52:06-root-INFO: grad norm: 83.528 81.074 20.096
2024-12-02-07:52:07-root-INFO: grad norm: 77.337 75.524 16.645
2024-12-02-07:52:07-root-INFO: grad norm: 74.777 72.531 18.189
2024-12-02-07:52:08-root-INFO: grad norm: 71.416 69.701 15.557
2024-12-02-07:52:08-root-INFO: Loss Change: 833.635 -> 813.186
2024-12-02-07:52:08-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-07:52:08-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-07:52:08-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:52:08-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-07:52:08-root-INFO: grad norm: 113.786 110.458 27.319
2024-12-02-07:52:09-root-INFO: Loss too large (821.444->1083.406)! Learning rate decreased to 0.00852.
2024-12-02-07:52:09-root-INFO: Loss too large (821.444->1011.034)! Learning rate decreased to 0.00682.
2024-12-02-07:52:09-root-INFO: Loss too large (821.444->929.530)! Learning rate decreased to 0.00545.
2024-12-02-07:52:09-root-INFO: Loss too large (821.444->869.086)! Learning rate decreased to 0.00436.
2024-12-02-07:52:09-root-INFO: Loss too large (821.444->834.943)! Learning rate decreased to 0.00349.
2024-12-02-07:52:10-root-INFO: grad norm: 155.093 152.010 30.771
2024-12-02-07:52:10-root-INFO: Loss too large (817.961->834.889)! Learning rate decreased to 0.00279.
2024-12-02-07:52:10-root-INFO: Loss too large (817.961->820.751)! Learning rate decreased to 0.00223.
2024-12-02-07:52:10-root-INFO: grad norm: 95.940 93.172 22.879
2024-12-02-07:52:11-root-INFO: grad norm: 39.263 38.011 9.835
2024-12-02-07:52:11-root-INFO: grad norm: 31.612 29.931 10.172
2024-12-02-07:52:12-root-INFO: grad norm: 28.121 26.739 8.706
2024-12-02-07:52:12-root-INFO: grad norm: 26.823 25.286 8.948
2024-12-02-07:52:13-root-INFO: grad norm: 26.150 24.724 8.516
2024-12-02-07:52:13-root-INFO: Loss Change: 821.444 -> 793.965
2024-12-02-07:52:13-root-INFO: Regularization Change: 0.000 -> 0.336
2024-12-02-07:52:13-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-07:52:13-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:52:13-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-07:52:13-root-INFO: grad norm: 52.163 50.489 13.107
2024-12-02-07:52:13-root-INFO: Loss too large (794.179->828.067)! Learning rate decreased to 0.00885.
2024-12-02-07:52:14-root-INFO: Loss too large (794.179->809.718)! Learning rate decreased to 0.00708.
2024-12-02-07:52:14-root-INFO: Loss too large (794.179->799.571)! Learning rate decreased to 0.00567.
2024-12-02-07:52:14-root-INFO: Loss too large (794.179->794.190)! Learning rate decreased to 0.00453.
2024-12-02-07:52:14-root-INFO: grad norm: 73.333 71.799 14.922
2024-12-02-07:52:15-root-INFO: Loss too large (791.540->797.802)! Learning rate decreased to 0.00363.
2024-12-02-07:52:15-root-INFO: Loss too large (791.540->792.626)! Learning rate decreased to 0.00290.
2024-12-02-07:52:15-root-INFO: grad norm: 65.795 63.809 16.042
2024-12-02-07:52:16-root-INFO: grad norm: 58.234 56.910 12.343
2024-12-02-07:52:16-root-INFO: grad norm: 54.690 52.925 13.781
2024-12-02-07:52:17-root-INFO: grad norm: 50.273 49.033 11.094
2024-12-02-07:52:17-root-INFO: grad norm: 47.911 46.280 12.395
2024-12-02-07:52:18-root-INFO: grad norm: 45.137 43.950 10.286
2024-12-02-07:52:18-root-INFO: Loss Change: 794.179 -> 776.621
2024-12-02-07:52:18-root-INFO: Regularization Change: 0.000 -> 0.415
2024-12-02-07:52:18-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-07:52:18-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-07:52:18-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-07:52:18-root-INFO: grad norm: 91.291 88.679 21.679
2024-12-02-07:52:18-root-INFO: Loss too large (783.512->985.721)! Learning rate decreased to 0.00919.
2024-12-02-07:52:18-root-INFO: Loss too large (783.512->899.577)! Learning rate decreased to 0.00735.
2024-12-02-07:52:19-root-INFO: Loss too large (783.512->839.334)! Learning rate decreased to 0.00588.
2024-12-02-07:52:19-root-INFO: Loss too large (783.512->805.051)! Learning rate decreased to 0.00471.
2024-12-02-07:52:19-root-INFO: Loss too large (783.512->787.108)! Learning rate decreased to 0.00376.
2024-12-02-07:52:19-root-INFO: grad norm: 108.115 106.191 20.308
2024-12-02-07:52:20-root-INFO: Loss too large (778.360->784.971)! Learning rate decreased to 0.00301.
2024-12-02-07:52:20-root-INFO: grad norm: 90.581 88.148 20.854
2024-12-02-07:52:21-root-INFO: grad norm: 64.102 62.813 12.791
2024-12-02-07:52:21-root-INFO: grad norm: 56.841 55.030 14.232
2024-12-02-07:52:22-root-INFO: grad norm: 48.189 47.063 10.354
2024-12-02-07:52:22-root-INFO: grad norm: 44.168 42.588 11.709
2024-12-02-07:52:22-root-INFO: grad norm: 39.750 38.680 9.161
2024-12-02-07:52:23-root-INFO: Loss Change: 783.512 -> 759.529
2024-12-02-07:52:23-root-INFO: Regularization Change: 0.000 -> 0.426
2024-12-02-07:52:23-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-07:52:23-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:52:23-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-07:52:23-root-INFO: grad norm: 81.078 78.492 20.310
2024-12-02-07:52:23-root-INFO: Loss too large (765.967->952.047)! Learning rate decreased to 0.00954.
2024-12-02-07:52:23-root-INFO: Loss too large (765.967->870.752)! Learning rate decreased to 0.00763.
2024-12-02-07:52:24-root-INFO: Loss too large (765.967->815.763)! Learning rate decreased to 0.00611.
2024-12-02-07:52:24-root-INFO: Loss too large (765.967->785.264)! Learning rate decreased to 0.00489.
2024-12-02-07:52:24-root-INFO: Loss too large (765.967->769.551)! Learning rate decreased to 0.00391.
2024-12-02-07:52:24-root-INFO: grad norm: 98.400 96.718 18.116
2024-12-02-07:52:25-root-INFO: Loss too large (761.967->766.782)! Learning rate decreased to 0.00313.
2024-12-02-07:52:25-root-INFO: grad norm: 81.349 79.077 19.090
2024-12-02-07:52:25-root-INFO: grad norm: 56.987 55.847 11.341
2024-12-02-07:52:26-root-INFO: grad norm: 49.686 47.965 12.962
2024-12-02-07:52:26-root-INFO: grad norm: 41.636 40.617 9.159
2024-12-02-07:52:27-root-INFO: grad norm: 37.712 36.190 10.605
2024-12-02-07:52:27-root-INFO: grad norm: 33.725 32.721 8.169
2024-12-02-07:52:28-root-INFO: Loss Change: 765.967 -> 744.177
2024-12-02-07:52:28-root-INFO: Regularization Change: 0.000 -> 0.418
2024-12-02-07:52:28-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-07:52:28-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:52:28-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-07:52:28-root-INFO: grad norm: 59.257 57.652 13.697
2024-12-02-07:52:28-root-INFO: Loss too large (746.553->834.706)! Learning rate decreased to 0.00991.
2024-12-02-07:52:28-root-INFO: Loss too large (746.553->790.715)! Learning rate decreased to 0.00792.
2024-12-02-07:52:28-root-INFO: Loss too large (746.553->766.316)! Learning rate decreased to 0.00634.
2024-12-02-07:52:28-root-INFO: Loss too large (746.553->753.268)! Learning rate decreased to 0.00507.
2024-12-02-07:52:29-root-INFO: Loss too large (746.553->746.557)! Learning rate decreased to 0.00406.
2024-12-02-07:52:29-root-INFO: grad norm: 63.754 62.540 12.385
2024-12-02-07:52:29-root-INFO: Loss too large (743.346->743.664)! Learning rate decreased to 0.00325.
2024-12-02-07:52:30-root-INFO: grad norm: 52.861 51.265 12.888
2024-12-02-07:52:30-root-INFO: grad norm: 40.930 39.905 9.103
2024-12-02-07:52:31-root-INFO: grad norm: 35.840 34.449 9.888
2024-12-02-07:52:31-root-INFO: grad norm: 30.915 29.889 7.900
2024-12-02-07:52:32-root-INFO: grad norm: 28.233 26.909 8.545
2024-12-02-07:52:32-root-INFO: grad norm: 25.867 24.800 7.352
2024-12-02-07:52:32-root-INFO: Loss Change: 746.553 -> 729.870
2024-12-02-07:52:32-root-INFO: Regularization Change: 0.000 -> 0.369
2024-12-02-07:52:32-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-07:52:32-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:52:33-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-07:52:33-root-INFO: grad norm: 60.574 58.786 14.609
2024-12-02-07:52:33-root-INFO: Loss too large (733.679->809.785)! Learning rate decreased to 0.01028.
2024-12-02-07:52:33-root-INFO: Loss too large (733.679->771.214)! Learning rate decreased to 0.00822.
2024-12-02-07:52:33-root-INFO: Loss too large (733.679->749.693)! Learning rate decreased to 0.00658.
2024-12-02-07:52:33-root-INFO: Loss too large (733.679->738.083)! Learning rate decreased to 0.00526.
2024-12-02-07:52:34-root-INFO: grad norm: 89.474 88.062 15.832
2024-12-02-07:52:34-root-INFO: Loss too large (732.120->743.213)! Learning rate decreased to 0.00421.
2024-12-02-07:52:34-root-INFO: Loss too large (732.120->734.803)! Learning rate decreased to 0.00337.
2024-12-02-07:52:35-root-INFO: grad norm: 68.732 66.971 15.456
2024-12-02-07:52:35-root-INFO: grad norm: 44.866 43.900 9.259
2024-12-02-07:52:36-root-INFO: grad norm: 37.777 36.413 10.062
2024-12-02-07:52:36-root-INFO: grad norm: 31.049 30.099 7.622
2024-12-02-07:52:36-root-INFO: grad norm: 27.750 26.469 8.335
2024-12-02-07:52:37-root-INFO: grad norm: 24.955 23.948 7.018
2024-12-02-07:52:37-root-INFO: Loss Change: 733.679 -> 715.730
2024-12-02-07:52:37-root-INFO: Regularization Change: 0.000 -> 0.419
2024-12-02-07:52:37-root-INFO: Undo step: 155
2024-12-02-07:52:37-root-INFO: Undo step: 156
2024-12-02-07:52:37-root-INFO: Undo step: 157
2024-12-02-07:52:37-root-INFO: Undo step: 158
2024-12-02-07:52:37-root-INFO: Undo step: 159
2024-12-02-07:52:37-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-07:52:38-root-INFO: grad norm: 334.101 323.388 83.926
2024-12-02-07:52:38-root-INFO: Loss too large (1461.629->1564.708)! Learning rate decreased to 0.00852.
2024-12-02-07:52:38-root-INFO: grad norm: 343.541 338.338 59.564
2024-12-02-07:52:39-root-INFO: grad norm: 408.747 402.596 70.641
2024-12-02-07:52:39-root-INFO: Loss too large (1238.828->1485.276)! Learning rate decreased to 0.00682.
2024-12-02-07:52:39-root-INFO: Loss too large (1238.828->1289.782)! Learning rate decreased to 0.00545.
2024-12-02-07:52:39-root-INFO: grad norm: 240.697 237.440 39.463
2024-12-02-07:52:40-root-INFO: grad norm: 102.612 100.156 22.316
2024-12-02-07:52:40-root-INFO: grad norm: 81.018 78.799 18.834
2024-12-02-07:52:41-root-INFO: grad norm: 73.868 71.727 17.655
2024-12-02-07:52:41-root-INFO: grad norm: 78.025 75.789 18.544
2024-12-02-07:52:42-root-INFO: Loss Change: 1461.629 -> 861.576
2024-12-02-07:52:42-root-INFO: Regularization Change: 0.000 -> 20.610
2024-12-02-07:52:42-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-07:52:42-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:52:42-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-07:52:42-root-INFO: grad norm: 97.318 95.234 20.030
2024-12-02-07:52:42-root-INFO: Loss too large (858.758->967.329)! Learning rate decreased to 0.00885.
2024-12-02-07:52:42-root-INFO: Loss too large (858.758->900.834)! Learning rate decreased to 0.00708.
2024-12-02-07:52:43-root-INFO: Loss too large (858.758->864.304)! Learning rate decreased to 0.00567.
2024-12-02-07:52:43-root-INFO: grad norm: 132.821 130.034 27.069
2024-12-02-07:52:43-root-INFO: Loss too large (847.555->861.281)! Learning rate decreased to 0.00453.
2024-12-02-07:52:44-root-INFO: grad norm: 114.249 112.202 21.527
2024-12-02-07:52:44-root-INFO: grad norm: 81.751 79.726 18.083
2024-12-02-07:52:44-root-INFO: grad norm: 76.823 75.166 15.872
2024-12-02-07:52:45-root-INFO: grad norm: 71.829 69.967 16.246
2024-12-02-07:52:45-root-INFO: grad norm: 70.051 68.492 14.697
2024-12-02-07:52:46-root-INFO: grad norm: 68.884 67.109 15.537
2024-12-02-07:52:46-root-INFO: Loss Change: 858.758 -> 793.855
2024-12-02-07:52:46-root-INFO: Regularization Change: 0.000 -> 2.181
2024-12-02-07:52:46-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-07:52:46-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-07:52:46-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-07:52:46-root-INFO: grad norm: 97.963 95.557 21.577
2024-12-02-07:52:47-root-INFO: Loss too large (797.655->934.583)! Learning rate decreased to 0.00919.
2024-12-02-07:52:47-root-INFO: Loss too large (797.655->858.722)! Learning rate decreased to 0.00735.
2024-12-02-07:52:47-root-INFO: Loss too large (797.655->816.020)! Learning rate decreased to 0.00588.
2024-12-02-07:52:47-root-INFO: grad norm: 140.620 138.130 26.342
2024-12-02-07:52:48-root-INFO: Loss too large (794.765->818.163)! Learning rate decreased to 0.00471.
2024-12-02-07:52:48-root-INFO: Loss too large (794.765->798.186)! Learning rate decreased to 0.00376.
2024-12-02-07:52:48-root-INFO: grad norm: 89.660 87.911 17.626
2024-12-02-07:52:49-root-INFO: grad norm: 39.971 38.562 10.518
2024-12-02-07:52:49-root-INFO: grad norm: 33.256 31.878 9.475
2024-12-02-07:52:50-root-INFO: grad norm: 29.737 28.326 9.050
2024-12-02-07:52:50-root-INFO: grad norm: 28.200 26.834 8.668
2024-12-02-07:52:51-root-INFO: grad norm: 27.252 25.890 8.508
2024-12-02-07:52:51-root-INFO: Loss Change: 797.655 -> 756.301
2024-12-02-07:52:51-root-INFO: Regularization Change: 0.000 -> 0.976
2024-12-02-07:52:51-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-07:52:51-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:52:51-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-07:52:51-root-INFO: grad norm: 53.838 51.565 15.479
2024-12-02-07:52:51-root-INFO: Loss too large (756.443->772.872)! Learning rate decreased to 0.00954.
2024-12-02-07:52:52-root-INFO: Loss too large (756.443->760.844)! Learning rate decreased to 0.00763.
2024-12-02-07:52:52-root-INFO: grad norm: 89.626 87.961 17.200
2024-12-02-07:52:52-root-INFO: Loss too large (754.547->776.507)! Learning rate decreased to 0.00611.
2024-12-02-07:52:52-root-INFO: Loss too large (754.547->761.751)! Learning rate decreased to 0.00489.
2024-12-02-07:52:53-root-INFO: grad norm: 82.787 81.297 15.639
2024-12-02-07:52:53-root-INFO: grad norm: 77.228 75.809 14.734
2024-12-02-07:52:54-root-INFO: grad norm: 74.498 73.102 14.354
2024-12-02-07:52:54-root-INFO: grad norm: 71.067 69.711 13.820
2024-12-02-07:52:55-root-INFO: grad norm: 69.599 68.273 13.520
2024-12-02-07:52:55-root-INFO: grad norm: 67.799 66.482 13.302
2024-12-02-07:52:56-root-INFO: Loss Change: 756.443 -> 732.517
2024-12-02-07:52:56-root-INFO: Regularization Change: 0.000 -> 1.126
2024-12-02-07:52:56-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-07:52:56-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:52:56-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-07:52:56-root-INFO: grad norm: 85.033 83.647 15.294
2024-12-02-07:52:56-root-INFO: Loss too large (735.889->867.156)! Learning rate decreased to 0.00991.
2024-12-02-07:52:56-root-INFO: Loss too large (735.889->793.999)! Learning rate decreased to 0.00792.
2024-12-02-07:52:56-root-INFO: Loss too large (735.889->754.680)! Learning rate decreased to 0.00634.
2024-12-02-07:52:57-root-INFO: grad norm: 122.566 120.738 21.092
2024-12-02-07:52:57-root-INFO: Loss too large (735.426->755.133)! Learning rate decreased to 0.00507.
2024-12-02-07:52:57-root-INFO: Loss too large (735.426->738.218)! Learning rate decreased to 0.00406.
2024-12-02-07:52:58-root-INFO: grad norm: 75.655 74.350 13.989
2024-12-02-07:52:58-root-INFO: grad norm: 32.228 31.211 8.032
2024-12-02-07:52:59-root-INFO: grad norm: 25.751 24.697 7.292
2024-12-02-07:52:59-root-INFO: grad norm: 22.896 21.801 6.996
2024-12-02-07:52:59-root-INFO: grad norm: 21.863 20.776 6.809
2024-12-02-07:53:00-root-INFO: grad norm: 21.339 20.256 6.712
2024-12-02-07:53:00-root-INFO: Loss Change: 735.889 -> 707.415
2024-12-02-07:53:00-root-INFO: Regularization Change: 0.000 -> 0.662
2024-12-02-07:53:00-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-07:53:00-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:53:00-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-07:53:01-root-INFO: grad norm: 47.329 45.698 12.318
2024-12-02-07:53:01-root-INFO: Loss too large (709.066->722.733)! Learning rate decreased to 0.01028.
2024-12-02-07:53:01-root-INFO: Loss too large (709.066->713.196)! Learning rate decreased to 0.00822.
2024-12-02-07:53:01-root-INFO: grad norm: 72.920 71.842 12.494
2024-12-02-07:53:02-root-INFO: Loss too large (707.988->720.956)! Learning rate decreased to 0.00658.
2024-12-02-07:53:02-root-INFO: Loss too large (707.988->710.563)! Learning rate decreased to 0.00526.
2024-12-02-07:53:02-root-INFO: grad norm: 62.731 61.746 11.070
2024-12-02-07:53:03-root-INFO: grad norm: 55.961 55.035 10.135
2024-12-02-07:53:03-root-INFO: grad norm: 52.142 51.163 10.057
2024-12-02-07:53:03-root-INFO: grad norm: 48.412 47.503 9.337
2024-12-02-07:53:04-root-INFO: grad norm: 46.279 45.332 9.315
2024-12-02-07:53:04-root-INFO: grad norm: 43.920 43.036 8.767
2024-12-02-07:53:05-root-INFO: Loss Change: 709.066 -> 688.788
2024-12-02-07:53:05-root-INFO: Regularization Change: 0.000 -> 0.887
2024-12-02-07:53:05-root-INFO: Undo step: 155
2024-12-02-07:53:05-root-INFO: Undo step: 156
2024-12-02-07:53:05-root-INFO: Undo step: 157
2024-12-02-07:53:05-root-INFO: Undo step: 158
2024-12-02-07:53:05-root-INFO: Undo step: 159
2024-12-02-07:53:05-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-07:53:05-root-INFO: grad norm: 353.788 341.831 91.200
2024-12-02-07:53:06-root-INFO: grad norm: 557.792 547.415 107.091
2024-12-02-07:53:06-root-INFO: Loss too large (1400.157->2313.355)! Learning rate decreased to 0.00852.
2024-12-02-07:53:06-root-INFO: Loss too large (1400.157->1808.139)! Learning rate decreased to 0.00682.
2024-12-02-07:53:06-root-INFO: Loss too large (1400.157->1483.983)! Learning rate decreased to 0.00545.
2024-12-02-07:53:06-root-INFO: grad norm: 408.730 404.402 59.324
2024-12-02-07:53:07-root-INFO: grad norm: 173.351 167.800 43.516
2024-12-02-07:53:07-root-INFO: grad norm: 116.695 112.490 31.044
2024-12-02-07:53:08-root-INFO: grad norm: 111.053 106.669 30.896
2024-12-02-07:53:08-root-INFO: grad norm: 126.735 121.308 36.688
2024-12-02-07:53:08-root-INFO: Loss too large (853.935->855.284)! Learning rate decreased to 0.00436.
2024-12-02-07:53:09-root-INFO: grad norm: 105.278 100.729 30.614
2024-12-02-07:53:09-root-INFO: Loss Change: 1469.831 -> 821.597
2024-12-02-07:53:09-root-INFO: Regularization Change: 0.000 -> 20.536
2024-12-02-07:53:09-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-07:53:09-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-07:53:09-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-07:53:10-root-INFO: grad norm: 57.101 55.254 14.404
2024-12-02-07:53:10-root-INFO: grad norm: 84.208 80.465 24.827
2024-12-02-07:53:10-root-INFO: Loss too large (801.984->862.704)! Learning rate decreased to 0.00885.
2024-12-02-07:53:10-root-INFO: Loss too large (801.984->819.041)! Learning rate decreased to 0.00708.
2024-12-02-07:53:11-root-INFO: grad norm: 144.780 138.822 41.108
2024-12-02-07:53:11-root-INFO: Loss too large (798.175->833.772)! Learning rate decreased to 0.00567.
2024-12-02-07:53:11-root-INFO: Loss too large (798.175->810.178)! Learning rate decreased to 0.00453.
2024-12-02-07:53:12-root-INFO: grad norm: 100.895 96.344 29.960
2024-12-02-07:53:12-root-INFO: grad norm: 42.548 41.005 11.353
2024-12-02-07:53:13-root-INFO: grad norm: 38.794 37.096 11.352
2024-12-02-07:53:13-root-INFO: grad norm: 35.755 34.474 9.485
2024-12-02-07:53:14-root-INFO: grad norm: 33.853 32.438 9.688
2024-12-02-07:53:14-root-INFO: Loss Change: 816.089 -> 750.685
2024-12-02-07:53:14-root-INFO: Regularization Change: 0.000 -> 2.876
2024-12-02-07:53:14-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-07:53:14-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-07:53:14-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-07:53:14-root-INFO: grad norm: 57.340 55.005 16.196
2024-12-02-07:53:14-root-INFO: Loss too large (748.937->757.988)! Learning rate decreased to 0.00919.
2024-12-02-07:53:15-root-INFO: Loss too large (748.937->749.349)! Learning rate decreased to 0.00735.
2024-12-02-07:53:15-root-INFO: grad norm: 75.452 74.037 14.545
2024-12-02-07:53:15-root-INFO: Loss too large (744.681->750.099)! Learning rate decreased to 0.00588.
2024-12-02-07:53:16-root-INFO: grad norm: 83.116 81.333 17.125
2024-12-02-07:53:16-root-INFO: grad norm: 96.049 94.194 18.787
2024-12-02-07:53:16-root-INFO: Loss too large (739.836->740.656)! Learning rate decreased to 0.00471.
2024-12-02-07:53:17-root-INFO: grad norm: 73.161 71.215 16.760
2024-12-02-07:53:17-root-INFO: grad norm: 56.407 55.144 11.872
2024-12-02-07:53:18-root-INFO: grad norm: 47.593 46.147 11.642
2024-12-02-07:53:18-root-INFO: grad norm: 40.373 39.336 9.090
2024-12-02-07:53:19-root-INFO: Loss Change: 748.937 -> 714.469
2024-12-02-07:53:19-root-INFO: Regularization Change: 0.000 -> 1.364
2024-12-02-07:53:19-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-07:53:19-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:53:19-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-07:53:19-root-INFO: grad norm: 71.820 69.123 19.495
2024-12-02-07:53:19-root-INFO: Loss too large (716.767->765.834)! Learning rate decreased to 0.00954.
2024-12-02-07:53:19-root-INFO: Loss too large (716.767->737.741)! Learning rate decreased to 0.00763.
2024-12-02-07:53:19-root-INFO: Loss too large (716.767->721.963)! Learning rate decreased to 0.00611.
2024-12-02-07:53:20-root-INFO: grad norm: 83.074 81.388 16.652
2024-12-02-07:53:20-root-INFO: Loss too large (713.583->715.007)! Learning rate decreased to 0.00489.
2024-12-02-07:53:20-root-INFO: grad norm: 66.539 64.749 15.331
2024-12-02-07:53:21-root-INFO: grad norm: 52.579 51.675 9.708
2024-12-02-07:53:21-root-INFO: grad norm: 44.774 43.575 10.295
2024-12-02-07:53:22-root-INFO: grad norm: 38.256 37.527 7.431
2024-12-02-07:53:22-root-INFO: grad norm: 33.973 32.988 8.122
2024-12-02-07:53:23-root-INFO: grad norm: 30.458 29.779 6.395
2024-12-02-07:53:23-root-INFO: Loss Change: 716.767 -> 688.870
2024-12-02-07:53:23-root-INFO: Regularization Change: 0.000 -> 0.883
2024-12-02-07:53:23-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-07:53:23-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:53:23-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-07:53:24-root-INFO: grad norm: 54.729 53.594 11.089
2024-12-02-07:53:24-root-INFO: Loss too large (690.341->718.308)! Learning rate decreased to 0.00991.
2024-12-02-07:53:24-root-INFO: Loss too large (690.341->702.776)! Learning rate decreased to 0.00792.
2024-12-02-07:53:24-root-INFO: Loss too large (690.341->693.612)! Learning rate decreased to 0.00634.
2024-12-02-07:53:25-root-INFO: grad norm: 64.544 63.752 10.077
2024-12-02-07:53:25-root-INFO: Loss too large (688.585->688.797)! Learning rate decreased to 0.00507.
2024-12-02-07:53:25-root-INFO: grad norm: 52.216 51.296 9.757
2024-12-02-07:53:26-root-INFO: grad norm: 42.269 41.637 7.284
2024-12-02-07:53:26-root-INFO: grad norm: 35.947 35.171 7.431
2024-12-02-07:53:27-root-INFO: grad norm: 30.980 30.375 6.095
2024-12-02-07:53:27-root-INFO: grad norm: 27.601 26.862 6.345
2024-12-02-07:53:27-root-INFO: grad norm: 24.987 24.364 5.544
2024-12-02-07:53:28-root-INFO: Loss Change: 690.341 -> 669.886
2024-12-02-07:53:28-root-INFO: Regularization Change: 0.000 -> 0.718
2024-12-02-07:53:28-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-07:53:28-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-07:53:28-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-07:53:28-root-INFO: grad norm: 57.276 55.615 13.695
2024-12-02-07:53:28-root-INFO: Loss too large (672.869->703.036)! Learning rate decreased to 0.01028.
2024-12-02-07:53:28-root-INFO: Loss too large (672.869->686.318)! Learning rate decreased to 0.00822.
2024-12-02-07:53:29-root-INFO: Loss too large (672.869->676.348)! Learning rate decreased to 0.00658.
2024-12-02-07:53:29-root-INFO: grad norm: 65.891 65.107 10.134
2024-12-02-07:53:29-root-INFO: Loss too large (670.831->671.410)! Learning rate decreased to 0.00526.
2024-12-02-07:53:30-root-INFO: grad norm: 52.866 52.002 9.523
2024-12-02-07:53:30-root-INFO: grad norm: 42.636 42.137 6.506
2024-12-02-07:53:31-root-INFO: grad norm: 35.984 35.293 7.016
2024-12-02-07:53:31-root-INFO: grad norm: 30.687 30.203 5.431
2024-12-02-07:53:32-root-INFO: grad norm: 27.049 26.390 5.936
2024-12-02-07:53:32-root-INFO: grad norm: 24.201 23.675 5.019
2024-12-02-07:53:32-root-INFO: Loss Change: 672.869 -> 653.102
2024-12-02-07:53:32-root-INFO: Regularization Change: 0.000 -> 0.682
2024-12-02-07:53:32-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-07:53:32-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-07:53:33-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-07:53:33-root-INFO: grad norm: 58.035 56.786 11.976
2024-12-02-07:53:33-root-INFO: Loss too large (656.293->691.072)! Learning rate decreased to 0.01067.
2024-12-02-07:53:33-root-INFO: Loss too large (656.293->672.764)! Learning rate decreased to 0.00853.
2024-12-02-07:53:33-root-INFO: Loss too large (656.293->661.508)! Learning rate decreased to 0.00683.
2024-12-02-07:53:34-root-INFO: grad norm: 68.959 68.328 9.302
2024-12-02-07:53:34-root-INFO: Loss too large (655.108->656.497)! Learning rate decreased to 0.00546.
2024-12-02-07:53:34-root-INFO: grad norm: 55.411 54.717 8.743
2024-12-02-07:53:35-root-INFO: grad norm: 44.364 43.935 6.152
2024-12-02-07:53:35-root-INFO: grad norm: 37.249 36.680 6.489
2024-12-02-07:53:36-root-INFO: grad norm: 31.490 31.056 5.208
2024-12-02-07:53:36-root-INFO: grad norm: 27.521 26.952 5.572
2024-12-02-07:53:37-root-INFO: grad norm: 24.363 23.876 4.844
2024-12-02-07:53:37-root-INFO: Loss Change: 656.293 -> 637.513
2024-12-02-07:53:37-root-INFO: Regularization Change: 0.000 -> 0.655
2024-12-02-07:53:37-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-07:53:37-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-07:53:37-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-07:53:38-root-INFO: grad norm: 58.102 56.665 12.843
2024-12-02-07:53:38-root-INFO: Loss too large (641.429->675.927)! Learning rate decreased to 0.01107.
2024-12-02-07:53:38-root-INFO: Loss too large (641.429->657.498)! Learning rate decreased to 0.00885.
2024-12-02-07:53:38-root-INFO: Loss too large (641.429->646.198)! Learning rate decreased to 0.00708.
2024-12-02-07:53:38-root-INFO: grad norm: 67.170 66.522 9.313
2024-12-02-07:53:39-root-INFO: Loss too large (639.803->641.092)! Learning rate decreased to 0.00567.
2024-12-02-07:53:39-root-INFO: grad norm: 53.349 52.745 8.009
2024-12-02-07:53:40-root-INFO: grad norm: 42.314 41.921 5.755
2024-12-02-07:53:40-root-INFO: grad norm: 35.287 34.796 5.866
2024-12-02-07:53:40-root-INFO: grad norm: 29.638 29.233 4.882
2024-12-02-07:53:41-root-INFO: grad norm: 25.793 25.281 5.117
2024-12-02-07:53:41-root-INFO: grad norm: 22.758 22.291 4.587
2024-12-02-07:53:42-root-INFO: Loss Change: 641.429 -> 623.058
2024-12-02-07:53:42-root-INFO: Regularization Change: 0.000 -> 0.641
2024-12-02-07:53:42-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-07:53:42-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-07:53:42-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-07:53:42-root-INFO: grad norm: 38.175 37.369 7.802
2024-12-02-07:53:42-root-INFO: Loss too large (623.672->637.347)! Learning rate decreased to 0.01148.
2024-12-02-07:53:42-root-INFO: Loss too large (623.672->629.233)! Learning rate decreased to 0.00919.
2024-12-02-07:53:43-root-INFO: Loss too large (623.672->624.560)! Learning rate decreased to 0.00735.
2024-12-02-07:53:43-root-INFO: grad norm: 43.856 43.448 5.962
2024-12-02-07:53:44-root-INFO: grad norm: 51.087 50.610 6.968
2024-12-02-07:53:44-root-INFO: grad norm: 59.619 59.247 6.645
2024-12-02-07:53:44-root-INFO: Loss too large (621.208->621.869)! Learning rate decreased to 0.00588.
2024-12-02-07:53:45-root-INFO: grad norm: 45.572 45.158 6.127
2024-12-02-07:53:45-root-INFO: grad norm: 34.806 34.442 5.027
2024-12-02-07:53:46-root-INFO: grad norm: 28.426 27.978 5.027
2024-12-02-07:53:46-root-INFO: grad norm: 23.685 23.248 4.528
2024-12-02-07:53:46-root-INFO: Loss Change: 623.672 -> 609.454
2024-12-02-07:53:46-root-INFO: Regularization Change: 0.000 -> 0.646
2024-12-02-07:53:46-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-07:53:46-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-07:53:46-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-07:53:47-root-INFO: grad norm: 52.967 51.811 11.009
2024-12-02-07:53:47-root-INFO: Loss too large (613.055->641.471)! Learning rate decreased to 0.01191.
2024-12-02-07:53:47-root-INFO: Loss too large (613.055->625.660)! Learning rate decreased to 0.00953.
2024-12-02-07:53:47-root-INFO: Loss too large (613.055->616.114)! Learning rate decreased to 0.00762.
2024-12-02-07:53:48-root-INFO: grad norm: 57.370 56.831 7.844
2024-12-02-07:53:48-root-INFO: grad norm: 62.648 62.086 8.374
2024-12-02-07:53:49-root-INFO: grad norm: 67.006 66.530 7.975
2024-12-02-07:53:49-root-INFO: grad norm: 68.704 68.220 8.138
2024-12-02-07:53:49-root-INFO: grad norm: 68.646 68.170 8.072
2024-12-02-07:53:50-root-INFO: grad norm: 67.539 67.076 7.890
2024-12-02-07:53:50-root-INFO: grad norm: 65.423 64.946 7.884
2024-12-02-07:53:51-root-INFO: Loss Change: 613.055 -> 602.055
2024-12-02-07:53:51-root-INFO: Regularization Change: 0.000 -> 0.887
2024-12-02-07:53:51-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-07:53:51-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:53:51-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-07:53:51-root-INFO: grad norm: 82.856 82.127 10.970
2024-12-02-07:53:51-root-INFO: Loss too large (608.481->672.072)! Learning rate decreased to 0.01235.
2024-12-02-07:53:51-root-INFO: Loss too large (608.481->639.578)! Learning rate decreased to 0.00988.
2024-12-02-07:53:51-root-INFO: Loss too large (608.481->616.908)! Learning rate decreased to 0.00790.
2024-12-02-07:53:52-root-INFO: grad norm: 75.414 74.881 8.954
2024-12-02-07:53:52-root-INFO: grad norm: 68.832 68.308 8.481
2024-12-02-07:53:53-root-INFO: grad norm: 62.406 61.872 8.146
2024-12-02-07:53:53-root-INFO: grad norm: 57.942 57.463 7.436
2024-12-02-07:53:54-root-INFO: grad norm: 53.990 53.483 7.382
2024-12-02-07:53:54-root-INFO: grad norm: 51.114 50.670 6.727
2024-12-02-07:53:55-root-INFO: grad norm: 48.604 48.132 6.760
2024-12-02-07:53:55-root-INFO: Loss Change: 608.481 -> 585.509
2024-12-02-07:53:55-root-INFO: Regularization Change: 0.000 -> 0.826
2024-12-02-07:53:55-root-INFO: Undo step: 150
2024-12-02-07:53:55-root-INFO: Undo step: 151
2024-12-02-07:53:55-root-INFO: Undo step: 152
2024-12-02-07:53:55-root-INFO: Undo step: 153
2024-12-02-07:53:55-root-INFO: Undo step: 154
2024-12-02-07:53:55-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-07:53:56-root-INFO: grad norm: 494.523 486.158 90.574
2024-12-02-07:53:56-root-INFO: grad norm: 205.874 202.354 37.909
2024-12-02-07:53:56-root-INFO: grad norm: 216.264 213.691 33.264
2024-12-02-07:53:57-root-INFO: Loss too large (895.835->973.304)! Learning rate decreased to 0.01028.
2024-12-02-07:53:57-root-INFO: grad norm: 193.014 190.307 32.210
2024-12-02-07:53:57-root-INFO: Loss too large (880.257->910.307)! Learning rate decreased to 0.00822.
2024-12-02-07:53:58-root-INFO: grad norm: 316.878 308.414 72.749
2024-12-02-07:53:58-root-INFO: Loss too large (823.339->1043.204)! Learning rate decreased to 0.00658.
2024-12-02-07:53:58-root-INFO: Loss too large (823.339->930.744)! Learning rate decreased to 0.00526.
2024-12-02-07:53:58-root-INFO: Loss too large (823.339->855.575)! Learning rate decreased to 0.00421.
2024-12-02-07:53:59-root-INFO: grad norm: 156.141 153.748 27.229
2024-12-02-07:53:59-root-INFO: grad norm: 65.547 63.616 15.791
2024-12-02-07:54:00-root-INFO: grad norm: 44.920 43.487 11.256
2024-12-02-07:54:00-root-INFO: Loss Change: 1570.027 -> 715.071
2024-12-02-07:54:00-root-INFO: Regularization Change: 0.000 -> 41.627
2024-12-02-07:54:00-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-07:54:00-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-07:54:00-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-07:54:00-root-INFO: grad norm: 53.416 51.718 13.364
2024-12-02-07:54:01-root-INFO: grad norm: 97.180 95.002 20.460
2024-12-02-07:54:01-root-INFO: Loss too large (701.786->782.131)! Learning rate decreased to 0.01067.
2024-12-02-07:54:01-root-INFO: Loss too large (701.786->742.258)! Learning rate decreased to 0.00853.
2024-12-02-07:54:01-root-INFO: Loss too large (701.786->716.981)! Learning rate decreased to 0.00683.
2024-12-02-07:54:02-root-INFO: grad norm: 103.495 101.606 19.684
2024-12-02-07:54:02-root-INFO: grad norm: 138.708 135.039 31.694
2024-12-02-07:54:02-root-INFO: Loss too large (695.099->716.461)! Learning rate decreased to 0.00546.
2024-12-02-07:54:03-root-INFO: Loss too large (695.099->696.519)! Learning rate decreased to 0.00437.
2024-12-02-07:54:03-root-INFO: grad norm: 79.496 77.754 16.552
2024-12-02-07:54:04-root-INFO: grad norm: 32.671 31.617 8.232
2024-12-02-07:54:04-root-INFO: grad norm: 28.935 27.870 7.779
2024-12-02-07:54:04-root-INFO: grad norm: 27.340 26.292 7.497
2024-12-02-07:54:05-root-INFO: Loss Change: 712.352 -> 658.951
2024-12-02-07:54:05-root-INFO: Regularization Change: 0.000 -> 2.444
2024-12-02-07:54:05-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-07:54:05-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-07:54:05-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-07:54:05-root-INFO: grad norm: 48.888 47.034 13.335
2024-12-02-07:54:05-root-INFO: Loss too large (659.870->666.731)! Learning rate decreased to 0.01107.
2024-12-02-07:54:06-root-INFO: grad norm: 85.685 83.936 17.224
2024-12-02-07:54:06-root-INFO: Loss too large (658.756->693.754)! Learning rate decreased to 0.00885.
2024-12-02-07:54:06-root-INFO: Loss too large (658.756->671.841)! Learning rate decreased to 0.00708.
2024-12-02-07:54:07-root-INFO: grad norm: 88.102 86.520 16.622
2024-12-02-07:54:07-root-INFO: grad norm: 106.543 103.945 23.386
2024-12-02-07:54:07-root-INFO: Loss too large (653.431->663.744)! Learning rate decreased to 0.00567.
2024-12-02-07:54:08-root-INFO: grad norm: 81.667 80.013 16.358
2024-12-02-07:54:08-root-INFO: grad norm: 47.960 46.659 11.093
2024-12-02-07:54:09-root-INFO: grad norm: 39.811 38.682 9.411
2024-12-02-07:54:09-root-INFO: grad norm: 33.199 32.106 8.448
2024-12-02-07:54:09-root-INFO: Loss Change: 659.870 -> 627.385
2024-12-02-07:54:09-root-INFO: Regularization Change: 0.000 -> 1.613
2024-12-02-07:54:09-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-07:54:09-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-07:54:10-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-07:54:10-root-INFO: grad norm: 42.812 41.523 10.427
2024-12-02-07:54:10-root-INFO: Loss too large (627.281->643.573)! Learning rate decreased to 0.01148.
2024-12-02-07:54:10-root-INFO: Loss too large (627.281->632.174)! Learning rate decreased to 0.00919.
2024-12-02-07:54:11-root-INFO: grad norm: 68.935 67.312 14.870
2024-12-02-07:54:11-root-INFO: Loss too large (626.423->637.013)! Learning rate decreased to 0.00735.
2024-12-02-07:54:11-root-INFO: Loss too large (626.423->628.144)! Learning rate decreased to 0.00588.
2024-12-02-07:54:11-root-INFO: grad norm: 53.330 52.035 11.681
2024-12-02-07:54:12-root-INFO: grad norm: 36.691 35.606 8.856
2024-12-02-07:54:12-root-INFO: grad norm: 30.740 29.708 7.897
2024-12-02-07:54:13-root-INFO: grad norm: 25.731 24.770 6.965
2024-12-02-07:54:13-root-INFO: grad norm: 23.180 22.218 6.608
2024-12-02-07:54:14-root-INFO: grad norm: 21.146 20.221 6.183
2024-12-02-07:54:14-root-INFO: Loss Change: 627.281 -> 605.435
2024-12-02-07:54:14-root-INFO: Regularization Change: 0.000 -> 0.981
2024-12-02-07:54:14-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-07:54:14-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-07:54:14-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-07:54:15-root-INFO: grad norm: 47.393 45.860 11.957
2024-12-02-07:54:15-root-INFO: Loss too large (607.079->623.358)! Learning rate decreased to 0.01191.
2024-12-02-07:54:15-root-INFO: Loss too large (607.079->611.778)! Learning rate decreased to 0.00953.
2024-12-02-07:54:15-root-INFO: grad norm: 64.384 63.231 12.129
2024-12-02-07:54:16-root-INFO: Loss too large (605.601->612.126)! Learning rate decreased to 0.00762.
2024-12-02-07:54:16-root-INFO: grad norm: 61.957 60.779 12.024
2024-12-02-07:54:17-root-INFO: grad norm: 60.536 59.335 11.998
2024-12-02-07:54:17-root-INFO: grad norm: 58.875 57.664 11.880
2024-12-02-07:54:18-root-INFO: grad norm: 57.032 55.792 11.829
2024-12-02-07:54:18-root-INFO: grad norm: 55.612 54.383 11.625
2024-12-02-07:54:19-root-INFO: grad norm: 53.703 52.463 11.475
2024-12-02-07:54:19-root-INFO: Loss Change: 607.079 -> 589.686
2024-12-02-07:54:19-root-INFO: Regularization Change: 0.000 -> 1.218
2024-12-02-07:54:19-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-07:54:19-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:54:19-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-07:54:19-root-INFO: grad norm: 66.173 64.834 13.244
2024-12-02-07:54:19-root-INFO: Loss too large (592.955->646.339)! Learning rate decreased to 0.01235.
2024-12-02-07:54:19-root-INFO: Loss too large (592.955->612.191)! Learning rate decreased to 0.00988.
2024-12-02-07:54:20-root-INFO: Loss too large (592.955->594.481)! Learning rate decreased to 0.00790.
2024-12-02-07:54:20-root-INFO: grad norm: 57.006 55.991 10.709
2024-12-02-07:54:21-root-INFO: grad norm: 52.288 51.251 10.362
2024-12-02-07:54:21-root-INFO: grad norm: 46.684 45.753 9.276
2024-12-02-07:54:22-root-INFO: grad norm: 43.533 42.579 9.062
2024-12-02-07:54:22-root-INFO: grad norm: 39.949 39.069 8.340
2024-12-02-07:54:23-root-INFO: grad norm: 37.798 36.891 8.230
2024-12-02-07:54:23-root-INFO: grad norm: 35.409 34.565 7.685
2024-12-02-07:54:23-root-INFO: Loss Change: 592.955 -> 569.646
2024-12-02-07:54:23-root-INFO: Regularization Change: 0.000 -> 0.989
2024-12-02-07:54:23-root-INFO: Undo step: 150
2024-12-02-07:54:23-root-INFO: Undo step: 151
2024-12-02-07:54:23-root-INFO: Undo step: 152
2024-12-02-07:54:23-root-INFO: Undo step: 153
2024-12-02-07:54:23-root-INFO: Undo step: 154
2024-12-02-07:54:24-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-07:54:24-root-INFO: grad norm: 445.721 432.608 107.322
2024-12-02-07:54:24-root-INFO: Loss too large (1299.203->1894.795)! Learning rate decreased to 0.01028.
2024-12-02-07:54:24-root-INFO: Loss too large (1299.203->1539.699)! Learning rate decreased to 0.00822.
2024-12-02-07:54:24-root-INFO: Loss too large (1299.203->1326.510)! Learning rate decreased to 0.00658.
2024-12-02-07:54:25-root-INFO: grad norm: 275.035 270.993 46.977
2024-12-02-07:54:25-root-INFO: grad norm: 149.348 145.699 32.809
2024-12-02-07:54:26-root-INFO: grad norm: 114.248 110.845 27.675
2024-12-02-07:54:26-root-INFO: grad norm: 111.819 108.506 27.019
2024-12-02-07:54:26-root-INFO: grad norm: 131.349 126.749 34.458
2024-12-02-07:54:27-root-INFO: Loss too large (736.681->738.265)! Learning rate decreased to 0.00526.
2024-12-02-07:54:27-root-INFO: grad norm: 101.633 98.768 23.961
2024-12-02-07:54:27-root-INFO: grad norm: 62.690 60.192 17.520
2024-12-02-07:54:28-root-INFO: Loss Change: 1299.203 -> 688.638
2024-12-02-07:54:28-root-INFO: Regularization Change: 0.000 -> 17.363
2024-12-02-07:54:28-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-07:54:28-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-07:54:28-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-07:54:28-root-INFO: grad norm: 71.652 69.289 18.249
2024-12-02-07:54:28-root-INFO: Loss too large (687.352->719.026)! Learning rate decreased to 0.01067.
2024-12-02-07:54:28-root-INFO: Loss too large (687.352->692.776)! Learning rate decreased to 0.00853.
2024-12-02-07:54:29-root-INFO: grad norm: 105.787 102.689 25.412
2024-12-02-07:54:29-root-INFO: Loss too large (680.203->694.832)! Learning rate decreased to 0.00683.
2024-12-02-07:54:29-root-INFO: grad norm: 95.415 93.128 20.763
2024-12-02-07:54:30-root-INFO: grad norm: 74.387 72.136 18.162
2024-12-02-07:54:30-root-INFO: grad norm: 68.740 66.798 16.225
2024-12-02-07:54:31-root-INFO: grad norm: 61.058 59.126 15.240
2024-12-02-07:54:31-root-INFO: grad norm: 57.606 55.863 14.062
2024-12-02-07:54:32-root-INFO: grad norm: 52.877 51.177 13.299
2024-12-02-07:54:32-root-INFO: Loss Change: 687.352 -> 630.360
2024-12-02-07:54:32-root-INFO: Regularization Change: 0.000 -> 3.062
2024-12-02-07:54:32-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-07:54:32-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-07:54:32-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-07:54:32-root-INFO: grad norm: 75.466 73.474 17.226
2024-12-02-07:54:33-root-INFO: Loss too large (635.410->681.689)! Learning rate decreased to 0.01107.
2024-12-02-07:54:33-root-INFO: Loss too large (635.410->649.308)! Learning rate decreased to 0.00885.
2024-12-02-07:54:33-root-INFO: grad norm: 96.639 94.758 18.977
2024-12-02-07:54:33-root-INFO: Loss too large (632.477->642.465)! Learning rate decreased to 0.00708.
2024-12-02-07:54:34-root-INFO: grad norm: 79.248 77.659 15.787
2024-12-02-07:54:34-root-INFO: grad norm: 57.716 56.547 11.558
2024-12-02-07:54:35-root-INFO: grad norm: 49.528 48.394 10.540
2024-12-02-07:54:35-root-INFO: grad norm: 41.589 40.603 9.003
2024-12-02-07:54:36-root-INFO: grad norm: 37.106 36.115 8.516
2024-12-02-07:54:36-root-INFO: grad norm: 32.869 31.975 7.614
2024-12-02-07:54:36-root-INFO: Loss Change: 635.410 -> 597.228
2024-12-02-07:54:36-root-INFO: Regularization Change: 0.000 -> 1.669
2024-12-02-07:54:36-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-07:54:36-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-07:54:37-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-07:54:37-root-INFO: grad norm: 45.824 44.719 10.004
2024-12-02-07:54:37-root-INFO: Loss too large (597.716->610.944)! Learning rate decreased to 0.01148.
2024-12-02-07:54:37-root-INFO: Loss too large (597.716->600.879)! Learning rate decreased to 0.00919.
2024-12-02-07:54:38-root-INFO: grad norm: 56.093 55.184 10.059
2024-12-02-07:54:38-root-INFO: Loss too large (595.601->596.619)! Learning rate decreased to 0.00735.
2024-12-02-07:54:38-root-INFO: grad norm: 47.097 46.191 9.194
2024-12-02-07:54:39-root-INFO: grad norm: 38.391 37.645 7.530
2024-12-02-07:54:39-root-INFO: grad norm: 33.305 32.542 7.088
2024-12-02-07:54:40-root-INFO: grad norm: 28.780 28.092 6.252
2024-12-02-07:54:40-root-INFO: grad norm: 25.835 25.124 6.020
2024-12-02-07:54:41-root-INFO: grad norm: 23.311 22.646 5.527
2024-12-02-07:54:41-root-INFO: Loss Change: 597.716 -> 575.024
2024-12-02-07:54:41-root-INFO: Regularization Change: 0.000 -> 1.186
2024-12-02-07:54:41-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-07:54:41-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-07:54:41-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-07:54:41-root-INFO: grad norm: 52.189 50.982 11.159
2024-12-02-07:54:41-root-INFO: Loss too large (577.751->596.719)! Learning rate decreased to 0.01191.
2024-12-02-07:54:42-root-INFO: Loss too large (577.751->583.467)! Learning rate decreased to 0.00953.
2024-12-02-07:54:42-root-INFO: grad norm: 61.805 61.060 9.567
2024-12-02-07:54:42-root-INFO: Loss too large (576.169->577.886)! Learning rate decreased to 0.00762.
2024-12-02-07:54:43-root-INFO: grad norm: 49.361 48.617 8.539
2024-12-02-07:54:43-root-INFO: grad norm: 38.519 37.935 6.683
2024-12-02-07:54:44-root-INFO: grad norm: 32.020 31.439 6.071
2024-12-02-07:54:44-root-INFO: grad norm: 26.704 26.162 5.355
2024-12-02-07:54:45-root-INFO: grad norm: 23.245 22.700 5.005
2024-12-02-07:54:45-root-INFO: grad norm: 20.539 19.994 4.699
2024-12-02-07:54:45-root-INFO: Loss Change: 577.751 -> 556.239
2024-12-02-07:54:45-root-INFO: Regularization Change: 0.000 -> 1.047
2024-12-02-07:54:45-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-07:54:45-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:54:46-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-07:54:46-root-INFO: grad norm: 37.678 36.961 7.315
2024-12-02-07:54:46-root-INFO: Loss too large (557.797->566.567)! Learning rate decreased to 0.01235.
2024-12-02-07:54:46-root-INFO: Loss too large (557.797->559.890)! Learning rate decreased to 0.00988.
2024-12-02-07:54:47-root-INFO: grad norm: 44.502 44.017 6.556
2024-12-02-07:54:47-root-INFO: Loss too large (556.293->556.437)! Learning rate decreased to 0.00790.
2024-12-02-07:54:47-root-INFO: grad norm: 36.390 35.910 5.889
2024-12-02-07:54:48-root-INFO: grad norm: 29.501 29.045 5.170
2024-12-02-07:54:48-root-INFO: grad norm: 25.024 24.584 4.668
2024-12-02-07:54:49-root-INFO: grad norm: 21.500 21.033 4.459
2024-12-02-07:54:49-root-INFO: grad norm: 19.111 18.658 4.137
2024-12-02-07:54:50-root-INFO: grad norm: 17.316 16.825 4.093
2024-12-02-07:54:50-root-INFO: Loss Change: 557.797 -> 541.638
2024-12-02-07:54:50-root-INFO: Regularization Change: 0.000 -> 0.896
2024-12-02-07:54:50-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-07:54:50-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:54:50-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-07:54:50-root-INFO: grad norm: 39.497 38.615 8.301
2024-12-02-07:54:51-root-INFO: Loss too large (543.156->553.281)! Learning rate decreased to 0.01281.
2024-12-02-07:54:51-root-INFO: Loss too large (543.156->545.720)! Learning rate decreased to 0.01024.
2024-12-02-07:54:51-root-INFO: grad norm: 45.608 45.072 6.972
2024-12-02-07:54:52-root-INFO: Loss too large (541.612->541.918)! Learning rate decreased to 0.00820.
2024-12-02-07:54:52-root-INFO: grad norm: 36.585 36.097 5.954
2024-12-02-07:54:52-root-INFO: grad norm: 29.269 28.827 5.068
2024-12-02-07:54:53-root-INFO: grad norm: 24.502 24.092 4.464
2024-12-02-07:54:53-root-INFO: grad norm: 20.841 20.401 4.257
2024-12-02-07:54:54-root-INFO: grad norm: 18.366 17.946 3.903
2024-12-02-07:54:54-root-INFO: grad norm: 16.559 16.096 3.891
2024-12-02-07:54:55-root-INFO: Loss Change: 543.156 -> 527.380
2024-12-02-07:54:55-root-INFO: Regularization Change: 0.000 -> 0.868
2024-12-02-07:54:55-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-07:54:55-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:54:55-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-07:54:55-root-INFO: grad norm: 40.220 39.396 8.103
2024-12-02-07:54:55-root-INFO: Loss too large (530.225->541.776)! Learning rate decreased to 0.01328.
2024-12-02-07:54:56-root-INFO: Loss too large (530.225->533.530)! Learning rate decreased to 0.01062.
2024-12-02-07:54:56-root-INFO: grad norm: 46.153 45.662 6.712
2024-12-02-07:54:56-root-INFO: Loss too large (528.976->529.360)! Learning rate decreased to 0.00850.
2024-12-02-07:54:57-root-INFO: grad norm: 36.524 36.108 5.498
2024-12-02-07:54:57-root-INFO: grad norm: 29.186 28.791 4.788
2024-12-02-07:54:58-root-INFO: grad norm: 24.450 24.101 4.119
2024-12-02-07:54:58-root-INFO: grad norm: 21.026 20.627 4.079
2024-12-02-07:54:59-root-INFO: grad norm: 18.705 18.337 3.690
2024-12-02-07:54:59-root-INFO: grad norm: 17.123 16.695 3.803
2024-12-02-07:54:59-root-INFO: Loss Change: 530.225 -> 515.088
2024-12-02-07:54:59-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-02-07:54:59-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-07:54:59-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-07:55:00-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-07:55:00-root-INFO: grad norm: 33.552 32.902 6.572
2024-12-02-07:55:00-root-INFO: Loss too large (516.416->523.839)! Learning rate decreased to 0.01376.
2024-12-02-07:55:00-root-INFO: Loss too large (516.416->518.245)! Learning rate decreased to 0.01101.
2024-12-02-07:55:01-root-INFO: grad norm: 38.626 38.256 5.338
2024-12-02-07:55:01-root-INFO: Loss too large (515.183->515.198)! Learning rate decreased to 0.00881.
2024-12-02-07:55:01-root-INFO: grad norm: 31.790 31.452 4.617
2024-12-02-07:55:02-root-INFO: grad norm: 26.722 26.355 4.410
2024-12-02-07:55:02-root-INFO: grad norm: 23.231 22.904 3.890
2024-12-02-07:55:03-root-INFO: grad norm: 20.864 20.466 4.051
2024-12-02-07:55:03-root-INFO: grad norm: 19.120 18.761 3.687
2024-12-02-07:55:04-root-INFO: grad norm: 18.008 17.581 3.897
2024-12-02-07:55:04-root-INFO: Loss Change: 516.416 -> 503.341
2024-12-02-07:55:04-root-INFO: Regularization Change: 0.000 -> 0.807
2024-12-02-07:55:04-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-07:55:04-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:55:04-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-07:55:04-root-INFO: grad norm: 31.027 30.470 5.849
2024-12-02-07:55:05-root-INFO: Loss too large (504.257->510.864)! Learning rate decreased to 0.01426.
2024-12-02-07:55:05-root-INFO: Loss too large (504.257->506.078)! Learning rate decreased to 0.01141.
2024-12-02-07:55:05-root-INFO: grad norm: 35.985 35.573 5.429
2024-12-02-07:55:06-root-INFO: grad norm: 46.361 45.985 5.897
2024-12-02-07:55:06-root-INFO: Loss too large (503.293->505.469)! Learning rate decreased to 0.00913.
2024-12-02-07:55:06-root-INFO: grad norm: 37.537 37.091 5.770
2024-12-02-07:55:07-root-INFO: grad norm: 28.600 28.263 4.371
2024-12-02-07:55:07-root-INFO: grad norm: 25.057 24.636 4.577
2024-12-02-07:55:08-root-INFO: grad norm: 22.367 22.002 4.028
2024-12-02-07:55:08-root-INFO: grad norm: 20.818 20.379 4.251
2024-12-02-07:55:09-root-INFO: Loss Change: 504.257 -> 492.297
2024-12-02-07:55:09-root-INFO: Regularization Change: 0.000 -> 0.818
2024-12-02-07:55:09-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-07:55:09-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:55:09-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-07:55:09-root-INFO: grad norm: 41.305 40.595 7.623
2024-12-02-07:55:09-root-INFO: Loss too large (495.525->510.859)! Learning rate decreased to 0.01478.
2024-12-02-07:55:09-root-INFO: Loss too large (495.525->501.018)! Learning rate decreased to 0.01182.
2024-12-02-07:55:10-root-INFO: grad norm: 48.098 47.673 6.379
2024-12-02-07:55:10-root-INFO: Loss too large (495.332->496.284)! Learning rate decreased to 0.00946.
2024-12-02-07:55:11-root-INFO: grad norm: 38.483 38.139 5.134
2024-12-02-07:55:11-root-INFO: grad norm: 31.908 31.529 4.906
2024-12-02-07:55:12-root-INFO: grad norm: 27.208 26.886 4.174
2024-12-02-07:55:12-root-INFO: grad norm: 24.437 24.039 4.394
2024-12-02-07:55:13-root-INFO: grad norm: 22.327 21.976 3.941
2024-12-02-07:55:13-root-INFO: grad norm: 21.167 20.743 4.218
2024-12-02-07:55:13-root-INFO: Loss Change: 495.525 -> 481.850
2024-12-02-07:55:13-root-INFO: Regularization Change: 0.000 -> 0.812
2024-12-02-07:55:13-root-INFO: Undo step: 145
2024-12-02-07:55:13-root-INFO: Undo step: 146
2024-12-02-07:55:13-root-INFO: Undo step: 147
2024-12-02-07:55:13-root-INFO: Undo step: 148
2024-12-02-07:55:13-root-INFO: Undo step: 149
2024-12-02-07:55:14-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-07:55:14-root-INFO: grad norm: 221.768 216.529 47.921
2024-12-02-07:55:14-root-INFO: grad norm: 172.347 168.756 34.998
2024-12-02-07:55:14-root-INFO: Loss too large (836.174->898.769)! Learning rate decreased to 0.01235.
2024-12-02-07:55:15-root-INFO: grad norm: 288.674 281.823 62.517
2024-12-02-07:55:15-root-INFO: Loss too large (795.022->995.574)! Learning rate decreased to 0.00988.
2024-12-02-07:55:15-root-INFO: Loss too large (795.022->862.397)! Learning rate decreased to 0.00790.
2024-12-02-07:55:16-root-INFO: grad norm: 151.974 150.167 23.366
2024-12-02-07:55:16-root-INFO: grad norm: 71.807 70.424 14.030
2024-12-02-07:55:17-root-INFO: grad norm: 47.812 46.425 11.432
2024-12-02-07:55:17-root-INFO: grad norm: 41.629 40.037 11.402
2024-12-02-07:55:18-root-INFO: grad norm: 37.219 35.799 10.182
2024-12-02-07:55:18-root-INFO: Loss Change: 1024.845 -> 594.995
2024-12-02-07:55:18-root-INFO: Regularization Change: 0.000 -> 21.308
2024-12-02-07:55:18-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-07:55:18-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:55:18-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-07:55:18-root-INFO: grad norm: 49.413 47.954 11.919
2024-12-02-07:55:19-root-INFO: grad norm: 88.870 87.750 14.062
2024-12-02-07:55:19-root-INFO: Loss too large (592.330->651.076)! Learning rate decreased to 0.01281.
2024-12-02-07:55:19-root-INFO: Loss too large (592.330->611.824)! Learning rate decreased to 0.01024.
2024-12-02-07:55:20-root-INFO: grad norm: 95.891 94.911 13.674
2024-12-02-07:55:20-root-INFO: grad norm: 98.428 97.345 14.557
2024-12-02-07:55:21-root-INFO: grad norm: 99.274 98.339 13.592
2024-12-02-07:55:21-root-INFO: grad norm: 97.196 96.139 14.297
2024-12-02-07:55:22-root-INFO: grad norm: 93.959 93.066 12.919
2024-12-02-07:55:22-root-INFO: grad norm: 90.316 89.312 13.428
2024-12-02-07:55:23-root-INFO: Loss Change: 594.187 -> 558.081
2024-12-02-07:55:23-root-INFO: Regularization Change: 0.000 -> 4.622
2024-12-02-07:55:23-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-07:55:23-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:55:23-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-07:55:23-root-INFO: grad norm: 113.927 112.532 17.776
2024-12-02-07:55:23-root-INFO: Loss too large (571.464->656.243)! Learning rate decreased to 0.01328.
2024-12-02-07:55:23-root-INFO: Loss too large (571.464->599.793)! Learning rate decreased to 0.01062.
2024-12-02-07:55:24-root-INFO: grad norm: 102.377 101.324 14.643
2024-12-02-07:55:24-root-INFO: grad norm: 88.576 87.624 12.954
2024-12-02-07:55:25-root-INFO: grad norm: 78.898 77.978 12.011
2024-12-02-07:55:25-root-INFO: grad norm: 70.574 69.789 10.497
2024-12-02-07:55:26-root-INFO: grad norm: 64.728 63.946 10.033
2024-12-02-07:55:26-root-INFO: grad norm: 59.654 58.966 9.033
2024-12-02-07:55:26-root-INFO: grad norm: 55.882 55.197 8.725
2024-12-02-07:55:27-root-INFO: Loss Change: 571.464 -> 521.465
2024-12-02-07:55:27-root-INFO: Regularization Change: 0.000 -> 2.118
2024-12-02-07:55:27-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-07:55:27-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-07:55:27-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-07:55:27-root-INFO: grad norm: 73.280 72.248 12.254
2024-12-02-07:55:27-root-INFO: Loss too large (527.004->564.889)! Learning rate decreased to 0.01376.
2024-12-02-07:55:27-root-INFO: Loss too large (527.004->538.874)! Learning rate decreased to 0.01101.
2024-12-02-07:55:28-root-INFO: grad norm: 68.000 67.302 9.717
2024-12-02-07:55:28-root-INFO: grad norm: 62.498 61.796 9.342
2024-12-02-07:55:29-root-INFO: grad norm: 57.803 57.130 8.793
2024-12-02-07:55:29-root-INFO: grad norm: 53.474 52.848 8.156
2024-12-02-07:55:30-root-INFO: grad norm: 50.185 49.562 7.886
2024-12-02-07:55:30-root-INFO: grad norm: 47.193 46.602 7.442
2024-12-02-07:55:31-root-INFO: grad norm: 44.944 44.357 7.240
2024-12-02-07:55:31-root-INFO: Loss Change: 527.004 -> 500.662
2024-12-02-07:55:31-root-INFO: Regularization Change: 0.000 -> 1.525
2024-12-02-07:55:31-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-07:55:31-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:55:31-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-07:55:32-root-INFO: grad norm: 59.923 59.021 10.360
2024-12-02-07:55:32-root-INFO: Loss too large (504.594->530.942)! Learning rate decreased to 0.01426.
2024-12-02-07:55:32-root-INFO: Loss too large (504.594->513.101)! Learning rate decreased to 0.01141.
2024-12-02-07:55:32-root-INFO: grad norm: 56.124 55.464 8.583
2024-12-02-07:55:33-root-INFO: grad norm: 52.029 51.381 8.186
2024-12-02-07:55:33-root-INFO: grad norm: 48.906 48.257 7.941
2024-12-02-07:55:34-root-INFO: grad norm: 45.933 45.328 7.433
2024-12-02-07:55:34-root-INFO: grad norm: 43.688 43.075 7.295
2024-12-02-07:55:35-root-INFO: grad norm: 41.596 41.012 6.941
2024-12-02-07:55:35-root-INFO: grad norm: 39.996 39.414 6.798
2024-12-02-07:55:35-root-INFO: Loss Change: 504.594 -> 485.073
2024-12-02-07:55:35-root-INFO: Regularization Change: 0.000 -> 1.278
2024-12-02-07:55:35-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-07:55:35-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:55:36-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-07:55:36-root-INFO: grad norm: 63.177 62.078 11.731
2024-12-02-07:55:36-root-INFO: Loss too large (491.790->522.314)! Learning rate decreased to 0.01478.
2024-12-02-07:55:36-root-INFO: Loss too large (491.790->501.939)! Learning rate decreased to 0.01182.
2024-12-02-07:55:37-root-INFO: grad norm: 59.214 58.516 9.066
2024-12-02-07:55:37-root-INFO: grad norm: 54.927 54.223 8.768
2024-12-02-07:55:37-root-INFO: grad norm: 51.061 50.373 8.359
2024-12-02-07:55:38-root-INFO: grad norm: 47.171 46.545 7.657
2024-12-02-07:55:38-root-INFO: grad norm: 44.391 43.756 7.485
2024-12-02-07:55:39-root-INFO: grad norm: 41.707 41.127 6.933
2024-12-02-07:55:39-root-INFO: grad norm: 39.755 39.169 6.802
2024-12-02-07:55:40-root-INFO: Loss Change: 491.790 -> 471.986
2024-12-02-07:55:40-root-INFO: Regularization Change: 0.000 -> 1.214
2024-12-02-07:55:40-root-INFO: Undo step: 145
2024-12-02-07:55:40-root-INFO: Undo step: 146
2024-12-02-07:55:40-root-INFO: Undo step: 147
2024-12-02-07:55:40-root-INFO: Undo step: 148
2024-12-02-07:55:40-root-INFO: Undo step: 149
2024-12-02-07:55:40-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-07:55:40-root-INFO: grad norm: 259.638 253.650 55.439
2024-12-02-07:55:40-root-INFO: grad norm: 127.889 125.275 25.723
2024-12-02-07:55:41-root-INFO: grad norm: 116.310 114.005 23.041
2024-12-02-07:55:41-root-INFO: grad norm: 113.084 110.565 23.733
2024-12-02-07:55:42-root-INFO: grad norm: 165.251 162.828 28.195
2024-12-02-07:55:42-root-INFO: Loss too large (649.476->759.145)! Learning rate decreased to 0.01235.
2024-12-02-07:55:42-root-INFO: Loss too large (649.476->682.817)! Learning rate decreased to 0.00988.
2024-12-02-07:55:43-root-INFO: grad norm: 124.479 122.587 21.623
2024-12-02-07:55:43-root-INFO: grad norm: 80.083 79.090 12.569
2024-12-02-07:55:44-root-INFO: grad norm: 67.386 66.439 11.253
2024-12-02-07:55:44-root-INFO: Loss Change: 1129.035 -> 559.958
2024-12-02-07:55:44-root-INFO: Regularization Change: 0.000 -> 31.550
2024-12-02-07:55:44-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-07:55:44-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:55:44-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-07:55:44-root-INFO: grad norm: 84.807 83.560 14.490
2024-12-02-07:55:44-root-INFO: Loss too large (564.696->601.207)! Learning rate decreased to 0.01281.
2024-12-02-07:55:45-root-INFO: Loss too large (564.696->574.106)! Learning rate decreased to 0.01024.
2024-12-02-07:55:45-root-INFO: grad norm: 76.644 75.767 11.565
2024-12-02-07:55:46-root-INFO: grad norm: 67.357 66.628 9.885
2024-12-02-07:55:46-root-INFO: grad norm: 63.175 62.377 10.006
2024-12-02-07:55:46-root-INFO: grad norm: 60.542 59.896 8.823
2024-12-02-07:55:47-root-INFO: grad norm: 60.618 59.773 10.083
2024-12-02-07:55:47-root-INFO: grad norm: 61.535 60.852 9.144
2024-12-02-07:55:48-root-INFO: grad norm: 65.559 64.529 11.579
2024-12-02-07:55:48-root-INFO: Loss Change: 564.696 -> 523.647
2024-12-02-07:55:48-root-INFO: Regularization Change: 0.000 -> 3.121
2024-12-02-07:55:48-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-07:55:48-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-07:55:48-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-07:55:48-root-INFO: grad norm: 97.360 96.137 15.386
2024-12-02-07:55:49-root-INFO: Loss too large (534.295->605.977)! Learning rate decreased to 0.01328.
2024-12-02-07:55:49-root-INFO: Loss too large (534.295->557.285)! Learning rate decreased to 0.01062.
2024-12-02-07:55:49-root-INFO: grad norm: 92.447 91.253 14.812
2024-12-02-07:55:50-root-INFO: grad norm: 82.700 81.782 12.284
2024-12-02-07:55:50-root-INFO: grad norm: 75.935 74.819 12.971
2024-12-02-07:55:51-root-INFO: grad norm: 71.352 70.532 10.783
2024-12-02-07:55:51-root-INFO: grad norm: 67.374 66.327 11.830
2024-12-02-07:55:52-root-INFO: grad norm: 64.417 63.650 9.910
2024-12-02-07:55:52-root-INFO: grad norm: 61.760 60.770 11.017
2024-12-02-07:55:52-root-INFO: Loss Change: 534.295 -> 499.316
2024-12-02-07:55:52-root-INFO: Regularization Change: 0.000 -> 1.815
2024-12-02-07:55:52-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-07:55:52-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-07:55:53-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-07:55:53-root-INFO: grad norm: 78.929 77.876 12.852
2024-12-02-07:55:53-root-INFO: Loss too large (505.156->550.825)! Learning rate decreased to 0.01376.
2024-12-02-07:55:53-root-INFO: Loss too large (505.156->517.600)! Learning rate decreased to 0.01101.
2024-12-02-07:55:54-root-INFO: grad norm: 71.672 70.809 11.087
2024-12-02-07:55:54-root-INFO: grad norm: 65.189 64.487 9.542
2024-12-02-07:55:54-root-INFO: grad norm: 59.733 58.968 9.523
2024-12-02-07:55:55-root-INFO: grad norm: 55.580 54.987 8.101
2024-12-02-07:55:55-root-INFO: grad norm: 52.122 51.436 8.429
2024-12-02-07:55:56-root-INFO: grad norm: 49.372 48.838 7.240
2024-12-02-07:55:56-root-INFO: grad norm: 47.086 46.454 7.690
2024-12-02-07:55:57-root-INFO: Loss Change: 505.156 -> 477.872
2024-12-02-07:55:57-root-INFO: Regularization Change: 0.000 -> 1.362
2024-12-02-07:55:57-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-07:55:57-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:55:57-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-07:55:57-root-INFO: grad norm: 63.532 62.696 10.272
2024-12-02-07:55:57-root-INFO: Loss too large (482.789->512.669)! Learning rate decreased to 0.01426.
2024-12-02-07:55:57-root-INFO: Loss too large (482.789->491.505)! Learning rate decreased to 0.01141.
2024-12-02-07:55:58-root-INFO: grad norm: 58.299 57.660 8.605
2024-12-02-07:55:58-root-INFO: grad norm: 53.382 52.855 7.482
2024-12-02-07:55:59-root-INFO: grad norm: 49.478 48.918 7.423
2024-12-02-07:55:59-root-INFO: grad norm: 46.221 45.774 6.416
2024-12-02-07:56:00-root-INFO: grad norm: 43.681 43.184 6.568
2024-12-02-07:56:00-root-INFO: grad norm: 41.592 41.187 5.795
2024-12-02-07:56:01-root-INFO: grad norm: 39.962 39.508 6.006
2024-12-02-07:56:01-root-INFO: Loss Change: 482.789 -> 463.036
2024-12-02-07:56:01-root-INFO: Regularization Change: 0.000 -> 1.152
2024-12-02-07:56:01-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-07:56:01-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:56:01-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-07:56:01-root-INFO: grad norm: 68.923 67.933 11.642
2024-12-02-07:56:01-root-INFO: Loss too large (471.604->508.598)! Learning rate decreased to 0.01478.
2024-12-02-07:56:01-root-INFO: Loss too large (471.604->484.017)! Learning rate decreased to 0.01182.
2024-12-02-07:56:02-root-INFO: grad norm: 64.055 63.445 8.817
2024-12-02-07:56:02-root-INFO: grad norm: 58.691 58.127 8.119
2024-12-02-07:56:03-root-INFO: grad norm: 54.369 53.817 7.725
2024-12-02-07:56:03-root-INFO: grad norm: 50.526 50.054 6.890
2024-12-02-07:56:04-root-INFO: grad norm: 47.588 47.090 6.864
2024-12-02-07:56:04-root-INFO: grad norm: 44.794 44.366 6.173
2024-12-02-07:56:05-root-INFO: grad norm: 42.756 42.295 6.258
2024-12-02-07:56:05-root-INFO: Loss Change: 471.604 -> 451.498
2024-12-02-07:56:05-root-INFO: Regularization Change: 0.000 -> 1.099
2024-12-02-07:56:05-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-07:56:05-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:56:05-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-07:56:05-root-INFO: grad norm: 60.308 59.468 10.033
2024-12-02-07:56:06-root-INFO: Loss too large (456.235->484.947)! Learning rate decreased to 0.01531.
2024-12-02-07:56:06-root-INFO: Loss too large (456.235->465.821)! Learning rate decreased to 0.01225.
2024-12-02-07:56:06-root-INFO: grad norm: 54.966 54.423 7.703
2024-12-02-07:56:07-root-INFO: grad norm: 49.276 48.773 7.022
2024-12-02-07:56:07-root-INFO: grad norm: 45.449 44.954 6.694
2024-12-02-07:56:08-root-INFO: grad norm: 41.743 41.313 5.972
2024-12-02-07:56:08-root-INFO: grad norm: 39.246 38.795 5.935
2024-12-02-07:56:09-root-INFO: grad norm: 36.894 36.498 5.388
2024-12-02-07:56:09-root-INFO: grad norm: 35.251 34.829 5.441
2024-12-02-07:56:09-root-INFO: Loss Change: 456.235 -> 438.679
2024-12-02-07:56:09-root-INFO: Regularization Change: 0.000 -> 1.020
2024-12-02-07:56:09-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-07:56:09-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-07:56:10-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-07:56:10-root-INFO: grad norm: 50.970 50.278 8.374
2024-12-02-07:56:10-root-INFO: Loss too large (442.220->463.827)! Learning rate decreased to 0.01586.
2024-12-02-07:56:10-root-INFO: Loss too large (442.220->449.393)! Learning rate decreased to 0.01269.
2024-12-02-07:56:11-root-INFO: grad norm: 47.854 47.351 6.924
2024-12-02-07:56:11-root-INFO: grad norm: 45.482 45.015 6.502
2024-12-02-07:56:12-root-INFO: grad norm: 43.136 42.649 6.462
2024-12-02-07:56:12-root-INFO: grad norm: 40.700 40.273 5.878
2024-12-02-07:56:13-root-INFO: grad norm: 38.985 38.526 5.961
2024-12-02-07:56:13-root-INFO: grad norm: 37.235 36.829 5.485
2024-12-02-07:56:13-root-INFO: grad norm: 36.018 35.581 5.591
2024-12-02-07:56:14-root-INFO: Loss Change: 442.220 -> 428.485
2024-12-02-07:56:14-root-INFO: Regularization Change: 0.000 -> 0.967
2024-12-02-07:56:14-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-07:56:14-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-07:56:14-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-07:56:14-root-INFO: grad norm: 60.363 59.371 10.897
2024-12-02-07:56:14-root-INFO: Loss too large (435.540->466.795)! Learning rate decreased to 0.01642.
2024-12-02-07:56:14-root-INFO: Loss too large (435.540->446.748)! Learning rate decreased to 0.01314.
2024-12-02-07:56:15-root-INFO: grad norm: 55.882 55.306 8.002
2024-12-02-07:56:15-root-INFO: grad norm: 50.984 50.430 7.498
2024-12-02-07:56:16-root-INFO: grad norm: 47.517 46.981 7.114
2024-12-02-07:56:16-root-INFO: grad norm: 43.852 43.385 6.385
2024-12-02-07:56:17-root-INFO: grad norm: 41.555 41.064 6.365
2024-12-02-07:56:17-root-INFO: grad norm: 39.301 38.872 5.786
2024-12-02-07:56:18-root-INFO: grad norm: 37.772 37.311 5.880
2024-12-02-07:56:18-root-INFO: Loss Change: 435.540 -> 418.939
2024-12-02-07:56:18-root-INFO: Regularization Change: 0.000 -> 1.016
2024-12-02-07:56:18-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-07:56:18-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-07:56:18-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-07:56:18-root-INFO: grad norm: 49.363 48.716 7.965
2024-12-02-07:56:18-root-INFO: Loss too large (421.968->443.968)! Learning rate decreased to 0.01701.
2024-12-02-07:56:19-root-INFO: Loss too large (421.968->430.016)! Learning rate decreased to 0.01361.
2024-12-02-07:56:19-root-INFO: grad norm: 45.870 45.373 6.732
2024-12-02-07:56:19-root-INFO: grad norm: 41.865 41.423 6.066
2024-12-02-07:56:20-root-INFO: grad norm: 39.675 39.204 6.096
2024-12-02-07:56:20-root-INFO: grad norm: 37.540 37.133 5.511
2024-12-02-07:56:21-root-INFO: grad norm: 36.076 35.631 5.652
2024-12-02-07:56:21-root-INFO: grad norm: 34.659 34.270 5.177
2024-12-02-07:56:22-root-INFO: grad norm: 33.680 33.253 5.349
2024-12-02-07:56:22-root-INFO: Loss Change: 421.968 -> 409.160
2024-12-02-07:56:22-root-INFO: Regularization Change: 0.000 -> 0.934
2024-12-02-07:56:22-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-07:56:22-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-07:56:22-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-07:56:23-root-INFO: grad norm: 48.884 48.173 8.306
2024-12-02-07:56:23-root-INFO: Loss too large (413.255->435.218)! Learning rate decreased to 0.01761.
2024-12-02-07:56:23-root-INFO: Loss too large (413.255->421.288)! Learning rate decreased to 0.01409.
2024-12-02-07:56:23-root-INFO: grad norm: 45.394 44.888 6.758
2024-12-02-07:56:24-root-INFO: grad norm: 41.650 41.195 6.143
2024-12-02-07:56:24-root-INFO: grad norm: 39.574 39.093 6.154
2024-12-02-07:56:25-root-INFO: grad norm: 37.491 37.081 5.530
2024-12-02-07:56:25-root-INFO: grad norm: 36.071 35.617 5.708
2024-12-02-07:56:26-root-INFO: grad norm: 34.657 34.269 5.171
2024-12-02-07:56:26-root-INFO: Loss too large (402.845->402.851)! Learning rate decreased to 0.01127.
2024-12-02-07:56:26-root-INFO: grad norm: 23.718 23.356 4.128
2024-12-02-07:56:27-root-INFO: Loss Change: 413.255 -> 398.278
2024-12-02-07:56:27-root-INFO: Regularization Change: 0.000 -> 0.911
2024-12-02-07:56:27-root-INFO: Undo step: 140
2024-12-02-07:56:27-root-INFO: Undo step: 141
2024-12-02-07:56:27-root-INFO: Undo step: 142
2024-12-02-07:56:27-root-INFO: Undo step: 143
2024-12-02-07:56:27-root-INFO: Undo step: 144
2024-12-02-07:56:27-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-07:56:27-root-INFO: grad norm: 292.947 287.172 57.882
2024-12-02-07:56:27-root-INFO: grad norm: 146.427 143.426 29.490
2024-12-02-07:56:28-root-INFO: grad norm: 105.279 103.303 20.299
2024-12-02-07:56:28-root-INFO: grad norm: 101.052 99.463 17.851
2024-12-02-07:56:29-root-INFO: grad norm: 126.811 124.400 24.612
2024-12-02-07:56:29-root-INFO: Loss too large (558.177->603.562)! Learning rate decreased to 0.01478.
2024-12-02-07:56:30-root-INFO: grad norm: 102.322 100.734 17.957
2024-12-02-07:56:30-root-INFO: grad norm: 64.139 62.888 12.604
2024-12-02-07:56:30-root-INFO: grad norm: 57.377 56.375 10.681
2024-12-02-07:56:31-root-INFO: Loss Change: 1082.633 -> 482.853
2024-12-02-07:56:31-root-INFO: Regularization Change: 0.000 -> 40.420
2024-12-02-07:56:31-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-07:56:31-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:56:31-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-07:56:31-root-INFO: grad norm: 69.127 68.088 11.937
2024-12-02-07:56:31-root-INFO: Loss too large (487.377->504.988)! Learning rate decreased to 0.01531.
2024-12-02-07:56:32-root-INFO: grad norm: 77.108 76.414 10.326
2024-12-02-07:56:32-root-INFO: grad norm: 85.883 85.152 11.186
2024-12-02-07:56:33-root-INFO: Loss too large (484.379->485.767)! Learning rate decreased to 0.01225.
2024-12-02-07:56:33-root-INFO: grad norm: 62.119 61.497 8.772
2024-12-02-07:56:33-root-INFO: grad norm: 46.688 46.267 6.256
2024-12-02-07:56:34-root-INFO: grad norm: 36.974 36.477 6.042
2024-12-02-07:56:34-root-INFO: grad norm: 30.459 30.085 4.758
2024-12-02-07:56:35-root-INFO: grad norm: 25.618 25.150 4.874
2024-12-02-07:56:35-root-INFO: Loss Change: 487.377 -> 441.697
2024-12-02-07:56:35-root-INFO: Regularization Change: 0.000 -> 3.201
2024-12-02-07:56:35-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-07:56:35-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-07:56:35-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-07:56:35-root-INFO: grad norm: 38.867 38.293 6.654
2024-12-02-07:56:36-root-INFO: Loss too large (443.862->450.219)! Learning rate decreased to 0.01586.
2024-12-02-07:56:36-root-INFO: grad norm: 46.760 46.267 6.769
2024-12-02-07:56:36-root-INFO: Loss too large (443.259->444.774)! Learning rate decreased to 0.01269.
2024-12-02-07:56:37-root-INFO: grad norm: 38.098 37.725 5.319
2024-12-02-07:56:37-root-INFO: grad norm: 31.366 30.942 5.138
2024-12-02-07:56:38-root-INFO: grad norm: 26.448 26.117 4.170
2024-12-02-07:56:38-root-INFO: grad norm: 22.690 22.293 4.225
2024-12-02-07:56:39-root-INFO: grad norm: 19.842 19.513 3.597
2024-12-02-07:56:39-root-INFO: grad norm: 17.636 17.243 3.703
2024-12-02-07:56:39-root-INFO: Loss Change: 443.862 -> 424.380
2024-12-02-07:56:39-root-INFO: Regularization Change: 0.000 -> 1.621
2024-12-02-07:56:39-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-07:56:39-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-07:56:39-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-07:56:40-root-INFO: grad norm: 39.444 38.616 8.039
2024-12-02-07:56:40-root-INFO: Loss too large (428.133->435.702)! Learning rate decreased to 0.01642.
2024-12-02-07:56:40-root-INFO: Loss too large (428.133->428.177)! Learning rate decreased to 0.01314.
2024-12-02-07:56:40-root-INFO: grad norm: 32.698 32.344 4.795
2024-12-02-07:56:41-root-INFO: grad norm: 28.151 27.780 4.557
2024-12-02-07:56:41-root-INFO: grad norm: 24.772 24.427 4.119
2024-12-02-07:56:42-root-INFO: grad norm: 22.243 21.930 3.720
2024-12-02-07:56:42-root-INFO: grad norm: 20.494 20.156 3.706
2024-12-02-07:56:43-root-INFO: grad norm: 19.649 19.345 3.443
2024-12-02-07:56:43-root-INFO: grad norm: 19.361 19.027 3.580
2024-12-02-07:56:44-root-INFO: Loss Change: 428.133 -> 411.716
2024-12-02-07:56:44-root-INFO: Regularization Change: 0.000 -> 1.309
2024-12-02-07:56:44-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-07:56:44-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-07:56:44-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-07:56:44-root-INFO: grad norm: 31.292 30.847 5.260
2024-12-02-07:56:44-root-INFO: Loss too large (412.871->419.413)! Learning rate decreased to 0.01701.
2024-12-02-07:56:44-root-INFO: Loss too large (412.871->414.234)! Learning rate decreased to 0.01361.
2024-12-02-07:56:45-root-INFO: grad norm: 29.636 29.313 4.360
2024-12-02-07:56:45-root-INFO: grad norm: 30.449 30.114 4.502
2024-12-02-07:56:46-root-INFO: grad norm: 29.581 29.232 4.532
2024-12-02-07:56:46-root-INFO: grad norm: 28.698 28.372 4.313
2024-12-02-07:56:46-root-INFO: grad norm: 27.849 27.503 4.378
2024-12-02-07:56:47-root-INFO: grad norm: 26.810 26.493 4.113
2024-12-02-07:56:47-root-INFO: grad norm: 26.264 25.925 4.206
2024-12-02-07:56:48-root-INFO: Loss Change: 412.871 -> 401.591
2024-12-02-07:56:48-root-INFO: Regularization Change: 0.000 -> 1.101
2024-12-02-07:56:48-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-07:56:48-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-07:56:48-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-07:56:48-root-INFO: grad norm: 38.587 38.094 6.151
2024-12-02-07:56:48-root-INFO: Loss too large (403.918->415.770)! Learning rate decreased to 0.01761.
2024-12-02-07:56:48-root-INFO: Loss too large (403.918->407.692)! Learning rate decreased to 0.01409.
2024-12-02-07:56:49-root-INFO: grad norm: 35.522 35.180 4.922
2024-12-02-07:56:49-root-INFO: grad norm: 33.125 32.799 4.631
2024-12-02-07:56:50-root-INFO: grad norm: 31.590 31.247 4.644
2024-12-02-07:56:50-root-INFO: grad norm: 30.103 29.794 4.303
2024-12-02-07:56:51-root-INFO: grad norm: 29.145 28.808 4.422
2024-12-02-07:56:51-root-INFO: grad norm: 28.273 27.969 4.135
2024-12-02-07:56:52-root-INFO: grad norm: 27.659 27.327 4.273
2024-12-02-07:56:52-root-INFO: Loss Change: 403.918 -> 391.975
2024-12-02-07:56:52-root-INFO: Regularization Change: 0.000 -> 1.055
2024-12-02-07:56:52-root-INFO: Undo step: 140
2024-12-02-07:56:52-root-INFO: Undo step: 141
2024-12-02-07:56:52-root-INFO: Undo step: 142
2024-12-02-07:56:52-root-INFO: Undo step: 143
2024-12-02-07:56:52-root-INFO: Undo step: 144
2024-12-02-07:56:52-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-07:56:52-root-INFO: grad norm: 290.878 284.876 58.784
2024-12-02-07:56:53-root-INFO: grad norm: 225.291 221.336 42.025
2024-12-02-07:56:53-root-INFO: grad norm: 195.946 191.799 40.097
2024-12-02-07:56:54-root-INFO: grad norm: 259.761 255.955 44.307
2024-12-02-07:56:54-root-INFO: Loss too large (662.618->1002.537)! Learning rate decreased to 0.01478.
2024-12-02-07:56:54-root-INFO: Loss too large (662.618->815.993)! Learning rate decreased to 0.01182.
2024-12-02-07:56:54-root-INFO: Loss too large (662.618->699.400)! Learning rate decreased to 0.00946.
2024-12-02-07:56:55-root-INFO: grad norm: 142.186 140.382 22.580
2024-12-02-07:56:55-root-INFO: grad norm: 49.018 48.078 9.556
2024-12-02-07:56:56-root-INFO: grad norm: 30.777 29.699 8.076
2024-12-02-07:56:56-root-INFO: grad norm: 27.066 26.024 7.438
2024-12-02-07:56:56-root-INFO: Loss Change: 1100.825 -> 474.943
2024-12-02-07:56:56-root-INFO: Regularization Change: 0.000 -> 36.097
2024-12-02-07:56:56-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-07:56:56-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-07:56:57-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-07:56:57-root-INFO: grad norm: 31.245 30.363 7.371
2024-12-02-07:56:57-root-INFO: grad norm: 35.722 34.778 8.158
2024-12-02-07:56:58-root-INFO: grad norm: 53.306 52.597 8.666
2024-12-02-07:56:58-root-INFO: Loss too large (461.300->472.725)! Learning rate decreased to 0.01531.
2024-12-02-07:56:58-root-INFO: grad norm: 62.401 61.428 10.978
2024-12-02-07:56:58-root-INFO: Loss too large (460.169->461.284)! Learning rate decreased to 0.01225.
2024-12-02-07:56:59-root-INFO: grad norm: 48.545 47.913 7.805
2024-12-02-07:56:59-root-INFO: grad norm: 39.418 38.675 7.617
2024-12-02-07:57:00-root-INFO: grad norm: 33.745 33.194 6.072
2024-12-02-07:57:00-root-INFO: grad norm: 29.924 29.283 6.161
2024-12-02-07:57:01-root-INFO: Loss Change: 472.993 -> 434.294
2024-12-02-07:57:01-root-INFO: Regularization Change: 0.000 -> 4.409
2024-12-02-07:57:01-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-07:57:01-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-07:57:01-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-07:57:01-root-INFO: grad norm: 45.148 44.391 8.233
2024-12-02-07:57:01-root-INFO: Loss too large (437.025->449.446)! Learning rate decreased to 0.01586.
2024-12-02-07:57:01-root-INFO: Loss too large (437.025->438.417)! Learning rate decreased to 0.01269.
2024-12-02-07:57:02-root-INFO: grad norm: 40.990 40.363 7.145
2024-12-02-07:57:02-root-INFO: grad norm: 38.214 37.659 6.489
2024-12-02-07:57:03-root-INFO: grad norm: 37.914 37.326 6.651
2024-12-02-07:57:03-root-INFO: grad norm: 35.983 35.455 6.142
2024-12-02-07:57:04-root-INFO: grad norm: 34.292 33.756 6.040
2024-12-02-07:57:04-root-INFO: grad norm: 33.221 32.720 5.749
2024-12-02-07:57:05-root-INFO: grad norm: 32.339 31.841 5.652
2024-12-02-07:57:05-root-INFO: Loss Change: 437.025 -> 417.834
2024-12-02-07:57:05-root-INFO: Regularization Change: 0.000 -> 1.708
2024-12-02-07:57:05-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-07:57:05-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-07:57:05-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-07:57:05-root-INFO: grad norm: 51.854 50.850 10.158
2024-12-02-07:57:05-root-INFO: Loss too large (422.339->443.768)! Learning rate decreased to 0.01642.
2024-12-02-07:57:06-root-INFO: Loss too large (422.339->427.110)! Learning rate decreased to 0.01314.
2024-12-02-07:57:06-root-INFO: grad norm: 49.207 48.622 7.568
2024-12-02-07:57:07-root-INFO: grad norm: 45.613 45.011 7.386
2024-12-02-07:57:07-root-INFO: grad norm: 42.070 41.505 6.869
2024-12-02-07:57:07-root-INFO: grad norm: 40.399 39.865 6.546
2024-12-02-07:57:08-root-INFO: grad norm: 39.546 39.008 6.500
2024-12-02-07:57:08-root-INFO: grad norm: 38.118 37.613 6.184
2024-12-02-07:57:09-root-INFO: grad norm: 36.665 36.158 6.077
2024-12-02-07:57:09-root-INFO: Loss Change: 422.339 -> 405.264
2024-12-02-07:57:09-root-INFO: Regularization Change: 0.000 -> 1.345
2024-12-02-07:57:09-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-07:57:09-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-07:57:09-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-07:57:09-root-INFO: grad norm: 45.750 45.058 7.922
2024-12-02-07:57:10-root-INFO: Loss too large (407.015->425.873)! Learning rate decreased to 0.01701.
2024-12-02-07:57:10-root-INFO: Loss too large (407.015->411.446)! Learning rate decreased to 0.01361.
2024-12-02-07:57:10-root-INFO: grad norm: 44.309 43.789 6.770
2024-12-02-07:57:11-root-INFO: grad norm: 41.936 41.416 6.583
2024-12-02-07:57:11-root-INFO: grad norm: 39.401 38.901 6.255
2024-12-02-07:57:12-root-INFO: grad norm: 38.176 37.696 6.033
2024-12-02-07:57:12-root-INFO: grad norm: 37.480 37.004 5.956
2024-12-02-07:57:12-root-INFO: Loss too large (396.677->396.817)! Learning rate decreased to 0.01088.
2024-12-02-07:57:13-root-INFO: grad norm: 26.441 26.071 4.409
2024-12-02-07:57:13-root-INFO: grad norm: 18.604 18.290 3.401
2024-12-02-07:57:14-root-INFO: Loss Change: 407.015 -> 390.457
2024-12-02-07:57:14-root-INFO: Regularization Change: 0.000 -> 1.012
2024-12-02-07:57:14-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-07:57:14-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-07:57:14-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-07:57:14-root-INFO: grad norm: 27.847 27.255 5.712
2024-12-02-07:57:14-root-INFO: Loss too large (391.572->397.074)! Learning rate decreased to 0.01761.
2024-12-02-07:57:14-root-INFO: Loss too large (391.572->392.347)! Learning rate decreased to 0.01409.
2024-12-02-07:57:15-root-INFO: grad norm: 28.872 28.524 4.468
2024-12-02-07:57:15-root-INFO: grad norm: 30.652 30.254 4.929
2024-12-02-07:57:16-root-INFO: grad norm: 35.324 34.926 5.287
2024-12-02-07:57:16-root-INFO: Loss too large (388.424->389.494)! Learning rate decreased to 0.01127.
2024-12-02-07:57:16-root-INFO: grad norm: 26.778 26.430 4.300
2024-12-02-07:57:17-root-INFO: grad norm: 18.921 18.639 3.254
2024-12-02-07:57:17-root-INFO: grad norm: 17.259 16.960 3.195
2024-12-02-07:57:18-root-INFO: grad norm: 16.081 15.825 2.858
2024-12-02-07:57:18-root-INFO: Loss Change: 391.572 -> 381.330
2024-12-02-07:57:18-root-INFO: Regularization Change: 0.000 -> 0.833
2024-12-02-07:57:18-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-07:57:18-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:57:18-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-07:57:18-root-INFO: grad norm: 26.858 26.290 5.493
2024-12-02-07:57:18-root-INFO: Loss too large (382.568->388.776)! Learning rate decreased to 0.01823.
2024-12-02-07:57:19-root-INFO: Loss too large (382.568->383.871)! Learning rate decreased to 0.01458.
2024-12-02-07:57:19-root-INFO: grad norm: 30.221 29.907 4.345
2024-12-02-07:57:19-root-INFO: Loss too large (381.239->381.869)! Learning rate decreased to 0.01167.
2024-12-02-07:57:20-root-INFO: grad norm: 24.774 24.434 4.089
2024-12-02-07:57:20-root-INFO: grad norm: 21.168 20.909 3.301
2024-12-02-07:57:21-root-INFO: grad norm: 19.864 19.574 3.383
2024-12-02-07:57:21-root-INFO: grad norm: 18.920 18.681 2.996
2024-12-02-07:57:21-root-INFO: grad norm: 18.547 18.273 3.176
2024-12-02-07:57:22-root-INFO: grad norm: 18.244 18.014 2.889
2024-12-02-07:57:22-root-INFO: Loss Change: 382.568 -> 373.783
2024-12-02-07:57:22-root-INFO: Regularization Change: 0.000 -> 0.719
2024-12-02-07:57:22-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-07:57:22-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:57:22-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-07:57:23-root-INFO: grad norm: 23.060 22.634 4.410
2024-12-02-07:57:23-root-INFO: Loss too large (374.076->379.698)! Learning rate decreased to 0.01887.
2024-12-02-07:57:23-root-INFO: Loss too large (374.076->375.514)! Learning rate decreased to 0.01509.
2024-12-02-07:57:23-root-INFO: grad norm: 28.864 28.557 4.200
2024-12-02-07:57:23-root-INFO: Loss too large (373.251->374.479)! Learning rate decreased to 0.01207.
2024-12-02-07:57:24-root-INFO: grad norm: 24.592 24.292 3.831
2024-12-02-07:57:24-root-INFO: grad norm: 20.436 20.189 3.167
2024-12-02-07:57:25-root-INFO: grad norm: 19.549 19.281 3.226
2024-12-02-07:57:25-root-INFO: grad norm: 18.791 18.560 2.935
2024-12-02-07:57:26-root-INFO: grad norm: 18.539 18.282 3.076
2024-12-02-07:57:26-root-INFO: grad norm: 18.327 18.104 2.851
2024-12-02-07:57:27-root-INFO: Loss Change: 374.076 -> 366.624
2024-12-02-07:57:27-root-INFO: Regularization Change: 0.000 -> 0.673
2024-12-02-07:57:27-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-07:57:27-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:57:27-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-07:57:27-root-INFO: grad norm: 32.060 31.277 7.042
2024-12-02-07:57:27-root-INFO: Loss too large (368.926->378.790)! Learning rate decreased to 0.01952.
2024-12-02-07:57:27-root-INFO: Loss too large (368.926->371.382)! Learning rate decreased to 0.01562.
2024-12-02-07:57:28-root-INFO: grad norm: 35.714 35.377 4.895
2024-12-02-07:57:28-root-INFO: Loss too large (367.303->368.834)! Learning rate decreased to 0.01250.
2024-12-02-07:57:28-root-INFO: grad norm: 28.915 28.574 4.427
2024-12-02-07:57:29-root-INFO: grad norm: 24.251 23.988 3.561
2024-12-02-07:57:29-root-INFO: grad norm: 22.693 22.417 3.529
2024-12-02-07:57:30-root-INFO: grad norm: 21.619 21.382 3.191
2024-12-02-07:57:30-root-INFO: grad norm: 21.156 20.900 3.279
2024-12-02-07:57:31-root-INFO: grad norm: 20.773 20.549 3.047
2024-12-02-07:57:31-root-INFO: Loss Change: 368.926 -> 359.224
2024-12-02-07:57:31-root-INFO: Regularization Change: 0.000 -> 0.780
2024-12-02-07:57:31-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-07:57:31-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-07:57:31-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-07:57:31-root-INFO: grad norm: 23.048 22.710 3.934
2024-12-02-07:57:31-root-INFO: Loss too large (359.660->366.898)! Learning rate decreased to 0.02020.
2024-12-02-07:57:32-root-INFO: Loss too large (359.660->361.972)! Learning rate decreased to 0.01616.
2024-12-02-07:57:32-root-INFO: grad norm: 30.809 30.523 4.192
2024-12-02-07:57:32-root-INFO: Loss too large (359.228->361.228)! Learning rate decreased to 0.01293.
2024-12-02-07:57:33-root-INFO: grad norm: 26.178 25.904 3.776
2024-12-02-07:57:33-root-INFO: grad norm: 20.917 20.684 3.115
2024-12-02-07:57:34-root-INFO: grad norm: 20.125 19.882 3.113
2024-12-02-07:57:34-root-INFO: grad norm: 19.430 19.210 2.917
2024-12-02-07:57:35-root-INFO: grad norm: 19.186 18.953 2.983
2024-12-02-07:57:35-root-INFO: grad norm: 18.971 18.757 2.841
2024-12-02-07:57:35-root-INFO: Loss Change: 359.660 -> 352.706
2024-12-02-07:57:35-root-INFO: Regularization Change: 0.000 -> 0.666
2024-12-02-07:57:35-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-07:57:35-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-07:57:36-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-07:57:36-root-INFO: grad norm: 25.468 25.028 4.711
2024-12-02-07:57:36-root-INFO: Loss too large (353.425->361.613)! Learning rate decreased to 0.02090.
2024-12-02-07:57:36-root-INFO: Loss too large (353.425->355.892)! Learning rate decreased to 0.01672.
2024-12-02-07:57:37-root-INFO: grad norm: 31.627 31.339 4.256
2024-12-02-07:57:37-root-INFO: Loss too large (352.703->354.647)! Learning rate decreased to 0.01338.
2024-12-02-07:57:37-root-INFO: grad norm: 26.556 26.277 3.838
2024-12-02-07:57:38-root-INFO: grad norm: 21.673 21.442 3.155
2024-12-02-07:57:38-root-INFO: grad norm: 20.612 20.373 3.129
2024-12-02-07:57:39-root-INFO: grad norm: 19.703 19.487 2.910
2024-12-02-07:57:39-root-INFO: grad norm: 19.328 19.100 2.954
2024-12-02-07:57:40-root-INFO: grad norm: 18.992 18.783 2.810
2024-12-02-07:57:40-root-INFO: Loss Change: 353.425 -> 345.963
2024-12-02-07:57:40-root-INFO: Regularization Change: 0.000 -> 0.690
2024-12-02-07:57:40-root-INFO: Undo step: 135
2024-12-02-07:57:40-root-INFO: Undo step: 136
2024-12-02-07:57:40-root-INFO: Undo step: 137
2024-12-02-07:57:40-root-INFO: Undo step: 138
2024-12-02-07:57:40-root-INFO: Undo step: 139
2024-12-02-07:57:40-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-07:57:40-root-INFO: grad norm: 199.468 194.456 44.434
2024-12-02-07:57:41-root-INFO: grad norm: 129.677 127.261 24.919
2024-12-02-07:57:41-root-INFO: grad norm: 142.067 140.479 21.183
2024-12-02-07:57:42-root-INFO: grad norm: 279.093 275.473 44.807
2024-12-02-07:57:42-root-INFO: Loss too large (598.404->1126.125)! Learning rate decreased to 0.01761.
2024-12-02-07:57:42-root-INFO: Loss too large (598.404->857.559)! Learning rate decreased to 0.01409.
2024-12-02-07:57:42-root-INFO: Loss too large (598.404->690.074)! Learning rate decreased to 0.01127.
2024-12-02-07:57:43-root-INFO: grad norm: 131.593 130.031 20.213
2024-12-02-07:57:43-root-INFO: grad norm: 66.559 65.609 11.206
2024-12-02-07:57:44-root-INFO: grad norm: 46.486 45.498 9.534
2024-12-02-07:57:44-root-INFO: grad norm: 31.559 30.577 7.809
2024-12-02-07:57:44-root-INFO: Loss Change: 922.379 -> 425.278
2024-12-02-07:57:44-root-INFO: Regularization Change: 0.000 -> 34.873
2024-12-02-07:57:44-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-07:57:44-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:57:45-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-07:57:45-root-INFO: grad norm: 35.658 34.734 8.067
2024-12-02-07:57:45-root-INFO: Loss too large (424.540->431.924)! Learning rate decreased to 0.01823.
2024-12-02-07:57:45-root-INFO: grad norm: 73.707 73.025 10.006
2024-12-02-07:57:46-root-INFO: Loss too large (424.286->433.324)! Learning rate decreased to 0.01458.
2024-12-02-07:57:46-root-INFO: Loss too large (424.286->427.588)! Learning rate decreased to 0.01167.
2024-12-02-07:57:46-root-INFO: grad norm: 38.626 38.031 6.748
2024-12-02-07:57:47-root-INFO: grad norm: 29.291 28.652 6.083
2024-12-02-07:57:47-root-INFO: grad norm: 25.440 24.838 5.504
2024-12-02-07:57:48-root-INFO: grad norm: 26.964 26.361 5.672
2024-12-02-07:57:48-root-INFO: grad norm: 31.146 30.665 5.453
2024-12-02-07:57:49-root-INFO: grad norm: 31.240 30.723 5.659
2024-12-02-07:57:49-root-INFO: Loss Change: 424.540 -> 392.225
2024-12-02-07:57:49-root-INFO: Regularization Change: 0.000 -> 3.207
2024-12-02-07:57:49-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-07:57:49-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:57:49-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-07:57:49-root-INFO: grad norm: 38.738 38.315 5.708
2024-12-02-07:57:49-root-INFO: Loss too large (392.785->404.364)! Learning rate decreased to 0.01887.
2024-12-02-07:57:49-root-INFO: Loss too large (392.785->398.886)! Learning rate decreased to 0.01509.
2024-12-02-07:57:50-root-INFO: Loss too large (392.785->394.838)! Learning rate decreased to 0.01207.
2024-12-02-07:57:50-root-INFO: grad norm: 33.389 32.928 5.532
2024-12-02-07:57:51-root-INFO: grad norm: 26.160 25.764 4.535
2024-12-02-07:57:51-root-INFO: grad norm: 27.171 26.729 4.881
2024-12-02-07:57:51-root-INFO: grad norm: 30.112 29.745 4.687
2024-12-02-07:57:52-root-INFO: grad norm: 29.597 29.193 4.878
2024-12-02-07:57:52-root-INFO: grad norm: 28.510 28.166 4.416
2024-12-02-07:57:53-root-INFO: grad norm: 28.549 28.166 4.663
2024-12-02-07:57:53-root-INFO: Loss Change: 392.785 -> 375.774
2024-12-02-07:57:53-root-INFO: Regularization Change: 0.000 -> 1.509
2024-12-02-07:57:53-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-07:57:53-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:57:53-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-07:57:54-root-INFO: grad norm: 44.599 44.043 7.018
2024-12-02-07:57:54-root-INFO: Loss too large (378.547->395.802)! Learning rate decreased to 0.01952.
2024-12-02-07:57:54-root-INFO: Loss too large (378.547->386.939)! Learning rate decreased to 0.01562.
2024-12-02-07:57:54-root-INFO: Loss too large (378.547->380.853)! Learning rate decreased to 0.01250.
2024-12-02-07:57:54-root-INFO: grad norm: 35.021 34.633 5.200
2024-12-02-07:57:55-root-INFO: grad norm: 25.910 25.597 4.017
2024-12-02-07:57:55-root-INFO: grad norm: 26.844 26.488 4.357
2024-12-02-07:57:56-root-INFO: grad norm: 29.835 29.538 4.196
2024-12-02-07:57:56-root-INFO: Loss too large (368.184->368.582)! Learning rate decreased to 0.01000.
2024-12-02-07:57:56-root-INFO: grad norm: 24.201 23.865 4.019
2024-12-02-07:57:57-root-INFO: grad norm: 17.884 17.581 3.279
2024-12-02-07:57:57-root-INFO: grad norm: 16.454 16.111 3.342
2024-12-02-07:57:58-root-INFO: Loss Change: 378.547 -> 362.616
2024-12-02-07:57:58-root-INFO: Regularization Change: 0.000 -> 1.022
2024-12-02-07:57:58-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-07:57:58-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-07:57:58-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-07:57:58-root-INFO: grad norm: 20.187 19.860 3.621
2024-12-02-07:57:58-root-INFO: Loss too large (363.045->366.177)! Learning rate decreased to 0.02020.
2024-12-02-07:57:58-root-INFO: Loss too large (363.045->363.913)! Learning rate decreased to 0.01616.
2024-12-02-07:57:59-root-INFO: grad norm: 25.864 25.561 3.947
2024-12-02-07:57:59-root-INFO: Loss too large (362.540->363.585)! Learning rate decreased to 0.01293.
2024-12-02-07:57:59-root-INFO: grad norm: 30.166 29.893 4.047
2024-12-02-07:58:00-root-INFO: Loss too large (361.092->361.786)! Learning rate decreased to 0.01034.
2024-12-02-07:58:00-root-INFO: grad norm: 24.226 23.935 3.739
2024-12-02-07:58:00-root-INFO: grad norm: 17.558 17.291 3.048
2024-12-02-07:58:01-root-INFO: grad norm: 16.097 15.803 3.061
2024-12-02-07:58:01-root-INFO: grad norm: 14.609 14.334 2.819
2024-12-02-07:58:02-root-INFO: grad norm: 13.861 13.565 2.853
2024-12-02-07:58:02-root-INFO: Loss Change: 363.045 -> 354.201
2024-12-02-07:58:02-root-INFO: Regularization Change: 0.000 -> 0.745
2024-12-02-07:58:02-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-07:58:02-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-07:58:02-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-07:58:03-root-INFO: grad norm: 22.354 21.981 4.066
2024-12-02-07:58:03-root-INFO: Loss too large (354.918->358.911)! Learning rate decreased to 0.02090.
2024-12-02-07:58:03-root-INFO: Loss too large (354.918->355.812)! Learning rate decreased to 0.01672.
2024-12-02-07:58:03-root-INFO: grad norm: 26.360 26.071 3.892
2024-12-02-07:58:04-root-INFO: Loss too large (354.035->355.581)! Learning rate decreased to 0.01338.
2024-12-02-07:58:04-root-INFO: grad norm: 31.114 30.863 3.945
2024-12-02-07:58:04-root-INFO: Loss too large (352.842->353.879)! Learning rate decreased to 0.01070.
2024-12-02-07:58:05-root-INFO: grad norm: 24.874 24.608 3.631
2024-12-02-07:58:05-root-INFO: grad norm: 17.956 17.723 2.882
2024-12-02-07:58:06-root-INFO: grad norm: 16.537 16.277 2.920
2024-12-02-07:58:06-root-INFO: grad norm: 15.037 14.799 2.662
2024-12-02-07:58:07-root-INFO: grad norm: 14.294 14.034 2.717
2024-12-02-07:58:07-root-INFO: Loss Change: 354.918 -> 346.366
2024-12-02-07:58:07-root-INFO: Regularization Change: 0.000 -> 0.700
2024-12-02-07:58:07-root-INFO: Undo step: 135
2024-12-02-07:58:07-root-INFO: Undo step: 136
2024-12-02-07:58:07-root-INFO: Undo step: 137
2024-12-02-07:58:07-root-INFO: Undo step: 138
2024-12-02-07:58:07-root-INFO: Undo step: 139
2024-12-02-07:58:07-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-07:58:07-root-INFO: grad norm: 209.662 205.207 42.992
2024-12-02-07:58:08-root-INFO: grad norm: 118.590 116.307 23.158
2024-12-02-07:58:08-root-INFO: grad norm: 115.889 114.220 19.599
2024-12-02-07:58:09-root-INFO: Loss too large (591.919->593.817)! Learning rate decreased to 0.01761.
2024-12-02-07:58:09-root-INFO: grad norm: 128.349 126.618 21.003
2024-12-02-07:58:09-root-INFO: Loss too large (533.444->562.053)! Learning rate decreased to 0.01409.
2024-12-02-07:58:10-root-INFO: grad norm: 99.200 97.850 16.315
2024-12-02-07:58:10-root-INFO: grad norm: 56.158 55.171 10.479
2024-12-02-07:58:11-root-INFO: grad norm: 44.524 43.897 7.442
2024-12-02-07:58:11-root-INFO: grad norm: 36.455 35.540 8.116
2024-12-02-07:58:11-root-INFO: Loss Change: 969.589 -> 424.492
2024-12-02-07:58:11-root-INFO: Regularization Change: 0.000 -> 39.548
2024-12-02-07:58:11-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-07:58:11-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:58:12-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-07:58:12-root-INFO: grad norm: 46.287 45.595 7.974
2024-12-02-07:58:12-root-INFO: Loss too large (427.311->434.535)! Learning rate decreased to 0.01823.
2024-12-02-07:58:12-root-INFO: grad norm: 57.878 57.052 9.739
2024-12-02-07:58:13-root-INFO: Loss too large (423.856->427.735)! Learning rate decreased to 0.01458.
2024-12-02-07:58:13-root-INFO: grad norm: 50.761 50.177 7.681
2024-12-02-07:58:13-root-INFO: grad norm: 44.300 43.601 7.841
2024-12-02-07:58:14-root-INFO: grad norm: 40.444 39.946 6.328
2024-12-02-07:58:14-root-INFO: grad norm: 37.825 37.220 6.735
2024-12-02-07:58:15-root-INFO: grad norm: 36.284 35.823 5.766
2024-12-02-07:58:15-root-INFO: grad norm: 35.831 35.289 6.208
2024-12-02-07:58:16-root-INFO: Loss Change: 427.311 -> 391.805
2024-12-02-07:58:16-root-INFO: Regularization Change: 0.000 -> 3.925
2024-12-02-07:58:16-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-07:58:16-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:58:16-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-07:58:16-root-INFO: grad norm: 45.104 44.461 7.591
2024-12-02-07:58:16-root-INFO: Loss too large (393.963->411.726)! Learning rate decreased to 0.01887.
2024-12-02-07:58:16-root-INFO: Loss too large (393.963->397.827)! Learning rate decreased to 0.01509.
2024-12-02-07:58:17-root-INFO: grad norm: 42.989 42.446 6.810
2024-12-02-07:58:17-root-INFO: grad norm: 41.112 40.629 6.286
2024-12-02-07:58:18-root-INFO: grad norm: 39.897 39.392 6.327
2024-12-02-07:58:18-root-INFO: grad norm: 38.568 38.121 5.853
2024-12-02-07:58:19-root-INFO: grad norm: 37.665 37.198 5.913
2024-12-02-07:58:19-root-INFO: grad norm: 36.779 36.355 5.564
2024-12-02-07:58:20-root-INFO: grad norm: 36.116 35.677 5.616
2024-12-02-07:58:20-root-INFO: Loss Change: 393.963 -> 374.895
2024-12-02-07:58:20-root-INFO: Regularization Change: 0.000 -> 2.024
2024-12-02-07:58:20-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-07:58:20-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-07:58:20-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-07:58:20-root-INFO: grad norm: 55.121 54.198 10.042
2024-12-02-07:58:21-root-INFO: Loss too large (380.313->410.391)! Learning rate decreased to 0.01952.
2024-12-02-07:58:21-root-INFO: Loss too large (380.313->389.233)! Learning rate decreased to 0.01562.
2024-12-02-07:58:21-root-INFO: grad norm: 52.288 51.734 7.591
2024-12-02-07:58:22-root-INFO: grad norm: 48.199 47.666 7.145
2024-12-02-07:58:22-root-INFO: grad norm: 44.828 44.315 6.764
2024-12-02-07:58:23-root-INFO: grad norm: 41.936 41.483 6.141
2024-12-02-07:58:23-root-INFO: grad norm: 39.842 39.380 6.049
2024-12-02-07:58:24-root-INFO: grad norm: 38.053 37.644 5.564
2024-12-02-07:58:24-root-INFO: grad norm: 36.717 36.293 5.562
2024-12-02-07:58:24-root-INFO: Loss Change: 380.313 -> 361.059
2024-12-02-07:58:24-root-INFO: Regularization Change: 0.000 -> 1.648
2024-12-02-07:58:24-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-07:58:24-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-07:58:25-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-07:58:25-root-INFO: grad norm: 41.809 41.253 6.793
2024-12-02-07:58:25-root-INFO: Loss too large (362.999->380.809)! Learning rate decreased to 0.02020.
2024-12-02-07:58:25-root-INFO: Loss too large (362.999->367.391)! Learning rate decreased to 0.01616.
2024-12-02-07:58:25-root-INFO: grad norm: 39.280 38.853 5.774
2024-12-02-07:58:26-root-INFO: grad norm: 37.623 37.215 5.529
2024-12-02-07:58:26-root-INFO: grad norm: 36.322 35.915 5.420
2024-12-02-07:58:27-root-INFO: grad norm: 35.097 34.722 5.116
2024-12-02-07:58:27-root-INFO: grad norm: 34.218 33.835 5.102
2024-12-02-07:58:28-root-INFO: grad norm: 33.360 33.006 4.849
2024-12-02-07:58:28-root-INFO: grad norm: 32.725 32.362 4.866
2024-12-02-07:58:29-root-INFO: Loss Change: 362.999 -> 349.966
2024-12-02-07:58:29-root-INFO: Regularization Change: 0.000 -> 1.310
2024-12-02-07:58:29-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-07:58:29-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-07:58:29-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-07:58:29-root-INFO: grad norm: 42.388 41.741 7.381
2024-12-02-07:58:29-root-INFO: Loss too large (352.765->372.096)! Learning rate decreased to 0.02090.
2024-12-02-07:58:29-root-INFO: Loss too large (352.765->357.894)! Learning rate decreased to 0.01672.
2024-12-02-07:58:30-root-INFO: grad norm: 40.065 39.657 5.701
2024-12-02-07:58:30-root-INFO: grad norm: 38.494 38.083 5.608
2024-12-02-07:58:31-root-INFO: grad norm: 37.137 36.735 5.446
2024-12-02-07:58:31-root-INFO: grad norm: 35.840 35.470 5.135
2024-12-02-07:58:32-root-INFO: grad norm: 34.867 34.486 5.141
2024-12-02-07:58:32-root-INFO: grad norm: 33.910 33.563 4.837
2024-12-02-07:58:33-root-INFO: grad norm: 33.191 32.829 4.894
2024-12-02-07:58:33-root-INFO: Loss Change: 352.765 -> 340.672
2024-12-02-07:58:33-root-INFO: Regularization Change: 0.000 -> 1.209
2024-12-02-07:58:33-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-07:58:33-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-07:58:33-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-07:58:33-root-INFO: grad norm: 44.384 43.763 7.394
2024-12-02-07:58:33-root-INFO: Loss too large (343.993->366.753)! Learning rate decreased to 0.02162.
2024-12-02-07:58:33-root-INFO: Loss too large (343.993->350.720)! Learning rate decreased to 0.01729.
2024-12-02-07:58:34-root-INFO: grad norm: 41.891 41.469 5.928
2024-12-02-07:58:34-root-INFO: grad norm: 39.386 38.978 5.653
2024-12-02-07:58:35-root-INFO: grad norm: 37.339 36.942 5.431
2024-12-02-07:58:35-root-INFO: grad norm: 35.496 35.138 5.026
2024-12-02-07:58:36-root-INFO: grad norm: 34.161 33.794 4.988
2024-12-02-07:58:36-root-INFO: grad norm: 32.929 32.601 4.636
2024-12-02-07:58:37-root-INFO: grad norm: 32.031 31.689 4.669
2024-12-02-07:58:37-root-INFO: Loss Change: 343.993 -> 331.663
2024-12-02-07:58:37-root-INFO: Regularization Change: 0.000 -> 1.143
2024-12-02-07:58:37-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-07:58:37-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-07:58:37-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-07:58:37-root-INFO: grad norm: 43.132 42.457 7.601
2024-12-02-07:58:38-root-INFO: Loss too large (335.273->357.484)! Learning rate decreased to 0.02236.
2024-12-02-07:58:38-root-INFO: Loss too large (335.273->341.725)! Learning rate decreased to 0.01789.
2024-12-02-07:58:38-root-INFO: grad norm: 40.830 40.426 5.728
2024-12-02-07:58:39-root-INFO: grad norm: 39.189 38.787 5.597
2024-12-02-07:58:39-root-INFO: grad norm: 37.698 37.300 5.468
2024-12-02-07:58:40-root-INFO: grad norm: 36.231 35.877 5.053
2024-12-02-07:58:40-root-INFO: grad norm: 35.132 34.757 5.123
2024-12-02-07:58:41-root-INFO: grad norm: 34.028 33.702 4.703
2024-12-02-07:58:41-root-INFO: grad norm: 33.219 32.865 4.837
2024-12-02-07:58:42-root-INFO: Loss Change: 335.273 -> 324.020
2024-12-02-07:58:42-root-INFO: Regularization Change: 0.000 -> 1.108
2024-12-02-07:58:42-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-07:58:42-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-07:58:42-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-07:58:42-root-INFO: grad norm: 39.925 39.408 6.407
2024-12-02-07:58:42-root-INFO: Loss too large (326.159->346.074)! Learning rate decreased to 0.02312.
2024-12-02-07:58:42-root-INFO: Loss too large (326.159->331.983)! Learning rate decreased to 0.01849.
2024-12-02-07:58:43-root-INFO: grad norm: 37.802 37.431 5.283
2024-12-02-07:58:43-root-INFO: grad norm: 35.876 35.527 4.991
2024-12-02-07:58:44-root-INFO: grad norm: 34.379 34.026 4.912
2024-12-02-07:58:44-root-INFO: grad norm: 32.999 32.690 4.506
2024-12-02-07:58:45-root-INFO: grad norm: 32.001 31.672 4.571
2024-12-02-07:58:45-root-INFO: grad norm: 31.056 30.770 4.199
2024-12-02-07:58:46-root-INFO: grad norm: 30.358 30.050 4.318
2024-12-02-07:58:46-root-INFO: Loss Change: 326.159 -> 315.866
2024-12-02-07:58:46-root-INFO: Regularization Change: 0.000 -> 1.057
2024-12-02-07:58:46-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-07:58:46-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-07:58:46-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-07:58:46-root-INFO: grad norm: 39.640 39.069 6.706
2024-12-02-07:58:46-root-INFO: Loss too large (318.684->338.898)! Learning rate decreased to 0.02390.
2024-12-02-07:58:47-root-INFO: Loss too large (318.684->324.648)! Learning rate decreased to 0.01912.
2024-12-02-07:58:47-root-INFO: grad norm: 37.755 37.392 5.227
2024-12-02-07:58:48-root-INFO: grad norm: 36.119 35.770 5.008
2024-12-02-07:58:48-root-INFO: grad norm: 34.609 34.250 4.969
2024-12-02-07:58:49-root-INFO: grad norm: 33.216 32.908 4.510
2024-12-02-07:58:49-root-INFO: grad norm: 32.121 31.784 4.638
2024-12-02-07:58:49-root-INFO: grad norm: 31.102 30.819 4.188
2024-12-02-07:58:50-root-INFO: grad norm: 30.330 30.013 4.371
2024-12-02-07:58:50-root-INFO: Loss Change: 318.684 -> 308.444
2024-12-02-07:58:50-root-INFO: Regularization Change: 0.000 -> 1.069
2024-12-02-07:58:50-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-07:58:50-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-07:58:50-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-07:58:51-root-INFO: grad norm: 35.369 34.899 5.747
2024-12-02-07:58:51-root-INFO: Loss too large (310.421->327.254)! Learning rate decreased to 0.02471.
2024-12-02-07:58:51-root-INFO: Loss too large (310.421->315.296)! Learning rate decreased to 0.01976.
2024-12-02-07:58:51-root-INFO: grad norm: 33.638 33.297 4.778
2024-12-02-07:58:52-root-INFO: grad norm: 32.190 31.879 4.464
2024-12-02-07:58:52-root-INFO: grad norm: 31.107 30.783 4.475
2024-12-02-07:58:53-root-INFO: grad norm: 30.096 29.821 4.065
2024-12-02-07:58:53-root-INFO: grad norm: 29.360 29.059 4.198
2024-12-02-07:58:54-root-INFO: grad norm: 28.656 28.401 3.819
2024-12-02-07:58:54-root-INFO: grad norm: 28.130 27.846 3.991
2024-12-02-07:58:54-root-INFO: Loss Change: 310.421 -> 301.667
2024-12-02-07:58:54-root-INFO: Regularization Change: 0.000 -> 1.018
2024-12-02-07:58:54-root-INFO: Undo step: 130
2024-12-02-07:58:54-root-INFO: Undo step: 131
2024-12-02-07:58:54-root-INFO: Undo step: 132
2024-12-02-07:58:54-root-INFO: Undo step: 133
2024-12-02-07:58:54-root-INFO: Undo step: 134
2024-12-02-07:58:54-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-07:58:55-root-INFO: grad norm: 160.210 157.357 30.099
2024-12-02-07:58:55-root-INFO: grad norm: 120.011 117.896 22.431
2024-12-02-07:58:56-root-INFO: grad norm: 153.662 151.383 26.363
2024-12-02-07:58:56-root-INFO: Loss too large (522.558->617.090)! Learning rate decreased to 0.02090.
2024-12-02-07:58:56-root-INFO: Loss too large (522.558->530.690)! Learning rate decreased to 0.01672.
2024-12-02-07:58:56-root-INFO: grad norm: 91.871 89.800 19.398
2024-12-02-07:58:57-root-INFO: grad norm: 32.502 31.466 8.143
2024-12-02-07:58:57-root-INFO: grad norm: 24.929 23.916 7.033
2024-12-02-07:58:58-root-INFO: grad norm: 21.736 20.829 6.216
2024-12-02-07:58:59-root-INFO: grad norm: 19.628 18.822 5.568
2024-12-02-07:58:59-root-INFO: Loss Change: 750.490 -> 364.609
2024-12-02-07:58:59-root-INFO: Regularization Change: 0.000 -> 33.132
2024-12-02-07:58:59-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-07:58:59-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-07:58:59-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-07:58:59-root-INFO: grad norm: 22.356 21.625 5.671
2024-12-02-07:59:00-root-INFO: grad norm: 25.412 24.744 5.786
2024-12-02-07:59:00-root-INFO: grad norm: 39.265 38.718 6.532
2024-12-02-07:59:01-root-INFO: Loss too large (355.149->364.602)! Learning rate decreased to 0.02162.
2024-12-02-07:59:01-root-INFO: Loss too large (355.149->355.474)! Learning rate decreased to 0.01729.
2024-12-02-07:59:01-root-INFO: grad norm: 32.753 32.226 5.848
2024-12-02-07:59:02-root-INFO: grad norm: 28.995 28.633 4.568
2024-12-02-07:59:02-root-INFO: grad norm: 27.348 26.944 4.683
2024-12-02-07:59:03-root-INFO: grad norm: 27.368 27.074 4.007
2024-12-02-07:59:03-root-INFO: grad norm: 27.314 26.967 4.342
2024-12-02-07:59:04-root-INFO: Loss Change: 363.664 -> 336.371
2024-12-02-07:59:04-root-INFO: Regularization Change: 0.000 -> 4.467
2024-12-02-07:59:04-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-07:59:04-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-07:59:04-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-07:59:04-root-INFO: grad norm: 39.153 38.520 7.009
2024-12-02-07:59:04-root-INFO: Loss too large (338.820->353.226)! Learning rate decreased to 0.02236.
2024-12-02-07:59:04-root-INFO: Loss too large (338.820->343.410)! Learning rate decreased to 0.01789.
2024-12-02-07:59:05-root-INFO: grad norm: 35.993 35.613 5.216
2024-12-02-07:59:05-root-INFO: grad norm: 33.837 33.513 4.673
2024-12-02-07:59:06-root-INFO: grad norm: 32.139 31.800 4.659
2024-12-02-07:59:06-root-INFO: grad norm: 30.202 29.929 4.049
2024-12-02-07:59:07-root-INFO: grad norm: 29.454 29.148 4.235
2024-12-02-07:59:07-root-INFO: grad norm: 28.837 28.583 3.819
2024-12-02-07:59:07-root-INFO: grad norm: 28.499 28.212 4.031
2024-12-02-07:59:08-root-INFO: Loss Change: 338.820 -> 322.310
2024-12-02-07:59:08-root-INFO: Regularization Change: 0.000 -> 1.950
2024-12-02-07:59:08-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-07:59:08-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-07:59:08-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-07:59:08-root-INFO: grad norm: 35.234 34.780 5.636
2024-12-02-07:59:08-root-INFO: Loss too large (323.493->336.849)! Learning rate decreased to 0.02312.
2024-12-02-07:59:09-root-INFO: Loss too large (323.493->328.552)! Learning rate decreased to 0.01849.
2024-12-02-07:59:09-root-INFO: grad norm: 33.015 32.715 4.442
2024-12-02-07:59:09-root-INFO: grad norm: 30.720 30.437 4.164
2024-12-02-07:59:10-root-INFO: grad norm: 29.585 29.307 4.047
2024-12-02-07:59:10-root-INFO: grad norm: 28.376 28.122 3.786
2024-12-02-07:59:11-root-INFO: Loss too large (316.226->316.282)! Learning rate decreased to 0.01479.
2024-12-02-07:59:11-root-INFO: grad norm: 20.619 20.396 3.029
2024-12-02-07:59:12-root-INFO: grad norm: 14.304 14.097 2.427
2024-12-02-07:59:12-root-INFO: grad norm: 12.177 11.972 2.225
2024-12-02-07:59:12-root-INFO: Loss Change: 323.493 -> 309.722
2024-12-02-07:59:12-root-INFO: Regularization Change: 0.000 -> 1.281
2024-12-02-07:59:12-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-07:59:12-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-07:59:12-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-07:59:13-root-INFO: grad norm: 20.675 20.075 4.945
2024-12-02-07:59:13-root-INFO: Loss too large (310.543->312.735)! Learning rate decreased to 0.02390.
2024-12-02-07:59:13-root-INFO: grad norm: 26.187 25.855 4.160
2024-12-02-07:59:13-root-INFO: Loss too large (310.061->312.375)! Learning rate decreased to 0.01912.
2024-12-02-07:59:14-root-INFO: grad norm: 27.922 27.614 4.140
2024-12-02-07:59:14-root-INFO: Loss too large (308.690->308.941)! Learning rate decreased to 0.01530.
2024-12-02-07:59:15-root-INFO: grad norm: 20.863 20.633 3.089
2024-12-02-07:59:15-root-INFO: grad norm: 15.615 15.418 2.476
2024-12-02-07:59:15-root-INFO: grad norm: 13.473 13.288 2.227
2024-12-02-07:59:16-root-INFO: grad norm: 11.732 11.543 2.093
2024-12-02-07:59:16-root-INFO: grad norm: 10.775 10.593 1.975
2024-12-02-07:59:17-root-INFO: Loss Change: 310.543 -> 300.977
2024-12-02-07:59:17-root-INFO: Regularization Change: 0.000 -> 1.082
2024-12-02-07:59:17-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-07:59:17-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-07:59:17-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-07:59:17-root-INFO: grad norm: 16.721 16.284 3.797
2024-12-02-07:59:17-root-INFO: Loss too large (301.854->303.570)! Learning rate decreased to 0.02471.
2024-12-02-07:59:18-root-INFO: grad norm: 23.068 22.819 3.380
2024-12-02-07:59:18-root-INFO: Loss too large (301.702->304.576)! Learning rate decreased to 0.01976.
2024-12-02-07:59:18-root-INFO: grad norm: 28.131 27.863 3.877
2024-12-02-07:59:18-root-INFO: Loss too large (301.198->302.556)! Learning rate decreased to 0.01581.
2024-12-02-07:59:19-root-INFO: grad norm: 22.670 22.465 3.047
2024-12-02-07:59:19-root-INFO: grad norm: 17.334 17.138 2.595
2024-12-02-07:59:20-root-INFO: grad norm: 15.329 15.160 2.267
2024-12-02-07:59:20-root-INFO: grad norm: 13.463 13.280 2.211
2024-12-02-07:59:21-root-INFO: grad norm: 12.461 12.298 2.008
2024-12-02-07:59:21-root-INFO: Loss Change: 301.854 -> 294.319
2024-12-02-07:59:21-root-INFO: Regularization Change: 0.000 -> 0.951
2024-12-02-07:59:21-root-INFO: Undo step: 130
2024-12-02-07:59:21-root-INFO: Undo step: 131
2024-12-02-07:59:21-root-INFO: Undo step: 132
2024-12-02-07:59:21-root-INFO: Undo step: 133
2024-12-02-07:59:21-root-INFO: Undo step: 134
2024-12-02-07:59:21-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-07:59:21-root-INFO: grad norm: 155.845 151.954 34.609
2024-12-02-07:59:22-root-INFO: grad norm: 92.002 90.048 18.860
2024-12-02-07:59:22-root-INFO: grad norm: 90.160 88.117 19.084
2024-12-02-07:59:23-root-INFO: grad norm: 109.939 108.200 19.476
2024-12-02-07:59:23-root-INFO: Loss too large (461.185->493.780)! Learning rate decreased to 0.02090.
2024-12-02-07:59:23-root-INFO: grad norm: 88.708 86.748 18.547
2024-12-02-07:59:24-root-INFO: grad norm: 65.652 64.543 12.014
2024-12-02-07:59:24-root-INFO: grad norm: 58.970 57.659 12.363
2024-12-02-07:59:25-root-INFO: grad norm: 54.838 53.913 10.030
2024-12-02-07:59:25-root-INFO: Loss Change: 808.835 -> 372.507
2024-12-02-07:59:25-root-INFO: Regularization Change: 0.000 -> 41.819
2024-12-02-07:59:25-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-07:59:25-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-07:59:25-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-07:59:25-root-INFO: grad norm: 46.565 45.823 8.281
2024-12-02-07:59:26-root-INFO: Loss too large (366.658->374.542)! Learning rate decreased to 0.02162.
2024-12-02-07:59:26-root-INFO: grad norm: 45.853 45.088 8.340
2024-12-02-07:59:27-root-INFO: grad norm: 45.005 44.158 8.692
2024-12-02-07:59:27-root-INFO: grad norm: 46.449 45.708 8.264
2024-12-02-07:59:27-root-INFO: grad norm: 49.054 48.155 9.346
2024-12-02-07:59:28-root-INFO: grad norm: 48.819 48.056 8.598
2024-12-02-07:59:28-root-INFO: grad norm: 47.205 46.276 9.318
2024-12-02-07:59:29-root-INFO: grad norm: 49.012 48.223 8.762
2024-12-02-07:59:29-root-INFO: Loss Change: 366.658 -> 341.869
2024-12-02-07:59:29-root-INFO: Regularization Change: 0.000 -> 4.886
2024-12-02-07:59:29-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-07:59:29-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-07:59:29-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-07:59:30-root-INFO: grad norm: 45.651 45.081 7.193
2024-12-02-07:59:30-root-INFO: Loss too large (336.612->350.134)! Learning rate decreased to 0.02236.
2024-12-02-07:59:30-root-INFO: Loss too large (336.612->338.281)! Learning rate decreased to 0.01789.
2024-12-02-07:59:30-root-INFO: grad norm: 33.481 32.933 6.035
2024-12-02-07:59:31-root-INFO: grad norm: 23.969 23.539 4.524
2024-12-02-07:59:31-root-INFO: grad norm: 21.548 21.160 4.069
2024-12-02-07:59:32-root-INFO: grad norm: 21.864 21.495 3.997
2024-12-02-07:59:32-root-INFO: grad norm: 21.730 21.345 4.073
2024-12-02-07:59:33-root-INFO: grad norm: 22.996 22.643 4.013
2024-12-02-07:59:33-root-INFO: grad norm: 22.760 22.367 4.214
2024-12-02-07:59:34-root-INFO: Loss Change: 336.612 -> 314.648
2024-12-02-07:59:34-root-INFO: Regularization Change: 0.000 -> 2.091
2024-12-02-07:59:34-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-07:59:34-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-07:59:34-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-07:59:34-root-INFO: grad norm: 26.785 26.323 4.954
2024-12-02-07:59:34-root-INFO: Loss too large (314.480->320.959)! Learning rate decreased to 0.02312.
2024-12-02-07:59:34-root-INFO: Loss too large (314.480->316.938)! Learning rate decreased to 0.01849.
2024-12-02-07:59:35-root-INFO: grad norm: 25.588 25.171 4.605
2024-12-02-07:59:35-root-INFO: grad norm: 24.715 24.377 4.077
2024-12-02-07:59:35-root-INFO: Loss too large (311.077->311.247)! Learning rate decreased to 0.01479.
2024-12-02-07:59:36-root-INFO: grad norm: 19.320 18.957 3.729
2024-12-02-07:59:36-root-INFO: grad norm: 14.015 13.750 2.709
2024-12-02-07:59:37-root-INFO: grad norm: 12.797 12.490 2.786
2024-12-02-07:59:37-root-INFO: grad norm: 11.774 11.516 2.451
2024-12-02-07:59:38-root-INFO: grad norm: 11.317 11.027 2.546
2024-12-02-07:59:38-root-INFO: Loss Change: 314.480 -> 303.486
2024-12-02-07:59:38-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-02-07:59:38-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-07:59:38-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-07:59:38-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-07:59:39-root-INFO: grad norm: 20.445 19.760 5.249
2024-12-02-07:59:39-root-INFO: Loss too large (304.266->306.871)! Learning rate decreased to 0.02390.
2024-12-02-07:59:39-root-INFO: grad norm: 27.201 26.700 5.200
2024-12-02-07:59:39-root-INFO: Loss too large (304.141->307.856)! Learning rate decreased to 0.01912.
2024-12-02-07:59:40-root-INFO: grad norm: 32.883 32.424 5.475
2024-12-02-07:59:40-root-INFO: Loss too large (303.317->305.120)! Learning rate decreased to 0.01530.
2024-12-02-07:59:40-root-INFO: grad norm: 24.660 24.250 4.477
2024-12-02-07:59:41-root-INFO: grad norm: 15.774 15.514 2.853
2024-12-02-07:59:41-root-INFO: grad norm: 14.237 13.946 2.864
2024-12-02-07:59:42-root-INFO: grad norm: 13.170 12.933 2.487
2024-12-02-07:59:42-root-INFO: grad norm: 12.728 12.461 2.592
2024-12-02-07:59:43-root-INFO: Loss Change: 304.266 -> 295.077
2024-12-02-07:59:43-root-INFO: Regularization Change: 0.000 -> 1.070
2024-12-02-07:59:43-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-07:59:43-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-07:59:43-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-07:59:43-root-INFO: grad norm: 18.905 18.424 4.237
2024-12-02-07:59:43-root-INFO: Loss too large (295.764->299.514)! Learning rate decreased to 0.02471.
2024-12-02-07:59:43-root-INFO: Loss too large (295.764->296.864)! Learning rate decreased to 0.01976.
2024-12-02-07:59:44-root-INFO: grad norm: 21.088 20.761 3.699
2024-12-02-07:59:44-root-INFO: grad norm: 28.936 28.568 4.599
2024-12-02-07:59:44-root-INFO: Loss too large (295.017->297.164)! Learning rate decreased to 0.01581.
2024-12-02-07:59:45-root-INFO: grad norm: 22.977 22.645 3.894
2024-12-02-07:59:45-root-INFO: grad norm: 15.079 14.844 2.651
2024-12-02-07:59:46-root-INFO: grad norm: 14.149 13.898 2.652
2024-12-02-07:59:46-root-INFO: grad norm: 13.444 13.224 2.421
2024-12-02-07:59:47-root-INFO: grad norm: 13.192 12.955 2.485
2024-12-02-07:59:47-root-INFO: Loss Change: 295.764 -> 288.550
2024-12-02-07:59:47-root-INFO: Regularization Change: 0.000 -> 0.860
2024-12-02-07:59:47-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-07:59:47-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-07:59:47-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-07:59:47-root-INFO: grad norm: 22.692 22.008 5.529
2024-12-02-07:59:48-root-INFO: Loss too large (290.082->296.298)! Learning rate decreased to 0.02553.
2024-12-02-07:59:48-root-INFO: Loss too large (290.082->291.933)! Learning rate decreased to 0.02043.
2024-12-02-07:59:48-root-INFO: grad norm: 24.089 23.730 4.145
2024-12-02-07:59:49-root-INFO: grad norm: 31.359 30.951 5.047
2024-12-02-07:59:49-root-INFO: Loss too large (289.318->291.693)! Learning rate decreased to 0.01634.
2024-12-02-07:59:49-root-INFO: grad norm: 24.224 23.883 4.047
2024-12-02-07:59:50-root-INFO: grad norm: 15.919 15.682 2.733
2024-12-02-07:59:50-root-INFO: grad norm: 14.476 14.235 2.636
2024-12-02-07:59:51-root-INFO: grad norm: 13.457 13.250 2.351
2024-12-02-07:59:51-root-INFO: grad norm: 13.029 12.808 2.391
2024-12-02-07:59:51-root-INFO: Loss Change: 290.082 -> 282.365
2024-12-02-07:59:51-root-INFO: Regularization Change: 0.000 -> 0.843
2024-12-02-07:59:51-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-07:59:51-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-07:59:52-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-07:59:52-root-INFO: grad norm: 20.908 20.278 5.092
2024-12-02-07:59:52-root-INFO: Loss too large (283.204->288.894)! Learning rate decreased to 0.02639.
2024-12-02-07:59:52-root-INFO: Loss too large (283.204->284.933)! Learning rate decreased to 0.02111.
2024-12-02-07:59:52-root-INFO: grad norm: 22.736 22.409 3.844
2024-12-02-07:59:53-root-INFO: Loss too large (282.654->282.832)! Learning rate decreased to 0.01689.
2024-12-02-07:59:53-root-INFO: grad norm: 19.211 18.939 3.220
2024-12-02-07:59:54-root-INFO: grad norm: 17.440 17.193 2.923
2024-12-02-07:59:54-root-INFO: grad norm: 16.019 15.806 2.604
2024-12-02-07:59:55-root-INFO: grad norm: 15.340 15.121 2.585
2024-12-02-07:59:55-root-INFO: grad norm: 14.679 14.484 2.385
2024-12-02-07:59:55-root-INFO: grad norm: 14.370 14.164 2.425
2024-12-02-07:59:56-root-INFO: Loss Change: 283.204 -> 276.626
2024-12-02-07:59:56-root-INFO: Regularization Change: 0.000 -> 0.746
2024-12-02-07:59:56-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-07:59:56-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-07:59:56-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-07:59:56-root-INFO: grad norm: 19.808 19.397 4.018
2024-12-02-07:59:56-root-INFO: Loss too large (277.442->283.557)! Learning rate decreased to 0.02726.
2024-12-02-07:59:56-root-INFO: Loss too large (277.442->280.001)! Learning rate decreased to 0.02181.
2024-12-02-07:59:57-root-INFO: Loss too large (277.442->277.753)! Learning rate decreased to 0.01745.
2024-12-02-07:59:57-root-INFO: grad norm: 17.234 17.016 2.729
2024-12-02-07:59:58-root-INFO: grad norm: 15.847 15.633 2.594
2024-12-02-07:59:58-root-INFO: grad norm: 15.079 14.876 2.466
2024-12-02-07:59:59-root-INFO: grad norm: 14.316 14.131 2.295
2024-12-02-07:59:59-root-INFO: grad norm: 13.927 13.735 2.303
2024-12-02-07:59:59-root-INFO: grad norm: 13.521 13.346 2.167
2024-12-02-08:00:00-root-INFO: grad norm: 13.319 13.135 2.210
2024-12-02-08:00:00-root-INFO: Loss Change: 277.442 -> 271.706
2024-12-02-08:00:00-root-INFO: Regularization Change: 0.000 -> 0.662
2024-12-02-08:00:00-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-08:00:00-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-08:00:00-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-08:00:00-root-INFO: grad norm: 16.914 16.505 3.700
2024-12-02-08:00:01-root-INFO: Loss too large (272.476->276.805)! Learning rate decreased to 0.02816.
2024-12-02-08:00:01-root-INFO: Loss too large (272.476->274.025)! Learning rate decreased to 0.02253.
2024-12-02-08:00:01-root-INFO: grad norm: 19.840 19.600 3.077
2024-12-02-08:00:01-root-INFO: Loss too large (272.364->272.698)! Learning rate decreased to 0.01802.
2024-12-02-08:00:02-root-INFO: grad norm: 17.758 17.543 2.756
2024-12-02-08:00:02-root-INFO: grad norm: 16.674 16.473 2.584
2024-12-02-08:00:03-root-INFO: grad norm: 15.612 15.430 2.377
2024-12-02-08:00:03-root-INFO: grad norm: 15.094 14.908 2.358
2024-12-02-08:00:04-root-INFO: grad norm: 14.535 14.365 2.214
2024-12-02-08:00:04-root-INFO: grad norm: 14.261 14.085 2.238
2024-12-02-08:00:04-root-INFO: Loss Change: 272.476 -> 267.377
2024-12-02-08:00:05-root-INFO: Regularization Change: 0.000 -> 0.680
2024-12-02-08:00:05-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-08:00:05-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-08:00:05-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-08:00:05-root-INFO: grad norm: 20.167 19.662 4.482
2024-12-02-08:00:05-root-INFO: Loss too large (268.310->275.193)! Learning rate decreased to 0.02909.
2024-12-02-08:00:05-root-INFO: Loss too large (268.310->270.935)! Learning rate decreased to 0.02327.
2024-12-02-08:00:05-root-INFO: Loss too large (268.310->268.375)! Learning rate decreased to 0.01862.
2024-12-02-08:00:06-root-INFO: grad norm: 16.848 16.649 2.581
2024-12-02-08:00:06-root-INFO: grad norm: 15.881 15.681 2.516
2024-12-02-08:00:07-root-INFO: grad norm: 15.302 15.119 2.359
2024-12-02-08:00:07-root-INFO: grad norm: 14.897 14.730 2.227
2024-12-02-08:00:08-root-INFO: grad norm: 14.637 14.465 2.242
2024-12-02-08:00:08-root-INFO: grad norm: 14.397 14.240 2.122
2024-12-02-08:00:09-root-INFO: grad norm: 14.255 14.089 2.171
2024-12-02-08:00:09-root-INFO: Loss Change: 268.310 -> 262.752
2024-12-02-08:00:09-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-08:00:09-root-INFO: Undo step: 125
2024-12-02-08:00:09-root-INFO: Undo step: 126
2024-12-02-08:00:09-root-INFO: Undo step: 127
2024-12-02-08:00:09-root-INFO: Undo step: 128
2024-12-02-08:00:09-root-INFO: Undo step: 129
2024-12-02-08:00:09-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-08:00:09-root-INFO: grad norm: 151.276 148.185 30.421
2024-12-02-08:00:10-root-INFO: grad norm: 117.534 115.885 19.619
2024-12-02-08:00:10-root-INFO: grad norm: 99.360 97.077 21.176
2024-12-02-08:00:11-root-INFO: grad norm: 85.675 84.419 14.613
2024-12-02-08:00:11-root-INFO: grad norm: 80.039 78.554 15.349
2024-12-02-08:00:11-root-INFO: grad norm: 77.891 76.742 13.332
2024-12-02-08:00:12-root-INFO: grad norm: 86.696 85.174 16.169
2024-12-02-08:00:12-root-INFO: Loss too large (370.951->377.261)! Learning rate decreased to 0.02471.
2024-12-02-08:00:13-root-INFO: grad norm: 51.936 51.165 8.919
2024-12-02-08:00:13-root-INFO: Loss Change: 696.394 -> 322.380
2024-12-02-08:00:13-root-INFO: Regularization Change: 0.000 -> 47.086
2024-12-02-08:00:13-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-08:00:13-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-08:00:13-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-08:00:13-root-INFO: grad norm: 28.484 27.981 5.332
2024-12-02-08:00:14-root-INFO: grad norm: 32.055 31.518 5.843
2024-12-02-08:00:14-root-INFO: Loss too large (311.983->313.815)! Learning rate decreased to 0.02553.
2024-12-02-08:00:14-root-INFO: grad norm: 30.247 29.660 5.930
2024-12-02-08:00:15-root-INFO: grad norm: 33.856 33.392 5.584
2024-12-02-08:00:15-root-INFO: Loss too large (304.876->306.043)! Learning rate decreased to 0.02043.
2024-12-02-08:00:15-root-INFO: grad norm: 25.800 25.351 4.792
2024-12-02-08:00:16-root-INFO: grad norm: 18.581 18.255 3.469
2024-12-02-08:00:16-root-INFO: grad norm: 17.845 17.475 3.618
2024-12-02-08:00:17-root-INFO: grad norm: 18.793 18.495 3.337
2024-12-02-08:00:17-root-INFO: Loss Change: 318.116 -> 292.268
2024-12-02-08:00:17-root-INFO: Regularization Change: 0.000 -> 3.928
2024-12-02-08:00:17-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-08:00:17-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-08:00:17-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-08:00:17-root-INFO: grad norm: 22.491 21.985 4.744
2024-12-02-08:00:17-root-INFO: Loss too large (292.289->296.310)! Learning rate decreased to 0.02639.
2024-12-02-08:00:18-root-INFO: grad norm: 33.560 33.215 4.802
2024-12-02-08:00:18-root-INFO: Loss too large (291.632->295.784)! Learning rate decreased to 0.02111.
2024-12-02-08:00:18-root-INFO: Loss too large (291.632->292.028)! Learning rate decreased to 0.01689.
2024-12-02-08:00:19-root-INFO: grad norm: 21.598 21.277 3.709
2024-12-02-08:00:19-root-INFO: grad norm: 10.648 10.347 2.511
2024-12-02-08:00:20-root-INFO: grad norm: 9.552 9.217 2.508
2024-12-02-08:00:20-root-INFO: grad norm: 8.866 8.552 2.338
2024-12-02-08:00:21-root-INFO: grad norm: 8.470 8.138 2.347
2024-12-02-08:00:21-root-INFO: grad norm: 8.153 7.843 2.227
2024-12-02-08:00:21-root-INFO: Loss Change: 292.289 -> 280.120
2024-12-02-08:00:21-root-INFO: Regularization Change: 0.000 -> 1.389
2024-12-02-08:00:21-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-08:00:21-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-08:00:22-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-08:00:22-root-INFO: grad norm: 11.833 11.353 3.336
2024-12-02-08:00:22-root-INFO: grad norm: 15.431 15.110 3.132
2024-12-02-08:00:22-root-INFO: Loss too large (278.890->279.878)! Learning rate decreased to 0.02726.
2024-12-02-08:00:23-root-INFO: grad norm: 19.039 18.718 3.480
2024-12-02-08:00:23-root-INFO: Loss too large (278.264->278.958)! Learning rate decreased to 0.02181.
2024-12-02-08:00:23-root-INFO: grad norm: 19.947 19.691 3.185
2024-12-02-08:00:24-root-INFO: grad norm: 20.726 20.462 3.299
2024-12-02-08:00:24-root-INFO: grad norm: 23.690 23.451 3.353
2024-12-02-08:00:25-root-INFO: Loss too large (275.630->276.544)! Learning rate decreased to 0.01745.
2024-12-02-08:00:25-root-INFO: grad norm: 18.708 18.469 2.983
2024-12-02-08:00:25-root-INFO: grad norm: 13.385 13.183 2.313
2024-12-02-08:00:26-root-INFO: Loss Change: 280.308 -> 271.905
2024-12-02-08:00:26-root-INFO: Regularization Change: 0.000 -> 1.515
2024-12-02-08:00:26-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-08:00:26-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-08:00:26-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-08:00:26-root-INFO: grad norm: 15.199 14.837 3.297
2024-12-02-08:00:26-root-INFO: Loss too large (272.270->274.702)! Learning rate decreased to 0.02816.
2024-12-02-08:00:26-root-INFO: Loss too large (272.270->272.399)! Learning rate decreased to 0.02253.
2024-12-02-08:00:27-root-INFO: grad norm: 17.201 16.995 2.657
2024-12-02-08:00:27-root-INFO: Loss too large (271.220->271.333)! Learning rate decreased to 0.01802.
2024-12-02-08:00:27-root-INFO: grad norm: 14.696 14.472 2.556
2024-12-02-08:00:28-root-INFO: grad norm: 12.511 12.321 2.172
2024-12-02-08:00:28-root-INFO: grad norm: 11.655 11.445 2.202
2024-12-02-08:00:29-root-INFO: grad norm: 10.793 10.605 2.004
2024-12-02-08:00:29-root-INFO: grad norm: 10.337 10.132 2.050
2024-12-02-08:00:30-root-INFO: grad norm: 9.884 9.699 1.907
2024-12-02-08:00:30-root-INFO: Loss Change: 272.270 -> 265.803
2024-12-02-08:00:30-root-INFO: Regularization Change: 0.000 -> 0.842
2024-12-02-08:00:30-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-08:00:30-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-08:00:30-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-08:00:30-root-INFO: grad norm: 15.062 14.636 3.559
2024-12-02-08:00:30-root-INFO: Loss too large (266.211->268.137)! Learning rate decreased to 0.02909.
2024-12-02-08:00:31-root-INFO: grad norm: 22.362 22.144 3.113
2024-12-02-08:00:31-root-INFO: Loss too large (266.089->269.429)! Learning rate decreased to 0.02327.
2024-12-02-08:00:31-root-INFO: Loss too large (266.089->266.590)! Learning rate decreased to 0.01862.
2024-12-02-08:00:32-root-INFO: grad norm: 17.491 17.276 2.733
2024-12-02-08:00:32-root-INFO: grad norm: 14.069 13.901 2.169
2024-12-02-08:00:33-root-INFO: grad norm: 12.972 12.782 2.210
2024-12-02-08:00:33-root-INFO: grad norm: 11.890 11.726 1.972
2024-12-02-08:00:34-root-INFO: grad norm: 11.356 11.173 2.031
2024-12-02-08:00:34-root-INFO: grad norm: 10.797 10.634 1.866
2024-12-02-08:00:34-root-INFO: Loss Change: 266.211 -> 260.121
2024-12-02-08:00:34-root-INFO: Regularization Change: 0.000 -> 0.858
2024-12-02-08:00:34-root-INFO: Undo step: 125
2024-12-02-08:00:34-root-INFO: Undo step: 126
2024-12-02-08:00:34-root-INFO: Undo step: 127
2024-12-02-08:00:34-root-INFO: Undo step: 128
2024-12-02-08:00:34-root-INFO: Undo step: 129
2024-12-02-08:00:35-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-08:00:35-root-INFO: grad norm: 135.939 132.736 29.337
2024-12-02-08:00:35-root-INFO: grad norm: 89.656 88.337 15.322
2024-12-02-08:00:36-root-INFO: grad norm: 77.188 75.033 18.113
2024-12-02-08:00:36-root-INFO: grad norm: 71.332 70.084 13.282
2024-12-02-08:00:37-root-INFO: grad norm: 80.395 78.600 16.891
2024-12-02-08:00:37-root-INFO: grad norm: 70.985 69.700 13.443
2024-12-02-08:00:38-root-INFO: grad norm: 70.162 68.122 16.796
2024-12-02-08:00:38-root-INFO: grad norm: 70.433 69.016 14.057
2024-12-02-08:00:38-root-INFO: Loss too large (352.926->356.583)! Learning rate decreased to 0.02471.
2024-12-02-08:00:39-root-INFO: Loss Change: 697.858 -> 332.366
2024-12-02-08:00:39-root-INFO: Regularization Change: 0.000 -> 45.812
2024-12-02-08:00:39-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-08:00:39-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-08:00:39-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-08:00:39-root-INFO: grad norm: 43.929 43.133 8.328
2024-12-02-08:00:39-root-INFO: Loss too large (325.235->325.268)! Learning rate decreased to 0.02553.
2024-12-02-08:00:40-root-INFO: grad norm: 36.211 35.658 6.304
2024-12-02-08:00:40-root-INFO: grad norm: 35.141 34.489 6.738
2024-12-02-08:00:41-root-INFO: grad norm: 34.200 33.781 5.340
2024-12-02-08:00:41-root-INFO: grad norm: 36.273 35.710 6.367
2024-12-02-08:00:41-root-INFO: Loss too large (304.236->304.522)! Learning rate decreased to 0.02043.
2024-12-02-08:00:42-root-INFO: grad norm: 25.532 25.250 3.781
2024-12-02-08:00:42-root-INFO: grad norm: 16.975 16.590 3.594
2024-12-02-08:00:43-root-INFO: grad norm: 15.716 15.473 2.754
2024-12-02-08:00:43-root-INFO: Loss Change: 325.235 -> 290.920
2024-12-02-08:00:43-root-INFO: Regularization Change: 0.000 -> 4.149
2024-12-02-08:00:43-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-08:00:43-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-08:00:43-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-08:00:43-root-INFO: grad norm: 18.127 17.736 3.747
2024-12-02-08:00:43-root-INFO: Loss too large (290.306->290.980)! Learning rate decreased to 0.02639.
2024-12-02-08:00:44-root-INFO: grad norm: 21.180 20.912 3.356
2024-12-02-08:00:44-root-INFO: grad norm: 32.493 32.261 3.877
2024-12-02-08:00:44-root-INFO: Loss too large (288.934->292.897)! Learning rate decreased to 0.02111.
2024-12-02-08:00:45-root-INFO: Loss too large (288.934->289.314)! Learning rate decreased to 0.01689.
2024-12-02-08:00:45-root-INFO: grad norm: 20.773 20.550 3.038
2024-12-02-08:00:46-root-INFO: grad norm: 10.028 9.777 2.229
2024-12-02-08:00:46-root-INFO: grad norm: 9.040 8.776 2.166
2024-12-02-08:00:47-root-INFO: grad norm: 8.477 8.213 2.097
2024-12-02-08:00:47-root-INFO: grad norm: 8.141 7.880 2.048
2024-12-02-08:00:47-root-INFO: Loss Change: 290.306 -> 278.714
2024-12-02-08:00:47-root-INFO: Regularization Change: 0.000 -> 1.608
2024-12-02-08:00:47-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-08:00:47-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-08:00:47-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-08:00:48-root-INFO: grad norm: 12.475 12.121 2.953
2024-12-02-08:00:48-root-INFO: grad norm: 18.212 17.919 3.254
2024-12-02-08:00:48-root-INFO: Loss too large (277.897->281.590)! Learning rate decreased to 0.02726.
2024-12-02-08:00:48-root-INFO: Loss too large (277.897->278.269)! Learning rate decreased to 0.02181.
2024-12-02-08:00:49-root-INFO: grad norm: 19.240 19.042 2.757
2024-12-02-08:00:49-root-INFO: grad norm: 19.965 19.751 2.916
2024-12-02-08:00:50-root-INFO: grad norm: 21.890 21.709 2.813
2024-12-02-08:00:50-root-INFO: Loss too large (274.860->275.161)! Learning rate decreased to 0.01745.
2024-12-02-08:00:51-root-INFO: grad norm: 16.886 16.708 2.445
2024-12-02-08:00:51-root-INFO: grad norm: 12.030 11.851 2.069
2024-12-02-08:00:52-root-INFO: grad norm: 10.687 10.508 1.951
2024-12-02-08:00:52-root-INFO: Loss Change: 278.876 -> 270.117
2024-12-02-08:00:52-root-INFO: Regularization Change: 0.000 -> 1.420
2024-12-02-08:00:52-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-08:00:52-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-08:00:52-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-08:00:52-root-INFO: grad norm: 12.591 12.271 2.822
2024-12-02-08:00:53-root-INFO: grad norm: 20.325 20.083 3.130
2024-12-02-08:00:53-root-INFO: Loss too large (270.287->276.779)! Learning rate decreased to 0.02816.
2024-12-02-08:00:53-root-INFO: Loss too large (270.287->271.859)! Learning rate decreased to 0.02253.
2024-12-02-08:00:53-root-INFO: grad norm: 22.346 22.160 2.874
2024-12-02-08:00:54-root-INFO: Loss too large (269.251->269.638)! Learning rate decreased to 0.01802.
2024-12-02-08:00:54-root-INFO: grad norm: 17.162 16.997 2.380
2024-12-02-08:00:54-root-INFO: grad norm: 12.835 12.677 2.007
2024-12-02-08:00:55-root-INFO: grad norm: 11.418 11.265 1.859
2024-12-02-08:00:56-root-INFO: grad norm: 10.136 9.972 1.814
2024-12-02-08:00:56-root-INFO: grad norm: 9.478 9.322 1.709
2024-12-02-08:00:56-root-INFO: Loss Change: 270.371 -> 263.317
2024-12-02-08:00:56-root-INFO: Regularization Change: 0.000 -> 1.092
2024-12-02-08:00:56-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-08:00:56-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-08:00:57-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-08:00:57-root-INFO: grad norm: 14.336 13.953 3.294
2024-12-02-08:00:57-root-INFO: Loss too large (263.610->264.509)! Learning rate decreased to 0.02909.
2024-12-02-08:00:57-root-INFO: grad norm: 18.228 18.038 2.628
2024-12-02-08:00:57-root-INFO: Loss too large (263.059->264.659)! Learning rate decreased to 0.02327.
2024-12-02-08:00:58-root-INFO: grad norm: 20.977 20.800 2.721
2024-12-02-08:00:58-root-INFO: Loss too large (262.427->262.941)! Learning rate decreased to 0.01862.
2024-12-02-08:00:59-root-INFO: grad norm: 16.735 16.587 2.218
2024-12-02-08:00:59-root-INFO: grad norm: 13.335 13.191 1.950
2024-12-02-08:00:59-root-INFO: grad norm: 12.013 11.878 1.793
2024-12-02-08:01:00-root-INFO: grad norm: 10.778 10.635 1.753
2024-12-02-08:01:00-root-INFO: grad norm: 10.134 10.000 1.646
2024-12-02-08:01:01-root-INFO: Loss Change: 263.610 -> 257.219
2024-12-02-08:01:01-root-INFO: Regularization Change: 0.000 -> 0.930
2024-12-02-08:01:01-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-08:01:01-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-08:01:01-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-08:01:01-root-INFO: grad norm: 19.714 19.276 4.135
2024-12-02-08:01:01-root-INFO: Loss too large (258.934->263.161)! Learning rate decreased to 0.03019.
2024-12-02-08:01:01-root-INFO: Loss too large (258.934->259.572)! Learning rate decreased to 0.02415.
2024-12-02-08:01:02-root-INFO: grad norm: 19.131 18.969 2.482
2024-12-02-08:01:02-root-INFO: grad norm: 22.973 22.792 2.878
2024-12-02-08:01:02-root-INFO: Loss too large (257.223->258.202)! Learning rate decreased to 0.01932.
2024-12-02-08:01:03-root-INFO: grad norm: 18.316 18.177 2.251
2024-12-02-08:01:03-root-INFO: grad norm: 14.262 14.122 1.995
2024-12-02-08:01:04-root-INFO: grad norm: 12.749 12.625 1.777
2024-12-02-08:01:04-root-INFO: grad norm: 11.363 11.227 1.750
2024-12-02-08:01:05-root-INFO: grad norm: 10.649 10.526 1.613
2024-12-02-08:01:05-root-INFO: Loss Change: 258.934 -> 251.803
2024-12-02-08:01:05-root-INFO: Regularization Change: 0.000 -> 0.890
2024-12-02-08:01:05-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-08:01:05-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-08:01:05-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-08:01:05-root-INFO: grad norm: 14.033 13.756 2.776
2024-12-02-08:01:06-root-INFO: Loss too large (252.557->255.082)! Learning rate decreased to 0.03117.
2024-12-02-08:01:06-root-INFO: Loss too large (252.557->253.124)! Learning rate decreased to 0.02494.
2024-12-02-08:01:06-root-INFO: grad norm: 15.545 15.418 1.982
2024-12-02-08:01:07-root-INFO: grad norm: 20.008 19.857 2.448
2024-12-02-08:01:07-root-INFO: Loss too large (251.883->252.840)! Learning rate decreased to 0.01995.
2024-12-02-08:01:07-root-INFO: grad norm: 16.822 16.703 1.998
2024-12-02-08:01:08-root-INFO: grad norm: 13.752 13.622 1.888
2024-12-02-08:01:08-root-INFO: grad norm: 12.481 12.369 1.669
2024-12-02-08:01:09-root-INFO: grad norm: 11.246 11.119 1.684
2024-12-02-08:01:09-root-INFO: grad norm: 10.574 10.462 1.536
2024-12-02-08:01:10-root-INFO: Loss Change: 252.557 -> 247.385
2024-12-02-08:01:10-root-INFO: Regularization Change: 0.000 -> 0.779
2024-12-02-08:01:10-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-08:01:10-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-08:01:10-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-08:01:10-root-INFO: grad norm: 14.739 14.484 2.730
2024-12-02-08:01:10-root-INFO: Loss too large (248.125->251.324)! Learning rate decreased to 0.03218.
2024-12-02-08:01:10-root-INFO: Loss too large (248.125->249.010)! Learning rate decreased to 0.02574.
2024-12-02-08:01:11-root-INFO: grad norm: 16.293 16.173 1.978
2024-12-02-08:01:11-root-INFO: grad norm: 20.771 20.628 2.435
2024-12-02-08:01:11-root-INFO: Loss too large (247.636->248.732)! Learning rate decreased to 0.02059.
2024-12-02-08:01:12-root-INFO: grad norm: 17.194 17.079 1.979
2024-12-02-08:01:12-root-INFO: grad norm: 13.793 13.671 1.832
2024-12-02-08:01:13-root-INFO: grad norm: 12.327 12.220 1.619
2024-12-02-08:01:13-root-INFO: grad norm: 10.950 10.831 1.609
2024-12-02-08:01:14-root-INFO: grad norm: 10.166 10.058 1.474
2024-12-02-08:01:14-root-INFO: Loss Change: 248.125 -> 243.042
2024-12-02-08:01:14-root-INFO: Regularization Change: 0.000 -> 0.757
2024-12-02-08:01:14-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-08:01:14-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-08:01:14-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-08:01:14-root-INFO: grad norm: 13.524 13.303 2.433
2024-12-02-08:01:14-root-INFO: Loss too large (243.456->246.322)! Learning rate decreased to 0.03321.
2024-12-02-08:01:15-root-INFO: Loss too large (243.456->244.247)! Learning rate decreased to 0.02657.
2024-12-02-08:01:15-root-INFO: grad norm: 15.340 15.227 1.864
2024-12-02-08:01:15-root-INFO: Loss too large (243.065->243.137)! Learning rate decreased to 0.02126.
2024-12-02-08:01:16-root-INFO: grad norm: 13.156 13.037 1.771
2024-12-02-08:01:16-root-INFO: grad norm: 11.977 11.871 1.594
2024-12-02-08:01:17-root-INFO: grad norm: 10.991 10.879 1.567
2024-12-02-08:01:17-root-INFO: grad norm: 10.372 10.267 1.471
2024-12-02-08:01:18-root-INFO: grad norm: 9.780 9.669 1.471
2024-12-02-08:01:18-root-INFO: grad norm: 9.381 9.276 1.398
2024-12-02-08:01:19-root-INFO: Loss Change: 243.456 -> 238.881
2024-12-02-08:01:19-root-INFO: Regularization Change: 0.000 -> 0.693
2024-12-02-08:01:19-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-08:01:19-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-08:01:19-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-08:01:19-root-INFO: grad norm: 13.460 13.234 2.456
2024-12-02-08:01:19-root-INFO: Loss too large (239.624->242.553)! Learning rate decreased to 0.03427.
2024-12-02-08:01:19-root-INFO: Loss too large (239.624->240.399)! Learning rate decreased to 0.02742.
2024-12-02-08:01:20-root-INFO: grad norm: 15.185 15.076 1.817
2024-12-02-08:01:20-root-INFO: Loss too large (239.189->239.349)! Learning rate decreased to 0.02194.
2024-12-02-08:01:20-root-INFO: grad norm: 13.217 13.099 1.761
2024-12-02-08:01:21-root-INFO: grad norm: 12.171 12.069 1.573
2024-12-02-08:01:21-root-INFO: grad norm: 11.355 11.248 1.557
2024-12-02-08:01:22-root-INFO: grad norm: 10.831 10.732 1.464
2024-12-02-08:01:22-root-INFO: grad norm: 10.337 10.232 1.465
2024-12-02-08:01:23-root-INFO: grad norm: 9.999 9.901 1.397
2024-12-02-08:01:23-root-INFO: Loss Change: 239.624 -> 235.204
2024-12-02-08:01:23-root-INFO: Regularization Change: 0.000 -> 0.694
2024-12-02-08:01:23-root-INFO: Undo step: 120
2024-12-02-08:01:23-root-INFO: Undo step: 121
2024-12-02-08:01:23-root-INFO: Undo step: 122
2024-12-02-08:01:23-root-INFO: Undo step: 123
2024-12-02-08:01:23-root-INFO: Undo step: 124
2024-12-02-08:01:23-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-08:01:24-root-INFO: grad norm: 115.079 113.040 21.567
2024-12-02-08:01:24-root-INFO: grad norm: 72.618 71.526 12.548
2024-12-02-08:01:24-root-INFO: grad norm: 69.462 68.473 11.678
2024-12-02-08:01:25-root-INFO: grad norm: 75.490 74.962 8.908
2024-12-02-08:01:25-root-INFO: Loss too large (364.928->374.817)! Learning rate decreased to 0.02909.
2024-12-02-08:01:26-root-INFO: grad norm: 62.483 61.888 8.606
2024-12-02-08:01:26-root-INFO: grad norm: 49.962 49.657 5.506
2024-12-02-08:01:26-root-INFO: grad norm: 42.551 42.184 5.580
2024-12-02-08:01:27-root-INFO: grad norm: 37.554 37.307 4.303
2024-12-02-08:01:27-root-INFO: Loss Change: 615.847 -> 292.706
2024-12-02-08:01:27-root-INFO: Regularization Change: 0.000 -> 46.072
2024-12-02-08:01:27-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-08:01:27-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-08:01:27-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-08:01:28-root-INFO: grad norm: 35.447 35.223 3.981
2024-12-02-08:01:28-root-INFO: Loss too large (291.718->295.880)! Learning rate decreased to 0.03019.
2024-12-02-08:01:28-root-INFO: grad norm: 32.471 32.283 3.491
2024-12-02-08:01:29-root-INFO: grad norm: 30.460 30.285 3.259
2024-12-02-08:01:29-root-INFO: grad norm: 29.625 29.448 3.229
2024-12-02-08:01:30-root-INFO: grad norm: 29.556 29.384 3.191
2024-12-02-08:01:30-root-INFO: grad norm: 29.783 29.605 3.248
2024-12-02-08:01:31-root-INFO: grad norm: 30.369 30.189 3.304
2024-12-02-08:01:31-root-INFO: Loss too large (272.250->272.494)! Learning rate decreased to 0.02415.
2024-12-02-08:01:31-root-INFO: grad norm: 22.117 21.960 2.634
2024-12-02-08:01:32-root-INFO: Loss Change: 291.718 -> 264.799
2024-12-02-08:01:32-root-INFO: Regularization Change: 0.000 -> 4.649
2024-12-02-08:01:32-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-08:01:32-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-08:01:32-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-08:01:32-root-INFO: grad norm: 17.207 17.020 2.535
2024-12-02-08:01:32-root-INFO: Loss too large (264.829->265.973)! Learning rate decreased to 0.03117.
2024-12-02-08:01:32-root-INFO: grad norm: 19.525 19.385 2.337
2024-12-02-08:01:33-root-INFO: grad norm: 24.589 24.453 2.575
2024-12-02-08:01:33-root-INFO: Loss too large (263.196->264.641)! Learning rate decreased to 0.02494.
2024-12-02-08:01:34-root-INFO: grad norm: 20.335 20.198 2.358
2024-12-02-08:01:34-root-INFO: grad norm: 16.204 16.074 2.051
2024-12-02-08:01:34-root-INFO: grad norm: 14.670 14.538 1.961
2024-12-02-08:01:35-root-INFO: grad norm: 13.492 13.362 1.866
2024-12-02-08:01:35-root-INFO: grad norm: 12.920 12.789 1.835
2024-12-02-08:01:36-root-INFO: Loss Change: 264.829 -> 254.067
2024-12-02-08:01:36-root-INFO: Regularization Change: 0.000 -> 2.086
2024-12-02-08:01:36-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-08:01:36-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-08:01:36-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-08:01:36-root-INFO: grad norm: 16.499 16.256 2.822
2024-12-02-08:01:36-root-INFO: Loss too large (254.670->257.108)! Learning rate decreased to 0.03218.
2024-12-02-08:01:36-root-INFO: Loss too large (254.670->254.731)! Learning rate decreased to 0.02574.
2024-12-02-08:01:37-root-INFO: grad norm: 15.117 14.975 2.066
2024-12-02-08:01:37-root-INFO: grad norm: 14.647 14.509 2.008
2024-12-02-08:01:38-root-INFO: grad norm: 14.414 14.279 1.967
2024-12-02-08:01:38-root-INFO: grad norm: 14.371 14.242 1.919
2024-12-02-08:01:39-root-INFO: grad norm: 14.377 14.246 1.939
2024-12-02-08:01:39-root-INFO: grad norm: 14.522 14.398 1.894
2024-12-02-08:01:40-root-INFO: grad norm: 14.637 14.509 1.935
2024-12-02-08:01:40-root-INFO: Loss Change: 254.670 -> 247.305
2024-12-02-08:01:40-root-INFO: Regularization Change: 0.000 -> 1.437
2024-12-02-08:01:40-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-08:01:40-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-08:01:40-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-08:01:40-root-INFO: grad norm: 19.529 19.298 2.995
2024-12-02-08:01:40-root-INFO: Loss too large (248.052->253.895)! Learning rate decreased to 0.03321.
2024-12-02-08:01:41-root-INFO: Loss too large (248.052->249.756)! Learning rate decreased to 0.02657.
2024-12-02-08:01:41-root-INFO: grad norm: 18.976 18.829 2.359
2024-12-02-08:01:42-root-INFO: grad norm: 18.949 18.808 2.303
2024-12-02-08:01:42-root-INFO: grad norm: 18.821 18.679 2.312
2024-12-02-08:01:43-root-INFO: grad norm: 18.754 18.621 2.232
2024-12-02-08:01:43-root-INFO: grad norm: 18.653 18.514 2.279
2024-12-02-08:01:44-root-INFO: grad norm: 18.574 18.444 2.196
2024-12-02-08:01:44-root-INFO: grad norm: 18.506 18.369 2.249
2024-12-02-08:01:44-root-INFO: Loss Change: 248.052 -> 241.964
2024-12-02-08:01:44-root-INFO: Regularization Change: 0.000 -> 1.235
2024-12-02-08:01:44-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-08:01:44-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-08:01:44-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-08:01:45-root-INFO: grad norm: 23.585 23.330 3.460
2024-12-02-08:01:45-root-INFO: Loss too large (243.441->253.574)! Learning rate decreased to 0.03427.
2024-12-02-08:01:45-root-INFO: Loss too large (243.441->246.870)! Learning rate decreased to 0.02742.
2024-12-02-08:01:45-root-INFO: grad norm: 22.921 22.763 2.691
2024-12-02-08:01:46-root-INFO: grad norm: 22.589 22.432 2.654
2024-12-02-08:01:46-root-INFO: grad norm: 22.181 22.029 2.589
2024-12-02-08:01:47-root-INFO: grad norm: 21.756 21.612 2.499
2024-12-02-08:01:47-root-INFO: grad norm: 21.378 21.231 2.497
2024-12-02-08:01:48-root-INFO: grad norm: 20.987 20.849 2.396
2024-12-02-08:01:48-root-INFO: grad norm: 20.680 20.539 2.413
2024-12-02-08:01:49-root-INFO: Loss Change: 243.441 -> 237.323
2024-12-02-08:01:49-root-INFO: Regularization Change: 0.000 -> 1.140
2024-12-02-08:01:49-root-INFO: Undo step: 120
2024-12-02-08:01:49-root-INFO: Undo step: 121
2024-12-02-08:01:49-root-INFO: Undo step: 122
2024-12-02-08:01:49-root-INFO: Undo step: 123
2024-12-02-08:01:49-root-INFO: Undo step: 124
2024-12-02-08:01:49-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-08:01:49-root-INFO: grad norm: 125.330 123.878 19.023
2024-12-02-08:01:49-root-INFO: grad norm: 78.772 77.941 11.407
2024-12-02-08:01:50-root-INFO: grad norm: 75.821 75.200 9.687
2024-12-02-08:01:50-root-INFO: grad norm: 68.281 67.637 9.353
2024-12-02-08:01:51-root-INFO: grad norm: 62.561 62.155 7.120
2024-12-02-08:01:51-root-INFO: Loss too large (343.995->347.203)! Learning rate decreased to 0.02909.
2024-12-02-08:01:51-root-INFO: grad norm: 47.220 46.654 7.285
2024-12-02-08:01:52-root-INFO: grad norm: 37.579 37.232 5.097
2024-12-02-08:01:52-root-INFO: grad norm: 36.285 35.830 5.728
2024-12-02-08:01:53-root-INFO: Loss Change: 654.188 -> 296.098
2024-12-02-08:01:53-root-INFO: Regularization Change: 0.000 -> 50.679
2024-12-02-08:01:53-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-08:01:53-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-08:01:53-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-08:01:53-root-INFO: grad norm: 48.208 47.645 7.347
2024-12-02-08:01:53-root-INFO: Loss too large (300.747->319.396)! Learning rate decreased to 0.03019.
2024-12-02-08:01:53-root-INFO: Loss too large (300.747->302.603)! Learning rate decreased to 0.02415.
2024-12-02-08:01:54-root-INFO: grad norm: 35.077 34.693 5.174
2024-12-02-08:01:54-root-INFO: grad norm: 23.890 23.616 3.612
2024-12-02-08:01:54-root-INFO: grad norm: 20.219 19.906 3.544
2024-12-02-08:01:55-root-INFO: grad norm: 17.758 17.519 2.903
2024-12-02-08:01:55-root-INFO: grad norm: 16.477 16.198 3.020
2024-12-02-08:01:56-root-INFO: grad norm: 15.572 15.354 2.597
2024-12-02-08:01:56-root-INFO: grad norm: 15.069 14.817 2.742
2024-12-02-08:01:57-root-INFO: Loss Change: 300.747 -> 266.636
2024-12-02-08:01:57-root-INFO: Regularization Change: 0.000 -> 4.273
2024-12-02-08:01:57-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-08:01:57-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-08:01:57-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-08:01:57-root-INFO: grad norm: 19.253 18.931 3.508
2024-12-02-08:01:57-root-INFO: Loss too large (267.322->269.948)! Learning rate decreased to 0.03117.
2024-12-02-08:01:58-root-INFO: grad norm: 24.766 24.525 3.452
2024-12-02-08:01:58-root-INFO: Loss too large (266.937->268.166)! Learning rate decreased to 0.02494.
2024-12-02-08:01:58-root-INFO: grad norm: 23.739 23.542 3.052
2024-12-02-08:01:59-root-INFO: grad norm: 23.063 22.849 3.134
2024-12-02-08:01:59-root-INFO: grad norm: 22.444 22.265 2.827
2024-12-02-08:02:00-root-INFO: grad norm: 22.098 21.898 2.966
2024-12-02-08:02:00-root-INFO: grad norm: 21.817 21.648 2.710
2024-12-02-08:02:00-root-INFO: grad norm: 21.712 21.521 2.869
2024-12-02-08:02:01-root-INFO: Loss Change: 267.322 -> 255.768
2024-12-02-08:02:01-root-INFO: Regularization Change: 0.000 -> 2.513
2024-12-02-08:02:01-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-08:02:01-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-08:02:01-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-08:02:01-root-INFO: grad norm: 27.976 27.654 4.228
2024-12-02-08:02:01-root-INFO: Loss too large (257.432->270.274)! Learning rate decreased to 0.03218.
2024-12-02-08:02:01-root-INFO: Loss too large (257.432->261.711)! Learning rate decreased to 0.02574.
2024-12-02-08:02:02-root-INFO: grad norm: 27.184 26.989 3.253
2024-12-02-08:02:02-root-INFO: grad norm: 26.494 26.307 3.145
2024-12-02-08:02:03-root-INFO: grad norm: 25.942 25.755 3.110
2024-12-02-08:02:03-root-INFO: grad norm: 25.306 25.134 2.948
2024-12-02-08:02:04-root-INFO: grad norm: 24.868 24.688 2.988
2024-12-02-08:02:04-root-INFO: grad norm: 24.417 24.252 2.835
2024-12-02-08:02:05-root-INFO: grad norm: 24.139 23.964 2.896
2024-12-02-08:02:05-root-INFO: Loss Change: 257.432 -> 247.763
2024-12-02-08:02:05-root-INFO: Regularization Change: 0.000 -> 1.758
2024-12-02-08:02:05-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-08:02:05-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-08:02:05-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-08:02:05-root-INFO: grad norm: 29.774 29.471 4.237
2024-12-02-08:02:05-root-INFO: Loss too large (249.378->265.487)! Learning rate decreased to 0.03321.
2024-12-02-08:02:06-root-INFO: Loss too large (249.378->255.151)! Learning rate decreased to 0.02657.
2024-12-02-08:02:06-root-INFO: grad norm: 28.841 28.651 3.306
2024-12-02-08:02:07-root-INFO: grad norm: 27.897 27.714 3.192
2024-12-02-08:02:07-root-INFO: grad norm: 27.174 26.994 3.121
2024-12-02-08:02:07-root-INFO: grad norm: 26.343 26.175 2.975
2024-12-02-08:02:08-root-INFO: grad norm: 25.785 25.613 2.970
2024-12-02-08:02:08-root-INFO: grad norm: 25.196 25.035 2.847
2024-12-02-08:02:09-root-INFO: grad norm: 24.832 24.666 2.861
2024-12-02-08:02:09-root-INFO: Loss Change: 249.378 -> 240.858
2024-12-02-08:02:09-root-INFO: Regularization Change: 0.000 -> 1.457
2024-12-02-08:02:09-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-08:02:09-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-08:02:09-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-08:02:09-root-INFO: grad norm: 30.503 30.203 4.266
2024-12-02-08:02:10-root-INFO: Loss too large (243.024->260.801)! Learning rate decreased to 0.03427.
2024-12-02-08:02:10-root-INFO: Loss too large (243.024->249.462)! Learning rate decreased to 0.02742.
2024-12-02-08:02:10-root-INFO: grad norm: 29.333 29.152 3.253
2024-12-02-08:02:11-root-INFO: grad norm: 28.161 27.980 3.183
2024-12-02-08:02:11-root-INFO: grad norm: 27.299 27.130 3.037
2024-12-02-08:02:12-root-INFO: grad norm: 26.366 26.202 2.938
2024-12-02-08:02:12-root-INFO: grad norm: 25.737 25.576 2.878
2024-12-02-08:02:13-root-INFO: grad norm: 25.099 24.942 2.797
2024-12-02-08:02:13-root-INFO: grad norm: 24.697 24.542 2.766
2024-12-02-08:02:14-root-INFO: Loss Change: 243.024 -> 235.029
2024-12-02-08:02:14-root-INFO: Regularization Change: 0.000 -> 1.290
2024-12-02-08:02:14-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-08:02:14-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-08:02:14-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-08:02:14-root-INFO: grad norm: 33.316 32.912 5.171
2024-12-02-08:02:14-root-INFO: Loss too large (238.189->260.013)! Learning rate decreased to 0.03536.
2024-12-02-08:02:14-root-INFO: Loss too large (238.189->246.332)! Learning rate decreased to 0.02829.
2024-12-02-08:02:15-root-INFO: grad norm: 31.855 31.671 3.420
2024-12-02-08:02:15-root-INFO: grad norm: 30.179 29.982 3.441
2024-12-02-08:02:16-root-INFO: grad norm: 28.992 28.821 3.144
2024-12-02-08:02:16-root-INFO: grad norm: 27.698 27.525 3.093
2024-12-02-08:02:17-root-INFO: grad norm: 26.857 26.695 2.944
2024-12-02-08:02:17-root-INFO: grad norm: 26.011 25.850 2.895
2024-12-02-08:02:18-root-INFO: grad norm: 25.487 25.332 2.809
2024-12-02-08:02:18-root-INFO: Loss Change: 238.189 -> 229.680
2024-12-02-08:02:18-root-INFO: Regularization Change: 0.000 -> 1.229
2024-12-02-08:02:18-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-08:02:18-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-08:02:18-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-08:02:18-root-INFO: grad norm: 27.518 27.314 3.342
2024-12-02-08:02:18-root-INFO: Loss too large (231.145->247.150)! Learning rate decreased to 0.03648.
2024-12-02-08:02:19-root-INFO: Loss too large (231.145->237.047)! Learning rate decreased to 0.02919.
2024-12-02-08:02:19-root-INFO: grad norm: 26.787 26.628 2.910
2024-12-02-08:02:20-root-INFO: grad norm: 26.028 25.868 2.882
2024-12-02-08:02:20-root-INFO: grad norm: 25.522 25.371 2.769
2024-12-02-08:02:20-root-INFO: grad norm: 24.978 24.827 2.742
2024-12-02-08:02:21-root-INFO: grad norm: 24.636 24.490 2.675
2024-12-02-08:02:21-root-INFO: grad norm: 24.296 24.150 2.662
2024-12-02-08:02:22-root-INFO: grad norm: 24.095 23.953 2.618
2024-12-02-08:02:22-root-INFO: Loss Change: 231.145 -> 225.197
2024-12-02-08:02:22-root-INFO: Regularization Change: 0.000 -> 1.093
2024-12-02-08:02:22-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-08:02:22-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-08:02:22-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-08:02:23-root-INFO: grad norm: 32.552 32.198 4.784
2024-12-02-08:02:23-root-INFO: Loss too large (228.549->251.270)! Learning rate decreased to 0.03763.
2024-12-02-08:02:23-root-INFO: Loss too large (228.549->237.089)! Learning rate decreased to 0.03011.
2024-12-02-08:02:23-root-INFO: grad norm: 31.200 31.026 3.291
2024-12-02-08:02:24-root-INFO: grad norm: 29.753 29.565 3.338
2024-12-02-08:02:24-root-INFO: Loss too large (226.022->226.059)! Learning rate decreased to 0.02409.
2024-12-02-08:02:24-root-INFO: grad norm: 18.816 18.702 2.072
2024-12-02-08:02:25-root-INFO: grad norm: 11.653 11.546 1.575
2024-12-02-08:02:25-root-INFO: grad norm: 8.282 8.192 1.219
2024-12-02-08:02:26-root-INFO: grad norm: 6.269 6.161 1.160
2024-12-02-08:02:26-root-INFO: grad norm: 5.231 5.124 1.050
2024-12-02-08:02:27-root-INFO: Loss Change: 228.549 -> 217.184
2024-12-02-08:02:27-root-INFO: Regularization Change: 0.000 -> 0.981
2024-12-02-08:02:27-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-08:02:27-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-08:02:27-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-08:02:27-root-INFO: grad norm: 9.946 9.676 2.299
2024-12-02-08:02:27-root-INFO: Loss too large (217.632->218.267)! Learning rate decreased to 0.03881.
2024-12-02-08:02:27-root-INFO: grad norm: 12.888 12.786 1.615
2024-12-02-08:02:28-root-INFO: Loss too large (217.301->218.229)! Learning rate decreased to 0.03105.
2024-12-02-08:02:28-root-INFO: grad norm: 13.798 13.683 1.784
2024-12-02-08:02:29-root-INFO: grad norm: 14.855 14.763 1.645
2024-12-02-08:02:29-root-INFO: grad norm: 16.212 16.102 1.882
2024-12-02-08:02:29-root-INFO: Loss too large (216.301->216.351)! Learning rate decreased to 0.02484.
2024-12-02-08:02:30-root-INFO: grad norm: 11.734 11.651 1.398
2024-12-02-08:02:30-root-INFO: grad norm: 8.543 8.448 1.269
2024-12-02-08:02:31-root-INFO: grad norm: 6.747 6.660 1.080
2024-12-02-08:02:31-root-INFO: Loss Change: 217.632 -> 213.085
2024-12-02-08:02:31-root-INFO: Regularization Change: 0.000 -> 0.924
2024-12-02-08:02:31-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-08:02:31-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-08:02:31-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-08:02:31-root-INFO: grad norm: 12.003 11.768 2.366
2024-12-02-08:02:31-root-INFO: Loss too large (214.110->216.383)! Learning rate decreased to 0.04002.
2024-12-02-08:02:32-root-INFO: Loss too large (214.110->214.497)! Learning rate decreased to 0.03202.
2024-12-02-08:02:32-root-INFO: grad norm: 12.166 12.080 1.446
2024-12-02-08:02:33-root-INFO: grad norm: 13.561 13.453 1.715
2024-12-02-08:02:33-root-INFO: grad norm: 15.011 14.924 1.612
2024-12-02-08:02:33-root-INFO: grad norm: 16.835 16.729 1.890
2024-12-02-08:02:34-root-INFO: Loss too large (212.857->213.109)! Learning rate decreased to 0.02561.
2024-12-02-08:02:34-root-INFO: grad norm: 12.454 12.373 1.418
2024-12-02-08:02:34-root-INFO: grad norm: 9.197 9.107 1.284
2024-12-02-08:02:35-root-INFO: grad norm: 7.292 7.212 1.078
2024-12-02-08:02:35-root-INFO: Loss Change: 214.110 -> 209.566
2024-12-02-08:02:35-root-INFO: Regularization Change: 0.000 -> 0.855
2024-12-02-08:02:35-root-INFO: Undo step: 115
2024-12-02-08:02:35-root-INFO: Undo step: 116
2024-12-02-08:02:35-root-INFO: Undo step: 117
2024-12-02-08:02:35-root-INFO: Undo step: 118
2024-12-02-08:02:35-root-INFO: Undo step: 119
2024-12-02-08:02:35-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-08:02:36-root-INFO: grad norm: 105.619 103.593 20.587
2024-12-02-08:02:36-root-INFO: grad norm: 74.919 73.792 12.950
2024-12-02-08:02:37-root-INFO: grad norm: 76.321 74.971 14.288
2024-12-02-08:02:37-root-INFO: grad norm: 73.833 72.770 12.484
2024-12-02-08:02:37-root-INFO: grad norm: 77.372 76.129 13.812
2024-12-02-08:02:38-root-INFO: grad norm: 81.997 81.101 12.088
2024-12-02-08:02:38-root-INFO: Loss too large (327.743->342.252)! Learning rate decreased to 0.03427.
2024-12-02-08:02:39-root-INFO: grad norm: 56.979 56.107 9.930
2024-12-02-08:02:39-root-INFO: grad norm: 39.231 38.822 5.654
2024-12-02-08:02:39-root-INFO: Loss Change: 578.896 -> 262.662
2024-12-02-08:02:39-root-INFO: Regularization Change: 0.000 -> 53.867
2024-12-02-08:02:39-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-08:02:39-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-08:02:40-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-08:02:40-root-INFO: grad norm: 32.203 31.920 4.257
2024-12-02-08:02:40-root-INFO: Loss too large (259.782->263.340)! Learning rate decreased to 0.03536.
2024-12-02-08:02:40-root-INFO: grad norm: 32.346 32.056 4.325
2024-12-02-08:02:40-root-INFO: Loss too large (253.738->253.858)! Learning rate decreased to 0.02829.
2024-12-02-08:02:41-root-INFO: grad norm: 23.974 23.751 3.266
2024-12-02-08:02:41-root-INFO: grad norm: 17.818 17.602 2.766
2024-12-02-08:02:42-root-INFO: grad norm: 15.558 15.369 2.418
2024-12-02-08:02:42-root-INFO: grad norm: 13.925 13.734 2.297
2024-12-02-08:02:43-root-INFO: grad norm: 13.009 12.834 2.123
2024-12-02-08:02:43-root-INFO: grad norm: 12.303 12.127 2.072
2024-12-02-08:02:44-root-INFO: Loss Change: 259.782 -> 234.219
2024-12-02-08:02:44-root-INFO: Regularization Change: 0.000 -> 3.935
2024-12-02-08:02:44-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-08:02:44-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-08:02:44-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-08:02:44-root-INFO: grad norm: 12.365 12.176 2.153
2024-12-02-08:02:44-root-INFO: Loss too large (234.431->234.525)! Learning rate decreased to 0.03648.
2024-12-02-08:02:45-root-INFO: grad norm: 15.875 15.721 2.203
2024-12-02-08:02:45-root-INFO: Loss too large (233.348->233.474)! Learning rate decreased to 0.02919.
2024-12-02-08:02:45-root-INFO: grad norm: 14.862 14.721 2.036
2024-12-02-08:02:46-root-INFO: grad norm: 13.926 13.783 1.996
2024-12-02-08:02:46-root-INFO: grad norm: 13.370 13.239 1.868
2024-12-02-08:02:47-root-INFO: grad norm: 12.863 12.727 1.865
2024-12-02-08:02:47-root-INFO: grad norm: 12.541 12.417 1.762
2024-12-02-08:02:47-root-INFO: grad norm: 12.262 12.133 1.777
2024-12-02-08:02:48-root-INFO: Loss Change: 234.431 -> 225.346
2024-12-02-08:02:48-root-INFO: Regularization Change: 0.000 -> 2.198
2024-12-02-08:02:48-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-08:02:48-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-08:02:48-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-08:02:48-root-INFO: grad norm: 16.229 15.839 3.539
2024-12-02-08:02:48-root-INFO: Loss too large (226.114->228.588)! Learning rate decreased to 0.03763.
2024-12-02-08:02:49-root-INFO: grad norm: 20.186 20.041 2.420
2024-12-02-08:02:49-root-INFO: Loss too large (225.739->227.186)! Learning rate decreased to 0.03011.
2024-12-02-08:02:49-root-INFO: grad norm: 18.140 17.990 2.334
2024-12-02-08:02:50-root-INFO: grad norm: 16.503 16.386 1.955
2024-12-02-08:02:50-root-INFO: grad norm: 15.403 15.280 1.944
2024-12-02-08:02:51-root-INFO: grad norm: 14.515 14.407 1.763
2024-12-02-08:02:51-root-INFO: grad norm: 13.856 13.745 1.748
2024-12-02-08:02:52-root-INFO: grad norm: 13.330 13.228 1.645
2024-12-02-08:02:52-root-INFO: Loss Change: 226.114 -> 218.335
2024-12-02-08:02:52-root-INFO: Regularization Change: 0.000 -> 1.726
2024-12-02-08:02:52-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-08:02:52-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-08:02:52-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-08:02:52-root-INFO: grad norm: 17.083 16.809 3.044
2024-12-02-08:02:52-root-INFO: Loss too large (219.053->223.685)! Learning rate decreased to 0.03881.
2024-12-02-08:02:53-root-INFO: Loss too large (219.053->219.904)! Learning rate decreased to 0.03105.
2024-12-02-08:02:53-root-INFO: grad norm: 15.735 15.635 1.772
2024-12-02-08:02:54-root-INFO: grad norm: 15.070 14.946 1.932
2024-12-02-08:02:54-root-INFO: grad norm: 14.580 14.486 1.653
2024-12-02-08:02:55-root-INFO: grad norm: 14.151 14.045 1.728
2024-12-02-08:02:55-root-INFO: grad norm: 13.839 13.749 1.576
2024-12-02-08:02:55-root-INFO: grad norm: 13.564 13.466 1.622
2024-12-02-08:02:56-root-INFO: grad norm: 13.377 13.291 1.520
2024-12-02-08:02:56-root-INFO: Loss Change: 219.053 -> 212.656
2024-12-02-08:02:56-root-INFO: Regularization Change: 0.000 -> 1.350
2024-12-02-08:02:56-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-08:02:56-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-08:02:56-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-08:02:57-root-INFO: grad norm: 18.149 17.891 3.049
2024-12-02-08:02:57-root-INFO: Loss too large (213.949->220.055)! Learning rate decreased to 0.04002.
2024-12-02-08:02:57-root-INFO: Loss too large (213.949->215.572)! Learning rate decreased to 0.03202.
2024-12-02-08:02:57-root-INFO: grad norm: 16.935 16.841 1.777
2024-12-02-08:02:58-root-INFO: grad norm: 16.282 16.163 1.966
2024-12-02-08:02:58-root-INFO: grad norm: 15.798 15.709 1.670
2024-12-02-08:02:59-root-INFO: grad norm: 15.312 15.209 1.773
2024-12-02-08:02:59-root-INFO: grad norm: 14.970 14.884 1.598
2024-12-02-08:03:00-root-INFO: grad norm: 14.633 14.537 1.672
2024-12-02-08:03:00-root-INFO: grad norm: 14.407 14.324 1.544
2024-12-02-08:03:00-root-INFO: Loss Change: 213.949 -> 208.176
2024-12-02-08:03:00-root-INFO: Regularization Change: 0.000 -> 1.208
2024-12-02-08:03:00-root-INFO: Undo step: 115
2024-12-02-08:03:00-root-INFO: Undo step: 116
2024-12-02-08:03:00-root-INFO: Undo step: 117
2024-12-02-08:03:00-root-INFO: Undo step: 118
2024-12-02-08:03:00-root-INFO: Undo step: 119
2024-12-02-08:03:01-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-08:03:01-root-INFO: grad norm: 102.484 100.080 22.068
2024-12-02-08:03:01-root-INFO: grad norm: 79.905 78.870 12.822
2024-12-02-08:03:02-root-INFO: grad norm: 86.516 85.648 12.225
2024-12-02-08:03:02-root-INFO: grad norm: 71.547 70.899 9.605
2024-12-02-08:03:03-root-INFO: grad norm: 53.951 53.102 9.536
2024-12-02-08:03:03-root-INFO: grad norm: 48.254 47.766 6.850
2024-12-02-08:03:03-root-INFO: Loss too large (284.974->285.369)! Learning rate decreased to 0.03427.
2024-12-02-08:03:04-root-INFO: grad norm: 37.614 37.101 6.191
2024-12-02-08:03:04-root-INFO: grad norm: 33.678 33.382 4.457
2024-12-02-08:03:04-root-INFO: Loss Change: 555.654 -> 256.763
2024-12-02-08:03:04-root-INFO: Regularization Change: 0.000 -> 51.993
2024-12-02-08:03:04-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-08:03:04-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-08:03:05-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-08:03:05-root-INFO: grad norm: 30.526 30.241 4.164
2024-12-02-08:03:05-root-INFO: Loss too large (254.242->257.964)! Learning rate decreased to 0.03536.
2024-12-02-08:03:05-root-INFO: grad norm: 31.406 31.164 3.888
2024-12-02-08:03:06-root-INFO: grad norm: 30.686 30.411 4.099
2024-12-02-08:03:06-root-INFO: grad norm: 30.402 30.167 3.775
2024-12-02-08:03:06-root-INFO: Loss too large (243.564->243.765)! Learning rate decreased to 0.02829.
2024-12-02-08:03:07-root-INFO: grad norm: 23.017 22.789 3.231
2024-12-02-08:03:07-root-INFO: grad norm: 18.639 18.461 2.568
2024-12-02-08:03:08-root-INFO: grad norm: 17.114 16.935 2.469
2024-12-02-08:03:08-root-INFO: grad norm: 16.349 16.190 2.272
2024-12-02-08:03:09-root-INFO: Loss Change: 254.242 -> 229.909
2024-12-02-08:03:09-root-INFO: Regularization Change: 0.000 -> 4.497
2024-12-02-08:03:09-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-08:03:09-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-08:03:09-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-08:03:09-root-INFO: grad norm: 15.376 15.216 2.213
2024-12-02-08:03:09-root-INFO: Loss too large (229.827->232.015)! Learning rate decreased to 0.03648.
2024-12-02-08:03:10-root-INFO: grad norm: 21.308 21.154 2.556
2024-12-02-08:03:10-root-INFO: Loss too large (229.456->231.368)! Learning rate decreased to 0.02919.
2024-12-02-08:03:10-root-INFO: grad norm: 20.173 20.031 2.387
2024-12-02-08:03:11-root-INFO: grad norm: 19.131 18.988 2.328
2024-12-02-08:03:11-root-INFO: grad norm: 18.676 18.546 2.199
2024-12-02-08:03:12-root-INFO: grad norm: 18.260 18.129 2.190
2024-12-02-08:03:12-root-INFO: grad norm: 18.045 17.923 2.097
2024-12-02-08:03:13-root-INFO: grad norm: 17.864 17.740 2.108
2024-12-02-08:03:13-root-INFO: Loss Change: 229.827 -> 221.336
2024-12-02-08:03:13-root-INFO: Regularization Change: 0.000 -> 2.241
2024-12-02-08:03:13-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-08:03:13-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-08:03:13-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-08:03:13-root-INFO: grad norm: 17.034 16.808 2.766
2024-12-02-08:03:14-root-INFO: Loss too large (220.852->224.268)! Learning rate decreased to 0.03763.
2024-12-02-08:03:14-root-INFO: grad norm: 22.511 22.386 2.371
2024-12-02-08:03:14-root-INFO: Loss too large (220.818->223.508)! Learning rate decreased to 0.03011.
2024-12-02-08:03:15-root-INFO: grad norm: 20.797 20.685 2.155
2024-12-02-08:03:15-root-INFO: grad norm: 19.458 19.341 2.125
2024-12-02-08:03:16-root-INFO: grad norm: 18.888 18.786 1.961
2024-12-02-08:03:16-root-INFO: grad norm: 18.482 18.373 2.008
2024-12-02-08:03:17-root-INFO: grad norm: 18.293 18.193 1.908
2024-12-02-08:03:17-root-INFO: grad norm: 18.190 18.084 1.961
2024-12-02-08:03:17-root-INFO: Loss Change: 220.852 -> 214.273
2024-12-02-08:03:17-root-INFO: Regularization Change: 0.000 -> 1.733
2024-12-02-08:03:17-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-08:03:17-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-08:03:18-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-08:03:18-root-INFO: grad norm: 17.535 17.385 2.292
2024-12-02-08:03:18-root-INFO: Loss too large (213.760->219.358)! Learning rate decreased to 0.03881.
2024-12-02-08:03:18-root-INFO: Loss too large (213.760->214.883)! Learning rate decreased to 0.03105.
2024-12-02-08:03:19-root-INFO: grad norm: 17.081 16.979 1.866
2024-12-02-08:03:19-root-INFO: grad norm: 17.232 17.140 1.781
2024-12-02-08:03:20-root-INFO: grad norm: 17.655 17.558 1.850
2024-12-02-08:03:20-root-INFO: grad norm: 18.020 17.929 1.813
2024-12-02-08:03:21-root-INFO: grad norm: 18.514 18.418 1.892
2024-12-02-08:03:21-root-INFO: grad norm: 18.859 18.765 1.885
2024-12-02-08:03:22-root-INFO: grad norm: 19.263 19.165 1.944
2024-12-02-08:03:22-root-INFO: Loss Change: 213.760 -> 209.092
2024-12-02-08:03:22-root-INFO: Regularization Change: 0.000 -> 1.337
2024-12-02-08:03:22-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-08:03:22-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-08:03:22-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-08:03:22-root-INFO: grad norm: 17.642 17.525 2.030
2024-12-02-08:03:23-root-INFO: Loss too large (208.494->214.926)! Learning rate decreased to 0.04002.
2024-12-02-08:03:23-root-INFO: Loss too large (208.494->210.050)! Learning rate decreased to 0.03202.
2024-12-02-08:03:23-root-INFO: grad norm: 17.540 17.444 1.826
2024-12-02-08:03:24-root-INFO: grad norm: 17.891 17.806 1.743
2024-12-02-08:03:24-root-INFO: grad norm: 18.470 18.377 1.848
2024-12-02-08:03:25-root-INFO: grad norm: 18.868 18.779 1.825
2024-12-02-08:03:25-root-INFO: grad norm: 19.347 19.253 1.905
2024-12-02-08:03:25-root-INFO: Loss too large (205.594->205.629)! Learning rate decreased to 0.02561.
2024-12-02-08:03:26-root-INFO: grad norm: 13.253 13.178 1.412
2024-12-02-08:03:26-root-INFO: grad norm: 9.091 9.012 1.196
2024-12-02-08:03:27-root-INFO: Loss Change: 208.494 -> 201.892
2024-12-02-08:03:27-root-INFO: Regularization Change: 0.000 -> 1.056
2024-12-02-08:03:27-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-08:03:27-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-08:03:27-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-08:03:27-root-INFO: grad norm: 8.255 8.075 1.715
2024-12-02-08:03:28-root-INFO: grad norm: 12.843 12.752 1.531
2024-12-02-08:03:28-root-INFO: Loss too large (201.963->204.958)! Learning rate decreased to 0.04126.
2024-12-02-08:03:28-root-INFO: Loss too large (201.963->202.563)! Learning rate decreased to 0.03301.
2024-12-02-08:03:28-root-INFO: grad norm: 12.225 12.148 1.366
2024-12-02-08:03:29-root-INFO: grad norm: 12.220 12.149 1.312
2024-12-02-08:03:29-root-INFO: grad norm: 12.484 12.419 1.277
2024-12-02-08:03:30-root-INFO: grad norm: 13.162 13.095 1.328
2024-12-02-08:03:30-root-INFO: grad norm: 13.999 13.936 1.329
2024-12-02-08:03:31-root-INFO: grad norm: 15.253 15.185 1.437
2024-12-02-08:03:31-root-INFO: Loss too large (199.324->199.366)! Learning rate decreased to 0.02641.
2024-12-02-08:03:31-root-INFO: Loss Change: 202.202 -> 198.172
2024-12-02-08:03:31-root-INFO: Regularization Change: 0.000 -> 1.224
2024-12-02-08:03:31-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-08:03:31-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-08:03:31-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-08:03:31-root-INFO: grad norm: 11.020 10.859 1.878
2024-12-02-08:03:32-root-INFO: Loss too large (198.016->199.936)! Learning rate decreased to 0.04253.
2024-12-02-08:03:32-root-INFO: Loss too large (198.016->198.204)! Learning rate decreased to 0.03403.
2024-12-02-08:03:32-root-INFO: grad norm: 10.910 10.843 1.207
2024-12-02-08:03:33-root-INFO: grad norm: 11.735 11.672 1.211
2024-12-02-08:03:33-root-INFO: grad norm: 13.145 13.083 1.271
2024-12-02-08:03:33-root-INFO: Loss too large (196.713->196.717)! Learning rate decreased to 0.02722.
2024-12-02-08:03:34-root-INFO: grad norm: 9.953 9.896 1.068
2024-12-02-08:03:34-root-INFO: grad norm: 7.756 7.690 1.008
2024-12-02-08:03:35-root-INFO: grad norm: 6.441 6.376 0.913
2024-12-02-08:03:35-root-INFO: grad norm: 5.500 5.425 0.905
2024-12-02-08:03:36-root-INFO: Loss Change: 198.016 -> 193.765
2024-12-02-08:03:36-root-INFO: Regularization Change: 0.000 -> 0.819
2024-12-02-08:03:36-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-08:03:36-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-08:03:36-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-08:03:36-root-INFO: grad norm: 8.254 7.966 2.159
2024-12-02-08:03:36-root-INFO: grad norm: 12.110 11.972 1.825
2024-12-02-08:03:36-root-INFO: Loss too large (193.860->196.733)! Learning rate decreased to 0.04384.
2024-12-02-08:03:37-root-INFO: Loss too large (193.860->194.382)! Learning rate decreased to 0.03507.
2024-12-02-08:03:37-root-INFO: grad norm: 11.577 11.476 1.527
2024-12-02-08:03:38-root-INFO: grad norm: 12.093 12.014 1.380
2024-12-02-08:03:38-root-INFO: grad norm: 13.214 13.130 1.486
2024-12-02-08:03:38-root-INFO: grad norm: 14.545 14.470 1.479
2024-12-02-08:03:39-root-INFO: grad norm: 16.521 16.438 1.656
2024-12-02-08:03:39-root-INFO: Loss too large (192.452->192.887)! Learning rate decreased to 0.02806.
2024-12-02-08:03:40-root-INFO: grad norm: 12.459 12.395 1.263
2024-12-02-08:03:40-root-INFO: Loss Change: 194.049 -> 190.348
2024-12-02-08:03:40-root-INFO: Regularization Change: 0.000 -> 1.083
2024-12-02-08:03:40-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-08:03:40-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-08:03:40-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-08:03:40-root-INFO: grad norm: 13.780 13.648 1.900
2024-12-02-08:03:41-root-INFO: Loss too large (191.111->196.604)! Learning rate decreased to 0.04517.
2024-12-02-08:03:41-root-INFO: Loss too large (191.111->193.222)! Learning rate decreased to 0.03614.
2024-12-02-08:03:41-root-INFO: Loss too large (191.111->191.217)! Learning rate decreased to 0.02891.
2024-12-02-08:03:41-root-INFO: grad norm: 10.718 10.658 1.136
2024-12-02-08:03:42-root-INFO: grad norm: 8.617 8.548 1.090
2024-12-02-08:03:42-root-INFO: grad norm: 7.328 7.270 0.918
2024-12-02-08:03:43-root-INFO: grad norm: 6.320 6.254 0.910
2024-12-02-08:03:43-root-INFO: grad norm: 5.624 5.561 0.834
2024-12-02-08:03:44-root-INFO: grad norm: 5.066 4.996 0.836
2024-12-02-08:03:44-root-INFO: grad norm: 4.658 4.590 0.793
2024-12-02-08:03:44-root-INFO: Loss Change: 191.111 -> 186.993
2024-12-02-08:03:44-root-INFO: Regularization Change: 0.000 -> 0.660
2024-12-02-08:03:44-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-08:03:44-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-08:03:45-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-08:03:45-root-INFO: grad norm: 8.468 8.311 1.623
2024-12-02-08:03:45-root-INFO: Loss too large (187.569->188.904)! Learning rate decreased to 0.04654.
2024-12-02-08:03:45-root-INFO: Loss too large (187.569->187.800)! Learning rate decreased to 0.03724.
2024-12-02-08:03:46-root-INFO: grad norm: 9.507 9.445 1.080
2024-12-02-08:03:46-root-INFO: grad norm: 11.899 11.827 1.302
2024-12-02-08:03:46-root-INFO: Loss too large (187.136->187.471)! Learning rate decreased to 0.02979.
2024-12-02-08:03:47-root-INFO: grad norm: 10.111 10.059 1.031
2024-12-02-08:03:47-root-INFO: grad norm: 8.629 8.569 1.014
2024-12-02-08:03:47-root-INFO: grad norm: 7.610 7.558 0.888
2024-12-02-08:03:48-root-INFO: grad norm: 6.742 6.682 0.894
2024-12-02-08:03:48-root-INFO: grad norm: 6.098 6.043 0.817
2024-12-02-08:03:49-root-INFO: Loss Change: 187.569 -> 184.370
2024-12-02-08:03:49-root-INFO: Regularization Change: 0.000 -> 0.720
2024-12-02-08:03:49-root-INFO: Undo step: 110
2024-12-02-08:03:49-root-INFO: Undo step: 111
2024-12-02-08:03:49-root-INFO: Undo step: 112
2024-12-02-08:03:49-root-INFO: Undo step: 113
2024-12-02-08:03:49-root-INFO: Undo step: 114
2024-12-02-08:03:49-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-08:03:49-root-INFO: grad norm: 81.045 80.106 12.304
2024-12-02-08:03:49-root-INFO: grad norm: 57.428 56.919 7.630
2024-12-02-08:03:50-root-INFO: grad norm: 68.558 68.127 7.676
2024-12-02-08:03:50-root-INFO: grad norm: 48.193 47.912 5.191
2024-12-02-08:03:51-root-INFO: grad norm: 49.848 49.613 4.835
2024-12-02-08:03:51-root-INFO: grad norm: 50.965 50.736 4.827
2024-12-02-08:03:52-root-INFO: grad norm: 51.201 50.976 4.803
2024-12-02-08:03:52-root-INFO: grad norm: 51.717 51.474 5.009
2024-12-02-08:03:52-root-INFO: Loss too large (255.398->256.595)! Learning rate decreased to 0.04002.
2024-12-02-08:03:53-root-INFO: Loss Change: 456.541 -> 235.176
2024-12-02-08:03:53-root-INFO: Regularization Change: 0.000 -> 47.695
2024-12-02-08:03:53-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-08:03:53-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-08:03:53-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-08:03:53-root-INFO: grad norm: 36.338 36.170 3.490
2024-12-02-08:03:53-root-INFO: Loss too large (234.292->238.637)! Learning rate decreased to 0.04126.
2024-12-02-08:03:54-root-INFO: grad norm: 29.940 29.775 3.139
2024-12-02-08:03:54-root-INFO: grad norm: 27.200 27.042 2.928
2024-12-02-08:03:55-root-INFO: grad norm: 26.269 26.116 2.825
2024-12-02-08:03:55-root-INFO: grad norm: 26.121 25.958 2.914
2024-12-02-08:03:56-root-INFO: grad norm: 26.781 26.626 2.883
2024-12-02-08:03:56-root-INFO: grad norm: 27.599 27.422 3.124
2024-12-02-08:03:57-root-INFO: grad norm: 29.029 28.858 3.150
2024-12-02-08:03:57-root-INFO: Loss Change: 234.292 -> 213.884
2024-12-02-08:03:57-root-INFO: Regularization Change: 0.000 -> 4.866
2024-12-02-08:03:57-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-08:03:57-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-08:03:57-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-08:03:57-root-INFO: grad norm: 28.766 28.607 3.020
2024-12-02-08:03:57-root-INFO: Loss too large (212.603->222.344)! Learning rate decreased to 0.04253.
2024-12-02-08:03:58-root-INFO: grad norm: 30.000 29.839 3.106
2024-12-02-08:03:58-root-INFO: grad norm: 30.699 30.510 3.398
2024-12-02-08:03:59-root-INFO: grad norm: 31.768 31.574 3.506
2024-12-02-08:03:59-root-INFO: Loss too large (210.872->211.488)! Learning rate decreased to 0.03403.
2024-12-02-08:03:59-root-INFO: grad norm: 21.666 21.498 2.691
2024-12-02-08:04:00-root-INFO: grad norm: 15.625 15.497 1.994
2024-12-02-08:04:00-root-INFO: grad norm: 12.466 12.340 1.770
2024-12-02-08:04:01-root-INFO: grad norm: 10.335 10.229 1.473
2024-12-02-08:04:01-root-INFO: Loss Change: 212.603 -> 197.264
2024-12-02-08:04:01-root-INFO: Regularization Change: 0.000 -> 2.345
2024-12-02-08:04:01-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-08:04:01-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-08:04:01-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-08:04:01-root-INFO: grad norm: 7.897 7.691 1.795
2024-12-02-08:04:02-root-INFO: grad norm: 9.318 9.221 1.336
2024-12-02-08:04:02-root-INFO: Loss too large (195.639->195.815)! Learning rate decreased to 0.04384.
2024-12-02-08:04:03-root-INFO: grad norm: 10.596 10.514 1.313
2024-12-02-08:04:03-root-INFO: grad norm: 13.267 13.188 1.444
2024-12-02-08:04:03-root-INFO: Loss too large (194.760->195.139)! Learning rate decreased to 0.03507.
2024-12-02-08:04:04-root-INFO: grad norm: 11.648 11.568 1.365
2024-12-02-08:04:04-root-INFO: grad norm: 10.543 10.464 1.288
2024-12-02-08:04:05-root-INFO: grad norm: 9.801 9.721 1.254
2024-12-02-08:04:05-root-INFO: grad norm: 9.253 9.175 1.205
2024-12-02-08:04:05-root-INFO: Loss Change: 196.696 -> 190.948
2024-12-02-08:04:05-root-INFO: Regularization Change: 0.000 -> 1.854
2024-12-02-08:04:05-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-08:04:05-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-08:04:05-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-08:04:06-root-INFO: grad norm: 7.867 7.757 1.308
2024-12-02-08:04:06-root-INFO: Loss too large (190.618->190.693)! Learning rate decreased to 0.04517.
2024-12-02-08:04:06-root-INFO: grad norm: 9.597 9.527 1.157
2024-12-02-08:04:06-root-INFO: Loss too large (190.101->190.133)! Learning rate decreased to 0.03614.
2024-12-02-08:04:07-root-INFO: grad norm: 9.010 8.941 1.119
2024-12-02-08:04:07-root-INFO: grad norm: 8.663 8.592 1.106
2024-12-02-08:04:08-root-INFO: grad norm: 8.456 8.385 1.090
2024-12-02-08:04:08-root-INFO: grad norm: 8.346 8.275 1.084
2024-12-02-08:04:09-root-INFO: grad norm: 8.309 8.238 1.082
2024-12-02-08:04:09-root-INFO: grad norm: 8.336 8.266 1.078
2024-12-02-08:04:10-root-INFO: Loss Change: 190.618 -> 186.376
2024-12-02-08:04:10-root-INFO: Regularization Change: 0.000 -> 1.284
2024-12-02-08:04:10-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-08:04:10-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-08:04:10-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-08:04:10-root-INFO: grad norm: 7.396 7.281 1.303
2024-12-02-08:04:10-root-INFO: Loss too large (186.264->186.295)! Learning rate decreased to 0.04654.
2024-12-02-08:04:11-root-INFO: grad norm: 9.006 8.943 1.060
2024-12-02-08:04:11-root-INFO: Loss too large (185.772->185.894)! Learning rate decreased to 0.03724.
2024-12-02-08:04:11-root-INFO: grad norm: 8.779 8.720 1.017
2024-12-02-08:04:12-root-INFO: grad norm: 8.827 8.765 1.040
2024-12-02-08:04:12-root-INFO: grad norm: 8.969 8.910 1.025
2024-12-02-08:04:13-root-INFO: grad norm: 9.226 9.165 1.064
2024-12-02-08:04:13-root-INFO: grad norm: 9.514 9.455 1.066
2024-12-02-08:04:13-root-INFO: grad norm: 9.891 9.828 1.109
2024-12-02-08:04:14-root-INFO: Loss Change: 186.264 -> 182.874
2024-12-02-08:04:14-root-INFO: Regularization Change: 0.000 -> 1.189
2024-12-02-08:04:14-root-INFO: Undo step: 110
2024-12-02-08:04:14-root-INFO: Undo step: 111
2024-12-02-08:04:14-root-INFO: Undo step: 112
2024-12-02-08:04:14-root-INFO: Undo step: 113
2024-12-02-08:04:14-root-INFO: Undo step: 114
2024-12-02-08:04:14-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-08:04:14-root-INFO: grad norm: 84.125 82.837 14.667
2024-12-02-08:04:15-root-INFO: grad norm: 64.091 63.414 9.289
2024-12-02-08:04:15-root-INFO: grad norm: 57.638 56.782 9.896
2024-12-02-08:04:15-root-INFO: grad norm: 52.473 51.800 8.383
2024-12-02-08:04:16-root-INFO: Loss too large (269.645->271.199)! Learning rate decreased to 0.04002.
2024-12-02-08:04:16-root-INFO: grad norm: 39.314 38.600 7.460
2024-12-02-08:04:17-root-INFO: grad norm: 31.531 31.170 4.760
2024-12-02-08:04:17-root-INFO: grad norm: 29.151 28.707 5.068
2024-12-02-08:04:17-root-INFO: grad norm: 28.574 28.298 3.966
2024-12-02-08:04:18-root-INFO: Loss Change: 436.223 -> 224.516
2024-12-02-08:04:18-root-INFO: Regularization Change: 0.000 -> 39.729
2024-12-02-08:04:18-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-08:04:18-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-08:04:18-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-08:04:18-root-INFO: grad norm: 26.903 26.678 3.468
2024-12-02-08:04:18-root-INFO: Loss too large (222.950->229.008)! Learning rate decreased to 0.04126.
2024-12-02-08:04:19-root-INFO: grad norm: 28.321 28.112 3.432
2024-12-02-08:04:19-root-INFO: grad norm: 29.054 28.813 3.731
2024-12-02-08:04:20-root-INFO: grad norm: 29.597 29.390 3.488
2024-12-02-08:04:20-root-INFO: grad norm: 29.549 29.309 3.759
2024-12-02-08:04:21-root-INFO: grad norm: 29.309 29.096 3.527
2024-12-02-08:04:21-root-INFO: grad norm: 29.097 28.852 3.770
2024-12-02-08:04:21-root-INFO: grad norm: 28.976 28.751 3.599
2024-12-02-08:04:22-root-INFO: Loss Change: 222.950 -> 210.271
2024-12-02-08:04:22-root-INFO: Regularization Change: 0.000 -> 4.982
2024-12-02-08:04:22-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-08:04:22-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-08:04:22-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-08:04:22-root-INFO: grad norm: 26.986 26.814 3.045
2024-12-02-08:04:22-root-INFO: Loss too large (208.646->215.713)! Learning rate decreased to 0.04253.
2024-12-02-08:04:23-root-INFO: grad norm: 27.167 26.974 3.231
2024-12-02-08:04:23-root-INFO: grad norm: 27.483 27.270 3.417
2024-12-02-08:04:24-root-INFO: grad norm: 28.152 27.931 3.519
2024-12-02-08:04:24-root-INFO: Loss too large (204.999->205.158)! Learning rate decreased to 0.03403.
2024-12-02-08:04:24-root-INFO: grad norm: 18.860 18.679 2.604
2024-12-02-08:04:25-root-INFO: grad norm: 12.907 12.764 1.915
2024-12-02-08:04:25-root-INFO: grad norm: 9.784 9.645 1.645
2024-12-02-08:04:26-root-INFO: grad norm: 7.773 7.644 1.410
2024-12-02-08:04:26-root-INFO: Loss Change: 208.646 -> 193.273
2024-12-02-08:04:26-root-INFO: Regularization Change: 0.000 -> 2.512
2024-12-02-08:04:26-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-08:04:26-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-08:04:26-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-08:04:26-root-INFO: grad norm: 7.936 7.645 2.128
2024-12-02-08:04:27-root-INFO: grad norm: 8.895 8.726 1.724
2024-12-02-08:04:27-root-INFO: grad norm: 13.316 13.146 2.120
2024-12-02-08:04:27-root-INFO: Loss too large (191.766->193.973)! Learning rate decreased to 0.04384.
2024-12-02-08:04:28-root-INFO: grad norm: 15.848 15.688 2.243
2024-12-02-08:04:28-root-INFO: Loss too large (191.636->192.055)! Learning rate decreased to 0.03507.
2024-12-02-08:04:29-root-INFO: grad norm: 12.856 12.730 1.798
2024-12-02-08:04:29-root-INFO: grad norm: 10.688 10.571 1.578
2024-12-02-08:04:30-root-INFO: grad norm: 9.083 8.978 1.377
2024-12-02-08:04:30-root-INFO: grad norm: 7.869 7.762 1.297
2024-12-02-08:04:30-root-INFO: Loss Change: 193.133 -> 186.623
2024-12-02-08:04:30-root-INFO: Regularization Change: 0.000 -> 2.078
2024-12-02-08:04:30-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-08:04:30-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-08:04:31-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-08:04:31-root-INFO: grad norm: 10.005 9.805 1.992
2024-12-02-08:04:31-root-INFO: Loss too large (186.977->187.769)! Learning rate decreased to 0.04517.
2024-12-02-08:04:31-root-INFO: grad norm: 11.933 11.800 1.778
2024-12-02-08:04:31-root-INFO: Loss too large (186.584->186.681)! Learning rate decreased to 0.03614.
2024-12-02-08:04:32-root-INFO: grad norm: 10.127 10.004 1.573
2024-12-02-08:04:32-root-INFO: grad norm: 8.752 8.643 1.379
2024-12-02-08:04:33-root-INFO: grad norm: 7.676 7.573 1.257
2024-12-02-08:04:33-root-INFO: grad norm: 6.836 6.733 1.179
2024-12-02-08:04:34-root-INFO: grad norm: 6.179 6.082 1.091
2024-12-02-08:04:34-root-INFO: grad norm: 5.664 5.564 1.062
2024-12-02-08:04:35-root-INFO: Loss Change: 186.977 -> 181.791
2024-12-02-08:04:35-root-INFO: Regularization Change: 0.000 -> 1.344
2024-12-02-08:04:35-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-08:04:35-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-08:04:35-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-08:04:35-root-INFO: grad norm: 8.900 8.689 1.924
2024-12-02-08:04:35-root-INFO: Loss too large (182.300->182.800)! Learning rate decreased to 0.04654.
2024-12-02-08:04:35-root-INFO: grad norm: 10.544 10.413 1.658
2024-12-02-08:04:36-root-INFO: Loss too large (181.892->181.976)! Learning rate decreased to 0.03724.
2024-12-02-08:04:36-root-INFO: grad norm: 9.207 9.083 1.503
2024-12-02-08:04:37-root-INFO: grad norm: 8.206 8.101 1.304
2024-12-02-08:04:37-root-INFO: grad norm: 7.370 7.268 1.224
2024-12-02-08:04:37-root-INFO: grad norm: 6.690 6.594 1.131
2024-12-02-08:04:38-root-INFO: grad norm: 6.123 6.029 1.068
2024-12-02-08:04:38-root-INFO: grad norm: 5.658 5.566 1.019
2024-12-02-08:04:39-root-INFO: Loss Change: 182.300 -> 177.849
2024-12-02-08:04:39-root-INFO: Regularization Change: 0.000 -> 1.210
2024-12-02-08:04:39-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-08:04:39-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-08:04:39-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-08:04:39-root-INFO: grad norm: 10.289 9.999 2.427
2024-12-02-08:04:39-root-INFO: Loss too large (178.826->180.138)! Learning rate decreased to 0.04795.
2024-12-02-08:04:40-root-INFO: grad norm: 12.531 12.378 1.949
2024-12-02-08:04:40-root-INFO: Loss too large (178.684->179.247)! Learning rate decreased to 0.03836.
2024-12-02-08:04:40-root-INFO: grad norm: 11.052 10.911 1.762
2024-12-02-08:04:41-root-INFO: grad norm: 9.962 9.852 1.475
2024-12-02-08:04:41-root-INFO: grad norm: 9.009 8.897 1.414
2024-12-02-08:04:42-root-INFO: grad norm: 8.236 8.139 1.256
2024-12-02-08:04:42-root-INFO: grad norm: 7.563 7.465 1.215
2024-12-02-08:04:43-root-INFO: grad norm: 7.018 6.929 1.112
2024-12-02-08:04:43-root-INFO: Loss Change: 178.826 -> 174.583
2024-12-02-08:04:43-root-INFO: Regularization Change: 0.000 -> 1.135
2024-12-02-08:04:43-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-08:04:43-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-08:04:43-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-08:04:43-root-INFO: grad norm: 10.932 10.716 2.166
2024-12-02-08:04:43-root-INFO: Loss too large (175.163->177.468)! Learning rate decreased to 0.04939.
2024-12-02-08:04:44-root-INFO: Loss too large (175.163->175.514)! Learning rate decreased to 0.03951.
2024-12-02-08:04:44-root-INFO: grad norm: 9.804 9.704 1.396
2024-12-02-08:04:44-root-INFO: grad norm: 9.054 8.936 1.461
2024-12-02-08:04:45-root-INFO: grad norm: 8.494 8.407 1.209
2024-12-02-08:04:45-root-INFO: grad norm: 8.031 7.933 1.247
2024-12-02-08:04:46-root-INFO: grad norm: 7.688 7.609 1.099
2024-12-02-08:04:46-root-INFO: grad norm: 7.430 7.344 1.127
2024-12-02-08:04:47-root-INFO: grad norm: 7.267 7.194 1.024
2024-12-02-08:04:47-root-INFO: Loss Change: 175.163 -> 171.304
2024-12-02-08:04:47-root-INFO: Regularization Change: 0.000 -> 1.003
2024-12-02-08:04:47-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-08:04:47-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-08:04:47-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-08:04:47-root-INFO: grad norm: 14.402 14.115 2.865
2024-12-02-08:04:47-root-INFO: Loss too large (173.052->178.010)! Learning rate decreased to 0.05086.
2024-12-02-08:04:48-root-INFO: Loss too large (173.052->174.334)! Learning rate decreased to 0.04069.
2024-12-02-08:04:48-root-INFO: grad norm: 13.374 13.261 1.733
2024-12-02-08:04:49-root-INFO: grad norm: 13.107 12.989 1.756
2024-12-02-08:04:49-root-INFO: grad norm: 13.029 12.936 1.551
2024-12-02-08:04:50-root-INFO: grad norm: 13.030 12.933 1.582
2024-12-02-08:04:50-root-INFO: grad norm: 13.099 13.015 1.487
2024-12-02-08:04:51-root-INFO: grad norm: 13.227 13.140 1.512
2024-12-02-08:04:51-root-INFO: grad norm: 13.373 13.294 1.454
2024-12-02-08:04:52-root-INFO: Loss Change: 173.052 -> 169.652
2024-12-02-08:04:52-root-INFO: Regularization Change: 0.000 -> 1.020
2024-12-02-08:04:52-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-08:04:52-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-08:04:52-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-08:04:52-root-INFO: grad norm: 18.222 17.988 2.913
2024-12-02-08:04:52-root-INFO: Loss too large (171.039->180.901)! Learning rate decreased to 0.05237.
2024-12-02-08:04:52-root-INFO: Loss too large (171.039->174.393)! Learning rate decreased to 0.04190.
2024-12-02-08:04:53-root-INFO: grad norm: 17.602 17.482 2.049
2024-12-02-08:04:53-root-INFO: grad norm: 17.118 16.981 2.161
2024-12-02-08:04:54-root-INFO: grad norm: 16.705 16.596 1.901
2024-12-02-08:04:54-root-INFO: grad norm: 16.244 16.126 1.954
2024-12-02-08:04:55-root-INFO: grad norm: 15.863 15.761 1.793
2024-12-02-08:04:55-root-INFO: grad norm: 15.465 15.358 1.820
2024-12-02-08:04:56-root-INFO: grad norm: 15.145 15.049 1.702
2024-12-02-08:04:56-root-INFO: Loss Change: 171.039 -> 166.984
2024-12-02-08:04:56-root-INFO: Regularization Change: 0.000 -> 0.987
2024-12-02-08:04:56-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-08:04:56-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-08:04:56-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-08:04:56-root-INFO: grad norm: 19.393 19.161 2.987
2024-12-02-08:04:56-root-INFO: Loss too large (168.949->180.568)! Learning rate decreased to 0.05391.
2024-12-02-08:04:57-root-INFO: Loss too large (168.949->172.988)! Learning rate decreased to 0.04313.
2024-12-02-08:04:57-root-INFO: grad norm: 18.618 18.490 2.185
2024-12-02-08:04:57-root-INFO: grad norm: 17.865 17.728 2.215
2024-12-02-08:04:58-root-INFO: grad norm: 17.248 17.134 1.984
2024-12-02-08:04:58-root-INFO: grad norm: 16.596 16.477 1.984
2024-12-02-08:04:59-root-INFO: grad norm: 16.077 15.971 1.842
2024-12-02-08:04:59-root-INFO: grad norm: 15.560 15.451 1.835
2024-12-02-08:05:00-root-INFO: grad norm: 15.154 15.055 1.730
2024-12-02-08:05:00-root-INFO: Loss Change: 168.949 -> 164.549
2024-12-02-08:05:00-root-INFO: Regularization Change: 0.000 -> 0.960
2024-12-02-08:05:00-root-INFO: Undo step: 105
2024-12-02-08:05:00-root-INFO: Undo step: 106
2024-12-02-08:05:00-root-INFO: Undo step: 107
2024-12-02-08:05:00-root-INFO: Undo step: 108
2024-12-02-08:05:00-root-INFO: Undo step: 109
2024-12-02-08:05:00-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-08:05:01-root-INFO: grad norm: 88.333 87.414 12.709
2024-12-02-08:05:01-root-INFO: grad norm: 67.208 66.610 8.946
2024-12-02-08:05:01-root-INFO: grad norm: 54.749 54.282 7.138
2024-12-02-08:05:02-root-INFO: grad norm: 56.911 56.386 7.708
2024-12-02-08:05:02-root-INFO: Loss too large (261.894->278.209)! Learning rate decreased to 0.04654.
2024-12-02-08:05:03-root-INFO: grad norm: 48.497 48.076 6.380
2024-12-02-08:05:03-root-INFO: grad norm: 41.114 40.790 5.149
2024-12-02-08:05:04-root-INFO: grad norm: 36.906 36.601 4.734
2024-12-02-08:05:04-root-INFO: grad norm: 35.276 34.995 4.441
2024-12-02-08:05:04-root-INFO: Loss Change: 432.993 -> 212.756
2024-12-02-08:05:04-root-INFO: Regularization Change: 0.000 -> 50.699
2024-12-02-08:05:04-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-08:05:04-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-08:05:05-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-08:05:05-root-INFO: grad norm: 32.370 32.175 3.544
2024-12-02-08:05:05-root-INFO: Loss too large (210.589->222.861)! Learning rate decreased to 0.04795.
2024-12-02-08:05:05-root-INFO: grad norm: 32.975 32.732 3.994
2024-12-02-08:05:06-root-INFO: grad norm: 33.781 33.542 4.010
2024-12-02-08:05:06-root-INFO: grad norm: 34.734 34.474 4.242
2024-12-02-08:05:07-root-INFO: grad norm: 35.265 35.000 4.319
2024-12-02-08:05:07-root-INFO: grad norm: 35.708 35.436 4.403
2024-12-02-08:05:08-root-INFO: grad norm: 35.792 35.511 4.476
2024-12-02-08:05:08-root-INFO: grad norm: 35.787 35.512 4.432
2024-12-02-08:05:09-root-INFO: Loss Change: 210.589 -> 199.663
2024-12-02-08:05:09-root-INFO: Regularization Change: 0.000 -> 5.707
2024-12-02-08:05:09-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-08:05:09-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-08:05:09-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-08:05:09-root-INFO: grad norm: 32.854 32.643 3.723
2024-12-02-08:05:09-root-INFO: Loss too large (197.443->211.374)! Learning rate decreased to 0.04939.
2024-12-02-08:05:09-root-INFO: grad norm: 32.716 32.463 4.062
2024-12-02-08:05:10-root-INFO: grad norm: 32.778 32.532 4.011
2024-12-02-08:05:10-root-INFO: grad norm: 32.865 32.612 4.067
2024-12-02-08:05:11-root-INFO: grad norm: 32.883 32.628 4.087
2024-12-02-08:05:11-root-INFO: grad norm: 32.849 32.598 4.052
2024-12-02-08:05:12-root-INFO: grad norm: 32.731 32.474 4.090
2024-12-02-08:05:12-root-INFO: grad norm: 32.560 32.313 4.007
2024-12-02-08:05:13-root-INFO: Loss Change: 197.443 -> 189.062
2024-12-02-08:05:13-root-INFO: Regularization Change: 0.000 -> 3.233
2024-12-02-08:05:13-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-08:05:13-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-08:05:13-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-08:05:13-root-INFO: grad norm: 29.601 29.436 3.116
2024-12-02-08:05:13-root-INFO: Loss too large (186.622->199.577)! Learning rate decreased to 0.05086.
2024-12-02-08:05:14-root-INFO: grad norm: 30.085 29.887 3.449
2024-12-02-08:05:14-root-INFO: grad norm: 30.634 30.438 3.466
2024-12-02-08:05:15-root-INFO: grad norm: 31.049 30.839 3.603
2024-12-02-08:05:15-root-INFO: grad norm: 31.183 30.968 3.654
2024-12-02-08:05:16-root-INFO: grad norm: 31.285 31.065 3.702
2024-12-02-08:05:16-root-INFO: grad norm: 31.172 30.944 3.764
2024-12-02-08:05:17-root-INFO: grad norm: 31.124 30.896 3.758
2024-12-02-08:05:17-root-INFO: Loss Change: 186.622 -> 182.431
2024-12-02-08:05:17-root-INFO: Regularization Change: 0.000 -> 2.403
2024-12-02-08:05:17-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-08:05:17-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-08:05:17-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-08:05:17-root-INFO: grad norm: 28.293 28.136 2.976
2024-12-02-08:05:17-root-INFO: Loss too large (180.213->192.352)! Learning rate decreased to 0.05237.
2024-12-02-08:05:18-root-INFO: grad norm: 28.775 28.559 3.517
2024-12-02-08:05:18-root-INFO: Loss too large (179.643->179.657)! Learning rate decreased to 0.04190.
2024-12-02-08:05:18-root-INFO: grad norm: 18.514 18.368 2.318
2024-12-02-08:05:19-root-INFO: grad norm: 12.582 12.466 1.701
2024-12-02-08:05:19-root-INFO: grad norm: 9.171 9.077 1.308
2024-12-02-08:05:20-root-INFO: grad norm: 7.030 6.946 1.081
2024-12-02-08:05:20-root-INFO: grad norm: 5.678 5.599 0.947
2024-12-02-08:05:21-root-INFO: grad norm: 4.797 4.720 0.854
2024-12-02-08:05:21-root-INFO: Loss Change: 180.213 -> 165.409
2024-12-02-08:05:21-root-INFO: Regularization Change: 0.000 -> 1.916
2024-12-02-08:05:21-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-08:05:21-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-08:05:21-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-08:05:21-root-INFO: grad norm: 5.281 5.123 1.282
2024-12-02-08:05:22-root-INFO: grad norm: 6.339 6.244 1.093
2024-12-02-08:05:22-root-INFO: Loss too large (165.024->165.124)! Learning rate decreased to 0.05391.
2024-12-02-08:05:23-root-INFO: grad norm: 7.723 7.636 1.156
2024-12-02-08:05:23-root-INFO: grad norm: 10.233 10.152 1.282
2024-12-02-08:05:23-root-INFO: Loss too large (164.639->165.144)! Learning rate decreased to 0.04313.
2024-12-02-08:05:24-root-INFO: grad norm: 9.601 9.527 1.190
2024-12-02-08:05:24-root-INFO: grad norm: 9.128 9.063 1.086
2024-12-02-08:05:25-root-INFO: grad norm: 8.724 8.658 1.071
2024-12-02-08:05:25-root-INFO: grad norm: 8.400 8.341 0.997
2024-12-02-08:05:25-root-INFO: Loss Change: 165.709 -> 162.175
2024-12-02-08:05:25-root-INFO: Regularization Change: 0.000 -> 1.538
2024-12-02-08:05:25-root-INFO: Undo step: 105
2024-12-02-08:05:25-root-INFO: Undo step: 106
2024-12-02-08:05:25-root-INFO: Undo step: 107
2024-12-02-08:05:25-root-INFO: Undo step: 108
2024-12-02-08:05:25-root-INFO: Undo step: 109
2024-12-02-08:05:25-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-08:05:26-root-INFO: grad norm: 75.708 74.414 13.937
2024-12-02-08:05:26-root-INFO: grad norm: 45.557 45.097 6.458
2024-12-02-08:05:27-root-INFO: grad norm: 50.793 50.437 6.004
2024-12-02-08:05:27-root-INFO: Loss too large (261.087->262.675)! Learning rate decreased to 0.04654.
2024-12-02-08:05:27-root-INFO: grad norm: 38.994 38.676 4.964
2024-12-02-08:05:28-root-INFO: grad norm: 28.944 28.604 4.423
2024-12-02-08:05:28-root-INFO: grad norm: 24.599 24.333 3.604
2024-12-02-08:05:29-root-INFO: grad norm: 22.998 22.696 3.715
2024-12-02-08:05:29-root-INFO: grad norm: 23.058 22.826 3.259
2024-12-02-08:05:29-root-INFO: Loss Change: 407.295 -> 200.442
2024-12-02-08:05:29-root-INFO: Regularization Change: 0.000 -> 45.679
2024-12-02-08:05:29-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-08:05:29-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-08:05:30-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-08:05:30-root-INFO: grad norm: 21.041 20.844 2.872
2024-12-02-08:05:30-root-INFO: Loss too large (198.577->201.940)! Learning rate decreased to 0.04795.
2024-12-02-08:05:30-root-INFO: grad norm: 22.340 22.148 2.923
2024-12-02-08:05:31-root-INFO: grad norm: 24.516 24.286 3.351
2024-12-02-08:05:31-root-INFO: grad norm: 27.350 27.127 3.486
2024-12-02-08:05:32-root-INFO: Loss too large (195.118->195.363)! Learning rate decreased to 0.03836.
2024-12-02-08:05:32-root-INFO: grad norm: 20.009 19.781 3.013
2024-12-02-08:05:32-root-INFO: grad norm: 15.848 15.689 2.239
2024-12-02-08:05:33-root-INFO: grad norm: 13.478 13.302 2.175
2024-12-02-08:05:33-root-INFO: grad norm: 12.081 11.945 1.810
2024-12-02-08:05:34-root-INFO: Loss Change: 198.577 -> 181.558
2024-12-02-08:05:34-root-INFO: Regularization Change: 0.000 -> 4.634
2024-12-02-08:05:34-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-08:05:34-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-08:05:34-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-08:05:34-root-INFO: grad norm: 9.070 8.949 1.473
2024-12-02-08:05:34-root-INFO: grad norm: 15.503 15.400 1.791
2024-12-02-08:05:35-root-INFO: Loss too large (180.638->185.824)! Learning rate decreased to 0.04939.
2024-12-02-08:05:35-root-INFO: Loss too large (180.638->181.827)! Learning rate decreased to 0.03951.
2024-12-02-08:05:35-root-INFO: grad norm: 14.729 14.624 1.755
2024-12-02-08:05:36-root-INFO: grad norm: 14.047 13.942 1.719
2024-12-02-08:05:36-root-INFO: grad norm: 13.487 13.383 1.673
2024-12-02-08:05:37-root-INFO: grad norm: 12.953 12.854 1.604
2024-12-02-08:05:37-root-INFO: grad norm: 12.534 12.434 1.575
2024-12-02-08:05:38-root-INFO: grad norm: 12.154 12.060 1.511
2024-12-02-08:05:38-root-INFO: Loss Change: 180.745 -> 174.150
2024-12-02-08:05:38-root-INFO: Regularization Change: 0.000 -> 2.471
2024-12-02-08:05:38-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-08:05:38-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-08:05:38-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-08:05:38-root-INFO: grad norm: 10.126 9.975 1.744
2024-12-02-08:05:39-root-INFO: Loss too large (173.682->174.352)! Learning rate decreased to 0.05086.
2024-12-02-08:05:39-root-INFO: grad norm: 12.143 12.063 1.389
2024-12-02-08:05:39-root-INFO: Loss too large (173.082->173.600)! Learning rate decreased to 0.04069.
2024-12-02-08:05:40-root-INFO: grad norm: 11.413 11.347 1.227
2024-12-02-08:05:40-root-INFO: grad norm: 11.021 10.949 1.261
2024-12-02-08:05:41-root-INFO: grad norm: 10.859 10.790 1.218
2024-12-02-08:05:41-root-INFO: grad norm: 10.777 10.705 1.245
2024-12-02-08:05:41-root-INFO: grad norm: 10.796 10.725 1.233
2024-12-02-08:05:42-root-INFO: grad norm: 10.875 10.803 1.252
2024-12-02-08:05:42-root-INFO: Loss Change: 173.682 -> 168.690
2024-12-02-08:05:42-root-INFO: Regularization Change: 0.000 -> 1.734
2024-12-02-08:05:42-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-08:05:42-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-08:05:42-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-08:05:43-root-INFO: grad norm: 9.925 9.810 1.506
2024-12-02-08:05:43-root-INFO: Loss too large (168.281->169.651)! Learning rate decreased to 0.05237.
2024-12-02-08:05:43-root-INFO: grad norm: 12.801 12.738 1.267
2024-12-02-08:05:43-root-INFO: Loss too large (168.117->169.093)! Learning rate decreased to 0.04190.
2024-12-02-08:05:44-root-INFO: grad norm: 11.984 11.929 1.151
2024-12-02-08:05:44-root-INFO: grad norm: 11.370 11.311 1.163
2024-12-02-08:05:45-root-INFO: grad norm: 10.991 10.935 1.103
2024-12-02-08:05:45-root-INFO: grad norm: 10.736 10.675 1.144
2024-12-02-08:05:46-root-INFO: grad norm: 10.628 10.569 1.116
2024-12-02-08:05:46-root-INFO: grad norm: 10.623 10.559 1.164
2024-12-02-08:05:46-root-INFO: Loss Change: 168.281 -> 164.216
2024-12-02-08:05:46-root-INFO: Regularization Change: 0.000 -> 1.454
2024-12-02-08:05:46-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-08:05:46-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-08:05:47-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-08:05:47-root-INFO: grad norm: 9.405 9.312 1.315
2024-12-02-08:05:47-root-INFO: Loss too large (164.140->165.701)! Learning rate decreased to 0.05391.
2024-12-02-08:05:47-root-INFO: Loss too large (164.140->164.189)! Learning rate decreased to 0.04313.
2024-12-02-08:05:48-root-INFO: grad norm: 8.675 8.618 0.991
2024-12-02-08:05:48-root-INFO: grad norm: 8.529 8.479 0.924
2024-12-02-08:05:49-root-INFO: grad norm: 8.569 8.514 0.968
2024-12-02-08:05:49-root-INFO: grad norm: 8.710 8.659 0.934
2024-12-02-08:05:49-root-INFO: grad norm: 8.937 8.881 0.997
2024-12-02-08:05:50-root-INFO: grad norm: 9.229 9.176 0.984
2024-12-02-08:05:50-root-INFO: grad norm: 9.598 9.540 1.056
2024-12-02-08:05:51-root-INFO: Loss Change: 164.140 -> 160.745
2024-12-02-08:05:51-root-INFO: Regularization Change: 0.000 -> 1.202
2024-12-02-08:05:51-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-08:05:51-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-08:05:51-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-08:05:51-root-INFO: grad norm: 8.992 8.845 1.621
2024-12-02-08:05:51-root-INFO: Loss too large (160.508->161.677)! Learning rate decreased to 0.05550.
2024-12-02-08:05:52-root-INFO: grad norm: 11.167 11.108 1.145
2024-12-02-08:05:52-root-INFO: Loss too large (160.379->161.105)! Learning rate decreased to 0.04440.
2024-12-02-08:05:52-root-INFO: grad norm: 10.348 10.300 1.000
2024-12-02-08:05:53-root-INFO: grad norm: 9.899 9.853 0.956
2024-12-02-08:05:53-root-INFO: grad norm: 9.647 9.605 0.906
2024-12-02-08:05:54-root-INFO: grad norm: 9.489 9.443 0.938
2024-12-02-08:05:54-root-INFO: grad norm: 9.452 9.408 0.904
2024-12-02-08:05:55-root-INFO: grad norm: 9.505 9.456 0.965
2024-12-02-08:05:55-root-INFO: Loss Change: 160.508 -> 157.349
2024-12-02-08:05:55-root-INFO: Regularization Change: 0.000 -> 1.195
2024-12-02-08:05:55-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-08:05:55-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-08:05:55-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-08:05:55-root-INFO: grad norm: 9.359 9.278 1.233
2024-12-02-08:05:55-root-INFO: Loss too large (157.378->159.329)! Learning rate decreased to 0.05711.
2024-12-02-08:05:56-root-INFO: Loss too large (157.378->157.622)! Learning rate decreased to 0.04569.
2024-12-02-08:05:56-root-INFO: grad norm: 8.559 8.511 0.905
2024-12-02-08:05:57-root-INFO: grad norm: 8.207 8.160 0.874
2024-12-02-08:05:57-root-INFO: grad norm: 7.980 7.936 0.837
2024-12-02-08:05:58-root-INFO: grad norm: 7.867 7.825 0.811
2024-12-02-08:05:58-root-INFO: grad norm: 7.837 7.793 0.825
2024-12-02-08:05:59-root-INFO: grad norm: 7.896 7.854 0.807
2024-12-02-08:05:59-root-INFO: grad norm: 8.040 7.995 0.847
2024-12-02-08:05:59-root-INFO: Loss Change: 157.378 -> 154.211
2024-12-02-08:05:59-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-02-08:05:59-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-08:05:59-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-08:06:00-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-08:06:00-root-INFO: grad norm: 7.676 7.567 1.291
2024-12-02-08:06:00-root-INFO: Loss too large (153.971->154.956)! Learning rate decreased to 0.05877.
2024-12-02-08:06:00-root-INFO: grad norm: 9.950 9.900 0.995
2024-12-02-08:06:01-root-INFO: Loss too large (153.928->154.623)! Learning rate decreased to 0.04702.
2024-12-02-08:06:01-root-INFO: grad norm: 9.468 9.423 0.923
2024-12-02-08:06:01-root-INFO: grad norm: 9.238 9.196 0.872
2024-12-02-08:06:02-root-INFO: grad norm: 9.107 9.068 0.842
2024-12-02-08:06:02-root-INFO: grad norm: 9.051 9.010 0.853
2024-12-02-08:06:03-root-INFO: grad norm: 9.092 9.053 0.835
2024-12-02-08:06:03-root-INFO: grad norm: 9.223 9.181 0.881
2024-12-02-08:06:04-root-INFO: Loss Change: 153.971 -> 151.493
2024-12-02-08:06:04-root-INFO: Regularization Change: 0.000 -> 1.095
2024-12-02-08:06:04-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-08:06:04-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-08:06:04-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-08:06:04-root-INFO: grad norm: 8.871 8.781 1.262
2024-12-02-08:06:04-root-INFO: Loss too large (151.430->153.383)! Learning rate decreased to 0.06047.
2024-12-02-08:06:04-root-INFO: Loss too large (151.430->151.760)! Learning rate decreased to 0.04837.
2024-12-02-08:06:05-root-INFO: grad norm: 8.185 8.142 0.843
2024-12-02-08:06:05-root-INFO: grad norm: 7.840 7.796 0.831
2024-12-02-08:06:06-root-INFO: grad norm: 7.706 7.666 0.785
2024-12-02-08:06:06-root-INFO: grad norm: 7.678 7.640 0.767
2024-12-02-08:06:07-root-INFO: grad norm: 7.748 7.708 0.790
2024-12-02-08:06:07-root-INFO: grad norm: 7.921 7.883 0.776
2024-12-02-08:06:08-root-INFO: grad norm: 8.199 8.156 0.839
2024-12-02-08:06:08-root-INFO: Loss Change: 151.430 -> 148.756
2024-12-02-08:06:08-root-INFO: Regularization Change: 0.000 -> 0.999
2024-12-02-08:06:08-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-08:06:08-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-08:06:08-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-08:06:08-root-INFO: grad norm: 8.103 8.005 1.255
2024-12-02-08:06:08-root-INFO: Loss too large (148.817->150.299)! Learning rate decreased to 0.06220.
2024-12-02-08:06:09-root-INFO: Loss too large (148.817->148.986)! Learning rate decreased to 0.04976.
2024-12-02-08:06:09-root-INFO: grad norm: 7.290 7.247 0.792
2024-12-02-08:06:10-root-INFO: grad norm: 6.856 6.811 0.781
2024-12-02-08:06:10-root-INFO: grad norm: 6.662 6.622 0.726
2024-12-02-08:06:10-root-INFO: grad norm: 6.562 6.525 0.701
2024-12-02-08:06:11-root-INFO: grad norm: 6.552 6.513 0.716
2024-12-02-08:06:11-root-INFO: grad norm: 6.630 6.594 0.689
2024-12-02-08:06:12-root-INFO: grad norm: 6.801 6.761 0.740
2024-12-02-08:06:12-root-INFO: Loss Change: 148.817 -> 146.121
2024-12-02-08:06:12-root-INFO: Regularization Change: 0.000 -> 0.985
2024-12-02-08:06:12-root-INFO: Undo step: 100
2024-12-02-08:06:12-root-INFO: Undo step: 101
2024-12-02-08:06:12-root-INFO: Undo step: 102
2024-12-02-08:06:12-root-INFO: Undo step: 103
2024-12-02-08:06:12-root-INFO: Undo step: 104
2024-12-02-08:06:12-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-08:06:13-root-INFO: grad norm: 67.026 66.329 9.646
2024-12-02-08:06:13-root-INFO: grad norm: 44.794 44.296 6.660
2024-12-02-08:06:14-root-INFO: grad norm: 39.999 39.537 6.065
2024-12-02-08:06:14-root-INFO: grad norm: 44.300 43.861 6.218
2024-12-02-08:06:14-root-INFO: Loss too large (223.366->234.323)! Learning rate decreased to 0.05391.
2024-12-02-08:06:15-root-INFO: grad norm: 37.513 37.040 5.933
2024-12-02-08:06:15-root-INFO: grad norm: 32.244 31.949 4.355
2024-12-02-08:06:16-root-INFO: grad norm: 31.072 30.722 4.651
2024-12-02-08:06:16-root-INFO: grad norm: 30.304 30.043 3.968
2024-12-02-08:06:16-root-INFO: Loss Change: 413.879 -> 188.242
2024-12-02-08:06:16-root-INFO: Regularization Change: 0.000 -> 58.987
2024-12-02-08:06:16-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-08:06:16-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-08:06:17-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-08:06:17-root-INFO: grad norm: 27.338 27.110 3.521
2024-12-02-08:06:17-root-INFO: Loss too large (185.295->194.461)! Learning rate decreased to 0.05550.
2024-12-02-08:06:17-root-INFO: grad norm: 27.745 27.519 3.533
2024-12-02-08:06:18-root-INFO: grad norm: 28.247 27.985 3.838
2024-12-02-08:06:18-root-INFO: grad norm: 28.848 28.621 3.614
2024-12-02-08:06:19-root-INFO: grad norm: 29.213 28.945 3.946
2024-12-02-08:06:19-root-INFO: grad norm: 29.633 29.402 3.695
2024-12-02-08:06:20-root-INFO: grad norm: 29.711 29.439 4.007
2024-12-02-08:06:20-root-INFO: grad norm: 29.715 29.480 3.727
2024-12-02-08:06:20-root-INFO: Loss Change: 185.295 -> 175.246
2024-12-02-08:06:20-root-INFO: Regularization Change: 0.000 -> 5.755
2024-12-02-08:06:20-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-08:06:20-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-08:06:21-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-08:06:21-root-INFO: grad norm: 27.233 27.018 3.412
2024-12-02-08:06:21-root-INFO: Loss too large (173.196->184.068)! Learning rate decreased to 0.05711.
2024-12-02-08:06:21-root-INFO: grad norm: 27.447 27.229 3.455
2024-12-02-08:06:22-root-INFO: grad norm: 27.430 27.174 3.734
2024-12-02-08:06:22-root-INFO: grad norm: 27.586 27.361 3.513
2024-12-02-08:06:22-root-INFO: Loss too large (169.685->170.029)! Learning rate decreased to 0.04569.
2024-12-02-08:06:23-root-INFO: grad norm: 18.170 17.975 2.654
2024-12-02-08:06:23-root-INFO: grad norm: 12.369 12.265 1.603
2024-12-02-08:06:24-root-INFO: grad norm: 9.336 9.216 1.495
2024-12-02-08:06:24-root-INFO: grad norm: 7.372 7.291 1.087
2024-12-02-08:06:25-root-INFO: Loss Change: 173.196 -> 156.120
2024-12-02-08:06:25-root-INFO: Regularization Change: 0.000 -> 2.943
2024-12-02-08:06:25-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-08:06:25-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-08:06:25-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-08:06:25-root-INFO: grad norm: 5.106 4.975 1.147
2024-12-02-08:06:25-root-INFO: grad norm: 5.559 5.489 0.884
2024-12-02-08:06:26-root-INFO: grad norm: 8.088 8.020 1.046
2024-12-02-08:06:26-root-INFO: Loss too large (154.218->155.033)! Learning rate decreased to 0.05877.
2024-12-02-08:06:26-root-INFO: grad norm: 9.958 9.890 1.161
2024-12-02-08:06:27-root-INFO: Loss too large (153.967->154.148)! Learning rate decreased to 0.04702.
2024-12-02-08:06:27-root-INFO: grad norm: 8.632 8.553 1.170
2024-12-02-08:06:28-root-INFO: grad norm: 7.629 7.562 1.008
2024-12-02-08:06:28-root-INFO: grad norm: 6.876 6.800 1.018
2024-12-02-08:06:29-root-INFO: grad norm: 6.268 6.204 0.892
2024-12-02-08:06:29-root-INFO: Loss Change: 155.666 -> 150.781
2024-12-02-08:06:29-root-INFO: Regularization Change: 0.000 -> 2.189
2024-12-02-08:06:29-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-08:06:29-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-08:06:29-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-08:06:29-root-INFO: grad norm: 4.760 4.639 1.068
2024-12-02-08:06:30-root-INFO: grad norm: 5.983 5.921 0.860
2024-12-02-08:06:30-root-INFO: Loss too large (150.032->150.127)! Learning rate decreased to 0.06047.
2024-12-02-08:06:30-root-INFO: grad norm: 7.043 6.984 0.909
2024-12-02-08:06:31-root-INFO: grad norm: 8.941 8.885 0.996
2024-12-02-08:06:31-root-INFO: Loss too large (149.517->149.761)! Learning rate decreased to 0.04837.
2024-12-02-08:06:31-root-INFO: grad norm: 7.925 7.866 0.962
2024-12-02-08:06:32-root-INFO: grad norm: 7.127 7.074 0.868
2024-12-02-08:06:32-root-INFO: grad norm: 6.553 6.495 0.871
2024-12-02-08:06:33-root-INFO: grad norm: 6.092 6.039 0.802
2024-12-02-08:06:33-root-INFO: Loss Change: 150.631 -> 147.006
2024-12-02-08:06:33-root-INFO: Regularization Change: 0.000 -> 1.643
2024-12-02-08:06:33-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-08:06:33-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-08:06:33-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-08:06:33-root-INFO: grad norm: 5.579 5.453 1.178
2024-12-02-08:06:34-root-INFO: grad norm: 8.106 8.027 1.127
2024-12-02-08:06:34-root-INFO: Loss too large (146.826->147.965)! Learning rate decreased to 0.06220.
2024-12-02-08:06:34-root-INFO: grad norm: 9.669 9.589 1.241
2024-12-02-08:06:35-root-INFO: Loss too large (146.742->146.959)! Learning rate decreased to 0.04976.
2024-12-02-08:06:35-root-INFO: grad norm: 7.923 7.860 0.997
2024-12-02-08:06:35-root-INFO: grad norm: 6.658 6.598 0.885
2024-12-02-08:06:36-root-INFO: grad norm: 5.714 5.663 0.767
2024-12-02-08:06:36-root-INFO: grad norm: 5.031 4.978 0.734
2024-12-02-08:06:37-root-INFO: grad norm: 4.498 4.448 0.671
2024-12-02-08:06:37-root-INFO: Loss Change: 146.995 -> 143.642
2024-12-02-08:06:37-root-INFO: Regularization Change: 0.000 -> 1.378
2024-12-02-08:06:37-root-INFO: Undo step: 100
2024-12-02-08:06:37-root-INFO: Undo step: 101
2024-12-02-08:06:37-root-INFO: Undo step: 102
2024-12-02-08:06:37-root-INFO: Undo step: 103
2024-12-02-08:06:37-root-INFO: Undo step: 104
2024-12-02-08:06:37-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-08:06:38-root-INFO: grad norm: 67.703 67.090 9.085
2024-12-02-08:06:38-root-INFO: grad norm: 42.539 42.069 6.307
2024-12-02-08:06:39-root-INFO: grad norm: 33.881 33.544 4.771
2024-12-02-08:06:39-root-INFO: grad norm: 36.902 36.632 4.454
2024-12-02-08:06:39-root-INFO: Loss too large (219.034->220.249)! Learning rate decreased to 0.05391.
2024-12-02-08:06:40-root-INFO: grad norm: 31.179 30.973 3.580
2024-12-02-08:06:40-root-INFO: grad norm: 28.428 28.248 3.194
2024-12-02-08:06:41-root-INFO: grad norm: 26.703 26.552 2.838
2024-12-02-08:06:41-root-INFO: grad norm: 25.058 24.917 2.655
2024-12-02-08:06:41-root-INFO: Loss Change: 397.286 -> 183.582
2024-12-02-08:06:41-root-INFO: Regularization Change: 0.000 -> 56.939
2024-12-02-08:06:41-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-08:06:41-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-08:06:42-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-08:06:42-root-INFO: grad norm: 25.800 25.653 2.754
2024-12-02-08:06:42-root-INFO: Loss too large (184.573->190.289)! Learning rate decreased to 0.05550.
2024-12-02-08:06:42-root-INFO: grad norm: 24.342 24.185 2.760
2024-12-02-08:06:43-root-INFO: grad norm: 23.422 23.282 2.556
2024-12-02-08:06:43-root-INFO: grad norm: 22.928 22.748 2.868
2024-12-02-08:06:44-root-INFO: grad norm: 22.956 22.790 2.757
2024-12-02-08:06:44-root-INFO: grad norm: 23.226 23.009 3.168
2024-12-02-08:06:45-root-INFO: grad norm: 23.984 23.781 3.110
2024-12-02-08:06:45-root-INFO: Loss too large (171.969->172.185)! Learning rate decreased to 0.04440.
2024-12-02-08:06:45-root-INFO: grad norm: 16.651 16.455 2.547
2024-12-02-08:06:46-root-INFO: Loss Change: 184.573 -> 164.307
2024-12-02-08:06:46-root-INFO: Regularization Change: 0.000 -> 5.896
2024-12-02-08:06:46-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-08:06:46-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-08:06:46-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-08:06:46-root-INFO: grad norm: 16.209 16.005 2.564
2024-12-02-08:06:46-root-INFO: Loss too large (165.643->170.276)! Learning rate decreased to 0.05711.
2024-12-02-08:06:46-root-INFO: Loss too large (165.643->165.791)! Learning rate decreased to 0.04569.
2024-12-02-08:06:47-root-INFO: grad norm: 12.505 12.344 2.001
2024-12-02-08:06:47-root-INFO: grad norm: 9.857 9.743 1.495
2024-12-02-08:06:48-root-INFO: grad norm: 8.159 8.036 1.413
2024-12-02-08:06:48-root-INFO: grad norm: 6.916 6.823 1.136
2024-12-02-08:06:49-root-INFO: grad norm: 6.056 5.949 1.136
2024-12-02-08:06:49-root-INFO: grad norm: 5.429 5.341 0.972
2024-12-02-08:06:50-root-INFO: grad norm: 4.976 4.876 0.991
2024-12-02-08:06:50-root-INFO: Loss Change: 165.643 -> 156.507
2024-12-02-08:06:50-root-INFO: Regularization Change: 0.000 -> 2.348
2024-12-02-08:06:50-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-08:06:50-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-08:06:50-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-08:06:50-root-INFO: grad norm: 8.353 8.189 1.648
2024-12-02-08:06:51-root-INFO: Loss too large (156.759->157.204)! Learning rate decreased to 0.05877.
2024-12-02-08:06:51-root-INFO: grad norm: 9.321 9.195 1.527
2024-12-02-08:06:51-root-INFO: grad norm: 11.191 11.058 1.717
2024-12-02-08:06:52-root-INFO: Loss too large (155.853->156.221)! Learning rate decreased to 0.04702.
2024-12-02-08:06:52-root-INFO: grad norm: 9.709 9.590 1.515
2024-12-02-08:06:53-root-INFO: grad norm: 8.496 8.398 1.285
2024-12-02-08:06:53-root-INFO: grad norm: 7.570 7.470 1.224
2024-12-02-08:06:54-root-INFO: grad norm: 6.810 6.726 1.068
2024-12-02-08:06:54-root-INFO: grad norm: 6.220 6.130 1.050
2024-12-02-08:06:54-root-INFO: Loss Change: 156.759 -> 151.510
2024-12-02-08:06:54-root-INFO: Regularization Change: 0.000 -> 1.938
2024-12-02-08:06:54-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-08:06:54-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-08:06:54-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-08:06:55-root-INFO: grad norm: 9.968 9.794 1.856
2024-12-02-08:06:55-root-INFO: Loss too large (152.254->154.164)! Learning rate decreased to 0.06047.
2024-12-02-08:06:55-root-INFO: Loss too large (152.254->152.508)! Learning rate decreased to 0.04837.
2024-12-02-08:06:55-root-INFO: grad norm: 8.373 8.269 1.317
2024-12-02-08:06:56-root-INFO: grad norm: 6.973 6.878 1.144
2024-12-02-08:06:56-root-INFO: grad norm: 6.274 6.191 1.017
2024-12-02-08:06:57-root-INFO: grad norm: 5.721 5.644 0.939
2024-12-02-08:06:57-root-INFO: grad norm: 5.307 5.230 0.902
2024-12-02-08:06:58-root-INFO: grad norm: 5.043 4.971 0.848
2024-12-02-08:06:58-root-INFO: grad norm: 4.844 4.769 0.848
2024-12-02-08:06:58-root-INFO: Loss Change: 152.254 -> 147.622
2024-12-02-08:06:58-root-INFO: Regularization Change: 0.000 -> 1.469
2024-12-02-08:06:58-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-08:06:58-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-08:06:59-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-08:06:59-root-INFO: grad norm: 9.487 9.304 1.856
2024-12-02-08:06:59-root-INFO: Loss too large (148.372->150.256)! Learning rate decreased to 0.06220.
2024-12-02-08:06:59-root-INFO: Loss too large (148.372->148.719)! Learning rate decreased to 0.04976.
2024-12-02-08:07:00-root-INFO: grad norm: 8.020 7.925 1.227
2024-12-02-08:07:00-root-INFO: grad norm: 6.702 6.608 1.119
2024-12-02-08:07:00-root-INFO: grad norm: 6.118 6.044 0.949
2024-12-02-08:07:01-root-INFO: grad norm: 5.657 5.583 0.917
2024-12-02-08:07:01-root-INFO: grad norm: 5.313 5.243 0.859
2024-12-02-08:07:02-root-INFO: grad norm: 5.128 5.059 0.834
2024-12-02-08:07:02-root-INFO: grad norm: 4.986 4.917 0.826
2024-12-02-08:07:03-root-INFO: Loss Change: 148.372 -> 144.344
2024-12-02-08:07:03-root-INFO: Regularization Change: 0.000 -> 1.317
2024-12-02-08:07:03-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-08:07:03-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-08:07:03-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-08:07:03-root-INFO: grad norm: 9.202 9.028 1.782
2024-12-02-08:07:03-root-INFO: Loss too large (144.847->146.763)! Learning rate decreased to 0.06397.
2024-12-02-08:07:03-root-INFO: Loss too large (144.847->145.270)! Learning rate decreased to 0.05118.
2024-12-02-08:07:04-root-INFO: grad norm: 7.810 7.718 1.193
2024-12-02-08:07:04-root-INFO: grad norm: 6.576 6.491 1.051
2024-12-02-08:07:05-root-INFO: grad norm: 6.068 5.996 0.928
2024-12-02-08:07:05-root-INFO: grad norm: 5.661 5.593 0.873
2024-12-02-08:07:06-root-INFO: grad norm: 5.352 5.286 0.841
2024-12-02-08:07:06-root-INFO: grad norm: 5.184 5.121 0.805
2024-12-02-08:07:07-root-INFO: grad norm: 5.088 5.022 0.814
2024-12-02-08:07:07-root-INFO: Loss Change: 144.847 -> 141.213
2024-12-02-08:07:07-root-INFO: Regularization Change: 0.000 -> 1.222
2024-12-02-08:07:07-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-08:07:07-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-08:07:07-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-08:07:07-root-INFO: grad norm: 9.783 9.622 1.765
2024-12-02-08:07:07-root-INFO: Loss too large (141.984->144.563)! Learning rate decreased to 0.06579.
2024-12-02-08:07:08-root-INFO: Loss too large (141.984->142.764)! Learning rate decreased to 0.05263.
2024-12-02-08:07:08-root-INFO: grad norm: 8.412 8.320 1.242
2024-12-02-08:07:09-root-INFO: grad norm: 7.359 7.275 1.108
2024-12-02-08:07:09-root-INFO: grad norm: 6.939 6.870 0.973
2024-12-02-08:07:10-root-INFO: grad norm: 6.601 6.535 0.929
2024-12-02-08:07:10-root-INFO: grad norm: 6.386 6.327 0.868
2024-12-02-08:07:10-root-INFO: grad norm: 6.184 6.127 0.838
2024-12-02-08:07:11-root-INFO: grad norm: 6.256 6.206 0.792
2024-12-02-08:07:11-root-INFO: Loss Change: 141.984 -> 138.689
2024-12-02-08:07:11-root-INFO: Regularization Change: 0.000 -> 1.184
2024-12-02-08:07:11-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-08:07:11-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-08:07:11-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-08:07:12-root-INFO: grad norm: 10.077 9.914 1.809
2024-12-02-08:07:12-root-INFO: Loss too large (139.480->142.662)! Learning rate decreased to 0.06764.
2024-12-02-08:07:12-root-INFO: Loss too large (139.480->140.171)! Learning rate decreased to 0.05411.
2024-12-02-08:07:12-root-INFO: grad norm: 9.301 9.217 1.251
2024-12-02-08:07:13-root-INFO: grad norm: 8.839 8.761 1.176
2024-12-02-08:07:13-root-INFO: grad norm: 8.629 8.561 1.086
2024-12-02-08:07:14-root-INFO: grad norm: 8.196 8.132 1.023
2024-12-02-08:07:14-root-INFO: grad norm: 7.740 7.679 0.970
2024-12-02-08:07:15-root-INFO: grad norm: 7.441 7.383 0.926
2024-12-02-08:07:15-root-INFO: grad norm: 7.248 7.193 0.887
2024-12-02-08:07:16-root-INFO: Loss Change: 139.480 -> 136.202
2024-12-02-08:07:16-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-02-08:07:16-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-08:07:16-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-08:07:16-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-08:07:16-root-INFO: grad norm: 9.713 9.590 1.542
2024-12-02-08:07:16-root-INFO: Loss too large (136.892->140.230)! Learning rate decreased to 0.06953.
2024-12-02-08:07:16-root-INFO: Loss too large (136.892->137.685)! Learning rate decreased to 0.05563.
2024-12-02-08:07:17-root-INFO: grad norm: 9.178 9.105 1.157
2024-12-02-08:07:17-root-INFO: grad norm: 8.778 8.704 1.136
2024-12-02-08:07:18-root-INFO: grad norm: 8.504 8.441 1.027
2024-12-02-08:07:18-root-INFO: grad norm: 8.129 8.066 1.008
2024-12-02-08:07:19-root-INFO: grad norm: 7.724 7.667 0.935
2024-12-02-08:07:19-root-INFO: grad norm: 7.473 7.416 0.921
2024-12-02-08:07:20-root-INFO: grad norm: 7.327 7.276 0.866
2024-12-02-08:07:20-root-INFO: Loss Change: 136.892 -> 133.897
2024-12-02-08:07:20-root-INFO: Regularization Change: 0.000 -> 1.075
2024-12-02-08:07:20-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-08:07:20-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-08:07:20-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-08:07:20-root-INFO: grad norm: 10.730 10.592 1.715
2024-12-02-08:07:21-root-INFO: Loss too large (134.996->139.396)! Learning rate decreased to 0.07147.
2024-12-02-08:07:21-root-INFO: Loss too large (134.996->136.189)! Learning rate decreased to 0.05718.
2024-12-02-08:07:21-root-INFO: grad norm: 10.182 10.103 1.268
2024-12-02-08:07:22-root-INFO: grad norm: 9.715 9.636 1.244
2024-12-02-08:07:22-root-INFO: grad norm: 9.389 9.322 1.121
2024-12-02-08:07:23-root-INFO: grad norm: 8.947 8.880 1.090
2024-12-02-08:07:23-root-INFO: grad norm: 8.482 8.422 1.011
2024-12-02-08:07:24-root-INFO: grad norm: 8.174 8.114 0.989
2024-12-02-08:07:24-root-INFO: grad norm: 7.981 7.927 0.927
2024-12-02-08:07:24-root-INFO: Loss Change: 134.996 -> 131.929
2024-12-02-08:07:24-root-INFO: Regularization Change: 0.000 -> 1.064
2024-12-02-08:07:24-root-INFO: Undo step: 95
2024-12-02-08:07:24-root-INFO: Undo step: 96
2024-12-02-08:07:24-root-INFO: Undo step: 97
2024-12-02-08:07:24-root-INFO: Undo step: 98
2024-12-02-08:07:24-root-INFO: Undo step: 99
2024-12-02-08:07:24-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-08:07:25-root-INFO: grad norm: 57.981 57.127 9.914
2024-12-02-08:07:25-root-INFO: grad norm: 47.598 47.228 5.921
2024-12-02-08:07:26-root-INFO: grad norm: 34.019 33.582 5.437
2024-12-02-08:07:26-root-INFO: grad norm: 36.791 36.436 5.094
2024-12-02-08:07:26-root-INFO: Loss too large (206.253->208.845)! Learning rate decreased to 0.06220.
2024-12-02-08:07:27-root-INFO: grad norm: 30.234 29.875 4.651
2024-12-02-08:07:27-root-INFO: grad norm: 27.658 27.379 3.915
2024-12-02-08:07:28-root-INFO: grad norm: 26.211 25.910 3.964
2024-12-02-08:07:28-root-INFO: grad norm: 25.711 25.440 3.721
2024-12-02-08:07:28-root-INFO: Loss Change: 354.478 -> 172.634
2024-12-02-08:07:28-root-INFO: Regularization Change: 0.000 -> 59.504
2024-12-02-08:07:28-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-08:07:28-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-08:07:29-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-08:07:29-root-INFO: grad norm: 23.394 23.167 3.250
2024-12-02-08:07:29-root-INFO: Loss too large (170.517->177.143)! Learning rate decreased to 0.06397.
2024-12-02-08:07:29-root-INFO: grad norm: 23.560 23.317 3.373
2024-12-02-08:07:30-root-INFO: grad norm: 23.872 23.600 3.595
2024-12-02-08:07:30-root-INFO: grad norm: 24.403 24.142 3.561
2024-12-02-08:07:31-root-INFO: grad norm: 24.757 24.462 3.805
2024-12-02-08:07:31-root-INFO: grad norm: 25.085 24.811 3.697
2024-12-02-08:07:31-root-INFO: Loss too large (162.027->162.165)! Learning rate decreased to 0.05118.
2024-12-02-08:07:32-root-INFO: grad norm: 16.694 16.478 2.679
2024-12-02-08:07:32-root-INFO: grad norm: 11.229 11.094 1.739
2024-12-02-08:07:33-root-INFO: Loss Change: 170.517 -> 150.522
2024-12-02-08:07:33-root-INFO: Regularization Change: 0.000 -> 5.912
2024-12-02-08:07:33-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-08:07:33-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-08:07:33-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-08:07:33-root-INFO: grad norm: 6.473 6.380 1.093
2024-12-02-08:07:33-root-INFO: grad norm: 8.616 8.526 1.242
2024-12-02-08:07:33-root-INFO: Loss too large (149.021->149.460)! Learning rate decreased to 0.06579.
2024-12-02-08:07:34-root-INFO: grad norm: 9.700 9.599 1.400
2024-12-02-08:07:34-root-INFO: grad norm: 11.324 11.208 1.618
2024-12-02-08:07:35-root-INFO: grad norm: 13.213 13.082 1.860
2024-12-02-08:07:35-root-INFO: Loss too large (147.821->147.911)! Learning rate decreased to 0.05263.
2024-12-02-08:07:36-root-INFO: grad norm: 10.154 10.043 1.497
2024-12-02-08:07:36-root-INFO: grad norm: 8.146 8.047 1.266
2024-12-02-08:07:37-root-INFO: grad norm: 6.662 6.572 1.088
2024-12-02-08:07:37-root-INFO: Loss Change: 149.994 -> 143.131
2024-12-02-08:07:37-root-INFO: Regularization Change: 0.000 -> 3.296
2024-12-02-08:07:37-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-08:07:37-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-08:07:37-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-08:07:37-root-INFO: grad norm: 5.227 5.090 1.190
2024-12-02-08:07:38-root-INFO: grad norm: 6.495 6.425 0.954
2024-12-02-08:07:38-root-INFO: Loss too large (142.051->142.140)! Learning rate decreased to 0.06764.
2024-12-02-08:07:38-root-INFO: grad norm: 7.382 7.311 1.024
2024-12-02-08:07:39-root-INFO: grad norm: 9.012 8.941 1.127
2024-12-02-08:07:39-root-INFO: Loss too large (141.224->141.307)! Learning rate decreased to 0.05411.
2024-12-02-08:07:39-root-INFO: grad norm: 7.619 7.551 1.012
2024-12-02-08:07:40-root-INFO: grad norm: 6.442 6.382 0.880
2024-12-02-08:07:40-root-INFO: grad norm: 5.653 5.590 0.841
2024-12-02-08:07:41-root-INFO: grad norm: 5.023 4.964 0.770
2024-12-02-08:07:41-root-INFO: Loss Change: 142.844 -> 137.935
2024-12-02-08:07:41-root-INFO: Regularization Change: 0.000 -> 2.357
2024-12-02-08:07:41-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-08:07:41-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-08:07:41-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-08:07:41-root-INFO: grad norm: 6.675 6.547 1.298
2024-12-02-08:07:42-root-INFO: Loss too large (138.292->138.771)! Learning rate decreased to 0.06953.
2024-12-02-08:07:42-root-INFO: grad norm: 8.223 8.138 1.176
2024-12-02-08:07:42-root-INFO: Loss too large (137.976->138.136)! Learning rate decreased to 0.05563.
2024-12-02-08:07:43-root-INFO: grad norm: 7.402 7.315 1.130
2024-12-02-08:07:43-root-INFO: grad norm: 6.741 6.670 0.977
2024-12-02-08:07:44-root-INFO: grad norm: 6.197 6.122 0.959
2024-12-02-08:07:44-root-INFO: grad norm: 5.726 5.660 0.863
2024-12-02-08:07:45-root-INFO: grad norm: 5.329 5.261 0.851
2024-12-02-08:07:45-root-INFO: grad norm: 4.986 4.924 0.783
2024-12-02-08:07:46-root-INFO: Loss Change: 138.292 -> 134.360
2024-12-02-08:07:46-root-INFO: Regularization Change: 0.000 -> 1.653
2024-12-02-08:07:46-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-08:07:46-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-08:07:46-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-08:07:46-root-INFO: grad norm: 8.385 8.219 1.657
2024-12-02-08:07:46-root-INFO: Loss too large (135.233->137.052)! Learning rate decreased to 0.07147.
2024-12-02-08:07:46-root-INFO: Loss too large (135.233->135.420)! Learning rate decreased to 0.05718.
2024-12-02-08:07:47-root-INFO: grad norm: 7.452 7.367 1.124
2024-12-02-08:07:47-root-INFO: grad norm: 6.831 6.741 1.106
2024-12-02-08:07:48-root-INFO: grad norm: 6.336 6.264 0.951
2024-12-02-08:07:48-root-INFO: grad norm: 5.899 5.824 0.936
2024-12-02-08:07:49-root-INFO: grad norm: 5.526 5.462 0.842
2024-12-02-08:07:49-root-INFO: grad norm: 5.197 5.130 0.833
2024-12-02-08:07:50-root-INFO: grad norm: 4.913 4.853 0.764
2024-12-02-08:07:50-root-INFO: Loss Change: 135.233 -> 131.504
2024-12-02-08:07:50-root-INFO: Regularization Change: 0.000 -> 1.385
2024-12-02-08:07:50-root-INFO: Undo step: 95
2024-12-02-08:07:50-root-INFO: Undo step: 96
2024-12-02-08:07:50-root-INFO: Undo step: 97
2024-12-02-08:07:50-root-INFO: Undo step: 98
2024-12-02-08:07:50-root-INFO: Undo step: 99
2024-12-02-08:07:50-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-08:07:50-root-INFO: grad norm: 71.956 71.487 8.206
2024-12-02-08:07:51-root-INFO: grad norm: 56.942 56.545 6.707
2024-12-02-08:07:51-root-INFO: grad norm: 44.231 43.939 5.074
2024-12-02-08:07:52-root-INFO: grad norm: 36.314 35.955 5.096
2024-12-02-08:07:52-root-INFO: grad norm: 36.746 36.330 5.514
2024-12-02-08:07:53-root-INFO: grad norm: 42.603 42.110 6.467
2024-12-02-08:07:53-root-INFO: Loss too large (195.504->208.201)! Learning rate decreased to 0.06220.
2024-12-02-08:07:53-root-INFO: grad norm: 32.980 32.570 5.184
2024-12-02-08:07:54-root-INFO: grad norm: 24.996 24.742 3.557
2024-12-02-08:07:54-root-INFO: Loss Change: 368.034 -> 165.815
2024-12-02-08:07:54-root-INFO: Regularization Change: 0.000 -> 58.599
2024-12-02-08:07:54-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-08:07:54-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-08:07:54-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-08:07:55-root-INFO: grad norm: 20.713 20.501 2.957
2024-12-02-08:07:55-root-INFO: Loss too large (163.994->167.120)! Learning rate decreased to 0.06397.
2024-12-02-08:07:55-root-INFO: grad norm: 19.398 19.210 2.691
2024-12-02-08:07:56-root-INFO: grad norm: 18.997 18.782 2.845
2024-12-02-08:07:56-root-INFO: grad norm: 18.967 18.777 2.674
2024-12-02-08:07:57-root-INFO: grad norm: 19.200 18.983 2.882
2024-12-02-08:07:57-root-INFO: grad norm: 19.699 19.499 2.803
2024-12-02-08:07:58-root-INFO: grad norm: 20.264 20.034 3.040
2024-12-02-08:07:58-root-INFO: grad norm: 21.087 20.871 3.009
2024-12-02-08:07:58-root-INFO: Loss too large (152.662->152.886)! Learning rate decreased to 0.05118.
2024-12-02-08:07:59-root-INFO: Loss Change: 163.994 -> 148.373
2024-12-02-08:07:59-root-INFO: Regularization Change: 0.000 -> 5.774
2024-12-02-08:07:59-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-08:07:59-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-08:07:59-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-08:07:59-root-INFO: grad norm: 12.079 11.969 1.626
2024-12-02-08:07:59-root-INFO: Loss too large (147.254->149.251)! Learning rate decreased to 0.06579.
2024-12-02-08:08:00-root-INFO: grad norm: 13.366 13.240 1.832
2024-12-02-08:08:00-root-INFO: grad norm: 15.121 14.970 2.135
2024-12-02-08:08:00-root-INFO: Loss too large (146.515->146.621)! Learning rate decreased to 0.05263.
2024-12-02-08:08:01-root-INFO: grad norm: 11.273 11.164 1.569
2024-12-02-08:08:01-root-INFO: grad norm: 8.922 8.820 1.347
2024-12-02-08:08:02-root-INFO: grad norm: 7.267 7.187 1.076
2024-12-02-08:08:02-root-INFO: grad norm: 6.157 6.077 0.989
2024-12-02-08:08:03-root-INFO: grad norm: 5.349 5.279 0.860
2024-12-02-08:08:03-root-INFO: Loss Change: 147.254 -> 139.460
2024-12-02-08:08:03-root-INFO: Regularization Change: 0.000 -> 2.678
2024-12-02-08:08:03-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-08:08:03-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-08:08:03-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-08:08:03-root-INFO: grad norm: 5.162 4.985 1.339
2024-12-02-08:08:04-root-INFO: grad norm: 5.808 5.710 1.062
2024-12-02-08:08:04-root-INFO: grad norm: 8.635 8.530 1.343
2024-12-02-08:08:05-root-INFO: Loss too large (138.192->139.198)! Learning rate decreased to 0.06764.
2024-12-02-08:08:05-root-INFO: grad norm: 9.191 9.085 1.392
2024-12-02-08:08:06-root-INFO: grad norm: 10.065 9.973 1.352
2024-12-02-08:08:06-root-INFO: grad norm: 11.817 11.721 1.506
2024-12-02-08:08:06-root-INFO: Loss too large (137.410->137.800)! Learning rate decreased to 0.05411.
2024-12-02-08:08:07-root-INFO: grad norm: 9.147 9.077 1.130
2024-12-02-08:08:07-root-INFO: grad norm: 7.048 6.987 0.924
2024-12-02-08:08:07-root-INFO: Loss Change: 139.343 -> 134.524
2024-12-02-08:08:07-root-INFO: Regularization Change: 0.000 -> 2.644
2024-12-02-08:08:07-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-08:08:07-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-08:08:08-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-08:08:08-root-INFO: grad norm: 7.270 7.144 1.347
2024-12-02-08:08:08-root-INFO: Loss too large (134.810->135.401)! Learning rate decreased to 0.06953.
2024-12-02-08:08:08-root-INFO: grad norm: 8.229 8.142 1.193
2024-12-02-08:08:09-root-INFO: grad norm: 9.812 9.711 1.405
2024-12-02-08:08:09-root-INFO: Loss too large (134.351->134.553)! Learning rate decreased to 0.05563.
2024-12-02-08:08:10-root-INFO: grad norm: 7.792 7.711 1.122
2024-12-02-08:08:10-root-INFO: grad norm: 6.266 6.197 0.927
2024-12-02-08:08:11-root-INFO: grad norm: 5.256 5.195 0.802
2024-12-02-08:08:11-root-INFO: grad norm: 4.546 4.489 0.718
2024-12-02-08:08:11-root-INFO: grad norm: 4.029 3.974 0.668
2024-12-02-08:08:12-root-INFO: Loss Change: 134.810 -> 130.771
2024-12-02-08:08:12-root-INFO: Regularization Change: 0.000 -> 1.630
2024-12-02-08:08:12-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-08:08:12-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-08:08:12-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-08:08:12-root-INFO: grad norm: 7.349 7.182 1.556
2024-12-02-08:08:12-root-INFO: Loss too large (131.539->132.569)! Learning rate decreased to 0.07147.
2024-12-02-08:08:13-root-INFO: grad norm: 8.752 8.637 1.413
2024-12-02-08:08:13-root-INFO: Loss too large (131.500->131.627)! Learning rate decreased to 0.05718.
2024-12-02-08:08:13-root-INFO: grad norm: 7.360 7.269 1.155
2024-12-02-08:08:14-root-INFO: grad norm: 6.658 6.589 0.958
2024-12-02-08:08:14-root-INFO: grad norm: 6.141 6.075 0.900
2024-12-02-08:08:15-root-INFO: grad norm: 5.738 5.680 0.817
2024-12-02-08:08:15-root-INFO: grad norm: 5.419 5.361 0.790
2024-12-02-08:08:16-root-INFO: grad norm: 5.159 5.106 0.739
2024-12-02-08:08:16-root-INFO: Loss Change: 131.539 -> 128.182
2024-12-02-08:08:16-root-INFO: Regularization Change: 0.000 -> 1.385
2024-12-02-08:08:16-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-08:08:16-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-08:08:16-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-08:08:17-root-INFO: grad norm: 8.680 8.507 1.727
2024-12-02-08:08:17-root-INFO: Loss too large (128.790->131.098)! Learning rate decreased to 0.07345.
2024-12-02-08:08:17-root-INFO: Loss too large (128.790->129.273)! Learning rate decreased to 0.05876.
2024-12-02-08:08:17-root-INFO: grad norm: 7.856 7.775 1.120
2024-12-02-08:08:18-root-INFO: grad norm: 7.281 7.205 1.048
2024-12-02-08:08:18-root-INFO: grad norm: 6.998 6.936 0.933
2024-12-02-08:08:19-root-INFO: grad norm: 6.770 6.709 0.909
2024-12-02-08:08:19-root-INFO: grad norm: 6.573 6.516 0.859
2024-12-02-08:08:20-root-INFO: grad norm: 6.399 6.343 0.846
2024-12-02-08:08:20-root-INFO: grad norm: 6.251 6.198 0.813
2024-12-02-08:08:21-root-INFO: Loss Change: 128.790 -> 125.634
2024-12-02-08:08:21-root-INFO: Regularization Change: 0.000 -> 1.224
2024-12-02-08:08:21-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-08:08:21-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-08:08:21-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-08:08:21-root-INFO: grad norm: 10.474 10.294 1.929
2024-12-02-08:08:21-root-INFO: Loss too large (126.561->130.481)! Learning rate decreased to 0.07547.
2024-12-02-08:08:21-root-INFO: Loss too large (126.561->127.693)! Learning rate decreased to 0.06038.
2024-12-02-08:08:22-root-INFO: grad norm: 9.422 9.336 1.267
2024-12-02-08:08:22-root-INFO: grad norm: 8.533 8.457 1.137
2024-12-02-08:08:23-root-INFO: grad norm: 8.167 8.107 0.987
2024-12-02-08:08:23-root-INFO: grad norm: 7.898 7.838 0.976
2024-12-02-08:08:24-root-INFO: grad norm: 7.651 7.596 0.923
2024-12-02-08:08:24-root-INFO: grad norm: 7.483 7.425 0.925
2024-12-02-08:08:25-root-INFO: grad norm: 7.282 7.227 0.892
2024-12-02-08:08:25-root-INFO: Loss Change: 126.561 -> 123.264
2024-12-02-08:08:25-root-INFO: Regularization Change: 0.000 -> 1.189
2024-12-02-08:08:25-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-08:08:25-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-08:08:25-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-08:08:25-root-INFO: grad norm: 9.811 9.670 1.659
2024-12-02-08:08:26-root-INFO: Loss too large (123.982->127.771)! Learning rate decreased to 0.07753.
2024-12-02-08:08:26-root-INFO: Loss too large (123.982->125.057)! Learning rate decreased to 0.06203.
2024-12-02-08:08:26-root-INFO: grad norm: 9.181 9.098 1.227
2024-12-02-08:08:27-root-INFO: grad norm: 8.725 8.647 1.164
2024-12-02-08:08:27-root-INFO: grad norm: 8.394 8.324 1.086
2024-12-02-08:08:28-root-INFO: grad norm: 8.096 8.029 1.037
2024-12-02-08:08:28-root-INFO: grad norm: 7.834 7.769 1.007
2024-12-02-08:08:28-root-INFO: grad norm: 7.593 7.531 0.967
2024-12-02-08:08:29-root-INFO: grad norm: 7.388 7.327 0.950
2024-12-02-08:08:29-root-INFO: Loss Change: 123.982 -> 120.991
2024-12-02-08:08:29-root-INFO: Regularization Change: 0.000 -> 1.137
2024-12-02-08:08:29-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-08:08:29-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-08:08:29-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-08:08:30-root-INFO: grad norm: 10.880 10.729 1.803
2024-12-02-08:08:30-root-INFO: Loss too large (121.932->126.894)! Learning rate decreased to 0.07964.
2024-12-02-08:08:30-root-INFO: Loss too large (121.932->123.600)! Learning rate decreased to 0.06371.
2024-12-02-08:08:30-root-INFO: grad norm: 9.926 9.834 1.350
2024-12-02-08:08:31-root-INFO: grad norm: 8.952 8.873 1.187
2024-12-02-08:08:31-root-INFO: grad norm: 8.495 8.430 1.047
2024-12-02-08:08:32-root-INFO: grad norm: 8.160 8.096 1.024
2024-12-02-08:08:32-root-INFO: grad norm: 7.884 7.823 0.979
2024-12-02-08:08:33-root-INFO: grad norm: 7.788 7.726 0.987
2024-12-02-08:08:33-root-INFO: grad norm: 7.563 7.501 0.966
2024-12-02-08:08:34-root-INFO: Loss Change: 121.932 -> 118.861
2024-12-02-08:08:34-root-INFO: Regularization Change: 0.000 -> 1.097
2024-12-02-08:08:34-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-08:08:34-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-08:08:34-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-08:08:34-root-INFO: grad norm: 10.541 10.387 1.795
2024-12-02-08:08:34-root-INFO: Loss too large (119.835->124.530)! Learning rate decreased to 0.08180.
2024-12-02-08:08:34-root-INFO: Loss too large (119.835->121.348)! Learning rate decreased to 0.06544.
2024-12-02-08:08:35-root-INFO: grad norm: 9.563 9.478 1.271
2024-12-02-08:08:35-root-INFO: grad norm: 8.556 8.479 1.145
2024-12-02-08:08:36-root-INFO: grad norm: 8.111 8.050 0.995
2024-12-02-08:08:36-root-INFO: grad norm: 7.887 7.823 1.001
2024-12-02-08:08:37-root-INFO: grad norm: 7.608 7.549 0.951
2024-12-02-08:08:37-root-INFO: grad norm: 7.472 7.411 0.959
2024-12-02-08:08:37-root-INFO: grad norm: 7.186 7.127 0.919
2024-12-02-08:08:38-root-INFO: Loss Change: 119.835 -> 116.761
2024-12-02-08:08:38-root-INFO: Regularization Change: 0.000 -> 1.094
2024-12-02-08:08:38-root-INFO: Undo step: 90
2024-12-02-08:08:38-root-INFO: Undo step: 91
2024-12-02-08:08:38-root-INFO: Undo step: 92
2024-12-02-08:08:38-root-INFO: Undo step: 93
2024-12-02-08:08:38-root-INFO: Undo step: 94
2024-12-02-08:08:38-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-08:08:38-root-INFO: grad norm: 50.299 49.724 7.589
2024-12-02-08:08:39-root-INFO: grad norm: 35.976 35.688 4.543
2024-12-02-08:08:39-root-INFO: grad norm: 37.893 37.525 5.268
2024-12-02-08:08:39-root-INFO: grad norm: 44.266 43.996 4.883
2024-12-02-08:08:40-root-INFO: Loss too large (200.241->209.251)! Learning rate decreased to 0.07147.
2024-12-02-08:08:40-root-INFO: grad norm: 33.373 32.988 5.058
2024-12-02-08:08:41-root-INFO: grad norm: 24.826 24.614 3.239
2024-12-02-08:08:41-root-INFO: grad norm: 22.786 22.496 3.624
2024-12-02-08:08:42-root-INFO: grad norm: 23.003 22.790 3.121
2024-12-02-08:08:42-root-INFO: Loss Change: 310.746 -> 153.315
2024-12-02-08:08:42-root-INFO: Regularization Change: 0.000 -> 61.730
2024-12-02-08:08:42-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-08:08:42-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-08:08:42-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-08:08:42-root-INFO: grad norm: 22.546 22.353 2.948
2024-12-02-08:08:42-root-INFO: Loss too large (151.256->161.712)! Learning rate decreased to 0.07345.
2024-12-02-08:08:43-root-INFO: grad norm: 24.743 24.537 3.184
2024-12-02-08:08:43-root-INFO: Loss too large (150.453->151.407)! Learning rate decreased to 0.05876.
2024-12-02-08:08:44-root-INFO: grad norm: 17.905 17.713 2.613
2024-12-02-08:08:44-root-INFO: grad norm: 13.374 13.256 1.771
2024-12-02-08:08:44-root-INFO: grad norm: 10.706 10.582 1.627
2024-12-02-08:08:45-root-INFO: grad norm: 8.846 8.754 1.267
2024-12-02-08:08:45-root-INFO: grad norm: 7.548 7.449 1.216
2024-12-02-08:08:46-root-INFO: grad norm: 6.588 6.506 1.037
2024-12-02-08:08:46-root-INFO: Loss Change: 151.256 -> 132.672
2024-12-02-08:08:46-root-INFO: Regularization Change: 0.000 -> 5.225
2024-12-02-08:08:46-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-08:08:46-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-08:08:46-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-08:08:46-root-INFO: grad norm: 5.181 5.050 1.159
2024-12-02-08:08:47-root-INFO: grad norm: 5.836 5.756 0.962
2024-12-02-08:08:47-root-INFO: grad norm: 8.485 8.437 0.902
2024-12-02-08:08:48-root-INFO: Loss too large (130.508->131.522)! Learning rate decreased to 0.07547.
2024-12-02-08:08:48-root-INFO: grad norm: 9.642 9.596 0.942
2024-12-02-08:08:49-root-INFO: grad norm: 11.318 11.281 0.920
2024-12-02-08:08:49-root-INFO: Loss too large (129.911->130.220)! Learning rate decreased to 0.06038.
2024-12-02-08:08:49-root-INFO: grad norm: 9.294 9.248 0.930
2024-12-02-08:08:50-root-INFO: grad norm: 7.986 7.938 0.879
2024-12-02-08:08:50-root-INFO: grad norm: 7.040 6.989 0.848
2024-12-02-08:08:51-root-INFO: Loss Change: 132.270 -> 126.154
2024-12-02-08:08:51-root-INFO: Regularization Change: 0.000 -> 3.723
2024-12-02-08:08:51-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-08:08:51-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-08:08:51-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-08:08:51-root-INFO: grad norm: 5.463 5.396 0.848
2024-12-02-08:08:51-root-INFO: grad norm: 8.288 8.253 0.761
2024-12-02-08:08:51-root-INFO: Loss too large (125.816->127.367)! Learning rate decreased to 0.07753.
2024-12-02-08:08:52-root-INFO: grad norm: 10.100 10.067 0.815
2024-12-02-08:08:52-root-INFO: Loss too large (125.795->126.146)! Learning rate decreased to 0.06203.
2024-12-02-08:08:53-root-INFO: grad norm: 8.449 8.410 0.809
2024-12-02-08:08:53-root-INFO: grad norm: 7.374 7.332 0.785
2024-12-02-08:08:54-root-INFO: grad norm: 6.588 6.544 0.760
2024-12-02-08:08:54-root-INFO: grad norm: 5.996 5.951 0.734
2024-12-02-08:08:54-root-INFO: grad norm: 5.529 5.483 0.708
2024-12-02-08:08:55-root-INFO: Loss Change: 126.016 -> 121.860
2024-12-02-08:08:55-root-INFO: Regularization Change: 0.000 -> 2.250
2024-12-02-08:08:55-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-08:08:55-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-08:08:55-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-08:08:55-root-INFO: grad norm: 4.308 4.233 0.800
2024-12-02-08:08:56-root-INFO: grad norm: 6.217 6.176 0.710
2024-12-02-08:08:56-root-INFO: Loss too large (121.357->122.054)! Learning rate decreased to 0.07964.
2024-12-02-08:08:56-root-INFO: grad norm: 7.708 7.680 0.653
2024-12-02-08:08:56-root-INFO: Loss too large (121.170->121.386)! Learning rate decreased to 0.06371.
2024-12-02-08:08:57-root-INFO: grad norm: 6.599 6.567 0.647
2024-12-02-08:08:57-root-INFO: grad norm: 5.703 5.675 0.565
2024-12-02-08:08:58-root-INFO: grad norm: 5.213 5.180 0.588
2024-12-02-08:08:58-root-INFO: grad norm: 4.833 4.802 0.549
2024-12-02-08:08:59-root-INFO: grad norm: 4.548 4.512 0.568
2024-12-02-08:08:59-root-INFO: Loss Change: 121.639 -> 118.385
2024-12-02-08:08:59-root-INFO: Regularization Change: 0.000 -> 1.833
2024-12-02-08:08:59-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-08:08:59-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-08:08:59-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-08:08:59-root-INFO: grad norm: 4.376 4.281 0.907
2024-12-02-08:09:00-root-INFO: grad norm: 6.066 6.010 0.818
2024-12-02-08:09:00-root-INFO: Loss too large (118.285->118.909)! Learning rate decreased to 0.08180.
2024-12-02-08:09:00-root-INFO: grad norm: 7.297 7.259 0.748
2024-12-02-08:09:01-root-INFO: Loss too large (118.014->118.248)! Learning rate decreased to 0.06544.
2024-12-02-08:09:01-root-INFO: grad norm: 6.057 6.023 0.639
2024-12-02-08:09:01-root-INFO: grad norm: 4.983 4.958 0.496
2024-12-02-08:09:02-root-INFO: grad norm: 4.541 4.513 0.503
2024-12-02-08:09:02-root-INFO: grad norm: 4.232 4.207 0.461
2024-12-02-08:09:03-root-INFO: grad norm: 4.016 3.986 0.491
2024-12-02-08:09:03-root-INFO: Loss Change: 118.455 -> 115.555
2024-12-02-08:09:03-root-INFO: Regularization Change: 0.000 -> 1.628
2024-12-02-08:09:03-root-INFO: Undo step: 90
2024-12-02-08:09:03-root-INFO: Undo step: 91
2024-12-02-08:09:03-root-INFO: Undo step: 92
2024-12-02-08:09:03-root-INFO: Undo step: 93
2024-12-02-08:09:03-root-INFO: Undo step: 94
2024-12-02-08:09:03-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-08:09:03-root-INFO: grad norm: 62.598 61.841 9.710
2024-12-02-08:09:04-root-INFO: grad norm: 48.530 48.052 6.793
2024-12-02-08:09:04-root-INFO: grad norm: 45.789 45.542 4.752
2024-12-02-08:09:05-root-INFO: grad norm: 41.229 40.991 4.422
2024-12-02-08:09:05-root-INFO: grad norm: 37.501 37.335 3.524
2024-12-02-08:09:06-root-INFO: grad norm: 34.569 34.347 3.910
2024-12-02-08:09:06-root-INFO: grad norm: 32.309 32.101 3.663
2024-12-02-08:09:07-root-INFO: Loss too large (167.770->168.046)! Learning rate decreased to 0.07147.
2024-12-02-08:09:07-root-INFO: grad norm: 22.592 22.436 2.652
2024-12-02-08:09:07-root-INFO: Loss Change: 331.400 -> 145.561
2024-12-02-08:09:07-root-INFO: Regularization Change: 0.000 -> 67.772
2024-12-02-08:09:07-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-08:09:07-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-08:09:08-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-08:09:08-root-INFO: grad norm: 17.192 17.072 2.026
2024-12-02-08:09:08-root-INFO: Loss too large (144.710->146.824)! Learning rate decreased to 0.07345.
2024-12-02-08:09:08-root-INFO: grad norm: 15.176 15.077 1.732
2024-12-02-08:09:09-root-INFO: grad norm: 13.856 13.742 1.772
2024-12-02-08:09:09-root-INFO: grad norm: 13.640 13.538 1.667
2024-12-02-08:09:10-root-INFO: grad norm: 13.772 13.643 1.876
2024-12-02-08:09:10-root-INFO: grad norm: 14.206 14.085 1.851
2024-12-02-08:09:11-root-INFO: grad norm: 14.849 14.695 2.136
2024-12-02-08:09:11-root-INFO: grad norm: 15.704 15.552 2.179
2024-12-02-08:09:11-root-INFO: Loss Change: 144.710 -> 133.502
2024-12-02-08:09:11-root-INFO: Regularization Change: 0.000 -> 6.506
2024-12-02-08:09:11-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-08:09:11-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-08:09:12-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-08:09:12-root-INFO: grad norm: 14.757 14.641 1.843
2024-12-02-08:09:12-root-INFO: Loss too large (132.054->136.416)! Learning rate decreased to 0.07547.
2024-12-02-08:09:12-root-INFO: grad norm: 15.328 15.199 1.980
2024-12-02-08:09:13-root-INFO: grad norm: 16.413 16.259 2.243
2024-12-02-08:09:13-root-INFO: Loss too large (131.130->131.662)! Learning rate decreased to 0.06038.
2024-12-02-08:09:13-root-INFO: grad norm: 11.633 11.532 1.526
2024-12-02-08:09:14-root-INFO: grad norm: 8.595 8.495 1.312
2024-12-02-08:09:14-root-INFO: grad norm: 7.186 7.110 1.044
2024-12-02-08:09:15-root-INFO: grad norm: 6.280 6.206 0.964
2024-12-02-08:09:15-root-INFO: grad norm: 5.664 5.602 0.838
2024-12-02-08:09:16-root-INFO: Loss Change: 132.054 -> 123.472
2024-12-02-08:09:16-root-INFO: Regularization Change: 0.000 -> 2.840
2024-12-02-08:09:16-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-08:09:16-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-08:09:16-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-08:09:16-root-INFO: grad norm: 4.308 4.221 0.863
2024-12-02-08:09:16-root-INFO: grad norm: 5.745 5.696 0.748
2024-12-02-08:09:17-root-INFO: Loss too large (122.675->123.215)! Learning rate decreased to 0.07753.
2024-12-02-08:09:17-root-INFO: grad norm: 8.715 8.675 0.829
2024-12-02-08:09:17-root-INFO: Loss too large (122.436->123.017)! Learning rate decreased to 0.06203.
2024-12-02-08:09:18-root-INFO: grad norm: 6.663 6.618 0.767
2024-12-02-08:09:18-root-INFO: grad norm: 5.457 5.412 0.701
2024-12-02-08:09:19-root-INFO: grad norm: 5.346 5.297 0.722
2024-12-02-08:09:19-root-INFO: grad norm: 5.326 5.273 0.753
2024-12-02-08:09:20-root-INFO: grad norm: 5.714 5.662 0.771
2024-12-02-08:09:20-root-INFO: Loss Change: 123.267 -> 119.578
2024-12-02-08:09:20-root-INFO: Regularization Change: 0.000 -> 2.228
2024-12-02-08:09:20-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-08:09:20-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-08:09:20-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-08:09:20-root-INFO: grad norm: 4.595 4.535 0.745
2024-12-02-08:09:20-root-INFO: Loss too large (119.250->119.268)! Learning rate decreased to 0.07964.
2024-12-02-08:09:21-root-INFO: grad norm: 6.951 6.915 0.710
2024-12-02-08:09:21-root-INFO: Loss too large (118.801->119.132)! Learning rate decreased to 0.06371.
2024-12-02-08:09:22-root-INFO: grad norm: 4.880 4.836 0.653
2024-12-02-08:09:22-root-INFO: grad norm: 3.600 3.558 0.546
2024-12-02-08:09:23-root-INFO: grad norm: 3.579 3.541 0.515
2024-12-02-08:09:23-root-INFO: grad norm: 3.698 3.659 0.536
2024-12-02-08:09:23-root-INFO: grad norm: 4.301 4.266 0.549
2024-12-02-08:09:24-root-INFO: grad norm: 4.175 4.136 0.570
2024-12-02-08:09:24-root-INFO: Loss Change: 119.250 -> 116.001
2024-12-02-08:09:24-root-INFO: Regularization Change: 0.000 -> 1.630
2024-12-02-08:09:24-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-08:09:24-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-08:09:24-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-08:09:25-root-INFO: grad norm: 5.998 5.862 1.267
2024-12-02-08:09:25-root-INFO: Loss too large (116.270->116.824)! Learning rate decreased to 0.08180.
2024-12-02-08:09:25-root-INFO: Loss too large (116.270->116.358)! Learning rate decreased to 0.06544.
2024-12-02-08:09:25-root-INFO: grad norm: 4.693 4.638 0.715
2024-12-02-08:09:26-root-INFO: grad norm: 3.030 2.965 0.622
2024-12-02-08:09:26-root-INFO: grad norm: 2.966 2.920 0.517
2024-12-02-08:09:27-root-INFO: grad norm: 3.454 3.409 0.560
2024-12-02-08:09:27-root-INFO: grad norm: 3.731 3.689 0.558
2024-12-02-08:09:28-root-INFO: grad norm: 4.668 4.625 0.631
2024-12-02-08:09:28-root-INFO: grad norm: 4.260 4.215 0.613
2024-12-02-08:09:29-root-INFO: Loss Change: 116.270 -> 113.368
2024-12-02-08:09:29-root-INFO: Regularization Change: 0.000 -> 1.360
2024-12-02-08:09:29-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-08:09:29-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-08:09:29-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-08:09:29-root-INFO: grad norm: 7.870 7.670 1.762
2024-12-02-08:09:29-root-INFO: Loss too large (113.888->115.960)! Learning rate decreased to 0.08399.
2024-12-02-08:09:29-root-INFO: Loss too large (113.888->114.491)! Learning rate decreased to 0.06719.
2024-12-02-08:09:30-root-INFO: grad norm: 6.902 6.823 1.043
2024-12-02-08:09:30-root-INFO: grad norm: 5.946 5.870 0.949
2024-12-02-08:09:31-root-INFO: grad norm: 6.061 6.004 0.826
2024-12-02-08:09:31-root-INFO: grad norm: 6.901 6.841 0.909
2024-12-02-08:09:31-root-INFO: Loss too large (112.327->112.342)! Learning rate decreased to 0.05375.
2024-12-02-08:09:32-root-INFO: grad norm: 4.935 4.890 0.665
2024-12-02-08:09:32-root-INFO: grad norm: 3.210 3.167 0.524
2024-12-02-08:09:33-root-INFO: grad norm: 2.816 2.780 0.449
2024-12-02-08:09:33-root-INFO: Loss Change: 113.888 -> 110.864
2024-12-02-08:09:33-root-INFO: Regularization Change: 0.000 -> 1.092
2024-12-02-08:09:33-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-08:09:33-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-08:09:33-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-08:09:33-root-INFO: grad norm: 5.261 5.163 1.012
2024-12-02-08:09:33-root-INFO: Loss too large (111.411->112.161)! Learning rate decreased to 0.08623.
2024-12-02-08:09:34-root-INFO: Loss too large (111.411->111.603)! Learning rate decreased to 0.06899.
2024-12-02-08:09:34-root-INFO: grad norm: 4.997 4.950 0.690
2024-12-02-08:09:34-root-INFO: grad norm: 4.959 4.907 0.716
2024-12-02-08:09:35-root-INFO: grad norm: 4.967 4.922 0.665
2024-12-02-08:09:35-root-INFO: grad norm: 4.949 4.901 0.685
2024-12-02-08:09:36-root-INFO: grad norm: 5.016 4.973 0.653
2024-12-02-08:09:36-root-INFO: grad norm: 5.119 5.074 0.682
2024-12-02-08:09:37-root-INFO: grad norm: 5.146 5.104 0.657
2024-12-02-08:09:37-root-INFO: Loss Change: 111.411 -> 109.212
2024-12-02-08:09:37-root-INFO: Regularization Change: 0.000 -> 1.205
2024-12-02-08:09:37-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-08:09:37-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-08:09:37-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-08:09:37-root-INFO: grad norm: 9.299 9.127 1.778
2024-12-02-08:09:38-root-INFO: Loss too large (109.926->113.447)! Learning rate decreased to 0.08852.
2024-12-02-08:09:38-root-INFO: Loss too large (109.926->111.215)! Learning rate decreased to 0.07082.
2024-12-02-08:09:38-root-INFO: grad norm: 7.925 7.847 1.109
2024-12-02-08:09:39-root-INFO: grad norm: 6.867 6.804 0.927
2024-12-02-08:09:39-root-INFO: grad norm: 6.584 6.543 0.739
2024-12-02-08:09:40-root-INFO: grad norm: 6.410 6.370 0.717
2024-12-02-08:09:40-root-INFO: grad norm: 6.398 6.367 0.636
2024-12-02-08:09:41-root-INFO: grad norm: 6.384 6.352 0.637
2024-12-02-08:09:41-root-INFO: grad norm: 6.940 6.914 0.600
2024-12-02-08:09:41-root-INFO: Loss too large (107.468->107.519)! Learning rate decreased to 0.05665.
2024-12-02-08:09:42-root-INFO: Loss Change: 109.926 -> 107.067
2024-12-02-08:09:42-root-INFO: Regularization Change: 0.000 -> 1.221
2024-12-02-08:09:42-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-08:09:42-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-08:09:42-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-08:09:42-root-INFO: grad norm: 6.852 6.760 1.119
2024-12-02-08:09:42-root-INFO: Loss too large (107.501->109.584)! Learning rate decreased to 0.09086.
2024-12-02-08:09:42-root-INFO: Loss too large (107.501->107.864)! Learning rate decreased to 0.07268.
2024-12-02-08:09:43-root-INFO: grad norm: 6.743 6.709 0.684
2024-12-02-08:09:43-root-INFO: grad norm: 6.339 6.305 0.650
2024-12-02-08:09:44-root-INFO: grad norm: 5.756 5.728 0.561
2024-12-02-08:09:44-root-INFO: grad norm: 5.737 5.709 0.567
2024-12-02-08:09:45-root-INFO: grad norm: 6.224 6.201 0.543
2024-12-02-08:09:45-root-INFO: Loss too large (105.843->105.921)! Learning rate decreased to 0.05815.
2024-12-02-08:09:45-root-INFO: grad norm: 4.418 4.392 0.473
2024-12-02-08:09:46-root-INFO: grad norm: 2.679 2.653 0.374
2024-12-02-08:09:46-root-INFO: Loss Change: 107.501 -> 104.827
2024-12-02-08:09:46-root-INFO: Regularization Change: 0.000 -> 1.026
2024-12-02-08:09:46-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-08:09:46-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-08:09:46-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-08:09:46-root-INFO: grad norm: 4.197 4.097 0.911
2024-12-02-08:09:47-root-INFO: Loss too large (105.012->105.221)! Learning rate decreased to 0.09324.
2024-12-02-08:09:47-root-INFO: grad norm: 5.240 5.195 0.686
2024-12-02-08:09:47-root-INFO: Loss too large (104.816->105.041)! Learning rate decreased to 0.07459.
2024-12-02-08:09:48-root-INFO: grad norm: 5.151 5.116 0.607
2024-12-02-08:09:48-root-INFO: grad norm: 6.086 6.061 0.556
2024-12-02-08:09:48-root-INFO: Loss too large (104.364->104.490)! Learning rate decreased to 0.05967.
2024-12-02-08:09:49-root-INFO: grad norm: 4.435 4.410 0.471
2024-12-02-08:09:49-root-INFO: grad norm: 2.750 2.724 0.376
2024-12-02-08:09:50-root-INFO: grad norm: 2.679 2.654 0.369
2024-12-02-08:09:50-root-INFO: grad norm: 2.980 2.954 0.387
2024-12-02-08:09:51-root-INFO: Loss Change: 105.012 -> 103.099
2024-12-02-08:09:51-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-08:09:51-root-INFO: Undo step: 85
2024-12-02-08:09:51-root-INFO: Undo step: 86
2024-12-02-08:09:51-root-INFO: Undo step: 87
2024-12-02-08:09:51-root-INFO: Undo step: 88
2024-12-02-08:09:51-root-INFO: Undo step: 89
2024-12-02-08:09:51-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-08:09:51-root-INFO: grad norm: 47.058 46.450 7.542
2024-12-02-08:09:51-root-INFO: grad norm: 32.759 32.408 4.784
2024-12-02-08:09:52-root-INFO: grad norm: 33.879 33.708 3.402
2024-12-02-08:09:52-root-INFO: grad norm: 33.558 33.368 3.569
2024-12-02-08:09:53-root-INFO: grad norm: 32.197 32.051 3.064
2024-12-02-08:09:53-root-INFO: grad norm: 31.353 31.184 3.251
2024-12-02-08:09:54-root-INFO: grad norm: 29.936 29.757 3.265
2024-12-02-08:09:54-root-INFO: grad norm: 29.930 29.693 3.755
2024-12-02-08:09:54-root-INFO: Loss Change: 292.781 -> 150.896
2024-12-02-08:09:54-root-INFO: Regularization Change: 0.000 -> 71.501
2024-12-02-08:09:54-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-08:09:54-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-08:09:55-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-08:09:55-root-INFO: grad norm: 28.796 28.549 3.762
2024-12-02-08:09:55-root-INFO: grad norm: 31.049 30.699 4.648
2024-12-02-08:09:55-root-INFO: Loss too large (146.739->151.349)! Learning rate decreased to 0.08399.
2024-12-02-08:09:56-root-INFO: grad norm: 22.022 21.655 4.001
2024-12-02-08:09:56-root-INFO: grad norm: 17.783 17.554 2.845
2024-12-02-08:09:57-root-INFO: grad norm: 16.297 16.034 2.912
2024-12-02-08:09:57-root-INFO: grad norm: 16.125 15.905 2.656
2024-12-02-08:09:58-root-INFO: grad norm: 16.069 15.807 2.885
2024-12-02-08:09:58-root-INFO: grad norm: 16.136 15.911 2.684
2024-12-02-08:09:59-root-INFO: Loss Change: 148.104 -> 121.290
2024-12-02-08:09:59-root-INFO: Regularization Change: 0.000 -> 8.121
2024-12-02-08:09:59-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-08:09:59-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-08:09:59-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-08:09:59-root-INFO: grad norm: 14.651 14.463 2.344
2024-12-02-08:09:59-root-INFO: Loss too large (120.273->125.022)! Learning rate decreased to 0.08623.
2024-12-02-08:09:59-root-INFO: grad norm: 15.488 15.294 2.445
2024-12-02-08:10:00-root-INFO: Loss too large (119.439->119.688)! Learning rate decreased to 0.06899.
2024-12-02-08:10:00-root-INFO: grad norm: 10.160 10.001 1.792
2024-12-02-08:10:01-root-INFO: grad norm: 6.603 6.510 1.106
2024-12-02-08:10:01-root-INFO: grad norm: 4.799 4.719 0.875
2024-12-02-08:10:02-root-INFO: grad norm: 3.787 3.718 0.720
2024-12-02-08:10:02-root-INFO: grad norm: 3.227 3.158 0.664
2024-12-02-08:10:02-root-INFO: grad norm: 2.911 2.846 0.614
2024-12-02-08:10:03-root-INFO: Loss Change: 120.273 -> 111.122
2024-12-02-08:10:03-root-INFO: Regularization Change: 0.000 -> 2.918
2024-12-02-08:10:03-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-08:10:03-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-08:10:03-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-08:10:03-root-INFO: grad norm: 4.425 4.272 1.154
2024-12-02-08:10:04-root-INFO: grad norm: 5.872 5.745 1.213
2024-12-02-08:10:04-root-INFO: Loss too large (110.564->111.309)! Learning rate decreased to 0.08852.
2024-12-02-08:10:04-root-INFO: grad norm: 8.477 8.369 1.347
2024-12-02-08:10:04-root-INFO: Loss too large (110.334->110.852)! Learning rate decreased to 0.07082.
2024-12-02-08:10:05-root-INFO: grad norm: 5.811 5.713 1.061
2024-12-02-08:10:05-root-INFO: grad norm: 4.145 4.082 0.719
2024-12-02-08:10:06-root-INFO: grad norm: 3.436 3.389 0.571
2024-12-02-08:10:06-root-INFO: grad norm: 3.219 3.171 0.552
2024-12-02-08:10:07-root-INFO: grad norm: 3.409 3.370 0.514
2024-12-02-08:10:07-root-INFO: Loss Change: 111.021 -> 107.292
2024-12-02-08:10:07-root-INFO: Regularization Change: 0.000 -> 2.283
2024-12-02-08:10:07-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-08:10:07-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-08:10:07-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-08:10:07-root-INFO: grad norm: 4.929 4.818 1.039
2024-12-02-08:10:07-root-INFO: Loss too large (107.510->107.654)! Learning rate decreased to 0.09086.
2024-12-02-08:10:08-root-INFO: grad norm: 6.082 6.031 0.787
2024-12-02-08:10:08-root-INFO: Loss too large (107.128->107.249)! Learning rate decreased to 0.07268.
2024-12-02-08:10:09-root-INFO: grad norm: 4.917 4.872 0.660
2024-12-02-08:10:09-root-INFO: grad norm: 3.599 3.560 0.525
2024-12-02-08:10:09-root-INFO: grad norm: 3.470 3.431 0.522
2024-12-02-08:10:10-root-INFO: grad norm: 3.973 3.941 0.501
2024-12-02-08:10:10-root-INFO: grad norm: 3.861 3.822 0.547
2024-12-02-08:10:11-root-INFO: grad norm: 3.738 3.704 0.506
2024-12-02-08:10:11-root-INFO: Loss Change: 107.510 -> 104.610
2024-12-02-08:10:11-root-INFO: Regularization Change: 0.000 -> 1.678
2024-12-02-08:10:11-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-08:10:11-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-08:10:11-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-08:10:11-root-INFO: grad norm: 4.669 4.592 0.845
2024-12-02-08:10:12-root-INFO: Loss too large (104.657->105.044)! Learning rate decreased to 0.09324.
2024-12-02-08:10:12-root-INFO: grad norm: 7.004 6.968 0.710
2024-12-02-08:10:12-root-INFO: Loss too large (104.392->104.877)! Learning rate decreased to 0.07459.
2024-12-02-08:10:12-root-INFO: Loss too large (104.392->104.411)! Learning rate decreased to 0.05967.
2024-12-02-08:10:13-root-INFO: grad norm: 4.171 4.135 0.545
2024-12-02-08:10:13-root-INFO: grad norm: 2.154 2.115 0.410
2024-12-02-08:10:14-root-INFO: grad norm: 2.037 1.997 0.404
2024-12-02-08:10:14-root-INFO: grad norm: 2.019 1.979 0.397
2024-12-02-08:10:15-root-INFO: grad norm: 2.017 1.978 0.392
2024-12-02-08:10:15-root-INFO: grad norm: 2.021 1.983 0.392
2024-12-02-08:10:16-root-INFO: Loss Change: 104.657 -> 102.185
2024-12-02-08:10:16-root-INFO: Regularization Change: 0.000 -> 1.084
2024-12-02-08:10:16-root-INFO: Undo step: 85
2024-12-02-08:10:16-root-INFO: Undo step: 86
2024-12-02-08:10:16-root-INFO: Undo step: 87
2024-12-02-08:10:16-root-INFO: Undo step: 88
2024-12-02-08:10:16-root-INFO: Undo step: 89
2024-12-02-08:10:16-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-08:10:16-root-INFO: grad norm: 46.740 46.364 5.911
2024-12-02-08:10:16-root-INFO: grad norm: 26.190 25.939 3.612
2024-12-02-08:10:17-root-INFO: grad norm: 22.366 22.204 2.692
2024-12-02-08:10:17-root-INFO: grad norm: 23.444 23.277 2.794
2024-12-02-08:10:18-root-INFO: grad norm: 26.505 26.327 3.066
2024-12-02-08:10:18-root-INFO: Loss too large (154.975->155.985)! Learning rate decreased to 0.08180.
2024-12-02-08:10:18-root-INFO: grad norm: 21.022 20.804 3.019
2024-12-02-08:10:19-root-INFO: grad norm: 18.810 18.632 2.584
2024-12-02-08:10:19-root-INFO: grad norm: 18.456 18.222 2.926
2024-12-02-08:10:20-root-INFO: Loss Change: 313.485 -> 133.478
2024-12-02-08:10:20-root-INFO: Regularization Change: 0.000 -> 79.664
2024-12-02-08:10:20-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-08:10:20-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-08:10:20-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-08:10:20-root-INFO: grad norm: 23.036 22.681 4.030
2024-12-02-08:10:20-root-INFO: Loss too large (136.224->148.007)! Learning rate decreased to 0.08399.
2024-12-02-08:10:21-root-INFO: grad norm: 22.890 22.544 3.967
2024-12-02-08:10:21-root-INFO: grad norm: 22.810 22.528 3.573
2024-12-02-08:10:22-root-INFO: grad norm: 22.566 22.245 3.795
2024-12-02-08:10:22-root-INFO: grad norm: 22.419 22.130 3.590
2024-12-02-08:10:22-root-INFO: grad norm: 22.052 21.716 3.836
2024-12-02-08:10:23-root-INFO: grad norm: 21.578 21.285 3.541
2024-12-02-08:10:23-root-INFO: grad norm: 21.101 20.772 3.716
2024-12-02-08:10:24-root-INFO: Loss Change: 136.224 -> 124.860
2024-12-02-08:10:24-root-INFO: Regularization Change: 0.000 -> 7.225
2024-12-02-08:10:24-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-08:10:24-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-08:10:24-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-08:10:24-root-INFO: grad norm: 22.552 22.208 3.923
2024-12-02-08:10:24-root-INFO: Loss too large (127.087->137.694)! Learning rate decreased to 0.08623.
2024-12-02-08:10:25-root-INFO: grad norm: 20.896 20.573 3.659
2024-12-02-08:10:25-root-INFO: grad norm: 19.243 18.997 3.073
2024-12-02-08:10:26-root-INFO: grad norm: 18.001 17.750 2.998
2024-12-02-08:10:26-root-INFO: grad norm: 17.045 16.831 2.692
2024-12-02-08:10:27-root-INFO: grad norm: 16.445 16.212 2.755
2024-12-02-08:10:27-root-INFO: grad norm: 16.884 16.667 2.702
2024-12-02-08:10:27-root-INFO: Loss too large (117.476->117.619)! Learning rate decreased to 0.06899.
2024-12-02-08:10:28-root-INFO: grad norm: 10.430 10.266 1.845
2024-12-02-08:10:28-root-INFO: Loss Change: 127.087 -> 111.674
2024-12-02-08:10:28-root-INFO: Regularization Change: 0.000 -> 4.111
2024-12-02-08:10:28-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-08:10:28-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-08:10:28-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-08:10:28-root-INFO: grad norm: 9.847 9.623 2.089
2024-12-02-08:10:29-root-INFO: Loss too large (112.476->114.026)! Learning rate decreased to 0.08852.
2024-12-02-08:10:29-root-INFO: grad norm: 9.792 9.643 1.702
2024-12-02-08:10:30-root-INFO: grad norm: 10.108 9.965 1.693
2024-12-02-08:10:30-root-INFO: grad norm: 10.597 10.451 1.757
2024-12-02-08:10:31-root-INFO: grad norm: 11.669 11.513 1.905
2024-12-02-08:10:31-root-INFO: Loss too large (110.781->111.215)! Learning rate decreased to 0.07082.
2024-12-02-08:10:31-root-INFO: grad norm: 8.376 8.255 1.416
2024-12-02-08:10:32-root-INFO: grad norm: 6.281 6.193 1.044
2024-12-02-08:10:32-root-INFO: grad norm: 5.285 5.212 0.874
2024-12-02-08:10:33-root-INFO: Loss Change: 112.476 -> 106.856
2024-12-02-08:10:33-root-INFO: Regularization Change: 0.000 -> 2.550
2024-12-02-08:10:33-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-08:10:33-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-08:10:33-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-08:10:33-root-INFO: grad norm: 8.604 8.437 1.683
2024-12-02-08:10:33-root-INFO: Loss too large (107.527->109.310)! Learning rate decreased to 0.09086.
2024-12-02-08:10:33-root-INFO: Loss too large (107.527->108.050)! Learning rate decreased to 0.07268.
2024-12-02-08:10:34-root-INFO: grad norm: 5.683 5.600 0.970
2024-12-02-08:10:34-root-INFO: grad norm: 4.173 4.117 0.685
2024-12-02-08:10:35-root-INFO: grad norm: 3.967 3.933 0.517
2024-12-02-08:10:35-root-INFO: grad norm: 3.894 3.855 0.553
2024-12-02-08:10:36-root-INFO: grad norm: 4.479 4.446 0.538
2024-12-02-08:10:36-root-INFO: grad norm: 4.031 3.989 0.578
2024-12-02-08:10:37-root-INFO: grad norm: 2.950 2.911 0.476
2024-12-02-08:10:37-root-INFO: Loss Change: 107.527 -> 103.860
2024-12-02-08:10:37-root-INFO: Regularization Change: 0.000 -> 1.723
2024-12-02-08:10:37-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-08:10:37-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-08:10:37-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-08:10:38-root-INFO: grad norm: 4.178 4.085 0.877
2024-12-02-08:10:38-root-INFO: Loss too large (103.876->104.062)! Learning rate decreased to 0.09324.
2024-12-02-08:10:38-root-INFO: grad norm: 6.712 6.677 0.677
2024-12-02-08:10:38-root-INFO: Loss too large (103.611->104.103)! Learning rate decreased to 0.07459.
2024-12-02-08:10:39-root-INFO: Loss too large (103.611->103.730)! Learning rate decreased to 0.05967.
2024-12-02-08:10:39-root-INFO: grad norm: 4.131 4.093 0.558
2024-12-02-08:10:40-root-INFO: grad norm: 2.252 2.214 0.409
2024-12-02-08:10:40-root-INFO: grad norm: 2.246 2.209 0.407
2024-12-02-08:10:41-root-INFO: grad norm: 2.330 2.294 0.408
2024-12-02-08:10:41-root-INFO: grad norm: 2.559 2.526 0.411
2024-12-02-08:10:42-root-INFO: grad norm: 2.744 2.709 0.433
2024-12-02-08:10:42-root-INFO: Loss Change: 103.876 -> 101.514
2024-12-02-08:10:42-root-INFO: Regularization Change: 0.000 -> 1.120
2024-12-02-08:10:42-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-08:10:42-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-08:10:42-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-08:10:43-root-INFO: grad norm: 4.724 4.645 0.859
2024-12-02-08:10:43-root-INFO: Loss too large (101.492->102.107)! Learning rate decreased to 0.09566.
2024-12-02-08:10:43-root-INFO: Loss too large (101.492->101.731)! Learning rate decreased to 0.07653.
2024-12-02-08:10:43-root-INFO: grad norm: 4.437 4.388 0.660
2024-12-02-08:10:44-root-INFO: grad norm: 4.144 4.096 0.627
2024-12-02-08:10:44-root-INFO: Loss too large (100.799->100.804)! Learning rate decreased to 0.06122.
2024-12-02-08:10:44-root-INFO: grad norm: 3.717 3.680 0.525
2024-12-02-08:10:45-root-INFO: grad norm: 3.201 3.168 0.460
2024-12-02-08:10:45-root-INFO: grad norm: 3.280 3.246 0.468
2024-12-02-08:10:46-root-INFO: grad norm: 3.525 3.495 0.460
2024-12-02-08:10:46-root-INFO: grad norm: 3.506 3.472 0.487
2024-12-02-08:10:47-root-INFO: Loss Change: 101.492 -> 99.416
2024-12-02-08:10:47-root-INFO: Regularization Change: 0.000 -> 1.007
2024-12-02-08:10:47-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-08:10:47-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-08:10:47-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-08:10:47-root-INFO: grad norm: 5.960 5.847 1.158
2024-12-02-08:10:47-root-INFO: Loss too large (99.667->100.795)! Learning rate decreased to 0.09814.
2024-12-02-08:10:47-root-INFO: Loss too large (99.667->100.149)! Learning rate decreased to 0.07851.
2024-12-02-08:10:47-root-INFO: Loss too large (99.667->99.714)! Learning rate decreased to 0.06281.
2024-12-02-08:10:48-root-INFO: grad norm: 4.243 4.202 0.588
2024-12-02-08:10:48-root-INFO: grad norm: 2.371 2.328 0.447
2024-12-02-08:10:49-root-INFO: grad norm: 2.435 2.403 0.391
2024-12-02-08:10:49-root-INFO: grad norm: 2.856 2.827 0.405
2024-12-02-08:10:50-root-INFO: grad norm: 3.087 3.056 0.439
2024-12-02-08:10:50-root-INFO: grad norm: 3.649 3.621 0.450
2024-12-02-08:10:51-root-INFO: grad norm: 3.618 3.585 0.489
2024-12-02-08:10:51-root-INFO: Loss Change: 99.667 -> 97.572
2024-12-02-08:10:51-root-INFO: Regularization Change: 0.000 -> 0.886
2024-12-02-08:10:51-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-08:10:51-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-08:10:51-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-08:10:51-root-INFO: grad norm: 5.936 5.856 0.970
2024-12-02-08:10:52-root-INFO: Loss too large (97.953->99.080)! Learning rate decreased to 0.10066.
2024-12-02-08:10:52-root-INFO: Loss too large (97.953->98.570)! Learning rate decreased to 0.08053.
2024-12-02-08:10:52-root-INFO: Loss too large (97.953->98.187)! Learning rate decreased to 0.06442.
2024-12-02-08:10:52-root-INFO: grad norm: 4.182 4.141 0.581
2024-12-02-08:10:53-root-INFO: grad norm: 1.961 1.921 0.396
2024-12-02-08:10:53-root-INFO: grad norm: 1.858 1.825 0.346
2024-12-02-08:10:54-root-INFO: grad norm: 1.907 1.876 0.343
2024-12-02-08:10:54-root-INFO: grad norm: 2.017 1.986 0.351
2024-12-02-08:10:55-root-INFO: grad norm: 2.315 2.287 0.354
2024-12-02-08:10:55-root-INFO: grad norm: 2.619 2.590 0.391
2024-12-02-08:10:55-root-INFO: Loss Change: 97.953 -> 96.022
2024-12-02-08:10:55-root-INFO: Regularization Change: 0.000 -> 0.848
2024-12-02-08:10:55-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-08:10:55-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-08:10:56-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-08:10:56-root-INFO: grad norm: 5.819 5.715 1.097
2024-12-02-08:10:56-root-INFO: Loss too large (96.113->97.371)! Learning rate decreased to 0.10323.
2024-12-02-08:10:56-root-INFO: Loss too large (96.113->96.750)! Learning rate decreased to 0.08259.
2024-12-02-08:10:56-root-INFO: Loss too large (96.113->96.311)! Learning rate decreased to 0.06607.
2024-12-02-08:10:57-root-INFO: grad norm: 4.309 4.271 0.575
2024-12-02-08:10:57-root-INFO: grad norm: 2.349 2.309 0.435
2024-12-02-08:10:58-root-INFO: grad norm: 2.528 2.501 0.373
2024-12-02-08:10:58-root-INFO: grad norm: 3.284 3.256 0.421
2024-12-02-08:10:59-root-INFO: grad norm: 3.517 3.487 0.457
2024-12-02-08:10:59-root-INFO: grad norm: 4.077 4.049 0.478
2024-12-02-08:10:59-root-INFO: Loss too large (94.611->94.637)! Learning rate decreased to 0.05285.
2024-12-02-08:11:00-root-INFO: grad norm: 3.399 3.370 0.442
2024-12-02-08:11:00-root-INFO: Loss Change: 96.113 -> 94.207
2024-12-02-08:11:00-root-INFO: Regularization Change: 0.000 -> 0.777
2024-12-02-08:11:00-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-08:11:00-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-08:11:00-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-08:11:00-root-INFO: grad norm: 5.701 5.571 1.212
2024-12-02-08:11:00-root-INFO: Loss too large (94.768->96.081)! Learning rate decreased to 0.10585.
2024-12-02-08:11:01-root-INFO: Loss too large (94.768->95.315)! Learning rate decreased to 0.08468.
2024-12-02-08:11:01-root-INFO: Loss too large (94.768->94.815)! Learning rate decreased to 0.06774.
2024-12-02-08:11:01-root-INFO: grad norm: 4.313 4.274 0.578
2024-12-02-08:11:02-root-INFO: grad norm: 3.294 3.256 0.499
2024-12-02-08:11:02-root-INFO: grad norm: 3.503 3.473 0.458
2024-12-02-08:11:03-root-INFO: grad norm: 4.236 4.207 0.494
2024-12-02-08:11:03-root-INFO: Loss too large (93.596->93.654)! Learning rate decreased to 0.05420.
2024-12-02-08:11:03-root-INFO: grad norm: 3.505 3.476 0.449
2024-12-02-08:11:04-root-INFO: grad norm: 2.425 2.399 0.353
2024-12-02-08:11:04-root-INFO: grad norm: 2.372 2.347 0.347
2024-12-02-08:11:05-root-INFO: Loss Change: 94.768 -> 92.885
2024-12-02-08:11:05-root-INFO: Regularization Change: 0.000 -> 0.717
2024-12-02-08:11:05-root-INFO: Undo step: 80
2024-12-02-08:11:05-root-INFO: Undo step: 81
2024-12-02-08:11:05-root-INFO: Undo step: 82
2024-12-02-08:11:05-root-INFO: Undo step: 83
2024-12-02-08:11:05-root-INFO: Undo step: 84
2024-12-02-08:11:05-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-08:11:05-root-INFO: grad norm: 51.731 51.201 7.382
2024-12-02-08:11:05-root-INFO: grad norm: 33.830 33.596 3.970
2024-12-02-08:11:06-root-INFO: grad norm: 29.739 29.471 3.982
2024-12-02-08:11:06-root-INFO: grad norm: 28.450 28.234 3.500
2024-12-02-08:11:07-root-INFO: grad norm: 26.737 26.479 3.700
2024-12-02-08:11:07-root-INFO: grad norm: 25.535 25.291 3.525
2024-12-02-08:11:08-root-INFO: grad norm: 24.335 24.045 3.747
2024-12-02-08:11:08-root-INFO: grad norm: 24.010 23.703 3.829
2024-12-02-08:11:08-root-INFO: Loss too large (133.692->135.099)! Learning rate decreased to 0.09324.
2024-12-02-08:11:09-root-INFO: Loss Change: 283.666 -> 124.677
2024-12-02-08:11:09-root-INFO: Regularization Change: 0.000 -> 79.664
2024-12-02-08:11:09-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-08:11:09-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-08:11:09-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-08:11:09-root-INFO: grad norm: 16.078 15.867 2.597
2024-12-02-08:11:09-root-INFO: grad norm: 18.710 18.469 2.994
2024-12-02-08:11:09-root-INFO: Loss too large (122.547->126.092)! Learning rate decreased to 0.09566.
2024-12-02-08:11:10-root-INFO: grad norm: 15.057 14.847 2.504
2024-12-02-08:11:10-root-INFO: grad norm: 12.098 11.944 1.925
2024-12-02-08:11:11-root-INFO: grad norm: 10.648 10.503 1.757
2024-12-02-08:11:11-root-INFO: grad norm: 9.557 9.434 1.527
2024-12-02-08:11:12-root-INFO: grad norm: 8.851 8.731 1.451
2024-12-02-08:11:12-root-INFO: grad norm: 8.300 8.193 1.329
2024-12-02-08:11:13-root-INFO: Loss Change: 123.721 -> 107.253
2024-12-02-08:11:13-root-INFO: Regularization Change: 0.000 -> 8.250
2024-12-02-08:11:13-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-08:11:13-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-08:11:13-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-08:11:13-root-INFO: grad norm: 6.775 6.728 0.793
2024-12-02-08:11:14-root-INFO: grad norm: 9.205 9.140 1.100
2024-12-02-08:11:14-root-INFO: Loss too large (106.340->107.368)! Learning rate decreased to 0.09814.
2024-12-02-08:11:14-root-INFO: grad norm: 8.763 8.701 1.045
2024-12-02-08:11:15-root-INFO: grad norm: 8.358 8.292 1.051
2024-12-02-08:11:15-root-INFO: grad norm: 8.066 8.006 0.988
2024-12-02-08:11:16-root-INFO: grad norm: 7.860 7.798 0.989
2024-12-02-08:11:16-root-INFO: grad norm: 7.745 7.681 0.987
2024-12-02-08:11:17-root-INFO: grad norm: 7.805 7.740 1.011
2024-12-02-08:11:17-root-INFO: Loss Change: 106.622 -> 101.365
2024-12-02-08:11:17-root-INFO: Regularization Change: 0.000 -> 4.436
2024-12-02-08:11:17-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-08:11:17-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-08:11:17-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-08:11:17-root-INFO: grad norm: 7.369 7.333 0.732
2024-12-02-08:11:17-root-INFO: Loss too large (101.261->102.016)! Learning rate decreased to 0.10066.
2024-12-02-08:11:18-root-INFO: grad norm: 7.575 7.530 0.828
2024-12-02-08:11:18-root-INFO: grad norm: 7.489 7.433 0.911
2024-12-02-08:11:19-root-INFO: grad norm: 7.406 7.350 0.903
2024-12-02-08:11:19-root-INFO: grad norm: 7.434 7.371 0.968
2024-12-02-08:11:20-root-INFO: grad norm: 7.698 7.634 0.986
2024-12-02-08:11:20-root-INFO: Loss too large (98.730->98.845)! Learning rate decreased to 0.08053.
2024-12-02-08:11:21-root-INFO: grad norm: 5.457 5.397 0.805
2024-12-02-08:11:21-root-INFO: grad norm: 3.570 3.526 0.558
2024-12-02-08:11:21-root-INFO: Loss Change: 101.261 -> 96.704
2024-12-02-08:11:21-root-INFO: Regularization Change: 0.000 -> 2.542
2024-12-02-08:11:21-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-08:11:21-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-08:11:21-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-08:11:22-root-INFO: grad norm: 3.480 3.415 0.668
2024-12-02-08:11:22-root-INFO: grad norm: 5.150 5.119 0.569
2024-12-02-08:11:22-root-INFO: Loss too large (96.184->96.781)! Learning rate decreased to 0.10323.
2024-12-02-08:11:23-root-INFO: grad norm: 5.462 5.426 0.630
2024-12-02-08:11:23-root-INFO: grad norm: 6.025 5.996 0.592
2024-12-02-08:11:23-root-INFO: Loss too large (95.630->95.785)! Learning rate decreased to 0.08259.
2024-12-02-08:11:24-root-INFO: grad norm: 4.750 4.717 0.561
2024-12-02-08:11:24-root-INFO: grad norm: 3.585 3.559 0.425
2024-12-02-08:11:25-root-INFO: grad norm: 3.336 3.305 0.458
2024-12-02-08:11:25-root-INFO: grad norm: 3.592 3.566 0.437
2024-12-02-08:11:26-root-INFO: Loss Change: 96.474 -> 93.841
2024-12-02-08:11:26-root-INFO: Regularization Change: 0.000 -> 2.035
2024-12-02-08:11:26-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-08:11:26-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-08:11:26-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-08:11:26-root-INFO: grad norm: 4.842 4.759 0.892
2024-12-02-08:11:26-root-INFO: Loss too large (94.148->94.576)! Learning rate decreased to 0.10585.
2024-12-02-08:11:27-root-INFO: grad norm: 6.200 6.166 0.649
2024-12-02-08:11:27-root-INFO: Loss too large (93.853->94.189)! Learning rate decreased to 0.08468.
2024-12-02-08:11:27-root-INFO: grad norm: 4.544 4.508 0.569
2024-12-02-08:11:28-root-INFO: grad norm: 2.717 2.689 0.391
2024-12-02-08:11:28-root-INFO: grad norm: 2.321 2.292 0.365
2024-12-02-08:11:29-root-INFO: grad norm: 2.235 2.211 0.328
2024-12-02-08:11:29-root-INFO: grad norm: 2.359 2.331 0.363
2024-12-02-08:11:30-root-INFO: grad norm: 3.056 3.032 0.381
2024-12-02-08:11:30-root-INFO: Loss Change: 94.148 -> 91.664
2024-12-02-08:11:30-root-INFO: Regularization Change: 0.000 -> 1.549
2024-12-02-08:11:30-root-INFO: Undo step: 80
2024-12-02-08:11:30-root-INFO: Undo step: 81
2024-12-02-08:11:30-root-INFO: Undo step: 82
2024-12-02-08:11:30-root-INFO: Undo step: 83
2024-12-02-08:11:30-root-INFO: Undo step: 84
2024-12-02-08:11:30-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-08:11:30-root-INFO: grad norm: 44.384 43.981 5.969
2024-12-02-08:11:31-root-INFO: grad norm: 30.025 29.779 3.835
2024-12-02-08:11:31-root-INFO: grad norm: 29.345 28.962 4.729
2024-12-02-08:11:32-root-INFO: grad norm: 31.898 31.537 4.785
2024-12-02-08:11:32-root-INFO: Loss too large (158.482->158.931)! Learning rate decreased to 0.09324.
2024-12-02-08:11:32-root-INFO: grad norm: 23.563 23.146 4.414
2024-12-02-08:11:33-root-INFO: grad norm: 19.086 18.846 3.018
2024-12-02-08:11:33-root-INFO: grad norm: 16.602 16.310 3.099
2024-12-02-08:11:34-root-INFO: grad norm: 14.984 14.793 2.387
2024-12-02-08:11:34-root-INFO: Loss Change: 282.783 -> 118.917
2024-12-02-08:11:34-root-INFO: Regularization Change: 0.000 -> 71.417
2024-12-02-08:11:34-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-08:11:34-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-08:11:34-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-08:11:34-root-INFO: grad norm: 12.832 12.672 2.016
2024-12-02-08:11:35-root-INFO: Loss too large (117.759->119.708)! Learning rate decreased to 0.09566.
2024-12-02-08:11:35-root-INFO: grad norm: 12.573 12.420 1.952
2024-12-02-08:11:35-root-INFO: grad norm: 12.732 12.555 2.115
2024-12-02-08:11:36-root-INFO: grad norm: 14.257 14.076 2.259
2024-12-02-08:11:36-root-INFO: Loss too large (113.623->113.676)! Learning rate decreased to 0.07653.
2024-12-02-08:11:37-root-INFO: grad norm: 9.373 9.214 1.722
2024-12-02-08:11:37-root-INFO: grad norm: 6.383 6.297 1.045
2024-12-02-08:11:38-root-INFO: grad norm: 4.729 4.662 0.798
2024-12-02-08:11:38-root-INFO: grad norm: 3.920 3.858 0.694
2024-12-02-08:11:38-root-INFO: Loss Change: 117.759 -> 105.615
2024-12-02-08:11:38-root-INFO: Regularization Change: 0.000 -> 5.947
2024-12-02-08:11:38-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-08:11:38-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-08:11:38-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-08:11:39-root-INFO: grad norm: 4.333 4.222 0.973
2024-12-02-08:11:39-root-INFO: grad norm: 5.375 5.274 1.036
2024-12-02-08:11:40-root-INFO: grad norm: 11.127 11.003 1.660
2024-12-02-08:11:40-root-INFO: Loss too large (104.798->106.589)! Learning rate decreased to 0.09814.
2024-12-02-08:11:40-root-INFO: Loss too large (104.798->105.358)! Learning rate decreased to 0.07851.
2024-12-02-08:11:40-root-INFO: grad norm: 5.634 5.514 1.156
2024-12-02-08:11:41-root-INFO: grad norm: 4.269 4.216 0.672
2024-12-02-08:11:41-root-INFO: grad norm: 3.608 3.555 0.614
2024-12-02-08:11:42-root-INFO: grad norm: 3.103 3.048 0.580
2024-12-02-08:11:42-root-INFO: grad norm: 3.084 3.033 0.559
2024-12-02-08:11:42-root-INFO: Loss Change: 105.537 -> 100.389
2024-12-02-08:11:42-root-INFO: Regularization Change: 0.000 -> 3.633
2024-12-02-08:11:42-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-08:11:42-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-08:11:43-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-08:11:43-root-INFO: grad norm: 5.162 5.064 1.006
2024-12-02-08:11:43-root-INFO: Loss too large (100.709->101.054)! Learning rate decreased to 0.10066.
2024-12-02-08:11:43-root-INFO: grad norm: 5.285 5.194 0.977
2024-12-02-08:11:44-root-INFO: grad norm: 5.466 5.368 1.032
2024-12-02-08:11:44-root-INFO: grad norm: 6.379 6.274 1.155
2024-12-02-08:11:45-root-INFO: grad norm: 8.392 8.275 1.396
2024-12-02-08:11:45-root-INFO: Loss too large (99.402->100.155)! Learning rate decreased to 0.08053.
2024-12-02-08:11:45-root-INFO: grad norm: 6.372 6.271 1.132
2024-12-02-08:11:46-root-INFO: grad norm: 4.491 4.426 0.759
2024-12-02-08:11:46-root-INFO: grad norm: 3.619 3.576 0.558
2024-12-02-08:11:47-root-INFO: Loss Change: 100.709 -> 96.909
2024-12-02-08:11:47-root-INFO: Regularization Change: 0.000 -> 2.598
2024-12-02-08:11:47-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-08:11:47-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-08:11:47-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-08:11:47-root-INFO: grad norm: 5.364 5.229 1.197
2024-12-02-08:11:47-root-INFO: Loss too large (97.064->97.607)! Learning rate decreased to 0.10323.
2024-12-02-08:11:48-root-INFO: grad norm: 6.331 6.238 1.078
2024-12-02-08:11:48-root-INFO: Loss too large (96.828->97.051)! Learning rate decreased to 0.08259.
2024-12-02-08:11:48-root-INFO: grad norm: 6.143 6.064 0.981
2024-12-02-08:11:49-root-INFO: grad norm: 5.395 5.321 0.887
2024-12-02-08:11:49-root-INFO: grad norm: 4.404 4.340 0.750
2024-12-02-08:11:50-root-INFO: grad norm: 4.301 4.245 0.692
2024-12-02-08:11:50-root-INFO: grad norm: 4.831 4.779 0.703
2024-12-02-08:11:51-root-INFO: grad norm: 4.389 4.333 0.700
2024-12-02-08:11:51-root-INFO: Loss Change: 97.064 -> 93.949
2024-12-02-08:11:51-root-INFO: Regularization Change: 0.000 -> 1.857
2024-12-02-08:11:51-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-08:11:51-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-08:11:51-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-08:11:51-root-INFO: grad norm: 7.139 6.957 1.602
2024-12-02-08:11:51-root-INFO: Loss too large (94.781->96.730)! Learning rate decreased to 0.10585.
2024-12-02-08:11:52-root-INFO: Loss too large (94.781->95.357)! Learning rate decreased to 0.08468.
2024-12-02-08:11:52-root-INFO: grad norm: 5.823 5.735 1.010
2024-12-02-08:11:53-root-INFO: grad norm: 4.252 4.178 0.792
2024-12-02-08:11:53-root-INFO: grad norm: 4.058 4.002 0.672
2024-12-02-08:11:54-root-INFO: grad norm: 4.927 4.876 0.709
2024-12-02-08:11:54-root-INFO: Loss too large (92.941->92.976)! Learning rate decreased to 0.06774.
2024-12-02-08:11:54-root-INFO: grad norm: 3.801 3.757 0.572
2024-12-02-08:11:55-root-INFO: grad norm: 2.506 2.472 0.411
2024-12-02-08:11:55-root-INFO: grad norm: 2.444 2.414 0.383
2024-12-02-08:11:56-root-INFO: Loss Change: 94.781 -> 91.800
2024-12-02-08:11:56-root-INFO: Regularization Change: 0.000 -> 1.339
2024-12-02-08:11:56-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-08:11:56-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-08:11:56-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-08:11:56-root-INFO: grad norm: 4.462 4.343 1.024
2024-12-02-08:11:56-root-INFO: Loss too large (91.870->92.454)! Learning rate decreased to 0.10852.
2024-12-02-08:11:56-root-INFO: Loss too large (91.870->91.960)! Learning rate decreased to 0.08682.
2024-12-02-08:11:57-root-INFO: grad norm: 4.341 4.286 0.693
2024-12-02-08:11:57-root-INFO: grad norm: 5.251 5.195 0.767
2024-12-02-08:11:57-root-INFO: Loss too large (91.280->91.450)! Learning rate decreased to 0.06945.
2024-12-02-08:11:58-root-INFO: grad norm: 3.970 3.926 0.589
2024-12-02-08:11:58-root-INFO: grad norm: 2.249 2.214 0.396
2024-12-02-08:11:59-root-INFO: grad norm: 2.216 2.189 0.348
2024-12-02-08:11:59-root-INFO: grad norm: 2.355 2.329 0.347
2024-12-02-08:12:00-root-INFO: grad norm: 2.463 2.436 0.360
2024-12-02-08:12:00-root-INFO: Loss Change: 91.870 -> 89.764
2024-12-02-08:12:00-root-INFO: Regularization Change: 0.000 -> 1.100
2024-12-02-08:12:00-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-08:12:00-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-08:12:00-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-08:12:00-root-INFO: grad norm: 4.520 4.426 0.919
2024-12-02-08:12:01-root-INFO: Loss too large (90.153->90.976)! Learning rate decreased to 0.11124.
2024-12-02-08:12:01-root-INFO: Loss too large (90.153->90.475)! Learning rate decreased to 0.08899.
2024-12-02-08:12:01-root-INFO: grad norm: 4.459 4.405 0.692
2024-12-02-08:12:02-root-INFO: grad norm: 4.861 4.804 0.743
2024-12-02-08:12:02-root-INFO: Loss too large (89.610->89.791)! Learning rate decreased to 0.07119.
2024-12-02-08:12:02-root-INFO: grad norm: 3.918 3.876 0.574
2024-12-02-08:12:03-root-INFO: grad norm: 2.583 2.549 0.419
2024-12-02-08:12:03-root-INFO: grad norm: 2.660 2.632 0.386
2024-12-02-08:12:04-root-INFO: grad norm: 2.977 2.951 0.394
2024-12-02-08:12:04-root-INFO: grad norm: 3.073 3.044 0.419
2024-12-02-08:12:05-root-INFO: Loss Change: 90.153 -> 88.263
2024-12-02-08:12:05-root-INFO: Regularization Change: 0.000 -> 1.036
2024-12-02-08:12:05-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-08:12:05-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-08:12:05-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-08:12:05-root-INFO: grad norm: 5.646 5.544 1.067
2024-12-02-08:12:05-root-INFO: Loss too large (88.310->89.667)! Learning rate decreased to 0.11401.
2024-12-02-08:12:05-root-INFO: Loss too large (88.310->88.993)! Learning rate decreased to 0.09121.
2024-12-02-08:12:05-root-INFO: Loss too large (88.310->88.512)! Learning rate decreased to 0.07296.
2024-12-02-08:12:06-root-INFO: grad norm: 4.187 4.140 0.627
2024-12-02-08:12:06-root-INFO: grad norm: 2.353 2.313 0.428
2024-12-02-08:12:07-root-INFO: grad norm: 2.405 2.377 0.369
2024-12-02-08:12:07-root-INFO: grad norm: 2.827 2.803 0.373
2024-12-02-08:12:08-root-INFO: grad norm: 3.034 3.006 0.415
2024-12-02-08:12:08-root-INFO: grad norm: 3.477 3.452 0.419
2024-12-02-08:12:09-root-INFO: grad norm: 3.474 3.443 0.466
2024-12-02-08:12:09-root-INFO: Loss Change: 88.310 -> 86.429
2024-12-02-08:12:09-root-INFO: Regularization Change: 0.000 -> 0.914
2024-12-02-08:12:09-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-08:12:09-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-08:12:09-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-08:12:09-root-INFO: grad norm: 6.709 6.600 1.203
2024-12-02-08:12:10-root-INFO: Loss too large (86.918->88.701)! Learning rate decreased to 0.11682.
2024-12-02-08:12:10-root-INFO: Loss too large (86.918->87.855)! Learning rate decreased to 0.09346.
2024-12-02-08:12:10-root-INFO: Loss too large (86.918->87.251)! Learning rate decreased to 0.07477.
2024-12-02-08:12:10-root-INFO: grad norm: 4.311 4.261 0.655
2024-12-02-08:12:11-root-INFO: grad norm: 1.987 1.953 0.368
2024-12-02-08:12:11-root-INFO: grad norm: 1.650 1.627 0.273
2024-12-02-08:12:12-root-INFO: grad norm: 1.596 1.573 0.267
2024-12-02-08:12:12-root-INFO: grad norm: 1.608 1.586 0.263
2024-12-02-08:12:13-root-INFO: grad norm: 1.654 1.632 0.267
2024-12-02-08:12:13-root-INFO: grad norm: 1.776 1.755 0.271
2024-12-02-08:12:13-root-INFO: Loss Change: 86.918 -> 84.791
2024-12-02-08:12:14-root-INFO: Regularization Change: 0.000 -> 0.948
2024-12-02-08:12:14-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-08:12:14-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-08:12:14-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-08:12:14-root-INFO: grad norm: 2.836 2.767 0.623
2024-12-02-08:12:14-root-INFO: grad norm: 4.658 4.619 0.600
2024-12-02-08:12:14-root-INFO: Loss too large (84.775->85.744)! Learning rate decreased to 0.11969.
2024-12-02-08:12:15-root-INFO: Loss too large (84.775->85.092)! Learning rate decreased to 0.09575.
2024-12-02-08:12:15-root-INFO: grad norm: 4.330 4.304 0.474
2024-12-02-08:12:16-root-INFO: grad norm: 5.271 5.249 0.481
2024-12-02-08:12:16-root-INFO: Loss too large (84.273->84.627)! Learning rate decreased to 0.07660.
2024-12-02-08:12:16-root-INFO: Loss too large (84.273->84.326)! Learning rate decreased to 0.06128.
2024-12-02-08:12:16-root-INFO: grad norm: 3.580 3.553 0.439
2024-12-02-08:12:17-root-INFO: grad norm: 1.874 1.851 0.296
2024-12-02-08:12:17-root-INFO: grad norm: 1.804 1.783 0.273
2024-12-02-08:12:18-root-INFO: grad norm: 1.763 1.742 0.266
2024-12-02-08:12:18-root-INFO: Loss Change: 84.896 -> 83.252
2024-12-02-08:12:18-root-INFO: Regularization Change: 0.000 -> 0.984
2024-12-02-08:12:18-root-INFO: Undo step: 75
2024-12-02-08:12:18-root-INFO: Undo step: 76
2024-12-02-08:12:18-root-INFO: Undo step: 77
2024-12-02-08:12:18-root-INFO: Undo step: 78
2024-12-02-08:12:18-root-INFO: Undo step: 79
2024-12-02-08:12:18-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-08:12:19-root-INFO: grad norm: 39.546 39.108 5.868
2024-12-02-08:12:19-root-INFO: grad norm: 29.111 28.850 3.890
2024-12-02-08:12:19-root-INFO: grad norm: 25.900 25.570 4.121
2024-12-02-08:12:20-root-INFO: grad norm: 28.458 28.077 4.637
2024-12-02-08:12:20-root-INFO: Loss too large (142.297->143.165)! Learning rate decreased to 0.10585.
2024-12-02-08:12:21-root-INFO: grad norm: 20.606 20.245 3.841
2024-12-02-08:12:21-root-INFO: grad norm: 15.616 15.385 2.680
2024-12-02-08:12:21-root-INFO: grad norm: 13.168 12.947 2.400
2024-12-02-08:12:22-root-INFO: grad norm: 12.343 12.141 2.227
2024-12-02-08:12:22-root-INFO: Loss Change: 260.778 -> 106.713
2024-12-02-08:12:22-root-INFO: Regularization Change: 0.000 -> 78.549
2024-12-02-08:12:22-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-08:12:22-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-08:12:22-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-08:12:23-root-INFO: grad norm: 10.459 10.326 1.665
2024-12-02-08:12:23-root-INFO: Loss too large (105.283->106.575)! Learning rate decreased to 0.10852.
2024-12-02-08:12:23-root-INFO: grad norm: 10.965 10.792 1.938
2024-12-02-08:12:24-root-INFO: grad norm: 10.973 10.795 1.969
2024-12-02-08:12:24-root-INFO: grad norm: 10.909 10.735 1.941
2024-12-02-08:12:25-root-INFO: grad norm: 11.262 11.094 1.938
2024-12-02-08:12:25-root-INFO: grad norm: 12.672 12.489 2.145
2024-12-02-08:12:25-root-INFO: Loss too large (99.974->100.430)! Learning rate decreased to 0.08682.
2024-12-02-08:12:26-root-INFO: grad norm: 8.461 8.313 1.574
2024-12-02-08:12:26-root-INFO: grad norm: 5.632 5.551 0.952
2024-12-02-08:12:27-root-INFO: Loss Change: 105.283 -> 94.759
2024-12-02-08:12:27-root-INFO: Regularization Change: 0.000 -> 6.497
2024-12-02-08:12:27-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-08:12:27-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-08:12:27-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-08:12:27-root-INFO: grad norm: 3.462 3.411 0.591
2024-12-02-08:12:27-root-INFO: grad norm: 4.208 4.169 0.568
2024-12-02-08:12:28-root-INFO: grad norm: 8.391 8.365 0.652
2024-12-02-08:12:28-root-INFO: Loss too large (93.869->95.018)! Learning rate decreased to 0.11124.
2024-12-02-08:12:28-root-INFO: Loss too large (93.869->94.145)! Learning rate decreased to 0.08899.
2024-12-02-08:12:29-root-INFO: grad norm: 4.615 4.578 0.589
2024-12-02-08:12:29-root-INFO: grad norm: 3.798 3.764 0.507
2024-12-02-08:12:30-root-INFO: grad norm: 2.656 2.618 0.447
2024-12-02-08:12:30-root-INFO: grad norm: 2.416 2.381 0.411
2024-12-02-08:12:31-root-INFO: grad norm: 2.338 2.302 0.409
2024-12-02-08:12:31-root-INFO: Loss Change: 94.713 -> 90.303
2024-12-02-08:12:31-root-INFO: Regularization Change: 0.000 -> 3.497
2024-12-02-08:12:31-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-08:12:31-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-08:12:31-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-08:12:31-root-INFO: grad norm: 3.006 2.911 0.748
2024-12-02-08:12:32-root-INFO: grad norm: 3.649 3.570 0.753
2024-12-02-08:12:32-root-INFO: grad norm: 5.899 5.787 1.143
2024-12-02-08:12:32-root-INFO: Loss too large (89.420->90.782)! Learning rate decreased to 0.11401.
2024-12-02-08:12:32-root-INFO: Loss too large (89.420->89.666)! Learning rate decreased to 0.09121.
2024-12-02-08:12:33-root-INFO: grad norm: 5.246 5.155 0.973
2024-12-02-08:12:33-root-INFO: grad norm: 6.163 6.099 0.886
2024-12-02-08:12:33-root-INFO: Loss too large (88.558->88.737)! Learning rate decreased to 0.07296.
2024-12-02-08:12:34-root-INFO: grad norm: 4.172 4.115 0.682
2024-12-02-08:12:34-root-INFO: grad norm: 2.126 2.088 0.404
2024-12-02-08:12:35-root-INFO: grad norm: 1.901 1.870 0.346
2024-12-02-08:12:35-root-INFO: Loss Change: 90.052 -> 87.046
2024-12-02-08:12:35-root-INFO: Regularization Change: 0.000 -> 2.193
2024-12-02-08:12:35-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-08:12:35-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-08:12:35-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-08:12:36-root-INFO: grad norm: 4.391 4.280 0.984
2024-12-02-08:12:36-root-INFO: Loss too large (87.284->87.861)! Learning rate decreased to 0.11682.
2024-12-02-08:12:36-root-INFO: Loss too large (87.284->87.345)! Learning rate decreased to 0.09346.
2024-12-02-08:12:36-root-INFO: grad norm: 4.279 4.224 0.681
2024-12-02-08:12:37-root-INFO: grad norm: 5.319 5.267 0.736
2024-12-02-08:12:37-root-INFO: Loss too large (86.658->86.855)! Learning rate decreased to 0.07477.
2024-12-02-08:12:37-root-INFO: grad norm: 4.000 3.954 0.603
2024-12-02-08:12:38-root-INFO: grad norm: 2.239 2.203 0.397
2024-12-02-08:12:38-root-INFO: grad norm: 2.167 2.138 0.351
2024-12-02-08:12:39-root-INFO: grad norm: 2.293 2.268 0.340
2024-12-02-08:12:39-root-INFO: grad norm: 2.410 2.383 0.361
2024-12-02-08:12:40-root-INFO: Loss Change: 87.284 -> 85.013
2024-12-02-08:12:40-root-INFO: Regularization Change: 0.000 -> 1.282
2024-12-02-08:12:40-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-08:12:40-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-08:12:40-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-08:12:40-root-INFO: grad norm: 4.148 4.060 0.849
2024-12-02-08:12:40-root-INFO: Loss too large (85.167->85.832)! Learning rate decreased to 0.11969.
2024-12-02-08:12:40-root-INFO: Loss too large (85.167->85.370)! Learning rate decreased to 0.09575.
2024-12-02-08:12:41-root-INFO: grad norm: 4.213 4.162 0.649
2024-12-02-08:12:41-root-INFO: grad norm: 5.256 5.207 0.720
2024-12-02-08:12:41-root-INFO: Loss too large (84.672->84.956)! Learning rate decreased to 0.07660.
2024-12-02-08:12:42-root-INFO: grad norm: 4.004 3.960 0.595
2024-12-02-08:12:42-root-INFO: grad norm: 2.189 2.154 0.391
2024-12-02-08:12:43-root-INFO: grad norm: 2.148 2.120 0.346
2024-12-02-08:12:43-root-INFO: grad norm: 2.373 2.349 0.337
2024-12-02-08:12:44-root-INFO: grad norm: 2.558 2.532 0.367
2024-12-02-08:12:44-root-INFO: Loss Change: 85.167 -> 83.190
2024-12-02-08:12:44-root-INFO: Regularization Change: 0.000 -> 1.174
2024-12-02-08:12:44-root-INFO: Undo step: 75
2024-12-02-08:12:44-root-INFO: Undo step: 76
2024-12-02-08:12:44-root-INFO: Undo step: 77
2024-12-02-08:12:44-root-INFO: Undo step: 78
2024-12-02-08:12:44-root-INFO: Undo step: 79
2024-12-02-08:12:44-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-08:12:44-root-INFO: grad norm: 39.178 38.745 5.813
2024-12-02-08:12:45-root-INFO: grad norm: 23.543 23.269 3.584
2024-12-02-08:12:45-root-INFO: grad norm: 23.484 23.184 3.736
2024-12-02-08:12:46-root-INFO: grad norm: 26.573 26.269 4.006
2024-12-02-08:12:46-root-INFO: Loss too large (138.261->139.200)! Learning rate decreased to 0.10585.
2024-12-02-08:12:46-root-INFO: grad norm: 20.705 20.400 3.542
2024-12-02-08:12:47-root-INFO: grad norm: 18.839 18.601 2.983
2024-12-02-08:12:47-root-INFO: grad norm: 16.498 16.247 2.870
2024-12-02-08:12:48-root-INFO: grad norm: 15.412 15.232 2.347
2024-12-02-08:12:48-root-INFO: Loss Change: 246.076 -> 108.071
2024-12-02-08:12:48-root-INFO: Regularization Change: 0.000 -> 71.483
2024-12-02-08:12:48-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-08:12:48-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-08:12:48-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-08:12:48-root-INFO: grad norm: 13.472 13.350 1.810
2024-12-02-08:12:48-root-INFO: Loss too large (106.686->110.104)! Learning rate decreased to 0.10852.
2024-12-02-08:12:49-root-INFO: grad norm: 13.674 13.511 2.105
2024-12-02-08:12:49-root-INFO: grad norm: 13.288 13.116 2.134
2024-12-02-08:12:50-root-INFO: grad norm: 12.976 12.815 2.039
2024-12-02-08:12:50-root-INFO: grad norm: 12.774 12.625 1.946
2024-12-02-08:12:51-root-INFO: grad norm: 12.818 12.667 1.962
2024-12-02-08:12:51-root-INFO: grad norm: 13.230 13.065 2.080
2024-12-02-08:12:51-root-INFO: Loss too large (99.459->99.863)! Learning rate decreased to 0.08682.
2024-12-02-08:12:52-root-INFO: grad norm: 11.364 11.253 1.583
2024-12-02-08:12:52-root-INFO: Loss Change: 106.686 -> 95.862
2024-12-02-08:12:52-root-INFO: Regularization Change: 0.000 -> 6.970
2024-12-02-08:12:52-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-08:12:52-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-08:12:52-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-08:12:52-root-INFO: grad norm: 5.743 5.686 0.803
2024-12-02-08:12:53-root-INFO: grad norm: 7.744 7.654 1.180
2024-12-02-08:12:53-root-INFO: Loss too large (94.427->95.417)! Learning rate decreased to 0.11124.
2024-12-02-08:12:54-root-INFO: grad norm: 7.871 7.806 1.011
2024-12-02-08:12:54-root-INFO: grad norm: 8.461 8.395 1.053
2024-12-02-08:12:54-root-INFO: grad norm: 10.450 10.404 0.981
2024-12-02-08:12:55-root-INFO: Loss too large (93.273->93.867)! Learning rate decreased to 0.08899.
2024-12-02-08:12:55-root-INFO: grad norm: 6.621 6.583 0.705
2024-12-02-08:12:56-root-INFO: grad norm: 4.923 4.884 0.621
2024-12-02-08:12:56-root-INFO: grad norm: 4.952 4.907 0.666
2024-12-02-08:12:56-root-INFO: Loss Change: 95.521 -> 89.959
2024-12-02-08:12:56-root-INFO: Regularization Change: 0.000 -> 3.742
2024-12-02-08:12:56-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-08:12:56-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-08:12:57-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-08:12:57-root-INFO: grad norm: 4.070 4.030 0.568
2024-12-02-08:12:57-root-INFO: Loss too large (89.455->89.614)! Learning rate decreased to 0.11401.
2024-12-02-08:12:57-root-INFO: grad norm: 6.889 6.858 0.645
2024-12-02-08:12:58-root-INFO: Loss too large (88.909->89.595)! Learning rate decreased to 0.09121.
2024-12-02-08:12:58-root-INFO: Loss too large (88.909->89.238)! Learning rate decreased to 0.07296.
2024-12-02-08:12:58-root-INFO: Loss too large (88.909->88.939)! Learning rate decreased to 0.05837.
2024-12-02-08:12:58-root-INFO: grad norm: 3.830 3.791 0.543
2024-12-02-08:12:59-root-INFO: grad norm: 2.423 2.387 0.418
2024-12-02-08:12:59-root-INFO: grad norm: 2.214 2.179 0.392
2024-12-02-08:13:00-root-INFO: grad norm: 2.168 2.133 0.390
2024-12-02-08:13:00-root-INFO: grad norm: 2.143 2.110 0.377
2024-12-02-08:13:01-root-INFO: grad norm: 2.132 2.098 0.381
2024-12-02-08:13:01-root-INFO: Loss Change: 89.455 -> 86.826
2024-12-02-08:13:01-root-INFO: Regularization Change: 0.000 -> 1.173
2024-12-02-08:13:01-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-08:13:01-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-08:13:01-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-08:13:01-root-INFO: grad norm: 4.973 4.869 1.010
2024-12-02-08:13:01-root-INFO: Loss too large (87.081->87.993)! Learning rate decreased to 0.11682.
2024-12-02-08:13:02-root-INFO: Loss too large (87.081->87.447)! Learning rate decreased to 0.09346.
2024-12-02-08:13:02-root-INFO: grad norm: 4.609 4.556 0.702
2024-12-02-08:13:03-root-INFO: grad norm: 4.354 4.297 0.702
2024-12-02-08:13:03-root-INFO: Loss too large (86.240->86.324)! Learning rate decreased to 0.07477.
2024-12-02-08:13:03-root-INFO: grad norm: 3.927 3.888 0.554
2024-12-02-08:13:04-root-INFO: grad norm: 3.677 3.645 0.486
2024-12-02-08:13:04-root-INFO: grad norm: 3.688 3.656 0.488
2024-12-02-08:13:05-root-INFO: grad norm: 3.814 3.785 0.469
2024-12-02-08:13:05-root-INFO: grad norm: 3.749 3.717 0.490
2024-12-02-08:13:05-root-INFO: Loss Change: 87.081 -> 84.666
2024-12-02-08:13:05-root-INFO: Regularization Change: 0.000 -> 1.394
2024-12-02-08:13:05-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-08:13:05-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-08:13:06-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-08:13:06-root-INFO: grad norm: 5.244 5.151 0.984
2024-12-02-08:13:06-root-INFO: Loss too large (84.876->86.107)! Learning rate decreased to 0.11969.
2024-12-02-08:13:06-root-INFO: Loss too large (84.876->85.494)! Learning rate decreased to 0.09575.
2024-12-02-08:13:06-root-INFO: Loss too large (84.876->85.064)! Learning rate decreased to 0.07660.
2024-12-02-08:13:07-root-INFO: grad norm: 4.129 4.091 0.562
2024-12-02-08:13:07-root-INFO: grad norm: 2.660 2.618 0.471
2024-12-02-08:13:08-root-INFO: grad norm: 2.933 2.904 0.413
2024-12-02-08:13:08-root-INFO: grad norm: 4.180 4.154 0.466
2024-12-02-08:13:08-root-INFO: Loss too large (83.681->83.751)! Learning rate decreased to 0.06128.
2024-12-02-08:13:09-root-INFO: grad norm: 3.514 3.486 0.445
2024-12-02-08:13:09-root-INFO: grad norm: 2.442 2.416 0.360
2024-12-02-08:13:10-root-INFO: grad norm: 2.455 2.430 0.354
2024-12-02-08:13:10-root-INFO: Loss Change: 84.876 -> 82.870
2024-12-02-08:13:10-root-INFO: Regularization Change: 0.000 -> 0.935
2024-12-02-08:13:10-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-08:13:10-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-08:13:10-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-08:13:10-root-INFO: grad norm: 4.737 4.658 0.859
2024-12-02-08:13:10-root-INFO: Loss too large (83.142->84.282)! Learning rate decreased to 0.12261.
2024-12-02-08:13:11-root-INFO: Loss too large (83.142->83.747)! Learning rate decreased to 0.09809.
2024-12-02-08:13:11-root-INFO: Loss too large (83.142->83.363)! Learning rate decreased to 0.07847.
2024-12-02-08:13:11-root-INFO: grad norm: 4.090 4.055 0.537
2024-12-02-08:13:12-root-INFO: grad norm: 3.386 3.351 0.483
2024-12-02-08:13:12-root-INFO: grad norm: 3.645 3.615 0.468
2024-12-02-08:13:13-root-INFO: grad norm: 4.576 4.549 0.497
2024-12-02-08:13:13-root-INFO: Loss too large (82.155->82.312)! Learning rate decreased to 0.06278.
2024-12-02-08:13:13-root-INFO: grad norm: 3.653 3.625 0.452
2024-12-02-08:13:14-root-INFO: grad norm: 2.188 2.162 0.339
2024-12-02-08:13:14-root-INFO: grad norm: 2.234 2.210 0.326
2024-12-02-08:13:15-root-INFO: Loss Change: 83.142 -> 81.344
2024-12-02-08:13:15-root-INFO: Regularization Change: 0.000 -> 0.877
2024-12-02-08:13:15-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-08:13:15-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-08:13:15-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-08:13:15-root-INFO: grad norm: 4.479 4.395 0.865
2024-12-02-08:13:15-root-INFO: Loss too large (81.570->82.622)! Learning rate decreased to 0.12558.
2024-12-02-08:13:15-root-INFO: Loss too large (81.570->82.109)! Learning rate decreased to 0.10047.
2024-12-02-08:13:16-root-INFO: Loss too large (81.570->81.746)! Learning rate decreased to 0.08037.
2024-12-02-08:13:16-root-INFO: grad norm: 4.034 4.002 0.504
2024-12-02-08:13:17-root-INFO: grad norm: 4.062 4.028 0.521
2024-12-02-08:13:17-root-INFO: Loss too large (80.989->81.095)! Learning rate decreased to 0.06430.
2024-12-02-08:13:17-root-INFO: grad norm: 3.535 3.510 0.426
2024-12-02-08:13:18-root-INFO: grad norm: 2.713 2.688 0.366
2024-12-02-08:13:18-root-INFO: grad norm: 2.788 2.766 0.356
2024-12-02-08:13:19-root-INFO: grad norm: 3.005 2.984 0.356
2024-12-02-08:13:19-root-INFO: grad norm: 3.050 3.027 0.374
2024-12-02-08:13:19-root-INFO: Loss Change: 81.570 -> 79.969
2024-12-02-08:13:19-root-INFO: Regularization Change: 0.000 -> 0.774
2024-12-02-08:13:19-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-08:13:19-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-08:13:20-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-08:13:20-root-INFO: grad norm: 5.397 5.313 0.944
2024-12-02-08:13:20-root-INFO: Loss too large (80.184->81.772)! Learning rate decreased to 0.12860.
2024-12-02-08:13:20-root-INFO: Loss too large (80.184->81.107)! Learning rate decreased to 0.10288.
2024-12-02-08:13:20-root-INFO: Loss too large (80.184->80.625)! Learning rate decreased to 0.08231.
2024-12-02-08:13:20-root-INFO: Loss too large (80.184->80.273)! Learning rate decreased to 0.06585.
2024-12-02-08:13:21-root-INFO: grad norm: 3.894 3.866 0.459
2024-12-02-08:13:21-root-INFO: grad norm: 2.173 2.140 0.378
2024-12-02-08:13:22-root-INFO: grad norm: 2.273 2.251 0.321
2024-12-02-08:13:22-root-INFO: grad norm: 2.643 2.621 0.337
2024-12-02-08:13:23-root-INFO: grad norm: 2.841 2.819 0.351
2024-12-02-08:13:23-root-INFO: grad norm: 3.266 3.245 0.362
2024-12-02-08:13:24-root-INFO: grad norm: 3.295 3.272 0.386
2024-12-02-08:13:24-root-INFO: Loss Change: 80.184 -> 78.605
2024-12-02-08:13:24-root-INFO: Regularization Change: 0.000 -> 0.687
2024-12-02-08:13:24-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-08:13:24-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-08:13:24-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-08:13:24-root-INFO: grad norm: 4.856 4.813 0.646
2024-12-02-08:13:24-root-INFO: Loss too large (78.857->80.196)! Learning rate decreased to 0.13168.
2024-12-02-08:13:25-root-INFO: Loss too large (78.857->79.730)! Learning rate decreased to 0.10534.
2024-12-02-08:13:25-root-INFO: Loss too large (78.857->79.359)! Learning rate decreased to 0.08427.
2024-12-02-08:13:25-root-INFO: Loss too large (78.857->79.060)! Learning rate decreased to 0.06742.
2024-12-02-08:13:25-root-INFO: grad norm: 3.819 3.794 0.437
2024-12-02-08:13:26-root-INFO: grad norm: 2.251 2.225 0.341
2024-12-02-08:13:26-root-INFO: grad norm: 2.402 2.381 0.316
2024-12-02-08:13:27-root-INFO: grad norm: 2.858 2.839 0.327
2024-12-02-08:13:27-root-INFO: grad norm: 3.028 3.007 0.359
2024-12-02-08:13:28-root-INFO: grad norm: 3.388 3.370 0.356
2024-12-02-08:13:28-root-INFO: grad norm: 3.364 3.341 0.389
2024-12-02-08:13:28-root-INFO: Loss Change: 78.857 -> 77.478
2024-12-02-08:13:28-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-02-08:13:28-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-08:13:28-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-08:13:29-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-08:13:29-root-INFO: grad norm: 6.543 6.435 1.182
2024-12-02-08:13:29-root-INFO: Loss too large (77.838->79.995)! Learning rate decreased to 0.13480.
2024-12-02-08:13:29-root-INFO: Loss too large (77.838->79.092)! Learning rate decreased to 0.10784.
2024-12-02-08:13:29-root-INFO: Loss too large (77.838->78.457)! Learning rate decreased to 0.08627.
2024-12-02-08:13:29-root-INFO: Loss too large (77.838->77.998)! Learning rate decreased to 0.06902.
2024-12-02-08:13:30-root-INFO: grad norm: 4.026 3.996 0.489
2024-12-02-08:13:30-root-INFO: grad norm: 1.761 1.725 0.353
2024-12-02-08:13:31-root-INFO: grad norm: 1.559 1.533 0.283
2024-12-02-08:13:31-root-INFO: grad norm: 1.553 1.529 0.270
2024-12-02-08:13:32-root-INFO: grad norm: 1.620 1.597 0.269
2024-12-02-08:13:32-root-INFO: grad norm: 1.744 1.723 0.269
2024-12-02-08:13:33-root-INFO: grad norm: 2.042 2.023 0.279
2024-12-02-08:13:33-root-INFO: Loss Change: 77.838 -> 76.008
2024-12-02-08:13:33-root-INFO: Regularization Change: 0.000 -> 0.758
2024-12-02-08:13:33-root-INFO: Undo step: 70
2024-12-02-08:13:33-root-INFO: Undo step: 71
2024-12-02-08:13:33-root-INFO: Undo step: 72
2024-12-02-08:13:33-root-INFO: Undo step: 73
2024-12-02-08:13:33-root-INFO: Undo step: 74
2024-12-02-08:13:33-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-08:13:33-root-INFO: grad norm: 38.025 37.770 4.398
2024-12-02-08:13:34-root-INFO: grad norm: 23.422 23.250 2.829
2024-12-02-08:13:34-root-INFO: grad norm: 18.565 18.433 2.214
2024-12-02-08:13:35-root-INFO: grad norm: 17.960 17.844 2.036
2024-12-02-08:13:35-root-INFO: grad norm: 18.345 18.176 2.485
2024-12-02-08:13:36-root-INFO: grad norm: 20.079 19.872 2.876
2024-12-02-08:13:36-root-INFO: Loss too large (114.036->114.894)! Learning rate decreased to 0.11969.
2024-12-02-08:13:36-root-INFO: grad norm: 14.715 14.502 2.498
2024-12-02-08:13:37-root-INFO: grad norm: 11.664 11.506 1.914
2024-12-02-08:13:37-root-INFO: Loss Change: 221.148 -> 97.396
2024-12-02-08:13:37-root-INFO: Regularization Change: 0.000 -> 78.314
2024-12-02-08:13:37-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-08:13:37-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-08:13:37-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-08:13:37-root-INFO: grad norm: 8.828 8.722 1.364
2024-12-02-08:13:38-root-INFO: Loss too large (96.490->96.586)! Learning rate decreased to 0.12261.
2024-12-02-08:13:38-root-INFO: grad norm: 8.087 7.978 1.326
2024-12-02-08:13:39-root-INFO: grad norm: 7.619 7.506 1.307
2024-12-02-08:13:39-root-INFO: grad norm: 7.325 7.220 1.233
2024-12-02-08:13:40-root-INFO: grad norm: 7.127 7.021 1.229
2024-12-02-08:13:40-root-INFO: grad norm: 7.006 6.906 1.182
2024-12-02-08:13:41-root-INFO: grad norm: 6.946 6.843 1.188
2024-12-02-08:13:41-root-INFO: grad norm: 6.929 6.831 1.165
2024-12-02-08:13:41-root-INFO: Loss Change: 96.490 -> 87.303
2024-12-02-08:13:41-root-INFO: Regularization Change: 0.000 -> 7.968
2024-12-02-08:13:41-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-08:13:41-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-08:13:42-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-08:13:42-root-INFO: grad norm: 5.589 5.539 0.747
2024-12-02-08:13:42-root-INFO: grad norm: 7.755 7.661 1.206
2024-12-02-08:13:42-root-INFO: Loss too large (86.572->87.829)! Learning rate decreased to 0.12558.
2024-12-02-08:13:43-root-INFO: grad norm: 7.720 7.622 1.229
2024-12-02-08:13:43-root-INFO: grad norm: 7.832 7.721 1.315
2024-12-02-08:13:44-root-INFO: grad norm: 8.005 7.888 1.365
2024-12-02-08:13:44-root-INFO: grad norm: 8.548 8.421 1.468
2024-12-02-08:13:45-root-INFO: Loss too large (84.486->84.508)! Learning rate decreased to 0.10047.
2024-12-02-08:13:45-root-INFO: grad norm: 5.856 5.759 1.059
2024-12-02-08:13:46-root-INFO: grad norm: 3.949 3.885 0.709
2024-12-02-08:13:46-root-INFO: Loss Change: 86.660 -> 81.537
2024-12-02-08:13:46-root-INFO: Regularization Change: 0.000 -> 4.162
2024-12-02-08:13:46-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-08:13:46-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-08:13:46-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-08:13:46-root-INFO: grad norm: 2.419 2.374 0.465
2024-12-02-08:13:47-root-INFO: grad norm: 2.979 2.953 0.393
2024-12-02-08:13:47-root-INFO: grad norm: 3.760 3.723 0.525
2024-12-02-08:13:48-root-INFO: grad norm: 6.694 6.642 0.831
2024-12-02-08:13:48-root-INFO: Loss too large (80.332->81.585)! Learning rate decreased to 0.12860.
2024-12-02-08:13:48-root-INFO: Loss too large (80.332->80.738)! Learning rate decreased to 0.10288.
2024-12-02-08:13:48-root-INFO: grad norm: 4.113 4.055 0.688
2024-12-02-08:13:49-root-INFO: grad norm: 2.787 2.755 0.417
2024-12-02-08:13:49-root-INFO: grad norm: 1.983 1.958 0.315
2024-12-02-08:13:50-root-INFO: grad norm: 1.814 1.787 0.312
2024-12-02-08:13:50-root-INFO: Loss Change: 81.246 -> 78.035
2024-12-02-08:13:50-root-INFO: Regularization Change: 0.000 -> 3.117
2024-12-02-08:13:50-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-08:13:50-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-08:13:50-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-08:13:51-root-INFO: grad norm: 2.410 2.369 0.441
2024-12-02-08:13:51-root-INFO: grad norm: 3.347 3.301 0.549
2024-12-02-08:13:51-root-INFO: Loss too large (77.862->78.315)! Learning rate decreased to 0.13168.
2024-12-02-08:13:52-root-INFO: grad norm: 5.758 5.713 0.715
2024-12-02-08:13:52-root-INFO: Loss too large (77.758->78.314)! Learning rate decreased to 0.10534.
2024-12-02-08:13:52-root-INFO: Loss too large (77.758->77.873)! Learning rate decreased to 0.08427.
2024-12-02-08:13:52-root-INFO: grad norm: 3.454 3.415 0.520
2024-12-02-08:13:53-root-INFO: grad norm: 1.849 1.822 0.312
2024-12-02-08:13:53-root-INFO: grad norm: 1.626 1.602 0.277
2024-12-02-08:13:54-root-INFO: grad norm: 1.597 1.573 0.278
2024-12-02-08:13:54-root-INFO: grad norm: 1.585 1.561 0.273
2024-12-02-08:13:55-root-INFO: Loss Change: 78.144 -> 75.986
2024-12-02-08:13:55-root-INFO: Regularization Change: 0.000 -> 1.669
2024-12-02-08:13:55-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-08:13:55-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-08:13:55-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-08:13:55-root-INFO: grad norm: 3.426 3.306 0.899
2024-12-02-08:13:55-root-INFO: grad norm: 4.992 4.901 0.950
2024-12-02-08:13:55-root-INFO: Loss too large (75.989->77.830)! Learning rate decreased to 0.13480.
2024-12-02-08:13:56-root-INFO: Loss too large (75.989->76.345)! Learning rate decreased to 0.10784.
2024-12-02-08:13:56-root-INFO: grad norm: 5.869 5.816 0.788
2024-12-02-08:13:56-root-INFO: Loss too large (75.589->75.824)! Learning rate decreased to 0.08627.
2024-12-02-08:13:57-root-INFO: grad norm: 3.645 3.604 0.546
2024-12-02-08:13:57-root-INFO: grad norm: 1.808 1.781 0.308
2024-12-02-08:13:58-root-INFO: grad norm: 1.555 1.534 0.260
2024-12-02-08:13:58-root-INFO: grad norm: 1.540 1.518 0.261
2024-12-02-08:13:59-root-INFO: grad norm: 1.560 1.539 0.259
2024-12-02-08:13:59-root-INFO: Loss Change: 76.041 -> 73.880
2024-12-02-08:13:59-root-INFO: Regularization Change: 0.000 -> 1.511
2024-12-02-08:13:59-root-INFO: Undo step: 70
2024-12-02-08:13:59-root-INFO: Undo step: 71
2024-12-02-08:13:59-root-INFO: Undo step: 72
2024-12-02-08:13:59-root-INFO: Undo step: 73
2024-12-02-08:13:59-root-INFO: Undo step: 74
2024-12-02-08:13:59-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-08:13:59-root-INFO: grad norm: 32.252 31.867 4.972
2024-12-02-08:14:00-root-INFO: grad norm: 24.377 24.196 2.965
2024-12-02-08:14:00-root-INFO: grad norm: 19.269 18.979 3.334
2024-12-02-08:14:01-root-INFO: grad norm: 21.685 21.475 3.006
2024-12-02-08:14:01-root-INFO: grad norm: 22.914 22.598 3.789
2024-12-02-08:14:01-root-INFO: grad norm: 22.956 22.707 3.371
2024-12-02-08:14:02-root-INFO: grad norm: 21.659 21.313 3.856
2024-12-02-08:14:02-root-INFO: grad norm: 21.275 21.041 3.144
2024-12-02-08:14:03-root-INFO: Loss Change: 217.951 -> 109.679
2024-12-02-08:14:03-root-INFO: Regularization Change: 0.000 -> 81.357
2024-12-02-08:14:03-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-08:14:03-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-08:14:03-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-08:14:03-root-INFO: grad norm: 19.300 19.043 3.140
2024-12-02-08:14:04-root-INFO: grad norm: 18.996 18.786 2.817
2024-12-02-08:14:04-root-INFO: grad norm: 18.886 18.596 3.294
2024-12-02-08:14:04-root-INFO: grad norm: 19.648 19.401 3.108
2024-12-02-08:14:05-root-INFO: grad norm: 19.598 19.287 3.482
2024-12-02-08:14:05-root-INFO: grad norm: 19.335 19.079 3.140
2024-12-02-08:14:06-root-INFO: grad norm: 19.110 18.788 3.490
2024-12-02-08:14:06-root-INFO: grad norm: 19.389 19.106 3.298
2024-12-02-08:14:06-root-INFO: Loss too large (98.293->98.647)! Learning rate decreased to 0.12261.
2024-12-02-08:14:07-root-INFO: Loss Change: 107.936 -> 89.690
2024-12-02-08:14:07-root-INFO: Regularization Change: 0.000 -> 11.808
2024-12-02-08:14:07-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-08:14:07-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-08:14:07-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-08:14:07-root-INFO: grad norm: 11.074 10.886 2.031
2024-12-02-08:14:08-root-INFO: grad norm: 12.024 11.843 2.083
2024-12-02-08:14:08-root-INFO: Loss too large (87.243->88.639)! Learning rate decreased to 0.12558.
2024-12-02-08:14:08-root-INFO: grad norm: 9.082 8.915 1.732
2024-12-02-08:14:09-root-INFO: grad norm: 7.054 6.952 1.194
2024-12-02-08:14:09-root-INFO: grad norm: 6.089 5.982 1.138
2024-12-02-08:14:10-root-INFO: grad norm: 5.765 5.684 0.960
2024-12-02-08:14:10-root-INFO: grad norm: 5.345 5.250 1.000
2024-12-02-08:14:11-root-INFO: grad norm: 4.903 4.829 0.849
2024-12-02-08:14:11-root-INFO: Loss Change: 88.345 -> 79.181
2024-12-02-08:14:11-root-INFO: Regularization Change: 0.000 -> 5.130
2024-12-02-08:14:11-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-08:14:11-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-08:14:11-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-08:14:11-root-INFO: grad norm: 3.811 3.769 0.566
2024-12-02-08:14:12-root-INFO: grad norm: 7.241 7.181 0.925
2024-12-02-08:14:12-root-INFO: Loss too large (78.519->79.842)! Learning rate decreased to 0.12860.
2024-12-02-08:14:12-root-INFO: Loss too large (78.519->78.984)! Learning rate decreased to 0.10288.
2024-12-02-08:14:13-root-INFO: grad norm: 4.177 4.110 0.748
2024-12-02-08:14:13-root-INFO: grad norm: 2.739 2.710 0.397
2024-12-02-08:14:13-root-INFO: grad norm: 1.900 1.874 0.315
2024-12-02-08:14:14-root-INFO: grad norm: 1.769 1.743 0.306
2024-12-02-08:14:15-root-INFO: grad norm: 1.720 1.694 0.296
2024-12-02-08:14:15-root-INFO: grad norm: 1.690 1.664 0.295
2024-12-02-08:14:15-root-INFO: Loss Change: 78.659 -> 75.439
2024-12-02-08:14:15-root-INFO: Regularization Change: 0.000 -> 2.483
2024-12-02-08:14:15-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-08:14:15-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-08:14:15-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-08:14:16-root-INFO: grad norm: 2.288 2.252 0.408
2024-12-02-08:14:16-root-INFO: grad norm: 3.066 3.020 0.530
2024-12-02-08:14:16-root-INFO: Loss too large (75.174->75.553)! Learning rate decreased to 0.13168.
2024-12-02-08:14:17-root-INFO: grad norm: 5.619 5.579 0.674
2024-12-02-08:14:17-root-INFO: Loss too large (75.092->75.688)! Learning rate decreased to 0.10534.
2024-12-02-08:14:17-root-INFO: Loss too large (75.092->75.271)! Learning rate decreased to 0.08427.
2024-12-02-08:14:17-root-INFO: grad norm: 3.519 3.479 0.529
2024-12-02-08:14:18-root-INFO: grad norm: 1.773 1.749 0.290
2024-12-02-08:14:18-root-INFO: grad norm: 1.590 1.568 0.264
2024-12-02-08:14:19-root-INFO: grad norm: 1.564 1.541 0.266
2024-12-02-08:14:19-root-INFO: grad norm: 1.549 1.527 0.259
2024-12-02-08:14:20-root-INFO: Loss Change: 75.517 -> 73.391
2024-12-02-08:14:20-root-INFO: Regularization Change: 0.000 -> 1.650
2024-12-02-08:14:20-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-08:14:20-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-08:14:20-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-08:14:20-root-INFO: grad norm: 3.365 3.254 0.855
2024-12-02-08:14:20-root-INFO: grad norm: 4.706 4.620 0.899
2024-12-02-08:14:21-root-INFO: Loss too large (73.331->74.944)! Learning rate decreased to 0.13480.
2024-12-02-08:14:21-root-INFO: Loss too large (73.331->73.579)! Learning rate decreased to 0.10784.
2024-12-02-08:14:21-root-INFO: grad norm: 5.544 5.497 0.718
2024-12-02-08:14:21-root-INFO: Loss too large (72.903->73.162)! Learning rate decreased to 0.08627.
2024-12-02-08:14:22-root-INFO: grad norm: 3.603 3.564 0.532
2024-12-02-08:14:22-root-INFO: grad norm: 1.688 1.664 0.283
2024-12-02-08:14:23-root-INFO: grad norm: 1.488 1.467 0.249
2024-12-02-08:14:23-root-INFO: grad norm: 1.461 1.440 0.247
2024-12-02-08:14:24-root-INFO: grad norm: 1.447 1.426 0.243
2024-12-02-08:14:24-root-INFO: Loss Change: 73.426 -> 71.304
2024-12-02-08:14:24-root-INFO: Regularization Change: 0.000 -> 1.477
2024-12-02-08:14:24-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-08:14:24-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-08:14:24-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-08:14:24-root-INFO: grad norm: 2.046 2.002 0.422
2024-12-02-08:14:25-root-INFO: grad norm: 2.438 2.397 0.444
2024-12-02-08:14:25-root-INFO: Loss too large (71.078->71.115)! Learning rate decreased to 0.13798.
2024-12-02-08:14:26-root-INFO: grad norm: 3.974 3.937 0.541
2024-12-02-08:14:26-root-INFO: Loss too large (70.924->71.390)! Learning rate decreased to 0.11038.
2024-12-02-08:14:26-root-INFO: Loss too large (70.924->71.077)! Learning rate decreased to 0.08831.
2024-12-02-08:14:26-root-INFO: grad norm: 3.363 3.331 0.463
2024-12-02-08:14:27-root-INFO: grad norm: 2.530 2.508 0.331
2024-12-02-08:14:27-root-INFO: grad norm: 2.603 2.579 0.353
2024-12-02-08:14:28-root-INFO: grad norm: 2.845 2.826 0.329
2024-12-02-08:14:28-root-INFO: grad norm: 2.860 2.835 0.376
2024-12-02-08:14:29-root-INFO: Loss Change: 71.412 -> 69.762
2024-12-02-08:14:29-root-INFO: Regularization Change: 0.000 -> 1.422
2024-12-02-08:14:29-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-08:14:29-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-08:14:29-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-08:14:29-root-INFO: grad norm: 5.953 5.855 1.077
2024-12-02-08:14:29-root-INFO: Loss too large (70.078->71.660)! Learning rate decreased to 0.14121.
2024-12-02-08:14:29-root-INFO: Loss too large (70.078->70.856)! Learning rate decreased to 0.11297.
2024-12-02-08:14:29-root-INFO: Loss too large (70.078->70.292)! Learning rate decreased to 0.09037.
2024-12-02-08:14:30-root-INFO: grad norm: 3.767 3.729 0.531
2024-12-02-08:14:30-root-INFO: grad norm: 1.704 1.677 0.302
2024-12-02-08:14:31-root-INFO: grad norm: 1.413 1.393 0.239
2024-12-02-08:14:31-root-INFO: grad norm: 1.369 1.349 0.232
2024-12-02-08:14:32-root-INFO: grad norm: 1.356 1.336 0.227
2024-12-02-08:14:32-root-INFO: grad norm: 1.350 1.331 0.227
2024-12-02-08:14:33-root-INFO: grad norm: 1.354 1.335 0.223
2024-12-02-08:14:33-root-INFO: Loss Change: 70.078 -> 68.006
2024-12-02-08:14:33-root-INFO: Regularization Change: 0.000 -> 1.077
2024-12-02-08:14:33-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-08:14:33-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-08:14:33-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-08:14:33-root-INFO: grad norm: 2.088 2.032 0.479
2024-12-02-08:14:34-root-INFO: grad norm: 2.370 2.327 0.448
2024-12-02-08:14:34-root-INFO: grad norm: 3.666 3.609 0.644
2024-12-02-08:14:35-root-INFO: Loss too large (67.560->68.332)! Learning rate decreased to 0.14449.
2024-12-02-08:14:35-root-INFO: Loss too large (67.560->67.771)! Learning rate decreased to 0.11559.
2024-12-02-08:14:35-root-INFO: grad norm: 3.819 3.774 0.583
2024-12-02-08:14:35-root-INFO: Loss too large (67.440->67.485)! Learning rate decreased to 0.09247.
2024-12-02-08:14:36-root-INFO: grad norm: 4.160 4.134 0.462
2024-12-02-08:14:36-root-INFO: Loss too large (67.108->67.241)! Learning rate decreased to 0.07398.
2024-12-02-08:14:37-root-INFO: grad norm: 3.169 3.144 0.392
2024-12-02-08:14:37-root-INFO: grad norm: 1.819 1.801 0.255
2024-12-02-08:14:37-root-INFO: grad norm: 1.750 1.732 0.250
2024-12-02-08:14:38-root-INFO: Loss Change: 67.992 -> 66.411
2024-12-02-08:14:38-root-INFO: Regularization Change: 0.000 -> 1.313
2024-12-02-08:14:38-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-08:14:38-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-08:14:38-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-08:14:38-root-INFO: grad norm: 3.105 3.063 0.511
2024-12-02-08:14:38-root-INFO: Loss too large (66.555->67.170)! Learning rate decreased to 0.14783.
2024-12-02-08:14:38-root-INFO: Loss too large (66.555->66.849)! Learning rate decreased to 0.11826.
2024-12-02-08:14:39-root-INFO: Loss too large (66.555->66.620)! Learning rate decreased to 0.09461.
2024-12-02-08:14:39-root-INFO: grad norm: 3.095 3.072 0.381
2024-12-02-08:14:40-root-INFO: grad norm: 3.633 3.611 0.394
2024-12-02-08:14:40-root-INFO: Loss too large (66.238->66.322)! Learning rate decreased to 0.07569.
2024-12-02-08:14:40-root-INFO: grad norm: 2.962 2.940 0.360
2024-12-02-08:14:41-root-INFO: grad norm: 2.019 2.002 0.262
2024-12-02-08:14:41-root-INFO: grad norm: 1.931 1.913 0.263
2024-12-02-08:14:42-root-INFO: grad norm: 1.846 1.831 0.239
2024-12-02-08:14:42-root-INFO: grad norm: 1.812 1.794 0.252
2024-12-02-08:14:42-root-INFO: Loss Change: 66.555 -> 65.366
2024-12-02-08:14:42-root-INFO: Regularization Change: 0.000 -> 0.686
2024-12-02-08:14:42-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-08:14:42-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-08:14:43-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-08:14:43-root-INFO: grad norm: 3.729 3.656 0.733
2024-12-02-08:14:43-root-INFO: Loss too large (65.569->66.445)! Learning rate decreased to 0.15121.
2024-12-02-08:14:43-root-INFO: Loss too large (65.569->65.964)! Learning rate decreased to 0.12097.
2024-12-02-08:14:43-root-INFO: Loss too large (65.569->65.634)! Learning rate decreased to 0.09678.
2024-12-02-08:14:44-root-INFO: grad norm: 3.374 3.348 0.414
2024-12-02-08:14:44-root-INFO: grad norm: 3.782 3.758 0.421
2024-12-02-08:14:44-root-INFO: Loss too large (65.119->65.239)! Learning rate decreased to 0.07742.
2024-12-02-08:14:45-root-INFO: grad norm: 3.074 3.053 0.362
2024-12-02-08:14:45-root-INFO: grad norm: 2.053 2.036 0.264
2024-12-02-08:14:46-root-INFO: grad norm: 1.997 1.980 0.260
2024-12-02-08:14:46-root-INFO: grad norm: 1.961 1.946 0.242
2024-12-02-08:14:47-root-INFO: grad norm: 1.953 1.936 0.256
2024-12-02-08:14:47-root-INFO: Loss Change: 65.569 -> 64.241
2024-12-02-08:14:47-root-INFO: Regularization Change: 0.000 -> 0.727
2024-12-02-08:14:47-root-INFO: Undo step: 65
2024-12-02-08:14:47-root-INFO: Undo step: 66
2024-12-02-08:14:47-root-INFO: Undo step: 67
2024-12-02-08:14:47-root-INFO: Undo step: 68
2024-12-02-08:14:47-root-INFO: Undo step: 69
2024-12-02-08:14:47-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-08:14:47-root-INFO: grad norm: 34.512 34.220 4.474
2024-12-02-08:14:48-root-INFO: grad norm: 22.229 22.105 2.339
2024-12-02-08:14:48-root-INFO: grad norm: 15.217 15.110 1.803
2024-12-02-08:14:49-root-INFO: grad norm: 13.404 13.299 1.675
2024-12-02-08:14:49-root-INFO: grad norm: 12.804 12.608 2.233
2024-12-02-08:14:50-root-INFO: grad norm: 12.422 12.302 1.723
2024-12-02-08:14:50-root-INFO: grad norm: 12.389 12.189 2.213
2024-12-02-08:14:51-root-INFO: grad norm: 12.291 12.188 1.589
2024-12-02-08:14:51-root-INFO: Loss Change: 205.655 -> 87.649
2024-12-02-08:14:51-root-INFO: Regularization Change: 0.000 -> 84.260
2024-12-02-08:14:51-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-08:14:51-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-08:14:51-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-08:14:51-root-INFO: grad norm: 13.075 12.948 1.819
2024-12-02-08:14:51-root-INFO: Loss too large (87.264->87.362)! Learning rate decreased to 0.13798.
2024-12-02-08:14:52-root-INFO: grad norm: 8.693 8.621 1.119
2024-12-02-08:14:52-root-INFO: grad norm: 6.462 6.379 1.033
2024-12-02-08:14:53-root-INFO: grad norm: 5.247 5.173 0.882
2024-12-02-08:14:53-root-INFO: grad norm: 4.698 4.624 0.834
2024-12-02-08:14:54-root-INFO: grad norm: 4.889 4.819 0.827
2024-12-02-08:14:54-root-INFO: grad norm: 4.901 4.823 0.871
2024-12-02-08:14:55-root-INFO: grad norm: 5.316 5.240 0.898
2024-12-02-08:14:55-root-INFO: Loss Change: 87.264 -> 75.112
2024-12-02-08:14:55-root-INFO: Regularization Change: 0.000 -> 8.488
2024-12-02-08:14:55-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-08:14:55-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-08:14:55-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-08:14:55-root-INFO: grad norm: 4.330 4.286 0.619
2024-12-02-08:14:56-root-INFO: grad norm: 6.968 6.911 0.891
2024-12-02-08:14:56-root-INFO: Loss too large (73.783->75.133)! Learning rate decreased to 0.14121.
2024-12-02-08:14:56-root-INFO: Loss too large (73.783->74.180)! Learning rate decreased to 0.11297.
2024-12-02-08:14:57-root-INFO: grad norm: 4.394 4.332 0.739
2024-12-02-08:14:57-root-INFO: grad norm: 2.713 2.680 0.424
2024-12-02-08:14:58-root-INFO: grad norm: 2.093 2.064 0.350
2024-12-02-08:14:58-root-INFO: grad norm: 1.942 1.912 0.337
2024-12-02-08:14:59-root-INFO: grad norm: 1.894 1.865 0.327
2024-12-02-08:14:59-root-INFO: grad norm: 1.894 1.866 0.327
2024-12-02-08:14:59-root-INFO: Loss Change: 74.374 -> 70.116
2024-12-02-08:14:59-root-INFO: Regularization Change: 0.000 -> 3.518
2024-12-02-08:14:59-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-08:14:59-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-08:15:00-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-08:15:00-root-INFO: grad norm: 2.761 2.696 0.599
2024-12-02-08:15:00-root-INFO: grad norm: 3.931 3.867 0.707
2024-12-02-08:15:00-root-INFO: Loss too large (69.871->70.644)! Learning rate decreased to 0.14449.
2024-12-02-08:15:01-root-INFO: grad norm: 6.544 6.482 0.902
2024-12-02-08:15:01-root-INFO: Loss too large (69.752->70.516)! Learning rate decreased to 0.11559.
2024-12-02-08:15:01-root-INFO: Loss too large (69.752->69.910)! Learning rate decreased to 0.09247.
2024-12-02-08:15:02-root-INFO: grad norm: 3.713 3.672 0.555
2024-12-02-08:15:02-root-INFO: grad norm: 1.986 1.960 0.316
2024-12-02-08:15:03-root-INFO: grad norm: 1.666 1.642 0.286
2024-12-02-08:15:03-root-INFO: grad norm: 1.614 1.589 0.281
2024-12-02-08:15:04-root-INFO: grad norm: 1.586 1.561 0.279
2024-12-02-08:15:04-root-INFO: Loss Change: 70.086 -> 67.590
2024-12-02-08:15:04-root-INFO: Regularization Change: 0.000 -> 2.095
2024-12-02-08:15:04-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-08:15:04-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-08:15:04-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-08:15:04-root-INFO: grad norm: 2.110 2.061 0.455
2024-12-02-08:15:05-root-INFO: grad norm: 2.671 2.626 0.491
2024-12-02-08:15:05-root-INFO: Loss too large (67.272->67.433)! Learning rate decreased to 0.14783.
2024-12-02-08:15:05-root-INFO: grad norm: 4.456 4.412 0.620
2024-12-02-08:15:06-root-INFO: Loss too large (67.134->67.615)! Learning rate decreased to 0.11826.
2024-12-02-08:15:06-root-INFO: Loss too large (67.134->67.236)! Learning rate decreased to 0.09461.
2024-12-02-08:15:06-root-INFO: grad norm: 3.343 3.312 0.455
2024-12-02-08:15:07-root-INFO: grad norm: 1.984 1.958 0.319
2024-12-02-08:15:07-root-INFO: grad norm: 1.885 1.863 0.287
2024-12-02-08:15:08-root-INFO: grad norm: 1.857 1.835 0.282
2024-12-02-08:15:08-root-INFO: grad norm: 1.849 1.828 0.278
2024-12-02-08:15:09-root-INFO: Loss Change: 67.648 -> 65.615
2024-12-02-08:15:09-root-INFO: Regularization Change: 0.000 -> 1.799
2024-12-02-08:15:09-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-08:15:09-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-08:15:09-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-08:15:09-root-INFO: grad norm: 3.596 3.509 0.788
2024-12-02-08:15:09-root-INFO: Loss too large (65.793->66.291)! Learning rate decreased to 0.15121.
2024-12-02-08:15:09-root-INFO: Loss too large (65.793->65.861)! Learning rate decreased to 0.12097.
2024-12-02-08:15:10-root-INFO: grad norm: 3.478 3.442 0.497
2024-12-02-08:15:10-root-INFO: grad norm: 4.296 4.261 0.541
2024-12-02-08:15:10-root-INFO: Loss too large (65.278->65.468)! Learning rate decreased to 0.09678.
2024-12-02-08:15:11-root-INFO: grad norm: 3.336 3.308 0.430
2024-12-02-08:15:11-root-INFO: grad norm: 1.973 1.949 0.305
2024-12-02-08:15:12-root-INFO: grad norm: 1.900 1.881 0.271
2024-12-02-08:15:12-root-INFO: grad norm: 1.926 1.906 0.272
2024-12-02-08:15:13-root-INFO: grad norm: 1.955 1.937 0.269
2024-12-02-08:15:13-root-INFO: Loss Change: 65.793 -> 63.922
2024-12-02-08:15:13-root-INFO: Regularization Change: 0.000 -> 1.349
2024-12-02-08:15:13-root-INFO: Undo step: 65
2024-12-02-08:15:13-root-INFO: Undo step: 66
2024-12-02-08:15:13-root-INFO: Undo step: 67
2024-12-02-08:15:13-root-INFO: Undo step: 68
2024-12-02-08:15:13-root-INFO: Undo step: 69
2024-12-02-08:15:13-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-08:15:13-root-INFO: grad norm: 32.641 32.305 4.668
2024-12-02-08:15:14-root-INFO: grad norm: 15.944 15.536 3.583
2024-12-02-08:15:14-root-INFO: grad norm: 11.981 11.778 2.192
2024-12-02-08:15:15-root-INFO: grad norm: 11.627 11.482 1.833
2024-12-02-08:15:15-root-INFO: grad norm: 13.687 13.535 2.031
2024-12-02-08:15:15-root-INFO: Loss too large (94.320->94.853)! Learning rate decreased to 0.13480.
2024-12-02-08:15:16-root-INFO: grad norm: 10.504 10.372 1.662
2024-12-02-08:15:16-root-INFO: grad norm: 7.766 7.677 1.176
2024-12-02-08:15:17-root-INFO: grad norm: 6.625 6.550 0.995
2024-12-02-08:15:17-root-INFO: Loss Change: 207.911 -> 81.323
2024-12-02-08:15:17-root-INFO: Regularization Change: 0.000 -> 86.972
2024-12-02-08:15:17-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-08:15:17-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-08:15:17-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-08:15:17-root-INFO: grad norm: 6.457 6.363 1.097
2024-12-02-08:15:18-root-INFO: grad norm: 8.847 8.740 1.373
2024-12-02-08:15:18-root-INFO: Loss too large (80.964->82.324)! Learning rate decreased to 0.13798.
2024-12-02-08:15:18-root-INFO: grad norm: 8.389 8.284 1.323
2024-12-02-08:15:19-root-INFO: grad norm: 7.004 6.880 1.313
2024-12-02-08:15:19-root-INFO: grad norm: 5.959 5.853 1.115
2024-12-02-08:15:20-root-INFO: grad norm: 5.889 5.788 1.084
2024-12-02-08:15:20-root-INFO: grad norm: 6.597 6.501 1.121
2024-12-02-08:15:21-root-INFO: grad norm: 6.620 6.511 1.199
2024-12-02-08:15:21-root-INFO: Loss Change: 81.515 -> 74.038
2024-12-02-08:15:21-root-INFO: Regularization Change: 0.000 -> 8.647
2024-12-02-08:15:21-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-08:15:21-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-08:15:21-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-08:15:21-root-INFO: grad norm: 8.979 8.751 2.011
2024-12-02-08:15:22-root-INFO: Loss too large (74.934->77.560)! Learning rate decreased to 0.14121.
2024-12-02-08:15:22-root-INFO: grad norm: 8.576 8.414 1.662
2024-12-02-08:15:23-root-INFO: grad norm: 7.840 7.711 1.416
2024-12-02-08:15:23-root-INFO: grad norm: 7.696 7.572 1.377
2024-12-02-08:15:23-root-INFO: grad norm: 8.151 8.031 1.395
2024-12-02-08:15:24-root-INFO: grad norm: 7.746 7.620 1.394
2024-12-02-08:15:24-root-INFO: grad norm: 7.039 6.930 1.234
2024-12-02-08:15:25-root-INFO: grad norm: 6.929 6.829 1.176
2024-12-02-08:15:25-root-INFO: Loss Change: 74.934 -> 70.159
2024-12-02-08:15:25-root-INFO: Regularization Change: 0.000 -> 4.648
2024-12-02-08:15:25-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-08:15:25-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-08:15:25-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-08:15:25-root-INFO: grad norm: 8.759 8.598 1.672
2024-12-02-08:15:26-root-INFO: Loss too large (70.648->73.620)! Learning rate decreased to 0.14449.
2024-12-02-08:15:26-root-INFO: Loss too large (70.648->70.818)! Learning rate decreased to 0.11559.
2024-12-02-08:15:26-root-INFO: grad norm: 5.661 5.567 1.029
2024-12-02-08:15:27-root-INFO: grad norm: 3.322 3.265 0.612
2024-12-02-08:15:27-root-INFO: grad norm: 2.536 2.500 0.424
2024-12-02-08:15:28-root-INFO: grad norm: 2.266 2.233 0.380
2024-12-02-08:15:28-root-INFO: grad norm: 2.173 2.145 0.347
2024-12-02-08:15:29-root-INFO: grad norm: 2.217 2.189 0.349
2024-12-02-08:15:29-root-INFO: grad norm: 2.261 2.235 0.342
2024-12-02-08:15:29-root-INFO: Loss Change: 70.648 -> 65.926
2024-12-02-08:15:29-root-INFO: Regularization Change: 0.000 -> 2.493
2024-12-02-08:15:29-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-08:15:29-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-08:15:29-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-08:15:30-root-INFO: grad norm: 3.661 3.597 0.678
2024-12-02-08:15:30-root-INFO: Loss too large (66.079->66.517)! Learning rate decreased to 0.14783.
2024-12-02-08:15:30-root-INFO: Loss too large (66.079->66.126)! Learning rate decreased to 0.11826.
2024-12-02-08:15:30-root-INFO: grad norm: 3.285 3.248 0.496
2024-12-02-08:15:31-root-INFO: grad norm: 2.873 2.837 0.454
2024-12-02-08:15:31-root-INFO: grad norm: 2.824 2.793 0.414
2024-12-02-08:15:32-root-INFO: grad norm: 2.817 2.787 0.411
2024-12-02-08:15:32-root-INFO: grad norm: 2.802 2.773 0.402
2024-12-02-08:15:33-root-INFO: grad norm: 2.795 2.766 0.401
2024-12-02-08:15:33-root-INFO: grad norm: 2.790 2.761 0.397
2024-12-02-08:15:33-root-INFO: Loss Change: 66.079 -> 63.937
2024-12-02-08:15:33-root-INFO: Regularization Change: 0.000 -> 1.902
2024-12-02-08:15:33-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-08:15:33-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-08:15:34-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-08:15:34-root-INFO: grad norm: 4.635 4.526 0.999
2024-12-02-08:15:34-root-INFO: Loss too large (64.206->65.179)! Learning rate decreased to 0.15121.
2024-12-02-08:15:34-root-INFO: Loss too large (64.206->64.473)! Learning rate decreased to 0.12097.
2024-12-02-08:15:35-root-INFO: grad norm: 3.813 3.765 0.606
2024-12-02-08:15:35-root-INFO: grad norm: 2.882 2.841 0.487
2024-12-02-08:15:36-root-INFO: grad norm: 2.826 2.795 0.421
2024-12-02-08:15:36-root-INFO: grad norm: 2.980 2.951 0.420
2024-12-02-08:15:36-root-INFO: grad norm: 2.973 2.944 0.418
2024-12-02-08:15:37-root-INFO: grad norm: 2.968 2.939 0.412
2024-12-02-08:15:37-root-INFO: grad norm: 2.960 2.931 0.412
2024-12-02-08:15:38-root-INFO: Loss Change: 64.206 -> 62.054
2024-12-02-08:15:38-root-INFO: Regularization Change: 0.000 -> 1.754
2024-12-02-08:15:38-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-08:15:38-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-08:15:38-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-08:15:38-root-INFO: grad norm: 4.078 4.007 0.759
2024-12-02-08:15:38-root-INFO: Loss too large (62.138->63.033)! Learning rate decreased to 0.15465.
2024-12-02-08:15:38-root-INFO: Loss too large (62.138->62.442)! Learning rate decreased to 0.12372.
2024-12-02-08:15:39-root-INFO: grad norm: 3.619 3.577 0.548
2024-12-02-08:15:39-root-INFO: grad norm: 3.059 3.021 0.480
2024-12-02-08:15:40-root-INFO: grad norm: 3.044 3.013 0.438
2024-12-02-08:15:40-root-INFO: grad norm: 3.162 3.131 0.437
2024-12-02-08:15:41-root-INFO: grad norm: 3.131 3.100 0.436
2024-12-02-08:15:41-root-INFO: grad norm: 3.062 3.033 0.424
2024-12-02-08:15:42-root-INFO: grad norm: 3.057 3.028 0.425
2024-12-02-08:15:42-root-INFO: Loss Change: 62.138 -> 60.322
2024-12-02-08:15:42-root-INFO: Regularization Change: 0.000 -> 1.613
2024-12-02-08:15:42-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-08:15:42-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-08:15:42-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-08:15:42-root-INFO: grad norm: 4.208 4.130 0.807
2024-12-02-08:15:42-root-INFO: Loss too large (60.588->61.575)! Learning rate decreased to 0.15815.
2024-12-02-08:15:42-root-INFO: Loss too large (60.588->60.926)! Learning rate decreased to 0.12652.
2024-12-02-08:15:43-root-INFO: grad norm: 3.664 3.622 0.554
2024-12-02-08:15:43-root-INFO: grad norm: 3.025 2.987 0.480
2024-12-02-08:15:44-root-INFO: grad norm: 3.001 2.970 0.432
2024-12-02-08:15:44-root-INFO: grad norm: 3.125 3.095 0.429
2024-12-02-08:15:45-root-INFO: grad norm: 3.098 3.068 0.431
2024-12-02-08:15:45-root-INFO: grad norm: 3.042 3.013 0.417
2024-12-02-08:15:46-root-INFO: grad norm: 3.036 3.007 0.421
2024-12-02-08:15:46-root-INFO: Loss Change: 60.588 -> 58.824
2024-12-02-08:15:46-root-INFO: Regularization Change: 0.000 -> 1.551
2024-12-02-08:15:46-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-08:15:46-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-08:15:46-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-08:15:46-root-INFO: grad norm: 4.851 4.745 1.011
2024-12-02-08:15:47-root-INFO: Loss too large (58.935->60.314)! Learning rate decreased to 0.16169.
2024-12-02-08:15:47-root-INFO: Loss too large (58.935->59.431)! Learning rate decreased to 0.12935.
2024-12-02-08:15:47-root-INFO: grad norm: 4.031 3.979 0.648
2024-12-02-08:15:48-root-INFO: grad norm: 3.053 3.010 0.508
2024-12-02-08:15:48-root-INFO: grad norm: 3.018 2.984 0.451
2024-12-02-08:15:49-root-INFO: grad norm: 3.254 3.223 0.442
2024-12-02-08:15:49-root-INFO: grad norm: 3.230 3.198 0.454
2024-12-02-08:15:50-root-INFO: grad norm: 3.168 3.139 0.433
2024-12-02-08:15:50-root-INFO: grad norm: 3.163 3.132 0.442
2024-12-02-08:15:50-root-INFO: Loss Change: 58.935 -> 57.059
2024-12-02-08:15:50-root-INFO: Regularization Change: 0.000 -> 1.544
2024-12-02-08:15:50-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-08:15:50-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-08:15:51-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-08:15:51-root-INFO: grad norm: 4.193 4.135 0.698
2024-12-02-08:15:51-root-INFO: Loss too large (57.257->58.383)! Learning rate decreased to 0.16529.
2024-12-02-08:15:51-root-INFO: Loss too large (57.257->57.715)! Learning rate decreased to 0.13223.
2024-12-02-08:15:51-root-INFO: Loss too large (57.257->57.259)! Learning rate decreased to 0.10579.
2024-12-02-08:15:52-root-INFO: grad norm: 2.918 2.892 0.387
2024-12-02-08:15:52-root-INFO: grad norm: 1.755 1.733 0.275
2024-12-02-08:15:53-root-INFO: grad norm: 1.531 1.516 0.219
2024-12-02-08:15:53-root-INFO: grad norm: 1.381 1.365 0.210
2024-12-02-08:15:54-root-INFO: grad norm: 1.308 1.293 0.197
2024-12-02-08:15:54-root-INFO: grad norm: 1.252 1.237 0.194
2024-12-02-08:15:54-root-INFO: grad norm: 1.218 1.203 0.189
2024-12-02-08:15:55-root-INFO: Loss Change: 57.257 -> 55.702
2024-12-02-08:15:55-root-INFO: Regularization Change: 0.000 -> 0.991
2024-12-02-08:15:55-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-08:15:55-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-08:15:55-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-08:15:55-root-INFO: grad norm: 2.302 2.233 0.559
2024-12-02-08:15:55-root-INFO: Loss too large (55.838->55.846)! Learning rate decreased to 0.16894.
2024-12-02-08:15:56-root-INFO: grad norm: 2.774 2.734 0.469
2024-12-02-08:15:56-root-INFO: Loss too large (55.677->55.831)! Learning rate decreased to 0.13515.
2024-12-02-08:15:56-root-INFO: grad norm: 3.494 3.464 0.460
2024-12-02-08:15:57-root-INFO: Loss too large (55.531->55.658)! Learning rate decreased to 0.10812.
2024-12-02-08:15:57-root-INFO: grad norm: 2.875 2.851 0.372
2024-12-02-08:15:57-root-INFO: grad norm: 2.077 2.058 0.281
2024-12-02-08:15:58-root-INFO: grad norm: 1.916 1.899 0.251
2024-12-02-08:15:58-root-INFO: grad norm: 1.761 1.745 0.235
2024-12-02-08:15:59-root-INFO: grad norm: 1.689 1.674 0.225
2024-12-02-08:15:59-root-INFO: Loss Change: 55.838 -> 54.522
2024-12-02-08:15:59-root-INFO: Regularization Change: 0.000 -> 1.156
2024-12-02-08:15:59-root-INFO: Undo step: 60
2024-12-02-08:15:59-root-INFO: Undo step: 61
2024-12-02-08:15:59-root-INFO: Undo step: 62
2024-12-02-08:15:59-root-INFO: Undo step: 63
2024-12-02-08:15:59-root-INFO: Undo step: 64
2024-12-02-08:15:59-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-08:16:00-root-INFO: grad norm: 27.316 27.126 3.218
2024-12-02-08:16:00-root-INFO: grad norm: 13.552 13.420 1.884
2024-12-02-08:16:01-root-INFO: grad norm: 12.594 12.484 1.664
2024-12-02-08:16:01-root-INFO: grad norm: 10.420 10.325 1.407
2024-12-02-08:16:01-root-INFO: grad norm: 11.816 11.662 1.901
2024-12-02-08:16:02-root-INFO: grad norm: 15.019 14.851 2.239
2024-12-02-08:16:02-root-INFO: Loss too large (83.696->86.617)! Learning rate decreased to 0.15121.
2024-12-02-08:16:03-root-INFO: grad norm: 12.362 12.177 2.130
2024-12-02-08:16:03-root-INFO: grad norm: 10.403 10.268 1.671
2024-12-02-08:16:03-root-INFO: Loss Change: 187.999 -> 73.434
2024-12-02-08:16:03-root-INFO: Regularization Change: 0.000 -> 91.475
2024-12-02-08:16:03-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-08:16:03-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-08:16:03-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-08:16:04-root-INFO: grad norm: 8.535 8.423 1.379
2024-12-02-08:16:04-root-INFO: Loss too large (72.646->73.774)! Learning rate decreased to 0.15465.
2024-12-02-08:16:04-root-INFO: grad norm: 8.104 7.991 1.347
2024-12-02-08:16:05-root-INFO: grad norm: 7.815 7.681 1.438
2024-12-02-08:16:05-root-INFO: grad norm: 7.907 7.783 1.398
2024-12-02-08:16:06-root-INFO: grad norm: 7.776 7.633 1.485
2024-12-02-08:16:06-root-INFO: grad norm: 7.592 7.466 1.377
2024-12-02-08:16:07-root-INFO: grad norm: 7.601 7.462 1.447
2024-12-02-08:16:07-root-INFO: grad norm: 7.718 7.592 1.388
2024-12-02-08:16:07-root-INFO: Loss Change: 72.646 -> 66.135
2024-12-02-08:16:07-root-INFO: Regularization Change: 0.000 -> 7.769
2024-12-02-08:16:07-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-08:16:07-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-08:16:07-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-08:16:08-root-INFO: grad norm: 6.852 6.759 1.126
2024-12-02-08:16:08-root-INFO: Loss too large (65.646->66.676)! Learning rate decreased to 0.15815.
2024-12-02-08:16:08-root-INFO: grad norm: 6.683 6.582 1.157
2024-12-02-08:16:09-root-INFO: grad norm: 6.689 6.576 1.220
2024-12-02-08:16:09-root-INFO: grad norm: 6.915 6.808 1.212
2024-12-02-08:16:09-root-INFO: Loss too large (63.727->63.778)! Learning rate decreased to 0.12652.
2024-12-02-08:16:10-root-INFO: grad norm: 4.809 4.723 0.903
2024-12-02-08:16:10-root-INFO: grad norm: 3.238 3.187 0.574
2024-12-02-08:16:11-root-INFO: grad norm: 2.711 2.668 0.479
2024-12-02-08:16:11-root-INFO: grad norm: 2.435 2.403 0.392
2024-12-02-08:16:12-root-INFO: Loss Change: 65.646 -> 60.548
2024-12-02-08:16:12-root-INFO: Regularization Change: 0.000 -> 3.723
2024-12-02-08:16:12-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-08:16:12-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-08:16:12-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-08:16:12-root-INFO: grad norm: 2.558 2.500 0.542
2024-12-02-08:16:12-root-INFO: grad norm: 3.188 3.161 0.415
2024-12-02-08:16:13-root-INFO: Loss too large (59.836->59.910)! Learning rate decreased to 0.16169.
2024-12-02-08:16:13-root-INFO: grad norm: 3.533 3.509 0.408
2024-12-02-08:16:13-root-INFO: grad norm: 4.432 4.411 0.430
2024-12-02-08:16:14-root-INFO: Loss too large (59.384->59.508)! Learning rate decreased to 0.12935.
2024-12-02-08:16:14-root-INFO: grad norm: 3.563 3.532 0.464
2024-12-02-08:16:15-root-INFO: grad norm: 2.786 2.760 0.376
2024-12-02-08:16:15-root-INFO: grad norm: 2.680 2.651 0.393
2024-12-02-08:16:15-root-INFO: grad norm: 2.716 2.690 0.372
2024-12-02-08:16:16-root-INFO: Loss Change: 60.245 -> 57.768
2024-12-02-08:16:16-root-INFO: Regularization Change: 0.000 -> 2.970
2024-12-02-08:16:16-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-08:16:16-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-08:16:16-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-08:16:16-root-INFO: grad norm: 2.615 2.596 0.317
2024-12-02-08:16:16-root-INFO: Loss too large (57.709->57.782)! Learning rate decreased to 0.16529.
2024-12-02-08:16:17-root-INFO: grad norm: 3.613 3.589 0.410
2024-12-02-08:16:17-root-INFO: Loss too large (57.496->57.700)! Learning rate decreased to 0.13223.
2024-12-02-08:16:17-root-INFO: grad norm: 3.357 3.325 0.466
2024-12-02-08:16:18-root-INFO: grad norm: 2.997 2.968 0.417
2024-12-02-08:16:18-root-INFO: grad norm: 2.951 2.919 0.434
2024-12-02-08:16:19-root-INFO: grad norm: 2.947 2.919 0.409
2024-12-02-08:16:19-root-INFO: grad norm: 2.941 2.909 0.437
2024-12-02-08:16:20-root-INFO: grad norm: 2.957 2.928 0.413
2024-12-02-08:16:20-root-INFO: Loss Change: 57.709 -> 55.927
2024-12-02-08:16:20-root-INFO: Regularization Change: 0.000 -> 2.059
2024-12-02-08:16:20-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-08:16:20-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-08:16:20-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-08:16:20-root-INFO: grad norm: 2.917 2.890 0.392
2024-12-02-08:16:21-root-INFO: Loss too large (55.906->56.196)! Learning rate decreased to 0.16894.
2024-12-02-08:16:21-root-INFO: grad norm: 4.150 4.129 0.423
2024-12-02-08:16:21-root-INFO: Loss too large (55.749->56.196)! Learning rate decreased to 0.13515.
2024-12-02-08:16:22-root-INFO: grad norm: 3.723 3.690 0.493
2024-12-02-08:16:22-root-INFO: grad norm: 3.060 3.031 0.418
2024-12-02-08:16:23-root-INFO: grad norm: 3.057 3.027 0.428
2024-12-02-08:16:23-root-INFO: grad norm: 3.181 3.153 0.417
2024-12-02-08:16:24-root-INFO: grad norm: 3.205 3.173 0.452
2024-12-02-08:16:24-root-INFO: grad norm: 3.257 3.228 0.436
2024-12-02-08:16:25-root-INFO: Loss Change: 55.906 -> 54.338
2024-12-02-08:16:25-root-INFO: Regularization Change: 0.000 -> 1.879
2024-12-02-08:16:25-root-INFO: Undo step: 60
2024-12-02-08:16:25-root-INFO: Undo step: 61
2024-12-02-08:16:25-root-INFO: Undo step: 62
2024-12-02-08:16:25-root-INFO: Undo step: 63
2024-12-02-08:16:25-root-INFO: Undo step: 64
2024-12-02-08:16:25-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-08:16:25-root-INFO: grad norm: 27.783 27.526 3.771
2024-12-02-08:16:25-root-INFO: grad norm: 15.156 14.948 2.508
2024-12-02-08:16:26-root-INFO: grad norm: 11.560 11.187 2.911
2024-12-02-08:16:26-root-INFO: grad norm: 9.747 9.634 1.482
2024-12-02-08:16:27-root-INFO: grad norm: 10.351 10.193 1.806
2024-12-02-08:16:27-root-INFO: grad norm: 11.999 11.817 2.080
2024-12-02-08:16:28-root-INFO: grad norm: 13.516 13.259 2.623
2024-12-02-08:16:28-root-INFO: grad norm: 15.251 15.008 2.712
2024-12-02-08:16:28-root-INFO: Loss too large (81.002->82.822)! Learning rate decreased to 0.15121.
2024-12-02-08:16:29-root-INFO: Loss Change: 184.654 -> 75.607
2024-12-02-08:16:29-root-INFO: Regularization Change: 0.000 -> 88.974
2024-12-02-08:16:29-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-08:16:29-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-08:16:29-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-08:16:29-root-INFO: grad norm: 10.286 10.083 2.033
2024-12-02-08:16:29-root-INFO: grad norm: 13.408 13.191 2.406
2024-12-02-08:16:30-root-INFO: Loss too large (74.308->77.692)! Learning rate decreased to 0.15465.
2024-12-02-08:16:30-root-INFO: grad norm: 9.641 9.439 1.963
2024-12-02-08:16:30-root-INFO: grad norm: 7.325 7.207 1.305
2024-12-02-08:16:31-root-INFO: grad norm: 6.153 6.047 1.137
2024-12-02-08:16:31-root-INFO: grad norm: 5.568 5.469 1.045
2024-12-02-08:16:32-root-INFO: grad norm: 5.384 5.294 0.982
2024-12-02-08:16:33-root-INFO: grad norm: 6.236 6.156 1.000
2024-12-02-08:16:33-root-INFO: Loss too large (64.272->64.440)! Learning rate decreased to 0.12372.
2024-12-02-08:16:33-root-INFO: Loss Change: 74.536 -> 63.714
2024-12-02-08:16:33-root-INFO: Regularization Change: 0.000 -> 8.929
2024-12-02-08:16:33-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-08:16:33-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-08:16:33-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-08:16:33-root-INFO: grad norm: 4.056 4.022 0.527
2024-12-02-08:16:34-root-INFO: grad norm: 6.463 6.414 0.800
2024-12-02-08:16:34-root-INFO: Loss too large (63.043->64.511)! Learning rate decreased to 0.15815.
2024-12-02-08:16:34-root-INFO: Loss too large (63.043->63.415)! Learning rate decreased to 0.12652.
2024-12-02-08:16:35-root-INFO: grad norm: 4.448 4.398 0.666
2024-12-02-08:16:35-root-INFO: grad norm: 2.480 2.448 0.396
2024-12-02-08:16:36-root-INFO: grad norm: 2.084 2.058 0.329
2024-12-02-08:16:36-root-INFO: grad norm: 1.920 1.896 0.305
2024-12-02-08:16:36-root-INFO: grad norm: 1.847 1.823 0.301
2024-12-02-08:16:37-root-INFO: grad norm: 1.806 1.783 0.285
2024-12-02-08:16:37-root-INFO: Loss Change: 63.513 -> 59.371
2024-12-02-08:16:37-root-INFO: Regularization Change: 0.000 -> 3.779
2024-12-02-08:16:37-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-08:16:37-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-08:16:37-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-08:16:38-root-INFO: grad norm: 2.592 2.511 0.644
2024-12-02-08:16:38-root-INFO: grad norm: 2.884 2.817 0.616
2024-12-02-08:16:39-root-INFO: grad norm: 4.241 4.166 0.794
2024-12-02-08:16:39-root-INFO: Loss too large (58.433->59.149)! Learning rate decreased to 0.16169.
2024-12-02-08:16:39-root-INFO: grad norm: 4.700 4.620 0.865
2024-12-02-08:16:40-root-INFO: grad norm: 6.409 6.335 0.968
2024-12-02-08:16:40-root-INFO: Loss too large (58.195->58.942)! Learning rate decreased to 0.12935.
2024-12-02-08:16:40-root-INFO: grad norm: 4.416 4.350 0.764
2024-12-02-08:16:41-root-INFO: grad norm: 2.527 2.495 0.401
2024-12-02-08:16:41-root-INFO: grad norm: 1.995 1.973 0.292
2024-12-02-08:16:42-root-INFO: Loss Change: 59.125 -> 56.189
2024-12-02-08:16:42-root-INFO: Regularization Change: 0.000 -> 3.529
2024-12-02-08:16:42-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-08:16:42-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-08:16:42-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-08:16:42-root-INFO: grad norm: 2.329 2.285 0.450
2024-12-02-08:16:42-root-INFO: grad norm: 3.513 3.469 0.556
2024-12-02-08:16:43-root-INFO: Loss too large (56.032->56.697)! Learning rate decreased to 0.16529.
2024-12-02-08:16:43-root-INFO: Loss too large (56.032->56.059)! Learning rate decreased to 0.13223.
2024-12-02-08:16:43-root-INFO: grad norm: 3.372 3.345 0.421
2024-12-02-08:16:44-root-INFO: grad norm: 3.215 3.187 0.424
2024-12-02-08:16:44-root-INFO: grad norm: 3.172 3.150 0.372
2024-12-02-08:16:45-root-INFO: grad norm: 3.090 3.065 0.396
2024-12-02-08:16:45-root-INFO: grad norm: 2.999 2.979 0.351
2024-12-02-08:16:46-root-INFO: grad norm: 2.957 2.933 0.378
2024-12-02-08:16:46-root-INFO: Loss Change: 56.215 -> 54.277
2024-12-02-08:16:46-root-INFO: Regularization Change: 0.000 -> 2.291
2024-12-02-08:16:46-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-08:16:46-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-08:16:46-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-08:16:46-root-INFO: grad norm: 3.990 3.918 0.755
2024-12-02-08:16:46-root-INFO: Loss too large (54.490->55.388)! Learning rate decreased to 0.16894.
2024-12-02-08:16:46-root-INFO: Loss too large (54.490->54.717)! Learning rate decreased to 0.13515.
2024-12-02-08:16:47-root-INFO: grad norm: 3.528 3.491 0.509
2024-12-02-08:16:47-root-INFO: grad norm: 3.095 3.067 0.412
2024-12-02-08:16:48-root-INFO: grad norm: 3.062 3.035 0.401
2024-12-02-08:16:48-root-INFO: grad norm: 3.074 3.053 0.360
2024-12-02-08:16:49-root-INFO: grad norm: 3.069 3.045 0.383
2024-12-02-08:16:49-root-INFO: grad norm: 3.038 3.018 0.346
2024-12-02-08:16:50-root-INFO: grad norm: 3.036 3.013 0.372
2024-12-02-08:16:50-root-INFO: Loss Change: 54.490 -> 52.636
2024-12-02-08:16:50-root-INFO: Regularization Change: 0.000 -> 1.799
2024-12-02-08:16:50-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-08:16:50-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-08:16:50-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-08:16:50-root-INFO: grad norm: 4.160 4.084 0.788
2024-12-02-08:16:51-root-INFO: Loss too large (52.774->53.837)! Learning rate decreased to 0.17265.
2024-12-02-08:16:51-root-INFO: Loss too large (52.774->53.096)! Learning rate decreased to 0.13812.
2024-12-02-08:16:51-root-INFO: grad norm: 3.625 3.590 0.500
2024-12-02-08:16:52-root-INFO: grad norm: 3.041 3.013 0.408
2024-12-02-08:16:52-root-INFO: grad norm: 3.031 3.008 0.378
2024-12-02-08:16:53-root-INFO: grad norm: 3.105 3.084 0.357
2024-12-02-08:16:53-root-INFO: grad norm: 3.111 3.089 0.372
2024-12-02-08:16:53-root-INFO: grad norm: 3.102 3.082 0.348
2024-12-02-08:16:54-root-INFO: grad norm: 3.092 3.070 0.366
2024-12-02-08:16:54-root-INFO: Loss Change: 52.774 -> 51.042
2024-12-02-08:16:54-root-INFO: Regularization Change: 0.000 -> 1.672
2024-12-02-08:16:54-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-08:16:54-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-08:16:54-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-08:16:55-root-INFO: grad norm: 4.060 3.994 0.727
2024-12-02-08:16:55-root-INFO: Loss too large (51.074->52.187)! Learning rate decreased to 0.17641.
2024-12-02-08:16:55-root-INFO: Loss too large (51.074->51.413)! Learning rate decreased to 0.14113.
2024-12-02-08:16:55-root-INFO: grad norm: 3.605 3.570 0.503
2024-12-02-08:16:56-root-INFO: grad norm: 3.142 3.115 0.415
2024-12-02-08:16:56-root-INFO: grad norm: 3.084 3.059 0.394
2024-12-02-08:16:57-root-INFO: grad norm: 3.072 3.051 0.363
2024-12-02-08:16:57-root-INFO: grad norm: 3.049 3.026 0.375
2024-12-02-08:16:58-root-INFO: grad norm: 3.015 2.995 0.348
2024-12-02-08:16:58-root-INFO: grad norm: 3.001 2.978 0.364
2024-12-02-08:16:59-root-INFO: Loss Change: 51.074 -> 49.439
2024-12-02-08:16:59-root-INFO: Regularization Change: 0.000 -> 1.583
2024-12-02-08:16:59-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-08:16:59-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-08:16:59-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-08:16:59-root-INFO: grad norm: 4.229 4.160 0.764
2024-12-02-08:16:59-root-INFO: Loss too large (49.454->50.677)! Learning rate decreased to 0.18022.
2024-12-02-08:16:59-root-INFO: Loss too large (49.454->49.857)! Learning rate decreased to 0.14417.
2024-12-02-08:17:00-root-INFO: grad norm: 3.642 3.602 0.532
2024-12-02-08:17:00-root-INFO: grad norm: 2.982 2.954 0.411
2024-12-02-08:17:01-root-INFO: grad norm: 2.927 2.902 0.383
2024-12-02-08:17:01-root-INFO: grad norm: 2.942 2.922 0.350
2024-12-02-08:17:02-root-INFO: grad norm: 2.948 2.925 0.363
2024-12-02-08:17:02-root-INFO: grad norm: 2.952 2.932 0.339
2024-12-02-08:17:03-root-INFO: grad norm: 2.951 2.930 0.356
2024-12-02-08:17:03-root-INFO: Loss Change: 49.454 -> 47.848
2024-12-02-08:17:03-root-INFO: Regularization Change: 0.000 -> 1.533
2024-12-02-08:17:03-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-08:17:03-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-08:17:03-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-08:17:03-root-INFO: grad norm: 3.855 3.788 0.720
2024-12-02-08:17:03-root-INFO: Loss too large (48.048->49.140)! Learning rate decreased to 0.18408.
2024-12-02-08:17:04-root-INFO: Loss too large (48.048->48.374)! Learning rate decreased to 0.14727.
2024-12-02-08:17:04-root-INFO: grad norm: 3.488 3.454 0.483
2024-12-02-08:17:05-root-INFO: grad norm: 3.177 3.149 0.417
2024-12-02-08:17:05-root-INFO: grad norm: 3.119 3.095 0.391
2024-12-02-08:17:05-root-INFO: grad norm: 3.057 3.036 0.363
2024-12-02-08:17:06-root-INFO: grad norm: 3.029 3.007 0.364
2024-12-02-08:17:06-root-INFO: grad norm: 2.988 2.968 0.342
2024-12-02-08:17:07-root-INFO: grad norm: 2.965 2.945 0.351
2024-12-02-08:17:07-root-INFO: Loss Change: 48.048 -> 46.584
2024-12-02-08:17:07-root-INFO: Regularization Change: 0.000 -> 1.487
2024-12-02-08:17:07-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-08:17:07-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-08:17:07-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-08:17:08-root-INFO: grad norm: 4.097 4.026 0.759
2024-12-02-08:17:08-root-INFO: Loss too large (46.751->48.035)! Learning rate decreased to 0.18800.
2024-12-02-08:17:08-root-INFO: Loss too large (46.751->47.175)! Learning rate decreased to 0.15040.
2024-12-02-08:17:08-root-INFO: grad norm: 3.622 3.586 0.512
2024-12-02-08:17:09-root-INFO: grad norm: 3.140 3.113 0.414
2024-12-02-08:17:09-root-INFO: grad norm: 3.080 3.055 0.386
2024-12-02-08:17:10-root-INFO: grad norm: 3.035 3.014 0.358
2024-12-02-08:17:10-root-INFO: grad norm: 3.012 2.991 0.360
2024-12-02-08:17:11-root-INFO: grad norm: 2.979 2.959 0.341
2024-12-02-08:17:11-root-INFO: grad norm: 2.959 2.939 0.348
2024-12-02-08:17:11-root-INFO: Loss Change: 46.751 -> 45.280
2024-12-02-08:17:11-root-INFO: Regularization Change: 0.000 -> 1.457
2024-12-02-08:17:11-root-INFO: Undo step: 55
2024-12-02-08:17:11-root-INFO: Undo step: 56
2024-12-02-08:17:11-root-INFO: Undo step: 57
2024-12-02-08:17:11-root-INFO: Undo step: 58
2024-12-02-08:17:11-root-INFO: Undo step: 59
2024-12-02-08:17:12-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-08:17:12-root-INFO: grad norm: 24.318 24.105 3.210
2024-12-02-08:17:12-root-INFO: grad norm: 17.168 17.036 2.127
2024-12-02-08:17:13-root-INFO: grad norm: 11.881 11.704 2.042
2024-12-02-08:17:13-root-INFO: grad norm: 10.899 10.753 1.776
2024-12-02-08:17:14-root-INFO: grad norm: 11.196 10.972 2.226
2024-12-02-08:17:14-root-INFO: grad norm: 12.075 11.884 2.142
2024-12-02-08:17:15-root-INFO: grad norm: 12.135 11.830 2.704
2024-12-02-08:17:15-root-INFO: grad norm: 11.974 11.757 2.270
2024-12-02-08:17:16-root-INFO: Loss Change: 164.097 -> 68.512
2024-12-02-08:17:16-root-INFO: Regularization Change: 0.000 -> 91.561
2024-12-02-08:17:16-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-08:17:16-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-08:17:16-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-08:17:16-root-INFO: grad norm: 11.283 11.061 2.229
2024-12-02-08:17:16-root-INFO: grad norm: 12.268 12.034 2.386
2024-12-02-08:17:17-root-INFO: Loss too large (66.268->67.550)! Learning rate decreased to 0.17265.
2024-12-02-08:17:17-root-INFO: grad norm: 8.610 8.405 1.866
2024-12-02-08:17:18-root-INFO: grad norm: 5.933 5.813 1.186
2024-12-02-08:17:18-root-INFO: grad norm: 4.981 4.882 0.986
2024-12-02-08:17:19-root-INFO: grad norm: 4.815 4.740 0.850
2024-12-02-08:17:19-root-INFO: grad norm: 4.585 4.508 0.832
2024-12-02-08:17:20-root-INFO: grad norm: 4.455 4.390 0.755
2024-12-02-08:17:20-root-INFO: Loss Change: 67.154 -> 55.228
2024-12-02-08:17:20-root-INFO: Regularization Change: 0.000 -> 9.432
2024-12-02-08:17:20-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-08:17:20-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-08:17:20-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-08:17:20-root-INFO: grad norm: 3.898 3.864 0.520
2024-12-02-08:17:21-root-INFO: grad norm: 6.209 6.155 0.815
2024-12-02-08:17:21-root-INFO: Loss too large (54.608->56.057)! Learning rate decreased to 0.17641.
2024-12-02-08:17:21-root-INFO: Loss too large (54.608->54.842)! Learning rate decreased to 0.14113.
2024-12-02-08:17:21-root-INFO: grad norm: 4.196 4.143 0.665
2024-12-02-08:17:22-root-INFO: grad norm: 2.252 2.221 0.370
2024-12-02-08:17:22-root-INFO: grad norm: 1.912 1.887 0.309
2024-12-02-08:17:23-root-INFO: grad norm: 1.739 1.718 0.269
2024-12-02-08:17:23-root-INFO: grad norm: 1.650 1.628 0.268
2024-12-02-08:17:24-root-INFO: grad norm: 1.589 1.570 0.246
2024-12-02-08:17:24-root-INFO: Loss Change: 54.745 -> 51.138
2024-12-02-08:17:24-root-INFO: Regularization Change: 0.000 -> 3.624
2024-12-02-08:17:24-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-08:17:24-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-08:17:24-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-08:17:24-root-INFO: grad norm: 2.029 1.979 0.449
2024-12-02-08:17:25-root-INFO: grad norm: 2.113 2.066 0.441
2024-12-02-08:17:25-root-INFO: grad norm: 2.769 2.722 0.504
2024-12-02-08:17:26-root-INFO: Loss too large (50.134->50.203)! Learning rate decreased to 0.18022.
2024-12-02-08:17:26-root-INFO: grad norm: 3.110 3.062 0.544
2024-12-02-08:17:26-root-INFO: grad norm: 4.329 4.287 0.606
2024-12-02-08:17:27-root-INFO: Loss too large (49.802->50.160)! Learning rate decreased to 0.14417.
2024-12-02-08:17:27-root-INFO: grad norm: 3.588 3.546 0.548
2024-12-02-08:17:28-root-INFO: grad norm: 2.597 2.568 0.388
2024-12-02-08:17:28-root-INFO: grad norm: 2.433 2.406 0.361
2024-12-02-08:17:28-root-INFO: Loss Change: 50.902 -> 48.493
2024-12-02-08:17:28-root-INFO: Regularization Change: 0.000 -> 3.397
2024-12-02-08:17:28-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-08:17:28-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-08:17:28-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-08:17:29-root-INFO: grad norm: 3.342 3.271 0.687
2024-12-02-08:17:29-root-INFO: Loss too large (48.674->49.162)! Learning rate decreased to 0.18408.
2024-12-02-08:17:29-root-INFO: grad norm: 3.774 3.720 0.636
2024-12-02-08:17:30-root-INFO: grad norm: 4.980 4.930 0.708
2024-12-02-08:17:30-root-INFO: Loss too large (48.487->49.057)! Learning rate decreased to 0.14727.
2024-12-02-08:17:30-root-INFO: grad norm: 3.908 3.861 0.602
2024-12-02-08:17:31-root-INFO: grad norm: 2.474 2.445 0.379
2024-12-02-08:17:31-root-INFO: grad norm: 2.239 2.214 0.331
2024-12-02-08:17:32-root-INFO: grad norm: 2.092 2.073 0.279
2024-12-02-08:17:32-root-INFO: grad norm: 2.032 2.011 0.290
2024-12-02-08:17:33-root-INFO: Loss Change: 48.674 -> 46.644
2024-12-02-08:17:33-root-INFO: Regularization Change: 0.000 -> 2.235
2024-12-02-08:17:33-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-08:17:33-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-08:17:33-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-08:17:33-root-INFO: grad norm: 3.294 3.223 0.681
2024-12-02-08:17:33-root-INFO: Loss too large (46.773->47.351)! Learning rate decreased to 0.18800.
2024-12-02-08:17:33-root-INFO: Loss too large (46.773->46.832)! Learning rate decreased to 0.15040.
2024-12-02-08:17:34-root-INFO: grad norm: 2.907 2.874 0.434
2024-12-02-08:17:34-root-INFO: grad norm: 2.742 2.718 0.365
2024-12-02-08:17:35-root-INFO: grad norm: 2.662 2.637 0.367
2024-12-02-08:17:35-root-INFO: grad norm: 2.572 2.551 0.327
2024-12-02-08:17:36-root-INFO: grad norm: 2.529 2.505 0.344
2024-12-02-08:17:36-root-INFO: grad norm: 2.481 2.461 0.313
2024-12-02-08:17:37-root-INFO: grad norm: 2.458 2.436 0.334
2024-12-02-08:17:37-root-INFO: Loss Change: 46.773 -> 45.101
2024-12-02-08:17:37-root-INFO: Regularization Change: 0.000 -> 1.788
2024-12-02-08:17:37-root-INFO: Undo step: 55
2024-12-02-08:17:37-root-INFO: Undo step: 56
2024-12-02-08:17:37-root-INFO: Undo step: 57
2024-12-02-08:17:37-root-INFO: Undo step: 58
2024-12-02-08:17:37-root-INFO: Undo step: 59
2024-12-02-08:17:37-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-08:17:37-root-INFO: grad norm: 24.839 24.554 3.751
2024-12-02-08:17:38-root-INFO: grad norm: 15.479 15.275 2.503
2024-12-02-08:17:38-root-INFO: grad norm: 10.355 10.022 2.604
2024-12-02-08:17:39-root-INFO: grad norm: 7.783 7.667 1.338
2024-12-02-08:17:39-root-INFO: grad norm: 7.570 7.471 1.223
2024-12-02-08:17:40-root-INFO: grad norm: 8.670 8.562 1.365
2024-12-02-08:17:40-root-INFO: grad norm: 9.699 9.555 1.666
2024-12-02-08:17:41-root-INFO: grad norm: 11.830 11.655 2.026
2024-12-02-08:17:41-root-INFO: Loss too large (67.397->69.783)! Learning rate decreased to 0.16894.
2024-12-02-08:17:41-root-INFO: Loss Change: 169.934 -> 65.047
2024-12-02-08:17:41-root-INFO: Regularization Change: 0.000 -> 97.364
2024-12-02-08:17:41-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-08:17:41-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-08:17:42-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-08:17:42-root-INFO: grad norm: 8.462 8.342 1.423
2024-12-02-08:17:42-root-INFO: grad norm: 10.758 10.601 1.828
2024-12-02-08:17:42-root-INFO: Loss too large (63.396->65.686)! Learning rate decreased to 0.17265.
2024-12-02-08:17:43-root-INFO: grad norm: 8.112 7.975 1.484
2024-12-02-08:17:43-root-INFO: grad norm: 6.176 6.085 1.055
2024-12-02-08:17:44-root-INFO: grad norm: 5.200 5.111 0.960
2024-12-02-08:17:44-root-INFO: grad norm: 4.580 4.513 0.783
2024-12-02-08:17:45-root-INFO: grad norm: 4.361 4.289 0.785
2024-12-02-08:17:45-root-INFO: grad norm: 4.424 4.367 0.711
2024-12-02-08:17:46-root-INFO: Loss Change: 64.179 -> 54.630
2024-12-02-08:17:46-root-INFO: Regularization Change: 0.000 -> 9.415
2024-12-02-08:17:46-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-08:17:46-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-08:17:46-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-08:17:46-root-INFO: grad norm: 3.929 3.890 0.551
2024-12-02-08:17:47-root-INFO: grad norm: 6.276 6.222 0.826
2024-12-02-08:17:47-root-INFO: Loss too large (54.072->55.657)! Learning rate decreased to 0.17641.
2024-12-02-08:17:47-root-INFO: Loss too large (54.072->54.270)! Learning rate decreased to 0.14113.
2024-12-02-08:17:47-root-INFO: grad norm: 4.363 4.309 0.690
2024-12-02-08:17:48-root-INFO: grad norm: 2.512 2.481 0.394
2024-12-02-08:17:48-root-INFO: grad norm: 2.104 2.076 0.341
2024-12-02-08:17:49-root-INFO: grad norm: 1.858 1.837 0.281
2024-12-02-08:17:49-root-INFO: grad norm: 1.730 1.707 0.279
2024-12-02-08:17:50-root-INFO: grad norm: 1.636 1.616 0.250
2024-12-02-08:17:50-root-INFO: Loss Change: 54.137 -> 50.400
2024-12-02-08:17:50-root-INFO: Regularization Change: 0.000 -> 3.843
2024-12-02-08:17:50-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-08:17:50-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-08:17:50-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-08:17:50-root-INFO: grad norm: 2.043 1.996 0.436
2024-12-02-08:17:51-root-INFO: grad norm: 2.165 2.120 0.441
2024-12-02-08:17:51-root-INFO: grad norm: 2.848 2.800 0.521
2024-12-02-08:17:52-root-INFO: Loss too large (49.348->49.377)! Learning rate decreased to 0.18022.
2024-12-02-08:17:52-root-INFO: grad norm: 3.050 2.999 0.554
2024-12-02-08:17:52-root-INFO: grad norm: 3.744 3.697 0.594
2024-12-02-08:17:53-root-INFO: Loss too large (48.853->48.937)! Learning rate decreased to 0.14417.
2024-12-02-08:17:53-root-INFO: grad norm: 3.130 3.089 0.504
2024-12-02-08:17:54-root-INFO: grad norm: 2.503 2.473 0.383
2024-12-02-08:17:54-root-INFO: grad norm: 2.293 2.267 0.349
2024-12-02-08:17:54-root-INFO: Loss Change: 50.144 -> 47.575
2024-12-02-08:17:54-root-INFO: Regularization Change: 0.000 -> 3.583
2024-12-02-08:17:54-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-08:17:54-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-08:17:55-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-08:17:55-root-INFO: grad norm: 3.134 3.062 0.670
2024-12-02-08:17:55-root-INFO: Loss too large (47.744->48.050)! Learning rate decreased to 0.18408.
2024-12-02-08:17:55-root-INFO: grad norm: 3.550 3.496 0.616
2024-12-02-08:17:56-root-INFO: grad norm: 4.664 4.610 0.705
2024-12-02-08:17:56-root-INFO: Loss too large (47.500->47.931)! Learning rate decreased to 0.14727.
2024-12-02-08:17:56-root-INFO: grad norm: 3.806 3.760 0.594
2024-12-02-08:17:57-root-INFO: grad norm: 2.730 2.697 0.418
2024-12-02-08:17:57-root-INFO: grad norm: 2.424 2.397 0.361
2024-12-02-08:17:58-root-INFO: grad norm: 2.151 2.129 0.304
2024-12-02-08:17:58-root-INFO: grad norm: 2.012 1.991 0.294
2024-12-02-08:17:59-root-INFO: Loss Change: 47.744 -> 45.662
2024-12-02-08:17:59-root-INFO: Regularization Change: 0.000 -> 2.331
2024-12-02-08:17:59-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-08:17:59-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-08:17:59-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-08:17:59-root-INFO: grad norm: 3.184 3.112 0.673
2024-12-02-08:17:59-root-INFO: Loss too large (45.782->46.244)! Learning rate decreased to 0.18800.
2024-12-02-08:18:00-root-INFO: grad norm: 3.667 3.614 0.621
2024-12-02-08:18:00-root-INFO: grad norm: 4.879 4.827 0.714
2024-12-02-08:18:00-root-INFO: Loss too large (45.692->46.259)! Learning rate decreased to 0.15040.
2024-12-02-08:18:01-root-INFO: grad norm: 3.957 3.911 0.597
2024-12-02-08:18:01-root-INFO: grad norm: 2.779 2.746 0.421
2024-12-02-08:18:02-root-INFO: grad norm: 2.456 2.430 0.355
2024-12-02-08:18:02-root-INFO: grad norm: 2.168 2.146 0.305
2024-12-02-08:18:03-root-INFO: grad norm: 2.024 2.003 0.287
2024-12-02-08:18:03-root-INFO: Loss Change: 45.782 -> 43.929
2024-12-02-08:18:03-root-INFO: Regularization Change: 0.000 -> 2.070
2024-12-02-08:18:03-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-08:18:03-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-08:18:03-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-08:18:03-root-INFO: grad norm: 2.897 2.833 0.605
2024-12-02-08:18:04-root-INFO: Loss too large (43.906->44.264)! Learning rate decreased to 0.19197.
2024-12-02-08:18:04-root-INFO: grad norm: 3.370 3.325 0.549
2024-12-02-08:18:04-root-INFO: grad norm: 4.552 4.503 0.671
2024-12-02-08:18:05-root-INFO: Loss too large (43.828->44.355)! Learning rate decreased to 0.15357.
2024-12-02-08:18:05-root-INFO: grad norm: 3.776 3.734 0.561
2024-12-02-08:18:06-root-INFO: grad norm: 2.775 2.743 0.422
2024-12-02-08:18:06-root-INFO: grad norm: 2.467 2.441 0.354
2024-12-02-08:18:06-root-INFO: grad norm: 2.175 2.152 0.314
2024-12-02-08:18:07-root-INFO: grad norm: 2.027 2.007 0.286
2024-12-02-08:18:07-root-INFO: Loss Change: 43.906 -> 42.256
2024-12-02-08:18:07-root-INFO: Regularization Change: 0.000 -> 1.914
2024-12-02-08:18:07-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-08:18:07-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-08:18:07-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-08:18:08-root-INFO: grad norm: 2.962 2.903 0.587
2024-12-02-08:18:08-root-INFO: Loss too large (42.443->42.926)! Learning rate decreased to 0.19599.
2024-12-02-08:18:08-root-INFO: Loss too large (42.443->42.466)! Learning rate decreased to 0.15679.
2024-12-02-08:18:08-root-INFO: grad norm: 2.568 2.537 0.394
2024-12-02-08:18:09-root-INFO: grad norm: 2.332 2.307 0.340
2024-12-02-08:18:09-root-INFO: grad norm: 2.209 2.187 0.317
2024-12-02-08:18:10-root-INFO: grad norm: 2.086 2.065 0.291
2024-12-02-08:18:10-root-INFO: grad norm: 2.014 1.994 0.284
2024-12-02-08:18:11-root-INFO: grad norm: 1.941 1.922 0.268
2024-12-02-08:18:11-root-INFO: grad norm: 1.897 1.878 0.267
2024-12-02-08:18:11-root-INFO: Loss Change: 42.443 -> 40.967
2024-12-02-08:18:11-root-INFO: Regularization Change: 0.000 -> 1.592
2024-12-02-08:18:11-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-08:18:11-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-08:18:12-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-08:18:12-root-INFO: grad norm: 2.755 2.699 0.551
2024-12-02-08:18:12-root-INFO: Loss too large (41.013->41.439)! Learning rate decreased to 0.20006.
2024-12-02-08:18:12-root-INFO: Loss too large (41.013->41.031)! Learning rate decreased to 0.16005.
2024-12-02-08:18:13-root-INFO: grad norm: 2.456 2.426 0.381
2024-12-02-08:18:13-root-INFO: grad norm: 2.330 2.307 0.328
2024-12-02-08:18:14-root-INFO: grad norm: 2.256 2.233 0.323
2024-12-02-08:18:14-root-INFO: grad norm: 2.175 2.156 0.292
2024-12-02-08:18:15-root-INFO: grad norm: 2.126 2.105 0.297
2024-12-02-08:18:15-root-INFO: grad norm: 2.071 2.053 0.275
2024-12-02-08:18:15-root-INFO: grad norm: 2.038 2.018 0.281
2024-12-02-08:18:16-root-INFO: Loss Change: 41.013 -> 39.671
2024-12-02-08:18:16-root-INFO: Regularization Change: 0.000 -> 1.531
2024-12-02-08:18:16-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-08:18:16-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-08:18:16-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-08:18:16-root-INFO: grad norm: 3.259 3.186 0.685
2024-12-02-08:18:16-root-INFO: Loss too large (39.821->40.596)! Learning rate decreased to 0.20419.
2024-12-02-08:18:16-root-INFO: Loss too large (39.821->39.977)! Learning rate decreased to 0.16335.
2024-12-02-08:18:17-root-INFO: grad norm: 2.901 2.870 0.424
2024-12-02-08:18:17-root-INFO: grad norm: 2.728 2.699 0.394
2024-12-02-08:18:18-root-INFO: grad norm: 2.624 2.599 0.355
2024-12-02-08:18:18-root-INFO: grad norm: 2.495 2.472 0.341
2024-12-02-08:18:19-root-INFO: grad norm: 2.419 2.397 0.324
2024-12-02-08:18:19-root-INFO: grad norm: 2.325 2.304 0.313
2024-12-02-08:18:20-root-INFO: grad norm: 2.269 2.248 0.303
2024-12-02-08:18:20-root-INFO: Loss Change: 39.821 -> 38.438
2024-12-02-08:18:20-root-INFO: Regularization Change: 0.000 -> 1.507
2024-12-02-08:18:20-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-08:18:20-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-08:18:20-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-08:18:20-root-INFO: grad norm: 3.335 3.272 0.646
2024-12-02-08:18:21-root-INFO: Loss too large (38.370->39.246)! Learning rate decreased to 0.20837.
2024-12-02-08:18:21-root-INFO: Loss too large (38.370->38.580)! Learning rate decreased to 0.16669.
2024-12-02-08:18:21-root-INFO: grad norm: 2.975 2.943 0.438
2024-12-02-08:18:22-root-INFO: grad norm: 2.732 2.705 0.379
2024-12-02-08:18:22-root-INFO: grad norm: 2.599 2.575 0.358
2024-12-02-08:18:23-root-INFO: grad norm: 2.446 2.424 0.329
2024-12-02-08:18:23-root-INFO: grad norm: 2.357 2.335 0.322
2024-12-02-08:18:24-root-INFO: grad norm: 2.255 2.235 0.302
2024-12-02-08:18:24-root-INFO: grad norm: 2.195 2.174 0.299
2024-12-02-08:18:24-root-INFO: Loss Change: 38.370 -> 36.995
2024-12-02-08:18:24-root-INFO: Regularization Change: 0.000 -> 1.483
2024-12-02-08:18:24-root-INFO: Undo step: 50
2024-12-02-08:18:24-root-INFO: Undo step: 51
2024-12-02-08:18:24-root-INFO: Undo step: 52
2024-12-02-08:18:24-root-INFO: Undo step: 53
2024-12-02-08:18:24-root-INFO: Undo step: 54
2024-12-02-08:18:25-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-08:18:25-root-INFO: grad norm: 22.296 22.132 2.703
2024-12-02-08:18:25-root-INFO: grad norm: 11.381 11.243 1.767
2024-12-02-08:18:26-root-INFO: grad norm: 8.928 8.702 1.998
2024-12-02-08:18:26-root-INFO: grad norm: 7.531 7.431 1.222
2024-12-02-08:18:27-root-INFO: grad norm: 7.034 6.895 1.394
2024-12-02-08:18:27-root-INFO: grad norm: 6.817 6.728 1.095
2024-12-02-08:18:28-root-INFO: grad norm: 6.689 6.555 1.332
2024-12-02-08:18:28-root-INFO: grad norm: 6.403 6.323 1.006
2024-12-02-08:18:28-root-INFO: Loss Change: 145.549 -> 53.350
2024-12-02-08:18:28-root-INFO: Regularization Change: 0.000 -> 98.660
2024-12-02-08:18:28-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-08:18:28-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-08:18:29-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-08:18:29-root-INFO: grad norm: 6.456 6.359 1.112
2024-12-02-08:18:29-root-INFO: grad norm: 6.483 6.419 0.910
2024-12-02-08:18:30-root-INFO: grad norm: 6.505 6.425 1.018
2024-12-02-08:18:30-root-INFO: grad norm: 6.553 6.494 0.879
2024-12-02-08:18:31-root-INFO: grad norm: 6.637 6.567 0.961
2024-12-02-08:18:31-root-INFO: grad norm: 6.720 6.664 0.867
2024-12-02-08:18:31-root-INFO: grad norm: 6.861 6.797 0.934
2024-12-02-08:18:32-root-INFO: grad norm: 6.968 6.912 0.880
2024-12-02-08:18:32-root-INFO: Loss Change: 53.101 -> 47.809
2024-12-02-08:18:32-root-INFO: Regularization Change: 0.000 -> 11.601
2024-12-02-08:18:32-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-08:18:32-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-08:18:32-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-08:18:33-root-INFO: grad norm: 7.685 7.610 1.075
2024-12-02-08:18:33-root-INFO: grad norm: 7.811 7.736 1.081
2024-12-02-08:18:34-root-INFO: grad norm: 7.860 7.783 1.096
2024-12-02-08:18:34-root-INFO: grad norm: 7.890 7.810 1.118
2024-12-02-08:18:34-root-INFO: grad norm: 7.890 7.811 1.116
2024-12-02-08:18:35-root-INFO: grad norm: 7.835 7.752 1.141
2024-12-02-08:18:35-root-INFO: grad norm: 7.771 7.690 1.120
2024-12-02-08:18:36-root-INFO: grad norm: 7.661 7.575 1.145
2024-12-02-08:18:36-root-INFO: Loss Change: 48.188 -> 44.965
2024-12-02-08:18:36-root-INFO: Regularization Change: 0.000 -> 6.539
2024-12-02-08:18:36-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-08:18:36-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-08:18:36-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-08:18:37-root-INFO: grad norm: 8.220 8.111 1.338
2024-12-02-08:18:37-root-INFO: grad norm: 7.866 7.754 1.320
2024-12-02-08:18:37-root-INFO: grad norm: 7.469 7.371 1.208
2024-12-02-08:18:38-root-INFO: grad norm: 7.238 7.128 1.255
2024-12-02-08:18:38-root-INFO: grad norm: 7.320 7.209 1.275
2024-12-02-08:18:39-root-INFO: Loss too large (43.185->43.600)! Learning rate decreased to 0.20006.
2024-12-02-08:18:39-root-INFO: grad norm: 5.174 5.079 0.987
2024-12-02-08:18:39-root-INFO: grad norm: 4.792 4.728 0.779
2024-12-02-08:18:40-root-INFO: Loss too large (40.532->40.602)! Learning rate decreased to 0.16005.
2024-12-02-08:18:40-root-INFO: grad norm: 3.262 3.218 0.538
2024-12-02-08:18:40-root-INFO: Loss Change: 45.482 -> 39.342
2024-12-02-08:18:40-root-INFO: Regularization Change: 0.000 -> 4.595
2024-12-02-08:18:40-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-08:18:40-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-08:18:40-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-08:18:41-root-INFO: grad norm: 3.212 3.125 0.743
2024-12-02-08:18:41-root-INFO: Loss too large (39.488->39.834)! Learning rate decreased to 0.20419.
2024-12-02-08:18:41-root-INFO: grad norm: 3.300 3.249 0.579
2024-12-02-08:18:42-root-INFO: grad norm: 3.922 3.870 0.634
2024-12-02-08:18:42-root-INFO: Loss too large (39.141->39.362)! Learning rate decreased to 0.16335.
2024-12-02-08:18:42-root-INFO: grad norm: 3.048 3.013 0.460
2024-12-02-08:18:43-root-INFO: grad norm: 2.131 2.102 0.346
2024-12-02-08:18:43-root-INFO: grad norm: 1.859 1.841 0.263
2024-12-02-08:18:44-root-INFO: grad norm: 1.652 1.633 0.251
2024-12-02-08:18:44-root-INFO: grad norm: 1.549 1.534 0.215
2024-12-02-08:18:45-root-INFO: Loss Change: 39.488 -> 37.636
2024-12-02-08:18:45-root-INFO: Regularization Change: 0.000 -> 2.075
2024-12-02-08:18:45-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-08:18:45-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-08:18:45-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-08:18:45-root-INFO: grad norm: 2.518 2.457 0.549
2024-12-02-08:18:45-root-INFO: Loss too large (37.462->37.678)! Learning rate decreased to 0.20837.
2024-12-02-08:18:45-root-INFO: grad norm: 2.846 2.806 0.474
2024-12-02-08:18:46-root-INFO: grad norm: 3.794 3.752 0.566
2024-12-02-08:18:46-root-INFO: Loss too large (37.317->37.658)! Learning rate decreased to 0.16669.
2024-12-02-08:18:47-root-INFO: grad norm: 3.136 3.102 0.461
2024-12-02-08:18:47-root-INFO: grad norm: 2.324 2.297 0.359
2024-12-02-08:18:48-root-INFO: grad norm: 2.075 2.054 0.291
2024-12-02-08:18:48-root-INFO: grad norm: 1.859 1.839 0.273
2024-12-02-08:18:49-root-INFO: grad norm: 1.754 1.738 0.241
2024-12-02-08:18:49-root-INFO: Loss Change: 37.462 -> 35.999
2024-12-02-08:18:49-root-INFO: Regularization Change: 0.000 -> 1.851
2024-12-02-08:18:49-root-INFO: Undo step: 50
2024-12-02-08:18:49-root-INFO: Undo step: 51
2024-12-02-08:18:49-root-INFO: Undo step: 52
2024-12-02-08:18:49-root-INFO: Undo step: 53
2024-12-02-08:18:49-root-INFO: Undo step: 54
2024-12-02-08:18:49-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-08:18:49-root-INFO: grad norm: 23.569 23.396 2.852
2024-12-02-08:18:50-root-INFO: grad norm: 13.342 13.179 2.082
2024-12-02-08:18:50-root-INFO: grad norm: 8.637 8.550 1.228
2024-12-02-08:18:51-root-INFO: grad norm: 6.596 6.525 0.961
2024-12-02-08:18:51-root-INFO: grad norm: 6.112 6.072 0.700
2024-12-02-08:18:52-root-INFO: grad norm: 5.719 5.674 0.719
2024-12-02-08:18:52-root-INFO: grad norm: 5.204 5.174 0.560
2024-12-02-08:18:52-root-INFO: grad norm: 5.324 5.287 0.629
2024-12-02-08:18:53-root-INFO: Loss Change: 153.136 -> 51.281
2024-12-02-08:18:53-root-INFO: Regularization Change: 0.000 -> 104.606
2024-12-02-08:18:53-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-08:18:53-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-08:18:53-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-08:18:53-root-INFO: grad norm: 6.243 6.205 0.693
2024-12-02-08:18:54-root-INFO: grad norm: 5.631 5.589 0.690
2024-12-02-08:18:54-root-INFO: grad norm: 5.470 5.435 0.617
2024-12-02-08:18:54-root-INFO: grad norm: 5.042 5.008 0.582
2024-12-02-08:18:55-root-INFO: grad norm: 5.134 5.100 0.585
2024-12-02-08:18:55-root-INFO: grad norm: 5.373 5.330 0.680
2024-12-02-08:18:56-root-INFO: grad norm: 6.126 6.065 0.866
2024-12-02-08:18:56-root-INFO: Loss too large (45.962->46.393)! Learning rate decreased to 0.19197.
2024-12-02-08:18:57-root-INFO: grad norm: 4.512 4.454 0.720
2024-12-02-08:18:57-root-INFO: Loss Change: 51.047 -> 43.688
2024-12-02-08:18:57-root-INFO: Regularization Change: 0.000 -> 11.190
2024-12-02-08:18:57-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-08:18:57-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-08:18:57-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-08:18:57-root-INFO: grad norm: 4.015 3.926 0.841
2024-12-02-08:18:57-root-INFO: Loss too large (43.930->43.989)! Learning rate decreased to 0.19599.
2024-12-02-08:18:58-root-INFO: grad norm: 3.561 3.504 0.632
2024-12-02-08:18:58-root-INFO: grad norm: 3.495 3.442 0.610
2024-12-02-08:18:59-root-INFO: grad norm: 3.411 3.362 0.572
2024-12-02-08:18:59-root-INFO: grad norm: 3.351 3.300 0.579
2024-12-02-08:19:00-root-INFO: grad norm: 3.341 3.294 0.558
2024-12-02-08:19:00-root-INFO: grad norm: 3.361 3.311 0.575
2024-12-02-08:19:00-root-INFO: grad norm: 3.362 3.314 0.564
2024-12-02-08:19:01-root-INFO: Loss Change: 43.930 -> 40.804
2024-12-02-08:19:01-root-INFO: Regularization Change: 0.000 -> 4.477
2024-12-02-08:19:01-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-08:19:01-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-08:19:01-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-08:19:01-root-INFO: grad norm: 4.291 4.203 0.865
2024-12-02-08:19:01-root-INFO: Loss too large (40.963->41.684)! Learning rate decreased to 0.20006.
2024-12-02-08:19:02-root-INFO: grad norm: 3.967 3.898 0.734
2024-12-02-08:19:02-root-INFO: grad norm: 3.551 3.492 0.645
2024-12-02-08:19:03-root-INFO: grad norm: 3.469 3.416 0.605
2024-12-02-08:19:03-root-INFO: grad norm: 3.438 3.387 0.593
2024-12-02-08:19:04-root-INFO: grad norm: 3.399 3.349 0.582
2024-12-02-08:19:04-root-INFO: grad norm: 3.347 3.297 0.576
2024-12-02-08:19:05-root-INFO: grad norm: 3.331 3.282 0.569
2024-12-02-08:19:05-root-INFO: Loss Change: 40.963 -> 38.554
2024-12-02-08:19:05-root-INFO: Regularization Change: 0.000 -> 3.389
2024-12-02-08:19:05-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-08:19:05-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-08:19:06-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-08:19:06-root-INFO: grad norm: 4.563 4.456 0.983
2024-12-02-08:19:06-root-INFO: Loss too large (38.851->39.809)! Learning rate decreased to 0.20419.
2024-12-02-08:19:06-root-INFO: grad norm: 4.186 4.112 0.785
2024-12-02-08:19:07-root-INFO: grad norm: 3.702 3.634 0.705
2024-12-02-08:19:07-root-INFO: grad norm: 3.592 3.535 0.634
2024-12-02-08:19:08-root-INFO: grad norm: 3.531 3.474 0.630
2024-12-02-08:19:08-root-INFO: grad norm: 3.471 3.419 0.601
2024-12-02-08:19:09-root-INFO: grad norm: 3.395 3.341 0.600
2024-12-02-08:19:09-root-INFO: grad norm: 3.366 3.316 0.581
2024-12-02-08:19:10-root-INFO: Loss Change: 38.851 -> 36.685
2024-12-02-08:19:10-root-INFO: Regularization Change: 0.000 -> 2.874
2024-12-02-08:19:10-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-08:19:10-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-08:19:10-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-08:19:10-root-INFO: grad norm: 4.490 4.391 0.940
2024-12-02-08:19:10-root-INFO: Loss too large (36.744->37.726)! Learning rate decreased to 0.20837.
2024-12-02-08:19:10-root-INFO: Loss too large (36.744->36.744)! Learning rate decreased to 0.16669.
2024-12-02-08:19:11-root-INFO: grad norm: 3.012 2.968 0.516
2024-12-02-08:19:11-root-INFO: grad norm: 1.905 1.877 0.326
2024-12-02-08:19:12-root-INFO: grad norm: 1.545 1.527 0.231
2024-12-02-08:19:12-root-INFO: grad norm: 1.312 1.295 0.206
2024-12-02-08:19:13-root-INFO: grad norm: 1.193 1.181 0.174
2024-12-02-08:19:13-root-INFO: grad norm: 1.109 1.096 0.171
2024-12-02-08:19:14-root-INFO: grad norm: 1.058 1.047 0.155
2024-12-02-08:19:14-root-INFO: Loss Change: 36.744 -> 34.608
2024-12-02-08:19:14-root-INFO: Regularization Change: 0.000 -> 1.814
2024-12-02-08:19:14-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-08:19:14-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-08:19:14-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-08:19:14-root-INFO: grad norm: 2.216 2.156 0.515
2024-12-02-08:19:15-root-INFO: grad norm: 2.997 2.940 0.578
2024-12-02-08:19:15-root-INFO: Loss too large (34.691->35.131)! Learning rate decreased to 0.21260.
2024-12-02-08:19:16-root-INFO: grad norm: 3.522 3.475 0.577
2024-12-02-08:19:16-root-INFO: Loss too large (34.549->34.711)! Learning rate decreased to 0.17008.
2024-12-02-08:19:16-root-INFO: grad norm: 2.755 2.722 0.422
2024-12-02-08:19:17-root-INFO: grad norm: 2.013 1.989 0.313
2024-12-02-08:19:17-root-INFO: grad norm: 1.728 1.710 0.248
2024-12-02-08:19:18-root-INFO: grad norm: 1.490 1.474 0.222
2024-12-02-08:19:18-root-INFO: grad norm: 1.360 1.346 0.193
2024-12-02-08:19:18-root-INFO: Loss Change: 34.708 -> 33.273
2024-12-02-08:19:18-root-INFO: Regularization Change: 0.000 -> 1.905
2024-12-02-08:19:18-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-08:19:18-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-08:19:19-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-08:19:19-root-INFO: grad norm: 1.999 1.951 0.434
2024-12-02-08:19:19-root-INFO: Loss too large (33.244->33.276)! Learning rate decreased to 0.21688.
2024-12-02-08:19:19-root-INFO: grad norm: 2.197 2.165 0.374
2024-12-02-08:19:20-root-INFO: grad norm: 2.865 2.833 0.422
2024-12-02-08:19:20-root-INFO: Loss too large (33.035->33.175)! Learning rate decreased to 0.17350.
2024-12-02-08:19:21-root-INFO: grad norm: 2.459 2.433 0.358
2024-12-02-08:19:21-root-INFO: grad norm: 2.009 1.988 0.287
2024-12-02-08:19:21-root-INFO: grad norm: 1.803 1.785 0.254
2024-12-02-08:19:22-root-INFO: grad norm: 1.609 1.593 0.227
2024-12-02-08:19:22-root-INFO: grad norm: 1.499 1.485 0.209
2024-12-02-08:19:23-root-INFO: Loss Change: 33.244 -> 32.024
2024-12-02-08:19:23-root-INFO: Regularization Change: 0.000 -> 1.652
2024-12-02-08:19:23-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-08:19:23-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-08:19:23-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-08:19:23-root-INFO: grad norm: 2.553 2.488 0.573
2024-12-02-08:19:23-root-INFO: Loss too large (32.130->32.446)! Learning rate decreased to 0.22121.
2024-12-02-08:19:24-root-INFO: grad norm: 2.859 2.817 0.487
2024-12-02-08:19:24-root-INFO: grad norm: 3.669 3.625 0.564
2024-12-02-08:19:24-root-INFO: Loss too large (32.057->32.361)! Learning rate decreased to 0.17697.
2024-12-02-08:19:25-root-INFO: grad norm: 2.952 2.919 0.438
2024-12-02-08:19:25-root-INFO: grad norm: 2.162 2.137 0.329
2024-12-02-08:19:26-root-INFO: grad norm: 1.874 1.856 0.263
2024-12-02-08:19:26-root-INFO: grad norm: 1.630 1.612 0.237
2024-12-02-08:19:27-root-INFO: grad norm: 1.497 1.482 0.206
2024-12-02-08:19:27-root-INFO: Loss Change: 32.130 -> 30.853
2024-12-02-08:19:27-root-INFO: Regularization Change: 0.000 -> 1.603
2024-12-02-08:19:27-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-08:19:27-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-08:19:27-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-08:19:27-root-INFO: grad norm: 2.337 2.285 0.494
2024-12-02-08:19:27-root-INFO: Loss too large (30.907->31.172)! Learning rate decreased to 0.22559.
2024-12-02-08:19:28-root-INFO: grad norm: 2.666 2.629 0.443
2024-12-02-08:19:28-root-INFO: grad norm: 3.458 3.421 0.506
2024-12-02-08:19:28-root-INFO: Loss too large (30.851->31.133)! Learning rate decreased to 0.18047.
2024-12-02-08:19:29-root-INFO: grad norm: 2.835 2.804 0.414
2024-12-02-08:19:29-root-INFO: grad norm: 2.158 2.136 0.310
2024-12-02-08:19:30-root-INFO: grad norm: 1.883 1.864 0.263
2024-12-02-08:19:30-root-INFO: grad norm: 1.645 1.629 0.229
2024-12-02-08:19:31-root-INFO: grad norm: 1.512 1.498 0.208
2024-12-02-08:19:31-root-INFO: Loss Change: 30.907 -> 29.739
2024-12-02-08:19:31-root-INFO: Regularization Change: 0.000 -> 1.535
2024-12-02-08:19:31-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-08:19:31-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-08:19:31-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-08:19:32-root-INFO: grad norm: 2.401 2.344 0.519
2024-12-02-08:19:32-root-INFO: Loss too large (29.737->30.051)! Learning rate decreased to 0.23002.
2024-12-02-08:19:32-root-INFO: grad norm: 2.741 2.702 0.463
2024-12-02-08:19:32-root-INFO: Loss too large (29.702->29.718)! Learning rate decreased to 0.18402.
2024-12-02-08:19:33-root-INFO: grad norm: 2.297 2.273 0.332
2024-12-02-08:19:33-root-INFO: grad norm: 2.065 2.045 0.287
2024-12-02-08:19:34-root-INFO: grad norm: 1.842 1.825 0.252
2024-12-02-08:19:34-root-INFO: grad norm: 1.714 1.698 0.233
2024-12-02-08:19:35-root-INFO: grad norm: 1.594 1.579 0.216
2024-12-02-08:19:35-root-INFO: grad norm: 1.521 1.507 0.205
2024-12-02-08:19:35-root-INFO: Loss Change: 29.737 -> 28.619
2024-12-02-08:19:35-root-INFO: Regularization Change: 0.000 -> 1.422
2024-12-02-08:19:35-root-INFO: Undo step: 45
2024-12-02-08:19:35-root-INFO: Undo step: 46
2024-12-02-08:19:35-root-INFO: Undo step: 47
2024-12-02-08:19:35-root-INFO: Undo step: 48
2024-12-02-08:19:35-root-INFO: Undo step: 49
2024-12-02-08:19:36-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-08:19:36-root-INFO: grad norm: 19.845 19.696 2.432
2024-12-02-08:19:36-root-INFO: grad norm: 11.589 11.475 1.622
2024-12-02-08:19:37-root-INFO: grad norm: 9.290 9.146 1.626
2024-12-02-08:19:37-root-INFO: grad norm: 8.445 8.314 1.478
2024-12-02-08:19:38-root-INFO: grad norm: 8.548 8.359 1.784
2024-12-02-08:19:38-root-INFO: grad norm: 10.303 10.139 1.830
2024-12-02-08:19:38-root-INFO: Loss too large (52.703->53.556)! Learning rate decreased to 0.20837.
2024-12-02-08:19:39-root-INFO: grad norm: 7.165 7.007 1.499
2024-12-02-08:19:39-root-INFO: grad norm: 4.563 4.490 0.813
2024-12-02-08:19:39-root-INFO: Loss Change: 132.665 -> 43.881
2024-12-02-08:19:39-root-INFO: Regularization Change: 0.000 -> 97.763
2024-12-02-08:19:39-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-08:19:39-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-08:19:40-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-08:19:40-root-INFO: grad norm: 3.235 3.202 0.458
2024-12-02-08:19:40-root-INFO: grad norm: 3.554 3.522 0.474
2024-12-02-08:19:41-root-INFO: grad norm: 3.970 3.937 0.505
2024-12-02-08:19:41-root-INFO: grad norm: 4.901 4.866 0.585
2024-12-02-08:19:41-root-INFO: Loss too large (40.902->41.020)! Learning rate decreased to 0.21260.
2024-12-02-08:19:42-root-INFO: grad norm: 3.961 3.931 0.482
2024-12-02-08:19:42-root-INFO: grad norm: 2.920 2.896 0.372
2024-12-02-08:19:43-root-INFO: grad norm: 2.640 2.621 0.318
2024-12-02-08:19:43-root-INFO: grad norm: 2.470 2.451 0.302
2024-12-02-08:19:44-root-INFO: Loss Change: 43.576 -> 37.219
2024-12-02-08:19:44-root-INFO: Regularization Change: 0.000 -> 11.504
2024-12-02-08:19:44-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-08:19:44-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-08:19:44-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-08:19:44-root-INFO: grad norm: 2.688 2.659 0.395
2024-12-02-08:19:44-root-INFO: grad norm: 3.460 3.427 0.472
2024-12-02-08:19:45-root-INFO: grad norm: 4.699 4.665 0.559
2024-12-02-08:19:45-root-INFO: Loss too large (36.827->37.103)! Learning rate decreased to 0.21688.
2024-12-02-08:19:46-root-INFO: grad norm: 3.633 3.595 0.529
2024-12-02-08:19:46-root-INFO: grad norm: 2.642 2.614 0.384
2024-12-02-08:19:46-root-INFO: grad norm: 2.471 2.445 0.357
2024-12-02-08:19:47-root-INFO: grad norm: 2.500 2.480 0.319
2024-12-02-08:19:47-root-INFO: grad norm: 2.556 2.531 0.356
2024-12-02-08:19:48-root-INFO: Loss Change: 37.090 -> 33.952
2024-12-02-08:19:48-root-INFO: Regularization Change: 0.000 -> 5.924
2024-12-02-08:19:48-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-08:19:48-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-08:19:48-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-08:19:48-root-INFO: grad norm: 3.781 3.710 0.732
2024-12-02-08:19:48-root-INFO: Loss too large (34.147->34.722)! Learning rate decreased to 0.22121.
2024-12-02-08:19:49-root-INFO: grad norm: 3.754 3.699 0.638
2024-12-02-08:19:49-root-INFO: grad norm: 3.907 3.860 0.607
2024-12-02-08:19:49-root-INFO: Loss too large (33.570->33.573)! Learning rate decreased to 0.17697.
2024-12-02-08:19:50-root-INFO: grad norm: 2.885 2.852 0.439
2024-12-02-08:19:50-root-INFO: grad norm: 2.084 2.062 0.301
2024-12-02-08:19:51-root-INFO: grad norm: 1.761 1.742 0.255
2024-12-02-08:19:51-root-INFO: grad norm: 1.522 1.508 0.211
2024-12-02-08:19:52-root-INFO: grad norm: 1.389 1.374 0.201
2024-12-02-08:19:52-root-INFO: Loss Change: 34.147 -> 31.684
2024-12-02-08:19:52-root-INFO: Regularization Change: 0.000 -> 2.861
2024-12-02-08:19:52-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-08:19:52-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-08:19:52-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-08:19:53-root-INFO: grad norm: 2.153 2.106 0.449
2024-12-02-08:19:53-root-INFO: grad norm: 3.065 3.012 0.569
2024-12-02-08:19:53-root-INFO: Loss too large (31.682->32.191)! Learning rate decreased to 0.22559.
2024-12-02-08:19:54-root-INFO: grad norm: 3.666 3.623 0.560
2024-12-02-08:19:54-root-INFO: Loss too large (31.545->31.699)! Learning rate decreased to 0.18047.
2024-12-02-08:19:54-root-INFO: grad norm: 2.873 2.839 0.444
2024-12-02-08:19:55-root-INFO: grad norm: 2.119 2.097 0.305
2024-12-02-08:19:55-root-INFO: grad norm: 1.811 1.791 0.267
2024-12-02-08:19:56-root-INFO: grad norm: 1.561 1.547 0.214
2024-12-02-08:19:56-root-INFO: grad norm: 1.422 1.407 0.206
2024-12-02-08:19:56-root-INFO: Loss Change: 31.708 -> 30.006
2024-12-02-08:19:56-root-INFO: Regularization Change: 0.000 -> 2.491
2024-12-02-08:19:56-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-08:19:56-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-08:19:57-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-08:19:57-root-INFO: grad norm: 2.296 2.242 0.494
2024-12-02-08:19:57-root-INFO: Loss too large (29.977->30.100)! Learning rate decreased to 0.23002.
2024-12-02-08:19:57-root-INFO: grad norm: 2.521 2.482 0.439
2024-12-02-08:19:58-root-INFO: grad norm: 3.178 3.144 0.468
2024-12-02-08:19:58-root-INFO: Loss too large (29.753->29.910)! Learning rate decreased to 0.18402.
2024-12-02-08:19:58-root-INFO: grad norm: 2.637 2.608 0.390
2024-12-02-08:19:59-root-INFO: grad norm: 2.088 2.068 0.289
2024-12-02-08:19:59-root-INFO: grad norm: 1.831 1.813 0.261
2024-12-02-08:20:00-root-INFO: grad norm: 1.607 1.592 0.215
2024-12-02-08:20:00-root-INFO: grad norm: 1.476 1.461 0.208
2024-12-02-08:20:01-root-INFO: Loss Change: 29.977 -> 28.509
2024-12-02-08:20:01-root-INFO: Regularization Change: 0.000 -> 2.038
2024-12-02-08:20:01-root-INFO: Undo step: 45
2024-12-02-08:20:01-root-INFO: Undo step: 46
2024-12-02-08:20:01-root-INFO: Undo step: 47
2024-12-02-08:20:01-root-INFO: Undo step: 48
2024-12-02-08:20:01-root-INFO: Undo step: 49
2024-12-02-08:20:01-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-08:20:01-root-INFO: grad norm: 19.364 19.217 2.383
2024-12-02-08:20:01-root-INFO: grad norm: 11.100 11.031 1.237
2024-12-02-08:20:02-root-INFO: grad norm: 7.272 7.192 1.074
2024-12-02-08:20:02-root-INFO: grad norm: 5.886 5.839 0.741
2024-12-02-08:20:03-root-INFO: grad norm: 5.260 5.217 0.671
2024-12-02-08:20:03-root-INFO: grad norm: 5.129 5.100 0.541
2024-12-02-08:20:04-root-INFO: grad norm: 5.535 5.503 0.596
2024-12-02-08:20:04-root-INFO: grad norm: 5.287 5.249 0.634
2024-12-02-08:20:04-root-INFO: Loss Change: 129.606 -> 43.530
2024-12-02-08:20:04-root-INFO: Regularization Change: 0.000 -> 101.917
2024-12-02-08:20:04-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-08:20:04-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-08:20:05-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-08:20:05-root-INFO: grad norm: 5.757 5.668 1.007
2024-12-02-08:20:05-root-INFO: grad norm: 5.938 5.828 1.138
2024-12-02-08:20:06-root-INFO: grad norm: 6.401 6.290 1.186
2024-12-02-08:20:06-root-INFO: Loss too large (42.131->42.626)! Learning rate decreased to 0.21260.
2024-12-02-08:20:06-root-INFO: grad norm: 5.058 4.954 1.020
2024-12-02-08:20:07-root-INFO: grad norm: 4.088 4.021 0.737
2024-12-02-08:20:07-root-INFO: grad norm: 3.946 3.881 0.717
2024-12-02-08:20:08-root-INFO: grad norm: 4.022 3.969 0.653
2024-12-02-08:20:08-root-INFO: grad norm: 3.980 3.917 0.702
2024-12-02-08:20:09-root-INFO: Loss Change: 43.794 -> 37.296
2024-12-02-08:20:09-root-INFO: Regularization Change: 0.000 -> 10.287
2024-12-02-08:20:09-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-08:20:09-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-08:20:09-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-08:20:09-root-INFO: grad norm: 4.734 4.648 0.902
2024-12-02-08:20:09-root-INFO: Loss too large (37.459->38.272)! Learning rate decreased to 0.21688.
2024-12-02-08:20:10-root-INFO: grad norm: 4.423 4.342 0.846
2024-12-02-08:20:10-root-INFO: grad norm: 4.054 3.995 0.688
2024-12-02-08:20:10-root-INFO: grad norm: 3.938 3.873 0.710
2024-12-02-08:20:11-root-INFO: grad norm: 3.841 3.789 0.631
2024-12-02-08:20:12-root-INFO: grad norm: 3.790 3.730 0.671
2024-12-02-08:20:12-root-INFO: grad norm: 3.741 3.692 0.609
2024-12-02-08:20:12-root-INFO: grad norm: 3.719 3.662 0.652
2024-12-02-08:20:13-root-INFO: Loss Change: 37.459 -> 34.118
2024-12-02-08:20:13-root-INFO: Regularization Change: 0.000 -> 5.168
2024-12-02-08:20:13-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-08:20:13-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-08:20:13-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-08:20:13-root-INFO: grad norm: 4.926 4.822 1.009
2024-12-02-08:20:13-root-INFO: Loss too large (34.522->35.635)! Learning rate decreased to 0.22121.
2024-12-02-08:20:14-root-INFO: grad norm: 4.536 4.449 0.882
2024-12-02-08:20:14-root-INFO: grad norm: 4.080 4.015 0.725
2024-12-02-08:20:15-root-INFO: grad norm: 3.932 3.868 0.705
2024-12-02-08:20:15-root-INFO: grad norm: 3.817 3.762 0.642
2024-12-02-08:20:16-root-INFO: grad norm: 3.755 3.697 0.657
2024-12-02-08:20:16-root-INFO: grad norm: 3.696 3.646 0.609
2024-12-02-08:20:17-root-INFO: grad norm: 3.668 3.613 0.633
2024-12-02-08:20:17-root-INFO: Loss Change: 34.522 -> 31.836
2024-12-02-08:20:17-root-INFO: Regularization Change: 0.000 -> 3.847
2024-12-02-08:20:17-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-08:20:17-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-08:20:17-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-08:20:17-root-INFO: grad norm: 4.659 4.568 0.914
2024-12-02-08:20:17-root-INFO: Loss too large (32.163->33.208)! Learning rate decreased to 0.22559.
2024-12-02-08:20:18-root-INFO: grad norm: 4.288 4.211 0.808
2024-12-02-08:20:18-root-INFO: grad norm: 3.879 3.822 0.663
2024-12-02-08:20:19-root-INFO: grad norm: 3.720 3.662 0.652
2024-12-02-08:20:19-root-INFO: grad norm: 3.577 3.530 0.583
2024-12-02-08:20:20-root-INFO: grad norm: 3.512 3.460 0.600
2024-12-02-08:20:20-root-INFO: grad norm: 3.454 3.409 0.552
2024-12-02-08:20:21-root-INFO: grad norm: 3.427 3.379 0.577
2024-12-02-08:20:21-root-INFO: Loss Change: 32.163 -> 29.922
2024-12-02-08:20:21-root-INFO: Regularization Change: 0.000 -> 3.135
2024-12-02-08:20:21-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-08:20:21-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-08:20:21-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-08:20:21-root-INFO: grad norm: 4.469 4.377 0.900
2024-12-02-08:20:22-root-INFO: Loss too large (30.160->31.193)! Learning rate decreased to 0.23002.
2024-12-02-08:20:22-root-INFO: grad norm: 4.145 4.069 0.788
2024-12-02-08:20:23-root-INFO: grad norm: 3.779 3.723 0.649
2024-12-02-08:20:23-root-INFO: grad norm: 3.624 3.569 0.632
2024-12-02-08:20:23-root-INFO: grad norm: 3.482 3.435 0.569
2024-12-02-08:20:24-root-INFO: grad norm: 3.415 3.366 0.579
2024-12-02-08:20:24-root-INFO: grad norm: 3.353 3.310 0.535
2024-12-02-08:20:25-root-INFO: grad norm: 3.324 3.277 0.555
2024-12-02-08:20:25-root-INFO: Loss Change: 30.160 -> 28.228
2024-12-02-08:20:25-root-INFO: Regularization Change: 0.000 -> 2.712
2024-12-02-08:20:25-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-08:20:25-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-08:20:25-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-08:20:25-root-INFO: grad norm: 4.216 4.134 0.829
2024-12-02-08:20:26-root-INFO: Loss too large (28.508->29.448)! Learning rate decreased to 0.23450.
2024-12-02-08:20:26-root-INFO: grad norm: 3.934 3.867 0.720
2024-12-02-08:20:27-root-INFO: grad norm: 3.620 3.568 0.610
2024-12-02-08:20:27-root-INFO: grad norm: 3.473 3.422 0.592
2024-12-02-08:20:28-root-INFO: grad norm: 3.339 3.296 0.536
2024-12-02-08:20:28-root-INFO: grad norm: 3.271 3.226 0.543
2024-12-02-08:20:28-root-INFO: grad norm: 3.212 3.172 0.504
2024-12-02-08:20:29-root-INFO: grad norm: 3.184 3.141 0.521
2024-12-02-08:20:29-root-INFO: Loss Change: 28.508 -> 26.797
2024-12-02-08:20:29-root-INFO: Regularization Change: 0.000 -> 2.436
2024-12-02-08:20:29-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-08:20:29-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-08:20:29-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-08:20:30-root-INFO: grad norm: 4.196 4.112 0.832
2024-12-02-08:20:30-root-INFO: Loss too large (26.969->27.911)! Learning rate decreased to 0.23903.
2024-12-02-08:20:30-root-INFO: grad norm: 3.890 3.822 0.721
2024-12-02-08:20:31-root-INFO: grad norm: 3.544 3.493 0.597
2024-12-02-08:20:31-root-INFO: grad norm: 3.377 3.328 0.575
2024-12-02-08:20:32-root-INFO: grad norm: 3.226 3.184 0.516
2024-12-02-08:20:32-root-INFO: grad norm: 3.142 3.099 0.518
2024-12-02-08:20:33-root-INFO: grad norm: 3.075 3.038 0.479
2024-12-02-08:20:33-root-INFO: grad norm: 3.041 3.001 0.492
2024-12-02-08:20:33-root-INFO: Loss Change: 26.969 -> 25.330
2024-12-02-08:20:33-root-INFO: Regularization Change: 0.000 -> 2.249
2024-12-02-08:20:33-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-08:20:33-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-08:20:33-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-08:20:34-root-INFO: grad norm: 3.708 3.641 0.703
2024-12-02-08:20:34-root-INFO: Loss too large (25.483->26.218)! Learning rate decreased to 0.24361.
2024-12-02-08:20:34-root-INFO: grad norm: 3.490 3.434 0.623
2024-12-02-08:20:35-root-INFO: grad norm: 3.281 3.238 0.529
2024-12-02-08:20:35-root-INFO: grad norm: 3.166 3.123 0.524
2024-12-02-08:20:36-root-INFO: grad norm: 3.054 3.018 0.468
2024-12-02-08:20:36-root-INFO: grad norm: 2.997 2.958 0.481
2024-12-02-08:20:37-root-INFO: grad norm: 2.945 2.912 0.440
2024-12-02-08:20:37-root-INFO: grad norm: 2.919 2.882 0.462
2024-12-02-08:20:37-root-INFO: Loss Change: 25.483 -> 24.105
2024-12-02-08:20:37-root-INFO: Regularization Change: 0.000 -> 2.091
2024-12-02-08:20:37-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-08:20:37-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-08:20:38-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-08:20:38-root-INFO: grad norm: 4.242 4.150 0.881
2024-12-02-08:20:38-root-INFO: Loss too large (24.499->25.412)! Learning rate decreased to 0.24866.
2024-12-02-08:20:38-root-INFO: grad norm: 3.810 3.743 0.715
2024-12-02-08:20:39-root-INFO: grad norm: 3.376 3.329 0.563
2024-12-02-08:20:39-root-INFO: grad norm: 3.159 3.115 0.527
2024-12-02-08:20:40-root-INFO: grad norm: 2.979 2.943 0.460
2024-12-02-08:20:40-root-INFO: grad norm: 2.879 2.843 0.457
2024-12-02-08:20:41-root-INFO: grad norm: 2.800 2.769 0.414
2024-12-02-08:20:41-root-INFO: grad norm: 2.758 2.725 0.425
2024-12-02-08:20:42-root-INFO: Loss Change: 24.499 -> 22.837
2024-12-02-08:20:42-root-INFO: Regularization Change: 0.000 -> 2.080
2024-12-02-08:20:42-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-08:20:42-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-08:20:42-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-08:20:42-root-INFO: grad norm: 3.787 3.712 0.747
2024-12-02-08:20:42-root-INFO: Loss too large (23.148->23.903)! Learning rate decreased to 0.25333.
2024-12-02-08:20:43-root-INFO: grad norm: 3.472 3.416 0.622
2024-12-02-08:20:43-root-INFO: grad norm: 3.144 3.103 0.501
2024-12-02-08:20:44-root-INFO: grad norm: 2.966 2.928 0.475
2024-12-02-08:20:44-root-INFO: grad norm: 2.813 2.782 0.417
2024-12-02-08:20:45-root-INFO: grad norm: 2.717 2.685 0.418
2024-12-02-08:20:45-root-INFO: grad norm: 2.648 2.621 0.378
2024-12-02-08:20:45-root-INFO: grad norm: 2.613 2.584 0.391
2024-12-02-08:20:46-root-INFO: Loss Change: 23.148 -> 21.730
2024-12-02-08:20:46-root-INFO: Regularization Change: 0.000 -> 1.961
2024-12-02-08:20:46-root-INFO: Undo step: 40
2024-12-02-08:20:46-root-INFO: Undo step: 41
2024-12-02-08:20:46-root-INFO: Undo step: 42
2024-12-02-08:20:46-root-INFO: Undo step: 43
2024-12-02-08:20:46-root-INFO: Undo step: 44
2024-12-02-08:20:46-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-08:20:46-root-INFO: grad norm: 18.943 18.799 2.329
2024-12-02-08:20:47-root-INFO: grad norm: 9.112 9.022 1.281
2024-12-02-08:20:47-root-INFO: grad norm: 6.061 6.001 0.850
2024-12-02-08:20:48-root-INFO: grad norm: 5.045 4.973 0.852
2024-12-02-08:20:48-root-INFO: grad norm: 5.073 5.007 0.816
2024-12-02-08:20:49-root-INFO: grad norm: 5.326 5.221 1.054
2024-12-02-08:20:49-root-INFO: grad norm: 6.040 5.946 1.065
2024-12-02-08:20:49-root-INFO: grad norm: 6.241 6.102 1.311
2024-12-02-08:20:50-root-INFO: Loss Change: 120.384 -> 37.336
2024-12-02-08:20:50-root-INFO: Regularization Change: 0.000 -> 110.800
2024-12-02-08:20:50-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-08:20:50-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-08:20:50-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-08:20:50-root-INFO: grad norm: 7.284 7.119 1.543
2024-12-02-08:20:50-root-INFO: Loss too large (37.949->38.054)! Learning rate decreased to 0.23450.
2024-12-02-08:20:51-root-INFO: grad norm: 5.242 5.119 1.130
2024-12-02-08:20:51-root-INFO: grad norm: 3.612 3.551 0.661
2024-12-02-08:20:52-root-INFO: grad norm: 2.966 2.915 0.551
2024-12-02-08:20:52-root-INFO: grad norm: 2.563 2.527 0.427
2024-12-02-08:20:52-root-INFO: grad norm: 2.342 2.307 0.400
2024-12-02-08:20:53-root-INFO: grad norm: 2.191 2.164 0.343
2024-12-02-08:20:53-root-INFO: grad norm: 2.100 2.072 0.343
2024-12-02-08:20:54-root-INFO: Loss Change: 37.949 -> 29.818
2024-12-02-08:20:54-root-INFO: Regularization Change: 0.000 -> 10.252
2024-12-02-08:20:54-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-08:20:54-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-08:20:54-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-08:20:54-root-INFO: grad norm: 2.978 2.915 0.610
2024-12-02-08:20:55-root-INFO: grad norm: 3.637 3.565 0.717
2024-12-02-08:20:55-root-INFO: grad norm: 4.906 4.826 0.878
2024-12-02-08:20:55-root-INFO: Loss too large (29.579->30.395)! Learning rate decreased to 0.23903.
2024-12-02-08:20:56-root-INFO: grad norm: 4.077 4.002 0.775
2024-12-02-08:20:56-root-INFO: grad norm: 3.074 3.022 0.565
2024-12-02-08:20:57-root-INFO: grad norm: 2.698 2.651 0.499
2024-12-02-08:20:57-root-INFO: grad norm: 2.439 2.402 0.422
2024-12-02-08:20:58-root-INFO: grad norm: 2.297 2.262 0.397
2024-12-02-08:20:58-root-INFO: Loss Change: 29.748 -> 26.672
2024-12-02-08:20:58-root-INFO: Regularization Change: 0.000 -> 5.902
2024-12-02-08:20:58-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-08:20:58-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-08:20:58-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-08:20:58-root-INFO: grad norm: 2.942 2.880 0.601
2024-12-02-08:20:58-root-INFO: Loss too large (26.730->26.867)! Learning rate decreased to 0.24361.
2024-12-02-08:20:59-root-INFO: grad norm: 2.697 2.652 0.495
2024-12-02-08:20:59-root-INFO: grad norm: 2.518 2.483 0.420
2024-12-02-08:21:00-root-INFO: grad norm: 2.426 2.391 0.407
2024-12-02-08:21:00-root-INFO: grad norm: 2.352 2.322 0.372
2024-12-02-08:21:01-root-INFO: grad norm: 2.305 2.274 0.375
2024-12-02-08:21:01-root-INFO: grad norm: 2.271 2.244 0.352
2024-12-02-08:21:02-root-INFO: grad norm: 2.252 2.223 0.362
2024-12-02-08:21:02-root-INFO: Loss Change: 26.730 -> 24.660
2024-12-02-08:21:02-root-INFO: Regularization Change: 0.000 -> 3.649
2024-12-02-08:21:02-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-08:21:02-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-08:21:02-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-08:21:02-root-INFO: grad norm: 3.578 3.487 0.802
2024-12-02-08:21:03-root-INFO: Loss too large (24.905->25.376)! Learning rate decreased to 0.24866.
2024-12-02-08:21:03-root-INFO: grad norm: 3.197 3.137 0.618
2024-12-02-08:21:03-root-INFO: grad norm: 2.862 2.818 0.500
2024-12-02-08:21:04-root-INFO: grad norm: 2.680 2.640 0.463
2024-12-02-08:21:04-root-INFO: grad norm: 2.516 2.482 0.413
2024-12-02-08:21:05-root-INFO: grad norm: 2.413 2.380 0.399
2024-12-02-08:21:05-root-INFO: grad norm: 2.327 2.297 0.369
2024-12-02-08:21:06-root-INFO: grad norm: 2.274 2.244 0.366
2024-12-02-08:21:06-root-INFO: Loss Change: 24.905 -> 22.957
2024-12-02-08:21:06-root-INFO: Regularization Change: 0.000 -> 2.986
2024-12-02-08:21:06-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-08:21:06-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-08:21:06-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-08:21:06-root-INFO: grad norm: 3.412 3.337 0.713
2024-12-02-08:21:07-root-INFO: Loss too large (23.203->23.727)! Learning rate decreased to 0.25333.
2024-12-02-08:21:07-root-INFO: grad norm: 3.051 2.997 0.568
2024-12-02-08:21:08-root-INFO: grad norm: 2.669 2.629 0.458
2024-12-02-08:21:08-root-INFO: grad norm: 2.513 2.477 0.429
2024-12-02-08:21:09-root-INFO: grad norm: 2.411 2.380 0.386
2024-12-02-08:21:09-root-INFO: grad norm: 2.327 2.297 0.376
2024-12-02-08:21:09-root-INFO: grad norm: 2.265 2.238 0.349
2024-12-02-08:21:10-root-INFO: grad norm: 2.218 2.190 0.348
2024-12-02-08:21:10-root-INFO: Loss Change: 23.203 -> 21.565
2024-12-02-08:21:10-root-INFO: Regularization Change: 0.000 -> 2.518
2024-12-02-08:21:10-root-INFO: Undo step: 40
2024-12-02-08:21:10-root-INFO: Undo step: 41
2024-12-02-08:21:10-root-INFO: Undo step: 42
2024-12-02-08:21:10-root-INFO: Undo step: 43
2024-12-02-08:21:10-root-INFO: Undo step: 44
2024-12-02-08:21:10-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-08:21:11-root-INFO: grad norm: 19.305 19.195 2.060
2024-12-02-08:21:11-root-INFO: grad norm: 8.593 8.497 1.284
2024-12-02-08:21:12-root-INFO: grad norm: 6.425 6.357 0.931
2024-12-02-08:21:12-root-INFO: grad norm: 5.953 5.879 0.938
2024-12-02-08:21:12-root-INFO: grad norm: 6.084 6.012 0.933
2024-12-02-08:21:13-root-INFO: grad norm: 5.495 5.404 0.992
2024-12-02-08:21:13-root-INFO: grad norm: 5.698 5.590 1.108
2024-12-02-08:21:14-root-INFO: grad norm: 5.964 5.819 1.306
2024-12-02-08:21:14-root-INFO: Loss Change: 122.648 -> 37.622
2024-12-02-08:21:14-root-INFO: Regularization Change: 0.000 -> 116.079
2024-12-02-08:21:14-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-08:21:14-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-08:21:14-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-08:21:14-root-INFO: grad norm: 7.720 7.504 1.814
2024-12-02-08:21:15-root-INFO: Loss too large (38.337->39.156)! Learning rate decreased to 0.23450.
2024-12-02-08:21:15-root-INFO: grad norm: 5.925 5.770 1.346
2024-12-02-08:21:16-root-INFO: grad norm: 4.470 4.385 0.868
2024-12-02-08:21:16-root-INFO: grad norm: 3.901 3.836 0.708
2024-12-02-08:21:17-root-INFO: grad norm: 3.543 3.489 0.616
2024-12-02-08:21:17-root-INFO: grad norm: 3.361 3.315 0.554
2024-12-02-08:21:18-root-INFO: grad norm: 3.196 3.153 0.525
2024-12-02-08:21:18-root-INFO: grad norm: 3.109 3.070 0.490
2024-12-02-08:21:18-root-INFO: Loss Change: 38.337 -> 30.218
2024-12-02-08:21:18-root-INFO: Regularization Change: 0.000 -> 10.309
2024-12-02-08:21:18-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-08:21:18-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-08:21:18-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-08:21:19-root-INFO: grad norm: 3.970 3.886 0.815
2024-12-02-08:21:19-root-INFO: Loss too large (30.250->30.601)! Learning rate decreased to 0.23903.
2024-12-02-08:21:19-root-INFO: grad norm: 3.703 3.643 0.667
2024-12-02-08:21:20-root-INFO: grad norm: 3.422 3.372 0.583
2024-12-02-08:21:20-root-INFO: grad norm: 3.249 3.205 0.532
2024-12-02-08:21:21-root-INFO: grad norm: 3.100 3.060 0.499
2024-12-02-08:21:21-root-INFO: grad norm: 3.011 2.974 0.471
2024-12-02-08:21:22-root-INFO: grad norm: 2.941 2.905 0.458
2024-12-02-08:21:22-root-INFO: grad norm: 2.899 2.865 0.442
2024-12-02-08:21:22-root-INFO: Loss Change: 30.250 -> 27.010
2024-12-02-08:21:22-root-INFO: Regularization Change: 0.000 -> 5.508
2024-12-02-08:21:22-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-08:21:22-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-08:21:22-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-08:21:23-root-INFO: grad norm: 3.474 3.410 0.667
2024-12-02-08:21:23-root-INFO: Loss too large (27.059->27.451)! Learning rate decreased to 0.24361.
2024-12-02-08:21:23-root-INFO: grad norm: 3.299 3.250 0.564
2024-12-02-08:21:24-root-INFO: grad norm: 3.142 3.102 0.499
2024-12-02-08:21:24-root-INFO: grad norm: 3.047 3.010 0.474
2024-12-02-08:21:25-root-INFO: grad norm: 2.948 2.914 0.442
2024-12-02-08:21:25-root-INFO: grad norm: 2.887 2.854 0.433
2024-12-02-08:21:26-root-INFO: grad norm: 2.836 2.806 0.413
2024-12-02-08:21:26-root-INFO: grad norm: 2.808 2.778 0.412
2024-12-02-08:21:26-root-INFO: Loss Change: 27.059 -> 24.844
2024-12-02-08:21:26-root-INFO: Regularization Change: 0.000 -> 3.899
2024-12-02-08:21:26-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-08:21:26-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-08:21:27-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-08:21:27-root-INFO: grad norm: 4.029 3.937 0.858
2024-12-02-08:21:27-root-INFO: Loss too large (25.128->25.787)! Learning rate decreased to 0.24866.
2024-12-02-08:21:27-root-INFO: grad norm: 3.629 3.568 0.663
2024-12-02-08:21:28-root-INFO: grad norm: 3.254 3.210 0.534
2024-12-02-08:21:28-root-INFO: grad norm: 3.064 3.026 0.483
2024-12-02-08:21:29-root-INFO: grad norm: 2.891 2.859 0.431
2024-12-02-08:21:29-root-INFO: grad norm: 2.796 2.766 0.410
2024-12-02-08:21:30-root-INFO: grad norm: 2.716 2.689 0.384
2024-12-02-08:21:30-root-INFO: grad norm: 2.672 2.645 0.377
2024-12-02-08:21:31-root-INFO: Loss Change: 25.128 -> 22.999
2024-12-02-08:21:31-root-INFO: Regularization Change: 0.000 -> 3.199
2024-12-02-08:21:31-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-08:21:31-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-08:21:31-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-08:21:31-root-INFO: grad norm: 3.573 3.502 0.709
2024-12-02-08:21:31-root-INFO: Loss too large (23.206->23.766)! Learning rate decreased to 0.25333.
2024-12-02-08:21:31-root-INFO: grad norm: 3.293 3.245 0.561
2024-12-02-08:21:32-root-INFO: grad norm: 3.019 2.983 0.460
2024-12-02-08:21:32-root-INFO: grad norm: 2.861 2.829 0.426
2024-12-02-08:21:33-root-INFO: grad norm: 2.717 2.690 0.381
2024-12-02-08:21:33-root-INFO: grad norm: 2.627 2.601 0.370
2024-12-02-08:21:34-root-INFO: grad norm: 2.560 2.536 0.344
2024-12-02-08:21:34-root-INFO: grad norm: 2.525 2.502 0.344
2024-12-02-08:21:35-root-INFO: Loss Change: 23.206 -> 21.514
2024-12-02-08:21:35-root-INFO: Regularization Change: 0.000 -> 2.676
2024-12-02-08:21:35-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-08:21:35-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-08:21:35-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-08:21:35-root-INFO: grad norm: 3.163 3.104 0.607
2024-12-02-08:21:35-root-INFO: Loss too large (21.560->22.011)! Learning rate decreased to 0.25805.
2024-12-02-08:21:36-root-INFO: grad norm: 2.974 2.933 0.490
2024-12-02-08:21:36-root-INFO: grad norm: 2.811 2.780 0.416
2024-12-02-08:21:36-root-INFO: grad norm: 2.712 2.683 0.396
2024-12-02-08:21:37-root-INFO: grad norm: 2.609 2.585 0.356
2024-12-02-08:21:37-root-INFO: grad norm: 2.544 2.520 0.353
2024-12-02-08:21:38-root-INFO: grad norm: 2.494 2.472 0.327
2024-12-02-08:21:38-root-INFO: grad norm: 2.469 2.447 0.332
2024-12-02-08:21:39-root-INFO: Loss Change: 21.560 -> 20.196
2024-12-02-08:21:39-root-INFO: Regularization Change: 0.000 -> 2.333
2024-12-02-08:21:39-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-08:21:39-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-08:21:39-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-08:21:39-root-INFO: grad norm: 3.295 3.229 0.656
2024-12-02-08:21:39-root-INFO: Loss too large (20.291->20.788)! Learning rate decreased to 0.26281.
2024-12-02-08:21:40-root-INFO: grad norm: 3.018 2.976 0.506
2024-12-02-08:21:40-root-INFO: grad norm: 2.784 2.753 0.419
2024-12-02-08:21:41-root-INFO: grad norm: 2.649 2.622 0.382
2024-12-02-08:21:41-root-INFO: grad norm: 2.524 2.500 0.344
2024-12-02-08:21:42-root-INFO: grad norm: 2.448 2.426 0.330
2024-12-02-08:21:42-root-INFO: grad norm: 2.384 2.364 0.309
2024-12-02-08:21:42-root-INFO: grad norm: 2.348 2.328 0.305
2024-12-02-08:21:43-root-INFO: Loss Change: 20.291 -> 18.937
2024-12-02-08:21:43-root-INFO: Regularization Change: 0.000 -> 2.131
2024-12-02-08:21:43-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-08:21:43-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-08:21:43-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-08:21:43-root-INFO: grad norm: 3.197 3.134 0.634
2024-12-02-08:21:43-root-INFO: Loss too large (19.068->19.545)! Learning rate decreased to 0.26762.
2024-12-02-08:21:44-root-INFO: grad norm: 2.928 2.887 0.486
2024-12-02-08:21:44-root-INFO: grad norm: 2.670 2.641 0.386
2024-12-02-08:21:45-root-INFO: grad norm: 2.504 2.479 0.354
2024-12-02-08:21:45-root-INFO: grad norm: 2.378 2.357 0.309
2024-12-02-08:21:46-root-INFO: grad norm: 2.306 2.286 0.302
2024-12-02-08:21:46-root-INFO: grad norm: 2.241 2.224 0.276
2024-12-02-08:21:47-root-INFO: grad norm: 2.200 2.182 0.277
2024-12-02-08:21:47-root-INFO: Loss Change: 19.068 -> 17.803
2024-12-02-08:21:47-root-INFO: Regularization Change: 0.000 -> 1.978
2024-12-02-08:21:47-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-08:21:47-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-08:21:47-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-08:21:47-root-INFO: grad norm: 2.765 2.715 0.523
2024-12-02-08:21:48-root-INFO: Loss too large (17.847->18.196)! Learning rate decreased to 0.27247.
2024-12-02-08:21:48-root-INFO: grad norm: 2.591 2.560 0.396
2024-12-02-08:21:48-root-INFO: grad norm: 2.459 2.436 0.340
2024-12-02-08:21:49-root-INFO: grad norm: 2.380 2.359 0.318
2024-12-02-08:21:49-root-INFO: grad norm: 2.294 2.276 0.288
2024-12-02-08:21:50-root-INFO: grad norm: 2.234 2.215 0.284
2024-12-02-08:21:50-root-INFO: grad norm: 2.187 2.171 0.262
2024-12-02-08:21:51-root-INFO: grad norm: 2.163 2.147 0.266
2024-12-02-08:21:51-root-INFO: Loss Change: 17.847 -> 16.810
2024-12-02-08:21:51-root-INFO: Regularization Change: 0.000 -> 1.836
2024-12-02-08:21:51-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-08:21:51-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-08:21:51-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-08:21:51-root-INFO: grad norm: 2.880 2.828 0.549
2024-12-02-08:21:52-root-INFO: Loss too large (16.941->17.318)! Learning rate decreased to 0.27737.
2024-12-02-08:21:52-root-INFO: grad norm: 2.636 2.601 0.430
2024-12-02-08:21:53-root-INFO: grad norm: 2.423 2.399 0.338
2024-12-02-08:21:53-root-INFO: grad norm: 2.292 2.270 0.316
2024-12-02-08:21:53-root-INFO: grad norm: 2.194 2.177 0.273
2024-12-02-08:21:54-root-INFO: grad norm: 2.138 2.120 0.270
2024-12-02-08:21:54-root-INFO: grad norm: 2.084 2.070 0.244
2024-12-02-08:21:55-root-INFO: grad norm: 2.048 2.033 0.247
2024-12-02-08:21:55-root-INFO: Loss Change: 16.941 -> 15.871
2024-12-02-08:21:55-root-INFO: Regularization Change: 0.000 -> 1.757
2024-12-02-08:21:55-root-INFO: Undo step: 35
2024-12-02-08:21:55-root-INFO: Undo step: 36
2024-12-02-08:21:55-root-INFO: Undo step: 37
2024-12-02-08:21:55-root-INFO: Undo step: 38
2024-12-02-08:21:55-root-INFO: Undo step: 39
2024-12-02-08:21:55-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-08:21:55-root-INFO: grad norm: 16.040 15.953 1.671
2024-12-02-08:21:56-root-INFO: grad norm: 8.063 7.994 1.047
2024-12-02-08:21:56-root-INFO: grad norm: 6.209 6.141 0.916
2024-12-02-08:21:57-root-INFO: grad norm: 5.436 5.398 0.641
2024-12-02-08:21:57-root-INFO: grad norm: 5.402 5.371 0.576
2024-12-02-08:21:58-root-INFO: grad norm: 4.674 4.640 0.558
2024-12-02-08:21:58-root-INFO: grad norm: 4.185 4.154 0.508
2024-12-02-08:21:59-root-INFO: grad norm: 4.057 4.025 0.505
2024-12-02-08:21:59-root-INFO: Loss Change: 105.331 -> 29.148
2024-12-02-08:21:59-root-INFO: Regularization Change: 0.000 -> 112.601
2024-12-02-08:21:59-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-08:21:59-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-08:21:59-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-08:22:00-root-INFO: grad norm: 5.070 5.005 0.810
2024-12-02-08:22:00-root-INFO: grad norm: 4.792 4.719 0.829
2024-12-02-08:22:01-root-INFO: grad norm: 4.475 4.398 0.828
2024-12-02-08:22:01-root-INFO: grad norm: 4.793 4.708 0.897
2024-12-02-08:22:02-root-INFO: grad norm: 6.021 5.931 1.042
2024-12-02-08:22:02-root-INFO: Loss too large (26.687->27.646)! Learning rate decreased to 0.25805.
2024-12-02-08:22:02-root-INFO: grad norm: 4.526 4.449 0.828
2024-12-02-08:22:03-root-INFO: grad norm: 3.001 2.948 0.561
2024-12-02-08:22:03-root-INFO: grad norm: 2.541 2.502 0.444
2024-12-02-08:22:03-root-INFO: Loss Change: 29.237 -> 23.123
2024-12-02-08:22:03-root-INFO: Regularization Change: 0.000 -> 11.596
2024-12-02-08:22:03-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-08:22:03-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-08:22:04-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-08:22:04-root-INFO: grad norm: 3.312 3.229 0.737
2024-12-02-08:22:04-root-INFO: Loss too large (23.201->23.340)! Learning rate decreased to 0.26281.
2024-12-02-08:22:04-root-INFO: grad norm: 2.993 2.945 0.533
2024-12-02-08:22:05-root-INFO: grad norm: 2.877 2.840 0.458
2024-12-02-08:22:05-root-INFO: grad norm: 2.795 2.763 0.419
2024-12-02-08:22:06-root-INFO: grad norm: 2.720 2.690 0.398
2024-12-02-08:22:06-root-INFO: grad norm: 2.668 2.640 0.384
2024-12-02-08:22:07-root-INFO: grad norm: 2.626 2.599 0.375
2024-12-02-08:22:07-root-INFO: grad norm: 2.605 2.579 0.371
2024-12-02-08:22:08-root-INFO: Loss Change: 23.201 -> 20.604
2024-12-02-08:22:08-root-INFO: Regularization Change: 0.000 -> 4.925
2024-12-02-08:22:08-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-08:22:08-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-08:22:08-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-08:22:08-root-INFO: grad norm: 3.658 3.585 0.729
2024-12-02-08:22:08-root-INFO: Loss too large (20.792->21.393)! Learning rate decreased to 0.26762.
2024-12-02-08:22:09-root-INFO: grad norm: 3.236 3.188 0.556
2024-12-02-08:22:09-root-INFO: grad norm: 2.766 2.730 0.443
2024-12-02-08:22:10-root-INFO: grad norm: 2.588 2.557 0.402
2024-12-02-08:22:10-root-INFO: grad norm: 2.505 2.478 0.368
2024-12-02-08:22:11-root-INFO: grad norm: 2.470 2.444 0.359
2024-12-02-08:22:11-root-INFO: grad norm: 2.491 2.467 0.345
2024-12-02-08:22:11-root-INFO: grad norm: 2.476 2.451 0.347
2024-12-02-08:22:12-root-INFO: Loss Change: 20.792 -> 18.739
2024-12-02-08:22:12-root-INFO: Regularization Change: 0.000 -> 3.486
2024-12-02-08:22:12-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-08:22:12-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-08:22:12-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-08:22:12-root-INFO: grad norm: 3.200 3.139 0.624
2024-12-02-08:22:12-root-INFO: Loss too large (18.836->19.375)! Learning rate decreased to 0.27247.
2024-12-02-08:22:13-root-INFO: grad norm: 2.960 2.922 0.474
2024-12-02-08:22:13-root-INFO: grad norm: 2.683 2.651 0.413
2024-12-02-08:22:14-root-INFO: grad norm: 2.580 2.551 0.382
2024-12-02-08:22:14-root-INFO: grad norm: 2.546 2.520 0.361
2024-12-02-08:22:15-root-INFO: grad norm: 2.497 2.472 0.353
2024-12-02-08:22:15-root-INFO: grad norm: 2.446 2.423 0.336
2024-12-02-08:22:16-root-INFO: grad norm: 2.428 2.404 0.339
2024-12-02-08:22:16-root-INFO: Loss Change: 18.836 -> 17.316
2024-12-02-08:22:16-root-INFO: Regularization Change: 0.000 -> 2.744
2024-12-02-08:22:16-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-08:22:16-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-08:22:16-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-08:22:16-root-INFO: grad norm: 3.310 3.246 0.650
2024-12-02-08:22:17-root-INFO: Loss too large (17.510->18.121)! Learning rate decreased to 0.27737.
2024-12-02-08:22:17-root-INFO: grad norm: 2.994 2.951 0.508
2024-12-02-08:22:18-root-INFO: grad norm: 2.627 2.596 0.406
2024-12-02-08:22:18-root-INFO: grad norm: 2.480 2.451 0.379
2024-12-02-08:22:19-root-INFO: grad norm: 2.460 2.436 0.344
2024-12-02-08:22:19-root-INFO: grad norm: 2.395 2.371 0.339
2024-12-02-08:22:20-root-INFO: grad norm: 2.308 2.286 0.314
2024-12-02-08:22:20-root-INFO: grad norm: 2.293 2.270 0.320
2024-12-02-08:22:20-root-INFO: Loss Change: 17.510 -> 16.099
2024-12-02-08:22:20-root-INFO: Regularization Change: 0.000 -> 2.332
2024-12-02-08:22:20-root-INFO: Undo step: 35
2024-12-02-08:22:20-root-INFO: Undo step: 36
2024-12-02-08:22:20-root-INFO: Undo step: 37
2024-12-02-08:22:20-root-INFO: Undo step: 38
2024-12-02-08:22:20-root-INFO: Undo step: 39
2024-12-02-08:22:21-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-08:22:21-root-INFO: grad norm: 17.251 17.171 1.665
2024-12-02-08:22:21-root-INFO: grad norm: 9.365 9.304 1.070
2024-12-02-08:22:22-root-INFO: grad norm: 6.614 6.510 1.167
2024-12-02-08:22:22-root-INFO: grad norm: 7.312 7.271 0.770
2024-12-02-08:22:23-root-INFO: grad norm: 4.804 4.759 0.652
2024-12-02-08:22:23-root-INFO: grad norm: 5.042 4.992 0.709
2024-12-02-08:22:24-root-INFO: grad norm: 5.046 4.981 0.807
2024-12-02-08:22:24-root-INFO: grad norm: 5.327 5.251 0.895
2024-12-02-08:22:24-root-INFO: Loss Change: 110.752 -> 30.992
2024-12-02-08:22:24-root-INFO: Regularization Change: 0.000 -> 114.969
2024-12-02-08:22:24-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-08:22:24-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-08:22:25-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-08:22:25-root-INFO: grad norm: 4.918 4.862 0.740
2024-12-02-08:22:25-root-INFO: grad norm: 5.000 4.928 0.842
2024-12-02-08:22:26-root-INFO: grad norm: 5.159 5.077 0.917
2024-12-02-08:22:26-root-INFO: grad norm: 5.497 5.409 0.978
2024-12-02-08:22:27-root-INFO: Loss too large (27.485->27.667)! Learning rate decreased to 0.25805.
2024-12-02-08:22:27-root-INFO: grad norm: 4.089 4.027 0.709
2024-12-02-08:22:27-root-INFO: grad norm: 2.999 2.957 0.500
2024-12-02-08:22:28-root-INFO: grad norm: 2.565 2.535 0.389
2024-12-02-08:22:28-root-INFO: grad norm: 2.328 2.302 0.345
2024-12-02-08:22:29-root-INFO: Loss Change: 30.296 -> 23.382
2024-12-02-08:22:29-root-INFO: Regularization Change: 0.000 -> 11.483
2024-12-02-08:22:29-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-08:22:29-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-08:22:29-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-08:22:29-root-INFO: grad norm: 1.962 1.947 0.241
2024-12-02-08:22:30-root-INFO: grad norm: 2.397 2.386 0.234
2024-12-02-08:22:30-root-INFO: grad norm: 2.854 2.841 0.278
2024-12-02-08:22:30-root-INFO: grad norm: 3.657 3.636 0.390
2024-12-02-08:22:31-root-INFO: Loss too large (22.327->22.690)! Learning rate decreased to 0.26281.
2024-12-02-08:22:31-root-INFO: grad norm: 3.142 3.118 0.392
2024-12-02-08:22:32-root-INFO: grad norm: 2.618 2.594 0.355
2024-12-02-08:22:32-root-INFO: grad norm: 2.389 2.367 0.325
2024-12-02-08:22:32-root-INFO: grad norm: 2.268 2.246 0.319
2024-12-02-08:22:33-root-INFO: Loss Change: 23.017 -> 20.506
2024-12-02-08:22:33-root-INFO: Regularization Change: 0.000 -> 6.035
2024-12-02-08:22:33-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-08:22:33-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-08:22:33-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-08:22:33-root-INFO: grad norm: 1.847 1.833 0.226
2024-12-02-08:22:34-root-INFO: grad norm: 2.345 2.335 0.221
2024-12-02-08:22:34-root-INFO: Loss too large (19.932->20.035)! Learning rate decreased to 0.26762.
2024-12-02-08:22:34-root-INFO: grad norm: 2.210 2.197 0.237
2024-12-02-08:22:35-root-INFO: grad norm: 2.076 2.061 0.243
2024-12-02-08:22:35-root-INFO: grad norm: 2.067 2.051 0.258
2024-12-02-08:22:36-root-INFO: grad norm: 2.107 2.090 0.268
2024-12-02-08:22:36-root-INFO: grad norm: 2.140 2.121 0.280
2024-12-02-08:22:37-root-INFO: grad norm: 2.190 2.171 0.287
2024-12-02-08:22:37-root-INFO: Loss Change: 20.160 -> 18.595
2024-12-02-08:22:37-root-INFO: Regularization Change: 0.000 -> 3.746
2024-12-02-08:22:37-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-08:22:37-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-08:22:37-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-08:22:37-root-INFO: grad norm: 1.959 1.949 0.196
2024-12-02-08:22:38-root-INFO: grad norm: 2.794 2.782 0.261
2024-12-02-08:22:38-root-INFO: Loss too large (18.269->18.683)! Learning rate decreased to 0.27247.
2024-12-02-08:22:38-root-INFO: grad norm: 2.585 2.569 0.290
2024-12-02-08:22:39-root-INFO: grad norm: 2.249 2.232 0.281
2024-12-02-08:22:39-root-INFO: grad norm: 2.162 2.143 0.285
2024-12-02-08:22:40-root-INFO: grad norm: 2.175 2.156 0.289
2024-12-02-08:22:40-root-INFO: grad norm: 2.196 2.176 0.296
2024-12-02-08:22:41-root-INFO: grad norm: 2.249 2.230 0.298
2024-12-02-08:22:41-root-INFO: Loss Change: 18.333 -> 17.132
2024-12-02-08:22:41-root-INFO: Regularization Change: 0.000 -> 2.951
2024-12-02-08:22:41-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-08:22:41-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-08:22:41-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-08:22:41-root-INFO: grad norm: 1.935 1.924 0.202
2024-12-02-08:22:42-root-INFO: grad norm: 2.764 2.754 0.239
2024-12-02-08:22:42-root-INFO: Loss too large (16.830->17.270)! Learning rate decreased to 0.27737.
2024-12-02-08:22:42-root-INFO: grad norm: 2.470 2.456 0.267
2024-12-02-08:22:43-root-INFO: grad norm: 2.112 2.097 0.251
2024-12-02-08:22:43-root-INFO: grad norm: 2.021 2.004 0.264
2024-12-02-08:22:44-root-INFO: grad norm: 2.029 2.011 0.265
2024-12-02-08:22:45-root-INFO: grad norm: 2.060 2.041 0.276
2024-12-02-08:22:45-root-INFO: grad norm: 2.150 2.131 0.281
2024-12-02-08:22:45-root-INFO: Loss Change: 16.876 -> 15.863
2024-12-02-08:22:45-root-INFO: Regularization Change: 0.000 -> 2.494
2024-12-02-08:22:45-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-08:22:45-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-08:22:46-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-08:22:46-root-INFO: grad norm: 1.861 1.853 0.177
2024-12-02-08:22:46-root-INFO: Loss too large (15.597->15.599)! Learning rate decreased to 0.28231.
2024-12-02-08:22:46-root-INFO: grad norm: 1.766 1.758 0.168
2024-12-02-08:22:47-root-INFO: grad norm: 1.794 1.783 0.196
2024-12-02-08:22:47-root-INFO: grad norm: 1.883 1.871 0.210
2024-12-02-08:22:48-root-INFO: grad norm: 1.956 1.942 0.237
2024-12-02-08:22:48-root-INFO: grad norm: 2.043 2.028 0.243
2024-12-02-08:22:49-root-INFO: grad norm: 2.104 2.087 0.267
2024-12-02-08:22:49-root-INFO: grad norm: 2.162 2.146 0.267
2024-12-02-08:22:50-root-INFO: Loss Change: 15.597 -> 14.804
2024-12-02-08:22:50-root-INFO: Regularization Change: 0.000 -> 2.052
2024-12-02-08:22:50-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-08:22:50-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-08:22:50-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-08:22:50-root-INFO: grad norm: 1.865 1.856 0.184
2024-12-02-08:22:50-root-INFO: Loss too large (14.426->14.449)! Learning rate decreased to 0.28730.
2024-12-02-08:22:50-root-INFO: grad norm: 1.781 1.773 0.170
2024-12-02-08:22:51-root-INFO: grad norm: 1.810 1.800 0.187
2024-12-02-08:22:51-root-INFO: grad norm: 1.892 1.880 0.210
2024-12-02-08:22:52-root-INFO: grad norm: 1.967 1.953 0.230
2024-12-02-08:22:52-root-INFO: grad norm: 2.057 2.042 0.242
2024-12-02-08:22:53-root-INFO: Loss too large (13.878->13.884)! Learning rate decreased to 0.22984.
2024-12-02-08:22:53-root-INFO: grad norm: 1.487 1.476 0.179
2024-12-02-08:22:53-root-INFO: grad norm: 1.033 1.025 0.125
2024-12-02-08:22:54-root-INFO: Loss Change: 14.426 -> 13.436
2024-12-02-08:22:54-root-INFO: Regularization Change: 0.000 -> 1.625
2024-12-02-08:22:54-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-08:22:54-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-08:22:54-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-08:22:54-root-INFO: grad norm: 1.260 1.221 0.309
2024-12-02-08:22:55-root-INFO: grad norm: 1.157 1.129 0.255
2024-12-02-08:22:55-root-INFO: grad norm: 1.274 1.250 0.248
2024-12-02-08:22:56-root-INFO: grad norm: 1.797 1.776 0.274
2024-12-02-08:22:56-root-INFO: Loss too large (13.085->13.202)! Learning rate decreased to 0.29232.
2024-12-02-08:22:56-root-INFO: grad norm: 1.489 1.474 0.207
2024-12-02-08:22:57-root-INFO: grad norm: 1.257 1.243 0.187
2024-12-02-08:22:57-root-INFO: grad norm: 1.498 1.487 0.183
2024-12-02-08:22:57-root-INFO: Loss too large (12.760->12.764)! Learning rate decreased to 0.23386.
2024-12-02-08:22:58-root-INFO: grad norm: 1.207 1.198 0.147
2024-12-02-08:22:58-root-INFO: Loss Change: 13.396 -> 12.549
2024-12-02-08:22:58-root-INFO: Regularization Change: 0.000 -> 1.896
2024-12-02-08:22:58-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-08:22:58-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-08:22:58-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-08:22:59-root-INFO: grad norm: 1.617 1.578 0.352
2024-12-02-08:22:59-root-INFO: Loss too large (12.527->12.596)! Learning rate decreased to 0.29738.
2024-12-02-08:22:59-root-INFO: grad norm: 1.593 1.572 0.256
2024-12-02-08:23:00-root-INFO: grad norm: 1.847 1.830 0.244
2024-12-02-08:23:00-root-INFO: Loss too large (12.357->12.391)! Learning rate decreased to 0.23791.
2024-12-02-08:23:00-root-INFO: grad norm: 1.412 1.400 0.178
2024-12-02-08:23:01-root-INFO: grad norm: 1.044 1.037 0.121
2024-12-02-08:23:02-root-INFO: grad norm: 0.911 0.905 0.106
2024-12-02-08:23:02-root-INFO: grad norm: 0.850 0.845 0.091
2024-12-02-08:23:03-root-INFO: grad norm: 0.819 0.814 0.091
2024-12-02-08:23:03-root-INFO: Loss Change: 12.527 -> 11.800
2024-12-02-08:23:03-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-02-08:23:03-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-08:23:03-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-08:23:03-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-08:23:03-root-INFO: grad norm: 1.788 1.750 0.369
2024-12-02-08:23:04-root-INFO: Loss too large (11.816->12.016)! Learning rate decreased to 0.30249.
2024-12-02-08:23:04-root-INFO: grad norm: 1.813 1.793 0.265
2024-12-02-08:23:05-root-INFO: grad norm: 2.138 2.123 0.254
2024-12-02-08:23:05-root-INFO: Loss too large (11.686->11.826)! Learning rate decreased to 0.24199.
2024-12-02-08:23:05-root-INFO: grad norm: 1.643 1.632 0.190
2024-12-02-08:23:06-root-INFO: grad norm: 1.055 1.049 0.116
2024-12-02-08:23:06-root-INFO: grad norm: 0.896 0.890 0.104
2024-12-02-08:23:07-root-INFO: grad norm: 0.830 0.825 0.085
2024-12-02-08:23:07-root-INFO: grad norm: 0.803 0.798 0.089
2024-12-02-08:23:08-root-INFO: Loss Change: 11.816 -> 11.100
2024-12-02-08:23:08-root-INFO: Regularization Change: 0.000 -> 1.145
2024-12-02-08:23:08-root-INFO: Undo step: 30
2024-12-02-08:23:08-root-INFO: Undo step: 31
2024-12-02-08:23:08-root-INFO: Undo step: 32
2024-12-02-08:23:08-root-INFO: Undo step: 33
2024-12-02-08:23:08-root-INFO: Undo step: 34
2024-12-02-08:23:08-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-08:23:08-root-INFO: grad norm: 15.591 15.522 1.471
2024-12-02-08:23:08-root-INFO: grad norm: 7.063 7.010 0.860
2024-12-02-08:23:09-root-INFO: grad norm: 5.016 4.982 0.585
2024-12-02-08:23:09-root-INFO: grad norm: 4.443 4.418 0.472
2024-12-02-08:23:10-root-INFO: grad norm: 4.501 4.479 0.444
2024-12-02-08:23:10-root-INFO: grad norm: 5.164 5.144 0.452
2024-12-02-08:23:11-root-INFO: grad norm: 4.755 4.732 0.471
2024-12-02-08:23:11-root-INFO: grad norm: 4.338 4.316 0.435
2024-12-02-08:23:12-root-INFO: Loss Change: 99.698 -> 24.270
2024-12-02-08:23:12-root-INFO: Regularization Change: 0.000 -> 118.737
2024-12-02-08:23:12-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-08:23:12-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-08:23:12-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-08:23:12-root-INFO: grad norm: 4.010 3.996 0.333
2024-12-02-08:23:12-root-INFO: grad norm: 4.021 4.007 0.337
2024-12-02-08:23:13-root-INFO: grad norm: 3.848 3.834 0.328
2024-12-02-08:23:13-root-INFO: grad norm: 3.737 3.724 0.319
2024-12-02-08:23:14-root-INFO: grad norm: 3.824 3.807 0.354
2024-12-02-08:23:14-root-INFO: grad norm: 4.347 4.330 0.388
2024-12-02-08:23:15-root-INFO: grad norm: 4.016 3.992 0.441
2024-12-02-08:23:15-root-INFO: grad norm: 3.908 3.884 0.427
2024-12-02-08:23:16-root-INFO: Loss Change: 23.863 -> 18.904
2024-12-02-08:23:16-root-INFO: Regularization Change: 0.000 -> 14.156
2024-12-02-08:23:16-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-08:23:16-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-08:23:16-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-08:23:16-root-INFO: grad norm: 3.506 3.491 0.328
2024-12-02-08:23:16-root-INFO: grad norm: 3.492 3.476 0.329
2024-12-02-08:23:17-root-INFO: grad norm: 3.529 3.507 0.389
2024-12-02-08:23:17-root-INFO: grad norm: 4.181 4.160 0.425
2024-12-02-08:23:17-root-INFO: Loss too large (17.470->17.696)! Learning rate decreased to 0.28730.
2024-12-02-08:23:18-root-INFO: grad norm: 2.682 2.660 0.341
2024-12-02-08:23:18-root-INFO: grad norm: 1.802 1.789 0.218
2024-12-02-08:23:19-root-INFO: grad norm: 1.381 1.369 0.185
2024-12-02-08:23:19-root-INFO: grad norm: 1.219 1.209 0.151
2024-12-02-08:23:20-root-INFO: Loss Change: 18.357 -> 15.187
2024-12-02-08:23:20-root-INFO: Regularization Change: 0.000 -> 5.693
2024-12-02-08:23:20-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-08:23:20-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-08:23:20-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-08:23:20-root-INFO: grad norm: 1.458 1.429 0.291
2024-12-02-08:23:20-root-INFO: grad norm: 1.532 1.508 0.273
2024-12-02-08:23:21-root-INFO: grad norm: 2.120 2.101 0.283
2024-12-02-08:23:21-root-INFO: Loss too large (14.712->14.869)! Learning rate decreased to 0.29232.
2024-12-02-08:23:21-root-INFO: grad norm: 1.957 1.942 0.243
2024-12-02-08:23:22-root-INFO: grad norm: 1.764 1.753 0.198
2024-12-02-08:23:22-root-INFO: grad norm: 1.770 1.758 0.200
2024-12-02-08:23:23-root-INFO: grad norm: 1.912 1.902 0.190
2024-12-02-08:23:23-root-INFO: grad norm: 1.883 1.872 0.200
2024-12-02-08:23:24-root-INFO: Loss Change: 15.087 -> 13.719
2024-12-02-08:23:24-root-INFO: Regularization Change: 0.000 -> 3.557
2024-12-02-08:23:24-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-08:23:24-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-08:23:24-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-08:23:24-root-INFO: grad norm: 2.451 2.416 0.415
2024-12-02-08:23:24-root-INFO: Loss too large (13.717->14.048)! Learning rate decreased to 0.29738.
2024-12-02-08:23:25-root-INFO: grad norm: 2.272 2.248 0.328
2024-12-02-08:23:25-root-INFO: grad norm: 2.117 2.100 0.270
2024-12-02-08:23:26-root-INFO: grad norm: 2.080 2.062 0.272
2024-12-02-08:23:26-root-INFO: grad norm: 2.105 2.091 0.247
2024-12-02-08:23:26-root-INFO: Loss too large (13.048->13.061)! Learning rate decreased to 0.23791.
2024-12-02-08:23:27-root-INFO: grad norm: 1.550 1.539 0.185
2024-12-02-08:23:27-root-INFO: grad norm: 1.064 1.056 0.126
2024-12-02-08:23:28-root-INFO: grad norm: 0.946 0.938 0.115
2024-12-02-08:23:28-root-INFO: Loss Change: 13.717 -> 12.448
2024-12-02-08:23:28-root-INFO: Regularization Change: 0.000 -> 2.055
2024-12-02-08:23:28-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-08:23:28-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-08:23:28-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-08:23:28-root-INFO: grad norm: 1.815 1.774 0.381
2024-12-02-08:23:28-root-INFO: Loss too large (12.436->12.588)! Learning rate decreased to 0.30249.
2024-12-02-08:23:29-root-INFO: grad norm: 1.862 1.839 0.294
2024-12-02-08:23:29-root-INFO: grad norm: 2.289 2.271 0.281
2024-12-02-08:23:29-root-INFO: Loss too large (12.258->12.402)! Learning rate decreased to 0.24199.
2024-12-02-08:23:30-root-INFO: grad norm: 1.749 1.736 0.215
2024-12-02-08:23:30-root-INFO: grad norm: 1.106 1.098 0.133
2024-12-02-08:23:31-root-INFO: grad norm: 0.949 0.942 0.118
2024-12-02-08:23:31-root-INFO: grad norm: 0.879 0.874 0.095
2024-12-02-08:23:32-root-INFO: grad norm: 0.847 0.841 0.098
2024-12-02-08:23:32-root-INFO: Loss Change: 12.436 -> 11.518
2024-12-02-08:23:32-root-INFO: Regularization Change: 0.000 -> 1.558
2024-12-02-08:23:32-root-INFO: Undo step: 30
2024-12-02-08:23:32-root-INFO: Undo step: 31
2024-12-02-08:23:32-root-INFO: Undo step: 32
2024-12-02-08:23:32-root-INFO: Undo step: 33
2024-12-02-08:23:32-root-INFO: Undo step: 34
2024-12-02-08:23:32-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-08:23:33-root-INFO: grad norm: 15.541 15.456 1.620
2024-12-02-08:23:33-root-INFO: grad norm: 7.923 7.864 0.967
2024-12-02-08:23:33-root-INFO: grad norm: 5.846 5.801 0.720
2024-12-02-08:23:34-root-INFO: grad norm: 4.519 4.487 0.537
2024-12-02-08:23:34-root-INFO: grad norm: 4.073 4.045 0.475
2024-12-02-08:23:35-root-INFO: grad norm: 3.707 3.683 0.420
2024-12-02-08:23:35-root-INFO: grad norm: 3.390 3.369 0.372
2024-12-02-08:23:36-root-INFO: grad norm: 3.658 3.639 0.367
2024-12-02-08:23:36-root-INFO: Loss Change: 97.524 -> 23.945
2024-12-02-08:23:36-root-INFO: Regularization Change: 0.000 -> 116.764
2024-12-02-08:23:36-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-08:23:36-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-08:23:36-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-08:23:36-root-INFO: grad norm: 3.768 3.743 0.430
2024-12-02-08:23:37-root-INFO: grad norm: 3.373 3.344 0.439
2024-12-02-08:23:37-root-INFO: grad norm: 3.141 3.119 0.368
2024-12-02-08:23:38-root-INFO: grad norm: 3.631 3.608 0.408
2024-12-02-08:23:38-root-INFO: grad norm: 3.553 3.532 0.379
2024-12-02-08:23:39-root-INFO: grad norm: 3.343 3.317 0.418
2024-12-02-08:23:39-root-INFO: grad norm: 3.269 3.248 0.373
2024-12-02-08:23:40-root-INFO: grad norm: 3.317 3.288 0.435
2024-12-02-08:23:40-root-INFO: Loss Change: 23.701 -> 18.468
2024-12-02-08:23:40-root-INFO: Regularization Change: 0.000 -> 14.314
2024-12-02-08:23:40-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-08:23:40-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-08:23:40-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-08:23:40-root-INFO: grad norm: 4.069 4.007 0.706
2024-12-02-08:23:41-root-INFO: Loss too large (18.383->18.602)! Learning rate decreased to 0.28730.
2024-12-02-08:23:41-root-INFO: grad norm: 3.130 3.087 0.520
2024-12-02-08:23:41-root-INFO: grad norm: 2.403 2.377 0.355
2024-12-02-08:23:42-root-INFO: grad norm: 2.194 2.172 0.309
2024-12-02-08:23:42-root-INFO: grad norm: 2.057 2.040 0.264
2024-12-02-08:23:43-root-INFO: grad norm: 2.042 2.027 0.252
2024-12-02-08:23:43-root-INFO: grad norm: 1.986 1.972 0.231
2024-12-02-08:23:44-root-INFO: grad norm: 1.939 1.925 0.228
2024-12-02-08:23:44-root-INFO: Loss Change: 18.383 -> 15.416
2024-12-02-08:23:44-root-INFO: Regularization Change: 0.000 -> 4.941
2024-12-02-08:23:44-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-08:23:44-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-08:23:44-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-08:23:44-root-INFO: grad norm: 2.638 2.588 0.512
2024-12-02-08:23:45-root-INFO: Loss too large (15.467->15.601)! Learning rate decreased to 0.29232.
2024-12-02-08:23:45-root-INFO: grad norm: 2.368 2.335 0.393
2024-12-02-08:23:45-root-INFO: grad norm: 2.140 2.119 0.296
2024-12-02-08:23:46-root-INFO: grad norm: 2.046 2.028 0.275
2024-12-02-08:23:46-root-INFO: grad norm: 1.988 1.973 0.237
2024-12-02-08:23:47-root-INFO: grad norm: 1.965 1.950 0.235
2024-12-02-08:23:47-root-INFO: grad norm: 1.942 1.930 0.211
2024-12-02-08:23:48-root-INFO: grad norm: 1.940 1.928 0.215
2024-12-02-08:23:48-root-INFO: Loss Change: 15.467 -> 13.867
2024-12-02-08:23:48-root-INFO: Regularization Change: 0.000 -> 3.304
2024-12-02-08:23:48-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-08:23:48-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-08:23:48-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-08:23:48-root-INFO: grad norm: 2.408 2.370 0.425
2024-12-02-08:23:49-root-INFO: Loss too large (13.811->13.985)! Learning rate decreased to 0.29738.
2024-12-02-08:23:49-root-INFO: grad norm: 2.230 2.206 0.327
2024-12-02-08:23:50-root-INFO: grad norm: 2.070 2.054 0.259
2024-12-02-08:23:50-root-INFO: grad norm: 1.994 1.979 0.242
2024-12-02-08:23:51-root-INFO: grad norm: 1.945 1.933 0.210
2024-12-02-08:23:51-root-INFO: grad norm: 1.937 1.926 0.208
2024-12-02-08:23:52-root-INFO: grad norm: 1.909 1.900 0.188
2024-12-02-08:23:52-root-INFO: grad norm: 1.892 1.883 0.191
2024-12-02-08:23:52-root-INFO: Loss Change: 13.811 -> 12.607
2024-12-02-08:23:52-root-INFO: Regularization Change: 0.000 -> 2.543
2024-12-02-08:23:52-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-08:23:52-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-08:23:53-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-08:23:53-root-INFO: grad norm: 2.378 2.339 0.430
2024-12-02-08:23:53-root-INFO: Loss too large (12.566->12.760)! Learning rate decreased to 0.30249.
2024-12-02-08:23:54-root-INFO: grad norm: 2.180 2.157 0.317
2024-12-02-08:23:54-root-INFO: grad norm: 1.981 1.966 0.239
2024-12-02-08:23:55-root-INFO: grad norm: 1.883 1.870 0.217
2024-12-02-08:23:55-root-INFO: grad norm: 1.838 1.829 0.183
2024-12-02-08:23:56-root-INFO: grad norm: 1.844 1.835 0.181
2024-12-02-08:23:56-root-INFO: grad norm: 1.812 1.805 0.163
2024-12-02-08:23:57-root-INFO: grad norm: 1.778 1.771 0.163
2024-12-02-08:23:57-root-INFO: Loss Change: 12.566 -> 11.538
2024-12-02-08:23:57-root-INFO: Regularization Change: 0.000 -> 2.112
2024-12-02-08:23:57-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-08:23:57-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-08:23:57-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-08:23:57-root-INFO: grad norm: 2.248 2.210 0.410
2024-12-02-08:23:58-root-INFO: Loss too large (11.519->11.678)! Learning rate decreased to 0.30763.
2024-12-02-08:23:58-root-INFO: grad norm: 2.026 2.005 0.286
2024-12-02-08:23:59-root-INFO: grad norm: 1.852 1.839 0.218
2024-12-02-08:23:59-root-INFO: grad norm: 1.796 1.786 0.193
2024-12-02-08:24:00-root-INFO: grad norm: 1.754 1.746 0.165
2024-12-02-08:24:00-root-INFO: grad norm: 1.741 1.733 0.160
2024-12-02-08:24:01-root-INFO: grad norm: 1.713 1.707 0.146
2024-12-02-08:24:01-root-INFO: grad norm: 1.701 1.695 0.144
2024-12-02-08:24:01-root-INFO: Loss Change: 11.519 -> 10.631
2024-12-02-08:24:01-root-INFO: Regularization Change: 0.000 -> 1.833
2024-12-02-08:24:01-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-08:24:01-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-08:24:01-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-08:24:02-root-INFO: grad norm: 2.176 2.139 0.402
2024-12-02-08:24:02-root-INFO: Loss too large (10.531->10.690)! Learning rate decreased to 0.31281.
2024-12-02-08:24:02-root-INFO: grad norm: 1.961 1.942 0.273
2024-12-02-08:24:03-root-INFO: grad norm: 1.762 1.751 0.200
2024-12-02-08:24:03-root-INFO: grad norm: 1.705 1.696 0.174
2024-12-02-08:24:04-root-INFO: grad norm: 1.679 1.672 0.147
2024-12-02-08:24:04-root-INFO: grad norm: 1.682 1.676 0.144
2024-12-02-08:24:04-root-INFO: Loss too large (9.899->9.901)! Learning rate decreased to 0.25025.
2024-12-02-08:24:05-root-INFO: grad norm: 1.223 1.219 0.096
2024-12-02-08:24:05-root-INFO: grad norm: 0.866 0.863 0.075
2024-12-02-08:24:06-root-INFO: Loss Change: 10.531 -> 9.572
2024-12-02-08:24:06-root-INFO: Regularization Change: 0.000 -> 1.463
2024-12-02-08:24:06-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-08:24:06-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-08:24:06-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-08:24:06-root-INFO: grad norm: 1.251 1.219 0.282
2024-12-02-08:24:06-root-INFO: grad norm: 1.545 1.520 0.280
2024-12-02-08:24:07-root-INFO: Loss too large (9.485->9.601)! Learning rate decreased to 0.31802.
2024-12-02-08:24:07-root-INFO: grad norm: 1.576 1.564 0.198
2024-12-02-08:24:08-root-INFO: grad norm: 1.757 1.748 0.183
2024-12-02-08:24:08-root-INFO: Loss too large (9.368->9.375)! Learning rate decreased to 0.25442.
2024-12-02-08:24:08-root-INFO: grad norm: 1.263 1.259 0.101
2024-12-02-08:24:09-root-INFO: grad norm: 0.900 0.897 0.073
2024-12-02-08:24:09-root-INFO: grad norm: 0.848 0.845 0.072
2024-12-02-08:24:10-root-INFO: grad norm: 0.828 0.826 0.066
2024-12-02-08:24:10-root-INFO: Loss Change: 9.536 -> 8.926
2024-12-02-08:24:10-root-INFO: Regularization Change: 0.000 -> 1.246
2024-12-02-08:24:10-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-08:24:10-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-08:24:10-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-08:24:10-root-INFO: grad norm: 1.332 1.297 0.306
2024-12-02-08:24:11-root-INFO: grad norm: 1.749 1.724 0.294
2024-12-02-08:24:11-root-INFO: Loss too large (8.867->9.099)! Learning rate decreased to 0.32327.
2024-12-02-08:24:11-root-INFO: grad norm: 1.772 1.760 0.206
2024-12-02-08:24:12-root-INFO: grad norm: 1.831 1.822 0.181
2024-12-02-08:24:12-root-INFO: Loss too large (8.756->8.764)! Learning rate decreased to 0.25862.
2024-12-02-08:24:13-root-INFO: grad norm: 1.283 1.279 0.096
2024-12-02-08:24:13-root-INFO: grad norm: 0.870 0.868 0.069
2024-12-02-08:24:14-root-INFO: grad norm: 0.817 0.814 0.068
2024-12-02-08:24:14-root-INFO: grad norm: 0.834 0.831 0.063
2024-12-02-08:24:14-root-INFO: Loss Change: 8.897 -> 8.309
2024-12-02-08:24:14-root-INFO: Regularization Change: 0.000 -> 1.178
2024-12-02-08:24:14-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-08:24:14-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-08:24:15-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-08:24:15-root-INFO: grad norm: 1.251 1.217 0.287
2024-12-02-08:24:15-root-INFO: grad norm: 1.595 1.575 0.252
2024-12-02-08:24:16-root-INFO: Loss too large (8.157->8.358)! Learning rate decreased to 0.32856.
2024-12-02-08:24:16-root-INFO: grad norm: 1.665 1.656 0.174
2024-12-02-08:24:17-root-INFO: grad norm: 1.827 1.820 0.160
2024-12-02-08:24:17-root-INFO: Loss too large (8.082->8.118)! Learning rate decreased to 0.26285.
2024-12-02-08:24:17-root-INFO: grad norm: 1.286 1.282 0.092
2024-12-02-08:24:18-root-INFO: grad norm: 0.838 0.836 0.062
2024-12-02-08:24:18-root-INFO: grad norm: 0.799 0.796 0.064
2024-12-02-08:24:19-root-INFO: grad norm: 0.804 0.802 0.059
2024-12-02-08:24:19-root-INFO: Loss Change: 8.206 -> 7.659
2024-12-02-08:24:19-root-INFO: Regularization Change: 0.000 -> 1.121
2024-12-02-08:24:19-root-INFO: Undo step: 25
2024-12-02-08:24:19-root-INFO: Undo step: 26
2024-12-02-08:24:19-root-INFO: Undo step: 27
2024-12-02-08:24:19-root-INFO: Undo step: 28
2024-12-02-08:24:19-root-INFO: Undo step: 29
2024-12-02-08:24:19-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-08:24:19-root-INFO: grad norm: 14.434 14.372 1.339
2024-12-02-08:24:20-root-INFO: grad norm: 6.916 6.872 0.772
2024-12-02-08:24:20-root-INFO: grad norm: 5.138 5.108 0.560
2024-12-02-08:24:21-root-INFO: grad norm: 4.822 4.786 0.593
2024-12-02-08:24:21-root-INFO: grad norm: 4.961 4.926 0.590
2024-12-02-08:24:22-root-INFO: grad norm: 4.946 4.895 0.704
2024-12-02-08:24:22-root-INFO: grad norm: 5.022 4.979 0.654
2024-12-02-08:24:23-root-INFO: grad norm: 4.804 4.748 0.727
2024-12-02-08:24:23-root-INFO: Loss Change: 90.429 -> 19.726
2024-12-02-08:24:23-root-INFO: Regularization Change: 0.000 -> 124.878
2024-12-02-08:24:23-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-08:24:23-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-08:24:23-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-08:24:23-root-INFO: grad norm: 5.495 5.410 0.965
2024-12-02-08:24:24-root-INFO: grad norm: 5.113 5.029 0.924
2024-12-02-08:24:24-root-INFO: grad norm: 4.748 4.684 0.779
2024-12-02-08:24:25-root-INFO: grad norm: 4.441 4.368 0.804
2024-12-02-08:24:25-root-INFO: grad norm: 4.554 4.485 0.789
2024-12-02-08:24:25-root-INFO: grad norm: 4.428 4.351 0.821
2024-12-02-08:24:26-root-INFO: grad norm: 4.507 4.442 0.764
2024-12-02-08:24:27-root-INFO: grad norm: 4.336 4.262 0.800
2024-12-02-08:24:27-root-INFO: Loss Change: 20.102 -> 14.775
2024-12-02-08:24:27-root-INFO: Regularization Change: 0.000 -> 14.388
2024-12-02-08:24:27-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-08:24:27-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-08:24:27-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-08:24:27-root-INFO: grad norm: 5.005 4.896 1.037
2024-12-02-08:24:28-root-INFO: grad norm: 4.700 4.600 0.961
2024-12-02-08:24:28-root-INFO: grad norm: 4.615 4.543 0.810
2024-12-02-08:24:29-root-INFO: grad norm: 4.173 4.099 0.783
2024-12-02-08:24:29-root-INFO: grad norm: 3.799 3.737 0.683
2024-12-02-08:24:30-root-INFO: grad norm: 3.582 3.512 0.703
2024-12-02-08:24:30-root-INFO: grad norm: 3.462 3.402 0.642
2024-12-02-08:24:31-root-INFO: grad norm: 3.401 3.335 0.667
2024-12-02-08:24:31-root-INFO: Loss Change: 15.215 -> 11.942
2024-12-02-08:24:31-root-INFO: Regularization Change: 0.000 -> 6.550
2024-12-02-08:24:31-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-08:24:31-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-08:24:31-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-08:24:31-root-INFO: grad norm: 4.365 4.272 0.898
2024-12-02-08:24:31-root-INFO: Loss too large (12.376->12.585)! Learning rate decreased to 0.31802.
2024-12-02-08:24:32-root-INFO: grad norm: 2.835 2.784 0.538
2024-12-02-08:24:32-root-INFO: grad norm: 1.696 1.675 0.266
2024-12-02-08:24:33-root-INFO: grad norm: 1.365 1.350 0.207
2024-12-02-08:24:33-root-INFO: grad norm: 1.392 1.384 0.149
2024-12-02-08:24:34-root-INFO: grad norm: 1.383 1.375 0.155
2024-12-02-08:24:34-root-INFO: grad norm: 1.463 1.457 0.128
2024-12-02-08:24:35-root-INFO: grad norm: 1.449 1.442 0.145
2024-12-02-08:24:35-root-INFO: Loss Change: 12.376 -> 9.732
2024-12-02-08:24:35-root-INFO: Regularization Change: 0.000 -> 3.238
2024-12-02-08:24:35-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-08:24:35-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-08:24:35-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-08:24:35-root-INFO: grad norm: 2.206 2.170 0.397
2024-12-02-08:24:35-root-INFO: Loss too large (9.775->10.005)! Learning rate decreased to 0.32327.
2024-12-02-08:24:36-root-INFO: grad norm: 1.823 1.805 0.257
2024-12-02-08:24:36-root-INFO: grad norm: 1.395 1.385 0.168
2024-12-02-08:24:37-root-INFO: grad norm: 1.355 1.345 0.166
2024-12-02-08:24:37-root-INFO: grad norm: 1.584 1.577 0.149
2024-12-02-08:24:37-root-INFO: Loss too large (9.133->9.172)! Learning rate decreased to 0.25862.
2024-12-02-08:24:38-root-INFO: grad norm: 1.245 1.239 0.128
2024-12-02-08:24:38-root-INFO: grad norm: 0.848 0.845 0.073
2024-12-02-08:24:39-root-INFO: grad norm: 0.771 0.767 0.081
2024-12-02-08:24:39-root-INFO: Loss Change: 9.775 -> 8.727
2024-12-02-08:24:39-root-INFO: Regularization Change: 0.000 -> 1.789
2024-12-02-08:24:39-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-08:24:39-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-08:24:39-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-08:24:40-root-INFO: grad norm: 1.591 1.559 0.320
2024-12-02-08:24:40-root-INFO: Loss too large (8.650->8.719)! Learning rate decreased to 0.32856.
2024-12-02-08:24:40-root-INFO: grad norm: 1.447 1.434 0.198
2024-12-02-08:24:41-root-INFO: grad norm: 1.548 1.539 0.162
2024-12-02-08:24:41-root-INFO: Loss too large (8.409->8.443)! Learning rate decreased to 0.26285.
2024-12-02-08:24:41-root-INFO: grad norm: 1.217 1.211 0.120
2024-12-02-08:24:42-root-INFO: grad norm: 0.864 0.861 0.074
2024-12-02-08:24:42-root-INFO: grad norm: 0.808 0.804 0.080
2024-12-02-08:24:43-root-INFO: grad norm: 0.787 0.785 0.063
2024-12-02-08:24:43-root-INFO: grad norm: 0.783 0.779 0.075
2024-12-02-08:24:44-root-INFO: Loss Change: 8.650 -> 7.914
2024-12-02-08:24:44-root-INFO: Regularization Change: 0.000 -> 1.341
2024-12-02-08:24:44-root-INFO: Undo step: 25
2024-12-02-08:24:44-root-INFO: Undo step: 26
2024-12-02-08:24:44-root-INFO: Undo step: 27
2024-12-02-08:24:44-root-INFO: Undo step: 28
2024-12-02-08:24:44-root-INFO: Undo step: 29
2024-12-02-08:24:44-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-08:24:44-root-INFO: grad norm: 14.207 14.162 1.128
2024-12-02-08:24:44-root-INFO: grad norm: 7.215 7.184 0.668
2024-12-02-08:24:45-root-INFO: grad norm: 5.221 5.180 0.648
2024-12-02-08:24:45-root-INFO: grad norm: 4.723 4.695 0.514
2024-12-02-08:24:46-root-INFO: grad norm: 4.724 4.682 0.634
2024-12-02-08:24:46-root-INFO: grad norm: 5.119 5.082 0.616
2024-12-02-08:24:47-root-INFO: grad norm: 5.023 4.969 0.737
2024-12-02-08:24:47-root-INFO: grad norm: 4.730 4.682 0.667
2024-12-02-08:24:47-root-INFO: Loss Change: 87.267 -> 20.054
2024-12-02-08:24:47-root-INFO: Regularization Change: 0.000 -> 116.055
2024-12-02-08:24:47-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-08:24:47-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-08:24:48-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-08:24:48-root-INFO: grad norm: 4.077 4.040 0.545
2024-12-02-08:24:48-root-INFO: grad norm: 4.190 4.152 0.563
2024-12-02-08:24:49-root-INFO: grad norm: 4.099 4.051 0.626
2024-12-02-08:24:49-root-INFO: grad norm: 4.043 3.994 0.625
2024-12-02-08:24:50-root-INFO: grad norm: 4.096 4.037 0.695
2024-12-02-08:24:50-root-INFO: grad norm: 4.556 4.501 0.706
2024-12-02-08:24:50-root-INFO: Loss too large (16.027->16.206)! Learning rate decreased to 0.30763.
2024-12-02-08:24:51-root-INFO: grad norm: 3.045 3.004 0.497
2024-12-02-08:24:51-root-INFO: grad norm: 2.026 2.002 0.306
2024-12-02-08:24:51-root-INFO: Loss Change: 19.333 -> 13.570
2024-12-02-08:24:51-root-INFO: Regularization Change: 0.000 -> 12.681
2024-12-02-08:24:51-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-08:24:51-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-08:24:52-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-08:24:52-root-INFO: grad norm: 1.374 1.364 0.169
2024-12-02-08:24:52-root-INFO: grad norm: 1.293 1.285 0.140
2024-12-02-08:24:53-root-INFO: grad norm: 1.716 1.709 0.153
2024-12-02-08:24:53-root-INFO: grad norm: 1.706 1.699 0.159
2024-12-02-08:24:54-root-INFO: grad norm: 1.777 1.770 0.151
2024-12-02-08:24:54-root-INFO: grad norm: 2.021 2.014 0.176
2024-12-02-08:24:55-root-INFO: grad norm: 2.865 2.855 0.235
2024-12-02-08:24:55-root-INFO: Loss too large (11.776->12.041)! Learning rate decreased to 0.31281.
2024-12-02-08:24:55-root-INFO: grad norm: 1.858 1.850 0.177
2024-12-02-08:24:56-root-INFO: Loss Change: 13.157 -> 11.108
2024-12-02-08:24:56-root-INFO: Regularization Change: 0.000 -> 6.314
2024-12-02-08:24:56-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-08:24:56-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-08:24:56-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-08:24:56-root-INFO: grad norm: 1.445 1.430 0.202
2024-12-02-08:24:56-root-INFO: grad norm: 1.338 1.328 0.163
2024-12-02-08:24:57-root-INFO: grad norm: 1.432 1.424 0.152
2024-12-02-08:24:57-root-INFO: grad norm: 1.582 1.575 0.157
2024-12-02-08:24:58-root-INFO: grad norm: 1.718 1.711 0.160
2024-12-02-08:24:58-root-INFO: grad norm: 2.021 2.013 0.179
2024-12-02-08:24:58-root-INFO: Loss too large (10.172->10.197)! Learning rate decreased to 0.31802.
2024-12-02-08:24:59-root-INFO: grad norm: 1.540 1.533 0.139
2024-12-02-08:24:59-root-INFO: grad norm: 1.247 1.243 0.103
2024-12-02-08:25:00-root-INFO: Loss Change: 10.957 -> 9.622
2024-12-02-08:25:00-root-INFO: Regularization Change: 0.000 -> 3.798
2024-12-02-08:25:00-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-08:25:00-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-08:25:00-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-08:25:00-root-INFO: grad norm: 1.636 1.607 0.303
2024-12-02-08:25:00-root-INFO: grad norm: 2.279 2.261 0.286
2024-12-02-08:25:00-root-INFO: Loss too large (9.465->9.700)! Learning rate decreased to 0.32327.
2024-12-02-08:25:01-root-INFO: grad norm: 1.585 1.574 0.184
2024-12-02-08:25:01-root-INFO: grad norm: 0.893 0.883 0.131
2024-12-02-08:25:02-root-INFO: grad norm: 0.749 0.742 0.098
2024-12-02-08:25:02-root-INFO: grad norm: 0.751 0.744 0.096
2024-12-02-08:25:03-root-INFO: grad norm: 1.083 1.078 0.098
2024-12-02-08:25:03-root-INFO: grad norm: 1.093 1.088 0.110
2024-12-02-08:25:04-root-INFO: Loss Change: 9.540 -> 8.580
2024-12-02-08:25:04-root-INFO: Regularization Change: 0.000 -> 2.341
2024-12-02-08:25:04-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-08:25:04-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-08:25:04-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-08:25:04-root-INFO: grad norm: 2.386 2.356 0.377
2024-12-02-08:25:04-root-INFO: Loss too large (8.581->8.969)! Learning rate decreased to 0.32856.
2024-12-02-08:25:04-root-INFO: Loss too large (8.581->8.652)! Learning rate decreased to 0.26285.
2024-12-02-08:25:05-root-INFO: grad norm: 1.554 1.547 0.153
2024-12-02-08:25:05-root-INFO: grad norm: 1.030 1.025 0.097
2024-12-02-08:25:06-root-INFO: grad norm: 0.819 0.815 0.081
2024-12-02-08:25:06-root-INFO: grad norm: 0.671 0.668 0.063
2024-12-02-08:25:06-root-INFO: grad norm: 0.649 0.646 0.063
2024-12-02-08:25:07-root-INFO: grad norm: 0.710 0.707 0.060
2024-12-02-08:25:08-root-INFO: grad norm: 0.788 0.785 0.069
2024-12-02-08:25:08-root-INFO: Loss Change: 8.581 -> 7.737
2024-12-02-08:25:08-root-INFO: Regularization Change: 0.000 -> 1.210
2024-12-02-08:25:08-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-08:25:08-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-08:25:08-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-08:25:08-root-INFO: grad norm: 2.173 2.145 0.348
2024-12-02-08:25:08-root-INFO: Loss too large (7.848->8.262)! Learning rate decreased to 0.33388.
2024-12-02-08:25:08-root-INFO: Loss too large (7.848->7.981)! Learning rate decreased to 0.26710.
2024-12-02-08:25:09-root-INFO: grad norm: 1.530 1.522 0.147
2024-12-02-08:25:09-root-INFO: grad norm: 1.004 1.000 0.086
2024-12-02-08:25:10-root-INFO: grad norm: 0.933 0.929 0.080
2024-12-02-08:25:10-root-INFO: grad norm: 0.991 0.988 0.076
2024-12-02-08:25:11-root-INFO: grad norm: 1.045 1.041 0.085
2024-12-02-08:25:11-root-INFO: grad norm: 1.229 1.226 0.085
2024-12-02-08:25:12-root-INFO: Loss too large (7.283->7.300)! Learning rate decreased to 0.21368.
2024-12-02-08:25:12-root-INFO: grad norm: 0.981 0.978 0.079
2024-12-02-08:25:12-root-INFO: Loss Change: 7.848 -> 7.154
2024-12-02-08:25:12-root-INFO: Regularization Change: 0.000 -> 0.976
2024-12-02-08:25:12-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-08:25:12-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-08:25:12-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-08:25:13-root-INFO: grad norm: 1.486 1.456 0.296
2024-12-02-08:25:13-root-INFO: Loss too large (7.112->7.199)! Learning rate decreased to 0.33923.
2024-12-02-08:25:13-root-INFO: grad norm: 1.471 1.458 0.196
2024-12-02-08:25:14-root-INFO: grad norm: 2.380 2.372 0.206
2024-12-02-08:25:14-root-INFO: Loss too large (7.032->7.353)! Learning rate decreased to 0.27138.
2024-12-02-08:25:14-root-INFO: Loss too large (7.032->7.125)! Learning rate decreased to 0.21711.
2024-12-02-08:25:15-root-INFO: grad norm: 1.341 1.337 0.109
2024-12-02-08:25:15-root-INFO: grad norm: 0.594 0.592 0.049
2024-12-02-08:25:15-root-INFO: grad norm: 0.492 0.490 0.048
2024-12-02-08:25:16-root-INFO: grad norm: 0.448 0.446 0.044
2024-12-02-08:25:16-root-INFO: grad norm: 0.434 0.432 0.045
2024-12-02-08:25:17-root-INFO: Loss Change: 7.112 -> 6.547
2024-12-02-08:25:17-root-INFO: Regularization Change: 0.000 -> 0.839
2024-12-02-08:25:17-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-08:25:17-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-08:25:17-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-08:25:17-root-INFO: grad norm: 1.362 1.331 0.289
2024-12-02-08:25:17-root-INFO: Loss too large (6.586->6.645)! Learning rate decreased to 0.34461.
2024-12-02-08:25:18-root-INFO: grad norm: 1.324 1.311 0.183
2024-12-02-08:25:18-root-INFO: grad norm: 2.046 2.037 0.190
2024-12-02-08:25:18-root-INFO: Loss too large (6.494->6.780)! Learning rate decreased to 0.27569.
2024-12-02-08:25:18-root-INFO: Loss too large (6.494->6.579)! Learning rate decreased to 0.22055.
2024-12-02-08:25:19-root-INFO: grad norm: 1.300 1.296 0.101
2024-12-02-08:25:19-root-INFO: grad norm: 0.536 0.534 0.046
2024-12-02-08:25:20-root-INFO: grad norm: 0.453 0.451 0.045
2024-12-02-08:25:20-root-INFO: grad norm: 0.428 0.426 0.040
2024-12-02-08:25:21-root-INFO: grad norm: 0.420 0.418 0.042
2024-12-02-08:25:21-root-INFO: Loss Change: 6.586 -> 6.080
2024-12-02-08:25:21-root-INFO: Regularization Change: 0.000 -> 0.777
2024-12-02-08:25:21-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-08:25:21-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-08:25:21-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-08:25:21-root-INFO: grad norm: 1.287 1.260 0.265
2024-12-02-08:25:22-root-INFO: Loss too large (6.085->6.117)! Learning rate decreased to 0.35002.
2024-12-02-08:25:22-root-INFO: grad norm: 1.205 1.192 0.173
2024-12-02-08:25:23-root-INFO: grad norm: 1.836 1.828 0.166
2024-12-02-08:25:23-root-INFO: Loss too large (5.977->6.231)! Learning rate decreased to 0.28002.
2024-12-02-08:25:23-root-INFO: Loss too large (5.977->6.055)! Learning rate decreased to 0.22401.
2024-12-02-08:25:23-root-INFO: grad norm: 1.234 1.230 0.097
2024-12-02-08:25:24-root-INFO: grad norm: 0.580 0.578 0.046
2024-12-02-08:25:24-root-INFO: grad norm: 0.484 0.482 0.046
2024-12-02-08:25:25-root-INFO: grad norm: 0.446 0.445 0.040
2024-12-02-08:25:25-root-INFO: grad norm: 0.444 0.442 0.043
2024-12-02-08:25:25-root-INFO: Loss Change: 6.085 -> 5.616
2024-12-02-08:25:25-root-INFO: Regularization Change: 0.000 -> 0.736
2024-12-02-08:25:25-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-08:25:25-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-08:25:26-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-08:25:26-root-INFO: grad norm: 1.508 1.477 0.302
2024-12-02-08:25:26-root-INFO: Loss too large (5.691->5.825)! Learning rate decreased to 0.35546.
2024-12-02-08:25:26-root-INFO: grad norm: 1.408 1.396 0.186
2024-12-02-08:25:27-root-INFO: grad norm: 1.996 1.988 0.175
2024-12-02-08:25:27-root-INFO: Loss too large (5.598->5.816)! Learning rate decreased to 0.28437.
2024-12-02-08:25:27-root-INFO: Loss too large (5.598->5.633)! Learning rate decreased to 0.22749.
2024-12-02-08:25:28-root-INFO: grad norm: 1.208 1.205 0.088
2024-12-02-08:25:28-root-INFO: grad norm: 0.764 0.762 0.047
2024-12-02-08:25:29-root-INFO: grad norm: 0.647 0.645 0.051
2024-12-02-08:25:29-root-INFO: grad norm: 0.477 0.475 0.038
2024-12-02-08:25:30-root-INFO: grad norm: 0.496 0.494 0.042
2024-12-02-08:25:30-root-INFO: Loss Change: 5.691 -> 5.207
2024-12-02-08:25:30-root-INFO: Regularization Change: 0.000 -> 0.717
2024-12-02-08:25:30-root-INFO: Undo step: 20
2024-12-02-08:25:30-root-INFO: Undo step: 21
2024-12-02-08:25:30-root-INFO: Undo step: 22
2024-12-02-08:25:30-root-INFO: Undo step: 23
2024-12-02-08:25:30-root-INFO: Undo step: 24
2024-12-02-08:25:30-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-08:25:30-root-INFO: grad norm: 13.506 13.465 1.053
2024-12-02-08:25:31-root-INFO: grad norm: 6.243 6.212 0.628
2024-12-02-08:25:31-root-INFO: grad norm: 4.819 4.790 0.525
2024-12-02-08:25:32-root-INFO: grad norm: 4.379 4.361 0.399
2024-12-02-08:25:32-root-INFO: grad norm: 4.249 4.222 0.474
2024-12-02-08:25:33-root-INFO: grad norm: 4.460 4.439 0.438
2024-12-02-08:25:33-root-INFO: grad norm: 4.130 4.097 0.521
2024-12-02-08:25:34-root-INFO: grad norm: 3.928 3.898 0.488
2024-12-02-08:25:34-root-INFO: Loss Change: 80.028 -> 15.073
2024-12-02-08:25:34-root-INFO: Regularization Change: 0.000 -> 117.893
2024-12-02-08:25:34-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-08:25:34-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-08:25:34-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-08:25:34-root-INFO: grad norm: 3.336 3.312 0.397
2024-12-02-08:25:35-root-INFO: grad norm: 3.299 3.274 0.404
2024-12-02-08:25:35-root-INFO: grad norm: 3.413 3.377 0.489
2024-12-02-08:25:36-root-INFO: grad norm: 3.493 3.456 0.504
2024-12-02-08:25:36-root-INFO: grad norm: 3.606 3.559 0.584
2024-12-02-08:25:37-root-INFO: grad norm: 3.662 3.616 0.577
2024-12-02-08:25:37-root-INFO: grad norm: 3.553 3.494 0.645
2024-12-02-08:25:37-root-INFO: grad norm: 3.591 3.539 0.606
2024-12-02-08:25:38-root-INFO: Loss Change: 14.458 -> 11.036
2024-12-02-08:25:38-root-INFO: Regularization Change: 0.000 -> 12.856
2024-12-02-08:25:38-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-08:25:38-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-08:25:38-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-08:25:38-root-INFO: grad norm: 3.728 3.700 0.452
2024-12-02-08:25:38-root-INFO: Loss too large (10.333->10.393)! Learning rate decreased to 0.33923.
2024-12-02-08:25:39-root-INFO: grad norm: 2.130 2.113 0.267
2024-12-02-08:25:39-root-INFO: grad norm: 1.422 1.406 0.213
2024-12-02-08:25:40-root-INFO: grad norm: 1.082 1.073 0.133
2024-12-02-08:25:40-root-INFO: grad norm: 0.944 0.935 0.131
2024-12-02-08:25:41-root-INFO: grad norm: 0.944 0.938 0.102
2024-12-02-08:25:41-root-INFO: grad norm: 1.045 1.039 0.118
2024-12-02-08:25:41-root-INFO: grad norm: 1.579 1.574 0.128
2024-12-02-08:25:42-root-INFO: Loss too large (8.042->8.107)! Learning rate decreased to 0.27138.
2024-12-02-08:25:42-root-INFO: Loss Change: 10.333 -> 8.003
2024-12-02-08:25:42-root-INFO: Regularization Change: 0.000 -> 4.121
2024-12-02-08:25:42-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-08:25:42-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-08:25:42-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-08:25:42-root-INFO: grad norm: 1.474 1.458 0.217
2024-12-02-08:25:43-root-INFO: grad norm: 2.589 2.580 0.210
2024-12-02-08:25:43-root-INFO: Loss too large (7.797->8.259)! Learning rate decreased to 0.34461.
2024-12-02-08:25:43-root-INFO: Loss too large (7.797->7.978)! Learning rate decreased to 0.27569.
2024-12-02-08:25:44-root-INFO: grad norm: 1.553 1.549 0.111
2024-12-02-08:25:44-root-INFO: grad norm: 0.801 0.798 0.069
2024-12-02-08:25:44-root-INFO: grad norm: 0.673 0.670 0.064
2024-12-02-08:25:45-root-INFO: grad norm: 0.661 0.658 0.058
2024-12-02-08:25:45-root-INFO: grad norm: 0.658 0.655 0.063
2024-12-02-08:25:46-root-INFO: grad norm: 0.693 0.691 0.057
2024-12-02-08:25:46-root-INFO: Loss Change: 7.851 -> 6.862
2024-12-02-08:25:46-root-INFO: Regularization Change: 0.000 -> 1.963
2024-12-02-08:25:46-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-08:25:46-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-08:25:46-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-08:25:46-root-INFO: grad norm: 1.183 1.160 0.236
2024-12-02-08:25:47-root-INFO: grad norm: 1.413 1.398 0.207
2024-12-02-08:25:47-root-INFO: grad norm: 1.461 1.447 0.208
2024-12-02-08:25:48-root-INFO: grad norm: 1.736 1.715 0.270
2024-12-02-08:25:48-root-INFO: Loss too large (6.630->6.759)! Learning rate decreased to 0.35002.
2024-12-02-08:25:49-root-INFO: grad norm: 2.090 2.079 0.218
2024-12-02-08:25:49-root-INFO: Loss too large (6.448->6.632)! Learning rate decreased to 0.28002.
2024-12-02-08:25:49-root-INFO: grad norm: 1.520 1.513 0.141
2024-12-02-08:25:50-root-INFO: grad norm: 0.951 0.948 0.074
2024-12-02-08:25:50-root-INFO: grad norm: 0.841 0.837 0.083
2024-12-02-08:25:50-root-INFO: Loss Change: 6.769 -> 5.993
2024-12-02-08:25:50-root-INFO: Regularization Change: 0.000 -> 1.951
2024-12-02-08:25:50-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-08:25:50-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-08:25:51-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-08:25:51-root-INFO: grad norm: 1.837 1.805 0.338
2024-12-02-08:25:51-root-INFO: Loss too large (6.067->6.349)! Learning rate decreased to 0.35546.
2024-12-02-08:25:51-root-INFO: Loss too large (6.067->6.087)! Learning rate decreased to 0.28437.
2024-12-02-08:25:52-root-INFO: grad norm: 1.393 1.385 0.152
2024-12-02-08:25:52-root-INFO: grad norm: 1.180 1.176 0.098
2024-12-02-08:25:53-root-INFO: grad norm: 1.155 1.151 0.095
2024-12-02-08:25:53-root-INFO: grad norm: 1.185 1.182 0.083
2024-12-02-08:25:53-root-INFO: Loss too large (5.639->5.643)! Learning rate decreased to 0.22749.
2024-12-02-08:25:54-root-INFO: grad norm: 0.933 0.930 0.075
2024-12-02-08:25:54-root-INFO: grad norm: 0.668 0.666 0.049
2024-12-02-08:25:55-root-INFO: grad norm: 0.620 0.617 0.054
2024-12-02-08:25:55-root-INFO: Loss Change: 6.067 -> 5.420
2024-12-02-08:25:55-root-INFO: Regularization Change: 0.000 -> 0.912
2024-12-02-08:25:55-root-INFO: Undo step: 20
2024-12-02-08:25:55-root-INFO: Undo step: 21
2024-12-02-08:25:55-root-INFO: Undo step: 22
2024-12-02-08:25:55-root-INFO: Undo step: 23
2024-12-02-08:25:55-root-INFO: Undo step: 24
2024-12-02-08:25:55-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-08:25:55-root-INFO: grad norm: 13.454 13.405 1.139
2024-12-02-08:25:56-root-INFO: grad norm: 6.009 5.971 0.672
2024-12-02-08:25:56-root-INFO: grad norm: 4.096 4.072 0.444
2024-12-02-08:25:57-root-INFO: grad norm: 3.252 3.232 0.354
2024-12-02-08:25:57-root-INFO: grad norm: 2.812 2.794 0.322
2024-12-02-08:25:58-root-INFO: grad norm: 2.697 2.683 0.278
2024-12-02-08:25:58-root-INFO: grad norm: 2.669 2.654 0.284
2024-12-02-08:25:59-root-INFO: grad norm: 2.762 2.749 0.260
2024-12-02-08:25:59-root-INFO: Loss Change: 81.047 -> 15.025
2024-12-02-08:25:59-root-INFO: Regularization Change: 0.000 -> 121.447
2024-12-02-08:25:59-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-08:25:59-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-08:25:59-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-08:25:59-root-INFO: grad norm: 4.957 4.939 0.428
2024-12-02-08:26:00-root-INFO: grad norm: 2.863 2.842 0.340
2024-12-02-08:26:00-root-INFO: grad norm: 2.687 2.669 0.306
2024-12-02-08:26:01-root-INFO: grad norm: 2.465 2.440 0.354
2024-12-02-08:26:01-root-INFO: grad norm: 2.395 2.375 0.312
2024-12-02-08:26:02-root-INFO: grad norm: 2.362 2.335 0.359
2024-12-02-08:26:02-root-INFO: grad norm: 2.402 2.381 0.323
2024-12-02-08:26:03-root-INFO: grad norm: 2.364 2.332 0.387
2024-12-02-08:26:03-root-INFO: Loss Change: 15.083 -> 10.425
2024-12-02-08:26:03-root-INFO: Regularization Change: 0.000 -> 14.288
2024-12-02-08:26:03-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-08:26:03-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-08:26:03-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-08:26:03-root-INFO: grad norm: 3.178 3.116 0.620
2024-12-02-08:26:04-root-INFO: grad norm: 3.038 2.975 0.614
2024-12-02-08:26:04-root-INFO: grad norm: 3.512 3.469 0.547
2024-12-02-08:26:05-root-INFO: Loss too large (9.887->10.134)! Learning rate decreased to 0.33923.
2024-12-02-08:26:05-root-INFO: grad norm: 2.333 2.302 0.379
2024-12-02-08:26:05-root-INFO: grad norm: 1.506 1.493 0.202
2024-12-02-08:26:06-root-INFO: grad norm: 1.299 1.285 0.192
2024-12-02-08:26:06-root-INFO: grad norm: 1.332 1.324 0.139
2024-12-02-08:26:07-root-INFO: grad norm: 1.344 1.335 0.156
2024-12-02-08:26:07-root-INFO: Loss Change: 10.477 -> 8.055
2024-12-02-08:26:07-root-INFO: Regularization Change: 0.000 -> 4.978
2024-12-02-08:26:07-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-08:26:07-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-08:26:07-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-08:26:08-root-INFO: grad norm: 2.477 2.444 0.400
2024-12-02-08:26:08-root-INFO: Loss too large (8.134->8.492)! Learning rate decreased to 0.34461.
2024-12-02-08:26:08-root-INFO: grad norm: 1.961 1.941 0.279
2024-12-02-08:26:09-root-INFO: grad norm: 1.402 1.390 0.184
2024-12-02-08:26:09-root-INFO: grad norm: 1.287 1.273 0.188
2024-12-02-08:26:10-root-INFO: grad norm: 1.373 1.364 0.155
2024-12-02-08:26:10-root-INFO: grad norm: 1.471 1.460 0.175
2024-12-02-08:26:10-root-INFO: grad norm: 1.823 1.815 0.165
2024-12-02-08:26:11-root-INFO: Loss too large (7.158->7.273)! Learning rate decreased to 0.27569.
2024-12-02-08:26:11-root-INFO: grad norm: 1.416 1.409 0.141
2024-12-02-08:26:11-root-INFO: Loss Change: 8.134 -> 6.861
2024-12-02-08:26:11-root-INFO: Regularization Change: 0.000 -> 2.450
2024-12-02-08:26:11-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-08:26:11-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-08:26:12-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-08:26:12-root-INFO: grad norm: 1.675 1.645 0.315
2024-12-02-08:26:12-root-INFO: Loss too large (6.831->6.933)! Learning rate decreased to 0.35002.
2024-12-02-08:26:12-root-INFO: grad norm: 1.571 1.554 0.233
2024-12-02-08:26:13-root-INFO: grad norm: 1.953 1.944 0.192
2024-12-02-08:26:13-root-INFO: Loss too large (6.606->6.752)! Learning rate decreased to 0.28002.
2024-12-02-08:26:13-root-INFO: grad norm: 1.435 1.427 0.147
2024-12-02-08:26:14-root-INFO: grad norm: 0.840 0.838 0.065
2024-12-02-08:26:14-root-INFO: grad norm: 0.724 0.720 0.075
2024-12-02-08:26:15-root-INFO: grad norm: 0.722 0.720 0.055
2024-12-02-08:26:15-root-INFO: grad norm: 0.767 0.764 0.074
2024-12-02-08:26:16-root-INFO: Loss Change: 6.831 -> 6.033
2024-12-02-08:26:16-root-INFO: Regularization Change: 0.000 -> 1.514
2024-12-02-08:26:16-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-08:26:16-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-08:26:16-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-08:26:16-root-INFO: grad norm: 1.948 1.917 0.347
2024-12-02-08:26:16-root-INFO: Loss too large (6.088->6.379)! Learning rate decreased to 0.35546.
2024-12-02-08:26:16-root-INFO: Loss too large (6.088->6.132)! Learning rate decreased to 0.28437.
2024-12-02-08:26:17-root-INFO: grad norm: 1.353 1.345 0.153
2024-12-02-08:26:17-root-INFO: grad norm: 0.960 0.957 0.074
2024-12-02-08:26:18-root-INFO: grad norm: 0.831 0.828 0.074
2024-12-02-08:26:18-root-INFO: grad norm: 0.806 0.804 0.057
2024-12-02-08:26:19-root-INFO: grad norm: 0.859 0.856 0.075
2024-12-02-08:26:19-root-INFO: grad norm: 1.097 1.095 0.068
2024-12-02-08:26:19-root-INFO: Loss too large (5.520->5.524)! Learning rate decreased to 0.22749.
2024-12-02-08:26:20-root-INFO: grad norm: 0.850 0.847 0.075
2024-12-02-08:26:20-root-INFO: Loss Change: 6.088 -> 5.400
2024-12-02-08:26:20-root-INFO: Regularization Change: 0.000 -> 1.038
2024-12-02-08:26:20-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-08:26:20-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-08:26:20-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-08:26:20-root-INFO: grad norm: 1.428 1.401 0.278
2024-12-02-08:26:21-root-INFO: Loss too large (5.397->5.496)! Learning rate decreased to 0.36092.
2024-12-02-08:26:21-root-INFO: grad norm: 1.409 1.395 0.196
2024-12-02-08:26:22-root-INFO: grad norm: 2.016 2.008 0.180
2024-12-02-08:26:22-root-INFO: Loss too large (5.296->5.491)! Learning rate decreased to 0.28874.
2024-12-02-08:26:22-root-INFO: Loss too large (5.296->5.313)! Learning rate decreased to 0.23099.
2024-12-02-08:26:22-root-INFO: grad norm: 1.107 1.102 0.100
2024-12-02-08:26:23-root-INFO: grad norm: 0.597 0.596 0.043
2024-12-02-08:26:23-root-INFO: grad norm: 0.444 0.442 0.043
2024-12-02-08:26:24-root-INFO: grad norm: 0.410 0.408 0.039
2024-12-02-08:26:24-root-INFO: grad norm: 0.397 0.395 0.041
2024-12-02-08:26:25-root-INFO: Loss Change: 5.397 -> 4.861
2024-12-02-08:26:25-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-02-08:26:25-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-08:26:25-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-08:26:25-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-08:26:25-root-INFO: grad norm: 1.387 1.354 0.302
2024-12-02-08:26:25-root-INFO: Loss too large (4.919->4.958)! Learning rate decreased to 0.36641.
2024-12-02-08:26:26-root-INFO: grad norm: 1.291 1.276 0.200
2024-12-02-08:26:26-root-INFO: grad norm: 1.952 1.943 0.183
2024-12-02-08:26:26-root-INFO: Loss too large (4.807->4.985)! Learning rate decreased to 0.29313.
2024-12-02-08:26:26-root-INFO: Loss too large (4.807->4.823)! Learning rate decreased to 0.23450.
2024-12-02-08:26:27-root-INFO: grad norm: 1.040 1.036 0.094
2024-12-02-08:26:27-root-INFO: grad norm: 0.609 0.607 0.039
2024-12-02-08:26:28-root-INFO: grad norm: 0.434 0.433 0.039
2024-12-02-08:26:28-root-INFO: grad norm: 0.392 0.391 0.034
2024-12-02-08:26:29-root-INFO: grad norm: 0.380 0.378 0.037
2024-12-02-08:26:29-root-INFO: Loss Change: 4.919 -> 4.413
2024-12-02-08:26:29-root-INFO: Regularization Change: 0.000 -> 0.782
2024-12-02-08:26:29-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-08:26:29-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-08:26:29-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-08:26:30-root-INFO: grad norm: 1.302 1.273 0.273
2024-12-02-08:26:30-root-INFO: Loss too large (4.470->4.519)! Learning rate decreased to 0.37193.
2024-12-02-08:26:30-root-INFO: grad norm: 1.198 1.184 0.182
2024-12-02-08:26:31-root-INFO: grad norm: 1.554 1.546 0.158
2024-12-02-08:26:31-root-INFO: Loss too large (4.339->4.471)! Learning rate decreased to 0.29754.
2024-12-02-08:26:31-root-INFO: Loss too large (4.339->4.341)! Learning rate decreased to 0.23803.
2024-12-02-08:26:31-root-INFO: grad norm: 0.948 0.944 0.086
2024-12-02-08:26:32-root-INFO: grad norm: 0.488 0.487 0.037
2024-12-02-08:26:32-root-INFO: grad norm: 0.392 0.390 0.038
2024-12-02-08:26:33-root-INFO: grad norm: 0.360 0.358 0.032
2024-12-02-08:26:33-root-INFO: grad norm: 0.351 0.349 0.035
2024-12-02-08:26:34-root-INFO: Loss Change: 4.470 -> 4.021
2024-12-02-08:26:34-root-INFO: Regularization Change: 0.000 -> 0.708
2024-12-02-08:26:34-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-08:26:34-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-08:26:34-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-08:26:34-root-INFO: grad norm: 1.288 1.259 0.271
2024-12-02-08:26:34-root-INFO: Loss too large (4.106->4.147)! Learning rate decreased to 0.37747.
2024-12-02-08:26:35-root-INFO: grad norm: 1.150 1.135 0.180
2024-12-02-08:26:35-root-INFO: grad norm: 1.446 1.439 0.149
2024-12-02-08:26:35-root-INFO: Loss too large (3.966->4.072)! Learning rate decreased to 0.30197.
2024-12-02-08:26:36-root-INFO: grad norm: 1.094 1.088 0.111
2024-12-02-08:26:36-root-INFO: grad norm: 0.771 0.769 0.052
2024-12-02-08:26:37-root-INFO: grad norm: 0.628 0.626 0.050
2024-12-02-08:26:37-root-INFO: grad norm: 0.543 0.542 0.034
2024-12-02-08:26:38-root-INFO: grad norm: 0.567 0.566 0.034
2024-12-02-08:26:38-root-INFO: Loss Change: 4.106 -> 3.670
2024-12-02-08:26:38-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-08:26:38-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-08:26:38-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-08:26:38-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-08:26:38-root-INFO: grad norm: 1.412 1.383 0.284
2024-12-02-08:26:39-root-INFO: Loss too large (3.786->3.816)! Learning rate decreased to 0.38303.
2024-12-02-08:26:39-root-INFO: grad norm: 1.278 1.269 0.148
2024-12-02-08:26:40-root-INFO: grad norm: 1.101 1.098 0.092
2024-12-02-08:26:40-root-INFO: grad norm: 1.031 1.027 0.096
2024-12-02-08:26:40-root-INFO: grad norm: 1.207 1.203 0.099
2024-12-02-08:26:41-root-INFO: Loss too large (3.501->3.569)! Learning rate decreased to 0.30642.
2024-12-02-08:26:41-root-INFO: grad norm: 0.986 0.982 0.088
2024-12-02-08:26:42-root-INFO: grad norm: 0.754 0.752 0.060
2024-12-02-08:26:42-root-INFO: grad norm: 0.760 0.757 0.066
2024-12-02-08:26:42-root-INFO: Loss Change: 3.786 -> 3.339
2024-12-02-08:26:42-root-INFO: Regularization Change: 0.000 -> 0.881
2024-12-02-08:26:42-root-INFO: Undo step: 15
2024-12-02-08:26:42-root-INFO: Undo step: 16
2024-12-02-08:26:42-root-INFO: Undo step: 17
2024-12-02-08:26:42-root-INFO: Undo step: 18
2024-12-02-08:26:42-root-INFO: Undo step: 19
2024-12-02-08:26:43-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-08:26:43-root-INFO: grad norm: 12.677 12.634 1.050
2024-12-02-08:26:43-root-INFO: grad norm: 5.638 5.608 0.573
2024-12-02-08:26:44-root-INFO: grad norm: 3.994 3.969 0.448
2024-12-02-08:26:44-root-INFO: grad norm: 3.394 3.375 0.354
2024-12-02-08:26:45-root-INFO: grad norm: 3.223 3.204 0.351
2024-12-02-08:26:45-root-INFO: grad norm: 3.040 3.024 0.312
2024-12-02-08:26:46-root-INFO: grad norm: 3.426 3.413 0.300
2024-12-02-08:26:46-root-INFO: grad norm: 2.799 2.785 0.278
2024-12-02-08:26:46-root-INFO: Loss Change: 74.336 -> 11.184
2024-12-02-08:26:46-root-INFO: Regularization Change: 0.000 -> 122.625
2024-12-02-08:26:46-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-08:26:46-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-08:26:47-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-08:26:47-root-INFO: grad norm: 3.200 3.193 0.221
2024-12-02-08:26:47-root-INFO: grad norm: 2.408 2.401 0.189
2024-12-02-08:26:48-root-INFO: grad norm: 2.114 2.103 0.216
2024-12-02-08:26:48-root-INFO: grad norm: 2.004 1.994 0.192
2024-12-02-08:26:49-root-INFO: grad norm: 1.980 1.965 0.242
2024-12-02-08:26:49-root-INFO: grad norm: 2.078 2.064 0.238
2024-12-02-08:26:50-root-INFO: grad norm: 2.228 2.209 0.293
2024-12-02-08:26:50-root-INFO: grad norm: 2.744 2.726 0.318
2024-12-02-08:26:50-root-INFO: Loss too large (7.570->7.790)! Learning rate decreased to 0.36092.
2024-12-02-08:26:51-root-INFO: Loss Change: 10.797 -> 7.334
2024-12-02-08:26:51-root-INFO: Regularization Change: 0.000 -> 11.757
2024-12-02-08:26:51-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-08:26:51-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-08:26:51-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-08:26:51-root-INFO: grad norm: 1.839 1.833 0.144
2024-12-02-08:26:51-root-INFO: grad norm: 1.887 1.883 0.129
2024-12-02-08:26:52-root-INFO: Loss too large (6.552->6.602)! Learning rate decreased to 0.36641.
2024-12-02-08:26:52-root-INFO: grad norm: 1.566 1.560 0.141
2024-12-02-08:26:52-root-INFO: grad norm: 1.290 1.285 0.106
2024-12-02-08:26:53-root-INFO: grad norm: 1.268 1.262 0.123
2024-12-02-08:26:53-root-INFO: grad norm: 1.442 1.437 0.113
2024-12-02-08:26:54-root-INFO: grad norm: 1.360 1.354 0.126
2024-12-02-08:26:54-root-INFO: grad norm: 1.280 1.276 0.096
2024-12-02-08:26:55-root-INFO: Loss Change: 6.971 -> 5.501
2024-12-02-08:26:55-root-INFO: Regularization Change: 0.000 -> 3.563
2024-12-02-08:26:55-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-08:26:55-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-08:26:55-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-08:26:55-root-INFO: grad norm: 1.308 1.295 0.181
2024-12-02-08:26:56-root-INFO: grad norm: 1.717 1.712 0.137
2024-12-02-08:26:56-root-INFO: Loss too large (5.224->5.406)! Learning rate decreased to 0.37193.
2024-12-02-08:26:56-root-INFO: grad norm: 1.449 1.446 0.095
2024-12-02-08:26:57-root-INFO: grad norm: 1.142 1.141 0.061
2024-12-02-08:26:57-root-INFO: grad norm: 1.098 1.096 0.080
2024-12-02-08:26:58-root-INFO: grad norm: 1.140 1.138 0.070
2024-12-02-08:26:58-root-INFO: grad norm: 1.211 1.208 0.091
2024-12-02-08:26:59-root-INFO: grad norm: 1.530 1.528 0.089
2024-12-02-08:26:59-root-INFO: Loss too large (4.663->4.714)! Learning rate decreased to 0.29754.
2024-12-02-08:26:59-root-INFO: Loss Change: 5.339 -> 4.615
2024-12-02-08:26:59-root-INFO: Regularization Change: 0.000 -> 2.145
2024-12-02-08:26:59-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-08:26:59-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-08:26:59-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-08:27:00-root-INFO: grad norm: 1.260 1.245 0.195
2024-12-02-08:27:00-root-INFO: grad norm: 1.420 1.411 0.162
2024-12-02-08:27:00-root-INFO: Loss too large (4.408->4.490)! Learning rate decreased to 0.37747.
2024-12-02-08:27:01-root-INFO: grad norm: 1.278 1.274 0.095
2024-12-02-08:27:01-root-INFO: grad norm: 1.423 1.421 0.072
2024-12-02-08:27:01-root-INFO: Loss too large (4.240->4.250)! Learning rate decreased to 0.30197.
2024-12-02-08:27:02-root-INFO: grad norm: 0.927 0.924 0.069
2024-12-02-08:27:02-root-INFO: grad norm: 0.624 0.623 0.039
2024-12-02-08:27:03-root-INFO: grad norm: 0.496 0.494 0.041
2024-12-02-08:27:03-root-INFO: grad norm: 0.433 0.431 0.034
2024-12-02-08:27:04-root-INFO: Loss Change: 4.537 -> 3.885
2024-12-02-08:27:04-root-INFO: Regularization Change: 0.000 -> 1.314
2024-12-02-08:27:04-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-08:27:04-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-08:27:04-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-08:27:04-root-INFO: grad norm: 1.226 1.195 0.275
2024-12-02-08:27:04-root-INFO: grad norm: 1.296 1.272 0.247
2024-12-02-08:27:05-root-INFO: Loss too large (3.868->3.945)! Learning rate decreased to 0.38303.
2024-12-02-08:27:05-root-INFO: grad norm: 1.457 1.447 0.170
2024-12-02-08:27:05-root-INFO: Loss too large (3.770->3.795)! Learning rate decreased to 0.30642.
2024-12-02-08:27:06-root-INFO: grad norm: 0.872 0.866 0.103
2024-12-02-08:27:06-root-INFO: grad norm: 0.596 0.595 0.041
2024-12-02-08:27:07-root-INFO: grad norm: 0.443 0.442 0.035
2024-12-02-08:27:07-root-INFO: grad norm: 0.394 0.393 0.029
2024-12-02-08:27:08-root-INFO: grad norm: 0.397 0.396 0.028
2024-12-02-08:27:08-root-INFO: Loss Change: 3.927 -> 3.426
2024-12-02-08:27:08-root-INFO: Regularization Change: 0.000 -> 1.014
2024-12-02-08:27:08-root-INFO: Undo step: 15
2024-12-02-08:27:08-root-INFO: Undo step: 16
2024-12-02-08:27:08-root-INFO: Undo step: 17
2024-12-02-08:27:08-root-INFO: Undo step: 18
2024-12-02-08:27:08-root-INFO: Undo step: 19
2024-12-02-08:27:08-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-08:27:08-root-INFO: grad norm: 13.510 13.471 1.020
2024-12-02-08:27:09-root-INFO: grad norm: 6.003 5.977 0.563
2024-12-02-08:27:09-root-INFO: grad norm: 4.147 4.126 0.419
2024-12-02-08:27:10-root-INFO: grad norm: 3.149 3.135 0.300
2024-12-02-08:27:10-root-INFO: grad norm: 2.682 2.669 0.271
2024-12-02-08:27:11-root-INFO: grad norm: 2.390 2.378 0.234
2024-12-02-08:27:11-root-INFO: grad norm: 2.499 2.487 0.249
2024-12-02-08:27:12-root-INFO: grad norm: 2.302 2.290 0.228
2024-12-02-08:27:12-root-INFO: Loss Change: 78.643 -> 10.978
2024-12-02-08:27:12-root-INFO: Regularization Change: 0.000 -> 128.156
2024-12-02-08:27:12-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-08:27:12-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-08:27:12-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-08:27:12-root-INFO: grad norm: 2.222 2.213 0.194
2024-12-02-08:27:13-root-INFO: grad norm: 2.622 2.615 0.186
2024-12-02-08:27:13-root-INFO: grad norm: 2.177 2.168 0.200
2024-12-02-08:27:14-root-INFO: grad norm: 1.848 1.839 0.175
2024-12-02-08:27:14-root-INFO: grad norm: 1.792 1.776 0.235
2024-12-02-08:27:15-root-INFO: grad norm: 2.035 2.019 0.251
2024-12-02-08:27:15-root-INFO: grad norm: 2.279 2.258 0.312
2024-12-02-08:27:16-root-INFO: grad norm: 2.938 2.918 0.340
2024-12-02-08:27:16-root-INFO: Loss too large (7.573->7.930)! Learning rate decreased to 0.36092.
2024-12-02-08:27:16-root-INFO: Loss Change: 10.478 -> 7.421
2024-12-02-08:27:16-root-INFO: Regularization Change: 0.000 -> 11.543
2024-12-02-08:27:16-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-08:27:16-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-08:27:16-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-08:27:16-root-INFO: grad norm: 1.935 1.929 0.158
2024-12-02-08:27:17-root-INFO: grad norm: 2.045 2.039 0.147
2024-12-02-08:27:17-root-INFO: Loss too large (6.514->6.560)! Learning rate decreased to 0.36641.
2024-12-02-08:27:17-root-INFO: grad norm: 1.525 1.518 0.147
2024-12-02-08:27:18-root-INFO: grad norm: 1.281 1.278 0.098
2024-12-02-08:27:18-root-INFO: grad norm: 1.205 1.199 0.114
2024-12-02-08:27:19-root-INFO: grad norm: 1.240 1.237 0.097
2024-12-02-08:27:19-root-INFO: grad norm: 1.292 1.287 0.119
2024-12-02-08:27:20-root-INFO: grad norm: 1.520 1.515 0.114
2024-12-02-08:27:20-root-INFO: Loss too large (5.504->5.520)! Learning rate decreased to 0.29313.
2024-12-02-08:27:20-root-INFO: Loss Change: 7.004 -> 5.423
2024-12-02-08:27:20-root-INFO: Regularization Change: 0.000 -> 3.534
2024-12-02-08:27:20-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-08:27:20-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-08:27:20-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-08:27:21-root-INFO: grad norm: 1.255 1.242 0.183
2024-12-02-08:27:21-root-INFO: grad norm: 1.500 1.493 0.145
2024-12-02-08:27:21-root-INFO: Loss too large (5.101->5.203)! Learning rate decreased to 0.37193.
2024-12-02-08:27:22-root-INFO: grad norm: 1.356 1.353 0.094
2024-12-02-08:27:22-root-INFO: grad norm: 1.280 1.278 0.077
2024-12-02-08:27:22-root-INFO: grad norm: 1.301 1.298 0.089
2024-12-02-08:27:23-root-INFO: grad norm: 1.443 1.440 0.088
2024-12-02-08:27:23-root-INFO: Loss too large (4.690->4.752)! Learning rate decreased to 0.29754.
2024-12-02-08:27:24-root-INFO: grad norm: 1.198 1.194 0.093
2024-12-02-08:27:24-root-INFO: grad norm: 0.927 0.925 0.061
2024-12-02-08:27:24-root-INFO: Loss Change: 5.266 -> 4.437
2024-12-02-08:27:24-root-INFO: Regularization Change: 0.000 -> 2.030
2024-12-02-08:27:24-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-08:27:24-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-08:27:24-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-08:27:25-root-INFO: grad norm: 1.166 1.148 0.204
2024-12-02-08:27:25-root-INFO: grad norm: 1.518 1.510 0.157
2024-12-02-08:27:25-root-INFO: Loss too large (4.297->4.399)! Learning rate decreased to 0.37747.
2024-12-02-08:27:26-root-INFO: grad norm: 1.216 1.212 0.088
2024-12-02-08:27:26-root-INFO: grad norm: 1.299 1.297 0.087
2024-12-02-08:27:27-root-INFO: grad norm: 1.238 1.236 0.079
2024-12-02-08:27:27-root-INFO: grad norm: 1.171 1.169 0.067
2024-12-02-08:27:28-root-INFO: grad norm: 1.248 1.245 0.082
2024-12-02-08:27:28-root-INFO: grad norm: 1.609 1.606 0.090
2024-12-02-08:27:28-root-INFO: Loss too large (3.944->4.015)! Learning rate decreased to 0.30197.
2024-12-02-08:27:29-root-INFO: Loss Change: 4.369 -> 3.899
2024-12-02-08:27:29-root-INFO: Regularization Change: 0.000 -> 1.609
2024-12-02-08:27:29-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-08:27:29-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-08:27:29-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-08:27:29-root-INFO: grad norm: 1.334 1.316 0.220
2024-12-02-08:27:29-root-INFO: grad norm: 1.401 1.390 0.176
2024-12-02-08:27:30-root-INFO: Loss too large (3.728->3.795)! Learning rate decreased to 0.38303.
2024-12-02-08:27:30-root-INFO: grad norm: 1.240 1.237 0.090
2024-12-02-08:27:30-root-INFO: grad norm: 1.436 1.434 0.074
2024-12-02-08:27:31-root-INFO: Loss too large (3.585->3.605)! Learning rate decreased to 0.30642.
2024-12-02-08:27:31-root-INFO: grad norm: 0.901 0.899 0.064
2024-12-02-08:27:32-root-INFO: grad norm: 0.572 0.571 0.034
2024-12-02-08:27:32-root-INFO: grad norm: 0.441 0.440 0.032
2024-12-02-08:27:32-root-INFO: grad norm: 0.392 0.391 0.030
2024-12-02-08:27:33-root-INFO: Loss Change: 3.858 -> 3.276
2024-12-02-08:27:33-root-INFO: Regularization Change: 0.000 -> 1.067
2024-12-02-08:27:33-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-08:27:33-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-08:27:33-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-08:27:33-root-INFO: grad norm: 1.246 1.218 0.264
2024-12-02-08:27:33-root-INFO: Loss too large (3.351->3.356)! Learning rate decreased to 0.38861.
2024-12-02-08:27:34-root-INFO: grad norm: 0.982 0.969 0.158
2024-12-02-08:27:34-root-INFO: grad norm: 0.943 0.935 0.120
2024-12-02-08:27:34-root-INFO: Loss too large (3.156->3.168)! Learning rate decreased to 0.31088.
2024-12-02-08:27:35-root-INFO: grad norm: 0.756 0.752 0.081
2024-12-02-08:27:35-root-INFO: grad norm: 0.607 0.604 0.051
2024-12-02-08:27:36-root-INFO: grad norm: 0.627 0.625 0.055
2024-12-02-08:27:36-root-INFO: grad norm: 0.708 0.707 0.047
2024-12-02-08:27:37-root-INFO: grad norm: 0.654 0.652 0.055
2024-12-02-08:27:37-root-INFO: Loss Change: 3.351 -> 2.945
2024-12-02-08:27:37-root-INFO: Regularization Change: 0.000 -> 0.763
2024-12-02-08:27:37-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-08:27:37-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-08:27:37-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-08:27:37-root-INFO: grad norm: 1.478 1.448 0.298
2024-12-02-08:27:38-root-INFO: Loss too large (3.104->3.167)! Learning rate decreased to 0.39420.
2024-12-02-08:27:38-root-INFO: grad norm: 1.102 1.088 0.172
2024-12-02-08:27:38-root-INFO: grad norm: 0.823 0.816 0.107
2024-12-02-08:27:39-root-INFO: grad norm: 0.771 0.765 0.095
2024-12-02-08:27:39-root-INFO: grad norm: 0.974 0.970 0.089
2024-12-02-08:27:40-root-INFO: Loss too large (2.799->2.825)! Learning rate decreased to 0.31536.
2024-12-02-08:27:40-root-INFO: grad norm: 0.735 0.732 0.072
2024-12-02-08:27:41-root-INFO: grad norm: 0.482 0.481 0.041
2024-12-02-08:27:41-root-INFO: grad norm: 0.503 0.501 0.044
2024-12-02-08:27:41-root-INFO: Loss Change: 3.104 -> 2.666
2024-12-02-08:27:41-root-INFO: Regularization Change: 0.000 -> 0.765
2024-12-02-08:27:41-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-08:27:41-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-08:27:41-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-08:27:42-root-INFO: grad norm: 1.386 1.361 0.261
2024-12-02-08:27:42-root-INFO: Loss too large (2.818->2.853)! Learning rate decreased to 0.39982.
2024-12-02-08:27:42-root-INFO: grad norm: 0.963 0.952 0.147
2024-12-02-08:27:43-root-INFO: grad norm: 0.664 0.658 0.086
2024-12-02-08:27:43-root-INFO: grad norm: 0.522 0.517 0.067
2024-12-02-08:27:44-root-INFO: grad norm: 0.470 0.467 0.051
2024-12-02-08:27:44-root-INFO: grad norm: 0.551 0.548 0.058
2024-12-02-08:27:44-root-INFO: Loss too large (2.480->2.486)! Learning rate decreased to 0.31985.
2024-12-02-08:27:45-root-INFO: grad norm: 0.633 0.631 0.043
2024-12-02-08:27:45-root-INFO: grad norm: 0.642 0.640 0.051
2024-12-02-08:27:46-root-INFO: Loss Change: 2.818 -> 2.431
2024-12-02-08:27:46-root-INFO: Regularization Change: 0.000 -> 0.706
2024-12-02-08:27:46-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-08:27:46-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-08:27:46-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-08:27:46-root-INFO: grad norm: 1.459 1.430 0.290
2024-12-02-08:27:46-root-INFO: Loss too large (2.634->2.663)! Learning rate decreased to 0.40545.
2024-12-02-08:27:46-root-INFO: grad norm: 0.977 0.966 0.149
2024-12-02-08:27:47-root-INFO: grad norm: 0.606 0.599 0.087
2024-12-02-08:27:47-root-INFO: grad norm: 0.469 0.465 0.062
2024-12-02-08:27:48-root-INFO: grad norm: 0.478 0.475 0.052
2024-12-02-08:27:48-root-INFO: grad norm: 0.630 0.627 0.059
2024-12-02-08:27:48-root-INFO: Loss too large (2.288->2.301)! Learning rate decreased to 0.32436.
2024-12-02-08:27:49-root-INFO: grad norm: 0.655 0.654 0.045
2024-12-02-08:27:49-root-INFO: grad norm: 0.626 0.624 0.048
2024-12-02-08:27:50-root-INFO: Loss Change: 2.634 -> 2.233
2024-12-02-08:27:50-root-INFO: Regularization Change: 0.000 -> 0.674
2024-12-02-08:27:50-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-08:27:50-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-08:27:50-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-08:27:50-root-INFO: grad norm: 1.428 1.398 0.291
2024-12-02-08:27:51-root-INFO: grad norm: 1.196 1.180 0.197
2024-12-02-08:27:51-root-INFO: grad norm: 1.055 1.042 0.169
2024-12-02-08:27:51-root-INFO: grad norm: 0.998 0.986 0.157
2024-12-02-08:27:52-root-INFO: grad norm: 1.000 0.988 0.156
2024-12-02-08:27:52-root-INFO: Loss too large (2.207->2.220)! Learning rate decreased to 0.41109.
2024-12-02-08:27:53-root-INFO: grad norm: 0.772 0.764 0.110
2024-12-02-08:27:53-root-INFO: grad norm: 0.810 0.805 0.083
2024-12-02-08:27:53-root-INFO: Loss too large (2.115->2.121)! Learning rate decreased to 0.32887.
2024-12-02-08:27:54-root-INFO: grad norm: 0.651 0.648 0.056
2024-12-02-08:27:54-root-INFO: Loss Change: 2.471 -> 2.053
2024-12-02-08:27:54-root-INFO: Regularization Change: 0.000 -> 0.813
2024-12-02-08:27:54-root-INFO: Undo step: 10
2024-12-02-08:27:54-root-INFO: Undo step: 11
2024-12-02-08:27:54-root-INFO: Undo step: 12
2024-12-02-08:27:54-root-INFO: Undo step: 13
2024-12-02-08:27:54-root-INFO: Undo step: 14
2024-12-02-08:27:54-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-08:27:54-root-INFO: grad norm: 12.868 12.845 0.778
2024-12-02-08:27:55-root-INFO: grad norm: 5.070 5.053 0.417
2024-12-02-08:27:55-root-INFO: grad norm: 3.286 3.274 0.289
2024-12-02-08:27:56-root-INFO: grad norm: 2.636 2.626 0.226
2024-12-02-08:27:56-root-INFO: grad norm: 2.310 2.302 0.192
2024-12-02-08:27:57-root-INFO: grad norm: 2.228 2.221 0.173
2024-12-02-08:27:57-root-INFO: grad norm: 2.177 2.171 0.161
2024-12-02-08:27:58-root-INFO: grad norm: 1.814 1.808 0.146
2024-12-02-08:27:58-root-INFO: Loss Change: 66.822 -> 7.205
2024-12-02-08:27:58-root-INFO: Regularization Change: 0.000 -> 118.582
2024-12-02-08:27:58-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-08:27:58-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-08:27:58-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-08:27:58-root-INFO: grad norm: 1.917 1.904 0.224
2024-12-02-08:27:59-root-INFO: grad norm: 2.002 1.992 0.204
2024-12-02-08:27:59-root-INFO: grad norm: 1.910 1.901 0.187
2024-12-02-08:28:00-root-INFO: grad norm: 1.637 1.625 0.198
2024-12-02-08:28:00-root-INFO: grad norm: 1.544 1.536 0.156
2024-12-02-08:28:01-root-INFO: grad norm: 1.685 1.676 0.176
2024-12-02-08:28:01-root-INFO: grad norm: 1.929 1.922 0.172
2024-12-02-08:28:02-root-INFO: grad norm: 1.764 1.754 0.179
2024-12-02-08:28:02-root-INFO: Loss Change: 6.893 -> 4.574
2024-12-02-08:28:02-root-INFO: Regularization Change: 0.000 -> 9.110
2024-12-02-08:28:02-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-08:28:02-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-08:28:02-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-08:28:02-root-INFO: grad norm: 2.116 2.092 0.318
2024-12-02-08:28:03-root-INFO: grad norm: 1.923 1.903 0.273
2024-12-02-08:28:03-root-INFO: grad norm: 1.886 1.872 0.230
2024-12-02-08:28:04-root-INFO: grad norm: 1.683 1.668 0.229
2024-12-02-08:28:04-root-INFO: grad norm: 1.549 1.539 0.173
2024-12-02-08:28:05-root-INFO: grad norm: 1.515 1.504 0.181
2024-12-02-08:28:05-root-INFO: grad norm: 1.481 1.473 0.155
2024-12-02-08:28:06-root-INFO: grad norm: 1.452 1.442 0.171
2024-12-02-08:28:06-root-INFO: Loss Change: 4.543 -> 3.392
2024-12-02-08:28:06-root-INFO: Regularization Change: 0.000 -> 3.531
2024-12-02-08:28:06-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-08:28:06-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-08:28:06-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-08:28:06-root-INFO: grad norm: 2.103 2.074 0.351
2024-12-02-08:28:07-root-INFO: grad norm: 1.810 1.787 0.285
2024-12-02-08:28:07-root-INFO: grad norm: 1.578 1.565 0.202
2024-12-02-08:28:08-root-INFO: grad norm: 1.421 1.407 0.198
2024-12-02-08:28:08-root-INFO: grad norm: 1.319 1.309 0.162
2024-12-02-08:28:09-root-INFO: grad norm: 1.261 1.250 0.166
2024-12-02-08:28:09-root-INFO: grad norm: 1.231 1.222 0.146
2024-12-02-08:28:09-root-INFO: grad norm: 1.295 1.284 0.163
2024-12-02-08:28:10-root-INFO: Loss too large (2.787->2.830)! Learning rate decreased to 0.39982.
2024-12-02-08:28:10-root-INFO: Loss Change: 3.483 -> 2.678
2024-12-02-08:28:10-root-INFO: Regularization Change: 0.000 -> 1.786
2024-12-02-08:28:10-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-08:28:10-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-08:28:10-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-08:28:10-root-INFO: grad norm: 1.802 1.771 0.329
2024-12-02-08:28:11-root-INFO: grad norm: 1.382 1.362 0.234
2024-12-02-08:28:11-root-INFO: grad norm: 1.163 1.151 0.168
2024-12-02-08:28:12-root-INFO: grad norm: 1.044 1.030 0.171
2024-12-02-08:28:12-root-INFO: grad norm: 1.122 1.110 0.165
2024-12-02-08:28:12-root-INFO: Loss too large (2.453->2.492)! Learning rate decreased to 0.40545.
2024-12-02-08:28:13-root-INFO: grad norm: 0.861 0.853 0.120
2024-12-02-08:28:13-root-INFO: grad norm: 0.700 0.696 0.079
2024-12-02-08:28:14-root-INFO: grad norm: 0.678 0.674 0.073
2024-12-02-08:28:14-root-INFO: Loss Change: 2.840 -> 2.267
2024-12-02-08:28:14-root-INFO: Regularization Change: 0.000 -> 1.079
2024-12-02-08:28:14-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-08:28:14-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-08:28:14-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-08:28:14-root-INFO: grad norm: 1.529 1.499 0.301
2024-12-02-08:28:15-root-INFO: grad norm: 1.233 1.218 0.194
2024-12-02-08:28:15-root-INFO: grad norm: 1.012 1.001 0.150
2024-12-02-08:28:16-root-INFO: grad norm: 0.898 0.888 0.135
2024-12-02-08:28:16-root-INFO: grad norm: 0.866 0.856 0.129
2024-12-02-08:28:17-root-INFO: grad norm: 0.944 0.935 0.133
2024-12-02-08:28:17-root-INFO: Loss too large (2.148->2.178)! Learning rate decreased to 0.41109.
2024-12-02-08:28:17-root-INFO: grad norm: 0.845 0.840 0.090
2024-12-02-08:28:18-root-INFO: grad norm: 0.736 0.732 0.078
2024-12-02-08:28:18-root-INFO: Loss Change: 2.473 -> 2.029
2024-12-02-08:28:18-root-INFO: Regularization Change: 0.000 -> 0.866
2024-12-02-08:28:18-root-INFO: Undo step: 10
2024-12-02-08:28:18-root-INFO: Undo step: 11
2024-12-02-08:28:18-root-INFO: Undo step: 12
2024-12-02-08:28:18-root-INFO: Undo step: 13
2024-12-02-08:28:18-root-INFO: Undo step: 14
2024-12-02-08:28:18-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-08:28:18-root-INFO: grad norm: 12.908 12.878 0.878
2024-12-02-08:28:19-root-INFO: grad norm: 5.706 5.684 0.504
2024-12-02-08:28:19-root-INFO: grad norm: 3.918 3.901 0.370
2024-12-02-08:28:20-root-INFO: grad norm: 2.991 2.979 0.262
2024-12-02-08:28:20-root-INFO: grad norm: 2.629 2.618 0.237
2024-12-02-08:28:21-root-INFO: grad norm: 2.531 2.523 0.203
2024-12-02-08:28:21-root-INFO: grad norm: 2.355 2.344 0.218
2024-12-02-08:28:22-root-INFO: grad norm: 2.280 2.272 0.197
2024-12-02-08:28:22-root-INFO: Loss Change: 69.726 -> 7.750
2024-12-02-08:28:22-root-INFO: Regularization Change: 0.000 -> 123.513
2024-12-02-08:28:22-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-08:28:22-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-08:28:22-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-08:28:22-root-INFO: grad norm: 2.243 2.238 0.161
2024-12-02-08:28:23-root-INFO: grad norm: 2.351 2.346 0.162
2024-12-02-08:28:23-root-INFO: grad norm: 2.048 2.042 0.163
2024-12-02-08:28:24-root-INFO: grad norm: 1.778 1.774 0.121
2024-12-02-08:28:24-root-INFO: grad norm: 1.743 1.737 0.151
2024-12-02-08:28:25-root-INFO: grad norm: 1.959 1.953 0.154
2024-12-02-08:28:25-root-INFO: grad norm: 1.904 1.895 0.177
2024-12-02-08:28:26-root-INFO: grad norm: 1.761 1.754 0.151
2024-12-02-08:28:26-root-INFO: Loss Change: 7.265 -> 4.646
2024-12-02-08:28:26-root-INFO: Regularization Change: 0.000 -> 9.539
2024-12-02-08:28:26-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-08:28:26-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-08:28:26-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-08:28:26-root-INFO: grad norm: 1.701 1.696 0.132
2024-12-02-08:28:27-root-INFO: grad norm: 1.907 1.903 0.121
2024-12-02-08:28:27-root-INFO: grad norm: 1.636 1.632 0.118
2024-12-02-08:28:28-root-INFO: grad norm: 1.486 1.483 0.093
2024-12-02-08:28:28-root-INFO: grad norm: 1.438 1.434 0.113
2024-12-02-08:28:29-root-INFO: grad norm: 1.591 1.587 0.119
2024-12-02-08:28:29-root-INFO: Loss too large (3.626->3.655)! Learning rate decreased to 0.39420.
2024-12-02-08:28:29-root-INFO: grad norm: 1.166 1.161 0.109
2024-12-02-08:28:30-root-INFO: grad norm: 0.825 0.822 0.067
2024-12-02-08:28:30-root-INFO: Loss Change: 4.352 -> 3.211
2024-12-02-08:28:30-root-INFO: Regularization Change: 0.000 -> 3.166
2024-12-02-08:28:30-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-08:28:30-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-08:28:30-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-08:28:30-root-INFO: grad norm: 0.958 0.947 0.143
2024-12-02-08:28:31-root-INFO: grad norm: 0.914 0.909 0.099
2024-12-02-08:28:31-root-INFO: grad norm: 0.905 0.902 0.074
2024-12-02-08:28:32-root-INFO: grad norm: 0.914 0.910 0.081
2024-12-02-08:28:32-root-INFO: grad norm: 1.031 1.028 0.073
2024-12-02-08:28:33-root-INFO: grad norm: 1.285 1.282 0.081
2024-12-02-08:28:33-root-INFO: Loss too large (2.847->2.876)! Learning rate decreased to 0.39982.
2024-12-02-08:28:33-root-INFO: grad norm: 0.883 0.881 0.060
2024-12-02-08:28:34-root-INFO: grad norm: 0.594 0.593 0.039
2024-12-02-08:28:34-root-INFO: Loss Change: 3.125 -> 2.589
2024-12-02-08:28:34-root-INFO: Regularization Change: 0.000 -> 1.691
2024-12-02-08:28:34-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-08:28:34-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-08:28:34-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-08:28:34-root-INFO: grad norm: 1.077 1.056 0.212
2024-12-02-08:28:35-root-INFO: grad norm: 0.941 0.927 0.164
2024-12-02-08:28:35-root-INFO: grad norm: 1.087 1.075 0.163
2024-12-02-08:28:36-root-INFO: Loss too large (2.499->2.549)! Learning rate decreased to 0.40545.
2024-12-02-08:28:36-root-INFO: grad norm: 0.908 0.900 0.125
2024-12-02-08:28:36-root-INFO: grad norm: 0.792 0.786 0.094
2024-12-02-08:28:37-root-INFO: grad norm: 0.751 0.746 0.085
2024-12-02-08:28:37-root-INFO: grad norm: 0.717 0.713 0.072
2024-12-02-08:28:38-root-INFO: grad norm: 0.701 0.697 0.073
2024-12-02-08:28:38-root-INFO: Loss Change: 2.638 -> 2.263
2024-12-02-08:28:38-root-INFO: Regularization Change: 0.000 -> 0.989
2024-12-02-08:28:38-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-08:28:38-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-08:28:38-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-08:28:39-root-INFO: grad norm: 1.534 1.505 0.298
2024-12-02-08:28:39-root-INFO: grad norm: 1.310 1.294 0.202
2024-12-02-08:28:39-root-INFO: grad norm: 1.142 1.129 0.172
2024-12-02-08:28:40-root-INFO: grad norm: 1.168 1.155 0.172
2024-12-02-08:28:40-root-INFO: Loss too large (2.252->2.255)! Learning rate decreased to 0.41109.
2024-12-02-08:28:41-root-INFO: grad norm: 0.883 0.876 0.109
2024-12-02-08:28:41-root-INFO: grad norm: 0.766 0.761 0.090
2024-12-02-08:28:41-root-INFO: grad norm: 0.688 0.685 0.072
2024-12-02-08:28:42-root-INFO: grad norm: 0.664 0.661 0.067
2024-12-02-08:28:42-root-INFO: Loss Change: 2.466 -> 2.023
2024-12-02-08:28:42-root-INFO: Regularization Change: 0.000 -> 0.832
2024-12-02-08:28:42-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-08:28:42-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-08:28:42-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-08:28:43-root-INFO: grad norm: 1.383 1.358 0.259
2024-12-02-08:28:43-root-INFO: grad norm: 1.157 1.145 0.168
2024-12-02-08:28:44-root-INFO: grad norm: 0.990 0.979 0.146
2024-12-02-08:28:44-root-INFO: grad norm: 1.057 1.048 0.141
2024-12-02-08:28:44-root-INFO: Loss too large (2.031->2.041)! Learning rate decreased to 0.41675.
2024-12-02-08:28:45-root-INFO: grad norm: 0.788 0.783 0.090
2024-12-02-08:28:45-root-INFO: grad norm: 0.668 0.664 0.071
2024-12-02-08:28:46-root-INFO: grad norm: 0.587 0.584 0.057
2024-12-02-08:28:46-root-INFO: grad norm: 0.545 0.542 0.050
2024-12-02-08:28:46-root-INFO: Loss Change: 2.220 -> 1.842
2024-12-02-08:28:46-root-INFO: Regularization Change: 0.000 -> 0.683
2024-12-02-08:28:46-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-08:28:46-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-08:28:47-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-08:28:47-root-INFO: grad norm: 1.280 1.257 0.242
2024-12-02-08:28:47-root-INFO: grad norm: 1.094 1.084 0.147
2024-12-02-08:28:48-root-INFO: grad norm: 1.014 1.006 0.129
2024-12-02-08:28:48-root-INFO: grad norm: 0.948 0.941 0.114
2024-12-02-08:28:49-root-INFO: grad norm: 0.904 0.896 0.113
2024-12-02-08:28:49-root-INFO: Loss too large (1.842->1.848)! Learning rate decreased to 0.42241.
2024-12-02-08:28:49-root-INFO: grad norm: 0.647 0.643 0.071
2024-12-02-08:28:50-root-INFO: grad norm: 0.491 0.489 0.046
2024-12-02-08:28:50-root-INFO: grad norm: 0.404 0.403 0.037
2024-12-02-08:28:51-root-INFO: Loss Change: 2.063 -> 1.705
2024-12-02-08:28:51-root-INFO: Regularization Change: 0.000 -> 0.655
2024-12-02-08:28:51-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-08:28:51-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-08:28:51-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-08:28:51-root-INFO: grad norm: 1.159 1.138 0.219
2024-12-02-08:28:51-root-INFO: grad norm: 0.955 0.948 0.115
2024-12-02-08:28:52-root-INFO: grad norm: 0.908 0.903 0.100
2024-12-02-08:28:52-root-INFO: grad norm: 0.833 0.829 0.084
2024-12-02-08:28:53-root-INFO: grad norm: 0.804 0.800 0.086
2024-12-02-08:28:53-root-INFO: grad norm: 0.792 0.789 0.075
2024-12-02-08:28:54-root-INFO: grad norm: 0.830 0.827 0.074
2024-12-02-08:28:54-root-INFO: grad norm: 0.771 0.768 0.071
2024-12-02-08:28:55-root-INFO: Loss Change: 1.926 -> 1.668
2024-12-02-08:28:55-root-INFO: Regularization Change: 0.000 -> 0.713
2024-12-02-08:28:55-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-08:28:55-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-08:28:55-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-08:28:55-root-INFO: grad norm: 1.497 1.477 0.245
2024-12-02-08:28:55-root-INFO: grad norm: 1.014 1.009 0.098
2024-12-02-08:28:56-root-INFO: grad norm: 0.880 0.877 0.066
2024-12-02-08:28:56-root-INFO: grad norm: 0.697 0.694 0.059
2024-12-02-08:28:57-root-INFO: grad norm: 0.644 0.642 0.052
2024-12-02-08:28:58-root-INFO: grad norm: 0.582 0.580 0.048
2024-12-02-08:28:58-root-INFO: grad norm: 0.559 0.557 0.046
2024-12-02-08:28:58-root-INFO: grad norm: 0.524 0.522 0.043
2024-12-02-08:28:59-root-INFO: Loss Change: 1.943 -> 1.530
2024-12-02-08:28:59-root-INFO: Regularization Change: 0.000 -> 0.703
2024-12-02-08:28:59-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-08:28:59-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-08:28:59-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-08:28:59-root-INFO: grad norm: 1.252 1.237 0.198
2024-12-02-08:29:00-root-INFO: grad norm: 0.777 0.774 0.067
2024-12-02-08:29:00-root-INFO: grad norm: 0.549 0.548 0.043
2024-12-02-08:29:00-root-INFO: grad norm: 0.426 0.424 0.035
2024-12-02-08:29:01-root-INFO: grad norm: 0.372 0.371 0.031
2024-12-02-08:29:01-root-INFO: grad norm: 0.337 0.336 0.029
2024-12-02-08:29:02-root-INFO: grad norm: 0.326 0.325 0.029
2024-12-02-08:29:02-root-INFO: grad norm: 0.323 0.321 0.029
2024-12-02-08:29:03-root-INFO: Loss Change: 1.783 -> 1.432
2024-12-02-08:29:03-root-INFO: Regularization Change: 0.000 -> 0.677
2024-12-02-08:29:03-root-INFO: Undo step: 5
2024-12-02-08:29:03-root-INFO: Undo step: 6
2024-12-02-08:29:03-root-INFO: Undo step: 7
2024-12-02-08:29:03-root-INFO: Undo step: 8
2024-12-02-08:29:03-root-INFO: Undo step: 9
2024-12-02-08:29:03-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-08:29:03-root-INFO: grad norm: 12.344 12.329 0.608
2024-12-02-08:29:03-root-INFO: grad norm: 4.687 4.677 0.314
2024-12-02-08:29:04-root-INFO: grad norm: 3.051 3.044 0.212
2024-12-02-08:29:04-root-INFO: grad norm: 2.087 2.082 0.143
2024-12-02-08:29:05-root-INFO: grad norm: 1.659 1.654 0.120
2024-12-02-08:29:05-root-INFO: grad norm: 1.523 1.520 0.099
2024-12-02-08:29:06-root-INFO: grad norm: 1.312 1.308 0.103
2024-12-02-08:29:06-root-INFO: grad norm: 1.180 1.178 0.079
2024-12-02-08:29:06-root-INFO: Loss Change: 55.840 -> 4.332
2024-12-02-08:29:06-root-INFO: Regularization Change: 0.000 -> 106.214
2024-12-02-08:29:06-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-08:29:06-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-08:29:07-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-08:29:07-root-INFO: grad norm: 1.348 1.342 0.133
2024-12-02-08:29:07-root-INFO: grad norm: 1.184 1.176 0.131
2024-12-02-08:29:08-root-INFO: grad norm: 1.143 1.139 0.095
2024-12-02-08:29:08-root-INFO: grad norm: 1.028 1.022 0.116
2024-12-02-08:29:09-root-INFO: grad norm: 0.936 0.933 0.083
2024-12-02-08:29:09-root-INFO: grad norm: 0.979 0.972 0.112
2024-12-02-08:29:10-root-INFO: grad norm: 1.171 1.166 0.099
2024-12-02-08:29:10-root-INFO: grad norm: 0.923 0.916 0.111
2024-12-02-08:29:10-root-INFO: Loss Change: 4.129 -> 2.602
2024-12-02-08:29:10-root-INFO: Regularization Change: 0.000 -> 6.024
2024-12-02-08:29:10-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-08:29:10-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-08:29:11-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-08:29:11-root-INFO: grad norm: 1.449 1.429 0.237
2024-12-02-08:29:11-root-INFO: grad norm: 1.154 1.141 0.173
2024-12-02-08:29:12-root-INFO: grad norm: 1.129 1.121 0.132
2024-12-02-08:29:12-root-INFO: grad norm: 0.989 0.980 0.135
2024-12-02-08:29:13-root-INFO: grad norm: 0.937 0.929 0.123
2024-12-02-08:29:13-root-INFO: grad norm: 0.971 0.963 0.123
2024-12-02-08:29:14-root-INFO: grad norm: 1.097 1.092 0.104
2024-12-02-08:29:14-root-INFO: grad norm: 0.888 0.881 0.114
2024-12-02-08:29:15-root-INFO: Loss Change: 2.692 -> 2.025
2024-12-02-08:29:15-root-INFO: Regularization Change: 0.000 -> 2.077
2024-12-02-08:29:15-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-08:29:15-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-08:29:15-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-08:29:15-root-INFO: grad norm: 1.624 1.594 0.314
2024-12-02-08:29:15-root-INFO: grad norm: 1.238 1.228 0.152
2024-12-02-08:29:16-root-INFO: grad norm: 1.165 1.162 0.087
2024-12-02-08:29:16-root-INFO: grad norm: 0.809 0.803 0.097
2024-12-02-08:29:17-root-INFO: grad norm: 0.930 0.926 0.087
2024-12-02-08:29:17-root-INFO: grad norm: 0.692 0.688 0.080
2024-12-02-08:29:18-root-INFO: grad norm: 0.714 0.710 0.079
2024-12-02-08:29:18-root-INFO: grad norm: 0.750 0.746 0.083
2024-12-02-08:29:19-root-INFO: Loss Change: 2.230 -> 1.744
2024-12-02-08:29:19-root-INFO: Regularization Change: 0.000 -> 1.166
2024-12-02-08:29:19-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-08:29:19-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-08:29:19-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-08:29:19-root-INFO: grad norm: 1.398 1.373 0.265
2024-12-02-08:29:19-root-INFO: grad norm: 1.048 1.041 0.116
2024-12-02-08:29:20-root-INFO: grad norm: 0.834 0.831 0.064
2024-12-02-08:29:20-root-INFO: grad norm: 0.664 0.660 0.067
2024-12-02-08:29:21-root-INFO: grad norm: 0.595 0.592 0.063
2024-12-02-08:29:21-root-INFO: grad norm: 0.554 0.551 0.058
2024-12-02-08:29:22-root-INFO: grad norm: 0.582 0.578 0.067
2024-12-02-08:29:22-root-INFO: grad norm: 0.553 0.550 0.057
2024-12-02-08:29:23-root-INFO: Loss Change: 1.950 -> 1.543
2024-12-02-08:29:23-root-INFO: Regularization Change: 0.000 -> 0.800
2024-12-02-08:29:23-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-08:29:23-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-08:29:23-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-08:29:23-root-INFO: grad norm: 1.202 1.181 0.223
2024-12-02-08:29:23-root-INFO: grad norm: 0.773 0.769 0.078
2024-12-02-08:29:24-root-INFO: grad norm: 0.532 0.531 0.040
2024-12-02-08:29:24-root-INFO: grad norm: 0.405 0.403 0.036
2024-12-02-08:29:25-root-INFO: grad norm: 0.358 0.357 0.032
2024-12-02-08:29:25-root-INFO: grad norm: 0.325 0.323 0.031
2024-12-02-08:29:26-root-INFO: grad norm: 0.345 0.343 0.037
2024-12-02-08:29:26-root-INFO: grad norm: 0.364 0.362 0.037
2024-12-02-08:29:27-root-INFO: Loss Change: 1.759 -> 1.426
2024-12-02-08:29:27-root-INFO: Regularization Change: 0.000 -> 0.672
2024-12-02-08:29:27-root-INFO: Undo step: 5
2024-12-02-08:29:27-root-INFO: Undo step: 6
2024-12-02-08:29:27-root-INFO: Undo step: 7
2024-12-02-08:29:27-root-INFO: Undo step: 8
2024-12-02-08:29:27-root-INFO: Undo step: 9
2024-12-02-08:29:27-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-08:29:27-root-INFO: grad norm: 13.052 13.037 0.626
2024-12-02-08:29:28-root-INFO: grad norm: 4.874 4.858 0.405
2024-12-02-08:29:28-root-INFO: grad norm: 3.294 3.286 0.220
2024-12-02-08:29:28-root-INFO: grad norm: 2.244 2.234 0.215
2024-12-02-08:29:29-root-INFO: grad norm: 1.777 1.772 0.123
2024-12-02-08:29:29-root-INFO: grad norm: 1.583 1.579 0.116
2024-12-02-08:29:30-root-INFO: grad norm: 1.444 1.440 0.114
2024-12-02-08:29:30-root-INFO: grad norm: 1.413 1.410 0.093
2024-12-02-08:29:31-root-INFO: Loss Change: 58.575 -> 4.512
2024-12-02-08:29:31-root-INFO: Regularization Change: 0.000 -> 112.231
2024-12-02-08:29:31-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-08:29:31-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-08:29:31-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-08:29:31-root-INFO: grad norm: 1.533 1.526 0.145
2024-12-02-08:29:31-root-INFO: grad norm: 1.356 1.351 0.114
2024-12-02-08:29:32-root-INFO: grad norm: 1.300 1.297 0.088
2024-12-02-08:29:32-root-INFO: grad norm: 1.324 1.321 0.084
2024-12-02-08:29:33-root-INFO: grad norm: 1.189 1.186 0.078
2024-12-02-08:29:33-root-INFO: grad norm: 1.105 1.102 0.078
2024-12-02-08:29:34-root-INFO: grad norm: 1.040 1.038 0.070
2024-12-02-08:29:34-root-INFO: grad norm: 1.016 1.014 0.064
2024-12-02-08:29:35-root-INFO: Loss Change: 4.252 -> 2.659
2024-12-02-08:29:35-root-INFO: Regularization Change: 0.000 -> 5.988
2024-12-02-08:29:35-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-08:29:35-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-08:29:35-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-08:29:35-root-INFO: grad norm: 1.299 1.289 0.157
2024-12-02-08:29:35-root-INFO: grad norm: 1.195 1.191 0.099
2024-12-02-08:29:36-root-INFO: grad norm: 0.995 0.992 0.072
2024-12-02-08:29:36-root-INFO: grad norm: 0.875 0.873 0.065
2024-12-02-08:29:37-root-INFO: grad norm: 0.822 0.821 0.054
2024-12-02-08:29:37-root-INFO: grad norm: 0.906 0.905 0.053
2024-12-02-08:29:38-root-INFO: grad norm: 0.868 0.867 0.055
2024-12-02-08:29:38-root-INFO: grad norm: 0.904 0.902 0.061
2024-12-02-08:29:39-root-INFO: Loss Change: 2.608 -> 2.047
2024-12-02-08:29:39-root-INFO: Regularization Change: 0.000 -> 1.970
2024-12-02-08:29:39-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-08:29:39-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-08:29:39-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-08:29:39-root-INFO: grad norm: 1.162 1.153 0.146
2024-12-02-08:29:39-root-INFO: grad norm: 1.057 1.055 0.076
2024-12-02-08:29:40-root-INFO: grad norm: 0.839 0.837 0.057
2024-12-02-08:29:40-root-INFO: grad norm: 0.852 0.851 0.048
2024-12-02-08:29:41-root-INFO: grad norm: 0.765 0.764 0.048
2024-12-02-08:29:41-root-INFO: grad norm: 0.920 0.919 0.054
2024-12-02-08:29:42-root-INFO: grad norm: 0.779 0.777 0.053
2024-12-02-08:29:42-root-INFO: grad norm: 0.795 0.794 0.051
2024-12-02-08:29:43-root-INFO: Loss Change: 2.080 -> 1.730
2024-12-02-08:29:43-root-INFO: Regularization Change: 0.000 -> 1.035
2024-12-02-08:29:43-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-08:29:43-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-08:29:43-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-08:29:43-root-INFO: grad norm: 0.972 0.965 0.120
2024-12-02-08:29:43-root-INFO: grad norm: 0.734 0.732 0.054
2024-12-02-08:29:44-root-INFO: grad norm: 0.632 0.631 0.042
2024-12-02-08:29:44-root-INFO: grad norm: 0.607 0.606 0.046
2024-12-02-08:29:45-root-INFO: grad norm: 0.583 0.581 0.042
2024-12-02-08:29:45-root-INFO: grad norm: 0.577 0.576 0.046
2024-12-02-08:29:46-root-INFO: grad norm: 0.575 0.573 0.044
2024-12-02-08:29:46-root-INFO: grad norm: 0.581 0.578 0.050
2024-12-02-08:29:47-root-INFO: Loss Change: 1.814 -> 1.547
2024-12-02-08:29:47-root-INFO: Regularization Change: 0.000 -> 0.740
2024-12-02-08:29:47-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-08:29:47-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-08:29:47-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-08:29:47-root-INFO: grad norm: 0.869 0.861 0.115
2024-12-02-08:29:47-root-INFO: grad norm: 0.525 0.523 0.044
2024-12-02-08:29:48-root-INFO: grad norm: 0.434 0.433 0.036
2024-12-02-08:29:48-root-INFO: grad norm: 0.413 0.411 0.038
2024-12-02-08:29:49-root-INFO: grad norm: 0.405 0.403 0.034
2024-12-02-08:29:49-root-INFO: grad norm: 0.406 0.404 0.039
2024-12-02-08:29:50-root-INFO: grad norm: 0.403 0.402 0.035
2024-12-02-08:29:50-root-INFO: grad norm: 0.402 0.400 0.038
2024-12-02-08:29:51-root-INFO: Loss Change: 1.670 -> 1.427
2024-12-02-08:29:51-root-INFO: Regularization Change: 0.000 -> 0.622
2024-12-02-08:29:51-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-08:29:51-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-08:29:51-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-08:29:51-root-INFO: grad norm: 0.761 0.755 0.095
2024-12-02-08:29:51-root-INFO: grad norm: 0.385 0.383 0.035
2024-12-02-08:29:52-root-INFO: grad norm: 0.311 0.310 0.028
2024-12-02-08:29:52-root-INFO: grad norm: 0.320 0.319 0.030
2024-12-02-08:29:53-root-INFO: grad norm: 0.322 0.321 0.029
2024-12-02-08:29:53-root-INFO: grad norm: 0.331 0.330 0.029
2024-12-02-08:29:54-root-INFO: grad norm: 0.327 0.326 0.028
2024-12-02-08:29:54-root-INFO: grad norm: 0.328 0.327 0.029
2024-12-02-08:29:55-root-INFO: Loss Change: 1.564 -> 1.350
2024-12-02-08:29:55-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-02-08:29:55-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-08:29:55-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-08:29:55-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-08:29:55-root-INFO: grad norm: 0.731 0.727 0.080
2024-12-02-08:29:55-root-INFO: grad norm: 0.323 0.322 0.032
2024-12-02-08:29:56-root-INFO: grad norm: 0.242 0.241 0.026
2024-12-02-08:29:56-root-INFO: grad norm: 0.225 0.224 0.025
2024-12-02-08:29:57-root-INFO: grad norm: 0.226 0.225 0.024
2024-12-02-08:29:57-root-INFO: grad norm: 0.249 0.248 0.024
2024-12-02-08:29:58-root-INFO: grad norm: 0.270 0.269 0.024
2024-12-02-08:29:58-root-INFO: grad norm: 0.264 0.263 0.023
2024-12-02-08:29:59-root-INFO: Loss Change: 1.507 -> 1.292
2024-12-02-08:29:59-root-INFO: Regularization Change: 0.000 -> 0.568
2024-12-02-08:29:59-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-08:29:59-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-08:29:59-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-08:29:59-root-INFO: grad norm: 0.684 0.681 0.062
2024-12-02-08:29:59-root-INFO: grad norm: 0.296 0.294 0.032
2024-12-02-08:30:00-root-INFO: grad norm: 0.207 0.206 0.025
2024-12-02-08:30:00-root-INFO: grad norm: 0.344 0.343 0.024
2024-12-02-08:30:01-root-INFO: grad norm: 0.166 0.165 0.022
2024-12-02-08:30:01-root-INFO: grad norm: 0.253 0.252 0.021
2024-12-02-08:30:02-root-INFO: grad norm: 0.150 0.149 0.020
2024-12-02-08:30:02-root-INFO: grad norm: 0.151 0.150 0.019
2024-12-02-08:30:03-root-INFO: Loss Change: 1.449 -> 1.230
2024-12-02-08:30:03-root-INFO: Regularization Change: 0.000 -> 0.619
2024-12-02-08:30:03-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-08:30:03-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-08:30:03-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-08:30:03-root-INFO: grad norm: 0.716 0.714 0.057
2024-12-02-08:30:03-root-INFO: grad norm: 0.344 0.343 0.027
2024-12-02-08:30:04-root-INFO: grad norm: 0.293 0.292 0.021
2024-12-02-08:30:04-root-INFO: grad norm: 0.240 0.239 0.021
2024-12-02-08:30:05-root-INFO: grad norm: 0.273 0.273 0.019
2024-12-02-08:30:05-root-INFO: grad norm: 0.197 0.196 0.019
2024-12-02-08:30:06-root-INFO: grad norm: 0.199 0.198 0.016
2024-12-02-08:30:06-root-INFO: grad norm: 0.181 0.180 0.017
2024-12-02-08:30:07-root-INFO: Loss Change: 1.376 -> 1.135
2024-12-02-08:30:07-root-INFO: Regularization Change: 0.000 -> 0.726
2024-12-02-08:30:07-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-08:30:07-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-08:30:07-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-08:30:07-root-INFO: grad norm: 0.742 0.737 0.078
2024-12-02-08:30:07-root-INFO: grad norm: 0.413 0.413 0.023
2024-12-02-08:30:08-root-INFO: grad norm: 0.345 0.343 0.043
2024-12-02-08:30:08-root-INFO: grad norm: 0.323 0.319 0.050
2024-12-02-08:30:09-root-INFO: grad norm: 0.312 0.307 0.051
2024-12-02-08:30:09-root-INFO: grad norm: 0.304 0.300 0.051
2024-12-02-08:30:10-root-INFO: grad norm: 0.299 0.295 0.049
2024-12-02-08:30:10-root-INFO: grad norm: 0.294 0.290 0.046
2024-12-02-08:30:11-root-INFO: Loss Change: 1.279 -> 0.693
2024-12-02-08:30:11-root-INFO: Regularization Change: 0.000 -> 2.250
2024-12-02-08:30:11-root-INFO: Undo step: 0
2024-12-02-08:30:11-root-INFO: Undo step: 1
2024-12-02-08:30:11-root-INFO: Undo step: 2
2024-12-02-08:30:11-root-INFO: Undo step: 3
2024-12-02-08:30:11-root-INFO: Undo step: 4
2024-12-02-08:30:11-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-08:30:11-root-INFO: grad norm: 11.707 11.701 0.371
2024-12-02-08:30:11-root-INFO: grad norm: 5.716 5.712 0.233
2024-12-02-08:30:12-root-INFO: grad norm: 3.057 3.056 0.096
2024-12-02-08:30:12-root-INFO: grad norm: 2.874 2.873 0.079
2024-12-02-08:30:13-root-INFO: grad norm: 1.659 1.658 0.054
2024-12-02-08:30:13-root-INFO: grad norm: 2.526 2.525 0.065
2024-12-02-08:30:14-root-INFO: grad norm: 1.444 1.443 0.039
2024-12-02-08:30:14-root-INFO: grad norm: 1.745 1.744 0.068
2024-12-02-08:30:15-root-INFO: Loss Change: 36.051 -> 2.943
2024-12-02-08:30:15-root-INFO: Regularization Change: 0.000 -> 66.251
2024-12-02-08:30:15-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-08:30:15-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-08:30:15-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-08:30:15-root-INFO: grad norm: 1.387 1.381 0.133
2024-12-02-08:30:15-root-INFO: grad norm: 1.122 1.120 0.071
2024-12-02-08:30:16-root-INFO: grad norm: 0.753 0.753 0.028
2024-12-02-08:30:16-root-INFO: grad norm: 0.498 0.497 0.034
2024-12-02-08:30:17-root-INFO: grad norm: 0.434 0.433 0.027
2024-12-02-08:30:17-root-INFO: grad norm: 0.374 0.372 0.030
2024-12-02-08:30:18-root-INFO: grad norm: 0.352 0.351 0.029
2024-12-02-08:30:18-root-INFO: grad norm: 0.357 0.355 0.033
2024-12-02-08:30:18-root-INFO: Loss Change: 2.966 -> 1.863
2024-12-02-08:30:18-root-INFO: Regularization Change: 0.000 -> 2.883
2024-12-02-08:30:18-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-08:30:18-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-08:30:19-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-08:30:19-root-INFO: grad norm: 0.954 0.946 0.123
2024-12-02-08:30:19-root-INFO: grad norm: 0.564 0.562 0.054
2024-12-02-08:30:20-root-INFO: grad norm: 0.384 0.383 0.025
2024-12-02-08:30:20-root-INFO: grad norm: 0.346 0.345 0.029
2024-12-02-08:30:21-root-INFO: grad norm: 0.847 0.847 0.024
2024-12-02-08:30:21-root-INFO: grad norm: 0.913 0.913 0.025
2024-12-02-08:30:21-root-INFO: Loss too large (1.565->1.635)! Learning rate decreased to 0.45084.
2024-12-02-08:30:22-root-INFO: grad norm: 0.658 0.657 0.021
2024-12-02-08:30:22-root-INFO: grad norm: 0.197 0.196 0.022
2024-12-02-08:30:23-root-INFO: Loss Change: 1.984 -> 1.494
2024-12-02-08:30:23-root-INFO: Regularization Change: 0.000 -> 1.267
2024-12-02-08:30:23-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-08:30:23-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-08:30:23-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-08:30:23-root-INFO: grad norm: 0.757 0.752 0.090
2024-12-02-08:30:23-root-INFO: grad norm: 0.703 0.702 0.043
2024-12-02-08:30:24-root-INFO: Loss too large (1.515->1.519)! Learning rate decreased to 0.45653.
2024-12-02-08:30:24-root-INFO: grad norm: 0.450 0.449 0.026
2024-12-02-08:30:25-root-INFO: grad norm: 0.217 0.215 0.026
2024-12-02-08:30:25-root-INFO: grad norm: 0.204 0.202 0.024
2024-12-02-08:30:25-root-INFO: grad norm: 0.196 0.195 0.023
2024-12-02-08:30:26-root-INFO: grad norm: 0.225 0.224 0.022
2024-12-02-08:30:26-root-INFO: grad norm: 0.489 0.489 0.020
2024-12-02-08:30:27-root-INFO: Loss Change: 1.628 -> 1.250
2024-12-02-08:30:27-root-INFO: Regularization Change: 0.000 -> 0.727
2024-12-02-08:30:27-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-08:30:27-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-08:30:27-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-08:30:27-root-INFO: grad norm: 0.701 0.698 0.067
2024-12-02-08:30:27-root-INFO: grad norm: 0.368 0.367 0.029
2024-12-02-08:30:28-root-INFO: grad norm: 0.348 0.347 0.019
2024-12-02-08:30:28-root-INFO: grad norm: 0.254 0.253 0.022
2024-12-02-08:30:29-root-INFO: grad norm: 0.228 0.227 0.018
2024-12-02-08:30:29-root-INFO: grad norm: 0.217 0.216 0.020
2024-12-02-08:30:30-root-INFO: grad norm: 0.210 0.210 0.016
2024-12-02-08:30:30-root-INFO: grad norm: 0.191 0.190 0.018
2024-12-02-08:30:31-root-INFO: Loss Change: 1.369 -> 1.118
2024-12-02-08:30:31-root-INFO: Regularization Change: 0.000 -> 0.788
2024-12-02-08:30:31-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-08:30:31-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-08:30:31-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-08:30:31-root-INFO: grad norm: 0.734 0.730 0.074
2024-12-02-08:30:31-root-INFO: grad norm: 0.415 0.414 0.021
2024-12-02-08:30:32-root-INFO: grad norm: 0.348 0.346 0.041
2024-12-02-08:30:32-root-INFO: grad norm: 0.322 0.319 0.048
2024-12-02-08:30:33-root-INFO: grad norm: 0.310 0.306 0.048
2024-12-02-08:30:33-root-INFO: grad norm: 0.302 0.299 0.047
2024-12-02-08:30:34-root-INFO: grad norm: 0.297 0.293 0.044
2024-12-02-08:30:34-root-INFO: grad norm: 0.290 0.287 0.041
2024-12-02-08:30:35-root-INFO: Loss Change: 1.236 -> 0.641
2024-12-02-08:30:35-root-INFO: Regularization Change: 0.000 -> 2.310
2024-12-02-08:30:35-root-INFO: Undo step: 0
2024-12-02-08:30:35-root-INFO: Undo step: 1
2024-12-02-08:30:35-root-INFO: Undo step: 2
2024-12-02-08:30:35-root-INFO: Undo step: 3
2024-12-02-08:30:35-root-INFO: Undo step: 4
2024-12-02-08:30:35-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-08:30:35-root-INFO: grad norm: 13.232 13.224 0.454
2024-12-02-08:30:35-root-INFO: grad norm: 6.812 6.805 0.306
2024-12-02-08:30:36-root-INFO: grad norm: 4.187 4.186 0.098
2024-12-02-08:30:36-root-INFO: grad norm: 2.844 2.842 0.092
2024-12-02-08:30:37-root-INFO: grad norm: 2.120 2.118 0.077
2024-12-02-08:30:37-root-INFO: grad norm: 1.941 1.939 0.073
2024-12-02-08:30:38-root-INFO: grad norm: 1.318 1.316 0.076
2024-12-02-08:30:38-root-INFO: grad norm: 1.477 1.475 0.077
2024-12-02-08:30:38-root-INFO: Loss Change: 38.087 -> 3.167
2024-12-02-08:30:38-root-INFO: Regularization Change: 0.000 -> 69.489
2024-12-02-08:30:38-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-08:30:38-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-08:30:39-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-08:30:39-root-INFO: grad norm: 1.375 1.369 0.126
2024-12-02-08:30:39-root-INFO: grad norm: 1.755 1.753 0.073
2024-12-02-08:30:40-root-INFO: grad norm: 1.519 1.518 0.062
2024-12-02-08:30:40-root-INFO: grad norm: 0.606 0.604 0.041
2024-12-02-08:30:41-root-INFO: grad norm: 0.647 0.646 0.040
2024-12-02-08:30:41-root-INFO: grad norm: 0.444 0.442 0.041
2024-12-02-08:30:41-root-INFO: grad norm: 0.926 0.924 0.055
2024-12-02-08:30:42-root-INFO: Loss too large (2.260->2.278)! Learning rate decreased to 0.44514.
2024-12-02-08:30:42-root-INFO: grad norm: 0.544 0.542 0.049
2024-12-02-08:30:42-root-INFO: Loss Change: 3.211 -> 2.160
2024-12-02-08:30:42-root-INFO: Regularization Change: 0.000 -> 3.138
2024-12-02-08:30:42-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-08:30:42-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-08:30:43-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-08:30:43-root-INFO: grad norm: 0.829 0.822 0.111
2024-12-02-08:30:43-root-INFO: grad norm: 0.589 0.586 0.059
2024-12-02-08:30:44-root-INFO: grad norm: 1.281 1.280 0.062
2024-12-02-08:30:44-root-INFO: grad norm: 0.549 0.547 0.050
2024-12-02-08:30:45-root-INFO: grad norm: 0.519 0.516 0.055
2024-12-02-08:30:45-root-INFO: grad norm: 0.437 0.434 0.049
2024-12-02-08:30:46-root-INFO: grad norm: 0.389 0.386 0.047
2024-12-02-08:30:46-root-INFO: grad norm: 0.967 0.966 0.047
2024-12-02-08:30:46-root-INFO: Loss too large (1.778->1.822)! Learning rate decreased to 0.45084.
2024-12-02-08:30:47-root-INFO: Loss Change: 2.289 -> 1.706
2024-12-02-08:30:47-root-INFO: Regularization Change: 0.000 -> 2.066
2024-12-02-08:30:47-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-08:30:47-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-08:30:47-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-08:30:47-root-INFO: grad norm: 0.842 0.835 0.106
2024-12-02-08:30:47-root-INFO: grad norm: 0.476 0.473 0.052
2024-12-02-08:30:48-root-INFO: grad norm: 0.928 0.927 0.043
2024-12-02-08:30:48-root-INFO: Loss too large (1.653->1.691)! Learning rate decreased to 0.45653.
2024-12-02-08:30:48-root-INFO: grad norm: 1.480 1.479 0.057
2024-12-02-08:30:49-root-INFO: Loss too large (1.619->1.735)! Learning rate decreased to 0.36522.
2024-12-02-08:30:49-root-INFO: Loss too large (1.619->1.629)! Learning rate decreased to 0.29218.
2024-12-02-08:30:49-root-INFO: Loss too large (1.619->1.624)! Learning rate decreased to 0.23374.
2024-12-02-08:30:49-root-INFO: grad norm: 0.969 0.968 0.043
2024-12-02-08:30:50-root-INFO: grad norm: 0.203 0.201 0.029
2024-12-02-08:30:50-root-INFO: grad norm: 0.199 0.196 0.029
2024-12-02-08:30:51-root-INFO: grad norm: 0.187 0.184 0.029
2024-12-02-08:30:51-root-INFO: Loss Change: 1.861 -> 1.522
2024-12-02-08:30:51-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-02-08:30:51-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-08:30:51-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-08:30:51-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-08:30:52-root-INFO: grad norm: 0.715 0.711 0.073
2024-12-02-08:30:52-root-INFO: grad norm: 0.555 0.553 0.037
2024-12-02-08:30:52-root-INFO: grad norm: 0.750 0.749 0.039
2024-12-02-08:30:53-root-INFO: grad norm: 1.503 1.502 0.034
2024-12-02-08:30:53-root-INFO: Loss too large (1.324->1.615)! Learning rate decreased to 0.46222.
2024-12-02-08:30:53-root-INFO: Loss too large (1.324->1.421)! Learning rate decreased to 0.36978.
2024-12-02-08:30:53-root-INFO: Loss too large (1.324->1.381)! Learning rate decreased to 0.29582.
2024-12-02-08:30:54-root-INFO: Loss too large (1.324->1.355)! Learning rate decreased to 0.23666.
2024-12-02-08:30:54-root-INFO: grad norm: 0.341 0.340 0.027
2024-12-02-08:30:55-root-INFO: grad norm: 0.395 0.394 0.026
2024-12-02-08:30:55-root-INFO: grad norm: 0.484 0.483 0.032
2024-12-02-08:30:56-root-INFO: grad norm: 0.180 0.178 0.021
2024-12-02-08:30:56-root-INFO: Loss Change: 1.673 -> 1.175
2024-12-02-08:30:56-root-INFO: Regularization Change: 0.000 -> 0.715
2024-12-02-08:30:56-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-08:30:56-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-08:30:56-root-INFO: loss_sample0_0: 1.1752915382385254
2024-12-02-08:30:56-root-INFO: It takes 3685.483 seconds for image sample0
2024-12-02-08:30:56-root-INFO: lpips_score_sample0: 0.160
2024-12-02-08:30:56-root-INFO: psnr_score_sample0: 18.058
2024-12-02-08:30:56-root-INFO: ssim_score_sample0: 0.720
2024-12-02-08:30:56-root-INFO: mean_lpips: 0.1600937694311142
2024-12-02-08:30:56-root-INFO: best_mean_lpips: 0.1600937694311142
2024-12-02-08:30:56-root-INFO: mean_psnr: 18.058347702026367
2024-12-02-08:30:56-root-INFO: best_mean_psnr: 18.058347702026367
2024-12-02-08:30:56-root-INFO: mean_ssim: 0.7198359966278076
2024-12-02-08:30:56-root-INFO: best_mean_ssim: 0.7198359966278076
2024-12-02-08:30:56-root-INFO: final_loss: 1.1752915382385254
2024-12-02-08:30:56-root-INFO: mean time: 3685.482885360718
2024-12-02-08:30:56-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample3_iter8_lr0.03_10009 
 
Enjoy.
