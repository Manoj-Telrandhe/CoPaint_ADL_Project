2024-12-01-16:59:39-root-INFO: Prepare model...
2024-12-01-16:59:54-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-01-17:00:15-root-INFO: Start sampling
2024-12-01-17:00:19-root-INFO: step: 249 lr_xt 0.00012706
2024-12-01-17:00:20-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-01-17:00:20-root-INFO: grad norm: 23712.223 17715.094 15762.136
2024-12-01-17:00:21-root-INFO: grad norm: 26791.061 21193.543 16388.859
2024-12-01-17:00:21-root-INFO: Loss too large (43809.453->60938.469)! Learning rate decreased to 0.00010.
2024-12-01-17:00:21-root-INFO: Loss too large (43809.453->45473.414)! Learning rate decreased to 0.00008.
2024-12-01-17:00:21-root-INFO: grad norm: 21161.055 16197.232 13617.633
2024-12-01-17:00:22-root-INFO: grad norm: 19142.789 15678.440 10983.301
2024-12-01-17:00:22-root-INFO: grad norm: 17823.029 14000.195 11029.729
2024-12-01-17:00:23-root-INFO: grad norm: 16609.783 13519.576 9649.143
2024-12-01-17:00:23-root-INFO: grad norm: 15349.657 12274.128 9217.253
2024-12-01-17:00:24-root-INFO: Loss Change: 77070.016 -> 21842.852
2024-12-01-17:00:24-root-INFO: Regularization Change: 0.000 -> 14.876
2024-12-01-17:00:24-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-01-17:00:24-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-17:00:24-root-INFO: step: 248 lr_xt 0.00013388
2024-12-01-17:00:24-root-INFO: grad norm: 13708.088 11021.792 8150.569
2024-12-01-17:00:24-root-INFO: Loss too large (22203.570->32157.561)! Learning rate decreased to 0.00011.
2024-12-01-17:00:24-root-INFO: Loss too large (22203.570->25109.410)! Learning rate decreased to 0.00009.
2024-12-01-17:00:25-root-INFO: grad norm: 12006.502 9573.826 7245.546
2024-12-01-17:00:25-root-INFO: grad norm: 10369.067 8292.964 6224.494
2024-12-01-17:00:26-root-INFO: grad norm: 8986.046 7202.863 5372.875
2024-12-01-17:00:26-root-INFO: grad norm: 7667.549 6109.314 4633.313
2024-12-01-17:00:27-root-INFO: grad norm: 6571.781 5282.635 3909.229
2024-12-01-17:00:27-root-INFO: grad norm: 5545.535 4409.721 3362.635
2024-12-01-17:00:27-root-INFO: grad norm: 4705.105 3787.783 2791.185
2024-12-01-17:00:28-root-INFO: Loss Change: 22203.570 -> 16910.113
2024-12-01-17:00:28-root-INFO: Regularization Change: 0.000 -> 1.222
2024-12-01-17:00:28-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-01-17:00:28-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-17:00:28-root-INFO: step: 247 lr_xt 0.00014104
2024-12-01-17:00:28-root-INFO: grad norm: 3929.217 3124.460 2382.540
2024-12-01-17:00:28-root-INFO: Loss too large (16757.668->17453.145)! Learning rate decreased to 0.00011.
2024-12-01-17:00:28-root-INFO: Loss too large (16757.668->16866.418)! Learning rate decreased to 0.00009.
2024-12-01-17:00:29-root-INFO: grad norm: 3143.719 2561.393 1822.700
2024-12-01-17:00:29-root-INFO: grad norm: 2509.538 2011.451 1500.615
2024-12-01-17:00:30-root-INFO: grad norm: 2036.146 1659.577 1179.702
2024-12-01-17:00:30-root-INFO: grad norm: 1668.167 1355.923 971.727
2024-12-01-17:00:31-root-INFO: grad norm: 1397.021 1141.616 805.219
2024-12-01-17:00:31-root-INFO: grad norm: 1193.037 990.911 664.405
2024-12-01-17:00:32-root-INFO: grad norm: 1046.650 862.562 592.843
2024-12-01-17:00:32-root-INFO: Loss Change: 16757.668 -> 15907.414
2024-12-01-17:00:32-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-01-17:00:32-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-01-17:00:32-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-17:00:32-root-INFO: step: 246 lr_xt 0.00014856
2024-12-01-17:00:33-root-INFO: grad norm: 1025.909 863.658 553.700
2024-12-01-17:00:33-root-INFO: grad norm: 1243.634 1016.896 715.925
2024-12-01-17:00:34-root-INFO: grad norm: 1798.459 1493.836 1001.454
2024-12-01-17:00:34-root-INFO: Loss too large (15567.979->15621.322)! Learning rate decreased to 0.00012.
2024-12-01-17:00:34-root-INFO: grad norm: 2025.146 1622.976 1211.266
2024-12-01-17:00:35-root-INFO: grad norm: 2314.601 1916.045 1298.518
2024-12-01-17:00:35-root-INFO: grad norm: 2648.524 2115.419 1593.638
2024-12-01-17:00:36-root-INFO: grad norm: 3057.349 2515.269 1738.046
2024-12-01-17:00:36-root-INFO: Loss too large (15468.562->15496.854)! Learning rate decreased to 0.00010.
2024-12-01-17:00:36-root-INFO: grad norm: 2305.619 1839.296 1390.276
2024-12-01-17:00:37-root-INFO: Loss Change: 15687.754 -> 15196.312
2024-12-01-17:00:37-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-01-17:00:37-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-01-17:00:37-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-17:00:37-root-INFO: step: 245 lr_xt 0.00015646
2024-12-01-17:00:37-root-INFO: grad norm: 1696.566 1421.363 926.318
2024-12-01-17:00:37-root-INFO: Loss too large (15078.044->15098.402)! Learning rate decreased to 0.00013.
2024-12-01-17:00:38-root-INFO: grad norm: 1815.687 1445.126 1099.240
2024-12-01-17:00:38-root-INFO: grad norm: 1979.715 1662.500 1074.879
2024-12-01-17:00:39-root-INFO: grad norm: 2166.909 1724.415 1312.206
2024-12-01-17:00:39-root-INFO: grad norm: 2387.935 1993.830 1314.106
2024-12-01-17:00:40-root-INFO: grad norm: 2631.396 2096.750 1589.932
2024-12-01-17:00:40-root-INFO: grad norm: 2917.012 2419.963 1628.724
2024-12-01-17:00:41-root-INFO: grad norm: 3215.267 2567.444 1935.504
2024-12-01-17:00:41-root-INFO: Loss Change: 15078.044 -> 14828.777
2024-12-01-17:00:41-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-01-17:00:41-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-01-17:00:41-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-17:00:41-root-INFO: step: 244 lr_xt 0.00016475
2024-12-01-17:00:41-root-INFO: grad norm: 3662.985 2984.780 2123.335
2024-12-01-17:00:42-root-INFO: Loss too large (14750.073->15195.651)! Learning rate decreased to 0.00013.
2024-12-01-17:00:42-root-INFO: grad norm: 3764.824 3041.635 2218.639
2024-12-01-17:00:43-root-INFO: grad norm: 3954.495 3261.656 2235.985
2024-12-01-17:00:43-root-INFO: Loss too large (14634.729->14640.499)! Learning rate decreased to 0.00011.
2024-12-01-17:00:43-root-INFO: grad norm: 2666.715 2152.955 1573.580
2024-12-01-17:00:44-root-INFO: grad norm: 1816.876 1545.637 955.011
2024-12-01-17:00:44-root-INFO: grad norm: 1325.578 1081.598 766.356
2024-12-01-17:00:45-root-INFO: grad norm: 1033.374 919.081 472.391
2024-12-01-17:00:45-root-INFO: grad norm: 878.133 746.802 461.956
2024-12-01-17:00:46-root-INFO: Loss Change: 14750.073 -> 13815.985
2024-12-01-17:00:46-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-01-17:00:46-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-01-17:00:46-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:00:46-root-INFO: step: 243 lr_xt 0.00017345
2024-12-01-17:00:46-root-INFO: grad norm: 1012.109 835.407 571.366
2024-12-01-17:00:46-root-INFO: grad norm: 976.565 786.526 578.840
2024-12-01-17:00:47-root-INFO: grad norm: 1060.075 889.966 575.951
2024-12-01-17:00:47-root-INFO: grad norm: 1233.644 959.441 775.468
2024-12-01-17:00:48-root-INFO: grad norm: 1537.523 1258.158 883.751
2024-12-01-17:00:48-root-INFO: grad norm: 2012.392 1556.086 1276.055
2024-12-01-17:00:49-root-INFO: Loss too large (13320.742->13354.342)! Learning rate decreased to 0.00014.
2024-12-01-17:00:49-root-INFO: grad norm: 1905.853 1574.907 1073.286
2024-12-01-17:00:50-root-INFO: grad norm: 1860.179 1479.503 1127.535
2024-12-01-17:00:50-root-INFO: Loss Change: 13697.788 -> 13074.775
2024-12-01-17:00:50-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-01-17:00:50-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-01-17:00:50-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:00:50-root-INFO: step: 242 lr_xt 0.00018258
2024-12-01-17:00:50-root-INFO: grad norm: 1945.168 1605.181 1098.670
2024-12-01-17:00:50-root-INFO: Loss too large (12881.296->12896.832)! Learning rate decreased to 0.00015.
2024-12-01-17:00:51-root-INFO: grad norm: 1838.303 1492.350 1073.430
2024-12-01-17:00:52-root-INFO: grad norm: 1783.597 1514.698 941.759
2024-12-01-17:00:52-root-INFO: grad norm: 1756.379 1433.641 1014.663
2024-12-01-17:00:53-root-INFO: grad norm: 1744.551 1493.070 902.330
2024-12-01-17:00:53-root-INFO: grad norm: 1743.600 1429.253 998.686
2024-12-01-17:00:53-root-INFO: grad norm: 1744.894 1494.856 900.035
2024-12-01-17:00:54-root-INFO: grad norm: 1747.764 1437.289 994.423
2024-12-01-17:00:54-root-INFO: Loss Change: 12881.296 -> 12240.984
2024-12-01-17:00:54-root-INFO: Regularization Change: 0.000 -> 0.714
2024-12-01-17:00:54-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-01-17:00:54-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:00:54-root-INFO: step: 241 lr_xt 0.00019216
2024-12-01-17:00:55-root-INFO: grad norm: 1781.012 1516.303 934.255
2024-12-01-17:00:55-root-INFO: Loss too large (12147.166->12162.891)! Learning rate decreased to 0.00015.
2024-12-01-17:00:55-root-INFO: grad norm: 1716.585 1434.772 942.387
2024-12-01-17:00:56-root-INFO: grad norm: 1663.263 1429.836 849.712
2024-12-01-17:00:56-root-INFO: grad norm: 1613.417 1347.079 887.971
2024-12-01-17:00:57-root-INFO: grad norm: 1564.136 1349.842 790.220
2024-12-01-17:00:57-root-INFO: grad norm: 1518.018 1269.381 832.496
2024-12-01-17:00:57-root-INFO: grad norm: 1471.944 1274.218 736.877
2024-12-01-17:00:58-root-INFO: grad norm: 1425.466 1195.223 776.785
2024-12-01-17:00:58-root-INFO: Loss Change: 12147.166 -> 11491.025
2024-12-01-17:00:58-root-INFO: Regularization Change: 0.000 -> 0.749
2024-12-01-17:00:58-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-01-17:00:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:00:58-root-INFO: step: 240 lr_xt 0.00020221
2024-12-01-17:00:59-root-INFO: grad norm: 1330.781 1145.826 676.801
2024-12-01-17:00:59-root-INFO: grad norm: 1673.190 1412.891 896.273
2024-12-01-17:00:59-root-INFO: grad norm: 2239.566 1921.314 1150.742
2024-12-01-17:01:00-root-INFO: Loss too large (11229.059->11320.450)! Learning rate decreased to 0.00016.
2024-12-01-17:01:00-root-INFO: grad norm: 2076.828 1744.921 1126.261
2024-12-01-17:01:01-root-INFO: grad norm: 1926.013 1659.250 977.965
2024-12-01-17:01:01-root-INFO: grad norm: 1789.661 1507.340 964.787
2024-12-01-17:01:02-root-INFO: grad norm: 1662.563 1439.089 832.549
2024-12-01-17:01:02-root-INFO: grad norm: 1548.291 1308.349 827.905
2024-12-01-17:01:02-root-INFO: Loss Change: 11297.898 -> 10670.743
2024-12-01-17:01:02-root-INFO: Regularization Change: 0.000 -> 0.879
2024-12-01-17:01:02-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-01-17:01:02-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:01:03-root-INFO: step: 239 lr_xt 0.00021275
2024-12-01-17:01:03-root-INFO: grad norm: 1347.549 1172.412 664.332
2024-12-01-17:01:03-root-INFO: grad norm: 1702.026 1440.688 906.263
2024-12-01-17:01:04-root-INFO: grad norm: 2211.431 1897.911 1135.059
2024-12-01-17:01:04-root-INFO: Loss too large (10527.668->10604.282)! Learning rate decreased to 0.00017.
2024-12-01-17:01:04-root-INFO: grad norm: 1960.564 1658.945 1044.850
2024-12-01-17:01:05-root-INFO: grad norm: 1739.718 1505.158 872.420
2024-12-01-17:01:05-root-INFO: grad norm: 1547.641 1314.137 817.457
2024-12-01-17:01:06-root-INFO: grad norm: 1378.858 1204.744 670.703
2024-12-01-17:01:06-root-INFO: grad norm: 1234.158 1054.065 641.945
2024-12-01-17:01:07-root-INFO: Loss Change: 10571.635 -> 9962.459
2024-12-01-17:01:07-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-01-17:01:07-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-01-17:01:07-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:01:07-root-INFO: step: 238 lr_xt 0.00022380
2024-12-01-17:01:07-root-INFO: grad norm: 1007.264 860.112 524.202
2024-12-01-17:01:08-root-INFO: grad norm: 1054.273 906.595 538.122
2024-12-01-17:01:08-root-INFO: grad norm: 1237.471 1086.978 591.450
2024-12-01-17:01:09-root-INFO: grad norm: 1501.503 1278.647 787.129
2024-12-01-17:01:09-root-INFO: grad norm: 1861.974 1620.109 917.712
2024-12-01-17:01:09-root-INFO: Loss too large (9543.582->9573.680)! Learning rate decreased to 0.00018.
2024-12-01-17:01:10-root-INFO: grad norm: 1561.275 1334.811 809.852
2024-12-01-17:01:10-root-INFO: grad norm: 1316.326 1162.210 618.048
2024-12-01-17:01:11-root-INFO: grad norm: 1123.156 968.755 568.324
2024-12-01-17:01:11-root-INFO: Loss Change: 9738.975 -> 9209.038
2024-12-01-17:01:11-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-01-17:01:11-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-01-17:01:11-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:01:11-root-INFO: step: 237 lr_xt 0.00023539
2024-12-01-17:01:11-root-INFO: grad norm: 747.669 678.975 313.053
2024-12-01-17:01:12-root-INFO: grad norm: 800.208 701.943 384.199
2024-12-01-17:01:13-root-INFO: grad norm: 897.914 814.343 378.280
2024-12-01-17:01:13-root-INFO: grad norm: 1032.012 895.325 513.267
2024-12-01-17:01:14-root-INFO: grad norm: 1207.950 1077.120 546.769
2024-12-01-17:01:14-root-INFO: grad norm: 1424.491 1228.514 721.061
2024-12-01-17:01:14-root-INFO: grad norm: 1693.926 1491.950 802.168
2024-12-01-17:01:15-root-INFO: Loss too large (8880.871->8892.787)! Learning rate decreased to 0.00019.
2024-12-01-17:01:15-root-INFO: grad norm: 1338.078 1155.057 675.496
2024-12-01-17:01:15-root-INFO: Loss Change: 9122.076 -> 8703.301
2024-12-01-17:01:15-root-INFO: Regularization Change: 0.000 -> 0.812
2024-12-01-17:01:15-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-01-17:01:15-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:01:16-root-INFO: step: 236 lr_xt 0.00024753
2024-12-01-17:01:16-root-INFO: grad norm: 989.489 876.570 459.036
2024-12-01-17:01:16-root-INFO: grad norm: 1110.099 968.250 542.964
2024-12-01-17:01:17-root-INFO: grad norm: 1261.050 1116.026 587.141
2024-12-01-17:01:17-root-INFO: grad norm: 1437.890 1244.520 720.206
2024-12-01-17:01:18-root-INFO: grad norm: 1643.730 1445.631 782.305
2024-12-01-17:01:18-root-INFO: grad norm: 1871.749 1616.120 944.246
2024-12-01-17:01:18-root-INFO: Loss too large (8444.785->8453.918)! Learning rate decreased to 0.00020.
2024-12-01-17:01:19-root-INFO: grad norm: 1375.897 1216.283 643.232
2024-12-01-17:01:19-root-INFO: grad norm: 1036.899 900.966 513.246
2024-12-01-17:01:19-root-INFO: Loss Change: 8562.354 -> 8164.395
2024-12-01-17:01:19-root-INFO: Regularization Change: 0.000 -> 0.702
2024-12-01-17:01:19-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-01-17:01:19-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:01:20-root-INFO: step: 235 lr_xt 0.00026027
2024-12-01-17:01:20-root-INFO: grad norm: 525.696 485.764 200.970
2024-12-01-17:01:20-root-INFO: grad norm: 522.377 471.074 225.760
2024-12-01-17:01:21-root-INFO: grad norm: 532.377 497.791 188.757
2024-12-01-17:01:21-root-INFO: grad norm: 548.619 492.523 241.669
2024-12-01-17:01:22-root-INFO: grad norm: 569.098 529.292 209.098
2024-12-01-17:01:22-root-INFO: grad norm: 593.833 529.339 269.141
2024-12-01-17:01:22-root-INFO: grad norm: 622.816 574.394 240.773
2024-12-01-17:01:23-root-INFO: grad norm: 656.129 580.686 305.466
2024-12-01-17:01:23-root-INFO: Loss Change: 8058.557 -> 7726.953
2024-12-01-17:01:23-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-01-17:01:23-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-01-17:01:23-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-17:01:23-root-INFO: step: 234 lr_xt 0.00027361
2024-12-01-17:01:24-root-INFO: grad norm: 610.809 556.224 252.393
2024-12-01-17:01:24-root-INFO: grad norm: 611.935 544.625 279.012
2024-12-01-17:01:25-root-INFO: grad norm: 625.041 572.824 250.097
2024-12-01-17:01:25-root-INFO: grad norm: 639.761 567.453 295.452
2024-12-01-17:01:26-root-INFO: grad norm: 655.813 599.458 265.974
2024-12-01-17:01:26-root-INFO: grad norm: 672.437 594.451 314.324
2024-12-01-17:01:27-root-INFO: grad norm: 689.234 627.836 284.369
2024-12-01-17:01:27-root-INFO: grad norm: 705.307 622.022 332.485
2024-12-01-17:01:27-root-INFO: Loss Change: 7661.644 -> 7368.772
2024-12-01-17:01:27-root-INFO: Regularization Change: 0.000 -> 0.668
2024-12-01-17:01:27-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-01-17:01:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:01:28-root-INFO: step: 233 lr_xt 0.00028759
2024-12-01-17:01:28-root-INFO: grad norm: 594.569 518.366 291.221
2024-12-01-17:01:28-root-INFO: grad norm: 539.431 476.801 252.282
2024-12-01-17:01:29-root-INFO: grad norm: 518.650 474.493 209.414
2024-12-01-17:01:29-root-INFO: grad norm: 506.796 453.922 225.383
2024-12-01-17:01:30-root-INFO: grad norm: 498.516 460.064 191.988
2024-12-01-17:01:30-root-INFO: grad norm: 491.379 441.412 215.892
2024-12-01-17:01:31-root-INFO: grad norm: 485.486 449.119 184.359
2024-12-01-17:01:31-root-INFO: grad norm: 480.246 431.784 210.235
2024-12-01-17:01:31-root-INFO: Loss Change: 7325.881 -> 7040.268
2024-12-01-17:01:31-root-INFO: Regularization Change: 0.000 -> 0.636
2024-12-01-17:01:31-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-01-17:01:31-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:01:32-root-INFO: step: 232 lr_xt 0.00030224
2024-12-01-17:01:32-root-INFO: grad norm: 477.565 404.747 253.472
2024-12-01-17:01:32-root-INFO: grad norm: 374.905 344.524 147.839
2024-12-01-17:01:33-root-INFO: grad norm: 348.068 317.339 142.994
2024-12-01-17:01:33-root-INFO: grad norm: 335.864 317.006 110.957
2024-12-01-17:01:34-root-INFO: grad norm: 328.447 304.234 123.769
2024-12-01-17:01:34-root-INFO: grad norm: 322.876 305.955 103.152
2024-12-01-17:01:35-root-INFO: grad norm: 318.267 296.219 116.395
2024-12-01-17:01:35-root-INFO: grad norm: 314.243 298.145 99.288
2024-12-01-17:01:35-root-INFO: Loss Change: 6914.834 -> 6666.146
2024-12-01-17:01:35-root-INFO: Regularization Change: 0.000 -> 0.574
2024-12-01-17:01:35-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-01-17:01:35-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:01:36-root-INFO: step: 231 lr_xt 0.00031758
2024-12-01-17:01:36-root-INFO: grad norm: 404.193 365.552 172.465
2024-12-01-17:01:36-root-INFO: grad norm: 386.040 361.001 136.765
2024-12-01-17:01:37-root-INFO: grad norm: 373.379 340.691 152.779
2024-12-01-17:01:37-root-INFO: grad norm: 362.269 340.035 124.961
2024-12-01-17:01:38-root-INFO: grad norm: 352.381 322.233 142.613
2024-12-01-17:01:38-root-INFO: grad norm: 343.068 322.690 116.477
2024-12-01-17:01:39-root-INFO: grad norm: 334.405 306.843 132.942
2024-12-01-17:01:39-root-INFO: grad norm: 326.086 307.265 109.182
2024-12-01-17:01:40-root-INFO: Loss Change: 6614.968 -> 6413.438
2024-12-01-17:01:40-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-01-17:01:40-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-01-17:01:40-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:01:40-root-INFO: step: 230 lr_xt 0.00033364
2024-12-01-17:01:40-root-INFO: grad norm: 516.006 457.291 239.056
2024-12-01-17:01:40-root-INFO: grad norm: 478.775 442.544 182.703
2024-12-01-17:01:41-root-INFO: grad norm: 448.719 403.022 197.287
2024-12-01-17:01:41-root-INFO: grad norm: 422.421 391.073 159.693
2024-12-01-17:01:42-root-INFO: grad norm: 399.096 361.084 169.989
2024-12-01-17:01:42-root-INFO: grad norm: 378.090 350.989 140.566
2024-12-01-17:01:43-root-INFO: grad norm: 359.997 327.574 149.310
2024-12-01-17:01:43-root-INFO: grad norm: 343.997 320.359 125.314
2024-12-01-17:01:44-root-INFO: Loss Change: 6348.309 -> 6156.031
2024-12-01-17:01:44-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-01-17:01:44-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-01-17:01:44-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:01:44-root-INFO: step: 229 lr_xt 0.00035047
2024-12-01-17:01:44-root-INFO: grad norm: 611.971 551.853 264.512
2024-12-01-17:01:44-root-INFO: grad norm: 548.849 504.035 217.221
2024-12-01-17:01:45-root-INFO: grad norm: 506.259 456.824 218.197
2024-12-01-17:01:45-root-INFO: grad norm: 469.744 431.722 185.135
2024-12-01-17:01:46-root-INFO: grad norm: 438.556 397.880 184.454
2024-12-01-17:01:46-root-INFO: grad norm: 411.140 378.623 160.253
2024-12-01-17:01:47-root-INFO: grad norm: 387.953 353.547 159.727
2024-12-01-17:01:47-root-INFO: grad norm: 367.319 339.196 140.959
2024-12-01-17:01:47-root-INFO: Loss Change: 6128.787 -> 5928.250
2024-12-01-17:01:47-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-01-17:01:47-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-01-17:01:47-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:01:48-root-INFO: step: 228 lr_xt 0.00036807
2024-12-01-17:01:48-root-INFO: grad norm: 517.325 457.216 242.031
2024-12-01-17:01:48-root-INFO: grad norm: 446.623 408.458 180.651
2024-12-01-17:01:49-root-INFO: grad norm: 407.138 371.014 167.660
2024-12-01-17:01:49-root-INFO: grad norm: 376.245 347.242 144.855
2024-12-01-17:01:50-root-INFO: grad norm: 350.494 320.914 140.926
2024-12-01-17:01:50-root-INFO: grad norm: 328.607 304.507 123.523
2024-12-01-17:01:51-root-INFO: grad norm: 310.131 285.201 121.826
2024-12-01-17:01:51-root-INFO: grad norm: 294.412 273.859 108.072
2024-12-01-17:01:51-root-INFO: Loss Change: 5904.824 -> 5730.417
2024-12-01-17:01:51-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-01-17:01:51-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-01-17:01:51-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:01:52-root-INFO: step: 227 lr_xt 0.00038651
2024-12-01-17:01:52-root-INFO: grad norm: 659.947 596.000 283.399
2024-12-01-17:01:52-root-INFO: grad norm: 569.944 522.697 227.211
2024-12-01-17:01:53-root-INFO: grad norm: 510.285 465.853 208.258
2024-12-01-17:01:53-root-INFO: grad norm: 461.831 424.266 182.446
2024-12-01-17:01:54-root-INFO: grad norm: 422.256 387.238 168.365
2024-12-01-17:01:54-root-INFO: grad norm: 388.636 357.739 151.858
2024-12-01-17:01:54-root-INFO: grad norm: 360.870 332.302 140.722
2024-12-01-17:01:55-root-INFO: grad norm: 336.888 311.070 129.340
2024-12-01-17:01:55-root-INFO: Loss Change: 5763.750 -> 5555.665
2024-12-01-17:01:55-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-01-17:01:55-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-01-17:01:55-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-17:01:55-root-INFO: step: 226 lr_xt 0.00040579
2024-12-01-17:01:56-root-INFO: grad norm: 385.636 332.741 194.933
2024-12-01-17:01:56-root-INFO: grad norm: 299.323 274.255 119.910
2024-12-01-17:01:57-root-INFO: grad norm: 274.017 252.217 107.108
2024-12-01-17:01:57-root-INFO: grad norm: 257.980 239.940 94.776
2024-12-01-17:01:58-root-INFO: grad norm: 245.986 227.641 93.211
2024-12-01-17:01:58-root-INFO: grad norm: 236.586 220.904 84.702
2024-12-01-17:01:58-root-INFO: grad norm: 229.058 212.915 84.468
2024-12-01-17:01:59-root-INFO: grad norm: 223.026 208.766 78.467
2024-12-01-17:01:59-root-INFO: Loss Change: 5511.942 -> 5350.130
2024-12-01-17:01:59-root-INFO: Regularization Change: 0.000 -> 0.488
2024-12-01-17:01:59-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-01-17:01:59-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:01:59-root-INFO: step: 225 lr_xt 0.00042598
2024-12-01-17:02:00-root-INFO: grad norm: 553.547 503.220 230.617
2024-12-01-17:02:00-root-INFO: grad norm: 472.475 436.535 180.748
2024-12-01-17:02:01-root-INFO: grad norm: 422.456 387.283 168.762
2024-12-01-17:02:01-root-INFO: grad norm: 382.154 353.160 146.013
2024-12-01-17:02:01-root-INFO: grad norm: 349.801 322.220 136.144
2024-12-01-17:02:02-root-INFO: grad norm: 322.723 298.682 122.228
2024-12-01-17:02:02-root-INFO: grad norm: 300.468 277.836 114.404
2024-12-01-17:02:03-root-INFO: grad norm: 281.679 261.258 105.294
2024-12-01-17:02:03-root-INFO: Loss Change: 5330.023 -> 5150.198
2024-12-01-17:02:03-root-INFO: Regularization Change: 0.000 -> 0.520
2024-12-01-17:02:03-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-01-17:02:03-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:02:03-root-INFO: step: 224 lr_xt 0.00044709
2024-12-01-17:02:04-root-INFO: grad norm: 483.501 438.169 204.405
2024-12-01-17:02:04-root-INFO: grad norm: 424.617 392.002 163.201
2024-12-01-17:02:04-root-INFO: grad norm: 390.741 361.011 149.499
2024-12-01-17:02:05-root-INFO: grad norm: 362.214 335.270 137.088
2024-12-01-17:02:05-root-INFO: grad norm: 338.059 313.503 126.491
2024-12-01-17:02:06-root-INFO: grad norm: 317.243 294.193 118.717
2024-12-01-17:02:06-root-INFO: grad norm: 299.830 278.851 110.181
2024-12-01-17:02:07-root-INFO: grad norm: 284.469 264.306 105.190
2024-12-01-17:02:07-root-INFO: Loss Change: 5143.719 -> 4982.730
2024-12-01-17:02:07-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-01-17:02:07-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-01-17:02:07-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:02:07-root-INFO: step: 223 lr_xt 0.00046917
2024-12-01-17:02:07-root-INFO: grad norm: 515.601 474.645 201.386
2024-12-01-17:02:08-root-INFO: grad norm: 467.436 433.195 175.608
2024-12-01-17:02:08-root-INFO: grad norm: 432.951 401.537 161.910
2024-12-01-17:02:09-root-INFO: grad norm: 402.962 373.781 150.553
2024-12-01-17:02:09-root-INFO: grad norm: 377.305 351.070 138.233
2024-12-01-17:02:10-root-INFO: grad norm: 354.463 329.112 131.642
2024-12-01-17:02:10-root-INFO: grad norm: 334.803 312.228 120.857
2024-12-01-17:02:11-root-INFO: grad norm: 317.512 295.226 116.858
2024-12-01-17:02:11-root-INFO: Loss Change: 4973.547 -> 4818.444
2024-12-01-17:02:11-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-01-17:02:11-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-01-17:02:11-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:02:11-root-INFO: step: 222 lr_xt 0.00049227
2024-12-01-17:02:11-root-INFO: grad norm: 372.521 347.038 135.414
2024-12-01-17:02:12-root-INFO: grad norm: 341.143 317.183 125.593
2024-12-01-17:02:12-root-INFO: grad norm: 317.796 297.361 112.117
2024-12-01-17:02:13-root-INFO: grad norm: 297.403 276.918 108.466
2024-12-01-17:02:13-root-INFO: grad norm: 279.602 262.134 97.278
2024-12-01-17:02:14-root-INFO: grad norm: 263.925 246.053 95.469
2024-12-01-17:02:14-root-INFO: grad norm: 250.492 235.225 86.111
2024-12-01-17:02:15-root-INFO: grad norm: 238.913 223.073 85.545
2024-12-01-17:02:15-root-INFO: Loss Change: 4761.310 -> 4632.205
2024-12-01-17:02:15-root-INFO: Regularization Change: 0.000 -> 0.468
2024-12-01-17:02:15-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-01-17:02:15-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:02:15-root-INFO: step: 221 lr_xt 0.00051641
2024-12-01-17:02:15-root-INFO: grad norm: 337.407 313.099 125.748
2024-12-01-17:02:16-root-INFO: grad norm: 301.169 281.372 107.391
2024-12-01-17:02:16-root-INFO: grad norm: 277.540 259.663 98.000
2024-12-01-17:02:17-root-INFO: grad norm: 257.707 241.140 90.909
2024-12-01-17:02:17-root-INFO: grad norm: 240.925 226.073 83.282
2024-12-01-17:02:18-root-INFO: grad norm: 226.816 212.539 79.201
2024-12-01-17:02:18-root-INFO: grad norm: 214.761 202.063 72.752
2024-12-01-17:02:19-root-INFO: grad norm: 204.486 191.917 70.588
2024-12-01-17:02:19-root-INFO: Loss Change: 4599.235 -> 4478.321
2024-12-01-17:02:19-root-INFO: Regularization Change: 0.000 -> 0.461
2024-12-01-17:02:19-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-01-17:02:19-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:02:19-root-INFO: step: 220 lr_xt 0.00054166
2024-12-01-17:02:19-root-INFO: grad norm: 498.863 457.960 197.830
2024-12-01-17:02:20-root-INFO: grad norm: 438.372 409.233 157.155
2024-12-01-17:02:20-root-INFO: grad norm: 398.046 370.578 145.303
2024-12-01-17:02:21-root-INFO: grad norm: 364.070 339.969 130.263
2024-12-01-17:02:21-root-INFO: grad norm: 335.767 314.077 118.724
2024-12-01-17:02:22-root-INFO: grad norm: 311.358 290.925 110.935
2024-12-01-17:02:22-root-INFO: grad norm: 290.067 272.176 100.296
2024-12-01-17:02:23-root-INFO: grad norm: 271.234 253.741 95.829
2024-12-01-17:02:23-root-INFO: Loss Change: 4483.885 -> 4336.988
2024-12-01-17:02:23-root-INFO: Regularization Change: 0.000 -> 0.520
2024-12-01-17:02:23-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-01-17:02:23-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:02:23-root-INFO: step: 219 lr_xt 0.00056804
2024-12-01-17:02:23-root-INFO: grad norm: 272.327 254.975 95.652
2024-12-01-17:02:24-root-INFO: grad norm: 241.698 226.585 84.125
2024-12-01-17:02:24-root-INFO: grad norm: 221.660 208.069 76.422
2024-12-01-17:02:25-root-INFO: grad norm: 205.507 192.866 70.961
2024-12-01-17:02:25-root-INFO: grad norm: 192.282 180.849 65.315
2024-12-01-17:02:26-root-INFO: grad norm: 181.488 170.527 62.117
2024-12-01-17:02:26-root-INFO: grad norm: 172.696 162.831 57.532
2024-12-01-17:02:27-root-INFO: grad norm: 165.504 155.798 55.845
2024-12-01-17:02:27-root-INFO: Loss Change: 4316.238 -> 4208.579
2024-12-01-17:02:27-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-01-17:02:27-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-01-17:02:27-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-17:02:27-root-INFO: step: 218 lr_xt 0.00059561
2024-12-01-17:02:27-root-INFO: grad norm: 325.115 301.893 120.667
2024-12-01-17:02:28-root-INFO: grad norm: 281.786 264.544 97.054
2024-12-01-17:02:28-root-INFO: grad norm: 249.472 234.006 86.473
2024-12-01-17:02:29-root-INFO: grad norm: 223.940 210.627 76.062
2024-12-01-17:02:29-root-INFO: grad norm: 203.517 191.983 67.541
2024-12-01-17:02:30-root-INFO: grad norm: 187.175 176.438 62.482
2024-12-01-17:02:30-root-INFO: grad norm: 174.313 165.105 55.905
2024-12-01-17:02:31-root-INFO: grad norm: 164.135 155.089 53.737
2024-12-01-17:02:31-root-INFO: Loss Change: 4191.367 -> 4081.724
2024-12-01-17:02:31-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-01-17:02:31-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-01-17:02:31-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:02:31-root-INFO: step: 217 lr_xt 0.00062443
2024-12-01-17:02:32-root-INFO: grad norm: 324.059 291.545 141.478
2024-12-01-17:02:32-root-INFO: grad norm: 260.848 244.225 91.630
2024-12-01-17:02:33-root-INFO: grad norm: 228.325 213.483 80.979
2024-12-01-17:02:33-root-INFO: grad norm: 203.723 191.271 70.133
2024-12-01-17:02:34-root-INFO: grad norm: 184.913 174.205 62.012
2024-12-01-17:02:34-root-INFO: grad norm: 170.240 160.218 57.549
2024-12-01-17:02:35-root-INFO: grad norm: 158.894 150.411 51.225
2024-12-01-17:02:35-root-INFO: grad norm: 150.048 141.538 49.814
2024-12-01-17:02:35-root-INFO: Loss Change: 4075.137 -> 3967.831
2024-12-01-17:02:35-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-01-17:02:35-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-01-17:02:35-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:02:35-root-INFO: step: 216 lr_xt 0.00065452
2024-12-01-17:02:36-root-INFO: grad norm: 327.203 298.523 133.961
2024-12-01-17:02:36-root-INFO: grad norm: 268.965 251.665 94.903
2024-12-01-17:02:37-root-INFO: grad norm: 235.001 220.625 80.933
2024-12-01-17:02:37-root-INFO: grad norm: 208.716 195.899 72.015
2024-12-01-17:02:38-root-INFO: grad norm: 188.263 177.886 61.641
2024-12-01-17:02:38-root-INFO: grad norm: 172.100 161.985 58.130
2024-12-01-17:02:39-root-INFO: grad norm: 159.166 151.049 50.180
2024-12-01-17:02:39-root-INFO: grad norm: 148.935 140.539 49.300
2024-12-01-17:02:40-root-INFO: Loss Change: 3957.430 -> 3853.158
2024-12-01-17:02:40-root-INFO: Regularization Change: 0.000 -> 0.466
2024-12-01-17:02:40-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-01-17:02:40-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:02:40-root-INFO: step: 215 lr_xt 0.00068596
2024-12-01-17:02:40-root-INFO: grad norm: 165.355 155.793 55.415
2024-12-01-17:02:40-root-INFO: grad norm: 149.081 140.864 48.809
2024-12-01-17:02:41-root-INFO: grad norm: 138.428 131.331 43.756
2024-12-01-17:02:41-root-INFO: grad norm: 130.798 123.891 41.944
2024-12-01-17:02:42-root-INFO: grad norm: 125.089 118.960 38.678
2024-12-01-17:02:42-root-INFO: grad norm: 120.930 114.699 38.315
2024-12-01-17:02:43-root-INFO: grad norm: 117.701 112.099 35.880
2024-12-01-17:02:43-root-INFO: grad norm: 115.292 109.482 36.139
2024-12-01-17:02:44-root-INFO: Loss Change: 3839.213 -> 3763.620
2024-12-01-17:02:44-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-01-17:02:44-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-01-17:02:44-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:02:44-root-INFO: step: 214 lr_xt 0.00071879
2024-12-01-17:02:44-root-INFO: grad norm: 226.636 206.853 92.607
2024-12-01-17:02:44-root-INFO: grad norm: 181.797 170.626 62.746
2024-12-01-17:02:45-root-INFO: grad norm: 158.271 149.520 51.898
2024-12-01-17:02:45-root-INFO: grad norm: 142.196 134.248 46.874
2024-12-01-17:02:46-root-INFO: grad norm: 130.903 124.659 39.947
2024-12-01-17:02:46-root-INFO: grad norm: 122.838 116.382 39.299
2024-12-01-17:02:47-root-INFO: grad norm: 117.052 111.843 34.531
2024-12-01-17:02:47-root-INFO: grad norm: 112.800 107.155 35.239
2024-12-01-17:02:48-root-INFO: Loss Change: 3741.924 -> 3663.680
2024-12-01-17:02:48-root-INFO: Regularization Change: 0.000 -> 0.405
2024-12-01-17:02:48-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-01-17:02:48-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:02:48-root-INFO: step: 213 lr_xt 0.00075308
2024-12-01-17:02:48-root-INFO: grad norm: 277.668 252.734 115.001
2024-12-01-17:02:49-root-INFO: grad norm: 218.571 204.781 76.408
2024-12-01-17:02:49-root-INFO: grad norm: 186.367 174.194 66.249
2024-12-01-17:02:50-root-INFO: grad norm: 164.038 154.421 55.340
2024-12-01-17:02:50-root-INFO: grad norm: 147.831 139.282 49.543
2024-12-01-17:02:51-root-INFO: grad norm: 135.831 128.223 44.824
2024-12-01-17:02:51-root-INFO: grad norm: 126.743 119.993 40.809
2024-12-01-17:02:52-root-INFO: grad norm: 119.833 113.371 38.821
2024-12-01-17:02:52-root-INFO: Loss Change: 3672.265 -> 3585.620
2024-12-01-17:02:52-root-INFO: Regularization Change: 0.000 -> 0.439
2024-12-01-17:02:52-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-01-17:02:52-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:02:52-root-INFO: step: 212 lr_xt 0.00078886
2024-12-01-17:02:52-root-INFO: grad norm: 120.702 114.806 37.264
2024-12-01-17:02:53-root-INFO: grad norm: 112.858 107.595 34.062
2024-12-01-17:02:54-root-INFO: grad norm: 107.728 102.850 32.050
2024-12-01-17:02:54-root-INFO: grad norm: 104.109 99.321 31.210
2024-12-01-17:02:54-root-INFO: grad norm: 101.542 97.130 29.606
2024-12-01-17:02:55-root-INFO: grad norm: 99.690 95.197 29.590
2024-12-01-17:02:55-root-INFO: grad norm: 98.246 94.075 28.321
2024-12-01-17:02:56-root-INFO: grad norm: 97.119 92.830 28.541
2024-12-01-17:02:56-root-INFO: Loss Change: 3567.806 -> 3506.722
2024-12-01-17:02:56-root-INFO: Regularization Change: 0.000 -> 0.380
2024-12-01-17:02:56-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-01-17:02:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-17:02:56-root-INFO: step: 211 lr_xt 0.00082622
2024-12-01-17:02:56-root-INFO: grad norm: 259.117 232.779 113.823
2024-12-01-17:02:57-root-INFO: grad norm: 179.146 167.027 64.771
2024-12-01-17:02:57-root-INFO: grad norm: 155.666 146.736 51.966
2024-12-01-17:02:58-root-INFO: grad norm: 143.369 135.029 48.185
2024-12-01-17:02:58-root-INFO: grad norm: 135.041 128.251 42.282
2024-12-01-17:02:59-root-INFO: grad norm: 128.756 121.723 41.973
2024-12-01-17:02:59-root-INFO: grad norm: 123.716 117.837 37.685
2024-12-01-17:03:00-root-INFO: grad norm: 119.681 113.366 38.361
2024-12-01-17:03:00-root-INFO: Loss Change: 3498.307 -> 3414.926
2024-12-01-17:03:00-root-INFO: Regularization Change: 0.000 -> 0.455
2024-12-01-17:03:00-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-01-17:03:00-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:03:00-root-INFO: step: 210 lr_xt 0.00086520
2024-12-01-17:03:00-root-INFO: grad norm: 184.708 173.398 63.641
2024-12-01-17:03:01-root-INFO: grad norm: 165.209 157.502 49.871
2024-12-01-17:03:01-root-INFO: grad norm: 152.812 144.696 49.140
2024-12-01-17:03:02-root-INFO: grad norm: 143.417 136.703 43.367
2024-12-01-17:03:02-root-INFO: grad norm: 135.923 129.014 42.782
2024-12-01-17:03:03-root-INFO: grad norm: 130.107 124.009 39.366
2024-12-01-17:03:03-root-INFO: grad norm: 125.403 119.312 38.610
2024-12-01-17:03:04-root-INFO: grad norm: 121.727 116.046 36.754
2024-12-01-17:03:04-root-INFO: Loss Change: 3409.123 -> 3346.238
2024-12-01-17:03:04-root-INFO: Regularization Change: 0.000 -> 0.404
2024-12-01-17:03:04-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-01-17:03:04-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:03:04-root-INFO: step: 209 lr_xt 0.00090588
2024-12-01-17:03:04-root-INFO: grad norm: 203.846 186.426 82.454
2024-12-01-17:03:05-root-INFO: grad norm: 168.948 159.679 55.192
2024-12-01-17:03:05-root-INFO: grad norm: 162.613 153.551 53.526
2024-12-01-17:03:06-root-INFO: grad norm: 162.018 154.225 49.643
2024-12-01-17:03:06-root-INFO: grad norm: 163.970 155.384 52.362
2024-12-01-17:03:07-root-INFO: grad norm: 167.494 159.488 51.166
2024-12-01-17:03:08-root-INFO: grad norm: 172.606 163.805 54.411
2024-12-01-17:03:08-root-INFO: grad norm: 179.071 170.341 55.230
2024-12-01-17:03:08-root-INFO: Loss Change: 3332.351 -> 3267.626
2024-12-01-17:03:08-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-01-17:03:08-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-01-17:03:08-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:03:09-root-INFO: step: 208 lr_xt 0.00094831
2024-12-01-17:03:09-root-INFO: grad norm: 264.525 246.706 95.445
2024-12-01-17:03:09-root-INFO: grad norm: 247.797 235.307 77.679
2024-12-01-17:03:10-root-INFO: grad norm: 258.766 245.189 82.720
2024-12-01-17:03:10-root-INFO: grad norm: 276.744 263.331 85.114
2024-12-01-17:03:11-root-INFO: grad norm: 300.352 284.929 95.012
2024-12-01-17:03:11-root-INFO: grad norm: 325.682 309.420 101.627
2024-12-01-17:03:12-root-INFO: grad norm: 354.725 336.804 111.323
2024-12-01-17:03:12-root-INFO: grad norm: 382.744 363.051 121.191
2024-12-01-17:03:13-root-INFO: Loss Change: 3272.084 -> 3229.863
2024-12-01-17:03:13-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-01-17:03:13-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-01-17:03:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:03:13-root-INFO: step: 207 lr_xt 0.00100094
2024-12-01-17:03:13-root-INFO: grad norm: 436.723 413.051 141.829
2024-12-01-17:03:13-root-INFO: grad norm: 444.024 422.494 136.588
2024-12-01-17:03:14-root-INFO: grad norm: 458.627 435.940 142.460
2024-12-01-17:03:14-root-INFO: grad norm: 468.590 445.701 144.662
2024-12-01-17:03:15-root-INFO: grad norm: 473.665 450.290 146.962
2024-12-01-17:03:15-root-INFO: grad norm: 473.079 449.565 147.293
2024-12-01-17:03:16-root-INFO: grad norm: 465.959 442.873 144.848
2024-12-01-17:03:16-root-INFO: grad norm: 455.952 433.044 142.705
2024-12-01-17:03:17-root-INFO: Loss Change: 3219.575 -> 3162.984
2024-12-01-17:03:17-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-01-17:03:17-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-01-17:03:17-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:03:17-root-INFO: step: 206 lr_xt 0.00104745
2024-12-01-17:03:17-root-INFO: grad norm: 467.419 443.693 147.028
2024-12-01-17:03:17-root-INFO: grad norm: 443.551 422.290 135.681
2024-12-01-17:03:18-root-INFO: grad norm: 427.768 407.085 131.404
2024-12-01-17:03:18-root-INFO: grad norm: 415.410 395.612 126.713
2024-12-01-17:03:19-root-INFO: grad norm: 403.056 383.325 124.561
2024-12-01-17:03:19-root-INFO: grad norm: 393.573 374.506 121.017
2024-12-01-17:03:20-root-INFO: grad norm: 384.603 365.684 119.142
2024-12-01-17:03:20-root-INFO: grad norm: 378.313 359.692 117.229
2024-12-01-17:03:20-root-INFO: Loss Change: 3151.515 -> 3068.977
2024-12-01-17:03:20-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-01-17:03:20-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-17:03:20-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:03:21-root-INFO: step: 205 lr_xt 0.00109594
2024-12-01-17:03:21-root-INFO: grad norm: 416.616 395.161 131.975
2024-12-01-17:03:21-root-INFO: grad norm: 401.813 382.460 123.200
2024-12-01-17:03:22-root-INFO: grad norm: 402.310 383.107 122.812
2024-12-01-17:03:22-root-INFO: grad norm: 405.700 386.362 123.760
2024-12-01-17:03:23-root-INFO: grad norm: 409.424 389.740 125.420
2024-12-01-17:03:23-root-INFO: grad norm: 412.781 392.733 127.081
2024-12-01-17:03:24-root-INFO: grad norm: 414.620 394.642 127.150
2024-12-01-17:03:24-root-INFO: grad norm: 415.350 394.829 128.943
2024-12-01-17:03:25-root-INFO: Loss Change: 3067.224 -> 3001.837
2024-12-01-17:03:25-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-01-17:03:25-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-17:03:25-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-17:03:25-root-INFO: step: 204 lr_xt 0.00114648
2024-12-01-17:03:25-root-INFO: grad norm: 434.375 413.278 133.727
2024-12-01-17:03:25-root-INFO: grad norm: 418.736 398.625 128.210
2024-12-01-17:03:26-root-INFO: grad norm: 422.762 402.607 128.980
2024-12-01-17:03:26-root-INFO: grad norm: 430.421 409.784 131.680
2024-12-01-17:03:27-root-INFO: grad norm: 437.940 416.923 134.038
2024-12-01-17:03:27-root-INFO: grad norm: 443.412 421.739 136.931
2024-12-01-17:03:28-root-INFO: grad norm: 445.309 423.954 136.247
2024-12-01-17:03:28-root-INFO: grad norm: 444.121 422.044 138.286
2024-12-01-17:03:29-root-INFO: Loss Change: 3003.525 -> 2934.596
2024-12-01-17:03:29-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-01-17:03:29-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-17:03:29-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:03:29-root-INFO: step: 203 lr_xt 0.00119917
2024-12-01-17:03:29-root-INFO: grad norm: 426.615 406.587 129.177
2024-12-01-17:03:30-root-INFO: grad norm: 407.280 387.889 124.172
2024-12-01-17:03:30-root-INFO: grad norm: 391.513 373.830 116.334
2024-12-01-17:03:31-root-INFO: grad norm: 380.861 362.651 116.359
2024-12-01-17:03:31-root-INFO: grad norm: 373.025 356.166 110.876
2024-12-01-17:03:32-root-INFO: grad norm: 369.820 351.886 113.769
2024-12-01-17:03:32-root-INFO: grad norm: 369.598 352.903 109.827
2024-12-01-17:03:33-root-INFO: grad norm: 372.772 354.453 115.421
2024-12-01-17:03:33-root-INFO: Loss Change: 2921.612 -> 2842.412
2024-12-01-17:03:33-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-01-17:03:33-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-01-17:03:33-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:03:33-root-INFO: step: 202 lr_xt 0.00125407
2024-12-01-17:03:33-root-INFO: grad norm: 387.989 369.647 117.884
2024-12-01-17:03:34-root-INFO: grad norm: 380.657 362.120 117.341
2024-12-01-17:03:34-root-INFO: grad norm: 415.281 397.626 119.799
2024-12-01-17:03:34-root-INFO: Loss too large (2806.565->2810.168)! Learning rate decreased to 0.00100.
2024-12-01-17:03:35-root-INFO: grad norm: 300.191 285.906 91.500
2024-12-01-17:03:35-root-INFO: grad norm: 226.532 216.564 66.457
2024-12-01-17:03:36-root-INFO: grad norm: 182.283 173.643 55.453
2024-12-01-17:03:36-root-INFO: grad norm: 154.305 147.855 44.146
2024-12-01-17:03:37-root-INFO: grad norm: 136.816 130.438 41.285
2024-12-01-17:03:37-root-INFO: Loss Change: 2832.789 -> 2714.301
2024-12-01-17:03:37-root-INFO: Regularization Change: 0.000 -> 0.629
2024-12-01-17:03:37-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-01-17:03:37-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:03:37-root-INFO: step: 201 lr_xt 0.00131127
2024-12-01-17:03:38-root-INFO: grad norm: 165.876 157.527 51.962
2024-12-01-17:03:38-root-INFO: grad norm: 202.133 193.299 59.103
2024-12-01-17:03:39-root-INFO: grad norm: 263.752 252.465 76.332
2024-12-01-17:03:39-root-INFO: Loss too large (2705.894->2712.518)! Learning rate decreased to 0.00105.
2024-12-01-17:03:39-root-INFO: grad norm: 243.613 232.672 72.188
2024-12-01-17:03:40-root-INFO: grad norm: 229.703 220.175 65.470
2024-12-01-17:03:40-root-INFO: grad norm: 221.601 211.505 66.125
2024-12-01-17:03:41-root-INFO: grad norm: 218.052 209.203 61.488
2024-12-01-17:03:41-root-INFO: grad norm: 218.783 208.684 65.705
2024-12-01-17:03:41-root-INFO: Loss Change: 2714.952 -> 2655.435
2024-12-01-17:03:41-root-INFO: Regularization Change: 0.000 -> 0.568
2024-12-01-17:03:41-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-01-17:03:41-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:03:42-root-INFO: step: 200 lr_xt 0.00137086
2024-12-01-17:03:42-root-INFO: grad norm: 191.756 181.835 60.879
2024-12-01-17:03:42-root-INFO: grad norm: 241.947 230.886 72.320
2024-12-01-17:03:42-root-INFO: Loss too large (2627.040->2639.430)! Learning rate decreased to 0.00110.
2024-12-01-17:03:43-root-INFO: grad norm: 271.104 260.825 73.945
2024-12-01-17:03:43-root-INFO: grad norm: 312.060 298.212 91.930
2024-12-01-17:03:43-root-INFO: Loss too large (2620.355->2621.348)! Learning rate decreased to 0.00088.
2024-12-01-17:03:44-root-INFO: grad norm: 239.555 230.547 65.074
2024-12-01-17:03:44-root-INFO: grad norm: 192.411 183.687 57.280
2024-12-01-17:03:45-root-INFO: grad norm: 160.520 154.668 42.950
2024-12-01-17:03:45-root-INFO: grad norm: 139.371 132.981 41.718
2024-12-01-17:03:46-root-INFO: Loss Change: 2636.513 -> 2569.000
2024-12-01-17:03:46-root-INFO: Regularization Change: 0.000 -> 0.470
2024-12-01-17:03:46-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-17:03:46-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:03:46-root-INFO: step: 199 lr_xt 0.00143293
2024-12-01-17:03:46-root-INFO: grad norm: 131.894 124.595 43.268
2024-12-01-17:03:46-root-INFO: grad norm: 170.388 163.280 48.701
2024-12-01-17:03:47-root-INFO: Loss too large (2540.503->2545.177)! Learning rate decreased to 0.00115.
2024-12-01-17:03:47-root-INFO: grad norm: 217.863 210.129 57.536
2024-12-01-17:03:47-root-INFO: Loss too large (2536.729->2538.284)! Learning rate decreased to 0.00092.
2024-12-01-17:03:48-root-INFO: grad norm: 202.120 193.748 57.566
2024-12-01-17:03:48-root-INFO: grad norm: 191.041 184.384 49.991
2024-12-01-17:03:49-root-INFO: grad norm: 183.978 176.339 52.462
2024-12-01-17:03:49-root-INFO: grad norm: 180.008 173.844 46.705
2024-12-01-17:03:50-root-INFO: grad norm: 178.831 171.380 51.080
2024-12-01-17:03:50-root-INFO: Loss Change: 2550.159 -> 2497.313
2024-12-01-17:03:50-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-01-17:03:50-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-01-17:03:50-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-17:03:50-root-INFO: step: 198 lr_xt 0.00149757
2024-12-01-17:03:51-root-INFO: grad norm: 188.141 178.753 58.687
2024-12-01-17:03:51-root-INFO: Loss too large (2491.138->2495.095)! Learning rate decreased to 0.00120.
2024-12-01-17:03:51-root-INFO: grad norm: 246.224 236.730 67.715
2024-12-01-17:03:51-root-INFO: Loss too large (2484.960->2496.138)! Learning rate decreased to 0.00096.
2024-12-01-17:03:52-root-INFO: grad norm: 266.671 257.108 70.774
2024-12-01-17:03:52-root-INFO: grad norm: 294.260 283.330 79.453
2024-12-01-17:03:53-root-INFO: grad norm: 328.213 317.003 85.049
2024-12-01-17:03:53-root-INFO: Loss too large (2476.479->2477.226)! Learning rate decreased to 0.00077.
2024-12-01-17:03:54-root-INFO: grad norm: 239.186 230.208 64.917
2024-12-01-17:03:54-root-INFO: grad norm: 180.918 174.652 47.203
2024-12-01-17:03:55-root-INFO: grad norm: 143.796 138.165 39.847
2024-12-01-17:03:55-root-INFO: Loss Change: 2491.138 -> 2437.032
2024-12-01-17:03:55-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-01-17:03:55-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-01-17:03:55-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:03:55-root-INFO: step: 197 lr_xt 0.00156486
2024-12-01-17:03:55-root-INFO: grad norm: 102.807 96.666 35.001
2024-12-01-17:03:56-root-INFO: grad norm: 117.936 112.953 33.919
2024-12-01-17:03:56-root-INFO: Loss too large (2407.522->2407.941)! Learning rate decreased to 0.00125.
2024-12-01-17:03:57-root-INFO: grad norm: 183.353 177.974 44.085
2024-12-01-17:03:57-root-INFO: Loss too large (2404.252->2413.216)! Learning rate decreased to 0.00100.
2024-12-01-17:03:57-root-INFO: grad norm: 234.697 226.299 62.221
2024-12-01-17:03:57-root-INFO: Loss too large (2403.267->2406.228)! Learning rate decreased to 0.00080.
2024-12-01-17:03:58-root-INFO: grad norm: 209.015 202.934 50.050
2024-12-01-17:03:58-root-INFO: grad norm: 189.649 182.745 50.705
2024-12-01-17:03:59-root-INFO: grad norm: 174.651 169.545 41.923
2024-12-01-17:03:59-root-INFO: grad norm: 163.455 157.420 44.006
2024-12-01-17:04:00-root-INFO: Loss Change: 2418.449 -> 2374.832
2024-12-01-17:04:00-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-01-17:04:00-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-01-17:04:00-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:04:00-root-INFO: step: 196 lr_xt 0.00163492
2024-12-01-17:04:00-root-INFO: grad norm: 165.375 156.579 53.218
2024-12-01-17:04:00-root-INFO: Loss too large (2365.535->2370.242)! Learning rate decreased to 0.00131.
2024-12-01-17:04:01-root-INFO: grad norm: 255.242 247.440 62.626
2024-12-01-17:04:01-root-INFO: Loss too large (2361.171->2397.606)! Learning rate decreased to 0.00105.
2024-12-01-17:04:01-root-INFO: Loss too large (2361.171->2370.889)! Learning rate decreased to 0.00084.
2024-12-01-17:04:02-root-INFO: grad norm: 258.600 250.531 64.095
2024-12-01-17:04:02-root-INFO: grad norm: 265.842 257.827 64.784
2024-12-01-17:04:03-root-INFO: grad norm: 276.164 267.958 66.821
2024-12-01-17:04:04-root-INFO: grad norm: 288.832 280.146 70.303
2024-12-01-17:04:04-root-INFO: grad norm: 303.802 294.992 72.630
2024-12-01-17:04:05-root-INFO: grad norm: 320.052 310.411 77.963
2024-12-01-17:04:05-root-INFO: Loss Change: 2365.535 -> 2340.500
2024-12-01-17:04:05-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-01-17:04:05-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-01-17:04:05-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:04:05-root-INFO: step: 195 lr_xt 0.00170783
2024-12-01-17:04:05-root-INFO: grad norm: 325.751 316.029 78.989
2024-12-01-17:04:05-root-INFO: Loss too large (2329.612->2476.086)! Learning rate decreased to 0.00137.
2024-12-01-17:04:06-root-INFO: Loss too large (2329.612->2398.547)! Learning rate decreased to 0.00109.
2024-12-01-17:04:06-root-INFO: Loss too large (2329.612->2351.879)! Learning rate decreased to 0.00087.
2024-12-01-17:04:06-root-INFO: grad norm: 330.424 321.147 77.744
2024-12-01-17:04:07-root-INFO: grad norm: 339.484 330.192 78.885
2024-12-01-17:04:07-root-INFO: grad norm: 351.389 341.597 82.378
2024-12-01-17:04:08-root-INFO: grad norm: 365.022 355.349 83.476
2024-12-01-17:04:08-root-INFO: grad norm: 378.948 368.338 89.045
2024-12-01-17:04:09-root-INFO: grad norm: 392.643 382.413 89.043
2024-12-01-17:04:09-root-INFO: grad norm: 404.654 393.287 95.237
2024-12-01-17:04:10-root-INFO: Loss Change: 2329.612 -> 2311.053
2024-12-01-17:04:10-root-INFO: Regularization Change: 0.000 -> 0.228
2024-12-01-17:04:10-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-01-17:04:10-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:04:10-root-INFO: step: 194 lr_xt 0.00178371
2024-12-01-17:04:10-root-INFO: grad norm: 401.394 390.921 91.096
2024-12-01-17:04:10-root-INFO: Loss too large (2300.946->2534.353)! Learning rate decreased to 0.00143.
2024-12-01-17:04:10-root-INFO: Loss too large (2300.946->2414.618)! Learning rate decreased to 0.00114.
2024-12-01-17:04:10-root-INFO: Loss too large (2300.946->2340.360)! Learning rate decreased to 0.00091.
2024-12-01-17:04:11-root-INFO: grad norm: 415.514 404.397 95.474
2024-12-01-17:04:11-root-INFO: grad norm: 436.002 425.183 96.525
2024-12-01-17:04:12-root-INFO: Loss too large (2295.290->2296.962)! Learning rate decreased to 0.00073.
2024-12-01-17:04:12-root-INFO: grad norm: 293.448 285.597 67.426
2024-12-01-17:04:13-root-INFO: grad norm: 206.006 200.415 47.670
2024-12-01-17:04:13-root-INFO: grad norm: 154.140 149.579 37.219
2024-12-01-17:04:14-root-INFO: grad norm: 121.599 117.868 29.889
2024-12-01-17:04:14-root-INFO: grad norm: 101.503 98.080 26.138
2024-12-01-17:04:15-root-INFO: Loss Change: 2300.946 -> 2234.440
2024-12-01-17:04:15-root-INFO: Regularization Change: 0.000 -> 0.211
2024-12-01-17:04:15-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-01-17:04:15-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:04:15-root-INFO: step: 193 lr_xt 0.00186266
2024-12-01-17:04:15-root-INFO: grad norm: 107.294 99.331 40.563
2024-12-01-17:04:15-root-INFO: grad norm: 78.459 74.841 23.552
2024-12-01-17:04:16-root-INFO: grad norm: 142.649 138.666 33.471
2024-12-01-17:04:16-root-INFO: Loss too large (2206.174->2238.495)! Learning rate decreased to 0.00149.
2024-12-01-17:04:16-root-INFO: Loss too large (2206.174->2220.823)! Learning rate decreased to 0.00119.
2024-12-01-17:04:17-root-INFO: Loss too large (2206.174->2210.665)! Learning rate decreased to 0.00095.
2024-12-01-17:04:17-root-INFO: grad norm: 188.263 183.422 42.416
2024-12-01-17:04:17-root-INFO: Loss too large (2205.129->2207.307)! Learning rate decreased to 0.00076.
2024-12-01-17:04:18-root-INFO: grad norm: 175.803 171.305 39.516
2024-12-01-17:04:18-root-INFO: grad norm: 166.072 161.663 38.014
2024-12-01-17:04:19-root-INFO: grad norm: 158.813 154.668 36.048
2024-12-01-17:04:19-root-INFO: grad norm: 153.495 149.342 35.464
2024-12-01-17:04:20-root-INFO: Loss Change: 2225.433 -> 2187.189
2024-12-01-17:04:20-root-INFO: Regularization Change: 0.000 -> 0.330
2024-12-01-17:04:20-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-01-17:04:20-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-17:04:20-root-INFO: step: 192 lr_xt 0.00194479
2024-12-01-17:04:20-root-INFO: grad norm: 223.697 217.890 50.639
2024-12-01-17:04:20-root-INFO: Loss too large (2177.785->2285.211)! Learning rate decreased to 0.00156.
2024-12-01-17:04:20-root-INFO: Loss too large (2177.785->2232.092)! Learning rate decreased to 0.00124.
2024-12-01-17:04:20-root-INFO: Loss too large (2177.785->2200.211)! Learning rate decreased to 0.00100.
2024-12-01-17:04:20-root-INFO: Loss too large (2177.785->2182.042)! Learning rate decreased to 0.00080.
2024-12-01-17:04:21-root-INFO: grad norm: 214.365 209.311 46.277
2024-12-01-17:04:21-root-INFO: grad norm: 211.506 206.449 45.972
2024-12-01-17:04:22-root-INFO: grad norm: 212.094 207.286 44.901
2024-12-01-17:04:22-root-INFO: grad norm: 215.655 210.613 46.362
2024-12-01-17:04:23-root-INFO: grad norm: 221.754 216.824 46.500
2024-12-01-17:04:23-root-INFO: grad norm: 230.106 224.818 49.044
2024-12-01-17:04:24-root-INFO: grad norm: 240.888 235.624 50.084
2024-12-01-17:04:24-root-INFO: Loss Change: 2177.785 -> 2153.813
2024-12-01-17:04:24-root-INFO: Regularization Change: 0.000 -> 0.164
2024-12-01-17:04:24-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-01-17:04:24-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-17:04:24-root-INFO: step: 191 lr_xt 0.00203021
2024-12-01-17:04:25-root-INFO: grad norm: 343.086 333.601 80.115
2024-12-01-17:04:25-root-INFO: Loss too large (2159.920->2427.317)! Learning rate decreased to 0.00162.
2024-12-01-17:04:25-root-INFO: Loss too large (2159.920->2305.217)! Learning rate decreased to 0.00130.
2024-12-01-17:04:25-root-INFO: Loss too large (2159.920->2226.451)! Learning rate decreased to 0.00104.
2024-12-01-17:04:25-root-INFO: Loss too large (2159.920->2178.999)! Learning rate decreased to 0.00083.
2024-12-01-17:04:26-root-INFO: grad norm: 360.919 353.332 73.613
2024-12-01-17:04:26-root-INFO: Loss too large (2152.528->2154.432)! Learning rate decreased to 0.00067.
2024-12-01-17:04:26-root-INFO: grad norm: 259.235 253.358 54.887
2024-12-01-17:04:27-root-INFO: grad norm: 191.222 186.259 43.281
2024-12-01-17:04:27-root-INFO: grad norm: 147.541 143.777 33.113
2024-12-01-17:04:28-root-INFO: grad norm: 118.003 114.132 29.974
2024-12-01-17:04:28-root-INFO: grad norm: 98.503 95.452 24.323
2024-12-01-17:04:29-root-INFO: grad norm: 85.408 81.972 23.979
2024-12-01-17:04:29-root-INFO: Loss Change: 2159.920 -> 2107.777
2024-12-01-17:04:29-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-01-17:04:29-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-01-17:04:29-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-17:04:29-root-INFO: step: 190 lr_xt 0.00211904
2024-12-01-17:04:30-root-INFO: grad norm: 164.471 158.836 42.686
2024-12-01-17:04:30-root-INFO: Loss too large (2105.281->2157.409)! Learning rate decreased to 0.00170.
2024-12-01-17:04:30-root-INFO: Loss too large (2105.281->2130.017)! Learning rate decreased to 0.00136.
2024-12-01-17:04:30-root-INFO: Loss too large (2105.281->2113.924)! Learning rate decreased to 0.00108.
2024-12-01-17:04:31-root-INFO: grad norm: 251.030 245.665 51.622
2024-12-01-17:04:31-root-INFO: Loss too large (2104.964->2126.946)! Learning rate decreased to 0.00087.
2024-12-01-17:04:31-root-INFO: Loss too large (2104.964->2108.336)! Learning rate decreased to 0.00069.
2024-12-01-17:04:31-root-INFO: grad norm: 208.792 204.641 41.427
2024-12-01-17:04:32-root-INFO: grad norm: 176.174 171.923 38.470
2024-12-01-17:04:32-root-INFO: grad norm: 151.908 148.653 31.279
2024-12-01-17:04:33-root-INFO: grad norm: 132.982 129.331 30.943
2024-12-01-17:04:33-root-INFO: grad norm: 118.571 115.732 25.790
2024-12-01-17:04:34-root-INFO: grad norm: 107.339 103.991 26.597
2024-12-01-17:04:34-root-INFO: Loss Change: 2105.281 -> 2076.414
2024-12-01-17:04:34-root-INFO: Regularization Change: 0.000 -> 0.148
2024-12-01-17:04:34-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-01-17:04:34-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-17:04:34-root-INFO: step: 189 lr_xt 0.00221139
2024-12-01-17:04:34-root-INFO: grad norm: 159.440 154.601 38.982
2024-12-01-17:04:35-root-INFO: Loss too large (2071.004->2132.859)! Learning rate decreased to 0.00177.
2024-12-01-17:04:35-root-INFO: Loss too large (2071.004->2102.290)! Learning rate decreased to 0.00142.
2024-12-01-17:04:35-root-INFO: Loss too large (2071.004->2084.021)! Learning rate decreased to 0.00113.
2024-12-01-17:04:35-root-INFO: Loss too large (2071.004->2073.608)! Learning rate decreased to 0.00091.
2024-12-01-17:04:35-root-INFO: grad norm: 196.608 191.805 43.194
2024-12-01-17:04:36-root-INFO: Loss too large (2068.051->2070.625)! Learning rate decreased to 0.00072.
2024-12-01-17:04:36-root-INFO: grad norm: 182.938 179.464 35.483
2024-12-01-17:04:37-root-INFO: grad norm: 172.099 167.917 37.709
2024-12-01-17:04:37-root-INFO: grad norm: 163.882 160.801 31.628
2024-12-01-17:04:38-root-INFO: grad norm: 157.354 153.480 34.704
2024-12-01-17:04:38-root-INFO: grad norm: 152.358 149.451 29.618
2024-12-01-17:04:39-root-INFO: grad norm: 148.514 144.826 32.893
2024-12-01-17:04:39-root-INFO: Loss Change: 2071.004 -> 2046.426
2024-12-01-17:04:39-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-01-17:04:39-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-17:04:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-17:04:39-root-INFO: step: 188 lr_xt 0.00230740
2024-12-01-17:04:39-root-INFO: grad norm: 278.211 273.024 53.475
2024-12-01-17:04:39-root-INFO: Loss too large (2046.994->2315.220)! Learning rate decreased to 0.00185.
2024-12-01-17:04:40-root-INFO: Loss too large (2046.994->2202.222)! Learning rate decreased to 0.00148.
2024-12-01-17:04:40-root-INFO: Loss too large (2046.994->2127.758)! Learning rate decreased to 0.00118.
2024-12-01-17:04:40-root-INFO: Loss too large (2046.994->2081.582)! Learning rate decreased to 0.00095.
2024-12-01-17:04:40-root-INFO: Loss too large (2046.994->2054.650)! Learning rate decreased to 0.00076.
2024-12-01-17:04:41-root-INFO: grad norm: 261.657 257.020 49.043
2024-12-01-17:04:41-root-INFO: grad norm: 255.009 250.750 46.414
2024-12-01-17:04:42-root-INFO: grad norm: 250.233 245.809 46.844
2024-12-01-17:04:42-root-INFO: grad norm: 247.410 243.405 44.336
2024-12-01-17:04:42-root-INFO: grad norm: 245.532 241.166 46.097
2024-12-01-17:04:43-root-INFO: grad norm: 244.777 240.857 43.629
2024-12-01-17:04:43-root-INFO: grad norm: 244.744 240.398 45.915
2024-12-01-17:04:44-root-INFO: Loss Change: 2046.994 -> 2019.746
2024-12-01-17:04:44-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-01-17:04:44-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-17:04:44-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-17:04:44-root-INFO: step: 187 lr_xt 0.00240719
2024-12-01-17:04:44-root-INFO: grad norm: 283.554 278.285 54.411
2024-12-01-17:04:44-root-INFO: Loss too large (2025.135->2324.013)! Learning rate decreased to 0.00193.
2024-12-01-17:04:44-root-INFO: Loss too large (2025.135->2201.606)! Learning rate decreased to 0.00154.
2024-12-01-17:04:45-root-INFO: Loss too large (2025.135->2119.295)! Learning rate decreased to 0.00123.
2024-12-01-17:04:45-root-INFO: Loss too large (2025.135->2067.404)! Learning rate decreased to 0.00099.
2024-12-01-17:04:45-root-INFO: Loss too large (2025.135->2036.682)! Learning rate decreased to 0.00079.
2024-12-01-17:04:45-root-INFO: grad norm: 283.614 278.340 54.442
2024-12-01-17:04:46-root-INFO: grad norm: 290.863 286.608 49.568
2024-12-01-17:04:46-root-INFO: grad norm: 299.240 294.058 55.447
2024-12-01-17:04:47-root-INFO: grad norm: 308.163 303.896 51.103
2024-12-01-17:04:47-root-INFO: grad norm: 316.651 311.363 57.627
2024-12-01-17:04:48-root-INFO: grad norm: 324.438 319.986 53.566
2024-12-01-17:04:48-root-INFO: grad norm: 331.130 325.738 59.514
2024-12-01-17:04:49-root-INFO: Loss Change: 2025.135 -> 2007.444
2024-12-01-17:04:49-root-INFO: Regularization Change: 0.000 -> 0.151
2024-12-01-17:04:49-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-17:04:49-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:04:49-root-INFO: step: 186 lr_xt 0.00251089
2024-12-01-17:04:49-root-INFO: grad norm: 436.134 430.070 72.475
2024-12-01-17:04:49-root-INFO: Loss too large (2014.220->2638.024)! Learning rate decreased to 0.00201.
2024-12-01-17:04:49-root-INFO: Loss too large (2014.220->2414.626)! Learning rate decreased to 0.00161.
2024-12-01-17:04:49-root-INFO: Loss too large (2014.220->2243.801)! Learning rate decreased to 0.00129.
2024-12-01-17:04:49-root-INFO: Loss too large (2014.220->2124.698)! Learning rate decreased to 0.00103.
2024-12-01-17:04:50-root-INFO: Loss too large (2014.220->2049.006)! Learning rate decreased to 0.00082.
2024-12-01-17:04:50-root-INFO: grad norm: 417.691 411.973 68.877
2024-12-01-17:04:51-root-INFO: grad norm: 403.964 398.519 66.100
2024-12-01-17:04:51-root-INFO: grad norm: 387.912 382.499 64.578
2024-12-01-17:04:52-root-INFO: grad norm: 372.834 367.843 60.801
2024-12-01-17:04:52-root-INFO: grad norm: 357.311 352.236 60.004
2024-12-01-17:04:53-root-INFO: grad norm: 343.326 338.721 56.047
2024-12-01-17:04:53-root-INFO: grad norm: 330.055 325.289 55.885
2024-12-01-17:04:53-root-INFO: Loss Change: 2014.220 -> 1972.176
2024-12-01-17:04:53-root-INFO: Regularization Change: 0.000 -> 0.152
2024-12-01-17:04:53-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-17:04:53-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:04:54-root-INFO: step: 185 lr_xt 0.00261863
2024-12-01-17:04:54-root-INFO: grad norm: 369.260 364.708 57.802
2024-12-01-17:04:54-root-INFO: Loss too large (1970.275->2466.884)! Learning rate decreased to 0.00209.
2024-12-01-17:04:54-root-INFO: Loss too large (1970.275->2280.368)! Learning rate decreased to 0.00168.
2024-12-01-17:04:54-root-INFO: Loss too large (1970.275->2143.436)! Learning rate decreased to 0.00134.
2024-12-01-17:04:54-root-INFO: Loss too large (1970.275->2051.343)! Learning rate decreased to 0.00107.
2024-12-01-17:04:55-root-INFO: Loss too large (1970.275->1994.430)! Learning rate decreased to 0.00086.
2024-12-01-17:04:55-root-INFO: grad norm: 345.852 340.985 57.821
2024-12-01-17:04:55-root-INFO: grad norm: 329.439 325.405 51.396
2024-12-01-17:04:56-root-INFO: grad norm: 314.906 310.437 52.867
2024-12-01-17:04:56-root-INFO: grad norm: 303.741 300.006 47.484
2024-12-01-17:04:57-root-INFO: grad norm: 294.297 290.074 49.674
2024-12-01-17:04:57-root-INFO: grad norm: 287.087 283.537 45.013
2024-12-01-17:04:58-root-INFO: grad norm: 281.272 277.204 47.663
2024-12-01-17:04:58-root-INFO: Loss Change: 1970.275 -> 1934.594
2024-12-01-17:04:58-root-INFO: Regularization Change: 0.000 -> 0.146
2024-12-01-17:04:58-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-01-17:04:58-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:04:58-root-INFO: step: 184 lr_xt 0.00273055
2024-12-01-17:04:58-root-INFO: grad norm: 415.351 409.069 71.963
2024-12-01-17:04:59-root-INFO: Loss too large (1956.918->2520.852)! Learning rate decreased to 0.00218.
2024-12-01-17:04:59-root-INFO: Loss too large (1956.918->2318.619)! Learning rate decreased to 0.00175.
2024-12-01-17:04:59-root-INFO: Loss too large (1956.918->2162.481)! Learning rate decreased to 0.00140.
2024-12-01-17:04:59-root-INFO: Loss too large (1956.918->2052.885)! Learning rate decreased to 0.00112.
2024-12-01-17:04:59-root-INFO: Loss too large (1956.918->1983.209)! Learning rate decreased to 0.00089.
2024-12-01-17:05:00-root-INFO: grad norm: 385.344 380.590 60.340
2024-12-01-17:05:00-root-INFO: grad norm: 373.278 368.646 58.627
2024-12-01-17:05:01-root-INFO: grad norm: 360.335 355.652 57.904
2024-12-01-17:05:01-root-INFO: grad norm: 348.744 344.638 53.358
2024-12-01-17:05:02-root-INFO: grad norm: 337.022 332.525 54.873
2024-12-01-17:05:02-root-INFO: grad norm: 326.241 322.459 49.531
2024-12-01-17:05:02-root-INFO: grad norm: 315.908 311.609 51.939
2024-12-01-17:05:03-root-INFO: Loss Change: 1956.918 -> 1910.338
2024-12-01-17:05:03-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-01-17:05:03-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-01-17:05:03-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:05:03-root-INFO: step: 183 lr_xt 0.00284680
2024-12-01-17:05:03-root-INFO: grad norm: 396.418 391.581 61.738
2024-12-01-17:05:03-root-INFO: Loss too large (1920.104->2471.483)! Learning rate decreased to 0.00228.
2024-12-01-17:05:03-root-INFO: Loss too large (1920.104->2273.397)! Learning rate decreased to 0.00182.
2024-12-01-17:05:04-root-INFO: Loss too large (1920.104->2120.963)! Learning rate decreased to 0.00146.
2024-12-01-17:05:04-root-INFO: Loss too large (1920.104->2014.318)! Learning rate decreased to 0.00117.
2024-12-01-17:05:04-root-INFO: Loss too large (1920.104->1946.729)! Learning rate decreased to 0.00093.
2024-12-01-17:05:04-root-INFO: grad norm: 358.980 354.721 55.133
2024-12-01-17:05:05-root-INFO: grad norm: 332.315 328.452 50.526
2024-12-01-17:05:05-root-INFO: grad norm: 308.415 304.511 48.919
2024-12-01-17:05:06-root-INFO: grad norm: 288.704 285.397 43.570
2024-12-01-17:05:06-root-INFO: grad norm: 271.616 267.992 44.224
2024-12-01-17:05:07-root-INFO: grad norm: 257.471 254.530 38.804
2024-12-01-17:05:08-root-INFO: grad norm: 245.427 242.000 40.870
2024-12-01-17:05:08-root-INFO: Loss Change: 1920.104 -> 1870.164
2024-12-01-17:05:08-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-01-17:05:08-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-01-17:05:08-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:05:08-root-INFO: step: 182 lr_xt 0.00296752
2024-12-01-17:05:08-root-INFO: grad norm: 308.887 305.289 47.007
2024-12-01-17:05:08-root-INFO: Loss too large (1875.637->2267.710)! Learning rate decreased to 0.00237.
2024-12-01-17:05:08-root-INFO: Loss too large (1875.637->2116.469)! Learning rate decreased to 0.00190.
2024-12-01-17:05:09-root-INFO: Loss too large (1875.637->2007.415)! Learning rate decreased to 0.00152.
2024-12-01-17:05:09-root-INFO: Loss too large (1875.637->1935.408)! Learning rate decreased to 0.00122.
2024-12-01-17:05:09-root-INFO: Loss too large (1875.637->1891.662)! Learning rate decreased to 0.00097.
2024-12-01-17:05:09-root-INFO: grad norm: 281.715 278.206 44.323
2024-12-01-17:05:10-root-INFO: grad norm: 263.440 260.623 38.425
2024-12-01-17:05:10-root-INFO: grad norm: 247.718 244.387 40.489
2024-12-01-17:05:11-root-INFO: grad norm: 234.970 232.494 34.026
2024-12-01-17:05:12-root-INFO: grad norm: 223.923 220.754 37.535
2024-12-01-17:05:12-root-INFO: grad norm: 214.713 212.440 31.160
2024-12-01-17:05:13-root-INFO: grad norm: 206.685 203.639 35.350
2024-12-01-17:05:13-root-INFO: Loss Change: 1875.637 -> 1838.397
2024-12-01-17:05:13-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-01-17:05:13-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-01-17:05:13-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-17:05:13-root-INFO: step: 181 lr_xt 0.00309285
2024-12-01-17:05:13-root-INFO: grad norm: 277.902 274.786 41.502
2024-12-01-17:05:14-root-INFO: Loss too large (1842.986->2174.743)! Learning rate decreased to 0.00247.
2024-12-01-17:05:14-root-INFO: Loss too large (1842.986->2043.874)! Learning rate decreased to 0.00198.
2024-12-01-17:05:14-root-INFO: Loss too large (1842.986->1951.193)! Learning rate decreased to 0.00158.
2024-12-01-17:05:14-root-INFO: Loss too large (1842.986->1890.988)! Learning rate decreased to 0.00127.
2024-12-01-17:05:14-root-INFO: Loss too large (1842.986->1854.866)! Learning rate decreased to 0.00101.
2024-12-01-17:05:15-root-INFO: grad norm: 250.515 247.075 41.373
2024-12-01-17:05:15-root-INFO: grad norm: 233.952 231.627 32.896
2024-12-01-17:05:16-root-INFO: grad norm: 219.463 216.251 37.413
2024-12-01-17:05:16-root-INFO: grad norm: 207.380 205.299 29.300
2024-12-01-17:05:17-root-INFO: grad norm: 196.474 193.469 34.233
2024-12-01-17:05:18-root-INFO: grad norm: 187.068 185.145 26.754
2024-12-01-17:05:18-root-INFO: grad norm: 178.544 175.685 31.821
2024-12-01-17:05:18-root-INFO: Loss Change: 1842.986 -> 1807.347
2024-12-01-17:05:18-root-INFO: Regularization Change: 0.000 -> 0.184
2024-12-01-17:05:18-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-01-17:05:18-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-17:05:19-root-INFO: step: 180 lr_xt 0.00322295
2024-12-01-17:05:19-root-INFO: grad norm: 271.670 267.884 45.198
2024-12-01-17:05:19-root-INFO: Loss too large (1818.456->2130.060)! Learning rate decreased to 0.00258.
2024-12-01-17:05:19-root-INFO: Loss too large (1818.456->2006.326)! Learning rate decreased to 0.00206.
2024-12-01-17:05:19-root-INFO: Loss too large (1818.456->1918.433)! Learning rate decreased to 0.00165.
2024-12-01-17:05:19-root-INFO: Loss too large (1818.456->1861.376)! Learning rate decreased to 0.00132.
2024-12-01-17:05:19-root-INFO: Loss too large (1818.456->1827.284)! Learning rate decreased to 0.00106.
2024-12-01-17:05:20-root-INFO: grad norm: 238.638 235.376 39.318
2024-12-01-17:05:20-root-INFO: grad norm: 219.905 217.486 32.527
2024-12-01-17:05:21-root-INFO: grad norm: 204.291 201.200 35.400
2024-12-01-17:05:21-root-INFO: grad norm: 191.203 189.166 27.833
2024-12-01-17:05:22-root-INFO: grad norm: 179.125 176.238 32.029
2024-12-01-17:05:22-root-INFO: grad norm: 168.518 166.699 24.695
2024-12-01-17:05:23-root-INFO: grad norm: 158.685 155.943 29.371
2024-12-01-17:05:23-root-INFO: Loss Change: 1818.456 -> 1778.594
2024-12-01-17:05:23-root-INFO: Regularization Change: 0.000 -> 0.216
2024-12-01-17:05:23-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-01-17:05:23-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-17:05:23-root-INFO: step: 179 lr_xt 0.00335799
2024-12-01-17:05:24-root-INFO: grad norm: 223.741 221.199 33.630
2024-12-01-17:05:24-root-INFO: Loss too large (1780.459->2019.571)! Learning rate decreased to 0.00269.
2024-12-01-17:05:24-root-INFO: Loss too large (1780.459->1920.471)! Learning rate decreased to 0.00215.
2024-12-01-17:05:24-root-INFO: Loss too large (1780.459->1853.249)! Learning rate decreased to 0.00172.
2024-12-01-17:05:24-root-INFO: Loss too large (1780.459->1811.113)! Learning rate decreased to 0.00138.
2024-12-01-17:05:24-root-INFO: Loss too large (1780.459->1786.519)! Learning rate decreased to 0.00110.
2024-12-01-17:05:25-root-INFO: grad norm: 196.705 193.875 33.249
2024-12-01-17:05:25-root-INFO: grad norm: 177.749 175.887 25.661
2024-12-01-17:05:26-root-INFO: grad norm: 161.569 158.903 29.227
2024-12-01-17:05:26-root-INFO: grad norm: 147.979 146.331 22.023
2024-12-01-17:05:27-root-INFO: grad norm: 135.721 133.191 26.087
2024-12-01-17:05:27-root-INFO: grad norm: 125.133 123.590 19.591
2024-12-01-17:05:28-root-INFO: grad norm: 115.583 113.128 23.697
2024-12-01-17:05:28-root-INFO: Loss Change: 1780.459 -> 1745.692
2024-12-01-17:05:28-root-INFO: Regularization Change: 0.000 -> 0.209
2024-12-01-17:05:28-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-01-17:05:28-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-17:05:28-root-INFO: step: 178 lr_xt 0.00349812
2024-12-01-17:05:28-root-INFO: grad norm: 194.681 191.600 34.499
2024-12-01-17:05:29-root-INFO: Loss too large (1748.543->1911.883)! Learning rate decreased to 0.00280.
2024-12-01-17:05:29-root-INFO: Loss too large (1748.543->1840.506)! Learning rate decreased to 0.00224.
2024-12-01-17:05:29-root-INFO: Loss too large (1748.543->1793.730)! Learning rate decreased to 0.00179.
2024-12-01-17:05:29-root-INFO: Loss too large (1748.543->1765.110)! Learning rate decreased to 0.00143.
2024-12-01-17:05:29-root-INFO: Loss too large (1748.543->1748.740)! Learning rate decreased to 0.00115.
2024-12-01-17:05:30-root-INFO: grad norm: 157.757 155.294 27.763
2024-12-01-17:05:30-root-INFO: grad norm: 133.929 132.018 22.543
2024-12-01-17:05:31-root-INFO: grad norm: 116.895 114.483 23.627
2024-12-01-17:05:31-root-INFO: grad norm: 103.431 101.762 18.500
2024-12-01-17:05:32-root-INFO: grad norm: 92.259 89.869 20.866
2024-12-01-17:05:32-root-INFO: grad norm: 83.251 81.598 16.506
2024-12-01-17:05:33-root-INFO: grad norm: 75.846 73.424 19.012
2024-12-01-17:05:33-root-INFO: Loss Change: 1748.543 -> 1711.928
2024-12-01-17:05:33-root-INFO: Regularization Change: 0.000 -> 0.246
2024-12-01-17:05:33-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-01-17:05:33-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-17:05:33-root-INFO: step: 177 lr_xt 0.00364350
2024-12-01-17:05:34-root-INFO: grad norm: 130.155 127.852 24.377
2024-12-01-17:05:34-root-INFO: Loss too large (1708.789->1775.461)! Learning rate decreased to 0.00291.
2024-12-01-17:05:34-root-INFO: Loss too large (1708.789->1743.706)! Learning rate decreased to 0.00233.
2024-12-01-17:05:34-root-INFO: Loss too large (1708.789->1724.168)! Learning rate decreased to 0.00187.
2024-12-01-17:05:34-root-INFO: Loss too large (1708.789->1712.734)! Learning rate decreased to 0.00149.
2024-12-01-17:05:35-root-INFO: grad norm: 151.602 149.170 27.043
2024-12-01-17:05:35-root-INFO: grad norm: 183.666 181.819 25.981
2024-12-01-17:05:35-root-INFO: Loss too large (1705.612->1707.568)! Learning rate decreased to 0.00119.
2024-12-01-17:05:36-root-INFO: grad norm: 151.924 149.462 27.244
2024-12-01-17:05:36-root-INFO: grad norm: 127.006 125.523 19.357
2024-12-01-17:05:37-root-INFO: grad norm: 107.505 105.186 22.209
2024-12-01-17:05:37-root-INFO: grad norm: 92.524 91.074 16.314
2024-12-01-17:05:38-root-INFO: grad norm: 80.871 78.551 19.232
2024-12-01-17:05:38-root-INFO: Loss Change: 1708.789 -> 1677.108
2024-12-01-17:05:38-root-INFO: Regularization Change: 0.000 -> 0.284
2024-12-01-17:05:38-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-01-17:05:38-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-17:05:38-root-INFO: step: 176 lr_xt 0.00379432
2024-12-01-17:05:39-root-INFO: grad norm: 163.925 161.178 29.885
2024-12-01-17:05:39-root-INFO: Loss too large (1679.970->1775.028)! Learning rate decreased to 0.00304.
2024-12-01-17:05:39-root-INFO: Loss too large (1679.970->1732.747)! Learning rate decreased to 0.00243.
2024-12-01-17:05:39-root-INFO: Loss too large (1679.970->1705.050)! Learning rate decreased to 0.00194.
2024-12-01-17:05:39-root-INFO: Loss too large (1679.970->1687.866)! Learning rate decreased to 0.00155.
2024-12-01-17:05:40-root-INFO: grad norm: 173.944 171.436 29.430
2024-12-01-17:05:40-root-INFO: grad norm: 190.452 188.481 27.326
2024-12-01-17:05:40-root-INFO: Loss too large (1673.273->1673.768)! Learning rate decreased to 0.00124.
2024-12-01-17:05:41-root-INFO: grad norm: 146.805 144.360 26.679
2024-12-01-17:05:41-root-INFO: grad norm: 113.092 111.596 18.335
2024-12-01-17:05:42-root-INFO: grad norm: 91.899 89.588 20.479
2024-12-01-17:05:42-root-INFO: grad norm: 77.338 75.753 15.578
2024-12-01-17:05:43-root-INFO: grad norm: 68.033 65.689 17.703
2024-12-01-17:05:43-root-INFO: Loss Change: 1679.970 -> 1640.038
2024-12-01-17:05:43-root-INFO: Regularization Change: 0.000 -> 0.346
2024-12-01-17:05:43-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-01-17:05:43-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-17:05:43-root-INFO: step: 175 lr_xt 0.00395074
2024-12-01-17:05:43-root-INFO: grad norm: 158.218 155.892 27.033
2024-12-01-17:05:43-root-INFO: Loss too large (1640.366->1723.969)! Learning rate decreased to 0.00316.
2024-12-01-17:05:44-root-INFO: Loss too large (1640.366->1687.274)! Learning rate decreased to 0.00253.
2024-12-01-17:05:44-root-INFO: Loss too large (1640.366->1662.864)! Learning rate decreased to 0.00202.
2024-12-01-17:05:44-root-INFO: Loss too large (1640.366->1647.428)! Learning rate decreased to 0.00162.
2024-12-01-17:05:44-root-INFO: grad norm: 164.182 161.643 28.760
2024-12-01-17:05:45-root-INFO: grad norm: 171.538 169.649 25.387
2024-12-01-17:05:45-root-INFO: grad norm: 180.892 178.159 31.327
2024-12-01-17:05:46-root-INFO: grad norm: 189.729 187.889 26.361
2024-12-01-17:05:46-root-INFO: grad norm: 196.668 193.773 33.621
2024-12-01-17:05:47-root-INFO: grad norm: 201.604 199.745 27.320
2024-12-01-17:05:47-root-INFO: grad norm: 203.419 200.443 34.666
2024-12-01-17:05:48-root-INFO: Loss Change: 1640.366 -> 1604.729
2024-12-01-17:05:48-root-INFO: Regularization Change: 0.000 -> 0.559
2024-12-01-17:05:48-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-01-17:05:48-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-17:05:48-root-INFO: step: 174 lr_xt 0.00411294
2024-12-01-17:05:48-root-INFO: grad norm: 228.119 225.873 31.925
2024-12-01-17:05:48-root-INFO: Loss too large (1606.614->1800.073)! Learning rate decreased to 0.00329.
2024-12-01-17:05:48-root-INFO: Loss too large (1606.614->1717.964)! Learning rate decreased to 0.00263.
2024-12-01-17:05:48-root-INFO: Loss too large (1606.614->1661.790)! Learning rate decreased to 0.00211.
2024-12-01-17:05:49-root-INFO: Loss too large (1606.614->1625.891)! Learning rate decreased to 0.00168.
2024-12-01-17:05:49-root-INFO: grad norm: 216.249 213.218 36.082
2024-12-01-17:05:49-root-INFO: grad norm: 202.940 200.964 28.251
2024-12-01-17:05:50-root-INFO: grad norm: 191.952 189.101 32.958
2024-12-01-17:05:50-root-INFO: grad norm: 180.197 178.334 25.847
2024-12-01-17:05:51-root-INFO: grad norm: 170.670 168.023 29.945
2024-12-01-17:05:51-root-INFO: grad norm: 160.636 158.831 24.019
2024-12-01-17:05:52-root-INFO: grad norm: 153.852 151.393 27.396
2024-12-01-17:05:52-root-INFO: Loss Change: 1606.614 -> 1554.013
2024-12-01-17:05:52-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-01-17:05:52-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-01-17:05:52-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-17:05:52-root-INFO: step: 173 lr_xt 0.00428111
2024-12-01-17:05:52-root-INFO: grad norm: 290.454 286.494 47.798
2024-12-01-17:05:53-root-INFO: Loss too large (1572.219->1712.840)! Learning rate decreased to 0.00342.
2024-12-01-17:05:53-root-INFO: Loss too large (1572.219->1669.160)! Learning rate decreased to 0.00274.
2024-12-01-17:05:53-root-INFO: Loss too large (1572.219->1632.192)! Learning rate decreased to 0.00219.
2024-12-01-17:05:53-root-INFO: Loss too large (1572.219->1601.699)! Learning rate decreased to 0.00175.
2024-12-01-17:05:53-root-INFO: Loss too large (1572.219->1578.035)! Learning rate decreased to 0.00140.
2024-12-01-17:05:54-root-INFO: grad norm: 175.387 172.973 28.999
2024-12-01-17:05:54-root-INFO: grad norm: 62.826 60.352 17.457
2024-12-01-17:05:55-root-INFO: grad norm: 57.079 54.948 15.450
2024-12-01-17:05:55-root-INFO: grad norm: 53.696 51.479 15.273
2024-12-01-17:05:56-root-INFO: grad norm: 51.713 49.688 14.327
2024-12-01-17:05:56-root-INFO: grad norm: 50.287 48.231 14.232
2024-12-01-17:05:57-root-INFO: grad norm: 49.325 47.371 13.746
2024-12-01-17:05:57-root-INFO: Loss Change: 1572.219 -> 1515.275
2024-12-01-17:05:57-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-01-17:05:57-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-01-17:05:57-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-17:05:57-root-INFO: step: 172 lr_xt 0.00445543
2024-12-01-17:05:57-root-INFO: grad norm: 70.182 68.019 17.290
2024-12-01-17:05:58-root-INFO: Loss too large (1510.891->1515.062)! Learning rate decreased to 0.00356.
2024-12-01-17:05:58-root-INFO: Loss too large (1510.891->1510.894)! Learning rate decreased to 0.00285.
2024-12-01-17:05:58-root-INFO: grad norm: 116.028 114.307 19.909
2024-12-01-17:05:58-root-INFO: Loss too large (1508.513->1525.933)! Learning rate decreased to 0.00228.
2024-12-01-17:05:59-root-INFO: Loss too large (1508.513->1511.852)! Learning rate decreased to 0.00182.
2024-12-01-17:05:59-root-INFO: grad norm: 141.454 139.443 23.768
2024-12-01-17:05:59-root-INFO: Loss too large (1504.688->1506.486)! Learning rate decreased to 0.00146.
2024-12-01-17:06:00-root-INFO: grad norm: 117.062 115.359 19.894
2024-12-01-17:06:00-root-INFO: grad norm: 88.830 87.110 17.392
2024-12-01-17:06:01-root-INFO: grad norm: 79.385 77.855 15.511
2024-12-01-17:06:01-root-INFO: grad norm: 69.710 68.061 15.076
2024-12-01-17:06:02-root-INFO: grad norm: 64.583 63.034 14.060
2024-12-01-17:06:02-root-INFO: Loss Change: 1510.891 -> 1479.836
2024-12-01-17:06:02-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-01-17:06:02-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-17:06:02-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-17:06:02-root-INFO: step: 171 lr_xt 0.00463611
2024-12-01-17:06:02-root-INFO: grad norm: 124.443 122.446 22.206
2024-12-01-17:06:02-root-INFO: Loss too large (1477.850->1529.895)! Learning rate decreased to 0.00371.
2024-12-01-17:06:03-root-INFO: Loss too large (1477.850->1510.385)! Learning rate decreased to 0.00297.
2024-12-01-17:06:03-root-INFO: Loss too large (1477.850->1495.440)! Learning rate decreased to 0.00237.
2024-12-01-17:06:03-root-INFO: Loss too large (1477.850->1484.886)! Learning rate decreased to 0.00190.
2024-12-01-17:06:03-root-INFO: Loss too large (1477.850->1478.044)! Learning rate decreased to 0.00152.
2024-12-01-17:06:04-root-INFO: grad norm: 110.943 109.228 19.434
2024-12-01-17:06:04-root-INFO: grad norm: 97.298 95.590 18.152
2024-12-01-17:06:04-root-INFO: grad norm: 91.400 89.786 17.099
2024-12-01-17:06:05-root-INFO: grad norm: 84.515 82.905 16.415
2024-12-01-17:06:05-root-INFO: grad norm: 80.709 79.114 15.965
2024-12-01-17:06:06-root-INFO: grad norm: 76.356 74.785 15.409
2024-12-01-17:06:06-root-INFO: grad norm: 73.722 72.115 15.309
2024-12-01-17:06:07-root-INFO: Loss Change: 1477.850 -> 1445.391
2024-12-01-17:06:07-root-INFO: Regularization Change: 0.000 -> 0.349
2024-12-01-17:06:07-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-01-17:06:07-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-17:06:07-root-INFO: step: 170 lr_xt 0.00482333
2024-12-01-17:06:07-root-INFO: grad norm: 148.771 146.161 27.745
2024-12-01-17:06:07-root-INFO: Loss too large (1450.344->1522.407)! Learning rate decreased to 0.00386.
2024-12-01-17:06:07-root-INFO: Loss too large (1450.344->1498.196)! Learning rate decreased to 0.00309.
2024-12-01-17:06:08-root-INFO: Loss too large (1450.344->1478.086)! Learning rate decreased to 0.00247.
2024-12-01-17:06:08-root-INFO: Loss too large (1450.344->1462.890)! Learning rate decreased to 0.00198.
2024-12-01-17:06:08-root-INFO: Loss too large (1450.344->1452.469)! Learning rate decreased to 0.00158.
2024-12-01-17:06:08-root-INFO: grad norm: 135.173 133.391 21.874
2024-12-01-17:06:09-root-INFO: grad norm: 119.758 117.709 22.059
2024-12-01-17:06:09-root-INFO: grad norm: 113.807 112.123 19.507
2024-12-01-17:06:10-root-INFO: grad norm: 106.144 104.318 19.606
2024-12-01-17:06:10-root-INFO: grad norm: 102.404 100.695 18.632
2024-12-01-17:06:11-root-INFO: grad norm: 97.685 95.938 18.390
2024-12-01-17:06:12-root-INFO: grad norm: 95.632 93.809 18.585
2024-12-01-17:06:12-root-INFO: Loss Change: 1450.344 -> 1403.890
2024-12-01-17:06:12-root-INFO: Regularization Change: 0.000 -> 0.509
2024-12-01-17:06:12-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-01-17:06:12-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-17:06:12-root-INFO: step: 169 lr_xt 0.00501730
2024-12-01-17:06:12-root-INFO: grad norm: 149.382 146.966 26.756
2024-12-01-17:06:12-root-INFO: Loss too large (1407.146->1476.464)! Learning rate decreased to 0.00401.
2024-12-01-17:06:12-root-INFO: Loss too large (1407.146->1450.541)! Learning rate decreased to 0.00321.
2024-12-01-17:06:13-root-INFO: Loss too large (1407.146->1429.642)! Learning rate decreased to 0.00257.
2024-12-01-17:06:13-root-INFO: Loss too large (1407.146->1414.500)! Learning rate decreased to 0.00206.
2024-12-01-17:06:13-root-INFO: grad norm: 184.479 182.034 29.939
2024-12-01-17:06:14-root-INFO: grad norm: 230.955 228.537 33.333
2024-12-01-17:06:14-root-INFO: Loss too large (1398.744->1401.640)! Learning rate decreased to 0.00164.
2024-12-01-17:06:14-root-INFO: grad norm: 195.999 193.189 33.073
2024-12-01-17:06:15-root-INFO: grad norm: 164.857 162.896 25.355
2024-12-01-17:06:15-root-INFO: grad norm: 154.571 151.888 28.673
2024-12-01-17:06:16-root-INFO: grad norm: 150.413 148.700 22.632
2024-12-01-17:06:16-root-INFO: grad norm: 150.741 148.135 27.904
2024-12-01-17:06:16-root-INFO: Loss Change: 1407.146 -> 1347.624
2024-12-01-17:06:16-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-01-17:06:16-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-01-17:06:16-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-17:06:17-root-INFO: step: 168 lr_xt 0.00521823
2024-12-01-17:06:17-root-INFO: grad norm: 209.187 207.056 29.789
2024-12-01-17:06:17-root-INFO: Loss too large (1353.768->1507.646)! Learning rate decreased to 0.00417.
2024-12-01-17:06:17-root-INFO: Loss too large (1353.768->1471.045)! Learning rate decreased to 0.00334.
2024-12-01-17:06:17-root-INFO: Loss too large (1353.768->1432.551)! Learning rate decreased to 0.00267.
2024-12-01-17:06:17-root-INFO: Loss too large (1353.768->1397.064)! Learning rate decreased to 0.00214.
2024-12-01-17:06:18-root-INFO: Loss too large (1353.768->1369.008)! Learning rate decreased to 0.00171.
2024-12-01-17:06:18-root-INFO: grad norm: 205.904 202.978 34.588
2024-12-01-17:06:19-root-INFO: grad norm: 200.379 198.416 27.982
2024-12-01-17:06:19-root-INFO: grad norm: 196.406 193.627 32.924
2024-12-01-17:06:20-root-INFO: grad norm: 190.939 189.056 26.748
2024-12-01-17:06:20-root-INFO: grad norm: 186.772 184.145 31.215
2024-12-01-17:06:21-root-INFO: grad norm: 181.770 179.949 25.670
2024-12-01-17:06:21-root-INFO: grad norm: 177.860 175.375 29.627
2024-12-01-17:06:21-root-INFO: Loss Change: 1353.768 -> 1318.033
2024-12-01-17:06:21-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-01-17:06:21-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-01-17:06:21-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-17:06:22-root-INFO: step: 167 lr_xt 0.00542633
2024-12-01-17:06:22-root-INFO: grad norm: 222.021 219.349 34.343
2024-12-01-17:06:22-root-INFO: Loss too large (1328.127->1492.739)! Learning rate decreased to 0.00434.
2024-12-01-17:06:22-root-INFO: Loss too large (1328.127->1453.802)! Learning rate decreased to 0.00347.
2024-12-01-17:06:22-root-INFO: Loss too large (1328.127->1411.418)! Learning rate decreased to 0.00278.
2024-12-01-17:06:22-root-INFO: Loss too large (1328.127->1371.280)! Learning rate decreased to 0.00222.
2024-12-01-17:06:23-root-INFO: Loss too large (1328.127->1339.357)! Learning rate decreased to 0.00178.
2024-12-01-17:06:23-root-INFO: grad norm: 203.839 201.325 31.913
2024-12-01-17:06:24-root-INFO: grad norm: 189.626 187.488 28.395
2024-12-01-17:06:24-root-INFO: grad norm: 178.569 176.319 28.262
2024-12-01-17:06:24-root-INFO: grad norm: 169.886 167.981 25.372
2024-12-01-17:06:25-root-INFO: grad norm: 162.434 160.352 25.927
2024-12-01-17:06:25-root-INFO: grad norm: 156.693 154.921 23.501
2024-12-01-17:06:26-root-INFO: grad norm: 151.743 149.773 24.372
2024-12-01-17:06:26-root-INFO: Loss Change: 1328.127 -> 1283.772
2024-12-01-17:06:26-root-INFO: Regularization Change: 0.000 -> 0.448
2024-12-01-17:06:26-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-01-17:06:26-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-17:06:26-root-INFO: step: 166 lr_xt 0.00564182
2024-12-01-17:06:27-root-INFO: grad norm: 198.440 196.157 30.013
2024-12-01-17:06:27-root-INFO: Loss too large (1290.677->1450.872)! Learning rate decreased to 0.00451.
2024-12-01-17:06:27-root-INFO: Loss too large (1290.677->1408.440)! Learning rate decreased to 0.00361.
2024-12-01-17:06:27-root-INFO: Loss too large (1290.677->1364.255)! Learning rate decreased to 0.00289.
2024-12-01-17:06:27-root-INFO: Loss too large (1290.677->1325.118)! Learning rate decreased to 0.00231.
2024-12-01-17:06:27-root-INFO: Loss too large (1290.677->1296.375)! Learning rate decreased to 0.00185.
2024-12-01-17:06:28-root-INFO: grad norm: 176.167 174.082 27.020
2024-12-01-17:06:28-root-INFO: grad norm: 163.738 161.892 24.516
2024-12-01-17:06:29-root-INFO: grad norm: 152.319 150.444 23.822
2024-12-01-17:06:29-root-INFO: grad norm: 145.388 143.717 21.981
2024-12-01-17:06:30-root-INFO: grad norm: 138.933 137.175 22.031
2024-12-01-17:06:30-root-INFO: grad norm: 134.976 133.396 20.592
2024-12-01-17:06:31-root-INFO: grad norm: 131.525 129.833 21.027
2024-12-01-17:06:31-root-INFO: Loss Change: 1290.677 -> 1248.317
2024-12-01-17:06:31-root-INFO: Regularization Change: 0.000 -> 0.443
2024-12-01-17:06:31-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-01-17:06:31-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-17:06:31-root-INFO: step: 165 lr_xt 0.00586491
2024-12-01-17:06:32-root-INFO: grad norm: 163.616 161.649 25.295
2024-12-01-17:06:32-root-INFO: Loss too large (1252.948->1400.853)! Learning rate decreased to 0.00469.
2024-12-01-17:06:32-root-INFO: Loss too large (1252.948->1357.191)! Learning rate decreased to 0.00375.
2024-12-01-17:06:32-root-INFO: Loss too large (1252.948->1315.313)! Learning rate decreased to 0.00300.
2024-12-01-17:06:32-root-INFO: Loss too large (1252.948->1281.424)! Learning rate decreased to 0.00240.
2024-12-01-17:06:32-root-INFO: Loss too large (1252.948->1258.429)! Learning rate decreased to 0.00192.
2024-12-01-17:06:33-root-INFO: grad norm: 157.743 155.953 23.692
2024-12-01-17:06:33-root-INFO: grad norm: 152.881 151.155 22.906
2024-12-01-17:06:34-root-INFO: grad norm: 146.815 145.098 22.391
2024-12-01-17:06:34-root-INFO: grad norm: 142.992 141.378 21.423
2024-12-01-17:06:35-root-INFO: grad norm: 138.739 137.078 21.406
2024-12-01-17:06:35-root-INFO: grad norm: 136.322 134.771 20.508
2024-12-01-17:06:36-root-INFO: grad norm: 133.936 132.308 20.818
2024-12-01-17:06:36-root-INFO: Loss Change: 1252.948 -> 1219.816
2024-12-01-17:06:36-root-INFO: Regularization Change: 0.000 -> 0.434
2024-12-01-17:06:36-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-01-17:06:36-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-17:06:36-root-INFO: step: 164 lr_xt 0.00609585
2024-12-01-17:06:36-root-INFO: grad norm: 172.276 170.211 26.594
2024-12-01-17:06:36-root-INFO: Loss too large (1227.222->1387.044)! Learning rate decreased to 0.00488.
2024-12-01-17:06:37-root-INFO: Loss too large (1227.222->1341.226)! Learning rate decreased to 0.00390.
2024-12-01-17:06:37-root-INFO: Loss too large (1227.222->1295.163)! Learning rate decreased to 0.00312.
2024-12-01-17:06:37-root-INFO: Loss too large (1227.222->1256.707)! Learning rate decreased to 0.00250.
2024-12-01-17:06:37-root-INFO: Loss too large (1227.222->1230.461)! Learning rate decreased to 0.00200.
2024-12-01-17:06:38-root-INFO: grad norm: 157.197 155.349 24.032
2024-12-01-17:06:38-root-INFO: grad norm: 150.086 148.360 22.699
2024-12-01-17:06:39-root-INFO: grad norm: 141.201 139.540 21.597
2024-12-01-17:06:39-root-INFO: grad norm: 136.778 135.180 20.848
2024-12-01-17:06:40-root-INFO: grad norm: 131.595 130.025 20.272
2024-12-01-17:06:40-root-INFO: grad norm: 129.075 127.545 19.818
2024-12-01-17:06:41-root-INFO: grad norm: 126.469 124.940 19.609
2024-12-01-17:06:41-root-INFO: Loss Change: 1227.222 -> 1189.067
2024-12-01-17:06:41-root-INFO: Regularization Change: 0.000 -> 0.467
2024-12-01-17:06:41-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-01-17:06:41-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-17:06:41-root-INFO: step: 163 lr_xt 0.00633485
2024-12-01-17:06:41-root-INFO: grad norm: 157.431 155.703 23.257
2024-12-01-17:06:41-root-INFO: Loss too large (1191.332->1347.713)! Learning rate decreased to 0.00507.
2024-12-01-17:06:42-root-INFO: Loss too large (1191.332->1300.704)! Learning rate decreased to 0.00405.
2024-12-01-17:06:42-root-INFO: Loss too large (1191.332->1254.868)! Learning rate decreased to 0.00324.
2024-12-01-17:06:42-root-INFO: Loss too large (1191.332->1218.082)! Learning rate decreased to 0.00259.
2024-12-01-17:06:42-root-INFO: Loss too large (1191.332->1193.962)! Learning rate decreased to 0.00208.
2024-12-01-17:06:42-root-INFO: grad norm: 143.483 141.872 21.439
2024-12-01-17:06:43-root-INFO: grad norm: 138.385 136.874 20.391
2024-12-01-17:06:43-root-INFO: grad norm: 131.948 130.414 20.061
2024-12-01-17:06:44-root-INFO: grad norm: 129.201 127.756 19.273
2024-12-01-17:06:44-root-INFO: grad norm: 126.002 124.507 19.351
2024-12-01-17:06:45-root-INFO: grad norm: 124.709 123.293 18.739
2024-12-01-17:06:45-root-INFO: grad norm: 123.511 122.032 19.057
2024-12-01-17:06:46-root-INFO: Loss Change: 1191.332 -> 1157.734
2024-12-01-17:06:46-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-01-17:06:46-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-01-17:06:46-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-17:06:46-root-INFO: step: 162 lr_xt 0.00658217
2024-12-01-17:06:46-root-INFO: grad norm: 164.288 161.942 27.660
2024-12-01-17:06:46-root-INFO: Loss too large (1171.268->1336.167)! Learning rate decreased to 0.00527.
2024-12-01-17:06:46-root-INFO: Loss too large (1171.268->1287.403)! Learning rate decreased to 0.00421.
2024-12-01-17:06:46-root-INFO: Loss too large (1171.268->1238.248)! Learning rate decreased to 0.00337.
2024-12-01-17:06:47-root-INFO: Loss too large (1171.268->1197.880)! Learning rate decreased to 0.00270.
2024-12-01-17:06:47-root-INFO: Loss too large (1171.268->1171.362)! Learning rate decreased to 0.00216.
2024-12-01-17:06:47-root-INFO: grad norm: 145.499 143.903 21.490
2024-12-01-17:06:48-root-INFO: grad norm: 138.880 137.052 22.463
2024-12-01-17:06:48-root-INFO: grad norm: 129.323 127.892 19.184
2024-12-01-17:06:49-root-INFO: grad norm: 125.332 123.689 20.226
2024-12-01-17:06:49-root-INFO: grad norm: 120.035 118.680 17.984
2024-12-01-17:06:50-root-INFO: grad norm: 117.858 116.314 19.012
2024-12-01-17:06:50-root-INFO: grad norm: 115.372 114.048 17.431
2024-12-01-17:06:50-root-INFO: Loss Change: 1171.268 -> 1131.301
2024-12-01-17:06:50-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-01-17:06:50-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-01-17:06:50-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-17:06:51-root-INFO: step: 161 lr_xt 0.00683803
2024-12-01-17:06:51-root-INFO: grad norm: 143.805 142.206 21.387
2024-12-01-17:06:51-root-INFO: Loss too large (1135.126->1288.991)! Learning rate decreased to 0.00547.
2024-12-01-17:06:51-root-INFO: Loss too large (1135.126->1239.403)! Learning rate decreased to 0.00438.
2024-12-01-17:06:51-root-INFO: Loss too large (1135.126->1192.610)! Learning rate decreased to 0.00350.
2024-12-01-17:06:51-root-INFO: Loss too large (1135.126->1156.879)! Learning rate decreased to 0.00280.
2024-12-01-17:06:52-root-INFO: grad norm: 202.077 199.925 29.411
2024-12-01-17:06:52-root-INFO: Loss too large (1134.813->1155.046)! Learning rate decreased to 0.00224.
2024-12-01-17:06:52-root-INFO: Loss too large (1134.813->1135.685)! Learning rate decreased to 0.00179.
2024-12-01-17:06:53-root-INFO: grad norm: 124.304 122.911 18.555
2024-12-01-17:06:53-root-INFO: grad norm: 60.724 59.559 11.837
2024-12-01-17:06:54-root-INFO: grad norm: 48.594 47.329 11.013
2024-12-01-17:06:54-root-INFO: grad norm: 41.964 40.706 10.201
2024-12-01-17:06:55-root-INFO: grad norm: 39.385 38.059 10.135
2024-12-01-17:06:55-root-INFO: grad norm: 38.047 36.738 9.894
2024-12-01-17:06:56-root-INFO: Loss Change: 1135.126 -> 1098.126
2024-12-01-17:06:56-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-01-17:06:56-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-01-17:06:56-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-17:06:56-root-INFO: step: 160 lr_xt 0.00710269
2024-12-01-17:06:56-root-INFO: grad norm: 71.970 70.324 15.304
2024-12-01-17:06:56-root-INFO: Loss too large (1101.471->1142.596)! Learning rate decreased to 0.00568.
2024-12-01-17:06:56-root-INFO: Loss too large (1101.471->1121.334)! Learning rate decreased to 0.00455.
2024-12-01-17:06:56-root-INFO: Loss too large (1101.471->1108.649)! Learning rate decreased to 0.00364.
2024-12-01-17:06:57-root-INFO: Loss too large (1101.471->1101.709)! Learning rate decreased to 0.00291.
2024-12-01-17:06:57-root-INFO: grad norm: 101.422 100.186 15.786
2024-12-01-17:06:57-root-INFO: Loss too large (1098.262->1102.019)! Learning rate decreased to 0.00233.
2024-12-01-17:06:58-root-INFO: grad norm: 102.517 101.138 16.763
2024-12-01-17:06:58-root-INFO: grad norm: 103.951 102.718 15.960
2024-12-01-17:06:59-root-INFO: grad norm: 104.185 102.846 16.650
2024-12-01-17:06:59-root-INFO: grad norm: 104.422 103.185 16.026
2024-12-01-17:07:00-root-INFO: grad norm: 104.342 103.027 16.513
2024-12-01-17:07:00-root-INFO: grad norm: 104.248 103.006 16.039
2024-12-01-17:07:00-root-INFO: Loss Change: 1101.471 -> 1078.574
2024-12-01-17:07:00-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-01-17:07:00-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-01-17:07:00-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-17:07:00-root-INFO: step: 159 lr_xt 0.00737641
2024-12-01-17:07:01-root-INFO: grad norm: 126.822 125.413 18.848
2024-12-01-17:07:01-root-INFO: Loss too large (1081.580->1231.699)! Learning rate decreased to 0.00590.
2024-12-01-17:07:01-root-INFO: Loss too large (1081.580->1180.661)! Learning rate decreased to 0.00472.
2024-12-01-17:07:01-root-INFO: Loss too large (1081.580->1134.374)! Learning rate decreased to 0.00378.
2024-12-01-17:07:01-root-INFO: Loss too large (1081.580->1100.820)! Learning rate decreased to 0.00302.
2024-12-01-17:07:02-root-INFO: grad norm: 187.332 185.358 27.124
2024-12-01-17:07:02-root-INFO: Loss too large (1081.125->1102.442)! Learning rate decreased to 0.00242.
2024-12-01-17:07:02-root-INFO: Loss too large (1081.125->1084.615)! Learning rate decreased to 0.00193.
2024-12-01-17:07:02-root-INFO: grad norm: 119.200 117.929 17.358
2024-12-01-17:07:03-root-INFO: grad norm: 54.558 53.377 11.289
2024-12-01-17:07:03-root-INFO: grad norm: 44.736 43.465 10.588
2024-12-01-17:07:04-root-INFO: grad norm: 39.135 37.837 9.995
2024-12-01-17:07:04-root-INFO: grad norm: 36.946 35.596 9.896
2024-12-01-17:07:05-root-INFO: grad norm: 35.789 34.449 9.702
2024-12-01-17:07:05-root-INFO: Loss Change: 1081.580 -> 1048.049
2024-12-01-17:07:05-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-01-17:07:05-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-01-17:07:05-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-17:07:05-root-INFO: step: 158 lr_xt 0.00765943
2024-12-01-17:07:05-root-INFO: grad norm: 67.039 65.496 14.302
2024-12-01-17:07:05-root-INFO: Loss too large (1050.239->1084.130)! Learning rate decreased to 0.00613.
2024-12-01-17:07:06-root-INFO: Loss too large (1050.239->1065.256)! Learning rate decreased to 0.00490.
2024-12-01-17:07:06-root-INFO: Loss too large (1050.239->1054.503)! Learning rate decreased to 0.00392.
2024-12-01-17:07:06-root-INFO: grad norm: 127.283 125.898 18.730
2024-12-01-17:07:06-root-INFO: Loss too large (1048.871->1072.802)! Learning rate decreased to 0.00314.
2024-12-01-17:07:07-root-INFO: Loss too large (1048.871->1058.673)! Learning rate decreased to 0.00251.
2024-12-01-17:07:07-root-INFO: Loss too large (1048.871->1049.641)! Learning rate decreased to 0.00201.
2024-12-01-17:07:07-root-INFO: grad norm: 91.192 90.021 14.568
2024-12-01-17:07:08-root-INFO: grad norm: 55.320 54.250 10.826
2024-12-01-17:07:08-root-INFO: grad norm: 46.089 44.885 10.466
2024-12-01-17:07:09-root-INFO: grad norm: 39.881 38.707 9.606
2024-12-01-17:07:09-root-INFO: grad norm: 37.232 35.968 9.619
2024-12-01-17:07:10-root-INFO: grad norm: 35.663 34.431 9.295
2024-12-01-17:07:10-root-INFO: Loss Change: 1050.239 -> 1024.009
2024-12-01-17:07:10-root-INFO: Regularization Change: 0.000 -> 0.434
2024-12-01-17:07:10-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-01-17:07:10-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-17:07:10-root-INFO: step: 157 lr_xt 0.00795203
2024-12-01-17:07:10-root-INFO: grad norm: 73.099 71.490 15.254
2024-12-01-17:07:11-root-INFO: Loss too large (1028.696->1086.788)! Learning rate decreased to 0.00636.
2024-12-01-17:07:11-root-INFO: Loss too large (1028.696->1058.057)! Learning rate decreased to 0.00509.
2024-12-01-17:07:11-root-INFO: Loss too large (1028.696->1040.474)! Learning rate decreased to 0.00407.
2024-12-01-17:07:11-root-INFO: Loss too large (1028.696->1030.778)! Learning rate decreased to 0.00326.
2024-12-01-17:07:12-root-INFO: grad norm: 110.440 109.169 16.710
2024-12-01-17:07:12-root-INFO: Loss too large (1025.914->1032.910)! Learning rate decreased to 0.00261.
2024-12-01-17:07:12-root-INFO: Loss too large (1025.914->1025.965)! Learning rate decreased to 0.00208.
2024-12-01-17:07:12-root-INFO: grad norm: 82.628 81.353 14.461
2024-12-01-17:07:13-root-INFO: grad norm: 54.039 52.947 10.809
2024-12-01-17:07:13-root-INFO: grad norm: 45.767 44.474 10.803
2024-12-01-17:07:14-root-INFO: grad norm: 39.823 38.626 9.688
2024-12-01-17:07:14-root-INFO: grad norm: 37.166 35.840 9.839
2024-12-01-17:07:15-root-INFO: grad norm: 35.496 34.238 9.365
2024-12-01-17:07:15-root-INFO: Loss Change: 1028.696 -> 1002.369
2024-12-01-17:07:15-root-INFO: Regularization Change: 0.000 -> 0.423
2024-12-01-17:07:15-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-01-17:07:15-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-17:07:15-root-INFO: step: 156 lr_xt 0.00825448
2024-12-01-17:07:16-root-INFO: grad norm: 57.828 56.513 12.262
2024-12-01-17:07:16-root-INFO: Loss too large (1002.184->1033.563)! Learning rate decreased to 0.00660.
2024-12-01-17:07:16-root-INFO: Loss too large (1002.184->1016.292)! Learning rate decreased to 0.00528.
2024-12-01-17:07:16-root-INFO: Loss too large (1002.184->1006.626)! Learning rate decreased to 0.00423.
2024-12-01-17:07:17-root-INFO: grad norm: 117.243 115.965 17.264
2024-12-01-17:07:17-root-INFO: Loss too large (1001.605->1023.139)! Learning rate decreased to 0.00338.
2024-12-01-17:07:17-root-INFO: Loss too large (1001.605->1010.641)! Learning rate decreased to 0.00270.
2024-12-01-17:07:17-root-INFO: Loss too large (1001.605->1002.588)! Learning rate decreased to 0.00216.
2024-12-01-17:07:18-root-INFO: grad norm: 83.810 82.718 13.485
2024-12-01-17:07:18-root-INFO: grad norm: 49.049 47.963 10.264
2024-12-01-17:07:19-root-INFO: grad norm: 41.508 40.308 9.911
2024-12-01-17:07:19-root-INFO: grad norm: 36.598 35.381 9.360
2024-12-01-17:07:19-root-INFO: grad norm: 34.634 33.357 9.320
2024-12-01-17:07:20-root-INFO: grad norm: 33.515 32.247 9.133
2024-12-01-17:07:20-root-INFO: Loss Change: 1002.184 -> 978.414
2024-12-01-17:07:20-root-INFO: Regularization Change: 0.000 -> 0.432
2024-12-01-17:07:20-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-01-17:07:20-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-17:07:21-root-INFO: step: 155 lr_xt 0.00856705
2024-12-01-17:07:21-root-INFO: grad norm: 59.408 58.050 12.628
2024-12-01-17:07:21-root-INFO: Loss too large (980.578->1012.027)! Learning rate decreased to 0.00685.
2024-12-01-17:07:21-root-INFO: Loss too large (980.578->994.164)! Learning rate decreased to 0.00548.
2024-12-01-17:07:21-root-INFO: Loss too large (980.578->984.287)! Learning rate decreased to 0.00439.
2024-12-01-17:07:22-root-INFO: grad norm: 117.178 115.873 17.439
2024-12-01-17:07:22-root-INFO: Loss too large (979.237->1001.419)! Learning rate decreased to 0.00351.
2024-12-01-17:07:22-root-INFO: Loss too large (979.237->988.749)! Learning rate decreased to 0.00281.
2024-12-01-17:07:22-root-INFO: Loss too large (979.237->980.529)! Learning rate decreased to 0.00225.
2024-12-01-17:07:23-root-INFO: grad norm: 84.110 83.021 13.490
2024-12-01-17:07:23-root-INFO: grad norm: 48.145 47.067 10.133
2024-12-01-17:07:24-root-INFO: grad norm: 40.964 39.768 9.829
2024-12-01-17:07:24-root-INFO: grad norm: 36.158 34.956 9.246
2024-12-01-17:07:25-root-INFO: grad norm: 34.210 32.939 9.236
2024-12-01-17:07:25-root-INFO: grad norm: 33.077 31.822 9.025
2024-12-01-17:07:25-root-INFO: Loss Change: 980.578 -> 955.611
2024-12-01-17:07:25-root-INFO: Regularization Change: 0.000 -> 0.467
2024-12-01-17:07:25-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-01-17:07:25-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-17:07:26-root-INFO: step: 154 lr_xt 0.00889002
2024-12-01-17:07:26-root-INFO: grad norm: 57.973 56.668 12.235
2024-12-01-17:07:26-root-INFO: Loss too large (956.761->989.278)! Learning rate decreased to 0.00711.
2024-12-01-17:07:26-root-INFO: Loss too large (956.761->970.750)! Learning rate decreased to 0.00569.
2024-12-01-17:07:26-root-INFO: Loss too large (956.761->960.605)! Learning rate decreased to 0.00455.
2024-12-01-17:07:27-root-INFO: grad norm: 113.016 111.778 16.680
2024-12-01-17:07:27-root-INFO: Loss too large (955.468->974.934)! Learning rate decreased to 0.00364.
2024-12-01-17:07:27-root-INFO: Loss too large (955.468->963.347)! Learning rate decreased to 0.00291.
2024-12-01-17:07:27-root-INFO: Loss too large (955.468->955.867)! Learning rate decreased to 0.00233.
2024-12-01-17:07:28-root-INFO: grad norm: 77.557 76.501 12.753
2024-12-01-17:07:28-root-INFO: grad norm: 42.310 41.180 9.717
2024-12-01-17:07:29-root-INFO: grad norm: 36.686 35.434 9.503
2024-12-01-17:07:29-root-INFO: grad norm: 33.655 32.394 9.125
2024-12-01-17:07:30-root-INFO: grad norm: 32.548 31.250 9.101
2024-12-01-17:07:30-root-INFO: grad norm: 31.979 30.695 8.971
2024-12-01-17:07:30-root-INFO: Loss Change: 956.761 -> 932.018
2024-12-01-17:07:30-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-01-17:07:30-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-01-17:07:30-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-17:07:31-root-INFO: step: 153 lr_xt 0.00922367
2024-12-01-17:07:31-root-INFO: grad norm: 56.652 55.322 12.206
2024-12-01-17:07:31-root-INFO: Loss too large (932.985->956.676)! Learning rate decreased to 0.00738.
2024-12-01-17:07:31-root-INFO: Loss too large (932.985->941.528)! Learning rate decreased to 0.00590.
2024-12-01-17:07:31-root-INFO: Loss too large (932.985->933.601)! Learning rate decreased to 0.00472.
2024-12-01-17:07:32-root-INFO: grad norm: 95.681 94.549 14.674
2024-12-01-17:07:32-root-INFO: Loss too large (929.801->940.863)! Learning rate decreased to 0.00378.
2024-12-01-17:07:32-root-INFO: Loss too large (929.801->932.888)! Learning rate decreased to 0.00302.
2024-12-01-17:07:33-root-INFO: grad norm: 81.546 80.515 12.922
2024-12-01-17:07:33-root-INFO: grad norm: 59.156 58.167 10.774
2024-12-01-17:07:34-root-INFO: grad norm: 53.758 52.715 10.539
2024-12-01-17:07:34-root-INFO: grad norm: 47.046 46.044 9.658
2024-12-01-17:07:35-root-INFO: grad norm: 44.028 42.947 9.697
2024-12-01-17:07:35-root-INFO: grad norm: 40.703 39.667 9.123
2024-12-01-17:07:35-root-INFO: Loss Change: 932.985 -> 903.954
2024-12-01-17:07:35-root-INFO: Regularization Change: 0.000 -> 0.711
2024-12-01-17:07:35-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-01-17:07:35-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-17:07:36-root-INFO: step: 152 lr_xt 0.00956831
2024-12-01-17:07:36-root-INFO: grad norm: 57.787 56.735 10.978
2024-12-01-17:07:36-root-INFO: Loss too large (904.445->943.630)! Learning rate decreased to 0.00765.
2024-12-01-17:07:36-root-INFO: Loss too large (904.445->921.153)! Learning rate decreased to 0.00612.
2024-12-01-17:07:36-root-INFO: Loss too large (904.445->909.075)! Learning rate decreased to 0.00490.
2024-12-01-17:07:37-root-INFO: grad norm: 105.657 104.470 15.788
2024-12-01-17:07:37-root-INFO: Loss too large (903.112->917.464)! Learning rate decreased to 0.00392.
2024-12-01-17:07:37-root-INFO: Loss too large (903.112->907.743)! Learning rate decreased to 0.00314.
2024-12-01-17:07:38-root-INFO: grad norm: 83.176 82.246 12.405
2024-12-01-17:07:38-root-INFO: grad norm: 49.120 48.127 9.826
2024-12-01-17:07:39-root-INFO: grad norm: 43.618 42.592 9.406
2024-12-01-17:07:39-root-INFO: grad norm: 38.066 37.014 8.889
2024-12-01-17:07:40-root-INFO: grad norm: 35.537 34.431 8.798
2024-12-01-17:07:40-root-INFO: grad norm: 33.292 32.188 8.503
2024-12-01-17:07:40-root-INFO: Loss Change: 904.445 -> 876.825
2024-12-01-17:07:40-root-INFO: Regularization Change: 0.000 -> 0.686
2024-12-01-17:07:40-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-01-17:07:40-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-17:07:40-root-INFO: step: 151 lr_xt 0.00992422
2024-12-01-17:07:41-root-INFO: grad norm: 56.240 55.079 11.369
2024-12-01-17:07:41-root-INFO: Loss too large (878.168->906.698)! Learning rate decreased to 0.00794.
2024-12-01-17:07:41-root-INFO: Loss too large (878.168->888.505)! Learning rate decreased to 0.00635.
2024-12-01-17:07:41-root-INFO: Loss too large (878.168->879.223)! Learning rate decreased to 0.00508.
2024-12-01-17:07:42-root-INFO: grad norm: 86.970 85.981 13.078
2024-12-01-17:07:42-root-INFO: Loss too large (874.873->881.908)! Learning rate decreased to 0.00406.
2024-12-01-17:07:42-root-INFO: Loss too large (874.873->875.659)! Learning rate decreased to 0.00325.
2024-12-01-17:07:42-root-INFO: grad norm: 66.066 65.106 11.226
2024-12-01-17:07:43-root-INFO: grad norm: 40.413 39.438 8.824
2024-12-01-17:07:43-root-INFO: grad norm: 35.515 34.406 8.804
2024-12-01-17:07:44-root-INFO: grad norm: 31.668 30.582 8.223
2024-12-01-17:07:44-root-INFO: grad norm: 29.958 28.789 8.284
2024-12-01-17:07:45-root-INFO: grad norm: 28.755 27.618 8.006
2024-12-01-17:07:45-root-INFO: Loss Change: 878.168 -> 851.355
2024-12-01-17:07:45-root-INFO: Regularization Change: 0.000 -> 0.683
2024-12-01-17:07:45-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-01-17:07:45-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-17:07:45-root-INFO: step: 150 lr_xt 0.01029171
2024-12-01-17:07:45-root-INFO: grad norm: 39.128 37.992 9.361
2024-12-01-17:07:45-root-INFO: Loss too large (851.422->853.973)! Learning rate decreased to 0.00823.
2024-12-01-17:07:46-root-INFO: grad norm: 95.042 94.067 13.584
2024-12-01-17:07:46-root-INFO: Loss too large (849.960->888.330)! Learning rate decreased to 0.00659.
2024-12-01-17:07:46-root-INFO: Loss too large (849.960->870.833)! Learning rate decreased to 0.00527.
2024-12-01-17:07:46-root-INFO: Loss too large (849.960->859.130)! Learning rate decreased to 0.00422.
2024-12-01-17:07:47-root-INFO: Loss too large (849.960->851.539)! Learning rate decreased to 0.00337.
2024-12-01-17:07:47-root-INFO: grad norm: 65.975 65.091 10.766
2024-12-01-17:07:47-root-INFO: grad norm: 33.206 32.232 7.987
2024-12-01-17:07:48-root-INFO: grad norm: 29.386 28.276 7.999
2024-12-01-17:07:49-root-INFO: grad norm: 27.060 25.964 7.621
2024-12-01-17:07:49-root-INFO: grad norm: 26.083 24.935 7.654
2024-12-01-17:07:50-root-INFO: grad norm: 25.481 24.355 7.492
2024-12-01-17:07:50-root-INFO: Loss Change: 851.422 -> 827.858
2024-12-01-17:07:50-root-INFO: Regularization Change: 0.000 -> 0.728
2024-12-01-17:07:50-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-01-17:07:50-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-17:07:50-root-INFO: step: 149 lr_xt 0.01067108
2024-12-01-17:07:50-root-INFO: grad norm: 43.015 41.909 9.691
2024-12-01-17:07:50-root-INFO: Loss too large (828.579->836.083)! Learning rate decreased to 0.00854.
2024-12-01-17:07:51-root-INFO: Loss too large (828.579->829.223)! Learning rate decreased to 0.00683.
2024-12-01-17:07:51-root-INFO: grad norm: 74.467 73.596 11.353
2024-12-01-17:07:51-root-INFO: Loss too large (825.990->836.721)! Learning rate decreased to 0.00546.
2024-12-01-17:07:51-root-INFO: Loss too large (825.990->829.599)! Learning rate decreased to 0.00437.
2024-12-01-17:07:52-root-INFO: grad norm: 64.720 63.857 10.537
2024-12-01-17:07:52-root-INFO: grad norm: 47.357 46.545 8.734
2024-12-01-17:07:53-root-INFO: grad norm: 43.786 42.888 8.823
2024-12-01-17:07:53-root-INFO: grad norm: 38.521 37.690 7.957
2024-12-01-17:07:54-root-INFO: grad norm: 36.318 35.382 8.193
2024-12-01-17:07:54-root-INFO: grad norm: 33.454 32.594 7.535
2024-12-01-17:07:54-root-INFO: Loss Change: 828.579 -> 804.756
2024-12-01-17:07:54-root-INFO: Regularization Change: 0.000 -> 0.858
2024-12-01-17:07:54-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-01-17:07:54-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-17:07:54-root-INFO: step: 148 lr_xt 0.01106266
2024-12-01-17:07:55-root-INFO: grad norm: 50.094 49.098 9.936
2024-12-01-17:07:55-root-INFO: Loss too large (807.451->826.263)! Learning rate decreased to 0.00885.
2024-12-01-17:07:55-root-INFO: Loss too large (807.451->812.198)! Learning rate decreased to 0.00708.
2024-12-01-17:07:55-root-INFO: grad norm: 85.692 84.820 12.192
2024-12-01-17:07:56-root-INFO: Loss too large (805.618->818.575)! Learning rate decreased to 0.00566.
2024-12-01-17:07:56-root-INFO: Loss too large (805.618->809.774)! Learning rate decreased to 0.00453.
2024-12-01-17:07:56-root-INFO: grad norm: 62.868 62.055 10.081
2024-12-01-17:07:57-root-INFO: grad norm: 30.445 29.558 7.298
2024-12-01-17:07:57-root-INFO: grad norm: 27.054 26.013 7.434
2024-12-01-17:07:58-root-INFO: grad norm: 24.471 23.466 6.941
2024-12-01-17:07:58-root-INFO: grad norm: 23.328 22.236 7.054
2024-12-01-17:07:59-root-INFO: grad norm: 22.535 21.485 6.799
2024-12-01-17:07:59-root-INFO: Loss Change: 807.451 -> 783.583
2024-12-01-17:07:59-root-INFO: Regularization Change: 0.000 -> 0.816
2024-12-01-17:07:59-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-01-17:07:59-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-17:07:59-root-INFO: step: 147 lr_xt 0.01146675
2024-12-01-17:07:59-root-INFO: grad norm: 33.243 32.202 8.254
2024-12-01-17:08:00-root-INFO: grad norm: 69.108 68.401 9.864
2024-12-01-17:08:00-root-INFO: Loss too large (780.990->807.792)! Learning rate decreased to 0.00917.
2024-12-01-17:08:00-root-INFO: Loss too large (780.990->794.150)! Learning rate decreased to 0.00734.
2024-12-01-17:08:00-root-INFO: Loss too large (780.990->785.499)! Learning rate decreased to 0.00587.
2024-12-01-17:08:01-root-INFO: grad norm: 59.507 58.735 9.550
2024-12-01-17:08:01-root-INFO: grad norm: 44.770 44.105 7.693
2024-12-01-17:08:02-root-INFO: grad norm: 41.973 41.193 8.052
2024-12-01-17:08:02-root-INFO: grad norm: 37.324 36.647 7.073
2024-12-01-17:08:03-root-INFO: grad norm: 35.678 34.881 7.500
2024-12-01-17:08:03-root-INFO: grad norm: 33.185 32.495 6.735
2024-12-01-17:08:03-root-INFO: Loss Change: 783.138 -> 758.991
2024-12-01-17:08:03-root-INFO: Regularization Change: 0.000 -> 1.275
2024-12-01-17:08:03-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-01-17:08:03-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-17:08:04-root-INFO: step: 146 lr_xt 0.01188369
2024-12-01-17:08:04-root-INFO: grad norm: 45.516 44.746 8.334
2024-12-01-17:08:04-root-INFO: Loss too large (760.610->769.980)! Learning rate decreased to 0.00951.
2024-12-01-17:08:04-root-INFO: grad norm: 89.353 88.533 12.073
2024-12-01-17:08:04-root-INFO: Loss too large (760.526->783.609)! Learning rate decreased to 0.00761.
2024-12-01-17:08:05-root-INFO: Loss too large (760.526->770.080)! Learning rate decreased to 0.00608.
2024-12-01-17:08:05-root-INFO: Loss too large (760.526->761.510)! Learning rate decreased to 0.00487.
2024-12-01-17:08:05-root-INFO: grad norm: 51.073 50.389 8.326
2024-12-01-17:08:06-root-INFO: grad norm: 20.451 19.495 6.181
2024-12-01-17:08:06-root-INFO: grad norm: 20.099 19.133 6.155
2024-12-01-17:08:07-root-INFO: grad norm: 19.945 18.991 6.093
2024-12-01-17:08:07-root-INFO: grad norm: 19.825 18.876 6.060
2024-12-01-17:08:08-root-INFO: grad norm: 19.719 18.778 6.018
2024-12-01-17:08:08-root-INFO: Loss Change: 760.610 -> 738.948
2024-12-01-17:08:08-root-INFO: Regularization Change: 0.000 -> 0.816
2024-12-01-17:08:08-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-01-17:08:08-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-17:08:08-root-INFO: step: 145 lr_xt 0.01231381
2024-12-01-17:08:08-root-INFO: grad norm: 33.245 32.304 7.853
2024-12-01-17:08:09-root-INFO: grad norm: 51.409 50.809 7.833
2024-12-01-17:08:09-root-INFO: Loss too large (737.501->745.316)! Learning rate decreased to 0.00985.
2024-12-01-17:08:09-root-INFO: Loss too large (737.501->738.516)! Learning rate decreased to 0.00788.
2024-12-01-17:08:10-root-INFO: grad norm: 46.510 45.778 8.215
2024-12-01-17:08:10-root-INFO: grad norm: 44.476 43.883 7.237
2024-12-01-17:08:11-root-INFO: grad norm: 42.974 42.262 7.791
2024-12-01-17:08:11-root-INFO: grad norm: 40.876 40.297 6.859
2024-12-01-17:08:12-root-INFO: grad norm: 39.825 39.120 7.463
2024-12-01-17:08:12-root-INFO: grad norm: 37.999 37.429 6.559
2024-12-01-17:08:12-root-INFO: Loss Change: 740.549 -> 715.344
2024-12-01-17:08:12-root-INFO: Regularization Change: 0.000 -> 1.785
2024-12-01-17:08:12-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-01-17:08:12-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-17:08:13-root-INFO: step: 144 lr_xt 0.01275743
2024-12-01-17:08:13-root-INFO: grad norm: 46.404 45.694 8.087
2024-12-01-17:08:13-root-INFO: Loss too large (716.777->720.293)! Learning rate decreased to 0.01021.
2024-12-01-17:08:13-root-INFO: grad norm: 61.131 60.496 8.794
2024-12-01-17:08:14-root-INFO: Loss too large (713.078->717.717)! Learning rate decreased to 0.00816.
2024-12-01-17:08:14-root-INFO: grad norm: 51.157 50.489 8.239
2024-12-01-17:08:15-root-INFO: grad norm: 34.269 33.682 6.318
2024-12-01-17:08:15-root-INFO: grad norm: 30.543 29.803 6.683
2024-12-01-17:08:16-root-INFO: grad norm: 26.701 26.080 5.725
2024-12-01-17:08:16-root-INFO: grad norm: 24.929 24.151 6.181
2024-12-01-17:08:16-root-INFO: grad norm: 23.245 22.592 5.472
2024-12-01-17:08:17-root-INFO: Loss Change: 716.777 -> 688.486
2024-12-01-17:08:17-root-INFO: Regularization Change: 0.000 -> 1.625
2024-12-01-17:08:17-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-01-17:08:17-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-17:08:17-root-INFO: step: 143 lr_xt 0.01321490
2024-12-01-17:08:17-root-INFO: grad norm: 32.504 31.814 6.666
2024-12-01-17:08:18-root-INFO: grad norm: 52.262 51.678 7.797
2024-12-01-17:08:18-root-INFO: Loss too large (686.644->694.939)! Learning rate decreased to 0.01057.
2024-12-01-17:08:18-root-INFO: Loss too large (686.644->687.294)! Learning rate decreased to 0.00846.
2024-12-01-17:08:18-root-INFO: grad norm: 41.873 41.205 7.447
2024-12-01-17:08:19-root-INFO: grad norm: 30.627 30.050 5.917
2024-12-01-17:08:19-root-INFO: grad norm: 26.948 26.208 6.271
2024-12-01-17:08:20-root-INFO: grad norm: 24.169 23.554 5.417
2024-12-01-17:08:20-root-INFO: grad norm: 22.729 21.957 5.876
2024-12-01-17:08:21-root-INFO: grad norm: 21.760 21.125 5.217
2024-12-01-17:08:21-root-INFO: Loss Change: 688.203 -> 663.928
2024-12-01-17:08:21-root-INFO: Regularization Change: 0.000 -> 1.656
2024-12-01-17:08:21-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-01-17:08:21-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-17:08:21-root-INFO: step: 142 lr_xt 0.01368658
2024-12-01-17:08:21-root-INFO: grad norm: 35.946 35.176 7.400
2024-12-01-17:08:22-root-INFO: grad norm: 48.500 47.933 7.396
2024-12-01-17:08:22-root-INFO: Loss too large (664.387->668.754)! Learning rate decreased to 0.01095.
2024-12-01-17:08:23-root-INFO: grad norm: 54.745 53.949 9.306
2024-12-01-17:08:23-root-INFO: grad norm: 54.502 53.891 8.140
2024-12-01-17:08:24-root-INFO: grad norm: 49.838 49.155 8.226
2024-12-01-17:08:24-root-INFO: grad norm: 52.682 52.095 7.837
2024-12-01-17:08:24-root-INFO: grad norm: 56.810 56.056 9.222
2024-12-01-17:08:25-root-INFO: grad norm: 56.567 55.948 8.348
2024-12-01-17:08:25-root-INFO: Loss Change: 665.735 -> 645.673
2024-12-01-17:08:25-root-INFO: Regularization Change: 0.000 -> 2.367
2024-12-01-17:08:25-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-01-17:08:25-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-17:08:25-root-INFO: step: 141 lr_xt 0.01417280
2024-12-01-17:08:26-root-INFO: grad norm: 66.464 65.732 9.838
2024-12-01-17:08:26-root-INFO: Loss too large (648.521->656.594)! Learning rate decreased to 0.01134.
2024-12-01-17:08:26-root-INFO: grad norm: 59.211 58.603 8.463
2024-12-01-17:08:27-root-INFO: grad norm: 51.933 51.301 8.078
2024-12-01-17:08:27-root-INFO: grad norm: 48.988 48.444 7.281
2024-12-01-17:08:28-root-INFO: grad norm: 49.643 48.956 8.233
2024-12-01-17:08:28-root-INFO: grad norm: 47.101 46.541 7.242
2024-12-01-17:08:29-root-INFO: grad norm: 44.713 44.054 7.649
2024-12-01-17:08:29-root-INFO: grad norm: 43.787 43.237 6.921
2024-12-01-17:08:29-root-INFO: Loss Change: 648.521 -> 618.759
2024-12-01-17:08:30-root-INFO: Regularization Change: 0.000 -> 2.022
2024-12-01-17:08:30-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-01-17:08:30-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-17:08:30-root-INFO: step: 140 lr_xt 0.01467393
2024-12-01-17:08:30-root-INFO: grad norm: 60.813 60.097 9.304
2024-12-01-17:08:30-root-INFO: Loss too large (621.936->631.692)! Learning rate decreased to 0.01174.
2024-12-01-17:08:30-root-INFO: grad norm: 55.166 54.582 8.000
2024-12-01-17:08:31-root-INFO: grad norm: 45.310 44.763 7.018
2024-12-01-17:08:31-root-INFO: grad norm: 44.338 43.836 6.657
2024-12-01-17:08:32-root-INFO: grad norm: 48.005 47.355 7.871
2024-12-01-17:08:32-root-INFO: grad norm: 46.183 45.635 7.093
2024-12-01-17:08:33-root-INFO: grad norm: 43.516 42.901 7.287
2024-12-01-17:08:33-root-INFO: grad norm: 43.248 42.711 6.794
2024-12-01-17:08:34-root-INFO: Loss Change: 621.936 -> 597.059
2024-12-01-17:08:34-root-INFO: Regularization Change: 0.000 -> 1.932
2024-12-01-17:08:34-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-01-17:08:34-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-17:08:34-root-INFO: step: 139 lr_xt 0.01519033
2024-12-01-17:08:34-root-INFO: grad norm: 57.739 57.065 8.796
2024-12-01-17:08:34-root-INFO: Loss too large (600.367->609.723)! Learning rate decreased to 0.01215.
2024-12-01-17:08:35-root-INFO: grad norm: 53.481 52.930 7.657
2024-12-01-17:08:35-root-INFO: grad norm: 45.493 44.985 6.781
2024-12-01-17:08:36-root-INFO: grad norm: 44.582 44.093 6.580
2024-12-01-17:08:36-root-INFO: grad norm: 47.788 47.175 7.630
2024-12-01-17:08:37-root-INFO: grad norm: 45.926 45.392 6.985
2024-12-01-17:08:37-root-INFO: grad norm: 43.167 42.597 6.993
2024-12-01-17:08:38-root-INFO: grad norm: 42.994 42.471 6.682
2024-12-01-17:08:38-root-INFO: Loss Change: 600.367 -> 577.354
2024-12-01-17:08:38-root-INFO: Regularization Change: 0.000 -> 1.885
2024-12-01-17:08:38-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-01-17:08:38-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-17:08:38-root-INFO: step: 138 lr_xt 0.01572237
2024-12-01-17:08:38-root-INFO: grad norm: 55.936 55.312 8.333
2024-12-01-17:08:38-root-INFO: Loss too large (579.548->589.238)! Learning rate decreased to 0.01258.
2024-12-01-17:08:39-root-INFO: Loss too large (579.548->580.255)! Learning rate decreased to 0.01006.
2024-12-01-17:08:39-root-INFO: grad norm: 38.322 37.772 6.469
2024-12-01-17:08:40-root-INFO: grad norm: 23.076 22.547 4.911
2024-12-01-17:08:40-root-INFO: grad norm: 20.284 19.777 4.507
2024-12-01-17:08:41-root-INFO: grad norm: 19.130 18.543 4.703
2024-12-01-17:08:41-root-INFO: grad norm: 18.807 18.290 4.375
2024-12-01-17:08:42-root-INFO: grad norm: 18.985 18.390 4.717
2024-12-01-17:08:42-root-INFO: grad norm: 19.255 18.750 4.379
2024-12-01-17:08:42-root-INFO: Loss Change: 579.548 -> 557.285
2024-12-01-17:08:42-root-INFO: Regularization Change: 0.000 -> 1.265
2024-12-01-17:08:42-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-01-17:08:42-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-17:08:42-root-INFO: step: 137 lr_xt 0.01627042
2024-12-01-17:08:43-root-INFO: grad norm: 36.910 36.282 6.777
2024-12-01-17:08:43-root-INFO: Loss too large (558.942->561.613)! Learning rate decreased to 0.01302.
2024-12-01-17:08:43-root-INFO: grad norm: 38.514 37.932 6.666
2024-12-01-17:08:44-root-INFO: grad norm: 48.666 47.956 8.286
2024-12-01-17:08:44-root-INFO: Loss too large (554.989->558.660)! Learning rate decreased to 0.01041.
2024-12-01-17:08:44-root-INFO: grad norm: 37.426 36.854 6.517
2024-12-01-17:08:45-root-INFO: grad norm: 21.033 20.451 4.914
2024-12-01-17:08:45-root-INFO: grad norm: 19.773 19.267 4.445
2024-12-01-17:08:46-root-INFO: grad norm: 19.684 19.102 4.754
2024-12-01-17:08:46-root-INFO: grad norm: 19.865 19.367 4.421
2024-12-01-17:08:47-root-INFO: Loss Change: 558.942 -> 540.143
2024-12-01-17:08:47-root-INFO: Regularization Change: 0.000 -> 1.451
2024-12-01-17:08:47-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-01-17:08:47-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-17:08:47-root-INFO: step: 136 lr_xt 0.01683487
2024-12-01-17:08:47-root-INFO: grad norm: 29.028 28.493 5.548
2024-12-01-17:08:47-root-INFO: Loss too large (540.517->542.907)! Learning rate decreased to 0.01347.
2024-12-01-17:08:47-root-INFO: Loss too large (540.517->540.550)! Learning rate decreased to 0.01077.
2024-12-01-17:08:48-root-INFO: grad norm: 27.688 27.176 5.298
2024-12-01-17:08:48-root-INFO: grad norm: 27.433 26.860 5.576
2024-12-01-17:08:49-root-INFO: grad norm: 27.177 26.679 5.181
2024-12-01-17:08:49-root-INFO: grad norm: 26.930 26.352 5.550
2024-12-01-17:08:50-root-INFO: grad norm: 26.802 26.310 5.112
2024-12-01-17:08:50-root-INFO: grad norm: 26.648 26.071 5.516
2024-12-01-17:08:51-root-INFO: grad norm: 26.593 26.105 5.071
2024-12-01-17:08:51-root-INFO: Loss Change: 540.517 -> 526.113
2024-12-01-17:08:51-root-INFO: Regularization Change: 0.000 -> 1.210
2024-12-01-17:08:51-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-01-17:08:51-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-17:08:52-root-INFO: step: 135 lr_xt 0.01741608
2024-12-01-17:08:52-root-INFO: grad norm: 34.080 33.566 5.898
2024-12-01-17:08:52-root-INFO: Loss too large (526.650->531.380)! Learning rate decreased to 0.01393.
2024-12-01-17:08:52-root-INFO: Loss too large (526.650->527.659)! Learning rate decreased to 0.01115.
2024-12-01-17:08:53-root-INFO: grad norm: 31.419 30.879 5.804
2024-12-01-17:08:53-root-INFO: grad norm: 29.466 28.915 5.674
2024-12-01-17:08:54-root-INFO: grad norm: 28.516 28.009 5.352
2024-12-01-17:08:54-root-INFO: grad norm: 27.203 26.646 5.475
2024-12-01-17:08:55-root-INFO: grad norm: 26.846 26.355 5.109
2024-12-01-17:08:55-root-INFO: grad norm: 26.414 25.857 5.398
2024-12-01-17:08:56-root-INFO: grad norm: 26.281 25.796 5.022
2024-12-01-17:08:56-root-INFO: Loss Change: 526.650 -> 511.688
2024-12-01-17:08:56-root-INFO: Regularization Change: 0.000 -> 1.229
2024-12-01-17:08:56-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-01-17:08:56-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-17:08:56-root-INFO: step: 134 lr_xt 0.01801447
2024-12-01-17:08:56-root-INFO: grad norm: 36.693 36.139 6.355
2024-12-01-17:08:57-root-INFO: Loss too large (512.291->518.483)! Learning rate decreased to 0.01441.
2024-12-01-17:08:57-root-INFO: Loss too large (512.291->514.032)! Learning rate decreased to 0.01153.
2024-12-01-17:08:57-root-INFO: grad norm: 32.549 32.024 5.826
2024-12-01-17:08:58-root-INFO: grad norm: 27.999 27.465 5.445
2024-12-01-17:08:58-root-INFO: grad norm: 27.256 26.772 5.112
2024-12-01-17:08:59-root-INFO: grad norm: 26.894 26.349 5.388
2024-12-01-17:08:59-root-INFO: grad norm: 26.576 26.092 5.050
2024-12-01-17:09:00-root-INFO: grad norm: 26.241 25.698 5.308
2024-12-01-17:09:00-root-INFO: grad norm: 26.134 25.653 4.988
2024-12-01-17:09:01-root-INFO: Loss Change: 512.291 -> 497.350
2024-12-01-17:09:01-root-INFO: Regularization Change: 0.000 -> 1.226
2024-12-01-17:09:01-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-01-17:09:01-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-17:09:01-root-INFO: step: 133 lr_xt 0.01863041
2024-12-01-17:09:01-root-INFO: grad norm: 35.444 34.938 5.967
2024-12-01-17:09:01-root-INFO: Loss too large (498.519->504.885)! Learning rate decreased to 0.01490.
2024-12-01-17:09:02-root-INFO: Loss too large (498.519->500.315)! Learning rate decreased to 0.01192.
2024-12-01-17:09:03-root-INFO: grad norm: 32.446 31.914 5.847
2024-12-01-17:09:04-root-INFO: grad norm: 29.946 29.428 5.543
2024-12-01-17:09:05-root-INFO: grad norm: 28.951 28.451 5.357
2024-12-01-17:09:06-root-INFO: grad norm: 27.778 27.255 5.363
2024-12-01-17:09:07-root-INFO: grad norm: 27.368 26.880 5.149
2024-12-01-17:09:08-root-INFO: grad norm: 26.879 26.357 5.270
2024-12-01-17:09:09-root-INFO: grad norm: 26.705 26.221 5.059
2024-12-01-17:09:09-root-INFO: Loss Change: 498.519 -> 484.197
2024-12-01-17:09:09-root-INFO: Regularization Change: 0.000 -> 1.227
2024-12-01-17:09:09-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-01-17:09:09-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-17:09:10-root-INFO: step: 132 lr_xt 0.01926430
2024-12-01-17:09:10-root-INFO: grad norm: 34.578 34.052 6.008
2024-12-01-17:09:10-root-INFO: Loss too large (484.755->491.329)! Learning rate decreased to 0.01541.
2024-12-01-17:09:11-root-INFO: Loss too large (484.755->487.019)! Learning rate decreased to 0.01233.
2024-12-01-17:09:12-root-INFO: grad norm: 31.609 31.072 5.802
2024-12-01-17:09:13-root-INFO: grad norm: 27.647 27.148 5.230
2024-12-01-17:09:14-root-INFO: grad norm: 27.142 26.653 5.129
2024-12-01-17:09:15-root-INFO: grad norm: 26.980 26.473 5.203
2024-12-01-17:09:16-root-INFO: grad norm: 26.637 26.149 5.079
2024-12-01-17:09:17-root-INFO: grad norm: 26.147 25.646 5.094
2024-12-01-17:09:18-root-INFO: grad norm: 26.005 25.522 4.988
2024-12-01-17:09:19-root-INFO: Loss Change: 484.755 -> 471.239
2024-12-01-17:09:19-root-INFO: Regularization Change: 0.000 -> 1.205
2024-12-01-17:09:19-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-01-17:09:19-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-17:09:19-root-INFO: step: 131 lr_xt 0.01991656
2024-12-01-17:09:19-root-INFO: grad norm: 36.554 36.020 6.223
2024-12-01-17:09:20-root-INFO: Loss too large (472.744->480.206)! Learning rate decreased to 0.01593.
2024-12-01-17:09:20-root-INFO: Loss too large (472.744->475.293)! Learning rate decreased to 0.01275.
2024-12-01-17:09:21-root-INFO: grad norm: 32.255 31.716 5.870
2024-12-01-17:09:22-root-INFO: grad norm: 26.263 25.794 4.941
2024-12-01-17:09:23-root-INFO: grad norm: 25.611 25.142 4.883
2024-12-01-17:09:24-root-INFO: grad norm: 25.825 25.346 4.948
2024-12-01-17:09:25-root-INFO: grad norm: 25.452 24.975 4.906
2024-12-01-17:09:26-root-INFO: grad norm: 25.018 24.544 4.846
2024-12-01-17:09:27-root-INFO: grad norm: 24.850 24.377 4.821
2024-12-01-17:09:28-root-INFO: Loss Change: 472.744 -> 458.828
2024-12-01-17:09:28-root-INFO: Regularization Change: 0.000 -> 1.226
2024-12-01-17:09:28-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-01-17:09:28-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-17:09:28-root-INFO: step: 130 lr_xt 0.02058758
2024-12-01-17:09:28-root-INFO: grad norm: 33.162 32.676 5.655
2024-12-01-17:09:29-root-INFO: Loss too large (459.692->466.473)! Learning rate decreased to 0.01647.
2024-12-01-17:09:29-root-INFO: Loss too large (459.692->462.174)! Learning rate decreased to 0.01318.
2024-12-01-17:09:30-root-INFO: grad norm: 30.303 29.764 5.690
2024-12-01-17:09:31-root-INFO: grad norm: 26.178 25.728 4.831
2024-12-01-17:09:32-root-INFO: grad norm: 25.838 25.358 4.955
2024-12-01-17:09:33-root-INFO: grad norm: 26.039 25.576 4.889
2024-12-01-17:09:34-root-INFO: grad norm: 25.658 25.176 4.951
2024-12-01-17:09:35-root-INFO: grad norm: 25.038 24.580 4.770
2024-12-01-17:09:36-root-INFO: grad norm: 24.845 24.371 4.831
2024-12-01-17:09:37-root-INFO: Loss Change: 459.692 -> 447.251
2024-12-01-17:09:37-root-INFO: Regularization Change: 0.000 -> 1.178
2024-12-01-17:09:37-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-01-17:09:37-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-17:09:37-root-INFO: step: 129 lr_xt 0.02127779
2024-12-01-17:09:38-root-INFO: grad norm: 34.953 34.469 5.796
2024-12-01-17:09:38-root-INFO: Loss too large (448.996->456.524)! Learning rate decreased to 0.01702.
2024-12-01-17:09:38-root-INFO: Loss too large (448.996->451.403)! Learning rate decreased to 0.01362.
2024-12-01-17:09:39-root-INFO: grad norm: 30.759 30.247 5.590
2024-12-01-17:09:40-root-INFO: grad norm: 25.674 25.246 4.667
2024-12-01-17:09:41-root-INFO: grad norm: 24.786 24.332 4.724
2024-12-01-17:09:42-root-INFO: grad norm: 24.566 24.130 4.607
2024-12-01-17:09:43-root-INFO: grad norm: 24.050 23.591 4.681
2024-12-01-17:09:44-root-INFO: grad norm: 23.434 23.003 4.476
2024-12-01-17:09:45-root-INFO: grad norm: 23.167 22.713 4.565
2024-12-01-17:09:46-root-INFO: Loss Change: 448.996 -> 436.004
2024-12-01-17:09:46-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-01-17:09:46-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-01-17:09:46-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-17:09:46-root-INFO: step: 128 lr_xt 0.02198759
2024-12-01-17:09:47-root-INFO: grad norm: 30.705 30.268 5.160
2024-12-01-17:09:47-root-INFO: Loss too large (436.711->442.276)! Learning rate decreased to 0.01759.
2024-12-01-17:09:47-root-INFO: Loss too large (436.711->438.078)! Learning rate decreased to 0.01407.
2024-12-01-17:09:48-root-INFO: grad norm: 27.630 27.135 5.208
2024-12-01-17:09:49-root-INFO: grad norm: 25.202 24.791 4.532
2024-12-01-17:09:50-root-INFO: grad norm: 24.268 23.802 4.729
2024-12-01-17:09:51-root-INFO: grad norm: 23.385 22.974 4.363
2024-12-01-17:09:52-root-INFO: grad norm: 22.909 22.453 4.550
2024-12-01-17:09:53-root-INFO: grad norm: 22.361 21.953 4.249
2024-12-01-17:09:54-root-INFO: grad norm: 22.090 21.640 4.436
2024-12-01-17:09:55-root-INFO: Loss Change: 436.711 -> 424.714
2024-12-01-17:09:55-root-INFO: Regularization Change: 0.000 -> 1.177
2024-12-01-17:09:55-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-01-17:09:55-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-17:09:55-root-INFO: step: 127 lr_xt 0.02271741
2024-12-01-17:09:56-root-INFO: grad norm: 31.365 30.899 5.388
2024-12-01-17:09:56-root-INFO: Loss too large (425.814->432.268)! Learning rate decreased to 0.01817.
2024-12-01-17:09:56-root-INFO: Loss too large (425.814->428.147)! Learning rate decreased to 0.01454.
2024-12-01-17:09:57-root-INFO: grad norm: 27.415 26.909 5.242
2024-12-01-17:09:58-root-INFO: grad norm: 21.400 21.023 3.996
2024-12-01-17:10:00-root-INFO: grad norm: 20.768 20.343 4.183
2024-12-01-17:10:01-root-INFO: grad norm: 20.802 20.421 3.962
2024-12-01-17:10:01-root-INFO: grad norm: 20.531 20.095 4.208
2024-12-01-17:10:02-root-INFO: grad norm: 20.312 19.932 3.911
2024-12-01-17:10:03-root-INFO: grad norm: 20.126 19.691 4.163
2024-12-01-17:10:04-root-INFO: Loss Change: 425.814 -> 414.404
2024-12-01-17:10:04-root-INFO: Regularization Change: 0.000 -> 1.133
2024-12-01-17:10:04-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-01-17:10:04-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-17:10:05-root-INFO: step: 126 lr_xt 0.02346768
2024-12-01-17:10:05-root-INFO: grad norm: 23.877 23.499 4.235
2024-12-01-17:10:05-root-INFO: Loss too large (414.732->417.998)! Learning rate decreased to 0.01877.
2024-12-01-17:10:06-root-INFO: Loss too large (414.732->415.343)! Learning rate decreased to 0.01502.
2024-12-01-17:10:07-root-INFO: grad norm: 22.384 21.933 4.469
2024-12-01-17:10:08-root-INFO: grad norm: 21.577 21.202 4.004
2024-12-01-17:10:09-root-INFO: grad norm: 21.005 20.564 4.280
2024-12-01-17:10:10-root-INFO: grad norm: 20.287 19.915 3.866
2024-12-01-17:10:11-root-INFO: grad norm: 19.964 19.532 4.130
2024-12-01-17:10:12-root-INFO: grad norm: 19.541 19.172 3.779
2024-12-01-17:10:13-root-INFO: grad norm: 19.336 18.908 4.042
2024-12-01-17:10:13-root-INFO: Loss Change: 414.732 -> 404.783
2024-12-01-17:10:13-root-INFO: Regularization Change: 0.000 -> 1.109
2024-12-01-17:10:13-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-01-17:10:13-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-17:10:14-root-INFO: step: 125 lr_xt 0.02423882
2024-12-01-17:10:14-root-INFO: grad norm: 25.173 24.805 4.290
2024-12-01-17:10:14-root-INFO: Loss too large (405.365->409.162)! Learning rate decreased to 0.01939.
2024-12-01-17:10:15-root-INFO: Loss too large (405.365->405.985)! Learning rate decreased to 0.01551.
2024-12-01-17:10:16-root-INFO: grad norm: 22.963 22.489 4.642
2024-12-01-17:10:17-root-INFO: grad norm: 21.840 21.479 3.955
2024-12-01-17:10:18-root-INFO: grad norm: 20.917 20.460 4.349
2024-12-01-17:10:19-root-INFO: grad norm: 19.808 19.449 3.754
2024-12-01-17:10:20-root-INFO: grad norm: 19.319 18.879 4.100
2024-12-01-17:10:21-root-INFO: grad norm: 18.717 18.360 3.636
2024-12-01-17:10:22-root-INFO: grad norm: 18.433 18.002 3.961
2024-12-01-17:10:22-root-INFO: Loss Change: 405.365 -> 395.156
2024-12-01-17:10:22-root-INFO: Regularization Change: 0.000 -> 1.123
2024-12-01-17:10:22-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-01-17:10:22-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-17:10:23-root-INFO: step: 124 lr_xt 0.02515763
2024-12-01-17:10:23-root-INFO: grad norm: 30.644 30.211 5.135
2024-12-01-17:10:23-root-INFO: Loss too large (397.379->403.436)! Learning rate decreased to 0.02013.
2024-12-01-17:10:24-root-INFO: Loss too large (397.379->398.658)! Learning rate decreased to 0.01610.
2024-12-01-17:10:25-root-INFO: grad norm: 26.031 25.521 5.129
2024-12-01-17:10:26-root-INFO: grad norm: 21.884 21.527 3.938
2024-12-01-17:10:27-root-INFO: grad norm: 20.440 19.988 4.275
2024-12-01-17:10:28-root-INFO: grad norm: 19.156 18.806 3.645
2024-12-01-17:10:29-root-INFO: grad norm: 18.480 18.040 4.008
2024-12-01-17:10:30-root-INFO: grad norm: 17.764 17.418 3.487
2024-12-01-17:10:31-root-INFO: grad norm: 17.401 16.970 3.849
2024-12-01-17:10:32-root-INFO: Loss Change: 397.379 -> 385.710
2024-12-01-17:10:32-root-INFO: Regularization Change: 0.000 -> 1.184
2024-12-01-17:10:32-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-01-17:10:32-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-17:10:32-root-INFO: step: 123 lr_xt 0.02597490
2024-12-01-17:10:32-root-INFO: grad norm: 23.010 22.646 4.078
2024-12-01-17:10:33-root-INFO: Loss too large (386.432->390.011)! Learning rate decreased to 0.02078.
2024-12-01-17:10:33-root-INFO: Loss too large (386.432->387.178)! Learning rate decreased to 0.01662.
2024-12-01-17:10:34-root-INFO: grad norm: 21.256 20.783 4.462
2024-12-01-17:10:35-root-INFO: grad norm: 19.893 19.541 3.723
2024-12-01-17:10:36-root-INFO: grad norm: 19.132 18.672 4.170
2024-12-01-17:10:37-root-INFO: grad norm: 18.246 17.900 3.534
2024-12-01-17:10:38-root-INFO: grad norm: 17.811 17.363 3.972
2024-12-01-17:10:39-root-INFO: grad norm: 17.259 16.919 3.413
2024-12-01-17:10:40-root-INFO: grad norm: 16.981 16.541 3.842
2024-12-01-17:10:41-root-INFO: Loss Change: 386.432 -> 377.083
2024-12-01-17:10:41-root-INFO: Regularization Change: 0.000 -> 1.117
2024-12-01-17:10:41-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-01-17:10:41-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-17:10:41-root-INFO: step: 122 lr_xt 0.02681440
2024-12-01-17:10:41-root-INFO: grad norm: 23.618 23.241 4.202
2024-12-01-17:10:42-root-INFO: Loss too large (378.017->381.772)! Learning rate decreased to 0.02145.
2024-12-01-17:10:42-root-INFO: Loss too large (378.017->378.795)! Learning rate decreased to 0.01716.
2024-12-01-17:10:43-root-INFO: grad norm: 21.096 20.602 4.539
2024-12-01-17:10:44-root-INFO: grad norm: 18.659 18.317 3.557
2024-12-01-17:10:45-root-INFO: grad norm: 17.663 17.213 3.962
2024-12-01-17:10:46-root-INFO: grad norm: 16.540 16.210 3.285
2024-12-01-17:10:47-root-INFO: grad norm: 15.993 15.564 3.683
2024-12-01-17:10:48-root-INFO: grad norm: 15.347 15.025 3.128
2024-12-01-17:10:49-root-INFO: grad norm: 15.008 14.591 3.514
2024-12-01-17:10:50-root-INFO: Loss Change: 378.017 -> 368.500
2024-12-01-17:10:50-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-01-17:10:50-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-01-17:10:50-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-17:10:50-root-INFO: step: 121 lr_xt 0.02767658
2024-12-01-17:10:50-root-INFO: grad norm: 20.842 20.490 3.816
2024-12-01-17:10:51-root-INFO: Loss too large (368.997->371.803)! Learning rate decreased to 0.02214.
2024-12-01-17:10:51-root-INFO: Loss too large (368.997->369.357)! Learning rate decreased to 0.01771.
2024-12-01-17:10:52-root-INFO: grad norm: 19.153 18.672 4.269
2024-12-01-17:10:53-root-INFO: grad norm: 18.039 17.698 3.489
2024-12-01-17:10:54-root-INFO: grad norm: 17.322 16.853 4.003
2024-12-01-17:10:55-root-INFO: grad norm: 16.477 16.145 3.294
2024-12-01-17:10:56-root-INFO: grad norm: 16.026 15.574 3.779
2024-12-01-17:10:57-root-INFO: grad norm: 15.451 15.125 3.159
2024-12-01-17:10:58-root-INFO: grad norm: 15.137 14.698 3.619
2024-12-01-17:10:59-root-INFO: Loss Change: 368.997 -> 360.186
2024-12-01-17:10:59-root-INFO: Regularization Change: 0.000 -> 1.121
2024-12-01-17:10:59-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-01-17:10:59-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-17:10:59-root-INFO: step: 120 lr_xt 0.02856188
2024-12-01-17:11:00-root-INFO: grad norm: 20.410 20.057 3.782
2024-12-01-17:11:00-root-INFO: Loss too large (361.060->363.784)! Learning rate decreased to 0.02285.
2024-12-01-17:11:00-root-INFO: Loss too large (361.060->361.352)! Learning rate decreased to 0.01828.
2024-12-01-17:11:01-root-INFO: grad norm: 18.640 18.157 4.212
2024-12-01-17:11:02-root-INFO: grad norm: 17.587 17.242 3.464
2024-12-01-17:11:03-root-INFO: grad norm: 16.883 16.406 3.984
2024-12-01-17:11:04-root-INFO: grad norm: 16.053 15.718 3.264
2024-12-01-17:11:05-root-INFO: grad norm: 15.592 15.132 3.760
2024-12-01-17:11:06-root-INFO: grad norm: 14.999 14.671 3.119
2024-12-01-17:11:07-root-INFO: grad norm: 14.664 14.219 3.589
2024-12-01-17:11:08-root-INFO: Loss Change: 361.060 -> 352.389
2024-12-01-17:11:08-root-INFO: Regularization Change: 0.000 -> 1.133
2024-12-01-17:11:08-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-01-17:11:08-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-17:11:08-root-INFO: step: 119 lr_xt 0.02947075
2024-12-01-17:11:09-root-INFO: grad norm: 20.826 20.451 3.933
2024-12-01-17:11:09-root-INFO: Loss too large (353.063->355.555)! Learning rate decreased to 0.02358.
2024-12-01-17:11:10-root-INFO: grad norm: 24.074 23.492 5.263
2024-12-01-17:11:11-root-INFO: grad norm: 30.355 29.866 5.430
2024-12-01-17:11:11-root-INFO: Loss too large (352.538->354.467)! Learning rate decreased to 0.01886.
2024-12-01-17:11:12-root-INFO: grad norm: 23.539 22.896 5.462
2024-12-01-17:11:13-root-INFO: grad norm: 16.255 15.939 3.191
2024-12-01-17:11:14-root-INFO: grad norm: 14.415 13.977 3.529
2024-12-01-17:11:16-root-INFO: grad norm: 13.017 12.716 2.783
2024-12-01-17:11:17-root-INFO: grad norm: 12.215 11.806 3.135
2024-12-01-17:11:17-root-INFO: Loss Change: 353.063 -> 343.434
2024-12-01-17:11:17-root-INFO: Regularization Change: 0.000 -> 1.323
2024-12-01-17:11:17-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-01-17:11:17-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-17:11:18-root-INFO: step: 118 lr_xt 0.03040366
2024-12-01-17:11:18-root-INFO: grad norm: 16.084 15.742 3.297
2024-12-01-17:11:18-root-INFO: Loss too large (344.157->345.356)! Learning rate decreased to 0.02432.
2024-12-01-17:11:19-root-INFO: grad norm: 19.186 18.639 4.549
2024-12-01-17:11:20-root-INFO: grad norm: 24.550 24.085 4.751
2024-12-01-17:11:21-root-INFO: Loss too large (343.399->344.511)! Learning rate decreased to 0.01946.
2024-12-01-17:11:22-root-INFO: grad norm: 19.973 19.371 4.864
2024-12-01-17:11:23-root-INFO: grad norm: 15.080 14.742 3.176
2024-12-01-17:11:24-root-INFO: grad norm: 13.412 12.975 3.397
2024-12-01-17:11:25-root-INFO: grad norm: 11.871 11.566 2.672
2024-12-01-17:11:26-root-INFO: grad norm: 10.978 10.590 2.890
2024-12-01-17:11:26-root-INFO: Loss Change: 344.157 -> 335.843
2024-12-01-17:11:26-root-INFO: Regularization Change: 0.000 -> 1.262
2024-12-01-17:11:26-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-01-17:11:26-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-17:11:27-root-INFO: step: 117 lr_xt 0.03136105
2024-12-01-17:11:27-root-INFO: grad norm: 16.612 16.290 3.257
2024-12-01-17:11:27-root-INFO: Loss too large (336.599->337.155)! Learning rate decreased to 0.02509.
2024-12-01-17:11:28-root-INFO: grad norm: 18.414 17.984 3.955
2024-12-01-17:11:29-root-INFO: grad norm: 22.731 22.310 4.354
2024-12-01-17:11:30-root-INFO: Loss too large (335.255->335.817)! Learning rate decreased to 0.02007.
2024-12-01-17:11:31-root-INFO: grad norm: 18.422 17.879 4.441
2024-12-01-17:11:32-root-INFO: grad norm: 14.658 14.320 3.131
2024-12-01-17:11:33-root-INFO: grad norm: 12.814 12.385 3.286
2024-12-01-17:11:34-root-INFO: grad norm: 11.213 10.910 2.590
2024-12-01-17:11:35-root-INFO: grad norm: 10.236 9.860 2.751
2024-12-01-17:11:36-root-INFO: Loss Change: 336.599 -> 327.944
2024-12-01-17:11:36-root-INFO: Regularization Change: 0.000 -> 1.298
2024-12-01-17:11:36-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-01-17:11:36-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-17:11:36-root-INFO: step: 116 lr_xt 0.03234339
2024-12-01-17:11:36-root-INFO: grad norm: 14.066 13.754 2.945
2024-12-01-17:11:37-root-INFO: Loss too large (328.162->328.370)! Learning rate decreased to 0.02587.
2024-12-01-17:11:38-root-INFO: grad norm: 15.998 15.535 3.819
2024-12-01-17:11:39-root-INFO: grad norm: 19.886 19.477 4.011
2024-12-01-17:11:39-root-INFO: Loss too large (326.848->327.152)! Learning rate decreased to 0.02070.
2024-12-01-17:11:40-root-INFO: grad norm: 16.449 15.920 4.136
2024-12-01-17:11:41-root-INFO: grad norm: 13.338 13.008 2.950
2024-12-01-17:11:42-root-INFO: grad norm: 11.716 11.300 3.092
2024-12-01-17:11:43-root-INFO: grad norm: 10.284 9.989 2.447
2024-12-01-17:11:44-root-INFO: grad norm: 9.383 9.022 2.580
2024-12-01-17:11:45-root-INFO: Loss Change: 328.162 -> 320.241
2024-12-01-17:11:45-root-INFO: Regularization Change: 0.000 -> 1.275
2024-12-01-17:11:45-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-01-17:11:45-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-17:11:45-root-INFO: step: 115 lr_xt 0.03335113
2024-12-01-17:11:46-root-INFO: grad norm: 13.903 13.598 2.897
2024-12-01-17:11:46-root-INFO: Loss too large (321.062->321.245)! Learning rate decreased to 0.02668.
2024-12-01-17:11:47-root-INFO: grad norm: 15.511 15.099 3.552
2024-12-01-17:11:48-root-INFO: grad norm: 18.812 18.424 3.799
2024-12-01-17:11:48-root-INFO: Loss too large (319.677->319.784)! Learning rate decreased to 0.02134.
2024-12-01-17:11:49-root-INFO: grad norm: 15.360 14.876 3.824
2024-12-01-17:11:50-root-INFO: grad norm: 12.469 12.148 2.815
2024-12-01-17:11:51-root-INFO: grad norm: 10.849 10.460 2.879
2024-12-01-17:11:52-root-INFO: grad norm: 9.493 9.203 2.326
2024-12-01-17:11:53-root-INFO: grad norm: 8.624 8.282 2.403
2024-12-01-17:11:54-root-INFO: Loss Change: 321.062 -> 313.299
2024-12-01-17:11:54-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-01-17:11:54-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-01-17:11:54-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-17:11:54-root-INFO: step: 114 lr_xt 0.03438473
2024-12-01-17:11:55-root-INFO: grad norm: 11.872 11.604 2.509
2024-12-01-17:11:56-root-INFO: grad norm: 17.239 16.826 3.747
2024-12-01-17:11:56-root-INFO: Loss too large (313.303->315.080)! Learning rate decreased to 0.02751.
2024-12-01-17:11:57-root-INFO: grad norm: 19.674 19.311 3.763
2024-12-01-17:11:58-root-INFO: grad norm: 21.451 20.880 4.918
2024-12-01-17:11:59-root-INFO: grad norm: 23.341 22.909 4.466
2024-12-01-17:12:00-root-INFO: grad norm: 24.090 23.439 5.559
2024-12-01-17:12:01-root-INFO: grad norm: 24.475 24.032 4.635
2024-12-01-17:12:02-root-INFO: grad norm: 24.570 23.930 5.571
2024-12-01-17:12:03-root-INFO: Loss Change: 313.653 -> 308.690
2024-12-01-17:12:03-root-INFO: Regularization Change: 0.000 -> 1.880
2024-12-01-17:12:03-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-01-17:12:03-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-17:12:03-root-INFO: step: 113 lr_xt 0.03544467
2024-12-01-17:12:03-root-INFO: grad norm: 29.774 29.298 5.303
2024-12-01-17:12:04-root-INFO: Loss too large (310.434->316.147)! Learning rate decreased to 0.02836.
2024-12-01-17:12:05-root-INFO: grad norm: 28.503 27.890 5.882
2024-12-01-17:12:06-root-INFO: grad norm: 26.878 26.460 4.726
2024-12-01-17:12:07-root-INFO: grad norm: 25.788 25.233 5.325
2024-12-01-17:12:08-root-INFO: grad norm: 24.605 24.209 4.398
2024-12-01-17:12:09-root-INFO: grad norm: 23.762 23.246 4.924
2024-12-01-17:12:10-root-INFO: grad norm: 22.893 22.514 4.150
2024-12-01-17:12:11-root-INFO: grad norm: 22.250 21.767 4.611
2024-12-01-17:12:12-root-INFO: Loss Change: 310.434 -> 300.214
2024-12-01-17:12:12-root-INFO: Regularization Change: 0.000 -> 1.715
2024-12-01-17:12:12-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-01-17:12:12-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-17:12:12-root-INFO: step: 112 lr_xt 0.03653141
2024-12-01-17:12:12-root-INFO: grad norm: 28.077 27.680 4.707
2024-12-01-17:12:13-root-INFO: Loss too large (302.346->306.913)! Learning rate decreased to 0.02923.
2024-12-01-17:12:14-root-INFO: grad norm: 26.367 25.899 4.942
2024-12-01-17:12:15-root-INFO: grad norm: 24.713 24.359 4.168
2024-12-01-17:12:16-root-INFO: grad norm: 23.473 23.042 4.481
2024-12-01-17:12:17-root-INFO: grad norm: 22.315 21.984 3.831
2024-12-01-17:12:18-root-INFO: grad norm: 21.454 21.057 4.107
2024-12-01-17:12:19-root-INFO: grad norm: 20.665 20.353 3.578
2024-12-01-17:12:20-root-INFO: grad norm: 20.066 19.698 3.827
2024-12-01-17:12:21-root-INFO: Loss Change: 302.346 -> 292.213
2024-12-01-17:12:21-root-INFO: Regularization Change: 0.000 -> 1.686
2024-12-01-17:12:21-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-01-17:12:21-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-17:12:21-root-INFO: step: 111 lr_xt 0.03764541
2024-12-01-17:12:21-root-INFO: grad norm: 24.298 23.953 4.077
2024-12-01-17:12:22-root-INFO: Loss too large (293.664->296.894)! Learning rate decreased to 0.03012.
2024-12-01-17:12:23-root-INFO: grad norm: 22.761 22.378 4.158
2024-12-01-17:12:24-root-INFO: grad norm: 21.342 21.038 3.593
2024-12-01-17:12:25-root-INFO: grad norm: 20.258 19.904 3.767
2024-12-01-17:12:26-root-INFO: grad norm: 19.290 19.009 3.281
2024-12-01-17:12:27-root-INFO: grad norm: 18.551 18.228 3.448
2024-12-01-17:12:28-root-INFO: grad norm: 17.898 17.637 3.047
2024-12-01-17:12:29-root-INFO: grad norm: 17.394 17.096 3.205
2024-12-01-17:12:29-root-INFO: Loss Change: 293.664 -> 284.579
2024-12-01-17:12:29-root-INFO: Regularization Change: 0.000 -> 1.639
2024-12-01-17:12:29-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-01-17:12:29-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-17:12:30-root-INFO: step: 110 lr_xt 0.03878715
2024-12-01-17:12:30-root-INFO: grad norm: 21.728 21.441 3.522
2024-12-01-17:12:30-root-INFO: Loss too large (285.914->288.569)! Learning rate decreased to 0.03103.
2024-12-01-17:12:31-root-INFO: grad norm: 20.826 20.540 3.435
2024-12-01-17:12:32-root-INFO: grad norm: 19.977 19.731 3.129
2024-12-01-17:12:33-root-INFO: grad norm: 19.252 18.984 3.203
2024-12-01-17:12:34-root-INFO: grad norm: 18.569 18.340 2.905
2024-12-01-17:12:35-root-INFO: grad norm: 18.015 17.765 2.990
2024-12-01-17:12:36-root-INFO: grad norm: 17.510 17.296 2.730
2024-12-01-17:12:37-root-INFO: grad norm: 17.106 16.872 2.821
2024-12-01-17:12:38-root-INFO: Loss Change: 285.914 -> 277.932
2024-12-01-17:12:38-root-INFO: Regularization Change: 0.000 -> 1.618
2024-12-01-17:12:38-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-01-17:12:38-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-17:12:38-root-INFO: step: 109 lr_xt 0.03995709
2024-12-01-17:12:39-root-INFO: grad norm: 21.846 21.594 3.305
2024-12-01-17:12:39-root-INFO: Loss too large (279.628->282.259)! Learning rate decreased to 0.03197.
2024-12-01-17:12:40-root-INFO: grad norm: 20.648 20.410 3.129
2024-12-01-17:12:41-root-INFO: grad norm: 19.600 19.384 2.902
2024-12-01-17:12:42-root-INFO: grad norm: 18.770 18.532 2.980
2024-12-01-17:12:43-root-INFO: grad norm: 18.018 17.812 2.718
2024-12-01-17:12:44-root-INFO: grad norm: 17.427 17.201 2.799
2024-12-01-17:12:45-root-INFO: grad norm: 16.902 16.706 2.566
2024-12-01-17:12:46-root-INFO: grad norm: 16.487 16.273 2.646
2024-12-01-17:12:47-root-INFO: Loss Change: 279.628 -> 271.542
2024-12-01-17:12:47-root-INFO: Regularization Change: 0.000 -> 1.623
2024-12-01-17:12:47-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-01-17:12:47-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-17:12:47-root-INFO: step: 108 lr_xt 0.04115569
2024-12-01-17:12:47-root-INFO: grad norm: 19.799 19.581 2.931
2024-12-01-17:12:48-root-INFO: Loss too large (272.336->274.376)! Learning rate decreased to 0.03292.
2024-12-01-17:12:49-root-INFO: grad norm: 18.655 18.450 2.759
2024-12-01-17:12:50-root-INFO: grad norm: 17.700 17.513 2.565
2024-12-01-17:12:51-root-INFO: grad norm: 16.940 16.744 2.574
2024-12-01-17:12:52-root-INFO: grad norm: 16.288 16.113 2.386
2024-12-01-17:12:53-root-INFO: grad norm: 15.775 15.589 2.412
2024-12-01-17:12:54-root-INFO: grad norm: 15.337 15.171 2.252
2024-12-01-17:12:55-root-INFO: grad norm: 14.994 14.819 2.287
2024-12-01-17:12:55-root-INFO: Loss Change: 272.336 -> 264.769
2024-12-01-17:12:56-root-INFO: Regularization Change: 0.000 -> 1.603
2024-12-01-17:12:56-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-01-17:12:56-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-17:12:56-root-INFO: step: 107 lr_xt 0.04238344
2024-12-01-17:12:56-root-INFO: grad norm: 20.825 20.629 2.853
2024-12-01-17:12:57-root-INFO: Loss too large (266.667->269.162)! Learning rate decreased to 0.03391.
2024-12-01-17:12:58-root-INFO: grad norm: 19.746 19.559 2.713
2024-12-01-17:12:59-root-INFO: grad norm: 18.768 18.589 2.587
2024-12-01-17:13:00-root-INFO: grad norm: 17.990 17.791 2.670
2024-12-01-17:13:01-root-INFO: grad norm: 17.286 17.107 2.481
2024-12-01-17:13:02-root-INFO: grad norm: 16.728 16.532 2.554
2024-12-01-17:13:03-root-INFO: grad norm: 16.242 16.067 2.373
2024-12-01-17:13:04-root-INFO: grad norm: 15.853 15.664 2.440
2024-12-01-17:13:04-root-INFO: Loss Change: 266.667 -> 259.039
2024-12-01-17:13:04-root-INFO: Regularization Change: 0.000 -> 1.629
2024-12-01-17:13:04-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-01-17:13:04-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-17:13:05-root-INFO: step: 106 lr_xt 0.04364080
2024-12-01-17:13:05-root-INFO: grad norm: 20.622 20.374 3.189
2024-12-01-17:13:05-root-INFO: Loss too large (260.466->262.900)! Learning rate decreased to 0.03491.
2024-12-01-17:13:06-root-INFO: grad norm: 19.394 19.183 2.856
2024-12-01-17:13:07-root-INFO: grad norm: 18.353 18.157 2.672
2024-12-01-17:13:08-root-INFO: grad norm: 17.476 17.283 2.588
2024-12-01-17:13:09-root-INFO: grad norm: 16.734 16.561 2.398
2024-12-01-17:13:10-root-INFO: grad norm: 16.126 15.952 2.365
2024-12-01-17:13:11-root-INFO: grad norm: 15.621 15.465 2.201
2024-12-01-17:13:12-root-INFO: grad norm: 15.210 15.050 2.197
2024-12-01-17:13:13-root-INFO: Loss Change: 260.466 -> 252.790
2024-12-01-17:13:13-root-INFO: Regularization Change: 0.000 -> 1.635
2024-12-01-17:13:13-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-01-17:13:13-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-17:13:13-root-INFO: step: 105 lr_xt 0.04492824
2024-12-01-17:13:14-root-INFO: grad norm: 19.166 18.976 2.691
2024-12-01-17:13:14-root-INFO: Loss too large (254.283->256.551)! Learning rate decreased to 0.03594.
2024-12-01-17:13:15-root-INFO: grad norm: 18.356 18.198 2.405
2024-12-01-17:13:16-root-INFO: grad norm: 17.565 17.413 2.304
2024-12-01-17:13:17-root-INFO: grad norm: 16.865 16.713 2.263
2024-12-01-17:13:18-root-INFO: grad norm: 16.257 16.117 2.127
2024-12-01-17:13:19-root-INFO: grad norm: 15.747 15.604 2.114
2024-12-01-17:13:20-root-INFO: grad norm: 15.321 15.191 1.991
2024-12-01-17:13:21-root-INFO: grad norm: 14.969 14.836 1.996
2024-12-01-17:13:22-root-INFO: Loss Change: 254.283 -> 247.347
2024-12-01-17:13:22-root-INFO: Regularization Change: 0.000 -> 1.604
2024-12-01-17:13:22-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-01-17:13:22-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-17:13:22-root-INFO: step: 104 lr_xt 0.04624623
2024-12-01-17:13:22-root-INFO: grad norm: 19.333 19.153 2.632
2024-12-01-17:13:23-root-INFO: Loss too large (248.931->250.904)! Learning rate decreased to 0.03700.
2024-12-01-17:13:24-root-INFO: grad norm: 17.875 17.726 2.309
2024-12-01-17:13:25-root-INFO: grad norm: 16.719 16.577 2.176
2024-12-01-17:13:26-root-INFO: grad norm: 15.773 15.632 2.105
2024-12-01-17:13:27-root-INFO: grad norm: 15.048 14.918 1.970
2024-12-01-17:13:28-root-INFO: grad norm: 14.450 14.321 1.927
2024-12-01-17:13:29-root-INFO: grad norm: 13.999 13.880 1.821
2024-12-01-17:13:30-root-INFO: grad norm: 13.630 13.511 1.799
2024-12-01-17:13:31-root-INFO: Loss Change: 248.931 -> 241.562
2024-12-01-17:13:31-root-INFO: Regularization Change: 0.000 -> 1.615
2024-12-01-17:13:31-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-01-17:13:31-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-17:13:31-root-INFO: step: 103 lr_xt 0.04759523
2024-12-01-17:13:31-root-INFO: grad norm: 16.898 16.767 2.103
2024-12-01-17:13:32-root-INFO: Loss too large (242.562->244.055)! Learning rate decreased to 0.03808.
2024-12-01-17:13:33-root-INFO: grad norm: 15.863 15.746 1.920
2024-12-01-17:13:34-root-INFO: grad norm: 15.013 14.904 1.799
2024-12-01-17:13:35-root-INFO: grad norm: 14.278 14.166 1.785
2024-12-01-17:13:36-root-INFO: grad norm: 13.727 13.625 1.670
2024-12-01-17:13:37-root-INFO: grad norm: 13.257 13.151 1.671
2024-12-01-17:13:38-root-INFO: grad norm: 12.914 12.817 1.579
2024-12-01-17:13:39-root-INFO: grad norm: 12.626 12.526 1.588
2024-12-01-17:13:39-root-INFO: Loss Change: 242.562 -> 236.058
2024-12-01-17:13:39-root-INFO: Regularization Change: 0.000 -> 1.579
2024-12-01-17:13:39-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-01-17:13:39-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-17:13:40-root-INFO: step: 102 lr_xt 0.04897571
2024-12-01-17:13:40-root-INFO: grad norm: 15.924 15.772 2.190
2024-12-01-17:13:40-root-INFO: Loss too large (236.878->238.266)! Learning rate decreased to 0.03918.
2024-12-01-17:13:41-root-INFO: grad norm: 15.354 15.243 1.847
2024-12-01-17:13:42-root-INFO: grad norm: 14.858 14.750 1.792
2024-12-01-17:13:43-root-INFO: grad norm: 14.377 14.277 1.688
2024-12-01-17:13:44-root-INFO: grad norm: 13.982 13.887 1.632
2024-12-01-17:13:45-root-INFO: grad norm: 13.609 13.518 1.572
2024-12-01-17:13:46-root-INFO: grad norm: 13.324 13.236 1.530
2024-12-01-17:13:47-root-INFO: grad norm: 13.061 12.976 1.494
2024-12-01-17:13:48-root-INFO: Loss Change: 236.878 -> 230.949
2024-12-01-17:13:48-root-INFO: Regularization Change: 0.000 -> 1.585
2024-12-01-17:13:48-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-01-17:13:48-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-17:13:48-root-INFO: step: 101 lr_xt 0.05038813
2024-12-01-17:13:49-root-INFO: grad norm: 16.343 16.217 2.028
2024-12-01-17:13:49-root-INFO: Loss too large (232.063->233.548)! Learning rate decreased to 0.04031.
2024-12-01-17:13:50-root-INFO: grad norm: 15.431 15.336 1.716
2024-12-01-17:13:51-root-INFO: grad norm: 14.665 14.570 1.670
2024-12-01-17:13:52-root-INFO: grad norm: 13.949 13.858 1.594
2024-12-01-17:13:53-root-INFO: grad norm: 13.429 13.341 1.536
2024-12-01-17:13:54-root-INFO: grad norm: 12.948 12.862 1.491
2024-12-01-17:13:55-root-INFO: grad norm: 12.609 12.526 1.444
2024-12-01-17:13:56-root-INFO: grad norm: 12.302 12.221 1.416
2024-12-01-17:13:57-root-INFO: Loss Change: 232.063 -> 225.939
2024-12-01-17:13:57-root-INFO: Regularization Change: 0.000 -> 1.557
2024-12-01-17:13:57-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-01-17:13:57-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-17:13:57-root-INFO: step: 100 lr_xt 0.05183295
2024-12-01-17:13:57-root-INFO: grad norm: 15.475 15.357 1.906
2024-12-01-17:13:58-root-INFO: Loss too large (227.024->228.294)! Learning rate decreased to 0.04147.
2024-12-01-17:13:59-root-INFO: grad norm: 14.520 14.435 1.568
2024-12-01-17:14:00-root-INFO: grad norm: 13.759 13.673 1.538
2024-12-01-17:14:01-root-INFO: grad norm: 13.037 12.958 1.434
2024-12-01-17:14:02-root-INFO: grad norm: 12.532 12.454 1.398
2024-12-01-17:14:03-root-INFO: grad norm: 12.054 11.980 1.335
2024-12-01-17:14:04-root-INFO: grad norm: 11.731 11.657 1.311
2024-12-01-17:14:05-root-INFO: grad norm: 11.432 11.361 1.269
2024-12-01-17:14:06-root-INFO: Loss Change: 227.024 -> 221.137
2024-12-01-17:14:06-root-INFO: Regularization Change: 0.000 -> 1.535
2024-12-01-17:14:06-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-01-17:14:06-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-17:14:06-root-INFO: step: 99 lr_xt 0.05331064
2024-12-01-17:14:06-root-INFO: grad norm: 14.099 13.996 1.707
2024-12-01-17:14:07-root-INFO: Loss too large (221.824->222.801)! Learning rate decreased to 0.04265.
2024-12-01-17:14:08-root-INFO: grad norm: 13.287 13.209 1.433
2024-12-01-17:14:09-root-INFO: grad norm: 12.664 12.587 1.394
2024-12-01-17:14:10-root-INFO: grad norm: 12.068 11.997 1.316
2024-12-01-17:14:11-root-INFO: grad norm: 11.654 11.583 1.281
2024-12-01-17:14:12-root-INFO: grad norm: 11.252 11.184 1.235
2024-12-01-17:14:13-root-INFO: grad norm: 10.985 10.917 1.212
2024-12-01-17:14:14-root-INFO: grad norm: 10.731 10.666 1.182
2024-12-01-17:14:15-root-INFO: Loss Change: 221.824 -> 216.354
2024-12-01-17:14:15-root-INFO: Regularization Change: 0.000 -> 1.522
2024-12-01-17:14:15-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-01-17:14:15-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-17:14:15-root-INFO: step: 98 lr_xt 0.05482165
2024-12-01-17:14:15-root-INFO: grad norm: 14.025 13.904 1.840
2024-12-01-17:14:16-root-INFO: Loss too large (217.435->218.490)! Learning rate decreased to 0.04386.
2024-12-01-17:14:17-root-INFO: grad norm: 13.315 13.237 1.438
2024-12-01-17:14:18-root-INFO: grad norm: 12.719 12.643 1.389
2024-12-01-17:14:19-root-INFO: grad norm: 12.123 12.055 1.276
2024-12-01-17:14:20-root-INFO: grad norm: 11.693 11.628 1.239
2024-12-01-17:14:21-root-INFO: grad norm: 11.253 11.191 1.181
2024-12-01-17:14:22-root-INFO: grad norm: 10.960 10.899 1.159
2024-12-01-17:14:23-root-INFO: grad norm: 10.665 10.606 1.125
2024-12-01-17:14:23-root-INFO: Loss Change: 217.435 -> 212.087
2024-12-01-17:14:23-root-INFO: Regularization Change: 0.000 -> 1.524
2024-12-01-17:14:23-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-01-17:14:23-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-17:14:24-root-INFO: step: 97 lr_xt 0.05636643
2024-12-01-17:14:24-root-INFO: grad norm: 13.907 13.804 1.687
2024-12-01-17:14:24-root-INFO: Loss too large (212.937->213.940)! Learning rate decreased to 0.04509.
2024-12-01-17:14:25-root-INFO: grad norm: 13.076 13.012 1.291
2024-12-01-17:14:26-root-INFO: grad norm: 12.414 12.347 1.281
2024-12-01-17:14:27-root-INFO: grad norm: 11.747 11.688 1.175
2024-12-01-17:14:28-root-INFO: grad norm: 11.298 11.238 1.159
2024-12-01-17:14:29-root-INFO: grad norm: 10.830 10.773 1.103
2024-12-01-17:14:30-root-INFO: grad norm: 10.535 10.478 1.092
2024-12-01-17:14:31-root-INFO: grad norm: 10.234 10.179 1.058
2024-12-01-17:14:32-root-INFO: Loss Change: 212.937 -> 207.575
2024-12-01-17:14:32-root-INFO: Regularization Change: 0.000 -> 1.526
2024-12-01-17:14:32-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-01-17:14:32-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-17:14:33-root-INFO: step: 96 lr_xt 0.05794543
2024-12-01-17:14:33-root-INFO: grad norm: 12.711 12.615 1.559
2024-12-01-17:14:33-root-INFO: Loss too large (208.341->209.047)! Learning rate decreased to 0.04636.
2024-12-01-17:14:34-root-INFO: grad norm: 11.765 11.704 1.201
2024-12-01-17:14:35-root-INFO: grad norm: 11.116 11.053 1.186
2024-12-01-17:14:36-root-INFO: grad norm: 10.480 10.424 1.079
2024-12-01-17:14:37-root-INFO: grad norm: 10.071 10.014 1.073
2024-12-01-17:14:38-root-INFO: grad norm: 9.661 9.608 1.016
2024-12-01-17:14:39-root-INFO: grad norm: 9.408 9.353 1.016
2024-12-01-17:14:40-root-INFO: grad norm: 9.161 9.109 0.981
2024-12-01-17:14:41-root-INFO: Loss Change: 208.341 -> 203.203
2024-12-01-17:14:41-root-INFO: Regularization Change: 0.000 -> 1.518
2024-12-01-17:14:41-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-01-17:14:41-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-17:14:41-root-INFO: step: 95 lr_xt 0.05955910
2024-12-01-17:14:42-root-INFO: grad norm: 12.136 12.039 1.530
2024-12-01-17:14:42-root-INFO: Loss too large (204.233->204.942)! Learning rate decreased to 0.04765.
2024-12-01-17:14:43-root-INFO: grad norm: 11.477 11.422 1.125
2024-12-01-17:14:44-root-INFO: grad norm: 10.971 10.912 1.140
2024-12-01-17:14:45-root-INFO: grad norm: 10.458 10.408 1.026
2024-12-01-17:14:46-root-INFO: grad norm: 10.110 10.057 1.038
2024-12-01-17:14:47-root-INFO: grad norm: 9.726 9.677 0.975
2024-12-01-17:14:48-root-INFO: grad norm: 9.490 9.438 0.987
2024-12-01-17:14:49-root-INFO: grad norm: 9.235 9.187 0.945
2024-12-01-17:14:50-root-INFO: Loss Change: 204.233 -> 199.405
2024-12-01-17:14:50-root-INFO: Regularization Change: 0.000 -> 1.525
2024-12-01-17:14:50-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-01-17:14:50-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-17:14:50-root-INFO: step: 94 lr_xt 0.06120788
2024-12-01-17:14:51-root-INFO: grad norm: 11.939 11.860 1.372
2024-12-01-17:14:51-root-INFO: Loss too large (200.045->200.730)! Learning rate decreased to 0.04897.
2024-12-01-17:14:52-root-INFO: grad norm: 11.287 11.238 1.049
2024-12-01-17:14:53-root-INFO: grad norm: 10.770 10.717 1.067
2024-12-01-17:14:54-root-INFO: grad norm: 10.217 10.170 0.980
2024-12-01-17:14:55-root-INFO: grad norm: 9.864 9.815 0.986
2024-12-01-17:14:56-root-INFO: grad norm: 9.463 9.416 0.937
2024-12-01-17:14:57-root-INFO: grad norm: 9.227 9.178 0.943
2024-12-01-17:14:58-root-INFO: grad norm: 8.965 8.919 0.910
2024-12-01-17:14:59-root-INFO: Loss Change: 200.045 -> 195.257
2024-12-01-17:14:59-root-INFO: Regularization Change: 0.000 -> 1.533
2024-12-01-17:14:59-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-01-17:14:59-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-17:14:59-root-INFO: step: 93 lr_xt 0.06289219
2024-12-01-17:15:00-root-INFO: grad norm: 11.826 11.737 1.447
2024-12-01-17:15:00-root-INFO: Loss too large (195.985->196.616)! Learning rate decreased to 0.05031.
2024-12-01-17:15:01-root-INFO: grad norm: 11.084 11.036 1.036
2024-12-01-17:15:02-root-INFO: grad norm: 10.525 10.472 1.056
2024-12-01-17:15:03-root-INFO: grad norm: 9.928 9.883 0.942
2024-12-01-17:15:04-root-INFO: grad norm: 9.552 9.504 0.959
2024-12-01-17:15:05-root-INFO: grad norm: 9.120 9.076 0.897
2024-12-01-17:15:06-root-INFO: grad norm: 8.872 8.825 0.911
2024-12-01-17:15:07-root-INFO: grad norm: 8.595 8.551 0.870
2024-12-01-17:15:08-root-INFO: Loss Change: 195.985 -> 191.158
2024-12-01-17:15:08-root-INFO: Regularization Change: 0.000 -> 1.560
2024-12-01-17:15:08-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-01-17:15:08-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-17:15:08-root-INFO: step: 92 lr_xt 0.06461248
2024-12-01-17:15:08-root-INFO: grad norm: 10.771 10.697 1.260
2024-12-01-17:15:09-root-INFO: Loss too large (191.820->192.247)! Learning rate decreased to 0.05169.
2024-12-01-17:15:10-root-INFO: grad norm: 10.000 9.951 0.994
2024-12-01-17:15:11-root-INFO: grad norm: 9.488 9.439 0.963
2024-12-01-17:15:12-root-INFO: grad norm: 8.964 8.918 0.911
2024-12-01-17:15:13-root-INFO: grad norm: 8.642 8.597 0.887
2024-12-01-17:15:14-root-INFO: grad norm: 8.292 8.246 0.868
2024-12-01-17:15:15-root-INFO: grad norm: 8.090 8.045 0.850
2024-12-01-17:15:16-root-INFO: grad norm: 7.878 7.833 0.842
2024-12-01-17:15:16-root-INFO: Loss Change: 191.820 -> 187.244
2024-12-01-17:15:16-root-INFO: Regularization Change: 0.000 -> 1.553
2024-12-01-17:15:16-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-01-17:15:16-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-17:15:17-root-INFO: step: 91 lr_xt 0.06636917
2024-12-01-17:15:17-root-INFO: grad norm: 10.228 10.159 1.188
2024-12-01-17:15:17-root-INFO: Loss too large (187.853->188.296)! Learning rate decreased to 0.05310.
2024-12-01-17:15:19-root-INFO: grad norm: 9.593 9.548 0.930
2024-12-01-17:15:19-root-INFO: grad norm: 9.125 9.078 0.927
2024-12-01-17:15:21-root-INFO: grad norm: 8.646 8.603 0.860
2024-12-01-17:15:22-root-INFO: grad norm: 8.339 8.295 0.861
2024-12-01-17:15:23-root-INFO: grad norm: 7.994 7.951 0.825
2024-12-01-17:15:24-root-INFO: grad norm: 7.792 7.748 0.827
2024-12-01-17:15:25-root-INFO: grad norm: 7.570 7.527 0.802
2024-12-01-17:15:25-root-INFO: Loss Change: 187.853 -> 183.504
2024-12-01-17:15:25-root-INFO: Regularization Change: 0.000 -> 1.543
2024-12-01-17:15:25-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-01-17:15:25-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-17:15:26-root-INFO: step: 90 lr_xt 0.06816268
2024-12-01-17:15:26-root-INFO: grad norm: 9.709 9.637 1.180
2024-12-01-17:15:26-root-INFO: Loss too large (184.216->184.515)! Learning rate decreased to 0.05453.
2024-12-01-17:15:27-root-INFO: grad norm: 9.006 8.964 0.869
2024-12-01-17:15:28-root-INFO: grad norm: 8.532 8.485 0.894
2024-12-01-17:15:29-root-INFO: grad norm: 8.047 8.006 0.809
2024-12-01-17:15:30-root-INFO: grad norm: 7.747 7.702 0.826
2024-12-01-17:15:31-root-INFO: grad norm: 7.408 7.367 0.780
2024-12-01-17:15:32-root-INFO: grad norm: 7.213 7.170 0.792
2024-12-01-17:15:33-root-INFO: grad norm: 6.999 6.957 0.761
2024-12-01-17:15:34-root-INFO: Loss Change: 184.216 -> 179.916
2024-12-01-17:15:34-root-INFO: Regularization Change: 0.000 -> 1.563
2024-12-01-17:15:34-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-01-17:15:34-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-17:15:34-root-INFO: step: 89 lr_xt 0.06999342
2024-12-01-17:15:35-root-INFO: grad norm: 10.112 10.032 1.269
2024-12-01-17:15:35-root-INFO: Loss too large (180.583->180.907)! Learning rate decreased to 0.05599.
2024-12-01-17:15:36-root-INFO: grad norm: 9.205 9.160 0.905
2024-12-01-17:15:37-root-INFO: grad norm: 8.598 8.552 0.892
2024-12-01-17:15:38-root-INFO: grad norm: 7.998 7.957 0.811
2024-12-01-17:15:39-root-INFO: grad norm: 7.628 7.585 0.807
2024-12-01-17:15:40-root-INFO: grad norm: 7.212 7.171 0.769
2024-12-01-17:15:41-root-INFO: grad norm: 6.975 6.932 0.767
2024-12-01-17:15:42-root-INFO: grad norm: 6.712 6.670 0.744
2024-12-01-17:15:43-root-INFO: Loss Change: 180.583 -> 176.083
2024-12-01-17:15:43-root-INFO: Regularization Change: 0.000 -> 1.604
2024-12-01-17:15:43-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-01-17:15:43-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-17:15:43-root-INFO: step: 88 lr_xt 0.07186179
2024-12-01-17:15:44-root-INFO: grad norm: 8.564 8.495 1.089
2024-12-01-17:15:44-root-INFO: Loss too large (176.810->176.913)! Learning rate decreased to 0.05749.
2024-12-01-17:15:45-root-INFO: grad norm: 7.875 7.835 0.797
2024-12-01-17:15:46-root-INFO: grad norm: 7.445 7.400 0.817
2024-12-01-17:15:47-root-INFO: grad norm: 6.993 6.954 0.743
2024-12-01-17:15:48-root-INFO: grad norm: 6.722 6.679 0.759
2024-12-01-17:15:49-root-INFO: grad norm: 6.409 6.369 0.718
2024-12-01-17:15:50-root-INFO: grad norm: 6.232 6.189 0.730
2024-12-01-17:15:51-root-INFO: grad norm: 6.033 5.992 0.702
2024-12-01-17:15:52-root-INFO: Loss Change: 176.810 -> 172.728
2024-12-01-17:15:52-root-INFO: Regularization Change: 0.000 -> 1.590
2024-12-01-17:15:52-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-01-17:15:52-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-17:15:52-root-INFO: step: 87 lr_xt 0.07376819
2024-12-01-17:15:52-root-INFO: grad norm: 8.565 8.494 1.099
2024-12-01-17:15:53-root-INFO: Loss too large (173.126->173.156)! Learning rate decreased to 0.05901.
2024-12-01-17:15:54-root-INFO: grad norm: 7.634 7.594 0.780
2024-12-01-17:15:55-root-INFO: grad norm: 7.084 7.040 0.786
2024-12-01-17:15:56-root-INFO: grad norm: 6.576 6.537 0.714
2024-12-01-17:15:57-root-INFO: grad norm: 6.266 6.225 0.719
2024-12-01-17:15:58-root-INFO: grad norm: 5.930 5.890 0.686
2024-12-01-17:15:59-root-INFO: grad norm: 5.735 5.694 0.690
2024-12-01-17:16:00-root-INFO: grad norm: 5.523 5.482 0.669
2024-12-01-17:16:00-root-INFO: Loss Change: 173.126 -> 168.958
2024-12-01-17:16:00-root-INFO: Regularization Change: 0.000 -> 1.621
2024-12-01-17:16:00-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-01-17:16:00-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-17:16:01-root-INFO: step: 86 lr_xt 0.07571301
2024-12-01-17:16:01-root-INFO: grad norm: 7.711 7.644 1.015
2024-12-01-17:16:02-root-INFO: grad norm: 10.629 10.590 0.910
2024-12-01-17:16:02-root-INFO: Loss too large (169.471->170.689)! Learning rate decreased to 0.06057.
2024-12-01-17:16:03-root-INFO: grad norm: 9.226 9.184 0.880
2024-12-01-17:16:04-root-INFO: grad norm: 7.381 7.343 0.750
2024-12-01-17:16:05-root-INFO: grad norm: 6.648 6.607 0.735
2024-12-01-17:16:06-root-INFO: grad norm: 5.861 5.821 0.682
2024-12-01-17:16:07-root-INFO: grad norm: 5.464 5.422 0.677
2024-12-01-17:16:08-root-INFO: grad norm: 5.056 5.014 0.651
2024-12-01-17:16:09-root-INFO: Loss Change: 169.510 -> 165.464
2024-12-01-17:16:09-root-INFO: Regularization Change: 0.000 -> 1.721
2024-12-01-17:16:09-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-01-17:16:09-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-17:16:10-root-INFO: step: 85 lr_xt 0.07769664
2024-12-01-17:16:10-root-INFO: grad norm: 6.924 6.859 0.949
2024-12-01-17:16:11-root-INFO: grad norm: 9.230 9.191 0.841
2024-12-01-17:16:11-root-INFO: Loss too large (165.613->166.381)! Learning rate decreased to 0.06216.
2024-12-01-17:16:12-root-INFO: grad norm: 8.048 8.008 0.806
2024-12-01-17:16:13-root-INFO: grad norm: 6.621 6.584 0.705
2024-12-01-17:16:14-root-INFO: grad norm: 5.987 5.947 0.689
2024-12-01-17:16:15-root-INFO: grad norm: 5.315 5.275 0.649
2024-12-01-17:16:16-root-INFO: grad norm: 4.966 4.925 0.642
2024-12-01-17:16:17-root-INFO: grad norm: 4.610 4.568 0.622
2024-12-01-17:16:18-root-INFO: Loss Change: 165.806 -> 161.914
2024-12-01-17:16:18-root-INFO: Regularization Change: 0.000 -> 1.740
2024-12-01-17:16:18-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-01-17:16:18-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-17:16:18-root-INFO: step: 84 lr_xt 0.07971945
2024-12-01-17:16:19-root-INFO: grad norm: 6.116 6.056 0.852
2024-12-01-17:16:20-root-INFO: grad norm: 8.131 8.093 0.777
2024-12-01-17:16:20-root-INFO: Loss too large (161.908->162.418)! Learning rate decreased to 0.06378.
2024-12-01-17:16:21-root-INFO: grad norm: 7.158 7.119 0.744
2024-12-01-17:16:22-root-INFO: grad norm: 5.996 5.959 0.668
2024-12-01-17:16:23-root-INFO: grad norm: 5.455 5.415 0.654
2024-12-01-17:16:24-root-INFO: grad norm: 4.873 4.833 0.622
2024-12-01-17:16:25-root-INFO: grad norm: 4.566 4.525 0.615
2024-12-01-17:16:26-root-INFO: grad norm: 4.249 4.207 0.599
2024-12-01-17:16:26-root-INFO: Loss Change: 162.140 -> 158.472
2024-12-01-17:16:26-root-INFO: Regularization Change: 0.000 -> 1.736
2024-12-01-17:16:26-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-01-17:16:26-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-17:16:27-root-INFO: step: 83 lr_xt 0.08178179
2024-12-01-17:16:27-root-INFO: grad norm: 6.289 6.225 0.893
2024-12-01-17:16:28-root-INFO: grad norm: 8.071 8.033 0.778
2024-12-01-17:16:28-root-INFO: Loss too large (158.592->159.050)! Learning rate decreased to 0.06543.
2024-12-01-17:16:29-root-INFO: grad norm: 6.976 6.937 0.734
2024-12-01-17:16:30-root-INFO: grad norm: 5.760 5.723 0.650
2024-12-01-17:16:31-root-INFO: grad norm: 5.184 5.146 0.631
2024-12-01-17:16:32-root-INFO: grad norm: 4.588 4.549 0.600
2024-12-01-17:16:33-root-INFO: grad norm: 4.271 4.230 0.591
2024-12-01-17:16:34-root-INFO: grad norm: 3.951 3.909 0.576
2024-12-01-17:16:35-root-INFO: Loss Change: 158.868 -> 155.133
2024-12-01-17:16:35-root-INFO: Regularization Change: 0.000 -> 1.765
2024-12-01-17:16:35-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-01-17:16:35-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-17:16:35-root-INFO: step: 82 lr_xt 0.08388403
2024-12-01-17:16:36-root-INFO: grad norm: 5.373 5.311 0.811
2024-12-01-17:16:37-root-INFO: grad norm: 6.751 6.714 0.709
2024-12-01-17:16:37-root-INFO: Loss too large (155.113->155.277)! Learning rate decreased to 0.06711.
2024-12-01-17:16:38-root-INFO: grad norm: 5.890 5.853 0.666
2024-12-01-17:16:39-root-INFO: grad norm: 4.994 4.956 0.608
2024-12-01-17:16:40-root-INFO: grad norm: 4.544 4.505 0.597
2024-12-01-17:16:41-root-INFO: grad norm: 4.075 4.034 0.572
2024-12-01-17:16:42-root-INFO: grad norm: 3.815 3.773 0.568
2024-12-01-17:16:43-root-INFO: grad norm: 3.551 3.508 0.554
2024-12-01-17:16:44-root-INFO: Loss Change: 155.451 -> 151.950
2024-12-01-17:16:44-root-INFO: Regularization Change: 0.000 -> 1.758
2024-12-01-17:16:44-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-01-17:16:44-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-17:16:44-root-INFO: step: 81 lr_xt 0.08602650
2024-12-01-17:16:44-root-INFO: grad norm: 5.438 5.371 0.851
2024-12-01-17:16:45-root-INFO: grad norm: 6.653 6.618 0.684
2024-12-01-17:16:46-root-INFO: Loss too large (151.802->151.926)! Learning rate decreased to 0.06882.
2024-12-01-17:16:47-root-INFO: grad norm: 5.738 5.700 0.665
2024-12-01-17:16:48-root-INFO: grad norm: 4.870 4.835 0.579
2024-12-01-17:16:49-root-INFO: grad norm: 4.412 4.374 0.581
2024-12-01-17:16:50-root-INFO: grad norm: 3.952 3.914 0.546
2024-12-01-17:16:51-root-INFO: grad norm: 3.695 3.654 0.549
2024-12-01-17:16:52-root-INFO: grad norm: 3.436 3.395 0.530
2024-12-01-17:16:53-root-INFO: Loss Change: 152.166 -> 148.657
2024-12-01-17:16:53-root-INFO: Regularization Change: 0.000 -> 1.787
2024-12-01-17:16:53-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-01-17:16:53-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-17:16:53-root-INFO: step: 80 lr_xt 0.08820955
2024-12-01-17:16:53-root-INFO: grad norm: 5.743 5.678 0.861
2024-12-01-17:16:54-root-INFO: grad norm: 6.976 6.939 0.715
2024-12-01-17:16:55-root-INFO: Loss too large (148.831->149.006)! Learning rate decreased to 0.07057.
2024-12-01-17:16:56-root-INFO: grad norm: 5.858 5.821 0.655
2024-12-01-17:16:57-root-INFO: grad norm: 4.773 4.739 0.575
2024-12-01-17:16:58-root-INFO: grad norm: 4.249 4.212 0.559
2024-12-01-17:16:59-root-INFO: grad norm: 3.739 3.701 0.532
2024-12-01-17:17:00-root-INFO: grad norm: 3.463 3.422 0.527
2024-12-01-17:17:01-root-INFO: grad norm: 3.196 3.154 0.514
2024-12-01-17:17:02-root-INFO: Loss Change: 149.182 -> 145.604
2024-12-01-17:17:02-root-INFO: Regularization Change: 0.000 -> 1.810
2024-12-01-17:17:02-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-01-17:17:02-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-17:17:02-root-INFO: step: 79 lr_xt 0.09043348
2024-12-01-17:17:02-root-INFO: grad norm: 4.782 4.721 0.765
2024-12-01-17:17:03-root-INFO: grad norm: 5.473 5.437 0.628
2024-12-01-17:17:04-root-INFO: grad norm: 6.320 6.281 0.703
2024-12-01-17:17:05-root-INFO: grad norm: 7.832 7.799 0.719
2024-12-01-17:17:06-root-INFO: Loss too large (144.933->145.296)! Learning rate decreased to 0.07235.
2024-12-01-17:17:07-root-INFO: grad norm: 6.226 6.192 0.648
2024-12-01-17:17:08-root-INFO: grad norm: 4.456 4.421 0.561
2024-12-01-17:17:09-root-INFO: grad norm: 3.796 3.759 0.534
2024-12-01-17:17:10-root-INFO: grad norm: 3.249 3.209 0.508
2024-12-01-17:17:10-root-INFO: Loss Change: 145.780 -> 142.262
2024-12-01-17:17:10-root-INFO: Regularization Change: 0.000 -> 2.044
2024-12-01-17:17:10-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-01-17:17:10-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-17:17:11-root-INFO: step: 78 lr_xt 0.09269861
2024-12-01-17:17:11-root-INFO: grad norm: 4.542 4.484 0.725
2024-12-01-17:17:12-root-INFO: grad norm: 5.220 5.184 0.608
2024-12-01-17:17:13-root-INFO: grad norm: 5.968 5.928 0.689
2024-12-01-17:17:14-root-INFO: grad norm: 7.212 7.179 0.690
2024-12-01-17:17:14-root-INFO: Loss too large (141.773->141.963)! Learning rate decreased to 0.07416.
2024-12-01-17:17:15-root-INFO: grad norm: 5.661 5.627 0.618
2024-12-01-17:17:16-root-INFO: grad norm: 4.093 4.057 0.536
2024-12-01-17:17:17-root-INFO: grad norm: 3.494 3.456 0.515
2024-12-01-17:17:18-root-INFO: grad norm: 3.012 2.971 0.491
2024-12-01-17:17:19-root-INFO: Loss Change: 142.626 -> 139.231
2024-12-01-17:17:19-root-INFO: Regularization Change: 0.000 -> 2.030
2024-12-01-17:17:19-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-01-17:17:19-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-17:17:19-root-INFO: step: 77 lr_xt 0.09500525
2024-12-01-17:17:20-root-INFO: grad norm: 4.395 4.338 0.702
2024-12-01-17:17:21-root-INFO: grad norm: 4.664 4.623 0.619
2024-12-01-17:17:22-root-INFO: grad norm: 5.129 5.090 0.633
2024-12-01-17:17:23-root-INFO: grad norm: 5.785 5.747 0.662
2024-12-01-17:17:24-root-INFO: grad norm: 6.232 6.194 0.682
2024-12-01-17:17:25-root-INFO: grad norm: 6.788 6.750 0.714
2024-12-01-17:17:26-root-INFO: grad norm: 7.007 6.969 0.731
2024-12-01-17:17:27-root-INFO: grad norm: 7.168 7.128 0.756
2024-12-01-17:17:27-root-INFO: Loss Change: 139.311 -> 136.787
2024-12-01-17:17:27-root-INFO: Regularization Change: 0.000 -> 2.708
2024-12-01-17:17:27-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-01-17:17:27-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-17:17:28-root-INFO: step: 76 lr_xt 0.09735366
2024-12-01-17:17:28-root-INFO: grad norm: 9.522 9.448 1.184
2024-12-01-17:17:29-root-INFO: grad norm: 8.908 8.854 0.979
2024-12-01-17:17:30-root-INFO: grad norm: 8.300 8.250 0.906
2024-12-01-17:17:31-root-INFO: grad norm: 7.906 7.859 0.868
2024-12-01-17:17:32-root-INFO: grad norm: 7.757 7.709 0.856
2024-12-01-17:17:33-root-INFO: grad norm: 7.548 7.500 0.851
2024-12-01-17:17:34-root-INFO: grad norm: 7.465 7.418 0.836
2024-12-01-17:17:35-root-INFO: grad norm: 7.317 7.268 0.838
2024-12-01-17:17:36-root-INFO: Loss Change: 137.640 -> 133.235
2024-12-01-17:17:36-root-INFO: Regularization Change: 0.000 -> 2.688
2024-12-01-17:17:36-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-01-17:17:36-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-17:17:36-root-INFO: step: 75 lr_xt 0.09974414
2024-12-01-17:17:37-root-INFO: grad norm: 9.194 9.117 1.186
2024-12-01-17:17:38-root-INFO: grad norm: 8.632 8.575 0.989
2024-12-01-17:17:39-root-INFO: grad norm: 8.108 8.055 0.932
2024-12-01-17:17:40-root-INFO: grad norm: 7.677 7.624 0.893
2024-12-01-17:17:41-root-INFO: grad norm: 7.361 7.313 0.842
2024-12-01-17:17:42-root-INFO: grad norm: 7.037 6.988 0.832
2024-12-01-17:17:43-root-INFO: grad norm: 6.791 6.746 0.783
2024-12-01-17:17:44-root-INFO: grad norm: 6.551 6.504 0.785
2024-12-01-17:17:44-root-INFO: Loss Change: 134.091 -> 129.592
2024-12-01-17:17:44-root-INFO: Regularization Change: 0.000 -> 2.682
2024-12-01-17:17:44-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-01-17:17:44-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-17:17:45-root-INFO: step: 74 lr_xt 0.10217692
2024-12-01-17:17:45-root-INFO: grad norm: 8.184 8.113 1.074
2024-12-01-17:17:46-root-INFO: grad norm: 7.474 7.419 0.905
2024-12-01-17:17:47-root-INFO: grad norm: 6.820 6.772 0.813
2024-12-01-17:17:48-root-INFO: grad norm: 6.368 6.320 0.781
2024-12-01-17:17:49-root-INFO: grad norm: 6.039 5.995 0.724
2024-12-01-17:17:50-root-INFO: grad norm: 5.730 5.685 0.716
2024-12-01-17:17:51-root-INFO: grad norm: 5.497 5.456 0.670
2024-12-01-17:17:52-root-INFO: grad norm: 5.282 5.239 0.672
2024-12-01-17:17:53-root-INFO: Loss Change: 130.309 -> 125.964
2024-12-01-17:17:53-root-INFO: Regularization Change: 0.000 -> 2.680
2024-12-01-17:17:53-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-01-17:17:53-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-17:17:53-root-INFO: step: 73 lr_xt 0.10465226
2024-12-01-17:17:54-root-INFO: grad norm: 7.152 7.079 1.020
2024-12-01-17:17:55-root-INFO: grad norm: 6.465 6.415 0.797
2024-12-01-17:17:56-root-INFO: grad norm: 5.818 5.772 0.729
2024-12-01-17:17:57-root-INFO: grad norm: 5.453 5.410 0.687
2024-12-01-17:17:58-root-INFO: grad norm: 5.186 5.146 0.648
2024-12-01-17:17:59-root-INFO: grad norm: 4.942 4.901 0.638
2024-12-01-17:18:00-root-INFO: grad norm: 4.757 4.719 0.603
2024-12-01-17:18:01-root-INFO: grad norm: 4.590 4.550 0.605
2024-12-01-17:18:01-root-INFO: Loss Change: 126.596 -> 122.559
2024-12-01-17:18:01-root-INFO: Regularization Change: 0.000 -> 2.698
2024-12-01-17:18:01-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-01-17:18:01-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-17:18:02-root-INFO: step: 72 lr_xt 0.10717038
2024-12-01-17:18:02-root-INFO: grad norm: 6.278 6.214 0.895
2024-12-01-17:18:03-root-INFO: grad norm: 5.724 5.677 0.734
2024-12-01-17:18:04-root-INFO: grad norm: 5.216 5.173 0.666
2024-12-01-17:18:05-root-INFO: grad norm: 4.892 4.849 0.645
2024-12-01-17:18:06-root-INFO: grad norm: 4.643 4.605 0.593
2024-12-01-17:18:07-root-INFO: grad norm: 4.432 4.391 0.597
2024-12-01-17:18:08-root-INFO: grad norm: 4.272 4.237 0.552
2024-12-01-17:18:09-root-INFO: grad norm: 4.134 4.095 0.568
2024-12-01-17:18:10-root-INFO: Loss Change: 122.972 -> 119.188
2024-12-01-17:18:10-root-INFO: Regularization Change: 0.000 -> 2.711
2024-12-01-17:18:10-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-01-17:18:10-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-17:18:10-root-INFO: step: 71 lr_xt 0.10973151
2024-12-01-17:18:11-root-INFO: grad norm: 5.321 5.269 0.741
2024-12-01-17:18:12-root-INFO: grad norm: 4.833 4.789 0.646
2024-12-01-17:18:13-root-INFO: grad norm: 4.319 4.281 0.570
2024-12-01-17:18:14-root-INFO: grad norm: 4.110 4.071 0.567
2024-12-01-17:18:15-root-INFO: grad norm: 3.963 3.929 0.522
2024-12-01-17:18:16-root-INFO: grad norm: 3.842 3.804 0.538
2024-12-01-17:18:17-root-INFO: grad norm: 3.759 3.726 0.498
2024-12-01-17:18:18-root-INFO: grad norm: 3.686 3.649 0.522
2024-12-01-17:18:18-root-INFO: Loss Change: 119.564 -> 116.101
2024-12-01-17:18:18-root-INFO: Regularization Change: 0.000 -> 2.693
2024-12-01-17:18:18-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-01-17:18:18-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-17:18:19-root-INFO: step: 70 lr_xt 0.11233583
2024-12-01-17:18:19-root-INFO: grad norm: 5.904 5.830 0.930
2024-12-01-17:18:20-root-INFO: grad norm: 5.178 5.132 0.687
2024-12-01-17:18:21-root-INFO: grad norm: 4.441 4.400 0.604
2024-12-01-17:18:22-root-INFO: grad norm: 4.162 4.123 0.572
2024-12-01-17:18:23-root-INFO: grad norm: 3.982 3.947 0.527
2024-12-01-17:18:24-root-INFO: grad norm: 3.830 3.792 0.535
2024-12-01-17:18:25-root-INFO: grad norm: 3.725 3.692 0.492
2024-12-01-17:18:26-root-INFO: grad norm: 3.639 3.602 0.514
2024-12-01-17:18:27-root-INFO: Loss Change: 116.479 -> 112.793
2024-12-01-17:18:27-root-INFO: Regularization Change: 0.000 -> 2.779
2024-12-01-17:18:27-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-01-17:18:27-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-17:18:27-root-INFO: step: 69 lr_xt 0.11498353
2024-12-01-17:18:27-root-INFO: grad norm: 4.895 4.842 0.714
2024-12-01-17:18:28-root-INFO: grad norm: 4.481 4.440 0.605
2024-12-01-17:18:29-root-INFO: grad norm: 4.062 4.025 0.542
2024-12-01-17:18:30-root-INFO: grad norm: 3.899 3.861 0.543
2024-12-01-17:18:31-root-INFO: grad norm: 3.782 3.749 0.495
2024-12-01-17:18:32-root-INFO: grad norm: 3.693 3.656 0.519
2024-12-01-17:18:33-root-INFO: grad norm: 3.632 3.601 0.473
2024-12-01-17:18:35-root-INFO: grad norm: 3.583 3.547 0.506
2024-12-01-17:18:35-root-INFO: Loss Change: 113.150 -> 109.855
2024-12-01-17:18:35-root-INFO: Regularization Change: 0.000 -> 2.746
2024-12-01-17:18:35-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-01-17:18:35-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-17:18:36-root-INFO: step: 68 lr_xt 0.11767478
2024-12-01-17:18:36-root-INFO: grad norm: 5.874 5.802 0.915
2024-12-01-17:18:37-root-INFO: grad norm: 5.023 4.974 0.701
2024-12-01-17:18:38-root-INFO: grad norm: 4.081 4.042 0.561
2024-12-01-17:18:39-root-INFO: grad norm: 3.850 3.810 0.551
2024-12-01-17:18:40-root-INFO: grad norm: 3.757 3.724 0.493
2024-12-01-17:18:41-root-INFO: grad norm: 3.658 3.621 0.519
2024-12-01-17:18:42-root-INFO: grad norm: 3.607 3.577 0.468
2024-12-01-17:18:43-root-INFO: grad norm: 3.558 3.522 0.503
2024-12-01-17:18:44-root-INFO: Loss Change: 110.248 -> 106.614
2024-12-01-17:18:44-root-INFO: Regularization Change: 0.000 -> 2.856
2024-12-01-17:18:44-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-01-17:18:44-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-17:18:44-root-INFO: step: 67 lr_xt 0.12040972
2024-12-01-17:18:44-root-INFO: grad norm: 4.806 4.752 0.718
2024-12-01-17:18:45-root-INFO: grad norm: 4.416 4.374 0.612
2024-12-01-17:18:46-root-INFO: grad norm: 4.019 3.984 0.532
2024-12-01-17:18:47-root-INFO: grad norm: 3.894 3.855 0.547
2024-12-01-17:18:48-root-INFO: grad norm: 3.807 3.776 0.489
2024-12-01-17:18:49-root-INFO: grad norm: 3.742 3.705 0.524
2024-12-01-17:18:50-root-INFO: grad norm: 3.693 3.663 0.469
2024-12-01-17:18:51-root-INFO: grad norm: 3.662 3.626 0.511
2024-12-01-17:18:52-root-INFO: Loss Change: 106.794 -> 103.570
2024-12-01-17:18:52-root-INFO: Regularization Change: 0.000 -> 2.840
2024-12-01-17:18:52-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-01-17:18:52-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-17:18:52-root-INFO: step: 66 lr_xt 0.12318848
2024-12-01-17:18:53-root-INFO: grad norm: 4.905 4.856 0.689
2024-12-01-17:18:54-root-INFO: grad norm: 4.384 4.342 0.611
2024-12-01-17:18:55-root-INFO: grad norm: 3.718 3.685 0.497
2024-12-01-17:18:56-root-INFO: grad norm: 3.631 3.594 0.516
2024-12-01-17:18:57-root-INFO: grad norm: 3.641 3.612 0.461
2024-12-01-17:18:58-root-INFO: grad norm: 3.609 3.574 0.503
2024-12-01-17:18:59-root-INFO: grad norm: 3.608 3.580 0.449
2024-12-01-17:19:00-root-INFO: grad norm: 3.594 3.559 0.496
2024-12-01-17:19:00-root-INFO: Loss Change: 103.874 -> 100.676
2024-12-01-17:19:00-root-INFO: Regularization Change: 0.000 -> 2.855
2024-12-01-17:19:00-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-01-17:19:00-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-17:19:01-root-INFO: step: 65 lr_xt 0.12601118
2024-12-01-17:19:01-root-INFO: grad norm: 5.287 5.227 0.797
2024-12-01-17:19:02-root-INFO: grad norm: 4.655 4.609 0.652
2024-12-01-17:19:03-root-INFO: grad norm: 3.906 3.871 0.520
2024-12-01-17:19:04-root-INFO: grad norm: 3.781 3.743 0.534
2024-12-01-17:19:05-root-INFO: grad norm: 3.770 3.740 0.470
2024-12-01-17:19:06-root-INFO: grad norm: 3.714 3.679 0.512
2024-12-01-17:19:07-root-INFO: grad norm: 3.681 3.653 0.452
2024-12-01-17:19:08-root-INFO: grad norm: 3.656 3.622 0.500
2024-12-01-17:19:09-root-INFO: Loss Change: 101.033 -> 97.707
2024-12-01-17:19:09-root-INFO: Regularization Change: 0.000 -> 2.932
2024-12-01-17:19:09-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-01-17:19:09-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-17:19:09-root-INFO: step: 64 lr_xt 0.12887791
2024-12-01-17:19:10-root-INFO: grad norm: 4.807 4.759 0.675
2024-12-01-17:19:11-root-INFO: grad norm: 4.419 4.377 0.609
2024-12-01-17:19:12-root-INFO: grad norm: 3.949 3.915 0.512
2024-12-01-17:19:13-root-INFO: grad norm: 3.870 3.833 0.534
2024-12-01-17:19:14-root-INFO: grad norm: 3.858 3.829 0.475
2024-12-01-17:19:15-root-INFO: grad norm: 3.811 3.776 0.517
2024-12-01-17:19:16-root-INFO: grad norm: 3.770 3.742 0.459
2024-12-01-17:19:17-root-INFO: grad norm: 3.751 3.717 0.506
2024-12-01-17:19:17-root-INFO: Loss Change: 97.920 -> 94.813
2024-12-01-17:19:17-root-INFO: Regularization Change: 0.000 -> 2.937
2024-12-01-17:19:17-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-01-17:19:17-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-17:19:18-root-INFO: step: 63 lr_xt 0.13178874
2024-12-01-17:19:18-root-INFO: grad norm: 4.946 4.890 0.745
2024-12-01-17:19:19-root-INFO: grad norm: 4.457 4.416 0.608
2024-12-01-17:19:20-root-INFO: grad norm: 3.903 3.869 0.518
2024-12-01-17:19:21-root-INFO: grad norm: 3.812 3.776 0.521
2024-12-01-17:19:22-root-INFO: grad norm: 3.804 3.774 0.472
2024-12-01-17:19:23-root-INFO: grad norm: 3.758 3.723 0.505
2024-12-01-17:19:24-root-INFO: grad norm: 3.724 3.696 0.455
2024-12-01-17:19:25-root-INFO: grad norm: 3.706 3.673 0.496
2024-12-01-17:19:26-root-INFO: Loss Change: 95.180 -> 92.036
2024-12-01-17:19:26-root-INFO: Regularization Change: 0.000 -> 2.973
2024-12-01-17:19:26-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-01-17:19:26-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-17:19:26-root-INFO: step: 62 lr_xt 0.13474373
2024-12-01-17:19:26-root-INFO: grad norm: 5.423 5.360 0.827
2024-12-01-17:19:27-root-INFO: grad norm: 4.761 4.713 0.671
2024-12-01-17:19:28-root-INFO: grad norm: 3.926 3.889 0.535
2024-12-01-17:19:29-root-INFO: grad norm: 3.841 3.804 0.535
2024-12-01-17:19:30-root-INFO: grad norm: 3.924 3.893 0.486
2024-12-01-17:19:31-root-INFO: grad norm: 3.878 3.843 0.521
2024-12-01-17:19:33-root-INFO: grad norm: 3.841 3.812 0.470
2024-12-01-17:19:34-root-INFO: grad norm: 3.823 3.789 0.509
2024-12-01-17:19:34-root-INFO: Loss Change: 92.234 -> 88.986
2024-12-01-17:19:34-root-INFO: Regularization Change: 0.000 -> 3.028
2024-12-01-17:19:34-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-01-17:19:34-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-17:19:35-root-INFO: step: 61 lr_xt 0.13774291
2024-12-01-17:19:35-root-INFO: grad norm: 4.716 4.670 0.659
2024-12-01-17:19:36-root-INFO: grad norm: 4.306 4.266 0.585
2024-12-01-17:19:37-root-INFO: grad norm: 3.776 3.743 0.500
2024-12-01-17:19:38-root-INFO: grad norm: 3.729 3.695 0.505
2024-12-01-17:19:39-root-INFO: grad norm: 3.759 3.730 0.471
2024-12-01-17:19:40-root-INFO: grad norm: 3.744 3.711 0.498
2024-12-01-17:19:41-root-INFO: grad norm: 3.748 3.719 0.462
2024-12-01-17:19:42-root-INFO: grad norm: 3.744 3.711 0.496
2024-12-01-17:19:43-root-INFO: Loss Change: 89.208 -> 86.288
2024-12-01-17:19:43-root-INFO: Regularization Change: 0.000 -> 2.951
2024-12-01-17:19:43-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-01-17:19:43-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-17:19:43-root-INFO: step: 60 lr_xt 0.14078630
2024-12-01-17:19:43-root-INFO: grad norm: 5.056 4.998 0.761
2024-12-01-17:19:44-root-INFO: grad norm: 4.566 4.522 0.628
2024-12-01-17:19:45-root-INFO: grad norm: 3.967 3.931 0.531
2024-12-01-17:19:46-root-INFO: grad norm: 3.915 3.879 0.533
2024-12-01-17:19:47-root-INFO: grad norm: 3.975 3.944 0.492
2024-12-01-17:19:48-root-INFO: grad norm: 3.932 3.897 0.522
2024-12-01-17:19:49-root-INFO: grad norm: 3.887 3.858 0.477
2024-12-01-17:19:50-root-INFO: grad norm: 3.874 3.840 0.512
2024-12-01-17:19:51-root-INFO: Loss Change: 86.617 -> 83.697
2024-12-01-17:19:51-root-INFO: Regularization Change: 0.000 -> 2.919
2024-12-01-17:19:51-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-01-17:19:51-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-17:19:51-root-INFO: step: 59 lr_xt 0.14387389
2024-12-01-17:19:52-root-INFO: grad norm: 5.177 5.119 0.768
2024-12-01-17:19:53-root-INFO: grad norm: 4.606 4.562 0.630
2024-12-01-17:19:54-root-INFO: grad norm: 3.871 3.835 0.524
2024-12-01-17:19:55-root-INFO: grad norm: 3.852 3.817 0.521
2024-12-01-17:19:56-root-INFO: grad norm: 3.993 3.963 0.488
2024-12-01-17:19:57-root-INFO: grad norm: 3.963 3.929 0.521
2024-12-01-17:19:58-root-INFO: grad norm: 3.930 3.901 0.477
2024-12-01-17:19:59-root-INFO: grad norm: 3.924 3.890 0.514
2024-12-01-17:19:59-root-INFO: Loss Change: 84.010 -> 81.141
2024-12-01-17:19:59-root-INFO: Regularization Change: 0.000 -> 2.895
2024-12-01-17:19:59-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-01-17:19:59-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-17:20:00-root-INFO: step: 58 lr_xt 0.14700566
2024-12-01-17:20:00-root-INFO: grad norm: 5.088 5.034 0.740
2024-12-01-17:20:01-root-INFO: grad norm: 4.621 4.576 0.644
2024-12-01-17:20:02-root-INFO: grad norm: 4.008 3.972 0.536
2024-12-01-17:20:03-root-INFO: grad norm: 3.964 3.927 0.543
2024-12-01-17:20:04-root-INFO: grad norm: 4.032 4.001 0.497
2024-12-01-17:20:05-root-INFO: grad norm: 3.986 3.950 0.533
2024-12-01-17:20:06-root-INFO: grad norm: 3.933 3.904 0.482
2024-12-01-17:20:07-root-INFO: grad norm: 3.923 3.888 0.521
2024-12-01-17:20:08-root-INFO: Loss Change: 81.297 -> 78.518
2024-12-01-17:20:08-root-INFO: Regularization Change: 0.000 -> 2.879
2024-12-01-17:20:08-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-01-17:20:08-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-17:20:08-root-INFO: step: 57 lr_xt 0.15018154
2024-12-01-17:20:09-root-INFO: grad norm: 5.323 5.268 0.765
2024-12-01-17:20:10-root-INFO: grad norm: 4.700 4.651 0.674
2024-12-01-17:20:11-root-INFO: grad norm: 3.879 3.843 0.528
2024-12-01-17:20:12-root-INFO: grad norm: 3.850 3.812 0.540
2024-12-01-17:20:13-root-INFO: grad norm: 3.988 3.958 0.492
2024-12-01-17:20:14-root-INFO: grad norm: 3.967 3.931 0.535
2024-12-01-17:20:15-root-INFO: grad norm: 3.955 3.925 0.483
2024-12-01-17:20:16-root-INFO: grad norm: 3.945 3.910 0.528
2024-12-01-17:20:16-root-INFO: Loss Change: 78.722 -> 75.905
2024-12-01-17:20:16-root-INFO: Regularization Change: 0.000 -> 2.890
2024-12-01-17:20:16-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-01-17:20:16-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-17:20:17-root-INFO: step: 56 lr_xt 0.15340147
2024-12-01-17:20:17-root-INFO: grad norm: 5.018 4.959 0.766
2024-12-01-17:20:18-root-INFO: grad norm: 4.590 4.544 0.645
2024-12-01-17:20:19-root-INFO: grad norm: 4.089 4.052 0.549
2024-12-01-17:20:20-root-INFO: grad norm: 4.037 3.998 0.556
2024-12-01-17:20:21-root-INFO: grad norm: 4.062 4.031 0.505
2024-12-01-17:20:22-root-INFO: grad norm: 4.013 3.977 0.540
2024-12-01-17:20:23-root-INFO: grad norm: 3.963 3.933 0.486
2024-12-01-17:20:24-root-INFO: grad norm: 3.947 3.911 0.528
2024-12-01-17:20:25-root-INFO: Loss Change: 76.219 -> 73.534
2024-12-01-17:20:25-root-INFO: Regularization Change: 0.000 -> 2.887
2024-12-01-17:20:25-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-01-17:20:25-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-17:20:25-root-INFO: step: 55 lr_xt 0.15666536
2024-12-01-17:20:25-root-INFO: grad norm: 5.176 5.118 0.773
2024-12-01-17:20:26-root-INFO: grad norm: 4.652 4.604 0.666
2024-12-01-17:20:27-root-INFO: grad norm: 4.001 3.965 0.536
2024-12-01-17:20:28-root-INFO: grad norm: 3.983 3.945 0.553
2024-12-01-17:20:29-root-INFO: grad norm: 4.093 4.062 0.499
2024-12-01-17:20:30-root-INFO: grad norm: 4.054 4.017 0.545
2024-12-01-17:20:31-root-INFO: grad norm: 4.010 3.980 0.487
2024-12-01-17:20:32-root-INFO: grad norm: 4.003 3.967 0.535
2024-12-01-17:20:33-root-INFO: Loss Change: 73.839 -> 71.168
2024-12-01-17:20:33-root-INFO: Regularization Change: 0.000 -> 2.883
2024-12-01-17:20:33-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-01-17:20:33-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-17:20:33-root-INFO: step: 54 lr_xt 0.15997308
2024-12-01-17:20:34-root-INFO: grad norm: 5.077 5.019 0.765
2024-12-01-17:20:35-root-INFO: grad norm: 4.639 4.593 0.653
2024-12-01-17:20:36-root-INFO: grad norm: 4.116 4.079 0.552
2024-12-01-17:20:37-root-INFO: grad norm: 4.101 4.062 0.561
2024-12-01-17:20:38-root-INFO: grad norm: 4.174 4.142 0.515
2024-12-01-17:20:39-root-INFO: grad norm: 4.135 4.097 0.554
2024-12-01-17:20:40-root-INFO: grad norm: 4.085 4.054 0.502
2024-12-01-17:20:41-root-INFO: grad norm: 4.079 4.042 0.544
2024-12-01-17:20:41-root-INFO: Loss Change: 71.352 -> 68.760
2024-12-01-17:20:41-root-INFO: Regularization Change: 0.000 -> 2.905
2024-12-01-17:20:41-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-01-17:20:41-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-17:20:42-root-INFO: step: 53 lr_xt 0.16332449
2024-12-01-17:20:42-root-INFO: grad norm: 5.167 5.113 0.749
2024-12-01-17:20:43-root-INFO: grad norm: 4.708 4.660 0.674
2024-12-01-17:20:44-root-INFO: grad norm: 4.150 4.114 0.548
2024-12-01-17:20:45-root-INFO: grad norm: 4.128 4.088 0.572
2024-12-01-17:20:46-root-INFO: grad norm: 4.185 4.153 0.514
2024-12-01-17:20:47-root-INFO: grad norm: 4.156 4.118 0.562
2024-12-01-17:20:48-root-INFO: grad norm: 4.124 4.093 0.505
2024-12-01-17:20:49-root-INFO: grad norm: 4.116 4.079 0.553
2024-12-01-17:20:50-root-INFO: Loss Change: 69.090 -> 66.535
2024-12-01-17:20:50-root-INFO: Regularization Change: 0.000 -> 2.884
2024-12-01-17:20:50-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-01-17:20:50-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-17:20:50-root-INFO: step: 52 lr_xt 0.16671942
2024-12-01-17:20:51-root-INFO: grad norm: 5.016 4.962 0.731
2024-12-01-17:20:52-root-INFO: grad norm: 4.618 4.571 0.660
2024-12-01-17:20:53-root-INFO: grad norm: 4.166 4.132 0.535
2024-12-01-17:20:54-root-INFO: grad norm: 4.114 4.074 0.566
2024-12-01-17:20:55-root-INFO: grad norm: 4.106 4.075 0.496
2024-12-01-17:20:56-root-INFO: grad norm: 4.064 4.027 0.546
2024-12-01-17:20:57-root-INFO: grad norm: 4.023 3.994 0.482
2024-12-01-17:20:58-root-INFO: grad norm: 4.006 3.970 0.534
2024-12-01-17:20:58-root-INFO: Loss Change: 66.762 -> 64.246
2024-12-01-17:20:58-root-INFO: Regularization Change: 0.000 -> 2.896
2024-12-01-17:20:58-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-01-17:20:58-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-17:20:59-root-INFO: step: 51 lr_xt 0.17015769
2024-12-01-17:20:59-root-INFO: grad norm: 5.171 5.109 0.804
2024-12-01-17:21:00-root-INFO: grad norm: 4.703 4.656 0.663
2024-12-01-17:21:01-root-INFO: grad norm: 4.195 4.158 0.554
2024-12-01-17:21:02-root-INFO: grad norm: 4.130 4.092 0.559
2024-12-01-17:21:03-root-INFO: grad norm: 4.110 4.079 0.503
2024-12-01-17:21:04-root-INFO: grad norm: 4.063 4.027 0.537
2024-12-01-17:21:05-root-INFO: grad norm: 4.013 3.984 0.484
2024-12-01-17:21:06-root-INFO: grad norm: 3.994 3.959 0.525
2024-12-01-17:21:07-root-INFO: Loss Change: 64.585 -> 62.018
2024-12-01-17:21:07-root-INFO: Regularization Change: 0.000 -> 2.923
2024-12-01-17:21:07-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-01-17:21:07-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-17:21:07-root-INFO: step: 50 lr_xt 0.17363908
2024-12-01-17:21:07-root-INFO: grad norm: 5.043 4.987 0.750
2024-12-01-17:21:08-root-INFO: grad norm: 4.599 4.553 0.648
2024-12-01-17:21:09-root-INFO: grad norm: 4.122 4.088 0.527
2024-12-01-17:21:10-root-INFO: grad norm: 4.060 4.022 0.550
2024-12-01-17:21:11-root-INFO: grad norm: 4.030 4.001 0.485
2024-12-01-17:21:12-root-INFO: grad norm: 3.992 3.956 0.529
2024-12-01-17:21:13-root-INFO: grad norm: 3.955 3.927 0.471
2024-12-01-17:21:14-root-INFO: grad norm: 3.937 3.903 0.518
2024-12-01-17:21:15-root-INFO: Loss Change: 62.105 -> 59.594
2024-12-01-17:21:15-root-INFO: Regularization Change: 0.000 -> 2.937
2024-12-01-17:21:15-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-01-17:21:15-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-17:21:15-root-INFO: step: 49 lr_xt 0.17716334
2024-12-01-17:21:16-root-INFO: grad norm: 5.099 5.039 0.776
2024-12-01-17:21:17-root-INFO: grad norm: 4.560 4.514 0.650
2024-12-01-17:21:18-root-INFO: grad norm: 4.039 4.005 0.522
2024-12-01-17:21:19-root-INFO: grad norm: 3.942 3.905 0.534
2024-12-01-17:21:20-root-INFO: grad norm: 3.873 3.845 0.470
2024-12-01-17:21:21-root-INFO: grad norm: 3.842 3.809 0.505
2024-12-01-17:21:22-root-INFO: grad norm: 3.821 3.794 0.455
2024-12-01-17:21:23-root-INFO: grad norm: 3.811 3.778 0.496
2024-12-01-17:21:24-root-INFO: Loss Change: 59.990 -> 57.413
2024-12-01-17:21:24-root-INFO: Regularization Change: 0.000 -> 2.962
2024-12-01-17:21:24-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-01-17:21:24-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-17:21:24-root-INFO: step: 48 lr_xt 0.18073022
2024-12-01-17:21:24-root-INFO: grad norm: 4.519 4.471 0.654
2024-12-01-17:21:25-root-INFO: grad norm: 4.243 4.202 0.589
2024-12-01-17:21:27-root-INFO: grad norm: 4.001 3.972 0.483
2024-12-01-17:21:28-root-INFO: grad norm: 3.925 3.889 0.524
2024-12-01-17:21:29-root-INFO: grad norm: 3.850 3.823 0.449
2024-12-01-17:21:30-root-INFO: grad norm: 3.816 3.783 0.500
2024-12-01-17:21:31-root-INFO: grad norm: 3.784 3.759 0.438
2024-12-01-17:21:32-root-INFO: grad norm: 3.769 3.737 0.490
2024-12-01-17:21:32-root-INFO: Loss Change: 57.569 -> 55.274
2024-12-01-17:21:32-root-INFO: Regularization Change: 0.000 -> 2.930
2024-12-01-17:21:32-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-01-17:21:32-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-17:21:33-root-INFO: step: 47 lr_xt 0.18433941
2024-12-01-17:21:33-root-INFO: grad norm: 4.825 4.765 0.757
2024-12-01-17:21:34-root-INFO: grad norm: 4.429 4.386 0.620
2024-12-01-17:21:35-root-INFO: grad norm: 4.081 4.049 0.516
2024-12-01-17:21:36-root-INFO: grad norm: 3.966 3.930 0.529
2024-12-01-17:21:37-root-INFO: grad norm: 3.854 3.826 0.462
2024-12-01-17:21:38-root-INFO: grad norm: 3.803 3.771 0.496
2024-12-01-17:21:39-root-INFO: grad norm: 3.756 3.729 0.442
2024-12-01-17:21:40-root-INFO: grad norm: 3.733 3.702 0.483
2024-12-01-17:21:41-root-INFO: Loss Change: 55.610 -> 53.190
2024-12-01-17:21:41-root-INFO: Regularization Change: 0.000 -> 2.955
2024-12-01-17:21:41-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-01-17:21:41-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-17:21:41-root-INFO: step: 46 lr_xt 0.18799060
2024-12-01-17:21:42-root-INFO: grad norm: 4.631 4.578 0.695
2024-12-01-17:21:43-root-INFO: grad norm: 4.256 4.214 0.593
2024-12-01-17:21:44-root-INFO: grad norm: 3.930 3.901 0.480
2024-12-01-17:21:45-root-INFO: grad norm: 3.800 3.766 0.506
2024-12-01-17:21:46-root-INFO: grad norm: 3.678 3.653 0.432
2024-12-01-17:21:47-root-INFO: grad norm: 3.626 3.595 0.472
2024-12-01-17:21:48-root-INFO: grad norm: 3.581 3.557 0.414
2024-12-01-17:21:49-root-INFO: grad norm: 3.563 3.534 0.459
2024-12-01-17:21:49-root-INFO: Loss Change: 53.444 -> 51.084
2024-12-01-17:21:49-root-INFO: Regularization Change: 0.000 -> 2.941
2024-12-01-17:21:49-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-01-17:21:49-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-17:21:50-root-INFO: step: 45 lr_xt 0.19168344
2024-12-01-17:21:50-root-INFO: grad norm: 4.493 4.440 0.692
2024-12-01-17:21:51-root-INFO: grad norm: 4.147 4.107 0.577
2024-12-01-17:21:52-root-INFO: grad norm: 3.861 3.832 0.471
2024-12-01-17:21:53-root-INFO: grad norm: 3.723 3.690 0.493
2024-12-01-17:21:54-root-INFO: grad norm: 3.596 3.571 0.423
2024-12-01-17:21:55-root-INFO: grad norm: 3.535 3.505 0.459
2024-12-01-17:21:56-root-INFO: grad norm: 3.486 3.462 0.403
2024-12-01-17:21:57-root-INFO: grad norm: 3.465 3.436 0.445
2024-12-01-17:21:58-root-INFO: Loss Change: 51.259 -> 48.941
2024-12-01-17:21:58-root-INFO: Regularization Change: 0.000 -> 2.954
2024-12-01-17:21:58-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-01-17:21:58-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-17:21:58-root-INFO: step: 44 lr_xt 0.19541757
2024-12-01-17:21:58-root-INFO: grad norm: 4.351 4.300 0.665
2024-12-01-17:22:00-root-INFO: grad norm: 4.048 4.011 0.550
2024-12-01-17:22:01-root-INFO: grad norm: 3.801 3.774 0.456
2024-12-01-17:22:02-root-INFO: grad norm: 3.666 3.635 0.477
2024-12-01-17:22:03-root-INFO: grad norm: 3.542 3.518 0.412
2024-12-01-17:22:04-root-INFO: grad norm: 3.482 3.454 0.446
2024-12-01-17:22:05-root-INFO: grad norm: 3.436 3.413 0.395
2024-12-01-17:22:06-root-INFO: grad norm: 3.421 3.393 0.435
2024-12-01-17:22:06-root-INFO: Loss Change: 49.239 -> 46.992
2024-12-01-17:22:06-root-INFO: Regularization Change: 0.000 -> 2.967
2024-12-01-17:22:06-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-01-17:22:06-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-17:22:07-root-INFO: step: 43 lr_xt 0.19919257
2024-12-01-17:22:07-root-INFO: grad norm: 4.296 4.247 0.649
2024-12-01-17:22:08-root-INFO: grad norm: 3.982 3.944 0.547
2024-12-01-17:22:09-root-INFO: grad norm: 3.742 3.715 0.447
2024-12-01-17:22:10-root-INFO: grad norm: 3.606 3.575 0.470
2024-12-01-17:22:11-root-INFO: grad norm: 3.488 3.465 0.405
2024-12-01-17:22:12-root-INFO: grad norm: 3.426 3.398 0.439
2024-12-01-17:22:13-root-INFO: grad norm: 3.377 3.355 0.389
2024-12-01-17:22:14-root-INFO: grad norm: 3.354 3.326 0.425
2024-12-01-17:22:15-root-INFO: Loss Change: 47.078 -> 44.828
2024-12-01-17:22:15-root-INFO: Regularization Change: 0.000 -> 3.002
2024-12-01-17:22:15-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-01-17:22:15-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-17:22:15-root-INFO: step: 42 lr_xt 0.20300803
2024-12-01-17:22:15-root-INFO: grad norm: 3.958 3.917 0.571
2024-12-01-17:22:16-root-INFO: grad norm: 3.753 3.719 0.501
2024-12-01-17:22:17-root-INFO: grad norm: 3.604 3.580 0.419
2024-12-01-17:22:18-root-INFO: grad norm: 3.495 3.466 0.449
2024-12-01-17:22:19-root-INFO: grad norm: 3.398 3.376 0.387
2024-12-01-17:22:20-root-INFO: grad norm: 3.333 3.305 0.424
2024-12-01-17:22:21-root-INFO: grad norm: 3.279 3.258 0.372
2024-12-01-17:22:22-root-INFO: grad norm: 3.246 3.219 0.412
2024-12-01-17:22:23-root-INFO: Loss Change: 44.917 -> 42.814
2024-12-01-17:22:23-root-INFO: Regularization Change: 0.000 -> 2.977
2024-12-01-17:22:23-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-01-17:22:23-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-17:22:23-root-INFO: step: 41 lr_xt 0.20721469
2024-12-01-17:22:24-root-INFO: grad norm: 4.422 4.361 0.735
2024-12-01-17:22:25-root-INFO: grad norm: 3.955 3.915 0.557
2024-12-01-17:22:26-root-INFO: grad norm: 3.613 3.587 0.433
2024-12-01-17:22:27-root-INFO: grad norm: 3.392 3.363 0.443
2024-12-01-17:22:28-root-INFO: grad norm: 3.210 3.188 0.370
2024-12-01-17:22:29-root-INFO: grad norm: 3.096 3.070 0.396
2024-12-01-17:22:30-root-INFO: grad norm: 3.009 2.990 0.343
2024-12-01-17:22:31-root-INFO: grad norm: 2.961 2.937 0.374
2024-12-01-17:22:31-root-INFO: Loss Change: 43.152 -> 40.746
2024-12-01-17:22:31-root-INFO: Regularization Change: 0.000 -> 3.020
2024-12-01-17:22:31-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-01-17:22:31-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-17:22:32-root-INFO: step: 40 lr_xt 0.21110784
2024-12-01-17:22:32-root-INFO: grad norm: 3.925 3.877 0.615
2024-12-01-17:22:33-root-INFO: grad norm: 3.561 3.527 0.492
2024-12-01-17:22:34-root-INFO: grad norm: 3.300 3.277 0.388
2024-12-01-17:22:35-root-INFO: grad norm: 3.119 3.092 0.402
2024-12-01-17:22:36-root-INFO: grad norm: 2.975 2.956 0.340
2024-12-01-17:22:37-root-INFO: grad norm: 2.880 2.857 0.366
2024-12-01-17:22:38-root-INFO: grad norm: 2.812 2.794 0.319
2024-12-01-17:22:39-root-INFO: grad norm: 2.772 2.750 0.350
2024-12-01-17:22:40-root-INFO: Loss Change: 40.989 -> 38.804
2024-12-01-17:22:40-root-INFO: Regularization Change: 0.000 -> 2.963
2024-12-01-17:22:40-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-01-17:22:40-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-17:22:40-root-INFO: step: 39 lr_xt 0.21503976
2024-12-01-17:22:40-root-INFO: grad norm: 3.470 3.428 0.536
2024-12-01-17:22:42-root-INFO: grad norm: 3.219 3.188 0.442
2024-12-01-17:22:43-root-INFO: grad norm: 3.054 3.033 0.358
2024-12-01-17:22:43-root-INFO: grad norm: 2.930 2.905 0.377
2024-12-01-17:22:45-root-INFO: grad norm: 2.830 2.811 0.322
2024-12-01-17:22:46-root-INFO: grad norm: 2.759 2.736 0.351
2024-12-01-17:22:47-root-INFO: grad norm: 2.708 2.690 0.306
2024-12-01-17:22:48-root-INFO: grad norm: 2.676 2.654 0.339
2024-12-01-17:22:48-root-INFO: Loss Change: 38.865 -> 36.884
2024-12-01-17:22:48-root-INFO: Regularization Change: 0.000 -> 2.929
2024-12-01-17:22:48-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-01-17:22:48-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-17:22:49-root-INFO: step: 38 lr_xt 0.21900989
2024-12-01-17:22:49-root-INFO: grad norm: 3.533 3.485 0.581
2024-12-01-17:22:50-root-INFO: grad norm: 3.215 3.184 0.448
2024-12-01-17:22:51-root-INFO: grad norm: 2.992 2.970 0.361
2024-12-01-17:22:52-root-INFO: grad norm: 2.829 2.805 0.363
2024-12-01-17:22:53-root-INFO: grad norm: 2.701 2.682 0.314
2024-12-01-17:22:54-root-INFO: grad norm: 2.609 2.589 0.330
2024-12-01-17:22:55-root-INFO: grad norm: 2.544 2.527 0.294
2024-12-01-17:22:56-root-INFO: grad norm: 2.502 2.483 0.314
2024-12-01-17:22:57-root-INFO: Loss Change: 36.985 -> 34.948
2024-12-01-17:22:57-root-INFO: Regularization Change: 0.000 -> 2.941
2024-12-01-17:22:57-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-01-17:22:57-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-17:22:57-root-INFO: step: 37 lr_xt 0.22301766
2024-12-01-17:22:57-root-INFO: grad norm: 3.441 3.395 0.562
2024-12-01-17:22:58-root-INFO: grad norm: 3.073 3.042 0.434
2024-12-01-17:22:59-root-INFO: grad norm: 2.819 2.799 0.337
2024-12-01-17:23:00-root-INFO: grad norm: 2.636 2.614 0.341
2024-12-01-17:23:01-root-INFO: grad norm: 2.499 2.483 0.289
2024-12-01-17:23:02-root-INFO: grad norm: 2.401 2.382 0.304
2024-12-01-17:23:03-root-INFO: grad norm: 2.332 2.317 0.269
2024-12-01-17:23:04-root-INFO: grad norm: 2.287 2.269 0.288
2024-12-01-17:23:05-root-INFO: Loss Change: 35.092 -> 33.055
2024-12-01-17:23:05-root-INFO: Regularization Change: 0.000 -> 2.933
2024-12-01-17:23:05-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-01-17:23:05-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-17:23:05-root-INFO: step: 36 lr_xt 0.22706247
2024-12-01-17:23:06-root-INFO: grad norm: 2.980 2.940 0.489
2024-12-01-17:23:07-root-INFO: grad norm: 2.747 2.722 0.373
2024-12-01-17:23:08-root-INFO: grad norm: 2.600 2.580 0.316
2024-12-01-17:23:09-root-INFO: grad norm: 2.488 2.468 0.315
2024-12-01-17:23:10-root-INFO: grad norm: 2.397 2.381 0.279
2024-12-01-17:23:11-root-INFO: grad norm: 2.329 2.311 0.292
2024-12-01-17:23:12-root-INFO: grad norm: 2.277 2.262 0.264
2024-12-01-17:23:13-root-INFO: grad norm: 2.241 2.224 0.280
2024-12-01-17:23:14-root-INFO: Loss Change: 33.091 -> 31.270
2024-12-01-17:23:14-root-INFO: Regularization Change: 0.000 -> 2.889
2024-12-01-17:23:14-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-01-17:23:14-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-17:23:14-root-INFO: step: 35 lr_xt 0.23114370
2024-12-01-17:23:14-root-INFO: grad norm: 3.058 3.017 0.498
2024-12-01-17:23:15-root-INFO: grad norm: 2.737 2.709 0.392
2024-12-01-17:23:16-root-INFO: grad norm: 2.508 2.489 0.303
2024-12-01-17:23:17-root-INFO: grad norm: 2.341 2.321 0.304
2024-12-01-17:23:18-root-INFO: grad norm: 2.218 2.203 0.259
2024-12-01-17:23:19-root-INFO: grad norm: 2.126 2.109 0.269
2024-12-01-17:23:20-root-INFO: grad norm: 2.061 2.047 0.240
2024-12-01-17:23:21-root-INFO: grad norm: 2.016 2.000 0.252
2024-12-01-17:23:22-root-INFO: Loss Change: 31.385 -> 29.519
2024-12-01-17:23:22-root-INFO: Regularization Change: 0.000 -> 2.854
2024-12-01-17:23:22-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-01-17:23:22-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-17:23:22-root-INFO: step: 34 lr_xt 0.23526068
2024-12-01-17:23:23-root-INFO: grad norm: 2.717 2.680 0.445
2024-12-01-17:23:24-root-INFO: grad norm: 2.442 2.418 0.340
2024-12-01-17:23:25-root-INFO: grad norm: 2.267 2.250 0.272
2024-12-01-17:23:26-root-INFO: grad norm: 2.136 2.119 0.274
2024-12-01-17:23:27-root-INFO: grad norm: 2.041 2.027 0.236
2024-12-01-17:23:28-root-INFO: grad norm: 1.967 1.951 0.248
2024-12-01-17:23:29-root-INFO: grad norm: 1.913 1.900 0.220
2024-12-01-17:23:30-root-INFO: grad norm: 1.873 1.858 0.234
2024-12-01-17:23:30-root-INFO: Loss Change: 29.537 -> 27.811
2024-12-01-17:23:31-root-INFO: Regularization Change: 0.000 -> 2.789
2024-12-01-17:23:31-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-01-17:23:31-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-17:23:31-root-INFO: step: 33 lr_xt 0.23941272
2024-12-01-17:23:31-root-INFO: grad norm: 2.641 2.601 0.457
2024-12-01-17:23:32-root-INFO: grad norm: 2.338 2.315 0.324
2024-12-01-17:23:33-root-INFO: grad norm: 2.143 2.127 0.263
2024-12-01-17:23:34-root-INFO: grad norm: 2.007 1.991 0.254
2024-12-01-17:23:35-root-INFO: grad norm: 1.907 1.894 0.223
2024-12-01-17:23:36-root-INFO: grad norm: 1.829 1.815 0.228
2024-12-01-17:23:37-root-INFO: grad norm: 1.772 1.760 0.206
2024-12-01-17:23:38-root-INFO: grad norm: 1.729 1.716 0.214
2024-12-01-17:23:39-root-INFO: Loss Change: 27.710 -> 26.020
2024-12-01-17:23:39-root-INFO: Regularization Change: 0.000 -> 2.753
2024-12-01-17:23:39-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-01-17:23:39-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-17:23:39-root-INFO: step: 32 lr_xt 0.24359912
2024-12-01-17:23:40-root-INFO: grad norm: 2.634 2.594 0.454
2024-12-01-17:23:41-root-INFO: grad norm: 2.244 2.221 0.323
2024-12-01-17:23:42-root-INFO: grad norm: 1.997 1.983 0.243
2024-12-01-17:23:43-root-INFO: grad norm: 1.837 1.821 0.237
2024-12-01-17:23:44-root-INFO: grad norm: 1.725 1.713 0.202
2024-12-01-17:23:45-root-INFO: grad norm: 1.642 1.628 0.208
2024-12-01-17:23:46-root-INFO: grad norm: 1.580 1.570 0.184
2024-12-01-17:23:47-root-INFO: grad norm: 1.535 1.523 0.193
2024-12-01-17:23:47-root-INFO: Loss Change: 26.127 -> 24.437
2024-12-01-17:23:47-root-INFO: Regularization Change: 0.000 -> 2.715
2024-12-01-17:23:47-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-01-17:23:47-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-17:23:48-root-INFO: step: 31 lr_xt 0.24781911
2024-12-01-17:23:48-root-INFO: grad norm: 2.210 2.176 0.384
2024-12-01-17:23:49-root-INFO: grad norm: 1.935 1.916 0.272
2024-12-01-17:23:50-root-INFO: grad norm: 1.781 1.768 0.216
2024-12-01-17:23:51-root-INFO: grad norm: 1.681 1.668 0.214
2024-12-01-17:23:52-root-INFO: grad norm: 1.606 1.595 0.187
2024-12-01-17:23:53-root-INFO: grad norm: 1.549 1.537 0.194
2024-12-01-17:23:54-root-INFO: grad norm: 1.505 1.495 0.174
2024-12-01-17:23:55-root-INFO: grad norm: 1.472 1.461 0.184
2024-12-01-17:23:56-root-INFO: Loss Change: 24.415 -> 22.911
2024-12-01-17:23:56-root-INFO: Regularization Change: 0.000 -> 2.628
2024-12-01-17:23:56-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-01-17:23:56-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-17:23:56-root-INFO: step: 30 lr_xt 0.25207194
2024-12-01-17:23:56-root-INFO: grad norm: 2.248 2.214 0.388
2024-12-01-17:23:57-root-INFO: grad norm: 1.900 1.880 0.271
2024-12-01-17:23:58-root-INFO: grad norm: 1.676 1.664 0.202
2024-12-01-17:23:59-root-INFO: grad norm: 1.546 1.533 0.199
2024-12-01-17:24:00-root-INFO: grad norm: 1.455 1.446 0.167
2024-12-01-17:24:01-root-INFO: grad norm: 1.388 1.377 0.176
2024-12-01-17:24:02-root-INFO: grad norm: 1.338 1.329 0.154
2024-12-01-17:24:03-root-INFO: grad norm: 1.300 1.289 0.164
2024-12-01-17:24:04-root-INFO: Loss Change: 22.894 -> 21.391
2024-12-01-17:24:04-root-INFO: Regularization Change: 0.000 -> 2.584
2024-12-01-17:24:04-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-01-17:24:04-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-17:24:05-root-INFO: step: 29 lr_xt 0.25635679
2024-12-01-17:24:05-root-INFO: grad norm: 2.077 2.042 0.378
2024-12-01-17:24:06-root-INFO: grad norm: 1.708 1.690 0.245
2024-12-01-17:24:07-root-INFO: grad norm: 1.509 1.497 0.184
2024-12-01-17:24:08-root-INFO: grad norm: 1.401 1.390 0.178
2024-12-01-17:24:09-root-INFO: grad norm: 1.322 1.313 0.153
2024-12-01-17:24:10-root-INFO: grad norm: 1.265 1.255 0.158
2024-12-01-17:24:11-root-INFO: grad norm: 1.219 1.211 0.141
2024-12-01-17:24:12-root-INFO: grad norm: 1.186 1.176 0.149
2024-12-01-17:24:12-root-INFO: Loss Change: 21.404 -> 19.966
2024-12-01-17:24:13-root-INFO: Regularization Change: 0.000 -> 2.544
2024-12-01-17:24:13-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-01-17:24:13-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-17:24:13-root-INFO: step: 28 lr_xt 0.26067283
2024-12-01-17:24:13-root-INFO: grad norm: 2.026 1.992 0.368
2024-12-01-17:24:14-root-INFO: grad norm: 1.621 1.604 0.233
2024-12-01-17:24:15-root-INFO: grad norm: 1.391 1.381 0.168
2024-12-01-17:24:16-root-INFO: grad norm: 1.279 1.269 0.163
2024-12-01-17:24:17-root-INFO: grad norm: 1.201 1.194 0.138
2024-12-01-17:24:18-root-INFO: grad norm: 1.147 1.138 0.145
2024-12-01-17:24:19-root-INFO: grad norm: 1.104 1.096 0.127
2024-12-01-17:24:20-root-INFO: grad norm: 1.072 1.064 0.136
2024-12-01-17:24:21-root-INFO: Loss Change: 19.856 -> 18.455
2024-12-01-17:24:21-root-INFO: Regularization Change: 0.000 -> 2.501
2024-12-01-17:24:21-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-01-17:24:21-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-17:24:21-root-INFO: step: 27 lr_xt 0.26501920
2024-12-01-17:24:22-root-INFO: grad norm: 1.751 1.722 0.317
2024-12-01-17:24:23-root-INFO: grad norm: 1.410 1.395 0.202
2024-12-01-17:24:24-root-INFO: grad norm: 1.228 1.219 0.149
2024-12-01-17:24:25-root-INFO: grad norm: 1.140 1.130 0.147
2024-12-01-17:24:26-root-INFO: grad norm: 1.072 1.065 0.125
2024-12-01-17:24:27-root-INFO: grad norm: 1.027 1.018 0.132
2024-12-01-17:24:28-root-INFO: grad norm: 0.989 0.982 0.116
2024-12-01-17:24:29-root-INFO: grad norm: 0.962 0.954 0.124
2024-12-01-17:24:29-root-INFO: Loss Change: 18.434 -> 17.143
2024-12-01-17:24:29-root-INFO: Regularization Change: 0.000 -> 2.423
2024-12-01-17:24:29-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-01-17:24:29-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-17:24:30-root-INFO: step: 26 lr_xt 0.26939500
2024-12-01-17:24:30-root-INFO: grad norm: 1.770 1.739 0.329
2024-12-01-17:24:31-root-INFO: grad norm: 1.362 1.348 0.199
2024-12-01-17:24:32-root-INFO: grad norm: 1.153 1.144 0.141
2024-12-01-17:24:33-root-INFO: grad norm: 1.066 1.058 0.138
2024-12-01-17:24:34-root-INFO: grad norm: 1.003 0.996 0.116
2024-12-01-17:24:35-root-INFO: grad norm: 0.962 0.954 0.123
2024-12-01-17:24:36-root-INFO: grad norm: 0.929 0.922 0.109
2024-12-01-17:24:37-root-INFO: grad norm: 0.905 0.898 0.117
2024-12-01-17:24:38-root-INFO: Loss Change: 17.121 -> 15.848
2024-12-01-17:24:38-root-INFO: Regularization Change: 0.000 -> 2.391
2024-12-01-17:24:38-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-01-17:24:38-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-17:24:38-root-INFO: step: 25 lr_xt 0.27379933
2024-12-01-17:24:39-root-INFO: grad norm: 1.646 1.616 0.313
2024-12-01-17:24:40-root-INFO: grad norm: 1.236 1.223 0.174
2024-12-01-17:24:41-root-INFO: grad norm: 1.041 1.034 0.126
2024-12-01-17:24:42-root-INFO: grad norm: 0.964 0.956 0.122
2024-12-01-17:24:43-root-INFO: grad norm: 0.906 0.900 0.105
2024-12-01-17:24:44-root-INFO: grad norm: 0.872 0.865 0.111
2024-12-01-17:24:44-root-INFO: grad norm: 0.844 0.838 0.099
2024-12-01-17:24:45-root-INFO: grad norm: 0.825 0.819 0.106
2024-12-01-17:24:46-root-INFO: Loss Change: 15.719 -> 14.500
2024-12-01-17:24:46-root-INFO: Regularization Change: 0.000 -> 2.346
2024-12-01-17:24:46-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-01-17:24:46-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-17:24:47-root-INFO: step: 24 lr_xt 0.27823123
2024-12-01-17:24:47-root-INFO: grad norm: 1.624 1.596 0.302
2024-12-01-17:24:48-root-INFO: grad norm: 1.184 1.172 0.169
2024-12-01-17:24:49-root-INFO: grad norm: 0.973 0.966 0.116
2024-12-01-17:24:50-root-INFO: grad norm: 0.900 0.893 0.116
2024-12-01-17:24:51-root-INFO: grad norm: 0.848 0.842 0.097
2024-12-01-17:24:52-root-INFO: grad norm: 0.820 0.813 0.105
2024-12-01-17:24:53-root-INFO: grad norm: 0.796 0.791 0.093
2024-12-01-17:24:54-root-INFO: grad norm: 0.782 0.775 0.101
2024-12-01-17:24:55-root-INFO: Loss Change: 14.524 -> 13.345
2024-12-01-17:24:55-root-INFO: Regularization Change: 0.000 -> 2.291
2024-12-01-17:24:55-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-01-17:24:55-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-17:24:55-root-INFO: step: 23 lr_xt 0.28268972
2024-12-01-17:24:55-root-INFO: grad norm: 1.531 1.502 0.294
2024-12-01-17:24:56-root-INFO: grad norm: 1.092 1.081 0.160
2024-12-01-17:24:58-root-INFO: grad norm: 0.919 0.913 0.108
2024-12-01-17:24:59-root-INFO: grad norm: 0.860 0.853 0.110
2024-12-01-17:25:00-root-INFO: grad norm: 0.813 0.808 0.091
2024-12-01-17:25:01-root-INFO: grad norm: 0.790 0.784 0.100
2024-12-01-17:25:02-root-INFO: grad norm: 0.770 0.765 0.087
2024-12-01-17:25:03-root-INFO: grad norm: 0.758 0.752 0.096
2024-12-01-17:25:03-root-INFO: Loss Change: 13.238 -> 12.121
2024-12-01-17:25:03-root-INFO: Regularization Change: 0.000 -> 2.217
2024-12-01-17:25:03-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-01-17:25:03-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-17:25:04-root-INFO: step: 22 lr_xt 0.28717380
2024-12-01-17:25:04-root-INFO: grad norm: 1.515 1.489 0.280
2024-12-01-17:25:05-root-INFO: grad norm: 1.083 1.072 0.150
2024-12-01-17:25:06-root-INFO: grad norm: 0.876 0.870 0.099
2024-12-01-17:25:07-root-INFO: grad norm: 0.815 0.809 0.102
2024-12-01-17:25:08-root-INFO: grad norm: 0.771 0.766 0.084
2024-12-01-17:25:09-root-INFO: grad norm: 0.750 0.744 0.093
2024-12-01-17:25:10-root-INFO: grad norm: 0.732 0.728 0.080
2024-12-01-17:25:11-root-INFO: grad norm: 0.722 0.716 0.090
2024-12-01-17:25:12-root-INFO: Loss Change: 12.108 -> 11.051
2024-12-01-17:25:12-root-INFO: Regularization Change: 0.000 -> 2.119
2024-12-01-17:25:12-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-01-17:25:12-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-17:25:12-root-INFO: step: 21 lr_xt 0.29168243
2024-12-01-17:25:13-root-INFO: grad norm: 1.446 1.421 0.267
2024-12-01-17:25:14-root-INFO: grad norm: 1.019 1.009 0.144
2024-12-01-17:25:15-root-INFO: grad norm: 0.854 0.849 0.092
2024-12-01-17:25:15-root-INFO: grad norm: 0.802 0.796 0.099
2024-12-01-17:25:17-root-INFO: grad norm: 0.759 0.755 0.078
2024-12-01-17:25:18-root-INFO: grad norm: 0.738 0.733 0.089
2024-12-01-17:25:19-root-INFO: grad norm: 0.719 0.715 0.075
2024-12-01-17:25:20-root-INFO: grad norm: 0.708 0.703 0.085
2024-12-01-17:25:20-root-INFO: Loss Change: 11.000 -> 10.001
2024-12-01-17:25:20-root-INFO: Regularization Change: 0.000 -> 2.036
2024-12-01-17:25:20-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-01-17:25:20-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-17:25:21-root-INFO: step: 20 lr_xt 0.29621455
2024-12-01-17:25:21-root-INFO: grad norm: 1.508 1.481 0.285
2024-12-01-17:25:22-root-INFO: grad norm: 1.034 1.025 0.137
2024-12-01-17:25:23-root-INFO: grad norm: 0.846 0.841 0.089
2024-12-01-17:25:24-root-INFO: grad norm: 0.786 0.781 0.091
2024-12-01-17:25:25-root-INFO: grad norm: 0.736 0.733 0.073
2024-12-01-17:25:26-root-INFO: grad norm: 0.712 0.707 0.082
2024-12-01-17:25:27-root-INFO: grad norm: 0.689 0.685 0.069
2024-12-01-17:25:28-root-INFO: grad norm: 0.674 0.670 0.078
2024-12-01-17:25:29-root-INFO: Loss Change: 9.997 -> 9.024
2024-12-01-17:25:29-root-INFO: Regularization Change: 0.000 -> 1.956
2024-12-01-17:25:29-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-01-17:25:29-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-17:25:29-root-INFO: step: 19 lr_xt 0.30076908
2024-12-01-17:25:29-root-INFO: grad norm: 1.373 1.351 0.249
2024-12-01-17:25:30-root-INFO: grad norm: 0.948 0.940 0.124
2024-12-01-17:25:31-root-INFO: grad norm: 0.805 0.801 0.079
2024-12-01-17:25:32-root-INFO: grad norm: 0.750 0.745 0.086
2024-12-01-17:25:33-root-INFO: grad norm: 0.701 0.698 0.066
2024-12-01-17:25:34-root-INFO: grad norm: 0.675 0.671 0.076
2024-12-01-17:25:35-root-INFO: grad norm: 0.650 0.647 0.062
2024-12-01-17:25:36-root-INFO: grad norm: 0.634 0.630 0.072
2024-12-01-17:25:37-root-INFO: Loss Change: 8.984 -> 8.090
2024-12-01-17:25:37-root-INFO: Regularization Change: 0.000 -> 1.853
2024-12-01-17:25:37-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-01-17:25:37-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-17:25:37-root-INFO: step: 18 lr_xt 0.30534490
2024-12-01-17:25:38-root-INFO: grad norm: 1.473 1.447 0.277
2024-12-01-17:25:39-root-INFO: grad norm: 0.975 0.967 0.125
2024-12-01-17:25:40-root-INFO: grad norm: 0.818 0.814 0.080
2024-12-01-17:25:41-root-INFO: grad norm: 0.752 0.748 0.083
2024-12-01-17:25:42-root-INFO: grad norm: 0.694 0.691 0.063
2024-12-01-17:25:43-root-INFO: grad norm: 0.662 0.658 0.072
2024-12-01-17:25:44-root-INFO: grad norm: 0.631 0.629 0.058
2024-12-01-17:25:45-root-INFO: grad norm: 0.610 0.607 0.066
2024-12-01-17:25:46-root-INFO: Loss Change: 8.122 -> 7.229
2024-12-01-17:25:46-root-INFO: Regularization Change: 0.000 -> 1.797
2024-12-01-17:25:46-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-01-17:25:46-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-17:25:46-root-INFO: step: 17 lr_xt 0.30994086
2024-12-01-17:25:46-root-INFO: grad norm: 1.343 1.321 0.245
2024-12-01-17:25:47-root-INFO: grad norm: 0.881 0.874 0.110
2024-12-01-17:25:48-root-INFO: grad norm: 0.738 0.735 0.068
2024-12-01-17:25:49-root-INFO: grad norm: 0.673 0.669 0.073
2024-12-01-17:25:50-root-INFO: grad norm: 0.619 0.617 0.054
2024-12-01-17:25:51-root-INFO: grad norm: 0.586 0.583 0.063
2024-12-01-17:25:52-root-INFO: grad norm: 0.557 0.555 0.050
2024-12-01-17:25:53-root-INFO: grad norm: 0.537 0.534 0.057
2024-12-01-17:25:54-root-INFO: Loss Change: 7.243 -> 6.438
2024-12-01-17:25:54-root-INFO: Regularization Change: 0.000 -> 1.667
2024-12-01-17:25:54-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-01-17:25:54-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-17:25:54-root-INFO: step: 16 lr_xt 0.31455579
2024-12-01-17:25:55-root-INFO: grad norm: 1.298 1.276 0.240
2024-12-01-17:25:56-root-INFO: grad norm: 0.795 0.789 0.103
2024-12-01-17:25:57-root-INFO: grad norm: 0.661 0.658 0.058
2024-12-01-17:25:58-root-INFO: grad norm: 0.599 0.596 0.064
2024-12-01-17:25:59-root-INFO: grad norm: 0.552 0.550 0.047
2024-12-01-17:26:00-root-INFO: grad norm: 0.523 0.520 0.055
2024-12-01-17:26:01-root-INFO: grad norm: 0.498 0.496 0.044
2024-12-01-17:26:02-root-INFO: grad norm: 0.481 0.478 0.050
2024-12-01-17:26:02-root-INFO: Loss Change: 6.475 -> 5.728
2024-12-01-17:26:02-root-INFO: Regularization Change: 0.000 -> 1.545
2024-12-01-17:26:02-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-01-17:26:02-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-17:26:03-root-INFO: step: 15 lr_xt 0.31918850
2024-12-01-17:26:03-root-INFO: grad norm: 1.314 1.288 0.256
2024-12-01-17:26:04-root-INFO: grad norm: 0.747 0.741 0.093
2024-12-01-17:26:05-root-INFO: grad norm: 0.604 0.602 0.053
2024-12-01-17:26:06-root-INFO: grad norm: 0.543 0.540 0.053
2024-12-01-17:26:07-root-INFO: grad norm: 0.500 0.499 0.040
2024-12-01-17:26:08-root-INFO: grad norm: 0.474 0.472 0.045
2024-12-01-17:26:09-root-INFO: grad norm: 0.454 0.453 0.038
2024-12-01-17:26:10-root-INFO: grad norm: 0.440 0.438 0.042
2024-12-01-17:26:11-root-INFO: Loss Change: 5.780 -> 5.065
2024-12-01-17:26:11-root-INFO: Regularization Change: 0.000 -> 1.454
2024-12-01-17:26:11-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-01-17:26:11-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-17:26:11-root-INFO: step: 14 lr_xt 0.32383775
2024-12-01-17:26:11-root-INFO: grad norm: 1.224 1.202 0.234
2024-12-01-17:26:12-root-INFO: grad norm: 0.680 0.675 0.081
2024-12-01-17:26:13-root-INFO: grad norm: 0.549 0.547 0.046
2024-12-01-17:26:14-root-INFO: grad norm: 0.496 0.494 0.046
2024-12-01-17:26:15-root-INFO: grad norm: 0.461 0.460 0.036
2024-12-01-17:26:16-root-INFO: grad norm: 0.441 0.439 0.040
2024-12-01-17:26:17-root-INFO: grad norm: 0.425 0.424 0.034
2024-12-01-17:26:18-root-INFO: grad norm: 0.414 0.412 0.037
2024-12-01-17:26:19-root-INFO: Loss Change: 5.126 -> 4.479
2024-12-01-17:26:19-root-INFO: Regularization Change: 0.000 -> 1.337
2024-12-01-17:26:19-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-01-17:26:19-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-17:26:20-root-INFO: step: 13 lr_xt 0.32850231
2024-12-01-17:26:20-root-INFO: grad norm: 1.239 1.217 0.236
2024-12-01-17:26:21-root-INFO: grad norm: 0.693 0.688 0.083
2024-12-01-17:26:22-root-INFO: grad norm: 0.570 0.568 0.047
2024-12-01-17:26:23-root-INFO: grad norm: 0.529 0.527 0.048
2024-12-01-17:26:24-root-INFO: grad norm: 0.524 0.522 0.037
2024-12-01-17:26:25-root-INFO: grad norm: 0.521 0.519 0.046
2024-12-01-17:26:26-root-INFO: grad norm: 0.560 0.559 0.037
2024-12-01-17:26:27-root-INFO: grad norm: 0.550 0.548 0.048
2024-12-01-17:26:28-root-INFO: Loss Change: 4.574 -> 3.970
2024-12-01-17:26:28-root-INFO: Regularization Change: 0.000 -> 1.244
2024-12-01-17:26:28-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-01-17:26:28-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-17:26:28-root-INFO: step: 12 lr_xt 0.33318090
2024-12-01-17:26:28-root-INFO: grad norm: 1.322 1.302 0.226
2024-12-01-17:26:29-root-INFO: grad norm: 0.743 0.738 0.083
2024-12-01-17:26:30-root-INFO: grad norm: 0.509 0.508 0.037
2024-12-01-17:26:31-root-INFO: grad norm: 0.448 0.447 0.035
2024-12-01-17:26:32-root-INFO: grad norm: 0.450 0.449 0.032
2024-12-01-17:26:34-root-INFO: grad norm: 0.604 0.603 0.035
2024-12-01-17:26:35-root-INFO: grad norm: 0.597 0.595 0.046
2024-12-01-17:26:36-root-INFO: grad norm: 0.578 0.577 0.035
2024-12-01-17:26:36-root-INFO: Loss Change: 4.079 -> 3.542
2024-12-01-17:26:36-root-INFO: Regularization Change: 0.000 -> 1.159
2024-12-01-17:26:36-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-01-17:26:36-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-17:26:37-root-INFO: step: 11 lr_xt 0.33787222
2024-12-01-17:26:37-root-INFO: grad norm: 1.204 1.186 0.208
2024-12-01-17:26:38-root-INFO: grad norm: 0.774 0.772 0.057
2024-12-01-17:26:39-root-INFO: grad norm: 0.617 0.615 0.043
2024-12-01-17:26:40-root-INFO: grad norm: 0.443 0.442 0.031
2024-12-01-17:26:41-root-INFO: grad norm: 0.540 0.539 0.038
2024-12-01-17:26:42-root-INFO: grad norm: 0.871 0.870 0.044
2024-12-01-17:26:43-root-INFO: grad norm: 0.549 0.548 0.045
2024-12-01-17:26:44-root-INFO: grad norm: 0.480 0.479 0.031
2024-12-01-17:26:45-root-INFO: Loss Change: 3.650 -> 3.137
2024-12-01-17:26:45-root-INFO: Regularization Change: 0.000 -> 1.053
2024-12-01-17:26:45-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-01-17:26:45-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-17:26:45-root-INFO: step: 10 lr_xt 0.34257494
2024-12-01-17:26:45-root-INFO: grad norm: 1.389 1.367 0.246
2024-12-01-17:26:46-root-INFO: grad norm: 0.699 0.695 0.073
2024-12-01-17:26:47-root-INFO: grad norm: 0.429 0.428 0.035
2024-12-01-17:26:48-root-INFO: grad norm: 0.397 0.396 0.028
2024-12-01-17:26:49-root-INFO: grad norm: 0.505 0.503 0.034
2024-12-01-17:26:50-root-INFO: grad norm: 0.776 0.775 0.041
2024-12-01-17:26:51-root-INFO: grad norm: 0.590 0.588 0.049
2024-12-01-17:26:52-root-INFO: grad norm: 0.354 0.353 0.026
2024-12-01-17:26:53-root-INFO: Loss Change: 3.318 -> 2.786
2024-12-01-17:26:53-root-INFO: Regularization Change: 0.000 -> 0.987
2024-12-01-17:26:53-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-01-17:26:53-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-17:26:53-root-INFO: step: 9 lr_xt 0.34728771
2024-12-01-17:26:54-root-INFO: grad norm: 1.031 1.013 0.193
2024-12-01-17:26:55-root-INFO: grad norm: 0.445 0.443 0.045
2024-12-01-17:26:56-root-INFO: grad norm: 0.347 0.345 0.030
2024-12-01-17:26:57-root-INFO: grad norm: 0.318 0.317 0.025
2024-12-01-17:26:58-root-INFO: grad norm: 0.304 0.303 0.024
2024-12-01-17:26:59-root-INFO: grad norm: 0.299 0.298 0.024
2024-12-01-17:27:00-root-INFO: grad norm: 0.324 0.323 0.024
2024-12-01-17:27:01-root-INFO: grad norm: 0.485 0.484 0.035
2024-12-01-17:27:01-root-INFO: Loss too large (2.530->2.536)! Learning rate decreased to 0.27783.
2024-12-01-17:27:02-root-INFO: Loss Change: 2.922 -> 2.523
2024-12-01-17:27:02-root-INFO: Regularization Change: 0.000 -> 0.841
2024-12-01-17:27:02-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-01-17:27:02-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-17:27:02-root-INFO: step: 8 lr_xt 0.35200918
2024-12-01-17:27:02-root-INFO: grad norm: 1.247 1.228 0.217
2024-12-01-17:27:03-root-INFO: grad norm: 0.752 0.749 0.069
2024-12-01-17:27:04-root-INFO: grad norm: 0.599 0.597 0.047
2024-12-01-17:27:05-root-INFO: grad norm: 0.685 0.683 0.053
2024-12-01-17:27:06-root-INFO: grad norm: 0.716 0.715 0.045
2024-12-01-17:27:07-root-INFO: grad norm: 0.678 0.676 0.056
2024-12-01-17:27:08-root-INFO: grad norm: 0.636 0.635 0.039
2024-12-01-17:27:09-root-INFO: grad norm: 0.867 0.865 0.063
2024-12-01-17:27:10-root-INFO: Loss Change: 2.712 -> 2.242
2024-12-01-17:27:10-root-INFO: Regularization Change: 0.000 -> 0.949
2024-12-01-17:27:10-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-01-17:27:10-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-17:27:10-root-INFO: step: 7 lr_xt 0.35673794
2024-12-01-17:27:11-root-INFO: grad norm: 1.325 1.304 0.236
2024-12-01-17:27:12-root-INFO: grad norm: 0.684 0.681 0.060
2024-12-01-17:27:13-root-INFO: grad norm: 0.558 0.556 0.045
2024-12-01-17:27:14-root-INFO: grad norm: 0.464 0.463 0.036
2024-12-01-17:27:15-root-INFO: grad norm: 0.437 0.435 0.034
2024-12-01-17:27:16-root-INFO: grad norm: 0.410 0.408 0.033
2024-12-01-17:27:17-root-INFO: grad norm: 0.396 0.395 0.031
2024-12-01-17:27:18-root-INFO: grad norm: 0.381 0.379 0.031
2024-12-01-17:27:18-root-INFO: Loss Change: 2.459 -> 1.995
2024-12-01-17:27:18-root-INFO: Regularization Change: 0.000 -> 0.796
2024-12-01-17:27:18-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-01-17:27:18-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-17:27:19-root-INFO: step: 6 lr_xt 0.36147257
2024-12-01-17:27:19-root-INFO: grad norm: 1.027 1.010 0.186
2024-12-01-17:27:20-root-INFO: grad norm: 0.478 0.476 0.041
2024-12-01-17:27:21-root-INFO: grad norm: 0.384 0.383 0.036
2024-12-01-17:27:22-root-INFO: grad norm: 0.320 0.319 0.029
2024-12-01-17:27:23-root-INFO: grad norm: 0.291 0.290 0.029
2024-12-01-17:27:24-root-INFO: grad norm: 0.267 0.265 0.026
2024-12-01-17:27:25-root-INFO: grad norm: 0.251 0.250 0.026
2024-12-01-17:27:26-root-INFO: grad norm: 0.237 0.236 0.025
2024-12-01-17:27:27-root-INFO: Loss Change: 2.180 -> 1.809
2024-12-01-17:27:27-root-INFO: Regularization Change: 0.000 -> 0.705
2024-12-01-17:27:27-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-01-17:27:27-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-17:27:27-root-INFO: step: 5 lr_xt 0.36621164
2024-12-01-17:27:27-root-INFO: grad norm: 0.935 0.920 0.166
2024-12-01-17:27:28-root-INFO: grad norm: 0.373 0.371 0.036
2024-12-01-17:27:30-root-INFO: grad norm: 0.290 0.288 0.033
2024-12-01-17:27:31-root-INFO: grad norm: 0.255 0.253 0.029
2024-12-01-17:27:32-root-INFO: grad norm: 0.236 0.234 0.028
2024-12-01-17:27:33-root-INFO: grad norm: 0.223 0.222 0.027
2024-12-01-17:27:34-root-INFO: grad norm: 0.214 0.212 0.026
2024-12-01-17:27:35-root-INFO: grad norm: 0.207 0.205 0.025
2024-12-01-17:27:35-root-INFO: Loss Change: 1.994 -> 1.658
2024-12-01-17:27:35-root-INFO: Regularization Change: 0.000 -> 0.656
2024-12-01-17:27:35-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-01-17:27:35-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-17:27:36-root-INFO: step: 4 lr_xt 0.37095370
2024-12-01-17:27:36-root-INFO: grad norm: 0.835 0.824 0.135
2024-12-01-17:27:37-root-INFO: grad norm: 0.345 0.343 0.036
2024-12-01-17:27:38-root-INFO: grad norm: 0.269 0.267 0.031
2024-12-01-17:27:39-root-INFO: grad norm: 0.245 0.244 0.029
2024-12-01-17:27:40-root-INFO: grad norm: 0.230 0.228 0.029
2024-12-01-17:27:41-root-INFO: grad norm: 0.217 0.216 0.028
2024-12-01-17:27:42-root-INFO: grad norm: 0.209 0.207 0.028
2024-12-01-17:27:43-root-INFO: grad norm: 0.202 0.200 0.027
2024-12-01-17:27:44-root-INFO: Loss Change: 1.823 -> 1.524
2024-12-01-17:27:44-root-INFO: Regularization Change: 0.000 -> 0.613
2024-12-01-17:27:44-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-01-17:27:44-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-17:27:44-root-INFO: step: 3 lr_xt 0.37569726
2024-12-01-17:27:44-root-INFO: grad norm: 0.805 0.796 0.123
2024-12-01-17:27:45-root-INFO: grad norm: 0.345 0.342 0.038
2024-12-01-17:27:46-root-INFO: grad norm: 0.264 0.262 0.032
2024-12-01-17:27:48-root-INFO: grad norm: 0.236 0.234 0.032
2024-12-01-17:27:49-root-INFO: grad norm: 0.217 0.215 0.030
2024-12-01-17:27:50-root-INFO: grad norm: 0.206 0.204 0.030
2024-12-01-17:27:51-root-INFO: grad norm: 0.201 0.199 0.028
2024-12-01-17:27:52-root-INFO: grad norm: 0.191 0.189 0.028
2024-12-01-17:27:52-root-INFO: Loss Change: 1.693 -> 1.410
2024-12-01-17:27:52-root-INFO: Regularization Change: 0.000 -> 0.585
2024-12-01-17:27:52-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-01-17:27:52-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-17:27:53-root-INFO: step: 2 lr_xt 0.38044082
2024-12-01-17:27:53-root-INFO: grad norm: 0.750 0.743 0.105
2024-12-01-17:27:54-root-INFO: grad norm: 0.350 0.348 0.034
2024-12-01-17:27:55-root-INFO: grad norm: 0.248 0.246 0.032
2024-12-01-17:27:56-root-INFO: grad norm: 0.233 0.231 0.032
2024-12-01-17:27:57-root-INFO: grad norm: 0.201 0.199 0.031
2024-12-01-17:27:58-root-INFO: grad norm: 0.199 0.197 0.030
2024-12-01-17:27:59-root-INFO: grad norm: 0.179 0.176 0.029
2024-12-01-17:28:00-root-INFO: grad norm: 0.180 0.178 0.028
2024-12-01-17:28:01-root-INFO: Loss Change: 1.567 -> 1.316
2024-12-01-17:28:01-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-01-17:28:01-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-01-17:28:01-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-17:28:01-root-INFO: step: 1 lr_xt 0.38518288
2024-12-01-17:28:01-root-INFO: grad norm: 0.725 0.720 0.093
2024-12-01-17:28:02-root-INFO: grad norm: 0.326 0.325 0.030
2024-12-01-17:28:04-root-INFO: grad norm: 0.256 0.255 0.028
2024-12-01-17:28:05-root-INFO: grad norm: 0.235 0.234 0.027
2024-12-01-17:28:06-root-INFO: grad norm: 0.208 0.206 0.027
2024-12-01-17:28:07-root-INFO: grad norm: 0.200 0.198 0.026
2024-12-01-17:28:07-root-INFO: grad norm: 0.183 0.181 0.025
2024-12-01-17:28:09-root-INFO: grad norm: 0.174 0.172 0.024
2024-12-01-17:28:09-root-INFO: Loss Change: 1.459 -> 1.203
2024-12-01-17:28:09-root-INFO: Regularization Change: 0.000 -> 0.564
2024-12-01-17:28:09-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-01-17:28:09-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-17:28:09-root-INFO: loss_sample0_0: 1.2032586336135864
2024-12-01-17:28:10-root-INFO: It takes 1673.970 seconds for image sample0
2024-12-01-17:28:10-root-INFO: lpips_score_sample0: 0.148
2024-12-01-17:28:10-root-INFO: psnr_score_sample0: 17.712
2024-12-01-17:28:10-root-INFO: ssim_score_sample0: 0.721
2024-12-01-17:28:10-root-INFO: mean_lpips: 0.14795616269111633
2024-12-01-17:28:10-root-INFO: best_mean_lpips: 0.14795616269111633
2024-12-01-17:28:10-root-INFO: mean_psnr: 17.712200164794922
2024-12-01-17:28:10-root-INFO: best_mean_psnr: 17.712200164794922
2024-12-01-17:28:10-root-INFO: mean_ssim: 0.7210004329681396
2024-12-01-17:28:10-root-INFO: best_mean_ssim: 0.7210004329681396
2024-12-01-17:28:10-root-INFO: final_loss: 1.2032586336135864
2024-12-01-17:28:10-root-INFO: mean time: 1673.97048163414
2024-12-01-17:28:10-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample1_iter8_lr0.02_10009 
 
Enjoy.
