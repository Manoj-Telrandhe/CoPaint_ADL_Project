2024-12-02-09:44:40-root-INFO: Prepare model...
2024-12-02-09:44:55-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-09:45:19-root-INFO: Start sampling
2024-12-02-09:45:23-root-INFO: step: 249 lr_xt 0.00012706
2024-12-02-09:45:23-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-09:45:24-root-INFO: grad norm: 23713.480 17716.025 15762.983
2024-12-02-09:45:24-root-INFO: grad norm: 26794.184 21195.986 16390.805
2024-12-02-09:45:24-root-INFO: Loss too large (43813.262->60944.414)! Learning rate decreased to 0.00010.
2024-12-02-09:45:24-root-INFO: Loss too large (43813.262->45478.254)! Learning rate decreased to 0.00008.
2024-12-02-09:45:25-root-INFO: grad norm: 21163.521 16199.203 13619.121
2024-12-02-09:45:25-root-INFO: grad norm: 19144.854 15680.129 10984.489
2024-12-02-09:45:26-root-INFO: grad norm: 17825.119 14001.935 11030.896
2024-12-02-09:45:26-root-INFO: grad norm: 16611.805 13521.206 9650.339
2024-12-02-09:45:27-root-INFO: grad norm: 15351.426 12275.585 9218.259
2024-12-02-09:45:27-root-INFO: Loss Change: 77070.016 -> 21843.703
2024-12-02-09:45:27-root-INFO: Regularization Change: 0.000 -> 14.877
2024-12-02-09:45:27-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-02-09:45:27-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:45:27-root-INFO: step: 248 lr_xt 0.00013388
2024-12-02-09:45:27-root-INFO: grad norm: 13709.596 11022.990 8151.483
2024-12-02-09:45:28-root-INFO: Loss too large (22204.469->32160.592)! Learning rate decreased to 0.00011.
2024-12-02-09:45:28-root-INFO: Loss too large (22204.469->25111.049)! Learning rate decreased to 0.00009.
2024-12-02-09:45:28-root-INFO: grad norm: 12007.765 9574.842 7246.297
2024-12-02-09:45:29-root-INFO: grad norm: 10370.055 8293.751 6225.088
2024-12-02-09:45:29-root-INFO: grad norm: 8986.892 7203.540 5373.382
2024-12-02-09:45:30-root-INFO: grad norm: 7668.375 6109.974 4633.809
2024-12-02-09:45:30-root-INFO: grad norm: 6572.405 5283.135 3909.603
2024-12-02-09:45:31-root-INFO: grad norm: 5545.996 4410.095 3362.906
2024-12-02-09:45:31-root-INFO: grad norm: 4705.585 3788.167 2791.473
2024-12-02-09:45:31-root-INFO: Loss Change: 22204.469 -> 16910.189
2024-12-02-09:45:31-root-INFO: Regularization Change: 0.000 -> 1.223
2024-12-02-09:45:32-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-02-09:45:32-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:45:32-root-INFO: step: 247 lr_xt 0.00014104
2024-12-02-09:45:32-root-INFO: grad norm: 3929.569 3124.744 2382.748
2024-12-02-09:45:32-root-INFO: Loss too large (16757.734->17453.332)! Learning rate decreased to 0.00011.
2024-12-02-09:45:32-root-INFO: Loss too large (16757.734->16866.510)! Learning rate decreased to 0.00009.
2024-12-02-09:45:33-root-INFO: grad norm: 3144.036 2561.647 1822.889
2024-12-02-09:45:33-root-INFO: grad norm: 2509.932 2011.766 1500.852
2024-12-02-09:45:34-root-INFO: grad norm: 2036.562 1659.907 1179.954
2024-12-02-09:45:34-root-INFO: grad norm: 1668.512 1356.195 971.940
2024-12-02-09:45:34-root-INFO: grad norm: 1397.281 1141.822 805.380
2024-12-02-09:45:35-root-INFO: grad norm: 1193.238 991.068 664.530
2024-12-02-09:45:35-root-INFO: grad norm: 1046.636 862.541 592.848
2024-12-02-09:45:36-root-INFO: Loss Change: 16757.734 -> 15907.407
2024-12-02-09:45:36-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-09:45:36-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-02-09:45:36-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:45:36-root-INFO: step: 246 lr_xt 0.00014856
2024-12-02-09:45:36-root-INFO: grad norm: 1025.770 863.564 553.589
2024-12-02-09:45:37-root-INFO: grad norm: 1243.419 1016.734 715.781
2024-12-02-09:45:37-root-INFO: grad norm: 1797.844 1493.340 1001.089
2024-12-02-09:45:37-root-INFO: Loss too large (15567.934->15621.146)! Learning rate decreased to 0.00012.
2024-12-02-09:45:38-root-INFO: grad norm: 2024.769 1622.672 1211.043
2024-12-02-09:45:38-root-INFO: grad norm: 2314.522 1915.985 1298.466
2024-12-02-09:45:39-root-INFO: grad norm: 2648.397 2115.309 1593.573
2024-12-02-09:45:39-root-INFO: grad norm: 3057.427 2515.320 1738.110
2024-12-02-09:45:39-root-INFO: Loss too large (15468.532->15496.941)! Learning rate decreased to 0.00010.
2024-12-02-09:45:40-root-INFO: grad norm: 2305.857 1839.498 1390.405
2024-12-02-09:45:40-root-INFO: Loss Change: 15687.740 -> 15196.314
2024-12-02-09:45:40-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-02-09:45:40-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-02-09:45:40-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:45:40-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-09:45:40-root-INFO: grad norm: 1696.721 1421.483 926.416
2024-12-02-09:45:40-root-INFO: Loss too large (15078.061->15098.454)! Learning rate decreased to 0.00013.
2024-12-02-09:45:41-root-INFO: grad norm: 1815.897 1445.291 1099.372
2024-12-02-09:45:41-root-INFO: grad norm: 1979.964 1662.700 1075.028
2024-12-02-09:45:42-root-INFO: grad norm: 2167.204 1724.647 1312.390
2024-12-02-09:45:42-root-INFO: grad norm: 2388.200 1994.048 1314.257
2024-12-02-09:45:43-root-INFO: grad norm: 2631.376 2096.740 1589.912
2024-12-02-09:45:43-root-INFO: grad norm: 2916.677 2419.691 1628.526
2024-12-02-09:45:44-root-INFO: grad norm: 3214.998 2567.236 1935.333
2024-12-02-09:45:44-root-INFO: Loss Change: 15078.061 -> 14828.740
2024-12-02-09:45:44-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-02-09:45:44-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-09:45:44-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-09:45:44-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-09:45:44-root-INFO: grad norm: 3663.037 2984.777 2123.428
2024-12-02-09:45:44-root-INFO: Loss too large (14750.084->15195.641)! Learning rate decreased to 0.00013.
2024-12-02-09:45:45-root-INFO: grad norm: 3764.711 3041.511 2218.617
2024-12-02-09:45:45-root-INFO: grad norm: 3954.280 3261.463 2235.888
2024-12-02-09:45:45-root-INFO: Loss too large (14634.685->14640.430)! Learning rate decreased to 0.00011.
2024-12-02-09:45:46-root-INFO: grad norm: 2666.210 2152.531 1573.305
2024-12-02-09:45:46-root-INFO: grad norm: 1816.178 1545.081 954.583
2024-12-02-09:45:47-root-INFO: grad norm: 1325.152 1081.270 766.083
2024-12-02-09:45:47-root-INFO: grad norm: 1033.133 918.896 472.223
2024-12-02-09:45:48-root-INFO: grad norm: 877.944 746.669 461.811
2024-12-02-09:45:48-root-INFO: Loss Change: 14750.084 -> 13815.984
2024-12-02-09:45:48-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-02-09:45:48-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-09:45:48-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:45:48-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-09:45:48-root-INFO: grad norm: 1012.139 835.408 571.419
2024-12-02-09:45:49-root-INFO: grad norm: 976.624 786.558 578.896
2024-12-02-09:45:49-root-INFO: grad norm: 1060.134 889.921 576.129
2024-12-02-09:45:50-root-INFO: grad norm: 1233.667 959.369 775.593
2024-12-02-09:45:50-root-INFO: grad norm: 1537.487 1257.901 884.055
2024-12-02-09:45:50-root-INFO: grad norm: 2012.120 1555.648 1276.160
2024-12-02-09:45:51-root-INFO: Loss too large (13320.694->13354.178)! Learning rate decreased to 0.00014.
2024-12-02-09:45:51-root-INFO: grad norm: 1905.235 1574.281 1073.107
2024-12-02-09:45:52-root-INFO: grad norm: 1859.318 1478.719 1127.143
2024-12-02-09:45:52-root-INFO: Loss Change: 13697.792 -> 13074.649
2024-12-02-09:45:52-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-02-09:45:52-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-09:45:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:45:52-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-09:45:52-root-INFO: grad norm: 1944.257 1604.354 1098.263
2024-12-02-09:45:52-root-INFO: Loss too large (12881.179->12896.511)! Learning rate decreased to 0.00015.
2024-12-02-09:45:53-root-INFO: grad norm: 1837.140 1491.346 1072.831
2024-12-02-09:45:53-root-INFO: grad norm: 1782.225 1513.534 941.031
2024-12-02-09:45:54-root-INFO: grad norm: 1754.982 1432.471 1013.898
2024-12-02-09:45:54-root-INFO: grad norm: 1743.100 1491.849 901.544
2024-12-02-09:45:54-root-INFO: grad norm: 1742.401 1428.267 998.005
2024-12-02-09:45:55-root-INFO: grad norm: 1744.116 1494.197 899.620
2024-12-02-09:45:55-root-INFO: grad norm: 1746.968 1436.631 993.976
2024-12-02-09:45:56-root-INFO: Loss Change: 12881.179 -> 12240.860
2024-12-02-09:45:56-root-INFO: Regularization Change: 0.000 -> 0.714
2024-12-02-09:45:56-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-09:45:56-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:45:56-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-09:45:56-root-INFO: grad norm: 1780.031 1515.505 933.679
2024-12-02-09:45:56-root-INFO: Loss too large (12147.039->12162.598)! Learning rate decreased to 0.00015.
2024-12-02-09:45:57-root-INFO: grad norm: 1715.562 1433.924 941.815
2024-12-02-09:45:57-root-INFO: grad norm: 1662.251 1429.008 849.126
2024-12-02-09:45:57-root-INFO: grad norm: 1612.404 1346.247 887.392
2024-12-02-09:45:58-root-INFO: grad norm: 1563.085 1348.985 789.603
2024-12-02-09:45:58-root-INFO: grad norm: 1517.057 1268.602 831.933
2024-12-02-09:45:59-root-INFO: grad norm: 1471.193 1273.607 736.434
2024-12-02-09:45:59-root-INFO: grad norm: 1424.856 1194.717 776.445
2024-12-02-09:46:00-root-INFO: Loss Change: 12147.039 -> 11490.966
2024-12-02-09:46:00-root-INFO: Regularization Change: 0.000 -> 0.749
2024-12-02-09:46:00-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-09:46:00-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:46:00-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-09:46:00-root-INFO: grad norm: 1330.291 1145.425 676.518
2024-12-02-09:46:00-root-INFO: grad norm: 1672.421 1412.254 895.841
2024-12-02-09:46:01-root-INFO: grad norm: 2238.449 1920.376 1150.135
2024-12-02-09:46:01-root-INFO: Loss too large (11228.848->11320.042)! Learning rate decreased to 0.00016.
2024-12-02-09:46:01-root-INFO: grad norm: 2075.848 1744.102 1125.724
2024-12-02-09:46:02-root-INFO: grad norm: 1925.082 1658.477 977.443
2024-12-02-09:46:02-root-INFO: grad norm: 1788.735 1506.573 964.267
2024-12-02-09:46:03-root-INFO: grad norm: 1661.649 1438.330 832.037
2024-12-02-09:46:03-root-INFO: grad norm: 1547.370 1307.589 827.383
2024-12-02-09:46:04-root-INFO: Loss Change: 11297.842 -> 10670.627
2024-12-02-09:46:04-root-INFO: Regularization Change: 0.000 -> 0.879
2024-12-02-09:46:04-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-09:46:04-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:46:04-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-09:46:04-root-INFO: grad norm: 1346.742 1171.733 663.893
2024-12-02-09:46:04-root-INFO: grad norm: 1700.768 1439.629 905.581
2024-12-02-09:46:05-root-INFO: grad norm: 2209.544 1896.321 1134.043
2024-12-02-09:46:05-root-INFO: Loss too large (10527.298->10603.582)! Learning rate decreased to 0.00017.
2024-12-02-09:46:05-root-INFO: grad norm: 1958.828 1657.489 1043.904
2024-12-02-09:46:06-root-INFO: grad norm: 1738.121 1503.823 871.538
2024-12-02-09:46:06-root-INFO: grad norm: 1546.387 1313.084 816.776
2024-12-02-09:46:07-root-INFO: grad norm: 1377.943 1203.980 670.193
2024-12-02-09:46:07-root-INFO: grad norm: 1233.458 1053.488 641.547
2024-12-02-09:46:08-root-INFO: Loss Change: 10571.543 -> 9962.411
2024-12-02-09:46:08-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-09:46:08-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-09:46:08-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:46:08-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-09:46:08-root-INFO: grad norm: 1006.981 859.854 524.081
2024-12-02-09:46:08-root-INFO: grad norm: 1053.801 906.189 537.884
2024-12-02-09:46:09-root-INFO: grad norm: 1236.739 1086.350 591.073
2024-12-02-09:46:09-root-INFO: grad norm: 1500.517 1277.815 786.599
2024-12-02-09:46:10-root-INFO: grad norm: 1860.749 1619.078 917.047
2024-12-02-09:46:10-root-INFO: Loss too large (9543.368->9573.316)! Learning rate decreased to 0.00018.
2024-12-02-09:46:10-root-INFO: grad norm: 1560.197 1333.910 809.258
2024-12-02-09:46:11-root-INFO: grad norm: 1315.455 1161.471 617.580
2024-12-02-09:46:11-root-INFO: grad norm: 1122.517 968.226 567.965
2024-12-02-09:46:12-root-INFO: Loss Change: 9738.947 -> 9208.993
2024-12-02-09:46:12-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-02-09:46:12-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-09:46:12-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:46:12-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-09:46:12-root-INFO: grad norm: 747.285 678.647 312.847
2024-12-02-09:46:13-root-INFO: grad norm: 799.742 701.555 383.936
2024-12-02-09:46:13-root-INFO: grad norm: 897.406 813.903 378.020
2024-12-02-09:46:13-root-INFO: grad norm: 1031.433 894.821 512.979
2024-12-02-09:46:14-root-INFO: grad norm: 1207.334 1076.591 546.450
2024-12-02-09:46:14-root-INFO: grad norm: 1423.674 1227.821 720.627
2024-12-02-09:46:15-root-INFO: grad norm: 1692.713 1490.908 801.542
2024-12-02-09:46:15-root-INFO: Loss too large (8880.661->8892.463)! Learning rate decreased to 0.00019.
2024-12-02-09:46:15-root-INFO: grad norm: 1337.096 1154.225 674.976
2024-12-02-09:46:16-root-INFO: Loss Change: 9122.056 -> 8703.220
2024-12-02-09:46:16-root-INFO: Regularization Change: 0.000 -> 0.812
2024-12-02-09:46:16-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-09:46:16-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:46:16-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-09:46:16-root-INFO: grad norm: 988.735 875.933 458.627
2024-12-02-09:46:16-root-INFO: grad norm: 1109.056 967.372 542.399
2024-12-02-09:46:17-root-INFO: grad norm: 1259.806 1114.958 586.497
2024-12-02-09:46:17-root-INFO: grad norm: 1436.581 1243.397 719.535
2024-12-02-09:46:18-root-INFO: grad norm: 1642.463 1444.539 781.660
2024-12-02-09:46:18-root-INFO: grad norm: 1870.474 1615.008 943.622
2024-12-02-09:46:19-root-INFO: Loss too large (8444.510->8453.560)! Learning rate decreased to 0.00020.
2024-12-02-09:46:19-root-INFO: grad norm: 1375.140 1215.609 642.888
2024-12-02-09:46:19-root-INFO: grad norm: 1036.492 900.609 513.048
2024-12-02-09:46:20-root-INFO: Loss Change: 8562.271 -> 8164.380
2024-12-02-09:46:20-root-INFO: Regularization Change: 0.000 -> 0.702
2024-12-02-09:46:20-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-09:46:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:46:20-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-09:46:20-root-INFO: grad norm: 525.548 485.630 200.908
2024-12-02-09:46:21-root-INFO: grad norm: 522.167 470.906 225.623
2024-12-02-09:46:21-root-INFO: grad norm: 532.248 497.667 188.721
2024-12-02-09:46:21-root-INFO: grad norm: 548.608 492.502 241.686
2024-12-02-09:46:22-root-INFO: grad norm: 569.065 529.264 209.081
2024-12-02-09:46:22-root-INFO: grad norm: 593.779 529.290 269.121
2024-12-02-09:46:23-root-INFO: grad norm: 622.735 574.321 240.737
2024-12-02-09:46:23-root-INFO: grad norm: 656.126 580.682 305.466
2024-12-02-09:46:24-root-INFO: Loss Change: 8058.557 -> 7726.965
2024-12-02-09:46:24-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-02-09:46:24-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-09:46:24-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-09:46:24-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-09:46:24-root-INFO: grad norm: 610.810 556.246 252.348
2024-12-02-09:46:24-root-INFO: grad norm: 611.891 544.590 278.984
2024-12-02-09:46:25-root-INFO: grad norm: 625.115 572.877 250.161
2024-12-02-09:46:25-root-INFO: grad norm: 639.846 567.527 295.491
2024-12-02-09:46:26-root-INFO: grad norm: 655.892 599.524 266.018
2024-12-02-09:46:26-root-INFO: grad norm: 672.509 594.513 314.359
2024-12-02-09:46:27-root-INFO: grad norm: 689.262 627.863 284.377
2024-12-02-09:46:27-root-INFO: grad norm: 705.254 621.983 332.446
2024-12-02-09:46:28-root-INFO: Loss Change: 7661.651 -> 7368.762
2024-12-02-09:46:28-root-INFO: Regularization Change: 0.000 -> 0.668
2024-12-02-09:46:28-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-09:46:28-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:46:28-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-09:46:28-root-INFO: grad norm: 594.531 518.329 291.207
2024-12-02-09:46:28-root-INFO: grad norm: 539.454 476.806 252.323
2024-12-02-09:46:29-root-INFO: grad norm: 518.704 474.525 209.475
2024-12-02-09:46:29-root-INFO: grad norm: 506.787 453.908 225.389
2024-12-02-09:46:30-root-INFO: grad norm: 498.445 459.994 191.973
2024-12-02-09:46:30-root-INFO: grad norm: 491.313 441.351 215.864
2024-12-02-09:46:31-root-INFO: grad norm: 485.446 449.081 184.349
2024-12-02-09:46:31-root-INFO: grad norm: 480.125 431.684 210.165
2024-12-02-09:46:32-root-INFO: Loss Change: 7325.880 -> 7040.255
2024-12-02-09:46:32-root-INFO: Regularization Change: 0.000 -> 0.636
2024-12-02-09:46:32-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-09:46:32-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:46:32-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-09:46:32-root-INFO: grad norm: 477.642 404.811 253.515
2024-12-02-09:46:32-root-INFO: grad norm: 375.003 344.618 147.871
2024-12-02-09:46:33-root-INFO: grad norm: 348.154 317.407 143.052
2024-12-02-09:46:33-root-INFO: grad norm: 335.930 317.067 110.984
2024-12-02-09:46:34-root-INFO: grad norm: 328.499 304.279 123.800
2024-12-02-09:46:34-root-INFO: grad norm: 322.922 305.999 103.166
2024-12-02-09:46:35-root-INFO: grad norm: 318.216 296.195 116.320
2024-12-02-09:46:35-root-INFO: grad norm: 314.124 298.038 99.234
2024-12-02-09:46:36-root-INFO: Loss Change: 6914.843 -> 6666.138
2024-12-02-09:46:36-root-INFO: Regularization Change: 0.000 -> 0.574
2024-12-02-09:46:36-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-09:46:36-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:46:36-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-09:46:36-root-INFO: grad norm: 403.966 365.358 172.344
2024-12-02-09:46:36-root-INFO: grad norm: 385.794 360.787 136.636
2024-12-02-09:46:37-root-INFO: grad norm: 373.115 340.471 152.625
2024-12-02-09:46:37-root-INFO: grad norm: 361.999 339.798 124.821
2024-12-02-09:46:38-root-INFO: grad norm: 352.152 322.045 142.471
2024-12-02-09:46:38-root-INFO: grad norm: 342.909 322.551 116.392
2024-12-02-09:46:39-root-INFO: grad norm: 334.247 306.710 132.853
2024-12-02-09:46:39-root-INFO: grad norm: 325.911 307.112 109.090
2024-12-02-09:46:39-root-INFO: Loss Change: 6614.954 -> 6413.430
2024-12-02-09:46:39-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-02-09:46:39-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-09:46:39-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:46:40-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-09:46:40-root-INFO: grad norm: 515.756 457.072 238.933
2024-12-02-09:46:40-root-INFO: grad norm: 478.555 442.349 182.598
2024-12-02-09:46:41-root-INFO: grad norm: 448.542 402.867 197.199
2024-12-02-09:46:41-root-INFO: grad norm: 422.300 390.962 159.642
2024-12-02-09:46:41-root-INFO: grad norm: 399.036 361.025 169.972
2024-12-02-09:46:42-root-INFO: grad norm: 378.123 351.019 140.580
2024-12-02-09:46:42-root-INFO: grad norm: 360.105 327.666 149.367
2024-12-02-09:46:43-root-INFO: grad norm: 344.058 320.418 125.332
2024-12-02-09:46:43-root-INFO: Loss Change: 6348.284 -> 6156.032
2024-12-02-09:46:43-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-09:46:43-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-09:46:43-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:46:43-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-09:46:43-root-INFO: grad norm: 612.015 551.895 264.527
2024-12-02-09:46:44-root-INFO: grad norm: 548.872 504.055 217.228
2024-12-02-09:46:44-root-INFO: grad norm: 506.246 456.818 218.180
2024-12-02-09:46:45-root-INFO: grad norm: 469.716 431.699 185.120
2024-12-02-09:46:45-root-INFO: grad norm: 438.535 397.861 184.443
2024-12-02-09:46:46-root-INFO: grad norm: 411.113 378.599 160.239
2024-12-02-09:46:46-root-INFO: grad norm: 387.927 353.520 159.720
2024-12-02-09:46:47-root-INFO: grad norm: 367.280 339.159 140.946
2024-12-02-09:46:47-root-INFO: Loss Change: 6128.793 -> 5928.247
2024-12-02-09:46:47-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-02-09:46:47-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-09:46:47-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:46:47-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-09:46:47-root-INFO: grad norm: 517.279 457.168 242.023
2024-12-02-09:46:48-root-INFO: grad norm: 446.560 408.398 180.630
2024-12-02-09:46:48-root-INFO: grad norm: 406.956 370.834 167.616
2024-12-02-09:46:49-root-INFO: grad norm: 376.004 347.022 144.756
2024-12-02-09:46:49-root-INFO: grad norm: 350.294 320.742 140.820
2024-12-02-09:46:50-root-INFO: grad norm: 328.323 304.263 123.372
2024-12-02-09:46:50-root-INFO: grad norm: 309.793 284.901 121.668
2024-12-02-09:46:51-root-INFO: grad norm: 294.119 273.599 107.935
2024-12-02-09:46:51-root-INFO: Loss Change: 5904.817 -> 5730.400
2024-12-02-09:46:51-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-02-09:46:51-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-09:46:51-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:46:51-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-09:46:51-root-INFO: grad norm: 659.724 595.784 283.333
2024-12-02-09:46:52-root-INFO: grad norm: 569.802 522.562 227.163
2024-12-02-09:46:52-root-INFO: grad norm: 510.204 465.770 208.247
2024-12-02-09:46:53-root-INFO: grad norm: 461.784 424.225 182.423
2024-12-02-09:46:53-root-INFO: grad norm: 422.184 387.178 168.323
2024-12-02-09:46:54-root-INFO: grad norm: 388.521 357.642 151.792
2024-12-02-09:46:54-root-INFO: grad norm: 360.750 332.194 140.667
2024-12-02-09:46:54-root-INFO: grad norm: 336.774 310.969 129.286
2024-12-02-09:46:55-root-INFO: Loss Change: 5763.703 -> 5555.655
2024-12-02-09:46:55-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-09:46:55-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-09:46:55-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-09:46:55-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-09:46:55-root-INFO: grad norm: 385.552 332.666 194.893
2024-12-02-09:46:56-root-INFO: grad norm: 299.244 274.186 119.870
2024-12-02-09:46:56-root-INFO: grad norm: 273.927 252.134 107.071
2024-12-02-09:46:57-root-INFO: grad norm: 257.935 239.897 94.763
2024-12-02-09:46:57-root-INFO: grad norm: 245.966 227.622 93.207
2024-12-02-09:46:57-root-INFO: grad norm: 236.571 220.892 84.691
2024-12-02-09:46:58-root-INFO: grad norm: 229.060 212.918 84.465
2024-12-02-09:46:58-root-INFO: grad norm: 223.031 208.768 78.475
2024-12-02-09:46:59-root-INFO: Loss Change: 5511.937 -> 5350.131
2024-12-02-09:46:59-root-INFO: Regularization Change: 0.000 -> 0.488
2024-12-02-09:46:59-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-09:46:59-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:46:59-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-09:46:59-root-INFO: grad norm: 553.559 503.227 230.630
2024-12-02-09:46:59-root-INFO: grad norm: 472.487 436.541 180.765
2024-12-02-09:47:00-root-INFO: grad norm: 422.471 387.296 168.770
2024-12-02-09:47:00-root-INFO: grad norm: 382.180 353.183 146.024
2024-12-02-09:47:01-root-INFO: grad norm: 349.833 322.247 136.160
2024-12-02-09:47:01-root-INFO: grad norm: 322.756 298.713 122.238
2024-12-02-09:47:02-root-INFO: grad norm: 300.498 277.862 114.419
2024-12-02-09:47:02-root-INFO: grad norm: 281.724 261.302 105.309
2024-12-02-09:47:03-root-INFO: Loss Change: 5330.021 -> 5150.203
2024-12-02-09:47:03-root-INFO: Regularization Change: 0.000 -> 0.520
2024-12-02-09:47:03-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-09:47:03-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:47:03-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-09:47:03-root-INFO: grad norm: 483.564 438.227 204.429
2024-12-02-09:47:03-root-INFO: grad norm: 424.669 392.049 163.222
2024-12-02-09:47:04-root-INFO: grad norm: 390.787 361.053 149.518
2024-12-02-09:47:04-root-INFO: grad norm: 362.254 335.307 137.105
2024-12-02-09:47:05-root-INFO: grad norm: 338.100 313.540 126.507
2024-12-02-09:47:05-root-INFO: grad norm: 317.266 294.213 118.730
2024-12-02-09:47:06-root-INFO: grad norm: 299.724 278.763 110.116
2024-12-02-09:47:06-root-INFO: grad norm: 284.269 264.125 105.104
2024-12-02-09:47:07-root-INFO: Loss Change: 5143.721 -> 4982.720
2024-12-02-09:47:07-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-02-09:47:07-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-09:47:07-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:47:07-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-09:47:07-root-INFO: grad norm: 515.400 474.459 201.308
2024-12-02-09:47:07-root-INFO: grad norm: 467.255 433.028 175.539
2024-12-02-09:47:08-root-INFO: grad norm: 432.773 401.373 161.841
2024-12-02-09:47:08-root-INFO: grad norm: 402.804 373.636 150.491
2024-12-02-09:47:09-root-INFO: grad norm: 377.170 350.947 138.179
2024-12-02-09:47:09-root-INFO: grad norm: 354.338 328.997 131.590
2024-12-02-09:47:10-root-INFO: grad norm: 334.681 312.117 120.809
2024-12-02-09:47:10-root-INFO: grad norm: 317.405 295.126 116.818
2024-12-02-09:47:10-root-INFO: Loss Change: 4973.523 -> 4818.438
2024-12-02-09:47:10-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-02-09:47:10-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-09:47:10-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:47:11-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-09:47:11-root-INFO: grad norm: 372.423 346.946 135.378
2024-12-02-09:47:11-root-INFO: grad norm: 341.057 317.098 125.573
2024-12-02-09:47:12-root-INFO: grad norm: 317.722 297.292 112.093
2024-12-02-09:47:12-root-INFO: grad norm: 297.346 276.867 108.441
2024-12-02-09:47:13-root-INFO: grad norm: 279.551 262.087 97.259
2024-12-02-09:47:13-root-INFO: grad norm: 263.877 246.009 95.450
2024-12-02-09:47:13-root-INFO: grad norm: 250.452 235.189 86.096
2024-12-02-09:47:14-root-INFO: grad norm: 238.884 223.047 85.532
2024-12-02-09:47:14-root-INFO: Loss Change: 4761.300 -> 4632.200
2024-12-02-09:47:14-root-INFO: Regularization Change: 0.000 -> 0.468
2024-12-02-09:47:14-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-09:47:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:47:14-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-09:47:15-root-INFO: grad norm: 337.375 313.070 125.733
2024-12-02-09:47:15-root-INFO: grad norm: 301.140 281.344 107.383
2024-12-02-09:47:16-root-INFO: grad norm: 277.508 259.639 97.973
2024-12-02-09:47:16-root-INFO: grad norm: 257.679 241.118 90.888
2024-12-02-09:47:17-root-INFO: grad norm: 240.913 226.060 83.283
2024-12-02-09:47:17-root-INFO: grad norm: 226.822 212.546 79.200
2024-12-02-09:47:18-root-INFO: grad norm: 214.774 202.074 72.758
2024-12-02-09:47:18-root-INFO: grad norm: 204.502 191.934 70.586
2024-12-02-09:47:18-root-INFO: Loss Change: 4599.230 -> 4478.322
2024-12-02-09:47:18-root-INFO: Regularization Change: 0.000 -> 0.461
2024-12-02-09:47:18-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-09:47:18-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:47:18-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-09:47:19-root-INFO: grad norm: 498.891 457.987 197.837
2024-12-02-09:47:19-root-INFO: grad norm: 438.413 409.272 157.171
2024-12-02-09:47:20-root-INFO: grad norm: 398.091 370.620 145.317
2024-12-02-09:47:20-root-INFO: grad norm: 364.104 339.999 130.278
2024-12-02-09:47:21-root-INFO: grad norm: 335.815 314.121 118.743
2024-12-02-09:47:21-root-INFO: grad norm: 311.420 290.983 110.958
2024-12-02-09:47:22-root-INFO: grad norm: 290.128 272.232 100.318
2024-12-02-09:47:22-root-INFO: grad norm: 271.297 253.800 95.853
2024-12-02-09:47:22-root-INFO: Loss Change: 4483.886 -> 4336.990
2024-12-02-09:47:22-root-INFO: Regularization Change: 0.000 -> 0.520
2024-12-02-09:47:22-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-09:47:22-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:47:23-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-09:47:23-root-INFO: grad norm: 272.383 255.029 95.671
2024-12-02-09:47:23-root-INFO: grad norm: 241.744 226.627 84.145
2024-12-02-09:47:24-root-INFO: grad norm: 221.691 208.098 76.432
2024-12-02-09:47:24-root-INFO: grad norm: 205.530 192.887 70.971
2024-12-02-09:47:25-root-INFO: grad norm: 192.301 180.867 65.321
2024-12-02-09:47:25-root-INFO: grad norm: 181.508 170.546 62.122
2024-12-02-09:47:25-root-INFO: grad norm: 172.702 162.837 57.533
2024-12-02-09:47:26-root-INFO: grad norm: 165.524 155.809 55.874
2024-12-02-09:47:26-root-INFO: Loss Change: 4316.241 -> 4208.582
2024-12-02-09:47:26-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-02-09:47:26-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-09:47:26-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-09:47:26-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-09:47:27-root-INFO: grad norm: 325.145 301.920 120.678
2024-12-02-09:47:27-root-INFO: grad norm: 281.811 264.568 97.065
2024-12-02-09:47:28-root-INFO: grad norm: 249.500 234.029 86.490
2024-12-02-09:47:28-root-INFO: grad norm: 223.972 210.654 76.080
2024-12-02-09:47:28-root-INFO: grad norm: 203.549 192.015 67.547
2024-12-02-09:47:29-root-INFO: grad norm: 187.194 176.456 62.490
2024-12-02-09:47:29-root-INFO: grad norm: 174.322 165.113 55.910
2024-12-02-09:47:30-root-INFO: grad norm: 164.138 155.091 53.740
2024-12-02-09:47:30-root-INFO: Loss Change: 4191.365 -> 4081.726
2024-12-02-09:47:30-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-09:47:30-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-09:47:30-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:47:30-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-09:47:30-root-INFO: grad norm: 324.058 291.543 141.477
2024-12-02-09:47:31-root-INFO: grad norm: 260.848 244.225 91.629
2024-12-02-09:47:31-root-INFO: grad norm: 228.331 213.487 80.984
2024-12-02-09:47:32-root-INFO: grad norm: 203.711 191.258 70.130
2024-12-02-09:47:32-root-INFO: grad norm: 184.898 174.189 62.013
2024-12-02-09:47:33-root-INFO: grad norm: 170.225 160.203 57.545
2024-12-02-09:47:33-root-INFO: grad norm: 158.880 150.397 51.220
2024-12-02-09:47:34-root-INFO: grad norm: 150.040 141.531 49.810
2024-12-02-09:47:34-root-INFO: Loss Change: 4075.138 -> 3967.831
2024-12-02-09:47:34-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-09:47:34-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-09:47:34-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:47:34-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-09:47:34-root-INFO: grad norm: 327.194 298.516 133.956
2024-12-02-09:47:35-root-INFO: grad norm: 268.961 251.662 94.902
2024-12-02-09:47:35-root-INFO: grad norm: 234.998 220.622 80.932
2024-12-02-09:47:36-root-INFO: grad norm: 208.713 195.895 72.014
2024-12-02-09:47:36-root-INFO: grad norm: 188.254 177.878 61.638
2024-12-02-09:47:37-root-INFO: grad norm: 172.088 161.975 58.124
2024-12-02-09:47:37-root-INFO: grad norm: 159.160 151.041 50.184
2024-12-02-09:47:38-root-INFO: grad norm: 148.936 140.539 49.303
2024-12-02-09:47:38-root-INFO: Loss Change: 3957.428 -> 3853.157
2024-12-02-09:47:38-root-INFO: Regularization Change: 0.000 -> 0.466
2024-12-02-09:47:38-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-09:47:38-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:47:38-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-09:47:38-root-INFO: grad norm: 165.365 155.801 55.422
2024-12-02-09:47:39-root-INFO: grad norm: 149.081 140.864 48.810
2024-12-02-09:47:39-root-INFO: grad norm: 138.430 131.333 43.755
2024-12-02-09:47:40-root-INFO: grad norm: 130.797 123.891 41.941
2024-12-02-09:47:40-root-INFO: grad norm: 125.087 118.957 38.676
2024-12-02-09:47:41-root-INFO: grad norm: 120.925 114.695 38.316
2024-12-02-09:47:41-root-INFO: grad norm: 117.699 112.096 35.882
2024-12-02-09:47:42-root-INFO: grad norm: 115.292 109.482 36.137
2024-12-02-09:47:42-root-INFO: Loss Change: 3839.217 -> 3763.621
2024-12-02-09:47:42-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-09:47:42-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-09:47:42-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:47:42-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-09:47:42-root-INFO: grad norm: 226.632 206.849 92.604
2024-12-02-09:47:43-root-INFO: grad norm: 181.792 170.621 62.743
2024-12-02-09:47:43-root-INFO: grad norm: 158.261 149.511 51.895
2024-12-02-09:47:44-root-INFO: grad norm: 142.185 134.238 46.870
2024-12-02-09:47:44-root-INFO: grad norm: 130.890 124.651 39.929
2024-12-02-09:47:45-root-INFO: grad norm: 122.820 116.365 39.293
2024-12-02-09:47:45-root-INFO: grad norm: 117.039 111.831 34.525
2024-12-02-09:47:46-root-INFO: grad norm: 112.792 107.147 35.236
2024-12-02-09:47:46-root-INFO: Loss Change: 3741.925 -> 3663.679
2024-12-02-09:47:46-root-INFO: Regularization Change: 0.000 -> 0.405
2024-12-02-09:47:46-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-09:47:46-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:47:46-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-09:47:46-root-INFO: grad norm: 277.651 252.718 114.993
2024-12-02-09:47:47-root-INFO: grad norm: 218.558 204.768 76.403
2024-12-02-09:47:47-root-INFO: grad norm: 186.361 174.187 66.252
2024-12-02-09:47:48-root-INFO: grad norm: 164.034 154.417 55.340
2024-12-02-09:47:48-root-INFO: grad norm: 147.830 139.280 49.543
2024-12-02-09:47:49-root-INFO: grad norm: 135.831 128.222 44.825
2024-12-02-09:47:49-root-INFO: grad norm: 126.738 119.991 40.800
2024-12-02-09:47:50-root-INFO: grad norm: 119.829 113.367 38.819
2024-12-02-09:47:50-root-INFO: Loss Change: 3672.262 -> 3585.621
2024-12-02-09:47:50-root-INFO: Regularization Change: 0.000 -> 0.439
2024-12-02-09:47:50-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-09:47:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:47:50-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-09:47:50-root-INFO: grad norm: 120.699 114.803 37.263
2024-12-02-09:47:51-root-INFO: grad norm: 112.855 107.592 34.060
2024-12-02-09:47:51-root-INFO: grad norm: 107.725 102.847 32.049
2024-12-02-09:47:52-root-INFO: grad norm: 104.107 99.318 31.209
2024-12-02-09:47:52-root-INFO: grad norm: 101.534 97.121 29.607
2024-12-02-09:47:53-root-INFO: grad norm: 99.690 95.197 29.588
2024-12-02-09:47:53-root-INFO: grad norm: 98.246 94.077 28.316
2024-12-02-09:47:54-root-INFO: grad norm: 97.108 92.820 28.538
2024-12-02-09:47:54-root-INFO: Loss Change: 3567.806 -> 3506.723
2024-12-02-09:47:54-root-INFO: Regularization Change: 0.000 -> 0.380
2024-12-02-09:47:54-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-09:47:54-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-09:47:54-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-09:47:54-root-INFO: grad norm: 259.092 232.761 113.803
2024-12-02-09:47:55-root-INFO: grad norm: 179.140 167.022 64.769
2024-12-02-09:47:55-root-INFO: grad norm: 155.660 146.733 51.959
2024-12-02-09:47:56-root-INFO: grad norm: 143.367 135.026 48.188
2024-12-02-09:47:56-root-INFO: grad norm: 135.025 128.240 42.262
2024-12-02-09:47:57-root-INFO: grad norm: 128.740 121.708 41.966
2024-12-02-09:47:57-root-INFO: grad norm: 123.701 117.823 37.678
2024-12-02-09:47:58-root-INFO: grad norm: 119.663 113.349 38.356
2024-12-02-09:47:58-root-INFO: Loss Change: 3498.300 -> 3414.925
2024-12-02-09:47:58-root-INFO: Regularization Change: 0.000 -> 0.455
2024-12-02-09:47:58-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-09:47:58-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:47:58-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-09:47:58-root-INFO: grad norm: 184.684 173.377 63.630
2024-12-02-09:47:59-root-INFO: grad norm: 165.184 157.480 49.858
2024-12-02-09:47:59-root-INFO: grad norm: 152.792 144.676 49.135
2024-12-02-09:48:00-root-INFO: grad norm: 143.405 136.691 43.365
2024-12-02-09:48:00-root-INFO: grad norm: 135.913 129.006 42.778
2024-12-02-09:48:00-root-INFO: grad norm: 130.097 123.999 39.363
2024-12-02-09:48:01-root-INFO: grad norm: 125.391 119.300 38.606
2024-12-02-09:48:01-root-INFO: grad norm: 121.715 116.034 36.749
2024-12-02-09:48:02-root-INFO: Loss Change: 3409.121 -> 3346.239
2024-12-02-09:48:02-root-INFO: Regularization Change: 0.000 -> 0.404
2024-12-02-09:48:02-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-09:48:02-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:48:02-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-09:48:02-root-INFO: grad norm: 203.850 186.425 82.466
2024-12-02-09:48:02-root-INFO: grad norm: 168.943 159.673 55.191
2024-12-02-09:48:03-root-INFO: grad norm: 162.620 153.554 53.538
2024-12-02-09:48:03-root-INFO: grad norm: 162.038 154.242 49.656
2024-12-02-09:48:04-root-INFO: grad norm: 163.988 155.404 52.362
2024-12-02-09:48:04-root-INFO: grad norm: 167.514 159.507 51.171
2024-12-02-09:48:05-root-INFO: grad norm: 172.632 163.828 54.424
2024-12-02-09:48:05-root-INFO: grad norm: 179.095 170.363 55.240
2024-12-02-09:48:05-root-INFO: Loss Change: 3332.352 -> 3267.629
2024-12-02-09:48:06-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-02-09:48:06-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-09:48:06-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:48:06-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-09:48:06-root-INFO: grad norm: 264.543 246.722 95.452
2024-12-02-09:48:06-root-INFO: grad norm: 247.811 235.321 77.682
2024-12-02-09:48:07-root-INFO: grad norm: 258.778 245.201 82.722
2024-12-02-09:48:07-root-INFO: grad norm: 276.750 263.336 85.116
2024-12-02-09:48:08-root-INFO: grad norm: 300.351 284.927 95.010
2024-12-02-09:48:08-root-INFO: grad norm: 325.671 309.409 101.624
2024-12-02-09:48:09-root-INFO: grad norm: 354.702 336.782 111.316
2024-12-02-09:48:09-root-INFO: grad norm: 382.707 363.017 121.176
2024-12-02-09:48:09-root-INFO: Loss Change: 3272.085 -> 3229.855
2024-12-02-09:48:09-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-02-09:48:09-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-09:48:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:48:10-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-09:48:10-root-INFO: grad norm: 436.673 413.006 141.810
2024-12-02-09:48:10-root-INFO: grad norm: 443.967 422.440 136.569
2024-12-02-09:48:11-root-INFO: grad norm: 458.562 435.879 142.440
2024-12-02-09:48:11-root-INFO: grad norm: 468.525 445.640 144.640
2024-12-02-09:48:12-root-INFO: grad norm: 473.610 450.237 146.946
2024-12-02-09:48:12-root-INFO: grad norm: 473.038 449.525 147.281
2024-12-02-09:48:13-root-INFO: grad norm: 465.934 442.850 144.841
2024-12-02-09:48:13-root-INFO: grad norm: 455.943 433.036 142.703
2024-12-02-09:48:13-root-INFO: Loss Change: 3219.567 -> 3162.984
2024-12-02-09:48:13-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-02-09:48:13-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-09:48:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:48:14-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-09:48:14-root-INFO: grad norm: 467.426 443.700 147.027
2024-12-02-09:48:14-root-INFO: grad norm: 443.571 422.310 135.679
2024-12-02-09:48:15-root-INFO: grad norm: 427.795 407.113 131.407
2024-12-02-09:48:15-root-INFO: grad norm: 415.439 395.641 126.718
2024-12-02-09:48:16-root-INFO: grad norm: 403.084 383.354 124.564
2024-12-02-09:48:16-root-INFO: grad norm: 393.604 374.536 121.026
2024-12-02-09:48:17-root-INFO: grad norm: 384.628 365.710 119.143
2024-12-02-09:48:17-root-INFO: grad norm: 378.329 359.708 117.230
2024-12-02-09:48:17-root-INFO: Loss Change: 3151.517 -> 3068.979
2024-12-02-09:48:17-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-02-09:48:17-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-09:48:17-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:48:18-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-09:48:18-root-INFO: grad norm: 416.627 395.171 131.978
2024-12-02-09:48:18-root-INFO: grad norm: 401.819 382.465 123.203
2024-12-02-09:48:19-root-INFO: grad norm: 402.312 383.108 122.813
2024-12-02-09:48:19-root-INFO: grad norm: 405.698 386.360 123.761
2024-12-02-09:48:20-root-INFO: grad norm: 409.414 389.732 125.413
2024-12-02-09:48:20-root-INFO: grad norm: 412.769 392.722 127.074
2024-12-02-09:48:21-root-INFO: grad norm: 414.611 394.633 127.148
2024-12-02-09:48:21-root-INFO: grad norm: 415.353 394.829 128.950
2024-12-02-09:48:21-root-INFO: Loss Change: 3067.228 -> 3001.840
2024-12-02-09:48:21-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-02-09:48:21-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-09:48:21-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-09:48:22-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-09:48:22-root-INFO: grad norm: 434.389 413.289 133.738
2024-12-02-09:48:22-root-INFO: grad norm: 418.753 398.640 128.218
2024-12-02-09:48:23-root-INFO: grad norm: 422.783 402.626 128.990
2024-12-02-09:48:23-root-INFO: grad norm: 430.445 409.804 131.693
2024-12-02-09:48:24-root-INFO: grad norm: 437.958 416.940 134.047
2024-12-02-09:48:24-root-INFO: grad norm: 443.424 421.751 136.935
2024-12-02-09:48:25-root-INFO: grad norm: 445.314 423.958 136.249
2024-12-02-09:48:25-root-INFO: grad norm: 444.118 422.041 138.286
2024-12-02-09:48:25-root-INFO: Loss Change: 3003.529 -> 2934.594
2024-12-02-09:48:25-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-02-09:48:25-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-09:48:25-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:48:26-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-09:48:26-root-INFO: grad norm: 426.607 406.580 129.175
2024-12-02-09:48:26-root-INFO: grad norm: 407.270 387.880 124.169
2024-12-02-09:48:27-root-INFO: grad norm: 391.501 373.818 116.331
2024-12-02-09:48:27-root-INFO: grad norm: 380.850 362.640 116.359
2024-12-02-09:48:28-root-INFO: grad norm: 373.019 356.159 110.879
2024-12-02-09:48:28-root-INFO: grad norm: 369.816 351.882 113.769
2024-12-02-09:48:29-root-INFO: grad norm: 369.596 352.901 109.827
2024-12-02-09:48:29-root-INFO: grad norm: 372.771 354.452 115.421
2024-12-02-09:48:29-root-INFO: Loss Change: 2921.611 -> 2842.411
2024-12-02-09:48:29-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-02-09:48:29-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-09:48:29-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:48:30-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-09:48:30-root-INFO: grad norm: 387.989 369.647 117.884
2024-12-02-09:48:30-root-INFO: grad norm: 380.657 362.120 117.341
2024-12-02-09:48:31-root-INFO: grad norm: 415.281 397.626 119.800
2024-12-02-09:48:31-root-INFO: Loss too large (2806.566->2810.168)! Learning rate decreased to 0.00100.
2024-12-02-09:48:31-root-INFO: grad norm: 300.192 285.907 91.500
2024-12-02-09:48:32-root-INFO: grad norm: 226.534 216.566 66.458
2024-12-02-09:48:32-root-INFO: grad norm: 182.285 173.645 55.454
2024-12-02-09:48:33-root-INFO: grad norm: 154.305 147.855 44.146
2024-12-02-09:48:33-root-INFO: grad norm: 136.814 130.436 41.285
2024-12-02-09:48:34-root-INFO: Loss Change: 2832.790 -> 2714.302
2024-12-02-09:48:34-root-INFO: Regularization Change: 0.000 -> 0.629
2024-12-02-09:48:34-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-09:48:34-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:48:34-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-09:48:34-root-INFO: grad norm: 165.873 157.525 51.961
2024-12-02-09:48:35-root-INFO: grad norm: 202.130 193.296 59.102
2024-12-02-09:48:35-root-INFO: grad norm: 263.751 252.464 76.332
2024-12-02-09:48:35-root-INFO: Loss too large (2705.894->2712.519)! Learning rate decreased to 0.00105.
2024-12-02-09:48:36-root-INFO: grad norm: 243.615 232.674 72.189
2024-12-02-09:48:36-root-INFO: grad norm: 229.706 220.178 65.472
2024-12-02-09:48:37-root-INFO: grad norm: 221.607 211.510 66.129
2024-12-02-09:48:37-root-INFO: grad norm: 218.059 209.209 61.492
2024-12-02-09:48:38-root-INFO: grad norm: 218.791 208.691 65.708
2024-12-02-09:48:38-root-INFO: Loss Change: 2714.953 -> 2655.437
2024-12-02-09:48:38-root-INFO: Regularization Change: 0.000 -> 0.568
2024-12-02-09:48:38-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-09:48:38-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:48:38-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-09:48:38-root-INFO: grad norm: 191.758 181.838 60.877
2024-12-02-09:48:39-root-INFO: grad norm: 241.977 230.911 72.337
2024-12-02-09:48:39-root-INFO: Loss too large (2627.046->2639.441)! Learning rate decreased to 0.00110.
2024-12-02-09:48:39-root-INFO: grad norm: 271.143 260.861 73.958
2024-12-02-09:48:40-root-INFO: grad norm: 312.105 298.255 91.944
2024-12-02-09:48:40-root-INFO: Loss too large (2620.363->2621.357)! Learning rate decreased to 0.00088.
2024-12-02-09:48:40-root-INFO: grad norm: 239.583 230.574 65.083
2024-12-02-09:48:41-root-INFO: grad norm: 192.429 183.705 57.286
2024-12-02-09:48:41-root-INFO: grad norm: 160.531 154.678 42.954
2024-12-02-09:48:42-root-INFO: grad norm: 139.378 132.987 41.720
2024-12-02-09:48:42-root-INFO: Loss Change: 2636.515 -> 2569.001
2024-12-02-09:48:42-root-INFO: Regularization Change: 0.000 -> 0.470
2024-12-02-09:48:42-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-09:48:42-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:48:42-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-09:48:42-root-INFO: grad norm: 131.899 124.599 43.269
2024-12-02-09:48:43-root-INFO: grad norm: 170.397 163.289 48.703
2024-12-02-09:48:43-root-INFO: Loss too large (2540.504->2545.181)! Learning rate decreased to 0.00115.
2024-12-02-09:48:43-root-INFO: grad norm: 217.877 210.142 57.539
2024-12-02-09:48:44-root-INFO: Loss too large (2536.733->2538.287)! Learning rate decreased to 0.00092.
2024-12-02-09:48:44-root-INFO: grad norm: 202.131 193.759 57.570
2024-12-02-09:48:44-root-INFO: grad norm: 191.050 184.393 49.993
2024-12-02-09:48:45-root-INFO: grad norm: 183.993 176.354 52.468
2024-12-02-09:48:45-root-INFO: grad norm: 180.029 173.863 46.713
2024-12-02-09:48:46-root-INFO: grad norm: 178.852 171.400 51.086
2024-12-02-09:48:46-root-INFO: Loss Change: 2550.161 -> 2497.316
2024-12-02-09:48:46-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-02-09:48:46-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-09:48:46-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-09:48:46-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-09:48:47-root-INFO: grad norm: 188.158 178.770 58.691
2024-12-02-09:48:47-root-INFO: Loss too large (2491.140->2495.102)! Learning rate decreased to 0.00120.
2024-12-02-09:48:47-root-INFO: grad norm: 246.252 236.756 67.722
2024-12-02-09:48:47-root-INFO: Loss too large (2484.963->2496.146)! Learning rate decreased to 0.00096.
2024-12-02-09:48:48-root-INFO: grad norm: 266.690 257.127 70.775
2024-12-02-09:48:48-root-INFO: grad norm: 294.273 283.343 79.458
2024-12-02-09:48:49-root-INFO: grad norm: 328.227 317.015 85.056
2024-12-02-09:48:49-root-INFO: Loss too large (2476.482->2477.228)! Learning rate decreased to 0.00077.
2024-12-02-09:48:49-root-INFO: grad norm: 239.193 230.215 64.918
2024-12-02-09:48:50-root-INFO: grad norm: 180.922 174.655 47.205
2024-12-02-09:48:50-root-INFO: grad norm: 143.799 138.168 39.847
2024-12-02-09:48:51-root-INFO: Loss Change: 2491.140 -> 2437.032
2024-12-02-09:48:51-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-02-09:48:51-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-09:48:51-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:48:51-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-09:48:51-root-INFO: grad norm: 102.809 96.667 35.001
2024-12-02-09:48:51-root-INFO: grad norm: 117.940 112.957 33.919
2024-12-02-09:48:52-root-INFO: Loss too large (2407.523->2407.943)! Learning rate decreased to 0.00125.
2024-12-02-09:48:52-root-INFO: grad norm: 183.361 177.982 44.087
2024-12-02-09:48:52-root-INFO: Loss too large (2404.253->2413.218)! Learning rate decreased to 0.00100.
2024-12-02-09:48:53-root-INFO: grad norm: 234.706 226.308 62.223
2024-12-02-09:48:53-root-INFO: Loss too large (2403.268->2406.229)! Learning rate decreased to 0.00080.
2024-12-02-09:48:53-root-INFO: grad norm: 209.014 202.935 50.042
2024-12-02-09:48:54-root-INFO: grad norm: 189.639 182.736 50.701
2024-12-02-09:48:54-root-INFO: grad norm: 174.641 169.535 41.921
2024-12-02-09:48:55-root-INFO: grad norm: 163.445 157.410 44.003
2024-12-02-09:48:55-root-INFO: Loss Change: 2418.449 -> 2374.832
2024-12-02-09:48:55-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-02-09:48:55-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-09:48:55-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:48:55-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-09:48:55-root-INFO: grad norm: 165.367 156.571 53.216
2024-12-02-09:48:56-root-INFO: Loss too large (2365.535->2370.239)! Learning rate decreased to 0.00131.
2024-12-02-09:48:56-root-INFO: grad norm: 255.229 247.427 62.626
2024-12-02-09:48:56-root-INFO: Loss too large (2361.170->2397.600)! Learning rate decreased to 0.00105.
2024-12-02-09:48:56-root-INFO: Loss too large (2361.170->2370.886)! Learning rate decreased to 0.00084.
2024-12-02-09:48:57-root-INFO: grad norm: 258.581 250.513 64.087
2024-12-02-09:48:57-root-INFO: grad norm: 265.815 257.802 64.775
2024-12-02-09:48:58-root-INFO: grad norm: 276.139 267.934 66.817
2024-12-02-09:48:58-root-INFO: grad norm: 288.812 280.125 70.300
2024-12-02-09:48:59-root-INFO: grad norm: 303.784 294.974 72.629
2024-12-02-09:48:59-root-INFO: grad norm: 320.035 310.395 77.959
2024-12-02-09:48:59-root-INFO: Loss Change: 2365.535 -> 2340.498
2024-12-02-09:48:59-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-02-09:48:59-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-09:48:59-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:49:00-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-09:49:00-root-INFO: grad norm: 325.738 316.017 78.986
2024-12-02-09:49:00-root-INFO: Loss too large (2329.610->2476.074)! Learning rate decreased to 0.00137.
2024-12-02-09:49:00-root-INFO: Loss too large (2329.610->2398.540)! Learning rate decreased to 0.00109.
2024-12-02-09:49:00-root-INFO: Loss too large (2329.610->2351.875)! Learning rate decreased to 0.00087.
2024-12-02-09:49:01-root-INFO: grad norm: 330.414 321.138 77.742
2024-12-02-09:49:01-root-INFO: grad norm: 339.480 330.188 78.884
2024-12-02-09:49:02-root-INFO: grad norm: 351.389 341.596 82.379
2024-12-02-09:49:02-root-INFO: grad norm: 365.024 355.351 83.476
2024-12-02-09:49:03-root-INFO: grad norm: 378.951 368.341 89.046
2024-12-02-09:49:03-root-INFO: grad norm: 392.646 382.416 89.045
2024-12-02-09:49:04-root-INFO: grad norm: 404.653 393.286 95.236
2024-12-02-09:49:04-root-INFO: Loss Change: 2329.610 -> 2311.053
2024-12-02-09:49:04-root-INFO: Regularization Change: 0.000 -> 0.228
2024-12-02-09:49:04-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-09:49:04-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:49:04-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-09:49:04-root-INFO: grad norm: 401.392 390.919 91.095
2024-12-02-09:49:04-root-INFO: Loss too large (2300.945->2534.351)! Learning rate decreased to 0.00143.
2024-12-02-09:49:04-root-INFO: Loss too large (2300.945->2414.618)! Learning rate decreased to 0.00114.
2024-12-02-09:49:05-root-INFO: Loss too large (2300.945->2340.360)! Learning rate decreased to 0.00091.
2024-12-02-09:49:05-root-INFO: grad norm: 415.514 404.397 95.475
2024-12-02-09:49:06-root-INFO: grad norm: 436.002 425.183 96.525
2024-12-02-09:49:06-root-INFO: Loss too large (2295.290->2296.961)! Learning rate decreased to 0.00073.
2024-12-02-09:49:06-root-INFO: grad norm: 293.446 285.595 67.426
2024-12-02-09:49:07-root-INFO: grad norm: 206.003 200.412 47.670
2024-12-02-09:49:07-root-INFO: grad norm: 154.137 149.576 37.219
2024-12-02-09:49:08-root-INFO: grad norm: 121.596 117.866 29.888
2024-12-02-09:49:08-root-INFO: grad norm: 101.500 98.077 26.137
2024-12-02-09:49:08-root-INFO: Loss Change: 2300.945 -> 2234.440
2024-12-02-09:49:08-root-INFO: Regularization Change: 0.000 -> 0.211
2024-12-02-09:49:08-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-09:49:08-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:49:09-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-09:49:09-root-INFO: grad norm: 107.294 99.332 40.563
2024-12-02-09:49:09-root-INFO: grad norm: 78.463 74.845 23.553
2024-12-02-09:49:10-root-INFO: grad norm: 142.677 138.694 33.477
2024-12-02-09:49:10-root-INFO: Loss too large (2206.176->2238.517)! Learning rate decreased to 0.00149.
2024-12-02-09:49:10-root-INFO: Loss too large (2206.176->2220.836)! Learning rate decreased to 0.00119.
2024-12-02-09:49:10-root-INFO: Loss too large (2206.176->2210.671)! Learning rate decreased to 0.00095.
2024-12-02-09:49:10-root-INFO: grad norm: 188.304 183.463 42.424
2024-12-02-09:49:11-root-INFO: Loss too large (2205.132->2207.313)! Learning rate decreased to 0.00076.
2024-12-02-09:49:11-root-INFO: grad norm: 175.839 171.339 39.524
2024-12-02-09:49:12-root-INFO: grad norm: 166.102 161.692 38.020
2024-12-02-09:49:12-root-INFO: grad norm: 158.838 154.692 36.053
2024-12-02-09:49:13-root-INFO: grad norm: 153.516 149.363 35.469
2024-12-02-09:49:13-root-INFO: Loss Change: 2225.432 -> 2187.190
2024-12-02-09:49:13-root-INFO: Regularization Change: 0.000 -> 0.330
2024-12-02-09:49:13-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-09:49:13-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-09:49:13-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-09:49:13-root-INFO: grad norm: 223.717 217.910 50.643
2024-12-02-09:49:13-root-INFO: Loss too large (2177.787->2285.237)! Learning rate decreased to 0.00156.
2024-12-02-09:49:14-root-INFO: Loss too large (2177.787->2232.107)! Learning rate decreased to 0.00124.
2024-12-02-09:49:14-root-INFO: Loss too large (2177.787->2200.219)! Learning rate decreased to 0.00100.
2024-12-02-09:49:14-root-INFO: Loss too large (2177.787->2182.046)! Learning rate decreased to 0.00080.
2024-12-02-09:49:14-root-INFO: grad norm: 214.385 209.329 46.281
2024-12-02-09:49:15-root-INFO: grad norm: 211.522 206.465 45.976
2024-12-02-09:49:15-root-INFO: grad norm: 212.108 207.301 44.904
2024-12-02-09:49:16-root-INFO: grad norm: 215.668 210.625 46.365
2024-12-02-09:49:16-root-INFO: grad norm: 221.763 216.833 46.502
2024-12-02-09:49:17-root-INFO: grad norm: 230.117 224.829 49.048
2024-12-02-09:49:17-root-INFO: grad norm: 240.901 235.636 50.088
2024-12-02-09:49:18-root-INFO: Loss Change: 2177.787 -> 2153.813
2024-12-02-09:49:18-root-INFO: Regularization Change: 0.000 -> 0.164
2024-12-02-09:49:18-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-09:49:18-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:49:18-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-09:49:18-root-INFO: grad norm: 343.095 333.610 80.116
2024-12-02-09:49:18-root-INFO: Loss too large (2159.921->2427.332)! Learning rate decreased to 0.00162.
2024-12-02-09:49:18-root-INFO: Loss too large (2159.921->2305.227)! Learning rate decreased to 0.00130.
2024-12-02-09:49:18-root-INFO: Loss too large (2159.921->2226.456)! Learning rate decreased to 0.00104.
2024-12-02-09:49:18-root-INFO: Loss too large (2159.921->2179.001)! Learning rate decreased to 0.00083.
2024-12-02-09:49:19-root-INFO: grad norm: 360.925 353.338 73.615
2024-12-02-09:49:19-root-INFO: Loss too large (2152.529->2154.432)! Learning rate decreased to 0.00067.
2024-12-02-09:49:20-root-INFO: grad norm: 259.235 253.358 54.887
2024-12-02-09:49:20-root-INFO: grad norm: 191.219 186.256 43.281
2024-12-02-09:49:21-root-INFO: grad norm: 147.539 143.776 33.112
2024-12-02-09:49:21-root-INFO: grad norm: 118.002 114.131 29.974
2024-12-02-09:49:21-root-INFO: grad norm: 98.502 95.452 24.323
2024-12-02-09:49:22-root-INFO: grad norm: 85.408 81.973 23.979
2024-12-02-09:49:22-root-INFO: Loss Change: 2159.921 -> 2107.777
2024-12-02-09:49:22-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-02-09:49:22-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-09:49:22-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:49:22-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-09:49:23-root-INFO: grad norm: 164.472 158.836 42.686
2024-12-02-09:49:23-root-INFO: Loss too large (2105.281->2157.409)! Learning rate decreased to 0.00170.
2024-12-02-09:49:23-root-INFO: Loss too large (2105.281->2130.018)! Learning rate decreased to 0.00136.
2024-12-02-09:49:23-root-INFO: Loss too large (2105.281->2113.925)! Learning rate decreased to 0.00108.
2024-12-02-09:49:23-root-INFO: grad norm: 251.033 245.667 51.622
2024-12-02-09:49:24-root-INFO: Loss too large (2104.964->2126.946)! Learning rate decreased to 0.00087.
2024-12-02-09:49:24-root-INFO: Loss too large (2104.964->2108.337)! Learning rate decreased to 0.00069.
2024-12-02-09:49:24-root-INFO: grad norm: 208.794 204.642 41.427
2024-12-02-09:49:25-root-INFO: grad norm: 176.175 171.923 38.471
2024-12-02-09:49:25-root-INFO: grad norm: 151.909 148.653 31.279
2024-12-02-09:49:26-root-INFO: grad norm: 132.982 129.332 30.943
2024-12-02-09:49:26-root-INFO: grad norm: 118.573 115.733 25.792
2024-12-02-09:49:27-root-INFO: grad norm: 107.339 103.992 26.598
2024-12-02-09:49:27-root-INFO: Loss Change: 2105.281 -> 2076.413
2024-12-02-09:49:27-root-INFO: Regularization Change: 0.000 -> 0.148
2024-12-02-09:49:27-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-09:49:27-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:49:27-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-09:49:27-root-INFO: grad norm: 159.438 154.599 38.982
2024-12-02-09:49:27-root-INFO: Loss too large (2071.004->2132.855)! Learning rate decreased to 0.00177.
2024-12-02-09:49:28-root-INFO: Loss too large (2071.004->2102.287)! Learning rate decreased to 0.00142.
2024-12-02-09:49:28-root-INFO: Loss too large (2071.004->2084.021)! Learning rate decreased to 0.00113.
2024-12-02-09:49:28-root-INFO: Loss too large (2071.004->2073.606)! Learning rate decreased to 0.00091.
2024-12-02-09:49:28-root-INFO: grad norm: 196.601 191.797 43.193
2024-12-02-09:49:29-root-INFO: Loss too large (2068.050->2070.624)! Learning rate decreased to 0.00072.
2024-12-02-09:49:29-root-INFO: grad norm: 182.929 179.455 35.482
2024-12-02-09:49:29-root-INFO: grad norm: 172.091 167.909 37.708
2024-12-02-09:49:30-root-INFO: grad norm: 163.876 160.795 31.627
2024-12-02-09:49:30-root-INFO: grad norm: 157.347 153.472 34.703
2024-12-02-09:49:31-root-INFO: grad norm: 152.350 149.443 29.617
2024-12-02-09:49:31-root-INFO: grad norm: 148.507 144.819 32.893
2024-12-02-09:49:32-root-INFO: Loss Change: 2071.004 -> 2046.426
2024-12-02-09:49:32-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-02-09:49:32-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-09:49:32-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:49:32-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-09:49:32-root-INFO: grad norm: 278.206 273.018 53.475
2024-12-02-09:49:32-root-INFO: Loss too large (2046.994->2315.207)! Learning rate decreased to 0.00185.
2024-12-02-09:49:32-root-INFO: Loss too large (2046.994->2202.214)! Learning rate decreased to 0.00148.
2024-12-02-09:49:32-root-INFO: Loss too large (2046.994->2127.753)! Learning rate decreased to 0.00118.
2024-12-02-09:49:33-root-INFO: Loss too large (2046.994->2081.578)! Learning rate decreased to 0.00095.
2024-12-02-09:49:33-root-INFO: Loss too large (2046.994->2054.648)! Learning rate decreased to 0.00076.
2024-12-02-09:49:33-root-INFO: grad norm: 261.650 257.013 49.041
2024-12-02-09:49:34-root-INFO: grad norm: 255.001 250.741 46.413
2024-12-02-09:49:34-root-INFO: grad norm: 250.222 245.798 46.843
2024-12-02-09:49:35-root-INFO: grad norm: 247.397 243.392 44.334
2024-12-02-09:49:35-root-INFO: grad norm: 245.519 241.154 46.095
2024-12-02-09:49:35-root-INFO: grad norm: 244.764 240.845 43.627
2024-12-02-09:49:36-root-INFO: grad norm: 244.732 240.386 45.913
2024-12-02-09:49:36-root-INFO: Loss Change: 2046.994 -> 2019.745
2024-12-02-09:49:36-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-02-09:49:36-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-09:49:36-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-09:49:36-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-09:49:37-root-INFO: grad norm: 283.543 278.273 54.409
2024-12-02-09:49:37-root-INFO: Loss too large (2025.133->2323.990)! Learning rate decreased to 0.00193.
2024-12-02-09:49:37-root-INFO: Loss too large (2025.133->2201.591)! Learning rate decreased to 0.00154.
2024-12-02-09:49:37-root-INFO: Loss too large (2025.133->2119.286)! Learning rate decreased to 0.00123.
2024-12-02-09:49:37-root-INFO: Loss too large (2025.133->2067.398)! Learning rate decreased to 0.00099.
2024-12-02-09:49:37-root-INFO: Loss too large (2025.133->2036.679)! Learning rate decreased to 0.00079.
2024-12-02-09:49:38-root-INFO: grad norm: 283.612 278.336 54.446
2024-12-02-09:49:38-root-INFO: grad norm: 290.867 286.612 49.568
2024-12-02-09:49:39-root-INFO: grad norm: 299.247 294.065 55.448
2024-12-02-09:49:39-root-INFO: grad norm: 308.171 303.904 51.103
2024-12-02-09:49:40-root-INFO: grad norm: 316.657 311.369 57.627
2024-12-02-09:49:40-root-INFO: grad norm: 324.443 319.990 53.568
2024-12-02-09:49:41-root-INFO: grad norm: 331.133 325.741 59.515
2024-12-02-09:49:41-root-INFO: Loss Change: 2025.133 -> 2007.445
2024-12-02-09:49:41-root-INFO: Regularization Change: 0.000 -> 0.151
2024-12-02-09:49:41-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-09:49:41-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:49:41-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-09:49:41-root-INFO: grad norm: 436.136 430.072 72.475
2024-12-02-09:49:42-root-INFO: Loss too large (2014.221->2638.027)! Learning rate decreased to 0.00201.
2024-12-02-09:49:42-root-INFO: Loss too large (2014.221->2414.627)! Learning rate decreased to 0.00161.
2024-12-02-09:49:42-root-INFO: Loss too large (2014.221->2243.802)! Learning rate decreased to 0.00129.
2024-12-02-09:49:42-root-INFO: Loss too large (2014.221->2124.699)! Learning rate decreased to 0.00103.
2024-12-02-09:49:42-root-INFO: Loss too large (2014.221->2049.006)! Learning rate decreased to 0.00082.
2024-12-02-09:49:43-root-INFO: grad norm: 417.690 411.971 68.877
2024-12-02-09:49:43-root-INFO: grad norm: 403.963 398.518 66.100
2024-12-02-09:49:44-root-INFO: grad norm: 387.913 382.500 64.578
2024-12-02-09:49:44-root-INFO: grad norm: 372.835 367.844 60.801
2024-12-02-09:49:45-root-INFO: grad norm: 357.313 352.238 60.005
2024-12-02-09:49:45-root-INFO: grad norm: 343.330 338.724 56.047
2024-12-02-09:49:46-root-INFO: grad norm: 330.058 325.292 55.885
2024-12-02-09:49:46-root-INFO: Loss Change: 2014.221 -> 1972.176
2024-12-02-09:49:46-root-INFO: Regularization Change: 0.000 -> 0.152
2024-12-02-09:49:46-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-09:49:46-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:49:46-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-09:49:46-root-INFO: grad norm: 369.262 364.710 57.803
2024-12-02-09:49:46-root-INFO: Loss too large (1970.275->2466.886)! Learning rate decreased to 0.00209.
2024-12-02-09:49:46-root-INFO: Loss too large (1970.275->2280.370)! Learning rate decreased to 0.00168.
2024-12-02-09:49:47-root-INFO: Loss too large (1970.275->2143.437)! Learning rate decreased to 0.00134.
2024-12-02-09:49:47-root-INFO: Loss too large (1970.275->2051.343)! Learning rate decreased to 0.00107.
2024-12-02-09:49:47-root-INFO: Loss too large (1970.275->1994.431)! Learning rate decreased to 0.00086.
2024-12-02-09:49:47-root-INFO: grad norm: 345.852 340.984 57.821
2024-12-02-09:49:48-root-INFO: grad norm: 329.438 325.404 51.396
2024-12-02-09:49:48-root-INFO: grad norm: 314.905 310.436 52.867
2024-12-02-09:49:49-root-INFO: grad norm: 303.738 300.004 47.483
2024-12-02-09:49:49-root-INFO: grad norm: 294.295 290.073 49.674
2024-12-02-09:49:50-root-INFO: grad norm: 287.087 283.536 45.014
2024-12-02-09:49:50-root-INFO: grad norm: 281.271 277.203 47.663
2024-12-02-09:49:51-root-INFO: Loss Change: 1970.275 -> 1934.593
2024-12-02-09:49:51-root-INFO: Regularization Change: 0.000 -> 0.146
2024-12-02-09:49:51-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-09:49:51-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:49:51-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-09:49:51-root-INFO: grad norm: 415.351 409.069 71.963
2024-12-02-09:49:51-root-INFO: Loss too large (1956.917->2520.852)! Learning rate decreased to 0.00218.
2024-12-02-09:49:51-root-INFO: Loss too large (1956.917->2318.618)! Learning rate decreased to 0.00175.
2024-12-02-09:49:51-root-INFO: Loss too large (1956.917->2162.481)! Learning rate decreased to 0.00140.
2024-12-02-09:49:52-root-INFO: Loss too large (1956.917->2052.885)! Learning rate decreased to 0.00112.
2024-12-02-09:49:52-root-INFO: Loss too large (1956.917->1983.209)! Learning rate decreased to 0.00089.
2024-12-02-09:49:52-root-INFO: grad norm: 385.345 380.592 60.340
2024-12-02-09:49:53-root-INFO: grad norm: 373.280 368.647 58.627
2024-12-02-09:49:53-root-INFO: grad norm: 360.339 355.656 57.904
2024-12-02-09:49:54-root-INFO: grad norm: 348.747 344.641 53.359
2024-12-02-09:49:54-root-INFO: grad norm: 337.020 332.523 54.872
2024-12-02-09:49:55-root-INFO: grad norm: 326.239 322.457 49.531
2024-12-02-09:49:55-root-INFO: grad norm: 315.907 311.608 51.939
2024-12-02-09:49:55-root-INFO: Loss Change: 1956.917 -> 1910.337
2024-12-02-09:49:55-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-02-09:49:55-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-09:49:55-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:49:55-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-09:49:56-root-INFO: grad norm: 396.416 391.579 61.737
2024-12-02-09:49:56-root-INFO: Loss too large (1920.103->2471.478)! Learning rate decreased to 0.00228.
2024-12-02-09:49:56-root-INFO: Loss too large (1920.103->2273.393)! Learning rate decreased to 0.00182.
2024-12-02-09:49:56-root-INFO: Loss too large (1920.103->2120.960)! Learning rate decreased to 0.00146.
2024-12-02-09:49:56-root-INFO: Loss too large (1920.103->2014.316)! Learning rate decreased to 0.00117.
2024-12-02-09:49:56-root-INFO: Loss too large (1920.103->1946.727)! Learning rate decreased to 0.00093.
2024-12-02-09:49:57-root-INFO: grad norm: 358.977 354.718 55.133
2024-12-02-09:49:57-root-INFO: grad norm: 332.313 328.449 50.525
2024-12-02-09:49:58-root-INFO: grad norm: 308.414 304.509 48.919
2024-12-02-09:49:58-root-INFO: grad norm: 288.702 285.395 43.570
2024-12-02-09:49:59-root-INFO: grad norm: 271.613 267.989 44.223
2024-12-02-09:49:59-root-INFO: grad norm: 257.468 254.528 38.803
2024-12-02-09:50:00-root-INFO: grad norm: 245.426 241.999 40.871
2024-12-02-09:50:00-root-INFO: Loss Change: 1920.103 -> 1870.164
2024-12-02-09:50:00-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-02-09:50:00-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-09:50:00-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:50:00-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-09:50:00-root-INFO: grad norm: 308.887 305.289 47.008
2024-12-02-09:50:00-root-INFO: Loss too large (1875.637->2267.711)! Learning rate decreased to 0.00237.
2024-12-02-09:50:01-root-INFO: Loss too large (1875.637->2116.470)! Learning rate decreased to 0.00190.
2024-12-02-09:50:01-root-INFO: Loss too large (1875.637->2007.416)! Learning rate decreased to 0.00152.
2024-12-02-09:50:01-root-INFO: Loss too large (1875.637->1935.408)! Learning rate decreased to 0.00122.
2024-12-02-09:50:01-root-INFO: Loss too large (1875.637->1891.662)! Learning rate decreased to 0.00097.
2024-12-02-09:50:02-root-INFO: grad norm: 281.715 278.206 44.323
2024-12-02-09:50:02-root-INFO: grad norm: 263.441 260.624 38.425
2024-12-02-09:50:03-root-INFO: grad norm: 247.719 244.388 40.489
2024-12-02-09:50:03-root-INFO: grad norm: 234.970 232.494 34.026
2024-12-02-09:50:03-root-INFO: grad norm: 223.922 220.754 37.535
2024-12-02-09:50:04-root-INFO: grad norm: 214.713 212.440 31.160
2024-12-02-09:50:04-root-INFO: grad norm: 206.685 203.639 35.350
2024-12-02-09:50:05-root-INFO: Loss Change: 1875.637 -> 1838.397
2024-12-02-09:50:05-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-02-09:50:05-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-09:50:05-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-09:50:05-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-09:50:05-root-INFO: grad norm: 277.902 274.786 41.502
2024-12-02-09:50:05-root-INFO: Loss too large (1842.986->2174.744)! Learning rate decreased to 0.00247.
2024-12-02-09:50:05-root-INFO: Loss too large (1842.986->2043.874)! Learning rate decreased to 0.00198.
2024-12-02-09:50:05-root-INFO: Loss too large (1842.986->1951.193)! Learning rate decreased to 0.00158.
2024-12-02-09:50:06-root-INFO: Loss too large (1842.986->1890.987)! Learning rate decreased to 0.00127.
2024-12-02-09:50:06-root-INFO: Loss too large (1842.986->1854.867)! Learning rate decreased to 0.00101.
2024-12-02-09:50:06-root-INFO: grad norm: 250.516 247.076 41.374
2024-12-02-09:50:07-root-INFO: grad norm: 233.954 231.629 32.897
2024-12-02-09:50:07-root-INFO: grad norm: 219.466 216.254 37.413
2024-12-02-09:50:08-root-INFO: grad norm: 207.382 205.302 29.300
2024-12-02-09:50:08-root-INFO: grad norm: 196.476 193.471 34.233
2024-12-02-09:50:09-root-INFO: grad norm: 187.071 185.147 26.755
2024-12-02-09:50:09-root-INFO: grad norm: 178.548 175.689 31.821
2024-12-02-09:50:09-root-INFO: Loss Change: 1842.986 -> 1807.347
2024-12-02-09:50:09-root-INFO: Regularization Change: 0.000 -> 0.184
2024-12-02-09:50:09-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-09:50:09-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:50:10-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-09:50:10-root-INFO: grad norm: 271.675 267.888 45.199
2024-12-02-09:50:10-root-INFO: Loss too large (1818.456->2130.070)! Learning rate decreased to 0.00258.
2024-12-02-09:50:10-root-INFO: Loss too large (1818.456->2006.333)! Learning rate decreased to 0.00206.
2024-12-02-09:50:10-root-INFO: Loss too large (1818.456->1918.437)! Learning rate decreased to 0.00165.
2024-12-02-09:50:10-root-INFO: Loss too large (1818.456->1861.379)! Learning rate decreased to 0.00132.
2024-12-02-09:50:11-root-INFO: Loss too large (1818.456->1827.286)! Learning rate decreased to 0.00106.
2024-12-02-09:50:11-root-INFO: grad norm: 238.643 235.382 39.318
2024-12-02-09:50:12-root-INFO: grad norm: 219.910 217.491 32.527
2024-12-02-09:50:12-root-INFO: grad norm: 204.296 201.205 35.401
2024-12-02-09:50:12-root-INFO: grad norm: 191.207 189.170 27.833
2024-12-02-09:50:13-root-INFO: grad norm: 179.127 176.241 32.030
2024-12-02-09:50:13-root-INFO: grad norm: 168.519 166.700 24.695
2024-12-02-09:50:14-root-INFO: grad norm: 158.686 155.944 29.372
2024-12-02-09:50:14-root-INFO: Loss Change: 1818.456 -> 1778.594
2024-12-02-09:50:14-root-INFO: Regularization Change: 0.000 -> 0.216
2024-12-02-09:50:14-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-09:50:14-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:50:14-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-09:50:14-root-INFO: grad norm: 223.743 221.201 33.630
2024-12-02-09:50:15-root-INFO: Loss too large (1780.459->2019.574)! Learning rate decreased to 0.00269.
2024-12-02-09:50:15-root-INFO: Loss too large (1780.459->1920.473)! Learning rate decreased to 0.00215.
2024-12-02-09:50:15-root-INFO: Loss too large (1780.459->1853.250)! Learning rate decreased to 0.00172.
2024-12-02-09:50:15-root-INFO: Loss too large (1780.459->1811.113)! Learning rate decreased to 0.00138.
2024-12-02-09:50:15-root-INFO: Loss too large (1780.459->1786.519)! Learning rate decreased to 0.00110.
2024-12-02-09:50:16-root-INFO: grad norm: 196.707 193.876 33.249
2024-12-02-09:50:16-root-INFO: grad norm: 177.750 175.888 25.661
2024-12-02-09:50:17-root-INFO: grad norm: 161.570 158.905 29.227
2024-12-02-09:50:17-root-INFO: grad norm: 147.981 146.333 22.023
2024-12-02-09:50:18-root-INFO: grad norm: 135.724 133.193 26.087
2024-12-02-09:50:18-root-INFO: grad norm: 125.135 123.592 19.591
2024-12-02-09:50:19-root-INFO: grad norm: 115.584 113.129 23.698
2024-12-02-09:50:19-root-INFO: Loss Change: 1780.459 -> 1745.693
2024-12-02-09:50:19-root-INFO: Regularization Change: 0.000 -> 0.209
2024-12-02-09:50:19-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-09:50:19-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:50:19-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-09:50:19-root-INFO: grad norm: 194.681 191.600 34.499
2024-12-02-09:50:20-root-INFO: Loss too large (1748.543->1911.884)! Learning rate decreased to 0.00280.
2024-12-02-09:50:20-root-INFO: Loss too large (1748.543->1840.506)! Learning rate decreased to 0.00224.
2024-12-02-09:50:20-root-INFO: Loss too large (1748.543->1793.729)! Learning rate decreased to 0.00179.
2024-12-02-09:50:20-root-INFO: Loss too large (1748.543->1765.110)! Learning rate decreased to 0.00143.
2024-12-02-09:50:20-root-INFO: Loss too large (1748.543->1748.740)! Learning rate decreased to 0.00115.
2024-12-02-09:50:21-root-INFO: grad norm: 157.756 155.294 27.762
2024-12-02-09:50:21-root-INFO: grad norm: 133.928 132.017 22.543
2024-12-02-09:50:22-root-INFO: grad norm: 116.894 114.482 23.627
2024-12-02-09:50:22-root-INFO: grad norm: 103.429 101.761 18.500
2024-12-02-09:50:23-root-INFO: grad norm: 92.258 89.867 20.866
2024-12-02-09:50:23-root-INFO: grad norm: 83.250 81.597 16.505
2024-12-02-09:50:24-root-INFO: grad norm: 75.845 73.424 19.012
2024-12-02-09:50:24-root-INFO: Loss Change: 1748.543 -> 1711.928
2024-12-02-09:50:24-root-INFO: Regularization Change: 0.000 -> 0.246
2024-12-02-09:50:24-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-09:50:24-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:50:24-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-09:50:24-root-INFO: grad norm: 130.155 127.852 24.377
2024-12-02-09:50:24-root-INFO: Loss too large (1708.789->1775.460)! Learning rate decreased to 0.00291.
2024-12-02-09:50:25-root-INFO: Loss too large (1708.789->1743.705)! Learning rate decreased to 0.00233.
2024-12-02-09:50:25-root-INFO: Loss too large (1708.789->1724.168)! Learning rate decreased to 0.00187.
2024-12-02-09:50:25-root-INFO: Loss too large (1708.789->1712.733)! Learning rate decreased to 0.00149.
2024-12-02-09:50:25-root-INFO: grad norm: 151.601 149.169 27.042
2024-12-02-09:50:26-root-INFO: grad norm: 183.664 181.817 25.981
2024-12-02-09:50:26-root-INFO: Loss too large (1705.611->1707.567)! Learning rate decreased to 0.00119.
2024-12-02-09:50:27-root-INFO: grad norm: 151.922 149.459 27.244
2024-12-02-09:50:27-root-INFO: grad norm: 127.004 125.520 19.356
2024-12-02-09:50:27-root-INFO: grad norm: 107.504 105.185 22.209
2024-12-02-09:50:28-root-INFO: grad norm: 92.523 91.073 16.314
2024-12-02-09:50:28-root-INFO: grad norm: 80.870 78.550 19.232
2024-12-02-09:50:29-root-INFO: Loss Change: 1708.789 -> 1677.108
2024-12-02-09:50:29-root-INFO: Regularization Change: 0.000 -> 0.284
2024-12-02-09:50:29-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-09:50:29-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-09:50:29-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-09:50:29-root-INFO: grad norm: 163.924 161.177 29.885
2024-12-02-09:50:29-root-INFO: Loss too large (1679.970->1775.024)! Learning rate decreased to 0.00304.
2024-12-02-09:50:29-root-INFO: Loss too large (1679.970->1732.746)! Learning rate decreased to 0.00243.
2024-12-02-09:50:29-root-INFO: Loss too large (1679.970->1705.049)! Learning rate decreased to 0.00194.
2024-12-02-09:50:30-root-INFO: Loss too large (1679.970->1687.865)! Learning rate decreased to 0.00155.
2024-12-02-09:50:30-root-INFO: grad norm: 173.941 171.434 29.430
2024-12-02-09:50:31-root-INFO: grad norm: 190.448 188.478 27.326
2024-12-02-09:50:31-root-INFO: Loss too large (1673.272->1673.767)! Learning rate decreased to 0.00124.
2024-12-02-09:50:31-root-INFO: grad norm: 146.802 144.357 26.679
2024-12-02-09:50:32-root-INFO: grad norm: 113.089 111.593 18.335
2024-12-02-09:50:32-root-INFO: grad norm: 91.897 89.587 20.479
2024-12-02-09:50:33-root-INFO: grad norm: 77.338 75.753 15.578
2024-12-02-09:50:33-root-INFO: grad norm: 68.033 65.690 17.703
2024-12-02-09:50:33-root-INFO: Loss Change: 1679.970 -> 1640.038
2024-12-02-09:50:33-root-INFO: Regularization Change: 0.000 -> 0.346
2024-12-02-09:50:33-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-09:50:33-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:50:33-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-09:50:34-root-INFO: grad norm: 158.220 155.893 27.033
2024-12-02-09:50:34-root-INFO: Loss too large (1640.366->1723.971)! Learning rate decreased to 0.00316.
2024-12-02-09:50:34-root-INFO: Loss too large (1640.366->1687.275)! Learning rate decreased to 0.00253.
2024-12-02-09:50:34-root-INFO: Loss too large (1640.366->1662.865)! Learning rate decreased to 0.00202.
2024-12-02-09:50:34-root-INFO: Loss too large (1640.366->1647.428)! Learning rate decreased to 0.00162.
2024-12-02-09:50:35-root-INFO: grad norm: 164.184 161.645 28.760
2024-12-02-09:50:35-root-INFO: grad norm: 171.540 169.651 25.387
2024-12-02-09:50:36-root-INFO: grad norm: 180.895 178.161 31.328
2024-12-02-09:50:36-root-INFO: grad norm: 189.731 187.891 26.362
2024-12-02-09:50:37-root-INFO: grad norm: 196.670 193.775 33.621
2024-12-02-09:50:37-root-INFO: grad norm: 201.606 199.747 27.320
2024-12-02-09:50:38-root-INFO: grad norm: 203.421 200.445 34.666
2024-12-02-09:50:38-root-INFO: Loss Change: 1640.366 -> 1604.729
2024-12-02-09:50:38-root-INFO: Regularization Change: 0.000 -> 0.559
2024-12-02-09:50:38-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-09:50:38-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:50:38-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-09:50:38-root-INFO: grad norm: 228.120 225.875 31.926
2024-12-02-09:50:39-root-INFO: Loss too large (1606.614->1800.075)! Learning rate decreased to 0.00329.
2024-12-02-09:50:39-root-INFO: Loss too large (1606.614->1717.964)! Learning rate decreased to 0.00263.
2024-12-02-09:50:39-root-INFO: Loss too large (1606.614->1661.791)! Learning rate decreased to 0.00211.
2024-12-02-09:50:39-root-INFO: Loss too large (1606.614->1625.891)! Learning rate decreased to 0.00168.
2024-12-02-09:50:39-root-INFO: grad norm: 216.250 213.219 36.082
2024-12-02-09:50:40-root-INFO: grad norm: 202.942 200.966 28.252
2024-12-02-09:50:40-root-INFO: grad norm: 191.953 189.103 32.959
2024-12-02-09:50:41-root-INFO: grad norm: 180.198 178.335 25.847
2024-12-02-09:50:41-root-INFO: grad norm: 170.671 168.023 29.945
2024-12-02-09:50:42-root-INFO: grad norm: 160.637 158.831 24.019
2024-12-02-09:50:42-root-INFO: grad norm: 153.852 151.393 27.396
2024-12-02-09:50:43-root-INFO: Loss Change: 1606.614 -> 1554.013
2024-12-02-09:50:43-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-02-09:50:43-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-09:50:43-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:50:43-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-09:50:43-root-INFO: grad norm: 290.454 286.494 47.798
2024-12-02-09:50:43-root-INFO: Loss too large (1572.219->1712.840)! Learning rate decreased to 0.00342.
2024-12-02-09:50:43-root-INFO: Loss too large (1572.219->1669.160)! Learning rate decreased to 0.00274.
2024-12-02-09:50:43-root-INFO: Loss too large (1572.219->1632.192)! Learning rate decreased to 0.00219.
2024-12-02-09:50:43-root-INFO: Loss too large (1572.219->1601.699)! Learning rate decreased to 0.00175.
2024-12-02-09:50:44-root-INFO: Loss too large (1572.219->1578.035)! Learning rate decreased to 0.00140.
2024-12-02-09:50:44-root-INFO: grad norm: 175.387 172.973 28.999
2024-12-02-09:50:45-root-INFO: grad norm: 62.826 60.353 17.457
2024-12-02-09:50:45-root-INFO: grad norm: 57.079 54.948 15.450
2024-12-02-09:50:45-root-INFO: grad norm: 53.696 51.478 15.273
2024-12-02-09:50:46-root-INFO: grad norm: 51.713 49.688 14.327
2024-12-02-09:50:46-root-INFO: grad norm: 50.287 48.231 14.232
2024-12-02-09:50:47-root-INFO: grad norm: 49.325 47.371 13.745
2024-12-02-09:50:47-root-INFO: Loss Change: 1572.219 -> 1515.275
2024-12-02-09:50:47-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-02-09:50:47-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-09:50:47-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-09:50:47-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-09:50:48-root-INFO: grad norm: 70.183 68.020 17.290
2024-12-02-09:50:48-root-INFO: Loss too large (1510.891->1515.063)! Learning rate decreased to 0.00356.
2024-12-02-09:50:48-root-INFO: Loss too large (1510.891->1510.894)! Learning rate decreased to 0.00285.
2024-12-02-09:50:48-root-INFO: grad norm: 116.030 114.309 19.910
2024-12-02-09:50:49-root-INFO: Loss too large (1508.513->1525.934)! Learning rate decreased to 0.00228.
2024-12-02-09:50:49-root-INFO: Loss too large (1508.513->1511.853)! Learning rate decreased to 0.00182.
2024-12-02-09:50:49-root-INFO: grad norm: 141.456 139.445 23.769
2024-12-02-09:50:49-root-INFO: Loss too large (1504.688->1506.486)! Learning rate decreased to 0.00146.
2024-12-02-09:50:50-root-INFO: grad norm: 117.064 115.361 19.894
2024-12-02-09:50:50-root-INFO: grad norm: 88.829 87.110 17.392
2024-12-02-09:50:51-root-INFO: grad norm: 79.385 77.855 15.511
2024-12-02-09:50:51-root-INFO: grad norm: 69.711 68.061 15.076
2024-12-02-09:50:52-root-INFO: grad norm: 64.584 63.035 14.060
2024-12-02-09:50:52-root-INFO: Loss Change: 1510.891 -> 1479.836
2024-12-02-09:50:52-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-02-09:50:52-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-09:50:52-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:50:52-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-09:50:52-root-INFO: grad norm: 124.445 122.447 22.206
2024-12-02-09:50:52-root-INFO: Loss too large (1477.850->1529.897)! Learning rate decreased to 0.00371.
2024-12-02-09:50:53-root-INFO: Loss too large (1477.850->1510.387)! Learning rate decreased to 0.00297.
2024-12-02-09:50:53-root-INFO: Loss too large (1477.850->1495.441)! Learning rate decreased to 0.00237.
2024-12-02-09:50:53-root-INFO: Loss too large (1477.850->1484.886)! Learning rate decreased to 0.00190.
2024-12-02-09:50:53-root-INFO: Loss too large (1477.850->1478.045)! Learning rate decreased to 0.00152.
2024-12-02-09:50:53-root-INFO: grad norm: 110.944 109.228 19.434
2024-12-02-09:50:54-root-INFO: grad norm: 97.299 95.590 18.152
2024-12-02-09:50:54-root-INFO: grad norm: 91.400 89.787 17.099
2024-12-02-09:50:55-root-INFO: grad norm: 84.515 82.906 16.415
2024-12-02-09:50:55-root-INFO: grad norm: 80.709 79.114 15.965
2024-12-02-09:50:56-root-INFO: grad norm: 76.356 74.785 15.409
2024-12-02-09:50:56-root-INFO: grad norm: 73.722 72.115 15.309
2024-12-02-09:50:57-root-INFO: Loss Change: 1477.850 -> 1445.391
2024-12-02-09:50:57-root-INFO: Regularization Change: 0.000 -> 0.349
2024-12-02-09:50:57-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-09:50:57-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:50:57-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-09:50:57-root-INFO: grad norm: 148.771 146.161 27.745
2024-12-02-09:50:57-root-INFO: Loss too large (1450.344->1522.408)! Learning rate decreased to 0.00386.
2024-12-02-09:50:57-root-INFO: Loss too large (1450.344->1498.197)! Learning rate decreased to 0.00309.
2024-12-02-09:50:57-root-INFO: Loss too large (1450.344->1478.087)! Learning rate decreased to 0.00247.
2024-12-02-09:50:58-root-INFO: Loss too large (1450.344->1462.891)! Learning rate decreased to 0.00198.
2024-12-02-09:50:58-root-INFO: Loss too large (1450.344->1452.469)! Learning rate decreased to 0.00158.
2024-12-02-09:50:58-root-INFO: grad norm: 135.174 133.392 21.874
2024-12-02-09:50:59-root-INFO: grad norm: 119.759 117.709 22.059
2024-12-02-09:50:59-root-INFO: grad norm: 113.807 112.123 19.507
2024-12-02-09:51:00-root-INFO: grad norm: 106.144 104.318 19.606
2024-12-02-09:51:00-root-INFO: grad norm: 102.404 100.695 18.631
2024-12-02-09:51:01-root-INFO: grad norm: 97.685 95.939 18.390
2024-12-02-09:51:01-root-INFO: grad norm: 95.632 93.809 18.585
2024-12-02-09:51:01-root-INFO: Loss Change: 1450.344 -> 1403.892
2024-12-02-09:51:01-root-INFO: Regularization Change: 0.000 -> 0.509
2024-12-02-09:51:01-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-09:51:01-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:51:02-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-09:51:02-root-INFO: grad norm: 149.384 146.969 26.757
2024-12-02-09:51:02-root-INFO: Loss too large (1407.149->1476.472)! Learning rate decreased to 0.00401.
2024-12-02-09:51:02-root-INFO: Loss too large (1407.149->1450.548)! Learning rate decreased to 0.00321.
2024-12-02-09:51:02-root-INFO: Loss too large (1407.149->1429.649)! Learning rate decreased to 0.00257.
2024-12-02-09:51:02-root-INFO: Loss too large (1407.149->1414.505)! Learning rate decreased to 0.00206.
2024-12-02-09:51:03-root-INFO: grad norm: 184.481 182.035 29.939
2024-12-02-09:51:03-root-INFO: grad norm: 230.956 228.538 33.333
2024-12-02-09:51:04-root-INFO: Loss too large (1398.748->1401.644)! Learning rate decreased to 0.00164.
2024-12-02-09:51:04-root-INFO: grad norm: 195.999 193.189 33.072
2024-12-02-09:51:04-root-INFO: grad norm: 164.853 162.892 25.355
2024-12-02-09:51:05-root-INFO: grad norm: 154.565 151.883 28.672
2024-12-02-09:51:05-root-INFO: grad norm: 150.405 148.692 22.632
2024-12-02-09:51:06-root-INFO: grad norm: 150.732 148.127 27.904
2024-12-02-09:51:06-root-INFO: Loss Change: 1407.149 -> 1347.623
2024-12-02-09:51:06-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-02-09:51:06-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-09:51:06-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:51:06-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-09:51:07-root-INFO: grad norm: 209.179 207.047 29.788
2024-12-02-09:51:07-root-INFO: Loss too large (1353.767->1507.641)! Learning rate decreased to 0.00417.
2024-12-02-09:51:07-root-INFO: Loss too large (1353.767->1471.039)! Learning rate decreased to 0.00334.
2024-12-02-09:51:07-root-INFO: Loss too large (1353.767->1432.546)! Learning rate decreased to 0.00267.
2024-12-02-09:51:07-root-INFO: Loss too large (1353.767->1397.061)! Learning rate decreased to 0.00214.
2024-12-02-09:51:07-root-INFO: Loss too large (1353.767->1369.006)! Learning rate decreased to 0.00171.
2024-12-02-09:51:08-root-INFO: grad norm: 205.899 202.973 34.587
2024-12-02-09:51:08-root-INFO: grad norm: 200.378 198.414 27.982
2024-12-02-09:51:09-root-INFO: grad norm: 196.406 193.627 32.924
2024-12-02-09:51:09-root-INFO: grad norm: 190.940 189.057 26.748
2024-12-02-09:51:10-root-INFO: grad norm: 186.774 184.147 31.215
2024-12-02-09:51:10-root-INFO: grad norm: 181.773 179.951 25.670
2024-12-02-09:51:11-root-INFO: grad norm: 177.862 175.377 29.628
2024-12-02-09:51:11-root-INFO: Loss Change: 1353.767 -> 1318.034
2024-12-02-09:51:11-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-02-09:51:11-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-09:51:11-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-09:51:11-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-09:51:11-root-INFO: grad norm: 222.023 219.351 34.343
2024-12-02-09:51:11-root-INFO: Loss too large (1328.128->1492.740)! Learning rate decreased to 0.00434.
2024-12-02-09:51:12-root-INFO: Loss too large (1328.128->1453.803)! Learning rate decreased to 0.00347.
2024-12-02-09:51:12-root-INFO: Loss too large (1328.128->1411.419)! Learning rate decreased to 0.00278.
2024-12-02-09:51:12-root-INFO: Loss too large (1328.128->1371.282)! Learning rate decreased to 0.00222.
2024-12-02-09:51:12-root-INFO: Loss too large (1328.128->1339.358)! Learning rate decreased to 0.00178.
2024-12-02-09:51:13-root-INFO: grad norm: 203.840 201.327 31.913
2024-12-02-09:51:13-root-INFO: grad norm: 189.627 187.489 28.396
2024-12-02-09:51:13-root-INFO: grad norm: 178.571 176.320 28.263
2024-12-02-09:51:14-root-INFO: grad norm: 169.887 167.981 25.372
2024-12-02-09:51:14-root-INFO: grad norm: 162.435 160.352 25.928
2024-12-02-09:51:15-root-INFO: grad norm: 156.693 154.921 23.501
2024-12-02-09:51:15-root-INFO: grad norm: 151.743 149.773 24.372
2024-12-02-09:51:16-root-INFO: Loss Change: 1328.128 -> 1283.772
2024-12-02-09:51:16-root-INFO: Regularization Change: 0.000 -> 0.448
2024-12-02-09:51:16-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-09:51:16-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:51:16-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-09:51:16-root-INFO: grad norm: 198.441 196.158 30.013
2024-12-02-09:51:16-root-INFO: Loss too large (1290.677->1450.873)! Learning rate decreased to 0.00451.
2024-12-02-09:51:16-root-INFO: Loss too large (1290.677->1408.441)! Learning rate decreased to 0.00361.
2024-12-02-09:51:16-root-INFO: Loss too large (1290.677->1364.256)! Learning rate decreased to 0.00289.
2024-12-02-09:51:17-root-INFO: Loss too large (1290.677->1325.119)! Learning rate decreased to 0.00231.
2024-12-02-09:51:17-root-INFO: Loss too large (1290.677->1296.376)! Learning rate decreased to 0.00185.
2024-12-02-09:51:17-root-INFO: grad norm: 176.168 174.083 27.021
2024-12-02-09:51:18-root-INFO: grad norm: 163.738 161.892 24.516
2024-12-02-09:51:18-root-INFO: grad norm: 152.318 150.443 23.822
2024-12-02-09:51:19-root-INFO: grad norm: 145.387 143.716 21.981
2024-12-02-09:51:19-root-INFO: grad norm: 138.932 137.174 22.031
2024-12-02-09:51:20-root-INFO: grad norm: 134.975 133.395 20.592
2024-12-02-09:51:20-root-INFO: grad norm: 131.523 129.831 21.027
2024-12-02-09:51:20-root-INFO: Loss Change: 1290.677 -> 1248.317
2024-12-02-09:51:20-root-INFO: Regularization Change: 0.000 -> 0.443
2024-12-02-09:51:20-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-09:51:20-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:51:21-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-09:51:21-root-INFO: grad norm: 163.615 161.648 25.295
2024-12-02-09:51:21-root-INFO: Loss too large (1252.948->1400.852)! Learning rate decreased to 0.00469.
2024-12-02-09:51:21-root-INFO: Loss too large (1252.948->1357.189)! Learning rate decreased to 0.00375.
2024-12-02-09:51:21-root-INFO: Loss too large (1252.948->1315.312)! Learning rate decreased to 0.00300.
2024-12-02-09:51:21-root-INFO: Loss too large (1252.948->1281.424)! Learning rate decreased to 0.00240.
2024-12-02-09:51:21-root-INFO: Loss too large (1252.948->1258.429)! Learning rate decreased to 0.00192.
2024-12-02-09:51:22-root-INFO: grad norm: 157.742 155.952 23.692
2024-12-02-09:51:22-root-INFO: grad norm: 152.880 151.154 22.906
2024-12-02-09:51:23-root-INFO: grad norm: 146.814 145.097 22.391
2024-12-02-09:51:23-root-INFO: grad norm: 142.990 141.376 21.423
2024-12-02-09:51:24-root-INFO: grad norm: 138.737 137.076 21.406
2024-12-02-09:51:24-root-INFO: grad norm: 136.321 134.769 20.508
2024-12-02-09:51:25-root-INFO: grad norm: 133.935 132.307 20.818
2024-12-02-09:51:25-root-INFO: Loss Change: 1252.948 -> 1219.816
2024-12-02-09:51:25-root-INFO: Regularization Change: 0.000 -> 0.434
2024-12-02-09:51:25-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-09:51:25-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:51:25-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-09:51:25-root-INFO: grad norm: 172.276 170.211 26.594
2024-12-02-09:51:25-root-INFO: Loss too large (1227.222->1387.044)! Learning rate decreased to 0.00488.
2024-12-02-09:51:26-root-INFO: Loss too large (1227.222->1341.226)! Learning rate decreased to 0.00390.
2024-12-02-09:51:26-root-INFO: Loss too large (1227.222->1295.163)! Learning rate decreased to 0.00312.
2024-12-02-09:51:26-root-INFO: Loss too large (1227.222->1256.707)! Learning rate decreased to 0.00250.
2024-12-02-09:51:26-root-INFO: Loss too large (1227.222->1230.461)! Learning rate decreased to 0.00200.
2024-12-02-09:51:27-root-INFO: grad norm: 157.198 155.350 24.032
2024-12-02-09:51:27-root-INFO: grad norm: 150.087 148.361 22.699
2024-12-02-09:51:28-root-INFO: grad norm: 141.202 139.541 21.597
2024-12-02-09:51:28-root-INFO: grad norm: 136.778 135.180 20.848
2024-12-02-09:51:28-root-INFO: grad norm: 131.596 130.025 20.272
2024-12-02-09:51:29-root-INFO: grad norm: 129.076 127.546 19.818
2024-12-02-09:51:29-root-INFO: grad norm: 126.471 124.941 19.609
2024-12-02-09:51:30-root-INFO: Loss Change: 1227.222 -> 1189.068
2024-12-02-09:51:30-root-INFO: Regularization Change: 0.000 -> 0.467
2024-12-02-09:51:30-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-09:51:30-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-09:51:30-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-09:51:30-root-INFO: grad norm: 157.432 155.705 23.257
2024-12-02-09:51:30-root-INFO: Loss too large (1191.333->1347.715)! Learning rate decreased to 0.00507.
2024-12-02-09:51:30-root-INFO: Loss too large (1191.333->1300.705)! Learning rate decreased to 0.00405.
2024-12-02-09:51:31-root-INFO: Loss too large (1191.333->1254.869)! Learning rate decreased to 0.00324.
2024-12-02-09:51:31-root-INFO: Loss too large (1191.333->1218.082)! Learning rate decreased to 0.00259.
2024-12-02-09:51:31-root-INFO: Loss too large (1191.333->1193.962)! Learning rate decreased to 0.00208.
2024-12-02-09:51:31-root-INFO: grad norm: 143.483 141.873 21.439
2024-12-02-09:51:32-root-INFO: grad norm: 138.385 136.874 20.391
2024-12-02-09:51:32-root-INFO: grad norm: 131.948 130.414 20.061
2024-12-02-09:51:33-root-INFO: grad norm: 129.201 127.756 19.273
2024-12-02-09:51:33-root-INFO: grad norm: 126.002 124.507 19.351
2024-12-02-09:51:34-root-INFO: grad norm: 124.709 123.293 18.739
2024-12-02-09:51:35-root-INFO: grad norm: 123.511 122.032 19.057
2024-12-02-09:51:35-root-INFO: Loss Change: 1191.333 -> 1157.734
2024-12-02-09:51:35-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-02-09:51:35-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-09:51:35-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:51:35-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-09:51:35-root-INFO: grad norm: 164.287 161.942 27.660
2024-12-02-09:51:35-root-INFO: Loss too large (1171.268->1336.167)! Learning rate decreased to 0.00527.
2024-12-02-09:51:36-root-INFO: Loss too large (1171.268->1287.402)! Learning rate decreased to 0.00421.
2024-12-02-09:51:36-root-INFO: Loss too large (1171.268->1238.247)! Learning rate decreased to 0.00337.
2024-12-02-09:51:36-root-INFO: Loss too large (1171.268->1197.880)! Learning rate decreased to 0.00270.
2024-12-02-09:51:36-root-INFO: Loss too large (1171.268->1171.362)! Learning rate decreased to 0.00216.
2024-12-02-09:51:37-root-INFO: grad norm: 145.498 143.902 21.489
2024-12-02-09:51:37-root-INFO: grad norm: 138.880 137.051 22.463
2024-12-02-09:51:38-root-INFO: grad norm: 129.323 127.892 19.184
2024-12-02-09:51:38-root-INFO: grad norm: 125.332 123.689 20.226
2024-12-02-09:51:39-root-INFO: grad norm: 120.035 118.680 17.984
2024-12-02-09:51:39-root-INFO: grad norm: 117.857 116.314 19.012
2024-12-02-09:51:40-root-INFO: grad norm: 115.372 114.047 17.431
2024-12-02-09:51:40-root-INFO: Loss Change: 1171.268 -> 1131.301
2024-12-02-09:51:40-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-09:51:40-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-09:51:40-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:51:40-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-09:51:40-root-INFO: grad norm: 143.804 142.205 21.387
2024-12-02-09:51:41-root-INFO: Loss too large (1135.126->1288.990)! Learning rate decreased to 0.00547.
2024-12-02-09:51:41-root-INFO: Loss too large (1135.126->1239.402)! Learning rate decreased to 0.00438.
2024-12-02-09:51:41-root-INFO: Loss too large (1135.126->1192.610)! Learning rate decreased to 0.00350.
2024-12-02-09:51:41-root-INFO: Loss too large (1135.126->1156.879)! Learning rate decreased to 0.00280.
2024-12-02-09:51:42-root-INFO: grad norm: 202.077 199.925 29.411
2024-12-02-09:51:42-root-INFO: Loss too large (1134.813->1155.046)! Learning rate decreased to 0.00224.
2024-12-02-09:51:42-root-INFO: Loss too large (1134.813->1135.685)! Learning rate decreased to 0.00179.
2024-12-02-09:51:42-root-INFO: grad norm: 124.304 122.911 18.555
2024-12-02-09:51:43-root-INFO: grad norm: 60.725 59.560 11.837
2024-12-02-09:51:43-root-INFO: grad norm: 48.594 47.330 11.013
2024-12-02-09:51:44-root-INFO: grad norm: 41.964 40.706 10.201
2024-12-02-09:51:44-root-INFO: grad norm: 39.385 38.059 10.135
2024-12-02-09:51:45-root-INFO: grad norm: 38.047 36.738 9.894
2024-12-02-09:51:45-root-INFO: Loss Change: 1135.126 -> 1098.126
2024-12-02-09:51:45-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-02-09:51:45-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-09:51:45-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:51:45-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-09:51:45-root-INFO: grad norm: 71.971 70.325 15.304
2024-12-02-09:51:46-root-INFO: Loss too large (1101.471->1142.598)! Learning rate decreased to 0.00568.
2024-12-02-09:51:46-root-INFO: Loss too large (1101.471->1121.335)! Learning rate decreased to 0.00455.
2024-12-02-09:51:46-root-INFO: Loss too large (1101.471->1108.649)! Learning rate decreased to 0.00364.
2024-12-02-09:51:46-root-INFO: Loss too large (1101.471->1101.709)! Learning rate decreased to 0.00291.
2024-12-02-09:51:46-root-INFO: grad norm: 101.424 100.188 15.786
2024-12-02-09:51:47-root-INFO: Loss too large (1098.263->1102.019)! Learning rate decreased to 0.00233.
2024-12-02-09:51:47-root-INFO: grad norm: 102.520 101.140 16.763
2024-12-02-09:51:48-root-INFO: grad norm: 103.952 102.720 15.960
2024-12-02-09:51:48-root-INFO: grad norm: 104.186 102.847 16.650
2024-12-02-09:51:49-root-INFO: grad norm: 104.422 103.185 16.026
2024-12-02-09:51:49-root-INFO: grad norm: 104.342 103.027 16.513
2024-12-02-09:51:50-root-INFO: grad norm: 104.248 103.007 16.039
2024-12-02-09:51:50-root-INFO: Loss Change: 1101.471 -> 1078.574
2024-12-02-09:51:50-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-02-09:51:50-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-09:51:50-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:51:50-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-09:51:50-root-INFO: grad norm: 126.822 125.413 18.848
2024-12-02-09:51:50-root-INFO: Loss too large (1081.581->1231.699)! Learning rate decreased to 0.00590.
2024-12-02-09:51:51-root-INFO: Loss too large (1081.581->1180.661)! Learning rate decreased to 0.00472.
2024-12-02-09:51:51-root-INFO: Loss too large (1081.581->1134.374)! Learning rate decreased to 0.00378.
2024-12-02-09:51:51-root-INFO: Loss too large (1081.581->1100.820)! Learning rate decreased to 0.00302.
2024-12-02-09:51:51-root-INFO: grad norm: 187.332 185.358 27.124
2024-12-02-09:51:52-root-INFO: Loss too large (1081.125->1102.442)! Learning rate decreased to 0.00242.
2024-12-02-09:51:52-root-INFO: Loss too large (1081.125->1084.615)! Learning rate decreased to 0.00193.
2024-12-02-09:51:52-root-INFO: grad norm: 119.200 117.929 17.358
2024-12-02-09:51:53-root-INFO: grad norm: 54.558 53.377 11.289
2024-12-02-09:51:53-root-INFO: grad norm: 44.736 43.464 10.588
2024-12-02-09:51:54-root-INFO: grad norm: 39.135 37.837 9.995
2024-12-02-09:51:54-root-INFO: grad norm: 36.946 35.596 9.896
2024-12-02-09:51:55-root-INFO: grad norm: 35.789 34.449 9.702
2024-12-02-09:51:55-root-INFO: Loss Change: 1081.581 -> 1048.049
2024-12-02-09:51:55-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-09:51:55-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-09:51:55-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-09:51:55-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-09:51:55-root-INFO: grad norm: 67.039 65.496 14.302
2024-12-02-09:51:55-root-INFO: Loss too large (1050.239->1084.130)! Learning rate decreased to 0.00613.
2024-12-02-09:51:55-root-INFO: Loss too large (1050.239->1065.256)! Learning rate decreased to 0.00490.
2024-12-02-09:51:56-root-INFO: Loss too large (1050.239->1054.503)! Learning rate decreased to 0.00392.
2024-12-02-09:51:56-root-INFO: grad norm: 127.283 125.897 18.730
2024-12-02-09:51:56-root-INFO: Loss too large (1048.871->1072.801)! Learning rate decreased to 0.00314.
2024-12-02-09:51:56-root-INFO: Loss too large (1048.871->1058.673)! Learning rate decreased to 0.00251.
2024-12-02-09:51:57-root-INFO: Loss too large (1048.871->1049.641)! Learning rate decreased to 0.00201.
2024-12-02-09:51:57-root-INFO: grad norm: 91.192 90.021 14.568
2024-12-02-09:51:58-root-INFO: grad norm: 55.320 54.250 10.826
2024-12-02-09:51:58-root-INFO: grad norm: 46.089 44.884 10.466
2024-12-02-09:51:58-root-INFO: grad norm: 39.881 38.707 9.606
2024-12-02-09:51:59-root-INFO: grad norm: 37.232 35.969 9.619
2024-12-02-09:51:59-root-INFO: grad norm: 35.663 34.431 9.295
2024-12-02-09:52:00-root-INFO: Loss Change: 1050.239 -> 1024.009
2024-12-02-09:52:00-root-INFO: Regularization Change: 0.000 -> 0.434
2024-12-02-09:52:00-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-09:52:00-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:52:00-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-09:52:00-root-INFO: grad norm: 73.100 71.490 15.254
2024-12-02-09:52:00-root-INFO: Loss too large (1028.697->1086.790)! Learning rate decreased to 0.00636.
2024-12-02-09:52:00-root-INFO: Loss too large (1028.697->1058.058)! Learning rate decreased to 0.00509.
2024-12-02-09:52:01-root-INFO: Loss too large (1028.697->1040.474)! Learning rate decreased to 0.00407.
2024-12-02-09:52:01-root-INFO: Loss too large (1028.697->1030.778)! Learning rate decreased to 0.00326.
2024-12-02-09:52:01-root-INFO: grad norm: 110.441 109.169 16.710
2024-12-02-09:52:01-root-INFO: Loss too large (1025.914->1032.910)! Learning rate decreased to 0.00261.
2024-12-02-09:52:02-root-INFO: Loss too large (1025.914->1025.965)! Learning rate decreased to 0.00208.
2024-12-02-09:52:02-root-INFO: grad norm: 82.629 81.353 14.462
2024-12-02-09:52:02-root-INFO: grad norm: 54.039 52.946 10.809
2024-12-02-09:52:03-root-INFO: grad norm: 45.766 44.473 10.803
2024-12-02-09:52:03-root-INFO: grad norm: 39.822 38.626 9.688
2024-12-02-09:52:04-root-INFO: grad norm: 37.166 35.839 9.839
2024-12-02-09:52:04-root-INFO: grad norm: 35.495 34.238 9.365
2024-12-02-09:52:05-root-INFO: Loss Change: 1028.697 -> 1002.368
2024-12-02-09:52:05-root-INFO: Regularization Change: 0.000 -> 0.423
2024-12-02-09:52:05-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-09:52:05-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:52:05-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-09:52:05-root-INFO: grad norm: 57.828 56.513 12.261
2024-12-02-09:52:05-root-INFO: Loss too large (1002.184->1033.563)! Learning rate decreased to 0.00660.
2024-12-02-09:52:05-root-INFO: Loss too large (1002.184->1016.292)! Learning rate decreased to 0.00528.
2024-12-02-09:52:05-root-INFO: Loss too large (1002.184->1006.626)! Learning rate decreased to 0.00423.
2024-12-02-09:52:06-root-INFO: grad norm: 117.242 115.964 17.264
2024-12-02-09:52:06-root-INFO: Loss too large (1001.605->1023.139)! Learning rate decreased to 0.00338.
2024-12-02-09:52:06-root-INFO: Loss too large (1001.605->1010.641)! Learning rate decreased to 0.00270.
2024-12-02-09:52:06-root-INFO: Loss too large (1001.605->1002.588)! Learning rate decreased to 0.00216.
2024-12-02-09:52:07-root-INFO: grad norm: 83.809 82.717 13.485
2024-12-02-09:52:07-root-INFO: grad norm: 49.049 47.963 10.264
2024-12-02-09:52:08-root-INFO: grad norm: 41.508 40.308 9.911
2024-12-02-09:52:08-root-INFO: grad norm: 36.598 35.381 9.360
2024-12-02-09:52:09-root-INFO: grad norm: 34.634 33.357 9.320
2024-12-02-09:52:09-root-INFO: grad norm: 33.515 32.247 9.133
2024-12-02-09:52:09-root-INFO: Loss Change: 1002.184 -> 978.414
2024-12-02-09:52:09-root-INFO: Regularization Change: 0.000 -> 0.432
2024-12-02-09:52:09-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-09:52:09-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:52:09-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-09:52:10-root-INFO: grad norm: 59.407 58.050 12.628
2024-12-02-09:52:10-root-INFO: Loss too large (980.578->1012.026)! Learning rate decreased to 0.00685.
2024-12-02-09:52:10-root-INFO: Loss too large (980.578->994.164)! Learning rate decreased to 0.00548.
2024-12-02-09:52:10-root-INFO: Loss too large (980.578->984.287)! Learning rate decreased to 0.00439.
2024-12-02-09:52:11-root-INFO: grad norm: 117.176 115.871 17.439
2024-12-02-09:52:11-root-INFO: Loss too large (979.236->1001.419)! Learning rate decreased to 0.00351.
2024-12-02-09:52:11-root-INFO: Loss too large (979.236->988.748)! Learning rate decreased to 0.00281.
2024-12-02-09:52:11-root-INFO: Loss too large (979.236->980.528)! Learning rate decreased to 0.00225.
2024-12-02-09:52:12-root-INFO: grad norm: 84.109 83.020 13.490
2024-12-02-09:52:12-root-INFO: grad norm: 48.145 47.067 10.133
2024-12-02-09:52:13-root-INFO: grad norm: 40.965 39.768 9.829
2024-12-02-09:52:13-root-INFO: grad norm: 36.159 34.956 9.246
2024-12-02-09:52:14-root-INFO: grad norm: 34.210 32.940 9.236
2024-12-02-09:52:14-root-INFO: grad norm: 33.077 31.822 9.025
2024-12-02-09:52:14-root-INFO: Loss Change: 980.578 -> 955.611
2024-12-02-09:52:14-root-INFO: Regularization Change: 0.000 -> 0.467
2024-12-02-09:52:14-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-09:52:14-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-09:52:15-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-09:52:15-root-INFO: grad norm: 57.973 56.668 12.235
2024-12-02-09:52:15-root-INFO: Loss too large (956.761->989.278)! Learning rate decreased to 0.00711.
2024-12-02-09:52:15-root-INFO: Loss too large (956.761->970.750)! Learning rate decreased to 0.00569.
2024-12-02-09:52:15-root-INFO: Loss too large (956.761->960.605)! Learning rate decreased to 0.00455.
2024-12-02-09:52:16-root-INFO: grad norm: 113.015 111.778 16.680
2024-12-02-09:52:16-root-INFO: Loss too large (955.468->974.934)! Learning rate decreased to 0.00364.
2024-12-02-09:52:16-root-INFO: Loss too large (955.468->963.347)! Learning rate decreased to 0.00291.
2024-12-02-09:52:16-root-INFO: Loss too large (955.468->955.867)! Learning rate decreased to 0.00233.
2024-12-02-09:52:17-root-INFO: grad norm: 77.557 76.501 12.753
2024-12-02-09:52:17-root-INFO: grad norm: 42.311 41.180 9.717
2024-12-02-09:52:18-root-INFO: grad norm: 36.686 35.434 9.503
2024-12-02-09:52:18-root-INFO: grad norm: 33.655 32.394 9.125
2024-12-02-09:52:18-root-INFO: grad norm: 32.548 31.250 9.101
2024-12-02-09:52:19-root-INFO: grad norm: 31.979 30.695 8.971
2024-12-02-09:52:19-root-INFO: Loss Change: 956.761 -> 932.018
2024-12-02-09:52:19-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-09:52:19-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-09:52:19-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-09:52:19-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-09:52:20-root-INFO: grad norm: 56.652 55.322 12.206
2024-12-02-09:52:20-root-INFO: Loss too large (932.985->956.676)! Learning rate decreased to 0.00738.
2024-12-02-09:52:20-root-INFO: Loss too large (932.985->941.528)! Learning rate decreased to 0.00590.
2024-12-02-09:52:20-root-INFO: Loss too large (932.985->933.601)! Learning rate decreased to 0.00472.
2024-12-02-09:52:21-root-INFO: grad norm: 95.680 94.548 14.674
2024-12-02-09:52:21-root-INFO: Loss too large (929.801->940.862)! Learning rate decreased to 0.00378.
2024-12-02-09:52:21-root-INFO: Loss too large (929.801->932.888)! Learning rate decreased to 0.00302.
2024-12-02-09:52:21-root-INFO: grad norm: 81.545 80.515 12.922
2024-12-02-09:52:22-root-INFO: grad norm: 59.156 58.167 10.774
2024-12-02-09:52:22-root-INFO: grad norm: 53.758 52.715 10.539
2024-12-02-09:52:23-root-INFO: grad norm: 47.046 46.044 9.658
2024-12-02-09:52:23-root-INFO: grad norm: 44.028 42.947 9.697
2024-12-02-09:52:24-root-INFO: grad norm: 40.703 39.667 9.123
2024-12-02-09:52:24-root-INFO: Loss Change: 932.985 -> 903.953
2024-12-02-09:52:24-root-INFO: Regularization Change: 0.000 -> 0.711
2024-12-02-09:52:24-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-09:52:24-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-09:52:24-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-09:52:24-root-INFO: grad norm: 57.787 56.735 10.978
2024-12-02-09:52:24-root-INFO: Loss too large (904.445->943.630)! Learning rate decreased to 0.00765.
2024-12-02-09:52:24-root-INFO: Loss too large (904.445->921.154)! Learning rate decreased to 0.00612.
2024-12-02-09:52:25-root-INFO: Loss too large (904.445->909.075)! Learning rate decreased to 0.00490.
2024-12-02-09:52:25-root-INFO: grad norm: 105.658 104.471 15.788
2024-12-02-09:52:25-root-INFO: Loss too large (903.113->917.465)! Learning rate decreased to 0.00392.
2024-12-02-09:52:25-root-INFO: Loss too large (903.113->907.744)! Learning rate decreased to 0.00314.
2024-12-02-09:52:26-root-INFO: grad norm: 83.177 82.247 12.405
2024-12-02-09:52:26-root-INFO: grad norm: 49.120 48.127 9.826
2024-12-02-09:52:27-root-INFO: grad norm: 43.618 42.592 9.406
2024-12-02-09:52:27-root-INFO: grad norm: 38.066 37.014 8.889
2024-12-02-09:52:28-root-INFO: grad norm: 35.537 34.431 8.798
2024-12-02-09:52:28-root-INFO: grad norm: 33.293 32.188 8.503
2024-12-02-09:52:28-root-INFO: Loss Change: 904.445 -> 876.825
2024-12-02-09:52:28-root-INFO: Regularization Change: 0.000 -> 0.686
2024-12-02-09:52:28-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-09:52:28-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-09:52:29-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-09:52:29-root-INFO: grad norm: 56.240 55.079 11.369
2024-12-02-09:52:29-root-INFO: Loss too large (878.168->906.699)! Learning rate decreased to 0.00794.
2024-12-02-09:52:29-root-INFO: Loss too large (878.168->888.505)! Learning rate decreased to 0.00635.
2024-12-02-09:52:29-root-INFO: Loss too large (878.168->879.223)! Learning rate decreased to 0.00508.
2024-12-02-09:52:30-root-INFO: grad norm: 86.970 85.982 13.078
2024-12-02-09:52:30-root-INFO: Loss too large (874.873->881.908)! Learning rate decreased to 0.00406.
2024-12-02-09:52:30-root-INFO: Loss too large (874.873->875.659)! Learning rate decreased to 0.00325.
2024-12-02-09:52:30-root-INFO: grad norm: 66.066 65.106 11.226
2024-12-02-09:52:31-root-INFO: grad norm: 40.413 39.437 8.824
2024-12-02-09:52:31-root-INFO: grad norm: 35.515 34.406 8.804
2024-12-02-09:52:32-root-INFO: grad norm: 31.668 30.582 8.223
2024-12-02-09:52:32-root-INFO: grad norm: 29.958 28.789 8.284
2024-12-02-09:52:33-root-INFO: grad norm: 28.755 27.618 8.006
2024-12-02-09:52:33-root-INFO: Loss Change: 878.168 -> 851.354
2024-12-02-09:52:33-root-INFO: Regularization Change: 0.000 -> 0.683
2024-12-02-09:52:33-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-09:52:33-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:52:33-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-09:52:33-root-INFO: grad norm: 39.129 37.992 9.361
2024-12-02-09:52:34-root-INFO: Loss too large (851.422->853.974)! Learning rate decreased to 0.00823.
2024-12-02-09:52:34-root-INFO: grad norm: 95.044 94.068 13.584
2024-12-02-09:52:34-root-INFO: Loss too large (849.960->888.332)! Learning rate decreased to 0.00659.
2024-12-02-09:52:34-root-INFO: Loss too large (849.960->870.834)! Learning rate decreased to 0.00527.
2024-12-02-09:52:35-root-INFO: Loss too large (849.960->859.131)! Learning rate decreased to 0.00422.
2024-12-02-09:52:35-root-INFO: Loss too large (849.960->851.539)! Learning rate decreased to 0.00337.
2024-12-02-09:52:35-root-INFO: grad norm: 65.976 65.092 10.766
2024-12-02-09:52:36-root-INFO: grad norm: 33.206 32.232 7.987
2024-12-02-09:52:36-root-INFO: grad norm: 29.386 28.276 7.999
2024-12-02-09:52:37-root-INFO: grad norm: 27.060 25.964 7.621
2024-12-02-09:52:37-root-INFO: grad norm: 26.083 24.935 7.654
2024-12-02-09:52:38-root-INFO: grad norm: 25.481 24.355 7.492
2024-12-02-09:52:38-root-INFO: Loss Change: 851.422 -> 827.858
2024-12-02-09:52:38-root-INFO: Regularization Change: 0.000 -> 0.728
2024-12-02-09:52:38-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-09:52:38-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:52:38-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-09:52:38-root-INFO: grad norm: 43.015 41.909 9.691
2024-12-02-09:52:38-root-INFO: Loss too large (828.579->836.083)! Learning rate decreased to 0.00854.
2024-12-02-09:52:38-root-INFO: Loss too large (828.579->829.223)! Learning rate decreased to 0.00683.
2024-12-02-09:52:39-root-INFO: grad norm: 74.466 73.595 11.352
2024-12-02-09:52:39-root-INFO: Loss too large (825.990->836.721)! Learning rate decreased to 0.00546.
2024-12-02-09:52:39-root-INFO: Loss too large (825.990->829.599)! Learning rate decreased to 0.00437.
2024-12-02-09:52:40-root-INFO: grad norm: 64.720 63.856 10.537
2024-12-02-09:52:40-root-INFO: grad norm: 47.357 46.545 8.734
2024-12-02-09:52:41-root-INFO: grad norm: 43.786 42.888 8.823
2024-12-02-09:52:41-root-INFO: grad norm: 38.521 37.690 7.957
2024-12-02-09:52:42-root-INFO: grad norm: 36.318 35.382 8.193
2024-12-02-09:52:42-root-INFO: grad norm: 33.453 32.594 7.535
2024-12-02-09:52:42-root-INFO: Loss Change: 828.579 -> 804.756
2024-12-02-09:52:42-root-INFO: Regularization Change: 0.000 -> 0.858
2024-12-02-09:52:42-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-09:52:42-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:52:42-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-09:52:43-root-INFO: grad norm: 50.093 49.098 9.936
2024-12-02-09:52:43-root-INFO: Loss too large (807.451->826.262)! Learning rate decreased to 0.00885.
2024-12-02-09:52:43-root-INFO: Loss too large (807.451->812.198)! Learning rate decreased to 0.00708.
2024-12-02-09:52:44-root-INFO: grad norm: 85.690 84.819 12.192
2024-12-02-09:52:44-root-INFO: Loss too large (805.617->818.574)! Learning rate decreased to 0.00566.
2024-12-02-09:52:44-root-INFO: Loss too large (805.617->809.774)! Learning rate decreased to 0.00453.
2024-12-02-09:52:44-root-INFO: grad norm: 62.868 62.054 10.081
2024-12-02-09:52:45-root-INFO: grad norm: 30.446 29.558 7.298
2024-12-02-09:52:45-root-INFO: grad norm: 27.055 26.013 7.434
2024-12-02-09:52:46-root-INFO: grad norm: 24.471 23.466 6.941
2024-12-02-09:52:46-root-INFO: grad norm: 23.328 22.236 7.054
2024-12-02-09:52:47-root-INFO: grad norm: 22.535 21.485 6.799
2024-12-02-09:52:47-root-INFO: Loss Change: 807.451 -> 783.583
2024-12-02-09:52:47-root-INFO: Regularization Change: 0.000 -> 0.816
2024-12-02-09:52:47-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-09:52:47-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-09:52:47-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-09:52:47-root-INFO: grad norm: 33.243 32.202 8.254
2024-12-02-09:52:48-root-INFO: grad norm: 69.109 68.402 9.865
2024-12-02-09:52:48-root-INFO: Loss too large (780.990->807.793)! Learning rate decreased to 0.00917.
2024-12-02-09:52:48-root-INFO: Loss too large (780.990->794.151)! Learning rate decreased to 0.00734.
2024-12-02-09:52:48-root-INFO: Loss too large (780.990->785.500)! Learning rate decreased to 0.00587.
2024-12-02-09:52:49-root-INFO: grad norm: 59.508 58.736 9.550
2024-12-02-09:52:49-root-INFO: grad norm: 44.771 44.105 7.693
2024-12-02-09:52:50-root-INFO: grad norm: 41.973 41.193 8.052
2024-12-02-09:52:50-root-INFO: grad norm: 37.324 36.647 7.073
2024-12-02-09:52:51-root-INFO: grad norm: 35.678 34.881 7.500
2024-12-02-09:52:51-root-INFO: grad norm: 33.185 32.495 6.735
2024-12-02-09:52:51-root-INFO: Loss Change: 783.138 -> 758.991
2024-12-02-09:52:51-root-INFO: Regularization Change: 0.000 -> 1.275
2024-12-02-09:52:51-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-09:52:51-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:52:52-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-09:52:52-root-INFO: grad norm: 45.516 44.746 8.334
2024-12-02-09:52:52-root-INFO: Loss too large (760.611->769.980)! Learning rate decreased to 0.00951.
2024-12-02-09:52:52-root-INFO: grad norm: 89.353 88.533 12.073
2024-12-02-09:52:53-root-INFO: Loss too large (760.527->783.609)! Learning rate decreased to 0.00761.
2024-12-02-09:52:53-root-INFO: Loss too large (760.527->770.080)! Learning rate decreased to 0.00608.
2024-12-02-09:52:53-root-INFO: Loss too large (760.527->761.510)! Learning rate decreased to 0.00487.
2024-12-02-09:52:53-root-INFO: grad norm: 51.072 50.389 8.326
2024-12-02-09:52:54-root-INFO: grad norm: 20.451 19.495 6.181
2024-12-02-09:52:54-root-INFO: grad norm: 20.099 19.133 6.155
2024-12-02-09:52:55-root-INFO: grad norm: 19.945 18.991 6.093
2024-12-02-09:52:55-root-INFO: grad norm: 19.825 18.876 6.060
2024-12-02-09:52:56-root-INFO: grad norm: 19.719 18.778 6.018
2024-12-02-09:52:56-root-INFO: Loss Change: 760.611 -> 738.948
2024-12-02-09:52:56-root-INFO: Regularization Change: 0.000 -> 0.816
2024-12-02-09:52:56-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-09:52:56-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:52:56-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-09:52:56-root-INFO: grad norm: 33.245 32.304 7.853
2024-12-02-09:52:57-root-INFO: grad norm: 51.409 50.809 7.833
2024-12-02-09:52:57-root-INFO: Loss too large (737.501->745.316)! Learning rate decreased to 0.00985.
2024-12-02-09:52:57-root-INFO: Loss too large (737.501->738.516)! Learning rate decreased to 0.00788.
2024-12-02-09:52:57-root-INFO: grad norm: 46.510 45.779 8.215
2024-12-02-09:52:58-root-INFO: grad norm: 44.476 43.883 7.237
2024-12-02-09:52:58-root-INFO: grad norm: 42.974 42.262 7.791
2024-12-02-09:52:59-root-INFO: grad norm: 40.876 40.297 6.859
2024-12-02-09:52:59-root-INFO: grad norm: 39.825 39.120 7.463
2024-12-02-09:53:00-root-INFO: grad norm: 37.999 37.429 6.559
2024-12-02-09:53:00-root-INFO: Loss Change: 740.549 -> 715.344
2024-12-02-09:53:00-root-INFO: Regularization Change: 0.000 -> 1.785
2024-12-02-09:53:00-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-09:53:00-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:53:00-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-09:53:00-root-INFO: grad norm: 46.404 45.694 8.087
2024-12-02-09:53:01-root-INFO: Loss too large (716.777->720.293)! Learning rate decreased to 0.01021.
2024-12-02-09:53:01-root-INFO: grad norm: 61.132 60.496 8.794
2024-12-02-09:53:01-root-INFO: Loss too large (713.078->717.718)! Learning rate decreased to 0.00816.
2024-12-02-09:53:02-root-INFO: grad norm: 51.157 50.489 8.239
2024-12-02-09:53:02-root-INFO: grad norm: 34.269 33.682 6.318
2024-12-02-09:53:03-root-INFO: grad norm: 30.543 29.803 6.683
2024-12-02-09:53:03-root-INFO: grad norm: 26.701 26.080 5.725
2024-12-02-09:53:04-root-INFO: grad norm: 24.929 24.151 6.181
2024-12-02-09:53:04-root-INFO: grad norm: 23.245 22.592 5.472
2024-12-02-09:53:04-root-INFO: Loss Change: 716.777 -> 688.486
2024-12-02-09:53:04-root-INFO: Regularization Change: 0.000 -> 1.625
2024-12-02-09:53:04-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-09:53:04-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-09:53:04-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-09:53:05-root-INFO: grad norm: 32.504 31.814 6.666
2024-12-02-09:53:05-root-INFO: grad norm: 52.262 51.677 7.797
2024-12-02-09:53:05-root-INFO: Loss too large (686.644->694.939)! Learning rate decreased to 0.01057.
2024-12-02-09:53:06-root-INFO: Loss too large (686.644->687.294)! Learning rate decreased to 0.00846.
2024-12-02-09:53:06-root-INFO: grad norm: 41.873 41.205 7.447
2024-12-02-09:53:07-root-INFO: grad norm: 30.627 30.050 5.917
2024-12-02-09:53:07-root-INFO: grad norm: 26.948 26.208 6.271
2024-12-02-09:53:07-root-INFO: grad norm: 24.169 23.555 5.417
2024-12-02-09:53:08-root-INFO: grad norm: 22.729 21.957 5.876
2024-12-02-09:53:08-root-INFO: grad norm: 21.760 21.125 5.217
2024-12-02-09:53:09-root-INFO: Loss Change: 688.203 -> 663.928
2024-12-02-09:53:09-root-INFO: Regularization Change: 0.000 -> 1.656
2024-12-02-09:53:09-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-09:53:09-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-09:53:09-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-09:53:09-root-INFO: grad norm: 35.946 35.176 7.400
2024-12-02-09:53:10-root-INFO: grad norm: 48.500 47.933 7.396
2024-12-02-09:53:10-root-INFO: Loss too large (664.387->668.754)! Learning rate decreased to 0.01095.
2024-12-02-09:53:10-root-INFO: grad norm: 54.745 53.948 9.306
2024-12-02-09:53:11-root-INFO: grad norm: 54.502 53.891 8.140
2024-12-02-09:53:11-root-INFO: grad norm: 49.838 49.155 8.226
2024-12-02-09:53:12-root-INFO: grad norm: 52.682 52.095 7.837
2024-12-02-09:53:12-root-INFO: grad norm: 56.810 56.056 9.222
2024-12-02-09:53:13-root-INFO: grad norm: 56.567 55.948 8.348
2024-12-02-09:53:13-root-INFO: Loss Change: 665.736 -> 645.673
2024-12-02-09:53:13-root-INFO: Regularization Change: 0.000 -> 2.367
2024-12-02-09:53:13-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-09:53:13-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-09:53:13-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-09:53:13-root-INFO: grad norm: 66.464 65.732 9.838
2024-12-02-09:53:13-root-INFO: Loss too large (648.521->656.594)! Learning rate decreased to 0.01134.
2024-12-02-09:53:14-root-INFO: grad norm: 59.211 58.603 8.464
2024-12-02-09:53:14-root-INFO: grad norm: 51.933 51.301 8.078
2024-12-02-09:53:15-root-INFO: grad norm: 48.988 48.444 7.281
2024-12-02-09:53:15-root-INFO: grad norm: 49.643 48.955 8.233
2024-12-02-09:53:16-root-INFO: grad norm: 47.101 46.541 7.242
2024-12-02-09:53:16-root-INFO: grad norm: 44.713 44.054 7.649
2024-12-02-09:53:17-root-INFO: grad norm: 43.787 43.237 6.921
2024-12-02-09:53:17-root-INFO: Loss Change: 648.521 -> 618.759
2024-12-02-09:53:17-root-INFO: Regularization Change: 0.000 -> 2.022
2024-12-02-09:53:17-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-09:53:17-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-09:53:17-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-09:53:17-root-INFO: grad norm: 60.813 60.097 9.304
2024-12-02-09:53:18-root-INFO: Loss too large (621.936->631.692)! Learning rate decreased to 0.01174.
2024-12-02-09:53:18-root-INFO: grad norm: 55.166 54.582 8.001
2024-12-02-09:53:19-root-INFO: grad norm: 45.310 44.763 7.019
2024-12-02-09:53:19-root-INFO: grad norm: 44.338 43.836 6.657
2024-12-02-09:53:20-root-INFO: grad norm: 48.005 47.355 7.871
2024-12-02-09:53:20-root-INFO: grad norm: 46.183 45.635 7.093
2024-12-02-09:53:21-root-INFO: grad norm: 43.515 42.901 7.287
2024-12-02-09:53:21-root-INFO: grad norm: 43.247 42.711 6.794
2024-12-02-09:53:21-root-INFO: Loss Change: 621.936 -> 597.059
2024-12-02-09:53:21-root-INFO: Regularization Change: 0.000 -> 1.932
2024-12-02-09:53:21-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-09:53:21-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:53:22-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-09:53:22-root-INFO: grad norm: 57.739 57.065 8.796
2024-12-02-09:53:22-root-INFO: Loss too large (600.367->609.723)! Learning rate decreased to 0.01215.
2024-12-02-09:53:22-root-INFO: grad norm: 53.481 52.930 7.657
2024-12-02-09:53:23-root-INFO: grad norm: 45.493 44.985 6.781
2024-12-02-09:53:23-root-INFO: grad norm: 44.581 44.093 6.580
2024-12-02-09:53:24-root-INFO: grad norm: 47.788 47.175 7.630
2024-12-02-09:53:24-root-INFO: grad norm: 45.926 45.392 6.985
2024-12-02-09:53:25-root-INFO: grad norm: 43.168 42.597 6.993
2024-12-02-09:53:25-root-INFO: grad norm: 42.994 42.472 6.682
2024-12-02-09:53:26-root-INFO: Loss Change: 600.367 -> 577.354
2024-12-02-09:53:26-root-INFO: Regularization Change: 0.000 -> 1.885
2024-12-02-09:53:26-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-09:53:26-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:53:26-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-09:53:26-root-INFO: grad norm: 55.936 55.312 8.333
2024-12-02-09:53:26-root-INFO: Loss too large (579.548->589.238)! Learning rate decreased to 0.01258.
2024-12-02-09:53:26-root-INFO: Loss too large (579.548->580.256)! Learning rate decreased to 0.01006.
2024-12-02-09:53:27-root-INFO: grad norm: 38.322 37.772 6.469
2024-12-02-09:53:27-root-INFO: grad norm: 23.076 22.547 4.911
2024-12-02-09:53:28-root-INFO: grad norm: 20.284 19.777 4.507
2024-12-02-09:53:28-root-INFO: grad norm: 19.130 18.543 4.703
2024-12-02-09:53:29-root-INFO: grad norm: 18.806 18.290 4.375
2024-12-02-09:53:29-root-INFO: grad norm: 18.985 18.389 4.717
2024-12-02-09:53:29-root-INFO: grad norm: 19.254 18.750 4.379
2024-12-02-09:53:30-root-INFO: Loss Change: 579.548 -> 557.285
2024-12-02-09:53:30-root-INFO: Regularization Change: 0.000 -> 1.265
2024-12-02-09:53:30-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-09:53:30-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:53:30-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-09:53:30-root-INFO: grad norm: 36.910 36.282 6.777
2024-12-02-09:53:30-root-INFO: Loss too large (558.942->561.613)! Learning rate decreased to 0.01302.
2024-12-02-09:53:31-root-INFO: grad norm: 38.514 37.932 6.666
2024-12-02-09:53:31-root-INFO: grad norm: 48.666 47.956 8.286
2024-12-02-09:53:31-root-INFO: Loss too large (554.989->558.660)! Learning rate decreased to 0.01041.
2024-12-02-09:53:32-root-INFO: grad norm: 37.426 36.854 6.517
2024-12-02-09:53:32-root-INFO: grad norm: 21.033 20.451 4.914
2024-12-02-09:53:33-root-INFO: grad norm: 19.774 19.267 4.445
2024-12-02-09:53:33-root-INFO: grad norm: 19.684 19.102 4.754
2024-12-02-09:53:34-root-INFO: grad norm: 19.865 19.367 4.421
2024-12-02-09:53:34-root-INFO: Loss Change: 558.942 -> 540.143
2024-12-02-09:53:34-root-INFO: Regularization Change: 0.000 -> 1.451
2024-12-02-09:53:34-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-09:53:34-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-09:53:34-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-09:53:34-root-INFO: grad norm: 29.028 28.493 5.548
2024-12-02-09:53:35-root-INFO: Loss too large (540.517->542.907)! Learning rate decreased to 0.01347.
2024-12-02-09:53:35-root-INFO: Loss too large (540.517->540.551)! Learning rate decreased to 0.01077.
2024-12-02-09:53:35-root-INFO: grad norm: 27.688 27.176 5.298
2024-12-02-09:53:36-root-INFO: grad norm: 27.433 26.860 5.576
2024-12-02-09:53:36-root-INFO: grad norm: 27.177 26.679 5.181
2024-12-02-09:53:37-root-INFO: grad norm: 26.931 26.352 5.550
2024-12-02-09:53:37-root-INFO: grad norm: 26.802 26.310 5.112
2024-12-02-09:53:38-root-INFO: grad norm: 26.648 26.071 5.516
2024-12-02-09:53:38-root-INFO: grad norm: 26.593 26.105 5.071
2024-12-02-09:53:38-root-INFO: Loss Change: 540.517 -> 526.113
2024-12-02-09:53:38-root-INFO: Regularization Change: 0.000 -> 1.210
2024-12-02-09:53:38-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-09:53:38-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-09:53:39-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-09:53:39-root-INFO: grad norm: 34.080 33.566 5.898
2024-12-02-09:53:39-root-INFO: Loss too large (526.650->531.380)! Learning rate decreased to 0.01393.
2024-12-02-09:53:39-root-INFO: Loss too large (526.650->527.659)! Learning rate decreased to 0.01115.
2024-12-02-09:53:39-root-INFO: grad norm: 31.419 30.879 5.804
2024-12-02-09:53:40-root-INFO: grad norm: 29.466 28.915 5.674
2024-12-02-09:53:40-root-INFO: grad norm: 28.516 28.009 5.352
2024-12-02-09:53:41-root-INFO: grad norm: 27.203 26.646 5.475
2024-12-02-09:53:41-root-INFO: grad norm: 26.846 26.355 5.109
2024-12-02-09:53:42-root-INFO: grad norm: 26.414 25.857 5.398
2024-12-02-09:53:42-root-INFO: grad norm: 26.280 25.796 5.022
2024-12-02-09:53:43-root-INFO: Loss Change: 526.650 -> 511.688
2024-12-02-09:53:43-root-INFO: Regularization Change: 0.000 -> 1.229
2024-12-02-09:53:43-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-09:53:43-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-09:53:43-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-09:53:43-root-INFO: grad norm: 36.693 36.139 6.355
2024-12-02-09:53:43-root-INFO: Loss too large (512.291->518.483)! Learning rate decreased to 0.01441.
2024-12-02-09:53:43-root-INFO: Loss too large (512.291->514.032)! Learning rate decreased to 0.01153.
2024-12-02-09:53:44-root-INFO: grad norm: 32.549 32.024 5.826
2024-12-02-09:53:44-root-INFO: grad norm: 27.999 27.465 5.445
2024-12-02-09:53:45-root-INFO: grad norm: 27.256 26.772 5.112
2024-12-02-09:53:45-root-INFO: grad norm: 26.894 26.349 5.388
2024-12-02-09:53:46-root-INFO: grad norm: 26.576 26.091 5.050
2024-12-02-09:53:46-root-INFO: grad norm: 26.241 25.698 5.308
2024-12-02-09:53:47-root-INFO: grad norm: 26.134 25.653 4.988
2024-12-02-09:53:47-root-INFO: Loss Change: 512.291 -> 497.350
2024-12-02-09:53:47-root-INFO: Regularization Change: 0.000 -> 1.226
2024-12-02-09:53:47-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-09:53:47-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-09:53:47-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-09:53:47-root-INFO: grad norm: 35.444 34.938 5.967
2024-12-02-09:53:47-root-INFO: Loss too large (498.519->504.885)! Learning rate decreased to 0.01490.
2024-12-02-09:53:48-root-INFO: Loss too large (498.519->500.315)! Learning rate decreased to 0.01192.
2024-12-02-09:53:48-root-INFO: grad norm: 32.445 31.914 5.847
2024-12-02-09:53:48-root-INFO: grad norm: 29.946 29.428 5.543
2024-12-02-09:53:49-root-INFO: grad norm: 28.952 28.452 5.357
2024-12-02-09:53:49-root-INFO: grad norm: 27.778 27.255 5.363
2024-12-02-09:53:50-root-INFO: grad norm: 27.368 26.880 5.149
2024-12-02-09:53:50-root-INFO: grad norm: 26.879 26.357 5.270
2024-12-02-09:53:51-root-INFO: grad norm: 26.705 26.221 5.059
2024-12-02-09:53:51-root-INFO: Loss Change: 498.519 -> 484.197
2024-12-02-09:53:51-root-INFO: Regularization Change: 0.000 -> 1.227
2024-12-02-09:53:51-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-09:53:51-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-09:53:51-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-09:53:51-root-INFO: grad norm: 34.578 34.052 6.008
2024-12-02-09:53:52-root-INFO: Loss too large (484.755->491.329)! Learning rate decreased to 0.01541.
2024-12-02-09:53:52-root-INFO: Loss too large (484.755->487.019)! Learning rate decreased to 0.01233.
2024-12-02-09:53:52-root-INFO: grad norm: 31.609 31.072 5.803
2024-12-02-09:53:53-root-INFO: grad norm: 27.647 27.148 5.230
2024-12-02-09:53:53-root-INFO: grad norm: 27.142 26.653 5.129
2024-12-02-09:53:54-root-INFO: grad norm: 26.980 26.473 5.203
2024-12-02-09:53:54-root-INFO: grad norm: 26.637 26.149 5.079
2024-12-02-09:53:54-root-INFO: grad norm: 26.147 25.646 5.094
2024-12-02-09:53:55-root-INFO: grad norm: 26.005 25.522 4.988
2024-12-02-09:53:55-root-INFO: Loss Change: 484.755 -> 471.239
2024-12-02-09:53:55-root-INFO: Regularization Change: 0.000 -> 1.205
2024-12-02-09:53:55-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-09:53:55-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-09:53:55-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-09:53:56-root-INFO: grad norm: 36.554 36.020 6.223
2024-12-02-09:53:56-root-INFO: Loss too large (472.744->480.206)! Learning rate decreased to 0.01593.
2024-12-02-09:53:56-root-INFO: Loss too large (472.744->475.293)! Learning rate decreased to 0.01275.
2024-12-02-09:53:56-root-INFO: grad norm: 32.255 31.716 5.870
2024-12-02-09:53:57-root-INFO: grad norm: 26.263 25.794 4.941
2024-12-02-09:53:57-root-INFO: grad norm: 25.611 25.142 4.883
2024-12-02-09:53:58-root-INFO: grad norm: 25.825 25.346 4.948
2024-12-02-09:53:58-root-INFO: grad norm: 25.452 24.975 4.906
2024-12-02-09:53:59-root-INFO: grad norm: 25.018 24.544 4.846
2024-12-02-09:53:59-root-INFO: grad norm: 24.850 24.378 4.821
2024-12-02-09:54:00-root-INFO: Loss Change: 472.744 -> 458.828
2024-12-02-09:54:00-root-INFO: Regularization Change: 0.000 -> 1.226
2024-12-02-09:54:00-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-09:54:00-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-09:54:00-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-09:54:00-root-INFO: grad norm: 33.162 32.676 5.655
2024-12-02-09:54:00-root-INFO: Loss too large (459.692->466.474)! Learning rate decreased to 0.01647.
2024-12-02-09:54:00-root-INFO: Loss too large (459.692->462.174)! Learning rate decreased to 0.01318.
2024-12-02-09:54:01-root-INFO: grad norm: 30.303 29.764 5.690
2024-12-02-09:54:01-root-INFO: grad norm: 26.178 25.728 4.831
2024-12-02-09:54:02-root-INFO: grad norm: 25.838 25.359 4.955
2024-12-02-09:54:02-root-INFO: grad norm: 26.039 25.576 4.889
2024-12-02-09:54:03-root-INFO: grad norm: 25.658 25.176 4.951
2024-12-02-09:54:03-root-INFO: grad norm: 25.038 24.580 4.770
2024-12-02-09:54:03-root-INFO: grad norm: 24.845 24.371 4.831
2024-12-02-09:54:04-root-INFO: Loss Change: 459.692 -> 447.251
2024-12-02-09:54:04-root-INFO: Regularization Change: 0.000 -> 1.178
2024-12-02-09:54:04-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-09:54:04-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-09:54:04-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-09:54:04-root-INFO: grad norm: 34.953 34.469 5.796
2024-12-02-09:54:04-root-INFO: Loss too large (448.996->456.525)! Learning rate decreased to 0.01702.
2024-12-02-09:54:04-root-INFO: Loss too large (448.996->451.403)! Learning rate decreased to 0.01362.
2024-12-02-09:54:05-root-INFO: grad norm: 30.759 30.247 5.590
2024-12-02-09:54:05-root-INFO: grad norm: 25.674 25.246 4.667
2024-12-02-09:54:06-root-INFO: grad norm: 24.786 24.332 4.724
2024-12-02-09:54:06-root-INFO: grad norm: 24.566 24.130 4.607
2024-12-02-09:54:07-root-INFO: grad norm: 24.051 23.591 4.681
2024-12-02-09:54:07-root-INFO: grad norm: 23.434 23.003 4.476
2024-12-02-09:54:08-root-INFO: grad norm: 23.167 22.713 4.565
2024-12-02-09:54:08-root-INFO: Loss Change: 448.996 -> 436.004
2024-12-02-09:54:08-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-02-09:54:08-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-09:54:08-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-09:54:08-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-09:54:08-root-INFO: grad norm: 30.705 30.268 5.159
2024-12-02-09:54:09-root-INFO: Loss too large (436.710->442.276)! Learning rate decreased to 0.01759.
2024-12-02-09:54:09-root-INFO: Loss too large (436.710->438.078)! Learning rate decreased to 0.01407.
2024-12-02-09:54:09-root-INFO: grad norm: 27.630 27.135 5.208
2024-12-02-09:54:10-root-INFO: grad norm: 25.202 24.791 4.532
2024-12-02-09:54:10-root-INFO: grad norm: 24.268 23.802 4.729
2024-12-02-09:54:11-root-INFO: grad norm: 23.385 22.974 4.363
2024-12-02-09:54:11-root-INFO: grad norm: 22.909 22.453 4.550
2024-12-02-09:54:11-root-INFO: grad norm: 22.361 21.953 4.249
2024-12-02-09:54:12-root-INFO: grad norm: 22.090 21.640 4.436
2024-12-02-09:54:12-root-INFO: Loss Change: 436.710 -> 424.714
2024-12-02-09:54:12-root-INFO: Regularization Change: 0.000 -> 1.177
2024-12-02-09:54:12-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-09:54:12-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-09:54:12-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-09:54:13-root-INFO: grad norm: 31.366 30.899 5.388
2024-12-02-09:54:13-root-INFO: Loss too large (425.814->432.268)! Learning rate decreased to 0.01817.
2024-12-02-09:54:13-root-INFO: Loss too large (425.814->428.147)! Learning rate decreased to 0.01454.
2024-12-02-09:54:13-root-INFO: grad norm: 27.415 26.909 5.242
2024-12-02-09:54:14-root-INFO: grad norm: 21.400 21.023 3.996
2024-12-02-09:54:14-root-INFO: grad norm: 20.768 20.342 4.183
2024-12-02-09:54:15-root-INFO: grad norm: 20.802 20.421 3.962
2024-12-02-09:54:15-root-INFO: grad norm: 20.531 20.095 4.208
2024-12-02-09:54:16-root-INFO: grad norm: 20.312 19.932 3.911
2024-12-02-09:54:16-root-INFO: grad norm: 20.126 19.690 4.163
2024-12-02-09:54:17-root-INFO: Loss Change: 425.814 -> 414.404
2024-12-02-09:54:17-root-INFO: Regularization Change: 0.000 -> 1.133
2024-12-02-09:54:17-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-09:54:17-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-09:54:17-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-09:54:17-root-INFO: grad norm: 23.877 23.499 4.235
2024-12-02-09:54:17-root-INFO: Loss too large (414.732->417.998)! Learning rate decreased to 0.01877.
2024-12-02-09:54:17-root-INFO: Loss too large (414.732->415.343)! Learning rate decreased to 0.01502.
2024-12-02-09:54:18-root-INFO: grad norm: 22.384 21.933 4.469
2024-12-02-09:54:18-root-INFO: grad norm: 21.577 21.202 4.004
2024-12-02-09:54:19-root-INFO: grad norm: 21.005 20.565 4.280
2024-12-02-09:54:19-root-INFO: grad norm: 20.286 19.915 3.866
2024-12-02-09:54:20-root-INFO: grad norm: 19.964 19.532 4.130
2024-12-02-09:54:20-root-INFO: grad norm: 19.541 19.172 3.779
2024-12-02-09:54:21-root-INFO: grad norm: 19.336 18.909 4.042
2024-12-02-09:54:21-root-INFO: Loss Change: 414.732 -> 404.783
2024-12-02-09:54:21-root-INFO: Regularization Change: 0.000 -> 1.109
2024-12-02-09:54:21-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-09:54:21-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-09:54:21-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-09:54:21-root-INFO: grad norm: 25.173 24.805 4.290
2024-12-02-09:54:21-root-INFO: Loss too large (405.365->409.162)! Learning rate decreased to 0.01939.
2024-12-02-09:54:22-root-INFO: Loss too large (405.365->405.985)! Learning rate decreased to 0.01551.
2024-12-02-09:54:22-root-INFO: grad norm: 22.963 22.489 4.642
2024-12-02-09:54:22-root-INFO: grad norm: 21.839 21.478 3.955
2024-12-02-09:54:23-root-INFO: grad norm: 20.917 20.459 4.349
2024-12-02-09:54:23-root-INFO: grad norm: 19.808 19.449 3.754
2024-12-02-09:54:24-root-INFO: grad norm: 19.319 18.879 4.100
2024-12-02-09:54:24-root-INFO: grad norm: 18.717 18.360 3.636
2024-12-02-09:54:25-root-INFO: grad norm: 18.433 18.002 3.961
2024-12-02-09:54:25-root-INFO: Loss Change: 405.365 -> 395.156
2024-12-02-09:54:25-root-INFO: Regularization Change: 0.000 -> 1.123
2024-12-02-09:54:25-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-09:54:25-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-09:54:25-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-09:54:25-root-INFO: grad norm: 30.644 30.211 5.135
2024-12-02-09:54:26-root-INFO: Loss too large (397.379->403.436)! Learning rate decreased to 0.02013.
2024-12-02-09:54:26-root-INFO: Loss too large (397.379->398.658)! Learning rate decreased to 0.01610.
2024-12-02-09:54:26-root-INFO: grad norm: 26.031 25.521 5.129
2024-12-02-09:54:27-root-INFO: grad norm: 21.884 21.527 3.938
2024-12-02-09:54:27-root-INFO: grad norm: 20.440 19.988 4.275
2024-12-02-09:54:28-root-INFO: grad norm: 19.156 18.806 3.645
2024-12-02-09:54:28-root-INFO: grad norm: 18.480 18.040 4.008
2024-12-02-09:54:29-root-INFO: grad norm: 17.764 17.419 3.488
2024-12-02-09:54:29-root-INFO: grad norm: 17.401 16.970 3.849
2024-12-02-09:54:29-root-INFO: Loss Change: 397.379 -> 385.710
2024-12-02-09:54:29-root-INFO: Regularization Change: 0.000 -> 1.184
2024-12-02-09:54:29-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-09:54:29-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-09:54:30-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-09:54:30-root-INFO: grad norm: 23.010 22.646 4.078
2024-12-02-09:54:30-root-INFO: Loss too large (386.432->390.012)! Learning rate decreased to 0.02078.
2024-12-02-09:54:30-root-INFO: Loss too large (386.432->387.178)! Learning rate decreased to 0.01662.
2024-12-02-09:54:30-root-INFO: grad norm: 21.256 20.783 4.462
2024-12-02-09:54:31-root-INFO: grad norm: 19.893 19.541 3.723
2024-12-02-09:54:31-root-INFO: grad norm: 19.132 18.672 4.170
2024-12-02-09:54:32-root-INFO: grad norm: 18.246 17.900 3.534
2024-12-02-09:54:32-root-INFO: grad norm: 17.811 17.363 3.972
2024-12-02-09:54:33-root-INFO: grad norm: 17.259 16.919 3.413
2024-12-02-09:54:33-root-INFO: grad norm: 16.981 16.541 3.842
2024-12-02-09:54:34-root-INFO: Loss Change: 386.432 -> 377.083
2024-12-02-09:54:34-root-INFO: Regularization Change: 0.000 -> 1.117
2024-12-02-09:54:34-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-09:54:34-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-09:54:34-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-09:54:34-root-INFO: grad norm: 23.618 23.241 4.202
2024-12-02-09:54:34-root-INFO: Loss too large (378.017->381.773)! Learning rate decreased to 0.02145.
2024-12-02-09:54:34-root-INFO: Loss too large (378.017->378.795)! Learning rate decreased to 0.01716.
2024-12-02-09:54:35-root-INFO: grad norm: 21.096 20.602 4.539
2024-12-02-09:54:35-root-INFO: grad norm: 18.659 18.317 3.557
2024-12-02-09:54:36-root-INFO: grad norm: 17.663 17.213 3.962
2024-12-02-09:54:36-root-INFO: grad norm: 16.540 16.210 3.285
2024-12-02-09:54:37-root-INFO: grad norm: 15.993 15.564 3.683
2024-12-02-09:54:37-root-INFO: grad norm: 15.347 15.025 3.128
2024-12-02-09:54:38-root-INFO: grad norm: 15.008 14.591 3.514
2024-12-02-09:54:38-root-INFO: Loss Change: 378.017 -> 368.500
2024-12-02-09:54:38-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-02-09:54:38-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-09:54:38-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-09:54:38-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-09:54:38-root-INFO: grad norm: 20.842 20.490 3.816
2024-12-02-09:54:39-root-INFO: Loss too large (368.997->371.803)! Learning rate decreased to 0.02214.
2024-12-02-09:54:39-root-INFO: Loss too large (368.997->369.357)! Learning rate decreased to 0.01771.
2024-12-02-09:54:39-root-INFO: grad norm: 19.153 18.672 4.269
2024-12-02-09:54:40-root-INFO: grad norm: 18.039 17.698 3.489
2024-12-02-09:54:40-root-INFO: grad norm: 17.322 16.853 4.003
2024-12-02-09:54:41-root-INFO: grad norm: 16.477 16.145 3.294
2024-12-02-09:54:41-root-INFO: grad norm: 16.026 15.574 3.779
2024-12-02-09:54:42-root-INFO: grad norm: 15.451 15.125 3.159
2024-12-02-09:54:42-root-INFO: grad norm: 15.137 14.698 3.619
2024-12-02-09:54:42-root-INFO: Loss Change: 368.997 -> 360.186
2024-12-02-09:54:42-root-INFO: Regularization Change: 0.000 -> 1.121
2024-12-02-09:54:42-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-09:54:42-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-09:54:43-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-09:54:43-root-INFO: grad norm: 20.410 20.057 3.782
2024-12-02-09:54:43-root-INFO: Loss too large (361.060->363.784)! Learning rate decreased to 0.02285.
2024-12-02-09:54:43-root-INFO: Loss too large (361.060->361.352)! Learning rate decreased to 0.01828.
2024-12-02-09:54:44-root-INFO: grad norm: 18.640 18.158 4.212
2024-12-02-09:54:44-root-INFO: grad norm: 17.587 17.242 3.464
2024-12-02-09:54:45-root-INFO: grad norm: 16.883 16.406 3.984
2024-12-02-09:54:45-root-INFO: grad norm: 16.053 15.718 3.264
2024-12-02-09:54:46-root-INFO: grad norm: 15.592 15.132 3.760
2024-12-02-09:54:46-root-INFO: grad norm: 14.999 14.671 3.119
2024-12-02-09:54:47-root-INFO: grad norm: 14.664 14.219 3.589
2024-12-02-09:54:47-root-INFO: Loss Change: 361.060 -> 352.389
2024-12-02-09:54:47-root-INFO: Regularization Change: 0.000 -> 1.133
2024-12-02-09:54:47-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-09:54:47-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-09:54:47-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-09:54:47-root-INFO: grad norm: 20.826 20.451 3.933
2024-12-02-09:54:47-root-INFO: Loss too large (353.063->355.555)! Learning rate decreased to 0.02358.
2024-12-02-09:54:48-root-INFO: grad norm: 24.074 23.492 5.263
2024-12-02-09:54:48-root-INFO: grad norm: 30.355 29.866 5.430
2024-12-02-09:54:48-root-INFO: Loss too large (352.538->354.467)! Learning rate decreased to 0.01886.
2024-12-02-09:54:49-root-INFO: grad norm: 23.539 22.896 5.462
2024-12-02-09:54:49-root-INFO: grad norm: 16.255 15.939 3.191
2024-12-02-09:54:50-root-INFO: grad norm: 14.415 13.977 3.529
2024-12-02-09:54:50-root-INFO: grad norm: 13.017 12.716 2.783
2024-12-02-09:54:51-root-INFO: grad norm: 12.215 11.806 3.135
2024-12-02-09:54:51-root-INFO: Loss Change: 353.063 -> 343.434
2024-12-02-09:54:51-root-INFO: Regularization Change: 0.000 -> 1.323
2024-12-02-09:54:51-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-09:54:51-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-09:54:51-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-09:54:52-root-INFO: grad norm: 16.084 15.742 3.297
2024-12-02-09:54:52-root-INFO: Loss too large (344.157->345.356)! Learning rate decreased to 0.02432.
2024-12-02-09:54:52-root-INFO: grad norm: 19.186 18.639 4.549
2024-12-02-09:54:53-root-INFO: grad norm: 24.550 24.085 4.751
2024-12-02-09:54:53-root-INFO: Loss too large (343.399->344.511)! Learning rate decreased to 0.01946.
2024-12-02-09:54:53-root-INFO: grad norm: 19.973 19.371 4.864
2024-12-02-09:54:54-root-INFO: grad norm: 15.080 14.742 3.176
2024-12-02-09:54:54-root-INFO: grad norm: 13.412 12.975 3.397
2024-12-02-09:54:55-root-INFO: grad norm: 11.871 11.566 2.672
2024-12-02-09:54:55-root-INFO: grad norm: 10.978 10.590 2.890
2024-12-02-09:54:56-root-INFO: Loss Change: 344.157 -> 335.843
2024-12-02-09:54:56-root-INFO: Regularization Change: 0.000 -> 1.262
2024-12-02-09:54:56-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-09:54:56-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-09:54:56-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-09:54:56-root-INFO: grad norm: 16.612 16.290 3.257
2024-12-02-09:54:56-root-INFO: Loss too large (336.599->337.155)! Learning rate decreased to 0.02509.
2024-12-02-09:54:57-root-INFO: grad norm: 18.414 17.984 3.955
2024-12-02-09:54:57-root-INFO: grad norm: 22.731 22.310 4.354
2024-12-02-09:54:57-root-INFO: Loss too large (335.255->335.817)! Learning rate decreased to 0.02007.
2024-12-02-09:54:58-root-INFO: grad norm: 18.422 17.879 4.441
2024-12-02-09:54:58-root-INFO: grad norm: 14.658 14.320 3.131
2024-12-02-09:54:59-root-INFO: grad norm: 12.814 12.385 3.286
2024-12-02-09:54:59-root-INFO: grad norm: 11.213 10.910 2.590
2024-12-02-09:54:59-root-INFO: grad norm: 10.236 9.860 2.751
2024-12-02-09:55:00-root-INFO: Loss Change: 336.599 -> 327.944
2024-12-02-09:55:00-root-INFO: Regularization Change: 0.000 -> 1.298
2024-12-02-09:55:00-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-09:55:00-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-09:55:00-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-09:55:00-root-INFO: grad norm: 14.066 13.754 2.945
2024-12-02-09:55:00-root-INFO: Loss too large (328.162->328.370)! Learning rate decreased to 0.02587.
2024-12-02-09:55:01-root-INFO: grad norm: 15.997 15.535 3.819
2024-12-02-09:55:01-root-INFO: grad norm: 19.886 19.477 4.011
2024-12-02-09:55:01-root-INFO: Loss too large (326.848->327.152)! Learning rate decreased to 0.02070.
2024-12-02-09:55:02-root-INFO: grad norm: 16.448 15.920 4.136
2024-12-02-09:55:02-root-INFO: grad norm: 13.338 13.008 2.950
2024-12-02-09:55:03-root-INFO: grad norm: 11.716 11.300 3.092
2024-12-02-09:55:03-root-INFO: grad norm: 10.284 9.989 2.447
2024-12-02-09:55:04-root-INFO: grad norm: 9.383 9.021 2.580
2024-12-02-09:55:04-root-INFO: Loss Change: 328.162 -> 320.241
2024-12-02-09:55:04-root-INFO: Regularization Change: 0.000 -> 1.275
2024-12-02-09:55:04-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-09:55:04-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-09:55:04-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-09:55:04-root-INFO: grad norm: 13.903 13.598 2.897
2024-12-02-09:55:04-root-INFO: Loss too large (321.062->321.245)! Learning rate decreased to 0.02668.
2024-12-02-09:55:05-root-INFO: grad norm: 15.511 15.099 3.552
2024-12-02-09:55:05-root-INFO: grad norm: 18.812 18.424 3.799
2024-12-02-09:55:05-root-INFO: Loss too large (319.677->319.784)! Learning rate decreased to 0.02134.
2024-12-02-09:55:06-root-INFO: grad norm: 15.359 14.876 3.824
2024-12-02-09:55:06-root-INFO: grad norm: 12.469 12.147 2.814
2024-12-02-09:55:07-root-INFO: grad norm: 10.849 10.460 2.879
2024-12-02-09:55:07-root-INFO: grad norm: 9.493 9.203 2.326
2024-12-02-09:55:08-root-INFO: grad norm: 8.624 8.282 2.403
2024-12-02-09:55:08-root-INFO: Loss Change: 321.062 -> 313.299
2024-12-02-09:55:08-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-02-09:55:08-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-09:55:08-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-09:55:08-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-09:55:08-root-INFO: grad norm: 11.872 11.604 2.509
2024-12-02-09:55:09-root-INFO: grad norm: 17.238 16.826 3.747
2024-12-02-09:55:09-root-INFO: Loss too large (313.303->315.080)! Learning rate decreased to 0.02751.
2024-12-02-09:55:09-root-INFO: grad norm: 19.674 19.311 3.763
2024-12-02-09:55:10-root-INFO: grad norm: 21.451 20.880 4.918
2024-12-02-09:55:10-root-INFO: grad norm: 23.341 22.910 4.466
2024-12-02-09:55:11-root-INFO: grad norm: 24.090 23.439 5.559
2024-12-02-09:55:11-root-INFO: grad norm: 24.475 24.032 4.635
2024-12-02-09:55:12-root-INFO: grad norm: 24.570 23.930 5.571
2024-12-02-09:55:12-root-INFO: Loss Change: 313.653 -> 308.690
2024-12-02-09:55:12-root-INFO: Regularization Change: 0.000 -> 1.880
2024-12-02-09:55:12-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-09:55:12-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-09:55:12-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-09:55:12-root-INFO: grad norm: 29.774 29.298 5.303
2024-12-02-09:55:12-root-INFO: Loss too large (310.434->316.147)! Learning rate decreased to 0.02836.
2024-12-02-09:55:13-root-INFO: grad norm: 28.503 27.890 5.882
2024-12-02-09:55:13-root-INFO: grad norm: 26.878 26.460 4.726
2024-12-02-09:55:14-root-INFO: grad norm: 25.788 25.233 5.325
2024-12-02-09:55:14-root-INFO: grad norm: 24.605 24.209 4.398
2024-12-02-09:55:15-root-INFO: grad norm: 23.762 23.246 4.924
2024-12-02-09:55:15-root-INFO: grad norm: 22.893 22.514 4.150
2024-12-02-09:55:16-root-INFO: grad norm: 22.250 21.767 4.611
2024-12-02-09:55:16-root-INFO: Loss Change: 310.434 -> 300.214
2024-12-02-09:55:16-root-INFO: Regularization Change: 0.000 -> 1.715
2024-12-02-09:55:16-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-09:55:16-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-09:55:16-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-09:55:16-root-INFO: grad norm: 28.077 27.680 4.707
2024-12-02-09:55:16-root-INFO: Loss too large (302.346->306.913)! Learning rate decreased to 0.02923.
2024-12-02-09:55:17-root-INFO: grad norm: 26.367 25.899 4.942
2024-12-02-09:55:17-root-INFO: grad norm: 24.713 24.359 4.168
2024-12-02-09:55:18-root-INFO: grad norm: 23.473 23.042 4.481
2024-12-02-09:55:18-root-INFO: grad norm: 22.315 21.984 3.831
2024-12-02-09:55:19-root-INFO: grad norm: 21.454 21.057 4.107
2024-12-02-09:55:19-root-INFO: grad norm: 20.665 20.353 3.578
2024-12-02-09:55:20-root-INFO: grad norm: 20.066 19.698 3.827
2024-12-02-09:55:20-root-INFO: Loss Change: 302.346 -> 292.213
2024-12-02-09:55:20-root-INFO: Regularization Change: 0.000 -> 1.686
2024-12-02-09:55:20-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-09:55:20-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-09:55:20-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-09:55:21-root-INFO: grad norm: 24.298 23.953 4.077
2024-12-02-09:55:21-root-INFO: Loss too large (293.664->296.894)! Learning rate decreased to 0.03012.
2024-12-02-09:55:21-root-INFO: grad norm: 22.761 22.378 4.158
2024-12-02-09:55:22-root-INFO: grad norm: 21.342 21.037 3.593
2024-12-02-09:55:22-root-INFO: grad norm: 20.258 19.904 3.767
2024-12-02-09:55:23-root-INFO: grad norm: 19.290 19.009 3.281
2024-12-02-09:55:23-root-INFO: grad norm: 18.551 18.228 3.448
2024-12-02-09:55:24-root-INFO: grad norm: 17.898 17.637 3.047
2024-12-02-09:55:24-root-INFO: grad norm: 17.394 17.096 3.205
2024-12-02-09:55:24-root-INFO: Loss Change: 293.664 -> 284.579
2024-12-02-09:55:24-root-INFO: Regularization Change: 0.000 -> 1.639
2024-12-02-09:55:24-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-09:55:24-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-09:55:25-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-09:55:25-root-INFO: grad norm: 21.728 21.441 3.522
2024-12-02-09:55:25-root-INFO: Loss too large (285.914->288.569)! Learning rate decreased to 0.03103.
2024-12-02-09:55:25-root-INFO: grad norm: 20.825 20.540 3.435
2024-12-02-09:55:26-root-INFO: grad norm: 19.977 19.731 3.129
2024-12-02-09:55:26-root-INFO: grad norm: 19.252 18.984 3.203
2024-12-02-09:55:27-root-INFO: grad norm: 18.569 18.340 2.905
2024-12-02-09:55:27-root-INFO: grad norm: 18.015 17.765 2.990
2024-12-02-09:55:28-root-INFO: grad norm: 17.510 17.296 2.730
2024-12-02-09:55:28-root-INFO: grad norm: 17.106 16.872 2.821
2024-12-02-09:55:29-root-INFO: Loss Change: 285.914 -> 277.932
2024-12-02-09:55:29-root-INFO: Regularization Change: 0.000 -> 1.618
2024-12-02-09:55:29-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-09:55:29-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-09:55:29-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-09:55:29-root-INFO: grad norm: 21.846 21.594 3.305
2024-12-02-09:55:29-root-INFO: Loss too large (279.628->282.259)! Learning rate decreased to 0.03197.
2024-12-02-09:55:30-root-INFO: grad norm: 20.648 20.410 3.129
2024-12-02-09:55:30-root-INFO: grad norm: 19.600 19.384 2.902
2024-12-02-09:55:30-root-INFO: grad norm: 18.770 18.532 2.980
2024-12-02-09:55:31-root-INFO: grad norm: 18.018 17.812 2.718
2024-12-02-09:55:31-root-INFO: grad norm: 17.427 17.201 2.799
2024-12-02-09:55:32-root-INFO: grad norm: 16.902 16.706 2.566
2024-12-02-09:55:32-root-INFO: grad norm: 16.486 16.273 2.646
2024-12-02-09:55:33-root-INFO: Loss Change: 279.628 -> 271.542
2024-12-02-09:55:33-root-INFO: Regularization Change: 0.000 -> 1.623
2024-12-02-09:55:33-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-09:55:33-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-09:55:33-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-09:55:33-root-INFO: grad norm: 19.799 19.581 2.931
2024-12-02-09:55:33-root-INFO: Loss too large (272.336->274.376)! Learning rate decreased to 0.03292.
2024-12-02-09:55:34-root-INFO: grad norm: 18.655 18.450 2.759
2024-12-02-09:55:34-root-INFO: grad norm: 17.700 17.513 2.565
2024-12-02-09:55:35-root-INFO: grad norm: 16.940 16.744 2.574
2024-12-02-09:55:35-root-INFO: grad norm: 16.288 16.113 2.386
2024-12-02-09:55:36-root-INFO: grad norm: 15.775 15.589 2.412
2024-12-02-09:55:36-root-INFO: grad norm: 15.337 15.171 2.252
2024-12-02-09:55:37-root-INFO: grad norm: 14.994 14.819 2.287
2024-12-02-09:55:37-root-INFO: Loss Change: 272.336 -> 264.769
2024-12-02-09:55:37-root-INFO: Regularization Change: 0.000 -> 1.603
2024-12-02-09:55:37-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-09:55:37-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-09:55:37-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-09:55:37-root-INFO: grad norm: 20.825 20.629 2.853
2024-12-02-09:55:37-root-INFO: Loss too large (266.667->269.162)! Learning rate decreased to 0.03391.
2024-12-02-09:55:38-root-INFO: grad norm: 19.746 19.559 2.713
2024-12-02-09:55:38-root-INFO: grad norm: 18.768 18.589 2.587
2024-12-02-09:55:39-root-INFO: grad norm: 17.990 17.791 2.670
2024-12-02-09:55:39-root-INFO: grad norm: 17.286 17.107 2.481
2024-12-02-09:55:40-root-INFO: grad norm: 16.728 16.532 2.554
2024-12-02-09:55:40-root-INFO: grad norm: 16.242 16.067 2.373
2024-12-02-09:55:41-root-INFO: grad norm: 15.853 15.664 2.440
2024-12-02-09:55:41-root-INFO: Loss Change: 266.667 -> 259.039
2024-12-02-09:55:41-root-INFO: Regularization Change: 0.000 -> 1.629
2024-12-02-09:55:41-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-09:55:41-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-09:55:41-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-09:55:41-root-INFO: grad norm: 20.622 20.374 3.189
2024-12-02-09:55:41-root-INFO: Loss too large (260.467->262.900)! Learning rate decreased to 0.03491.
2024-12-02-09:55:42-root-INFO: grad norm: 19.395 19.183 2.856
2024-12-02-09:55:42-root-INFO: grad norm: 18.353 18.157 2.672
2024-12-02-09:55:43-root-INFO: grad norm: 17.476 17.283 2.588
2024-12-02-09:55:43-root-INFO: grad norm: 16.734 16.561 2.398
2024-12-02-09:55:44-root-INFO: grad norm: 16.126 15.952 2.364
2024-12-02-09:55:44-root-INFO: grad norm: 15.621 15.465 2.201
2024-12-02-09:55:45-root-INFO: grad norm: 15.210 15.050 2.197
2024-12-02-09:55:45-root-INFO: Loss Change: 260.467 -> 252.790
2024-12-02-09:55:45-root-INFO: Regularization Change: 0.000 -> 1.635
2024-12-02-09:55:45-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-09:55:45-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-09:55:45-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-09:55:46-root-INFO: grad norm: 19.166 18.976 2.691
2024-12-02-09:55:46-root-INFO: Loss too large (254.283->256.551)! Learning rate decreased to 0.03594.
2024-12-02-09:55:46-root-INFO: grad norm: 18.356 18.198 2.405
2024-12-02-09:55:47-root-INFO: grad norm: 17.565 17.413 2.304
2024-12-02-09:55:47-root-INFO: grad norm: 16.865 16.713 2.263
2024-12-02-09:55:48-root-INFO: grad norm: 16.257 16.117 2.127
2024-12-02-09:55:48-root-INFO: grad norm: 15.747 15.604 2.114
2024-12-02-09:55:48-root-INFO: grad norm: 15.321 15.191 1.991
2024-12-02-09:55:49-root-INFO: grad norm: 14.969 14.836 1.996
2024-12-02-09:55:49-root-INFO: Loss Change: 254.283 -> 247.347
2024-12-02-09:55:49-root-INFO: Regularization Change: 0.000 -> 1.604
2024-12-02-09:55:49-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-09:55:49-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-09:55:49-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-09:55:50-root-INFO: grad norm: 19.333 19.153 2.632
2024-12-02-09:55:50-root-INFO: Loss too large (248.931->250.904)! Learning rate decreased to 0.03700.
2024-12-02-09:55:50-root-INFO: grad norm: 17.875 17.726 2.309
2024-12-02-09:55:51-root-INFO: grad norm: 16.719 16.577 2.176
2024-12-02-09:55:51-root-INFO: grad norm: 15.773 15.632 2.105
2024-12-02-09:55:52-root-INFO: grad norm: 15.048 14.918 1.970
2024-12-02-09:55:52-root-INFO: grad norm: 14.450 14.321 1.927
2024-12-02-09:55:53-root-INFO: grad norm: 13.999 13.880 1.821
2024-12-02-09:55:53-root-INFO: grad norm: 13.630 13.511 1.799
2024-12-02-09:55:53-root-INFO: Loss Change: 248.931 -> 241.562
2024-12-02-09:55:53-root-INFO: Regularization Change: 0.000 -> 1.615
2024-12-02-09:55:53-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-09:55:53-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-09:55:53-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-09:55:54-root-INFO: grad norm: 16.898 16.767 2.103
2024-12-02-09:55:54-root-INFO: Loss too large (242.562->244.055)! Learning rate decreased to 0.03808.
2024-12-02-09:55:54-root-INFO: grad norm: 15.863 15.746 1.920
2024-12-02-09:55:55-root-INFO: grad norm: 15.013 14.904 1.799
2024-12-02-09:55:55-root-INFO: grad norm: 14.278 14.166 1.785
2024-12-02-09:55:56-root-INFO: grad norm: 13.727 13.625 1.670
2024-12-02-09:55:56-root-INFO: grad norm: 13.257 13.151 1.671
2024-12-02-09:55:57-root-INFO: grad norm: 12.914 12.817 1.579
2024-12-02-09:55:57-root-INFO: grad norm: 12.626 12.526 1.588
2024-12-02-09:55:58-root-INFO: Loss Change: 242.562 -> 236.058
2024-12-02-09:55:58-root-INFO: Regularization Change: 0.000 -> 1.579
2024-12-02-09:55:58-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-09:55:58-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-09:55:58-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-09:55:58-root-INFO: grad norm: 15.924 15.772 2.190
2024-12-02-09:55:58-root-INFO: Loss too large (236.878->238.266)! Learning rate decreased to 0.03918.
2024-12-02-09:55:58-root-INFO: grad norm: 15.354 15.243 1.847
2024-12-02-09:55:59-root-INFO: grad norm: 14.858 14.750 1.792
2024-12-02-09:55:59-root-INFO: grad norm: 14.377 14.277 1.688
2024-12-02-09:56:00-root-INFO: grad norm: 13.982 13.887 1.632
2024-12-02-09:56:00-root-INFO: grad norm: 13.609 13.518 1.572
2024-12-02-09:56:01-root-INFO: grad norm: 13.325 13.236 1.530
2024-12-02-09:56:01-root-INFO: grad norm: 13.061 12.976 1.494
2024-12-02-09:56:02-root-INFO: Loss Change: 236.878 -> 230.949
2024-12-02-09:56:02-root-INFO: Regularization Change: 0.000 -> 1.585
2024-12-02-09:56:02-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-09:56:02-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-09:56:02-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-09:56:02-root-INFO: grad norm: 16.343 16.217 2.028
2024-12-02-09:56:02-root-INFO: Loss too large (232.063->233.548)! Learning rate decreased to 0.04031.
2024-12-02-09:56:03-root-INFO: grad norm: 15.431 15.336 1.716
2024-12-02-09:56:03-root-INFO: grad norm: 14.666 14.570 1.670
2024-12-02-09:56:04-root-INFO: grad norm: 13.949 13.858 1.594
2024-12-02-09:56:04-root-INFO: grad norm: 13.429 13.341 1.536
2024-12-02-09:56:04-root-INFO: grad norm: 12.948 12.862 1.491
2024-12-02-09:56:05-root-INFO: grad norm: 12.609 12.526 1.444
2024-12-02-09:56:05-root-INFO: grad norm: 12.302 12.221 1.416
2024-12-02-09:56:06-root-INFO: Loss Change: 232.063 -> 225.939
2024-12-02-09:56:06-root-INFO: Regularization Change: 0.000 -> 1.557
2024-12-02-09:56:06-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-09:56:06-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-09:56:06-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-09:56:06-root-INFO: grad norm: 15.475 15.357 1.906
2024-12-02-09:56:06-root-INFO: Loss too large (227.024->228.294)! Learning rate decreased to 0.04147.
2024-12-02-09:56:07-root-INFO: grad norm: 14.520 14.435 1.568
2024-12-02-09:56:07-root-INFO: grad norm: 13.759 13.673 1.538
2024-12-02-09:56:08-root-INFO: grad norm: 13.037 12.958 1.434
2024-12-02-09:56:08-root-INFO: grad norm: 12.532 12.454 1.398
2024-12-02-09:56:09-root-INFO: grad norm: 12.054 11.980 1.335
2024-12-02-09:56:09-root-INFO: grad norm: 11.731 11.657 1.311
2024-12-02-09:56:09-root-INFO: grad norm: 11.432 11.361 1.269
2024-12-02-09:56:10-root-INFO: Loss Change: 227.024 -> 221.137
2024-12-02-09:56:10-root-INFO: Regularization Change: 0.000 -> 1.535
2024-12-02-09:56:10-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-09:56:10-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-09:56:10-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-09:56:10-root-INFO: grad norm: 14.099 13.996 1.707
2024-12-02-09:56:10-root-INFO: Loss too large (221.824->222.801)! Learning rate decreased to 0.04265.
2024-12-02-09:56:11-root-INFO: grad norm: 13.287 13.209 1.433
2024-12-02-09:56:11-root-INFO: grad norm: 12.664 12.587 1.394
2024-12-02-09:56:12-root-INFO: grad norm: 12.069 11.997 1.316
2024-12-02-09:56:12-root-INFO: grad norm: 11.654 11.583 1.281
2024-12-02-09:56:13-root-INFO: grad norm: 11.252 11.184 1.235
2024-12-02-09:56:13-root-INFO: grad norm: 10.985 10.917 1.212
2024-12-02-09:56:14-root-INFO: grad norm: 10.731 10.666 1.182
2024-12-02-09:56:14-root-INFO: Loss Change: 221.824 -> 216.354
2024-12-02-09:56:14-root-INFO: Regularization Change: 0.000 -> 1.522
2024-12-02-09:56:14-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-09:56:14-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-09:56:14-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-09:56:14-root-INFO: grad norm: 14.025 13.904 1.840
2024-12-02-09:56:14-root-INFO: Loss too large (217.435->218.490)! Learning rate decreased to 0.04386.
2024-12-02-09:56:15-root-INFO: grad norm: 13.315 13.237 1.438
2024-12-02-09:56:15-root-INFO: grad norm: 12.719 12.643 1.389
2024-12-02-09:56:16-root-INFO: grad norm: 12.123 12.055 1.276
2024-12-02-09:56:16-root-INFO: grad norm: 11.693 11.628 1.239
2024-12-02-09:56:17-root-INFO: grad norm: 11.253 11.191 1.181
2024-12-02-09:56:17-root-INFO: grad norm: 10.960 10.899 1.159
2024-12-02-09:56:18-root-INFO: grad norm: 10.665 10.606 1.125
2024-12-02-09:56:18-root-INFO: Loss Change: 217.435 -> 212.087
2024-12-02-09:56:18-root-INFO: Regularization Change: 0.000 -> 1.524
2024-12-02-09:56:18-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-09:56:18-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-09:56:18-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-09:56:18-root-INFO: grad norm: 13.907 13.804 1.687
2024-12-02-09:56:19-root-INFO: Loss too large (212.937->213.940)! Learning rate decreased to 0.04509.
2024-12-02-09:56:19-root-INFO: grad norm: 13.076 13.012 1.291
2024-12-02-09:56:20-root-INFO: grad norm: 12.414 12.347 1.281
2024-12-02-09:56:20-root-INFO: grad norm: 11.747 11.688 1.175
2024-12-02-09:56:20-root-INFO: grad norm: 11.298 11.238 1.159
2024-12-02-09:56:21-root-INFO: grad norm: 10.830 10.773 1.103
2024-12-02-09:56:22-root-INFO: grad norm: 10.535 10.478 1.092
2024-12-02-09:56:22-root-INFO: grad norm: 10.234 10.179 1.058
2024-12-02-09:56:22-root-INFO: Loss Change: 212.937 -> 207.576
2024-12-02-09:56:22-root-INFO: Regularization Change: 0.000 -> 1.526
2024-12-02-09:56:22-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-09:56:22-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-09:56:23-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-09:56:23-root-INFO: grad norm: 12.711 12.615 1.559
2024-12-02-09:56:23-root-INFO: Loss too large (208.341->209.047)! Learning rate decreased to 0.04636.
2024-12-02-09:56:23-root-INFO: grad norm: 11.765 11.704 1.201
2024-12-02-09:56:24-root-INFO: grad norm: 11.116 11.053 1.186
2024-12-02-09:56:24-root-INFO: grad norm: 10.480 10.424 1.079
2024-12-02-09:56:25-root-INFO: grad norm: 10.071 10.014 1.073
2024-12-02-09:56:25-root-INFO: grad norm: 9.661 9.608 1.016
2024-12-02-09:56:26-root-INFO: grad norm: 9.408 9.353 1.016
2024-12-02-09:56:26-root-INFO: grad norm: 9.161 9.109 0.981
2024-12-02-09:56:27-root-INFO: Loss Change: 208.341 -> 203.203
2024-12-02-09:56:27-root-INFO: Regularization Change: 0.000 -> 1.518
2024-12-02-09:56:27-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-09:56:27-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-09:56:27-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-09:56:27-root-INFO: grad norm: 12.136 12.039 1.530
2024-12-02-09:56:27-root-INFO: Loss too large (204.233->204.942)! Learning rate decreased to 0.04765.
2024-12-02-09:56:28-root-INFO: grad norm: 11.477 11.422 1.125
2024-12-02-09:56:28-root-INFO: grad norm: 10.971 10.912 1.140
2024-12-02-09:56:29-root-INFO: grad norm: 10.458 10.408 1.026
2024-12-02-09:56:29-root-INFO: grad norm: 10.110 10.057 1.038
2024-12-02-09:56:30-root-INFO: grad norm: 9.726 9.677 0.975
2024-12-02-09:56:30-root-INFO: grad norm: 9.490 9.438 0.987
2024-12-02-09:56:31-root-INFO: grad norm: 9.235 9.187 0.945
2024-12-02-09:56:31-root-INFO: Loss Change: 204.233 -> 199.405
2024-12-02-09:56:31-root-INFO: Regularization Change: 0.000 -> 1.525
2024-12-02-09:56:31-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-09:56:31-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-09:56:31-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-09:56:31-root-INFO: grad norm: 11.939 11.860 1.372
2024-12-02-09:56:31-root-INFO: Loss too large (200.045->200.730)! Learning rate decreased to 0.04897.
2024-12-02-09:56:32-root-INFO: grad norm: 11.287 11.238 1.049
2024-12-02-09:56:32-root-INFO: grad norm: 10.770 10.717 1.067
2024-12-02-09:56:33-root-INFO: grad norm: 10.217 10.170 0.980
2024-12-02-09:56:33-root-INFO: grad norm: 9.864 9.815 0.986
2024-12-02-09:56:34-root-INFO: grad norm: 9.463 9.416 0.937
2024-12-02-09:56:34-root-INFO: grad norm: 9.226 9.178 0.943
2024-12-02-09:56:35-root-INFO: grad norm: 8.965 8.919 0.910
2024-12-02-09:56:35-root-INFO: Loss Change: 200.045 -> 195.257
2024-12-02-09:56:35-root-INFO: Regularization Change: 0.000 -> 1.533
2024-12-02-09:56:35-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-09:56:35-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-09:56:35-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-09:56:36-root-INFO: grad norm: 11.826 11.737 1.447
2024-12-02-09:56:36-root-INFO: Loss too large (195.985->196.616)! Learning rate decreased to 0.05031.
2024-12-02-09:56:36-root-INFO: grad norm: 11.084 11.036 1.036
2024-12-02-09:56:37-root-INFO: grad norm: 10.525 10.471 1.056
2024-12-02-09:56:37-root-INFO: grad norm: 9.927 9.883 0.942
2024-12-02-09:56:38-root-INFO: grad norm: 9.552 9.504 0.959
2024-12-02-09:56:38-root-INFO: grad norm: 9.120 9.076 0.897
2024-12-02-09:56:39-root-INFO: grad norm: 8.872 8.825 0.911
2024-12-02-09:56:39-root-INFO: grad norm: 8.595 8.551 0.870
2024-12-02-09:56:39-root-INFO: Loss Change: 195.985 -> 191.158
2024-12-02-09:56:39-root-INFO: Regularization Change: 0.000 -> 1.560
2024-12-02-09:56:39-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-09:56:39-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-09:56:40-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-09:56:40-root-INFO: grad norm: 10.771 10.697 1.260
2024-12-02-09:56:40-root-INFO: Loss too large (191.820->192.247)! Learning rate decreased to 0.05169.
2024-12-02-09:56:40-root-INFO: grad norm: 10.000 9.951 0.994
2024-12-02-09:56:41-root-INFO: grad norm: 9.488 9.439 0.963
2024-12-02-09:56:41-root-INFO: grad norm: 8.964 8.918 0.911
2024-12-02-09:56:42-root-INFO: grad norm: 8.642 8.597 0.887
2024-12-02-09:56:42-root-INFO: grad norm: 8.292 8.246 0.868
2024-12-02-09:56:43-root-INFO: grad norm: 8.090 8.045 0.850
2024-12-02-09:56:43-root-INFO: grad norm: 7.878 7.833 0.842
2024-12-02-09:56:44-root-INFO: Loss Change: 191.820 -> 187.244
2024-12-02-09:56:44-root-INFO: Regularization Change: 0.000 -> 1.553
2024-12-02-09:56:44-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-09:56:44-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-09:56:44-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-09:56:44-root-INFO: grad norm: 10.228 10.159 1.188
2024-12-02-09:56:44-root-INFO: Loss too large (187.853->188.296)! Learning rate decreased to 0.05310.
2024-12-02-09:56:45-root-INFO: grad norm: 9.593 9.548 0.930
2024-12-02-09:56:45-root-INFO: grad norm: 9.125 9.078 0.927
2024-12-02-09:56:46-root-INFO: grad norm: 8.646 8.603 0.860
2024-12-02-09:56:46-root-INFO: grad norm: 8.339 8.295 0.861
2024-12-02-09:56:47-root-INFO: grad norm: 7.994 7.951 0.825
2024-12-02-09:56:47-root-INFO: grad norm: 7.792 7.748 0.827
2024-12-02-09:56:48-root-INFO: grad norm: 7.570 7.527 0.802
2024-12-02-09:56:48-root-INFO: Loss Change: 187.853 -> 183.504
2024-12-02-09:56:48-root-INFO: Regularization Change: 0.000 -> 1.543
2024-12-02-09:56:48-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-09:56:48-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-09:56:48-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-09:56:49-root-INFO: grad norm: 9.709 9.637 1.180
2024-12-02-09:56:49-root-INFO: Loss too large (184.216->184.515)! Learning rate decreased to 0.05453.
2024-12-02-09:56:49-root-INFO: grad norm: 9.006 8.964 0.869
2024-12-02-09:56:50-root-INFO: grad norm: 8.532 8.485 0.894
2024-12-02-09:56:50-root-INFO: grad norm: 8.047 8.006 0.809
2024-12-02-09:56:51-root-INFO: grad norm: 7.747 7.702 0.826
2024-12-02-09:56:51-root-INFO: grad norm: 7.408 7.367 0.780
2024-12-02-09:56:52-root-INFO: grad norm: 7.213 7.170 0.792
2024-12-02-09:56:52-root-INFO: grad norm: 6.999 6.957 0.761
2024-12-02-09:56:52-root-INFO: Loss Change: 184.216 -> 179.916
2024-12-02-09:56:52-root-INFO: Regularization Change: 0.000 -> 1.563
2024-12-02-09:56:52-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-09:56:52-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-09:56:53-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-09:56:53-root-INFO: grad norm: 10.112 10.032 1.269
2024-12-02-09:56:53-root-INFO: Loss too large (180.583->180.907)! Learning rate decreased to 0.05599.
2024-12-02-09:56:53-root-INFO: grad norm: 9.205 9.160 0.905
2024-12-02-09:56:54-root-INFO: grad norm: 8.598 8.551 0.892
2024-12-02-09:56:54-root-INFO: grad norm: 7.998 7.957 0.811
2024-12-02-09:56:55-root-INFO: grad norm: 7.628 7.585 0.807
2024-12-02-09:56:55-root-INFO: grad norm: 7.212 7.171 0.769
2024-12-02-09:56:56-root-INFO: grad norm: 6.975 6.932 0.767
2024-12-02-09:56:56-root-INFO: grad norm: 6.712 6.670 0.744
2024-12-02-09:56:57-root-INFO: Loss Change: 180.583 -> 176.083
2024-12-02-09:56:57-root-INFO: Regularization Change: 0.000 -> 1.604
2024-12-02-09:56:57-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-09:56:57-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-09:56:57-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-09:56:57-root-INFO: grad norm: 8.564 8.495 1.089
2024-12-02-09:56:57-root-INFO: Loss too large (176.810->176.913)! Learning rate decreased to 0.05749.
2024-12-02-09:56:58-root-INFO: grad norm: 7.875 7.835 0.797
2024-12-02-09:56:58-root-INFO: grad norm: 7.445 7.400 0.817
2024-12-02-09:56:59-root-INFO: grad norm: 6.993 6.954 0.743
2024-12-02-09:56:59-root-INFO: grad norm: 6.722 6.679 0.759
2024-12-02-09:57:00-root-INFO: grad norm: 6.409 6.369 0.718
2024-12-02-09:57:00-root-INFO: grad norm: 6.232 6.189 0.730
2024-12-02-09:57:01-root-INFO: grad norm: 6.033 5.992 0.702
2024-12-02-09:57:01-root-INFO: Loss Change: 176.810 -> 172.728
2024-12-02-09:57:01-root-INFO: Regularization Change: 0.000 -> 1.590
2024-12-02-09:57:01-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-09:57:01-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-09:57:01-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-09:57:01-root-INFO: grad norm: 8.565 8.494 1.099
2024-12-02-09:57:01-root-INFO: Loss too large (173.126->173.156)! Learning rate decreased to 0.05901.
2024-12-02-09:57:02-root-INFO: grad norm: 7.634 7.594 0.780
2024-12-02-09:57:02-root-INFO: grad norm: 7.084 7.040 0.786
2024-12-02-09:57:03-root-INFO: grad norm: 6.576 6.537 0.714
2024-12-02-09:57:03-root-INFO: grad norm: 6.266 6.225 0.719
2024-12-02-09:57:04-root-INFO: grad norm: 5.930 5.890 0.686
2024-12-02-09:57:04-root-INFO: grad norm: 5.735 5.694 0.690
2024-12-02-09:57:05-root-INFO: grad norm: 5.523 5.482 0.669
2024-12-02-09:57:05-root-INFO: Loss Change: 173.126 -> 168.958
2024-12-02-09:57:05-root-INFO: Regularization Change: 0.000 -> 1.621
2024-12-02-09:57:05-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-09:57:05-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-09:57:05-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-09:57:05-root-INFO: grad norm: 7.711 7.644 1.015
2024-12-02-09:57:06-root-INFO: grad norm: 10.629 10.590 0.910
2024-12-02-09:57:06-root-INFO: Loss too large (169.471->170.689)! Learning rate decreased to 0.06057.
2024-12-02-09:57:06-root-INFO: grad norm: 9.226 9.184 0.880
2024-12-02-09:57:07-root-INFO: grad norm: 7.381 7.343 0.750
2024-12-02-09:57:07-root-INFO: grad norm: 6.648 6.607 0.735
2024-12-02-09:57:08-root-INFO: grad norm: 5.861 5.821 0.682
2024-12-02-09:57:08-root-INFO: grad norm: 5.464 5.422 0.677
2024-12-02-09:57:09-root-INFO: grad norm: 5.056 5.014 0.651
2024-12-02-09:57:09-root-INFO: Loss Change: 169.510 -> 165.464
2024-12-02-09:57:09-root-INFO: Regularization Change: 0.000 -> 1.721
2024-12-02-09:57:09-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-09:57:09-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-09:57:09-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-09:57:10-root-INFO: grad norm: 6.924 6.859 0.949
2024-12-02-09:57:10-root-INFO: grad norm: 9.230 9.191 0.841
2024-12-02-09:57:10-root-INFO: Loss too large (165.613->166.381)! Learning rate decreased to 0.06216.
2024-12-02-09:57:11-root-INFO: grad norm: 8.048 8.008 0.806
2024-12-02-09:57:11-root-INFO: grad norm: 6.621 6.584 0.705
2024-12-02-09:57:12-root-INFO: grad norm: 5.987 5.947 0.689
2024-12-02-09:57:12-root-INFO: grad norm: 5.315 5.275 0.649
2024-12-02-09:57:13-root-INFO: grad norm: 4.966 4.925 0.642
2024-12-02-09:57:13-root-INFO: grad norm: 4.610 4.568 0.622
2024-12-02-09:57:13-root-INFO: Loss Change: 165.806 -> 161.914
2024-12-02-09:57:13-root-INFO: Regularization Change: 0.000 -> 1.740
2024-12-02-09:57:13-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-09:57:13-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-09:57:13-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-09:57:14-root-INFO: grad norm: 6.116 6.056 0.852
2024-12-02-09:57:14-root-INFO: grad norm: 8.131 8.093 0.777
2024-12-02-09:57:14-root-INFO: Loss too large (161.908->162.418)! Learning rate decreased to 0.06378.
2024-12-02-09:57:15-root-INFO: grad norm: 7.158 7.119 0.744
2024-12-02-09:57:15-root-INFO: grad norm: 5.996 5.959 0.668
2024-12-02-09:57:16-root-INFO: grad norm: 5.455 5.415 0.654
2024-12-02-09:57:16-root-INFO: grad norm: 4.873 4.833 0.622
2024-12-02-09:57:16-root-INFO: grad norm: 4.566 4.525 0.615
2024-12-02-09:57:17-root-INFO: grad norm: 4.249 4.207 0.599
2024-12-02-09:57:17-root-INFO: Loss Change: 162.140 -> 158.472
2024-12-02-09:57:17-root-INFO: Regularization Change: 0.000 -> 1.736
2024-12-02-09:57:17-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-09:57:17-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-09:57:17-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-09:57:18-root-INFO: grad norm: 6.289 6.225 0.893
2024-12-02-09:57:18-root-INFO: grad norm: 8.071 8.033 0.778
2024-12-02-09:57:18-root-INFO: Loss too large (158.592->159.050)! Learning rate decreased to 0.06543.
2024-12-02-09:57:19-root-INFO: grad norm: 6.976 6.937 0.734
2024-12-02-09:57:19-root-INFO: grad norm: 5.760 5.723 0.650
2024-12-02-09:57:19-root-INFO: grad norm: 5.184 5.146 0.631
2024-12-02-09:57:20-root-INFO: grad norm: 4.588 4.549 0.600
2024-12-02-09:57:20-root-INFO: grad norm: 4.271 4.230 0.591
2024-12-02-09:57:21-root-INFO: grad norm: 3.951 3.909 0.576
2024-12-02-09:57:21-root-INFO: Loss Change: 158.868 -> 155.133
2024-12-02-09:57:21-root-INFO: Regularization Change: 0.000 -> 1.765
2024-12-02-09:57:21-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-09:57:21-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-09:57:21-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-09:57:21-root-INFO: grad norm: 5.373 5.311 0.811
2024-12-02-09:57:22-root-INFO: grad norm: 6.751 6.714 0.709
2024-12-02-09:57:22-root-INFO: Loss too large (155.113->155.277)! Learning rate decreased to 0.06711.
2024-12-02-09:57:23-root-INFO: grad norm: 5.890 5.853 0.666
2024-12-02-09:57:23-root-INFO: grad norm: 4.994 4.956 0.608
2024-12-02-09:57:23-root-INFO: grad norm: 4.544 4.505 0.597
2024-12-02-09:57:24-root-INFO: grad norm: 4.074 4.034 0.572
2024-12-02-09:57:24-root-INFO: grad norm: 3.815 3.772 0.568
2024-12-02-09:57:25-root-INFO: grad norm: 3.551 3.508 0.554
2024-12-02-09:57:25-root-INFO: Loss Change: 155.451 -> 151.950
2024-12-02-09:57:25-root-INFO: Regularization Change: 0.000 -> 1.758
2024-12-02-09:57:25-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-09:57:25-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-09:57:25-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-09:57:25-root-INFO: grad norm: 5.438 5.371 0.851
2024-12-02-09:57:26-root-INFO: grad norm: 6.653 6.618 0.684
2024-12-02-09:57:26-root-INFO: Loss too large (151.802->151.926)! Learning rate decreased to 0.06882.
2024-12-02-09:57:27-root-INFO: grad norm: 5.738 5.700 0.665
2024-12-02-09:57:27-root-INFO: grad norm: 4.870 4.835 0.579
2024-12-02-09:57:27-root-INFO: grad norm: 4.412 4.373 0.581
2024-12-02-09:57:28-root-INFO: grad norm: 3.952 3.914 0.546
2024-12-02-09:57:28-root-INFO: grad norm: 3.695 3.654 0.549
2024-12-02-09:57:29-root-INFO: grad norm: 3.436 3.395 0.530
2024-12-02-09:57:29-root-INFO: Loss Change: 152.166 -> 148.657
2024-12-02-09:57:29-root-INFO: Regularization Change: 0.000 -> 1.787
2024-12-02-09:57:29-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-09:57:29-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-09:57:29-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-09:57:29-root-INFO: grad norm: 5.743 5.678 0.861
2024-12-02-09:57:30-root-INFO: grad norm: 6.976 6.939 0.715
2024-12-02-09:57:30-root-INFO: Loss too large (148.831->149.006)! Learning rate decreased to 0.07057.
2024-12-02-09:57:31-root-INFO: grad norm: 5.858 5.821 0.655
2024-12-02-09:57:31-root-INFO: grad norm: 4.773 4.739 0.575
2024-12-02-09:57:31-root-INFO: grad norm: 4.249 4.212 0.559
2024-12-02-09:57:32-root-INFO: grad norm: 3.739 3.701 0.532
2024-12-02-09:57:32-root-INFO: grad norm: 3.463 3.422 0.527
2024-12-02-09:57:33-root-INFO: grad norm: 3.196 3.154 0.514
2024-12-02-09:57:33-root-INFO: Loss Change: 149.182 -> 145.604
2024-12-02-09:57:33-root-INFO: Regularization Change: 0.000 -> 1.810
2024-12-02-09:57:33-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-09:57:33-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-09:57:33-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-09:57:34-root-INFO: grad norm: 4.782 4.721 0.765
2024-12-02-09:57:34-root-INFO: grad norm: 5.473 5.437 0.628
2024-12-02-09:57:35-root-INFO: grad norm: 6.321 6.281 0.703
2024-12-02-09:57:35-root-INFO: grad norm: 7.832 7.799 0.719
2024-12-02-09:57:35-root-INFO: Loss too large (144.933->145.296)! Learning rate decreased to 0.07235.
2024-12-02-09:57:36-root-INFO: grad norm: 6.226 6.192 0.648
2024-12-02-09:57:36-root-INFO: grad norm: 4.456 4.421 0.561
2024-12-02-09:57:36-root-INFO: grad norm: 3.796 3.759 0.534
2024-12-02-09:57:37-root-INFO: grad norm: 3.249 3.209 0.508
2024-12-02-09:57:37-root-INFO: Loss Change: 145.780 -> 142.262
2024-12-02-09:57:37-root-INFO: Regularization Change: 0.000 -> 2.044
2024-12-02-09:57:37-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-09:57:37-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-09:57:38-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-09:57:38-root-INFO: grad norm: 4.542 4.484 0.725
2024-12-02-09:57:38-root-INFO: grad norm: 5.220 5.184 0.608
2024-12-02-09:57:39-root-INFO: grad norm: 5.968 5.928 0.689
2024-12-02-09:57:39-root-INFO: grad norm: 7.212 7.179 0.690
2024-12-02-09:57:39-root-INFO: Loss too large (141.773->141.963)! Learning rate decreased to 0.07416.
2024-12-02-09:57:40-root-INFO: grad norm: 5.661 5.627 0.618
2024-12-02-09:57:40-root-INFO: grad norm: 4.093 4.057 0.536
2024-12-02-09:57:41-root-INFO: grad norm: 3.494 3.456 0.515
2024-12-02-09:57:41-root-INFO: grad norm: 3.012 2.971 0.491
2024-12-02-09:57:41-root-INFO: Loss Change: 142.626 -> 139.231
2024-12-02-09:57:41-root-INFO: Regularization Change: 0.000 -> 2.030
2024-12-02-09:57:41-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-09:57:41-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-09:57:42-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-09:57:42-root-INFO: grad norm: 4.395 4.338 0.702
2024-12-02-09:57:42-root-INFO: grad norm: 4.664 4.623 0.619
2024-12-02-09:57:43-root-INFO: grad norm: 5.129 5.090 0.633
2024-12-02-09:57:43-root-INFO: grad norm: 5.785 5.747 0.662
2024-12-02-09:57:44-root-INFO: grad norm: 6.232 6.194 0.682
2024-12-02-09:57:44-root-INFO: grad norm: 6.788 6.750 0.714
2024-12-02-09:57:45-root-INFO: grad norm: 7.007 6.969 0.731
2024-12-02-09:57:45-root-INFO: grad norm: 7.168 7.128 0.756
2024-12-02-09:57:45-root-INFO: Loss Change: 139.311 -> 136.787
2024-12-02-09:57:45-root-INFO: Regularization Change: 0.000 -> 2.708
2024-12-02-09:57:45-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-09:57:45-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-09:57:46-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-09:57:46-root-INFO: grad norm: 9.522 9.448 1.184
2024-12-02-09:57:46-root-INFO: grad norm: 8.908 8.854 0.979
2024-12-02-09:57:47-root-INFO: grad norm: 8.300 8.250 0.906
2024-12-02-09:57:47-root-INFO: grad norm: 7.906 7.859 0.868
2024-12-02-09:57:48-root-INFO: grad norm: 7.757 7.709 0.856
2024-12-02-09:57:48-root-INFO: grad norm: 7.548 7.500 0.851
2024-12-02-09:57:48-root-INFO: grad norm: 7.465 7.418 0.836
2024-12-02-09:57:49-root-INFO: grad norm: 7.317 7.268 0.838
2024-12-02-09:57:49-root-INFO: Loss Change: 137.640 -> 133.235
2024-12-02-09:57:49-root-INFO: Regularization Change: 0.000 -> 2.688
2024-12-02-09:57:49-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-09:57:49-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-09:57:49-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-09:57:50-root-INFO: grad norm: 9.194 9.117 1.186
2024-12-02-09:57:50-root-INFO: grad norm: 8.632 8.575 0.989
2024-12-02-09:57:50-root-INFO: grad norm: 8.108 8.055 0.932
2024-12-02-09:57:51-root-INFO: grad norm: 7.677 7.624 0.893
2024-12-02-09:57:51-root-INFO: grad norm: 7.361 7.313 0.842
2024-12-02-09:57:52-root-INFO: grad norm: 7.037 6.988 0.832
2024-12-02-09:57:52-root-INFO: grad norm: 6.791 6.746 0.783
2024-12-02-09:57:53-root-INFO: grad norm: 6.551 6.504 0.785
2024-12-02-09:57:53-root-INFO: Loss Change: 134.091 -> 129.592
2024-12-02-09:57:53-root-INFO: Regularization Change: 0.000 -> 2.682
2024-12-02-09:57:53-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-09:57:53-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-09:57:53-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-09:57:54-root-INFO: grad norm: 8.184 8.113 1.074
2024-12-02-09:57:54-root-INFO: grad norm: 7.474 7.419 0.905
2024-12-02-09:57:54-root-INFO: grad norm: 6.820 6.772 0.813
2024-12-02-09:57:55-root-INFO: grad norm: 6.368 6.320 0.781
2024-12-02-09:57:55-root-INFO: grad norm: 6.039 5.995 0.724
2024-12-02-09:57:56-root-INFO: grad norm: 5.730 5.685 0.716
2024-12-02-09:57:56-root-INFO: grad norm: 5.497 5.456 0.670
2024-12-02-09:57:57-root-INFO: grad norm: 5.282 5.239 0.672
2024-12-02-09:57:57-root-INFO: Loss Change: 130.309 -> 125.964
2024-12-02-09:57:57-root-INFO: Regularization Change: 0.000 -> 2.680
2024-12-02-09:57:57-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-09:57:57-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-09:57:57-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-09:57:58-root-INFO: grad norm: 7.152 7.079 1.020
2024-12-02-09:57:58-root-INFO: grad norm: 6.465 6.415 0.797
2024-12-02-09:57:58-root-INFO: grad norm: 5.818 5.772 0.729
2024-12-02-09:57:59-root-INFO: grad norm: 5.453 5.410 0.687
2024-12-02-09:57:59-root-INFO: grad norm: 5.186 5.146 0.648
2024-12-02-09:58:00-root-INFO: grad norm: 4.942 4.901 0.638
2024-12-02-09:58:00-root-INFO: grad norm: 4.757 4.719 0.603
2024-12-02-09:58:01-root-INFO: grad norm: 4.590 4.550 0.605
2024-12-02-09:58:01-root-INFO: Loss Change: 126.596 -> 122.559
2024-12-02-09:58:01-root-INFO: Regularization Change: 0.000 -> 2.698
2024-12-02-09:58:01-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-09:58:01-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-09:58:01-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-09:58:01-root-INFO: grad norm: 6.278 6.214 0.895
2024-12-02-09:58:02-root-INFO: grad norm: 5.724 5.677 0.734
2024-12-02-09:58:02-root-INFO: grad norm: 5.216 5.173 0.666
2024-12-02-09:58:03-root-INFO: grad norm: 4.892 4.849 0.645
2024-12-02-09:58:03-root-INFO: grad norm: 4.643 4.605 0.593
2024-12-02-09:58:04-root-INFO: grad norm: 4.432 4.391 0.597
2024-12-02-09:58:04-root-INFO: grad norm: 4.272 4.237 0.552
2024-12-02-09:58:05-root-INFO: grad norm: 4.134 4.095 0.568
2024-12-02-09:58:05-root-INFO: Loss Change: 122.972 -> 119.188
2024-12-02-09:58:05-root-INFO: Regularization Change: 0.000 -> 2.711
2024-12-02-09:58:05-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-09:58:05-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-09:58:05-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-09:58:05-root-INFO: grad norm: 5.321 5.269 0.741
2024-12-02-09:58:06-root-INFO: grad norm: 4.833 4.789 0.646
2024-12-02-09:58:06-root-INFO: grad norm: 4.319 4.281 0.570
2024-12-02-09:58:07-root-INFO: grad norm: 4.110 4.071 0.567
2024-12-02-09:58:07-root-INFO: grad norm: 3.963 3.929 0.522
2024-12-02-09:58:08-root-INFO: grad norm: 3.842 3.804 0.538
2024-12-02-09:58:08-root-INFO: grad norm: 3.759 3.726 0.498
2024-12-02-09:58:09-root-INFO: grad norm: 3.686 3.649 0.522
2024-12-02-09:58:09-root-INFO: Loss Change: 119.564 -> 116.101
2024-12-02-09:58:09-root-INFO: Regularization Change: 0.000 -> 2.693
2024-12-02-09:58:09-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-09:58:09-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-09:58:09-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-09:58:09-root-INFO: grad norm: 5.904 5.830 0.930
2024-12-02-09:58:10-root-INFO: grad norm: 5.178 5.132 0.687
2024-12-02-09:58:10-root-INFO: grad norm: 4.441 4.400 0.604
2024-12-02-09:58:11-root-INFO: grad norm: 4.162 4.123 0.572
2024-12-02-09:58:11-root-INFO: grad norm: 3.982 3.947 0.527
2024-12-02-09:58:11-root-INFO: grad norm: 3.830 3.792 0.535
2024-12-02-09:58:12-root-INFO: grad norm: 3.725 3.692 0.492
2024-12-02-09:58:12-root-INFO: grad norm: 3.639 3.602 0.514
2024-12-02-09:58:13-root-INFO: Loss Change: 116.479 -> 112.793
2024-12-02-09:58:13-root-INFO: Regularization Change: 0.000 -> 2.779
2024-12-02-09:58:13-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-09:58:13-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-09:58:13-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-09:58:13-root-INFO: grad norm: 4.895 4.842 0.714
2024-12-02-09:58:13-root-INFO: grad norm: 4.481 4.440 0.605
2024-12-02-09:58:14-root-INFO: grad norm: 4.062 4.025 0.542
2024-12-02-09:58:14-root-INFO: grad norm: 3.899 3.861 0.543
2024-12-02-09:58:15-root-INFO: grad norm: 3.782 3.749 0.495
2024-12-02-09:58:15-root-INFO: grad norm: 3.693 3.656 0.519
2024-12-02-09:58:16-root-INFO: grad norm: 3.632 3.601 0.473
2024-12-02-09:58:16-root-INFO: grad norm: 3.583 3.547 0.506
2024-12-02-09:58:17-root-INFO: Loss Change: 113.150 -> 109.855
2024-12-02-09:58:17-root-INFO: Regularization Change: 0.000 -> 2.746
2024-12-02-09:58:17-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-09:58:17-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-09:58:17-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-09:58:17-root-INFO: grad norm: 5.874 5.802 0.915
2024-12-02-09:58:18-root-INFO: grad norm: 5.023 4.974 0.701
2024-12-02-09:58:18-root-INFO: grad norm: 4.081 4.042 0.561
2024-12-02-09:58:18-root-INFO: grad norm: 3.850 3.810 0.551
2024-12-02-09:58:19-root-INFO: grad norm: 3.757 3.724 0.493
2024-12-02-09:58:19-root-INFO: grad norm: 3.658 3.621 0.519
2024-12-02-09:58:20-root-INFO: grad norm: 3.607 3.577 0.468
2024-12-02-09:58:20-root-INFO: grad norm: 3.558 3.522 0.503
2024-12-02-09:58:21-root-INFO: Loss Change: 110.248 -> 106.614
2024-12-02-09:58:21-root-INFO: Regularization Change: 0.000 -> 2.856
2024-12-02-09:58:21-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-09:58:21-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-09:58:21-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-09:58:21-root-INFO: grad norm: 4.806 4.752 0.718
2024-12-02-09:58:21-root-INFO: grad norm: 4.416 4.374 0.612
2024-12-02-09:58:22-root-INFO: grad norm: 4.019 3.984 0.532
2024-12-02-09:58:22-root-INFO: grad norm: 3.894 3.855 0.547
2024-12-02-09:58:23-root-INFO: grad norm: 3.807 3.776 0.489
2024-12-02-09:58:23-root-INFO: grad norm: 3.742 3.705 0.524
2024-12-02-09:58:24-root-INFO: grad norm: 3.693 3.663 0.469
2024-12-02-09:58:24-root-INFO: grad norm: 3.662 3.626 0.511
2024-12-02-09:58:25-root-INFO: Loss Change: 106.794 -> 103.570
2024-12-02-09:58:25-root-INFO: Regularization Change: 0.000 -> 2.840
2024-12-02-09:58:25-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-09:58:25-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-09:58:25-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-09:58:25-root-INFO: grad norm: 4.905 4.856 0.689
2024-12-02-09:58:25-root-INFO: grad norm: 4.384 4.342 0.611
2024-12-02-09:58:26-root-INFO: grad norm: 3.718 3.685 0.497
2024-12-02-09:58:26-root-INFO: grad norm: 3.631 3.594 0.516
2024-12-02-09:58:27-root-INFO: grad norm: 3.641 3.612 0.461
2024-12-02-09:58:27-root-INFO: grad norm: 3.609 3.574 0.503
2024-12-02-09:58:28-root-INFO: grad norm: 3.608 3.580 0.449
2024-12-02-09:58:28-root-INFO: grad norm: 3.594 3.559 0.496
2024-12-02-09:58:28-root-INFO: Loss Change: 103.874 -> 100.676
2024-12-02-09:58:28-root-INFO: Regularization Change: 0.000 -> 2.855
2024-12-02-09:58:28-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-09:58:28-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-09:58:29-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-09:58:29-root-INFO: grad norm: 5.287 5.227 0.797
2024-12-02-09:58:29-root-INFO: grad norm: 4.655 4.609 0.652
2024-12-02-09:58:30-root-INFO: grad norm: 3.906 3.871 0.520
2024-12-02-09:58:30-root-INFO: grad norm: 3.781 3.743 0.534
2024-12-02-09:58:31-root-INFO: grad norm: 3.770 3.740 0.470
2024-12-02-09:58:31-root-INFO: grad norm: 3.714 3.679 0.512
2024-12-02-09:58:32-root-INFO: grad norm: 3.681 3.653 0.452
2024-12-02-09:58:32-root-INFO: grad norm: 3.656 3.622 0.500
2024-12-02-09:58:32-root-INFO: Loss Change: 101.033 -> 97.707
2024-12-02-09:58:32-root-INFO: Regularization Change: 0.000 -> 2.932
2024-12-02-09:58:32-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-09:58:32-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-09:58:33-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-09:58:33-root-INFO: grad norm: 4.807 4.759 0.675
2024-12-02-09:58:33-root-INFO: grad norm: 4.419 4.377 0.609
2024-12-02-09:58:34-root-INFO: grad norm: 3.949 3.915 0.512
2024-12-02-09:58:34-root-INFO: grad norm: 3.870 3.833 0.534
2024-12-02-09:58:35-root-INFO: grad norm: 3.858 3.829 0.475
2024-12-02-09:58:35-root-INFO: grad norm: 3.811 3.776 0.517
2024-12-02-09:58:36-root-INFO: grad norm: 3.770 3.742 0.459
2024-12-02-09:58:36-root-INFO: grad norm: 3.751 3.717 0.506
2024-12-02-09:58:36-root-INFO: Loss Change: 97.920 -> 94.813
2024-12-02-09:58:36-root-INFO: Regularization Change: 0.000 -> 2.937
2024-12-02-09:58:36-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-09:58:36-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-09:58:37-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-09:58:37-root-INFO: grad norm: 4.946 4.890 0.745
2024-12-02-09:58:37-root-INFO: grad norm: 4.457 4.416 0.608
2024-12-02-09:58:38-root-INFO: grad norm: 3.903 3.869 0.518
2024-12-02-09:58:38-root-INFO: grad norm: 3.812 3.776 0.521
2024-12-02-09:58:39-root-INFO: grad norm: 3.804 3.774 0.472
2024-12-02-09:58:39-root-INFO: grad norm: 3.758 3.723 0.505
2024-12-02-09:58:40-root-INFO: grad norm: 3.724 3.696 0.455
2024-12-02-09:58:40-root-INFO: grad norm: 3.706 3.673 0.496
2024-12-02-09:58:41-root-INFO: Loss Change: 95.180 -> 92.036
2024-12-02-09:58:41-root-INFO: Regularization Change: 0.000 -> 2.973
2024-12-02-09:58:41-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-09:58:41-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-09:58:41-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-09:58:41-root-INFO: grad norm: 5.423 5.360 0.827
2024-12-02-09:58:41-root-INFO: grad norm: 4.761 4.713 0.671
2024-12-02-09:58:42-root-INFO: grad norm: 3.926 3.889 0.535
2024-12-02-09:58:42-root-INFO: grad norm: 3.841 3.804 0.535
2024-12-02-09:58:43-root-INFO: grad norm: 3.924 3.893 0.486
2024-12-02-09:58:43-root-INFO: grad norm: 3.878 3.843 0.521
2024-12-02-09:58:44-root-INFO: grad norm: 3.841 3.812 0.470
2024-12-02-09:58:44-root-INFO: grad norm: 3.823 3.789 0.509
2024-12-02-09:58:44-root-INFO: Loss Change: 92.234 -> 88.986
2024-12-02-09:58:44-root-INFO: Regularization Change: 0.000 -> 3.028
2024-12-02-09:58:44-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-09:58:44-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-09:58:45-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-09:58:45-root-INFO: grad norm: 4.716 4.670 0.659
2024-12-02-09:58:45-root-INFO: grad norm: 4.306 4.266 0.585
2024-12-02-09:58:46-root-INFO: grad norm: 3.776 3.743 0.500
2024-12-02-09:58:46-root-INFO: grad norm: 3.729 3.695 0.505
2024-12-02-09:58:47-root-INFO: grad norm: 3.759 3.730 0.471
2024-12-02-09:58:47-root-INFO: grad norm: 3.744 3.711 0.498
2024-12-02-09:58:48-root-INFO: grad norm: 3.748 3.719 0.462
2024-12-02-09:58:48-root-INFO: grad norm: 3.744 3.711 0.496
2024-12-02-09:58:49-root-INFO: Loss Change: 89.208 -> 86.288
2024-12-02-09:58:49-root-INFO: Regularization Change: 0.000 -> 2.951
2024-12-02-09:58:49-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-09:58:49-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-09:58:49-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-09:58:49-root-INFO: grad norm: 5.056 4.998 0.761
2024-12-02-09:58:49-root-INFO: grad norm: 4.566 4.522 0.628
2024-12-02-09:58:50-root-INFO: grad norm: 3.967 3.931 0.531
2024-12-02-09:58:50-root-INFO: grad norm: 3.915 3.879 0.533
2024-12-02-09:58:51-root-INFO: grad norm: 3.975 3.944 0.492
2024-12-02-09:58:51-root-INFO: grad norm: 3.932 3.897 0.522
2024-12-02-09:58:52-root-INFO: grad norm: 3.887 3.858 0.477
2024-12-02-09:58:52-root-INFO: grad norm: 3.874 3.840 0.512
2024-12-02-09:58:53-root-INFO: Loss Change: 86.617 -> 83.697
2024-12-02-09:58:53-root-INFO: Regularization Change: 0.000 -> 2.919
2024-12-02-09:58:53-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-09:58:53-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-09:58:53-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-09:58:53-root-INFO: grad norm: 5.177 5.119 0.768
2024-12-02-09:58:53-root-INFO: grad norm: 4.606 4.562 0.630
2024-12-02-09:58:54-root-INFO: grad norm: 3.871 3.835 0.524
2024-12-02-09:58:54-root-INFO: grad norm: 3.852 3.817 0.521
2024-12-02-09:58:55-root-INFO: grad norm: 3.993 3.963 0.488
2024-12-02-09:58:55-root-INFO: grad norm: 3.963 3.929 0.521
2024-12-02-09:58:56-root-INFO: grad norm: 3.930 3.901 0.477
2024-12-02-09:58:56-root-INFO: grad norm: 3.924 3.890 0.514
2024-12-02-09:58:57-root-INFO: Loss Change: 84.010 -> 81.141
2024-12-02-09:58:57-root-INFO: Regularization Change: 0.000 -> 2.895
2024-12-02-09:58:57-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-09:58:57-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-09:58:57-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-09:58:57-root-INFO: grad norm: 5.088 5.034 0.740
2024-12-02-09:58:57-root-INFO: grad norm: 4.621 4.576 0.644
2024-12-02-09:58:58-root-INFO: grad norm: 4.008 3.972 0.536
2024-12-02-09:58:58-root-INFO: grad norm: 3.964 3.927 0.543
2024-12-02-09:58:59-root-INFO: grad norm: 4.032 4.001 0.497
2024-12-02-09:58:59-root-INFO: grad norm: 3.986 3.950 0.533
2024-12-02-09:59:00-root-INFO: grad norm: 3.933 3.904 0.482
2024-12-02-09:59:00-root-INFO: grad norm: 3.923 3.888 0.521
2024-12-02-09:59:01-root-INFO: Loss Change: 81.297 -> 78.518
2024-12-02-09:59:01-root-INFO: Regularization Change: 0.000 -> 2.879
2024-12-02-09:59:01-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-09:59:01-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-09:59:01-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-09:59:01-root-INFO: grad norm: 5.323 5.268 0.765
2024-12-02-09:59:01-root-INFO: grad norm: 4.700 4.651 0.674
2024-12-02-09:59:02-root-INFO: grad norm: 3.879 3.843 0.528
2024-12-02-09:59:02-root-INFO: grad norm: 3.850 3.812 0.540
2024-12-02-09:59:03-root-INFO: grad norm: 3.988 3.958 0.492
2024-12-02-09:59:03-root-INFO: grad norm: 3.967 3.931 0.535
2024-12-02-09:59:04-root-INFO: grad norm: 3.955 3.925 0.483
2024-12-02-09:59:04-root-INFO: grad norm: 3.945 3.910 0.528
2024-12-02-09:59:05-root-INFO: Loss Change: 78.722 -> 75.905
2024-12-02-09:59:05-root-INFO: Regularization Change: 0.000 -> 2.890
2024-12-02-09:59:05-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-09:59:05-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-09:59:05-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-09:59:05-root-INFO: grad norm: 5.018 4.959 0.766
2024-12-02-09:59:05-root-INFO: grad norm: 4.590 4.544 0.645
2024-12-02-09:59:06-root-INFO: grad norm: 4.089 4.052 0.549
2024-12-02-09:59:06-root-INFO: grad norm: 4.037 3.998 0.556
2024-12-02-09:59:07-root-INFO: grad norm: 4.062 4.031 0.505
2024-12-02-09:59:07-root-INFO: grad norm: 4.013 3.977 0.540
2024-12-02-09:59:08-root-INFO: grad norm: 3.963 3.933 0.486
2024-12-02-09:59:08-root-INFO: grad norm: 3.947 3.912 0.528
2024-12-02-09:59:08-root-INFO: Loss Change: 76.219 -> 73.534
2024-12-02-09:59:08-root-INFO: Regularization Change: 0.000 -> 2.887
2024-12-02-09:59:08-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-09:59:08-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-09:59:09-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-09:59:09-root-INFO: grad norm: 5.176 5.118 0.773
2024-12-02-09:59:09-root-INFO: grad norm: 4.652 4.604 0.666
2024-12-02-09:59:10-root-INFO: grad norm: 4.001 3.965 0.536
2024-12-02-09:59:10-root-INFO: grad norm: 3.983 3.945 0.553
2024-12-02-09:59:11-root-INFO: grad norm: 4.093 4.062 0.499
2024-12-02-09:59:11-root-INFO: grad norm: 4.054 4.017 0.545
2024-12-02-09:59:12-root-INFO: grad norm: 4.010 3.980 0.487
2024-12-02-09:59:12-root-INFO: grad norm: 4.003 3.967 0.535
2024-12-02-09:59:12-root-INFO: Loss Change: 73.839 -> 71.168
2024-12-02-09:59:12-root-INFO: Regularization Change: 0.000 -> 2.883
2024-12-02-09:59:12-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-09:59:12-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-09:59:13-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-09:59:13-root-INFO: grad norm: 5.077 5.019 0.765
2024-12-02-09:59:13-root-INFO: grad norm: 4.639 4.593 0.653
2024-12-02-09:59:14-root-INFO: grad norm: 4.116 4.079 0.552
2024-12-02-09:59:14-root-INFO: grad norm: 4.101 4.062 0.561
2024-12-02-09:59:15-root-INFO: grad norm: 4.174 4.142 0.515
2024-12-02-09:59:15-root-INFO: grad norm: 4.135 4.097 0.554
2024-12-02-09:59:16-root-INFO: grad norm: 4.085 4.054 0.502
2024-12-02-09:59:16-root-INFO: grad norm: 4.079 4.042 0.544
2024-12-02-09:59:16-root-INFO: Loss Change: 71.352 -> 68.760
2024-12-02-09:59:16-root-INFO: Regularization Change: 0.000 -> 2.905
2024-12-02-09:59:16-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-09:59:16-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-09:59:17-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-09:59:17-root-INFO: grad norm: 5.167 5.113 0.749
2024-12-02-09:59:17-root-INFO: grad norm: 4.708 4.660 0.674
2024-12-02-09:59:18-root-INFO: grad norm: 4.150 4.114 0.548
2024-12-02-09:59:18-root-INFO: grad norm: 4.128 4.088 0.572
2024-12-02-09:59:19-root-INFO: grad norm: 4.185 4.153 0.514
2024-12-02-09:59:19-root-INFO: grad norm: 4.156 4.118 0.562
2024-12-02-09:59:20-root-INFO: grad norm: 4.124 4.093 0.505
2024-12-02-09:59:20-root-INFO: grad norm: 4.116 4.079 0.553
2024-12-02-09:59:20-root-INFO: Loss Change: 69.090 -> 66.535
2024-12-02-09:59:20-root-INFO: Regularization Change: 0.000 -> 2.884
2024-12-02-09:59:20-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-09:59:20-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-09:59:21-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-09:59:21-root-INFO: grad norm: 5.016 4.962 0.731
2024-12-02-09:59:21-root-INFO: grad norm: 4.618 4.571 0.660
2024-12-02-09:59:22-root-INFO: grad norm: 4.166 4.132 0.535
2024-12-02-09:59:22-root-INFO: grad norm: 4.114 4.074 0.566
2024-12-02-09:59:23-root-INFO: grad norm: 4.106 4.075 0.496
2024-12-02-09:59:23-root-INFO: grad norm: 4.064 4.027 0.546
2024-12-02-09:59:24-root-INFO: grad norm: 4.023 3.994 0.482
2024-12-02-09:59:24-root-INFO: grad norm: 4.006 3.970 0.534
2024-12-02-09:59:24-root-INFO: Loss Change: 66.762 -> 64.246
2024-12-02-09:59:24-root-INFO: Regularization Change: 0.000 -> 2.896
2024-12-02-09:59:24-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-09:59:24-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-09:59:25-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-09:59:25-root-INFO: grad norm: 5.171 5.109 0.804
2024-12-02-09:59:25-root-INFO: grad norm: 4.703 4.656 0.663
2024-12-02-09:59:26-root-INFO: grad norm: 4.195 4.158 0.554
2024-12-02-09:59:26-root-INFO: grad norm: 4.130 4.092 0.559
2024-12-02-09:59:27-root-INFO: grad norm: 4.110 4.079 0.503
2024-12-02-09:59:27-root-INFO: grad norm: 4.063 4.027 0.537
2024-12-02-09:59:28-root-INFO: grad norm: 4.013 3.984 0.484
2024-12-02-09:59:28-root-INFO: grad norm: 3.994 3.959 0.525
2024-12-02-09:59:29-root-INFO: Loss Change: 64.585 -> 62.018
2024-12-02-09:59:29-root-INFO: Regularization Change: 0.000 -> 2.923
2024-12-02-09:59:29-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-09:59:29-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-09:59:29-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-09:59:29-root-INFO: grad norm: 5.043 4.987 0.750
2024-12-02-09:59:29-root-INFO: grad norm: 4.599 4.553 0.648
2024-12-02-09:59:30-root-INFO: grad norm: 4.122 4.088 0.527
2024-12-02-09:59:30-root-INFO: grad norm: 4.060 4.022 0.550
2024-12-02-09:59:31-root-INFO: grad norm: 4.030 4.001 0.485
2024-12-02-09:59:31-root-INFO: grad norm: 3.992 3.956 0.529
2024-12-02-09:59:32-root-INFO: grad norm: 3.955 3.927 0.471
2024-12-02-09:59:32-root-INFO: grad norm: 3.937 3.903 0.518
2024-12-02-09:59:33-root-INFO: Loss Change: 62.105 -> 59.594
2024-12-02-09:59:33-root-INFO: Regularization Change: 0.000 -> 2.937
2024-12-02-09:59:33-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-09:59:33-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-09:59:33-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-09:59:33-root-INFO: grad norm: 5.099 5.039 0.776
2024-12-02-09:59:33-root-INFO: grad norm: 4.560 4.514 0.650
2024-12-02-09:59:34-root-INFO: grad norm: 4.039 4.005 0.522
2024-12-02-09:59:34-root-INFO: grad norm: 3.942 3.905 0.534
2024-12-02-09:59:35-root-INFO: grad norm: 3.873 3.845 0.470
2024-12-02-09:59:35-root-INFO: grad norm: 3.842 3.809 0.505
2024-12-02-09:59:36-root-INFO: grad norm: 3.821 3.794 0.455
2024-12-02-09:59:36-root-INFO: grad norm: 3.811 3.778 0.496
2024-12-02-09:59:36-root-INFO: Loss Change: 59.990 -> 57.413
2024-12-02-09:59:36-root-INFO: Regularization Change: 0.000 -> 2.962
2024-12-02-09:59:36-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-09:59:36-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-09:59:37-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-09:59:37-root-INFO: grad norm: 4.519 4.471 0.654
2024-12-02-09:59:37-root-INFO: grad norm: 4.243 4.202 0.589
2024-12-02-09:59:38-root-INFO: grad norm: 4.001 3.972 0.483
2024-12-02-09:59:38-root-INFO: grad norm: 3.925 3.889 0.524
2024-12-02-09:59:39-root-INFO: grad norm: 3.850 3.823 0.449
2024-12-02-09:59:39-root-INFO: grad norm: 3.816 3.783 0.500
2024-12-02-09:59:40-root-INFO: grad norm: 3.784 3.759 0.438
2024-12-02-09:59:40-root-INFO: grad norm: 3.769 3.737 0.490
2024-12-02-09:59:40-root-INFO: Loss Change: 57.569 -> 55.274
2024-12-02-09:59:40-root-INFO: Regularization Change: 0.000 -> 2.930
2024-12-02-09:59:40-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-09:59:40-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-09:59:41-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-09:59:41-root-INFO: grad norm: 4.825 4.765 0.757
2024-12-02-09:59:41-root-INFO: grad norm: 4.429 4.386 0.620
2024-12-02-09:59:42-root-INFO: grad norm: 4.081 4.049 0.516
2024-12-02-09:59:42-root-INFO: grad norm: 3.966 3.930 0.529
2024-12-02-09:59:43-root-INFO: grad norm: 3.854 3.826 0.462
2024-12-02-09:59:43-root-INFO: grad norm: 3.803 3.771 0.496
2024-12-02-09:59:44-root-INFO: grad norm: 3.756 3.729 0.442
2024-12-02-09:59:44-root-INFO: grad norm: 3.733 3.702 0.483
2024-12-02-09:59:45-root-INFO: Loss Change: 55.610 -> 53.190
2024-12-02-09:59:45-root-INFO: Regularization Change: 0.000 -> 2.955
2024-12-02-09:59:45-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-09:59:45-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-09:59:45-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-09:59:45-root-INFO: grad norm: 4.631 4.578 0.695
2024-12-02-09:59:45-root-INFO: grad norm: 4.256 4.214 0.593
2024-12-02-09:59:46-root-INFO: grad norm: 3.930 3.901 0.480
2024-12-02-09:59:46-root-INFO: grad norm: 3.800 3.766 0.506
2024-12-02-09:59:47-root-INFO: grad norm: 3.678 3.653 0.432
2024-12-02-09:59:47-root-INFO: grad norm: 3.626 3.595 0.472
2024-12-02-09:59:48-root-INFO: grad norm: 3.581 3.557 0.414
2024-12-02-09:59:48-root-INFO: grad norm: 3.563 3.534 0.459
2024-12-02-09:59:49-root-INFO: Loss Change: 53.444 -> 51.084
2024-12-02-09:59:49-root-INFO: Regularization Change: 0.000 -> 2.941
2024-12-02-09:59:49-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-09:59:49-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-09:59:49-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-09:59:49-root-INFO: grad norm: 4.493 4.440 0.692
2024-12-02-09:59:49-root-INFO: grad norm: 4.147 4.107 0.577
2024-12-02-09:59:50-root-INFO: grad norm: 3.861 3.832 0.471
2024-12-02-09:59:50-root-INFO: grad norm: 3.723 3.690 0.493
2024-12-02-09:59:51-root-INFO: grad norm: 3.596 3.571 0.423
2024-12-02-09:59:52-root-INFO: grad norm: 3.535 3.505 0.459
2024-12-02-09:59:52-root-INFO: grad norm: 3.486 3.462 0.403
2024-12-02-09:59:53-root-INFO: grad norm: 3.465 3.436 0.445
2024-12-02-09:59:53-root-INFO: Loss Change: 51.259 -> 48.941
2024-12-02-09:59:53-root-INFO: Regularization Change: 0.000 -> 2.954
2024-12-02-09:59:53-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-09:59:53-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-09:59:53-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-09:59:53-root-INFO: grad norm: 4.351 4.300 0.665
2024-12-02-09:59:54-root-INFO: grad norm: 4.048 4.011 0.550
2024-12-02-09:59:54-root-INFO: grad norm: 3.801 3.774 0.456
2024-12-02-09:59:55-root-INFO: grad norm: 3.666 3.635 0.477
2024-12-02-09:59:55-root-INFO: grad norm: 3.542 3.518 0.412
2024-12-02-09:59:56-root-INFO: grad norm: 3.482 3.454 0.446
2024-12-02-09:59:56-root-INFO: grad norm: 3.436 3.413 0.395
2024-12-02-09:59:56-root-INFO: grad norm: 3.421 3.393 0.435
2024-12-02-09:59:57-root-INFO: Loss Change: 49.239 -> 46.992
2024-12-02-09:59:57-root-INFO: Regularization Change: 0.000 -> 2.967
2024-12-02-09:59:57-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-09:59:57-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-09:59:57-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-09:59:57-root-INFO: grad norm: 4.296 4.247 0.649
2024-12-02-09:59:58-root-INFO: grad norm: 3.982 3.944 0.547
2024-12-02-09:59:58-root-INFO: grad norm: 3.742 3.715 0.447
2024-12-02-09:59:59-root-INFO: grad norm: 3.606 3.575 0.470
2024-12-02-09:59:59-root-INFO: grad norm: 3.489 3.465 0.405
2024-12-02-10:00:00-root-INFO: grad norm: 3.426 3.398 0.439
2024-12-02-10:00:00-root-INFO: grad norm: 3.377 3.355 0.389
2024-12-02-10:00:01-root-INFO: grad norm: 3.354 3.326 0.425
2024-12-02-10:00:01-root-INFO: Loss Change: 47.078 -> 44.828
2024-12-02-10:00:01-root-INFO: Regularization Change: 0.000 -> 3.002
2024-12-02-10:00:01-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-10:00:01-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-10:00:01-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-10:00:01-root-INFO: grad norm: 3.958 3.917 0.571
2024-12-02-10:00:02-root-INFO: grad norm: 3.753 3.719 0.501
2024-12-02-10:00:02-root-INFO: grad norm: 3.604 3.580 0.419
2024-12-02-10:00:03-root-INFO: grad norm: 3.495 3.466 0.449
2024-12-02-10:00:03-root-INFO: grad norm: 3.398 3.376 0.387
2024-12-02-10:00:04-root-INFO: grad norm: 3.333 3.305 0.424
2024-12-02-10:00:04-root-INFO: grad norm: 3.279 3.258 0.372
2024-12-02-10:00:05-root-INFO: grad norm: 3.246 3.219 0.412
2024-12-02-10:00:05-root-INFO: Loss Change: 44.917 -> 42.814
2024-12-02-10:00:05-root-INFO: Regularization Change: 0.000 -> 2.977
2024-12-02-10:00:05-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-10:00:05-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-10:00:05-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-10:00:06-root-INFO: grad norm: 4.422 4.361 0.735
2024-12-02-10:00:06-root-INFO: grad norm: 3.955 3.915 0.557
2024-12-02-10:00:07-root-INFO: grad norm: 3.613 3.587 0.433
2024-12-02-10:00:07-root-INFO: grad norm: 3.392 3.363 0.443
2024-12-02-10:00:08-root-INFO: grad norm: 3.210 3.188 0.370
2024-12-02-10:00:08-root-INFO: grad norm: 3.096 3.070 0.396
2024-12-02-10:00:09-root-INFO: grad norm: 3.009 2.990 0.343
2024-12-02-10:00:09-root-INFO: grad norm: 2.961 2.937 0.374
2024-12-02-10:00:09-root-INFO: Loss Change: 43.152 -> 40.746
2024-12-02-10:00:09-root-INFO: Regularization Change: 0.000 -> 3.020
2024-12-02-10:00:09-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-10:00:09-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-10:00:10-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-10:00:10-root-INFO: grad norm: 3.925 3.877 0.615
2024-12-02-10:00:10-root-INFO: grad norm: 3.561 3.527 0.492
2024-12-02-10:00:11-root-INFO: grad norm: 3.300 3.277 0.388
2024-12-02-10:00:11-root-INFO: grad norm: 3.119 3.092 0.402
2024-12-02-10:00:12-root-INFO: grad norm: 2.975 2.956 0.340
2024-12-02-10:00:12-root-INFO: grad norm: 2.880 2.857 0.366
2024-12-02-10:00:13-root-INFO: grad norm: 2.812 2.794 0.319
2024-12-02-10:00:13-root-INFO: grad norm: 2.772 2.750 0.350
2024-12-02-10:00:14-root-INFO: Loss Change: 40.989 -> 38.804
2024-12-02-10:00:14-root-INFO: Regularization Change: 0.000 -> 2.963
2024-12-02-10:00:14-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-10:00:14-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-10:00:14-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-10:00:14-root-INFO: grad norm: 3.470 3.428 0.536
2024-12-02-10:00:14-root-INFO: grad norm: 3.219 3.188 0.442
2024-12-02-10:00:15-root-INFO: grad norm: 3.054 3.033 0.358
2024-12-02-10:00:15-root-INFO: grad norm: 2.930 2.905 0.377
2024-12-02-10:00:16-root-INFO: grad norm: 2.830 2.811 0.322
2024-12-02-10:00:16-root-INFO: grad norm: 2.759 2.736 0.351
2024-12-02-10:00:17-root-INFO: grad norm: 2.708 2.690 0.306
2024-12-02-10:00:17-root-INFO: grad norm: 2.676 2.654 0.339
2024-12-02-10:00:18-root-INFO: Loss Change: 38.865 -> 36.884
2024-12-02-10:00:18-root-INFO: Regularization Change: 0.000 -> 2.929
2024-12-02-10:00:18-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-10:00:18-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-10:00:18-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-10:00:18-root-INFO: grad norm: 3.533 3.485 0.581
2024-12-02-10:00:19-root-INFO: grad norm: 3.215 3.184 0.448
2024-12-02-10:00:19-root-INFO: grad norm: 2.992 2.970 0.361
2024-12-02-10:00:20-root-INFO: grad norm: 2.829 2.805 0.363
2024-12-02-10:00:20-root-INFO: grad norm: 2.701 2.682 0.314
2024-12-02-10:00:21-root-INFO: grad norm: 2.609 2.589 0.330
2024-12-02-10:00:21-root-INFO: grad norm: 2.544 2.527 0.294
2024-12-02-10:00:22-root-INFO: grad norm: 2.502 2.483 0.314
2024-12-02-10:00:22-root-INFO: Loss Change: 36.985 -> 34.948
2024-12-02-10:00:22-root-INFO: Regularization Change: 0.000 -> 2.941
2024-12-02-10:00:22-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-10:00:22-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-10:00:22-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-10:00:22-root-INFO: grad norm: 3.441 3.395 0.562
2024-12-02-10:00:23-root-INFO: grad norm: 3.073 3.042 0.434
2024-12-02-10:00:23-root-INFO: grad norm: 2.819 2.799 0.337
2024-12-02-10:00:24-root-INFO: grad norm: 2.636 2.614 0.341
2024-12-02-10:00:24-root-INFO: grad norm: 2.499 2.483 0.289
2024-12-02-10:00:25-root-INFO: grad norm: 2.401 2.382 0.304
2024-12-02-10:00:25-root-INFO: grad norm: 2.332 2.317 0.269
2024-12-02-10:00:26-root-INFO: grad norm: 2.287 2.269 0.288
2024-12-02-10:00:26-root-INFO: Loss Change: 35.092 -> 33.055
2024-12-02-10:00:26-root-INFO: Regularization Change: 0.000 -> 2.933
2024-12-02-10:00:26-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-10:00:26-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-10:00:26-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-10:00:26-root-INFO: grad norm: 2.980 2.940 0.489
2024-12-02-10:00:27-root-INFO: grad norm: 2.747 2.722 0.373
2024-12-02-10:00:27-root-INFO: grad norm: 2.600 2.580 0.316
2024-12-02-10:00:28-root-INFO: grad norm: 2.488 2.468 0.315
2024-12-02-10:00:28-root-INFO: grad norm: 2.397 2.381 0.279
2024-12-02-10:00:29-root-INFO: grad norm: 2.329 2.311 0.292
2024-12-02-10:00:29-root-INFO: grad norm: 2.277 2.262 0.264
2024-12-02-10:00:30-root-INFO: grad norm: 2.241 2.224 0.280
2024-12-02-10:00:30-root-INFO: Loss Change: 33.091 -> 31.270
2024-12-02-10:00:30-root-INFO: Regularization Change: 0.000 -> 2.889
2024-12-02-10:00:30-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-10:00:30-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-10:00:30-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-10:00:30-root-INFO: grad norm: 3.058 3.017 0.498
2024-12-02-10:00:31-root-INFO: grad norm: 2.737 2.709 0.392
2024-12-02-10:00:31-root-INFO: grad norm: 2.508 2.489 0.303
2024-12-02-10:00:32-root-INFO: grad norm: 2.341 2.321 0.304
2024-12-02-10:00:32-root-INFO: grad norm: 2.218 2.203 0.259
2024-12-02-10:00:33-root-INFO: grad norm: 2.126 2.109 0.269
2024-12-02-10:00:33-root-INFO: grad norm: 2.061 2.047 0.240
2024-12-02-10:00:34-root-INFO: grad norm: 2.016 2.000 0.252
2024-12-02-10:00:34-root-INFO: Loss Change: 31.385 -> 29.519
2024-12-02-10:00:34-root-INFO: Regularization Change: 0.000 -> 2.854
2024-12-02-10:00:34-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-10:00:34-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-10:00:34-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-10:00:35-root-INFO: grad norm: 2.717 2.680 0.445
2024-12-02-10:00:35-root-INFO: grad norm: 2.442 2.418 0.340
2024-12-02-10:00:36-root-INFO: grad norm: 2.267 2.250 0.272
2024-12-02-10:00:36-root-INFO: grad norm: 2.136 2.119 0.274
2024-12-02-10:00:36-root-INFO: grad norm: 2.041 2.027 0.236
2024-12-02-10:00:37-root-INFO: grad norm: 1.967 1.951 0.248
2024-12-02-10:00:37-root-INFO: grad norm: 1.913 1.900 0.220
2024-12-02-10:00:38-root-INFO: grad norm: 1.873 1.858 0.234
2024-12-02-10:00:38-root-INFO: Loss Change: 29.537 -> 27.811
2024-12-02-10:00:38-root-INFO: Regularization Change: 0.000 -> 2.789
2024-12-02-10:00:38-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-10:00:38-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-10:00:39-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-10:00:39-root-INFO: grad norm: 2.641 2.601 0.457
2024-12-02-10:00:39-root-INFO: grad norm: 2.338 2.315 0.324
2024-12-02-10:00:40-root-INFO: grad norm: 2.143 2.127 0.263
2024-12-02-10:00:40-root-INFO: grad norm: 2.007 1.991 0.254
2024-12-02-10:00:41-root-INFO: grad norm: 1.907 1.893 0.223
2024-12-02-10:00:41-root-INFO: grad norm: 1.829 1.815 0.228
2024-12-02-10:00:42-root-INFO: grad norm: 1.772 1.760 0.206
2024-12-02-10:00:42-root-INFO: grad norm: 1.729 1.716 0.214
2024-12-02-10:00:42-root-INFO: Loss Change: 27.710 -> 26.020
2024-12-02-10:00:42-root-INFO: Regularization Change: 0.000 -> 2.753
2024-12-02-10:00:42-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-10:00:42-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-10:00:43-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-10:00:43-root-INFO: grad norm: 2.634 2.594 0.454
2024-12-02-10:00:43-root-INFO: grad norm: 2.244 2.221 0.323
2024-12-02-10:00:44-root-INFO: grad norm: 1.997 1.983 0.243
2024-12-02-10:00:44-root-INFO: grad norm: 1.837 1.821 0.237
2024-12-02-10:00:45-root-INFO: grad norm: 1.725 1.713 0.202
2024-12-02-10:00:45-root-INFO: grad norm: 1.642 1.628 0.208
2024-12-02-10:00:46-root-INFO: grad norm: 1.580 1.570 0.184
2024-12-02-10:00:46-root-INFO: grad norm: 1.535 1.523 0.193
2024-12-02-10:00:46-root-INFO: Loss Change: 26.127 -> 24.437
2024-12-02-10:00:46-root-INFO: Regularization Change: 0.000 -> 2.715
2024-12-02-10:00:46-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-10:00:46-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-10:00:47-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-10:00:47-root-INFO: grad norm: 2.210 2.176 0.384
2024-12-02-10:00:47-root-INFO: grad norm: 1.935 1.916 0.272
2024-12-02-10:00:48-root-INFO: grad norm: 1.781 1.768 0.216
2024-12-02-10:00:48-root-INFO: grad norm: 1.681 1.668 0.214
2024-12-02-10:00:49-root-INFO: grad norm: 1.606 1.595 0.187
2024-12-02-10:00:49-root-INFO: grad norm: 1.549 1.537 0.194
2024-12-02-10:00:49-root-INFO: grad norm: 1.505 1.495 0.174
2024-12-02-10:00:50-root-INFO: grad norm: 1.472 1.461 0.184
2024-12-02-10:00:50-root-INFO: Loss Change: 24.415 -> 22.911
2024-12-02-10:00:50-root-INFO: Regularization Change: 0.000 -> 2.628
2024-12-02-10:00:50-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-10:00:50-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-10:00:50-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-10:00:51-root-INFO: grad norm: 2.248 2.214 0.388
2024-12-02-10:00:51-root-INFO: grad norm: 1.900 1.880 0.271
2024-12-02-10:00:52-root-INFO: grad norm: 1.676 1.664 0.202
2024-12-02-10:00:52-root-INFO: grad norm: 1.546 1.533 0.199
2024-12-02-10:00:53-root-INFO: grad norm: 1.455 1.446 0.167
2024-12-02-10:00:53-root-INFO: grad norm: 1.388 1.377 0.176
2024-12-02-10:00:53-root-INFO: grad norm: 1.338 1.329 0.154
2024-12-02-10:00:54-root-INFO: grad norm: 1.300 1.289 0.164
2024-12-02-10:00:54-root-INFO: Loss Change: 22.894 -> 21.391
2024-12-02-10:00:54-root-INFO: Regularization Change: 0.000 -> 2.584
2024-12-02-10:00:54-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-10:00:54-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-10:00:54-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-10:00:55-root-INFO: grad norm: 2.077 2.042 0.378
2024-12-02-10:00:55-root-INFO: grad norm: 1.708 1.690 0.245
2024-12-02-10:00:55-root-INFO: grad norm: 1.509 1.497 0.184
2024-12-02-10:00:56-root-INFO: grad norm: 1.401 1.390 0.178
2024-12-02-10:00:56-root-INFO: grad norm: 1.322 1.313 0.153
2024-12-02-10:00:57-root-INFO: grad norm: 1.265 1.255 0.158
2024-12-02-10:00:57-root-INFO: grad norm: 1.219 1.211 0.141
2024-12-02-10:00:58-root-INFO: grad norm: 1.186 1.176 0.149
2024-12-02-10:00:58-root-INFO: Loss Change: 21.404 -> 19.966
2024-12-02-10:00:58-root-INFO: Regularization Change: 0.000 -> 2.544
2024-12-02-10:00:58-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-10:00:58-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-10:00:58-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-10:00:59-root-INFO: grad norm: 2.026 1.992 0.368
2024-12-02-10:00:59-root-INFO: grad norm: 1.621 1.604 0.233
2024-12-02-10:01:00-root-INFO: grad norm: 1.391 1.381 0.168
2024-12-02-10:01:00-root-INFO: grad norm: 1.279 1.269 0.163
2024-12-02-10:01:01-root-INFO: grad norm: 1.201 1.194 0.138
2024-12-02-10:01:01-root-INFO: grad norm: 1.147 1.138 0.145
2024-12-02-10:01:01-root-INFO: grad norm: 1.104 1.096 0.127
2024-12-02-10:01:02-root-INFO: grad norm: 1.072 1.064 0.136
2024-12-02-10:01:02-root-INFO: Loss Change: 19.856 -> 18.455
2024-12-02-10:01:02-root-INFO: Regularization Change: 0.000 -> 2.501
2024-12-02-10:01:02-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-10:01:02-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-10:01:02-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-10:01:03-root-INFO: grad norm: 1.751 1.722 0.317
2024-12-02-10:01:03-root-INFO: grad norm: 1.410 1.395 0.202
2024-12-02-10:01:04-root-INFO: grad norm: 1.228 1.219 0.149
2024-12-02-10:01:04-root-INFO: grad norm: 1.140 1.130 0.147
2024-12-02-10:01:05-root-INFO: grad norm: 1.072 1.065 0.125
2024-12-02-10:01:05-root-INFO: grad norm: 1.027 1.018 0.132
2024-12-02-10:01:06-root-INFO: grad norm: 0.989 0.982 0.116
2024-12-02-10:01:06-root-INFO: grad norm: 0.962 0.954 0.124
2024-12-02-10:01:06-root-INFO: Loss Change: 18.434 -> 17.143
2024-12-02-10:01:06-root-INFO: Regularization Change: 0.000 -> 2.423
2024-12-02-10:01:06-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-10:01:06-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-10:01:07-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-10:01:07-root-INFO: grad norm: 1.770 1.739 0.329
2024-12-02-10:01:07-root-INFO: grad norm: 1.362 1.348 0.199
2024-12-02-10:01:08-root-INFO: grad norm: 1.153 1.144 0.141
2024-12-02-10:01:08-root-INFO: grad norm: 1.066 1.058 0.138
2024-12-02-10:01:09-root-INFO: grad norm: 1.003 0.996 0.116
2024-12-02-10:01:09-root-INFO: grad norm: 0.962 0.954 0.123
2024-12-02-10:01:10-root-INFO: grad norm: 0.929 0.922 0.109
2024-12-02-10:01:10-root-INFO: grad norm: 0.905 0.898 0.117
2024-12-02-10:01:11-root-INFO: Loss Change: 17.121 -> 15.848
2024-12-02-10:01:11-root-INFO: Regularization Change: 0.000 -> 2.391
2024-12-02-10:01:11-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-10:01:11-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-10:01:11-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-10:01:11-root-INFO: grad norm: 1.646 1.616 0.313
2024-12-02-10:01:11-root-INFO: grad norm: 1.236 1.223 0.174
2024-12-02-10:01:12-root-INFO: grad norm: 1.041 1.034 0.126
2024-12-02-10:01:12-root-INFO: grad norm: 0.964 0.956 0.122
2024-12-02-10:01:13-root-INFO: grad norm: 0.906 0.900 0.105
2024-12-02-10:01:14-root-INFO: grad norm: 0.872 0.865 0.111
2024-12-02-10:01:14-root-INFO: grad norm: 0.844 0.838 0.099
2024-12-02-10:01:14-root-INFO: grad norm: 0.825 0.819 0.106
2024-12-02-10:01:15-root-INFO: Loss Change: 15.719 -> 14.500
2024-12-02-10:01:15-root-INFO: Regularization Change: 0.000 -> 2.346
2024-12-02-10:01:15-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-10:01:15-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-10:01:15-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-10:01:15-root-INFO: grad norm: 1.624 1.596 0.302
2024-12-02-10:01:16-root-INFO: grad norm: 1.184 1.172 0.169
2024-12-02-10:01:16-root-INFO: grad norm: 0.973 0.966 0.116
2024-12-02-10:01:17-root-INFO: grad norm: 0.900 0.893 0.116
2024-12-02-10:01:17-root-INFO: grad norm: 0.848 0.842 0.097
2024-12-02-10:01:18-root-INFO: grad norm: 0.820 0.813 0.105
2024-12-02-10:01:18-root-INFO: grad norm: 0.796 0.791 0.093
2024-12-02-10:01:18-root-INFO: grad norm: 0.782 0.775 0.101
2024-12-02-10:01:19-root-INFO: Loss Change: 14.524 -> 13.345
2024-12-02-10:01:19-root-INFO: Regularization Change: 0.000 -> 2.291
2024-12-02-10:01:19-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-10:01:19-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-10:01:19-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-10:01:19-root-INFO: grad norm: 1.531 1.502 0.294
2024-12-02-10:01:20-root-INFO: grad norm: 1.092 1.081 0.160
2024-12-02-10:01:20-root-INFO: grad norm: 0.919 0.913 0.108
2024-12-02-10:01:21-root-INFO: grad norm: 0.860 0.853 0.110
2024-12-02-10:01:21-root-INFO: grad norm: 0.813 0.808 0.091
2024-12-02-10:01:21-root-INFO: grad norm: 0.790 0.784 0.100
2024-12-02-10:01:22-root-INFO: grad norm: 0.770 0.765 0.087
2024-12-02-10:01:22-root-INFO: grad norm: 0.758 0.752 0.096
2024-12-02-10:01:23-root-INFO: Loss Change: 13.238 -> 12.121
2024-12-02-10:01:23-root-INFO: Regularization Change: 0.000 -> 2.217
2024-12-02-10:01:23-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-10:01:23-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-10:01:23-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-10:01:23-root-INFO: grad norm: 1.515 1.489 0.280
2024-12-02-10:01:24-root-INFO: grad norm: 1.083 1.072 0.150
2024-12-02-10:01:24-root-INFO: grad norm: 0.876 0.870 0.099
2024-12-02-10:01:24-root-INFO: grad norm: 0.815 0.809 0.102
2024-12-02-10:01:25-root-INFO: grad norm: 0.771 0.766 0.084
2024-12-02-10:01:25-root-INFO: grad norm: 0.750 0.744 0.093
2024-12-02-10:01:26-root-INFO: grad norm: 0.732 0.728 0.080
2024-12-02-10:01:26-root-INFO: grad norm: 0.722 0.716 0.090
2024-12-02-10:01:27-root-INFO: Loss Change: 12.108 -> 11.051
2024-12-02-10:01:27-root-INFO: Regularization Change: 0.000 -> 2.119
2024-12-02-10:01:27-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-10:01:27-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-10:01:27-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-10:01:27-root-INFO: grad norm: 1.446 1.421 0.267
2024-12-02-10:01:27-root-INFO: grad norm: 1.019 1.009 0.144
2024-12-02-10:01:28-root-INFO: grad norm: 0.854 0.849 0.092
2024-12-02-10:01:28-root-INFO: grad norm: 0.802 0.796 0.099
2024-12-02-10:01:29-root-INFO: grad norm: 0.759 0.755 0.078
2024-12-02-10:01:29-root-INFO: grad norm: 0.738 0.733 0.089
2024-12-02-10:01:30-root-INFO: grad norm: 0.719 0.715 0.075
2024-12-02-10:01:30-root-INFO: grad norm: 0.708 0.703 0.085
2024-12-02-10:01:31-root-INFO: Loss Change: 11.000 -> 10.001
2024-12-02-10:01:31-root-INFO: Regularization Change: 0.000 -> 2.036
2024-12-02-10:01:31-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-10:01:31-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-10:01:31-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-10:01:31-root-INFO: grad norm: 1.508 1.481 0.285
2024-12-02-10:01:31-root-INFO: grad norm: 1.034 1.025 0.137
2024-12-02-10:01:32-root-INFO: grad norm: 0.846 0.841 0.089
2024-12-02-10:01:32-root-INFO: grad norm: 0.786 0.781 0.091
2024-12-02-10:01:33-root-INFO: grad norm: 0.736 0.733 0.073
2024-12-02-10:01:33-root-INFO: grad norm: 0.712 0.707 0.082
2024-12-02-10:01:34-root-INFO: grad norm: 0.689 0.685 0.069
2024-12-02-10:01:34-root-INFO: grad norm: 0.674 0.670 0.078
2024-12-02-10:01:34-root-INFO: Loss Change: 9.997 -> 9.024
2024-12-02-10:01:34-root-INFO: Regularization Change: 0.000 -> 1.956
2024-12-02-10:01:34-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-10:01:34-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-10:01:35-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-10:01:35-root-INFO: grad norm: 1.373 1.351 0.249
2024-12-02-10:01:35-root-INFO: grad norm: 0.948 0.940 0.124
2024-12-02-10:01:36-root-INFO: grad norm: 0.805 0.801 0.079
2024-12-02-10:01:36-root-INFO: grad norm: 0.750 0.745 0.086
2024-12-02-10:01:37-root-INFO: grad norm: 0.701 0.698 0.066
2024-12-02-10:01:37-root-INFO: grad norm: 0.675 0.671 0.076
2024-12-02-10:01:38-root-INFO: grad norm: 0.650 0.647 0.062
2024-12-02-10:01:38-root-INFO: grad norm: 0.634 0.630 0.072
2024-12-02-10:01:38-root-INFO: Loss Change: 8.984 -> 8.090
2024-12-02-10:01:38-root-INFO: Regularization Change: 0.000 -> 1.853
2024-12-02-10:01:38-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-10:01:38-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-10:01:38-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-10:01:39-root-INFO: grad norm: 1.473 1.447 0.277
2024-12-02-10:01:39-root-INFO: grad norm: 0.975 0.967 0.125
2024-12-02-10:01:40-root-INFO: grad norm: 0.818 0.814 0.080
2024-12-02-10:01:40-root-INFO: grad norm: 0.752 0.748 0.083
2024-12-02-10:01:41-root-INFO: grad norm: 0.694 0.691 0.063
2024-12-02-10:01:41-root-INFO: grad norm: 0.662 0.658 0.072
2024-12-02-10:01:41-root-INFO: grad norm: 0.631 0.629 0.058
2024-12-02-10:01:42-root-INFO: grad norm: 0.610 0.607 0.066
2024-12-02-10:01:42-root-INFO: Loss Change: 8.122 -> 7.229
2024-12-02-10:01:42-root-INFO: Regularization Change: 0.000 -> 1.797
2024-12-02-10:01:42-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-10:01:42-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-10:01:42-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-10:01:43-root-INFO: grad norm: 1.343 1.321 0.245
2024-12-02-10:01:43-root-INFO: grad norm: 0.881 0.874 0.110
2024-12-02-10:01:44-root-INFO: grad norm: 0.738 0.735 0.068
2024-12-02-10:01:44-root-INFO: grad norm: 0.673 0.669 0.073
2024-12-02-10:01:45-root-INFO: grad norm: 0.619 0.617 0.054
2024-12-02-10:01:45-root-INFO: grad norm: 0.586 0.583 0.063
2024-12-02-10:01:46-root-INFO: grad norm: 0.557 0.555 0.050
2024-12-02-10:01:46-root-INFO: grad norm: 0.537 0.534 0.057
2024-12-02-10:01:46-root-INFO: Loss Change: 7.243 -> 6.438
2024-12-02-10:01:46-root-INFO: Regularization Change: 0.000 -> 1.667
2024-12-02-10:01:46-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-10:01:46-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-10:01:47-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-10:01:47-root-INFO: grad norm: 1.298 1.276 0.240
2024-12-02-10:01:47-root-INFO: grad norm: 0.795 0.789 0.103
2024-12-02-10:01:48-root-INFO: grad norm: 0.661 0.658 0.058
2024-12-02-10:01:48-root-INFO: grad norm: 0.599 0.596 0.064
2024-12-02-10:01:49-root-INFO: grad norm: 0.552 0.550 0.047
2024-12-02-10:01:49-root-INFO: grad norm: 0.523 0.520 0.055
2024-12-02-10:01:50-root-INFO: grad norm: 0.498 0.496 0.044
2024-12-02-10:01:50-root-INFO: grad norm: 0.481 0.478 0.050
2024-12-02-10:01:51-root-INFO: Loss Change: 6.475 -> 5.728
2024-12-02-10:01:51-root-INFO: Regularization Change: 0.000 -> 1.545
2024-12-02-10:01:51-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-10:01:51-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-10:01:51-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-10:01:51-root-INFO: grad norm: 1.314 1.288 0.256
2024-12-02-10:01:51-root-INFO: grad norm: 0.747 0.741 0.093
2024-12-02-10:01:52-root-INFO: grad norm: 0.604 0.602 0.053
2024-12-02-10:01:52-root-INFO: grad norm: 0.543 0.540 0.053
2024-12-02-10:01:53-root-INFO: grad norm: 0.500 0.499 0.040
2024-12-02-10:01:53-root-INFO: grad norm: 0.474 0.472 0.045
2024-12-02-10:01:54-root-INFO: grad norm: 0.454 0.453 0.038
2024-12-02-10:01:54-root-INFO: grad norm: 0.440 0.438 0.042
2024-12-02-10:01:55-root-INFO: Loss Change: 5.780 -> 5.065
2024-12-02-10:01:55-root-INFO: Regularization Change: 0.000 -> 1.454
2024-12-02-10:01:55-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-10:01:55-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-10:01:55-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-10:01:55-root-INFO: grad norm: 1.224 1.202 0.234
2024-12-02-10:01:56-root-INFO: grad norm: 0.680 0.675 0.081
2024-12-02-10:01:56-root-INFO: grad norm: 0.549 0.547 0.046
2024-12-02-10:01:57-root-INFO: grad norm: 0.496 0.494 0.046
2024-12-02-10:01:57-root-INFO: grad norm: 0.461 0.460 0.036
2024-12-02-10:01:58-root-INFO: grad norm: 0.441 0.439 0.040
2024-12-02-10:01:58-root-INFO: grad norm: 0.425 0.424 0.034
2024-12-02-10:01:59-root-INFO: grad norm: 0.414 0.412 0.037
2024-12-02-10:01:59-root-INFO: Loss Change: 5.126 -> 4.479
2024-12-02-10:01:59-root-INFO: Regularization Change: 0.000 -> 1.337
2024-12-02-10:01:59-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-10:01:59-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-10:01:59-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-10:01:59-root-INFO: grad norm: 1.239 1.217 0.236
2024-12-02-10:02:00-root-INFO: grad norm: 0.693 0.688 0.083
2024-12-02-10:02:00-root-INFO: grad norm: 0.570 0.568 0.047
2024-12-02-10:02:01-root-INFO: grad norm: 0.529 0.527 0.048
2024-12-02-10:02:01-root-INFO: grad norm: 0.524 0.522 0.037
2024-12-02-10:02:02-root-INFO: grad norm: 0.521 0.519 0.046
2024-12-02-10:02:02-root-INFO: grad norm: 0.560 0.559 0.037
2024-12-02-10:02:03-root-INFO: grad norm: 0.550 0.548 0.048
2024-12-02-10:02:03-root-INFO: Loss Change: 4.574 -> 3.970
2024-12-02-10:02:03-root-INFO: Regularization Change: 0.000 -> 1.244
2024-12-02-10:02:03-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-10:02:03-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-10:02:03-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-10:02:03-root-INFO: grad norm: 1.322 1.302 0.226
2024-12-02-10:02:04-root-INFO: grad norm: 0.743 0.738 0.083
2024-12-02-10:02:04-root-INFO: grad norm: 0.509 0.508 0.037
2024-12-02-10:02:05-root-INFO: grad norm: 0.448 0.447 0.035
2024-12-02-10:02:05-root-INFO: grad norm: 0.450 0.449 0.032
2024-12-02-10:02:06-root-INFO: grad norm: 0.604 0.603 0.035
2024-12-02-10:02:06-root-INFO: grad norm: 0.597 0.595 0.046
2024-12-02-10:02:06-root-INFO: grad norm: 0.578 0.577 0.035
2024-12-02-10:02:07-root-INFO: Loss Change: 4.079 -> 3.542
2024-12-02-10:02:07-root-INFO: Regularization Change: 0.000 -> 1.159
2024-12-02-10:02:07-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-10:02:07-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-10:02:07-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-10:02:07-root-INFO: grad norm: 1.204 1.186 0.208
2024-12-02-10:02:07-root-INFO: grad norm: 0.774 0.772 0.057
2024-12-02-10:02:08-root-INFO: grad norm: 0.617 0.615 0.043
2024-12-02-10:02:08-root-INFO: grad norm: 0.443 0.442 0.031
2024-12-02-10:02:09-root-INFO: grad norm: 0.541 0.539 0.038
2024-12-02-10:02:09-root-INFO: grad norm: 0.872 0.871 0.044
2024-12-02-10:02:10-root-INFO: grad norm: 0.549 0.548 0.045
2024-12-02-10:02:10-root-INFO: grad norm: 0.480 0.479 0.031
2024-12-02-10:02:11-root-INFO: Loss Change: 3.650 -> 3.137
2024-12-02-10:02:11-root-INFO: Regularization Change: 0.000 -> 1.053
2024-12-02-10:02:11-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-10:02:11-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-10:02:11-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-10:02:11-root-INFO: grad norm: 1.389 1.367 0.246
2024-12-02-10:02:11-root-INFO: grad norm: 0.699 0.695 0.073
2024-12-02-10:02:12-root-INFO: grad norm: 0.429 0.428 0.035
2024-12-02-10:02:12-root-INFO: grad norm: 0.397 0.396 0.028
2024-12-02-10:02:13-root-INFO: grad norm: 0.504 0.503 0.034
2024-12-02-10:02:13-root-INFO: grad norm: 0.775 0.774 0.041
2024-12-02-10:02:14-root-INFO: grad norm: 0.590 0.588 0.049
2024-12-02-10:02:14-root-INFO: grad norm: 0.354 0.353 0.026
2024-12-02-10:02:15-root-INFO: Loss Change: 3.318 -> 2.786
2024-12-02-10:02:15-root-INFO: Regularization Change: 0.000 -> 0.987
2024-12-02-10:02:15-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-10:02:15-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-10:02:15-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-10:02:15-root-INFO: grad norm: 1.031 1.013 0.193
2024-12-02-10:02:15-root-INFO: grad norm: 0.445 0.443 0.045
2024-12-02-10:02:16-root-INFO: grad norm: 0.347 0.345 0.029
2024-12-02-10:02:16-root-INFO: grad norm: 0.319 0.318 0.025
2024-12-02-10:02:17-root-INFO: grad norm: 0.311 0.310 0.024
2024-12-02-10:02:17-root-INFO: grad norm: 0.337 0.336 0.025
2024-12-02-10:02:18-root-INFO: grad norm: 0.506 0.505 0.036
2024-12-02-10:02:18-root-INFO: Loss too large (2.559->2.565)! Learning rate decreased to 0.27783.
2024-12-02-10:02:18-root-INFO: grad norm: 0.646 0.645 0.037
2024-12-02-10:02:19-root-INFO: Loss Change: 2.922 -> 2.538
2024-12-02-10:02:19-root-INFO: Regularization Change: 0.000 -> 0.809
2024-12-02-10:02:19-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-10:02:19-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-10:02:19-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-10:02:19-root-INFO: grad norm: 1.145 1.131 0.177
2024-12-02-10:02:20-root-INFO: grad norm: 0.772 0.771 0.047
2024-12-02-10:02:20-root-INFO: grad norm: 0.720 0.718 0.054
2024-12-02-10:02:21-root-INFO: grad norm: 0.671 0.669 0.043
2024-12-02-10:02:21-root-INFO: grad norm: 0.677 0.675 0.054
2024-12-02-10:02:22-root-INFO: grad norm: 0.669 0.668 0.042
2024-12-02-10:02:22-root-INFO: grad norm: 0.648 0.646 0.053
2024-12-02-10:02:22-root-INFO: grad norm: 0.838 0.837 0.042
2024-12-02-10:02:23-root-INFO: Loss Change: 2.691 -> 2.233
2024-12-02-10:02:23-root-INFO: Regularization Change: 0.000 -> 0.940
2024-12-02-10:02:23-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-10:02:23-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-10:02:23-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-10:02:23-root-INFO: grad norm: 1.140 1.125 0.185
2024-12-02-10:02:24-root-INFO: grad norm: 0.513 0.512 0.040
2024-12-02-10:02:24-root-INFO: grad norm: 0.456 0.454 0.033
2024-12-02-10:02:25-root-INFO: grad norm: 0.448 0.446 0.036
2024-12-02-10:02:25-root-INFO: grad norm: 0.449 0.448 0.035
2024-12-02-10:02:25-root-INFO: grad norm: 0.444 0.442 0.035
2024-12-02-10:02:26-root-INFO: grad norm: 0.433 0.432 0.034
2024-12-02-10:02:26-root-INFO: grad norm: 0.422 0.421 0.033
2024-12-02-10:02:27-root-INFO: Loss Change: 2.407 -> 1.997
2024-12-02-10:02:27-root-INFO: Regularization Change: 0.000 -> 0.779
2024-12-02-10:02:27-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-10:02:27-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-10:02:27-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-10:02:27-root-INFO: grad norm: 0.935 0.921 0.159
2024-12-02-10:02:28-root-INFO: grad norm: 0.371 0.369 0.035
2024-12-02-10:02:28-root-INFO: grad norm: 0.301 0.300 0.029
2024-12-02-10:02:28-root-INFO: grad norm: 0.276 0.275 0.028
2024-12-02-10:02:29-root-INFO: grad norm: 0.260 0.259 0.026
2024-12-02-10:02:29-root-INFO: grad norm: 0.248 0.247 0.026
2024-12-02-10:02:30-root-INFO: grad norm: 0.238 0.237 0.025
2024-12-02-10:02:30-root-INFO: grad norm: 0.230 0.229 0.025
2024-12-02-10:02:31-root-INFO: Loss Change: 2.160 -> 1.811
2024-12-02-10:02:31-root-INFO: Regularization Change: 0.000 -> 0.698
2024-12-02-10:02:31-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-10:02:31-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-10:02:31-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-10:02:31-root-INFO: grad norm: 0.915 0.900 0.162
2024-12-02-10:02:31-root-INFO: grad norm: 0.359 0.357 0.036
2024-12-02-10:02:32-root-INFO: grad norm: 0.283 0.281 0.032
2024-12-02-10:02:32-root-INFO: grad norm: 0.253 0.251 0.029
2024-12-02-10:02:33-root-INFO: grad norm: 0.235 0.234 0.028
2024-12-02-10:02:33-root-INFO: grad norm: 0.223 0.222 0.027
2024-12-02-10:02:34-root-INFO: grad norm: 0.214 0.213 0.026
2024-12-02-10:02:34-root-INFO: grad norm: 0.207 0.205 0.025
2024-12-02-10:02:35-root-INFO: Loss Change: 1.991 -> 1.659
2024-12-02-10:02:35-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-02-10:02:35-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-10:02:35-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-10:02:35-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-10:02:35-root-INFO: grad norm: 0.836 0.825 0.136
2024-12-02-10:02:35-root-INFO: grad norm: 0.346 0.344 0.036
2024-12-02-10:02:36-root-INFO: grad norm: 0.270 0.268 0.031
2024-12-02-10:02:36-root-INFO: grad norm: 0.246 0.244 0.029
2024-12-02-10:02:37-root-INFO: grad norm: 0.230 0.228 0.029
2024-12-02-10:02:37-root-INFO: grad norm: 0.218 0.216 0.028
2024-12-02-10:02:38-root-INFO: grad norm: 0.209 0.207 0.028
2024-12-02-10:02:38-root-INFO: grad norm: 0.202 0.201 0.027
2024-12-02-10:02:38-root-INFO: Loss Change: 1.824 -> 1.525
2024-12-02-10:02:38-root-INFO: Regularization Change: 0.000 -> 0.614
2024-12-02-10:02:38-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-10:02:38-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-10:02:39-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-10:02:39-root-INFO: grad norm: 0.806 0.797 0.123
2024-12-02-10:02:39-root-INFO: grad norm: 0.345 0.343 0.038
2024-12-02-10:02:40-root-INFO: grad norm: 0.264 0.262 0.032
2024-12-02-10:02:40-root-INFO: grad norm: 0.236 0.234 0.032
2024-12-02-10:02:40-root-INFO: grad norm: 0.217 0.215 0.030
2024-12-02-10:02:41-root-INFO: grad norm: 0.207 0.204 0.030
2024-12-02-10:02:41-root-INFO: grad norm: 0.201 0.199 0.028
2024-12-02-10:02:42-root-INFO: grad norm: 0.192 0.190 0.028
2024-12-02-10:02:42-root-INFO: Loss Change: 1.693 -> 1.410
2024-12-02-10:02:42-root-INFO: Regularization Change: 0.000 -> 0.586
2024-12-02-10:02:42-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-10:02:42-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-10:02:42-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-10:02:42-root-INFO: grad norm: 0.750 0.743 0.105
2024-12-02-10:02:43-root-INFO: grad norm: 0.350 0.349 0.034
2024-12-02-10:02:43-root-INFO: grad norm: 0.248 0.246 0.032
2024-12-02-10:02:44-root-INFO: grad norm: 0.233 0.231 0.032
2024-12-02-10:02:44-root-INFO: grad norm: 0.201 0.198 0.031
2024-12-02-10:02:45-root-INFO: grad norm: 0.188 0.185 0.030
2024-12-02-10:02:45-root-INFO: grad norm: 0.182 0.179 0.029
2024-12-02-10:02:46-root-INFO: grad norm: 0.181 0.179 0.028
2024-12-02-10:02:46-root-INFO: Loss Change: 1.568 -> 1.316
2024-12-02-10:02:46-root-INFO: Regularization Change: 0.000 -> 0.529
2024-12-02-10:02:46-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-10:02:46-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-10:02:46-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-10:02:46-root-INFO: grad norm: 0.725 0.719 0.092
2024-12-02-10:02:47-root-INFO: grad norm: 0.327 0.325 0.030
2024-12-02-10:02:47-root-INFO: grad norm: 0.258 0.256 0.028
2024-12-02-10:02:48-root-INFO: grad norm: 0.239 0.237 0.027
2024-12-02-10:02:48-root-INFO: grad norm: 0.209 0.207 0.027
2024-12-02-10:02:49-root-INFO: grad norm: 0.202 0.200 0.026
2024-12-02-10:02:49-root-INFO: grad norm: 0.202 0.200 0.025
2024-12-02-10:02:50-root-INFO: grad norm: 0.174 0.173 0.024
2024-12-02-10:02:50-root-INFO: Loss Change: 1.459 -> 1.203
2024-12-02-10:02:50-root-INFO: Regularization Change: 0.000 -> 0.564
2024-12-02-10:02:50-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-10:02:50-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-10:02:50-root-INFO: loss_sample0_0: 1.203420639038086
2024-12-02-10:02:50-root-INFO: It takes 1051.291 seconds for image sample0
2024-12-02-10:02:50-root-INFO: lpips_score_sample0: 0.148
2024-12-02-10:02:50-root-INFO: psnr_score_sample0: 17.712
2024-12-02-10:02:50-root-INFO: ssim_score_sample0: 0.721
2024-12-02-10:02:50-root-INFO: mean_lpips: 0.14795349538326263
2024-12-02-10:02:50-root-INFO: best_mean_lpips: 0.14795349538326263
2024-12-02-10:02:50-root-INFO: mean_psnr: 17.71221160888672
2024-12-02-10:02:50-root-INFO: best_mean_psnr: 17.71221160888672
2024-12-02-10:02:50-root-INFO: mean_ssim: 0.7210121750831604
2024-12-02-10:02:50-root-INFO: best_mean_ssim: 0.7210121750831604
2024-12-02-10:02:50-root-INFO: final_loss: 1.203420639038086
2024-12-02-10:02:50-root-INFO: mean time: 1051.2909162044525
2024-12-02-10:02:50-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample1_iter8_lr0.02_10009 
 
Enjoy.
