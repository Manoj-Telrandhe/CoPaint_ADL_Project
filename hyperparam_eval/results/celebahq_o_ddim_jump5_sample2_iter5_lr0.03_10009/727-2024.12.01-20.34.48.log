2024-12-01-20:34:51-root-INFO: Prepare model...
2024-12-01-20:35:04-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-01-20:35:28-root-INFO: Start sampling
2024-12-01-20:35:32-root-INFO: step: 249 lr_xt 0.00019059
2024-12-01-20:35:32-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-01-20:35:32-root-INFO: Loss too large (77070.016->78612.742)! Learning rate decreased to 0.00015.
2024-12-01-20:35:33-root-INFO: grad norm: 15661.115 11248.844 10896.515
2024-12-01-20:35:33-root-INFO: grad norm: 14205.958 10378.391 9700.425
2024-12-01-20:35:34-root-INFO: grad norm: 15590.470 11542.625 10480.009
2024-12-01-20:35:34-root-INFO: Loss too large (28301.709->35851.582)! Learning rate decreased to 0.00012.
2024-12-01-20:35:34-root-INFO: grad norm: 16985.395 12431.950 11573.688
2024-12-01-20:35:35-root-INFO: Loss too large (28113.932->29014.451)! Learning rate decreased to 0.00010.
2024-12-01-20:35:35-root-INFO: Loss Change: 77070.016 -> 22717.590
2024-12-01-20:35:35-root-INFO: Regularization Change: 0.000 -> 14.512
2024-12-01-20:35:35-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-01-20:35:35-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-20:35:35-root-INFO: step: 248 lr_xt 0.00020082
2024-12-01-20:35:35-root-INFO: grad norm: 13147.072 10137.142 8371.610
2024-12-01-20:35:35-root-INFO: Loss too large (23084.727->44016.348)! Learning rate decreased to 0.00016.
2024-12-01-20:35:35-root-INFO: Loss too large (23084.727->34075.672)! Learning rate decreased to 0.00013.
2024-12-01-20:35:36-root-INFO: Loss too large (23084.727->26826.102)! Learning rate decreased to 0.00010.
2024-12-01-20:35:36-root-INFO: grad norm: 12869.338 10069.596 8013.932
2024-12-01-20:35:37-root-INFO: grad norm: 14421.511 11505.937 8694.445
2024-12-01-20:35:37-root-INFO: Loss too large (22209.613->24127.541)! Learning rate decreased to 0.00008.
2024-12-01-20:35:37-root-INFO: grad norm: 11117.030 8914.617 6642.136
2024-12-01-20:35:38-root-INFO: grad norm: 8516.609 6794.818 5134.500
2024-12-01-20:35:38-root-INFO: Loss Change: 23084.727 -> 17988.020
2024-12-01-20:35:38-root-INFO: Regularization Change: 0.000 -> 1.692
2024-12-01-20:35:38-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-01-20:35:38-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-20:35:38-root-INFO: step: 247 lr_xt 0.00021156
2024-12-01-20:35:38-root-INFO: grad norm: 6318.525 5138.155 3677.380
2024-12-01-20:35:38-root-INFO: Loss too large (17729.527->26604.779)! Learning rate decreased to 0.00017.
2024-12-01-20:35:39-root-INFO: Loss too large (17729.527->22042.596)! Learning rate decreased to 0.00014.
2024-12-01-20:35:39-root-INFO: Loss too large (17729.527->19384.625)! Learning rate decreased to 0.00011.
2024-12-01-20:35:39-root-INFO: Loss too large (17729.527->17905.137)! Learning rate decreased to 0.00009.
2024-12-01-20:35:39-root-INFO: grad norm: 4648.598 3702.534 2810.820
2024-12-01-20:35:40-root-INFO: grad norm: 3494.667 2850.782 2021.321
2024-12-01-20:35:41-root-INFO: grad norm: 2611.597 2081.038 1577.885
2024-12-01-20:35:41-root-INFO: grad norm: 1995.756 1635.771 1143.370
2024-12-01-20:35:41-root-INFO: Loss Change: 17729.527 -> 16356.018
2024-12-01-20:35:41-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-01-20:35:41-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-01-20:35:41-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-20:35:41-root-INFO: step: 246 lr_xt 0.00022285
2024-12-01-20:35:42-root-INFO: grad norm: 1556.903 1253.969 922.771
2024-12-01-20:35:42-root-INFO: Loss too large (16122.148->16330.172)! Learning rate decreased to 0.00018.
2024-12-01-20:35:42-root-INFO: Loss too large (16122.148->16170.590)! Learning rate decreased to 0.00014.
2024-12-01-20:35:42-root-INFO: grad norm: 2204.217 1809.139 1259.202
2024-12-01-20:35:43-root-INFO: Loss too large (16084.914->16169.449)! Learning rate decreased to 0.00011.
2024-12-01-20:35:43-root-INFO: grad norm: 2375.786 1931.751 1383.003
2024-12-01-20:35:44-root-INFO: grad norm: 2567.475 2083.747 1499.975
2024-12-01-20:35:44-root-INFO: grad norm: 2787.204 2274.501 1610.947
2024-12-01-20:35:44-root-INFO: Loss Change: 16122.148 -> 15913.516
2024-12-01-20:35:44-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-01-20:35:44-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-01-20:35:44-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-20:35:44-root-INFO: step: 245 lr_xt 0.00023469
2024-12-01-20:35:45-root-INFO: grad norm: 2947.084 2377.571 1741.396
2024-12-01-20:35:45-root-INFO: Loss too large (15795.664->17532.396)! Learning rate decreased to 0.00019.
2024-12-01-20:35:45-root-INFO: Loss too large (15795.664->16571.816)! Learning rate decreased to 0.00015.
2024-12-01-20:35:45-root-INFO: Loss too large (15795.664->16025.713)! Learning rate decreased to 0.00012.
2024-12-01-20:35:45-root-INFO: grad norm: 3040.937 2492.513 1742.032
2024-12-01-20:35:46-root-INFO: grad norm: 3140.008 2525.127 1866.381
2024-12-01-20:35:46-root-INFO: grad norm: 3247.434 2669.393 1849.369
2024-12-01-20:35:47-root-INFO: grad norm: 3351.752 2689.466 2000.255
2024-12-01-20:35:47-root-INFO: Loss Change: 15795.664 -> 15539.937
2024-12-01-20:35:47-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-01-20:35:47-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-01-20:35:47-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-20:35:47-root-INFO: step: 244 lr_xt 0.00024712
2024-12-01-20:35:47-root-INFO: grad norm: 3648.931 2955.990 2139.351
2024-12-01-20:35:48-root-INFO: Loss too large (15494.692->17987.148)! Learning rate decreased to 0.00020.
2024-12-01-20:35:48-root-INFO: Loss too large (15494.692->16618.602)! Learning rate decreased to 0.00016.
2024-12-01-20:35:48-root-INFO: Loss too large (15494.692->15815.559)! Learning rate decreased to 0.00013.
2024-12-01-20:35:48-root-INFO: grad norm: 3487.027 2818.877 2052.630
2024-12-01-20:35:49-root-INFO: grad norm: 3393.662 2799.160 1918.761
2024-12-01-20:35:49-root-INFO: grad norm: 3338.552 2695.784 1969.436
2024-12-01-20:35:50-root-INFO: grad norm: 3293.938 2734.285 1836.767
2024-12-01-20:35:50-root-INFO: Loss Change: 15494.692 -> 15013.180
2024-12-01-20:35:50-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-01-20:35:50-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-01-20:35:50-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:35:50-root-INFO: step: 243 lr_xt 0.00026017
2024-12-01-20:35:50-root-INFO: grad norm: 3311.611 2651.051 1984.615
2024-12-01-20:35:51-root-INFO: Loss too large (14914.260->17108.824)! Learning rate decreased to 0.00021.
2024-12-01-20:35:51-root-INFO: Loss too large (14914.260->15846.020)! Learning rate decreased to 0.00017.
2024-12-01-20:35:51-root-INFO: Loss too large (14914.260->15134.008)! Learning rate decreased to 0.00013.
2024-12-01-20:35:51-root-INFO: grad norm: 3080.866 2573.770 1693.352
2024-12-01-20:35:52-root-INFO: grad norm: 2926.767 2353.598 1739.696
2024-12-01-20:35:52-root-INFO: grad norm: 2789.114 2346.199 1508.147
2024-12-01-20:35:53-root-INFO: grad norm: 2674.136 2153.608 1585.237
2024-12-01-20:35:53-root-INFO: Loss Change: 14914.260 -> 14353.503
2024-12-01-20:35:53-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-01-20:35:53-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-01-20:35:53-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:35:53-root-INFO: step: 242 lr_xt 0.00027387
2024-12-01-20:35:53-root-INFO: grad norm: 2584.733 2173.670 1398.572
2024-12-01-20:35:54-root-INFO: Loss too large (14157.007->15402.479)! Learning rate decreased to 0.00022.
2024-12-01-20:35:54-root-INFO: Loss too large (14157.007->14673.416)! Learning rate decreased to 0.00018.
2024-12-01-20:35:54-root-INFO: Loss too large (14157.007->14258.372)! Learning rate decreased to 0.00014.
2024-12-01-20:35:54-root-INFO: grad norm: 2379.970 1939.274 1379.664
2024-12-01-20:35:55-root-INFO: grad norm: 2208.554 1880.547 1158.126
2024-12-01-20:35:55-root-INFO: grad norm: 2064.804 1675.331 1206.930
2024-12-01-20:35:56-root-INFO: grad norm: 1934.347 1656.704 998.514
2024-12-01-20:35:56-root-INFO: Loss Change: 14157.007 -> 13641.039
2024-12-01-20:35:56-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-01-20:35:56-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-01-20:35:56-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:35:56-root-INFO: step: 241 lr_xt 0.00028824
2024-12-01-20:35:56-root-INFO: grad norm: 1676.586 1381.740 949.596
2024-12-01-20:35:57-root-INFO: Loss too large (13532.068->13919.211)! Learning rate decreased to 0.00023.
2024-12-01-20:35:57-root-INFO: Loss too large (13532.068->13648.788)! Learning rate decreased to 0.00018.
2024-12-01-20:35:57-root-INFO: grad norm: 2174.022 1857.031 1130.402
2024-12-01-20:35:57-root-INFO: Loss too large (13502.289->13545.756)! Learning rate decreased to 0.00015.
2024-12-01-20:35:58-root-INFO: grad norm: 1973.201 1621.107 1124.959
2024-12-01-20:35:58-root-INFO: grad norm: 1796.692 1546.722 914.195
2024-12-01-20:35:59-root-INFO: grad norm: 1645.608 1354.444 934.616
2024-12-01-20:35:59-root-INFO: Loss Change: 13532.068 -> 13090.621
2024-12-01-20:35:59-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-01-20:35:59-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-01-20:35:59-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:35:59-root-INFO: step: 240 lr_xt 0.00030331
2024-12-01-20:35:59-root-INFO: grad norm: 1539.655 1325.033 784.108
2024-12-01-20:36:00-root-INFO: Loss too large (12911.045->13151.388)! Learning rate decreased to 0.00024.
2024-12-01-20:36:00-root-INFO: Loss too large (12911.045->12950.521)! Learning rate decreased to 0.00019.
2024-12-01-20:36:00-root-INFO: grad norm: 1884.093 1566.460 1046.905
2024-12-01-20:36:01-root-INFO: grad norm: 2416.261 2072.567 1242.088
2024-12-01-20:36:01-root-INFO: Loss too large (12830.327->12899.758)! Learning rate decreased to 0.00016.
2024-12-01-20:36:01-root-INFO: grad norm: 2125.023 1764.771 1183.767
2024-12-01-20:36:02-root-INFO: grad norm: 1874.787 1618.895 945.518
2024-12-01-20:36:02-root-INFO: Loss Change: 12911.045 -> 12457.652
2024-12-01-20:36:02-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-01-20:36:02-root-INFO: Undo step: 240
2024-12-01-20:36:02-root-INFO: Undo step: 241
2024-12-01-20:36:02-root-INFO: Undo step: 242
2024-12-01-20:36:02-root-INFO: Undo step: 243
2024-12-01-20:36:02-root-INFO: Undo step: 244
2024-12-01-20:36:02-root-INFO: step: 245 lr_xt 0.00023469
2024-12-01-20:36:02-root-INFO: grad norm: 10431.536 7518.697 7230.916
2024-12-01-20:36:02-root-INFO: Loss too large (19433.088->30373.910)! Learning rate decreased to 0.00019.
2024-12-01-20:36:03-root-INFO: Loss too large (19433.088->23218.844)! Learning rate decreased to 0.00015.
2024-12-01-20:36:03-root-INFO: grad norm: 9897.670 7096.980 6899.038
2024-12-01-20:36:04-root-INFO: grad norm: 10146.526 7417.355 6923.499
2024-12-01-20:36:04-root-INFO: Loss too large (18286.730->19009.914)! Learning rate decreased to 0.00012.
2024-12-01-20:36:04-root-INFO: grad norm: 7079.807 5207.047 4796.908
2024-12-01-20:36:05-root-INFO: grad norm: 5212.465 3968.737 3379.189
2024-12-01-20:36:05-root-INFO: Loss Change: 19433.088 -> 14979.250
2024-12-01-20:36:05-root-INFO: Regularization Change: 0.000 -> 1.588
2024-12-01-20:36:05-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-01-20:36:05-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-20:36:05-root-INFO: step: 244 lr_xt 0.00024712
2024-12-01-20:36:05-root-INFO: grad norm: 4396.722 3278.780 2929.294
2024-12-01-20:36:05-root-INFO: Loss too large (14994.676->17919.145)! Learning rate decreased to 0.00020.
2024-12-01-20:36:06-root-INFO: Loss too large (14994.676->16047.733)! Learning rate decreased to 0.00016.
2024-12-01-20:36:06-root-INFO: Loss too large (14994.676->15011.651)! Learning rate decreased to 0.00013.
2024-12-01-20:36:06-root-INFO: grad norm: 3317.039 2588.250 2074.539
2024-12-01-20:36:07-root-INFO: grad norm: 2778.352 2117.493 1798.739
2024-12-01-20:36:07-root-INFO: grad norm: 2398.024 1960.138 1381.441
2024-12-01-20:36:08-root-INFO: grad norm: 2165.361 1675.254 1371.974
2024-12-01-20:36:08-root-INFO: Loss Change: 14994.676 -> 13860.919
2024-12-01-20:36:08-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-01-20:36:08-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-01-20:36:08-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:08-root-INFO: step: 243 lr_xt 0.00026017
2024-12-01-20:36:08-root-INFO: grad norm: 2049.837 1668.897 1190.216
2024-12-01-20:36:08-root-INFO: Loss too large (13641.959->14122.090)! Learning rate decreased to 0.00021.
2024-12-01-20:36:09-root-INFO: Loss too large (13641.959->13781.281)! Learning rate decreased to 0.00017.
2024-12-01-20:36:09-root-INFO: grad norm: 2485.582 1911.930 1588.281
2024-12-01-20:36:09-root-INFO: Loss too large (13594.010->13625.799)! Learning rate decreased to 0.00013.
2024-12-01-20:36:10-root-INFO: grad norm: 2158.995 1807.846 1180.233
2024-12-01-20:36:10-root-INFO: grad norm: 1946.025 1517.006 1218.896
2024-12-01-20:36:11-root-INFO: grad norm: 1779.198 1520.840 923.360
2024-12-01-20:36:11-root-INFO: Loss Change: 13641.959 -> 13145.212
2024-12-01-20:36:11-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-01-20:36:11-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-01-20:36:11-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:11-root-INFO: step: 242 lr_xt 0.00027387
2024-12-01-20:36:11-root-INFO: grad norm: 1499.641 1203.014 895.365
2024-12-01-20:36:11-root-INFO: Loss too large (12914.379->13101.977)! Learning rate decreased to 0.00022.
2024-12-01-20:36:12-root-INFO: Loss too large (12914.379->12933.812)! Learning rate decreased to 0.00018.
2024-12-01-20:36:12-root-INFO: grad norm: 1855.849 1585.499 964.557
2024-12-01-20:36:13-root-INFO: grad norm: 2404.076 1919.957 1446.839
2024-12-01-20:36:13-root-INFO: Loss too large (12836.442->12903.656)! Learning rate decreased to 0.00014.
2024-12-01-20:36:13-root-INFO: grad norm: 2162.859 1841.020 1135.167
2024-12-01-20:36:14-root-INFO: grad norm: 1961.851 1564.451 1183.788
2024-12-01-20:36:14-root-INFO: Loss Change: 12914.379 -> 12513.196
2024-12-01-20:36:14-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-01-20:36:14-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-01-20:36:14-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:14-root-INFO: step: 241 lr_xt 0.00028824
2024-12-01-20:36:14-root-INFO: grad norm: 1852.038 1574.635 974.971
2024-12-01-20:36:14-root-INFO: Loss too large (12432.264->12947.928)! Learning rate decreased to 0.00023.
2024-12-01-20:36:15-root-INFO: Loss too large (12432.264->12608.324)! Learning rate decreased to 0.00018.
2024-12-01-20:36:15-root-INFO: grad norm: 2345.355 1898.840 1376.625
2024-12-01-20:36:15-root-INFO: Loss too large (12420.271->12471.741)! Learning rate decreased to 0.00015.
2024-12-01-20:36:16-root-INFO: grad norm: 2043.154 1738.357 1073.588
2024-12-01-20:36:16-root-INFO: grad norm: 1798.739 1456.027 1056.147
2024-12-01-20:36:16-root-INFO: grad norm: 1587.969 1364.640 812.038
2024-12-01-20:36:17-root-INFO: Loss Change: 12432.264 -> 11988.894
2024-12-01-20:36:17-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-01-20:36:17-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-01-20:36:17-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:17-root-INFO: step: 240 lr_xt 0.00030331
2024-12-01-20:36:17-root-INFO: grad norm: 1390.320 1153.754 775.784
2024-12-01-20:36:17-root-INFO: Loss too large (11809.606->11961.281)! Learning rate decreased to 0.00024.
2024-12-01-20:36:17-root-INFO: Loss too large (11809.606->11811.414)! Learning rate decreased to 0.00019.
2024-12-01-20:36:18-root-INFO: grad norm: 1634.653 1400.729 842.643
2024-12-01-20:36:18-root-INFO: grad norm: 2001.371 1634.718 1154.636
2024-12-01-20:36:19-root-INFO: Loss too large (11700.780->11704.350)! Learning rate decreased to 0.00016.
2024-12-01-20:36:19-root-INFO: grad norm: 1685.782 1447.684 863.755
2024-12-01-20:36:20-root-INFO: grad norm: 1441.970 1181.493 826.651
2024-12-01-20:36:20-root-INFO: Loss Change: 11809.606 -> 11385.151
2024-12-01-20:36:20-root-INFO: Regularization Change: 0.000 -> 0.386
2024-12-01-20:36:20-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-20:36:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:20-root-INFO: step: 239 lr_xt 0.00031912
2024-12-01-20:36:20-root-INFO: grad norm: 1175.377 1021.760 580.964
2024-12-01-20:36:20-root-INFO: Loss too large (11295.194->11372.466)! Learning rate decreased to 0.00026.
2024-12-01-20:36:21-root-INFO: grad norm: 1834.959 1507.531 1046.148
2024-12-01-20:36:21-root-INFO: Loss too large (11274.822->11457.830)! Learning rate decreased to 0.00020.
2024-12-01-20:36:22-root-INFO: grad norm: 2210.668 1876.092 1169.329
2024-12-01-20:36:22-root-INFO: Loss too large (11253.338->11277.588)! Learning rate decreased to 0.00016.
2024-12-01-20:36:22-root-INFO: grad norm: 1789.689 1477.467 1009.989
2024-12-01-20:36:23-root-INFO: grad norm: 1459.574 1256.675 742.378
2024-12-01-20:36:23-root-INFO: Loss Change: 11295.194 -> 10897.782
2024-12-01-20:36:23-root-INFO: Regularization Change: 0.000 -> 0.419
2024-12-01-20:36:23-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-20:36:23-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:23-root-INFO: step: 238 lr_xt 0.00033570
2024-12-01-20:36:23-root-INFO: grad norm: 1335.646 1134.961 704.141
2024-12-01-20:36:23-root-INFO: Loss too large (10686.432->10796.453)! Learning rate decreased to 0.00027.
2024-12-01-20:36:24-root-INFO: grad norm: 1954.017 1669.684 1015.057
2024-12-01-20:36:24-root-INFO: Loss too large (10659.254->10881.997)! Learning rate decreased to 0.00021.
2024-12-01-20:36:25-root-INFO: grad norm: 2237.977 1862.256 1241.186
2024-12-01-20:36:25-root-INFO: Loss too large (10639.581->10642.434)! Learning rate decreased to 0.00017.
2024-12-01-20:36:25-root-INFO: grad norm: 1694.370 1463.067 854.590
2024-12-01-20:36:26-root-INFO: grad norm: 1321.273 1108.105 719.628
2024-12-01-20:36:26-root-INFO: Loss Change: 10686.432 -> 10261.639
2024-12-01-20:36:26-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-01-20:36:26-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-20:36:26-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:26-root-INFO: step: 237 lr_xt 0.00035308
2024-12-01-20:36:26-root-INFO: grad norm: 914.900 805.015 434.733
2024-12-01-20:36:27-root-INFO: grad norm: 1641.709 1372.752 900.422
2024-12-01-20:36:27-root-INFO: Loss too large (10145.109->10577.070)! Learning rate decreased to 0.00028.
2024-12-01-20:36:27-root-INFO: Loss too large (10145.109->10268.340)! Learning rate decreased to 0.00023.
2024-12-01-20:36:28-root-INFO: grad norm: 1812.548 1570.445 905.004
2024-12-01-20:36:28-root-INFO: grad norm: 2012.235 1694.107 1085.859
2024-12-01-20:36:28-root-INFO: grad norm: 2241.592 1933.701 1133.815
2024-12-01-20:36:29-root-INFO: Loss too large (10054.955->10056.112)! Learning rate decreased to 0.00018.
2024-12-01-20:36:29-root-INFO: Loss Change: 10169.189 -> 9875.222
2024-12-01-20:36:29-root-INFO: Regularization Change: 0.000 -> 0.531
2024-12-01-20:36:29-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-01-20:36:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:29-root-INFO: step: 236 lr_xt 0.00037130
2024-12-01-20:36:29-root-INFO: grad norm: 1537.606 1316.067 795.110
2024-12-01-20:36:29-root-INFO: Loss too large (9730.809->10098.486)! Learning rate decreased to 0.00030.
2024-12-01-20:36:30-root-INFO: Loss too large (9730.809->9825.455)! Learning rate decreased to 0.00024.
2024-12-01-20:36:30-root-INFO: grad norm: 1650.127 1426.751 829.036
2024-12-01-20:36:31-root-INFO: grad norm: 1773.951 1507.038 935.809
2024-12-01-20:36:31-root-INFO: grad norm: 1906.630 1648.720 957.582
2024-12-01-20:36:32-root-INFO: grad norm: 2044.277 1734.045 1082.661
2024-12-01-20:36:32-root-INFO: Loss Change: 9730.809 -> 9537.475
2024-12-01-20:36:32-root-INFO: Regularization Change: 0.000 -> 0.565
2024-12-01-20:36:32-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-01-20:36:32-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:32-root-INFO: step: 235 lr_xt 0.00039040
2024-12-01-20:36:32-root-INFO: grad norm: 1852.466 1607.484 920.665
2024-12-01-20:36:32-root-INFO: Loss too large (9370.986->9980.109)! Learning rate decreased to 0.00031.
2024-12-01-20:36:33-root-INFO: Loss too large (9370.986->9550.613)! Learning rate decreased to 0.00025.
2024-12-01-20:36:33-root-INFO: grad norm: 1883.621 1607.906 981.156
2024-12-01-20:36:34-root-INFO: grad norm: 1923.902 1678.828 939.646
2024-12-01-20:36:34-root-INFO: grad norm: 1959.839 1672.436 1021.726
2024-12-01-20:36:35-root-INFO: grad norm: 1991.464 1737.284 973.538
2024-12-01-20:36:35-root-INFO: Loss Change: 9370.986 -> 9114.242
2024-12-01-20:36:35-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-01-20:36:35-root-INFO: Undo step: 235
2024-12-01-20:36:35-root-INFO: Undo step: 236
2024-12-01-20:36:35-root-INFO: Undo step: 237
2024-12-01-20:36:35-root-INFO: Undo step: 238
2024-12-01-20:36:35-root-INFO: Undo step: 239
2024-12-01-20:36:35-root-INFO: step: 240 lr_xt 0.00030331
2024-12-01-20:36:35-root-INFO: grad norm: 6987.094 5678.272 4071.452
2024-12-01-20:36:35-root-INFO: Loss too large (13557.491->18150.820)! Learning rate decreased to 0.00024.
2024-12-01-20:36:36-root-INFO: Loss too large (13557.491->14930.365)! Learning rate decreased to 0.00019.
2024-12-01-20:36:36-root-INFO: grad norm: 6355.609 5101.132 3791.072
2024-12-01-20:36:37-root-INFO: grad norm: 6687.867 5556.727 3721.606
2024-12-01-20:36:37-root-INFO: Loss too large (12622.811->13188.209)! Learning rate decreased to 0.00016.
2024-12-01-20:36:37-root-INFO: grad norm: 4849.267 4008.116 2729.541
2024-12-01-20:36:38-root-INFO: grad norm: 3508.645 2945.310 1906.762
2024-12-01-20:36:38-root-INFO: Loss Change: 13557.491 -> 10740.635
2024-12-01-20:36:38-root-INFO: Regularization Change: 0.000 -> 1.402
2024-12-01-20:36:38-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-20:36:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:38-root-INFO: step: 239 lr_xt 0.00031912
2024-12-01-20:36:38-root-INFO: grad norm: 2620.733 2169.070 1470.843
2024-12-01-20:36:39-root-INFO: Loss too large (10613.604->11912.334)! Learning rate decreased to 0.00026.
2024-12-01-20:36:39-root-INFO: Loss too large (10613.604->11090.913)! Learning rate decreased to 0.00020.
2024-12-01-20:36:39-root-INFO: Loss too large (10613.604->10635.540)! Learning rate decreased to 0.00016.
2024-12-01-20:36:39-root-INFO: grad norm: 1939.015 1651.439 1016.135
2024-12-01-20:36:40-root-INFO: grad norm: 1471.529 1220.678 821.792
2024-12-01-20:36:40-root-INFO: grad norm: 1135.042 990.691 553.941
2024-12-01-20:36:41-root-INFO: grad norm: 906.948 764.518 487.920
2024-12-01-20:36:41-root-INFO: Loss Change: 10613.604 -> 10041.071
2024-12-01-20:36:41-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-01-20:36:41-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-20:36:41-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:41-root-INFO: step: 238 lr_xt 0.00033570
2024-12-01-20:36:41-root-INFO: grad norm: 848.543 716.599 454.435
2024-12-01-20:36:42-root-INFO: grad norm: 1129.816 910.641 668.742
2024-12-01-20:36:42-root-INFO: Loss too large (9753.246->9860.634)! Learning rate decreased to 0.00027.
2024-12-01-20:36:43-root-INFO: grad norm: 1625.752 1366.193 881.241
2024-12-01-20:36:43-root-INFO: Loss too large (9753.212->9866.355)! Learning rate decreased to 0.00021.
2024-12-01-20:36:43-root-INFO: grad norm: 1697.220 1395.681 965.728
2024-12-01-20:36:44-root-INFO: grad norm: 1804.978 1538.031 944.673
2024-12-01-20:36:44-root-INFO: Loss Change: 9833.734 -> 9655.095
2024-12-01-20:36:44-root-INFO: Regularization Change: 0.000 -> 0.475
2024-12-01-20:36:44-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-20:36:44-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:44-root-INFO: step: 237 lr_xt 0.00035308
2024-12-01-20:36:44-root-INFO: grad norm: 2045.355 1708.683 1124.224
2024-12-01-20:36:44-root-INFO: Loss too large (9606.042->10324.123)! Learning rate decreased to 0.00028.
2024-12-01-20:36:45-root-INFO: Loss too large (9606.042->9825.194)! Learning rate decreased to 0.00023.
2024-12-01-20:36:45-root-INFO: grad norm: 2090.682 1793.971 1073.601
2024-12-01-20:36:46-root-INFO: grad norm: 2155.216 1809.716 1170.421
2024-12-01-20:36:46-root-INFO: grad norm: 2230.979 1919.109 1137.670
2024-12-01-20:36:46-root-INFO: grad norm: 2307.386 1944.054 1242.853
2024-12-01-20:36:47-root-INFO: Loss Change: 9606.042 -> 9431.711
2024-12-01-20:36:47-root-INFO: Regularization Change: 0.000 -> 0.513
2024-12-01-20:36:47-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-01-20:36:47-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:47-root-INFO: step: 236 lr_xt 0.00037130
2024-12-01-20:36:47-root-INFO: grad norm: 2246.740 1922.372 1162.895
2024-12-01-20:36:47-root-INFO: Loss too large (9269.735->10196.348)! Learning rate decreased to 0.00030.
2024-12-01-20:36:47-root-INFO: Loss too large (9269.735->9574.826)! Learning rate decreased to 0.00024.
2024-12-01-20:36:48-root-INFO: grad norm: 2255.861 1911.870 1197.357
2024-12-01-20:36:48-root-INFO: grad norm: 2266.530 1946.993 1160.335
2024-12-01-20:36:49-root-INFO: grad norm: 2271.412 1925.301 1205.209
2024-12-01-20:36:49-root-INFO: grad norm: 2272.052 1953.398 1160.368
2024-12-01-20:36:50-root-INFO: Loss Change: 9269.735 -> 9048.217
2024-12-01-20:36:50-root-INFO: Regularization Change: 0.000 -> 0.561
2024-12-01-20:36:50-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-01-20:36:50-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-20:36:50-root-INFO: step: 235 lr_xt 0.00039040
2024-12-01-20:36:50-root-INFO: grad norm: 2403.070 2052.984 1249.000
2024-12-01-20:36:50-root-INFO: Loss too large (9011.938->10061.098)! Learning rate decreased to 0.00031.
2024-12-01-20:36:50-root-INFO: Loss too large (9011.938->9313.067)! Learning rate decreased to 0.00025.
2024-12-01-20:36:51-root-INFO: grad norm: 2269.000 1965.882 1132.991
2024-12-01-20:36:51-root-INFO: grad norm: 2162.522 1844.526 1128.817
2024-12-01-20:36:52-root-INFO: grad norm: 2058.420 1787.469 1020.808
2024-12-01-20:36:52-root-INFO: grad norm: 1963.424 1676.648 1021.707
2024-12-01-20:36:52-root-INFO: Loss Change: 9011.938 -> 8627.965
2024-12-01-20:36:52-root-INFO: Regularization Change: 0.000 -> 0.573
2024-12-01-20:36:52-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-01-20:36:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-20:36:53-root-INFO: step: 234 lr_xt 0.00041042
2024-12-01-20:36:53-root-INFO: grad norm: 1704.280 1480.959 843.404
2024-12-01-20:36:53-root-INFO: Loss too large (8538.656->9020.258)! Learning rate decreased to 0.00033.
2024-12-01-20:36:53-root-INFO: Loss too large (8538.656->8661.580)! Learning rate decreased to 0.00026.
2024-12-01-20:36:53-root-INFO: grad norm: 1571.521 1348.104 807.647
2024-12-01-20:36:54-root-INFO: grad norm: 1447.084 1265.301 702.186
2024-12-01-20:36:54-root-INFO: grad norm: 1333.922 1147.284 680.505
2024-12-01-20:36:55-root-INFO: grad norm: 1227.385 1078.693 585.571
2024-12-01-20:36:55-root-INFO: Loss Change: 8538.656 -> 8212.289
2024-12-01-20:36:55-root-INFO: Regularization Change: 0.000 -> 0.426
2024-12-01-20:36:55-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-20:36:55-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:36:55-root-INFO: step: 233 lr_xt 0.00043139
2024-12-01-20:36:55-root-INFO: grad norm: 1195.893 1038.087 593.747
2024-12-01-20:36:56-root-INFO: Loss too large (8212.485->8357.639)! Learning rate decreased to 0.00035.
2024-12-01-20:36:56-root-INFO: grad norm: 1511.636 1326.160 725.495
2024-12-01-20:36:56-root-INFO: Loss too large (8204.351->8270.266)! Learning rate decreased to 0.00028.
2024-12-01-20:36:57-root-INFO: grad norm: 1325.386 1146.385 665.170
2024-12-01-20:36:57-root-INFO: grad norm: 1162.148 1026.124 545.581
2024-12-01-20:36:58-root-INFO: grad norm: 1029.235 894.605 508.928
2024-12-01-20:36:58-root-INFO: Loss Change: 8212.485 -> 7922.070
2024-12-01-20:36:58-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-01-20:36:58-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-01-20:36:58-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:36:58-root-INFO: step: 232 lr_xt 0.00045336
2024-12-01-20:36:58-root-INFO: grad norm: 676.395 597.114 317.748
2024-12-01-20:36:59-root-INFO: grad norm: 931.931 794.763 486.669
2024-12-01-20:36:59-root-INFO: Loss too large (7685.169->7744.987)! Learning rate decreased to 0.00036.
2024-12-01-20:37:00-root-INFO: grad norm: 1125.169 999.798 516.148
2024-12-01-20:37:00-root-INFO: Loss too large (7659.830->7666.799)! Learning rate decreased to 0.00029.
2024-12-01-20:37:00-root-INFO: grad norm: 956.689 832.701 471.024
2024-12-01-20:37:01-root-INFO: grad norm: 825.002 743.817 356.882
2024-12-01-20:37:01-root-INFO: Loss Change: 7729.008 -> 7480.801
2024-12-01-20:37:01-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-01-20:37:01-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-01-20:37:01-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:01-root-INFO: step: 231 lr_xt 0.00047637
2024-12-01-20:37:01-root-INFO: grad norm: 817.274 719.025 388.508
2024-12-01-20:37:02-root-INFO: Loss too large (7427.880->7472.934)! Learning rate decreased to 0.00038.
2024-12-01-20:37:02-root-INFO: grad norm: 999.781 895.122 445.332
2024-12-01-20:37:02-root-INFO: grad norm: 1235.697 1078.572 603.017
2024-12-01-20:37:03-root-INFO: Loss too large (7404.414->7427.560)! Learning rate decreased to 0.00030.
2024-12-01-20:37:03-root-INFO: grad norm: 1026.456 916.906 461.406
2024-12-01-20:37:04-root-INFO: grad norm: 866.432 760.228 415.642
2024-12-01-20:37:04-root-INFO: Loss Change: 7427.880 -> 7210.305
2024-12-01-20:37:04-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-01-20:37:04-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-01-20:37:04-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:04-root-INFO: step: 230 lr_xt 0.00050047
2024-12-01-20:37:04-root-INFO: grad norm: 523.479 482.679 202.609
2024-12-01-20:37:05-root-INFO: grad norm: 765.504 670.446 369.456
2024-12-01-20:37:05-root-INFO: Loss too large (7068.193->7107.691)! Learning rate decreased to 0.00040.
2024-12-01-20:37:05-root-INFO: grad norm: 918.083 824.526 403.772
2024-12-01-20:37:06-root-INFO: grad norm: 1113.052 975.745 535.544
2024-12-01-20:37:06-root-INFO: Loss too large (7041.169->7053.509)! Learning rate decreased to 0.00032.
2024-12-01-20:37:06-root-INFO: grad norm: 904.875 811.458 400.419
2024-12-01-20:37:07-root-INFO: Loss Change: 7097.652 -> 6914.686
2024-12-01-20:37:07-root-INFO: Regularization Change: 0.000 -> 0.446
2024-12-01-20:37:07-root-INFO: Undo step: 230
2024-12-01-20:37:07-root-INFO: Undo step: 231
2024-12-01-20:37:07-root-INFO: Undo step: 232
2024-12-01-20:37:07-root-INFO: Undo step: 233
2024-12-01-20:37:07-root-INFO: Undo step: 234
2024-12-01-20:37:07-root-INFO: step: 235 lr_xt 0.00039040
2024-12-01-20:37:07-root-INFO: grad norm: 9461.576 6982.604 6384.722
2024-12-01-20:37:08-root-INFO: grad norm: 8642.189 6421.262 5784.014
2024-12-01-20:37:08-root-INFO: grad norm: 8062.148 6187.785 5168.129
2024-12-01-20:37:08-root-INFO: Loss too large (15712.580->16599.994)! Learning rate decreased to 0.00031.
2024-12-01-20:37:09-root-INFO: grad norm: 5988.811 4892.673 3453.637
2024-12-01-20:37:09-root-INFO: grad norm: 5863.354 4882.546 3246.486
2024-12-01-20:37:09-root-INFO: Loss too large (11039.378->11914.522)! Learning rate decreased to 0.00025.
2024-12-01-20:37:10-root-INFO: Loss Change: 18258.596 -> 9931.287
2024-12-01-20:37:10-root-INFO: Regularization Change: 0.000 -> 6.292
2024-12-01-20:37:10-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-01-20:37:10-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-20:37:10-root-INFO: step: 234 lr_xt 0.00041042
2024-12-01-20:37:10-root-INFO: grad norm: 4379.059 3696.434 2347.879
2024-12-01-20:37:10-root-INFO: Loss too large (9798.824->12636.374)! Learning rate decreased to 0.00033.
2024-12-01-20:37:10-root-INFO: Loss too large (9798.824->10286.805)! Learning rate decreased to 0.00026.
2024-12-01-20:37:11-root-INFO: grad norm: 3527.084 3005.463 1845.945
2024-12-01-20:37:11-root-INFO: grad norm: 3058.738 2592.764 1622.792
2024-12-01-20:37:12-root-INFO: grad norm: 2689.967 2306.485 1384.214
2024-12-01-20:37:12-root-INFO: grad norm: 2410.638 2048.840 1270.210
2024-12-01-20:37:12-root-INFO: Loss Change: 9798.824 -> 8170.475
2024-12-01-20:37:12-root-INFO: Regularization Change: 0.000 -> 1.481
2024-12-01-20:37:12-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-20:37:12-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:13-root-INFO: step: 233 lr_xt 0.00043139
2024-12-01-20:37:13-root-INFO: grad norm: 2037.521 1743.671 1054.091
2024-12-01-20:37:13-root-INFO: Loss too large (8128.449->8734.004)! Learning rate decreased to 0.00035.
2024-12-01-20:37:13-root-INFO: Loss too large (8128.449->8232.150)! Learning rate decreased to 0.00028.
2024-12-01-20:37:14-root-INFO: grad norm: 1742.099 1491.892 899.538
2024-12-01-20:37:14-root-INFO: grad norm: 1520.827 1315.566 763.021
2024-12-01-20:37:15-root-INFO: grad norm: 1341.910 1154.465 684.057
2024-12-01-20:37:15-root-INFO: grad norm: 1187.475 1031.570 588.184
2024-12-01-20:37:15-root-INFO: Loss Change: 8128.449 -> 7590.901
2024-12-01-20:37:15-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-01-20:37:15-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-01-20:37:15-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:16-root-INFO: step: 232 lr_xt 0.00045336
2024-12-01-20:37:16-root-INFO: grad norm: 1322.239 1131.953 683.373
2024-12-01-20:37:16-root-INFO: Loss too large (7488.627->7660.965)! Learning rate decreased to 0.00036.
2024-12-01-20:37:16-root-INFO: grad norm: 1582.548 1380.275 774.144
2024-12-01-20:37:17-root-INFO: Loss too large (7468.725->7530.996)! Learning rate decreased to 0.00029.
2024-12-01-20:37:17-root-INFO: grad norm: 1323.141 1143.567 665.550
2024-12-01-20:37:17-root-INFO: grad norm: 1112.490 975.767 534.334
2024-12-01-20:37:18-root-INFO: grad norm: 948.846 827.602 464.095
2024-12-01-20:37:18-root-INFO: Loss Change: 7488.627 -> 7154.124
2024-12-01-20:37:18-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-01-20:37:18-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-01-20:37:18-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:18-root-INFO: step: 231 lr_xt 0.00047637
2024-12-01-20:37:19-root-INFO: grad norm: 692.262 615.452 316.931
2024-12-01-20:37:19-root-INFO: Loss too large (7043.119->7056.773)! Learning rate decreased to 0.00038.
2024-12-01-20:37:19-root-INFO: grad norm: 821.233 720.098 394.820
2024-12-01-20:37:20-root-INFO: grad norm: 1005.170 885.323 475.993
2024-12-01-20:37:20-root-INFO: Loss too large (7001.304->7007.577)! Learning rate decreased to 0.00030.
2024-12-01-20:37:20-root-INFO: grad norm: 837.832 736.372 399.650
2024-12-01-20:37:21-root-INFO: grad norm: 703.954 626.216 321.566
2024-12-01-20:37:21-root-INFO: Loss Change: 7043.119 -> 6854.128
2024-12-01-20:37:21-root-INFO: Regularization Change: 0.000 -> 0.340
2024-12-01-20:37:21-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-01-20:37:21-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:21-root-INFO: step: 230 lr_xt 0.00050047
2024-12-01-20:37:21-root-INFO: grad norm: 762.216 670.166 363.113
2024-12-01-20:37:21-root-INFO: Loss too large (6792.109->6830.765)! Learning rate decreased to 0.00040.
2024-12-01-20:37:22-root-INFO: grad norm: 879.249 781.436 403.035
2024-12-01-20:37:22-root-INFO: grad norm: 1050.141 921.181 504.204
2024-12-01-20:37:23-root-INFO: Loss too large (6765.363->6775.609)! Learning rate decreased to 0.00032.
2024-12-01-20:37:23-root-INFO: grad norm: 834.542 740.412 385.032
2024-12-01-20:37:24-root-INFO: grad norm: 675.013 597.781 313.529
2024-12-01-20:37:24-root-INFO: Loss Change: 6792.109 -> 6613.785
2024-12-01-20:37:24-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-01-20:37:24-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-01-20:37:24-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:24-root-INFO: step: 229 lr_xt 0.00052570
2024-12-01-20:37:24-root-INFO: grad norm: 486.689 431.681 224.760
2024-12-01-20:37:25-root-INFO: grad norm: 543.102 474.103 264.926
2024-12-01-20:37:25-root-INFO: grad norm: 793.568 704.728 364.840
2024-12-01-20:37:25-root-INFO: Loss too large (6496.395->6559.024)! Learning rate decreased to 0.00042.
2024-12-01-20:37:26-root-INFO: grad norm: 915.810 810.626 426.136
2024-12-01-20:37:26-root-INFO: grad norm: 1082.237 959.758 500.102
2024-12-01-20:37:27-root-INFO: Loss too large (6482.955->6500.180)! Learning rate decreased to 0.00034.
2024-12-01-20:37:27-root-INFO: Loss Change: 6556.130 -> 6415.885
2024-12-01-20:37:27-root-INFO: Regularization Change: 0.000 -> 0.456
2024-12-01-20:37:27-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-01-20:37:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:27-root-INFO: step: 228 lr_xt 0.00055211
2024-12-01-20:37:27-root-INFO: grad norm: 946.586 836.371 443.294
2024-12-01-20:37:27-root-INFO: Loss too large (6417.167->6527.109)! Learning rate decreased to 0.00044.
2024-12-01-20:37:28-root-INFO: grad norm: 1065.598 945.782 490.912
2024-12-01-20:37:28-root-INFO: Loss too large (6407.093->6415.136)! Learning rate decreased to 0.00035.
2024-12-01-20:37:29-root-INFO: grad norm: 794.205 706.403 362.983
2024-12-01-20:37:29-root-INFO: grad norm: 601.410 539.680 265.405
2024-12-01-20:37:30-root-INFO: grad norm: 473.420 426.623 205.230
2024-12-01-20:37:30-root-INFO: Loss Change: 6417.167 -> 6213.848
2024-12-01-20:37:30-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-01-20:37:30-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-01-20:37:30-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:30-root-INFO: step: 227 lr_xt 0.00057976
2024-12-01-20:37:30-root-INFO: grad norm: 435.550 377.818 216.697
2024-12-01-20:37:31-root-INFO: grad norm: 359.917 315.524 173.160
2024-12-01-20:37:31-root-INFO: grad norm: 343.976 305.031 158.984
2024-12-01-20:37:32-root-INFO: grad norm: 369.099 326.984 171.219
2024-12-01-20:37:32-root-INFO: grad norm: 452.945 399.084 214.222
2024-12-01-20:37:32-root-INFO: Loss Change: 6230.241 -> 6038.504
2024-12-01-20:37:32-root-INFO: Regularization Change: 0.000 -> 0.651
2024-12-01-20:37:32-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-01-20:37:32-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-20:37:33-root-INFO: step: 226 lr_xt 0.00060869
2024-12-01-20:37:33-root-INFO: grad norm: 693.409 593.624 358.367
2024-12-01-20:37:33-root-INFO: Loss too large (5998.534->6013.774)! Learning rate decreased to 0.00049.
2024-12-01-20:37:33-root-INFO: grad norm: 665.234 583.216 319.993
2024-12-01-20:37:34-root-INFO: grad norm: 685.707 610.906 311.430
2024-12-01-20:37:34-root-INFO: grad norm: 727.713 646.188 334.676
2024-12-01-20:37:35-root-INFO: grad norm: 782.810 700.775 348.863
2024-12-01-20:37:35-root-INFO: Loss Change: 5998.534 -> 5884.291
2024-12-01-20:37:35-root-INFO: Regularization Change: 0.000 -> 0.451
2024-12-01-20:37:35-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-01-20:37:35-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:37:35-root-INFO: step: 225 lr_xt 0.00063896
2024-12-01-20:37:35-root-INFO: grad norm: 1113.311 981.597 525.289
2024-12-01-20:37:36-root-INFO: Loss too large (5920.069->6054.235)! Learning rate decreased to 0.00051.
2024-12-01-20:37:36-root-INFO: grad norm: 1136.361 1022.393 496.014
2024-12-01-20:37:37-root-INFO: grad norm: 1200.570 1069.021 546.409
2024-12-01-20:37:37-root-INFO: grad norm: 1274.864 1143.548 563.540
2024-12-01-20:37:37-root-INFO: Loss too large (5857.058->5863.300)! Learning rate decreased to 0.00041.
2024-12-01-20:37:38-root-INFO: grad norm: 858.178 765.254 388.401
2024-12-01-20:37:38-root-INFO: Loss Change: 5920.069 -> 5658.738
2024-12-01-20:37:38-root-INFO: Regularization Change: 0.000 -> 0.502
2024-12-01-20:37:38-root-INFO: Undo step: 225
2024-12-01-20:37:38-root-INFO: Undo step: 226
2024-12-01-20:37:38-root-INFO: Undo step: 227
2024-12-01-20:37:38-root-INFO: Undo step: 228
2024-12-01-20:37:38-root-INFO: Undo step: 229
2024-12-01-20:37:38-root-INFO: step: 230 lr_xt 0.00050047
2024-12-01-20:37:38-root-INFO: grad norm: 3686.499 2804.646 2392.537
2024-12-01-20:37:39-root-INFO: grad norm: 3114.825 2358.524 2034.576
2024-12-01-20:37:39-root-INFO: grad norm: 2872.554 2186.487 1863.019
2024-12-01-20:37:40-root-INFO: grad norm: 3176.234 2477.802 1987.199
2024-12-01-20:37:40-root-INFO: Loss too large (7791.264->8255.245)! Learning rate decreased to 0.00040.
2024-12-01-20:37:40-root-INFO: grad norm: 2736.436 2313.302 1461.751
2024-12-01-20:37:41-root-INFO: Loss Change: 9171.436 -> 7264.568
2024-12-01-20:37:41-root-INFO: Regularization Change: 0.000 -> 3.812
2024-12-01-20:37:41-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-01-20:37:41-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:41-root-INFO: step: 229 lr_xt 0.00052570
2024-12-01-20:37:41-root-INFO: grad norm: 2646.745 2288.233 1330.131
2024-12-01-20:37:41-root-INFO: Loss too large (7035.730->8084.073)! Learning rate decreased to 0.00042.
2024-12-01-20:37:41-root-INFO: Loss too large (7035.730->7136.537)! Learning rate decreased to 0.00034.
2024-12-01-20:37:42-root-INFO: grad norm: 1887.764 1676.768 867.238
2024-12-01-20:37:42-root-INFO: grad norm: 1425.628 1259.850 667.228
2024-12-01-20:37:43-root-INFO: grad norm: 1111.248 994.611 495.601
2024-12-01-20:37:43-root-INFO: grad norm: 871.256 771.624 404.579
2024-12-01-20:37:44-root-INFO: Loss Change: 7035.730 -> 6188.505
2024-12-01-20:37:44-root-INFO: Regularization Change: 0.000 -> 0.734
2024-12-01-20:37:44-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-01-20:37:44-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:44-root-INFO: step: 228 lr_xt 0.00055211
2024-12-01-20:37:44-root-INFO: grad norm: 816.796 731.425 363.557
2024-12-01-20:37:44-root-INFO: Loss too large (6165.888->6225.791)! Learning rate decreased to 0.00044.
2024-12-01-20:37:45-root-INFO: grad norm: 925.321 820.455 427.871
2024-12-01-20:37:45-root-INFO: grad norm: 1065.474 956.492 469.423
2024-12-01-20:37:45-root-INFO: Loss too large (6139.036->6141.929)! Learning rate decreased to 0.00035.
2024-12-01-20:37:46-root-INFO: grad norm: 807.151 717.509 369.694
2024-12-01-20:37:46-root-INFO: grad norm: 631.630 572.724 266.353
2024-12-01-20:37:46-root-INFO: Loss Change: 6165.888 -> 5966.241
2024-12-01-20:37:46-root-INFO: Regularization Change: 0.000 -> 0.365
2024-12-01-20:37:46-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-01-20:37:46-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-20:37:47-root-INFO: step: 227 lr_xt 0.00057976
2024-12-01-20:37:47-root-INFO: grad norm: 424.229 365.167 215.924
2024-12-01-20:37:47-root-INFO: grad norm: 372.581 329.237 174.412
2024-12-01-20:37:48-root-INFO: grad norm: 365.563 319.246 178.096
2024-12-01-20:37:48-root-INFO: grad norm: 402.436 357.783 184.245
2024-12-01-20:37:49-root-INFO: grad norm: 514.621 450.485 248.794
2024-12-01-20:37:49-root-INFO: Loss Change: 5941.451 -> 5760.577
2024-12-01-20:37:49-root-INFO: Regularization Change: 0.000 -> 0.689
2024-12-01-20:37:49-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-01-20:37:49-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-20:37:49-root-INFO: step: 226 lr_xt 0.00060869
2024-12-01-20:37:49-root-INFO: grad norm: 794.139 688.273 396.152
2024-12-01-20:37:49-root-INFO: Loss too large (5712.819->5761.532)! Learning rate decreased to 0.00049.
2024-12-01-20:37:50-root-INFO: grad norm: 822.642 723.647 391.248
2024-12-01-20:37:50-root-INFO: grad norm: 897.383 802.279 402.051
2024-12-01-20:37:51-root-INFO: grad norm: 1004.342 898.013 449.750
2024-12-01-20:37:51-root-INFO: Loss too large (5660.054->5666.156)! Learning rate decreased to 0.00039.
2024-12-01-20:37:51-root-INFO: grad norm: 734.346 661.757 318.343
2024-12-01-20:37:52-root-INFO: Loss Change: 5712.819 -> 5536.228
2024-12-01-20:37:52-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-01-20:37:52-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-01-20:37:52-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:37:52-root-INFO: step: 225 lr_xt 0.00063896
2024-12-01-20:37:52-root-INFO: grad norm: 416.518 363.244 203.814
2024-12-01-20:37:52-root-INFO: grad norm: 369.750 331.263 164.255
2024-12-01-20:37:53-root-INFO: grad norm: 414.639 376.258 174.227
2024-12-01-20:37:53-root-INFO: grad norm: 568.958 514.729 242.418
2024-12-01-20:37:54-root-INFO: Loss too large (5369.634->5392.881)! Learning rate decreased to 0.00051.
2024-12-01-20:37:54-root-INFO: grad norm: 624.884 568.243 259.961
2024-12-01-20:37:54-root-INFO: Loss Change: 5473.299 -> 5339.769
2024-12-01-20:37:54-root-INFO: Regularization Change: 0.000 -> 0.543
2024-12-01-20:37:54-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-01-20:37:54-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:37:54-root-INFO: step: 224 lr_xt 0.00067063
2024-12-01-20:37:55-root-INFO: grad norm: 896.053 803.409 396.792
2024-12-01-20:37:55-root-INFO: Loss too large (5355.114->5473.871)! Learning rate decreased to 0.00054.
2024-12-01-20:37:55-root-INFO: grad norm: 1004.707 913.703 417.833
2024-12-01-20:37:55-root-INFO: Loss too large (5342.587->5357.335)! Learning rate decreased to 0.00043.
2024-12-01-20:37:56-root-INFO: grad norm: 744.199 673.470 316.657
2024-12-01-20:37:56-root-INFO: grad norm: 551.869 506.134 219.973
2024-12-01-20:37:57-root-INFO: grad norm: 425.632 388.165 174.616
2024-12-01-20:37:57-root-INFO: Loss Change: 5355.114 -> 5152.730
2024-12-01-20:37:57-root-INFO: Regularization Change: 0.000 -> 0.343
2024-12-01-20:37:57-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-01-20:37:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:37:57-root-INFO: step: 223 lr_xt 0.00070376
2024-12-01-20:37:58-root-INFO: grad norm: 300.786 265.976 140.459
2024-12-01-20:37:58-root-INFO: grad norm: 256.516 233.357 106.512
2024-12-01-20:37:59-root-INFO: grad norm: 231.903 212.751 92.284
2024-12-01-20:37:59-root-INFO: grad norm: 216.588 200.894 80.944
2024-12-01-20:38:00-root-INFO: grad norm: 206.514 193.071 73.293
2024-12-01-20:38:00-root-INFO: Loss Change: 5130.487 -> 4970.654
2024-12-01-20:38:00-root-INFO: Regularization Change: 0.000 -> 0.550
2024-12-01-20:38:00-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-20:38:00-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:00-root-INFO: step: 222 lr_xt 0.00073840
2024-12-01-20:38:00-root-INFO: grad norm: 217.215 199.478 85.968
2024-12-01-20:38:01-root-INFO: grad norm: 219.572 204.582 79.737
2024-12-01-20:38:01-root-INFO: grad norm: 246.101 226.979 95.112
2024-12-01-20:38:02-root-INFO: grad norm: 316.987 294.225 117.951
2024-12-01-20:38:02-root-INFO: grad norm: 462.733 424.366 184.486
2024-12-01-20:38:02-root-INFO: Loss too large (4824.574->4845.930)! Learning rate decreased to 0.00059.
2024-12-01-20:38:02-root-INFO: Loss Change: 4900.873 -> 4812.730
2024-12-01-20:38:02-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-01-20:38:03-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-20:38:03-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:03-root-INFO: step: 221 lr_xt 0.00077462
2024-12-01-20:38:03-root-INFO: grad norm: 404.035 373.737 153.509
2024-12-01-20:38:03-root-INFO: Loss too large (4758.061->4761.049)! Learning rate decreased to 0.00062.
2024-12-01-20:38:04-root-INFO: grad norm: 416.960 380.593 170.308
2024-12-01-20:38:04-root-INFO: grad norm: 446.712 415.244 164.695
2024-12-01-20:38:04-root-INFO: grad norm: 482.119 441.728 193.172
2024-12-01-20:38:05-root-INFO: grad norm: 523.753 485.074 197.537
2024-12-01-20:38:05-root-INFO: Loss Change: 4758.061 -> 4691.515
2024-12-01-20:38:05-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-01-20:38:05-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-20:38:05-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:06-root-INFO: step: 220 lr_xt 0.00081248
2024-12-01-20:38:06-root-INFO: grad norm: 857.245 775.149 366.078
2024-12-01-20:38:06-root-INFO: Loss too large (4733.866->4855.668)! Learning rate decreased to 0.00065.
2024-12-01-20:38:06-root-INFO: grad norm: 910.294 842.267 345.286
2024-12-01-20:38:07-root-INFO: Loss too large (4713.145->4719.976)! Learning rate decreased to 0.00052.
2024-12-01-20:38:07-root-INFO: grad norm: 634.746 580.797 256.081
2024-12-01-20:38:07-root-INFO: grad norm: 443.469 414.393 157.936
2024-12-01-20:38:08-root-INFO: grad norm: 324.394 298.058 128.033
2024-12-01-20:38:08-root-INFO: Loss Change: 4733.866 -> 4535.346
2024-12-01-20:38:08-root-INFO: Regularization Change: 0.000 -> 0.356
2024-12-01-20:38:08-root-INFO: Undo step: 220
2024-12-01-20:38:08-root-INFO: Undo step: 221
2024-12-01-20:38:08-root-INFO: Undo step: 222
2024-12-01-20:38:08-root-INFO: Undo step: 223
2024-12-01-20:38:08-root-INFO: Undo step: 224
2024-12-01-20:38:09-root-INFO: step: 225 lr_xt 0.00063896
2024-12-01-20:38:09-root-INFO: grad norm: 1964.409 1642.545 1077.473
2024-12-01-20:38:09-root-INFO: Loss too large (6227.432->6496.722)! Learning rate decreased to 0.00051.
2024-12-01-20:38:09-root-INFO: grad norm: 2125.296 1993.469 736.864
2024-12-01-20:38:09-root-INFO: Loss too large (6012.867->6104.752)! Learning rate decreased to 0.00041.
2024-12-01-20:38:10-root-INFO: grad norm: 1685.699 1508.618 752.099
2024-12-01-20:38:10-root-INFO: grad norm: 1327.957 1235.507 486.819
2024-12-01-20:38:11-root-INFO: grad norm: 1050.968 939.131 471.769
2024-12-01-20:38:11-root-INFO: Loss Change: 6227.432 -> 5274.968
2024-12-01-20:38:11-root-INFO: Regularization Change: 0.000 -> 1.672
2024-12-01-20:38:11-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-01-20:38:11-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:11-root-INFO: step: 224 lr_xt 0.00067063
2024-12-01-20:38:12-root-INFO: grad norm: 995.201 917.433 385.670
2024-12-01-20:38:12-root-INFO: Loss too large (5253.158->5411.910)! Learning rate decreased to 0.00054.
2024-12-01-20:38:12-root-INFO: grad norm: 1158.681 1049.837 490.289
2024-12-01-20:38:12-root-INFO: Loss too large (5248.101->5277.241)! Learning rate decreased to 0.00043.
2024-12-01-20:38:13-root-INFO: grad norm: 891.013 820.702 346.917
2024-12-01-20:38:13-root-INFO: grad norm: 686.200 621.188 291.540
2024-12-01-20:38:14-root-INFO: grad norm: 535.843 492.072 212.116
2024-12-01-20:38:14-root-INFO: Loss Change: 5253.158 -> 5005.310
2024-12-01-20:38:14-root-INFO: Regularization Change: 0.000 -> 0.454
2024-12-01-20:38:14-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-01-20:38:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:14-root-INFO: step: 223 lr_xt 0.00070376
2024-12-01-20:38:15-root-INFO: grad norm: 301.166 273.608 125.856
2024-12-01-20:38:15-root-INFO: grad norm: 299.834 264.905 140.448
2024-12-01-20:38:16-root-INFO: grad norm: 377.113 347.020 147.619
2024-12-01-20:38:16-root-INFO: grad norm: 562.738 515.478 225.735
2024-12-01-20:38:16-root-INFO: Loss too large (4902.422->4945.331)! Learning rate decreased to 0.00056.
2024-12-01-20:38:17-root-INFO: grad norm: 635.494 582.455 254.164
2024-12-01-20:38:17-root-INFO: Loss Change: 4960.924 -> 4891.797
2024-12-01-20:38:17-root-INFO: Regularization Change: 0.000 -> 0.461
2024-12-01-20:38:17-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-20:38:17-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:17-root-INFO: step: 222 lr_xt 0.00073840
2024-12-01-20:38:17-root-INFO: grad norm: 764.804 702.829 301.591
2024-12-01-20:38:17-root-INFO: Loss too large (4831.174->4937.689)! Learning rate decreased to 0.00059.
2024-12-01-20:38:18-root-INFO: grad norm: 848.211 776.469 341.406
2024-12-01-20:38:18-root-INFO: Loss too large (4829.807->4836.540)! Learning rate decreased to 0.00047.
2024-12-01-20:38:18-root-INFO: grad norm: 602.879 553.780 238.309
2024-12-01-20:38:19-root-INFO: grad norm: 434.319 399.356 170.729
2024-12-01-20:38:19-root-INFO: grad norm: 325.343 299.876 126.183
2024-12-01-20:38:20-root-INFO: Loss Change: 4831.174 -> 4683.514
2024-12-01-20:38:20-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-01-20:38:20-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-20:38:20-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:20-root-INFO: step: 221 lr_xt 0.00077462
2024-12-01-20:38:20-root-INFO: grad norm: 200.333 185.406 75.881
2024-12-01-20:38:21-root-INFO: grad norm: 209.772 190.427 87.987
2024-12-01-20:38:21-root-INFO: grad norm: 251.756 234.907 90.554
2024-12-01-20:38:21-root-INFO: grad norm: 341.092 311.786 138.325
2024-12-01-20:38:22-root-INFO: Loss too large (4591.374->4592.520)! Learning rate decreased to 0.00062.
2024-12-01-20:38:22-root-INFO: grad norm: 358.106 331.769 134.794
2024-12-01-20:38:22-root-INFO: Loss Change: 4641.240 -> 4564.833
2024-12-01-20:38:22-root-INFO: Regularization Change: 0.000 -> 0.346
2024-12-01-20:38:22-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-20:38:22-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:23-root-INFO: step: 220 lr_xt 0.00081248
2024-12-01-20:38:23-root-INFO: grad norm: 651.529 586.471 283.799
2024-12-01-20:38:23-root-INFO: Loss too large (4586.856->4636.385)! Learning rate decreased to 0.00065.
2024-12-01-20:38:23-root-INFO: grad norm: 659.491 608.193 255.011
2024-12-01-20:38:24-root-INFO: grad norm: 689.764 630.345 280.071
2024-12-01-20:38:24-root-INFO: grad norm: 724.097 668.209 278.950
2024-12-01-20:38:25-root-INFO: grad norm: 760.988 697.539 304.207
2024-12-01-20:38:25-root-INFO: Loss Change: 4586.856 -> 4528.091
2024-12-01-20:38:25-root-INFO: Regularization Change: 0.000 -> 0.494
2024-12-01-20:38:25-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-20:38:25-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:25-root-INFO: step: 219 lr_xt 0.00085206
2024-12-01-20:38:25-root-INFO: grad norm: 763.994 702.363 300.622
2024-12-01-20:38:25-root-INFO: Loss too large (4498.841->4600.524)! Learning rate decreased to 0.00068.
2024-12-01-20:38:26-root-INFO: grad norm: 765.356 701.062 307.052
2024-12-01-20:38:26-root-INFO: grad norm: 767.075 707.966 295.276
2024-12-01-20:38:27-root-INFO: grad norm: 770.215 705.997 307.896
2024-12-01-20:38:27-root-INFO: grad norm: 773.257 713.495 298.078
2024-12-01-20:38:28-root-INFO: Loss Change: 4498.841 -> 4428.853
2024-12-01-20:38:28-root-INFO: Regularization Change: 0.000 -> 0.521
2024-12-01-20:38:28-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-01-20:38:28-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-20:38:28-root-INFO: step: 218 lr_xt 0.00089342
2024-12-01-20:38:28-root-INFO: grad norm: 930.985 851.448 376.522
2024-12-01-20:38:28-root-INFO: Loss too large (4460.450->4609.963)! Learning rate decreased to 0.00071.
2024-12-01-20:38:29-root-INFO: grad norm: 905.591 837.316 344.960
2024-12-01-20:38:29-root-INFO: grad norm: 884.028 811.077 351.652
2024-12-01-20:38:30-root-INFO: grad norm: 862.299 796.806 329.636
2024-12-01-20:38:30-root-INFO: grad norm: 843.330 774.578 333.518
2024-12-01-20:38:30-root-INFO: Loss Change: 4460.450 -> 4345.708
2024-12-01-20:38:30-root-INFO: Regularization Change: 0.000 -> 0.674
2024-12-01-20:38:30-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-01-20:38:30-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:38:31-root-INFO: step: 217 lr_xt 0.00093664
2024-12-01-20:38:31-root-INFO: grad norm: 663.047 617.963 240.317
2024-12-01-20:38:31-root-INFO: Loss too large (4292.275->4346.659)! Learning rate decreased to 0.00075.
2024-12-01-20:38:31-root-INFO: grad norm: 618.382 565.523 250.161
2024-12-01-20:38:32-root-INFO: grad norm: 589.198 548.492 215.200
2024-12-01-20:38:32-root-INFO: grad norm: 563.895 517.525 223.931
2024-12-01-20:38:33-root-INFO: grad norm: 540.301 502.523 198.484
2024-12-01-20:38:33-root-INFO: Loss Change: 4292.275 -> 4178.292
2024-12-01-20:38:33-root-INFO: Regularization Change: 0.000 -> 0.495
2024-12-01-20:38:33-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-01-20:38:33-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:38:34-root-INFO: step: 216 lr_xt 0.00098179
2024-12-01-20:38:34-root-INFO: grad norm: 669.365 608.111 279.733
2024-12-01-20:38:34-root-INFO: Loss too large (4210.272->4263.000)! Learning rate decreased to 0.00079.
2024-12-01-20:38:34-root-INFO: grad norm: 616.731 571.730 231.263
2024-12-01-20:38:35-root-INFO: grad norm: 582.894 536.114 228.796
2024-12-01-20:38:35-root-INFO: grad norm: 552.064 513.318 203.173
2024-12-01-20:38:36-root-INFO: grad norm: 525.500 484.463 203.584
2024-12-01-20:38:36-root-INFO: Loss Change: 4210.272 -> 4083.389
2024-12-01-20:38:36-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-01-20:38:36-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-01-20:38:36-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:38:36-root-INFO: step: 215 lr_xt 0.00102894
2024-12-01-20:38:37-root-INFO: grad norm: 457.255 426.772 164.159
2024-12-01-20:38:37-root-INFO: Loss too large (4051.627->4070.606)! Learning rate decreased to 0.00082.
2024-12-01-20:38:37-root-INFO: grad norm: 419.009 385.366 164.504
2024-12-01-20:38:38-root-INFO: grad norm: 385.432 361.013 135.010
2024-12-01-20:38:38-root-INFO: grad norm: 355.772 327.336 139.374
2024-12-01-20:38:39-root-INFO: grad norm: 329.521 309.001 114.466
2024-12-01-20:38:39-root-INFO: Loss Change: 4051.627 -> 3956.178
2024-12-01-20:38:39-root-INFO: Regularization Change: 0.000 -> 0.387
2024-12-01-20:38:39-root-INFO: Undo step: 215
2024-12-01-20:38:39-root-INFO: Undo step: 216
2024-12-01-20:38:39-root-INFO: Undo step: 217
2024-12-01-20:38:39-root-INFO: Undo step: 218
2024-12-01-20:38:39-root-INFO: Undo step: 219
2024-12-01-20:38:39-root-INFO: step: 220 lr_xt 0.00081248
2024-12-01-20:38:39-root-INFO: grad norm: 3281.359 2652.490 1931.740
2024-12-01-20:38:40-root-INFO: grad norm: 3413.931 3147.132 1323.059
2024-12-01-20:38:40-root-INFO: Loss too large (7366.474->8202.562)! Learning rate decreased to 0.00065.
2024-12-01-20:38:40-root-INFO: grad norm: 3068.522 2839.156 1164.051
2024-12-01-20:38:41-root-INFO: grad norm: 2709.695 2514.967 1008.658
2024-12-01-20:38:41-root-INFO: grad norm: 2675.474 2423.040 1134.476
2024-12-01-20:38:42-root-INFO: Loss Change: 7946.742 -> 5797.835
2024-12-01-20:38:42-root-INFO: Regularization Change: 0.000 -> 8.127
2024-12-01-20:38:42-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-20:38:42-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-20:38:42-root-INFO: step: 219 lr_xt 0.00085206
2024-12-01-20:38:42-root-INFO: grad norm: 2466.616 2236.978 1039.292
2024-12-01-20:38:42-root-INFO: Loss too large (5778.163->6607.404)! Learning rate decreased to 0.00068.
2024-12-01-20:38:43-root-INFO: grad norm: 2230.995 2023.067 940.498
2024-12-01-20:38:43-root-INFO: grad norm: 2069.323 1867.681 890.990
2024-12-01-20:38:44-root-INFO: grad norm: 1915.315 1735.310 810.637
2024-12-01-20:38:44-root-INFO: grad norm: 1793.999 1616.248 778.573
2024-12-01-20:38:45-root-INFO: Loss Change: 5778.163 -> 4932.728
2024-12-01-20:38:45-root-INFO: Regularization Change: 0.000 -> 2.807
2024-12-01-20:38:45-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-01-20:38:45-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-20:38:45-root-INFO: step: 218 lr_xt 0.00089342
2024-12-01-20:38:45-root-INFO: grad norm: 1458.001 1330.033 597.309
2024-12-01-20:38:45-root-INFO: Loss too large (4802.519->5133.176)! Learning rate decreased to 0.00071.
2024-12-01-20:38:46-root-INFO: grad norm: 1330.902 1199.281 577.084
2024-12-01-20:38:46-root-INFO: grad norm: 1221.714 1113.141 503.489
2024-12-01-20:38:46-root-INFO: grad norm: 1130.597 1019.872 487.968
2024-12-01-20:38:47-root-INFO: grad norm: 1043.880 951.546 429.240
2024-12-01-20:38:47-root-INFO: Loss Change: 4802.519 -> 4461.595
2024-12-01-20:38:47-root-INFO: Regularization Change: 0.000 -> 1.251
2024-12-01-20:38:47-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-01-20:38:47-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:38:47-root-INFO: step: 217 lr_xt 0.00093664
2024-12-01-20:38:48-root-INFO: grad norm: 1077.977 968.589 473.149
2024-12-01-20:38:48-root-INFO: Loss too large (4499.532->4637.273)! Learning rate decreased to 0.00075.
2024-12-01-20:38:48-root-INFO: grad norm: 949.558 870.796 378.649
2024-12-01-20:38:49-root-INFO: grad norm: 856.208 773.434 367.277
2024-12-01-20:38:49-root-INFO: grad norm: 770.536 707.664 304.857
2024-12-01-20:38:50-root-INFO: grad norm: 699.985 633.825 297.061
2024-12-01-20:38:50-root-INFO: Loss Change: 4499.532 -> 4227.231
2024-12-01-20:38:50-root-INFO: Regularization Change: 0.000 -> 0.877
2024-12-01-20:38:50-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-01-20:38:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:38:50-root-INFO: step: 216 lr_xt 0.00098179
2024-12-01-20:38:50-root-INFO: grad norm: 503.457 469.224 182.477
2024-12-01-20:38:51-root-INFO: grad norm: 626.153 568.911 261.549
2024-12-01-20:38:51-root-INFO: Loss too large (4167.923->4204.835)! Learning rate decreased to 0.00079.
2024-12-01-20:38:52-root-INFO: grad norm: 557.607 516.982 208.938
2024-12-01-20:38:52-root-INFO: grad norm: 503.230 458.701 206.963
2024-12-01-20:38:53-root-INFO: grad norm: 456.128 423.439 169.563
2024-12-01-20:38:53-root-INFO: Loss Change: 4171.714 -> 4049.295
2024-12-01-20:38:53-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-01-20:38:53-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-01-20:38:53-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:38:53-root-INFO: step: 215 lr_xt 0.00102894
2024-12-01-20:38:53-root-INFO: grad norm: 429.026 391.779 174.851
2024-12-01-20:38:53-root-INFO: Loss too large (4037.837->4042.491)! Learning rate decreased to 0.00082.
2024-12-01-20:38:54-root-INFO: grad norm: 374.853 350.070 134.037
2024-12-01-20:38:54-root-INFO: grad norm: 332.193 303.165 135.806
2024-12-01-20:38:55-root-INFO: grad norm: 296.418 278.076 102.651
2024-12-01-20:38:55-root-INFO: grad norm: 267.064 244.081 108.386
2024-12-01-20:38:56-root-INFO: Loss Change: 4037.837 -> 3929.724
2024-12-01-20:38:56-root-INFO: Regularization Change: 0.000 -> 0.405
2024-12-01-20:38:56-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-01-20:38:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:38:56-root-INFO: step: 214 lr_xt 0.00107819
2024-12-01-20:38:56-root-INFO: grad norm: 191.833 179.268 68.285
2024-12-01-20:38:56-root-INFO: grad norm: 183.446 166.809 76.335
2024-12-01-20:38:57-root-INFO: grad norm: 204.660 195.263 61.306
2024-12-01-20:38:57-root-INFO: grad norm: 246.984 228.743 93.154
2024-12-01-20:38:58-root-INFO: grad norm: 313.543 297.431 99.215
2024-12-01-20:38:58-root-INFO: Loss too large (3824.717->3824.933)! Learning rate decreased to 0.00086.
2024-12-01-20:38:58-root-INFO: Loss Change: 3884.766 -> 3807.832
2024-12-01-20:38:58-root-INFO: Regularization Change: 0.000 -> 0.478
2024-12-01-20:38:58-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-01-20:38:58-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:38:58-root-INFO: step: 213 lr_xt 0.00112961
2024-12-01-20:38:59-root-INFO: grad norm: 422.326 383.964 175.871
2024-12-01-20:38:59-root-INFO: grad norm: 521.553 491.136 175.508
2024-12-01-20:38:59-root-INFO: Loss too large (3821.479->3851.859)! Learning rate decreased to 0.00090.
2024-12-01-20:39:00-root-INFO: grad norm: 460.264 429.457 165.557
2024-12-01-20:39:00-root-INFO: grad norm: 415.931 395.810 127.799
2024-12-01-20:39:01-root-INFO: grad norm: 382.540 359.492 130.778
2024-12-01-20:39:01-root-INFO: Loss Change: 3824.174 -> 3732.392
2024-12-01-20:39:01-root-INFO: Regularization Change: 0.000 -> 0.487
2024-12-01-20:39:01-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-01-20:39:01-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:39:01-root-INFO: step: 212 lr_xt 0.00118329
2024-12-01-20:39:01-root-INFO: grad norm: 338.477 324.439 96.468
2024-12-01-20:39:01-root-INFO: Loss too large (3714.106->3719.781)! Learning rate decreased to 0.00095.
2024-12-01-20:39:02-root-INFO: grad norm: 306.612 288.760 103.095
2024-12-01-20:39:02-root-INFO: grad norm: 282.055 271.475 76.525
2024-12-01-20:39:03-root-INFO: grad norm: 262.254 247.426 86.934
2024-12-01-20:39:03-root-INFO: grad norm: 246.495 237.554 65.788
2024-12-01-20:39:04-root-INFO: Loss Change: 3714.106 -> 3638.347
2024-12-01-20:39:04-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-01-20:39:04-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-01-20:39:04-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-20:39:04-root-INFO: step: 211 lr_xt 0.00123933
2024-12-01-20:39:04-root-INFO: grad norm: 346.787 315.090 144.843
2024-12-01-20:39:05-root-INFO: grad norm: 405.877 385.697 126.389
2024-12-01-20:39:05-root-INFO: Loss too large (3612.666->3632.239)! Learning rate decreased to 0.00099.
2024-12-01-20:39:05-root-INFO: grad norm: 378.254 359.082 118.895
2024-12-01-20:39:06-root-INFO: grad norm: 373.593 359.126 102.958
2024-12-01-20:39:06-root-INFO: grad norm: 375.618 358.880 110.878
2024-12-01-20:39:06-root-INFO: Loss Change: 3627.008 -> 3557.465
2024-12-01-20:39:06-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-01-20:39:06-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-01-20:39:06-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:07-root-INFO: step: 210 lr_xt 0.00129780
2024-12-01-20:39:07-root-INFO: grad norm: 331.141 319.149 88.309
2024-12-01-20:39:07-root-INFO: Loss too large (3541.197->3556.196)! Learning rate decreased to 0.00104.
2024-12-01-20:39:07-root-INFO: grad norm: 325.410 309.910 99.233
2024-12-01-20:39:08-root-INFO: grad norm: 323.774 312.512 84.650
2024-12-01-20:39:08-root-INFO: grad norm: 323.110 308.208 96.994
2024-12-01-20:39:09-root-INFO: grad norm: 323.600 312.100 85.501
2024-12-01-20:39:09-root-INFO: Loss Change: 3541.197 -> 3483.487
2024-12-01-20:39:09-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-01-20:39:09-root-INFO: Undo step: 210
2024-12-01-20:39:09-root-INFO: Undo step: 211
2024-12-01-20:39:09-root-INFO: Undo step: 212
2024-12-01-20:39:09-root-INFO: Undo step: 213
2024-12-01-20:39:09-root-INFO: Undo step: 214
2024-12-01-20:39:09-root-INFO: step: 215 lr_xt 0.00102894
2024-12-01-20:39:09-root-INFO: grad norm: 2314.208 1903.001 1316.869
2024-12-01-20:39:10-root-INFO: grad norm: 2024.790 1803.776 919.873
2024-12-01-20:39:10-root-INFO: grad norm: 2174.739 2011.470 826.729
2024-12-01-20:39:10-root-INFO: Loss too large (5002.293->5810.783)! Learning rate decreased to 0.00082.
2024-12-01-20:39:11-root-INFO: grad norm: 1872.458 1731.368 713.068
2024-12-01-20:39:11-root-INFO: grad norm: 1555.121 1442.437 581.186
2024-12-01-20:39:12-root-INFO: Loss Change: 5710.317 -> 4351.993
2024-12-01-20:39:12-root-INFO: Regularization Change: 0.000 -> 5.108
2024-12-01-20:39:12-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-01-20:39:12-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:39:12-root-INFO: step: 214 lr_xt 0.00107819
2024-12-01-20:39:12-root-INFO: grad norm: 1414.717 1307.689 539.790
2024-12-01-20:39:12-root-INFO: Loss too large (4378.891->4595.575)! Learning rate decreased to 0.00086.
2024-12-01-20:39:13-root-INFO: grad norm: 1159.816 1076.622 431.347
2024-12-01-20:39:13-root-INFO: grad norm: 984.783 910.424 375.400
2024-12-01-20:39:14-root-INFO: grad norm: 822.071 762.497 307.245
2024-12-01-20:39:14-root-INFO: grad norm: 700.367 647.296 267.434
2024-12-01-20:39:14-root-INFO: Loss Change: 4378.891 -> 3836.653
2024-12-01-20:39:14-root-INFO: Regularization Change: 0.000 -> 1.457
2024-12-01-20:39:14-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-01-20:39:14-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:39:14-root-INFO: step: 213 lr_xt 0.00112961
2024-12-01-20:39:15-root-INFO: grad norm: 452.577 421.501 164.810
2024-12-01-20:39:15-root-INFO: grad norm: 541.187 499.807 207.547
2024-12-01-20:39:15-root-INFO: Loss too large (3781.600->3795.792)! Learning rate decreased to 0.00090.
2024-12-01-20:39:16-root-INFO: grad norm: 446.138 415.155 163.358
2024-12-01-20:39:16-root-INFO: grad norm: 373.864 345.152 143.681
2024-12-01-20:39:17-root-INFO: grad norm: 314.142 292.350 114.966
2024-12-01-20:39:17-root-INFO: Loss Change: 3789.006 -> 3656.735
2024-12-01-20:39:17-root-INFO: Regularization Change: 0.000 -> 0.589
2024-12-01-20:39:17-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-01-20:39:17-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-20:39:17-root-INFO: step: 212 lr_xt 0.00118329
2024-12-01-20:39:17-root-INFO: grad norm: 273.669 253.008 104.315
2024-12-01-20:39:18-root-INFO: grad norm: 319.377 298.585 113.353
2024-12-01-20:39:18-root-INFO: grad norm: 377.886 349.882 142.760
2024-12-01-20:39:19-root-INFO: grad norm: 454.154 424.688 160.923
2024-12-01-20:39:19-root-INFO: Loss too large (3620.287->3630.201)! Learning rate decreased to 0.00095.
2024-12-01-20:39:19-root-INFO: grad norm: 361.931 335.435 135.931
2024-12-01-20:39:20-root-INFO: Loss Change: 3640.829 -> 3562.171
2024-12-01-20:39:20-root-INFO: Regularization Change: 0.000 -> 0.520
2024-12-01-20:39:20-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-01-20:39:20-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-20:39:20-root-INFO: step: 211 lr_xt 0.00123933
2024-12-01-20:39:20-root-INFO: grad norm: 236.166 217.632 91.710
2024-12-01-20:39:21-root-INFO: grad norm: 210.987 195.929 78.275
2024-12-01-20:39:21-root-INFO: grad norm: 223.804 212.416 70.481
2024-12-01-20:39:22-root-INFO: grad norm: 251.823 235.845 88.273
2024-12-01-20:39:22-root-INFO: grad norm: 291.993 276.801 92.959
2024-12-01-20:39:22-root-INFO: Loss Change: 3527.330 -> 3455.443
2024-12-01-20:39:22-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-01-20:39:22-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-01-20:39:22-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:23-root-INFO: step: 210 lr_xt 0.00129780
2024-12-01-20:39:23-root-INFO: grad norm: 381.932 356.849 136.129
2024-12-01-20:39:23-root-INFO: grad norm: 434.281 412.159 136.839
2024-12-01-20:39:23-root-INFO: Loss too large (3451.602->3455.285)! Learning rate decreased to 0.00104.
2024-12-01-20:39:24-root-INFO: grad norm: 323.463 303.102 112.947
2024-12-01-20:39:24-root-INFO: grad norm: 245.581 233.812 75.113
2024-12-01-20:39:25-root-INFO: grad norm: 192.891 180.386 68.321
2024-12-01-20:39:25-root-INFO: Loss Change: 3454.471 -> 3357.246
2024-12-01-20:39:25-root-INFO: Regularization Change: 0.000 -> 0.418
2024-12-01-20:39:25-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-01-20:39:25-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:25-root-INFO: step: 209 lr_xt 0.00135882
2024-12-01-20:39:25-root-INFO: grad norm: 167.420 155.316 62.501
2024-12-01-20:39:26-root-INFO: grad norm: 163.351 153.741 55.201
2024-12-01-20:39:26-root-INFO: grad norm: 177.869 170.569 50.436
2024-12-01-20:39:27-root-INFO: grad norm: 203.726 193.237 64.525
2024-12-01-20:39:27-root-INFO: grad norm: 241.817 233.023 64.620
2024-12-01-20:39:28-root-INFO: Loss Change: 3333.826 -> 3279.054
2024-12-01-20:39:28-root-INFO: Regularization Change: 0.000 -> 0.545
2024-12-01-20:39:28-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-01-20:39:28-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:28-root-INFO: step: 208 lr_xt 0.00142247
2024-12-01-20:39:28-root-INFO: grad norm: 321.599 304.546 103.331
2024-12-01-20:39:29-root-INFO: grad norm: 381.328 366.539 105.170
2024-12-01-20:39:29-root-INFO: Loss too large (3268.890->3280.260)! Learning rate decreased to 0.00114.
2024-12-01-20:39:29-root-INFO: grad norm: 317.400 303.281 93.610
2024-12-01-20:39:30-root-INFO: grad norm: 271.683 261.653 73.138
2024-12-01-20:39:30-root-INFO: grad norm: 237.305 225.699 73.306
2024-12-01-20:39:30-root-INFO: Loss Change: 3273.875 -> 3193.379
2024-12-01-20:39:30-root-INFO: Regularization Change: 0.000 -> 0.454
2024-12-01-20:39:30-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-01-20:39:30-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:31-root-INFO: step: 207 lr_xt 0.00150141
2024-12-01-20:39:31-root-INFO: grad norm: 236.872 224.563 75.362
2024-12-01-20:39:31-root-INFO: grad norm: 309.991 293.091 100.956
2024-12-01-20:39:31-root-INFO: Loss too large (3170.615->3183.429)! Learning rate decreased to 0.00120.
2024-12-01-20:39:32-root-INFO: grad norm: 290.198 277.220 85.816
2024-12-01-20:39:32-root-INFO: grad norm: 274.034 258.791 90.122
2024-12-01-20:39:33-root-INFO: grad norm: 260.721 249.127 76.885
2024-12-01-20:39:33-root-INFO: Loss Change: 3172.938 -> 3117.375
2024-12-01-20:39:33-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-01-20:39:33-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-01-20:39:33-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:33-root-INFO: step: 206 lr_xt 0.00157117
2024-12-01-20:39:34-root-INFO: grad norm: 234.192 222.657 72.594
2024-12-01-20:39:34-root-INFO: grad norm: 322.648 308.630 94.069
2024-12-01-20:39:34-root-INFO: Loss too large (3088.942->3112.826)! Learning rate decreased to 0.00126.
2024-12-01-20:39:35-root-INFO: grad norm: 322.452 305.631 102.785
2024-12-01-20:39:35-root-INFO: grad norm: 325.090 309.721 98.775
2024-12-01-20:39:36-root-INFO: grad norm: 326.796 308.784 106.997
2024-12-01-20:39:36-root-INFO: Loss Change: 3089.007 -> 3048.198
2024-12-01-20:39:36-root-INFO: Regularization Change: 0.000 -> 0.482
2024-12-01-20:39:36-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-20:39:36-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:36-root-INFO: step: 205 lr_xt 0.00164390
2024-12-01-20:39:36-root-INFO: grad norm: 372.478 346.835 135.814
2024-12-01-20:39:36-root-INFO: Loss too large (3042.433->3085.882)! Learning rate decreased to 0.00132.
2024-12-01-20:39:37-root-INFO: grad norm: 387.051 364.701 129.620
2024-12-01-20:39:38-root-INFO: grad norm: 402.039 379.728 132.067
2024-12-01-20:39:38-root-INFO: grad norm: 408.222 384.845 136.160
2024-12-01-20:39:38-root-INFO: grad norm: 404.807 383.056 130.910
2024-12-01-20:39:39-root-INFO: Loss Change: 3042.433 -> 2998.585
2024-12-01-20:39:39-root-INFO: Regularization Change: 0.000 -> 0.599
2024-12-01-20:39:39-root-INFO: Undo step: 205
2024-12-01-20:39:39-root-INFO: Undo step: 206
2024-12-01-20:39:39-root-INFO: Undo step: 207
2024-12-01-20:39:39-root-INFO: Undo step: 208
2024-12-01-20:39:39-root-INFO: Undo step: 209
2024-12-01-20:39:39-root-INFO: step: 210 lr_xt 0.00129780
2024-12-01-20:39:39-root-INFO: grad norm: 2716.172 2344.240 1371.907
2024-12-01-20:39:40-root-INFO: grad norm: 2396.427 2250.867 822.472
2024-12-01-20:39:40-root-INFO: Loss too large (5414.816->5734.855)! Learning rate decreased to 0.00104.
2024-12-01-20:39:40-root-INFO: grad norm: 1792.229 1670.612 648.954
2024-12-01-20:39:41-root-INFO: grad norm: 1444.608 1358.693 490.761
2024-12-01-20:39:41-root-INFO: grad norm: 1125.263 1056.287 387.910
2024-12-01-20:39:41-root-INFO: Loss Change: 6772.618 -> 3627.640
2024-12-01-20:39:41-root-INFO: Regularization Change: 0.000 -> 9.028
2024-12-01-20:39:41-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-01-20:39:41-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:42-root-INFO: step: 209 lr_xt 0.00135882
2024-12-01-20:39:42-root-INFO: grad norm: 914.231 851.403 333.063
2024-12-01-20:39:42-root-INFO: Loss too large (3635.772->3675.075)! Learning rate decreased to 0.00109.
2024-12-01-20:39:42-root-INFO: grad norm: 680.224 640.925 227.858
2024-12-01-20:39:43-root-INFO: grad norm: 526.992 492.408 187.761
2024-12-01-20:39:43-root-INFO: grad norm: 410.158 385.941 138.850
2024-12-01-20:39:44-root-INFO: grad norm: 328.072 306.601 116.736
2024-12-01-20:39:44-root-INFO: Loss Change: 3635.772 -> 3256.516
2024-12-01-20:39:44-root-INFO: Regularization Change: 0.000 -> 1.248
2024-12-01-20:39:44-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-01-20:39:44-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:44-root-INFO: step: 208 lr_xt 0.00142247
2024-12-01-20:39:44-root-INFO: grad norm: 229.471 217.943 71.817
2024-12-01-20:39:45-root-INFO: grad norm: 237.388 222.168 83.634
2024-12-01-20:39:45-root-INFO: grad norm: 273.148 260.267 82.893
2024-12-01-20:39:46-root-INFO: grad norm: 326.630 308.325 107.809
2024-12-01-20:39:46-root-INFO: grad norm: 401.363 382.832 120.548
2024-12-01-20:39:47-root-INFO: Loss too large (3179.191->3188.958)! Learning rate decreased to 0.00114.
2024-12-01-20:39:47-root-INFO: Loss Change: 3240.814 -> 3149.023
2024-12-01-20:39:47-root-INFO: Regularization Change: 0.000 -> 0.855
2024-12-01-20:39:47-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-01-20:39:47-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:47-root-INFO: step: 207 lr_xt 0.00150141
2024-12-01-20:39:47-root-INFO: grad norm: 338.651 317.226 118.542
2024-12-01-20:39:48-root-INFO: grad norm: 393.804 377.312 112.769
2024-12-01-20:39:48-root-INFO: Loss too large (3124.012->3130.181)! Learning rate decreased to 0.00120.
2024-12-01-20:39:48-root-INFO: grad norm: 318.331 301.371 102.520
2024-12-01-20:39:49-root-INFO: grad norm: 266.418 256.614 71.609
2024-12-01-20:39:49-root-INFO: grad norm: 230.787 217.810 76.299
2024-12-01-20:39:50-root-INFO: Loss Change: 3132.582 -> 3029.791
2024-12-01-20:39:50-root-INFO: Regularization Change: 0.000 -> 0.595
2024-12-01-20:39:50-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-01-20:39:50-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:50-root-INFO: step: 206 lr_xt 0.00157117
2024-12-01-20:39:50-root-INFO: grad norm: 220.628 210.509 66.051
2024-12-01-20:39:51-root-INFO: grad norm: 286.624 269.708 97.011
2024-12-01-20:39:51-root-INFO: Loss too large (2996.133->3004.997)! Learning rate decreased to 0.00126.
2024-12-01-20:39:51-root-INFO: grad norm: 275.419 263.283 80.854
2024-12-01-20:39:52-root-INFO: grad norm: 268.424 252.490 91.106
2024-12-01-20:39:52-root-INFO: grad norm: 264.474 252.508 78.655
2024-12-01-20:39:52-root-INFO: Loss Change: 3002.628 -> 2943.723
2024-12-01-20:39:52-root-INFO: Regularization Change: 0.000 -> 0.509
2024-12-01-20:39:52-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-20:39:52-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-20:39:53-root-INFO: step: 205 lr_xt 0.00164390
2024-12-01-20:39:53-root-INFO: grad norm: 246.825 232.993 81.465
2024-12-01-20:39:53-root-INFO: Loss too large (2922.798->2925.910)! Learning rate decreased to 0.00132.
2024-12-01-20:39:53-root-INFO: grad norm: 250.332 239.268 73.601
2024-12-01-20:39:54-root-INFO: grad norm: 270.866 255.153 90.913
2024-12-01-20:39:54-root-INFO: grad norm: 301.991 286.894 94.287
2024-12-01-20:39:55-root-INFO: grad norm: 336.159 315.973 114.734
2024-12-01-20:39:55-root-INFO: Loss Change: 2922.798 -> 2886.790
2024-12-01-20:39:55-root-INFO: Regularization Change: 0.000 -> 0.518
2024-12-01-20:39:55-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-01-20:39:55-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-20:39:55-root-INFO: step: 204 lr_xt 0.00171973
2024-12-01-20:39:55-root-INFO: grad norm: 437.157 405.264 163.913
2024-12-01-20:39:56-root-INFO: Loss too large (2886.509->2971.808)! Learning rate decreased to 0.00138.
2024-12-01-20:39:56-root-INFO: Loss too large (2886.509->2892.403)! Learning rate decreased to 0.00110.
2024-12-01-20:39:56-root-INFO: grad norm: 330.589 309.430 116.371
2024-12-01-20:39:57-root-INFO: grad norm: 256.821 241.482 87.428
2024-12-01-20:39:57-root-INFO: grad norm: 210.950 197.359 74.495
2024-12-01-20:39:58-root-INFO: grad norm: 178.716 169.176 57.609
2024-12-01-20:39:58-root-INFO: Loss Change: 2886.509 -> 2783.065
2024-12-01-20:39:58-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-01-20:39:58-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-01-20:39:58-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:39:58-root-INFO: step: 203 lr_xt 0.00179875
2024-12-01-20:39:58-root-INFO: grad norm: 125.990 117.813 44.649
2024-12-01-20:39:59-root-INFO: grad norm: 184.147 174.793 57.945
2024-12-01-20:39:59-root-INFO: Loss too large (2756.384->2765.314)! Learning rate decreased to 0.00144.
2024-12-01-20:39:59-root-INFO: grad norm: 241.251 226.195 83.892
2024-12-01-20:40:00-root-INFO: Loss too large (2752.407->2756.140)! Learning rate decreased to 0.00115.
2024-12-01-20:40:00-root-INFO: grad norm: 227.719 214.966 75.139
2024-12-01-20:40:01-root-INFO: grad norm: 217.877 204.174 76.049
2024-12-01-20:40:01-root-INFO: Loss Change: 2766.342 -> 2721.419
2024-12-01-20:40:01-root-INFO: Regularization Change: 0.000 -> 0.380
2024-12-01-20:40:01-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-01-20:40:01-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:01-root-INFO: step: 202 lr_xt 0.00188111
2024-12-01-20:40:01-root-INFO: grad norm: 362.738 329.313 152.091
2024-12-01-20:40:01-root-INFO: Loss too large (2728.187->2812.235)! Learning rate decreased to 0.00150.
2024-12-01-20:40:01-root-INFO: Loss too large (2728.187->2745.725)! Learning rate decreased to 0.00120.
2024-12-01-20:40:02-root-INFO: grad norm: 354.309 332.855 121.417
2024-12-01-20:40:02-root-INFO: grad norm: 367.881 343.900 130.650
2024-12-01-20:40:03-root-INFO: grad norm: 380.170 356.963 130.792
2024-12-01-20:40:03-root-INFO: grad norm: 389.356 364.868 135.904
2024-12-01-20:40:04-root-INFO: Loss Change: 2728.187 -> 2680.719
2024-12-01-20:40:04-root-INFO: Regularization Change: 0.000 -> 0.523
2024-12-01-20:40:04-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-01-20:40:04-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:04-root-INFO: step: 201 lr_xt 0.00196691
2024-12-01-20:40:04-root-INFO: grad norm: 413.404 387.292 144.596
2024-12-01-20:40:04-root-INFO: Loss too large (2685.042->2841.951)! Learning rate decreased to 0.00157.
2024-12-01-20:40:04-root-INFO: Loss too large (2685.042->2736.190)! Learning rate decreased to 0.00126.
2024-12-01-20:40:05-root-INFO: grad norm: 416.608 390.931 143.998
2024-12-01-20:40:05-root-INFO: grad norm: 411.936 385.538 145.091
2024-12-01-20:40:06-root-INFO: grad norm: 401.504 376.185 140.321
2024-12-01-20:40:06-root-INFO: grad norm: 387.418 362.575 136.500
2024-12-01-20:40:06-root-INFO: Loss Change: 2685.042 -> 2629.561
2024-12-01-20:40:06-root-INFO: Regularization Change: 0.000 -> 0.529
2024-12-01-20:40:06-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-01-20:40:06-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:07-root-INFO: step: 200 lr_xt 0.00205630
2024-12-01-20:40:07-root-INFO: grad norm: 494.052 455.704 190.843
2024-12-01-20:40:07-root-INFO: Loss too large (2646.219->2860.990)! Learning rate decreased to 0.00165.
2024-12-01-20:40:07-root-INFO: Loss too large (2646.219->2715.874)! Learning rate decreased to 0.00132.
2024-12-01-20:40:08-root-INFO: grad norm: 476.011 446.403 165.261
2024-12-01-20:40:08-root-INFO: grad norm: 455.344 424.676 164.282
2024-12-01-20:40:09-root-INFO: grad norm: 428.083 400.969 149.931
2024-12-01-20:40:09-root-INFO: grad norm: 398.744 371.950 143.700
2024-12-01-20:40:09-root-INFO: Loss Change: 2646.219 -> 2564.762
2024-12-01-20:40:09-root-INFO: Regularization Change: 0.000 -> 0.656
2024-12-01-20:40:09-root-INFO: Undo step: 200
2024-12-01-20:40:09-root-INFO: Undo step: 201
2024-12-01-20:40:09-root-INFO: Undo step: 202
2024-12-01-20:40:09-root-INFO: Undo step: 203
2024-12-01-20:40:09-root-INFO: Undo step: 204
2024-12-01-20:40:10-root-INFO: step: 205 lr_xt 0.00164390
2024-12-01-20:40:10-root-INFO: grad norm: 1316.137 1116.577 696.760
2024-12-01-20:40:10-root-INFO: grad norm: 1100.229 1030.204 386.242
2024-12-01-20:40:10-root-INFO: Loss too large (3692.445->4002.498)! Learning rate decreased to 0.00132.
2024-12-01-20:40:11-root-INFO: grad norm: 1097.152 1056.238 296.821
2024-12-01-20:40:11-root-INFO: grad norm: 1007.951 952.009 331.126
2024-12-01-20:40:12-root-INFO: grad norm: 884.427 846.868 255.001
2024-12-01-20:40:12-root-INFO: Loss Change: 4633.343 -> 3194.676
2024-12-01-20:40:12-root-INFO: Regularization Change: 0.000 -> 8.132
2024-12-01-20:40:12-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-01-20:40:12-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-20:40:12-root-INFO: step: 204 lr_xt 0.00171973
2024-12-01-20:40:12-root-INFO: grad norm: 630.362 605.579 175.015
2024-12-01-20:40:13-root-INFO: Loss too large (3133.882->3157.301)! Learning rate decreased to 0.00138.
2024-12-01-20:40:13-root-INFO: grad norm: 507.205 486.592 143.127
2024-12-01-20:40:14-root-INFO: grad norm: 407.383 389.742 118.586
2024-12-01-20:40:14-root-INFO: grad norm: 340.473 328.206 90.567
2024-12-01-20:40:14-root-INFO: grad norm: 292.202 279.958 83.700
2024-12-01-20:40:15-root-INFO: Loss Change: 3133.882 -> 2853.657
2024-12-01-20:40:15-root-INFO: Regularization Change: 0.000 -> 1.508
2024-12-01-20:40:15-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-01-20:40:15-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:15-root-INFO: step: 203 lr_xt 0.00179875
2024-12-01-20:40:15-root-INFO: grad norm: 263.185 253.624 70.294
2024-12-01-20:40:16-root-INFO: grad norm: 343.792 332.038 89.127
2024-12-01-20:40:16-root-INFO: Loss too large (2825.993->2847.417)! Learning rate decreased to 0.00144.
2024-12-01-20:40:16-root-INFO: grad norm: 339.203 330.118 77.978
2024-12-01-20:40:17-root-INFO: grad norm: 354.971 343.324 90.183
2024-12-01-20:40:17-root-INFO: grad norm: 382.131 372.091 87.023
2024-12-01-20:40:17-root-INFO: Loss Change: 2836.437 -> 2774.817
2024-12-01-20:40:17-root-INFO: Regularization Change: 0.000 -> 0.903
2024-12-01-20:40:17-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-01-20:40:17-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:18-root-INFO: step: 202 lr_xt 0.00188111
2024-12-01-20:40:18-root-INFO: grad norm: 513.246 485.617 166.125
2024-12-01-20:40:18-root-INFO: Loss too large (2782.866->2899.457)! Learning rate decreased to 0.00150.
2024-12-01-20:40:18-root-INFO: Loss too large (2782.866->2786.467)! Learning rate decreased to 0.00120.
2024-12-01-20:40:18-root-INFO: grad norm: 382.460 371.507 90.876
2024-12-01-20:40:19-root-INFO: grad norm: 301.935 289.831 84.632
2024-12-01-20:40:19-root-INFO: grad norm: 259.208 251.391 63.177
2024-12-01-20:40:20-root-INFO: grad norm: 232.199 223.127 64.271
2024-12-01-20:40:20-root-INFO: Loss Change: 2782.866 -> 2635.629
2024-12-01-20:40:20-root-INFO: Regularization Change: 0.000 -> 0.630
2024-12-01-20:40:20-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-01-20:40:20-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:20-root-INFO: step: 201 lr_xt 0.00196691
2024-12-01-20:40:20-root-INFO: grad norm: 237.492 229.828 59.843
2024-12-01-20:40:21-root-INFO: Loss too large (2634.993->2666.353)! Learning rate decreased to 0.00157.
2024-12-01-20:40:21-root-INFO: Loss too large (2634.993->2636.267)! Learning rate decreased to 0.00126.
2024-12-01-20:40:21-root-INFO: grad norm: 235.969 227.342 63.219
2024-12-01-20:40:22-root-INFO: grad norm: 245.533 237.515 62.234
2024-12-01-20:40:22-root-INFO: grad norm: 266.974 257.308 71.185
2024-12-01-20:40:23-root-INFO: grad norm: 297.083 287.850 73.489
2024-12-01-20:40:23-root-INFO: Loss Change: 2634.993 -> 2592.889
2024-12-01-20:40:23-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-01-20:40:23-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-01-20:40:23-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:23-root-INFO: step: 200 lr_xt 0.00205630
2024-12-01-20:40:23-root-INFO: grad norm: 466.569 442.743 147.191
2024-12-01-20:40:23-root-INFO: Loss too large (2593.170->2839.275)! Learning rate decreased to 0.00165.
2024-12-01-20:40:24-root-INFO: Loss too large (2593.170->2695.858)! Learning rate decreased to 0.00132.
2024-12-01-20:40:24-root-INFO: Loss too large (2593.170->2608.190)! Learning rate decreased to 0.00105.
2024-12-01-20:40:24-root-INFO: grad norm: 367.824 357.659 85.873
2024-12-01-20:40:25-root-INFO: grad norm: 299.617 287.990 82.656
2024-12-01-20:40:25-root-INFO: grad norm: 260.150 252.305 63.407
2024-12-01-20:40:26-root-INFO: grad norm: 232.270 223.539 63.084
2024-12-01-20:40:26-root-INFO: Loss Change: 2593.170 -> 2496.405
2024-12-01-20:40:26-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-01-20:40:26-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-01-20:40:26-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:26-root-INFO: step: 199 lr_xt 0.00214940
2024-12-01-20:40:26-root-INFO: grad norm: 184.018 178.135 46.160
2024-12-01-20:40:26-root-INFO: Loss too large (2477.150->2500.427)! Learning rate decreased to 0.00172.
2024-12-01-20:40:27-root-INFO: Loss too large (2477.150->2479.650)! Learning rate decreased to 0.00138.
2024-12-01-20:40:27-root-INFO: grad norm: 237.952 228.968 64.766
2024-12-01-20:40:27-root-INFO: Loss too large (2468.991->2477.462)! Learning rate decreased to 0.00110.
2024-12-01-20:40:28-root-INFO: grad norm: 247.120 239.807 59.671
2024-12-01-20:40:28-root-INFO: grad norm: 265.565 256.289 69.577
2024-12-01-20:40:29-root-INFO: grad norm: 289.705 281.282 69.351
2024-12-01-20:40:29-root-INFO: Loss Change: 2477.150 -> 2448.220
2024-12-01-20:40:29-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-01-20:40:29-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-01-20:40:29-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-20:40:29-root-INFO: step: 198 lr_xt 0.00224635
2024-12-01-20:40:29-root-INFO: grad norm: 424.958 404.799 129.333
2024-12-01-20:40:29-root-INFO: Loss too large (2454.932->2790.258)! Learning rate decreased to 0.00180.
2024-12-01-20:40:30-root-INFO: Loss too large (2454.932->2628.653)! Learning rate decreased to 0.00144.
2024-12-01-20:40:30-root-INFO: Loss too large (2454.932->2524.684)! Learning rate decreased to 0.00115.
2024-12-01-20:40:30-root-INFO: Loss too large (2454.932->2462.151)! Learning rate decreased to 0.00092.
2024-12-01-20:40:30-root-INFO: grad norm: 328.516 320.043 74.128
2024-12-01-20:40:31-root-INFO: grad norm: 260.563 250.621 71.290
2024-12-01-20:40:31-root-INFO: grad norm: 220.303 213.763 53.279
2024-12-01-20:40:32-root-INFO: grad norm: 190.850 183.801 51.390
2024-12-01-20:40:32-root-INFO: Loss Change: 2454.932 -> 2378.083
2024-12-01-20:40:32-root-INFO: Regularization Change: 0.000 -> 0.253
2024-12-01-20:40:32-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-01-20:40:32-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:40:32-root-INFO: step: 197 lr_xt 0.00234729
2024-12-01-20:40:32-root-INFO: grad norm: 117.905 112.361 35.731
2024-12-01-20:40:33-root-INFO: grad norm: 225.349 218.804 53.915
2024-12-01-20:40:33-root-INFO: Loss too large (2342.485->2463.581)! Learning rate decreased to 0.00188.
2024-12-01-20:40:33-root-INFO: Loss too large (2342.485->2404.444)! Learning rate decreased to 0.00150.
2024-12-01-20:40:33-root-INFO: Loss too large (2342.485->2368.375)! Learning rate decreased to 0.00120.
2024-12-01-20:40:33-root-INFO: Loss too large (2342.485->2347.456)! Learning rate decreased to 0.00096.
2024-12-01-20:40:34-root-INFO: grad norm: 226.871 220.308 54.174
2024-12-01-20:40:34-root-INFO: grad norm: 233.794 226.510 57.902
2024-12-01-20:40:35-root-INFO: grad norm: 243.894 237.247 56.551
2024-12-01-20:40:35-root-INFO: Loss Change: 2352.597 -> 2319.662
2024-12-01-20:40:35-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-01-20:40:35-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-01-20:40:35-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:40:35-root-INFO: step: 196 lr_xt 0.00245238
2024-12-01-20:40:35-root-INFO: grad norm: 367.280 350.488 109.787
2024-12-01-20:40:36-root-INFO: Loss too large (2320.850->2677.741)! Learning rate decreased to 0.00196.
2024-12-01-20:40:36-root-INFO: Loss too large (2320.850->2521.324)! Learning rate decreased to 0.00157.
2024-12-01-20:40:36-root-INFO: Loss too large (2320.850->2418.142)! Learning rate decreased to 0.00126.
2024-12-01-20:40:36-root-INFO: Loss too large (2320.850->2353.942)! Learning rate decreased to 0.00100.
2024-12-01-20:40:37-root-INFO: grad norm: 392.967 384.325 81.957
2024-12-01-20:40:37-root-INFO: grad norm: 440.510 426.550 110.019
2024-12-01-20:40:37-root-INFO: Loss too large (2315.259->2322.707)! Learning rate decreased to 0.00080.
2024-12-01-20:40:38-root-INFO: grad norm: 319.710 311.814 70.614
2024-12-01-20:40:38-root-INFO: grad norm: 231.542 223.889 59.038
2024-12-01-20:40:39-root-INFO: Loss Change: 2320.850 -> 2261.459
2024-12-01-20:40:39-root-INFO: Regularization Change: 0.000 -> 0.213
2024-12-01-20:40:39-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-01-20:40:39-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:40:39-root-INFO: step: 195 lr_xt 0.00256175
2024-12-01-20:40:39-root-INFO: grad norm: 141.888 136.825 37.567
2024-12-01-20:40:39-root-INFO: Loss too large (2244.386->2277.010)! Learning rate decreased to 0.00205.
2024-12-01-20:40:39-root-INFO: Loss too large (2244.386->2256.522)! Learning rate decreased to 0.00164.
2024-12-01-20:40:39-root-INFO: Loss too large (2244.386->2245.236)! Learning rate decreased to 0.00131.
2024-12-01-20:40:40-root-INFO: grad norm: 212.353 205.503 53.500
2024-12-01-20:40:40-root-INFO: Loss too large (2239.463->2250.850)! Learning rate decreased to 0.00105.
2024-12-01-20:40:40-root-INFO: grad norm: 258.772 252.229 57.823
2024-12-01-20:40:41-root-INFO: Loss too large (2237.575->2238.393)! Learning rate decreased to 0.00084.
2024-12-01-20:40:41-root-INFO: grad norm: 218.013 211.205 54.054
2024-12-01-20:40:41-root-INFO: grad norm: 191.020 185.749 44.561
2024-12-01-20:40:42-root-INFO: Loss Change: 2244.386 -> 2210.818
2024-12-01-20:40:42-root-INFO: Regularization Change: 0.000 -> 0.185
2024-12-01-20:40:42-root-INFO: Undo step: 195
2024-12-01-20:40:42-root-INFO: Undo step: 196
2024-12-01-20:40:42-root-INFO: Undo step: 197
2024-12-01-20:40:42-root-INFO: Undo step: 198
2024-12-01-20:40:42-root-INFO: Undo step: 199
2024-12-01-20:40:42-root-INFO: step: 200 lr_xt 0.00205630
2024-12-01-20:40:42-root-INFO: grad norm: 1467.974 1382.052 494.851
2024-12-01-20:40:43-root-INFO: grad norm: 1134.839 1100.745 276.078
2024-12-01-20:40:43-root-INFO: grad norm: 777.246 741.886 231.770
2024-12-01-20:40:43-root-INFO: grad norm: 709.317 675.353 216.863
2024-12-01-20:40:44-root-INFO: grad norm: 663.040 637.118 183.582
2024-12-01-20:40:44-root-INFO: Loss Change: 4224.707 -> 2868.518
2024-12-01-20:40:44-root-INFO: Regularization Change: 0.000 -> 11.116
2024-12-01-20:40:44-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-01-20:40:44-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-20:40:44-root-INFO: step: 199 lr_xt 0.00214940
2024-12-01-20:40:45-root-INFO: grad norm: 627.619 600.789 181.546
2024-12-01-20:40:45-root-INFO: grad norm: 601.687 582.615 150.290
2024-12-01-20:40:45-root-INFO: grad norm: 597.565 577.957 151.820
2024-12-01-20:40:46-root-INFO: grad norm: 610.013 591.089 150.763
2024-12-01-20:40:46-root-INFO: Loss too large (2724.398->2725.888)! Learning rate decreased to 0.00172.
2024-12-01-20:40:47-root-INFO: grad norm: 419.207 406.175 103.713
2024-12-01-20:40:47-root-INFO: Loss Change: 2850.657 -> 2534.637
2024-12-01-20:40:47-root-INFO: Regularization Change: 0.000 -> 2.223
2024-12-01-20:40:47-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-01-20:40:47-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-20:40:47-root-INFO: step: 198 lr_xt 0.00224635
2024-12-01-20:40:47-root-INFO: grad norm: 406.074 385.745 126.874
2024-12-01-20:40:47-root-INFO: Loss too large (2536.756->2577.244)! Learning rate decreased to 0.00180.
2024-12-01-20:40:48-root-INFO: grad norm: 366.018 352.412 98.867
2024-12-01-20:40:48-root-INFO: grad norm: 337.845 322.185 101.664
2024-12-01-20:40:49-root-INFO: grad norm: 333.400 320.302 92.533
2024-12-01-20:40:49-root-INFO: grad norm: 346.328 329.958 105.218
2024-12-01-20:40:50-root-INFO: Loss Change: 2536.756 -> 2422.540
2024-12-01-20:40:50-root-INFO: Regularization Change: 0.000 -> 1.379
2024-12-01-20:40:50-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-01-20:40:50-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:40:50-root-INFO: step: 197 lr_xt 0.00234729
2024-12-01-20:40:50-root-INFO: grad norm: 317.447 306.261 83.530
2024-12-01-20:40:50-root-INFO: Loss too large (2380.642->2435.149)! Learning rate decreased to 0.00188.
2024-12-01-20:40:51-root-INFO: grad norm: 390.684 372.619 117.427
2024-12-01-20:40:51-root-INFO: Loss too large (2373.560->2403.794)! Learning rate decreased to 0.00150.
2024-12-01-20:40:51-root-INFO: grad norm: 331.955 319.178 91.213
2024-12-01-20:40:52-root-INFO: grad norm: 297.818 284.795 87.107
2024-12-01-20:40:52-root-INFO: grad norm: 287.963 277.651 76.374
2024-12-01-20:40:52-root-INFO: Loss Change: 2380.642 -> 2293.614
2024-12-01-20:40:52-root-INFO: Regularization Change: 0.000 -> 0.723
2024-12-01-20:40:52-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-01-20:40:52-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:40:53-root-INFO: step: 196 lr_xt 0.00245238
2024-12-01-20:40:53-root-INFO: grad norm: 572.120 551.205 153.277
2024-12-01-20:40:53-root-INFO: Loss too large (2326.667->2578.445)! Learning rate decreased to 0.00196.
2024-12-01-20:40:53-root-INFO: Loss too large (2326.667->2463.016)! Learning rate decreased to 0.00157.
2024-12-01-20:40:53-root-INFO: Loss too large (2326.667->2380.964)! Learning rate decreased to 0.00126.
2024-12-01-20:40:54-root-INFO: grad norm: 334.896 325.043 80.637
2024-12-01-20:40:54-root-INFO: grad norm: 223.846 216.578 56.581
2024-12-01-20:40:55-root-INFO: grad norm: 159.191 152.968 44.075
2024-12-01-20:40:55-root-INFO: grad norm: 134.040 128.697 37.468
2024-12-01-20:40:55-root-INFO: Loss Change: 2326.667 -> 2204.748
2024-12-01-20:40:55-root-INFO: Regularization Change: 0.000 -> 0.381
2024-12-01-20:40:55-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-01-20:40:55-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:40:56-root-INFO: step: 195 lr_xt 0.00256175
2024-12-01-20:40:56-root-INFO: grad norm: 180.268 174.484 45.298
2024-12-01-20:40:56-root-INFO: Loss too large (2194.751->2227.817)! Learning rate decreased to 0.00205.
2024-12-01-20:40:56-root-INFO: Loss too large (2194.751->2211.269)! Learning rate decreased to 0.00164.
2024-12-01-20:40:56-root-INFO: Loss too large (2194.751->2199.360)! Learning rate decreased to 0.00131.
2024-12-01-20:40:57-root-INFO: grad norm: 191.374 187.804 36.789
2024-12-01-20:40:57-root-INFO: grad norm: 227.399 222.284 47.959
2024-12-01-20:40:57-root-INFO: Loss too large (2180.498->2185.375)! Learning rate decreased to 0.00105.
2024-12-01-20:40:58-root-INFO: grad norm: 191.047 187.330 37.503
2024-12-01-20:40:58-root-INFO: grad norm: 144.798 140.020 36.892
2024-12-01-20:40:58-root-INFO: Loss Change: 2194.751 -> 2155.199
2024-12-01-20:40:58-root-INFO: Regularization Change: 0.000 -> 0.237
2024-12-01-20:40:58-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-01-20:40:58-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:40:59-root-INFO: step: 194 lr_xt 0.00267557
2024-12-01-20:40:59-root-INFO: grad norm: 113.764 104.580 44.780
2024-12-01-20:40:59-root-INFO: grad norm: 223.164 214.671 60.982
2024-12-01-20:40:59-root-INFO: Loss too large (2133.364->2264.754)! Learning rate decreased to 0.00214.
2024-12-01-20:41:00-root-INFO: Loss too large (2133.364->2194.784)! Learning rate decreased to 0.00171.
2024-12-01-20:41:00-root-INFO: Loss too large (2133.364->2154.059)! Learning rate decreased to 0.00137.
2024-12-01-20:41:00-root-INFO: grad norm: 280.364 269.087 78.717
2024-12-01-20:41:00-root-INFO: Loss too large (2131.836->2142.392)! Learning rate decreased to 0.00110.
2024-12-01-20:41:01-root-INFO: grad norm: 245.254 236.786 63.888
2024-12-01-20:41:01-root-INFO: grad norm: 228.180 219.210 63.352
2024-12-01-20:41:02-root-INFO: Loss Change: 2139.645 -> 2108.166
2024-12-01-20:41:02-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-01-20:41:02-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-01-20:41:02-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:41:02-root-INFO: step: 193 lr_xt 0.00279399
2024-12-01-20:41:02-root-INFO: grad norm: 146.613 141.183 39.534
2024-12-01-20:41:02-root-INFO: Loss too large (2081.514->2119.749)! Learning rate decreased to 0.00224.
2024-12-01-20:41:02-root-INFO: Loss too large (2081.514->2096.669)! Learning rate decreased to 0.00179.
2024-12-01-20:41:02-root-INFO: Loss too large (2081.514->2083.622)! Learning rate decreased to 0.00143.
2024-12-01-20:41:03-root-INFO: grad norm: 198.372 192.974 45.960
2024-12-01-20:41:03-root-INFO: Loss too large (2076.742->2087.125)! Learning rate decreased to 0.00114.
2024-12-01-20:41:03-root-INFO: grad norm: 247.355 242.324 49.637
2024-12-01-20:41:04-root-INFO: Loss too large (2073.741->2080.195)! Learning rate decreased to 0.00092.
2024-12-01-20:41:04-root-INFO: grad norm: 212.920 209.691 36.938
2024-12-01-20:41:05-root-INFO: grad norm: 197.893 193.608 40.958
2024-12-01-20:41:05-root-INFO: Loss Change: 2081.514 -> 2057.600
2024-12-01-20:41:05-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-01-20:41:05-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-01-20:41:05-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-20:41:05-root-INFO: step: 192 lr_xt 0.00291718
2024-12-01-20:41:05-root-INFO: grad norm: 195.544 190.743 43.064
2024-12-01-20:41:06-root-INFO: Loss too large (2037.110->2230.804)! Learning rate decreased to 0.00233.
2024-12-01-20:41:06-root-INFO: Loss too large (2037.110->2145.508)! Learning rate decreased to 0.00187.
2024-12-01-20:41:06-root-INFO: Loss too large (2037.110->2091.448)! Learning rate decreased to 0.00149.
2024-12-01-20:41:06-root-INFO: Loss too large (2037.110->2059.176)! Learning rate decreased to 0.00119.
2024-12-01-20:41:06-root-INFO: Loss too large (2037.110->2041.065)! Learning rate decreased to 0.00096.
2024-12-01-20:41:07-root-INFO: grad norm: 214.338 210.357 41.118
2024-12-01-20:41:07-root-INFO: Loss too large (2031.665->2033.051)! Learning rate decreased to 0.00076.
2024-12-01-20:41:07-root-INFO: grad norm: 176.944 174.016 32.056
2024-12-01-20:41:08-root-INFO: grad norm: 151.182 147.538 32.990
2024-12-01-20:41:08-root-INFO: grad norm: 143.184 140.367 28.266
2024-12-01-20:41:08-root-INFO: Loss Change: 2037.110 -> 2011.932
2024-12-01-20:41:08-root-INFO: Regularization Change: 0.000 -> 0.091
2024-12-01-20:41:08-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-01-20:41:08-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-20:41:09-root-INFO: step: 191 lr_xt 0.00304531
2024-12-01-20:41:09-root-INFO: grad norm: 542.804 531.499 110.204
2024-12-01-20:41:09-root-INFO: Loss too large (2047.231->2317.314)! Learning rate decreased to 0.00244.
2024-12-01-20:41:09-root-INFO: Loss too large (2047.231->2257.169)! Learning rate decreased to 0.00195.
2024-12-01-20:41:09-root-INFO: Loss too large (2047.231->2206.235)! Learning rate decreased to 0.00156.
2024-12-01-20:41:09-root-INFO: Loss too large (2047.231->2161.812)! Learning rate decreased to 0.00125.
2024-12-01-20:41:10-root-INFO: Loss too large (2047.231->2121.768)! Learning rate decreased to 0.00100.
2024-12-01-20:41:10-root-INFO: Loss too large (2047.231->2085.227)! Learning rate decreased to 0.00080.
2024-12-01-20:41:10-root-INFO: Loss too large (2047.231->2053.550)! Learning rate decreased to 0.00064.
2024-12-01-20:41:10-root-INFO: grad norm: 301.610 297.073 52.118
2024-12-01-20:41:11-root-INFO: grad norm: 86.229 79.169 34.172
2024-12-01-20:41:11-root-INFO: grad norm: 78.847 73.183 29.345
2024-12-01-20:41:12-root-INFO: grad norm: 74.318 68.901 27.853
2024-12-01-20:41:12-root-INFO: Loss Change: 2047.231 -> 1982.294
2024-12-01-20:41:12-root-INFO: Regularization Change: 0.000 -> 0.101
2024-12-01-20:41:12-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-01-20:41:12-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-20:41:12-root-INFO: step: 190 lr_xt 0.00317856
2024-12-01-20:41:13-root-INFO: grad norm: 309.125 301.160 69.722
2024-12-01-20:41:13-root-INFO: Loss too large (1984.108->2208.717)! Learning rate decreased to 0.00254.
2024-12-01-20:41:13-root-INFO: Loss too large (1984.108->2151.970)! Learning rate decreased to 0.00203.
2024-12-01-20:41:13-root-INFO: Loss too large (1984.108->2104.706)! Learning rate decreased to 0.00163.
2024-12-01-20:41:13-root-INFO: Loss too large (1984.108->2064.753)! Learning rate decreased to 0.00130.
2024-12-01-20:41:13-root-INFO: Loss too large (1984.108->2031.895)! Learning rate decreased to 0.00104.
2024-12-01-20:41:14-root-INFO: Loss too large (1984.108->2006.634)! Learning rate decreased to 0.00083.
2024-12-01-20:41:14-root-INFO: Loss too large (1984.108->1988.834)! Learning rate decreased to 0.00067.
2024-12-01-20:41:14-root-INFO: grad norm: 233.877 230.320 40.636
2024-12-01-20:41:15-root-INFO: grad norm: 157.197 152.461 38.296
2024-12-01-20:41:15-root-INFO: grad norm: 137.970 135.071 28.134
2024-12-01-20:41:16-root-INFO: grad norm: 118.619 114.567 30.740
2024-12-01-20:41:16-root-INFO: Loss Change: 1984.108 -> 1952.958
2024-12-01-20:41:16-root-INFO: Regularization Change: 0.000 -> 0.078
2024-12-01-20:41:16-root-INFO: Undo step: 190
2024-12-01-20:41:16-root-INFO: Undo step: 191
2024-12-01-20:41:16-root-INFO: Undo step: 192
2024-12-01-20:41:16-root-INFO: Undo step: 193
2024-12-01-20:41:16-root-INFO: Undo step: 194
2024-12-01-20:41:16-root-INFO: step: 195 lr_xt 0.00256175
2024-12-01-20:41:16-root-INFO: grad norm: 994.596 934.125 341.515
2024-12-01-20:41:17-root-INFO: grad norm: 1088.921 1062.821 236.983
2024-12-01-20:41:17-root-INFO: Loss too large (3059.785->3569.443)! Learning rate decreased to 0.00205.
2024-12-01-20:41:17-root-INFO: grad norm: 1081.511 1046.351 273.524
2024-12-01-20:41:18-root-INFO: grad norm: 792.311 772.517 175.994
2024-12-01-20:41:18-root-INFO: grad norm: 524.934 509.804 125.119
2024-12-01-20:41:19-root-INFO: Loss Change: 3393.421 -> 2410.223
2024-12-01-20:41:19-root-INFO: Regularization Change: 0.000 -> 8.070
2024-12-01-20:41:19-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-01-20:41:19-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:41:19-root-INFO: step: 194 lr_xt 0.00267557
2024-12-01-20:41:19-root-INFO: grad norm: 351.796 345.100 68.310
2024-12-01-20:41:20-root-INFO: grad norm: 426.925 414.532 102.119
2024-12-01-20:41:20-root-INFO: Loss too large (2311.831->2413.880)! Learning rate decreased to 0.00214.
2024-12-01-20:41:20-root-INFO: Loss too large (2311.831->2320.765)! Learning rate decreased to 0.00171.
2024-12-01-20:41:20-root-INFO: grad norm: 294.360 288.374 59.060
2024-12-01-20:41:21-root-INFO: grad norm: 229.605 222.566 56.419
2024-12-01-20:41:21-root-INFO: Loss too large (2206.574->2207.255)! Learning rate decreased to 0.00137.
2024-12-01-20:41:21-root-INFO: grad norm: 209.774 205.152 43.793
2024-12-01-20:41:22-root-INFO: Loss Change: 2371.344 -> 2177.177
2024-12-01-20:41:22-root-INFO: Regularization Change: 0.000 -> 1.140
2024-12-01-20:41:22-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-01-20:41:22-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-20:41:22-root-INFO: step: 193 lr_xt 0.00279399
2024-12-01-20:41:22-root-INFO: grad norm: 542.776 531.966 107.788
2024-12-01-20:41:22-root-INFO: Loss too large (2199.833->2337.921)! Learning rate decreased to 0.00224.
2024-12-01-20:41:22-root-INFO: Loss too large (2199.833->2298.413)! Learning rate decreased to 0.00179.
2024-12-01-20:41:23-root-INFO: Loss too large (2199.833->2263.575)! Learning rate decreased to 0.00143.
2024-12-01-20:41:23-root-INFO: Loss too large (2199.833->2232.782)! Learning rate decreased to 0.00114.
2024-12-01-20:41:23-root-INFO: Loss too large (2199.833->2205.508)! Learning rate decreased to 0.00092.
2024-12-01-20:41:23-root-INFO: grad norm: 239.894 235.374 46.348
2024-12-01-20:41:24-root-INFO: grad norm: 155.055 150.073 38.990
2024-12-01-20:41:24-root-INFO: grad norm: 118.104 112.520 35.884
2024-12-01-20:41:25-root-INFO: grad norm: 109.961 105.313 31.632
2024-12-01-20:41:25-root-INFO: Loss Change: 2199.833 -> 2107.958
2024-12-01-20:41:25-root-INFO: Regularization Change: 0.000 -> 0.209
2024-12-01-20:41:25-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-01-20:41:25-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-20:41:25-root-INFO: step: 192 lr_xt 0.00291718
2024-12-01-20:41:25-root-INFO: grad norm: 199.479 192.816 51.126
2024-12-01-20:41:26-root-INFO: Loss too large (2094.875->2161.523)! Learning rate decreased to 0.00233.
2024-12-01-20:41:26-root-INFO: Loss too large (2094.875->2138.343)! Learning rate decreased to 0.00187.
2024-12-01-20:41:26-root-INFO: Loss too large (2094.875->2119.693)! Learning rate decreased to 0.00149.
2024-12-01-20:41:26-root-INFO: Loss too large (2094.875->2105.575)! Learning rate decreased to 0.00119.
2024-12-01-20:41:26-root-INFO: Loss too large (2094.875->2095.782)! Learning rate decreased to 0.00096.
2024-12-01-20:41:27-root-INFO: grad norm: 193.640 189.744 38.646
2024-12-01-20:41:27-root-INFO: grad norm: 205.082 200.681 42.257
2024-12-01-20:41:28-root-INFO: grad norm: 209.496 205.674 39.838
2024-12-01-20:41:28-root-INFO: grad norm: 223.904 219.744 42.960
2024-12-01-20:41:28-root-INFO: Loss too large (2067.832->2069.401)! Learning rate decreased to 0.00076.
2024-12-01-20:41:29-root-INFO: Loss Change: 2094.875 -> 2062.974
2024-12-01-20:41:29-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-01-20:41:29-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-01-20:41:29-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-20:41:29-root-INFO: step: 191 lr_xt 0.00304531
2024-12-01-20:41:29-root-INFO: grad norm: 149.442 137.642 58.204
2024-12-01-20:41:29-root-INFO: Loss too large (2048.876->2060.671)! Learning rate decreased to 0.00244.
2024-12-01-20:41:29-root-INFO: grad norm: 279.954 275.029 52.284
2024-12-01-20:41:30-root-INFO: Loss too large (2047.592->2306.223)! Learning rate decreased to 0.00195.
2024-12-01-20:41:30-root-INFO: Loss too large (2047.592->2184.046)! Learning rate decreased to 0.00156.
2024-12-01-20:41:30-root-INFO: Loss too large (2047.592->2103.554)! Learning rate decreased to 0.00125.
2024-12-01-20:41:30-root-INFO: Loss too large (2047.592->2056.203)! Learning rate decreased to 0.00100.
2024-12-01-20:41:31-root-INFO: grad norm: 329.650 324.698 56.923
2024-12-01-20:41:31-root-INFO: Loss too large (2031.877->2050.436)! Learning rate decreased to 0.00080.
2024-12-01-20:41:31-root-INFO: Loss too large (2031.877->2034.537)! Learning rate decreased to 0.00064.
2024-12-01-20:41:31-root-INFO: grad norm: 216.531 213.037 38.742
2024-12-01-20:41:32-root-INFO: grad norm: 109.813 105.984 28.746
2024-12-01-20:41:32-root-INFO: Loss Change: 2048.876 -> 2005.449
2024-12-01-20:41:32-root-INFO: Regularization Change: 0.000 -> 0.224
2024-12-01-20:41:32-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-01-20:41:32-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-20:41:32-root-INFO: step: 190 lr_xt 0.00317856
2024-12-01-20:41:33-root-INFO: grad norm: 219.164 212.672 52.947
2024-12-01-20:41:33-root-INFO: Loss too large (1996.935->2114.588)! Learning rate decreased to 0.00254.
2024-12-01-20:41:33-root-INFO: Loss too large (1996.935->2083.011)! Learning rate decreased to 0.00203.
2024-12-01-20:41:33-root-INFO: Loss too large (1996.935->2056.101)! Learning rate decreased to 0.00163.
2024-12-01-20:41:33-root-INFO: Loss too large (1996.935->2033.672)! Learning rate decreased to 0.00130.
2024-12-01-20:41:33-root-INFO: Loss too large (1996.935->2016.165)! Learning rate decreased to 0.00104.
2024-12-01-20:41:33-root-INFO: Loss too large (1996.935->2003.620)! Learning rate decreased to 0.00083.
2024-12-01-20:41:34-root-INFO: grad norm: 226.009 222.806 37.913
2024-12-01-20:41:34-root-INFO: grad norm: 263.868 259.334 48.709
2024-12-01-20:41:35-root-INFO: Loss too large (1988.363->1993.879)! Learning rate decreased to 0.00067.
2024-12-01-20:41:35-root-INFO: grad norm: 214.342 211.279 36.111
2024-12-01-20:41:35-root-INFO: grad norm: 155.001 151.253 33.883
2024-12-01-20:41:36-root-INFO: Loss Change: 1996.935 -> 1970.970
2024-12-01-20:41:36-root-INFO: Regularization Change: 0.000 -> 0.093
2024-12-01-20:41:36-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-01-20:41:36-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-20:41:36-root-INFO: step: 189 lr_xt 0.00331709
2024-12-01-20:41:36-root-INFO: grad norm: 115.132 108.429 38.711
2024-12-01-20:41:36-root-INFO: Loss too large (1957.196->1981.803)! Learning rate decreased to 0.00265.
2024-12-01-20:41:36-root-INFO: Loss too large (1957.196->1968.372)! Learning rate decreased to 0.00212.
2024-12-01-20:41:37-root-INFO: Loss too large (1957.196->1959.993)! Learning rate decreased to 0.00170.
2024-12-01-20:41:37-root-INFO: grad norm: 222.324 219.284 36.638
2024-12-01-20:41:37-root-INFO: Loss too large (1955.128->2055.785)! Learning rate decreased to 0.00136.
2024-12-01-20:41:37-root-INFO: Loss too large (1955.128->2004.465)! Learning rate decreased to 0.00109.
2024-12-01-20:41:38-root-INFO: Loss too large (1955.128->1973.128)! Learning rate decreased to 0.00087.
2024-12-01-20:41:38-root-INFO: Loss too large (1955.128->1955.727)! Learning rate decreased to 0.00070.
2024-12-01-20:41:38-root-INFO: grad norm: 215.774 212.072 39.799
2024-12-01-20:41:39-root-INFO: grad norm: 217.067 214.194 35.200
2024-12-01-20:41:39-root-INFO: grad norm: 221.009 217.397 39.798
2024-12-01-20:41:39-root-INFO: Loss too large (1940.667->1941.205)! Learning rate decreased to 0.00056.
2024-12-01-20:41:40-root-INFO: Loss Change: 1957.196 -> 1936.453
2024-12-01-20:41:40-root-INFO: Regularization Change: 0.000 -> 0.110
2024-12-01-20:41:40-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-01-20:41:40-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-20:41:40-root-INFO: step: 188 lr_xt 0.00346111
2024-12-01-20:41:40-root-INFO: grad norm: 110.964 104.949 36.037
2024-12-01-20:41:40-root-INFO: Loss too large (1920.679->1922.941)! Learning rate decreased to 0.00277.
2024-12-01-20:41:40-root-INFO: grad norm: 189.194 185.882 35.244
2024-12-01-20:41:41-root-INFO: Loss too large (1915.453->1967.638)! Learning rate decreased to 0.00222.
2024-12-01-20:41:41-root-INFO: Loss too large (1915.453->1933.142)! Learning rate decreased to 0.00177.
2024-12-01-20:41:41-root-INFO: grad norm: 245.574 240.778 48.298
2024-12-01-20:41:41-root-INFO: Loss too large (1913.936->1962.741)! Learning rate decreased to 0.00142.
2024-12-01-20:41:42-root-INFO: Loss too large (1913.936->1928.141)! Learning rate decreased to 0.00113.
2024-12-01-20:41:42-root-INFO: grad norm: 370.126 366.310 53.008
2024-12-01-20:41:42-root-INFO: Loss too large (1908.988->1965.790)! Learning rate decreased to 0.00091.
2024-12-01-20:41:42-root-INFO: Loss too large (1908.988->1940.515)! Learning rate decreased to 0.00073.
2024-12-01-20:41:43-root-INFO: Loss too large (1908.988->1919.976)! Learning rate decreased to 0.00058.
2024-12-01-20:41:43-root-INFO: grad norm: 281.509 278.139 43.428
2024-12-01-20:41:43-root-INFO: Loss Change: 1920.679 -> 1888.895
2024-12-01-20:41:43-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-01-20:41:43-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-01-20:41:43-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-20:41:43-root-INFO: step: 187 lr_xt 0.00361079
2024-12-01-20:41:44-root-INFO: grad norm: 509.906 503.021 83.507
2024-12-01-20:41:44-root-INFO: Loss too large (1916.857->2136.128)! Learning rate decreased to 0.00289.
2024-12-01-20:41:44-root-INFO: Loss too large (1916.857->2111.391)! Learning rate decreased to 0.00231.
2024-12-01-20:41:44-root-INFO: Loss too large (1916.857->2085.982)! Learning rate decreased to 0.00185.
2024-12-01-20:41:44-root-INFO: Loss too large (1916.857->2059.473)! Learning rate decreased to 0.00148.
2024-12-01-20:41:44-root-INFO: Loss too large (1916.857->2031.991)! Learning rate decreased to 0.00118.
2024-12-01-20:41:45-root-INFO: Loss too large (1916.857->2003.555)! Learning rate decreased to 0.00095.
2024-12-01-20:41:45-root-INFO: Loss too large (1916.857->1974.224)! Learning rate decreased to 0.00076.
2024-12-01-20:41:45-root-INFO: Loss too large (1916.857->1945.555)! Learning rate decreased to 0.00061.
2024-12-01-20:41:45-root-INFO: Loss too large (1916.857->1920.720)! Learning rate decreased to 0.00048.
2024-12-01-20:41:46-root-INFO: grad norm: 299.355 296.161 43.613
2024-12-01-20:41:46-root-INFO: grad norm: 108.741 104.075 31.513
2024-12-01-20:41:46-root-INFO: grad norm: 91.592 88.264 24.467
2024-12-01-20:41:47-root-INFO: grad norm: 79.710 75.320 26.090
2024-12-01-20:41:47-root-INFO: Loss Change: 1916.857 -> 1872.692
2024-12-01-20:41:47-root-INFO: Regularization Change: 0.000 -> 0.050
2024-12-01-20:41:47-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-01-20:41:47-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:41:47-root-INFO: step: 186 lr_xt 0.00376634
2024-12-01-20:41:48-root-INFO: grad norm: 216.455 211.157 47.597
2024-12-01-20:41:48-root-INFO: Loss too large (1864.405->2027.620)! Learning rate decreased to 0.00301.
2024-12-01-20:41:48-root-INFO: Loss too large (1864.405->1995.222)! Learning rate decreased to 0.00241.
2024-12-01-20:41:48-root-INFO: Loss too large (1864.405->1965.785)! Learning rate decreased to 0.00193.
2024-12-01-20:41:48-root-INFO: Loss too large (1864.405->1938.636)! Learning rate decreased to 0.00154.
2024-12-01-20:41:48-root-INFO: Loss too large (1864.405->1914.518)! Learning rate decreased to 0.00123.
2024-12-01-20:41:49-root-INFO: Loss too large (1864.405->1894.690)! Learning rate decreased to 0.00099.
2024-12-01-20:41:49-root-INFO: Loss too large (1864.405->1879.769)! Learning rate decreased to 0.00079.
2024-12-01-20:41:49-root-INFO: Loss too large (1864.405->1869.485)! Learning rate decreased to 0.00063.
2024-12-01-20:41:49-root-INFO: grad norm: 233.558 230.593 37.098
2024-12-01-20:41:50-root-INFO: grad norm: 278.202 274.213 46.941
2024-12-01-20:41:50-root-INFO: Loss too large (1859.391->1863.905)! Learning rate decreased to 0.00051.
2024-12-01-20:41:50-root-INFO: grad norm: 223.841 221.101 34.911
2024-12-01-20:41:51-root-INFO: grad norm: 164.574 161.308 32.623
2024-12-01-20:41:51-root-INFO: Loss Change: 1864.405 -> 1845.932
2024-12-01-20:41:51-root-INFO: Regularization Change: 0.000 -> 0.052
2024-12-01-20:41:51-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-01-20:41:51-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:41:51-root-INFO: step: 185 lr_xt 0.00392795
2024-12-01-20:41:52-root-INFO: grad norm: 81.622 76.999 27.081
2024-12-01-20:41:52-root-INFO: grad norm: 213.009 209.828 36.674
2024-12-01-20:41:52-root-INFO: Loss too large (1827.076->2336.055)! Learning rate decreased to 0.00314.
2024-12-01-20:41:52-root-INFO: Loss too large (1827.076->2180.791)! Learning rate decreased to 0.00251.
2024-12-01-20:41:53-root-INFO: Loss too large (1827.076->2060.344)! Learning rate decreased to 0.00201.
2024-12-01-20:41:53-root-INFO: Loss too large (1827.076->1971.287)! Learning rate decreased to 0.00161.
2024-12-01-20:41:53-root-INFO: Loss too large (1827.076->1909.138)! Learning rate decreased to 0.00129.
2024-12-01-20:41:53-root-INFO: Loss too large (1827.076->1868.442)! Learning rate decreased to 0.00103.
2024-12-01-20:41:53-root-INFO: Loss too large (1827.076->1843.513)! Learning rate decreased to 0.00082.
2024-12-01-20:41:53-root-INFO: Loss too large (1827.076->1829.309)! Learning rate decreased to 0.00066.
2024-12-01-20:41:54-root-INFO: grad norm: 267.810 265.105 37.972
2024-12-01-20:41:54-root-INFO: Loss too large (1821.915->1830.964)! Learning rate decreased to 0.00053.
2024-12-01-20:41:54-root-INFO: Loss too large (1821.915->1821.916)! Learning rate decreased to 0.00042.
2024-12-01-20:41:55-root-INFO: grad norm: 183.079 180.785 28.891
2024-12-01-20:41:55-root-INFO: grad norm: 116.499 114.263 22.714
2024-12-01-20:41:55-root-INFO: Loss Change: 1834.242 -> 1809.624
2024-12-01-20:41:55-root-INFO: Regularization Change: 0.000 -> 0.152
2024-12-01-20:41:55-root-INFO: Undo step: 185
2024-12-01-20:41:55-root-INFO: Undo step: 186
2024-12-01-20:41:55-root-INFO: Undo step: 187
2024-12-01-20:41:55-root-INFO: Undo step: 188
2024-12-01-20:41:55-root-INFO: Undo step: 189
2024-12-01-20:41:56-root-INFO: step: 190 lr_xt 0.00317856
2024-12-01-20:41:56-root-INFO: grad norm: 839.568 765.958 343.775
2024-12-01-20:41:56-root-INFO: grad norm: 1042.615 1005.026 277.434
2024-12-01-20:41:56-root-INFO: Loss too large (3030.439->3360.956)! Learning rate decreased to 0.00254.
2024-12-01-20:41:57-root-INFO: grad norm: 758.108 741.534 157.657
2024-12-01-20:41:57-root-INFO: grad norm: 286.820 279.283 65.318
2024-12-01-20:41:58-root-INFO: grad norm: 242.262 237.115 49.673
2024-12-01-20:41:58-root-INFO: Loss too large (2276.235->2289.457)! Learning rate decreased to 0.00203.
2024-12-01-20:41:58-root-INFO: Loss Change: 3362.522 -> 2252.784
2024-12-01-20:41:58-root-INFO: Regularization Change: 0.000 -> 10.416
2024-12-01-20:41:58-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-01-20:41:58-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-20:41:58-root-INFO: step: 189 lr_xt 0.00331709
2024-12-01-20:41:59-root-INFO: grad norm: 651.154 637.975 130.343
2024-12-01-20:41:59-root-INFO: Loss too large (2284.072->2422.805)! Learning rate decreased to 0.00265.
2024-12-01-20:41:59-root-INFO: Loss too large (2284.072->2387.579)! Learning rate decreased to 0.00212.
2024-12-01-20:41:59-root-INFO: Loss too large (2284.072->2352.534)! Learning rate decreased to 0.00170.
2024-12-01-20:41:59-root-INFO: Loss too large (2284.072->2317.787)! Learning rate decreased to 0.00136.
2024-12-01-20:41:59-root-INFO: Loss too large (2284.072->2284.460)! Learning rate decreased to 0.00109.
2024-12-01-20:42:00-root-INFO: grad norm: 249.447 241.980 60.578
2024-12-01-20:42:00-root-INFO: grad norm: 223.907 217.088 54.837
2024-12-01-20:42:01-root-INFO: grad norm: 131.105 124.222 41.923
2024-12-01-20:42:01-root-INFO: grad norm: 124.589 118.982 36.957
2024-12-01-20:42:02-root-INFO: Loss Change: 2284.072 -> 2124.363
2024-12-01-20:42:02-root-INFO: Regularization Change: 0.000 -> 0.454
2024-12-01-20:42:02-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-01-20:42:02-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-20:42:02-root-INFO: step: 188 lr_xt 0.00346111
2024-12-01-20:42:02-root-INFO: grad norm: 251.369 242.385 66.601
2024-12-01-20:42:02-root-INFO: Loss too large (2117.767->2225.168)! Learning rate decreased to 0.00277.
2024-12-01-20:42:02-root-INFO: Loss too large (2117.767->2193.308)! Learning rate decreased to 0.00222.
2024-12-01-20:42:02-root-INFO: Loss too large (2117.767->2166.124)! Learning rate decreased to 0.00177.
2024-12-01-20:42:03-root-INFO: Loss too large (2117.767->2143.605)! Learning rate decreased to 0.00142.
2024-12-01-20:42:03-root-INFO: Loss too large (2117.767->2126.148)! Learning rate decreased to 0.00113.
2024-12-01-20:42:03-root-INFO: grad norm: 252.003 246.270 53.447
2024-12-01-20:42:04-root-INFO: grad norm: 288.336 281.464 62.575
2024-12-01-20:42:04-root-INFO: Loss too large (2094.293->2103.483)! Learning rate decreased to 0.00091.
2024-12-01-20:42:04-root-INFO: grad norm: 246.027 240.715 50.850
2024-12-01-20:42:05-root-INFO: grad norm: 192.316 186.385 47.392
2024-12-01-20:42:05-root-INFO: Loss Change: 2117.767 -> 2063.110
2024-12-01-20:42:05-root-INFO: Regularization Change: 0.000 -> 0.286
2024-12-01-20:42:05-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-01-20:42:05-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-20:42:05-root-INFO: step: 187 lr_xt 0.00361079
2024-12-01-20:42:05-root-INFO: grad norm: 113.593 105.726 41.539
2024-12-01-20:42:06-root-INFO: grad norm: 224.551 220.027 44.847
2024-12-01-20:42:06-root-INFO: Loss too large (2025.270->2165.721)! Learning rate decreased to 0.00289.
2024-12-01-20:42:06-root-INFO: Loss too large (2025.270->2129.450)! Learning rate decreased to 0.00231.
2024-12-01-20:42:06-root-INFO: Loss too large (2025.270->2098.226)! Learning rate decreased to 0.00185.
2024-12-01-20:42:07-root-INFO: Loss too large (2025.270->2071.553)! Learning rate decreased to 0.00148.
2024-12-01-20:42:07-root-INFO: Loss too large (2025.270->2050.126)! Learning rate decreased to 0.00118.
2024-12-01-20:42:07-root-INFO: Loss too large (2025.270->2034.445)! Learning rate decreased to 0.00095.
2024-12-01-20:42:07-root-INFO: grad norm: 256.283 251.879 47.305
2024-12-01-20:42:08-root-INFO: grad norm: 368.415 361.778 69.615
2024-12-01-20:42:08-root-INFO: Loss too large (2021.921->2044.033)! Learning rate decreased to 0.00076.
2024-12-01-20:42:08-root-INFO: Loss too large (2021.921->2024.251)! Learning rate decreased to 0.00061.
2024-12-01-20:42:09-root-INFO: grad norm: 241.572 237.443 44.469
2024-12-01-20:42:09-root-INFO: Loss Change: 2050.504 -> 1995.681
2024-12-01-20:42:09-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-01-20:42:09-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-01-20:42:09-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:42:09-root-INFO: step: 186 lr_xt 0.00376634
2024-12-01-20:42:09-root-INFO: grad norm: 321.184 314.175 66.734
2024-12-01-20:42:09-root-INFO: Loss too large (1990.472->2196.038)! Learning rate decreased to 0.00301.
2024-12-01-20:42:10-root-INFO: Loss too large (1990.472->2159.739)! Learning rate decreased to 0.00241.
2024-12-01-20:42:10-root-INFO: Loss too large (1990.472->2125.549)! Learning rate decreased to 0.00193.
2024-12-01-20:42:10-root-INFO: Loss too large (1990.472->2093.152)! Learning rate decreased to 0.00154.
2024-12-01-20:42:10-root-INFO: Loss too large (1990.472->2062.571)! Learning rate decreased to 0.00123.
2024-12-01-20:42:10-root-INFO: Loss too large (1990.472->2034.738)! Learning rate decreased to 0.00099.
2024-12-01-20:42:10-root-INFO: Loss too large (1990.472->2011.491)! Learning rate decreased to 0.00079.
2024-12-01-20:42:10-root-INFO: Loss too large (1990.472->1994.201)! Learning rate decreased to 0.00063.
2024-12-01-20:42:11-root-INFO: grad norm: 245.615 241.294 45.873
2024-12-01-20:42:11-root-INFO: grad norm: 169.783 164.872 40.539
2024-12-01-20:42:12-root-INFO: grad norm: 151.911 148.498 32.024
2024-12-01-20:42:12-root-INFO: grad norm: 135.497 130.964 34.753
2024-12-01-20:42:13-root-INFO: Loss Change: 1990.472 -> 1955.191
2024-12-01-20:42:13-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-01-20:42:13-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-01-20:42:13-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:42:13-root-INFO: step: 185 lr_xt 0.00392795
2024-12-01-20:42:13-root-INFO: grad norm: 95.022 88.923 33.494
2024-12-01-20:42:13-root-INFO: grad norm: 283.369 278.426 52.701
2024-12-01-20:42:14-root-INFO: Loss too large (1937.581->2663.436)! Learning rate decreased to 0.00314.
2024-12-01-20:42:14-root-INFO: Loss too large (1937.581->2462.581)! Learning rate decreased to 0.00251.
2024-12-01-20:42:14-root-INFO: Loss too large (1937.581->2296.738)! Learning rate decreased to 0.00201.
2024-12-01-20:42:14-root-INFO: Loss too large (1937.581->2165.835)! Learning rate decreased to 0.00161.
2024-12-01-20:42:14-root-INFO: Loss too large (1937.581->2069.111)! Learning rate decreased to 0.00129.
2024-12-01-20:42:14-root-INFO: Loss too large (1937.581->2002.913)! Learning rate decreased to 0.00103.
2024-12-01-20:42:15-root-INFO: Loss too large (1937.581->1961.228)! Learning rate decreased to 0.00082.
2024-12-01-20:42:15-root-INFO: grad norm: 419.126 413.589 67.904
2024-12-01-20:42:15-root-INFO: Loss too large (1937.312->1967.583)! Learning rate decreased to 0.00066.
2024-12-01-20:42:15-root-INFO: Loss too large (1937.312->1943.836)! Learning rate decreased to 0.00053.
2024-12-01-20:42:16-root-INFO: grad norm: 285.666 281.405 49.154
2024-12-01-20:42:16-root-INFO: grad norm: 164.267 160.486 35.043
2024-12-01-20:42:17-root-INFO: Loss Change: 1943.500 -> 1908.195
2024-12-01-20:42:17-root-INFO: Regularization Change: 0.000 -> 0.239
2024-12-01-20:42:17-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-01-20:42:17-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:42:17-root-INFO: step: 184 lr_xt 0.00409583
2024-12-01-20:42:17-root-INFO: grad norm: 256.206 248.985 60.399
2024-12-01-20:42:17-root-INFO: Loss too large (1906.673->2129.139)! Learning rate decreased to 0.00328.
2024-12-01-20:42:17-root-INFO: Loss too large (1906.673->2084.352)! Learning rate decreased to 0.00262.
2024-12-01-20:42:17-root-INFO: Loss too large (1906.673->2045.348)! Learning rate decreased to 0.00210.
2024-12-01-20:42:18-root-INFO: Loss too large (1906.673->2010.348)! Learning rate decreased to 0.00168.
2024-12-01-20:42:18-root-INFO: Loss too large (1906.673->1978.815)! Learning rate decreased to 0.00134.
2024-12-01-20:42:18-root-INFO: Loss too large (1906.673->1951.710)! Learning rate decreased to 0.00107.
2024-12-01-20:42:18-root-INFO: Loss too large (1906.673->1930.314)! Learning rate decreased to 0.00086.
2024-12-01-20:42:18-root-INFO: Loss too large (1906.673->1914.995)! Learning rate decreased to 0.00069.
2024-12-01-20:42:19-root-INFO: grad norm: 283.163 279.043 48.127
2024-12-01-20:42:19-root-INFO: grad norm: 353.534 347.628 64.351
2024-12-01-20:42:19-root-INFO: Loss too large (1901.299->1911.584)! Learning rate decreased to 0.00055.
2024-12-01-20:42:20-root-INFO: grad norm: 288.314 284.329 47.767
2024-12-01-20:42:20-root-INFO: grad norm: 216.118 211.483 44.518
2024-12-01-20:42:20-root-INFO: Loss Change: 1906.673 -> 1881.481
2024-12-01-20:42:20-root-INFO: Regularization Change: 0.000 -> 0.084
2024-12-01-20:42:20-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-01-20:42:20-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:42:21-root-INFO: step: 183 lr_xt 0.00427020
2024-12-01-20:42:21-root-INFO: grad norm: 132.237 127.443 35.285
2024-12-01-20:42:21-root-INFO: Loss too large (1870.137->2019.128)! Learning rate decreased to 0.00342.
2024-12-01-20:42:21-root-INFO: Loss too large (1870.137->1960.237)! Learning rate decreased to 0.00273.
2024-12-01-20:42:21-root-INFO: Loss too large (1870.137->1920.698)! Learning rate decreased to 0.00219.
2024-12-01-20:42:21-root-INFO: Loss too large (1870.137->1895.496)! Learning rate decreased to 0.00175.
2024-12-01-20:42:22-root-INFO: Loss too large (1870.137->1880.252)! Learning rate decreased to 0.00140.
2024-12-01-20:42:22-root-INFO: Loss too large (1870.137->1871.561)! Learning rate decreased to 0.00112.
2024-12-01-20:42:22-root-INFO: grad norm: 260.099 255.696 47.659
2024-12-01-20:42:22-root-INFO: Loss too large (1866.985->1910.378)! Learning rate decreased to 0.00090.
2024-12-01-20:42:23-root-INFO: Loss too large (1866.985->1890.144)! Learning rate decreased to 0.00072.
2024-12-01-20:42:23-root-INFO: Loss too large (1866.985->1875.669)! Learning rate decreased to 0.00057.
2024-12-01-20:42:23-root-INFO: grad norm: 259.355 255.653 43.664
2024-12-01-20:42:24-root-INFO: grad norm: 259.967 255.689 46.965
2024-12-01-20:42:24-root-INFO: grad norm: 260.955 257.340 43.286
2024-12-01-20:42:24-root-INFO: Loss Change: 1870.137 -> 1854.363
2024-12-01-20:42:24-root-INFO: Regularization Change: 0.000 -> 0.083
2024-12-01-20:42:24-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-01-20:42:24-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:42:25-root-INFO: step: 182 lr_xt 0.00445127
2024-12-01-20:42:25-root-INFO: grad norm: 549.167 542.616 84.575
2024-12-01-20:42:25-root-INFO: Loss too large (1879.218->2149.606)! Learning rate decreased to 0.00356.
2024-12-01-20:42:25-root-INFO: Loss too large (1879.218->2124.678)! Learning rate decreased to 0.00285.
2024-12-01-20:42:25-root-INFO: Loss too large (1879.218->2101.436)! Learning rate decreased to 0.00228.
2024-12-01-20:42:25-root-INFO: Loss too large (1879.218->2077.102)! Learning rate decreased to 0.00182.
2024-12-01-20:42:25-root-INFO: Loss too large (1879.218->2050.186)! Learning rate decreased to 0.00146.
2024-12-01-20:42:26-root-INFO: Loss too large (1879.218->2020.410)! Learning rate decreased to 0.00117.
2024-12-01-20:42:26-root-INFO: Loss too large (1879.218->1987.935)! Learning rate decreased to 0.00093.
2024-12-01-20:42:26-root-INFO: Loss too large (1879.218->1953.066)! Learning rate decreased to 0.00075.
2024-12-01-20:42:26-root-INFO: Loss too large (1879.218->1917.589)! Learning rate decreased to 0.00060.
2024-12-01-20:42:26-root-INFO: Loss too large (1879.218->1885.854)! Learning rate decreased to 0.00048.
2024-12-01-20:42:27-root-INFO: grad norm: 356.947 352.378 56.934
2024-12-01-20:42:27-root-INFO: grad norm: 193.299 189.369 38.781
2024-12-01-20:42:28-root-INFO: grad norm: 166.312 163.497 30.468
2024-12-01-20:42:28-root-INFO: grad norm: 141.827 138.242 31.685
2024-12-01-20:42:28-root-INFO: Loss Change: 1879.218 -> 1828.218
2024-12-01-20:42:28-root-INFO: Regularization Change: 0.000 -> 0.062
2024-12-01-20:42:28-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-01-20:42:28-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-20:42:29-root-INFO: step: 181 lr_xt 0.00463927
2024-12-01-20:42:29-root-INFO: grad norm: 200.134 195.440 43.089
2024-12-01-20:42:29-root-INFO: Loss too large (1826.880->2036.239)! Learning rate decreased to 0.00371.
2024-12-01-20:42:29-root-INFO: Loss too large (1826.880->2002.402)! Learning rate decreased to 0.00297.
2024-12-01-20:42:29-root-INFO: Loss too large (1826.880->1969.955)! Learning rate decreased to 0.00238.
2024-12-01-20:42:29-root-INFO: Loss too large (1826.880->1938.278)! Learning rate decreased to 0.00190.
2024-12-01-20:42:30-root-INFO: Loss too large (1826.880->1908.053)! Learning rate decreased to 0.00152.
2024-12-01-20:42:30-root-INFO: Loss too large (1826.880->1881.256)! Learning rate decreased to 0.00122.
2024-12-01-20:42:30-root-INFO: Loss too large (1826.880->1859.677)! Learning rate decreased to 0.00097.
2024-12-01-20:42:30-root-INFO: Loss too large (1826.880->1843.868)! Learning rate decreased to 0.00078.
2024-12-01-20:42:30-root-INFO: Loss too large (1826.880->1833.262)! Learning rate decreased to 0.00062.
2024-12-01-20:42:31-root-INFO: grad norm: 254.635 251.451 40.145
2024-12-01-20:42:31-root-INFO: Loss too large (1826.743->1828.675)! Learning rate decreased to 0.00050.
2024-12-01-20:42:31-root-INFO: grad norm: 235.192 231.135 43.497
2024-12-01-20:42:32-root-INFO: grad norm: 225.608 222.709 36.047
2024-12-01-20:42:32-root-INFO: grad norm: 215.574 211.841 39.944
2024-12-01-20:42:33-root-INFO: Loss Change: 1826.880 -> 1812.252
2024-12-01-20:42:33-root-INFO: Regularization Change: 0.000 -> 0.051
2024-12-01-20:42:33-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-01-20:42:33-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-20:42:33-root-INFO: step: 180 lr_xt 0.00483443
2024-12-01-20:42:33-root-INFO: grad norm: 129.538 123.823 38.053
2024-12-01-20:42:33-root-INFO: Loss too large (1807.734->1911.904)! Learning rate decreased to 0.00387.
2024-12-01-20:42:33-root-INFO: Loss too large (1807.734->1876.879)! Learning rate decreased to 0.00309.
2024-12-01-20:42:33-root-INFO: Loss too large (1807.734->1850.220)! Learning rate decreased to 0.00248.
2024-12-01-20:42:34-root-INFO: Loss too large (1807.734->1831.106)! Learning rate decreased to 0.00198.
2024-12-01-20:42:34-root-INFO: Loss too large (1807.734->1818.261)! Learning rate decreased to 0.00158.
2024-12-01-20:42:34-root-INFO: Loss too large (1807.734->1810.208)! Learning rate decreased to 0.00127.
2024-12-01-20:42:34-root-INFO: grad norm: 280.336 276.912 43.677
2024-12-01-20:42:35-root-INFO: Loss too large (1805.552->1922.651)! Learning rate decreased to 0.00101.
2024-12-01-20:42:35-root-INFO: Loss too large (1805.552->1870.355)! Learning rate decreased to 0.00081.
2024-12-01-20:42:35-root-INFO: Loss too large (1805.552->1835.436)! Learning rate decreased to 0.00065.
2024-12-01-20:42:35-root-INFO: Loss too large (1805.552->1813.984)! Learning rate decreased to 0.00052.
2024-12-01-20:42:35-root-INFO: grad norm: 307.039 302.708 51.383
2024-12-01-20:42:36-root-INFO: Loss too large (1801.982->1803.244)! Learning rate decreased to 0.00042.
2024-12-01-20:42:36-root-INFO: grad norm: 230.424 227.471 36.768
2024-12-01-20:42:37-root-INFO: grad norm: 169.505 165.778 35.349
2024-12-01-20:42:37-root-INFO: Loss Change: 1807.734 -> 1786.313
2024-12-01-20:42:37-root-INFO: Regularization Change: 0.000 -> 0.070
2024-12-01-20:42:37-root-INFO: Undo step: 180
2024-12-01-20:42:37-root-INFO: Undo step: 181
2024-12-01-20:42:37-root-INFO: Undo step: 182
2024-12-01-20:42:37-root-INFO: Undo step: 183
2024-12-01-20:42:37-root-INFO: Undo step: 184
2024-12-01-20:42:37-root-INFO: step: 185 lr_xt 0.00392795
2024-12-01-20:42:37-root-INFO: grad norm: 598.156 568.922 184.713
2024-12-01-20:42:38-root-INFO: grad norm: 734.696 706.328 202.187
2024-12-01-20:42:38-root-INFO: Loss too large (2840.637->2997.900)! Learning rate decreased to 0.00314.
2024-12-01-20:42:38-root-INFO: grad norm: 629.123 613.484 139.401
2024-12-01-20:42:39-root-INFO: grad norm: 509.360 494.257 123.118
2024-12-01-20:42:39-root-INFO: grad norm: 400.459 389.231 94.162
2024-12-01-20:42:40-root-INFO: Loss Change: 2858.446 -> 2174.867
2024-12-01-20:42:40-root-INFO: Regularization Change: 0.000 -> 10.135
2024-12-01-20:42:40-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-01-20:42:40-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:42:40-root-INFO: step: 184 lr_xt 0.00409583
2024-12-01-20:42:40-root-INFO: grad norm: 387.960 377.610 89.018
2024-12-01-20:42:40-root-INFO: Loss too large (2172.290->2424.556)! Learning rate decreased to 0.00328.
2024-12-01-20:42:40-root-INFO: Loss too large (2172.290->2249.295)! Learning rate decreased to 0.00262.
2024-12-01-20:42:41-root-INFO: grad norm: 573.476 562.504 111.642
2024-12-01-20:42:41-root-INFO: Loss too large (2137.159->2205.692)! Learning rate decreased to 0.00210.
2024-12-01-20:42:41-root-INFO: Loss too large (2137.159->2165.851)! Learning rate decreased to 0.00168.
2024-12-01-20:42:42-root-INFO: grad norm: 226.173 220.827 48.884
2024-12-01-20:42:42-root-INFO: grad norm: 224.918 220.050 46.540
2024-12-01-20:42:43-root-INFO: grad norm: 141.513 135.976 39.197
2024-12-01-20:42:43-root-INFO: Loss Change: 2172.290 -> 1964.612
2024-12-01-20:42:43-root-INFO: Regularization Change: 0.000 -> 1.119
2024-12-01-20:42:43-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-01-20:42:43-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:42:43-root-INFO: step: 183 lr_xt 0.00427020
2024-12-01-20:42:43-root-INFO: grad norm: 128.489 123.522 35.380
2024-12-01-20:42:43-root-INFO: Loss too large (1953.023->2023.231)! Learning rate decreased to 0.00342.
2024-12-01-20:42:43-root-INFO: Loss too large (1953.023->1986.123)! Learning rate decreased to 0.00273.
2024-12-01-20:42:44-root-INFO: Loss too large (1953.023->1963.500)! Learning rate decreased to 0.00219.
2024-12-01-20:42:44-root-INFO: grad norm: 392.144 385.585 71.423
2024-12-01-20:42:44-root-INFO: Loss too large (1951.027->2074.686)! Learning rate decreased to 0.00175.
2024-12-01-20:42:44-root-INFO: Loss too large (1951.027->2047.367)! Learning rate decreased to 0.00140.
2024-12-01-20:42:45-root-INFO: Loss too large (1951.027->2019.462)! Learning rate decreased to 0.00112.
2024-12-01-20:42:45-root-INFO: Loss too large (1951.027->1992.574)! Learning rate decreased to 0.00090.
2024-12-01-20:42:45-root-INFO: Loss too large (1951.027->1968.863)! Learning rate decreased to 0.00072.
2024-12-01-20:42:45-root-INFO: grad norm: 290.311 285.101 54.749
2024-12-01-20:42:46-root-INFO: grad norm: 160.835 156.107 38.711
2024-12-01-20:42:46-root-INFO: grad norm: 169.305 165.693 34.785
2024-12-01-20:42:47-root-INFO: Loss Change: 1953.023 -> 1910.874
2024-12-01-20:42:47-root-INFO: Regularization Change: 0.000 -> 0.228
2024-12-01-20:42:47-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-01-20:42:47-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-20:42:47-root-INFO: step: 182 lr_xt 0.00445127
2024-12-01-20:42:47-root-INFO: grad norm: 455.321 448.175 80.351
2024-12-01-20:42:47-root-INFO: Loss too large (1923.273->2146.894)! Learning rate decreased to 0.00356.
2024-12-01-20:42:47-root-INFO: Loss too large (1923.273->2125.994)! Learning rate decreased to 0.00285.
2024-12-01-20:42:47-root-INFO: Loss too large (1923.273->2103.831)! Learning rate decreased to 0.00228.
2024-12-01-20:42:48-root-INFO: Loss too large (1923.273->2079.485)! Learning rate decreased to 0.00182.
2024-12-01-20:42:48-root-INFO: Loss too large (1923.273->2052.950)! Learning rate decreased to 0.00146.
2024-12-01-20:42:48-root-INFO: Loss too large (1923.273->2024.525)! Learning rate decreased to 0.00117.
2024-12-01-20:42:48-root-INFO: Loss too large (1923.273->1994.960)! Learning rate decreased to 0.00093.
2024-12-01-20:42:48-root-INFO: Loss too large (1923.273->1965.938)! Learning rate decreased to 0.00075.
2024-12-01-20:42:48-root-INFO: Loss too large (1923.273->1939.997)! Learning rate decreased to 0.00060.
2024-12-01-20:42:49-root-INFO: grad norm: 320.268 314.982 57.949
2024-12-01-20:42:49-root-INFO: grad norm: 161.955 157.421 38.054
2024-12-01-20:42:50-root-INFO: grad norm: 162.669 159.324 32.818
2024-12-01-20:42:50-root-INFO: grad norm: 168.952 164.696 37.686
2024-12-01-20:42:51-root-INFO: Loss Change: 1923.273 -> 1878.825
2024-12-01-20:42:51-root-INFO: Regularization Change: 0.000 -> 0.092
2024-12-01-20:42:51-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-01-20:42:51-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-20:42:51-root-INFO: step: 181 lr_xt 0.00463927
2024-12-01-20:42:51-root-INFO: grad norm: 104.278 99.079 32.516
2024-12-01-20:42:51-root-INFO: Loss too large (1867.380->1890.291)! Learning rate decreased to 0.00371.
2024-12-01-20:42:51-root-INFO: Loss too large (1867.380->1877.385)! Learning rate decreased to 0.00297.
2024-12-01-20:42:51-root-INFO: Loss too large (1867.380->1869.067)! Learning rate decreased to 0.00238.
2024-12-01-20:42:52-root-INFO: grad norm: 295.472 291.169 50.245
2024-12-01-20:42:52-root-INFO: Loss too large (1864.152->2232.190)! Learning rate decreased to 0.00190.
2024-12-01-20:42:52-root-INFO: Loss too large (1864.152->2123.680)! Learning rate decreased to 0.00152.
2024-12-01-20:42:52-root-INFO: Loss too large (1864.152->2033.697)! Learning rate decreased to 0.00122.
2024-12-01-20:42:52-root-INFO: Loss too large (1864.152->1963.469)! Learning rate decreased to 0.00097.
2024-12-01-20:42:53-root-INFO: Loss too large (1864.152->1912.926)! Learning rate decreased to 0.00078.
2024-12-01-20:42:53-root-INFO: Loss too large (1864.152->1880.031)! Learning rate decreased to 0.00062.
2024-12-01-20:42:53-root-INFO: grad norm: 372.555 367.341 62.110
2024-12-01-20:42:53-root-INFO: Loss too large (1861.015->1872.872)! Learning rate decreased to 0.00050.
2024-12-01-20:42:54-root-INFO: grad norm: 304.384 299.983 51.571
2024-12-01-20:42:54-root-INFO: grad norm: 230.019 225.986 42.885
2024-12-01-20:42:55-root-INFO: Loss Change: 1867.380 -> 1843.196
2024-12-01-20:42:55-root-INFO: Regularization Change: 0.000 -> 0.140
2024-12-01-20:42:55-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-01-20:42:55-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-20:42:55-root-INFO: step: 180 lr_xt 0.00483443
2024-12-01-20:42:55-root-INFO: grad norm: 113.287 107.978 34.275
2024-12-01-20:42:55-root-INFO: Loss too large (1832.126->1885.088)! Learning rate decreased to 0.00387.
2024-12-01-20:42:56-root-INFO: Loss too large (1832.126->1864.037)! Learning rate decreased to 0.00309.
2024-12-01-20:42:56-root-INFO: Loss too large (1832.126->1848.916)! Learning rate decreased to 0.00248.
2024-12-01-20:42:56-root-INFO: Loss too large (1832.126->1838.736)! Learning rate decreased to 0.00198.
2024-12-01-20:42:56-root-INFO: Loss too large (1832.126->1832.371)! Learning rate decreased to 0.00158.
2024-12-01-20:42:56-root-INFO: grad norm: 277.763 273.902 46.155
2024-12-01-20:42:57-root-INFO: Loss too large (1828.747->2021.079)! Learning rate decreased to 0.00127.
2024-12-01-20:42:57-root-INFO: Loss too large (1828.747->1950.337)! Learning rate decreased to 0.00101.
2024-12-01-20:42:57-root-INFO: Loss too large (1828.747->1897.527)! Learning rate decreased to 0.00081.
2024-12-01-20:42:57-root-INFO: Loss too large (1828.747->1861.229)! Learning rate decreased to 0.00065.
2024-12-01-20:42:57-root-INFO: Loss too large (1828.747->1838.502)! Learning rate decreased to 0.00052.
2024-12-01-20:42:58-root-INFO: grad norm: 325.852 321.043 55.773
2024-12-01-20:42:58-root-INFO: Loss too large (1825.695->1830.203)! Learning rate decreased to 0.00042.
2024-12-01-20:42:58-root-INFO: grad norm: 259.604 256.012 43.036
2024-12-01-20:42:59-root-INFO: grad norm: 197.180 193.364 38.605
2024-12-01-20:42:59-root-INFO: Loss Change: 1832.126 -> 1810.794
2024-12-01-20:42:59-root-INFO: Regularization Change: 0.000 -> 0.083
2024-12-01-20:42:59-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-01-20:42:59-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-20:42:59-root-INFO: step: 179 lr_xt 0.00503698
2024-12-01-20:43:00-root-INFO: grad norm: 173.130 169.022 37.491
2024-12-01-20:43:00-root-INFO: Loss too large (1805.017->1987.168)! Learning rate decreased to 0.00403.
2024-12-01-20:43:00-root-INFO: Loss too large (1805.017->1962.586)! Learning rate decreased to 0.00322.
2024-12-01-20:43:00-root-INFO: Loss too large (1805.017->1936.775)! Learning rate decreased to 0.00258.
2024-12-01-20:43:00-root-INFO: Loss too large (1805.017->1910.144)! Learning rate decreased to 0.00206.
2024-12-01-20:43:00-root-INFO: Loss too large (1805.017->1884.085)! Learning rate decreased to 0.00165.
2024-12-01-20:43:01-root-INFO: Loss too large (1805.017->1860.481)! Learning rate decreased to 0.00132.
2024-12-01-20:43:01-root-INFO: Loss too large (1805.017->1840.865)! Learning rate decreased to 0.00106.
2024-12-01-20:43:01-root-INFO: Loss too large (1805.017->1825.905)! Learning rate decreased to 0.00085.
2024-12-01-20:43:01-root-INFO: Loss too large (1805.017->1815.390)! Learning rate decreased to 0.00068.
2024-12-01-20:43:01-root-INFO: Loss too large (1805.017->1808.554)! Learning rate decreased to 0.00054.
2024-12-01-20:43:02-root-INFO: grad norm: 230.870 227.603 38.706
2024-12-01-20:43:02-root-INFO: Loss too large (1804.459->1806.186)! Learning rate decreased to 0.00043.
2024-12-01-20:43:02-root-INFO: grad norm: 228.529 224.552 42.449
2024-12-01-20:43:03-root-INFO: grad norm: 228.101 224.950 37.781
2024-12-01-20:43:03-root-INFO: grad norm: 228.771 224.964 41.560
2024-12-01-20:43:04-root-INFO: Loss Change: 1805.017 -> 1794.021
2024-12-01-20:43:04-root-INFO: Regularization Change: 0.000 -> 0.039
2024-12-01-20:43:04-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-01-20:43:04-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-20:43:04-root-INFO: step: 178 lr_xt 0.00524717
2024-12-01-20:43:04-root-INFO: grad norm: 129.735 124.920 35.017
2024-12-01-20:43:04-root-INFO: Loss too large (1787.824->1908.011)! Learning rate decreased to 0.00420.
2024-12-01-20:43:04-root-INFO: Loss too large (1787.824->1877.395)! Learning rate decreased to 0.00336.
2024-12-01-20:43:04-root-INFO: Loss too large (1787.824->1850.964)! Learning rate decreased to 0.00269.
2024-12-01-20:43:05-root-INFO: Loss too large (1787.824->1829.264)! Learning rate decreased to 0.00215.
2024-12-01-20:43:05-root-INFO: Loss too large (1787.824->1812.603)! Learning rate decreased to 0.00172.
2024-12-01-20:43:05-root-INFO: Loss too large (1787.824->1800.697)! Learning rate decreased to 0.00138.
2024-12-01-20:43:05-root-INFO: Loss too large (1787.824->1792.792)! Learning rate decreased to 0.00110.
2024-12-01-20:43:05-root-INFO: Loss too large (1787.824->1787.935)! Learning rate decreased to 0.00088.
2024-12-01-20:43:06-root-INFO: grad norm: 255.743 252.226 42.268
2024-12-01-20:43:06-root-INFO: Loss too large (1785.222->1834.035)! Learning rate decreased to 0.00070.
2024-12-01-20:43:06-root-INFO: Loss too large (1785.222->1808.399)! Learning rate decreased to 0.00056.
2024-12-01-20:43:06-root-INFO: Loss too large (1785.222->1792.525)! Learning rate decreased to 0.00045.
2024-12-01-20:43:07-root-INFO: grad norm: 305.645 301.211 51.875
2024-12-01-20:43:07-root-INFO: Loss too large (1783.538->1785.718)! Learning rate decreased to 0.00036.
2024-12-01-20:43:07-root-INFO: grad norm: 245.311 242.004 40.146
2024-12-01-20:43:08-root-INFO: grad norm: 193.441 189.799 37.360
2024-12-01-20:43:08-root-INFO: Loss Change: 1787.824 -> 1770.547
2024-12-01-20:43:08-root-INFO: Regularization Change: 0.000 -> 0.048
2024-12-01-20:43:08-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-01-20:43:08-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-20:43:08-root-INFO: step: 177 lr_xt 0.00546525
2024-12-01-20:43:08-root-INFO: grad norm: 119.405 115.061 31.915
2024-12-01-20:43:08-root-INFO: Loss too large (1765.099->1884.208)! Learning rate decreased to 0.00437.
2024-12-01-20:43:09-root-INFO: Loss too large (1765.099->1857.115)! Learning rate decreased to 0.00350.
2024-12-01-20:43:09-root-INFO: Loss too large (1765.099->1832.293)! Learning rate decreased to 0.00280.
2024-12-01-20:43:09-root-INFO: Loss too large (1765.099->1810.969)! Learning rate decreased to 0.00224.
2024-12-01-20:43:09-root-INFO: Loss too large (1765.099->1794.007)! Learning rate decreased to 0.00179.
2024-12-01-20:43:09-root-INFO: Loss too large (1765.099->1781.519)! Learning rate decreased to 0.00143.
2024-12-01-20:43:09-root-INFO: Loss too large (1765.099->1772.983)! Learning rate decreased to 0.00115.
2024-12-01-20:43:10-root-INFO: Loss too large (1765.099->1767.555)! Learning rate decreased to 0.00092.
2024-12-01-20:43:10-root-INFO: grad norm: 272.549 269.012 43.767
2024-12-01-20:43:10-root-INFO: Loss too large (1764.361->1832.866)! Learning rate decreased to 0.00073.
2024-12-01-20:43:10-root-INFO: Loss too large (1764.361->1800.460)! Learning rate decreased to 0.00059.
2024-12-01-20:43:11-root-INFO: Loss too large (1764.361->1779.343)! Learning rate decreased to 0.00047.
2024-12-01-20:43:11-root-INFO: Loss too large (1764.361->1766.694)! Learning rate decreased to 0.00038.
2024-12-01-20:43:11-root-INFO: grad norm: 255.065 251.250 43.950
2024-12-01-20:43:12-root-INFO: grad norm: 245.241 242.025 39.590
2024-12-01-20:43:12-root-INFO: grad norm: 235.137 231.572 40.791
2024-12-01-20:43:12-root-INFO: Loss Change: 1765.099 -> 1752.421
2024-12-01-20:43:12-root-INFO: Regularization Change: 0.000 -> 0.045
2024-12-01-20:43:12-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-01-20:43:12-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-20:43:13-root-INFO: step: 176 lr_xt 0.00569148
2024-12-01-20:43:13-root-INFO: grad norm: 205.406 201.160 41.547
2024-12-01-20:43:13-root-INFO: Loss too large (1752.099->1973.656)! Learning rate decreased to 0.00455.
2024-12-01-20:43:13-root-INFO: Loss too large (1752.099->1951.842)! Learning rate decreased to 0.00364.
2024-12-01-20:43:13-root-INFO: Loss too large (1752.099->1929.409)! Learning rate decreased to 0.00291.
2024-12-01-20:43:13-root-INFO: Loss too large (1752.099->1905.311)! Learning rate decreased to 0.00233.
2024-12-01-20:43:14-root-INFO: Loss too large (1752.099->1879.195)! Learning rate decreased to 0.00186.
2024-12-01-20:43:14-root-INFO: Loss too large (1752.099->1851.815)! Learning rate decreased to 0.00149.
2024-12-01-20:43:14-root-INFO: Loss too large (1752.099->1825.080)! Learning rate decreased to 0.00119.
2024-12-01-20:43:14-root-INFO: Loss too large (1752.099->1801.270)! Learning rate decreased to 0.00095.
2024-12-01-20:43:14-root-INFO: Loss too large (1752.099->1782.042)! Learning rate decreased to 0.00076.
2024-12-01-20:43:14-root-INFO: Loss too large (1752.099->1767.915)! Learning rate decreased to 0.00061.
2024-12-01-20:43:14-root-INFO: Loss too large (1752.099->1758.420)! Learning rate decreased to 0.00049.
2024-12-01-20:43:15-root-INFO: Loss too large (1752.099->1752.571)! Learning rate decreased to 0.00039.
2024-12-01-20:43:15-root-INFO: grad norm: 220.537 217.396 37.092
2024-12-01-20:43:15-root-INFO: grad norm: 240.737 236.750 43.631
2024-12-01-20:43:16-root-INFO: grad norm: 257.627 254.419 40.525
2024-12-01-20:43:17-root-INFO: grad norm: 276.920 272.976 46.568
2024-12-01-20:43:17-root-INFO: Loss Change: 1752.099 -> 1741.915
2024-12-01-20:43:17-root-INFO: Regularization Change: 0.000 -> 0.038
2024-12-01-20:43:17-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-01-20:43:17-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-20:43:17-root-INFO: step: 175 lr_xt 0.00592610
2024-12-01-20:43:17-root-INFO: grad norm: 110.823 106.481 30.717
2024-12-01-20:43:17-root-INFO: Loss too large (1734.372->1803.384)! Learning rate decreased to 0.00474.
2024-12-01-20:43:18-root-INFO: Loss too large (1734.372->1777.259)! Learning rate decreased to 0.00379.
2024-12-01-20:43:18-root-INFO: Loss too large (1734.372->1758.038)! Learning rate decreased to 0.00303.
2024-12-01-20:43:18-root-INFO: Loss too large (1734.372->1744.820)! Learning rate decreased to 0.00243.
2024-12-01-20:43:18-root-INFO: Loss too large (1734.372->1736.394)! Learning rate decreased to 0.00194.
2024-12-01-20:43:19-root-INFO: grad norm: 365.660 361.723 53.516
2024-12-01-20:43:19-root-INFO: Loss too large (1731.485->2112.143)! Learning rate decreased to 0.00155.
2024-12-01-20:43:19-root-INFO: Loss too large (1731.485->2018.341)! Learning rate decreased to 0.00124.
2024-12-01-20:43:19-root-INFO: Loss too large (1731.485->1935.956)! Learning rate decreased to 0.00099.
2024-12-01-20:43:19-root-INFO: Loss too large (1731.485->1867.074)! Learning rate decreased to 0.00080.
2024-12-01-20:43:19-root-INFO: Loss too large (1731.485->1812.870)! Learning rate decreased to 0.00064.
2024-12-01-20:43:20-root-INFO: Loss too large (1731.485->1773.146)! Learning rate decreased to 0.00051.
2024-12-01-20:43:20-root-INFO: Loss too large (1731.485->1746.360)! Learning rate decreased to 0.00041.
2024-12-01-20:43:20-root-INFO: grad norm: 398.778 394.626 57.396
2024-12-01-20:43:21-root-INFO: Loss too large (1729.990->1731.759)! Learning rate decreased to 0.00033.
2024-12-01-20:43:21-root-INFO: grad norm: 292.297 289.084 43.221
2024-12-01-20:43:22-root-INFO: grad norm: 224.031 220.940 37.086
2024-12-01-20:43:22-root-INFO: Loss Change: 1734.372 -> 1712.236
2024-12-01-20:43:22-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-01-20:43:22-root-INFO: Undo step: 175
2024-12-01-20:43:22-root-INFO: Undo step: 176
2024-12-01-20:43:22-root-INFO: Undo step: 177
2024-12-01-20:43:22-root-INFO: Undo step: 178
2024-12-01-20:43:22-root-INFO: Undo step: 179
2024-12-01-20:43:22-root-INFO: step: 180 lr_xt 0.00483443
2024-12-01-20:43:22-root-INFO: grad norm: 927.078 909.374 180.315
2024-12-01-20:43:22-root-INFO: Loss too large (2957.149->2957.799)! Learning rate decreased to 0.00387.
2024-12-01-20:43:23-root-INFO: grad norm: 597.343 583.623 127.289
2024-12-01-20:43:23-root-INFO: grad norm: 441.005 433.586 80.556
2024-12-01-20:43:24-root-INFO: grad norm: 321.920 315.777 62.593
2024-12-01-20:43:24-root-INFO: grad norm: 254.523 249.718 49.221
2024-12-01-20:43:25-root-INFO: Loss Change: 2957.149 -> 1939.872
2024-12-01-20:43:25-root-INFO: Regularization Change: 0.000 -> 11.776
2024-12-01-20:43:25-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-01-20:43:25-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-20:43:25-root-INFO: step: 179 lr_xt 0.00503698
2024-12-01-20:43:25-root-INFO: grad norm: 283.895 278.979 52.598
2024-12-01-20:43:25-root-INFO: Loss too large (1947.295->2019.106)! Learning rate decreased to 0.00403.
2024-12-01-20:43:25-root-INFO: Loss too large (1947.295->1959.677)! Learning rate decreased to 0.00322.
2024-12-01-20:43:26-root-INFO: grad norm: 245.720 241.917 43.062
2024-12-01-20:43:26-root-INFO: grad norm: 471.879 466.932 68.150
2024-12-01-20:43:27-root-INFO: Loss too large (1895.439->2015.619)! Learning rate decreased to 0.00258.
2024-12-01-20:43:27-root-INFO: Loss too large (1895.439->1971.744)! Learning rate decreased to 0.00206.
2024-12-01-20:43:27-root-INFO: Loss too large (1895.439->1930.541)! Learning rate decreased to 0.00165.
2024-12-01-20:43:27-root-INFO: grad norm: 221.513 217.470 42.129
2024-12-01-20:43:28-root-INFO: grad norm: 156.998 153.906 31.001
2024-12-01-20:43:28-root-INFO: Loss Change: 1947.295 -> 1800.080
2024-12-01-20:43:28-root-INFO: Regularization Change: 0.000 -> 1.146
2024-12-01-20:43:28-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-01-20:43:28-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-20:43:28-root-INFO: step: 178 lr_xt 0.00524717
2024-12-01-20:43:29-root-INFO: grad norm: 240.586 236.885 42.036
2024-12-01-20:43:29-root-INFO: Loss too large (1802.596->1957.719)! Learning rate decreased to 0.00420.
2024-12-01-20:43:29-root-INFO: Loss too large (1802.596->1917.278)! Learning rate decreased to 0.00336.
2024-12-01-20:43:29-root-INFO: Loss too large (1802.596->1880.623)! Learning rate decreased to 0.00269.
2024-12-01-20:43:29-root-INFO: Loss too large (1802.596->1849.105)! Learning rate decreased to 0.00215.
2024-12-01-20:43:29-root-INFO: Loss too large (1802.596->1823.830)! Learning rate decreased to 0.00172.
2024-12-01-20:43:30-root-INFO: Loss too large (1802.596->1805.243)! Learning rate decreased to 0.00138.
2024-12-01-20:43:30-root-INFO: grad norm: 210.181 206.859 37.218
2024-12-01-20:43:30-root-INFO: grad norm: 220.452 217.599 35.349
2024-12-01-20:43:31-root-INFO: Loss too large (1771.163->1777.656)! Learning rate decreased to 0.00110.
2024-12-01-20:43:31-root-INFO: grad norm: 208.386 205.330 35.560
2024-12-01-20:43:32-root-INFO: grad norm: 221.759 219.190 33.657
2024-12-01-20:43:32-root-INFO: Loss too large (1753.551->1757.552)! Learning rate decreased to 0.00088.
2024-12-01-20:43:32-root-INFO: Loss Change: 1802.596 -> 1749.566
2024-12-01-20:43:32-root-INFO: Regularization Change: 0.000 -> 0.313
2024-12-01-20:43:32-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-01-20:43:32-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-20:43:32-root-INFO: step: 177 lr_xt 0.00546525
2024-12-01-20:43:33-root-INFO: grad norm: 163.686 160.461 32.331
2024-12-01-20:43:33-root-INFO: Loss too large (1735.521->2160.570)! Learning rate decreased to 0.00437.
2024-12-01-20:43:33-root-INFO: Loss too large (1735.521->2040.890)! Learning rate decreased to 0.00350.
2024-12-01-20:43:33-root-INFO: Loss too large (1735.521->1939.983)! Learning rate decreased to 0.00280.
2024-12-01-20:43:33-root-INFO: Loss too large (1735.521->1861.207)! Learning rate decreased to 0.00224.
2024-12-01-20:43:33-root-INFO: Loss too large (1735.521->1804.927)! Learning rate decreased to 0.00179.
2024-12-01-20:43:34-root-INFO: Loss too large (1735.521->1768.234)! Learning rate decreased to 0.00143.
2024-12-01-20:43:34-root-INFO: Loss too large (1735.521->1746.390)! Learning rate decreased to 0.00115.
2024-12-01-20:43:34-root-INFO: grad norm: 290.924 288.256 39.311
2024-12-01-20:43:34-root-INFO: Loss too large (1734.563->1759.787)! Learning rate decreased to 0.00092.
2024-12-01-20:43:35-root-INFO: Loss too large (1734.563->1743.032)! Learning rate decreased to 0.00073.
2024-12-01-20:43:35-root-INFO: grad norm: 234.681 231.563 38.132
2024-12-01-20:43:36-root-INFO: grad norm: 172.409 170.118 28.012
2024-12-01-20:43:36-root-INFO: grad norm: 171.650 168.867 30.782
2024-12-01-20:43:36-root-INFO: Loss Change: 1735.521 -> 1708.012
2024-12-01-20:43:36-root-INFO: Regularization Change: 0.000 -> 0.118
2024-12-01-20:43:36-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-01-20:43:36-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-20:43:36-root-INFO: step: 176 lr_xt 0.00569148
2024-12-01-20:43:37-root-INFO: grad norm: 429.257 425.823 54.194
2024-12-01-20:43:37-root-INFO: Loss too large (1726.760->1955.487)! Learning rate decreased to 0.00455.
2024-12-01-20:43:37-root-INFO: Loss too large (1726.760->1937.611)! Learning rate decreased to 0.00364.
2024-12-01-20:43:37-root-INFO: Loss too large (1726.760->1918.463)! Learning rate decreased to 0.00291.
2024-12-01-20:43:37-root-INFO: Loss too large (1726.760->1896.479)! Learning rate decreased to 0.00233.
2024-12-01-20:43:37-root-INFO: Loss too large (1726.760->1871.415)! Learning rate decreased to 0.00186.
2024-12-01-20:43:38-root-INFO: Loss too large (1726.760->1843.903)! Learning rate decreased to 0.00149.
2024-12-01-20:43:38-root-INFO: Loss too large (1726.760->1814.889)! Learning rate decreased to 0.00119.
2024-12-01-20:43:38-root-INFO: Loss too large (1726.760->1785.688)! Learning rate decreased to 0.00095.
2024-12-01-20:43:38-root-INFO: Loss too large (1726.760->1758.113)! Learning rate decreased to 0.00076.
2024-12-01-20:43:38-root-INFO: Loss too large (1726.760->1734.421)! Learning rate decreased to 0.00061.
2024-12-01-20:43:39-root-INFO: grad norm: 282.725 279.473 42.758
2024-12-01-20:43:39-root-INFO: grad norm: 140.966 138.464 26.439
2024-12-01-20:43:39-root-INFO: grad norm: 129.468 126.766 26.315
2024-12-01-20:43:40-root-INFO: grad norm: 121.322 118.997 23.635
2024-12-01-20:43:40-root-INFO: Loss Change: 1726.760 -> 1681.901
2024-12-01-20:43:40-root-INFO: Regularization Change: 0.000 -> 0.083
2024-12-01-20:43:40-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-01-20:43:40-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-20:43:40-root-INFO: step: 175 lr_xt 0.00592610
2024-12-01-20:43:41-root-INFO: grad norm: 137.416 134.592 27.715
2024-12-01-20:43:41-root-INFO: Loss too large (1678.219->1810.398)! Learning rate decreased to 0.00474.
2024-12-01-20:43:41-root-INFO: Loss too large (1678.219->1784.144)! Learning rate decreased to 0.00379.
2024-12-01-20:43:41-root-INFO: Loss too large (1678.219->1758.891)! Learning rate decreased to 0.00303.
2024-12-01-20:43:41-root-INFO: Loss too large (1678.219->1735.759)! Learning rate decreased to 0.00243.
2024-12-01-20:43:41-root-INFO: Loss too large (1678.219->1716.015)! Learning rate decreased to 0.00194.
2024-12-01-20:43:42-root-INFO: Loss too large (1678.219->1700.509)! Learning rate decreased to 0.00155.
2024-12-01-20:43:42-root-INFO: Loss too large (1678.219->1689.330)! Learning rate decreased to 0.00124.
2024-12-01-20:43:42-root-INFO: Loss too large (1678.219->1681.919)! Learning rate decreased to 0.00099.
2024-12-01-20:43:42-root-INFO: grad norm: 227.141 224.304 35.787
2024-12-01-20:43:42-root-INFO: Loss too large (1677.417->1697.474)! Learning rate decreased to 0.00080.
2024-12-01-20:43:43-root-INFO: Loss too large (1677.417->1680.911)! Learning rate decreased to 0.00064.
2024-12-01-20:43:43-root-INFO: grad norm: 239.001 236.774 32.549
2024-12-01-20:43:44-root-INFO: grad norm: 247.080 244.286 37.049
2024-12-01-20:43:44-root-INFO: grad norm: 261.575 259.401 33.655
2024-12-01-20:43:44-root-INFO: Loss too large (1666.578->1667.869)! Learning rate decreased to 0.00051.
2024-12-01-20:43:45-root-INFO: Loss Change: 1678.219 -> 1661.299
2024-12-01-20:43:45-root-INFO: Regularization Change: 0.000 -> 0.080
2024-12-01-20:43:45-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-01-20:43:45-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-20:43:45-root-INFO: step: 174 lr_xt 0.00616941
2024-12-01-20:43:45-root-INFO: grad norm: 142.845 140.213 27.298
2024-12-01-20:43:45-root-INFO: Loss too large (1657.006->2078.522)! Learning rate decreased to 0.00494.
2024-12-01-20:43:45-root-INFO: Loss too large (1657.006->1977.562)! Learning rate decreased to 0.00395.
2024-12-01-20:43:45-root-INFO: Loss too large (1657.006->1890.353)! Learning rate decreased to 0.00316.
2024-12-01-20:43:45-root-INFO: Loss too large (1657.006->1818.369)! Learning rate decreased to 0.00253.
2024-12-01-20:43:46-root-INFO: Loss too large (1657.006->1762.371)! Learning rate decreased to 0.00202.
2024-12-01-20:43:46-root-INFO: Loss too large (1657.006->1721.510)! Learning rate decreased to 0.00162.
2024-12-01-20:43:46-root-INFO: Loss too large (1657.006->1693.522)! Learning rate decreased to 0.00129.
2024-12-01-20:43:46-root-INFO: Loss too large (1657.006->1675.472)! Learning rate decreased to 0.00104.
2024-12-01-20:43:46-root-INFO: Loss too large (1657.006->1664.494)! Learning rate decreased to 0.00083.
2024-12-01-20:43:46-root-INFO: Loss too large (1657.006->1658.223)! Learning rate decreased to 0.00066.
2024-12-01-20:43:47-root-INFO: grad norm: 188.833 186.894 26.991
2024-12-01-20:43:47-root-INFO: Loss too large (1654.919->1655.827)! Learning rate decreased to 0.00053.
2024-12-01-20:43:47-root-INFO: grad norm: 176.603 174.121 29.505
2024-12-01-20:43:48-root-INFO: grad norm: 165.223 163.410 24.412
2024-12-01-20:43:48-root-INFO: grad norm: 159.137 156.781 27.283
2024-12-01-20:43:49-root-INFO: Loss Change: 1657.006 -> 1642.618
2024-12-01-20:43:49-root-INFO: Regularization Change: 0.000 -> 0.046
2024-12-01-20:43:49-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-01-20:43:49-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-20:43:49-root-INFO: step: 173 lr_xt 0.00642166
2024-12-01-20:43:49-root-INFO: grad norm: 408.090 404.903 50.905
2024-12-01-20:43:49-root-INFO: Loss too large (1663.618->1896.839)! Learning rate decreased to 0.00514.
2024-12-01-20:43:49-root-INFO: Loss too large (1663.618->1883.698)! Learning rate decreased to 0.00411.
2024-12-01-20:43:49-root-INFO: Loss too large (1663.618->1870.122)! Learning rate decreased to 0.00329.
2024-12-01-20:43:50-root-INFO: Loss too large (1663.618->1854.322)! Learning rate decreased to 0.00263.
2024-12-01-20:43:50-root-INFO: Loss too large (1663.618->1835.167)! Learning rate decreased to 0.00210.
2024-12-01-20:43:50-root-INFO: Loss too large (1663.618->1812.491)! Learning rate decreased to 0.00168.
2024-12-01-20:43:50-root-INFO: Loss too large (1663.618->1786.575)! Learning rate decreased to 0.00135.
2024-12-01-20:43:50-root-INFO: Loss too large (1663.618->1758.051)! Learning rate decreased to 0.00108.
2024-12-01-20:43:50-root-INFO: Loss too large (1663.618->1728.166)! Learning rate decreased to 0.00086.
2024-12-01-20:43:51-root-INFO: Loss too large (1663.618->1699.300)! Learning rate decreased to 0.00069.
2024-12-01-20:43:51-root-INFO: Loss too large (1663.618->1674.668)! Learning rate decreased to 0.00055.
2024-12-01-20:43:51-root-INFO: grad norm: 325.053 321.780 46.015
2024-12-01-20:43:52-root-INFO: grad norm: 247.092 244.704 34.275
2024-12-01-20:43:52-root-INFO: grad norm: 231.322 228.684 34.832
2024-12-01-20:43:53-root-INFO: grad norm: 216.004 213.870 30.293
2024-12-01-20:43:53-root-INFO: Loss Change: 1663.618 -> 1629.193
2024-12-01-20:43:53-root-INFO: Regularization Change: 0.000 -> 0.079
2024-12-01-20:43:53-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-01-20:43:53-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-20:43:53-root-INFO: step: 172 lr_xt 0.00668315
2024-12-01-20:43:53-root-INFO: grad norm: 120.256 117.672 24.795
2024-12-01-20:43:53-root-INFO: Loss too large (1622.915->1918.202)! Learning rate decreased to 0.00535.
2024-12-01-20:43:54-root-INFO: Loss too large (1622.915->1839.992)! Learning rate decreased to 0.00428.
2024-12-01-20:43:54-root-INFO: Loss too large (1622.915->1775.420)! Learning rate decreased to 0.00342.
2024-12-01-20:43:54-root-INFO: Loss too large (1622.915->1724.703)! Learning rate decreased to 0.00274.
2024-12-01-20:43:54-root-INFO: Loss too large (1622.915->1687.060)! Learning rate decreased to 0.00219.
2024-12-01-20:43:54-root-INFO: Loss too large (1622.915->1660.680)! Learning rate decreased to 0.00175.
2024-12-01-20:43:54-root-INFO: Loss too large (1622.915->1643.190)! Learning rate decreased to 0.00140.
2024-12-01-20:43:55-root-INFO: Loss too large (1622.915->1632.205)! Learning rate decreased to 0.00112.
2024-12-01-20:43:55-root-INFO: Loss too large (1622.915->1625.684)! Learning rate decreased to 0.00090.
2024-12-01-20:43:55-root-INFO: grad norm: 225.964 223.856 30.795
2024-12-01-20:43:55-root-INFO: Loss too large (1622.068->1640.337)! Learning rate decreased to 0.00072.
2024-12-01-20:43:56-root-INFO: Loss too large (1622.068->1628.308)! Learning rate decreased to 0.00057.
2024-12-01-20:43:56-root-INFO: grad norm: 239.809 237.356 34.211
2024-12-01-20:43:57-root-INFO: grad norm: 255.625 253.529 32.674
2024-12-01-20:43:57-root-INFO: grad norm: 264.937 262.430 36.361
2024-12-01-20:43:57-root-INFO: Loss Change: 1622.915 -> 1613.773
2024-12-01-20:43:57-root-INFO: Regularization Change: 0.000 -> 0.065
2024-12-01-20:43:57-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-01-20:43:57-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-20:43:58-root-INFO: step: 171 lr_xt 0.00695416
2024-12-01-20:43:58-root-INFO: grad norm: 463.231 460.340 51.672
2024-12-01-20:43:58-root-INFO: Loss too large (1634.669->1870.636)! Learning rate decreased to 0.00556.
2024-12-01-20:43:58-root-INFO: Loss too large (1634.669->1860.324)! Learning rate decreased to 0.00445.
2024-12-01-20:43:58-root-INFO: Loss too large (1634.669->1849.984)! Learning rate decreased to 0.00356.
2024-12-01-20:43:58-root-INFO: Loss too large (1634.669->1838.155)! Learning rate decreased to 0.00285.
2024-12-01-20:43:59-root-INFO: Loss too large (1634.669->1823.311)! Learning rate decreased to 0.00228.
2024-12-01-20:43:59-root-INFO: Loss too large (1634.669->1804.390)! Learning rate decreased to 0.00182.
2024-12-01-20:43:59-root-INFO: Loss too large (1634.669->1781.112)! Learning rate decreased to 0.00146.
2024-12-01-20:43:59-root-INFO: Loss too large (1634.669->1753.573)! Learning rate decreased to 0.00117.
2024-12-01-20:43:59-root-INFO: Loss too large (1634.669->1722.243)! Learning rate decreased to 0.00093.
2024-12-01-20:43:59-root-INFO: Loss too large (1634.669->1688.559)! Learning rate decreased to 0.00075.
2024-12-01-20:44:00-root-INFO: Loss too large (1634.669->1656.015)! Learning rate decreased to 0.00060.
2024-12-01-20:44:00-root-INFO: grad norm: 389.340 385.796 52.407
2024-12-01-20:44:00-root-INFO: grad norm: 328.540 326.302 38.285
2024-12-01-20:44:01-root-INFO: grad norm: 325.385 322.375 44.158
2024-12-01-20:44:01-root-INFO: grad norm: 320.303 318.148 37.097
2024-12-01-20:44:02-root-INFO: Loss Change: 1634.669 -> 1603.141
2024-12-01-20:44:02-root-INFO: Regularization Change: 0.000 -> 0.104
2024-12-01-20:44:02-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-01-20:44:02-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-20:44:02-root-INFO: step: 170 lr_xt 0.00723499
2024-12-01-20:44:02-root-INFO: grad norm: 224.995 222.413 33.989
2024-12-01-20:44:02-root-INFO: Loss too large (1595.110->2346.987)! Learning rate decreased to 0.00579.
2024-12-01-20:44:02-root-INFO: Loss too large (1595.110->2193.184)! Learning rate decreased to 0.00463.
2024-12-01-20:44:03-root-INFO: Loss too large (1595.110->2065.992)! Learning rate decreased to 0.00370.
2024-12-01-20:44:03-root-INFO: Loss too large (1595.110->1958.360)! Learning rate decreased to 0.00296.
2024-12-01-20:44:03-root-INFO: Loss too large (1595.110->1866.423)! Learning rate decreased to 0.00237.
2024-12-01-20:44:03-root-INFO: Loss too large (1595.110->1788.803)! Learning rate decreased to 0.00190.
2024-12-01-20:44:03-root-INFO: Loss too large (1595.110->1725.526)! Learning rate decreased to 0.00152.
2024-12-01-20:44:03-root-INFO: Loss too large (1595.110->1676.603)! Learning rate decreased to 0.00121.
2024-12-01-20:44:03-root-INFO: Loss too large (1595.110->1641.104)! Learning rate decreased to 0.00097.
2024-12-01-20:44:04-root-INFO: Loss too large (1595.110->1617.062)! Learning rate decreased to 0.00078.
2024-12-01-20:44:04-root-INFO: Loss too large (1595.110->1601.938)! Learning rate decreased to 0.00062.
2024-12-01-20:44:04-root-INFO: grad norm: 263.646 261.469 33.812
2024-12-01-20:44:04-root-INFO: Loss too large (1593.199->1593.404)! Learning rate decreased to 0.00050.
2024-12-01-20:44:05-root-INFO: grad norm: 208.970 206.573 31.563
2024-12-01-20:44:05-root-INFO: grad norm: 169.814 167.912 25.344
2024-12-01-20:44:06-root-INFO: grad norm: 144.971 142.767 25.187
2024-12-01-20:44:06-root-INFO: Loss Change: 1595.110 -> 1574.337
2024-12-01-20:44:06-root-INFO: Regularization Change: 0.000 -> 0.049
2024-12-01-20:44:06-root-INFO: Undo step: 170
2024-12-01-20:44:06-root-INFO: Undo step: 171
2024-12-01-20:44:06-root-INFO: Undo step: 172
2024-12-01-20:44:06-root-INFO: Undo step: 173
2024-12-01-20:44:06-root-INFO: Undo step: 174
2024-12-01-20:44:06-root-INFO: step: 175 lr_xt 0.00592610
2024-12-01-20:44:06-root-INFO: grad norm: 620.630 598.755 163.322
2024-12-01-20:44:07-root-INFO: grad norm: 841.695 825.914 162.220
2024-12-01-20:44:07-root-INFO: grad norm: 455.696 441.451 113.048
2024-12-01-20:44:08-root-INFO: grad norm: 533.061 526.211 85.179
2024-12-01-20:44:08-root-INFO: Loss too large (2454.004->2701.217)! Learning rate decreased to 0.00474.
2024-12-01-20:44:09-root-INFO: grad norm: 628.456 620.258 101.179
2024-12-01-20:44:09-root-INFO: Loss too large (2326.014->2414.257)! Learning rate decreased to 0.00379.
2024-12-01-20:44:09-root-INFO: Loss Change: 2685.953 -> 2198.323
2024-12-01-20:44:09-root-INFO: Regularization Change: 0.000 -> 22.613
2024-12-01-20:44:09-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-01-20:44:09-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-20:44:09-root-INFO: step: 174 lr_xt 0.00616941
2024-12-01-20:44:09-root-INFO: grad norm: 399.666 395.012 60.814
2024-12-01-20:44:10-root-INFO: grad norm: 382.483 377.148 63.661
2024-12-01-20:44:10-root-INFO: grad norm: 280.178 275.267 52.228
2024-12-01-20:44:11-root-INFO: grad norm: 208.946 203.873 45.763
2024-12-01-20:44:11-root-INFO: grad norm: 195.373 189.457 47.712
2024-12-01-20:44:12-root-INFO: Loss Change: 2183.304 -> 1795.514
2024-12-01-20:44:12-root-INFO: Regularization Change: 0.000 -> 8.652
2024-12-01-20:44:12-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-01-20:44:12-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-20:44:12-root-INFO: step: 173 lr_xt 0.00642166
2024-12-01-20:44:12-root-INFO: grad norm: 246.184 239.220 58.141
2024-12-01-20:44:13-root-INFO: grad norm: 255.025 247.916 59.797
2024-12-01-20:44:13-root-INFO: grad norm: 318.060 313.525 53.516
2024-12-01-20:44:13-root-INFO: Loss too large (1738.247->1782.502)! Learning rate decreased to 0.00514.
2024-12-01-20:44:14-root-INFO: grad norm: 253.928 248.757 50.982
2024-12-01-20:44:14-root-INFO: grad norm: 285.745 280.759 53.150
2024-12-01-20:44:14-root-INFO: Loss too large (1621.463->1703.058)! Learning rate decreased to 0.00411.
2024-12-01-20:44:15-root-INFO: Loss too large (1621.463->1646.926)! Learning rate decreased to 0.00329.
2024-12-01-20:44:15-root-INFO: Loss Change: 1814.566 -> 1604.496
2024-12-01-20:44:15-root-INFO: Regularization Change: 0.000 -> 4.958
2024-12-01-20:44:15-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-01-20:44:15-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-20:44:15-root-INFO: step: 172 lr_xt 0.00668315
2024-12-01-20:44:15-root-INFO: grad norm: 208.550 201.861 52.394
2024-12-01-20:44:15-root-INFO: Loss too large (1598.603->1747.200)! Learning rate decreased to 0.00535.
2024-12-01-20:44:16-root-INFO: Loss too large (1598.603->1633.580)! Learning rate decreased to 0.00428.
2024-12-01-20:44:16-root-INFO: grad norm: 256.902 249.791 60.023
2024-12-01-20:44:16-root-INFO: Loss too large (1569.958->1615.606)! Learning rate decreased to 0.00342.
2024-12-01-20:44:16-root-INFO: Loss too large (1569.958->1570.141)! Learning rate decreased to 0.00274.
2024-12-01-20:44:17-root-INFO: grad norm: 177.389 172.078 43.080
2024-12-01-20:44:17-root-INFO: grad norm: 123.901 120.156 30.234
2024-12-01-20:44:18-root-INFO: grad norm: 124.109 120.874 28.151
2024-12-01-20:44:18-root-INFO: Loss Change: 1598.603 -> 1481.527
2024-12-01-20:44:18-root-INFO: Regularization Change: 0.000 -> 1.300
2024-12-01-20:44:18-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-01-20:44:18-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-20:44:18-root-INFO: step: 171 lr_xt 0.00695416
2024-12-01-20:44:18-root-INFO: grad norm: 202.458 199.183 36.263
2024-12-01-20:44:19-root-INFO: Loss too large (1485.440->1646.796)! Learning rate decreased to 0.00556.
2024-12-01-20:44:19-root-INFO: Loss too large (1485.440->1603.465)! Learning rate decreased to 0.00445.
2024-12-01-20:44:19-root-INFO: Loss too large (1485.440->1561.796)! Learning rate decreased to 0.00356.
2024-12-01-20:44:19-root-INFO: Loss too large (1485.440->1525.514)! Learning rate decreased to 0.00285.
2024-12-01-20:44:19-root-INFO: Loss too large (1485.440->1497.393)! Learning rate decreased to 0.00228.
2024-12-01-20:44:20-root-INFO: grad norm: 186.339 181.880 40.521
2024-12-01-20:44:20-root-INFO: grad norm: 194.551 191.434 34.686
2024-12-01-20:44:20-root-INFO: Loss too large (1462.009->1467.772)! Learning rate decreased to 0.00182.
2024-12-01-20:44:21-root-INFO: grad norm: 167.472 164.007 33.893
2024-12-01-20:44:21-root-INFO: grad norm: 158.246 155.660 28.491
2024-12-01-20:44:22-root-INFO: Loss Change: 1485.440 -> 1439.641
2024-12-01-20:44:22-root-INFO: Regularization Change: 0.000 -> 0.478
2024-12-01-20:44:22-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-01-20:44:22-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-20:44:22-root-INFO: step: 170 lr_xt 0.00723499
2024-12-01-20:44:22-root-INFO: grad norm: 149.173 145.975 30.725
2024-12-01-20:44:22-root-INFO: Loss too large (1430.947->1711.134)! Learning rate decreased to 0.00579.
2024-12-01-20:44:22-root-INFO: Loss too large (1430.947->1620.015)! Learning rate decreased to 0.00463.
2024-12-01-20:44:22-root-INFO: Loss too large (1430.947->1549.700)! Learning rate decreased to 0.00370.
2024-12-01-20:44:22-root-INFO: Loss too large (1430.947->1497.886)! Learning rate decreased to 0.00296.
2024-12-01-20:44:23-root-INFO: Loss too large (1430.947->1462.225)! Learning rate decreased to 0.00237.
2024-12-01-20:44:23-root-INFO: Loss too large (1430.947->1439.714)! Learning rate decreased to 0.00190.
2024-12-01-20:44:23-root-INFO: grad norm: 196.341 194.004 30.200
2024-12-01-20:44:23-root-INFO: Loss too large (1426.927->1436.249)! Learning rate decreased to 0.00152.
2024-12-01-20:44:24-root-INFO: grad norm: 181.933 178.750 33.883
2024-12-01-20:44:24-root-INFO: grad norm: 175.214 173.078 27.275
2024-12-01-20:44:25-root-INFO: grad norm: 181.191 178.183 32.882
2024-12-01-20:44:25-root-INFO: Loss Change: 1430.947 -> 1407.581
2024-12-01-20:44:25-root-INFO: Regularization Change: 0.000 -> 0.258
2024-12-01-20:44:25-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-01-20:44:25-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-20:44:25-root-INFO: step: 169 lr_xt 0.00752595
2024-12-01-20:44:25-root-INFO: grad norm: 268.045 265.542 36.539
2024-12-01-20:44:26-root-INFO: Loss too large (1416.933->1629.419)! Learning rate decreased to 0.00602.
2024-12-01-20:44:26-root-INFO: Loss too large (1416.933->1605.292)! Learning rate decreased to 0.00482.
2024-12-01-20:44:26-root-INFO: Loss too large (1416.933->1576.801)! Learning rate decreased to 0.00385.
2024-12-01-20:44:26-root-INFO: Loss too large (1416.933->1544.012)! Learning rate decreased to 0.00308.
2024-12-01-20:44:26-root-INFO: Loss too large (1416.933->1508.885)! Learning rate decreased to 0.00247.
2024-12-01-20:44:26-root-INFO: Loss too large (1416.933->1474.231)! Learning rate decreased to 0.00197.
2024-12-01-20:44:27-root-INFO: Loss too large (1416.933->1443.147)! Learning rate decreased to 0.00158.
2024-12-01-20:44:27-root-INFO: Loss too large (1416.933->1418.513)! Learning rate decreased to 0.00126.
2024-12-01-20:44:27-root-INFO: grad norm: 190.910 187.922 33.645
2024-12-01-20:44:28-root-INFO: grad norm: 142.706 140.725 23.694
2024-12-01-20:44:28-root-INFO: grad norm: 129.254 126.976 24.160
2024-12-01-20:44:29-root-INFO: grad norm: 121.757 119.997 20.630
2024-12-01-20:44:29-root-INFO: Loss Change: 1416.933 -> 1377.166
2024-12-01-20:44:29-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-01-20:44:29-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-01-20:44:29-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-20:44:29-root-INFO: step: 168 lr_xt 0.00782735
2024-12-01-20:44:29-root-INFO: grad norm: 63.428 61.097 17.037
2024-12-01-20:44:29-root-INFO: Loss too large (1371.796->1396.517)! Learning rate decreased to 0.00626.
2024-12-01-20:44:30-root-INFO: Loss too large (1371.796->1383.531)! Learning rate decreased to 0.00501.
2024-12-01-20:44:30-root-INFO: Loss too large (1371.796->1375.599)! Learning rate decreased to 0.00401.
2024-12-01-20:44:30-root-INFO: grad norm: 190.510 188.821 25.309
2024-12-01-20:44:30-root-INFO: Loss too large (1371.083->1494.962)! Learning rate decreased to 0.00321.
2024-12-01-20:44:31-root-INFO: Loss too large (1371.083->1462.893)! Learning rate decreased to 0.00256.
2024-12-01-20:44:31-root-INFO: Loss too large (1371.083->1432.200)! Learning rate decreased to 0.00205.
2024-12-01-20:44:31-root-INFO: Loss too large (1371.083->1405.678)! Learning rate decreased to 0.00164.
2024-12-01-20:44:31-root-INFO: Loss too large (1371.083->1385.432)! Learning rate decreased to 0.00131.
2024-12-01-20:44:31-root-INFO: Loss too large (1371.083->1371.923)! Learning rate decreased to 0.00105.
2024-12-01-20:44:32-root-INFO: grad norm: 147.387 145.190 25.353
2024-12-01-20:44:32-root-INFO: grad norm: 120.393 118.924 18.746
2024-12-01-20:44:33-root-INFO: grad norm: 103.812 101.918 19.737
2024-12-01-20:44:33-root-INFO: Loss Change: 1371.796 -> 1352.899
2024-12-01-20:44:33-root-INFO: Regularization Change: 0.000 -> 0.171
2024-12-01-20:44:33-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-01-20:44:33-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-20:44:33-root-INFO: step: 167 lr_xt 0.00813950
2024-12-01-20:44:33-root-INFO: grad norm: 171.785 169.821 25.899
2024-12-01-20:44:33-root-INFO: Loss too large (1357.990->1560.697)! Learning rate decreased to 0.00651.
2024-12-01-20:44:34-root-INFO: Loss too large (1357.990->1534.872)! Learning rate decreased to 0.00521.
2024-12-01-20:44:34-root-INFO: Loss too large (1357.990->1505.560)! Learning rate decreased to 0.00417.
2024-12-01-20:44:34-root-INFO: Loss too large (1357.990->1474.010)! Learning rate decreased to 0.00333.
2024-12-01-20:44:34-root-INFO: Loss too large (1357.990->1442.202)! Learning rate decreased to 0.00267.
2024-12-01-20:44:34-root-INFO: Loss too large (1357.990->1412.727)! Learning rate decreased to 0.00213.
2024-12-01-20:44:34-root-INFO: Loss too large (1357.990->1388.212)! Learning rate decreased to 0.00171.
2024-12-01-20:44:34-root-INFO: Loss too large (1357.990->1370.194)! Learning rate decreased to 0.00137.
2024-12-01-20:44:35-root-INFO: Loss too large (1357.990->1358.541)! Learning rate decreased to 0.00109.
2024-12-01-20:44:35-root-INFO: grad norm: 148.223 145.923 26.006
2024-12-01-20:44:36-root-INFO: grad norm: 132.080 130.402 20.987
2024-12-01-20:44:36-root-INFO: grad norm: 120.953 118.957 21.880
2024-12-01-20:44:37-root-INFO: grad norm: 112.768 111.226 18.585
2024-12-01-20:44:37-root-INFO: Loss Change: 1357.990 -> 1337.309
2024-12-01-20:44:37-root-INFO: Regularization Change: 0.000 -> 0.106
2024-12-01-20:44:37-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-01-20:44:37-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-20:44:37-root-INFO: step: 166 lr_xt 0.00846273
2024-12-01-20:44:37-root-INFO: grad norm: 62.038 59.433 17.790
2024-12-01-20:44:38-root-INFO: grad norm: 124.514 123.206 17.998
2024-12-01-20:44:38-root-INFO: Loss too large (1316.738->1486.639)! Learning rate decreased to 0.00677.
2024-12-01-20:44:38-root-INFO: Loss too large (1316.738->1457.533)! Learning rate decreased to 0.00542.
2024-12-01-20:44:38-root-INFO: Loss too large (1316.738->1427.827)! Learning rate decreased to 0.00433.
2024-12-01-20:44:38-root-INFO: Loss too large (1316.738->1398.650)! Learning rate decreased to 0.00347.
2024-12-01-20:44:39-root-INFO: Loss too large (1316.738->1371.959)! Learning rate decreased to 0.00277.
2024-12-01-20:44:39-root-INFO: Loss too large (1316.738->1349.879)! Learning rate decreased to 0.00222.
2024-12-01-20:44:39-root-INFO: Loss too large (1316.738->1333.571)! Learning rate decreased to 0.00177.
2024-12-01-20:44:39-root-INFO: Loss too large (1316.738->1322.781)! Learning rate decreased to 0.00142.
2024-12-01-20:44:39-root-INFO: grad norm: 172.894 170.848 26.520
2024-12-01-20:44:40-root-INFO: Loss too large (1316.350->1321.903)! Learning rate decreased to 0.00114.
2024-12-01-20:44:40-root-INFO: grad norm: 165.838 164.283 22.655
2024-12-01-20:44:41-root-INFO: grad norm: 158.211 156.303 24.495
2024-12-01-20:44:41-root-INFO: Loss Change: 1333.964 -> 1307.530
2024-12-01-20:44:41-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-01-20:44:41-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-01-20:44:41-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-20:44:41-root-INFO: step: 165 lr_xt 0.00879737
2024-12-01-20:44:41-root-INFO: grad norm: 217.839 216.181 26.831
2024-12-01-20:44:41-root-INFO: Loss too large (1313.843->1541.834)! Learning rate decreased to 0.00704.
2024-12-01-20:44:42-root-INFO: Loss too large (1313.843->1524.417)! Learning rate decreased to 0.00563.
2024-12-01-20:44:42-root-INFO: Loss too large (1313.843->1502.266)! Learning rate decreased to 0.00450.
2024-12-01-20:44:42-root-INFO: Loss too large (1313.843->1475.051)! Learning rate decreased to 0.00360.
2024-12-01-20:44:42-root-INFO: Loss too large (1313.843->1443.566)! Learning rate decreased to 0.00288.
2024-12-01-20:44:42-root-INFO: Loss too large (1313.843->1409.355)! Learning rate decreased to 0.00231.
2024-12-01-20:44:42-root-INFO: Loss too large (1313.843->1375.095)! Learning rate decreased to 0.00184.
2024-12-01-20:44:42-root-INFO: Loss too large (1313.843->1344.776)! Learning rate decreased to 0.00148.
2024-12-01-20:44:43-root-INFO: Loss too large (1313.843->1321.962)! Learning rate decreased to 0.00118.
2024-12-01-20:44:43-root-INFO: grad norm: 205.193 202.773 31.425
2024-12-01-20:44:44-root-INFO: grad norm: 195.325 193.717 25.010
2024-12-01-20:44:44-root-INFO: grad norm: 184.091 181.938 28.075
2024-12-01-20:44:44-root-INFO: grad norm: 175.933 174.385 23.293
2024-12-01-20:44:45-root-INFO: Loss Change: 1313.843 -> 1293.923
2024-12-01-20:44:45-root-INFO: Regularization Change: 0.000 -> 0.127
2024-12-01-20:44:45-root-INFO: Undo step: 165
2024-12-01-20:44:45-root-INFO: Undo step: 166
2024-12-01-20:44:45-root-INFO: Undo step: 167
2024-12-01-20:44:45-root-INFO: Undo step: 168
2024-12-01-20:44:45-root-INFO: Undo step: 169
2024-12-01-20:44:45-root-INFO: step: 170 lr_xt 0.00723499
2024-12-01-20:44:45-root-INFO: grad norm: 417.399 407.893 88.572
2024-12-01-20:44:46-root-INFO: grad norm: 419.607 412.936 74.525
2024-12-01-20:44:46-root-INFO: Loss too large (1926.537->2199.817)! Learning rate decreased to 0.00579.
2024-12-01-20:44:46-root-INFO: grad norm: 610.033 601.333 102.662
2024-12-01-20:44:46-root-INFO: Loss too large (1914.647->1949.233)! Learning rate decreased to 0.00463.
2024-12-01-20:44:47-root-INFO: grad norm: 276.001 269.917 57.631
2024-12-01-20:44:47-root-INFO: grad norm: 180.542 178.047 29.911
2024-12-01-20:44:48-root-INFO: Loss Change: 2218.386 -> 1515.791
2024-12-01-20:44:48-root-INFO: Regularization Change: 0.000 -> 13.716
2024-12-01-20:44:48-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-01-20:44:48-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-20:44:48-root-INFO: step: 169 lr_xt 0.00752595
2024-12-01-20:44:48-root-INFO: grad norm: 242.861 236.249 56.283
2024-12-01-20:44:48-root-INFO: Loss too large (1504.678->1913.216)! Learning rate decreased to 0.00602.
2024-12-01-20:44:48-root-INFO: Loss too large (1504.678->1758.158)! Learning rate decreased to 0.00482.
2024-12-01-20:44:48-root-INFO: Loss too large (1504.678->1647.409)! Learning rate decreased to 0.00385.
2024-12-01-20:44:49-root-INFO: Loss too large (1504.678->1569.166)! Learning rate decreased to 0.00308.
2024-12-01-20:44:49-root-INFO: Loss too large (1504.678->1515.801)! Learning rate decreased to 0.00247.
2024-12-01-20:44:49-root-INFO: grad norm: 275.805 272.524 42.416
2024-12-01-20:44:50-root-INFO: grad norm: 248.866 242.294 56.813
2024-12-01-20:44:50-root-INFO: grad norm: 216.491 212.904 39.245
2024-12-01-20:44:50-root-INFO: Loss too large (1437.058->1439.304)! Learning rate decreased to 0.00197.
2024-12-01-20:44:51-root-INFO: grad norm: 183.132 178.711 40.000
2024-12-01-20:44:51-root-INFO: Loss Change: 1504.678 -> 1407.144
2024-12-01-20:44:51-root-INFO: Regularization Change: 0.000 -> 1.002
2024-12-01-20:44:51-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-01-20:44:51-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-20:44:51-root-INFO: step: 168 lr_xt 0.00782735
2024-12-01-20:44:51-root-INFO: grad norm: 220.044 216.896 37.083
2024-12-01-20:44:52-root-INFO: Loss too large (1414.788->1606.499)! Learning rate decreased to 0.00626.
2024-12-01-20:44:52-root-INFO: Loss too large (1414.788->1576.483)! Learning rate decreased to 0.00501.
2024-12-01-20:44:52-root-INFO: Loss too large (1414.788->1540.352)! Learning rate decreased to 0.00401.
2024-12-01-20:44:52-root-INFO: Loss too large (1414.788->1500.766)! Learning rate decreased to 0.00321.
2024-12-01-20:44:52-root-INFO: Loss too large (1414.788->1462.128)! Learning rate decreased to 0.00256.
2024-12-01-20:44:52-root-INFO: Loss too large (1414.788->1429.604)! Learning rate decreased to 0.00205.
2024-12-01-20:44:53-root-INFO: grad norm: 212.376 207.438 45.531
2024-12-01-20:44:53-root-INFO: grad norm: 208.616 205.494 35.953
2024-12-01-20:44:54-root-INFO: grad norm: 210.409 205.485 45.250
2024-12-01-20:44:54-root-INFO: grad norm: 212.519 209.332 36.664
2024-12-01-20:44:55-root-INFO: Loss Change: 1414.788 -> 1381.013
2024-12-01-20:44:55-root-INFO: Regularization Change: 0.000 -> 0.531
2024-12-01-20:44:55-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-01-20:44:55-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-20:44:55-root-INFO: step: 167 lr_xt 0.00813950
2024-12-01-20:44:55-root-INFO: grad norm: 198.658 194.169 41.990
2024-12-01-20:44:55-root-INFO: Loss too large (1371.552->1770.329)! Learning rate decreased to 0.00651.
2024-12-01-20:44:55-root-INFO: Loss too large (1371.552->1644.920)! Learning rate decreased to 0.00521.
2024-12-01-20:44:55-root-INFO: Loss too large (1371.552->1550.320)! Learning rate decreased to 0.00417.
2024-12-01-20:44:55-root-INFO: Loss too large (1371.552->1479.734)! Learning rate decreased to 0.00333.
2024-12-01-20:44:56-root-INFO: Loss too large (1371.552->1428.352)! Learning rate decreased to 0.00267.
2024-12-01-20:44:56-root-INFO: Loss too large (1371.552->1392.739)! Learning rate decreased to 0.00213.
2024-12-01-20:44:56-root-INFO: grad norm: 218.422 215.259 37.036
2024-12-01-20:44:57-root-INFO: grad norm: 229.256 223.862 49.438
2024-12-01-20:44:57-root-INFO: grad norm: 229.819 226.583 38.431
2024-12-01-20:44:58-root-INFO: grad norm: 225.581 220.025 49.760
2024-12-01-20:44:58-root-INFO: Loss Change: 1371.552 -> 1350.693
2024-12-01-20:44:58-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-01-20:44:58-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-01-20:44:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-20:44:58-root-INFO: step: 166 lr_xt 0.00846273
2024-12-01-20:44:58-root-INFO: grad norm: 248.311 245.145 39.525
2024-12-01-20:44:59-root-INFO: Loss too large (1358.327->1566.011)! Learning rate decreased to 0.00677.
2024-12-01-20:44:59-root-INFO: Loss too large (1358.327->1537.517)! Learning rate decreased to 0.00542.
2024-12-01-20:44:59-root-INFO: Loss too large (1358.327->1501.334)! Learning rate decreased to 0.00433.
2024-12-01-20:44:59-root-INFO: Loss too large (1358.327->1458.331)! Learning rate decreased to 0.00347.
2024-12-01-20:44:59-root-INFO: Loss too large (1358.327->1412.518)! Learning rate decreased to 0.00277.
2024-12-01-20:44:59-root-INFO: Loss too large (1358.327->1370.784)! Learning rate decreased to 0.00222.
2024-12-01-20:45:00-root-INFO: grad norm: 220.361 214.834 49.042
2024-12-01-20:45:00-root-INFO: grad norm: 206.262 203.221 35.288
2024-12-01-20:45:01-root-INFO: grad norm: 197.358 192.246 44.628
2024-12-01-20:45:01-root-INFO: grad norm: 191.493 188.443 34.038
2024-12-01-20:45:02-root-INFO: Loss Change: 1358.327 -> 1310.767
2024-12-01-20:45:02-root-INFO: Regularization Change: 0.000 -> 0.544
2024-12-01-20:45:02-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-01-20:45:02-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-20:45:02-root-INFO: step: 165 lr_xt 0.00879737
2024-12-01-20:45:02-root-INFO: grad norm: 172.211 167.733 39.012
2024-12-01-20:45:02-root-INFO: Loss too large (1304.395->1637.096)! Learning rate decreased to 0.00704.
2024-12-01-20:45:02-root-INFO: Loss too large (1304.395->1530.802)! Learning rate decreased to 0.00563.
2024-12-01-20:45:02-root-INFO: Loss too large (1304.395->1451.604)! Learning rate decreased to 0.00450.
2024-12-01-20:45:02-root-INFO: Loss too large (1304.395->1393.356)! Learning rate decreased to 0.00360.
2024-12-01-20:45:03-root-INFO: Loss too large (1304.395->1351.618)! Learning rate decreased to 0.00288.
2024-12-01-20:45:03-root-INFO: Loss too large (1304.395->1323.060)! Learning rate decreased to 0.00231.
2024-12-01-20:45:03-root-INFO: Loss too large (1304.395->1304.849)! Learning rate decreased to 0.00184.
2024-12-01-20:45:03-root-INFO: grad norm: 133.191 130.562 26.330
2024-12-01-20:45:04-root-INFO: grad norm: 106.636 103.696 24.866
2024-12-01-20:45:04-root-INFO: grad norm: 92.831 90.482 20.749
2024-12-01-20:45:05-root-INFO: grad norm: 82.467 79.996 20.036
2024-12-01-20:45:05-root-INFO: Loss Change: 1304.395 -> 1271.686
2024-12-01-20:45:05-root-INFO: Regularization Change: 0.000 -> 0.239
2024-12-01-20:45:05-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-01-20:45:05-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-20:45:05-root-INFO: step: 164 lr_xt 0.00914377
2024-12-01-20:45:06-root-INFO: grad norm: 119.673 116.942 25.422
2024-12-01-20:45:06-root-INFO: Loss too large (1273.518->1432.196)! Learning rate decreased to 0.00732.
2024-12-01-20:45:06-root-INFO: Loss too large (1273.518->1393.944)! Learning rate decreased to 0.00585.
2024-12-01-20:45:06-root-INFO: Loss too large (1273.518->1355.852)! Learning rate decreased to 0.00468.
2024-12-01-20:45:06-root-INFO: Loss too large (1273.518->1322.639)! Learning rate decreased to 0.00375.
2024-12-01-20:45:06-root-INFO: Loss too large (1273.518->1297.509)! Learning rate decreased to 0.00300.
2024-12-01-20:45:06-root-INFO: Loss too large (1273.518->1280.873)! Learning rate decreased to 0.00240.
2024-12-01-20:45:07-root-INFO: grad norm: 152.749 148.836 34.352
2024-12-01-20:45:07-root-INFO: Loss too large (1271.147->1274.821)! Learning rate decreased to 0.00192.
2024-12-01-20:45:07-root-INFO: grad norm: 132.376 129.709 26.438
2024-12-01-20:45:08-root-INFO: grad norm: 114.119 111.109 26.038
2024-12-01-20:45:08-root-INFO: grad norm: 104.129 101.668 22.502
2024-12-01-20:45:09-root-INFO: Loss Change: 1273.518 -> 1248.707
2024-12-01-20:45:09-root-INFO: Regularization Change: 0.000 -> 0.258
2024-12-01-20:45:09-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-01-20:45:09-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-20:45:09-root-INFO: step: 163 lr_xt 0.00950228
2024-12-01-20:45:09-root-INFO: grad norm: 66.554 64.252 17.352
2024-12-01-20:45:09-root-INFO: Loss too large (1242.671->1283.169)! Learning rate decreased to 0.00760.
2024-12-01-20:45:09-root-INFO: Loss too large (1242.671->1265.193)! Learning rate decreased to 0.00608.
2024-12-01-20:45:09-root-INFO: Loss too large (1242.671->1253.393)! Learning rate decreased to 0.00487.
2024-12-01-20:45:10-root-INFO: Loss too large (1242.671->1246.042)! Learning rate decreased to 0.00389.
2024-12-01-20:45:10-root-INFO: grad norm: 126.923 124.487 24.749
2024-12-01-20:45:10-root-INFO: Loss too large (1241.760->1277.813)! Learning rate decreased to 0.00311.
2024-12-01-20:45:10-root-INFO: Loss too large (1241.760->1255.899)! Learning rate decreased to 0.00249.
2024-12-01-20:45:11-root-INFO: Loss too large (1241.760->1242.416)! Learning rate decreased to 0.00199.
2024-12-01-20:45:11-root-INFO: grad norm: 115.414 112.501 25.770
2024-12-01-20:45:12-root-INFO: grad norm: 108.760 106.397 22.548
2024-12-01-20:45:12-root-INFO: grad norm: 101.912 99.242 23.173
2024-12-01-20:45:12-root-INFO: Loss Change: 1242.671 -> 1223.599
2024-12-01-20:45:12-root-INFO: Regularization Change: 0.000 -> 0.285
2024-12-01-20:45:12-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-01-20:45:12-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-20:45:13-root-INFO: step: 162 lr_xt 0.00987325
2024-12-01-20:45:13-root-INFO: grad norm: 144.853 141.683 30.142
2024-12-01-20:45:13-root-INFO: Loss too large (1232.402->1430.198)! Learning rate decreased to 0.00790.
2024-12-01-20:45:13-root-INFO: Loss too large (1232.402->1394.833)! Learning rate decreased to 0.00632.
2024-12-01-20:45:13-root-INFO: Loss too large (1232.402->1354.056)! Learning rate decreased to 0.00506.
2024-12-01-20:45:13-root-INFO: Loss too large (1232.402->1312.086)! Learning rate decreased to 0.00404.
2024-12-01-20:45:14-root-INFO: Loss too large (1232.402->1275.074)! Learning rate decreased to 0.00324.
2024-12-01-20:45:14-root-INFO: Loss too large (1232.402->1247.668)! Learning rate decreased to 0.00259.
2024-12-01-20:45:14-root-INFO: grad norm: 186.974 182.217 41.907
2024-12-01-20:45:14-root-INFO: Loss too large (1230.552->1239.773)! Learning rate decreased to 0.00207.
2024-12-01-20:45:15-root-INFO: grad norm: 154.767 151.818 30.071
2024-12-01-20:45:15-root-INFO: grad norm: 121.295 118.108 27.622
2024-12-01-20:45:16-root-INFO: grad norm: 108.277 105.719 23.399
2024-12-01-20:45:16-root-INFO: Loss Change: 1232.402 -> 1201.862
2024-12-01-20:45:16-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-01-20:45:16-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-01-20:45:16-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-20:45:16-root-INFO: step: 161 lr_xt 0.01025704
2024-12-01-20:45:16-root-INFO: grad norm: 63.949 61.736 16.677
2024-12-01-20:45:17-root-INFO: Loss too large (1196.319->1230.634)! Learning rate decreased to 0.00821.
2024-12-01-20:45:17-root-INFO: Loss too large (1196.319->1214.786)! Learning rate decreased to 0.00656.
2024-12-01-20:45:17-root-INFO: Loss too large (1196.319->1204.464)! Learning rate decreased to 0.00525.
2024-12-01-20:45:17-root-INFO: Loss too large (1196.319->1198.092)! Learning rate decreased to 0.00420.
2024-12-01-20:45:17-root-INFO: grad norm: 114.203 111.859 23.022
2024-12-01-20:45:18-root-INFO: Loss too large (1194.434->1223.365)! Learning rate decreased to 0.00336.
2024-12-01-20:45:18-root-INFO: Loss too large (1194.434->1204.051)! Learning rate decreased to 0.00269.
2024-12-01-20:45:18-root-INFO: grad norm: 148.199 144.278 33.866
2024-12-01-20:45:18-root-INFO: Loss too large (1192.808->1198.293)! Learning rate decreased to 0.00215.
2024-12-01-20:45:19-root-INFO: grad norm: 124.559 122.082 24.716
2024-12-01-20:45:19-root-INFO: grad norm: 99.250 96.472 23.319
2024-12-01-20:45:20-root-INFO: Loss Change: 1196.319 -> 1175.941
2024-12-01-20:45:20-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-01-20:45:20-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-01-20:45:20-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-20:45:20-root-INFO: step: 160 lr_xt 0.01065404
2024-12-01-20:45:20-root-INFO: grad norm: 127.316 124.608 26.120
2024-12-01-20:45:20-root-INFO: Loss too large (1183.106->1373.138)! Learning rate decreased to 0.00852.
2024-12-01-20:45:20-root-INFO: Loss too large (1183.106->1335.192)! Learning rate decreased to 0.00682.
2024-12-01-20:45:20-root-INFO: Loss too large (1183.106->1292.698)! Learning rate decreased to 0.00545.
2024-12-01-20:45:20-root-INFO: Loss too large (1183.106->1250.906)! Learning rate decreased to 0.00436.
2024-12-01-20:45:21-root-INFO: Loss too large (1183.106->1216.317)! Learning rate decreased to 0.00349.
2024-12-01-20:45:21-root-INFO: Loss too large (1183.106->1192.505)! Learning rate decreased to 0.00279.
2024-12-01-20:45:21-root-INFO: grad norm: 154.425 150.328 35.336
2024-12-01-20:45:21-root-INFO: Loss too large (1178.685->1184.068)! Learning rate decreased to 0.00223.
2024-12-01-20:45:22-root-INFO: grad norm: 124.603 122.023 25.226
2024-12-01-20:45:22-root-INFO: grad norm: 92.706 90.113 21.774
2024-12-01-20:45:23-root-INFO: grad norm: 81.225 79.074 18.572
2024-12-01-20:45:23-root-INFO: Loss Change: 1183.106 -> 1154.116
2024-12-01-20:45:23-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-01-20:45:23-root-INFO: Undo step: 160
2024-12-01-20:45:23-root-INFO: Undo step: 161
2024-12-01-20:45:23-root-INFO: Undo step: 162
2024-12-01-20:45:23-root-INFO: Undo step: 163
2024-12-01-20:45:23-root-INFO: Undo step: 164
2024-12-01-20:45:23-root-INFO: step: 165 lr_xt 0.00879737
2024-12-01-20:45:23-root-INFO: grad norm: 407.158 399.372 79.246
2024-12-01-20:45:24-root-INFO: grad norm: 419.647 404.876 110.360
2024-12-01-20:45:24-root-INFO: Loss too large (1721.396->2450.673)! Learning rate decreased to 0.00704.
2024-12-01-20:45:24-root-INFO: Loss too large (1721.396->2109.914)! Learning rate decreased to 0.00563.
2024-12-01-20:45:24-root-INFO: Loss too large (1721.396->1876.465)! Learning rate decreased to 0.00450.
2024-12-01-20:45:25-root-INFO: grad norm: 453.639 447.003 77.306
2024-12-01-20:45:25-root-INFO: grad norm: 165.420 160.668 39.368
2024-12-01-20:45:26-root-INFO: grad norm: 354.266 342.576 90.255
2024-12-01-20:45:26-root-INFO: Loss too large (1453.245->1578.114)! Learning rate decreased to 0.00360.
2024-12-01-20:45:26-root-INFO: Loss too large (1453.245->1473.088)! Learning rate decreased to 0.00288.
2024-12-01-20:45:26-root-INFO: Loss Change: 2006.986 -> 1397.069
2024-12-01-20:45:26-root-INFO: Regularization Change: 0.000 -> 15.584
2024-12-01-20:45:26-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-01-20:45:26-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-20:45:26-root-INFO: step: 164 lr_xt 0.00914377
2024-12-01-20:45:27-root-INFO: grad norm: 334.141 326.668 70.273
2024-12-01-20:45:27-root-INFO: Loss too large (1410.645->1556.004)! Learning rate decreased to 0.00732.
2024-12-01-20:45:27-root-INFO: Loss too large (1410.645->1542.202)! Learning rate decreased to 0.00585.
2024-12-01-20:45:27-root-INFO: Loss too large (1410.645->1523.445)! Learning rate decreased to 0.00468.
2024-12-01-20:45:27-root-INFO: Loss too large (1410.645->1492.934)! Learning rate decreased to 0.00375.
2024-12-01-20:45:27-root-INFO: Loss too large (1410.645->1441.531)! Learning rate decreased to 0.00300.
2024-12-01-20:45:28-root-INFO: grad norm: 376.848 361.259 107.267
2024-12-01-20:45:28-root-INFO: Loss too large (1368.957->1410.236)! Learning rate decreased to 0.00240.
2024-12-01-20:45:28-root-INFO: grad norm: 289.604 282.147 65.296
2024-12-01-20:45:29-root-INFO: grad norm: 225.746 216.824 62.838
2024-12-01-20:45:29-root-INFO: Loss too large (1275.227->1285.228)! Learning rate decreased to 0.00192.
2024-12-01-20:45:30-root-INFO: grad norm: 181.963 175.046 49.691
2024-12-01-20:45:30-root-INFO: Loss Change: 1410.645 -> 1244.677
2024-12-01-20:45:30-root-INFO: Regularization Change: 0.000 -> 1.223
2024-12-01-20:45:30-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-01-20:45:30-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-20:45:30-root-INFO: step: 163 lr_xt 0.00950228
2024-12-01-20:45:30-root-INFO: grad norm: 96.127 92.331 26.747
2024-12-01-20:45:30-root-INFO: Loss too large (1234.753->1311.179)! Learning rate decreased to 0.00760.
2024-12-01-20:45:31-root-INFO: Loss too large (1234.753->1279.939)! Learning rate decreased to 0.00608.
2024-12-01-20:45:31-root-INFO: Loss too large (1234.753->1258.419)! Learning rate decreased to 0.00487.
2024-12-01-20:45:31-root-INFO: Loss too large (1234.753->1244.228)! Learning rate decreased to 0.00389.
2024-12-01-20:45:31-root-INFO: Loss too large (1234.753->1235.397)! Learning rate decreased to 0.00311.
2024-12-01-20:45:31-root-INFO: grad norm: 136.191 130.693 38.305
2024-12-01-20:45:32-root-INFO: Loss too large (1230.328->1238.047)! Learning rate decreased to 0.00249.
2024-12-01-20:45:32-root-INFO: grad norm: 175.341 169.132 46.247
2024-12-01-20:45:32-root-INFO: Loss too large (1224.064->1230.732)! Learning rate decreased to 0.00199.
2024-12-01-20:45:33-root-INFO: grad norm: 148.037 142.687 39.438
2024-12-01-20:45:33-root-INFO: grad norm: 113.691 109.717 29.794
2024-12-01-20:45:33-root-INFO: Loss Change: 1234.753 -> 1199.174
2024-12-01-20:45:33-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-01-20:45:33-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-01-20:45:33-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-20:45:34-root-INFO: step: 162 lr_xt 0.00987325
2024-12-01-20:45:34-root-INFO: grad norm: 142.098 136.674 38.884
2024-12-01-20:45:34-root-INFO: Loss too large (1204.144->1420.282)! Learning rate decreased to 0.00790.
2024-12-01-20:45:34-root-INFO: Loss too large (1204.144->1387.179)! Learning rate decreased to 0.00632.
2024-12-01-20:45:34-root-INFO: Loss too large (1204.144->1341.572)! Learning rate decreased to 0.00506.
2024-12-01-20:45:34-root-INFO: Loss too large (1204.144->1288.859)! Learning rate decreased to 0.00404.
2024-12-01-20:45:34-root-INFO: Loss too large (1204.144->1243.218)! Learning rate decreased to 0.00324.
2024-12-01-20:45:35-root-INFO: Loss too large (1204.144->1213.073)! Learning rate decreased to 0.00259.
2024-12-01-20:45:35-root-INFO: grad norm: 181.658 175.766 45.889
2024-12-01-20:45:35-root-INFO: Loss too large (1196.795->1205.913)! Learning rate decreased to 0.00207.
2024-12-01-20:45:36-root-INFO: grad norm: 150.112 145.150 38.276
2024-12-01-20:45:36-root-INFO: grad norm: 107.288 103.908 26.718
2024-12-01-20:45:37-root-INFO: grad norm: 96.182 92.033 27.942
2024-12-01-20:45:37-root-INFO: Loss Change: 1204.144 -> 1164.683
2024-12-01-20:45:37-root-INFO: Regularization Change: 0.000 -> 0.385
2024-12-01-20:45:37-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-01-20:45:37-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-20:45:37-root-INFO: step: 161 lr_xt 0.01025704
2024-12-01-20:45:37-root-INFO: grad norm: 57.884 55.229 17.329
2024-12-01-20:45:38-root-INFO: grad norm: 152.685 148.904 33.767
2024-12-01-20:45:38-root-INFO: Loss too large (1154.067->1387.818)! Learning rate decreased to 0.00821.
2024-12-01-20:45:38-root-INFO: Loss too large (1154.067->1357.884)! Learning rate decreased to 0.00656.
2024-12-01-20:45:38-root-INFO: Loss too large (1154.067->1313.990)! Learning rate decreased to 0.00525.
2024-12-01-20:45:38-root-INFO: Loss too large (1154.067->1257.133)! Learning rate decreased to 0.00420.
2024-12-01-20:45:38-root-INFO: Loss too large (1154.067->1201.997)! Learning rate decreased to 0.00336.
2024-12-01-20:45:39-root-INFO: Loss too large (1154.067->1163.698)! Learning rate decreased to 0.00269.
2024-12-01-20:45:39-root-INFO: grad norm: 174.399 168.726 44.118
2024-12-01-20:45:39-root-INFO: Loss too large (1143.219->1153.548)! Learning rate decreased to 0.00215.
2024-12-01-20:45:40-root-INFO: grad norm: 138.261 134.485 32.090
2024-12-01-20:45:40-root-INFO: grad norm: 89.408 86.517 22.551
2024-12-01-20:45:40-root-INFO: Loss Change: 1158.895 -> 1121.934
2024-12-01-20:45:40-root-INFO: Regularization Change: 0.000 -> 0.797
2024-12-01-20:45:40-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-01-20:45:40-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-20:45:41-root-INFO: step: 160 lr_xt 0.01065404
2024-12-01-20:45:41-root-INFO: grad norm: 112.417 108.837 28.146
2024-12-01-20:45:41-root-INFO: Loss too large (1125.859->1337.504)! Learning rate decreased to 0.00852.
2024-12-01-20:45:41-root-INFO: Loss too large (1125.859->1294.515)! Learning rate decreased to 0.00682.
2024-12-01-20:45:41-root-INFO: Loss too large (1125.859->1240.717)! Learning rate decreased to 0.00545.
2024-12-01-20:45:41-root-INFO: Loss too large (1125.859->1189.257)! Learning rate decreased to 0.00436.
2024-12-01-20:45:42-root-INFO: Loss too large (1125.859->1152.292)! Learning rate decreased to 0.00349.
2024-12-01-20:45:42-root-INFO: Loss too large (1125.859->1130.763)! Learning rate decreased to 0.00279.
2024-12-01-20:45:42-root-INFO: grad norm: 135.781 131.830 32.516
2024-12-01-20:45:42-root-INFO: Loss too large (1119.958->1124.187)! Learning rate decreased to 0.00223.
2024-12-01-20:45:43-root-INFO: grad norm: 110.586 107.240 26.994
2024-12-01-20:45:43-root-INFO: grad norm: 78.139 75.811 18.931
2024-12-01-20:45:44-root-INFO: grad norm: 68.816 66.033 19.372
2024-12-01-20:45:44-root-INFO: Loss Change: 1125.859 -> 1098.208
2024-12-01-20:45:44-root-INFO: Regularization Change: 0.000 -> 0.279
2024-12-01-20:45:44-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-01-20:45:44-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-20:45:44-root-INFO: step: 159 lr_xt 0.01106461
2024-12-01-20:45:44-root-INFO: grad norm: 45.898 43.843 13.579
2024-12-01-20:45:45-root-INFO: grad norm: 97.933 95.348 22.353
2024-12-01-20:45:45-root-INFO: Loss too large (1082.573->1281.596)! Learning rate decreased to 0.00885.
2024-12-01-20:45:45-root-INFO: Loss too large (1082.573->1233.048)! Learning rate decreased to 0.00708.
2024-12-01-20:45:45-root-INFO: Loss too large (1082.573->1178.743)! Learning rate decreased to 0.00567.
2024-12-01-20:45:46-root-INFO: Loss too large (1082.573->1132.889)! Learning rate decreased to 0.00453.
2024-12-01-20:45:46-root-INFO: Loss too large (1082.573->1102.696)! Learning rate decreased to 0.00363.
2024-12-01-20:45:46-root-INFO: Loss too large (1082.573->1085.886)! Learning rate decreased to 0.00290.
2024-12-01-20:45:46-root-INFO: grad norm: 116.993 113.564 28.118
2024-12-01-20:45:46-root-INFO: Loss too large (1077.624->1080.596)! Learning rate decreased to 0.00232.
2024-12-01-20:45:47-root-INFO: grad norm: 96.255 93.638 22.293
2024-12-01-20:45:47-root-INFO: grad norm: 70.127 67.939 17.381
2024-12-01-20:45:48-root-INFO: Loss Change: 1093.461 -> 1063.778
2024-12-01-20:45:48-root-INFO: Regularization Change: 0.000 -> 0.670
2024-12-01-20:45:48-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-01-20:45:48-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-20:45:48-root-INFO: step: 158 lr_xt 0.01148915
2024-12-01-20:45:48-root-INFO: grad norm: 88.102 85.311 22.001
2024-12-01-20:45:48-root-INFO: Loss too large (1064.756->1245.044)! Learning rate decreased to 0.00919.
2024-12-01-20:45:48-root-INFO: Loss too large (1064.756->1190.602)! Learning rate decreased to 0.00735.
2024-12-01-20:45:49-root-INFO: Loss too large (1064.756->1138.259)! Learning rate decreased to 0.00588.
2024-12-01-20:45:49-root-INFO: Loss too large (1064.756->1099.957)! Learning rate decreased to 0.00471.
2024-12-01-20:45:49-root-INFO: Loss too large (1064.756->1076.966)! Learning rate decreased to 0.00376.
2024-12-01-20:45:49-root-INFO: Loss too large (1064.756->1064.819)! Learning rate decreased to 0.00301.
2024-12-01-20:45:49-root-INFO: grad norm: 100.804 98.031 23.481
2024-12-01-20:45:50-root-INFO: Loss too large (1059.091->1059.297)! Learning rate decreased to 0.00241.
2024-12-01-20:45:50-root-INFO: grad norm: 82.152 79.731 19.798
2024-12-01-20:45:51-root-INFO: grad norm: 60.463 58.621 14.814
2024-12-01-20:45:51-root-INFO: grad norm: 53.033 51.008 14.515
2024-12-01-20:45:51-root-INFO: Loss Change: 1064.756 -> 1041.680
2024-12-01-20:45:51-root-INFO: Regularization Change: 0.000 -> 0.260
2024-12-01-20:45:51-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-01-20:45:51-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-20:45:51-root-INFO: step: 157 lr_xt 0.01192805
2024-12-01-20:45:52-root-INFO: grad norm: 51.300 48.646 16.284
2024-12-01-20:45:52-root-INFO: Loss too large (1040.689->1058.262)! Learning rate decreased to 0.00954.
2024-12-01-20:45:52-root-INFO: Loss too large (1040.689->1045.666)! Learning rate decreased to 0.00763.
2024-12-01-20:45:52-root-INFO: grad norm: 151.468 147.473 34.555
2024-12-01-20:45:53-root-INFO: Loss too large (1039.128->1165.038)! Learning rate decreased to 0.00611.
2024-12-01-20:45:53-root-INFO: Loss too large (1039.128->1119.072)! Learning rate decreased to 0.00489.
2024-12-01-20:45:53-root-INFO: Loss too large (1039.128->1086.115)! Learning rate decreased to 0.00391.
2024-12-01-20:45:53-root-INFO: Loss too large (1039.128->1062.835)! Learning rate decreased to 0.00313.
2024-12-01-20:45:53-root-INFO: Loss too large (1039.128->1046.937)! Learning rate decreased to 0.00250.
2024-12-01-20:45:54-root-INFO: grad norm: 111.668 109.155 23.559
2024-12-01-20:45:54-root-INFO: grad norm: 58.527 56.828 14.000
2024-12-01-20:45:55-root-INFO: grad norm: 50.445 48.488 13.914
2024-12-01-20:45:55-root-INFO: Loss Change: 1040.689 -> 1016.109
2024-12-01-20:45:55-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-01-20:45:55-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-01-20:45:55-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-20:45:55-root-INFO: step: 156 lr_xt 0.01238172
2024-12-01-20:45:56-root-INFO: grad norm: 38.262 36.412 11.754
2024-12-01-20:45:56-root-INFO: grad norm: 87.270 85.054 19.543
2024-12-01-20:45:56-root-INFO: Loss too large (1002.737->1108.655)! Learning rate decreased to 0.00991.
2024-12-01-20:45:56-root-INFO: Loss too large (1002.737->1071.705)! Learning rate decreased to 0.00792.
2024-12-01-20:45:56-root-INFO: Loss too large (1002.737->1045.566)! Learning rate decreased to 0.00634.
2024-12-01-20:45:57-root-INFO: Loss too large (1002.737->1027.290)! Learning rate decreased to 0.00507.
2024-12-01-20:45:57-root-INFO: Loss too large (1002.737->1014.836)! Learning rate decreased to 0.00406.
2024-12-01-20:45:57-root-INFO: Loss too large (1002.737->1006.689)! Learning rate decreased to 0.00325.
2024-12-01-20:45:57-root-INFO: grad norm: 87.420 85.484 18.292
2024-12-01-20:45:58-root-INFO: grad norm: 90.662 88.344 20.369
2024-12-01-20:45:58-root-INFO: Loss too large (995.625->995.734)! Learning rate decreased to 0.00260.
2024-12-01-20:45:58-root-INFO: grad norm: 70.421 68.630 15.782
2024-12-01-20:45:59-root-INFO: Loss Change: 1013.487 -> 986.772
2024-12-01-20:45:59-root-INFO: Regularization Change: 0.000 -> 0.671
2024-12-01-20:45:59-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-01-20:45:59-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-20:45:59-root-INFO: step: 155 lr_xt 0.01285057
2024-12-01-20:45:59-root-INFO: grad norm: 38.756 36.904 11.838
2024-12-01-20:46:00-root-INFO: grad norm: 68.649 66.954 15.158
2024-12-01-20:46:00-root-INFO: Loss too large (972.536->1032.756)! Learning rate decreased to 0.01028.
2024-12-01-20:46:00-root-INFO: Loss too large (972.536->1008.869)! Learning rate decreased to 0.00822.
2024-12-01-20:46:00-root-INFO: Loss too large (972.536->992.733)! Learning rate decreased to 0.00658.
2024-12-01-20:46:00-root-INFO: Loss too large (972.536->982.088)! Learning rate decreased to 0.00526.
2024-12-01-20:46:00-root-INFO: Loss too large (972.536->975.328)! Learning rate decreased to 0.00421.
2024-12-01-20:46:01-root-INFO: grad norm: 82.698 80.865 17.312
2024-12-01-20:46:01-root-INFO: grad norm: 139.114 135.646 30.870
2024-12-01-20:46:01-root-INFO: Loss too large (971.045->990.241)! Learning rate decreased to 0.00337.
2024-12-01-20:46:02-root-INFO: Loss too large (971.045->976.777)! Learning rate decreased to 0.00269.
2024-12-01-20:46:02-root-INFO: grad norm: 95.230 93.353 18.815
2024-12-01-20:46:02-root-INFO: Loss Change: 984.064 -> 956.135
2024-12-01-20:46:02-root-INFO: Regularization Change: 0.000 -> 0.762
2024-12-01-20:46:02-root-INFO: Undo step: 155
2024-12-01-20:46:02-root-INFO: Undo step: 156
2024-12-01-20:46:02-root-INFO: Undo step: 157
2024-12-01-20:46:02-root-INFO: Undo step: 158
2024-12-01-20:46:02-root-INFO: Undo step: 159
2024-12-01-20:46:03-root-INFO: step: 160 lr_xt 0.01065404
2024-12-01-20:46:03-root-INFO: grad norm: 265.722 256.307 70.108
2024-12-01-20:46:03-root-INFO: Loss too large (1491.996->1524.161)! Learning rate decreased to 0.00852.
2024-12-01-20:46:03-root-INFO: grad norm: 333.721 330.353 47.288
2024-12-01-20:46:03-root-INFO: Loss too large (1436.513->1519.336)! Learning rate decreased to 0.00682.
2024-12-01-20:46:04-root-INFO: Loss too large (1436.513->1472.106)! Learning rate decreased to 0.00545.
2024-12-01-20:46:04-root-INFO: grad norm: 356.853 347.191 82.476
2024-12-01-20:46:05-root-INFO: grad norm: 320.700 317.191 47.313
2024-12-01-20:46:05-root-INFO: grad norm: 352.212 343.262 78.897
2024-12-01-20:46:05-root-INFO: Loss too large (1208.407->1366.777)! Learning rate decreased to 0.00436.
2024-12-01-20:46:05-root-INFO: Loss too large (1208.407->1280.050)! Learning rate decreased to 0.00349.
2024-12-01-20:46:06-root-INFO: Loss too large (1208.407->1222.610)! Learning rate decreased to 0.00279.
2024-12-01-20:46:06-root-INFO: Loss Change: 1491.996 -> 1184.789
2024-12-01-20:46:06-root-INFO: Regularization Change: 0.000 -> 10.691
2024-12-01-20:46:06-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-01-20:46:06-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-20:46:06-root-INFO: step: 159 lr_xt 0.01106461
2024-12-01-20:46:06-root-INFO: grad norm: 192.523 190.393 28.557
2024-12-01-20:46:07-root-INFO: Loss too large (1183.190->1342.583)! Learning rate decreased to 0.00885.
2024-12-01-20:46:07-root-INFO: Loss too large (1183.190->1284.817)! Learning rate decreased to 0.00708.
2024-12-01-20:46:07-root-INFO: Loss too large (1183.190->1207.868)! Learning rate decreased to 0.00567.
2024-12-01-20:46:07-root-INFO: grad norm: 270.534 263.968 59.240
2024-12-01-20:46:08-root-INFO: Loss too large (1136.738->1237.051)! Learning rate decreased to 0.00453.
2024-12-01-20:46:08-root-INFO: Loss too large (1136.738->1179.898)! Learning rate decreased to 0.00363.
2024-12-01-20:46:08-root-INFO: Loss too large (1136.738->1140.458)! Learning rate decreased to 0.00290.
2024-12-01-20:46:08-root-INFO: grad norm: 144.652 142.465 25.057
2024-12-01-20:46:09-root-INFO: grad norm: 53.321 50.939 15.760
2024-12-01-20:46:09-root-INFO: grad norm: 50.790 48.532 14.977
2024-12-01-20:46:10-root-INFO: Loss Change: 1183.190 -> 1058.889
2024-12-01-20:46:10-root-INFO: Regularization Change: 0.000 -> 1.302
2024-12-01-20:46:10-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-01-20:46:10-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-20:46:10-root-INFO: step: 158 lr_xt 0.01148915
2024-12-01-20:46:10-root-INFO: grad norm: 61.675 59.471 16.338
2024-12-01-20:46:10-root-INFO: Loss too large (1054.825->1054.848)! Learning rate decreased to 0.00919.
2024-12-01-20:46:10-root-INFO: grad norm: 154.380 151.073 31.783
2024-12-01-20:46:11-root-INFO: Loss too large (1046.623->1183.936)! Learning rate decreased to 0.00735.
2024-12-01-20:46:11-root-INFO: Loss too large (1046.623->1129.914)! Learning rate decreased to 0.00588.
2024-12-01-20:46:11-root-INFO: Loss too large (1046.623->1092.496)! Learning rate decreased to 0.00471.
2024-12-01-20:46:11-root-INFO: Loss too large (1046.623->1066.860)! Learning rate decreased to 0.00376.
2024-12-01-20:46:11-root-INFO: Loss too large (1046.623->1049.819)! Learning rate decreased to 0.00301.
2024-12-01-20:46:12-root-INFO: grad norm: 106.083 104.232 19.728
2024-12-01-20:46:12-root-INFO: grad norm: 52.956 51.175 13.620
2024-12-01-20:46:13-root-INFO: grad norm: 47.199 45.362 13.040
2024-12-01-20:46:13-root-INFO: Loss Change: 1054.825 -> 1010.914
2024-12-01-20:46:13-root-INFO: Regularization Change: 0.000 -> 0.925
2024-12-01-20:46:13-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-01-20:46:13-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-20:46:13-root-INFO: step: 157 lr_xt 0.01192805
2024-12-01-20:46:13-root-INFO: grad norm: 54.106 51.741 15.820
2024-12-01-20:46:13-root-INFO: Loss too large (1008.308->1021.130)! Learning rate decreased to 0.00954.
2024-12-01-20:46:14-root-INFO: Loss too large (1008.308->1009.292)! Learning rate decreased to 0.00763.
2024-12-01-20:46:14-root-INFO: grad norm: 123.722 121.213 24.794
2024-12-01-20:46:14-root-INFO: Loss too large (1003.601->1064.965)! Learning rate decreased to 0.00611.
2024-12-01-20:46:14-root-INFO: Loss too large (1003.601->1037.791)! Learning rate decreased to 0.00489.
2024-12-01-20:46:15-root-INFO: Loss too large (1003.601->1019.108)! Learning rate decreased to 0.00391.
2024-12-01-20:46:15-root-INFO: Loss too large (1003.601->1006.727)! Learning rate decreased to 0.00313.
2024-12-01-20:46:15-root-INFO: grad norm: 90.891 89.092 17.993
2024-12-01-20:46:16-root-INFO: grad norm: 51.476 49.945 12.458
2024-12-01-20:46:16-root-INFO: grad norm: 45.092 43.366 12.355
2024-12-01-20:46:16-root-INFO: Loss Change: 1008.308 -> 978.174
2024-12-01-20:46:16-root-INFO: Regularization Change: 0.000 -> 0.601
2024-12-01-20:46:16-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-01-20:46:16-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-20:46:17-root-INFO: step: 156 lr_xt 0.01238172
2024-12-01-20:46:17-root-INFO: grad norm: 37.847 36.042 11.547
2024-12-01-20:46:17-root-INFO: grad norm: 61.301 59.941 12.840
2024-12-01-20:46:17-root-INFO: Loss too large (962.100->995.531)! Learning rate decreased to 0.00991.
2024-12-01-20:46:18-root-INFO: Loss too large (962.100->980.333)! Learning rate decreased to 0.00792.
2024-12-01-20:46:18-root-INFO: Loss too large (962.100->970.381)! Learning rate decreased to 0.00634.
2024-12-01-20:46:18-root-INFO: Loss too large (962.100->964.122)! Learning rate decreased to 0.00507.
2024-12-01-20:46:18-root-INFO: grad norm: 76.995 75.578 14.706
2024-12-01-20:46:19-root-INFO: Loss too large (960.417->960.649)! Learning rate decreased to 0.00406.
2024-12-01-20:46:19-root-INFO: grad norm: 82.548 80.876 16.533
2024-12-01-20:46:19-root-INFO: grad norm: 83.509 82.114 15.197
2024-12-01-20:46:20-root-INFO: Loss Change: 974.569 -> 946.765
2024-12-01-20:46:20-root-INFO: Regularization Change: 0.000 -> 0.982
2024-12-01-20:46:20-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-01-20:46:20-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-20:46:20-root-INFO: step: 155 lr_xt 0.01285057
2024-12-01-20:46:20-root-INFO: grad norm: 51.450 50.055 11.898
2024-12-01-20:46:20-root-INFO: Loss too large (941.121->955.412)! Learning rate decreased to 0.01028.
2024-12-01-20:46:20-root-INFO: Loss too large (941.121->947.026)! Learning rate decreased to 0.00822.
2024-12-01-20:46:21-root-INFO: Loss too large (941.121->941.821)! Learning rate decreased to 0.00658.
2024-12-01-20:46:21-root-INFO: grad norm: 73.730 72.260 14.654
2024-12-01-20:46:21-root-INFO: Loss too large (938.802->949.788)! Learning rate decreased to 0.00526.
2024-12-01-20:46:22-root-INFO: grad norm: 121.903 119.667 23.244
2024-12-01-20:46:22-root-INFO: Loss too large (937.334->955.120)! Learning rate decreased to 0.00421.
2024-12-01-20:46:22-root-INFO: Loss too large (937.334->942.333)! Learning rate decreased to 0.00337.
2024-12-01-20:46:22-root-INFO: grad norm: 83.967 82.541 15.411
2024-12-01-20:46:23-root-INFO: grad norm: 38.439 37.308 9.254
2024-12-01-20:46:23-root-INFO: Loss Change: 941.121 -> 919.138
2024-12-01-20:46:23-root-INFO: Regularization Change: 0.000 -> 0.470
2024-12-01-20:46:23-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-01-20:46:23-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-20:46:23-root-INFO: step: 154 lr_xt 0.01333503
2024-12-01-20:46:23-root-INFO: grad norm: 52.663 51.026 13.028
2024-12-01-20:46:24-root-INFO: Loss too large (918.183->967.214)! Learning rate decreased to 0.01067.
2024-12-01-20:46:24-root-INFO: Loss too large (918.183->939.263)! Learning rate decreased to 0.00853.
2024-12-01-20:46:24-root-INFO: Loss too large (918.183->924.452)! Learning rate decreased to 0.00683.
2024-12-01-20:46:24-root-INFO: grad norm: 105.674 103.937 19.082
2024-12-01-20:46:25-root-INFO: Loss too large (917.153->941.881)! Learning rate decreased to 0.00546.
2024-12-01-20:46:25-root-INFO: Loss too large (917.153->927.539)! Learning rate decreased to 0.00437.
2024-12-01-20:46:25-root-INFO: Loss too large (917.153->918.166)! Learning rate decreased to 0.00350.
2024-12-01-20:46:25-root-INFO: grad norm: 69.470 68.100 13.731
2024-12-01-20:46:26-root-INFO: grad norm: 33.988 32.868 8.653
2024-12-01-20:46:26-root-INFO: grad norm: 30.454 29.114 8.932
2024-12-01-20:46:27-root-INFO: Loss Change: 918.183 -> 897.574
2024-12-01-20:46:27-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-01-20:46:27-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-01-20:46:27-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-20:46:27-root-INFO: step: 153 lr_xt 0.01383551
2024-12-01-20:46:27-root-INFO: grad norm: 41.487 39.720 11.979
2024-12-01-20:46:27-root-INFO: Loss too large (896.533->897.708)! Learning rate decreased to 0.01107.
2024-12-01-20:46:28-root-INFO: grad norm: 95.331 93.845 16.766
2024-12-01-20:46:28-root-INFO: Loss too large (892.920->952.464)! Learning rate decreased to 0.00885.
2024-12-01-20:46:28-root-INFO: Loss too large (892.920->926.527)! Learning rate decreased to 0.00708.
2024-12-01-20:46:28-root-INFO: Loss too large (892.920->909.118)! Learning rate decreased to 0.00567.
2024-12-01-20:46:28-root-INFO: Loss too large (892.920->897.722)! Learning rate decreased to 0.00453.
2024-12-01-20:46:29-root-INFO: grad norm: 74.822 73.566 13.654
2024-12-01-20:46:29-root-INFO: grad norm: 45.437 44.474 9.306
2024-12-01-20:46:29-root-INFO: grad norm: 40.736 39.544 9.781
2024-12-01-20:46:30-root-INFO: Loss Change: 896.533 -> 872.178
2024-12-01-20:46:30-root-INFO: Regularization Change: 0.000 -> 0.713
2024-12-01-20:46:30-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-01-20:46:30-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-20:46:30-root-INFO: step: 152 lr_xt 0.01435246
2024-12-01-20:46:30-root-INFO: grad norm: 29.866 28.599 8.607
2024-12-01-20:46:31-root-INFO: grad norm: 34.713 33.601 8.719
2024-12-01-20:46:31-root-INFO: Loss too large (859.552->859.773)! Learning rate decreased to 0.01148.
2024-12-01-20:46:31-root-INFO: grad norm: 77.771 76.440 14.327
2024-12-01-20:46:31-root-INFO: Loss too large (856.462->894.374)! Learning rate decreased to 0.00919.
2024-12-01-20:46:32-root-INFO: Loss too large (856.462->876.921)! Learning rate decreased to 0.00735.
2024-12-01-20:46:32-root-INFO: Loss too large (856.462->865.315)! Learning rate decreased to 0.00588.
2024-12-01-20:46:32-root-INFO: Loss too large (856.462->857.858)! Learning rate decreased to 0.00470.
2024-12-01-20:46:32-root-INFO: grad norm: 60.245 59.196 11.197
2024-12-01-20:46:33-root-INFO: grad norm: 38.524 37.664 8.096
2024-12-01-20:46:33-root-INFO: Loss Change: 869.455 -> 842.648
2024-12-01-20:46:33-root-INFO: Regularization Change: 0.000 -> 1.106
2024-12-01-20:46:33-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-01-20:46:33-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-20:46:33-root-INFO: step: 151 lr_xt 0.01488633
2024-12-01-20:46:33-root-INFO: grad norm: 51.630 50.260 11.811
2024-12-01-20:46:34-root-INFO: Loss too large (843.426->885.116)! Learning rate decreased to 0.01191.
2024-12-01-20:46:34-root-INFO: Loss too large (843.426->858.694)! Learning rate decreased to 0.00953.
2024-12-01-20:46:34-root-INFO: Loss too large (843.426->845.634)! Learning rate decreased to 0.00762.
2024-12-01-20:46:34-root-INFO: grad norm: 75.944 74.815 13.050
2024-12-01-20:46:35-root-INFO: Loss too large (839.613->846.691)! Learning rate decreased to 0.00610.
2024-12-01-20:46:35-root-INFO: Loss too large (839.613->839.741)! Learning rate decreased to 0.00488.
2024-12-01-20:46:35-root-INFO: grad norm: 54.421 53.327 10.858
2024-12-01-20:46:36-root-INFO: grad norm: 31.622 30.781 7.247
2024-12-01-20:46:36-root-INFO: grad norm: 27.930 26.874 7.609
2024-12-01-20:46:36-root-INFO: Loss Change: 843.426 -> 821.938
2024-12-01-20:46:36-root-INFO: Regularization Change: 0.000 -> 0.520
2024-12-01-20:46:36-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-01-20:46:36-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-20:46:36-root-INFO: step: 150 lr_xt 0.01543756
2024-12-01-20:46:37-root-INFO: grad norm: 27.966 26.791 8.022
2024-12-01-20:46:37-root-INFO: grad norm: 31.913 31.123 7.060
2024-12-01-20:46:38-root-INFO: grad norm: 60.040 59.095 10.612
2024-12-01-20:46:38-root-INFO: Loss too large (808.487->877.753)! Learning rate decreased to 0.01235.
2024-12-01-20:46:38-root-INFO: Loss too large (808.487->835.934)! Learning rate decreased to 0.00988.
2024-12-01-20:46:38-root-INFO: Loss too large (808.487->815.009)! Learning rate decreased to 0.00790.
2024-12-01-20:46:39-root-INFO: grad norm: 84.064 82.787 14.597
2024-12-01-20:46:39-root-INFO: Loss too large (805.182->814.588)! Learning rate decreased to 0.00632.
2024-12-01-20:46:39-root-INFO: Loss too large (805.182->805.821)! Learning rate decreased to 0.00506.
2024-12-01-20:46:39-root-INFO: grad norm: 55.107 54.200 9.959
2024-12-01-20:46:40-root-INFO: Loss Change: 820.812 -> 792.500
2024-12-01-20:46:40-root-INFO: Regularization Change: 0.000 -> 1.395
2024-12-01-20:46:40-root-INFO: Undo step: 150
2024-12-01-20:46:40-root-INFO: Undo step: 151
2024-12-01-20:46:40-root-INFO: Undo step: 152
2024-12-01-20:46:40-root-INFO: Undo step: 153
2024-12-01-20:46:40-root-INFO: Undo step: 154
2024-12-01-20:46:40-root-INFO: step: 155 lr_xt 0.01285057
2024-12-01-20:46:40-root-INFO: grad norm: 288.022 279.662 68.889
2024-12-01-20:46:41-root-INFO: grad norm: 309.721 305.945 48.220
2024-12-01-20:46:41-root-INFO: grad norm: 370.415 362.369 76.783
2024-12-01-20:46:41-root-INFO: Loss too large (1301.227->1818.962)! Learning rate decreased to 0.01028.
2024-12-01-20:46:41-root-INFO: Loss too large (1301.227->1495.465)! Learning rate decreased to 0.00822.
2024-12-01-20:46:42-root-INFO: grad norm: 276.522 274.054 36.860
2024-12-01-20:46:42-root-INFO: grad norm: 72.462 70.224 17.869
2024-12-01-20:46:43-root-INFO: Loss Change: 1552.306 -> 939.316
2024-12-01-20:46:43-root-INFO: Regularization Change: 0.000 -> 20.442
2024-12-01-20:46:43-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-01-20:46:43-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-20:46:43-root-INFO: step: 154 lr_xt 0.01333503
2024-12-01-20:46:43-root-INFO: grad norm: 81.886 79.417 19.958
2024-12-01-20:46:43-root-INFO: Loss too large (937.545->989.571)! Learning rate decreased to 0.01067.
2024-12-01-20:46:43-root-INFO: Loss too large (937.545->942.391)! Learning rate decreased to 0.00853.
2024-12-01-20:46:44-root-INFO: grad norm: 133.552 129.244 33.646
2024-12-01-20:46:44-root-INFO: Loss too large (922.333->959.148)! Learning rate decreased to 0.00683.
2024-12-01-20:46:44-root-INFO: Loss too large (922.333->935.539)! Learning rate decreased to 0.00546.
2024-12-01-20:46:45-root-INFO: grad norm: 97.094 94.652 21.643
2024-12-01-20:46:45-root-INFO: grad norm: 43.986 42.256 12.216
2024-12-01-20:46:45-root-INFO: grad norm: 40.619 38.788 12.060
2024-12-01-20:46:46-root-INFO: Loss Change: 937.545 -> 874.471
2024-12-01-20:46:46-root-INFO: Regularization Change: 0.000 -> 1.699
2024-12-01-20:46:46-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-01-20:46:46-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-20:46:46-root-INFO: step: 153 lr_xt 0.01383551
2024-12-01-20:46:46-root-INFO: grad norm: 41.307 39.200 13.023
2024-12-01-20:46:47-root-INFO: grad norm: 45.247 43.591 12.131
2024-12-01-20:46:47-root-INFO: grad norm: 84.143 81.458 21.089
2024-12-01-20:46:47-root-INFO: Loss too large (852.140->969.819)! Learning rate decreased to 0.01107.
2024-12-01-20:46:47-root-INFO: Loss too large (852.140->898.409)! Learning rate decreased to 0.00885.
2024-12-01-20:46:48-root-INFO: Loss too large (852.140->860.733)! Learning rate decreased to 0.00708.
2024-12-01-20:46:48-root-INFO: grad norm: 105.614 101.406 29.515
2024-12-01-20:46:48-root-INFO: Loss too large (843.690->854.568)! Learning rate decreased to 0.00567.
2024-12-01-20:46:49-root-INFO: grad norm: 83.069 80.549 20.304
2024-12-01-20:46:49-root-INFO: Loss Change: 871.128 -> 825.547
2024-12-01-20:46:49-root-INFO: Regularization Change: 0.000 -> 2.164
2024-12-01-20:46:49-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-01-20:46:49-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-20:46:49-root-INFO: step: 152 lr_xt 0.01435246
2024-12-01-20:46:49-root-INFO: grad norm: 35.530 34.312 9.222
2024-12-01-20:46:50-root-INFO: grad norm: 59.889 57.740 15.900
2024-12-01-20:46:50-root-INFO: Loss too large (815.284->868.095)! Learning rate decreased to 0.01148.
2024-12-01-20:46:50-root-INFO: Loss too large (815.284->833.270)! Learning rate decreased to 0.00919.
2024-12-01-20:46:50-root-INFO: Loss too large (815.284->816.635)! Learning rate decreased to 0.00735.
2024-12-01-20:46:51-root-INFO: grad norm: 69.787 66.987 19.570
2024-12-01-20:46:51-root-INFO: Loss too large (809.258->810.455)! Learning rate decreased to 0.00588.
2024-12-01-20:46:51-root-INFO: grad norm: 56.518 54.530 14.856
2024-12-01-20:46:52-root-INFO: grad norm: 39.352 37.877 10.673
2024-12-01-20:46:52-root-INFO: Loss Change: 820.889 -> 793.904
2024-12-01-20:46:52-root-INFO: Regularization Change: 0.000 -> 1.101
2024-12-01-20:46:52-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-01-20:46:52-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-20:46:52-root-INFO: step: 151 lr_xt 0.01488633
2024-12-01-20:46:52-root-INFO: grad norm: 50.742 48.450 15.076
2024-12-01-20:46:52-root-INFO: Loss too large (794.868->815.975)! Learning rate decreased to 0.01191.
2024-12-01-20:46:53-root-INFO: Loss too large (794.868->799.381)! Learning rate decreased to 0.00953.
2024-12-01-20:46:53-root-INFO: grad norm: 78.069 74.946 21.860
2024-12-01-20:46:53-root-INFO: Loss too large (791.544->802.789)! Learning rate decreased to 0.00762.
2024-12-01-20:46:53-root-INFO: Loss too large (791.544->792.718)! Learning rate decreased to 0.00610.
2024-12-01-20:46:54-root-INFO: grad norm: 56.549 54.401 15.438
2024-12-01-20:46:54-root-INFO: grad norm: 33.056 31.906 8.643
2024-12-01-20:46:55-root-INFO: grad norm: 28.257 27.082 8.065
2024-12-01-20:46:55-root-INFO: Loss Change: 794.868 -> 770.301
2024-12-01-20:46:55-root-INFO: Regularization Change: 0.000 -> 0.748
2024-12-01-20:46:55-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-01-20:46:55-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-20:46:55-root-INFO: step: 150 lr_xt 0.01543756
2024-12-01-20:46:56-root-INFO: grad norm: 25.783 24.857 6.846
2024-12-01-20:46:56-root-INFO: grad norm: 24.595 23.740 6.430
2024-12-01-20:46:57-root-INFO: grad norm: 28.317 27.518 6.678
2024-12-01-20:46:57-root-INFO: grad norm: 45.211 43.810 11.166
2024-12-01-20:46:57-root-INFO: Loss too large (748.735->760.394)! Learning rate decreased to 0.01235.
2024-12-01-20:46:57-root-INFO: Loss too large (748.735->751.192)! Learning rate decreased to 0.00988.
2024-12-01-20:46:58-root-INFO: grad norm: 54.478 52.585 14.234
2024-12-01-20:46:58-root-INFO: Loss too large (746.306->747.562)! Learning rate decreased to 0.00790.
2024-12-01-20:46:58-root-INFO: Loss Change: 768.475 -> 741.893
2024-12-01-20:46:58-root-INFO: Regularization Change: 0.000 -> 2.012
2024-12-01-20:46:58-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-01-20:46:58-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-20:46:59-root-INFO: step: 149 lr_xt 0.01600663
2024-12-01-20:46:59-root-INFO: grad norm: 35.837 34.575 9.425
2024-12-01-20:46:59-root-INFO: Loss too large (737.102->738.866)! Learning rate decreased to 0.01281.
2024-12-01-20:46:59-root-INFO: grad norm: 53.429 51.314 14.883
2024-12-01-20:47:00-root-INFO: Loss too large (735.191->745.823)! Learning rate decreased to 0.01024.
2024-12-01-20:47:00-root-INFO: grad norm: 78.266 74.906 22.687
2024-12-01-20:47:00-root-INFO: Loss too large (734.830->745.389)! Learning rate decreased to 0.00820.
2024-12-01-20:47:01-root-INFO: grad norm: 67.727 65.354 17.771
2024-12-01-20:47:01-root-INFO: grad norm: 50.123 48.231 13.641
2024-12-01-20:47:02-root-INFO: Loss Change: 737.102 -> 720.055
2024-12-01-20:47:02-root-INFO: Regularization Change: 0.000 -> 1.029
2024-12-01-20:47:02-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-01-20:47:02-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-20:47:02-root-INFO: step: 148 lr_xt 0.01659399
2024-12-01-20:47:02-root-INFO: grad norm: 56.259 53.940 15.985
2024-12-01-20:47:02-root-INFO: Loss too large (721.807->749.503)! Learning rate decreased to 0.01328.
2024-12-01-20:47:02-root-INFO: Loss too large (721.807->727.911)! Learning rate decreased to 0.01062.
2024-12-01-20:47:03-root-INFO: grad norm: 66.299 63.720 18.311
2024-12-01-20:47:03-root-INFO: Loss too large (717.636->720.803)! Learning rate decreased to 0.00850.
2024-12-01-20:47:03-root-INFO: grad norm: 53.208 51.139 14.691
2024-12-01-20:47:04-root-INFO: grad norm: 38.763 37.421 10.111
2024-12-01-20:47:04-root-INFO: grad norm: 33.167 31.791 9.454
2024-12-01-20:47:05-root-INFO: Loss Change: 721.807 -> 698.430
2024-12-01-20:47:05-root-INFO: Regularization Change: 0.000 -> 0.886
2024-12-01-20:47:05-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-01-20:47:05-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-20:47:05-root-INFO: step: 147 lr_xt 0.01720013
2024-12-01-20:47:05-root-INFO: grad norm: 23.586 22.826 5.936
2024-12-01-20:47:05-root-INFO: grad norm: 29.296 28.047 8.464
2024-12-01-20:47:06-root-INFO: Loss too large (690.447->691.245)! Learning rate decreased to 0.01376.
2024-12-01-20:47:06-root-INFO: grad norm: 44.994 43.249 12.407
2024-12-01-20:47:06-root-INFO: Loss too large (688.492->693.538)! Learning rate decreased to 0.01101.
2024-12-01-20:47:07-root-INFO: grad norm: 52.258 50.333 14.054
2024-12-01-20:47:07-root-INFO: grad norm: 61.152 58.790 16.830
2024-12-01-20:47:07-root-INFO: Loss too large (686.071->687.067)! Learning rate decreased to 0.00881.
2024-12-01-20:47:08-root-INFO: Loss Change: 696.019 -> 680.590
2024-12-01-20:47:08-root-INFO: Regularization Change: 0.000 -> 1.337
2024-12-01-20:47:08-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-01-20:47:08-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-20:47:08-root-INFO: step: 146 lr_xt 0.01782554
2024-12-01-20:47:08-root-INFO: grad norm: 57.040 55.143 14.588
2024-12-01-20:47:08-root-INFO: Loss too large (682.405->708.795)! Learning rate decreased to 0.01426.
2024-12-01-20:47:09-root-INFO: Loss too large (682.405->688.762)! Learning rate decreased to 0.01141.
2024-12-01-20:47:09-root-INFO: grad norm: 59.731 57.841 14.906
2024-12-01-20:47:10-root-INFO: grad norm: 66.410 64.530 15.690
2024-12-01-20:47:10-root-INFO: grad norm: 69.719 67.643 16.890
2024-12-01-20:47:10-root-INFO: grad norm: 71.754 69.877 16.308
2024-12-01-20:47:11-root-INFO: Loss Change: 682.405 -> 672.218
2024-12-01-20:47:11-root-INFO: Regularization Change: 0.000 -> 1.460
2024-12-01-20:47:11-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-01-20:47:11-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-20:47:11-root-INFO: step: 145 lr_xt 0.01847071
2024-12-01-20:47:11-root-INFO: grad norm: 59.638 58.181 13.100
2024-12-01-20:47:11-root-INFO: Loss too large (666.623->694.425)! Learning rate decreased to 0.01478.
2024-12-01-20:47:11-root-INFO: Loss too large (666.623->672.521)! Learning rate decreased to 0.01182.
2024-12-01-20:47:12-root-INFO: grad norm: 60.486 58.956 13.518
2024-12-01-20:47:12-root-INFO: grad norm: 61.607 60.125 13.431
2024-12-01-20:47:13-root-INFO: grad norm: 62.219 60.744 13.469
2024-12-01-20:47:13-root-INFO: grad norm: 62.367 60.834 13.744
2024-12-01-20:47:14-root-INFO: Loss Change: 666.623 -> 649.602
2024-12-01-20:47:14-root-INFO: Regularization Change: 0.000 -> 1.438
2024-12-01-20:47:14-root-INFO: Undo step: 145
2024-12-01-20:47:14-root-INFO: Undo step: 146
2024-12-01-20:47:14-root-INFO: Undo step: 147
2024-12-01-20:47:14-root-INFO: Undo step: 148
2024-12-01-20:47:14-root-INFO: Undo step: 149
2024-12-01-20:47:14-root-INFO: step: 150 lr_xt 0.01543756
2024-12-01-20:47:14-root-INFO: grad norm: 263.708 259.457 47.162
2024-12-01-20:47:15-root-INFO: grad norm: 153.137 150.069 30.501
2024-12-01-20:47:15-root-INFO: grad norm: 157.340 155.623 23.179
2024-12-01-20:47:15-root-INFO: Loss too large (899.162->967.753)! Learning rate decreased to 0.01235.
2024-12-01-20:47:16-root-INFO: grad norm: 276.417 266.129 74.713
2024-12-01-20:47:16-root-INFO: Loss too large (847.027->1104.583)! Learning rate decreased to 0.00988.
2024-12-01-20:47:16-root-INFO: Loss too large (847.027->973.229)! Learning rate decreased to 0.00790.
2024-12-01-20:47:16-root-INFO: Loss too large (847.027->889.080)! Learning rate decreased to 0.00632.
2024-12-01-20:47:17-root-INFO: grad norm: 129.773 128.090 20.835
2024-12-01-20:47:17-root-INFO: Loss Change: 1237.928 -> 760.016
2024-12-01-20:47:17-root-INFO: Regularization Change: 0.000 -> 17.332
2024-12-01-20:47:17-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-01-20:47:17-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-20:47:17-root-INFO: step: 149 lr_xt 0.01600663
2024-12-01-20:47:17-root-INFO: grad norm: 63.364 61.589 14.892
2024-12-01-20:47:18-root-INFO: grad norm: 104.637 99.744 31.622
2024-12-01-20:47:18-root-INFO: Loss too large (744.364->798.062)! Learning rate decreased to 0.01281.
2024-12-01-20:47:18-root-INFO: Loss too large (744.364->765.317)! Learning rate decreased to 0.01024.
2024-12-01-20:47:18-root-INFO: Loss too large (744.364->745.096)! Learning rate decreased to 0.00820.
2024-12-01-20:47:19-root-INFO: grad norm: 64.447 62.509 15.684
2024-12-01-20:47:19-root-INFO: grad norm: 30.218 29.065 8.269
2024-12-01-20:47:20-root-INFO: grad norm: 26.913 25.784 7.713
2024-12-01-20:47:20-root-INFO: Loss Change: 760.111 -> 701.297
2024-12-01-20:47:20-root-INFO: Regularization Change: 0.000 -> 2.526
2024-12-01-20:47:20-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-01-20:47:20-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-20:47:20-root-INFO: step: 148 lr_xt 0.01659399
2024-12-01-20:47:20-root-INFO: grad norm: 31.346 30.020 9.018
2024-12-01-20:47:21-root-INFO: grad norm: 35.639 34.524 8.847
2024-12-01-20:47:21-root-INFO: grad norm: 54.314 53.051 11.641
2024-12-01-20:47:22-root-INFO: Loss too large (686.483->696.076)! Learning rate decreased to 0.01328.
2024-12-01-20:47:22-root-INFO: grad norm: 65.753 63.984 15.151
2024-12-01-20:47:22-root-INFO: Loss too large (685.408->685.809)! Learning rate decreased to 0.01062.
2024-12-01-20:47:23-root-INFO: grad norm: 54.758 53.220 12.888
2024-12-01-20:47:23-root-INFO: Loss Change: 699.971 -> 670.367
2024-12-01-20:47:23-root-INFO: Regularization Change: 0.000 -> 2.450
2024-12-01-20:47:23-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-01-20:47:23-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-20:47:23-root-INFO: step: 147 lr_xt 0.01720013
2024-12-01-20:47:23-root-INFO: grad norm: 39.340 38.252 9.188
2024-12-01-20:47:24-root-INFO: Loss too large (665.836->668.313)! Learning rate decreased to 0.01376.
2024-12-01-20:47:24-root-INFO: grad norm: 54.693 53.046 13.321
2024-12-01-20:47:24-root-INFO: Loss too large (662.566->666.828)! Learning rate decreased to 0.01101.
2024-12-01-20:47:25-root-INFO: grad norm: 49.732 48.397 11.446
2024-12-01-20:47:25-root-INFO: grad norm: 42.430 41.176 10.237
2024-12-01-20:47:26-root-INFO: grad norm: 40.524 39.376 9.580
2024-12-01-20:47:26-root-INFO: Loss Change: 665.836 -> 644.264
2024-12-01-20:47:26-root-INFO: Regularization Change: 0.000 -> 1.344
2024-12-01-20:47:26-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-01-20:47:26-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-20:47:26-root-INFO: step: 146 lr_xt 0.01782554
2024-12-01-20:47:26-root-INFO: grad norm: 50.338 49.010 11.483
2024-12-01-20:47:26-root-INFO: Loss too large (645.289->657.971)! Learning rate decreased to 0.01426.
2024-12-01-20:47:27-root-INFO: Loss too large (645.289->648.448)! Learning rate decreased to 0.01141.
2024-12-01-20:47:27-root-INFO: grad norm: 46.741 45.563 10.428
2024-12-01-20:47:28-root-INFO: grad norm: 43.384 42.311 9.588
2024-12-01-20:47:28-root-INFO: grad norm: 42.177 41.111 9.424
2024-12-01-20:47:28-root-INFO: grad norm: 40.938 39.924 9.056
2024-12-01-20:47:29-root-INFO: Loss Change: 645.289 -> 627.602
2024-12-01-20:47:29-root-INFO: Regularization Change: 0.000 -> 1.187
2024-12-01-20:47:29-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-01-20:47:29-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-20:47:29-root-INFO: step: 145 lr_xt 0.01847071
2024-12-01-20:47:29-root-INFO: grad norm: 36.466 35.301 9.144
2024-12-01-20:47:29-root-INFO: Loss too large (625.538->629.534)! Learning rate decreased to 0.01478.
2024-12-01-20:47:30-root-INFO: grad norm: 54.288 53.015 11.689
2024-12-01-20:47:30-root-INFO: Loss too large (623.373->630.104)! Learning rate decreased to 0.01182.
2024-12-01-20:47:30-root-INFO: Loss too large (623.373->623.541)! Learning rate decreased to 0.00946.
2024-12-01-20:47:31-root-INFO: grad norm: 38.140 37.075 8.948
2024-12-01-20:47:31-root-INFO: grad norm: 22.837 22.131 5.632
2024-12-01-20:47:31-root-INFO: grad norm: 19.707 18.974 5.325
2024-12-01-20:47:32-root-INFO: Loss Change: 625.538 -> 607.801
2024-12-01-20:47:32-root-INFO: Regularization Change: 0.000 -> 0.815
2024-12-01-20:47:32-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-01-20:47:32-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-20:47:32-root-INFO: step: 144 lr_xt 0.01913614
2024-12-01-20:47:32-root-INFO: grad norm: 25.946 25.058 6.728
2024-12-01-20:47:33-root-INFO: grad norm: 41.557 40.644 8.664
2024-12-01-20:47:33-root-INFO: Loss too large (606.244->619.608)! Learning rate decreased to 0.01531.
2024-12-01-20:47:33-root-INFO: Loss too large (606.244->608.704)! Learning rate decreased to 0.01225.
2024-12-01-20:47:33-root-INFO: grad norm: 45.309 44.418 8.945
2024-12-01-20:47:34-root-INFO: grad norm: 46.360 45.417 9.302
2024-12-01-20:47:34-root-INFO: grad norm: 47.513 46.585 9.346
2024-12-01-20:47:35-root-INFO: Loss Change: 607.681 -> 598.061
2024-12-01-20:47:35-root-INFO: Regularization Change: 0.000 -> 1.276
2024-12-01-20:47:35-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-01-20:47:35-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-20:47:35-root-INFO: step: 143 lr_xt 0.01982236
2024-12-01-20:47:35-root-INFO: grad norm: 43.732 42.745 9.237
2024-12-01-20:47:35-root-INFO: Loss too large (595.278->609.469)! Learning rate decreased to 0.01586.
2024-12-01-20:47:35-root-INFO: Loss too large (595.278->596.791)! Learning rate decreased to 0.01269.
2024-12-01-20:47:36-root-INFO: grad norm: 45.249 44.341 9.022
2024-12-01-20:47:36-root-INFO: Loss too large (590.415->590.504)! Learning rate decreased to 0.01015.
2024-12-01-20:47:37-root-INFO: grad norm: 34.415 33.544 7.690
2024-12-01-20:47:37-root-INFO: grad norm: 24.406 23.775 5.515
2024-12-01-20:47:38-root-INFO: grad norm: 21.069 20.396 5.282
2024-12-01-20:47:38-root-INFO: Loss Change: 595.278 -> 577.625
2024-12-01-20:47:38-root-INFO: Regularization Change: 0.000 -> 0.740
2024-12-01-20:47:38-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-01-20:47:38-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-20:47:38-root-INFO: step: 142 lr_xt 0.02052986
2024-12-01-20:47:38-root-INFO: grad norm: 30.635 29.692 7.544
2024-12-01-20:47:38-root-INFO: Loss too large (578.274->580.360)! Learning rate decreased to 0.01642.
2024-12-01-20:47:39-root-INFO: grad norm: 40.560 39.699 8.316
2024-12-01-20:47:39-root-INFO: Loss too large (576.868->581.188)! Learning rate decreased to 0.01314.
2024-12-01-20:47:40-root-INFO: grad norm: 48.264 47.350 9.348
2024-12-01-20:47:40-root-INFO: Loss too large (574.712->576.289)! Learning rate decreased to 0.01051.
2024-12-01-20:47:40-root-INFO: grad norm: 38.127 37.265 8.062
2024-12-01-20:47:41-root-INFO: grad norm: 27.851 27.212 5.928
2024-12-01-20:47:41-root-INFO: Loss Change: 578.274 -> 564.426
2024-12-01-20:47:41-root-INFO: Regularization Change: 0.000 -> 0.860
2024-12-01-20:47:41-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-01-20:47:41-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-20:47:41-root-INFO: step: 141 lr_xt 0.02125920
2024-12-01-20:47:41-root-INFO: grad norm: 22.327 21.564 5.789
2024-12-01-20:47:42-root-INFO: grad norm: 47.876 46.887 9.685
2024-12-01-20:47:42-root-INFO: Loss too large (563.292->580.522)! Learning rate decreased to 0.01701.
2024-12-01-20:47:42-root-INFO: Loss too large (563.292->571.982)! Learning rate decreased to 0.01361.
2024-12-01-20:47:42-root-INFO: Loss too large (563.292->565.544)! Learning rate decreased to 0.01088.
2024-12-01-20:47:43-root-INFO: grad norm: 37.685 36.740 8.384
2024-12-01-20:47:43-root-INFO: grad norm: 25.673 25.039 5.670
2024-12-01-20:47:44-root-INFO: grad norm: 22.932 22.277 5.443
2024-12-01-20:47:44-root-INFO: Loss Change: 563.297 -> 550.949
2024-12-01-20:47:44-root-INFO: Regularization Change: 0.000 -> 0.817
2024-12-01-20:47:44-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-01-20:47:44-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-20:47:44-root-INFO: step: 140 lr_xt 0.02201089
2024-12-01-20:47:45-root-INFO: grad norm: 29.795 29.060 6.580
2024-12-01-20:47:45-root-INFO: Loss too large (551.535->557.083)! Learning rate decreased to 0.01761.
2024-12-01-20:47:45-root-INFO: Loss too large (551.535->552.616)! Learning rate decreased to 0.01409.
2024-12-01-20:47:45-root-INFO: grad norm: 34.237 33.449 7.303
2024-12-01-20:47:46-root-INFO: grad norm: 45.746 44.883 8.843
2024-12-01-20:47:46-root-INFO: Loss too large (549.133->552.004)! Learning rate decreased to 0.01127.
2024-12-01-20:47:47-root-INFO: grad norm: 38.336 37.467 8.118
2024-12-01-20:47:47-root-INFO: grad norm: 29.302 28.680 6.007
2024-12-01-20:47:47-root-INFO: Loss Change: 551.535 -> 540.378
2024-12-01-20:47:47-root-INFO: Regularization Change: 0.000 -> 0.766
2024-12-01-20:47:47-root-INFO: Undo step: 140
2024-12-01-20:47:47-root-INFO: Undo step: 141
2024-12-01-20:47:47-root-INFO: Undo step: 142
2024-12-01-20:47:47-root-INFO: Undo step: 143
2024-12-01-20:47:47-root-INFO: Undo step: 144
2024-12-01-20:47:48-root-INFO: step: 145 lr_xt 0.01847071
2024-12-01-20:47:48-root-INFO: grad norm: 181.898 176.739 43.015
2024-12-01-20:47:48-root-INFO: grad norm: 154.174 146.958 46.616
2024-12-01-20:47:49-root-INFO: grad norm: 171.792 167.155 39.648
2024-12-01-20:47:49-root-INFO: Loss too large (776.851->840.959)! Learning rate decreased to 0.01478.
2024-12-01-20:47:49-root-INFO: grad norm: 146.696 144.142 27.259
2024-12-01-20:47:50-root-INFO: grad norm: 89.964 88.132 18.059
2024-12-01-20:47:50-root-INFO: Loss Change: 993.893 -> 629.369
2024-12-01-20:47:50-root-INFO: Regularization Change: 0.000 -> 23.880
2024-12-01-20:47:50-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-01-20:47:50-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-20:47:50-root-INFO: step: 144 lr_xt 0.01913614
2024-12-01-20:47:50-root-INFO: grad norm: 76.828 75.468 14.395
2024-12-01-20:47:51-root-INFO: Loss too large (631.866->632.908)! Learning rate decreased to 0.01531.
2024-12-01-20:47:51-root-INFO: grad norm: 64.450 63.288 12.181
2024-12-01-20:47:52-root-INFO: grad norm: 60.916 59.910 11.021
2024-12-01-20:47:52-root-INFO: grad norm: 56.402 55.423 10.460
2024-12-01-20:47:52-root-INFO: grad norm: 51.360 50.552 9.075
2024-12-01-20:47:53-root-INFO: Loss Change: 631.866 -> 582.418
2024-12-01-20:47:53-root-INFO: Regularization Change: 0.000 -> 3.934
2024-12-01-20:47:53-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-01-20:47:53-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-20:47:53-root-INFO: step: 143 lr_xt 0.01982236
2024-12-01-20:47:53-root-INFO: grad norm: 49.676 48.834 9.108
2024-12-01-20:47:53-root-INFO: Loss too large (578.448->583.841)! Learning rate decreased to 0.01586.
2024-12-01-20:47:54-root-INFO: grad norm: 58.199 57.330 10.017
2024-12-01-20:47:54-root-INFO: Loss too large (572.376->576.075)! Learning rate decreased to 0.01269.
2024-12-01-20:47:54-root-INFO: grad norm: 46.667 45.809 8.912
2024-12-01-20:47:55-root-INFO: grad norm: 34.214 33.578 6.570
2024-12-01-20:47:55-root-INFO: grad norm: 31.709 31.004 6.650
2024-12-01-20:47:55-root-INFO: Loss Change: 578.448 -> 551.012
2024-12-01-20:47:55-root-INFO: Regularization Change: 0.000 -> 1.587
2024-12-01-20:47:56-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-01-20:47:56-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-20:47:56-root-INFO: step: 142 lr_xt 0.02052986
2024-12-01-20:47:56-root-INFO: grad norm: 42.434 41.367 9.456
2024-12-01-20:47:56-root-INFO: Loss too large (552.859->560.344)! Learning rate decreased to 0.01642.
2024-12-01-20:47:56-root-INFO: grad norm: 51.129 50.233 9.525
2024-12-01-20:47:57-root-INFO: Loss too large (552.753->553.998)! Learning rate decreased to 0.01314.
2024-12-01-20:47:57-root-INFO: grad norm: 45.941 45.189 8.278
2024-12-01-20:47:58-root-INFO: grad norm: 43.594 42.804 8.263
2024-12-01-20:47:58-root-INFO: grad norm: 41.679 40.964 7.684
2024-12-01-20:47:59-root-INFO: Loss Change: 552.859 -> 537.388
2024-12-01-20:47:59-root-INFO: Regularization Change: 0.000 -> 1.376
2024-12-01-20:47:59-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-01-20:47:59-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-20:47:59-root-INFO: step: 141 lr_xt 0.02125920
2024-12-01-20:47:59-root-INFO: grad norm: 37.723 36.915 7.767
2024-12-01-20:47:59-root-INFO: Loss too large (535.322->544.709)! Learning rate decreased to 0.01701.
2024-12-01-20:47:59-root-INFO: Loss too large (535.322->535.734)! Learning rate decreased to 0.01361.
2024-12-01-20:48:00-root-INFO: grad norm: 38.189 37.481 7.320
2024-12-01-20:48:00-root-INFO: grad norm: 38.252 37.445 7.814
2024-12-01-20:48:01-root-INFO: grad norm: 38.412 37.696 7.378
2024-12-01-20:48:01-root-INFO: grad norm: 38.510 37.709 7.813
2024-12-01-20:48:02-root-INFO: Loss Change: 535.322 -> 521.545
2024-12-01-20:48:02-root-INFO: Regularization Change: 0.000 -> 1.094
2024-12-01-20:48:02-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-01-20:48:02-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-20:48:02-root-INFO: step: 140 lr_xt 0.02201089
2024-12-01-20:48:02-root-INFO: grad norm: 49.208 48.197 9.922
2024-12-01-20:48:02-root-INFO: Loss too large (523.787->543.175)! Learning rate decreased to 0.01761.
2024-12-01-20:48:02-root-INFO: Loss too large (523.787->531.224)! Learning rate decreased to 0.01409.
2024-12-01-20:48:03-root-INFO: grad norm: 47.501 46.599 9.211
2024-12-01-20:48:03-root-INFO: grad norm: 45.433 44.695 8.157
2024-12-01-20:48:04-root-INFO: grad norm: 45.048 44.220 8.597
2024-12-01-20:48:04-root-INFO: grad norm: 44.454 43.737 7.951
2024-12-01-20:48:05-root-INFO: Loss Change: 523.787 -> 512.882
2024-12-01-20:48:05-root-INFO: Regularization Change: 0.000 -> 1.212
2024-12-01-20:48:05-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-01-20:48:05-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-20:48:05-root-INFO: step: 139 lr_xt 0.02278550
2024-12-01-20:48:05-root-INFO: grad norm: 41.344 40.470 8.457
2024-12-01-20:48:05-root-INFO: Loss too large (511.916->525.291)! Learning rate decreased to 0.01823.
2024-12-01-20:48:05-root-INFO: Loss too large (511.916->513.257)! Learning rate decreased to 0.01458.
2024-12-01-20:48:06-root-INFO: grad norm: 40.445 39.748 7.478
2024-12-01-20:48:06-root-INFO: grad norm: 40.535 39.707 8.149
2024-12-01-20:48:07-root-INFO: grad norm: 40.462 39.771 7.445
2024-12-01-20:48:07-root-INFO: grad norm: 40.734 39.928 8.063
2024-12-01-20:48:07-root-INFO: Loss Change: 511.916 -> 499.010
2024-12-01-20:48:07-root-INFO: Regularization Change: 0.000 -> 1.120
2024-12-01-20:48:07-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-01-20:48:07-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-20:48:08-root-INFO: step: 138 lr_xt 0.02358356
2024-12-01-20:48:08-root-INFO: grad norm: 48.724 47.850 9.186
2024-12-01-20:48:08-root-INFO: Loss too large (500.772->522.997)! Learning rate decreased to 0.01887.
2024-12-01-20:48:08-root-INFO: Loss too large (500.772->509.754)! Learning rate decreased to 0.01509.
2024-12-01-20:48:08-root-INFO: Loss too large (500.772->501.058)! Learning rate decreased to 0.01207.
2024-12-01-20:48:09-root-INFO: grad norm: 34.751 34.042 6.983
2024-12-01-20:48:09-root-INFO: grad norm: 23.997 23.526 4.733
2024-12-01-20:48:10-root-INFO: grad norm: 19.913 19.388 4.545
2024-12-01-20:48:10-root-INFO: grad norm: 17.179 16.751 3.810
2024-12-01-20:48:11-root-INFO: Loss Change: 500.772 -> 485.456
2024-12-01-20:48:11-root-INFO: Regularization Change: 0.000 -> 0.648
2024-12-01-20:48:11-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-01-20:48:11-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-20:48:11-root-INFO: step: 137 lr_xt 0.02440563
2024-12-01-20:48:11-root-INFO: grad norm: 22.651 21.662 6.620
2024-12-01-20:48:11-root-INFO: grad norm: 30.435 29.815 6.111
2024-12-01-20:48:12-root-INFO: Loss too large (483.686->490.153)! Learning rate decreased to 0.01952.
2024-12-01-20:48:12-root-INFO: Loss too large (483.686->484.806)! Learning rate decreased to 0.01562.
2024-12-01-20:48:12-root-INFO: grad norm: 31.018 30.481 5.749
2024-12-01-20:48:13-root-INFO: grad norm: 36.778 36.284 6.005
2024-12-01-20:48:13-root-INFO: Loss too large (480.497->481.214)! Learning rate decreased to 0.01250.
2024-12-01-20:48:13-root-INFO: grad norm: 29.473 28.931 5.629
2024-12-01-20:48:14-root-INFO: Loss Change: 486.420 -> 474.893
2024-12-01-20:48:14-root-INFO: Regularization Change: 0.000 -> 0.964
2024-12-01-20:48:14-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-01-20:48:14-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-20:48:14-root-INFO: step: 136 lr_xt 0.02525230
2024-12-01-20:48:14-root-INFO: grad norm: 27.001 26.429 5.528
2024-12-01-20:48:14-root-INFO: Loss too large (475.297->481.450)! Learning rate decreased to 0.02020.
2024-12-01-20:48:14-root-INFO: Loss too large (475.297->477.311)! Learning rate decreased to 0.01616.
2024-12-01-20:48:15-root-INFO: grad norm: 30.651 29.969 6.432
2024-12-01-20:48:15-root-INFO: grad norm: 37.246 36.607 6.865
2024-12-01-20:48:15-root-INFO: Loss too large (473.425->474.609)! Learning rate decreased to 0.01293.
2024-12-01-20:48:16-root-INFO: grad norm: 29.807 29.143 6.257
2024-12-01-20:48:16-root-INFO: grad norm: 22.599 22.134 4.561
2024-12-01-20:48:17-root-INFO: Loss Change: 475.297 -> 466.111
2024-12-01-20:48:17-root-INFO: Regularization Change: 0.000 -> 0.674
2024-12-01-20:48:17-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-01-20:48:17-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-20:48:17-root-INFO: step: 135 lr_xt 0.02612413
2024-12-01-20:48:17-root-INFO: grad norm: 22.472 21.850 5.251
2024-12-01-20:48:17-root-INFO: Loss too large (465.926->468.373)! Learning rate decreased to 0.02090.
2024-12-01-20:48:18-root-INFO: grad norm: 34.367 33.854 5.917
2024-12-01-20:48:18-root-INFO: Loss too large (465.330->470.307)! Learning rate decreased to 0.01672.
2024-12-01-20:48:18-root-INFO: Loss too large (465.330->465.745)! Learning rate decreased to 0.01338.
2024-12-01-20:48:18-root-INFO: grad norm: 27.456 26.875 5.616
2024-12-01-20:48:19-root-INFO: grad norm: 21.691 21.271 4.244
2024-12-01-20:48:19-root-INFO: grad norm: 19.272 18.800 4.240
2024-12-01-20:48:20-root-INFO: Loss Change: 465.926 -> 456.554
2024-12-01-20:48:20-root-INFO: Regularization Change: 0.000 -> 0.685
2024-12-01-20:48:20-root-INFO: Undo step: 135
2024-12-01-20:48:20-root-INFO: Undo step: 136
2024-12-01-20:48:20-root-INFO: Undo step: 137
2024-12-01-20:48:20-root-INFO: Undo step: 138
2024-12-01-20:48:20-root-INFO: Undo step: 139
2024-12-01-20:48:20-root-INFO: step: 140 lr_xt 0.02201089
2024-12-01-20:48:20-root-INFO: grad norm: 156.734 152.356 36.789
2024-12-01-20:48:20-root-INFO: grad norm: 118.390 115.710 25.048
2024-12-01-20:48:21-root-INFO: grad norm: 101.765 99.726 20.267
2024-12-01-20:48:21-root-INFO: grad norm: 102.577 100.950 18.195
2024-12-01-20:48:22-root-INFO: grad norm: 168.926 164.701 37.545
2024-12-01-20:48:22-root-INFO: Loss too large (609.276->803.608)! Learning rate decreased to 0.01761.
2024-12-01-20:48:22-root-INFO: Loss too large (609.276->691.090)! Learning rate decreased to 0.01409.
2024-12-01-20:48:22-root-INFO: Loss too large (609.276->620.739)! Learning rate decreased to 0.01127.
2024-12-01-20:48:22-root-INFO: Loss Change: 873.197 -> 579.697
2024-12-01-20:48:22-root-INFO: Regularization Change: 0.000 -> 24.466
2024-12-01-20:48:22-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-01-20:48:22-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-20:48:23-root-INFO: step: 139 lr_xt 0.02278550
2024-12-01-20:48:23-root-INFO: grad norm: 90.266 88.924 15.506
2024-12-01-20:48:23-root-INFO: grad norm: 152.082 147.392 37.477
2024-12-01-20:48:23-root-INFO: Loss too large (572.383->725.524)! Learning rate decreased to 0.01823.
2024-12-01-20:48:24-root-INFO: Loss too large (572.383->633.286)! Learning rate decreased to 0.01458.
2024-12-01-20:48:24-root-INFO: Loss too large (572.383->577.446)! Learning rate decreased to 0.01167.
2024-12-01-20:48:24-root-INFO: grad norm: 67.124 65.945 12.522
2024-12-01-20:48:25-root-INFO: grad norm: 31.340 30.601 6.769
2024-12-01-20:48:25-root-INFO: grad norm: 28.639 27.937 6.302
2024-12-01-20:48:25-root-INFO: Loss Change: 582.079 -> 505.555
2024-12-01-20:48:25-root-INFO: Regularization Change: 0.000 -> 3.550
2024-12-01-20:48:25-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-01-20:48:25-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-20:48:26-root-INFO: step: 138 lr_xt 0.02358356
2024-12-01-20:48:26-root-INFO: grad norm: 28.869 28.129 6.494
2024-12-01-20:48:26-root-INFO: Loss too large (504.267->508.817)! Learning rate decreased to 0.01887.
2024-12-01-20:48:26-root-INFO: grad norm: 52.459 51.785 8.387
2024-12-01-20:48:27-root-INFO: Loss too large (504.054->510.763)! Learning rate decreased to 0.01509.
2024-12-01-20:48:27-root-INFO: Loss too large (504.054->505.701)! Learning rate decreased to 0.01207.
2024-12-01-20:48:27-root-INFO: grad norm: 38.242 37.556 7.212
2024-12-01-20:48:28-root-INFO: grad norm: 22.441 21.841 5.154
2024-12-01-20:48:28-root-INFO: grad norm: 21.890 21.292 5.080
2024-12-01-20:48:29-root-INFO: Loss Change: 504.267 -> 487.322
2024-12-01-20:48:29-root-INFO: Regularization Change: 0.000 -> 1.149
2024-12-01-20:48:29-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-01-20:48:29-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-20:48:29-root-INFO: step: 137 lr_xt 0.02440563
2024-12-01-20:48:29-root-INFO: grad norm: 34.313 33.377 7.961
2024-12-01-20:48:29-root-INFO: Loss too large (489.068->496.645)! Learning rate decreased to 0.01952.
2024-12-01-20:48:29-root-INFO: Loss too large (489.068->490.748)! Learning rate decreased to 0.01562.
2024-12-01-20:48:30-root-INFO: grad norm: 38.317 37.599 7.380
2024-12-01-20:48:30-root-INFO: grad norm: 49.723 49.010 8.393
2024-12-01-20:48:30-root-INFO: Loss too large (486.102->488.996)! Learning rate decreased to 0.01250.
2024-12-01-20:48:31-root-INFO: grad norm: 39.198 38.513 7.295
2024-12-01-20:48:31-root-INFO: grad norm: 27.839 27.316 5.373
2024-12-01-20:48:32-root-INFO: Loss Change: 489.068 -> 474.785
2024-12-01-20:48:32-root-INFO: Regularization Change: 0.000 -> 1.011
2024-12-01-20:48:32-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-01-20:48:32-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-20:48:32-root-INFO: step: 136 lr_xt 0.02525230
2024-12-01-20:48:32-root-INFO: grad norm: 23.248 22.646 5.258
2024-12-01-20:48:32-root-INFO: Loss too large (473.989->477.197)! Learning rate decreased to 0.02020.
2024-12-01-20:48:33-root-INFO: grad norm: 39.672 39.103 6.691
2024-12-01-20:48:33-root-INFO: Loss too large (473.782->480.369)! Learning rate decreased to 0.01616.
2024-12-01-20:48:33-root-INFO: Loss too large (473.782->475.184)! Learning rate decreased to 0.01293.
2024-12-01-20:48:34-root-INFO: grad norm: 32.675 32.050 6.358
2024-12-01-20:48:34-root-INFO: grad norm: 25.585 25.090 5.008
2024-12-01-20:48:35-root-INFO: grad norm: 23.592 23.055 5.007
2024-12-01-20:48:35-root-INFO: Loss Change: 473.989 -> 463.262
2024-12-01-20:48:35-root-INFO: Regularization Change: 0.000 -> 0.816
2024-12-01-20:48:35-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-01-20:48:35-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-20:48:35-root-INFO: step: 135 lr_xt 0.02612413
2024-12-01-20:48:35-root-INFO: grad norm: 29.127 28.465 6.173
2024-12-01-20:48:35-root-INFO: Loss too large (463.945->472.551)! Learning rate decreased to 0.02090.
2024-12-01-20:48:36-root-INFO: Loss too large (463.945->466.687)! Learning rate decreased to 0.01672.
2024-12-01-20:48:36-root-INFO: grad norm: 35.148 34.520 6.614
2024-12-01-20:48:36-root-INFO: Loss too large (463.186->463.563)! Learning rate decreased to 0.01338.
2024-12-01-20:48:37-root-INFO: grad norm: 30.923 30.445 5.412
2024-12-01-20:48:37-root-INFO: grad norm: 28.343 27.805 5.494
2024-12-01-20:48:38-root-INFO: grad norm: 25.993 25.554 4.756
2024-12-01-20:48:38-root-INFO: Loss Change: 463.945 -> 454.134
2024-12-01-20:48:38-root-INFO: Regularization Change: 0.000 -> 0.755
2024-12-01-20:48:38-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-01-20:48:38-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-20:48:38-root-INFO: step: 134 lr_xt 0.02702170
2024-12-01-20:48:38-root-INFO: grad norm: 23.556 22.888 5.568
2024-12-01-20:48:39-root-INFO: Loss too large (453.351->457.786)! Learning rate decreased to 0.02162.
2024-12-01-20:48:39-root-INFO: Loss too large (453.351->453.781)! Learning rate decreased to 0.01729.
2024-12-01-20:48:39-root-INFO: grad norm: 29.161 28.703 5.148
2024-12-01-20:48:39-root-INFO: Loss too large (451.642->452.289)! Learning rate decreased to 0.01384.
2024-12-01-20:48:40-root-INFO: grad norm: 26.609 26.048 5.435
2024-12-01-20:48:40-root-INFO: grad norm: 24.043 23.605 4.565
2024-12-01-20:48:41-root-INFO: grad norm: 22.956 22.444 4.820
2024-12-01-20:48:41-root-INFO: Loss Change: 453.351 -> 444.343
2024-12-01-20:48:41-root-INFO: Regularization Change: 0.000 -> 0.697
2024-12-01-20:48:41-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-01-20:48:41-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-20:48:41-root-INFO: step: 133 lr_xt 0.02794561
2024-12-01-20:48:42-root-INFO: grad norm: 30.021 29.371 6.211
2024-12-01-20:48:42-root-INFO: Loss too large (445.686->457.498)! Learning rate decreased to 0.02236.
2024-12-01-20:48:42-root-INFO: Loss too large (445.686->450.340)! Learning rate decreased to 0.01789.
2024-12-01-20:48:42-root-INFO: Loss too large (445.686->445.911)! Learning rate decreased to 0.01431.
2024-12-01-20:48:43-root-INFO: grad norm: 27.699 27.159 5.443
2024-12-01-20:48:43-root-INFO: grad norm: 26.602 26.177 4.738
2024-12-01-20:48:44-root-INFO: grad norm: 25.789 25.298 5.006
2024-12-01-20:48:44-root-INFO: grad norm: 25.020 24.620 4.458
2024-12-01-20:48:44-root-INFO: Loss Change: 445.686 -> 436.918
2024-12-01-20:48:44-root-INFO: Regularization Change: 0.000 -> 0.693
2024-12-01-20:48:44-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-01-20:48:44-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-20:48:45-root-INFO: step: 132 lr_xt 0.02889645
2024-12-01-20:48:45-root-INFO: grad norm: 22.865 22.325 4.942
2024-12-01-20:48:45-root-INFO: Loss too large (436.361->442.906)! Learning rate decreased to 0.02312.
2024-12-01-20:48:45-root-INFO: Loss too large (436.361->438.174)! Learning rate decreased to 0.01849.
2024-12-01-20:48:46-root-INFO: grad norm: 30.774 30.340 5.150
2024-12-01-20:48:46-root-INFO: Loss too large (435.544->437.234)! Learning rate decreased to 0.01479.
2024-12-01-20:48:46-root-INFO: grad norm: 28.713 28.179 5.515
2024-12-01-20:48:47-root-INFO: grad norm: 26.269 25.859 4.625
2024-12-01-20:48:47-root-INFO: grad norm: 25.113 24.628 4.913
2024-12-01-20:48:48-root-INFO: Loss Change: 436.361 -> 428.706
2024-12-01-20:48:48-root-INFO: Regularization Change: 0.000 -> 0.700
2024-12-01-20:48:48-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-01-20:48:48-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-20:48:48-root-INFO: step: 131 lr_xt 0.02987484
2024-12-01-20:48:48-root-INFO: grad norm: 32.260 31.651 6.241
2024-12-01-20:48:48-root-INFO: Loss too large (430.816->447.093)! Learning rate decreased to 0.02390.
2024-12-01-20:48:48-root-INFO: Loss too large (430.816->437.740)! Learning rate decreased to 0.01912.
2024-12-01-20:48:48-root-INFO: Loss too large (430.816->431.864)! Learning rate decreased to 0.01530.
2024-12-01-20:48:49-root-INFO: grad norm: 29.737 29.238 5.427
2024-12-01-20:48:49-root-INFO: grad norm: 27.942 27.552 4.650
2024-12-01-20:48:50-root-INFO: grad norm: 26.527 26.079 4.859
2024-12-01-20:48:50-root-INFO: grad norm: 25.193 24.837 4.220
2024-12-01-20:48:50-root-INFO: Loss Change: 430.816 -> 421.677
2024-12-01-20:48:50-root-INFO: Regularization Change: 0.000 -> 0.745
2024-12-01-20:48:50-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-01-20:48:50-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-20:48:51-root-INFO: step: 130 lr_xt 0.03088137
2024-12-01-20:48:51-root-INFO: grad norm: 21.443 20.982 4.422
2024-12-01-20:48:51-root-INFO: Loss too large (420.925->427.262)! Learning rate decreased to 0.02471.
2024-12-01-20:48:51-root-INFO: Loss too large (420.925->422.782)! Learning rate decreased to 0.01976.
2024-12-01-20:48:52-root-INFO: grad norm: 29.074 28.703 4.632
2024-12-01-20:48:52-root-INFO: Loss too large (420.288->422.099)! Learning rate decreased to 0.01581.
2024-12-01-20:48:52-root-INFO: grad norm: 27.962 27.496 5.082
2024-12-01-20:48:53-root-INFO: grad norm: 26.560 26.198 4.373
2024-12-01-20:48:53-root-INFO: grad norm: 25.619 25.184 4.702
2024-12-01-20:48:53-root-INFO: Loss Change: 420.925 -> 414.176
2024-12-01-20:48:53-root-INFO: Regularization Change: 0.000 -> 0.715
2024-12-01-20:48:53-root-INFO: Undo step: 130
2024-12-01-20:48:53-root-INFO: Undo step: 131
2024-12-01-20:48:53-root-INFO: Undo step: 132
2024-12-01-20:48:53-root-INFO: Undo step: 133
2024-12-01-20:48:53-root-INFO: Undo step: 134
2024-12-01-20:48:54-root-INFO: step: 135 lr_xt 0.02612413
2024-12-01-20:48:54-root-INFO: grad norm: 164.182 160.134 36.235
2024-12-01-20:48:54-root-INFO: grad norm: 106.372 104.152 21.621
2024-12-01-20:48:55-root-INFO: grad norm: 69.284 67.243 16.692
2024-12-01-20:48:55-root-INFO: grad norm: 70.755 69.237 14.576
2024-12-01-20:48:56-root-INFO: grad norm: 81.543 80.411 13.543
2024-12-01-20:48:56-root-INFO: Loss too large (512.327->526.797)! Learning rate decreased to 0.02090.
2024-12-01-20:48:56-root-INFO: Loss Change: 817.881 -> 497.739
2024-12-01-20:48:56-root-INFO: Regularization Change: 0.000 -> 31.133
2024-12-01-20:48:56-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-01-20:48:56-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-20:48:56-root-INFO: step: 134 lr_xt 0.02702170
2024-12-01-20:48:56-root-INFO: grad norm: 72.097 71.047 12.261
2024-12-01-20:48:57-root-INFO: Loss too large (499.512->504.055)! Learning rate decreased to 0.02162.
2024-12-01-20:48:57-root-INFO: grad norm: 60.468 59.757 9.244
2024-12-01-20:48:57-root-INFO: grad norm: 54.745 54.011 8.933
2024-12-01-20:48:58-root-INFO: grad norm: 51.229 50.588 8.083
2024-12-01-20:48:58-root-INFO: grad norm: 49.632 48.986 7.987
2024-12-01-20:48:59-root-INFO: Loss Change: 499.512 -> 452.167
2024-12-01-20:48:59-root-INFO: Regularization Change: 0.000 -> 4.801
2024-12-01-20:48:59-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-01-20:48:59-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-20:48:59-root-INFO: step: 133 lr_xt 0.02794561
2024-12-01-20:48:59-root-INFO: grad norm: 46.620 46.051 7.260
2024-12-01-20:48:59-root-INFO: Loss too large (449.636->459.299)! Learning rate decreased to 0.02236.
2024-12-01-20:49:00-root-INFO: grad norm: 47.852 47.216 7.773
2024-12-01-20:49:00-root-INFO: grad norm: 50.083 49.557 7.239
2024-12-01-20:49:00-root-INFO: grad norm: 51.745 51.159 7.767
2024-12-01-20:49:01-root-INFO: grad norm: 52.387 51.861 7.404
2024-12-01-20:49:01-root-INFO: Loss Change: 449.636 -> 437.362
2024-12-01-20:49:01-root-INFO: Regularization Change: 0.000 -> 3.056
2024-12-01-20:49:01-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-01-20:49:01-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-20:49:01-root-INFO: step: 132 lr_xt 0.02889645
2024-12-01-20:49:02-root-INFO: grad norm: 57.065 56.372 8.868
2024-12-01-20:49:02-root-INFO: Loss too large (440.063->455.212)! Learning rate decreased to 0.02312.
2024-12-01-20:49:02-root-INFO: grad norm: 54.709 54.184 7.562
2024-12-01-20:49:03-root-INFO: grad norm: 51.825 51.266 7.588
2024-12-01-20:49:03-root-INFO: grad norm: 49.404 48.894 7.077
2024-12-01-20:49:04-root-INFO: grad norm: 47.787 47.269 7.017
2024-12-01-20:49:04-root-INFO: Loss Change: 440.063 -> 420.134
2024-12-01-20:49:04-root-INFO: Regularization Change: 0.000 -> 2.942
2024-12-01-20:49:04-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-01-20:49:04-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-20:49:04-root-INFO: step: 131 lr_xt 0.02987484
2024-12-01-20:49:04-root-INFO: grad norm: 43.345 42.921 6.046
2024-12-01-20:49:04-root-INFO: Loss too large (417.838->427.421)! Learning rate decreased to 0.02390.
2024-12-01-20:49:05-root-INFO: grad norm: 43.304 42.815 6.488
2024-12-01-20:49:05-root-INFO: grad norm: 44.389 43.968 6.098
2024-12-01-20:49:06-root-INFO: grad norm: 45.405 44.954 6.385
2024-12-01-20:49:06-root-INFO: grad norm: 45.933 45.510 6.222
2024-12-01-20:49:06-root-INFO: Loss Change: 417.838 -> 408.705
2024-12-01-20:49:06-root-INFO: Regularization Change: 0.000 -> 2.435
2024-12-01-20:49:06-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-01-20:49:06-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-20:49:07-root-INFO: step: 130 lr_xt 0.03088137
2024-12-01-20:49:07-root-INFO: grad norm: 50.747 50.184 7.536
2024-12-01-20:49:07-root-INFO: Loss too large (411.473->425.434)! Learning rate decreased to 0.02471.
2024-12-01-20:49:07-root-INFO: grad norm: 49.146 48.717 6.477
2024-12-01-20:49:08-root-INFO: grad norm: 46.880 46.438 6.417
2024-12-01-20:49:08-root-INFO: grad norm: 44.767 44.348 6.108
2024-12-01-20:49:09-root-INFO: grad norm: 43.364 42.950 5.972
2024-12-01-20:49:09-root-INFO: Loss Change: 411.473 -> 396.522
2024-12-01-20:49:09-root-INFO: Regularization Change: 0.000 -> 2.529
2024-12-01-20:49:09-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-01-20:49:09-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-20:49:09-root-INFO: step: 129 lr_xt 0.03191668
2024-12-01-20:49:10-root-INFO: grad norm: 38.371 38.044 5.004
2024-12-01-20:49:10-root-INFO: Loss too large (393.775->402.181)! Learning rate decreased to 0.02553.
2024-12-01-20:49:10-root-INFO: grad norm: 38.717 38.327 5.476
2024-12-01-20:49:11-root-INFO: grad norm: 40.017 39.669 5.270
2024-12-01-20:49:11-root-INFO: grad norm: 41.176 40.804 5.528
2024-12-01-20:49:12-root-INFO: grad norm: 41.962 41.601 5.491
2024-12-01-20:49:12-root-INFO: Loss Change: 393.775 -> 387.068
2024-12-01-20:49:12-root-INFO: Regularization Change: 0.000 -> 2.177
2024-12-01-20:49:12-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-01-20:49:12-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-20:49:12-root-INFO: step: 128 lr_xt 0.03298138
2024-12-01-20:49:12-root-INFO: grad norm: 49.572 49.004 7.481
2024-12-01-20:49:12-root-INFO: Loss too large (390.992->405.842)! Learning rate decreased to 0.02639.
2024-12-01-20:49:13-root-INFO: grad norm: 47.876 47.463 6.275
2024-12-01-20:49:13-root-INFO: grad norm: 45.299 44.892 6.063
2024-12-01-20:49:14-root-INFO: grad norm: 42.833 42.435 5.826
2024-12-01-20:49:14-root-INFO: grad norm: 41.158 40.787 5.517
2024-12-01-20:49:15-root-INFO: Loss Change: 390.992 -> 376.766
2024-12-01-20:49:15-root-INFO: Regularization Change: 0.000 -> 2.524
2024-12-01-20:49:15-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-01-20:49:15-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-20:49:15-root-INFO: step: 127 lr_xt 0.03407612
2024-12-01-20:49:15-root-INFO: grad norm: 37.992 37.681 4.848
2024-12-01-20:49:15-root-INFO: Loss too large (375.392->384.768)! Learning rate decreased to 0.02726.
2024-12-01-20:49:16-root-INFO: grad norm: 38.290 37.933 5.214
2024-12-01-20:49:16-root-INFO: grad norm: 39.109 38.771 5.127
2024-12-01-20:49:17-root-INFO: grad norm: 39.794 39.449 5.231
2024-12-01-20:49:17-root-INFO: grad norm: 40.227 39.880 5.269
2024-12-01-20:49:17-root-INFO: Loss Change: 375.392 -> 368.785
2024-12-01-20:49:17-root-INFO: Regularization Change: 0.000 -> 2.197
2024-12-01-20:49:17-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-01-20:49:17-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-20:49:18-root-INFO: step: 126 lr_xt 0.03520152
2024-12-01-20:49:18-root-INFO: grad norm: 45.020 44.555 6.460
2024-12-01-20:49:18-root-INFO: Loss too large (371.473->385.782)! Learning rate decreased to 0.02816.
2024-12-01-20:49:18-root-INFO: grad norm: 44.439 44.069 5.721
2024-12-01-20:49:19-root-INFO: grad norm: 43.143 42.781 5.577
2024-12-01-20:49:19-root-INFO: grad norm: 41.641 41.280 5.472
2024-12-01-20:49:20-root-INFO: grad norm: 40.482 40.144 5.221
2024-12-01-20:49:20-root-INFO: Loss Change: 371.473 -> 360.961
2024-12-01-20:49:20-root-INFO: Regularization Change: 0.000 -> 2.481
2024-12-01-20:49:20-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-01-20:49:20-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-20:49:20-root-INFO: step: 125 lr_xt 0.03635823
2024-12-01-20:49:21-root-INFO: grad norm: 36.153 35.876 4.469
2024-12-01-20:49:21-root-INFO: Loss too large (358.308->367.263)! Learning rate decreased to 0.02909.
2024-12-01-20:49:21-root-INFO: grad norm: 36.384 36.075 4.729
2024-12-01-20:49:22-root-INFO: grad norm: 37.019 36.716 4.723
2024-12-01-20:49:22-root-INFO: grad norm: 37.596 37.293 4.762
2024-12-01-20:49:23-root-INFO: grad norm: 37.972 37.655 4.895
2024-12-01-20:49:23-root-INFO: Loss Change: 358.308 -> 351.884
2024-12-01-20:49:23-root-INFO: Regularization Change: 0.000 -> 2.200
2024-12-01-20:49:23-root-INFO: Undo step: 125
2024-12-01-20:49:23-root-INFO: Undo step: 126
2024-12-01-20:49:23-root-INFO: Undo step: 127
2024-12-01-20:49:23-root-INFO: Undo step: 128
2024-12-01-20:49:23-root-INFO: Undo step: 129
2024-12-01-20:49:23-root-INFO: step: 130 lr_xt 0.03088137
2024-12-01-20:49:23-root-INFO: grad norm: 119.217 117.185 21.918
2024-12-01-20:49:24-root-INFO: grad norm: 74.989 73.608 14.321
2024-12-01-20:49:24-root-INFO: grad norm: 53.867 52.730 11.012
2024-12-01-20:49:25-root-INFO: grad norm: 40.737 39.850 8.455
2024-12-01-20:49:25-root-INFO: grad norm: 36.113 35.331 7.474
2024-12-01-20:49:25-root-INFO: Loss Change: 736.613 -> 408.161
2024-12-01-20:49:25-root-INFO: Regularization Change: 0.000 -> 35.507
2024-12-01-20:49:25-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-01-20:49:25-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-20:49:26-root-INFO: step: 129 lr_xt 0.03191668
2024-12-01-20:49:26-root-INFO: grad norm: 39.303 38.448 8.152
2024-12-01-20:49:26-root-INFO: grad norm: 40.836 39.978 8.327
2024-12-01-20:49:27-root-INFO: grad norm: 45.945 45.030 9.125
2024-12-01-20:49:27-root-INFO: Loss too large (398.197->399.137)! Learning rate decreased to 0.02553.
2024-12-01-20:49:27-root-INFO: grad norm: 34.908 34.176 7.111
2024-12-01-20:49:28-root-INFO: grad norm: 26.737 26.159 5.529
2024-12-01-20:49:28-root-INFO: Loss Change: 407.906 -> 376.143
2024-12-01-20:49:28-root-INFO: Regularization Change: 0.000 -> 4.190
2024-12-01-20:49:28-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-01-20:49:28-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-20:49:28-root-INFO: step: 128 lr_xt 0.03298138
2024-12-01-20:49:28-root-INFO: grad norm: 24.337 23.722 5.437
2024-12-01-20:49:29-root-INFO: grad norm: 30.073 29.466 6.007
2024-12-01-20:49:29-root-INFO: Loss too large (372.897->373.461)! Learning rate decreased to 0.02639.
2024-12-01-20:49:29-root-INFO: grad norm: 25.648 25.107 5.240
2024-12-01-20:49:30-root-INFO: grad norm: 21.671 21.217 4.413
2024-12-01-20:49:30-root-INFO: grad norm: 19.814 19.372 4.161
2024-12-01-20:49:31-root-INFO: Loss Change: 375.325 -> 358.429
2024-12-01-20:49:31-root-INFO: Regularization Change: 0.000 -> 2.364
2024-12-01-20:49:31-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-01-20:49:31-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-20:49:31-root-INFO: step: 127 lr_xt 0.03407612
2024-12-01-20:49:31-root-INFO: grad norm: 21.055 20.610 4.306
2024-12-01-20:49:31-root-INFO: grad norm: 26.273 25.745 5.239
2024-12-01-20:49:32-root-INFO: Loss too large (357.904->358.429)! Learning rate decreased to 0.02726.
2024-12-01-20:49:32-root-INFO: grad norm: 24.398 23.996 4.408
2024-12-01-20:49:33-root-INFO: grad norm: 25.161 24.716 4.709
2024-12-01-20:49:33-root-INFO: grad norm: 27.878 27.524 4.431
2024-12-01-20:49:33-root-INFO: Loss too large (351.008->351.222)! Learning rate decreased to 0.02181.
2024-12-01-20:49:34-root-INFO: Loss Change: 358.580 -> 347.825
2024-12-01-20:49:34-root-INFO: Regularization Change: 0.000 -> 1.754
2024-12-01-20:49:34-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-01-20:49:34-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-20:49:34-root-INFO: step: 126 lr_xt 0.03520152
2024-12-01-20:49:34-root-INFO: grad norm: 19.838 19.547 3.383
2024-12-01-20:49:34-root-INFO: Loss too large (346.647->348.337)! Learning rate decreased to 0.02816.
2024-12-01-20:49:35-root-INFO: grad norm: 23.852 23.596 3.490
2024-12-01-20:49:35-root-INFO: Loss too large (345.574->345.875)! Learning rate decreased to 0.02253.
2024-12-01-20:49:35-root-INFO: grad norm: 21.247 20.986 3.317
2024-12-01-20:49:36-root-INFO: grad norm: 19.582 19.377 2.822
2024-12-01-20:49:36-root-INFO: grad norm: 18.354 18.119 2.928
2024-12-01-20:49:36-root-INFO: Loss Change: 346.647 -> 338.052
2024-12-01-20:49:36-root-INFO: Regularization Change: 0.000 -> 1.100
2024-12-01-20:49:36-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-01-20:49:36-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-20:49:37-root-INFO: step: 125 lr_xt 0.03635823
2024-12-01-20:49:37-root-INFO: grad norm: 26.136 25.745 4.502
2024-12-01-20:49:37-root-INFO: Loss too large (339.977->347.082)! Learning rate decreased to 0.02909.
2024-12-01-20:49:37-root-INFO: Loss too large (339.977->340.791)! Learning rate decreased to 0.02327.
2024-12-01-20:49:38-root-INFO: grad norm: 23.523 23.293 3.278
2024-12-01-20:49:38-root-INFO: grad norm: 21.637 21.432 2.974
2024-12-01-20:49:38-root-INFO: grad norm: 19.922 19.704 2.933
2024-12-01-20:49:39-root-INFO: grad norm: 18.601 18.418 2.604
2024-12-01-20:49:39-root-INFO: Loss Change: 339.977 -> 330.458
2024-12-01-20:49:39-root-INFO: Regularization Change: 0.000 -> 1.068
2024-12-01-20:49:39-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-01-20:49:39-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-20:49:39-root-INFO: step: 124 lr_xt 0.03773645
2024-12-01-20:49:40-root-INFO: grad norm: 14.635 14.239 3.381
2024-12-01-20:49:40-root-INFO: grad norm: 18.090 17.889 2.687
2024-12-01-20:49:40-root-INFO: Loss too large (327.977->329.637)! Learning rate decreased to 0.03019.
2024-12-01-20:49:41-root-INFO: grad norm: 21.744 21.604 2.468
2024-12-01-20:49:41-root-INFO: Loss too large (327.050->327.636)! Learning rate decreased to 0.02415.
2024-12-01-20:49:41-root-INFO: grad norm: 19.418 19.264 2.435
2024-12-01-20:49:42-root-INFO: grad norm: 18.023 17.859 2.426
2024-12-01-20:49:42-root-INFO: Loss Change: 329.786 -> 322.237
2024-12-01-20:49:42-root-INFO: Regularization Change: 0.000 -> 1.220
2024-12-01-20:49:42-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-01-20:49:42-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-20:49:42-root-INFO: step: 123 lr_xt 0.03896235
2024-12-01-20:49:42-root-INFO: grad norm: 23.177 22.896 3.596
2024-12-01-20:49:43-root-INFO: Loss too large (323.619->330.536)! Learning rate decreased to 0.03117.
2024-12-01-20:49:43-root-INFO: Loss too large (323.619->324.873)! Learning rate decreased to 0.02494.
2024-12-01-20:49:43-root-INFO: grad norm: 21.987 21.812 2.768
2024-12-01-20:49:44-root-INFO: grad norm: 20.977 20.799 2.728
2024-12-01-20:49:44-root-INFO: grad norm: 19.929 19.752 2.654
2024-12-01-20:49:45-root-INFO: grad norm: 19.090 18.926 2.501
2024-12-01-20:49:45-root-INFO: Loss Change: 323.619 -> 316.119
2024-12-01-20:49:45-root-INFO: Regularization Change: 0.000 -> 1.013
2024-12-01-20:49:45-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-01-20:49:45-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-20:49:45-root-INFO: step: 122 lr_xt 0.04022160
2024-12-01-20:49:45-root-INFO: grad norm: 14.996 14.809 2.366
2024-12-01-20:49:45-root-INFO: Loss too large (315.257->316.278)! Learning rate decreased to 0.03218.
2024-12-01-20:49:46-root-INFO: grad norm: 18.832 18.681 2.384
2024-12-01-20:49:46-root-INFO: Loss too large (314.548->315.067)! Learning rate decreased to 0.02574.
2024-12-01-20:49:46-root-INFO: grad norm: 17.863 17.716 2.289
2024-12-01-20:49:47-root-INFO: grad norm: 17.252 17.101 2.278
2024-12-01-20:49:47-root-INFO: grad norm: 16.713 16.558 2.271
2024-12-01-20:49:48-root-INFO: Loss Change: 315.257 -> 309.454
2024-12-01-20:49:48-root-INFO: Regularization Change: 0.000 -> 0.960
2024-12-01-20:49:48-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-01-20:49:48-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-20:49:48-root-INFO: step: 121 lr_xt 0.04151486
2024-12-01-20:49:48-root-INFO: grad norm: 22.437 22.159 3.517
2024-12-01-20:49:48-root-INFO: Loss too large (310.731->317.876)! Learning rate decreased to 0.03321.
2024-12-01-20:49:48-root-INFO: Loss too large (310.731->312.135)! Learning rate decreased to 0.02657.
2024-12-01-20:49:49-root-INFO: grad norm: 21.255 21.094 2.615
2024-12-01-20:49:49-root-INFO: grad norm: 20.366 20.200 2.596
2024-12-01-20:49:50-root-INFO: grad norm: 19.472 19.312 2.493
2024-12-01-20:49:50-root-INFO: grad norm: 18.759 18.606 2.395
2024-12-01-20:49:51-root-INFO: Loss Change: 310.731 -> 303.878
2024-12-01-20:49:51-root-INFO: Regularization Change: 0.000 -> 1.008
2024-12-01-20:49:51-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-01-20:49:51-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-20:49:51-root-INFO: step: 120 lr_xt 0.04284282
2024-12-01-20:49:51-root-INFO: grad norm: 14.482 14.312 2.213
2024-12-01-20:49:51-root-INFO: Loss too large (303.091->304.397)! Learning rate decreased to 0.03427.
2024-12-01-20:49:51-root-INFO: grad norm: 18.573 18.429 2.308
2024-12-01-20:49:52-root-INFO: Loss too large (302.572->303.480)! Learning rate decreased to 0.02742.
2024-12-01-20:49:52-root-INFO: grad norm: 18.004 17.868 2.205
2024-12-01-20:49:53-root-INFO: grad norm: 17.670 17.525 2.253
2024-12-01-20:49:53-root-INFO: grad norm: 17.368 17.228 2.204
2024-12-01-20:49:53-root-INFO: Loss Change: 303.091 -> 298.094
2024-12-01-20:49:53-root-INFO: Regularization Change: 0.000 -> 0.967
2024-12-01-20:49:53-root-INFO: Undo step: 120
2024-12-01-20:49:53-root-INFO: Undo step: 121
2024-12-01-20:49:53-root-INFO: Undo step: 122
2024-12-01-20:49:53-root-INFO: Undo step: 123
2024-12-01-20:49:53-root-INFO: Undo step: 124
2024-12-01-20:49:54-root-INFO: step: 125 lr_xt 0.03635823
2024-12-01-20:49:54-root-INFO: grad norm: 110.040 108.365 19.127
2024-12-01-20:49:54-root-INFO: grad norm: 80.849 79.411 15.182
2024-12-01-20:49:55-root-INFO: grad norm: 70.799 69.488 13.562
2024-12-01-20:49:55-root-INFO: grad norm: 63.954 62.980 11.116
2024-12-01-20:49:56-root-INFO: grad norm: 63.322 62.248 11.614
2024-12-01-20:49:56-root-INFO: Loss Change: 637.424 -> 385.021
2024-12-01-20:49:56-root-INFO: Regularization Change: 0.000 -> 37.548
2024-12-01-20:49:56-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-01-20:49:56-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-20:49:56-root-INFO: step: 124 lr_xt 0.03773645
2024-12-01-20:49:56-root-INFO: grad norm: 74.542 73.292 13.589
2024-12-01-20:49:56-root-INFO: Loss too large (395.708->395.765)! Learning rate decreased to 0.03019.
2024-12-01-20:49:57-root-INFO: grad norm: 48.818 47.968 9.069
2024-12-01-20:49:57-root-INFO: grad norm: 32.023 31.522 5.647
2024-12-01-20:49:58-root-INFO: grad norm: 28.485 27.978 5.348
2024-12-01-20:49:58-root-INFO: grad norm: 28.009 27.587 4.843
2024-12-01-20:49:59-root-INFO: Loss Change: 395.708 -> 330.146
2024-12-01-20:49:59-root-INFO: Regularization Change: 0.000 -> 6.116
2024-12-01-20:49:59-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-01-20:49:59-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-20:49:59-root-INFO: step: 123 lr_xt 0.03896235
2024-12-01-20:49:59-root-INFO: grad norm: 25.857 25.444 4.602
2024-12-01-20:49:59-root-INFO: Loss too large (328.563->330.283)! Learning rate decreased to 0.03117.
2024-12-01-20:50:00-root-INFO: grad norm: 26.959 26.554 4.652
2024-12-01-20:50:00-root-INFO: grad norm: 28.066 27.655 4.788
2024-12-01-20:50:01-root-INFO: grad norm: 29.167 28.784 4.714
2024-12-01-20:50:01-root-INFO: grad norm: 30.658 30.269 4.870
2024-12-01-20:50:01-root-INFO: Loss Change: 328.563 -> 318.460
2024-12-01-20:50:01-root-INFO: Regularization Change: 0.000 -> 2.664
2024-12-01-20:50:01-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-01-20:50:01-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-20:50:02-root-INFO: step: 122 lr_xt 0.04022160
2024-12-01-20:50:02-root-INFO: grad norm: 36.715 36.218 6.022
2024-12-01-20:50:02-root-INFO: Loss too large (320.853->331.742)! Learning rate decreased to 0.03218.
2024-12-01-20:50:02-root-INFO: grad norm: 37.408 37.022 5.360
2024-12-01-20:50:03-root-INFO: grad norm: 37.041 36.659 5.306
2024-12-01-20:50:03-root-INFO: grad norm: 36.671 36.302 5.191
2024-12-01-20:50:04-root-INFO: grad norm: 36.312 35.953 5.093
2024-12-01-20:50:04-root-INFO: Loss Change: 320.853 -> 312.073
2024-12-01-20:50:04-root-INFO: Regularization Change: 0.000 -> 2.767
2024-12-01-20:50:04-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-01-20:50:04-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-20:50:04-root-INFO: step: 121 lr_xt 0.04151486
2024-12-01-20:50:04-root-INFO: grad norm: 32.373 32.079 4.357
2024-12-01-20:50:04-root-INFO: Loss too large (309.492->317.961)! Learning rate decreased to 0.03321.
2024-12-01-20:50:05-root-INFO: grad norm: 33.150 32.821 4.660
2024-12-01-20:50:05-root-INFO: grad norm: 34.089 33.762 4.708
2024-12-01-20:50:06-root-INFO: grad norm: 34.821 34.487 4.807
2024-12-01-20:50:06-root-INFO: grad norm: 35.324 34.990 4.850
2024-12-01-20:50:07-root-INFO: Loss Change: 309.492 -> 303.813
2024-12-01-20:50:07-root-INFO: Regularization Change: 0.000 -> 2.498
2024-12-01-20:50:07-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-01-20:50:07-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-20:50:07-root-INFO: step: 120 lr_xt 0.04284282
2024-12-01-20:50:07-root-INFO: grad norm: 41.299 40.821 6.266
2024-12-01-20:50:07-root-INFO: Loss too large (307.731->322.180)! Learning rate decreased to 0.03427.
2024-12-01-20:50:08-root-INFO: grad norm: 40.291 39.947 5.248
2024-12-01-20:50:08-root-INFO: grad norm: 38.568 38.212 5.233
2024-12-01-20:50:08-root-INFO: grad norm: 36.977 36.649 4.909
2024-12-01-20:50:09-root-INFO: grad norm: 35.772 35.445 4.825
2024-12-01-20:50:09-root-INFO: Loss Change: 307.731 -> 297.032
2024-12-01-20:50:09-root-INFO: Regularization Change: 0.000 -> 2.872
2024-12-01-20:50:09-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-01-20:50:09-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-20:50:09-root-INFO: step: 119 lr_xt 0.04420613
2024-12-01-20:50:10-root-INFO: grad norm: 30.701 30.451 3.915
2024-12-01-20:50:10-root-INFO: Loss too large (293.888->301.881)! Learning rate decreased to 0.03536.
2024-12-01-20:50:10-root-INFO: grad norm: 31.049 30.747 4.315
2024-12-01-20:50:11-root-INFO: grad norm: 31.743 31.464 4.201
2024-12-01-20:50:11-root-INFO: grad norm: 32.535 32.234 4.416
2024-12-01-20:50:12-root-INFO: grad norm: 33.176 32.884 4.388
2024-12-01-20:50:12-root-INFO: Loss Change: 293.888 -> 289.083
2024-12-01-20:50:12-root-INFO: Regularization Change: 0.000 -> 2.376
2024-12-01-20:50:12-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-01-20:50:12-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-20:50:12-root-INFO: step: 118 lr_xt 0.04560549
2024-12-01-20:50:12-root-INFO: grad norm: 37.170 36.780 5.367
2024-12-01-20:50:12-root-INFO: Loss too large (291.859->305.384)! Learning rate decreased to 0.03648.
2024-12-01-20:50:13-root-INFO: grad norm: 37.076 36.783 4.648
2024-12-01-20:50:13-root-INFO: grad norm: 36.349 36.036 4.762
2024-12-01-20:50:14-root-INFO: grad norm: 35.452 35.170 4.465
2024-12-01-20:50:14-root-INFO: grad norm: 34.641 34.350 4.481
2024-12-01-20:50:15-root-INFO: Loss Change: 291.859 -> 284.544
2024-12-01-20:50:15-root-INFO: Regularization Change: 0.000 -> 2.723
2024-12-01-20:50:15-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-01-20:50:15-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-20:50:15-root-INFO: step: 117 lr_xt 0.04704158
2024-12-01-20:50:15-root-INFO: grad norm: 29.719 29.525 3.393
2024-12-01-20:50:15-root-INFO: Loss too large (281.571->290.208)! Learning rate decreased to 0.03763.
2024-12-01-20:50:16-root-INFO: grad norm: 30.549 30.285 4.013
2024-12-01-20:50:16-root-INFO: grad norm: 31.733 31.503 3.814
2024-12-01-20:50:16-root-INFO: grad norm: 32.772 32.510 4.134
2024-12-01-20:50:17-root-INFO: grad norm: 33.539 33.301 3.992
2024-12-01-20:50:17-root-INFO: Loss Change: 281.571 -> 278.656
2024-12-01-20:50:17-root-INFO: Regularization Change: 0.000 -> 2.425
2024-12-01-20:50:17-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-01-20:50:17-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-20:50:17-root-INFO: step: 116 lr_xt 0.04851508
2024-12-01-20:50:18-root-INFO: grad norm: 38.679 38.309 5.344
2024-12-01-20:50:18-root-INFO: Loss too large (282.002->297.169)! Learning rate decreased to 0.03881.
2024-12-01-20:50:18-root-INFO: grad norm: 37.797 37.546 4.350
2024-12-01-20:50:19-root-INFO: grad norm: 36.265 36.001 4.363
2024-12-01-20:50:19-root-INFO: grad norm: 34.994 34.767 3.976
2024-12-01-20:50:20-root-INFO: grad norm: 34.042 33.817 3.914
2024-12-01-20:50:20-root-INFO: Loss Change: 282.002 -> 273.526
2024-12-01-20:50:20-root-INFO: Regularization Change: 0.000 -> 2.954
2024-12-01-20:50:20-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-01-20:50:20-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-20:50:20-root-INFO: step: 115 lr_xt 0.05002669
2024-12-01-20:50:20-root-INFO: grad norm: 30.129 29.996 2.828
2024-12-01-20:50:20-root-INFO: Loss too large (271.262->281.524)! Learning rate decreased to 0.04002.
2024-12-01-20:50:21-root-INFO: grad norm: 31.340 31.150 3.445
2024-12-01-20:50:21-root-INFO: Loss too large (270.705->270.894)! Learning rate decreased to 0.03202.
2024-12-01-20:50:22-root-INFO: grad norm: 21.424 21.288 2.406
2024-12-01-20:50:22-root-INFO: grad norm: 16.070 15.929 2.125
2024-12-01-20:50:22-root-INFO: grad norm: 13.089 12.961 1.829
2024-12-01-20:50:23-root-INFO: Loss Change: 271.262 -> 258.662
2024-12-01-20:50:23-root-INFO: Regularization Change: 0.000 -> 1.293
2024-12-01-20:50:23-root-INFO: Undo step: 115
2024-12-01-20:50:23-root-INFO: Undo step: 116
2024-12-01-20:50:23-root-INFO: Undo step: 117
2024-12-01-20:50:23-root-INFO: Undo step: 118
2024-12-01-20:50:23-root-INFO: Undo step: 119
2024-12-01-20:50:23-root-INFO: step: 120 lr_xt 0.04284282
2024-12-01-20:50:23-root-INFO: grad norm: 92.233 90.265 18.950
2024-12-01-20:50:24-root-INFO: grad norm: 48.438 47.628 8.821
2024-12-01-20:50:24-root-INFO: grad norm: 37.728 37.055 7.096
2024-12-01-20:50:25-root-INFO: grad norm: 34.867 34.403 5.673
2024-12-01-20:50:25-root-INFO: grad norm: 37.061 36.595 5.861
2024-12-01-20:50:25-root-INFO: Loss Change: 556.642 -> 316.407
2024-12-01-20:50:25-root-INFO: Regularization Change: 0.000 -> 38.044
2024-12-01-20:50:25-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-01-20:50:25-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-20:50:25-root-INFO: step: 119 lr_xt 0.04420613
2024-12-01-20:50:26-root-INFO: grad norm: 42.239 41.767 6.299
2024-12-01-20:50:26-root-INFO: Loss too large (316.539->318.189)! Learning rate decreased to 0.03536.
2024-12-01-20:50:26-root-INFO: grad norm: 34.129 33.789 4.808
2024-12-01-20:50:27-root-INFO: grad norm: 28.888 28.646 3.732
2024-12-01-20:50:27-root-INFO: grad norm: 26.417 26.202 3.364
2024-12-01-20:50:28-root-INFO: grad norm: 25.077 24.879 3.146
2024-12-01-20:50:28-root-INFO: Loss Change: 316.539 -> 286.269
2024-12-01-20:50:28-root-INFO: Regularization Change: 0.000 -> 4.502
2024-12-01-20:50:28-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-01-20:50:28-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-20:50:28-root-INFO: step: 118 lr_xt 0.04560549
2024-12-01-20:50:28-root-INFO: grad norm: 23.959 23.756 3.113
2024-12-01-20:50:28-root-INFO: Loss too large (286.372->289.386)! Learning rate decreased to 0.03648.
2024-12-01-20:50:29-root-INFO: grad norm: 23.248 23.078 2.809
2024-12-01-20:50:29-root-INFO: grad norm: 22.601 22.451 2.604
2024-12-01-20:50:30-root-INFO: grad norm: 22.573 22.421 2.615
2024-12-01-20:50:30-root-INFO: grad norm: 22.807 22.673 2.469
2024-12-01-20:50:31-root-INFO: Loss Change: 286.372 -> 276.876
2024-12-01-20:50:31-root-INFO: Regularization Change: 0.000 -> 2.532
2024-12-01-20:50:31-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-01-20:50:31-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-20:50:31-root-INFO: step: 117 lr_xt 0.04704158
2024-12-01-20:50:31-root-INFO: grad norm: 27.559 27.277 3.932
2024-12-01-20:50:31-root-INFO: Loss too large (278.471->285.106)! Learning rate decreased to 0.03763.
2024-12-01-20:50:32-root-INFO: grad norm: 29.751 29.602 2.976
2024-12-01-20:50:32-root-INFO: Loss too large (276.817->277.670)! Learning rate decreased to 0.03011.
2024-12-01-20:50:32-root-INFO: grad norm: 21.701 21.549 2.571
2024-12-01-20:50:33-root-INFO: grad norm: 15.970 15.842 2.024
2024-12-01-20:50:33-root-INFO: grad norm: 12.976 12.827 1.961
2024-12-01-20:50:33-root-INFO: Loss Change: 278.471 -> 265.353
2024-12-01-20:50:33-root-INFO: Regularization Change: 0.000 -> 1.489
2024-12-01-20:50:33-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-01-20:50:33-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-20:50:34-root-INFO: step: 116 lr_xt 0.04851508
2024-12-01-20:50:34-root-INFO: grad norm: 10.929 10.669 2.370
2024-12-01-20:50:34-root-INFO: grad norm: 15.297 15.150 2.116
2024-12-01-20:50:34-root-INFO: Loss too large (264.287->266.313)! Learning rate decreased to 0.03881.
2024-12-01-20:50:35-root-INFO: grad norm: 18.682 18.557 2.156
2024-12-01-20:50:35-root-INFO: Loss too large (263.686->264.245)! Learning rate decreased to 0.03105.
2024-12-01-20:50:35-root-INFO: grad norm: 15.705 15.563 2.104
2024-12-01-20:50:36-root-INFO: grad norm: 13.196 13.067 1.838
2024-12-01-20:50:36-root-INFO: Loss Change: 265.167 -> 259.005
2024-12-01-20:50:36-root-INFO: Regularization Change: 0.000 -> 1.245
2024-12-01-20:50:36-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-01-20:50:36-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-20:50:36-root-INFO: step: 115 lr_xt 0.05002669
2024-12-01-20:50:37-root-INFO: grad norm: 14.140 13.900 2.592
2024-12-01-20:50:37-root-INFO: Loss too large (259.670->261.281)! Learning rate decreased to 0.04002.
2024-12-01-20:50:37-root-INFO: grad norm: 17.527 17.407 2.043
2024-12-01-20:50:37-root-INFO: Loss too large (259.085->259.872)! Learning rate decreased to 0.03202.
2024-12-01-20:50:38-root-INFO: grad norm: 15.666 15.522 2.117
2024-12-01-20:50:38-root-INFO: grad norm: 14.086 13.971 1.794
2024-12-01-20:50:39-root-INFO: grad norm: 12.896 12.757 1.891
2024-12-01-20:50:39-root-INFO: Loss Change: 259.670 -> 254.017
2024-12-01-20:50:39-root-INFO: Regularization Change: 0.000 -> 1.013
2024-12-01-20:50:39-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-01-20:50:39-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-20:50:39-root-INFO: step: 114 lr_xt 0.05157710
2024-12-01-20:50:39-root-INFO: grad norm: 11.188 10.978 2.157
2024-12-01-20:50:40-root-INFO: Loss too large (253.931->254.365)! Learning rate decreased to 0.04126.
2024-12-01-20:50:40-root-INFO: grad norm: 13.648 13.494 2.041
2024-12-01-20:50:40-root-INFO: Loss too large (253.211->253.290)! Learning rate decreased to 0.03301.
2024-12-01-20:50:41-root-INFO: grad norm: 12.573 12.440 1.826
2024-12-01-20:50:41-root-INFO: grad norm: 11.744 11.599 1.842
2024-12-01-20:50:42-root-INFO: grad norm: 10.987 10.859 1.673
2024-12-01-20:50:42-root-INFO: Loss Change: 253.931 -> 249.111
2024-12-01-20:50:42-root-INFO: Regularization Change: 0.000 -> 0.918
2024-12-01-20:50:42-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-01-20:50:42-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-20:50:42-root-INFO: step: 113 lr_xt 0.05316701
2024-12-01-20:50:42-root-INFO: grad norm: 11.865 11.642 2.291
2024-12-01-20:50:42-root-INFO: Loss too large (249.131->250.259)! Learning rate decreased to 0.04253.
2024-12-01-20:50:43-root-INFO: grad norm: 15.120 15.005 1.861
2024-12-01-20:50:43-root-INFO: Loss too large (248.647->249.457)! Learning rate decreased to 0.03403.
2024-12-01-20:50:44-root-INFO: grad norm: 14.222 14.085 1.966
2024-12-01-20:50:44-root-INFO: grad norm: 13.434 13.326 1.697
2024-12-01-20:50:44-root-INFO: grad norm: 12.787 12.657 1.817
2024-12-01-20:50:45-root-INFO: Loss Change: 249.131 -> 244.794
2024-12-01-20:50:45-root-INFO: Regularization Change: 0.000 -> 0.922
2024-12-01-20:50:45-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-01-20:50:45-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-20:50:45-root-INFO: step: 112 lr_xt 0.05479712
2024-12-01-20:50:45-root-INFO: grad norm: 13.387 13.164 2.434
2024-12-01-20:50:45-root-INFO: Loss too large (244.922->247.103)! Learning rate decreased to 0.04384.
2024-12-01-20:50:46-root-INFO: grad norm: 16.921 16.760 2.329
2024-12-01-20:50:46-root-INFO: Loss too large (244.862->245.925)! Learning rate decreased to 0.03507.
2024-12-01-20:50:46-root-INFO: grad norm: 15.605 15.468 2.067
2024-12-01-20:50:47-root-INFO: grad norm: 14.588 14.442 2.061
2024-12-01-20:50:47-root-INFO: grad norm: 13.551 13.423 1.860
2024-12-01-20:50:48-root-INFO: Loss Change: 244.922 -> 240.615
2024-12-01-20:50:48-root-INFO: Regularization Change: 0.000 -> 0.970
2024-12-01-20:50:48-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-01-20:50:48-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-20:50:48-root-INFO: step: 111 lr_xt 0.05646812
2024-12-01-20:50:48-root-INFO: grad norm: 13.677 13.508 2.148
2024-12-01-20:50:48-root-INFO: Loss too large (240.836->244.104)! Learning rate decreased to 0.04517.
2024-12-01-20:50:48-root-INFO: Loss too large (240.836->241.224)! Learning rate decreased to 0.03614.
2024-12-01-20:50:49-root-INFO: grad norm: 12.824 12.712 1.698
2024-12-01-20:50:49-root-INFO: grad norm: 12.328 12.197 1.792
2024-12-01-20:50:50-root-INFO: grad norm: 11.878 11.771 1.591
2024-12-01-20:50:50-root-INFO: grad norm: 11.508 11.384 1.688
2024-12-01-20:50:50-root-INFO: Loss Change: 240.836 -> 236.438
2024-12-01-20:50:50-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-01-20:50:50-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-01-20:50:50-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-20:50:51-root-INFO: step: 110 lr_xt 0.05818072
2024-12-01-20:50:51-root-INFO: grad norm: 11.994 11.809 2.103
2024-12-01-20:50:51-root-INFO: Loss too large (236.730->238.850)! Learning rate decreased to 0.04654.
2024-12-01-20:50:51-root-INFO: Loss too large (236.730->236.867)! Learning rate decreased to 0.03724.
2024-12-01-20:50:51-root-INFO: grad norm: 11.343 11.216 1.692
2024-12-01-20:50:52-root-INFO: grad norm: 11.053 10.930 1.645
2024-12-01-20:50:52-root-INFO: grad norm: 10.791 10.672 1.598
2024-12-01-20:50:53-root-INFO: grad norm: 10.511 10.396 1.550
2024-12-01-20:50:53-root-INFO: Loss Change: 236.730 -> 232.743
2024-12-01-20:50:53-root-INFO: Regularization Change: 0.000 -> 0.835
2024-12-01-20:50:53-root-INFO: Undo step: 110
2024-12-01-20:50:53-root-INFO: Undo step: 111
2024-12-01-20:50:53-root-INFO: Undo step: 112
2024-12-01-20:50:53-root-INFO: Undo step: 113
2024-12-01-20:50:53-root-INFO: Undo step: 114
2024-12-01-20:50:53-root-INFO: step: 115 lr_xt 0.05002669
2024-12-01-20:50:54-root-INFO: grad norm: 90.088 88.836 14.970
2024-12-01-20:50:54-root-INFO: grad norm: 75.877 75.063 11.088
2024-12-01-20:50:55-root-INFO: grad norm: 74.456 73.735 10.337
2024-12-01-20:50:55-root-INFO: grad norm: 67.202 66.760 7.700
2024-12-01-20:50:56-root-INFO: grad norm: 61.695 61.038 8.974
2024-12-01-20:50:56-root-INFO: Loss Change: 520.348 -> 315.409
2024-12-01-20:50:56-root-INFO: Regularization Change: 0.000 -> 46.369
2024-12-01-20:50:56-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-01-20:50:56-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-20:50:56-root-INFO: step: 114 lr_xt 0.05157710
2024-12-01-20:50:56-root-INFO: grad norm: 64.576 63.921 9.175
2024-12-01-20:50:56-root-INFO: Loss too large (321.987->326.864)! Learning rate decreased to 0.04126.
2024-12-01-20:50:57-root-INFO: grad norm: 45.124 44.592 6.911
2024-12-01-20:50:57-root-INFO: grad norm: 33.225 32.941 4.334
2024-12-01-20:50:58-root-INFO: grad norm: 29.296 28.982 4.276
2024-12-01-20:50:58-root-INFO: grad norm: 27.113 26.893 3.451
2024-12-01-20:50:59-root-INFO: Loss Change: 321.987 -> 260.602
2024-12-01-20:50:59-root-INFO: Regularization Change: 0.000 -> 7.613
2024-12-01-20:50:59-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-01-20:50:59-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-20:50:59-root-INFO: step: 113 lr_xt 0.05316701
2024-12-01-20:50:59-root-INFO: grad norm: 23.554 23.388 2.792
2024-12-01-20:50:59-root-INFO: Loss too large (258.141->262.101)! Learning rate decreased to 0.04253.
2024-12-01-20:50:59-root-INFO: grad norm: 24.215 24.028 3.000
2024-12-01-20:51:00-root-INFO: grad norm: 25.556 25.347 3.260
2024-12-01-20:51:00-root-INFO: grad norm: 27.417 27.218 3.302
2024-12-01-20:51:01-root-INFO: grad norm: 29.237 29.002 3.696
2024-12-01-20:51:01-root-INFO: Loss Change: 258.141 -> 252.917
2024-12-01-20:51:01-root-INFO: Regularization Change: 0.000 -> 3.145
2024-12-01-20:51:01-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-01-20:51:01-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-20:51:01-root-INFO: step: 112 lr_xt 0.05479712
2024-12-01-20:51:02-root-INFO: grad norm: 37.399 37.005 5.412
2024-12-01-20:51:02-root-INFO: Loss too large (256.979->274.769)! Learning rate decreased to 0.04384.
2024-12-01-20:51:02-root-INFO: Loss too large (256.979->257.875)! Learning rate decreased to 0.03507.
2024-12-01-20:51:02-root-INFO: grad norm: 25.284 25.049 3.440
2024-12-01-20:51:03-root-INFO: grad norm: 17.439 17.268 2.435
2024-12-01-20:51:03-root-INFO: grad norm: 13.476 13.305 2.136
2024-12-01-20:51:04-root-INFO: grad norm: 10.940 10.790 1.803
2024-12-01-20:51:04-root-INFO: Loss Change: 256.979 -> 237.447
2024-12-01-20:51:04-root-INFO: Regularization Change: 0.000 -> 1.832
2024-12-01-20:51:04-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-01-20:51:04-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-20:51:04-root-INFO: step: 111 lr_xt 0.05646812
2024-12-01-20:51:04-root-INFO: grad norm: 7.128 6.944 1.606
2024-12-01-20:51:05-root-INFO: grad norm: 8.725 8.586 1.547
2024-12-01-20:51:05-root-INFO: grad norm: 13.616 13.488 1.861
2024-12-01-20:51:06-root-INFO: Loss too large (234.999->237.416)! Learning rate decreased to 0.04517.
2024-12-01-20:51:06-root-INFO: grad norm: 17.083 16.934 2.250
2024-12-01-20:51:06-root-INFO: Loss too large (234.874->235.597)! Learning rate decreased to 0.03614.
2024-12-01-20:51:07-root-INFO: grad norm: 14.586 14.436 2.090
2024-12-01-20:51:07-root-INFO: Loss Change: 236.858 -> 231.846
2024-12-01-20:51:07-root-INFO: Regularization Change: 0.000 -> 1.498
2024-12-01-20:51:07-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-01-20:51:07-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-20:51:07-root-INFO: step: 110 lr_xt 0.05818072
2024-12-01-20:51:07-root-INFO: grad norm: 17.448 17.194 2.970
2024-12-01-20:51:07-root-INFO: Loss too large (233.235->238.767)! Learning rate decreased to 0.04654.
2024-12-01-20:51:08-root-INFO: Loss too large (233.235->234.165)! Learning rate decreased to 0.03724.
2024-12-01-20:51:08-root-INFO: grad norm: 15.235 15.088 2.108
2024-12-01-20:51:09-root-INFO: grad norm: 13.490 13.329 2.075
2024-12-01-20:51:09-root-INFO: grad norm: 12.200 12.062 1.829
2024-12-01-20:51:09-root-INFO: grad norm: 11.136 10.992 1.785
2024-12-01-20:51:10-root-INFO: Loss Change: 233.235 -> 227.086
2024-12-01-20:51:10-root-INFO: Regularization Change: 0.000 -> 1.044
2024-12-01-20:51:10-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-01-20:51:10-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-20:51:10-root-INFO: step: 109 lr_xt 0.05993563
2024-12-01-20:51:10-root-INFO: grad norm: 7.631 7.445 1.674
2024-12-01-20:51:11-root-INFO: grad norm: 10.873 10.749 1.633
2024-12-01-20:51:11-root-INFO: Loss too large (225.969->227.436)! Learning rate decreased to 0.04795.
2024-12-01-20:51:11-root-INFO: grad norm: 14.003 13.869 1.933
2024-12-01-20:51:11-root-INFO: Loss too large (225.823->226.441)! Learning rate decreased to 0.03836.
2024-12-01-20:51:12-root-INFO: grad norm: 12.868 12.723 1.926
2024-12-01-20:51:12-root-INFO: grad norm: 12.049 11.914 1.803
2024-12-01-20:51:13-root-INFO: Loss Change: 226.772 -> 222.894
2024-12-01-20:51:13-root-INFO: Regularization Change: 0.000 -> 1.112
2024-12-01-20:51:13-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-01-20:51:13-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-20:51:13-root-INFO: step: 108 lr_xt 0.06173354
2024-12-01-20:51:13-root-INFO: grad norm: 15.260 15.030 2.643
2024-12-01-20:51:13-root-INFO: Loss too large (223.646->228.599)! Learning rate decreased to 0.04939.
2024-12-01-20:51:13-root-INFO: Loss too large (223.646->224.716)! Learning rate decreased to 0.03951.
2024-12-01-20:51:14-root-INFO: grad norm: 14.021 13.882 1.966
2024-12-01-20:51:14-root-INFO: grad norm: 13.018 12.865 1.987
2024-12-01-20:51:15-root-INFO: grad norm: 12.235 12.107 1.762
2024-12-01-20:51:15-root-INFO: grad norm: 11.537 11.400 1.773
2024-12-01-20:51:16-root-INFO: Loss Change: 223.646 -> 218.974
2024-12-01-20:51:16-root-INFO: Regularization Change: 0.000 -> 0.959
2024-12-01-20:51:16-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-01-20:51:16-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-20:51:16-root-INFO: step: 107 lr_xt 0.06357517
2024-12-01-20:51:16-root-INFO: grad norm: 8.175 7.986 1.752
2024-12-01-20:51:17-root-INFO: grad norm: 12.441 12.312 1.785
2024-12-01-20:51:17-root-INFO: Loss too large (218.301->221.344)! Learning rate decreased to 0.05086.
2024-12-01-20:51:17-root-INFO: Loss too large (218.301->218.772)! Learning rate decreased to 0.04069.
2024-12-01-20:51:17-root-INFO: grad norm: 11.590 11.459 1.734
2024-12-01-20:51:18-root-INFO: grad norm: 11.251 11.118 1.729
2024-12-01-20:51:18-root-INFO: grad norm: 11.013 10.889 1.653
2024-12-01-20:51:19-root-INFO: Loss Change: 218.659 -> 215.212
2024-12-01-20:51:19-root-INFO: Regularization Change: 0.000 -> 0.995
2024-12-01-20:51:19-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-01-20:51:19-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-20:51:19-root-INFO: step: 106 lr_xt 0.06546120
2024-12-01-20:51:19-root-INFO: grad norm: 15.877 15.618 2.856
2024-12-01-20:51:19-root-INFO: Loss too large (216.438->222.713)! Learning rate decreased to 0.05237.
2024-12-01-20:51:19-root-INFO: Loss too large (216.438->218.088)! Learning rate decreased to 0.04190.
2024-12-01-20:51:20-root-INFO: grad norm: 15.057 14.922 2.009
2024-12-01-20:51:20-root-INFO: grad norm: 14.485 14.324 2.153
2024-12-01-20:51:21-root-INFO: grad norm: 14.007 13.882 1.860
2024-12-01-20:51:21-root-INFO: grad norm: 13.542 13.400 1.955
2024-12-01-20:51:22-root-INFO: Loss Change: 216.438 -> 212.324
2024-12-01-20:51:22-root-INFO: Regularization Change: 0.000 -> 1.053
2024-12-01-20:51:22-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-01-20:51:22-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-20:51:22-root-INFO: step: 105 lr_xt 0.06739236
2024-12-01-20:51:22-root-INFO: grad norm: 10.543 10.433 1.520
2024-12-01-20:51:22-root-INFO: Loss too large (211.948->213.986)! Learning rate decreased to 0.05391.
2024-12-01-20:51:22-root-INFO: Loss too large (211.948->212.066)! Learning rate decreased to 0.04313.
2024-12-01-20:51:23-root-INFO: grad norm: 10.114 9.984 1.615
2024-12-01-20:51:23-root-INFO: grad norm: 10.093 9.994 1.409
2024-12-01-20:51:24-root-INFO: grad norm: 10.153 10.033 1.555
2024-12-01-20:51:24-root-INFO: grad norm: 10.250 10.151 1.419
2024-12-01-20:51:25-root-INFO: Loss Change: 211.948 -> 208.643
2024-12-01-20:51:25-root-INFO: Regularization Change: 0.000 -> 0.867
2024-12-01-20:51:25-root-INFO: Undo step: 105
2024-12-01-20:51:25-root-INFO: Undo step: 106
2024-12-01-20:51:25-root-INFO: Undo step: 107
2024-12-01-20:51:25-root-INFO: Undo step: 108
2024-12-01-20:51:25-root-INFO: Undo step: 109
2024-12-01-20:51:25-root-INFO: step: 110 lr_xt 0.05818072
2024-12-01-20:51:25-root-INFO: grad norm: 65.230 64.195 11.569
2024-12-01-20:51:26-root-INFO: grad norm: 49.884 49.323 7.459
2024-12-01-20:51:26-root-INFO: grad norm: 52.409 51.911 7.205
2024-12-01-20:51:27-root-INFO: grad norm: 57.615 57.216 6.774
2024-12-01-20:51:27-root-INFO: Loss too large (301.188->304.416)! Learning rate decreased to 0.04654.
2024-12-01-20:51:27-root-INFO: grad norm: 42.229 41.901 5.258
2024-12-01-20:51:28-root-INFO: Loss Change: 415.938 -> 257.415
2024-12-01-20:51:28-root-INFO: Regularization Change: 0.000 -> 33.494
2024-12-01-20:51:28-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-01-20:51:28-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-20:51:28-root-INFO: step: 109 lr_xt 0.05993563
2024-12-01-20:51:28-root-INFO: grad norm: 38.289 37.968 4.955
2024-12-01-20:51:28-root-INFO: Loss too large (260.748->274.439)! Learning rate decreased to 0.04795.
2024-12-01-20:51:29-root-INFO: grad norm: 36.509 36.227 4.533
2024-12-01-20:51:29-root-INFO: grad norm: 35.971 35.729 4.172
2024-12-01-20:51:30-root-INFO: grad norm: 35.525 35.271 4.239
2024-12-01-20:51:30-root-INFO: grad norm: 34.827 34.597 3.999
2024-12-01-20:51:31-root-INFO: Loss Change: 260.748 -> 244.251
2024-12-01-20:51:31-root-INFO: Regularization Change: 0.000 -> 6.624
2024-12-01-20:51:31-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-01-20:51:31-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-20:51:31-root-INFO: step: 108 lr_xt 0.06173354
2024-12-01-20:51:31-root-INFO: grad norm: 32.434 32.257 3.383
2024-12-01-20:51:31-root-INFO: Loss too large (241.777->253.774)! Learning rate decreased to 0.04939.
2024-12-01-20:51:32-root-INFO: grad norm: 32.644 32.432 3.709
2024-12-01-20:51:32-root-INFO: grad norm: 33.065 32.849 3.770
2024-12-01-20:51:33-root-INFO: grad norm: 33.436 33.214 3.844
2024-12-01-20:51:33-root-INFO: grad norm: 33.716 33.489 3.904
2024-12-01-20:51:33-root-INFO: Loss Change: 241.777 -> 233.643
2024-12-01-20:51:33-root-INFO: Regularization Change: 0.000 -> 4.660
2024-12-01-20:51:33-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-01-20:51:33-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-20:51:34-root-INFO: step: 107 lr_xt 0.06357517
2024-12-01-20:51:34-root-INFO: grad norm: 38.715 38.360 5.232
2024-12-01-20:51:34-root-INFO: Loss too large (238.541->258.868)! Learning rate decreased to 0.05086.
2024-12-01-20:51:34-root-INFO: grad norm: 37.369 37.095 4.515
2024-12-01-20:51:35-root-INFO: grad norm: 35.563 35.306 4.263
2024-12-01-20:51:35-root-INFO: grad norm: 34.332 34.090 4.073
2024-12-01-20:51:36-root-INFO: grad norm: 33.173 32.942 3.907
2024-12-01-20:51:36-root-INFO: Loss Change: 238.541 -> 228.127
2024-12-01-20:51:36-root-INFO: Regularization Change: 0.000 -> 4.831
2024-12-01-20:51:36-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-01-20:51:36-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-20:51:36-root-INFO: step: 106 lr_xt 0.06546120
2024-12-01-20:51:37-root-INFO: grad norm: 30.084 29.926 3.076
2024-12-01-20:51:37-root-INFO: Loss too large (225.666->237.956)! Learning rate decreased to 0.05237.
2024-12-01-20:51:37-root-INFO: grad norm: 30.261 30.050 3.568
2024-12-01-20:51:38-root-INFO: grad norm: 30.556 30.355 3.499
2024-12-01-20:51:38-root-INFO: grad norm: 30.867 30.643 3.706
2024-12-01-20:51:39-root-INFO: grad norm: 31.063 30.843 3.687
2024-12-01-20:51:39-root-INFO: Loss Change: 225.666 -> 220.833
2024-12-01-20:51:39-root-INFO: Regularization Change: 0.000 -> 3.846
2024-12-01-20:51:39-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-01-20:51:39-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-20:51:39-root-INFO: step: 105 lr_xt 0.06739236
2024-12-01-20:51:39-root-INFO: grad norm: 34.595 34.272 4.715
2024-12-01-20:51:39-root-INFO: Loss too large (224.309->242.241)! Learning rate decreased to 0.05391.
2024-12-01-20:51:40-root-INFO: grad norm: 33.791 33.539 4.121
2024-12-01-20:51:40-root-INFO: grad norm: 32.612 32.359 4.049
2024-12-01-20:51:41-root-INFO: grad norm: 31.613 31.379 3.838
2024-12-01-20:51:41-root-INFO: grad norm: 30.631 30.400 3.752
2024-12-01-20:51:42-root-INFO: Loss Change: 224.309 -> 217.001
2024-12-01-20:51:42-root-INFO: Regularization Change: 0.000 -> 4.219
2024-12-01-20:51:42-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-01-20:51:42-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-20:51:42-root-INFO: step: 104 lr_xt 0.06936934
2024-12-01-20:51:42-root-INFO: grad norm: 27.308 27.166 2.779
2024-12-01-20:51:42-root-INFO: Loss too large (214.687->225.044)! Learning rate decreased to 0.05550.
2024-12-01-20:51:43-root-INFO: grad norm: 26.946 26.759 3.172
2024-12-01-20:51:43-root-INFO: grad norm: 26.860 26.690 3.023
2024-12-01-20:51:44-root-INFO: grad norm: 26.855 26.663 3.213
2024-12-01-20:51:44-root-INFO: grad norm: 26.905 26.722 3.131
2024-12-01-20:51:44-root-INFO: Loss Change: 214.687 -> 209.770
2024-12-01-20:51:44-root-INFO: Regularization Change: 0.000 -> 3.367
2024-12-01-20:51:44-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-01-20:51:44-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-20:51:45-root-INFO: step: 103 lr_xt 0.07139284
2024-12-01-20:51:45-root-INFO: grad norm: 30.119 29.838 4.105
2024-12-01-20:51:45-root-INFO: Loss too large (212.707->226.697)! Learning rate decreased to 0.05711.
2024-12-01-20:51:45-root-INFO: grad norm: 29.295 29.067 3.651
2024-12-01-20:51:46-root-INFO: grad norm: 28.326 28.096 3.597
2024-12-01-20:51:46-root-INFO: grad norm: 27.517 27.302 3.432
2024-12-01-20:51:47-root-INFO: grad norm: 26.776 26.564 3.364
2024-12-01-20:51:47-root-INFO: Loss Change: 212.707 -> 206.471
2024-12-01-20:51:47-root-INFO: Regularization Change: 0.000 -> 3.658
2024-12-01-20:51:47-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-01-20:51:47-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-20:51:47-root-INFO: step: 102 lr_xt 0.07346356
2024-12-01-20:51:47-root-INFO: grad norm: 24.651 24.499 2.729
2024-12-01-20:51:48-root-INFO: Loss too large (204.978->214.334)! Learning rate decreased to 0.05877.
2024-12-01-20:51:48-root-INFO: grad norm: 24.662 24.473 3.051
2024-12-01-20:51:49-root-INFO: grad norm: 24.798 24.622 2.950
2024-12-01-20:51:49-root-INFO: grad norm: 24.956 24.762 3.099
2024-12-01-20:51:50-root-INFO: grad norm: 25.102 24.916 3.050
2024-12-01-20:51:50-root-INFO: Loss Change: 204.978 -> 201.423
2024-12-01-20:51:50-root-INFO: Regularization Change: 0.000 -> 3.154
2024-12-01-20:51:50-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-01-20:51:50-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-20:51:50-root-INFO: step: 101 lr_xt 0.07558219
2024-12-01-20:51:50-root-INFO: grad norm: 28.388 28.106 3.990
2024-12-01-20:51:50-root-INFO: Loss too large (204.323->217.432)! Learning rate decreased to 0.06047.
2024-12-01-20:51:51-root-INFO: grad norm: 27.566 27.348 3.458
2024-12-01-20:51:51-root-INFO: grad norm: 26.590 26.364 3.461
2024-12-01-20:51:52-root-INFO: grad norm: 25.798 25.593 3.244
2024-12-01-20:51:52-root-INFO: grad norm: 25.062 24.855 3.218
2024-12-01-20:51:53-root-INFO: Loss Change: 204.323 -> 198.364
2024-12-01-20:51:53-root-INFO: Regularization Change: 0.000 -> 3.588
2024-12-01-20:51:53-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-01-20:51:53-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-20:51:53-root-INFO: step: 100 lr_xt 0.07774943
2024-12-01-20:51:53-root-INFO: grad norm: 22.173 22.040 2.429
2024-12-01-20:51:53-root-INFO: Loss too large (196.641->204.503)! Learning rate decreased to 0.06220.
2024-12-01-20:51:53-root-INFO: grad norm: 22.001 21.826 2.770
2024-12-01-20:51:54-root-INFO: grad norm: 21.972 21.817 2.610
2024-12-01-20:51:54-root-INFO: grad norm: 21.992 21.817 2.771
2024-12-01-20:51:55-root-INFO: grad norm: 22.059 21.897 2.668
2024-12-01-20:51:55-root-INFO: Loss Change: 196.641 -> 193.046
2024-12-01-20:51:55-root-INFO: Regularization Change: 0.000 -> 2.875
2024-12-01-20:51:55-root-INFO: Undo step: 100
2024-12-01-20:51:55-root-INFO: Undo step: 101
2024-12-01-20:51:55-root-INFO: Undo step: 102
2024-12-01-20:51:55-root-INFO: Undo step: 103
2024-12-01-20:51:55-root-INFO: Undo step: 104
2024-12-01-20:51:55-root-INFO: step: 105 lr_xt 0.06739236
2024-12-01-20:51:56-root-INFO: grad norm: 68.504 67.030 14.131
2024-12-01-20:51:56-root-INFO: grad norm: 38.713 38.133 6.677
2024-12-01-20:51:57-root-INFO: grad norm: 36.223 35.893 4.872
2024-12-01-20:51:57-root-INFO: grad norm: 41.847 41.519 5.225
2024-12-01-20:51:57-root-INFO: Loss too large (262.436->266.353)! Learning rate decreased to 0.05391.
2024-12-01-20:51:58-root-INFO: grad norm: 34.199 33.912 4.420
2024-12-01-20:51:58-root-INFO: Loss Change: 439.334 -> 235.637
2024-12-01-20:51:58-root-INFO: Regularization Change: 0.000 -> 46.477
2024-12-01-20:51:58-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-01-20:51:58-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-20:51:58-root-INFO: step: 104 lr_xt 0.06936934
2024-12-01-20:51:58-root-INFO: grad norm: 26.639 26.375 3.741
2024-12-01-20:51:59-root-INFO: Loss too large (233.223->235.191)! Learning rate decreased to 0.05550.
2024-12-01-20:51:59-root-INFO: grad norm: 23.981 23.735 3.429
2024-12-01-20:52:00-root-INFO: grad norm: 22.375 22.119 3.379
2024-12-01-20:52:00-root-INFO: grad norm: 21.504 21.285 3.064
2024-12-01-20:52:01-root-INFO: grad norm: 21.048 20.819 3.100
2024-12-01-20:52:01-root-INFO: Loss Change: 233.223 -> 212.727
2024-12-01-20:52:01-root-INFO: Regularization Change: 0.000 -> 6.326
2024-12-01-20:52:01-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-01-20:52:01-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-20:52:01-root-INFO: step: 103 lr_xt 0.07139284
2024-12-01-20:52:01-root-INFO: grad norm: 23.999 23.738 3.530
2024-12-01-20:52:01-root-INFO: Loss too large (214.732->221.156)! Learning rate decreased to 0.05711.
2024-12-01-20:52:02-root-INFO: grad norm: 23.713 23.476 3.344
2024-12-01-20:52:02-root-INFO: grad norm: 23.607 23.393 3.166
2024-12-01-20:52:03-root-INFO: grad norm: 23.575 23.357 3.199
2024-12-01-20:52:03-root-INFO: grad norm: 23.677 23.474 3.090
2024-12-01-20:52:03-root-INFO: Loss Change: 214.732 -> 206.086
2024-12-01-20:52:03-root-INFO: Regularization Change: 0.000 -> 4.250
2024-12-01-20:52:04-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-01-20:52:04-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-20:52:04-root-INFO: step: 102 lr_xt 0.07346356
2024-12-01-20:52:04-root-INFO: grad norm: 21.856 21.705 2.565
2024-12-01-20:52:04-root-INFO: Loss too large (204.336->211.261)! Learning rate decreased to 0.05877.
2024-12-01-20:52:04-root-INFO: grad norm: 22.331 22.144 2.881
2024-12-01-20:52:05-root-INFO: grad norm: 22.841 22.658 2.883
2024-12-01-20:52:05-root-INFO: grad norm: 23.393 23.198 3.018
2024-12-01-20:52:06-root-INFO: grad norm: 23.828 23.630 3.065
2024-12-01-20:52:06-root-INFO: Loss Change: 204.336 -> 200.099
2024-12-01-20:52:06-root-INFO: Regularization Change: 0.000 -> 3.494
2024-12-01-20:52:06-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-01-20:52:06-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-20:52:06-root-INFO: step: 101 lr_xt 0.07558219
2024-12-01-20:52:06-root-INFO: grad norm: 27.696 27.407 3.987
2024-12-01-20:52:07-root-INFO: Loss too large (203.049->214.976)! Learning rate decreased to 0.06047.
2024-12-01-20:52:07-root-INFO: grad norm: 27.156 26.922 3.561
2024-12-01-20:52:08-root-INFO: grad norm: 26.511 26.271 3.562
2024-12-01-20:52:08-root-INFO: grad norm: 25.909 25.676 3.470
2024-12-01-20:52:09-root-INFO: grad norm: 25.442 25.203 3.479
2024-12-01-20:52:09-root-INFO: Loss Change: 203.049 -> 196.739
2024-12-01-20:52:09-root-INFO: Regularization Change: 0.000 -> 3.947
2024-12-01-20:52:09-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-01-20:52:09-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-20:52:09-root-INFO: step: 100 lr_xt 0.07774943
2024-12-01-20:52:09-root-INFO: grad norm: 22.394 22.230 2.708
2024-12-01-20:52:09-root-INFO: Loss too large (194.608->202.754)! Learning rate decreased to 0.06220.
2024-12-01-20:52:10-root-INFO: grad norm: 22.571 22.341 3.214
2024-12-01-20:52:10-root-INFO: grad norm: 22.795 22.571 3.186
2024-12-01-20:52:11-root-INFO: grad norm: 23.141 22.883 3.447
2024-12-01-20:52:11-root-INFO: grad norm: 23.369 23.116 3.427
2024-12-01-20:52:12-root-INFO: Loss Change: 194.608 -> 191.153
2024-12-01-20:52:12-root-INFO: Regularization Change: 0.000 -> 3.217
2024-12-01-20:52:12-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-01-20:52:12-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-20:52:12-root-INFO: step: 99 lr_xt 0.07996596
2024-12-01-20:52:12-root-INFO: grad norm: 26.520 26.169 4.299
2024-12-01-20:52:12-root-INFO: Loss too large (193.626->205.536)! Learning rate decreased to 0.06397.
2024-12-01-20:52:13-root-INFO: grad norm: 25.746 25.455 3.866
2024-12-01-20:52:13-root-INFO: grad norm: 24.740 24.448 3.789
2024-12-01-20:52:13-root-INFO: grad norm: 23.872 23.597 3.614
2024-12-01-20:52:14-root-INFO: grad norm: 23.002 22.736 3.491
2024-12-01-20:52:14-root-INFO: Loss Change: 193.626 -> 187.235
2024-12-01-20:52:14-root-INFO: Regularization Change: 0.000 -> 3.624
2024-12-01-20:52:14-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-01-20:52:14-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-20:52:14-root-INFO: step: 98 lr_xt 0.08223248
2024-12-01-20:52:15-root-INFO: grad norm: 19.705 19.519 2.701
2024-12-01-20:52:15-root-INFO: Loss too large (185.333->191.701)! Learning rate decreased to 0.06579.
2024-12-01-20:52:15-root-INFO: grad norm: 19.553 19.322 2.998
2024-12-01-20:52:16-root-INFO: grad norm: 19.547 19.329 2.911
2024-12-01-20:52:16-root-INFO: grad norm: 19.620 19.389 2.998
2024-12-01-20:52:17-root-INFO: grad norm: 19.733 19.510 2.959
2024-12-01-20:52:17-root-INFO: Loss Change: 185.333 -> 181.786
2024-12-01-20:52:17-root-INFO: Regularization Change: 0.000 -> 2.773
2024-12-01-20:52:17-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-01-20:52:17-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-20:52:17-root-INFO: step: 97 lr_xt 0.08454965
2024-12-01-20:52:17-root-INFO: grad norm: 23.359 23.036 3.871
2024-12-01-20:52:17-root-INFO: Loss too large (184.392->194.246)! Learning rate decreased to 0.06764.
2024-12-01-20:52:18-root-INFO: grad norm: 22.894 22.642 3.391
2024-12-01-20:52:18-root-INFO: grad norm: 22.274 22.025 3.323
2024-12-01-20:52:19-root-INFO: grad norm: 21.701 21.465 3.191
2024-12-01-20:52:19-root-INFO: grad norm: 21.133 20.902 3.116
2024-12-01-20:52:20-root-INFO: Loss Change: 184.392 -> 179.505
2024-12-01-20:52:20-root-INFO: Regularization Change: 0.000 -> 3.295
2024-12-01-20:52:20-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-01-20:52:20-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-20:52:20-root-INFO: step: 96 lr_xt 0.08691815
2024-12-01-20:52:20-root-INFO: grad norm: 18.553 18.387 2.480
2024-12-01-20:52:20-root-INFO: Loss too large (178.072->184.071)! Learning rate decreased to 0.06953.
2024-12-01-20:52:21-root-INFO: grad norm: 18.305 18.106 2.694
2024-12-01-20:52:21-root-INFO: grad norm: 18.156 17.969 2.600
2024-12-01-20:52:21-root-INFO: grad norm: 18.088 17.891 2.665
2024-12-01-20:52:22-root-INFO: grad norm: 18.072 17.882 2.610
2024-12-01-20:52:22-root-INFO: Loss Change: 178.072 -> 174.537
2024-12-01-20:52:22-root-INFO: Regularization Change: 0.000 -> 2.657
2024-12-01-20:52:22-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-01-20:52:22-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-20:52:22-root-INFO: step: 95 lr_xt 0.08933865
2024-12-01-20:52:22-root-INFO: grad norm: 21.353 21.069 3.473
2024-12-01-20:52:23-root-INFO: Loss too large (177.125->185.724)! Learning rate decreased to 0.07147.
2024-12-01-20:52:23-root-INFO: grad norm: 20.897 20.680 3.003
2024-12-01-20:52:24-root-INFO: grad norm: 20.322 20.100 2.994
2024-12-01-20:52:24-root-INFO: grad norm: 19.788 19.585 2.827
2024-12-01-20:52:24-root-INFO: grad norm: 19.277 19.072 2.803
2024-12-01-20:52:25-root-INFO: Loss Change: 177.125 -> 172.572
2024-12-01-20:52:25-root-INFO: Regularization Change: 0.000 -> 3.113
2024-12-01-20:52:25-root-INFO: Undo step: 95
2024-12-01-20:52:25-root-INFO: Undo step: 96
2024-12-01-20:52:25-root-INFO: Undo step: 97
2024-12-01-20:52:25-root-INFO: Undo step: 98
2024-12-01-20:52:25-root-INFO: Undo step: 99
2024-12-01-20:52:25-root-INFO: step: 100 lr_xt 0.07774943
2024-12-01-20:52:25-root-INFO: grad norm: 61.667 60.559 11.634
2024-12-01-20:52:26-root-INFO: grad norm: 38.609 38.109 6.194
2024-12-01-20:52:26-root-INFO: grad norm: 37.617 37.360 4.385
2024-12-01-20:52:27-root-INFO: grad norm: 38.255 38.011 4.308
2024-12-01-20:52:27-root-INFO: grad norm: 37.684 37.427 4.392
2024-12-01-20:52:27-root-INFO: Loss Change: 394.540 -> 227.324
2024-12-01-20:52:27-root-INFO: Regularization Change: 0.000 -> 53.103
2024-12-01-20:52:27-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-01-20:52:27-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-20:52:28-root-INFO: step: 99 lr_xt 0.07996596
2024-12-01-20:52:28-root-INFO: grad norm: 40.522 40.137 5.570
2024-12-01-20:52:28-root-INFO: Loss too large (230.021->231.435)! Learning rate decreased to 0.06397.
2024-12-01-20:52:28-root-INFO: grad norm: 28.646 28.294 4.480
2024-12-01-20:52:29-root-INFO: grad norm: 22.149 21.895 3.345
2024-12-01-20:52:29-root-INFO: grad norm: 18.979 18.691 3.295
2024-12-01-20:52:30-root-INFO: grad norm: 16.781 16.567 2.674
2024-12-01-20:52:30-root-INFO: Loss Change: 230.021 -> 189.187
2024-12-01-20:52:30-root-INFO: Regularization Change: 0.000 -> 8.029
2024-12-01-20:52:30-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-01-20:52:30-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-20:52:30-root-INFO: step: 98 lr_xt 0.08223248
2024-12-01-20:52:30-root-INFO: grad norm: 13.457 13.264 2.274
2024-12-01-20:52:30-root-INFO: Loss too large (188.113->188.353)! Learning rate decreased to 0.06579.
2024-12-01-20:52:31-root-INFO: grad norm: 12.568 12.379 2.167
2024-12-01-20:52:31-root-INFO: grad norm: 12.108 11.915 2.150
2024-12-01-20:52:32-root-INFO: grad norm: 11.880 11.710 2.000
2024-12-01-20:52:32-root-INFO: grad norm: 11.861 11.681 2.060
2024-12-01-20:52:33-root-INFO: Loss Change: 188.113 -> 180.012
2024-12-01-20:52:33-root-INFO: Regularization Change: 0.000 -> 3.078
2024-12-01-20:52:33-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-01-20:52:33-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-20:52:33-root-INFO: step: 97 lr_xt 0.08454965
2024-12-01-20:52:33-root-INFO: grad norm: 15.505 15.248 2.808
2024-12-01-20:52:33-root-INFO: Loss too large (181.388->184.653)! Learning rate decreased to 0.06764.
2024-12-01-20:52:33-root-INFO: grad norm: 15.602 15.395 2.534
2024-12-01-20:52:34-root-INFO: grad norm: 15.868 15.676 2.463
2024-12-01-20:52:34-root-INFO: grad norm: 16.236 16.036 2.543
2024-12-01-20:52:35-root-INFO: grad norm: 16.648 16.458 2.506
2024-12-01-20:52:35-root-INFO: Loss Change: 181.388 -> 177.109
2024-12-01-20:52:35-root-INFO: Regularization Change: 0.000 -> 2.902
2024-12-01-20:52:35-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-01-20:52:35-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-20:52:35-root-INFO: step: 96 lr_xt 0.08691815
2024-12-01-20:52:35-root-INFO: grad norm: 15.027 14.875 2.132
2024-12-01-20:52:36-root-INFO: Loss too large (175.950->179.637)! Learning rate decreased to 0.06953.
2024-12-01-20:52:36-root-INFO: grad norm: 15.438 15.258 2.350
2024-12-01-20:52:37-root-INFO: grad norm: 15.916 15.741 2.351
2024-12-01-20:52:37-root-INFO: grad norm: 16.428 16.244 2.456
2024-12-01-20:52:38-root-INFO: grad norm: 16.924 16.736 2.514
2024-12-01-20:52:38-root-INFO: Loss Change: 175.950 -> 173.032
2024-12-01-20:52:38-root-INFO: Regularization Change: 0.000 -> 2.667
2024-12-01-20:52:38-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-01-20:52:38-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-20:52:38-root-INFO: step: 95 lr_xt 0.08933865
2024-12-01-20:52:38-root-INFO: grad norm: 20.656 20.381 3.355
2024-12-01-20:52:39-root-INFO: Loss too large (175.449->183.660)! Learning rate decreased to 0.07147.
2024-12-01-20:52:39-root-INFO: grad norm: 20.629 20.402 3.050
2024-12-01-20:52:40-root-INFO: grad norm: 20.418 20.193 3.026
2024-12-01-20:52:40-root-INFO: grad norm: 20.117 19.894 2.982
2024-12-01-20:52:41-root-INFO: grad norm: 19.773 19.558 2.907
2024-12-01-20:52:41-root-INFO: Loss Change: 175.449 -> 171.244
2024-12-01-20:52:41-root-INFO: Regularization Change: 0.000 -> 3.354
2024-12-01-20:52:41-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-01-20:52:41-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-20:52:41-root-INFO: step: 94 lr_xt 0.09181181
2024-12-01-20:52:41-root-INFO: grad norm: 17.180 17.027 2.286
2024-12-01-20:52:41-root-INFO: Loss too large (169.373->174.958)! Learning rate decreased to 0.07345.
2024-12-01-20:52:42-root-INFO: grad norm: 17.263 17.077 2.528
2024-12-01-20:52:42-root-INFO: grad norm: 17.336 17.156 2.488
2024-12-01-20:52:43-root-INFO: grad norm: 17.455 17.266 2.559
2024-12-01-20:52:43-root-INFO: grad norm: 17.546 17.357 2.568
2024-12-01-20:52:44-root-INFO: Loss Change: 169.373 -> 166.277
2024-12-01-20:52:44-root-INFO: Regularization Change: 0.000 -> 2.811
2024-12-01-20:52:44-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-01-20:52:44-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-20:52:44-root-INFO: step: 93 lr_xt 0.09433829
2024-12-01-20:52:44-root-INFO: grad norm: 20.715 20.449 3.307
2024-12-01-20:52:44-root-INFO: Loss too large (168.438->176.883)! Learning rate decreased to 0.07547.
2024-12-01-20:52:45-root-INFO: grad norm: 20.231 20.014 2.955
2024-12-01-20:52:45-root-INFO: grad norm: 19.634 19.425 2.856
2024-12-01-20:52:46-root-INFO: grad norm: 19.060 18.857 2.779
2024-12-01-20:52:46-root-INFO: grad norm: 18.498 18.304 2.673
2024-12-01-20:52:46-root-INFO: Loss Change: 168.438 -> 163.615
2024-12-01-20:52:46-root-INFO: Regularization Change: 0.000 -> 3.323
2024-12-01-20:52:46-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-01-20:52:46-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-20:52:47-root-INFO: step: 92 lr_xt 0.09691873
2024-12-01-20:52:47-root-INFO: grad norm: 16.078 15.929 2.180
2024-12-01-20:52:47-root-INFO: Loss too large (162.309->167.274)! Learning rate decreased to 0.07753.
2024-12-01-20:52:47-root-INFO: grad norm: 15.969 15.797 2.339
2024-12-01-20:52:48-root-INFO: grad norm: 15.939 15.772 2.303
2024-12-01-20:52:48-root-INFO: grad norm: 15.971 15.800 2.329
2024-12-01-20:52:49-root-INFO: grad norm: 16.027 15.855 2.339
2024-12-01-20:52:49-root-INFO: Loss Change: 162.309 -> 159.222
2024-12-01-20:52:49-root-INFO: Regularization Change: 0.000 -> 2.659
2024-12-01-20:52:49-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-01-20:52:49-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-20:52:49-root-INFO: step: 91 lr_xt 0.09955376
2024-12-01-20:52:49-root-INFO: grad norm: 18.771 18.534 2.972
2024-12-01-20:52:50-root-INFO: Loss too large (161.075->168.667)! Learning rate decreased to 0.07964.
2024-12-01-20:52:50-root-INFO: grad norm: 18.505 18.302 2.734
2024-12-01-20:52:51-root-INFO: grad norm: 18.111 17.915 2.659
2024-12-01-20:52:51-root-INFO: grad norm: 17.725 17.535 2.592
2024-12-01-20:52:52-root-INFO: grad norm: 17.334 17.151 2.516
2024-12-01-20:52:52-root-INFO: Loss Change: 161.075 -> 157.344
2024-12-01-20:52:52-root-INFO: Regularization Change: 0.000 -> 3.114
2024-12-01-20:52:52-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-01-20:52:52-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-20:52:52-root-INFO: step: 90 lr_xt 0.10224402
2024-12-01-20:52:52-root-INFO: grad norm: 14.941 14.817 1.918
2024-12-01-20:52:52-root-INFO: Loss too large (156.000->160.414)! Learning rate decreased to 0.08180.
2024-12-01-20:52:53-root-INFO: grad norm: 14.688 14.535 2.117
2024-12-01-20:52:53-root-INFO: grad norm: 14.546 14.405 2.022
2024-12-01-20:52:54-root-INFO: grad norm: 14.480 14.328 2.090
2024-12-01-20:52:54-root-INFO: grad norm: 14.455 14.310 2.039
2024-12-01-20:52:55-root-INFO: Loss Change: 156.000 -> 152.908
2024-12-01-20:52:55-root-INFO: Regularization Change: 0.000 -> 2.510
2024-12-01-20:52:55-root-INFO: Undo step: 90
2024-12-01-20:52:55-root-INFO: Undo step: 91
2024-12-01-20:52:55-root-INFO: Undo step: 92
2024-12-01-20:52:55-root-INFO: Undo step: 93
2024-12-01-20:52:55-root-INFO: Undo step: 94
2024-12-01-20:52:55-root-INFO: step: 95 lr_xt 0.08933865
2024-12-01-20:52:55-root-INFO: grad norm: 49.336 48.313 9.996
2024-12-01-20:52:55-root-INFO: grad norm: 32.304 31.799 5.686
2024-12-01-20:52:56-root-INFO: grad norm: 31.616 31.238 4.874
2024-12-01-20:52:56-root-INFO: grad norm: 33.335 33.028 4.519
2024-12-01-20:52:57-root-INFO: grad norm: 35.861 35.532 4.848
2024-12-01-20:52:57-root-INFO: Loss Change: 344.948 -> 210.550
2024-12-01-20:52:57-root-INFO: Regularization Change: 0.000 -> 55.784
2024-12-01-20:52:57-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-01-20:52:57-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-20:52:57-root-INFO: step: 94 lr_xt 0.09181181
2024-12-01-20:52:57-root-INFO: grad norm: 35.563 35.211 4.993
2024-12-01-20:52:58-root-INFO: grad norm: 36.913 36.537 5.250
2024-12-01-20:52:58-root-INFO: Loss too large (205.390->206.269)! Learning rate decreased to 0.07345.
2024-12-01-20:52:58-root-INFO: grad norm: 25.512 25.114 4.490
2024-12-01-20:52:59-root-INFO: grad norm: 19.402 19.192 2.848
2024-12-01-20:52:59-root-INFO: grad norm: 16.651 16.374 3.024
2024-12-01-20:53:00-root-INFO: Loss Change: 208.202 -> 169.854
2024-12-01-20:53:00-root-INFO: Regularization Change: 0.000 -> 9.164
2024-12-01-20:53:00-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-01-20:53:00-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-20:53:00-root-INFO: step: 93 lr_xt 0.09433829
2024-12-01-20:53:00-root-INFO: grad norm: 18.153 17.903 3.004
2024-12-01-20:53:00-root-INFO: Loss too large (171.589->175.283)! Learning rate decreased to 0.07547.
2024-12-01-20:53:01-root-INFO: grad norm: 16.667 16.409 2.922
2024-12-01-20:53:01-root-INFO: grad norm: 15.583 15.387 2.464
2024-12-01-20:53:01-root-INFO: grad norm: 14.966 14.739 2.598
2024-12-01-20:53:02-root-INFO: grad norm: 14.518 14.328 2.337
2024-12-01-20:53:02-root-INFO: Loss Change: 171.589 -> 162.617
2024-12-01-20:53:02-root-INFO: Regularization Change: 0.000 -> 3.952
2024-12-01-20:53:02-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-01-20:53:02-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-20:53:02-root-INFO: step: 92 lr_xt 0.09691873
2024-12-01-20:53:03-root-INFO: grad norm: 12.405 12.226 2.097
2024-12-01-20:53:03-root-INFO: Loss too large (161.622->163.514)! Learning rate decreased to 0.07753.
2024-12-01-20:53:03-root-INFO: grad norm: 12.304 12.136 2.029
2024-12-01-20:53:04-root-INFO: grad norm: 12.359 12.170 2.151
2024-12-01-20:53:04-root-INFO: grad norm: 12.507 12.337 2.054
2024-12-01-20:53:05-root-INFO: grad norm: 12.709 12.519 2.188
2024-12-01-20:53:05-root-INFO: Loss Change: 161.622 -> 157.179
2024-12-01-20:53:05-root-INFO: Regularization Change: 0.000 -> 2.706
2024-12-01-20:53:05-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-01-20:53:05-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-20:53:05-root-INFO: step: 91 lr_xt 0.09955376
2024-12-01-20:53:05-root-INFO: grad norm: 15.760 15.513 2.779
2024-12-01-20:53:05-root-INFO: Loss too large (158.646->163.728)! Learning rate decreased to 0.07964.
2024-12-01-20:53:06-root-INFO: grad norm: 15.860 15.632 2.674
2024-12-01-20:53:06-root-INFO: grad norm: 15.882 15.669 2.590
2024-12-01-20:53:07-root-INFO: grad norm: 15.876 15.653 2.651
2024-12-01-20:53:07-root-INFO: grad norm: 15.842 15.635 2.554
2024-12-01-20:53:07-root-INFO: Loss Change: 158.646 -> 155.288
2024-12-01-20:53:07-root-INFO: Regularization Change: 0.000 -> 2.984
2024-12-01-20:53:07-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-01-20:53:07-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-20:53:08-root-INFO: step: 90 lr_xt 0.10224402
2024-12-01-20:53:08-root-INFO: grad norm: 13.679 13.514 2.117
2024-12-01-20:53:08-root-INFO: Loss too large (153.974->157.488)! Learning rate decreased to 0.08180.
2024-12-01-20:53:08-root-INFO: grad norm: 13.616 13.439 2.186
2024-12-01-20:53:09-root-INFO: grad norm: 13.637 13.453 2.231
2024-12-01-20:53:09-root-INFO: grad norm: 13.721 13.540 2.222
2024-12-01-20:53:10-root-INFO: grad norm: 13.807 13.617 2.277
2024-12-01-20:53:10-root-INFO: Loss Change: 153.974 -> 150.827
2024-12-01-20:53:10-root-INFO: Regularization Change: 0.000 -> 2.525
2024-12-01-20:53:10-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-01-20:53:10-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-20:53:11-root-INFO: step: 89 lr_xt 0.10499012
2024-12-01-20:53:11-root-INFO: grad norm: 17.509 17.231 3.105
2024-12-01-20:53:11-root-INFO: Loss too large (152.927->159.996)! Learning rate decreased to 0.08399.
2024-12-01-20:53:11-root-INFO: grad norm: 17.374 17.129 2.907
2024-12-01-20:53:12-root-INFO: grad norm: 17.094 16.869 2.763
2024-12-01-20:53:12-root-INFO: grad norm: 16.815 16.581 2.796
2024-12-01-20:53:13-root-INFO: grad norm: 16.488 16.274 2.651
2024-12-01-20:53:13-root-INFO: Loss Change: 152.927 -> 149.537
2024-12-01-20:53:13-root-INFO: Regularization Change: 0.000 -> 3.191
2024-12-01-20:53:13-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-01-20:53:13-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-20:53:14-root-INFO: step: 88 lr_xt 0.10779268
2024-12-01-20:53:14-root-INFO: grad norm: 14.307 14.141 2.174
2024-12-01-20:53:14-root-INFO: Loss too large (148.493->152.605)! Learning rate decreased to 0.08623.
2024-12-01-20:53:14-root-INFO: grad norm: 13.984 13.804 2.235
2024-12-01-20:53:15-root-INFO: grad norm: 13.754 13.572 2.230
2024-12-01-20:53:15-root-INFO: grad norm: 13.576 13.396 2.207
2024-12-01-20:53:16-root-INFO: grad norm: 13.434 13.252 2.206
2024-12-01-20:53:16-root-INFO: Loss Change: 148.493 -> 145.050
2024-12-01-20:53:16-root-INFO: Regularization Change: 0.000 -> 2.545
2024-12-01-20:53:16-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-01-20:53:16-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-20:53:16-root-INFO: step: 87 lr_xt 0.11065228
2024-12-01-20:53:16-root-INFO: grad norm: 16.481 16.230 2.864
2024-12-01-20:53:17-root-INFO: Loss too large (146.890->153.104)! Learning rate decreased to 0.08852.
2024-12-01-20:53:17-root-INFO: grad norm: 15.928 15.715 2.595
2024-12-01-20:53:18-root-INFO: grad norm: 15.298 15.100 2.454
2024-12-01-20:53:18-root-INFO: grad norm: 14.841 14.641 2.427
2024-12-01-20:53:18-root-INFO: grad norm: 14.413 14.222 2.333
2024-12-01-20:53:19-root-INFO: Loss Change: 146.890 -> 143.003
2024-12-01-20:53:19-root-INFO: Regularization Change: 0.000 -> 2.937
2024-12-01-20:53:19-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-01-20:53:19-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-20:53:19-root-INFO: step: 86 lr_xt 0.11356952
2024-12-01-20:53:19-root-INFO: grad norm: 11.946 11.813 1.779
2024-12-01-20:53:19-root-INFO: Loss too large (141.713->144.666)! Learning rate decreased to 0.09086.
2024-12-01-20:53:20-root-INFO: grad norm: 11.762 11.606 1.906
2024-12-01-20:53:20-root-INFO: grad norm: 11.677 11.523 1.891
2024-12-01-20:53:21-root-INFO: grad norm: 11.682 11.523 1.920
2024-12-01-20:53:21-root-INFO: grad norm: 11.678 11.519 1.923
2024-12-01-20:53:21-root-INFO: Loss Change: 141.713 -> 139.028
2024-12-01-20:53:21-root-INFO: Regularization Change: 0.000 -> 2.210
2024-12-01-20:53:21-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-01-20:53:21-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-20:53:22-root-INFO: step: 85 lr_xt 0.11654496
2024-12-01-20:53:22-root-INFO: grad norm: 14.077 13.853 2.497
2024-12-01-20:53:22-root-INFO: Loss too large (140.363->145.245)! Learning rate decreased to 0.09324.
2024-12-01-20:53:22-root-INFO: grad norm: 13.800 13.609 2.289
2024-12-01-20:53:23-root-INFO: grad norm: 13.461 13.273 2.242
2024-12-01-20:53:23-root-INFO: grad norm: 13.263 13.074 2.235
2024-12-01-20:53:24-root-INFO: grad norm: 13.126 12.937 2.219
2024-12-01-20:53:24-root-INFO: Loss Change: 140.363 -> 137.608
2024-12-01-20:53:24-root-INFO: Regularization Change: 0.000 -> 2.646
2024-12-01-20:53:24-root-INFO: Undo step: 85
2024-12-01-20:53:24-root-INFO: Undo step: 86
2024-12-01-20:53:24-root-INFO: Undo step: 87
2024-12-01-20:53:24-root-INFO: Undo step: 88
2024-12-01-20:53:24-root-INFO: Undo step: 89
2024-12-01-20:53:24-root-INFO: step: 90 lr_xt 0.10224402
2024-12-01-20:53:25-root-INFO: grad norm: 47.482 46.815 7.930
2024-12-01-20:53:25-root-INFO: grad norm: 30.702 30.300 4.950
2024-12-01-20:53:25-root-INFO: grad norm: 29.208 28.845 4.594
2024-12-01-20:53:26-root-INFO: grad norm: 34.750 34.359 5.199
2024-12-01-20:53:26-root-INFO: Loss too large (196.816->202.792)! Learning rate decreased to 0.08180.
2024-12-01-20:53:27-root-INFO: grad norm: 26.483 26.105 4.457
2024-12-01-20:53:27-root-INFO: Loss Change: 323.453 -> 169.446
2024-12-01-20:53:27-root-INFO: Regularization Change: 0.000 -> 54.483
2024-12-01-20:53:27-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-01-20:53:27-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-20:53:27-root-INFO: step: 89 lr_xt 0.10499012
2024-12-01-20:53:27-root-INFO: grad norm: 22.713 22.380 3.871
2024-12-01-20:53:27-root-INFO: Loss too large (171.809->175.620)! Learning rate decreased to 0.08399.
2024-12-01-20:53:28-root-INFO: grad norm: 19.479 19.195 3.315
2024-12-01-20:53:28-root-INFO: grad norm: 17.205 16.974 2.812
2024-12-01-20:53:29-root-INFO: grad norm: 16.013 15.789 2.666
2024-12-01-20:53:29-root-INFO: grad norm: 15.120 14.916 2.474
2024-12-01-20:53:30-root-INFO: Loss Change: 171.809 -> 153.670
2024-12-01-20:53:30-root-INFO: Regularization Change: 0.000 -> 7.476
2024-12-01-20:53:30-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-01-20:53:30-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-20:53:30-root-INFO: step: 88 lr_xt 0.10779268
2024-12-01-20:53:30-root-INFO: grad norm: 13.037 12.893 1.938
2024-12-01-20:53:30-root-INFO: Loss too large (152.827->154.504)! Learning rate decreased to 0.08623.
2024-12-01-20:53:31-root-INFO: grad norm: 12.638 12.472 2.038
2024-12-01-20:53:31-root-INFO: grad norm: 12.489 12.340 1.928
2024-12-01-20:53:32-root-INFO: grad norm: 12.434 12.274 1.988
2024-12-01-20:53:32-root-INFO: grad norm: 12.496 12.347 1.922
2024-12-01-20:53:32-root-INFO: Loss Change: 152.827 -> 146.383
2024-12-01-20:53:32-root-INFO: Regularization Change: 0.000 -> 3.743
2024-12-01-20:53:32-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-01-20:53:32-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-20:53:32-root-INFO: step: 87 lr_xt 0.11065228
2024-12-01-20:53:33-root-INFO: grad norm: 15.842 15.587 2.830
2024-12-01-20:53:33-root-INFO: Loss too large (148.052->153.324)! Learning rate decreased to 0.08852.
2024-12-01-20:53:33-root-INFO: grad norm: 15.512 15.329 2.378
2024-12-01-20:53:34-root-INFO: grad norm: 15.123 14.937 2.366
2024-12-01-20:53:34-root-INFO: grad norm: 14.893 14.722 2.256
2024-12-01-20:53:35-root-INFO: grad norm: 14.633 14.459 2.249
2024-12-01-20:53:35-root-INFO: Loss Change: 148.052 -> 143.305
2024-12-01-20:53:35-root-INFO: Regularization Change: 0.000 -> 3.594
2024-12-01-20:53:35-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-01-20:53:35-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-20:53:35-root-INFO: step: 86 lr_xt 0.11356952
2024-12-01-20:53:35-root-INFO: grad norm: 12.376 12.269 1.618
2024-12-01-20:53:36-root-INFO: Loss too large (141.914->144.885)! Learning rate decreased to 0.09086.
2024-12-01-20:53:36-root-INFO: grad norm: 12.281 12.143 1.840
2024-12-01-20:53:37-root-INFO: grad norm: 12.258 12.132 1.757
2024-12-01-20:53:37-root-INFO: grad norm: 12.300 12.161 1.845
2024-12-01-20:53:38-root-INFO: grad norm: 12.326 12.193 1.807
2024-12-01-20:53:38-root-INFO: Loss Change: 141.914 -> 138.463
2024-12-01-20:53:38-root-INFO: Regularization Change: 0.000 -> 2.739
2024-12-01-20:53:38-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-01-20:53:38-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-20:53:38-root-INFO: step: 85 lr_xt 0.11654496
2024-12-01-20:53:38-root-INFO: grad norm: 14.651 14.438 2.485
2024-12-01-20:53:39-root-INFO: Loss too large (139.792->144.769)! Learning rate decreased to 0.09324.
2024-12-01-20:53:39-root-INFO: grad norm: 14.295 14.133 2.146
2024-12-01-20:53:40-root-INFO: grad norm: 13.865 13.703 2.112
2024-12-01-20:53:40-root-INFO: grad norm: 13.560 13.408 2.026
2024-12-01-20:53:40-root-INFO: grad norm: 13.295 13.143 2.009
2024-12-01-20:53:41-root-INFO: Loss Change: 139.792 -> 136.041
2024-12-01-20:53:41-root-INFO: Regularization Change: 0.000 -> 3.014
2024-12-01-20:53:41-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-01-20:53:41-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-20:53:41-root-INFO: step: 84 lr_xt 0.11957917
2024-12-01-20:53:41-root-INFO: grad norm: 11.605 11.499 1.567
2024-12-01-20:53:41-root-INFO: Loss too large (135.017->137.866)! Learning rate decreased to 0.09566.
2024-12-01-20:53:42-root-INFO: grad norm: 11.406 11.279 1.696
2024-12-01-20:53:42-root-INFO: grad norm: 11.283 11.164 1.638
2024-12-01-20:53:43-root-INFO: grad norm: 11.238 11.113 1.677
2024-12-01-20:53:43-root-INFO: grad norm: 11.176 11.054 1.648
2024-12-01-20:53:44-root-INFO: Loss Change: 135.017 -> 131.994
2024-12-01-20:53:44-root-INFO: Regularization Change: 0.000 -> 2.410
2024-12-01-20:53:44-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-01-20:53:44-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-20:53:44-root-INFO: step: 83 lr_xt 0.12267269
2024-12-01-20:53:44-root-INFO: grad norm: 13.540 13.344 2.296
2024-12-01-20:53:44-root-INFO: Loss too large (133.400->137.854)! Learning rate decreased to 0.09814.
2024-12-01-20:53:45-root-INFO: grad norm: 13.082 12.933 1.968
2024-12-01-20:53:45-root-INFO: grad norm: 12.526 12.386 1.868
2024-12-01-20:53:46-root-INFO: grad norm: 12.188 12.054 1.802
2024-12-01-20:53:46-root-INFO: grad norm: 11.972 11.840 1.770
2024-12-01-20:53:46-root-INFO: Loss Change: 133.400 -> 129.984
2024-12-01-20:53:46-root-INFO: Regularization Change: 0.000 -> 2.730
2024-12-01-20:53:46-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-01-20:53:46-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-20:53:47-root-INFO: step: 82 lr_xt 0.12582604
2024-12-01-20:53:47-root-INFO: grad norm: 10.365 10.276 1.358
2024-12-01-20:53:47-root-INFO: Loss too large (129.311->131.600)! Learning rate decreased to 0.10066.
2024-12-01-20:53:47-root-INFO: grad norm: 10.118 10.014 1.447
2024-12-01-20:53:48-root-INFO: grad norm: 9.969 9.869 1.406
2024-12-01-20:53:48-root-INFO: grad norm: 9.927 9.824 1.431
2024-12-01-20:53:49-root-INFO: grad norm: 9.831 9.730 1.409
2024-12-01-20:53:49-root-INFO: Loss Change: 129.311 -> 126.531
2024-12-01-20:53:49-root-INFO: Regularization Change: 0.000 -> 2.162
2024-12-01-20:53:49-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-01-20:53:49-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-20:53:49-root-INFO: step: 81 lr_xt 0.12903975
2024-12-01-20:53:49-root-INFO: grad norm: 11.924 11.755 2.001
2024-12-01-20:53:50-root-INFO: Loss too large (127.599->131.129)! Learning rate decreased to 0.10323.
2024-12-01-20:53:50-root-INFO: grad norm: 11.433 11.312 1.664
2024-12-01-20:53:50-root-INFO: grad norm: 10.863 10.747 1.585
2024-12-01-20:53:51-root-INFO: grad norm: 10.597 10.490 1.504
2024-12-01-20:53:51-root-INFO: grad norm: 10.540 10.430 1.517
2024-12-01-20:53:52-root-INFO: Loss Change: 127.599 -> 124.621
2024-12-01-20:53:52-root-INFO: Regularization Change: 0.000 -> 2.461
2024-12-01-20:53:52-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-01-20:53:52-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-20:53:52-root-INFO: step: 80 lr_xt 0.13231432
2024-12-01-20:53:52-root-INFO: grad norm: 8.258 8.198 0.987
2024-12-01-20:53:52-root-INFO: Loss too large (123.655->124.865)! Learning rate decreased to 0.10585.
2024-12-01-20:53:53-root-INFO: grad norm: 8.038 7.959 1.121
2024-12-01-20:53:53-root-INFO: grad norm: 8.065 7.992 1.077
2024-12-01-20:53:53-root-INFO: grad norm: 8.280 8.200 1.151
2024-12-01-20:53:54-root-INFO: grad norm: 8.257 8.178 1.140
2024-12-01-20:53:54-root-INFO: Loss Change: 123.655 -> 121.258
2024-12-01-20:53:54-root-INFO: Regularization Change: 0.000 -> 1.878
2024-12-01-20:53:54-root-INFO: Undo step: 80
2024-12-01-20:53:54-root-INFO: Undo step: 81
2024-12-01-20:53:54-root-INFO: Undo step: 82
2024-12-01-20:53:54-root-INFO: Undo step: 83
2024-12-01-20:53:54-root-INFO: Undo step: 84
2024-12-01-20:53:54-root-INFO: step: 85 lr_xt 0.11654496
2024-12-01-20:53:55-root-INFO: grad norm: 40.759 40.238 6.493
2024-12-01-20:53:55-root-INFO: grad norm: 27.776 27.363 4.770
2024-12-01-20:53:56-root-INFO: grad norm: 23.041 22.647 4.241
2024-12-01-20:53:56-root-INFO: grad norm: 19.092 18.877 2.859
2024-12-01-20:53:56-root-INFO: grad norm: 20.827 20.552 3.374
2024-12-01-20:53:57-root-INFO: Loss Change: 296.114 -> 158.090
2024-12-01-20:53:57-root-INFO: Regularization Change: 0.000 -> 65.284
2024-12-01-20:53:57-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-01-20:53:57-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-20:53:57-root-INFO: step: 84 lr_xt 0.11957917
2024-12-01-20:53:57-root-INFO: grad norm: 26.205 25.860 4.243
2024-12-01-20:53:57-root-INFO: Loss too large (159.934->167.286)! Learning rate decreased to 0.09566.
2024-12-01-20:53:58-root-INFO: grad norm: 21.217 20.921 3.535
2024-12-01-20:53:58-root-INFO: grad norm: 17.082 16.875 2.656
2024-12-01-20:53:59-root-INFO: grad norm: 14.929 14.719 2.495
2024-12-01-20:53:59-root-INFO: grad norm: 13.239 13.077 2.062
2024-12-01-20:53:59-root-INFO: Loss Change: 159.934 -> 137.116
2024-12-01-20:53:59-root-INFO: Regularization Change: 0.000 -> 8.094
2024-12-01-20:53:59-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-01-20:53:59-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-20:54:00-root-INFO: step: 83 lr_xt 0.12267269
2024-12-01-20:54:00-root-INFO: grad norm: 10.618 10.507 1.532
2024-12-01-20:54:00-root-INFO: Loss too large (135.966->136.941)! Learning rate decreased to 0.09814.
2024-12-01-20:54:00-root-INFO: grad norm: 10.100 9.979 1.561
2024-12-01-20:54:01-root-INFO: grad norm: 9.651 9.526 1.550
2024-12-01-20:54:01-root-INFO: grad norm: 9.355 9.240 1.459
2024-12-01-20:54:02-root-INFO: grad norm: 9.154 9.033 1.484
2024-12-01-20:54:02-root-INFO: Loss Change: 135.966 -> 129.985
2024-12-01-20:54:02-root-INFO: Regularization Change: 0.000 -> 3.392
2024-12-01-20:54:02-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-01-20:54:02-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-20:54:02-root-INFO: step: 82 lr_xt 0.12582604
2024-12-01-20:54:03-root-INFO: grad norm: 10.929 10.770 1.857
2024-12-01-20:54:03-root-INFO: Loss too large (130.960->133.281)! Learning rate decreased to 0.10066.
2024-12-01-20:54:03-root-INFO: grad norm: 10.386 10.252 1.662
2024-12-01-20:54:04-root-INFO: grad norm: 9.798 9.686 1.480
2024-12-01-20:54:04-root-INFO: grad norm: 9.573 9.461 1.462
2024-12-01-20:54:05-root-INFO: grad norm: 9.624 9.519 1.421
2024-12-01-20:54:05-root-INFO: Loss Change: 130.960 -> 126.876
2024-12-01-20:54:05-root-INFO: Regularization Change: 0.000 -> 2.832
2024-12-01-20:54:05-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-01-20:54:05-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-20:54:05-root-INFO: step: 81 lr_xt 0.12903975
2024-12-01-20:54:05-root-INFO: grad norm: 7.973 7.907 1.028
2024-12-01-20:54:06-root-INFO: Loss too large (125.907->126.736)! Learning rate decreased to 0.10323.
2024-12-01-20:54:06-root-INFO: grad norm: 7.974 7.890 1.149
2024-12-01-20:54:06-root-INFO: grad norm: 8.012 7.928 1.155
2024-12-01-20:54:07-root-INFO: grad norm: 8.102 8.018 1.161
2024-12-01-20:54:07-root-INFO: grad norm: 8.216 8.128 1.193
2024-12-01-20:54:08-root-INFO: Loss Change: 125.907 -> 122.889
2024-12-01-20:54:08-root-INFO: Regularization Change: 0.000 -> 2.194
2024-12-01-20:54:08-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-01-20:54:08-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-20:54:08-root-INFO: step: 80 lr_xt 0.13231432
2024-12-01-20:54:08-root-INFO: grad norm: 11.186 11.030 1.862
2024-12-01-20:54:08-root-INFO: Loss too large (124.310->127.455)! Learning rate decreased to 0.10585.
2024-12-01-20:54:09-root-INFO: grad norm: 10.580 10.468 1.535
2024-12-01-20:54:09-root-INFO: grad norm: 9.980 9.887 1.362
2024-12-01-20:54:10-root-INFO: grad norm: 9.612 9.524 1.298
2024-12-01-20:54:10-root-INFO: grad norm: 9.388 9.304 1.254
2024-12-01-20:54:11-root-INFO: Loss Change: 124.310 -> 120.856
2024-12-01-20:54:11-root-INFO: Regularization Change: 0.000 -> 2.471
2024-12-01-20:54:11-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-01-20:54:11-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-20:54:11-root-INFO: step: 79 lr_xt 0.13565022
2024-12-01-20:54:11-root-INFO: grad norm: 7.957 7.900 0.955
2024-12-01-20:54:11-root-INFO: Loss too large (120.078->121.383)! Learning rate decreased to 0.10852.
2024-12-01-20:54:12-root-INFO: grad norm: 8.227 8.156 1.080
2024-12-01-20:54:12-root-INFO: grad norm: 8.159 8.084 1.109
2024-12-01-20:54:13-root-INFO: grad norm: 7.996 7.923 1.078
2024-12-01-20:54:13-root-INFO: grad norm: 8.134 8.056 1.122
2024-12-01-20:54:13-root-INFO: Loss Change: 120.078 -> 117.762
2024-12-01-20:54:13-root-INFO: Regularization Change: 0.000 -> 2.029
2024-12-01-20:54:13-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-01-20:54:13-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-20:54:13-root-INFO: step: 78 lr_xt 0.13904792
2024-12-01-20:54:14-root-INFO: grad norm: 10.560 10.434 1.627
2024-12-01-20:54:14-root-INFO: Loss too large (118.797->121.822)! Learning rate decreased to 0.11124.
2024-12-01-20:54:14-root-INFO: Loss too large (118.797->118.798)! Learning rate decreased to 0.08899.
2024-12-01-20:54:14-root-INFO: grad norm: 6.472 6.404 0.933
2024-12-01-20:54:15-root-INFO: grad norm: 4.089 4.043 0.610
2024-12-01-20:54:15-root-INFO: grad norm: 2.939 2.900 0.474
2024-12-01-20:54:16-root-INFO: grad norm: 2.472 2.431 0.448
2024-12-01-20:54:16-root-INFO: Loss Change: 118.797 -> 114.281
2024-12-01-20:54:16-root-INFO: Regularization Change: 0.000 -> 1.147
2024-12-01-20:54:16-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-01-20:54:16-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-20:54:16-root-INFO: step: 77 lr_xt 0.14250787
2024-12-01-20:54:17-root-INFO: grad norm: 3.386 3.304 0.744
2024-12-01-20:54:17-root-INFO: grad norm: 4.150 4.088 0.719
2024-12-01-20:54:17-root-INFO: grad norm: 7.921 7.855 1.023
2024-12-01-20:54:18-root-INFO: Loss too large (113.738->114.781)! Learning rate decreased to 0.11401.
2024-12-01-20:54:18-root-INFO: Loss too large (113.738->114.037)! Learning rate decreased to 0.09121.
2024-12-01-20:54:18-root-INFO: grad norm: 4.043 3.982 0.699
2024-12-01-20:54:19-root-INFO: grad norm: 3.232 3.194 0.493
2024-12-01-20:54:19-root-INFO: Loss Change: 114.135 -> 111.796
2024-12-01-20:54:19-root-INFO: Regularization Change: 0.000 -> 1.263
2024-12-01-20:54:19-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-01-20:54:19-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-20:54:19-root-INFO: step: 76 lr_xt 0.14603050
2024-12-01-20:54:19-root-INFO: grad norm: 3.129 3.046 0.716
2024-12-01-20:54:20-root-INFO: grad norm: 3.764 3.704 0.671
2024-12-01-20:54:20-root-INFO: Loss too large (111.427->111.525)! Learning rate decreased to 0.11682.
2024-12-01-20:54:20-root-INFO: grad norm: 5.167 5.119 0.704
2024-12-01-20:54:21-root-INFO: Loss too large (111.077->111.194)! Learning rate decreased to 0.09346.
2024-12-01-20:54:21-root-INFO: grad norm: 3.726 3.676 0.605
2024-12-01-20:54:22-root-INFO: grad norm: 2.216 2.175 0.422
2024-12-01-20:54:22-root-INFO: Loss Change: 111.887 -> 109.686
2024-12-01-20:54:22-root-INFO: Regularization Change: 0.000 -> 1.129
2024-12-01-20:54:22-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-01-20:54:22-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-20:54:22-root-INFO: step: 75 lr_xt 0.14961620
2024-12-01-20:54:22-root-INFO: grad norm: 2.585 2.512 0.611
2024-12-01-20:54:23-root-INFO: grad norm: 2.749 2.699 0.524
2024-12-01-20:54:23-root-INFO: grad norm: 4.262 4.211 0.660
2024-12-01-20:54:23-root-INFO: Loss too large (108.854->109.233)! Learning rate decreased to 0.11969.
2024-12-01-20:54:24-root-INFO: grad norm: 4.393 4.336 0.707
2024-12-01-20:54:24-root-INFO: grad norm: 4.682 4.632 0.682
2024-12-01-20:54:24-root-INFO: Loss too large (108.143->108.216)! Learning rate decreased to 0.09575.
2024-12-01-20:54:25-root-INFO: Loss Change: 109.732 -> 107.889
2024-12-01-20:54:25-root-INFO: Regularization Change: 0.000 -> 1.478
2024-12-01-20:54:25-root-INFO: Undo step: 75
2024-12-01-20:54:25-root-INFO: Undo step: 76
2024-12-01-20:54:25-root-INFO: Undo step: 77
2024-12-01-20:54:25-root-INFO: Undo step: 78
2024-12-01-20:54:25-root-INFO: Undo step: 79
2024-12-01-20:54:25-root-INFO: step: 80 lr_xt 0.13231432
2024-12-01-20:54:25-root-INFO: grad norm: 36.700 36.391 4.750
2024-12-01-20:54:26-root-INFO: grad norm: 20.087 19.872 2.932
2024-12-01-20:54:26-root-INFO: grad norm: 18.497 18.272 2.875
2024-12-01-20:54:27-root-INFO: grad norm: 21.487 21.234 3.293
2024-12-01-20:54:27-root-INFO: Loss too large (150.193->152.887)! Learning rate decreased to 0.10585.
2024-12-01-20:54:27-root-INFO: grad norm: 18.443 18.208 2.936
2024-12-01-20:54:27-root-INFO: Loss Change: 264.525 -> 137.260
2024-12-01-20:54:28-root-INFO: Regularization Change: 0.000 -> 57.478
2024-12-01-20:54:28-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-01-20:54:28-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-20:54:28-root-INFO: step: 79 lr_xt 0.13565022
2024-12-01-20:54:28-root-INFO: grad norm: 17.779 17.541 2.899
2024-12-01-20:54:28-root-INFO: Loss too large (138.762->141.612)! Learning rate decreased to 0.10852.
2024-12-01-20:54:28-root-INFO: grad norm: 15.366 15.162 2.496
2024-12-01-20:54:29-root-INFO: grad norm: 13.581 13.417 2.099
2024-12-01-20:54:29-root-INFO: grad norm: 12.410 12.251 1.976
2024-12-01-20:54:30-root-INFO: grad norm: 11.866 11.723 1.837
2024-12-01-20:54:30-root-INFO: Loss Change: 138.762 -> 123.980
2024-12-01-20:54:30-root-INFO: Regularization Change: 0.000 -> 7.701
2024-12-01-20:54:30-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-01-20:54:30-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-20:54:30-root-INFO: step: 78 lr_xt 0.13904792
2024-12-01-20:54:30-root-INFO: grad norm: 9.965 9.864 1.415
2024-12-01-20:54:31-root-INFO: Loss too large (123.196->124.391)! Learning rate decreased to 0.11124.
2024-12-01-20:54:31-root-INFO: grad norm: 9.745 9.630 1.491
2024-12-01-20:54:32-root-INFO: grad norm: 9.652 9.542 1.457
2024-12-01-20:54:32-root-INFO: grad norm: 9.752 9.640 1.475
2024-12-01-20:54:33-root-INFO: grad norm: 9.692 9.579 1.473
2024-12-01-20:54:33-root-INFO: Loss Change: 123.196 -> 117.799
2024-12-01-20:54:33-root-INFO: Regularization Change: 0.000 -> 3.920
2024-12-01-20:54:33-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-01-20:54:33-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-20:54:33-root-INFO: step: 77 lr_xt 0.14250787
2024-12-01-20:54:33-root-INFO: grad norm: 11.489 11.322 1.955
2024-12-01-20:54:33-root-INFO: Loss too large (118.595->121.756)! Learning rate decreased to 0.11401.
2024-12-01-20:54:34-root-INFO: grad norm: 10.907 10.773 1.706
2024-12-01-20:54:34-root-INFO: grad norm: 10.165 10.045 1.552
2024-12-01-20:54:35-root-INFO: grad norm: 9.835 9.716 1.526
2024-12-01-20:54:35-root-INFO: grad norm: 10.057 9.945 1.499
2024-12-01-20:54:35-root-INFO: Loss Change: 118.595 -> 114.392
2024-12-01-20:54:35-root-INFO: Regularization Change: 0.000 -> 3.533
2024-12-01-20:54:35-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-01-20:54:35-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-20:54:36-root-INFO: step: 76 lr_xt 0.14603050
2024-12-01-20:54:36-root-INFO: grad norm: 7.984 7.911 1.078
2024-12-01-20:54:36-root-INFO: Loss too large (113.429->113.865)! Learning rate decreased to 0.11682.
2024-12-01-20:54:36-root-INFO: grad norm: 7.339 7.261 1.066
2024-12-01-20:54:37-root-INFO: grad norm: 7.266 7.189 1.054
2024-12-01-20:54:37-root-INFO: grad norm: 7.748 7.669 1.101
2024-12-01-20:54:38-root-INFO: grad norm: 7.672 7.586 1.147
2024-12-01-20:54:38-root-INFO: Loss Change: 113.429 -> 109.783
2024-12-01-20:54:38-root-INFO: Regularization Change: 0.000 -> 2.531
2024-12-01-20:54:38-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-01-20:54:38-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-20:54:38-root-INFO: step: 75 lr_xt 0.14961620
2024-12-01-20:54:38-root-INFO: grad norm: 9.229 9.096 1.561
2024-12-01-20:54:39-root-INFO: Loss too large (110.645->112.999)! Learning rate decreased to 0.11969.
2024-12-01-20:54:39-root-INFO: grad norm: 9.049 8.946 1.360
2024-12-01-20:54:40-root-INFO: grad norm: 9.017 8.918 1.331
2024-12-01-20:54:40-root-INFO: grad norm: 8.694 8.593 1.318
2024-12-01-20:54:40-root-INFO: grad norm: 8.175 8.086 1.202
2024-12-01-20:54:41-root-INFO: Loss Change: 110.645 -> 107.686
2024-12-01-20:54:41-root-INFO: Regularization Change: 0.000 -> 2.554
2024-12-01-20:54:41-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-01-20:54:41-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-20:54:41-root-INFO: step: 74 lr_xt 0.15326538
2024-12-01-20:54:41-root-INFO: grad norm: 6.779 6.725 0.852
2024-12-01-20:54:41-root-INFO: Loss too large (107.038->108.086)! Learning rate decreased to 0.12261.
2024-12-01-20:54:42-root-INFO: grad norm: 7.415 7.347 1.007
2024-12-01-20:54:42-root-INFO: Loss too large (106.500->106.552)! Learning rate decreased to 0.09809.
2024-12-01-20:54:42-root-INFO: grad norm: 5.053 4.996 0.760
2024-12-01-20:54:43-root-INFO: grad norm: 3.116 3.073 0.515
2024-12-01-20:54:43-root-INFO: grad norm: 2.594 2.554 0.457
2024-12-01-20:54:44-root-INFO: Loss Change: 107.038 -> 103.710
2024-12-01-20:54:44-root-INFO: Regularization Change: 0.000 -> 1.265
2024-12-01-20:54:44-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-01-20:54:44-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-20:54:44-root-INFO: step: 73 lr_xt 0.15697839
2024-12-01-20:54:44-root-INFO: grad norm: 4.159 4.069 0.860
2024-12-01-20:54:44-root-INFO: Loss too large (103.933->104.029)! Learning rate decreased to 0.12558.
2024-12-01-20:54:45-root-INFO: grad norm: 4.429 4.376 0.685
2024-12-01-20:54:45-root-INFO: grad norm: 5.782 5.728 0.792
2024-12-01-20:54:45-root-INFO: Loss too large (103.219->103.465)! Learning rate decreased to 0.10047.
2024-12-01-20:54:46-root-INFO: grad norm: 4.153 4.105 0.631
2024-12-01-20:54:46-root-INFO: grad norm: 2.463 2.424 0.441
2024-12-01-20:54:47-root-INFO: Loss Change: 103.933 -> 101.588
2024-12-01-20:54:47-root-INFO: Regularization Change: 0.000 -> 1.179
2024-12-01-20:54:47-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-01-20:54:47-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-20:54:47-root-INFO: step: 72 lr_xt 0.16075558
2024-12-01-20:54:47-root-INFO: grad norm: 2.513 2.440 0.602
2024-12-01-20:54:48-root-INFO: grad norm: 2.761 2.717 0.492
2024-12-01-20:54:48-root-INFO: grad norm: 4.989 4.945 0.667
2024-12-01-20:54:48-root-INFO: Loss too large (100.693->101.339)! Learning rate decreased to 0.12860.
2024-12-01-20:54:48-root-INFO: Loss too large (100.693->100.888)! Learning rate decreased to 0.10288.
2024-12-01-20:54:49-root-INFO: grad norm: 3.818 3.776 0.563
2024-12-01-20:54:49-root-INFO: grad norm: 2.248 2.211 0.408
2024-12-01-20:54:50-root-INFO: Loss Change: 101.502 -> 99.311
2024-12-01-20:54:50-root-INFO: Regularization Change: 0.000 -> 1.373
2024-12-01-20:54:50-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-01-20:54:50-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-20:54:50-root-INFO: step: 71 lr_xt 0.16459726
2024-12-01-20:54:50-root-INFO: grad norm: 2.246 2.205 0.427
2024-12-01-20:54:51-root-INFO: grad norm: 3.290 3.261 0.438
2024-12-01-20:54:51-root-INFO: Loss too large (98.930->99.071)! Learning rate decreased to 0.13168.
2024-12-01-20:54:51-root-INFO: grad norm: 3.719 3.685 0.502
2024-12-01-20:54:52-root-INFO: grad norm: 5.047 5.018 0.546
2024-12-01-20:54:52-root-INFO: Loss too large (98.375->98.683)! Learning rate decreased to 0.10534.
2024-12-01-20:54:52-root-INFO: grad norm: 3.673 3.637 0.511
2024-12-01-20:54:53-root-INFO: Loss Change: 99.378 -> 97.373
2024-12-01-20:54:53-root-INFO: Regularization Change: 0.000 -> 1.286
2024-12-01-20:54:53-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-01-20:54:53-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-20:54:53-root-INFO: step: 70 lr_xt 0.16850375
2024-12-01-20:54:53-root-INFO: grad norm: 3.723 3.618 0.880
2024-12-01-20:54:53-root-INFO: grad norm: 5.008 4.943 0.805
2024-12-01-20:54:54-root-INFO: Loss too large (97.293->98.444)! Learning rate decreased to 0.13480.
2024-12-01-20:54:54-root-INFO: grad norm: 7.525 7.469 0.920
2024-12-01-20:54:54-root-INFO: Loss too large (97.204->97.836)! Learning rate decreased to 0.10784.
2024-12-01-20:54:55-root-INFO: grad norm: 4.303 4.252 0.661
2024-12-01-20:54:55-root-INFO: grad norm: 3.337 3.310 0.426
2024-12-01-20:54:56-root-INFO: Loss Change: 97.464 -> 95.291
2024-12-01-20:54:56-root-INFO: Regularization Change: 0.000 -> 1.271
2024-12-01-20:54:56-root-INFO: Undo step: 70
2024-12-01-20:54:56-root-INFO: Undo step: 71
2024-12-01-20:54:56-root-INFO: Undo step: 72
2024-12-01-20:54:56-root-INFO: Undo step: 73
2024-12-01-20:54:56-root-INFO: Undo step: 74
2024-12-01-20:54:56-root-INFO: step: 75 lr_xt 0.14961620
2024-12-01-20:54:56-root-INFO: grad norm: 38.461 38.124 5.082
2024-12-01-20:54:57-root-INFO: grad norm: 18.076 17.861 2.781
2024-12-01-20:54:57-root-INFO: grad norm: 15.099 14.866 2.641
2024-12-01-20:54:58-root-INFO: grad norm: 16.858 16.664 2.550
2024-12-01-20:54:58-root-INFO: Loss too large (131.494->132.148)! Learning rate decreased to 0.11969.
2024-12-01-20:54:58-root-INFO: grad norm: 13.854 13.682 2.177
2024-12-01-20:54:59-root-INFO: Loss Change: 250.008 -> 120.081
2024-12-01-20:54:59-root-INFO: Regularization Change: 0.000 -> 64.777
2024-12-01-20:54:59-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-01-20:54:59-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-20:54:59-root-INFO: step: 74 lr_xt 0.15326538
2024-12-01-20:54:59-root-INFO: grad norm: 12.687 12.497 2.192
2024-12-01-20:54:59-root-INFO: Loss too large (121.054->121.940)! Learning rate decreased to 0.12261.
2024-12-01-20:55:00-root-INFO: grad norm: 10.904 10.763 1.752
2024-12-01-20:55:00-root-INFO: grad norm: 9.426 9.315 1.440
2024-12-01-20:55:01-root-INFO: grad norm: 8.581 8.493 1.226
2024-12-01-20:55:01-root-INFO: grad norm: 8.066 7.980 1.175
2024-12-01-20:55:01-root-INFO: Loss Change: 121.054 -> 109.662
2024-12-01-20:55:01-root-INFO: Regularization Change: 0.000 -> 6.715
2024-12-01-20:55:01-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-01-20:55:01-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-20:55:02-root-INFO: step: 73 lr_xt 0.15697839
2024-12-01-20:55:02-root-INFO: grad norm: 6.911 6.861 0.832
2024-12-01-20:55:02-root-INFO: Loss too large (109.045->109.267)! Learning rate decreased to 0.12558.
2024-12-01-20:55:02-root-INFO: grad norm: 7.475 7.406 1.014
2024-12-01-20:55:03-root-INFO: grad norm: 7.174 7.109 0.964
2024-12-01-20:55:03-root-INFO: grad norm: 6.600 6.534 0.930
2024-12-01-20:55:04-root-INFO: grad norm: 6.727 6.672 0.859
2024-12-01-20:55:04-root-INFO: Loss Change: 109.045 -> 104.602
2024-12-01-20:55:04-root-INFO: Regularization Change: 0.000 -> 3.615
2024-12-01-20:55:04-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-01-20:55:04-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-20:55:04-root-INFO: step: 72 lr_xt 0.16075558
2024-12-01-20:55:04-root-INFO: grad norm: 9.789 9.666 1.546
2024-12-01-20:55:05-root-INFO: Loss too large (105.241->107.515)! Learning rate decreased to 0.12860.
2024-12-01-20:55:05-root-INFO: grad norm: 8.095 8.011 1.160
2024-12-01-20:55:05-root-INFO: grad norm: 7.620 7.556 0.983
2024-12-01-20:55:06-root-INFO: grad norm: 6.804 6.755 0.816
2024-12-01-20:55:06-root-INFO: grad norm: 6.516 6.458 0.867
2024-12-01-20:55:07-root-INFO: Loss Change: 105.241 -> 100.603
2024-12-01-20:55:07-root-INFO: Regularization Change: 0.000 -> 3.023
2024-12-01-20:55:07-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-01-20:55:07-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-20:55:07-root-INFO: step: 71 lr_xt 0.16459726
2024-12-01-20:55:07-root-INFO: grad norm: 5.858 5.819 0.682
2024-12-01-20:55:07-root-INFO: Loss too large (100.451->100.926)! Learning rate decreased to 0.13168.
2024-12-01-20:55:08-root-INFO: grad norm: 6.082 6.032 0.782
2024-12-01-20:55:08-root-INFO: grad norm: 6.372 6.319 0.824
2024-12-01-20:55:09-root-INFO: grad norm: 7.793 7.732 0.972
2024-12-01-20:55:09-root-INFO: Loss too large (99.113->99.429)! Learning rate decreased to 0.10534.
2024-12-01-20:55:09-root-INFO: grad norm: 4.805 4.752 0.713
2024-12-01-20:55:10-root-INFO: Loss Change: 100.451 -> 97.144
2024-12-01-20:55:10-root-INFO: Regularization Change: 0.000 -> 1.818
2024-12-01-20:55:10-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-01-20:55:10-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-20:55:10-root-INFO: step: 70 lr_xt 0.16850375
2024-12-01-20:55:10-root-INFO: grad norm: 4.703 4.591 1.021
2024-12-01-20:55:11-root-INFO: grad norm: 6.071 6.016 0.815
2024-12-01-20:55:11-root-INFO: Loss too large (96.950->97.708)! Learning rate decreased to 0.13480.
2024-12-01-20:55:11-root-INFO: grad norm: 5.993 5.950 0.717
2024-12-01-20:55:12-root-INFO: grad norm: 7.241 7.213 0.631
2024-12-01-20:55:12-root-INFO: Loss too large (96.079->96.319)! Learning rate decreased to 0.10784.
2024-12-01-20:55:12-root-INFO: grad norm: 4.342 4.312 0.512
2024-12-01-20:55:13-root-INFO: Loss Change: 97.262 -> 94.267
2024-12-01-20:55:13-root-INFO: Regularization Change: 0.000 -> 1.744
2024-12-01-20:55:13-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-01-20:55:13-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-20:55:13-root-INFO: step: 69 lr_xt 0.17247530
2024-12-01-20:55:13-root-INFO: grad norm: 2.738 2.697 0.473
2024-12-01-20:55:13-root-INFO: grad norm: 4.501 4.475 0.482
2024-12-01-20:55:14-root-INFO: Loss too large (93.904->94.432)! Learning rate decreased to 0.13798.
2024-12-01-20:55:14-root-INFO: Loss too large (93.904->94.087)! Learning rate decreased to 0.11038.
2024-12-01-20:55:14-root-INFO: grad norm: 3.801 3.767 0.505
2024-12-01-20:55:15-root-INFO: grad norm: 2.799 2.767 0.428
2024-12-01-20:55:15-root-INFO: grad norm: 3.200 3.169 0.447
2024-12-01-20:55:16-root-INFO: Loss Change: 94.316 -> 92.416
2024-12-01-20:55:16-root-INFO: Regularization Change: 0.000 -> 1.311
2024-12-01-20:55:16-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-01-20:55:16-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-20:55:16-root-INFO: step: 68 lr_xt 0.17651217
2024-12-01-20:55:16-root-INFO: grad norm: 6.798 6.700 1.150
2024-12-01-20:55:16-root-INFO: Loss too large (92.762->93.844)! Learning rate decreased to 0.14121.
2024-12-01-20:55:16-root-INFO: Loss too large (92.762->93.176)! Learning rate decreased to 0.11297.
2024-12-01-20:55:17-root-INFO: grad norm: 3.869 3.824 0.588
2024-12-01-20:55:17-root-INFO: grad norm: 3.128 3.100 0.417
2024-12-01-20:55:18-root-INFO: grad norm: 3.336 3.310 0.417
2024-12-01-20:55:18-root-INFO: grad norm: 3.473 3.445 0.444
2024-12-01-20:55:19-root-INFO: Loss Change: 92.762 -> 90.298
2024-12-01-20:55:19-root-INFO: Regularization Change: 0.000 -> 1.064
2024-12-01-20:55:19-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-01-20:55:19-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-20:55:19-root-INFO: step: 67 lr_xt 0.18061458
2024-12-01-20:55:19-root-INFO: grad norm: 4.891 4.831 0.766
2024-12-01-20:55:19-root-INFO: Loss too large (90.354->91.133)! Learning rate decreased to 0.14449.
2024-12-01-20:55:19-root-INFO: Loss too large (90.354->90.715)! Learning rate decreased to 0.11559.
2024-12-01-20:55:19-root-INFO: Loss too large (90.354->90.375)! Learning rate decreased to 0.09247.
2024-12-01-20:55:20-root-INFO: grad norm: 3.625 3.597 0.457
2024-12-01-20:55:20-root-INFO: grad norm: 2.220 2.186 0.385
2024-12-01-20:55:21-root-INFO: grad norm: 2.296 2.267 0.362
2024-12-01-20:55:21-root-INFO: grad norm: 2.522 2.495 0.370
2024-12-01-20:55:22-root-INFO: Loss Change: 90.354 -> 88.660
2024-12-01-20:55:22-root-INFO: Regularization Change: 0.000 -> 0.722
2024-12-01-20:55:22-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-01-20:55:22-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-20:55:22-root-INFO: step: 66 lr_xt 0.18478272
2024-12-01-20:55:22-root-INFO: grad norm: 2.621 2.584 0.437
2024-12-01-20:55:22-root-INFO: grad norm: 5.978 5.957 0.503
2024-12-01-20:55:23-root-INFO: Loss too large (88.675->89.652)! Learning rate decreased to 0.14783.
2024-12-01-20:55:23-root-INFO: Loss too large (88.675->89.243)! Learning rate decreased to 0.11826.
2024-12-01-20:55:23-root-INFO: Loss too large (88.675->88.867)! Learning rate decreased to 0.09461.
2024-12-01-20:55:23-root-INFO: grad norm: 3.756 3.725 0.480
2024-12-01-20:55:24-root-INFO: grad norm: 1.724 1.692 0.331
2024-12-01-20:55:24-root-INFO: grad norm: 1.647 1.614 0.325
2024-12-01-20:55:25-root-INFO: Loss Change: 88.686 -> 86.991
2024-12-01-20:55:25-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-01-20:55:25-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-01-20:55:25-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-20:55:25-root-INFO: step: 65 lr_xt 0.18901677
2024-12-01-20:55:25-root-INFO: grad norm: 2.773 2.697 0.645
2024-12-01-20:55:25-root-INFO: grad norm: 4.106 4.053 0.658
2024-12-01-20:55:26-root-INFO: Loss too large (86.798->88.411)! Learning rate decreased to 0.15121.
2024-12-01-20:55:26-root-INFO: Loss too large (86.798->87.269)! Learning rate decreased to 0.12097.
2024-12-01-20:55:26-root-INFO: grad norm: 5.839 5.807 0.610
2024-12-01-20:55:26-root-INFO: Loss too large (86.636->86.950)! Learning rate decreased to 0.09678.
2024-12-01-20:55:27-root-INFO: grad norm: 3.830 3.798 0.497
2024-12-01-20:55:27-root-INFO: grad norm: 1.706 1.676 0.318
2024-12-01-20:55:27-root-INFO: Loss Change: 87.076 -> 85.272
2024-12-01-20:55:28-root-INFO: Regularization Change: 0.000 -> 1.008
2024-12-01-20:55:28-root-INFO: Undo step: 65
2024-12-01-20:55:28-root-INFO: Undo step: 66
2024-12-01-20:55:28-root-INFO: Undo step: 67
2024-12-01-20:55:28-root-INFO: Undo step: 68
2024-12-01-20:55:28-root-INFO: Undo step: 69
2024-12-01-20:55:28-root-INFO: step: 70 lr_xt 0.16850375
2024-12-01-20:55:28-root-INFO: grad norm: 33.599 33.288 4.556
2024-12-01-20:55:28-root-INFO: grad norm: 19.094 18.844 3.080
2024-12-01-20:55:29-root-INFO: grad norm: 16.055 15.852 2.547
2024-12-01-20:55:29-root-INFO: grad norm: 16.726 16.548 2.430
2024-12-01-20:55:30-root-INFO: grad norm: 17.288 17.099 2.547
2024-12-01-20:55:30-root-INFO: Loss Change: 238.366 -> 117.660
2024-12-01-20:55:30-root-INFO: Regularization Change: 0.000 -> 77.939
2024-12-01-20:55:30-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-01-20:55:30-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-20:55:30-root-INFO: step: 69 lr_xt 0.17247530
2024-12-01-20:55:30-root-INFO: grad norm: 18.027 17.837 2.615
2024-12-01-20:55:31-root-INFO: grad norm: 17.380 17.179 2.638
2024-12-01-20:55:31-root-INFO: grad norm: 16.819 16.657 2.330
2024-12-01-20:55:32-root-INFO: grad norm: 16.680 16.494 2.488
2024-12-01-20:55:32-root-INFO: grad norm: 17.667 17.509 2.359
2024-12-01-20:55:32-root-INFO: Loss Change: 119.110 -> 110.055
2024-12-01-20:55:32-root-INFO: Regularization Change: 0.000 -> 16.983
2024-12-01-20:55:32-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-01-20:55:32-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-20:55:33-root-INFO: step: 68 lr_xt 0.17651217
2024-12-01-20:55:33-root-INFO: grad norm: 15.484 15.343 2.080
2024-12-01-20:55:33-root-INFO: grad norm: 14.495 14.381 1.810
2024-12-01-20:55:34-root-INFO: grad norm: 13.622 13.491 1.883
2024-12-01-20:55:34-root-INFO: grad norm: 13.387 13.268 1.779
2024-12-01-20:55:35-root-INFO: grad norm: 13.283 13.139 1.950
2024-12-01-20:55:35-root-INFO: Loss Change: 107.944 -> 98.727
2024-12-01-20:55:35-root-INFO: Regularization Change: 0.000 -> 11.303
2024-12-01-20:55:35-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-01-20:55:35-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-20:55:35-root-INFO: step: 67 lr_xt 0.18061458
2024-12-01-20:55:35-root-INFO: grad norm: 14.420 14.261 2.138
2024-12-01-20:55:36-root-INFO: grad norm: 13.708 13.544 2.118
2024-12-01-20:55:36-root-INFO: grad norm: 12.910 12.789 1.763
2024-12-01-20:55:37-root-INFO: grad norm: 12.307 12.176 1.791
2024-12-01-20:55:37-root-INFO: grad norm: 12.078 11.962 1.665
2024-12-01-20:55:37-root-INFO: Loss Change: 99.754 -> 93.916
2024-12-01-20:55:37-root-INFO: Regularization Change: 0.000 -> 8.612
2024-12-01-20:55:37-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-01-20:55:37-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-20:55:38-root-INFO: step: 66 lr_xt 0.18478272
2024-12-01-20:55:38-root-INFO: grad norm: 11.426 11.312 1.608
2024-12-01-20:55:38-root-INFO: grad norm: 12.507 12.391 1.694
2024-12-01-20:55:38-root-INFO: Loss too large (93.016->93.420)! Learning rate decreased to 0.14783.
2024-12-01-20:55:39-root-INFO: grad norm: 7.466 7.365 1.225
2024-12-01-20:55:39-root-INFO: grad norm: 4.953 4.911 0.639
2024-12-01-20:55:40-root-INFO: grad norm: 3.346 3.310 0.488
2024-12-01-20:55:40-root-INFO: Loss Change: 93.210 -> 84.794
2024-12-01-20:55:40-root-INFO: Regularization Change: 0.000 -> 3.396
2024-12-01-20:55:40-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-01-20:55:40-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-20:55:40-root-INFO: step: 65 lr_xt 0.18901677
2024-12-01-20:55:40-root-INFO: grad norm: 3.768 3.697 0.726
2024-12-01-20:55:41-root-INFO: grad norm: 4.073 4.015 0.682
2024-12-01-20:55:41-root-INFO: grad norm: 5.322 5.267 0.761
2024-12-01-20:55:41-root-INFO: Loss too large (84.202->84.754)! Learning rate decreased to 0.15121.
2024-12-01-20:55:42-root-INFO: grad norm: 4.781 4.726 0.723
2024-12-01-20:55:42-root-INFO: grad norm: 4.573 4.534 0.594
2024-12-01-20:55:43-root-INFO: Loss Change: 84.970 -> 82.928
2024-12-01-20:55:43-root-INFO: Regularization Change: 0.000 -> 2.352
2024-12-01-20:55:43-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-01-20:55:43-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-20:55:43-root-INFO: step: 64 lr_xt 0.19331686
2024-12-01-20:55:43-root-INFO: grad norm: 3.998 3.966 0.504
2024-12-01-20:55:43-root-INFO: grad norm: 5.940 5.907 0.631
2024-12-01-20:55:44-root-INFO: Loss too large (82.339->83.322)! Learning rate decreased to 0.15465.
2024-12-01-20:55:44-root-INFO: Loss too large (82.339->82.450)! Learning rate decreased to 0.12372.
2024-12-01-20:55:44-root-INFO: grad norm: 3.785 3.749 0.521
2024-12-01-20:55:45-root-INFO: grad norm: 2.028 2.002 0.323
2024-12-01-20:55:45-root-INFO: grad norm: 1.738 1.713 0.295
2024-12-01-20:55:46-root-INFO: Loss Change: 82.688 -> 80.129
2024-12-01-20:55:46-root-INFO: Regularization Change: 0.000 -> 1.372
2024-12-01-20:55:46-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-01-20:55:46-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-20:55:46-root-INFO: step: 63 lr_xt 0.19768311
2024-12-01-20:55:46-root-INFO: grad norm: 2.331 2.276 0.500
2024-12-01-20:55:46-root-INFO: grad norm: 2.769 2.731 0.458
2024-12-01-20:55:47-root-INFO: grad norm: 5.297 5.260 0.618
2024-12-01-20:55:47-root-INFO: Loss too large (79.784->80.751)! Learning rate decreased to 0.15815.
2024-12-01-20:55:47-root-INFO: Loss too large (79.784->80.121)! Learning rate decreased to 0.12652.
2024-12-01-20:55:48-root-INFO: grad norm: 3.735 3.699 0.517
2024-12-01-20:55:48-root-INFO: grad norm: 1.918 1.892 0.316
2024-12-01-20:55:48-root-INFO: Loss Change: 80.239 -> 78.268
2024-12-01-20:55:48-root-INFO: Regularization Change: 0.000 -> 1.484
2024-12-01-20:55:48-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-01-20:55:48-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-20:55:49-root-INFO: step: 62 lr_xt 0.20211560
2024-12-01-20:55:49-root-INFO: grad norm: 2.389 2.321 0.568
2024-12-01-20:55:49-root-INFO: grad norm: 2.559 2.519 0.449
2024-12-01-20:55:50-root-INFO: grad norm: 3.706 3.659 0.583
2024-12-01-20:55:50-root-INFO: Loss too large (77.340->77.657)! Learning rate decreased to 0.16169.
2024-12-01-20:55:50-root-INFO: grad norm: 3.947 3.909 0.542
2024-12-01-20:55:51-root-INFO: grad norm: 5.356 5.320 0.625
2024-12-01-20:55:51-root-INFO: Loss too large (76.954->77.468)! Learning rate decreased to 0.12935.
2024-12-01-20:55:51-root-INFO: Loss Change: 78.074 -> 76.901
2024-12-01-20:55:51-root-INFO: Regularization Change: 0.000 -> 1.885
2024-12-01-20:55:51-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-01-20:55:51-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-20:55:51-root-INFO: step: 61 lr_xt 0.20661437
2024-12-01-20:55:51-root-INFO: grad norm: 3.807 3.783 0.425
2024-12-01-20:55:52-root-INFO: grad norm: 6.449 6.418 0.633
2024-12-01-20:55:52-root-INFO: Loss too large (76.559->77.916)! Learning rate decreased to 0.16529.
2024-12-01-20:55:52-root-INFO: Loss too large (76.559->77.053)! Learning rate decreased to 0.13223.
2024-12-01-20:55:53-root-INFO: grad norm: 3.856 3.821 0.518
2024-12-01-20:55:53-root-INFO: grad norm: 2.356 2.333 0.335
2024-12-01-20:55:54-root-INFO: grad norm: 1.986 1.967 0.275
2024-12-01-20:55:54-root-INFO: Loss Change: 76.766 -> 74.454
2024-12-01-20:55:54-root-INFO: Regularization Change: 0.000 -> 1.256
2024-12-01-20:55:54-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-01-20:55:54-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-20:55:54-root-INFO: step: 60 lr_xt 0.21117946
2024-12-01-20:55:54-root-INFO: grad norm: 2.366 2.316 0.482
2024-12-01-20:55:55-root-INFO: grad norm: 2.997 2.970 0.398
2024-12-01-20:55:55-root-INFO: Loss too large (74.147->74.264)! Learning rate decreased to 0.16894.
2024-12-01-20:55:55-root-INFO: grad norm: 3.480 3.456 0.413
2024-12-01-20:55:56-root-INFO: grad norm: 5.099 5.081 0.429
2024-12-01-20:55:56-root-INFO: Loss too large (73.900->74.293)! Learning rate decreased to 0.13515.
2024-12-01-20:55:56-root-INFO: grad norm: 3.630 3.606 0.423
2024-12-01-20:55:57-root-INFO: Loss Change: 74.509 -> 72.776
2024-12-01-20:55:57-root-INFO: Regularization Change: 0.000 -> 1.403
2024-12-01-20:55:57-root-INFO: Undo step: 60
2024-12-01-20:55:57-root-INFO: Undo step: 61
2024-12-01-20:55:57-root-INFO: Undo step: 62
2024-12-01-20:55:57-root-INFO: Undo step: 63
2024-12-01-20:55:57-root-INFO: Undo step: 64
2024-12-01-20:55:57-root-INFO: step: 65 lr_xt 0.18901677
2024-12-01-20:55:57-root-INFO: grad norm: 27.990 27.764 3.549
2024-12-01-20:55:58-root-INFO: grad norm: 12.818 12.647 2.089
2024-12-01-20:55:58-root-INFO: grad norm: 9.470 9.336 1.589
2024-12-01-20:55:59-root-INFO: grad norm: 10.160 10.052 1.479
2024-12-01-20:55:59-root-INFO: grad norm: 10.028 9.907 1.550
2024-12-01-20:55:59-root-INFO: Loss Change: 202.123 -> 95.531
2024-12-01-20:55:59-root-INFO: Regularization Change: 0.000 -> 71.490
2024-12-01-20:55:59-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-01-20:55:59-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-20:55:59-root-INFO: step: 64 lr_xt 0.19331686
2024-12-01-20:56:00-root-INFO: grad norm: 11.688 11.556 1.749
2024-12-01-20:56:00-root-INFO: grad norm: 12.572 12.449 1.751
2024-12-01-20:56:01-root-INFO: grad norm: 12.794 12.667 1.798
2024-12-01-20:56:01-root-INFO: grad norm: 12.726 12.580 1.925
2024-12-01-20:56:01-root-INFO: grad norm: 13.738 13.577 2.098
2024-12-01-20:56:02-root-INFO: Loss too large (91.790->91.924)! Learning rate decreased to 0.15465.
2024-12-01-20:56:02-root-INFO: Loss Change: 96.084 -> 87.052
2024-12-01-20:56:02-root-INFO: Regularization Change: 0.000 -> 10.719
2024-12-01-20:56:02-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-01-20:56:02-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-20:56:02-root-INFO: step: 63 lr_xt 0.19768311
2024-12-01-20:56:02-root-INFO: grad norm: 7.728 7.629 1.230
2024-12-01-20:56:03-root-INFO: grad norm: 7.658 7.562 1.214
2024-12-01-20:56:03-root-INFO: grad norm: 7.986 7.894 1.211
2024-12-01-20:56:04-root-INFO: grad norm: 8.724 8.619 1.349
2024-12-01-20:56:04-root-INFO: Loss too large (82.892->83.425)! Learning rate decreased to 0.15815.
2024-12-01-20:56:04-root-INFO: grad norm: 6.641 6.557 1.053
2024-12-01-20:56:05-root-INFO: Loss Change: 86.382 -> 79.459
2024-12-01-20:56:05-root-INFO: Regularization Change: 0.000 -> 5.554
2024-12-01-20:56:05-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-01-20:56:05-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-20:56:05-root-INFO: step: 62 lr_xt 0.20211560
2024-12-01-20:56:05-root-INFO: grad norm: 7.310 7.182 1.362
2024-12-01-20:56:05-root-INFO: Loss too large (79.940->80.795)! Learning rate decreased to 0.16169.
2024-12-01-20:56:05-root-INFO: grad norm: 5.644 5.561 0.970
2024-12-01-20:56:06-root-INFO: grad norm: 3.978 3.917 0.694
2024-12-01-20:56:06-root-INFO: grad norm: 3.472 3.427 0.557
2024-12-01-20:56:07-root-INFO: grad norm: 3.477 3.435 0.540
2024-12-01-20:56:07-root-INFO: Loss Change: 79.940 -> 75.710
2024-12-01-20:56:07-root-INFO: Regularization Change: 0.000 -> 2.883
2024-12-01-20:56:07-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-01-20:56:07-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-20:56:07-root-INFO: step: 61 lr_xt 0.20661437
2024-12-01-20:56:07-root-INFO: grad norm: 3.233 3.204 0.437
2024-12-01-20:56:08-root-INFO: grad norm: 5.396 5.356 0.652
2024-12-01-20:56:08-root-INFO: Loss too large (75.347->76.143)! Learning rate decreased to 0.16529.
2024-12-01-20:56:08-root-INFO: Loss too large (75.347->75.410)! Learning rate decreased to 0.13223.
2024-12-01-20:56:09-root-INFO: grad norm: 3.760 3.721 0.537
2024-12-01-20:56:09-root-INFO: grad norm: 2.160 2.130 0.360
2024-12-01-20:56:10-root-INFO: grad norm: 1.947 1.921 0.317
2024-12-01-20:56:10-root-INFO: Loss Change: 75.513 -> 73.146
2024-12-01-20:56:10-root-INFO: Regularization Change: 0.000 -> 1.601
2024-12-01-20:56:10-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-01-20:56:10-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-20:56:10-root-INFO: step: 60 lr_xt 0.21117946
2024-12-01-20:56:10-root-INFO: grad norm: 2.786 2.718 0.613
2024-12-01-20:56:11-root-INFO: grad norm: 3.675 3.625 0.606
2024-12-01-20:56:11-root-INFO: Loss too large (72.987->73.286)! Learning rate decreased to 0.16894.
2024-12-01-20:56:11-root-INFO: grad norm: 4.602 4.559 0.625
2024-12-01-20:56:12-root-INFO: Loss too large (72.644->72.780)! Learning rate decreased to 0.13515.
2024-12-01-20:56:12-root-INFO: grad norm: 3.585 3.549 0.509
2024-12-01-20:56:12-root-INFO: grad norm: 2.429 2.400 0.369
2024-12-01-20:56:13-root-INFO: Loss Change: 73.264 -> 71.241
2024-12-01-20:56:13-root-INFO: Regularization Change: 0.000 -> 1.605
2024-12-01-20:56:13-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-01-20:56:13-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-20:56:13-root-INFO: step: 59 lr_xt 0.21581084
2024-12-01-20:56:13-root-INFO: grad norm: 2.274 2.230 0.446
2024-12-01-20:56:14-root-INFO: grad norm: 3.144 3.123 0.369
2024-12-01-20:56:14-root-INFO: Loss too large (70.750->70.960)! Learning rate decreased to 0.17265.
2024-12-01-20:56:14-root-INFO: grad norm: 3.547 3.520 0.437
2024-12-01-20:56:15-root-INFO: grad norm: 4.500 4.472 0.503
2024-12-01-20:56:15-root-INFO: Loss too large (70.297->70.549)! Learning rate decreased to 0.13812.
2024-12-01-20:56:15-root-INFO: grad norm: 3.659 3.628 0.472
2024-12-01-20:56:16-root-INFO: Loss Change: 71.155 -> 69.332
2024-12-01-20:56:16-root-INFO: Regularization Change: 0.000 -> 1.622
2024-12-01-20:56:16-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-01-20:56:16-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-20:56:16-root-INFO: step: 58 lr_xt 0.22050848
2024-12-01-20:56:16-root-INFO: grad norm: 3.534 3.474 0.649
2024-12-01-20:56:16-root-INFO: Loss too large (69.337->69.703)! Learning rate decreased to 0.17641.
2024-12-01-20:56:17-root-INFO: grad norm: 3.895 3.854 0.566
2024-12-01-20:56:17-root-INFO: grad norm: 4.886 4.845 0.630
2024-12-01-20:56:17-root-INFO: Loss too large (68.901->69.241)! Learning rate decreased to 0.14113.
2024-12-01-20:56:18-root-INFO: grad norm: 3.839 3.802 0.535
2024-12-01-20:56:18-root-INFO: grad norm: 2.584 2.557 0.372
2024-12-01-20:56:19-root-INFO: Loss Change: 69.337 -> 67.536
2024-12-01-20:56:19-root-INFO: Regularization Change: 0.000 -> 1.381
2024-12-01-20:56:19-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-01-20:56:19-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-20:56:19-root-INFO: step: 57 lr_xt 0.22527231
2024-12-01-20:56:19-root-INFO: grad norm: 2.258 2.223 0.393
2024-12-01-20:56:19-root-INFO: grad norm: 3.396 3.373 0.388
2024-12-01-20:56:20-root-INFO: Loss too large (66.950->67.422)! Learning rate decreased to 0.18022.
2024-12-01-20:56:20-root-INFO: Loss too large (66.950->66.968)! Learning rate decreased to 0.14417.
2024-12-01-20:56:20-root-INFO: grad norm: 3.104 3.078 0.396
2024-12-01-20:56:21-root-INFO: grad norm: 2.933 2.912 0.351
2024-12-01-20:56:21-root-INFO: grad norm: 2.886 2.861 0.379
2024-12-01-20:56:21-root-INFO: Loss Change: 67.235 -> 65.737
2024-12-01-20:56:21-root-INFO: Regularization Change: 0.000 -> 1.328
2024-12-01-20:56:21-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-01-20:56:21-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-20:56:22-root-INFO: step: 56 lr_xt 0.23010221
2024-12-01-20:56:22-root-INFO: grad norm: 3.667 3.615 0.620
2024-12-01-20:56:22-root-INFO: Loss too large (65.876->66.488)! Learning rate decreased to 0.18408.
2024-12-01-20:56:22-root-INFO: Loss too large (65.876->65.954)! Learning rate decreased to 0.14727.
2024-12-01-20:56:22-root-INFO: grad norm: 3.319 3.289 0.444
2024-12-01-20:56:23-root-INFO: grad norm: 3.088 3.064 0.386
2024-12-01-20:56:23-root-INFO: grad norm: 3.043 3.017 0.400
2024-12-01-20:56:24-root-INFO: grad norm: 3.010 2.988 0.360
2024-12-01-20:56:24-root-INFO: Loss Change: 65.876 -> 64.438
2024-12-01-20:56:24-root-INFO: Regularization Change: 0.000 -> 1.204
2024-12-01-20:56:24-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-01-20:56:24-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-20:56:24-root-INFO: step: 55 lr_xt 0.23499803
2024-12-01-20:56:24-root-INFO: grad norm: 2.844 2.813 0.419
2024-12-01-20:56:25-root-INFO: Loss too large (64.294->64.482)! Learning rate decreased to 0.18800.
2024-12-01-20:56:25-root-INFO: grad norm: 3.883 3.863 0.392
2024-12-01-20:56:25-root-INFO: Loss too large (64.077->64.367)! Learning rate decreased to 0.15040.
2024-12-01-20:56:26-root-INFO: grad norm: 3.545 3.518 0.437
2024-12-01-20:56:26-root-INFO: grad norm: 3.123 3.101 0.373
2024-12-01-20:56:27-root-INFO: grad norm: 3.113 3.088 0.400
2024-12-01-20:56:27-root-INFO: Loss Change: 64.294 -> 62.877
2024-12-01-20:56:27-root-INFO: Regularization Change: 0.000 -> 1.253
2024-12-01-20:56:27-root-INFO: Undo step: 55
2024-12-01-20:56:27-root-INFO: Undo step: 56
2024-12-01-20:56:27-root-INFO: Undo step: 57
2024-12-01-20:56:27-root-INFO: Undo step: 58
2024-12-01-20:56:27-root-INFO: Undo step: 59
2024-12-01-20:56:27-root-INFO: step: 60 lr_xt 0.21117946
2024-12-01-20:56:27-root-INFO: grad norm: 25.867 25.561 3.968
2024-12-01-20:56:28-root-INFO: grad norm: 14.176 14.055 1.849
2024-12-01-20:56:28-root-INFO: grad norm: 10.446 10.300 1.738
2024-12-01-20:56:29-root-INFO: grad norm: 9.093 8.989 1.375
2024-12-01-20:56:29-root-INFO: grad norm: 8.169 8.065 1.299
2024-12-01-20:56:29-root-INFO: Loss Change: 179.688 -> 82.931
2024-12-01-20:56:29-root-INFO: Regularization Change: 0.000 -> 72.582
2024-12-01-20:56:29-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-01-20:56:29-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-20:56:30-root-INFO: step: 59 lr_xt 0.21581084
2024-12-01-20:56:30-root-INFO: grad norm: 8.949 8.813 1.552
2024-12-01-20:56:30-root-INFO: grad norm: 9.277 9.138 1.603
2024-12-01-20:56:31-root-INFO: grad norm: 11.233 11.084 1.825
2024-12-01-20:56:31-root-INFO: Loss too large (81.114->81.732)! Learning rate decreased to 0.17265.
2024-12-01-20:56:31-root-INFO: grad norm: 7.031 6.905 1.325
2024-12-01-20:56:32-root-INFO: grad norm: 4.678 4.619 0.743
2024-12-01-20:56:32-root-INFO: Loss Change: 83.565 -> 73.185
2024-12-01-20:56:32-root-INFO: Regularization Change: 0.000 -> 8.042
2024-12-01-20:56:32-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-01-20:56:32-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-20:56:32-root-INFO: step: 58 lr_xt 0.22050848
2024-12-01-20:56:32-root-INFO: grad norm: 2.949 2.915 0.446
2024-12-01-20:56:33-root-INFO: grad norm: 3.007 2.976 0.435
2024-12-01-20:56:33-root-INFO: grad norm: 3.579 3.555 0.414
2024-12-01-20:56:34-root-INFO: grad norm: 4.564 4.532 0.534
2024-12-01-20:56:34-root-INFO: grad norm: 5.930 5.902 0.578
2024-12-01-20:56:35-root-INFO: Loss Change: 72.827 -> 70.393
2024-12-01-20:56:35-root-INFO: Regularization Change: 0.000 -> 5.671
2024-12-01-20:56:35-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-01-20:56:35-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-20:56:35-root-INFO: step: 57 lr_xt 0.22527231
2024-12-01-20:56:35-root-INFO: grad norm: 5.108 5.069 0.627
2024-12-01-20:56:35-root-INFO: grad norm: 4.643 4.603 0.607
2024-12-01-20:56:36-root-INFO: grad norm: 4.838 4.793 0.658
2024-12-01-20:56:36-root-INFO: grad norm: 5.256 5.198 0.781
2024-12-01-20:56:37-root-INFO: grad norm: 5.695 5.636 0.816
2024-12-01-20:56:37-root-INFO: Loss Change: 70.158 -> 66.939
2024-12-01-20:56:37-root-INFO: Regularization Change: 0.000 -> 4.761
2024-12-01-20:56:37-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-01-20:56:37-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-20:56:37-root-INFO: step: 56 lr_xt 0.23010221
2024-12-01-20:56:37-root-INFO: grad norm: 5.895 5.837 0.823
2024-12-01-20:56:38-root-INFO: grad norm: 6.540 6.483 0.865
2024-12-01-20:56:38-root-INFO: Loss too large (66.409->66.657)! Learning rate decreased to 0.18408.
2024-12-01-20:56:39-root-INFO: grad norm: 4.887 4.833 0.720
2024-12-01-20:56:39-root-INFO: grad norm: 4.070 4.027 0.592
2024-12-01-20:56:40-root-INFO: grad norm: 3.868 3.823 0.585
2024-12-01-20:56:40-root-INFO: Loss Change: 66.655 -> 63.125
2024-12-01-20:56:40-root-INFO: Regularization Change: 0.000 -> 2.785
2024-12-01-20:56:40-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-01-20:56:40-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-20:56:40-root-INFO: step: 55 lr_xt 0.23499803
2024-12-01-20:56:40-root-INFO: grad norm: 5.148 5.063 0.936
2024-12-01-20:56:40-root-INFO: Loss too large (63.471->64.441)! Learning rate decreased to 0.18800.
2024-12-01-20:56:41-root-INFO: grad norm: 4.544 4.479 0.767
2024-12-01-20:56:41-root-INFO: grad norm: 3.819 3.765 0.641
2024-12-01-20:56:42-root-INFO: grad norm: 3.939 3.888 0.632
2024-12-01-20:56:42-root-INFO: grad norm: 4.490 4.445 0.636
2024-12-01-20:56:42-root-INFO: Loss too large (61.666->61.812)! Learning rate decreased to 0.15040.
2024-12-01-20:56:43-root-INFO: Loss Change: 63.471 -> 61.293
2024-12-01-20:56:43-root-INFO: Regularization Change: 0.000 -> 2.113
2024-12-01-20:56:43-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-01-20:56:43-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-20:56:43-root-INFO: step: 54 lr_xt 0.23995961
2024-12-01-20:56:43-root-INFO: grad norm: 3.246 3.217 0.429
2024-12-01-20:56:43-root-INFO: Loss too large (61.001->61.040)! Learning rate decreased to 0.19197.
2024-12-01-20:56:43-root-INFO: grad norm: 3.677 3.646 0.474
2024-12-01-20:56:44-root-INFO: Loss too large (60.542->60.588)! Learning rate decreased to 0.15357.
2024-12-01-20:56:44-root-INFO: grad norm: 3.100 3.070 0.432
2024-12-01-20:56:45-root-INFO: grad norm: 2.508 2.483 0.356
2024-12-01-20:56:45-root-INFO: grad norm: 2.357 2.334 0.333
2024-12-01-20:56:45-root-INFO: Loss Change: 61.001 -> 59.124
2024-12-01-20:56:45-root-INFO: Regularization Change: 0.000 -> 1.392
2024-12-01-20:56:45-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-01-20:56:45-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-20:56:46-root-INFO: step: 53 lr_xt 0.24498673
2024-12-01-20:56:46-root-INFO: grad norm: 3.165 3.116 0.558
2024-12-01-20:56:46-root-INFO: Loss too large (59.273->59.608)! Learning rate decreased to 0.19599.
2024-12-01-20:56:46-root-INFO: grad norm: 3.573 3.531 0.541
2024-12-01-20:56:47-root-INFO: grad norm: 4.487 4.447 0.600
2024-12-01-20:56:47-root-INFO: Loss too large (58.927->59.235)! Learning rate decreased to 0.15679.
2024-12-01-20:56:47-root-INFO: grad norm: 3.633 3.594 0.533
2024-12-01-20:56:48-root-INFO: grad norm: 2.678 2.652 0.373
2024-12-01-20:56:48-root-INFO: Loss Change: 59.273 -> 57.686
2024-12-01-20:56:48-root-INFO: Regularization Change: 0.000 -> 1.436
2024-12-01-20:56:48-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-01-20:56:48-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-20:56:48-root-INFO: step: 52 lr_xt 0.25007913
2024-12-01-20:56:49-root-INFO: grad norm: 2.378 2.353 0.350
2024-12-01-20:56:49-root-INFO: grad norm: 3.993 3.969 0.430
2024-12-01-20:56:49-root-INFO: Loss too large (57.458->58.366)! Learning rate decreased to 0.20006.
2024-12-01-20:56:49-root-INFO: Loss too large (57.458->57.665)! Learning rate decreased to 0.16005.
2024-12-01-20:56:50-root-INFO: grad norm: 3.415 3.384 0.461
2024-12-01-20:56:50-root-INFO: grad norm: 2.819 2.797 0.354
2024-12-01-20:56:51-root-INFO: grad norm: 2.648 2.622 0.365
2024-12-01-20:56:51-root-INFO: Loss Change: 57.543 -> 56.077
2024-12-01-20:56:51-root-INFO: Regularization Change: 0.000 -> 1.401
2024-12-01-20:56:51-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-01-20:56:51-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-20:56:51-root-INFO: step: 51 lr_xt 0.25523653
2024-12-01-20:56:51-root-INFO: grad norm: 3.513 3.453 0.646
2024-12-01-20:56:51-root-INFO: Loss too large (56.198->56.822)! Learning rate decreased to 0.20419.
2024-12-01-20:56:52-root-INFO: Loss too large (56.198->56.224)! Learning rate decreased to 0.16335.
2024-12-01-20:56:52-root-INFO: grad norm: 3.098 3.069 0.422
2024-12-01-20:56:53-root-INFO: grad norm: 2.860 2.835 0.376
2024-12-01-20:56:53-root-INFO: grad norm: 2.767 2.742 0.371
2024-12-01-20:56:53-root-INFO: grad norm: 2.686 2.665 0.340
2024-12-01-20:56:54-root-INFO: Loss Change: 56.198 -> 54.727
2024-12-01-20:56:54-root-INFO: Regularization Change: 0.000 -> 1.266
2024-12-01-20:56:54-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-01-20:56:54-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-20:56:54-root-INFO: step: 50 lr_xt 0.26045862
2024-12-01-20:56:54-root-INFO: grad norm: 2.462 2.436 0.358
2024-12-01-20:56:54-root-INFO: Loss too large (54.359->54.392)! Learning rate decreased to 0.20837.
2024-12-01-20:56:55-root-INFO: grad norm: 3.153 3.133 0.354
2024-12-01-20:56:55-root-INFO: Loss too large (54.112->54.225)! Learning rate decreased to 0.16669.
2024-12-01-20:56:55-root-INFO: grad norm: 3.013 2.988 0.388
2024-12-01-20:56:56-root-INFO: grad norm: 2.886 2.864 0.352
2024-12-01-20:56:56-root-INFO: grad norm: 2.848 2.823 0.376
2024-12-01-20:56:56-root-INFO: Loss Change: 54.359 -> 53.051
2024-12-01-20:56:56-root-INFO: Regularization Change: 0.000 -> 1.309
2024-12-01-20:56:56-root-INFO: Undo step: 50
2024-12-01-20:56:56-root-INFO: Undo step: 51
2024-12-01-20:56:56-root-INFO: Undo step: 52
2024-12-01-20:56:56-root-INFO: Undo step: 53
2024-12-01-20:56:56-root-INFO: Undo step: 54
2024-12-01-20:56:57-root-INFO: step: 55 lr_xt 0.23499803
2024-12-01-20:56:57-root-INFO: grad norm: 22.712 22.586 2.385
2024-12-01-20:56:57-root-INFO: grad norm: 10.068 9.847 2.095
2024-12-01-20:56:58-root-INFO: grad norm: 8.940 8.773 1.717
2024-12-01-20:56:58-root-INFO: grad norm: 8.376 7.983 2.536
2024-12-01-20:56:59-root-INFO: grad norm: 6.508 6.389 1.238
2024-12-01-20:56:59-root-INFO: Loss Change: 162.469 -> 71.434
2024-12-01-20:56:59-root-INFO: Regularization Change: 0.000 -> 74.301
2024-12-01-20:56:59-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-01-20:56:59-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-20:56:59-root-INFO: step: 54 lr_xt 0.23995961
2024-12-01-20:56:59-root-INFO: grad norm: 6.039 5.866 1.438
2024-12-01-20:57:00-root-INFO: grad norm: 6.310 6.202 1.162
2024-12-01-20:57:00-root-INFO: grad norm: 7.141 6.989 1.467
2024-12-01-20:57:01-root-INFO: grad norm: 7.481 7.378 1.233
2024-12-01-20:57:01-root-INFO: grad norm: 7.317 7.167 1.474
2024-12-01-20:57:01-root-INFO: Loss Change: 71.279 -> 65.401
2024-12-01-20:57:01-root-INFO: Regularization Change: 0.000 -> 10.867
2024-12-01-20:57:01-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-01-20:57:01-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-20:57:02-root-INFO: step: 53 lr_xt 0.24498673
2024-12-01-20:57:02-root-INFO: grad norm: 7.015 6.934 1.065
2024-12-01-20:57:02-root-INFO: grad norm: 7.704 7.592 1.309
2024-12-01-20:57:02-root-INFO: Loss too large (64.408->64.513)! Learning rate decreased to 0.19599.
2024-12-01-20:57:03-root-INFO: grad norm: 5.363 5.294 0.861
2024-12-01-20:57:03-root-INFO: grad norm: 3.840 3.791 0.611
2024-12-01-20:57:04-root-INFO: grad norm: 3.407 3.361 0.556
2024-12-01-20:57:04-root-INFO: Loss Change: 65.075 -> 59.172
2024-12-01-20:57:04-root-INFO: Regularization Change: 0.000 -> 4.614
2024-12-01-20:57:04-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-01-20:57:04-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-20:57:04-root-INFO: step: 52 lr_xt 0.25007913
2024-12-01-20:57:05-root-INFO: grad norm: 4.099 4.032 0.734
2024-12-01-20:57:05-root-INFO: Loss too large (59.263->59.482)! Learning rate decreased to 0.20006.
2024-12-01-20:57:05-root-INFO: grad norm: 3.956 3.903 0.646
2024-12-01-20:57:06-root-INFO: grad norm: 3.957 3.915 0.572
2024-12-01-20:57:06-root-INFO: grad norm: 3.907 3.861 0.595
2024-12-01-20:57:07-root-INFO: grad norm: 3.836 3.797 0.547
2024-12-01-20:57:07-root-INFO: Loss Change: 59.263 -> 56.914
2024-12-01-20:57:07-root-INFO: Regularization Change: 0.000 -> 3.017
2024-12-01-20:57:07-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-01-20:57:07-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-20:57:07-root-INFO: step: 51 lr_xt 0.25523653
2024-12-01-20:57:07-root-INFO: grad norm: 3.657 3.630 0.446
2024-12-01-20:57:08-root-INFO: grad norm: 5.294 5.262 0.587
2024-12-01-20:57:08-root-INFO: Loss too large (56.588->57.364)! Learning rate decreased to 0.20419.
2024-12-01-20:57:08-root-INFO: grad norm: 4.271 4.226 0.613
2024-12-01-20:57:09-root-INFO: grad norm: 3.056 3.023 0.449
2024-12-01-20:57:09-root-INFO: grad norm: 2.999 2.968 0.429
2024-12-01-20:57:10-root-INFO: Loss Change: 56.630 -> 54.173
2024-12-01-20:57:10-root-INFO: Regularization Change: 0.000 -> 2.745
2024-12-01-20:57:10-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-01-20:57:10-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-20:57:10-root-INFO: step: 50 lr_xt 0.26045862
2024-12-01-20:57:10-root-INFO: grad norm: 4.102 4.034 0.744
2024-12-01-20:57:10-root-INFO: Loss too large (54.084->54.650)! Learning rate decreased to 0.20837.
2024-12-01-20:57:11-root-INFO: grad norm: 3.968 3.918 0.630
2024-12-01-20:57:11-root-INFO: grad norm: 3.856 3.814 0.568
2024-12-01-20:57:12-root-INFO: grad norm: 3.835 3.790 0.580
2024-12-01-20:57:12-root-INFO: grad norm: 3.833 3.793 0.550
2024-12-01-20:57:12-root-INFO: Loss Change: 54.084 -> 52.398
2024-12-01-20:57:12-root-INFO: Regularization Change: 0.000 -> 2.526
2024-12-01-20:57:12-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-01-20:57:12-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-20:57:13-root-INFO: step: 49 lr_xt 0.26574501
2024-12-01-20:57:13-root-INFO: grad norm: 3.576 3.549 0.433
2024-12-01-20:57:13-root-INFO: Loss too large (52.143->52.207)! Learning rate decreased to 0.21260.
2024-12-01-20:57:13-root-INFO: grad norm: 3.547 3.519 0.443
2024-12-01-20:57:14-root-INFO: grad norm: 3.593 3.560 0.482
2024-12-01-20:57:14-root-INFO: grad norm: 3.712 3.680 0.486
2024-12-01-20:57:15-root-INFO: grad norm: 3.720 3.682 0.531
2024-12-01-20:57:15-root-INFO: Loss Change: 52.143 -> 50.309
2024-12-01-20:57:15-root-INFO: Regularization Change: 0.000 -> 2.325
2024-12-01-20:57:15-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-01-20:57:15-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-20:57:15-root-INFO: step: 48 lr_xt 0.27109532
2024-12-01-20:57:16-root-INFO: grad norm: 4.341 4.277 0.743
2024-12-01-20:57:16-root-INFO: Loss too large (50.402->51.175)! Learning rate decreased to 0.21688.
2024-12-01-20:57:16-root-INFO: grad norm: 4.074 4.022 0.654
2024-12-01-20:57:17-root-INFO: grad norm: 3.761 3.719 0.558
2024-12-01-20:57:17-root-INFO: grad norm: 3.717 3.672 0.572
2024-12-01-20:57:18-root-INFO: grad norm: 3.722 3.683 0.535
2024-12-01-20:57:18-root-INFO: Loss Change: 50.402 -> 48.782
2024-12-01-20:57:18-root-INFO: Regularization Change: 0.000 -> 2.414
2024-12-01-20:57:18-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-01-20:57:18-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-20:57:18-root-INFO: step: 47 lr_xt 0.27650911
2024-12-01-20:57:18-root-INFO: grad norm: 3.411 3.386 0.411
2024-12-01-20:57:19-root-INFO: Loss too large (48.507->48.595)! Learning rate decreased to 0.22121.
2024-12-01-20:57:19-root-INFO: grad norm: 3.383 3.356 0.426
2024-12-01-20:57:20-root-INFO: grad norm: 3.461 3.431 0.460
2024-12-01-20:57:20-root-INFO: grad norm: 3.593 3.562 0.472
2024-12-01-20:57:21-root-INFO: grad norm: 3.621 3.585 0.513
2024-12-01-20:57:21-root-INFO: Loss Change: 48.507 -> 46.890
2024-12-01-20:57:21-root-INFO: Regularization Change: 0.000 -> 2.205
2024-12-01-20:57:21-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-01-20:57:21-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-20:57:21-root-INFO: step: 46 lr_xt 0.28198590
2024-12-01-20:57:21-root-INFO: grad norm: 4.373 4.308 0.751
2024-12-01-20:57:21-root-INFO: Loss too large (47.068->47.924)! Learning rate decreased to 0.22559.
2024-12-01-20:57:22-root-INFO: grad norm: 4.077 4.023 0.659
2024-12-01-20:57:22-root-INFO: grad norm: 3.726 3.684 0.561
2024-12-01-20:57:23-root-INFO: grad norm: 3.685 3.640 0.574
2024-12-01-20:57:23-root-INFO: grad norm: 3.681 3.641 0.542
2024-12-01-20:57:24-root-INFO: Loss Change: 47.068 -> 45.519
2024-12-01-20:57:24-root-INFO: Regularization Change: 0.000 -> 2.390
2024-12-01-20:57:24-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-01-20:57:24-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-20:57:24-root-INFO: step: 45 lr_xt 0.28752516
2024-12-01-20:57:24-root-INFO: grad norm: 3.383 3.354 0.435
2024-12-01-20:57:24-root-INFO: Loss too large (45.161->45.300)! Learning rate decreased to 0.23002.
2024-12-01-20:57:24-root-INFO: grad norm: 3.322 3.292 0.447
2024-12-01-20:57:25-root-INFO: grad norm: 3.384 3.349 0.481
2024-12-01-20:57:25-root-INFO: grad norm: 3.495 3.461 0.484
2024-12-01-20:57:26-root-INFO: grad norm: 3.532 3.493 0.527
2024-12-01-20:57:26-root-INFO: Loss Change: 45.161 -> 43.640
2024-12-01-20:57:26-root-INFO: Regularization Change: 0.000 -> 2.197
2024-12-01-20:57:26-root-INFO: Undo step: 45
2024-12-01-20:57:26-root-INFO: Undo step: 46
2024-12-01-20:57:26-root-INFO: Undo step: 47
2024-12-01-20:57:26-root-INFO: Undo step: 48
2024-12-01-20:57:26-root-INFO: Undo step: 49
2024-12-01-20:57:26-root-INFO: step: 50 lr_xt 0.26045862
2024-12-01-20:57:26-root-INFO: grad norm: 21.211 21.077 2.377
2024-12-01-20:57:27-root-INFO: grad norm: 10.648 10.546 1.470
2024-12-01-20:57:27-root-INFO: grad norm: 7.822 7.745 1.095
2024-12-01-20:57:28-root-INFO: grad norm: 6.813 6.741 0.993
2024-12-01-20:57:28-root-INFO: grad norm: 6.844 6.783 0.915
2024-12-01-20:57:29-root-INFO: Loss Change: 145.103 -> 62.160
2024-12-01-20:57:29-root-INFO: Regularization Change: 0.000 -> 77.446
2024-12-01-20:57:29-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-01-20:57:29-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-20:57:29-root-INFO: step: 49 lr_xt 0.26574501
2024-12-01-20:57:29-root-INFO: grad norm: 6.895 6.795 1.173
2024-12-01-20:57:30-root-INFO: grad norm: 6.821 6.726 1.135
2024-12-01-20:57:30-root-INFO: grad norm: 7.230 7.044 1.630
2024-12-01-20:57:30-root-INFO: grad norm: 7.582 7.467 1.320
2024-12-01-20:57:31-root-INFO: grad norm: 8.102 7.922 1.699
2024-12-01-20:57:31-root-INFO: Loss Change: 62.279 -> 56.381
2024-12-01-20:57:31-root-INFO: Regularization Change: 0.000 -> 13.337
2024-12-01-20:57:31-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-01-20:57:31-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-20:57:31-root-INFO: step: 48 lr_xt 0.27109532
2024-12-01-20:57:32-root-INFO: grad norm: 6.922 6.824 1.157
2024-12-01-20:57:32-root-INFO: grad norm: 6.234 6.112 1.231
2024-12-01-20:57:33-root-INFO: grad norm: 5.838 5.748 1.025
2024-12-01-20:57:33-root-INFO: grad norm: 6.287 6.193 1.085
2024-12-01-20:57:33-root-INFO: Loss too large (51.469->51.542)! Learning rate decreased to 0.21688.
2024-12-01-20:57:34-root-INFO: grad norm: 4.479 4.402 0.828
2024-12-01-20:57:34-root-INFO: Loss Change: 55.945 -> 48.717
2024-12-01-20:57:34-root-INFO: Regularization Change: 0.000 -> 6.635
2024-12-01-20:57:34-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-01-20:57:34-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-20:57:34-root-INFO: step: 47 lr_xt 0.27650911
2024-12-01-20:57:34-root-INFO: grad norm: 4.612 4.529 0.872
2024-12-01-20:57:35-root-INFO: Loss too large (48.997->49.441)! Learning rate decreased to 0.22121.
2024-12-01-20:57:35-root-INFO: grad norm: 4.113 4.051 0.708
2024-12-01-20:57:35-root-INFO: grad norm: 3.803 3.759 0.575
2024-12-01-20:57:36-root-INFO: grad norm: 3.762 3.714 0.599
2024-12-01-20:57:36-root-INFO: grad norm: 3.784 3.747 0.524
2024-12-01-20:57:37-root-INFO: Loss Change: 48.997 -> 46.557
2024-12-01-20:57:37-root-INFO: Regularization Change: 0.000 -> 3.352
2024-12-01-20:57:37-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-01-20:57:37-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-20:57:37-root-INFO: step: 46 lr_xt 0.28198590
2024-12-01-20:57:37-root-INFO: grad norm: 3.420 3.393 0.433
2024-12-01-20:57:38-root-INFO: grad norm: 5.046 5.015 0.558
2024-12-01-20:57:38-root-INFO: Loss too large (46.103->47.056)! Learning rate decreased to 0.22559.
2024-12-01-20:57:38-root-INFO: grad norm: 4.085 4.037 0.624
2024-12-01-20:57:39-root-INFO: grad norm: 2.794 2.765 0.400
2024-12-01-20:57:39-root-INFO: grad norm: 2.780 2.750 0.405
2024-12-01-20:57:39-root-INFO: Loss Change: 46.228 -> 43.873
2024-12-01-20:57:39-root-INFO: Regularization Change: 0.000 -> 2.848
2024-12-01-20:57:39-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-01-20:57:39-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-20:57:40-root-INFO: step: 45 lr_xt 0.28752516
2024-12-01-20:57:40-root-INFO: grad norm: 3.864 3.803 0.688
2024-12-01-20:57:40-root-INFO: Loss too large (43.948->44.493)! Learning rate decreased to 0.23002.
2024-12-01-20:57:40-root-INFO: grad norm: 3.734 3.687 0.595
2024-12-01-20:57:41-root-INFO: grad norm: 3.671 3.636 0.508
2024-12-01-20:57:41-root-INFO: grad norm: 3.614 3.573 0.545
2024-12-01-20:57:42-root-INFO: grad norm: 3.538 3.506 0.473
2024-12-01-20:57:42-root-INFO: Loss Change: 43.948 -> 42.340
2024-12-01-20:57:42-root-INFO: Regularization Change: 0.000 -> 2.588
2024-12-01-20:57:42-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-01-20:57:42-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-20:57:42-root-INFO: step: 44 lr_xt 0.29312635
2024-12-01-20:57:42-root-INFO: grad norm: 3.241 3.217 0.387
2024-12-01-20:57:43-root-INFO: Loss too large (42.089->42.146)! Learning rate decreased to 0.23450.
2024-12-01-20:57:43-root-INFO: grad norm: 3.214 3.193 0.370
2024-12-01-20:57:44-root-INFO: grad norm: 3.265 3.236 0.434
2024-12-01-20:57:44-root-INFO: grad norm: 3.338 3.314 0.405
2024-12-01-20:57:44-root-INFO: grad norm: 3.360 3.327 0.467
2024-12-01-20:57:45-root-INFO: Loss Change: 42.089 -> 40.430
2024-12-01-20:57:45-root-INFO: Regularization Change: 0.000 -> 2.332
2024-12-01-20:57:45-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-01-20:57:45-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-20:57:45-root-INFO: step: 43 lr_xt 0.29878886
2024-12-01-20:57:45-root-INFO: grad norm: 4.154 4.093 0.710
2024-12-01-20:57:45-root-INFO: Loss too large (40.469->41.208)! Learning rate decreased to 0.23903.
2024-12-01-20:57:46-root-INFO: grad norm: 3.833 3.784 0.615
2024-12-01-20:57:46-root-INFO: grad norm: 3.464 3.429 0.486
2024-12-01-20:57:47-root-INFO: grad norm: 3.376 3.339 0.501
2024-12-01-20:57:47-root-INFO: grad norm: 3.308 3.279 0.435
2024-12-01-20:57:47-root-INFO: Loss Change: 40.469 -> 38.819
2024-12-01-20:57:47-root-INFO: Regularization Change: 0.000 -> 2.434
2024-12-01-20:57:47-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-01-20:57:47-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-20:57:48-root-INFO: step: 42 lr_xt 0.30451205
2024-12-01-20:57:48-root-INFO: grad norm: 3.049 3.028 0.360
2024-12-01-20:57:48-root-INFO: Loss too large (38.558->38.665)! Learning rate decreased to 0.24361.
2024-12-01-20:57:48-root-INFO: grad norm: 3.011 2.992 0.337
2024-12-01-20:57:49-root-INFO: grad norm: 3.058 3.031 0.403
2024-12-01-20:57:49-root-INFO: grad norm: 3.128 3.107 0.368
2024-12-01-20:57:50-root-INFO: grad norm: 3.158 3.129 0.432
2024-12-01-20:57:50-root-INFO: Loss Change: 38.558 -> 37.133
2024-12-01-20:57:50-root-INFO: Regularization Change: 0.000 -> 2.159
2024-12-01-20:57:50-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-01-20:57:50-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-20:57:50-root-INFO: step: 41 lr_xt 0.31082203
2024-12-01-20:57:50-root-INFO: grad norm: 4.260 4.187 0.786
2024-12-01-20:57:51-root-INFO: Loss too large (37.429->38.145)! Learning rate decreased to 0.24866.
2024-12-01-20:57:51-root-INFO: grad norm: 3.811 3.760 0.620
2024-12-01-20:57:52-root-INFO: grad norm: 3.359 3.326 0.466
2024-12-01-20:57:52-root-INFO: grad norm: 3.208 3.174 0.469
2024-12-01-20:57:52-root-INFO: grad norm: 3.093 3.068 0.396
2024-12-01-20:57:53-root-INFO: Loss Change: 37.429 -> 35.650
2024-12-01-20:57:53-root-INFO: Regularization Change: 0.000 -> 2.416
2024-12-01-20:57:53-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-01-20:57:53-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-20:57:53-root-INFO: step: 40 lr_xt 0.31666177
2024-12-01-20:57:53-root-INFO: grad norm: 2.680 2.663 0.297
2024-12-01-20:57:54-root-INFO: grad norm: 3.717 3.700 0.358
2024-12-01-20:57:54-root-INFO: Loss too large (35.329->35.872)! Learning rate decreased to 0.25333.
2024-12-01-20:57:54-root-INFO: grad norm: 3.369 3.342 0.429
2024-12-01-20:57:55-root-INFO: grad norm: 2.964 2.944 0.338
2024-12-01-20:57:55-root-INFO: grad norm: 2.846 2.821 0.376
2024-12-01-20:57:55-root-INFO: Loss Change: 35.336 -> 33.980
2024-12-01-20:57:55-root-INFO: Regularization Change: 0.000 -> 2.189
2024-12-01-20:57:55-root-INFO: Undo step: 40
2024-12-01-20:57:55-root-INFO: Undo step: 41
2024-12-01-20:57:55-root-INFO: Undo step: 42
2024-12-01-20:57:55-root-INFO: Undo step: 43
2024-12-01-20:57:55-root-INFO: Undo step: 44
2024-12-01-20:57:56-root-INFO: step: 45 lr_xt 0.28752516
2024-12-01-20:57:56-root-INFO: grad norm: 20.649 20.522 2.280
2024-12-01-20:57:56-root-INFO: grad norm: 11.195 11.080 1.600
2024-12-01-20:57:57-root-INFO: grad norm: 7.632 7.560 1.045
2024-12-01-20:57:57-root-INFO: grad norm: 5.855 5.774 0.967
2024-12-01-20:57:58-root-INFO: grad norm: 4.924 4.859 0.801
2024-12-01-20:57:58-root-INFO: Loss Change: 139.878 -> 51.229
2024-12-01-20:57:58-root-INFO: Regularization Change: 0.000 -> 87.229
2024-12-01-20:57:58-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-01-20:57:58-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-20:57:58-root-INFO: step: 44 lr_xt 0.29312635
2024-12-01-20:57:58-root-INFO: grad norm: 5.336 5.238 1.017
2024-12-01-20:57:59-root-INFO: grad norm: 5.575 5.477 1.039
2024-12-01-20:57:59-root-INFO: grad norm: 6.486 6.394 1.091
2024-12-01-20:58:00-root-INFO: grad norm: 5.580 5.460 1.149
2024-12-01-20:58:00-root-INFO: grad norm: 5.115 5.011 1.023
2024-12-01-20:58:00-root-INFO: Loss Change: 51.367 -> 45.073
2024-12-01-20:58:00-root-INFO: Regularization Change: 0.000 -> 11.509
2024-12-01-20:58:00-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-01-20:58:00-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-20:58:01-root-INFO: step: 43 lr_xt 0.29878886
2024-12-01-20:58:01-root-INFO: grad norm: 4.502 4.431 0.793
2024-12-01-20:58:01-root-INFO: grad norm: 4.352 4.288 0.746
2024-12-01-20:58:02-root-INFO: grad norm: 4.432 4.368 0.751
2024-12-01-20:58:02-root-INFO: grad norm: 4.605 4.548 0.724
2024-12-01-20:58:03-root-INFO: grad norm: 5.032 4.979 0.726
2024-12-01-20:58:03-root-INFO: Loss Change: 44.414 -> 41.375
2024-12-01-20:58:03-root-INFO: Regularization Change: 0.000 -> 7.151
2024-12-01-20:58:03-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-01-20:58:03-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-20:58:03-root-INFO: step: 42 lr_xt 0.30451205
2024-12-01-20:58:04-root-INFO: grad norm: 5.339 5.268 0.871
2024-12-01-20:58:04-root-INFO: grad norm: 5.109 5.028 0.908
2024-12-01-20:58:04-root-INFO: grad norm: 5.300 5.231 0.858
2024-12-01-20:58:05-root-INFO: Loss too large (39.942->39.979)! Learning rate decreased to 0.24361.
2024-12-01-20:58:05-root-INFO: grad norm: 3.788 3.737 0.617
2024-12-01-20:58:06-root-INFO: grad norm: 2.790 2.763 0.389
2024-12-01-20:58:06-root-INFO: Loss Change: 41.467 -> 37.217
2024-12-01-20:58:06-root-INFO: Regularization Change: 0.000 -> 4.263
2024-12-01-20:58:06-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-01-20:58:06-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-20:58:06-root-INFO: step: 41 lr_xt 0.31082203
2024-12-01-20:58:06-root-INFO: grad norm: 2.781 2.749 0.421
2024-12-01-20:58:07-root-INFO: grad norm: 3.078 3.052 0.399
2024-12-01-20:58:07-root-INFO: grad norm: 3.555 3.535 0.378
2024-12-01-20:58:07-root-INFO: Loss too large (36.462->36.485)! Learning rate decreased to 0.24866.
2024-12-01-20:58:08-root-INFO: grad norm: 2.864 2.842 0.351
2024-12-01-20:58:08-root-INFO: grad norm: 2.332 2.317 0.263
2024-12-01-20:58:09-root-INFO: Loss Change: 37.051 -> 35.006
2024-12-01-20:58:09-root-INFO: Regularization Change: 0.000 -> 2.946
2024-12-01-20:58:09-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-01-20:58:09-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-20:58:09-root-INFO: step: 40 lr_xt 0.31666177
2024-12-01-20:58:09-root-INFO: grad norm: 2.164 2.146 0.281
2024-12-01-20:58:09-root-INFO: grad norm: 2.661 2.650 0.238
2024-12-01-20:58:10-root-INFO: grad norm: 2.980 2.965 0.290
2024-12-01-20:58:11-root-INFO: grad norm: 3.366 3.354 0.293
2024-12-01-20:58:11-root-INFO: Loss too large (34.088->34.176)! Learning rate decreased to 0.25333.
2024-12-01-20:58:11-root-INFO: grad norm: 2.841 2.823 0.320
2024-12-01-20:58:11-root-INFO: Loss Change: 34.775 -> 33.174
2024-12-01-20:58:11-root-INFO: Regularization Change: 0.000 -> 2.772
2024-12-01-20:58:11-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-01-20:58:11-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-20:58:12-root-INFO: step: 39 lr_xt 0.32255964
2024-12-01-20:58:12-root-INFO: grad norm: 3.134 3.092 0.511
2024-12-01-20:58:12-root-INFO: Loss too large (33.165->33.377)! Learning rate decreased to 0.25805.
2024-12-01-20:58:12-root-INFO: grad norm: 2.934 2.902 0.432
2024-12-01-20:58:13-root-INFO: grad norm: 2.850 2.827 0.359
2024-12-01-20:58:13-root-INFO: grad norm: 2.788 2.760 0.395
2024-12-01-20:58:14-root-INFO: grad norm: 2.717 2.697 0.334
2024-12-01-20:58:14-root-INFO: Loss Change: 33.165 -> 31.690
2024-12-01-20:58:14-root-INFO: Regularization Change: 0.000 -> 2.257
2024-12-01-20:58:14-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-01-20:58:14-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-20:58:14-root-INFO: step: 38 lr_xt 0.32851483
2024-12-01-20:58:14-root-INFO: grad norm: 2.438 2.424 0.264
2024-12-01-20:58:15-root-INFO: grad norm: 3.238 3.224 0.301
2024-12-01-20:58:15-root-INFO: Loss too large (31.250->31.558)! Learning rate decreased to 0.26281.
2024-12-01-20:58:16-root-INFO: grad norm: 2.950 2.929 0.356
2024-12-01-20:58:16-root-INFO: grad norm: 2.657 2.640 0.297
2024-12-01-20:58:17-root-INFO: grad norm: 2.551 2.530 0.327
2024-12-01-20:58:17-root-INFO: Loss Change: 31.340 -> 29.993
2024-12-01-20:58:17-root-INFO: Regularization Change: 0.000 -> 2.189
2024-12-01-20:58:17-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-01-20:58:17-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-20:58:17-root-INFO: step: 37 lr_xt 0.33452649
2024-12-01-20:58:17-root-INFO: grad norm: 3.248 3.197 0.573
2024-12-01-20:58:17-root-INFO: Loss too large (30.093->30.345)! Learning rate decreased to 0.26762.
2024-12-01-20:58:18-root-INFO: grad norm: 2.930 2.894 0.456
2024-12-01-20:58:18-root-INFO: grad norm: 2.705 2.682 0.353
2024-12-01-20:58:19-root-INFO: grad norm: 2.574 2.548 0.366
2024-12-01-20:58:19-root-INFO: grad norm: 2.475 2.456 0.307
2024-12-01-20:58:20-root-INFO: Loss Change: 30.093 -> 28.579
2024-12-01-20:58:20-root-INFO: Regularization Change: 0.000 -> 2.127
2024-12-01-20:58:20-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-01-20:58:20-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-20:58:20-root-INFO: step: 36 lr_xt 0.34059371
2024-12-01-20:58:20-root-INFO: grad norm: 2.162 2.151 0.225
2024-12-01-20:58:21-root-INFO: grad norm: 2.859 2.846 0.268
2024-12-01-20:58:21-root-INFO: Loss too large (28.226->28.442)! Learning rate decreased to 0.27247.
2024-12-01-20:58:21-root-INFO: grad norm: 2.656 2.637 0.318
2024-12-01-20:58:22-root-INFO: grad norm: 2.473 2.458 0.277
2024-12-01-20:58:22-root-INFO: grad norm: 2.379 2.359 0.306
2024-12-01-20:58:23-root-INFO: Loss Change: 28.302 -> 27.148
2024-12-01-20:58:23-root-INFO: Regularization Change: 0.000 -> 2.019
2024-12-01-20:58:23-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-01-20:58:23-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-20:58:23-root-INFO: step: 35 lr_xt 0.34671555
2024-12-01-20:58:23-root-INFO: grad norm: 3.035 2.988 0.531
2024-12-01-20:58:23-root-INFO: Loss too large (27.258->27.490)! Learning rate decreased to 0.27737.
2024-12-01-20:58:24-root-INFO: grad norm: 2.755 2.721 0.429
2024-12-01-20:58:24-root-INFO: grad norm: 2.553 2.531 0.332
2024-12-01-20:58:24-root-INFO: grad norm: 2.427 2.402 0.343
2024-12-01-20:58:25-root-INFO: grad norm: 2.332 2.314 0.288
2024-12-01-20:58:25-root-INFO: Loss Change: 27.258 -> 25.874
2024-12-01-20:58:25-root-INFO: Regularization Change: 0.000 -> 2.010
2024-12-01-20:58:25-root-INFO: Undo step: 35
2024-12-01-20:58:25-root-INFO: Undo step: 36
2024-12-01-20:58:25-root-INFO: Undo step: 37
2024-12-01-20:58:25-root-INFO: Undo step: 38
2024-12-01-20:58:25-root-INFO: Undo step: 39
2024-12-01-20:58:25-root-INFO: step: 40 lr_xt 0.31666177
2024-12-01-20:58:26-root-INFO: grad norm: 16.939 16.829 1.929
2024-12-01-20:58:26-root-INFO: grad norm: 8.767 8.687 1.186
2024-12-01-20:58:27-root-INFO: grad norm: 6.589 6.477 1.206
2024-12-01-20:58:27-root-INFO: grad norm: 5.639 5.549 1.005
2024-12-01-20:58:28-root-INFO: grad norm: 5.447 5.344 1.057
2024-12-01-20:58:28-root-INFO: Loss Change: 116.142 -> 42.796
2024-12-01-20:58:28-root-INFO: Regularization Change: 0.000 -> 84.463
2024-12-01-20:58:28-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-01-20:58:28-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-20:58:28-root-INFO: step: 39 lr_xt 0.32255964
2024-12-01-20:58:28-root-INFO: grad norm: 6.540 6.419 1.252
2024-12-01-20:58:29-root-INFO: grad norm: 6.609 6.476 1.323
2024-12-01-20:58:29-root-INFO: grad norm: 6.624 6.517 1.189
2024-12-01-20:58:30-root-INFO: grad norm: 6.456 6.324 1.302
2024-12-01-20:58:30-root-INFO: grad norm: 6.230 6.124 1.147
2024-12-01-20:58:31-root-INFO: Loss Change: 43.193 -> 37.615
2024-12-01-20:58:31-root-INFO: Regularization Change: 0.000 -> 13.888
2024-12-01-20:58:31-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-01-20:58:31-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-20:58:31-root-INFO: step: 38 lr_xt 0.32851483
2024-12-01-20:58:31-root-INFO: grad norm: 5.582 5.495 0.981
2024-12-01-20:58:31-root-INFO: grad norm: 5.577 5.491 0.972
2024-12-01-20:58:32-root-INFO: grad norm: 5.570 5.469 1.059
2024-12-01-20:58:32-root-INFO: grad norm: 5.546 5.457 0.990
2024-12-01-20:58:33-root-INFO: grad norm: 5.520 5.414 1.076
2024-12-01-20:58:33-root-INFO: Loss Change: 36.653 -> 32.834
2024-12-01-20:58:33-root-INFO: Regularization Change: 0.000 -> 8.569
2024-12-01-20:58:33-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-01-20:58:33-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-20:58:33-root-INFO: step: 37 lr_xt 0.33452649
2024-12-01-20:58:33-root-INFO: grad norm: 6.457 6.324 1.301
2024-12-01-20:58:34-root-INFO: grad norm: 6.067 5.934 1.263
2024-12-01-20:58:34-root-INFO: grad norm: 5.488 5.390 1.031
2024-12-01-20:58:35-root-INFO: grad norm: 5.283 5.176 1.055
2024-12-01-20:58:35-root-INFO: grad norm: 5.188 5.099 0.956
2024-12-01-20:58:35-root-INFO: Loss Change: 33.502 -> 30.466
2024-12-01-20:58:36-root-INFO: Regularization Change: 0.000 -> 7.408
2024-12-01-20:58:36-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-01-20:58:36-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-20:58:36-root-INFO: step: 36 lr_xt 0.34059371
2024-12-01-20:58:36-root-INFO: grad norm: 4.572 4.506 0.776
2024-12-01-20:58:36-root-INFO: grad norm: 4.543 4.477 0.777
2024-12-01-20:58:37-root-INFO: grad norm: 4.541 4.465 0.828
2024-12-01-20:58:37-root-INFO: grad norm: 4.552 4.483 0.787
2024-12-01-20:58:38-root-INFO: grad norm: 4.562 4.482 0.851
2024-12-01-20:58:38-root-INFO: Loss Change: 29.767 -> 27.695
2024-12-01-20:58:38-root-INFO: Regularization Change: 0.000 -> 5.377
2024-12-01-20:58:38-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-01-20:58:38-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-20:58:38-root-INFO: step: 35 lr_xt 0.34671555
2024-12-01-20:58:38-root-INFO: grad norm: 5.432 5.323 1.080
2024-12-01-20:58:39-root-INFO: grad norm: 5.191 5.083 1.052
2024-12-01-20:58:39-root-INFO: grad norm: 4.834 4.751 0.892
2024-12-01-20:58:40-root-INFO: grad norm: 4.682 4.592 0.915
2024-12-01-20:58:40-root-INFO: grad norm: 4.598 4.522 0.833
2024-12-01-20:58:41-root-INFO: Loss Change: 28.229 -> 26.286
2024-12-01-20:58:41-root-INFO: Regularization Change: 0.000 -> 5.502
2024-12-01-20:58:41-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-01-20:58:41-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-20:58:41-root-INFO: step: 34 lr_xt 0.35289102
2024-12-01-20:58:41-root-INFO: grad norm: 3.958 3.904 0.655
2024-12-01-20:58:41-root-INFO: grad norm: 3.894 3.842 0.633
2024-12-01-20:58:42-root-INFO: grad norm: 3.894 3.833 0.688
2024-12-01-20:58:42-root-INFO: grad norm: 3.927 3.873 0.650
2024-12-01-20:58:43-root-INFO: grad norm: 3.948 3.883 0.713
2024-12-01-20:58:43-root-INFO: Loss Change: 25.625 -> 24.081
2024-12-01-20:58:43-root-INFO: Regularization Change: 0.000 -> 4.219
2024-12-01-20:58:43-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-01-20:58:43-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-20:58:43-root-INFO: step: 33 lr_xt 0.35911909
2024-12-01-20:58:43-root-INFO: grad norm: 4.783 4.689 0.944
2024-12-01-20:58:44-root-INFO: grad norm: 4.555 4.468 0.888
2024-12-01-20:58:44-root-INFO: grad norm: 4.216 4.148 0.754
2024-12-01-20:58:45-root-INFO: grad norm: 4.087 4.015 0.767
2024-12-01-20:58:45-root-INFO: grad norm: 4.035 3.973 0.703
2024-12-01-20:58:46-root-INFO: Loss Change: 24.351 -> 22.775
2024-12-01-20:58:46-root-INFO: Regularization Change: 0.000 -> 4.534
2024-12-01-20:58:46-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-01-20:58:46-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-20:58:46-root-INFO: step: 32 lr_xt 0.36539868
2024-12-01-20:58:46-root-INFO: grad norm: 3.314 3.278 0.492
2024-12-01-20:58:46-root-INFO: grad norm: 3.300 3.262 0.500
2024-12-01-20:58:47-root-INFO: grad norm: 3.302 3.255 0.554
2024-12-01-20:58:47-root-INFO: grad norm: 3.318 3.277 0.520
2024-12-01-20:58:48-root-INFO: grad norm: 3.368 3.318 0.579
2024-12-01-20:58:48-root-INFO: Loss Change: 22.138 -> 20.929
2024-12-01-20:58:48-root-INFO: Regularization Change: 0.000 -> 3.451
2024-12-01-20:58:48-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-01-20:58:48-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-20:58:48-root-INFO: step: 31 lr_xt 0.37172867
2024-12-01-20:58:48-root-INFO: grad norm: 4.128 4.054 0.779
2024-12-01-20:58:49-root-INFO: Loss too large (21.194->21.199)! Learning rate decreased to 0.29738.
2024-12-01-20:58:49-root-INFO: grad norm: 2.704 2.659 0.490
2024-12-01-20:58:50-root-INFO: grad norm: 1.740 1.717 0.282
2024-12-01-20:58:50-root-INFO: grad norm: 1.345 1.328 0.214
2024-12-01-20:58:50-root-INFO: grad norm: 1.126 1.114 0.161
2024-12-01-20:58:51-root-INFO: Loss Change: 21.194 -> 18.929
2024-12-01-20:58:51-root-INFO: Regularization Change: 0.000 -> 1.954
2024-12-01-20:58:51-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-01-20:58:51-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-20:58:51-root-INFO: step: 30 lr_xt 0.37810791
2024-12-01-20:58:51-root-INFO: grad norm: 1.122 1.098 0.227
2024-12-01-20:58:52-root-INFO: grad norm: 1.033 1.019 0.173
2024-12-01-20:58:52-root-INFO: grad norm: 1.094 1.080 0.173
2024-12-01-20:58:53-root-INFO: grad norm: 1.419 1.404 0.203
2024-12-01-20:58:53-root-INFO: grad norm: 1.488 1.466 0.253
2024-12-01-20:58:53-root-INFO: Loss Change: 18.743 -> 17.807
2024-12-01-20:58:53-root-INFO: Regularization Change: 0.000 -> 2.022
2024-12-01-20:58:53-root-INFO: Undo step: 30
2024-12-01-20:58:53-root-INFO: Undo step: 31
2024-12-01-20:58:53-root-INFO: Undo step: 32
2024-12-01-20:58:53-root-INFO: Undo step: 33
2024-12-01-20:58:53-root-INFO: Undo step: 34
2024-12-01-20:58:53-root-INFO: step: 35 lr_xt 0.34671555
2024-12-01-20:58:54-root-INFO: grad norm: 15.175 15.097 1.530
2024-12-01-20:58:54-root-INFO: grad norm: 8.222 8.162 0.984
2024-12-01-20:58:55-root-INFO: grad norm: 5.640 5.590 0.744
2024-12-01-20:58:55-root-INFO: grad norm: 4.775 4.741 0.567
2024-12-01-20:58:55-root-INFO: grad norm: 4.142 4.112 0.496
2024-12-01-20:58:56-root-INFO: Loss Change: 99.943 -> 33.380
2024-12-01-20:58:56-root-INFO: Regularization Change: 0.000 -> 83.762
2024-12-01-20:58:56-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-01-20:58:56-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-20:58:56-root-INFO: step: 34 lr_xt 0.35289102
2024-12-01-20:58:56-root-INFO: grad norm: 3.955 3.924 0.493
2024-12-01-20:58:57-root-INFO: grad norm: 3.659 3.630 0.457
2024-12-01-20:58:57-root-INFO: grad norm: 3.487 3.464 0.400
2024-12-01-20:58:57-root-INFO: grad norm: 3.324 3.301 0.394
2024-12-01-20:58:58-root-INFO: grad norm: 3.210 3.189 0.367
2024-12-01-20:58:58-root-INFO: Loss Change: 33.152 -> 26.796
2024-12-01-20:58:58-root-INFO: Regularization Change: 0.000 -> 11.768
2024-12-01-20:58:58-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-01-20:58:58-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-20:58:58-root-INFO: step: 33 lr_xt 0.35911909
2024-12-01-20:58:59-root-INFO: grad norm: 3.231 3.208 0.387
2024-12-01-20:58:59-root-INFO: grad norm: 3.098 3.076 0.370
2024-12-01-20:58:59-root-INFO: grad norm: 2.969 2.949 0.346
2024-12-01-20:59:00-root-INFO: grad norm: 2.921 2.899 0.358
2024-12-01-20:59:00-root-INFO: grad norm: 2.895 2.874 0.342
2024-12-01-20:59:01-root-INFO: Loss Change: 26.396 -> 23.494
2024-12-01-20:59:01-root-INFO: Regularization Change: 0.000 -> 6.309
2024-12-01-20:59:01-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-01-20:59:01-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-20:59:01-root-INFO: step: 32 lr_xt 0.36539868
2024-12-01-20:59:01-root-INFO: grad norm: 2.935 2.912 0.365
2024-12-01-20:59:01-root-INFO: grad norm: 2.843 2.823 0.332
2024-12-01-20:59:02-root-INFO: grad norm: 2.808 2.788 0.330
2024-12-01-20:59:02-root-INFO: grad norm: 2.807 2.788 0.326
2024-12-01-20:59:03-root-INFO: grad norm: 2.796 2.774 0.343
2024-12-01-20:59:03-root-INFO: Loss Change: 23.281 -> 21.147
2024-12-01-20:59:03-root-INFO: Regularization Change: 0.000 -> 4.556
2024-12-01-20:59:03-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-01-20:59:03-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-20:59:03-root-INFO: step: 31 lr_xt 0.37172867
2024-12-01-20:59:03-root-INFO: grad norm: 3.038 3.010 0.406
2024-12-01-20:59:04-root-INFO: grad norm: 2.935 2.907 0.404
2024-12-01-20:59:04-root-INFO: grad norm: 2.818 2.795 0.356
2024-12-01-20:59:05-root-INFO: grad norm: 2.805 2.778 0.386
2024-12-01-20:59:05-root-INFO: grad norm: 2.816 2.792 0.362
2024-12-01-20:59:06-root-INFO: Loss Change: 21.014 -> 19.588
2024-12-01-20:59:06-root-INFO: Regularization Change: 0.000 -> 3.869
2024-12-01-20:59:06-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-01-20:59:06-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-20:59:06-root-INFO: step: 30 lr_xt 0.37810791
2024-12-01-20:59:06-root-INFO: grad norm: 2.744 2.722 0.346
2024-12-01-20:59:07-root-INFO: grad norm: 2.701 2.683 0.315
2024-12-01-20:59:07-root-INFO: grad norm: 2.677 2.654 0.347
2024-12-01-20:59:07-root-INFO: grad norm: 2.665 2.647 0.315
2024-12-01-20:59:08-root-INFO: grad norm: 2.670 2.646 0.356
2024-12-01-20:59:08-root-INFO: Loss Change: 19.284 -> 17.875
2024-12-01-20:59:08-root-INFO: Regularization Change: 0.000 -> 3.311
2024-12-01-20:59:08-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-01-20:59:08-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-20:59:08-root-INFO: step: 29 lr_xt 0.38453518
2024-12-01-20:59:09-root-INFO: grad norm: 3.162 3.122 0.496
2024-12-01-20:59:09-root-INFO: grad norm: 3.048 3.009 0.484
2024-12-01-20:59:09-root-INFO: grad norm: 2.917 2.886 0.424
2024-12-01-20:59:10-root-INFO: grad norm: 2.906 2.869 0.463
2024-12-01-20:59:10-root-INFO: grad norm: 2.928 2.894 0.444
2024-12-01-20:59:11-root-INFO: Loss Change: 17.910 -> 16.898
2024-12-01-20:59:11-root-INFO: Regularization Change: 0.000 -> 3.350
2024-12-01-20:59:11-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-01-20:59:11-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-20:59:11-root-INFO: step: 28 lr_xt 0.39100924
2024-12-01-20:59:11-root-INFO: grad norm: 2.579 2.557 0.331
2024-12-01-20:59:12-root-INFO: grad norm: 2.560 2.539 0.332
2024-12-01-20:59:12-root-INFO: grad norm: 2.577 2.549 0.379
2024-12-01-20:59:13-root-INFO: grad norm: 2.626 2.600 0.369
2024-12-01-20:59:13-root-INFO: grad norm: 2.656 2.623 0.419
2024-12-01-20:59:13-root-INFO: Loss Change: 16.364 -> 15.306
2024-12-01-20:59:13-root-INFO: Regularization Change: 0.000 -> 2.859
2024-12-01-20:59:13-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-01-20:59:13-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-20:59:14-root-INFO: step: 27 lr_xt 0.39752879
2024-12-01-20:59:14-root-INFO: grad norm: 3.380 3.325 0.610
2024-12-01-20:59:14-root-INFO: Loss too large (15.523->15.556)! Learning rate decreased to 0.31802.
2024-12-01-20:59:14-root-INFO: grad norm: 2.261 2.227 0.390
2024-12-01-20:59:15-root-INFO: grad norm: 1.446 1.428 0.232
2024-12-01-20:59:15-root-INFO: grad norm: 1.141 1.126 0.180
2024-12-01-20:59:16-root-INFO: grad norm: 0.976 0.966 0.139
2024-12-01-20:59:16-root-INFO: Loss Change: 15.523 -> 13.833
2024-12-01-20:59:16-root-INFO: Regularization Change: 0.000 -> 1.656
2024-12-01-20:59:16-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-01-20:59:16-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-20:59:16-root-INFO: step: 26 lr_xt 0.40409250
2024-12-01-20:59:16-root-INFO: grad norm: 1.053 1.031 0.218
2024-12-01-20:59:17-root-INFO: grad norm: 0.904 0.890 0.154
2024-12-01-20:59:17-root-INFO: grad norm: 0.907 0.896 0.140
2024-12-01-20:59:18-root-INFO: grad norm: 0.987 0.977 0.140
2024-12-01-20:59:18-root-INFO: grad norm: 1.117 1.106 0.156
2024-12-01-20:59:19-root-INFO: Loss Change: 13.679 -> 12.885
2024-12-01-20:59:19-root-INFO: Regularization Change: 0.000 -> 1.830
2024-12-01-20:59:19-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-01-20:59:19-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-20:59:19-root-INFO: step: 25 lr_xt 0.41069899
2024-12-01-20:59:19-root-INFO: grad norm: 2.119 2.098 0.294
2024-12-01-20:59:19-root-INFO: Loss too large (12.722->12.784)! Learning rate decreased to 0.32856.
2024-12-01-20:59:20-root-INFO: grad norm: 1.494 1.482 0.195
2024-12-01-20:59:20-root-INFO: grad norm: 1.083 1.076 0.128
2024-12-01-20:59:20-root-INFO: grad norm: 0.944 0.936 0.122
2024-12-01-20:59:21-root-INFO: grad norm: 0.855 0.849 0.101
2024-12-01-20:59:21-root-INFO: Loss Change: 12.722 -> 11.876
2024-12-01-20:59:21-root-INFO: Regularization Change: 0.000 -> 1.204
2024-12-01-20:59:21-root-INFO: Undo step: 25
2024-12-01-20:59:21-root-INFO: Undo step: 26
2024-12-01-20:59:21-root-INFO: Undo step: 27
2024-12-01-20:59:21-root-INFO: Undo step: 28
2024-12-01-20:59:21-root-INFO: Undo step: 29
2024-12-01-20:59:21-root-INFO: step: 30 lr_xt 0.37810791
2024-12-01-20:59:22-root-INFO: grad norm: 13.936 13.885 1.197
2024-12-01-20:59:22-root-INFO: grad norm: 6.839 6.797 0.760
2024-12-01-20:59:23-root-INFO: grad norm: 4.731 4.691 0.611
2024-12-01-20:59:23-root-INFO: grad norm: 4.204 4.170 0.539
2024-12-01-20:59:23-root-INFO: grad norm: 4.218 4.188 0.499
2024-12-01-20:59:24-root-INFO: Loss Change: 88.543 -> 26.937
2024-12-01-20:59:24-root-INFO: Regularization Change: 0.000 -> 85.339
2024-12-01-20:59:24-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-01-20:59:24-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-20:59:24-root-INFO: step: 29 lr_xt 0.38453518
2024-12-01-20:59:24-root-INFO: grad norm: 4.449 4.412 0.577
2024-12-01-20:59:25-root-INFO: grad norm: 4.288 4.251 0.564
2024-12-01-20:59:25-root-INFO: grad norm: 4.162 4.130 0.511
2024-12-01-20:59:25-root-INFO: grad norm: 3.925 3.886 0.550
2024-12-01-20:59:26-root-INFO: grad norm: 3.947 3.910 0.537
2024-12-01-20:59:26-root-INFO: Loss Change: 26.832 -> 20.915
2024-12-01-20:59:26-root-INFO: Regularization Change: 0.000 -> 13.039
2024-12-01-20:59:26-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-01-20:59:26-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-20:59:26-root-INFO: step: 28 lr_xt 0.39100924
2024-12-01-20:59:27-root-INFO: grad norm: 3.462 3.437 0.410
2024-12-01-20:59:27-root-INFO: grad norm: 3.562 3.536 0.431
2024-12-01-20:59:28-root-INFO: grad norm: 3.544 3.506 0.518
2024-12-01-20:59:28-root-INFO: grad norm: 3.557 3.522 0.498
2024-12-01-20:59:28-root-INFO: grad norm: 3.570 3.521 0.590
2024-12-01-20:59:29-root-INFO: Loss Change: 20.141 -> 17.640
2024-12-01-20:59:29-root-INFO: Regularization Change: 0.000 -> 6.850
2024-12-01-20:59:29-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-01-20:59:29-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-20:59:29-root-INFO: step: 27 lr_xt 0.39752879
2024-12-01-20:59:29-root-INFO: grad norm: 4.351 4.278 0.795
2024-12-01-20:59:30-root-INFO: grad norm: 4.194 4.115 0.810
2024-12-01-20:59:30-root-INFO: grad norm: 3.938 3.875 0.705
2024-12-01-20:59:31-root-INFO: grad norm: 3.870 3.794 0.762
2024-12-01-20:59:31-root-INFO: grad norm: 3.978 3.912 0.721
2024-12-01-20:59:31-root-INFO: Loss Change: 17.931 -> 16.248
2024-12-01-20:59:31-root-INFO: Regularization Change: 0.000 -> 5.958
2024-12-01-20:59:31-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-01-20:59:31-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-20:59:32-root-INFO: step: 26 lr_xt 0.40409250
2024-12-01-20:59:32-root-INFO: grad norm: 3.402 3.352 0.583
2024-12-01-20:59:32-root-INFO: grad norm: 3.305 3.259 0.553
2024-12-01-20:59:33-root-INFO: grad norm: 3.295 3.237 0.613
2024-12-01-20:59:33-root-INFO: grad norm: 3.366 3.315 0.579
2024-12-01-20:59:34-root-INFO: grad norm: 3.367 3.306 0.638
2024-12-01-20:59:34-root-INFO: Loss Change: 15.568 -> 14.068
2024-12-01-20:59:34-root-INFO: Regularization Change: 0.000 -> 4.344
2024-12-01-20:59:34-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-01-20:59:34-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-20:59:34-root-INFO: step: 25 lr_xt 0.41069899
2024-12-01-20:59:34-root-INFO: grad norm: 4.096 4.013 0.822
2024-12-01-20:59:35-root-INFO: grad norm: 3.813 3.736 0.761
2024-12-01-20:59:35-root-INFO: grad norm: 3.428 3.371 0.621
2024-12-01-20:59:36-root-INFO: grad norm: 3.250 3.190 0.623
2024-12-01-20:59:36-root-INFO: grad norm: 3.198 3.148 0.561
2024-12-01-20:59:37-root-INFO: Loss Change: 14.318 -> 12.808
2024-12-01-20:59:37-root-INFO: Regularization Change: 0.000 -> 4.219
2024-12-01-20:59:37-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-01-20:59:37-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-20:59:37-root-INFO: step: 24 lr_xt 0.41734684
2024-12-01-20:59:37-root-INFO: grad norm: 2.587 2.558 0.383
2024-12-01-20:59:37-root-INFO: grad norm: 2.486 2.458 0.372
2024-12-01-20:59:38-root-INFO: grad norm: 2.459 2.425 0.412
2024-12-01-20:59:38-root-INFO: grad norm: 2.654 2.623 0.407
2024-12-01-20:59:38-root-INFO: Loss too large (11.588->11.652)! Learning rate decreased to 0.33388.
2024-12-01-20:59:39-root-INFO: grad norm: 1.808 1.784 0.295
2024-12-01-20:59:39-root-INFO: Loss Change: 12.310 -> 10.873
2024-12-01-20:59:39-root-INFO: Regularization Change: 0.000 -> 2.109
2024-12-01-20:59:39-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-01-20:59:39-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-20:59:39-root-INFO: step: 23 lr_xt 0.42403458
2024-12-01-20:59:40-root-INFO: grad norm: 1.943 1.900 0.408
2024-12-01-20:59:40-root-INFO: grad norm: 1.880 1.845 0.360
2024-12-01-20:59:41-root-INFO: grad norm: 2.038 2.009 0.343
2024-12-01-20:59:41-root-INFO: Loss too large (10.485->10.494)! Learning rate decreased to 0.33923.
2024-12-01-20:59:41-root-INFO: grad norm: 1.507 1.486 0.248
2024-12-01-20:59:42-root-INFO: grad norm: 1.080 1.069 0.154
2024-12-01-20:59:42-root-INFO: Loss Change: 10.851 -> 9.837
2024-12-01-20:59:42-root-INFO: Regularization Change: 0.000 -> 1.556
2024-12-01-20:59:42-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-01-20:59:42-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-20:59:42-root-INFO: step: 22 lr_xt 0.43076069
2024-12-01-20:59:42-root-INFO: grad norm: 0.983 0.966 0.179
2024-12-01-20:59:43-root-INFO: grad norm: 0.987 0.979 0.123
2024-12-01-20:59:43-root-INFO: grad norm: 0.952 0.946 0.103
2024-12-01-20:59:44-root-INFO: grad norm: 0.931 0.926 0.094
2024-12-01-20:59:44-root-INFO: grad norm: 0.976 0.970 0.104
2024-12-01-20:59:44-root-INFO: Loss Change: 9.695 -> 9.055
2024-12-01-20:59:44-root-INFO: Regularization Change: 0.000 -> 1.609
2024-12-01-20:59:44-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-01-20:59:44-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-20:59:45-root-INFO: step: 21 lr_xt 0.43752364
2024-12-01-20:59:45-root-INFO: grad norm: 2.387 2.367 0.312
2024-12-01-20:59:45-root-INFO: Loss too large (9.030->9.115)! Learning rate decreased to 0.35002.
2024-12-01-20:59:45-root-INFO: grad norm: 1.405 1.393 0.188
2024-12-01-20:59:46-root-INFO: grad norm: 0.947 0.940 0.108
2024-12-01-20:59:46-root-INFO: grad norm: 0.786 0.779 0.102
2024-12-01-20:59:47-root-INFO: grad norm: 0.694 0.690 0.073
2024-12-01-20:59:47-root-INFO: Loss Change: 9.030 -> 8.277
2024-12-01-20:59:47-root-INFO: Regularization Change: 0.000 -> 1.185
2024-12-01-20:59:47-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-01-20:59:47-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-20:59:47-root-INFO: step: 20 lr_xt 0.44432183
2024-12-01-20:59:47-root-INFO: grad norm: 1.085 1.060 0.233
2024-12-01-20:59:48-root-INFO: grad norm: 0.887 0.872 0.162
2024-12-01-20:59:48-root-INFO: grad norm: 0.866 0.855 0.136
2024-12-01-20:59:49-root-INFO: grad norm: 0.880 0.869 0.140
2024-12-01-20:59:49-root-INFO: grad norm: 0.926 0.917 0.127
2024-12-01-20:59:50-root-INFO: Loss Change: 8.213 -> 7.556
2024-12-01-20:59:50-root-INFO: Regularization Change: 0.000 -> 1.520
2024-12-01-20:59:50-root-INFO: Undo step: 20
2024-12-01-20:59:50-root-INFO: Undo step: 21
2024-12-01-20:59:50-root-INFO: Undo step: 22
2024-12-01-20:59:50-root-INFO: Undo step: 23
2024-12-01-20:59:50-root-INFO: Undo step: 24
2024-12-01-20:59:50-root-INFO: step: 25 lr_xt 0.41069899
2024-12-01-20:59:50-root-INFO: grad norm: 13.504 13.448 1.225
2024-12-01-20:59:50-root-INFO: grad norm: 6.033 5.987 0.743
2024-12-01-20:59:51-root-INFO: grad norm: 4.416 4.384 0.529
2024-12-01-20:59:51-root-INFO: grad norm: 3.758 3.731 0.446
2024-12-01-20:59:52-root-INFO: grad norm: 3.381 3.359 0.387
2024-12-01-20:59:52-root-INFO: Loss Change: 83.282 -> 19.832
2024-12-01-20:59:52-root-INFO: Regularization Change: 0.000 -> 92.589
2024-12-01-20:59:52-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-01-20:59:52-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-20:59:52-root-INFO: step: 24 lr_xt 0.41734684
2024-12-01-20:59:53-root-INFO: grad norm: 3.313 3.294 0.348
2024-12-01-20:59:53-root-INFO: grad norm: 3.033 3.014 0.340
2024-12-01-20:59:53-root-INFO: grad norm: 2.816 2.801 0.287
2024-12-01-20:59:54-root-INFO: grad norm: 2.644 2.626 0.306
2024-12-01-20:59:54-root-INFO: grad norm: 2.494 2.480 0.258
2024-12-01-20:59:55-root-INFO: Loss Change: 19.526 -> 14.412
2024-12-01-20:59:55-root-INFO: Regularization Change: 0.000 -> 11.494
2024-12-01-20:59:55-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-01-20:59:55-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-20:59:55-root-INFO: step: 23 lr_xt 0.42403458
2024-12-01-20:59:55-root-INFO: grad norm: 2.644 2.618 0.373
2024-12-01-20:59:55-root-INFO: grad norm: 2.657 2.638 0.319
2024-12-01-20:59:56-root-INFO: grad norm: 2.555 2.536 0.311
2024-12-01-20:59:56-root-INFO: grad norm: 2.371 2.356 0.268
2024-12-01-20:59:57-root-INFO: grad norm: 2.316 2.300 0.279
2024-12-01-20:59:57-root-INFO: Loss Change: 14.094 -> 11.835
2024-12-01-20:59:57-root-INFO: Regularization Change: 0.000 -> 5.616
2024-12-01-20:59:57-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-01-20:59:57-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-20:59:57-root-INFO: step: 22 lr_xt 0.43076069
2024-12-01-20:59:58-root-INFO: grad norm: 3.345 3.325 0.365
2024-12-01-20:59:58-root-INFO: Loss too large (11.719->11.737)! Learning rate decreased to 0.34461.
2024-12-01-20:59:58-root-INFO: grad norm: 1.987 1.974 0.230
2024-12-01-20:59:59-root-INFO: grad norm: 1.194 1.188 0.121
2024-12-01-20:59:59-root-INFO: grad norm: 0.971 0.965 0.110
2024-12-01-21:00:00-root-INFO: grad norm: 0.878 0.873 0.094
2024-12-01-21:00:00-root-INFO: Loss Change: 11.719 -> 9.972
2024-12-01-21:00:00-root-INFO: Regularization Change: 0.000 -> 2.446
2024-12-01-21:00:00-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-01-21:00:00-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-21:00:00-root-INFO: step: 21 lr_xt 0.43752364
2024-12-01-21:00:00-root-INFO: grad norm: 1.245 1.221 0.244
2024-12-01-21:00:01-root-INFO: grad norm: 1.158 1.142 0.191
2024-12-01-21:00:01-root-INFO: grad norm: 1.282 1.269 0.179
2024-12-01-21:00:02-root-INFO: grad norm: 1.357 1.343 0.200
2024-12-01-21:00:02-root-INFO: grad norm: 1.448 1.434 0.201
2024-12-01-21:00:03-root-INFO: Loss Change: 9.812 -> 8.875
2024-12-01-21:00:03-root-INFO: Regularization Change: 0.000 -> 2.590
2024-12-01-21:00:03-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-01-21:00:03-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-21:00:03-root-INFO: step: 20 lr_xt 0.44432183
2024-12-01-21:00:03-root-INFO: grad norm: 1.842 1.828 0.223
2024-12-01-21:00:04-root-INFO: grad norm: 1.712 1.700 0.202
2024-12-01-21:00:04-root-INFO: grad norm: 1.972 1.964 0.177
2024-12-01-21:00:05-root-INFO: grad norm: 1.788 1.780 0.177
2024-12-01-21:00:05-root-INFO: grad norm: 1.934 1.925 0.177
2024-12-01-21:00:05-root-INFO: Loss Change: 8.693 -> 8.139
2024-12-01-21:00:05-root-INFO: Regularization Change: 0.000 -> 2.369
2024-12-01-21:00:05-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-01-21:00:05-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-21:00:06-root-INFO: step: 19 lr_xt 0.45115363
2024-12-01-21:00:06-root-INFO: grad norm: 2.112 2.091 0.299
2024-12-01-21:00:06-root-INFO: grad norm: 1.906 1.890 0.246
2024-12-01-21:00:07-root-INFO: grad norm: 1.836 1.822 0.224
2024-12-01-21:00:07-root-INFO: grad norm: 1.972 1.958 0.232
2024-12-01-21:00:08-root-INFO: grad norm: 1.992 1.979 0.231
2024-12-01-21:00:08-root-INFO: Loss Change: 8.006 -> 7.214
2024-12-01-21:00:08-root-INFO: Regularization Change: 0.000 -> 2.312
2024-12-01-21:00:08-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-01-21:00:08-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-21:00:08-root-INFO: step: 18 lr_xt 0.45801735
2024-12-01-21:00:09-root-INFO: grad norm: 2.378 2.363 0.262
2024-12-01-21:00:09-root-INFO: Loss too large (7.149->7.155)! Learning rate decreased to 0.36641.
2024-12-01-21:00:09-root-INFO: grad norm: 1.483 1.475 0.158
2024-12-01-21:00:10-root-INFO: grad norm: 0.905 0.901 0.077
2024-12-01-21:00:10-root-INFO: grad norm: 0.732 0.728 0.076
2024-12-01-21:00:11-root-INFO: grad norm: 0.643 0.640 0.060
2024-12-01-21:00:11-root-INFO: Loss Change: 7.149 -> 6.179
2024-12-01-21:00:11-root-INFO: Regularization Change: 0.000 -> 1.132
2024-12-01-21:00:11-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-01-21:00:11-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-21:00:11-root-INFO: step: 17 lr_xt 0.46491129
2024-12-01-21:00:11-root-INFO: grad norm: 1.083 1.061 0.216
2024-12-01-21:00:12-root-INFO: grad norm: 0.969 0.954 0.171
2024-12-01-21:00:12-root-INFO: grad norm: 1.352 1.341 0.172
2024-12-01-21:00:13-root-INFO: Loss too large (5.903->5.941)! Learning rate decreased to 0.37193.
2024-12-01-21:00:13-root-INFO: grad norm: 0.967 0.958 0.132
2024-12-01-21:00:14-root-INFO: grad norm: 0.726 0.722 0.075
2024-12-01-21:00:14-root-INFO: Loss Change: 6.143 -> 5.583
2024-12-01-21:00:14-root-INFO: Regularization Change: 0.000 -> 1.040
2024-12-01-21:00:14-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-01-21:00:14-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-21:00:14-root-INFO: step: 16 lr_xt 0.47183369
2024-12-01-21:00:14-root-INFO: grad norm: 0.976 0.960 0.178
2024-12-01-21:00:15-root-INFO: grad norm: 0.762 0.752 0.127
2024-12-01-21:00:15-root-INFO: grad norm: 0.821 0.814 0.109
2024-12-01-21:00:16-root-INFO: grad norm: 1.106 1.096 0.149
2024-12-01-21:00:16-root-INFO: Loss too large (5.211->5.345)! Learning rate decreased to 0.37747.
2024-12-01-21:00:17-root-INFO: grad norm: 1.736 1.730 0.152
2024-12-01-21:00:17-root-INFO: Loss too large (5.187->5.235)! Learning rate decreased to 0.30197.
2024-12-01-21:00:17-root-INFO: Loss Change: 5.542 -> 5.148
2024-12-01-21:00:17-root-INFO: Regularization Change: 0.000 -> 1.126
2024-12-01-21:00:17-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-01-21:00:17-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-21:00:17-root-INFO: step: 15 lr_xt 0.47878275
2024-12-01-21:00:17-root-INFO: grad norm: 1.214 1.197 0.205
2024-12-01-21:00:18-root-INFO: grad norm: 1.240 1.229 0.165
2024-12-01-21:00:18-root-INFO: grad norm: 1.277 1.268 0.157
2024-12-01-21:00:19-root-INFO: grad norm: 1.501 1.493 0.151
2024-12-01-21:00:19-root-INFO: Loss too large (4.777->4.853)! Learning rate decreased to 0.38303.
2024-12-01-21:00:20-root-INFO: grad norm: 1.107 1.101 0.114
2024-12-01-21:00:20-root-INFO: Loss Change: 5.106 -> 4.532
2024-12-01-21:00:20-root-INFO: Regularization Change: 0.000 -> 1.102
2024-12-01-21:00:20-root-INFO: Undo step: 15
2024-12-01-21:00:20-root-INFO: Undo step: 16
2024-12-01-21:00:20-root-INFO: Undo step: 17
2024-12-01-21:00:20-root-INFO: Undo step: 18
2024-12-01-21:00:20-root-INFO: Undo step: 19
2024-12-01-21:00:20-root-INFO: step: 20 lr_xt 0.44432183
2024-12-01-21:00:20-root-INFO: grad norm: 12.798 12.751 1.099
2024-12-01-21:00:21-root-INFO: grad norm: 5.455 5.422 0.595
2024-12-01-21:00:21-root-INFO: grad norm: 3.790 3.770 0.392
2024-12-01-21:00:22-root-INFO: grad norm: 3.128 3.104 0.383
2024-12-01-21:00:22-root-INFO: grad norm: 2.689 2.673 0.293
2024-12-01-21:00:23-root-INFO: Loss Change: 74.069 -> 14.603
2024-12-01-21:00:23-root-INFO: Regularization Change: 0.000 -> 92.286
2024-12-01-21:00:23-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-01-21:00:23-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-21:00:23-root-INFO: step: 19 lr_xt 0.45115363
2024-12-01-21:00:23-root-INFO: grad norm: 2.725 2.708 0.312
2024-12-01-21:00:23-root-INFO: grad norm: 2.382 2.365 0.277
2024-12-01-21:00:24-root-INFO: grad norm: 2.046 2.036 0.203
2024-12-01-21:00:24-root-INFO: grad norm: 1.986 1.975 0.210
2024-12-01-21:00:25-root-INFO: grad norm: 2.349 2.338 0.227
2024-12-01-21:00:25-root-INFO: Loss Change: 14.225 -> 10.073
2024-12-01-21:00:25-root-INFO: Regularization Change: 0.000 -> 10.789
2024-12-01-21:00:25-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-01-21:00:25-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-21:00:25-root-INFO: step: 18 lr_xt 0.45801735
2024-12-01-21:00:26-root-INFO: grad norm: 2.241 2.222 0.290
2024-12-01-21:00:26-root-INFO: grad norm: 1.827 1.813 0.224
2024-12-01-21:00:27-root-INFO: grad norm: 1.696 1.685 0.198
2024-12-01-21:00:27-root-INFO: grad norm: 1.876 1.862 0.225
2024-12-01-21:00:28-root-INFO: grad norm: 1.854 1.840 0.232
2024-12-01-21:00:28-root-INFO: Loss Change: 9.766 -> 7.616
2024-12-01-21:00:28-root-INFO: Regularization Change: 0.000 -> 4.866
2024-12-01-21:00:28-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-01-21:00:28-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-21:00:28-root-INFO: step: 17 lr_xt 0.46491129
2024-12-01-21:00:28-root-INFO: grad norm: 1.845 1.835 0.185
2024-12-01-21:00:29-root-INFO: grad norm: 1.787 1.778 0.185
2024-12-01-21:00:29-root-INFO: grad norm: 1.632 1.626 0.144
2024-12-01-21:00:30-root-INFO: grad norm: 1.580 1.571 0.171
2024-12-01-21:00:30-root-INFO: grad norm: 1.552 1.545 0.142
2024-12-01-21:00:31-root-INFO: Loss Change: 7.406 -> 6.334
2024-12-01-21:00:31-root-INFO: Regularization Change: 0.000 -> 2.933
2024-12-01-21:00:31-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-01-21:00:31-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-21:00:31-root-INFO: step: 16 lr_xt 0.47183369
2024-12-01-21:00:31-root-INFO: grad norm: 1.743 1.727 0.235
2024-12-01-21:00:32-root-INFO: grad norm: 1.920 1.911 0.188
2024-12-01-21:00:32-root-INFO: grad norm: 1.571 1.560 0.185
2024-12-01-21:00:32-root-INFO: grad norm: 1.503 1.493 0.172
2024-12-01-21:00:33-root-INFO: grad norm: 1.467 1.456 0.175
2024-12-01-21:00:33-root-INFO: Loss Change: 6.191 -> 5.378
2024-12-01-21:00:33-root-INFO: Regularization Change: 0.000 -> 2.178
2024-12-01-21:00:33-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-01-21:00:33-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-21:00:34-root-INFO: step: 15 lr_xt 0.47878275
2024-12-01-21:00:34-root-INFO: grad norm: 1.778 1.766 0.208
2024-12-01-21:00:34-root-INFO: grad norm: 1.499 1.488 0.183
2024-12-01-21:00:35-root-INFO: grad norm: 1.454 1.449 0.126
2024-12-01-21:00:35-root-INFO: grad norm: 1.250 1.242 0.142
2024-12-01-21:00:36-root-INFO: grad norm: 1.202 1.197 0.114
2024-12-01-21:00:36-root-INFO: Loss Change: 5.315 -> 4.602
2024-12-01-21:00:36-root-INFO: Regularization Change: 0.000 -> 1.695
2024-12-01-21:00:36-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-01-21:00:36-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-21:00:36-root-INFO: step: 14 lr_xt 0.48575663
2024-12-01-21:00:36-root-INFO: grad norm: 1.236 1.225 0.162
2024-12-01-21:00:37-root-INFO: grad norm: 1.153 1.147 0.124
2024-12-01-21:00:37-root-INFO: grad norm: 1.134 1.128 0.118
2024-12-01-21:00:38-root-INFO: grad norm: 1.119 1.112 0.118
2024-12-01-21:00:38-root-INFO: grad norm: 1.153 1.147 0.116
2024-12-01-21:00:39-root-INFO: Loss Change: 4.495 -> 4.045
2024-12-01-21:00:39-root-INFO: Regularization Change: 0.000 -> 1.311
2024-12-01-21:00:39-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-01-21:00:39-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-21:00:39-root-INFO: step: 13 lr_xt 0.49275347
2024-12-01-21:00:39-root-INFO: grad norm: 1.841 1.828 0.221
2024-12-01-21:00:39-root-INFO: grad norm: 1.391 1.381 0.170
2024-12-01-21:00:40-root-INFO: grad norm: 1.172 1.166 0.120
2024-12-01-21:00:40-root-INFO: grad norm: 1.089 1.080 0.135
2024-12-01-21:00:41-root-INFO: grad norm: 1.049 1.044 0.106
2024-12-01-21:00:41-root-INFO: Loss Change: 4.117 -> 3.564
2024-12-01-21:00:41-root-INFO: Regularization Change: 0.000 -> 1.220
2024-12-01-21:00:41-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-01-21:00:41-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-21:00:41-root-INFO: step: 12 lr_xt 0.49977135
2024-12-01-21:00:41-root-INFO: grad norm: 1.054 1.047 0.128
2024-12-01-21:00:42-root-INFO: grad norm: 0.950 0.945 0.103
2024-12-01-21:00:42-root-INFO: grad norm: 1.275 1.271 0.104
2024-12-01-21:00:42-root-INFO: Loss too large (3.368->3.375)! Learning rate decreased to 0.39982.
2024-12-01-21:00:43-root-INFO: grad norm: 0.725 0.721 0.079
2024-12-01-21:00:43-root-INFO: grad norm: 0.582 0.580 0.047
2024-12-01-21:00:44-root-INFO: Loss Change: 3.519 -> 3.111
2024-12-01-21:00:44-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-01-21:00:44-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-01-21:00:44-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-21:00:44-root-INFO: step: 11 lr_xt 0.50680833
2024-12-01-21:00:44-root-INFO: grad norm: 1.070 1.053 0.190
2024-12-01-21:00:45-root-INFO: grad norm: 0.988 0.977 0.146
2024-12-01-21:00:45-root-INFO: Loss too large (3.089->3.119)! Learning rate decreased to 0.40545.
2024-12-01-21:00:45-root-INFO: grad norm: 1.106 1.102 0.097
2024-12-01-21:00:46-root-INFO: grad norm: 0.680 0.676 0.073
2024-12-01-21:00:46-root-INFO: grad norm: 0.562 0.561 0.040
2024-12-01-21:00:47-root-INFO: Loss Change: 3.208 -> 2.828
2024-12-01-21:00:47-root-INFO: Regularization Change: 0.000 -> 0.632
2024-12-01-21:00:47-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-01-21:00:47-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-21:00:47-root-INFO: step: 10 lr_xt 0.51386241
2024-12-01-21:00:47-root-INFO: grad norm: 1.147 1.130 0.197
2024-12-01-21:00:47-root-INFO: grad norm: 0.920 0.910 0.134
2024-12-01-21:00:48-root-INFO: grad norm: 0.865 0.858 0.117
2024-12-01-21:00:48-root-INFO: grad norm: 1.001 0.993 0.128
2024-12-01-21:00:48-root-INFO: Loss too large (2.745->2.758)! Learning rate decreased to 0.41109.
2024-12-01-21:00:49-root-INFO: grad norm: 0.956 0.952 0.088
2024-12-01-21:00:49-root-INFO: Loss Change: 2.968 -> 2.649
2024-12-01-21:00:49-root-INFO: Regularization Change: 0.000 -> 0.803
2024-12-01-21:00:49-root-INFO: Undo step: 10
2024-12-01-21:00:49-root-INFO: Undo step: 11
2024-12-01-21:00:49-root-INFO: Undo step: 12
2024-12-01-21:00:49-root-INFO: Undo step: 13
2024-12-01-21:00:49-root-INFO: Undo step: 14
2024-12-01-21:00:49-root-INFO: step: 15 lr_xt 0.47878275
2024-12-01-21:00:50-root-INFO: grad norm: 12.700 12.670 0.876
2024-12-01-21:00:50-root-INFO: grad norm: 4.974 4.955 0.433
2024-12-01-21:00:51-root-INFO: grad norm: 3.188 3.174 0.300
2024-12-01-21:00:51-root-INFO: grad norm: 2.421 2.409 0.241
2024-12-01-21:00:52-root-INFO: grad norm: 1.994 1.984 0.198
2024-12-01-21:00:52-root-INFO: Loss Change: 68.746 -> 10.061
2024-12-01-21:00:52-root-INFO: Regularization Change: 0.000 -> 95.254
2024-12-01-21:00:52-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-01-21:00:52-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-21:00:52-root-INFO: step: 14 lr_xt 0.48575663
2024-12-01-21:00:52-root-INFO: grad norm: 1.889 1.877 0.207
2024-12-01-21:00:53-root-INFO: grad norm: 1.572 1.561 0.179
2024-12-01-21:00:53-root-INFO: grad norm: 1.451 1.444 0.137
2024-12-01-21:00:53-root-INFO: grad norm: 1.311 1.302 0.156
2024-12-01-21:00:54-root-INFO: grad norm: 1.171 1.165 0.120
2024-12-01-21:00:54-root-INFO: Loss Change: 9.660 -> 6.071
2024-12-01-21:00:54-root-INFO: Regularization Change: 0.000 -> 8.755
2024-12-01-21:00:54-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-01-21:00:54-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-21:00:54-root-INFO: step: 13 lr_xt 0.49275347
2024-12-01-21:00:55-root-INFO: grad norm: 1.312 1.302 0.160
2024-12-01-21:00:55-root-INFO: grad norm: 1.154 1.145 0.146
2024-12-01-21:00:56-root-INFO: grad norm: 1.149 1.142 0.120
2024-12-01-21:00:56-root-INFO: grad norm: 1.178 1.169 0.142
2024-12-01-21:00:57-root-INFO: grad norm: 1.475 1.469 0.131
2024-12-01-21:00:57-root-INFO: Loss Change: 5.835 -> 4.627
2024-12-01-21:00:57-root-INFO: Regularization Change: 0.000 -> 3.602
2024-12-01-21:00:57-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-01-21:00:57-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-21:00:57-root-INFO: step: 12 lr_xt 0.49977135
2024-12-01-21:00:57-root-INFO: grad norm: 1.325 1.318 0.135
2024-12-01-21:00:58-root-INFO: grad norm: 1.143 1.137 0.118
2024-12-01-21:00:58-root-INFO: grad norm: 1.053 1.049 0.090
2024-12-01-21:00:59-root-INFO: grad norm: 1.147 1.143 0.091
2024-12-01-21:00:59-root-INFO: grad norm: 1.121 1.117 0.097
2024-12-01-21:00:59-root-INFO: Loss Change: 4.442 -> 3.603
2024-12-01-21:00:59-root-INFO: Regularization Change: 0.000 -> 2.104
2024-12-01-21:00:59-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-01-21:00:59-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-21:01:00-root-INFO: step: 11 lr_xt 0.50680833
2024-12-01-21:01:00-root-INFO: grad norm: 1.667 1.651 0.230
2024-12-01-21:01:00-root-INFO: grad norm: 1.447 1.435 0.187
2024-12-01-21:01:01-root-INFO: grad norm: 1.255 1.246 0.148
2024-12-01-21:01:01-root-INFO: grad norm: 1.265 1.255 0.163
2024-12-01-21:01:02-root-INFO: grad norm: 1.498 1.490 0.153
2024-12-01-21:01:02-root-INFO: Loss too large (3.191->3.219)! Learning rate decreased to 0.40545.
2024-12-01-21:01:02-root-INFO: Loss Change: 3.664 -> 3.098
2024-12-01-21:01:02-root-INFO: Regularization Change: 0.000 -> 1.451
2024-12-01-21:01:02-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-01-21:01:02-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-21:01:02-root-INFO: step: 10 lr_xt 0.51386241
2024-12-01-21:01:02-root-INFO: grad norm: 1.040 1.033 0.122
2024-12-01-21:01:03-root-INFO: grad norm: 0.749 0.746 0.071
2024-12-01-21:01:03-root-INFO: grad norm: 0.662 0.661 0.050
2024-12-01-21:01:04-root-INFO: grad norm: 0.684 0.682 0.053
2024-12-01-21:01:04-root-INFO: grad norm: 0.841 0.838 0.061
2024-12-01-21:01:04-root-INFO: Loss too large (2.646->2.672)! Learning rate decreased to 0.41109.
2024-12-01-21:01:05-root-INFO: Loss Change: 3.060 -> 2.604
2024-12-01-21:01:05-root-INFO: Regularization Change: 0.000 -> 1.015
2024-12-01-21:01:05-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-01-21:01:05-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-21:01:05-root-INFO: step: 9 lr_xt 0.52093157
2024-12-01-21:01:05-root-INFO: grad norm: 1.387 1.373 0.201
2024-12-01-21:01:06-root-INFO: grad norm: 1.018 1.010 0.131
2024-12-01-21:01:06-root-INFO: grad norm: 0.901 0.896 0.098
2024-12-01-21:01:07-root-INFO: grad norm: 0.796 0.790 0.095
2024-12-01-21:01:07-root-INFO: grad norm: 0.763 0.758 0.078
2024-12-01-21:01:07-root-INFO: Loss Change: 2.731 -> 2.327
2024-12-01-21:01:07-root-INFO: Regularization Change: 0.000 -> 0.860
2024-12-01-21:01:07-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-01-21:01:07-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-21:01:07-root-INFO: step: 8 lr_xt 0.52801377
2024-12-01-21:01:08-root-INFO: grad norm: 1.000 0.993 0.119
2024-12-01-21:01:08-root-INFO: grad norm: 0.801 0.798 0.078
2024-12-01-21:01:09-root-INFO: grad norm: 0.757 0.754 0.068
2024-12-01-21:01:09-root-INFO: grad norm: 0.802 0.798 0.075
2024-12-01-21:01:10-root-INFO: grad norm: 0.898 0.896 0.061
2024-12-01-21:01:10-root-INFO: Loss too large (2.163->2.164)! Learning rate decreased to 0.42241.
2024-12-01-21:01:10-root-INFO: Loss Change: 2.387 -> 2.108
2024-12-01-21:01:10-root-INFO: Regularization Change: 0.000 -> 0.692
2024-12-01-21:01:10-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-01-21:01:10-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-21:01:10-root-INFO: step: 7 lr_xt 0.53510690
2024-12-01-21:01:10-root-INFO: grad norm: 1.051 1.043 0.134
2024-12-01-21:01:11-root-INFO: grad norm: 0.830 0.828 0.062
2024-12-01-21:01:11-root-INFO: grad norm: 0.802 0.799 0.061
2024-12-01-21:01:12-root-INFO: grad norm: 0.820 0.818 0.048
2024-12-01-21:01:12-root-INFO: grad norm: 0.780 0.778 0.062
2024-12-01-21:01:13-root-INFO: Loss Change: 2.232 -> 1.942
2024-12-01-21:01:13-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-01-21:01:13-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-01-21:01:13-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-21:01:13-root-INFO: step: 6 lr_xt 0.54220886
2024-12-01-21:01:13-root-INFO: grad norm: 1.175 1.163 0.166
2024-12-01-21:01:13-root-INFO: grad norm: 0.901 0.897 0.094
2024-12-01-21:01:14-root-INFO: grad norm: 0.788 0.785 0.061
2024-12-01-21:01:14-root-INFO: grad norm: 0.725 0.722 0.068
2024-12-01-21:01:15-root-INFO: grad norm: 0.714 0.711 0.058
2024-12-01-21:01:15-root-INFO: Loss Change: 2.115 -> 1.787
2024-12-01-21:01:15-root-INFO: Regularization Change: 0.000 -> 0.735
2024-12-01-21:01:15-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-01-21:01:15-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-21:01:15-root-INFO: step: 5 lr_xt 0.54931747
2024-12-01-21:01:16-root-INFO: grad norm: 0.904 0.899 0.099
2024-12-01-21:01:16-root-INFO: grad norm: 0.579 0.577 0.040
2024-12-01-21:01:17-root-INFO: grad norm: 0.495 0.494 0.039
2024-12-01-21:01:17-root-INFO: grad norm: 0.487 0.486 0.037
2024-12-01-21:01:18-root-INFO: grad norm: 0.485 0.484 0.042
2024-12-01-21:01:18-root-INFO: Loss Change: 1.911 -> 1.636
2024-12-01-21:01:18-root-INFO: Regularization Change: 0.000 -> 0.605
2024-12-01-21:01:18-root-INFO: Undo step: 5
2024-12-01-21:01:18-root-INFO: Undo step: 6
2024-12-01-21:01:18-root-INFO: Undo step: 7
2024-12-01-21:01:18-root-INFO: Undo step: 8
2024-12-01-21:01:18-root-INFO: Undo step: 9
2024-12-01-21:01:18-root-INFO: step: 10 lr_xt 0.51386241
2024-12-01-21:01:18-root-INFO: grad norm: 13.461 13.441 0.723
2024-12-01-21:01:19-root-INFO: grad norm: 4.745 4.727 0.404
2024-12-01-21:01:19-root-INFO: grad norm: 2.880 2.873 0.214
2024-12-01-21:01:20-root-INFO: grad norm: 2.365 2.359 0.167
2024-12-01-21:01:20-root-INFO: grad norm: 1.728 1.723 0.137
2024-12-01-21:01:21-root-INFO: Loss Change: 60.225 -> 6.176
2024-12-01-21:01:21-root-INFO: Regularization Change: 0.000 -> 94.234
2024-12-01-21:01:21-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-01-21:01:21-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-21:01:21-root-INFO: step: 9 lr_xt 0.52093157
2024-12-01-21:01:21-root-INFO: grad norm: 1.689 1.683 0.143
2024-12-01-21:01:21-root-INFO: grad norm: 1.397 1.391 0.127
2024-12-01-21:01:22-root-INFO: grad norm: 1.254 1.251 0.084
2024-12-01-21:01:22-root-INFO: grad norm: 1.276 1.273 0.083
2024-12-01-21:01:23-root-INFO: grad norm: 1.045 1.042 0.080
2024-12-01-21:01:23-root-INFO: Loss Change: 5.873 -> 3.509
2024-12-01-21:01:23-root-INFO: Regularization Change: 0.000 -> 6.079
2024-12-01-21:01:23-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-01-21:01:23-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-21:01:23-root-INFO: step: 8 lr_xt 0.52801377
2024-12-01-21:01:23-root-INFO: grad norm: 1.368 1.358 0.169
2024-12-01-21:01:24-root-INFO: grad norm: 1.103 1.094 0.136
2024-12-01-21:01:24-root-INFO: grad norm: 1.059 1.055 0.100
2024-12-01-21:01:25-root-INFO: grad norm: 1.021 1.015 0.112
2024-12-01-21:01:25-root-INFO: grad norm: 1.005 1.001 0.094
2024-12-01-21:01:26-root-INFO: Loss Change: 3.442 -> 2.652
2024-12-01-21:01:26-root-INFO: Regularization Change: 0.000 -> 2.274
2024-12-01-21:01:26-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-01-21:01:26-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-21:01:26-root-INFO: step: 7 lr_xt 0.53510690
2024-12-01-21:01:26-root-INFO: grad norm: 1.076 1.073 0.080
2024-12-01-21:01:26-root-INFO: grad norm: 0.900 0.899 0.047
2024-12-01-21:01:27-root-INFO: grad norm: 0.828 0.826 0.056
2024-12-01-21:01:27-root-INFO: grad norm: 0.858 0.857 0.050
2024-12-01-21:01:28-root-INFO: grad norm: 0.852 0.849 0.065
2024-12-01-21:01:28-root-INFO: Loss Change: 2.568 -> 2.140
2024-12-01-21:01:28-root-INFO: Regularization Change: 0.000 -> 1.230
2024-12-01-21:01:28-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-01-21:01:28-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-21:01:28-root-INFO: step: 6 lr_xt 0.54220886
2024-12-01-21:01:28-root-INFO: grad norm: 1.386 1.377 0.163
2024-12-01-21:01:29-root-INFO: grad norm: 0.915 0.911 0.092
2024-12-01-21:01:29-root-INFO: grad norm: 0.883 0.880 0.063
2024-12-01-21:01:30-root-INFO: grad norm: 0.747 0.745 0.065
2024-12-01-21:01:30-root-INFO: grad norm: 0.682 0.681 0.045
2024-12-01-21:01:31-root-INFO: Loss Change: 2.282 -> 1.827
2024-12-01-21:01:31-root-INFO: Regularization Change: 0.000 -> 0.965
2024-12-01-21:01:31-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-01-21:01:31-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-21:01:31-root-INFO: step: 5 lr_xt 0.54931747
2024-12-01-21:01:31-root-INFO: grad norm: 0.833 0.829 0.082
2024-12-01-21:01:31-root-INFO: grad norm: 0.497 0.496 0.031
2024-12-01-21:01:32-root-INFO: grad norm: 0.427 0.426 0.027
2024-12-01-21:01:32-root-INFO: grad norm: 0.408 0.407 0.025
2024-12-01-21:01:33-root-INFO: grad norm: 0.399 0.398 0.027
2024-12-01-21:01:33-root-INFO: Loss Change: 1.901 -> 1.621
2024-12-01-21:01:33-root-INFO: Regularization Change: 0.000 -> 0.615
2024-12-01-21:01:33-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-01-21:01:33-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-21:01:33-root-INFO: step: 4 lr_xt 0.55643055
2024-12-01-21:01:34-root-INFO: grad norm: 0.906 0.899 0.105
2024-12-01-21:01:34-root-INFO: grad norm: 0.565 0.563 0.047
2024-12-01-21:01:34-root-INFO: grad norm: 0.469 0.468 0.035
2024-12-01-21:01:35-root-INFO: grad norm: 0.411 0.410 0.033
2024-12-01-21:01:35-root-INFO: grad norm: 0.400 0.399 0.031
2024-12-01-21:01:36-root-INFO: Loss Change: 1.776 -> 1.502
2024-12-01-21:01:36-root-INFO: Regularization Change: 0.000 -> 0.565
2024-12-01-21:01:36-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-01-21:01:36-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-21:01:36-root-INFO: step: 3 lr_xt 0.56354589
2024-12-01-21:01:36-root-INFO: grad norm: 0.727 0.724 0.066
2024-12-01-21:01:37-root-INFO: grad norm: 0.359 0.357 0.035
2024-12-01-21:01:37-root-INFO: grad norm: 0.323 0.321 0.030
2024-12-01-21:01:37-root-INFO: grad norm: 0.318 0.317 0.031
2024-12-01-21:01:38-root-INFO: grad norm: 0.315 0.314 0.029
2024-12-01-21:01:38-root-INFO: Loss Change: 1.631 -> 1.399
2024-12-01-21:01:38-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-01-21:01:38-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-01-21:01:38-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-21:01:38-root-INFO: step: 2 lr_xt 0.57066124
2024-12-01-21:01:39-root-INFO: grad norm: 0.887 0.883 0.074
2024-12-01-21:01:39-root-INFO: grad norm: 0.482 0.481 0.036
2024-12-01-21:01:39-root-INFO: grad norm: 0.323 0.322 0.028
2024-12-01-21:01:40-root-INFO: grad norm: 0.228 0.227 0.028
2024-12-01-21:01:40-root-INFO: grad norm: 0.203 0.202 0.026
2024-12-01-21:01:41-root-INFO: Loss Change: 1.570 -> 1.315
2024-12-01-21:01:41-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-01-21:01:41-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-01-21:01:41-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-21:01:41-root-INFO: step: 1 lr_xt 0.57777431
2024-12-01-21:01:41-root-INFO: grad norm: 0.684 0.683 0.046
2024-12-01-21:01:42-root-INFO: grad norm: 0.325 0.324 0.026
2024-12-01-21:01:42-root-INFO: grad norm: 0.280 0.279 0.024
2024-12-01-21:01:43-root-INFO: grad norm: 0.246 0.245 0.024
2024-12-01-21:01:43-root-INFO: grad norm: 0.237 0.235 0.022
2024-12-01-21:01:43-root-INFO: Loss Change: 1.443 -> 1.214
2024-12-01-21:01:43-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-01-21:01:43-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-01-21:01:43-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-21:01:44-root-INFO: step: 0 lr_xt 0.58488282
2024-12-01-21:01:44-root-INFO: grad norm: 0.759 0.756 0.070
2024-12-01-21:01:44-root-INFO: grad norm: 0.420 0.419 0.025
2024-12-01-21:01:45-root-INFO: grad norm: 0.344 0.341 0.046
2024-12-01-21:01:45-root-INFO: grad norm: 0.317 0.313 0.052
2024-12-01-21:01:46-root-INFO: grad norm: 0.304 0.299 0.052
2024-12-01-21:01:46-root-INFO: Loss Change: 1.351 -> 0.870
2024-12-01-21:01:46-root-INFO: Regularization Change: 0.000 -> 1.223
2024-12-01-21:01:46-root-INFO: Undo step: 0
2024-12-01-21:01:46-root-INFO: Undo step: 1
2024-12-01-21:01:46-root-INFO: Undo step: 2
2024-12-01-21:01:46-root-INFO: Undo step: 3
2024-12-01-21:01:46-root-INFO: Undo step: 4
2024-12-01-21:01:46-root-INFO: step: 5 lr_xt 0.54931747
2024-12-01-21:01:47-root-INFO: grad norm: 12.460 12.454 0.394
2024-12-01-21:01:47-root-INFO: grad norm: 6.568 6.564 0.242
2024-12-01-21:01:48-root-INFO: grad norm: 4.352 4.350 0.139
2024-12-01-21:01:48-root-INFO: grad norm: 2.475 2.473 0.088
2024-12-01-21:01:49-root-INFO: grad norm: 2.119 2.118 0.055
2024-12-01-21:01:49-root-INFO: Loss Change: 37.956 -> 3.789
2024-12-01-21:01:49-root-INFO: Regularization Change: 0.000 -> 60.998
2024-12-01-21:01:49-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-01-21:01:49-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-21:01:49-root-INFO: step: 4 lr_xt 0.55643055
2024-12-01-21:01:49-root-INFO: grad norm: 1.530 1.529 0.064
2024-12-01-21:01:50-root-INFO: grad norm: 1.106 1.105 0.047
2024-12-01-21:01:50-root-INFO: grad norm: 0.901 0.899 0.046
2024-12-01-21:01:51-root-INFO: grad norm: 1.280 1.280 0.034
2024-12-01-21:01:51-root-INFO: grad norm: 1.262 1.261 0.040
2024-12-01-21:01:52-root-INFO: Loss Change: 3.685 -> 2.709
2024-12-01-21:01:52-root-INFO: Regularization Change: 0.000 -> 3.204
2024-12-01-21:01:52-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-01-21:01:52-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-21:01:52-root-INFO: step: 3 lr_xt 0.56354589
2024-12-01-21:01:52-root-INFO: grad norm: 0.985 0.982 0.081
2024-12-01-21:01:53-root-INFO: grad norm: 0.559 0.557 0.046
2024-12-01-21:01:53-root-INFO: grad norm: 0.458 0.456 0.036
2024-12-01-21:01:54-root-INFO: grad norm: 0.410 0.409 0.037
2024-12-01-21:01:54-root-INFO: grad norm: 0.956 0.955 0.032
2024-12-01-21:01:54-root-INFO: Loss Change: 2.790 -> 2.297
2024-12-01-21:01:54-root-INFO: Regularization Change: 0.000 -> 1.594
2024-12-01-21:01:54-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-01-21:01:54-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-21:01:55-root-INFO: step: 2 lr_xt 0.57066124
2024-12-01-21:01:55-root-INFO: grad norm: 0.726 0.723 0.064
2024-12-01-21:01:55-root-INFO: grad norm: 0.447 0.446 0.039
2024-12-01-21:01:56-root-INFO: grad norm: 0.514 0.512 0.037
2024-12-01-21:01:56-root-INFO: grad norm: 1.482 1.481 0.056
2024-12-01-21:01:56-root-INFO: Loss too large (1.872->1.932)! Learning rate decreased to 0.45653.
2024-12-01-21:01:57-root-INFO: Loss too large (1.872->1.905)! Learning rate decreased to 0.36522.
2024-12-01-21:01:57-root-INFO: grad norm: 1.646 1.645 0.058
2024-12-01-21:01:57-root-INFO: Loss too large (1.854->1.881)! Learning rate decreased to 0.29218.
2024-12-01-21:01:58-root-INFO: Loss Change: 2.403 -> 1.802
2024-12-01-21:01:58-root-INFO: Regularization Change: 0.000 -> 0.858
2024-12-01-21:01:58-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-01-21:01:58-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-21:01:58-root-INFO: step: 1 lr_xt 0.57777431
2024-12-01-21:01:58-root-INFO: grad norm: 1.609 1.607 0.081
2024-12-01-21:01:58-root-INFO: Loss too large (1.941->1.984)! Learning rate decreased to 0.46222.
2024-12-01-21:01:59-root-INFO: grad norm: 0.532 0.531 0.031
2024-12-01-21:01:59-root-INFO: grad norm: 1.521 1.521 0.042
2024-12-01-21:02:00-root-INFO: grad norm: 0.877 0.877 0.019
2024-12-01-21:02:00-root-INFO: grad norm: 1.444 1.444 0.024
2024-12-01-21:02:01-root-INFO: Loss Change: 1.941 -> 1.568
2024-12-01-21:02:01-root-INFO: Regularization Change: 0.000 -> 1.675
2024-12-01-21:02:01-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-01-21:02:01-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-21:02:01-root-INFO: loss_sample0_0: 1.5675623416900635
2024-12-01-21:02:01-root-INFO: It takes 1592.678 seconds for image sample0
2024-12-01-21:02:01-root-INFO: lpips_score_sample0: 0.151
2024-12-01-21:02:01-root-INFO: psnr_score_sample0: 17.316
2024-12-01-21:02:01-root-INFO: ssim_score_sample0: 0.703
2024-12-01-21:02:01-root-INFO: mean_lpips: 0.15137909352779388
2024-12-01-21:02:01-root-INFO: best_mean_lpips: 0.15137909352779388
2024-12-01-21:02:01-root-INFO: mean_psnr: 17.31625747680664
2024-12-01-21:02:01-root-INFO: best_mean_psnr: 17.31625747680664
2024-12-01-21:02:01-root-INFO: mean_ssim: 0.7033923864364624
2024-12-01-21:02:01-root-INFO: best_mean_ssim: 0.7033923864364624
2024-12-01-21:02:01-root-INFO: final_loss: 1.5675623416900635
2024-12-01-21:02:01-root-INFO: mean time: 1592.6781866550446
2024-12-01-21:02:01-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample2_iter5_lr0.03_10009 
 
Enjoy.
