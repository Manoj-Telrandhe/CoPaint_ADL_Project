2024-12-01-17:28:27-root-INFO: Prepare model...
2024-12-01-17:28:44-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-01-17:29:14-root-INFO: Start sampling
2024-12-01-17:29:22-root-INFO: step: 249 lr_xt 0.00019059
2024-12-01-17:29:22-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-01-17:29:23-root-INFO: Loss too large (77070.016->78612.719)! Learning rate decreased to 0.00015.
2024-12-01-17:29:24-root-INFO: grad norm: 15661.119 11248.848 10896.518
2024-12-01-17:29:25-root-INFO: grad norm: 14205.959 10378.389 9700.428
2024-12-01-17:29:26-root-INFO: grad norm: 15590.490 11542.619 10480.042
2024-12-01-17:29:26-root-INFO: Loss too large (28301.748->35851.527)! Learning rate decreased to 0.00012.
2024-12-01-17:29:27-root-INFO: grad norm: 16985.312 12431.857 11573.668
2024-12-01-17:29:28-root-INFO: Loss too large (28113.893->29014.229)! Learning rate decreased to 0.00010.
2024-12-01-17:29:29-root-INFO: grad norm: 13500.317 10439.793 8559.748
2024-12-01-17:29:30-root-INFO: grad norm: 13455.174 10629.314 8249.810
2024-12-01-17:29:30-root-INFO: Loss too large (22323.736->22381.922)! Learning rate decreased to 0.00008.
2024-12-01-17:29:31-root-INFO: grad norm: 9590.984 7722.000 5688.382
2024-12-01-17:29:32-root-INFO: Loss Change: 77070.016 -> 18637.527
2024-12-01-17:29:32-root-INFO: Regularization Change: 0.000 -> 18.364
2024-12-01-17:29:32-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-01-17:29:32-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-17:29:32-root-INFO: step: 248 lr_xt 0.00020082
2024-12-01-17:29:32-root-INFO: grad norm: 7522.244 5981.593 4561.216
2024-12-01-17:29:33-root-INFO: Loss too large (18708.312->30950.307)! Learning rate decreased to 0.00016.
2024-12-01-17:29:33-root-INFO: Loss too large (18708.312->24730.969)! Learning rate decreased to 0.00013.
2024-12-01-17:29:33-root-INFO: Loss too large (18708.312->21088.270)! Learning rate decreased to 0.00010.
2024-12-01-17:29:34-root-INFO: Loss too large (18708.312->19048.988)! Learning rate decreased to 0.00008.
2024-12-01-17:29:35-root-INFO: grad norm: 6006.236 4799.234 3611.402
2024-12-01-17:29:36-root-INFO: grad norm: 4839.978 3873.829 2901.523
2024-12-01-17:29:37-root-INFO: grad norm: 3853.856 3061.429 2340.910
2024-12-01-17:29:38-root-INFO: grad norm: 3089.498 2487.535 1832.258
2024-12-01-17:29:39-root-INFO: grad norm: 2467.975 1955.334 1505.845
2024-12-01-17:29:40-root-INFO: grad norm: 1998.244 1619.340 1170.777
2024-12-01-17:29:41-root-INFO: grad norm: 1631.640 1299.012 987.328
2024-12-01-17:29:42-root-INFO: Loss Change: 18708.312 -> 16490.881
2024-12-01-17:29:42-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-01-17:29:42-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-01-17:29:42-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-17:29:42-root-INFO: step: 247 lr_xt 0.00021156
2024-12-01-17:29:42-root-INFO: grad norm: 1317.954 1091.874 738.115
2024-12-01-17:29:42-root-INFO: Loss too large (16314.118->16376.037)! Learning rate decreased to 0.00017.
2024-12-01-17:29:43-root-INFO: grad norm: 2398.768 1882.893 1486.203
2024-12-01-17:29:44-root-INFO: Loss too large (16294.775->16719.117)! Learning rate decreased to 0.00014.
2024-12-01-17:29:44-root-INFO: Loss too large (16294.775->16416.395)! Learning rate decreased to 0.00011.
2024-12-01-17:29:45-root-INFO: grad norm: 2589.722 2072.222 1553.240
2024-12-01-17:29:46-root-INFO: grad norm: 2843.362 2268.303 1714.499
2024-12-01-17:29:47-root-INFO: grad norm: 3135.447 2504.551 1886.333
2024-12-01-17:29:48-root-INFO: grad norm: 3488.301 2803.598 2075.592
2024-12-01-17:29:49-root-INFO: Loss too large (16180.104->16200.787)! Learning rate decreased to 0.00009.
2024-12-01-17:29:50-root-INFO: grad norm: 2509.695 2006.690 1507.237
2024-12-01-17:29:51-root-INFO: grad norm: 1823.401 1496.501 1041.765
2024-12-01-17:29:51-root-INFO: Loss Change: 16314.118 -> 15773.610
2024-12-01-17:29:51-root-INFO: Regularization Change: 0.000 -> 0.476
2024-12-01-17:29:51-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-01-17:29:51-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-17:29:52-root-INFO: step: 246 lr_xt 0.00022285
2024-12-01-17:29:52-root-INFO: grad norm: 1388.288 1137.389 796.046
2024-12-01-17:29:52-root-INFO: Loss too large (15552.636->15703.603)! Learning rate decreased to 0.00018.
2024-12-01-17:29:53-root-INFO: Loss too large (15552.636->15579.516)! Learning rate decreased to 0.00014.
2024-12-01-17:29:54-root-INFO: grad norm: 1924.571 1584.573 1092.291
2024-12-01-17:29:54-root-INFO: Loss too large (15514.330->15563.280)! Learning rate decreased to 0.00011.
2024-12-01-17:29:55-root-INFO: grad norm: 2028.987 1629.555 1208.859
2024-12-01-17:29:56-root-INFO: grad norm: 2161.713 1787.453 1215.736
2024-12-01-17:29:57-root-INFO: grad norm: 2301.703 1840.236 1382.522
2024-12-01-17:29:58-root-INFO: grad norm: 2461.068 2033.091 1386.866
2024-12-01-17:29:59-root-INFO: grad norm: 2626.331 2095.808 1582.783
2024-12-01-17:30:00-root-INFO: grad norm: 2814.083 2319.206 1593.848
2024-12-01-17:30:01-root-INFO: Loss Change: 15552.636 -> 15230.171
2024-12-01-17:30:01-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-01-17:30:01-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-01-17:30:01-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-17:30:01-root-INFO: step: 245 lr_xt 0.00023469
2024-12-01-17:30:01-root-INFO: grad norm: 2958.552 2359.504 1784.873
2024-12-01-17:30:02-root-INFO: Loss too large (15122.892->16866.004)! Learning rate decreased to 0.00019.
2024-12-01-17:30:02-root-INFO: Loss too large (15122.892->15899.004)! Learning rate decreased to 0.00015.
2024-12-01-17:30:02-root-INFO: Loss too large (15122.892->15349.565)! Learning rate decreased to 0.00012.
2024-12-01-17:30:03-root-INFO: grad norm: 3004.083 2476.629 1700.242
2024-12-01-17:30:04-root-INFO: grad norm: 3064.208 2445.999 1845.658
2024-12-01-17:30:05-root-INFO: grad norm: 3137.197 2590.836 1769.060
2024-12-01-17:30:06-root-INFO: grad norm: 3208.735 2562.560 1931.132
2024-12-01-17:30:08-root-INFO: grad norm: 3286.066 2713.240 1853.795
2024-12-01-17:30:09-root-INFO: grad norm: 3360.347 2685.615 2019.755
2024-12-01-17:30:10-root-INFO: grad norm: 3440.009 2838.007 1944.063
2024-12-01-17:30:10-root-INFO: Loss Change: 15122.892 -> 14724.191
2024-12-01-17:30:10-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-01-17:30:10-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-01-17:30:10-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-17:30:11-root-INFO: step: 244 lr_xt 0.00024712
2024-12-01-17:30:11-root-INFO: grad norm: 3398.268 2753.693 1991.331
2024-12-01-17:30:11-root-INFO: Loss too large (14595.578->16828.422)! Learning rate decreased to 0.00020.
2024-12-01-17:30:12-root-INFO: Loss too large (14595.578->15548.514)! Learning rate decreased to 0.00016.
2024-12-01-17:30:12-root-INFO: Loss too large (14595.578->14826.577)! Learning rate decreased to 0.00013.
2024-12-01-17:30:13-root-INFO: grad norm: 3217.750 2667.436 1799.639
2024-12-01-17:30:14-root-INFO: grad norm: 3129.080 2531.280 1839.501
2024-12-01-17:30:15-root-INFO: grad norm: 3063.534 2556.305 1688.356
2024-12-01-17:30:16-root-INFO: grad norm: 3013.524 2433.404 1777.603
2024-12-01-17:30:17-root-INFO: grad norm: 2969.458 2482.642 1629.163
2024-12-01-17:30:18-root-INFO: grad norm: 2930.648 2365.962 1729.428
2024-12-01-17:30:19-root-INFO: grad norm: 2891.732 2419.367 1583.911
2024-12-01-17:30:20-root-INFO: Loss Change: 14595.578 -> 13887.604
2024-12-01-17:30:20-root-INFO: Regularization Change: 0.000 -> 0.588
2024-12-01-17:30:20-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-01-17:30:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:30:20-root-INFO: step: 243 lr_xt 0.00026017
2024-12-01-17:30:20-root-INFO: grad norm: 2945.310 2366.632 1753.255
2024-12-01-17:30:21-root-INFO: Loss too large (13795.973->15472.076)! Learning rate decreased to 0.00021.
2024-12-01-17:30:21-root-INFO: Loss too large (13795.973->14497.432)! Learning rate decreased to 0.00017.
2024-12-01-17:30:21-root-INFO: Loss too large (13795.973->13948.827)! Learning rate decreased to 0.00013.
2024-12-01-17:30:22-root-INFO: grad norm: 2746.775 2306.240 1491.988
2024-12-01-17:30:23-root-INFO: grad norm: 2607.863 2104.845 1539.668
2024-12-01-17:30:24-root-INFO: grad norm: 2483.584 2097.339 1330.173
2024-12-01-17:30:25-root-INFO: grad norm: 2376.593 1922.402 1397.343
2024-12-01-17:30:26-root-INFO: grad norm: 2275.302 1926.675 1210.340
2024-12-01-17:30:27-root-INFO: grad norm: 2186.878 1773.162 1279.973
2024-12-01-17:30:28-root-INFO: grad norm: 2101.488 1782.903 1112.434
2024-12-01-17:30:29-root-INFO: Loss Change: 13795.973 -> 13042.955
2024-12-01-17:30:29-root-INFO: Regularization Change: 0.000 -> 0.626
2024-12-01-17:30:29-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-01-17:30:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:30:29-root-INFO: step: 242 lr_xt 0.00027387
2024-12-01-17:30:30-root-INFO: grad norm: 1942.296 1605.861 1092.578
2024-12-01-17:30:30-root-INFO: Loss too large (12825.550->13416.674)! Learning rate decreased to 0.00022.
2024-12-01-17:30:30-root-INFO: Loss too large (12825.550->13035.703)! Learning rate decreased to 0.00018.
2024-12-01-17:30:31-root-INFO: Loss too large (12825.550->12826.086)! Learning rate decreased to 0.00014.
2024-12-01-17:30:32-root-INFO: grad norm: 1776.777 1510.635 935.371
2024-12-01-17:30:33-root-INFO: grad norm: 1649.225 1354.193 941.333
2024-12-01-17:30:34-root-INFO: grad norm: 1540.210 1326.142 783.322
2024-12-01-17:30:35-root-INFO: grad norm: 1445.939 1188.927 822.916
2024-12-01-17:30:36-root-INFO: grad norm: 1361.954 1180.868 678.579
2024-12-01-17:30:37-root-INFO: grad norm: 1286.610 1063.464 724.160
2024-12-01-17:30:38-root-INFO: grad norm: 1216.858 1061.239 595.412
2024-12-01-17:30:38-root-INFO: Loss Change: 12825.550 -> 12147.531
2024-12-01-17:30:38-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-01-17:30:38-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-01-17:30:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:30:39-root-INFO: step: 241 lr_xt 0.00028824
2024-12-01-17:30:39-root-INFO: grad norm: 1089.940 929.025 569.983
2024-12-01-17:30:39-root-INFO: Loss too large (12037.743->12047.732)! Learning rate decreased to 0.00023.
2024-12-01-17:30:40-root-INFO: grad norm: 1718.950 1471.861 887.928
2024-12-01-17:30:41-root-INFO: Loss too large (11989.215->12137.773)! Learning rate decreased to 0.00018.
2024-12-01-17:30:42-root-INFO: grad norm: 2220.108 1844.204 1236.038
2024-12-01-17:30:42-root-INFO: Loss too large (11976.311->12026.673)! Learning rate decreased to 0.00015.
2024-12-01-17:30:43-root-INFO: grad norm: 1984.703 1695.577 1031.534
2024-12-01-17:30:44-root-INFO: grad norm: 1785.892 1488.771 986.393
2024-12-01-17:30:45-root-INFO: grad norm: 1608.431 1385.999 816.124
2024-12-01-17:30:46-root-INFO: grad norm: 1455.936 1220.293 794.126
2024-12-01-17:30:47-root-INFO: grad norm: 1322.735 1151.471 650.955
2024-12-01-17:30:48-root-INFO: Loss Change: 12037.743 -> 11409.948
2024-12-01-17:30:48-root-INFO: Regularization Change: 0.000 -> 0.833
2024-12-01-17:30:48-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-01-17:30:48-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:30:48-root-INFO: step: 240 lr_xt 0.00030331
2024-12-01-17:30:49-root-INFO: grad norm: 1251.518 1078.645 634.683
2024-12-01-17:30:49-root-INFO: Loss too large (11225.301->11305.641)! Learning rate decreased to 0.00024.
2024-12-01-17:30:50-root-INFO: grad norm: 1996.395 1715.853 1020.511
2024-12-01-17:30:50-root-INFO: Loss too large (11199.928->11458.412)! Learning rate decreased to 0.00019.
2024-12-01-17:30:51-root-INFO: Loss too large (11199.928->11212.184)! Learning rate decreased to 0.00016.
2024-12-01-17:30:52-root-INFO: grad norm: 1719.729 1447.238 928.961
2024-12-01-17:30:53-root-INFO: grad norm: 1491.202 1297.515 734.941
2024-12-01-17:30:54-root-INFO: grad norm: 1308.941 1109.732 694.134
2024-12-01-17:30:55-root-INFO: grad norm: 1158.782 1021.436 547.215
2024-12-01-17:30:56-root-INFO: grad norm: 1038.181 891.200 532.524
2024-12-01-17:30:57-root-INFO: grad norm: 940.373 840.806 421.125
2024-12-01-17:30:57-root-INFO: Loss Change: 11225.301 -> 10576.600
2024-12-01-17:30:57-root-INFO: Regularization Change: 0.000 -> 0.818
2024-12-01-17:30:57-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-17:30:57-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:30:58-root-INFO: step: 239 lr_xt 0.00031912
2024-12-01-17:30:58-root-INFO: grad norm: 942.245 810.072 481.259
2024-12-01-17:30:59-root-INFO: grad norm: 1767.393 1502.934 929.982
2024-12-01-17:30:59-root-INFO: Loss too large (10455.771->10953.820)! Learning rate decreased to 0.00026.
2024-12-01-17:31:00-root-INFO: Loss too large (10455.771->10618.459)! Learning rate decreased to 0.00020.
2024-12-01-17:31:00-root-INFO: grad norm: 2112.263 1773.648 1147.096
2024-12-01-17:31:01-root-INFO: Loss too large (10433.614->10445.126)! Learning rate decreased to 0.00016.
2024-12-01-17:31:02-root-INFO: grad norm: 1709.435 1472.313 868.599
2024-12-01-17:31:03-root-INFO: grad norm: 1409.566 1198.420 742.070
2024-12-01-17:31:04-root-INFO: grad norm: 1176.010 1034.445 559.394
2024-12-01-17:31:05-root-INFO: grad norm: 1001.311 864.485 505.262
2024-12-01-17:31:06-root-INFO: grad norm: 869.761 783.086 378.499
2024-12-01-17:31:07-root-INFO: Loss Change: 10490.975 -> 9879.941
2024-12-01-17:31:07-root-INFO: Regularization Change: 0.000 -> 0.909
2024-12-01-17:31:07-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-17:31:07-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:31:07-root-INFO: step: 238 lr_xt 0.00033570
2024-12-01-17:31:07-root-INFO: grad norm: 1032.558 903.361 500.115
2024-12-01-17:31:08-root-INFO: grad norm: 1869.967 1621.433 931.522
2024-12-01-17:31:09-root-INFO: Loss too large (9677.526->10283.742)! Learning rate decreased to 0.00027.
2024-12-01-17:31:09-root-INFO: Loss too large (9677.526->9883.449)! Learning rate decreased to 0.00021.
2024-12-01-17:31:10-root-INFO: grad norm: 2130.803 1806.564 1129.888
2024-12-01-17:31:10-root-INFO: Loss too large (9661.088->9665.676)! Learning rate decreased to 0.00017.
2024-12-01-17:31:11-root-INFO: grad norm: 1610.853 1412.927 773.618
2024-12-01-17:31:12-root-INFO: grad norm: 1257.885 1080.662 643.775
2024-12-01-17:31:13-root-INFO: grad norm: 1004.492 901.930 442.183
2024-12-01-17:31:14-root-INFO: grad norm: 831.099 728.326 400.332
2024-12-01-17:31:15-root-INFO: grad norm: 712.312 656.904 275.437
2024-12-01-17:31:16-root-INFO: Loss Change: 9688.518 -> 9148.799
2024-12-01-17:31:16-root-INFO: Regularization Change: 0.000 -> 0.800
2024-12-01-17:31:16-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-17:31:16-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:31:16-root-INFO: step: 237 lr_xt 0.00035308
2024-12-01-17:31:17-root-INFO: grad norm: 834.461 726.406 410.681
2024-12-01-17:31:18-root-INFO: grad norm: 1541.699 1356.002 733.549
2024-12-01-17:31:18-root-INFO: Loss too large (9085.676->9480.399)! Learning rate decreased to 0.00028.
2024-12-01-17:31:18-root-INFO: Loss too large (9085.676->9207.033)! Learning rate decreased to 0.00023.
2024-12-01-17:31:19-root-INFO: grad norm: 1705.394 1462.797 876.695
2024-12-01-17:31:20-root-INFO: grad norm: 1900.709 1666.911 913.292
2024-12-01-17:31:22-root-INFO: grad norm: 2113.053 1816.733 1079.109
2024-12-01-17:31:23-root-INFO: grad norm: 2349.840 2053.822 1141.737
2024-12-01-17:31:23-root-INFO: Loss too large (9028.283->9049.070)! Learning rate decreased to 0.00018.
2024-12-01-17:31:24-root-INFO: grad norm: 1670.320 1438.584 848.791
2024-12-01-17:31:25-root-INFO: grad norm: 1201.842 1070.366 546.571
2024-12-01-17:31:26-root-INFO: Loss Change: 9095.494 -> 8655.781
2024-12-01-17:31:26-root-INFO: Regularization Change: 0.000 -> 0.810
2024-12-01-17:31:26-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-01-17:31:26-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:31:26-root-INFO: step: 236 lr_xt 0.00037130
2024-12-01-17:31:26-root-INFO: grad norm: 918.792 810.596 432.564
2024-12-01-17:31:27-root-INFO: Loss too large (8522.922->8578.271)! Learning rate decreased to 0.00030.
2024-12-01-17:31:28-root-INFO: grad norm: 1329.294 1174.015 623.467
2024-12-01-17:31:28-root-INFO: Loss too large (8508.236->8580.860)! Learning rate decreased to 0.00024.
2024-12-01-17:31:29-root-INFO: grad norm: 1412.604 1223.843 705.449
2024-12-01-17:31:30-root-INFO: grad norm: 1501.571 1324.068 708.209
2024-12-01-17:31:31-root-INFO: grad norm: 1592.802 1377.254 800.120
2024-12-01-17:31:32-root-INFO: grad norm: 1688.959 1485.393 803.860
2024-12-01-17:31:33-root-INFO: grad norm: 1782.152 1539.158 898.366
2024-12-01-17:31:34-root-INFO: grad norm: 1875.925 1646.263 899.397
2024-12-01-17:31:35-root-INFO: Loss Change: 8522.922 -> 8303.652
2024-12-01-17:31:35-root-INFO: Regularization Change: 0.000 -> 0.807
2024-12-01-17:31:35-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-01-17:31:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-17:31:35-root-INFO: step: 235 lr_xt 0.00039040
2024-12-01-17:31:36-root-INFO: grad norm: 2202.206 1912.712 1091.441
2024-12-01-17:31:36-root-INFO: Loss too large (8300.279->9253.441)! Learning rate decreased to 0.00031.
2024-12-01-17:31:36-root-INFO: Loss too large (8300.279->8602.840)! Learning rate decreased to 0.00025.
2024-12-01-17:31:37-root-INFO: grad norm: 2203.492 1943.903 1037.603
2024-12-01-17:31:38-root-INFO: grad norm: 2204.851 1914.893 1092.954
2024-12-01-17:31:39-root-INFO: grad norm: 2204.387 1944.658 1038.088
2024-12-01-17:31:40-root-INFO: grad norm: 2200.310 1911.916 1089.009
2024-12-01-17:31:41-root-INFO: grad norm: 2192.593 1933.426 1034.084
2024-12-01-17:31:42-root-INFO: grad norm: 2179.005 1894.104 1077.234
2024-12-01-17:31:43-root-INFO: grad norm: 2160.885 1905.240 1019.551
2024-12-01-17:31:44-root-INFO: Loss Change: 8300.279 -> 7960.371
2024-12-01-17:31:44-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-01-17:31:44-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-01-17:31:44-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-17:31:44-root-INFO: step: 234 lr_xt 0.00041042
2024-12-01-17:31:45-root-INFO: grad norm: 2122.129 1850.271 1039.197
2024-12-01-17:31:45-root-INFO: Loss too large (7912.172->8780.730)! Learning rate decreased to 0.00033.
2024-12-01-17:31:45-root-INFO: Loss too large (7912.172->8169.005)! Learning rate decreased to 0.00026.
2024-12-01-17:31:46-root-INFO: grad norm: 2018.790 1781.901 948.862
2024-12-01-17:31:47-root-INFO: grad norm: 1922.309 1676.383 940.750
2024-12-01-17:31:48-root-INFO: grad norm: 1825.539 1614.533 851.982
2024-12-01-17:31:49-root-INFO: grad norm: 1735.153 1513.125 849.240
2024-12-01-17:31:50-root-INFO: grad norm: 1647.335 1459.228 764.438
2024-12-01-17:31:51-root-INFO: grad norm: 1566.578 1366.368 766.292
2024-12-01-17:31:52-root-INFO: grad norm: 1487.926 1320.105 686.474
2024-12-01-17:31:53-root-INFO: Loss Change: 7912.172 -> 7455.445
2024-12-01-17:31:53-root-INFO: Regularization Change: 0.000 -> 0.619
2024-12-01-17:31:53-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-17:31:53-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:31:53-root-INFO: step: 233 lr_xt 0.00043139
2024-12-01-17:31:54-root-INFO: grad norm: 1551.835 1367.136 734.255
2024-12-01-17:31:54-root-INFO: Loss too large (7464.261->7858.877)! Learning rate decreased to 0.00035.
2024-12-01-17:31:54-root-INFO: Loss too large (7464.261->7547.589)! Learning rate decreased to 0.00028.
2024-12-01-17:31:56-root-INFO: grad norm: 1384.595 1230.245 635.295
2024-12-01-17:31:57-root-INFO: grad norm: 1252.610 1099.478 600.152
2024-12-01-17:31:58-root-INFO: grad norm: 1135.250 1014.421 509.649
2024-12-01-17:31:59-root-INFO: grad norm: 1035.423 909.574 494.749
2024-12-01-17:32:00-root-INFO: grad norm: 944.587 847.683 416.747
2024-12-01-17:32:01-root-INFO: grad norm: 865.228 761.439 410.889
2024-12-01-17:32:02-root-INFO: grad norm: 792.417 714.774 342.086
2024-12-01-17:32:02-root-INFO: Loss Change: 7464.261 -> 7060.425
2024-12-01-17:32:02-root-INFO: Regularization Change: 0.000 -> 0.601
2024-12-01-17:32:02-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-01-17:32:02-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:32:03-root-INFO: step: 232 lr_xt 0.00045336
2024-12-01-17:32:03-root-INFO: grad norm: 1168.791 1013.451 582.228
2024-12-01-17:32:03-root-INFO: Loss too large (7004.084->7189.701)! Learning rate decreased to 0.00036.
2024-12-01-17:32:04-root-INFO: Loss too large (7004.084->7021.940)! Learning rate decreased to 0.00029.
2024-12-01-17:32:05-root-INFO: grad norm: 1000.427 905.208 425.974
2024-12-01-17:32:06-root-INFO: grad norm: 887.201 779.805 423.119
2024-12-01-17:32:07-root-INFO: grad norm: 792.202 720.640 329.031
2024-12-01-17:32:08-root-INFO: grad norm: 713.856 632.113 331.698
2024-12-01-17:32:09-root-INFO: grad norm: 646.320 590.540 262.663
2024-12-01-17:32:10-root-INFO: grad norm: 588.801 524.538 267.480
2024-12-01-17:32:11-root-INFO: grad norm: 538.906 495.153 212.705
2024-12-01-17:32:12-root-INFO: Loss Change: 7004.084 -> 6685.009
2024-12-01-17:32:12-root-INFO: Regularization Change: 0.000 -> 0.545
2024-12-01-17:32:12-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-01-17:32:12-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:32:12-root-INFO: step: 231 lr_xt 0.00047637
2024-12-01-17:32:12-root-INFO: grad norm: 634.346 564.641 289.094
2024-12-01-17:32:13-root-INFO: Loss too large (6641.454->6669.020)! Learning rate decreased to 0.00038.
2024-12-01-17:32:14-root-INFO: grad norm: 797.721 722.926 337.247
2024-12-01-17:32:14-root-INFO: Loss too large (6628.396->6631.997)! Learning rate decreased to 0.00030.
2024-12-01-17:32:15-root-INFO: grad norm: 701.372 624.690 318.881
2024-12-01-17:32:16-root-INFO: grad norm: 619.842 566.358 251.879
2024-12-01-17:32:17-root-INFO: grad norm: 551.204 492.859 246.812
2024-12-01-17:32:18-root-INFO: grad norm: 493.651 454.872 191.786
2024-12-01-17:32:19-root-INFO: grad norm: 446.210 401.700 194.269
2024-12-01-17:32:20-root-INFO: grad norm: 406.472 377.847 149.838
2024-12-01-17:32:21-root-INFO: Loss Change: 6641.454 -> 6424.495
2024-12-01-17:32:21-root-INFO: Regularization Change: 0.000 -> 0.498
2024-12-01-17:32:21-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-01-17:32:21-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:32:21-root-INFO: step: 230 lr_xt 0.00050047
2024-12-01-17:32:21-root-INFO: grad norm: 592.885 524.189 277.018
2024-12-01-17:32:22-root-INFO: Loss too large (6363.660->6386.249)! Learning rate decreased to 0.00040.
2024-12-01-17:32:22-root-INFO: grad norm: 727.676 663.240 299.374
2024-12-01-17:32:23-root-INFO: grad norm: 910.901 812.276 412.247
2024-12-01-17:32:24-root-INFO: Loss too large (6348.720->6365.108)! Learning rate decreased to 0.00032.
2024-12-01-17:32:25-root-INFO: grad norm: 771.429 700.888 322.270
2024-12-01-17:32:26-root-INFO: grad norm: 660.969 591.402 295.168
2024-12-01-17:32:27-root-INFO: grad norm: 570.874 521.747 231.683
2024-12-01-17:32:28-root-INFO: grad norm: 498.123 448.029 217.708
2024-12-01-17:32:29-root-INFO: grad norm: 438.767 404.153 170.814
2024-12-01-17:32:30-root-INFO: Loss Change: 6363.660 -> 6163.516
2024-12-01-17:32:30-root-INFO: Regularization Change: 0.000 -> 0.502
2024-12-01-17:32:30-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-01-17:32:30-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:32:30-root-INFO: step: 229 lr_xt 0.00052570
2024-12-01-17:32:30-root-INFO: grad norm: 684.487 616.100 298.234
2024-12-01-17:32:31-root-INFO: Loss too large (6141.360->6183.852)! Learning rate decreased to 0.00042.
2024-12-01-17:32:32-root-INFO: grad norm: 831.975 758.848 341.076
2024-12-01-17:32:32-root-INFO: Loss too large (6128.584->6138.108)! Learning rate decreased to 0.00034.
2024-12-01-17:32:33-root-INFO: grad norm: 704.133 634.088 306.162
2024-12-01-17:32:34-root-INFO: grad norm: 598.892 547.666 242.349
2024-12-01-17:32:35-root-INFO: grad norm: 515.814 466.531 220.029
2024-12-01-17:32:36-root-INFO: grad norm: 449.395 412.928 177.330
2024-12-01-17:32:37-root-INFO: grad norm: 397.681 361.946 164.757
2024-12-01-17:32:38-root-INFO: grad norm: 356.319 329.397 135.873
2024-12-01-17:32:39-root-INFO: Loss Change: 6141.360 -> 5931.838
2024-12-01-17:32:39-root-INFO: Regularization Change: 0.000 -> 0.494
2024-12-01-17:32:39-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-01-17:32:39-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:32:39-root-INFO: step: 228 lr_xt 0.00055211
2024-12-01-17:32:39-root-INFO: grad norm: 494.456 435.312 234.499
2024-12-01-17:32:40-root-INFO: Loss too large (5907.715->5911.816)! Learning rate decreased to 0.00044.
2024-12-01-17:32:41-root-INFO: grad norm: 559.316 507.202 235.756
2024-12-01-17:32:42-root-INFO: grad norm: 669.859 605.458 286.586
2024-12-01-17:32:43-root-INFO: grad norm: 826.544 752.404 342.146
2024-12-01-17:32:43-root-INFO: Loss too large (5876.475->5891.192)! Learning rate decreased to 0.00035.
2024-12-01-17:32:44-root-INFO: grad norm: 686.419 622.225 289.840
2024-12-01-17:32:45-root-INFO: grad norm: 574.135 525.130 232.099
2024-12-01-17:32:46-root-INFO: grad norm: 487.929 443.584 203.244
2024-12-01-17:32:47-root-INFO: grad norm: 419.430 385.893 164.343
2024-12-01-17:32:48-root-INFO: Loss Change: 5907.715 -> 5733.068
2024-12-01-17:32:48-root-INFO: Regularization Change: 0.000 -> 0.507
2024-12-01-17:32:48-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-01-17:32:48-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-17:32:48-root-INFO: step: 227 lr_xt 0.00057976
2024-12-01-17:32:48-root-INFO: grad norm: 760.216 687.369 324.734
2024-12-01-17:32:49-root-INFO: Loss too large (5775.695->5841.079)! Learning rate decreased to 0.00046.
2024-12-01-17:32:50-root-INFO: grad norm: 902.131 824.622 365.839
2024-12-01-17:32:50-root-INFO: Loss too large (5763.333->5778.719)! Learning rate decreased to 0.00037.
2024-12-01-17:32:51-root-INFO: grad norm: 734.954 669.806 302.517
2024-12-01-17:32:52-root-INFO: grad norm: 605.099 554.344 242.585
2024-12-01-17:32:53-root-INFO: grad norm: 507.528 464.076 205.469
2024-12-01-17:32:54-root-INFO: grad norm: 431.087 396.205 169.874
2024-12-01-17:32:55-root-INFO: grad norm: 373.867 343.765 146.977
2024-12-01-17:32:56-root-INFO: grad norm: 329.966 304.885 126.186
2024-12-01-17:32:57-root-INFO: Loss Change: 5775.695 -> 5553.493
2024-12-01-17:32:57-root-INFO: Regularization Change: 0.000 -> 0.531
2024-12-01-17:32:57-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-01-17:32:57-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-17:32:57-root-INFO: step: 226 lr_xt 0.00060869
2024-12-01-17:32:57-root-INFO: grad norm: 374.296 321.648 191.416
2024-12-01-17:32:58-root-INFO: grad norm: 452.614 401.998 207.983
2024-12-01-17:32:59-root-INFO: Loss too large (5485.018->5485.394)! Learning rate decreased to 0.00049.
2024-12-01-17:33:00-root-INFO: grad norm: 494.598 445.394 215.061
2024-12-01-17:33:01-root-INFO: grad norm: 580.633 530.513 235.987
2024-12-01-17:33:02-root-INFO: grad norm: 696.776 634.696 287.503
2024-12-01-17:33:02-root-INFO: Loss too large (5447.718->5451.412)! Learning rate decreased to 0.00039.
2024-12-01-17:33:03-root-INFO: grad norm: 563.388 517.061 223.728
2024-12-01-17:33:04-root-INFO: grad norm: 463.739 423.864 188.130
2024-12-01-17:33:05-root-INFO: grad norm: 387.913 358.001 149.370
2024-12-01-17:33:06-root-INFO: Loss Change: 5510.406 -> 5340.302
2024-12-01-17:33:06-root-INFO: Regularization Change: 0.000 -> 0.602
2024-12-01-17:33:06-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-01-17:33:06-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:33:06-root-INFO: step: 225 lr_xt 0.00063896
2024-12-01-17:33:06-root-INFO: grad norm: 710.996 647.144 294.482
2024-12-01-17:33:07-root-INFO: Loss too large (5337.026->5404.450)! Learning rate decreased to 0.00051.
2024-12-01-17:33:08-root-INFO: grad norm: 843.938 776.542 330.477
2024-12-01-17:33:08-root-INFO: Loss too large (5327.412->5346.146)! Learning rate decreased to 0.00041.
2024-12-01-17:33:09-root-INFO: grad norm: 686.588 629.095 275.033
2024-12-01-17:33:10-root-INFO: grad norm: 561.295 517.249 217.958
2024-12-01-17:33:11-root-INFO: grad norm: 467.693 429.665 184.727
2024-12-01-17:33:12-root-INFO: grad norm: 394.661 364.460 151.414
2024-12-01-17:33:13-root-INFO: grad norm: 339.867 313.504 131.243
2024-12-01-17:33:14-root-INFO: grad norm: 298.545 276.707 112.083
2024-12-01-17:33:15-root-INFO: Loss Change: 5337.026 -> 5136.855
2024-12-01-17:33:15-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-01-17:33:15-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-01-17:33:15-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:33:15-root-INFO: step: 224 lr_xt 0.00067063
2024-12-01-17:33:16-root-INFO: grad norm: 486.121 440.154 206.344
2024-12-01-17:33:16-root-INFO: Loss too large (5130.739->5142.479)! Learning rate decreased to 0.00054.
2024-12-01-17:33:17-root-INFO: grad norm: 564.821 520.385 219.596
2024-12-01-17:33:18-root-INFO: grad norm: 698.162 644.471 268.491
2024-12-01-17:33:18-root-INFO: Loss too large (5110.554->5122.870)! Learning rate decreased to 0.00043.
2024-12-01-17:33:19-root-INFO: grad norm: 585.789 541.250 224.046
2024-12-01-17:33:20-root-INFO: grad norm: 497.737 460.390 189.165
2024-12-01-17:33:21-root-INFO: grad norm: 426.183 394.453 161.366
2024-12-01-17:33:22-root-INFO: grad norm: 369.924 343.132 138.216
2024-12-01-17:33:23-root-INFO: grad norm: 325.076 301.629 121.219
2024-12-01-17:33:24-root-INFO: Loss Change: 5130.739 -> 4969.336
2024-12-01-17:33:24-root-INFO: Regularization Change: 0.000 -> 0.522
2024-12-01-17:33:24-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-01-17:33:24-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:33:24-root-INFO: step: 223 lr_xt 0.00070376
2024-12-01-17:33:25-root-INFO: grad norm: 536.873 494.108 209.977
2024-12-01-17:33:25-root-INFO: Loss too large (4961.793->5001.488)! Learning rate decreased to 0.00056.
2024-12-01-17:33:26-root-INFO: grad norm: 658.945 609.960 249.313
2024-12-01-17:33:26-root-INFO: Loss too large (4954.427->4966.101)! Learning rate decreased to 0.00045.
2024-12-01-17:33:27-root-INFO: grad norm: 559.141 518.214 209.981
2024-12-01-17:33:28-root-INFO: grad norm: 477.129 442.453 178.569
2024-12-01-17:33:29-root-INFO: grad norm: 412.503 383.420 152.143
2024-12-01-17:33:30-root-INFO: grad norm: 359.756 334.124 133.362
2024-12-01-17:33:31-root-INFO: grad norm: 317.858 296.400 114.806
2024-12-01-17:33:33-root-INFO: grad norm: 284.522 264.887 103.863
2024-12-01-17:33:33-root-INFO: Loss Change: 4961.793 -> 4803.359
2024-12-01-17:33:33-root-INFO: Regularization Change: 0.000 -> 0.491
2024-12-01-17:33:33-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-17:33:33-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:33:34-root-INFO: step: 222 lr_xt 0.00073840
2024-12-01-17:33:34-root-INFO: grad norm: 327.062 304.284 119.922
2024-12-01-17:33:35-root-INFO: grad norm: 525.428 487.792 195.279
2024-12-01-17:33:35-root-INFO: Loss too large (4743.573->4797.817)! Learning rate decreased to 0.00059.
2024-12-01-17:33:36-root-INFO: Loss too large (4743.573->4745.707)! Learning rate decreased to 0.00047.
2024-12-01-17:33:36-root-INFO: grad norm: 443.741 413.813 160.205
2024-12-01-17:33:38-root-INFO: grad norm: 379.615 352.937 139.796
2024-12-01-17:33:39-root-INFO: grad norm: 328.941 307.642 116.442
2024-12-01-17:33:40-root-INFO: grad norm: 288.850 269.127 104.905
2024-12-01-17:33:41-root-INFO: grad norm: 257.901 241.960 89.266
2024-12-01-17:33:42-root-INFO: grad norm: 233.903 218.523 83.415
2024-12-01-17:33:42-root-INFO: Loss Change: 4745.170 -> 4617.277
2024-12-01-17:33:42-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-01-17:33:42-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-17:33:42-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:33:43-root-INFO: step: 221 lr_xt 0.00077462
2024-12-01-17:33:43-root-INFO: grad norm: 320.702 297.376 120.072
2024-12-01-17:33:44-root-INFO: grad norm: 508.933 472.986 187.875
2024-12-01-17:33:44-root-INFO: Loss too large (4583.051->4634.954)! Learning rate decreased to 0.00062.
2024-12-01-17:33:44-root-INFO: Loss too large (4583.051->4584.279)! Learning rate decreased to 0.00050.
2024-12-01-17:33:45-root-INFO: grad norm: 420.269 391.659 152.410
2024-12-01-17:33:46-root-INFO: grad norm: 352.559 328.829 127.158
2024-12-01-17:33:47-root-INFO: grad norm: 300.686 281.178 106.541
2024-12-01-17:33:48-root-INFO: grad norm: 261.212 244.238 92.625
2024-12-01-17:33:49-root-INFO: grad norm: 231.585 217.504 79.521
2024-12-01-17:33:50-root-INFO: grad norm: 209.327 196.386 72.457
2024-12-01-17:33:51-root-INFO: Loss Change: 4583.741 -> 4462.522
2024-12-01-17:33:51-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-01-17:33:51-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-17:33:51-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:33:52-root-INFO: step: 220 lr_xt 0.00081248
2024-12-01-17:33:52-root-INFO: grad norm: 492.191 451.689 195.523
2024-12-01-17:33:52-root-INFO: Loss too large (4467.197->4497.189)! Learning rate decreased to 0.00065.
2024-12-01-17:33:53-root-INFO: grad norm: 582.555 542.406 212.521
2024-12-01-17:33:54-root-INFO: Loss too large (4454.662->4462.371)! Learning rate decreased to 0.00052.
2024-12-01-17:33:55-root-INFO: grad norm: 484.630 451.208 176.856
2024-12-01-17:33:56-root-INFO: grad norm: 406.132 379.122 145.638
2024-12-01-17:33:57-root-INFO: grad norm: 345.485 323.012 122.571
2024-12-01-17:33:58-root-INFO: grad norm: 297.764 278.441 105.516
2024-12-01-17:33:59-root-INFO: grad norm: 260.356 244.524 89.407
2024-12-01-17:34:00-root-INFO: grad norm: 231.047 216.666 80.240
2024-12-01-17:34:00-root-INFO: Loss Change: 4467.197 -> 4320.738
2024-12-01-17:34:00-root-INFO: Regularization Change: 0.000 -> 0.507
2024-12-01-17:34:00-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-17:34:00-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-17:34:01-root-INFO: step: 219 lr_xt 0.00085206
2024-12-01-17:34:01-root-INFO: grad norm: 229.140 214.038 81.810
2024-12-01-17:34:02-root-INFO: grad norm: 319.736 299.723 111.344
2024-12-01-17:34:02-root-INFO: Loss too large (4286.949->4293.905)! Learning rate decreased to 0.00068.
2024-12-01-17:34:03-root-INFO: grad norm: 375.787 351.413 133.136
2024-12-01-17:34:04-root-INFO: grad norm: 450.992 420.785 162.275
2024-12-01-17:34:05-root-INFO: Loss too large (4272.024->4272.040)! Learning rate decreased to 0.00055.
2024-12-01-17:34:06-root-INFO: grad norm: 366.035 342.220 129.875
2024-12-01-17:34:07-root-INFO: grad norm: 301.208 281.500 107.162
2024-12-01-17:34:08-root-INFO: grad norm: 253.104 237.384 87.809
2024-12-01-17:34:09-root-INFO: grad norm: 217.460 203.759 75.966
2024-12-01-17:34:09-root-INFO: Loss Change: 4299.801 -> 4190.275
2024-12-01-17:34:09-root-INFO: Regularization Change: 0.000 -> 0.532
2024-12-01-17:34:09-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-01-17:34:09-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-17:34:10-root-INFO: step: 218 lr_xt 0.00089342
2024-12-01-17:34:10-root-INFO: grad norm: 372.240 345.776 137.848
2024-12-01-17:34:10-root-INFO: Loss too large (4176.866->4192.354)! Learning rate decreased to 0.00071.
2024-12-01-17:34:12-root-INFO: grad norm: 430.580 402.678 152.478
2024-12-01-17:34:13-root-INFO: grad norm: 510.168 476.936 181.117
2024-12-01-17:34:13-root-INFO: Loss too large (4164.294->4168.106)! Learning rate decreased to 0.00057.
2024-12-01-17:34:14-root-INFO: grad norm: 402.072 375.927 142.620
2024-12-01-17:34:15-root-INFO: grad norm: 321.902 301.807 111.951
2024-12-01-17:34:16-root-INFO: grad norm: 262.287 245.922 91.198
2024-12-01-17:34:17-root-INFO: grad norm: 219.541 206.897 73.429
2024-12-01-17:34:18-root-INFO: grad norm: 189.112 178.083 63.638
2024-12-01-17:34:19-root-INFO: Loss Change: 4176.866 -> 4062.897
2024-12-01-17:34:19-root-INFO: Regularization Change: 0.000 -> 0.473
2024-12-01-17:34:19-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-01-17:34:19-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:34:19-root-INFO: step: 217 lr_xt 0.00093664
2024-12-01-17:34:19-root-INFO: grad norm: 339.551 306.289 146.567
2024-12-01-17:34:20-root-INFO: Loss too large (4057.772->4059.528)! Learning rate decreased to 0.00075.
2024-12-01-17:34:21-root-INFO: grad norm: 361.777 337.924 129.188
2024-12-01-17:34:22-root-INFO: grad norm: 418.640 391.496 148.292
2024-12-01-17:34:23-root-INFO: grad norm: 493.391 460.821 176.290
2024-12-01-17:34:23-root-INFO: Loss too large (4033.090->4037.717)! Learning rate decreased to 0.00060.
2024-12-01-17:34:24-root-INFO: grad norm: 384.727 360.678 133.890
2024-12-01-17:34:25-root-INFO: grad norm: 303.694 284.225 106.987
2024-12-01-17:34:26-root-INFO: grad norm: 245.253 230.784 82.992
2024-12-01-17:34:27-root-INFO: grad norm: 203.547 190.988 70.392
2024-12-01-17:34:28-root-INFO: Loss Change: 4057.772 -> 3948.634
2024-12-01-17:34:28-root-INFO: Regularization Change: 0.000 -> 0.493
2024-12-01-17:34:28-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-01-17:34:28-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:34:28-root-INFO: step: 216 lr_xt 0.00098179
2024-12-01-17:34:28-root-INFO: grad norm: 369.401 338.296 148.367
2024-12-01-17:34:29-root-INFO: Loss too large (3942.064->3956.264)! Learning rate decreased to 0.00079.
2024-12-01-17:34:30-root-INFO: grad norm: 410.245 382.615 148.007
2024-12-01-17:34:31-root-INFO: grad norm: 481.349 451.165 167.772
2024-12-01-17:34:31-root-INFO: Loss too large (3927.173->3931.283)! Learning rate decreased to 0.00063.
2024-12-01-17:34:32-root-INFO: grad norm: 374.984 350.831 132.402
2024-12-01-17:34:33-root-INFO: grad norm: 297.390 279.710 101.008
2024-12-01-17:34:34-root-INFO: grad norm: 240.518 225.537 83.557
2024-12-01-17:34:35-root-INFO: grad norm: 199.708 188.728 65.305
2024-12-01-17:34:36-root-INFO: grad norm: 170.833 160.800 57.683
2024-12-01-17:34:37-root-INFO: Loss Change: 3942.064 -> 3834.037
2024-12-01-17:34:37-root-INFO: Regularization Change: 0.000 -> 0.471
2024-12-01-17:34:37-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-01-17:34:37-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:34:37-root-INFO: step: 215 lr_xt 0.00102894
2024-12-01-17:34:38-root-INFO: grad norm: 176.478 166.051 59.761
2024-12-01-17:34:39-root-INFO: grad norm: 246.379 231.165 85.237
2024-12-01-17:34:39-root-INFO: Loss too large (3811.592->3815.163)! Learning rate decreased to 0.00082.
2024-12-01-17:34:40-root-INFO: grad norm: 278.897 262.451 94.358
2024-12-01-17:34:41-root-INFO: grad norm: 318.925 298.883 111.273
2024-12-01-17:34:42-root-INFO: grad norm: 367.222 345.627 124.073
2024-12-01-17:34:43-root-INFO: grad norm: 424.584 397.345 149.627
2024-12-01-17:34:44-root-INFO: Loss too large (3792.178->3793.990)! Learning rate decreased to 0.00066.
2024-12-01-17:34:45-root-INFO: grad norm: 321.563 302.785 108.280
2024-12-01-17:34:46-root-INFO: grad norm: 248.525 233.075 86.259
2024-12-01-17:34:46-root-INFO: Loss Change: 3820.544 -> 3741.726
2024-12-01-17:34:46-root-INFO: Regularization Change: 0.000 -> 0.508
2024-12-01-17:34:46-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-01-17:34:46-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:34:47-root-INFO: step: 214 lr_xt 0.00107819
2024-12-01-17:34:47-root-INFO: grad norm: 339.154 313.101 130.357
2024-12-01-17:34:47-root-INFO: Loss too large (3728.736->3745.646)! Learning rate decreased to 0.00086.
2024-12-01-17:34:48-root-INFO: grad norm: 368.621 344.676 130.691
2024-12-01-17:34:49-root-INFO: grad norm: 416.994 392.136 141.823
2024-12-01-17:34:50-root-INFO: Loss too large (3716.711->3717.229)! Learning rate decreased to 0.00069.
2024-12-01-17:34:51-root-INFO: grad norm: 309.145 289.999 107.103
2024-12-01-17:34:52-root-INFO: grad norm: 235.106 222.123 77.048
2024-12-01-17:34:53-root-INFO: grad norm: 185.337 174.383 62.770
2024-12-01-17:34:54-root-INFO: grad norm: 152.564 145.030 47.352
2024-12-01-17:34:55-root-INFO: grad norm: 131.729 124.571 42.833
2024-12-01-17:34:56-root-INFO: Loss Change: 3728.736 -> 3639.212
2024-12-01-17:34:56-root-INFO: Regularization Change: 0.000 -> 0.411
2024-12-01-17:34:56-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-01-17:34:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:34:56-root-INFO: step: 213 lr_xt 0.00112961
2024-12-01-17:34:56-root-INFO: grad norm: 295.581 269.677 121.005
2024-12-01-17:34:57-root-INFO: Loss too large (3650.674->3652.663)! Learning rate decreased to 0.00090.
2024-12-01-17:34:58-root-INFO: grad norm: 307.497 287.890 108.043
2024-12-01-17:34:59-root-INFO: grad norm: 347.382 325.781 120.585
2024-12-01-17:35:00-root-INFO: grad norm: 401.053 376.290 138.744
2024-12-01-17:35:00-root-INFO: Loss too large (3628.170->3630.549)! Learning rate decreased to 0.00072.
2024-12-01-17:35:01-root-INFO: grad norm: 306.062 288.048 103.452
2024-12-01-17:35:02-root-INFO: grad norm: 239.423 225.336 80.913
2024-12-01-17:35:03-root-INFO: grad norm: 192.105 181.265 63.619
2024-12-01-17:35:04-root-INFO: grad norm: 159.612 150.616 52.827
2024-12-01-17:35:05-root-INFO: Loss Change: 3650.674 -> 3561.874
2024-12-01-17:35:05-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-01-17:35:05-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-01-17:35:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-17:35:05-root-INFO: step: 212 lr_xt 0.00118329
2024-12-01-17:35:05-root-INFO: grad norm: 142.906 135.436 45.597
2024-12-01-17:35:06-root-INFO: grad norm: 197.116 186.482 63.870
2024-12-01-17:35:07-root-INFO: Loss too large (3537.129->3538.114)! Learning rate decreased to 0.00095.
2024-12-01-17:35:08-root-INFO: grad norm: 221.736 210.096 70.901
2024-12-01-17:35:09-root-INFO: grad norm: 252.316 237.950 83.923
2024-12-01-17:35:10-root-INFO: grad norm: 290.559 275.172 93.300
2024-12-01-17:35:11-root-INFO: grad norm: 336.369 316.571 113.698
2024-12-01-17:35:11-root-INFO: Loss too large (3518.619->3518.774)! Learning rate decreased to 0.00076.
2024-12-01-17:35:12-root-INFO: grad norm: 257.278 243.681 82.532
2024-12-01-17:35:13-root-INFO: grad norm: 202.463 191.120 66.815
2024-12-01-17:35:14-root-INFO: Loss Change: 3544.712 -> 3479.693
2024-12-01-17:35:14-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-01-17:35:14-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-01-17:35:14-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-17:35:14-root-INFO: step: 211 lr_xt 0.00123933
2024-12-01-17:35:15-root-INFO: grad norm: 341.015 311.159 139.540
2024-12-01-17:35:15-root-INFO: Loss too large (3479.781->3489.030)! Learning rate decreased to 0.00099.
2024-12-01-17:35:16-root-INFO: grad norm: 346.410 323.540 123.781
2024-12-01-17:35:17-root-INFO: grad norm: 405.749 383.150 133.520
2024-12-01-17:35:17-root-INFO: Loss too large (3460.880->3468.756)! Learning rate decreased to 0.00079.
2024-12-01-17:35:18-root-INFO: grad norm: 322.051 303.353 108.138
2024-12-01-17:35:19-root-INFO: grad norm: 260.187 246.654 82.820
2024-12-01-17:35:20-root-INFO: grad norm: 215.314 203.302 70.912
2024-12-01-17:35:21-root-INFO: grad norm: 181.114 171.953 56.872
2024-12-01-17:35:22-root-INFO: grad norm: 156.379 147.934 50.694
2024-12-01-17:35:23-root-INFO: Loss Change: 3479.781 -> 3386.583
2024-12-01-17:35:23-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-01-17:35:23-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-01-17:35:23-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:35:23-root-INFO: step: 210 lr_xt 0.00129780
2024-12-01-17:35:24-root-INFO: grad norm: 208.445 195.628 71.965
2024-12-01-17:35:24-root-INFO: Loss too large (3382.447->3387.014)! Learning rate decreased to 0.00104.
2024-12-01-17:35:25-root-INFO: grad norm: 238.727 226.970 73.995
2024-12-01-17:35:26-root-INFO: grad norm: 285.043 270.030 91.286
2024-12-01-17:35:26-root-INFO: Loss too large (3372.930->3373.582)! Learning rate decreased to 0.00083.
2024-12-01-17:35:27-root-INFO: grad norm: 231.799 220.166 72.509
2024-12-01-17:35:28-root-INFO: grad norm: 191.506 181.422 61.322
2024-12-01-17:35:29-root-INFO: grad norm: 163.062 155.175 50.102
2024-12-01-17:35:30-root-INFO: grad norm: 142.038 134.867 44.562
2024-12-01-17:35:32-root-INFO: grad norm: 127.226 121.298 38.384
2024-12-01-17:35:32-root-INFO: Loss Change: 3382.447 -> 3317.351
2024-12-01-17:35:32-root-INFO: Regularization Change: 0.000 -> 0.411
2024-12-01-17:35:32-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-01-17:35:32-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:35:33-root-INFO: step: 209 lr_xt 0.00135882
2024-12-01-17:35:33-root-INFO: grad norm: 200.042 182.590 81.718
2024-12-01-17:35:34-root-INFO: grad norm: 283.556 265.925 98.426
2024-12-01-17:35:34-root-INFO: Loss too large (3298.558->3326.897)! Learning rate decreased to 0.00109.
2024-12-01-17:35:35-root-INFO: Loss too large (3298.558->3299.049)! Learning rate decreased to 0.00087.
2024-12-01-17:35:36-root-INFO: grad norm: 239.487 226.229 78.580
2024-12-01-17:35:37-root-INFO: grad norm: 215.455 204.727 67.140
2024-12-01-17:35:38-root-INFO: grad norm: 197.193 186.852 63.021
2024-12-01-17:35:39-root-INFO: grad norm: 183.222 174.336 56.366
2024-12-01-17:35:40-root-INFO: grad norm: 171.785 162.956 54.366
2024-12-01-17:35:41-root-INFO: grad norm: 162.952 155.131 49.879
2024-12-01-17:35:41-root-INFO: Loss Change: 3303.856 -> 3236.522
2024-12-01-17:35:41-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-01-17:35:41-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-01-17:35:41-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:35:42-root-INFO: step: 208 lr_xt 0.00142247
2024-12-01-17:35:42-root-INFO: grad norm: 234.200 217.223 87.543
2024-12-01-17:35:42-root-INFO: Loss too large (3240.251->3246.258)! Learning rate decreased to 0.00114.
2024-12-01-17:35:44-root-INFO: grad norm: 273.450 259.045 87.582
2024-12-01-17:35:44-root-INFO: Loss too large (3231.481->3236.201)! Learning rate decreased to 0.00091.
2024-12-01-17:35:45-root-INFO: grad norm: 255.759 242.236 82.062
2024-12-01-17:35:46-root-INFO: grad norm: 248.283 236.251 76.354
2024-12-01-17:35:47-root-INFO: grad norm: 244.296 231.492 78.052
2024-12-01-17:35:48-root-INFO: grad norm: 242.583 230.770 74.780
2024-12-01-17:35:49-root-INFO: grad norm: 242.418 229.852 77.036
2024-12-01-17:35:50-root-INFO: grad norm: 243.788 231.744 75.679
2024-12-01-17:35:51-root-INFO: Loss Change: 3240.251 -> 3177.879
2024-12-01-17:35:51-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-01-17:35:51-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-01-17:35:51-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:35:51-root-INFO: step: 207 lr_xt 0.00150141
2024-12-01-17:35:51-root-INFO: grad norm: 276.407 259.507 95.168
2024-12-01-17:35:52-root-INFO: Loss too large (3164.448->3203.671)! Learning rate decreased to 0.00120.
2024-12-01-17:35:52-root-INFO: Loss too large (3164.448->3171.620)! Learning rate decreased to 0.00096.
2024-12-01-17:35:53-root-INFO: grad norm: 266.631 254.125 80.701
2024-12-01-17:35:54-root-INFO: grad norm: 270.909 256.685 86.628
2024-12-01-17:35:55-root-INFO: grad norm: 279.242 266.071 84.748
2024-12-01-17:35:56-root-INFO: grad norm: 291.213 276.207 92.276
2024-12-01-17:35:57-root-INFO: grad norm: 305.138 290.283 94.047
2024-12-01-17:35:58-root-INFO: grad norm: 322.079 305.731 101.308
2024-12-01-17:35:59-root-INFO: grad norm: 339.633 322.574 106.288
2024-12-01-17:36:00-root-INFO: Loss Change: 3164.448 -> 3117.349
2024-12-01-17:36:00-root-INFO: Regularization Change: 0.000 -> 0.441
2024-12-01-17:36:00-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-01-17:36:00-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:36:00-root-INFO: step: 206 lr_xt 0.00157117
2024-12-01-17:36:00-root-INFO: grad norm: 386.036 366.251 122.002
2024-12-01-17:36:01-root-INFO: Loss too large (3102.981->3215.878)! Learning rate decreased to 0.00126.
2024-12-01-17:36:01-root-INFO: Loss too large (3102.981->3140.653)! Learning rate decreased to 0.00101.
2024-12-01-17:36:02-root-INFO: grad norm: 398.466 379.191 122.429
2024-12-01-17:36:03-root-INFO: grad norm: 421.464 401.267 128.905
2024-12-01-17:36:03-root-INFO: Loss too large (3093.264->3093.266)! Learning rate decreased to 0.00080.
2024-12-01-17:36:04-root-INFO: grad norm: 286.700 273.310 86.596
2024-12-01-17:36:05-root-INFO: grad norm: 199.399 189.179 63.021
2024-12-01-17:36:06-root-INFO: grad norm: 150.529 143.961 43.981
2024-12-01-17:36:08-root-INFO: grad norm: 121.310 115.293 37.729
2024-12-01-17:36:09-root-INFO: grad norm: 105.077 100.778 29.749
2024-12-01-17:36:09-root-INFO: Loss Change: 3102.981 -> 3018.096
2024-12-01-17:36:09-root-INFO: Regularization Change: 0.000 -> 0.366
2024-12-01-17:36:09-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-17:36:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-17:36:10-root-INFO: step: 205 lr_xt 0.00164390
2024-12-01-17:36:10-root-INFO: grad norm: 151.970 139.973 59.181
2024-12-01-17:36:11-root-INFO: grad norm: 233.640 221.500 74.334
2024-12-01-17:36:11-root-INFO: Loss too large (3000.689->3041.060)! Learning rate decreased to 0.00132.
2024-12-01-17:36:12-root-INFO: Loss too large (3000.689->3011.976)! Learning rate decreased to 0.00105.
2024-12-01-17:36:13-root-INFO: grad norm: 272.737 259.559 83.753
2024-12-01-17:36:13-root-INFO: Loss too large (2996.257->2998.367)! Learning rate decreased to 0.00084.
2024-12-01-17:36:14-root-INFO: grad norm: 228.369 217.938 68.229
2024-12-01-17:36:15-root-INFO: grad norm: 194.512 184.920 60.328
2024-12-01-17:36:16-root-INFO: grad norm: 170.377 162.754 50.393
2024-12-01-17:36:17-root-INFO: grad norm: 151.976 144.622 46.702
2024-12-01-17:36:18-root-INFO: grad norm: 138.631 132.569 40.545
2024-12-01-17:36:19-root-INFO: Loss Change: 3007.985 -> 2947.027
2024-12-01-17:36:19-root-INFO: Regularization Change: 0.000 -> 0.447
2024-12-01-17:36:19-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-01-17:36:19-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-17:36:19-root-INFO: step: 204 lr_xt 0.00171973
2024-12-01-17:36:19-root-INFO: grad norm: 174.212 161.984 64.119
2024-12-01-17:36:20-root-INFO: grad norm: 314.301 299.519 95.255
2024-12-01-17:36:21-root-INFO: Loss too large (2942.069->3058.777)! Learning rate decreased to 0.00138.
2024-12-01-17:36:21-root-INFO: Loss too large (2942.069->2989.952)! Learning rate decreased to 0.00110.
2024-12-01-17:36:21-root-INFO: Loss too large (2942.069->2950.784)! Learning rate decreased to 0.00088.
2024-12-01-17:36:22-root-INFO: grad norm: 289.949 275.924 89.086
2024-12-01-17:36:23-root-INFO: grad norm: 280.756 267.863 84.103
2024-12-01-17:36:24-root-INFO: grad norm: 273.755 260.456 84.288
2024-12-01-17:36:25-root-INFO: grad norm: 269.464 257.016 80.954
2024-12-01-17:36:26-root-INFO: grad norm: 266.961 254.157 81.687
2024-12-01-17:36:27-root-INFO: grad norm: 266.568 254.134 80.464
2024-12-01-17:36:28-root-INFO: Loss Change: 2943.388 -> 2889.077
2024-12-01-17:36:28-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-01-17:36:28-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-01-17:36:28-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:36:28-root-INFO: step: 203 lr_xt 0.00179875
2024-12-01-17:36:29-root-INFO: grad norm: 255.521 243.347 77.929
2024-12-01-17:36:29-root-INFO: Loss too large (2874.676->2954.865)! Learning rate decreased to 0.00144.
2024-12-01-17:36:29-root-INFO: Loss too large (2874.676->2908.567)! Learning rate decreased to 0.00115.
2024-12-01-17:36:30-root-INFO: Loss too large (2874.676->2881.806)! Learning rate decreased to 0.00092.
2024-12-01-17:36:31-root-INFO: grad norm: 251.538 240.210 74.638
2024-12-01-17:36:32-root-INFO: grad norm: 252.790 241.644 74.234
2024-12-01-17:36:33-root-INFO: grad norm: 256.974 245.288 76.612
2024-12-01-17:36:34-root-INFO: grad norm: 263.806 252.255 77.206
2024-12-01-17:36:35-root-INFO: grad norm: 272.842 260.321 81.707
2024-12-01-17:36:36-root-INFO: grad norm: 284.490 272.082 83.099
2024-12-01-17:36:37-root-INFO: grad norm: 297.844 284.040 89.623
2024-12-01-17:36:37-root-INFO: Loss Change: 2874.676 -> 2829.481
2024-12-01-17:36:37-root-INFO: Regularization Change: 0.000 -> 0.379
2024-12-01-17:36:37-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-01-17:36:37-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:36:38-root-INFO: step: 202 lr_xt 0.00188111
2024-12-01-17:36:38-root-INFO: grad norm: 324.749 308.944 100.077
2024-12-01-17:36:39-root-INFO: Loss too large (2818.629->2942.084)! Learning rate decreased to 0.00150.
2024-12-01-17:36:39-root-INFO: Loss too large (2818.629->2867.430)! Learning rate decreased to 0.00120.
2024-12-01-17:36:39-root-INFO: Loss too large (2818.629->2824.720)! Learning rate decreased to 0.00096.
2024-12-01-17:36:40-root-INFO: grad norm: 324.638 308.791 100.189
2024-12-01-17:36:41-root-INFO: grad norm: 365.364 349.850 105.338
2024-12-01-17:36:42-root-INFO: Loss too large (2797.998->2800.148)! Learning rate decreased to 0.00077.
2024-12-01-17:36:43-root-INFO: grad norm: 275.367 262.849 82.081
2024-12-01-17:36:44-root-INFO: grad norm: 213.005 203.583 62.651
2024-12-01-17:36:45-root-INFO: grad norm: 172.863 165.090 51.252
2024-12-01-17:36:46-root-INFO: grad norm: 145.400 139.024 42.585
2024-12-01-17:36:47-root-INFO: grad norm: 127.236 121.634 37.340
2024-12-01-17:36:47-root-INFO: Loss Change: 2818.629 -> 2735.708
2024-12-01-17:36:47-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-01-17:36:47-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-01-17:36:47-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:36:48-root-INFO: step: 201 lr_xt 0.00196691
2024-12-01-17:36:48-root-INFO: grad norm: 160.568 152.245 51.025
2024-12-01-17:36:48-root-INFO: Loss too large (2735.549->2760.491)! Learning rate decreased to 0.00157.
2024-12-01-17:36:49-root-INFO: Loss too large (2735.549->2743.467)! Learning rate decreased to 0.00126.
2024-12-01-17:36:50-root-INFO: grad norm: 252.423 242.189 71.148
2024-12-01-17:36:50-root-INFO: Loss too large (2734.120->2753.770)! Learning rate decreased to 0.00101.
2024-12-01-17:36:51-root-INFO: grad norm: 311.744 298.564 89.686
2024-12-01-17:36:51-root-INFO: Loss too large (2733.694->2738.904)! Learning rate decreased to 0.00081.
2024-12-01-17:36:52-root-INFO: grad norm: 259.810 249.080 73.895
2024-12-01-17:36:53-root-INFO: grad norm: 219.616 210.429 62.857
2024-12-01-17:36:54-root-INFO: grad norm: 190.890 183.004 54.299
2024-12-01-17:36:55-root-INFO: grad norm: 169.148 162.252 47.803
2024-12-01-17:36:56-root-INFO: grad norm: 153.437 147.086 43.685
2024-12-01-17:36:57-root-INFO: Loss Change: 2735.549 -> 2682.011
2024-12-01-17:36:57-root-INFO: Regularization Change: 0.000 -> 0.366
2024-12-01-17:36:57-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-01-17:36:57-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:36:57-root-INFO: step: 200 lr_xt 0.00205630
2024-12-01-17:36:58-root-INFO: grad norm: 133.732 123.079 52.304
2024-12-01-17:36:59-root-INFO: grad norm: 171.233 163.505 50.863
2024-12-01-17:36:59-root-INFO: Loss too large (2648.546->2686.393)! Learning rate decreased to 0.00165.
2024-12-01-17:36:59-root-INFO: Loss too large (2648.546->2663.065)! Learning rate decreased to 0.00132.
2024-12-01-17:37:00-root-INFO: Loss too large (2648.546->2650.042)! Learning rate decreased to 0.00105.
2024-12-01-17:37:01-root-INFO: grad norm: 221.932 213.533 60.476
2024-12-01-17:37:01-root-INFO: Loss too large (2643.269->2648.256)! Learning rate decreased to 0.00084.
2024-12-01-17:37:02-root-INFO: grad norm: 231.966 222.290 66.298
2024-12-01-17:37:03-root-INFO: grad norm: 245.200 236.012 66.495
2024-12-01-17:37:04-root-INFO: grad norm: 260.944 250.107 74.422
2024-12-01-17:37:05-root-INFO: grad norm: 279.996 269.634 75.466
2024-12-01-17:37:06-root-INFO: grad norm: 301.017 288.515 85.853
2024-12-01-17:37:07-root-INFO: Loss Change: 2665.929 -> 2619.063
2024-12-01-17:37:07-root-INFO: Regularization Change: 0.000 -> 0.517
2024-12-01-17:37:07-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-01-17:37:07-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-17:37:07-root-INFO: step: 199 lr_xt 0.00214940
2024-12-01-17:37:08-root-INFO: grad norm: 311.662 300.481 82.729
2024-12-01-17:37:08-root-INFO: Loss too large (2599.147->2839.259)! Learning rate decreased to 0.00172.
2024-12-01-17:37:08-root-INFO: Loss too large (2599.147->2727.474)! Learning rate decreased to 0.00138.
2024-12-01-17:37:09-root-INFO: Loss too large (2599.147->2657.471)! Learning rate decreased to 0.00110.
2024-12-01-17:37:09-root-INFO: Loss too large (2599.147->2616.005)! Learning rate decreased to 0.00088.
2024-12-01-17:37:10-root-INFO: grad norm: 323.920 310.803 91.243
2024-12-01-17:37:11-root-INFO: grad norm: 345.990 333.626 91.665
2024-12-01-17:37:12-root-INFO: grad norm: 371.382 357.006 102.331
2024-12-01-17:37:13-root-INFO: grad norm: 399.061 385.037 104.862
2024-12-01-17:37:13-root-INFO: Loss too large (2585.034->2585.113)! Learning rate decreased to 0.00070.
2024-12-01-17:37:14-root-INFO: grad norm: 275.283 264.507 76.265
2024-12-01-17:37:14-root-INFO: grad norm: 196.689 189.738 51.827
2024-12-01-17:37:15-root-INFO: grad norm: 150.185 144.088 42.358
2024-12-01-17:37:15-root-INFO: Loss Change: 2599.147 -> 2535.471
2024-12-01-17:37:15-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-01-17:37:15-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-01-17:37:15-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-17:37:15-root-INFO: step: 198 lr_xt 0.00224635
2024-12-01-17:37:15-root-INFO: grad norm: 141.443 131.652 51.710
2024-12-01-17:37:16-root-INFO: Loss too large (2529.881->2531.627)! Learning rate decreased to 0.00180.
2024-12-01-17:37:16-root-INFO: grad norm: 271.528 262.146 70.758
2024-12-01-17:37:16-root-INFO: Loss too large (2523.481->2651.217)! Learning rate decreased to 0.00144.
2024-12-01-17:37:16-root-INFO: Loss too large (2523.481->2585.502)! Learning rate decreased to 0.00115.
2024-12-01-17:37:16-root-INFO: Loss too large (2523.481->2546.479)! Learning rate decreased to 0.00092.
2024-12-01-17:37:17-root-INFO: Loss too large (2523.481->2524.555)! Learning rate decreased to 0.00074.
2024-12-01-17:37:17-root-INFO: grad norm: 226.521 218.005 61.530
2024-12-01-17:37:18-root-INFO: grad norm: 194.917 187.721 52.472
2024-12-01-17:37:18-root-INFO: grad norm: 171.274 165.025 45.843
2024-12-01-17:37:19-root-INFO: grad norm: 154.076 148.235 42.019
2024-12-01-17:37:19-root-INFO: grad norm: 141.045 135.977 37.468
2024-12-01-17:37:20-root-INFO: grad norm: 131.372 126.297 36.165
2024-12-01-17:37:20-root-INFO: Loss Change: 2529.881 -> 2476.777
2024-12-01-17:37:20-root-INFO: Regularization Change: 0.000 -> 0.352
2024-12-01-17:37:20-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-01-17:37:20-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:37:20-root-INFO: step: 197 lr_xt 0.00234729
2024-12-01-17:37:20-root-INFO: grad norm: 102.337 96.414 34.310
2024-12-01-17:37:21-root-INFO: grad norm: 189.097 181.993 51.344
2024-12-01-17:37:21-root-INFO: Loss too large (2444.905->2568.138)! Learning rate decreased to 0.00188.
2024-12-01-17:37:21-root-INFO: Loss too large (2444.905->2511.616)! Learning rate decreased to 0.00150.
2024-12-01-17:37:21-root-INFO: Loss too large (2444.905->2477.180)! Learning rate decreased to 0.00120.
2024-12-01-17:37:21-root-INFO: Loss too large (2444.905->2456.988)! Learning rate decreased to 0.00096.
2024-12-01-17:37:21-root-INFO: Loss too large (2444.905->2445.713)! Learning rate decreased to 0.00077.
2024-12-01-17:37:22-root-INFO: grad norm: 189.492 183.735 46.356
2024-12-01-17:37:22-root-INFO: grad norm: 192.528 185.512 51.500
2024-12-01-17:37:23-root-INFO: grad norm: 197.558 191.738 47.599
2024-12-01-17:37:23-root-INFO: grad norm: 204.393 197.076 54.197
2024-12-01-17:37:24-root-INFO: grad norm: 213.361 207.104 51.290
2024-12-01-17:37:24-root-INFO: grad norm: 224.208 216.319 58.950
2024-12-01-17:37:25-root-INFO: Loss Change: 2457.123 -> 2416.603
2024-12-01-17:37:25-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-01-17:37:25-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-01-17:37:25-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:37:25-root-INFO: step: 196 lr_xt 0.00245238
2024-12-01-17:37:25-root-INFO: grad norm: 238.670 229.581 65.238
2024-12-01-17:37:25-root-INFO: Loss too large (2406.504->2609.940)! Learning rate decreased to 0.00196.
2024-12-01-17:37:25-root-INFO: Loss too large (2406.504->2518.785)! Learning rate decreased to 0.00157.
2024-12-01-17:37:25-root-INFO: Loss too large (2406.504->2461.940)! Learning rate decreased to 0.00126.
2024-12-01-17:37:26-root-INFO: Loss too large (2406.504->2428.054)! Learning rate decreased to 0.00100.
2024-12-01-17:37:26-root-INFO: Loss too large (2406.504->2408.886)! Learning rate decreased to 0.00080.
2024-12-01-17:37:26-root-INFO: grad norm: 246.261 237.568 64.853
2024-12-01-17:37:27-root-INFO: grad norm: 271.046 262.349 68.109
2024-12-01-17:37:27-root-INFO: grad norm: 304.802 295.543 74.555
2024-12-01-17:37:27-root-INFO: Loss too large (2393.972->2394.037)! Learning rate decreased to 0.00064.
2024-12-01-17:37:28-root-INFO: grad norm: 225.110 217.913 56.466
2024-12-01-17:37:28-root-INFO: grad norm: 174.035 168.448 43.741
2024-12-01-17:37:29-root-INFO: grad norm: 139.354 134.520 36.388
2024-12-01-17:37:29-root-INFO: grad norm: 116.678 112.603 30.565
2024-12-01-17:37:30-root-INFO: Loss Change: 2406.504 -> 2360.074
2024-12-01-17:37:30-root-INFO: Regularization Change: 0.000 -> 0.206
2024-12-01-17:37:30-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-01-17:37:30-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:37:30-root-INFO: step: 195 lr_xt 0.00256175
2024-12-01-17:37:30-root-INFO: grad norm: 102.116 95.896 35.093
2024-12-01-17:37:30-root-INFO: Loss too large (2348.768->2353.996)! Learning rate decreased to 0.00205.
2024-12-01-17:37:31-root-INFO: grad norm: 266.546 258.886 63.440
2024-12-01-17:37:31-root-INFO: Loss too large (2347.739->2559.248)! Learning rate decreased to 0.00164.
2024-12-01-17:37:31-root-INFO: Loss too large (2347.739->2464.439)! Learning rate decreased to 0.00131.
2024-12-01-17:37:31-root-INFO: Loss too large (2347.739->2405.141)! Learning rate decreased to 0.00105.
2024-12-01-17:37:31-root-INFO: Loss too large (2347.739->2369.787)! Learning rate decreased to 0.00084.
2024-12-01-17:37:31-root-INFO: Loss too large (2347.739->2349.831)! Learning rate decreased to 0.00067.
2024-12-01-17:37:32-root-INFO: grad norm: 216.313 210.050 51.676
2024-12-01-17:37:32-root-INFO: grad norm: 180.712 175.274 44.000
2024-12-01-17:37:33-root-INFO: grad norm: 153.844 149.179 37.596
2024-12-01-17:37:33-root-INFO: grad norm: 134.424 130.112 33.772
2024-12-01-17:37:34-root-INFO: grad norm: 119.780 115.936 30.099
2024-12-01-17:37:34-root-INFO: grad norm: 109.022 105.281 28.314
2024-12-01-17:37:35-root-INFO: Loss Change: 2348.768 -> 2312.379
2024-12-01-17:37:35-root-INFO: Regularization Change: 0.000 -> 0.243
2024-12-01-17:37:35-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-01-17:37:35-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:37:35-root-INFO: step: 194 lr_xt 0.00267557
2024-12-01-17:37:35-root-INFO: grad norm: 112.800 104.756 41.833
2024-12-01-17:37:35-root-INFO: Loss too large (2302.908->2307.079)! Learning rate decreased to 0.00214.
2024-12-01-17:37:36-root-INFO: grad norm: 273.991 266.938 61.765
2024-12-01-17:37:36-root-INFO: Loss too large (2299.895->2564.199)! Learning rate decreased to 0.00171.
2024-12-01-17:37:36-root-INFO: Loss too large (2299.895->2451.050)! Learning rate decreased to 0.00137.
2024-12-01-17:37:36-root-INFO: Loss too large (2299.895->2378.642)! Learning rate decreased to 0.00110.
2024-12-01-17:37:36-root-INFO: Loss too large (2299.895->2334.499)! Learning rate decreased to 0.00088.
2024-12-01-17:37:36-root-INFO: Loss too large (2299.895->2308.925)! Learning rate decreased to 0.00070.
2024-12-01-17:37:37-root-INFO: grad norm: 263.389 255.944 62.183
2024-12-01-17:37:37-root-INFO: grad norm: 254.737 247.978 58.288
2024-12-01-17:37:38-root-INFO: grad norm: 247.366 240.508 57.844
2024-12-01-17:37:38-root-INFO: grad norm: 241.697 235.202 55.656
2024-12-01-17:37:39-root-INFO: grad norm: 237.267 230.730 55.310
2024-12-01-17:37:39-root-INFO: grad norm: 234.314 227.971 54.152
2024-12-01-17:37:40-root-INFO: Loss Change: 2302.908 -> 2272.858
2024-12-01-17:37:40-root-INFO: Regularization Change: 0.000 -> 0.257
2024-12-01-17:37:40-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-01-17:37:40-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-17:37:40-root-INFO: step: 193 lr_xt 0.00279399
2024-12-01-17:37:40-root-INFO: grad norm: 184.784 177.754 50.487
2024-12-01-17:37:40-root-INFO: Loss too large (2259.232->2420.087)! Learning rate decreased to 0.00224.
2024-12-01-17:37:40-root-INFO: Loss too large (2259.232->2349.700)! Learning rate decreased to 0.00179.
2024-12-01-17:37:40-root-INFO: Loss too large (2259.232->2305.867)! Learning rate decreased to 0.00143.
2024-12-01-17:37:40-root-INFO: Loss too large (2259.232->2279.578)! Learning rate decreased to 0.00114.
2024-12-01-17:37:41-root-INFO: Loss too large (2259.232->2264.484)! Learning rate decreased to 0.00092.
2024-12-01-17:37:41-root-INFO: grad norm: 242.793 236.178 56.288
2024-12-01-17:37:41-root-INFO: Loss too large (2256.326->2264.029)! Learning rate decreased to 0.00073.
2024-12-01-17:37:42-root-INFO: grad norm: 247.923 241.077 57.858
2024-12-01-17:37:42-root-INFO: grad norm: 256.470 250.064 56.963
2024-12-01-17:37:43-root-INFO: grad norm: 267.474 260.423 61.010
2024-12-01-17:37:43-root-INFO: grad norm: 279.865 272.985 61.671
2024-12-01-17:37:44-root-INFO: grad norm: 293.937 286.427 66.017
2024-12-01-17:37:44-root-INFO: grad norm: 308.416 300.845 67.917
2024-12-01-17:37:44-root-INFO: Loss Change: 2259.232 -> 2240.631
2024-12-01-17:37:44-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-01-17:37:44-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-01-17:37:44-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-17:37:44-root-INFO: step: 192 lr_xt 0.00291718
2024-12-01-17:37:45-root-INFO: grad norm: 253.651 247.123 57.173
2024-12-01-17:37:45-root-INFO: Loss too large (2218.265->2615.832)! Learning rate decreased to 0.00233.
2024-12-01-17:37:45-root-INFO: Loss too large (2218.265->2463.291)! Learning rate decreased to 0.00187.
2024-12-01-17:37:45-root-INFO: Loss too large (2218.265->2360.020)! Learning rate decreased to 0.00149.
2024-12-01-17:37:45-root-INFO: Loss too large (2218.265->2293.400)! Learning rate decreased to 0.00119.
2024-12-01-17:37:45-root-INFO: Loss too large (2218.265->2252.337)! Learning rate decreased to 0.00096.
2024-12-01-17:37:46-root-INFO: Loss too large (2218.265->2228.233)! Learning rate decreased to 0.00076.
2024-12-01-17:37:46-root-INFO: grad norm: 262.763 256.333 57.777
2024-12-01-17:37:47-root-INFO: grad norm: 278.226 271.707 59.876
2024-12-01-17:37:47-root-INFO: grad norm: 297.566 290.590 64.057
2024-12-01-17:37:48-root-INFO: grad norm: 320.906 313.655 67.829
2024-12-01-17:37:48-root-INFO: Loss too large (2210.124->2210.748)! Learning rate decreased to 0.00061.
2024-12-01-17:37:48-root-INFO: grad norm: 223.276 217.849 48.927
2024-12-01-17:37:49-root-INFO: grad norm: 159.770 155.671 35.958
2024-12-01-17:37:49-root-INFO: grad norm: 121.476 117.959 29.018
2024-12-01-17:37:50-root-INFO: Loss Change: 2218.265 -> 2182.931
2024-12-01-17:37:50-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-01-17:37:50-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-01-17:37:50-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-17:37:50-root-INFO: step: 191 lr_xt 0.00304531
2024-12-01-17:37:50-root-INFO: grad norm: 134.919 124.671 51.576
2024-12-01-17:37:50-root-INFO: grad norm: 211.279 206.198 46.054
2024-12-01-17:37:51-root-INFO: Loss too large (2168.189->2451.469)! Learning rate decreased to 0.00244.
2024-12-01-17:37:51-root-INFO: Loss too large (2168.189->2337.835)! Learning rate decreased to 0.00195.
2024-12-01-17:37:51-root-INFO: Loss too large (2168.189->2263.481)! Learning rate decreased to 0.00156.
2024-12-01-17:37:51-root-INFO: Loss too large (2168.189->2216.816)! Learning rate decreased to 0.00125.
2024-12-01-17:37:51-root-INFO: Loss too large (2168.189->2188.716)! Learning rate decreased to 0.00100.
2024-12-01-17:37:51-root-INFO: Loss too large (2168.189->2172.579)! Learning rate decreased to 0.00080.
2024-12-01-17:37:52-root-INFO: grad norm: 253.074 246.618 56.797
2024-12-01-17:37:52-root-INFO: Loss too large (2163.905->2166.152)! Learning rate decreased to 0.00064.
2024-12-01-17:37:52-root-INFO: grad norm: 217.072 212.446 44.575
2024-12-01-17:37:53-root-INFO: grad norm: 187.485 182.571 42.644
2024-12-01-17:37:53-root-INFO: grad norm: 164.161 160.500 34.474
2024-12-01-17:37:54-root-INFO: grad norm: 145.170 141.012 34.495
2024-12-01-17:37:54-root-INFO: grad norm: 130.185 127.027 28.501
2024-12-01-17:37:55-root-INFO: Loss Change: 2179.780 -> 2138.067
2024-12-01-17:37:55-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-01-17:37:55-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-01-17:37:55-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-17:37:55-root-INFO: step: 190 lr_xt 0.00317856
2024-12-01-17:37:55-root-INFO: grad norm: 95.419 89.102 34.139
2024-12-01-17:37:55-root-INFO: grad norm: 197.490 193.194 40.972
2024-12-01-17:37:56-root-INFO: Loss too large (2121.859->2431.542)! Learning rate decreased to 0.00254.
2024-12-01-17:37:56-root-INFO: Loss too large (2121.859->2312.104)! Learning rate decreased to 0.00203.
2024-12-01-17:37:56-root-INFO: Loss too large (2121.859->2232.566)! Learning rate decreased to 0.00163.
2024-12-01-17:37:56-root-INFO: Loss too large (2121.859->2181.816)! Learning rate decreased to 0.00130.
2024-12-01-17:37:56-root-INFO: Loss too large (2121.859->2150.678)! Learning rate decreased to 0.00104.
2024-12-01-17:37:56-root-INFO: Loss too large (2121.859->2132.345)! Learning rate decreased to 0.00083.
2024-12-01-17:37:56-root-INFO: Loss too large (2121.859->2122.096)! Learning rate decreased to 0.00067.
2024-12-01-17:37:57-root-INFO: grad norm: 185.771 181.173 41.072
2024-12-01-17:37:57-root-INFO: grad norm: 177.295 173.588 36.064
2024-12-01-17:37:58-root-INFO: grad norm: 170.838 166.545 38.057
2024-12-01-17:37:58-root-INFO: grad norm: 165.849 162.349 33.894
2024-12-01-17:37:59-root-INFO: grad norm: 161.797 157.657 36.369
2024-12-01-17:37:59-root-INFO: grad norm: 158.683 155.281 32.680
2024-12-01-17:38:00-root-INFO: Loss Change: 2129.675 -> 2099.492
2024-12-01-17:38:00-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-01-17:38:00-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-01-17:38:00-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-17:38:00-root-INFO: step: 189 lr_xt 0.00331709
2024-12-01-17:38:00-root-INFO: grad norm: 133.660 127.618 39.732
2024-12-01-17:38:00-root-INFO: Loss too large (2089.123->2205.158)! Learning rate decreased to 0.00265.
2024-12-01-17:38:00-root-INFO: Loss too large (2089.123->2155.290)! Learning rate decreased to 0.00212.
2024-12-01-17:38:00-root-INFO: Loss too large (2089.123->2124.335)! Learning rate decreased to 0.00170.
2024-12-01-17:38:01-root-INFO: Loss too large (2089.123->2105.708)! Learning rate decreased to 0.00136.
2024-12-01-17:38:01-root-INFO: Loss too large (2089.123->2094.898)! Learning rate decreased to 0.00109.
2024-12-01-17:38:01-root-INFO: grad norm: 227.160 222.925 43.660
2024-12-01-17:38:01-root-INFO: Loss too large (2088.925->2114.180)! Learning rate decreased to 0.00087.
2024-12-01-17:38:02-root-INFO: Loss too large (2088.925->2096.113)! Learning rate decreased to 0.00070.
2024-12-01-17:38:02-root-INFO: grad norm: 231.178 225.818 49.491
2024-12-01-17:38:03-root-INFO: grad norm: 236.687 232.431 44.683
2024-12-01-17:38:03-root-INFO: grad norm: 243.257 237.884 50.842
2024-12-01-17:38:04-root-INFO: grad norm: 250.460 245.978 47.174
2024-12-01-17:38:04-root-INFO: grad norm: 258.357 252.855 53.034
2024-12-01-17:38:05-root-INFO: grad norm: 266.388 261.609 50.232
2024-12-01-17:38:05-root-INFO: Loss Change: 2089.123 -> 2075.369
2024-12-01-17:38:05-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-01-17:38:05-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-01-17:38:05-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-17:38:05-root-INFO: step: 188 lr_xt 0.00346111
2024-12-01-17:38:05-root-INFO: grad norm: 171.852 166.824 41.266
2024-12-01-17:38:05-root-INFO: Loss too large (2056.634->2296.631)! Learning rate decreased to 0.00277.
2024-12-01-17:38:05-root-INFO: Loss too large (2056.634->2200.900)! Learning rate decreased to 0.00222.
2024-12-01-17:38:06-root-INFO: Loss too large (2056.634->2138.686)! Learning rate decreased to 0.00177.
2024-12-01-17:38:06-root-INFO: Loss too large (2056.634->2099.727)! Learning rate decreased to 0.00142.
2024-12-01-17:38:06-root-INFO: Loss too large (2056.634->2076.200)! Learning rate decreased to 0.00113.
2024-12-01-17:38:06-root-INFO: Loss too large (2056.634->2062.565)! Learning rate decreased to 0.00091.
2024-12-01-17:38:07-root-INFO: grad norm: 230.697 226.363 44.505
2024-12-01-17:38:07-root-INFO: Loss too large (2055.092->2063.448)! Learning rate decreased to 0.00073.
2024-12-01-17:38:07-root-INFO: grad norm: 240.876 236.152 47.471
2024-12-01-17:38:08-root-INFO: grad norm: 253.502 249.106 47.008
2024-12-01-17:38:08-root-INFO: grad norm: 268.003 262.964 51.721
2024-12-01-17:38:09-root-INFO: grad norm: 283.152 278.384 51.747
2024-12-01-17:38:09-root-INFO: grad norm: 299.210 293.772 56.788
2024-12-01-17:38:10-root-INFO: grad norm: 314.815 309.578 57.185
2024-12-01-17:38:10-root-INFO: Loss Change: 2056.634 -> 2046.545
2024-12-01-17:38:10-root-INFO: Regularization Change: 0.000 -> 0.142
2024-12-01-17:38:10-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-01-17:38:10-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-17:38:10-root-INFO: step: 187 lr_xt 0.00361079
2024-12-01-17:38:10-root-INFO: grad norm: 331.075 324.040 67.892
2024-12-01-17:38:10-root-INFO: Loss too large (2048.304->2956.192)! Learning rate decreased to 0.00289.
2024-12-01-17:38:11-root-INFO: Loss too large (2048.304->2667.958)! Learning rate decreased to 0.00231.
2024-12-01-17:38:11-root-INFO: Loss too large (2048.304->2446.965)! Learning rate decreased to 0.00185.
2024-12-01-17:38:11-root-INFO: Loss too large (2048.304->2287.994)! Learning rate decreased to 0.00148.
2024-12-01-17:38:11-root-INFO: Loss too large (2048.304->2180.226)! Learning rate decreased to 0.00118.
2024-12-01-17:38:11-root-INFO: Loss too large (2048.304->2111.158)! Learning rate decreased to 0.00095.
2024-12-01-17:38:11-root-INFO: Loss too large (2048.304->2069.344)! Learning rate decreased to 0.00076.
2024-12-01-17:38:12-root-INFO: grad norm: 344.326 338.874 61.028
2024-12-01-17:38:12-root-INFO: grad norm: 362.330 355.755 68.711
2024-12-01-17:38:13-root-INFO: Loss too large (2043.637->2044.134)! Learning rate decreased to 0.00061.
2024-12-01-17:38:13-root-INFO: grad norm: 244.081 240.162 43.565
2024-12-01-17:38:14-root-INFO: grad norm: 168.884 164.649 37.582
2024-12-01-17:38:14-root-INFO: grad norm: 123.578 120.879 25.685
2024-12-01-17:38:15-root-INFO: grad norm: 95.224 91.596 26.033
2024-12-01-17:38:15-root-INFO: grad norm: 78.432 75.735 20.389
2024-12-01-17:38:15-root-INFO: Loss Change: 2048.304 -> 2007.274
2024-12-01-17:38:15-root-INFO: Regularization Change: 0.000 -> 0.120
2024-12-01-17:38:15-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-01-17:38:15-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:38:16-root-INFO: step: 186 lr_xt 0.00376634
2024-12-01-17:38:16-root-INFO: grad norm: 112.144 107.433 32.163
2024-12-01-17:38:16-root-INFO: Loss too large (1996.719->2056.887)! Learning rate decreased to 0.00301.
2024-12-01-17:38:16-root-INFO: Loss too large (1996.719->2028.998)! Learning rate decreased to 0.00241.
2024-12-01-17:38:16-root-INFO: Loss too large (1996.719->2011.892)! Learning rate decreased to 0.00193.
2024-12-01-17:38:16-root-INFO: Loss too large (1996.719->2001.826)! Learning rate decreased to 0.00154.
2024-12-01-17:38:17-root-INFO: grad norm: 227.058 222.974 42.871
2024-12-01-17:38:17-root-INFO: Loss too large (1996.212->2067.183)! Learning rate decreased to 0.00123.
2024-12-01-17:38:17-root-INFO: Loss too large (1996.212->2030.274)! Learning rate decreased to 0.00099.
2024-12-01-17:38:17-root-INFO: Loss too large (1996.212->2008.322)! Learning rate decreased to 0.00079.
2024-12-01-17:38:18-root-INFO: grad norm: 254.255 250.448 43.832
2024-12-01-17:38:18-root-INFO: Loss too large (1995.976->1996.225)! Learning rate decreased to 0.00063.
2024-12-01-17:38:19-root-INFO: grad norm: 183.900 180.153 36.934
2024-12-01-17:38:19-root-INFO: grad norm: 138.101 135.520 26.578
2024-12-01-17:38:20-root-INFO: grad norm: 107.070 103.888 25.911
2024-12-01-17:38:20-root-INFO: grad norm: 87.075 84.626 20.505
2024-12-01-17:38:21-root-INFO: grad norm: 74.275 71.114 21.436
2024-12-01-17:38:21-root-INFO: Loss Change: 1996.719 -> 1972.267
2024-12-01-17:38:21-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-01-17:38:21-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-01-17:38:21-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:38:21-root-INFO: step: 185 lr_xt 0.00392795
2024-12-01-17:38:21-root-INFO: grad norm: 113.118 109.839 27.036
2024-12-01-17:38:22-root-INFO: Loss too large (1962.032->2074.355)! Learning rate decreased to 0.00314.
2024-12-01-17:38:22-root-INFO: Loss too large (1962.032->2028.006)! Learning rate decreased to 0.00251.
2024-12-01-17:38:22-root-INFO: Loss too large (1962.032->1998.561)! Learning rate decreased to 0.00201.
2024-12-01-17:38:22-root-INFO: Loss too large (1962.032->1980.485)! Learning rate decreased to 0.00161.
2024-12-01-17:38:22-root-INFO: Loss too large (1962.032->1969.773)! Learning rate decreased to 0.00129.
2024-12-01-17:38:22-root-INFO: Loss too large (1962.032->1963.689)! Learning rate decreased to 0.00103.
2024-12-01-17:38:23-root-INFO: grad norm: 163.383 159.548 35.195
2024-12-01-17:38:23-root-INFO: Loss too large (1960.445->1965.398)! Learning rate decreased to 0.00082.
2024-12-01-17:38:24-root-INFO: grad norm: 186.302 183.578 31.741
2024-12-01-17:38:24-root-INFO: grad norm: 214.525 210.506 41.328
2024-12-01-17:38:24-root-INFO: Loss too large (1958.662->1959.151)! Learning rate decreased to 0.00066.
2024-12-01-17:38:25-root-INFO: grad norm: 162.784 160.302 28.317
2024-12-01-17:38:25-root-INFO: grad norm: 126.094 122.858 28.383
2024-12-01-17:38:26-root-INFO: grad norm: 101.208 98.999 21.029
2024-12-01-17:38:26-root-INFO: grad norm: 84.159 81.069 22.592
2024-12-01-17:38:26-root-INFO: Loss Change: 1962.032 -> 1940.921
2024-12-01-17:38:26-root-INFO: Regularization Change: 0.000 -> 0.117
2024-12-01-17:38:26-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-01-17:38:26-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:38:27-root-INFO: step: 184 lr_xt 0.00409583
2024-12-01-17:38:27-root-INFO: grad norm: 218.897 213.182 49.689
2024-12-01-17:38:27-root-INFO: Loss too large (1948.498->2351.061)! Learning rate decreased to 0.00328.
2024-12-01-17:38:27-root-INFO: Loss too large (1948.498->2211.612)! Learning rate decreased to 0.00262.
2024-12-01-17:38:27-root-INFO: Loss too large (1948.498->2108.885)! Learning rate decreased to 0.00210.
2024-12-01-17:38:27-root-INFO: Loss too large (1948.498->2038.321)! Learning rate decreased to 0.00168.
2024-12-01-17:38:27-root-INFO: Loss too large (1948.498->1992.790)! Learning rate decreased to 0.00134.
2024-12-01-17:38:28-root-INFO: Loss too large (1948.498->1965.064)! Learning rate decreased to 0.00107.
2024-12-01-17:38:28-root-INFO: Loss too large (1948.498->1949.194)! Learning rate decreased to 0.00086.
2024-12-01-17:38:28-root-INFO: grad norm: 221.316 216.909 43.944
2024-12-01-17:38:29-root-INFO: grad norm: 257.633 254.017 43.013
2024-12-01-17:38:29-root-INFO: Loss too large (1940.542->1941.937)! Learning rate decreased to 0.00069.
2024-12-01-17:38:29-root-INFO: grad norm: 197.464 193.524 39.254
2024-12-01-17:38:30-root-INFO: grad norm: 155.374 152.789 28.221
2024-12-01-17:38:30-root-INFO: grad norm: 124.395 120.956 29.048
2024-12-01-17:38:31-root-INFO: grad norm: 102.378 100.051 21.704
2024-12-01-17:38:31-root-INFO: grad norm: 86.568 83.249 23.740
2024-12-01-17:38:32-root-INFO: Loss Change: 1948.498 -> 1914.388
2024-12-01-17:38:32-root-INFO: Regularization Change: 0.000 -> 0.147
2024-12-01-17:38:32-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-01-17:38:32-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:38:32-root-INFO: step: 183 lr_xt 0.00427020
2024-12-01-17:38:32-root-INFO: grad norm: 171.824 168.152 35.329
2024-12-01-17:38:32-root-INFO: Loss too large (1911.339->2229.744)! Learning rate decreased to 0.00342.
2024-12-01-17:38:32-root-INFO: Loss too large (1911.339->2114.818)! Learning rate decreased to 0.00273.
2024-12-01-17:38:33-root-INFO: Loss too large (1911.339->2033.802)! Learning rate decreased to 0.00219.
2024-12-01-17:38:33-root-INFO: Loss too large (1911.339->1980.024)! Learning rate decreased to 0.00175.
2024-12-01-17:38:33-root-INFO: Loss too large (1911.339->1946.102)! Learning rate decreased to 0.00140.
2024-12-01-17:38:33-root-INFO: Loss too large (1911.339->1925.677)! Learning rate decreased to 0.00112.
2024-12-01-17:38:33-root-INFO: Loss too large (1911.339->1913.993)! Learning rate decreased to 0.00090.
2024-12-01-17:38:34-root-INFO: grad norm: 186.123 182.491 36.592
2024-12-01-17:38:34-root-INFO: grad norm: 214.597 211.665 35.354
2024-12-01-17:38:34-root-INFO: Loss too large (1906.941->1907.056)! Learning rate decreased to 0.00072.
2024-12-01-17:38:35-root-INFO: grad norm: 163.239 159.904 32.824
2024-12-01-17:38:35-root-INFO: grad norm: 127.724 125.391 24.297
2024-12-01-17:38:36-root-INFO: grad norm: 102.476 99.366 25.056
2024-12-01-17:38:36-root-INFO: grad norm: 85.230 82.868 19.925
2024-12-01-17:38:37-root-INFO: grad norm: 73.488 70.320 21.346
2024-12-01-17:38:37-root-INFO: Loss Change: 1911.339 -> 1884.703
2024-12-01-17:38:37-root-INFO: Regularization Change: 0.000 -> 0.131
2024-12-01-17:38:37-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-01-17:38:37-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-17:38:37-root-INFO: step: 182 lr_xt 0.00445127
2024-12-01-17:38:37-root-INFO: grad norm: 138.433 135.316 29.212
2024-12-01-17:38:38-root-INFO: Loss too large (1881.878->2092.327)! Learning rate decreased to 0.00356.
2024-12-01-17:38:38-root-INFO: Loss too large (1881.878->2011.801)! Learning rate decreased to 0.00285.
2024-12-01-17:38:38-root-INFO: Loss too large (1881.878->1957.618)! Learning rate decreased to 0.00228.
2024-12-01-17:38:38-root-INFO: Loss too large (1881.878->1922.903)! Learning rate decreased to 0.00182.
2024-12-01-17:38:38-root-INFO: Loss too large (1881.878->1901.581)! Learning rate decreased to 0.00146.
2024-12-01-17:38:39-root-INFO: Loss too large (1881.878->1889.028)! Learning rate decreased to 0.00117.
2024-12-01-17:38:39-root-INFO: Loss too large (1881.878->1882.012)! Learning rate decreased to 0.00093.
2024-12-01-17:38:39-root-INFO: grad norm: 146.360 143.153 30.469
2024-12-01-17:38:40-root-INFO: grad norm: 166.318 164.090 27.135
2024-12-01-17:38:40-root-INFO: grad norm: 191.083 187.790 35.318
2024-12-01-17:38:41-root-INFO: grad norm: 219.487 217.058 32.563
2024-12-01-17:38:41-root-INFO: Loss too large (1875.687->1876.253)! Learning rate decreased to 0.00075.
2024-12-01-17:38:41-root-INFO: grad norm: 163.807 160.748 31.508
2024-12-01-17:38:42-root-INFO: grad norm: 125.880 124.021 21.554
2024-12-01-17:38:42-root-INFO: grad norm: 99.320 96.516 23.433
2024-12-01-17:38:43-root-INFO: Loss Change: 1881.878 -> 1858.294
2024-12-01-17:38:43-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-01-17:38:43-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-01-17:38:43-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-17:38:43-root-INFO: step: 181 lr_xt 0.00463927
2024-12-01-17:38:43-root-INFO: grad norm: 164.200 161.255 30.960
2024-12-01-17:38:43-root-INFO: Loss too large (1856.904->2160.807)! Learning rate decreased to 0.00371.
2024-12-01-17:38:43-root-INFO: Loss too large (1856.904->2051.887)! Learning rate decreased to 0.00297.
2024-12-01-17:38:43-root-INFO: Loss too large (1856.904->1974.357)! Learning rate decreased to 0.00238.
2024-12-01-17:38:44-root-INFO: Loss too large (1856.904->1922.639)! Learning rate decreased to 0.00190.
2024-12-01-17:38:44-root-INFO: Loss too large (1856.904->1889.953)! Learning rate decreased to 0.00152.
2024-12-01-17:38:44-root-INFO: Loss too large (1856.904->1870.275)! Learning rate decreased to 0.00122.
2024-12-01-17:38:44-root-INFO: Loss too large (1856.904->1859.032)! Learning rate decreased to 0.00097.
2024-12-01-17:38:45-root-INFO: grad norm: 168.330 164.963 33.499
2024-12-01-17:38:45-root-INFO: grad norm: 187.682 185.580 28.011
2024-12-01-17:38:46-root-INFO: grad norm: 210.705 207.261 37.940
2024-12-01-17:38:46-root-INFO: Loss too large (1850.870->1850.980)! Learning rate decreased to 0.00078.
2024-12-01-17:38:46-root-INFO: grad norm: 154.304 152.445 23.877
2024-12-01-17:38:47-root-INFO: grad norm: 115.503 112.611 25.685
2024-12-01-17:38:47-root-INFO: grad norm: 90.322 88.500 18.051
2024-12-01-17:38:48-root-INFO: grad norm: 73.928 71.026 20.509
2024-12-01-17:38:48-root-INFO: Loss Change: 1856.904 -> 1831.249
2024-12-01-17:38:48-root-INFO: Regularization Change: 0.000 -> 0.139
2024-12-01-17:38:48-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-01-17:38:48-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-17:38:48-root-INFO: step: 180 lr_xt 0.00483443
2024-12-01-17:38:49-root-INFO: grad norm: 167.043 163.014 36.466
2024-12-01-17:38:49-root-INFO: Loss too large (1835.596->2113.637)! Learning rate decreased to 0.00387.
2024-12-01-17:38:49-root-INFO: Loss too large (1835.596->2013.616)! Learning rate decreased to 0.00309.
2024-12-01-17:38:49-root-INFO: Loss too large (1835.596->1941.897)! Learning rate decreased to 0.00248.
2024-12-01-17:38:49-root-INFO: Loss too large (1835.596->1893.857)! Learning rate decreased to 0.00198.
2024-12-01-17:38:49-root-INFO: Loss too large (1835.596->1863.483)! Learning rate decreased to 0.00158.
2024-12-01-17:38:50-root-INFO: Loss too large (1835.596->1845.273)! Learning rate decreased to 0.00127.
2024-12-01-17:38:50-root-INFO: grad norm: 225.071 221.656 39.062
2024-12-01-17:38:50-root-INFO: Loss too large (1834.991->1848.678)! Learning rate decreased to 0.00101.
2024-12-01-17:38:51-root-INFO: grad norm: 243.547 240.984 35.235
2024-12-01-17:38:51-root-INFO: grad norm: 266.403 262.640 44.615
2024-12-01-17:38:51-root-INFO: Loss too large (1833.152->1833.871)! Learning rate decreased to 0.00081.
2024-12-01-17:38:52-root-INFO: grad norm: 186.314 184.248 27.667
2024-12-01-17:38:52-root-INFO: grad norm: 132.659 129.735 27.698
2024-12-01-17:38:53-root-INFO: grad norm: 99.169 97.354 18.886
2024-12-01-17:38:53-root-INFO: grad norm: 77.862 74.963 21.051
2024-12-01-17:38:54-root-INFO: Loss Change: 1835.596 -> 1806.035
2024-12-01-17:38:54-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-01-17:38:54-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-01-17:38:54-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-17:38:54-root-INFO: step: 179 lr_xt 0.00503698
2024-12-01-17:38:54-root-INFO: grad norm: 138.147 135.583 26.494
2024-12-01-17:38:54-root-INFO: Loss too large (1803.216->2033.256)! Learning rate decreased to 0.00403.
2024-12-01-17:38:54-root-INFO: Loss too large (1803.216->1947.440)! Learning rate decreased to 0.00322.
2024-12-01-17:38:54-root-INFO: Loss too large (1803.216->1888.055)! Learning rate decreased to 0.00258.
2024-12-01-17:38:55-root-INFO: Loss too large (1803.216->1849.404)! Learning rate decreased to 0.00206.
2024-12-01-17:38:55-root-INFO: Loss too large (1803.216->1825.475)! Learning rate decreased to 0.00165.
2024-12-01-17:38:55-root-INFO: Loss too large (1803.216->1811.332)! Learning rate decreased to 0.00132.
2024-12-01-17:38:55-root-INFO: Loss too large (1803.216->1803.411)! Learning rate decreased to 0.00106.
2024-12-01-17:38:56-root-INFO: grad norm: 136.788 133.965 27.649
2024-12-01-17:38:56-root-INFO: grad norm: 144.175 142.386 22.638
2024-12-01-17:38:56-root-INFO: grad norm: 153.553 150.754 29.185
2024-12-01-17:38:57-root-INFO: grad norm: 163.511 161.749 23.936
2024-12-01-17:38:58-root-INFO: grad norm: 174.075 171.242 31.280
2024-12-01-17:38:58-root-INFO: grad norm: 184.445 182.597 26.043
2024-12-01-17:38:59-root-INFO: grad norm: 194.901 191.980 33.618
2024-12-01-17:38:59-root-INFO: Loss Change: 1803.216 -> 1786.115
2024-12-01-17:38:59-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-01-17:38:59-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-01-17:38:59-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-17:38:59-root-INFO: step: 178 lr_xt 0.00524717
2024-12-01-17:38:59-root-INFO: grad norm: 282.641 279.257 43.604
2024-12-01-17:39:00-root-INFO: Loss too large (1792.824->2531.606)! Learning rate decreased to 0.00420.
2024-12-01-17:39:00-root-INFO: Loss too large (1792.824->2332.578)! Learning rate decreased to 0.00336.
2024-12-01-17:39:00-root-INFO: Loss too large (1792.824->2159.822)! Learning rate decreased to 0.00269.
2024-12-01-17:39:00-root-INFO: Loss too large (1792.824->2020.090)! Learning rate decreased to 0.00215.
2024-12-01-17:39:00-root-INFO: Loss too large (1792.824->1917.348)! Learning rate decreased to 0.00172.
2024-12-01-17:39:00-root-INFO: Loss too large (1792.824->1849.067)! Learning rate decreased to 0.00138.
2024-12-01-17:39:01-root-INFO: Loss too large (1792.824->1807.664)! Learning rate decreased to 0.00110.
2024-12-01-17:39:01-root-INFO: grad norm: 263.479 260.293 40.853
2024-12-01-17:39:01-root-INFO: grad norm: 252.679 250.180 35.445
2024-12-01-17:39:02-root-INFO: grad norm: 242.489 239.375 38.736
2024-12-01-17:39:02-root-INFO: grad norm: 232.815 230.588 32.120
2024-12-01-17:39:03-root-INFO: grad norm: 222.369 219.397 36.238
2024-12-01-17:39:03-root-INFO: grad norm: 212.312 210.289 29.241
2024-12-01-17:39:04-root-INFO: grad norm: 201.815 198.982 33.698
2024-12-01-17:39:04-root-INFO: Loss Change: 1792.824 -> 1754.884
2024-12-01-17:39:04-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-01-17:39:04-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-01-17:39:04-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-17:39:04-root-INFO: step: 177 lr_xt 0.00546525
2024-12-01-17:39:05-root-INFO: grad norm: 249.826 247.143 36.513
2024-12-01-17:39:05-root-INFO: Loss too large (1756.668->2401.687)! Learning rate decreased to 0.00437.
2024-12-01-17:39:05-root-INFO: Loss too large (1756.668->2217.279)! Learning rate decreased to 0.00350.
2024-12-01-17:39:05-root-INFO: Loss too large (1756.668->2061.598)! Learning rate decreased to 0.00280.
2024-12-01-17:39:05-root-INFO: Loss too large (1756.668->1940.230)! Learning rate decreased to 0.00224.
2024-12-01-17:39:05-root-INFO: Loss too large (1756.668->1854.361)! Learning rate decreased to 0.00179.
2024-12-01-17:39:06-root-INFO: Loss too large (1756.668->1799.049)! Learning rate decreased to 0.00143.
2024-12-01-17:39:06-root-INFO: Loss too large (1756.668->1766.258)! Learning rate decreased to 0.00115.
2024-12-01-17:39:06-root-INFO: grad norm: 220.853 217.988 35.460
2024-12-01-17:39:07-root-INFO: grad norm: 199.132 197.157 27.982
2024-12-01-17:39:07-root-INFO: grad norm: 180.268 177.615 30.812
2024-12-01-17:39:08-root-INFO: grad norm: 164.443 162.759 23.475
2024-12-01-17:39:08-root-INFO: grad norm: 149.712 147.230 27.149
2024-12-01-17:39:09-root-INFO: grad norm: 137.067 135.546 20.363
2024-12-01-17:39:09-root-INFO: grad norm: 125.375 123.001 24.279
2024-12-01-17:39:09-root-INFO: Loss Change: 1756.668 -> 1716.893
2024-12-01-17:39:09-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-01-17:39:09-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-01-17:39:09-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-17:39:10-root-INFO: step: 176 lr_xt 0.00569148
2024-12-01-17:39:10-root-INFO: grad norm: 201.362 198.472 33.989
2024-12-01-17:39:10-root-INFO: Loss too large (1722.084->2122.552)! Learning rate decreased to 0.00455.
2024-12-01-17:39:10-root-INFO: Loss too large (1722.084->1995.645)! Learning rate decreased to 0.00364.
2024-12-01-17:39:10-root-INFO: Loss too large (1722.084->1894.423)! Learning rate decreased to 0.00291.
2024-12-01-17:39:11-root-INFO: Loss too large (1722.084->1820.857)! Learning rate decreased to 0.00233.
2024-12-01-17:39:11-root-INFO: Loss too large (1722.084->1771.693)! Learning rate decreased to 0.00186.
2024-12-01-17:39:11-root-INFO: Loss too large (1722.084->1741.065)! Learning rate decreased to 0.00149.
2024-12-01-17:39:11-root-INFO: Loss too large (1722.084->1723.216)! Learning rate decreased to 0.00119.
2024-12-01-17:39:11-root-INFO: grad norm: 159.522 157.041 28.021
2024-12-01-17:39:12-root-INFO: grad norm: 129.555 127.810 21.195
2024-12-01-17:39:12-root-INFO: grad norm: 111.478 109.072 23.035
2024-12-01-17:39:13-root-INFO: grad norm: 97.536 95.958 17.469
2024-12-01-17:39:14-root-INFO: grad norm: 86.279 83.907 20.092
2024-12-01-17:39:14-root-INFO: grad norm: 77.535 75.911 15.785
2024-12-01-17:39:15-root-INFO: grad norm: 70.605 68.217 18.209
2024-12-01-17:39:15-root-INFO: Loss Change: 1722.084 -> 1683.183
2024-12-01-17:39:15-root-INFO: Regularization Change: 0.000 -> 0.268
2024-12-01-17:39:15-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-01-17:39:15-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-17:39:15-root-INFO: step: 175 lr_xt 0.00592610
2024-12-01-17:39:15-root-INFO: grad norm: 162.707 160.172 28.607
2024-12-01-17:39:16-root-INFO: Loss too large (1684.360->1933.808)! Learning rate decreased to 0.00474.
2024-12-01-17:39:16-root-INFO: Loss too large (1684.360->1847.395)! Learning rate decreased to 0.00379.
2024-12-01-17:39:16-root-INFO: Loss too large (1684.360->1783.731)! Learning rate decreased to 0.00303.
2024-12-01-17:39:16-root-INFO: Loss too large (1684.360->1739.856)! Learning rate decreased to 0.00243.
2024-12-01-17:39:16-root-INFO: Loss too large (1684.360->1711.231)! Learning rate decreased to 0.00194.
2024-12-01-17:39:16-root-INFO: Loss too large (1684.360->1693.489)! Learning rate decreased to 0.00155.
2024-12-01-17:39:17-root-INFO: grad norm: 176.989 174.463 29.793
2024-12-01-17:39:17-root-INFO: grad norm: 196.954 194.948 28.041
2024-12-01-17:39:17-root-INFO: Loss too large (1679.246->1680.485)! Learning rate decreased to 0.00124.
2024-12-01-17:39:18-root-INFO: grad norm: 152.981 150.538 27.228
2024-12-01-17:39:18-root-INFO: grad norm: 118.623 117.117 18.847
2024-12-01-17:39:19-root-INFO: grad norm: 96.509 94.232 20.839
2024-12-01-17:39:19-root-INFO: grad norm: 80.991 79.439 15.777
2024-12-01-17:39:20-root-INFO: grad norm: 70.649 68.342 17.905
2024-12-01-17:39:20-root-INFO: Loss Change: 1684.360 -> 1645.552
2024-12-01-17:39:20-root-INFO: Regularization Change: 0.000 -> 0.338
2024-12-01-17:39:20-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-01-17:39:20-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-17:39:20-root-INFO: step: 174 lr_xt 0.00616941
2024-12-01-17:39:20-root-INFO: grad norm: 80.664 78.705 17.670
2024-12-01-17:39:21-root-INFO: Loss too large (1644.264->1679.752)! Learning rate decreased to 0.00494.
2024-12-01-17:39:21-root-INFO: Loss too large (1644.264->1661.575)! Learning rate decreased to 0.00395.
2024-12-01-17:39:21-root-INFO: Loss too large (1644.264->1650.658)! Learning rate decreased to 0.00316.
2024-12-01-17:39:21-root-INFO: Loss too large (1644.264->1644.452)! Learning rate decreased to 0.00253.
2024-12-01-17:39:22-root-INFO: grad norm: 140.168 137.719 26.085
2024-12-01-17:39:22-root-INFO: Loss too large (1641.191->1662.255)! Learning rate decreased to 0.00202.
2024-12-01-17:39:22-root-INFO: Loss too large (1641.191->1646.334)! Learning rate decreased to 0.00162.
2024-12-01-17:39:23-root-INFO: grad norm: 160.616 158.990 22.797
2024-12-01-17:39:23-root-INFO: grad norm: 177.274 174.578 30.797
2024-12-01-17:39:24-root-INFO: grad norm: 197.736 195.901 26.876
2024-12-01-17:39:24-root-INFO: grad norm: 209.957 206.972 35.277
2024-12-01-17:39:24-root-INFO: grad norm: 221.523 219.545 29.536
2024-12-01-17:39:25-root-INFO: grad norm: 226.695 223.539 37.695
2024-12-01-17:39:25-root-INFO: Loss Change: 1644.264 -> 1617.426
2024-12-01-17:39:25-root-INFO: Regularization Change: 0.000 -> 0.628
2024-12-01-17:39:25-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-01-17:39:25-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-17:39:25-root-INFO: step: 173 lr_xt 0.00642166
2024-12-01-17:39:26-root-INFO: grad norm: 360.723 356.638 54.139
2024-12-01-17:39:26-root-INFO: Loss too large (1640.351->2238.892)! Learning rate decreased to 0.00514.
2024-12-01-17:39:26-root-INFO: Loss too large (1640.351->2087.125)! Learning rate decreased to 0.00411.
2024-12-01-17:39:26-root-INFO: Loss too large (1640.351->1950.026)! Learning rate decreased to 0.00329.
2024-12-01-17:39:26-root-INFO: Loss too large (1640.351->1834.914)! Learning rate decreased to 0.00263.
2024-12-01-17:39:26-root-INFO: Loss too large (1640.351->1746.338)! Learning rate decreased to 0.00210.
2024-12-01-17:39:27-root-INFO: Loss too large (1640.351->1683.364)! Learning rate decreased to 0.00168.
2024-12-01-17:39:27-root-INFO: Loss too large (1640.351->1641.512)! Learning rate decreased to 0.00135.
2024-12-01-17:39:27-root-INFO: grad norm: 205.344 202.409 34.591
2024-12-01-17:39:28-root-INFO: grad norm: 83.903 82.022 17.666
2024-12-01-17:39:28-root-INFO: grad norm: 65.154 62.730 17.607
2024-12-01-17:39:29-root-INFO: grad norm: 59.320 57.166 15.841
2024-12-01-17:39:29-root-INFO: grad norm: 56.927 54.702 15.761
2024-12-01-17:39:30-root-INFO: grad norm: 55.498 53.378 15.194
2024-12-01-17:39:30-root-INFO: grad norm: 54.425 52.312 15.019
2024-12-01-17:39:31-root-INFO: Loss Change: 1640.351 -> 1559.052
2024-12-01-17:39:31-root-INFO: Regularization Change: 0.000 -> 0.462
2024-12-01-17:39:31-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-01-17:39:31-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-17:39:31-root-INFO: step: 172 lr_xt 0.00668315
2024-12-01-17:39:31-root-INFO: grad norm: 74.138 71.967 17.810
2024-12-01-17:39:31-root-INFO: Loss too large (1553.942->1571.788)! Learning rate decreased to 0.00535.
2024-12-01-17:39:31-root-INFO: Loss too large (1553.942->1562.249)! Learning rate decreased to 0.00428.
2024-12-01-17:39:32-root-INFO: Loss too large (1553.942->1556.033)! Learning rate decreased to 0.00342.
2024-12-01-17:39:32-root-INFO: grad norm: 141.341 139.110 25.011
2024-12-01-17:39:32-root-INFO: Loss too large (1552.288->1611.306)! Learning rate decreased to 0.00274.
2024-12-01-17:39:33-root-INFO: Loss too large (1552.288->1575.348)! Learning rate decreased to 0.00219.
2024-12-01-17:39:33-root-INFO: Loss too large (1552.288->1555.433)! Learning rate decreased to 0.00175.
2024-12-01-17:39:33-root-INFO: grad norm: 155.849 153.777 25.328
2024-12-01-17:39:33-root-INFO: Loss too large (1545.500->1547.030)! Learning rate decreased to 0.00140.
2024-12-01-17:39:34-root-INFO: grad norm: 122.645 120.672 21.911
2024-12-01-17:39:34-root-INFO: grad norm: 86.364 84.595 17.391
2024-12-01-17:39:35-root-INFO: grad norm: 76.051 74.347 16.006
2024-12-01-17:39:35-root-INFO: grad norm: 66.191 64.442 15.113
2024-12-01-17:39:36-root-INFO: grad norm: 61.082 59.371 14.358
2024-12-01-17:39:36-root-INFO: Loss Change: 1553.942 -> 1518.227
2024-12-01-17:39:36-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-01-17:39:36-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-01-17:39:36-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-17:39:36-root-INFO: step: 171 lr_xt 0.00695416
2024-12-01-17:39:37-root-INFO: grad norm: 120.531 118.461 22.247
2024-12-01-17:39:37-root-INFO: Loss too large (1515.191->1609.169)! Learning rate decreased to 0.00556.
2024-12-01-17:39:37-root-INFO: Loss too large (1515.191->1582.885)! Learning rate decreased to 0.00445.
2024-12-01-17:39:37-root-INFO: Loss too large (1515.191->1560.491)! Learning rate decreased to 0.00356.
2024-12-01-17:39:37-root-INFO: Loss too large (1515.191->1542.611)! Learning rate decreased to 0.00285.
2024-12-01-17:39:37-root-INFO: Loss too large (1515.191->1529.328)! Learning rate decreased to 0.00228.
2024-12-01-17:39:38-root-INFO: Loss too large (1515.191->1520.181)! Learning rate decreased to 0.00182.
2024-12-01-17:39:38-root-INFO: grad norm: 134.054 132.033 23.185
2024-12-01-17:39:39-root-INFO: grad norm: 166.414 164.232 26.857
2024-12-01-17:39:39-root-INFO: Loss too large (1510.197->1515.030)! Learning rate decreased to 0.00146.
2024-12-01-17:39:39-root-INFO: grad norm: 136.845 134.828 23.411
2024-12-01-17:39:40-root-INFO: grad norm: 100.134 98.373 18.696
2024-12-01-17:39:41-root-INFO: grad norm: 90.780 89.117 17.296
2024-12-01-17:39:41-root-INFO: grad norm: 80.289 78.628 16.245
2024-12-01-17:39:41-root-INFO: grad norm: 74.901 73.290 15.452
2024-12-01-17:39:42-root-INFO: Loss Change: 1515.191 -> 1483.316
2024-12-01-17:39:42-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-01-17:39:42-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-01-17:39:42-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-17:39:42-root-INFO: step: 170 lr_xt 0.00723499
2024-12-01-17:39:42-root-INFO: grad norm: 153.733 150.910 29.327
2024-12-01-17:39:42-root-INFO: Loss too large (1487.937->1609.925)! Learning rate decreased to 0.00579.
2024-12-01-17:39:42-root-INFO: Loss too large (1487.937->1584.152)! Learning rate decreased to 0.00463.
2024-12-01-17:39:43-root-INFO: Loss too large (1487.937->1558.917)! Learning rate decreased to 0.00370.
2024-12-01-17:39:43-root-INFO: Loss too large (1487.937->1535.734)! Learning rate decreased to 0.00296.
2024-12-01-17:39:43-root-INFO: Loss too large (1487.937->1516.175)! Learning rate decreased to 0.00237.
2024-12-01-17:39:43-root-INFO: Loss too large (1487.937->1501.146)! Learning rate decreased to 0.00190.
2024-12-01-17:39:43-root-INFO: Loss too large (1487.937->1490.659)! Learning rate decreased to 0.00152.
2024-12-01-17:39:44-root-INFO: grad norm: 134.136 132.356 21.778
2024-12-01-17:39:44-root-INFO: grad norm: 111.944 109.774 21.934
2024-12-01-17:39:45-root-INFO: grad norm: 104.534 102.950 18.133
2024-12-01-17:39:45-root-INFO: grad norm: 95.544 93.649 18.936
2024-12-01-17:39:46-root-INFO: grad norm: 90.858 89.325 16.617
2024-12-01-17:39:46-root-INFO: grad norm: 85.236 83.476 17.228
2024-12-01-17:39:47-root-INFO: grad norm: 81.829 80.307 15.711
2024-12-01-17:39:47-root-INFO: Loss Change: 1487.937 -> 1451.880
2024-12-01-17:39:47-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-01-17:39:47-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-01-17:39:47-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-17:39:47-root-INFO: step: 169 lr_xt 0.00752595
2024-12-01-17:39:48-root-INFO: grad norm: 146.845 144.260 27.432
2024-12-01-17:39:48-root-INFO: Loss too large (1455.823->1579.090)! Learning rate decreased to 0.00602.
2024-12-01-17:39:48-root-INFO: Loss too large (1455.823->1554.753)! Learning rate decreased to 0.00482.
2024-12-01-17:39:48-root-INFO: Loss too large (1455.823->1529.674)! Learning rate decreased to 0.00385.
2024-12-01-17:39:48-root-INFO: Loss too large (1455.823->1506.006)! Learning rate decreased to 0.00308.
2024-12-01-17:39:48-root-INFO: Loss too large (1455.823->1485.813)! Learning rate decreased to 0.00247.
2024-12-01-17:39:49-root-INFO: Loss too large (1455.823->1470.259)! Learning rate decreased to 0.00197.
2024-12-01-17:39:49-root-INFO: Loss too large (1455.823->1459.429)! Learning rate decreased to 0.00158.
2024-12-01-17:39:49-root-INFO: grad norm: 138.802 137.023 22.151
2024-12-01-17:39:50-root-INFO: grad norm: 128.588 126.443 23.392
2024-12-01-17:39:51-root-INFO: grad norm: 124.544 122.863 20.394
2024-12-01-17:39:51-root-INFO: grad norm: 118.588 116.636 21.432
2024-12-01-17:39:51-root-INFO: grad norm: 115.619 113.952 19.562
2024-12-01-17:39:52-root-INFO: grad norm: 111.059 109.216 20.152
2024-12-01-17:39:52-root-INFO: grad norm: 108.701 106.994 19.188
2024-12-01-17:39:53-root-INFO: Loss Change: 1455.823 -> 1414.948
2024-12-01-17:39:53-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-01-17:39:53-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-01-17:39:53-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-17:39:53-root-INFO: step: 168 lr_xt 0.00782735
2024-12-01-17:39:53-root-INFO: grad norm: 151.986 149.793 25.729
2024-12-01-17:39:53-root-INFO: Loss too large (1416.597->1551.600)! Learning rate decreased to 0.00626.
2024-12-01-17:39:53-root-INFO: Loss too large (1416.597->1523.148)! Learning rate decreased to 0.00501.
2024-12-01-17:39:54-root-INFO: Loss too large (1416.597->1494.278)! Learning rate decreased to 0.00401.
2024-12-01-17:39:54-root-INFO: Loss too large (1416.597->1467.487)! Learning rate decreased to 0.00321.
2024-12-01-17:39:54-root-INFO: Loss too large (1416.597->1445.083)! Learning rate decreased to 0.00256.
2024-12-01-17:39:54-root-INFO: Loss too large (1416.597->1428.267)! Learning rate decreased to 0.00205.
2024-12-01-17:39:54-root-INFO: Loss too large (1416.597->1416.962)! Learning rate decreased to 0.00164.
2024-12-01-17:39:55-root-INFO: grad norm: 145.101 143.028 24.443
2024-12-01-17:39:55-root-INFO: grad norm: 135.716 133.727 23.146
2024-12-01-17:39:56-root-INFO: grad norm: 132.103 129.830 24.402
2024-12-01-17:39:56-root-INFO: grad norm: 125.374 123.429 21.996
2024-12-01-17:39:56-root-INFO: grad norm: 120.077 117.537 24.569
2024-12-01-17:39:57-root-INFO: grad norm: 114.761 112.972 20.181
2024-12-01-17:39:57-root-INFO: grad norm: 112.334 109.715 24.118
2024-12-01-17:39:58-root-INFO: Loss Change: 1416.597 -> 1351.270
2024-12-01-17:39:58-root-INFO: Regularization Change: 0.000 -> 0.837
2024-12-01-17:39:58-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-01-17:39:58-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-17:39:58-root-INFO: step: 167 lr_xt 0.00813950
2024-12-01-17:39:58-root-INFO: grad norm: 172.070 169.724 28.317
2024-12-01-17:39:58-root-INFO: Loss too large (1357.346->1548.440)! Learning rate decreased to 0.00651.
2024-12-01-17:39:58-root-INFO: Loss too large (1357.346->1518.233)! Learning rate decreased to 0.00521.
2024-12-01-17:39:59-root-INFO: Loss too large (1357.346->1483.474)! Learning rate decreased to 0.00417.
2024-12-01-17:39:59-root-INFO: Loss too large (1357.346->1446.622)! Learning rate decreased to 0.00333.
2024-12-01-17:39:59-root-INFO: Loss too large (1357.346->1412.049)! Learning rate decreased to 0.00267.
2024-12-01-17:39:59-root-INFO: Loss too large (1357.346->1383.891)! Learning rate decreased to 0.00213.
2024-12-01-17:39:59-root-INFO: Loss too large (1357.346->1364.039)! Learning rate decreased to 0.00171.
2024-12-01-17:40:00-root-INFO: grad norm: 176.076 173.302 31.129
2024-12-01-17:40:00-root-INFO: grad norm: 181.658 179.660 26.869
2024-12-01-17:40:01-root-INFO: grad norm: 186.112 183.371 31.826
2024-12-01-17:40:01-root-INFO: grad norm: 187.868 185.921 26.975
2024-12-01-17:40:02-root-INFO: grad norm: 187.948 185.280 31.560
2024-12-01-17:40:02-root-INFO: grad norm: 185.604 183.694 26.560
2024-12-01-17:40:03-root-INFO: grad norm: 182.693 180.140 30.434
2024-12-01-17:40:03-root-INFO: Loss Change: 1357.346 -> 1324.519
2024-12-01-17:40:03-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-01-17:40:03-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-01-17:40:03-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-17:40:03-root-INFO: step: 166 lr_xt 0.00846273
2024-12-01-17:40:03-root-INFO: grad norm: 230.155 227.681 33.654
2024-12-01-17:40:03-root-INFO: Loss too large (1333.464->1554.856)! Learning rate decreased to 0.00677.
2024-12-01-17:40:03-root-INFO: Loss too large (1333.464->1528.907)! Learning rate decreased to 0.00542.
2024-12-01-17:40:04-root-INFO: Loss too large (1333.464->1497.476)! Learning rate decreased to 0.00433.
2024-12-01-17:40:04-root-INFO: Loss too large (1333.464->1459.233)! Learning rate decreased to 0.00347.
2024-12-01-17:40:04-root-INFO: Loss too large (1333.464->1416.530)! Learning rate decreased to 0.00277.
2024-12-01-17:40:04-root-INFO: Loss too large (1333.464->1375.146)! Learning rate decreased to 0.00222.
2024-12-01-17:40:04-root-INFO: Loss too large (1333.464->1341.777)! Learning rate decreased to 0.00177.
2024-12-01-17:40:05-root-INFO: grad norm: 201.966 199.413 32.007
2024-12-01-17:40:05-root-INFO: grad norm: 184.784 182.816 26.900
2024-12-01-17:40:06-root-INFO: grad norm: 171.062 168.814 27.644
2024-12-01-17:40:06-root-INFO: grad norm: 161.881 160.119 23.816
2024-12-01-17:40:07-root-INFO: grad norm: 153.600 151.533 25.114
2024-12-01-17:40:07-root-INFO: grad norm: 147.781 146.134 22.002
2024-12-01-17:40:08-root-INFO: grad norm: 142.453 140.509 23.454
2024-12-01-17:40:08-root-INFO: Loss Change: 1333.464 -> 1283.952
2024-12-01-17:40:08-root-INFO: Regularization Change: 0.000 -> 0.462
2024-12-01-17:40:08-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-01-17:40:08-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-17:40:08-root-INFO: step: 165 lr_xt 0.00879737
2024-12-01-17:40:08-root-INFO: grad norm: 173.970 171.924 26.600
2024-12-01-17:40:08-root-INFO: Loss too large (1288.783->1504.469)! Learning rate decreased to 0.00704.
2024-12-01-17:40:08-root-INFO: Loss too large (1288.783->1473.425)! Learning rate decreased to 0.00563.
2024-12-01-17:40:09-root-INFO: Loss too large (1288.783->1435.865)! Learning rate decreased to 0.00450.
2024-12-01-17:40:09-root-INFO: Loss too large (1288.783->1393.958)! Learning rate decreased to 0.00360.
2024-12-01-17:40:09-root-INFO: Loss too large (1288.783->1352.836)! Learning rate decreased to 0.00288.
2024-12-01-17:40:09-root-INFO: Loss too large (1288.783->1318.600)! Learning rate decreased to 0.00231.
2024-12-01-17:40:09-root-INFO: Loss too large (1288.783->1294.701)! Learning rate decreased to 0.00184.
2024-12-01-17:40:10-root-INFO: grad norm: 165.706 163.710 25.642
2024-12-01-17:40:10-root-INFO: grad norm: 158.777 157.002 23.680
2024-12-01-17:40:10-root-INFO: grad norm: 150.662 148.793 23.660
2024-12-01-17:40:11-root-INFO: grad norm: 145.249 143.610 21.757
2024-12-01-17:40:11-root-INFO: grad norm: 139.416 137.646 22.146
2024-12-01-17:40:12-root-INFO: grad norm: 135.778 134.220 20.512
2024-12-01-17:40:12-root-INFO: grad norm: 132.146 130.443 21.147
2024-12-01-17:40:13-root-INFO: Loss Change: 1288.783 -> 1252.415
2024-12-01-17:40:13-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-01-17:40:13-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-01-17:40:13-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-17:40:13-root-INFO: step: 164 lr_xt 0.00914377
2024-12-01-17:40:13-root-INFO: grad norm: 171.426 169.326 26.747
2024-12-01-17:40:13-root-INFO: Loss too large (1259.603->1480.837)! Learning rate decreased to 0.00732.
2024-12-01-17:40:13-root-INFO: Loss too large (1259.603->1449.566)! Learning rate decreased to 0.00585.
2024-12-01-17:40:13-root-INFO: Loss too large (1259.603->1410.619)! Learning rate decreased to 0.00468.
2024-12-01-17:40:14-root-INFO: Loss too large (1259.603->1366.348)! Learning rate decreased to 0.00375.
2024-12-01-17:40:14-root-INFO: Loss too large (1259.603->1322.672)! Learning rate decreased to 0.00300.
2024-12-01-17:40:14-root-INFO: Loss too large (1259.603->1286.753)! Learning rate decreased to 0.00240.
2024-12-01-17:40:14-root-INFO: Loss too large (1259.603->1262.408)! Learning rate decreased to 0.00192.
2024-12-01-17:40:15-root-INFO: grad norm: 155.818 153.898 24.389
2024-12-01-17:40:15-root-INFO: grad norm: 147.906 146.172 22.583
2024-12-01-17:40:16-root-INFO: grad norm: 138.655 136.945 21.713
2024-12-01-17:40:16-root-INFO: grad norm: 133.691 132.098 20.582
2024-12-01-17:40:16-root-INFO: grad norm: 128.167 126.564 20.211
2024-12-01-17:40:17-root-INFO: grad norm: 125.244 123.724 19.449
2024-12-01-17:40:17-root-INFO: grad norm: 122.311 120.762 19.403
2024-12-01-17:40:18-root-INFO: Loss Change: 1259.603 -> 1220.560
2024-12-01-17:40:18-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-01-17:40:18-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-01-17:40:18-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-17:40:18-root-INFO: step: 163 lr_xt 0.00950228
2024-12-01-17:40:18-root-INFO: grad norm: 154.671 152.922 23.197
2024-12-01-17:40:18-root-INFO: Loss too large (1222.387->1443.014)! Learning rate decreased to 0.00760.
2024-12-01-17:40:18-root-INFO: Loss too large (1222.387->1409.930)! Learning rate decreased to 0.00608.
2024-12-01-17:40:19-root-INFO: Loss too large (1222.387->1368.940)! Learning rate decreased to 0.00487.
2024-12-01-17:40:19-root-INFO: Loss too large (1222.387->1323.429)! Learning rate decreased to 0.00389.
2024-12-01-17:40:19-root-INFO: Loss too large (1222.387->1280.220)! Learning rate decreased to 0.00311.
2024-12-01-17:40:19-root-INFO: Loss too large (1222.387->1246.335)! Learning rate decreased to 0.00249.
2024-12-01-17:40:19-root-INFO: Loss too large (1222.387->1224.422)! Learning rate decreased to 0.00199.
2024-12-01-17:40:20-root-INFO: grad norm: 140.665 139.032 21.370
2024-12-01-17:40:20-root-INFO: grad norm: 135.253 133.738 20.193
2024-12-01-17:40:21-root-INFO: grad norm: 128.882 127.331 19.934
2024-12-01-17:40:21-root-INFO: grad norm: 126.008 124.564 19.026
2024-12-01-17:40:21-root-INFO: grad norm: 122.870 121.362 19.197
2024-12-01-17:40:22-root-INFO: grad norm: 121.529 120.116 18.475
2024-12-01-17:40:22-root-INFO: grad norm: 120.393 118.902 18.889
2024-12-01-17:40:23-root-INFO: Loss Change: 1222.387 -> 1188.185
2024-12-01-17:40:23-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-01-17:40:23-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-01-17:40:23-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-17:40:23-root-INFO: step: 162 lr_xt 0.00987325
2024-12-01-17:40:23-root-INFO: grad norm: 162.490 160.105 27.735
2024-12-01-17:40:23-root-INFO: Loss too large (1201.351->1432.078)! Learning rate decreased to 0.00790.
2024-12-01-17:40:23-root-INFO: Loss too large (1201.351->1399.157)! Learning rate decreased to 0.00632.
2024-12-01-17:40:23-root-INFO: Loss too large (1201.351->1357.869)! Learning rate decreased to 0.00506.
2024-12-01-17:40:24-root-INFO: Loss too large (1201.351->1310.363)! Learning rate decreased to 0.00404.
2024-12-01-17:40:24-root-INFO: Loss too large (1201.351->1263.424)! Learning rate decreased to 0.00324.
2024-12-01-17:40:24-root-INFO: Loss too large (1201.351->1225.611)! Learning rate decreased to 0.00259.
2024-12-01-17:40:24-root-INFO: grad norm: 226.064 223.749 32.268
2024-12-01-17:40:25-root-INFO: Loss too large (1201.066->1222.537)! Learning rate decreased to 0.00207.
2024-12-01-17:40:25-root-INFO: grad norm: 185.240 183.209 27.359
2024-12-01-17:40:25-root-INFO: grad norm: 130.161 128.682 19.566
2024-12-01-17:40:26-root-INFO: grad norm: 119.712 118.083 19.679
2024-12-01-17:40:26-root-INFO: grad norm: 107.897 106.588 16.757
2024-12-01-17:40:27-root-INFO: grad norm: 103.430 101.939 17.498
2024-12-01-17:40:27-root-INFO: grad norm: 98.903 97.655 15.666
2024-12-01-17:40:28-root-INFO: Loss Change: 1201.351 -> 1158.127
2024-12-01-17:40:28-root-INFO: Regularization Change: 0.000 -> 0.545
2024-12-01-17:40:28-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-01-17:40:28-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-17:40:28-root-INFO: step: 161 lr_xt 0.01025704
2024-12-01-17:40:28-root-INFO: grad norm: 129.291 127.729 20.032
2024-12-01-17:40:28-root-INFO: Loss too large (1160.792->1375.663)! Learning rate decreased to 0.00821.
2024-12-01-17:40:28-root-INFO: Loss too large (1160.792->1336.870)! Learning rate decreased to 0.00656.
2024-12-01-17:40:29-root-INFO: Loss too large (1160.792->1291.262)! Learning rate decreased to 0.00525.
2024-12-01-17:40:29-root-INFO: Loss too large (1160.792->1244.396)! Learning rate decreased to 0.00420.
2024-12-01-17:40:29-root-INFO: Loss too large (1160.792->1204.278)! Learning rate decreased to 0.00336.
2024-12-01-17:40:29-root-INFO: Loss too large (1160.792->1176.140)! Learning rate decreased to 0.00269.
2024-12-01-17:40:30-root-INFO: grad norm: 179.875 177.913 26.493
2024-12-01-17:40:30-root-INFO: Loss too large (1159.659->1173.897)! Learning rate decreased to 0.00215.
2024-12-01-17:40:30-root-INFO: grad norm: 151.900 150.376 21.462
2024-12-01-17:40:31-root-INFO: grad norm: 112.716 111.316 17.715
2024-12-01-17:40:31-root-INFO: grad norm: 103.712 102.403 16.428
2024-12-01-17:40:32-root-INFO: grad norm: 93.349 92.080 15.343
2024-12-01-17:40:32-root-INFO: grad norm: 89.359 88.107 14.910
2024-12-01-17:40:33-root-INFO: grad norm: 85.305 84.085 14.374
2024-12-01-17:40:33-root-INFO: Loss Change: 1160.792 -> 1125.890
2024-12-01-17:40:33-root-INFO: Regularization Change: 0.000 -> 0.496
2024-12-01-17:40:33-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-01-17:40:33-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-17:40:33-root-INFO: step: 160 lr_xt 0.01065404
2024-12-01-17:40:34-root-INFO: grad norm: 122.054 120.294 20.651
2024-12-01-17:40:34-root-INFO: Loss too large (1132.946->1349.438)! Learning rate decreased to 0.00852.
2024-12-01-17:40:34-root-INFO: Loss too large (1132.946->1308.101)! Learning rate decreased to 0.00682.
2024-12-01-17:40:34-root-INFO: Loss too large (1132.946->1260.394)! Learning rate decreased to 0.00545.
2024-12-01-17:40:34-root-INFO: Loss too large (1132.946->1212.536)! Learning rate decreased to 0.00436.
2024-12-01-17:40:34-root-INFO: Loss too large (1132.946->1172.923)! Learning rate decreased to 0.00349.
2024-12-01-17:40:35-root-INFO: Loss too large (1132.946->1146.161)! Learning rate decreased to 0.00279.
2024-12-01-17:40:35-root-INFO: grad norm: 171.660 169.860 24.792
2024-12-01-17:40:35-root-INFO: Loss too large (1131.001->1145.055)! Learning rate decreased to 0.00223.
2024-12-01-17:40:35-root-INFO: Loss too large (1131.001->1131.186)! Learning rate decreased to 0.00179.
2024-12-01-17:40:36-root-INFO: grad norm: 108.966 107.526 17.656
2024-12-01-17:40:36-root-INFO: grad norm: 56.596 55.440 11.379
2024-12-01-17:40:37-root-INFO: grad norm: 46.436 45.070 11.177
2024-12-01-17:40:37-root-INFO: grad norm: 41.136 39.853 10.195
2024-12-01-17:40:38-root-INFO: grad norm: 39.127 37.752 10.280
2024-12-01-17:40:38-root-INFO: grad norm: 38.101 36.774 9.969
2024-12-01-17:40:38-root-INFO: Loss Change: 1132.946 -> 1099.538
2024-12-01-17:40:38-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-01-17:40:38-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-01-17:40:38-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-17:40:39-root-INFO: step: 159 lr_xt 0.01106461
2024-12-01-17:40:39-root-INFO: grad norm: 57.563 56.179 12.548
2024-12-01-17:40:39-root-INFO: Loss too large (1098.425->1159.283)! Learning rate decreased to 0.00885.
2024-12-01-17:40:39-root-INFO: Loss too large (1098.425->1131.795)! Learning rate decreased to 0.00708.
2024-12-01-17:40:39-root-INFO: Loss too large (1098.425->1114.265)! Learning rate decreased to 0.00567.
2024-12-01-17:40:39-root-INFO: Loss too large (1098.425->1104.052)! Learning rate decreased to 0.00453.
2024-12-01-17:40:40-root-INFO: Loss too large (1098.425->1098.538)! Learning rate decreased to 0.00363.
2024-12-01-17:40:40-root-INFO: grad norm: 100.802 99.516 16.054
2024-12-01-17:40:40-root-INFO: Loss too large (1095.820->1107.572)! Learning rate decreased to 0.00290.
2024-12-01-17:40:40-root-INFO: Loss too large (1095.820->1099.353)! Learning rate decreased to 0.00232.
2024-12-01-17:40:41-root-INFO: grad norm: 98.969 97.753 15.466
2024-12-01-17:40:41-root-INFO: grad norm: 96.034 94.779 15.473
2024-12-01-17:40:42-root-INFO: grad norm: 94.739 93.537 15.046
2024-12-01-17:40:42-root-INFO: grad norm: 92.968 91.734 15.094
2024-12-01-17:40:43-root-INFO: grad norm: 92.282 91.093 14.764
2024-12-01-17:40:43-root-INFO: grad norm: 91.551 90.331 14.896
2024-12-01-17:40:44-root-INFO: Loss Change: 1098.425 -> 1074.857
2024-12-01-17:40:44-root-INFO: Regularization Change: 0.000 -> 0.546
2024-12-01-17:40:44-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-01-17:40:44-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-17:40:44-root-INFO: step: 158 lr_xt 0.01148915
2024-12-01-17:40:44-root-INFO: grad norm: 123.761 122.171 19.779
2024-12-01-17:40:44-root-INFO: Loss too large (1081.309->1313.316)! Learning rate decreased to 0.00919.
2024-12-01-17:40:44-root-INFO: Loss too large (1081.309->1272.953)! Learning rate decreased to 0.00735.
2024-12-01-17:40:44-root-INFO: Loss too large (1081.309->1224.314)! Learning rate decreased to 0.00588.
2024-12-01-17:40:44-root-INFO: Loss too large (1081.309->1172.664)! Learning rate decreased to 0.00471.
2024-12-01-17:40:45-root-INFO: Loss too large (1081.309->1127.476)! Learning rate decreased to 0.00376.
2024-12-01-17:40:45-root-INFO: Loss too large (1081.309->1096.141)! Learning rate decreased to 0.00301.
2024-12-01-17:40:45-root-INFO: grad norm: 177.447 175.626 25.353
2024-12-01-17:40:45-root-INFO: Loss too large (1078.513->1096.525)! Learning rate decreased to 0.00241.
2024-12-01-17:40:46-root-INFO: Loss too large (1078.513->1081.074)! Learning rate decreased to 0.00193.
2024-12-01-17:40:46-root-INFO: grad norm: 112.851 111.578 16.900
2024-12-01-17:40:47-root-INFO: grad norm: 51.345 50.216 10.706
2024-12-01-17:40:47-root-INFO: grad norm: 42.978 41.710 10.360
2024-12-01-17:40:48-root-INFO: grad norm: 38.528 37.292 9.679
2024-12-01-17:40:48-root-INFO: grad norm: 36.822 35.527 9.678
2024-12-01-17:40:49-root-INFO: grad norm: 35.926 34.661 9.451
2024-12-01-17:40:49-root-INFO: Loss Change: 1081.309 -> 1046.598
2024-12-01-17:40:49-root-INFO: Regularization Change: 0.000 -> 0.411
2024-12-01-17:40:49-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-01-17:40:49-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-17:40:49-root-INFO: step: 157 lr_xt 0.01192805
2024-12-01-17:40:49-root-INFO: grad norm: 71.879 70.186 15.508
2024-12-01-17:40:50-root-INFO: Loss too large (1051.183->1182.079)! Learning rate decreased to 0.00954.
2024-12-01-17:40:50-root-INFO: Loss too large (1051.183->1135.644)! Learning rate decreased to 0.00763.
2024-12-01-17:40:50-root-INFO: Loss too large (1051.183->1098.693)! Learning rate decreased to 0.00611.
2024-12-01-17:40:50-root-INFO: Loss too large (1051.183->1073.875)! Learning rate decreased to 0.00489.
2024-12-01-17:40:50-root-INFO: Loss too large (1051.183->1059.214)! Learning rate decreased to 0.00391.
2024-12-01-17:40:50-root-INFO: Loss too large (1051.183->1051.353)! Learning rate decreased to 0.00313.
2024-12-01-17:40:51-root-INFO: grad norm: 100.889 99.660 15.704
2024-12-01-17:40:51-root-INFO: Loss too large (1047.542->1051.199)! Learning rate decreased to 0.00250.
2024-12-01-17:40:51-root-INFO: grad norm: 96.938 95.611 15.980
2024-12-01-17:40:52-root-INFO: grad norm: 89.696 88.580 14.109
2024-12-01-17:40:52-root-INFO: grad norm: 86.703 85.457 14.645
2024-12-01-17:40:53-root-INFO: grad norm: 81.944 80.880 13.160
2024-12-01-17:40:53-root-INFO: grad norm: 79.918 78.718 13.800
2024-12-01-17:40:54-root-INFO: grad norm: 77.020 75.983 12.594
2024-12-01-17:40:54-root-INFO: Loss Change: 1051.183 -> 1024.023
2024-12-01-17:40:54-root-INFO: Regularization Change: 0.000 -> 0.575
2024-12-01-17:40:54-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-01-17:40:54-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-17:40:54-root-INFO: step: 156 lr_xt 0.01238172
2024-12-01-17:40:54-root-INFO: grad norm: 99.864 98.531 16.259
2024-12-01-17:40:55-root-INFO: Loss too large (1026.595->1245.737)! Learning rate decreased to 0.00991.
2024-12-01-17:40:55-root-INFO: Loss too large (1026.595->1199.465)! Learning rate decreased to 0.00792.
2024-12-01-17:40:55-root-INFO: Loss too large (1026.595->1147.184)! Learning rate decreased to 0.00634.
2024-12-01-17:40:55-root-INFO: Loss too large (1026.595->1097.078)! Learning rate decreased to 0.00507.
2024-12-01-17:40:55-root-INFO: Loss too large (1026.595->1058.865)! Learning rate decreased to 0.00406.
2024-12-01-17:40:55-root-INFO: Loss too large (1026.595->1035.509)! Learning rate decreased to 0.00325.
2024-12-01-17:40:56-root-INFO: grad norm: 139.407 137.964 20.006
2024-12-01-17:40:56-root-INFO: Loss too large (1023.448->1034.142)! Learning rate decreased to 0.00260.
2024-12-01-17:40:56-root-INFO: Loss too large (1023.448->1024.081)! Learning rate decreased to 0.00208.
2024-12-01-17:40:57-root-INFO: grad norm: 88.841 87.703 14.170
2024-12-01-17:40:59-root-INFO: grad norm: 43.238 42.080 9.940
2024-12-01-17:41:00-root-INFO: grad norm: 37.514 36.230 9.732
2024-12-01-17:41:01-root-INFO: grad norm: 34.925 33.646 9.367
2024-12-01-17:41:02-root-INFO: grad norm: 34.032 32.722 9.352
2024-12-01-17:41:03-root-INFO: grad norm: 33.591 32.294 9.242
2024-12-01-17:41:03-root-INFO: Loss Change: 1026.595 -> 997.547
2024-12-01-17:41:03-root-INFO: Regularization Change: 0.000 -> 0.398
2024-12-01-17:41:03-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-01-17:41:03-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-17:41:04-root-INFO: step: 155 lr_xt 0.01285057
2024-12-01-17:41:04-root-INFO: grad norm: 57.424 56.008 12.671
2024-12-01-17:41:04-root-INFO: Loss too large (999.457->1078.459)! Learning rate decreased to 0.01028.
2024-12-01-17:41:05-root-INFO: Loss too large (999.457->1042.795)! Learning rate decreased to 0.00822.
2024-12-01-17:41:05-root-INFO: Loss too large (999.457->1019.984)! Learning rate decreased to 0.00658.
2024-12-01-17:41:06-root-INFO: Loss too large (999.457->1006.881)! Learning rate decreased to 0.00526.
2024-12-01-17:41:06-root-INFO: Loss too large (999.457->999.929)! Learning rate decreased to 0.00421.
2024-12-01-17:41:07-root-INFO: grad norm: 99.539 98.347 15.358
2024-12-01-17:41:07-root-INFO: Loss too large (996.552->1008.826)! Learning rate decreased to 0.00337.
2024-12-01-17:41:08-root-INFO: Loss too large (996.552->1000.481)! Learning rate decreased to 0.00269.
2024-12-01-17:41:09-root-INFO: grad norm: 90.448 89.328 14.189
2024-12-01-17:41:10-root-INFO: grad norm: 74.782 73.727 12.518
2024-12-01-17:41:11-root-INFO: grad norm: 70.255 69.171 12.296
2024-12-01-17:41:12-root-INFO: grad norm: 63.623 62.601 11.355
2024-12-01-17:41:13-root-INFO: grad norm: 60.860 59.782 11.406
2024-12-01-17:41:14-root-INFO: grad norm: 57.200 56.186 10.723
2024-12-01-17:41:14-root-INFO: Loss Change: 999.457 -> 972.334
2024-12-01-17:41:14-root-INFO: Regularization Change: 0.000 -> 0.623
2024-12-01-17:41:14-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-01-17:41:14-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-17:41:15-root-INFO: step: 154 lr_xt 0.01333503
2024-12-01-17:41:15-root-INFO: grad norm: 83.493 82.210 14.578
2024-12-01-17:41:16-root-INFO: Loss too large (975.517->1173.469)! Learning rate decreased to 0.01067.
2024-12-01-17:41:16-root-INFO: Loss too large (975.517->1121.396)! Learning rate decreased to 0.00853.
2024-12-01-17:41:16-root-INFO: Loss too large (975.517->1067.973)! Learning rate decreased to 0.00683.
2024-12-01-17:41:17-root-INFO: Loss too large (975.517->1023.726)! Learning rate decreased to 0.00546.
2024-12-01-17:41:17-root-INFO: Loss too large (975.517->994.779)! Learning rate decreased to 0.00437.
2024-12-01-17:41:17-root-INFO: Loss too large (975.517->978.909)! Learning rate decreased to 0.00350.
2024-12-01-17:41:18-root-INFO: grad norm: 108.844 107.629 16.219
2024-12-01-17:41:19-root-INFO: Loss too large (971.260->975.976)! Learning rate decreased to 0.00280.
2024-12-01-17:41:20-root-INFO: grad norm: 90.611 89.521 14.014
2024-12-01-17:41:21-root-INFO: grad norm: 61.521 60.495 11.189
2024-12-01-17:41:22-root-INFO: grad norm: 55.444 54.354 10.943
2024-12-01-17:41:23-root-INFO: grad norm: 48.250 47.201 10.004
2024-12-01-17:41:24-root-INFO: grad norm: 45.042 43.909 10.038
2024-12-01-17:41:25-root-INFO: grad norm: 41.654 40.564 9.465
2024-12-01-17:41:26-root-INFO: Loss Change: 975.517 -> 944.832
2024-12-01-17:41:26-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-01-17:41:26-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-01-17:41:26-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-17:41:26-root-INFO: step: 153 lr_xt 0.01383551
2024-12-01-17:41:26-root-INFO: grad norm: 68.347 67.059 13.205
2024-12-01-17:41:27-root-INFO: Loss too large (946.734->1092.561)! Learning rate decreased to 0.01107.
2024-12-01-17:41:27-root-INFO: Loss too large (946.734->1038.434)! Learning rate decreased to 0.00885.
2024-12-01-17:41:27-root-INFO: Loss too large (946.734->995.349)! Learning rate decreased to 0.00708.
2024-12-01-17:41:28-root-INFO: Loss too large (946.734->967.688)! Learning rate decreased to 0.00567.
2024-12-01-17:41:28-root-INFO: Loss too large (946.734->952.415)! Learning rate decreased to 0.00453.
2024-12-01-17:41:29-root-INFO: grad norm: 118.455 117.177 17.352
2024-12-01-17:41:29-root-INFO: Loss too large (944.814->960.466)! Learning rate decreased to 0.00363.
2024-12-01-17:41:30-root-INFO: Loss too large (944.814->949.537)! Learning rate decreased to 0.00290.
2024-12-01-17:41:31-root-INFO: grad norm: 89.066 88.027 13.566
2024-12-01-17:41:32-root-INFO: grad norm: 48.197 47.177 9.864
2024-12-01-17:41:33-root-INFO: grad norm: 42.374 41.248 9.701
2024-12-01-17:41:34-root-INFO: grad norm: 37.385 36.283 9.010
2024-12-01-17:41:35-root-INFO: grad norm: 35.160 33.971 9.065
2024-12-01-17:41:36-root-INFO: grad norm: 33.475 32.319 8.721
2024-12-01-17:41:37-root-INFO: Loss Change: 946.734 -> 916.370
2024-12-01-17:41:37-root-INFO: Regularization Change: 0.000 -> 0.673
2024-12-01-17:41:37-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-01-17:41:37-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-17:41:37-root-INFO: step: 152 lr_xt 0.01435246
2024-12-01-17:41:37-root-INFO: grad norm: 48.794 47.665 10.434
2024-12-01-17:41:38-root-INFO: Loss too large (916.182->975.111)! Learning rate decreased to 0.01148.
2024-12-01-17:41:38-root-INFO: Loss too large (916.182->945.368)! Learning rate decreased to 0.00919.
2024-12-01-17:41:38-root-INFO: Loss too large (916.182->928.142)! Learning rate decreased to 0.00735.
2024-12-01-17:41:39-root-INFO: Loss too large (916.182->919.010)! Learning rate decreased to 0.00588.
2024-12-01-17:41:40-root-INFO: grad norm: 102.239 101.077 15.371
2024-12-01-17:41:40-root-INFO: Loss too large (914.514->937.527)! Learning rate decreased to 0.00470.
2024-12-01-17:41:40-root-INFO: Loss too large (914.514->924.915)! Learning rate decreased to 0.00376.
2024-12-01-17:41:41-root-INFO: Loss too large (914.514->916.691)! Learning rate decreased to 0.00301.
2024-12-01-17:41:42-root-INFO: grad norm: 76.147 75.212 11.897
2024-12-01-17:41:43-root-INFO: grad norm: 43.442 42.420 9.370
2024-12-01-17:41:44-root-INFO: grad norm: 38.216 37.125 9.066
2024-12-01-17:41:45-root-INFO: grad norm: 34.185 33.071 8.656
2024-12-01-17:41:46-root-INFO: grad norm: 32.463 31.301 8.609
2024-12-01-17:41:47-root-INFO: grad norm: 31.283 30.129 8.418
2024-12-01-17:41:47-root-INFO: Loss Change: 916.182 -> 888.585
2024-12-01-17:41:47-root-INFO: Regularization Change: 0.000 -> 0.711
2024-12-01-17:41:47-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-01-17:41:47-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-17:41:48-root-INFO: step: 151 lr_xt 0.01488633
2024-12-01-17:41:48-root-INFO: grad norm: 52.925 51.723 11.218
2024-12-01-17:41:48-root-INFO: Loss too large (889.389->964.214)! Learning rate decreased to 0.01191.
2024-12-01-17:41:49-root-INFO: Loss too large (889.389->926.775)! Learning rate decreased to 0.00953.
2024-12-01-17:41:49-root-INFO: Loss too large (889.389->904.821)! Learning rate decreased to 0.00762.
2024-12-01-17:41:49-root-INFO: Loss too large (889.389->893.218)! Learning rate decreased to 0.00610.
2024-12-01-17:41:51-root-INFO: grad norm: 105.178 104.091 15.083
2024-12-01-17:41:51-root-INFO: Loss too large (887.534->910.447)! Learning rate decreased to 0.00488.
2024-12-01-17:41:51-root-INFO: Loss too large (887.534->897.544)! Learning rate decreased to 0.00390.
2024-12-01-17:41:52-root-INFO: Loss too large (887.534->889.142)! Learning rate decreased to 0.00312.
2024-12-01-17:41:53-root-INFO: grad norm: 72.679 71.727 11.726
2024-12-01-17:41:54-root-INFO: grad norm: 36.470 35.443 8.595
2024-12-01-17:41:55-root-INFO: grad norm: 32.627 31.477 8.588
2024-12-01-17:41:56-root-INFO: grad norm: 30.377 29.250 8.196
2024-12-01-17:41:57-root-INFO: grad norm: 29.422 28.250 8.221
2024-12-01-17:41:58-root-INFO: grad norm: 28.826 27.678 8.053
2024-12-01-17:41:58-root-INFO: Loss Change: 889.389 -> 861.743
2024-12-01-17:41:58-root-INFO: Regularization Change: 0.000 -> 0.721
2024-12-01-17:41:58-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-01-17:41:58-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-17:41:59-root-INFO: step: 150 lr_xt 0.01543756
2024-12-01-17:41:59-root-INFO: grad norm: 38.180 37.021 9.334
2024-12-01-17:41:59-root-INFO: Loss too large (861.490->875.382)! Learning rate decreased to 0.01235.
2024-12-01-17:42:00-root-INFO: Loss too large (861.490->865.416)! Learning rate decreased to 0.00988.
2024-12-01-17:42:01-root-INFO: grad norm: 109.378 108.298 15.336
2024-12-01-17:42:01-root-INFO: Loss too large (860.438->936.579)! Learning rate decreased to 0.00790.
2024-12-01-17:42:01-root-INFO: Loss too large (860.438->905.738)! Learning rate decreased to 0.00632.
2024-12-01-17:42:02-root-INFO: Loss too large (860.438->884.906)! Learning rate decreased to 0.00506.
2024-12-01-17:42:02-root-INFO: Loss too large (860.438->870.990)! Learning rate decreased to 0.00405.
2024-12-01-17:42:02-root-INFO: Loss too large (860.438->861.944)! Learning rate decreased to 0.00324.
2024-12-01-17:42:04-root-INFO: grad norm: 69.797 68.918 11.044
2024-12-01-17:42:05-root-INFO: grad norm: 30.327 29.290 7.863
2024-12-01-17:42:06-root-INFO: grad norm: 27.947 26.816 7.871
2024-12-01-17:42:07-root-INFO: grad norm: 26.826 25.714 7.642
2024-12-01-17:42:08-root-INFO: grad norm: 26.306 25.172 7.641
2024-12-01-17:42:09-root-INFO: grad norm: 25.951 24.832 7.541
2024-12-01-17:42:09-root-INFO: Loss Change: 861.490 -> 836.229
2024-12-01-17:42:09-root-INFO: Regularization Change: 0.000 -> 0.805
2024-12-01-17:42:09-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-01-17:42:09-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-17:42:10-root-INFO: step: 149 lr_xt 0.01600663
2024-12-01-17:42:10-root-INFO: grad norm: 42.902 41.772 9.779
2024-12-01-17:42:10-root-INFO: Loss too large (836.718->873.542)! Learning rate decreased to 0.01281.
2024-12-01-17:42:11-root-INFO: Loss too large (836.718->852.488)! Learning rate decreased to 0.01024.
2024-12-01-17:42:11-root-INFO: Loss too large (836.718->841.437)! Learning rate decreased to 0.00820.
2024-12-01-17:42:12-root-INFO: grad norm: 98.014 97.013 13.968
2024-12-01-17:42:12-root-INFO: Loss too large (835.950->871.080)! Learning rate decreased to 0.00656.
2024-12-01-17:42:13-root-INFO: Loss too large (835.950->854.183)! Learning rate decreased to 0.00525.
2024-12-01-17:42:13-root-INFO: Loss too large (835.950->843.013)! Learning rate decreased to 0.00420.
2024-12-01-17:42:14-root-INFO: grad norm: 75.782 74.921 11.388
2024-12-01-17:42:15-root-INFO: grad norm: 38.747 37.885 8.126
2024-12-01-17:42:16-root-INFO: grad norm: 34.961 33.980 8.223
2024-12-01-17:42:17-root-INFO: grad norm: 30.929 29.998 7.529
2024-12-01-17:42:18-root-INFO: grad norm: 29.015 27.971 7.712
2024-12-01-17:42:19-root-INFO: grad norm: 27.169 26.182 7.257
2024-12-01-17:42:20-root-INFO: Loss Change: 836.718 -> 811.476
2024-12-01-17:42:20-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-01-17:42:20-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-01-17:42:20-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-17:42:20-root-INFO: step: 148 lr_xt 0.01659399
2024-12-01-17:42:21-root-INFO: grad norm: 44.299 43.243 9.618
2024-12-01-17:42:21-root-INFO: Loss too large (813.514->862.052)! Learning rate decreased to 0.01328.
2024-12-01-17:42:21-root-INFO: Loss too large (813.514->834.582)! Learning rate decreased to 0.01062.
2024-12-01-17:42:22-root-INFO: Loss too large (813.514->820.318)! Learning rate decreased to 0.00850.
2024-12-01-17:42:23-root-INFO: grad norm: 96.601 95.667 13.398
2024-12-01-17:42:23-root-INFO: Loss too large (813.350->843.623)! Learning rate decreased to 0.00680.
2024-12-01-17:42:23-root-INFO: Loss too large (813.350->828.024)! Learning rate decreased to 0.00544.
2024-12-01-17:42:24-root-INFO: Loss too large (813.350->817.847)! Learning rate decreased to 0.00435.
2024-12-01-17:42:25-root-INFO: grad norm: 65.678 64.859 10.336
2024-12-01-17:42:26-root-INFO: grad norm: 26.860 25.889 7.158
2024-12-01-17:42:27-root-INFO: grad norm: 24.547 23.449 7.257
2024-12-01-17:42:28-root-INFO: grad norm: 23.215 22.154 6.938
2024-12-01-17:42:29-root-INFO: grad norm: 22.606 21.500 6.985
2024-12-01-17:42:30-root-INFO: grad norm: 22.215 21.139 6.830
2024-12-01-17:42:30-root-INFO: Loss Change: 813.514 -> 790.131
2024-12-01-17:42:30-root-INFO: Regularization Change: 0.000 -> 0.835
2024-12-01-17:42:30-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-01-17:42:30-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-17:42:31-root-INFO: step: 147 lr_xt 0.01720013
2024-12-01-17:42:31-root-INFO: grad norm: 32.623 31.556 8.278
2024-12-01-17:42:32-root-INFO: Loss too large (789.526->793.388)! Learning rate decreased to 0.01376.
2024-12-01-17:42:33-root-INFO: grad norm: 82.062 81.291 11.217
2024-12-01-17:42:33-root-INFO: Loss too large (788.625->850.738)! Learning rate decreased to 0.01101.
2024-12-01-17:42:33-root-INFO: Loss too large (788.625->823.087)! Learning rate decreased to 0.00881.
2024-12-01-17:42:34-root-INFO: Loss too large (788.625->805.375)! Learning rate decreased to 0.00705.
2024-12-01-17:42:34-root-INFO: Loss too large (788.625->794.207)! Learning rate decreased to 0.00564.
2024-12-01-17:42:35-root-INFO: grad norm: 64.244 63.464 9.979
2024-12-01-17:42:36-root-INFO: grad norm: 38.303 37.615 7.224
2024-12-01-17:42:37-root-INFO: grad norm: 34.958 34.124 7.590
2024-12-01-17:42:38-root-INFO: grad norm: 30.704 29.969 6.676
2024-12-01-17:42:39-root-INFO: grad norm: 28.960 28.088 7.056
2024-12-01-17:42:40-root-INFO: grad norm: 26.885 26.108 6.416
2024-12-01-17:42:41-root-INFO: Loss Change: 789.526 -> 764.356
2024-12-01-17:42:41-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-01-17:42:41-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-01-17:42:41-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-17:42:41-root-INFO: step: 146 lr_xt 0.01782554
2024-12-01-17:42:41-root-INFO: grad norm: 40.256 39.437 8.077
2024-12-01-17:42:42-root-INFO: Loss too large (765.484->803.150)! Learning rate decreased to 0.01426.
2024-12-01-17:42:42-root-INFO: Loss too large (765.484->779.959)! Learning rate decreased to 0.01141.
2024-12-01-17:42:42-root-INFO: Loss too large (765.484->768.800)! Learning rate decreased to 0.00913.
2024-12-01-17:42:43-root-INFO: grad norm: 67.746 67.043 9.738
2024-12-01-17:42:44-root-INFO: Loss too large (763.670->773.582)! Learning rate decreased to 0.00730.
2024-12-01-17:42:44-root-INFO: Loss too large (763.670->766.307)! Learning rate decreased to 0.00584.
2024-12-01-17:42:45-root-INFO: grad norm: 50.847 50.138 8.462
2024-12-01-17:42:46-root-INFO: grad norm: 27.523 26.787 6.321
2024-12-01-17:42:47-root-INFO: grad norm: 24.324 23.436 6.514
2024-12-01-17:42:48-root-INFO: grad norm: 21.846 21.000 6.022
2024-12-01-17:42:49-root-INFO: grad norm: 20.813 19.877 6.171
2024-12-01-17:42:50-root-INFO: grad norm: 20.090 19.198 5.921
2024-12-01-17:42:51-root-INFO: Loss Change: 765.484 -> 742.711
2024-12-01-17:42:51-root-INFO: Regularization Change: 0.000 -> 1.031
2024-12-01-17:42:51-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-01-17:42:51-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-17:42:51-root-INFO: step: 145 lr_xt 0.01847071
2024-12-01-17:42:52-root-INFO: grad norm: 34.925 33.979 8.074
2024-12-01-17:42:52-root-INFO: Loss too large (744.450->753.463)! Learning rate decreased to 0.01478.
2024-12-01-17:42:52-root-INFO: Loss too large (744.450->745.960)! Learning rate decreased to 0.01182.
2024-12-01-17:42:54-root-INFO: grad norm: 57.517 56.904 8.376
2024-12-01-17:42:54-root-INFO: Loss too large (742.234->753.208)! Learning rate decreased to 0.00946.
2024-12-01-17:42:54-root-INFO: Loss too large (742.234->744.944)! Learning rate decreased to 0.00757.
2024-12-01-17:42:55-root-INFO: grad norm: 50.397 49.642 8.693
2024-12-01-17:42:56-root-INFO: grad norm: 42.369 41.779 7.045
2024-12-01-17:42:57-root-INFO: grad norm: 39.624 38.866 7.713
2024-12-01-17:42:58-root-INFO: grad norm: 35.585 35.000 6.423
2024-12-01-17:42:59-root-INFO: grad norm: 34.002 33.231 7.200
2024-12-01-17:43:00-root-INFO: grad norm: 31.710 31.122 6.075
2024-12-01-17:43:01-root-INFO: Loss Change: 744.450 -> 719.554
2024-12-01-17:43:01-root-INFO: Regularization Change: 0.000 -> 1.590
2024-12-01-17:43:01-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-01-17:43:01-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-17:43:01-root-INFO: step: 144 lr_xt 0.01913614
2024-12-01-17:43:02-root-INFO: grad norm: 41.288 40.515 7.956
2024-12-01-17:43:02-root-INFO: Loss too large (720.728->751.303)! Learning rate decreased to 0.01531.
2024-12-01-17:43:03-root-INFO: Loss too large (720.728->730.860)! Learning rate decreased to 0.01225.
2024-12-01-17:43:03-root-INFO: Loss too large (720.728->721.396)! Learning rate decreased to 0.00980.
2024-12-01-17:43:04-root-INFO: grad norm: 49.725 49.125 7.700
2024-12-01-17:43:04-root-INFO: Loss too large (717.166->717.728)! Learning rate decreased to 0.00784.
2024-12-01-17:43:05-root-INFO: grad norm: 41.558 40.810 7.847
2024-12-01-17:43:06-root-INFO: grad norm: 31.272 30.678 6.066
2024-12-01-17:43:07-root-INFO: grad norm: 27.982 27.177 6.662
2024-12-01-17:43:08-root-INFO: grad norm: 24.905 24.271 5.586
2024-12-01-17:43:09-root-INFO: grad norm: 23.310 22.476 6.178
2024-12-01-17:43:10-root-INFO: grad norm: 21.962 21.293 5.377
2024-12-01-17:43:11-root-INFO: Loss Change: 720.728 -> 695.132
2024-12-01-17:43:11-root-INFO: Regularization Change: 0.000 -> 1.467
2024-12-01-17:43:11-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-01-17:43:11-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-17:43:11-root-INFO: step: 143 lr_xt 0.01982236
2024-12-01-17:43:12-root-INFO: grad norm: 31.341 30.600 6.775
2024-12-01-17:43:12-root-INFO: Loss too large (694.811->701.414)! Learning rate decreased to 0.01586.
2024-12-01-17:43:12-root-INFO: Loss too large (694.811->695.617)! Learning rate decreased to 0.01269.
2024-12-01-17:43:14-root-INFO: grad norm: 46.120 45.560 7.165
2024-12-01-17:43:14-root-INFO: Loss too large (692.733->696.856)! Learning rate decreased to 0.01015.
2024-12-01-17:43:15-root-INFO: grad norm: 51.862 51.051 9.138
2024-12-01-17:43:16-root-INFO: grad norm: 53.497 52.881 8.092
2024-12-01-17:43:17-root-INFO: grad norm: 51.810 51.013 9.051
2024-12-01-17:43:18-root-INFO: grad norm: 52.177 51.572 7.922
2024-12-01-17:43:19-root-INFO: grad norm: 51.952 51.159 9.041
2024-12-01-17:43:20-root-INFO: grad norm: 50.422 49.823 7.749
2024-12-01-17:43:21-root-INFO: Loss Change: 694.811 -> 672.897
2024-12-01-17:43:21-root-INFO: Regularization Change: 0.000 -> 2.257
2024-12-01-17:43:21-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-01-17:43:21-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-17:43:21-root-INFO: step: 142 lr_xt 0.02052986
2024-12-01-17:43:21-root-INFO: grad norm: 63.260 62.441 10.149
2024-12-01-17:43:22-root-INFO: Loss too large (677.189->742.983)! Learning rate decreased to 0.01642.
2024-12-01-17:43:22-root-INFO: Loss too large (677.189->699.699)! Learning rate decreased to 0.01314.
2024-12-01-17:43:22-root-INFO: Loss too large (677.189->679.776)! Learning rate decreased to 0.01051.
2024-12-01-17:43:23-root-INFO: grad norm: 53.873 53.283 7.951
2024-12-01-17:43:24-root-INFO: grad norm: 47.447 46.712 8.317
2024-12-01-17:43:26-root-INFO: grad norm: 43.886 43.339 6.912
2024-12-01-17:43:27-root-INFO: grad norm: 42.479 41.754 7.818
2024-12-01-17:43:28-root-INFO: grad norm: 40.167 39.614 6.647
2024-12-01-17:43:29-root-INFO: grad norm: 38.639 37.912 7.458
2024-12-01-17:43:30-root-INFO: grad norm: 37.261 36.704 6.418
2024-12-01-17:43:30-root-INFO: Loss Change: 677.189 -> 643.854
2024-12-01-17:43:30-root-INFO: Regularization Change: 0.000 -> 2.148
2024-12-01-17:43:30-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-01-17:43:30-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-17:43:31-root-INFO: step: 141 lr_xt 0.02125920
2024-12-01-17:43:31-root-INFO: grad norm: 49.698 48.984 8.396
2024-12-01-17:43:31-root-INFO: Loss too large (645.321->673.177)! Learning rate decreased to 0.01701.
2024-12-01-17:43:32-root-INFO: Loss too large (645.321->657.289)! Learning rate decreased to 0.01361.
2024-12-01-17:43:32-root-INFO: Loss too large (645.321->648.014)! Learning rate decreased to 0.01088.
2024-12-01-17:43:33-root-INFO: grad norm: 43.459 42.890 7.008
2024-12-01-17:43:34-root-INFO: grad norm: 35.496 34.856 6.712
2024-12-01-17:43:35-root-INFO: grad norm: 33.667 33.135 5.959
2024-12-01-17:43:36-root-INFO: grad norm: 33.591 32.933 6.613
2024-12-01-17:43:37-root-INFO: grad norm: 32.628 32.082 5.943
2024-12-01-17:43:38-root-INFO: grad norm: 32.557 31.876 6.625
2024-12-01-17:43:39-root-INFO: grad norm: 31.922 31.367 5.928
2024-12-01-17:43:40-root-INFO: Loss Change: 645.321 -> 619.523
2024-12-01-17:43:40-root-INFO: Regularization Change: 0.000 -> 1.937
2024-12-01-17:43:40-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-01-17:43:40-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-17:43:40-root-INFO: step: 140 lr_xt 0.02201089
2024-12-01-17:43:41-root-INFO: grad norm: 47.043 46.358 7.996
2024-12-01-17:43:41-root-INFO: Loss too large (621.234->646.882)! Learning rate decreased to 0.01761.
2024-12-01-17:43:41-root-INFO: Loss too large (621.234->633.292)! Learning rate decreased to 0.01409.
2024-12-01-17:43:42-root-INFO: Loss too large (621.234->624.805)! Learning rate decreased to 0.01127.
2024-12-01-17:43:43-root-INFO: grad norm: 43.185 42.608 7.035
2024-12-01-17:43:44-root-INFO: grad norm: 37.758 37.136 6.824
2024-12-01-17:43:45-root-INFO: grad norm: 37.029 36.489 6.303
2024-12-01-17:43:46-root-INFO: grad norm: 38.209 37.547 7.081
2024-12-01-17:43:47-root-INFO: grad norm: 37.141 36.587 6.392
2024-12-01-17:43:48-root-INFO: grad norm: 35.783 35.130 6.805
2024-12-01-17:43:49-root-INFO: grad norm: 35.513 34.962 6.231
2024-12-01-17:43:50-root-INFO: Loss Change: 621.234 -> 598.650
2024-12-01-17:43:50-root-INFO: Regularization Change: 0.000 -> 1.832
2024-12-01-17:43:50-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-01-17:43:50-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-17:43:50-root-INFO: step: 139 lr_xt 0.02278550
2024-12-01-17:43:50-root-INFO: grad norm: 49.119 48.458 8.028
2024-12-01-17:43:51-root-INFO: Loss too large (601.169->632.732)! Learning rate decreased to 0.01823.
2024-12-01-17:43:51-root-INFO: Loss too large (601.169->616.640)! Learning rate decreased to 0.01458.
2024-12-01-17:43:51-root-INFO: Loss too large (601.169->606.349)! Learning rate decreased to 0.01167.
2024-12-01-17:43:52-root-INFO: grad norm: 46.048 45.489 7.148
2024-12-01-17:43:54-root-INFO: grad norm: 41.071 40.499 6.831
2024-12-01-17:43:55-root-INFO: grad norm: 40.251 39.733 6.434
2024-12-01-17:43:56-root-INFO: grad norm: 41.410 40.788 7.148
2024-12-01-17:43:57-root-INFO: grad norm: 40.120 39.583 6.547
2024-12-01-17:43:58-root-INFO: grad norm: 38.509 37.903 6.805
2024-12-01-17:43:59-root-INFO: grad norm: 38.171 37.638 6.360
2024-12-01-17:43:59-root-INFO: Loss Change: 601.169 -> 579.725
2024-12-01-17:43:59-root-INFO: Regularization Change: 0.000 -> 1.777
2024-12-01-17:43:59-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-01-17:43:59-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-17:44:00-root-INFO: step: 138 lr_xt 0.02358356
2024-12-01-17:44:00-root-INFO: grad norm: 50.147 49.532 7.832
2024-12-01-17:44:00-root-INFO: Loss too large (581.509->615.966)! Learning rate decreased to 0.01887.
2024-12-01-17:44:01-root-INFO: Loss too large (581.509->599.057)! Learning rate decreased to 0.01509.
2024-12-01-17:44:01-root-INFO: Loss too large (581.509->588.035)! Learning rate decreased to 0.01207.
2024-12-01-17:44:02-root-INFO: grad norm: 46.511 45.953 7.182
2024-12-01-17:44:03-root-INFO: grad norm: 40.309 39.772 6.559
2024-12-01-17:44:04-root-INFO: grad norm: 39.586 39.082 6.303
2024-12-01-17:44:05-root-INFO: grad norm: 41.653 41.031 7.173
2024-12-01-17:44:06-root-INFO: grad norm: 40.140 39.603 6.543
2024-12-01-17:44:07-root-INFO: grad norm: 37.724 37.137 6.632
2024-12-01-17:44:08-root-INFO: grad norm: 37.596 37.070 6.266
2024-12-01-17:44:09-root-INFO: Loss Change: 581.509 -> 560.916
2024-12-01-17:44:09-root-INFO: Regularization Change: 0.000 -> 1.733
2024-12-01-17:44:09-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-01-17:44:09-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-17:44:09-root-INFO: step: 137 lr_xt 0.02440563
2024-12-01-17:44:10-root-INFO: grad norm: 58.538 57.816 9.166
2024-12-01-17:44:10-root-INFO: Loss too large (565.109->608.438)! Learning rate decreased to 0.01952.
2024-12-01-17:44:10-root-INFO: Loss too large (565.109->587.742)! Learning rate decreased to 0.01562.
2024-12-01-17:44:11-root-INFO: Loss too large (565.109->573.877)! Learning rate decreased to 0.01250.
2024-12-01-17:44:11-root-INFO: Loss too large (565.109->565.115)! Learning rate decreased to 0.01000.
2024-12-01-17:44:12-root-INFO: grad norm: 38.224 37.620 6.772
2024-12-01-17:44:13-root-INFO: grad norm: 20.623 20.083 4.690
2024-12-01-17:44:14-root-INFO: grad norm: 17.365 16.824 4.301
2024-12-01-17:44:15-root-INFO: grad norm: 15.971 15.367 4.349
2024-12-01-17:44:16-root-INFO: grad norm: 15.383 14.824 4.107
2024-12-01-17:44:17-root-INFO: grad norm: 15.119 14.510 4.245
2024-12-01-17:44:18-root-INFO: grad norm: 15.032 14.485 4.016
2024-12-01-17:44:19-root-INFO: Loss Change: 565.109 -> 541.850
2024-12-01-17:44:19-root-INFO: Regularization Change: 0.000 -> 1.266
2024-12-01-17:44:19-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-01-17:44:19-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-17:44:19-root-INFO: step: 136 lr_xt 0.02525230
2024-12-01-17:44:19-root-INFO: grad norm: 21.986 21.474 4.719
2024-12-01-17:44:20-root-INFO: Loss too large (541.931->544.914)! Learning rate decreased to 0.02020.
2024-12-01-17:44:20-root-INFO: Loss too large (541.931->542.460)! Learning rate decreased to 0.01616.
2024-12-01-17:44:21-root-INFO: grad norm: 30.849 30.329 5.643
2024-12-01-17:44:22-root-INFO: Loss too large (541.018->543.637)! Learning rate decreased to 0.01293.
2024-12-01-17:44:23-root-INFO: grad norm: 42.977 42.277 7.729
2024-12-01-17:44:23-root-INFO: Loss too large (539.451->542.282)! Learning rate decreased to 0.01034.
2024-12-01-17:44:24-root-INFO: grad norm: 34.073 33.521 6.109
2024-12-01-17:44:25-root-INFO: grad norm: 20.583 20.025 4.759
2024-12-01-17:44:26-root-INFO: grad norm: 19.523 19.045 4.294
2024-12-01-17:44:27-root-INFO: grad norm: 19.237 18.673 4.626
2024-12-01-17:44:28-root-INFO: grad norm: 19.263 18.786 4.261
2024-12-01-17:44:29-root-INFO: Loss Change: 541.931 -> 526.696
2024-12-01-17:44:29-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-01-17:44:29-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-01-17:44:29-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-17:44:29-root-INFO: step: 135 lr_xt 0.02612413
2024-12-01-17:44:29-root-INFO: grad norm: 27.434 26.948 5.138
2024-12-01-17:44:30-root-INFO: Loss too large (527.066->536.774)! Learning rate decreased to 0.02090.
2024-12-01-17:44:30-root-INFO: Loss too large (527.066->531.360)! Learning rate decreased to 0.01672.
2024-12-01-17:44:30-root-INFO: Loss too large (527.066->527.978)! Learning rate decreased to 0.01338.
2024-12-01-17:44:31-root-INFO: grad norm: 31.402 30.885 5.677
2024-12-01-17:44:33-root-INFO: grad norm: 45.084 44.400 7.822
2024-12-01-17:44:33-root-INFO: Loss too large (524.912->528.424)! Learning rate decreased to 0.01070.
2024-12-01-17:44:34-root-INFO: grad norm: 35.227 34.668 6.249
2024-12-01-17:44:35-root-INFO: grad norm: 20.439 19.921 4.572
2024-12-01-17:44:36-root-INFO: grad norm: 19.069 18.606 4.174
2024-12-01-17:44:37-root-INFO: grad norm: 18.753 18.216 4.458
2024-12-01-17:44:38-root-INFO: grad norm: 18.789 18.324 4.157
2024-12-01-17:44:39-root-INFO: Loss Change: 527.066 -> 511.944
2024-12-01-17:44:39-root-INFO: Regularization Change: 0.000 -> 1.281
2024-12-01-17:44:39-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-01-17:44:39-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-17:44:39-root-INFO: step: 134 lr_xt 0.02702170
2024-12-01-17:44:39-root-INFO: grad norm: 29.953 29.447 5.483
2024-12-01-17:44:40-root-INFO: Loss too large (512.274->525.464)! Learning rate decreased to 0.02162.
2024-12-01-17:44:40-root-INFO: Loss too large (512.274->518.799)! Learning rate decreased to 0.01729.
2024-12-01-17:44:40-root-INFO: Loss too large (512.274->514.475)! Learning rate decreased to 0.01384.
2024-12-01-17:44:41-root-INFO: grad norm: 33.463 32.950 5.837
2024-12-01-17:44:42-root-INFO: grad norm: 45.302 44.621 7.823
2024-12-01-17:44:43-root-INFO: Loss too large (510.183->513.839)! Learning rate decreased to 0.01107.
2024-12-01-17:44:44-root-INFO: grad norm: 35.023 34.476 6.163
2024-12-01-17:44:45-root-INFO: grad norm: 20.180 19.681 4.460
2024-12-01-17:44:46-root-INFO: grad norm: 18.648 18.201 4.059
2024-12-01-17:44:47-root-INFO: grad norm: 18.370 17.853 4.324
2024-12-01-17:44:48-root-INFO: grad norm: 18.473 18.018 4.072
2024-12-01-17:44:48-root-INFO: Loss Change: 512.274 -> 497.309
2024-12-01-17:44:48-root-INFO: Regularization Change: 0.000 -> 1.274
2024-12-01-17:44:48-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-01-17:44:48-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-17:44:49-root-INFO: step: 133 lr_xt 0.02794561
2024-12-01-17:44:49-root-INFO: grad norm: 28.708 28.241 5.156
2024-12-01-17:44:49-root-INFO: Loss too large (498.227->511.682)! Learning rate decreased to 0.02236.
2024-12-01-17:44:50-root-INFO: Loss too large (498.227->504.802)! Learning rate decreased to 0.01789.
2024-12-01-17:44:50-root-INFO: Loss too large (498.227->500.408)! Learning rate decreased to 0.01431.
2024-12-01-17:44:51-root-INFO: grad norm: 33.282 32.775 5.787
2024-12-01-17:44:52-root-INFO: grad norm: 47.786 47.110 8.008
2024-12-01-17:44:53-root-INFO: Loss too large (496.957->501.536)! Learning rate decreased to 0.01145.
2024-12-01-17:44:53-root-INFO: Loss too large (496.957->497.003)! Learning rate decreased to 0.00916.
2024-12-01-17:44:54-root-INFO: grad norm: 29.232 28.717 5.465
2024-12-01-17:44:55-root-INFO: grad norm: 13.635 13.130 3.675
2024-12-01-17:44:56-root-INFO: grad norm: 12.295 11.808 3.426
2024-12-01-17:44:57-root-INFO: grad norm: 11.736 11.222 3.433
2024-12-01-17:44:58-root-INFO: grad norm: 11.480 10.984 3.339
2024-12-01-17:44:59-root-INFO: Loss Change: 498.227 -> 484.958
2024-12-01-17:44:59-root-INFO: Regularization Change: 0.000 -> 0.964
2024-12-01-17:44:59-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-01-17:44:59-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-17:44:59-root-INFO: step: 132 lr_xt 0.02889645
2024-12-01-17:44:59-root-INFO: grad norm: 17.135 16.691 3.875
2024-12-01-17:45:01-root-INFO: grad norm: 36.372 35.923 5.695
2024-12-01-17:45:01-root-INFO: Loss too large (484.543->524.815)! Learning rate decreased to 0.02312.
2024-12-01-17:45:01-root-INFO: Loss too large (484.543->503.832)! Learning rate decreased to 0.01849.
2024-12-01-17:45:02-root-INFO: Loss too large (484.543->491.454)! Learning rate decreased to 0.01479.
2024-12-01-17:45:02-root-INFO: Loss too large (484.543->484.634)! Learning rate decreased to 0.01184.
2024-12-01-17:45:03-root-INFO: grad norm: 32.492 31.986 5.709
2024-12-01-17:45:04-root-INFO: grad norm: 29.373 28.884 5.333
2024-12-01-17:45:05-root-INFO: grad norm: 26.913 26.404 5.213
2024-12-01-17:45:06-root-INFO: grad norm: 26.238 25.757 5.000
2024-12-01-17:45:07-root-INFO: grad norm: 25.625 25.116 5.083
2024-12-01-17:45:08-root-INFO: grad norm: 25.310 24.833 4.890
2024-12-01-17:45:09-root-INFO: Loss Change: 484.768 -> 471.520
2024-12-01-17:45:09-root-INFO: Regularization Change: 0.000 -> 1.554
2024-12-01-17:45:09-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-01-17:45:09-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-17:45:09-root-INFO: step: 131 lr_xt 0.02987484
2024-12-01-17:45:09-root-INFO: grad norm: 35.954 35.414 6.207
2024-12-01-17:45:10-root-INFO: Loss too large (472.979->494.559)! Learning rate decreased to 0.02390.
2024-12-01-17:45:10-root-INFO: Loss too large (472.979->485.771)! Learning rate decreased to 0.01912.
2024-12-01-17:45:10-root-INFO: Loss too large (472.979->479.437)! Learning rate decreased to 0.01530.
2024-12-01-17:45:11-root-INFO: Loss too large (472.979->475.043)! Learning rate decreased to 0.01224.
2024-12-01-17:45:12-root-INFO: grad norm: 31.187 30.653 5.747
2024-12-01-17:45:13-root-INFO: grad norm: 24.895 24.415 4.865
2024-12-01-17:45:14-root-INFO: grad norm: 24.227 23.761 4.729
2024-12-01-17:45:15-root-INFO: grad norm: 24.235 23.755 4.802
2024-12-01-17:45:16-root-INFO: grad norm: 23.939 23.474 4.697
2024-12-01-17:45:17-root-INFO: grad norm: 23.616 23.141 4.711
2024-12-01-17:45:18-root-INFO: grad norm: 23.485 23.025 4.622
2024-12-01-17:45:19-root-INFO: Loss Change: 472.979 -> 459.519
2024-12-01-17:45:19-root-INFO: Regularization Change: 0.000 -> 1.133
2024-12-01-17:45:19-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-01-17:45:19-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-17:45:19-root-INFO: step: 130 lr_xt 0.03088137
2024-12-01-17:45:19-root-INFO: grad norm: 31.876 31.389 5.548
2024-12-01-17:45:20-root-INFO: Loss too large (460.295->478.115)! Learning rate decreased to 0.02471.
2024-12-01-17:45:20-root-INFO: Loss too large (460.295->471.046)! Learning rate decreased to 0.01976.
2024-12-01-17:45:20-root-INFO: Loss too large (460.295->465.855)! Learning rate decreased to 0.01581.
2024-12-01-17:45:21-root-INFO: Loss too large (460.295->462.207)! Learning rate decreased to 0.01265.
2024-12-01-17:45:22-root-INFO: grad norm: 28.952 28.421 5.519
2024-12-01-17:45:23-root-INFO: grad norm: 25.011 24.554 4.760
2024-12-01-17:45:24-root-INFO: grad norm: 24.665 24.191 4.813
2024-12-01-17:45:25-root-INFO: grad norm: 24.643 24.180 4.755
2024-12-01-17:45:26-root-INFO: grad norm: 24.357 23.887 4.758
2024-12-01-17:45:27-root-INFO: grad norm: 23.911 23.452 4.664
2024-12-01-17:45:28-root-INFO: grad norm: 23.756 23.295 4.659
2024-12-01-17:45:29-root-INFO: Loss Change: 460.295 -> 448.340
2024-12-01-17:45:29-root-INFO: Regularization Change: 0.000 -> 1.094
2024-12-01-17:45:29-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-01-17:45:29-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-17:45:29-root-INFO: step: 129 lr_xt 0.03191668
2024-12-01-17:45:30-root-INFO: grad norm: 33.963 33.477 5.729
2024-12-01-17:45:30-root-INFO: Loss too large (449.984->472.154)! Learning rate decreased to 0.02553.
2024-12-01-17:45:30-root-INFO: Loss too large (449.984->462.832)! Learning rate decreased to 0.02043.
2024-12-01-17:45:31-root-INFO: Loss too large (449.984->456.279)! Learning rate decreased to 0.01634.
2024-12-01-17:45:31-root-INFO: Loss too large (449.984->451.851)! Learning rate decreased to 0.01307.
2024-12-01-17:45:32-root-INFO: grad norm: 29.535 29.029 5.441
2024-12-01-17:45:33-root-INFO: grad norm: 24.492 24.054 4.611
2024-12-01-17:45:34-root-INFO: grad norm: 23.683 23.233 4.591
2024-12-01-17:45:35-root-INFO: grad norm: 23.351 22.913 4.503
2024-12-01-17:45:36-root-INFO: grad norm: 22.960 22.513 4.506
2024-12-01-17:45:37-root-INFO: grad norm: 22.499 22.066 4.391
2024-12-01-17:45:38-root-INFO: grad norm: 22.307 21.867 4.409
2024-12-01-17:45:39-root-INFO: Loss Change: 449.984 -> 437.455
2024-12-01-17:45:39-root-INFO: Regularization Change: 0.000 -> 1.112
2024-12-01-17:45:39-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-01-17:45:39-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-17:45:39-root-INFO: step: 128 lr_xt 0.03298138
2024-12-01-17:45:40-root-INFO: grad norm: 29.832 29.393 5.097
2024-12-01-17:45:40-root-INFO: Loss too large (438.076->456.285)! Learning rate decreased to 0.02639.
2024-12-01-17:45:40-root-INFO: Loss too large (438.076->448.144)! Learning rate decreased to 0.02111.
2024-12-01-17:45:41-root-INFO: Loss too large (438.076->442.619)! Learning rate decreased to 0.01689.
2024-12-01-17:45:41-root-INFO: Loss too large (438.076->439.021)! Learning rate decreased to 0.01351.
2024-12-01-17:45:42-root-INFO: grad norm: 26.555 26.067 5.068
2024-12-01-17:45:43-root-INFO: grad norm: 24.185 23.769 4.465
2024-12-01-17:45:44-root-INFO: grad norm: 23.349 22.894 4.590
2024-12-01-17:45:45-root-INFO: grad norm: 22.497 22.085 4.290
2024-12-01-17:45:46-root-INFO: grad norm: 22.126 21.683 4.403
2024-12-01-17:45:47-root-INFO: grad norm: 21.671 21.262 4.186
2024-12-01-17:45:48-root-INFO: grad norm: 21.470 21.035 4.297
2024-12-01-17:45:49-root-INFO: Loss Change: 438.076 -> 426.464
2024-12-01-17:45:49-root-INFO: Regularization Change: 0.000 -> 1.103
2024-12-01-17:45:49-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-01-17:45:49-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-17:45:49-root-INFO: step: 127 lr_xt 0.03407612
2024-12-01-17:45:50-root-INFO: grad norm: 30.881 30.417 5.332
2024-12-01-17:45:50-root-INFO: Loss too large (427.498->445.387)! Learning rate decreased to 0.02726.
2024-12-01-17:45:50-root-INFO: Loss too large (427.498->438.364)! Learning rate decreased to 0.02181.
2024-12-01-17:45:51-root-INFO: Loss too large (427.498->433.197)! Learning rate decreased to 0.01745.
2024-12-01-17:45:51-root-INFO: Loss too large (427.498->429.519)! Learning rate decreased to 0.01396.
2024-12-01-17:45:52-root-INFO: grad norm: 26.828 26.340 5.096
2024-12-01-17:45:53-root-INFO: grad norm: 20.816 20.430 3.987
2024-12-01-17:45:54-root-INFO: grad norm: 20.268 19.852 4.088
2024-12-01-17:45:55-root-INFO: grad norm: 20.242 19.859 3.918
2024-12-01-17:45:56-root-INFO: grad norm: 20.036 19.617 4.077
2024-12-01-17:45:57-root-INFO: grad norm: 19.868 19.488 3.865
2024-12-01-17:45:58-root-INFO: grad norm: 19.737 19.320 4.033
2024-12-01-17:45:59-root-INFO: Loss Change: 427.498 -> 416.419
2024-12-01-17:45:59-root-INFO: Regularization Change: 0.000 -> 1.063
2024-12-01-17:45:59-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-01-17:45:59-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-17:45:59-root-INFO: step: 126 lr_xt 0.03520152
2024-12-01-17:45:59-root-INFO: grad norm: 23.468 23.090 4.198
2024-12-01-17:46:00-root-INFO: Loss too large (416.718->428.396)! Learning rate decreased to 0.02816.
2024-12-01-17:46:00-root-INFO: Loss too large (416.718->423.119)! Learning rate decreased to 0.02253.
2024-12-01-17:46:00-root-INFO: Loss too large (416.718->419.501)! Learning rate decreased to 0.01802.
2024-12-01-17:46:01-root-INFO: Loss too large (416.718->417.146)! Learning rate decreased to 0.01442.
2024-12-01-17:46:02-root-INFO: grad norm: 21.941 21.505 4.355
2024-12-01-17:46:03-root-INFO: grad norm: 21.206 20.832 3.967
2024-12-01-17:46:04-root-INFO: grad norm: 20.695 20.272 4.164
2024-12-01-17:46:05-root-INFO: grad norm: 20.030 19.659 3.837
2024-12-01-17:46:06-root-INFO: grad norm: 19.756 19.343 4.019
2024-12-01-17:46:07-root-INFO: grad norm: 19.374 19.007 3.754
2024-12-01-17:46:08-root-INFO: grad norm: 19.203 18.796 3.934
2024-12-01-17:46:09-root-INFO: Loss Change: 416.718 -> 407.027
2024-12-01-17:46:09-root-INFO: Regularization Change: 0.000 -> 1.043
2024-12-01-17:46:09-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-01-17:46:09-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-17:46:09-root-INFO: step: 125 lr_xt 0.03635823
2024-12-01-17:46:09-root-INFO: grad norm: 24.832 24.465 4.252
2024-12-01-17:46:10-root-INFO: Loss too large (407.547->421.956)! Learning rate decreased to 0.02909.
2024-12-01-17:46:10-root-INFO: Loss too large (407.547->415.257)! Learning rate decreased to 0.02327.
2024-12-01-17:46:10-root-INFO: Loss too large (407.547->410.796)! Learning rate decreased to 0.01862.
2024-12-01-17:46:11-root-INFO: Loss too large (407.547->407.966)! Learning rate decreased to 0.01489.
2024-12-01-17:46:12-root-INFO: grad norm: 22.490 22.027 4.539
2024-12-01-17:46:13-root-INFO: grad norm: 21.398 21.040 3.900
2024-12-01-17:46:14-root-INFO: grad norm: 20.559 20.120 4.227
2024-12-01-17:46:15-root-INFO: grad norm: 19.515 19.159 3.713
2024-12-01-17:46:16-root-INFO: grad norm: 19.099 18.679 3.983
2024-12-01-17:46:17-root-INFO: grad norm: 18.552 18.199 3.602
2024-12-01-17:46:18-root-INFO: grad norm: 18.318 17.908 3.851
2024-12-01-17:46:19-root-INFO: Loss Change: 407.547 -> 397.591
2024-12-01-17:46:19-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-01-17:46:19-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-01-17:46:19-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-17:46:19-root-INFO: step: 124 lr_xt 0.03773645
2024-12-01-17:46:19-root-INFO: grad norm: 30.408 29.982 5.076
2024-12-01-17:46:20-root-INFO: Loss too large (399.714->421.571)! Learning rate decreased to 0.03019.
2024-12-01-17:46:20-root-INFO: Loss too large (399.714->411.658)! Learning rate decreased to 0.02415.
2024-12-01-17:46:20-root-INFO: Loss too large (399.714->405.007)! Learning rate decreased to 0.01932.
2024-12-01-17:46:21-root-INFO: Loss too large (399.714->400.709)! Learning rate decreased to 0.01546.
2024-12-01-17:46:22-root-INFO: grad norm: 25.499 25.009 4.975
2024-12-01-17:46:23-root-INFO: grad norm: 21.264 20.908 3.872
2024-12-01-17:46:24-root-INFO: grad norm: 19.965 19.533 4.130
2024-12-01-17:46:25-root-INFO: grad norm: 18.769 18.422 3.590
2024-12-01-17:46:26-root-INFO: grad norm: 18.213 17.797 3.871
2024-12-01-17:46:27-root-INFO: grad norm: 17.586 17.245 3.444
2024-12-01-17:46:28-root-INFO: grad norm: 17.299 16.893 3.724
2024-12-01-17:46:29-root-INFO: Loss Change: 399.714 -> 388.320
2024-12-01-17:46:29-root-INFO: Regularization Change: 0.000 -> 1.119
2024-12-01-17:46:29-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-01-17:46:29-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-17:46:29-root-INFO: step: 123 lr_xt 0.03896235
2024-12-01-17:46:29-root-INFO: grad norm: 22.814 22.459 4.008
2024-12-01-17:46:30-root-INFO: Loss too large (388.979->401.707)! Learning rate decreased to 0.03117.
2024-12-01-17:46:30-root-INFO: Loss too large (388.979->396.057)! Learning rate decreased to 0.02494.
2024-12-01-17:46:30-root-INFO: Loss too large (388.979->392.159)! Learning rate decreased to 0.01995.
2024-12-01-17:46:31-root-INFO: Loss too large (388.979->389.597)! Learning rate decreased to 0.01596.
2024-12-01-17:46:32-root-INFO: grad norm: 21.019 20.572 4.314
2024-12-01-17:46:33-root-INFO: grad norm: 19.705 19.362 3.662
2024-12-01-17:46:34-root-INFO: grad norm: 19.021 18.590 4.025
2024-12-01-17:46:35-root-INFO: grad norm: 18.184 17.847 3.483
2024-12-01-17:46:36-root-INFO: grad norm: 17.811 17.394 3.836
2024-12-01-17:46:37-root-INFO: grad norm: 17.307 16.976 3.369
2024-12-01-17:46:38-root-INFO: grad norm: 17.071 16.662 3.716
2024-12-01-17:46:39-root-INFO: Loss Change: 388.979 -> 379.854
2024-12-01-17:46:39-root-INFO: Regularization Change: 0.000 -> 1.055
2024-12-01-17:46:39-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-01-17:46:39-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-17:46:39-root-INFO: step: 122 lr_xt 0.04022160
2024-12-01-17:46:39-root-INFO: grad norm: 23.675 23.314 4.121
2024-12-01-17:46:40-root-INFO: Loss too large (380.760->394.206)! Learning rate decreased to 0.03218.
2024-12-01-17:46:40-root-INFO: Loss too large (380.760->388.339)! Learning rate decreased to 0.02574.
2024-12-01-17:46:40-root-INFO: Loss too large (380.760->384.211)! Learning rate decreased to 0.02059.
2024-12-01-17:46:41-root-INFO: Loss too large (380.760->381.446)! Learning rate decreased to 0.01647.
2024-12-01-17:46:42-root-INFO: grad norm: 21.037 20.577 4.373
2024-12-01-17:46:43-root-INFO: grad norm: 18.539 18.209 3.483
2024-12-01-17:46:44-root-INFO: grad norm: 17.607 17.191 3.809
2024-12-01-17:46:45-root-INFO: grad norm: 16.525 16.207 3.228
2024-12-01-17:46:46-root-INFO: grad norm: 16.038 15.640 3.549
2024-12-01-17:46:47-root-INFO: grad norm: 15.438 15.127 3.083
2024-12-01-17:46:48-root-INFO: grad norm: 15.141 14.755 3.398
2024-12-01-17:46:49-root-INFO: Loss Change: 380.760 -> 371.426
2024-12-01-17:46:49-root-INFO: Regularization Change: 0.000 -> 1.064
2024-12-01-17:46:49-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-01-17:46:49-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-17:46:49-root-INFO: step: 121 lr_xt 0.04151486
2024-12-01-17:46:49-root-INFO: grad norm: 20.929 20.593 3.736
2024-12-01-17:46:50-root-INFO: Loss too large (371.892->383.174)! Learning rate decreased to 0.03321.
2024-12-01-17:46:50-root-INFO: Loss too large (371.892->378.011)! Learning rate decreased to 0.02657.
2024-12-01-17:46:50-root-INFO: Loss too large (371.892->374.477)! Learning rate decreased to 0.02126.
2024-12-01-17:46:51-root-INFO: Loss too large (371.892->372.193)! Learning rate decreased to 0.01700.
2024-12-01-17:46:52-root-INFO: grad norm: 19.165 18.718 4.115
2024-12-01-17:46:53-root-INFO: grad norm: 18.033 17.710 3.401
2024-12-01-17:46:54-root-INFO: grad norm: 17.360 16.931 3.837
2024-12-01-17:46:55-root-INFO: grad norm: 16.538 16.222 3.218
2024-12-01-17:46:56-root-INFO: grad norm: 16.132 15.720 3.625
2024-12-01-17:46:57-root-INFO: grad norm: 15.594 15.284 3.094
2024-12-01-17:46:58-root-INFO: grad norm: 15.316 14.915 3.482
2024-12-01-17:46:59-root-INFO: Loss Change: 371.892 -> 363.234
2024-12-01-17:46:59-root-INFO: Regularization Change: 0.000 -> 1.062
2024-12-01-17:46:59-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-01-17:46:59-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-17:46:59-root-INFO: step: 120 lr_xt 0.04284282
2024-12-01-17:46:59-root-INFO: grad norm: 20.463 20.129 3.682
2024-12-01-17:47:00-root-INFO: Loss too large (364.080->375.325)! Learning rate decreased to 0.03427.
2024-12-01-17:47:00-root-INFO: Loss too large (364.080->370.107)! Learning rate decreased to 0.02742.
2024-12-01-17:47:00-root-INFO: Loss too large (364.080->366.570)! Learning rate decreased to 0.02194.
2024-12-01-17:47:01-root-INFO: Loss too large (364.080->364.308)! Learning rate decreased to 0.01755.
2024-12-01-17:47:02-root-INFO: grad norm: 18.636 18.195 4.032
2024-12-01-17:47:03-root-INFO: grad norm: 17.621 17.297 3.362
2024-12-01-17:47:04-root-INFO: grad norm: 16.979 16.547 3.805
2024-12-01-17:47:05-root-INFO: grad norm: 16.191 15.875 3.179
2024-12-01-17:47:06-root-INFO: grad norm: 15.783 15.366 3.603
2024-12-01-17:47:07-root-INFO: grad norm: 15.235 14.926 3.051
2024-12-01-17:47:08-root-INFO: grad norm: 14.942 14.537 3.456
2024-12-01-17:47:08-root-INFO: Loss Change: 364.080 -> 355.562
2024-12-01-17:47:08-root-INFO: Regularization Change: 0.000 -> 1.075
2024-12-01-17:47:08-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-01-17:47:08-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-17:47:09-root-INFO: step: 119 lr_xt 0.04420613
2024-12-01-17:47:09-root-INFO: grad norm: 20.899 20.552 3.794
2024-12-01-17:47:09-root-INFO: Loss too large (356.190->367.909)! Learning rate decreased to 0.03536.
2024-12-01-17:47:10-root-INFO: Loss too large (356.190->362.205)! Learning rate decreased to 0.02829.
2024-12-01-17:47:10-root-INFO: Loss too large (356.190->358.418)! Learning rate decreased to 0.02263.
2024-12-01-17:47:11-root-INFO: grad norm: 23.709 23.183 4.967
2024-12-01-17:47:12-root-INFO: grad norm: 29.655 29.217 5.073
2024-12-01-17:47:13-root-INFO: Loss too large (355.339->357.055)! Learning rate decreased to 0.01811.
2024-12-01-17:47:14-root-INFO: grad norm: 22.899 22.333 5.061
2024-12-01-17:47:15-root-INFO: grad norm: 15.537 15.241 3.018
2024-12-01-17:47:16-root-INFO: grad norm: 14.012 13.612 3.324
2024-12-01-17:47:17-root-INFO: grad norm: 12.810 12.528 2.676
2024-12-01-17:47:18-root-INFO: grad norm: 12.161 11.785 2.999
2024-12-01-17:47:18-root-INFO: Loss Change: 356.190 -> 346.679
2024-12-01-17:47:18-root-INFO: Regularization Change: 0.000 -> 1.259
2024-12-01-17:47:18-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-01-17:47:18-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-17:47:19-root-INFO: step: 118 lr_xt 0.04560549
2024-12-01-17:47:19-root-INFO: grad norm: 16.113 15.796 3.177
2024-12-01-17:47:19-root-INFO: Loss too large (347.369->354.028)! Learning rate decreased to 0.03648.
2024-12-01-17:47:20-root-INFO: Loss too large (347.369->350.695)! Learning rate decreased to 0.02919.
2024-12-01-17:47:20-root-INFO: Loss too large (347.369->348.463)! Learning rate decreased to 0.02335.
2024-12-01-17:47:21-root-INFO: grad norm: 19.116 18.624 4.308
2024-12-01-17:47:22-root-INFO: grad norm: 24.461 24.046 4.487
2024-12-01-17:47:22-root-INFO: Loss too large (346.522->347.561)! Learning rate decreased to 0.01868.
2024-12-01-17:47:23-root-INFO: grad norm: 19.854 19.321 4.569
2024-12-01-17:47:24-root-INFO: grad norm: 14.848 14.536 3.025
2024-12-01-17:47:25-root-INFO: grad norm: 13.345 12.946 3.239
2024-12-01-17:47:26-root-INFO: grad norm: 11.911 11.625 2.595
2024-12-01-17:47:27-root-INFO: grad norm: 11.113 10.752 2.810
2024-12-01-17:47:28-root-INFO: Loss Change: 347.369 -> 339.168
2024-12-01-17:47:28-root-INFO: Regularization Change: 0.000 -> 1.201
2024-12-01-17:47:28-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-01-17:47:28-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-17:47:28-root-INFO: step: 117 lr_xt 0.04704158
2024-12-01-17:47:29-root-INFO: grad norm: 16.445 16.136 3.169
2024-12-01-17:47:29-root-INFO: Loss too large (339.872->346.746)! Learning rate decreased to 0.03763.
2024-12-01-17:47:29-root-INFO: Loss too large (339.872->342.612)! Learning rate decreased to 0.03011.
2024-12-01-17:47:30-root-INFO: Loss too large (339.872->340.170)! Learning rate decreased to 0.02409.
2024-12-01-17:47:31-root-INFO: grad norm: 17.888 17.485 3.774
2024-12-01-17:47:32-root-INFO: grad norm: 22.159 21.775 4.108
2024-12-01-17:47:32-root-INFO: Loss too large (338.337->338.833)! Learning rate decreased to 0.01927.
2024-12-01-17:47:33-root-INFO: grad norm: 18.089 17.592 4.207
2024-12-01-17:47:34-root-INFO: grad norm: 14.403 14.089 2.991
2024-12-01-17:47:35-root-INFO: grad norm: 12.776 12.378 3.166
2024-12-01-17:47:36-root-INFO: grad norm: 11.294 11.008 2.529
2024-12-01-17:47:37-root-INFO: grad norm: 10.424 10.067 2.704
2024-12-01-17:47:38-root-INFO: Loss Change: 339.872 -> 331.341
2024-12-01-17:47:38-root-INFO: Regularization Change: 0.000 -> 1.239
2024-12-01-17:47:38-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-01-17:47:38-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-17:47:38-root-INFO: step: 116 lr_xt 0.04851508
2024-12-01-17:47:39-root-INFO: grad norm: 14.172 13.874 2.890
2024-12-01-17:47:39-root-INFO: Loss too large (331.528->336.306)! Learning rate decreased to 0.03881.
2024-12-01-17:47:39-root-INFO: Loss too large (331.528->333.428)! Learning rate decreased to 0.03105.
2024-12-01-17:47:40-root-INFO: Loss too large (331.528->331.681)! Learning rate decreased to 0.02484.
2024-12-01-17:47:41-root-INFO: grad norm: 16.049 15.609 3.732
2024-12-01-17:47:42-root-INFO: grad norm: 20.069 19.692 3.871
2024-12-01-17:47:42-root-INFO: Loss too large (330.178->330.507)! Learning rate decreased to 0.01987.
2024-12-01-17:47:43-root-INFO: grad norm: 16.642 16.149 4.020
2024-12-01-17:47:44-root-INFO: grad norm: 13.423 13.115 2.859
2024-12-01-17:47:45-root-INFO: grad norm: 11.903 11.511 3.028
2024-12-01-17:47:46-root-INFO: grad norm: 10.508 10.228 2.412
2024-12-01-17:47:47-root-INFO: grad norm: 9.658 9.311 2.566
2024-12-01-17:47:48-root-INFO: Loss Change: 331.528 -> 323.681
2024-12-01-17:47:48-root-INFO: Regularization Change: 0.000 -> 1.219
2024-12-01-17:47:48-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-01-17:47:48-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-17:47:48-root-INFO: step: 115 lr_xt 0.05002669
2024-12-01-17:47:49-root-INFO: grad norm: 14.012 13.713 2.880
2024-12-01-17:47:49-root-INFO: Loss too large (324.481->329.447)! Learning rate decreased to 0.04002.
2024-12-01-17:47:49-root-INFO: Loss too large (324.481->326.391)! Learning rate decreased to 0.03202.
2024-12-01-17:47:50-root-INFO: Loss too large (324.481->324.582)! Learning rate decreased to 0.02561.
2024-12-01-17:47:51-root-INFO: grad norm: 15.488 15.088 3.497
2024-12-01-17:47:52-root-INFO: grad norm: 18.859 18.492 3.701
2024-12-01-17:47:52-root-INFO: Loss too large (323.033->323.150)! Learning rate decreased to 0.02049.
2024-12-01-17:47:53-root-INFO: grad norm: 15.478 15.017 3.749
2024-12-01-17:47:54-root-INFO: grad norm: 12.557 12.252 2.754
2024-12-01-17:47:55-root-INFO: grad norm: 11.046 10.673 2.846
2024-12-01-17:47:56-root-INFO: grad norm: 9.728 9.449 2.313
2024-12-01-17:47:57-root-INFO: grad norm: 8.903 8.571 2.408
2024-12-01-17:47:58-root-INFO: Loss Change: 324.481 -> 316.780
2024-12-01-17:47:58-root-INFO: Regularization Change: 0.000 -> 1.218
2024-12-01-17:47:58-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-01-17:47:58-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-17:47:58-root-INFO: step: 114 lr_xt 0.05157710
2024-12-01-17:47:58-root-INFO: grad norm: 11.891 11.627 2.491
2024-12-01-17:47:59-root-INFO: Loss too large (317.097->319.846)! Learning rate decreased to 0.04126.
2024-12-01-17:47:59-root-INFO: Loss too large (317.097->317.805)! Learning rate decreased to 0.03301.
2024-12-01-17:48:00-root-INFO: grad norm: 16.910 16.508 3.667
2024-12-01-17:48:01-root-INFO: Loss too large (316.663->318.099)! Learning rate decreased to 0.02641.
2024-12-01-17:48:02-root-INFO: grad norm: 19.196 18.849 3.630
2024-12-01-17:48:03-root-INFO: grad norm: 20.557 20.007 4.720
2024-12-01-17:48:04-root-INFO: grad norm: 22.018 21.619 4.174
2024-12-01-17:48:05-root-INFO: grad norm: 22.255 21.644 5.177
2024-12-01-17:48:06-root-INFO: grad norm: 22.099 21.696 4.204
2024-12-01-17:48:07-root-INFO: grad norm: 21.881 21.287 5.066
2024-12-01-17:48:07-root-INFO: Loss Change: 317.097 -> 311.105
2024-12-01-17:48:07-root-INFO: Regularization Change: 0.000 -> 1.783
2024-12-01-17:48:07-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-01-17:48:07-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-17:48:08-root-INFO: step: 113 lr_xt 0.05316701
2024-12-01-17:48:08-root-INFO: grad norm: 26.589 26.147 4.830
2024-12-01-17:48:09-root-INFO: Loss too large (312.424->335.300)! Learning rate decreased to 0.04253.
2024-12-01-17:48:09-root-INFO: Loss too large (312.424->324.022)! Learning rate decreased to 0.03403.
2024-12-01-17:48:09-root-INFO: Loss too large (312.424->316.401)! Learning rate decreased to 0.02722.
2024-12-01-17:48:10-root-INFO: grad norm: 25.222 24.642 5.375
2024-12-01-17:48:11-root-INFO: grad norm: 23.610 23.226 4.245
2024-12-01-17:48:12-root-INFO: grad norm: 22.663 22.139 4.847
2024-12-01-17:48:13-root-INFO: grad norm: 21.578 21.215 3.939
2024-12-01-17:48:14-root-INFO: grad norm: 20.848 20.361 4.479
2024-12-01-17:48:15-root-INFO: grad norm: 20.040 19.694 3.711
2024-12-01-17:48:16-root-INFO: grad norm: 19.465 19.009 4.190
2024-12-01-17:48:17-root-INFO: Loss Change: 312.424 -> 302.788
2024-12-01-17:48:17-root-INFO: Regularization Change: 0.000 -> 1.638
2024-12-01-17:48:17-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-01-17:48:17-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-17:48:17-root-INFO: step: 112 lr_xt 0.05479712
2024-12-01-17:48:18-root-INFO: grad norm: 25.201 24.837 4.265
2024-12-01-17:48:18-root-INFO: Loss too large (304.490->326.315)! Learning rate decreased to 0.04384.
2024-12-01-17:48:18-root-INFO: Loss too large (304.490->315.066)! Learning rate decreased to 0.03507.
2024-12-01-17:48:19-root-INFO: Loss too large (304.490->307.672)! Learning rate decreased to 0.02806.
2024-12-01-17:48:20-root-INFO: grad norm: 23.656 23.217 4.539
2024-12-01-17:48:21-root-INFO: grad norm: 22.204 21.885 3.750
2024-12-01-17:48:22-root-INFO: grad norm: 21.097 20.694 4.103
2024-12-01-17:48:23-root-INFO: grad norm: 20.016 19.718 3.441
2024-12-01-17:48:24-root-INFO: grad norm: 19.206 18.837 3.746
2024-12-01-17:48:25-root-INFO: grad norm: 18.433 18.152 3.208
2024-12-01-17:48:26-root-INFO: grad norm: 17.844 17.502 3.475
2024-12-01-17:48:27-root-INFO: Loss Change: 304.490 -> 295.019
2024-12-01-17:48:27-root-INFO: Regularization Change: 0.000 -> 1.612
2024-12-01-17:48:27-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-01-17:48:27-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-17:48:27-root-INFO: step: 111 lr_xt 0.05646812
2024-12-01-17:48:28-root-INFO: grad norm: 22.111 21.804 3.670
2024-12-01-17:48:28-root-INFO: Loss too large (296.231->313.538)! Learning rate decreased to 0.04517.
2024-12-01-17:48:28-root-INFO: Loss too large (296.231->304.395)! Learning rate decreased to 0.03614.
2024-12-01-17:48:29-root-INFO: Loss too large (296.231->298.482)! Learning rate decreased to 0.02891.
2024-12-01-17:48:30-root-INFO: grad norm: 20.676 20.325 3.797
2024-12-01-17:48:31-root-INFO: grad norm: 19.367 19.095 3.232
2024-12-01-17:48:32-root-INFO: grad norm: 18.336 18.012 3.435
2024-12-01-17:48:33-root-INFO: grad norm: 17.395 17.142 2.952
2024-12-01-17:48:34-root-INFO: grad norm: 16.662 16.364 3.134
2024-12-01-17:48:35-root-INFO: grad norm: 15.999 15.762 2.740
2024-12-01-17:48:36-root-INFO: grad norm: 15.479 15.205 2.901
2024-12-01-17:48:36-root-INFO: Loss Change: 296.231 -> 287.561
2024-12-01-17:48:36-root-INFO: Regularization Change: 0.000 -> 1.568
2024-12-01-17:48:36-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-01-17:48:36-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-17:48:37-root-INFO: step: 110 lr_xt 0.05818072
2024-12-01-17:48:37-root-INFO: grad norm: 19.762 19.503 3.192
2024-12-01-17:48:38-root-INFO: Loss too large (288.676->302.965)! Learning rate decreased to 0.04654.
2024-12-01-17:48:38-root-INFO: Loss too large (288.676->295.317)! Learning rate decreased to 0.03724.
2024-12-01-17:48:38-root-INFO: Loss too large (288.676->290.430)! Learning rate decreased to 0.02979.
2024-12-01-17:48:39-root-INFO: grad norm: 18.857 18.601 3.099
2024-12-01-17:48:40-root-INFO: grad norm: 18.064 17.844 2.809
2024-12-01-17:48:41-root-INFO: grad norm: 17.368 17.128 2.878
2024-12-01-17:48:42-root-INFO: grad norm: 16.713 16.509 2.601
2024-12-01-17:48:43-root-INFO: grad norm: 16.168 15.945 2.676
2024-12-01-17:48:44-root-INFO: grad norm: 15.671 15.480 2.441
2024-12-01-17:48:45-root-INFO: grad norm: 15.264 15.055 2.514
2024-12-01-17:48:46-root-INFO: Loss Change: 288.676 -> 280.934
2024-12-01-17:48:46-root-INFO: Regularization Change: 0.000 -> 1.550
2024-12-01-17:48:46-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-01-17:48:46-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-17:48:47-root-INFO: step: 109 lr_xt 0.05993563
2024-12-01-17:48:47-root-INFO: grad norm: 19.987 19.764 2.978
2024-12-01-17:48:47-root-INFO: Loss too large (282.405->296.935)! Learning rate decreased to 0.04795.
2024-12-01-17:48:48-root-INFO: Loss too large (282.405->289.211)! Learning rate decreased to 0.03836.
2024-12-01-17:48:48-root-INFO: Loss too large (282.405->284.191)! Learning rate decreased to 0.03069.
2024-12-01-17:48:49-root-INFO: grad norm: 18.866 18.661 2.771
2024-12-01-17:48:50-root-INFO: grad norm: 17.934 17.748 2.575
2024-12-01-17:48:51-root-INFO: grad norm: 17.159 16.955 2.637
2024-12-01-17:48:52-root-INFO: grad norm: 16.455 16.277 2.411
2024-12-01-17:48:53-root-INFO: grad norm: 15.881 15.687 2.477
2024-12-01-17:48:54-root-INFO: grad norm: 15.373 15.203 2.280
2024-12-01-17:48:55-root-INFO: grad norm: 14.959 14.775 2.341
2024-12-01-17:48:56-root-INFO: Loss Change: 282.405 -> 274.641
2024-12-01-17:48:56-root-INFO: Regularization Change: 0.000 -> 1.553
2024-12-01-17:48:56-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-01-17:48:56-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-17:48:56-root-INFO: step: 108 lr_xt 0.06173354
2024-12-01-17:48:57-root-INFO: grad norm: 18.219 18.031 2.615
2024-12-01-17:48:57-root-INFO: Loss too large (275.272->287.508)! Learning rate decreased to 0.04939.
2024-12-01-17:48:57-root-INFO: Loss too large (275.272->280.903)! Learning rate decreased to 0.03951.
2024-12-01-17:48:58-root-INFO: Loss too large (275.272->276.625)! Learning rate decreased to 0.03161.
2024-12-01-17:48:59-root-INFO: grad norm: 17.101 16.928 2.425
2024-12-01-17:49:00-root-INFO: grad norm: 16.207 16.047 2.273
2024-12-01-17:49:01-root-INFO: grad norm: 15.476 15.309 2.267
2024-12-01-17:49:02-root-INFO: grad norm: 14.855 14.702 2.123
2024-12-01-17:49:03-root-INFO: grad norm: 14.352 14.193 2.131
2024-12-01-17:49:04-root-INFO: grad norm: 13.929 13.782 2.014
2024-12-01-17:49:05-root-INFO: grad norm: 13.588 13.436 2.027
2024-12-01-17:49:06-root-INFO: Loss Change: 275.272 -> 267.944
2024-12-01-17:49:06-root-INFO: Regularization Change: 0.000 -> 1.534
2024-12-01-17:49:06-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-01-17:49:06-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-17:49:06-root-INFO: step: 107 lr_xt 0.06357517
2024-12-01-17:49:06-root-INFO: grad norm: 19.346 19.173 2.584
2024-12-01-17:49:07-root-INFO: Loss too large (269.607->283.577)! Learning rate decreased to 0.05086.
2024-12-01-17:49:07-root-INFO: Loss too large (269.607->276.254)! Learning rate decreased to 0.04069.
2024-12-01-17:49:07-root-INFO: Loss too large (269.607->271.370)! Learning rate decreased to 0.03255.
2024-12-01-17:49:09-root-INFO: grad norm: 18.307 18.154 2.355
2024-12-01-17:49:10-root-INFO: grad norm: 17.418 17.270 2.269
2024-12-01-17:49:11-root-INFO: grad norm: 16.675 16.515 2.306
2024-12-01-17:49:12-root-INFO: grad norm: 16.012 15.865 2.169
2024-12-01-17:49:13-root-INFO: grad norm: 15.465 15.307 2.208
2024-12-01-17:49:14-root-INFO: grad norm: 14.997 14.852 2.080
2024-12-01-17:49:15-root-INFO: grad norm: 14.611 14.457 2.116
2024-12-01-17:49:15-root-INFO: Loss Change: 269.607 -> 262.227
2024-12-01-17:49:15-root-INFO: Regularization Change: 0.000 -> 1.561
2024-12-01-17:49:15-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-01-17:49:15-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-17:49:16-root-INFO: step: 106 lr_xt 0.06546120
2024-12-01-17:49:16-root-INFO: grad norm: 19.255 19.055 2.767
2024-12-01-17:49:16-root-INFO: Loss too large (263.421->278.105)! Learning rate decreased to 0.05237.
2024-12-01-17:49:17-root-INFO: Loss too large (263.421->270.285)! Learning rate decreased to 0.04190.
2024-12-01-17:49:17-root-INFO: Loss too large (263.421->265.164)! Learning rate decreased to 0.03352.
2024-12-01-17:49:18-root-INFO: grad norm: 18.034 17.868 2.443
2024-12-01-17:49:19-root-INFO: grad norm: 17.043 16.885 2.310
2024-12-01-17:49:20-root-INFO: grad norm: 16.190 16.036 2.228
2024-12-01-17:49:21-root-INFO: grad norm: 15.479 15.337 2.093
2024-12-01-17:49:22-root-INFO: grad norm: 14.885 14.743 2.050
2024-12-01-17:49:23-root-INFO: grad norm: 14.400 14.269 1.941
2024-12-01-17:49:24-root-INFO: grad norm: 13.998 13.866 1.919
2024-12-01-17:49:25-root-INFO: Loss Change: 263.421 -> 255.988
2024-12-01-17:49:25-root-INFO: Regularization Change: 0.000 -> 1.568
2024-12-01-17:49:25-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-01-17:49:25-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-17:49:25-root-INFO: step: 105 lr_xt 0.06739236
2024-12-01-17:49:26-root-INFO: grad norm: 17.887 17.728 2.380
2024-12-01-17:49:26-root-INFO: Loss too large (257.305->270.224)! Learning rate decreased to 0.05391.
2024-12-01-17:49:26-root-INFO: Loss too large (257.305->263.487)! Learning rate decreased to 0.04313.
2024-12-01-17:49:27-root-INFO: Loss too large (257.305->258.962)! Learning rate decreased to 0.03450.
2024-12-01-17:49:28-root-INFO: grad norm: 17.138 17.013 2.064
2024-12-01-17:49:29-root-INFO: grad norm: 16.427 16.303 2.016
2024-12-01-17:49:30-root-INFO: grad norm: 15.772 15.651 1.956
2024-12-01-17:49:31-root-INFO: grad norm: 15.209 15.093 1.877
2024-12-01-17:49:32-root-INFO: grad norm: 14.719 14.603 1.844
2024-12-01-17:49:33-root-INFO: grad norm: 14.318 14.208 1.775
2024-12-01-17:49:34-root-INFO: grad norm: 13.977 13.866 1.756
2024-12-01-17:49:34-root-INFO: Loss Change: 257.305 -> 250.586
2024-12-01-17:49:34-root-INFO: Regularization Change: 0.000 -> 1.540
2024-12-01-17:49:34-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-01-17:49:34-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-17:49:35-root-INFO: step: 104 lr_xt 0.06936934
2024-12-01-17:49:35-root-INFO: grad norm: 18.270 18.119 2.342
2024-12-01-17:49:36-root-INFO: Loss too large (251.986->265.219)! Learning rate decreased to 0.05550.
2024-12-01-17:49:36-root-INFO: Loss too large (251.986->258.171)! Learning rate decreased to 0.04440.
2024-12-01-17:49:36-root-INFO: Loss too large (251.986->253.422)! Learning rate decreased to 0.03552.
2024-12-01-17:49:37-root-INFO: grad norm: 16.829 16.711 1.984
2024-12-01-17:49:38-root-INFO: grad norm: 15.729 15.613 1.907
2024-12-01-17:49:39-root-INFO: grad norm: 14.804 14.691 1.827
2024-12-01-17:49:40-root-INFO: grad norm: 14.112 14.003 1.750
2024-12-01-17:49:41-root-INFO: grad norm: 13.528 13.421 1.698
2024-12-01-17:49:42-root-INFO: grad norm: 13.098 12.994 1.643
2024-12-01-17:49:43-root-INFO: grad norm: 12.738 12.636 1.608
2024-12-01-17:49:44-root-INFO: Loss Change: 251.986 -> 244.826
2024-12-01-17:49:44-root-INFO: Regularization Change: 0.000 -> 1.555
2024-12-01-17:49:44-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-01-17:49:44-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-17:49:44-root-INFO: step: 103 lr_xt 0.07139284
2024-12-01-17:49:45-root-INFO: grad norm: 15.933 15.819 1.904
2024-12-01-17:49:45-root-INFO: Loss too large (245.699->256.268)! Learning rate decreased to 0.05711.
2024-12-01-17:49:45-root-INFO: Loss too large (245.699->250.563)! Learning rate decreased to 0.04569.
2024-12-01-17:49:46-root-INFO: Loss too large (245.699->246.760)! Learning rate decreased to 0.03655.
2024-12-01-17:49:47-root-INFO: grad norm: 14.925 14.827 1.699
2024-12-01-17:49:48-root-INFO: grad norm: 14.127 14.032 1.630
2024-12-01-17:49:49-root-INFO: grad norm: 13.420 13.324 1.598
2024-12-01-17:49:50-root-INFO: grad norm: 12.902 12.811 1.534
2024-12-01-17:49:51-root-INFO: grad norm: 12.448 12.355 1.516
2024-12-01-17:49:52-root-INFO: grad norm: 12.125 12.036 1.470
2024-12-01-17:49:53-root-INFO: grad norm: 11.847 11.757 1.459
2024-12-01-17:49:54-root-INFO: Loss Change: 245.699 -> 239.340
2024-12-01-17:49:54-root-INFO: Regularization Change: 0.000 -> 1.523
2024-12-01-17:49:54-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-01-17:49:54-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-17:49:54-root-INFO: step: 102 lr_xt 0.07346356
2024-12-01-17:49:54-root-INFO: grad norm: 15.097 14.969 1.960
2024-12-01-17:49:55-root-INFO: Loss too large (240.033->249.928)! Learning rate decreased to 0.05877.
2024-12-01-17:49:55-root-INFO: Loss too large (240.033->244.597)! Learning rate decreased to 0.04702.
2024-12-01-17:49:56-root-INFO: Loss too large (240.033->241.045)! Learning rate decreased to 0.03761.
2024-12-01-17:49:57-root-INFO: grad norm: 14.565 14.473 1.634
2024-12-01-17:49:58-root-INFO: grad norm: 14.121 14.028 1.614
2024-12-01-17:49:59-root-INFO: grad norm: 13.676 13.591 1.520
2024-12-01-17:50:00-root-INFO: grad norm: 13.312 13.227 1.502
2024-12-01-17:50:01-root-INFO: grad norm: 12.956 12.875 1.441
2024-12-01-17:50:02-root-INFO: grad norm: 12.688 12.607 1.431
2024-12-01-17:50:03-root-INFO: grad norm: 12.432 12.354 1.386
2024-12-01-17:50:03-root-INFO: Loss Change: 240.033 -> 234.197
2024-12-01-17:50:03-root-INFO: Regularization Change: 0.000 -> 1.533
2024-12-01-17:50:03-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-01-17:50:03-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-17:50:04-root-INFO: step: 101 lr_xt 0.07558219
2024-12-01-17:50:04-root-INFO: grad norm: 15.611 15.500 1.857
2024-12-01-17:50:04-root-INFO: Loss too large (235.188->245.978)! Learning rate decreased to 0.06047.
2024-12-01-17:50:05-root-INFO: Loss too large (235.188->240.208)! Learning rate decreased to 0.04837.
2024-12-01-17:50:05-root-INFO: Loss too large (235.188->236.301)! Learning rate decreased to 0.03870.
2024-12-01-17:50:06-root-INFO: grad norm: 14.706 14.626 1.536
2024-12-01-17:50:07-root-INFO: grad norm: 13.971 13.888 1.517
2024-12-01-17:50:08-root-INFO: grad norm: 13.264 13.185 1.441
2024-12-01-17:50:09-root-INFO: grad norm: 12.765 12.686 1.415
2024-12-01-17:50:10-root-INFO: grad norm: 12.291 12.215 1.367
2024-12-01-17:50:11-root-INFO: grad norm: 11.968 11.891 1.351
2024-12-01-17:50:12-root-INFO: grad norm: 11.667 11.593 1.314
2024-12-01-17:50:13-root-INFO: Loss Change: 235.188 -> 229.150
2024-12-01-17:50:13-root-INFO: Regularization Change: 0.000 -> 1.509
2024-12-01-17:50:13-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-01-17:50:13-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-17:50:13-root-INFO: step: 100 lr_xt 0.07774943
2024-12-01-17:50:14-root-INFO: grad norm: 14.728 14.625 1.736
2024-12-01-17:50:14-root-INFO: Loss too large (230.109->240.142)! Learning rate decreased to 0.06220.
2024-12-01-17:50:14-root-INFO: Loss too large (230.109->234.663)! Learning rate decreased to 0.04976.
2024-12-01-17:50:15-root-INFO: Loss too large (230.109->231.026)! Learning rate decreased to 0.03981.
2024-12-01-17:50:16-root-INFO: grad norm: 13.782 13.711 1.401
2024-12-01-17:50:17-root-INFO: grad norm: 13.064 12.988 1.405
2024-12-01-17:50:18-root-INFO: grad norm: 12.366 12.296 1.309
2024-12-01-17:50:19-root-INFO: grad norm: 11.893 11.821 1.307
2024-12-01-17:50:20-root-INFO: grad norm: 11.433 11.366 1.243
2024-12-01-17:50:21-root-INFO: grad norm: 11.134 11.064 1.247
2024-12-01-17:50:22-root-INFO: grad norm: 10.849 10.783 1.199
2024-12-01-17:50:23-root-INFO: Loss Change: 230.109 -> 224.323
2024-12-01-17:50:23-root-INFO: Regularization Change: 0.000 -> 1.488
2024-12-01-17:50:23-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-01-17:50:23-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-17:50:23-root-INFO: step: 99 lr_xt 0.07996596
2024-12-01-17:50:23-root-INFO: grad norm: 13.436 13.344 1.574
2024-12-01-17:50:24-root-INFO: Loss too large (224.907->233.586)! Learning rate decreased to 0.06397.
2024-12-01-17:50:24-root-INFO: Loss too large (224.907->228.742)! Learning rate decreased to 0.05118.
2024-12-01-17:50:24-root-INFO: Loss too large (224.907->225.590)! Learning rate decreased to 0.04094.
2024-12-01-17:50:25-root-INFO: grad norm: 12.628 12.561 1.302
2024-12-01-17:50:26-root-INFO: grad norm: 12.042 11.972 1.290
2024-12-01-17:50:27-root-INFO: grad norm: 11.471 11.407 1.217
2024-12-01-17:50:29-root-INFO: grad norm: 11.087 11.021 1.209
2024-12-01-17:50:30-root-INFO: grad norm: 10.704 10.641 1.161
2024-12-01-17:50:31-root-INFO: grad norm: 10.457 10.393 1.161
2024-12-01-17:50:32-root-INFO: grad norm: 10.218 10.156 1.124
2024-12-01-17:50:32-root-INFO: Loss Change: 224.907 -> 219.515
2024-12-01-17:50:32-root-INFO: Regularization Change: 0.000 -> 1.475
2024-12-01-17:50:32-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-01-17:50:32-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-17:50:33-root-INFO: step: 98 lr_xt 0.08223248
2024-12-01-17:50:33-root-INFO: grad norm: 13.387 13.287 1.633
2024-12-01-17:50:33-root-INFO: Loss too large (220.469->229.546)! Learning rate decreased to 0.06579.
2024-12-01-17:50:34-root-INFO: Loss too large (220.469->224.501)! Learning rate decreased to 0.05263.
2024-12-01-17:50:34-root-INFO: Loss too large (220.469->221.236)! Learning rate decreased to 0.04210.
2024-12-01-17:50:35-root-INFO: grad norm: 12.696 12.630 1.293
2024-12-01-17:50:36-root-INFO: grad norm: 12.142 12.076 1.263
2024-12-01-17:50:37-root-INFO: grad norm: 11.575 11.515 1.180
2024-12-01-17:50:38-root-INFO: grad norm: 11.178 11.117 1.162
2024-12-01-17:50:39-root-INFO: grad norm: 10.757 10.699 1.115
2024-12-01-17:50:40-root-INFO: grad norm: 10.485 10.426 1.109
2024-12-01-17:50:41-root-INFO: grad norm: 10.203 10.146 1.074
2024-12-01-17:50:42-root-INFO: Loss Change: 220.469 -> 215.219
2024-12-01-17:50:42-root-INFO: Regularization Change: 0.000 -> 1.473
2024-12-01-17:50:42-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-01-17:50:42-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-17:50:42-root-INFO: step: 97 lr_xt 0.08454965
2024-12-01-17:50:43-root-INFO: grad norm: 13.323 13.233 1.545
2024-12-01-17:50:43-root-INFO: Loss too large (215.951->225.167)! Learning rate decreased to 0.06764.
2024-12-01-17:50:43-root-INFO: Loss too large (215.951->220.024)! Learning rate decreased to 0.05411.
2024-12-01-17:50:44-root-INFO: Loss too large (215.951->216.680)! Learning rate decreased to 0.04329.
2024-12-01-17:50:45-root-INFO: grad norm: 12.501 12.444 1.184
2024-12-01-17:50:46-root-INFO: grad norm: 11.879 11.821 1.181
2024-12-01-17:50:47-root-INFO: grad norm: 11.239 11.185 1.096
2024-12-01-17:50:48-root-INFO: grad norm: 10.824 10.768 1.093
2024-12-01-17:50:49-root-INFO: grad norm: 10.376 10.323 1.045
2024-12-01-17:50:50-root-INFO: grad norm: 10.104 10.050 1.047
2024-12-01-17:50:51-root-INFO: grad norm: 9.818 9.766 1.013
2024-12-01-17:50:52-root-INFO: Loss Change: 215.951 -> 210.698
2024-12-01-17:50:52-root-INFO: Regularization Change: 0.000 -> 1.472
2024-12-01-17:50:52-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-01-17:50:52-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-17:50:52-root-INFO: step: 96 lr_xt 0.08691815
2024-12-01-17:50:52-root-INFO: grad norm: 12.171 12.089 1.412
2024-12-01-17:50:53-root-INFO: Loss too large (211.363->219.268)! Learning rate decreased to 0.06953.
2024-12-01-17:50:53-root-INFO: Loss too large (211.363->214.716)! Learning rate decreased to 0.05563.
2024-12-01-17:50:53-root-INFO: Loss too large (211.363->211.837)! Learning rate decreased to 0.04450.
2024-12-01-17:50:54-root-INFO: grad norm: 11.212 11.156 1.112
2024-12-01-17:50:55-root-INFO: grad norm: 10.588 10.530 1.108
2024-12-01-17:50:56-root-INFO: grad norm: 9.970 9.918 1.022
2024-12-01-17:50:57-root-INFO: grad norm: 9.589 9.534 1.027
2024-12-01-17:50:58-root-INFO: grad norm: 9.199 9.147 0.977
2024-12-01-17:51:00-root-INFO: grad norm: 8.966 8.912 0.986
2024-12-01-17:51:01-root-INFO: grad norm: 8.734 8.682 0.951
2024-12-01-17:51:01-root-INFO: Loss Change: 211.363 -> 206.318
2024-12-01-17:51:01-root-INFO: Regularization Change: 0.000 -> 1.463
2024-12-01-17:51:01-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-01-17:51:01-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-17:51:02-root-INFO: step: 95 lr_xt 0.08933865
2024-12-01-17:51:02-root-INFO: grad norm: 11.611 11.526 1.406
2024-12-01-17:51:02-root-INFO: Loss too large (207.244->214.844)! Learning rate decreased to 0.07147.
2024-12-01-17:51:03-root-INFO: Loss too large (207.244->210.467)! Learning rate decreased to 0.05718.
2024-12-01-17:51:03-root-INFO: Loss too large (207.244->207.729)! Learning rate decreased to 0.04574.
2024-12-01-17:51:04-root-INFO: grad norm: 10.954 10.902 1.065
2024-12-01-17:51:05-root-INFO: grad norm: 10.489 10.434 1.071
2024-12-01-17:51:06-root-INFO: grad norm: 10.011 9.962 0.987
2024-12-01-17:51:07-root-INFO: grad norm: 9.695 9.644 0.997
2024-12-01-17:51:08-root-INFO: grad norm: 9.337 9.289 0.947
2024-12-01-17:51:09-root-INFO: grad norm: 9.121 9.071 0.961
2024-12-01-17:51:10-root-INFO: grad norm: 8.883 8.835 0.923
2024-12-01-17:51:11-root-INFO: Loss Change: 207.244 -> 202.509
2024-12-01-17:51:11-root-INFO: Regularization Change: 0.000 -> 1.468
2024-12-01-17:51:11-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-01-17:51:11-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-17:51:11-root-INFO: step: 94 lr_xt 0.09181181
2024-12-01-17:51:12-root-INFO: grad norm: 11.529 11.455 1.310
2024-12-01-17:51:12-root-INFO: Loss too large (203.074->210.812)! Learning rate decreased to 0.07345.
2024-12-01-17:51:12-root-INFO: Loss too large (203.074->206.377)! Learning rate decreased to 0.05876.
2024-12-01-17:51:13-root-INFO: Loss too large (203.074->203.568)! Learning rate decreased to 0.04701.
2024-12-01-17:51:14-root-INFO: grad norm: 10.879 10.833 0.998
2024-12-01-17:51:15-root-INFO: grad norm: 10.393 10.343 1.018
2024-12-01-17:51:16-root-INFO: grad norm: 9.860 9.815 0.941
2024-12-01-17:51:17-root-INFO: grad norm: 9.532 9.484 0.955
2024-12-01-17:51:18-root-INFO: grad norm: 9.145 9.100 0.909
2024-12-01-17:51:19-root-INFO: grad norm: 8.925 8.877 0.923
2024-12-01-17:51:20-root-INFO: grad norm: 8.674 8.629 0.889
2024-12-01-17:51:21-root-INFO: Loss Change: 203.074 -> 198.359
2024-12-01-17:51:21-root-INFO: Regularization Change: 0.000 -> 1.478
2024-12-01-17:51:21-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-01-17:51:21-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-17:51:21-root-INFO: step: 93 lr_xt 0.09433829
2024-12-01-17:51:21-root-INFO: grad norm: 11.417 11.339 1.335
2024-12-01-17:51:22-root-INFO: Loss too large (198.988->206.830)! Learning rate decreased to 0.07547.
2024-12-01-17:51:22-root-INFO: Loss too large (198.988->202.274)! Learning rate decreased to 0.06038.
2024-12-01-17:51:22-root-INFO: Loss too large (198.988->199.430)! Learning rate decreased to 0.04830.
2024-12-01-17:51:23-root-INFO: grad norm: 10.682 10.636 0.988
2024-12-01-17:51:25-root-INFO: grad norm: 10.161 10.112 1.001
2024-12-01-17:51:26-root-INFO: grad norm: 9.594 9.551 0.911
2024-12-01-17:51:27-root-INFO: grad norm: 9.252 9.205 0.929
2024-12-01-17:51:28-root-INFO: grad norm: 8.843 8.800 0.876
2024-12-01-17:51:29-root-INFO: grad norm: 8.617 8.571 0.894
2024-12-01-17:51:30-root-INFO: grad norm: 8.356 8.312 0.855
2024-12-01-17:51:30-root-INFO: Loss Change: 198.988 -> 194.250
2024-12-01-17:51:30-root-INFO: Regularization Change: 0.000 -> 1.504
2024-12-01-17:51:30-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-01-17:51:30-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-17:51:31-root-INFO: step: 92 lr_xt 0.09691873
2024-12-01-17:51:31-root-INFO: grad norm: 10.427 10.360 1.187
2024-12-01-17:51:31-root-INFO: Loss too large (194.836->201.525)! Learning rate decreased to 0.07753.
2024-12-01-17:51:32-root-INFO: Loss too large (194.836->197.528)! Learning rate decreased to 0.06203.
2024-12-01-17:51:32-root-INFO: Loss too large (194.836->195.102)! Learning rate decreased to 0.04962.
2024-12-01-17:51:33-root-INFO: grad norm: 9.638 9.591 0.958
2024-12-01-17:51:34-root-INFO: grad norm: 9.149 9.102 0.926
2024-12-01-17:51:35-root-INFO: grad norm: 8.642 8.596 0.885
2024-12-01-17:51:36-root-INFO: grad norm: 8.344 8.299 0.866
2024-12-01-17:51:37-root-INFO: grad norm: 8.011 7.966 0.850
2024-12-01-17:51:38-root-INFO: grad norm: 7.827 7.782 0.839
2024-12-01-17:51:39-root-INFO: grad norm: 7.629 7.583 0.829
2024-12-01-17:51:40-root-INFO: Loss Change: 194.836 -> 190.314
2024-12-01-17:51:40-root-INFO: Regularization Change: 0.000 -> 1.500
2024-12-01-17:51:40-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-01-17:51:40-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-17:51:40-root-INFO: step: 91 lr_xt 0.09955376
2024-12-01-17:51:41-root-INFO: grad norm: 9.885 9.822 1.122
2024-12-01-17:51:41-root-INFO: Loss too large (190.854->197.242)! Learning rate decreased to 0.07964.
2024-12-01-17:51:41-root-INFO: Loss too large (190.854->193.422)! Learning rate decreased to 0.06371.
2024-12-01-17:51:42-root-INFO: Loss too large (190.854->191.137)! Learning rate decreased to 0.05097.
2024-12-01-17:51:43-root-INFO: grad norm: 9.236 9.192 0.899
2024-12-01-17:51:44-root-INFO: grad norm: 8.797 8.751 0.892
2024-12-01-17:51:45-root-INFO: grad norm: 8.343 8.301 0.839
2024-12-01-17:51:46-root-INFO: grad norm: 8.067 8.023 0.842
2024-12-01-17:51:47-root-INFO: grad norm: 7.746 7.704 0.811
2024-12-01-17:51:48-root-INFO: grad norm: 7.567 7.522 0.816
2024-12-01-17:51:49-root-INFO: grad norm: 7.364 7.321 0.793
2024-12-01-17:51:50-root-INFO: Loss Change: 190.854 -> 186.560
2024-12-01-17:51:50-root-INFO: Regularization Change: 0.000 -> 1.490
2024-12-01-17:51:50-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-01-17:51:50-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-17:51:50-root-INFO: step: 90 lr_xt 0.10224402
2024-12-01-17:51:51-root-INFO: grad norm: 9.433 9.367 1.118
2024-12-01-17:51:51-root-INFO: Loss too large (187.216->193.088)! Learning rate decreased to 0.08180.
2024-12-01-17:51:51-root-INFO: Loss too large (187.216->189.503)! Learning rate decreased to 0.06544.
2024-12-01-17:51:52-root-INFO: Loss too large (187.216->187.391)! Learning rate decreased to 0.05235.
2024-12-01-17:51:53-root-INFO: grad norm: 8.736 8.695 0.849
2024-12-01-17:51:54-root-INFO: grad norm: 8.296 8.251 0.866
2024-12-01-17:51:55-root-INFO: grad norm: 7.839 7.799 0.797
2024-12-01-17:51:56-root-INFO: grad norm: 7.568 7.524 0.813
2024-12-01-17:51:57-root-INFO: grad norm: 7.251 7.209 0.774
2024-12-01-17:51:58-root-INFO: grad norm: 7.075 7.031 0.788
2024-12-01-17:51:59-root-INFO: grad norm: 6.877 6.835 0.759
2024-12-01-17:51:59-root-INFO: Loss Change: 187.216 -> 182.971
2024-12-01-17:51:59-root-INFO: Regularization Change: 0.000 -> 1.510
2024-12-01-17:51:59-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-01-17:51:59-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-17:52:00-root-INFO: step: 89 lr_xt 0.10499012
2024-12-01-17:52:00-root-INFO: grad norm: 9.866 9.791 1.213
2024-12-01-17:52:00-root-INFO: Loss too large (183.560->190.112)! Learning rate decreased to 0.08399.
2024-12-01-17:52:01-root-INFO: Loss too large (183.560->186.088)! Learning rate decreased to 0.06719.
2024-12-01-17:52:01-root-INFO: Loss too large (183.560->183.735)! Learning rate decreased to 0.05375.
2024-12-01-17:52:02-root-INFO: grad norm: 8.923 8.879 0.879
2024-12-01-17:52:03-root-INFO: grad norm: 8.348 8.304 0.859
2024-12-01-17:52:04-root-INFO: grad norm: 7.780 7.739 0.794
2024-12-01-17:52:05-root-INFO: grad norm: 7.447 7.405 0.791
2024-12-01-17:52:06-root-INFO: grad norm: 7.061 7.019 0.760
2024-12-01-17:52:07-root-INFO: grad norm: 6.851 6.808 0.761
2024-12-01-17:52:08-root-INFO: grad norm: 6.610 6.569 0.741
2024-12-01-17:52:09-root-INFO: Loss Change: 183.560 -> 179.118
2024-12-01-17:52:09-root-INFO: Regularization Change: 0.000 -> 1.551
2024-12-01-17:52:09-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-01-17:52:09-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-17:52:09-root-INFO: step: 88 lr_xt 0.10779268
2024-12-01-17:52:10-root-INFO: grad norm: 8.410 8.344 1.048
2024-12-01-17:52:10-root-INFO: Loss too large (179.803->184.609)! Learning rate decreased to 0.08623.
2024-12-01-17:52:10-root-INFO: Loss too large (179.803->181.563)! Learning rate decreased to 0.06899.
2024-12-01-17:52:11-root-INFO: Loss too large (179.803->179.822)! Learning rate decreased to 0.05519.
2024-12-01-17:52:12-root-INFO: grad norm: 7.722 7.682 0.788
2024-12-01-17:52:13-root-INFO: grad norm: 7.320 7.276 0.802
2024-12-01-17:52:14-root-INFO: grad norm: 6.889 6.849 0.741
2024-12-01-17:52:15-root-INFO: grad norm: 6.639 6.596 0.756
2024-12-01-17:52:16-root-INFO: grad norm: 6.343 6.302 0.720
2024-12-01-17:52:17-root-INFO: grad norm: 6.181 6.137 0.733
2024-12-01-17:52:18-root-INFO: grad norm: 5.994 5.952 0.707
2024-12-01-17:52:19-root-INFO: Loss Change: 179.803 -> 175.753
2024-12-01-17:52:19-root-INFO: Regularization Change: 0.000 -> 1.536
2024-12-01-17:52:19-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-01-17:52:19-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-17:52:19-root-INFO: step: 87 lr_xt 0.11065228
2024-12-01-17:52:19-root-INFO: grad norm: 8.436 8.368 1.068
2024-12-01-17:52:20-root-INFO: Loss too large (176.096->180.846)! Learning rate decreased to 0.08852.
2024-12-01-17:52:20-root-INFO: Loss too large (176.096->177.760)! Learning rate decreased to 0.07082.
2024-12-01-17:52:21-root-INFO: grad norm: 11.523 11.485 0.942
2024-12-01-17:52:22-root-INFO: Loss too large (176.034->177.371)! Learning rate decreased to 0.05665.
2024-12-01-17:52:23-root-INFO: grad norm: 9.866 9.825 0.898
2024-12-01-17:52:24-root-INFO: grad norm: 7.711 7.672 0.768
2024-12-01-17:52:25-root-INFO: grad norm: 6.929 6.889 0.749
2024-12-01-17:52:26-root-INFO: grad norm: 6.097 6.056 0.703
2024-12-01-17:52:27-root-INFO: grad norm: 5.696 5.653 0.698
2024-12-01-17:52:28-root-INFO: grad norm: 5.281 5.238 0.675
2024-12-01-17:52:28-root-INFO: Loss Change: 176.096 -> 171.814
2024-12-01-17:52:28-root-INFO: Regularization Change: 0.000 -> 1.663
2024-12-01-17:52:28-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-01-17:52:28-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-17:52:29-root-INFO: step: 86 lr_xt 0.11356952
2024-12-01-17:52:29-root-INFO: grad norm: 7.330 7.264 0.984
2024-12-01-17:52:29-root-INFO: Loss too large (172.313->175.641)! Learning rate decreased to 0.09086.
2024-12-01-17:52:30-root-INFO: Loss too large (172.313->173.348)! Learning rate decreased to 0.07268.
2024-12-01-17:52:31-root-INFO: grad norm: 9.693 9.655 0.858
2024-12-01-17:52:31-root-INFO: Loss too large (172.108->172.883)! Learning rate decreased to 0.05815.
2024-12-01-17:52:32-root-INFO: grad norm: 8.377 8.336 0.822
2024-12-01-17:52:33-root-INFO: grad norm: 6.820 6.782 0.720
2024-12-01-17:52:34-root-INFO: grad norm: 6.170 6.129 0.712
2024-12-01-17:52:35-root-INFO: grad norm: 5.473 5.432 0.672
2024-12-01-17:52:36-root-INFO: grad norm: 5.126 5.082 0.671
2024-12-01-17:52:37-root-INFO: grad norm: 4.766 4.721 0.650
2024-12-01-17:52:38-root-INFO: Loss Change: 172.313 -> 168.315
2024-12-01-17:52:38-root-INFO: Regularization Change: 0.000 -> 1.659
2024-12-01-17:52:38-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-01-17:52:38-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-17:52:38-root-INFO: step: 85 lr_xt 0.11654496
2024-12-01-17:52:39-root-INFO: grad norm: 6.587 6.523 0.919
2024-12-01-17:52:39-root-INFO: Loss too large (168.602->171.105)! Learning rate decreased to 0.09324.
2024-12-01-17:52:39-root-INFO: Loss too large (168.602->169.284)! Learning rate decreased to 0.07459.
2024-12-01-17:52:41-root-INFO: grad norm: 8.637 8.600 0.801
2024-12-01-17:52:41-root-INFO: Loss too large (168.320->168.867)! Learning rate decreased to 0.05967.
2024-12-01-17:52:42-root-INFO: grad norm: 7.573 7.534 0.766
2024-12-01-17:52:43-root-INFO: grad norm: 6.332 6.295 0.688
2024-12-01-17:52:44-root-INFO: grad norm: 5.782 5.742 0.678
2024-12-01-17:52:45-root-INFO: grad norm: 5.179 5.138 0.648
2024-12-01-17:52:46-root-INFO: grad norm: 4.874 4.831 0.644
2024-12-01-17:52:47-root-INFO: grad norm: 4.550 4.507 0.629
2024-12-01-17:52:48-root-INFO: Loss Change: 168.602 -> 164.784
2024-12-01-17:52:48-root-INFO: Regularization Change: 0.000 -> 1.677
2024-12-01-17:52:48-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-01-17:52:48-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-17:52:48-root-INFO: step: 84 lr_xt 0.11957917
2024-12-01-17:52:49-root-INFO: grad norm: 6.006 5.948 0.839
2024-12-01-17:52:49-root-INFO: Loss too large (164.985->167.048)! Learning rate decreased to 0.09566.
2024-12-01-17:52:49-root-INFO: Loss too large (164.985->165.510)! Learning rate decreased to 0.07653.
2024-12-01-17:52:50-root-INFO: grad norm: 7.940 7.903 0.757
2024-12-01-17:52:51-root-INFO: Loss too large (164.705->165.130)! Learning rate decreased to 0.06122.
2024-12-01-17:52:52-root-INFO: grad norm: 7.005 6.968 0.724
2024-12-01-17:52:53-root-INFO: grad norm: 5.880 5.843 0.663
2024-12-01-17:52:54-root-INFO: grad norm: 5.379 5.340 0.651
2024-12-01-17:52:55-root-INFO: grad norm: 4.823 4.782 0.628
2024-12-01-17:52:56-root-INFO: grad norm: 4.539 4.496 0.622
2024-12-01-17:52:57-root-INFO: grad norm: 4.237 4.193 0.610
2024-12-01-17:52:57-root-INFO: Loss Change: 164.985 -> 161.333
2024-12-01-17:52:57-root-INFO: Regularization Change: 0.000 -> 1.675
2024-12-01-17:52:58-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-01-17:52:58-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-17:52:58-root-INFO: step: 83 lr_xt 0.12267269
2024-12-01-17:52:58-root-INFO: grad norm: 6.207 6.144 0.879
2024-12-01-17:52:59-root-INFO: Loss too large (161.700->163.866)! Learning rate decreased to 0.09814.
2024-12-01-17:52:59-root-INFO: Loss too large (161.700->162.225)! Learning rate decreased to 0.07851.
2024-12-01-17:53:00-root-INFO: grad norm: 7.893 7.857 0.754
2024-12-01-17:53:00-root-INFO: Loss too large (161.369->161.747)! Learning rate decreased to 0.06281.
2024-12-01-17:53:01-root-INFO: grad norm: 6.826 6.789 0.713
2024-12-01-17:53:02-root-INFO: grad norm: 5.632 5.595 0.644
2024-12-01-17:53:03-root-INFO: grad norm: 5.102 5.063 0.628
2024-12-01-17:53:04-root-INFO: grad norm: 4.532 4.492 0.606
2024-12-01-17:53:06-root-INFO: grad norm: 4.241 4.199 0.598
2024-12-01-17:53:07-root-INFO: grad norm: 3.936 3.892 0.588
2024-12-01-17:53:07-root-INFO: Loss Change: 161.700 -> 157.972
2024-12-01-17:53:07-root-INFO: Regularization Change: 0.000 -> 1.707
2024-12-01-17:53:07-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-01-17:53:07-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-17:53:08-root-INFO: step: 82 lr_xt 0.12582604
2024-12-01-17:53:08-root-INFO: grad norm: 5.312 5.250 0.811
2024-12-01-17:53:08-root-INFO: Loss too large (158.273->159.545)! Learning rate decreased to 0.10066.
2024-12-01-17:53:09-root-INFO: Loss too large (158.273->158.450)! Learning rate decreased to 0.08053.
2024-12-01-17:53:10-root-INFO: grad norm: 6.601 6.564 0.697
2024-12-01-17:53:10-root-INFO: Loss too large (157.893->157.996)! Learning rate decreased to 0.06442.
2024-12-01-17:53:11-root-INFO: grad norm: 5.770 5.732 0.657
2024-12-01-17:53:12-root-INFO: grad norm: 4.896 4.858 0.612
2024-12-01-17:53:13-root-INFO: grad norm: 4.476 4.436 0.601
2024-12-01-17:53:14-root-INFO: grad norm: 4.025 3.982 0.583
2024-12-01-17:53:15-root-INFO: grad norm: 3.781 3.737 0.578
2024-12-01-17:53:16-root-INFO: grad norm: 3.528 3.482 0.568
2024-12-01-17:53:17-root-INFO: Loss Change: 158.273 -> 154.763
2024-12-01-17:53:17-root-INFO: Regularization Change: 0.000 -> 1.703
2024-12-01-17:53:17-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-01-17:53:17-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-17:53:17-root-INFO: step: 81 lr_xt 0.12903975
2024-12-01-17:53:18-root-INFO: grad norm: 5.349 5.282 0.846
2024-12-01-17:53:18-root-INFO: Loss too large (154.950->156.186)! Learning rate decreased to 0.10323.
2024-12-01-17:53:18-root-INFO: Loss too large (154.950->155.089)! Learning rate decreased to 0.08259.
2024-12-01-17:53:19-root-INFO: grad norm: 6.418 6.384 0.666
2024-12-01-17:53:20-root-INFO: Loss too large (154.533->154.576)! Learning rate decreased to 0.06607.
2024-12-01-17:53:21-root-INFO: grad norm: 5.537 5.499 0.649
2024-12-01-17:53:22-root-INFO: grad norm: 4.713 4.677 0.580
2024-12-01-17:53:23-root-INFO: grad norm: 4.300 4.261 0.581
2024-12-01-17:53:24-root-INFO: grad norm: 3.870 3.830 0.555
2024-12-01-17:53:25-root-INFO: grad norm: 3.636 3.593 0.557
2024-12-01-17:53:26-root-INFO: grad norm: 3.393 3.349 0.543
2024-12-01-17:53:27-root-INFO: Loss Change: 154.950 -> 151.436
2024-12-01-17:53:27-root-INFO: Regularization Change: 0.000 -> 1.731
2024-12-01-17:53:27-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-01-17:53:27-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-17:53:27-root-INFO: step: 80 lr_xt 0.13231432
2024-12-01-17:53:27-root-INFO: grad norm: 5.633 5.569 0.848
2024-12-01-17:53:28-root-INFO: Loss too large (151.925->153.497)! Learning rate decreased to 0.10585.
2024-12-01-17:53:28-root-INFO: Loss too large (151.925->152.185)! Learning rate decreased to 0.08468.
2024-12-01-17:53:29-root-INFO: grad norm: 6.710 6.674 0.689
2024-12-01-17:53:29-root-INFO: Loss too large (151.513->151.599)! Learning rate decreased to 0.06774.
2024-12-01-17:53:30-root-INFO: grad norm: 5.634 5.598 0.636
2024-12-01-17:53:31-root-INFO: grad norm: 4.604 4.569 0.570
2024-12-01-17:53:33-root-INFO: grad norm: 4.131 4.093 0.559
2024-12-01-17:53:34-root-INFO: grad norm: 3.655 3.615 0.538
2024-12-01-17:53:35-root-INFO: grad norm: 3.404 3.362 0.535
2024-12-01-17:53:36-root-INFO: grad norm: 3.155 3.111 0.525
2024-12-01-17:53:36-root-INFO: Loss Change: 151.925 -> 148.355
2024-12-01-17:53:36-root-INFO: Regularization Change: 0.000 -> 1.752
2024-12-01-17:53:36-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-01-17:53:36-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-17:53:37-root-INFO: step: 79 lr_xt 0.13565022
2024-12-01-17:53:37-root-INFO: grad norm: 4.692 4.630 0.760
2024-12-01-17:53:37-root-INFO: Loss too large (148.510->149.108)! Learning rate decreased to 0.10852.
2024-12-01-17:53:38-root-INFO: grad norm: 7.437 7.400 0.736
2024-12-01-17:53:39-root-INFO: Loss too large (148.377->149.799)! Learning rate decreased to 0.08682.
2024-12-01-17:53:39-root-INFO: Loss too large (148.377->148.544)! Learning rate decreased to 0.06945.
2024-12-01-17:53:40-root-INFO: grad norm: 5.948 5.914 0.642
2024-12-01-17:53:41-root-INFO: grad norm: 4.510 4.475 0.559
2024-12-01-17:53:42-root-INFO: grad norm: 3.915 3.878 0.542
2024-12-01-17:53:43-root-INFO: grad norm: 3.379 3.340 0.518
2024-12-01-17:53:44-root-INFO: grad norm: 3.105 3.062 0.514
2024-12-01-17:53:45-root-INFO: grad norm: 2.861 2.816 0.505
2024-12-01-17:53:46-root-INFO: Loss Change: 148.510 -> 145.024
2024-12-01-17:53:46-root-INFO: Regularization Change: 0.000 -> 1.899
2024-12-01-17:53:46-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-01-17:53:46-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-17:53:46-root-INFO: step: 78 lr_xt 0.13904792
2024-12-01-17:53:47-root-INFO: grad norm: 4.151 4.092 0.694
2024-12-01-17:53:47-root-INFO: Loss too large (145.336->145.683)! Learning rate decreased to 0.11124.
2024-12-01-17:53:48-root-INFO: grad norm: 6.552 6.517 0.675
2024-12-01-17:53:48-root-INFO: Loss too large (145.149->146.175)! Learning rate decreased to 0.08899.
2024-12-01-17:53:49-root-INFO: Loss too large (145.149->145.196)! Learning rate decreased to 0.07119.
2024-12-01-17:53:50-root-INFO: grad norm: 5.323 5.288 0.610
2024-12-01-17:53:51-root-INFO: grad norm: 4.177 4.143 0.536
2024-12-01-17:53:52-root-INFO: grad norm: 3.676 3.638 0.528
2024-12-01-17:53:53-root-INFO: grad norm: 3.215 3.175 0.504
2024-12-01-17:53:54-root-INFO: grad norm: 2.973 2.930 0.503
2024-12-01-17:53:55-root-INFO: grad norm: 2.755 2.711 0.492
2024-12-01-17:53:56-root-INFO: Loss Change: 145.336 -> 142.044
2024-12-01-17:53:56-root-INFO: Regularization Change: 0.000 -> 1.882
2024-12-01-17:53:56-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-01-17:53:56-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-17:53:56-root-INFO: step: 77 lr_xt 0.14250787
2024-12-01-17:53:56-root-INFO: grad norm: 4.132 4.075 0.682
2024-12-01-17:53:57-root-INFO: Loss too large (142.086->142.312)! Learning rate decreased to 0.11401.
2024-12-01-17:53:58-root-INFO: grad norm: 5.981 5.940 0.700
2024-12-01-17:53:58-root-INFO: Loss too large (141.828->142.430)! Learning rate decreased to 0.09121.
2024-12-01-17:53:59-root-INFO: grad norm: 6.400 6.362 0.696
2024-12-01-17:54:00-root-INFO: grad norm: 7.095 7.059 0.708
2024-12-01-17:54:01-root-INFO: grad norm: 7.219 7.183 0.719
2024-12-01-17:54:02-root-INFO: grad norm: 7.192 7.156 0.719
2024-12-01-17:54:03-root-INFO: grad norm: 7.093 7.057 0.717
2024-12-01-17:54:04-root-INFO: grad norm: 6.805 6.768 0.713
2024-12-01-17:54:05-root-INFO: Loss Change: 142.086 -> 139.269
2024-12-01-17:54:05-root-INFO: Regularization Change: 0.000 -> 2.790
2024-12-01-17:54:05-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-01-17:54:05-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-17:54:05-root-INFO: step: 76 lr_xt 0.14603050
2024-12-01-17:54:06-root-INFO: grad norm: 8.875 8.807 1.097
2024-12-01-17:54:06-root-INFO: Loss too large (140.014->144.440)! Learning rate decreased to 0.11682.
2024-12-01-17:54:06-root-INFO: Loss too large (140.014->140.904)! Learning rate decreased to 0.09346.
2024-12-01-17:54:07-root-INFO: grad norm: 7.969 7.923 0.861
2024-12-01-17:54:08-root-INFO: grad norm: 7.180 7.137 0.778
2024-12-01-17:54:09-root-INFO: grad norm: 6.630 6.591 0.722
2024-12-01-17:54:10-root-INFO: grad norm: 6.351 6.312 0.702
2024-12-01-17:54:11-root-INFO: grad norm: 6.039 6.001 0.680
2024-12-01-17:54:13-root-INFO: grad norm: 5.875 5.837 0.665
2024-12-01-17:54:14-root-INFO: grad norm: 5.681 5.643 0.654
2024-12-01-17:54:14-root-INFO: Loss Change: 140.014 -> 135.349
2024-12-01-17:54:14-root-INFO: Regularization Change: 0.000 -> 2.608
2024-12-01-17:54:14-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-01-17:54:14-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-17:54:15-root-INFO: step: 75 lr_xt 0.14961620
2024-12-01-17:54:15-root-INFO: grad norm: 7.416 7.352 0.975
2024-12-01-17:54:15-root-INFO: Loss too large (135.958->138.959)! Learning rate decreased to 0.11969.
2024-12-01-17:54:16-root-INFO: Loss too large (135.958->136.473)! Learning rate decreased to 0.09575.
2024-12-01-17:54:17-root-INFO: grad norm: 6.824 6.780 0.771
2024-12-01-17:54:18-root-INFO: grad norm: 6.352 6.309 0.737
2024-12-01-17:54:19-root-INFO: grad norm: 5.945 5.904 0.693
2024-12-01-17:54:20-root-INFO: grad norm: 5.653 5.614 0.660
2024-12-01-17:54:21-root-INFO: grad norm: 5.370 5.332 0.644
2024-12-01-17:54:22-root-INFO: grad norm: 5.172 5.135 0.615
2024-12-01-17:54:23-root-INFO: grad norm: 4.977 4.939 0.610
2024-12-01-17:54:24-root-INFO: Loss Change: 135.958 -> 131.773
2024-12-01-17:54:24-root-INFO: Regularization Change: 0.000 -> 2.591
2024-12-01-17:54:24-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-01-17:54:24-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-17:54:24-root-INFO: step: 74 lr_xt 0.15326538
2024-12-01-17:54:24-root-INFO: grad norm: 6.566 6.506 0.885
2024-12-01-17:54:25-root-INFO: Loss too large (132.281->134.393)! Learning rate decreased to 0.12261.
2024-12-01-17:54:25-root-INFO: Loss too large (132.281->132.574)! Learning rate decreased to 0.09809.
2024-12-01-17:54:26-root-INFO: grad norm: 5.902 5.858 0.717
2024-12-01-17:54:27-root-INFO: grad norm: 5.345 5.304 0.657
2024-12-01-17:54:28-root-INFO: grad norm: 4.955 4.915 0.624
2024-12-01-17:54:29-root-INFO: grad norm: 4.676 4.639 0.587
2024-12-01-17:54:30-root-INFO: grad norm: 4.425 4.387 0.576
2024-12-01-17:54:31-root-INFO: grad norm: 4.248 4.212 0.549
2024-12-01-17:54:32-root-INFO: grad norm: 4.084 4.047 0.546
2024-12-01-17:54:33-root-INFO: Loss Change: 132.281 -> 128.293
2024-12-01-17:54:33-root-INFO: Regularization Change: 0.000 -> 2.584
2024-12-01-17:54:33-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-01-17:54:33-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-17:54:33-root-INFO: step: 73 lr_xt 0.15697839
2024-12-01-17:54:34-root-INFO: grad norm: 5.899 5.833 0.878
2024-12-01-17:54:34-root-INFO: Loss too large (128.747->130.212)! Learning rate decreased to 0.12558.
2024-12-01-17:54:34-root-INFO: Loss too large (128.747->128.881)! Learning rate decreased to 0.10047.
2024-12-01-17:54:35-root-INFO: grad norm: 5.246 5.205 0.652
2024-12-01-17:54:36-root-INFO: grad norm: 4.696 4.656 0.613
2024-12-01-17:54:37-root-INFO: grad norm: 4.381 4.344 0.571
2024-12-01-17:54:38-root-INFO: grad norm: 4.141 4.105 0.547
2024-12-01-17:54:39-root-INFO: grad norm: 3.942 3.906 0.534
2024-12-01-17:54:40-root-INFO: grad norm: 3.796 3.761 0.514
2024-12-01-17:54:41-root-INFO: grad norm: 3.668 3.632 0.512
2024-12-01-17:54:42-root-INFO: Loss Change: 128.747 -> 124.954
2024-12-01-17:54:42-root-INFO: Regularization Change: 0.000 -> 2.603
2024-12-01-17:54:42-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-01-17:54:42-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-17:54:43-root-INFO: step: 72 lr_xt 0.16075558
2024-12-01-17:54:43-root-INFO: grad norm: 5.281 5.222 0.783
2024-12-01-17:54:43-root-INFO: Loss too large (125.229->126.310)! Learning rate decreased to 0.12860.
2024-12-01-17:54:44-root-INFO: Loss too large (125.229->125.262)! Learning rate decreased to 0.10288.
2024-12-01-17:54:45-root-INFO: grad norm: 4.746 4.706 0.618
2024-12-01-17:54:46-root-INFO: grad norm: 4.311 4.273 0.573
2024-12-01-17:54:47-root-INFO: grad norm: 4.028 3.990 0.548
2024-12-01-17:54:48-root-INFO: grad norm: 3.808 3.774 0.513
2024-12-01-17:54:49-root-INFO: grad norm: 3.637 3.601 0.512
2024-12-01-17:54:50-root-INFO: grad norm: 3.512 3.479 0.483
2024-12-01-17:54:51-root-INFO: grad norm: 3.410 3.374 0.492
2024-12-01-17:54:52-root-INFO: Loss Change: 125.229 -> 121.619
2024-12-01-17:54:52-root-INFO: Regularization Change: 0.000 -> 2.615
2024-12-01-17:54:52-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-01-17:54:52-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-17:54:52-root-INFO: step: 71 lr_xt 0.16459726
2024-12-01-17:54:52-root-INFO: grad norm: 4.577 4.529 0.661
2024-12-01-17:54:53-root-INFO: Loss too large (121.920->122.529)! Learning rate decreased to 0.13168.
2024-12-01-17:54:54-root-INFO: grad norm: 5.446 5.400 0.702
2024-12-01-17:54:55-root-INFO: grad norm: 6.708 6.660 0.805
2024-12-01-17:54:55-root-INFO: Loss too large (121.612->122.027)! Learning rate decreased to 0.10534.
2024-12-01-17:54:56-root-INFO: grad norm: 5.586 5.540 0.718
2024-12-01-17:54:57-root-INFO: grad norm: 4.486 4.450 0.572
2024-12-01-17:54:58-root-INFO: grad norm: 3.987 3.950 0.542
2024-12-01-17:54:59-root-INFO: grad norm: 3.687 3.654 0.492
2024-12-01-17:55:00-root-INFO: grad norm: 3.447 3.411 0.493
2024-12-01-17:55:01-root-INFO: Loss Change: 121.920 -> 118.426
2024-12-01-17:55:01-root-INFO: Regularization Change: 0.000 -> 2.916
2024-12-01-17:55:01-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-01-17:55:01-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-17:55:01-root-INFO: step: 70 lr_xt 0.16850375
2024-12-01-17:55:02-root-INFO: grad norm: 5.537 5.466 0.882
2024-12-01-17:55:02-root-INFO: Loss too large (118.756->119.915)! Learning rate decreased to 0.13480.
2024-12-01-17:55:02-root-INFO: Loss too large (118.756->118.793)! Learning rate decreased to 0.10784.
2024-12-01-17:55:03-root-INFO: grad norm: 4.713 4.672 0.620
2024-12-01-17:55:04-root-INFO: grad norm: 4.068 4.029 0.559
2024-12-01-17:55:05-root-INFO: grad norm: 3.726 3.690 0.519
2024-12-01-17:55:06-root-INFO: grad norm: 3.472 3.438 0.479
2024-12-01-17:55:07-root-INFO: grad norm: 3.308 3.273 0.478
2024-12-01-17:55:08-root-INFO: grad norm: 3.192 3.161 0.445
2024-12-01-17:55:09-root-INFO: grad norm: 3.117 3.083 0.459
2024-12-01-17:55:10-root-INFO: Loss Change: 118.756 -> 115.072
2024-12-01-17:55:10-root-INFO: Regularization Change: 0.000 -> 2.681
2024-12-01-17:55:10-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-01-17:55:10-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-17:55:11-root-INFO: step: 69 lr_xt 0.17247530
2024-12-01-17:55:11-root-INFO: grad norm: 4.296 4.247 0.647
2024-12-01-17:55:11-root-INFO: Loss too large (115.360->115.884)! Learning rate decreased to 0.13798.
2024-12-01-17:55:12-root-INFO: grad norm: 5.031 4.985 0.676
2024-12-01-17:55:13-root-INFO: grad norm: 6.302 6.253 0.782
2024-12-01-17:55:14-root-INFO: Loss too large (114.967->115.402)! Learning rate decreased to 0.11038.
2024-12-01-17:55:15-root-INFO: grad norm: 5.160 5.114 0.693
2024-12-01-17:55:16-root-INFO: grad norm: 3.877 3.841 0.524
2024-12-01-17:55:17-root-INFO: grad norm: 3.473 3.438 0.498
2024-12-01-17:55:18-root-INFO: grad norm: 3.265 3.234 0.448
2024-12-01-17:55:19-root-INFO: grad norm: 3.122 3.088 0.459
2024-12-01-17:55:19-root-INFO: Loss Change: 115.360 -> 111.975
2024-12-01-17:55:19-root-INFO: Regularization Change: 0.000 -> 2.961
2024-12-01-17:55:19-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-01-17:55:19-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-17:55:20-root-INFO: step: 68 lr_xt 0.17651217
2024-12-01-17:55:20-root-INFO: grad norm: 5.349 5.280 0.854
2024-12-01-17:55:20-root-INFO: Loss too large (112.295->113.344)! Learning rate decreased to 0.14121.
2024-12-01-17:55:21-root-INFO: Loss too large (112.295->112.356)! Learning rate decreased to 0.11297.
2024-12-01-17:55:22-root-INFO: grad norm: 4.513 4.469 0.625
2024-12-01-17:55:23-root-INFO: grad norm: 3.800 3.765 0.520
2024-12-01-17:55:24-root-INFO: grad norm: 3.534 3.498 0.506
2024-12-01-17:55:25-root-INFO: grad norm: 3.344 3.314 0.450
2024-12-01-17:55:26-root-INFO: grad norm: 3.237 3.203 0.468
2024-12-01-17:55:27-root-INFO: grad norm: 3.165 3.137 0.424
2024-12-01-17:55:28-root-INFO: grad norm: 3.126 3.094 0.452
2024-12-01-17:55:29-root-INFO: Loss Change: 112.295 -> 108.744
2024-12-01-17:55:29-root-INFO: Regularization Change: 0.000 -> 2.736
2024-12-01-17:55:29-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-01-17:55:29-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-17:55:29-root-INFO: step: 67 lr_xt 0.18061458
2024-12-01-17:55:29-root-INFO: grad norm: 4.293 4.243 0.656
2024-12-01-17:55:30-root-INFO: Loss too large (108.864->109.458)! Learning rate decreased to 0.14449.
2024-12-01-17:55:30-root-INFO: Loss too large (108.864->108.865)! Learning rate decreased to 0.11559.
2024-12-01-17:55:31-root-INFO: grad norm: 3.923 3.886 0.542
2024-12-01-17:55:32-root-INFO: grad norm: 3.624 3.592 0.481
2024-12-01-17:55:33-root-INFO: grad norm: 3.506 3.471 0.492
2024-12-01-17:55:34-root-INFO: grad norm: 3.406 3.377 0.442
2024-12-01-17:55:35-root-INFO: grad norm: 3.358 3.325 0.472
2024-12-01-17:55:36-root-INFO: grad norm: 3.327 3.300 0.426
2024-12-01-17:55:37-root-INFO: grad norm: 3.312 3.279 0.464
2024-12-01-17:55:38-root-INFO: Loss Change: 108.864 -> 105.721
2024-12-01-17:55:38-root-INFO: Regularization Change: 0.000 -> 2.714
2024-12-01-17:55:38-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-01-17:55:38-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-17:55:38-root-INFO: step: 66 lr_xt 0.18478272
2024-12-01-17:55:39-root-INFO: grad norm: 4.523 4.478 0.637
2024-12-01-17:55:39-root-INFO: Loss too large (105.979->106.723)! Learning rate decreased to 0.14783.
2024-12-01-17:55:39-root-INFO: Loss too large (105.979->106.097)! Learning rate decreased to 0.11826.
2024-12-01-17:55:40-root-INFO: grad norm: 4.029 3.990 0.554
2024-12-01-17:55:41-root-INFO: grad norm: 3.442 3.412 0.456
2024-12-01-17:55:42-root-INFO: grad norm: 3.352 3.318 0.475
2024-12-01-17:55:44-root-INFO: grad norm: 3.316 3.289 0.422
2024-12-01-17:55:45-root-INFO: grad norm: 3.294 3.262 0.460
2024-12-01-17:55:46-root-INFO: grad norm: 3.300 3.274 0.413
2024-12-01-17:55:47-root-INFO: grad norm: 3.299 3.267 0.457
2024-12-01-17:55:47-root-INFO: Loss Change: 105.979 -> 102.850
2024-12-01-17:55:47-root-INFO: Regularization Change: 0.000 -> 2.726
2024-12-01-17:55:47-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-01-17:55:47-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-17:55:48-root-INFO: step: 65 lr_xt 0.18901677
2024-12-01-17:55:48-root-INFO: grad norm: 4.933 4.876 0.746
2024-12-01-17:55:48-root-INFO: Loss too large (103.147->104.156)! Learning rate decreased to 0.15121.
2024-12-01-17:55:49-root-INFO: Loss too large (103.147->103.340)! Learning rate decreased to 0.12097.
2024-12-01-17:55:50-root-INFO: grad norm: 4.306 4.265 0.594
2024-12-01-17:55:51-root-INFO: grad norm: 3.634 3.603 0.476
2024-12-01-17:55:52-root-INFO: grad norm: 3.508 3.474 0.491
2024-12-01-17:55:53-root-INFO: grad norm: 3.453 3.427 0.429
2024-12-01-17:55:54-root-INFO: grad norm: 3.413 3.380 0.469
2024-12-01-17:55:55-root-INFO: grad norm: 3.396 3.371 0.415
2024-12-01-17:55:56-root-INFO: grad norm: 3.386 3.354 0.461
2024-12-01-17:55:57-root-INFO: Loss Change: 103.147 -> 99.897
2024-12-01-17:55:57-root-INFO: Regularization Change: 0.000 -> 2.801
2024-12-01-17:55:57-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-01-17:55:57-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-17:55:57-root-INFO: step: 64 lr_xt 0.19331686
2024-12-01-17:55:57-root-INFO: grad norm: 4.494 4.449 0.631
2024-12-01-17:55:58-root-INFO: Loss too large (100.070->100.965)! Learning rate decreased to 0.15465.
2024-12-01-17:55:58-root-INFO: Loss too large (100.070->100.267)! Learning rate decreased to 0.12372.
2024-12-01-17:55:59-root-INFO: grad norm: 4.117 4.079 0.560
2024-12-01-17:56:00-root-INFO: grad norm: 3.707 3.677 0.471
2024-12-01-17:56:01-root-INFO: grad norm: 3.627 3.593 0.494
2024-12-01-17:56:02-root-INFO: grad norm: 3.584 3.557 0.436
2024-12-01-17:56:03-root-INFO: grad norm: 3.553 3.520 0.478
2024-12-01-17:56:04-root-INFO: grad norm: 3.533 3.507 0.424
2024-12-01-17:56:05-root-INFO: grad norm: 3.523 3.492 0.471
2024-12-01-17:56:06-root-INFO: Loss Change: 100.070 -> 97.015
2024-12-01-17:56:06-root-INFO: Regularization Change: 0.000 -> 2.811
2024-12-01-17:56:06-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-01-17:56:06-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-17:56:06-root-INFO: step: 63 lr_xt 0.19768311
2024-12-01-17:56:07-root-INFO: grad norm: 4.673 4.620 0.700
2024-12-01-17:56:07-root-INFO: Loss too large (97.341->98.326)! Learning rate decreased to 0.15815.
2024-12-01-17:56:07-root-INFO: Loss too large (97.341->97.552)! Learning rate decreased to 0.12652.
2024-12-01-17:56:08-root-INFO: grad norm: 4.179 4.141 0.561
2024-12-01-17:56:10-root-INFO: grad norm: 3.665 3.634 0.476
2024-12-01-17:56:11-root-INFO: grad norm: 3.572 3.540 0.483
2024-12-01-17:56:12-root-INFO: grad norm: 3.535 3.509 0.432
2024-12-01-17:56:13-root-INFO: grad norm: 3.503 3.472 0.467
2024-12-01-17:56:14-root-INFO: grad norm: 3.489 3.464 0.418
2024-12-01-17:56:15-root-INFO: grad norm: 3.480 3.450 0.461
2024-12-01-17:56:15-root-INFO: Loss Change: 97.341 -> 94.234
2024-12-01-17:56:15-root-INFO: Regularization Change: 0.000 -> 2.857
2024-12-01-17:56:15-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-01-17:56:15-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-17:56:16-root-INFO: step: 62 lr_xt 0.20211560
2024-12-01-17:56:16-root-INFO: grad norm: 5.152 5.093 0.778
2024-12-01-17:56:16-root-INFO: Loss too large (94.382->95.712)! Learning rate decreased to 0.16169.
2024-12-01-17:56:17-root-INFO: Loss too large (94.382->94.755)! Learning rate decreased to 0.12935.
2024-12-01-17:56:18-root-INFO: grad norm: 4.487 4.444 0.621
2024-12-01-17:56:19-root-INFO: grad norm: 3.707 3.674 0.490
2024-12-01-17:56:20-root-INFO: grad norm: 3.622 3.588 0.495
2024-12-01-17:56:21-root-INFO: grad norm: 3.653 3.626 0.442
2024-12-01-17:56:22-root-INFO: grad norm: 3.623 3.591 0.479
2024-12-01-17:56:23-root-INFO: grad norm: 3.611 3.585 0.429
2024-12-01-17:56:24-root-INFO: grad norm: 3.600 3.570 0.471
2024-12-01-17:56:25-root-INFO: Loss Change: 94.382 -> 91.160
2024-12-01-17:56:25-root-INFO: Regularization Change: 0.000 -> 2.928
2024-12-01-17:56:25-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-01-17:56:25-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-17:56:25-root-INFO: step: 61 lr_xt 0.20661437
2024-12-01-17:56:25-root-INFO: grad norm: 4.474 4.433 0.610
2024-12-01-17:56:26-root-INFO: Loss too large (91.352->92.330)! Learning rate decreased to 0.16529.
2024-12-01-17:56:26-root-INFO: Loss too large (91.352->91.606)! Learning rate decreased to 0.13223.
2024-12-01-17:56:27-root-INFO: grad norm: 4.058 4.022 0.539
2024-12-01-17:56:28-root-INFO: grad norm: 3.552 3.523 0.452
2024-12-01-17:56:29-root-INFO: grad norm: 3.491 3.460 0.464
2024-12-01-17:56:30-root-INFO: grad norm: 3.484 3.458 0.421
2024-12-01-17:56:31-root-INFO: grad norm: 3.472 3.442 0.453
2024-12-01-17:56:32-root-INFO: grad norm: 3.485 3.460 0.414
2024-12-01-17:56:33-root-INFO: grad norm: 3.487 3.457 0.452
2024-12-01-17:56:34-root-INFO: Loss Change: 91.352 -> 88.393
2024-12-01-17:56:34-root-INFO: Regularization Change: 0.000 -> 2.891
2024-12-01-17:56:34-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-01-17:56:34-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-17:56:34-root-INFO: step: 60 lr_xt 0.21117946
2024-12-01-17:56:35-root-INFO: grad norm: 4.780 4.727 0.710
2024-12-01-17:56:35-root-INFO: Loss too large (88.680->89.930)! Learning rate decreased to 0.16894.
2024-12-01-17:56:35-root-INFO: Loss too large (88.680->89.046)! Learning rate decreased to 0.13515.
2024-12-01-17:56:36-root-INFO: grad norm: 4.284 4.246 0.570
2024-12-01-17:56:38-root-INFO: grad norm: 3.727 3.696 0.478
2024-12-01-17:56:39-root-INFO: grad norm: 3.667 3.635 0.484
2024-12-01-17:56:40-root-INFO: grad norm: 3.685 3.658 0.438
2024-12-01-17:56:41-root-INFO: grad norm: 3.658 3.627 0.472
2024-12-01-17:56:42-root-INFO: grad norm: 3.642 3.617 0.427
2024-12-01-17:56:43-root-INFO: grad norm: 3.634 3.604 0.466
2024-12-01-17:56:43-root-INFO: Loss Change: 88.680 -> 85.718
2024-12-01-17:56:43-root-INFO: Regularization Change: 0.000 -> 2.887
2024-12-01-17:56:43-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-01-17:56:43-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-17:56:44-root-INFO: step: 59 lr_xt 0.21581084
2024-12-01-17:56:44-root-INFO: grad norm: 4.939 4.886 0.722
2024-12-01-17:56:44-root-INFO: Loss too large (85.999->87.401)! Learning rate decreased to 0.17265.
2024-12-01-17:56:45-root-INFO: Loss too large (85.999->86.463)! Learning rate decreased to 0.13812.
2024-12-01-17:56:46-root-INFO: grad norm: 4.361 4.323 0.576
2024-12-01-17:56:47-root-INFO: grad norm: 3.658 3.627 0.475
2024-12-01-17:56:48-root-INFO: grad norm: 3.628 3.597 0.474
2024-12-01-17:56:49-root-INFO: grad norm: 3.714 3.688 0.438
2024-12-01-17:56:50-root-INFO: grad norm: 3.700 3.670 0.471
2024-12-01-17:56:51-root-INFO: grad norm: 3.699 3.674 0.430
2024-12-01-17:56:52-root-INFO: grad norm: 3.696 3.666 0.469
2024-12-01-17:56:53-root-INFO: Loss Change: 85.999 -> 83.093
2024-12-01-17:56:53-root-INFO: Regularization Change: 0.000 -> 2.854
2024-12-01-17:56:53-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-01-17:56:53-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-17:56:53-root-INFO: step: 58 lr_xt 0.22050848
2024-12-01-17:56:53-root-INFO: grad norm: 4.841 4.791 0.690
2024-12-01-17:56:54-root-INFO: Loss too large (83.214->84.690)! Learning rate decreased to 0.17641.
2024-12-01-17:56:54-root-INFO: Loss too large (83.214->83.720)! Learning rate decreased to 0.14113.
2024-12-01-17:56:55-root-INFO: grad norm: 4.371 4.331 0.590
2024-12-01-17:56:56-root-INFO: grad norm: 3.795 3.764 0.486
2024-12-01-17:56:57-root-INFO: grad norm: 3.745 3.712 0.496
2024-12-01-17:56:58-root-INFO: grad norm: 3.771 3.744 0.449
2024-12-01-17:56:59-root-INFO: grad norm: 3.745 3.713 0.485
2024-12-01-17:57:00-root-INFO: grad norm: 3.725 3.699 0.439
2024-12-01-17:57:01-root-INFO: grad norm: 3.721 3.690 0.479
2024-12-01-17:57:02-root-INFO: Loss Change: 83.214 -> 80.422
2024-12-01-17:57:02-root-INFO: Regularization Change: 0.000 -> 2.821
2024-12-01-17:57:02-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-01-17:57:02-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-17:57:02-root-INFO: step: 57 lr_xt 0.22527231
2024-12-01-17:57:03-root-INFO: grad norm: 5.119 5.068 0.722
2024-12-01-17:57:03-root-INFO: Loss too large (80.596->82.277)! Learning rate decreased to 0.18022.
2024-12-01-17:57:03-root-INFO: Loss too large (80.596->81.197)! Learning rate decreased to 0.14417.
2024-12-01-17:57:04-root-INFO: grad norm: 4.489 4.445 0.624
2024-12-01-17:57:05-root-INFO: grad norm: 3.704 3.672 0.487
2024-12-01-17:57:06-root-INFO: grad norm: 3.669 3.635 0.500
2024-12-01-17:57:07-root-INFO: grad norm: 3.755 3.728 0.454
2024-12-01-17:57:09-root-INFO: grad norm: 3.753 3.720 0.494
2024-12-01-17:57:10-root-INFO: grad norm: 3.770 3.743 0.452
2024-12-01-17:57:11-root-INFO: grad norm: 3.771 3.739 0.492
2024-12-01-17:57:11-root-INFO: Loss Change: 80.596 -> 77.777
2024-12-01-17:57:11-root-INFO: Regularization Change: 0.000 -> 2.822
2024-12-01-17:57:11-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-01-17:57:11-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-17:57:12-root-INFO: step: 56 lr_xt 0.23010221
2024-12-01-17:57:12-root-INFO: grad norm: 4.838 4.782 0.736
2024-12-01-17:57:12-root-INFO: Loss too large (78.060->79.687)! Learning rate decreased to 0.18408.
2024-12-01-17:57:13-root-INFO: Loss too large (78.060->78.613)! Learning rate decreased to 0.14727.
2024-12-01-17:57:14-root-INFO: grad norm: 4.418 4.376 0.607
2024-12-01-17:57:15-root-INFO: grad norm: 3.959 3.924 0.531
2024-12-01-17:57:16-root-INFO: grad norm: 3.908 3.872 0.530
2024-12-01-17:57:17-root-INFO: grad norm: 3.908 3.876 0.493
2024-12-01-17:57:18-root-INFO: grad norm: 3.876 3.841 0.515
2024-12-01-17:57:19-root-INFO: grad norm: 3.851 3.821 0.477
2024-12-01-17:57:20-root-INFO: grad norm: 3.836 3.802 0.506
2024-12-01-17:57:21-root-INFO: Loss Change: 78.060 -> 75.381
2024-12-01-17:57:21-root-INFO: Regularization Change: 0.000 -> 2.812
2024-12-01-17:57:21-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-01-17:57:21-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-17:57:21-root-INFO: step: 55 lr_xt 0.23499803
2024-12-01-17:57:21-root-INFO: grad norm: 5.029 4.973 0.747
2024-12-01-17:57:22-root-INFO: Loss too large (75.652->77.464)! Learning rate decreased to 0.18800.
2024-12-01-17:57:22-root-INFO: Loss too large (75.652->76.312)! Learning rate decreased to 0.15040.
2024-12-01-17:57:23-root-INFO: grad norm: 4.505 4.461 0.628
2024-12-01-17:57:24-root-INFO: grad norm: 3.886 3.852 0.516
2024-12-01-17:57:25-root-INFO: grad norm: 3.852 3.816 0.524
2024-12-01-17:57:26-root-INFO: grad norm: 3.909 3.880 0.478
2024-12-01-17:57:27-root-INFO: grad norm: 3.883 3.849 0.512
2024-12-01-17:57:28-root-INFO: grad norm: 3.864 3.836 0.466
2024-12-01-17:57:29-root-INFO: grad norm: 3.855 3.822 0.505
2024-12-01-17:57:30-root-INFO: Loss Change: 75.652 -> 72.970
2024-12-01-17:57:30-root-INFO: Regularization Change: 0.000 -> 2.806
2024-12-01-17:57:30-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-01-17:57:30-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-17:57:30-root-INFO: step: 54 lr_xt 0.23995961
2024-12-01-17:57:31-root-INFO: grad norm: 4.901 4.846 0.732
2024-12-01-17:57:31-root-INFO: Loss too large (73.125->74.895)! Learning rate decreased to 0.19197.
2024-12-01-17:57:31-root-INFO: Loss too large (73.125->73.772)! Learning rate decreased to 0.15357.
2024-12-01-17:57:32-root-INFO: grad norm: 4.457 4.415 0.610
2024-12-01-17:57:33-root-INFO: grad norm: 3.955 3.921 0.522
2024-12-01-17:57:34-root-INFO: grad norm: 3.928 3.893 0.525
2024-12-01-17:57:35-root-INFO: grad norm: 3.965 3.936 0.485
2024-12-01-17:57:36-root-INFO: grad norm: 3.942 3.908 0.517
2024-12-01-17:57:38-root-INFO: grad norm: 3.920 3.891 0.475
2024-12-01-17:57:39-root-INFO: grad norm: 3.914 3.881 0.511
2024-12-01-17:57:39-root-INFO: Loss Change: 73.125 -> 70.526
2024-12-01-17:57:39-root-INFO: Regularization Change: 0.000 -> 2.826
2024-12-01-17:57:39-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-01-17:57:39-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-17:57:40-root-INFO: step: 53 lr_xt 0.24498673
2024-12-01-17:57:40-root-INFO: grad norm: 4.997 4.944 0.723
2024-12-01-17:57:40-root-INFO: Loss too large (70.827->72.769)! Learning rate decreased to 0.19599.
2024-12-01-17:57:41-root-INFO: Loss too large (70.827->71.557)! Learning rate decreased to 0.15679.
2024-12-01-17:57:42-root-INFO: grad norm: 4.535 4.491 0.634
2024-12-01-17:57:43-root-INFO: grad norm: 4.004 3.969 0.525
2024-12-01-17:57:44-root-INFO: grad norm: 3.974 3.937 0.541
2024-12-01-17:57:45-root-INFO: grad norm: 4.004 3.973 0.493
2024-12-01-17:57:46-root-INFO: grad norm: 3.989 3.954 0.531
2024-12-01-17:57:47-root-INFO: grad norm: 3.981 3.951 0.487
2024-12-01-17:57:48-root-INFO: grad norm: 3.976 3.941 0.526
2024-12-01-17:57:49-root-INFO: Loss Change: 70.827 -> 68.273
2024-12-01-17:57:49-root-INFO: Regularization Change: 0.000 -> 2.805
2024-12-01-17:57:49-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-01-17:57:49-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-17:57:49-root-INFO: step: 52 lr_xt 0.25007913
2024-12-01-17:57:49-root-INFO: grad norm: 4.872 4.820 0.711
2024-12-01-17:57:50-root-INFO: Loss too large (68.478->70.397)! Learning rate decreased to 0.20006.
2024-12-01-17:57:50-root-INFO: Loss too large (68.478->69.176)! Learning rate decreased to 0.16005.
2024-12-01-17:57:51-root-INFO: grad norm: 4.478 4.433 0.628
2024-12-01-17:57:52-root-INFO: grad norm: 4.051 4.017 0.518
2024-12-01-17:57:53-root-INFO: grad norm: 3.990 3.953 0.541
2024-12-01-17:57:54-root-INFO: grad norm: 3.962 3.933 0.479
2024-12-01-17:57:55-root-INFO: grad norm: 3.928 3.894 0.520
2024-12-01-17:57:56-root-INFO: grad norm: 3.900 3.873 0.464
2024-12-01-17:57:57-root-INFO: grad norm: 3.885 3.852 0.509
2024-12-01-17:57:58-root-INFO: Loss Change: 68.478 -> 65.961
2024-12-01-17:57:58-root-INFO: Regularization Change: 0.000 -> 2.817
2024-12-01-17:57:58-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-01-17:57:58-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-17:57:58-root-INFO: step: 51 lr_xt 0.25523653
2024-12-01-17:57:59-root-INFO: grad norm: 5.045 4.983 0.784
2024-12-01-17:57:59-root-INFO: Loss too large (66.272->68.346)! Learning rate decreased to 0.20419.
2024-12-01-17:57:59-root-INFO: Loss too large (66.272->67.025)! Learning rate decreased to 0.16335.
2024-12-01-17:58:00-root-INFO: grad norm: 4.575 4.531 0.630
2024-12-01-17:58:01-root-INFO: grad norm: 4.089 4.054 0.533
2024-12-01-17:58:02-root-INFO: grad norm: 4.018 3.982 0.533
2024-12-01-17:58:03-root-INFO: grad norm: 3.977 3.947 0.483
2024-12-01-17:58:04-root-INFO: grad norm: 3.938 3.905 0.511
2024-12-01-17:58:05-root-INFO: grad norm: 3.902 3.874 0.465
2024-12-01-17:58:06-root-INFO: grad norm: 3.885 3.853 0.501
2024-12-01-17:58:07-root-INFO: Loss Change: 66.272 -> 63.704
2024-12-01-17:58:07-root-INFO: Regularization Change: 0.000 -> 2.845
2024-12-01-17:58:07-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-01-17:58:07-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-17:58:08-root-INFO: step: 50 lr_xt 0.26045862
2024-12-01-17:58:08-root-INFO: grad norm: 4.936 4.882 0.730
2024-12-01-17:58:08-root-INFO: Loss too large (63.770->65.771)! Learning rate decreased to 0.20837.
2024-12-01-17:58:09-root-INFO: Loss too large (63.770->64.494)! Learning rate decreased to 0.16669.
2024-12-01-17:58:10-root-INFO: grad norm: 4.488 4.446 0.617
2024-12-01-17:58:11-root-INFO: grad norm: 4.025 3.993 0.507
2024-12-01-17:58:12-root-INFO: grad norm: 3.955 3.920 0.525
2024-12-01-17:58:13-root-INFO: grad norm: 3.909 3.881 0.464
2024-12-01-17:58:14-root-INFO: grad norm: 3.877 3.844 0.503
2024-12-01-17:58:15-root-INFO: grad norm: 3.849 3.823 0.451
2024-12-01-17:58:16-root-INFO: grad norm: 3.835 3.803 0.494
2024-12-01-17:58:16-root-INFO: Loss Change: 63.770 -> 61.252
2024-12-01-17:58:16-root-INFO: Regularization Change: 0.000 -> 2.861
2024-12-01-17:58:16-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-01-17:58:16-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-17:58:17-root-INFO: step: 49 lr_xt 0.26574501
2024-12-01-17:58:17-root-INFO: grad norm: 5.006 4.947 0.764
2024-12-01-17:58:17-root-INFO: Loss too large (61.626->63.615)! Learning rate decreased to 0.21260.
2024-12-01-17:58:18-root-INFO: Loss too large (61.626->62.286)! Learning rate decreased to 0.17008.
2024-12-01-17:58:19-root-INFO: grad norm: 4.457 4.414 0.622
2024-12-01-17:58:20-root-INFO: grad norm: 3.949 3.917 0.507
2024-12-01-17:58:21-root-INFO: grad norm: 3.846 3.812 0.512
2024-12-01-17:58:22-root-INFO: grad norm: 3.766 3.738 0.457
2024-12-01-17:58:23-root-INFO: grad norm: 3.736 3.705 0.484
2024-12-01-17:58:24-root-INFO: grad norm: 3.718 3.691 0.442
2024-12-01-17:58:25-root-INFO: grad norm: 3.711 3.680 0.476
2024-12-01-17:58:26-root-INFO: Loss Change: 61.626 -> 59.039
2024-12-01-17:58:26-root-INFO: Regularization Change: 0.000 -> 2.889
2024-12-01-17:58:26-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-01-17:58:26-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-17:58:26-root-INFO: step: 48 lr_xt 0.27109532
2024-12-01-17:58:26-root-INFO: grad norm: 4.409 4.364 0.634
2024-12-01-17:58:27-root-INFO: Loss too large (59.174->60.843)! Learning rate decreased to 0.21688.
2024-12-01-17:58:27-root-INFO: Loss too large (59.174->59.718)! Learning rate decreased to 0.17350.
2024-12-01-17:58:28-root-INFO: grad norm: 4.134 4.095 0.562
2024-12-01-17:58:29-root-INFO: grad norm: 3.904 3.877 0.464
2024-12-01-17:58:30-root-INFO: grad norm: 3.826 3.794 0.501
2024-12-01-17:58:31-root-INFO: grad norm: 3.750 3.725 0.430
2024-12-01-17:58:32-root-INFO: grad norm: 3.717 3.687 0.478
2024-12-01-17:58:33-root-INFO: grad norm: 3.687 3.663 0.419
2024-12-01-17:58:34-root-INFO: grad norm: 3.674 3.644 0.468
2024-12-01-17:58:35-root-INFO: Loss Change: 59.174 -> 56.866
2024-12-01-17:58:35-root-INFO: Regularization Change: 0.000 -> 2.858
2024-12-01-17:58:35-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-01-17:58:35-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-17:58:35-root-INFO: step: 47 lr_xt 0.27650911
2024-12-01-17:58:36-root-INFO: grad norm: 4.723 4.666 0.737
2024-12-01-17:58:36-root-INFO: Loss too large (57.176->59.090)! Learning rate decreased to 0.22121.
2024-12-01-17:58:36-root-INFO: Loss too large (57.176->57.795)! Learning rate decreased to 0.17697.
2024-12-01-17:58:37-root-INFO: grad norm: 4.319 4.279 0.589
2024-12-01-17:58:39-root-INFO: grad norm: 3.981 3.951 0.492
2024-12-01-17:58:40-root-INFO: grad norm: 3.864 3.831 0.504
2024-12-01-17:58:41-root-INFO: grad norm: 3.749 3.723 0.440
2024-12-01-17:58:42-root-INFO: grad norm: 3.700 3.669 0.472
2024-12-01-17:58:43-root-INFO: grad norm: 3.653 3.629 0.420
2024-12-01-17:58:44-root-INFO: grad norm: 3.633 3.604 0.459
2024-12-01-17:58:44-root-INFO: Loss Change: 57.176 -> 54.745
2024-12-01-17:58:44-root-INFO: Regularization Change: 0.000 -> 2.884
2024-12-01-17:58:44-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-01-17:58:44-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-17:58:45-root-INFO: step: 46 lr_xt 0.28198590
2024-12-01-17:58:45-root-INFO: grad norm: 4.532 4.481 0.679
2024-12-01-17:58:45-root-INFO: Loss too large (54.979->56.759)! Learning rate decreased to 0.22559.
2024-12-01-17:58:46-root-INFO: Loss too large (54.979->55.521)! Learning rate decreased to 0.18047.
2024-12-01-17:58:47-root-INFO: grad norm: 4.146 4.107 0.563
2024-12-01-17:58:48-root-INFO: grad norm: 3.828 3.801 0.457
2024-12-01-17:58:49-root-INFO: grad norm: 3.696 3.664 0.481
2024-12-01-17:58:50-root-INFO: grad norm: 3.575 3.551 0.411
2024-12-01-17:58:51-root-INFO: grad norm: 3.522 3.493 0.449
2024-12-01-17:58:52-root-INFO: grad norm: 3.478 3.456 0.393
2024-12-01-17:58:53-root-INFO: grad norm: 3.462 3.434 0.437
2024-12-01-17:58:54-root-INFO: Loss Change: 54.979 -> 52.603
2024-12-01-17:58:54-root-INFO: Regularization Change: 0.000 -> 2.874
2024-12-01-17:58:54-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-01-17:58:54-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-17:58:54-root-INFO: step: 45 lr_xt 0.28752516
2024-12-01-17:58:54-root-INFO: grad norm: 4.386 4.335 0.672
2024-12-01-17:58:55-root-INFO: Loss too large (52.755->54.456)! Learning rate decreased to 0.23002.
2024-12-01-17:58:55-root-INFO: Loss too large (52.755->53.244)! Learning rate decreased to 0.18402.
2024-12-01-17:58:56-root-INFO: grad norm: 4.033 3.996 0.547
2024-12-01-17:58:57-root-INFO: grad norm: 3.756 3.729 0.447
2024-12-01-17:58:58-root-INFO: grad norm: 3.618 3.588 0.468
2024-12-01-17:58:59-root-INFO: grad norm: 3.494 3.471 0.400
2024-12-01-17:59:00-root-INFO: grad norm: 3.434 3.406 0.436
2024-12-01-17:59:01-root-INFO: grad norm: 3.385 3.364 0.382
2024-12-01-17:59:02-root-INFO: grad norm: 3.365 3.339 0.423
2024-12-01-17:59:03-root-INFO: Loss Change: 52.755 -> 50.425
2024-12-01-17:59:03-root-INFO: Regularization Change: 0.000 -> 2.889
2024-12-01-17:59:03-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-01-17:59:03-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-17:59:03-root-INFO: step: 44 lr_xt 0.29312635
2024-12-01-17:59:04-root-INFO: grad norm: 4.244 4.195 0.647
2024-12-01-17:59:04-root-INFO: Loss too large (50.702->52.346)! Learning rate decreased to 0.23450.
2024-12-01-17:59:04-root-INFO: Loss too large (50.702->51.161)! Learning rate decreased to 0.18760.
2024-12-01-17:59:05-root-INFO: grad norm: 3.935 3.900 0.521
2024-12-01-17:59:06-root-INFO: grad norm: 3.697 3.672 0.433
2024-12-01-17:59:07-root-INFO: grad norm: 3.561 3.532 0.453
2024-12-01-17:59:08-root-INFO: grad norm: 3.438 3.416 0.389
2024-12-01-17:59:10-root-INFO: grad norm: 3.373 3.347 0.423
2024-12-01-17:59:11-root-INFO: grad norm: 3.321 3.300 0.372
2024-12-01-17:59:12-root-INFO: grad norm: 3.298 3.272 0.411
2024-12-01-17:59:12-root-INFO: Loss Change: 50.702 -> 48.435
2024-12-01-17:59:12-root-INFO: Regularization Change: 0.000 -> 2.893
2024-12-01-17:59:12-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-01-17:59:12-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-17:59:13-root-INFO: step: 43 lr_xt 0.29878886
2024-12-01-17:59:13-root-INFO: grad norm: 4.153 4.106 0.624
2024-12-01-17:59:13-root-INFO: Loss too large (48.493->50.080)! Learning rate decreased to 0.23903.
2024-12-01-17:59:14-root-INFO: Loss too large (48.493->48.895)! Learning rate decreased to 0.19122.
2024-12-01-17:59:15-root-INFO: grad norm: 3.818 3.783 0.513
2024-12-01-17:59:16-root-INFO: grad norm: 3.574 3.550 0.415
2024-12-01-17:59:17-root-INFO: grad norm: 3.427 3.399 0.438
2024-12-01-17:59:18-root-INFO: grad norm: 3.306 3.285 0.373
2024-12-01-17:59:19-root-INFO: grad norm: 3.238 3.212 0.408
2024-12-01-17:59:20-root-INFO: grad norm: 3.187 3.167 0.357
2024-12-01-17:59:21-root-INFO: grad norm: 3.165 3.140 0.395
2024-12-01-17:59:21-root-INFO: Loss Change: 48.493 -> 46.228
2024-12-01-17:59:21-root-INFO: Regularization Change: 0.000 -> 2.913
2024-12-01-17:59:21-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-01-17:59:21-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-17:59:22-root-INFO: step: 42 lr_xt 0.30451205
2024-12-01-17:59:22-root-INFO: grad norm: 3.782 3.743 0.548
2024-12-01-17:59:23-root-INFO: Loss too large (46.300->47.680)! Learning rate decreased to 0.24361.
2024-12-01-17:59:23-root-INFO: Loss too large (46.300->46.635)! Learning rate decreased to 0.19489.
2024-12-01-17:59:24-root-INFO: grad norm: 3.573 3.542 0.469
2024-12-01-17:59:25-root-INFO: grad norm: 3.436 3.414 0.390
2024-12-01-17:59:26-root-INFO: grad norm: 3.337 3.310 0.423
2024-12-01-17:59:27-root-INFO: grad norm: 3.253 3.233 0.360
2024-12-01-17:59:28-root-INFO: grad norm: 3.197 3.172 0.402
2024-12-01-17:59:29-root-INFO: grad norm: 3.154 3.134 0.348
2024-12-01-17:59:30-root-INFO: grad norm: 3.128 3.103 0.392
2024-12-01-17:59:31-root-INFO: Loss Change: 46.300 -> 44.192
2024-12-01-17:59:31-root-INFO: Regularization Change: 0.000 -> 2.922
2024-12-01-17:59:31-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-01-17:59:31-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-17:59:31-root-INFO: step: 41 lr_xt 0.31082203
2024-12-01-17:59:32-root-INFO: grad norm: 4.297 4.238 0.712
2024-12-01-17:59:32-root-INFO: Loss too large (44.496->46.212)! Learning rate decreased to 0.24866.
2024-12-01-17:59:32-root-INFO: Loss too large (44.496->44.868)! Learning rate decreased to 0.19893.
2024-12-01-17:59:33-root-INFO: grad norm: 3.822 3.786 0.524
2024-12-01-17:59:34-root-INFO: grad norm: 3.499 3.475 0.406
2024-12-01-17:59:35-root-INFO: grad norm: 3.282 3.255 0.421
2024-12-01-17:59:36-root-INFO: grad norm: 3.107 3.087 0.348
2024-12-01-17:59:37-root-INFO: grad norm: 2.992 2.968 0.376
2024-12-01-17:59:38-root-INFO: grad norm: 2.906 2.888 0.323
2024-12-01-17:59:39-root-INFO: grad norm: 2.855 2.833 0.356
2024-12-01-17:59:40-root-INFO: Loss Change: 44.496 -> 42.064
2024-12-01-17:59:40-root-INFO: Regularization Change: 0.000 -> 3.005
2024-12-01-17:59:40-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-01-17:59:40-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-17:59:40-root-INFO: step: 40 lr_xt 0.31666177
2024-12-01-17:59:41-root-INFO: grad norm: 3.819 3.772 0.599
2024-12-01-17:59:41-root-INFO: Loss too large (42.281->43.677)! Learning rate decreased to 0.25333.
2024-12-01-17:59:41-root-INFO: Loss too large (42.281->42.558)! Learning rate decreased to 0.20266.
2024-12-01-17:59:42-root-INFO: grad norm: 3.444 3.413 0.464
2024-12-01-17:59:43-root-INFO: grad norm: 3.192 3.171 0.365
2024-12-01-17:59:44-root-INFO: grad norm: 3.012 2.988 0.382
2024-12-01-17:59:46-root-INFO: grad norm: 2.872 2.854 0.319
2024-12-01-17:59:47-root-INFO: grad norm: 2.776 2.754 0.347
2024-12-01-17:59:48-root-INFO: grad norm: 2.707 2.690 0.299
2024-12-01-17:59:49-root-INFO: grad norm: 2.665 2.645 0.331
2024-12-01-17:59:49-root-INFO: Loss Change: 42.281 -> 40.080
2024-12-01-17:59:49-root-INFO: Regularization Change: 0.000 -> 2.911
2024-12-01-17:59:49-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-01-17:59:49-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-17:59:50-root-INFO: step: 39 lr_xt 0.32255964
2024-12-01-17:59:50-root-INFO: grad norm: 3.360 3.319 0.520
2024-12-01-17:59:50-root-INFO: Loss too large (40.124->41.203)! Learning rate decreased to 0.25805.
2024-12-01-17:59:51-root-INFO: Loss too large (40.124->40.306)! Learning rate decreased to 0.20644.
2024-12-01-17:59:52-root-INFO: grad norm: 3.096 3.068 0.416
2024-12-01-17:59:53-root-INFO: grad norm: 2.936 2.917 0.334
2024-12-01-17:59:54-root-INFO: grad norm: 2.813 2.790 0.355
2024-12-01-17:59:55-root-INFO: grad norm: 2.716 2.699 0.300
2024-12-01-17:59:56-root-INFO: grad norm: 2.644 2.624 0.331
2024-12-01-17:59:57-root-INFO: grad norm: 2.593 2.578 0.286
2024-12-01-17:59:58-root-INFO: grad norm: 2.561 2.541 0.319
2024-12-01-17:59:59-root-INFO: Loss Change: 40.124 -> 38.120
2024-12-01-17:59:59-root-INFO: Regularization Change: 0.000 -> 2.875
2024-12-01-17:59:59-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-01-17:59:59-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-17:59:59-root-INFO: step: 38 lr_xt 0.32851483
2024-12-01-17:59:59-root-INFO: grad norm: 3.410 3.363 0.561
2024-12-01-18:00:00-root-INFO: Loss too large (38.197->39.308)! Learning rate decreased to 0.26281.
2024-12-01-18:00:00-root-INFO: Loss too large (38.197->38.377)! Learning rate decreased to 0.21025.
2024-12-01-18:00:01-root-INFO: grad norm: 3.083 3.055 0.417
2024-12-01-18:00:02-root-INFO: grad norm: 2.870 2.851 0.335
2024-12-01-18:00:03-root-INFO: grad norm: 2.710 2.689 0.341
2024-12-01-18:00:04-root-INFO: grad norm: 2.586 2.569 0.293
2024-12-01-18:00:05-root-INFO: grad norm: 2.494 2.475 0.309
2024-12-01-18:00:06-root-INFO: grad norm: 2.429 2.413 0.274
2024-12-01-18:00:07-root-INFO: grad norm: 2.385 2.367 0.294
2024-12-01-18:00:08-root-INFO: Loss Change: 38.197 -> 36.148
2024-12-01-18:00:08-root-INFO: Regularization Change: 0.000 -> 2.888
2024-12-01-18:00:08-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-01-18:00:08-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-18:00:08-root-INFO: step: 37 lr_xt 0.33452649
2024-12-01-18:00:09-root-INFO: grad norm: 3.325 3.280 0.545
2024-12-01-18:00:09-root-INFO: Loss too large (36.267->37.302)! Learning rate decreased to 0.26762.
2024-12-01-18:00:09-root-INFO: Loss too large (36.267->36.403)! Learning rate decreased to 0.21410.
2024-12-01-18:00:10-root-INFO: grad norm: 2.943 2.915 0.406
2024-12-01-18:00:11-root-INFO: grad norm: 2.698 2.680 0.312
2024-12-01-18:00:13-root-INFO: grad norm: 2.517 2.497 0.319
2024-12-01-18:00:14-root-INFO: grad norm: 2.383 2.368 0.268
2024-12-01-18:00:15-root-INFO: grad norm: 2.284 2.267 0.285
2024-12-01-18:00:16-root-INFO: grad norm: 2.215 2.201 0.249
2024-12-01-18:00:17-root-INFO: grad norm: 2.169 2.152 0.269
2024-12-01-18:00:17-root-INFO: Loss Change: 36.267 -> 34.219
2024-12-01-18:00:17-root-INFO: Regularization Change: 0.000 -> 2.882
2024-12-01-18:00:17-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-01-18:00:17-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-18:00:18-root-INFO: step: 36 lr_xt 0.34059371
2024-12-01-18:00:18-root-INFO: grad norm: 2.864 2.825 0.473
2024-12-01-18:00:18-root-INFO: Loss too large (34.237->34.982)! Learning rate decreased to 0.27247.
2024-12-01-18:00:19-root-INFO: Loss too large (34.237->34.306)! Learning rate decreased to 0.21798.
2024-12-01-18:00:20-root-INFO: grad norm: 2.619 2.596 0.347
2024-12-01-18:00:21-root-INFO: grad norm: 2.478 2.460 0.293
2024-12-01-18:00:22-root-INFO: grad norm: 2.371 2.352 0.294
2024-12-01-18:00:23-root-INFO: grad norm: 2.285 2.270 0.259
2024-12-01-18:00:24-root-INFO: grad norm: 2.219 2.203 0.273
2024-12-01-18:00:25-root-INFO: grad norm: 2.170 2.157 0.245
2024-12-01-18:00:26-root-INFO: grad norm: 2.136 2.120 0.262
2024-12-01-18:00:26-root-INFO: Loss Change: 34.237 -> 32.394
2024-12-01-18:00:27-root-INFO: Regularization Change: 0.000 -> 2.846
2024-12-01-18:00:27-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-01-18:00:27-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-18:00:27-root-INFO: step: 35 lr_xt 0.34671555
2024-12-01-18:00:27-root-INFO: grad norm: 2.954 2.914 0.482
2024-12-01-18:00:28-root-INFO: Loss too large (32.486->33.281)! Learning rate decreased to 0.27737.
2024-12-01-18:00:28-root-INFO: Loss too large (32.486->32.557)! Learning rate decreased to 0.22190.
2024-12-01-18:00:29-root-INFO: grad norm: 2.623 2.597 0.367
2024-12-01-18:00:30-root-INFO: grad norm: 2.401 2.385 0.281
2024-12-01-18:00:31-root-INFO: grad norm: 2.238 2.219 0.285
2024-12-01-18:00:32-root-INFO: grad norm: 2.117 2.103 0.240
2024-12-01-18:00:33-root-INFO: grad norm: 2.026 2.010 0.252
2024-12-01-18:00:34-root-INFO: grad norm: 1.961 1.948 0.222
2024-12-01-18:00:35-root-INFO: grad norm: 1.914 1.899 0.235
2024-12-01-18:00:36-root-INFO: Loss Change: 32.486 -> 30.594
2024-12-01-18:00:36-root-INFO: Regularization Change: 0.000 -> 2.831
2024-12-01-18:00:36-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-01-18:00:36-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-18:00:36-root-INFO: step: 34 lr_xt 0.35289102
2024-12-01-18:00:37-root-INFO: grad norm: 2.618 2.582 0.432
2024-12-01-18:00:37-root-INFO: Loss too large (30.594->31.153)! Learning rate decreased to 0.28231.
2024-12-01-18:00:38-root-INFO: grad norm: 3.303 3.268 0.479
2024-12-01-18:00:38-root-INFO: Loss too large (30.589->30.770)! Learning rate decreased to 0.22585.
2024-12-01-18:00:39-root-INFO: grad norm: 2.827 2.808 0.333
2024-12-01-18:00:40-root-INFO: grad norm: 2.450 2.429 0.316
2024-12-01-18:00:41-root-INFO: grad norm: 2.191 2.177 0.251
2024-12-01-18:00:42-root-INFO: grad norm: 1.999 1.983 0.252
2024-12-01-18:00:43-root-INFO: grad norm: 1.869 1.857 0.215
2024-12-01-18:00:44-root-INFO: grad norm: 1.774 1.760 0.221
2024-12-01-18:00:45-root-INFO: Loss Change: 30.594 -> 28.787
2024-12-01-18:00:45-root-INFO: Regularization Change: 0.000 -> 2.941
2024-12-01-18:00:45-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-01-18:00:45-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-18:00:46-root-INFO: step: 33 lr_xt 0.35911909
2024-12-01-18:00:46-root-INFO: grad norm: 2.513 2.474 0.444
2024-12-01-18:00:46-root-INFO: Loss too large (28.664->29.124)! Learning rate decreased to 0.28730.
2024-12-01-18:00:47-root-INFO: grad norm: 3.073 3.039 0.451
2024-12-01-18:00:48-root-INFO: Loss too large (28.617->28.717)! Learning rate decreased to 0.22984.
2024-12-01-18:00:49-root-INFO: grad norm: 2.589 2.569 0.314
2024-12-01-18:00:50-root-INFO: grad norm: 2.235 2.216 0.284
2024-12-01-18:00:51-root-INFO: grad norm: 1.985 1.972 0.231
2024-12-01-18:00:52-root-INFO: grad norm: 1.804 1.790 0.224
2024-12-01-18:00:53-root-INFO: grad norm: 1.679 1.668 0.196
2024-12-01-18:00:54-root-INFO: grad norm: 1.589 1.577 0.197
2024-12-01-18:00:55-root-INFO: Loss Change: 28.664 -> 26.901
2024-12-01-18:00:55-root-INFO: Regularization Change: 0.000 -> 2.891
2024-12-01-18:00:55-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-01-18:00:55-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-18:00:55-root-INFO: step: 32 lr_xt 0.36539868
2024-12-01-18:00:55-root-INFO: grad norm: 2.474 2.434 0.440
2024-12-01-18:00:56-root-INFO: Loss too large (26.981->27.370)! Learning rate decreased to 0.29232.
2024-12-01-18:00:57-root-INFO: grad norm: 2.901 2.866 0.444
2024-12-01-18:00:57-root-INFO: Loss too large (26.893->26.920)! Learning rate decreased to 0.23386.
2024-12-01-18:00:58-root-INFO: grad norm: 2.363 2.346 0.286
2024-12-01-18:00:59-root-INFO: grad norm: 2.002 1.985 0.260
2024-12-01-18:01:00-root-INFO: grad norm: 1.764 1.752 0.206
2024-12-01-18:01:01-root-INFO: grad norm: 1.596 1.583 0.202
2024-12-01-18:01:02-root-INFO: grad norm: 1.481 1.470 0.174
2024-12-01-18:01:03-root-INFO: grad norm: 1.397 1.386 0.176
2024-12-01-18:01:04-root-INFO: Loss Change: 26.981 -> 25.243
2024-12-01-18:01:04-root-INFO: Regularization Change: 0.000 -> 2.837
2024-12-01-18:01:04-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-01-18:01:04-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-18:01:04-root-INFO: step: 31 lr_xt 0.37172867
2024-12-01-18:01:05-root-INFO: grad norm: 2.051 2.017 0.369
2024-12-01-18:01:05-root-INFO: Loss too large (25.200->25.380)! Learning rate decreased to 0.29738.
2024-12-01-18:01:06-root-INFO: grad norm: 2.412 2.385 0.363
2024-12-01-18:01:07-root-INFO: grad norm: 2.993 2.970 0.375
2024-12-01-18:01:07-root-INFO: Loss too large (25.061->25.153)! Learning rate decreased to 0.23791.
2024-12-01-18:01:08-root-INFO: grad norm: 2.413 2.391 0.322
2024-12-01-18:01:09-root-INFO: grad norm: 1.941 1.928 0.230
2024-12-01-18:01:10-root-INFO: grad norm: 1.652 1.639 0.211
2024-12-01-18:01:11-root-INFO: grad norm: 1.478 1.467 0.175
2024-12-01-18:01:12-root-INFO: grad norm: 1.359 1.348 0.172
2024-12-01-18:01:13-root-INFO: Loss Change: 25.200 -> 23.607
2024-12-01-18:01:13-root-INFO: Regularization Change: 0.000 -> 2.901
2024-12-01-18:01:13-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-01-18:01:13-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-18:01:13-root-INFO: step: 30 lr_xt 0.37810791
2024-12-01-18:01:14-root-INFO: grad norm: 2.088 2.054 0.376
2024-12-01-18:01:14-root-INFO: Loss too large (23.566->23.747)! Learning rate decreased to 0.30249.
2024-12-01-18:01:15-root-INFO: grad norm: 2.349 2.321 0.362
2024-12-01-18:01:16-root-INFO: grad norm: 2.775 2.753 0.350
2024-12-01-18:01:17-root-INFO: Loss too large (23.362->23.379)! Learning rate decreased to 0.24199.
2024-12-01-18:01:18-root-INFO: grad norm: 2.162 2.142 0.291
2024-12-01-18:01:19-root-INFO: grad norm: 1.704 1.692 0.201
2024-12-01-18:01:20-root-INFO: grad norm: 1.436 1.424 0.185
2024-12-01-18:01:21-root-INFO: grad norm: 1.276 1.267 0.151
2024-12-01-18:01:22-root-INFO: grad norm: 1.170 1.160 0.151
2024-12-01-18:01:22-root-INFO: Loss Change: 23.566 -> 21.987
2024-12-01-18:01:22-root-INFO: Regularization Change: 0.000 -> 2.835
2024-12-01-18:01:22-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-01-18:01:22-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-18:01:23-root-INFO: step: 29 lr_xt 0.38453518
2024-12-01-18:01:23-root-INFO: grad norm: 1.909 1.873 0.365
2024-12-01-18:01:24-root-INFO: Loss too large (21.978->22.032)! Learning rate decreased to 0.30763.
2024-12-01-18:01:25-root-INFO: grad norm: 2.036 2.010 0.325
2024-12-01-18:01:26-root-INFO: grad norm: 2.397 2.377 0.312
2024-12-01-18:01:27-root-INFO: grad norm: 2.686 2.661 0.372
2024-12-01-18:01:28-root-INFO: grad norm: 2.842 2.821 0.346
2024-12-01-18:01:29-root-INFO: grad norm: 2.939 2.911 0.407
2024-12-01-18:01:30-root-INFO: grad norm: 3.011 2.988 0.373
2024-12-01-18:01:31-root-INFO: grad norm: 3.019 2.989 0.421
2024-12-01-18:01:31-root-INFO: Loss Change: 21.978 -> 20.914
2024-12-01-18:01:31-root-INFO: Regularization Change: 0.000 -> 3.869
2024-12-01-18:01:31-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-01-18:01:31-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-18:01:32-root-INFO: step: 28 lr_xt 0.39100924
2024-12-01-18:01:32-root-INFO: grad norm: 3.811 3.759 0.630
2024-12-01-18:01:33-root-INFO: Loss too large (21.074->21.830)! Learning rate decreased to 0.31281.
2024-12-01-18:01:34-root-INFO: grad norm: 3.422 3.378 0.548
2024-12-01-18:01:35-root-INFO: grad norm: 3.050 3.022 0.419
2024-12-01-18:01:36-root-INFO: grad norm: 2.816 2.786 0.410
2024-12-01-18:01:37-root-INFO: grad norm: 2.684 2.662 0.344
2024-12-01-18:01:38-root-INFO: grad norm: 2.559 2.533 0.359
2024-12-01-18:01:39-root-INFO: grad norm: 2.447 2.428 0.308
2024-12-01-18:01:40-root-INFO: grad norm: 2.368 2.345 0.328
2024-12-01-18:01:40-root-INFO: Loss Change: 21.074 -> 18.880
2024-12-01-18:01:40-root-INFO: Regularization Change: 0.000 -> 3.703
2024-12-01-18:01:40-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-01-18:01:40-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-18:01:41-root-INFO: step: 27 lr_xt 0.39752879
2024-12-01-18:01:41-root-INFO: grad norm: 3.028 2.985 0.509
2024-12-01-18:01:41-root-INFO: Loss too large (19.028->19.434)! Learning rate decreased to 0.31802.
2024-12-01-18:01:42-root-INFO: grad norm: 2.708 2.673 0.435
2024-12-01-18:01:43-root-INFO: grad norm: 2.412 2.389 0.336
2024-12-01-18:01:44-root-INFO: grad norm: 2.237 2.213 0.329
2024-12-01-18:01:45-root-INFO: grad norm: 2.148 2.130 0.277
2024-12-01-18:01:46-root-INFO: grad norm: 2.060 2.039 0.291
2024-12-01-18:01:48-root-INFO: grad norm: 1.982 1.966 0.251
2024-12-01-18:01:49-root-INFO: grad norm: 1.927 1.909 0.269
2024-12-01-18:01:49-root-INFO: Loss Change: 19.028 -> 17.217
2024-12-01-18:01:49-root-INFO: Regularization Change: 0.000 -> 3.479
2024-12-01-18:01:49-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-01-18:01:49-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-18:01:50-root-INFO: step: 26 lr_xt 0.40409250
2024-12-01-18:01:50-root-INFO: grad norm: 2.739 2.696 0.481
2024-12-01-18:01:50-root-INFO: Loss too large (17.348->17.623)! Learning rate decreased to 0.32327.
2024-12-01-18:01:51-root-INFO: grad norm: 2.389 2.356 0.394
2024-12-01-18:01:52-root-INFO: grad norm: 2.073 2.052 0.293
2024-12-01-18:01:53-root-INFO: grad norm: 1.906 1.885 0.281
2024-12-01-18:01:54-root-INFO: grad norm: 1.819 1.805 0.232
2024-12-01-18:01:55-root-INFO: grad norm: 1.741 1.724 0.244
2024-12-01-18:01:56-root-INFO: grad norm: 1.677 1.664 0.208
2024-12-01-18:01:57-root-INFO: grad norm: 1.629 1.614 0.224
2024-12-01-18:01:58-root-INFO: Loss Change: 17.348 -> 15.665
2024-12-01-18:01:58-root-INFO: Regularization Change: 0.000 -> 3.341
2024-12-01-18:01:58-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-01-18:01:58-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-18:01:59-root-INFO: step: 25 lr_xt 0.41069899
2024-12-01-18:01:59-root-INFO: grad norm: 2.374 2.335 0.431
2024-12-01-18:01:59-root-INFO: Loss too large (15.643->15.776)! Learning rate decreased to 0.32856.
2024-12-01-18:02:00-root-INFO: grad norm: 2.035 2.008 0.332
2024-12-01-18:02:01-root-INFO: grad norm: 1.749 1.731 0.247
2024-12-01-18:02:02-root-INFO: grad norm: 1.607 1.590 0.235
2024-12-01-18:02:03-root-INFO: grad norm: 1.533 1.521 0.194
2024-12-01-18:02:04-root-INFO: grad norm: 1.469 1.455 0.204
2024-12-01-18:02:05-root-INFO: grad norm: 1.417 1.407 0.172
2024-12-01-18:02:06-root-INFO: grad norm: 1.380 1.367 0.187
2024-12-01-18:02:07-root-INFO: Loss Change: 15.643 -> 14.121
2024-12-01-18:02:07-root-INFO: Regularization Change: 0.000 -> 3.191
2024-12-01-18:02:07-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-01-18:02:07-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-18:02:08-root-INFO: step: 24 lr_xt 0.41734684
2024-12-01-18:02:08-root-INFO: grad norm: 2.206 2.170 0.398
2024-12-01-18:02:08-root-INFO: Loss too large (14.243->14.334)! Learning rate decreased to 0.33388.
2024-12-01-18:02:09-root-INFO: grad norm: 1.863 1.837 0.307
2024-12-01-18:02:10-root-INFO: grad norm: 1.574 1.559 0.220
2024-12-01-18:02:11-root-INFO: grad norm: 1.446 1.430 0.213
2024-12-01-18:02:12-root-INFO: grad norm: 1.381 1.371 0.171
2024-12-01-18:02:13-root-INFO: grad norm: 1.327 1.314 0.185
2024-12-01-18:02:14-root-INFO: grad norm: 1.287 1.278 0.153
2024-12-01-18:02:15-root-INFO: grad norm: 1.257 1.245 0.172
2024-12-01-18:02:16-root-INFO: Loss Change: 14.243 -> 12.839
2024-12-01-18:02:16-root-INFO: Regularization Change: 0.000 -> 3.025
2024-12-01-18:02:16-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-01-18:02:16-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-18:02:16-root-INFO: step: 23 lr_xt 0.42403458
2024-12-01-18:02:17-root-INFO: grad norm: 2.035 2.000 0.378
2024-12-01-18:02:17-root-INFO: Loss too large (12.812->12.859)! Learning rate decreased to 0.33923.
2024-12-01-18:02:18-root-INFO: grad norm: 1.701 1.677 0.286
2024-12-01-18:02:19-root-INFO: grad norm: 1.455 1.440 0.202
2024-12-01-18:02:20-root-INFO: grad norm: 1.350 1.335 0.200
2024-12-01-18:02:21-root-INFO: grad norm: 1.295 1.285 0.159
2024-12-01-18:02:22-root-INFO: grad norm: 1.253 1.241 0.175
2024-12-01-18:02:23-root-INFO: grad norm: 1.225 1.217 0.145
2024-12-01-18:02:24-root-INFO: grad norm: 1.206 1.194 0.166
2024-12-01-18:02:25-root-INFO: Loss Change: 12.812 -> 11.530
2024-12-01-18:02:25-root-INFO: Regularization Change: 0.000 -> 2.860
2024-12-01-18:02:25-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-01-18:02:25-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-18:02:25-root-INFO: step: 22 lr_xt 0.43076069
2024-12-01-18:02:26-root-INFO: grad norm: 2.046 2.013 0.366
2024-12-01-18:02:26-root-INFO: Loss too large (11.609->11.712)! Learning rate decreased to 0.34461.
2024-12-01-18:02:27-root-INFO: grad norm: 1.722 1.699 0.282
2024-12-01-18:02:28-root-INFO: grad norm: 1.450 1.437 0.197
2024-12-01-18:02:29-root-INFO: grad norm: 1.349 1.334 0.200
2024-12-01-18:02:30-root-INFO: grad norm: 1.289 1.279 0.156
2024-12-01-18:02:31-root-INFO: grad norm: 1.251 1.238 0.177
2024-12-01-18:02:32-root-INFO: grad norm: 1.225 1.217 0.143
2024-12-01-18:02:33-root-INFO: grad norm: 1.207 1.195 0.168
2024-12-01-18:02:34-root-INFO: Loss Change: 11.609 -> 10.412
2024-12-01-18:02:34-root-INFO: Regularization Change: 0.000 -> 2.680
2024-12-01-18:02:34-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-01-18:02:34-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-18:02:34-root-INFO: step: 21 lr_xt 0.43752364
2024-12-01-18:02:35-root-INFO: grad norm: 1.956 1.926 0.346
2024-12-01-18:02:35-root-INFO: Loss too large (10.438->10.520)! Learning rate decreased to 0.35002.
2024-12-01-18:02:36-root-INFO: grad norm: 1.633 1.611 0.268
2024-12-01-18:02:37-root-INFO: grad norm: 1.381 1.370 0.180
2024-12-01-18:02:38-root-INFO: grad norm: 1.289 1.275 0.189
2024-12-01-18:02:39-root-INFO: grad norm: 1.227 1.219 0.143
2024-12-01-18:02:40-root-INFO: grad norm: 1.193 1.182 0.166
2024-12-01-18:02:41-root-INFO: grad norm: 1.170 1.162 0.131
2024-12-01-18:02:42-root-INFO: grad norm: 1.156 1.145 0.158
2024-12-01-18:02:43-root-INFO: Loss Change: 10.438 -> 9.327
2024-12-01-18:02:43-root-INFO: Regularization Change: 0.000 -> 2.521
2024-12-01-18:02:43-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-01-18:02:43-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-18:02:43-root-INFO: step: 20 lr_xt 0.44432183
2024-12-01-18:02:44-root-INFO: grad norm: 1.996 1.963 0.360
2024-12-01-18:02:44-root-INFO: Loss too large (9.408->9.504)! Learning rate decreased to 0.35546.
2024-12-01-18:02:45-root-INFO: grad norm: 1.634 1.613 0.263
2024-12-01-18:02:46-root-INFO: grad norm: 1.344 1.332 0.176
2024-12-01-18:02:47-root-INFO: grad norm: 1.243 1.230 0.179
2024-12-01-18:02:48-root-INFO: grad norm: 1.169 1.161 0.134
2024-12-01-18:02:49-root-INFO: grad norm: 1.133 1.123 0.155
2024-12-01-18:02:50-root-INFO: grad norm: 1.108 1.102 0.122
2024-12-01-18:02:51-root-INFO: grad norm: 1.095 1.085 0.147
2024-12-01-18:02:52-root-INFO: Loss Change: 9.408 -> 8.340
2024-12-01-18:02:52-root-INFO: Regularization Change: 0.000 -> 2.364
2024-12-01-18:02:52-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-01-18:02:52-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-18:02:52-root-INFO: step: 19 lr_xt 0.45115363
2024-12-01-18:02:53-root-INFO: grad norm: 1.816 1.788 0.316
2024-12-01-18:02:53-root-INFO: Loss too large (8.371->8.423)! Learning rate decreased to 0.36092.
2024-12-01-18:02:54-root-INFO: grad norm: 1.486 1.467 0.235
2024-12-01-18:02:55-root-INFO: grad norm: 1.250 1.241 0.153
2024-12-01-18:02:56-root-INFO: grad norm: 1.156 1.144 0.163
2024-12-01-18:02:57-root-INFO: grad norm: 1.077 1.071 0.117
2024-12-01-18:02:58-root-INFO: grad norm: 1.040 1.031 0.140
2024-12-01-18:02:59-root-INFO: grad norm: 1.012 1.006 0.106
2024-12-01-18:03:00-root-INFO: grad norm: 0.999 0.990 0.132
2024-12-01-18:03:01-root-INFO: Loss Change: 8.371 -> 7.404
2024-12-01-18:03:01-root-INFO: Regularization Change: 0.000 -> 2.205
2024-12-01-18:03:01-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-01-18:03:01-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-18:03:01-root-INFO: step: 18 lr_xt 0.45801735
2024-12-01-18:03:02-root-INFO: grad norm: 1.871 1.840 0.338
2024-12-01-18:03:02-root-INFO: Loss too large (7.511->7.556)! Learning rate decreased to 0.36641.
2024-12-01-18:03:03-root-INFO: grad norm: 1.493 1.474 0.235
2024-12-01-18:03:04-root-INFO: grad norm: 1.232 1.223 0.150
2024-12-01-18:03:05-root-INFO: grad norm: 1.121 1.111 0.155
2024-12-01-18:03:06-root-INFO: grad norm: 1.026 1.020 0.110
2024-12-01-18:03:07-root-INFO: grad norm: 0.979 0.970 0.129
2024-12-01-18:03:08-root-INFO: grad norm: 0.939 0.935 0.096
2024-12-01-18:03:09-root-INFO: grad norm: 0.920 0.912 0.119
2024-12-01-18:03:10-root-INFO: Loss Change: 7.511 -> 6.561
2024-12-01-18:03:10-root-INFO: Regularization Change: 0.000 -> 2.083
2024-12-01-18:03:10-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-01-18:03:10-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-18:03:10-root-INFO: step: 17 lr_xt 0.46491129
2024-12-01-18:03:10-root-INFO: grad norm: 1.678 1.651 0.300
2024-12-01-18:03:11-root-INFO: Loss too large (6.641->6.648)! Learning rate decreased to 0.37193.
2024-12-01-18:03:12-root-INFO: grad norm: 1.327 1.311 0.206
2024-12-01-18:03:13-root-INFO: grad norm: 1.115 1.107 0.129
2024-12-01-18:03:14-root-INFO: grad norm: 1.011 1.001 0.137
2024-12-01-18:03:15-root-INFO: grad norm: 0.920 0.916 0.093
2024-12-01-18:03:16-root-INFO: grad norm: 0.872 0.865 0.112
2024-12-01-18:03:17-root-INFO: grad norm: 0.830 0.827 0.079
2024-12-01-18:03:18-root-INFO: grad norm: 0.808 0.801 0.102
2024-12-01-18:03:19-root-INFO: Loss Change: 6.641 -> 5.806
2024-12-01-18:03:19-root-INFO: Regularization Change: 0.000 -> 1.879
2024-12-01-18:03:19-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-01-18:03:19-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-18:03:19-root-INFO: step: 16 lr_xt 0.47183369
2024-12-01-18:03:19-root-INFO: grad norm: 1.604 1.577 0.290
2024-12-01-18:03:20-root-INFO: grad norm: 1.723 1.698 0.294
2024-12-01-18:03:21-root-INFO: grad norm: 2.043 2.025 0.271
2024-12-01-18:03:22-root-INFO: Loss too large (5.848->5.999)! Learning rate decreased to 0.37747.
2024-12-01-18:03:23-root-INFO: grad norm: 1.542 1.525 0.233
2024-12-01-18:03:24-root-INFO: grad norm: 1.064 1.057 0.124
2024-12-01-18:03:25-root-INFO: grad norm: 0.883 0.875 0.126
2024-12-01-18:03:26-root-INFO: grad norm: 0.770 0.766 0.075
2024-12-01-18:03:27-root-INFO: grad norm: 0.708 0.702 0.093
2024-12-01-18:03:27-root-INFO: Loss Change: 5.908 -> 5.109
2024-12-01-18:03:27-root-INFO: Regularization Change: 0.000 -> 1.908
2024-12-01-18:03:28-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-01-18:03:28-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-18:03:28-root-INFO: step: 15 lr_xt 0.47878275
2024-12-01-18:03:28-root-INFO: grad norm: 1.576 1.545 0.308
2024-12-01-18:03:29-root-INFO: grad norm: 1.609 1.585 0.279
2024-12-01-18:03:30-root-INFO: grad norm: 1.843 1.826 0.256
2024-12-01-18:03:31-root-INFO: Loss too large (5.121->5.221)! Learning rate decreased to 0.38303.
2024-12-01-18:03:32-root-INFO: grad norm: 1.375 1.360 0.206
2024-12-01-18:03:33-root-INFO: grad norm: 0.959 0.953 0.112
2024-12-01-18:03:34-root-INFO: grad norm: 0.793 0.786 0.108
2024-12-01-18:03:35-root-INFO: grad norm: 0.700 0.697 0.068
2024-12-01-18:03:36-root-INFO: grad norm: 0.652 0.647 0.081
2024-12-01-18:03:37-root-INFO: Loss Change: 5.232 -> 4.482
2024-12-01-18:03:37-root-INFO: Regularization Change: 0.000 -> 1.743
2024-12-01-18:03:37-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-01-18:03:37-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-18:03:37-root-INFO: step: 14 lr_xt 0.48575663
2024-12-01-18:03:37-root-INFO: grad norm: 1.508 1.481 0.288
2024-12-01-18:03:38-root-INFO: grad norm: 1.489 1.468 0.251
2024-12-01-18:03:39-root-INFO: grad norm: 1.598 1.582 0.220
2024-12-01-18:03:40-root-INFO: Loss too large (4.478->4.512)! Learning rate decreased to 0.38861.
2024-12-01-18:03:41-root-INFO: grad norm: 1.179 1.166 0.175
2024-12-01-18:03:42-root-INFO: grad norm: 0.948 0.942 0.106
2024-12-01-18:03:43-root-INFO: grad norm: 0.790 0.784 0.103
2024-12-01-18:03:44-root-INFO: grad norm: 0.626 0.623 0.058
2024-12-01-18:03:45-root-INFO: grad norm: 0.591 0.587 0.071
2024-12-01-18:03:45-root-INFO: Loss Change: 4.612 -> 3.949
2024-12-01-18:03:46-root-INFO: Regularization Change: 0.000 -> 1.564
2024-12-01-18:03:46-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-01-18:03:46-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-18:03:46-root-INFO: step: 13 lr_xt 0.49275347
2024-12-01-18:03:46-root-INFO: grad norm: 1.619 1.592 0.294
2024-12-01-18:03:47-root-INFO: grad norm: 1.434 1.414 0.242
2024-12-01-18:03:48-root-INFO: grad norm: 1.443 1.430 0.196
2024-12-01-18:03:49-root-INFO: grad norm: 1.442 1.424 0.228
2024-12-01-18:03:50-root-INFO: grad norm: 1.577 1.562 0.214
2024-12-01-18:03:51-root-INFO: Loss too large (3.857->3.922)! Learning rate decreased to 0.39420.
2024-12-01-18:03:52-root-INFO: grad norm: 1.077 1.065 0.161
2024-12-01-18:03:53-root-INFO: grad norm: 0.732 0.728 0.075
2024-12-01-18:03:54-root-INFO: grad norm: 0.543 0.539 0.067
2024-12-01-18:03:54-root-INFO: Loss Change: 4.118 -> 3.462
2024-12-01-18:03:54-root-INFO: Regularization Change: 0.000 -> 1.544
2024-12-01-18:03:54-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-01-18:03:54-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-18:03:55-root-INFO: step: 12 lr_xt 0.49977135
2024-12-01-18:03:55-root-INFO: grad norm: 1.266 1.242 0.245
2024-12-01-18:03:56-root-INFO: grad norm: 1.118 1.101 0.197
2024-12-01-18:03:57-root-INFO: grad norm: 1.150 1.141 0.148
2024-12-01-18:03:57-root-INFO: Loss too large (3.411->3.445)! Learning rate decreased to 0.39982.
2024-12-01-18:03:59-root-INFO: grad norm: 1.133 1.129 0.099
2024-12-01-18:04:00-root-INFO: grad norm: 0.651 0.649 0.053
2024-12-01-18:04:01-root-INFO: grad norm: 0.553 0.551 0.057
2024-12-01-18:04:02-root-INFO: grad norm: 0.400 0.399 0.031
2024-12-01-18:04:03-root-INFO: grad norm: 0.383 0.381 0.041
2024-12-01-18:04:03-root-INFO: Loss Change: 3.599 -> 3.072
2024-12-01-18:04:03-root-INFO: Regularization Change: 0.000 -> 1.246
2024-12-01-18:04:03-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-01-18:04:03-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-18:04:04-root-INFO: step: 11 lr_xt 0.50680833
2024-12-01-18:04:04-root-INFO: grad norm: 1.421 1.395 0.271
2024-12-01-18:04:05-root-INFO: grad norm: 1.136 1.120 0.193
2024-12-01-18:04:06-root-INFO: grad norm: 1.078 1.067 0.151
2024-12-01-18:04:07-root-INFO: grad norm: 1.015 1.003 0.160
2024-12-01-18:04:08-root-INFO: grad norm: 1.226 1.216 0.154
2024-12-01-18:04:08-root-INFO: Loss too large (2.976->3.035)! Learning rate decreased to 0.40545.
2024-12-01-18:04:09-root-INFO: grad norm: 0.800 0.792 0.111
2024-12-01-18:04:11-root-INFO: grad norm: 0.612 0.610 0.053
2024-12-01-18:04:12-root-INFO: grad norm: 0.655 0.654 0.040
2024-12-01-18:04:12-root-INFO: Loss too large (2.777->2.782)! Learning rate decreased to 0.32436.
2024-12-01-18:04:13-root-INFO: Loss Change: 3.253 -> 2.764
2024-12-01-18:04:13-root-INFO: Regularization Change: 0.000 -> 1.224
2024-12-01-18:04:13-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-01-18:04:13-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-18:04:13-root-INFO: step: 10 lr_xt 0.51386241
2024-12-01-18:04:13-root-INFO: grad norm: 1.245 1.222 0.236
2024-12-01-18:04:14-root-INFO: grad norm: 1.211 1.203 0.141
2024-12-01-18:04:15-root-INFO: grad norm: 0.835 0.827 0.110
2024-12-01-18:04:16-root-INFO: grad norm: 0.786 0.778 0.112
2024-12-01-18:04:17-root-INFO: grad norm: 0.713 0.707 0.093
2024-12-01-18:04:18-root-INFO: grad norm: 0.758 0.750 0.109
2024-12-01-18:04:19-root-INFO: Loss too large (2.568->2.581)! Learning rate decreased to 0.41109.
2024-12-01-18:04:20-root-INFO: grad norm: 0.806 0.803 0.075
2024-12-01-18:04:20-root-INFO: Loss too large (2.530->2.532)! Learning rate decreased to 0.32887.
2024-12-01-18:04:21-root-INFO: grad norm: 0.612 0.608 0.066
2024-12-01-18:04:22-root-INFO: Loss Change: 2.937 -> 2.445
2024-12-01-18:04:22-root-INFO: Regularization Change: 0.000 -> 1.156
2024-12-01-18:04:22-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-01-18:04:22-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-18:04:22-root-INFO: step: 9 lr_xt 0.52093157
2024-12-01-18:04:22-root-INFO: grad norm: 1.300 1.280 0.232
2024-12-01-18:04:23-root-INFO: grad norm: 0.953 0.941 0.151
2024-12-01-18:04:24-root-INFO: grad norm: 0.819 0.810 0.122
2024-12-01-18:04:25-root-INFO: grad norm: 0.899 0.890 0.127
2024-12-01-18:04:26-root-INFO: grad norm: 1.107 1.100 0.123
2024-12-01-18:04:27-root-INFO: Loss too large (2.359->2.371)! Learning rate decreased to 0.41675.
2024-12-01-18:04:28-root-INFO: grad norm: 0.685 0.678 0.092
2024-12-01-18:04:29-root-INFO: grad norm: 0.395 0.392 0.041
2024-12-01-18:04:30-root-INFO: grad norm: 0.284 0.282 0.032
2024-12-01-18:04:31-root-INFO: Loss Change: 2.623 -> 2.142
2024-12-01-18:04:31-root-INFO: Regularization Change: 0.000 -> 1.072
2024-12-01-18:04:31-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-01-18:04:31-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-18:04:31-root-INFO: step: 8 lr_xt 0.52801377
2024-12-01-18:04:31-root-INFO: grad norm: 1.128 1.103 0.233
2024-12-01-18:04:32-root-INFO: grad norm: 0.966 0.954 0.155
2024-12-01-18:04:33-root-INFO: grad norm: 1.085 1.077 0.135
2024-12-01-18:04:34-root-INFO: grad norm: 0.772 0.763 0.113
2024-12-01-18:04:35-root-INFO: grad norm: 0.612 0.607 0.072
2024-12-01-18:04:36-root-INFO: grad norm: 0.506 0.502 0.064
2024-12-01-18:04:37-root-INFO: grad norm: 0.462 0.460 0.046
2024-12-01-18:04:38-root-INFO: grad norm: 0.478 0.476 0.043
2024-12-01-18:04:39-root-INFO: Loss Change: 2.347 -> 1.963
2024-12-01-18:04:39-root-INFO: Regularization Change: 0.000 -> 1.080
2024-12-01-18:04:39-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-01-18:04:39-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-18:04:40-root-INFO: step: 7 lr_xt 0.53510690
2024-12-01-18:04:40-root-INFO: grad norm: 1.219 1.198 0.227
2024-12-01-18:04:41-root-INFO: grad norm: 0.866 0.859 0.108
2024-12-01-18:04:42-root-INFO: grad norm: 0.741 0.736 0.082
2024-12-01-18:04:43-root-INFO: grad norm: 0.716 0.714 0.053
2024-12-01-18:04:44-root-INFO: grad norm: 0.685 0.680 0.089
2024-12-01-18:04:45-root-INFO: grad norm: 0.663 0.660 0.056
2024-12-01-18:04:46-root-INFO: grad norm: 0.659 0.652 0.096
2024-12-01-18:04:47-root-INFO: grad norm: 0.649 0.646 0.062
2024-12-01-18:04:48-root-INFO: Loss Change: 2.188 -> 1.807
2024-12-01-18:04:48-root-INFO: Regularization Change: 0.000 -> 1.018
2024-12-01-18:04:48-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-01-18:04:48-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-18:04:48-root-INFO: step: 6 lr_xt 0.54220886
2024-12-01-18:04:49-root-INFO: grad norm: 1.038 1.026 0.161
2024-12-01-18:04:50-root-INFO: grad norm: 0.660 0.657 0.064
2024-12-01-18:04:51-root-INFO: grad norm: 0.524 0.521 0.054
2024-12-01-18:04:52-root-INFO: grad norm: 0.471 0.469 0.039
2024-12-01-18:04:53-root-INFO: grad norm: 0.442 0.439 0.048
2024-12-01-18:04:54-root-INFO: grad norm: 0.425 0.424 0.039
2024-12-01-18:04:55-root-INFO: grad norm: 0.415 0.412 0.051
2024-12-01-18:04:56-root-INFO: grad norm: 0.407 0.405 0.041
2024-12-01-18:04:56-root-INFO: Loss Change: 1.974 -> 1.627
2024-12-01-18:04:56-root-INFO: Regularization Change: 0.000 -> 0.870
2024-12-01-18:04:56-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-01-18:04:56-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-18:04:57-root-INFO: step: 5 lr_xt 0.54931747
2024-12-01-18:04:57-root-INFO: grad norm: 0.924 0.909 0.165
2024-12-01-18:04:58-root-INFO: grad norm: 0.488 0.485 0.059
2024-12-01-18:04:59-root-INFO: grad norm: 0.332 0.329 0.039
2024-12-01-18:05:00-root-INFO: grad norm: 0.270 0.268 0.030
2024-12-01-18:05:01-root-INFO: grad norm: 0.245 0.244 0.026
2024-12-01-18:05:02-root-INFO: grad norm: 0.230 0.229 0.026
2024-12-01-18:05:03-root-INFO: grad norm: 0.224 0.223 0.024
2024-12-01-18:05:04-root-INFO: grad norm: 0.221 0.220 0.024
2024-12-01-18:05:05-root-INFO: Loss Change: 1.812 -> 1.492
2024-12-01-18:05:05-root-INFO: Regularization Change: 0.000 -> 0.808
2024-12-01-18:05:05-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-01-18:05:05-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-18:05:05-root-INFO: step: 4 lr_xt 0.55643055
2024-12-01-18:05:06-root-INFO: grad norm: 0.869 0.853 0.164
2024-12-01-18:05:07-root-INFO: grad norm: 0.433 0.428 0.063
2024-12-01-18:05:08-root-INFO: grad norm: 0.322 0.321 0.034
2024-12-01-18:05:09-root-INFO: grad norm: 0.214 0.212 0.032
2024-12-01-18:05:10-root-INFO: grad norm: 0.191 0.189 0.027
2024-12-01-18:05:11-root-INFO: grad norm: 0.177 0.175 0.028
2024-12-01-18:05:12-root-INFO: grad norm: 0.170 0.168 0.024
2024-12-01-18:05:13-root-INFO: grad norm: 0.169 0.167 0.025
2024-12-01-18:05:13-root-INFO: Loss Change: 1.677 -> 1.388
2024-12-01-18:05:13-root-INFO: Regularization Change: 0.000 -> 0.744
2024-12-01-18:05:13-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-01-18:05:13-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-18:05:14-root-INFO: step: 3 lr_xt 0.56354589
2024-12-01-18:05:14-root-INFO: grad norm: 0.841 0.826 0.157
2024-12-01-18:05:15-root-INFO: grad norm: 0.424 0.420 0.060
2024-12-01-18:05:16-root-INFO: grad norm: 0.280 0.278 0.036
2024-12-01-18:05:17-root-INFO: grad norm: 0.231 0.228 0.036
2024-12-01-18:05:18-root-INFO: grad norm: 0.255 0.253 0.029
2024-12-01-18:05:19-root-INFO: grad norm: 0.213 0.210 0.033
2024-12-01-18:05:20-root-INFO: grad norm: 0.202 0.200 0.028
2024-12-01-18:05:21-root-INFO: grad norm: 0.210 0.207 0.032
2024-12-01-18:05:22-root-INFO: Loss Change: 1.579 -> 1.311
2024-12-01-18:05:22-root-INFO: Regularization Change: 0.000 -> 0.690
2024-12-01-18:05:22-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-01-18:05:22-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-18:05:22-root-INFO: step: 2 lr_xt 0.57066124
2024-12-01-18:05:23-root-INFO: grad norm: 0.826 0.813 0.147
2024-12-01-18:05:24-root-INFO: grad norm: 0.416 0.412 0.051
2024-12-01-18:05:25-root-INFO: grad norm: 0.365 0.364 0.030
2024-12-01-18:05:26-root-INFO: grad norm: 0.221 0.219 0.032
2024-12-01-18:05:27-root-INFO: grad norm: 0.199 0.197 0.026
2024-12-01-18:05:28-root-INFO: grad norm: 0.343 0.342 0.026
2024-12-01-18:05:29-root-INFO: grad norm: 0.328 0.327 0.022
2024-12-01-18:05:30-root-INFO: grad norm: 0.170 0.168 0.024
2024-12-01-18:05:31-root-INFO: Loss Change: 1.498 -> 1.242
2024-12-01-18:05:31-root-INFO: Regularization Change: 0.000 -> 0.684
2024-12-01-18:05:31-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-01-18:05:31-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-18:05:31-root-INFO: step: 1 lr_xt 0.57777431
2024-12-01-18:05:31-root-INFO: grad norm: 0.746 0.739 0.102
2024-12-01-18:05:32-root-INFO: grad norm: 0.366 0.365 0.033
2024-12-01-18:05:33-root-INFO: grad norm: 0.301 0.300 0.027
2024-12-01-18:05:34-root-INFO: grad norm: 0.241 0.240 0.023
2024-12-01-18:05:35-root-INFO: grad norm: 0.212 0.210 0.021
2024-12-01-18:05:37-root-INFO: grad norm: 0.198 0.197 0.021
2024-12-01-18:05:38-root-INFO: grad norm: 0.236 0.235 0.018
2024-12-01-18:05:39-root-INFO: grad norm: 0.183 0.182 0.018
2024-12-01-18:05:39-root-INFO: Loss Change: 1.401 -> 1.140
2024-12-01-18:05:39-root-INFO: Regularization Change: 0.000 -> 0.774
2024-12-01-18:05:39-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-01-18:05:39-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-18:05:39-root-INFO: loss_sample0_0: 1.140284538269043
2024-12-01-18:05:40-root-INFO: It takes 2185.427 seconds for image sample0
2024-12-01-18:05:40-root-INFO: lpips_score_sample0: 0.146
2024-12-01-18:05:40-root-INFO: psnr_score_sample0: 17.785
2024-12-01-18:05:40-root-INFO: ssim_score_sample0: 0.723
2024-12-01-18:05:40-root-INFO: mean_lpips: 0.14614440500736237
2024-12-01-18:05:40-root-INFO: best_mean_lpips: 0.14614440500736237
2024-12-01-18:05:40-root-INFO: mean_psnr: 17.784692764282227
2024-12-01-18:05:40-root-INFO: best_mean_psnr: 17.784692764282227
2024-12-01-18:05:40-root-INFO: mean_ssim: 0.7233495116233826
2024-12-01-18:05:40-root-INFO: best_mean_ssim: 0.7233495116233826
2024-12-01-18:05:40-root-INFO: final_loss: 1.140284538269043
2024-12-01-18:05:40-root-INFO: mean time: 2185.4273648262024
2024-12-01-18:05:40-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample1_iter8_lr0.03_10009 
 
Enjoy.
