2024-12-01-16:19:24-root-INFO: Prepare model...
2024-12-01-16:19:42-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-01-16:20:09-root-INFO: Start sampling
2024-12-01-16:20:15-root-INFO: step: 249 lr_xt 0.00019059
2024-12-01-16:20:15-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-01-16:20:15-root-INFO: Loss too large (77070.016->78612.750)! Learning rate decreased to 0.00015.
2024-12-01-16:20:15-root-INFO: grad norm: 15661.119 11248.847 10896.518
2024-12-01-16:20:16-root-INFO: grad norm: 14205.955 10378.387 9700.424
2024-12-01-16:20:16-root-INFO: grad norm: 15590.423 11542.578 10479.988
2024-12-01-16:20:17-root-INFO: Loss too large (28301.678->35851.414)! Learning rate decreased to 0.00012.
2024-12-01-16:20:17-root-INFO: grad norm: 16985.279 12431.846 11573.631
2024-12-01-16:20:17-root-INFO: Loss too large (28113.834->29014.264)! Learning rate decreased to 0.00010.
2024-12-01-16:20:18-root-INFO: Loss Change: 77070.016 -> 22717.492
2024-12-01-16:20:18-root-INFO: Regularization Change: 0.000 -> 14.512
2024-12-01-16:20:18-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-01-16:20:18-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-16:20:18-root-INFO: step: 248 lr_xt 0.00020082
2024-12-01-16:20:18-root-INFO: grad norm: 13146.819 10136.922 8371.480
2024-12-01-16:20:18-root-INFO: Loss too large (23084.584->44015.266)! Learning rate decreased to 0.00016.
2024-12-01-16:20:18-root-INFO: Loss too large (23084.584->34074.871)! Learning rate decreased to 0.00013.
2024-12-01-16:20:18-root-INFO: Loss too large (23084.584->26825.635)! Learning rate decreased to 0.00010.
2024-12-01-16:20:19-root-INFO: grad norm: 12869.175 10069.450 8013.853
2024-12-01-16:20:19-root-INFO: grad norm: 14421.383 11505.826 8694.380
2024-12-01-16:20:20-root-INFO: Loss too large (22209.574->24127.299)! Learning rate decreased to 0.00008.
2024-12-01-16:20:20-root-INFO: grad norm: 11116.840 8914.460 6642.027
2024-12-01-16:20:21-root-INFO: grad norm: 8516.629 6794.837 5134.508
2024-12-01-16:20:21-root-INFO: Loss Change: 23084.584 -> 17988.047
2024-12-01-16:20:21-root-INFO: Regularization Change: 0.000 -> 1.692
2024-12-01-16:20:21-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-01-16:20:21-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-16:20:21-root-INFO: step: 247 lr_xt 0.00021156
2024-12-01-16:20:21-root-INFO: grad norm: 6318.482 5138.122 3677.354
2024-12-01-16:20:21-root-INFO: Loss too large (17729.531->26604.568)! Learning rate decreased to 0.00017.
2024-12-01-16:20:22-root-INFO: Loss too large (17729.531->22042.441)! Learning rate decreased to 0.00014.
2024-12-01-16:20:22-root-INFO: Loss too large (17729.531->19384.553)! Learning rate decreased to 0.00011.
2024-12-01-16:20:22-root-INFO: Loss too large (17729.531->17905.086)! Learning rate decreased to 0.00009.
2024-12-01-16:20:22-root-INFO: grad norm: 4648.454 3702.417 2810.735
2024-12-01-16:20:23-root-INFO: grad norm: 3494.581 2850.712 2021.272
2024-12-01-16:20:23-root-INFO: grad norm: 2611.617 2081.053 1577.899
2024-12-01-16:20:24-root-INFO: grad norm: 1995.785 1635.793 1143.389
2024-12-01-16:20:24-root-INFO: Loss Change: 17729.531 -> 16356.021
2024-12-01-16:20:24-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-01-16:20:24-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-01-16:20:24-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-16:20:24-root-INFO: step: 246 lr_xt 0.00022285
2024-12-01-16:20:24-root-INFO: grad norm: 1556.910 1253.975 922.777
2024-12-01-16:20:25-root-INFO: Loss too large (16122.154->16330.166)! Learning rate decreased to 0.00018.
2024-12-01-16:20:25-root-INFO: Loss too large (16122.154->16170.598)! Learning rate decreased to 0.00014.
2024-12-01-16:20:25-root-INFO: grad norm: 2204.431 1809.300 1259.345
2024-12-01-16:20:25-root-INFO: Loss too large (16084.912->16169.585)! Learning rate decreased to 0.00011.
2024-12-01-16:20:26-root-INFO: grad norm: 2376.275 1932.137 1383.304
2024-12-01-16:20:26-root-INFO: grad norm: 2568.039 2084.199 1500.314
2024-12-01-16:20:27-root-INFO: grad norm: 2787.834 2275.002 1611.331
2024-12-01-16:20:27-root-INFO: Loss Change: 16122.154 -> 15913.622
2024-12-01-16:20:27-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-01-16:20:27-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-01-16:20:27-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-01-16:20:27-root-INFO: step: 245 lr_xt 0.00023469
2024-12-01-16:20:28-root-INFO: grad norm: 2947.823 2378.159 1741.844
2024-12-01-16:20:28-root-INFO: Loss too large (15795.797->17533.578)! Learning rate decreased to 0.00019.
2024-12-01-16:20:28-root-INFO: Loss too large (15795.797->16572.445)! Learning rate decreased to 0.00015.
2024-12-01-16:20:28-root-INFO: Loss too large (15795.797->16025.999)! Learning rate decreased to 0.00012.
2024-12-01-16:20:28-root-INFO: grad norm: 3041.794 2493.194 1742.554
2024-12-01-16:20:29-root-INFO: grad norm: 3140.994 2525.920 1866.968
2024-12-01-16:20:29-root-INFO: grad norm: 3248.541 2670.281 1850.031
2024-12-01-16:20:30-root-INFO: grad norm: 3352.980 2690.455 2000.981
2024-12-01-16:20:30-root-INFO: Loss Change: 15795.797 -> 15540.197
2024-12-01-16:20:30-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-01-16:20:30-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-01-16:20:30-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-01-16:20:30-root-INFO: step: 244 lr_xt 0.00024712
2024-12-01-16:20:31-root-INFO: grad norm: 3650.187 2957.014 2140.078
2024-12-01-16:20:31-root-INFO: Loss too large (15494.966->17989.375)! Learning rate decreased to 0.00020.
2024-12-01-16:20:31-root-INFO: Loss too large (15494.966->16619.855)! Learning rate decreased to 0.00016.
2024-12-01-16:20:31-root-INFO: Loss too large (15494.966->15816.190)! Learning rate decreased to 0.00013.
2024-12-01-16:20:32-root-INFO: grad norm: 3488.462 2820.044 2053.466
2024-12-01-16:20:32-root-INFO: grad norm: 3395.235 2800.424 1919.699
2024-12-01-16:20:33-root-INFO: grad norm: 3340.182 2697.078 1970.427
2024-12-01-16:20:33-root-INFO: grad norm: 3295.677 2735.670 1837.824
2024-12-01-16:20:33-root-INFO: Loss Change: 15494.966 -> 15013.561
2024-12-01-16:20:33-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-01-16:20:33-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-01-16:20:33-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:34-root-INFO: step: 243 lr_xt 0.00026017
2024-12-01-16:20:34-root-INFO: grad norm: 3313.000 2652.162 1985.449
2024-12-01-16:20:34-root-INFO: Loss too large (14914.649->17110.916)! Learning rate decreased to 0.00021.
2024-12-01-16:20:34-root-INFO: Loss too large (14914.649->15847.089)! Learning rate decreased to 0.00017.
2024-12-01-16:20:34-root-INFO: Loss too large (14914.649->15134.490)! Learning rate decreased to 0.00013.
2024-12-01-16:20:35-root-INFO: grad norm: 3081.743 2574.457 1693.905
2024-12-01-16:20:35-root-INFO: grad norm: 2927.552 2354.226 1740.167
2024-12-01-16:20:36-root-INFO: grad norm: 2789.714 2346.666 1508.531
2024-12-01-16:20:36-root-INFO: grad norm: 2674.959 2154.253 1585.749
2024-12-01-16:20:36-root-INFO: Loss Change: 14914.649 -> 14353.713
2024-12-01-16:20:36-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-01-16:20:36-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-01-16:20:36-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:37-root-INFO: step: 242 lr_xt 0.00027387
2024-12-01-16:20:37-root-INFO: grad norm: 2585.688 2174.458 1399.114
2024-12-01-16:20:37-root-INFO: Loss too large (14157.211->15403.691)! Learning rate decreased to 0.00022.
2024-12-01-16:20:37-root-INFO: Loss too large (14157.211->14674.080)! Learning rate decreased to 0.00018.
2024-12-01-16:20:37-root-INFO: Loss too large (14157.211->14258.703)! Learning rate decreased to 0.00014.
2024-12-01-16:20:38-root-INFO: grad norm: 2380.779 1939.934 1380.132
2024-12-01-16:20:38-root-INFO: grad norm: 2209.456 1881.290 1158.638
2024-12-01-16:20:39-root-INFO: grad norm: 2065.795 1676.140 1207.504
2024-12-01-16:20:39-root-INFO: grad norm: 1935.322 1657.507 999.069
2024-12-01-16:20:39-root-INFO: Loss Change: 14157.211 -> 13641.162
2024-12-01-16:20:39-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-01-16:20:39-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-01-16:20:39-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:40-root-INFO: step: 241 lr_xt 0.00028824
2024-12-01-16:20:40-root-INFO: grad norm: 1677.458 1382.453 950.100
2024-12-01-16:20:40-root-INFO: Loss too large (13532.177->13920.031)! Learning rate decreased to 0.00023.
2024-12-01-16:20:40-root-INFO: Loss too large (13532.177->13649.242)! Learning rate decreased to 0.00018.
2024-12-01-16:20:41-root-INFO: grad norm: 2175.227 1858.023 1131.090
2024-12-01-16:20:41-root-INFO: Loss too large (13502.514->13546.111)! Learning rate decreased to 0.00015.
2024-12-01-16:20:41-root-INFO: grad norm: 1974.204 1621.923 1125.544
2024-12-01-16:20:42-root-INFO: grad norm: 1797.657 1547.514 914.752
2024-12-01-16:20:42-root-INFO: grad norm: 1646.547 1355.208 935.163
2024-12-01-16:20:42-root-INFO: Loss Change: 13532.177 -> 13090.721
2024-12-01-16:20:42-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-01-16:20:42-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-01-16:20:42-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:43-root-INFO: step: 240 lr_xt 0.00030331
2024-12-01-16:20:43-root-INFO: grad norm: 1540.448 1325.700 784.537
2024-12-01-16:20:43-root-INFO: Loss too large (12911.158->13152.105)! Learning rate decreased to 0.00024.
2024-12-01-16:20:43-root-INFO: Loss too large (12911.158->12950.893)! Learning rate decreased to 0.00019.
2024-12-01-16:20:44-root-INFO: grad norm: 1885.230 1567.413 1047.525
2024-12-01-16:20:44-root-INFO: grad norm: 2417.660 2073.743 1242.847
2024-12-01-16:20:44-root-INFO: Loss too large (12830.635->12900.196)! Learning rate decreased to 0.00016.
2024-12-01-16:20:45-root-INFO: grad norm: 2126.136 1765.693 1184.392
2024-12-01-16:20:45-root-INFO: grad norm: 1875.878 1619.805 946.125
2024-12-01-16:20:45-root-INFO: Loss Change: 12911.158 -> 12457.794
2024-12-01-16:20:45-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-01-16:20:45-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-01-16:20:45-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:46-root-INFO: step: 239 lr_xt 0.00031912
2024-12-01-16:20:46-root-INFO: grad norm: 1697.358 1412.917 940.579
2024-12-01-16:20:46-root-INFO: Loss too large (12398.417->12812.269)! Learning rate decreased to 0.00026.
2024-12-01-16:20:46-root-INFO: Loss too large (12398.417->12515.876)! Learning rate decreased to 0.00020.
2024-12-01-16:20:47-root-INFO: grad norm: 2106.403 1807.450 1081.691
2024-12-01-16:20:47-root-INFO: Loss too large (12355.582->12363.955)! Learning rate decreased to 0.00016.
2024-12-01-16:20:47-root-INFO: grad norm: 1806.513 1511.565 989.272
2024-12-01-16:20:48-root-INFO: grad norm: 1561.214 1354.139 776.980
2024-12-01-16:20:48-root-INFO: grad norm: 1365.069 1148.854 737.257
2024-12-01-16:20:49-root-INFO: Loss Change: 12398.417 -> 11875.755
2024-12-01-16:20:49-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-01-16:20:49-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-01-16:20:49-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:49-root-INFO: step: 238 lr_xt 0.00033570
2024-12-01-16:20:49-root-INFO: grad norm: 1245.269 1053.916 663.291
2024-12-01-16:20:50-root-INFO: grad norm: 2342.911 1952.420 1295.102
2024-12-01-16:20:50-root-INFO: Loss too large (11643.751->12691.059)! Learning rate decreased to 0.00027.
2024-12-01-16:20:50-root-INFO: Loss too large (11643.751->12018.883)! Learning rate decreased to 0.00021.
2024-12-01-16:20:50-root-INFO: Loss too large (11643.751->11647.070)! Learning rate decreased to 0.00017.
2024-12-01-16:20:50-root-INFO: grad norm: 1886.281 1634.379 941.734
2024-12-01-16:20:51-root-INFO: grad norm: 1564.301 1326.285 829.461
2024-12-01-16:20:51-root-INFO: grad norm: 1320.845 1162.385 627.290
2024-12-01-16:20:52-root-INFO: Loss Change: 11652.689 -> 11084.286
2024-12-01-16:20:52-root-INFO: Regularization Change: 0.000 -> 0.599
2024-12-01-16:20:52-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-01-16:20:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:52-root-INFO: step: 237 lr_xt 0.00035308
2024-12-01-16:20:52-root-INFO: grad norm: 1278.036 1092.156 663.755
2024-12-01-16:20:52-root-INFO: Loss too large (11025.006->11110.540)! Learning rate decreased to 0.00028.
2024-12-01-16:20:53-root-INFO: grad norm: 1957.894 1711.700 950.489
2024-12-01-16:20:53-root-INFO: Loss too large (10987.367->11204.395)! Learning rate decreased to 0.00023.
2024-12-01-16:20:53-root-INFO: grad norm: 2311.418 1980.681 1191.452
2024-12-01-16:20:54-root-INFO: Loss too large (10954.196->10956.510)! Learning rate decreased to 0.00018.
2024-12-01-16:20:54-root-INFO: grad norm: 1825.841 1602.532 874.977
2024-12-01-16:20:54-root-INFO: grad norm: 1471.773 1267.208 748.532
2024-12-01-16:20:55-root-INFO: Loss Change: 11025.006 -> 10497.393
2024-12-01-16:20:55-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-01-16:20:55-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-01-16:20:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:55-root-INFO: step: 236 lr_xt 0.00037130
2024-12-01-16:20:55-root-INFO: grad norm: 1217.288 1063.677 591.930
2024-12-01-16:20:55-root-INFO: Loss too large (10379.580->10459.414)! Learning rate decreased to 0.00030.
2024-12-01-16:20:56-root-INFO: grad norm: 1831.196 1584.594 917.791
2024-12-01-16:20:56-root-INFO: Loss too large (10343.133->10511.024)! Learning rate decreased to 0.00024.
2024-12-01-16:20:56-root-INFO: grad norm: 2087.371 1825.443 1012.360
2024-12-01-16:20:57-root-INFO: grad norm: 2377.170 2052.002 1200.094
2024-12-01-16:20:57-root-INFO: grad norm: 2708.235 2362.023 1324.910
2024-12-01-16:20:57-root-INFO: Loss too large (10254.105->10280.174)! Learning rate decreased to 0.00019.
2024-12-01-16:20:58-root-INFO: Loss Change: 10379.580 -> 9993.580
2024-12-01-16:20:58-root-INFO: Regularization Change: 0.000 -> 0.744
2024-12-01-16:20:58-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-01-16:20:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-01-16:20:58-root-INFO: step: 235 lr_xt 0.00039040
2024-12-01-16:20:58-root-INFO: grad norm: 2137.730 1859.949 1053.792
2024-12-01-16:20:58-root-INFO: Loss too large (9925.269->10810.846)! Learning rate decreased to 0.00031.
2024-12-01-16:20:58-root-INFO: Loss too large (9925.269->10206.854)! Learning rate decreased to 0.00025.
2024-12-01-16:20:59-root-INFO: grad norm: 2339.484 2065.477 1098.631
2024-12-01-16:20:59-root-INFO: grad norm: 2560.237 2225.838 1265.093
2024-12-01-16:21:00-root-INFO: grad norm: 2798.423 2464.518 1325.640
2024-12-01-16:21:00-root-INFO: Loss too large (9833.230->9839.115)! Learning rate decreased to 0.00020.
2024-12-01-16:21:01-root-INFO: grad norm: 1949.376 1694.389 963.906
2024-12-01-16:21:01-root-INFO: Loss Change: 9925.269 -> 9345.418
2024-12-01-16:21:01-root-INFO: Regularization Change: 0.000 -> 0.648
2024-12-01-16:21:01-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-01-16:21:01-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-01-16:21:01-root-INFO: step: 234 lr_xt 0.00041042
2024-12-01-16:21:01-root-INFO: grad norm: 1327.138 1179.485 608.368
2024-12-01-16:21:01-root-INFO: Loss too large (9276.212->9495.714)! Learning rate decreased to 0.00033.
2024-12-01-16:21:02-root-INFO: Loss too large (9276.212->9302.043)! Learning rate decreased to 0.00026.
2024-12-01-16:21:02-root-INFO: grad norm: 1380.651 1207.321 669.757
2024-12-01-16:21:03-root-INFO: grad norm: 1437.161 1281.515 650.500
2024-12-01-16:21:03-root-INFO: grad norm: 1491.858 1302.682 727.091
2024-12-01-16:21:04-root-INFO: grad norm: 1544.010 1374.634 703.099
2024-12-01-16:21:04-root-INFO: Loss Change: 9276.212 -> 8935.702
2024-12-01-16:21:04-root-INFO: Regularization Change: 0.000 -> 0.645
2024-12-01-16:21:04-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-01-16:21:04-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:21:04-root-INFO: step: 233 lr_xt 0.00043139
2024-12-01-16:21:04-root-INFO: grad norm: 1618.436 1432.862 752.489
2024-12-01-16:21:04-root-INFO: Loss too large (8897.609->9309.803)! Learning rate decreased to 0.00035.
2024-12-01-16:21:05-root-INFO: Loss too large (8897.609->8980.232)! Learning rate decreased to 0.00028.
2024-12-01-16:21:05-root-INFO: grad norm: 1574.826 1401.733 717.789
2024-12-01-16:21:06-root-INFO: grad norm: 1544.683 1357.022 737.928
2024-12-01-16:21:06-root-INFO: grad norm: 1514.622 1352.688 681.407
2024-12-01-16:21:07-root-INFO: grad norm: 1480.389 1299.572 708.988
2024-12-01-16:21:07-root-INFO: Loss Change: 8897.609 -> 8505.327
2024-12-01-16:21:07-root-INFO: Regularization Change: 0.000 -> 0.658
2024-12-01-16:21:07-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-01-16:21:07-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:21:07-root-INFO: step: 232 lr_xt 0.00045336
2024-12-01-16:21:07-root-INFO: grad norm: 1109.886 993.978 493.818
2024-12-01-16:21:08-root-INFO: Loss too large (8280.656->8377.857)! Learning rate decreased to 0.00036.
2024-12-01-16:21:08-root-INFO: grad norm: 1431.127 1259.635 679.298
2024-12-01-16:21:08-root-INFO: Loss too large (8253.799->8307.830)! Learning rate decreased to 0.00029.
2024-12-01-16:21:09-root-INFO: grad norm: 1333.880 1207.074 567.632
2024-12-01-16:21:09-root-INFO: grad norm: 1249.739 1106.283 581.365
2024-12-01-16:21:10-root-INFO: grad norm: 1170.432 1061.224 493.674
2024-12-01-16:21:10-root-INFO: Loss Change: 8280.656 -> 7951.444
2024-12-01-16:21:10-root-INFO: Regularization Change: 0.000 -> 0.592
2024-12-01-16:21:10-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-01-16:21:10-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:21:11-root-INFO: step: 231 lr_xt 0.00047637
2024-12-01-16:21:11-root-INFO: grad norm: 1143.150 1014.770 526.340
2024-12-01-16:21:11-root-INFO: Loss too large (7892.604->8057.147)! Learning rate decreased to 0.00038.
2024-12-01-16:21:11-root-INFO: Loss too large (7892.604->7898.424)! Learning rate decreased to 0.00030.
2024-12-01-16:21:12-root-INFO: grad norm: 1040.150 945.939 432.562
2024-12-01-16:21:12-root-INFO: grad norm: 948.137 842.961 434.028
2024-12-01-16:21:13-root-INFO: grad norm: 863.716 789.876 349.429
2024-12-01-16:21:13-root-INFO: grad norm: 788.113 703.034 356.181
2024-12-01-16:21:14-root-INFO: Loss Change: 7892.604 -> 7559.104
2024-12-01-16:21:14-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-01-16:21:14-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-01-16:21:14-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:21:14-root-INFO: step: 230 lr_xt 0.00050047
2024-12-01-16:21:14-root-INFO: grad norm: 542.377 507.748 190.696
2024-12-01-16:21:15-root-INFO: grad norm: 777.233 693.757 350.416
2024-12-01-16:21:15-root-INFO: Loss too large (7390.096->7412.326)! Learning rate decreased to 0.00040.
2024-12-01-16:21:15-root-INFO: grad norm: 959.965 876.599 391.287
2024-12-01-16:21:16-root-INFO: grad norm: 1210.187 1079.529 546.964
2024-12-01-16:21:16-root-INFO: Loss too large (7345.178->7368.779)! Learning rate decreased to 0.00032.
2024-12-01-16:21:16-root-INFO: grad norm: 1041.270 947.903 430.957
2024-12-01-16:21:17-root-INFO: Loss Change: 7444.288 -> 7199.550
2024-12-01-16:21:17-root-INFO: Regularization Change: 0.000 -> 0.615
2024-12-01-16:21:17-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-01-16:21:17-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:21:17-root-INFO: step: 229 lr_xt 0.00052570
2024-12-01-16:21:17-root-INFO: grad norm: 1092.451 980.512 481.709
2024-12-01-16:21:17-root-INFO: Loss too large (7150.961->7308.521)! Learning rate decreased to 0.00042.
2024-12-01-16:21:18-root-INFO: grad norm: 1355.304 1239.602 547.939
2024-12-01-16:21:18-root-INFO: Loss too large (7149.882->7200.726)! Learning rate decreased to 0.00034.
2024-12-01-16:21:18-root-INFO: grad norm: 1137.026 1021.622 499.117
2024-12-01-16:21:19-root-INFO: grad norm: 956.038 876.517 381.741
2024-12-01-16:21:19-root-INFO: grad norm: 813.923 733.109 353.586
2024-12-01-16:21:20-root-INFO: Loss Change: 7150.961 -> 6867.456
2024-12-01-16:21:20-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-01-16:21:20-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-01-16:21:20-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:21:20-root-INFO: step: 228 lr_xt 0.00055211
2024-12-01-16:21:20-root-INFO: grad norm: 571.556 517.797 241.997
2024-12-01-16:21:21-root-INFO: grad norm: 862.506 770.651 387.315
2024-12-01-16:21:21-root-INFO: Loss too large (6785.954->6866.310)! Learning rate decreased to 0.00044.
2024-12-01-16:21:21-root-INFO: grad norm: 1031.808 938.663 428.414
2024-12-01-16:21:21-root-INFO: Loss too large (6771.309->6780.880)! Learning rate decreased to 0.00035.
2024-12-01-16:21:22-root-INFO: grad norm: 843.551 761.583 362.726
2024-12-01-16:21:22-root-INFO: grad norm: 699.894 643.517 275.203
2024-12-01-16:21:23-root-INFO: Loss Change: 6803.663 -> 6601.123
2024-12-01-16:21:23-root-INFO: Regularization Change: 0.000 -> 0.461
2024-12-01-16:21:23-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-01-16:21:23-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-01-16:21:23-root-INFO: step: 227 lr_xt 0.00057976
2024-12-01-16:21:23-root-INFO: grad norm: 913.870 824.517 394.120
2024-12-01-16:21:23-root-INFO: Loss too large (6638.820->6730.058)! Learning rate decreased to 0.00046.
2024-12-01-16:21:24-root-INFO: grad norm: 1074.216 986.804 424.449
2024-12-01-16:21:24-root-INFO: Loss too large (6618.668->6632.089)! Learning rate decreased to 0.00037.
2024-12-01-16:21:24-root-INFO: grad norm: 862.806 784.142 359.939
2024-12-01-16:21:25-root-INFO: grad norm: 700.639 644.741 274.234
2024-12-01-16:21:25-root-INFO: grad norm: 581.059 529.857 238.499
2024-12-01-16:21:25-root-INFO: Loss Change: 6638.820 -> 6393.894
2024-12-01-16:21:25-root-INFO: Regularization Change: 0.000 -> 0.427
2024-12-01-16:21:25-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-01-16:21:25-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-01-16:21:26-root-INFO: step: 226 lr_xt 0.00060869
2024-12-01-16:21:26-root-INFO: grad norm: 515.571 465.310 222.038
2024-12-01-16:21:26-root-INFO: grad norm: 723.357 657.614 301.313
2024-12-01-16:21:26-root-INFO: Loss too large (6314.272->6361.404)! Learning rate decreased to 0.00049.
2024-12-01-16:21:27-root-INFO: grad norm: 827.519 759.028 329.644
2024-12-01-16:21:27-root-INFO: grad norm: 972.716 884.985 403.704
2024-12-01-16:21:28-root-INFO: Loss too large (6286.828->6293.959)! Learning rate decreased to 0.00039.
2024-12-01-16:21:28-root-INFO: grad norm: 759.947 697.607 301.436
2024-12-01-16:21:28-root-INFO: Loss Change: 6335.573 -> 6167.304
2024-12-01-16:21:28-root-INFO: Regularization Change: 0.000 -> 0.469
2024-12-01-16:21:28-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-01-16:21:28-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:21:28-root-INFO: step: 225 lr_xt 0.00063896
2024-12-01-16:21:29-root-INFO: grad norm: 923.096 839.532 383.787
2024-12-01-16:21:29-root-INFO: Loss too large (6160.237->6275.708)! Learning rate decreased to 0.00051.
2024-12-01-16:21:29-root-INFO: grad norm: 1066.227 982.170 414.949
2024-12-01-16:21:29-root-INFO: Loss too large (6145.195->6166.243)! Learning rate decreased to 0.00041.
2024-12-01-16:21:30-root-INFO: grad norm: 833.336 763.145 334.751
2024-12-01-16:21:30-root-INFO: grad norm: 656.496 604.721 255.538
2024-12-01-16:21:31-root-INFO: grad norm: 529.849 486.601 209.663
2024-12-01-16:21:31-root-INFO: Loss Change: 6160.237 -> 5927.677
2024-12-01-16:21:31-root-INFO: Regularization Change: 0.000 -> 0.416
2024-12-01-16:21:31-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-01-16:21:31-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:21:31-root-INFO: step: 224 lr_xt 0.00067063
2024-12-01-16:21:31-root-INFO: grad norm: 346.483 313.527 147.484
2024-12-01-16:21:32-root-INFO: grad norm: 386.139 352.513 157.601
2024-12-01-16:21:32-root-INFO: grad norm: 556.189 512.343 216.451
2024-12-01-16:21:33-root-INFO: Loss too large (5815.104->5840.447)! Learning rate decreased to 0.00054.
2024-12-01-16:21:33-root-INFO: grad norm: 657.305 608.361 248.891
2024-12-01-16:21:33-root-INFO: grad norm: 794.370 732.689 306.905
2024-12-01-16:21:34-root-INFO: Loss too large (5790.649->5798.599)! Learning rate decreased to 0.00043.
2024-12-01-16:21:34-root-INFO: Loss Change: 5884.828 -> 5741.850
2024-12-01-16:21:34-root-INFO: Regularization Change: 0.000 -> 0.548
2024-12-01-16:21:34-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-01-16:21:34-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:21:34-root-INFO: step: 223 lr_xt 0.00070376
2024-12-01-16:21:34-root-INFO: grad norm: 862.464 796.461 330.901
2024-12-01-16:21:34-root-INFO: Loss too large (5743.234->5881.920)! Learning rate decreased to 0.00056.
2024-12-01-16:21:35-root-INFO: Loss too large (5743.234->5746.932)! Learning rate decreased to 0.00045.
2024-12-01-16:21:35-root-INFO: grad norm: 688.966 637.320 261.720
2024-12-01-16:21:36-root-INFO: grad norm: 565.474 525.252 209.454
2024-12-01-16:21:36-root-INFO: grad norm: 470.755 434.441 181.306
2024-12-01-16:21:37-root-INFO: grad norm: 401.174 373.652 146.030
2024-12-01-16:21:37-root-INFO: Loss Change: 5743.234 -> 5535.208
2024-12-01-16:21:37-root-INFO: Regularization Change: 0.000 -> 0.368
2024-12-01-16:21:37-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-01-16:21:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:21:37-root-INFO: step: 222 lr_xt 0.00073840
2024-12-01-16:21:37-root-INFO: grad norm: 299.623 273.733 121.835
2024-12-01-16:21:38-root-INFO: grad norm: 398.551 372.271 142.328
2024-12-01-16:21:38-root-INFO: grad norm: 630.499 583.390 239.136
2024-12-01-16:21:39-root-INFO: Loss too large (5432.084->5499.955)! Learning rate decreased to 0.00059.
2024-12-01-16:21:39-root-INFO: grad norm: 763.427 712.537 274.065
2024-12-01-16:21:39-root-INFO: Loss too large (5428.849->5438.673)! Learning rate decreased to 0.00047.
2024-12-01-16:21:40-root-INFO: grad norm: 619.240 573.669 233.155
2024-12-01-16:21:40-root-INFO: Loss Change: 5466.222 -> 5342.017
2024-12-01-16:21:40-root-INFO: Regularization Change: 0.000 -> 0.475
2024-12-01-16:21:40-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-01-16:21:40-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:21:40-root-INFO: step: 221 lr_xt 0.00077462
2024-12-01-16:21:41-root-INFO: grad norm: 607.569 565.206 222.895
2024-12-01-16:21:41-root-INFO: Loss too large (5308.880->5366.514)! Learning rate decreased to 0.00062.
2024-12-01-16:21:41-root-INFO: grad norm: 717.977 666.531 266.886
2024-12-01-16:21:41-root-INFO: Loss too large (5299.646->5304.550)! Learning rate decreased to 0.00050.
2024-12-01-16:21:42-root-INFO: grad norm: 573.074 534.832 205.835
2024-12-01-16:21:42-root-INFO: grad norm: 464.109 430.585 173.187
2024-12-01-16:21:43-root-INFO: grad norm: 384.761 359.675 136.657
2024-12-01-16:21:43-root-INFO: Loss Change: 5308.880 -> 5156.962
2024-12-01-16:21:43-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-01-16:21:43-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-01-16:21:43-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:21:43-root-INFO: step: 220 lr_xt 0.00081248
2024-12-01-16:21:44-root-INFO: grad norm: 286.926 265.113 109.735
2024-12-01-16:21:44-root-INFO: grad norm: 243.964 221.889 101.410
2024-12-01-16:21:45-root-INFO: grad norm: 274.568 257.639 94.919
2024-12-01-16:21:45-root-INFO: grad norm: 388.430 359.754 146.475
2024-12-01-16:21:45-root-INFO: Loss too large (5018.467->5024.993)! Learning rate decreased to 0.00065.
2024-12-01-16:21:46-root-INFO: grad norm: 470.091 442.001 160.066
2024-12-01-16:21:46-root-INFO: Loss Change: 5125.864 -> 4994.997
2024-12-01-16:21:46-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-01-16:21:46-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-01-16:21:46-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-01-16:21:46-root-INFO: step: 219 lr_xt 0.00085206
2024-12-01-16:21:47-root-INFO: grad norm: 553.899 513.280 208.200
2024-12-01-16:21:47-root-INFO: Loss too large (4967.439->5025.620)! Learning rate decreased to 0.00068.
2024-12-01-16:21:47-root-INFO: grad norm: 669.976 628.499 232.071
2024-12-01-16:21:47-root-INFO: Loss too large (4963.252->4972.044)! Learning rate decreased to 0.00055.
2024-12-01-16:21:48-root-INFO: grad norm: 545.862 508.999 197.194
2024-12-01-16:21:48-root-INFO: grad norm: 451.513 423.699 156.021
2024-12-01-16:21:49-root-INFO: grad norm: 378.855 352.627 138.512
2024-12-01-16:21:49-root-INFO: Loss Change: 4967.439 -> 4832.451
2024-12-01-16:21:49-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-01-16:21:49-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-01-16:21:49-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-01-16:21:50-root-INFO: step: 218 lr_xt 0.00089342
2024-12-01-16:21:50-root-INFO: grad norm: 499.898 465.963 181.041
2024-12-01-16:21:50-root-INFO: Loss too large (4824.223->4861.093)! Learning rate decreased to 0.00071.
2024-12-01-16:21:50-root-INFO: grad norm: 598.930 559.419 213.934
2024-12-01-16:21:51-root-INFO: Loss too large (4811.890->4816.116)! Learning rate decreased to 0.00057.
2024-12-01-16:21:51-root-INFO: grad norm: 494.531 464.438 169.877
2024-12-01-16:21:52-root-INFO: grad norm: 412.697 385.176 148.184
2024-12-01-16:21:52-root-INFO: grad norm: 350.651 329.908 118.813
2024-12-01-16:21:53-root-INFO: Loss Change: 4824.223 -> 4692.028
2024-12-01-16:21:53-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-01-16:21:53-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-01-16:21:53-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:21:53-root-INFO: step: 217 lr_xt 0.00093664
2024-12-01-16:21:53-root-INFO: grad norm: 247.948 219.628 115.072
2024-12-01-16:21:53-root-INFO: grad norm: 229.143 210.659 90.162
2024-12-01-16:21:54-root-INFO: grad norm: 284.181 263.245 107.055
2024-12-01-16:21:54-root-INFO: grad norm: 440.613 415.666 146.156
2024-12-01-16:21:54-root-INFO: Loss too large (4586.104->4623.307)! Learning rate decreased to 0.00075.
2024-12-01-16:21:55-root-INFO: grad norm: 547.661 511.965 194.486
2024-12-01-16:21:55-root-INFO: Loss too large (4580.867->4587.889)! Learning rate decreased to 0.00060.
2024-12-01-16:21:55-root-INFO: Loss Change: 4663.042 -> 4549.486
2024-12-01-16:21:55-root-INFO: Regularization Change: 0.000 -> 0.605
2024-12-01-16:21:56-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-01-16:21:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:21:56-root-INFO: step: 216 lr_xt 0.00098179
2024-12-01-16:21:56-root-INFO: grad norm: 670.436 625.396 241.588
2024-12-01-16:21:56-root-INFO: Loss too large (4563.131->4690.170)! Learning rate decreased to 0.00079.
2024-12-01-16:21:56-root-INFO: Loss too large (4563.131->4573.580)! Learning rate decreased to 0.00063.
2024-12-01-16:21:57-root-INFO: grad norm: 555.348 519.851 195.363
2024-12-01-16:21:57-root-INFO: grad norm: 473.476 446.250 158.240
2024-12-01-16:21:58-root-INFO: grad norm: 407.350 380.984 144.172
2024-12-01-16:21:58-root-INFO: grad norm: 354.237 334.511 116.563
2024-12-01-16:21:59-root-INFO: Loss Change: 4563.131 -> 4401.927
2024-12-01-16:21:59-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-01-16:21:59-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-01-16:21:59-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:21:59-root-INFO: step: 215 lr_xt 0.00102894
2024-12-01-16:21:59-root-INFO: grad norm: 278.703 260.814 98.242
2024-12-01-16:21:59-root-INFO: grad norm: 438.138 413.966 143.517
2024-12-01-16:22:00-root-INFO: Loss too large (4374.551->4417.590)! Learning rate decreased to 0.00082.
2024-12-01-16:22:00-root-INFO: grad norm: 537.892 504.215 187.337
2024-12-01-16:22:00-root-INFO: Loss too large (4370.722->4377.353)! Learning rate decreased to 0.00066.
2024-12-01-16:22:01-root-INFO: grad norm: 443.541 419.067 145.298
2024-12-01-16:22:01-root-INFO: grad norm: 368.636 345.280 129.129
2024-12-01-16:22:02-root-INFO: Loss Change: 4382.299 -> 4285.936
2024-12-01-16:22:02-root-INFO: Regularization Change: 0.000 -> 0.419
2024-12-01-16:22:02-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-01-16:22:02-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:22:02-root-INFO: step: 214 lr_xt 0.00107819
2024-12-01-16:22:02-root-INFO: grad norm: 450.021 419.291 163.445
2024-12-01-16:22:02-root-INFO: Loss too large (4272.607->4311.802)! Learning rate decreased to 0.00086.
2024-12-01-16:22:03-root-INFO: grad norm: 528.286 494.708 185.339
2024-12-01-16:22:03-root-INFO: Loss too large (4262.623->4266.907)! Learning rate decreased to 0.00069.
2024-12-01-16:22:03-root-INFO: grad norm: 425.622 402.199 139.248
2024-12-01-16:22:04-root-INFO: grad norm: 346.918 325.196 120.829
2024-12-01-16:22:04-root-INFO: grad norm: 287.482 272.147 92.638
2024-12-01-16:22:04-root-INFO: Loss Change: 4272.607 -> 4156.399
2024-12-01-16:22:04-root-INFO: Regularization Change: 0.000 -> 0.379
2024-12-01-16:22:04-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-01-16:22:04-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:22:04-root-INFO: step: 213 lr_xt 0.00112961
2024-12-01-16:22:05-root-INFO: grad norm: 200.003 178.908 89.403
2024-12-01-16:22:05-root-INFO: grad norm: 160.738 148.932 60.464
2024-12-01-16:22:06-root-INFO: grad norm: 151.821 140.621 57.230
2024-12-01-16:22:06-root-INFO: grad norm: 150.215 139.940 54.600
2024-12-01-16:22:07-root-INFO: grad norm: 159.714 149.448 56.337
2024-12-01-16:22:07-root-INFO: Loss Change: 4136.377 -> 4012.164
2024-12-01-16:22:07-root-INFO: Regularization Change: 0.000 -> 0.708
2024-12-01-16:22:07-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-01-16:22:07-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-01-16:22:07-root-INFO: step: 212 lr_xt 0.00118329
2024-12-01-16:22:07-root-INFO: grad norm: 189.741 177.632 66.698
2024-12-01-16:22:08-root-INFO: grad norm: 264.233 250.121 85.195
2024-12-01-16:22:08-root-INFO: Loss too large (3974.894->3978.215)! Learning rate decreased to 0.00095.
2024-12-01-16:22:09-root-INFO: grad norm: 305.737 286.687 106.233
2024-12-01-16:22:09-root-INFO: grad norm: 361.308 343.619 111.666
2024-12-01-16:22:10-root-INFO: grad norm: 427.755 401.766 146.829
2024-12-01-16:22:10-root-INFO: Loss too large (3952.731->3953.587)! Learning rate decreased to 0.00076.
2024-12-01-16:22:10-root-INFO: Loss Change: 3988.831 -> 3925.392
2024-12-01-16:22:10-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-01-16:22:10-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-01-16:22:10-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-01-16:22:10-root-INFO: step: 211 lr_xt 0.00123933
2024-12-01-16:22:10-root-INFO: grad norm: 496.094 461.522 181.952
2024-12-01-16:22:10-root-INFO: Loss too large (3927.921->3988.870)! Learning rate decreased to 0.00099.
2024-12-01-16:22:11-root-INFO: grad norm: 553.883 518.707 194.240
2024-12-01-16:22:11-root-INFO: Loss too large (3918.082->3928.250)! Learning rate decreased to 0.00079.
2024-12-01-16:22:12-root-INFO: grad norm: 427.368 405.956 133.577
2024-12-01-16:22:12-root-INFO: grad norm: 337.264 316.955 115.266
2024-12-01-16:22:13-root-INFO: grad norm: 269.105 256.005 82.937
2024-12-01-16:22:13-root-INFO: Loss Change: 3927.921 -> 3810.375
2024-12-01-16:22:13-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-01-16:22:13-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-01-16:22:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:22:13-root-INFO: step: 210 lr_xt 0.00129780
2024-12-01-16:22:13-root-INFO: grad norm: 153.890 143.779 54.861
2024-12-01-16:22:14-root-INFO: grad norm: 193.492 183.215 62.222
2024-12-01-16:22:14-root-INFO: grad norm: 287.285 271.096 95.075
2024-12-01-16:22:14-root-INFO: Loss too large (3766.465->3782.384)! Learning rate decreased to 0.00104.
2024-12-01-16:22:15-root-INFO: grad norm: 331.798 316.029 101.072
2024-12-01-16:22:15-root-INFO: grad norm: 382.372 360.989 126.076
2024-12-01-16:22:16-root-INFO: Loss Change: 3787.865 -> 3754.407
2024-12-01-16:22:16-root-INFO: Regularization Change: 0.000 -> 0.487
2024-12-01-16:22:16-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-01-16:22:16-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:22:16-root-INFO: step: 209 lr_xt 0.00135882
2024-12-01-16:22:16-root-INFO: grad norm: 512.217 483.631 168.724
2024-12-01-16:22:16-root-INFO: Loss too large (3745.973->3833.967)! Learning rate decreased to 0.00109.
2024-12-01-16:22:16-root-INFO: Loss too large (3745.973->3747.138)! Learning rate decreased to 0.00087.
2024-12-01-16:22:17-root-INFO: grad norm: 370.422 350.440 120.018
2024-12-01-16:22:17-root-INFO: grad norm: 276.273 262.360 86.569
2024-12-01-16:22:18-root-INFO: grad norm: 216.942 205.305 70.097
2024-12-01-16:22:18-root-INFO: grad norm: 175.910 166.864 55.683
2024-12-01-16:22:18-root-INFO: Loss Change: 3745.973 -> 3630.730
2024-12-01-16:22:18-root-INFO: Regularization Change: 0.000 -> 0.330
2024-12-01-16:22:18-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-01-16:22:18-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:22:19-root-INFO: step: 208 lr_xt 0.00142247
2024-12-01-16:22:19-root-INFO: grad norm: 155.796 141.051 66.158
2024-12-01-16:22:19-root-INFO: grad norm: 140.667 131.553 49.809
2024-12-01-16:22:20-root-INFO: grad norm: 164.763 156.779 50.669
2024-12-01-16:22:20-root-INFO: grad norm: 236.617 225.180 72.674
2024-12-01-16:22:20-root-INFO: Loss too large (3577.357->3587.598)! Learning rate decreased to 0.00114.
2024-12-01-16:22:21-root-INFO: grad norm: 276.015 262.745 84.556
2024-12-01-16:22:21-root-INFO: Loss Change: 3621.386 -> 3568.536
2024-12-01-16:22:21-root-INFO: Regularization Change: 0.000 -> 0.519
2024-12-01-16:22:21-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-01-16:22:21-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:22:21-root-INFO: step: 207 lr_xt 0.00150141
2024-12-01-16:22:21-root-INFO: grad norm: 367.675 348.340 117.660
2024-12-01-16:22:22-root-INFO: Loss too large (3550.912->3602.812)! Learning rate decreased to 0.00120.
2024-12-01-16:22:22-root-INFO: Loss too large (3550.912->3553.479)! Learning rate decreased to 0.00096.
2024-12-01-16:22:22-root-INFO: grad norm: 286.440 273.324 85.684
2024-12-01-16:22:23-root-INFO: grad norm: 226.833 215.435 70.999
2024-12-01-16:22:23-root-INFO: grad norm: 188.867 180.244 56.417
2024-12-01-16:22:24-root-INFO: grad norm: 160.827 152.497 51.087
2024-12-01-16:22:24-root-INFO: Loss Change: 3550.912 -> 3473.312
2024-12-01-16:22:24-root-INFO: Regularization Change: 0.000 -> 0.285
2024-12-01-16:22:24-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-01-16:22:24-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:22:24-root-INFO: step: 206 lr_xt 0.00157117
2024-12-01-16:22:24-root-INFO: grad norm: 139.017 130.633 47.546
2024-12-01-16:22:25-root-INFO: grad norm: 177.813 169.039 55.167
2024-12-01-16:22:25-root-INFO: grad norm: 283.950 272.013 81.463
2024-12-01-16:22:26-root-INFO: Loss too large (3436.561->3471.538)! Learning rate decreased to 0.00126.
2024-12-01-16:22:26-root-INFO: Loss too large (3436.561->3437.981)! Learning rate decreased to 0.00101.
2024-12-01-16:22:26-root-INFO: grad norm: 244.174 232.878 73.407
2024-12-01-16:22:27-root-INFO: grad norm: 218.101 208.280 64.711
2024-12-01-16:22:27-root-INFO: Loss Change: 3449.314 -> 3397.952
2024-12-01-16:22:27-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-01-16:22:27-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-01-16:22:27-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-01-16:22:27-root-INFO: step: 205 lr_xt 0.00164390
2024-12-01-16:22:27-root-INFO: grad norm: 249.326 235.210 82.704
2024-12-01-16:22:27-root-INFO: Loss too large (3388.832->3406.919)! Learning rate decreased to 0.00132.
2024-12-01-16:22:28-root-INFO: grad norm: 305.676 291.843 90.915
2024-12-01-16:22:28-root-INFO: Loss too large (3384.296->3391.771)! Learning rate decreased to 0.00105.
2024-12-01-16:22:29-root-INFO: grad norm: 275.762 263.081 82.662
2024-12-01-16:22:29-root-INFO: grad norm: 254.354 243.412 73.799
2024-12-01-16:22:30-root-INFO: grad norm: 235.742 224.726 71.220
2024-12-01-16:22:30-root-INFO: Loss Change: 3388.832 -> 3335.782
2024-12-01-16:22:30-root-INFO: Regularization Change: 0.000 -> 0.326
2024-12-01-16:22:30-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-01-16:22:30-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-01-16:22:30-root-INFO: step: 204 lr_xt 0.00171973
2024-12-01-16:22:30-root-INFO: grad norm: 223.575 209.647 77.678
2024-12-01-16:22:30-root-INFO: Loss too large (3327.969->3337.139)! Learning rate decreased to 0.00138.
2024-12-01-16:22:31-root-INFO: grad norm: 282.370 268.232 88.230
2024-12-01-16:22:31-root-INFO: Loss too large (3319.547->3329.837)! Learning rate decreased to 0.00110.
2024-12-01-16:22:31-root-INFO: grad norm: 272.658 260.962 79.002
2024-12-01-16:22:32-root-INFO: grad norm: 267.323 254.511 81.765
2024-12-01-16:22:32-root-INFO: grad norm: 265.544 254.152 76.945
2024-12-01-16:22:33-root-INFO: Loss Change: 3327.969 -> 3279.666
2024-12-01-16:22:33-root-INFO: Regularization Change: 0.000 -> 0.362
2024-12-01-16:22:33-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-01-16:22:33-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:22:33-root-INFO: step: 203 lr_xt 0.00179875
2024-12-01-16:22:33-root-INFO: grad norm: 251.940 239.889 76.986
2024-12-01-16:22:33-root-INFO: Loss too large (3264.021->3301.804)! Learning rate decreased to 0.00144.
2024-12-01-16:22:33-root-INFO: Loss too large (3264.021->3270.794)! Learning rate decreased to 0.00115.
2024-12-01-16:22:34-root-INFO: grad norm: 245.269 234.933 70.452
2024-12-01-16:22:34-root-INFO: grad norm: 243.694 232.997 71.409
2024-12-01-16:22:35-root-INFO: grad norm: 245.121 234.771 70.478
2024-12-01-16:22:35-root-INFO: grad norm: 249.218 238.371 72.724
2024-12-01-16:22:36-root-INFO: Loss Change: 3264.021 -> 3219.303
2024-12-01-16:22:36-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-01-16:22:36-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-01-16:22:36-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:22:36-root-INFO: step: 202 lr_xt 0.00188111
2024-12-01-16:22:36-root-INFO: grad norm: 301.429 281.882 106.782
2024-12-01-16:22:36-root-INFO: Loss too large (3211.907->3264.598)! Learning rate decreased to 0.00150.
2024-12-01-16:22:36-root-INFO: Loss too large (3211.907->3217.337)! Learning rate decreased to 0.00120.
2024-12-01-16:22:37-root-INFO: grad norm: 300.898 287.256 89.574
2024-12-01-16:22:37-root-INFO: grad norm: 328.200 314.334 94.389
2024-12-01-16:22:38-root-INFO: grad norm: 365.249 349.745 105.286
2024-12-01-16:22:38-root-INFO: Loss too large (3182.967->3184.512)! Learning rate decreased to 0.00096.
2024-12-01-16:22:38-root-INFO: grad norm: 267.238 256.149 76.182
2024-12-01-16:22:39-root-INFO: Loss Change: 3211.907 -> 3141.525
2024-12-01-16:22:39-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-01-16:22:39-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-01-16:22:39-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:22:39-root-INFO: step: 201 lr_xt 0.00196691
2024-12-01-16:22:39-root-INFO: grad norm: 230.056 218.389 72.334
2024-12-01-16:22:39-root-INFO: Loss too large (3143.925->3182.892)! Learning rate decreased to 0.00157.
2024-12-01-16:22:39-root-INFO: Loss too large (3143.925->3152.804)! Learning rate decreased to 0.00126.
2024-12-01-16:22:40-root-INFO: grad norm: 248.675 239.023 68.609
2024-12-01-16:22:40-root-INFO: grad norm: 277.334 265.074 81.548
2024-12-01-16:22:41-root-INFO: grad norm: 311.178 298.705 87.220
2024-12-01-16:22:41-root-INFO: grad norm: 353.801 338.861 101.725
2024-12-01-16:22:41-root-INFO: Loss too large (3124.593->3127.757)! Learning rate decreased to 0.00101.
2024-12-01-16:22:42-root-INFO: Loss Change: 3143.925 -> 3102.073
2024-12-01-16:22:42-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-01-16:22:42-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-01-16:22:42-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:22:42-root-INFO: step: 200 lr_xt 0.00205630
2024-12-01-16:22:42-root-INFO: grad norm: 326.688 309.709 103.949
2024-12-01-16:22:42-root-INFO: Loss too large (3101.222->3212.703)! Learning rate decreased to 0.00165.
2024-12-01-16:22:42-root-INFO: Loss too large (3101.222->3136.781)! Learning rate decreased to 0.00132.
2024-12-01-16:22:43-root-INFO: grad norm: 372.423 357.327 104.957
2024-12-01-16:22:43-root-INFO: Loss too large (3094.864->3102.872)! Learning rate decreased to 0.00105.
2024-12-01-16:22:43-root-INFO: grad norm: 292.022 279.974 83.014
2024-12-01-16:22:44-root-INFO: grad norm: 232.363 222.589 66.685
2024-12-01-16:22:44-root-INFO: grad norm: 195.276 187.095 55.930
2024-12-01-16:22:45-root-INFO: Loss Change: 3101.222 -> 3028.446
2024-12-01-16:22:45-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-01-16:22:45-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-01-16:22:45-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-01-16:22:45-root-INFO: step: 199 lr_xt 0.00214940
2024-12-01-16:22:45-root-INFO: grad norm: 167.031 159.278 50.298
2024-12-01-16:22:45-root-INFO: Loss too large (3010.270->3017.861)! Learning rate decreased to 0.00172.
2024-12-01-16:22:46-root-INFO: grad norm: 256.896 247.322 69.478
2024-12-01-16:22:46-root-INFO: Loss too large (3005.346->3037.746)! Learning rate decreased to 0.00138.
2024-12-01-16:22:46-root-INFO: Loss too large (3005.346->3007.500)! Learning rate decreased to 0.00110.
2024-12-01-16:22:47-root-INFO: grad norm: 230.340 220.970 65.028
2024-12-01-16:22:47-root-INFO: grad norm: 213.216 204.921 58.893
2024-12-01-16:22:48-root-INFO: grad norm: 201.399 193.252 56.704
2024-12-01-16:22:48-root-INFO: Loss Change: 3010.270 -> 2961.205
2024-12-01-16:22:48-root-INFO: Regularization Change: 0.000 -> 0.343
2024-12-01-16:22:48-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-01-16:22:48-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-01-16:22:48-root-INFO: step: 198 lr_xt 0.00224635
2024-12-01-16:22:48-root-INFO: grad norm: 218.261 207.319 68.240
2024-12-01-16:22:48-root-INFO: Loss too large (2957.961->3011.903)! Learning rate decreased to 0.00180.
2024-12-01-16:22:49-root-INFO: Loss too large (2957.961->2974.727)! Learning rate decreased to 0.00144.
2024-12-01-16:22:49-root-INFO: grad norm: 300.335 288.047 85.030
2024-12-01-16:22:49-root-INFO: Loss too large (2954.495->2974.685)! Learning rate decreased to 0.00115.
2024-12-01-16:22:50-root-INFO: grad norm: 309.490 297.996 83.559
2024-12-01-16:22:50-root-INFO: grad norm: 323.033 310.625 88.672
2024-12-01-16:22:51-root-INFO: grad norm: 338.157 325.467 91.765
2024-12-01-16:22:51-root-INFO: Loss Change: 2957.961 -> 2926.995
2024-12-01-16:22:51-root-INFO: Regularization Change: 0.000 -> 0.398
2024-12-01-16:22:51-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-01-16:22:51-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:22:51-root-INFO: step: 197 lr_xt 0.00234729
2024-12-01-16:22:51-root-INFO: grad norm: 310.504 300.341 78.790
2024-12-01-16:22:52-root-INFO: Loss too large (2902.729->3074.392)! Learning rate decreased to 0.00188.
2024-12-01-16:22:52-root-INFO: Loss too large (2902.729->2982.115)! Learning rate decreased to 0.00150.
2024-12-01-16:22:52-root-INFO: Loss too large (2902.729->2926.528)! Learning rate decreased to 0.00120.
2024-12-01-16:22:52-root-INFO: grad norm: 325.526 313.537 87.532
2024-12-01-16:22:53-root-INFO: grad norm: 349.660 338.191 88.821
2024-12-01-16:22:54-root-INFO: grad norm: 375.803 362.332 99.715
2024-12-01-16:22:54-root-INFO: grad norm: 403.565 390.535 101.718
2024-12-01-16:22:54-root-INFO: Loss Change: 2902.729 -> 2880.115
2024-12-01-16:22:54-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-01-16:22:54-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-01-16:22:54-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:22:54-root-INFO: step: 196 lr_xt 0.00245238
2024-12-01-16:22:55-root-INFO: grad norm: 441.433 424.825 119.942
2024-12-01-16:22:55-root-INFO: Loss too large (2875.831->3259.276)! Learning rate decreased to 0.00196.
2024-12-01-16:22:55-root-INFO: Loss too large (2875.831->3060.127)! Learning rate decreased to 0.00157.
2024-12-01-16:22:55-root-INFO: Loss too large (2875.831->2935.948)! Learning rate decreased to 0.00126.
2024-12-01-16:22:56-root-INFO: grad norm: 460.029 445.133 116.117
2024-12-01-16:22:56-root-INFO: grad norm: 474.877 458.817 122.456
2024-12-01-16:22:57-root-INFO: grad norm: 477.682 462.532 119.348
2024-12-01-16:22:57-root-INFO: grad norm: 468.228 452.109 121.798
2024-12-01-16:22:57-root-INFO: Loss Change: 2875.831 -> 2824.474
2024-12-01-16:22:57-root-INFO: Regularization Change: 0.000 -> 0.652
2024-12-01-16:22:57-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-01-16:22:57-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:22:57-root-INFO: step: 195 lr_xt 0.00256175
2024-12-01-16:22:58-root-INFO: grad norm: 431.725 417.882 108.449
2024-12-01-16:22:58-root-INFO: Loss too large (2814.045->3129.208)! Learning rate decreased to 0.00205.
2024-12-01-16:22:58-root-INFO: Loss too large (2814.045->2960.212)! Learning rate decreased to 0.00164.
2024-12-01-16:22:58-root-INFO: Loss too large (2814.045->2853.835)! Learning rate decreased to 0.00131.
2024-12-01-16:22:59-root-INFO: grad norm: 390.382 376.954 101.509
2024-12-01-16:22:59-root-INFO: grad norm: 356.745 345.873 87.399
2024-12-01-16:23:00-root-INFO: grad norm: 337.222 325.290 88.910
2024-12-01-16:23:00-root-INFO: grad norm: 326.780 317.106 78.922
2024-12-01-16:23:00-root-INFO: Loss Change: 2814.045 -> 2727.559
2024-12-01-16:23:00-root-INFO: Regularization Change: 0.000 -> 0.612
2024-12-01-16:23:00-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-01-16:23:00-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:23:01-root-INFO: step: 194 lr_xt 0.00267557
2024-12-01-16:23:01-root-INFO: grad norm: 340.717 327.927 92.474
2024-12-01-16:23:01-root-INFO: Loss too large (2720.115->2956.995)! Learning rate decreased to 0.00214.
2024-12-01-16:23:01-root-INFO: Loss too large (2720.115->2826.070)! Learning rate decreased to 0.00171.
2024-12-01-16:23:01-root-INFO: Loss too large (2720.115->2748.059)! Learning rate decreased to 0.00137.
2024-12-01-16:23:02-root-INFO: grad norm: 348.198 338.244 82.659
2024-12-01-16:23:02-root-INFO: grad norm: 370.707 358.332 94.985
2024-12-01-16:23:03-root-INFO: grad norm: 401.118 390.064 93.520
2024-12-01-16:23:03-root-INFO: grad norm: 431.331 417.181 109.574
2024-12-01-16:23:04-root-INFO: Loss Change: 2720.115 -> 2688.981
2024-12-01-16:23:04-root-INFO: Regularization Change: 0.000 -> 0.673
2024-12-01-16:23:04-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-01-16:23:04-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-01-16:23:04-root-INFO: step: 193 lr_xt 0.00279399
2024-12-01-16:23:04-root-INFO: grad norm: 420.572 410.767 90.284
2024-12-01-16:23:04-root-INFO: Loss too large (2665.668->3040.354)! Learning rate decreased to 0.00224.
2024-12-01-16:23:04-root-INFO: Loss too large (2665.668->2851.542)! Learning rate decreased to 0.00179.
2024-12-01-16:23:04-root-INFO: Loss too large (2665.668->2730.250)! Learning rate decreased to 0.00143.
2024-12-01-16:23:05-root-INFO: grad norm: 434.573 421.104 107.356
2024-12-01-16:23:05-root-INFO: grad norm: 448.883 437.429 100.757
2024-12-01-16:23:06-root-INFO: grad norm: 456.864 442.917 112.023
2024-12-01-16:23:06-root-INFO: grad norm: 454.074 442.735 100.841
2024-12-01-16:23:07-root-INFO: Loss Change: 2665.668 -> 2620.239
2024-12-01-16:23:07-root-INFO: Regularization Change: 0.000 -> 0.789
2024-12-01-16:23:07-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-01-16:23:07-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-01-16:23:07-root-INFO: step: 192 lr_xt 0.00291718
2024-12-01-16:23:07-root-INFO: grad norm: 461.239 447.742 110.765
2024-12-01-16:23:07-root-INFO: Loss too large (2615.735->3031.044)! Learning rate decreased to 0.00233.
2024-12-01-16:23:07-root-INFO: Loss too large (2615.735->2809.853)! Learning rate decreased to 0.00187.
2024-12-01-16:23:07-root-INFO: Loss too large (2615.735->2668.085)! Learning rate decreased to 0.00149.
2024-12-01-16:23:08-root-INFO: grad norm: 419.934 410.377 89.083
2024-12-01-16:23:08-root-INFO: grad norm: 391.662 380.159 94.222
2024-12-01-16:23:09-root-INFO: grad norm: 370.784 362.518 77.858
2024-12-01-16:23:09-root-INFO: grad norm: 361.684 351.059 87.021
2024-12-01-16:23:10-root-INFO: Loss Change: 2615.735 -> 2521.252
2024-12-01-16:23:10-root-INFO: Regularization Change: 0.000 -> 0.794
2024-12-01-16:23:10-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-01-16:23:10-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-16:23:10-root-INFO: step: 191 lr_xt 0.00304531
2024-12-01-16:23:10-root-INFO: grad norm: 342.594 335.501 69.356
2024-12-01-16:23:10-root-INFO: Loss too large (2503.751->2775.116)! Learning rate decreased to 0.00244.
2024-12-01-16:23:10-root-INFO: Loss too large (2503.751->2629.730)! Learning rate decreased to 0.00195.
2024-12-01-16:23:11-root-INFO: Loss too large (2503.751->2540.696)! Learning rate decreased to 0.00156.
2024-12-01-16:23:11-root-INFO: grad norm: 357.809 348.167 82.502
2024-12-01-16:23:11-root-INFO: grad norm: 396.151 387.412 82.751
2024-12-01-16:23:12-root-INFO: Loss too large (2487.493->2493.452)! Learning rate decreased to 0.00125.
2024-12-01-16:23:12-root-INFO: grad norm: 294.338 286.427 67.780
2024-12-01-16:23:13-root-INFO: grad norm: 234.270 229.081 49.033
2024-12-01-16:23:13-root-INFO: Loss Change: 2503.751 -> 2415.516
2024-12-01-16:23:13-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-01-16:23:13-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-01-16:23:13-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-16:23:13-root-INFO: step: 190 lr_xt 0.00317856
2024-12-01-16:23:13-root-INFO: grad norm: 245.988 238.567 59.969
2024-12-01-16:23:13-root-INFO: Loss too large (2413.050->2592.026)! Learning rate decreased to 0.00254.
2024-12-01-16:23:13-root-INFO: Loss too large (2413.050->2499.107)! Learning rate decreased to 0.00203.
2024-12-01-16:23:14-root-INFO: Loss too large (2413.050->2443.436)! Learning rate decreased to 0.00163.
2024-12-01-16:23:14-root-INFO: grad norm: 318.190 311.543 64.696
2024-12-01-16:23:14-root-INFO: Loss too large (2412.208->2433.462)! Learning rate decreased to 0.00130.
2024-12-01-16:23:15-root-INFO: grad norm: 300.804 293.725 64.874
2024-12-01-16:23:15-root-INFO: grad norm: 294.845 288.920 58.814
2024-12-01-16:23:16-root-INFO: grad norm: 299.149 292.155 64.307
2024-12-01-16:23:16-root-INFO: Loss Change: 2413.050 -> 2374.682
2024-12-01-16:23:16-root-INFO: Regularization Change: 0.000 -> 0.435
2024-12-01-16:23:16-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-01-16:23:16-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-16:23:16-root-INFO: step: 189 lr_xt 0.00331709
2024-12-01-16:23:16-root-INFO: grad norm: 294.215 288.308 58.662
2024-12-01-16:23:17-root-INFO: Loss too large (2358.963->2701.591)! Learning rate decreased to 0.00265.
2024-12-01-16:23:17-root-INFO: Loss too large (2358.963->2547.309)! Learning rate decreased to 0.00212.
2024-12-01-16:23:17-root-INFO: Loss too large (2358.963->2447.917)! Learning rate decreased to 0.00170.
2024-12-01-16:23:17-root-INFO: Loss too large (2358.963->2387.797)! Learning rate decreased to 0.00136.
2024-12-01-16:23:18-root-INFO: grad norm: 319.434 312.775 64.881
2024-12-01-16:23:18-root-INFO: grad norm: 360.178 353.374 69.679
2024-12-01-16:23:18-root-INFO: Loss too large (2353.189->2359.049)! Learning rate decreased to 0.00109.
2024-12-01-16:23:19-root-INFO: grad norm: 271.258 265.482 55.679
2024-12-01-16:23:19-root-INFO: grad norm: 215.701 211.354 43.085
2024-12-01-16:23:19-root-INFO: Loss Change: 2358.963 -> 2302.219
2024-12-01-16:23:19-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-01-16:23:19-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-01-16:23:19-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-01-16:23:20-root-INFO: step: 188 lr_xt 0.00346111
2024-12-01-16:23:20-root-INFO: grad norm: 261.265 255.223 55.860
2024-12-01-16:23:20-root-INFO: Loss too large (2300.499->2617.698)! Learning rate decreased to 0.00277.
2024-12-01-16:23:20-root-INFO: Loss too large (2300.499->2478.036)! Learning rate decreased to 0.00222.
2024-12-01-16:23:20-root-INFO: Loss too large (2300.499->2387.591)! Learning rate decreased to 0.00177.
2024-12-01-16:23:20-root-INFO: Loss too large (2300.499->2332.586)! Learning rate decreased to 0.00142.
2024-12-01-16:23:20-root-INFO: Loss too large (2300.499->2301.329)! Learning rate decreased to 0.00113.
2024-12-01-16:23:21-root-INFO: grad norm: 221.789 217.769 42.036
2024-12-01-16:23:21-root-INFO: grad norm: 202.369 197.924 42.181
2024-12-01-16:23:22-root-INFO: grad norm: 192.635 188.827 38.110
2024-12-01-16:23:22-root-INFO: grad norm: 190.808 186.653 39.603
2024-12-01-16:23:23-root-INFO: Loss Change: 2300.499 -> 2253.777
2024-12-01-16:23:23-root-INFO: Regularization Change: 0.000 -> 0.263
2024-12-01-16:23:23-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-01-16:23:23-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-01-16:23:23-root-INFO: step: 187 lr_xt 0.00361079
2024-12-01-16:23:23-root-INFO: grad norm: 197.568 192.434 44.748
2024-12-01-16:23:23-root-INFO: Loss too large (2250.829->2460.704)! Learning rate decreased to 0.00289.
2024-12-01-16:23:23-root-INFO: Loss too large (2250.829->2367.189)! Learning rate decreased to 0.00231.
2024-12-01-16:23:23-root-INFO: Loss too large (2250.829->2308.695)! Learning rate decreased to 0.00185.
2024-12-01-16:23:23-root-INFO: Loss too large (2250.829->2273.731)! Learning rate decreased to 0.00148.
2024-12-01-16:23:24-root-INFO: Loss too large (2250.829->2253.900)! Learning rate decreased to 0.00118.
2024-12-01-16:23:24-root-INFO: grad norm: 209.306 205.057 41.956
2024-12-01-16:23:25-root-INFO: grad norm: 234.264 229.556 46.733
2024-12-01-16:23:25-root-INFO: grad norm: 270.337 265.847 49.066
2024-12-01-16:23:25-root-INFO: Loss too large (2237.555->2238.710)! Learning rate decreased to 0.00095.
2024-12-01-16:23:26-root-INFO: grad norm: 210.051 205.739 42.340
2024-12-01-16:23:26-root-INFO: Loss Change: 2250.829 -> 2215.339
2024-12-01-16:23:26-root-INFO: Regularization Change: 0.000 -> 0.207
2024-12-01-16:23:26-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-01-16:23:26-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:23:26-root-INFO: step: 186 lr_xt 0.00376634
2024-12-01-16:23:26-root-INFO: grad norm: 246.895 242.078 48.529
2024-12-01-16:23:26-root-INFO: Loss too large (2210.309->2606.919)! Learning rate decreased to 0.00301.
2024-12-01-16:23:27-root-INFO: Loss too large (2210.309->2449.788)! Learning rate decreased to 0.00241.
2024-12-01-16:23:27-root-INFO: Loss too large (2210.309->2342.667)! Learning rate decreased to 0.00193.
2024-12-01-16:23:27-root-INFO: Loss too large (2210.309->2274.091)! Learning rate decreased to 0.00154.
2024-12-01-16:23:27-root-INFO: Loss too large (2210.309->2232.764)! Learning rate decreased to 0.00123.
2024-12-01-16:23:27-root-INFO: grad norm: 298.402 293.489 53.927
2024-12-01-16:23:28-root-INFO: Loss too large (2209.465->2217.141)! Learning rate decreased to 0.00099.
2024-12-01-16:23:28-root-INFO: grad norm: 248.300 244.124 45.349
2024-12-01-16:23:29-root-INFO: grad norm: 211.915 207.921 40.947
2024-12-01-16:23:29-root-INFO: grad norm: 187.505 184.073 35.710
2024-12-01-16:23:29-root-INFO: Loss Change: 2210.309 -> 2172.650
2024-12-01-16:23:29-root-INFO: Regularization Change: 0.000 -> 0.187
2024-12-01-16:23:29-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-01-16:23:29-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:23:30-root-INFO: step: 185 lr_xt 0.00392795
2024-12-01-16:23:30-root-INFO: grad norm: 142.978 138.897 33.920
2024-12-01-16:23:30-root-INFO: Loss too large (2156.907->2284.595)! Learning rate decreased to 0.00314.
2024-12-01-16:23:30-root-INFO: Loss too large (2156.907->2227.342)! Learning rate decreased to 0.00251.
2024-12-01-16:23:30-root-INFO: Loss too large (2156.907->2192.163)! Learning rate decreased to 0.00201.
2024-12-01-16:23:30-root-INFO: Loss too large (2156.907->2171.324)! Learning rate decreased to 0.00161.
2024-12-01-16:23:30-root-INFO: Loss too large (2156.907->2159.525)! Learning rate decreased to 0.00129.
2024-12-01-16:23:31-root-INFO: grad norm: 183.395 180.229 33.930
2024-12-01-16:23:31-root-INFO: Loss too large (2153.275->2154.670)! Learning rate decreased to 0.00103.
2024-12-01-16:23:32-root-INFO: grad norm: 175.446 171.668 36.212
2024-12-01-16:23:32-root-INFO: grad norm: 172.033 169.052 31.884
2024-12-01-16:23:32-root-INFO: grad norm: 172.070 168.382 35.435
2024-12-01-16:23:33-root-INFO: Loss Change: 2156.907 -> 2132.424
2024-12-01-16:23:33-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-01-16:23:33-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-01-16:23:33-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:23:33-root-INFO: step: 184 lr_xt 0.00409583
2024-12-01-16:23:33-root-INFO: grad norm: 283.684 278.239 55.313
2024-12-01-16:23:33-root-INFO: Loss too large (2139.305->2716.750)! Learning rate decreased to 0.00328.
2024-12-01-16:23:33-root-INFO: Loss too large (2139.305->2515.022)! Learning rate decreased to 0.00262.
2024-12-01-16:23:33-root-INFO: Loss too large (2139.305->2364.427)! Learning rate decreased to 0.00210.
2024-12-01-16:23:34-root-INFO: Loss too large (2139.305->2260.297)! Learning rate decreased to 0.00168.
2024-12-01-16:23:34-root-INFO: Loss too large (2139.305->2193.278)! Learning rate decreased to 0.00134.
2024-12-01-16:23:34-root-INFO: Loss too large (2139.305->2153.080)! Learning rate decreased to 0.00107.
2024-12-01-16:23:34-root-INFO: grad norm: 288.926 284.251 51.767
2024-12-01-16:23:35-root-INFO: grad norm: 306.423 302.133 51.092
2024-12-01-16:23:35-root-INFO: grad norm: 325.952 320.882 57.267
2024-12-01-16:23:36-root-INFO: grad norm: 345.588 341.120 55.389
2024-12-01-16:23:36-root-INFO: Loss Change: 2139.305 -> 2123.508
2024-12-01-16:23:36-root-INFO: Regularization Change: 0.000 -> 0.263
2024-12-01-16:23:36-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-01-16:23:36-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:23:36-root-INFO: step: 183 lr_xt 0.00427020
2024-12-01-16:23:36-root-INFO: grad norm: 300.568 295.958 52.441
2024-12-01-16:23:37-root-INFO: Loss too large (2103.156->2837.718)! Learning rate decreased to 0.00342.
2024-12-01-16:23:37-root-INFO: Loss too large (2103.156->2577.030)! Learning rate decreased to 0.00273.
2024-12-01-16:23:37-root-INFO: Loss too large (2103.156->2389.288)! Learning rate decreased to 0.00219.
2024-12-01-16:23:37-root-INFO: Loss too large (2103.156->2261.310)! Learning rate decreased to 0.00175.
2024-12-01-16:23:37-root-INFO: Loss too large (2103.156->2178.661)! Learning rate decreased to 0.00140.
2024-12-01-16:23:37-root-INFO: Loss too large (2103.156->2128.258)! Learning rate decreased to 0.00112.
2024-12-01-16:23:38-root-INFO: grad norm: 313.976 309.719 51.524
2024-12-01-16:23:38-root-INFO: grad norm: 331.295 326.556 55.832
2024-12-01-16:23:39-root-INFO: grad norm: 349.205 344.864 54.892
2024-12-01-16:23:40-root-INFO: grad norm: 366.154 361.078 60.752
2024-12-01-16:23:40-root-INFO: Loss Change: 2103.156 -> 2093.422
2024-12-01-16:23:40-root-INFO: Regularization Change: 0.000 -> 0.280
2024-12-01-16:23:40-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-01-16:23:40-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-01-16:23:40-root-INFO: step: 182 lr_xt 0.00445127
2024-12-01-16:23:40-root-INFO: grad norm: 438.171 433.035 66.893
2024-12-01-16:23:40-root-INFO: Loss too large (2102.080->3190.089)! Learning rate decreased to 0.00356.
2024-12-01-16:23:41-root-INFO: Loss too large (2102.080->2890.406)! Learning rate decreased to 0.00285.
2024-12-01-16:23:41-root-INFO: Loss too large (2102.080->2631.144)! Learning rate decreased to 0.00228.
2024-12-01-16:23:41-root-INFO: Loss too large (2102.080->2420.149)! Learning rate decreased to 0.00182.
2024-12-01-16:23:41-root-INFO: Loss too large (2102.080->2262.927)! Learning rate decreased to 0.00146.
2024-12-01-16:23:41-root-INFO: Loss too large (2102.080->2157.176)! Learning rate decreased to 0.00117.
2024-12-01-16:23:42-root-INFO: grad norm: 431.901 426.593 67.500
2024-12-01-16:23:42-root-INFO: grad norm: 423.479 418.675 63.611
2024-12-01-16:23:43-root-INFO: grad norm: 410.920 405.764 64.894
2024-12-01-16:23:43-root-INFO: grad norm: 398.557 394.036 59.862
2024-12-01-16:23:44-root-INFO: Loss Change: 2102.080 -> 2065.068
2024-12-01-16:23:44-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-01-16:23:44-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-01-16:23:44-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-01-16:23:44-root-INFO: step: 181 lr_xt 0.00463927
2024-12-01-16:23:44-root-INFO: grad norm: 342.689 337.948 56.807
2024-12-01-16:23:44-root-INFO: Loss too large (2047.607->2994.701)! Learning rate decreased to 0.00371.
2024-12-01-16:23:44-root-INFO: Loss too large (2047.607->2668.946)! Learning rate decreased to 0.00297.
2024-12-01-16:23:44-root-INFO: Loss too large (2047.607->2428.473)! Learning rate decreased to 0.00238.
2024-12-01-16:23:45-root-INFO: Loss too large (2047.607->2260.669)! Learning rate decreased to 0.00190.
2024-12-01-16:23:45-root-INFO: Loss too large (2047.607->2149.927)! Learning rate decreased to 0.00152.
2024-12-01-16:23:45-root-INFO: Loss too large (2047.607->2081.156)! Learning rate decreased to 0.00122.
2024-12-01-16:23:45-root-INFO: grad norm: 336.622 332.890 49.986
2024-12-01-16:23:46-root-INFO: grad norm: 335.742 331.268 54.627
2024-12-01-16:23:46-root-INFO: grad norm: 338.823 335.168 49.629
2024-12-01-16:23:47-root-INFO: grad norm: 343.797 339.264 55.644
2024-12-01-16:23:47-root-INFO: Loss Change: 2047.607 -> 2024.811
2024-12-01-16:23:47-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-01-16:23:47-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-01-16:23:47-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-16:23:47-root-INFO: step: 180 lr_xt 0.00483443
2024-12-01-16:23:47-root-INFO: grad norm: 419.253 414.347 63.951
2024-12-01-16:23:48-root-INFO: Loss too large (2039.865->3076.413)! Learning rate decreased to 0.00387.
2024-12-01-16:23:48-root-INFO: Loss too large (2039.865->2795.391)! Learning rate decreased to 0.00309.
2024-12-01-16:23:48-root-INFO: Loss too large (2039.865->2549.901)! Learning rate decreased to 0.00248.
2024-12-01-16:23:48-root-INFO: Loss too large (2039.865->2347.963)! Learning rate decreased to 0.00198.
2024-12-01-16:23:48-root-INFO: Loss too large (2039.865->2195.262)! Learning rate decreased to 0.00158.
2024-12-01-16:23:48-root-INFO: Loss too large (2039.865->2091.533)! Learning rate decreased to 0.00127.
2024-12-01-16:23:49-root-INFO: grad norm: 411.341 406.447 63.262
2024-12-01-16:23:49-root-INFO: grad norm: 405.958 401.646 59.011
2024-12-01-16:23:50-root-INFO: grad norm: 397.909 393.019 62.191
2024-12-01-16:23:50-root-INFO: grad norm: 389.771 385.694 56.227
2024-12-01-16:23:51-root-INFO: Loss Change: 2039.865 -> 2003.057
2024-12-01-16:23:51-root-INFO: Regularization Change: 0.000 -> 0.418
2024-12-01-16:23:51-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-01-16:23:51-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-16:23:51-root-INFO: step: 179 lr_xt 0.00503698
2024-12-01-16:23:51-root-INFO: grad norm: 333.046 328.885 52.484
2024-12-01-16:23:51-root-INFO: Loss too large (1983.229->2954.742)! Learning rate decreased to 0.00403.
2024-12-01-16:23:51-root-INFO: Loss too large (1983.229->2622.451)! Learning rate decreased to 0.00322.
2024-12-01-16:23:52-root-INFO: Loss too large (1983.229->2377.138)! Learning rate decreased to 0.00258.
2024-12-01-16:23:52-root-INFO: Loss too large (1983.229->2205.510)! Learning rate decreased to 0.00206.
2024-12-01-16:23:52-root-INFO: Loss too large (1983.229->2091.725)! Learning rate decreased to 0.00165.
2024-12-01-16:23:52-root-INFO: Loss too large (1983.229->2020.618)! Learning rate decreased to 0.00132.
2024-12-01-16:23:53-root-INFO: grad norm: 331.655 328.235 47.506
2024-12-01-16:23:53-root-INFO: grad norm: 333.191 328.996 52.700
2024-12-01-16:23:54-root-INFO: grad norm: 337.047 333.688 47.472
2024-12-01-16:23:54-root-INFO: grad norm: 341.551 337.259 53.976
2024-12-01-16:23:55-root-INFO: Loss Change: 1983.229 -> 1963.733
2024-12-01-16:23:55-root-INFO: Regularization Change: 0.000 -> 0.351
2024-12-01-16:23:55-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-01-16:23:55-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-16:23:55-root-INFO: step: 178 lr_xt 0.00524717
2024-12-01-16:23:55-root-INFO: grad norm: 406.452 402.022 59.845
2024-12-01-16:23:55-root-INFO: Loss too large (1976.548->2979.773)! Learning rate decreased to 0.00420.
2024-12-01-16:23:55-root-INFO: Loss too large (1976.548->2713.028)! Learning rate decreased to 0.00336.
2024-12-01-16:23:55-root-INFO: Loss too large (1976.548->2476.483)! Learning rate decreased to 0.00269.
2024-12-01-16:23:56-root-INFO: Loss too large (1976.548->2278.589)! Learning rate decreased to 0.00215.
2024-12-01-16:23:56-root-INFO: Loss too large (1976.548->2126.551)! Learning rate decreased to 0.00172.
2024-12-01-16:23:56-root-INFO: Loss too large (1976.548->2022.715)! Learning rate decreased to 0.00138.
2024-12-01-16:23:56-root-INFO: grad norm: 383.799 379.533 57.063
2024-12-01-16:23:57-root-INFO: grad norm: 366.995 363.360 51.529
2024-12-01-16:23:57-root-INFO: grad norm: 350.305 346.136 53.888
2024-12-01-16:23:58-root-INFO: grad norm: 337.061 333.792 46.833
2024-12-01-16:23:58-root-INFO: Loss Change: 1976.548 -> 1927.163
2024-12-01-16:23:58-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-01-16:23:58-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-01-16:23:58-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-01-16:23:58-root-INFO: step: 177 lr_xt 0.00546525
2024-12-01-16:23:59-root-INFO: grad norm: 284.641 281.028 45.204
2024-12-01-16:23:59-root-INFO: Loss too large (1910.521->2709.379)! Learning rate decreased to 0.00437.
2024-12-01-16:23:59-root-INFO: Loss too large (1910.521->2424.242)! Learning rate decreased to 0.00350.
2024-12-01-16:23:59-root-INFO: Loss too large (1910.521->2220.229)! Learning rate decreased to 0.00280.
2024-12-01-16:23:59-root-INFO: Loss too large (1910.521->2081.334)! Learning rate decreased to 0.00224.
2024-12-01-16:23:59-root-INFO: Loss too large (1910.521->1991.431)! Learning rate decreased to 0.00179.
2024-12-01-16:23:59-root-INFO: Loss too large (1910.521->1936.392)! Learning rate decreased to 0.00143.
2024-12-01-16:24:00-root-INFO: grad norm: 275.483 272.730 38.850
2024-12-01-16:24:00-root-INFO: grad norm: 269.791 266.206 43.830
2024-12-01-16:24:01-root-INFO: grad norm: 267.564 265.009 36.892
2024-12-01-16:24:01-root-INFO: grad norm: 266.514 262.909 43.688
2024-12-01-16:24:02-root-INFO: Loss Change: 1910.521 -> 1884.291
2024-12-01-16:24:02-root-INFO: Regularization Change: 0.000 -> 0.320
2024-12-01-16:24:02-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-01-16:24:02-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-01-16:24:02-root-INFO: step: 176 lr_xt 0.00569148
2024-12-01-16:24:02-root-INFO: grad norm: 327.048 323.659 46.959
2024-12-01-16:24:02-root-INFO: Loss too large (1894.586->2672.507)! Learning rate decreased to 0.00455.
2024-12-01-16:24:02-root-INFO: Loss too large (1894.586->2455.909)! Learning rate decreased to 0.00364.
2024-12-01-16:24:02-root-INFO: Loss too large (1894.586->2266.105)! Learning rate decreased to 0.00291.
2024-12-01-16:24:03-root-INFO: Loss too large (1894.586->2111.504)! Learning rate decreased to 0.00233.
2024-12-01-16:24:03-root-INFO: Loss too large (1894.586->1998.089)! Learning rate decreased to 0.00186.
2024-12-01-16:24:03-root-INFO: Loss too large (1894.586->1924.362)! Learning rate decreased to 0.00149.
2024-12-01-16:24:03-root-INFO: grad norm: 302.698 298.999 47.178
2024-12-01-16:24:04-root-INFO: grad norm: 283.483 280.900 38.176
2024-12-01-16:24:04-root-INFO: grad norm: 268.228 264.661 43.596
2024-12-01-16:24:05-root-INFO: grad norm: 256.068 253.756 34.332
2024-12-01-16:24:05-root-INFO: Loss Change: 1894.586 -> 1850.687
2024-12-01-16:24:05-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-01-16:24:05-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-01-16:24:05-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-16:24:05-root-INFO: step: 175 lr_xt 0.00592610
2024-12-01-16:24:05-root-INFO: grad norm: 179.623 176.500 33.352
2024-12-01-16:24:06-root-INFO: Loss too large (1834.083->2196.015)! Learning rate decreased to 0.00474.
2024-12-01-16:24:06-root-INFO: Loss too large (1834.083->2050.581)! Learning rate decreased to 0.00379.
2024-12-01-16:24:06-root-INFO: Loss too large (1834.083->1955.234)! Learning rate decreased to 0.00303.
2024-12-01-16:24:06-root-INFO: Loss too large (1834.083->1895.120)! Learning rate decreased to 0.00243.
2024-12-01-16:24:06-root-INFO: Loss too large (1834.083->1858.761)! Learning rate decreased to 0.00194.
2024-12-01-16:24:06-root-INFO: Loss too large (1834.083->1837.849)! Learning rate decreased to 0.00155.
2024-12-01-16:24:07-root-INFO: grad norm: 162.927 161.111 24.255
2024-12-01-16:24:07-root-INFO: grad norm: 151.683 148.760 29.635
2024-12-01-16:24:08-root-INFO: grad norm: 144.468 142.785 21.986
2024-12-01-16:24:08-root-INFO: grad norm: 138.712 135.856 28.002
2024-12-01-16:24:09-root-INFO: Loss Change: 1834.083 -> 1804.433
2024-12-01-16:24:09-root-INFO: Regularization Change: 0.000 -> 0.241
2024-12-01-16:24:09-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-01-16:24:09-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-16:24:09-root-INFO: step: 174 lr_xt 0.00616941
2024-12-01-16:24:09-root-INFO: grad norm: 151.616 149.898 22.761
2024-12-01-16:24:09-root-INFO: Loss too large (1803.917->2070.670)! Learning rate decreased to 0.00494.
2024-12-01-16:24:09-root-INFO: Loss too large (1803.917->1967.779)! Learning rate decreased to 0.00395.
2024-12-01-16:24:09-root-INFO: Loss too large (1803.917->1895.758)! Learning rate decreased to 0.00316.
2024-12-01-16:24:10-root-INFO: Loss too large (1803.917->1849.350)! Learning rate decreased to 0.00253.
2024-12-01-16:24:10-root-INFO: Loss too large (1803.917->1821.414)! Learning rate decreased to 0.00202.
2024-12-01-16:24:10-root-INFO: Loss too large (1803.917->1805.642)! Learning rate decreased to 0.00162.
2024-12-01-16:24:10-root-INFO: grad norm: 141.970 139.151 28.148
2024-12-01-16:24:11-root-INFO: grad norm: 136.047 134.421 20.965
2024-12-01-16:24:11-root-INFO: grad norm: 130.793 128.040 26.693
2024-12-01-16:24:12-root-INFO: grad norm: 127.286 125.685 20.128
2024-12-01-16:24:12-root-INFO: Loss Change: 1803.917 -> 1777.446
2024-12-01-16:24:12-root-INFO: Regularization Change: 0.000 -> 0.231
2024-12-01-16:24:12-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-01-16:24:12-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-01-16:24:13-root-INFO: step: 173 lr_xt 0.00642166
2024-12-01-16:24:13-root-INFO: grad norm: 80.584 76.881 24.147
2024-12-01-16:24:13-root-INFO: Loss too large (1771.876->1784.988)! Learning rate decreased to 0.00514.
2024-12-01-16:24:13-root-INFO: Loss too large (1771.876->1773.954)! Learning rate decreased to 0.00411.
2024-12-01-16:24:14-root-INFO: grad norm: 164.590 162.763 24.452
2024-12-01-16:24:14-root-INFO: Loss too large (1768.025->1873.916)! Learning rate decreased to 0.00329.
2024-12-01-16:24:14-root-INFO: Loss too large (1768.025->1818.165)! Learning rate decreased to 0.00263.
2024-12-01-16:24:14-root-INFO: Loss too large (1768.025->1785.235)! Learning rate decreased to 0.00210.
2024-12-01-16:24:15-root-INFO: grad norm: 204.651 201.713 34.552
2024-12-01-16:24:15-root-INFO: Loss too large (1767.025->1772.747)! Learning rate decreased to 0.00168.
2024-12-01-16:24:15-root-INFO: grad norm: 166.576 164.773 24.443
2024-12-01-16:24:16-root-INFO: grad norm: 132.421 129.673 26.834
2024-12-01-16:24:16-root-INFO: Loss Change: 1771.876 -> 1740.306
2024-12-01-16:24:16-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-01-16:24:16-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-01-16:24:16-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-01-16:24:16-root-INFO: step: 172 lr_xt 0.00668315
2024-12-01-16:24:16-root-INFO: grad norm: 118.403 117.013 18.086
2024-12-01-16:24:17-root-INFO: Loss too large (1733.937->1883.871)! Learning rate decreased to 0.00535.
2024-12-01-16:24:17-root-INFO: Loss too large (1733.937->1817.755)! Learning rate decreased to 0.00428.
2024-12-01-16:24:17-root-INFO: Loss too large (1733.937->1776.227)! Learning rate decreased to 0.00342.
2024-12-01-16:24:17-root-INFO: Loss too large (1733.937->1751.394)! Learning rate decreased to 0.00274.
2024-12-01-16:24:17-root-INFO: Loss too large (1733.937->1737.294)! Learning rate decreased to 0.00219.
2024-12-01-16:24:18-root-INFO: grad norm: 140.901 138.127 27.819
2024-12-01-16:24:18-root-INFO: grad norm: 168.225 166.579 23.476
2024-12-01-16:24:19-root-INFO: grad norm: 210.546 207.733 34.297
2024-12-01-16:24:19-root-INFO: Loss too large (1727.968->1734.799)! Learning rate decreased to 0.00175.
2024-12-01-16:24:19-root-INFO: grad norm: 169.734 167.802 25.535
2024-12-01-16:24:20-root-INFO: Loss Change: 1733.937 -> 1705.810
2024-12-01-16:24:20-root-INFO: Regularization Change: 0.000 -> 0.340
2024-12-01-16:24:20-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-01-16:24:20-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-16:24:20-root-INFO: step: 171 lr_xt 0.00695416
2024-12-01-16:24:20-root-INFO: grad norm: 188.104 185.527 31.031
2024-12-01-16:24:20-root-INFO: Loss too large (1701.365->1818.036)! Learning rate decreased to 0.00556.
2024-12-01-16:24:20-root-INFO: Loss too large (1701.365->1795.161)! Learning rate decreased to 0.00445.
2024-12-01-16:24:21-root-INFO: Loss too large (1701.365->1772.551)! Learning rate decreased to 0.00356.
2024-12-01-16:24:21-root-INFO: Loss too large (1701.365->1750.473)! Learning rate decreased to 0.00285.
2024-12-01-16:24:21-root-INFO: Loss too large (1701.365->1730.484)! Learning rate decreased to 0.00228.
2024-12-01-16:24:21-root-INFO: Loss too large (1701.365->1714.133)! Learning rate decreased to 0.00182.
2024-12-01-16:24:21-root-INFO: Loss too large (1701.365->1702.153)! Learning rate decreased to 0.00146.
2024-12-01-16:24:22-root-INFO: grad norm: 135.277 132.877 25.364
2024-12-01-16:24:22-root-INFO: grad norm: 83.887 81.621 19.364
2024-12-01-16:24:23-root-INFO: grad norm: 75.725 73.223 19.302
2024-12-01-16:24:23-root-INFO: grad norm: 68.655 66.276 17.917
2024-12-01-16:24:24-root-INFO: Loss Change: 1701.365 -> 1667.279
2024-12-01-16:24:24-root-INFO: Regularization Change: 0.000 -> 0.194
2024-12-01-16:24:24-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-01-16:24:24-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-16:24:24-root-INFO: step: 170 lr_xt 0.00723499
2024-12-01-16:24:24-root-INFO: grad norm: 77.041 74.601 19.233
2024-12-01-16:24:24-root-INFO: Loss too large (1663.068->1672.487)! Learning rate decreased to 0.00579.
2024-12-01-16:24:24-root-INFO: Loss too large (1663.068->1663.182)! Learning rate decreased to 0.00463.
2024-12-01-16:24:25-root-INFO: grad norm: 169.722 166.746 31.641
2024-12-01-16:24:25-root-INFO: Loss too large (1658.112->1829.385)! Learning rate decreased to 0.00370.
2024-12-01-16:24:25-root-INFO: Loss too large (1658.112->1747.068)! Learning rate decreased to 0.00296.
2024-12-01-16:24:25-root-INFO: Loss too large (1658.112->1695.665)! Learning rate decreased to 0.00237.
2024-12-01-16:24:25-root-INFO: Loss too large (1658.112->1666.075)! Learning rate decreased to 0.00190.
2024-12-01-16:24:26-root-INFO: grad norm: 195.888 193.737 28.947
2024-12-01-16:24:26-root-INFO: Loss too large (1650.582->1654.433)! Learning rate decreased to 0.00152.
2024-12-01-16:24:27-root-INFO: grad norm: 152.902 150.238 28.414
2024-12-01-16:24:27-root-INFO: grad norm: 107.658 105.654 20.675
2024-12-01-16:24:27-root-INFO: Loss Change: 1663.068 -> 1625.366
2024-12-01-16:24:27-root-INFO: Regularization Change: 0.000 -> 0.450
2024-12-01-16:24:27-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-01-16:24:27-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-16:24:28-root-INFO: step: 169 lr_xt 0.00752595
2024-12-01-16:24:28-root-INFO: grad norm: 75.107 72.688 18.905
2024-12-01-16:24:28-root-INFO: Loss too large (1619.055->1643.111)! Learning rate decreased to 0.00602.
2024-12-01-16:24:28-root-INFO: Loss too large (1619.055->1627.356)! Learning rate decreased to 0.00482.
2024-12-01-16:24:29-root-INFO: grad norm: 240.593 238.096 34.573
2024-12-01-16:24:29-root-INFO: Loss too large (1618.479->1716.486)! Learning rate decreased to 0.00385.
2024-12-01-16:24:29-root-INFO: Loss too large (1618.479->1697.330)! Learning rate decreased to 0.00308.
2024-12-01-16:24:29-root-INFO: Loss too large (1618.479->1675.417)! Learning rate decreased to 0.00247.
2024-12-01-16:24:29-root-INFO: Loss too large (1618.479->1652.913)! Learning rate decreased to 0.00197.
2024-12-01-16:24:29-root-INFO: Loss too large (1618.479->1632.734)! Learning rate decreased to 0.00158.
2024-12-01-16:24:30-root-INFO: grad norm: 184.760 182.015 31.729
2024-12-01-16:24:30-root-INFO: grad norm: 109.400 107.343 21.115
2024-12-01-16:24:31-root-INFO: grad norm: 107.593 105.432 21.455
2024-12-01-16:24:31-root-INFO: Loss Change: 1619.055 -> 1582.662
2024-12-01-16:24:31-root-INFO: Regularization Change: 0.000 -> 0.439
2024-12-01-16:24:31-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-01-16:24:31-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-01-16:24:31-root-INFO: step: 168 lr_xt 0.00782735
2024-12-01-16:24:31-root-INFO: grad norm: 152.268 150.118 25.497
2024-12-01-16:24:32-root-INFO: Loss too large (1582.224->1700.855)! Learning rate decreased to 0.00626.
2024-12-01-16:24:32-root-INFO: Loss too large (1582.224->1679.347)! Learning rate decreased to 0.00501.
2024-12-01-16:24:32-root-INFO: Loss too large (1582.224->1657.040)! Learning rate decreased to 0.00401.
2024-12-01-16:24:32-root-INFO: Loss too large (1582.224->1635.068)! Learning rate decreased to 0.00321.
2024-12-01-16:24:32-root-INFO: Loss too large (1582.224->1615.309)! Learning rate decreased to 0.00256.
2024-12-01-16:24:32-root-INFO: Loss too large (1582.224->1599.317)! Learning rate decreased to 0.00205.
2024-12-01-16:24:33-root-INFO: Loss too large (1582.224->1587.678)! Learning rate decreased to 0.00164.
2024-12-01-16:24:33-root-INFO: grad norm: 146.075 143.916 25.020
2024-12-01-16:24:33-root-INFO: grad norm: 138.104 136.090 23.501
2024-12-01-16:24:34-root-INFO: grad norm: 136.448 134.370 23.723
2024-12-01-16:24:34-root-INFO: grad norm: 134.007 132.059 22.767
2024-12-01-16:24:35-root-INFO: Loss Change: 1582.224 -> 1559.220
2024-12-01-16:24:35-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-01-16:24:35-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-01-16:24:35-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-01-16:24:35-root-INFO: step: 167 lr_xt 0.00813950
2024-12-01-16:24:35-root-INFO: grad norm: 104.369 102.555 19.374
2024-12-01-16:24:35-root-INFO: Loss too large (1553.034->1745.645)! Learning rate decreased to 0.00651.
2024-12-01-16:24:35-root-INFO: Loss too large (1553.034->1677.289)! Learning rate decreased to 0.00521.
2024-12-01-16:24:36-root-INFO: Loss too large (1553.034->1627.701)! Learning rate decreased to 0.00417.
2024-12-01-16:24:36-root-INFO: Loss too large (1553.034->1593.608)! Learning rate decreased to 0.00333.
2024-12-01-16:24:36-root-INFO: Loss too large (1553.034->1571.682)! Learning rate decreased to 0.00267.
2024-12-01-16:24:36-root-INFO: Loss too large (1553.034->1558.585)! Learning rate decreased to 0.00213.
2024-12-01-16:24:37-root-INFO: grad norm: 165.049 162.723 27.609
2024-12-01-16:24:37-root-INFO: Loss too large (1551.396->1561.734)! Learning rate decreased to 0.00171.
2024-12-01-16:24:37-root-INFO: Loss too large (1551.396->1551.531)! Learning rate decreased to 0.00137.
2024-12-01-16:24:38-root-INFO: grad norm: 125.116 123.307 21.201
2024-12-01-16:24:38-root-INFO: grad norm: 88.475 86.437 18.883
2024-12-01-16:24:39-root-INFO: grad norm: 75.778 74.024 16.209
2024-12-01-16:24:39-root-INFO: Loss Change: 1553.034 -> 1528.942
2024-12-01-16:24:39-root-INFO: Regularization Change: 0.000 -> 0.173
2024-12-01-16:24:39-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-01-16:24:39-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-16:24:39-root-INFO: step: 166 lr_xt 0.00846273
2024-12-01-16:24:39-root-INFO: grad norm: 134.919 132.649 24.644
2024-12-01-16:24:39-root-INFO: Loss too large (1528.896->1644.622)! Learning rate decreased to 0.00677.
2024-12-01-16:24:40-root-INFO: Loss too large (1528.896->1624.148)! Learning rate decreased to 0.00542.
2024-12-01-16:24:40-root-INFO: Loss too large (1528.896->1602.310)! Learning rate decreased to 0.00433.
2024-12-01-16:24:40-root-INFO: Loss too large (1528.896->1580.651)! Learning rate decreased to 0.00347.
2024-12-01-16:24:40-root-INFO: Loss too large (1528.896->1561.307)! Learning rate decreased to 0.00277.
2024-12-01-16:24:40-root-INFO: Loss too large (1528.896->1545.871)! Learning rate decreased to 0.00222.
2024-12-01-16:24:40-root-INFO: Loss too large (1528.896->1534.810)! Learning rate decreased to 0.00177.
2024-12-01-16:24:41-root-INFO: grad norm: 145.130 143.337 22.740
2024-12-01-16:24:41-root-INFO: grad norm: 161.128 158.952 26.390
2024-12-01-16:24:41-root-INFO: Loss too large (1521.959->1522.853)! Learning rate decreased to 0.00142.
2024-12-01-16:24:42-root-INFO: grad norm: 126.132 124.489 20.287
2024-12-01-16:24:42-root-INFO: grad norm: 93.460 91.543 18.833
2024-12-01-16:24:43-root-INFO: Loss Change: 1528.896 -> 1504.590
2024-12-01-16:24:43-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-01-16:24:43-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-01-16:24:43-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-16:24:43-root-INFO: step: 165 lr_xt 0.00879737
2024-12-01-16:24:43-root-INFO: grad norm: 68.670 66.923 15.392
2024-12-01-16:24:43-root-INFO: Loss too large (1502.483->1549.831)! Learning rate decreased to 0.00704.
2024-12-01-16:24:43-root-INFO: Loss too large (1502.483->1528.333)! Learning rate decreased to 0.00563.
2024-12-01-16:24:43-root-INFO: Loss too large (1502.483->1514.495)! Learning rate decreased to 0.00450.
2024-12-01-16:24:44-root-INFO: Loss too large (1502.483->1506.115)! Learning rate decreased to 0.00360.
2024-12-01-16:24:44-root-INFO: grad norm: 165.355 163.298 25.998
2024-12-01-16:24:44-root-INFO: Loss too large (1501.403->1555.380)! Learning rate decreased to 0.00288.
2024-12-01-16:24:44-root-INFO: Loss too large (1501.403->1533.689)! Learning rate decreased to 0.00231.
2024-12-01-16:24:45-root-INFO: Loss too large (1501.403->1516.203)! Learning rate decreased to 0.00184.
2024-12-01-16:24:45-root-INFO: Loss too large (1501.403->1503.822)! Learning rate decreased to 0.00148.
2024-12-01-16:24:45-root-INFO: grad norm: 135.530 133.900 20.957
2024-12-01-16:24:46-root-INFO: grad norm: 106.877 105.062 19.612
2024-12-01-16:24:46-root-INFO: grad norm: 94.364 92.897 16.576
2024-12-01-16:24:47-root-INFO: Loss Change: 1502.483 -> 1479.680
2024-12-01-16:24:47-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-01-16:24:47-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-01-16:24:47-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-01-16:24:47-root-INFO: step: 164 lr_xt 0.00914377
2024-12-01-16:24:47-root-INFO: grad norm: 135.375 133.194 24.205
2024-12-01-16:24:47-root-INFO: Loss too large (1483.809->1609.565)! Learning rate decreased to 0.00732.
2024-12-01-16:24:47-root-INFO: Loss too large (1483.809->1589.942)! Learning rate decreased to 0.00585.
2024-12-01-16:24:47-root-INFO: Loss too large (1483.809->1567.726)! Learning rate decreased to 0.00468.
2024-12-01-16:24:48-root-INFO: Loss too large (1483.809->1544.445)! Learning rate decreased to 0.00375.
2024-12-01-16:24:48-root-INFO: Loss too large (1483.809->1522.730)! Learning rate decreased to 0.00300.
2024-12-01-16:24:48-root-INFO: Loss too large (1483.809->1504.887)! Learning rate decreased to 0.00240.
2024-12-01-16:24:48-root-INFO: Loss too large (1483.809->1491.872)! Learning rate decreased to 0.00192.
2024-12-01-16:24:49-root-INFO: grad norm: 155.359 153.496 23.989
2024-12-01-16:24:49-root-INFO: grad norm: 180.306 178.158 27.751
2024-12-01-16:24:49-root-INFO: Loss too large (1479.473->1481.491)! Learning rate decreased to 0.00153.
2024-12-01-16:24:50-root-INFO: grad norm: 141.366 139.689 21.711
2024-12-01-16:24:50-root-INFO: grad norm: 107.809 105.976 19.793
2024-12-01-16:24:51-root-INFO: Loss Change: 1483.809 -> 1458.609
2024-12-01-16:24:51-root-INFO: Regularization Change: 0.000 -> 0.216
2024-12-01-16:24:51-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-01-16:24:51-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-01-16:24:51-root-INFO: step: 163 lr_xt 0.00950228
2024-12-01-16:24:51-root-INFO: grad norm: 61.283 59.579 14.350
2024-12-01-16:24:51-root-INFO: Loss too large (1451.432->1472.843)! Learning rate decreased to 0.00760.
2024-12-01-16:24:51-root-INFO: Loss too large (1451.432->1460.789)! Learning rate decreased to 0.00608.
2024-12-01-16:24:52-root-INFO: Loss too large (1451.432->1453.402)! Learning rate decreased to 0.00487.
2024-12-01-16:24:52-root-INFO: grad norm: 163.009 161.169 24.428
2024-12-01-16:24:52-root-INFO: Loss too large (1449.232->1531.175)! Learning rate decreased to 0.00389.
2024-12-01-16:24:52-root-INFO: Loss too large (1449.232->1505.420)! Learning rate decreased to 0.00311.
2024-12-01-16:24:52-root-INFO: Loss too large (1449.232->1481.639)! Learning rate decreased to 0.00249.
2024-12-01-16:24:53-root-INFO: Loss too large (1449.232->1462.735)! Learning rate decreased to 0.00199.
2024-12-01-16:24:53-root-INFO: Loss too large (1449.232->1449.664)! Learning rate decreased to 0.00159.
2024-12-01-16:24:53-root-INFO: grad norm: 128.962 127.465 19.590
2024-12-01-16:24:54-root-INFO: grad norm: 101.529 99.872 18.271
2024-12-01-16:24:54-root-INFO: grad norm: 86.954 85.544 15.595
2024-12-01-16:24:54-root-INFO: Loss Change: 1451.432 -> 1424.417
2024-12-01-16:24:54-root-INFO: Regularization Change: 0.000 -> 0.320
2024-12-01-16:24:54-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-01-16:24:54-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-16:24:55-root-INFO: step: 162 lr_xt 0.00987325
2024-12-01-16:24:55-root-INFO: grad norm: 122.549 120.118 24.291
2024-12-01-16:24:55-root-INFO: Loss too large (1433.962->1555.673)! Learning rate decreased to 0.00790.
2024-12-01-16:24:55-root-INFO: Loss too large (1433.962->1532.983)! Learning rate decreased to 0.00632.
2024-12-01-16:24:55-root-INFO: Loss too large (1433.962->1508.347)! Learning rate decreased to 0.00506.
2024-12-01-16:24:55-root-INFO: Loss too large (1433.962->1484.181)! Learning rate decreased to 0.00404.
2024-12-01-16:24:56-root-INFO: Loss too large (1433.962->1463.308)! Learning rate decreased to 0.00324.
2024-12-01-16:24:56-root-INFO: Loss too large (1433.962->1447.407)! Learning rate decreased to 0.00259.
2024-12-01-16:24:56-root-INFO: Loss too large (1433.962->1436.630)! Learning rate decreased to 0.00207.
2024-12-01-16:24:56-root-INFO: grad norm: 140.395 138.711 21.677
2024-12-01-16:24:57-root-INFO: grad norm: 157.544 155.432 25.710
2024-12-01-16:24:57-root-INFO: grad norm: 165.569 163.814 24.044
2024-12-01-16:24:58-root-INFO: grad norm: 165.715 163.698 25.773
2024-12-01-16:24:58-root-INFO: Loss Change: 1433.962 -> 1405.631
2024-12-01-16:24:58-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-01-16:24:58-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-01-16:24:58-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-16:24:58-root-INFO: step: 161 lr_xt 0.01025704
2024-12-01-16:24:59-root-INFO: grad norm: 137.618 135.860 21.928
2024-12-01-16:24:59-root-INFO: Loss too large (1397.988->1660.409)! Learning rate decreased to 0.00821.
2024-12-01-16:24:59-root-INFO: Loss too large (1397.988->1565.834)! Learning rate decreased to 0.00656.
2024-12-01-16:24:59-root-INFO: Loss too large (1397.988->1500.944)! Learning rate decreased to 0.00525.
2024-12-01-16:24:59-root-INFO: Loss too large (1397.988->1456.489)! Learning rate decreased to 0.00420.
2024-12-01-16:24:59-root-INFO: Loss too large (1397.988->1426.470)! Learning rate decreased to 0.00336.
2024-12-01-16:25:00-root-INFO: Loss too large (1397.988->1406.909)! Learning rate decreased to 0.00269.
2024-12-01-16:25:00-root-INFO: grad norm: 179.946 177.978 26.539
2024-12-01-16:25:01-root-INFO: grad norm: 212.700 210.348 31.545
2024-12-01-16:25:01-root-INFO: grad norm: 212.303 210.338 28.820
2024-12-01-16:25:01-root-INFO: grad norm: 215.121 212.416 34.007
2024-12-01-16:25:02-root-INFO: Loss too large (1356.821->1361.532)! Learning rate decreased to 0.00215.
2024-12-01-16:25:02-root-INFO: Loss Change: 1397.988 -> 1342.363
2024-12-01-16:25:02-root-INFO: Regularization Change: 0.000 -> 1.024
2024-12-01-16:25:02-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-01-16:25:02-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-16:25:02-root-INFO: step: 160 lr_xt 0.01065404
2024-12-01-16:25:02-root-INFO: grad norm: 189.379 187.436 27.062
2024-12-01-16:25:03-root-INFO: Loss too large (1351.346->1550.723)! Learning rate decreased to 0.00852.
2024-12-01-16:25:03-root-INFO: Loss too large (1351.346->1525.123)! Learning rate decreased to 0.00682.
2024-12-01-16:25:03-root-INFO: Loss too large (1351.346->1492.350)! Learning rate decreased to 0.00545.
2024-12-01-16:25:03-root-INFO: Loss too large (1351.346->1451.512)! Learning rate decreased to 0.00436.
2024-12-01-16:25:03-root-INFO: Loss too large (1351.346->1407.118)! Learning rate decreased to 0.00349.
2024-12-01-16:25:03-root-INFO: Loss too large (1351.346->1367.694)! Learning rate decreased to 0.00279.
2024-12-01-16:25:04-root-INFO: grad norm: 213.704 211.105 33.226
2024-12-01-16:25:04-root-INFO: Loss too large (1340.134->1347.124)! Learning rate decreased to 0.00223.
2024-12-01-16:25:05-root-INFO: grad norm: 164.370 162.757 22.974
2024-12-01-16:25:05-root-INFO: grad norm: 121.316 119.153 22.805
2024-12-01-16:25:06-root-INFO: grad norm: 108.966 107.598 17.211
2024-12-01-16:25:06-root-INFO: Loss Change: 1351.346 -> 1293.926
2024-12-01-16:25:06-root-INFO: Regularization Change: 0.000 -> 0.542
2024-12-01-16:25:06-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-01-16:25:06-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-01-16:25:06-root-INFO: step: 159 lr_xt 0.01106461
2024-12-01-16:25:06-root-INFO: grad norm: 73.490 71.352 17.596
2024-12-01-16:25:06-root-INFO: Loss too large (1288.907->1337.613)! Learning rate decreased to 0.00885.
2024-12-01-16:25:07-root-INFO: Loss too large (1288.907->1315.714)! Learning rate decreased to 0.00708.
2024-12-01-16:25:07-root-INFO: Loss too large (1288.907->1301.223)! Learning rate decreased to 0.00567.
2024-12-01-16:25:07-root-INFO: Loss too large (1288.907->1292.097)! Learning rate decreased to 0.00453.
2024-12-01-16:25:07-root-INFO: grad norm: 133.472 132.204 18.356
2024-12-01-16:25:08-root-INFO: Loss too large (1286.739->1326.088)! Learning rate decreased to 0.00363.
2024-12-01-16:25:08-root-INFO: Loss too large (1286.739->1300.012)! Learning rate decreased to 0.00290.
2024-12-01-16:25:08-root-INFO: grad norm: 175.261 173.145 27.150
2024-12-01-16:25:08-root-INFO: Loss too large (1283.838->1291.613)! Learning rate decreased to 0.00232.
2024-12-01-16:25:09-root-INFO: grad norm: 142.343 141.049 19.146
2024-12-01-16:25:09-root-INFO: grad norm: 104.280 102.468 19.351
2024-12-01-16:25:10-root-INFO: Loss Change: 1288.907 -> 1256.912
2024-12-01-16:25:10-root-INFO: Regularization Change: 0.000 -> 0.537
2024-12-01-16:25:10-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-01-16:25:10-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-01-16:25:10-root-INFO: step: 158 lr_xt 0.01148915
2024-12-01-16:25:10-root-INFO: grad norm: 116.378 114.977 18.001
2024-12-01-16:25:10-root-INFO: Loss too large (1261.061->1437.282)! Learning rate decreased to 0.00919.
2024-12-01-16:25:10-root-INFO: Loss too large (1261.061->1398.848)! Learning rate decreased to 0.00735.
2024-12-01-16:25:11-root-INFO: Loss too large (1261.061->1356.255)! Learning rate decreased to 0.00588.
2024-12-01-16:25:11-root-INFO: Loss too large (1261.061->1316.050)! Learning rate decreased to 0.00471.
2024-12-01-16:25:11-root-INFO: Loss too large (1261.061->1284.630)! Learning rate decreased to 0.00376.
2024-12-01-16:25:11-root-INFO: Loss too large (1261.061->1264.408)! Learning rate decreased to 0.00301.
2024-12-01-16:25:11-root-INFO: grad norm: 139.660 137.837 22.497
2024-12-01-16:25:12-root-INFO: Loss too large (1253.526->1255.906)! Learning rate decreased to 0.00241.
2024-12-01-16:25:12-root-INFO: grad norm: 115.510 114.321 16.529
2024-12-01-16:25:13-root-INFO: grad norm: 87.701 86.086 16.756
2024-12-01-16:25:13-root-INFO: grad norm: 78.331 77.179 13.386
2024-12-01-16:25:14-root-INFO: Loss Change: 1261.061 -> 1223.955
2024-12-01-16:25:14-root-INFO: Regularization Change: 0.000 -> 0.427
2024-12-01-16:25:14-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-01-16:25:14-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-16:25:14-root-INFO: step: 157 lr_xt 0.01192805
2024-12-01-16:25:14-root-INFO: grad norm: 52.910 50.765 14.914
2024-12-01-16:25:14-root-INFO: grad norm: 85.365 84.273 13.609
2024-12-01-16:25:15-root-INFO: Loss too large (1201.719->1316.310)! Learning rate decreased to 0.00954.
2024-12-01-16:25:15-root-INFO: Loss too large (1201.719->1282.492)! Learning rate decreased to 0.00763.
2024-12-01-16:25:15-root-INFO: Loss too large (1201.719->1251.682)! Learning rate decreased to 0.00611.
2024-12-01-16:25:15-root-INFO: Loss too large (1201.719->1227.296)! Learning rate decreased to 0.00489.
2024-12-01-16:25:15-root-INFO: Loss too large (1201.719->1210.848)! Learning rate decreased to 0.00391.
2024-12-01-16:25:16-root-INFO: grad norm: 157.273 155.610 22.812
2024-12-01-16:25:16-root-INFO: Loss too large (1201.227->1227.621)! Learning rate decreased to 0.00313.
2024-12-01-16:25:16-root-INFO: Loss too large (1201.227->1209.759)! Learning rate decreased to 0.00250.
2024-12-01-16:25:17-root-INFO: grad norm: 125.549 124.371 17.162
2024-12-01-16:25:17-root-INFO: grad norm: 80.766 79.356 15.023
2024-12-01-16:25:17-root-INFO: Loss Change: 1224.403 -> 1178.014
2024-12-01-16:25:17-root-INFO: Regularization Change: 0.000 -> 1.168
2024-12-01-16:25:17-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-01-16:25:17-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-16:25:18-root-INFO: step: 156 lr_xt 0.01238172
2024-12-01-16:25:18-root-INFO: grad norm: 90.056 88.890 14.448
2024-12-01-16:25:18-root-INFO: Loss too large (1177.633->1339.588)! Learning rate decreased to 0.00991.
2024-12-01-16:25:18-root-INFO: Loss too large (1177.633->1295.603)! Learning rate decreased to 0.00792.
2024-12-01-16:25:18-root-INFO: Loss too large (1177.633->1251.968)! Learning rate decreased to 0.00634.
2024-12-01-16:25:18-root-INFO: Loss too large (1177.633->1215.959)! Learning rate decreased to 0.00507.
2024-12-01-16:25:18-root-INFO: Loss too large (1177.633->1191.699)! Learning rate decreased to 0.00406.
2024-12-01-16:25:19-root-INFO: Loss too large (1177.633->1177.979)! Learning rate decreased to 0.00325.
2024-12-01-16:25:19-root-INFO: grad norm: 107.175 105.733 17.524
2024-12-01-16:25:19-root-INFO: Loss too large (1171.313->1171.639)! Learning rate decreased to 0.00260.
2024-12-01-16:25:20-root-INFO: grad norm: 89.071 88.011 13.700
2024-12-01-16:25:20-root-INFO: grad norm: 66.721 65.331 13.548
2024-12-01-16:25:21-root-INFO: grad norm: 60.163 59.027 11.634
2024-12-01-16:25:21-root-INFO: Loss Change: 1177.633 -> 1148.502
2024-12-01-16:25:21-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-01-16:25:21-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-01-16:25:21-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-01-16:25:21-root-INFO: step: 155 lr_xt 0.01285057
2024-12-01-16:25:21-root-INFO: grad norm: 45.879 44.197 12.311
2024-12-01-16:25:22-root-INFO: grad norm: 55.399 54.323 10.868
2024-12-01-16:25:22-root-INFO: Loss too large (1126.892->1137.214)! Learning rate decreased to 0.01028.
2024-12-01-16:25:23-root-INFO: grad norm: 158.130 156.495 22.687
2024-12-01-16:25:23-root-INFO: Loss too large (1126.736->1336.755)! Learning rate decreased to 0.00822.
2024-12-01-16:25:23-root-INFO: Loss too large (1126.736->1257.493)! Learning rate decreased to 0.00658.
2024-12-01-16:25:23-root-INFO: Loss too large (1126.736->1203.390)! Learning rate decreased to 0.00526.
2024-12-01-16:25:23-root-INFO: Loss too large (1126.736->1166.686)! Learning rate decreased to 0.00421.
2024-12-01-16:25:23-root-INFO: Loss too large (1126.736->1142.090)! Learning rate decreased to 0.00337.
2024-12-01-16:25:24-root-INFO: grad norm: 135.735 134.713 16.622
2024-12-01-16:25:24-root-INFO: grad norm: 117.836 116.431 18.142
2024-12-01-16:25:25-root-INFO: Loss too large (1106.191->1108.757)! Learning rate decreased to 0.00269.
2024-12-01-16:25:25-root-INFO: Loss Change: 1147.334 -> 1102.482
2024-12-01-16:25:25-root-INFO: Regularization Change: 0.000 -> 1.743
2024-12-01-16:25:25-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-01-16:25:25-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-01-16:25:25-root-INFO: step: 154 lr_xt 0.01333503
2024-12-01-16:25:25-root-INFO: grad norm: 105.489 104.403 15.095
2024-12-01-16:25:25-root-INFO: Loss too large (1105.750->1305.787)! Learning rate decreased to 0.01067.
2024-12-01-16:25:26-root-INFO: Loss too large (1105.750->1260.311)! Learning rate decreased to 0.00853.
2024-12-01-16:25:26-root-INFO: Loss too large (1105.750->1208.530)! Learning rate decreased to 0.00683.
2024-12-01-16:25:26-root-INFO: Loss too large (1105.750->1159.042)! Learning rate decreased to 0.00546.
2024-12-01-16:25:26-root-INFO: Loss too large (1105.750->1122.464)! Learning rate decreased to 0.00437.
2024-12-01-16:25:26-root-INFO: grad norm: 167.784 166.132 23.486
2024-12-01-16:25:27-root-INFO: Loss too large (1101.892->1128.309)! Learning rate decreased to 0.00350.
2024-12-01-16:25:27-root-INFO: Loss too large (1101.892->1109.693)! Learning rate decreased to 0.00280.
2024-12-01-16:25:27-root-INFO: grad norm: 109.056 108.079 14.563
2024-12-01-16:25:28-root-INFO: grad norm: 42.823 41.406 10.922
2024-12-01-16:25:28-root-INFO: grad norm: 39.681 38.356 10.169
2024-12-01-16:25:29-root-INFO: Loss Change: 1105.750 -> 1071.025
2024-12-01-16:25:29-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-01-16:25:29-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-01-16:25:29-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-16:25:29-root-INFO: step: 153 lr_xt 0.01383551
2024-12-01-16:25:29-root-INFO: grad norm: 47.599 46.110 11.814
2024-12-01-16:25:29-root-INFO: Loss too large (1070.017->1074.075)! Learning rate decreased to 0.01107.
2024-12-01-16:25:30-root-INFO: grad norm: 154.364 152.901 21.206
2024-12-01-16:25:30-root-INFO: Loss too large (1066.573->1280.802)! Learning rate decreased to 0.00885.
2024-12-01-16:25:30-root-INFO: Loss too large (1066.573->1199.995)! Learning rate decreased to 0.00708.
2024-12-01-16:25:30-root-INFO: Loss too large (1066.573->1146.154)! Learning rate decreased to 0.00567.
2024-12-01-16:25:30-root-INFO: Loss too large (1066.573->1110.315)! Learning rate decreased to 0.00453.
2024-12-01-16:25:30-root-INFO: Loss too large (1066.573->1086.507)! Learning rate decreased to 0.00363.
2024-12-01-16:25:31-root-INFO: Loss too large (1066.573->1070.886)! Learning rate decreased to 0.00290.
2024-12-01-16:25:31-root-INFO: grad norm: 97.968 97.050 13.383
2024-12-01-16:25:32-root-INFO: grad norm: 39.284 37.915 10.280
2024-12-01-16:25:32-root-INFO: grad norm: 37.013 35.695 9.789
2024-12-01-16:25:32-root-INFO: Loss Change: 1070.017 -> 1037.537
2024-12-01-16:25:32-root-INFO: Regularization Change: 0.000 -> 0.732
2024-12-01-16:25:32-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-01-16:25:32-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-01-16:25:33-root-INFO: step: 152 lr_xt 0.01435246
2024-12-01-16:25:33-root-INFO: grad norm: 41.550 40.225 10.411
2024-12-01-16:25:33-root-INFO: grad norm: 162.249 160.728 22.167
2024-12-01-16:25:33-root-INFO: Loss too large (1032.989->1402.494)! Learning rate decreased to 0.01148.
2024-12-01-16:25:34-root-INFO: Loss too large (1032.989->1269.617)! Learning rate decreased to 0.00919.
2024-12-01-16:25:34-root-INFO: Loss too large (1032.989->1180.645)! Learning rate decreased to 0.00735.
2024-12-01-16:25:34-root-INFO: Loss too large (1032.989->1121.322)! Learning rate decreased to 0.00588.
2024-12-01-16:25:34-root-INFO: Loss too large (1032.989->1081.856)! Learning rate decreased to 0.00470.
2024-12-01-16:25:34-root-INFO: Loss too large (1032.989->1055.654)! Learning rate decreased to 0.00376.
2024-12-01-16:25:34-root-INFO: Loss too large (1032.989->1038.428)! Learning rate decreased to 0.00301.
2024-12-01-16:25:35-root-INFO: grad norm: 97.801 96.954 12.843
2024-12-01-16:25:35-root-INFO: grad norm: 35.376 33.978 9.847
2024-12-01-16:25:36-root-INFO: grad norm: 34.499 33.153 9.541
2024-12-01-16:25:36-root-INFO: Loss Change: 1036.339 -> 1003.047
2024-12-01-16:25:36-root-INFO: Regularization Change: 0.000 -> 0.880
2024-12-01-16:25:36-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-01-16:25:36-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-01-16:25:36-root-INFO: step: 151 lr_xt 0.01488633
2024-12-01-16:25:37-root-INFO: grad norm: 45.136 43.869 10.619
2024-12-01-16:25:37-root-INFO: Loss too large (1002.465->1012.745)! Learning rate decreased to 0.01191.
2024-12-01-16:25:37-root-INFO: Loss too large (1002.465->1002.627)! Learning rate decreased to 0.00953.
2024-12-01-16:25:37-root-INFO: grad norm: 114.006 112.849 16.201
2024-12-01-16:25:38-root-INFO: Loss too large (997.870->1069.136)! Learning rate decreased to 0.00762.
2024-12-01-16:25:38-root-INFO: Loss too large (997.870->1038.614)! Learning rate decreased to 0.00610.
2024-12-01-16:25:38-root-INFO: Loss too large (997.870->1018.342)! Learning rate decreased to 0.00488.
2024-12-01-16:25:38-root-INFO: Loss too large (997.870->1005.039)! Learning rate decreased to 0.00390.
2024-12-01-16:25:38-root-INFO: grad norm: 89.057 88.206 12.283
2024-12-01-16:25:39-root-INFO: grad norm: 48.700 47.560 10.472
2024-12-01-16:25:39-root-INFO: grad norm: 44.604 43.572 9.540
2024-12-01-16:25:40-root-INFO: Loss Change: 1002.465 -> 972.003
2024-12-01-16:25:40-root-INFO: Regularization Change: 0.000 -> 0.771
2024-12-01-16:25:40-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-01-16:25:40-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-16:25:40-root-INFO: step: 150 lr_xt 0.01543756
2024-12-01-16:25:40-root-INFO: grad norm: 36.076 34.802 9.501
2024-12-01-16:25:41-root-INFO: grad norm: 49.053 48.155 9.345
2024-12-01-16:25:41-root-INFO: Loss too large (954.426->988.783)! Learning rate decreased to 0.01235.
2024-12-01-16:25:41-root-INFO: Loss too large (954.426->968.394)! Learning rate decreased to 0.00988.
2024-12-01-16:25:41-root-INFO: Loss too large (954.426->957.042)! Learning rate decreased to 0.00790.
2024-12-01-16:25:42-root-INFO: grad norm: 100.853 99.723 15.058
2024-12-01-16:25:42-root-INFO: Loss too large (951.401->980.928)! Learning rate decreased to 0.00632.
2024-12-01-16:25:42-root-INFO: Loss too large (951.401->965.086)! Learning rate decreased to 0.00506.
2024-12-01-16:25:42-root-INFO: Loss too large (951.401->954.745)! Learning rate decreased to 0.00405.
2024-12-01-16:25:43-root-INFO: grad norm: 77.169 76.386 10.966
2024-12-01-16:25:43-root-INFO: grad norm: 44.812 43.736 9.759
2024-12-01-16:25:44-root-INFO: Loss Change: 970.168 -> 932.220
2024-12-01-16:25:44-root-INFO: Regularization Change: 0.000 -> 1.383
2024-12-01-16:25:44-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-01-16:25:44-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-16:25:44-root-INFO: step: 149 lr_xt 0.01600663
2024-12-01-16:25:44-root-INFO: grad norm: 55.636 54.671 10.319
2024-12-01-16:25:44-root-INFO: Loss too large (932.576->1014.707)! Learning rate decreased to 0.01281.
2024-12-01-16:25:45-root-INFO: Loss too large (932.576->972.948)! Learning rate decreased to 0.01024.
2024-12-01-16:25:45-root-INFO: Loss too large (932.576->947.405)! Learning rate decreased to 0.00820.
2024-12-01-16:25:45-root-INFO: Loss too large (932.576->934.249)! Learning rate decreased to 0.00656.
2024-12-01-16:25:45-root-INFO: grad norm: 91.342 90.292 13.810
2024-12-01-16:25:46-root-INFO: Loss too large (928.232->938.018)! Learning rate decreased to 0.00525.
2024-12-01-16:25:46-root-INFO: Loss too large (928.232->929.699)! Learning rate decreased to 0.00420.
2024-12-01-16:25:46-root-INFO: grad norm: 68.502 67.702 10.439
2024-12-01-16:25:47-root-INFO: grad norm: 39.171 38.068 9.227
2024-12-01-16:25:47-root-INFO: grad norm: 35.698 34.635 8.644
2024-12-01-16:25:48-root-INFO: Loss Change: 932.576 -> 905.384
2024-12-01-16:25:48-root-INFO: Regularization Change: 0.000 -> 0.593
2024-12-01-16:25:48-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-01-16:25:48-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-01-16:25:48-root-INFO: step: 148 lr_xt 0.01659399
2024-12-01-16:25:48-root-INFO: grad norm: 37.141 35.929 9.410
2024-12-01-16:25:48-root-INFO: grad norm: 80.104 79.158 12.275
2024-12-01-16:25:49-root-INFO: Loss too large (893.455->981.241)! Learning rate decreased to 0.01328.
2024-12-01-16:25:49-root-INFO: Loss too large (893.455->943.475)! Learning rate decreased to 0.01062.
2024-12-01-16:25:49-root-INFO: Loss too large (893.455->919.342)! Learning rate decreased to 0.00850.
2024-12-01-16:25:49-root-INFO: Loss too large (893.455->904.132)! Learning rate decreased to 0.00680.
2024-12-01-16:25:49-root-INFO: Loss too large (893.455->894.798)! Learning rate decreased to 0.00544.
2024-12-01-16:25:50-root-INFO: grad norm: 67.082 66.287 10.296
2024-12-01-16:25:50-root-INFO: grad norm: 51.803 50.866 9.806
2024-12-01-16:25:50-root-INFO: grad norm: 48.689 47.847 9.017
2024-12-01-16:25:51-root-INFO: Loss Change: 904.346 -> 869.128
2024-12-01-16:25:51-root-INFO: Regularization Change: 0.000 -> 1.408
2024-12-01-16:25:51-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-01-16:25:51-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-01-16:25:51-root-INFO: step: 147 lr_xt 0.01720013
2024-12-01-16:25:51-root-INFO: grad norm: 36.190 35.097 8.829
2024-12-01-16:25:52-root-INFO: grad norm: 69.252 68.543 9.884
2024-12-01-16:25:52-root-INFO: Loss too large (860.589->980.037)! Learning rate decreased to 0.01376.
2024-12-01-16:25:52-root-INFO: Loss too large (860.589->924.375)! Learning rate decreased to 0.01101.
2024-12-01-16:25:52-root-INFO: Loss too large (860.589->882.392)! Learning rate decreased to 0.00881.
2024-12-01-16:25:53-root-INFO: grad norm: 136.276 135.040 18.314
2024-12-01-16:25:53-root-INFO: Loss too large (860.099->907.143)! Learning rate decreased to 0.00705.
2024-12-01-16:25:53-root-INFO: Loss too large (860.099->881.408)! Learning rate decreased to 0.00564.
2024-12-01-16:25:53-root-INFO: Loss too large (860.099->864.658)! Learning rate decreased to 0.00451.
2024-12-01-16:25:54-root-INFO: grad norm: 72.532 71.879 9.706
2024-12-01-16:25:54-root-INFO: grad norm: 27.534 26.441 7.681
2024-12-01-16:25:54-root-INFO: Loss Change: 865.332 -> 835.349
2024-12-01-16:25:54-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-01-16:25:54-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-01-16:25:54-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-16:25:55-root-INFO: step: 146 lr_xt 0.01782554
2024-12-01-16:25:55-root-INFO: grad norm: 32.112 31.105 7.981
2024-12-01-16:25:55-root-INFO: grad norm: 80.969 80.128 11.636
2024-12-01-16:25:55-root-INFO: Loss too large (830.054->916.672)! Learning rate decreased to 0.01426.
2024-12-01-16:25:56-root-INFO: Loss too large (830.054->878.689)! Learning rate decreased to 0.01141.
2024-12-01-16:25:56-root-INFO: Loss too large (830.054->854.495)! Learning rate decreased to 0.00913.
2024-12-01-16:25:56-root-INFO: Loss too large (830.054->839.328)! Learning rate decreased to 0.00730.
2024-12-01-16:25:56-root-INFO: Loss too large (830.054->830.085)! Learning rate decreased to 0.00584.
2024-12-01-16:25:57-root-INFO: grad norm: 53.845 53.142 8.673
2024-12-01-16:25:57-root-INFO: grad norm: 27.425 26.423 7.345
2024-12-01-16:25:58-root-INFO: grad norm: 24.915 23.887 7.083
2024-12-01-16:25:58-root-INFO: Loss Change: 834.341 -> 808.907
2024-12-01-16:25:58-root-INFO: Regularization Change: 0.000 -> 1.039
2024-12-01-16:25:58-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-01-16:25:58-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-16:25:58-root-INFO: step: 145 lr_xt 0.01847071
2024-12-01-16:25:58-root-INFO: grad norm: 31.654 30.568 8.221
2024-12-01-16:25:59-root-INFO: grad norm: 61.536 60.819 9.369
2024-12-01-16:25:59-root-INFO: Loss too large (804.860->848.514)! Learning rate decreased to 0.01478.
2024-12-01-16:25:59-root-INFO: Loss too large (804.860->824.608)! Learning rate decreased to 0.01182.
2024-12-01-16:25:59-root-INFO: Loss too large (804.860->810.342)! Learning rate decreased to 0.00946.
2024-12-01-16:26:00-root-INFO: grad norm: 62.437 61.703 9.547
2024-12-01-16:26:00-root-INFO: grad norm: 76.612 75.880 10.565
2024-12-01-16:26:00-root-INFO: Loss too large (796.973->802.716)! Learning rate decreased to 0.00757.
2024-12-01-16:26:01-root-INFO: grad norm: 59.371 58.714 8.809
2024-12-01-16:26:01-root-INFO: Loss Change: 809.264 -> 783.020
2024-12-01-16:26:01-root-INFO: Regularization Change: 0.000 -> 1.454
2024-12-01-16:26:01-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-01-16:26:01-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-01-16:26:01-root-INFO: step: 144 lr_xt 0.01913614
2024-12-01-16:26:01-root-INFO: grad norm: 27.347 26.350 7.317
2024-12-01-16:26:02-root-INFO: grad norm: 41.063 40.319 7.778
2024-12-01-16:26:02-root-INFO: Loss too large (775.432->802.204)! Learning rate decreased to 0.01531.
2024-12-01-16:26:02-root-INFO: Loss too large (775.432->781.758)! Learning rate decreased to 0.01225.
2024-12-01-16:26:03-root-INFO: grad norm: 72.956 72.212 10.398
2024-12-01-16:26:03-root-INFO: Loss too large (773.026->789.881)! Learning rate decreased to 0.00980.
2024-12-01-16:26:03-root-INFO: Loss too large (773.026->778.691)! Learning rate decreased to 0.00784.
2024-12-01-16:26:04-root-INFO: grad norm: 53.581 52.988 7.946
2024-12-01-16:26:04-root-INFO: grad norm: 26.421 25.536 6.783
2024-12-01-16:26:04-root-INFO: Loss Change: 781.242 -> 756.941
2024-12-01-16:26:04-root-INFO: Regularization Change: 0.000 -> 1.347
2024-12-01-16:26:04-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-01-16:26:04-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-01-16:26:05-root-INFO: step: 143 lr_xt 0.01982236
2024-12-01-16:26:05-root-INFO: grad norm: 31.366 30.584 6.958
2024-12-01-16:26:05-root-INFO: Loss too large (756.051->759.108)! Learning rate decreased to 0.01586.
2024-12-01-16:26:05-root-INFO: grad norm: 61.345 60.609 9.474
2024-12-01-16:26:06-root-INFO: Loss too large (754.297->775.958)! Learning rate decreased to 0.01269.
2024-12-01-16:26:06-root-INFO: Loss too large (754.297->762.871)! Learning rate decreased to 0.01015.
2024-12-01-16:26:06-root-INFO: Loss too large (754.297->754.674)! Learning rate decreased to 0.00812.
2024-12-01-16:26:06-root-INFO: grad norm: 44.981 44.380 7.327
2024-12-01-16:26:07-root-INFO: grad norm: 29.070 28.255 6.834
2024-12-01-16:26:07-root-INFO: grad norm: 25.816 25.089 6.085
2024-12-01-16:26:08-root-INFO: Loss Change: 756.051 -> 735.862
2024-12-01-16:26:08-root-INFO: Regularization Change: 0.000 -> 0.950
2024-12-01-16:26:08-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-01-16:26:08-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-16:26:08-root-INFO: step: 142 lr_xt 0.02052986
2024-12-01-16:26:08-root-INFO: grad norm: 31.948 30.994 7.751
2024-12-01-16:26:08-root-INFO: grad norm: 50.981 50.350 7.993
2024-12-01-16:26:09-root-INFO: Loss too large (735.981->760.967)! Learning rate decreased to 0.01642.
2024-12-01-16:26:09-root-INFO: Loss too large (735.981->739.819)! Learning rate decreased to 0.01314.
2024-12-01-16:26:09-root-INFO: grad norm: 59.982 59.181 9.771
2024-12-01-16:26:09-root-INFO: Loss too large (729.537->733.046)! Learning rate decreased to 0.01051.
2024-12-01-16:26:10-root-INFO: grad norm: 47.221 46.643 7.371
2024-12-01-16:26:10-root-INFO: grad norm: 32.061 31.402 6.468
2024-12-01-16:26:11-root-INFO: Loss Change: 736.752 -> 712.367
2024-12-01-16:26:11-root-INFO: Regularization Change: 0.000 -> 1.642
2024-12-01-16:26:11-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-01-16:26:11-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-01-16:26:11-root-INFO: step: 141 lr_xt 0.02125920
2024-12-01-16:26:11-root-INFO: grad norm: 22.931 22.167 5.867
2024-12-01-16:26:12-root-INFO: grad norm: 35.310 34.595 7.070
2024-12-01-16:26:12-root-INFO: Loss too large (704.889->712.545)! Learning rate decreased to 0.01701.
2024-12-01-16:26:12-root-INFO: Loss too large (704.889->706.147)! Learning rate decreased to 0.01361.
2024-12-01-16:26:12-root-INFO: grad norm: 42.716 42.179 6.750
2024-12-01-16:26:13-root-INFO: grad norm: 60.140 59.343 9.760
2024-12-01-16:26:13-root-INFO: Loss too large (701.233->705.043)! Learning rate decreased to 0.01088.
2024-12-01-16:26:13-root-INFO: grad norm: 44.712 44.164 6.980
2024-12-01-16:26:14-root-INFO: Loss Change: 710.226 -> 688.373
2024-12-01-16:26:14-root-INFO: Regularization Change: 0.000 -> 1.674
2024-12-01-16:26:14-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-01-16:26:14-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-01-16:26:14-root-INFO: step: 140 lr_xt 0.02201089
2024-12-01-16:26:14-root-INFO: grad norm: 40.166 39.586 6.803
2024-12-01-16:26:14-root-INFO: Loss too large (689.690->702.639)! Learning rate decreased to 0.01761.
2024-12-01-16:26:14-root-INFO: Loss too large (689.690->692.121)! Learning rate decreased to 0.01409.
2024-12-01-16:26:15-root-INFO: grad norm: 48.213 47.723 6.857
2024-12-01-16:26:15-root-INFO: Loss too large (686.480->686.588)! Learning rate decreased to 0.01127.
2024-12-01-16:26:15-root-INFO: grad norm: 40.554 40.025 6.528
2024-12-01-16:26:16-root-INFO: grad norm: 33.713 33.194 5.894
2024-12-01-16:26:16-root-INFO: grad norm: 31.649 31.017 6.291
2024-12-01-16:26:17-root-INFO: Loss Change: 689.690 -> 669.197
2024-12-01-16:26:17-root-INFO: Regularization Change: 0.000 -> 1.204
2024-12-01-16:26:17-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-01-16:26:17-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-16:26:17-root-INFO: step: 139 lr_xt 0.02278550
2024-12-01-16:26:17-root-INFO: grad norm: 26.155 25.458 5.999
2024-12-01-16:26:17-root-INFO: Loss too large (667.830->671.705)! Learning rate decreased to 0.01823.
2024-12-01-16:26:18-root-INFO: grad norm: 62.697 61.796 10.591
2024-12-01-16:26:18-root-INFO: Loss too large (667.050->678.026)! Learning rate decreased to 0.01458.
2024-12-01-16:26:18-root-INFO: Loss too large (667.050->673.125)! Learning rate decreased to 0.01167.
2024-12-01-16:26:18-root-INFO: Loss too large (667.050->668.905)! Learning rate decreased to 0.00933.
2024-12-01-16:26:19-root-INFO: grad norm: 34.852 34.263 6.378
2024-12-01-16:26:19-root-INFO: grad norm: 20.644 19.999 5.118
2024-12-01-16:26:20-root-INFO: grad norm: 17.983 17.179 5.316
2024-12-01-16:26:20-root-INFO: Loss Change: 667.830 -> 650.826
2024-12-01-16:26:20-root-INFO: Regularization Change: 0.000 -> 0.900
2024-12-01-16:26:20-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-01-16:26:20-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-16:26:20-root-INFO: step: 138 lr_xt 0.02358356
2024-12-01-16:26:20-root-INFO: grad norm: 22.022 21.367 5.330
2024-12-01-16:26:21-root-INFO: grad norm: 41.234 40.636 6.994
2024-12-01-16:26:21-root-INFO: Loss too large (647.084->665.109)! Learning rate decreased to 0.01887.
2024-12-01-16:26:21-root-INFO: Loss too large (647.084->655.006)! Learning rate decreased to 0.01509.
2024-12-01-16:26:21-root-INFO: Loss too large (647.084->648.565)! Learning rate decreased to 0.01207.
2024-12-01-16:26:22-root-INFO: grad norm: 38.532 38.094 5.794
2024-12-01-16:26:22-root-INFO: grad norm: 38.773 38.188 6.706
2024-12-01-16:26:23-root-INFO: grad norm: 36.728 36.255 5.875
2024-12-01-16:26:23-root-INFO: Loss Change: 650.385 -> 633.200
2024-12-01-16:26:23-root-INFO: Regularization Change: 0.000 -> 1.388
2024-12-01-16:26:23-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-01-16:26:23-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-01-16:26:23-root-INFO: step: 137 lr_xt 0.02440563
2024-12-01-16:26:23-root-INFO: grad norm: 39.051 38.374 7.238
2024-12-01-16:26:23-root-INFO: Loss too large (633.321->641.348)! Learning rate decreased to 0.01952.
2024-12-01-16:26:24-root-INFO: Loss too large (633.321->636.993)! Learning rate decreased to 0.01562.
2024-12-01-16:26:24-root-INFO: Loss too large (633.321->633.755)! Learning rate decreased to 0.01250.
2024-12-01-16:26:24-root-INFO: grad norm: 32.660 32.027 6.401
2024-12-01-16:26:25-root-INFO: grad norm: 23.172 22.476 5.634
2024-12-01-16:26:25-root-INFO: grad norm: 24.024 23.426 5.325
2024-12-01-16:26:26-root-INFO: grad norm: 27.428 26.750 6.058
2024-12-01-16:26:26-root-INFO: Loss Change: 633.321 -> 617.109
2024-12-01-16:26:26-root-INFO: Regularization Change: 0.000 -> 1.099
2024-12-01-16:26:26-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-01-16:26:26-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-01-16:26:26-root-INFO: step: 136 lr_xt 0.02525230
2024-12-01-16:26:26-root-INFO: grad norm: 29.290 28.718 5.760
2024-12-01-16:26:27-root-INFO: Loss too large (616.715->628.719)! Learning rate decreased to 0.02020.
2024-12-01-16:26:27-root-INFO: Loss too large (616.715->619.547)! Learning rate decreased to 0.01616.
2024-12-01-16:26:27-root-INFO: grad norm: 48.862 48.191 8.073
2024-12-01-16:26:27-root-INFO: Loss too large (614.760->619.637)! Learning rate decreased to 0.01293.
2024-12-01-16:26:28-root-INFO: Loss too large (614.760->615.610)! Learning rate decreased to 0.01034.
2024-12-01-16:26:28-root-INFO: grad norm: 31.613 31.072 5.823
2024-12-01-16:26:29-root-INFO: grad norm: 15.906 15.215 4.638
2024-12-01-16:26:29-root-INFO: grad norm: 14.996 14.290 4.547
2024-12-01-16:26:30-root-INFO: Loss Change: 616.715 -> 601.182
2024-12-01-16:26:30-root-INFO: Regularization Change: 0.000 -> 0.785
2024-12-01-16:26:30-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-01-16:26:30-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-16:26:30-root-INFO: step: 135 lr_xt 0.02612413
2024-12-01-16:26:30-root-INFO: grad norm: 19.395 18.762 4.915
2024-12-01-16:26:30-root-INFO: grad norm: 31.838 31.358 5.508
2024-12-01-16:26:31-root-INFO: Loss too large (597.131->607.309)! Learning rate decreased to 0.02090.
2024-12-01-16:26:31-root-INFO: Loss too large (597.131->600.346)! Learning rate decreased to 0.01672.
2024-12-01-16:26:31-root-INFO: grad norm: 39.119 38.737 5.454
2024-12-01-16:26:32-root-INFO: grad norm: 53.141 52.767 6.297
2024-12-01-16:26:32-root-INFO: Loss too large (595.788->601.829)! Learning rate decreased to 0.01338.
2024-12-01-16:26:32-root-INFO: grad norm: 48.033 47.580 6.583
2024-12-01-16:26:33-root-INFO: Loss Change: 601.059 -> 587.798
2024-12-01-16:26:33-root-INFO: Regularization Change: 0.000 -> 1.705
2024-12-01-16:26:33-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-01-16:26:33-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-01-16:26:33-root-INFO: step: 134 lr_xt 0.02702170
2024-12-01-16:26:33-root-INFO: grad norm: 33.502 32.995 5.803
2024-12-01-16:26:33-root-INFO: Loss too large (584.587->605.908)! Learning rate decreased to 0.02162.
2024-12-01-16:26:33-root-INFO: Loss too large (584.587->592.403)! Learning rate decreased to 0.01729.
2024-12-01-16:26:33-root-INFO: Loss too large (584.587->584.830)! Learning rate decreased to 0.01384.
2024-12-01-16:26:34-root-INFO: grad norm: 37.713 37.098 6.784
2024-12-01-16:26:34-root-INFO: grad norm: 34.197 33.651 6.090
2024-12-01-16:26:35-root-INFO: grad norm: 26.625 26.066 5.425
2024-12-01-16:26:35-root-INFO: grad norm: 27.381 26.857 5.330
2024-12-01-16:26:36-root-INFO: Loss Change: 584.587 -> 569.189
2024-12-01-16:26:36-root-INFO: Regularization Change: 0.000 -> 1.097
2024-12-01-16:26:36-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-01-16:26:36-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-01-16:26:36-root-INFO: step: 133 lr_xt 0.02794561
2024-12-01-16:26:36-root-INFO: grad norm: 40.110 39.540 6.737
2024-12-01-16:26:36-root-INFO: Loss too large (570.866->592.306)! Learning rate decreased to 0.02236.
2024-12-01-16:26:37-root-INFO: Loss too large (570.866->581.186)! Learning rate decreased to 0.01789.
2024-12-01-16:26:37-root-INFO: Loss too large (570.866->574.004)! Learning rate decreased to 0.01431.
2024-12-01-16:26:37-root-INFO: grad norm: 36.964 36.421 6.312
2024-12-01-16:26:38-root-INFO: grad norm: 31.733 31.229 5.635
2024-12-01-16:26:38-root-INFO: grad norm: 31.536 31.020 5.677
2024-12-01-16:26:39-root-INFO: grad norm: 33.240 32.692 6.009
2024-12-01-16:26:39-root-INFO: Loss Change: 570.866 -> 557.033
2024-12-01-16:26:39-root-INFO: Regularization Change: 0.000 -> 1.213
2024-12-01-16:26:39-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-01-16:26:39-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-16:26:39-root-INFO: step: 132 lr_xt 0.02889645
2024-12-01-16:26:39-root-INFO: grad norm: 29.468 28.882 5.848
2024-12-01-16:26:39-root-INFO: Loss too large (555.499->569.666)! Learning rate decreased to 0.02312.
2024-12-01-16:26:40-root-INFO: Loss too large (555.499->558.945)! Learning rate decreased to 0.01849.
2024-12-01-16:26:40-root-INFO: grad norm: 46.751 46.069 7.958
2024-12-01-16:26:40-root-INFO: Loss too large (553.248->558.386)! Learning rate decreased to 0.01479.
2024-12-01-16:26:40-root-INFO: Loss too large (553.248->554.056)! Learning rate decreased to 0.01184.
2024-12-01-16:26:41-root-INFO: grad norm: 29.462 28.864 5.907
2024-12-01-16:26:41-root-INFO: grad norm: 15.045 14.468 4.126
2024-12-01-16:26:42-root-INFO: grad norm: 13.920 13.303 4.098
2024-12-01-16:26:42-root-INFO: Loss Change: 555.499 -> 539.639
2024-12-01-16:26:42-root-INFO: Regularization Change: 0.000 -> 0.874
2024-12-01-16:26:42-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-01-16:26:42-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-01-16:26:43-root-INFO: step: 131 lr_xt 0.02987484
2024-12-01-16:26:43-root-INFO: grad norm: 19.956 19.431 4.549
2024-12-01-16:26:43-root-INFO: grad norm: 39.875 39.426 5.964
2024-12-01-16:26:43-root-INFO: Loss too large (538.273->567.195)! Learning rate decreased to 0.02390.
2024-12-01-16:26:44-root-INFO: Loss too large (538.273->551.156)! Learning rate decreased to 0.01912.
2024-12-01-16:26:44-root-INFO: Loss too large (538.273->541.453)! Learning rate decreased to 0.01530.
2024-12-01-16:26:44-root-INFO: grad norm: 39.715 39.286 5.825
2024-12-01-16:26:45-root-INFO: grad norm: 37.510 36.980 6.281
2024-12-01-16:26:45-root-INFO: grad norm: 37.933 37.443 6.082
2024-12-01-16:26:45-root-INFO: Loss Change: 540.037 -> 527.485
2024-12-01-16:26:45-root-INFO: Regularization Change: 0.000 -> 1.714
2024-12-01-16:26:45-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-01-16:26:45-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-01-16:26:46-root-INFO: step: 130 lr_xt 0.03088137
2024-12-01-16:26:46-root-INFO: grad norm: 31.900 31.298 6.169
2024-12-01-16:26:46-root-INFO: Loss too large (525.679->544.053)! Learning rate decreased to 0.02471.
2024-12-01-16:26:46-root-INFO: Loss too large (525.679->530.944)! Learning rate decreased to 0.01976.
2024-12-01-16:26:46-root-INFO: grad norm: 48.466 47.832 7.810
2024-12-01-16:26:47-root-INFO: Loss too large (523.775->529.560)! Learning rate decreased to 0.01581.
2024-12-01-16:26:47-root-INFO: Loss too large (523.775->523.932)! Learning rate decreased to 0.01265.
2024-12-01-16:26:47-root-INFO: grad norm: 29.700 29.063 6.114
2024-12-01-16:26:48-root-INFO: grad norm: 15.233 14.745 3.824
2024-12-01-16:26:48-root-INFO: grad norm: 13.544 12.996 3.816
2024-12-01-16:26:49-root-INFO: Loss Change: 525.679 -> 508.716
2024-12-01-16:26:49-root-INFO: Regularization Change: 0.000 -> 0.964
2024-12-01-16:26:49-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-01-16:26:49-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-16:26:49-root-INFO: step: 129 lr_xt 0.03191668
2024-12-01-16:26:49-root-INFO: grad norm: 19.772 19.340 4.109
2024-12-01-16:26:49-root-INFO: grad norm: 41.562 41.136 5.932
2024-12-01-16:26:49-root-INFO: Loss too large (508.591->542.000)! Learning rate decreased to 0.02553.
2024-12-01-16:26:50-root-INFO: Loss too large (508.591->523.842)! Learning rate decreased to 0.02043.
2024-12-01-16:26:50-root-INFO: Loss too large (508.591->512.730)! Learning rate decreased to 0.01634.
2024-12-01-16:26:50-root-INFO: grad norm: 39.481 39.089 5.546
2024-12-01-16:26:51-root-INFO: grad norm: 35.920 35.378 6.216
2024-12-01-16:26:51-root-INFO: grad norm: 36.153 35.669 5.898
2024-12-01-16:26:51-root-INFO: Loss Change: 509.007 -> 498.019
2024-12-01-16:26:51-root-INFO: Regularization Change: 0.000 -> 1.661
2024-12-01-16:26:51-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-01-16:26:51-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-01-16:26:52-root-INFO: step: 128 lr_xt 0.03298138
2024-12-01-16:26:52-root-INFO: grad norm: 29.215 28.547 6.208
2024-12-01-16:26:52-root-INFO: Loss too large (496.023->511.522)! Learning rate decreased to 0.02639.
2024-12-01-16:26:52-root-INFO: Loss too large (496.023->499.845)! Learning rate decreased to 0.02111.
2024-12-01-16:26:53-root-INFO: grad norm: 42.828 42.164 7.509
2024-12-01-16:26:53-root-INFO: Loss too large (493.583->498.915)! Learning rate decreased to 0.01689.
2024-12-01-16:26:53-root-INFO: Loss too large (493.583->494.067)! Learning rate decreased to 0.01351.
2024-12-01-16:26:53-root-INFO: grad norm: 27.341 26.658 6.072
2024-12-01-16:26:54-root-INFO: grad norm: 13.904 13.441 3.559
2024-12-01-16:26:54-root-INFO: grad norm: 12.406 11.883 3.562
2024-12-01-16:26:55-root-INFO: Loss Change: 496.023 -> 480.856
2024-12-01-16:26:55-root-INFO: Regularization Change: 0.000 -> 0.912
2024-12-01-16:26:55-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-01-16:26:55-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-01-16:26:55-root-INFO: step: 127 lr_xt 0.03407612
2024-12-01-16:26:55-root-INFO: grad norm: 17.284 16.847 3.866
2024-12-01-16:26:55-root-INFO: grad norm: 35.054 34.443 6.516
2024-12-01-16:26:56-root-INFO: Loss too large (480.670->512.224)! Learning rate decreased to 0.02726.
2024-12-01-16:26:56-root-INFO: Loss too large (480.670->493.946)! Learning rate decreased to 0.02181.
2024-12-01-16:26:56-root-INFO: Loss too large (480.670->483.304)! Learning rate decreased to 0.01745.
2024-12-01-16:26:56-root-INFO: grad norm: 36.147 35.611 6.205
2024-12-01-16:26:57-root-INFO: grad norm: 34.136 33.475 6.685
2024-12-01-16:26:57-root-INFO: grad norm: 30.963 30.476 5.473
2024-12-01-16:26:58-root-INFO: Loss Change: 480.746 -> 470.833
2024-12-01-16:26:58-root-INFO: Regularization Change: 0.000 -> 1.539
2024-12-01-16:26:58-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-01-16:26:58-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-16:26:58-root-INFO: step: 126 lr_xt 0.03520152
2024-12-01-16:26:58-root-INFO: grad norm: 28.195 27.544 6.025
2024-12-01-16:26:58-root-INFO: Loss too large (469.836->489.555)! Learning rate decreased to 0.02816.
2024-12-01-16:26:58-root-INFO: Loss too large (469.836->477.016)! Learning rate decreased to 0.02253.
2024-12-01-16:26:58-root-INFO: Loss too large (469.836->469.965)! Learning rate decreased to 0.01802.
2024-12-01-16:26:59-root-INFO: grad norm: 29.368 28.800 5.751
2024-12-01-16:26:59-root-INFO: grad norm: 29.238 28.544 6.334
2024-12-01-16:27:00-root-INFO: grad norm: 28.864 28.288 5.740
2024-12-01-16:27:00-root-INFO: grad norm: 29.033 28.337 6.318
2024-12-01-16:27:01-root-INFO: Loss Change: 469.836 -> 458.193
2024-12-01-16:27:01-root-INFO: Regularization Change: 0.000 -> 1.179
2024-12-01-16:27:01-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-01-16:27:01-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-01-16:27:01-root-INFO: step: 125 lr_xt 0.03635823
2024-12-01-16:27:01-root-INFO: grad norm: 36.360 35.713 6.830
2024-12-01-16:27:01-root-INFO: Loss too large (459.648->486.552)! Learning rate decreased to 0.02909.
2024-12-01-16:27:01-root-INFO: Loss too large (459.648->473.334)! Learning rate decreased to 0.02327.
2024-12-01-16:27:02-root-INFO: Loss too large (459.648->464.580)! Learning rate decreased to 0.01862.
2024-12-01-16:27:02-root-INFO: grad norm: 34.555 33.816 7.104
2024-12-01-16:27:02-root-INFO: grad norm: 31.997 31.435 5.974
2024-12-01-16:27:03-root-INFO: grad norm: 31.358 30.657 6.597
2024-12-01-16:27:03-root-INFO: grad norm: 31.169 30.581 6.021
2024-12-01-16:27:04-root-INFO: Loss Change: 459.648 -> 449.451
2024-12-01-16:27:04-root-INFO: Regularization Change: 0.000 -> 1.313
2024-12-01-16:27:04-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-01-16:27:04-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-01-16:27:04-root-INFO: step: 124 lr_xt 0.03773645
2024-12-01-16:27:04-root-INFO: grad norm: 26.653 25.914 6.232
2024-12-01-16:27:04-root-INFO: Loss too large (447.539->464.432)! Learning rate decreased to 0.03019.
2024-12-01-16:27:05-root-INFO: Loss too large (447.539->453.098)! Learning rate decreased to 0.02415.
2024-12-01-16:27:05-root-INFO: grad norm: 41.879 41.028 8.400
2024-12-01-16:27:05-root-INFO: Loss too large (446.727->453.809)! Learning rate decreased to 0.01932.
2024-12-01-16:27:05-root-INFO: Loss too large (446.727->448.171)! Learning rate decreased to 0.01546.
2024-12-01-16:27:06-root-INFO: grad norm: 27.834 27.074 6.457
2024-12-01-16:27:06-root-INFO: grad norm: 13.999 13.593 3.347
2024-12-01-16:27:07-root-INFO: grad norm: 12.276 11.803 3.375
2024-12-01-16:27:07-root-INFO: Loss Change: 447.539 -> 434.615
2024-12-01-16:27:07-root-INFO: Regularization Change: 0.000 -> 0.902
2024-12-01-16:27:07-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-01-16:27:07-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-16:27:07-root-INFO: step: 123 lr_xt 0.03896235
2024-12-01-16:27:08-root-INFO: grad norm: 16.164 15.760 3.593
2024-12-01-16:27:08-root-INFO: Loss too large (434.840->436.594)! Learning rate decreased to 0.03117.
2024-12-01-16:27:08-root-INFO: grad norm: 28.332 27.708 5.913
2024-12-01-16:27:08-root-INFO: Loss too large (434.487->446.181)! Learning rate decreased to 0.02494.
2024-12-01-16:27:09-root-INFO: Loss too large (434.487->437.687)! Learning rate decreased to 0.01995.
2024-12-01-16:27:09-root-INFO: grad norm: 32.974 32.343 6.424
2024-12-01-16:27:09-root-INFO: Loss too large (433.047->433.590)! Learning rate decreased to 0.01596.
2024-12-01-16:27:10-root-INFO: grad norm: 24.948 24.252 5.853
2024-12-01-16:27:10-root-INFO: grad norm: 17.832 17.385 3.968
2024-12-01-16:27:10-root-INFO: Loss Change: 434.840 -> 425.354
2024-12-01-16:27:10-root-INFO: Regularization Change: 0.000 -> 0.995
2024-12-01-16:27:11-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-01-16:27:11-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-01-16:27:11-root-INFO: step: 122 lr_xt 0.04022160
2024-12-01-16:27:11-root-INFO: grad norm: 14.526 14.063 3.640
2024-12-01-16:27:11-root-INFO: Loss too large (424.856->425.894)! Learning rate decreased to 0.03218.
2024-12-01-16:27:12-root-INFO: grad norm: 28.769 28.166 5.858
2024-12-01-16:27:12-root-INFO: Loss too large (424.072->432.233)! Learning rate decreased to 0.02574.
2024-12-01-16:27:12-root-INFO: Loss too large (424.072->427.774)! Learning rate decreased to 0.02059.
2024-12-01-16:27:12-root-INFO: Loss too large (424.072->424.478)! Learning rate decreased to 0.01647.
2024-12-01-16:27:13-root-INFO: grad norm: 22.513 21.844 5.447
2024-12-01-16:27:13-root-INFO: grad norm: 16.253 15.808 3.776
2024-12-01-16:27:14-root-INFO: grad norm: 14.897 14.388 3.858
2024-12-01-16:27:14-root-INFO: Loss Change: 424.856 -> 415.867
2024-12-01-16:27:14-root-INFO: Regularization Change: 0.000 -> 0.897
2024-12-01-16:27:14-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-01-16:27:14-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-01-16:27:14-root-INFO: step: 121 lr_xt 0.04151486
2024-12-01-16:27:14-root-INFO: grad norm: 19.235 18.769 4.209
2024-12-01-16:27:14-root-INFO: Loss too large (416.102->423.208)! Learning rate decreased to 0.03321.
2024-12-01-16:27:15-root-INFO: Loss too large (416.102->419.175)! Learning rate decreased to 0.02657.
2024-12-01-16:27:15-root-INFO: Loss too large (416.102->416.596)! Learning rate decreased to 0.02126.
2024-12-01-16:27:15-root-INFO: grad norm: 21.951 21.326 5.198
2024-12-01-16:27:16-root-INFO: grad norm: 27.916 27.298 5.843
2024-12-01-16:27:16-root-INFO: Loss too large (413.938->414.801)! Learning rate decreased to 0.01700.
2024-12-01-16:27:16-root-INFO: grad norm: 22.879 22.213 5.482
2024-12-01-16:27:17-root-INFO: grad norm: 17.341 16.899 3.889
2024-12-01-16:27:17-root-INFO: Loss Change: 416.102 -> 407.921
2024-12-01-16:27:17-root-INFO: Regularization Change: 0.000 -> 0.807
2024-12-01-16:27:17-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-01-16:27:17-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-16:27:18-root-INFO: step: 120 lr_xt 0.04284282
2024-12-01-16:27:18-root-INFO: grad norm: 15.640 15.183 3.753
2024-12-01-16:27:18-root-INFO: Loss too large (407.840->411.905)! Learning rate decreased to 0.03427.
2024-12-01-16:27:18-root-INFO: Loss too large (407.840->408.713)! Learning rate decreased to 0.02742.
2024-12-01-16:27:19-root-INFO: grad norm: 25.393 24.868 5.137
2024-12-01-16:27:19-root-INFO: Loss too large (406.967->410.162)! Learning rate decreased to 0.02194.
2024-12-01-16:27:19-root-INFO: Loss too large (406.967->407.312)! Learning rate decreased to 0.01755.
2024-12-01-16:27:20-root-INFO: grad norm: 20.864 20.260 4.985
2024-12-01-16:27:20-root-INFO: grad norm: 16.318 15.897 3.685
2024-12-01-16:27:20-root-INFO: grad norm: 15.177 14.686 3.829
2024-12-01-16:27:21-root-INFO: Loss Change: 407.840 -> 399.741
2024-12-01-16:27:21-root-INFO: Regularization Change: 0.000 -> 0.804
2024-12-01-16:27:21-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-01-16:27:21-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-01-16:27:21-root-INFO: step: 119 lr_xt 0.04420613
2024-12-01-16:27:21-root-INFO: grad norm: 19.944 19.491 4.223
2024-12-01-16:27:21-root-INFO: Loss too large (400.268->408.791)! Learning rate decreased to 0.03536.
2024-12-01-16:27:21-root-INFO: Loss too large (400.268->404.023)! Learning rate decreased to 0.02829.
2024-12-01-16:27:22-root-INFO: Loss too large (400.268->400.973)! Learning rate decreased to 0.02263.
2024-12-01-16:27:22-root-INFO: grad norm: 22.214 21.615 5.121
2024-12-01-16:27:23-root-INFO: grad norm: 27.457 26.898 5.509
2024-12-01-16:27:23-root-INFO: Loss too large (398.035->398.755)! Learning rate decreased to 0.01811.
2024-12-01-16:27:23-root-INFO: grad norm: 22.065 21.443 5.203
2024-12-01-16:27:24-root-INFO: grad norm: 16.621 16.220 3.629
2024-12-01-16:27:24-root-INFO: Loss Change: 400.268 -> 391.905
2024-12-01-16:27:24-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-01-16:27:24-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-01-16:27:24-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-01-16:27:24-root-INFO: step: 118 lr_xt 0.04560549
2024-12-01-16:27:25-root-INFO: grad norm: 12.976 12.564 3.244
2024-12-01-16:27:25-root-INFO: Loss too large (391.512->393.287)! Learning rate decreased to 0.03648.
2024-12-01-16:27:25-root-INFO: grad norm: 27.110 26.591 5.278
2024-12-01-16:27:25-root-INFO: Loss too large (391.395->399.596)! Learning rate decreased to 0.02919.
2024-12-01-16:27:26-root-INFO: Loss too large (391.395->395.248)! Learning rate decreased to 0.02335.
2024-12-01-16:27:26-root-INFO: Loss too large (391.395->391.887)! Learning rate decreased to 0.01868.
2024-12-01-16:27:26-root-INFO: grad norm: 21.155 20.584 4.881
2024-12-01-16:27:27-root-INFO: grad norm: 15.459 15.080 3.402
2024-12-01-16:27:27-root-INFO: grad norm: 13.986 13.553 3.455
2024-12-01-16:27:28-root-INFO: Loss Change: 391.512 -> 383.693
2024-12-01-16:27:28-root-INFO: Regularization Change: 0.000 -> 0.900
2024-12-01-16:27:28-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-01-16:27:28-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-01-16:27:28-root-INFO: step: 117 lr_xt 0.04704158
2024-12-01-16:27:28-root-INFO: grad norm: 18.052 17.678 3.658
2024-12-01-16:27:28-root-INFO: Loss too large (384.360->391.674)! Learning rate decreased to 0.03763.
2024-12-01-16:27:28-root-INFO: Loss too large (384.360->387.225)! Learning rate decreased to 0.03011.
2024-12-01-16:27:28-root-INFO: Loss too large (384.360->384.531)! Learning rate decreased to 0.02409.
2024-12-01-16:27:29-root-INFO: grad norm: 19.766 19.295 4.286
2024-12-01-16:27:29-root-INFO: grad norm: 23.979 23.519 4.674
2024-12-01-16:27:30-root-INFO: Loss too large (382.012->382.178)! Learning rate decreased to 0.01927.
2024-12-01-16:27:30-root-INFO: grad norm: 19.431 18.921 4.423
2024-12-01-16:27:30-root-INFO: grad norm: 15.349 14.988 3.309
2024-12-01-16:27:31-root-INFO: Loss Change: 384.360 -> 376.493
2024-12-01-16:27:31-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-01-16:27:31-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-01-16:27:31-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-01-16:27:31-root-INFO: step: 116 lr_xt 0.04851508
2024-12-01-16:27:31-root-INFO: grad norm: 13.028 12.637 3.169
2024-12-01-16:27:31-root-INFO: Loss too large (375.997->377.945)! Learning rate decreased to 0.03881.
2024-12-01-16:27:32-root-INFO: grad norm: 26.005 25.565 4.762
2024-12-01-16:27:32-root-INFO: Loss too large (375.926->383.389)! Learning rate decreased to 0.03105.
2024-12-01-16:27:32-root-INFO: Loss too large (375.926->379.194)! Learning rate decreased to 0.02484.
2024-12-01-16:27:32-root-INFO: Loss too large (375.926->375.963)! Learning rate decreased to 0.01987.
2024-12-01-16:27:33-root-INFO: grad norm: 19.631 19.119 4.454
2024-12-01-16:27:33-root-INFO: grad norm: 14.321 13.976 3.124
2024-12-01-16:27:34-root-INFO: grad norm: 12.826 12.428 3.171
2024-12-01-16:27:34-root-INFO: Loss Change: 375.997 -> 368.234
2024-12-01-16:27:34-root-INFO: Regularization Change: 0.000 -> 0.931
2024-12-01-16:27:34-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-01-16:27:34-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-16:27:34-root-INFO: step: 115 lr_xt 0.05002669
2024-12-01-16:27:34-root-INFO: grad norm: 16.090 15.750 3.291
2024-12-01-16:27:34-root-INFO: Loss too large (369.014->374.844)! Learning rate decreased to 0.04002.
2024-12-01-16:27:35-root-INFO: Loss too large (369.014->371.291)! Learning rate decreased to 0.03202.
2024-12-01-16:27:35-root-INFO: Loss too large (369.014->369.096)! Learning rate decreased to 0.02561.
2024-12-01-16:27:35-root-INFO: grad norm: 17.938 17.496 3.954
2024-12-01-16:27:36-root-INFO: grad norm: 21.505 21.098 4.162
2024-12-01-16:27:36-root-INFO: grad norm: 22.866 22.315 4.991
2024-12-01-16:27:37-root-INFO: grad norm: 24.188 23.753 4.564
2024-12-01-16:27:37-root-INFO: Loss Change: 369.014 -> 364.572
2024-12-01-16:27:37-root-INFO: Regularization Change: 0.000 -> 1.218
2024-12-01-16:27:37-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-01-16:27:37-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-01-16:27:37-root-INFO: step: 114 lr_xt 0.05157710
2024-12-01-16:27:38-root-INFO: grad norm: 22.551 21.979 5.047
2024-12-01-16:27:38-root-INFO: Loss too large (363.805->379.156)! Learning rate decreased to 0.04126.
2024-12-01-16:27:38-root-INFO: Loss too large (363.805->369.624)! Learning rate decreased to 0.03301.
2024-12-01-16:27:38-root-INFO: Loss too large (363.805->363.857)! Learning rate decreased to 0.02641.
2024-12-01-16:27:39-root-INFO: grad norm: 21.275 20.881 4.076
2024-12-01-16:27:39-root-INFO: grad norm: 20.949 20.401 4.759
2024-12-01-16:27:40-root-INFO: grad norm: 20.372 19.985 3.952
2024-12-01-16:27:40-root-INFO: grad norm: 20.145 19.607 4.624
2024-12-01-16:27:41-root-INFO: Loss Change: 363.805 -> 354.675
2024-12-01-16:27:41-root-INFO: Regularization Change: 0.000 -> 1.278
2024-12-01-16:27:41-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-01-16:27:41-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-01-16:27:41-root-INFO: step: 113 lr_xt 0.05316701
2024-12-01-16:27:41-root-INFO: grad norm: 24.191 23.775 4.467
2024-12-01-16:27:41-root-INFO: Loss too large (355.574->371.437)! Learning rate decreased to 0.04253.
2024-12-01-16:27:41-root-INFO: Loss too large (355.574->363.439)! Learning rate decreased to 0.03403.
2024-12-01-16:27:41-root-INFO: Loss too large (355.574->357.928)! Learning rate decreased to 0.02722.
2024-12-01-16:27:42-root-INFO: grad norm: 22.923 22.378 4.969
2024-12-01-16:27:42-root-INFO: grad norm: 21.405 21.023 4.026
2024-12-01-16:27:43-root-INFO: grad norm: 20.610 20.091 4.596
2024-12-01-16:27:43-root-INFO: grad norm: 19.654 19.285 3.790
2024-12-01-16:27:44-root-INFO: Loss Change: 355.574 -> 347.388
2024-12-01-16:27:44-root-INFO: Regularization Change: 0.000 -> 1.296
2024-12-01-16:27:44-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-01-16:27:44-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-16:27:44-root-INFO: step: 112 lr_xt 0.05479712
2024-12-01-16:27:44-root-INFO: grad norm: 16.814 16.325 4.026
2024-12-01-16:27:44-root-INFO: Loss too large (346.403->353.750)! Learning rate decreased to 0.04384.
2024-12-01-16:27:44-root-INFO: Loss too large (346.403->348.781)! Learning rate decreased to 0.03507.
2024-12-01-16:27:45-root-INFO: grad norm: 22.900 22.483 4.351
2024-12-01-16:27:45-root-INFO: Loss too large (345.890->347.459)! Learning rate decreased to 0.02806.
2024-12-01-16:27:45-root-INFO: grad norm: 20.430 19.865 4.772
2024-12-01-16:27:46-root-INFO: grad norm: 17.828 17.470 3.557
2024-12-01-16:27:46-root-INFO: grad norm: 16.920 16.433 4.028
2024-12-01-16:27:47-root-INFO: Loss Change: 346.403 -> 338.587
2024-12-01-16:27:47-root-INFO: Regularization Change: 0.000 -> 1.277
2024-12-01-16:27:47-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-01-16:27:47-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-01-16:27:47-root-INFO: step: 111 lr_xt 0.05646812
2024-12-01-16:27:47-root-INFO: grad norm: 19.889 19.527 3.777
2024-12-01-16:27:47-root-INFO: Loss too large (339.347->350.261)! Learning rate decreased to 0.04517.
2024-12-01-16:27:47-root-INFO: Loss too large (339.347->344.144)! Learning rate decreased to 0.03614.
2024-12-01-16:27:47-root-INFO: Loss too large (339.347->340.135)! Learning rate decreased to 0.02891.
2024-12-01-16:27:48-root-INFO: grad norm: 18.461 17.968 4.237
2024-12-01-16:27:48-root-INFO: grad norm: 17.084 16.749 3.369
2024-12-01-16:27:49-root-INFO: grad norm: 16.152 15.695 3.815
2024-12-01-16:27:49-root-INFO: grad norm: 15.232 14.910 3.112
2024-12-01-16:27:50-root-INFO: Loss Change: 339.347 -> 331.380
2024-12-01-16:27:50-root-INFO: Regularization Change: 0.000 -> 1.199
2024-12-01-16:27:50-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-01-16:27:50-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-01-16:27:50-root-INFO: step: 110 lr_xt 0.05818072
2024-12-01-16:27:50-root-INFO: grad norm: 12.817 12.408 3.212
2024-12-01-16:27:50-root-INFO: Loss too large (330.783->333.647)! Learning rate decreased to 0.04654.
2024-12-01-16:27:50-root-INFO: Loss too large (330.783->331.143)! Learning rate decreased to 0.03724.
2024-12-01-16:27:51-root-INFO: grad norm: 16.578 16.231 3.375
2024-12-01-16:27:51-root-INFO: Loss too large (329.783->329.888)! Learning rate decreased to 0.02979.
2024-12-01-16:27:52-root-INFO: grad norm: 15.382 14.920 3.739
2024-12-01-16:27:52-root-INFO: grad norm: 14.210 13.888 3.008
2024-12-01-16:27:53-root-INFO: grad norm: 13.444 13.026 3.324
2024-12-01-16:27:53-root-INFO: Loss Change: 330.783 -> 323.811
2024-12-01-16:27:53-root-INFO: Regularization Change: 0.000 -> 1.196
2024-12-01-16:27:53-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-01-16:27:53-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-01-16:27:53-root-INFO: step: 109 lr_xt 0.05993563
2024-12-01-16:27:53-root-INFO: grad norm: 16.510 16.196 3.207
2024-12-01-16:27:53-root-INFO: Loss too large (324.662->332.429)! Learning rate decreased to 0.04795.
2024-12-01-16:27:54-root-INFO: Loss too large (324.662->327.555)! Learning rate decreased to 0.03836.
2024-12-01-16:27:54-root-INFO: grad norm: 21.220 20.732 4.526
2024-12-01-16:27:54-root-INFO: Loss too large (324.616->325.512)! Learning rate decreased to 0.03069.
2024-12-01-16:27:55-root-INFO: grad norm: 18.975 18.654 3.477
2024-12-01-16:27:55-root-INFO: grad norm: 17.441 17.028 3.771
2024-12-01-16:27:56-root-INFO: grad norm: 16.094 15.804 3.044
2024-12-01-16:27:56-root-INFO: Loss Change: 324.662 -> 317.673
2024-12-01-16:27:56-root-INFO: Regularization Change: 0.000 -> 1.294
2024-12-01-16:27:56-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-01-16:27:56-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-01-16:27:56-root-INFO: step: 108 lr_xt 0.06173354
2024-12-01-16:27:56-root-INFO: grad norm: 13.321 12.957 3.092
2024-12-01-16:27:56-root-INFO: Loss too large (316.858->321.039)! Learning rate decreased to 0.04939.
2024-12-01-16:27:57-root-INFO: Loss too large (316.858->317.881)! Learning rate decreased to 0.03951.
2024-12-01-16:27:57-root-INFO: grad norm: 17.069 16.766 3.199
2024-12-01-16:27:57-root-INFO: Loss too large (316.128->316.459)! Learning rate decreased to 0.03161.
2024-12-01-16:27:58-root-INFO: grad norm: 15.657 15.286 3.388
2024-12-01-16:27:58-root-INFO: grad norm: 14.503 14.237 2.764
2024-12-01-16:27:59-root-INFO: grad norm: 13.696 13.382 2.919
2024-12-01-16:27:59-root-INFO: Loss Change: 316.858 -> 310.312
2024-12-01-16:27:59-root-INFO: Regularization Change: 0.000 -> 1.216
2024-12-01-16:27:59-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-01-16:27:59-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-16:27:59-root-INFO: step: 107 lr_xt 0.06357517
2024-12-01-16:27:59-root-INFO: grad norm: 18.240 18.001 2.945
2024-12-01-16:27:59-root-INFO: Loss too large (311.615->323.136)! Learning rate decreased to 0.05086.
2024-12-01-16:27:59-root-INFO: Loss too large (311.615->316.555)! Learning rate decreased to 0.04069.
2024-12-01-16:28:00-root-INFO: Loss too large (311.615->312.401)! Learning rate decreased to 0.03255.
2024-12-01-16:28:00-root-INFO: grad norm: 17.357 17.080 3.088
2024-12-01-16:28:01-root-INFO: grad norm: 16.680 16.461 2.698
2024-12-01-16:28:01-root-INFO: grad norm: 16.092 15.829 2.895
2024-12-01-16:28:01-root-INFO: grad norm: 15.551 15.345 2.526
2024-12-01-16:28:02-root-INFO: Loss Change: 311.615 -> 304.725
2024-12-01-16:28:02-root-INFO: Regularization Change: 0.000 -> 1.246
2024-12-01-16:28:02-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-01-16:28:02-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-01-16:28:02-root-INFO: step: 106 lr_xt 0.06546120
2024-12-01-16:28:02-root-INFO: grad norm: 12.544 12.328 2.316
2024-12-01-16:28:02-root-INFO: Loss too large (303.666->307.466)! Learning rate decreased to 0.05237.
2024-12-01-16:28:02-root-INFO: Loss too large (303.666->304.716)! Learning rate decreased to 0.04190.
2024-12-01-16:28:03-root-INFO: grad norm: 16.399 16.224 2.395
2024-12-01-16:28:03-root-INFO: Loss too large (303.117->303.578)! Learning rate decreased to 0.03352.
2024-12-01-16:28:04-root-INFO: grad norm: 15.580 15.383 2.471
2024-12-01-16:28:04-root-INFO: grad norm: 15.018 14.850 2.241
2024-12-01-16:28:04-root-INFO: grad norm: 14.572 14.385 2.330
2024-12-01-16:28:05-root-INFO: Loss Change: 303.666 -> 297.935
2024-12-01-16:28:05-root-INFO: Regularization Change: 0.000 -> 1.251
2024-12-01-16:28:05-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-01-16:28:05-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-01-16:28:05-root-INFO: step: 105 lr_xt 0.06739236
2024-12-01-16:28:05-root-INFO: grad norm: 17.905 17.721 2.557
2024-12-01-16:28:05-root-INFO: Loss too large (299.119->310.367)! Learning rate decreased to 0.05391.
2024-12-01-16:28:05-root-INFO: Loss too large (299.119->304.262)! Learning rate decreased to 0.04313.
2024-12-01-16:28:06-root-INFO: Loss too large (299.119->300.127)! Learning rate decreased to 0.03450.
2024-12-01-16:28:06-root-INFO: grad norm: 17.193 17.015 2.467
2024-12-01-16:28:06-root-INFO: grad norm: 16.536 16.376 2.291
2024-12-01-16:28:07-root-INFO: grad norm: 15.947 15.772 2.350
2024-12-01-16:28:07-root-INFO: grad norm: 15.452 15.300 2.159
2024-12-01-16:28:08-root-INFO: Loss Change: 299.119 -> 292.591
2024-12-01-16:28:08-root-INFO: Regularization Change: 0.000 -> 1.271
2024-12-01-16:28:08-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-01-16:28:08-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-01-16:28:08-root-INFO: step: 104 lr_xt 0.06936934
2024-12-01-16:28:08-root-INFO: grad norm: 12.563 12.394 2.052
2024-12-01-16:28:08-root-INFO: Loss too large (291.814->296.294)! Learning rate decreased to 0.05550.
2024-12-01-16:28:08-root-INFO: Loss too large (291.814->293.349)! Learning rate decreased to 0.04440.
2024-12-01-16:28:09-root-INFO: grad norm: 16.421 16.293 2.046
2024-12-01-16:28:09-root-INFO: Loss too large (291.567->292.059)! Learning rate decreased to 0.03552.
2024-12-01-16:28:10-root-INFO: grad norm: 15.402 15.263 2.065
2024-12-01-16:28:10-root-INFO: grad norm: 14.804 14.675 1.951
2024-12-01-16:28:10-root-INFO: grad norm: 14.293 14.154 1.991
2024-12-01-16:28:11-root-INFO: Loss Change: 291.814 -> 286.459
2024-12-01-16:28:11-root-INFO: Regularization Change: 0.000 -> 1.268
2024-12-01-16:28:11-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-01-16:28:11-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-01-16:28:11-root-INFO: step: 103 lr_xt 0.07139284
2024-12-01-16:28:11-root-INFO: grad norm: 16.898 16.765 2.117
2024-12-01-16:28:11-root-INFO: Loss too large (287.274->297.458)! Learning rate decreased to 0.05711.
2024-12-01-16:28:11-root-INFO: Loss too large (287.274->291.814)! Learning rate decreased to 0.04569.
2024-12-01-16:28:12-root-INFO: Loss too large (287.274->287.946)! Learning rate decreased to 0.03655.
2024-12-01-16:28:12-root-INFO: grad norm: 15.816 15.681 2.061
2024-12-01-16:28:13-root-INFO: grad norm: 14.997 14.872 1.934
2024-12-01-16:28:13-root-INFO: grad norm: 14.296 14.158 1.983
2024-12-01-16:28:13-root-INFO: grad norm: 13.796 13.670 1.859
2024-12-01-16:28:14-root-INFO: Loss Change: 287.274 -> 280.760
2024-12-01-16:28:14-root-INFO: Regularization Change: 0.000 -> 1.259
2024-12-01-16:28:14-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-01-16:28:14-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-01-16:28:14-root-INFO: step: 102 lr_xt 0.07346356
2024-12-01-16:28:14-root-INFO: grad norm: 11.998 11.863 1.793
2024-12-01-16:28:14-root-INFO: Loss too large (280.058->284.659)! Learning rate decreased to 0.05877.
2024-12-01-16:28:14-root-INFO: Loss too large (280.058->281.738)! Learning rate decreased to 0.04702.
2024-12-01-16:28:15-root-INFO: grad norm: 15.949 15.843 1.832
2024-12-01-16:28:15-root-INFO: Loss too large (279.961->280.575)! Learning rate decreased to 0.03761.
2024-12-01-16:28:16-root-INFO: grad norm: 15.146 15.042 1.768
2024-12-01-16:28:16-root-INFO: grad norm: 14.626 14.519 1.773
2024-12-01-16:28:17-root-INFO: grad norm: 14.144 14.036 1.748
2024-12-01-16:28:17-root-INFO: Loss Change: 280.058 -> 275.150
2024-12-01-16:28:17-root-INFO: Regularization Change: 0.000 -> 1.294
2024-12-01-16:28:17-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-01-16:28:17-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-01-16:28:17-root-INFO: step: 101 lr_xt 0.07558219
2024-12-01-16:28:17-root-INFO: grad norm: 16.670 16.541 2.068
2024-12-01-16:28:17-root-INFO: Loss too large (276.091->286.545)! Learning rate decreased to 0.06047.
2024-12-01-16:28:18-root-INFO: Loss too large (276.091->280.827)! Learning rate decreased to 0.04837.
2024-12-01-16:28:18-root-INFO: Loss too large (276.091->276.832)! Learning rate decreased to 0.03870.
2024-12-01-16:28:18-root-INFO: grad norm: 15.631 15.517 1.884
2024-12-01-16:28:19-root-INFO: grad norm: 14.846 14.732 1.832
2024-12-01-16:28:19-root-INFO: grad norm: 14.139 14.020 1.830
2024-12-01-16:28:20-root-INFO: grad norm: 13.650 13.535 1.763
2024-12-01-16:28:20-root-INFO: Loss Change: 276.091 -> 269.705
2024-12-01-16:28:20-root-INFO: Regularization Change: 0.000 -> 1.305
2024-12-01-16:28:20-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-01-16:28:20-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-16:28:20-root-INFO: step: 100 lr_xt 0.07774943
2024-12-01-16:28:20-root-INFO: grad norm: 11.518 11.394 1.690
2024-12-01-16:28:20-root-INFO: Loss too large (269.238->273.670)! Learning rate decreased to 0.06220.
2024-12-01-16:28:21-root-INFO: Loss too large (269.238->270.895)! Learning rate decreased to 0.04976.
2024-12-01-16:28:21-root-INFO: grad norm: 14.955 14.855 1.724
2024-12-01-16:28:21-root-INFO: Loss too large (269.187->269.485)! Learning rate decreased to 0.03981.
2024-12-01-16:28:22-root-INFO: grad norm: 13.876 13.774 1.681
2024-12-01-16:28:22-root-INFO: grad norm: 13.322 13.216 1.677
2024-12-01-16:28:23-root-INFO: grad norm: 12.803 12.695 1.656
2024-12-01-16:28:23-root-INFO: Loss Change: 269.238 -> 264.415
2024-12-01-16:28:23-root-INFO: Regularization Change: 0.000 -> 1.291
2024-12-01-16:28:23-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-01-16:28:23-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-01-16:28:23-root-INFO: step: 99 lr_xt 0.07996596
2024-12-01-16:28:23-root-INFO: grad norm: 14.848 14.726 1.901
2024-12-01-16:28:24-root-INFO: Loss too large (264.971->273.829)! Learning rate decreased to 0.06397.
2024-12-01-16:28:24-root-INFO: Loss too large (264.971->268.786)! Learning rate decreased to 0.05118.
2024-12-01-16:28:24-root-INFO: Loss too large (264.971->265.355)! Learning rate decreased to 0.04094.
2024-12-01-16:28:24-root-INFO: grad norm: 13.943 13.831 1.762
2024-12-01-16:28:25-root-INFO: grad norm: 13.301 13.191 1.710
2024-12-01-16:28:25-root-INFO: grad norm: 12.695 12.581 1.694
2024-12-01-16:28:26-root-INFO: grad norm: 12.300 12.189 1.644
2024-12-01-16:28:26-root-INFO: Loss Change: 264.971 -> 258.967
2024-12-01-16:28:26-root-INFO: Regularization Change: 0.000 -> 1.283
2024-12-01-16:28:26-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-01-16:28:26-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-01-16:28:26-root-INFO: step: 98 lr_xt 0.08223248
2024-12-01-16:28:26-root-INFO: grad norm: 10.565 10.441 1.609
2024-12-01-16:28:27-root-INFO: Loss too large (258.597->262.201)! Learning rate decreased to 0.06579.
2024-12-01-16:28:27-root-INFO: Loss too large (258.597->259.862)! Learning rate decreased to 0.05263.
2024-12-01-16:28:27-root-INFO: grad norm: 13.609 13.509 1.647
2024-12-01-16:28:28-root-INFO: Loss too large (258.428->258.586)! Learning rate decreased to 0.04210.
2024-12-01-16:28:28-root-INFO: grad norm: 12.795 12.700 1.553
2024-12-01-16:28:29-root-INFO: grad norm: 12.390 12.289 1.579
2024-12-01-16:28:29-root-INFO: grad norm: 11.974 11.876 1.528
2024-12-01-16:28:29-root-INFO: Loss Change: 258.597 -> 254.031
2024-12-01-16:28:29-root-INFO: Regularization Change: 0.000 -> 1.304
2024-12-01-16:28:29-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-01-16:28:29-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-01-16:28:30-root-INFO: step: 97 lr_xt 0.08454965
2024-12-01-16:28:30-root-INFO: grad norm: 14.314 14.198 1.817
2024-12-01-16:28:30-root-INFO: Loss too large (254.596->263.357)! Learning rate decreased to 0.06764.
2024-12-01-16:28:30-root-INFO: Loss too large (254.596->258.392)! Learning rate decreased to 0.05411.
2024-12-01-16:28:30-root-INFO: Loss too large (254.596->254.966)! Learning rate decreased to 0.04329.
2024-12-01-16:28:31-root-INFO: grad norm: 13.562 13.466 1.608
2024-12-01-16:28:31-root-INFO: grad norm: 13.002 12.905 1.593
2024-12-01-16:28:32-root-INFO: grad norm: 12.402 12.305 1.551
2024-12-01-16:28:32-root-INFO: grad norm: 12.038 11.941 1.529
2024-12-01-16:28:32-root-INFO: Loss Change: 254.596 -> 248.790
2024-12-01-16:28:32-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-01-16:28:32-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-01-16:28:32-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-01-16:28:33-root-INFO: step: 96 lr_xt 0.08691815
2024-12-01-16:28:33-root-INFO: grad norm: 10.228 10.118 1.493
2024-12-01-16:28:33-root-INFO: Loss too large (248.350->251.972)! Learning rate decreased to 0.06953.
2024-12-01-16:28:33-root-INFO: Loss too large (248.350->249.684)! Learning rate decreased to 0.05563.
2024-12-01-16:28:33-root-INFO: grad norm: 12.919 12.827 1.545
2024-12-01-16:28:34-root-INFO: grad norm: 16.898 16.829 1.519
2024-12-01-16:28:34-root-INFO: Loss too large (248.188->249.478)! Learning rate decreased to 0.04450.
2024-12-01-16:28:35-root-INFO: grad norm: 14.180 14.097 1.533
2024-12-01-16:28:35-root-INFO: grad norm: 11.857 11.769 1.440
2024-12-01-16:28:35-root-INFO: Loss Change: 248.350 -> 243.830
2024-12-01-16:28:35-root-INFO: Regularization Change: 0.000 -> 1.424
2024-12-01-16:28:35-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-01-16:28:35-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-01-16:28:36-root-INFO: step: 95 lr_xt 0.08933865
2024-12-01-16:28:36-root-INFO: grad norm: 13.340 13.224 1.760
2024-12-01-16:28:36-root-INFO: Loss too large (244.705->252.463)! Learning rate decreased to 0.07147.
2024-12-01-16:28:36-root-INFO: Loss too large (244.705->247.769)! Learning rate decreased to 0.05718.
2024-12-01-16:28:37-root-INFO: grad norm: 17.209 17.126 1.695
2024-12-01-16:28:37-root-INFO: Loss too large (244.656->246.025)! Learning rate decreased to 0.04574.
2024-12-01-16:28:37-root-INFO: grad norm: 14.227 14.137 1.599
2024-12-01-16:28:38-root-INFO: grad norm: 11.673 11.581 1.461
2024-12-01-16:28:38-root-INFO: grad norm: 10.669 10.573 1.427
2024-12-01-16:28:39-root-INFO: Loss Change: 244.705 -> 238.770
2024-12-01-16:28:39-root-INFO: Regularization Change: 0.000 -> 1.386
2024-12-01-16:28:39-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-01-16:28:39-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-01-16:28:39-root-INFO: step: 94 lr_xt 0.09181181
2024-12-01-16:28:39-root-INFO: grad norm: 8.324 8.208 1.381
2024-12-01-16:28:39-root-INFO: Loss too large (238.199->239.911)! Learning rate decreased to 0.07345.
2024-12-01-16:28:39-root-INFO: Loss too large (238.199->238.508)! Learning rate decreased to 0.05876.
2024-12-01-16:28:40-root-INFO: grad norm: 10.294 10.205 1.350
2024-12-01-16:28:40-root-INFO: grad norm: 14.144 14.077 1.372
2024-12-01-16:28:40-root-INFO: Loss too large (237.447->238.667)! Learning rate decreased to 0.04701.
2024-12-01-16:28:41-root-INFO: grad norm: 12.582 12.505 1.385
2024-12-01-16:28:42-root-INFO: grad norm: 10.908 10.829 1.313
2024-12-01-16:28:42-root-INFO: Loss Change: 238.199 -> 234.167
2024-12-01-16:28:42-root-INFO: Regularization Change: 0.000 -> 1.377
2024-12-01-16:28:42-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-01-16:28:42-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-01-16:28:42-root-INFO: step: 93 lr_xt 0.09433829
2024-12-01-16:28:42-root-INFO: grad norm: 12.586 12.480 1.629
2024-12-01-16:28:42-root-INFO: Loss too large (234.691->242.393)! Learning rate decreased to 0.07547.
2024-12-01-16:28:42-root-INFO: Loss too large (234.691->237.715)! Learning rate decreased to 0.06038.
2024-12-01-16:28:43-root-INFO: grad norm: 16.716 16.644 1.549
2024-12-01-16:28:43-root-INFO: Loss too large (234.665->236.404)! Learning rate decreased to 0.04830.
2024-12-01-16:28:44-root-INFO: grad norm: 13.922 13.844 1.473
2024-12-01-16:28:44-root-INFO: grad norm: 11.123 11.045 1.313
2024-12-01-16:28:45-root-INFO: grad norm: 10.163 10.080 1.296
2024-12-01-16:28:45-root-INFO: Loss Change: 234.691 -> 229.095
2024-12-01-16:28:45-root-INFO: Regularization Change: 0.000 -> 1.380
2024-12-01-16:28:45-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-01-16:28:45-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-01-16:28:45-root-INFO: step: 92 lr_xt 0.09691873
2024-12-01-16:28:45-root-INFO: grad norm: 8.177 8.075 1.286
2024-12-01-16:28:45-root-INFO: Loss too large (228.795->230.609)! Learning rate decreased to 0.07753.
2024-12-01-16:28:46-root-INFO: Loss too large (228.795->229.179)! Learning rate decreased to 0.06203.
2024-12-01-16:28:46-root-INFO: grad norm: 9.864 9.785 1.248
2024-12-01-16:28:47-root-INFO: grad norm: 13.342 13.282 1.264
2024-12-01-16:28:47-root-INFO: Loss too large (227.982->229.102)! Learning rate decreased to 0.04962.
2024-12-01-16:28:47-root-INFO: grad norm: 11.674 11.606 1.256
2024-12-01-16:28:48-root-INFO: grad norm: 9.788 9.715 1.192
2024-12-01-16:28:48-root-INFO: Loss Change: 228.795 -> 224.793
2024-12-01-16:28:48-root-INFO: Regularization Change: 0.000 -> 1.363
2024-12-01-16:28:48-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-01-16:28:48-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-01-16:28:48-root-INFO: step: 91 lr_xt 0.09955376
2024-12-01-16:28:49-root-INFO: grad norm: 10.955 10.866 1.387
2024-12-01-16:28:49-root-INFO: Loss too large (225.274->231.457)! Learning rate decreased to 0.07964.
2024-12-01-16:28:49-root-INFO: Loss too large (225.274->227.488)! Learning rate decreased to 0.06371.
2024-12-01-16:28:49-root-INFO: grad norm: 14.553 14.492 1.335
2024-12-01-16:28:49-root-INFO: Loss too large (225.041->226.442)! Learning rate decreased to 0.05097.
2024-12-01-16:28:50-root-INFO: grad norm: 12.261 12.194 1.283
2024-12-01-16:28:51-root-INFO: grad norm: 9.756 9.686 1.164
2024-12-01-16:28:51-root-INFO: grad norm: 8.891 8.816 1.150
2024-12-01-16:28:51-root-INFO: Loss Change: 225.274 -> 220.235
2024-12-01-16:28:51-root-INFO: Regularization Change: 0.000 -> 1.306
2024-12-01-16:28:51-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-01-16:28:51-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-01-16:28:52-root-INFO: step: 90 lr_xt 0.10224402
2024-12-01-16:28:52-root-INFO: grad norm: 7.049 6.953 1.155
2024-12-01-16:28:52-root-INFO: Loss too large (220.143->221.117)! Learning rate decreased to 0.08180.
2024-12-01-16:28:52-root-INFO: grad norm: 10.503 10.436 1.184
2024-12-01-16:28:52-root-INFO: Loss too large (220.120->221.864)! Learning rate decreased to 0.06544.
2024-12-01-16:28:53-root-INFO: grad norm: 13.582 13.532 1.169
2024-12-01-16:28:53-root-INFO: Loss too large (219.583->220.802)! Learning rate decreased to 0.05235.
2024-12-01-16:28:54-root-INFO: grad norm: 11.334 11.274 1.164
2024-12-01-16:28:54-root-INFO: grad norm: 8.727 8.661 1.066
2024-12-01-16:28:54-root-INFO: Loss Change: 220.143 -> 216.137
2024-12-01-16:28:54-root-INFO: Regularization Change: 0.000 -> 1.458
2024-12-01-16:28:54-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-01-16:28:54-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-01-16:28:55-root-INFO: step: 89 lr_xt 0.10499012
2024-12-01-16:28:55-root-INFO: grad norm: 10.386 10.297 1.354
2024-12-01-16:28:55-root-INFO: Loss too large (216.583->222.260)! Learning rate decreased to 0.08399.
2024-12-01-16:28:55-root-INFO: Loss too large (216.583->218.409)! Learning rate decreased to 0.06719.
2024-12-01-16:28:56-root-INFO: grad norm: 13.419 13.362 1.237
2024-12-01-16:28:56-root-INFO: Loss too large (216.152->217.289)! Learning rate decreased to 0.05375.
2024-12-01-16:28:56-root-INFO: grad norm: 11.167 11.105 1.172
2024-12-01-16:28:57-root-INFO: grad norm: 8.667 8.603 1.050
2024-12-01-16:28:57-root-INFO: grad norm: 7.824 7.755 1.035
2024-12-01-16:28:57-root-INFO: Loss Change: 216.583 -> 211.596
2024-12-01-16:28:57-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-01-16:28:57-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-01-16:28:57-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-01-16:28:58-root-INFO: step: 88 lr_xt 0.10779268
2024-12-01-16:28:58-root-INFO: grad norm: 6.128 6.030 1.091
2024-12-01-16:28:58-root-INFO: Loss too large (211.644->211.957)! Learning rate decreased to 0.08623.
2024-12-01-16:28:59-root-INFO: grad norm: 8.927 8.863 1.067
2024-12-01-16:28:59-root-INFO: Loss too large (211.292->212.480)! Learning rate decreased to 0.06899.
2024-12-01-16:28:59-root-INFO: grad norm: 11.692 11.644 1.059
2024-12-01-16:28:59-root-INFO: Loss too large (210.782->211.630)! Learning rate decreased to 0.05519.
2024-12-01-16:29:00-root-INFO: grad norm: 9.947 9.892 1.049
2024-12-01-16:29:00-root-INFO: grad norm: 7.849 7.789 0.972
2024-12-01-16:29:01-root-INFO: Loss Change: 211.644 -> 207.797
2024-12-01-16:29:01-root-INFO: Regularization Change: 0.000 -> 1.447
2024-12-01-16:29:01-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-01-16:29:01-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-01-16:29:01-root-INFO: step: 87 lr_xt 0.11065228
2024-12-01-16:29:01-root-INFO: grad norm: 9.180 9.102 1.190
2024-12-01-16:29:01-root-INFO: Loss too large (208.007->212.437)! Learning rate decreased to 0.08852.
2024-12-01-16:29:01-root-INFO: Loss too large (208.007->209.252)! Learning rate decreased to 0.07082.
2024-12-01-16:29:02-root-INFO: grad norm: 11.713 11.664 1.075
2024-12-01-16:29:02-root-INFO: Loss too large (207.468->208.222)! Learning rate decreased to 0.05665.
2024-12-01-16:29:02-root-INFO: grad norm: 9.767 9.711 1.045
2024-12-01-16:29:03-root-INFO: grad norm: 7.585 7.526 0.940
2024-12-01-16:29:03-root-INFO: grad norm: 6.828 6.764 0.935
2024-12-01-16:29:04-root-INFO: Loss Change: 208.007 -> 203.387
2024-12-01-16:29:04-root-INFO: Regularization Change: 0.000 -> 1.281
2024-12-01-16:29:04-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-01-16:29:04-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-01-16:29:04-root-INFO: step: 86 lr_xt 0.11356952
2024-12-01-16:29:04-root-INFO: grad norm: 5.367 5.278 0.972
2024-12-01-16:29:04-root-INFO: grad norm: 8.906 8.851 0.983
2024-12-01-16:29:05-root-INFO: Loss too large (203.011->206.969)! Learning rate decreased to 0.09086.
2024-12-01-16:29:05-root-INFO: Loss too large (203.011->203.861)! Learning rate decreased to 0.07268.
2024-12-01-16:29:05-root-INFO: grad norm: 10.886 10.843 0.960
2024-12-01-16:29:05-root-INFO: Loss too large (202.148->202.826)! Learning rate decreased to 0.05815.
2024-12-01-16:29:06-root-INFO: grad norm: 9.000 8.950 0.950
2024-12-01-16:29:06-root-INFO: grad norm: 6.689 6.631 0.879
2024-12-01-16:29:07-root-INFO: Loss Change: 203.280 -> 199.247
2024-12-01-16:29:07-root-INFO: Regularization Change: 0.000 -> 1.614
2024-12-01-16:29:07-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-01-16:29:07-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-01-16:29:07-root-INFO: step: 85 lr_xt 0.11654496
2024-12-01-16:29:07-root-INFO: grad norm: 7.667 7.593 1.065
2024-12-01-16:29:07-root-INFO: Loss too large (199.478->202.406)! Learning rate decreased to 0.09324.
2024-12-01-16:29:07-root-INFO: Loss too large (199.478->200.097)! Learning rate decreased to 0.07459.
2024-12-01-16:29:08-root-INFO: grad norm: 9.820 9.773 0.960
2024-12-01-16:29:08-root-INFO: Loss too large (198.873->199.303)! Learning rate decreased to 0.05967.
2024-12-01-16:29:09-root-INFO: grad norm: 8.382 8.329 0.945
2024-12-01-16:29:09-root-INFO: grad norm: 6.689 6.632 0.867
2024-12-01-16:29:10-root-INFO: grad norm: 6.102 6.041 0.864
2024-12-01-16:29:10-root-INFO: Loss Change: 199.478 -> 195.351
2024-12-01-16:29:10-root-INFO: Regularization Change: 0.000 -> 1.242
2024-12-01-16:29:10-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-01-16:29:10-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-01-16:29:10-root-INFO: step: 84 lr_xt 0.11957917
2024-12-01-16:29:10-root-INFO: grad norm: 4.963 4.878 0.911
2024-12-01-16:29:11-root-INFO: grad norm: 8.218 8.168 0.906
2024-12-01-16:29:11-root-INFO: Loss too large (194.873->198.465)! Learning rate decreased to 0.09566.
2024-12-01-16:29:11-root-INFO: Loss too large (194.873->195.580)! Learning rate decreased to 0.07653.
2024-12-01-16:29:11-root-INFO: grad norm: 10.140 10.100 0.897
2024-12-01-16:29:12-root-INFO: Loss too large (194.034->194.659)! Learning rate decreased to 0.06122.
2024-12-01-16:29:12-root-INFO: grad norm: 8.366 8.320 0.878
2024-12-01-16:29:13-root-INFO: grad norm: 6.065 6.009 0.820
2024-12-01-16:29:13-root-INFO: Loss Change: 195.164 -> 191.302
2024-12-01-16:29:13-root-INFO: Regularization Change: 0.000 -> 1.609
2024-12-01-16:29:13-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-01-16:29:13-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-01-16:29:13-root-INFO: step: 83 lr_xt 0.12267269
2024-12-01-16:29:13-root-INFO: grad norm: 7.214 7.144 1.000
2024-12-01-16:29:13-root-INFO: Loss too large (191.620->194.182)! Learning rate decreased to 0.09814.
2024-12-01-16:29:14-root-INFO: Loss too large (191.620->192.067)! Learning rate decreased to 0.07851.
2024-12-01-16:29:14-root-INFO: grad norm: 9.010 8.966 0.892
2024-12-01-16:29:14-root-INFO: Loss too large (190.974->191.245)! Learning rate decreased to 0.06281.
2024-12-01-16:29:15-root-INFO: grad norm: 7.612 7.562 0.871
2024-12-01-16:29:15-root-INFO: grad norm: 6.006 5.953 0.803
2024-12-01-16:29:16-root-INFO: grad norm: 5.460 5.402 0.796
2024-12-01-16:29:16-root-INFO: Loss Change: 191.620 -> 187.645
2024-12-01-16:29:16-root-INFO: Regularization Change: 0.000 -> 1.238
2024-12-01-16:29:16-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-01-16:29:16-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-01-16:29:16-root-INFO: step: 82 lr_xt 0.12582604
2024-12-01-16:29:16-root-INFO: grad norm: 4.575 4.488 0.885
2024-12-01-16:29:17-root-INFO: grad norm: 7.213 7.161 0.859
2024-12-01-16:29:17-root-INFO: Loss too large (187.122->189.768)! Learning rate decreased to 0.10066.
2024-12-01-16:29:17-root-INFO: Loss too large (187.122->187.481)! Learning rate decreased to 0.08053.
2024-12-01-16:29:18-root-INFO: grad norm: 8.744 8.703 0.837
2024-12-01-16:29:18-root-INFO: Loss too large (186.319->186.628)! Learning rate decreased to 0.06442.
2024-12-01-16:29:18-root-INFO: grad norm: 7.281 7.236 0.808
2024-12-01-16:29:19-root-INFO: grad norm: 5.446 5.392 0.767
2024-12-01-16:29:19-root-INFO: Loss Change: 187.623 -> 183.868
2024-12-01-16:29:19-root-INFO: Regularization Change: 0.000 -> 1.612
2024-12-01-16:29:19-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-01-16:29:19-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-01-16:29:19-root-INFO: step: 81 lr_xt 0.12903975
2024-12-01-16:29:19-root-INFO: grad norm: 6.490 6.419 0.956
2024-12-01-16:29:19-root-INFO: Loss too large (184.012->185.902)! Learning rate decreased to 0.10323.
2024-12-01-16:29:20-root-INFO: Loss too large (184.012->184.196)! Learning rate decreased to 0.08259.
2024-12-01-16:29:20-root-INFO: grad norm: 7.971 7.929 0.815
2024-12-01-16:29:20-root-INFO: Loss too large (183.340->183.417)! Learning rate decreased to 0.06607.
2024-12-01-16:29:21-root-INFO: grad norm: 6.752 6.703 0.810
2024-12-01-16:29:21-root-INFO: grad norm: 5.386 5.334 0.745
2024-12-01-16:29:22-root-INFO: grad norm: 4.911 4.854 0.746
2024-12-01-16:29:22-root-INFO: Loss Change: 184.012 -> 180.260
2024-12-01-16:29:22-root-INFO: Regularization Change: 0.000 -> 1.234
2024-12-01-16:29:22-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-01-16:29:22-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-01-16:29:22-root-INFO: step: 80 lr_xt 0.13231432
2024-12-01-16:29:22-root-INFO: grad norm: 4.253 4.168 0.847
2024-12-01-16:29:23-root-INFO: grad norm: 6.010 5.957 0.794
2024-12-01-16:29:23-root-INFO: Loss too large (179.472->180.492)! Learning rate decreased to 0.10585.
2024-12-01-16:29:23-root-INFO: grad norm: 9.633 9.595 0.858
2024-12-01-16:29:24-root-INFO: Loss too large (179.225->181.037)! Learning rate decreased to 0.08468.
2024-12-01-16:29:24-root-INFO: Loss too large (179.225->179.472)! Learning rate decreased to 0.06774.
2024-12-01-16:29:24-root-INFO: grad norm: 7.226 7.186 0.757
2024-12-01-16:29:25-root-INFO: grad norm: 4.753 4.699 0.713
2024-12-01-16:29:25-root-INFO: Loss Change: 180.281 -> 176.361
2024-12-01-16:29:25-root-INFO: Regularization Change: 0.000 -> 1.793
2024-12-01-16:29:25-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-01-16:29:25-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-01-16:29:25-root-INFO: step: 79 lr_xt 0.13565022
2024-12-01-16:29:25-root-INFO: grad norm: 5.476 5.410 0.847
2024-12-01-16:29:26-root-INFO: Loss too large (176.466->177.260)! Learning rate decreased to 0.10852.
2024-12-01-16:29:26-root-INFO: grad norm: 9.276 9.239 0.827
2024-12-01-16:29:26-root-INFO: Loss too large (176.210->178.226)! Learning rate decreased to 0.08682.
2024-12-01-16:29:26-root-INFO: Loss too large (176.210->176.599)! Learning rate decreased to 0.06945.
2024-12-01-16:29:27-root-INFO: grad norm: 7.221 7.178 0.781
2024-12-01-16:29:27-root-INFO: grad norm: 4.735 4.682 0.705
2024-12-01-16:29:28-root-INFO: grad norm: 4.182 4.124 0.692
2024-12-01-16:29:28-root-INFO: Loss Change: 176.466 -> 172.769
2024-12-01-16:29:28-root-INFO: Regularization Change: 0.000 -> 1.365
2024-12-01-16:29:28-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-01-16:29:28-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-01-16:29:28-root-INFO: step: 78 lr_xt 0.13904792
2024-12-01-16:29:28-root-INFO: grad norm: 3.602 3.523 0.749
2024-12-01-16:29:29-root-INFO: grad norm: 4.819 4.769 0.694
2024-12-01-16:29:29-root-INFO: Loss too large (171.913->172.189)! Learning rate decreased to 0.11124.
2024-12-01-16:29:30-root-INFO: grad norm: 7.698 7.661 0.747
2024-12-01-16:29:30-root-INFO: Loss too large (171.483->172.518)! Learning rate decreased to 0.08899.
2024-12-01-16:29:30-root-INFO: Loss too large (171.483->171.496)! Learning rate decreased to 0.07119.
2024-12-01-16:29:30-root-INFO: grad norm: 6.064 6.024 0.695
2024-12-01-16:29:31-root-INFO: grad norm: 4.304 4.252 0.668
2024-12-01-16:29:31-root-INFO: Loss Change: 172.842 -> 169.134
2024-12-01-16:29:31-root-INFO: Regularization Change: 0.000 -> 1.790
2024-12-01-16:29:31-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-01-16:29:31-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-01-16:29:31-root-INFO: step: 77 lr_xt 0.14250787
2024-12-01-16:29:32-root-INFO: grad norm: 4.936 4.874 0.779
2024-12-01-16:29:32-root-INFO: Loss too large (169.131->169.470)! Learning rate decreased to 0.11401.
2024-12-01-16:29:32-root-INFO: grad norm: 7.744 7.705 0.777
2024-12-01-16:29:32-root-INFO: Loss too large (168.713->169.806)! Learning rate decreased to 0.09121.
2024-12-01-16:29:33-root-INFO: grad norm: 7.635 7.594 0.787
2024-12-01-16:29:33-root-INFO: grad norm: 7.163 7.124 0.751
2024-12-01-16:29:34-root-INFO: grad norm: 6.969 6.928 0.753
2024-12-01-16:29:34-root-INFO: Loss Change: 169.131 -> 165.731
2024-12-01-16:29:34-root-INFO: Regularization Change: 0.000 -> 1.963
2024-12-01-16:29:34-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-01-16:29:34-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-01-16:29:35-root-INFO: step: 76 lr_xt 0.14603050
2024-12-01-16:29:35-root-INFO: grad norm: 5.424 5.378 0.710
2024-12-01-16:29:35-root-INFO: Loss too large (165.364->165.777)! Learning rate decreased to 0.11682.
2024-12-01-16:29:35-root-INFO: grad norm: 6.439 6.401 0.695
2024-12-01-16:29:36-root-INFO: grad norm: 9.418 9.388 0.740
2024-12-01-16:29:36-root-INFO: Loss too large (164.560->166.141)! Learning rate decreased to 0.09346.
2024-12-01-16:29:36-root-INFO: Loss too large (164.560->164.613)! Learning rate decreased to 0.07477.
2024-12-01-16:29:37-root-INFO: grad norm: 6.149 6.110 0.690
2024-12-01-16:29:37-root-INFO: grad norm: 3.213 3.152 0.621
2024-12-01-16:29:38-root-INFO: Loss Change: 165.364 -> 161.572
2024-12-01-16:29:38-root-INFO: Regularization Change: 0.000 -> 1.619
2024-12-01-16:29:38-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-01-16:29:38-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-01-16:29:38-root-INFO: step: 75 lr_xt 0.14961620
2024-12-01-16:29:38-root-INFO: grad norm: 4.112 4.039 0.771
2024-12-01-16:29:39-root-INFO: grad norm: 6.573 6.524 0.803
2024-12-01-16:29:39-root-INFO: Loss too large (161.194->162.816)! Learning rate decreased to 0.11969.
2024-12-01-16:29:39-root-INFO: Loss too large (161.194->161.338)! Learning rate decreased to 0.09575.
2024-12-01-16:29:39-root-INFO: grad norm: 6.181 6.133 0.765
2024-12-01-16:29:40-root-INFO: grad norm: 5.991 5.949 0.709
2024-12-01-16:29:40-root-INFO: grad norm: 5.780 5.737 0.705
2024-12-01-16:29:41-root-INFO: Loss Change: 161.732 -> 158.242
2024-12-01-16:29:41-root-INFO: Regularization Change: 0.000 -> 2.132
2024-12-01-16:29:41-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-01-16:29:41-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-01-16:29:41-root-INFO: step: 74 lr_xt 0.15326538
2024-12-01-16:29:41-root-INFO: grad norm: 4.501 4.457 0.631
2024-12-01-16:29:42-root-INFO: grad norm: 6.515 6.480 0.682
2024-12-01-16:29:42-root-INFO: Loss too large (157.891->158.918)! Learning rate decreased to 0.12261.
2024-12-01-16:29:42-root-INFO: grad norm: 8.575 8.544 0.731
2024-12-01-16:29:42-root-INFO: Loss too large (157.168->158.196)! Learning rate decreased to 0.09809.
2024-12-01-16:29:43-root-INFO: grad norm: 7.051 7.012 0.738
2024-12-01-16:29:43-root-INFO: grad norm: 4.816 4.771 0.659
2024-12-01-16:29:44-root-INFO: Loss Change: 157.989 -> 154.282
2024-12-01-16:29:44-root-INFO: Regularization Change: 0.000 -> 2.292
2024-12-01-16:29:44-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-01-16:29:44-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-01-16:29:44-root-INFO: step: 73 lr_xt 0.15697839
2024-12-01-16:29:44-root-INFO: grad norm: 5.962 5.897 0.883
2024-12-01-16:29:44-root-INFO: Loss too large (154.665->155.464)! Learning rate decreased to 0.12558.
2024-12-01-16:29:45-root-INFO: grad norm: 7.412 7.366 0.826
2024-12-01-16:29:45-root-INFO: Loss too large (154.248->154.454)! Learning rate decreased to 0.10047.
2024-12-01-16:29:45-root-INFO: grad norm: 6.310 6.264 0.761
2024-12-01-16:29:46-root-INFO: grad norm: 5.336 5.293 0.674
2024-12-01-16:29:46-root-INFO: grad norm: 4.800 4.755 0.656
2024-12-01-16:29:47-root-INFO: Loss Change: 154.665 -> 150.795
2024-12-01-16:29:47-root-INFO: Regularization Change: 0.000 -> 2.004
2024-12-01-16:29:47-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-01-16:29:47-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-01-16:29:47-root-INFO: step: 72 lr_xt 0.16075558
2024-12-01-16:29:47-root-INFO: grad norm: 3.376 3.324 0.592
2024-12-01-16:29:48-root-INFO: grad norm: 4.491 4.453 0.582
2024-12-01-16:29:48-root-INFO: grad norm: 8.320 8.292 0.680
2024-12-01-16:29:48-root-INFO: Loss too large (149.727->152.559)! Learning rate decreased to 0.12860.
2024-12-01-16:29:48-root-INFO: Loss too large (149.727->150.461)! Learning rate decreased to 0.10288.
2024-12-01-16:29:49-root-INFO: grad norm: 6.479 6.446 0.662
2024-12-01-16:29:49-root-INFO: grad norm: 4.163 4.120 0.598
2024-12-01-16:29:50-root-INFO: Loss Change: 150.464 -> 146.744
2024-12-01-16:29:50-root-INFO: Regularization Change: 0.000 -> 2.485
2024-12-01-16:29:50-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-01-16:29:50-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-01-16:29:50-root-INFO: step: 71 lr_xt 0.16459726
2024-12-01-16:29:50-root-INFO: grad norm: 4.664 4.609 0.716
2024-12-01-16:29:50-root-INFO: Loss too large (146.998->147.023)! Learning rate decreased to 0.13168.
2024-12-01-16:29:51-root-INFO: grad norm: 5.496 5.452 0.695
2024-12-01-16:29:51-root-INFO: grad norm: 6.587 6.538 0.799
2024-12-01-16:29:52-root-INFO: grad norm: 7.970 7.923 0.864
2024-12-01-16:29:52-root-INFO: Loss too large (145.913->146.089)! Learning rate decreased to 0.10534.
2024-12-01-16:29:52-root-INFO: grad norm: 6.296 6.253 0.728
2024-12-01-16:29:53-root-INFO: Loss Change: 146.998 -> 143.587
2024-12-01-16:29:53-root-INFO: Regularization Change: 0.000 -> 2.322
2024-12-01-16:29:53-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-01-16:29:53-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-01-16:29:53-root-INFO: step: 70 lr_xt 0.16850375
2024-12-01-16:29:53-root-INFO: grad norm: 3.654 3.609 0.571
2024-12-01-16:29:53-root-INFO: grad norm: 4.755 4.720 0.573
2024-12-01-16:29:54-root-INFO: Loss too large (142.403->142.426)! Learning rate decreased to 0.13480.
2024-12-01-16:29:54-root-INFO: grad norm: 5.464 5.432 0.587
2024-12-01-16:29:54-root-INFO: grad norm: 6.078 6.042 0.664
2024-12-01-16:29:55-root-INFO: grad norm: 6.895 6.858 0.706
2024-12-01-16:29:55-root-INFO: Loss Change: 143.063 -> 140.821
2024-12-01-16:29:55-root-INFO: Regularization Change: 0.000 -> 3.198
2024-12-01-16:29:55-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-01-16:29:55-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-01-16:29:55-root-INFO: step: 69 lr_xt 0.17247530
2024-12-01-16:29:56-root-INFO: grad norm: 8.984 8.921 1.060
2024-12-01-16:29:56-root-INFO: Loss too large (141.556->144.423)! Learning rate decreased to 0.13798.
2024-12-01-16:29:56-root-INFO: grad norm: 9.648 9.591 1.049
2024-12-01-16:29:57-root-INFO: grad norm: 10.239 10.176 1.139
2024-12-01-16:29:57-root-INFO: grad norm: 10.950 10.880 1.229
2024-12-01-16:29:57-root-INFO: Loss too large (140.552->140.684)! Learning rate decreased to 0.11038.
2024-12-01-16:29:58-root-INFO: grad norm: 7.612 7.560 0.890
2024-12-01-16:29:58-root-INFO: Loss Change: 141.556 -> 136.446
2024-12-01-16:29:58-root-INFO: Regularization Change: 0.000 -> 2.918
2024-12-01-16:29:58-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-01-16:29:58-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-01-16:29:58-root-INFO: step: 68 lr_xt 0.17651217
2024-12-01-16:29:58-root-INFO: grad norm: 3.992 3.949 0.581
2024-12-01-16:29:59-root-INFO: grad norm: 5.514 5.471 0.685
2024-12-01-16:29:59-root-INFO: Loss too large (135.280->135.822)! Learning rate decreased to 0.14121.
2024-12-01-16:30:00-root-INFO: grad norm: 5.987 5.940 0.746
2024-12-01-16:30:00-root-INFO: grad norm: 6.585 6.535 0.815
2024-12-01-16:30:01-root-INFO: grad norm: 7.126 7.071 0.880
2024-12-01-16:30:01-root-INFO: Loss Change: 135.808 -> 133.459
2024-12-01-16:30:01-root-INFO: Regularization Change: 0.000 -> 3.295
2024-12-01-16:30:01-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-01-16:30:01-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-01-16:30:01-root-INFO: step: 67 lr_xt 0.18061458
2024-12-01-16:30:01-root-INFO: grad norm: 8.841 8.765 1.155
2024-12-01-16:30:02-root-INFO: Loss too large (134.092->136.900)! Learning rate decreased to 0.14449.
2024-12-01-16:30:02-root-INFO: grad norm: 9.216 9.144 1.143
2024-12-01-16:30:03-root-INFO: grad norm: 9.470 9.399 1.151
2024-12-01-16:30:03-root-INFO: grad norm: 9.736 9.659 1.219
2024-12-01-16:30:04-root-INFO: grad norm: 9.941 9.868 1.206
2024-12-01-16:30:04-root-INFO: Loss Change: 134.092 -> 131.585
2024-12-01-16:30:04-root-INFO: Regularization Change: 0.000 -> 4.220
2024-12-01-16:30:04-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-01-16:30:04-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-01-16:30:04-root-INFO: step: 66 lr_xt 0.18478272
2024-12-01-16:30:04-root-INFO: grad norm: 8.997 8.930 1.093
2024-12-01-16:30:05-root-INFO: Loss too large (130.920->133.445)! Learning rate decreased to 0.14783.
2024-12-01-16:30:05-root-INFO: grad norm: 8.918 8.852 1.079
2024-12-01-16:30:06-root-INFO: grad norm: 8.808 8.736 1.131
2024-12-01-16:30:06-root-INFO: grad norm: 8.742 8.676 1.069
2024-12-01-16:30:07-root-INFO: grad norm: 8.664 8.590 1.129
2024-12-01-16:30:07-root-INFO: Loss Change: 130.920 -> 127.254
2024-12-01-16:30:07-root-INFO: Regularization Change: 0.000 -> 4.088
2024-12-01-16:30:07-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-01-16:30:07-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-01-16:30:07-root-INFO: step: 65 lr_xt 0.18901677
2024-12-01-16:30:07-root-INFO: grad norm: 10.132 10.042 1.348
2024-12-01-16:30:08-root-INFO: Loss too large (128.304->131.520)! Learning rate decreased to 0.15121.
2024-12-01-16:30:08-root-INFO: grad norm: 9.496 9.410 1.275
2024-12-01-16:30:09-root-INFO: grad norm: 8.764 8.695 1.097
2024-12-01-16:30:09-root-INFO: grad norm: 8.394 8.317 1.132
2024-12-01-16:30:10-root-INFO: grad norm: 8.296 8.230 1.045
2024-12-01-16:30:10-root-INFO: Loss Change: 128.304 -> 123.699
2024-12-01-16:30:10-root-INFO: Regularization Change: 0.000 -> 4.368
2024-12-01-16:30:10-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-01-16:30:10-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-01-16:30:10-root-INFO: step: 64 lr_xt 0.19331686
2024-12-01-16:30:10-root-INFO: grad norm: 7.139 7.076 0.948
2024-12-01-16:30:10-root-INFO: Loss too large (123.104->124.216)! Learning rate decreased to 0.15465.
2024-12-01-16:30:11-root-INFO: grad norm: 6.999 6.939 0.913
2024-12-01-16:30:11-root-INFO: grad norm: 6.943 6.876 0.965
2024-12-01-16:30:12-root-INFO: grad norm: 6.976 6.915 0.920
2024-12-01-16:30:12-root-INFO: grad norm: 6.944 6.875 0.977
2024-12-01-16:30:13-root-INFO: Loss Change: 123.104 -> 119.496
2024-12-01-16:30:13-root-INFO: Regularization Change: 0.000 -> 3.604
2024-12-01-16:30:13-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-01-16:30:13-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-01-16:30:13-root-INFO: step: 63 lr_xt 0.19768311
2024-12-01-16:30:13-root-INFO: grad norm: 8.052 7.970 1.144
2024-12-01-16:30:13-root-INFO: Loss too large (120.201->122.291)! Learning rate decreased to 0.15815.
2024-12-01-16:30:14-root-INFO: grad norm: 7.742 7.666 1.083
2024-12-01-16:30:14-root-INFO: grad norm: 7.357 7.289 0.998
2024-12-01-16:30:15-root-INFO: grad norm: 7.206 7.134 1.020
2024-12-01-16:30:15-root-INFO: grad norm: 7.174 7.109 0.964
2024-12-01-16:30:16-root-INFO: Loss Change: 120.201 -> 116.649
2024-12-01-16:30:16-root-INFO: Regularization Change: 0.000 -> 3.887
2024-12-01-16:30:16-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-01-16:30:16-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-01-16:30:16-root-INFO: step: 62 lr_xt 0.20211560
2024-12-01-16:30:16-root-INFO: grad norm: 6.116 6.055 0.861
2024-12-01-16:30:16-root-INFO: Loss too large (115.889->116.388)! Learning rate decreased to 0.16169.
2024-12-01-16:30:17-root-INFO: grad norm: 5.981 5.925 0.819
2024-12-01-16:30:17-root-INFO: grad norm: 5.874 5.811 0.855
2024-12-01-16:30:18-root-INFO: grad norm: 5.811 5.758 0.783
2024-12-01-16:30:18-root-INFO: grad norm: 5.717 5.655 0.839
2024-12-01-16:30:19-root-INFO: Loss Change: 115.889 -> 112.195
2024-12-01-16:30:19-root-INFO: Regularization Change: 0.000 -> 3.400
2024-12-01-16:30:19-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-01-16:30:19-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-01-16:30:19-root-INFO: step: 61 lr_xt 0.20661437
2024-12-01-16:30:19-root-INFO: grad norm: 6.366 6.304 0.883
2024-12-01-16:30:19-root-INFO: Loss too large (112.511->113.604)! Learning rate decreased to 0.16529.
2024-12-01-16:30:20-root-INFO: grad norm: 6.026 5.961 0.883
2024-12-01-16:30:20-root-INFO: grad norm: 5.580 5.528 0.762
2024-12-01-16:30:21-root-INFO: grad norm: 5.472 5.412 0.812
2024-12-01-16:30:21-root-INFO: grad norm: 5.495 5.446 0.730
2024-12-01-16:30:21-root-INFO: Loss Change: 112.511 -> 109.207
2024-12-01-16:30:21-root-INFO: Regularization Change: 0.000 -> 3.444
2024-12-01-16:30:21-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-01-16:30:21-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-01-16:30:21-root-INFO: step: 60 lr_xt 0.21117946
2024-12-01-16:30:22-root-INFO: grad norm: 4.762 4.716 0.658
2024-12-01-16:30:22-root-INFO: grad norm: 7.102 7.047 0.875
2024-12-01-16:30:22-root-INFO: Loss too large (108.775->110.342)! Learning rate decreased to 0.16894.
2024-12-01-16:30:23-root-INFO: grad norm: 6.202 6.137 0.895
2024-12-01-16:30:23-root-INFO: grad norm: 4.981 4.930 0.711
2024-12-01-16:30:24-root-INFO: grad norm: 4.762 4.705 0.732
2024-12-01-16:30:24-root-INFO: Loss Change: 108.827 -> 105.420
2024-12-01-16:30:24-root-INFO: Regularization Change: 0.000 -> 3.443
2024-12-01-16:30:24-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-01-16:30:24-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-01-16:30:24-root-INFO: step: 59 lr_xt 0.21581084
2024-12-01-16:30:24-root-INFO: grad norm: 5.928 5.864 0.872
2024-12-01-16:30:24-root-INFO: Loss too large (105.816->106.796)! Learning rate decreased to 0.17265.
2024-12-01-16:30:25-root-INFO: grad norm: 5.776 5.712 0.858
2024-12-01-16:30:25-root-INFO: grad norm: 5.727 5.674 0.777
2024-12-01-16:30:26-root-INFO: grad norm: 5.640 5.575 0.853
2024-12-01-16:30:26-root-INFO: grad norm: 5.558 5.505 0.768
2024-12-01-16:30:27-root-INFO: Loss Change: 105.816 -> 102.884
2024-12-01-16:30:27-root-INFO: Regularization Change: 0.000 -> 3.509
2024-12-01-16:30:27-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-01-16:30:27-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-01-16:30:27-root-INFO: step: 58 lr_xt 0.22050848
2024-12-01-16:30:27-root-INFO: grad norm: 5.081 5.023 0.763
2024-12-01-16:30:27-root-INFO: Loss too large (102.409->102.702)! Learning rate decreased to 0.17641.
2024-12-01-16:30:28-root-INFO: grad norm: 5.247 5.196 0.729
2024-12-01-16:30:28-root-INFO: grad norm: 5.323 5.259 0.823
2024-12-01-16:30:29-root-INFO: grad norm: 5.370 5.316 0.753
2024-12-01-16:30:29-root-INFO: grad norm: 5.356 5.292 0.827
2024-12-01-16:30:30-root-INFO: Loss Change: 102.409 -> 99.245
2024-12-01-16:30:30-root-INFO: Regularization Change: 0.000 -> 3.391
2024-12-01-16:30:30-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-01-16:30:30-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-01-16:30:30-root-INFO: step: 57 lr_xt 0.22527231
2024-12-01-16:30:30-root-INFO: grad norm: 6.271 6.210 0.874
2024-12-01-16:30:30-root-INFO: Loss too large (99.462->100.756)! Learning rate decreased to 0.18022.
2024-12-01-16:30:31-root-INFO: grad norm: 5.849 5.777 0.915
2024-12-01-16:30:31-root-INFO: grad norm: 5.239 5.186 0.741
2024-12-01-16:30:32-root-INFO: grad norm: 5.170 5.106 0.810
2024-12-01-16:30:32-root-INFO: grad norm: 5.278 5.230 0.712
2024-12-01-16:30:32-root-INFO: Loss Change: 99.462 -> 96.324
2024-12-01-16:30:32-root-INFO: Regularization Change: 0.000 -> 3.658
2024-12-01-16:30:32-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-01-16:30:32-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-01-16:30:33-root-INFO: step: 56 lr_xt 0.23010221
2024-12-01-16:30:33-root-INFO: grad norm: 4.902 4.853 0.691
2024-12-01-16:30:33-root-INFO: Loss too large (96.088->96.171)! Learning rate decreased to 0.18408.
2024-12-01-16:30:33-root-INFO: grad norm: 4.830 4.788 0.634
2024-12-01-16:30:34-root-INFO: grad norm: 4.849 4.797 0.711
2024-12-01-16:30:34-root-INFO: grad norm: 4.923 4.882 0.634
2024-12-01-16:30:35-root-INFO: grad norm: 4.909 4.855 0.723
2024-12-01-16:30:35-root-INFO: Loss Change: 96.088 -> 92.818
2024-12-01-16:30:35-root-INFO: Regularization Change: 0.000 -> 3.426
2024-12-01-16:30:35-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-01-16:30:35-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-01-16:30:35-root-INFO: step: 55 lr_xt 0.23499803
2024-12-01-16:30:35-root-INFO: grad norm: 5.784 5.726 0.823
2024-12-01-16:30:36-root-INFO: Loss too large (93.107->94.247)! Learning rate decreased to 0.18800.
2024-12-01-16:30:36-root-INFO: grad norm: 5.456 5.391 0.838
2024-12-01-16:30:37-root-INFO: grad norm: 4.983 4.934 0.696
2024-12-01-16:30:37-root-INFO: grad norm: 4.966 4.907 0.766
2024-12-01-16:30:38-root-INFO: grad norm: 5.095 5.049 0.683
2024-12-01-16:30:38-root-INFO: Loss Change: 93.107 -> 90.247
2024-12-01-16:30:38-root-INFO: Regularization Change: 0.000 -> 3.659
2024-12-01-16:30:38-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-01-16:30:38-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-01-16:30:38-root-INFO: step: 54 lr_xt 0.23995961
2024-12-01-16:30:38-root-INFO: grad norm: 4.706 4.661 0.650
2024-12-01-16:30:39-root-INFO: Loss too large (89.858->89.966)! Learning rate decreased to 0.19197.
2024-12-01-16:30:39-root-INFO: grad norm: 4.749 4.708 0.620
2024-12-01-16:30:40-root-INFO: grad norm: 4.799 4.749 0.696
2024-12-01-16:30:40-root-INFO: grad norm: 4.868 4.825 0.641
2024-12-01-16:30:41-root-INFO: grad norm: 4.887 4.834 0.723
2024-12-01-16:30:41-root-INFO: Loss Change: 89.858 -> 86.758
2024-12-01-16:30:41-root-INFO: Regularization Change: 0.000 -> 3.488
2024-12-01-16:30:41-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-01-16:30:41-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-01-16:30:41-root-INFO: step: 53 lr_xt 0.24498673
2024-12-01-16:30:41-root-INFO: grad norm: 5.745 5.688 0.807
2024-12-01-16:30:41-root-INFO: Loss too large (87.074->88.326)! Learning rate decreased to 0.19599.
2024-12-01-16:30:42-root-INFO: grad norm: 5.444 5.378 0.843
2024-12-01-16:30:42-root-INFO: grad norm: 5.004 4.954 0.704
2024-12-01-16:30:43-root-INFO: grad norm: 5.011 4.950 0.780
2024-12-01-16:30:43-root-INFO: grad norm: 5.144 5.096 0.697
2024-12-01-16:30:44-root-INFO: Loss Change: 87.074 -> 84.408
2024-12-01-16:30:44-root-INFO: Regularization Change: 0.000 -> 3.730
2024-12-01-16:30:44-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-01-16:30:44-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-01-16:30:44-root-INFO: step: 52 lr_xt 0.25007913
2024-12-01-16:30:44-root-INFO: grad norm: 4.791 4.742 0.684
2024-12-01-16:30:44-root-INFO: Loss too large (84.105->84.332)! Learning rate decreased to 0.20006.
2024-12-01-16:30:45-root-INFO: grad norm: 4.767 4.726 0.622
2024-12-01-16:30:45-root-INFO: grad norm: 4.816 4.763 0.715
2024-12-01-16:30:46-root-INFO: grad norm: 4.919 4.877 0.636
2024-12-01-16:30:46-root-INFO: grad norm: 4.949 4.894 0.735
2024-12-01-16:30:47-root-INFO: Loss Change: 84.105 -> 81.189
2024-12-01-16:30:47-root-INFO: Regularization Change: 0.000 -> 3.514
2024-12-01-16:30:47-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-01-16:30:47-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-01-16:30:47-root-INFO: step: 51 lr_xt 0.25523653
2024-12-01-16:30:47-root-INFO: grad norm: 5.969 5.905 0.876
2024-12-01-16:30:47-root-INFO: Loss too large (81.566->83.162)! Learning rate decreased to 0.20419.
2024-12-01-16:30:48-root-INFO: grad norm: 5.683 5.618 0.859
2024-12-01-16:30:48-root-INFO: grad norm: 5.304 5.255 0.725
2024-12-01-16:30:49-root-INFO: grad norm: 5.278 5.218 0.790
2024-12-01-16:30:49-root-INFO: grad norm: 5.338 5.291 0.702
2024-12-01-16:30:49-root-INFO: Loss Change: 81.566 -> 79.109
2024-12-01-16:30:49-root-INFO: Regularization Change: 0.000 -> 3.862
2024-12-01-16:30:49-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-01-16:30:49-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-01-16:30:50-root-INFO: step: 50 lr_xt 0.26045862
2024-12-01-16:30:50-root-INFO: grad norm: 4.835 4.792 0.646
2024-12-01-16:30:50-root-INFO: Loss too large (78.469->78.843)! Learning rate decreased to 0.20837.
2024-12-01-16:30:50-root-INFO: grad norm: 4.913 4.875 0.605
2024-12-01-16:30:51-root-INFO: grad norm: 4.973 4.923 0.705
2024-12-01-16:30:51-root-INFO: grad norm: 5.068 5.029 0.630
2024-12-01-16:30:52-root-INFO: grad norm: 5.084 5.032 0.732
2024-12-01-16:30:52-root-INFO: Loss Change: 78.469 -> 75.670
2024-12-01-16:30:52-root-INFO: Regularization Change: 0.000 -> 3.583
2024-12-01-16:30:52-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-01-16:30:52-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-01-16:30:52-root-INFO: step: 49 lr_xt 0.26574501
2024-12-01-16:30:53-root-INFO: grad norm: 6.113 6.052 0.864
2024-12-01-16:30:53-root-INFO: Loss too large (76.143->77.883)! Learning rate decreased to 0.21260.
2024-12-01-16:30:53-root-INFO: grad norm: 5.712 5.645 0.870
2024-12-01-16:30:54-root-INFO: grad norm: 5.196 5.149 0.700
2024-12-01-16:30:54-root-INFO: grad norm: 5.182 5.123 0.777
2024-12-01-16:30:55-root-INFO: grad norm: 5.270 5.225 0.685
2024-12-01-16:30:55-root-INFO: Loss Change: 76.143 -> 73.638
2024-12-01-16:30:55-root-INFO: Regularization Change: 0.000 -> 3.992
2024-12-01-16:30:55-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-01-16:30:55-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-01-16:30:55-root-INFO: step: 48 lr_xt 0.27109532
2024-12-01-16:30:55-root-INFO: grad norm: 4.869 4.827 0.635
2024-12-01-16:30:56-root-INFO: Loss too large (73.277->73.706)! Learning rate decreased to 0.21688.
2024-12-01-16:30:56-root-INFO: grad norm: 4.878 4.843 0.587
2024-12-01-16:30:57-root-INFO: grad norm: 4.934 4.885 0.697
2024-12-01-16:30:57-root-INFO: grad norm: 5.016 4.978 0.617
2024-12-01-16:30:58-root-INFO: grad norm: 5.047 4.995 0.724
2024-12-01-16:30:58-root-INFO: Loss Change: 73.277 -> 70.552
2024-12-01-16:30:58-root-INFO: Regularization Change: 0.000 -> 3.681
2024-12-01-16:30:58-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-01-16:30:58-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-01-16:30:58-root-INFO: step: 47 lr_xt 0.27650911
2024-12-01-16:30:58-root-INFO: grad norm: 6.121 6.053 0.914
2024-12-01-16:30:59-root-INFO: Loss too large (71.045->73.004)! Learning rate decreased to 0.22121.
2024-12-01-16:30:59-root-INFO: Loss too large (71.045->71.060)! Learning rate decreased to 0.17697.
2024-12-01-16:30:59-root-INFO: grad norm: 4.344 4.300 0.612
2024-12-01-16:31:00-root-INFO: grad norm: 3.088 3.060 0.411
2024-12-01-16:31:00-root-INFO: grad norm: 2.671 2.646 0.363
2024-12-01-16:31:01-root-INFO: grad norm: 2.427 2.407 0.315
2024-12-01-16:31:01-root-INFO: Loss Change: 71.045 -> 67.449
2024-12-01-16:31:01-root-INFO: Regularization Change: 0.000 -> 2.303
2024-12-01-16:31:01-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-01-16:31:01-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-01-16:31:01-root-INFO: step: 46 lr_xt 0.28198590
2024-12-01-16:31:01-root-INFO: grad norm: 2.083 2.063 0.289
2024-12-01-16:31:02-root-INFO: grad norm: 2.907 2.894 0.281
2024-12-01-16:31:02-root-INFO: Loss too large (66.789->66.964)! Learning rate decreased to 0.22559.
2024-12-01-16:31:02-root-INFO: grad norm: 3.530 3.510 0.373
2024-12-01-16:31:03-root-INFO: grad norm: 4.438 4.417 0.433
2024-12-01-16:31:03-root-INFO: Loss too large (66.395->66.509)! Learning rate decreased to 0.18047.
2024-12-01-16:31:04-root-INFO: grad norm: 3.745 3.718 0.445
2024-12-01-16:31:04-root-INFO: Loss Change: 67.251 -> 65.195
2024-12-01-16:31:04-root-INFO: Regularization Change: 0.000 -> 2.693
2024-12-01-16:31:04-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-01-16:31:04-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-01-16:31:04-root-INFO: step: 45 lr_xt 0.28752516
2024-12-01-16:31:04-root-INFO: grad norm: 4.015 3.971 0.594
2024-12-01-16:31:04-root-INFO: Loss too large (65.268->66.190)! Learning rate decreased to 0.23002.
2024-12-01-16:31:04-root-INFO: Loss too large (65.268->65.274)! Learning rate decreased to 0.18402.
2024-12-01-16:31:05-root-INFO: grad norm: 3.527 3.498 0.456
2024-12-01-16:31:05-root-INFO: grad norm: 3.194 3.170 0.391
2024-12-01-16:31:06-root-INFO: grad norm: 3.044 3.019 0.387
2024-12-01-16:31:06-root-INFO: grad norm: 2.946 2.924 0.355
2024-12-01-16:31:07-root-INFO: Loss Change: 65.268 -> 62.981
2024-12-01-16:31:07-root-INFO: Regularization Change: 0.000 -> 2.108
2024-12-01-16:31:07-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-01-16:31:07-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-01-16:31:07-root-INFO: step: 44 lr_xt 0.29312635
2024-12-01-16:31:07-root-INFO: grad norm: 2.584 2.568 0.291
2024-12-01-16:31:08-root-INFO: grad norm: 4.281 4.262 0.398
2024-12-01-16:31:08-root-INFO: Loss too large (62.821->63.938)! Learning rate decreased to 0.23450.
2024-12-01-16:31:08-root-INFO: Loss too large (62.821->62.942)! Learning rate decreased to 0.18760.
2024-12-01-16:31:08-root-INFO: grad norm: 3.794 3.767 0.450
2024-12-01-16:31:09-root-INFO: grad norm: 3.485 3.461 0.411
2024-12-01-16:31:09-root-INFO: grad norm: 3.343 3.316 0.418
2024-12-01-16:31:10-root-INFO: Loss Change: 62.838 -> 60.919
2024-12-01-16:31:10-root-INFO: Regularization Change: 0.000 -> 2.411
2024-12-01-16:31:10-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-01-16:31:10-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-01-16:31:10-root-INFO: step: 43 lr_xt 0.29878886
2024-12-01-16:31:10-root-INFO: grad norm: 3.991 3.947 0.591
2024-12-01-16:31:10-root-INFO: Loss too large (60.912->62.005)! Learning rate decreased to 0.23903.
2024-12-01-16:31:10-root-INFO: Loss too large (60.912->61.002)! Learning rate decreased to 0.19122.
2024-12-01-16:31:11-root-INFO: grad norm: 3.653 3.621 0.481
2024-12-01-16:31:11-root-INFO: grad norm: 3.433 3.406 0.425
2024-12-01-16:31:12-root-INFO: grad norm: 3.309 3.282 0.426
2024-12-01-16:31:12-root-INFO: grad norm: 3.224 3.199 0.396
2024-12-01-16:31:13-root-INFO: Loss Change: 60.912 -> 58.731
2024-12-01-16:31:13-root-INFO: Regularization Change: 0.000 -> 2.205
2024-12-01-16:31:13-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-01-16:31:13-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-01-16:31:13-root-INFO: step: 42 lr_xt 0.30451205
2024-12-01-16:31:13-root-INFO: grad norm: 2.876 2.860 0.302
2024-12-01-16:31:13-root-INFO: Loss too large (58.464->58.792)! Learning rate decreased to 0.24361.
2024-12-01-16:31:14-root-INFO: grad norm: 3.768 3.748 0.390
2024-12-01-16:31:14-root-INFO: Loss too large (58.295->58.402)! Learning rate decreased to 0.19489.
2024-12-01-16:31:14-root-INFO: grad norm: 3.552 3.526 0.427
2024-12-01-16:31:15-root-INFO: grad norm: 3.406 3.383 0.398
2024-12-01-16:31:15-root-INFO: grad norm: 3.319 3.293 0.412
2024-12-01-16:31:16-root-INFO: Loss Change: 58.464 -> 56.643
2024-12-01-16:31:16-root-INFO: Regularization Change: 0.000 -> 2.280
2024-12-01-16:31:16-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-01-16:31:16-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-01-16:31:16-root-INFO: step: 41 lr_xt 0.31082203
2024-12-01-16:31:16-root-INFO: grad norm: 4.264 4.210 0.679
2024-12-01-16:31:16-root-INFO: Loss too large (56.888->58.212)! Learning rate decreased to 0.24866.
2024-12-01-16:31:16-root-INFO: Loss too large (56.888->56.992)! Learning rate decreased to 0.19893.
2024-12-01-16:31:17-root-INFO: grad norm: 3.811 3.777 0.508
2024-12-01-16:31:17-root-INFO: grad norm: 3.537 3.510 0.438
2024-12-01-16:31:18-root-INFO: grad norm: 3.357 3.329 0.436
2024-12-01-16:31:18-root-INFO: grad norm: 3.231 3.206 0.398
2024-12-01-16:31:19-root-INFO: Loss Change: 56.888 -> 54.517
2024-12-01-16:31:19-root-INFO: Regularization Change: 0.000 -> 2.378
2024-12-01-16:31:19-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-01-16:31:19-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-01-16:31:19-root-INFO: step: 40 lr_xt 0.31666177
2024-12-01-16:31:19-root-INFO: grad norm: 2.714 2.697 0.309
2024-12-01-16:31:19-root-INFO: Loss too large (54.221->54.348)! Learning rate decreased to 0.25333.
2024-12-01-16:31:20-root-INFO: grad norm: 3.381 3.360 0.378
2024-12-01-16:31:20-root-INFO: grad norm: 4.372 4.336 0.555
2024-12-01-16:31:20-root-INFO: Loss too large (53.941->54.153)! Learning rate decreased to 0.20266.
2024-12-01-16:31:21-root-INFO: grad norm: 3.812 3.781 0.484
2024-12-01-16:31:21-root-INFO: grad norm: 3.460 3.428 0.466
2024-12-01-16:31:22-root-INFO: Loss Change: 54.221 -> 52.361
2024-12-01-16:31:22-root-INFO: Regularization Change: 0.000 -> 2.583
2024-12-01-16:31:22-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-01-16:31:22-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-01-16:31:22-root-INFO: step: 39 lr_xt 0.32255964
2024-12-01-16:31:22-root-INFO: grad norm: 3.859 3.811 0.606
2024-12-01-16:31:22-root-INFO: Loss too large (52.427->53.528)! Learning rate decreased to 0.25805.
2024-12-01-16:31:22-root-INFO: Loss too large (52.427->52.432)! Learning rate decreased to 0.20644.
2024-12-01-16:31:23-root-INFO: grad norm: 3.430 3.397 0.474
2024-12-01-16:31:23-root-INFO: grad norm: 3.177 3.150 0.416
2024-12-01-16:31:24-root-INFO: grad norm: 2.994 2.968 0.397
2024-12-01-16:31:24-root-INFO: grad norm: 2.874 2.851 0.366
2024-12-01-16:31:25-root-INFO: Loss Change: 52.427 -> 50.136
2024-12-01-16:31:25-root-INFO: Regularization Change: 0.000 -> 2.331
2024-12-01-16:31:25-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-01-16:31:25-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-01-16:31:25-root-INFO: step: 38 lr_xt 0.32851483
2024-12-01-16:31:25-root-INFO: grad norm: 2.509 2.490 0.309
2024-12-01-16:31:26-root-INFO: grad norm: 3.982 3.958 0.444
2024-12-01-16:31:26-root-INFO: Loss too large (49.857->51.023)! Learning rate decreased to 0.26281.
2024-12-01-16:31:26-root-INFO: Loss too large (49.857->49.883)! Learning rate decreased to 0.21025.
2024-12-01-16:31:26-root-INFO: grad norm: 3.463 3.434 0.449
2024-12-01-16:31:27-root-INFO: grad norm: 3.163 3.136 0.411
2024-12-01-16:31:27-root-INFO: grad norm: 2.959 2.934 0.389
2024-12-01-16:31:28-root-INFO: Loss Change: 49.858 -> 47.932
2024-12-01-16:31:28-root-INFO: Regularization Change: 0.000 -> 2.628
2024-12-01-16:31:28-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-01-16:31:28-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-01-16:31:28-root-INFO: step: 37 lr_xt 0.33452649
2024-12-01-16:31:28-root-INFO: grad norm: 3.606 3.559 0.581
2024-12-01-16:31:28-root-INFO: Loss too large (48.028->48.918)! Learning rate decreased to 0.26762.
2024-12-01-16:31:29-root-INFO: grad norm: 4.482 4.435 0.648
2024-12-01-16:31:29-root-INFO: Loss too large (47.936->48.171)! Learning rate decreased to 0.21410.
2024-12-01-16:31:30-root-INFO: grad norm: 3.724 3.693 0.475
2024-12-01-16:31:30-root-INFO: grad norm: 3.138 3.109 0.425
2024-12-01-16:31:31-root-INFO: grad norm: 2.817 2.793 0.365
2024-12-01-16:31:31-root-INFO: Loss Change: 48.028 -> 45.719
2024-12-01-16:31:31-root-INFO: Regularization Change: 0.000 -> 2.548
2024-12-01-16:31:31-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-01-16:31:31-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-01-16:31:31-root-INFO: step: 36 lr_xt 0.34059371
2024-12-01-16:31:31-root-INFO: grad norm: 2.304 2.289 0.266
2024-12-01-16:31:32-root-INFO: grad norm: 3.596 3.575 0.384
2024-12-01-16:31:32-root-INFO: Loss too large (45.341->46.249)! Learning rate decreased to 0.27247.
2024-12-01-16:31:32-root-INFO: grad norm: 4.326 4.291 0.552
2024-12-01-16:31:32-root-INFO: Loss too large (45.295->45.319)! Learning rate decreased to 0.21798.
2024-12-01-16:31:33-root-INFO: grad norm: 3.431 3.403 0.443
2024-12-01-16:31:33-root-INFO: grad norm: 2.883 2.856 0.388
2024-12-01-16:31:34-root-INFO: Loss Change: 45.453 -> 43.470
2024-12-01-16:31:34-root-INFO: Regularization Change: 0.000 -> 2.866
2024-12-01-16:31:34-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-01-16:31:34-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-01-16:31:34-root-INFO: step: 35 lr_xt 0.34671555
2024-12-01-16:31:34-root-INFO: grad norm: 3.276 3.234 0.525
2024-12-01-16:31:34-root-INFO: Loss too large (43.560->44.150)! Learning rate decreased to 0.27737.
2024-12-01-16:31:35-root-INFO: grad norm: 3.950 3.905 0.593
2024-12-01-16:31:35-root-INFO: Loss too large (43.348->43.419)! Learning rate decreased to 0.22190.
2024-12-01-16:31:35-root-INFO: grad norm: 3.254 3.226 0.429
2024-12-01-16:31:36-root-INFO: grad norm: 2.714 2.688 0.375
2024-12-01-16:31:36-root-INFO: grad norm: 2.411 2.389 0.322
2024-12-01-16:31:37-root-INFO: Loss Change: 43.560 -> 41.286
2024-12-01-16:31:37-root-INFO: Regularization Change: 0.000 -> 2.524
2024-12-01-16:31:37-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-01-16:31:37-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-01-16:31:37-root-INFO: step: 34 lr_xt 0.35289102
2024-12-01-16:31:37-root-INFO: grad norm: 1.962 1.948 0.238
2024-12-01-16:31:38-root-INFO: grad norm: 2.782 2.764 0.312
2024-12-01-16:31:38-root-INFO: Loss too large (40.678->41.015)! Learning rate decreased to 0.28231.
2024-12-01-16:31:38-root-INFO: grad norm: 3.369 3.343 0.420
2024-12-01-16:31:39-root-INFO: grad norm: 4.013 3.983 0.495
2024-12-01-16:31:39-root-INFO: grad norm: 4.526 4.479 0.655
2024-12-01-16:31:40-root-INFO: Loss Change: 41.013 -> 40.257
2024-12-01-16:31:40-root-INFO: Regularization Change: 0.000 -> 3.986
2024-12-01-16:31:40-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-01-16:31:40-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-01-16:31:40-root-INFO: step: 33 lr_xt 0.35911909
2024-12-01-16:31:40-root-INFO: grad norm: 5.287 5.224 0.816
2024-12-01-16:31:40-root-INFO: Loss too large (40.348->41.735)! Learning rate decreased to 0.28730.
2024-12-01-16:31:41-root-INFO: grad norm: 5.068 4.998 0.840
2024-12-01-16:31:41-root-INFO: grad norm: 4.774 4.721 0.710
2024-12-01-16:31:42-root-INFO: grad norm: 4.524 4.464 0.730
2024-12-01-16:31:42-root-INFO: grad norm: 4.382 4.334 0.650
2024-12-01-16:31:42-root-INFO: Loss Change: 40.348 -> 37.542
2024-12-01-16:31:42-root-INFO: Regularization Change: 0.000 -> 4.788
2024-12-01-16:31:42-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-01-16:31:42-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-01-16:31:43-root-INFO: step: 32 lr_xt 0.36539868
2024-12-01-16:31:43-root-INFO: grad norm: 3.639 3.609 0.465
2024-12-01-16:31:43-root-INFO: Loss too large (36.990->37.363)! Learning rate decreased to 0.29232.
2024-12-01-16:31:43-root-INFO: grad norm: 3.488 3.455 0.477
2024-12-01-16:31:44-root-INFO: grad norm: 3.447 3.410 0.506
2024-12-01-16:31:44-root-INFO: grad norm: 3.462 3.427 0.493
2024-12-01-16:31:45-root-INFO: grad norm: 3.499 3.459 0.526
2024-12-01-16:31:45-root-INFO: Loss Change: 36.990 -> 34.925
2024-12-01-16:31:45-root-INFO: Regularization Change: 0.000 -> 3.895
2024-12-01-16:31:45-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-01-16:31:45-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-01-16:31:45-root-INFO: step: 31 lr_xt 0.37172867
2024-12-01-16:31:45-root-INFO: grad norm: 4.137 4.081 0.680
2024-12-01-16:31:46-root-INFO: Loss too large (35.078->35.840)! Learning rate decreased to 0.29738.
2024-12-01-16:31:46-root-INFO: grad norm: 3.972 3.918 0.649
2024-12-01-16:31:47-root-INFO: grad norm: 3.779 3.737 0.562
2024-12-01-16:31:47-root-INFO: grad norm: 3.630 3.584 0.572
2024-12-01-16:31:47-root-INFO: grad norm: 3.536 3.498 0.522
2024-12-01-16:31:48-root-INFO: Loss Change: 35.078 -> 32.798
2024-12-01-16:31:48-root-INFO: Regularization Change: 0.000 -> 4.003
2024-12-01-16:31:48-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-01-16:31:48-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-01-16:31:48-root-INFO: step: 30 lr_xt 0.37810791
2024-12-01-16:31:48-root-INFO: grad norm: 2.936 2.913 0.364
2024-12-01-16:31:48-root-INFO: Loss too large (32.277->32.368)! Learning rate decreased to 0.30249.
2024-12-01-16:31:49-root-INFO: grad norm: 2.808 2.782 0.379
2024-12-01-16:31:49-root-INFO: grad norm: 2.775 2.746 0.396
2024-12-01-16:31:50-root-INFO: grad norm: 2.799 2.771 0.396
2024-12-01-16:31:50-root-INFO: grad norm: 2.840 2.809 0.420
2024-12-01-16:31:50-root-INFO: Loss Change: 32.277 -> 30.405
2024-12-01-16:31:50-root-INFO: Regularization Change: 0.000 -> 3.436
2024-12-01-16:31:50-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-01-16:31:50-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-01-16:31:51-root-INFO: step: 29 lr_xt 0.38453518
2024-12-01-16:31:51-root-INFO: grad norm: 3.600 3.545 0.623
2024-12-01-16:31:51-root-INFO: Loss too large (30.568->31.034)! Learning rate decreased to 0.30763.
2024-12-01-16:31:51-root-INFO: grad norm: 3.387 3.340 0.564
2024-12-01-16:31:52-root-INFO: grad norm: 3.178 3.141 0.487
2024-12-01-16:31:52-root-INFO: grad norm: 3.048 3.010 0.479
2024-12-01-16:31:53-root-INFO: grad norm: 2.982 2.948 0.446
2024-12-01-16:31:53-root-INFO: Loss Change: 30.568 -> 28.457
2024-12-01-16:31:53-root-INFO: Regularization Change: 0.000 -> 3.654
2024-12-01-16:31:53-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-01-16:31:53-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-01-16:31:53-root-INFO: step: 28 lr_xt 0.39100924
2024-12-01-16:31:53-root-INFO: grad norm: 2.449 2.431 0.297
2024-12-01-16:31:54-root-INFO: grad norm: 3.241 3.214 0.417
2024-12-01-16:31:54-root-INFO: Loss too large (27.756->28.048)! Learning rate decreased to 0.31281.
2024-12-01-16:31:55-root-INFO: grad norm: 2.961 2.933 0.409
2024-12-01-16:31:55-root-INFO: grad norm: 2.690 2.662 0.387
2024-12-01-16:31:56-root-INFO: grad norm: 2.543 2.516 0.370
2024-12-01-16:31:56-root-INFO: Loss Change: 27.874 -> 26.086
2024-12-01-16:31:56-root-INFO: Regularization Change: 0.000 -> 3.433
2024-12-01-16:31:56-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-01-16:31:56-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-01-16:31:56-root-INFO: step: 27 lr_xt 0.39752879
2024-12-01-16:31:56-root-INFO: grad norm: 3.155 3.106 0.554
2024-12-01-16:31:56-root-INFO: Loss too large (26.187->26.440)! Learning rate decreased to 0.31802.
2024-12-01-16:31:57-root-INFO: grad norm: 2.857 2.818 0.470
2024-12-01-16:31:57-root-INFO: grad norm: 2.579 2.548 0.394
2024-12-01-16:31:58-root-INFO: grad norm: 2.425 2.397 0.368
2024-12-01-16:31:58-root-INFO: grad norm: 2.352 2.329 0.333
2024-12-01-16:31:59-root-INFO: Loss Change: 26.187 -> 24.234
2024-12-01-16:31:59-root-INFO: Regularization Change: 0.000 -> 3.205
2024-12-01-16:31:59-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-01-16:31:59-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-01-16:31:59-root-INFO: step: 26 lr_xt 0.40409250
2024-12-01-16:31:59-root-INFO: grad norm: 1.896 1.886 0.197
2024-12-01-16:31:59-root-INFO: grad norm: 2.412 2.398 0.256
2024-12-01-16:32:00-root-INFO: Loss too large (23.589->23.613)! Learning rate decreased to 0.32327.
2024-12-01-16:32:00-root-INFO: grad norm: 2.200 2.185 0.263
2024-12-01-16:32:01-root-INFO: grad norm: 1.984 1.969 0.243
2024-12-01-16:32:01-root-INFO: grad norm: 1.898 1.883 0.244
2024-12-01-16:32:02-root-INFO: Loss Change: 23.847 -> 22.243
2024-12-01-16:32:02-root-INFO: Regularization Change: 0.000 -> 2.967
2024-12-01-16:32:02-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-01-16:32:02-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-01-16:32:02-root-INFO: step: 25 lr_xt 0.41069899
2024-12-01-16:32:02-root-INFO: grad norm: 2.580 2.542 0.440
2024-12-01-16:32:02-root-INFO: Loss too large (22.142->22.176)! Learning rate decreased to 0.32856.
2024-12-01-16:32:03-root-INFO: grad norm: 2.275 2.247 0.353
2024-12-01-16:32:03-root-INFO: grad norm: 2.002 1.983 0.280
2024-12-01-16:32:04-root-INFO: grad norm: 1.878 1.859 0.264
2024-12-01-16:32:04-root-INFO: grad norm: 1.837 1.823 0.231
2024-12-01-16:32:04-root-INFO: Loss Change: 22.142 -> 20.449
2024-12-01-16:32:04-root-INFO: Regularization Change: 0.000 -> 2.788
2024-12-01-16:32:04-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-01-16:32:04-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-01-16:32:05-root-INFO: step: 24 lr_xt 0.41734684
2024-12-01-16:32:05-root-INFO: grad norm: 1.574 1.565 0.171
2024-12-01-16:32:05-root-INFO: grad norm: 1.869 1.861 0.169
2024-12-01-16:32:06-root-INFO: grad norm: 2.161 2.150 0.218
2024-12-01-16:32:06-root-INFO: grad norm: 2.509 2.497 0.241
2024-12-01-16:32:07-root-INFO: grad norm: 2.857 2.838 0.325
2024-12-01-16:32:07-root-INFO: Loss Change: 20.195 -> 19.312
2024-12-01-16:32:07-root-INFO: Regularization Change: 0.000 -> 4.135
2024-12-01-16:32:07-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-01-16:32:07-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-01-16:32:07-root-INFO: step: 23 lr_xt 0.42403458
2024-12-01-16:32:07-root-INFO: grad norm: 3.760 3.722 0.534
2024-12-01-16:32:08-root-INFO: Loss too large (19.332->19.354)! Learning rate decreased to 0.33923.
2024-12-01-16:32:08-root-INFO: grad norm: 2.618 2.588 0.396
2024-12-01-16:32:09-root-INFO: grad norm: 1.840 1.822 0.250
2024-12-01-16:32:09-root-INFO: grad norm: 1.477 1.462 0.210
2024-12-01-16:32:10-root-INFO: grad norm: 1.278 1.268 0.158
2024-12-01-16:32:10-root-INFO: Loss Change: 19.332 -> 16.827
2024-12-01-16:32:10-root-INFO: Regularization Change: 0.000 -> 2.892
2024-12-01-16:32:10-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-01-16:32:10-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-01-16:32:10-root-INFO: step: 22 lr_xt 0.43076069
2024-12-01-16:32:10-root-INFO: grad norm: 1.225 1.212 0.181
2024-12-01-16:32:11-root-INFO: grad norm: 1.214 1.203 0.161
2024-12-01-16:32:11-root-INFO: grad norm: 1.433 1.425 0.152
2024-12-01-16:32:12-root-INFO: grad norm: 1.635 1.625 0.177
2024-12-01-16:32:12-root-INFO: grad norm: 1.662 1.654 0.169
2024-12-01-16:32:13-root-INFO: Loss Change: 16.611 -> 15.319
2024-12-01-16:32:13-root-INFO: Regularization Change: 0.000 -> 3.391
2024-12-01-16:32:13-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-01-16:32:13-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-01-16:32:13-root-INFO: step: 21 lr_xt 0.43752364
2024-12-01-16:32:13-root-INFO: grad norm: 2.037 2.024 0.229
2024-12-01-16:32:14-root-INFO: grad norm: 2.223 2.210 0.245
2024-12-01-16:32:14-root-INFO: grad norm: 2.443 2.432 0.229
2024-12-01-16:32:15-root-INFO: grad norm: 2.542 2.526 0.283
2024-12-01-16:32:15-root-INFO: grad norm: 2.571 2.558 0.256
2024-12-01-16:32:15-root-INFO: Loss Change: 15.136 -> 14.158
2024-12-01-16:32:15-root-INFO: Regularization Change: 0.000 -> 3.810
2024-12-01-16:32:15-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-01-16:32:15-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-01-16:32:15-root-INFO: step: 20 lr_xt 0.44432183
2024-12-01-16:32:16-root-INFO: grad norm: 2.448 2.439 0.213
2024-12-01-16:32:16-root-INFO: grad norm: 2.369 2.361 0.196
2024-12-01-16:32:17-root-INFO: grad norm: 2.250 2.240 0.215
2024-12-01-16:32:17-root-INFO: grad norm: 2.182 2.174 0.188
2024-12-01-16:32:18-root-INFO: grad norm: 2.209 2.198 0.222
2024-12-01-16:32:18-root-INFO: Loss Change: 13.832 -> 12.461
2024-12-01-16:32:18-root-INFO: Regularization Change: 0.000 -> 3.740
2024-12-01-16:32:18-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-01-16:32:18-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-01-16:32:18-root-INFO: step: 19 lr_xt 0.45115363
2024-12-01-16:32:18-root-INFO: grad norm: 2.749 2.727 0.353
2024-12-01-16:32:19-root-INFO: grad norm: 2.613 2.587 0.365
2024-12-01-16:32:19-root-INFO: grad norm: 2.541 2.523 0.303
2024-12-01-16:32:20-root-INFO: grad norm: 2.513 2.487 0.362
2024-12-01-16:32:20-root-INFO: grad norm: 2.559 2.541 0.304
2024-12-01-16:32:21-root-INFO: Loss Change: 12.459 -> 11.352
2024-12-01-16:32:21-root-INFO: Regularization Change: 0.000 -> 3.877
2024-12-01-16:32:21-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-01-16:32:21-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-01-16:32:21-root-INFO: step: 18 lr_xt 0.45801735
2024-12-01-16:32:21-root-INFO: grad norm: 2.309 2.297 0.238
2024-12-01-16:32:22-root-INFO: grad norm: 2.184 2.175 0.205
2024-12-01-16:32:22-root-INFO: grad norm: 2.096 2.081 0.246
2024-12-01-16:32:23-root-INFO: grad norm: 2.070 2.061 0.199
2024-12-01-16:32:23-root-INFO: grad norm: 2.065 2.050 0.250
2024-12-01-16:32:24-root-INFO: Loss Change: 10.978 -> 9.722
2024-12-01-16:32:24-root-INFO: Regularization Change: 0.000 -> 3.298
2024-12-01-16:32:24-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-01-16:32:24-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-01-16:32:24-root-INFO: step: 17 lr_xt 0.46491129
2024-12-01-16:32:24-root-INFO: grad norm: 2.570 2.544 0.367
2024-12-01-16:32:24-root-INFO: grad norm: 2.358 2.331 0.356
2024-12-01-16:32:25-root-INFO: grad norm: 2.144 2.127 0.270
2024-12-01-16:32:25-root-INFO: grad norm: 2.067 2.044 0.304
2024-12-01-16:32:26-root-INFO: grad norm: 2.036 2.021 0.244
2024-12-01-16:32:26-root-INFO: Loss Change: 9.814 -> 8.652
2024-12-01-16:32:26-root-INFO: Regularization Change: 0.000 -> 3.247
2024-12-01-16:32:26-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-01-16:32:26-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-01-16:32:26-root-INFO: step: 16 lr_xt 0.47183369
2024-12-01-16:32:26-root-INFO: grad norm: 1.792 1.784 0.174
2024-12-01-16:32:27-root-INFO: grad norm: 1.659 1.652 0.144
2024-12-01-16:32:27-root-INFO: grad norm: 1.608 1.599 0.171
2024-12-01-16:32:28-root-INFO: grad norm: 1.608 1.602 0.139
2024-12-01-16:32:28-root-INFO: grad norm: 1.598 1.588 0.177
2024-12-01-16:32:29-root-INFO: Loss Change: 8.368 -> 7.407
2024-12-01-16:32:29-root-INFO: Regularization Change: 0.000 -> 2.568
2024-12-01-16:32:29-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-01-16:32:29-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-01-16:32:29-root-INFO: step: 15 lr_xt 0.47878275
2024-12-01-16:32:29-root-INFO: grad norm: 2.162 2.136 0.331
2024-12-01-16:32:29-root-INFO: grad norm: 1.951 1.929 0.292
2024-12-01-16:32:30-root-INFO: grad norm: 1.767 1.753 0.218
2024-12-01-16:32:30-root-INFO: grad norm: 1.670 1.653 0.237
2024-12-01-16:32:31-root-INFO: grad norm: 1.598 1.588 0.184
2024-12-01-16:32:31-root-INFO: Loss Change: 7.488 -> 6.481
2024-12-01-16:32:31-root-INFO: Regularization Change: 0.000 -> 2.584
2024-12-01-16:32:31-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-01-16:32:31-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-01-16:32:31-root-INFO: step: 14 lr_xt 0.48575663
2024-12-01-16:32:31-root-INFO: grad norm: 1.417 1.412 0.122
2024-12-01-16:32:32-root-INFO: grad norm: 1.278 1.274 0.098
2024-12-01-16:32:32-root-INFO: grad norm: 1.248 1.244 0.100
2024-12-01-16:32:33-root-INFO: grad norm: 1.224 1.220 0.092
2024-12-01-16:32:33-root-INFO: grad norm: 1.193 1.189 0.101
2024-12-01-16:32:34-root-INFO: Loss Change: 6.280 -> 5.529
2024-12-01-16:32:34-root-INFO: Regularization Change: 0.000 -> 1.967
2024-12-01-16:32:34-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-01-16:32:34-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-01-16:32:34-root-INFO: step: 13 lr_xt 0.49275347
2024-12-01-16:32:34-root-INFO: grad norm: 1.712 1.693 0.259
2024-12-01-16:32:34-root-INFO: grad norm: 1.521 1.506 0.216
2024-12-01-16:32:35-root-INFO: grad norm: 1.395 1.386 0.160
2024-12-01-16:32:35-root-INFO: grad norm: 1.323 1.311 0.175
2024-12-01-16:32:36-root-INFO: grad norm: 1.268 1.261 0.134
2024-12-01-16:32:36-root-INFO: Loss Change: 5.606 -> 4.863
2024-12-01-16:32:36-root-INFO: Regularization Change: 0.000 -> 1.917
2024-12-01-16:32:36-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-01-16:32:36-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-01-16:32:36-root-INFO: step: 12 lr_xt 0.49977135
2024-12-01-16:32:37-root-INFO: grad norm: 1.214 1.210 0.101
2024-12-01-16:32:37-root-INFO: grad norm: 1.069 1.067 0.077
2024-12-01-16:32:38-root-INFO: grad norm: 1.051 1.048 0.067
2024-12-01-16:32:38-root-INFO: grad norm: 1.043 1.041 0.071
2024-12-01-16:32:39-root-INFO: grad norm: 1.024 1.022 0.071
2024-12-01-16:32:39-root-INFO: Loss Change: 4.763 -> 4.192
2024-12-01-16:32:39-root-INFO: Regularization Change: 0.000 -> 1.540
2024-12-01-16:32:39-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-01-16:32:39-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-01-16:32:39-root-INFO: step: 11 lr_xt 0.50680833
2024-12-01-16:32:39-root-INFO: grad norm: 1.538 1.521 0.234
2024-12-01-16:32:40-root-INFO: grad norm: 1.293 1.281 0.173
2024-12-01-16:32:40-root-INFO: grad norm: 1.134 1.127 0.126
2024-12-01-16:32:41-root-INFO: grad norm: 1.040 1.032 0.133
2024-12-01-16:32:41-root-INFO: grad norm: 1.070 1.064 0.108
2024-12-01-16:32:42-root-INFO: Loss Change: 4.293 -> 3.707
2024-12-01-16:32:42-root-INFO: Regularization Change: 0.000 -> 1.542
2024-12-01-16:32:42-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-01-16:32:42-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-01-16:32:42-root-INFO: step: 10 lr_xt 0.51386241
2024-12-01-16:32:42-root-INFO: grad norm: 1.137 1.130 0.123
2024-12-01-16:32:42-root-INFO: grad norm: 0.889 0.886 0.074
2024-12-01-16:32:43-root-INFO: grad norm: 0.894 0.893 0.058
2024-12-01-16:32:43-root-INFO: grad norm: 1.218 1.217 0.067
2024-12-01-16:32:44-root-INFO: grad norm: 0.856 0.854 0.054
2024-12-01-16:32:44-root-INFO: Loss Change: 3.702 -> 3.233
2024-12-01-16:32:44-root-INFO: Regularization Change: 0.000 -> 1.217
2024-12-01-16:32:44-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-01-16:32:44-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-01-16:32:44-root-INFO: step: 9 lr_xt 0.52093157
2024-12-01-16:32:45-root-INFO: grad norm: 1.199 1.184 0.184
2024-12-01-16:32:45-root-INFO: grad norm: 0.946 0.938 0.122
2024-12-01-16:32:45-root-INFO: grad norm: 1.025 1.019 0.107
2024-12-01-16:32:46-root-INFO: grad norm: 0.822 0.817 0.097
2024-12-01-16:32:46-root-INFO: grad norm: 0.752 0.749 0.067
2024-12-01-16:32:47-root-INFO: Loss Change: 3.332 -> 2.805
2024-12-01-16:32:47-root-INFO: Regularization Change: 0.000 -> 1.172
2024-12-01-16:32:47-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-01-16:32:47-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-01-16:32:47-root-INFO: step: 8 lr_xt 0.52801377
2024-12-01-16:32:47-root-INFO: grad norm: 1.107 1.097 0.150
2024-12-01-16:32:48-root-INFO: grad norm: 0.738 0.733 0.085
2024-12-01-16:32:48-root-INFO: grad norm: 0.589 0.586 0.055
2024-12-01-16:32:48-root-INFO: grad norm: 0.591 0.589 0.047
2024-12-01-16:32:49-root-INFO: grad norm: 0.690 0.688 0.058
2024-12-01-16:32:49-root-INFO: Loss Change: 2.902 -> 2.525
2024-12-01-16:32:49-root-INFO: Regularization Change: 0.000 -> 0.911
2024-12-01-16:32:49-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-01-16:32:49-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-01-16:32:49-root-INFO: step: 7 lr_xt 0.53510690
2024-12-01-16:32:50-root-INFO: grad norm: 1.277 1.261 0.203
2024-12-01-16:32:50-root-INFO: grad norm: 0.763 0.757 0.100
2024-12-01-16:32:50-root-INFO: grad norm: 0.600 0.597 0.062
2024-12-01-16:32:51-root-INFO: grad norm: 0.584 0.583 0.043
2024-12-01-16:32:51-root-INFO: grad norm: 0.653 0.650 0.063
2024-12-01-16:32:52-root-INFO: Loss Change: 2.679 -> 2.249
2024-12-01-16:32:52-root-INFO: Regularization Change: 0.000 -> 0.841
2024-12-01-16:32:52-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-01-16:32:52-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-01-16:32:52-root-INFO: step: 6 lr_xt 0.54220886
2024-12-01-16:32:52-root-INFO: grad norm: 1.152 1.137 0.185
2024-12-01-16:32:53-root-INFO: grad norm: 0.749 0.743 0.100
2024-12-01-16:32:53-root-INFO: grad norm: 0.567 0.563 0.061
2024-12-01-16:32:53-root-INFO: grad norm: 0.645 0.640 0.078
2024-12-01-16:32:54-root-INFO: grad norm: 0.700 0.697 0.059
2024-12-01-16:32:54-root-INFO: Loss Change: 2.407 -> 2.019
2024-12-01-16:32:54-root-INFO: Regularization Change: 0.000 -> 0.897
2024-12-01-16:32:54-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-01-16:32:54-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-01-16:32:54-root-INFO: step: 5 lr_xt 0.54931747
2024-12-01-16:32:55-root-INFO: grad norm: 0.975 0.966 0.129
2024-12-01-16:32:55-root-INFO: grad norm: 0.648 0.646 0.050
2024-12-01-16:32:56-root-INFO: grad norm: 0.533 0.530 0.055
2024-12-01-16:32:56-root-INFO: grad norm: 0.497 0.495 0.040
2024-12-01-16:32:57-root-INFO: grad norm: 0.462 0.460 0.047
2024-12-01-16:32:57-root-INFO: Loss Change: 2.144 -> 1.796
2024-12-01-16:32:57-root-INFO: Regularization Change: 0.000 -> 0.771
2024-12-01-16:32:57-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-01-16:32:57-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-01-16:32:57-root-INFO: step: 4 lr_xt 0.55643055
2024-12-01-16:32:57-root-INFO: grad norm: 1.483 1.474 0.156
2024-12-01-16:32:58-root-INFO: grad norm: 1.893 1.891 0.087
2024-12-01-16:32:58-root-INFO: grad norm: 0.742 0.741 0.044
2024-12-01-16:32:59-root-INFO: grad norm: 0.331 0.329 0.035
2024-12-01-16:32:59-root-INFO: grad norm: 0.277 0.275 0.030
2024-12-01-16:33:00-root-INFO: Loss Change: 1.959 -> 1.575
2024-12-01-16:33:00-root-INFO: Regularization Change: 0.000 -> 1.299
2024-12-01-16:33:00-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-01-16:33:00-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-01-16:33:00-root-INFO: step: 3 lr_xt 0.56354589
2024-12-01-16:33:00-root-INFO: grad norm: 0.797 0.789 0.117
2024-12-01-16:33:01-root-INFO: grad norm: 0.398 0.394 0.050
2024-12-01-16:33:01-root-INFO: grad norm: 0.279 0.277 0.034
2024-12-01-16:33:02-root-INFO: grad norm: 0.243 0.240 0.034
2024-12-01-16:33:02-root-INFO: grad norm: 0.238 0.236 0.030
2024-12-01-16:33:02-root-INFO: Loss Change: 1.731 -> 1.442
2024-12-01-16:33:02-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-01-16:33:02-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-01-16:33:02-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-01-16:33:03-root-INFO: step: 2 lr_xt 0.57066124
2024-12-01-16:33:03-root-INFO: grad norm: 0.712 0.706 0.092
2024-12-01-16:33:03-root-INFO: grad norm: 0.505 0.503 0.047
2024-12-01-16:33:04-root-INFO: grad norm: 0.442 0.440 0.034
2024-12-01-16:33:04-root-INFO: grad norm: 0.483 0.482 0.035
2024-12-01-16:33:04-root-INFO: Loss too large (1.380->1.385)! Learning rate decreased to 0.45653.
2024-12-01-16:33:05-root-INFO: grad norm: 0.646 0.645 0.029
2024-12-01-16:33:05-root-INFO: Loss Change: 1.584 -> 1.361
2024-12-01-16:33:05-root-INFO: Regularization Change: 0.000 -> 0.506
2024-12-01-16:33:05-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-01-16:33:05-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-01-16:33:05-root-INFO: step: 1 lr_xt 0.57777431
2024-12-01-16:33:05-root-INFO: grad norm: 0.715 0.711 0.083
2024-12-01-16:33:06-root-INFO: grad norm: 0.497 0.496 0.039
2024-12-01-16:33:06-root-INFO: grad norm: 0.467 0.466 0.031
2024-12-01-16:33:07-root-INFO: grad norm: 0.468 0.467 0.031
2024-12-01-16:33:07-root-INFO: grad norm: 0.246 0.245 0.026
2024-12-01-16:33:08-root-INFO: Loss Change: 1.492 -> 1.233
2024-12-01-16:33:08-root-INFO: Regularization Change: 0.000 -> 0.636
2024-12-01-16:33:08-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-01-16:33:08-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-01-16:33:08-root-INFO: loss_sample0_0: 1.2325859069824219
2024-12-01-16:33:08-root-INFO: It takes 778.877 seconds for image sample0
2024-12-01-16:33:08-root-INFO: lpips_score_sample0: 0.142
2024-12-01-16:33:08-root-INFO: psnr_score_sample0: 17.690
2024-12-01-16:33:08-root-INFO: ssim_score_sample0: 0.726
2024-12-01-16:33:08-root-INFO: mean_lpips: 0.14249888062477112
2024-12-01-16:33:08-root-INFO: best_mean_lpips: 0.14249888062477112
2024-12-01-16:33:08-root-INFO: mean_psnr: 17.689870834350586
2024-12-01-16:33:08-root-INFO: best_mean_psnr: 17.689870834350586
2024-12-01-16:33:08-root-INFO: mean_ssim: 0.7264540195465088
2024-12-01-16:33:08-root-INFO: best_mean_ssim: 0.7264540195465088
2024-12-01-16:33:08-root-INFO: final_loss: 1.2325859069824219
2024-12-01-16:33:08-root-INFO: mean time: 778.8773119449615
2024-12-01-16:33:08-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample1_iter5_lr0.03_10009 
 
Enjoy.
