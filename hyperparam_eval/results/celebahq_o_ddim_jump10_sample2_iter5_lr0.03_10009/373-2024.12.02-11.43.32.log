2024-12-02-11:43:36-root-INFO: Prepare model...
2024-12-02-11:43:51-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-11:44:14-root-INFO: Start sampling
2024-12-02-11:44:18-root-INFO: step: 249 lr_xt 0.00019059
2024-12-02-11:44:19-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-11:44:19-root-INFO: Loss too large (77070.016->78612.703)! Learning rate decreased to 0.00015.
2024-12-02-11:44:19-root-INFO: grad norm: 15661.109 11248.839 10896.513
2024-12-02-11:44:20-root-INFO: grad norm: 14206.018 10378.438 9700.464
2024-12-02-11:44:20-root-INFO: grad norm: 15590.674 11542.787 10480.132
2024-12-02-11:44:20-root-INFO: Loss too large (28301.871->35851.984)! Learning rate decreased to 0.00012.
2024-12-02-11:44:21-root-INFO: grad norm: 16985.691 12432.188 11573.867
2024-12-02-11:44:21-root-INFO: Loss too large (28114.189->29015.012)! Learning rate decreased to 0.00010.
2024-12-02-11:44:21-root-INFO: Loss Change: 77070.016 -> 22717.830
2024-12-02-11:44:21-root-INFO: Regularization Change: 0.000 -> 14.512
2024-12-02-11:44:21-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-11:44:21-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-11:44:21-root-INFO: step: 248 lr_xt 0.00020082
2024-12-02-11:44:21-root-INFO: grad norm: 13147.489 10137.499 8371.832
2024-12-02-11:44:22-root-INFO: Loss too large (23084.973->44018.008)! Learning rate decreased to 0.00016.
2024-12-02-11:44:22-root-INFO: Loss too large (23084.973->34076.910)! Learning rate decreased to 0.00013.
2024-12-02-11:44:22-root-INFO: Loss too large (23084.973->26826.844)! Learning rate decreased to 0.00010.
2024-12-02-11:44:22-root-INFO: grad norm: 12870.007 10070.154 8014.303
2024-12-02-11:44:23-root-INFO: grad norm: 14422.459 11506.705 8695.004
2024-12-02-11:44:23-root-INFO: Loss too large (22210.219->24128.434)! Learning rate decreased to 0.00008.
2024-12-02-11:44:23-root-INFO: grad norm: 11117.797 8915.237 6642.586
2024-12-02-11:44:24-root-INFO: grad norm: 8517.220 6795.307 5134.865
2024-12-02-11:44:24-root-INFO: Loss Change: 23084.973 -> 17988.189
2024-12-02-11:44:24-root-INFO: Regularization Change: 0.000 -> 1.692
2024-12-02-11:44:24-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-02-11:44:24-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-11:44:24-root-INFO: step: 247 lr_xt 0.00021156
2024-12-02-11:44:25-root-INFO: grad norm: 6318.961 5138.511 3677.633
2024-12-02-11:44:25-root-INFO: Loss too large (17729.668->26606.191)! Learning rate decreased to 0.00017.
2024-12-02-11:44:25-root-INFO: Loss too large (17729.668->22043.367)! Learning rate decreased to 0.00014.
2024-12-02-11:44:25-root-INFO: Loss too large (17729.668->19385.027)! Learning rate decreased to 0.00011.
2024-12-02-11:44:25-root-INFO: Loss too large (17729.668->17905.312)! Learning rate decreased to 0.00009.
2024-12-02-11:44:26-root-INFO: grad norm: 4648.850 3702.737 2810.968
2024-12-02-11:44:26-root-INFO: grad norm: 3494.831 2850.912 2021.421
2024-12-02-11:44:27-root-INFO: grad norm: 2612.595 2081.913 1578.382
2024-12-02-11:44:27-root-INFO: grad norm: 1997.215 1636.934 1144.254
2024-12-02-11:44:27-root-INFO: Loss Change: 17729.668 -> 16356.093
2024-12-02-11:44:27-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-02-11:44:27-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-02-11:44:27-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-11:44:28-root-INFO: step: 246 lr_xt 0.00022285
2024-12-02-11:44:28-root-INFO: grad norm: 1557.899 1254.773 923.361
2024-12-02-11:44:28-root-INFO: Loss too large (16122.258->16330.968)! Learning rate decreased to 0.00018.
2024-12-02-11:44:28-root-INFO: Loss too large (16122.258->16171.029)! Learning rate decreased to 0.00014.
2024-12-02-11:44:28-root-INFO: grad norm: 2206.373 1810.834 1260.541
2024-12-02-11:44:29-root-INFO: Loss too large (16085.143->16170.223)! Learning rate decreased to 0.00011.
2024-12-02-11:44:29-root-INFO: grad norm: 2378.550 1933.981 1384.636
2024-12-02-11:44:30-root-INFO: grad norm: 2570.625 2086.254 1501.885
2024-12-02-11:44:30-root-INFO: grad norm: 2790.671 2277.313 1612.974
2024-12-02-11:44:30-root-INFO: Loss Change: 16122.258 -> 15914.150
2024-12-02-11:44:30-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-02-11:44:30-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-02-11:44:30-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-11:44:30-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-11:44:31-root-INFO: grad norm: 2950.332 2380.224 1743.271
2024-12-02-11:44:31-root-INFO: Loss too large (15796.262->17537.479)! Learning rate decreased to 0.00019.
2024-12-02-11:44:31-root-INFO: Loss too large (15796.262->16574.549)! Learning rate decreased to 0.00015.
2024-12-02-11:44:31-root-INFO: Loss too large (15796.262->16027.054)! Learning rate decreased to 0.00012.
2024-12-02-11:44:32-root-INFO: grad norm: 3044.286 2495.246 1743.968
2024-12-02-11:44:32-root-INFO: grad norm: 3143.103 2527.626 1868.208
2024-12-02-11:44:32-root-INFO: grad norm: 3250.317 2671.722 1851.070
2024-12-02-11:44:33-root-INFO: grad norm: 3354.960 2692.055 2002.147
2024-12-02-11:44:33-root-INFO: Loss Change: 15796.262 -> 15540.652
2024-12-02-11:44:33-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-02-11:44:33-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-11:44:33-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-11:44:33-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-11:44:34-root-INFO: grad norm: 3652.026 2958.531 2141.119
2024-12-02-11:44:34-root-INFO: Loss too large (15495.414->17992.619)! Learning rate decreased to 0.00020.
2024-12-02-11:44:34-root-INFO: Loss too large (15495.414->16621.631)! Learning rate decreased to 0.00016.
2024-12-02-11:44:34-root-INFO: Loss too large (15495.414->15817.104)! Learning rate decreased to 0.00013.
2024-12-02-11:44:34-root-INFO: grad norm: 3490.313 2821.542 2054.552
2024-12-02-11:44:35-root-INFO: grad norm: 3397.098 2801.942 1920.779
2024-12-02-11:44:35-root-INFO: grad norm: 3342.010 2698.563 1971.494
2024-12-02-11:44:36-root-INFO: grad norm: 3297.597 2737.237 1838.933
2024-12-02-11:44:36-root-INFO: Loss Change: 15495.414 -> 15014.002
2024-12-02-11:44:36-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-02-11:44:36-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-11:44:36-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:44:36-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-11:44:37-root-INFO: grad norm: 3314.702 2653.557 1986.424
2024-12-02-11:44:37-root-INFO: Loss too large (14915.106->17113.943)! Learning rate decreased to 0.00021.
2024-12-02-11:44:37-root-INFO: Loss too large (14915.106->15848.663)! Learning rate decreased to 0.00017.
2024-12-02-11:44:37-root-INFO: Loss too large (14915.106->15135.254)! Learning rate decreased to 0.00013.
2024-12-02-11:44:37-root-INFO: grad norm: 3083.392 2575.787 1694.882
2024-12-02-11:44:38-root-INFO: grad norm: 2929.103 2355.489 1741.067
2024-12-02-11:44:38-root-INFO: grad norm: 2791.149 2347.832 1509.369
2024-12-02-11:44:39-root-INFO: grad norm: 2676.145 2155.234 1586.417
2024-12-02-11:44:39-root-INFO: Loss Change: 14915.106 -> 14353.879
2024-12-02-11:44:39-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-02-11:44:39-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-11:44:39-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:44:39-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-11:44:39-root-INFO: grad norm: 2586.479 2175.112 1399.557
2024-12-02-11:44:40-root-INFO: Loss too large (14157.375->15404.837)! Learning rate decreased to 0.00022.
2024-12-02-11:44:40-root-INFO: Loss too large (14157.375->14674.672)! Learning rate decreased to 0.00018.
2024-12-02-11:44:40-root-INFO: Loss too large (14157.375->14259.015)! Learning rate decreased to 0.00014.
2024-12-02-11:44:40-root-INFO: grad norm: 2381.522 1940.543 1380.557
2024-12-02-11:44:41-root-INFO: grad norm: 2210.121 1881.840 1159.015
2024-12-02-11:44:41-root-INFO: grad norm: 2066.245 1676.510 1207.758
2024-12-02-11:44:42-root-INFO: grad norm: 1935.524 1657.675 999.183
2024-12-02-11:44:42-root-INFO: Loss Change: 14157.375 -> 13641.188
2024-12-02-11:44:42-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-02-11:44:42-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-11:44:42-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:44:42-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-11:44:42-root-INFO: grad norm: 1677.609 1382.573 950.192
2024-12-02-11:44:42-root-INFO: Loss too large (13532.217->13920.212)! Learning rate decreased to 0.00023.
2024-12-02-11:44:43-root-INFO: Loss too large (13532.217->13649.329)! Learning rate decreased to 0.00018.
2024-12-02-11:44:43-root-INFO: grad norm: 2175.440 1858.196 1131.216
2024-12-02-11:44:43-root-INFO: Loss too large (13502.563->13546.183)! Learning rate decreased to 0.00015.
2024-12-02-11:44:44-root-INFO: grad norm: 1974.391 1622.072 1125.656
2024-12-02-11:44:44-root-INFO: grad norm: 1797.787 1547.630 914.812
2024-12-02-11:44:45-root-INFO: grad norm: 1646.598 1355.249 935.192
2024-12-02-11:44:45-root-INFO: Loss Change: 13532.217 -> 13090.746
2024-12-02-11:44:45-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-11:44:45-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-11:44:45-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:44:45-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-11:44:45-root-INFO: grad norm: 1540.485 1325.730 784.561
2024-12-02-11:44:45-root-INFO: Loss too large (12911.181->13152.145)! Learning rate decreased to 0.00024.
2024-12-02-11:44:46-root-INFO: Loss too large (12911.181->12950.918)! Learning rate decreased to 0.00019.
2024-12-02-11:44:46-root-INFO: grad norm: 1885.276 1567.450 1047.552
2024-12-02-11:44:47-root-INFO: grad norm: 2417.725 2073.798 1242.882
2024-12-02-11:44:47-root-INFO: Loss too large (12830.656->12900.232)! Learning rate decreased to 0.00016.
2024-12-02-11:44:47-root-INFO: grad norm: 2126.184 1765.732 1184.419
2024-12-02-11:44:48-root-INFO: grad norm: 1875.907 1619.830 946.139
2024-12-02-11:44:48-root-INFO: Loss Change: 12911.181 -> 12457.801
2024-12-02-11:44:48-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-02-11:44:48-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-11:44:48-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:44:48-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-11:44:48-root-INFO: grad norm: 1697.357 1412.919 940.576
2024-12-02-11:44:48-root-INFO: Loss too large (12398.428->12812.281)! Learning rate decreased to 0.00026.
2024-12-02-11:44:49-root-INFO: Loss too large (12398.428->12515.888)! Learning rate decreased to 0.00020.
2024-12-02-11:44:49-root-INFO: grad norm: 2106.391 1807.442 1081.683
2024-12-02-11:44:49-root-INFO: Loss too large (12355.593->12363.953)! Learning rate decreased to 0.00016.
2024-12-02-11:44:50-root-INFO: grad norm: 1806.539 1511.585 989.289
2024-12-02-11:44:50-root-INFO: grad norm: 1561.259 1354.174 777.009
2024-12-02-11:44:51-root-INFO: grad norm: 1365.072 1148.857 737.257
2024-12-02-11:44:51-root-INFO: Loss Change: 12398.428 -> 11875.766
2024-12-02-11:44:51-root-INFO: Regularization Change: 0.000 -> 0.444
2024-12-02-11:44:51-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-11:44:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:44:51-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-11:44:51-root-INFO: grad norm: 1245.255 1053.901 663.288
2024-12-02-11:44:52-root-INFO: grad norm: 2342.858 1952.365 1295.090
2024-12-02-11:44:52-root-INFO: Loss too large (11643.750->12690.988)! Learning rate decreased to 0.00027.
2024-12-02-11:44:52-root-INFO: Loss too large (11643.750->12018.851)! Learning rate decreased to 0.00021.
2024-12-02-11:44:52-root-INFO: Loss too large (11643.750->11647.063)! Learning rate decreased to 0.00017.
2024-12-02-11:44:53-root-INFO: grad norm: 1886.200 1634.304 941.701
2024-12-02-11:44:53-root-INFO: grad norm: 1564.151 1326.159 829.380
2024-12-02-11:44:54-root-INFO: grad norm: 1320.769 1162.311 627.266
2024-12-02-11:44:54-root-INFO: Loss Change: 11652.700 -> 11084.303
2024-12-02-11:44:54-root-INFO: Regularization Change: 0.000 -> 0.599
2024-12-02-11:44:54-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-11:44:54-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:44:54-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-11:44:54-root-INFO: grad norm: 1278.017 1092.145 663.737
2024-12-02-11:44:54-root-INFO: Loss too large (11025.016->11110.542)! Learning rate decreased to 0.00028.
2024-12-02-11:44:55-root-INFO: grad norm: 1957.861 1711.689 950.441
2024-12-02-11:44:55-root-INFO: Loss too large (10987.372->11204.387)! Learning rate decreased to 0.00023.
2024-12-02-11:44:55-root-INFO: grad norm: 2311.377 1980.654 1191.416
2024-12-02-11:44:55-root-INFO: Loss too large (10954.198->10956.508)! Learning rate decreased to 0.00018.
2024-12-02-11:44:56-root-INFO: grad norm: 1825.779 1602.484 874.936
2024-12-02-11:44:56-root-INFO: grad norm: 1471.682 1267.133 748.480
2024-12-02-11:44:57-root-INFO: Loss Change: 11025.016 -> 10497.398
2024-12-02-11:44:57-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-02-11:44:57-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-11:44:57-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:44:57-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-11:44:57-root-INFO: grad norm: 1217.177 1063.588 591.863
2024-12-02-11:44:57-root-INFO: Loss too large (10379.576->10459.332)! Learning rate decreased to 0.00030.
2024-12-02-11:44:58-root-INFO: grad norm: 1830.937 1584.377 917.649
2024-12-02-11:44:58-root-INFO: Loss too large (10343.095->10510.870)! Learning rate decreased to 0.00024.
2024-12-02-11:44:58-root-INFO: grad norm: 2087.061 1825.181 1012.194
2024-12-02-11:44:59-root-INFO: grad norm: 2376.806 2051.690 1199.906
2024-12-02-11:44:59-root-INFO: grad norm: 2707.841 2361.681 1324.713
2024-12-02-11:44:59-root-INFO: Loss too large (10254.003->10280.046)! Learning rate decreased to 0.00019.
2024-12-02-11:45:00-root-INFO: Loss Change: 10379.576 -> 9993.539
2024-12-02-11:45:00-root-INFO: Regularization Change: 0.000 -> 0.744
2024-12-02-11:45:00-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-11:45:00-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:45:00-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-11:45:00-root-INFO: grad norm: 2137.482 1859.735 1053.668
2024-12-02-11:45:00-root-INFO: Loss too large (9925.214->10810.529)! Learning rate decreased to 0.00031.
2024-12-02-11:45:00-root-INFO: Loss too large (9925.214->10206.685)! Learning rate decreased to 0.00025.
2024-12-02-11:45:01-root-INFO: grad norm: 2339.221 2065.249 1098.499
2024-12-02-11:45:01-root-INFO: grad norm: 2559.928 2225.571 1264.937
2024-12-02-11:45:02-root-INFO: grad norm: 2798.016 2464.167 1325.434
2024-12-02-11:45:02-root-INFO: Loss too large (9833.107->9838.944)! Learning rate decreased to 0.00020.
2024-12-02-11:45:02-root-INFO: grad norm: 1949.060 1694.117 963.745
2024-12-02-11:45:03-root-INFO: Loss Change: 9925.214 -> 9345.385
2024-12-02-11:45:03-root-INFO: Regularization Change: 0.000 -> 0.648
2024-12-02-11:45:03-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-11:45:03-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-11:45:03-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-11:45:03-root-INFO: grad norm: 1326.932 1179.306 608.265
2024-12-02-11:45:03-root-INFO: Loss too large (9276.193->9495.546)! Learning rate decreased to 0.00033.
2024-12-02-11:45:03-root-INFO: Loss too large (9276.193->9301.963)! Learning rate decreased to 0.00026.
2024-12-02-11:45:04-root-INFO: grad norm: 1380.435 1207.135 669.647
2024-12-02-11:45:04-root-INFO: grad norm: 1436.943 1281.325 650.393
2024-12-02-11:45:05-root-INFO: grad norm: 1491.607 1302.465 726.964
2024-12-02-11:45:05-root-INFO: grad norm: 1543.714 1374.378 702.949
2024-12-02-11:45:05-root-INFO: Loss Change: 9276.193 -> 8935.646
2024-12-02-11:45:05-root-INFO: Regularization Change: 0.000 -> 0.645
2024-12-02-11:45:05-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-11:45:05-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:06-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-11:45:06-root-INFO: grad norm: 1618.258 1432.698 752.418
2024-12-02-11:45:06-root-INFO: Loss too large (8897.549->9309.673)! Learning rate decreased to 0.00035.
2024-12-02-11:45:06-root-INFO: Loss too large (8897.549->8980.183)! Learning rate decreased to 0.00028.
2024-12-02-11:45:07-root-INFO: grad norm: 1574.724 1401.645 717.737
2024-12-02-11:45:07-root-INFO: grad norm: 1544.539 1356.898 737.855
2024-12-02-11:45:08-root-INFO: grad norm: 1514.464 1352.545 681.339
2024-12-02-11:45:08-root-INFO: grad norm: 1480.274 1299.467 708.940
2024-12-02-11:45:08-root-INFO: Loss Change: 8897.549 -> 8505.318
2024-12-02-11:45:08-root-INFO: Regularization Change: 0.000 -> 0.658
2024-12-02-11:45:08-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-11:45:08-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:08-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-11:45:09-root-INFO: grad norm: 1109.796 993.899 493.774
2024-12-02-11:45:09-root-INFO: Loss too large (8280.654->8377.805)! Learning rate decreased to 0.00036.
2024-12-02-11:45:09-root-INFO: grad norm: 1430.994 1259.517 679.235
2024-12-02-11:45:09-root-INFO: Loss too large (8253.775->8307.787)! Learning rate decreased to 0.00029.
2024-12-02-11:45:10-root-INFO: grad norm: 1333.743 1206.959 567.557
2024-12-02-11:45:10-root-INFO: grad norm: 1249.597 1106.161 581.292
2024-12-02-11:45:11-root-INFO: grad norm: 1170.276 1061.087 493.601
2024-12-02-11:45:11-root-INFO: Loss Change: 8280.654 -> 7951.428
2024-12-02-11:45:11-root-INFO: Regularization Change: 0.000 -> 0.592
2024-12-02-11:45:11-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-11:45:11-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:11-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-11:45:11-root-INFO: grad norm: 1142.917 1014.576 526.207
2024-12-02-11:45:12-root-INFO: Loss too large (7892.573->8056.960)! Learning rate decreased to 0.00038.
2024-12-02-11:45:12-root-INFO: Loss too large (7892.573->7898.327)! Learning rate decreased to 0.00030.
2024-12-02-11:45:12-root-INFO: grad norm: 1039.910 945.728 432.448
2024-12-02-11:45:13-root-INFO: grad norm: 947.976 842.813 433.963
2024-12-02-11:45:13-root-INFO: grad norm: 863.619 789.787 349.392
2024-12-02-11:45:14-root-INFO: grad norm: 788.080 703.000 356.175
2024-12-02-11:45:14-root-INFO: Loss Change: 7892.573 -> 7559.107
2024-12-02-11:45:14-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-11:45:14-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-11:45:14-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:14-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-11:45:14-root-INFO: grad norm: 542.356 507.730 190.683
2024-12-02-11:45:15-root-INFO: grad norm: 777.184 693.715 350.391
2024-12-02-11:45:15-root-INFO: Loss too large (7390.095->7412.313)! Learning rate decreased to 0.00040.
2024-12-02-11:45:15-root-INFO: grad norm: 959.896 876.539 391.255
2024-12-02-11:45:16-root-INFO: grad norm: 1210.090 1079.443 546.919
2024-12-02-11:45:16-root-INFO: Loss too large (7345.166->7368.751)! Learning rate decreased to 0.00032.
2024-12-02-11:45:16-root-INFO: grad norm: 1041.195 947.836 430.923
2024-12-02-11:45:17-root-INFO: Loss Change: 7444.293 -> 7199.546
2024-12-02-11:45:17-root-INFO: Regularization Change: 0.000 -> 0.615
2024-12-02-11:45:17-root-INFO: Undo step: 230
2024-12-02-11:45:17-root-INFO: Undo step: 231
2024-12-02-11:45:17-root-INFO: Undo step: 232
2024-12-02-11:45:17-root-INFO: Undo step: 233
2024-12-02-11:45:17-root-INFO: Undo step: 234
2024-12-02-11:45:17-root-INFO: Undo step: 235
2024-12-02-11:45:17-root-INFO: Undo step: 236
2024-12-02-11:45:17-root-INFO: Undo step: 237
2024-12-02-11:45:17-root-INFO: Undo step: 238
2024-12-02-11:45:17-root-INFO: Undo step: 239
2024-12-02-11:45:17-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-11:45:17-root-INFO: grad norm: 18758.053 15919.236 9921.820
2024-12-02-11:45:17-root-INFO: Loss too large (28240.602->52024.816)! Learning rate decreased to 0.00024.
2024-12-02-11:45:17-root-INFO: Loss too large (28240.602->37897.086)! Learning rate decreased to 0.00019.
2024-12-02-11:45:18-root-INFO: grad norm: 14494.876 11870.476 8318.249
2024-12-02-11:45:18-root-INFO: grad norm: 9181.725 7406.431 5426.679
2024-12-02-11:45:19-root-INFO: grad norm: 7745.338 6487.462 4231.204
2024-12-02-11:45:19-root-INFO: grad norm: 6906.433 5667.654 3946.709
2024-12-02-11:45:19-root-INFO: Loss too large (12493.467->12598.464)! Learning rate decreased to 0.00016.
2024-12-02-11:45:20-root-INFO: Loss Change: 28240.602 -> 11136.363
2024-12-02-11:45:20-root-INFO: Regularization Change: 0.000 -> 6.794
2024-12-02-11:45:20-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-11:45:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:45:20-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-11:45:20-root-INFO: grad norm: 4471.804 3759.758 2421.002
2024-12-02-11:45:20-root-INFO: Loss too large (10979.836->14620.538)! Learning rate decreased to 0.00026.
2024-12-02-11:45:20-root-INFO: Loss too large (10979.836->12218.582)! Learning rate decreased to 0.00020.
2024-12-02-11:45:21-root-INFO: grad norm: 4856.326 4061.231 2662.762
2024-12-02-11:45:21-root-INFO: Loss too large (10901.600->11163.123)! Learning rate decreased to 0.00016.
2024-12-02-11:45:21-root-INFO: grad norm: 3526.533 2938.074 1950.425
2024-12-02-11:45:22-root-INFO: grad norm: 2569.322 2174.916 1367.902
2024-12-02-11:45:22-root-INFO: grad norm: 1960.687 1629.408 1090.560
2024-12-02-11:45:23-root-INFO: Loss Change: 10979.836 -> 9617.941
2024-12-02-11:45:23-root-INFO: Regularization Change: 0.000 -> 0.688
2024-12-02-11:45:23-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-11:45:23-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:45:23-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-11:45:23-root-INFO: grad norm: 1415.055 1171.362 793.910
2024-12-02-11:45:23-root-INFO: Loss too large (9404.350->9541.854)! Learning rate decreased to 0.00027.
2024-12-02-11:45:24-root-INFO: grad norm: 1978.370 1637.717 1109.877
2024-12-02-11:45:24-root-INFO: Loss too large (9384.102->9611.195)! Learning rate decreased to 0.00021.
2024-12-02-11:45:24-root-INFO: grad norm: 2206.464 1891.521 1136.060
2024-12-02-11:45:24-root-INFO: Loss too large (9359.455->9365.417)! Learning rate decreased to 0.00017.
2024-12-02-11:45:25-root-INFO: grad norm: 1611.291 1351.312 877.617
2024-12-02-11:45:25-root-INFO: grad norm: 1198.600 1051.376 575.544
2024-12-02-11:45:25-root-INFO: Loss Change: 9404.350 -> 9014.758
2024-12-02-11:45:25-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-02-11:45:25-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-11:45:25-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:45:26-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-11:45:26-root-INFO: grad norm: 1140.242 969.631 599.973
2024-12-02-11:45:26-root-INFO: Loss too large (8949.300->9079.676)! Learning rate decreased to 0.00028.
2024-12-02-11:45:26-root-INFO: Loss too large (8949.300->8958.412)! Learning rate decreased to 0.00023.
2024-12-02-11:45:27-root-INFO: grad norm: 1203.366 1056.881 575.407
2024-12-02-11:45:27-root-INFO: grad norm: 1294.569 1100.090 682.429
2024-12-02-11:45:28-root-INFO: grad norm: 1403.669 1227.673 680.518
2024-12-02-11:45:28-root-INFO: grad norm: 1524.236 1298.705 797.910
2024-12-02-11:45:28-root-INFO: Loss Change: 8949.300 -> 8760.619
2024-12-02-11:45:28-root-INFO: Regularization Change: 0.000 -> 0.381
2024-12-02-11:45:28-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-11:45:28-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:45:28-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-11:45:29-root-INFO: grad norm: 1591.459 1370.667 808.711
2024-12-02-11:45:29-root-INFO: Loss too large (8616.072->9055.077)! Learning rate decreased to 0.00030.
2024-12-02-11:45:29-root-INFO: Loss too large (8616.072->8749.918)! Learning rate decreased to 0.00024.
2024-12-02-11:45:29-root-INFO: grad norm: 1675.271 1437.631 860.087
2024-12-02-11:45:30-root-INFO: grad norm: 1771.616 1535.480 883.699
2024-12-02-11:45:30-root-INFO: grad norm: 1867.249 1602.407 958.599
2024-12-02-11:45:31-root-INFO: grad norm: 1967.036 1703.655 983.254
2024-12-02-11:45:31-root-INFO: Loss Change: 8616.072 -> 8484.241
2024-12-02-11:45:31-root-INFO: Regularization Change: 0.000 -> 0.453
2024-12-02-11:45:31-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-11:45:31-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:45:31-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-11:45:31-root-INFO: grad norm: 2262.358 1962.297 1125.902
2024-12-02-11:45:31-root-INFO: Loss too large (8461.035->9470.172)! Learning rate decreased to 0.00031.
2024-12-02-11:45:32-root-INFO: Loss too large (8461.035->8779.069)! Learning rate decreased to 0.00025.
2024-12-02-11:45:32-root-INFO: grad norm: 2261.762 1974.136 1103.792
2024-12-02-11:45:33-root-INFO: grad norm: 2269.756 1963.224 1139.098
2024-12-02-11:45:33-root-INFO: grad norm: 2276.852 1988.188 1109.579
2024-12-02-11:45:33-root-INFO: grad norm: 2279.173 1973.599 1139.972
2024-12-02-11:45:34-root-INFO: Loss Change: 8461.035 -> 8242.935
2024-12-02-11:45:34-root-INFO: Regularization Change: 0.000 -> 0.577
2024-12-02-11:45:34-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-11:45:34-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-11:45:34-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-11:45:34-root-INFO: grad norm: 2115.472 1845.458 1034.169
2024-12-02-11:45:34-root-INFO: Loss too large (8167.267->9035.459)! Learning rate decreased to 0.00033.
2024-12-02-11:45:34-root-INFO: Loss too large (8167.267->8442.243)! Learning rate decreased to 0.00026.
2024-12-02-11:45:35-root-INFO: grad norm: 2043.452 1776.772 1009.344
2024-12-02-11:45:35-root-INFO: grad norm: 1973.268 1728.023 952.746
2024-12-02-11:45:36-root-INFO: grad norm: 1906.635 1659.669 938.486
2024-12-02-11:45:36-root-INFO: grad norm: 1838.637 1612.773 882.921
2024-12-02-11:45:36-root-INFO: Loss Change: 8167.267 -> 7887.393
2024-12-02-11:45:36-root-INFO: Regularization Change: 0.000 -> 0.523
2024-12-02-11:45:36-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-11:45:36-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:37-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-11:45:37-root-INFO: grad norm: 1823.606 1608.366 859.475
2024-12-02-11:45:37-root-INFO: Loss too large (7904.197->8481.711)! Learning rate decreased to 0.00035.
2024-12-02-11:45:37-root-INFO: Loss too large (7904.197->8036.467)! Learning rate decreased to 0.00028.
2024-12-02-11:45:38-root-INFO: grad norm: 1649.971 1454.354 779.268
2024-12-02-11:45:38-root-INFO: grad norm: 1526.915 1340.701 730.747
2024-12-02-11:45:38-root-INFO: grad norm: 1414.789 1250.687 661.372
2024-12-02-11:45:39-root-INFO: grad norm: 1317.716 1158.695 627.537
2024-12-02-11:45:39-root-INFO: Loss Change: 7904.197 -> 7558.419
2024-12-02-11:45:39-root-INFO: Regularization Change: 0.000 -> 0.467
2024-12-02-11:45:39-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-11:45:39-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:39-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-11:45:40-root-INFO: grad norm: 844.953 742.219 403.804
2024-12-02-11:45:40-root-INFO: Loss too large (7348.669->7378.963)! Learning rate decreased to 0.00036.
2024-12-02-11:45:40-root-INFO: grad norm: 1008.872 886.052 482.427
2024-12-02-11:45:40-root-INFO: Loss too large (7316.587->7324.670)! Learning rate decreased to 0.00029.
2024-12-02-11:45:41-root-INFO: grad norm: 908.482 820.198 390.661
2024-12-02-11:45:41-root-INFO: grad norm: 833.510 741.753 380.187
2024-12-02-11:45:42-root-INFO: grad norm: 767.291 694.765 325.634
2024-12-02-11:45:42-root-INFO: Loss Change: 7348.669 -> 7135.397
2024-12-02-11:45:42-root-INFO: Regularization Change: 0.000 -> 0.344
2024-12-02-11:45:42-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-11:45:42-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:42-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-11:45:42-root-INFO: grad norm: 792.763 711.389 349.857
2024-12-02-11:45:42-root-INFO: Loss too large (7087.631->7150.568)! Learning rate decreased to 0.00038.
2024-12-02-11:45:43-root-INFO: grad norm: 1029.118 926.720 447.518
2024-12-02-11:45:43-root-INFO: Loss too large (7079.421->7104.958)! Learning rate decreased to 0.00030.
2024-12-02-11:45:43-root-INFO: grad norm: 922.702 823.951 415.312
2024-12-02-11:45:44-root-INFO: grad norm: 827.555 747.869 354.315
2024-12-02-11:45:44-root-INFO: grad norm: 747.999 670.308 331.948
2024-12-02-11:45:45-root-INFO: Loss Change: 7087.631 -> 6906.913
2024-12-02-11:45:45-root-INFO: Regularization Change: 0.000 -> 0.314
2024-12-02-11:45:45-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-11:45:45-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:45-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-11:45:45-root-INFO: grad norm: 464.808 428.726 179.557
2024-12-02-11:45:45-root-INFO: grad norm: 702.056 627.881 314.085
2024-12-02-11:45:46-root-INFO: Loss too large (6768.022->6811.149)! Learning rate decreased to 0.00040.
2024-12-02-11:45:46-root-INFO: grad norm: 890.280 804.951 380.333
2024-12-02-11:45:46-root-INFO: Loss too large (6755.787->6766.839)! Learning rate decreased to 0.00032.
2024-12-02-11:45:47-root-INFO: grad norm: 782.034 702.366 343.888
2024-12-02-11:45:47-root-INFO: grad norm: 688.472 625.869 286.848
2024-12-02-11:45:48-root-INFO: Loss Change: 6792.317 -> 6636.553
2024-12-02-11:45:48-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-02-11:45:48-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-11:45:48-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:48-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-11:45:48-root-INFO: grad norm: 864.958 781.228 371.261
2024-12-02-11:45:48-root-INFO: Loss too large (6615.601->6710.027)! Learning rate decreased to 0.00042.
2024-12-02-11:45:48-root-INFO: grad norm: 1090.036 990.865 454.274
2024-12-02-11:45:49-root-INFO: Loss too large (6611.457->6649.500)! Learning rate decreased to 0.00034.
2024-12-02-11:45:49-root-INFO: grad norm: 941.815 848.596 408.533
2024-12-02-11:45:50-root-INFO: grad norm: 812.535 738.927 337.934
2024-12-02-11:45:50-root-INFO: grad norm: 710.556 642.212 304.062
2024-12-02-11:45:50-root-INFO: Loss Change: 6615.601 -> 6428.142
2024-12-02-11:45:50-root-INFO: Regularization Change: 0.000 -> 0.335
2024-12-02-11:45:50-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-11:45:50-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:50-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-11:45:51-root-INFO: grad norm: 502.354 448.447 226.397
2024-12-02-11:45:51-root-INFO: grad norm: 773.523 690.812 348.019
2024-12-02-11:45:51-root-INFO: Loss too large (6376.696->6452.094)! Learning rate decreased to 0.00044.
2024-12-02-11:45:52-root-INFO: grad norm: 949.513 856.465 409.932
2024-12-02-11:45:52-root-INFO: Loss too large (6371.436->6391.858)! Learning rate decreased to 0.00035.
2024-12-02-11:45:52-root-INFO: grad norm: 795.867 719.422 340.349
2024-12-02-11:45:53-root-INFO: grad norm: 672.696 612.273 278.642
2024-12-02-11:45:53-root-INFO: Loss Change: 6384.875 -> 6243.219
2024-12-02-11:45:53-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-11:45:53-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-11:45:53-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:45:53-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-11:45:53-root-INFO: grad norm: 936.366 842.718 408.176
2024-12-02-11:45:54-root-INFO: Loss too large (6307.022->6426.616)! Learning rate decreased to 0.00046.
2024-12-02-11:45:54-root-INFO: grad norm: 1137.292 1035.945 469.310
2024-12-02-11:45:54-root-INFO: Loss too large (6299.867->6340.594)! Learning rate decreased to 0.00037.
2024-12-02-11:45:55-root-INFO: grad norm: 939.533 850.649 398.896
2024-12-02-11:45:55-root-INFO: grad norm: 774.299 705.670 318.699
2024-12-02-11:45:56-root-INFO: grad norm: 651.270 591.309 272.958
2024-12-02-11:45:56-root-INFO: Loss Change: 6307.022 -> 6092.846
2024-12-02-11:45:56-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-02-11:45:56-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-11:45:56-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-11:45:56-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-11:45:56-root-INFO: grad norm: 582.038 522.678 256.078
2024-12-02-11:45:56-root-INFO: Loss too large (6026.952->6042.313)! Learning rate decreased to 0.00049.
2024-12-02-11:45:57-root-INFO: grad norm: 653.200 595.049 269.420
2024-12-02-11:45:57-root-INFO: grad norm: 774.579 704.433 322.097
2024-12-02-11:45:57-root-INFO: Loss too large (5991.677->5993.535)! Learning rate decreased to 0.00039.
2024-12-02-11:45:58-root-INFO: grad norm: 626.031 569.355 260.289
2024-12-02-11:45:58-root-INFO: grad norm: 511.795 468.420 206.195
2024-12-02-11:45:59-root-INFO: Loss Change: 6026.952 -> 5878.375
2024-12-02-11:45:59-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-02-11:45:59-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-11:45:59-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:45:59-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-11:45:59-root-INFO: grad norm: 774.119 696.280 338.312
2024-12-02-11:45:59-root-INFO: Loss too large (5871.870->5944.166)! Learning rate decreased to 0.00051.
2024-12-02-11:46:00-root-INFO: grad norm: 905.379 828.056 366.106
2024-12-02-11:46:00-root-INFO: Loss too large (5854.998->5870.525)! Learning rate decreased to 0.00041.
2024-12-02-11:46:00-root-INFO: grad norm: 728.726 660.145 308.626
2024-12-02-11:46:01-root-INFO: grad norm: 587.846 538.806 235.055
2024-12-02-11:46:01-root-INFO: grad norm: 487.196 443.074 202.596
2024-12-02-11:46:01-root-INFO: Loss Change: 5871.870 -> 5688.200
2024-12-02-11:46:01-root-INFO: Regularization Change: 0.000 -> 0.348
2024-12-02-11:46:01-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-11:46:01-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:02-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-11:46:02-root-INFO: grad norm: 321.958 289.692 140.481
2024-12-02-11:46:02-root-INFO: grad norm: 353.578 318.079 154.413
2024-12-02-11:46:03-root-INFO: grad norm: 505.024 463.257 201.103
2024-12-02-11:46:03-root-INFO: Loss too large (5581.515->5600.419)! Learning rate decreased to 0.00054.
2024-12-02-11:46:03-root-INFO: grad norm: 600.746 547.481 247.307
2024-12-02-11:46:04-root-INFO: grad norm: 734.857 673.216 294.612
2024-12-02-11:46:04-root-INFO: Loss too large (5559.691->5568.610)! Learning rate decreased to 0.00043.
2024-12-02-11:46:04-root-INFO: Loss Change: 5646.626 -> 5519.652
2024-12-02-11:46:04-root-INFO: Regularization Change: 0.000 -> 0.487
2024-12-02-11:46:04-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-11:46:04-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:04-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-11:46:05-root-INFO: grad norm: 826.242 749.283 348.210
2024-12-02-11:46:05-root-INFO: Loss too large (5534.808->5660.694)! Learning rate decreased to 0.00056.
2024-12-02-11:46:05-root-INFO: Loss too large (5534.808->5536.085)! Learning rate decreased to 0.00045.
2024-12-02-11:46:05-root-INFO: grad norm: 654.990 601.719 258.741
2024-12-02-11:46:06-root-INFO: grad norm: 537.562 489.111 223.033
2024-12-02-11:46:06-root-INFO: grad norm: 445.182 409.482 174.675
2024-12-02-11:46:07-root-INFO: grad norm: 377.328 344.499 153.938
2024-12-02-11:46:07-root-INFO: Loss Change: 5534.808 -> 5343.327
2024-12-02-11:46:07-root-INFO: Regularization Change: 0.000 -> 0.336
2024-12-02-11:46:07-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-11:46:07-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:07-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-11:46:07-root-INFO: grad norm: 280.910 258.079 110.930
2024-12-02-11:46:08-root-INFO: grad norm: 373.780 342.719 149.180
2024-12-02-11:46:08-root-INFO: grad norm: 586.836 539.108 231.817
2024-12-02-11:46:09-root-INFO: Loss too large (5245.663->5300.564)! Learning rate decreased to 0.00059.
2024-12-02-11:46:09-root-INFO: grad norm: 700.625 641.395 281.936
2024-12-02-11:46:09-root-INFO: Loss too large (5240.659->5243.916)! Learning rate decreased to 0.00047.
2024-12-02-11:46:10-root-INFO: grad norm: 559.855 514.962 219.664
2024-12-02-11:46:10-root-INFO: Loss Change: 5278.041 -> 5162.628
2024-12-02-11:46:10-root-INFO: Regularization Change: 0.000 -> 0.427
2024-12-02-11:46:10-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-11:46:10-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:10-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-11:46:10-root-INFO: grad norm: 558.194 508.509 230.216
2024-12-02-11:46:11-root-INFO: Loss too large (5143.094->5183.402)! Learning rate decreased to 0.00062.
2024-12-02-11:46:11-root-INFO: grad norm: 642.295 590.475 252.749
2024-12-02-11:46:11-root-INFO: grad norm: 748.124 683.773 303.552
2024-12-02-11:46:12-root-INFO: Loss too large (5127.101->5130.424)! Learning rate decreased to 0.00050.
2024-12-02-11:46:12-root-INFO: grad norm: 573.337 527.891 223.710
2024-12-02-11:46:13-root-INFO: grad norm: 452.651 413.825 183.417
2024-12-02-11:46:13-root-INFO: Loss Change: 5143.094 -> 5006.836
2024-12-02-11:46:13-root-INFO: Regularization Change: 0.000 -> 0.356
2024-12-02-11:46:13-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-11:46:13-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:13-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-11:46:13-root-INFO: grad norm: 267.551 239.857 118.542
2024-12-02-11:46:14-root-INFO: grad norm: 221.977 198.733 98.889
2024-12-02-11:46:14-root-INFO: grad norm: 209.270 193.048 80.787
2024-12-02-11:46:15-root-INFO: grad norm: 212.335 193.461 87.516
2024-12-02-11:46:15-root-INFO: grad norm: 238.813 221.424 89.462
2024-12-02-11:46:15-root-INFO: Loss Change: 4970.291 -> 4803.500
2024-12-02-11:46:15-root-INFO: Regularization Change: 0.000 -> 0.703
2024-12-02-11:46:15-root-INFO: Undo step: 220
2024-12-02-11:46:15-root-INFO: Undo step: 221
2024-12-02-11:46:15-root-INFO: Undo step: 222
2024-12-02-11:46:15-root-INFO: Undo step: 223
2024-12-02-11:46:15-root-INFO: Undo step: 224
2024-12-02-11:46:15-root-INFO: Undo step: 225
2024-12-02-11:46:15-root-INFO: Undo step: 226
2024-12-02-11:46:15-root-INFO: Undo step: 227
2024-12-02-11:46:15-root-INFO: Undo step: 228
2024-12-02-11:46:15-root-INFO: Undo step: 229
2024-12-02-11:46:16-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-11:46:16-root-INFO: grad norm: 5965.186 4487.421 3930.203
2024-12-02-11:46:16-root-INFO: Loss too large (14160.854->14178.081)! Learning rate decreased to 0.00040.
2024-12-02-11:46:16-root-INFO: grad norm: 4970.485 3799.822 3204.228
2024-12-02-11:46:17-root-INFO: grad norm: 4611.921 3728.219 2714.811
2024-12-02-11:46:17-root-INFO: grad norm: 4689.348 3885.330 2625.680
2024-12-02-11:46:18-root-INFO: Loss too large (9928.319->10334.993)! Learning rate decreased to 0.00032.
2024-12-02-11:46:18-root-INFO: grad norm: 3532.651 3040.998 1797.764
2024-12-02-11:46:18-root-INFO: Loss Change: 14160.854 -> 8168.240
2024-12-02-11:46:18-root-INFO: Regularization Change: 0.000 -> 7.738
2024-12-02-11:46:18-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-11:46:18-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:46:18-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-11:46:19-root-INFO: grad norm: 2676.562 2322.052 1331.187
2024-12-02-11:46:19-root-INFO: Loss too large (7957.198->9047.796)! Learning rate decreased to 0.00042.
2024-12-02-11:46:19-root-INFO: Loss too large (7957.198->8073.476)! Learning rate decreased to 0.00034.
2024-12-02-11:46:19-root-INFO: grad norm: 2146.288 1891.112 1015.011
2024-12-02-11:46:20-root-INFO: grad norm: 1771.261 1571.536 817.091
2024-12-02-11:46:20-root-INFO: grad norm: 1491.843 1322.057 691.202
2024-12-02-11:46:21-root-INFO: grad norm: 1260.819 1127.183 564.909
2024-12-02-11:46:21-root-INFO: Loss Change: 7957.198 -> 6804.111
2024-12-02-11:46:21-root-INFO: Regularization Change: 0.000 -> 1.470
2024-12-02-11:46:21-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-11:46:21-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:46:21-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-11:46:22-root-INFO: grad norm: 1121.190 1002.351 502.355
2024-12-02-11:46:22-root-INFO: Loss too large (6750.153->6847.988)! Learning rate decreased to 0.00044.
2024-12-02-11:46:22-root-INFO: grad norm: 1332.551 1197.252 585.047
2024-12-02-11:46:22-root-INFO: Loss too large (6700.788->6703.389)! Learning rate decreased to 0.00035.
2024-12-02-11:46:23-root-INFO: grad norm: 1092.179 980.653 480.806
2024-12-02-11:46:23-root-INFO: grad norm: 902.035 815.292 385.961
2024-12-02-11:46:24-root-INFO: grad norm: 756.021 682.667 324.859
2024-12-02-11:46:24-root-INFO: Loss Change: 6750.153 -> 6334.798
2024-12-02-11:46:24-root-INFO: Regularization Change: 0.000 -> 0.730
2024-12-02-11:46:24-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-11:46:24-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:46:24-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-11:46:25-root-INFO: grad norm: 482.912 430.164 219.461
2024-12-02-11:46:25-root-INFO: grad norm: 520.484 454.428 253.770
2024-12-02-11:46:26-root-INFO: grad norm: 714.275 639.547 318.071
2024-12-02-11:46:26-root-INFO: Loss too large (6137.287->6163.351)! Learning rate decreased to 0.00046.
2024-12-02-11:46:26-root-INFO: grad norm: 820.483 741.225 351.821
2024-12-02-11:46:27-root-INFO: grad norm: 985.027 891.444 419.052
2024-12-02-11:46:27-root-INFO: Loss too large (6091.783->6099.681)! Learning rate decreased to 0.00037.
2024-12-02-11:46:27-root-INFO: Loss Change: 6245.946 -> 6025.569
2024-12-02-11:46:27-root-INFO: Regularization Change: 0.000 -> 0.696
2024-12-02-11:46:27-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-11:46:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-11:46:27-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-11:46:28-root-INFO: grad norm: 794.288 710.365 355.351
2024-12-02-11:46:28-root-INFO: Loss too large (5955.034->6011.992)! Learning rate decreased to 0.00049.
2024-12-02-11:46:28-root-INFO: grad norm: 911.552 822.193 393.608
2024-12-02-11:46:29-root-INFO: grad norm: 1071.507 972.007 450.921
2024-12-02-11:46:29-root-INFO: Loss too large (5925.261->5937.350)! Learning rate decreased to 0.00039.
2024-12-02-11:46:29-root-INFO: grad norm: 837.027 759.964 350.812
2024-12-02-11:46:30-root-INFO: grad norm: 669.277 609.945 275.498
2024-12-02-11:46:30-root-INFO: Loss Change: 5955.034 -> 5739.532
2024-12-02-11:46:30-root-INFO: Regularization Change: 0.000 -> 0.456
2024-12-02-11:46:30-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-11:46:30-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:30-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-11:46:30-root-INFO: grad norm: 396.420 358.710 168.748
2024-12-02-11:46:31-root-INFO: grad norm: 348.672 307.798 163.808
2024-12-02-11:46:31-root-INFO: grad norm: 391.108 360.720 151.151
2024-12-02-11:46:32-root-INFO: grad norm: 548.685 499.021 228.108
2024-12-02-11:46:32-root-INFO: Loss too large (5516.496->5535.771)! Learning rate decreased to 0.00051.
2024-12-02-11:46:33-root-INFO: grad norm: 636.760 582.640 256.893
2024-12-02-11:46:33-root-INFO: Loss Change: 5638.129 -> 5488.756
2024-12-02-11:46:33-root-INFO: Regularization Change: 0.000 -> 0.619
2024-12-02-11:46:33-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-11:46:33-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:33-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-11:46:33-root-INFO: grad norm: 944.128 859.969 389.655
2024-12-02-11:46:33-root-INFO: Loss too large (5485.736->5645.164)! Learning rate decreased to 0.00054.
2024-12-02-11:46:33-root-INFO: Loss too large (5485.736->5490.057)! Learning rate decreased to 0.00043.
2024-12-02-11:46:34-root-INFO: grad norm: 732.579 673.932 287.208
2024-12-02-11:46:34-root-INFO: grad norm: 586.365 537.337 234.719
2024-12-02-11:46:35-root-INFO: grad norm: 474.006 436.698 184.327
2024-12-02-11:46:36-root-INFO: grad norm: 392.759 361.932 152.528
2024-12-02-11:46:36-root-INFO: Loss Change: 5485.736 -> 5268.141
2024-12-02-11:46:36-root-INFO: Regularization Change: 0.000 -> 0.345
2024-12-02-11:46:36-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-11:46:36-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:36-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-11:46:36-root-INFO: grad norm: 280.308 255.665 114.926
2024-12-02-11:46:37-root-INFO: grad norm: 243.435 221.239 101.555
2024-12-02-11:46:37-root-INFO: grad norm: 229.086 213.590 82.822
2024-12-02-11:46:38-root-INFO: grad norm: 232.101 214.394 88.917
2024-12-02-11:46:38-root-INFO: grad norm: 263.261 245.371 95.391
2024-12-02-11:46:39-root-INFO: Loss Change: 5226.964 -> 5068.327
2024-12-02-11:46:39-root-INFO: Regularization Change: 0.000 -> 0.592
2024-12-02-11:46:39-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-11:46:39-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:39-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-11:46:39-root-INFO: grad norm: 296.118 271.750 117.634
2024-12-02-11:46:39-root-INFO: grad norm: 406.857 376.058 155.284
2024-12-02-11:46:40-root-INFO: Loss too large (4981.077->4986.290)! Learning rate decreased to 0.00059.
2024-12-02-11:46:40-root-INFO: grad norm: 461.102 424.948 178.979
2024-12-02-11:46:41-root-INFO: grad norm: 530.849 490.184 203.765
2024-12-02-11:46:41-root-INFO: grad norm: 618.974 569.226 243.127
2024-12-02-11:46:41-root-INFO: Loss Change: 4999.669 -> 4944.553
2024-12-02-11:46:41-root-INFO: Regularization Change: 0.000 -> 0.420
2024-12-02-11:46:41-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-11:46:41-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:42-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-11:46:42-root-INFO: grad norm: 818.660 751.451 324.847
2024-12-02-11:46:42-root-INFO: Loss too large (4927.509->5063.268)! Learning rate decreased to 0.00062.
2024-12-02-11:46:42-root-INFO: Loss too large (4927.509->4929.565)! Learning rate decreased to 0.00050.
2024-12-02-11:46:43-root-INFO: grad norm: 591.360 545.417 228.532
2024-12-02-11:46:43-root-INFO: grad norm: 443.722 409.319 171.311
2024-12-02-11:46:44-root-INFO: grad norm: 340.810 316.037 127.562
2024-12-02-11:46:44-root-INFO: grad norm: 274.310 255.464 99.920
2024-12-02-11:46:44-root-INFO: Loss Change: 4927.509 -> 4755.000
2024-12-02-11:46:44-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-02-11:46:44-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-11:46:44-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:45-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-11:46:45-root-INFO: grad norm: 298.975 266.999 134.528
2024-12-02-11:46:45-root-INFO: grad norm: 335.110 309.273 129.030
2024-12-02-11:46:46-root-INFO: grad norm: 481.501 442.164 190.616
2024-12-02-11:46:46-root-INFO: Loss too large (4684.626->4713.163)! Learning rate decreased to 0.00065.
2024-12-02-11:46:46-root-INFO: grad norm: 539.030 497.305 207.943
2024-12-02-11:46:47-root-INFO: grad norm: 607.753 559.821 236.565
2024-12-02-11:46:47-root-INFO: Loss Change: 4728.936 -> 4662.558
2024-12-02-11:46:47-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-11:46:47-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-11:46:47-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:46:47-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-11:46:47-root-INFO: grad norm: 649.872 595.950 259.185
2024-12-02-11:46:47-root-INFO: Loss too large (4628.254->4709.370)! Learning rate decreased to 0.00068.
2024-12-02-11:46:48-root-INFO: grad norm: 706.866 651.604 273.994
2024-12-02-11:46:48-root-INFO: grad norm: 774.913 712.971 303.582
2024-12-02-11:46:49-root-INFO: Loss too large (4618.992->4623.875)! Learning rate decreased to 0.00055.
2024-12-02-11:46:49-root-INFO: grad norm: 545.042 502.693 210.642
2024-12-02-11:46:50-root-INFO: grad norm: 387.138 357.514 148.525
2024-12-02-11:46:50-root-INFO: Loss Change: 4628.254 -> 4494.531
2024-12-02-11:46:50-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-02-11:46:50-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-11:46:50-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-11:46:50-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-11:46:50-root-INFO: grad norm: 468.060 427.188 191.285
2024-12-02-11:46:50-root-INFO: Loss too large (4484.229->4505.642)! Learning rate decreased to 0.00071.
2024-12-02-11:46:51-root-INFO: grad norm: 495.656 459.328 186.259
2024-12-02-11:46:51-root-INFO: grad norm: 529.708 487.848 206.386
2024-12-02-11:46:52-root-INFO: grad norm: 569.571 525.664 219.292
2024-12-02-11:46:52-root-INFO: grad norm: 612.358 564.746 236.738
2024-12-02-11:46:52-root-INFO: Loss Change: 4484.229 -> 4428.693
2024-12-02-11:46:52-root-INFO: Regularization Change: 0.000 -> 0.467
2024-12-02-11:46:52-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-11:46:52-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:46:53-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-11:46:53-root-INFO: grad norm: 481.864 445.753 183.022
2024-12-02-11:46:53-root-INFO: Loss too large (4380.081->4400.129)! Learning rate decreased to 0.00075.
2024-12-02-11:46:53-root-INFO: grad norm: 486.880 446.600 193.908
2024-12-02-11:46:54-root-INFO: grad norm: 510.325 472.219 193.496
2024-12-02-11:46:54-root-INFO: grad norm: 536.539 495.109 206.738
2024-12-02-11:46:55-root-INFO: grad norm: 566.074 522.383 218.071
2024-12-02-11:46:55-root-INFO: Loss Change: 4380.081 -> 4312.852
2024-12-02-11:46:55-root-INFO: Regularization Change: 0.000 -> 0.481
2024-12-02-11:46:55-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-11:46:55-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:46:55-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-11:46:55-root-INFO: grad norm: 777.556 708.819 319.638
2024-12-02-11:46:55-root-INFO: Loss too large (4335.337->4455.588)! Learning rate decreased to 0.00079.
2024-12-02-11:46:56-root-INFO: grad norm: 807.156 745.427 309.578
2024-12-02-11:46:56-root-INFO: grad norm: 843.372 776.579 328.939
2024-12-02-11:46:57-root-INFO: grad norm: 883.486 814.724 341.720
2024-12-02-11:46:57-root-INFO: Loss too large (4302.682->4307.535)! Learning rate decreased to 0.00063.
2024-12-02-11:46:57-root-INFO: grad norm: 582.917 537.678 225.155
2024-12-02-11:46:58-root-INFO: Loss Change: 4335.337 -> 4159.031
2024-12-02-11:46:58-root-INFO: Regularization Change: 0.000 -> 0.503
2024-12-02-11:46:58-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-11:46:58-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:46:58-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-11:46:58-root-INFO: grad norm: 333.992 309.356 125.894
2024-12-02-11:46:58-root-INFO: Loss too large (4134.019->4138.185)! Learning rate decreased to 0.00082.
2024-12-02-11:46:59-root-INFO: grad norm: 334.687 310.502 124.915
2024-12-02-11:46:59-root-INFO: grad norm: 336.874 311.937 127.197
2024-12-02-11:47:00-root-INFO: grad norm: 339.383 314.965 126.403
2024-12-02-11:47:00-root-INFO: grad norm: 342.371 316.387 130.832
2024-12-02-11:47:00-root-INFO: Loss Change: 4134.019 -> 4061.681
2024-12-02-11:47:00-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-11:47:00-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-11:47:00-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:01-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-11:47:01-root-INFO: grad norm: 471.550 428.384 197.096
2024-12-02-11:47:01-root-INFO: Loss too large (4056.120->4082.131)! Learning rate decreased to 0.00086.
2024-12-02-11:47:01-root-INFO: grad norm: 456.427 421.991 173.921
2024-12-02-11:47:02-root-INFO: grad norm: 449.693 415.522 171.945
2024-12-02-11:47:02-root-INFO: grad norm: 443.154 409.380 169.687
2024-12-02-11:47:03-root-INFO: grad norm: 437.464 405.376 164.455
2024-12-02-11:47:03-root-INFO: Loss Change: 4056.120 -> 3972.752
2024-12-02-11:47:03-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-11:47:03-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-11:47:03-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:03-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-11:47:04-root-INFO: grad norm: 291.089 271.242 105.643
2024-12-02-11:47:04-root-INFO: grad norm: 361.280 334.326 136.929
2024-12-02-11:47:04-root-INFO: Loss too large (3932.216->3940.752)! Learning rate decreased to 0.00090.
2024-12-02-11:47:05-root-INFO: grad norm: 349.011 324.164 129.332
2024-12-02-11:47:05-root-INFO: grad norm: 339.344 314.472 127.520
2024-12-02-11:47:05-root-INFO: grad norm: 330.640 306.347 124.398
2024-12-02-11:47:06-root-INFO: Loss Change: 3948.016 -> 3869.111
2024-12-02-11:47:06-root-INFO: Regularization Change: 0.000 -> 0.461
2024-12-02-11:47:06-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-11:47:06-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:06-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-11:47:06-root-INFO: grad norm: 326.345 303.009 121.188
2024-12-02-11:47:06-root-INFO: Loss too large (3851.806->3856.767)! Learning rate decreased to 0.00095.
2024-12-02-11:47:07-root-INFO: grad norm: 303.442 281.776 112.603
2024-12-02-11:47:07-root-INFO: grad norm: 283.705 264.161 103.476
2024-12-02-11:47:08-root-INFO: grad norm: 265.474 246.427 98.743
2024-12-02-11:47:08-root-INFO: grad norm: 249.252 232.927 88.723
2024-12-02-11:47:08-root-INFO: Loss Change: 3851.806 -> 3776.133
2024-12-02-11:47:08-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-11:47:08-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-11:47:08-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-11:47:09-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-11:47:09-root-INFO: grad norm: 199.397 177.979 89.903
2024-12-02-11:47:09-root-INFO: grad norm: 155.661 146.815 51.728
2024-12-02-11:47:10-root-INFO: grad norm: 146.440 139.556 44.370
2024-12-02-11:47:10-root-INFO: grad norm: 149.582 143.582 41.939
2024-12-02-11:47:11-root-INFO: grad norm: 163.621 155.957 49.491
2024-12-02-11:47:11-root-INFO: Loss Change: 3749.392 -> 3661.580
2024-12-02-11:47:11-root-INFO: Regularization Change: 0.000 -> 0.553
2024-12-02-11:47:11-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-11:47:11-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:47:11-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-11:47:11-root-INFO: grad norm: 240.645 226.539 81.178
2024-12-02-11:47:12-root-INFO: grad norm: 298.190 280.869 100.150
2024-12-02-11:47:12-root-INFO: Loss too large (3647.377->3648.526)! Learning rate decreased to 0.00104.
2024-12-02-11:47:12-root-INFO: grad norm: 260.661 245.907 86.452
2024-12-02-11:47:13-root-INFO: grad norm: 230.908 217.268 78.186
2024-12-02-11:47:13-root-INFO: grad norm: 206.731 195.194 68.096
2024-12-02-11:47:14-root-INFO: Loss Change: 3654.139 -> 3586.428
2024-12-02-11:47:14-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-02-11:47:14-root-INFO: Undo step: 210
2024-12-02-11:47:14-root-INFO: Undo step: 211
2024-12-02-11:47:14-root-INFO: Undo step: 212
2024-12-02-11:47:14-root-INFO: Undo step: 213
2024-12-02-11:47:14-root-INFO: Undo step: 214
2024-12-02-11:47:14-root-INFO: Undo step: 215
2024-12-02-11:47:14-root-INFO: Undo step: 216
2024-12-02-11:47:14-root-INFO: Undo step: 217
2024-12-02-11:47:14-root-INFO: Undo step: 218
2024-12-02-11:47:14-root-INFO: Undo step: 219
2024-12-02-11:47:14-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-11:47:14-root-INFO: grad norm: 14911.340 13763.308 5737.545
2024-12-02-11:47:14-root-INFO: grad norm: 5461.905 4653.196 2860.100
2024-12-02-11:47:15-root-INFO: grad norm: 3958.255 3542.604 1765.711
2024-12-02-11:47:15-root-INFO: grad norm: 3275.048 2900.601 1520.677
2024-12-02-11:47:16-root-INFO: grad norm: 2577.929 2380.725 988.871
2024-12-02-11:47:16-root-INFO: Loss Change: 30626.479 -> 10324.898
2024-12-02-11:47:16-root-INFO: Regularization Change: 0.000 -> 122.931
2024-12-02-11:47:16-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-11:47:16-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:47:16-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-11:47:17-root-INFO: grad norm: 2390.887 2125.546 1094.713
2024-12-02-11:47:17-root-INFO: grad norm: 2031.271 1845.967 847.623
2024-12-02-11:47:17-root-INFO: grad norm: 1951.565 1705.845 947.997
2024-12-02-11:47:18-root-INFO: grad norm: 1839.915 1705.609 690.063
2024-12-02-11:47:18-root-INFO: Loss too large (8298.201->8370.468)! Learning rate decreased to 0.00068.
2024-12-02-11:47:19-root-INFO: grad norm: 1588.604 1505.927 505.813
2024-12-02-11:47:19-root-INFO: Loss Change: 10327.498 -> 7661.331
2024-12-02-11:47:19-root-INFO: Regularization Change: 0.000 -> 6.713
2024-12-02-11:47:19-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-11:47:19-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-11:47:19-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-11:47:19-root-INFO: grad norm: 1413.386 1307.315 537.204
2024-12-02-11:47:20-root-INFO: grad norm: 1965.948 1796.941 797.469
2024-12-02-11:47:20-root-INFO: grad norm: 2338.325 2162.114 890.520
2024-12-02-11:47:21-root-INFO: grad norm: 2223.892 1968.430 1034.881
2024-12-02-11:47:21-root-INFO: grad norm: 1940.823 1793.325 742.146
2024-12-02-11:47:21-root-INFO: Loss too large (5447.489->5653.517)! Learning rate decreased to 0.00071.
2024-12-02-11:47:22-root-INFO: Loss Change: 7439.243 -> 5022.564
2024-12-02-11:47:22-root-INFO: Regularization Change: 0.000 -> 11.824
2024-12-02-11:47:22-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-11:47:22-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:22-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-11:47:22-root-INFO: grad norm: 1448.953 1338.108 555.816
2024-12-02-11:47:22-root-INFO: Loss too large (4894.957->5167.681)! Learning rate decreased to 0.00075.
2024-12-02-11:47:22-root-INFO: grad norm: 1319.415 1219.287 504.177
2024-12-02-11:47:23-root-INFO: grad norm: 1202.068 1109.071 463.605
2024-12-02-11:47:23-root-INFO: grad norm: 1101.370 1019.464 416.785
2024-12-02-11:47:24-root-INFO: grad norm: 1010.256 930.923 392.428
2024-12-02-11:47:24-root-INFO: Loss Change: 4894.957 -> 4398.543
2024-12-02-11:47:24-root-INFO: Regularization Change: 0.000 -> 1.836
2024-12-02-11:47:24-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-11:47:24-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:24-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-11:47:25-root-INFO: grad norm: 1058.697 978.735 403.631
2024-12-02-11:47:25-root-INFO: Loss too large (4406.305->4542.487)! Learning rate decreased to 0.00079.
2024-12-02-11:47:25-root-INFO: grad norm: 945.814 870.172 370.628
2024-12-02-11:47:26-root-INFO: grad norm: 858.896 797.993 317.663
2024-12-02-11:47:26-root-INFO: grad norm: 779.887 718.226 303.933
2024-12-02-11:47:27-root-INFO: grad norm: 711.964 661.918 262.214
2024-12-02-11:47:27-root-INFO: Loss Change: 4406.305 -> 4111.767
2024-12-02-11:47:27-root-INFO: Regularization Change: 0.000 -> 1.062
2024-12-02-11:47:27-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-11:47:27-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:27-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-11:47:27-root-INFO: grad norm: 608.458 561.733 233.832
2024-12-02-11:47:27-root-INFO: Loss too large (4092.046->4122.374)! Learning rate decreased to 0.00082.
2024-12-02-11:47:28-root-INFO: grad norm: 536.246 497.785 199.423
2024-12-02-11:47:28-root-INFO: grad norm: 474.066 438.363 180.489
2024-12-02-11:47:29-root-INFO: grad norm: 421.497 392.549 153.508
2024-12-02-11:47:29-root-INFO: grad norm: 376.267 347.850 143.449
2024-12-02-11:47:30-root-INFO: Loss Change: 4092.046 -> 3932.723
2024-12-02-11:47:30-root-INFO: Regularization Change: 0.000 -> 0.586
2024-12-02-11:47:30-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-11:47:30-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:30-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-11:47:30-root-INFO: grad norm: 446.229 409.189 178.002
2024-12-02-11:47:31-root-INFO: grad norm: 560.301 513.911 223.232
2024-12-02-11:47:31-root-INFO: Loss too large (3911.066->3941.315)! Learning rate decreased to 0.00086.
2024-12-02-11:47:31-root-INFO: grad norm: 486.056 452.247 178.109
2024-12-02-11:47:32-root-INFO: grad norm: 423.106 390.366 163.195
2024-12-02-11:47:32-root-INFO: grad norm: 371.599 347.520 131.589
2024-12-02-11:47:32-root-INFO: Loss Change: 3911.108 -> 3799.585
2024-12-02-11:47:32-root-INFO: Regularization Change: 0.000 -> 0.520
2024-12-02-11:47:32-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-11:47:32-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:33-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-11:47:33-root-INFO: grad norm: 249.674 232.220 91.712
2024-12-02-11:47:33-root-INFO: grad norm: 242.756 231.301 73.690
2024-12-02-11:47:34-root-INFO: grad norm: 279.745 261.595 99.124
2024-12-02-11:47:34-root-INFO: grad norm: 346.842 326.424 117.246
2024-12-02-11:47:34-root-INFO: Loss too large (3728.326->3729.996)! Learning rate decreased to 0.00090.
2024-12-02-11:47:35-root-INFO: grad norm: 304.277 282.525 112.977
2024-12-02-11:47:35-root-INFO: Loss Change: 3781.232 -> 3689.441
2024-12-02-11:47:35-root-INFO: Regularization Change: 0.000 -> 0.531
2024-12-02-11:47:35-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-11:47:35-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:47:35-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-11:47:35-root-INFO: grad norm: 266.579 249.689 93.381
2024-12-02-11:47:36-root-INFO: grad norm: 326.041 302.652 121.262
2024-12-02-11:47:36-root-INFO: grad norm: 407.350 380.896 144.403
2024-12-02-11:47:36-root-INFO: Loss too large (3658.367->3668.338)! Learning rate decreased to 0.00095.
2024-12-02-11:47:37-root-INFO: grad norm: 344.207 319.510 128.030
2024-12-02-11:47:37-root-INFO: grad norm: 293.760 276.032 100.505
2024-12-02-11:47:38-root-INFO: Loss Change: 3667.336 -> 3594.500
2024-12-02-11:47:38-root-INFO: Regularization Change: 0.000 -> 0.429
2024-12-02-11:47:38-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-11:47:38-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-11:47:38-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-11:47:38-root-INFO: grad norm: 253.896 230.564 106.317
2024-12-02-11:47:39-root-INFO: grad norm: 216.755 207.418 62.930
2024-12-02-11:47:39-root-INFO: grad norm: 213.427 201.601 70.059
2024-12-02-11:47:40-root-INFO: grad norm: 231.296 222.929 61.646
2024-12-02-11:47:40-root-INFO: grad norm: 268.272 254.408 85.125
2024-12-02-11:47:40-root-INFO: Loss Change: 3573.859 -> 3496.043
2024-12-02-11:47:40-root-INFO: Regularization Change: 0.000 -> 0.613
2024-12-02-11:47:40-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-11:47:40-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:47:40-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-11:47:41-root-INFO: grad norm: 360.499 341.418 115.730
2024-12-02-11:47:41-root-INFO: Loss too large (3485.563->3490.307)! Learning rate decreased to 0.00104.
2024-12-02-11:47:41-root-INFO: grad norm: 300.795 285.265 95.402
2024-12-02-11:47:42-root-INFO: grad norm: 255.245 242.969 78.205
2024-12-02-11:47:42-root-INFO: grad norm: 221.537 210.141 70.138
2024-12-02-11:47:43-root-INFO: grad norm: 195.267 186.678 57.276
2024-12-02-11:47:43-root-INFO: Loss Change: 3485.563 -> 3396.728
2024-12-02-11:47:43-root-INFO: Regularization Change: 0.000 -> 0.390
2024-12-02-11:47:43-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-11:47:43-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:47:43-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-11:47:43-root-INFO: grad norm: 217.369 200.571 83.787
2024-12-02-11:47:44-root-INFO: grad norm: 241.578 230.712 71.638
2024-12-02-11:47:44-root-INFO: grad norm: 311.032 295.122 98.203
2024-12-02-11:47:44-root-INFO: Loss too large (3355.235->3364.598)! Learning rate decreased to 0.00109.
2024-12-02-11:47:45-root-INFO: grad norm: 293.603 281.610 83.058
2024-12-02-11:47:45-root-INFO: grad norm: 282.792 269.404 85.981
2024-12-02-11:47:46-root-INFO: Loss Change: 3377.218 -> 3314.535
2024-12-02-11:47:46-root-INFO: Regularization Change: 0.000 -> 0.471
2024-12-02-11:47:46-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-11:47:46-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:47:46-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-11:47:46-root-INFO: grad norm: 298.802 281.440 100.370
2024-12-02-11:47:46-root-INFO: Loss too large (3307.621->3309.565)! Learning rate decreased to 0.00114.
2024-12-02-11:47:47-root-INFO: grad norm: 284.414 270.894 86.649
2024-12-02-11:47:47-root-INFO: grad norm: 293.381 281.472 82.741
2024-12-02-11:47:47-root-INFO: grad norm: 315.434 301.117 93.951
2024-12-02-11:47:48-root-INFO: grad norm: 345.624 331.899 96.431
2024-12-02-11:47:48-root-INFO: Loss Change: 3307.621 -> 3256.209
2024-12-02-11:47:48-root-INFO: Regularization Change: 0.000 -> 0.489
2024-12-02-11:47:48-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-11:47:48-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:47:48-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-11:47:49-root-INFO: grad norm: 437.506 414.378 140.366
2024-12-02-11:47:49-root-INFO: Loss too large (3251.389->3326.824)! Learning rate decreased to 0.00120.
2024-12-02-11:47:49-root-INFO: Loss too large (3251.389->3256.165)! Learning rate decreased to 0.00096.
2024-12-02-11:47:49-root-INFO: grad norm: 334.905 318.792 102.630
2024-12-02-11:47:50-root-INFO: grad norm: 266.892 254.576 80.138
2024-12-02-11:47:50-root-INFO: grad norm: 222.464 211.678 68.432
2024-12-02-11:47:51-root-INFO: grad norm: 192.180 183.792 56.158
2024-12-02-11:47:51-root-INFO: Loss Change: 3251.389 -> 3153.876
2024-12-02-11:47:51-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-11:47:51-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-11:47:51-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:47:51-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-11:47:51-root-INFO: grad norm: 184.967 174.524 61.271
2024-12-02-11:47:52-root-INFO: grad norm: 275.939 263.918 80.557
2024-12-02-11:47:52-root-INFO: Loss too large (3124.768->3156.848)! Learning rate decreased to 0.00126.
2024-12-02-11:47:52-root-INFO: Loss too large (3124.768->3126.088)! Learning rate decreased to 0.00101.
2024-12-02-11:47:53-root-INFO: grad norm: 255.038 243.991 74.250
2024-12-02-11:47:53-root-INFO: grad norm: 250.816 239.561 74.293
2024-12-02-11:47:54-root-INFO: grad norm: 250.766 239.485 74.369
2024-12-02-11:47:54-root-INFO: Loss Change: 3132.329 -> 3079.142
2024-12-02-11:47:54-root-INFO: Regularization Change: 0.000 -> 0.369
2024-12-02-11:47:54-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-11:47:54-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:47:54-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-11:47:54-root-INFO: grad norm: 331.953 309.903 118.968
2024-12-02-11:47:54-root-INFO: Loss too large (3075.928->3148.133)! Learning rate decreased to 0.00132.
2024-12-02-11:47:55-root-INFO: Loss too large (3075.928->3095.410)! Learning rate decreased to 0.00105.
2024-12-02-11:47:55-root-INFO: grad norm: 349.558 332.946 106.480
2024-12-02-11:47:55-root-INFO: grad norm: 385.864 366.933 119.377
2024-12-02-11:47:56-root-INFO: grad norm: 425.810 407.179 124.579
2024-12-02-11:47:56-root-INFO: grad norm: 466.010 443.362 143.512
2024-12-02-11:47:56-root-INFO: Loss too large (3057.904->3058.608)! Learning rate decreased to 0.00084.
2024-12-02-11:47:57-root-INFO: Loss Change: 3075.928 -> 3022.168
2024-12-02-11:47:57-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-11:47:57-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-11:47:57-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-11:47:57-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-11:47:57-root-INFO: grad norm: 311.778 296.017 97.873
2024-12-02-11:47:57-root-INFO: Loss too large (3008.182->3075.441)! Learning rate decreased to 0.00138.
2024-12-02-11:47:57-root-INFO: Loss too large (3008.182->3025.143)! Learning rate decreased to 0.00110.
2024-12-02-11:47:58-root-INFO: grad norm: 351.047 334.689 105.911
2024-12-02-11:47:58-root-INFO: Loss too large (2997.602->2999.465)! Learning rate decreased to 0.00088.
2024-12-02-11:47:58-root-INFO: grad norm: 287.783 273.836 88.506
2024-12-02-11:47:59-root-INFO: grad norm: 245.517 234.367 73.148
2024-12-02-11:47:59-root-INFO: grad norm: 217.230 206.827 66.417
2024-12-02-11:48:00-root-INFO: Loss Change: 3008.182 -> 2937.552
2024-12-02-11:48:00-root-INFO: Regularization Change: 0.000 -> 0.301
2024-12-02-11:48:00-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-11:48:00-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:00-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-11:48:00-root-INFO: grad norm: 230.508 219.055 71.756
2024-12-02-11:48:00-root-INFO: Loss too large (2925.780->2969.839)! Learning rate decreased to 0.00144.
2024-12-02-11:48:00-root-INFO: Loss too large (2925.780->2939.135)! Learning rate decreased to 0.00115.
2024-12-02-11:48:01-root-INFO: grad norm: 301.592 288.176 88.951
2024-12-02-11:48:01-root-INFO: Loss too large (2922.143->2929.730)! Learning rate decreased to 0.00092.
2024-12-02-11:48:02-root-INFO: grad norm: 285.509 272.848 84.080
2024-12-02-11:48:02-root-INFO: grad norm: 275.318 263.248 80.625
2024-12-02-11:48:03-root-INFO: grad norm: 269.941 258.175 78.827
2024-12-02-11:48:03-root-INFO: Loss Change: 2925.780 -> 2880.313
2024-12-02-11:48:03-root-INFO: Regularization Change: 0.000 -> 0.283
2024-12-02-11:48:03-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-11:48:03-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:03-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-11:48:03-root-INFO: grad norm: 233.205 218.567 81.321
2024-12-02-11:48:03-root-INFO: Loss too large (2865.983->2876.669)! Learning rate decreased to 0.00150.
2024-12-02-11:48:04-root-INFO: grad norm: 361.152 346.112 103.135
2024-12-02-11:48:04-root-INFO: Loss too large (2856.181->2949.654)! Learning rate decreased to 0.00120.
2024-12-02-11:48:04-root-INFO: Loss too large (2856.181->2886.857)! Learning rate decreased to 0.00096.
2024-12-02-11:48:05-root-INFO: grad norm: 399.009 382.257 114.404
2024-12-02-11:48:05-root-INFO: grad norm: 449.815 429.531 133.556
2024-12-02-11:48:05-root-INFO: Loss too large (2850.170->2853.403)! Learning rate decreased to 0.00077.
2024-12-02-11:48:06-root-INFO: grad norm: 331.023 316.878 95.732
2024-12-02-11:48:06-root-INFO: Loss Change: 2865.983 -> 2802.812
2024-12-02-11:48:06-root-INFO: Regularization Change: 0.000 -> 0.348
2024-12-02-11:48:06-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-11:48:06-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:06-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-11:48:06-root-INFO: grad norm: 239.498 229.372 68.904
2024-12-02-11:48:06-root-INFO: Loss too large (2794.206->2875.492)! Learning rate decreased to 0.00157.
2024-12-02-11:48:07-root-INFO: Loss too large (2794.206->2829.300)! Learning rate decreased to 0.00126.
2024-12-02-11:48:07-root-INFO: Loss too large (2794.206->2802.439)! Learning rate decreased to 0.00101.
2024-12-02-11:48:07-root-INFO: grad norm: 279.665 266.990 83.240
2024-12-02-11:48:08-root-INFO: grad norm: 337.719 323.644 96.483
2024-12-02-11:48:08-root-INFO: Loss too large (2785.523->2788.981)! Learning rate decreased to 0.00081.
2024-12-02-11:48:08-root-INFO: grad norm: 278.518 266.401 81.257
2024-12-02-11:48:09-root-INFO: grad norm: 237.020 227.538 66.370
2024-12-02-11:48:09-root-INFO: Loss Change: 2794.206 -> 2746.096
2024-12-02-11:48:09-root-INFO: Regularization Change: 0.000 -> 0.228
2024-12-02-11:48:09-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-11:48:09-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:09-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-11:48:09-root-INFO: grad norm: 146.509 136.408 53.457
2024-12-02-11:48:10-root-INFO: grad norm: 232.749 224.367 61.897
2024-12-02-11:48:10-root-INFO: Loss too large (2708.257->2809.075)! Learning rate decreased to 0.00165.
2024-12-02-11:48:10-root-INFO: Loss too large (2708.257->2756.467)! Learning rate decreased to 0.00132.
2024-12-02-11:48:10-root-INFO: Loss too large (2708.257->2725.237)! Learning rate decreased to 0.00105.
2024-12-02-11:48:11-root-INFO: grad norm: 324.656 311.575 91.230
2024-12-02-11:48:11-root-INFO: Loss too large (2707.740->2725.450)! Learning rate decreased to 0.00084.
2024-12-02-11:48:11-root-INFO: grad norm: 335.904 321.983 95.698
2024-12-02-11:48:12-root-INFO: grad norm: 350.005 336.256 97.138
2024-12-02-11:48:12-root-INFO: Loss Change: 2726.217 -> 2690.940
2024-12-02-11:48:12-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-02-11:48:12-root-INFO: Undo step: 200
2024-12-02-11:48:12-root-INFO: Undo step: 201
2024-12-02-11:48:12-root-INFO: Undo step: 202
2024-12-02-11:48:12-root-INFO: Undo step: 203
2024-12-02-11:48:12-root-INFO: Undo step: 204
2024-12-02-11:48:12-root-INFO: Undo step: 205
2024-12-02-11:48:12-root-INFO: Undo step: 206
2024-12-02-11:48:12-root-INFO: Undo step: 207
2024-12-02-11:48:12-root-INFO: Undo step: 208
2024-12-02-11:48:12-root-INFO: Undo step: 209
2024-12-02-11:48:12-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-11:48:13-root-INFO: grad norm: 4177.672 3755.913 1829.224
2024-12-02-11:48:13-root-INFO: grad norm: 2091.234 1928.240 809.413
2024-12-02-11:48:14-root-INFO: grad norm: 1233.809 1147.182 454.158
2024-12-02-11:48:14-root-INFO: grad norm: 1246.685 1165.193 443.337
2024-12-02-11:48:14-root-INFO: Loss too large (4284.531->4355.517)! Learning rate decreased to 0.00104.
2024-12-02-11:48:15-root-INFO: grad norm: 1045.247 991.879 329.724
2024-12-02-11:48:15-root-INFO: Loss Change: 9179.230 -> 3914.999
2024-12-02-11:48:15-root-INFO: Regularization Change: 0.000 -> 21.489
2024-12-02-11:48:15-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-11:48:15-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:48:15-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-11:48:15-root-INFO: grad norm: 1050.841 973.538 395.590
2024-12-02-11:48:15-root-INFO: Loss too large (3925.971->4137.018)! Learning rate decreased to 0.00109.
2024-12-02-11:48:16-root-INFO: grad norm: 986.548 937.541 307.072
2024-12-02-11:48:16-root-INFO: grad norm: 946.441 881.813 343.739
2024-12-02-11:48:17-root-INFO: grad norm: 905.954 858.114 290.505
2024-12-02-11:48:17-root-INFO: grad norm: 873.481 815.514 312.900
2024-12-02-11:48:18-root-INFO: Loss Change: 3925.971 -> 3595.978
2024-12-02-11:48:18-root-INFO: Regularization Change: 0.000 -> 2.222
2024-12-02-11:48:18-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-11:48:18-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:48:18-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-11:48:18-root-INFO: grad norm: 761.549 727.956 223.689
2024-12-02-11:48:18-root-INFO: Loss too large (3545.987->3677.483)! Learning rate decreased to 0.00114.
2024-12-02-11:48:19-root-INFO: grad norm: 724.278 679.336 251.160
2024-12-02-11:48:19-root-INFO: grad norm: 696.681 659.645 224.127
2024-12-02-11:48:20-root-INFO: grad norm: 671.890 630.611 231.875
2024-12-02-11:48:20-root-INFO: grad norm: 649.708 613.242 214.603
2024-12-02-11:48:20-root-INFO: Loss Change: 3545.987 -> 3367.793
2024-12-02-11:48:20-root-INFO: Regularization Change: 0.000 -> 1.348
2024-12-02-11:48:20-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-11:48:20-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:48:20-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-11:48:21-root-INFO: grad norm: 641.285 596.511 235.417
2024-12-02-11:48:21-root-INFO: Loss too large (3348.230->3429.470)! Learning rate decreased to 0.00120.
2024-12-02-11:48:21-root-INFO: grad norm: 584.451 552.454 190.728
2024-12-02-11:48:22-root-INFO: grad norm: 546.737 515.171 183.087
2024-12-02-11:48:22-root-INFO: grad norm: 514.761 488.328 162.834
2024-12-02-11:48:23-root-INFO: grad norm: 491.280 466.169 155.057
2024-12-02-11:48:23-root-INFO: Loss Change: 3348.230 -> 3188.008
2024-12-02-11:48:23-root-INFO: Regularization Change: 0.000 -> 1.039
2024-12-02-11:48:23-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-11:48:23-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:48:23-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-11:48:23-root-INFO: grad norm: 434.318 419.391 112.884
2024-12-02-11:48:23-root-INFO: Loss too large (3149.753->3187.312)! Learning rate decreased to 0.00126.
2024-12-02-11:48:24-root-INFO: grad norm: 419.165 403.444 113.719
2024-12-02-11:48:24-root-INFO: grad norm: 409.710 394.744 109.724
2024-12-02-11:48:25-root-INFO: grad norm: 402.702 388.323 106.650
2024-12-02-11:48:25-root-INFO: grad norm: 398.051 383.515 106.587
2024-12-02-11:48:26-root-INFO: Loss Change: 3149.753 -> 3056.016
2024-12-02-11:48:26-root-INFO: Regularization Change: 0.000 -> 0.772
2024-12-02-11:48:26-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-11:48:26-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:48:26-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-11:48:26-root-INFO: grad norm: 406.126 387.000 123.165
2024-12-02-11:48:26-root-INFO: Loss too large (3039.534->3075.469)! Learning rate decreased to 0.00132.
2024-12-02-11:48:27-root-INFO: grad norm: 398.055 383.408 106.986
2024-12-02-11:48:27-root-INFO: grad norm: 407.131 393.791 103.366
2024-12-02-11:48:28-root-INFO: grad norm: 423.986 410.288 106.900
2024-12-02-11:48:28-root-INFO: grad norm: 444.020 430.907 107.113
2024-12-02-11:48:28-root-INFO: Loss Change: 3039.534 -> 2979.941
2024-12-02-11:48:28-root-INFO: Regularization Change: 0.000 -> 0.785
2024-12-02-11:48:28-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-11:48:28-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-11:48:29-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-11:48:29-root-INFO: grad norm: 509.945 492.075 133.813
2024-12-02-11:48:29-root-INFO: Loss too large (2973.468->3072.503)! Learning rate decreased to 0.00138.
2024-12-02-11:48:29-root-INFO: grad norm: 535.751 520.071 128.669
2024-12-02-11:48:30-root-INFO: grad norm: 550.086 533.167 135.377
2024-12-02-11:48:30-root-INFO: grad norm: 547.103 530.837 132.414
2024-12-02-11:48:31-root-INFO: grad norm: 526.915 510.770 129.436
2024-12-02-11:48:31-root-INFO: Loss Change: 2973.468 -> 2909.134
2024-12-02-11:48:31-root-INFO: Regularization Change: 0.000 -> 0.971
2024-12-02-11:48:31-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-11:48:31-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:31-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-11:48:31-root-INFO: grad norm: 486.077 471.339 118.787
2024-12-02-11:48:31-root-INFO: Loss too large (2888.006->2959.643)! Learning rate decreased to 0.00144.
2024-12-02-11:48:32-root-INFO: grad norm: 462.271 449.557 107.673
2024-12-02-11:48:32-root-INFO: grad norm: 436.612 423.791 105.027
2024-12-02-11:48:33-root-INFO: grad norm: 413.076 401.653 96.471
2024-12-02-11:48:33-root-INFO: grad norm: 391.781 380.101 94.948
2024-12-02-11:48:34-root-INFO: Loss Change: 2888.006 -> 2791.215
2024-12-02-11:48:34-root-INFO: Regularization Change: 0.000 -> 0.827
2024-12-02-11:48:34-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-11:48:34-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:34-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-11:48:34-root-INFO: grad norm: 503.534 476.895 161.610
2024-12-02-11:48:34-root-INFO: Loss too large (2813.291->2893.661)! Learning rate decreased to 0.00150.
2024-12-02-11:48:35-root-INFO: grad norm: 486.731 470.282 125.467
2024-12-02-11:48:35-root-INFO: grad norm: 472.547 456.439 122.329
2024-12-02-11:48:36-root-INFO: grad norm: 459.302 443.622 118.986
2024-12-02-11:48:36-root-INFO: grad norm: 442.075 427.755 111.605
2024-12-02-11:48:37-root-INFO: Loss Change: 2813.291 -> 2724.702
2024-12-02-11:48:37-root-INFO: Regularization Change: 0.000 -> 0.985
2024-12-02-11:48:37-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-11:48:37-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:37-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-11:48:37-root-INFO: grad norm: 431.423 415.859 114.834
2024-12-02-11:48:37-root-INFO: Loss too large (2719.070->2783.052)! Learning rate decreased to 0.00157.
2024-12-02-11:48:38-root-INFO: grad norm: 422.190 409.511 102.691
2024-12-02-11:48:38-root-INFO: grad norm: 414.977 400.847 107.370
2024-12-02-11:48:38-root-INFO: grad norm: 409.166 396.581 100.701
2024-12-02-11:48:39-root-INFO: grad norm: 404.381 390.894 103.566
2024-12-02-11:48:39-root-INFO: Loss Change: 2719.070 -> 2650.414
2024-12-02-11:48:39-root-INFO: Regularization Change: 0.000 -> 0.855
2024-12-02-11:48:39-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-11:48:39-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:39-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-11:48:40-root-INFO: grad norm: 513.206 489.329 154.718
2024-12-02-11:48:40-root-INFO: Loss too large (2671.224->2786.027)! Learning rate decreased to 0.00165.
2024-12-02-11:48:40-root-INFO: grad norm: 507.107 489.031 134.188
2024-12-02-11:48:41-root-INFO: grad norm: 492.220 474.401 131.243
2024-12-02-11:48:41-root-INFO: grad norm: 473.898 456.398 127.595
2024-12-02-11:48:42-root-INFO: grad norm: 450.506 434.116 120.413
2024-12-02-11:48:42-root-INFO: Loss Change: 2671.224 -> 2596.816
2024-12-02-11:48:42-root-INFO: Regularization Change: 0.000 -> 1.035
2024-12-02-11:48:42-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-11:48:42-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:48:42-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-11:48:42-root-INFO: grad norm: 390.181 376.724 101.590
2024-12-02-11:48:42-root-INFO: Loss too large (2562.892->2622.386)! Learning rate decreased to 0.00172.
2024-12-02-11:48:43-root-INFO: grad norm: 391.800 378.030 102.962
2024-12-02-11:48:43-root-INFO: grad norm: 400.764 386.170 107.168
2024-12-02-11:48:44-root-INFO: grad norm: 415.944 401.140 109.982
2024-12-02-11:48:44-root-INFO: Loss too large (2533.983->2535.230)! Learning rate decreased to 0.00138.
2024-12-02-11:48:45-root-INFO: grad norm: 280.850 270.003 77.299
2024-12-02-11:48:45-root-INFO: Loss Change: 2562.892 -> 2461.729
2024-12-02-11:48:45-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-11:48:45-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-11:48:45-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-11:48:45-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-11:48:45-root-INFO: grad norm: 283.492 268.090 92.170
2024-12-02-11:48:45-root-INFO: Loss too large (2467.129->2508.392)! Learning rate decreased to 0.00180.
2024-12-02-11:48:46-root-INFO: grad norm: 326.221 313.845 89.003
2024-12-02-11:48:46-root-INFO: Loss too large (2465.949->2474.002)! Learning rate decreased to 0.00144.
2024-12-02-11:48:46-root-INFO: grad norm: 262.530 253.704 67.500
2024-12-02-11:48:47-root-INFO: grad norm: 227.298 219.177 60.213
2024-12-02-11:48:47-root-INFO: grad norm: 203.608 197.246 50.503
2024-12-02-11:48:48-root-INFO: Loss Change: 2467.129 -> 2401.222
2024-12-02-11:48:48-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-11:48:48-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-11:48:48-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:48:48-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-11:48:48-root-INFO: grad norm: 146.590 141.051 39.918
2024-12-02-11:48:48-root-INFO: Loss too large (2375.530->2376.160)! Learning rate decreased to 0.00188.
2024-12-02-11:48:49-root-INFO: grad norm: 187.143 181.623 45.118
2024-12-02-11:48:49-root-INFO: Loss too large (2367.603->2372.855)! Learning rate decreased to 0.00150.
2024-12-02-11:48:49-root-INFO: grad norm: 195.211 189.564 46.612
2024-12-02-11:48:50-root-INFO: grad norm: 212.446 206.496 49.926
2024-12-02-11:48:50-root-INFO: grad norm: 235.683 229.399 54.064
2024-12-02-11:48:51-root-INFO: Loss Change: 2375.530 -> 2347.965
2024-12-02-11:48:51-root-INFO: Regularization Change: 0.000 -> 0.413
2024-12-02-11:48:51-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-11:48:51-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:48:51-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-11:48:51-root-INFO: grad norm: 369.007 353.814 104.796
2024-12-02-11:48:51-root-INFO: Loss too large (2354.478->2542.176)! Learning rate decreased to 0.00196.
2024-12-02-11:48:51-root-INFO: Loss too large (2354.478->2434.063)! Learning rate decreased to 0.00157.
2024-12-02-11:48:51-root-INFO: Loss too large (2354.478->2367.943)! Learning rate decreased to 0.00126.
2024-12-02-11:48:52-root-INFO: grad norm: 293.212 285.935 64.921
2024-12-02-11:48:52-root-INFO: grad norm: 236.883 230.328 55.340
2024-12-02-11:48:53-root-INFO: grad norm: 203.037 197.656 46.432
2024-12-02-11:48:53-root-INFO: grad norm: 177.958 173.126 41.189
2024-12-02-11:48:54-root-INFO: Loss Change: 2354.478 -> 2282.432
2024-12-02-11:48:54-root-INFO: Regularization Change: 0.000 -> 0.328
2024-12-02-11:48:54-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-11:48:54-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:48:54-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-11:48:54-root-INFO: grad norm: 130.918 125.579 37.006
2024-12-02-11:48:54-root-INFO: Loss too large (2269.608->2282.122)! Learning rate decreased to 0.00205.
2024-12-02-11:48:54-root-INFO: Loss too large (2269.608->2270.307)! Learning rate decreased to 0.00164.
2024-12-02-11:48:55-root-INFO: grad norm: 168.213 163.599 39.129
2024-12-02-11:48:55-root-INFO: Loss too large (2264.270->2265.569)! Learning rate decreased to 0.00131.
2024-12-02-11:48:55-root-INFO: grad norm: 165.023 160.364 38.934
2024-12-02-11:48:56-root-INFO: grad norm: 165.852 161.571 37.438
2024-12-02-11:48:56-root-INFO: grad norm: 169.851 165.334 38.913
2024-12-02-11:48:57-root-INFO: Loss Change: 2269.608 -> 2240.357
2024-12-02-11:48:57-root-INFO: Regularization Change: 0.000 -> 0.259
2024-12-02-11:48:57-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-11:48:57-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:48:57-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-11:48:57-root-INFO: grad norm: 276.155 265.142 77.210
2024-12-02-11:48:57-root-INFO: Loss too large (2239.708->2399.524)! Learning rate decreased to 0.00214.
2024-12-02-11:48:57-root-INFO: Loss too large (2239.708->2318.125)! Learning rate decreased to 0.00171.
2024-12-02-11:48:57-root-INFO: Loss too large (2239.708->2267.564)! Learning rate decreased to 0.00137.
2024-12-02-11:48:58-root-INFO: grad norm: 299.324 293.126 60.597
2024-12-02-11:48:58-root-INFO: grad norm: 336.592 328.681 72.549
2024-12-02-11:48:58-root-INFO: Loss too large (2236.620->2243.292)! Learning rate decreased to 0.00110.
2024-12-02-11:48:59-root-INFO: grad norm: 247.707 242.117 52.330
2024-12-02-11:48:59-root-INFO: grad norm: 181.950 177.597 39.560
2024-12-02-11:49:00-root-INFO: Loss Change: 2239.708 -> 2193.681
2024-12-02-11:49:00-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-02-11:49:00-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-11:49:00-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:49:00-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-11:49:00-root-INFO: grad norm: 110.862 105.582 33.806
2024-12-02-11:49:00-root-INFO: grad norm: 178.983 176.321 30.754
2024-12-02-11:49:01-root-INFO: Loss too large (2167.519->2235.383)! Learning rate decreased to 0.00224.
2024-12-02-11:49:01-root-INFO: Loss too large (2167.519->2198.412)! Learning rate decreased to 0.00179.
2024-12-02-11:49:01-root-INFO: Loss too large (2167.519->2176.595)! Learning rate decreased to 0.00143.
2024-12-02-11:49:01-root-INFO: grad norm: 211.493 206.618 45.145
2024-12-02-11:49:02-root-INFO: Loss too large (2164.479->2166.315)! Learning rate decreased to 0.00114.
2024-12-02-11:49:02-root-INFO: grad norm: 180.541 176.433 38.294
2024-12-02-11:49:02-root-INFO: grad norm: 159.760 155.954 34.665
2024-12-02-11:49:03-root-INFO: Loss Change: 2176.081 -> 2141.752
2024-12-02-11:49:03-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-02-11:49:03-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-11:49:03-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-11:49:03-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-11:49:03-root-INFO: grad norm: 230.670 223.857 55.645
2024-12-02-11:49:03-root-INFO: Loss too large (2136.041->2286.549)! Learning rate decreased to 0.00233.
2024-12-02-11:49:03-root-INFO: Loss too large (2136.041->2214.813)! Learning rate decreased to 0.00187.
2024-12-02-11:49:04-root-INFO: Loss too large (2136.041->2169.797)! Learning rate decreased to 0.00149.
2024-12-02-11:49:04-root-INFO: Loss too large (2136.041->2143.114)! Learning rate decreased to 0.00119.
2024-12-02-11:49:04-root-INFO: grad norm: 210.509 206.506 40.859
2024-12-02-11:49:05-root-INFO: grad norm: 194.254 189.771 41.491
2024-12-02-11:49:05-root-INFO: grad norm: 182.624 178.779 37.274
2024-12-02-11:49:06-root-INFO: grad norm: 172.570 168.699 36.346
2024-12-02-11:49:06-root-INFO: Loss Change: 2136.041 -> 2103.157
2024-12-02-11:49:06-root-INFO: Regularization Change: 0.000 -> 0.203
2024-12-02-11:49:06-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-11:49:06-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:49:06-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-11:49:06-root-INFO: grad norm: 149.207 143.127 42.160
2024-12-02-11:49:07-root-INFO: grad norm: 247.762 244.714 38.743
2024-12-02-11:49:07-root-INFO: Loss too large (2092.754->2250.731)! Learning rate decreased to 0.00244.
2024-12-02-11:49:07-root-INFO: Loss too large (2092.754->2172.228)! Learning rate decreased to 0.00195.
2024-12-02-11:49:07-root-INFO: Loss too large (2092.754->2123.592)! Learning rate decreased to 0.00156.
2024-12-02-11:49:07-root-INFO: Loss too large (2092.754->2095.115)! Learning rate decreased to 0.00125.
2024-12-02-11:49:08-root-INFO: grad norm: 234.261 228.601 51.183
2024-12-02-11:49:08-root-INFO: grad norm: 237.878 233.438 45.746
2024-12-02-11:49:09-root-INFO: grad norm: 243.354 238.391 48.899
2024-12-02-11:49:09-root-INFO: Loss Change: 2092.918 -> 2064.785
2024-12-02-11:49:09-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-11:49:09-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-11:49:09-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:49:09-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-11:49:09-root-INFO: grad norm: 359.278 350.239 80.085
2024-12-02-11:49:10-root-INFO: Loss too large (2075.146->2485.050)! Learning rate decreased to 0.00254.
2024-12-02-11:49:10-root-INFO: Loss too large (2075.146->2314.465)! Learning rate decreased to 0.00203.
2024-12-02-11:49:10-root-INFO: Loss too large (2075.146->2196.750)! Learning rate decreased to 0.00163.
2024-12-02-11:49:10-root-INFO: Loss too large (2075.146->2120.729)! Learning rate decreased to 0.00130.
2024-12-02-11:49:10-root-INFO: grad norm: 354.697 348.594 65.510
2024-12-02-11:49:11-root-INFO: grad norm: 349.471 342.695 68.484
2024-12-02-11:49:11-root-INFO: grad norm: 341.380 335.095 65.205
2024-12-02-11:49:12-root-INFO: grad norm: 327.742 321.435 63.991
2024-12-02-11:49:12-root-INFO: Loss Change: 2075.146 -> 2045.895
2024-12-02-11:49:12-root-INFO: Regularization Change: 0.000 -> 0.362
2024-12-02-11:49:12-root-INFO: Undo step: 190
2024-12-02-11:49:12-root-INFO: Undo step: 191
2024-12-02-11:49:12-root-INFO: Undo step: 192
2024-12-02-11:49:12-root-INFO: Undo step: 193
2024-12-02-11:49:12-root-INFO: Undo step: 194
2024-12-02-11:49:12-root-INFO: Undo step: 195
2024-12-02-11:49:12-root-INFO: Undo step: 196
2024-12-02-11:49:12-root-INFO: Undo step: 197
2024-12-02-11:49:12-root-INFO: Undo step: 198
2024-12-02-11:49:12-root-INFO: Undo step: 199
2024-12-02-11:49:12-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-11:49:12-root-INFO: grad norm: 1295.707 1115.506 659.167
2024-12-02-11:49:13-root-INFO: grad norm: 1214.747 1104.187 506.343
2024-12-02-11:49:13-root-INFO: Loss too large (3668.029->4000.743)! Learning rate decreased to 0.00165.
2024-12-02-11:49:14-root-INFO: grad norm: 949.674 897.385 310.775
2024-12-02-11:49:14-root-INFO: grad norm: 702.319 670.371 209.413
2024-12-02-11:49:15-root-INFO: grad norm: 642.733 609.461 204.115
2024-12-02-11:49:15-root-INFO: Loss Change: 4966.007 -> 3017.940
2024-12-02-11:49:15-root-INFO: Regularization Change: 0.000 -> 12.045
2024-12-02-11:49:15-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-11:49:15-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:49:15-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-11:49:15-root-INFO: grad norm: 622.334 587.874 204.214
2024-12-02-11:49:15-root-INFO: Loss too large (3002.800->3117.665)! Learning rate decreased to 0.00172.
2024-12-02-11:49:16-root-INFO: grad norm: 599.185 565.967 196.735
2024-12-02-11:49:16-root-INFO: grad norm: 593.506 559.069 199.227
2024-12-02-11:49:17-root-INFO: grad norm: 594.336 560.476 197.742
2024-12-02-11:49:17-root-INFO: grad norm: 611.158 575.393 206.002
2024-12-02-11:49:17-root-INFO: Loss too large (2821.325->2827.822)! Learning rate decreased to 0.00138.
2024-12-02-11:49:18-root-INFO: Loss Change: 3002.800 -> 2730.779
2024-12-02-11:49:18-root-INFO: Regularization Change: 0.000 -> 2.292
2024-12-02-11:49:18-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-11:49:18-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-11:49:18-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-11:49:18-root-INFO: grad norm: 373.251 353.044 121.144
2024-12-02-11:49:18-root-INFO: Loss too large (2698.841->2739.783)! Learning rate decreased to 0.00180.
2024-12-02-11:49:19-root-INFO: grad norm: 406.976 384.903 132.208
2024-12-02-11:49:19-root-INFO: Loss too large (2670.274->2673.178)! Learning rate decreased to 0.00144.
2024-12-02-11:49:19-root-INFO: grad norm: 313.563 296.067 103.278
2024-12-02-11:49:20-root-INFO: grad norm: 245.160 232.675 77.236
2024-12-02-11:49:20-root-INFO: grad norm: 210.974 199.313 69.169
2024-12-02-11:49:20-root-INFO: Loss Change: 2698.841 -> 2544.763
2024-12-02-11:49:20-root-INFO: Regularization Change: 0.000 -> 0.967
2024-12-02-11:49:20-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-11:49:20-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:49:21-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-11:49:21-root-INFO: grad norm: 246.566 233.773 78.389
2024-12-02-11:49:21-root-INFO: Loss too large (2522.718->2553.201)! Learning rate decreased to 0.00188.
2024-12-02-11:49:21-root-INFO: grad norm: 314.726 301.006 91.914
2024-12-02-11:49:21-root-INFO: Loss too large (2521.291->2532.849)! Learning rate decreased to 0.00150.
2024-12-02-11:49:22-root-INFO: grad norm: 293.402 281.529 82.623
2024-12-02-11:49:22-root-INFO: grad norm: 283.010 271.537 79.765
2024-12-02-11:49:23-root-INFO: grad norm: 276.443 265.852 75.787
2024-12-02-11:49:23-root-INFO: Loss Change: 2522.718 -> 2456.468
2024-12-02-11:49:23-root-INFO: Regularization Change: 0.000 -> 0.723
2024-12-02-11:49:23-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-11:49:23-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:49:23-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-11:49:24-root-INFO: grad norm: 234.809 226.714 61.123
2024-12-02-11:49:24-root-INFO: Loss too large (2428.646->2470.976)! Learning rate decreased to 0.00196.
2024-12-02-11:49:24-root-INFO: Loss too large (2428.646->2431.757)! Learning rate decreased to 0.00157.
2024-12-02-11:49:24-root-INFO: grad norm: 244.932 236.457 63.872
2024-12-02-11:49:25-root-INFO: grad norm: 266.609 256.985 70.986
2024-12-02-11:49:25-root-INFO: grad norm: 303.898 294.137 76.401
2024-12-02-11:49:25-root-INFO: Loss too large (2397.172->2400.699)! Learning rate decreased to 0.00126.
2024-12-02-11:49:26-root-INFO: grad norm: 235.722 227.183 62.869
2024-12-02-11:49:26-root-INFO: Loss Change: 2428.646 -> 2358.810
2024-12-02-11:49:26-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-11:49:26-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-11:49:26-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:49:26-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-11:49:27-root-INFO: grad norm: 216.455 209.151 55.755
2024-12-02-11:49:27-root-INFO: Loss too large (2349.896->2401.137)! Learning rate decreased to 0.00205.
2024-12-02-11:49:27-root-INFO: Loss too large (2349.896->2365.594)! Learning rate decreased to 0.00164.
2024-12-02-11:49:27-root-INFO: grad norm: 250.908 242.278 65.242
2024-12-02-11:49:28-root-INFO: grad norm: 309.737 300.731 74.146
2024-12-02-11:49:28-root-INFO: Loss too large (2342.690->2354.557)! Learning rate decreased to 0.00131.
2024-12-02-11:49:28-root-INFO: grad norm: 256.981 248.587 65.143
2024-12-02-11:49:29-root-INFO: grad norm: 210.297 204.057 50.847
2024-12-02-11:49:29-root-INFO: Loss Change: 2349.896 -> 2297.178
2024-12-02-11:49:29-root-INFO: Regularization Change: 0.000 -> 0.416
2024-12-02-11:49:29-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-11:49:29-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:49:29-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-11:49:30-root-INFO: grad norm: 149.233 143.703 40.248
2024-12-02-11:49:30-root-INFO: Loss too large (2276.174->2286.468)! Learning rate decreased to 0.00214.
2024-12-02-11:49:30-root-INFO: grad norm: 247.318 241.330 54.094
2024-12-02-11:49:30-root-INFO: Loss too large (2272.533->2320.236)! Learning rate decreased to 0.00171.
2024-12-02-11:49:30-root-INFO: Loss too large (2272.533->2284.440)! Learning rate decreased to 0.00137.
2024-12-02-11:49:31-root-INFO: grad norm: 237.433 230.242 57.991
2024-12-02-11:49:32-root-INFO: grad norm: 230.816 224.917 51.849
2024-12-02-11:49:32-root-INFO: grad norm: 228.682 221.943 55.109
2024-12-02-11:49:32-root-INFO: Loss Change: 2276.174 -> 2237.097
2024-12-02-11:49:32-root-INFO: Regularization Change: 0.000 -> 0.415
2024-12-02-11:49:32-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-11:49:32-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:49:33-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-11:49:33-root-INFO: grad norm: 301.381 291.036 78.284
2024-12-02-11:49:33-root-INFO: Loss too large (2229.583->2400.274)! Learning rate decreased to 0.00224.
2024-12-02-11:49:33-root-INFO: Loss too large (2229.583->2310.373)! Learning rate decreased to 0.00179.
2024-12-02-11:49:33-root-INFO: Loss too large (2229.583->2254.427)! Learning rate decreased to 0.00143.
2024-12-02-11:49:34-root-INFO: grad norm: 293.325 285.589 66.922
2024-12-02-11:49:34-root-INFO: grad norm: 293.040 285.954 64.051
2024-12-02-11:49:35-root-INFO: grad norm: 293.968 286.261 66.869
2024-12-02-11:49:35-root-INFO: grad norm: 296.245 289.283 63.848
2024-12-02-11:49:35-root-INFO: Loss Change: 2229.583 -> 2192.636
2024-12-02-11:49:35-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-02-11:49:35-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-11:49:35-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-11:49:35-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-11:49:36-root-INFO: grad norm: 250.278 244.324 54.269
2024-12-02-11:49:36-root-INFO: Loss too large (2160.833->2313.332)! Learning rate decreased to 0.00233.
2024-12-02-11:49:36-root-INFO: Loss too large (2160.833->2226.728)! Learning rate decreased to 0.00187.
2024-12-02-11:49:36-root-INFO: Loss too large (2160.833->2177.581)! Learning rate decreased to 0.00149.
2024-12-02-11:49:37-root-INFO: grad norm: 261.145 254.925 56.659
2024-12-02-11:49:37-root-INFO: grad norm: 273.551 267.017 59.433
2024-12-02-11:49:38-root-INFO: grad norm: 291.298 284.887 60.774
2024-12-02-11:49:38-root-INFO: Loss too large (2141.968->2143.061)! Learning rate decreased to 0.00119.
2024-12-02-11:49:38-root-INFO: grad norm: 206.917 201.707 46.138
2024-12-02-11:49:39-root-INFO: Loss Change: 2160.833 -> 2108.993
2024-12-02-11:49:39-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-02-11:49:39-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-11:49:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:49:39-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-11:49:39-root-INFO: grad norm: 248.390 238.101 70.748
2024-12-02-11:49:39-root-INFO: Loss too large (2111.442->2221.434)! Learning rate decreased to 0.00244.
2024-12-02-11:49:39-root-INFO: Loss too large (2111.442->2159.246)! Learning rate decreased to 0.00195.
2024-12-02-11:49:40-root-INFO: Loss too large (2111.442->2121.588)! Learning rate decreased to 0.00156.
2024-12-02-11:49:40-root-INFO: grad norm: 246.698 240.482 55.026
2024-12-02-11:49:41-root-INFO: grad norm: 282.576 276.700 57.327
2024-12-02-11:49:41-root-INFO: Loss too large (2095.304->2100.644)! Learning rate decreased to 0.00125.
2024-12-02-11:49:41-root-INFO: grad norm: 217.925 212.350 48.979
2024-12-02-11:49:42-root-INFO: grad norm: 165.227 161.612 34.376
2024-12-02-11:49:42-root-INFO: Loss Change: 2111.442 -> 2056.878
2024-12-02-11:49:42-root-INFO: Regularization Change: 0.000 -> 0.325
2024-12-02-11:49:42-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-11:49:42-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:49:42-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-11:49:43-root-INFO: grad norm: 109.191 104.831 30.548
2024-12-02-11:49:43-root-INFO: grad norm: 206.999 203.555 37.602
2024-12-02-11:49:43-root-INFO: Loss too large (2035.507->2140.704)! Learning rate decreased to 0.00254.
2024-12-02-11:49:43-root-INFO: Loss too large (2035.507->2086.347)! Learning rate decreased to 0.00203.
2024-12-02-11:49:44-root-INFO: Loss too large (2035.507->2053.052)! Learning rate decreased to 0.00163.
2024-12-02-11:49:44-root-INFO: grad norm: 241.981 236.480 51.302
2024-12-02-11:49:44-root-INFO: Loss too large (2033.886->2037.105)! Learning rate decreased to 0.00130.
2024-12-02-11:49:45-root-INFO: grad norm: 201.694 197.836 39.260
2024-12-02-11:49:45-root-INFO: grad norm: 176.823 172.565 38.570
2024-12-02-11:49:46-root-INFO: Loss Change: 2040.128 -> 2003.070
2024-12-02-11:49:46-root-INFO: Regularization Change: 0.000 -> 0.370
2024-12-02-11:49:46-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-11:49:46-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:49:46-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-11:49:46-root-INFO: grad norm: 211.983 206.323 48.657
2024-12-02-11:49:46-root-INFO: Loss too large (1999.056->2116.628)! Learning rate decreased to 0.00265.
2024-12-02-11:49:46-root-INFO: Loss too large (1999.056->2057.260)! Learning rate decreased to 0.00212.
2024-12-02-11:49:46-root-INFO: Loss too large (1999.056->2020.494)! Learning rate decreased to 0.00170.
2024-12-02-11:49:46-root-INFO: Loss too large (1999.056->1999.061)! Learning rate decreased to 0.00136.
2024-12-02-11:49:47-root-INFO: grad norm: 173.370 169.295 37.365
2024-12-02-11:49:47-root-INFO: grad norm: 150.408 147.339 30.226
2024-12-02-11:49:48-root-INFO: grad norm: 135.788 132.124 31.331
2024-12-02-11:49:48-root-INFO: grad norm: 123.113 120.459 25.427
2024-12-02-11:49:49-root-INFO: Loss Change: 1999.056 -> 1958.732
2024-12-02-11:49:49-root-INFO: Regularization Change: 0.000 -> 0.241
2024-12-02-11:49:49-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-11:49:49-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:49:49-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-11:49:49-root-INFO: grad norm: 117.336 113.292 30.543
2024-12-02-11:49:49-root-INFO: Loss too large (1941.315->1941.875)! Learning rate decreased to 0.00277.
2024-12-02-11:49:50-root-INFO: grad norm: 142.360 139.115 30.223
2024-12-02-11:49:50-root-INFO: Loss too large (1935.601->1938.854)! Learning rate decreased to 0.00222.
2024-12-02-11:49:50-root-INFO: grad norm: 176.498 172.546 37.142
2024-12-02-11:49:51-root-INFO: Loss too large (1927.799->1944.650)! Learning rate decreased to 0.00177.
2024-12-02-11:49:51-root-INFO: grad norm: 226.972 222.832 43.154
2024-12-02-11:49:51-root-INFO: Loss too large (1926.833->1937.349)! Learning rate decreased to 0.00142.
2024-12-02-11:49:52-root-INFO: grad norm: 204.403 200.184 41.314
2024-12-02-11:49:52-root-INFO: Loss Change: 1941.315 -> 1910.319
2024-12-02-11:49:52-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-11:49:52-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-11:49:52-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-11:49:52-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-11:49:52-root-INFO: grad norm: 212.470 206.508 49.983
2024-12-02-11:49:52-root-INFO: Loss too large (1909.589->2047.514)! Learning rate decreased to 0.00289.
2024-12-02-11:49:52-root-INFO: Loss too large (1909.589->1980.128)! Learning rate decreased to 0.00231.
2024-12-02-11:49:53-root-INFO: Loss too large (1909.589->1937.932)! Learning rate decreased to 0.00185.
2024-12-02-11:49:53-root-INFO: Loss too large (1909.589->1912.974)! Learning rate decreased to 0.00148.
2024-12-02-11:49:53-root-INFO: grad norm: 184.234 179.978 39.367
2024-12-02-11:49:54-root-INFO: grad norm: 164.190 161.114 31.629
2024-12-02-11:49:54-root-INFO: grad norm: 151.527 147.811 33.351
2024-12-02-11:49:55-root-INFO: grad norm: 139.702 137.120 26.738
2024-12-02-11:49:55-root-INFO: Loss Change: 1909.589 -> 1870.473
2024-12-02-11:49:55-root-INFO: Regularization Change: 0.000 -> 0.274
2024-12-02-11:49:55-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-11:49:55-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:49:55-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-11:49:55-root-INFO: grad norm: 138.703 134.592 33.520
2024-12-02-11:49:55-root-INFO: Loss too large (1853.635->1891.695)! Learning rate decreased to 0.00301.
2024-12-02-11:49:56-root-INFO: Loss too large (1853.635->1868.972)! Learning rate decreased to 0.00241.
2024-12-02-11:49:56-root-INFO: Loss too large (1853.635->1855.554)! Learning rate decreased to 0.00193.
2024-12-02-11:49:56-root-INFO: grad norm: 146.162 143.100 29.763
2024-12-02-11:49:57-root-INFO: grad norm: 167.336 163.179 37.064
2024-12-02-11:49:57-root-INFO: Loss too large (1843.526->1844.013)! Learning rate decreased to 0.00154.
2024-12-02-11:49:57-root-INFO: grad norm: 146.306 143.403 29.000
2024-12-02-11:49:58-root-INFO: grad norm: 133.305 129.710 30.749
2024-12-02-11:49:58-root-INFO: Loss Change: 1853.635 -> 1822.340
2024-12-02-11:49:58-root-INFO: Regularization Change: 0.000 -> 0.279
2024-12-02-11:49:58-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-11:49:58-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:49:58-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-11:49:58-root-INFO: grad norm: 146.009 142.868 30.122
2024-12-02-11:49:59-root-INFO: Loss too large (1812.552->1880.611)! Learning rate decreased to 0.00314.
2024-12-02-11:49:59-root-INFO: Loss too large (1812.552->1845.500)! Learning rate decreased to 0.00251.
2024-12-02-11:49:59-root-INFO: Loss too large (1812.552->1824.265)! Learning rate decreased to 0.00201.
2024-12-02-11:49:59-root-INFO: grad norm: 183.011 179.130 37.490
2024-12-02-11:49:59-root-INFO: Loss too large (1812.117->1814.638)! Learning rate decreased to 0.00161.
2024-12-02-11:50:00-root-INFO: grad norm: 160.738 158.003 29.523
2024-12-02-11:50:00-root-INFO: grad norm: 145.659 142.338 30.925
2024-12-02-11:50:01-root-INFO: grad norm: 131.047 128.684 24.773
2024-12-02-11:50:01-root-INFO: Loss Change: 1812.552 -> 1783.591
2024-12-02-11:50:01-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-11:50:01-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-11:50:01-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:50:01-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-11:50:01-root-INFO: grad norm: 202.919 195.423 54.644
2024-12-02-11:50:02-root-INFO: Loss too large (1786.536->1867.843)! Learning rate decreased to 0.00328.
2024-12-02-11:50:02-root-INFO: Loss too large (1786.536->1829.823)! Learning rate decreased to 0.00262.
2024-12-02-11:50:02-root-INFO: Loss too large (1786.536->1803.760)! Learning rate decreased to 0.00210.
2024-12-02-11:50:02-root-INFO: Loss too large (1786.536->1786.716)! Learning rate decreased to 0.00168.
2024-12-02-11:50:02-root-INFO: grad norm: 155.409 152.281 31.021
2024-12-02-11:50:03-root-INFO: grad norm: 107.412 103.007 30.445
2024-12-02-11:50:03-root-INFO: grad norm: 99.526 97.170 21.528
2024-12-02-11:50:04-root-INFO: grad norm: 92.858 89.296 25.471
2024-12-02-11:50:04-root-INFO: Loss Change: 1786.536 -> 1742.708
2024-12-02-11:50:04-root-INFO: Regularization Change: 0.000 -> 0.308
2024-12-02-11:50:04-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-11:50:04-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:50:04-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-11:50:04-root-INFO: grad norm: 108.787 105.469 26.666
2024-12-02-11:50:05-root-INFO: Loss too large (1733.798->1757.740)! Learning rate decreased to 0.00342.
2024-12-02-11:50:05-root-INFO: Loss too large (1733.798->1741.868)! Learning rate decreased to 0.00273.
2024-12-02-11:50:05-root-INFO: grad norm: 169.018 165.494 34.335
2024-12-02-11:50:05-root-INFO: Loss too large (1732.895->1755.699)! Learning rate decreased to 0.00219.
2024-12-02-11:50:06-root-INFO: Loss too large (1732.895->1733.791)! Learning rate decreased to 0.00175.
2024-12-02-11:50:06-root-INFO: grad norm: 146.628 144.250 26.296
2024-12-02-11:50:07-root-INFO: grad norm: 132.031 129.056 27.869
2024-12-02-11:50:07-root-INFO: grad norm: 118.429 116.362 22.032
2024-12-02-11:50:07-root-INFO: Loss Change: 1733.798 -> 1703.660
2024-12-02-11:50:07-root-INFO: Regularization Change: 0.000 -> 0.326
2024-12-02-11:50:07-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-11:50:07-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:50:08-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-11:50:08-root-INFO: grad norm: 93.143 90.014 23.939
2024-12-02-11:50:08-root-INFO: Loss too large (1692.153->1708.690)! Learning rate decreased to 0.00356.
2024-12-02-11:50:08-root-INFO: Loss too large (1692.153->1696.446)! Learning rate decreased to 0.00285.
2024-12-02-11:50:08-root-INFO: grad norm: 156.679 154.118 28.212
2024-12-02-11:50:09-root-INFO: Loss too large (1689.885->1719.283)! Learning rate decreased to 0.00228.
2024-12-02-11:50:09-root-INFO: Loss too large (1689.885->1697.263)! Learning rate decreased to 0.00182.
2024-12-02-11:50:09-root-INFO: grad norm: 169.220 165.870 33.505
2024-12-02-11:50:10-root-INFO: grad norm: 173.096 169.955 32.827
2024-12-02-11:50:10-root-INFO: grad norm: 192.362 188.561 38.050
2024-12-02-11:50:10-root-INFO: Loss too large (1677.087->1680.694)! Learning rate decreased to 0.00146.
2024-12-02-11:50:11-root-INFO: Loss Change: 1692.153 -> 1670.920
2024-12-02-11:50:11-root-INFO: Regularization Change: 0.000 -> 0.314
2024-12-02-11:50:11-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-11:50:11-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-11:50:11-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-11:50:11-root-INFO: grad norm: 121.075 118.919 22.744
2024-12-02-11:50:11-root-INFO: Loss too large (1658.501->1745.396)! Learning rate decreased to 0.00371.
2024-12-02-11:50:11-root-INFO: Loss too large (1658.501->1700.845)! Learning rate decreased to 0.00297.
2024-12-02-11:50:11-root-INFO: Loss too large (1658.501->1675.015)! Learning rate decreased to 0.00238.
2024-12-02-11:50:12-root-INFO: Loss too large (1658.501->1660.906)! Learning rate decreased to 0.00190.
2024-12-02-11:50:12-root-INFO: grad norm: 154.644 151.114 32.852
2024-12-02-11:50:12-root-INFO: Loss too large (1653.774->1657.461)! Learning rate decreased to 0.00152.
2024-12-02-11:50:13-root-INFO: grad norm: 138.351 135.865 26.109
2024-12-02-11:50:13-root-INFO: grad norm: 123.609 120.838 26.022
2024-12-02-11:50:14-root-INFO: grad norm: 120.065 117.679 23.816
2024-12-02-11:50:14-root-INFO: Loss Change: 1658.501 -> 1632.867
2024-12-02-11:50:14-root-INFO: Regularization Change: 0.000 -> 0.212
2024-12-02-11:50:14-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-11:50:14-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:50:14-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-11:50:14-root-INFO: grad norm: 276.526 269.899 60.177
2024-12-02-11:50:14-root-INFO: Loss too large (1645.675->1797.444)! Learning rate decreased to 0.00387.
2024-12-02-11:50:15-root-INFO: Loss too large (1645.675->1760.598)! Learning rate decreased to 0.00309.
2024-12-02-11:50:15-root-INFO: Loss too large (1645.675->1726.964)! Learning rate decreased to 0.00248.
2024-12-02-11:50:15-root-INFO: Loss too large (1645.675->1697.068)! Learning rate decreased to 0.00198.
2024-12-02-11:50:15-root-INFO: Loss too large (1645.675->1671.836)! Learning rate decreased to 0.00158.
2024-12-02-11:50:15-root-INFO: Loss too large (1645.675->1652.075)! Learning rate decreased to 0.00127.
2024-12-02-11:50:16-root-INFO: grad norm: 178.409 175.730 30.804
2024-12-02-11:50:16-root-INFO: grad norm: 72.321 69.075 21.422
2024-12-02-11:50:17-root-INFO: grad norm: 65.437 63.199 16.970
2024-12-02-11:50:17-root-INFO: grad norm: 61.037 58.557 17.223
2024-12-02-11:50:17-root-INFO: Loss Change: 1645.675 -> 1603.290
2024-12-02-11:50:17-root-INFO: Regularization Change: 0.000 -> 0.173
2024-12-02-11:50:17-root-INFO: Undo step: 180
2024-12-02-11:50:17-root-INFO: Undo step: 181
2024-12-02-11:50:17-root-INFO: Undo step: 182
2024-12-02-11:50:17-root-INFO: Undo step: 183
2024-12-02-11:50:17-root-INFO: Undo step: 184
2024-12-02-11:50:17-root-INFO: Undo step: 185
2024-12-02-11:50:17-root-INFO: Undo step: 186
2024-12-02-11:50:17-root-INFO: Undo step: 187
2024-12-02-11:50:17-root-INFO: Undo step: 188
2024-12-02-11:50:17-root-INFO: Undo step: 189
2024-12-02-11:50:17-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-11:50:18-root-INFO: grad norm: 951.741 811.852 496.697
2024-12-02-11:50:18-root-INFO: grad norm: 536.046 494.166 207.714
2024-12-02-11:50:19-root-INFO: grad norm: 758.248 721.531 233.093
2024-12-02-11:50:19-root-INFO: Loss too large (2750.139->3341.260)! Learning rate decreased to 0.00254.
2024-12-02-11:50:19-root-INFO: Loss too large (2750.139->2963.572)! Learning rate decreased to 0.00203.
2024-12-02-11:50:19-root-INFO: grad norm: 689.792 667.105 175.451
2024-12-02-11:50:20-root-INFO: grad norm: 651.710 632.760 156.014
2024-12-02-11:50:20-root-INFO: Loss too large (2511.113->2570.270)! Learning rate decreased to 0.00163.
2024-12-02-11:50:20-root-INFO: Loss Change: 4000.734 -> 2446.225
2024-12-02-11:50:20-root-INFO: Regularization Change: 0.000 -> 14.569
2024-12-02-11:50:20-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-11:50:20-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:50:21-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-11:50:21-root-INFO: grad norm: 490.753 476.301 118.220
2024-12-02-11:50:21-root-INFO: Loss too large (2445.283->2743.184)! Learning rate decreased to 0.00265.
2024-12-02-11:50:21-root-INFO: Loss too large (2445.283->2501.628)! Learning rate decreased to 0.00212.
2024-12-02-11:50:21-root-INFO: grad norm: 520.343 508.893 108.558
2024-12-02-11:50:22-root-INFO: Loss too large (2358.465->2396.099)! Learning rate decreased to 0.00170.
2024-12-02-11:50:22-root-INFO: grad norm: 405.165 393.720 95.622
2024-12-02-11:50:23-root-INFO: grad norm: 296.774 288.773 68.447
2024-12-02-11:50:23-root-INFO: grad norm: 270.940 261.974 69.126
2024-12-02-11:50:23-root-INFO: Loss Change: 2445.283 -> 2156.313
2024-12-02-11:50:23-root-INFO: Regularization Change: 0.000 -> 1.960
2024-12-02-11:50:23-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-11:50:23-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:50:23-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-11:50:24-root-INFO: grad norm: 224.231 218.784 49.123
2024-12-02-11:50:24-root-INFO: Loss too large (2125.140->2174.399)! Learning rate decreased to 0.00277.
2024-12-02-11:50:24-root-INFO: Loss too large (2125.140->2132.181)! Learning rate decreased to 0.00222.
2024-12-02-11:50:25-root-INFO: grad norm: 281.312 271.708 72.876
2024-12-02-11:50:25-root-INFO: Loss too large (2108.802->2112.229)! Learning rate decreased to 0.00177.
2024-12-02-11:50:25-root-INFO: grad norm: 280.171 272.714 64.210
2024-12-02-11:50:26-root-INFO: grad norm: 291.250 281.472 74.832
2024-12-02-11:50:26-root-INFO: grad norm: 308.284 299.828 71.708
2024-12-02-11:50:27-root-INFO: Loss Change: 2125.140 -> 2038.241
2024-12-02-11:50:27-root-INFO: Regularization Change: 0.000 -> 1.256
2024-12-02-11:50:27-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-11:50:27-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-11:50:27-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-11:50:27-root-INFO: grad norm: 345.035 332.810 91.031
2024-12-02-11:50:27-root-INFO: Loss too large (2033.135->2338.134)! Learning rate decreased to 0.00289.
2024-12-02-11:50:27-root-INFO: Loss too large (2033.135->2177.082)! Learning rate decreased to 0.00231.
2024-12-02-11:50:27-root-INFO: Loss too large (2033.135->2074.284)! Learning rate decreased to 0.00185.
2024-12-02-11:50:28-root-INFO: grad norm: 362.335 351.483 88.013
2024-12-02-11:50:28-root-INFO: grad norm: 384.506 371.458 99.314
2024-12-02-11:50:29-root-INFO: grad norm: 401.622 388.558 101.601
2024-12-02-11:50:29-root-INFO: grad norm: 418.305 402.859 112.622
2024-12-02-11:50:30-root-INFO: Loss Change: 2033.135 -> 1980.197
2024-12-02-11:50:30-root-INFO: Regularization Change: 0.000 -> 1.247
2024-12-02-11:50:30-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-11:50:30-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:50:30-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-11:50:30-root-INFO: grad norm: 440.975 426.847 110.730
2024-12-02-11:50:30-root-INFO: Loss too large (1959.174->2433.603)! Learning rate decreased to 0.00301.
2024-12-02-11:50:30-root-INFO: Loss too large (1959.174->2207.891)! Learning rate decreased to 0.00241.
2024-12-02-11:50:30-root-INFO: Loss too large (1959.174->2054.742)! Learning rate decreased to 0.00193.
2024-12-02-11:50:31-root-INFO: grad norm: 423.213 404.632 124.025
2024-12-02-11:50:31-root-INFO: grad norm: 397.166 381.825 109.318
2024-12-02-11:50:32-root-INFO: grad norm: 405.711 388.423 117.171
2024-12-02-11:50:32-root-INFO: grad norm: 434.820 419.370 114.881
2024-12-02-11:50:32-root-INFO: Loss too large (1903.122->1907.999)! Learning rate decreased to 0.00154.
2024-12-02-11:50:33-root-INFO: Loss Change: 1959.174 -> 1853.834
2024-12-02-11:50:33-root-INFO: Regularization Change: 0.000 -> 1.027
2024-12-02-11:50:33-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-11:50:33-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:50:33-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-11:50:33-root-INFO: grad norm: 298.492 286.093 85.136
2024-12-02-11:50:33-root-INFO: Loss too large (1840.167->2148.014)! Learning rate decreased to 0.00314.
2024-12-02-11:50:33-root-INFO: Loss too large (1840.167->1998.468)! Learning rate decreased to 0.00251.
2024-12-02-11:50:33-root-INFO: Loss too large (1840.167->1898.208)! Learning rate decreased to 0.00201.
2024-12-02-11:50:34-root-INFO: grad norm: 381.916 369.609 96.172
2024-12-02-11:50:34-root-INFO: Loss too large (1837.654->1866.982)! Learning rate decreased to 0.00161.
2024-12-02-11:50:34-root-INFO: grad norm: 267.838 255.489 80.390
2024-12-02-11:50:35-root-INFO: grad norm: 146.383 140.014 42.710
2024-12-02-11:50:35-root-INFO: grad norm: 141.684 135.291 42.078
2024-12-02-11:50:36-root-INFO: Loss Change: 1840.167 -> 1755.441
2024-12-02-11:50:36-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-11:50:36-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-11:50:36-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:50:36-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-11:50:36-root-INFO: grad norm: 403.649 388.857 108.273
2024-12-02-11:50:36-root-INFO: Loss too large (1775.729->1940.103)! Learning rate decreased to 0.00328.
2024-12-02-11:50:36-root-INFO: Loss too large (1775.729->1904.418)! Learning rate decreased to 0.00262.
2024-12-02-11:50:37-root-INFO: Loss too large (1775.729->1867.673)! Learning rate decreased to 0.00210.
2024-12-02-11:50:37-root-INFO: Loss too large (1775.729->1831.322)! Learning rate decreased to 0.00168.
2024-12-02-11:50:37-root-INFO: Loss too large (1775.729->1797.783)! Learning rate decreased to 0.00134.
2024-12-02-11:50:37-root-INFO: grad norm: 243.800 233.216 71.054
2024-12-02-11:50:38-root-INFO: grad norm: 80.105 75.965 25.421
2024-12-02-11:50:38-root-INFO: grad norm: 73.544 69.988 22.592
2024-12-02-11:50:39-root-INFO: grad norm: 70.744 67.345 21.663
2024-12-02-11:50:39-root-INFO: Loss Change: 1775.729 -> 1694.433
2024-12-02-11:50:39-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-02-11:50:39-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-11:50:39-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:50:39-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-11:50:39-root-INFO: grad norm: 153.732 147.811 42.255
2024-12-02-11:50:40-root-INFO: Loss too large (1684.263->1760.154)! Learning rate decreased to 0.00342.
2024-12-02-11:50:40-root-INFO: Loss too large (1684.263->1732.859)! Learning rate decreased to 0.00273.
2024-12-02-11:50:40-root-INFO: Loss too large (1684.263->1711.836)! Learning rate decreased to 0.00219.
2024-12-02-11:50:40-root-INFO: Loss too large (1684.263->1696.664)! Learning rate decreased to 0.00175.
2024-12-02-11:50:40-root-INFO: Loss too large (1684.263->1686.531)! Learning rate decreased to 0.00140.
2024-12-02-11:50:41-root-INFO: grad norm: 171.435 164.163 49.400
2024-12-02-11:50:41-root-INFO: grad norm: 229.039 221.502 58.274
2024-12-02-11:50:41-root-INFO: Loss too large (1676.294->1685.363)! Learning rate decreased to 0.00112.
2024-12-02-11:50:42-root-INFO: grad norm: 197.211 188.695 57.326
2024-12-02-11:50:42-root-INFO: grad norm: 159.166 153.757 41.143
2024-12-02-11:50:43-root-INFO: Loss Change: 1684.263 -> 1657.051
2024-12-02-11:50:43-root-INFO: Regularization Change: 0.000 -> 0.200
2024-12-02-11:50:43-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-11:50:43-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:50:43-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-11:50:43-root-INFO: grad norm: 83.087 79.974 22.531
2024-12-02-11:50:43-root-INFO: grad norm: 275.976 267.547 67.686
2024-12-02-11:50:43-root-INFO: Loss too large (1640.990->1833.627)! Learning rate decreased to 0.00356.
2024-12-02-11:50:44-root-INFO: Loss too large (1640.990->1793.060)! Learning rate decreased to 0.00285.
2024-12-02-11:50:44-root-INFO: Loss too large (1640.990->1755.078)! Learning rate decreased to 0.00228.
2024-12-02-11:50:44-root-INFO: Loss too large (1640.990->1719.809)! Learning rate decreased to 0.00182.
2024-12-02-11:50:44-root-INFO: Loss too large (1640.990->1688.435)! Learning rate decreased to 0.00146.
2024-12-02-11:50:44-root-INFO: Loss too large (1640.990->1662.578)! Learning rate decreased to 0.00117.
2024-12-02-11:50:44-root-INFO: Loss too large (1640.990->1643.400)! Learning rate decreased to 0.00093.
2024-12-02-11:50:45-root-INFO: grad norm: 203.359 194.785 58.424
2024-12-02-11:50:45-root-INFO: grad norm: 145.695 140.846 37.278
2024-12-02-11:50:46-root-INFO: grad norm: 130.301 124.895 37.143
2024-12-02-11:50:46-root-INFO: Loss Change: 1641.664 -> 1610.465
2024-12-02-11:50:46-root-INFO: Regularization Change: 0.000 -> 0.274
2024-12-02-11:50:46-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-11:50:46-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-11:50:46-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-11:50:46-root-INFO: grad norm: 281.188 271.601 72.801
2024-12-02-11:50:47-root-INFO: Loss too large (1616.295->1806.306)! Learning rate decreased to 0.00371.
2024-12-02-11:50:47-root-INFO: Loss too large (1616.295->1775.670)! Learning rate decreased to 0.00297.
2024-12-02-11:50:47-root-INFO: Loss too large (1616.295->1743.347)! Learning rate decreased to 0.00238.
2024-12-02-11:50:47-root-INFO: Loss too large (1616.295->1710.506)! Learning rate decreased to 0.00190.
2024-12-02-11:50:47-root-INFO: Loss too large (1616.295->1679.003)! Learning rate decreased to 0.00152.
2024-12-02-11:50:47-root-INFO: Loss too large (1616.295->1651.116)! Learning rate decreased to 0.00122.
2024-12-02-11:50:47-root-INFO: Loss too large (1616.295->1628.856)! Learning rate decreased to 0.00097.
2024-12-02-11:50:48-root-INFO: grad norm: 235.106 226.116 64.391
2024-12-02-11:50:48-root-INFO: grad norm: 183.074 176.655 48.051
2024-12-02-11:50:49-root-INFO: grad norm: 173.277 166.627 47.545
2024-12-02-11:50:49-root-INFO: grad norm: 163.422 157.799 42.499
2024-12-02-11:50:50-root-INFO: Loss Change: 1616.295 -> 1587.456
2024-12-02-11:50:50-root-INFO: Regularization Change: 0.000 -> 0.129
2024-12-02-11:50:50-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-11:50:50-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:50:50-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-11:50:50-root-INFO: grad norm: 88.551 84.789 25.535
2024-12-02-11:50:50-root-INFO: grad norm: 138.684 133.079 39.031
2024-12-02-11:50:50-root-INFO: Loss too large (1563.017->1743.944)! Learning rate decreased to 0.00387.
2024-12-02-11:50:51-root-INFO: Loss too large (1563.017->1675.832)! Learning rate decreased to 0.00309.
2024-12-02-11:50:51-root-INFO: Loss too large (1563.017->1628.253)! Learning rate decreased to 0.00248.
2024-12-02-11:50:51-root-INFO: Loss too large (1563.017->1596.841)! Learning rate decreased to 0.00198.
2024-12-02-11:50:51-root-INFO: Loss too large (1563.017->1577.297)! Learning rate decreased to 0.00158.
2024-12-02-11:50:51-root-INFO: Loss too large (1563.017->1565.895)! Learning rate decreased to 0.00127.
2024-12-02-11:50:52-root-INFO: grad norm: 195.289 189.480 47.276
2024-12-02-11:50:52-root-INFO: Loss too large (1559.761->1570.430)! Learning rate decreased to 0.00101.
2024-12-02-11:50:52-root-INFO: Loss too large (1559.761->1560.370)! Learning rate decreased to 0.00081.
2024-12-02-11:50:52-root-INFO: grad norm: 154.302 148.325 42.530
2024-12-02-11:50:53-root-INFO: grad norm: 120.219 116.310 30.410
2024-12-02-11:50:53-root-INFO: Loss Change: 1578.690 -> 1545.789
2024-12-02-11:50:53-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-02-11:50:53-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-11:50:53-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:50:53-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-11:50:53-root-INFO: grad norm: 96.288 92.808 25.654
2024-12-02-11:50:54-root-INFO: Loss too large (1539.026->1599.930)! Learning rate decreased to 0.00403.
2024-12-02-11:50:54-root-INFO: Loss too large (1539.026->1578.911)! Learning rate decreased to 0.00322.
2024-12-02-11:50:54-root-INFO: Loss too large (1539.026->1562.938)! Learning rate decreased to 0.00258.
2024-12-02-11:50:54-root-INFO: Loss too large (1539.026->1551.670)! Learning rate decreased to 0.00206.
2024-12-02-11:50:54-root-INFO: Loss too large (1539.026->1544.272)! Learning rate decreased to 0.00165.
2024-12-02-11:50:54-root-INFO: Loss too large (1539.026->1539.759)! Learning rate decreased to 0.00132.
2024-12-02-11:50:55-root-INFO: grad norm: 147.513 142.286 38.919
2024-12-02-11:50:55-root-INFO: Loss too large (1537.236->1543.891)! Learning rate decreased to 0.00106.
2024-12-02-11:50:56-root-INFO: grad norm: 193.795 187.589 48.654
2024-12-02-11:50:56-root-INFO: Loss too large (1536.716->1539.841)! Learning rate decreased to 0.00085.
2024-12-02-11:50:56-root-INFO: grad norm: 169.241 163.089 45.214
2024-12-02-11:50:57-root-INFO: grad norm: 146.001 141.179 37.213
2024-12-02-11:50:57-root-INFO: Loss Change: 1539.026 -> 1524.875
2024-12-02-11:50:57-root-INFO: Regularization Change: 0.000 -> 0.088
2024-12-02-11:50:57-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-11:50:57-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:50:57-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-11:50:57-root-INFO: grad norm: 109.079 104.770 30.355
2024-12-02-11:50:57-root-INFO: Loss too large (1518.611->1579.137)! Learning rate decreased to 0.00420.
2024-12-02-11:50:58-root-INFO: Loss too large (1518.611->1555.808)! Learning rate decreased to 0.00336.
2024-12-02-11:50:58-root-INFO: Loss too large (1518.611->1538.805)! Learning rate decreased to 0.00269.
2024-12-02-11:50:58-root-INFO: Loss too large (1518.611->1527.275)! Learning rate decreased to 0.00215.
2024-12-02-11:50:58-root-INFO: Loss too large (1518.611->1520.032)! Learning rate decreased to 0.00172.
2024-12-02-11:50:59-root-INFO: grad norm: 191.303 184.581 50.268
2024-12-02-11:50:59-root-INFO: Loss too large (1515.876->1555.057)! Learning rate decreased to 0.00138.
2024-12-02-11:50:59-root-INFO: Loss too large (1515.876->1531.864)! Learning rate decreased to 0.00110.
2024-12-02-11:50:59-root-INFO: Loss too large (1515.876->1517.898)! Learning rate decreased to 0.00088.
2024-12-02-11:50:59-root-INFO: grad norm: 176.831 170.933 45.289
2024-12-02-11:51:00-root-INFO: grad norm: 167.769 161.805 44.336
2024-12-02-11:51:00-root-INFO: grad norm: 157.976 152.751 40.294
2024-12-02-11:51:01-root-INFO: Loss Change: 1518.611 -> 1500.486
2024-12-02-11:51:01-root-INFO: Regularization Change: 0.000 -> 0.122
2024-12-02-11:51:01-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-11:51:01-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:51:01-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-11:51:01-root-INFO: grad norm: 72.739 70.002 19.767
2024-12-02-11:51:01-root-INFO: grad norm: 128.375 123.457 35.194
2024-12-02-11:51:02-root-INFO: Loss too large (1477.668->1678.792)! Learning rate decreased to 0.00437.
2024-12-02-11:51:02-root-INFO: Loss too large (1477.668->1613.233)! Learning rate decreased to 0.00350.
2024-12-02-11:51:02-root-INFO: Loss too large (1477.668->1563.909)! Learning rate decreased to 0.00280.
2024-12-02-11:51:02-root-INFO: Loss too large (1477.668->1528.710)! Learning rate decreased to 0.00224.
2024-12-02-11:51:02-root-INFO: Loss too large (1477.668->1504.995)! Learning rate decreased to 0.00179.
2024-12-02-11:51:02-root-INFO: Loss too large (1477.668->1489.930)! Learning rate decreased to 0.00143.
2024-12-02-11:51:03-root-INFO: Loss too large (1477.668->1480.936)! Learning rate decreased to 0.00115.
2024-12-02-11:51:03-root-INFO: grad norm: 177.347 172.115 42.759
2024-12-02-11:51:03-root-INFO: Loss too large (1475.955->1481.112)! Learning rate decreased to 0.00092.
2024-12-02-11:51:04-root-INFO: grad norm: 180.015 173.261 48.844
2024-12-02-11:51:04-root-INFO: grad norm: 181.891 176.324 44.655
2024-12-02-11:51:04-root-INFO: Loss Change: 1492.739 -> 1469.009
2024-12-02-11:51:04-root-INFO: Regularization Change: 0.000 -> 0.304
2024-12-02-11:51:04-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-11:51:04-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-11:51:05-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-11:51:05-root-INFO: grad norm: 71.041 68.857 17.480
2024-12-02-11:51:05-root-INFO: grad norm: 258.936 252.242 58.498
2024-12-02-11:51:05-root-INFO: Loss too large (1455.352->1673.117)! Learning rate decreased to 0.00455.
2024-12-02-11:51:06-root-INFO: Loss too large (1455.352->1649.107)! Learning rate decreased to 0.00364.
2024-12-02-11:51:06-root-INFO: Loss too large (1455.352->1620.981)! Learning rate decreased to 0.00291.
2024-12-02-11:51:06-root-INFO: Loss too large (1455.352->1589.041)! Learning rate decreased to 0.00233.
2024-12-02-11:51:06-root-INFO: Loss too large (1455.352->1554.707)! Learning rate decreased to 0.00186.
2024-12-02-11:51:06-root-INFO: Loss too large (1455.352->1520.642)! Learning rate decreased to 0.00149.
2024-12-02-11:51:06-root-INFO: Loss too large (1455.352->1490.479)! Learning rate decreased to 0.00119.
2024-12-02-11:51:07-root-INFO: Loss too large (1455.352->1467.189)! Learning rate decreased to 0.00095.
2024-12-02-11:51:07-root-INFO: grad norm: 245.105 235.924 66.457
2024-12-02-11:51:07-root-INFO: grad norm: 230.818 224.274 54.571
2024-12-02-11:51:08-root-INFO: grad norm: 219.786 211.652 59.240
2024-12-02-11:51:08-root-INFO: Loss Change: 1457.515 -> 1437.889
2024-12-02-11:51:08-root-INFO: Regularization Change: 0.000 -> 0.297
2024-12-02-11:51:08-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-11:51:08-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:51:09-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-11:51:09-root-INFO: grad norm: 358.269 348.499 83.095
2024-12-02-11:51:09-root-INFO: Loss too large (1458.128->1694.011)! Learning rate decreased to 0.00474.
2024-12-02-11:51:09-root-INFO: Loss too large (1458.128->1670.998)! Learning rate decreased to 0.00379.
2024-12-02-11:51:09-root-INFO: Loss too large (1458.128->1645.351)! Learning rate decreased to 0.00303.
2024-12-02-11:51:10-root-INFO: Loss too large (1458.128->1615.478)! Learning rate decreased to 0.00243.
2024-12-02-11:51:10-root-INFO: Loss too large (1458.128->1580.938)! Learning rate decreased to 0.00194.
2024-12-02-11:51:10-root-INFO: Loss too large (1458.128->1542.755)! Learning rate decreased to 0.00155.
2024-12-02-11:51:10-root-INFO: Loss too large (1458.128->1503.782)! Learning rate decreased to 0.00124.
2024-12-02-11:51:10-root-INFO: Loss too large (1458.128->1468.926)! Learning rate decreased to 0.00099.
2024-12-02-11:51:11-root-INFO: grad norm: 285.210 274.651 76.889
2024-12-02-11:51:11-root-INFO: grad norm: 235.347 228.113 57.903
2024-12-02-11:51:12-root-INFO: grad norm: 214.313 206.391 57.730
2024-12-02-11:51:12-root-INFO: grad norm: 195.428 189.261 48.705
2024-12-02-11:51:13-root-INFO: Loss Change: 1458.128 -> 1414.903
2024-12-02-11:51:13-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-02-11:51:13-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-11:51:13-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:51:13-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-11:51:13-root-INFO: grad norm: 149.170 143.939 39.158
2024-12-02-11:51:13-root-INFO: Loss too large (1411.915->1668.156)! Learning rate decreased to 0.00494.
2024-12-02-11:51:13-root-INFO: Loss too large (1411.915->1599.412)! Learning rate decreased to 0.00395.
2024-12-02-11:51:14-root-INFO: Loss too large (1411.915->1542.513)! Learning rate decreased to 0.00316.
2024-12-02-11:51:14-root-INFO: Loss too large (1411.915->1497.295)! Learning rate decreased to 0.00253.
2024-12-02-11:51:14-root-INFO: Loss too large (1411.915->1463.333)! Learning rate decreased to 0.00202.
2024-12-02-11:51:14-root-INFO: Loss too large (1411.915->1439.440)! Learning rate decreased to 0.00162.
2024-12-02-11:51:14-root-INFO: Loss too large (1411.915->1423.756)! Learning rate decreased to 0.00129.
2024-12-02-11:51:14-root-INFO: Loss too large (1411.915->1414.195)! Learning rate decreased to 0.00104.
2024-12-02-11:51:15-root-INFO: grad norm: 154.566 149.626 38.764
2024-12-02-11:51:15-root-INFO: grad norm: 159.131 153.332 42.569
2024-12-02-11:51:16-root-INFO: grad norm: 162.242 157.142 40.361
2024-12-02-11:51:16-root-INFO: grad norm: 163.979 157.916 44.179
2024-12-02-11:51:17-root-INFO: Loss Change: 1411.915 -> 1398.120
2024-12-02-11:51:17-root-INFO: Regularization Change: 0.000 -> 0.103
2024-12-02-11:51:17-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-11:51:17-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:51:17-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-11:51:17-root-INFO: grad norm: 326.145 316.268 79.654
2024-12-02-11:51:17-root-INFO: Loss too large (1425.738->1661.445)! Learning rate decreased to 0.00514.
2024-12-02-11:51:17-root-INFO: Loss too large (1425.738->1636.332)! Learning rate decreased to 0.00411.
2024-12-02-11:51:18-root-INFO: Loss too large (1425.738->1608.885)! Learning rate decreased to 0.00329.
2024-12-02-11:51:18-root-INFO: Loss too large (1425.738->1577.183)! Learning rate decreased to 0.00263.
2024-12-02-11:51:18-root-INFO: Loss too large (1425.738->1540.759)! Learning rate decreased to 0.00210.
2024-12-02-11:51:18-root-INFO: Loss too large (1425.738->1501.048)! Learning rate decreased to 0.00168.
2024-12-02-11:51:18-root-INFO: Loss too large (1425.738->1461.882)! Learning rate decreased to 0.00135.
2024-12-02-11:51:18-root-INFO: Loss too large (1425.738->1428.836)! Learning rate decreased to 0.00108.
2024-12-02-11:51:19-root-INFO: grad norm: 257.886 248.199 70.017
2024-12-02-11:51:19-root-INFO: grad norm: 217.739 210.407 56.027
2024-12-02-11:51:20-root-INFO: grad norm: 192.679 185.433 52.342
2024-12-02-11:51:20-root-INFO: grad norm: 173.561 167.568 45.216
2024-12-02-11:51:21-root-INFO: Loss Change: 1425.738 -> 1377.993
2024-12-02-11:51:21-root-INFO: Regularization Change: 0.000 -> 0.194
2024-12-02-11:51:21-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-11:51:21-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-11:51:21-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-11:51:21-root-INFO: grad norm: 102.343 98.966 26.072
2024-12-02-11:51:21-root-INFO: Loss too large (1372.082->1490.649)! Learning rate decreased to 0.00535.
2024-12-02-11:51:21-root-INFO: Loss too large (1372.082->1451.443)! Learning rate decreased to 0.00428.
2024-12-02-11:51:21-root-INFO: Loss too large (1372.082->1421.895)! Learning rate decreased to 0.00342.
2024-12-02-11:51:22-root-INFO: Loss too large (1372.082->1400.799)! Learning rate decreased to 0.00274.
2024-12-02-11:51:22-root-INFO: Loss too large (1372.082->1386.588)! Learning rate decreased to 0.00219.
2024-12-02-11:51:22-root-INFO: Loss too large (1372.082->1377.584)! Learning rate decreased to 0.00175.
2024-12-02-11:51:22-root-INFO: Loss too large (1372.082->1372.258)! Learning rate decreased to 0.00140.
2024-12-02-11:51:23-root-INFO: grad norm: 133.882 129.125 35.372
2024-12-02-11:51:23-root-INFO: Loss too large (1369.379->1369.818)! Learning rate decreased to 0.00112.
2024-12-02-11:51:23-root-INFO: grad norm: 132.389 127.617 35.224
2024-12-02-11:51:24-root-INFO: grad norm: 130.604 126.071 34.112
2024-12-02-11:51:24-root-INFO: grad norm: 128.526 123.827 34.436
2024-12-02-11:51:24-root-INFO: Loss Change: 1372.082 -> 1356.090
2024-12-02-11:51:24-root-INFO: Regularization Change: 0.000 -> 0.119
2024-12-02-11:51:24-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-11:51:24-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:51:25-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-11:51:25-root-INFO: grad norm: 236.376 229.062 58.345
2024-12-02-11:51:25-root-INFO: Loss too large (1366.806->1594.392)! Learning rate decreased to 0.00556.
2024-12-02-11:51:25-root-INFO: Loss too large (1366.806->1569.866)! Learning rate decreased to 0.00445.
2024-12-02-11:51:25-root-INFO: Loss too large (1366.806->1540.541)! Learning rate decreased to 0.00356.
2024-12-02-11:51:25-root-INFO: Loss too large (1366.806->1506.016)! Learning rate decreased to 0.00285.
2024-12-02-11:51:25-root-INFO: Loss too large (1366.806->1467.683)! Learning rate decreased to 0.00228.
2024-12-02-11:51:26-root-INFO: Loss too large (1366.806->1429.165)! Learning rate decreased to 0.00182.
2024-12-02-11:51:26-root-INFO: Loss too large (1366.806->1395.712)! Learning rate decreased to 0.00146.
2024-12-02-11:51:26-root-INFO: Loss too large (1366.806->1371.218)! Learning rate decreased to 0.00117.
2024-12-02-11:51:26-root-INFO: grad norm: 207.910 199.664 57.973
2024-12-02-11:51:27-root-INFO: grad norm: 188.946 182.806 47.774
2024-12-02-11:51:27-root-INFO: grad norm: 170.020 163.233 47.558
2024-12-02-11:51:28-root-INFO: grad norm: 156.778 151.507 40.313
2024-12-02-11:51:28-root-INFO: Loss Change: 1366.806 -> 1336.299
2024-12-02-11:51:28-root-INFO: Regularization Change: 0.000 -> 0.158
2024-12-02-11:51:28-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-11:51:28-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:51:28-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-11:51:28-root-INFO: grad norm: 79.168 76.484 20.440
2024-12-02-11:51:29-root-INFO: Loss too large (1332.760->1342.047)! Learning rate decreased to 0.00579.
2024-12-02-11:51:29-root-INFO: grad norm: 232.378 226.498 51.945
2024-12-02-11:51:29-root-INFO: Loss too large (1332.645->1534.769)! Learning rate decreased to 0.00463.
2024-12-02-11:51:29-root-INFO: Loss too large (1332.645->1506.792)! Learning rate decreased to 0.00370.
2024-12-02-11:51:30-root-INFO: Loss too large (1332.645->1472.370)! Learning rate decreased to 0.00296.
2024-12-02-11:51:30-root-INFO: Loss too large (1332.645->1433.195)! Learning rate decreased to 0.00237.
2024-12-02-11:51:30-root-INFO: Loss too large (1332.645->1393.364)! Learning rate decreased to 0.00190.
2024-12-02-11:51:30-root-INFO: Loss too large (1332.645->1358.858)! Learning rate decreased to 0.00152.
2024-12-02-11:51:30-root-INFO: Loss too large (1332.645->1334.042)! Learning rate decreased to 0.00121.
2024-12-02-11:51:31-root-INFO: grad norm: 194.559 186.742 54.593
2024-12-02-11:51:31-root-INFO: grad norm: 176.507 171.260 42.713
2024-12-02-11:51:32-root-INFO: grad norm: 157.125 150.869 43.896
2024-12-02-11:51:32-root-INFO: Loss Change: 1332.760 -> 1305.743
2024-12-02-11:51:32-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-02-11:51:32-root-INFO: Undo step: 170
2024-12-02-11:51:32-root-INFO: Undo step: 171
2024-12-02-11:51:32-root-INFO: Undo step: 172
2024-12-02-11:51:32-root-INFO: Undo step: 173
2024-12-02-11:51:32-root-INFO: Undo step: 174
2024-12-02-11:51:32-root-INFO: Undo step: 175
2024-12-02-11:51:32-root-INFO: Undo step: 176
2024-12-02-11:51:32-root-INFO: Undo step: 177
2024-12-02-11:51:32-root-INFO: Undo step: 178
2024-12-02-11:51:32-root-INFO: Undo step: 179
2024-12-02-11:51:32-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-11:51:32-root-INFO: grad norm: 809.060 768.406 253.239
2024-12-02-11:51:33-root-INFO: grad norm: 835.650 800.911 238.437
2024-12-02-11:51:33-root-INFO: Loss too large (2919.119->3075.739)! Learning rate decreased to 0.00387.
2024-12-02-11:51:33-root-INFO: grad norm: 746.620 718.217 203.976
2024-12-02-11:51:34-root-INFO: grad norm: 461.472 435.281 153.255
2024-12-02-11:51:34-root-INFO: grad norm: 394.110 373.020 127.196
2024-12-02-11:51:35-root-INFO: Loss Change: 3464.927 -> 2076.190
2024-12-02-11:51:35-root-INFO: Regularization Change: 0.000 -> 19.068
2024-12-02-11:51:35-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-11:51:35-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:51:35-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-11:51:35-root-INFO: grad norm: 330.814 306.299 124.975
2024-12-02-11:51:35-root-INFO: Loss too large (2069.333->2110.375)! Learning rate decreased to 0.00403.
2024-12-02-11:51:36-root-INFO: grad norm: 353.257 335.829 109.587
2024-12-02-11:51:36-root-INFO: grad norm: 389.216 366.052 132.270
2024-12-02-11:51:37-root-INFO: grad norm: 465.412 447.011 129.572
2024-12-02-11:51:37-root-INFO: Loss too large (1981.278->2037.782)! Learning rate decreased to 0.00322.
2024-12-02-11:51:37-root-INFO: grad norm: 396.469 380.234 112.294
2024-12-02-11:51:38-root-INFO: Loss Change: 2069.333 -> 1825.923
2024-12-02-11:51:38-root-INFO: Regularization Change: 0.000 -> 5.863
2024-12-02-11:51:38-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-11:51:38-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:51:38-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-11:51:38-root-INFO: grad norm: 391.401 378.506 99.641
2024-12-02-11:51:38-root-INFO: Loss too large (1821.537->2056.773)! Learning rate decreased to 0.00420.
2024-12-02-11:51:38-root-INFO: Loss too large (1821.537->1902.055)! Learning rate decreased to 0.00336.
2024-12-02-11:51:39-root-INFO: grad norm: 321.934 308.114 93.313
2024-12-02-11:51:39-root-INFO: grad norm: 272.244 262.470 72.292
2024-12-02-11:51:40-root-INFO: grad norm: 329.995 316.858 92.184
2024-12-02-11:51:40-root-INFO: Loss too large (1686.839->1747.398)! Learning rate decreased to 0.00269.
2024-12-02-11:51:40-root-INFO: grad norm: 346.437 337.471 78.305
2024-12-02-11:51:41-root-INFO: Loss Change: 1821.537 -> 1667.658
2024-12-02-11:51:41-root-INFO: Regularization Change: 0.000 -> 2.769
2024-12-02-11:51:41-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-11:51:41-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:51:41-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-11:51:41-root-INFO: grad norm: 271.225 259.582 78.616
2024-12-02-11:51:41-root-INFO: Loss too large (1651.437->1829.840)! Learning rate decreased to 0.00437.
2024-12-02-11:51:41-root-INFO: Loss too large (1651.437->1713.943)! Learning rate decreased to 0.00350.
2024-12-02-11:51:42-root-INFO: grad norm: 337.228 327.241 81.463
2024-12-02-11:51:42-root-INFO: Loss too large (1634.955->1676.147)! Learning rate decreased to 0.00280.
2024-12-02-11:51:43-root-INFO: grad norm: 258.683 246.358 78.898
2024-12-02-11:51:43-root-INFO: grad norm: 194.601 187.032 53.743
2024-12-02-11:51:44-root-INFO: grad norm: 228.929 217.934 70.093
2024-12-02-11:51:44-root-INFO: Loss too large (1548.161->1554.607)! Learning rate decreased to 0.00224.
2024-12-02-11:51:44-root-INFO: Loss Change: 1651.437 -> 1527.339
2024-12-02-11:51:44-root-INFO: Regularization Change: 0.000 -> 1.401
2024-12-02-11:51:44-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-11:51:44-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-11:51:44-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-11:51:44-root-INFO: grad norm: 275.882 266.934 69.691
2024-12-02-11:51:45-root-INFO: Loss too large (1539.661->1701.265)! Learning rate decreased to 0.00455.
2024-12-02-11:51:45-root-INFO: Loss too large (1539.661->1652.764)! Learning rate decreased to 0.00364.
2024-12-02-11:51:45-root-INFO: Loss too large (1539.661->1605.614)! Learning rate decreased to 0.00291.
2024-12-02-11:51:45-root-INFO: Loss too large (1539.661->1562.424)! Learning rate decreased to 0.00233.
2024-12-02-11:51:46-root-INFO: grad norm: 238.425 226.937 73.118
2024-12-02-11:51:46-root-INFO: grad norm: 209.929 202.743 54.456
2024-12-02-11:51:47-root-INFO: grad norm: 213.527 202.739 67.014
2024-12-02-11:51:47-root-INFO: grad norm: 216.180 208.811 55.959
2024-12-02-11:51:47-root-INFO: Loss Change: 1539.661 -> 1481.208
2024-12-02-11:51:47-root-INFO: Regularization Change: 0.000 -> 0.826
2024-12-02-11:51:47-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-11:51:47-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:51:47-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-11:51:48-root-INFO: grad norm: 183.020 174.465 55.301
2024-12-02-11:51:48-root-INFO: Loss too large (1458.081->1628.736)! Learning rate decreased to 0.00474.
2024-12-02-11:51:48-root-INFO: Loss too large (1458.081->1564.399)! Learning rate decreased to 0.00379.
2024-12-02-11:51:48-root-INFO: Loss too large (1458.081->1515.636)! Learning rate decreased to 0.00303.
2024-12-02-11:51:48-root-INFO: Loss too large (1458.081->1481.066)! Learning rate decreased to 0.00243.
2024-12-02-11:51:48-root-INFO: Loss too large (1458.081->1458.597)! Learning rate decreased to 0.00194.
2024-12-02-11:51:49-root-INFO: grad norm: 155.859 149.695 43.400
2024-12-02-11:51:49-root-INFO: grad norm: 142.357 135.687 43.065
2024-12-02-11:51:50-root-INFO: grad norm: 135.141 129.807 37.595
2024-12-02-11:51:50-root-INFO: grad norm: 131.745 125.708 39.424
2024-12-02-11:51:51-root-INFO: Loss Change: 1458.081 -> 1418.313
2024-12-02-11:51:51-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-02-11:51:51-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-11:51:51-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:51:51-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-11:51:51-root-INFO: grad norm: 158.077 151.930 43.652
2024-12-02-11:51:51-root-INFO: Loss too large (1418.712->1550.320)! Learning rate decreased to 0.00494.
2024-12-02-11:51:52-root-INFO: Loss too large (1418.712->1510.730)! Learning rate decreased to 0.00395.
2024-12-02-11:51:52-root-INFO: Loss too large (1418.712->1473.959)! Learning rate decreased to 0.00316.
2024-12-02-11:51:52-root-INFO: Loss too large (1418.712->1444.390)! Learning rate decreased to 0.00253.
2024-12-02-11:51:52-root-INFO: Loss too large (1418.712->1423.989)! Learning rate decreased to 0.00202.
2024-12-02-11:51:52-root-INFO: grad norm: 158.664 151.343 47.641
2024-12-02-11:51:53-root-INFO: grad norm: 157.626 151.664 42.944
2024-12-02-11:51:53-root-INFO: grad norm: 156.424 149.098 47.312
2024-12-02-11:51:54-root-INFO: grad norm: 154.701 148.735 42.545
2024-12-02-11:51:54-root-INFO: Loss Change: 1418.712 -> 1391.047
2024-12-02-11:51:54-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-02-11:51:54-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-11:51:54-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:51:54-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-11:51:55-root-INFO: grad norm: 74.369 72.034 18.488
2024-12-02-11:51:55-root-INFO: grad norm: 177.550 171.657 45.366
2024-12-02-11:51:55-root-INFO: Loss too large (1375.275->1534.582)! Learning rate decreased to 0.00514.
2024-12-02-11:51:55-root-INFO: Loss too large (1375.275->1492.459)! Learning rate decreased to 0.00411.
2024-12-02-11:51:55-root-INFO: Loss too large (1375.275->1449.707)! Learning rate decreased to 0.00329.
2024-12-02-11:51:56-root-INFO: Loss too large (1375.275->1412.065)! Learning rate decreased to 0.00263.
2024-12-02-11:51:56-root-INFO: Loss too large (1375.275->1384.328)! Learning rate decreased to 0.00210.
2024-12-02-11:51:56-root-INFO: grad norm: 179.345 170.851 54.540
2024-12-02-11:51:57-root-INFO: grad norm: 177.063 170.474 47.854
2024-12-02-11:51:57-root-INFO: grad norm: 171.574 163.486 52.059
2024-12-02-11:51:57-root-INFO: Loss Change: 1376.893 -> 1355.369
2024-12-02-11:51:57-root-INFO: Regularization Change: 0.000 -> 0.583
2024-12-02-11:51:57-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-11:51:57-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-11:51:58-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-11:51:58-root-INFO: grad norm: 192.767 185.529 52.327
2024-12-02-11:51:58-root-INFO: Loss too large (1358.733->1531.097)! Learning rate decreased to 0.00535.
2024-12-02-11:51:58-root-INFO: Loss too large (1358.733->1487.205)! Learning rate decreased to 0.00428.
2024-12-02-11:51:58-root-INFO: Loss too large (1358.733->1439.679)! Learning rate decreased to 0.00342.
2024-12-02-11:51:58-root-INFO: Loss too large (1358.733->1395.825)! Learning rate decreased to 0.00274.
2024-12-02-11:51:59-root-INFO: Loss too large (1358.733->1363.153)! Learning rate decreased to 0.00219.
2024-12-02-11:51:59-root-INFO: grad norm: 170.324 162.703 50.380
2024-12-02-11:52:00-root-INFO: grad norm: 159.133 152.620 45.059
2024-12-02-11:52:00-root-INFO: grad norm: 146.220 139.824 42.774
2024-12-02-11:52:00-root-INFO: grad norm: 139.863 133.801 40.731
2024-12-02-11:52:01-root-INFO: Loss Change: 1358.733 -> 1320.954
2024-12-02-11:52:01-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-02-11:52:01-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-11:52:01-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:52:01-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-11:52:01-root-INFO: grad norm: 71.398 69.473 16.469
2024-12-02-11:52:01-root-INFO: Loss too large (1306.871->1324.396)! Learning rate decreased to 0.00556.
2024-12-02-11:52:01-root-INFO: Loss too large (1306.871->1313.740)! Learning rate decreased to 0.00445.
2024-12-02-11:52:02-root-INFO: Loss too large (1306.871->1307.354)! Learning rate decreased to 0.00356.
2024-12-02-11:52:02-root-INFO: grad norm: 112.182 107.031 33.605
2024-12-02-11:52:02-root-INFO: Loss too large (1303.836->1319.998)! Learning rate decreased to 0.00285.
2024-12-02-11:52:02-root-INFO: Loss too large (1303.836->1306.418)! Learning rate decreased to 0.00228.
2024-12-02-11:52:03-root-INFO: grad norm: 119.821 115.133 33.189
2024-12-02-11:52:03-root-INFO: grad norm: 125.472 120.096 36.335
2024-12-02-11:52:04-root-INFO: grad norm: 133.786 128.316 37.865
2024-12-02-11:52:04-root-INFO: Loss Change: 1306.871 -> 1292.280
2024-12-02-11:52:04-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-02-11:52:04-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-11:52:04-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:52:04-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-11:52:04-root-INFO: grad norm: 192.076 185.081 51.365
2024-12-02-11:52:05-root-INFO: Loss too large (1303.587->1493.029)! Learning rate decreased to 0.00579.
2024-12-02-11:52:05-root-INFO: Loss too large (1303.587->1449.096)! Learning rate decreased to 0.00463.
2024-12-02-11:52:05-root-INFO: Loss too large (1303.587->1399.091)! Learning rate decreased to 0.00370.
2024-12-02-11:52:05-root-INFO: Loss too large (1303.587->1350.170)! Learning rate decreased to 0.00296.
2024-12-02-11:52:05-root-INFO: Loss too large (1303.587->1312.258)! Learning rate decreased to 0.00237.
2024-12-02-11:52:06-root-INFO: grad norm: 190.141 182.137 54.588
2024-12-02-11:52:06-root-INFO: Loss too large (1289.096->1291.873)! Learning rate decreased to 0.00190.
2024-12-02-11:52:06-root-INFO: grad norm: 136.237 130.488 39.159
2024-12-02-11:52:07-root-INFO: grad norm: 90.020 87.090 22.779
2024-12-02-11:52:07-root-INFO: grad norm: 74.108 70.224 23.676
2024-12-02-11:52:08-root-INFO: Loss Change: 1303.587 -> 1258.833
2024-12-02-11:52:08-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-11:52:08-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-11:52:08-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:52:08-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-11:52:08-root-INFO: grad norm: 51.489 48.906 16.104
2024-12-02-11:52:08-root-INFO: grad norm: 69.312 67.288 16.628
2024-12-02-11:52:09-root-INFO: Loss too large (1242.263->1272.828)! Learning rate decreased to 0.00602.
2024-12-02-11:52:09-root-INFO: Loss too large (1242.263->1258.305)! Learning rate decreased to 0.00482.
2024-12-02-11:52:09-root-INFO: Loss too large (1242.263->1249.047)! Learning rate decreased to 0.00385.
2024-12-02-11:52:09-root-INFO: Loss too large (1242.263->1243.464)! Learning rate decreased to 0.00308.
2024-12-02-11:52:10-root-INFO: grad norm: 101.449 97.366 28.491
2024-12-02-11:52:10-root-INFO: Loss too large (1240.340->1246.800)! Learning rate decreased to 0.00247.
2024-12-02-11:52:10-root-INFO: grad norm: 132.749 127.971 35.295
2024-12-02-11:52:11-root-INFO: Loss too large (1238.742->1242.871)! Learning rate decreased to 0.00197.
2024-12-02-11:52:11-root-INFO: grad norm: 115.664 111.264 31.598
2024-12-02-11:52:11-root-INFO: Loss Change: 1255.701 -> 1228.943
2024-12-02-11:52:11-root-INFO: Regularization Change: 0.000 -> 0.521
2024-12-02-11:52:11-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-11:52:11-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:52:12-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-11:52:12-root-INFO: grad norm: 46.276 44.800 11.593
2024-12-02-11:52:12-root-INFO: grad norm: 101.069 97.255 27.504
2024-12-02-11:52:12-root-INFO: Loss too large (1216.190->1349.535)! Learning rate decreased to 0.00626.
2024-12-02-11:52:12-root-INFO: Loss too large (1216.190->1304.989)! Learning rate decreased to 0.00501.
2024-12-02-11:52:13-root-INFO: Loss too large (1216.190->1267.537)! Learning rate decreased to 0.00401.
2024-12-02-11:52:13-root-INFO: Loss too large (1216.190->1241.031)! Learning rate decreased to 0.00321.
2024-12-02-11:52:13-root-INFO: Loss too large (1216.190->1224.642)! Learning rate decreased to 0.00256.
2024-12-02-11:52:13-root-INFO: grad norm: 142.525 137.435 37.748
2024-12-02-11:52:14-root-INFO: Loss too large (1215.553->1223.551)! Learning rate decreased to 0.00205.
2024-12-02-11:52:14-root-INFO: grad norm: 129.563 125.129 33.605
2024-12-02-11:52:15-root-INFO: grad norm: 113.286 109.550 28.851
2024-12-02-11:52:15-root-INFO: Loss Change: 1223.908 -> 1204.423
2024-12-02-11:52:15-root-INFO: Regularization Change: 0.000 -> 0.440
2024-12-02-11:52:15-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-11:52:15-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-11:52:15-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-11:52:15-root-INFO: grad norm: 155.482 150.032 40.806
2024-12-02-11:52:15-root-INFO: Loss too large (1212.676->1417.105)! Learning rate decreased to 0.00651.
2024-12-02-11:52:16-root-INFO: Loss too large (1212.676->1372.477)! Learning rate decreased to 0.00521.
2024-12-02-11:52:16-root-INFO: Loss too large (1212.676->1320.846)! Learning rate decreased to 0.00417.
2024-12-02-11:52:16-root-INFO: Loss too large (1212.676->1271.285)! Learning rate decreased to 0.00333.
2024-12-02-11:52:16-root-INFO: Loss too large (1212.676->1233.771)! Learning rate decreased to 0.00267.
2024-12-02-11:52:17-root-INFO: grad norm: 208.657 200.783 56.778
2024-12-02-11:52:17-root-INFO: Loss too large (1210.784->1228.773)! Learning rate decreased to 0.00213.
2024-12-02-11:52:17-root-INFO: grad norm: 168.123 162.855 41.756
2024-12-02-11:52:18-root-INFO: grad norm: 113.805 110.265 28.163
2024-12-02-11:52:18-root-INFO: grad norm: 103.039 98.746 29.433
2024-12-02-11:52:19-root-INFO: Loss Change: 1212.676 -> 1181.818
2024-12-02-11:52:19-root-INFO: Regularization Change: 0.000 -> 0.288
2024-12-02-11:52:19-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-11:52:19-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:52:19-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-11:52:19-root-INFO: grad norm: 50.546 48.316 14.847
2024-12-02-11:52:19-root-INFO: grad norm: 71.306 68.236 20.698
2024-12-02-11:52:20-root-INFO: Loss too large (1163.527->1217.300)! Learning rate decreased to 0.00677.
2024-12-02-11:52:20-root-INFO: Loss too large (1163.527->1190.348)! Learning rate decreased to 0.00542.
2024-12-02-11:52:20-root-INFO: Loss too large (1163.527->1174.308)! Learning rate decreased to 0.00433.
2024-12-02-11:52:20-root-INFO: Loss too large (1163.527->1165.395)! Learning rate decreased to 0.00347.
2024-12-02-11:52:20-root-INFO: grad norm: 115.691 112.157 28.374
2024-12-02-11:52:21-root-INFO: Loss too large (1160.821->1178.146)! Learning rate decreased to 0.00277.
2024-12-02-11:52:21-root-INFO: Loss too large (1160.821->1166.864)! Learning rate decreased to 0.00222.
2024-12-02-11:52:21-root-INFO: grad norm: 115.486 111.738 29.181
2024-12-02-11:52:22-root-INFO: grad norm: 118.398 114.804 28.950
2024-12-02-11:52:22-root-INFO: Loss too large (1154.910->1154.918)! Learning rate decreased to 0.00177.
2024-12-02-11:52:22-root-INFO: Loss Change: 1175.094 -> 1150.608
2024-12-02-11:52:22-root-INFO: Regularization Change: 0.000 -> 0.515
2024-12-02-11:52:22-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-11:52:22-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:52:22-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-11:52:23-root-INFO: grad norm: 118.810 114.699 30.983
2024-12-02-11:52:23-root-INFO: Loss too large (1154.848->1341.144)! Learning rate decreased to 0.00704.
2024-12-02-11:52:23-root-INFO: Loss too large (1154.848->1292.932)! Learning rate decreased to 0.00563.
2024-12-02-11:52:23-root-INFO: Loss too large (1154.848->1243.119)! Learning rate decreased to 0.00450.
2024-12-02-11:52:23-root-INFO: Loss too large (1154.848->1201.628)! Learning rate decreased to 0.00360.
2024-12-02-11:52:23-root-INFO: Loss too large (1154.848->1173.304)! Learning rate decreased to 0.00288.
2024-12-02-11:52:24-root-INFO: Loss too large (1154.848->1156.757)! Learning rate decreased to 0.00231.
2024-12-02-11:52:24-root-INFO: grad norm: 119.237 115.865 28.158
2024-12-02-11:52:24-root-INFO: grad norm: 118.645 114.718 30.273
2024-12-02-11:52:25-root-INFO: grad norm: 117.902 114.481 28.198
2024-12-02-11:52:25-root-INFO: grad norm: 117.505 113.663 29.804
2024-12-02-11:52:26-root-INFO: Loss Change: 1154.848 -> 1135.686
2024-12-02-11:52:26-root-INFO: Regularization Change: 0.000 -> 0.255
2024-12-02-11:52:26-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-11:52:26-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:52:26-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-11:52:26-root-INFO: grad norm: 58.452 56.569 14.714
2024-12-02-11:52:26-root-INFO: Loss too large (1128.865->1142.048)! Learning rate decreased to 0.00732.
2024-12-02-11:52:26-root-INFO: Loss too large (1128.865->1133.614)! Learning rate decreased to 0.00585.
2024-12-02-11:52:27-root-INFO: grad norm: 123.945 119.923 31.320
2024-12-02-11:52:27-root-INFO: Loss too large (1128.586->1232.962)! Learning rate decreased to 0.00468.
2024-12-02-11:52:27-root-INFO: Loss too large (1128.586->1185.658)! Learning rate decreased to 0.00375.
2024-12-02-11:52:27-root-INFO: Loss too large (1128.586->1152.083)! Learning rate decreased to 0.00300.
2024-12-02-11:52:28-root-INFO: Loss too large (1128.586->1132.077)! Learning rate decreased to 0.00240.
2024-12-02-11:52:28-root-INFO: grad norm: 127.053 123.371 30.365
2024-12-02-11:52:28-root-INFO: Loss too large (1121.786->1123.225)! Learning rate decreased to 0.00192.
2024-12-02-11:52:29-root-INFO: grad norm: 96.145 92.689 25.544
2024-12-02-11:52:29-root-INFO: grad norm: 66.005 64.407 14.434
2024-12-02-11:52:29-root-INFO: Loss Change: 1128.865 -> 1108.361
2024-12-02-11:52:29-root-INFO: Regularization Change: 0.000 -> 0.296
2024-12-02-11:52:29-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-11:52:29-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-11:52:30-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-11:52:30-root-INFO: grad norm: 97.005 93.576 25.564
2024-12-02-11:52:30-root-INFO: Loss too large (1108.838->1276.533)! Learning rate decreased to 0.00760.
2024-12-02-11:52:30-root-INFO: Loss too large (1108.838->1227.213)! Learning rate decreased to 0.00608.
2024-12-02-11:52:30-root-INFO: Loss too large (1108.838->1181.172)! Learning rate decreased to 0.00487.
2024-12-02-11:52:30-root-INFO: Loss too large (1108.838->1146.293)! Learning rate decreased to 0.00389.
2024-12-02-11:52:31-root-INFO: Loss too large (1108.838->1123.921)! Learning rate decreased to 0.00311.
2024-12-02-11:52:31-root-INFO: Loss too large (1108.838->1111.240)! Learning rate decreased to 0.00249.
2024-12-02-11:52:31-root-INFO: grad norm: 110.055 107.352 24.242
2024-12-02-11:52:31-root-INFO: Loss too large (1104.807->1105.463)! Learning rate decreased to 0.00199.
2024-12-02-11:52:32-root-INFO: grad norm: 87.413 84.299 23.124
2024-12-02-11:52:32-root-INFO: grad norm: 64.446 63.002 13.563
2024-12-02-11:52:33-root-INFO: grad norm: 55.695 53.250 16.322
2024-12-02-11:52:33-root-INFO: Loss Change: 1108.838 -> 1089.786
2024-12-02-11:52:33-root-INFO: Regularization Change: 0.000 -> 0.170
2024-12-02-11:52:33-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-11:52:33-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:52:33-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-11:52:33-root-INFO: grad norm: 64.921 61.596 20.509
2024-12-02-11:52:34-root-INFO: Loss too large (1095.537->1129.532)! Learning rate decreased to 0.00790.
2024-12-02-11:52:34-root-INFO: Loss too large (1095.537->1110.423)! Learning rate decreased to 0.00632.
2024-12-02-11:52:34-root-INFO: Loss too large (1095.537->1099.304)! Learning rate decreased to 0.00506.
2024-12-02-11:52:34-root-INFO: grad norm: 146.564 142.695 33.455
2024-12-02-11:52:35-root-INFO: Loss too large (1093.358->1159.071)! Learning rate decreased to 0.00404.
2024-12-02-11:52:35-root-INFO: Loss too large (1093.358->1131.109)! Learning rate decreased to 0.00324.
2024-12-02-11:52:35-root-INFO: Loss too large (1093.358->1111.223)! Learning rate decreased to 0.00259.
2024-12-02-11:52:35-root-INFO: Loss too large (1093.358->1097.865)! Learning rate decreased to 0.00207.
2024-12-02-11:52:36-root-INFO: grad norm: 113.255 109.810 27.722
2024-12-02-11:52:36-root-INFO: grad norm: 74.841 73.172 15.715
2024-12-02-11:52:36-root-INFO: grad norm: 64.211 61.530 18.360
2024-12-02-11:52:37-root-INFO: Loss Change: 1095.537 -> 1073.271
2024-12-02-11:52:37-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-02-11:52:37-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-11:52:37-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:52:37-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-11:52:37-root-INFO: grad norm: 47.642 45.302 14.747
2024-12-02-11:52:37-root-INFO: Loss too large (1070.235->1075.179)! Learning rate decreased to 0.00821.
2024-12-02-11:52:38-root-INFO: grad norm: 155.645 151.490 35.723
2024-12-02-11:52:38-root-INFO: Loss too large (1069.304->1240.212)! Learning rate decreased to 0.00656.
2024-12-02-11:52:38-root-INFO: Loss too large (1069.304->1185.304)! Learning rate decreased to 0.00525.
2024-12-02-11:52:38-root-INFO: Loss too large (1069.304->1143.866)! Learning rate decreased to 0.00420.
2024-12-02-11:52:38-root-INFO: Loss too large (1069.304->1112.990)! Learning rate decreased to 0.00336.
2024-12-02-11:52:39-root-INFO: Loss too large (1069.304->1090.736)! Learning rate decreased to 0.00269.
2024-12-02-11:52:39-root-INFO: Loss too large (1069.304->1075.511)! Learning rate decreased to 0.00215.
2024-12-02-11:52:39-root-INFO: grad norm: 117.126 114.202 26.009
2024-12-02-11:52:40-root-INFO: grad norm: 71.125 69.556 14.857
2024-12-02-11:52:40-root-INFO: grad norm: 61.065 58.801 16.474
2024-12-02-11:52:40-root-INFO: Loss Change: 1070.235 -> 1048.211
2024-12-02-11:52:40-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-02-11:52:40-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-11:52:40-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:52:41-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-11:52:41-root-INFO: grad norm: 46.977 44.635 14.648
2024-12-02-11:52:41-root-INFO: Loss too large (1049.476->1051.906)! Learning rate decreased to 0.00852.
2024-12-02-11:52:41-root-INFO: grad norm: 142.296 138.808 31.315
2024-12-02-11:52:42-root-INFO: Loss too large (1047.024->1202.455)! Learning rate decreased to 0.00682.
2024-12-02-11:52:42-root-INFO: Loss too large (1047.024->1150.772)! Learning rate decreased to 0.00545.
2024-12-02-11:52:42-root-INFO: Loss too large (1047.024->1112.537)! Learning rate decreased to 0.00436.
2024-12-02-11:52:42-root-INFO: Loss too large (1047.024->1084.614)! Learning rate decreased to 0.00349.
2024-12-02-11:52:42-root-INFO: Loss too large (1047.024->1064.879)! Learning rate decreased to 0.00279.
2024-12-02-11:52:42-root-INFO: Loss too large (1047.024->1051.626)! Learning rate decreased to 0.00223.
2024-12-02-11:52:43-root-INFO: grad norm: 108.921 106.172 24.318
2024-12-02-11:52:43-root-INFO: grad norm: 70.063 68.609 14.200
2024-12-02-11:52:44-root-INFO: grad norm: 60.785 58.577 16.235
2024-12-02-11:52:44-root-INFO: Loss Change: 1049.476 -> 1027.148
2024-12-02-11:52:44-root-INFO: Regularization Change: 0.000 -> 0.404
2024-12-02-11:52:44-root-INFO: Undo step: 160
2024-12-02-11:52:44-root-INFO: Undo step: 161
2024-12-02-11:52:44-root-INFO: Undo step: 162
2024-12-02-11:52:44-root-INFO: Undo step: 163
2024-12-02-11:52:44-root-INFO: Undo step: 164
2024-12-02-11:52:44-root-INFO: Undo step: 165
2024-12-02-11:52:44-root-INFO: Undo step: 166
2024-12-02-11:52:44-root-INFO: Undo step: 167
2024-12-02-11:52:44-root-INFO: Undo step: 168
2024-12-02-11:52:44-root-INFO: Undo step: 169
2024-12-02-11:52:44-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-11:52:45-root-INFO: grad norm: 707.251 686.051 171.867
2024-12-02-11:52:45-root-INFO: grad norm: 1094.432 1068.199 238.184
2024-12-02-11:52:45-root-INFO: grad norm: 658.636 638.847 160.235
2024-12-02-11:52:46-root-INFO: grad norm: 331.015 323.625 69.554
2024-12-02-11:52:46-root-INFO: grad norm: 203.323 197.550 48.104
2024-12-02-11:52:47-root-INFO: Loss Change: 3179.667 -> 1621.029
2024-12-02-11:52:47-root-INFO: Regularization Change: 0.000 -> 55.521
2024-12-02-11:52:47-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-11:52:47-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:52:47-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-11:52:47-root-INFO: grad norm: 157.271 152.324 39.135
2024-12-02-11:52:47-root-INFO: Loss too large (1618.692->1627.007)! Learning rate decreased to 0.00602.
2024-12-02-11:52:48-root-INFO: grad norm: 318.115 314.552 47.473
2024-12-02-11:52:48-root-INFO: Loss too large (1610.403->2020.381)! Learning rate decreased to 0.00482.
2024-12-02-11:52:48-root-INFO: Loss too large (1610.403->1854.838)! Learning rate decreased to 0.00385.
2024-12-02-11:52:48-root-INFO: Loss too large (1610.403->1733.391)! Learning rate decreased to 0.00308.
2024-12-02-11:52:48-root-INFO: Loss too large (1610.403->1644.317)! Learning rate decreased to 0.00247.
2024-12-02-11:52:49-root-INFO: grad norm: 345.299 342.047 47.277
2024-12-02-11:52:49-root-INFO: Loss too large (1581.141->1592.348)! Learning rate decreased to 0.00197.
2024-12-02-11:52:49-root-INFO: grad norm: 310.701 307.186 46.606
2024-12-02-11:52:50-root-INFO: grad norm: 302.141 299.237 41.786
2024-12-02-11:52:50-root-INFO: Loss Change: 1618.692 -> 1516.056
2024-12-02-11:52:50-root-INFO: Regularization Change: 0.000 -> 2.237
2024-12-02-11:52:50-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-11:52:50-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:52:50-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-11:52:51-root-INFO: grad norm: 299.805 296.643 43.425
2024-12-02-11:52:51-root-INFO: Loss too large (1499.442->2150.952)! Learning rate decreased to 0.00626.
2024-12-02-11:52:51-root-INFO: Loss too large (1499.442->1955.807)! Learning rate decreased to 0.00501.
2024-12-02-11:52:51-root-INFO: Loss too large (1499.442->1811.213)! Learning rate decreased to 0.00401.
2024-12-02-11:52:51-root-INFO: Loss too large (1499.442->1703.213)! Learning rate decreased to 0.00321.
2024-12-02-11:52:51-root-INFO: Loss too large (1499.442->1621.924)! Learning rate decreased to 0.00256.
2024-12-02-11:52:52-root-INFO: Loss too large (1499.442->1560.941)! Learning rate decreased to 0.00205.
2024-12-02-11:52:52-root-INFO: Loss too large (1499.442->1516.470)! Learning rate decreased to 0.00164.
2024-12-02-11:52:52-root-INFO: grad norm: 258.693 256.093 36.582
2024-12-02-11:52:53-root-INFO: grad norm: 242.518 239.641 37.248
2024-12-02-11:52:53-root-INFO: grad norm: 242.441 240.055 33.930
2024-12-02-11:52:54-root-INFO: grad norm: 248.405 245.534 37.661
2024-12-02-11:52:54-root-INFO: Loss too large (1444.136->1446.780)! Learning rate decreased to 0.00131.
2024-12-02-11:52:54-root-INFO: Loss Change: 1499.442 -> 1431.847
2024-12-02-11:52:54-root-INFO: Regularization Change: 0.000 -> 0.492
2024-12-02-11:52:54-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-11:52:54-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-11:52:54-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-11:52:54-root-INFO: grad norm: 239.040 236.303 36.066
2024-12-02-11:52:55-root-INFO: Loss too large (1438.219->1657.833)! Learning rate decreased to 0.00651.
2024-12-02-11:52:55-root-INFO: Loss too large (1438.219->1638.153)! Learning rate decreased to 0.00521.
2024-12-02-11:52:55-root-INFO: Loss too large (1438.219->1611.674)! Learning rate decreased to 0.00417.
2024-12-02-11:52:55-root-INFO: Loss too large (1438.219->1576.752)! Learning rate decreased to 0.00333.
2024-12-02-11:52:55-root-INFO: Loss too large (1438.219->1534.522)! Learning rate decreased to 0.00267.
2024-12-02-11:52:55-root-INFO: Loss too large (1438.219->1489.799)! Learning rate decreased to 0.00213.
2024-12-02-11:52:55-root-INFO: Loss too large (1438.219->1450.854)! Learning rate decreased to 0.00171.
2024-12-02-11:52:56-root-INFO: grad norm: 266.799 264.058 38.146
2024-12-02-11:52:56-root-INFO: Loss too large (1424.219->1431.772)! Learning rate decreased to 0.00137.
2024-12-02-11:52:57-root-INFO: grad norm: 211.471 209.144 31.288
2024-12-02-11:52:57-root-INFO: grad norm: 164.684 162.278 28.046
2024-12-02-11:52:57-root-INFO: grad norm: 155.024 152.821 26.041
2024-12-02-11:52:58-root-INFO: Loss Change: 1438.219 -> 1379.515
2024-12-02-11:52:58-root-INFO: Regularization Change: 0.000 -> 0.350
2024-12-02-11:52:58-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-11:52:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:52:58-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-11:52:58-root-INFO: grad norm: 84.511 81.540 22.210
2024-12-02-11:52:58-root-INFO: Loss too large (1368.621->1382.331)! Learning rate decreased to 0.00677.
2024-12-02-11:52:58-root-INFO: Loss too large (1368.621->1370.959)! Learning rate decreased to 0.00542.
2024-12-02-11:52:59-root-INFO: grad norm: 215.287 213.485 27.796
2024-12-02-11:52:59-root-INFO: Loss too large (1364.065->1551.194)! Learning rate decreased to 0.00433.
2024-12-02-11:52:59-root-INFO: Loss too large (1364.065->1517.025)! Learning rate decreased to 0.00347.
2024-12-02-11:52:59-root-INFO: Loss too large (1364.065->1474.650)! Learning rate decreased to 0.00277.
2024-12-02-11:52:59-root-INFO: Loss too large (1364.065->1428.709)! Learning rate decreased to 0.00222.
2024-12-02-11:53:00-root-INFO: Loss too large (1364.065->1387.965)! Learning rate decreased to 0.00177.
2024-12-02-11:53:00-root-INFO: grad norm: 288.244 285.532 39.443
2024-12-02-11:53:00-root-INFO: Loss too large (1359.707->1382.065)! Learning rate decreased to 0.00142.
2024-12-02-11:53:01-root-INFO: grad norm: 232.151 230.344 28.906
2024-12-02-11:53:01-root-INFO: grad norm: 163.118 161.001 26.199
2024-12-02-11:53:01-root-INFO: Loss Change: 1368.621 -> 1328.946
2024-12-02-11:53:01-root-INFO: Regularization Change: 0.000 -> 0.586
2024-12-02-11:53:01-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-11:53:01-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:53:02-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-11:53:02-root-INFO: grad norm: 196.609 194.742 27.036
2024-12-02-11:53:02-root-INFO: Loss too large (1334.847->1570.484)! Learning rate decreased to 0.00704.
2024-12-02-11:53:02-root-INFO: Loss too large (1334.847->1551.957)! Learning rate decreased to 0.00563.
2024-12-02-11:53:02-root-INFO: Loss too large (1334.847->1525.430)! Learning rate decreased to 0.00450.
2024-12-02-11:53:02-root-INFO: Loss too large (1334.847->1489.911)! Learning rate decreased to 0.00360.
2024-12-02-11:53:03-root-INFO: Loss too large (1334.847->1446.575)! Learning rate decreased to 0.00288.
2024-12-02-11:53:03-root-INFO: Loss too large (1334.847->1400.691)! Learning rate decreased to 0.00231.
2024-12-02-11:53:03-root-INFO: Loss too large (1334.847->1361.194)! Learning rate decreased to 0.00184.
2024-12-02-11:53:03-root-INFO: grad norm: 289.914 287.363 38.380
2024-12-02-11:53:03-root-INFO: Loss too large (1334.422->1361.528)! Learning rate decreased to 0.00148.
2024-12-02-11:53:04-root-INFO: Loss too large (1334.422->1337.381)! Learning rate decreased to 0.00118.
2024-12-02-11:53:04-root-INFO: grad norm: 186.667 184.941 25.325
2024-12-02-11:53:05-root-INFO: grad norm: 95.388 93.322 19.748
2024-12-02-11:53:05-root-INFO: grad norm: 81.736 79.691 18.170
2024-12-02-11:53:05-root-INFO: Loss Change: 1334.847 -> 1293.763
2024-12-02-11:53:05-root-INFO: Regularization Change: 0.000 -> 0.197
2024-12-02-11:53:05-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-11:53:05-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:53:06-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-11:53:06-root-INFO: grad norm: 82.283 79.810 20.018
2024-12-02-11:53:06-root-INFO: Loss too large (1291.673->1378.278)! Learning rate decreased to 0.00732.
2024-12-02-11:53:06-root-INFO: Loss too large (1291.673->1345.319)! Learning rate decreased to 0.00585.
2024-12-02-11:53:06-root-INFO: Loss too large (1291.673->1320.261)! Learning rate decreased to 0.00468.
2024-12-02-11:53:06-root-INFO: Loss too large (1291.673->1303.647)! Learning rate decreased to 0.00375.
2024-12-02-11:53:06-root-INFO: Loss too large (1291.673->1293.817)! Learning rate decreased to 0.00300.
2024-12-02-11:53:07-root-INFO: grad norm: 195.245 193.255 27.803
2024-12-02-11:53:07-root-INFO: Loss too large (1288.595->1350.937)! Learning rate decreased to 0.00240.
2024-12-02-11:53:07-root-INFO: Loss too large (1288.595->1324.015)! Learning rate decreased to 0.00192.
2024-12-02-11:53:07-root-INFO: Loss too large (1288.595->1304.676)! Learning rate decreased to 0.00153.
2024-12-02-11:53:08-root-INFO: Loss too large (1288.595->1291.563)! Learning rate decreased to 0.00123.
2024-12-02-11:53:08-root-INFO: grad norm: 150.322 148.652 22.341
2024-12-02-11:53:08-root-INFO: grad norm: 101.270 99.404 19.351
2024-12-02-11:53:09-root-INFO: grad norm: 88.530 86.704 17.890
2024-12-02-11:53:09-root-INFO: Loss Change: 1291.673 -> 1263.639
2024-12-02-11:53:09-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-02-11:53:09-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-11:53:09-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-11:53:09-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-11:53:10-root-INFO: grad norm: 63.102 60.782 16.954
2024-12-02-11:53:10-root-INFO: grad norm: 312.269 309.711 39.888
2024-12-02-11:53:10-root-INFO: Loss too large (1257.131->2048.217)! Learning rate decreased to 0.00760.
2024-12-02-11:53:10-root-INFO: Loss too large (1257.131->1829.863)! Learning rate decreased to 0.00608.
2024-12-02-11:53:10-root-INFO: Loss too large (1257.131->1666.467)! Learning rate decreased to 0.00487.
2024-12-02-11:53:11-root-INFO: Loss too large (1257.131->1545.234)! Learning rate decreased to 0.00389.
2024-12-02-11:53:11-root-INFO: Loss too large (1257.131->1455.397)! Learning rate decreased to 0.00311.
2024-12-02-11:53:11-root-INFO: Loss too large (1257.131->1388.170)! Learning rate decreased to 0.00249.
2024-12-02-11:53:11-root-INFO: Loss too large (1257.131->1337.338)! Learning rate decreased to 0.00199.
2024-12-02-11:53:11-root-INFO: Loss too large (1257.131->1298.964)! Learning rate decreased to 0.00159.
2024-12-02-11:53:11-root-INFO: Loss too large (1257.131->1270.664)! Learning rate decreased to 0.00128.
2024-12-02-11:53:12-root-INFO: grad norm: 199.734 198.325 23.677
2024-12-02-11:53:12-root-INFO: grad norm: 71.282 69.376 16.373
2024-12-02-11:53:13-root-INFO: grad norm: 65.143 63.301 15.382
2024-12-02-11:53:13-root-INFO: Loss Change: 1259.037 -> 1217.135
2024-12-02-11:53:13-root-INFO: Regularization Change: 0.000 -> 0.637
2024-12-02-11:53:13-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-11:53:13-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:53:13-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-11:53:13-root-INFO: grad norm: 75.887 73.778 17.767
2024-12-02-11:53:14-root-INFO: Loss too large (1217.829->1324.047)! Learning rate decreased to 0.00790.
2024-12-02-11:53:14-root-INFO: Loss too large (1217.829->1286.477)! Learning rate decreased to 0.00632.
2024-12-02-11:53:14-root-INFO: Loss too large (1217.829->1256.861)! Learning rate decreased to 0.00506.
2024-12-02-11:53:14-root-INFO: Loss too large (1217.829->1236.727)! Learning rate decreased to 0.00404.
2024-12-02-11:53:14-root-INFO: Loss too large (1217.829->1224.536)! Learning rate decreased to 0.00324.
2024-12-02-11:53:15-root-INFO: grad norm: 205.422 203.713 26.446
2024-12-02-11:53:15-root-INFO: Loss too large (1217.821->1293.520)! Learning rate decreased to 0.00259.
2024-12-02-11:53:15-root-INFO: Loss too large (1217.821->1263.529)! Learning rate decreased to 0.00207.
2024-12-02-11:53:15-root-INFO: Loss too large (1217.821->1241.421)! Learning rate decreased to 0.00166.
2024-12-02-11:53:15-root-INFO: Loss too large (1217.821->1225.831)! Learning rate decreased to 0.00133.
2024-12-02-11:53:16-root-INFO: grad norm: 158.015 156.521 21.684
2024-12-02-11:53:16-root-INFO: grad norm: 94.927 93.467 16.588
2024-12-02-11:53:17-root-INFO: grad norm: 84.866 83.330 16.071
2024-12-02-11:53:17-root-INFO: Loss Change: 1217.829 -> 1194.695
2024-12-02-11:53:17-root-INFO: Regularization Change: 0.000 -> 0.205
2024-12-02-11:53:17-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-11:53:17-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:53:17-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-11:53:17-root-INFO: grad norm: 60.338 58.372 15.276
2024-12-02-11:53:18-root-INFO: Loss too large (1191.806->1222.580)! Learning rate decreased to 0.00821.
2024-12-02-11:53:18-root-INFO: Loss too large (1191.806->1205.199)! Learning rate decreased to 0.00656.
2024-12-02-11:53:18-root-INFO: Loss too large (1191.806->1194.982)! Learning rate decreased to 0.00525.
2024-12-02-11:53:18-root-INFO: grad norm: 205.357 203.651 26.415
2024-12-02-11:53:19-root-INFO: Loss too large (1189.550->1356.489)! Learning rate decreased to 0.00420.
2024-12-02-11:53:19-root-INFO: Loss too large (1189.550->1304.273)! Learning rate decreased to 0.00336.
2024-12-02-11:53:19-root-INFO: Loss too large (1189.550->1264.809)! Learning rate decreased to 0.00269.
2024-12-02-11:53:19-root-INFO: Loss too large (1189.550->1235.094)! Learning rate decreased to 0.00215.
2024-12-02-11:53:19-root-INFO: Loss too large (1189.550->1213.181)! Learning rate decreased to 0.00172.
2024-12-02-11:53:19-root-INFO: Loss too large (1189.550->1197.661)! Learning rate decreased to 0.00138.
2024-12-02-11:53:20-root-INFO: grad norm: 150.737 149.468 19.519
2024-12-02-11:53:20-root-INFO: grad norm: 79.271 77.787 15.268
2024-12-02-11:53:21-root-INFO: grad norm: 70.365 68.889 14.338
2024-12-02-11:53:21-root-INFO: Loss Change: 1191.806 -> 1166.857
2024-12-02-11:53:21-root-INFO: Regularization Change: 0.000 -> 0.277
2024-12-02-11:53:21-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-11:53:21-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:53:21-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-11:53:22-root-INFO: grad norm: 60.196 58.324 14.897
2024-12-02-11:53:22-root-INFO: Loss too large (1165.956->1219.303)! Learning rate decreased to 0.00852.
2024-12-02-11:53:22-root-INFO: Loss too large (1165.956->1194.260)! Learning rate decreased to 0.00682.
2024-12-02-11:53:22-root-INFO: Loss too large (1165.956->1178.302)! Learning rate decreased to 0.00545.
2024-12-02-11:53:22-root-INFO: Loss too large (1165.956->1169.132)! Learning rate decreased to 0.00436.
2024-12-02-11:53:23-root-INFO: grad norm: 183.891 182.380 23.524
2024-12-02-11:53:23-root-INFO: Loss too large (1164.326->1262.874)! Learning rate decreased to 0.00349.
2024-12-02-11:53:23-root-INFO: Loss too large (1164.326->1228.837)! Learning rate decreased to 0.00279.
2024-12-02-11:53:23-root-INFO: Loss too large (1164.326->1203.280)! Learning rate decreased to 0.00223.
2024-12-02-11:53:23-root-INFO: Loss too large (1164.326->1184.535)! Learning rate decreased to 0.00179.
2024-12-02-11:53:24-root-INFO: Loss too large (1164.326->1171.352)! Learning rate decreased to 0.00143.
2024-12-02-11:53:24-root-INFO: grad norm: 139.321 138.048 18.784
2024-12-02-11:53:25-root-INFO: grad norm: 78.081 76.709 14.573
2024-12-02-11:53:25-root-INFO: grad norm: 69.667 68.238 14.040
2024-12-02-11:53:25-root-INFO: Loss Change: 1165.956 -> 1144.196
2024-12-02-11:53:25-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-02-11:53:25-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-11:53:25-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:53:25-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-11:53:26-root-INFO: grad norm: 55.425 53.604 14.092
2024-12-02-11:53:26-root-INFO: Loss too large (1142.453->1176.851)! Learning rate decreased to 0.00885.
2024-12-02-11:53:26-root-INFO: Loss too large (1142.453->1158.309)! Learning rate decreased to 0.00708.
2024-12-02-11:53:26-root-INFO: Loss too large (1142.453->1147.401)! Learning rate decreased to 0.00567.
2024-12-02-11:53:27-root-INFO: grad norm: 201.091 199.519 25.094
2024-12-02-11:53:27-root-INFO: Loss too large (1141.556->1304.419)! Learning rate decreased to 0.00453.
2024-12-02-11:53:27-root-INFO: Loss too large (1141.556->1253.499)! Learning rate decreased to 0.00363.
2024-12-02-11:53:27-root-INFO: Loss too large (1141.556->1215.351)! Learning rate decreased to 0.00290.
2024-12-02-11:53:27-root-INFO: Loss too large (1141.556->1186.726)! Learning rate decreased to 0.00232.
2024-12-02-11:53:27-root-INFO: Loss too large (1141.556->1165.548)! Learning rate decreased to 0.00186.
2024-12-02-11:53:28-root-INFO: Loss too large (1141.556->1150.398)! Learning rate decreased to 0.00149.
2024-12-02-11:53:28-root-INFO: grad norm: 141.753 140.577 18.220
2024-12-02-11:53:28-root-INFO: grad norm: 62.256 60.738 13.665
2024-12-02-11:53:29-root-INFO: grad norm: 56.098 54.540 13.129
2024-12-02-11:53:29-root-INFO: Loss Change: 1142.453 -> 1119.567
2024-12-02-11:53:29-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-11:53:29-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-11:53:29-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-11:53:29-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-11:53:30-root-INFO: grad norm: 64.596 62.803 15.111
2024-12-02-11:53:30-root-INFO: Loss too large (1120.177->1218.705)! Learning rate decreased to 0.00919.
2024-12-02-11:53:30-root-INFO: Loss too large (1120.177->1180.511)! Learning rate decreased to 0.00735.
2024-12-02-11:53:30-root-INFO: Loss too large (1120.177->1152.397)! Learning rate decreased to 0.00588.
2024-12-02-11:53:30-root-INFO: Loss too large (1120.177->1134.553)! Learning rate decreased to 0.00471.
2024-12-02-11:53:30-root-INFO: Loss too large (1120.177->1124.383)! Learning rate decreased to 0.00376.
2024-12-02-11:53:31-root-INFO: grad norm: 170.514 169.161 21.439
2024-12-02-11:53:31-root-INFO: Loss too large (1119.080->1175.457)! Learning rate decreased to 0.00301.
2024-12-02-11:53:31-root-INFO: Loss too large (1119.080->1152.810)! Learning rate decreased to 0.00241.
2024-12-02-11:53:31-root-INFO: Loss too large (1119.080->1136.288)! Learning rate decreased to 0.00193.
2024-12-02-11:53:32-root-INFO: Loss too large (1119.080->1124.708)! Learning rate decreased to 0.00154.
2024-12-02-11:53:32-root-INFO: grad norm: 124.815 123.590 17.441
2024-12-02-11:53:33-root-INFO: grad norm: 63.680 62.267 13.341
2024-12-02-11:53:33-root-INFO: grad norm: 56.577 55.048 13.064
2024-12-02-11:53:33-root-INFO: Loss Change: 1120.177 -> 1099.691
2024-12-02-11:53:33-root-INFO: Regularization Change: 0.000 -> 0.204
2024-12-02-11:53:33-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-11:53:33-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:53:34-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-11:53:34-root-INFO: grad norm: 63.403 61.455 15.599
2024-12-02-11:53:34-root-INFO: Loss too large (1100.958->1205.849)! Learning rate decreased to 0.00954.
2024-12-02-11:53:34-root-INFO: Loss too large (1100.958->1166.096)! Learning rate decreased to 0.00763.
2024-12-02-11:53:34-root-INFO: Loss too large (1100.958->1136.141)! Learning rate decreased to 0.00611.
2024-12-02-11:53:34-root-INFO: Loss too large (1100.958->1116.951)! Learning rate decreased to 0.00489.
2024-12-02-11:53:35-root-INFO: Loss too large (1100.958->1105.993)! Learning rate decreased to 0.00391.
2024-12-02-11:53:35-root-INFO: grad norm: 172.005 170.656 21.505
2024-12-02-11:53:35-root-INFO: Loss too large (1100.276->1157.208)! Learning rate decreased to 0.00313.
2024-12-02-11:53:35-root-INFO: Loss too large (1100.276->1134.292)! Learning rate decreased to 0.00250.
2024-12-02-11:53:36-root-INFO: Loss too large (1100.276->1117.574)! Learning rate decreased to 0.00200.
2024-12-02-11:53:36-root-INFO: Loss too large (1100.276->1105.828)! Learning rate decreased to 0.00160.
2024-12-02-11:53:36-root-INFO: grad norm: 122.088 120.806 17.643
2024-12-02-11:53:37-root-INFO: grad norm: 57.167 55.650 13.081
2024-12-02-11:53:37-root-INFO: grad norm: 51.064 49.396 12.944
2024-12-02-11:53:37-root-INFO: Loss Change: 1100.958 -> 1080.129
2024-12-02-11:53:37-root-INFO: Regularization Change: 0.000 -> 0.214
2024-12-02-11:53:37-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-11:53:37-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:53:38-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-11:53:38-root-INFO: grad norm: 54.650 52.929 13.609
2024-12-02-11:53:38-root-INFO: Loss too large (1079.858->1148.761)! Learning rate decreased to 0.00991.
2024-12-02-11:53:38-root-INFO: Loss too large (1079.858->1117.795)! Learning rate decreased to 0.00792.
2024-12-02-11:53:38-root-INFO: Loss too large (1079.858->1097.682)! Learning rate decreased to 0.00634.
2024-12-02-11:53:38-root-INFO: Loss too large (1079.858->1086.100)! Learning rate decreased to 0.00507.
2024-12-02-11:53:39-root-INFO: Loss too large (1079.858->1079.997)! Learning rate decreased to 0.00406.
2024-12-02-11:53:39-root-INFO: grad norm: 129.124 127.930 17.520
2024-12-02-11:53:39-root-INFO: Loss too large (1077.077->1109.419)! Learning rate decreased to 0.00325.
2024-12-02-11:53:39-root-INFO: Loss too large (1077.077->1095.016)! Learning rate decreased to 0.00260.
2024-12-02-11:53:39-root-INFO: Loss too large (1077.077->1084.872)! Learning rate decreased to 0.00208.
2024-12-02-11:53:40-root-INFO: Loss too large (1077.077->1078.078)! Learning rate decreased to 0.00166.
2024-12-02-11:53:40-root-INFO: grad norm: 93.365 92.185 14.797
2024-12-02-11:53:41-root-INFO: grad norm: 53.009 51.498 12.565
2024-12-02-11:53:41-root-INFO: grad norm: 46.895 45.263 12.263
2024-12-02-11:53:41-root-INFO: Loss Change: 1079.858 -> 1060.983
2024-12-02-11:53:41-root-INFO: Regularization Change: 0.000 -> 0.201
2024-12-02-11:53:41-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-11:53:41-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:53:42-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-11:53:42-root-INFO: grad norm: 62.044 60.385 14.252
2024-12-02-11:53:42-root-INFO: Loss too large (1062.641->1187.031)! Learning rate decreased to 0.01028.
2024-12-02-11:53:42-root-INFO: Loss too large (1062.641->1142.790)! Learning rate decreased to 0.00822.
2024-12-02-11:53:42-root-INFO: Loss too large (1062.641->1107.341)! Learning rate decreased to 0.00658.
2024-12-02-11:53:42-root-INFO: Loss too large (1062.641->1083.907)! Learning rate decreased to 0.00526.
2024-12-02-11:53:43-root-INFO: Loss too large (1062.641->1070.387)! Learning rate decreased to 0.00421.
2024-12-02-11:53:43-root-INFO: Loss too large (1062.641->1063.288)! Learning rate decreased to 0.00337.
2024-12-02-11:53:43-root-INFO: grad norm: 122.489 121.317 16.903
2024-12-02-11:53:43-root-INFO: Loss too large (1059.895->1075.658)! Learning rate decreased to 0.00269.
2024-12-02-11:53:43-root-INFO: Loss too large (1059.895->1066.449)! Learning rate decreased to 0.00216.
2024-12-02-11:53:44-root-INFO: Loss too large (1059.895->1060.318)! Learning rate decreased to 0.00172.
2024-12-02-11:53:44-root-INFO: grad norm: 88.795 87.558 14.764
2024-12-02-11:53:45-root-INFO: grad norm: 51.209 49.674 12.446
2024-12-02-11:53:45-root-INFO: grad norm: 45.516 43.838 12.245
2024-12-02-11:53:45-root-INFO: Loss Change: 1062.641 -> 1043.819
2024-12-02-11:53:45-root-INFO: Regularization Change: 0.000 -> 0.189
2024-12-02-11:53:45-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-11:53:45-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-11:53:46-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-11:53:46-root-INFO: grad norm: 61.983 60.314 14.286
2024-12-02-11:53:46-root-INFO: Loss too large (1045.589->1173.139)! Learning rate decreased to 0.01067.
2024-12-02-11:53:46-root-INFO: Loss too large (1045.589->1127.384)! Learning rate decreased to 0.00853.
2024-12-02-11:53:46-root-INFO: Loss too large (1045.589->1090.589)! Learning rate decreased to 0.00683.
2024-12-02-11:53:46-root-INFO: Loss too large (1045.589->1066.451)! Learning rate decreased to 0.00546.
2024-12-02-11:53:47-root-INFO: Loss too large (1045.589->1052.721)! Learning rate decreased to 0.00437.
2024-12-02-11:53:47-root-INFO: Loss too large (1045.589->1045.638)! Learning rate decreased to 0.00350.
2024-12-02-11:53:47-root-INFO: grad norm: 116.629 115.479 16.337
2024-12-02-11:53:47-root-INFO: Loss too large (1042.333->1055.199)! Learning rate decreased to 0.00280.
2024-12-02-11:53:48-root-INFO: Loss too large (1042.333->1047.033)! Learning rate decreased to 0.00224.
2024-12-02-11:53:48-root-INFO: grad norm: 101.224 100.048 15.385
2024-12-02-11:53:48-root-INFO: grad norm: 71.785 70.560 13.201
2024-12-02-11:53:49-root-INFO: grad norm: 68.178 66.906 13.110
2024-12-02-11:53:49-root-INFO: Loss Change: 1045.589 -> 1024.649
2024-12-02-11:53:49-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-11:53:49-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-11:53:49-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-11:53:49-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-11:53:50-root-INFO: grad norm: 47.324 45.358 13.499
2024-12-02-11:53:50-root-INFO: grad norm: 212.230 210.843 24.225
2024-12-02-11:53:50-root-INFO: Loss too large (1018.292->1554.427)! Learning rate decreased to 0.01107.
2024-12-02-11:53:50-root-INFO: Loss too large (1018.292->1387.752)! Learning rate decreased to 0.00885.
2024-12-02-11:53:51-root-INFO: Loss too large (1018.292->1269.615)! Learning rate decreased to 0.00708.
2024-12-02-11:53:51-root-INFO: Loss too large (1018.292->1186.496)! Learning rate decreased to 0.00567.
2024-12-02-11:53:51-root-INFO: Loss too large (1018.292->1128.037)! Learning rate decreased to 0.00453.
2024-12-02-11:53:51-root-INFO: Loss too large (1018.292->1086.660)! Learning rate decreased to 0.00363.
2024-12-02-11:53:51-root-INFO: Loss too large (1018.292->1057.150)! Learning rate decreased to 0.00290.
2024-12-02-11:53:51-root-INFO: Loss too large (1018.292->1036.098)! Learning rate decreased to 0.00232.
2024-12-02-11:53:52-root-INFO: Loss too large (1018.292->1021.321)! Learning rate decreased to 0.00186.
2024-12-02-11:53:52-root-INFO: grad norm: 107.842 106.864 14.491
2024-12-02-11:53:52-root-INFO: grad norm: 42.149 40.626 11.228
2024-12-02-11:53:53-root-INFO: grad norm: 37.422 35.785 10.947
2024-12-02-11:53:53-root-INFO: Loss Change: 1023.623 -> 990.995
2024-12-02-11:53:53-root-INFO: Regularization Change: 0.000 -> 0.721
2024-12-02-11:53:53-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-11:53:53-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-11:53:53-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-11:53:54-root-INFO: grad norm: 53.388 52.031 11.959
2024-12-02-11:53:54-root-INFO: Loss too large (990.657->1103.905)! Learning rate decreased to 0.01148.
2024-12-02-11:53:54-root-INFO: Loss too large (990.657->1058.289)! Learning rate decreased to 0.00919.
2024-12-02-11:53:54-root-INFO: Loss too large (990.657->1024.994)! Learning rate decreased to 0.00735.
2024-12-02-11:53:54-root-INFO: Loss too large (990.657->1005.118)! Learning rate decreased to 0.00588.
2024-12-02-11:53:54-root-INFO: Loss too large (990.657->994.615)! Learning rate decreased to 0.00470.
2024-12-02-11:53:55-root-INFO: grad norm: 124.066 123.056 15.800
2024-12-02-11:53:55-root-INFO: Loss too large (989.514->1015.027)! Learning rate decreased to 0.00376.
2024-12-02-11:53:55-root-INFO: Loss too large (989.514->1002.270)! Learning rate decreased to 0.00301.
2024-12-02-11:53:55-root-INFO: Loss too large (989.514->993.473)! Learning rate decreased to 0.00241.
2024-12-02-11:53:56-root-INFO: grad norm: 89.193 88.178 13.414
2024-12-02-11:53:56-root-INFO: grad norm: 42.762 41.345 10.917
2024-12-02-11:53:57-root-INFO: grad norm: 39.665 38.163 10.811
2024-12-02-11:53:57-root-INFO: Loss Change: 990.657 -> 971.088
2024-12-02-11:53:57-root-INFO: Regularization Change: 0.000 -> 0.272
2024-12-02-11:53:57-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-11:53:57-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-11:53:57-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-11:53:57-root-INFO: grad norm: 50.498 48.982 12.278
2024-12-02-11:53:58-root-INFO: Loss too large (971.831->1036.689)! Learning rate decreased to 0.01191.
2024-12-02-11:53:58-root-INFO: Loss too large (971.831->1003.666)! Learning rate decreased to 0.00953.
2024-12-02-11:53:58-root-INFO: Loss too large (971.831->984.273)! Learning rate decreased to 0.00762.
2024-12-02-11:53:58-root-INFO: Loss too large (971.831->974.136)! Learning rate decreased to 0.00610.
2024-12-02-11:53:58-root-INFO: grad norm: 128.485 127.500 15.877
2024-12-02-11:53:59-root-INFO: Loss too large (969.296->1013.186)! Learning rate decreased to 0.00488.
2024-12-02-11:53:59-root-INFO: Loss too large (969.296->994.589)! Learning rate decreased to 0.00390.
2024-12-02-11:53:59-root-INFO: Loss too large (969.296->981.461)! Learning rate decreased to 0.00312.
2024-12-02-11:53:59-root-INFO: Loss too large (969.296->972.440)! Learning rate decreased to 0.00250.
2024-12-02-11:54:00-root-INFO: grad norm: 85.429 84.408 13.169
2024-12-02-11:54:00-root-INFO: grad norm: 37.342 35.837 10.496
2024-12-02-11:54:01-root-INFO: grad norm: 35.764 34.213 10.418
2024-12-02-11:54:01-root-INFO: Loss Change: 971.831 -> 949.600
2024-12-02-11:54:01-root-INFO: Regularization Change: 0.000 -> 0.350
2024-12-02-11:54:01-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-11:54:01-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:54:01-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-11:54:01-root-INFO: grad norm: 43.094 41.610 11.213
2024-12-02-11:54:01-root-INFO: Loss too large (949.911->972.562)! Learning rate decreased to 0.01235.
2024-12-02-11:54:01-root-INFO: Loss too large (949.911->957.193)! Learning rate decreased to 0.00988.
2024-12-02-11:54:02-root-INFO: grad norm: 167.080 166.027 18.729
2024-12-02-11:54:02-root-INFO: Loss too large (949.469->1114.736)! Learning rate decreased to 0.00790.
2024-12-02-11:54:02-root-INFO: Loss too large (949.469->1057.539)! Learning rate decreased to 0.00632.
2024-12-02-11:54:02-root-INFO: Loss too large (949.469->1017.550)! Learning rate decreased to 0.00506.
2024-12-02-11:54:03-root-INFO: Loss too large (949.469->989.432)! Learning rate decreased to 0.00405.
2024-12-02-11:54:03-root-INFO: Loss too large (949.469->969.633)! Learning rate decreased to 0.00324.
2024-12-02-11:54:03-root-INFO: Loss too large (949.469->955.839)! Learning rate decreased to 0.00259.
2024-12-02-11:54:03-root-INFO: grad norm: 93.156 92.237 13.053
2024-12-02-11:54:04-root-INFO: grad norm: 36.312 34.880 10.098
2024-12-02-11:54:04-root-INFO: grad norm: 34.154 32.688 9.899
2024-12-02-11:54:05-root-INFO: Loss Change: 949.911 -> 925.055
2024-12-02-11:54:05-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-11:54:05-root-INFO: Undo step: 150
2024-12-02-11:54:05-root-INFO: Undo step: 151
2024-12-02-11:54:05-root-INFO: Undo step: 152
2024-12-02-11:54:05-root-INFO: Undo step: 153
2024-12-02-11:54:05-root-INFO: Undo step: 154
2024-12-02-11:54:05-root-INFO: Undo step: 155
2024-12-02-11:54:05-root-INFO: Undo step: 156
2024-12-02-11:54:05-root-INFO: Undo step: 157
2024-12-02-11:54:05-root-INFO: Undo step: 158
2024-12-02-11:54:05-root-INFO: Undo step: 159
2024-12-02-11:54:05-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-11:54:05-root-INFO: grad norm: 405.162 398.731 71.902
2024-12-02-11:54:05-root-INFO: grad norm: 362.410 357.823 57.478
2024-12-02-11:54:06-root-INFO: grad norm: 154.056 150.287 33.870
2024-12-02-11:54:06-root-INFO: grad norm: 210.285 207.285 35.393
2024-12-02-11:54:07-root-INFO: grad norm: 278.902 276.509 36.453
2024-12-02-11:54:07-root-INFO: Loss too large (1303.644->1525.710)! Learning rate decreased to 0.00852.
2024-12-02-11:54:07-root-INFO: Loss too large (1303.644->1398.717)! Learning rate decreased to 0.00682.
2024-12-02-11:54:07-root-INFO: Loss Change: 2206.270 -> 1300.710
2024-12-02-11:54:07-root-INFO: Regularization Change: 0.000 -> 33.030
2024-12-02-11:54:07-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-11:54:07-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:54:07-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-11:54:08-root-INFO: grad norm: 260.107 257.514 36.630
2024-12-02-11:54:08-root-INFO: Loss too large (1301.027->1482.025)! Learning rate decreased to 0.00885.
2024-12-02-11:54:08-root-INFO: Loss too large (1301.027->1334.889)! Learning rate decreased to 0.00708.
2024-12-02-11:54:08-root-INFO: grad norm: 222.248 220.935 24.128
2024-12-02-11:54:09-root-INFO: grad norm: 277.598 274.754 39.634
2024-12-02-11:54:09-root-INFO: Loss too large (1198.966->1299.641)! Learning rate decreased to 0.00567.
2024-12-02-11:54:09-root-INFO: Loss too large (1198.966->1213.517)! Learning rate decreased to 0.00453.
2024-12-02-11:54:10-root-INFO: grad norm: 190.578 189.314 21.915
2024-12-02-11:54:10-root-INFO: grad norm: 146.437 144.056 26.300
2024-12-02-11:54:10-root-INFO: Loss too large (1081.536->1094.877)! Learning rate decreased to 0.00363.
2024-12-02-11:54:10-root-INFO: Loss Change: 1301.027 -> 1079.938
2024-12-02-11:54:10-root-INFO: Regularization Change: 0.000 -> 3.206
2024-12-02-11:54:11-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-11:54:11-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-11:54:11-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-11:54:11-root-INFO: grad norm: 143.138 141.953 18.379
2024-12-02-11:54:11-root-INFO: Loss too large (1082.316->1278.812)! Learning rate decreased to 0.00919.
2024-12-02-11:54:11-root-INFO: Loss too large (1082.316->1228.804)! Learning rate decreased to 0.00735.
2024-12-02-11:54:11-root-INFO: Loss too large (1082.316->1169.636)! Learning rate decreased to 0.00588.
2024-12-02-11:54:11-root-INFO: Loss too large (1082.316->1110.140)! Learning rate decreased to 0.00471.
2024-12-02-11:54:12-root-INFO: grad norm: 219.930 217.502 32.590
2024-12-02-11:54:12-root-INFO: Loss too large (1069.380->1125.116)! Learning rate decreased to 0.00376.
2024-12-02-11:54:12-root-INFO: Loss too large (1069.380->1091.442)! Learning rate decreased to 0.00301.
2024-12-02-11:54:13-root-INFO: grad norm: 142.896 141.778 17.841
2024-12-02-11:54:13-root-INFO: grad norm: 47.554 45.745 12.989
2024-12-02-11:54:14-root-INFO: grad norm: 45.011 43.389 11.976
2024-12-02-11:54:14-root-INFO: Loss Change: 1082.316 -> 1020.347
2024-12-02-11:54:14-root-INFO: Regularization Change: 0.000 -> 0.694
2024-12-02-11:54:14-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-11:54:14-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:54:14-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-11:54:14-root-INFO: grad norm: 51.074 49.457 12.749
2024-12-02-11:54:14-root-INFO: Loss too large (1017.352->1019.377)! Learning rate decreased to 0.00954.
2024-12-02-11:54:15-root-INFO: grad norm: 161.365 159.680 23.261
2024-12-02-11:54:15-root-INFO: Loss too large (1012.456->1193.879)! Learning rate decreased to 0.00763.
2024-12-02-11:54:15-root-INFO: Loss too large (1012.456->1131.296)! Learning rate decreased to 0.00611.
2024-12-02-11:54:15-root-INFO: Loss too large (1012.456->1086.628)! Learning rate decreased to 0.00489.
2024-12-02-11:54:16-root-INFO: Loss too large (1012.456->1054.823)! Learning rate decreased to 0.00391.
2024-12-02-11:54:16-root-INFO: Loss too large (1012.456->1032.484)! Learning rate decreased to 0.00313.
2024-12-02-11:54:16-root-INFO: Loss too large (1012.456->1017.273)! Learning rate decreased to 0.00250.
2024-12-02-11:54:16-root-INFO: grad norm: 103.360 102.277 14.923
2024-12-02-11:54:17-root-INFO: grad norm: 42.577 41.093 11.144
2024-12-02-11:54:17-root-INFO: grad norm: 39.516 38.064 10.614
2024-12-02-11:54:18-root-INFO: Loss Change: 1017.352 -> 985.310
2024-12-02-11:54:18-root-INFO: Regularization Change: 0.000 -> 0.625
2024-12-02-11:54:18-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-11:54:18-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:54:18-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-11:54:18-root-INFO: grad norm: 44.112 42.732 10.946
2024-12-02-11:54:18-root-INFO: Loss too large (982.571->984.341)! Learning rate decreased to 0.00991.
2024-12-02-11:54:18-root-INFO: grad norm: 144.388 142.922 20.518
2024-12-02-11:54:19-root-INFO: Loss too large (978.984->1136.730)! Learning rate decreased to 0.00792.
2024-12-02-11:54:19-root-INFO: Loss too large (978.984->1082.448)! Learning rate decreased to 0.00634.
2024-12-02-11:54:19-root-INFO: Loss too large (978.984->1043.788)! Learning rate decreased to 0.00507.
2024-12-02-11:54:19-root-INFO: Loss too large (978.984->1016.315)! Learning rate decreased to 0.00406.
2024-12-02-11:54:19-root-INFO: Loss too large (978.984->997.058)! Learning rate decreased to 0.00325.
2024-12-02-11:54:19-root-INFO: Loss too large (978.984->983.969)! Learning rate decreased to 0.00260.
2024-12-02-11:54:20-root-INFO: grad norm: 96.567 95.648 13.294
2024-12-02-11:54:20-root-INFO: grad norm: 41.423 40.102 10.378
2024-12-02-11:54:21-root-INFO: grad norm: 37.931 36.664 9.719
2024-12-02-11:54:21-root-INFO: Loss Change: 982.571 -> 956.005
2024-12-02-11:54:21-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-11:54:21-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-11:54:21-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:54:21-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-11:54:21-root-INFO: grad norm: 46.352 45.106 10.677
2024-12-02-11:54:22-root-INFO: Loss too large (954.645->973.267)! Learning rate decreased to 0.01028.
2024-12-02-11:54:22-root-INFO: Loss too large (954.645->960.364)! Learning rate decreased to 0.00822.
2024-12-02-11:54:22-root-INFO: grad norm: 145.257 143.895 19.846
2024-12-02-11:54:22-root-INFO: Loss too large (953.821->1064.245)! Learning rate decreased to 0.00658.
2024-12-02-11:54:23-root-INFO: Loss too large (953.821->1024.024)! Learning rate decreased to 0.00526.
2024-12-02-11:54:23-root-INFO: Loss too large (953.821->995.379)! Learning rate decreased to 0.00421.
2024-12-02-11:54:23-root-INFO: Loss too large (953.821->975.165)! Learning rate decreased to 0.00337.
2024-12-02-11:54:23-root-INFO: Loss too large (953.821->961.249)! Learning rate decreased to 0.00269.
2024-12-02-11:54:23-root-INFO: grad norm: 98.830 97.932 13.295
2024-12-02-11:54:24-root-INFO: grad norm: 39.318 38.142 9.544
2024-12-02-11:54:24-root-INFO: grad norm: 36.272 35.118 9.077
2024-12-02-11:54:25-root-INFO: Loss Change: 954.645 -> 931.789
2024-12-02-11:54:25-root-INFO: Regularization Change: 0.000 -> 0.429
2024-12-02-11:54:25-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-11:54:25-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-11:54:25-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-11:54:25-root-INFO: grad norm: 43.716 42.487 10.293
2024-12-02-11:54:25-root-INFO: Loss too large (930.710->945.532)! Learning rate decreased to 0.01067.
2024-12-02-11:54:25-root-INFO: Loss too large (930.710->934.494)! Learning rate decreased to 0.00853.
2024-12-02-11:54:26-root-INFO: grad norm: 132.075 130.858 17.887
2024-12-02-11:54:26-root-INFO: Loss too large (929.038->1023.768)! Learning rate decreased to 0.00683.
2024-12-02-11:54:26-root-INFO: Loss too large (929.038->988.710)! Learning rate decreased to 0.00546.
2024-12-02-11:54:26-root-INFO: Loss too large (929.038->963.880)! Learning rate decreased to 0.00437.
2024-12-02-11:54:26-root-INFO: Loss too large (929.038->946.482)! Learning rate decreased to 0.00350.
2024-12-02-11:54:27-root-INFO: Loss too large (929.038->934.612)! Learning rate decreased to 0.00280.
2024-12-02-11:54:27-root-INFO: grad norm: 89.649 88.818 12.178
2024-12-02-11:54:28-root-INFO: grad norm: 37.110 36.005 8.989
2024-12-02-11:54:28-root-INFO: grad norm: 34.154 33.064 8.560
2024-12-02-11:54:28-root-INFO: Loss Change: 930.710 -> 908.994
2024-12-02-11:54:28-root-INFO: Regularization Change: 0.000 -> 0.424
2024-12-02-11:54:28-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-11:54:28-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-11:54:28-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-11:54:29-root-INFO: grad norm: 43.842 42.631 10.232
2024-12-02-11:54:29-root-INFO: Loss too large (907.389->924.760)! Learning rate decreased to 0.01107.
2024-12-02-11:54:29-root-INFO: Loss too large (907.389->912.457)! Learning rate decreased to 0.00885.
2024-12-02-11:54:29-root-INFO: grad norm: 131.220 130.055 17.449
2024-12-02-11:54:29-root-INFO: Loss too large (906.318->1000.138)! Learning rate decreased to 0.00708.
2024-12-02-11:54:30-root-INFO: Loss too large (906.318->964.759)! Learning rate decreased to 0.00567.
2024-12-02-11:54:30-root-INFO: Loss too large (906.318->939.931)! Learning rate decreased to 0.00453.
2024-12-02-11:54:30-root-INFO: Loss too large (906.318->922.681)! Learning rate decreased to 0.00363.
2024-12-02-11:54:30-root-INFO: Loss too large (906.318->910.998)! Learning rate decreased to 0.00290.
2024-12-02-11:54:31-root-INFO: grad norm: 84.688 83.917 11.400
2024-12-02-11:54:31-root-INFO: grad norm: 32.740 31.687 8.235
2024-12-02-11:54:31-root-INFO: grad norm: 30.758 29.721 7.920
2024-12-02-11:54:32-root-INFO: Loss Change: 907.389 -> 886.383
2024-12-02-11:54:32-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-02-11:54:32-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-11:54:32-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-11:54:32-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-11:54:32-root-INFO: grad norm: 37.453 36.431 8.689
2024-12-02-11:54:32-root-INFO: Loss too large (885.482->891.995)! Learning rate decreased to 0.01148.
2024-12-02-11:54:33-root-INFO: grad norm: 137.780 136.605 17.958
2024-12-02-11:54:33-root-INFO: Loss too large (885.359->1043.565)! Learning rate decreased to 0.00919.
2024-12-02-11:54:33-root-INFO: Loss too large (885.359->987.994)! Learning rate decreased to 0.00735.
2024-12-02-11:54:33-root-INFO: Loss too large (885.359->949.247)! Learning rate decreased to 0.00588.
2024-12-02-11:54:33-root-INFO: Loss too large (885.359->922.209)! Learning rate decreased to 0.00470.
2024-12-02-11:54:33-root-INFO: Loss too large (885.359->903.458)! Learning rate decreased to 0.00376.
2024-12-02-11:54:34-root-INFO: Loss too large (885.359->890.716)! Learning rate decreased to 0.00301.
2024-12-02-11:54:34-root-INFO: grad norm: 83.499 82.815 10.662
2024-12-02-11:54:35-root-INFO: grad norm: 28.774 27.761 7.570
2024-12-02-11:54:35-root-INFO: grad norm: 28.012 27.024 7.371
2024-12-02-11:54:35-root-INFO: Loss Change: 885.482 -> 864.987
2024-12-02-11:54:35-root-INFO: Regularization Change: 0.000 -> 0.478
2024-12-02-11:54:35-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-11:54:35-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-11:54:36-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-11:54:36-root-INFO: grad norm: 41.573 40.544 9.193
2024-12-02-11:54:36-root-INFO: Loss too large (864.943->888.482)! Learning rate decreased to 0.01191.
2024-12-02-11:54:36-root-INFO: Loss too large (864.943->873.361)! Learning rate decreased to 0.00953.
2024-12-02-11:54:36-root-INFO: Loss too large (864.943->865.794)! Learning rate decreased to 0.00762.
2024-12-02-11:54:37-root-INFO: grad norm: 90.571 89.684 12.641
2024-12-02-11:54:37-root-INFO: Loss too large (862.265->889.454)! Learning rate decreased to 0.00610.
2024-12-02-11:54:37-root-INFO: Loss too large (862.265->876.282)! Learning rate decreased to 0.00488.
2024-12-02-11:54:37-root-INFO: Loss too large (862.265->867.440)! Learning rate decreased to 0.00390.
2024-12-02-11:54:38-root-INFO: grad norm: 73.755 73.067 10.049
2024-12-02-11:54:38-root-INFO: grad norm: 44.924 44.148 8.311
2024-12-02-11:54:39-root-INFO: grad norm: 41.772 41.045 7.756
2024-12-02-11:54:39-root-INFO: Loss Change: 864.943 -> 845.903
2024-12-02-11:54:39-root-INFO: Regularization Change: 0.000 -> 0.448
2024-12-02-11:54:39-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-11:54:39-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:54:39-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-11:54:39-root-INFO: grad norm: 30.580 29.627 7.576
2024-12-02-11:54:40-root-INFO: grad norm: 45.264 44.615 7.635
2024-12-02-11:54:40-root-INFO: Loss too large (835.842->894.365)! Learning rate decreased to 0.01235.
2024-12-02-11:54:40-root-INFO: Loss too large (835.842->861.768)! Learning rate decreased to 0.00988.
2024-12-02-11:54:40-root-INFO: Loss too large (835.842->844.712)! Learning rate decreased to 0.00790.
2024-12-02-11:54:40-root-INFO: Loss too large (835.842->836.540)! Learning rate decreased to 0.00632.
2024-12-02-11:54:41-root-INFO: grad norm: 72.496 71.803 10.001
2024-12-02-11:54:41-root-INFO: Loss too large (832.902->840.731)! Learning rate decreased to 0.00506.
2024-12-02-11:54:41-root-INFO: Loss too large (832.902->835.010)! Learning rate decreased to 0.00405.
2024-12-02-11:54:42-root-INFO: grad norm: 59.816 59.216 8.447
2024-12-02-11:54:42-root-INFO: grad norm: 40.118 39.419 7.454
2024-12-02-11:54:42-root-INFO: Loss Change: 845.356 -> 822.534
2024-12-02-11:54:42-root-INFO: Regularization Change: 0.000 -> 0.813
2024-12-02-11:54:42-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-11:54:42-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:54:42-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-11:54:43-root-INFO: grad norm: 52.305 51.529 8.972
2024-12-02-11:54:43-root-INFO: Loss too large (823.515->935.057)! Learning rate decreased to 0.01281.
2024-12-02-11:54:43-root-INFO: Loss too large (823.515->880.761)! Learning rate decreased to 0.01024.
2024-12-02-11:54:43-root-INFO: Loss too large (823.515->847.054)! Learning rate decreased to 0.00820.
2024-12-02-11:54:43-root-INFO: Loss too large (823.515->830.113)! Learning rate decreased to 0.00656.
2024-12-02-11:54:44-root-INFO: grad norm: 97.891 97.043 12.858
2024-12-02-11:54:44-root-INFO: Loss too large (822.338->838.980)! Learning rate decreased to 0.00525.
2024-12-02-11:54:44-root-INFO: Loss too large (822.338->828.723)! Learning rate decreased to 0.00420.
2024-12-02-11:54:45-root-INFO: grad norm: 70.858 70.261 9.178
2024-12-02-11:54:45-root-INFO: grad norm: 30.564 29.782 6.869
2024-12-02-11:54:46-root-INFO: grad norm: 28.118 27.356 6.502
2024-12-02-11:54:46-root-INFO: Loss Change: 823.515 -> 805.496
2024-12-02-11:54:46-root-INFO: Regularization Change: 0.000 -> 0.375
2024-12-02-11:54:46-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-11:54:46-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:54:46-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-11:54:46-root-INFO: grad norm: 32.390 31.423 7.854
2024-12-02-11:54:47-root-INFO: grad norm: 97.293 96.178 14.689
2024-12-02-11:54:47-root-INFO: Loss too large (803.916->938.528)! Learning rate decreased to 0.01328.
2024-12-02-11:54:47-root-INFO: Loss too large (803.916->886.842)! Learning rate decreased to 0.01062.
2024-12-02-11:54:47-root-INFO: Loss too large (803.916->852.589)! Learning rate decreased to 0.00850.
2024-12-02-11:54:47-root-INFO: Loss too large (803.916->829.951)! Learning rate decreased to 0.00680.
2024-12-02-11:54:47-root-INFO: Loss too large (803.916->815.126)! Learning rate decreased to 0.00544.
2024-12-02-11:54:48-root-INFO: Loss too large (803.916->805.631)! Learning rate decreased to 0.00435.
2024-12-02-11:54:48-root-INFO: grad norm: 62.509 61.901 8.698
2024-12-02-11:54:48-root-INFO: grad norm: 26.047 25.226 6.489
2024-12-02-11:54:49-root-INFO: grad norm: 24.336 23.537 6.185
2024-12-02-11:54:49-root-INFO: Loss Change: 806.315 -> 785.350
2024-12-02-11:54:49-root-INFO: Regularization Change: 0.000 -> 0.706
2024-12-02-11:54:49-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-11:54:49-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-11:54:49-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-11:54:50-root-INFO: grad norm: 28.965 28.098 7.031
2024-12-02-11:54:50-root-INFO: grad norm: 78.370 77.549 11.309
2024-12-02-11:54:50-root-INFO: Loss too large (780.378->870.141)! Learning rate decreased to 0.01376.
2024-12-02-11:54:50-root-INFO: Loss too large (780.378->834.803)! Learning rate decreased to 0.01101.
2024-12-02-11:54:50-root-INFO: Loss too large (780.378->811.379)! Learning rate decreased to 0.00881.
2024-12-02-11:54:51-root-INFO: Loss too large (780.378->795.964)! Learning rate decreased to 0.00705.
2024-12-02-11:54:51-root-INFO: Loss too large (780.378->785.992)! Learning rate decreased to 0.00564.
2024-12-02-11:54:51-root-INFO: grad norm: 63.089 62.547 8.253
2024-12-02-11:54:52-root-INFO: grad norm: 38.656 38.036 6.895
2024-12-02-11:54:52-root-INFO: grad norm: 36.257 35.664 6.528
2024-12-02-11:54:53-root-INFO: Loss Change: 784.443 -> 763.879
2024-12-02-11:54:53-root-INFO: Regularization Change: 0.000 -> 0.848
2024-12-02-11:54:53-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-11:54:53-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:54:53-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-11:54:53-root-INFO: grad norm: 25.622 24.866 6.179
2024-12-02-11:54:53-root-INFO: grad norm: 35.918 35.027 7.953
2024-12-02-11:54:54-root-INFO: Loss too large (755.319->770.509)! Learning rate decreased to 0.01426.
2024-12-02-11:54:54-root-INFO: Loss too large (755.319->759.258)! Learning rate decreased to 0.01141.
2024-12-02-11:54:54-root-INFO: grad norm: 73.351 72.764 9.258
2024-12-02-11:54:54-root-INFO: Loss too large (753.946->777.866)! Learning rate decreased to 0.00913.
2024-12-02-11:54:55-root-INFO: Loss too large (753.946->765.508)! Learning rate decreased to 0.00730.
2024-12-02-11:54:55-root-INFO: Loss too large (753.946->757.451)! Learning rate decreased to 0.00584.
2024-12-02-11:54:55-root-INFO: grad norm: 55.247 54.758 7.339
2024-12-02-11:54:56-root-INFO: grad norm: 30.314 29.714 6.002
2024-12-02-11:54:56-root-INFO: Loss Change: 762.435 -> 741.324
2024-12-02-11:54:56-root-INFO: Regularization Change: 0.000 -> 1.017
2024-12-02-11:54:56-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-11:54:56-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:54:56-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-11:54:56-root-INFO: grad norm: 41.471 40.775 7.565
2024-12-02-11:54:57-root-INFO: Loss too large (743.218->782.512)! Learning rate decreased to 0.01478.
2024-12-02-11:54:57-root-INFO: Loss too large (743.218->758.074)! Learning rate decreased to 0.01182.
2024-12-02-11:54:57-root-INFO: Loss too large (743.218->746.337)! Learning rate decreased to 0.00946.
2024-12-02-11:54:57-root-INFO: grad norm: 67.337 66.634 9.704
2024-12-02-11:54:58-root-INFO: Loss too large (740.958->750.119)! Learning rate decreased to 0.00757.
2024-12-02-11:54:58-root-INFO: Loss too large (740.958->742.964)! Learning rate decreased to 0.00605.
2024-12-02-11:54:58-root-INFO: grad norm: 49.334 48.818 7.122
2024-12-02-11:54:59-root-INFO: grad norm: 26.465 25.814 5.838
2024-12-02-11:54:59-root-INFO: grad norm: 23.611 22.942 5.583
2024-12-02-11:55:00-root-INFO: Loss Change: 743.218 -> 725.964
2024-12-02-11:55:00-root-INFO: Regularization Change: 0.000 -> 0.526
2024-12-02-11:55:00-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-11:55:00-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:55:00-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-11:55:00-root-INFO: grad norm: 23.730 22.904 6.207
2024-12-02-11:55:00-root-INFO: grad norm: 32.276 31.226 8.166
2024-12-02-11:55:01-root-INFO: Loss too large (718.532->723.099)! Learning rate decreased to 0.01531.
2024-12-02-11:55:01-root-INFO: grad norm: 57.106 56.136 10.481
2024-12-02-11:55:01-root-INFO: Loss too large (718.485->753.709)! Learning rate decreased to 0.01225.
2024-12-02-11:55:01-root-INFO: Loss too large (718.485->728.348)! Learning rate decreased to 0.00980.
2024-12-02-11:55:02-root-INFO: grad norm: 80.875 79.969 12.069
2024-12-02-11:55:02-root-INFO: Loss too large (716.471->728.348)! Learning rate decreased to 0.00784.
2024-12-02-11:55:02-root-INFO: Loss too large (716.471->718.184)! Learning rate decreased to 0.00627.
2024-12-02-11:55:03-root-INFO: grad norm: 50.686 50.207 6.955
2024-12-02-11:55:03-root-INFO: Loss Change: 725.482 -> 703.338
2024-12-02-11:55:03-root-INFO: Regularization Change: 0.000 -> 1.267
2024-12-02-11:55:03-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-11:55:03-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-11:55:03-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-11:55:03-root-INFO: grad norm: 21.342 20.632 5.458
2024-12-02-11:55:04-root-INFO: grad norm: 26.305 25.398 6.847
2024-12-02-11:55:04-root-INFO: grad norm: 54.689 53.724 10.230
2024-12-02-11:55:05-root-INFO: Loss too large (695.043->761.008)! Learning rate decreased to 0.01586.
2024-12-02-11:55:05-root-INFO: Loss too large (695.043->721.093)! Learning rate decreased to 0.01269.
2024-12-02-11:55:05-root-INFO: Loss too large (695.043->701.312)! Learning rate decreased to 0.01015.
2024-12-02-11:55:05-root-INFO: grad norm: 66.147 65.276 10.699
2024-12-02-11:55:06-root-INFO: Loss too large (691.844->696.498)! Learning rate decreased to 0.00812.
2024-12-02-11:55:06-root-INFO: grad norm: 51.877 51.417 6.900
2024-12-02-11:55:06-root-INFO: Loss Change: 701.968 -> 681.149
2024-12-02-11:55:06-root-INFO: Regularization Change: 0.000 -> 1.483
2024-12-02-11:55:06-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-11:55:06-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-11:55:07-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-11:55:07-root-INFO: grad norm: 24.931 24.268 5.710
2024-12-02-11:55:07-root-INFO: grad norm: 40.796 39.992 8.061
2024-12-02-11:55:07-root-INFO: Loss too large (676.247->712.436)! Learning rate decreased to 0.01642.
2024-12-02-11:55:08-root-INFO: Loss too large (676.247->687.993)! Learning rate decreased to 0.01314.
2024-12-02-11:55:08-root-INFO: Loss too large (676.247->676.726)! Learning rate decreased to 0.01051.
2024-12-02-11:55:08-root-INFO: grad norm: 46.577 46.143 6.340
2024-12-02-11:55:08-root-INFO: Loss too large (671.905->672.903)! Learning rate decreased to 0.00841.
2024-12-02-11:55:09-root-INFO: grad norm: 38.115 37.633 6.043
2024-12-02-11:55:09-root-INFO: grad norm: 26.504 26.029 4.995
2024-12-02-11:55:10-root-INFO: Loss Change: 679.610 -> 662.135
2024-12-02-11:55:10-root-INFO: Regularization Change: 0.000 -> 1.010
2024-12-02-11:55:10-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-11:55:10-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-11:55:10-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-11:55:10-root-INFO: grad norm: 30.117 29.570 5.713
2024-12-02-11:55:10-root-INFO: Loss too large (662.350->671.663)! Learning rate decreased to 0.01701.
2024-12-02-11:55:10-root-INFO: Loss too large (662.350->663.745)! Learning rate decreased to 0.01361.
2024-12-02-11:55:11-root-INFO: grad norm: 46.834 46.311 6.978
2024-12-02-11:55:11-root-INFO: Loss too large (660.138->665.406)! Learning rate decreased to 0.01088.
2024-12-02-11:55:11-root-INFO: Loss too large (660.138->660.184)! Learning rate decreased to 0.00871.
2024-12-02-11:55:12-root-INFO: grad norm: 35.465 35.008 5.674
2024-12-02-11:55:12-root-INFO: grad norm: 23.004 22.478 4.891
2024-12-02-11:55:13-root-INFO: grad norm: 19.935 19.367 4.726
2024-12-02-11:55:13-root-INFO: Loss Change: 662.350 -> 647.063
2024-12-02-11:55:13-root-INFO: Regularization Change: 0.000 -> 0.692
2024-12-02-11:55:13-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-11:55:13-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-11:55:13-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-11:55:13-root-INFO: grad norm: 20.193 19.534 5.117
2024-12-02-11:55:14-root-INFO: grad norm: 26.856 25.757 7.602
2024-12-02-11:55:14-root-INFO: Loss too large (641.568->643.352)! Learning rate decreased to 0.01761.
2024-12-02-11:55:14-root-INFO: grad norm: 42.090 41.097 9.086
2024-12-02-11:55:14-root-INFO: Loss too large (640.122->647.091)! Learning rate decreased to 0.01409.
2024-12-02-11:55:15-root-INFO: grad norm: 52.387 51.184 11.165
2024-12-02-11:55:15-root-INFO: Loss too large (639.968->641.485)! Learning rate decreased to 0.01127.
2024-12-02-11:55:15-root-INFO: grad norm: 44.448 43.914 6.869
2024-12-02-11:55:16-root-INFO: Loss Change: 646.715 -> 630.652
2024-12-02-11:55:16-root-INFO: Regularization Change: 0.000 -> 1.564
2024-12-02-11:55:16-root-INFO: Undo step: 140
2024-12-02-11:55:16-root-INFO: Undo step: 141
2024-12-02-11:55:16-root-INFO: Undo step: 142
2024-12-02-11:55:16-root-INFO: Undo step: 143
2024-12-02-11:55:16-root-INFO: Undo step: 144
2024-12-02-11:55:16-root-INFO: Undo step: 145
2024-12-02-11:55:16-root-INFO: Undo step: 146
2024-12-02-11:55:16-root-INFO: Undo step: 147
2024-12-02-11:55:16-root-INFO: Undo step: 148
2024-12-02-11:55:16-root-INFO: Undo step: 149
2024-12-02-11:55:16-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-11:55:16-root-INFO: grad norm: 366.383 361.161 61.638
2024-12-02-11:55:17-root-INFO: grad norm: 476.570 474.170 47.767
2024-12-02-11:55:17-root-INFO: grad norm: 187.544 184.364 34.391
2024-12-02-11:55:17-root-INFO: grad norm: 185.706 181.692 38.404
2024-12-02-11:55:18-root-INFO: grad norm: 300.018 298.270 32.334
2024-12-02-11:55:18-root-INFO: Loss too large (1229.289->1233.840)! Learning rate decreased to 0.01235.
2024-12-02-11:55:18-root-INFO: Loss Change: 1795.012 -> 1174.947
2024-12-02-11:55:18-root-INFO: Regularization Change: 0.000 -> 59.742
2024-12-02-11:55:18-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-11:55:18-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:55:19-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-11:55:19-root-INFO: grad norm: 136.277 134.671 20.861
2024-12-02-11:55:19-root-INFO: grad norm: 174.265 172.660 23.600
2024-12-02-11:55:19-root-INFO: Loss too large (1067.508->1071.725)! Learning rate decreased to 0.01281.
2024-12-02-11:55:20-root-INFO: grad norm: 221.695 219.981 27.520
2024-12-02-11:55:20-root-INFO: Loss too large (1013.867->1050.875)! Learning rate decreased to 0.01024.
2024-12-02-11:55:20-root-INFO: grad norm: 163.229 162.149 18.743
2024-12-02-11:55:21-root-INFO: grad norm: 117.431 116.068 17.840
2024-12-02-11:55:21-root-INFO: Loss too large (831.223->846.977)! Learning rate decreased to 0.00820.
2024-12-02-11:55:21-root-INFO: Loss Change: 1167.404 -> 826.735
2024-12-02-11:55:21-root-INFO: Regularization Change: 0.000 -> 14.318
2024-12-02-11:55:21-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-11:55:21-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:55:21-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-11:55:22-root-INFO: grad norm: 111.985 111.348 11.924
2024-12-02-11:55:22-root-INFO: Loss too large (826.713->944.245)! Learning rate decreased to 0.01328.
2024-12-02-11:55:22-root-INFO: Loss too large (826.713->867.110)! Learning rate decreased to 0.01062.
2024-12-02-11:55:22-root-INFO: grad norm: 182.744 181.623 20.205
2024-12-02-11:55:23-root-INFO: Loss too large (810.309->901.417)! Learning rate decreased to 0.00850.
2024-12-02-11:55:23-root-INFO: Loss too large (810.309->847.350)! Learning rate decreased to 0.00680.
2024-12-02-11:55:23-root-INFO: Loss too large (810.309->812.657)! Learning rate decreased to 0.00544.
2024-12-02-11:55:23-root-INFO: grad norm: 89.361 88.916 8.910
2024-12-02-11:55:24-root-INFO: grad norm: 37.972 37.079 8.187
2024-12-02-11:55:24-root-INFO: grad norm: 32.433 31.402 8.114
2024-12-02-11:55:25-root-INFO: Loss Change: 826.713 -> 748.832
2024-12-02-11:55:25-root-INFO: Regularization Change: 0.000 -> 1.770
2024-12-02-11:55:25-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-11:55:25-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-11:55:25-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-11:55:25-root-INFO: grad norm: 33.292 32.319 7.991
2024-12-02-11:55:25-root-INFO: grad norm: 42.642 41.852 8.167
2024-12-02-11:55:25-root-INFO: Loss too large (731.799->734.977)! Learning rate decreased to 0.01376.
2024-12-02-11:55:26-root-INFO: grad norm: 61.690 61.210 7.678
2024-12-02-11:55:26-root-INFO: Loss too large (730.059->747.286)! Learning rate decreased to 0.01101.
2024-12-02-11:55:27-root-INFO: grad norm: 107.393 106.714 12.057
2024-12-02-11:55:27-root-INFO: Loss too large (726.398->760.431)! Learning rate decreased to 0.00881.
2024-12-02-11:55:27-root-INFO: Loss too large (726.398->739.660)! Learning rate decreased to 0.00705.
2024-12-02-11:55:27-root-INFO: Loss too large (726.398->726.554)! Learning rate decreased to 0.00564.
2024-12-02-11:55:28-root-INFO: grad norm: 56.175 55.748 6.918
2024-12-02-11:55:28-root-INFO: Loss Change: 745.349 -> 706.933
2024-12-02-11:55:28-root-INFO: Regularization Change: 0.000 -> 2.057
2024-12-02-11:55:28-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-11:55:28-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:55:28-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-11:55:28-root-INFO: grad norm: 28.325 27.560 6.536
2024-12-02-11:55:29-root-INFO: grad norm: 50.336 49.723 7.830
2024-12-02-11:55:29-root-INFO: Loss too large (698.437->716.451)! Learning rate decreased to 0.01426.
2024-12-02-11:55:29-root-INFO: Loss too large (698.437->705.940)! Learning rate decreased to 0.01141.
2024-12-02-11:55:29-root-INFO: Loss too large (698.437->699.495)! Learning rate decreased to 0.00913.
2024-12-02-11:55:30-root-INFO: grad norm: 45.697 45.256 6.335
2024-12-02-11:55:30-root-INFO: grad norm: 38.943 38.391 6.535
2024-12-02-11:55:31-root-INFO: grad norm: 37.153 36.682 5.893
2024-12-02-11:55:31-root-INFO: Loss Change: 705.147 -> 680.194
2024-12-02-11:55:31-root-INFO: Regularization Change: 0.000 -> 1.444
2024-12-02-11:55:31-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-11:55:31-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:55:31-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-11:55:31-root-INFO: grad norm: 29.574 28.803 6.709
2024-12-02-11:55:32-root-INFO: grad norm: 42.815 42.306 6.585
2024-12-02-11:55:32-root-INFO: Loss too large (674.218->691.133)! Learning rate decreased to 0.01478.
2024-12-02-11:55:32-root-INFO: Loss too large (674.218->675.804)! Learning rate decreased to 0.01182.
2024-12-02-11:55:32-root-INFO: grad norm: 53.666 53.214 6.948
2024-12-02-11:55:33-root-INFO: Loss too large (669.297->672.811)! Learning rate decreased to 0.00946.
2024-12-02-11:55:33-root-INFO: grad norm: 45.957 45.547 6.122
2024-12-02-11:55:34-root-INFO: grad norm: 33.749 33.257 5.741
2024-12-02-11:55:34-root-INFO: Loss Change: 678.276 -> 657.222
2024-12-02-11:55:34-root-INFO: Regularization Change: 0.000 -> 1.346
2024-12-02-11:55:34-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-11:55:34-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:55:34-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-11:55:34-root-INFO: grad norm: 37.243 36.629 6.736
2024-12-02-11:55:34-root-INFO: Loss too large (657.067->667.300)! Learning rate decreased to 0.01531.
2024-12-02-11:55:34-root-INFO: Loss too large (657.067->657.096)! Learning rate decreased to 0.01225.
2024-12-02-11:55:35-root-INFO: grad norm: 45.339 44.811 6.897
2024-12-02-11:55:35-root-INFO: Loss too large (652.739->653.530)! Learning rate decreased to 0.00980.
2024-12-02-11:55:36-root-INFO: grad norm: 38.386 37.946 5.795
2024-12-02-11:55:36-root-INFO: grad norm: 28.746 28.234 5.402
2024-12-02-11:55:36-root-INFO: grad norm: 25.915 25.420 5.040
2024-12-02-11:55:37-root-INFO: Loss Change: 657.067 -> 637.907
2024-12-02-11:55:37-root-INFO: Regularization Change: 0.000 -> 0.914
2024-12-02-11:55:37-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-11:55:37-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-11:55:37-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-11:55:37-root-INFO: grad norm: 20.784 20.091 5.323
2024-12-02-11:55:38-root-INFO: grad norm: 25.059 24.467 5.418
2024-12-02-11:55:38-root-INFO: grad norm: 51.315 50.863 6.797
2024-12-02-11:55:38-root-INFO: Loss too large (629.758->651.826)! Learning rate decreased to 0.01586.
2024-12-02-11:55:38-root-INFO: Loss too large (629.758->639.043)! Learning rate decreased to 0.01269.
2024-12-02-11:55:39-root-INFO: Loss too large (629.758->631.206)! Learning rate decreased to 0.01015.
2024-12-02-11:55:39-root-INFO: grad norm: 39.247 38.843 5.613
2024-12-02-11:55:40-root-INFO: grad norm: 25.987 25.547 4.758
2024-12-02-11:55:40-root-INFO: Loss Change: 636.017 -> 617.581
2024-12-02-11:55:40-root-INFO: Regularization Change: 0.000 -> 1.353
2024-12-02-11:55:40-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-11:55:40-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-11:55:40-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-11:55:40-root-INFO: grad norm: 31.866 31.113 6.886
2024-12-02-11:55:40-root-INFO: Loss too large (618.263->622.019)! Learning rate decreased to 0.01642.
2024-12-02-11:55:41-root-INFO: grad norm: 52.304 51.693 7.967
2024-12-02-11:55:41-root-INFO: Loss too large (616.451->628.322)! Learning rate decreased to 0.01314.
2024-12-02-11:55:41-root-INFO: Loss too large (616.451->619.329)! Learning rate decreased to 0.01051.
2024-12-02-11:55:42-root-INFO: grad norm: 41.125 40.683 6.009
2024-12-02-11:55:42-root-INFO: grad norm: 26.691 26.213 5.031
2024-12-02-11:55:43-root-INFO: grad norm: 22.826 22.362 4.579
2024-12-02-11:55:43-root-INFO: Loss Change: 618.263 -> 601.487
2024-12-02-11:55:43-root-INFO: Regularization Change: 0.000 -> 0.914
2024-12-02-11:55:43-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-11:55:43-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-11:55:43-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-11:55:43-root-INFO: grad norm: 18.598 17.964 4.818
2024-12-02-11:55:44-root-INFO: grad norm: 22.921 22.353 5.074
2024-12-02-11:55:44-root-INFO: grad norm: 45.719 45.180 7.006
2024-12-02-11:55:44-root-INFO: Loss too large (595.244->609.329)! Learning rate decreased to 0.01701.
2024-12-02-11:55:44-root-INFO: Loss too large (595.244->600.761)! Learning rate decreased to 0.01361.
2024-12-02-11:55:45-root-INFO: grad norm: 41.462 40.909 6.748
2024-12-02-11:55:45-root-INFO: grad norm: 37.604 37.186 5.586
2024-12-02-11:55:46-root-INFO: Loss Change: 600.168 -> 587.641
2024-12-02-11:55:46-root-INFO: Regularization Change: 0.000 -> 1.573
2024-12-02-11:55:46-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-11:55:46-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-11:55:46-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-11:55:46-root-INFO: grad norm: 41.524 41.143 5.614
2024-12-02-11:55:46-root-INFO: Loss too large (588.116->602.022)! Learning rate decreased to 0.01761.
2024-12-02-11:55:47-root-INFO: grad norm: 67.625 67.204 7.527
2024-12-02-11:55:47-root-INFO: Loss too large (587.836->608.919)! Learning rate decreased to 0.01409.
2024-12-02-11:55:47-root-INFO: Loss too large (587.836->593.968)! Learning rate decreased to 0.01127.
2024-12-02-11:55:47-root-INFO: grad norm: 46.440 46.111 5.515
2024-12-02-11:55:48-root-INFO: grad norm: 22.274 21.889 4.126
2024-12-02-11:55:48-root-INFO: grad norm: 18.245 17.840 3.826
2024-12-02-11:55:49-root-INFO: Loss Change: 588.116 -> 569.259
2024-12-02-11:55:49-root-INFO: Regularization Change: 0.000 -> 0.905
2024-12-02-11:55:49-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-11:55:49-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:55:49-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-11:55:49-root-INFO: grad norm: 16.813 16.156 4.652
2024-12-02-11:55:49-root-INFO: grad norm: 18.019 17.377 4.767
2024-12-02-11:55:50-root-INFO: grad norm: 29.816 29.087 6.552
2024-12-02-11:55:50-root-INFO: Loss too large (562.324->567.372)! Learning rate decreased to 0.01823.
2024-12-02-11:55:50-root-INFO: Loss too large (562.324->562.961)! Learning rate decreased to 0.01458.
2024-12-02-11:55:51-root-INFO: grad norm: 29.924 29.077 7.070
2024-12-02-11:55:51-root-INFO: grad norm: 30.346 29.647 6.475
2024-12-02-11:55:52-root-INFO: Loss Change: 568.920 -> 555.956
2024-12-02-11:55:52-root-INFO: Regularization Change: 0.000 -> 1.476
2024-12-02-11:55:52-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-11:55:52-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:55:52-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-11:55:52-root-INFO: grad norm: 27.922 27.338 5.684
2024-12-02-11:55:52-root-INFO: Loss too large (554.096->558.409)! Learning rate decreased to 0.01887.
2024-12-02-11:55:53-root-INFO: grad norm: 41.876 41.281 7.035
2024-12-02-11:55:53-root-INFO: Loss too large (553.607->558.879)! Learning rate decreased to 0.01509.
2024-12-02-11:55:53-root-INFO: grad norm: 40.270 39.589 7.376
2024-12-02-11:55:54-root-INFO: grad norm: 36.994 36.470 6.205
2024-12-02-11:55:54-root-INFO: grad norm: 37.312 36.713 6.661
2024-12-02-11:55:54-root-INFO: Loss Change: 554.096 -> 543.798
2024-12-02-11:55:54-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-02-11:55:54-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-11:55:54-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:55:55-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-11:55:55-root-INFO: grad norm: 50.615 49.528 10.432
2024-12-02-11:55:55-root-INFO: Loss too large (547.915->569.440)! Learning rate decreased to 0.01952.
2024-12-02-11:55:55-root-INFO: Loss too large (547.915->553.986)! Learning rate decreased to 0.01562.
2024-12-02-11:55:56-root-INFO: grad norm: 47.594 46.678 9.290
2024-12-02-11:55:56-root-INFO: grad norm: 44.245 43.595 7.554
2024-12-02-11:55:57-root-INFO: grad norm: 41.908 41.266 7.306
2024-12-02-11:55:57-root-INFO: grad norm: 39.863 39.375 6.220
2024-12-02-11:55:58-root-INFO: Loss Change: 547.915 -> 531.599
2024-12-02-11:55:58-root-INFO: Regularization Change: 0.000 -> 1.426
2024-12-02-11:55:58-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-11:55:58-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-11:55:58-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-11:55:58-root-INFO: grad norm: 35.489 35.012 5.794
2024-12-02-11:55:58-root-INFO: Loss too large (530.285->541.134)! Learning rate decreased to 0.02020.
2024-12-02-11:55:58-root-INFO: Loss too large (530.285->532.195)! Learning rate decreased to 0.01616.
2024-12-02-11:55:59-root-INFO: grad norm: 34.999 34.599 5.271
2024-12-02-11:55:59-root-INFO: grad norm: 34.589 34.158 5.442
2024-12-02-11:56:00-root-INFO: grad norm: 34.368 34.014 4.920
2024-12-02-11:56:00-root-INFO: grad norm: 33.979 33.583 5.170
2024-12-02-11:56:01-root-INFO: Loss Change: 530.285 -> 518.170
2024-12-02-11:56:01-root-INFO: Regularization Change: 0.000 -> 1.184
2024-12-02-11:56:01-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-11:56:01-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-11:56:01-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-11:56:01-root-INFO: grad norm: 39.980 39.484 6.278
2024-12-02-11:56:01-root-INFO: Loss too large (519.197->533.755)! Learning rate decreased to 0.02090.
2024-12-02-11:56:01-root-INFO: Loss too large (519.197->523.053)! Learning rate decreased to 0.01672.
2024-12-02-11:56:02-root-INFO: grad norm: 38.659 38.207 5.889
2024-12-02-11:56:02-root-INFO: grad norm: 37.496 37.148 5.100
2024-12-02-11:56:03-root-INFO: grad norm: 36.378 35.984 5.335
2024-12-02-11:56:03-root-INFO: grad norm: 35.417 35.098 4.740
2024-12-02-11:56:03-root-INFO: Loss Change: 519.197 -> 506.891
2024-12-02-11:56:03-root-INFO: Regularization Change: 0.000 -> 1.286
2024-12-02-11:56:03-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-11:56:03-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-11:56:04-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-11:56:04-root-INFO: grad norm: 30.368 30.007 4.668
2024-12-02-11:56:04-root-INFO: Loss too large (504.826->512.240)! Learning rate decreased to 0.02162.
2024-12-02-11:56:04-root-INFO: Loss too large (504.826->505.316)! Learning rate decreased to 0.01729.
2024-12-02-11:56:04-root-INFO: grad norm: 30.075 29.737 4.498
2024-12-02-11:56:05-root-INFO: grad norm: 30.061 29.703 4.625
2024-12-02-11:56:05-root-INFO: grad norm: 30.062 29.744 4.361
2024-12-02-11:56:06-root-INFO: grad norm: 30.074 29.727 4.555
2024-12-02-11:56:06-root-INFO: Loss Change: 504.826 -> 493.427
2024-12-02-11:56:06-root-INFO: Regularization Change: 0.000 -> 1.163
2024-12-02-11:56:06-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-11:56:06-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-11:56:06-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-11:56:07-root-INFO: grad norm: 35.981 35.518 5.756
2024-12-02-11:56:07-root-INFO: Loss too large (495.151->507.630)! Learning rate decreased to 0.02236.
2024-12-02-11:56:07-root-INFO: Loss too large (495.151->498.389)! Learning rate decreased to 0.01789.
2024-12-02-11:56:07-root-INFO: grad norm: 35.144 34.761 5.175
2024-12-02-11:56:08-root-INFO: grad norm: 34.539 34.220 4.681
2024-12-02-11:56:08-root-INFO: grad norm: 33.819 33.466 4.876
2024-12-02-11:56:09-root-INFO: grad norm: 33.149 32.848 4.452
2024-12-02-11:56:09-root-INFO: Loss Change: 495.151 -> 484.062
2024-12-02-11:56:09-root-INFO: Regularization Change: 0.000 -> 1.289
2024-12-02-11:56:09-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-11:56:09-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-11:56:09-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-11:56:10-root-INFO: grad norm: 30.298 29.972 4.439
2024-12-02-11:56:10-root-INFO: Loss too large (482.774->491.223)! Learning rate decreased to 0.02312.
2024-12-02-11:56:10-root-INFO: Loss too large (482.774->483.725)! Learning rate decreased to 0.01849.
2024-12-02-11:56:10-root-INFO: grad norm: 29.959 29.624 4.471
2024-12-02-11:56:11-root-INFO: grad norm: 29.684 29.344 4.480
2024-12-02-11:56:11-root-INFO: grad norm: 29.306 28.994 4.266
2024-12-02-11:56:12-root-INFO: grad norm: 29.087 28.752 4.399
2024-12-02-11:56:12-root-INFO: Loss Change: 482.774 -> 471.567
2024-12-02-11:56:12-root-INFO: Regularization Change: 0.000 -> 1.210
2024-12-02-11:56:12-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-11:56:12-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-11:56:12-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-11:56:13-root-INFO: grad norm: 35.400 34.928 5.763
2024-12-02-11:56:13-root-INFO: Loss too large (473.645->486.065)! Learning rate decreased to 0.02390.
2024-12-02-11:56:13-root-INFO: Loss too large (473.645->476.946)! Learning rate decreased to 0.01912.
2024-12-02-11:56:13-root-INFO: grad norm: 33.782 33.409 5.013
2024-12-02-11:56:14-root-INFO: grad norm: 32.128 31.822 4.421
2024-12-02-11:56:14-root-INFO: grad norm: 31.195 30.859 4.569
2024-12-02-11:56:15-root-INFO: grad norm: 30.362 30.076 4.160
2024-12-02-11:56:15-root-INFO: Loss Change: 473.645 -> 462.137
2024-12-02-11:56:15-root-INFO: Regularization Change: 0.000 -> 1.336
2024-12-02-11:56:15-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-11:56:15-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-11:56:15-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-11:56:15-root-INFO: grad norm: 27.955 27.611 4.373
2024-12-02-11:56:16-root-INFO: Loss too large (461.203->468.880)! Learning rate decreased to 0.02471.
2024-12-02-11:56:16-root-INFO: Loss too large (461.203->462.023)! Learning rate decreased to 0.01976.
2024-12-02-11:56:16-root-INFO: grad norm: 28.025 27.700 4.256
2024-12-02-11:56:17-root-INFO: grad norm: 27.713 27.375 4.318
2024-12-02-11:56:17-root-INFO: grad norm: 27.260 26.955 4.072
2024-12-02-11:56:18-root-INFO: grad norm: 27.001 26.669 4.216
2024-12-02-11:56:18-root-INFO: Loss Change: 461.203 -> 450.521
2024-12-02-11:56:18-root-INFO: Regularization Change: 0.000 -> 1.216
2024-12-02-11:56:18-root-INFO: Undo step: 130
2024-12-02-11:56:18-root-INFO: Undo step: 131
2024-12-02-11:56:18-root-INFO: Undo step: 132
2024-12-02-11:56:18-root-INFO: Undo step: 133
2024-12-02-11:56:18-root-INFO: Undo step: 134
2024-12-02-11:56:18-root-INFO: Undo step: 135
2024-12-02-11:56:18-root-INFO: Undo step: 136
2024-12-02-11:56:18-root-INFO: Undo step: 137
2024-12-02-11:56:18-root-INFO: Undo step: 138
2024-12-02-11:56:18-root-INFO: Undo step: 139
2024-12-02-11:56:18-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-11:56:18-root-INFO: grad norm: 372.889 368.056 59.838
2024-12-02-11:56:18-root-INFO: Loss too large (1628.458->1847.821)! Learning rate decreased to 0.01761.
2024-12-02-11:56:19-root-INFO: grad norm: 347.622 344.248 48.318
2024-12-02-11:56:19-root-INFO: grad norm: 158.600 154.770 34.644
2024-12-02-11:56:20-root-INFO: grad norm: 119.535 117.246 23.284
2024-12-02-11:56:20-root-INFO: grad norm: 93.315 91.436 18.630
2024-12-02-11:56:21-root-INFO: Loss Change: 1628.458 -> 687.362
2024-12-02-11:56:21-root-INFO: Regularization Change: 0.000 -> 60.271
2024-12-02-11:56:21-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-11:56:21-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:56:21-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-11:56:21-root-INFO: grad norm: 85.433 84.411 13.173
2024-12-02-11:56:21-root-INFO: grad norm: 107.815 106.564 16.376
2024-12-02-11:56:22-root-INFO: Loss too large (670.359->688.162)! Learning rate decreased to 0.01823.
2024-12-02-11:56:22-root-INFO: grad norm: 87.701 86.744 12.921
2024-12-02-11:56:22-root-INFO: grad norm: 64.274 63.180 11.808
2024-12-02-11:56:23-root-INFO: grad norm: 55.445 54.607 9.603
2024-12-02-11:56:23-root-INFO: Loss Change: 688.801 -> 571.272
2024-12-02-11:56:23-root-INFO: Regularization Change: 0.000 -> 10.499
2024-12-02-11:56:23-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-11:56:23-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:56:23-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-11:56:24-root-INFO: grad norm: 47.020 45.992 9.778
2024-12-02-11:56:24-root-INFO: grad norm: 59.683 58.756 10.478
2024-12-02-11:56:24-root-INFO: Loss too large (563.269->567.000)! Learning rate decreased to 0.01887.
2024-12-02-11:56:25-root-INFO: grad norm: 55.792 55.088 8.837
2024-12-02-11:56:25-root-INFO: grad norm: 53.443 52.855 7.910
2024-12-02-11:56:26-root-INFO: grad norm: 51.857 51.346 7.261
2024-12-02-11:56:26-root-INFO: Loss Change: 568.054 -> 531.419
2024-12-02-11:56:26-root-INFO: Regularization Change: 0.000 -> 4.689
2024-12-02-11:56:26-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-11:56:26-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:56:26-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-11:56:26-root-INFO: grad norm: 60.776 60.190 8.422
2024-12-02-11:56:26-root-INFO: Loss too large (535.302->544.657)! Learning rate decreased to 0.01952.
2024-12-02-11:56:27-root-INFO: grad norm: 57.713 57.405 5.960
2024-12-02-11:56:27-root-INFO: grad norm: 54.553 54.213 6.075
2024-12-02-11:56:28-root-INFO: grad norm: 52.284 51.986 5.576
2024-12-02-11:56:28-root-INFO: grad norm: 50.792 50.496 5.477
2024-12-02-11:56:29-root-INFO: Loss Change: 535.302 -> 505.247
2024-12-02-11:56:29-root-INFO: Regularization Change: 0.000 -> 3.411
2024-12-02-11:56:29-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-11:56:29-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-11:56:29-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-11:56:29-root-INFO: grad norm: 47.656 47.403 4.900
2024-12-02-11:56:29-root-INFO: Loss too large (503.083->512.355)! Learning rate decreased to 0.02020.
2024-12-02-11:56:30-root-INFO: grad norm: 48.435 48.150 5.250
2024-12-02-11:56:30-root-INFO: grad norm: 49.464 49.209 5.015
2024-12-02-11:56:31-root-INFO: grad norm: 50.329 50.066 5.135
2024-12-02-11:56:31-root-INFO: grad norm: 50.860 50.607 5.066
2024-12-02-11:56:31-root-INFO: Loss Change: 503.083 -> 488.849
2024-12-02-11:56:31-root-INFO: Regularization Change: 0.000 -> 2.695
2024-12-02-11:56:31-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-11:56:31-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-11:56:32-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-11:56:32-root-INFO: grad norm: 58.675 58.264 6.927
2024-12-02-11:56:32-root-INFO: Loss too large (492.457->506.267)! Learning rate decreased to 0.02090.
2024-12-02-11:56:32-root-INFO: grad norm: 57.073 56.820 5.370
2024-12-02-11:56:33-root-INFO: grad norm: 54.653 54.391 5.348
2024-12-02-11:56:33-root-INFO: grad norm: 52.333 52.079 5.147
2024-12-02-11:56:34-root-INFO: grad norm: 50.843 50.604 4.927
2024-12-02-11:56:34-root-INFO: Loss Change: 492.457 -> 471.644
2024-12-02-11:56:34-root-INFO: Regularization Change: 0.000 -> 2.805
2024-12-02-11:56:34-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-11:56:34-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-11:56:34-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-11:56:34-root-INFO: grad norm: 44.767 44.547 4.436
2024-12-02-11:56:35-root-INFO: Loss too large (467.726->477.512)! Learning rate decreased to 0.02162.
2024-12-02-11:56:35-root-INFO: grad norm: 45.119 44.847 4.951
2024-12-02-11:56:35-root-INFO: grad norm: 46.027 45.807 4.499
2024-12-02-11:56:36-root-INFO: grad norm: 46.942 46.702 4.739
2024-12-02-11:56:36-root-INFO: grad norm: 47.632 47.408 4.612
2024-12-02-11:56:37-root-INFO: Loss Change: 467.726 -> 457.758
2024-12-02-11:56:37-root-INFO: Regularization Change: 0.000 -> 2.348
2024-12-02-11:56:37-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-11:56:37-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-11:56:37-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-11:56:37-root-INFO: grad norm: 55.941 55.530 6.764
2024-12-02-11:56:37-root-INFO: Loss too large (462.407->476.975)! Learning rate decreased to 0.02236.
2024-12-02-11:56:38-root-INFO: grad norm: 54.711 54.469 5.140
2024-12-02-11:56:38-root-INFO: grad norm: 52.549 52.294 5.173
2024-12-02-11:56:39-root-INFO: grad norm: 50.282 50.042 4.911
2024-12-02-11:56:39-root-INFO: grad norm: 48.681 48.448 4.756
2024-12-02-11:56:39-root-INFO: Loss Change: 462.407 -> 444.646
2024-12-02-11:56:39-root-INFO: Regularization Change: 0.000 -> 2.634
2024-12-02-11:56:39-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-11:56:39-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-11:56:39-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-11:56:40-root-INFO: grad norm: 44.362 44.166 4.164
2024-12-02-11:56:40-root-INFO: Loss too large (442.124->453.068)! Learning rate decreased to 0.02312.
2024-12-02-11:56:40-root-INFO: grad norm: 43.959 43.703 4.741
2024-12-02-11:56:41-root-INFO: grad norm: 43.601 43.395 4.233
2024-12-02-11:56:41-root-INFO: grad norm: 43.628 43.402 4.437
2024-12-02-11:56:42-root-INFO: grad norm: 43.596 43.387 4.258
2024-12-02-11:56:42-root-INFO: Loss Change: 442.124 -> 432.229
2024-12-02-11:56:42-root-INFO: Regularization Change: 0.000 -> 2.227
2024-12-02-11:56:42-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-11:56:42-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-11:56:42-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-11:56:42-root-INFO: grad norm: 50.441 50.057 6.213
2024-12-02-11:56:43-root-INFO: Loss too large (436.067->448.841)! Learning rate decreased to 0.02390.
2024-12-02-11:56:43-root-INFO: grad norm: 49.305 49.089 4.617
2024-12-02-11:56:44-root-INFO: grad norm: 47.388 47.148 4.763
2024-12-02-11:56:44-root-INFO: grad norm: 45.399 45.178 4.471
2024-12-02-11:56:44-root-INFO: grad norm: 43.923 43.701 4.416
2024-12-02-11:56:45-root-INFO: Loss Change: 436.067 -> 420.648
2024-12-02-11:56:45-root-INFO: Regularization Change: 0.000 -> 2.444
2024-12-02-11:56:45-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-11:56:45-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-11:56:45-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-11:56:45-root-INFO: grad norm: 40.476 40.271 4.064
2024-12-02-11:56:45-root-INFO: Loss too large (419.077->428.856)! Learning rate decreased to 0.02471.
2024-12-02-11:56:46-root-INFO: grad norm: 40.063 39.807 4.524
2024-12-02-11:56:46-root-INFO: grad norm: 39.706 39.502 4.020
2024-12-02-11:56:47-root-INFO: grad norm: 39.646 39.418 4.250
2024-12-02-11:56:47-root-INFO: grad norm: 39.534 39.329 4.013
2024-12-02-11:56:47-root-INFO: Loss Change: 419.077 -> 410.309
2024-12-02-11:56:47-root-INFO: Regularization Change: 0.000 -> 2.089
2024-12-02-11:56:47-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-11:56:47-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-11:56:48-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-11:56:48-root-INFO: grad norm: 46.592 46.220 5.874
2024-12-02-11:56:48-root-INFO: Loss too large (413.937->425.611)! Learning rate decreased to 0.02553.
2024-12-02-11:56:48-root-INFO: grad norm: 45.259 45.046 4.390
2024-12-02-11:56:49-root-INFO: grad norm: 43.508 43.272 4.533
2024-12-02-11:56:49-root-INFO: grad norm: 41.722 41.509 4.215
2024-12-02-11:56:50-root-INFO: grad norm: 40.397 40.174 4.239
2024-12-02-11:56:50-root-INFO: Loss Change: 413.937 -> 400.188
2024-12-02-11:56:50-root-INFO: Regularization Change: 0.000 -> 2.337
2024-12-02-11:56:50-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-11:56:50-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-11:56:50-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-11:56:50-root-INFO: grad norm: 35.469 35.247 3.956
2024-12-02-11:56:51-root-INFO: Loss too large (397.569->405.125)! Learning rate decreased to 0.02639.
2024-12-02-11:56:51-root-INFO: grad norm: 35.103 34.849 4.214
2024-12-02-11:56:52-root-INFO: grad norm: 35.039 34.840 3.729
2024-12-02-11:56:52-root-INFO: grad norm: 35.238 35.013 3.980
2024-12-02-11:56:52-root-INFO: grad norm: 35.450 35.250 3.760
2024-12-02-11:56:53-root-INFO: Loss Change: 397.569 -> 389.922
2024-12-02-11:56:53-root-INFO: Regularization Change: 0.000 -> 1.960
2024-12-02-11:56:53-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-11:56:53-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-11:56:53-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-11:56:53-root-INFO: grad norm: 39.724 39.430 4.824
2024-12-02-11:56:53-root-INFO: Loss too large (391.794->401.973)! Learning rate decreased to 0.02726.
2024-12-02-11:56:54-root-INFO: grad norm: 40.083 39.887 3.963
2024-12-02-11:56:54-root-INFO: grad norm: 39.825 39.596 4.265
2024-12-02-11:56:55-root-INFO: grad norm: 39.268 39.063 4.008
2024-12-02-11:56:55-root-INFO: grad norm: 38.746 38.521 4.170
2024-12-02-11:56:55-root-INFO: Loss Change: 391.794 -> 382.609
2024-12-02-11:56:55-root-INFO: Regularization Change: 0.000 -> 2.190
2024-12-02-11:56:55-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-11:56:55-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-11:56:56-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-11:56:56-root-INFO: grad norm: 35.998 35.805 3.722
2024-12-02-11:56:56-root-INFO: Loss too large (381.341->390.363)! Learning rate decreased to 0.02816.
2024-12-02-11:56:56-root-INFO: grad norm: 35.990 35.749 4.164
2024-12-02-11:56:57-root-INFO: grad norm: 36.053 35.855 3.770
2024-12-02-11:56:57-root-INFO: grad norm: 36.158 35.930 4.056
2024-12-02-11:56:58-root-INFO: grad norm: 36.219 36.017 3.824
2024-12-02-11:56:58-root-INFO: Loss Change: 381.341 -> 374.441
2024-12-02-11:56:58-root-INFO: Regularization Change: 0.000 -> 2.075
2024-12-02-11:56:58-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-11:56:58-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-11:56:58-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-11:56:58-root-INFO: grad norm: 41.584 41.248 5.282
2024-12-02-11:56:59-root-INFO: Loss too large (377.207->388.702)! Learning rate decreased to 0.02909.
2024-12-02-11:56:59-root-INFO: grad norm: 40.753 40.546 4.104
2024-12-02-11:57:00-root-INFO: grad norm: 39.642 39.407 4.305
2024-12-02-11:57:00-root-INFO: grad norm: 38.458 38.249 4.005
2024-12-02-11:57:00-root-INFO: grad norm: 37.502 37.277 4.099
2024-12-02-11:57:01-root-INFO: Loss Change: 377.207 -> 366.533
2024-12-02-11:57:01-root-INFO: Regularization Change: 0.000 -> 2.369
2024-12-02-11:57:01-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-11:57:01-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-11:57:01-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-11:57:01-root-INFO: grad norm: 34.376 34.103 4.323
2024-12-02-11:57:01-root-INFO: Loss too large (364.729->372.528)! Learning rate decreased to 0.03019.
2024-12-02-11:57:02-root-INFO: grad norm: 33.389 33.107 4.325
2024-12-02-11:57:02-root-INFO: grad norm: 32.747 32.530 3.758
2024-12-02-11:57:03-root-INFO: grad norm: 32.646 32.406 3.954
2024-12-02-11:57:03-root-INFO: grad norm: 32.609 32.400 3.686
2024-12-02-11:57:03-root-INFO: Loss Change: 364.729 -> 356.840
2024-12-02-11:57:03-root-INFO: Regularization Change: 0.000 -> 2.109
2024-12-02-11:57:03-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-11:57:04-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-11:57:04-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-11:57:04-root-INFO: grad norm: 36.569 36.269 4.677
2024-12-02-11:57:04-root-INFO: Loss too large (358.927->369.065)! Learning rate decreased to 0.03117.
2024-12-02-11:57:04-root-INFO: grad norm: 36.708 36.510 3.807
2024-12-02-11:57:05-root-INFO: grad norm: 36.607 36.374 4.122
2024-12-02-11:57:05-root-INFO: grad norm: 36.308 36.105 3.837
2024-12-02-11:57:06-root-INFO: grad norm: 36.026 35.800 4.032
2024-12-02-11:57:06-root-INFO: Loss Change: 358.927 -> 351.237
2024-12-02-11:57:06-root-INFO: Regularization Change: 0.000 -> 2.336
2024-12-02-11:57:06-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-11:57:06-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-11:57:06-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-11:57:06-root-INFO: grad norm: 33.911 33.704 3.749
2024-12-02-11:57:07-root-INFO: Loss too large (350.085->359.043)! Learning rate decreased to 0.03218.
2024-12-02-11:57:07-root-INFO: grad norm: 33.517 33.264 4.109
2024-12-02-11:57:08-root-INFO: grad norm: 33.117 32.915 3.659
2024-12-02-11:57:08-root-INFO: grad norm: 33.032 32.805 3.868
2024-12-02-11:57:08-root-INFO: grad norm: 32.970 32.770 3.631
2024-12-02-11:57:09-root-INFO: Loss Change: 350.085 -> 343.044
2024-12-02-11:57:09-root-INFO: Regularization Change: 0.000 -> 2.213
2024-12-02-11:57:09-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-11:57:09-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-11:57:09-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-11:57:09-root-INFO: grad norm: 36.661 36.369 4.623
2024-12-02-11:57:09-root-INFO: Loss too large (344.919->356.077)! Learning rate decreased to 0.03321.
2024-12-02-11:57:10-root-INFO: grad norm: 36.482 36.282 3.817
2024-12-02-11:57:10-root-INFO: grad norm: 36.075 35.849 4.039
2024-12-02-11:57:11-root-INFO: grad norm: 35.550 35.346 3.809
2024-12-02-11:57:11-root-INFO: grad norm: 35.114 34.893 3.929
2024-12-02-11:57:11-root-INFO: Loss Change: 344.919 -> 337.290
2024-12-02-11:57:11-root-INFO: Regularization Change: 0.000 -> 2.467
2024-12-02-11:57:11-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-11:57:11-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-11:57:12-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-11:57:12-root-INFO: grad norm: 32.783 32.583 3.617
2024-12-02-11:57:12-root-INFO: Loss too large (336.145->345.066)! Learning rate decreased to 0.03427.
2024-12-02-11:57:12-root-INFO: grad norm: 32.656 32.418 3.934
2024-12-02-11:57:13-root-INFO: grad norm: 32.613 32.409 3.638
2024-12-02-11:57:13-root-INFO: grad norm: 32.689 32.466 3.811
2024-12-02-11:57:14-root-INFO: grad norm: 32.761 32.557 3.654
2024-12-02-11:57:14-root-INFO: Loss Change: 336.145 -> 329.745
2024-12-02-11:57:14-root-INFO: Regularization Change: 0.000 -> 2.324
2024-12-02-11:57:14-root-INFO: Undo step: 120
2024-12-02-11:57:14-root-INFO: Undo step: 121
2024-12-02-11:57:14-root-INFO: Undo step: 122
2024-12-02-11:57:14-root-INFO: Undo step: 123
2024-12-02-11:57:14-root-INFO: Undo step: 124
2024-12-02-11:57:14-root-INFO: Undo step: 125
2024-12-02-11:57:14-root-INFO: Undo step: 126
2024-12-02-11:57:14-root-INFO: Undo step: 127
2024-12-02-11:57:14-root-INFO: Undo step: 128
2024-12-02-11:57:14-root-INFO: Undo step: 129
2024-12-02-11:57:14-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-11:57:14-root-INFO: grad norm: 234.694 230.219 45.611
2024-12-02-11:57:15-root-INFO: grad norm: 96.778 94.907 18.940
2024-12-02-11:57:15-root-INFO: grad norm: 77.989 75.695 18.774
2024-12-02-11:57:16-root-INFO: grad norm: 67.318 65.940 13.549
2024-12-02-11:57:16-root-INFO: grad norm: 61.930 60.068 15.070
2024-12-02-11:57:17-root-INFO: Loss Change: 1048.097 -> 476.380
2024-12-02-11:57:17-root-INFO: Regularization Change: 0.000 -> 76.930
2024-12-02-11:57:17-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-11:57:17-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-11:57:17-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-11:57:17-root-INFO: grad norm: 73.980 72.593 14.260
2024-12-02-11:57:17-root-INFO: grad norm: 74.189 72.542 15.544
2024-12-02-11:57:18-root-INFO: grad norm: 73.405 72.475 11.648
2024-12-02-11:57:18-root-INFO: grad norm: 74.563 73.398 13.126
2024-12-02-11:57:19-root-INFO: grad norm: 76.523 75.801 10.492
2024-12-02-11:57:19-root-INFO: Loss too large (451.036->456.878)! Learning rate decreased to 0.02553.
2024-12-02-11:57:19-root-INFO: Loss Change: 485.324 -> 425.384
2024-12-02-11:57:19-root-INFO: Regularization Change: 0.000 -> 10.898
2024-12-02-11:57:19-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-11:57:19-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-11:57:19-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-11:57:20-root-INFO: grad norm: 55.016 54.479 7.667
2024-12-02-11:57:20-root-INFO: Loss too large (419.830->424.563)! Learning rate decreased to 0.02639.
2024-12-02-11:57:20-root-INFO: grad norm: 40.604 40.200 5.713
2024-12-02-11:57:21-root-INFO: grad norm: 28.593 28.121 5.178
2024-12-02-11:57:21-root-INFO: grad norm: 28.282 27.919 4.517
2024-12-02-11:57:22-root-INFO: grad norm: 34.538 34.141 5.226
2024-12-02-11:57:22-root-INFO: Loss too large (385.698->386.359)! Learning rate decreased to 0.02111.
2024-12-02-11:57:22-root-INFO: Loss Change: 419.830 -> 382.067
2024-12-02-11:57:22-root-INFO: Regularization Change: 0.000 -> 3.931
2024-12-02-11:57:22-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-11:57:22-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-11:57:22-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-11:57:22-root-INFO: grad norm: 31.783 31.365 5.133
2024-12-02-11:57:23-root-INFO: Loss too large (382.797->389.018)! Learning rate decreased to 0.02726.
2024-12-02-11:57:23-root-INFO: grad norm: 39.868 39.495 5.440
2024-12-02-11:57:23-root-INFO: Loss too large (380.876->383.006)! Learning rate decreased to 0.02181.
2024-12-02-11:57:24-root-INFO: grad norm: 30.335 29.953 4.795
2024-12-02-11:57:24-root-INFO: grad norm: 21.134 20.764 3.937
2024-12-02-11:57:25-root-INFO: grad norm: 20.372 20.005 3.852
2024-12-02-11:57:25-root-INFO: Loss Change: 382.797 -> 364.427
2024-12-02-11:57:25-root-INFO: Regularization Change: 0.000 -> 1.906
2024-12-02-11:57:25-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-11:57:25-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-11:57:25-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-11:57:25-root-INFO: grad norm: 20.120 19.742 3.878
2024-12-02-11:57:25-root-INFO: Loss too large (364.147->365.593)! Learning rate decreased to 0.02816.
2024-12-02-11:57:26-root-INFO: grad norm: 24.550 24.161 4.355
2024-12-02-11:57:26-root-INFO: grad norm: 33.324 32.995 4.674
2024-12-02-11:57:26-root-INFO: Loss too large (362.337->365.129)! Learning rate decreased to 0.02253.
2024-12-02-11:57:27-root-INFO: grad norm: 27.098 26.714 4.543
2024-12-02-11:57:27-root-INFO: grad norm: 19.938 19.637 3.454
2024-12-02-11:57:28-root-INFO: Loss Change: 364.147 -> 353.167
2024-12-02-11:57:28-root-INFO: Regularization Change: 0.000 -> 1.575
2024-12-02-11:57:28-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-11:57:28-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-11:57:28-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-11:57:28-root-INFO: grad norm: 22.115 21.706 4.233
2024-12-02-11:57:28-root-INFO: Loss too large (353.208->356.562)! Learning rate decreased to 0.02909.
2024-12-02-11:57:29-root-INFO: grad norm: 29.612 29.320 4.146
2024-12-02-11:57:29-root-INFO: Loss too large (352.374->354.573)! Learning rate decreased to 0.02327.
2024-12-02-11:57:29-root-INFO: grad norm: 25.302 24.983 4.004
2024-12-02-11:57:30-root-INFO: grad norm: 20.843 20.571 3.352
2024-12-02-11:57:30-root-INFO: grad norm: 19.957 19.672 3.363
2024-12-02-11:57:31-root-INFO: Loss Change: 353.208 -> 342.774
2024-12-02-11:57:31-root-INFO: Regularization Change: 0.000 -> 1.338
2024-12-02-11:57:31-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-11:57:31-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-11:57:31-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-11:57:31-root-INFO: grad norm: 23.737 23.306 4.503
2024-12-02-11:57:31-root-INFO: Loss too large (343.408->347.578)! Learning rate decreased to 0.03019.
2024-12-02-11:57:31-root-INFO: Loss too large (343.408->343.507)! Learning rate decreased to 0.02415.
2024-12-02-11:57:32-root-INFO: grad norm: 21.408 21.083 3.715
2024-12-02-11:57:32-root-INFO: grad norm: 20.555 20.297 3.246
2024-12-02-11:57:33-root-INFO: grad norm: 20.181 19.888 3.421
2024-12-02-11:57:33-root-INFO: grad norm: 19.861 19.616 3.110
2024-12-02-11:57:34-root-INFO: Loss Change: 343.408 -> 334.421
2024-12-02-11:57:34-root-INFO: Regularization Change: 0.000 -> 1.227
2024-12-02-11:57:34-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-11:57:34-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-11:57:34-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-11:57:34-root-INFO: grad norm: 19.953 19.638 3.532
2024-12-02-11:57:34-root-INFO: Loss too large (334.207->338.199)! Learning rate decreased to 0.03117.
2024-12-02-11:57:34-root-INFO: Loss too large (334.207->334.255)! Learning rate decreased to 0.02494.
2024-12-02-11:57:35-root-INFO: grad norm: 19.537 19.303 3.012
2024-12-02-11:57:35-root-INFO: grad norm: 19.422 19.162 3.169
2024-12-02-11:57:36-root-INFO: grad norm: 19.370 19.141 2.967
2024-12-02-11:57:36-root-INFO: grad norm: 19.327 19.078 3.093
2024-12-02-11:57:37-root-INFO: Loss Change: 334.207 -> 326.579
2024-12-02-11:57:37-root-INFO: Regularization Change: 0.000 -> 1.098
2024-12-02-11:57:37-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-11:57:37-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-11:57:37-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-11:57:37-root-INFO: grad norm: 21.385 21.082 3.589
2024-12-02-11:57:37-root-INFO: Loss too large (326.963->331.778)! Learning rate decreased to 0.03218.
2024-12-02-11:57:37-root-INFO: Loss too large (326.963->327.857)! Learning rate decreased to 0.02574.
2024-12-02-11:57:38-root-INFO: grad norm: 20.134 19.866 3.274
2024-12-02-11:57:38-root-INFO: grad norm: 19.397 19.165 2.992
2024-12-02-11:57:39-root-INFO: grad norm: 19.024 18.778 3.050
2024-12-02-11:57:39-root-INFO: grad norm: 18.679 18.455 2.880
2024-12-02-11:57:40-root-INFO: Loss Change: 326.963 -> 319.796
2024-12-02-11:57:40-root-INFO: Regularization Change: 0.000 -> 1.090
2024-12-02-11:57:40-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-11:57:40-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-11:57:40-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-11:57:40-root-INFO: grad norm: 19.245 18.961 3.299
2024-12-02-11:57:40-root-INFO: Loss too large (319.651->323.979)! Learning rate decreased to 0.03321.
2024-12-02-11:57:40-root-INFO: Loss too large (319.651->320.011)! Learning rate decreased to 0.02657.
2024-12-02-11:57:41-root-INFO: grad norm: 18.698 18.487 2.801
2024-12-02-11:57:41-root-INFO: grad norm: 18.590 18.362 2.905
2024-12-02-11:57:42-root-INFO: grad norm: 18.568 18.358 2.788
2024-12-02-11:57:42-root-INFO: grad norm: 18.535 18.316 2.839
2024-12-02-11:57:42-root-INFO: Loss Change: 319.651 -> 313.005
2024-12-02-11:57:42-root-INFO: Regularization Change: 0.000 -> 1.048
2024-12-02-11:57:42-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-11:57:42-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-11:57:42-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-11:57:43-root-INFO: grad norm: 19.741 19.445 3.405
2024-12-02-11:57:43-root-INFO: Loss too large (313.393->317.972)! Learning rate decreased to 0.03427.
2024-12-02-11:57:43-root-INFO: Loss too large (313.393->314.220)! Learning rate decreased to 0.02742.
2024-12-02-11:57:43-root-INFO: grad norm: 18.891 18.651 3.002
2024-12-02-11:57:44-root-INFO: grad norm: 18.690 18.467 2.878
2024-12-02-11:57:44-root-INFO: grad norm: 18.644 18.421 2.873
2024-12-02-11:57:45-root-INFO: grad norm: 18.621 18.405 2.828
2024-12-02-11:57:45-root-INFO: Loss Change: 313.393 -> 307.297
2024-12-02-11:57:45-root-INFO: Regularization Change: 0.000 -> 1.062
2024-12-02-11:57:45-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-11:57:45-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-11:57:45-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-11:57:45-root-INFO: grad norm: 19.510 19.205 3.435
2024-12-02-11:57:46-root-INFO: Loss too large (306.886->311.751)! Learning rate decreased to 0.03536.
2024-12-02-11:57:46-root-INFO: Loss too large (306.886->307.392)! Learning rate decreased to 0.02829.
2024-12-02-11:57:46-root-INFO: grad norm: 18.501 18.305 2.692
2024-12-02-11:57:47-root-INFO: grad norm: 18.284 18.079 2.730
2024-12-02-11:57:47-root-INFO: grad norm: 18.210 18.018 2.639
2024-12-02-11:57:48-root-INFO: grad norm: 18.173 17.980 2.639
2024-12-02-11:57:48-root-INFO: Loss Change: 306.886 -> 300.519
2024-12-02-11:57:48-root-INFO: Regularization Change: 0.000 -> 1.073
2024-12-02-11:57:48-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-11:57:48-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-11:57:48-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-11:57:48-root-INFO: grad norm: 18.715 18.473 2.995
2024-12-02-11:57:48-root-INFO: Loss too large (301.592->306.184)! Learning rate decreased to 0.03648.
2024-12-02-11:57:49-root-INFO: Loss too large (301.592->302.461)! Learning rate decreased to 0.02919.
2024-12-02-11:57:49-root-INFO: grad norm: 17.851 17.639 2.741
2024-12-02-11:57:49-root-INFO: grad norm: 17.395 17.192 2.649
2024-12-02-11:57:50-root-INFO: grad norm: 17.090 16.897 2.560
2024-12-02-11:57:50-root-INFO: grad norm: 16.830 16.633 2.566
2024-12-02-11:57:51-root-INFO: Loss Change: 301.592 -> 295.680
2024-12-02-11:57:51-root-INFO: Regularization Change: 0.000 -> 1.031
2024-12-02-11:57:51-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-11:57:51-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-11:57:51-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-11:57:51-root-INFO: grad norm: 20.404 20.128 3.346
2024-12-02-11:57:51-root-INFO: Loss too large (296.403->303.167)! Learning rate decreased to 0.03763.
2024-12-02-11:57:51-root-INFO: Loss too large (296.403->297.740)! Learning rate decreased to 0.03011.
2024-12-02-11:57:52-root-INFO: grad norm: 19.562 19.392 2.573
2024-12-02-11:57:52-root-INFO: grad norm: 19.687 19.510 2.627
2024-12-02-11:57:53-root-INFO: grad norm: 20.012 19.847 2.559
2024-12-02-11:57:53-root-INFO: grad norm: 20.501 20.340 2.568
2024-12-02-11:57:54-root-INFO: Loss Change: 296.403 -> 291.172
2024-12-02-11:57:54-root-INFO: Regularization Change: 0.000 -> 1.147
2024-12-02-11:57:54-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-11:57:54-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-11:57:54-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-11:57:54-root-INFO: grad norm: 18.436 18.254 2.588
2024-12-02-11:57:54-root-INFO: Loss too large (290.233->295.748)! Learning rate decreased to 0.03881.
2024-12-02-11:57:54-root-INFO: Loss too large (290.233->291.302)! Learning rate decreased to 0.03105.
2024-12-02-11:57:55-root-INFO: grad norm: 17.918 17.736 2.542
2024-12-02-11:57:55-root-INFO: grad norm: 18.081 17.921 2.406
2024-12-02-11:57:56-root-INFO: grad norm: 18.442 18.283 2.417
2024-12-02-11:57:56-root-INFO: grad norm: 18.836 18.678 2.439
2024-12-02-11:57:56-root-INFO: Loss Change: 290.233 -> 285.277
2024-12-02-11:57:56-root-INFO: Regularization Change: 0.000 -> 1.089
2024-12-02-11:57:56-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-11:57:56-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-11:57:57-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-11:57:57-root-INFO: grad norm: 24.710 24.476 3.395
2024-12-02-11:57:57-root-INFO: Loss too large (287.127->301.083)! Learning rate decreased to 0.04002.
2024-12-02-11:57:57-root-INFO: Loss too large (287.127->291.761)! Learning rate decreased to 0.03202.
2024-12-02-11:57:58-root-INFO: grad norm: 24.752 24.599 2.743
2024-12-02-11:57:58-root-INFO: grad norm: 24.888 24.726 2.831
2024-12-02-11:57:58-root-INFO: grad norm: 25.032 24.879 2.757
2024-12-02-11:57:59-root-INFO: grad norm: 25.002 24.853 2.727
2024-12-02-11:57:59-root-INFO: Loss Change: 287.127 -> 282.790
2024-12-02-11:57:59-root-INFO: Regularization Change: 0.000 -> 1.378
2024-12-02-11:57:59-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-11:57:59-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-11:57:59-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-11:58:00-root-INFO: grad norm: 21.258 21.124 2.384
2024-12-02-11:58:00-root-INFO: Loss too large (281.546->291.051)! Learning rate decreased to 0.04126.
2024-12-02-11:58:00-root-INFO: Loss too large (281.546->283.880)! Learning rate decreased to 0.03301.
2024-12-02-11:58:00-root-INFO: grad norm: 20.614 20.460 2.513
2024-12-02-11:58:01-root-INFO: grad norm: 20.516 20.377 2.385
2024-12-02-11:58:01-root-INFO: grad norm: 20.534 20.393 2.403
2024-12-02-11:58:02-root-INFO: grad norm: 20.594 20.454 2.400
2024-12-02-11:58:02-root-INFO: Loss Change: 281.546 -> 276.312
2024-12-02-11:58:02-root-INFO: Regularization Change: 0.000 -> 1.230
2024-12-02-11:58:02-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-11:58:02-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-11:58:02-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-11:58:02-root-INFO: grad norm: 25.754 25.529 3.394
2024-12-02-11:58:03-root-INFO: Loss too large (278.061->294.090)! Learning rate decreased to 0.04253.
2024-12-02-11:58:03-root-INFO: Loss too large (278.061->283.446)! Learning rate decreased to 0.03403.
2024-12-02-11:58:03-root-INFO: grad norm: 25.092 24.949 2.670
2024-12-02-11:58:04-root-INFO: grad norm: 24.343 24.192 2.713
2024-12-02-11:58:04-root-INFO: grad norm: 23.873 23.734 2.572
2024-12-02-11:58:05-root-INFO: grad norm: 23.307 23.169 2.531
2024-12-02-11:58:05-root-INFO: Loss Change: 278.061 -> 272.765
2024-12-02-11:58:05-root-INFO: Regularization Change: 0.000 -> 1.430
2024-12-02-11:58:05-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-11:58:05-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-11:58:05-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-11:58:05-root-INFO: grad norm: 19.257 19.132 2.187
2024-12-02-11:58:05-root-INFO: Loss too large (271.221->279.057)! Learning rate decreased to 0.04384.
2024-12-02-11:58:06-root-INFO: Loss too large (271.221->272.932)! Learning rate decreased to 0.03507.
2024-12-02-11:58:06-root-INFO: grad norm: 18.456 18.310 2.317
2024-12-02-11:58:07-root-INFO: grad norm: 18.339 18.212 2.147
2024-12-02-11:58:07-root-INFO: grad norm: 18.329 18.198 2.192
2024-12-02-11:58:07-root-INFO: grad norm: 18.384 18.255 2.173
2024-12-02-11:58:08-root-INFO: Loss Change: 271.221 -> 266.124
2024-12-02-11:58:08-root-INFO: Regularization Change: 0.000 -> 1.206
2024-12-02-11:58:08-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-11:58:08-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-11:58:08-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-11:58:08-root-INFO: grad norm: 22.087 21.903 2.843
2024-12-02-11:58:08-root-INFO: Loss too large (267.352->279.794)! Learning rate decreased to 0.04517.
2024-12-02-11:58:08-root-INFO: Loss too large (267.352->271.308)! Learning rate decreased to 0.03614.
2024-12-02-11:58:09-root-INFO: grad norm: 21.690 21.565 2.328
2024-12-02-11:58:09-root-INFO: grad norm: 21.360 21.225 2.400
2024-12-02-11:58:10-root-INFO: grad norm: 21.052 20.926 2.304
2024-12-02-11:58:10-root-INFO: grad norm: 20.741 20.615 2.283
2024-12-02-11:58:11-root-INFO: Loss Change: 267.352 -> 262.686
2024-12-02-11:58:11-root-INFO: Regularization Change: 0.000 -> 1.348
2024-12-02-11:58:11-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-11:58:11-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-11:58:11-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-11:58:11-root-INFO: grad norm: 17.545 17.434 1.974
2024-12-02-11:58:11-root-INFO: Loss too large (261.705->268.930)! Learning rate decreased to 0.04654.
2024-12-02-11:58:11-root-INFO: Loss too large (261.705->263.377)! Learning rate decreased to 0.03724.
2024-12-02-11:58:12-root-INFO: grad norm: 17.182 17.049 2.134
2024-12-02-11:58:12-root-INFO: grad norm: 17.226 17.111 1.987
2024-12-02-11:58:13-root-INFO: grad norm: 17.343 17.223 2.038
2024-12-02-11:58:13-root-INFO: grad norm: 17.502 17.385 2.017
2024-12-02-11:58:13-root-INFO: Loss Change: 261.705 -> 257.237
2024-12-02-11:58:13-root-INFO: Regularization Change: 0.000 -> 1.193
2024-12-02-11:58:13-root-INFO: Undo step: 110
2024-12-02-11:58:13-root-INFO: Undo step: 111
2024-12-02-11:58:13-root-INFO: Undo step: 112
2024-12-02-11:58:13-root-INFO: Undo step: 113
2024-12-02-11:58:13-root-INFO: Undo step: 114
2024-12-02-11:58:13-root-INFO: Undo step: 115
2024-12-02-11:58:13-root-INFO: Undo step: 116
2024-12-02-11:58:13-root-INFO: Undo step: 117
2024-12-02-11:58:13-root-INFO: Undo step: 118
2024-12-02-11:58:13-root-INFO: Undo step: 119
2024-12-02-11:58:14-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-11:58:14-root-INFO: grad norm: 123.867 121.171 25.702
2024-12-02-11:58:14-root-INFO: grad norm: 82.570 81.102 15.500
2024-12-02-11:58:15-root-INFO: grad norm: 60.647 59.158 13.354
2024-12-02-11:58:15-root-INFO: grad norm: 56.532 55.659 9.899
2024-12-02-11:58:16-root-INFO: grad norm: 57.279 56.406 9.963
2024-12-02-11:58:16-root-INFO: Loss Change: 876.265 -> 392.098
2024-12-02-11:58:16-root-INFO: Regularization Change: 0.000 -> 79.054
2024-12-02-11:58:16-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-11:58:16-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-11:58:16-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-11:58:16-root-INFO: grad norm: 61.209 60.474 9.460
2024-12-02-11:58:17-root-INFO: grad norm: 62.490 61.666 10.111
2024-12-02-11:58:17-root-INFO: grad norm: 65.343 64.670 9.349
2024-12-02-11:58:18-root-INFO: grad norm: 62.422 61.579 10.221
2024-12-02-11:58:18-root-INFO: grad norm: 61.319 60.675 8.859
2024-12-02-11:58:18-root-INFO: Loss too large (359.315->361.757)! Learning rate decreased to 0.03536.
2024-12-02-11:58:19-root-INFO: Loss Change: 396.128 -> 335.677
2024-12-02-11:58:19-root-INFO: Regularization Change: 0.000 -> 13.494
2024-12-02-11:58:19-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-11:58:19-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-11:58:19-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-11:58:19-root-INFO: grad norm: 41.297 40.776 6.541
2024-12-02-11:58:19-root-INFO: grad norm: 49.721 49.204 7.156
2024-12-02-11:58:20-root-INFO: Loss too large (332.113->344.239)! Learning rate decreased to 0.03648.
2024-12-02-11:58:20-root-INFO: grad norm: 42.462 41.938 6.651
2024-12-02-11:58:21-root-INFO: grad norm: 36.725 36.345 5.267
2024-12-02-11:58:21-root-INFO: grad norm: 34.355 33.945 5.293
2024-12-02-11:58:21-root-INFO: Loss Change: 333.456 -> 305.445
2024-12-02-11:58:21-root-INFO: Regularization Change: 0.000 -> 5.594
2024-12-02-11:58:21-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-11:58:21-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-11:58:22-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-11:58:22-root-INFO: grad norm: 40.994 40.453 6.635
2024-12-02-11:58:22-root-INFO: Loss too large (309.813->323.882)! Learning rate decreased to 0.03763.
2024-12-02-11:58:22-root-INFO: grad norm: 39.333 38.906 5.782
2024-12-02-11:58:23-root-INFO: grad norm: 37.241 36.869 5.249
2024-12-02-11:58:23-root-INFO: grad norm: 36.349 35.966 5.265
2024-12-02-11:58:24-root-INFO: grad norm: 36.104 35.766 4.929
2024-12-02-11:58:24-root-INFO: Loss Change: 309.813 -> 295.541
2024-12-02-11:58:24-root-INFO: Regularization Change: 0.000 -> 4.159
2024-12-02-11:58:24-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-11:58:24-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-11:58:24-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-11:58:24-root-INFO: grad norm: 32.905 32.632 4.237
2024-12-02-11:58:24-root-INFO: Loss too large (292.781->302.631)! Learning rate decreased to 0.03881.
2024-12-02-11:58:25-root-INFO: grad norm: 33.207 32.901 4.495
2024-12-02-11:58:25-root-INFO: grad norm: 33.729 33.415 4.590
2024-12-02-11:58:26-root-INFO: grad norm: 34.656 34.351 4.584
2024-12-02-11:58:26-root-INFO: grad norm: 35.203 34.874 4.804
2024-12-02-11:58:27-root-INFO: Loss Change: 292.781 -> 285.444
2024-12-02-11:58:27-root-INFO: Regularization Change: 0.000 -> 3.393
2024-12-02-11:58:27-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-11:58:27-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-11:58:27-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-11:58:27-root-INFO: grad norm: 42.223 41.757 6.254
2024-12-02-11:58:27-root-INFO: Loss too large (290.050->309.339)! Learning rate decreased to 0.04002.
2024-12-02-11:58:27-root-INFO: Loss too large (290.050->290.087)! Learning rate decreased to 0.03202.
2024-12-02-11:58:28-root-INFO: grad norm: 27.042 26.775 3.793
2024-12-02-11:58:28-root-INFO: grad norm: 17.377 17.176 2.632
2024-12-02-11:58:29-root-INFO: grad norm: 13.137 12.958 2.161
2024-12-02-11:58:29-root-INFO: grad norm: 10.633 10.462 1.897
2024-12-02-11:58:30-root-INFO: Loss Change: 290.050 -> 267.104
2024-12-02-11:58:30-root-INFO: Regularization Change: 0.000 -> 1.870
2024-12-02-11:58:30-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-11:58:30-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-11:58:30-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-11:58:30-root-INFO: grad norm: 7.398 7.161 1.859
2024-12-02-11:58:30-root-INFO: grad norm: 7.212 7.041 1.562
2024-12-02-11:58:31-root-INFO: grad norm: 8.856 8.695 1.678
2024-12-02-11:58:31-root-INFO: grad norm: 13.969 13.824 2.009
2024-12-02-11:58:31-root-INFO: Loss too large (262.345->264.480)! Learning rate decreased to 0.04126.
2024-12-02-11:58:32-root-INFO: grad norm: 17.242 17.068 2.441
2024-12-02-11:58:32-root-INFO: Loss too large (262.248->262.381)! Learning rate decreased to 0.03301.
2024-12-02-11:58:33-root-INFO: Loss Change: 266.659 -> 260.246
2024-12-02-11:58:33-root-INFO: Regularization Change: 0.000 -> 1.864
2024-12-02-11:58:33-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-11:58:33-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-11:58:33-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-11:58:33-root-INFO: grad norm: 20.681 20.368 3.587
2024-12-02-11:58:33-root-INFO: Loss too large (261.693->268.807)! Learning rate decreased to 0.04253.
2024-12-02-11:58:33-root-INFO: Loss too large (261.693->263.129)! Learning rate decreased to 0.03403.
2024-12-02-11:58:34-root-INFO: grad norm: 17.707 17.539 2.437
2024-12-02-11:58:34-root-INFO: grad norm: 15.118 14.939 2.316
2024-12-02-11:58:35-root-INFO: grad norm: 13.640 13.488 2.033
2024-12-02-11:58:35-root-INFO: grad norm: 12.392 12.238 1.947
2024-12-02-11:58:35-root-INFO: Loss Change: 261.693 -> 254.226
2024-12-02-11:58:35-root-INFO: Regularization Change: 0.000 -> 1.130
2024-12-02-11:58:35-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-11:58:35-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-11:58:36-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-11:58:36-root-INFO: grad norm: 8.329 8.116 1.871
2024-12-02-11:58:36-root-INFO: grad norm: 11.186 11.070 1.609
2024-12-02-11:58:36-root-INFO: Loss too large (251.957->252.897)! Learning rate decreased to 0.04384.
2024-12-02-11:58:37-root-INFO: grad norm: 13.738 13.608 1.880
2024-12-02-11:58:37-root-INFO: grad norm: 18.577 18.424 2.382
2024-12-02-11:58:37-root-INFO: Loss too large (251.521->253.435)! Learning rate decreased to 0.03507.
2024-12-02-11:58:38-root-INFO: grad norm: 16.816 16.662 2.269
2024-12-02-11:58:38-root-INFO: Loss Change: 253.187 -> 248.840
2024-12-02-11:58:38-root-INFO: Regularization Change: 0.000 -> 1.315
2024-12-02-11:58:38-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-11:58:38-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-11:58:38-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-11:58:38-root-INFO: grad norm: 19.620 19.384 3.035
2024-12-02-11:58:39-root-INFO: Loss too large (250.057->258.225)! Learning rate decreased to 0.04517.
2024-12-02-11:58:39-root-INFO: Loss too large (250.057->252.235)! Learning rate decreased to 0.03614.
2024-12-02-11:58:39-root-INFO: grad norm: 17.939 17.790 2.314
2024-12-02-11:58:40-root-INFO: grad norm: 16.404 16.249 2.248
2024-12-02-11:58:40-root-INFO: grad norm: 15.424 15.283 2.079
2024-12-02-11:58:40-root-INFO: grad norm: 14.558 14.419 2.007
2024-12-02-11:58:41-root-INFO: Loss Change: 250.057 -> 244.203
2024-12-02-11:58:41-root-INFO: Regularization Change: 0.000 -> 1.116
2024-12-02-11:58:41-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-11:58:41-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-11:58:41-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-11:58:41-root-INFO: grad norm: 10.401 10.281 1.577
2024-12-02-11:58:41-root-INFO: Loss too large (243.302->244.236)! Learning rate decreased to 0.04654.
2024-12-02-11:58:42-root-INFO: grad norm: 13.926 13.790 1.945
2024-12-02-11:58:42-root-INFO: Loss too large (242.855->243.818)! Learning rate decreased to 0.03724.
2024-12-02-11:58:42-root-INFO: grad norm: 13.629 13.505 1.835
2024-12-02-11:58:43-root-INFO: grad norm: 13.414 13.280 1.893
2024-12-02-11:58:43-root-INFO: grad norm: 13.335 13.209 1.831
2024-12-02-11:58:44-root-INFO: Loss Change: 243.302 -> 239.567
2024-12-02-11:58:44-root-INFO: Regularization Change: 0.000 -> 0.995
2024-12-02-11:58:44-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-11:58:44-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-11:58:44-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-11:58:44-root-INFO: grad norm: 19.672 19.418 3.150
2024-12-02-11:58:44-root-INFO: Loss too large (241.577->251.269)! Learning rate decreased to 0.04795.
2024-12-02-11:58:44-root-INFO: Loss too large (241.577->244.635)! Learning rate decreased to 0.03836.
2024-12-02-11:58:45-root-INFO: grad norm: 18.897 18.749 2.359
2024-12-02-11:58:45-root-INFO: grad norm: 18.202 18.043 2.402
2024-12-02-11:58:46-root-INFO: grad norm: 17.749 17.607 2.238
2024-12-02-11:58:46-root-INFO: grad norm: 17.270 17.127 2.221
2024-12-02-11:58:46-root-INFO: Loss Change: 241.577 -> 237.037
2024-12-02-11:58:46-root-INFO: Regularization Change: 0.000 -> 1.213
2024-12-02-11:58:46-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-11:58:46-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-11:58:47-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-11:58:47-root-INFO: grad norm: 13.578 13.475 1.671
2024-12-02-11:58:47-root-INFO: Loss too large (235.646->239.627)! Learning rate decreased to 0.04939.
2024-12-02-11:58:47-root-INFO: Loss too large (235.646->236.282)! Learning rate decreased to 0.03951.
2024-12-02-11:58:47-root-INFO: grad norm: 13.110 12.973 1.890
2024-12-02-11:58:48-root-INFO: grad norm: 13.030 12.916 1.723
2024-12-02-11:58:48-root-INFO: grad norm: 13.045 12.918 1.816
2024-12-02-11:58:49-root-INFO: grad norm: 13.120 13.003 1.745
2024-12-02-11:58:49-root-INFO: Loss Change: 235.646 -> 231.664
2024-12-02-11:58:49-root-INFO: Regularization Change: 0.000 -> 0.987
2024-12-02-11:58:49-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-11:58:49-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-11:58:49-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-11:58:50-root-INFO: grad norm: 21.120 20.862 3.287
2024-12-02-11:58:50-root-INFO: Loss too large (234.289->246.739)! Learning rate decreased to 0.05086.
2024-12-02-11:58:50-root-INFO: Loss too large (234.289->238.542)! Learning rate decreased to 0.04069.
2024-12-02-11:58:50-root-INFO: grad norm: 20.505 20.360 2.432
2024-12-02-11:58:51-root-INFO: grad norm: 19.932 19.774 2.508
2024-12-02-11:58:51-root-INFO: grad norm: 19.570 19.431 2.327
2024-12-02-11:58:52-root-INFO: grad norm: 19.128 18.985 2.334
2024-12-02-11:58:52-root-INFO: Loss Change: 234.289 -> 230.156
2024-12-02-11:58:52-root-INFO: Regularization Change: 0.000 -> 1.371
2024-12-02-11:58:52-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-11:58:52-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-11:58:52-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-11:58:52-root-INFO: grad norm: 14.865 14.768 1.695
2024-12-02-11:58:52-root-INFO: Loss too large (228.372->234.184)! Learning rate decreased to 0.05237.
2024-12-02-11:58:53-root-INFO: Loss too large (228.372->229.692)! Learning rate decreased to 0.04190.
2024-12-02-11:58:53-root-INFO: grad norm: 14.779 14.642 2.008
2024-12-02-11:58:53-root-INFO: grad norm: 15.172 15.063 1.817
2024-12-02-11:58:54-root-INFO: grad norm: 15.717 15.587 2.015
2024-12-02-11:58:54-root-INFO: grad norm: 16.239 16.121 1.948
2024-12-02-11:58:55-root-INFO: Loss Change: 228.372 -> 225.186
2024-12-02-11:58:55-root-INFO: Regularization Change: 0.000 -> 1.142
2024-12-02-11:58:55-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-11:58:55-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-11:58:55-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-11:58:55-root-INFO: grad norm: 22.140 21.918 3.130
2024-12-02-11:58:55-root-INFO: Loss too large (227.552->243.162)! Learning rate decreased to 0.05391.
2024-12-02-11:58:55-root-INFO: Loss too large (227.552->233.290)! Learning rate decreased to 0.04313.
2024-12-02-11:58:56-root-INFO: grad norm: 22.086 21.946 2.482
2024-12-02-11:58:56-root-INFO: grad norm: 21.893 21.735 2.622
2024-12-02-11:58:57-root-INFO: grad norm: 21.654 21.515 2.447
2024-12-02-11:58:57-root-INFO: grad norm: 21.281 21.135 2.481
2024-12-02-11:58:58-root-INFO: Loss Change: 227.552 -> 224.250
2024-12-02-11:58:58-root-INFO: Regularization Change: 0.000 -> 1.570
2024-12-02-11:58:58-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-11:58:58-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-11:58:58-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-11:58:58-root-INFO: grad norm: 16.867 16.780 1.710
2024-12-02-11:58:58-root-INFO: Loss too large (222.339->230.837)! Learning rate decreased to 0.05550.
2024-12-02-11:58:58-root-INFO: Loss too large (222.339->224.569)! Learning rate decreased to 0.04440.
2024-12-02-11:58:59-root-INFO: grad norm: 16.474 16.348 2.026
2024-12-02-11:58:59-root-INFO: grad norm: 16.494 16.390 1.848
2024-12-02-11:59:00-root-INFO: grad norm: 16.614 16.493 2.002
2024-12-02-11:59:00-root-INFO: grad norm: 16.740 16.630 1.915
2024-12-02-11:59:00-root-INFO: Loss Change: 222.339 -> 218.699
2024-12-02-11:59:00-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-02-11:59:00-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-11:59:00-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-11:59:01-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-11:59:01-root-INFO: grad norm: 21.558 21.375 2.806
2024-12-02-11:59:01-root-INFO: Loss too large (220.654->235.944)! Learning rate decreased to 0.05711.
2024-12-02-11:59:01-root-INFO: Loss too large (220.654->226.102)! Learning rate decreased to 0.04569.
2024-12-02-11:59:02-root-INFO: grad norm: 21.013 20.879 2.367
2024-12-02-11:59:02-root-INFO: grad norm: 20.340 20.202 2.366
2024-12-02-11:59:02-root-INFO: grad norm: 19.771 19.644 2.241
2024-12-02-11:59:03-root-INFO: grad norm: 19.153 19.025 2.204
2024-12-02-11:59:03-root-INFO: Loss Change: 220.654 -> 216.593
2024-12-02-11:59:03-root-INFO: Regularization Change: 0.000 -> 1.560
2024-12-02-11:59:03-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-11:59:03-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-11:59:03-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-11:59:04-root-INFO: grad norm: 15.927 15.848 1.583
2024-12-02-11:59:04-root-INFO: Loss too large (215.230->223.441)! Learning rate decreased to 0.05877.
2024-12-02-11:59:04-root-INFO: Loss too large (215.230->217.460)! Learning rate decreased to 0.04702.
2024-12-02-11:59:04-root-INFO: grad norm: 15.778 15.657 1.950
2024-12-02-11:59:05-root-INFO: grad norm: 15.891 15.795 1.743
2024-12-02-11:59:05-root-INFO: grad norm: 16.078 15.963 1.916
2024-12-02-11:59:06-root-INFO: grad norm: 16.267 16.165 1.817
2024-12-02-11:59:06-root-INFO: Loss Change: 215.230 -> 211.952
2024-12-02-11:59:06-root-INFO: Regularization Change: 0.000 -> 1.303
2024-12-02-11:59:06-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-11:59:06-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-11:59:06-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-11:59:06-root-INFO: grad norm: 21.171 20.989 2.770
2024-12-02-11:59:06-root-INFO: Loss too large (214.088->229.597)! Learning rate decreased to 0.06047.
2024-12-02-11:59:07-root-INFO: Loss too large (214.088->219.636)! Learning rate decreased to 0.04837.
2024-12-02-11:59:07-root-INFO: grad norm: 20.606 20.484 2.236
2024-12-02-11:59:07-root-INFO: grad norm: 19.889 19.758 2.282
2024-12-02-11:59:08-root-INFO: grad norm: 19.311 19.195 2.110
2024-12-02-11:59:08-root-INFO: grad norm: 18.673 18.553 2.110
2024-12-02-11:59:09-root-INFO: Loss Change: 214.088 -> 209.993
2024-12-02-11:59:09-root-INFO: Regularization Change: 0.000 -> 1.643
2024-12-02-11:59:09-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-11:59:09-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-11:59:09-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-11:59:09-root-INFO: grad norm: 14.826 14.753 1.472
2024-12-02-11:59:09-root-INFO: Loss too large (208.551->215.925)! Learning rate decreased to 0.06220.
2024-12-02-11:59:09-root-INFO: Loss too large (208.551->210.518)! Learning rate decreased to 0.04976.
2024-12-02-11:59:10-root-INFO: grad norm: 14.480 14.376 1.733
2024-12-02-11:59:10-root-INFO: grad norm: 14.366 14.278 1.579
2024-12-02-11:59:11-root-INFO: grad norm: 14.314 14.214 1.693
2024-12-02-11:59:11-root-INFO: grad norm: 14.313 14.223 1.601
2024-12-02-11:59:12-root-INFO: Loss Change: 208.551 -> 205.101
2024-12-02-11:59:12-root-INFO: Regularization Change: 0.000 -> 1.262
2024-12-02-11:59:12-root-INFO: Undo step: 100
2024-12-02-11:59:12-root-INFO: Undo step: 101
2024-12-02-11:59:12-root-INFO: Undo step: 102
2024-12-02-11:59:12-root-INFO: Undo step: 103
2024-12-02-11:59:12-root-INFO: Undo step: 104
2024-12-02-11:59:12-root-INFO: Undo step: 105
2024-12-02-11:59:12-root-INFO: Undo step: 106
2024-12-02-11:59:12-root-INFO: Undo step: 107
2024-12-02-11:59:12-root-INFO: Undo step: 108
2024-12-02-11:59:12-root-INFO: Undo step: 109
2024-12-02-11:59:12-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-11:59:12-root-INFO: grad norm: 119.130 117.634 18.824
2024-12-02-11:59:12-root-INFO: grad norm: 99.107 98.113 14.003
2024-12-02-11:59:13-root-INFO: grad norm: 72.007 70.822 13.007
2024-12-02-11:59:13-root-INFO: grad norm: 55.407 54.893 7.530
2024-12-02-11:59:14-root-INFO: grad norm: 52.676 51.958 8.663
2024-12-02-11:59:14-root-INFO: Loss Change: 693.174 -> 324.819
2024-12-02-11:59:14-root-INFO: Regularization Change: 0.000 -> 83.620
2024-12-02-11:59:14-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-11:59:14-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-11:59:14-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-11:59:14-root-INFO: grad norm: 61.434 60.821 8.653
2024-12-02-11:59:15-root-INFO: Loss too large (331.878->340.558)! Learning rate decreased to 0.04795.
2024-12-02-11:59:15-root-INFO: grad norm: 43.562 42.910 7.509
2024-12-02-11:59:16-root-INFO: grad norm: 30.358 30.052 4.297
2024-12-02-11:59:16-root-INFO: grad norm: 27.466 27.069 4.654
2024-12-02-11:59:17-root-INFO: grad norm: 26.191 25.920 3.759
2024-12-02-11:59:17-root-INFO: Loss Change: 331.878 -> 263.202
2024-12-02-11:59:17-root-INFO: Regularization Change: 0.000 -> 11.310
2024-12-02-11:59:17-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-11:59:17-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-11:59:17-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-11:59:17-root-INFO: grad norm: 23.980 23.716 3.549
2024-12-02-11:59:17-root-INFO: Loss too large (261.089->263.614)! Learning rate decreased to 0.04939.
2024-12-02-11:59:18-root-INFO: grad norm: 24.442 24.195 3.463
2024-12-02-11:59:18-root-INFO: grad norm: 25.122 24.838 3.764
2024-12-02-11:59:19-root-INFO: grad norm: 26.277 26.028 3.609
2024-12-02-11:59:19-root-INFO: Loss too large (251.498->251.573)! Learning rate decreased to 0.03951.
2024-12-02-11:59:19-root-INFO: grad norm: 19.122 18.880 3.033
2024-12-02-11:59:20-root-INFO: Loss Change: 261.089 -> 242.087
2024-12-02-11:59:20-root-INFO: Regularization Change: 0.000 -> 3.809
2024-12-02-11:59:20-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-11:59:20-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-11:59:20-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-11:59:20-root-INFO: grad norm: 20.160 19.865 3.432
2024-12-02-11:59:20-root-INFO: Loss too large (244.178->249.205)! Learning rate decreased to 0.05086.
2024-12-02-11:59:21-root-INFO: grad norm: 22.333 22.073 3.395
2024-12-02-11:59:21-root-INFO: grad norm: 25.933 25.678 3.632
2024-12-02-11:59:21-root-INFO: Loss too large (242.557->244.523)! Learning rate decreased to 0.04069.
2024-12-02-11:59:22-root-INFO: grad norm: 20.204 19.968 3.078
2024-12-02-11:59:22-root-INFO: grad norm: 15.513 15.338 2.324
2024-12-02-11:59:22-root-INFO: Loss Change: 244.178 -> 232.882
2024-12-02-11:59:22-root-INFO: Regularization Change: 0.000 -> 2.485
2024-12-02-11:59:22-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-11:59:22-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-11:59:23-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-11:59:23-root-INFO: grad norm: 10.493 10.349 1.732
2024-12-02-11:59:23-root-INFO: grad norm: 16.885 16.732 2.270
2024-12-02-11:59:23-root-INFO: Loss too large (230.964->235.824)! Learning rate decreased to 0.05237.
2024-12-02-11:59:24-root-INFO: Loss too large (230.964->231.596)! Learning rate decreased to 0.04190.
2024-12-02-11:59:24-root-INFO: grad norm: 14.791 14.644 2.076
2024-12-02-11:59:25-root-INFO: grad norm: 13.196 13.049 1.968
2024-12-02-11:59:25-root-INFO: grad norm: 12.343 12.203 1.852
2024-12-02-11:59:25-root-INFO: Loss Change: 231.421 -> 224.446
2024-12-02-11:59:25-root-INFO: Regularization Change: 0.000 -> 1.848
2024-12-02-11:59:25-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-11:59:25-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-11:59:26-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-11:59:26-root-INFO: grad norm: 16.042 15.825 2.627
2024-12-02-11:59:26-root-INFO: Loss too large (225.729->231.310)! Learning rate decreased to 0.05391.
2024-12-02-11:59:26-root-INFO: Loss too large (225.729->226.918)! Learning rate decreased to 0.04313.
2024-12-02-11:59:27-root-INFO: grad norm: 15.092 14.946 2.090
2024-12-02-11:59:27-root-INFO: grad norm: 14.409 14.260 2.070
2024-12-02-11:59:28-root-INFO: grad norm: 14.000 13.865 1.939
2024-12-02-11:59:28-root-INFO: grad norm: 13.677 13.542 1.917
2024-12-02-11:59:28-root-INFO: Loss Change: 225.729 -> 219.725
2024-12-02-11:59:28-root-INFO: Regularization Change: 0.000 -> 1.521
2024-12-02-11:59:28-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-11:59:28-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-11:59:29-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-11:59:29-root-INFO: grad norm: 10.503 10.386 1.565
2024-12-02-11:59:29-root-INFO: Loss too large (218.583->219.760)! Learning rate decreased to 0.05550.
2024-12-02-11:59:29-root-INFO: grad norm: 13.929 13.818 1.756
2024-12-02-11:59:30-root-INFO: Loss too large (218.124->219.117)! Learning rate decreased to 0.04440.
2024-12-02-11:59:30-root-INFO: grad norm: 13.470 13.367 1.656
2024-12-02-11:59:30-root-INFO: grad norm: 13.247 13.132 1.742
2024-12-02-11:59:31-root-INFO: grad norm: 13.186 13.079 1.676
2024-12-02-11:59:31-root-INFO: Loss Change: 218.583 -> 214.247
2024-12-02-11:59:31-root-INFO: Regularization Change: 0.000 -> 1.383
2024-12-02-11:59:31-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-11:59:31-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-11:59:31-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-11:59:32-root-INFO: grad norm: 17.277 17.098 2.483
2024-12-02-11:59:32-root-INFO: Loss too large (215.504->223.897)! Learning rate decreased to 0.05711.
2024-12-02-11:59:32-root-INFO: Loss too large (215.504->218.159)! Learning rate decreased to 0.04569.
2024-12-02-11:59:32-root-INFO: grad norm: 16.788 16.655 2.106
2024-12-02-11:59:33-root-INFO: grad norm: 16.332 16.196 2.106
2024-12-02-11:59:33-root-INFO: grad norm: 16.021 15.893 2.024
2024-12-02-11:59:34-root-INFO: grad norm: 15.724 15.597 1.999
2024-12-02-11:59:34-root-INFO: Loss Change: 215.504 -> 210.917
2024-12-02-11:59:34-root-INFO: Regularization Change: 0.000 -> 1.489
2024-12-02-11:59:34-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-11:59:34-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-11:59:34-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-11:59:35-root-INFO: grad norm: 13.058 12.965 1.554
2024-12-02-11:59:35-root-INFO: Loss too large (209.715->214.270)! Learning rate decreased to 0.05877.
2024-12-02-11:59:35-root-INFO: Loss too large (209.715->210.705)! Learning rate decreased to 0.04702.
2024-12-02-11:59:35-root-INFO: grad norm: 13.164 13.048 1.746
2024-12-02-11:59:36-root-INFO: grad norm: 13.463 13.364 1.629
2024-12-02-11:59:36-root-INFO: grad norm: 13.834 13.722 1.761
2024-12-02-11:59:37-root-INFO: grad norm: 14.159 14.054 1.717
2024-12-02-11:59:37-root-INFO: Loss Change: 209.715 -> 206.117
2024-12-02-11:59:37-root-INFO: Regularization Change: 0.000 -> 1.299
2024-12-02-11:59:37-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-11:59:37-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-11:59:37-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-11:59:37-root-INFO: grad norm: 18.735 18.554 2.600
2024-12-02-11:59:37-root-INFO: Loss too large (207.723->219.005)! Learning rate decreased to 0.06047.
2024-12-02-11:59:38-root-INFO: Loss too large (207.723->211.665)! Learning rate decreased to 0.04837.
2024-12-02-11:59:38-root-INFO: grad norm: 18.360 18.229 2.188
2024-12-02-11:59:38-root-INFO: grad norm: 17.936 17.798 2.219
2024-12-02-11:59:39-root-INFO: grad norm: 17.605 17.477 2.122
2024-12-02-11:59:39-root-INFO: grad norm: 17.248 17.119 2.108
2024-12-02-11:59:40-root-INFO: Loss Change: 207.723 -> 203.713
2024-12-02-11:59:40-root-INFO: Regularization Change: 0.000 -> 1.594
2024-12-02-11:59:40-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-11:59:40-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-11:59:40-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-11:59:40-root-INFO: grad norm: 14.046 13.964 1.524
2024-12-02-11:59:40-root-INFO: Loss too large (202.399->208.659)! Learning rate decreased to 0.06220.
2024-12-02-11:59:40-root-INFO: Loss too large (202.399->204.086)! Learning rate decreased to 0.04976.
2024-12-02-11:59:41-root-INFO: grad norm: 13.960 13.851 1.738
2024-12-02-11:59:41-root-INFO: grad norm: 14.037 13.941 1.636
2024-12-02-11:59:42-root-INFO: grad norm: 14.181 14.074 1.738
2024-12-02-11:59:42-root-INFO: grad norm: 14.327 14.227 1.692
2024-12-02-11:59:43-root-INFO: Loss Change: 202.399 -> 199.035
2024-12-02-11:59:43-root-INFO: Regularization Change: 0.000 -> 1.313
2024-12-02-11:59:43-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-11:59:43-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-11:59:43-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-11:59:43-root-INFO: grad norm: 17.998 17.835 2.415
2024-12-02-11:59:43-root-INFO: Loss too large (200.310->211.427)! Learning rate decreased to 0.06397.
2024-12-02-11:59:43-root-INFO: Loss too large (200.310->204.097)! Learning rate decreased to 0.05118.
2024-12-02-11:59:44-root-INFO: grad norm: 17.556 17.435 2.058
2024-12-02-11:59:44-root-INFO: grad norm: 17.105 16.980 2.065
2024-12-02-11:59:45-root-INFO: grad norm: 16.745 16.628 1.977
2024-12-02-11:59:45-root-INFO: grad norm: 16.389 16.272 1.959
2024-12-02-11:59:45-root-INFO: Loss Change: 200.310 -> 196.462
2024-12-02-11:59:45-root-INFO: Regularization Change: 0.000 -> 1.567
2024-12-02-11:59:45-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-11:59:45-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-11:59:46-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-11:59:46-root-INFO: grad norm: 13.161 13.083 1.437
2024-12-02-11:59:46-root-INFO: Loss too large (195.162->201.015)! Learning rate decreased to 0.06579.
2024-12-02-11:59:46-root-INFO: Loss too large (195.162->196.776)! Learning rate decreased to 0.05263.
2024-12-02-11:59:47-root-INFO: grad norm: 13.124 13.025 1.614
2024-12-02-11:59:47-root-INFO: grad norm: 13.277 13.189 1.531
2024-12-02-11:59:48-root-INFO: grad norm: 13.504 13.406 1.625
2024-12-02-11:59:48-root-INFO: grad norm: 13.738 13.644 1.600
2024-12-02-11:59:48-root-INFO: Loss Change: 195.162 -> 192.251
2024-12-02-11:59:48-root-INFO: Regularization Change: 0.000 -> 1.294
2024-12-02-11:59:48-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-11:59:48-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-11:59:48-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-11:59:49-root-INFO: grad norm: 18.305 18.133 2.499
2024-12-02-11:59:49-root-INFO: Loss too large (193.968->206.575)! Learning rate decreased to 0.06764.
2024-12-02-11:59:49-root-INFO: Loss too large (193.968->198.378)! Learning rate decreased to 0.05411.
2024-12-02-11:59:49-root-INFO: grad norm: 18.023 17.901 2.092
2024-12-02-11:59:50-root-INFO: grad norm: 17.655 17.530 2.102
2024-12-02-11:59:50-root-INFO: grad norm: 17.323 17.205 2.013
2024-12-02-11:59:51-root-INFO: grad norm: 16.957 16.841 1.985
2024-12-02-11:59:51-root-INFO: Loss Change: 193.968 -> 190.402
2024-12-02-11:59:51-root-INFO: Regularization Change: 0.000 -> 1.682
2024-12-02-11:59:51-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-11:59:51-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-11:59:51-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-11:59:51-root-INFO: grad norm: 14.101 14.022 1.490
2024-12-02-11:59:52-root-INFO: Loss too large (189.171->196.701)! Learning rate decreased to 0.06953.
2024-12-02-11:59:52-root-INFO: Loss too large (189.171->191.447)! Learning rate decreased to 0.05563.
2024-12-02-11:59:52-root-INFO: grad norm: 13.920 13.822 1.649
2024-12-02-11:59:53-root-INFO: grad norm: 13.883 13.795 1.563
2024-12-02-11:59:53-root-INFO: grad norm: 13.915 13.819 1.632
2024-12-02-11:59:54-root-INFO: grad norm: 13.972 13.881 1.593
2024-12-02-11:59:54-root-INFO: Loss Change: 189.171 -> 186.233
2024-12-02-11:59:54-root-INFO: Regularization Change: 0.000 -> 1.383
2024-12-02-11:59:54-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-11:59:54-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-11:59:54-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-11:59:54-root-INFO: grad norm: 17.982 17.825 2.371
2024-12-02-11:59:54-root-INFO: Loss too large (188.234->201.088)! Learning rate decreased to 0.07147.
2024-12-02-11:59:54-root-INFO: Loss too large (188.234->192.630)! Learning rate decreased to 0.05718.
2024-12-02-11:59:55-root-INFO: grad norm: 17.561 17.448 1.989
2024-12-02-11:59:55-root-INFO: grad norm: 17.051 16.934 2.001
2024-12-02-11:59:56-root-INFO: grad norm: 16.601 16.492 1.891
2024-12-02-11:59:56-root-INFO: grad norm: 16.155 16.047 1.869
2024-12-02-11:59:57-root-INFO: Loss Change: 188.234 -> 184.451
2024-12-02-11:59:57-root-INFO: Regularization Change: 0.000 -> 1.718
2024-12-02-11:59:57-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-11:59:57-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-11:59:57-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-11:59:57-root-INFO: grad norm: 13.152 13.081 1.366
2024-12-02-11:59:57-root-INFO: Loss too large (183.059->190.041)! Learning rate decreased to 0.07345.
2024-12-02-11:59:57-root-INFO: Loss too large (183.059->185.256)! Learning rate decreased to 0.05876.
2024-12-02-11:59:58-root-INFO: grad norm: 13.090 13.002 1.517
2024-12-02-11:59:58-root-INFO: grad norm: 13.133 13.053 1.439
2024-12-02-11:59:59-root-INFO: grad norm: 13.225 13.139 1.509
2024-12-02-11:59:59-root-INFO: grad norm: 13.321 13.239 1.479
2024-12-02-12:00:00-root-INFO: Loss Change: 183.059 -> 180.493
2024-12-02-12:00:00-root-INFO: Regularization Change: 0.000 -> 1.382
2024-12-02-12:00:00-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-12:00:00-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-12:00:00-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-12:00:00-root-INFO: grad norm: 17.228 17.080 2.248
2024-12-02-12:00:00-root-INFO: Loss too large (182.155->194.552)! Learning rate decreased to 0.07547.
2024-12-02-12:00:00-root-INFO: Loss too large (182.155->186.245)! Learning rate decreased to 0.06038.
2024-12-02-12:00:01-root-INFO: grad norm: 16.779 16.675 1.865
2024-12-02-12:00:01-root-INFO: grad norm: 16.251 16.142 1.877
2024-12-02-12:00:02-root-INFO: grad norm: 15.775 15.676 1.764
2024-12-02-12:00:02-root-INFO: grad norm: 15.331 15.232 1.747
2024-12-02-12:00:02-root-INFO: Loss Change: 182.155 -> 178.263
2024-12-02-12:00:02-root-INFO: Regularization Change: 0.000 -> 1.750
2024-12-02-12:00:02-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-12:00:02-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-12:00:03-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-12:00:03-root-INFO: grad norm: 12.698 12.628 1.327
2024-12-02-12:00:03-root-INFO: Loss too large (177.289->184.039)! Learning rate decreased to 0.07753.
2024-12-02-12:00:03-root-INFO: Loss too large (177.289->179.380)! Learning rate decreased to 0.06203.
2024-12-02-12:00:03-root-INFO: grad norm: 12.451 12.365 1.460
2024-12-02-12:00:04-root-INFO: grad norm: 12.348 12.270 1.381
2024-12-02-12:00:04-root-INFO: grad norm: 12.347 12.264 1.431
2024-12-02-12:00:05-root-INFO: grad norm: 12.369 12.290 1.399
2024-12-02-12:00:05-root-INFO: Loss Change: 177.289 -> 174.601
2024-12-02-12:00:05-root-INFO: Regularization Change: 0.000 -> 1.402
2024-12-02-12:00:05-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-12:00:05-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-12:00:05-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-12:00:06-root-INFO: grad norm: 15.474 15.348 1.970
2024-12-02-12:00:06-root-INFO: Loss too large (175.932->186.840)! Learning rate decreased to 0.07964.
2024-12-02-12:00:06-root-INFO: Loss too large (175.932->179.479)! Learning rate decreased to 0.06371.
2024-12-02-12:00:06-root-INFO: grad norm: 15.221 15.124 1.716
2024-12-02-12:00:07-root-INFO: grad norm: 14.869 14.768 1.727
2024-12-02-12:00:07-root-INFO: grad norm: 14.537 14.444 1.643
2024-12-02-12:00:08-root-INFO: grad norm: 14.219 14.125 1.636
2024-12-02-12:00:08-root-INFO: Loss Change: 175.932 -> 172.568
2024-12-02-12:00:08-root-INFO: Regularization Change: 0.000 -> 1.671
2024-12-02-12:00:08-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-12:00:08-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-12:00:08-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-12:00:08-root-INFO: grad norm: 11.562 11.503 1.164
2024-12-02-12:00:08-root-INFO: Loss too large (171.612->177.510)! Learning rate decreased to 0.08180.
2024-12-02-12:00:09-root-INFO: Loss too large (171.612->173.442)! Learning rate decreased to 0.06544.
2024-12-02-12:00:09-root-INFO: grad norm: 11.358 11.278 1.351
2024-12-02-12:00:09-root-INFO: grad norm: 11.253 11.186 1.233
2024-12-02-12:00:10-root-INFO: grad norm: 11.234 11.156 1.322
2024-12-02-12:00:10-root-INFO: grad norm: 11.217 11.147 1.249
2024-12-02-12:00:11-root-INFO: Loss Change: 171.612 -> 169.072
2024-12-02-12:00:11-root-INFO: Regularization Change: 0.000 -> 1.352
2024-12-02-12:00:11-root-INFO: Undo step: 90
2024-12-02-12:00:11-root-INFO: Undo step: 91
2024-12-02-12:00:11-root-INFO: Undo step: 92
2024-12-02-12:00:11-root-INFO: Undo step: 93
2024-12-02-12:00:11-root-INFO: Undo step: 94
2024-12-02-12:00:11-root-INFO: Undo step: 95
2024-12-02-12:00:11-root-INFO: Undo step: 96
2024-12-02-12:00:11-root-INFO: Undo step: 97
2024-12-02-12:00:11-root-INFO: Undo step: 98
2024-12-02-12:00:11-root-INFO: Undo step: 99
2024-12-02-12:00:11-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-12:00:11-root-INFO: grad norm: 83.367 82.304 13.273
2024-12-02-12:00:11-root-INFO: grad norm: 50.085 49.529 7.446
2024-12-02-12:00:12-root-INFO: grad norm: 34.059 33.636 5.353
2024-12-02-12:00:12-root-INFO: grad norm: 32.723 32.458 4.156
2024-12-02-12:00:13-root-INFO: grad norm: 33.783 33.452 4.720
2024-12-02-12:00:13-root-INFO: Loss Change: 595.902 -> 258.993
2024-12-02-12:00:13-root-INFO: Regularization Change: 0.000 -> 97.868
2024-12-02-12:00:13-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-12:00:13-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-12:00:13-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-12:00:14-root-INFO: grad norm: 41.687 41.394 4.935
2024-12-02-12:00:14-root-INFO: Loss too large (261.995->280.942)! Learning rate decreased to 0.06397.
2024-12-02-12:00:14-root-INFO: grad norm: 37.021 36.619 5.442
2024-12-02-12:00:15-root-INFO: grad norm: 32.724 32.487 3.932
2024-12-02-12:00:15-root-INFO: grad norm: 33.150 32.816 4.696
2024-12-02-12:00:16-root-INFO: grad norm: 33.460 33.231 3.905
2024-12-02-12:00:16-root-INFO: Loss Change: 261.995 -> 232.237
2024-12-02-12:00:16-root-INFO: Regularization Change: 0.000 -> 12.645
2024-12-02-12:00:16-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-12:00:16-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-12:00:16-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-12:00:16-root-INFO: grad norm: 30.764 30.527 3.807
2024-12-02-12:00:16-root-INFO: Loss too large (229.033->242.601)! Learning rate decreased to 0.06579.
2024-12-02-12:00:17-root-INFO: grad norm: 30.792 30.584 3.576
2024-12-02-12:00:17-root-INFO: grad norm: 30.439 30.191 3.881
2024-12-02-12:00:18-root-INFO: grad norm: 29.936 29.731 3.498
2024-12-02-12:00:18-root-INFO: grad norm: 29.350 29.104 3.792
2024-12-02-12:00:19-root-INFO: Loss Change: 229.033 -> 214.112
2024-12-02-12:00:19-root-INFO: Regularization Change: 0.000 -> 7.923
2024-12-02-12:00:19-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-12:00:19-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-12:00:19-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-12:00:19-root-INFO: grad norm: 32.157 31.890 4.132
2024-12-02-12:00:19-root-INFO: Loss too large (217.567->233.815)! Learning rate decreased to 0.06764.
2024-12-02-12:00:20-root-INFO: grad norm: 30.343 30.075 4.030
2024-12-02-12:00:20-root-INFO: grad norm: 28.612 28.391 3.553
2024-12-02-12:00:21-root-INFO: grad norm: 27.798 27.549 3.716
2024-12-02-12:00:21-root-INFO: grad norm: 27.315 27.099 3.428
2024-12-02-12:00:21-root-INFO: Loss Change: 217.567 -> 204.943
2024-12-02-12:00:21-root-INFO: Regularization Change: 0.000 -> 6.815
2024-12-02-12:00:21-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-12:00:21-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-12:00:22-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-12:00:22-root-INFO: grad norm: 24.923 24.729 3.108
2024-12-02-12:00:22-root-INFO: Loss too large (202.686->213.281)! Learning rate decreased to 0.06953.
2024-12-02-12:00:22-root-INFO: grad norm: 25.093 24.895 3.146
2024-12-02-12:00:23-root-INFO: grad norm: 25.420 25.203 3.318
2024-12-02-12:00:23-root-INFO: grad norm: 25.775 25.575 3.204
2024-12-02-12:00:24-root-INFO: grad norm: 26.049 25.826 3.405
2024-12-02-12:00:24-root-INFO: Loss Change: 202.686 -> 196.834
2024-12-02-12:00:24-root-INFO: Regularization Change: 0.000 -> 5.324
2024-12-02-12:00:24-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-12:00:24-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-12:00:24-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-12:00:24-root-INFO: grad norm: 29.408 29.153 3.867
2024-12-02-12:00:24-root-INFO: Loss too large (200.390->216.608)! Learning rate decreased to 0.07147.
2024-12-02-12:00:25-root-INFO: grad norm: 28.456 28.213 3.715
2024-12-02-12:00:25-root-INFO: grad norm: 27.076 26.870 3.334
2024-12-02-12:00:26-root-INFO: grad norm: 26.100 25.883 3.364
2024-12-02-12:00:26-root-INFO: grad norm: 25.219 25.028 3.097
2024-12-02-12:00:27-root-INFO: Loss Change: 200.390 -> 191.147
2024-12-02-12:00:27-root-INFO: Regularization Change: 0.000 -> 5.612
2024-12-02-12:00:27-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-12:00:27-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-12:00:27-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-12:00:27-root-INFO: grad norm: 22.481 22.328 2.617
2024-12-02-12:00:27-root-INFO: Loss too large (188.716->198.589)! Learning rate decreased to 0.07345.
2024-12-02-12:00:28-root-INFO: grad norm: 22.579 22.410 2.756
2024-12-02-12:00:28-root-INFO: grad norm: 22.755 22.578 2.829
2024-12-02-12:00:28-root-INFO: grad norm: 23.040 22.867 2.820
2024-12-02-12:00:29-root-INFO: grad norm: 23.243 23.056 2.937
2024-12-02-12:00:29-root-INFO: Loss Change: 188.716 -> 184.853
2024-12-02-12:00:29-root-INFO: Regularization Change: 0.000 -> 4.422
2024-12-02-12:00:29-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-12:00:29-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-12:00:29-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-12:00:30-root-INFO: grad norm: 26.720 26.482 3.557
2024-12-02-12:00:30-root-INFO: Loss too large (187.891->202.089)! Learning rate decreased to 0.07547.
2024-12-02-12:00:30-root-INFO: grad norm: 25.832 25.622 3.285
2024-12-02-12:00:31-root-INFO: grad norm: 24.586 24.403 2.991
2024-12-02-12:00:31-root-INFO: grad norm: 23.686 23.500 2.960
2024-12-02-12:00:32-root-INFO: grad norm: 22.993 22.820 2.818
2024-12-02-12:00:32-root-INFO: Loss Change: 187.891 -> 180.343
2024-12-02-12:00:32-root-INFO: Regularization Change: 0.000 -> 4.958
2024-12-02-12:00:32-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-12:00:32-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-12:00:32-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-12:00:33-root-INFO: grad norm: 20.583 20.446 2.369
2024-12-02-12:00:33-root-INFO: Loss too large (178.756->187.331)! Learning rate decreased to 0.07753.
2024-12-02-12:00:33-root-INFO: grad norm: 20.458 20.306 2.490
2024-12-02-12:00:34-root-INFO: grad norm: 20.494 20.338 2.526
2024-12-02-12:00:34-root-INFO: grad norm: 20.713 20.556 2.544
2024-12-02-12:00:35-root-INFO: grad norm: 20.872 20.707 2.623
2024-12-02-12:00:35-root-INFO: Loss Change: 178.756 -> 175.195
2024-12-02-12:00:35-root-INFO: Regularization Change: 0.000 -> 3.971
2024-12-02-12:00:35-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-12:00:35-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-12:00:35-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-12:00:35-root-INFO: grad norm: 23.651 23.448 3.090
2024-12-02-12:00:35-root-INFO: Loss too large (177.562->189.717)! Learning rate decreased to 0.07964.
2024-12-02-12:00:36-root-INFO: grad norm: 23.137 22.950 2.940
2024-12-02-12:00:36-root-INFO: grad norm: 22.291 22.123 2.734
2024-12-02-12:00:37-root-INFO: grad norm: 21.655 21.483 2.724
2024-12-02-12:00:37-root-INFO: grad norm: 21.175 21.012 2.622
2024-12-02-12:00:37-root-INFO: Loss Change: 177.562 -> 172.049
2024-12-02-12:00:38-root-INFO: Regularization Change: 0.000 -> 4.455
2024-12-02-12:00:38-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-12:00:38-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-12:00:38-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-12:00:38-root-INFO: grad norm: 18.538 18.419 2.095
2024-12-02-12:00:38-root-INFO: Loss too large (170.269->177.380)! Learning rate decreased to 0.08180.
2024-12-02-12:00:38-root-INFO: grad norm: 18.211 18.073 2.237
2024-12-02-12:00:39-root-INFO: grad norm: 18.118 17.984 2.196
2024-12-02-12:00:39-root-INFO: grad norm: 18.246 18.105 2.264
2024-12-02-12:00:40-root-INFO: grad norm: 18.304 18.163 2.268
2024-12-02-12:00:40-root-INFO: Loss Change: 170.269 -> 166.686
2024-12-02-12:00:40-root-INFO: Regularization Change: 0.000 -> 3.551
2024-12-02-12:00:40-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-12:00:40-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-12:00:40-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-12:00:40-root-INFO: grad norm: 21.920 21.712 3.011
2024-12-02-12:00:41-root-INFO: Loss too large (169.376->180.484)! Learning rate decreased to 0.08399.
2024-12-02-12:00:41-root-INFO: grad norm: 21.378 21.203 2.730
2024-12-02-12:00:42-root-INFO: grad norm: 20.425 20.273 2.483
2024-12-02-12:00:42-root-INFO: grad norm: 19.808 19.655 2.458
2024-12-02-12:00:43-root-INFO: grad norm: 19.540 19.388 2.433
2024-12-02-12:00:43-root-INFO: Loss Change: 169.376 -> 164.424
2024-12-02-12:00:43-root-INFO: Regularization Change: 0.000 -> 4.237
2024-12-02-12:00:43-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-12:00:43-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-12:00:43-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-12:00:43-root-INFO: grad norm: 17.078 16.965 1.963
2024-12-02-12:00:43-root-INFO: Loss too large (163.141->169.138)! Learning rate decreased to 0.08623.
2024-12-02-12:00:44-root-INFO: grad norm: 16.512 16.389 2.008
2024-12-02-12:00:44-root-INFO: grad norm: 16.302 16.184 1.957
2024-12-02-12:00:45-root-INFO: grad norm: 16.432 16.305 2.039
2024-12-02-12:00:45-root-INFO: grad norm: 16.349 16.224 2.020
2024-12-02-12:00:45-root-INFO: Loss Change: 163.141 -> 159.296
2024-12-02-12:00:45-root-INFO: Regularization Change: 0.000 -> 3.321
2024-12-02-12:00:45-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-12:00:45-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-12:00:46-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-12:00:46-root-INFO: grad norm: 19.396 19.209 2.686
2024-12-02-12:00:46-root-INFO: Loss too large (161.562->170.206)! Learning rate decreased to 0.08852.
2024-12-02-12:00:46-root-INFO: grad norm: 18.704 18.557 2.343
2024-12-02-12:00:47-root-INFO: grad norm: 17.795 17.661 2.179
2024-12-02-12:00:47-root-INFO: grad norm: 17.224 17.094 2.112
2024-12-02-12:00:48-root-INFO: grad norm: 16.960 16.829 2.107
2024-12-02-12:00:48-root-INFO: Loss Change: 161.562 -> 156.693
2024-12-02-12:00:48-root-INFO: Regularization Change: 0.000 -> 3.789
2024-12-02-12:00:48-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-12:00:48-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-12:00:48-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-12:00:49-root-INFO: grad norm: 14.372 14.284 1.589
2024-12-02-12:00:49-root-INFO: Loss too large (155.131->159.543)! Learning rate decreased to 0.09086.
2024-12-02-12:00:49-root-INFO: grad norm: 13.980 13.876 1.701
2024-12-02-12:00:50-root-INFO: grad norm: 13.913 13.813 1.664
2024-12-02-12:00:50-root-INFO: grad norm: 14.273 14.160 1.792
2024-12-02-12:00:51-root-INFO: grad norm: 14.228 14.118 1.772
2024-12-02-12:00:51-root-INFO: Loss Change: 155.131 -> 152.010
2024-12-02-12:00:51-root-INFO: Regularization Change: 0.000 -> 2.888
2024-12-02-12:00:51-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-12:00:51-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-12:00:51-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-12:00:51-root-INFO: grad norm: 16.495 16.340 2.253
2024-12-02-12:00:51-root-INFO: Loss too large (153.724->160.340)! Learning rate decreased to 0.09324.
2024-12-02-12:00:52-root-INFO: grad norm: 16.172 16.050 1.982
2024-12-02-12:00:52-root-INFO: grad norm: 15.852 15.728 1.974
2024-12-02-12:00:53-root-INFO: grad norm: 15.506 15.388 1.906
2024-12-02-12:00:53-root-INFO: grad norm: 15.141 15.027 1.858
2024-12-02-12:00:54-root-INFO: Loss Change: 153.724 -> 149.961
2024-12-02-12:00:54-root-INFO: Regularization Change: 0.000 -> 3.367
2024-12-02-12:00:54-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-12:00:54-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-12:00:54-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-12:00:54-root-INFO: grad norm: 13.489 13.403 1.522
2024-12-02-12:00:54-root-INFO: Loss too large (148.860->153.308)! Learning rate decreased to 0.09566.
2024-12-02-12:00:55-root-INFO: grad norm: 13.469 13.366 1.664
2024-12-02-12:00:55-root-INFO: grad norm: 13.383 13.282 1.638
2024-12-02-12:00:55-root-INFO: grad norm: 13.313 13.212 1.639
2024-12-02-12:00:56-root-INFO: grad norm: 13.375 13.273 1.645
2024-12-02-12:00:56-root-INFO: Loss Change: 148.860 -> 146.154
2024-12-02-12:00:56-root-INFO: Regularization Change: 0.000 -> 2.848
2024-12-02-12:00:56-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-12:00:56-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-12:00:56-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-12:00:56-root-INFO: grad norm: 16.007 15.856 2.195
2024-12-02-12:00:57-root-INFO: Loss too large (147.838->154.506)! Learning rate decreased to 0.09814.
2024-12-02-12:00:57-root-INFO: grad norm: 15.394 15.275 1.916
2024-12-02-12:00:58-root-INFO: grad norm: 14.511 14.411 1.708
2024-12-02-12:00:58-root-INFO: grad norm: 14.033 13.935 1.650
2024-12-02-12:00:59-root-INFO: grad norm: 14.183 14.076 1.736
2024-12-02-12:00:59-root-INFO: Loss Change: 147.838 -> 144.131
2024-12-02-12:00:59-root-INFO: Regularization Change: 0.000 -> 3.380
2024-12-02-12:00:59-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-12:00:59-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-12:00:59-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-12:00:59-root-INFO: grad norm: 12.238 12.160 1.379
2024-12-02-12:00:59-root-INFO: Loss too large (143.262->146.439)! Learning rate decreased to 0.10066.
2024-12-02-12:01:00-root-INFO: grad norm: 11.750 11.666 1.401
2024-12-02-12:01:00-root-INFO: grad norm: 11.692 11.609 1.387
2024-12-02-12:01:01-root-INFO: grad norm: 12.309 12.213 1.538
2024-12-02-12:01:01-root-INFO: grad norm: 11.979 11.887 1.485
2024-12-02-12:01:02-root-INFO: Loss Change: 143.262 -> 140.108
2024-12-02-12:01:02-root-INFO: Regularization Change: 0.000 -> 2.668
2024-12-02-12:01:02-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-12:01:02-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-12:01:02-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-12:01:02-root-INFO: grad norm: 13.661 13.539 1.817
2024-12-02-12:01:02-root-INFO: Loss too large (141.416->145.901)! Learning rate decreased to 0.10323.
2024-12-02-12:01:03-root-INFO: grad norm: 13.259 13.168 1.554
2024-12-02-12:01:03-root-INFO: grad norm: 13.665 13.558 1.706
2024-12-02-12:01:03-root-INFO: grad norm: 12.898 12.801 1.575
2024-12-02-12:01:04-root-INFO: grad norm: 12.082 12.005 1.356
2024-12-02-12:01:04-root-INFO: Loss Change: 141.416 -> 137.396
2024-12-02-12:01:04-root-INFO: Regularization Change: 0.000 -> 2.968
2024-12-02-12:01:04-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-12:01:04-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-12:01:04-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-12:01:05-root-INFO: grad norm: 9.690 9.647 0.916
2024-12-02-12:01:05-root-INFO: Loss too large (136.428->138.467)! Learning rate decreased to 0.10585.
2024-12-02-12:01:05-root-INFO: grad norm: 9.363 9.307 1.022
2024-12-02-12:01:06-root-INFO: grad norm: 9.235 9.182 0.983
2024-12-02-12:01:06-root-INFO: grad norm: 9.488 9.423 1.105
2024-12-02-12:01:07-root-INFO: grad norm: 9.756 9.687 1.159
2024-12-02-12:01:07-root-INFO: Loss Change: 136.428 -> 134.168
2024-12-02-12:01:07-root-INFO: Regularization Change: 0.000 -> 2.291
2024-12-02-12:01:07-root-INFO: Undo step: 80
2024-12-02-12:01:07-root-INFO: Undo step: 81
2024-12-02-12:01:07-root-INFO: Undo step: 82
2024-12-02-12:01:07-root-INFO: Undo step: 83
2024-12-02-12:01:07-root-INFO: Undo step: 84
2024-12-02-12:01:07-root-INFO: Undo step: 85
2024-12-02-12:01:07-root-INFO: Undo step: 86
2024-12-02-12:01:07-root-INFO: Undo step: 87
2024-12-02-12:01:07-root-INFO: Undo step: 88
2024-12-02-12:01:07-root-INFO: Undo step: 89
2024-12-02-12:01:07-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-12:01:07-root-INFO: grad norm: 61.638 60.647 11.010
2024-12-02-12:01:08-root-INFO: grad norm: 40.959 40.640 5.104
2024-12-02-12:01:08-root-INFO: grad norm: 34.397 34.166 3.972
2024-12-02-12:01:09-root-INFO: grad norm: 27.997 27.818 3.164
2024-12-02-12:01:09-root-INFO: grad norm: 24.119 23.972 2.657
2024-12-02-12:01:09-root-INFO: Loss Change: 471.931 -> 200.518
2024-12-02-12:01:09-root-INFO: Regularization Change: 0.000 -> 99.358
2024-12-02-12:01:09-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-12:01:09-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-12:01:10-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-12:01:10-root-INFO: grad norm: 21.943 21.817 2.353
2024-12-02-12:01:10-root-INFO: grad norm: 20.888 20.756 2.349
2024-12-02-12:01:11-root-INFO: grad norm: 20.889 20.776 2.175
2024-12-02-12:01:11-root-INFO: grad norm: 22.006 21.877 2.380
2024-12-02-12:01:12-root-INFO: grad norm: 24.367 24.237 2.517
2024-12-02-12:01:12-root-INFO: Loss too large (183.026->184.152)! Learning rate decreased to 0.08399.
2024-12-02-12:01:12-root-INFO: Loss Change: 199.983 -> 173.817
2024-12-02-12:01:12-root-INFO: Regularization Change: 0.000 -> 14.466
2024-12-02-12:01:12-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-12:01:12-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-12:01:12-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-12:01:12-root-INFO: grad norm: 17.210 17.091 2.026
2024-12-02-12:01:13-root-INFO: Loss too large (172.857->173.582)! Learning rate decreased to 0.08623.
2024-12-02-12:01:13-root-INFO: grad norm: 14.612 14.490 1.885
2024-12-02-12:01:13-root-INFO: grad norm: 13.810 13.668 1.973
2024-12-02-12:01:14-root-INFO: grad norm: 13.806 13.672 1.916
2024-12-02-12:01:14-root-INFO: grad norm: 14.187 14.033 2.084
2024-12-02-12:01:15-root-INFO: Loss Change: 172.857 -> 160.943
2024-12-02-12:01:15-root-INFO: Regularization Change: 0.000 -> 5.810
2024-12-02-12:01:15-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-12:01:15-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-12:01:15-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-12:01:15-root-INFO: grad norm: 18.427 18.211 2.812
2024-12-02-12:01:15-root-INFO: Loss too large (162.916->170.702)! Learning rate decreased to 0.08852.
2024-12-02-12:01:16-root-INFO: grad norm: 18.673 18.464 2.787
2024-12-02-12:01:16-root-INFO: grad norm: 18.639 18.462 2.565
2024-12-02-12:01:17-root-INFO: grad norm: 18.526 18.326 2.714
2024-12-02-12:01:17-root-INFO: grad norm: 18.336 18.162 2.521
2024-12-02-12:01:17-root-INFO: Loss Change: 162.916 -> 157.254
2024-12-02-12:01:17-root-INFO: Regularization Change: 0.000 -> 5.308
2024-12-02-12:01:17-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-12:01:17-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-12:01:18-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-12:01:18-root-INFO: grad norm: 15.748 15.602 2.141
2024-12-02-12:01:18-root-INFO: Loss too large (155.293->160.105)! Learning rate decreased to 0.09086.
2024-12-02-12:01:18-root-INFO: grad norm: 15.528 15.376 2.167
2024-12-02-12:01:19-root-INFO: grad norm: 15.389 15.223 2.251
2024-12-02-12:01:19-root-INFO: grad norm: 15.396 15.243 2.165
2024-12-02-12:01:20-root-INFO: grad norm: 15.260 15.091 2.269
2024-12-02-12:01:20-root-INFO: Loss Change: 155.293 -> 149.742
2024-12-02-12:01:20-root-INFO: Regularization Change: 0.000 -> 4.084
2024-12-02-12:01:20-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-12:01:20-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-12:01:20-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-12:01:20-root-INFO: grad norm: 17.617 17.410 2.692
2024-12-02-12:01:20-root-INFO: Loss too large (151.473->158.666)! Learning rate decreased to 0.09324.
2024-12-02-12:01:21-root-INFO: grad norm: 16.870 16.682 2.512
2024-12-02-12:01:21-root-INFO: grad norm: 15.882 15.730 2.193
2024-12-02-12:01:22-root-INFO: grad norm: 15.296 15.135 2.211
2024-12-02-12:01:22-root-INFO: grad norm: 15.118 14.968 2.128
2024-12-02-12:01:23-root-INFO: Loss Change: 151.473 -> 145.874
2024-12-02-12:01:23-root-INFO: Regularization Change: 0.000 -> 4.113
2024-12-02-12:01:23-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-12:01:23-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-12:01:23-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-12:01:23-root-INFO: grad norm: 13.046 12.919 1.815
2024-12-02-12:01:23-root-INFO: Loss too large (144.651->147.625)! Learning rate decreased to 0.09566.
2024-12-02-12:01:24-root-INFO: grad norm: 12.440 12.319 1.729
2024-12-02-12:01:24-root-INFO: grad norm: 12.197 12.073 1.739
2024-12-02-12:01:24-root-INFO: grad norm: 12.492 12.366 1.772
2024-12-02-12:01:25-root-INFO: grad norm: 12.301 12.162 1.841
2024-12-02-12:01:25-root-INFO: Loss Change: 144.651 -> 140.163
2024-12-02-12:01:25-root-INFO: Regularization Change: 0.000 -> 3.066
2024-12-02-12:01:25-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-12:01:25-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-12:01:25-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-12:01:25-root-INFO: grad norm: 14.463 14.288 2.241
2024-12-02-12:01:26-root-INFO: Loss too large (141.642->146.625)! Learning rate decreased to 0.09814.
2024-12-02-12:01:26-root-INFO: grad norm: 14.014 13.862 2.052
2024-12-02-12:01:26-root-INFO: grad norm: 13.862 13.722 1.967
2024-12-02-12:01:27-root-INFO: grad norm: 13.339 13.192 1.975
2024-12-02-12:01:27-root-INFO: grad norm: 12.637 12.518 1.728
2024-12-02-12:01:28-root-INFO: Loss Change: 141.642 -> 137.285
2024-12-02-12:01:28-root-INFO: Regularization Change: 0.000 -> 3.204
2024-12-02-12:01:28-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-12:01:28-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-12:01:28-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-12:01:28-root-INFO: grad norm: 10.837 10.751 1.369
2024-12-02-12:01:28-root-INFO: Loss too large (136.512->139.069)! Learning rate decreased to 0.10066.
2024-12-02-12:01:29-root-INFO: grad norm: 10.879 10.777 1.493
2024-12-02-12:01:29-root-INFO: grad norm: 10.767 10.656 1.542
2024-12-02-12:01:30-root-INFO: grad norm: 10.674 10.568 1.496
2024-12-02-12:01:30-root-INFO: grad norm: 10.677 10.567 1.530
2024-12-02-12:01:30-root-INFO: Loss Change: 136.512 -> 133.451
2024-12-02-12:01:30-root-INFO: Regularization Change: 0.000 -> 2.520
2024-12-02-12:01:30-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-12:01:30-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-12:01:30-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-12:01:31-root-INFO: grad norm: 13.444 13.273 2.141
2024-12-02-12:01:31-root-INFO: Loss too large (134.720->139.567)! Learning rate decreased to 0.10323.
2024-12-02-12:01:31-root-INFO: grad norm: 12.664 12.529 1.842
2024-12-02-12:01:32-root-INFO: grad norm: 11.760 11.657 1.555
2024-12-02-12:01:32-root-INFO: grad norm: 11.238 11.143 1.459
2024-12-02-12:01:33-root-INFO: grad norm: 11.054 10.956 1.463
2024-12-02-12:01:33-root-INFO: Loss Change: 134.720 -> 130.949
2024-12-02-12:01:33-root-INFO: Regularization Change: 0.000 -> 2.841
2024-12-02-12:01:33-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-12:01:33-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-12:01:33-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-12:01:33-root-INFO: grad norm: 8.864 8.805 1.023
2024-12-02-12:01:33-root-INFO: Loss too large (129.885->131.525)! Learning rate decreased to 0.10585.
2024-12-02-12:01:34-root-INFO: grad norm: 9.113 9.027 1.246
2024-12-02-12:01:34-root-INFO: grad norm: 8.979 8.889 1.268
2024-12-02-12:01:35-root-INFO: grad norm: 8.659 8.577 1.189
2024-12-02-12:01:35-root-INFO: grad norm: 8.756 8.672 1.214
2024-12-02-12:01:36-root-INFO: Loss Change: 129.885 -> 127.292
2024-12-02-12:01:36-root-INFO: Regularization Change: 0.000 -> 2.156
2024-12-02-12:01:36-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-12:01:36-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-12:01:36-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-12:01:36-root-INFO: grad norm: 11.727 11.574 1.887
2024-12-02-12:01:36-root-INFO: Loss too large (128.185->132.068)! Learning rate decreased to 0.10852.
2024-12-02-12:01:36-root-INFO: Loss too large (128.185->128.399)! Learning rate decreased to 0.08682.
2024-12-02-12:01:37-root-INFO: grad norm: 7.236 7.151 1.107
2024-12-02-12:01:37-root-INFO: grad norm: 4.475 4.427 0.658
2024-12-02-12:01:38-root-INFO: grad norm: 3.092 3.053 0.495
2024-12-02-12:01:38-root-INFO: grad norm: 2.514 2.470 0.471
2024-12-02-12:01:38-root-INFO: Loss Change: 128.185 -> 123.082
2024-12-02-12:01:38-root-INFO: Regularization Change: 0.000 -> 1.226
2024-12-02-12:01:38-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-12:01:38-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-12:01:39-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-12:01:39-root-INFO: grad norm: 2.847 2.777 0.629
2024-12-02-12:01:39-root-INFO: grad norm: 3.451 3.394 0.624
2024-12-02-12:01:40-root-INFO: grad norm: 7.334 7.256 1.066
2024-12-02-12:01:40-root-INFO: Loss too large (122.695->123.844)! Learning rate decreased to 0.11124.
2024-12-02-12:01:40-root-INFO: Loss too large (122.695->123.108)! Learning rate decreased to 0.08899.
2024-12-02-12:01:40-root-INFO: grad norm: 4.407 4.332 0.808
2024-12-02-12:01:41-root-INFO: grad norm: 3.065 3.025 0.493
2024-12-02-12:01:41-root-INFO: Loss Change: 123.262 -> 120.895
2024-12-02-12:01:41-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-02-12:01:41-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-12:01:41-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-12:01:41-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-12:01:41-root-INFO: grad norm: 3.227 3.149 0.708
2024-12-02-12:01:42-root-INFO: grad norm: 4.010 3.942 0.738
2024-12-02-12:01:42-root-INFO: Loss too large (120.332->120.372)! Learning rate decreased to 0.11401.
2024-12-02-12:01:43-root-INFO: grad norm: 5.263 5.201 0.807
2024-12-02-12:01:43-root-INFO: Loss too large (119.866->120.005)! Learning rate decreased to 0.09121.
2024-12-02-12:01:43-root-INFO: grad norm: 3.958 3.894 0.711
2024-12-02-12:01:44-root-INFO: grad norm: 2.391 2.348 0.452
2024-12-02-12:01:44-root-INFO: Loss Change: 120.702 -> 118.423
2024-12-02-12:01:44-root-INFO: Regularization Change: 0.000 -> 1.160
2024-12-02-12:01:44-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-12:01:44-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-12:01:44-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-12:01:44-root-INFO: grad norm: 2.981 2.911 0.644
2024-12-02-12:01:45-root-INFO: grad norm: 3.660 3.602 0.648
2024-12-02-12:01:45-root-INFO: Loss too large (117.964->118.066)! Learning rate decreased to 0.11682.
2024-12-02-12:01:45-root-INFO: grad norm: 5.227 5.170 0.775
2024-12-02-12:01:45-root-INFO: Loss too large (117.657->117.822)! Learning rate decreased to 0.09346.
2024-12-02-12:01:46-root-INFO: grad norm: 3.972 3.911 0.695
2024-12-02-12:01:46-root-INFO: grad norm: 2.393 2.351 0.446
2024-12-02-12:01:47-root-INFO: Loss Change: 118.440 -> 116.234
2024-12-02-12:01:47-root-INFO: Regularization Change: 0.000 -> 1.161
2024-12-02-12:01:47-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-12:01:47-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-12:01:47-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-12:01:47-root-INFO: grad norm: 2.600 2.542 0.548
2024-12-02-12:01:47-root-INFO: grad norm: 2.658 2.620 0.446
2024-12-02-12:01:48-root-INFO: grad norm: 3.209 3.170 0.502
2024-12-02-12:01:48-root-INFO: grad norm: 4.446 4.412 0.552
2024-12-02-12:01:49-root-INFO: Loss too large (114.905->115.139)! Learning rate decreased to 0.11969.
2024-12-02-12:01:49-root-INFO: grad norm: 4.792 4.755 0.592
2024-12-02-12:01:49-root-INFO: Loss Change: 116.217 -> 114.371
2024-12-02-12:01:49-root-INFO: Regularization Change: 0.000 -> 1.837
2024-12-02-12:01:49-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-12:01:49-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-12:01:49-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-12:01:50-root-INFO: grad norm: 6.515 6.465 0.800
2024-12-02-12:01:50-root-INFO: Loss too large (114.366->115.499)! Learning rate decreased to 0.12261.
2024-12-02-12:01:50-root-INFO: Loss too large (114.366->114.697)! Learning rate decreased to 0.09809.
2024-12-02-12:01:50-root-INFO: grad norm: 4.324 4.278 0.624
2024-12-02-12:01:51-root-INFO: grad norm: 2.810 2.781 0.398
2024-12-02-12:01:51-root-INFO: grad norm: 2.510 2.480 0.386
2024-12-02-12:01:52-root-INFO: grad norm: 2.514 2.479 0.417
2024-12-02-12:01:52-root-INFO: Loss Change: 114.366 -> 111.982
2024-12-02-12:01:52-root-INFO: Regularization Change: 0.000 -> 0.907
2024-12-02-12:01:52-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-12:01:52-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-12:01:52-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-12:01:53-root-INFO: grad norm: 4.461 4.385 0.818
2024-12-02-12:01:53-root-INFO: Loss too large (112.168->112.595)! Learning rate decreased to 0.12558.
2024-12-02-12:01:53-root-INFO: grad norm: 4.542 4.484 0.726
2024-12-02-12:01:54-root-INFO: grad norm: 4.891 4.835 0.740
2024-12-02-12:01:54-root-INFO: Loss too large (111.374->111.534)! Learning rate decreased to 0.10047.
2024-12-02-12:01:54-root-INFO: grad norm: 4.176 4.124 0.659
2024-12-02-12:01:55-root-INFO: grad norm: 3.384 3.345 0.509
2024-12-02-12:01:55-root-INFO: Loss Change: 112.168 -> 110.174
2024-12-02-12:01:55-root-INFO: Regularization Change: 0.000 -> 1.155
2024-12-02-12:01:55-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-12:01:55-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-12:01:55-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-12:01:55-root-INFO: grad norm: 3.287 3.249 0.503
2024-12-02-12:01:56-root-INFO: grad norm: 6.942 6.901 0.745
2024-12-02-12:01:56-root-INFO: Loss too large (109.997->111.100)! Learning rate decreased to 0.12860.
2024-12-02-12:01:56-root-INFO: Loss too large (109.997->110.457)! Learning rate decreased to 0.10288.
2024-12-02-12:01:57-root-INFO: grad norm: 4.027 3.976 0.638
2024-12-02-12:01:57-root-INFO: grad norm: 3.054 3.027 0.405
2024-12-02-12:01:58-root-INFO: grad norm: 2.479 2.451 0.370
2024-12-02-12:01:58-root-INFO: Loss Change: 110.019 -> 107.962
2024-12-02-12:01:58-root-INFO: Regularization Change: 0.000 -> 1.112
2024-12-02-12:01:58-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-12:01:58-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-12:01:58-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-12:01:58-root-INFO: grad norm: 2.703 2.668 0.430
2024-12-02-12:01:59-root-INFO: grad norm: 4.470 4.444 0.482
2024-12-02-12:01:59-root-INFO: Loss too large (107.725->108.232)! Learning rate decreased to 0.13168.
2024-12-02-12:01:59-root-INFO: Loss too large (107.725->107.806)! Learning rate decreased to 0.10534.
2024-12-02-12:01:59-root-INFO: grad norm: 3.702 3.664 0.532
2024-12-02-12:02:00-root-INFO: grad norm: 2.768 2.739 0.398
2024-12-02-12:02:00-root-INFO: grad norm: 2.869 2.833 0.455
2024-12-02-12:02:01-root-INFO: Loss Change: 108.040 -> 106.195
2024-12-02-12:02:01-root-INFO: Regularization Change: 0.000 -> 1.137
2024-12-02-12:02:01-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-12:02:01-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-12:02:01-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-12:02:01-root-INFO: grad norm: 5.401 5.315 0.963
2024-12-02-12:02:01-root-INFO: Loss too large (106.376->107.165)! Learning rate decreased to 0.13480.
2024-12-02-12:02:01-root-INFO: Loss too large (106.376->106.548)! Learning rate decreased to 0.10784.
2024-12-02-12:02:02-root-INFO: grad norm: 4.028 3.979 0.628
2024-12-02-12:02:02-root-INFO: grad norm: 2.397 2.360 0.416
2024-12-02-12:02:03-root-INFO: grad norm: 2.292 2.258 0.394
2024-12-02-12:02:03-root-INFO: grad norm: 2.595 2.565 0.393
2024-12-02-12:02:04-root-INFO: Loss Change: 106.376 -> 104.204
2024-12-02-12:02:04-root-INFO: Regularization Change: 0.000 -> 1.064
2024-12-02-12:02:04-root-INFO: Undo step: 70
2024-12-02-12:02:04-root-INFO: Undo step: 71
2024-12-02-12:02:04-root-INFO: Undo step: 72
2024-12-02-12:02:04-root-INFO: Undo step: 73
2024-12-02-12:02:04-root-INFO: Undo step: 74
2024-12-02-12:02:04-root-INFO: Undo step: 75
2024-12-02-12:02:04-root-INFO: Undo step: 76
2024-12-02-12:02:04-root-INFO: Undo step: 77
2024-12-02-12:02:04-root-INFO: Undo step: 78
2024-12-02-12:02:04-root-INFO: Undo step: 79
2024-12-02-12:02:04-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-12:02:04-root-INFO: grad norm: 47.878 47.281 7.542
2024-12-02-12:02:05-root-INFO: grad norm: 28.611 28.271 4.399
2024-12-02-12:02:05-root-INFO: grad norm: 24.566 24.341 3.322
2024-12-02-12:02:06-root-INFO: grad norm: 23.080 22.891 2.949
2024-12-02-12:02:06-root-INFO: grad norm: 23.088 22.890 3.020
2024-12-02-12:02:07-root-INFO: Loss Change: 403.150 -> 171.235
2024-12-02-12:02:07-root-INFO: Regularization Change: 0.000 -> 117.492
2024-12-02-12:02:07-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-12:02:07-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-12:02:07-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-12:02:07-root-INFO: grad norm: 25.341 25.129 3.271
2024-12-02-12:02:07-root-INFO: grad norm: 25.574 25.348 3.395
2024-12-02-12:02:08-root-INFO: grad norm: 24.582 24.393 3.041
2024-12-02-12:02:08-root-INFO: grad norm: 23.928 23.696 3.321
2024-12-02-12:02:09-root-INFO: grad norm: 23.282 23.075 3.099
2024-12-02-12:02:09-root-INFO: Loss Change: 172.934 -> 154.484
2024-12-02-12:02:09-root-INFO: Regularization Change: 0.000 -> 23.173
2024-12-02-12:02:09-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-12:02:09-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-12:02:10-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-12:02:10-root-INFO: grad norm: 22.217 22.005 3.067
2024-12-02-12:02:10-root-INFO: grad norm: 24.275 24.046 3.331
2024-12-02-12:02:10-root-INFO: Loss too large (149.714->151.338)! Learning rate decreased to 0.11124.
2024-12-02-12:02:11-root-INFO: grad norm: 15.230 15.043 2.378
2024-12-02-12:02:11-root-INFO: grad norm: 10.405 10.306 1.434
2024-12-02-12:02:12-root-INFO: grad norm: 8.264 8.169 1.243
2024-12-02-12:02:12-root-INFO: Loss Change: 152.814 -> 128.041
2024-12-02-12:02:12-root-INFO: Regularization Change: 0.000 -> 8.518
2024-12-02-12:02:12-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-12:02:12-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-12:02:12-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-12:02:12-root-INFO: grad norm: 8.344 8.229 1.384
2024-12-02-12:02:13-root-INFO: grad norm: 10.016 9.908 1.462
2024-12-02-12:02:13-root-INFO: Loss too large (127.364->127.677)! Learning rate decreased to 0.11401.
2024-12-02-12:02:14-root-INFO: grad norm: 8.539 8.442 1.282
2024-12-02-12:02:14-root-INFO: grad norm: 7.661 7.570 1.177
2024-12-02-12:02:14-root-INFO: grad norm: 7.786 7.699 1.156
2024-12-02-12:02:15-root-INFO: Loss Change: 128.300 -> 121.943
2024-12-02-12:02:15-root-INFO: Regularization Change: 0.000 -> 4.101
2024-12-02-12:02:15-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-12:02:15-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-12:02:15-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-12:02:15-root-INFO: grad norm: 5.847 5.790 0.819
2024-12-02-12:02:16-root-INFO: grad norm: 7.356 7.284 1.029
2024-12-02-12:02:16-root-INFO: Loss too large (120.188->120.870)! Learning rate decreased to 0.11682.
2024-12-02-12:02:16-root-INFO: grad norm: 6.900 6.825 1.014
2024-12-02-12:02:17-root-INFO: grad norm: 6.429 6.361 0.935
2024-12-02-12:02:17-root-INFO: grad norm: 6.417 6.347 0.944
2024-12-02-12:02:18-root-INFO: Loss Change: 121.174 -> 116.833
2024-12-02-12:02:18-root-INFO: Regularization Change: 0.000 -> 3.080
2024-12-02-12:02:18-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-12:02:18-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-12:02:18-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-12:02:18-root-INFO: grad norm: 8.576 8.468 1.359
2024-12-02-12:02:18-root-INFO: Loss too large (117.362->118.966)! Learning rate decreased to 0.11969.
2024-12-02-12:02:19-root-INFO: grad norm: 7.490 7.409 1.099
2024-12-02-12:02:19-root-INFO: grad norm: 6.838 6.775 0.921
2024-12-02-12:02:19-root-INFO: grad norm: 6.493 6.444 0.792
2024-12-02-12:02:20-root-INFO: grad norm: 6.442 6.390 0.817
2024-12-02-12:02:20-root-INFO: Loss Change: 117.362 -> 113.466
2024-12-02-12:02:20-root-INFO: Regularization Change: 0.000 -> 2.543
2024-12-02-12:02:20-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-12:02:20-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-12:02:20-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-12:02:21-root-INFO: grad norm: 7.023 6.979 0.788
2024-12-02-12:02:21-root-INFO: Loss too large (113.326->114.335)! Learning rate decreased to 0.12261.
2024-12-02-12:02:21-root-INFO: grad norm: 6.059 6.011 0.766
2024-12-02-12:02:22-root-INFO: grad norm: 5.494 5.455 0.652
2024-12-02-12:02:22-root-INFO: grad norm: 5.780 5.736 0.719
2024-12-02-12:02:23-root-INFO: grad norm: 6.157 6.107 0.785
2024-12-02-12:02:23-root-INFO: Loss Change: 113.326 -> 110.490
2024-12-02-12:02:23-root-INFO: Regularization Change: 0.000 -> 2.188
2024-12-02-12:02:23-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-12:02:23-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-12:02:23-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-12:02:23-root-INFO: grad norm: 8.706 8.614 1.256
2024-12-02-12:02:24-root-INFO: Loss too large (110.838->112.916)! Learning rate decreased to 0.12558.
2024-12-02-12:02:24-root-INFO: Loss too large (110.838->111.108)! Learning rate decreased to 0.10047.
2024-12-02-12:02:24-root-INFO: grad norm: 5.275 5.222 0.750
2024-12-02-12:02:25-root-INFO: grad norm: 3.763 3.723 0.548
2024-12-02-12:02:25-root-INFO: grad norm: 2.937 2.900 0.465
2024-12-02-12:02:26-root-INFO: grad norm: 2.874 2.830 0.496
2024-12-02-12:02:26-root-INFO: Loss Change: 110.838 -> 107.096
2024-12-02-12:02:26-root-INFO: Regularization Change: 0.000 -> 1.284
2024-12-02-12:02:26-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-12:02:26-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-12:02:26-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-12:02:26-root-INFO: grad norm: 4.578 4.510 0.786
2024-12-02-12:02:26-root-INFO: Loss too large (107.112->107.400)! Learning rate decreased to 0.12860.
2024-12-02-12:02:27-root-INFO: grad norm: 4.038 3.984 0.662
2024-12-02-12:02:27-root-INFO: grad norm: 2.925 2.876 0.535
2024-12-02-12:02:28-root-INFO: grad norm: 3.072 3.025 0.536
2024-12-02-12:02:28-root-INFO: grad norm: 5.220 5.178 0.666
2024-12-02-12:02:28-root-INFO: Loss too large (105.084->105.368)! Learning rate decreased to 0.10288.
2024-12-02-12:02:29-root-INFO: Loss Change: 107.112 -> 105.074
2024-12-02-12:02:29-root-INFO: Regularization Change: 0.000 -> 1.757
2024-12-02-12:02:29-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-12:02:29-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-12:02:29-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-12:02:29-root-INFO: grad norm: 3.626 3.586 0.537
2024-12-02-12:02:29-root-INFO: grad norm: 5.044 5.007 0.614
2024-12-02-12:02:30-root-INFO: Loss too large (104.098->104.625)! Learning rate decreased to 0.13168.
2024-12-02-12:02:30-root-INFO: Loss too large (104.098->104.266)! Learning rate decreased to 0.10534.
2024-12-02-12:02:30-root-INFO: grad norm: 3.507 3.462 0.565
2024-12-02-12:02:31-root-INFO: grad norm: 2.143 2.105 0.402
2024-12-02-12:02:31-root-INFO: grad norm: 2.068 2.031 0.389
2024-12-02-12:02:32-root-INFO: Loss Change: 105.025 -> 102.223
2024-12-02-12:02:32-root-INFO: Regularization Change: 0.000 -> 1.323
2024-12-02-12:02:32-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-12:02:32-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-12:02:32-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-12:02:32-root-INFO: grad norm: 3.177 3.101 0.690
2024-12-02-12:02:32-root-INFO: grad norm: 3.553 3.510 0.551
2024-12-02-12:02:33-root-INFO: grad norm: 4.684 4.641 0.634
2024-12-02-12:02:33-root-INFO: Loss too large (101.260->101.669)! Learning rate decreased to 0.13480.
2024-12-02-12:02:33-root-INFO: grad norm: 5.736 5.708 0.567
2024-12-02-12:02:34-root-INFO: Loss too large (100.902->101.112)! Learning rate decreased to 0.10784.
2024-12-02-12:02:34-root-INFO: grad norm: 4.076 4.042 0.524
2024-12-02-12:02:34-root-INFO: Loss Change: 102.108 -> 99.547
2024-12-02-12:02:34-root-INFO: Regularization Change: 0.000 -> 1.695
2024-12-02-12:02:34-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-12:02:34-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-12:02:35-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-12:02:35-root-INFO: grad norm: 2.587 2.543 0.477
2024-12-02-12:02:35-root-INFO: grad norm: 3.651 3.611 0.542
2024-12-02-12:02:35-root-INFO: Loss too large (99.262->100.176)! Learning rate decreased to 0.13798.
2024-12-02-12:02:36-root-INFO: Loss too large (99.262->99.381)! Learning rate decreased to 0.11038.
2024-12-02-12:02:36-root-INFO: grad norm: 5.151 5.119 0.571
2024-12-02-12:02:36-root-INFO: Loss too large (98.966->99.129)! Learning rate decreased to 0.08831.
2024-12-02-12:02:37-root-INFO: grad norm: 3.551 3.512 0.520
2024-12-02-12:02:37-root-INFO: grad norm: 1.813 1.779 0.350
2024-12-02-12:02:37-root-INFO: Loss Change: 99.632 -> 97.764
2024-12-02-12:02:37-root-INFO: Regularization Change: 0.000 -> 0.980
2024-12-02-12:02:37-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-12:02:37-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-12:02:38-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-12:02:38-root-INFO: grad norm: 3.403 3.325 0.720
2024-12-02-12:02:38-root-INFO: grad norm: 4.652 4.591 0.748
2024-12-02-12:02:38-root-INFO: Loss too large (97.602->98.759)! Learning rate decreased to 0.14121.
2024-12-02-12:02:39-root-INFO: grad norm: 7.861 7.804 0.944
2024-12-02-12:02:39-root-INFO: Loss too large (97.550->98.176)! Learning rate decreased to 0.11297.
2024-12-02-12:02:39-root-INFO: Loss too large (97.550->97.608)! Learning rate decreased to 0.09037.
2024-12-02-12:02:40-root-INFO: grad norm: 3.519 3.476 0.550
2024-12-02-12:02:40-root-INFO: grad norm: 3.034 3.006 0.416
2024-12-02-12:02:40-root-INFO: Loss Change: 97.777 -> 95.691
2024-12-02-12:02:40-root-INFO: Regularization Change: 0.000 -> 1.143
2024-12-02-12:02:40-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-12:02:40-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-12:02:41-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-12:02:41-root-INFO: grad norm: 3.254 3.202 0.583
2024-12-02-12:02:41-root-INFO: Loss too large (95.673->95.795)! Learning rate decreased to 0.14449.
2024-12-02-12:02:41-root-INFO: grad norm: 3.790 3.748 0.563
2024-12-02-12:02:42-root-INFO: grad norm: 6.327 6.284 0.733
2024-12-02-12:02:42-root-INFO: Loss too large (95.262->95.845)! Learning rate decreased to 0.11559.
2024-12-02-12:02:42-root-INFO: Loss too large (95.262->95.444)! Learning rate decreased to 0.09247.
2024-12-02-12:02:43-root-INFO: grad norm: 3.439 3.400 0.514
2024-12-02-12:02:43-root-INFO: grad norm: 2.378 2.350 0.359
2024-12-02-12:02:43-root-INFO: Loss Change: 95.673 -> 93.854
2024-12-02-12:02:43-root-INFO: Regularization Change: 0.000 -> 0.975
2024-12-02-12:02:43-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-12:02:43-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-12:02:43-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-12:02:44-root-INFO: grad norm: 3.537 3.495 0.547
2024-12-02-12:02:44-root-INFO: Loss too large (93.995->94.336)! Learning rate decreased to 0.14783.
2024-12-02-12:02:44-root-INFO: Loss too large (93.995->94.072)! Learning rate decreased to 0.11826.
2024-12-02-12:02:44-root-INFO: grad norm: 3.509 3.474 0.496
2024-12-02-12:02:45-root-INFO: grad norm: 3.895 3.866 0.473
2024-12-02-12:02:45-root-INFO: Loss too large (93.311->93.372)! Learning rate decreased to 0.09461.
2024-12-02-12:02:45-root-INFO: grad norm: 3.362 3.329 0.474
2024-12-02-12:02:46-root-INFO: grad norm: 2.585 2.560 0.362
2024-12-02-12:02:46-root-INFO: Loss Change: 93.995 -> 92.430
2024-12-02-12:02:46-root-INFO: Regularization Change: 0.000 -> 0.831
2024-12-02-12:02:46-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-12:02:46-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-12:02:46-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-12:02:47-root-INFO: grad norm: 3.014 2.970 0.516
2024-12-02-12:02:47-root-INFO: Loss too large (92.404->92.452)! Learning rate decreased to 0.15121.
2024-12-02-12:02:47-root-INFO: grad norm: 4.923 4.900 0.474
2024-12-02-12:02:47-root-INFO: Loss too large (92.141->92.573)! Learning rate decreased to 0.12097.
2024-12-02-12:02:47-root-INFO: Loss too large (92.141->92.277)! Learning rate decreased to 0.09678.
2024-12-02-12:02:48-root-INFO: grad norm: 3.536 3.504 0.475
2024-12-02-12:02:48-root-INFO: grad norm: 1.895 1.868 0.318
2024-12-02-12:02:49-root-INFO: grad norm: 2.001 1.972 0.338
2024-12-02-12:02:49-root-INFO: Loss Change: 92.404 -> 90.702
2024-12-02-12:02:49-root-INFO: Regularization Change: 0.000 -> 0.869
2024-12-02-12:02:49-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-12:02:49-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-12:02:49-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-12:02:49-root-INFO: grad norm: 3.184 3.142 0.514
2024-12-02-12:02:50-root-INFO: Loss too large (90.737->91.019)! Learning rate decreased to 0.15465.
2024-12-02-12:02:50-root-INFO: Loss too large (90.737->90.764)! Learning rate decreased to 0.12372.
2024-12-02-12:02:50-root-INFO: grad norm: 3.569 3.536 0.485
2024-12-02-12:02:51-root-INFO: grad norm: 5.189 5.159 0.559
2024-12-02-12:02:51-root-INFO: Loss too large (90.317->90.592)! Learning rate decreased to 0.09898.
2024-12-02-12:02:51-root-INFO: grad norm: 3.549 3.515 0.490
2024-12-02-12:02:52-root-INFO: grad norm: 1.696 1.669 0.302
2024-12-02-12:02:52-root-INFO: Loss Change: 90.737 -> 89.085
2024-12-02-12:02:52-root-INFO: Regularization Change: 0.000 -> 0.826
2024-12-02-12:02:52-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-12:02:52-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-12:02:52-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-12:02:52-root-INFO: grad norm: 2.329 2.278 0.487
2024-12-02-12:02:53-root-INFO: grad norm: 2.790 2.752 0.456
2024-12-02-12:02:53-root-INFO: Loss too large (88.715->88.949)! Learning rate decreased to 0.15815.
2024-12-02-12:02:53-root-INFO: grad norm: 5.054 5.019 0.590
2024-12-02-12:02:54-root-INFO: Loss too large (88.611->89.230)! Learning rate decreased to 0.12652.
2024-12-02-12:02:54-root-INFO: Loss too large (88.611->88.859)! Learning rate decreased to 0.10121.
2024-12-02-12:02:54-root-INFO: grad norm: 3.652 3.618 0.499
2024-12-02-12:02:55-root-INFO: grad norm: 1.895 1.870 0.309
2024-12-02-12:02:55-root-INFO: Loss Change: 89.223 -> 87.405
2024-12-02-12:02:55-root-INFO: Regularization Change: 0.000 -> 1.174
2024-12-02-12:02:55-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-12:02:55-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-12:02:55-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-12:02:55-root-INFO: grad norm: 2.510 2.459 0.505
2024-12-02-12:02:56-root-INFO: grad norm: 3.167 3.141 0.405
2024-12-02-12:02:56-root-INFO: Loss too large (86.751->86.971)! Learning rate decreased to 0.16169.
2024-12-02-12:02:56-root-INFO: grad norm: 3.909 3.882 0.453
2024-12-02-12:02:57-root-INFO: Loss too large (86.670->86.799)! Learning rate decreased to 0.12935.
2024-12-02-12:02:57-root-INFO: grad norm: 4.956 4.933 0.475
2024-12-02-12:02:57-root-INFO: Loss too large (86.236->86.512)! Learning rate decreased to 0.10348.
2024-12-02-12:02:58-root-INFO: grad norm: 3.592 3.561 0.472
2024-12-02-12:02:58-root-INFO: Loss Change: 87.238 -> 85.303
2024-12-02-12:02:58-root-INFO: Regularization Change: 0.000 -> 1.293
2024-12-02-12:02:58-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-12:02:58-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-12:02:58-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-12:02:58-root-INFO: grad norm: 2.579 2.542 0.430
2024-12-02-12:02:58-root-INFO: Loss too large (85.374->85.396)! Learning rate decreased to 0.16529.
2024-12-02-12:02:59-root-INFO: grad norm: 3.648 3.612 0.513
2024-12-02-12:02:59-root-INFO: Loss too large (85.221->85.670)! Learning rate decreased to 0.13223.
2024-12-02-12:02:59-root-INFO: grad norm: 5.400 5.370 0.572
2024-12-02-12:03:00-root-INFO: Loss too large (85.086->85.365)! Learning rate decreased to 0.10579.
2024-12-02-12:03:00-root-INFO: grad norm: 3.584 3.550 0.489
2024-12-02-12:03:01-root-INFO: grad norm: 1.648 1.624 0.281
2024-12-02-12:03:01-root-INFO: Loss Change: 85.374 -> 83.754
2024-12-02-12:03:01-root-INFO: Regularization Change: 0.000 -> 0.944
2024-12-02-12:03:01-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-12:03:01-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-12:03:01-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-12:03:01-root-INFO: grad norm: 2.207 2.159 0.459
2024-12-02-12:03:02-root-INFO: grad norm: 2.501 2.465 0.421
2024-12-02-12:03:02-root-INFO: Loss too large (83.300->83.315)! Learning rate decreased to 0.16894.
2024-12-02-12:03:02-root-INFO: grad norm: 4.116 4.086 0.497
2024-12-02-12:03:02-root-INFO: Loss too large (83.096->83.639)! Learning rate decreased to 0.13515.
2024-12-02-12:03:03-root-INFO: Loss too large (83.096->83.298)! Learning rate decreased to 0.10812.
2024-12-02-12:03:03-root-INFO: grad norm: 3.668 3.638 0.471
2024-12-02-12:03:04-root-INFO: grad norm: 3.218 3.196 0.372
2024-12-02-12:03:04-root-INFO: Loss Change: 83.813 -> 82.356
2024-12-02-12:03:04-root-INFO: Regularization Change: 0.000 -> 1.249
2024-12-02-12:03:04-root-INFO: Undo step: 60
2024-12-02-12:03:04-root-INFO: Undo step: 61
2024-12-02-12:03:04-root-INFO: Undo step: 62
2024-12-02-12:03:04-root-INFO: Undo step: 63
2024-12-02-12:03:04-root-INFO: Undo step: 64
2024-12-02-12:03:04-root-INFO: Undo step: 65
2024-12-02-12:03:04-root-INFO: Undo step: 66
2024-12-02-12:03:04-root-INFO: Undo step: 67
2024-12-02-12:03:04-root-INFO: Undo step: 68
2024-12-02-12:03:04-root-INFO: Undo step: 69
2024-12-02-12:03:04-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-12:03:04-root-INFO: grad norm: 41.356 40.881 6.251
2024-12-02-12:03:05-root-INFO: grad norm: 19.363 19.120 3.057
2024-12-02-12:03:05-root-INFO: grad norm: 14.600 14.416 2.312
2024-12-02-12:03:06-root-INFO: grad norm: 14.931 14.829 1.748
2024-12-02-12:03:06-root-INFO: grad norm: 15.635 15.509 1.974
2024-12-02-12:03:06-root-INFO: Loss Change: 334.848 -> 133.168
2024-12-02-12:03:06-root-INFO: Regularization Change: 0.000 -> 121.204
2024-12-02-12:03:06-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-12:03:06-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-12:03:07-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-12:03:07-root-INFO: grad norm: 16.747 16.618 2.078
2024-12-02-12:03:07-root-INFO: grad norm: 17.321 17.167 2.304
2024-12-02-12:03:08-root-INFO: grad norm: 17.731 17.575 2.352
2024-12-02-12:03:08-root-INFO: grad norm: 17.554 17.371 2.527
2024-12-02-12:03:09-root-INFO: grad norm: 17.403 17.257 2.248
2024-12-02-12:03:09-root-INFO: Loss Change: 134.292 -> 119.920
2024-12-02-12:03:09-root-INFO: Regularization Change: 0.000 -> 21.554
2024-12-02-12:03:09-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-12:03:09-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-12:03:09-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-12:03:09-root-INFO: grad norm: 16.490 16.378 1.921
2024-12-02-12:03:10-root-INFO: grad norm: 15.734 15.625 1.843
2024-12-02-12:03:10-root-INFO: grad norm: 15.465 15.321 2.102
2024-12-02-12:03:11-root-INFO: grad norm: 16.563 16.410 2.245
2024-12-02-12:03:11-root-INFO: grad norm: 15.711 15.545 2.281
2024-12-02-12:03:11-root-INFO: Loss Change: 117.848 -> 111.087
2024-12-02-12:03:11-root-INFO: Regularization Change: 0.000 -> 15.165
2024-12-02-12:03:11-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-12:03:11-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-12:03:12-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-12:03:12-root-INFO: grad norm: 16.156 15.997 2.257
2024-12-02-12:03:12-root-INFO: grad norm: 15.620 15.455 2.262
2024-12-02-12:03:13-root-INFO: grad norm: 14.989 14.850 2.033
2024-12-02-12:03:13-root-INFO: grad norm: 14.511 14.360 2.085
2024-12-02-12:03:13-root-INFO: Loss too large (104.773->105.228)! Learning rate decreased to 0.14449.
2024-12-02-12:03:14-root-INFO: grad norm: 11.403 11.306 1.481
2024-12-02-12:03:14-root-INFO: Loss Change: 112.194 -> 97.073
2024-12-02-12:03:14-root-INFO: Regularization Change: 0.000 -> 8.158
2024-12-02-12:03:14-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-12:03:14-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-12:03:14-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-12:03:14-root-INFO: grad norm: 6.045 5.991 0.802
2024-12-02-12:03:15-root-INFO: grad norm: 6.077 6.028 0.767
2024-12-02-12:03:15-root-INFO: grad norm: 6.696 6.644 0.834
2024-12-02-12:03:15-root-INFO: Loss too large (93.812->94.005)! Learning rate decreased to 0.14783.
2024-12-02-12:03:16-root-INFO: grad norm: 6.192 6.139 0.804
2024-12-02-12:03:16-root-INFO: grad norm: 4.890 4.843 0.680
2024-12-02-12:03:17-root-INFO: Loss Change: 96.712 -> 90.584
2024-12-02-12:03:17-root-INFO: Regularization Change: 0.000 -> 4.037
2024-12-02-12:03:17-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-12:03:17-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-12:03:17-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-12:03:17-root-INFO: grad norm: 4.734 4.652 0.875
2024-12-02-12:03:17-root-INFO: grad norm: 5.618 5.557 0.825
2024-12-02-12:03:18-root-INFO: Loss too large (90.239->91.412)! Learning rate decreased to 0.15121.
2024-12-02-12:03:18-root-INFO: grad norm: 8.072 8.017 0.944
2024-12-02-12:03:18-root-INFO: Loss too large (90.009->90.334)! Learning rate decreased to 0.12097.
2024-12-02-12:03:19-root-INFO: grad norm: 3.761 3.721 0.547
2024-12-02-12:03:19-root-INFO: grad norm: 3.089 3.061 0.417
2024-12-02-12:03:19-root-INFO: Loss Change: 90.815 -> 87.438
2024-12-02-12:03:20-root-INFO: Regularization Change: 0.000 -> 1.991
2024-12-02-12:03:20-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-12:03:20-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-12:03:20-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-12:03:20-root-INFO: grad norm: 2.510 2.464 0.479
2024-12-02-12:03:20-root-INFO: grad norm: 2.660 2.633 0.373
2024-12-02-12:03:21-root-INFO: grad norm: 3.789 3.761 0.455
2024-12-02-12:03:21-root-INFO: Loss too large (86.355->87.205)! Learning rate decreased to 0.15465.
2024-12-02-12:03:21-root-INFO: grad norm: 7.274 7.242 0.678
2024-12-02-12:03:22-root-INFO: Loss too large (86.286->86.867)! Learning rate decreased to 0.12372.
2024-12-02-12:03:22-root-INFO: Loss too large (86.286->86.485)! Learning rate decreased to 0.09898.
2024-12-02-12:03:22-root-INFO: grad norm: 3.359 3.329 0.447
2024-12-02-12:03:22-root-INFO: Loss Change: 87.336 -> 85.084
2024-12-02-12:03:22-root-INFO: Regularization Change: 0.000 -> 1.806
2024-12-02-12:03:22-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-12:03:22-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-12:03:23-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-12:03:23-root-INFO: grad norm: 3.302 3.259 0.528
2024-12-02-12:03:23-root-INFO: Loss too large (85.167->85.225)! Learning rate decreased to 0.15815.
2024-12-02-12:03:23-root-INFO: grad norm: 5.290 5.265 0.514
2024-12-02-12:03:24-root-INFO: Loss too large (84.762->85.239)! Learning rate decreased to 0.12652.
2024-12-02-12:03:24-root-INFO: Loss too large (84.762->84.928)! Learning rate decreased to 0.10121.
2024-12-02-12:03:24-root-INFO: grad norm: 3.476 3.448 0.442
2024-12-02-12:03:25-root-INFO: grad norm: 1.816 1.788 0.316
2024-12-02-12:03:25-root-INFO: grad norm: 1.731 1.704 0.304
2024-12-02-12:03:25-root-INFO: Loss Change: 85.167 -> 83.094
2024-12-02-12:03:25-root-INFO: Regularization Change: 0.000 -> 1.008
2024-12-02-12:03:25-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-12:03:25-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-12:03:26-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-12:03:26-root-INFO: grad norm: 2.617 2.552 0.579
2024-12-02-12:03:26-root-INFO: grad norm: 3.317 3.274 0.534
2024-12-02-12:03:26-root-INFO: Loss too large (82.367->83.348)! Learning rate decreased to 0.16169.
2024-12-02-12:03:27-root-INFO: Loss too large (82.367->82.626)! Learning rate decreased to 0.12935.
2024-12-02-12:03:27-root-INFO: grad norm: 5.440 5.409 0.575
2024-12-02-12:03:27-root-INFO: Loss too large (82.221->82.635)! Learning rate decreased to 0.10348.
2024-12-02-12:03:27-root-INFO: Loss too large (82.221->82.311)! Learning rate decreased to 0.08279.
2024-12-02-12:03:28-root-INFO: grad norm: 3.658 3.630 0.445
2024-12-02-12:03:28-root-INFO: grad norm: 1.809 1.785 0.296
2024-12-02-12:03:29-root-INFO: Loss Change: 82.902 -> 81.080
2024-12-02-12:03:29-root-INFO: Regularization Change: 0.000 -> 0.999
2024-12-02-12:03:29-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-12:03:29-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-12:03:29-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-12:03:29-root-INFO: grad norm: 1.977 1.944 0.361
2024-12-02-12:03:30-root-INFO: grad norm: 2.183 2.159 0.322
2024-12-02-12:03:30-root-INFO: grad norm: 3.693 3.670 0.410
2024-12-02-12:03:30-root-INFO: Loss too large (80.305->82.189)! Learning rate decreased to 0.16529.
2024-12-02-12:03:30-root-INFO: Loss too large (80.305->81.058)! Learning rate decreased to 0.13223.
2024-12-02-12:03:30-root-INFO: Loss too large (80.305->80.356)! Learning rate decreased to 0.10579.
2024-12-02-12:03:31-root-INFO: grad norm: 4.571 4.548 0.456
2024-12-02-12:03:31-root-INFO: Loss too large (79.973->80.122)! Learning rate decreased to 0.08463.
2024-12-02-12:03:32-root-INFO: grad norm: 3.638 3.612 0.432
2024-12-02-12:03:32-root-INFO: Loss Change: 81.082 -> 79.296
2024-12-02-12:03:32-root-INFO: Regularization Change: 0.000 -> 1.251
2024-12-02-12:03:32-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-12:03:32-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-12:03:32-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-12:03:32-root-INFO: grad norm: 3.549 3.498 0.602
2024-12-02-12:03:32-root-INFO: Loss too large (79.418->80.040)! Learning rate decreased to 0.16894.
2024-12-02-12:03:33-root-INFO: Loss too large (79.418->79.707)! Learning rate decreased to 0.13515.
2024-12-02-12:03:33-root-INFO: Loss too large (79.418->79.446)! Learning rate decreased to 0.10812.
2024-12-02-12:03:33-root-INFO: grad norm: 3.617 3.593 0.415
2024-12-02-12:03:34-root-INFO: grad norm: 4.470 4.446 0.462
2024-12-02-12:03:34-root-INFO: Loss too large (78.904->79.051)! Learning rate decreased to 0.08650.
2024-12-02-12:03:34-root-INFO: grad norm: 3.599 3.575 0.415
2024-12-02-12:03:35-root-INFO: grad norm: 2.450 2.429 0.317
2024-12-02-12:03:35-root-INFO: Loss Change: 79.418 -> 78.063
2024-12-02-12:03:35-root-INFO: Regularization Change: 0.000 -> 0.636
2024-12-02-12:03:35-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-12:03:35-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-12:03:35-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-12:03:36-root-INFO: grad norm: 2.569 2.530 0.445
2024-12-02-12:03:36-root-INFO: Loss too large (78.023->78.185)! Learning rate decreased to 0.17265.
2024-12-02-12:03:36-root-INFO: grad norm: 5.222 5.202 0.459
2024-12-02-12:03:36-root-INFO: Loss too large (77.897->78.724)! Learning rate decreased to 0.13812.
2024-12-02-12:03:36-root-INFO: Loss too large (77.897->78.406)! Learning rate decreased to 0.11050.
2024-12-02-12:03:37-root-INFO: Loss too large (77.897->78.086)! Learning rate decreased to 0.08840.
2024-12-02-12:03:37-root-INFO: grad norm: 3.790 3.765 0.430
2024-12-02-12:03:38-root-INFO: grad norm: 2.058 2.037 0.292
2024-12-02-12:03:38-root-INFO: grad norm: 2.217 2.196 0.303
2024-12-02-12:03:38-root-INFO: Loss Change: 78.023 -> 76.649
2024-12-02-12:03:38-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-02-12:03:38-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-12:03:38-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-12:03:39-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-12:03:39-root-INFO: grad norm: 3.588 3.546 0.546
2024-12-02-12:03:39-root-INFO: Loss too large (76.637->77.410)! Learning rate decreased to 0.17641.
2024-12-02-12:03:39-root-INFO: Loss too large (76.637->77.086)! Learning rate decreased to 0.14113.
2024-12-02-12:03:39-root-INFO: Loss too large (76.637->76.807)! Learning rate decreased to 0.11290.
2024-12-02-12:03:40-root-INFO: grad norm: 3.785 3.762 0.419
2024-12-02-12:03:40-root-INFO: grad norm: 4.564 4.540 0.459
2024-12-02-12:03:40-root-INFO: Loss too large (76.183->76.363)! Learning rate decreased to 0.09032.
2024-12-02-12:03:41-root-INFO: grad norm: 3.674 3.651 0.409
2024-12-02-12:03:41-root-INFO: grad norm: 2.542 2.523 0.313
2024-12-02-12:03:42-root-INFO: Loss Change: 76.637 -> 75.330
2024-12-02-12:03:42-root-INFO: Regularization Change: 0.000 -> 0.660
2024-12-02-12:03:42-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-12:03:42-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-12:03:42-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-12:03:42-root-INFO: grad norm: 2.573 2.541 0.406
2024-12-02-12:03:42-root-INFO: Loss too large (75.120->75.362)! Learning rate decreased to 0.18022.
2024-12-02-12:03:42-root-INFO: grad norm: 5.150 5.131 0.441
2024-12-02-12:03:43-root-INFO: Loss too large (75.043->75.847)! Learning rate decreased to 0.14417.
2024-12-02-12:03:43-root-INFO: Loss too large (75.043->75.514)! Learning rate decreased to 0.11534.
2024-12-02-12:03:43-root-INFO: Loss too large (75.043->75.177)! Learning rate decreased to 0.09227.
2024-12-02-12:03:44-root-INFO: grad norm: 3.799 3.775 0.427
2024-12-02-12:03:44-root-INFO: grad norm: 2.350 2.332 0.295
2024-12-02-12:03:45-root-INFO: grad norm: 2.538 2.518 0.317
2024-12-02-12:03:45-root-INFO: Loss Change: 75.120 -> 73.790
2024-12-02-12:03:45-root-INFO: Regularization Change: 0.000 -> 0.758
2024-12-02-12:03:45-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-12:03:45-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-12:03:45-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-12:03:45-root-INFO: grad norm: 3.634 3.595 0.534
2024-12-02-12:03:45-root-INFO: Loss too large (73.892->74.714)! Learning rate decreased to 0.18408.
2024-12-02-12:03:45-root-INFO: Loss too large (73.892->74.376)! Learning rate decreased to 0.14727.
2024-12-02-12:03:46-root-INFO: Loss too large (73.892->74.080)! Learning rate decreased to 0.11781.
2024-12-02-12:03:46-root-INFO: grad norm: 3.783 3.761 0.407
2024-12-02-12:03:47-root-INFO: grad norm: 4.338 4.317 0.427
2024-12-02-12:03:47-root-INFO: Loss too large (73.410->73.538)! Learning rate decreased to 0.09425.
2024-12-02-12:03:47-root-INFO: grad norm: 3.607 3.586 0.393
2024-12-02-12:03:48-root-INFO: grad norm: 2.793 2.776 0.314
2024-12-02-12:03:48-root-INFO: Loss Change: 73.892 -> 72.598
2024-12-02-12:03:48-root-INFO: Regularization Change: 0.000 -> 0.703
2024-12-02-12:03:48-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-12:03:48-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-12:03:48-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-12:03:48-root-INFO: grad norm: 2.814 2.784 0.412
2024-12-02-12:03:48-root-INFO: Loss too large (72.524->73.293)! Learning rate decreased to 0.18800.
2024-12-02-12:03:49-root-INFO: Loss too large (72.524->72.768)! Learning rate decreased to 0.15040.
2024-12-02-12:03:49-root-INFO: grad norm: 4.794 4.775 0.431
2024-12-02-12:03:49-root-INFO: Loss too large (72.452->72.902)! Learning rate decreased to 0.12032.
2024-12-02-12:03:49-root-INFO: Loss too large (72.452->72.566)! Learning rate decreased to 0.09626.
2024-12-02-12:03:50-root-INFO: grad norm: 3.732 3.711 0.401
2024-12-02-12:03:50-root-INFO: grad norm: 2.675 2.657 0.308
2024-12-02-12:03:51-root-INFO: grad norm: 2.824 2.805 0.327
2024-12-02-12:03:51-root-INFO: Loss Change: 72.524 -> 71.276
2024-12-02-12:03:51-root-INFO: Regularization Change: 0.000 -> 0.713
2024-12-02-12:03:51-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-12:03:51-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-12:03:51-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-12:03:52-root-INFO: grad norm: 3.851 3.811 0.553
2024-12-02-12:03:52-root-INFO: Loss too large (71.295->72.192)! Learning rate decreased to 0.19197.
2024-12-02-12:03:52-root-INFO: Loss too large (71.295->71.839)! Learning rate decreased to 0.15357.
2024-12-02-12:03:52-root-INFO: Loss too large (71.295->71.521)! Learning rate decreased to 0.12286.
2024-12-02-12:03:52-root-INFO: grad norm: 3.787 3.766 0.400
2024-12-02-12:03:53-root-INFO: grad norm: 3.872 3.851 0.399
2024-12-02-12:03:53-root-INFO: Loss too large (70.690->70.735)! Learning rate decreased to 0.09829.
2024-12-02-12:03:54-root-INFO: grad norm: 3.409 3.389 0.366
2024-12-02-12:03:54-root-INFO: grad norm: 2.976 2.958 0.322
2024-12-02-12:03:54-root-INFO: Loss Change: 71.295 -> 69.943
2024-12-02-12:03:54-root-INFO: Regularization Change: 0.000 -> 0.762
2024-12-02-12:03:54-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-12:03:54-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-12:03:55-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-12:03:55-root-INFO: grad norm: 2.804 2.780 0.365
2024-12-02-12:03:55-root-INFO: Loss too large (69.891->70.717)! Learning rate decreased to 0.19599.
2024-12-02-12:03:55-root-INFO: Loss too large (69.891->70.181)! Learning rate decreased to 0.15679.
2024-12-02-12:03:55-root-INFO: grad norm: 4.602 4.584 0.408
2024-12-02-12:03:56-root-INFO: Loss too large (69.851->70.212)! Learning rate decreased to 0.12543.
2024-12-02-12:03:56-root-INFO: Loss too large (69.851->69.859)! Learning rate decreased to 0.10035.
2024-12-02-12:03:56-root-INFO: grad norm: 3.616 3.596 0.383
2024-12-02-12:03:57-root-INFO: grad norm: 2.853 2.837 0.308
2024-12-02-12:03:57-root-INFO: grad norm: 2.863 2.844 0.323
2024-12-02-12:03:57-root-INFO: Loss Change: 69.891 -> 68.642
2024-12-02-12:03:57-root-INFO: Regularization Change: 0.000 -> 0.745
2024-12-02-12:03:57-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-12:03:57-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-12:03:58-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-12:03:58-root-INFO: grad norm: 3.578 3.545 0.482
2024-12-02-12:03:58-root-INFO: Loss too large (68.697->69.549)! Learning rate decreased to 0.20006.
2024-12-02-12:03:58-root-INFO: Loss too large (68.697->69.188)! Learning rate decreased to 0.16005.
2024-12-02-12:03:58-root-INFO: Loss too large (68.697->68.862)! Learning rate decreased to 0.12804.
2024-12-02-12:03:59-root-INFO: grad norm: 3.724 3.703 0.395
2024-12-02-12:03:59-root-INFO: grad norm: 4.011 3.993 0.383
2024-12-02-12:04:00-root-INFO: grad norm: 3.741 3.719 0.402
2024-12-02-12:04:00-root-INFO: grad norm: 3.362 3.345 0.339
2024-12-02-12:04:01-root-INFO: Loss Change: 68.697 -> 67.479
2024-12-02-12:04:01-root-INFO: Regularization Change: 0.000 -> 1.041
2024-12-02-12:04:01-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-12:04:01-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-12:04:01-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-12:04:01-root-INFO: grad norm: 3.614 3.590 0.418
2024-12-02-12:04:01-root-INFO: Loss too large (67.373->68.800)! Learning rate decreased to 0.20419.
2024-12-02-12:04:01-root-INFO: Loss too large (67.373->67.900)! Learning rate decreased to 0.16335.
2024-12-02-12:04:02-root-INFO: grad norm: 4.928 4.908 0.441
2024-12-02-12:04:02-root-INFO: Loss too large (67.318->67.424)! Learning rate decreased to 0.13068.
2024-12-02-12:04:02-root-INFO: grad norm: 3.720 3.699 0.393
2024-12-02-12:04:03-root-INFO: grad norm: 2.730 2.713 0.311
2024-12-02-12:04:03-root-INFO: grad norm: 3.084 3.064 0.343
2024-12-02-12:04:04-root-INFO: Loss Change: 67.373 -> 65.803
2024-12-02-12:04:04-root-INFO: Regularization Change: 0.000 -> 1.170
2024-12-02-12:04:04-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-12:04:04-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-12:04:04-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-12:04:04-root-INFO: grad norm: 4.143 4.108 0.541
2024-12-02-12:04:04-root-INFO: Loss too large (65.713->66.719)! Learning rate decreased to 0.20837.
2024-12-02-12:04:04-root-INFO: Loss too large (65.713->66.219)! Learning rate decreased to 0.16669.
2024-12-02-12:04:04-root-INFO: Loss too large (65.713->65.781)! Learning rate decreased to 0.13335.
2024-12-02-12:04:05-root-INFO: grad norm: 3.715 3.694 0.399
2024-12-02-12:04:05-root-INFO: grad norm: 3.435 3.416 0.360
2024-12-02-12:04:06-root-INFO: grad norm: 3.493 3.472 0.380
2024-12-02-12:04:06-root-INFO: grad norm: 3.521 3.503 0.351
2024-12-02-12:04:07-root-INFO: Loss Change: 65.713 -> 64.214
2024-12-02-12:04:07-root-INFO: Regularization Change: 0.000 -> 1.161
2024-12-02-12:04:07-root-INFO: Undo step: 50
2024-12-02-12:04:07-root-INFO: Undo step: 51
2024-12-02-12:04:07-root-INFO: Undo step: 52
2024-12-02-12:04:07-root-INFO: Undo step: 53
2024-12-02-12:04:07-root-INFO: Undo step: 54
2024-12-02-12:04:07-root-INFO: Undo step: 55
2024-12-02-12:04:07-root-INFO: Undo step: 56
2024-12-02-12:04:07-root-INFO: Undo step: 57
2024-12-02-12:04:07-root-INFO: Undo step: 58
2024-12-02-12:04:07-root-INFO: Undo step: 59
2024-12-02-12:04:07-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-12:04:07-root-INFO: grad norm: 31.015 30.753 4.019
2024-12-02-12:04:07-root-INFO: grad norm: 16.960 16.785 2.430
2024-12-02-12:04:08-root-INFO: grad norm: 11.178 11.040 1.756
2024-12-02-12:04:08-root-INFO: grad norm: 8.985 8.874 1.406
2024-12-02-12:04:09-root-INFO: grad norm: 7.981 7.885 1.233
2024-12-02-12:04:09-root-INFO: Loss Change: 267.347 -> 104.157
2024-12-02-12:04:09-root-INFO: Regularization Change: 0.000 -> 123.924
2024-12-02-12:04:09-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-12:04:09-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-12:04:09-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-12:04:10-root-INFO: grad norm: 8.656 8.562 1.274
2024-12-02-12:04:10-root-INFO: grad norm: 8.013 7.916 1.243
2024-12-02-12:04:10-root-INFO: grad norm: 7.778 7.707 1.054
2024-12-02-12:04:11-root-INFO: grad norm: 8.030 7.960 1.058
2024-12-02-12:04:11-root-INFO: grad norm: 7.447 7.396 0.871
2024-12-02-12:04:12-root-INFO: Loss Change: 104.179 -> 89.307
2024-12-02-12:04:12-root-INFO: Regularization Change: 0.000 -> 18.211
2024-12-02-12:04:12-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-12:04:12-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-12:04:12-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-12:04:12-root-INFO: grad norm: 7.006 6.962 0.782
2024-12-02-12:04:12-root-INFO: grad norm: 7.237 7.195 0.779
2024-12-02-12:04:13-root-INFO: grad norm: 7.781 7.724 0.939
2024-12-02-12:04:13-root-INFO: grad norm: 10.473 10.404 1.204
2024-12-02-12:04:14-root-INFO: Loss too large (85.734->86.628)! Learning rate decreased to 0.17641.
2024-12-02-12:04:14-root-INFO: grad norm: 6.446 6.376 0.942
2024-12-02-12:04:14-root-INFO: Loss Change: 88.826 -> 81.060
2024-12-02-12:04:14-root-INFO: Regularization Change: 0.000 -> 8.384
2024-12-02-12:04:14-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-12:04:14-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-12:04:15-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-12:04:15-root-INFO: grad norm: 5.087 5.038 0.709
2024-12-02-12:04:15-root-INFO: grad norm: 5.149 5.098 0.720
2024-12-02-12:04:16-root-INFO: grad norm: 6.700 6.642 0.883
2024-12-02-12:04:16-root-INFO: Loss too large (78.555->79.580)! Learning rate decreased to 0.18022.
2024-12-02-12:04:16-root-INFO: grad norm: 5.799 5.728 0.905
2024-12-02-12:04:17-root-INFO: grad norm: 5.058 5.008 0.706
2024-12-02-12:04:17-root-INFO: Loss Change: 80.886 -> 75.888
2024-12-02-12:04:17-root-INFO: Regularization Change: 0.000 -> 5.118
2024-12-02-12:04:17-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-12:04:17-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-12:04:17-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-12:04:17-root-INFO: grad norm: 4.527 4.481 0.648
2024-12-02-12:04:18-root-INFO: grad norm: 6.588 6.533 0.850
2024-12-02-12:04:18-root-INFO: Loss too large (75.048->76.347)! Learning rate decreased to 0.18408.
2024-12-02-12:04:18-root-INFO: Loss too large (75.048->75.104)! Learning rate decreased to 0.14727.
2024-12-02-12:04:19-root-INFO: grad norm: 4.309 4.249 0.717
2024-12-02-12:04:19-root-INFO: grad norm: 2.315 2.285 0.371
2024-12-02-12:04:20-root-INFO: grad norm: 2.128 2.097 0.363
2024-12-02-12:04:20-root-INFO: Loss Change: 75.691 -> 71.634
2024-12-02-12:04:20-root-INFO: Regularization Change: 0.000 -> 2.700
2024-12-02-12:04:20-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-12:04:20-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-12:04:20-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-12:04:20-root-INFO: grad norm: 3.030 2.977 0.565
2024-12-02-12:04:21-root-INFO: grad norm: 4.004 3.945 0.690
2024-12-02-12:04:21-root-INFO: Loss too large (71.449->71.519)! Learning rate decreased to 0.18800.
2024-12-02-12:04:21-root-INFO: grad norm: 4.762 4.712 0.688
2024-12-02-12:04:22-root-INFO: Loss too large (70.745->70.946)! Learning rate decreased to 0.15040.
2024-12-02-12:04:22-root-INFO: grad norm: 3.924 3.868 0.663
2024-12-02-12:04:23-root-INFO: grad norm: 2.963 2.929 0.444
2024-12-02-12:04:23-root-INFO: Loss Change: 71.641 -> 69.076
2024-12-02-12:04:23-root-INFO: Regularization Change: 0.000 -> 2.436
2024-12-02-12:04:23-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-12:04:23-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-12:04:23-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-12:04:23-root-INFO: grad norm: 3.007 2.970 0.473
2024-12-02-12:04:23-root-INFO: Loss too large (68.832->68.868)! Learning rate decreased to 0.19197.
2024-12-02-12:04:24-root-INFO: grad norm: 4.172 4.137 0.539
2024-12-02-12:04:24-root-INFO: Loss too large (68.487->68.655)! Learning rate decreased to 0.15357.
2024-12-02-12:04:24-root-INFO: grad norm: 3.758 3.710 0.598
2024-12-02-12:04:25-root-INFO: grad norm: 3.241 3.209 0.453
2024-12-02-12:04:25-root-INFO: grad norm: 3.279 3.238 0.514
2024-12-02-12:04:26-root-INFO: Loss Change: 68.832 -> 66.708
2024-12-02-12:04:26-root-INFO: Regularization Change: 0.000 -> 1.888
2024-12-02-12:04:26-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-12:04:26-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-12:04:26-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-12:04:26-root-INFO: grad norm: 4.061 4.011 0.638
2024-12-02-12:04:26-root-INFO: Loss too large (66.852->67.513)! Learning rate decreased to 0.19599.
2024-12-02-12:04:26-root-INFO: Loss too large (66.852->66.927)! Learning rate decreased to 0.15679.
2024-12-02-12:04:27-root-INFO: grad norm: 3.684 3.637 0.590
2024-12-02-12:04:27-root-INFO: grad norm: 3.421 3.387 0.481
2024-12-02-12:04:28-root-INFO: grad norm: 3.406 3.362 0.544
2024-12-02-12:04:28-root-INFO: grad norm: 3.376 3.344 0.460
2024-12-02-12:04:29-root-INFO: Loss Change: 66.852 -> 64.888
2024-12-02-12:04:29-root-INFO: Regularization Change: 0.000 -> 1.756
2024-12-02-12:04:29-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-12:04:29-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-12:04:29-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-12:04:29-root-INFO: grad norm: 3.272 3.235 0.487
2024-12-02-12:04:29-root-INFO: Loss too large (64.718->65.101)! Learning rate decreased to 0.20006.
2024-12-02-12:04:30-root-INFO: grad norm: 4.538 4.501 0.579
2024-12-02-12:04:30-root-INFO: Loss too large (64.500->64.800)! Learning rate decreased to 0.16005.
2024-12-02-12:04:30-root-INFO: grad norm: 3.842 3.792 0.619
2024-12-02-12:04:31-root-INFO: grad norm: 2.995 2.969 0.390
2024-12-02-12:04:31-root-INFO: grad norm: 2.947 2.913 0.444
2024-12-02-12:04:31-root-INFO: Loss Change: 64.718 -> 62.707
2024-12-02-12:04:31-root-INFO: Regularization Change: 0.000 -> 1.767
2024-12-02-12:04:31-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-12:04:31-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-12:04:32-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-12:04:32-root-INFO: grad norm: 3.780 3.723 0.653
2024-12-02-12:04:32-root-INFO: Loss too large (62.791->63.405)! Learning rate decreased to 0.20419.
2024-12-02-12:04:32-root-INFO: Loss too large (62.791->62.801)! Learning rate decreased to 0.16335.
2024-12-02-12:04:33-root-INFO: grad norm: 3.465 3.424 0.528
2024-12-02-12:04:33-root-INFO: grad norm: 3.338 3.305 0.463
2024-12-02-12:04:33-root-INFO: grad norm: 3.277 3.239 0.500
2024-12-02-12:04:34-root-INFO: grad norm: 3.204 3.174 0.433
2024-12-02-12:04:34-root-INFO: Loss Change: 62.791 -> 60.982
2024-12-02-12:04:34-root-INFO: Regularization Change: 0.000 -> 1.663
2024-12-02-12:04:34-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-12:04:34-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-12:04:34-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-12:04:35-root-INFO: grad norm: 3.051 3.018 0.447
2024-12-02-12:04:35-root-INFO: Loss too large (60.585->60.912)! Learning rate decreased to 0.20837.
2024-12-02-12:04:35-root-INFO: grad norm: 4.230 4.195 0.543
2024-12-02-12:04:35-root-INFO: Loss too large (60.376->60.683)! Learning rate decreased to 0.16669.
2024-12-02-12:04:36-root-INFO: grad norm: 3.682 3.638 0.566
2024-12-02-12:04:37-root-INFO: grad norm: 2.981 2.959 0.368
2024-12-02-12:04:37-root-INFO: grad norm: 2.898 2.869 0.409
2024-12-02-12:04:37-root-INFO: Loss Change: 60.585 -> 58.777
2024-12-02-12:04:37-root-INFO: Regularization Change: 0.000 -> 1.705
2024-12-02-12:04:37-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-12:04:37-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-12:04:38-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-12:04:38-root-INFO: grad norm: 3.738 3.690 0.597
2024-12-02-12:04:38-root-INFO: Loss too large (58.956->59.632)! Learning rate decreased to 0.21260.
2024-12-02-12:04:38-root-INFO: grad norm: 4.292 4.240 0.669
2024-12-02-12:04:39-root-INFO: grad norm: 5.317 5.272 0.694
2024-12-02-12:04:39-root-INFO: Loss too large (58.725->59.168)! Learning rate decreased to 0.17008.
2024-12-02-12:04:39-root-INFO: grad norm: 4.217 4.170 0.625
2024-12-02-12:04:40-root-INFO: grad norm: 3.223 3.202 0.363
2024-12-02-12:04:40-root-INFO: Loss Change: 58.956 -> 56.975
2024-12-02-12:04:40-root-INFO: Regularization Change: 0.000 -> 1.905
2024-12-02-12:04:40-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-12:04:40-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-12:04:40-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-12:04:40-root-INFO: grad norm: 2.797 2.778 0.331
2024-12-02-12:04:41-root-INFO: Loss too large (56.810->56.947)! Learning rate decreased to 0.21688.
2024-12-02-12:04:41-root-INFO: grad norm: 3.346 3.327 0.358
2024-12-02-12:04:42-root-INFO: grad norm: 4.173 4.144 0.490
2024-12-02-12:04:42-root-INFO: Loss too large (56.487->56.626)! Learning rate decreased to 0.17350.
2024-12-02-12:04:42-root-INFO: grad norm: 3.653 3.628 0.429
2024-12-02-12:04:43-root-INFO: grad norm: 3.212 3.180 0.448
2024-12-02-12:04:43-root-INFO: Loss Change: 56.810 -> 55.165
2024-12-02-12:04:43-root-INFO: Regularization Change: 0.000 -> 1.796
2024-12-02-12:04:43-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-12:04:43-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-12:04:43-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-12:04:43-root-INFO: grad norm: 3.832 3.780 0.633
2024-12-02-12:04:43-root-INFO: Loss too large (55.333->56.231)! Learning rate decreased to 0.22121.
2024-12-02-12:04:44-root-INFO: Loss too large (55.333->55.405)! Learning rate decreased to 0.17697.
2024-12-02-12:04:44-root-INFO: grad norm: 3.338 3.304 0.477
2024-12-02-12:04:45-root-INFO: grad norm: 2.998 2.971 0.397
2024-12-02-12:04:45-root-INFO: grad norm: 2.856 2.827 0.403
2024-12-02-12:04:46-root-INFO: grad norm: 2.774 2.750 0.365
2024-12-02-12:04:46-root-INFO: Loss Change: 55.333 -> 53.537
2024-12-02-12:04:46-root-INFO: Regularization Change: 0.000 -> 1.623
2024-12-02-12:04:46-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-12:04:46-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-12:04:46-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-12:04:46-root-INFO: grad norm: 2.637 2.607 0.394
2024-12-02-12:04:46-root-INFO: Loss too large (53.380->53.568)! Learning rate decreased to 0.22559.
2024-12-02-12:04:47-root-INFO: grad norm: 3.682 3.652 0.468
2024-12-02-12:04:47-root-INFO: Loss too large (53.146->53.398)! Learning rate decreased to 0.18047.
2024-12-02-12:04:48-root-INFO: grad norm: 3.121 3.088 0.456
2024-12-02-12:04:48-root-INFO: grad norm: 2.417 2.400 0.282
2024-12-02-12:04:48-root-INFO: grad norm: 2.370 2.352 0.298
2024-12-02-12:04:49-root-INFO: Loss Change: 53.380 -> 51.749
2024-12-02-12:04:49-root-INFO: Regularization Change: 0.000 -> 1.612
2024-12-02-12:04:49-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-12:04:49-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-12:04:49-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-12:04:49-root-INFO: grad norm: 3.107 3.066 0.504
2024-12-02-12:04:49-root-INFO: Loss too large (51.714->52.269)! Learning rate decreased to 0.23002.
2024-12-02-12:04:50-root-INFO: grad norm: 3.954 3.916 0.541
2024-12-02-12:04:50-root-INFO: Loss too large (51.677->51.850)! Learning rate decreased to 0.18402.
2024-12-02-12:04:50-root-INFO: grad norm: 3.621 3.595 0.435
2024-12-02-12:04:51-root-INFO: grad norm: 3.310 3.280 0.449
2024-12-02-12:04:51-root-INFO: grad norm: 3.037 3.015 0.370
2024-12-02-12:04:52-root-INFO: Loss Change: 51.714 -> 50.193
2024-12-02-12:04:52-root-INFO: Regularization Change: 0.000 -> 1.727
2024-12-02-12:04:52-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-12:04:52-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-12:04:52-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-12:04:52-root-INFO: grad norm: 2.775 2.747 0.390
2024-12-02-12:04:52-root-INFO: Loss too large (50.078->50.422)! Learning rate decreased to 0.23450.
2024-12-02-12:04:53-root-INFO: grad norm: 3.856 3.827 0.468
2024-12-02-12:04:53-root-INFO: Loss too large (49.904->50.210)! Learning rate decreased to 0.18760.
2024-12-02-12:04:53-root-INFO: grad norm: 3.213 3.181 0.450
2024-12-02-12:04:54-root-INFO: grad norm: 2.438 2.422 0.279
2024-12-02-12:04:54-root-INFO: grad norm: 2.330 2.313 0.284
2024-12-02-12:04:55-root-INFO: Loss Change: 50.078 -> 48.435
2024-12-02-12:04:55-root-INFO: Regularization Change: 0.000 -> 1.654
2024-12-02-12:04:55-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-12:04:55-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-12:04:55-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-12:04:55-root-INFO: grad norm: 3.010 2.972 0.480
2024-12-02-12:04:55-root-INFO: Loss too large (48.314->48.840)! Learning rate decreased to 0.23903.
2024-12-02-12:04:55-root-INFO: grad norm: 3.768 3.733 0.514
2024-12-02-12:04:56-root-INFO: Loss too large (48.271->48.376)! Learning rate decreased to 0.19122.
2024-12-02-12:04:56-root-INFO: grad norm: 3.386 3.362 0.403
2024-12-02-12:04:57-root-INFO: grad norm: 3.098 3.070 0.412
2024-12-02-12:04:57-root-INFO: grad norm: 2.877 2.856 0.349
2024-12-02-12:04:57-root-INFO: Loss Change: 48.314 -> 46.822
2024-12-02-12:04:57-root-INFO: Regularization Change: 0.000 -> 1.741
2024-12-02-12:04:57-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-12:04:57-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-12:04:57-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-12:04:58-root-INFO: grad norm: 2.692 2.667 0.360
2024-12-02-12:04:58-root-INFO: Loss too large (46.646->47.010)! Learning rate decreased to 0.24361.
2024-12-02-12:04:58-root-INFO: grad norm: 3.789 3.763 0.447
2024-12-02-12:04:58-root-INFO: Loss too large (46.486->46.792)! Learning rate decreased to 0.19489.
2024-12-02-12:04:59-root-INFO: grad norm: 3.085 3.055 0.432
2024-12-02-12:04:59-root-INFO: grad norm: 2.242 2.228 0.251
2024-12-02-12:05:00-root-INFO: grad norm: 2.130 2.114 0.257
2024-12-02-12:05:00-root-INFO: Loss Change: 46.646 -> 45.014
2024-12-02-12:05:00-root-INFO: Regularization Change: 0.000 -> 1.661
2024-12-02-12:05:00-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-12:05:00-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-12:05:00-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-12:05:00-root-INFO: grad norm: 3.132 3.084 0.549
2024-12-02-12:05:01-root-INFO: Loss too large (45.104->45.635)! Learning rate decreased to 0.24866.
2024-12-02-12:05:01-root-INFO: grad norm: 3.755 3.720 0.510
2024-12-02-12:05:01-root-INFO: Loss too large (45.016->45.078)! Learning rate decreased to 0.19893.
2024-12-02-12:05:02-root-INFO: grad norm: 3.292 3.269 0.387
2024-12-02-12:05:02-root-INFO: grad norm: 2.979 2.954 0.390
2024-12-02-12:05:03-root-INFO: grad norm: 2.760 2.740 0.330
2024-12-02-12:05:03-root-INFO: Loss Change: 45.104 -> 43.515
2024-12-02-12:05:03-root-INFO: Regularization Change: 0.000 -> 1.839
2024-12-02-12:05:03-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-12:05:03-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-12:05:03-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-12:05:03-root-INFO: grad norm: 2.593 2.566 0.373
2024-12-02-12:05:03-root-INFO: Loss too large (43.335->43.567)! Learning rate decreased to 0.25333.
2024-12-02-12:05:04-root-INFO: grad norm: 3.527 3.503 0.414
2024-12-02-12:05:04-root-INFO: Loss too large (43.106->43.335)! Learning rate decreased to 0.20266.
2024-12-02-12:05:04-root-INFO: grad norm: 2.847 2.821 0.391
2024-12-02-12:05:05-root-INFO: grad norm: 2.018 2.004 0.235
2024-12-02-12:05:05-root-INFO: grad norm: 1.961 1.946 0.238
2024-12-02-12:05:06-root-INFO: Loss Change: 43.335 -> 41.677
2024-12-02-12:05:06-root-INFO: Regularization Change: 0.000 -> 1.721
2024-12-02-12:05:06-root-INFO: Undo step: 40
2024-12-02-12:05:06-root-INFO: Undo step: 41
2024-12-02-12:05:06-root-INFO: Undo step: 42
2024-12-02-12:05:06-root-INFO: Undo step: 43
2024-12-02-12:05:06-root-INFO: Undo step: 44
2024-12-02-12:05:06-root-INFO: Undo step: 45
2024-12-02-12:05:06-root-INFO: Undo step: 46
2024-12-02-12:05:06-root-INFO: Undo step: 47
2024-12-02-12:05:06-root-INFO: Undo step: 48
2024-12-02-12:05:06-root-INFO: Undo step: 49
2024-12-02-12:05:06-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-12:05:06-root-INFO: grad norm: 26.856 26.685 3.030
2024-12-02-12:05:06-root-INFO: grad norm: 12.833 12.716 1.729
2024-12-02-12:05:07-root-INFO: grad norm: 10.716 10.626 1.388
2024-12-02-12:05:07-root-INFO: grad norm: 9.450 9.385 1.105
2024-12-02-12:05:08-root-INFO: grad norm: 8.522 8.458 1.046
2024-12-02-12:05:08-root-INFO: Loss Change: 235.401 -> 82.143
2024-12-02-12:05:08-root-INFO: Regularization Change: 0.000 -> 142.647
2024-12-02-12:05:08-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-12:05:08-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-12:05:08-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-12:05:09-root-INFO: grad norm: 8.389 8.336 0.942
2024-12-02-12:05:09-root-INFO: grad norm: 6.715 6.667 0.803
2024-12-02-12:05:10-root-INFO: grad norm: 6.242 6.196 0.761
2024-12-02-12:05:10-root-INFO: grad norm: 6.003 5.968 0.648
2024-12-02-12:05:10-root-INFO: grad norm: 6.229 6.183 0.754
2024-12-02-12:05:11-root-INFO: Loss Change: 82.268 -> 67.579
2024-12-02-12:05:11-root-INFO: Regularization Change: 0.000 -> 20.287
2024-12-02-12:05:11-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-12:05:11-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-12:05:11-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-12:05:11-root-INFO: grad norm: 8.237 8.168 1.065
2024-12-02-12:05:11-root-INFO: Loss too large (67.645->68.618)! Learning rate decreased to 0.21688.
2024-12-02-12:05:12-root-INFO: grad norm: 5.397 5.339 0.788
2024-12-02-12:05:12-root-INFO: grad norm: 3.987 3.955 0.499
2024-12-02-12:05:13-root-INFO: grad norm: 3.874 3.849 0.439
2024-12-02-12:05:13-root-INFO: grad norm: 3.812 3.781 0.483
2024-12-02-12:05:14-root-INFO: Loss Change: 67.645 -> 59.701
2024-12-02-12:05:14-root-INFO: Regularization Change: 0.000 -> 6.854
2024-12-02-12:05:14-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-12:05:14-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-12:05:14-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-12:05:14-root-INFO: grad norm: 4.643 4.595 0.670
2024-12-02-12:05:14-root-INFO: Loss too large (59.742->59.985)! Learning rate decreased to 0.22121.
2024-12-02-12:05:15-root-INFO: grad norm: 4.151 4.111 0.579
2024-12-02-12:05:15-root-INFO: grad norm: 3.973 3.930 0.581
2024-12-02-12:05:15-root-INFO: grad norm: 4.195 4.151 0.603
2024-12-02-12:05:16-root-INFO: grad norm: 4.966 4.934 0.561
2024-12-02-12:05:16-root-INFO: Loss Change: 59.742 -> 56.489
2024-12-02-12:05:16-root-INFO: Regularization Change: 0.000 -> 5.450
2024-12-02-12:05:16-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-12:05:16-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-12:05:16-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-12:05:17-root-INFO: grad norm: 4.160 4.129 0.508
2024-12-02-12:05:17-root-INFO: grad norm: 5.577 5.533 0.699
2024-12-02-12:05:17-root-INFO: Loss too large (55.188->56.186)! Learning rate decreased to 0.22559.
2024-12-02-12:05:18-root-INFO: grad norm: 4.572 4.524 0.660
2024-12-02-12:05:18-root-INFO: grad norm: 4.083 4.061 0.428
2024-12-02-12:05:19-root-INFO: grad norm: 3.720 3.694 0.437
2024-12-02-12:05:19-root-INFO: Loss Change: 56.184 -> 51.925
2024-12-02-12:05:19-root-INFO: Regularization Change: 0.000 -> 4.692
2024-12-02-12:05:19-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-12:05:19-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-12:05:19-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-12:05:19-root-INFO: grad norm: 3.859 3.814 0.588
2024-12-02-12:05:19-root-INFO: Loss too large (51.844->52.075)! Learning rate decreased to 0.23002.
2024-12-02-12:05:20-root-INFO: grad norm: 3.860 3.829 0.488
2024-12-02-12:05:20-root-INFO: grad norm: 4.107 4.076 0.505
2024-12-02-12:05:21-root-INFO: grad norm: 4.266 4.226 0.580
2024-12-02-12:05:21-root-INFO: grad norm: 4.733 4.691 0.626
2024-12-02-12:05:22-root-INFO: Loss too large (49.998->50.146)! Learning rate decreased to 0.18402.
2024-12-02-12:05:22-root-INFO: Loss Change: 51.844 -> 49.468
2024-12-02-12:05:22-root-INFO: Regularization Change: 0.000 -> 3.302
2024-12-02-12:05:22-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-12:05:22-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-12:05:22-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-12:05:22-root-INFO: grad norm: 3.484 3.452 0.470
2024-12-02-12:05:22-root-INFO: Loss too large (49.267->49.309)! Learning rate decreased to 0.23450.
2024-12-02-12:05:23-root-INFO: grad norm: 3.760 3.733 0.454
2024-12-02-12:05:23-root-INFO: grad norm: 4.002 3.968 0.525
2024-12-02-12:05:24-root-INFO: grad norm: 4.276 4.243 0.524
2024-12-02-12:05:24-root-INFO: Loss too large (47.972->47.973)! Learning rate decreased to 0.18760.
2024-12-02-12:05:24-root-INFO: grad norm: 3.331 3.301 0.447
2024-12-02-12:05:25-root-INFO: Loss Change: 49.267 -> 46.694
2024-12-02-12:05:25-root-INFO: Regularization Change: 0.000 -> 2.648
2024-12-02-12:05:25-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-12:05:25-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-12:05:25-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-12:05:25-root-INFO: grad norm: 3.243 3.196 0.551
2024-12-02-12:05:25-root-INFO: Loss too large (46.563->46.878)! Learning rate decreased to 0.23903.
2024-12-02-12:05:26-root-INFO: grad norm: 3.615 3.580 0.500
2024-12-02-12:05:26-root-INFO: grad norm: 4.366 4.329 0.570
2024-12-02-12:05:26-root-INFO: Loss too large (46.089->46.211)! Learning rate decreased to 0.19122.
2024-12-02-12:05:27-root-INFO: grad norm: 3.376 3.343 0.471
2024-12-02-12:05:27-root-INFO: grad norm: 2.512 2.492 0.316
2024-12-02-12:05:27-root-INFO: Loss Change: 46.563 -> 44.454
2024-12-02-12:05:27-root-INFO: Regularization Change: 0.000 -> 2.166
2024-12-02-12:05:27-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-12:05:27-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-12:05:28-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-12:05:28-root-INFO: grad norm: 2.235 2.216 0.289
2024-12-02-12:05:28-root-INFO: grad norm: 3.651 3.627 0.415
2024-12-02-12:05:28-root-INFO: Loss too large (44.135->45.006)! Learning rate decreased to 0.24361.
2024-12-02-12:05:29-root-INFO: Loss too large (44.135->44.229)! Learning rate decreased to 0.19489.
2024-12-02-12:05:29-root-INFO: grad norm: 3.113 3.087 0.402
2024-12-02-12:05:29-root-INFO: grad norm: 2.642 2.622 0.328
2024-12-02-12:05:30-root-INFO: grad norm: 2.490 2.469 0.317
2024-12-02-12:05:30-root-INFO: Loss Change: 44.243 -> 42.564
2024-12-02-12:05:30-root-INFO: Regularization Change: 0.000 -> 1.987
2024-12-02-12:05:30-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-12:05:30-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-12:05:30-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-12:05:31-root-INFO: grad norm: 3.372 3.313 0.626
2024-12-02-12:05:31-root-INFO: Loss too large (42.634->43.208)! Learning rate decreased to 0.24866.
2024-12-02-12:05:31-root-INFO: grad norm: 3.701 3.663 0.532
2024-12-02-12:05:32-root-INFO: grad norm: 4.201 4.167 0.536
2024-12-02-12:05:32-root-INFO: Loss too large (42.230->42.342)! Learning rate decreased to 0.19893.
2024-12-02-12:05:32-root-INFO: grad norm: 3.252 3.223 0.435
2024-12-02-12:05:33-root-INFO: grad norm: 2.486 2.468 0.304
2024-12-02-12:05:33-root-INFO: Loss Change: 42.634 -> 40.713
2024-12-02-12:05:33-root-INFO: Regularization Change: 0.000 -> 2.037
2024-12-02-12:05:33-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-12:05:33-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-12:05:33-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-12:05:34-root-INFO: grad norm: 2.216 2.192 0.327
2024-12-02-12:05:34-root-INFO: grad norm: 3.480 3.457 0.399
2024-12-02-12:05:34-root-INFO: Loss too large (40.416->41.175)! Learning rate decreased to 0.25333.
2024-12-02-12:05:34-root-INFO: Loss too large (40.416->40.465)! Learning rate decreased to 0.20266.
2024-12-02-12:05:35-root-INFO: grad norm: 2.837 2.814 0.361
2024-12-02-12:05:35-root-INFO: grad norm: 2.314 2.296 0.286
2024-12-02-12:05:36-root-INFO: grad norm: 2.197 2.180 0.276
2024-12-02-12:05:36-root-INFO: Loss Change: 40.528 -> 38.942
2024-12-02-12:05:36-root-INFO: Regularization Change: 0.000 -> 1.873
2024-12-02-12:05:36-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-12:05:36-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-12:05:36-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-12:05:37-root-INFO: grad norm: 2.748 2.709 0.460
2024-12-02-12:05:37-root-INFO: Loss too large (38.863->39.243)! Learning rate decreased to 0.25805.
2024-12-02-12:05:37-root-INFO: grad norm: 3.184 3.153 0.443
2024-12-02-12:05:38-root-INFO: grad norm: 3.895 3.865 0.487
2024-12-02-12:05:38-root-INFO: Loss too large (38.594->38.793)! Learning rate decreased to 0.20644.
2024-12-02-12:05:38-root-INFO: grad norm: 3.051 3.023 0.412
2024-12-02-12:05:39-root-INFO: grad norm: 2.240 2.224 0.269
2024-12-02-12:05:39-root-INFO: Loss Change: 38.863 -> 37.272
2024-12-02-12:05:39-root-INFO: Regularization Change: 0.000 -> 1.830
2024-12-02-12:05:39-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-12:05:39-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-12:05:39-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-12:05:39-root-INFO: grad norm: 2.025 2.000 0.315
2024-12-02-12:05:40-root-INFO: grad norm: 3.069 3.049 0.349
2024-12-02-12:05:40-root-INFO: Loss too large (36.860->37.485)! Learning rate decreased to 0.26281.
2024-12-02-12:05:40-root-INFO: Loss too large (36.860->36.877)! Learning rate decreased to 0.21025.
2024-12-02-12:05:41-root-INFO: grad norm: 2.611 2.592 0.313
2024-12-02-12:05:41-root-INFO: grad norm: 2.283 2.265 0.280
2024-12-02-12:05:42-root-INFO: grad norm: 2.156 2.139 0.268
2024-12-02-12:05:42-root-INFO: Loss Change: 37.019 -> 35.581
2024-12-02-12:05:42-root-INFO: Regularization Change: 0.000 -> 1.781
2024-12-02-12:05:42-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-12:05:42-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-12:05:42-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-12:05:42-root-INFO: grad norm: 2.773 2.730 0.483
2024-12-02-12:05:42-root-INFO: Loss too large (35.543->35.931)! Learning rate decreased to 0.26762.
2024-12-02-12:05:43-root-INFO: grad norm: 3.058 3.027 0.437
2024-12-02-12:05:43-root-INFO: grad norm: 3.537 3.509 0.443
2024-12-02-12:05:43-root-INFO: Loss too large (35.185->35.295)! Learning rate decreased to 0.21410.
2024-12-02-12:05:44-root-INFO: grad norm: 2.754 2.730 0.367
2024-12-02-12:05:44-root-INFO: grad norm: 2.049 2.033 0.253
2024-12-02-12:05:45-root-INFO: Loss Change: 35.543 -> 33.989
2024-12-02-12:05:45-root-INFO: Regularization Change: 0.000 -> 1.790
2024-12-02-12:05:45-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-12:05:45-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-12:05:45-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-12:05:45-root-INFO: grad norm: 1.851 1.830 0.278
2024-12-02-12:05:45-root-INFO: grad norm: 2.961 2.941 0.341
2024-12-02-12:05:46-root-INFO: Loss too large (33.647->34.251)! Learning rate decreased to 0.27247.
2024-12-02-12:05:46-root-INFO: Loss too large (33.647->33.717)! Learning rate decreased to 0.21798.
2024-12-02-12:05:46-root-INFO: grad norm: 2.495 2.476 0.308
2024-12-02-12:05:47-root-INFO: grad norm: 2.033 2.018 0.250
2024-12-02-12:05:47-root-INFO: grad norm: 1.923 1.908 0.234
2024-12-02-12:05:48-root-INFO: Loss Change: 33.790 -> 32.444
2024-12-02-12:05:48-root-INFO: Regularization Change: 0.000 -> 1.712
2024-12-02-12:05:48-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-12:05:48-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-12:05:48-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-12:05:48-root-INFO: grad norm: 2.513 2.475 0.434
2024-12-02-12:05:48-root-INFO: Loss too large (32.428->32.701)! Learning rate decreased to 0.27737.
2024-12-02-12:05:48-root-INFO: grad norm: 2.784 2.756 0.391
2024-12-02-12:05:49-root-INFO: grad norm: 3.290 3.265 0.404
2024-12-02-12:05:49-root-INFO: Loss too large (32.085->32.181)! Learning rate decreased to 0.22190.
2024-12-02-12:05:50-root-INFO: grad norm: 2.580 2.558 0.335
2024-12-02-12:05:50-root-INFO: grad norm: 1.919 1.904 0.235
2024-12-02-12:05:50-root-INFO: Loss Change: 32.428 -> 30.991
2024-12-02-12:05:50-root-INFO: Regularization Change: 0.000 -> 1.730
2024-12-02-12:05:50-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-12:05:50-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-12:05:51-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-12:05:51-root-INFO: grad norm: 1.789 1.769 0.267
2024-12-02-12:05:51-root-INFO: grad norm: 2.728 2.710 0.312
2024-12-02-12:05:51-root-INFO: Loss too large (30.626->31.077)! Learning rate decreased to 0.28231.
2024-12-02-12:05:52-root-INFO: grad norm: 2.880 2.860 0.335
2024-12-02-12:05:52-root-INFO: grad norm: 2.978 2.961 0.315
2024-12-02-12:05:53-root-INFO: grad norm: 3.073 3.052 0.357
2024-12-02-12:05:53-root-INFO: Loss Change: 30.785 -> 29.776
2024-12-02-12:05:53-root-INFO: Regularization Change: 0.000 -> 2.464
2024-12-02-12:05:53-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-12:05:53-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-12:05:53-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-12:05:53-root-INFO: grad norm: 3.454 3.418 0.499
2024-12-02-12:05:54-root-INFO: Loss too large (29.625->30.143)! Learning rate decreased to 0.28730.
2024-12-02-12:05:54-root-INFO: grad norm: 3.229 3.202 0.414
2024-12-02-12:05:54-root-INFO: grad norm: 2.963 2.944 0.339
2024-12-02-12:05:55-root-INFO: grad norm: 2.880 2.859 0.345
2024-12-02-12:05:55-root-INFO: grad norm: 2.889 2.870 0.328
2024-12-02-12:05:56-root-INFO: Loss Change: 29.625 -> 28.102
2024-12-02-12:05:56-root-INFO: Regularization Change: 0.000 -> 2.678
2024-12-02-12:05:56-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-12:05:56-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-12:05:56-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-12:05:56-root-INFO: grad norm: 2.688 2.670 0.312
2024-12-02-12:05:56-root-INFO: Loss too large (27.846->27.864)! Learning rate decreased to 0.29232.
2024-12-02-12:05:57-root-INFO: grad norm: 2.550 2.535 0.279
2024-12-02-12:05:57-root-INFO: grad norm: 2.518 2.501 0.293
2024-12-02-12:05:58-root-INFO: grad norm: 2.526 2.511 0.283
2024-12-02-12:05:58-root-INFO: grad norm: 2.518 2.499 0.305
2024-12-02-12:05:58-root-INFO: Loss Change: 27.846 -> 26.377
2024-12-02-12:05:58-root-INFO: Regularization Change: 0.000 -> 2.389
2024-12-02-12:05:58-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-12:05:58-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-12:05:59-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-12:05:59-root-INFO: grad norm: 2.926 2.893 0.441
2024-12-02-12:05:59-root-INFO: Loss too large (26.338->26.641)! Learning rate decreased to 0.29738.
2024-12-02-12:05:59-root-INFO: grad norm: 2.709 2.684 0.366
2024-12-02-12:06:00-root-INFO: grad norm: 2.505 2.486 0.300
2024-12-02-12:06:00-root-INFO: grad norm: 2.447 2.428 0.305
2024-12-02-12:06:01-root-INFO: grad norm: 2.428 2.411 0.281
2024-12-02-12:06:01-root-INFO: Loss Change: 26.338 -> 24.969
2024-12-02-12:06:01-root-INFO: Regularization Change: 0.000 -> 2.410
2024-12-02-12:06:01-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-12:06:01-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-12:06:01-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-12:06:02-root-INFO: grad norm: 2.295 2.279 0.273
2024-12-02-12:06:02-root-INFO: grad norm: 3.122 3.105 0.324
2024-12-02-12:06:02-root-INFO: Loss too large (24.605->24.963)! Learning rate decreased to 0.30249.
2024-12-02-12:06:03-root-INFO: grad norm: 2.669 2.651 0.313
2024-12-02-12:06:03-root-INFO: grad norm: 2.147 2.134 0.233
2024-12-02-12:06:04-root-INFO: grad norm: 2.035 2.022 0.231
2024-12-02-12:06:04-root-INFO: Loss Change: 24.681 -> 23.262
2024-12-02-12:06:04-root-INFO: Regularization Change: 0.000 -> 2.374
2024-12-02-12:06:04-root-INFO: Undo step: 30
2024-12-02-12:06:04-root-INFO: Undo step: 31
2024-12-02-12:06:04-root-INFO: Undo step: 32
2024-12-02-12:06:04-root-INFO: Undo step: 33
2024-12-02-12:06:04-root-INFO: Undo step: 34
2024-12-02-12:06:04-root-INFO: Undo step: 35
2024-12-02-12:06:04-root-INFO: Undo step: 36
2024-12-02-12:06:04-root-INFO: Undo step: 37
2024-12-02-12:06:04-root-INFO: Undo step: 38
2024-12-02-12:06:04-root-INFO: Undo step: 39
2024-12-02-12:06:04-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-12:06:04-root-INFO: grad norm: 23.065 22.933 2.463
2024-12-02-12:06:05-root-INFO: grad norm: 10.365 10.261 1.468
2024-12-02-12:06:05-root-INFO: grad norm: 8.451 8.366 1.198
2024-12-02-12:06:06-root-INFO: grad norm: 8.434 8.384 0.915
2024-12-02-12:06:06-root-INFO: grad norm: 7.349 7.280 1.004
2024-12-02-12:06:07-root-INFO: Loss Change: 193.722 -> 59.587
2024-12-02-12:06:07-root-INFO: Regularization Change: 0.000 -> 152.166
2024-12-02-12:06:07-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-12:06:07-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-12:06:07-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-12:06:07-root-INFO: grad norm: 6.130 6.091 0.691
2024-12-02-12:06:07-root-INFO: grad norm: 5.568 5.514 0.773
2024-12-02-12:06:08-root-INFO: grad norm: 6.044 6.006 0.674
2024-12-02-12:06:08-root-INFO: grad norm: 5.484 5.428 0.779
2024-12-02-12:06:09-root-INFO: grad norm: 4.610 4.580 0.524
2024-12-02-12:06:09-root-INFO: Loss Change: 59.344 -> 45.723
2024-12-02-12:06:09-root-INFO: Regularization Change: 0.000 -> 22.363
2024-12-02-12:06:09-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-12:06:09-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-12:06:09-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-12:06:09-root-INFO: grad norm: 4.385 4.353 0.531
2024-12-02-12:06:10-root-INFO: grad norm: 4.699 4.671 0.514
2024-12-02-12:06:10-root-INFO: grad norm: 4.579 4.538 0.612
2024-12-02-12:06:11-root-INFO: grad norm: 4.304 4.278 0.469
2024-12-02-12:06:11-root-INFO: grad norm: 4.372 4.334 0.572
2024-12-02-12:06:12-root-INFO: Loss Change: 45.193 -> 39.661
2024-12-02-12:06:12-root-INFO: Regularization Change: 0.000 -> 11.444
2024-12-02-12:06:12-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-12:06:12-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-12:06:12-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-12:06:12-root-INFO: grad norm: 5.148 5.096 0.730
2024-12-02-12:06:13-root-INFO: grad norm: 4.763 4.706 0.737
2024-12-02-12:06:13-root-INFO: grad norm: 4.194 4.161 0.523
2024-12-02-12:06:14-root-INFO: grad norm: 4.139 4.097 0.582
2024-12-02-12:06:14-root-INFO: grad norm: 4.507 4.471 0.568
2024-12-02-12:06:14-root-INFO: Loss Change: 39.691 -> 35.764
2024-12-02-12:06:14-root-INFO: Regularization Change: 0.000 -> 8.794
2024-12-02-12:06:14-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-12:06:14-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-12:06:14-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-12:06:15-root-INFO: grad norm: 4.277 4.245 0.523
2024-12-02-12:06:15-root-INFO: grad norm: 4.085 4.060 0.448
2024-12-02-12:06:16-root-INFO: grad norm: 4.165 4.133 0.517
2024-12-02-12:06:16-root-INFO: grad norm: 4.389 4.362 0.493
2024-12-02-12:06:17-root-INFO: grad norm: 4.408 4.372 0.564
2024-12-02-12:06:17-root-INFO: Loss Change: 35.328 -> 32.379
2024-12-02-12:06:17-root-INFO: Regularization Change: 0.000 -> 6.884
2024-12-02-12:06:17-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-12:06:17-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-12:06:17-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-12:06:17-root-INFO: grad norm: 4.800 4.751 0.683
2024-12-02-12:06:18-root-INFO: grad norm: 4.669 4.619 0.678
2024-12-02-12:06:18-root-INFO: grad norm: 4.543 4.508 0.567
2024-12-02-12:06:19-root-INFO: grad norm: 4.433 4.391 0.604
2024-12-02-12:06:19-root-INFO: grad norm: 4.328 4.296 0.524
2024-12-02-12:06:19-root-INFO: Loss Change: 32.509 -> 29.918
2024-12-02-12:06:19-root-INFO: Regularization Change: 0.000 -> 6.313
2024-12-02-12:06:19-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-12:06:19-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-12:06:20-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-12:06:20-root-INFO: grad norm: 4.052 4.028 0.443
2024-12-02-12:06:20-root-INFO: grad norm: 3.961 3.939 0.418
2024-12-02-12:06:21-root-INFO: grad norm: 3.876 3.849 0.461
2024-12-02-12:06:21-root-INFO: grad norm: 3.837 3.814 0.417
2024-12-02-12:06:22-root-INFO: grad norm: 3.808 3.779 0.471
2024-12-02-12:06:22-root-INFO: Loss Change: 29.474 -> 27.342
2024-12-02-12:06:22-root-INFO: Regularization Change: 0.000 -> 5.182
2024-12-02-12:06:22-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-12:06:22-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-12:06:22-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-12:06:22-root-INFO: grad norm: 4.235 4.188 0.629
2024-12-02-12:06:23-root-INFO: grad norm: 4.046 4.001 0.599
2024-12-02-12:06:23-root-INFO: grad norm: 3.803 3.769 0.502
2024-12-02-12:06:24-root-INFO: grad norm: 3.705 3.668 0.516
2024-12-02-12:06:24-root-INFO: grad norm: 3.663 3.632 0.469
2024-12-02-12:06:24-root-INFO: Loss Change: 27.242 -> 25.226
2024-12-02-12:06:24-root-INFO: Regularization Change: 0.000 -> 4.876
2024-12-02-12:06:24-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-12:06:24-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-12:06:25-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-12:06:25-root-INFO: grad norm: 3.389 3.369 0.368
2024-12-02-12:06:25-root-INFO: grad norm: 3.263 3.244 0.355
2024-12-02-12:06:26-root-INFO: grad norm: 3.162 3.141 0.363
2024-12-02-12:06:26-root-INFO: grad norm: 3.192 3.174 0.340
2024-12-02-12:06:27-root-INFO: grad norm: 3.226 3.202 0.387
2024-12-02-12:06:27-root-INFO: Loss Change: 24.829 -> 23.240
2024-12-02-12:06:27-root-INFO: Regularization Change: 0.000 -> 4.065
2024-12-02-12:06:27-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-12:06:27-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-12:06:27-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-12:06:27-root-INFO: grad norm: 3.634 3.596 0.525
2024-12-02-12:06:28-root-INFO: grad norm: 3.533 3.496 0.505
2024-12-02-12:06:28-root-INFO: grad norm: 3.386 3.357 0.443
2024-12-02-12:06:29-root-INFO: grad norm: 3.326 3.295 0.455
2024-12-02-12:06:29-root-INFO: grad norm: 3.291 3.264 0.419
2024-12-02-12:06:30-root-INFO: Loss Change: 23.237 -> 21.712
2024-12-02-12:06:30-root-INFO: Regularization Change: 0.000 -> 4.029
2024-12-02-12:06:30-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-12:06:30-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-12:06:30-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-12:06:30-root-INFO: grad norm: 3.119 3.101 0.335
2024-12-02-12:06:30-root-INFO: grad norm: 2.996 2.977 0.330
2024-12-02-12:06:31-root-INFO: grad norm: 2.880 2.862 0.320
2024-12-02-12:06:31-root-INFO: grad norm: 2.913 2.897 0.306
2024-12-02-12:06:32-root-INFO: grad norm: 2.975 2.954 0.345
2024-12-02-12:06:32-root-INFO: Loss Change: 21.335 -> 20.068
2024-12-02-12:06:32-root-INFO: Regularization Change: 0.000 -> 3.556
2024-12-02-12:06:32-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-12:06:32-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-12:06:32-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-12:06:32-root-INFO: grad norm: 3.407 3.369 0.506
2024-12-02-12:06:33-root-INFO: grad norm: 3.273 3.239 0.467
2024-12-02-12:06:33-root-INFO: grad norm: 3.116 3.089 0.410
2024-12-02-12:06:34-root-INFO: grad norm: 3.047 3.019 0.408
2024-12-02-12:06:34-root-INFO: grad norm: 3.001 2.977 0.380
2024-12-02-12:06:35-root-INFO: Loss Change: 20.096 -> 18.691
2024-12-02-12:06:35-root-INFO: Regularization Change: 0.000 -> 3.605
2024-12-02-12:06:35-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-12:06:35-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-12:06:35-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-12:06:35-root-INFO: grad norm: 2.848 2.832 0.300
2024-12-02-12:06:36-root-INFO: grad norm: 2.698 2.681 0.295
2024-12-02-12:06:36-root-INFO: grad norm: 2.544 2.530 0.265
2024-12-02-12:06:37-root-INFO: grad norm: 2.581 2.568 0.260
2024-12-02-12:06:37-root-INFO: grad norm: 2.658 2.642 0.294
2024-12-02-12:06:37-root-INFO: Loss Change: 18.246 -> 17.152
2024-12-02-12:06:37-root-INFO: Regularization Change: 0.000 -> 3.140
2024-12-02-12:06:37-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-12:06:37-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-12:06:37-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-12:06:38-root-INFO: grad norm: 3.026 2.995 0.435
2024-12-02-12:06:38-root-INFO: grad norm: 2.934 2.906 0.402
2024-12-02-12:06:39-root-INFO: grad norm: 2.806 2.782 0.367
2024-12-02-12:06:39-root-INFO: grad norm: 2.738 2.715 0.358
2024-12-02-12:06:39-root-INFO: grad norm: 2.703 2.682 0.342
2024-12-02-12:06:40-root-INFO: Loss Change: 17.156 -> 15.960
2024-12-02-12:06:40-root-INFO: Regularization Change: 0.000 -> 3.158
2024-12-02-12:06:40-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-12:06:40-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-12:06:40-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-12:06:40-root-INFO: grad norm: 2.597 2.583 0.271
2024-12-02-12:06:40-root-INFO: grad norm: 2.468 2.452 0.276
2024-12-02-12:06:41-root-INFO: grad norm: 2.328 2.316 0.236
2024-12-02-12:06:41-root-INFO: grad norm: 2.367 2.355 0.244
2024-12-02-12:06:42-root-INFO: grad norm: 2.453 2.439 0.263
2024-12-02-12:06:42-root-INFO: Loss Change: 15.681 -> 14.765
2024-12-02-12:06:42-root-INFO: Regularization Change: 0.000 -> 2.836
2024-12-02-12:06:42-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-12:06:42-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-12:06:42-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-12:06:43-root-INFO: grad norm: 2.802 2.772 0.407
2024-12-02-12:06:43-root-INFO: grad norm: 2.666 2.643 0.352
2024-12-02-12:06:43-root-INFO: grad norm: 2.539 2.519 0.322
2024-12-02-12:06:44-root-INFO: grad norm: 2.484 2.465 0.310
2024-12-02-12:06:44-root-INFO: grad norm: 2.453 2.435 0.300
2024-12-02-12:06:45-root-INFO: Loss Change: 14.631 -> 13.523
2024-12-02-12:06:45-root-INFO: Regularization Change: 0.000 -> 2.886
2024-12-02-12:06:45-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-12:06:45-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-12:06:45-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-12:06:45-root-INFO: grad norm: 2.497 2.484 0.262
2024-12-02-12:06:46-root-INFO: grad norm: 2.300 2.285 0.263
2024-12-02-12:06:46-root-INFO: grad norm: 2.076 2.067 0.195
2024-12-02-12:06:47-root-INFO: grad norm: 2.108 2.099 0.199
2024-12-02-12:06:47-root-INFO: grad norm: 2.209 2.198 0.219
2024-12-02-12:06:47-root-INFO: Loss Change: 13.380 -> 12.495
2024-12-02-12:06:47-root-INFO: Regularization Change: 0.000 -> 2.608
2024-12-02-12:06:47-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-12:06:47-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-12:06:47-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-12:06:48-root-INFO: grad norm: 2.562 2.537 0.361
2024-12-02-12:06:48-root-INFO: grad norm: 2.461 2.441 0.310
2024-12-02-12:06:49-root-INFO: grad norm: 2.358 2.340 0.288
2024-12-02-12:06:49-root-INFO: grad norm: 2.311 2.295 0.272
2024-12-02-12:06:50-root-INFO: grad norm: 2.288 2.272 0.267
2024-12-02-12:06:50-root-INFO: Loss Change: 12.374 -> 11.420
2024-12-02-12:06:50-root-INFO: Regularization Change: 0.000 -> 2.638
2024-12-02-12:06:50-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-12:06:50-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-12:06:50-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-12:06:50-root-INFO: grad norm: 2.338 2.327 0.235
2024-12-02-12:06:51-root-INFO: grad norm: 2.165 2.151 0.242
2024-12-02-12:06:51-root-INFO: grad norm: 1.968 1.960 0.176
2024-12-02-12:06:52-root-INFO: grad norm: 1.973 1.964 0.189
2024-12-02-12:06:52-root-INFO: grad norm: 2.035 2.026 0.190
2024-12-02-12:06:52-root-INFO: Loss Change: 11.267 -> 10.448
2024-12-02-12:06:52-root-INFO: Regularization Change: 0.000 -> 2.400
2024-12-02-12:06:52-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-12:06:52-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-12:06:52-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-12:06:53-root-INFO: grad norm: 2.342 2.321 0.312
2024-12-02-12:06:53-root-INFO: grad norm: 2.245 2.230 0.266
2024-12-02-12:06:54-root-INFO: grad norm: 2.142 2.128 0.252
2024-12-02-12:06:54-root-INFO: grad norm: 2.106 2.093 0.233
2024-12-02-12:06:54-root-INFO: grad norm: 2.096 2.083 0.235
2024-12-02-12:06:55-root-INFO: Loss Change: 10.378 -> 9.542
2024-12-02-12:06:55-root-INFO: Regularization Change: 0.000 -> 2.398
2024-12-02-12:06:55-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-12:06:55-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-12:06:55-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-12:06:55-root-INFO: grad norm: 2.214 2.202 0.234
2024-12-02-12:06:56-root-INFO: grad norm: 2.030 2.017 0.229
2024-12-02-12:06:56-root-INFO: grad norm: 1.850 1.843 0.161
2024-12-02-12:06:57-root-INFO: grad norm: 1.854 1.845 0.181
2024-12-02-12:06:57-root-INFO: grad norm: 1.906 1.899 0.166
2024-12-02-12:06:57-root-INFO: Loss Change: 9.418 -> 8.663
2024-12-02-12:06:57-root-INFO: Regularization Change: 0.000 -> 2.236
2024-12-02-12:06:57-root-INFO: Undo step: 20
2024-12-02-12:06:57-root-INFO: Undo step: 21
2024-12-02-12:06:57-root-INFO: Undo step: 22
2024-12-02-12:06:57-root-INFO: Undo step: 23
2024-12-02-12:06:57-root-INFO: Undo step: 24
2024-12-02-12:06:57-root-INFO: Undo step: 25
2024-12-02-12:06:57-root-INFO: Undo step: 26
2024-12-02-12:06:57-root-INFO: Undo step: 27
2024-12-02-12:06:57-root-INFO: Undo step: 28
2024-12-02-12:06:57-root-INFO: Undo step: 29
2024-12-02-12:06:58-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-12:06:58-root-INFO: grad norm: 19.138 19.047 1.864
2024-12-02-12:06:58-root-INFO: grad norm: 8.680 8.606 1.137
2024-12-02-12:06:59-root-INFO: grad norm: 6.286 6.241 0.755
2024-12-02-12:06:59-root-INFO: grad norm: 5.421 5.378 0.677
2024-12-02-12:07:00-root-INFO: grad norm: 5.095 5.067 0.531
2024-12-02-12:07:00-root-INFO: Loss Change: 158.601 -> 40.286
2024-12-02-12:07:00-root-INFO: Regularization Change: 0.000 -> 163.774
2024-12-02-12:07:00-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-12:07:00-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-12:07:00-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-12:07:00-root-INFO: grad norm: 4.891 4.865 0.503
2024-12-02-12:07:01-root-INFO: grad norm: 4.443 4.424 0.410
2024-12-02-12:07:01-root-INFO: grad norm: 4.075 4.052 0.431
2024-12-02-12:07:02-root-INFO: grad norm: 3.837 3.820 0.351
2024-12-02-12:07:02-root-INFO: grad norm: 3.684 3.662 0.400
2024-12-02-12:07:02-root-INFO: Loss Change: 39.782 -> 27.733
2024-12-02-12:07:02-root-INFO: Regularization Change: 0.000 -> 24.199
2024-12-02-12:07:02-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-12:07:02-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-12:07:02-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-12:07:03-root-INFO: grad norm: 4.032 4.001 0.495
2024-12-02-12:07:03-root-INFO: grad norm: 3.783 3.748 0.516
2024-12-02-12:07:04-root-INFO: grad norm: 3.542 3.516 0.429
2024-12-02-12:07:04-root-INFO: grad norm: 3.459 3.428 0.462
2024-12-02-12:07:05-root-INFO: grad norm: 3.524 3.499 0.418
2024-12-02-12:07:05-root-INFO: Loss Change: 27.452 -> 22.131
2024-12-02-12:07:05-root-INFO: Regularization Change: 0.000 -> 12.038
2024-12-02-12:07:05-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-12:07:05-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-12:07:05-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-12:07:05-root-INFO: grad norm: 3.165 3.149 0.317
2024-12-02-12:07:06-root-INFO: grad norm: 2.972 2.958 0.288
2024-12-02-12:07:06-root-INFO: grad norm: 2.948 2.929 0.331
2024-12-02-12:07:07-root-INFO: grad norm: 3.110 3.094 0.317
2024-12-02-12:07:07-root-INFO: grad norm: 3.092 3.070 0.365
2024-12-02-12:07:07-root-INFO: Loss Change: 21.613 -> 18.437
2024-12-02-12:07:07-root-INFO: Regularization Change: 0.000 -> 7.611
2024-12-02-12:07:07-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-12:07:07-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-12:07:07-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-12:07:08-root-INFO: grad norm: 3.591 3.551 0.533
2024-12-02-12:07:08-root-INFO: grad norm: 3.347 3.307 0.515
2024-12-02-12:07:09-root-INFO: grad norm: 3.325 3.295 0.443
2024-12-02-12:07:09-root-INFO: grad norm: 3.171 3.136 0.466
2024-12-02-12:07:09-root-INFO: grad norm: 3.039 3.011 0.412
2024-12-02-12:07:10-root-INFO: Loss Change: 18.514 -> 15.928
2024-12-02-12:07:10-root-INFO: Regularization Change: 0.000 -> 6.021
2024-12-02-12:07:10-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-12:07:10-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-12:07:10-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-12:07:10-root-INFO: grad norm: 2.501 2.485 0.282
2024-12-02-12:07:11-root-INFO: grad norm: 2.507 2.491 0.285
2024-12-02-12:07:11-root-INFO: grad norm: 2.543 2.519 0.345
2024-12-02-12:07:12-root-INFO: grad norm: 2.647 2.625 0.334
2024-12-02-12:07:12-root-INFO: grad norm: 2.718 2.690 0.394
2024-12-02-12:07:12-root-INFO: Loss Change: 15.256 -> 13.785
2024-12-02-12:07:12-root-INFO: Regularization Change: 0.000 -> 4.256
2024-12-02-12:07:12-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-12:07:12-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-12:07:13-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-12:07:13-root-INFO: grad norm: 3.600 3.551 0.589
2024-12-02-12:07:13-root-INFO: grad norm: 3.357 3.310 0.560
2024-12-02-12:07:14-root-INFO: grad norm: 3.009 2.977 0.437
2024-12-02-12:07:14-root-INFO: grad norm: 2.891 2.855 0.454
2024-12-02-12:07:15-root-INFO: grad norm: 2.951 2.921 0.417
2024-12-02-12:07:15-root-INFO: Loss Change: 14.068 -> 12.581
2024-12-02-12:07:15-root-INFO: Regularization Change: 0.000 -> 4.387
2024-12-02-12:07:15-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-12:07:15-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-12:07:15-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-12:07:15-root-INFO: grad norm: 2.431 2.414 0.290
2024-12-02-12:07:16-root-INFO: grad norm: 2.203 2.188 0.256
2024-12-02-12:07:16-root-INFO: grad norm: 2.142 2.121 0.305
2024-12-02-12:07:17-root-INFO: grad norm: 2.305 2.286 0.292
2024-12-02-12:07:17-root-INFO: Loss too large (11.044->11.057)! Learning rate decreased to 0.33923.
2024-12-02-12:07:17-root-INFO: grad norm: 1.643 1.626 0.236
2024-12-02-12:07:18-root-INFO: Loss Change: 11.995 -> 10.402
2024-12-02-12:07:18-root-INFO: Regularization Change: 0.000 -> 2.398
2024-12-02-12:07:18-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-12:07:18-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-12:07:18-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-12:07:18-root-INFO: grad norm: 1.860 1.829 0.335
2024-12-02-12:07:18-root-INFO: grad norm: 1.835 1.808 0.317
2024-12-02-12:07:19-root-INFO: grad norm: 2.385 2.363 0.320
2024-12-02-12:07:19-root-INFO: Loss too large (10.081->10.208)! Learning rate decreased to 0.34461.
2024-12-02-12:07:20-root-INFO: grad norm: 1.633 1.616 0.235
2024-12-02-12:07:20-root-INFO: grad norm: 1.098 1.090 0.134
2024-12-02-12:07:21-root-INFO: Loss Change: 10.412 -> 9.395
2024-12-02-12:07:21-root-INFO: Regularization Change: 0.000 -> 1.724
2024-12-02-12:07:21-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-12:07:21-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-12:07:21-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-12:07:21-root-INFO: grad norm: 1.044 1.030 0.168
2024-12-02-12:07:21-root-INFO: grad norm: 0.930 0.923 0.112
2024-12-02-12:07:22-root-INFO: grad norm: 0.977 0.972 0.101
2024-12-02-12:07:22-root-INFO: grad norm: 1.231 1.227 0.099
2024-12-02-12:07:23-root-INFO: grad norm: 1.229 1.223 0.124
2024-12-02-12:07:23-root-INFO: Loss Change: 9.188 -> 8.415
2024-12-02-12:07:23-root-INFO: Regularization Change: 0.000 -> 1.854
2024-12-02-12:07:23-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-12:07:23-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-12:07:23-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-12:07:23-root-INFO: grad norm: 1.889 1.862 0.320
2024-12-02-12:07:24-root-INFO: Loss too large (8.392->8.400)! Learning rate decreased to 0.35546.
2024-12-02-12:07:24-root-INFO: grad norm: 1.391 1.376 0.203
2024-12-02-12:07:25-root-INFO: grad norm: 1.061 1.054 0.122
2024-12-02-12:07:25-root-INFO: grad norm: 0.927 0.919 0.118
2024-12-02-12:07:25-root-INFO: grad norm: 0.914 0.909 0.096
2024-12-02-12:07:26-root-INFO: Loss Change: 8.392 -> 7.622
2024-12-02-12:07:26-root-INFO: Regularization Change: 0.000 -> 1.185
2024-12-02-12:07:26-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-12:07:26-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-12:07:26-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-12:07:26-root-INFO: grad norm: 1.103 1.090 0.169
2024-12-02-12:07:26-root-INFO: grad norm: 1.390 1.385 0.114
2024-12-02-12:07:27-root-INFO: Loss too large (7.311->7.316)! Learning rate decreased to 0.36092.
2024-12-02-12:07:27-root-INFO: grad norm: 0.988 0.984 0.079
2024-12-02-12:07:28-root-INFO: grad norm: 0.828 0.826 0.061
2024-12-02-12:07:28-root-INFO: grad norm: 0.783 0.780 0.066
2024-12-02-12:07:28-root-INFO: Loss Change: 7.474 -> 6.863
2024-12-02-12:07:28-root-INFO: Regularization Change: 0.000 -> 1.079
2024-12-02-12:07:28-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-12:07:28-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-12:07:28-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-12:07:29-root-INFO: grad norm: 1.433 1.409 0.263
2024-12-02-12:07:29-root-INFO: grad norm: 1.472 1.455 0.227
2024-12-02-12:07:30-root-INFO: grad norm: 2.385 2.370 0.260
2024-12-02-12:07:30-root-INFO: Loss too large (6.743->6.929)! Learning rate decreased to 0.36641.
2024-12-02-12:07:30-root-INFO: grad norm: 1.474 1.462 0.189
2024-12-02-12:07:31-root-INFO: grad norm: 1.051 1.044 0.119
2024-12-02-12:07:31-root-INFO: Loss Change: 6.864 -> 6.301
2024-12-02-12:07:31-root-INFO: Regularization Change: 0.000 -> 1.322
2024-12-02-12:07:31-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-12:07:31-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-12:07:31-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-12:07:31-root-INFO: grad norm: 0.999 0.988 0.148
2024-12-02-12:07:32-root-INFO: grad norm: 0.878 0.874 0.090
2024-12-02-12:07:32-root-INFO: grad norm: 0.942 0.939 0.074
2024-12-02-12:07:33-root-INFO: grad norm: 1.168 1.165 0.078
2024-12-02-12:07:33-root-INFO: grad norm: 1.231 1.228 0.098
2024-12-02-12:07:34-root-INFO: Loss Change: 6.192 -> 5.675
2024-12-02-12:07:34-root-INFO: Regularization Change: 0.000 -> 1.459
2024-12-02-12:07:34-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-12:07:34-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-12:07:34-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-12:07:34-root-INFO: grad norm: 1.864 1.840 0.295
2024-12-02-12:07:34-root-INFO: grad norm: 1.838 1.818 0.265
2024-12-02-12:07:35-root-INFO: grad norm: 2.121 2.106 0.257
2024-12-02-12:07:35-root-INFO: Loss too large (5.571->5.697)! Learning rate decreased to 0.37747.
2024-12-02-12:07:36-root-INFO: grad norm: 1.404 1.391 0.190
2024-12-02-12:07:36-root-INFO: grad norm: 0.962 0.957 0.100
2024-12-02-12:07:36-root-INFO: Loss Change: 5.753 -> 5.072
2024-12-02-12:07:36-root-INFO: Regularization Change: 0.000 -> 1.141
2024-12-02-12:07:36-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-12:07:36-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-12:07:36-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-12:07:37-root-INFO: grad norm: 1.028 1.014 0.167
2024-12-02-12:07:37-root-INFO: grad norm: 0.795 0.789 0.101
2024-12-02-12:07:38-root-INFO: grad norm: 0.773 0.769 0.071
2024-12-02-12:07:38-root-INFO: grad norm: 0.874 0.871 0.071
2024-12-02-12:07:38-root-INFO: grad norm: 1.170 1.168 0.072
2024-12-02-12:07:39-root-INFO: Loss Change: 5.025 -> 4.625
2024-12-02-12:07:39-root-INFO: Regularization Change: 0.000 -> 1.225
2024-12-02-12:07:39-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-12:07:39-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-12:07:39-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-12:07:39-root-INFO: grad norm: 1.420 1.407 0.192
2024-12-02-12:07:39-root-INFO: grad norm: 1.282 1.273 0.151
2024-12-02-12:07:40-root-INFO: grad norm: 1.261 1.252 0.146
2024-12-02-12:07:40-root-INFO: grad norm: 1.335 1.326 0.159
2024-12-02-12:07:41-root-INFO: grad norm: 1.981 1.972 0.180
2024-12-02-12:07:41-root-INFO: Loss too large (4.312->4.390)! Learning rate decreased to 0.38861.
2024-12-02-12:07:41-root-INFO: Loss Change: 4.638 -> 4.223
2024-12-02-12:07:41-root-INFO: Regularization Change: 0.000 -> 1.167
2024-12-02-12:07:41-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-12:07:41-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-12:07:42-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-12:07:42-root-INFO: grad norm: 1.162 1.154 0.133
2024-12-02-12:07:42-root-INFO: grad norm: 0.981 0.978 0.075
2024-12-02-12:07:43-root-INFO: grad norm: 1.001 0.999 0.063
2024-12-02-12:07:43-root-INFO: grad norm: 1.040 1.039 0.063
2024-12-02-12:07:44-root-INFO: grad norm: 1.067 1.065 0.062
2024-12-02-12:07:44-root-INFO: Loss Change: 4.185 -> 3.713
2024-12-02-12:07:44-root-INFO: Regularization Change: 0.000 -> 1.323
2024-12-02-12:07:44-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-12:07:44-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-12:07:44-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-12:07:44-root-INFO: grad norm: 1.669 1.653 0.232
2024-12-02-12:07:45-root-INFO: grad norm: 1.397 1.387 0.166
2024-12-02-12:07:45-root-INFO: grad norm: 1.262 1.256 0.129
2024-12-02-12:07:46-root-INFO: grad norm: 1.165 1.158 0.134
2024-12-02-12:07:46-root-INFO: grad norm: 1.240 1.233 0.135
2024-12-02-12:07:46-root-INFO: Loss too large (3.401->3.434)! Learning rate decreased to 0.39982.
2024-12-02-12:07:46-root-INFO: Loss Change: 3.818 -> 3.325
2024-12-02-12:07:46-root-INFO: Regularization Change: 0.000 -> 1.083
2024-12-02-12:07:46-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-12:07:46-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-12:07:47-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-12:07:47-root-INFO: grad norm: 1.061 1.052 0.134
2024-12-02-12:07:47-root-INFO: grad norm: 0.931 0.928 0.071
2024-12-02-12:07:48-root-INFO: grad norm: 0.872 0.870 0.065
2024-12-02-12:07:48-root-INFO: grad norm: 0.808 0.806 0.049
2024-12-02-12:07:49-root-INFO: grad norm: 0.823 0.822 0.048
2024-12-02-12:07:49-root-INFO: Loss Change: 3.346 -> 2.988
2024-12-02-12:07:49-root-INFO: Regularization Change: 0.000 -> 0.978
2024-12-02-12:07:49-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-12:07:49-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-12:07:49-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-12:07:49-root-INFO: grad norm: 1.650 1.632 0.244
2024-12-02-12:07:50-root-INFO: grad norm: 1.199 1.190 0.152
2024-12-02-12:07:50-root-INFO: grad norm: 1.074 1.067 0.124
2024-12-02-12:07:51-root-INFO: grad norm: 1.036 1.029 0.126
2024-12-02-12:07:51-root-INFO: grad norm: 1.100 1.094 0.120
2024-12-02-12:07:51-root-INFO: Loss Change: 3.159 -> 2.697
2024-12-02-12:07:51-root-INFO: Regularization Change: 0.000 -> 1.158
2024-12-02-12:07:51-root-INFO: Undo step: 10
2024-12-02-12:07:51-root-INFO: Undo step: 11
2024-12-02-12:07:51-root-INFO: Undo step: 12
2024-12-02-12:07:51-root-INFO: Undo step: 13
2024-12-02-12:07:51-root-INFO: Undo step: 14
2024-12-02-12:07:51-root-INFO: Undo step: 15
2024-12-02-12:07:51-root-INFO: Undo step: 16
2024-12-02-12:07:51-root-INFO: Undo step: 17
2024-12-02-12:07:51-root-INFO: Undo step: 18
2024-12-02-12:07:51-root-INFO: Undo step: 19
2024-12-02-12:07:52-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-12:07:52-root-INFO: grad norm: 16.777 16.734 1.198
2024-12-02-12:07:52-root-INFO: grad norm: 7.937 7.899 0.774
2024-12-02-12:07:53-root-INFO: grad norm: 5.926 5.901 0.537
2024-12-02-12:07:53-root-INFO: grad norm: 4.892 4.871 0.454
2024-12-02-12:07:54-root-INFO: grad norm: 4.224 4.209 0.357
2024-12-02-12:07:54-root-INFO: Loss Change: 133.334 -> 25.826
2024-12-02-12:07:54-root-INFO: Regularization Change: 0.000 -> 169.330
2024-12-02-12:07:54-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-12:07:54-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-12:07:54-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-12:07:54-root-INFO: grad norm: 3.880 3.866 0.332
2024-12-02-12:07:55-root-INFO: grad norm: 3.363 3.350 0.292
2024-12-02-12:07:55-root-INFO: grad norm: 3.037 3.026 0.266
2024-12-02-12:07:56-root-INFO: grad norm: 2.902 2.892 0.244
2024-12-02-12:07:56-root-INFO: grad norm: 2.786 2.775 0.243
2024-12-02-12:07:56-root-INFO: Loss Change: 25.131 -> 15.540
2024-12-02-12:07:56-root-INFO: Regularization Change: 0.000 -> 22.127
2024-12-02-12:07:56-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-12:07:56-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-12:07:57-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-12:07:57-root-INFO: grad norm: 2.820 2.808 0.267
2024-12-02-12:07:57-root-INFO: grad norm: 2.610 2.598 0.252
2024-12-02-12:07:58-root-INFO: grad norm: 2.494 2.484 0.224
2024-12-02-12:07:58-root-INFO: grad norm: 2.466 2.455 0.225
2024-12-02-12:07:59-root-INFO: grad norm: 2.408 2.396 0.237
2024-12-02-12:07:59-root-INFO: Loss Change: 15.109 -> 11.115
2024-12-02-12:07:59-root-INFO: Regularization Change: 0.000 -> 9.896
2024-12-02-12:07:59-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-12:07:59-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-12:07:59-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-12:07:59-root-INFO: grad norm: 2.644 2.628 0.291
2024-12-02-12:08:00-root-INFO: grad norm: 2.436 2.420 0.276
2024-12-02-12:08:00-root-INFO: grad norm: 2.191 2.183 0.193
2024-12-02-12:08:01-root-INFO: grad norm: 2.139 2.127 0.226
2024-12-02-12:08:01-root-INFO: grad norm: 2.493 2.482 0.232
2024-12-02-12:08:02-root-INFO: Loss Change: 10.846 -> 8.705
2024-12-02-12:08:02-root-INFO: Regularization Change: 0.000 -> 6.245
2024-12-02-12:08:02-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-12:08:02-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-12:08:02-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-12:08:02-root-INFO: grad norm: 2.138 2.127 0.212
2024-12-02-12:08:03-root-INFO: grad norm: 1.914 1.908 0.160
2024-12-02-12:08:03-root-INFO: grad norm: 1.810 1.802 0.173
2024-12-02-12:08:03-root-INFO: grad norm: 1.793 1.787 0.155
2024-12-02-12:08:04-root-INFO: grad norm: 1.751 1.742 0.182
2024-12-02-12:08:04-root-INFO: Loss Change: 8.371 -> 6.741
2024-12-02-12:08:04-root-INFO: Regularization Change: 0.000 -> 4.030
2024-12-02-12:08:04-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-12:08:04-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-12:08:05-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-12:08:05-root-INFO: grad norm: 2.161 2.140 0.304
2024-12-02-12:08:05-root-INFO: grad norm: 1.961 1.943 0.264
2024-12-02-12:08:06-root-INFO: grad norm: 1.882 1.870 0.210
2024-12-02-12:08:06-root-INFO: grad norm: 1.770 1.756 0.224
2024-12-02-12:08:07-root-INFO: grad norm: 1.746 1.735 0.195
2024-12-02-12:08:07-root-INFO: Loss Change: 6.650 -> 5.565
2024-12-02-12:08:07-root-INFO: Regularization Change: 0.000 -> 3.010
2024-12-02-12:08:07-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-12:08:07-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-12:08:07-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-12:08:07-root-INFO: grad norm: 1.566 1.560 0.139
2024-12-02-12:08:08-root-INFO: grad norm: 1.390 1.386 0.109
2024-12-02-12:08:08-root-INFO: grad norm: 1.328 1.323 0.118
2024-12-02-12:08:09-root-INFO: grad norm: 1.304 1.299 0.107
2024-12-02-12:08:09-root-INFO: grad norm: 1.291 1.285 0.125
2024-12-02-12:08:09-root-INFO: Loss Change: 5.297 -> 4.505
2024-12-02-12:08:09-root-INFO: Regularization Change: 0.000 -> 2.068
2024-12-02-12:08:09-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-12:08:09-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-12:08:10-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-12:08:10-root-INFO: grad norm: 1.888 1.863 0.308
2024-12-02-12:08:10-root-INFO: grad norm: 1.726 1.709 0.245
2024-12-02-12:08:11-root-INFO: grad norm: 1.493 1.482 0.183
2024-12-02-12:08:11-root-INFO: grad norm: 1.406 1.392 0.202
2024-12-02-12:08:12-root-INFO: grad norm: 1.801 1.789 0.209
2024-12-02-12:08:12-root-INFO: Loss too large (4.021->4.061)! Learning rate decreased to 0.39420.
2024-12-02-12:08:12-root-INFO: Loss Change: 4.566 -> 3.908
2024-12-02-12:08:12-root-INFO: Regularization Change: 0.000 -> 1.656
2024-12-02-12:08:12-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-12:08:12-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-12:08:12-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-12:08:12-root-INFO: grad norm: 1.003 0.998 0.098
2024-12-02-12:08:13-root-INFO: grad norm: 0.807 0.805 0.057
2024-12-02-12:08:13-root-INFO: grad norm: 0.811 0.810 0.052
2024-12-02-12:08:14-root-INFO: grad norm: 0.900 0.899 0.056
2024-12-02-12:08:14-root-INFO: grad norm: 0.960 0.957 0.074
2024-12-02-12:08:15-root-INFO: Loss Change: 3.794 -> 3.342
2024-12-02-12:08:15-root-INFO: Regularization Change: 0.000 -> 1.342
2024-12-02-12:08:15-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-12:08:15-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-12:08:15-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-12:08:15-root-INFO: grad norm: 1.972 1.950 0.291
2024-12-02-12:08:15-root-INFO: grad norm: 1.423 1.409 0.203
2024-12-02-12:08:16-root-INFO: grad norm: 1.284 1.273 0.166
2024-12-02-12:08:16-root-INFO: grad norm: 1.228 1.216 0.172
2024-12-02-12:08:17-root-INFO: grad norm: 1.182 1.173 0.150
2024-12-02-12:08:17-root-INFO: Loss Change: 3.489 -> 2.998
2024-12-02-12:08:17-root-INFO: Regularization Change: 0.000 -> 1.310
2024-12-02-12:08:17-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-12:08:17-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-12:08:17-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-12:08:17-root-INFO: grad norm: 1.042 1.038 0.094
2024-12-02-12:08:18-root-INFO: grad norm: 0.786 0.784 0.048
2024-12-02-12:08:18-root-INFO: grad norm: 0.723 0.721 0.046
2024-12-02-12:08:19-root-INFO: grad norm: 0.777 0.776 0.049
2024-12-02-12:08:19-root-INFO: Loss too large (2.632->2.647)! Learning rate decreased to 0.41109.
2024-12-02-12:08:19-root-INFO: grad norm: 0.883 0.882 0.042
2024-12-02-12:08:20-root-INFO: Loss Change: 2.940 -> 2.584
2024-12-02-12:08:20-root-INFO: Regularization Change: 0.000 -> 0.764
2024-12-02-12:08:20-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-12:08:20-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-12:08:20-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-12:08:20-root-INFO: grad norm: 1.107 1.091 0.187
2024-12-02-12:08:21-root-INFO: grad norm: 0.821 0.812 0.117
2024-12-02-12:08:21-root-INFO: grad norm: 0.766 0.760 0.088
2024-12-02-12:08:22-root-INFO: grad norm: 1.007 1.004 0.077
2024-12-02-12:08:22-root-INFO: Loss too large (2.422->2.431)! Learning rate decreased to 0.41675.
2024-12-02-12:08:22-root-INFO: grad norm: 0.623 0.620 0.056
2024-12-02-12:08:22-root-INFO: Loss Change: 2.700 -> 2.278
2024-12-02-12:08:22-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-02-12:08:22-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-12:08:22-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-12:08:23-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-12:08:23-root-INFO: grad norm: 0.993 0.976 0.184
2024-12-02-12:08:23-root-INFO: grad norm: 0.786 0.778 0.113
2024-12-02-12:08:23-root-INFO: Loss too large (2.274->2.290)! Learning rate decreased to 0.42241.
2024-12-02-12:08:24-root-INFO: grad norm: 0.857 0.855 0.066
2024-12-02-12:08:24-root-INFO: grad norm: 0.626 0.622 0.067
2024-12-02-12:08:25-root-INFO: grad norm: 0.429 0.427 0.041
2024-12-02-12:08:25-root-INFO: Loss Change: 2.418 -> 2.096
2024-12-02-12:08:25-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-02-12:08:25-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-12:08:25-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-12:08:25-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-12:08:25-root-INFO: grad norm: 0.923 0.909 0.160
2024-12-02-12:08:26-root-INFO: grad norm: 0.651 0.647 0.071
2024-12-02-12:08:26-root-INFO: grad norm: 0.736 0.733 0.065
2024-12-02-12:08:27-root-INFO: Loss too large (2.058->2.068)! Learning rate decreased to 0.42809.
2024-12-02-12:08:27-root-INFO: grad norm: 0.692 0.691 0.045
2024-12-02-12:08:27-root-INFO: grad norm: 0.690 0.687 0.066
2024-12-02-12:08:28-root-INFO: Loss Change: 2.241 -> 1.972
2024-12-02-12:08:28-root-INFO: Regularization Change: 0.000 -> 0.585
2024-12-02-12:08:28-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-12:08:28-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-12:08:28-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-12:08:28-root-INFO: grad norm: 1.173 1.157 0.194
2024-12-02-12:08:28-root-INFO: grad norm: 0.862 0.854 0.117
2024-12-02-12:08:29-root-INFO: grad norm: 0.706 0.702 0.075
2024-12-02-12:08:29-root-INFO: grad norm: 0.773 0.767 0.093
2024-12-02-12:08:30-root-INFO: grad norm: 0.808 0.805 0.075
2024-12-02-12:08:30-root-INFO: Loss Change: 2.168 -> 1.847
2024-12-02-12:08:30-root-INFO: Regularization Change: 0.000 -> 0.805
2024-12-02-12:08:30-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-12:08:30-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-12:08:30-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-12:08:31-root-INFO: grad norm: 1.009 1.002 0.112
2024-12-02-12:08:31-root-INFO: grad norm: 0.716 0.714 0.052
2024-12-02-12:08:32-root-INFO: grad norm: 0.600 0.597 0.060
2024-12-02-12:08:32-root-INFO: grad norm: 0.575 0.574 0.047
2024-12-02-12:08:33-root-INFO: grad norm: 0.530 0.526 0.057
2024-12-02-12:08:33-root-INFO: Loss Change: 1.964 -> 1.668
2024-12-02-12:08:33-root-INFO: Regularization Change: 0.000 -> 0.673
2024-12-02-12:08:33-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-12:08:33-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-12:08:33-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-12:08:33-root-INFO: grad norm: 1.007 0.994 0.156
2024-12-02-12:08:34-root-INFO: grad norm: 0.604 0.599 0.078
2024-12-02-12:08:34-root-INFO: grad norm: 0.454 0.452 0.046
2024-12-02-12:08:35-root-INFO: grad norm: 0.371 0.368 0.048
2024-12-02-12:08:35-root-INFO: grad norm: 0.337 0.335 0.038
2024-12-02-12:08:35-root-INFO: Loss Change: 1.855 -> 1.524
2024-12-02-12:08:35-root-INFO: Regularization Change: 0.000 -> 0.627
2024-12-02-12:08:35-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-12:08:35-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-12:08:36-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-12:08:36-root-INFO: grad norm: 0.741 0.735 0.092
2024-12-02-12:08:36-root-INFO: grad norm: 0.349 0.346 0.048
2024-12-02-12:08:37-root-INFO: grad norm: 0.283 0.280 0.041
2024-12-02-12:08:37-root-INFO: grad norm: 0.271 0.268 0.039
2024-12-02-12:08:38-root-INFO: grad norm: 0.268 0.265 0.039
2024-12-02-12:08:38-root-INFO: Loss Change: 1.680 -> 1.421
2024-12-02-12:08:38-root-INFO: Regularization Change: 0.000 -> 0.546
2024-12-02-12:08:38-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-12:08:38-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-12:08:38-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-12:08:38-root-INFO: grad norm: 0.732 0.727 0.088
2024-12-02-12:08:39-root-INFO: grad norm: 0.375 0.370 0.057
2024-12-02-12:08:39-root-INFO: grad norm: 0.280 0.277 0.038
2024-12-02-12:08:40-root-INFO: grad norm: 0.237 0.233 0.042
2024-12-02-12:08:40-root-INFO: grad norm: 0.218 0.215 0.034
2024-12-02-12:08:41-root-INFO: Loss Change: 1.577 -> 1.331
2024-12-02-12:08:41-root-INFO: Regularization Change: 0.000 -> 0.516
2024-12-02-12:08:41-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-12:08:41-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-12:08:41-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-12:08:41-root-INFO: grad norm: 0.683 0.679 0.076
2024-12-02-12:08:42-root-INFO: grad norm: 0.339 0.336 0.044
2024-12-02-12:08:42-root-INFO: grad norm: 0.337 0.335 0.034
2024-12-02-12:08:43-root-INFO: grad norm: 0.239 0.236 0.036
2024-12-02-12:08:43-root-INFO: grad norm: 0.223 0.221 0.030
2024-12-02-12:08:43-root-INFO: Loss Change: 1.462 -> 1.221
2024-12-02-12:08:43-root-INFO: Regularization Change: 0.000 -> 0.541
2024-12-02-12:08:43-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-12:08:43-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-12:08:44-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-12:08:44-root-INFO: grad norm: 0.775 0.771 0.075
2024-12-02-12:08:44-root-INFO: grad norm: 0.427 0.426 0.028
2024-12-02-12:08:45-root-INFO: grad norm: 0.348 0.344 0.048
2024-12-02-12:08:45-root-INFO: grad norm: 0.320 0.315 0.055
2024-12-02-12:08:46-root-INFO: grad norm: 0.307 0.302 0.055
2024-12-02-12:08:46-root-INFO: Loss Change: 1.359 -> 0.864
2024-12-02-12:08:46-root-INFO: Regularization Change: 0.000 -> 1.256
2024-12-02-12:08:46-root-INFO: Undo step: 0
2024-12-02-12:08:46-root-INFO: Undo step: 1
2024-12-02-12:08:46-root-INFO: Undo step: 2
2024-12-02-12:08:46-root-INFO: Undo step: 3
2024-12-02-12:08:46-root-INFO: Undo step: 4
2024-12-02-12:08:46-root-INFO: Undo step: 5
2024-12-02-12:08:46-root-INFO: Undo step: 6
2024-12-02-12:08:46-root-INFO: Undo step: 7
2024-12-02-12:08:46-root-INFO: Undo step: 8
2024-12-02-12:08:46-root-INFO: Undo step: 9
2024-12-02-12:08:46-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-12:08:46-root-INFO: grad norm: 15.914 15.890 0.869
2024-12-02-12:08:47-root-INFO: grad norm: 6.650 6.634 0.458
2024-12-02-12:08:47-root-INFO: grad norm: 4.141 4.131 0.277
2024-12-02-12:08:48-root-INFO: grad norm: 2.754 2.744 0.237
2024-12-02-12:08:48-root-INFO: grad norm: 2.390 2.384 0.171
2024-12-02-12:08:49-root-INFO: Loss Change: 93.092 -> 11.037
2024-12-02-12:08:49-root-INFO: Regularization Change: 0.000 -> 143.822
2024-12-02-12:08:49-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-12:08:49-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-12:08:49-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-12:08:49-root-INFO: grad norm: 2.190 2.185 0.143
2024-12-02-12:08:50-root-INFO: grad norm: 1.879 1.873 0.140
2024-12-02-12:08:50-root-INFO: grad norm: 1.708 1.705 0.105
2024-12-02-12:08:51-root-INFO: grad norm: 1.579 1.575 0.110
2024-12-02-12:08:51-root-INFO: grad norm: 1.461 1.459 0.086
2024-12-02-12:08:51-root-INFO: Loss Change: 10.562 -> 5.832
2024-12-02-12:08:51-root-INFO: Regularization Change: 0.000 -> 12.241
2024-12-02-12:08:51-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-12:08:51-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-12:08:52-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-12:08:52-root-INFO: grad norm: 1.508 1.504 0.112
2024-12-02-12:08:52-root-INFO: grad norm: 1.255 1.252 0.084
2024-12-02-12:08:53-root-INFO: grad norm: 1.137 1.136 0.063
2024-12-02-12:08:53-root-INFO: grad norm: 1.055 1.053 0.063
2024-12-02-12:08:54-root-INFO: grad norm: 1.002 1.000 0.060
2024-12-02-12:08:54-root-INFO: Loss Change: 5.586 -> 3.819
2024-12-02-12:08:54-root-INFO: Regularization Change: 0.000 -> 4.696
2024-12-02-12:08:54-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-12:08:54-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-12:08:54-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-12:08:54-root-INFO: grad norm: 1.401 1.389 0.180
2024-12-02-12:08:55-root-INFO: grad norm: 1.015 1.009 0.109
2024-12-02-12:08:55-root-INFO: grad norm: 0.897 0.894 0.069
2024-12-02-12:08:56-root-INFO: grad norm: 0.809 0.806 0.065
2024-12-02-12:08:56-root-INFO: grad norm: 0.850 0.849 0.051
2024-12-02-12:08:56-root-INFO: Loss Change: 3.757 -> 2.840
2024-12-02-12:08:56-root-INFO: Regularization Change: 0.000 -> 2.402
2024-12-02-12:08:56-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-12:08:56-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-12:08:57-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-12:08:57-root-INFO: grad norm: 1.243 1.234 0.144
2024-12-02-12:08:57-root-INFO: grad norm: 0.783 0.779 0.076
2024-12-02-12:08:58-root-INFO: grad norm: 0.609 0.608 0.038
2024-12-02-12:08:58-root-INFO: grad norm: 0.619 0.618 0.035
2024-12-02-12:08:59-root-INFO: grad norm: 0.745 0.743 0.055
2024-12-02-12:08:59-root-INFO: Loss Change: 2.848 -> 2.232
2024-12-02-12:08:59-root-INFO: Regularization Change: 0.000 -> 1.460
2024-12-02-12:08:59-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-12:08:59-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-12:08:59-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-12:08:59-root-INFO: grad norm: 1.297 1.285 0.178
2024-12-02-12:09:00-root-INFO: grad norm: 0.827 0.821 0.097
2024-12-02-12:09:00-root-INFO: grad norm: 0.727 0.725 0.052
2024-12-02-12:09:01-root-INFO: grad norm: 0.701 0.698 0.071
2024-12-02-12:09:01-root-INFO: grad norm: 0.757 0.756 0.051
2024-12-02-12:09:02-root-INFO: Loss Change: 2.345 -> 1.851
2024-12-02-12:09:02-root-INFO: Regularization Change: 0.000 -> 1.127
2024-12-02-12:09:02-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-12:09:02-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-12:09:02-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-12:09:02-root-INFO: grad norm: 0.902 0.898 0.084
2024-12-02-12:09:02-root-INFO: grad norm: 0.602 0.601 0.037
2024-12-02-12:09:03-root-INFO: grad norm: 0.414 0.413 0.036
2024-12-02-12:09:03-root-INFO: grad norm: 0.367 0.365 0.030
2024-12-02-12:09:04-root-INFO: grad norm: 0.329 0.327 0.033
2024-12-02-12:09:04-root-INFO: Loss Change: 1.924 -> 1.591
2024-12-02-12:09:04-root-INFO: Regularization Change: 0.000 -> 0.737
2024-12-02-12:09:04-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-12:09:04-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-12:09:04-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-12:09:04-root-INFO: grad norm: 0.808 0.800 0.118
2024-12-02-12:09:05-root-INFO: grad norm: 0.459 0.455 0.062
2024-12-02-12:09:05-root-INFO: grad norm: 0.406 0.403 0.053
2024-12-02-12:09:06-root-INFO: grad norm: 0.374 0.370 0.055
2024-12-02-12:09:06-root-INFO: grad norm: 0.353 0.350 0.048
2024-12-02-12:09:07-root-INFO: Loss Change: 1.730 -> 1.446
2024-12-02-12:09:07-root-INFO: Regularization Change: 0.000 -> 0.637
2024-12-02-12:09:07-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-12:09:07-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-12:09:07-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-12:09:07-root-INFO: grad norm: 0.676 0.672 0.071
2024-12-02-12:09:07-root-INFO: grad norm: 0.328 0.325 0.047
2024-12-02-12:09:08-root-INFO: grad norm: 0.456 0.454 0.039
2024-12-02-12:09:08-root-INFO: grad norm: 0.275 0.272 0.039
2024-12-02-12:09:09-root-INFO: grad norm: 0.269 0.266 0.042
2024-12-02-12:09:09-root-INFO: Loss Change: 1.559 -> 1.327
2024-12-02-12:09:09-root-INFO: Regularization Change: 0.000 -> 0.534
2024-12-02-12:09:09-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-12:09:09-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-12:09:09-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-12:09:09-root-INFO: grad norm: 0.765 0.756 0.117
2024-12-02-12:09:10-root-INFO: grad norm: 0.407 0.403 0.055
2024-12-02-12:09:10-root-INFO: grad norm: 0.311 0.309 0.035
2024-12-02-12:09:11-root-INFO: grad norm: 0.262 0.260 0.037
2024-12-02-12:09:11-root-INFO: grad norm: 0.225 0.223 0.030
2024-12-02-12:09:11-root-INFO: Loss Change: 1.458 -> 1.210
2024-12-02-12:09:11-root-INFO: Regularization Change: 0.000 -> 0.524
2024-12-02-12:09:11-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-12:09:11-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-12:09:11-root-INFO: loss_sample0_0: 1.209871768951416
2024-12-02-12:09:12-root-INFO: It takes 1497.585 seconds for image sample0
2024-12-02-12:09:12-root-INFO: lpips_score_sample0: 0.149
2024-12-02-12:09:12-root-INFO: psnr_score_sample0: 15.758
2024-12-02-12:09:12-root-INFO: ssim_score_sample0: 0.710
2024-12-02-12:09:12-root-INFO: mean_lpips: 0.14906246960163116
2024-12-02-12:09:12-root-INFO: best_mean_lpips: 0.14906246960163116
2024-12-02-12:09:12-root-INFO: mean_psnr: 15.757664680480957
2024-12-02-12:09:12-root-INFO: best_mean_psnr: 15.757664680480957
2024-12-02-12:09:12-root-INFO: mean_ssim: 0.7099694013595581
2024-12-02-12:09:12-root-INFO: best_mean_ssim: 0.7099694013595581
2024-12-02-12:09:12-root-INFO: final_loss: 1.209871768951416
2024-12-02-12:09:12-root-INFO: mean time: 1497.5848653316498
2024-12-02-12:09:12-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample2_iter5_lr0.03_10009 
 
Enjoy.
