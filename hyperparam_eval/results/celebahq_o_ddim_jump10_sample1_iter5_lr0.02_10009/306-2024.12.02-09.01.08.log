2024-12-02-09:01:12-root-INFO: Prepare model...
2024-12-02-09:01:27-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-09:01:50-root-INFO: Start sampling
2024-12-02-09:01:55-root-INFO: step: 249 lr_xt 0.00012706
2024-12-02-09:01:55-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-09:01:56-root-INFO: grad norm: 23714.566 17717.039 15763.478
2024-12-02-09:01:56-root-INFO: grad norm: 26795.582 21197.633 16390.959
2024-12-02-09:01:56-root-INFO: Loss too large (43813.414->60954.977)! Learning rate decreased to 0.00010.
2024-12-02-09:01:56-root-INFO: Loss too large (43813.414->45484.473)! Learning rate decreased to 0.00008.
2024-12-02-09:01:57-root-INFO: grad norm: 21166.740 16201.949 13620.855
2024-12-02-09:01:57-root-INFO: grad norm: 19147.838 15682.541 10986.245
2024-12-02-09:01:58-root-INFO: Loss Change: 77070.016 -> 26880.426
2024-12-02-09:01:58-root-INFO: Regularization Change: 0.000 -> 12.887
2024-12-02-09:01:58-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-02-09:01:58-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:01:58-root-INFO: step: 248 lr_xt 0.00013388
2024-12-02-09:01:58-root-INFO: grad norm: 16837.162 13089.591 10590.216
2024-12-02-09:01:58-root-INFO: Loss too large (26925.527->41989.414)! Learning rate decreased to 0.00011.
2024-12-02-09:01:58-root-INFO: Loss too large (26925.527->30482.660)! Learning rate decreased to 0.00009.
2024-12-02-09:01:59-root-INFO: grad norm: 14965.730 12114.635 8786.849
2024-12-02-09:01:59-root-INFO: grad norm: 13333.771 10544.188 8161.467
2024-12-02-09:02:00-root-INFO: grad norm: 11798.628 9479.819 7024.290
2024-12-02-09:02:00-root-INFO: grad norm: 10415.567 8321.704 6263.650
2024-12-02-09:02:01-root-INFO: Loss Change: 26925.527 -> 19161.316
2024-12-02-09:02:01-root-INFO: Regularization Change: 0.000 -> 2.795
2024-12-02-09:02:01-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-02-09:02:01-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:02:01-root-INFO: step: 247 lr_xt 0.00014104
2024-12-02-09:02:01-root-INFO: grad norm: 8805.622 7052.562 5272.604
2024-12-02-09:02:01-root-INFO: Loss too large (18954.098->23039.410)! Learning rate decreased to 0.00011.
2024-12-02-09:02:01-root-INFO: Loss too large (18954.098->19965.592)! Learning rate decreased to 0.00009.
2024-12-02-09:02:02-root-INFO: grad norm: 7323.694 5943.907 4278.604
2024-12-02-09:02:02-root-INFO: grad norm: 5996.283 4787.209 3610.823
2024-12-02-09:02:03-root-INFO: grad norm: 4944.508 4021.126 2877.273
2024-12-02-09:02:03-root-INFO: grad norm: 4023.149 3208.036 2427.804
2024-12-02-09:02:03-root-INFO: Loss Change: 18954.098 -> 16716.371
2024-12-02-09:02:03-root-INFO: Regularization Change: 0.000 -> 0.649
2024-12-02-09:02:03-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-02-09:02:03-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:02:04-root-INFO: step: 246 lr_xt 0.00014856
2024-12-02-09:02:04-root-INFO: grad norm: 3163.459 2611.760 1784.989
2024-12-02-09:02:04-root-INFO: Loss too large (16469.217->16841.562)! Learning rate decreased to 0.00012.
2024-12-02-09:02:04-root-INFO: grad norm: 3733.900 2999.401 2223.871
2024-12-02-09:02:05-root-INFO: Loss too large (16467.025->16537.387)! Learning rate decreased to 0.00010.
2024-12-02-09:02:05-root-INFO: grad norm: 2894.223 2370.340 1660.728
2024-12-02-09:02:05-root-INFO: grad norm: 2242.541 1817.690 1313.390
2024-12-02-09:02:06-root-INFO: grad norm: 1776.055 1456.523 1016.322
2024-12-02-09:02:06-root-INFO: Loss Change: 16469.217 -> 15881.557
2024-12-02-09:02:06-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-09:02:06-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-02-09:02:06-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-09:02:06-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-09:02:07-root-INFO: grad norm: 1396.750 1149.076 794.062
2024-12-02-09:02:07-root-INFO: grad norm: 2001.235 1614.728 1182.198
2024-12-02-09:02:07-root-INFO: Loss too large (15714.586->15791.730)! Learning rate decreased to 0.00013.
2024-12-02-09:02:08-root-INFO: grad norm: 2205.659 1820.575 1245.166
2024-12-02-09:02:08-root-INFO: grad norm: 2437.490 1958.453 1451.144
2024-12-02-09:02:09-root-INFO: grad norm: 2714.513 2240.385 1532.728
2024-12-02-09:02:09-root-INFO: Loss Change: 15753.115 -> 15577.885
2024-12-02-09:02:09-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-09:02:09-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-09:02:09-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-09:02:09-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-09:02:09-root-INFO: grad norm: 3027.365 2437.371 1795.595
2024-12-02-09:02:09-root-INFO: Loss too large (15495.229->15710.678)! Learning rate decreased to 0.00013.
2024-12-02-09:02:10-root-INFO: grad norm: 3079.678 2534.213 1749.908
2024-12-02-09:02:10-root-INFO: grad norm: 3223.689 2600.891 1904.609
2024-12-02-09:02:11-root-INFO: grad norm: 3422.362 2837.425 1913.525
2024-12-02-09:02:11-root-INFO: grad norm: 3642.100 2939.123 2150.917
2024-12-02-09:02:12-root-INFO: Loss Change: 15495.229 -> 15238.960
2024-12-02-09:02:12-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-02-09:02:12-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-09:02:12-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:12-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-09:02:12-root-INFO: grad norm: 3729.246 3093.251 2083.044
2024-12-02-09:02:12-root-INFO: Loss too large (15090.592->15566.654)! Learning rate decreased to 0.00014.
2024-12-02-09:02:13-root-INFO: grad norm: 3749.051 3020.835 2220.346
2024-12-02-09:02:13-root-INFO: grad norm: 3819.137 3188.687 2101.925
2024-12-02-09:02:13-root-INFO: grad norm: 3909.809 3160.749 2301.364
2024-12-02-09:02:14-root-INFO: grad norm: 4022.375 3357.636 2214.900
2024-12-02-09:02:14-root-INFO: Loss Change: 15090.592 -> 14810.181
2024-12-02-09:02:14-root-INFO: Regularization Change: 0.000 -> 0.561
2024-12-02-09:02:14-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-09:02:14-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:14-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-09:02:15-root-INFO: grad norm: 3846.993 3156.536 2199.007
2024-12-02-09:02:15-root-INFO: Loss too large (14551.070->15039.375)! Learning rate decreased to 0.00015.
2024-12-02-09:02:15-root-INFO: grad norm: 3786.828 3176.153 2062.067
2024-12-02-09:02:16-root-INFO: grad norm: 3760.135 3069.351 2172.026
2024-12-02-09:02:16-root-INFO: grad norm: 3743.089 3146.701 2027.063
2024-12-02-09:02:17-root-INFO: grad norm: 3736.364 3048.793 2159.924
2024-12-02-09:02:17-root-INFO: Loss Change: 14551.070 -> 14081.561
2024-12-02-09:02:17-root-INFO: Regularization Change: 0.000 -> 0.597
2024-12-02-09:02:17-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-09:02:17-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:17-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-09:02:17-root-INFO: grad norm: 3713.385 3124.655 2006.429
2024-12-02-09:02:17-root-INFO: Loss too large (14017.818->14494.916)! Learning rate decreased to 0.00015.
2024-12-02-09:02:18-root-INFO: grad norm: 3579.827 2948.156 2030.648
2024-12-02-09:02:18-root-INFO: grad norm: 3456.700 2919.689 1850.457
2024-12-02-09:02:19-root-INFO: grad norm: 3350.093 2755.648 1905.131
2024-12-02-09:02:19-root-INFO: grad norm: 3244.159 2744.110 1730.441
2024-12-02-09:02:20-root-INFO: Loss Change: 14017.818 -> 13457.971
2024-12-02-09:02:20-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-02-09:02:20-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-09:02:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:20-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-09:02:20-root-INFO: grad norm: 2988.546 2491.724 1650.067
2024-12-02-09:02:20-root-INFO: Loss too large (13258.693->13466.660)! Learning rate decreased to 0.00016.
2024-12-02-09:02:21-root-INFO: grad norm: 2772.713 2366.940 1444.137
2024-12-02-09:02:21-root-INFO: grad norm: 2603.910 2157.259 1458.282
2024-12-02-09:02:22-root-INFO: grad norm: 2452.584 2104.394 1259.641
2024-12-02-09:02:22-root-INFO: grad norm: 2320.446 1923.252 1298.296
2024-12-02-09:02:22-root-INFO: Loss Change: 13258.693 -> 12626.620
2024-12-02-09:02:22-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-09:02:22-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-09:02:22-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:22-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-09:02:23-root-INFO: grad norm: 2090.395 1800.486 1062.074
2024-12-02-09:02:23-root-INFO: Loss too large (12550.141->12593.021)! Learning rate decreased to 0.00017.
2024-12-02-09:02:23-root-INFO: grad norm: 1926.535 1605.551 1064.774
2024-12-02-09:02:24-root-INFO: grad norm: 1779.157 1539.818 891.269
2024-12-02-09:02:24-root-INFO: grad norm: 1650.091 1379.627 905.223
2024-12-02-09:02:25-root-INFO: grad norm: 1532.052 1331.933 757.059
2024-12-02-09:02:25-root-INFO: Loss Change: 12550.141 -> 11990.020
2024-12-02-09:02:25-root-INFO: Regularization Change: 0.000 -> 0.470
2024-12-02-09:02:25-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-09:02:25-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:25-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-09:02:25-root-INFO: grad norm: 1543.119 1323.262 793.846
2024-12-02-09:02:26-root-INFO: grad norm: 1860.722 1611.962 929.443
2024-12-02-09:02:26-root-INFO: grad norm: 2384.289 2015.897 1273.183
2024-12-02-09:02:26-root-INFO: Loss too large (11678.572->11752.892)! Learning rate decreased to 0.00018.
2024-12-02-09:02:27-root-INFO: grad norm: 2107.732 1831.370 1043.368
2024-12-02-09:02:27-root-INFO: grad norm: 1879.324 1594.151 995.261
2024-12-02-09:02:28-root-INFO: Loss Change: 11781.830 -> 11259.285
2024-12-02-09:02:28-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-02-09:02:28-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-09:02:28-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:28-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-09:02:28-root-INFO: grad norm: 1520.084 1334.563 727.733
2024-12-02-09:02:29-root-INFO: grad norm: 1860.521 1588.966 967.846
2024-12-02-09:02:29-root-INFO: grad norm: 2349.082 2052.556 1142.453
2024-12-02-09:02:29-root-INFO: Loss too large (11078.767->11141.931)! Learning rate decreased to 0.00019.
2024-12-02-09:02:30-root-INFO: grad norm: 2018.565 1730.995 1038.394
2024-12-02-09:02:30-root-INFO: grad norm: 1743.473 1532.978 830.468
2024-12-02-09:02:31-root-INFO: Loss Change: 11163.572 -> 10640.277
2024-12-02-09:02:31-root-INFO: Regularization Change: 0.000 -> 0.620
2024-12-02-09:02:31-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-09:02:31-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:31-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-09:02:31-root-INFO: grad norm: 1446.081 1264.782 701.055
2024-12-02-09:02:31-root-INFO: grad norm: 1731.018 1515.835 835.864
2024-12-02-09:02:32-root-INFO: grad norm: 2102.259 1815.142 1060.544
2024-12-02-09:02:32-root-INFO: Loss too large (10407.785->10417.729)! Learning rate decreased to 0.00020.
2024-12-02-09:02:32-root-INFO: grad norm: 1724.944 1515.798 823.278
2024-12-02-09:02:33-root-INFO: grad norm: 1431.880 1239.048 717.662
2024-12-02-09:02:33-root-INFO: Loss Change: 10513.120 -> 9985.076
2024-12-02-09:02:33-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-09:02:33-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-09:02:33-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-09:02:33-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-09:02:33-root-INFO: grad norm: 1011.385 902.268 456.960
2024-12-02-09:02:34-root-INFO: grad norm: 1115.172 973.820 543.401
2024-12-02-09:02:34-root-INFO: grad norm: 1267.706 1133.655 567.367
2024-12-02-09:02:35-root-INFO: grad norm: 1463.812 1273.998 720.885
2024-12-02-09:02:35-root-INFO: grad norm: 1704.580 1512.554 785.985
2024-12-02-09:02:36-root-INFO: Loss Change: 9844.640 -> 9526.092
2024-12-02-09:02:36-root-INFO: Regularization Change: 0.000 -> 0.754
2024-12-02-09:02:36-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-09:02:36-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-09:02:36-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-09:02:36-root-INFO: grad norm: 1907.049 1670.261 920.361
2024-12-02-09:02:36-root-INFO: grad norm: 2148.911 1899.348 1005.135
2024-12-02-09:02:37-root-INFO: grad norm: 2414.516 2106.535 1179.999
2024-12-02-09:02:37-root-INFO: Loss too large (9417.508->9428.569)! Learning rate decreased to 0.00022.
2024-12-02-09:02:38-root-INFO: grad norm: 1740.976 1545.536 801.448
2024-12-02-09:02:38-root-INFO: grad norm: 1283.366 1121.742 623.476
2024-12-02-09:02:38-root-INFO: Loss Change: 9452.553 -> 8909.164
2024-12-02-09:02:38-root-INFO: Regularization Change: 0.000 -> 0.572
2024-12-02-09:02:38-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-09:02:38-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:02:39-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-09:02:39-root-INFO: grad norm: 920.392 806.394 443.679
2024-12-02-09:02:39-root-INFO: grad norm: 924.249 814.504 436.829
2024-12-02-09:02:40-root-INFO: grad norm: 958.724 864.849 413.749
2024-12-02-09:02:40-root-INFO: grad norm: 1001.518 882.586 473.372
2024-12-02-09:02:41-root-INFO: grad norm: 1048.625 945.264 453.972
2024-12-02-09:02:41-root-INFO: Loss Change: 8848.532 -> 8495.908
2024-12-02-09:02:41-root-INFO: Regularization Change: 0.000 -> 0.607
2024-12-02-09:02:41-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-09:02:41-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:02:41-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-09:02:41-root-INFO: grad norm: 1381.414 1206.764 672.329
2024-12-02-09:02:42-root-INFO: grad norm: 1363.852 1230.569 588.040
2024-12-02-09:02:42-root-INFO: grad norm: 1375.263 1213.292 647.512
2024-12-02-09:02:43-root-INFO: grad norm: 1390.181 1254.959 598.065
2024-12-02-09:02:43-root-INFO: grad norm: 1403.332 1241.386 654.447
2024-12-02-09:02:44-root-INFO: Loss Change: 8380.861 -> 8054.062
2024-12-02-09:02:44-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-09:02:44-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-09:02:44-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:02:44-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-09:02:44-root-INFO: grad norm: 1252.154 1134.732 529.406
2024-12-02-09:02:44-root-INFO: grad norm: 1227.484 1088.386 567.567
2024-12-02-09:02:45-root-INFO: grad norm: 1198.803 1085.830 508.038
2024-12-02-09:02:45-root-INFO: grad norm: 1168.373 1036.064 540.061
2024-12-02-09:02:46-root-INFO: grad norm: 1135.412 1029.482 478.882
2024-12-02-09:02:46-root-INFO: Loss Change: 7957.600 -> 7642.536
2024-12-02-09:02:46-root-INFO: Regularization Change: 0.000 -> 0.589
2024-12-02-09:02:46-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-09:02:46-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:02:46-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-09:02:47-root-INFO: grad norm: 1237.321 1096.293 573.677
2024-12-02-09:02:47-root-INFO: grad norm: 1151.488 1048.745 475.456
2024-12-02-09:02:48-root-INFO: grad norm: 1076.012 957.414 491.081
2024-12-02-09:02:48-root-INFO: grad norm: 1005.002 916.412 412.575
2024-12-02-09:02:49-root-INFO: grad norm: 940.305 838.973 424.617
2024-12-02-09:02:49-root-INFO: Loss Change: 7578.918 -> 7255.491
2024-12-02-09:02:49-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-02-09:02:49-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-09:02:49-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:02:49-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-09:02:49-root-INFO: grad norm: 656.148 607.361 248.278
2024-12-02-09:02:50-root-INFO: grad norm: 593.692 533.525 260.425
2024-12-02-09:02:50-root-INFO: grad norm: 551.987 514.805 199.162
2024-12-02-09:02:51-root-INFO: grad norm: 516.452 468.487 217.353
2024-12-02-09:02:51-root-INFO: grad norm: 485.793 453.653 173.762
2024-12-02-09:02:52-root-INFO: Loss Change: 7137.076 -> 6892.331
2024-12-02-09:02:52-root-INFO: Regularization Change: 0.000 -> 0.424
2024-12-02-09:02:52-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-09:02:52-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:02:52-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-09:02:52-root-INFO: grad norm: 590.337 524.419 271.077
2024-12-02-09:02:52-root-INFO: grad norm: 519.671 482.017 194.209
2024-12-02-09:02:53-root-INFO: grad norm: 475.151 432.897 195.880
2024-12-02-09:02:53-root-INFO: grad norm: 440.251 411.333 156.927
2024-12-02-09:02:54-root-INFO: grad norm: 411.603 377.380 164.322
2024-12-02-09:02:54-root-INFO: Loss Change: 6855.396 -> 6643.429
2024-12-02-09:02:54-root-INFO: Regularization Change: 0.000 -> 0.378
2024-12-02-09:02:54-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-09:02:54-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-09:02:54-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-09:02:55-root-INFO: grad norm: 405.963 362.055 183.637
2024-12-02-09:02:55-root-INFO: grad norm: 344.882 318.730 131.735
2024-12-02-09:02:56-root-INFO: grad norm: 324.195 298.531 126.417
2024-12-02-09:02:56-root-INFO: grad norm: 312.221 291.267 112.454
2024-12-02-09:02:56-root-INFO: grad norm: 303.664 282.135 112.302
2024-12-02-09:02:57-root-INFO: Loss Change: 6627.511 -> 6431.984
2024-12-02-09:02:57-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-02-09:02:57-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-09:02:57-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-09:02:57-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-09:02:57-root-INFO: grad norm: 365.890 320.170 177.106
2024-12-02-09:02:58-root-INFO: grad norm: 295.292 274.941 107.725
2024-12-02-09:02:58-root-INFO: grad norm: 283.988 266.089 99.226
2024-12-02-09:02:58-root-INFO: grad norm: 278.670 260.516 98.937
2024-12-02-09:02:59-root-INFO: grad norm: 274.527 257.304 95.706
2024-12-02-09:02:59-root-INFO: Loss Change: 6374.370 -> 6205.501
2024-12-02-09:02:59-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-02-09:02:59-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-09:02:59-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:02:59-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-09:03:00-root-INFO: grad norm: 493.518 447.898 207.237
2024-12-02-09:03:00-root-INFO: grad norm: 403.958 374.470 151.509
2024-12-02-09:03:01-root-INFO: grad norm: 362.981 333.028 144.385
2024-12-02-09:03:01-root-INFO: grad norm: 333.691 309.163 125.570
2024-12-02-09:03:02-root-INFO: grad norm: 311.795 287.939 119.613
2024-12-02-09:03:02-root-INFO: Loss Change: 6155.986 -> 5978.440
2024-12-02-09:03:02-root-INFO: Regularization Change: 0.000 -> 0.358
2024-12-02-09:03:02-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-09:03:02-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:03:02-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-09:03:02-root-INFO: grad norm: 317.099 283.302 142.449
2024-12-02-09:03:03-root-INFO: grad norm: 272.429 249.931 108.407
2024-12-02-09:03:03-root-INFO: grad norm: 260.371 239.922 101.145
2024-12-02-09:03:04-root-INFO: grad norm: 253.204 233.743 97.348
2024-12-02-09:03:04-root-INFO: grad norm: 248.384 229.907 94.007
2024-12-02-09:03:04-root-INFO: Loss Change: 5944.321 -> 5789.338
2024-12-02-09:03:04-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-02-09:03:04-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-09:03:04-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:03:05-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-09:03:05-root-INFO: grad norm: 356.535 326.354 143.563
2024-12-02-09:03:05-root-INFO: grad norm: 313.821 287.966 124.738
2024-12-02-09:03:06-root-INFO: grad norm: 292.629 271.167 110.000
2024-12-02-09:03:06-root-INFO: grad norm: 277.509 255.253 108.889
2024-12-02-09:03:07-root-INFO: grad norm: 266.155 247.529 97.816
2024-12-02-09:03:07-root-INFO: Loss Change: 5755.133 -> 5604.751
2024-12-02-09:03:07-root-INFO: Regularization Change: 0.000 -> 0.347
2024-12-02-09:03:07-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-09:03:07-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:03:07-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-09:03:07-root-INFO: grad norm: 245.013 223.691 99.969
2024-12-02-09:03:08-root-INFO: grad norm: 233.035 216.769 85.536
2024-12-02-09:03:08-root-INFO: grad norm: 229.734 212.803 86.560
2024-12-02-09:03:09-root-INFO: grad norm: 227.034 211.024 83.746
2024-12-02-09:03:09-root-INFO: grad norm: 224.620 208.346 83.940
2024-12-02-09:03:09-root-INFO: Loss Change: 5538.894 -> 5409.783
2024-12-02-09:03:09-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-02-09:03:09-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-09:03:09-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:03:10-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-09:03:10-root-INFO: grad norm: 270.577 249.678 104.272
2024-12-02-09:03:10-root-INFO: grad norm: 247.706 229.204 93.935
2024-12-02-09:03:11-root-INFO: grad norm: 237.966 222.010 85.670
2024-12-02-09:03:11-root-INFO: grad norm: 230.676 213.675 86.914
2024-12-02-09:03:12-root-INFO: grad norm: 224.845 209.801 80.863
2024-12-02-09:03:12-root-INFO: Loss Change: 5361.812 -> 5236.462
2024-12-02-09:03:12-root-INFO: Regularization Change: 0.000 -> 0.321
2024-12-02-09:03:12-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-09:03:12-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:03:12-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-09:03:12-root-INFO: grad norm: 375.818 347.429 143.290
2024-12-02-09:03:13-root-INFO: grad norm: 317.433 294.227 119.139
2024-12-02-09:03:13-root-INFO: grad norm: 291.200 272.273 103.269
2024-12-02-09:03:14-root-INFO: grad norm: 273.529 253.068 103.802
2024-12-02-09:03:14-root-INFO: grad norm: 260.198 243.989 90.402
2024-12-02-09:03:14-root-INFO: Loss Change: 5221.196 -> 5075.812
2024-12-02-09:03:14-root-INFO: Regularization Change: 0.000 -> 0.381
2024-12-02-09:03:14-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-09:03:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-09:03:15-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-09:03:15-root-INFO: grad norm: 244.399 223.014 99.979
2024-12-02-09:03:15-root-INFO: grad norm: 228.308 213.364 81.242
2024-12-02-09:03:16-root-INFO: grad norm: 220.739 204.105 84.064
2024-12-02-09:03:16-root-INFO: grad norm: 214.654 200.327 77.105
2024-12-02-09:03:17-root-INFO: grad norm: 209.591 193.808 79.791
2024-12-02-09:03:17-root-INFO: Loss Change: 5049.424 -> 4933.920
2024-12-02-09:03:17-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-02-09:03:17-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-09:03:17-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-09:03:17-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-09:03:17-root-INFO: grad norm: 340.389 316.756 124.619
2024-12-02-09:03:18-root-INFO: grad norm: 307.719 286.498 112.294
2024-12-02-09:03:18-root-INFO: grad norm: 286.683 269.415 97.994
2024-12-02-09:03:19-root-INFO: grad norm: 270.446 251.175 100.260
2024-12-02-09:03:19-root-INFO: grad norm: 257.263 242.137 86.914
2024-12-02-09:03:20-root-INFO: Loss Change: 4914.375 -> 4790.507
2024-12-02-09:03:20-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-09:03:20-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-09:03:20-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:03:20-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-09:03:20-root-INFO: grad norm: 247.361 217.788 117.285
2024-12-02-09:03:20-root-INFO: grad norm: 200.832 184.991 78.178
2024-12-02-09:03:21-root-INFO: grad norm: 191.039 176.661 72.711
2024-12-02-09:03:21-root-INFO: grad norm: 186.382 172.393 70.843
2024-12-02-09:03:22-root-INFO: grad norm: 183.365 170.188 68.256
2024-12-02-09:03:22-root-INFO: Loss Change: 4766.234 -> 4645.312
2024-12-02-09:03:22-root-INFO: Regularization Change: 0.000 -> 0.368
2024-12-02-09:03:22-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-09:03:22-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:03:22-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-09:03:22-root-INFO: grad norm: 314.030 288.107 124.936
2024-12-02-09:03:23-root-INFO: grad norm: 276.319 256.299 103.261
2024-12-02-09:03:23-root-INFO: grad norm: 265.687 249.978 90.004
2024-12-02-09:03:24-root-INFO: grad norm: 259.182 240.575 96.432
2024-12-02-09:03:24-root-INFO: grad norm: 254.615 240.499 83.601
2024-12-02-09:03:25-root-INFO: Loss Change: 4629.150 -> 4507.252
2024-12-02-09:03:25-root-INFO: Regularization Change: 0.000 -> 0.401
2024-12-02-09:03:25-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-09:03:25-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:03:25-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-09:03:25-root-INFO: grad norm: 227.748 211.796 83.736
2024-12-02-09:03:25-root-INFO: grad norm: 220.661 207.791 74.259
2024-12-02-09:03:26-root-INFO: grad norm: 215.202 199.913 79.667
2024-12-02-09:03:26-root-INFO: grad norm: 210.122 197.976 70.402
2024-12-02-09:03:27-root-INFO: grad norm: 205.303 190.591 76.318
2024-12-02-09:03:27-root-INFO: Loss Change: 4489.577 -> 4387.452
2024-12-02-09:03:27-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-02-09:03:27-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-09:03:27-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:03:27-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-09:03:28-root-INFO: grad norm: 330.659 306.991 122.848
2024-12-02-09:03:28-root-INFO: grad norm: 302.907 282.936 108.167
2024-12-02-09:03:28-root-INFO: grad norm: 289.236 273.818 93.171
2024-12-02-09:03:29-root-INFO: grad norm: 277.598 259.151 99.506
2024-12-02-09:03:29-root-INFO: grad norm: 266.911 253.343 84.016
2024-12-02-09:03:30-root-INFO: Loss Change: 4365.184 -> 4258.321
2024-12-02-09:03:30-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-02-09:03:30-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-09:03:30-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:03:30-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-09:03:30-root-INFO: grad norm: 209.313 187.717 92.597
2024-12-02-09:03:30-root-INFO: grad norm: 170.133 156.881 65.829
2024-12-02-09:03:31-root-INFO: grad norm: 160.082 148.184 60.563
2024-12-02-09:03:31-root-INFO: grad norm: 154.963 143.908 57.479
2024-12-02-09:03:32-root-INFO: grad norm: 151.424 140.944 55.352
2024-12-02-09:03:32-root-INFO: Loss Change: 4236.105 -> 4132.946
2024-12-02-09:03:32-root-INFO: Regularization Change: 0.000 -> 0.380
2024-12-02-09:03:32-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-09:03:32-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-09:03:32-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-09:03:32-root-INFO: grad norm: 150.231 141.244 51.183
2024-12-02-09:03:33-root-INFO: grad norm: 146.498 137.487 50.586
2024-12-02-09:03:33-root-INFO: grad norm: 143.906 135.063 49.668
2024-12-02-09:03:34-root-INFO: grad norm: 141.660 132.734 49.488
2024-12-02-09:03:34-root-INFO: grad norm: 139.539 130.772 48.681
2024-12-02-09:03:35-root-INFO: Loss Change: 4108.805 -> 4027.380
2024-12-02-09:03:35-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-09:03:35-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-09:03:35-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-09:03:35-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-09:03:35-root-INFO: grad norm: 262.594 236.110 114.925
2024-12-02-09:03:35-root-INFO: grad norm: 205.368 189.768 78.511
2024-12-02-09:03:36-root-INFO: grad norm: 193.275 182.502 63.624
2024-12-02-09:03:36-root-INFO: grad norm: 187.024 173.799 69.077
2024-12-02-09:03:37-root-INFO: grad norm: 182.425 173.333 56.873
2024-12-02-09:03:37-root-INFO: Loss Change: 4007.523 -> 3914.104
2024-12-02-09:03:37-root-INFO: Regularization Change: 0.000 -> 0.371
2024-12-02-09:03:37-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-09:03:37-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:03:37-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-09:03:37-root-INFO: grad norm: 137.834 127.202 53.083
2024-12-02-09:03:38-root-INFO: grad norm: 130.071 121.977 45.167
2024-12-02-09:03:38-root-INFO: grad norm: 127.421 119.104 45.279
2024-12-02-09:03:39-root-INFO: grad norm: 125.247 117.504 43.354
2024-12-02-09:03:39-root-INFO: grad norm: 123.333 115.263 43.878
2024-12-02-09:03:40-root-INFO: Loss Change: 3892.620 -> 3823.448
2024-12-02-09:03:40-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-02-09:03:40-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-09:03:40-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:03:40-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-09:03:40-root-INFO: grad norm: 185.109 168.129 77.448
2024-12-02-09:03:40-root-INFO: grad norm: 153.885 143.088 56.625
2024-12-02-09:03:41-root-INFO: grad norm: 146.823 138.168 49.666
2024-12-02-09:03:41-root-INFO: grad norm: 143.342 134.053 50.763
2024-12-02-09:03:42-root-INFO: grad norm: 140.950 133.333 45.708
2024-12-02-09:03:42-root-INFO: Loss Change: 3797.019 -> 3726.890
2024-12-02-09:03:42-root-INFO: Regularization Change: 0.000 -> 0.312
2024-12-02-09:03:42-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-09:03:42-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:03:42-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-09:03:42-root-INFO: grad norm: 157.058 141.544 68.062
2024-12-02-09:03:43-root-INFO: grad norm: 122.380 113.249 46.384
2024-12-02-09:03:43-root-INFO: grad norm: 114.204 106.451 41.362
2024-12-02-09:03:44-root-INFO: grad norm: 110.933 103.641 39.556
2024-12-02-09:03:44-root-INFO: grad norm: 108.866 101.987 38.084
2024-12-02-09:03:45-root-INFO: Loss Change: 3715.574 -> 3648.555
2024-12-02-09:03:45-root-INFO: Regularization Change: 0.000 -> 0.307
2024-12-02-09:03:45-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-09:03:45-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:03:45-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-09:03:45-root-INFO: grad norm: 134.536 122.934 54.656
2024-12-02-09:03:46-root-INFO: grad norm: 118.402 111.243 40.547
2024-12-02-09:03:46-root-INFO: grad norm: 115.702 108.467 40.273
2024-12-02-09:03:46-root-INFO: grad norm: 114.785 108.098 38.603
2024-12-02-09:03:47-root-INFO: grad norm: 114.627 107.971 38.490
2024-12-02-09:03:47-root-INFO: Loss Change: 3621.983 -> 3564.502
2024-12-02-09:03:47-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-02-09:03:47-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-09:03:47-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:03:47-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-09:03:48-root-INFO: grad norm: 122.908 113.585 46.956
2024-12-02-09:03:48-root-INFO: grad norm: 105.952 99.171 37.294
2024-12-02-09:03:48-root-INFO: grad norm: 103.012 96.909 34.929
2024-12-02-09:03:49-root-INFO: grad norm: 101.984 95.949 34.560
2024-12-02-09:03:49-root-INFO: grad norm: 101.535 95.812 33.606
2024-12-02-09:03:50-root-INFO: Loss Change: 3539.958 -> 3484.900
2024-12-02-09:03:50-root-INFO: Regularization Change: 0.000 -> 0.285
2024-12-02-09:03:50-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-09:03:50-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-09:03:50-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-09:03:50-root-INFO: grad norm: 152.761 140.564 59.814
2024-12-02-09:03:51-root-INFO: grad norm: 138.815 131.303 45.046
2024-12-02-09:03:51-root-INFO: grad norm: 146.989 139.156 47.344
2024-12-02-09:03:51-root-INFO: grad norm: 160.840 153.511 47.998
2024-12-02-09:03:52-root-INFO: grad norm: 181.041 172.391 55.293
2024-12-02-09:03:52-root-INFO: Loss Change: 3470.672 -> 3421.519
2024-12-02-09:03:52-root-INFO: Regularization Change: 0.000 -> 0.315
2024-12-02-09:03:52-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-09:03:52-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-09:03:52-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-09:03:53-root-INFO: grad norm: 200.856 187.872 71.042
2024-12-02-09:03:53-root-INFO: grad norm: 208.986 197.849 67.312
2024-12-02-09:03:53-root-INFO: grad norm: 242.637 232.413 69.693
2024-12-02-09:03:54-root-INFO: grad norm: 293.755 280.288 87.924
2024-12-02-09:03:54-root-INFO: Loss too large (3386.371->3388.135)! Learning rate decreased to 0.00092.
2024-12-02-09:03:55-root-INFO: grad norm: 242.343 232.181 69.443
2024-12-02-09:03:55-root-INFO: Loss Change: 3411.848 -> 3358.897
2024-12-02-09:03:55-root-INFO: Regularization Change: 0.000 -> 0.305
2024-12-02-09:03:55-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-09:03:55-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:03:55-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-09:03:55-root-INFO: grad norm: 190.044 180.286 60.114
2024-12-02-09:03:56-root-INFO: grad norm: 217.672 208.636 62.065
2024-12-02-09:03:56-root-INFO: grad norm: 261.308 250.245 75.229
2024-12-02-09:03:56-root-INFO: grad norm: 312.613 299.756 88.733
2024-12-02-09:03:57-root-INFO: Loss too large (3329.737->3330.588)! Learning rate decreased to 0.00096.
2024-12-02-09:03:57-root-INFO: grad norm: 248.922 238.404 71.595
2024-12-02-09:03:57-root-INFO: Loss Change: 3342.965 -> 3298.507
2024-12-02-09:03:57-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-02-09:03:57-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-09:03:57-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:03:58-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-09:03:58-root-INFO: grad norm: 249.057 231.326 92.292
2024-12-02-09:03:58-root-INFO: grad norm: 275.910 263.045 83.269
2024-12-02-09:03:58-root-INFO: Loss too large (3272.594->3274.529)! Learning rate decreased to 0.00100.
2024-12-02-09:03:59-root-INFO: grad norm: 244.928 234.730 69.941
2024-12-02-09:03:59-root-INFO: grad norm: 220.716 210.685 65.782
2024-12-02-09:04:00-root-INFO: grad norm: 203.819 195.521 57.566
2024-12-02-09:04:00-root-INFO: Loss Change: 3288.711 -> 3225.656
2024-12-02-09:04:00-root-INFO: Regularization Change: 0.000 -> 0.329
2024-12-02-09:04:00-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-09:04:00-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:04:00-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-09:04:00-root-INFO: grad norm: 224.648 213.326 70.420
2024-12-02-09:04:01-root-INFO: grad norm: 286.857 276.140 77.676
2024-12-02-09:04:01-root-INFO: Loss too large (3225.409->3230.968)! Learning rate decreased to 0.00105.
2024-12-02-09:04:01-root-INFO: grad norm: 256.802 245.602 75.010
2024-12-02-09:04:02-root-INFO: grad norm: 235.166 226.185 64.369
2024-12-02-09:04:02-root-INFO: grad norm: 217.221 207.730 63.506
2024-12-02-09:04:03-root-INFO: Loss Change: 3226.998 -> 3179.829
2024-12-02-09:04:03-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-09:04:03-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-09:04:03-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:04:03-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-09:04:03-root-INFO: grad norm: 266.083 251.822 85.941
2024-12-02-09:04:03-root-INFO: Loss too large (3174.958->3175.445)! Learning rate decreased to 0.00110.
2024-12-02-09:04:04-root-INFO: grad norm: 247.399 237.116 70.585
2024-12-02-09:04:04-root-INFO: grad norm: 244.939 235.164 68.506
2024-12-02-09:04:05-root-INFO: grad norm: 247.591 237.449 70.138
2024-12-02-09:04:05-root-INFO: grad norm: 254.042 244.123 70.294
2024-12-02-09:04:05-root-INFO: Loss Change: 3174.958 -> 3124.887
2024-12-02-09:04:05-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-02-09:04:05-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-09:04:05-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-09:04:06-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-09:04:06-root-INFO: grad norm: 256.087 246.138 70.688
2024-12-02-09:04:06-root-INFO: Loss too large (3106.406->3113.159)! Learning rate decreased to 0.00115.
2024-12-02-09:04:06-root-INFO: grad norm: 257.927 248.190 70.201
2024-12-02-09:04:07-root-INFO: grad norm: 270.747 260.062 75.310
2024-12-02-09:04:07-root-INFO: grad norm: 288.015 277.435 77.346
2024-12-02-09:04:08-root-INFO: grad norm: 310.415 298.548 85.010
2024-12-02-09:04:08-root-INFO: Loss Change: 3106.406 -> 3071.467
2024-12-02-09:04:08-root-INFO: Regularization Change: 0.000 -> 0.370
2024-12-02-09:04:08-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-09:04:08-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-09:04:08-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-09:04:08-root-INFO: grad norm: 350.091 335.629 99.583
2024-12-02-09:04:09-root-INFO: Loss too large (3069.740->3104.323)! Learning rate decreased to 0.00120.
2024-12-02-09:04:09-root-INFO: grad norm: 380.664 366.081 104.354
2024-12-02-09:04:09-root-INFO: Loss too large (3061.651->3062.743)! Learning rate decreased to 0.00096.
2024-12-02-09:04:10-root-INFO: grad norm: 277.565 267.391 74.462
2024-12-02-09:04:10-root-INFO: grad norm: 207.828 199.043 59.786
2024-12-02-09:04:11-root-INFO: grad norm: 167.043 160.528 46.198
2024-12-02-09:04:11-root-INFO: Loss Change: 3069.740 -> 2994.176
2024-12-02-09:04:11-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-02-09:04:11-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-09:04:11-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:04:11-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-09:04:11-root-INFO: grad norm: 118.230 112.490 36.394
2024-12-02-09:04:12-root-INFO: grad norm: 141.949 136.016 40.611
2024-12-02-09:04:12-root-INFO: grad norm: 225.844 218.030 58.893
2024-12-02-09:04:12-root-INFO: Loss too large (2957.297->2976.039)! Learning rate decreased to 0.00125.
2024-12-02-09:04:13-root-INFO: grad norm: 293.275 282.943 77.160
2024-12-02-09:04:13-root-INFO: Loss too large (2956.786->2963.727)! Learning rate decreased to 0.00100.
2024-12-02-09:04:13-root-INFO: grad norm: 265.346 256.544 67.775
2024-12-02-09:04:14-root-INFO: Loss Change: 2976.037 -> 2933.125
2024-12-02-09:04:14-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-02-09:04:14-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-09:04:14-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:04:14-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-09:04:14-root-INFO: grad norm: 262.962 252.088 74.838
2024-12-02-09:04:14-root-INFO: Loss too large (2925.755->2955.604)! Learning rate decreased to 0.00131.
2024-12-02-09:04:14-root-INFO: Loss too large (2925.755->2926.172)! Learning rate decreased to 0.00105.
2024-12-02-09:04:15-root-INFO: grad norm: 244.851 235.976 65.326
2024-12-02-09:04:15-root-INFO: grad norm: 239.433 231.112 62.574
2024-12-02-09:04:16-root-INFO: grad norm: 240.246 231.693 63.535
2024-12-02-09:04:16-root-INFO: grad norm: 246.160 237.707 63.953
2024-12-02-09:04:17-root-INFO: Loss Change: 2925.755 -> 2874.486
2024-12-02-09:04:17-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-09:04:17-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-09:04:17-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:04:17-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-09:04:17-root-INFO: grad norm: 247.194 237.918 67.080
2024-12-02-09:04:17-root-INFO: Loss too large (2863.594->2896.644)! Learning rate decreased to 0.00137.
2024-12-02-09:04:17-root-INFO: Loss too large (2863.594->2868.499)! Learning rate decreased to 0.00109.
2024-12-02-09:04:18-root-INFO: grad norm: 248.112 239.626 64.336
2024-12-02-09:04:18-root-INFO: grad norm: 256.183 247.692 65.409
2024-12-02-09:04:19-root-INFO: grad norm: 269.686 260.520 69.713
2024-12-02-09:04:19-root-INFO: grad norm: 289.450 280.353 71.997
2024-12-02-09:04:20-root-INFO: Loss Change: 2863.594 -> 2824.123
2024-12-02-09:04:20-root-INFO: Regularization Change: 0.000 -> 0.344
2024-12-02-09:04:20-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-09:04:20-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:04:20-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-09:04:20-root-INFO: grad norm: 326.566 315.074 85.871
2024-12-02-09:04:20-root-INFO: Loss too large (2816.086->2905.334)! Learning rate decreased to 0.00143.
2024-12-02-09:04:20-root-INFO: Loss too large (2816.086->2842.459)! Learning rate decreased to 0.00114.
2024-12-02-09:04:21-root-INFO: grad norm: 356.920 346.078 87.302
2024-12-02-09:04:21-root-INFO: grad norm: 395.115 382.542 98.879
2024-12-02-09:04:22-root-INFO: grad norm: 438.778 425.955 105.301
2024-12-02-09:04:22-root-INFO: Loss too large (2804.760->2808.552)! Learning rate decreased to 0.00091.
2024-12-02-09:04:22-root-INFO: grad norm: 315.460 305.005 80.542
2024-12-02-09:04:23-root-INFO: Loss Change: 2816.086 -> 2751.443
2024-12-02-09:04:23-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-02-09:04:23-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-09:04:23-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-09:04:23-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-09:04:23-root-INFO: grad norm: 200.917 194.823 49.107
2024-12-02-09:04:23-root-INFO: Loss too large (2733.472->2748.458)! Learning rate decreased to 0.00149.
2024-12-02-09:04:24-root-INFO: grad norm: 301.299 291.678 75.531
2024-12-02-09:04:24-root-INFO: Loss too large (2730.879->2762.094)! Learning rate decreased to 0.00119.
2024-12-02-09:04:24-root-INFO: grad norm: 359.750 349.614 84.795
2024-12-02-09:04:24-root-INFO: Loss too large (2728.764->2734.875)! Learning rate decreased to 0.00095.
2024-12-02-09:04:25-root-INFO: grad norm: 289.486 280.112 73.071
2024-12-02-09:04:25-root-INFO: grad norm: 238.244 231.348 56.904
2024-12-02-09:04:26-root-INFO: Loss Change: 2733.472 -> 2679.057
2024-12-02-09:04:26-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-02-09:04:26-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-09:04:26-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-09:04:26-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-09:04:26-root-INFO: grad norm: 243.980 235.543 63.604
2024-12-02-09:04:26-root-INFO: Loss too large (2664.810->2718.749)! Learning rate decreased to 0.00156.
2024-12-02-09:04:26-root-INFO: Loss too large (2664.810->2680.241)! Learning rate decreased to 0.00124.
2024-12-02-09:04:27-root-INFO: grad norm: 298.010 290.532 66.343
2024-12-02-09:04:27-root-INFO: Loss too large (2659.322->2662.997)! Learning rate decreased to 0.00100.
2024-12-02-09:04:27-root-INFO: grad norm: 256.646 248.241 65.140
2024-12-02-09:04:28-root-INFO: grad norm: 226.730 220.923 50.985
2024-12-02-09:04:28-root-INFO: grad norm: 208.280 201.070 54.328
2024-12-02-09:04:29-root-INFO: Loss Change: 2664.810 -> 2608.572
2024-12-02-09:04:29-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-02-09:04:29-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-09:04:29-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:04:29-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-09:04:29-root-INFO: grad norm: 190.257 183.926 48.673
2024-12-02-09:04:29-root-INFO: Loss too large (2592.781->2609.458)! Learning rate decreased to 0.00162.
2024-12-02-09:04:30-root-INFO: grad norm: 324.437 315.494 75.648
2024-12-02-09:04:30-root-INFO: Loss too large (2591.482->2665.631)! Learning rate decreased to 0.00130.
2024-12-02-09:04:30-root-INFO: Loss too large (2591.482->2611.612)! Learning rate decreased to 0.00104.
2024-12-02-09:04:30-root-INFO: grad norm: 334.237 325.634 75.344
2024-12-02-09:04:31-root-INFO: grad norm: 347.702 338.232 80.599
2024-12-02-09:04:31-root-INFO: grad norm: 364.683 355.804 79.980
2024-12-02-09:04:32-root-INFO: Loss Change: 2592.781 -> 2565.084
2024-12-02-09:04:32-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-02-09:04:32-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-09:04:32-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:04:32-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-09:04:32-root-INFO: grad norm: 422.833 411.619 96.734
2024-12-02-09:04:32-root-INFO: Loss too large (2565.628->2860.404)! Learning rate decreased to 0.00170.
2024-12-02-09:04:32-root-INFO: Loss too large (2565.628->2704.862)! Learning rate decreased to 0.00136.
2024-12-02-09:04:32-root-INFO: Loss too large (2565.628->2608.101)! Learning rate decreased to 0.00108.
2024-12-02-09:04:33-root-INFO: grad norm: 428.899 419.325 90.118
2024-12-02-09:04:33-root-INFO: grad norm: 436.002 425.175 96.559
2024-12-02-09:04:34-root-INFO: grad norm: 439.241 429.588 91.580
2024-12-02-09:04:34-root-INFO: grad norm: 439.028 428.118 97.265
2024-12-02-09:04:35-root-INFO: Loss Change: 2565.628 -> 2521.401
2024-12-02-09:04:35-root-INFO: Regularization Change: 0.000 -> 0.455
2024-12-02-09:04:35-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-09:04:35-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:04:35-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-09:04:35-root-INFO: grad norm: 417.199 408.350 85.472
2024-12-02-09:04:35-root-INFO: Loss too large (2504.754->2791.320)! Learning rate decreased to 0.00177.
2024-12-02-09:04:35-root-INFO: Loss too large (2504.754->2644.208)! Learning rate decreased to 0.00142.
2024-12-02-09:04:35-root-INFO: Loss too large (2504.754->2551.346)! Learning rate decreased to 0.00113.
2024-12-02-09:04:36-root-INFO: grad norm: 415.550 405.919 88.946
2024-12-02-09:04:36-root-INFO: grad norm: 416.866 408.160 84.752
2024-12-02-09:04:37-root-INFO: grad norm: 421.986 412.307 89.863
2024-12-02-09:04:37-root-INFO: grad norm: 427.145 418.458 85.710
2024-12-02-09:04:38-root-INFO: Loss Change: 2504.754 -> 2467.110
2024-12-02-09:04:38-root-INFO: Regularization Change: 0.000 -> 0.468
2024-12-02-09:04:38-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-09:04:38-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-09:04:38-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-09:04:38-root-INFO: grad norm: 491.707 480.860 102.712
2024-12-02-09:04:38-root-INFO: Loss too large (2475.150->2865.684)! Learning rate decreased to 0.00185.
2024-12-02-09:04:38-root-INFO: Loss too large (2475.150->2663.197)! Learning rate decreased to 0.00148.
2024-12-02-09:04:38-root-INFO: Loss too large (2475.150->2530.765)! Learning rate decreased to 0.00118.
2024-12-02-09:04:39-root-INFO: grad norm: 466.725 458.339 88.079
2024-12-02-09:04:39-root-INFO: grad norm: 448.302 438.826 91.688
2024-12-02-09:04:40-root-INFO: grad norm: 427.828 420.069 81.110
2024-12-02-09:04:40-root-INFO: grad norm: 413.751 404.986 84.714
2024-12-02-09:04:41-root-INFO: Loss Change: 2475.150 -> 2404.973
2024-12-02-09:04:41-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-02-09:04:41-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-09:04:41-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-09:04:41-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-09:04:41-root-INFO: grad norm: 410.807 402.988 79.766
2024-12-02-09:04:41-root-INFO: Loss too large (2404.640->2709.240)! Learning rate decreased to 0.00193.
2024-12-02-09:04:41-root-INFO: Loss too large (2404.640->2553.020)! Learning rate decreased to 0.00154.
2024-12-02-09:04:41-root-INFO: Loss too large (2404.640->2454.178)! Learning rate decreased to 0.00123.
2024-12-02-09:04:42-root-INFO: grad norm: 410.072 402.153 80.204
2024-12-02-09:04:42-root-INFO: grad norm: 415.599 407.917 79.540
2024-12-02-09:04:43-root-INFO: grad norm: 427.101 419.094 82.316
2024-12-02-09:04:43-root-INFO: grad norm: 439.657 431.732 83.099
2024-12-02-09:04:44-root-INFO: Loss Change: 2404.640 -> 2375.642
2024-12-02-09:04:44-root-INFO: Regularization Change: 0.000 -> 0.521
2024-12-02-09:04:44-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-09:04:44-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:04:44-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-09:04:44-root-INFO: grad norm: 498.038 488.871 95.117
2024-12-02-09:04:44-root-INFO: Loss too large (2381.252->2811.590)! Learning rate decreased to 0.00201.
2024-12-02-09:04:44-root-INFO: Loss too large (2381.252->2596.876)! Learning rate decreased to 0.00161.
2024-12-02-09:04:44-root-INFO: Loss too large (2381.252->2451.483)! Learning rate decreased to 0.00129.
2024-12-02-09:04:45-root-INFO: grad norm: 480.755 472.912 86.482
2024-12-02-09:04:45-root-INFO: grad norm: 464.796 456.440 87.739
2024-12-02-09:04:46-root-INFO: grad norm: 444.464 437.186 80.105
2024-12-02-09:04:46-root-INFO: grad norm: 428.411 420.663 81.111
2024-12-02-09:04:47-root-INFO: Loss Change: 2381.252 -> 2316.552
2024-12-02-09:04:47-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-02-09:04:47-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-09:04:47-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:04:47-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-09:04:47-root-INFO: grad norm: 397.335 390.985 70.750
2024-12-02-09:04:47-root-INFO: Loss too large (2300.042->2610.499)! Learning rate decreased to 0.00209.
2024-12-02-09:04:47-root-INFO: Loss too large (2300.042->2450.884)! Learning rate decreased to 0.00168.
2024-12-02-09:04:47-root-INFO: Loss too large (2300.042->2349.833)! Learning rate decreased to 0.00134.
2024-12-02-09:04:48-root-INFO: grad norm: 390.559 384.166 70.376
2024-12-02-09:04:48-root-INFO: grad norm: 391.915 385.712 69.455
2024-12-02-09:04:49-root-INFO: grad norm: 402.114 395.574 72.229
2024-12-02-09:04:49-root-INFO: grad norm: 416.722 410.225 73.303
2024-12-02-09:04:50-root-INFO: Loss Change: 2300.042 -> 2272.356
2024-12-02-09:04:50-root-INFO: Regularization Change: 0.000 -> 0.537
2024-12-02-09:04:50-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-09:04:50-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:04:50-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-09:04:50-root-INFO: grad norm: 504.215 495.875 91.330
2024-12-02-09:04:50-root-INFO: Loss too large (2291.271->2763.311)! Learning rate decreased to 0.00218.
2024-12-02-09:04:50-root-INFO: Loss too large (2291.271->2539.198)! Learning rate decreased to 0.00175.
2024-12-02-09:04:50-root-INFO: Loss too large (2291.271->2380.941)! Learning rate decreased to 0.00140.
2024-12-02-09:04:51-root-INFO: grad norm: 507.136 499.724 86.385
2024-12-02-09:04:51-root-INFO: grad norm: 504.559 496.922 87.453
2024-12-02-09:04:52-root-INFO: grad norm: 485.647 478.443 83.338
2024-12-02-09:04:52-root-INFO: grad norm: 461.980 454.954 80.269
2024-12-02-09:04:53-root-INFO: Loss Change: 2291.271 -> 2235.723
2024-12-02-09:04:53-root-INFO: Regularization Change: 0.000 -> 0.705
2024-12-02-09:04:53-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-09:04:53-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:04:53-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-09:04:53-root-INFO: grad norm: 396.024 390.693 64.765
2024-12-02-09:04:53-root-INFO: Loss too large (2213.084->2539.789)! Learning rate decreased to 0.00228.
2024-12-02-09:04:53-root-INFO: Loss too large (2213.084->2370.426)! Learning rate decreased to 0.00182.
2024-12-02-09:04:53-root-INFO: Loss too large (2213.084->2262.847)! Learning rate decreased to 0.00146.
2024-12-02-09:04:54-root-INFO: grad norm: 384.079 378.299 66.380
2024-12-02-09:04:54-root-INFO: grad norm: 384.014 378.541 64.608
2024-12-02-09:04:55-root-INFO: grad norm: 394.600 388.954 66.514
2024-12-02-09:04:55-root-INFO: grad norm: 410.439 404.578 69.111
2024-12-02-09:04:56-root-INFO: Loss Change: 2213.084 -> 2182.482
2024-12-02-09:04:56-root-INFO: Regularization Change: 0.000 -> 0.611
2024-12-02-09:04:56-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-09:04:56-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-09:04:56-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-09:04:56-root-INFO: grad norm: 464.069 457.960 75.049
2024-12-02-09:04:56-root-INFO: Loss too large (2190.889->2623.941)! Learning rate decreased to 0.00237.
2024-12-02-09:04:56-root-INFO: Loss too large (2190.889->2418.385)! Learning rate decreased to 0.00190.
2024-12-02-09:04:56-root-INFO: Loss too large (2190.889->2273.145)! Learning rate decreased to 0.00152.
2024-12-02-09:04:57-root-INFO: grad norm: 457.761 451.736 74.026
2024-12-02-09:04:57-root-INFO: grad norm: 450.650 444.842 72.116
2024-12-02-09:04:58-root-INFO: grad norm: 440.663 434.786 71.729
2024-12-02-09:04:58-root-INFO: grad norm: 431.342 425.743 69.276
2024-12-02-09:04:59-root-INFO: Loss Change: 2190.889 -> 2145.219
2024-12-02-09:04:59-root-INFO: Regularization Change: 0.000 -> 0.701
2024-12-02-09:04:59-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-09:04:59-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-09:04:59-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-09:04:59-root-INFO: grad norm: 396.495 391.118 65.081
2024-12-02-09:04:59-root-INFO: Loss too large (2127.664->2494.308)! Learning rate decreased to 0.00247.
2024-12-02-09:04:59-root-INFO: Loss too large (2127.664->2310.195)! Learning rate decreased to 0.00198.
2024-12-02-09:04:59-root-INFO: Loss too large (2127.664->2191.205)! Learning rate decreased to 0.00158.
2024-12-02-09:05:00-root-INFO: grad norm: 396.530 391.706 61.665
2024-12-02-09:05:00-root-INFO: grad norm: 402.889 397.472 65.843
2024-12-02-09:05:01-root-INFO: grad norm: 414.255 409.290 63.946
2024-12-02-09:05:01-root-INFO: grad norm: 425.245 419.606 69.018
2024-12-02-09:05:02-root-INFO: Loss Change: 2127.664 -> 2106.848
2024-12-02-09:05:02-root-INFO: Regularization Change: 0.000 -> 0.676
2024-12-02-09:05:02-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-09:05:02-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:05:02-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-09:05:02-root-INFO: grad norm: 479.645 473.676 75.438
2024-12-02-09:05:02-root-INFO: Loss too large (2123.894->2587.322)! Learning rate decreased to 0.00258.
2024-12-02-09:05:02-root-INFO: Loss too large (2123.894->2371.292)! Learning rate decreased to 0.00206.
2024-12-02-09:05:02-root-INFO: Loss too large (2123.894->2212.723)! Learning rate decreased to 0.00165.
2024-12-02-09:05:03-root-INFO: grad norm: 463.718 457.978 72.733
2024-12-02-09:05:03-root-INFO: grad norm: 448.517 443.248 68.547
2024-12-02-09:05:04-root-INFO: grad norm: 432.915 427.383 68.988
2024-12-02-09:05:04-root-INFO: grad norm: 419.417 414.482 64.146
2024-12-02-09:05:05-root-INFO: Loss Change: 2123.894 -> 2063.881
2024-12-02-09:05:05-root-INFO: Regularization Change: 0.000 -> 0.814
2024-12-02-09:05:05-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-09:05:05-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:05:05-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-09:05:05-root-INFO: grad norm: 379.019 374.368 59.198
2024-12-02-09:05:05-root-INFO: Loss too large (2043.130->2411.174)! Learning rate decreased to 0.00269.
2024-12-02-09:05:05-root-INFO: Loss too large (2043.130->2228.677)! Learning rate decreased to 0.00215.
2024-12-02-09:05:05-root-INFO: Loss too large (2043.130->2110.173)! Learning rate decreased to 0.00172.
2024-12-02-09:05:06-root-INFO: grad norm: 383.258 378.975 57.135
2024-12-02-09:05:06-root-INFO: grad norm: 392.582 387.568 62.541
2024-12-02-09:05:06-root-INFO: Loss too large (2032.726->2032.964)! Learning rate decreased to 0.00138.
2024-12-02-09:05:07-root-INFO: grad norm: 264.400 261.237 40.776
2024-12-02-09:05:07-root-INFO: grad norm: 195.563 192.104 36.619
2024-12-02-09:05:08-root-INFO: Loss Change: 2043.130 -> 1958.646
2024-12-02-09:05:08-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-09:05:08-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-09:05:08-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:05:08-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-09:05:08-root-INFO: grad norm: 219.584 216.288 37.904
2024-12-02-09:05:08-root-INFO: Loss too large (1959.264->2108.762)! Learning rate decreased to 0.00280.
2024-12-02-09:05:08-root-INFO: Loss too large (1959.264->2032.377)! Learning rate decreased to 0.00224.
2024-12-02-09:05:08-root-INFO: Loss too large (1959.264->1985.362)! Learning rate decreased to 0.00179.
2024-12-02-09:05:09-root-INFO: grad norm: 262.992 259.262 44.139
2024-12-02-09:05:09-root-INFO: Loss too large (1958.576->1967.055)! Learning rate decreased to 0.00143.
2024-12-02-09:05:09-root-INFO: grad norm: 217.989 215.391 33.551
2024-12-02-09:05:10-root-INFO: grad norm: 187.582 184.116 35.891
2024-12-02-09:05:10-root-INFO: grad norm: 168.025 165.839 27.016
2024-12-02-09:05:11-root-INFO: Loss Change: 1959.264 -> 1918.234
2024-12-02-09:05:11-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-02-09:05:11-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-09:05:11-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-09:05:11-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-09:05:11-root-INFO: grad norm: 119.895 116.476 28.432
2024-12-02-09:05:11-root-INFO: Loss too large (1904.904->1939.681)! Learning rate decreased to 0.00291.
2024-12-02-09:05:11-root-INFO: Loss too large (1904.904->1918.964)! Learning rate decreased to 0.00233.
2024-12-02-09:05:12-root-INFO: Loss too large (1904.904->1907.234)! Learning rate decreased to 0.00187.
2024-12-02-09:05:12-root-INFO: grad norm: 150.936 148.923 24.564
2024-12-02-09:05:12-root-INFO: Loss too large (1901.024->1901.664)! Learning rate decreased to 0.00149.
2024-12-02-09:05:13-root-INFO: grad norm: 142.494 139.157 30.661
2024-12-02-09:05:13-root-INFO: grad norm: 137.988 136.100 22.751
2024-12-02-09:05:14-root-INFO: grad norm: 135.560 132.275 29.660
2024-12-02-09:05:14-root-INFO: Loss Change: 1904.904 -> 1879.039
2024-12-02-09:05:14-root-INFO: Regularization Change: 0.000 -> 0.243
2024-12-02-09:05:14-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-09:05:14-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-09:05:14-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-09:05:14-root-INFO: grad norm: 197.903 195.165 32.805
2024-12-02-09:05:15-root-INFO: Loss too large (1880.857->2029.730)! Learning rate decreased to 0.00304.
2024-12-02-09:05:15-root-INFO: Loss too large (1880.857->1958.901)! Learning rate decreased to 0.00243.
2024-12-02-09:05:15-root-INFO: Loss too large (1880.857->1913.995)! Learning rate decreased to 0.00194.
2024-12-02-09:05:15-root-INFO: Loss too large (1880.857->1887.522)! Learning rate decreased to 0.00155.
2024-12-02-09:05:15-root-INFO: grad norm: 184.273 181.014 34.505
2024-12-02-09:05:16-root-INFO: grad norm: 174.634 172.636 26.339
2024-12-02-09:05:16-root-INFO: grad norm: 169.491 166.213 33.173
2024-12-02-09:05:17-root-INFO: grad norm: 165.805 163.977 24.555
2024-12-02-09:05:17-root-INFO: Loss Change: 1880.857 -> 1850.222
2024-12-02-09:05:17-root-INFO: Regularization Change: 0.000 -> 0.269
2024-12-02-09:05:17-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-09:05:17-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:05:17-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-09:05:18-root-INFO: grad norm: 109.694 106.313 27.027
2024-12-02-09:05:18-root-INFO: Loss too large (1836.781->1865.719)! Learning rate decreased to 0.00316.
2024-12-02-09:05:18-root-INFO: Loss too large (1836.781->1847.906)! Learning rate decreased to 0.00253.
2024-12-02-09:05:18-root-INFO: Loss too large (1836.781->1837.886)! Learning rate decreased to 0.00202.
2024-12-02-09:05:18-root-INFO: grad norm: 135.138 133.462 21.213
2024-12-02-09:05:19-root-INFO: grad norm: 179.006 175.698 34.257
2024-12-02-09:05:19-root-INFO: Loss too large (1831.924->1837.844)! Learning rate decreased to 0.00162.
2024-12-02-09:05:20-root-INFO: grad norm: 166.341 164.666 23.547
2024-12-02-09:05:20-root-INFO: grad norm: 157.218 154.062 31.342
2024-12-02-09:05:20-root-INFO: Loss Change: 1836.781 -> 1814.059
2024-12-02-09:05:20-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-02-09:05:20-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-09:05:20-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:05:21-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-09:05:21-root-INFO: grad norm: 166.142 164.455 23.612
2024-12-02-09:05:21-root-INFO: Loss too large (1813.201->1931.205)! Learning rate decreased to 0.00329.
2024-12-02-09:05:21-root-INFO: Loss too large (1813.201->1872.782)! Learning rate decreased to 0.00263.
2024-12-02-09:05:21-root-INFO: Loss too large (1813.201->1836.999)! Learning rate decreased to 0.00211.
2024-12-02-09:05:21-root-INFO: Loss too large (1813.201->1816.564)! Learning rate decreased to 0.00168.
2024-12-02-09:05:22-root-INFO: grad norm: 154.258 151.179 30.666
2024-12-02-09:05:22-root-INFO: grad norm: 147.472 145.830 21.949
2024-12-02-09:05:23-root-INFO: grad norm: 141.784 138.791 28.979
2024-12-02-09:05:23-root-INFO: grad norm: 139.177 137.513 21.454
2024-12-02-09:05:24-root-INFO: Loss Change: 1813.201 -> 1784.788
2024-12-02-09:05:24-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-09:05:24-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-09:05:24-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-09:05:24-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-09:05:24-root-INFO: grad norm: 81.661 78.018 24.121
2024-12-02-09:05:24-root-INFO: grad norm: 177.778 176.139 24.086
2024-12-02-09:05:25-root-INFO: Loss too large (1774.451->1901.424)! Learning rate decreased to 0.00342.
2024-12-02-09:05:25-root-INFO: Loss too large (1774.451->1837.416)! Learning rate decreased to 0.00274.
2024-12-02-09:05:25-root-INFO: Loss too large (1774.451->1798.272)! Learning rate decreased to 0.00219.
2024-12-02-09:05:25-root-INFO: Loss too large (1774.451->1775.940)! Learning rate decreased to 0.00175.
2024-12-02-09:05:25-root-INFO: grad norm: 148.206 145.224 29.578
2024-12-02-09:05:26-root-INFO: grad norm: 125.385 123.945 18.952
2024-12-02-09:05:26-root-INFO: grad norm: 107.002 104.076 24.849
2024-12-02-09:05:27-root-INFO: Loss Change: 1776.881 -> 1742.833
2024-12-02-09:05:27-root-INFO: Regularization Change: 0.000 -> 0.413
2024-12-02-09:05:27-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-09:05:27-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-09:05:27-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-09:05:27-root-INFO: grad norm: 112.627 111.004 19.052
2024-12-02-09:05:27-root-INFO: Loss too large (1737.360->1777.483)! Learning rate decreased to 0.00356.
2024-12-02-09:05:27-root-INFO: Loss too large (1737.360->1754.041)! Learning rate decreased to 0.00285.
2024-12-02-09:05:27-root-INFO: Loss too large (1737.360->1740.687)! Learning rate decreased to 0.00228.
2024-12-02-09:05:28-root-INFO: grad norm: 141.304 138.369 28.648
2024-12-02-09:05:28-root-INFO: Loss too large (1733.598->1734.805)! Learning rate decreased to 0.00182.
2024-12-02-09:05:29-root-INFO: grad norm: 136.032 134.298 21.650
2024-12-02-09:05:29-root-INFO: grad norm: 134.946 132.102 27.555
2024-12-02-09:05:29-root-INFO: grad norm: 149.640 147.637 24.397
2024-12-02-09:05:30-root-INFO: Loss Change: 1737.360 -> 1715.820
2024-12-02-09:05:30-root-INFO: Regularization Change: 0.000 -> 0.319
2024-12-02-09:05:30-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-09:05:30-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:05:30-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-09:05:30-root-INFO: grad norm: 121.794 119.323 24.410
2024-12-02-09:05:30-root-INFO: Loss too large (1700.902->1793.700)! Learning rate decreased to 0.00371.
2024-12-02-09:05:30-root-INFO: Loss too large (1700.902->1746.906)! Learning rate decreased to 0.00297.
2024-12-02-09:05:31-root-INFO: Loss too large (1700.902->1718.595)! Learning rate decreased to 0.00237.
2024-12-02-09:05:31-root-INFO: Loss too large (1700.902->1702.971)! Learning rate decreased to 0.00190.
2024-12-02-09:05:31-root-INFO: grad norm: 153.221 150.967 26.185
2024-12-02-09:05:31-root-INFO: Loss too large (1695.222->1697.387)! Learning rate decreased to 0.00152.
2024-12-02-09:05:32-root-INFO: grad norm: 130.384 127.812 25.768
2024-12-02-09:05:32-root-INFO: grad norm: 101.533 99.400 20.705
2024-12-02-09:05:33-root-INFO: grad norm: 94.717 92.204 21.674
2024-12-02-09:05:33-root-INFO: Loss Change: 1700.902 -> 1670.824
2024-12-02-09:05:33-root-INFO: Regularization Change: 0.000 -> 0.226
2024-12-02-09:05:33-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-09:05:33-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:05:33-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-09:05:34-root-INFO: grad norm: 167.719 165.117 29.428
2024-12-02-09:05:34-root-INFO: Loss too large (1672.361->1752.285)! Learning rate decreased to 0.00386.
2024-12-02-09:05:34-root-INFO: Loss too large (1672.361->1726.558)! Learning rate decreased to 0.00309.
2024-12-02-09:05:34-root-INFO: Loss too large (1672.361->1704.922)! Learning rate decreased to 0.00247.
2024-12-02-09:05:34-root-INFO: Loss too large (1672.361->1688.025)! Learning rate decreased to 0.00198.
2024-12-02-09:05:34-root-INFO: Loss too large (1672.361->1675.960)! Learning rate decreased to 0.00158.
2024-12-02-09:05:35-root-INFO: grad norm: 147.033 144.527 27.027
2024-12-02-09:05:35-root-INFO: grad norm: 121.705 119.421 23.467
2024-12-02-09:05:36-root-INFO: grad norm: 117.058 114.690 23.428
2024-12-02-09:05:36-root-INFO: grad norm: 111.213 109.042 21.869
2024-12-02-09:05:37-root-INFO: Loss Change: 1672.361 -> 1639.482
2024-12-02-09:05:37-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-09:05:37-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-09:05:37-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:05:37-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-09:05:37-root-INFO: grad norm: 84.867 82.495 19.925
2024-12-02-09:05:37-root-INFO: Loss too large (1632.947->1652.009)! Learning rate decreased to 0.00401.
2024-12-02-09:05:37-root-INFO: Loss too large (1632.947->1638.799)! Learning rate decreased to 0.00321.
2024-12-02-09:05:38-root-INFO: grad norm: 205.048 202.508 32.176
2024-12-02-09:05:38-root-INFO: Loss too large (1631.548->1683.559)! Learning rate decreased to 0.00257.
2024-12-02-09:05:38-root-INFO: Loss too large (1631.548->1663.020)! Learning rate decreased to 0.00206.
2024-12-02-09:05:38-root-INFO: Loss too large (1631.548->1645.444)! Learning rate decreased to 0.00164.
2024-12-02-09:05:38-root-INFO: Loss too large (1631.548->1632.274)! Learning rate decreased to 0.00132.
2024-12-02-09:05:39-root-INFO: grad norm: 145.276 142.869 26.337
2024-12-02-09:05:39-root-INFO: grad norm: 88.812 86.545 19.936
2024-12-02-09:05:40-root-INFO: grad norm: 77.239 74.900 18.867
2024-12-02-09:05:40-root-INFO: Loss Change: 1632.947 -> 1601.356
2024-12-02-09:05:40-root-INFO: Regularization Change: 0.000 -> 0.266
2024-12-02-09:05:40-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-09:05:40-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-09:05:40-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-09:05:40-root-INFO: grad norm: 101.365 99.212 20.778
2024-12-02-09:05:41-root-INFO: Loss too large (1598.685->1633.986)! Learning rate decreased to 0.00417.
2024-12-02-09:05:41-root-INFO: Loss too large (1598.685->1619.544)! Learning rate decreased to 0.00334.
2024-12-02-09:05:41-root-INFO: Loss too large (1598.685->1608.846)! Learning rate decreased to 0.00267.
2024-12-02-09:05:41-root-INFO: Loss too large (1598.685->1601.582)! Learning rate decreased to 0.00214.
2024-12-02-09:05:41-root-INFO: grad norm: 139.059 136.838 24.754
2024-12-02-09:05:42-root-INFO: Loss too large (1597.082->1599.676)! Learning rate decreased to 0.00171.
2024-12-02-09:05:42-root-INFO: grad norm: 157.733 155.597 25.871
2024-12-02-09:05:42-root-INFO: Loss too large (1590.763->1591.313)! Learning rate decreased to 0.00137.
2024-12-02-09:05:43-root-INFO: grad norm: 126.866 124.777 22.928
2024-12-02-09:05:43-root-INFO: grad norm: 95.000 93.009 19.344
2024-12-02-09:05:44-root-INFO: Loss Change: 1598.685 -> 1572.979
2024-12-02-09:05:44-root-INFO: Regularization Change: 0.000 -> 0.212
2024-12-02-09:05:44-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-09:05:44-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-09:05:44-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-09:05:44-root-INFO: grad norm: 61.518 58.927 17.664
2024-12-02-09:05:44-root-INFO: grad norm: 95.819 93.803 19.552
2024-12-02-09:05:45-root-INFO: Loss too large (1554.902->1592.252)! Learning rate decreased to 0.00434.
2024-12-02-09:05:45-root-INFO: Loss too large (1554.902->1577.080)! Learning rate decreased to 0.00347.
2024-12-02-09:05:45-root-INFO: Loss too large (1554.902->1565.938)! Learning rate decreased to 0.00278.
2024-12-02-09:05:45-root-INFO: Loss too large (1554.902->1558.443)! Learning rate decreased to 0.00222.
2024-12-02-09:05:46-root-INFO: grad norm: 137.085 135.244 22.386
2024-12-02-09:05:46-root-INFO: Loss too large (1553.836->1561.084)! Learning rate decreased to 0.00178.
2024-12-02-09:05:46-root-INFO: grad norm: 172.799 170.645 27.198
2024-12-02-09:05:46-root-INFO: Loss too large (1550.657->1554.879)! Learning rate decreased to 0.00142.
2024-12-02-09:05:47-root-INFO: grad norm: 143.052 141.170 23.133
2024-12-02-09:05:47-root-INFO: Loss Change: 1569.384 -> 1537.617
2024-12-02-09:05:47-root-INFO: Regularization Change: 0.000 -> 0.438
2024-12-02-09:05:47-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-09:05:47-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:05:47-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-09:05:47-root-INFO: grad norm: 190.633 188.188 30.434
2024-12-02-09:05:48-root-INFO: Loss too large (1540.478->1649.020)! Learning rate decreased to 0.00451.
2024-12-02-09:05:48-root-INFO: Loss too large (1540.478->1628.013)! Learning rate decreased to 0.00361.
2024-12-02-09:05:48-root-INFO: Loss too large (1540.478->1605.089)! Learning rate decreased to 0.00289.
2024-12-02-09:05:48-root-INFO: Loss too large (1540.478->1582.256)! Learning rate decreased to 0.00231.
2024-12-02-09:05:48-root-INFO: Loss too large (1540.478->1562.184)! Learning rate decreased to 0.00185.
2024-12-02-09:05:48-root-INFO: Loss too large (1540.478->1546.748)! Learning rate decreased to 0.00148.
2024-12-02-09:05:49-root-INFO: grad norm: 157.595 155.738 24.126
2024-12-02-09:05:49-root-INFO: grad norm: 120.649 118.616 22.055
2024-12-02-09:05:50-root-INFO: grad norm: 110.677 109.095 18.645
2024-12-02-09:05:50-root-INFO: grad norm: 99.598 97.692 19.392
2024-12-02-09:05:51-root-INFO: Loss Change: 1540.478 -> 1512.643
2024-12-02-09:05:51-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-09:05:51-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-09:05:51-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:05:51-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-09:05:51-root-INFO: grad norm: 81.237 79.657 15.941
2024-12-02-09:05:51-root-INFO: Loss too large (1510.022->1555.722)! Learning rate decreased to 0.00469.
2024-12-02-09:05:51-root-INFO: Loss too large (1510.022->1534.662)! Learning rate decreased to 0.00375.
2024-12-02-09:05:51-root-INFO: Loss too large (1510.022->1521.248)! Learning rate decreased to 0.00300.
2024-12-02-09:05:51-root-INFO: Loss too large (1510.022->1513.234)! Learning rate decreased to 0.00240.
2024-12-02-09:05:52-root-INFO: grad norm: 146.543 144.490 24.443
2024-12-02-09:05:52-root-INFO: Loss too large (1508.797->1524.178)! Learning rate decreased to 0.00192.
2024-12-02-09:05:52-root-INFO: Loss too large (1508.797->1513.026)! Learning rate decreased to 0.00154.
2024-12-02-09:05:53-root-INFO: grad norm: 138.044 136.407 21.197
2024-12-02-09:05:53-root-INFO: grad norm: 127.238 125.333 21.937
2024-12-02-09:05:54-root-INFO: grad norm: 121.880 120.337 19.330
2024-12-02-09:05:54-root-INFO: Loss Change: 1510.022 -> 1491.426
2024-12-02-09:05:54-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-02-09:05:54-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-09:05:54-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-09:05:54-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-09:05:55-root-INFO: grad norm: 171.597 169.267 28.178
2024-12-02-09:05:55-root-INFO: Loss too large (1497.232->1607.492)! Learning rate decreased to 0.00488.
2024-12-02-09:05:55-root-INFO: Loss too large (1497.232->1585.887)! Learning rate decreased to 0.00390.
2024-12-02-09:05:55-root-INFO: Loss too large (1497.232->1562.077)! Learning rate decreased to 0.00312.
2024-12-02-09:05:55-root-INFO: Loss too large (1497.232->1538.611)! Learning rate decreased to 0.00250.
2024-12-02-09:05:55-root-INFO: Loss too large (1497.232->1518.465)! Learning rate decreased to 0.00200.
2024-12-02-09:05:55-root-INFO: Loss too large (1497.232->1503.402)! Learning rate decreased to 0.00160.
2024-12-02-09:05:56-root-INFO: grad norm: 156.533 154.637 24.288
2024-12-02-09:05:56-root-INFO: grad norm: 138.762 136.725 23.689
2024-12-02-09:05:57-root-INFO: grad norm: 131.009 129.370 20.656
2024-12-02-09:05:58-root-INFO: grad norm: 121.548 119.659 21.345
2024-12-02-09:05:58-root-INFO: Loss Change: 1497.232 -> 1471.741
2024-12-02-09:05:58-root-INFO: Regularization Change: 0.000 -> 0.212
2024-12-02-09:05:58-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-09:05:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-09:05:58-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-09:05:58-root-INFO: grad norm: 82.056 80.568 15.556
2024-12-02-09:05:58-root-INFO: Loss too large (1463.450->1515.499)! Learning rate decreased to 0.00507.
2024-12-02-09:05:59-root-INFO: Loss too large (1463.450->1492.724)! Learning rate decreased to 0.00405.
2024-12-02-09:05:59-root-INFO: Loss too large (1463.450->1477.775)! Learning rate decreased to 0.00324.
2024-12-02-09:05:59-root-INFO: Loss too large (1463.450->1468.542)! Learning rate decreased to 0.00259.
2024-12-02-09:05:59-root-INFO: grad norm: 149.844 147.951 23.744
2024-12-02-09:06:00-root-INFO: Loss too large (1463.234->1480.532)! Learning rate decreased to 0.00208.
2024-12-02-09:06:00-root-INFO: Loss too large (1463.234->1467.702)! Learning rate decreased to 0.00166.
2024-12-02-09:06:00-root-INFO: grad norm: 139.989 138.443 20.750
2024-12-02-09:06:01-root-INFO: grad norm: 128.238 126.471 21.213
2024-12-02-09:06:01-root-INFO: grad norm: 121.308 119.866 18.649
2024-12-02-09:06:02-root-INFO: Loss Change: 1463.450 -> 1444.067
2024-12-02-09:06:02-root-INFO: Regularization Change: 0.000 -> 0.223
2024-12-02-09:06:02-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-09:06:02-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:06:02-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-09:06:02-root-INFO: grad norm: 166.885 164.354 28.951
2024-12-02-09:06:02-root-INFO: Loss too large (1456.599->1568.843)! Learning rate decreased to 0.00527.
2024-12-02-09:06:02-root-INFO: Loss too large (1456.599->1546.497)! Learning rate decreased to 0.00421.
2024-12-02-09:06:03-root-INFO: Loss too large (1456.599->1521.685)! Learning rate decreased to 0.00337.
2024-12-02-09:06:03-root-INFO: Loss too large (1456.599->1497.174)! Learning rate decreased to 0.00270.
2024-12-02-09:06:03-root-INFO: Loss too large (1456.599->1476.271)! Learning rate decreased to 0.00216.
2024-12-02-09:06:03-root-INFO: Loss too large (1456.599->1460.881)! Learning rate decreased to 0.00173.
2024-12-02-09:06:04-root-INFO: grad norm: 153.070 151.309 23.156
2024-12-02-09:06:04-root-INFO: grad norm: 137.733 135.568 24.326
2024-12-02-09:06:05-root-INFO: grad norm: 128.926 127.360 20.037
2024-12-02-09:06:05-root-INFO: grad norm: 118.631 116.666 21.502
2024-12-02-09:06:06-root-INFO: Loss Change: 1456.599 -> 1426.103
2024-12-02-09:06:06-root-INFO: Regularization Change: 0.000 -> 0.265
2024-12-02-09:06:06-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-09:06:06-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:06:06-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-09:06:06-root-INFO: grad norm: 83.204 81.584 16.338
2024-12-02-09:06:06-root-INFO: Loss too large (1420.293->1462.397)! Learning rate decreased to 0.00547.
2024-12-02-09:06:06-root-INFO: Loss too large (1420.293->1443.134)! Learning rate decreased to 0.00438.
2024-12-02-09:06:06-root-INFO: Loss too large (1420.293->1430.447)! Learning rate decreased to 0.00350.
2024-12-02-09:06:07-root-INFO: Loss too large (1420.293->1422.598)! Learning rate decreased to 0.00280.
2024-12-02-09:06:07-root-INFO: grad norm: 137.064 135.250 22.229
2024-12-02-09:06:07-root-INFO: Loss too large (1418.122->1428.509)! Learning rate decreased to 0.00224.
2024-12-02-09:06:08-root-INFO: grad norm: 164.145 162.411 23.793
2024-12-02-09:06:08-root-INFO: grad norm: 184.136 182.208 26.578
2024-12-02-09:06:09-root-INFO: grad norm: 188.637 186.703 26.941
2024-12-02-09:06:09-root-INFO: Loss Change: 1420.293 -> 1400.798
2024-12-02-09:06:09-root-INFO: Regularization Change: 0.000 -> 0.480
2024-12-02-09:06:09-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-09:06:09-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:06:09-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-09:06:09-root-INFO: grad norm: 210.119 207.874 30.634
2024-12-02-09:06:10-root-INFO: Loss too large (1411.221->1540.113)! Learning rate decreased to 0.00568.
2024-12-02-09:06:10-root-INFO: Loss too large (1411.221->1511.954)! Learning rate decreased to 0.00455.
2024-12-02-09:06:10-root-INFO: Loss too large (1411.221->1478.950)! Learning rate decreased to 0.00364.
2024-12-02-09:06:10-root-INFO: Loss too large (1411.221->1444.721)! Learning rate decreased to 0.00291.
2024-12-02-09:06:10-root-INFO: Loss too large (1411.221->1415.303)! Learning rate decreased to 0.00233.
2024-12-02-09:06:11-root-INFO: grad norm: 191.119 189.110 27.638
2024-12-02-09:06:11-root-INFO: grad norm: 170.004 167.995 26.059
2024-12-02-09:06:11-root-INFO: grad norm: 154.880 152.775 25.450
2024-12-02-09:06:12-root-INFO: grad norm: 137.237 135.369 22.562
2024-12-02-09:06:12-root-INFO: Loss Change: 1411.221 -> 1327.926
2024-12-02-09:06:12-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-09:06:12-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-09:06:12-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-09:06:12-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-09:06:13-root-INFO: grad norm: 95.341 92.892 21.470
2024-12-02-09:06:13-root-INFO: Loss too large (1322.636->1364.367)! Learning rate decreased to 0.00590.
2024-12-02-09:06:13-root-INFO: Loss too large (1322.636->1342.108)! Learning rate decreased to 0.00472.
2024-12-02-09:06:13-root-INFO: Loss too large (1322.636->1328.037)! Learning rate decreased to 0.00378.
2024-12-02-09:06:14-root-INFO: grad norm: 149.998 148.535 20.900
2024-12-02-09:06:14-root-INFO: Loss too large (1319.789->1345.005)! Learning rate decreased to 0.00302.
2024-12-02-09:06:14-root-INFO: Loss too large (1319.789->1322.031)! Learning rate decreased to 0.00242.
2024-12-02-09:06:14-root-INFO: grad norm: 151.365 149.094 26.118
2024-12-02-09:06:15-root-INFO: grad norm: 150.827 149.435 20.444
2024-12-02-09:06:15-root-INFO: grad norm: 148.716 146.552 25.279
2024-12-02-09:06:16-root-INFO: Loss Change: 1322.636 -> 1292.815
2024-12-02-09:06:16-root-INFO: Regularization Change: 0.000 -> 0.614
2024-12-02-09:06:16-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-09:06:16-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-09:06:16-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-09:06:16-root-INFO: grad norm: 165.398 163.838 22.664
2024-12-02-09:06:16-root-INFO: Loss too large (1299.838->1445.367)! Learning rate decreased to 0.00613.
2024-12-02-09:06:16-root-INFO: Loss too large (1299.838->1403.896)! Learning rate decreased to 0.00490.
2024-12-02-09:06:16-root-INFO: Loss too large (1299.838->1359.911)! Learning rate decreased to 0.00392.
2024-12-02-09:06:16-root-INFO: Loss too large (1299.838->1321.526)! Learning rate decreased to 0.00314.
2024-12-02-09:06:17-root-INFO: grad norm: 212.930 210.629 31.220
2024-12-02-09:06:17-root-INFO: Loss too large (1294.842->1308.908)! Learning rate decreased to 0.00251.
2024-12-02-09:06:18-root-INFO: grad norm: 166.032 164.620 21.608
2024-12-02-09:06:18-root-INFO: grad norm: 111.806 109.924 20.430
2024-12-02-09:06:19-root-INFO: grad norm: 100.942 99.764 15.379
2024-12-02-09:06:19-root-INFO: Loss Change: 1299.838 -> 1247.835
2024-12-02-09:06:19-root-INFO: Regularization Change: 0.000 -> 0.583
2024-12-02-09:06:19-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-09:06:19-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:06:19-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-09:06:19-root-INFO: grad norm: 64.489 62.260 16.810
2024-12-02-09:06:20-root-INFO: grad norm: 149.933 148.647 19.594
2024-12-02-09:06:20-root-INFO: Loss too large (1244.702->1379.265)! Learning rate decreased to 0.00636.
2024-12-02-09:06:20-root-INFO: Loss too large (1244.702->1337.531)! Learning rate decreased to 0.00509.
2024-12-02-09:06:20-root-INFO: Loss too large (1244.702->1295.744)! Learning rate decreased to 0.00407.
2024-12-02-09:06:20-root-INFO: Loss too large (1244.702->1260.919)! Learning rate decreased to 0.00326.
2024-12-02-09:06:21-root-INFO: grad norm: 190.331 188.362 27.306
2024-12-02-09:06:21-root-INFO: Loss too large (1238.000->1250.355)! Learning rate decreased to 0.00261.
2024-12-02-09:06:21-root-INFO: grad norm: 143.455 142.181 19.075
2024-12-02-09:06:22-root-INFO: grad norm: 83.002 81.414 16.161
2024-12-02-09:06:22-root-INFO: Loss Change: 1246.812 -> 1205.085
2024-12-02-09:06:22-root-INFO: Regularization Change: 0.000 -> 0.865
2024-12-02-09:06:22-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-09:06:22-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:06:22-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-09:06:22-root-INFO: grad norm: 90.204 89.000 14.686
2024-12-02-09:06:23-root-INFO: Loss too large (1204.448->1276.857)! Learning rate decreased to 0.00660.
2024-12-02-09:06:23-root-INFO: Loss too large (1204.448->1241.714)! Learning rate decreased to 0.00528.
2024-12-02-09:06:23-root-INFO: Loss too large (1204.448->1217.868)! Learning rate decreased to 0.00423.
2024-12-02-09:06:23-root-INFO: grad norm: 163.758 161.972 24.117
2024-12-02-09:06:24-root-INFO: Loss too large (1204.265->1231.773)! Learning rate decreased to 0.00338.
2024-12-02-09:06:24-root-INFO: Loss too large (1204.265->1212.344)! Learning rate decreased to 0.00270.
2024-12-02-09:06:24-root-INFO: grad norm: 122.292 121.182 16.442
2024-12-02-09:06:25-root-INFO: grad norm: 68.718 67.191 14.406
2024-12-02-09:06:25-root-INFO: grad norm: 60.706 59.534 11.869
2024-12-02-09:06:25-root-INFO: Loss Change: 1204.448 -> 1170.382
2024-12-02-09:06:25-root-INFO: Regularization Change: 0.000 -> 0.475
2024-12-02-09:06:25-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-09:06:25-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-09:06:26-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-09:06:26-root-INFO: grad norm: 47.259 45.489 12.810
2024-12-02-09:06:26-root-INFO: grad norm: 48.761 47.540 10.842
2024-12-02-09:06:27-root-INFO: grad norm: 108.107 106.649 17.695
2024-12-02-09:06:27-root-INFO: Loss too large (1144.286->1217.892)! Learning rate decreased to 0.00685.
2024-12-02-09:06:27-root-INFO: Loss too large (1144.286->1187.656)! Learning rate decreased to 0.00548.
2024-12-02-09:06:27-root-INFO: Loss too large (1144.286->1167.084)! Learning rate decreased to 0.00439.
2024-12-02-09:06:27-root-INFO: Loss too large (1144.286->1153.380)! Learning rate decreased to 0.00351.
2024-12-02-09:06:27-root-INFO: Loss too large (1144.286->1144.624)! Learning rate decreased to 0.00281.
2024-12-02-09:06:28-root-INFO: grad norm: 89.817 88.787 13.563
2024-12-02-09:06:28-root-INFO: grad norm: 69.976 68.653 13.541
2024-12-02-09:06:29-root-INFO: Loss Change: 1169.116 -> 1126.271
2024-12-02-09:06:29-root-INFO: Regularization Change: 0.000 -> 1.163
2024-12-02-09:06:29-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-09:06:29-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-09:06:29-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-09:06:29-root-INFO: grad norm: 82.769 81.624 13.721
2024-12-02-09:06:29-root-INFO: Loss too large (1128.149->1201.083)! Learning rate decreased to 0.00711.
2024-12-02-09:06:29-root-INFO: Loss too large (1128.149->1164.681)! Learning rate decreased to 0.00569.
2024-12-02-09:06:29-root-INFO: Loss too large (1128.149->1140.767)! Learning rate decreased to 0.00455.
2024-12-02-09:06:30-root-INFO: grad norm: 154.909 153.324 22.104
2024-12-02-09:06:30-root-INFO: Loss too large (1127.660->1154.602)! Learning rate decreased to 0.00364.
2024-12-02-09:06:30-root-INFO: Loss too large (1127.660->1136.901)! Learning rate decreased to 0.00291.
2024-12-02-09:06:31-root-INFO: grad norm: 111.818 110.807 15.001
2024-12-02-09:06:31-root-INFO: grad norm: 52.403 51.002 12.037
2024-12-02-09:06:32-root-INFO: grad norm: 47.306 46.085 10.680
2024-12-02-09:06:32-root-INFO: Loss Change: 1128.149 -> 1097.391
2024-12-02-09:06:32-root-INFO: Regularization Change: 0.000 -> 0.450
2024-12-02-09:06:32-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-09:06:32-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-09:06:32-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-09:06:32-root-INFO: grad norm: 45.320 43.685 12.061
2024-12-02-09:06:33-root-INFO: grad norm: 69.395 68.264 12.478
2024-12-02-09:06:33-root-INFO: Loss too large (1083.117->1102.288)! Learning rate decreased to 0.00738.
2024-12-02-09:06:33-root-INFO: Loss too large (1083.117->1091.628)! Learning rate decreased to 0.00590.
2024-12-02-09:06:33-root-INFO: Loss too large (1083.117->1084.936)! Learning rate decreased to 0.00472.
2024-12-02-09:06:34-root-INFO: grad norm: 85.030 84.053 12.848
2024-12-02-09:06:34-root-INFO: grad norm: 139.779 138.334 20.049
2024-12-02-09:06:34-root-INFO: Loss too large (1078.245->1097.178)! Learning rate decreased to 0.00378.
2024-12-02-09:06:34-root-INFO: Loss too large (1078.245->1082.931)! Learning rate decreased to 0.00302.
2024-12-02-09:06:35-root-INFO: grad norm: 94.750 93.824 13.210
2024-12-02-09:06:35-root-INFO: Loss Change: 1095.867 -> 1059.302
2024-12-02-09:06:35-root-INFO: Regularization Change: 0.000 -> 0.919
2024-12-02-09:06:35-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-09:06:35-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-09:06:35-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-09:06:36-root-INFO: grad norm: 39.810 38.304 10.846
2024-12-02-09:06:36-root-INFO: grad norm: 40.842 39.680 9.674
2024-12-02-09:06:37-root-INFO: grad norm: 50.394 48.965 11.918
2024-12-02-09:06:37-root-INFO: grad norm: 55.657 54.722 10.154
2024-12-02-09:06:38-root-INFO: grad norm: 75.190 73.939 13.655
2024-12-02-09:06:38-root-INFO: Loss too large (1014.455->1048.173)! Learning rate decreased to 0.00765.
2024-12-02-09:06:38-root-INFO: Loss too large (1014.455->1025.302)! Learning rate decreased to 0.00612.
2024-12-02-09:06:38-root-INFO: Loss Change: 1057.462 -> 1013.412
2024-12-02-09:06:38-root-INFO: Regularization Change: 0.000 -> 2.567
2024-12-02-09:06:38-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-09:06:38-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-09:06:38-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-09:06:39-root-INFO: grad norm: 94.172 93.127 13.988
2024-12-02-09:06:39-root-INFO: Loss too large (1005.854->1054.610)! Learning rate decreased to 0.00794.
2024-12-02-09:06:39-root-INFO: Loss too large (1005.854->1032.719)! Learning rate decreased to 0.00635.
2024-12-02-09:06:39-root-INFO: Loss too large (1005.854->1018.354)! Learning rate decreased to 0.00508.
2024-12-02-09:06:39-root-INFO: Loss too large (1005.854->1009.086)! Learning rate decreased to 0.00406.
2024-12-02-09:06:40-root-INFO: grad norm: 84.352 83.474 12.139
2024-12-02-09:06:40-root-INFO: grad norm: 66.060 64.994 11.818
2024-12-02-09:06:41-root-INFO: grad norm: 62.587 61.696 10.525
2024-12-02-09:06:41-root-INFO: grad norm: 57.018 55.953 10.971
2024-12-02-09:06:42-root-INFO: Loss Change: 1005.854 -> 977.659
2024-12-02-09:06:42-root-INFO: Regularization Change: 0.000 -> 0.566
2024-12-02-09:06:42-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-09:06:42-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:06:42-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-09:06:42-root-INFO: grad norm: 64.850 63.920 10.946
2024-12-02-09:06:42-root-INFO: Loss too large (978.138->1017.719)! Learning rate decreased to 0.00823.
2024-12-02-09:06:42-root-INFO: Loss too large (978.138->991.921)! Learning rate decreased to 0.00659.
2024-12-02-09:06:42-root-INFO: Loss too large (978.138->978.457)! Learning rate decreased to 0.00527.
2024-12-02-09:06:43-root-INFO: grad norm: 86.763 85.709 13.483
2024-12-02-09:06:43-root-INFO: Loss too large (972.395->975.080)! Learning rate decreased to 0.00422.
2024-12-02-09:06:43-root-INFO: grad norm: 74.065 73.248 10.971
2024-12-02-09:06:44-root-INFO: grad norm: 52.141 51.096 10.388
2024-12-02-09:06:44-root-INFO: grad norm: 48.347 47.426 9.390
2024-12-02-09:06:45-root-INFO: Loss Change: 978.138 -> 949.657
2024-12-02-09:06:45-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-02-09:06:45-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-09:06:45-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:06:45-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-09:06:45-root-INFO: grad norm: 37.465 36.172 9.758
2024-12-02-09:06:45-root-INFO: grad norm: 39.326 38.335 8.772
2024-12-02-09:06:46-root-INFO: grad norm: 67.110 66.020 12.046
2024-12-02-09:06:46-root-INFO: Loss too large (927.635->943.933)! Learning rate decreased to 0.00854.
2024-12-02-09:06:46-root-INFO: Loss too large (927.635->933.886)! Learning rate decreased to 0.00683.
2024-12-02-09:06:47-root-INFO: grad norm: 80.750 80.011 10.896
2024-12-02-09:06:47-root-INFO: grad norm: 153.090 151.652 20.931
2024-12-02-09:06:47-root-INFO: Loss too large (926.063->968.444)! Learning rate decreased to 0.00546.
2024-12-02-09:06:48-root-INFO: Loss too large (926.063->944.358)! Learning rate decreased to 0.00437.
2024-12-02-09:06:48-root-INFO: Loss too large (926.063->928.546)! Learning rate decreased to 0.00350.
2024-12-02-09:06:48-root-INFO: Loss Change: 947.647 -> 918.488
2024-12-02-09:06:48-root-INFO: Regularization Change: 0.000 -> 1.666
2024-12-02-09:06:48-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-09:06:48-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-09:06:48-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-09:06:48-root-INFO: grad norm: 87.499 86.674 11.987
2024-12-02-09:06:48-root-INFO: Loss too large (921.195->972.599)! Learning rate decreased to 0.00885.
2024-12-02-09:06:49-root-INFO: Loss too large (921.195->927.810)! Learning rate decreased to 0.00708.
2024-12-02-09:06:49-root-INFO: grad norm: 110.234 109.125 15.598
2024-12-02-09:06:49-root-INFO: Loss too large (907.640->921.878)! Learning rate decreased to 0.00566.
2024-12-02-09:06:49-root-INFO: Loss too large (907.640->909.804)! Learning rate decreased to 0.00453.
2024-12-02-09:06:50-root-INFO: grad norm: 69.465 68.706 10.242
2024-12-02-09:06:50-root-INFO: grad norm: 31.581 30.413 8.510
2024-12-02-09:06:51-root-INFO: grad norm: 30.384 29.239 8.264
2024-12-02-09:06:51-root-INFO: Loss Change: 921.195 -> 880.577
2024-12-02-09:06:51-root-INFO: Regularization Change: 0.000 -> 0.752
2024-12-02-09:06:51-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-09:06:51-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-09:06:51-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-09:06:51-root-INFO: grad norm: 34.565 33.405 8.879
2024-12-02-09:06:52-root-INFO: grad norm: 44.398 43.527 8.752
2024-12-02-09:06:53-root-INFO: grad norm: 70.805 70.022 10.499
2024-12-02-09:06:53-root-INFO: Loss too large (867.570->897.437)! Learning rate decreased to 0.00917.
2024-12-02-09:06:53-root-INFO: Loss too large (867.570->870.117)! Learning rate decreased to 0.00734.
2024-12-02-09:06:53-root-INFO: grad norm: 81.417 80.559 11.794
2024-12-02-09:06:54-root-INFO: Loss too large (858.637->862.644)! Learning rate decreased to 0.00587.
2024-12-02-09:06:54-root-INFO: grad norm: 63.632 62.930 9.426
2024-12-02-09:06:55-root-INFO: Loss Change: 877.770 -> 844.684
2024-12-02-09:06:55-root-INFO: Regularization Change: 0.000 -> 1.440
2024-12-02-09:06:55-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-09:06:55-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:06:55-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-09:06:55-root-INFO: grad norm: 29.737 28.638 8.008
2024-12-02-09:06:55-root-INFO: grad norm: 31.391 30.490 7.468
2024-12-02-09:06:56-root-INFO: grad norm: 49.051 48.126 9.480
2024-12-02-09:06:56-root-INFO: Loss too large (828.694->832.522)! Learning rate decreased to 0.00951.
2024-12-02-09:06:57-root-INFO: grad norm: 57.800 57.135 8.747
2024-12-02-09:06:57-root-INFO: grad norm: 93.977 93.014 13.421
2024-12-02-09:06:57-root-INFO: Loss too large (824.373->844.183)! Learning rate decreased to 0.00761.
2024-12-02-09:06:57-root-INFO: Loss too large (824.373->830.779)! Learning rate decreased to 0.00608.
2024-12-02-09:06:58-root-INFO: Loss Change: 842.213 -> 822.350
2024-12-02-09:06:58-root-INFO: Regularization Change: 0.000 -> 1.613
2024-12-02-09:06:58-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-09:06:58-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:06:58-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-09:06:58-root-INFO: grad norm: 71.068 70.304 10.395
2024-12-02-09:06:58-root-INFO: Loss too large (825.344->834.370)! Learning rate decreased to 0.00985.
2024-12-02-09:06:59-root-INFO: grad norm: 89.268 88.375 12.597
2024-12-02-09:06:59-root-INFO: Loss too large (815.247->828.260)! Learning rate decreased to 0.00788.
2024-12-02-09:06:59-root-INFO: Loss too large (815.247->816.491)! Learning rate decreased to 0.00630.
2024-12-02-09:06:59-root-INFO: grad norm: 54.108 53.441 8.468
2024-12-02-09:07:00-root-INFO: grad norm: 24.226 23.169 7.079
2024-12-02-09:07:00-root-INFO: grad norm: 22.611 21.575 6.768
2024-12-02-09:07:01-root-INFO: Loss Change: 825.344 -> 792.322
2024-12-02-09:07:01-root-INFO: Regularization Change: 0.000 -> 0.823
2024-12-02-09:07:01-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-09:07:01-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-09:07:01-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-09:07:01-root-INFO: grad norm: 28.348 27.257 7.788
2024-12-02-09:07:01-root-INFO: grad norm: 31.010 30.205 7.020
2024-12-02-09:07:02-root-INFO: grad norm: 40.002 39.197 7.984
2024-12-02-09:07:02-root-INFO: grad norm: 62.143 61.497 8.939
2024-12-02-09:07:02-root-INFO: Loss too large (780.125->788.528)! Learning rate decreased to 0.01021.
2024-12-02-09:07:03-root-INFO: grad norm: 61.564 60.863 9.262
2024-12-02-09:07:03-root-INFO: Loss Change: 792.152 -> 771.001
2024-12-02-09:07:03-root-INFO: Regularization Change: 0.000 -> 1.700
2024-12-02-09:07:03-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-09:07:03-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-09:07:03-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-09:07:04-root-INFO: grad norm: 46.124 45.461 7.796
2024-12-02-09:07:04-root-INFO: Loss too large (766.071->766.594)! Learning rate decreased to 0.01057.
2024-12-02-09:07:04-root-INFO: grad norm: 45.053 44.305 8.176
2024-12-02-09:07:05-root-INFO: grad norm: 42.556 41.937 7.234
2024-12-02-09:07:05-root-INFO: grad norm: 42.032 41.262 8.008
2024-12-02-09:07:06-root-INFO: grad norm: 40.141 39.530 6.979
2024-12-02-09:07:06-root-INFO: Loss Change: 766.071 -> 744.070
2024-12-02-09:07:06-root-INFO: Regularization Change: 0.000 -> 1.268
2024-12-02-09:07:06-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-09:07:06-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-09:07:06-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-09:07:06-root-INFO: grad norm: 53.571 52.697 9.639
2024-12-02-09:07:07-root-INFO: grad norm: 67.299 66.706 8.915
2024-12-02-09:07:07-root-INFO: Loss too large (747.440->749.723)! Learning rate decreased to 0.01095.
2024-12-02-09:07:07-root-INFO: grad norm: 54.881 54.218 8.505
2024-12-02-09:07:08-root-INFO: grad norm: 45.904 45.333 7.217
2024-12-02-09:07:08-root-INFO: grad norm: 42.001 41.337 7.437
2024-12-02-09:07:09-root-INFO: Loss Change: 747.488 -> 721.680
2024-12-02-09:07:09-root-INFO: Regularization Change: 0.000 -> 1.473
2024-12-02-09:07:09-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-09:07:09-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-09:07:09-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-09:07:09-root-INFO: grad norm: 29.385 28.710 6.265
2024-12-02-09:07:09-root-INFO: grad norm: 38.220 37.446 7.656
2024-12-02-09:07:10-root-INFO: grad norm: 45.444 44.880 7.137
2024-12-02-09:07:10-root-INFO: grad norm: 58.626 57.801 9.800
2024-12-02-09:07:10-root-INFO: Loss too large (710.442->714.233)! Learning rate decreased to 0.01134.
2024-12-02-09:07:11-root-INFO: grad norm: 45.949 45.385 7.178
2024-12-02-09:07:11-root-INFO: Loss Change: 718.835 -> 697.567
2024-12-02-09:07:11-root-INFO: Regularization Change: 0.000 -> 1.538
2024-12-02-09:07:11-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-09:07:11-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-09:07:11-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-09:07:12-root-INFO: grad norm: 43.341 42.731 7.245
2024-12-02-09:07:12-root-INFO: grad norm: 54.006 53.487 7.465
2024-12-02-09:07:12-root-INFO: Loss too large (696.696->698.529)! Learning rate decreased to 0.01174.
2024-12-02-09:07:13-root-INFO: grad norm: 46.529 45.958 7.265
2024-12-02-09:07:13-root-INFO: grad norm: 39.327 38.793 6.458
2024-12-02-09:07:14-root-INFO: grad norm: 37.677 37.014 7.039
2024-12-02-09:07:14-root-INFO: Loss Change: 699.218 -> 677.927
2024-12-02-09:07:14-root-INFO: Regularization Change: 0.000 -> 1.391
2024-12-02-09:07:14-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-09:07:14-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:07:14-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-09:07:14-root-INFO: grad norm: 30.923 30.235 6.487
2024-12-02-09:07:15-root-INFO: grad norm: 51.454 50.607 9.295
2024-12-02-09:07:15-root-INFO: Loss too large (673.309->677.502)! Learning rate decreased to 0.01215.
2024-12-02-09:07:15-root-INFO: Loss too large (673.309->673.786)! Learning rate decreased to 0.00972.
2024-12-02-09:07:15-root-INFO: grad norm: 33.317 32.702 6.371
2024-12-02-09:07:16-root-INFO: grad norm: 17.640 16.863 5.178
2024-12-02-09:07:16-root-INFO: grad norm: 16.870 16.054 5.184
2024-12-02-09:07:17-root-INFO: Loss Change: 676.278 -> 657.760
2024-12-02-09:07:17-root-INFO: Regularization Change: 0.000 -> 0.902
2024-12-02-09:07:17-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-09:07:17-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:07:17-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-09:07:17-root-INFO: grad norm: 21.758 21.070 5.426
2024-12-02-09:07:17-root-INFO: grad norm: 23.015 22.389 5.331
2024-12-02-09:07:18-root-INFO: grad norm: 27.296 26.753 5.419
2024-12-02-09:07:18-root-INFO: grad norm: 36.154 35.699 5.716
2024-12-02-09:07:18-root-INFO: Loss too large (646.804->646.950)! Learning rate decreased to 0.01258.
2024-12-02-09:07:19-root-INFO: grad norm: 35.473 34.943 6.110
2024-12-02-09:07:19-root-INFO: Loss Change: 657.338 -> 640.418
2024-12-02-09:07:19-root-INFO: Regularization Change: 0.000 -> 1.471
2024-12-02-09:07:19-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-09:07:19-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-09:07:19-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-09:07:20-root-INFO: grad norm: 28.355 27.638 6.333
2024-12-02-09:07:20-root-INFO: grad norm: 44.610 43.856 8.168
2024-12-02-09:07:20-root-INFO: Loss too large (635.540->639.699)! Learning rate decreased to 0.01302.
2024-12-02-09:07:20-root-INFO: Loss too large (635.540->635.899)! Learning rate decreased to 0.01041.
2024-12-02-09:07:21-root-INFO: grad norm: 32.384 31.786 6.194
2024-12-02-09:07:21-root-INFO: grad norm: 18.188 17.436 5.176
2024-12-02-09:07:22-root-INFO: grad norm: 17.319 16.624 4.858
2024-12-02-09:07:22-root-INFO: Loss Change: 638.413 -> 621.611
2024-12-02-09:07:22-root-INFO: Regularization Change: 0.000 -> 0.911
2024-12-02-09:07:22-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-09:07:22-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-09:07:22-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-09:07:22-root-INFO: grad norm: 22.429 21.767 5.407
2024-12-02-09:07:23-root-INFO: grad norm: 28.834 28.273 5.662
2024-12-02-09:07:23-root-INFO: grad norm: 56.271 55.453 9.563
2024-12-02-09:07:24-root-INFO: Loss too large (618.852->626.453)! Learning rate decreased to 0.01347.
2024-12-02-09:07:24-root-INFO: Loss too large (618.852->621.335)! Learning rate decreased to 0.01077.
2024-12-02-09:07:24-root-INFO: grad norm: 33.807 33.246 6.135
2024-12-02-09:07:25-root-INFO: grad norm: 17.953 17.355 4.593
2024-12-02-09:07:25-root-INFO: Loss Change: 621.615 -> 605.913
2024-12-02-09:07:25-root-INFO: Regularization Change: 0.000 -> 1.020
2024-12-02-09:07:25-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-09:07:25-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-09:07:25-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-09:07:25-root-INFO: grad norm: 19.523 18.872 4.999
2024-12-02-09:07:26-root-INFO: grad norm: 22.205 21.628 5.033
2024-12-02-09:07:26-root-INFO: grad norm: 37.062 36.409 6.925
2024-12-02-09:07:26-root-INFO: Loss too large (599.901->603.227)! Learning rate decreased to 0.01393.
2024-12-02-09:07:27-root-INFO: grad norm: 34.871 34.324 6.153
2024-12-02-09:07:27-root-INFO: grad norm: 28.672 28.095 5.722
2024-12-02-09:07:28-root-INFO: Loss Change: 605.585 -> 591.279
2024-12-02-09:07:28-root-INFO: Regularization Change: 0.000 -> 1.341
2024-12-02-09:07:28-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-09:07:28-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-09:07:28-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-09:07:28-root-INFO: grad norm: 26.002 25.418 5.480
2024-12-02-09:07:28-root-INFO: grad norm: 45.973 45.242 8.164
2024-12-02-09:07:28-root-INFO: Loss too large (587.478->592.404)! Learning rate decreased to 0.01441.
2024-12-02-09:07:29-root-INFO: Loss too large (587.478->588.753)! Learning rate decreased to 0.01153.
2024-12-02-09:07:29-root-INFO: grad norm: 29.957 29.380 5.852
2024-12-02-09:07:30-root-INFO: grad norm: 15.146 14.489 4.409
2024-12-02-09:07:30-root-INFO: grad norm: 14.327 13.638 4.388
2024-12-02-09:07:30-root-INFO: Loss Change: 589.235 -> 574.119
2024-12-02-09:07:30-root-INFO: Regularization Change: 0.000 -> 0.874
2024-12-02-09:07:30-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-09:07:30-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-09:07:30-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-09:07:31-root-INFO: grad norm: 20.741 20.188 4.758
2024-12-02-09:07:31-root-INFO: grad norm: 25.474 25.037 4.702
2024-12-02-09:07:32-root-INFO: grad norm: 33.773 33.394 5.044
2024-12-02-09:07:32-root-INFO: Loss too large (569.911->570.236)! Learning rate decreased to 0.01490.
2024-12-02-09:07:32-root-INFO: grad norm: 34.208 33.814 5.178
2024-12-02-09:07:33-root-INFO: grad norm: 37.490 37.017 5.932
2024-12-02-09:07:33-root-INFO: Loss Change: 574.446 -> 563.016
2024-12-02-09:07:33-root-INFO: Regularization Change: 0.000 -> 1.438
2024-12-02-09:07:33-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-09:07:33-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-09:07:33-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-09:07:33-root-INFO: grad norm: 34.391 33.891 5.846
2024-12-02-09:07:33-root-INFO: Loss too large (560.974->563.106)! Learning rate decreased to 0.01541.
2024-12-02-09:07:34-root-INFO: grad norm: 41.511 40.900 7.095
2024-12-02-09:07:34-root-INFO: Loss too large (558.013->558.870)! Learning rate decreased to 0.01233.
2024-12-02-09:07:35-root-INFO: grad norm: 30.649 30.086 5.850
2024-12-02-09:07:35-root-INFO: grad norm: 18.390 17.819 4.546
2024-12-02-09:07:35-root-INFO: grad norm: 17.088 16.518 4.378
2024-12-02-09:07:36-root-INFO: Loss Change: 560.974 -> 545.117
2024-12-02-09:07:36-root-INFO: Regularization Change: 0.000 -> 0.868
2024-12-02-09:07:36-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-09:07:36-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-09:07:36-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-09:07:36-root-INFO: grad norm: 25.416 24.867 5.255
2024-12-02-09:07:36-root-INFO: grad norm: 34.507 33.949 6.182
2024-12-02-09:07:37-root-INFO: Loss too large (545.221->547.002)! Learning rate decreased to 0.01593.
2024-12-02-09:07:37-root-INFO: grad norm: 39.173 38.598 6.688
2024-12-02-09:07:37-root-INFO: Loss too large (541.777->542.050)! Learning rate decreased to 0.01275.
2024-12-02-09:07:38-root-INFO: grad norm: 29.428 28.848 5.815
2024-12-02-09:07:38-root-INFO: grad norm: 19.355 18.823 4.507
2024-12-02-09:07:39-root-INFO: Loss Change: 545.969 -> 531.486
2024-12-02-09:07:39-root-INFO: Regularization Change: 0.000 -> 1.057
2024-12-02-09:07:39-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-09:07:39-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-09:07:39-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-09:07:39-root-INFO: grad norm: 17.367 16.790 4.439
2024-12-02-09:07:39-root-INFO: grad norm: 26.262 25.721 5.303
2024-12-02-09:07:39-root-INFO: Loss too large (527.710->528.591)! Learning rate decreased to 0.01647.
2024-12-02-09:07:40-root-INFO: grad norm: 26.933 26.333 5.656
2024-12-02-09:07:40-root-INFO: grad norm: 28.022 27.459 5.590
2024-12-02-09:07:41-root-INFO: grad norm: 27.218 26.599 5.770
2024-12-02-09:07:41-root-INFO: Loss Change: 530.701 -> 516.222
2024-12-02-09:07:41-root-INFO: Regularization Change: 0.000 -> 1.359
2024-12-02-09:07:41-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-09:07:41-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-09:07:41-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-09:07:41-root-INFO: grad norm: 32.364 31.824 5.884
2024-12-02-09:07:42-root-INFO: Loss too large (517.099->518.942)! Learning rate decreased to 0.01702.
2024-12-02-09:07:42-root-INFO: grad norm: 31.000 30.373 6.204
2024-12-02-09:07:43-root-INFO: grad norm: 28.138 27.669 5.115
2024-12-02-09:07:43-root-INFO: grad norm: 29.392 28.813 5.802
2024-12-02-09:07:44-root-INFO: grad norm: 31.386 30.909 5.451
2024-12-02-09:07:44-root-INFO: Loss Change: 517.099 -> 504.025
2024-12-02-09:07:44-root-INFO: Regularization Change: 0.000 -> 1.500
2024-12-02-09:07:44-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-09:07:44-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-09:07:44-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-09:07:44-root-INFO: grad norm: 27.810 27.159 5.981
2024-12-02-09:07:45-root-INFO: grad norm: 42.226 41.604 7.218
2024-12-02-09:07:45-root-INFO: Loss too large (499.665->504.924)! Learning rate decreased to 0.01759.
2024-12-02-09:07:45-root-INFO: Loss too large (499.665->500.210)! Learning rate decreased to 0.01407.
2024-12-02-09:07:46-root-INFO: grad norm: 27.447 26.792 5.961
2024-12-02-09:07:46-root-INFO: grad norm: 14.766 14.313 3.630
2024-12-02-09:07:46-root-INFO: grad norm: 13.106 12.586 3.655
2024-12-02-09:07:47-root-INFO: Loss Change: 502.173 -> 486.253
2024-12-02-09:07:47-root-INFO: Regularization Change: 0.000 -> 1.040
2024-12-02-09:07:47-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-09:07:47-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-09:07:47-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-09:07:47-root-INFO: grad norm: 17.687 17.259 3.868
2024-12-02-09:07:48-root-INFO: grad norm: 23.560 23.063 4.816
2024-12-02-09:07:48-root-INFO: Loss too large (483.773->484.142)! Learning rate decreased to 0.01817.
2024-12-02-09:07:48-root-INFO: grad norm: 27.906 27.462 4.955
2024-12-02-09:07:49-root-INFO: grad norm: 29.982 29.390 5.929
2024-12-02-09:07:49-root-INFO: grad norm: 32.653 32.167 5.610
2024-12-02-09:07:49-root-INFO: Loss Change: 486.138 -> 477.823
2024-12-02-09:07:49-root-INFO: Regularization Change: 0.000 -> 1.428
2024-12-02-09:07:49-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-09:07:49-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-09:07:50-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-09:07:50-root-INFO: grad norm: 30.200 29.574 6.113
2024-12-02-09:07:50-root-INFO: grad norm: 45.212 44.559 7.651
2024-12-02-09:07:50-root-INFO: Loss too large (475.985->483.875)! Learning rate decreased to 0.01877.
2024-12-02-09:07:50-root-INFO: Loss too large (475.985->476.453)! Learning rate decreased to 0.01502.
2024-12-02-09:07:51-root-INFO: grad norm: 28.902 28.240 6.151
2024-12-02-09:07:51-root-INFO: grad norm: 15.889 15.528 3.368
2024-12-02-09:07:52-root-INFO: grad norm: 13.145 12.689 3.434
2024-12-02-09:07:52-root-INFO: Loss Change: 476.763 -> 461.416
2024-12-02-09:07:52-root-INFO: Regularization Change: 0.000 -> 1.007
2024-12-02-09:07:52-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-09:07:52-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-09:07:52-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-09:07:52-root-INFO: grad norm: 18.275 17.882 3.768
2024-12-02-09:07:53-root-INFO: grad norm: 24.430 24.003 4.551
2024-12-02-09:07:53-root-INFO: Loss too large (459.545->460.683)! Learning rate decreased to 0.01939.
2024-12-02-09:07:53-root-INFO: grad norm: 27.706 27.293 4.765
2024-12-02-09:07:54-root-INFO: grad norm: 30.225 29.622 6.006
2024-12-02-09:07:54-root-INFO: grad norm: 35.022 34.455 6.276
2024-12-02-09:07:55-root-INFO: Loss too large (455.321->455.713)! Learning rate decreased to 0.01551.
2024-12-02-09:07:55-root-INFO: Loss Change: 461.667 -> 452.464
2024-12-02-09:07:55-root-INFO: Regularization Change: 0.000 -> 1.258
2024-12-02-09:07:55-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-09:07:55-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-09:07:55-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-09:07:55-root-INFO: grad norm: 23.066 22.403 5.489
2024-12-02-09:07:56-root-INFO: grad norm: 38.199 37.490 7.321
2024-12-02-09:07:56-root-INFO: Loss too large (449.970->455.670)! Learning rate decreased to 0.02013.
2024-12-02-09:07:56-root-INFO: Loss too large (449.970->451.363)! Learning rate decreased to 0.01610.
2024-12-02-09:07:56-root-INFO: grad norm: 26.144 25.404 6.176
2024-12-02-09:07:57-root-INFO: grad norm: 13.930 13.518 3.365
2024-12-02-09:07:57-root-INFO: grad norm: 12.549 12.069 3.439
2024-12-02-09:07:58-root-INFO: Loss Change: 450.966 -> 438.534
2024-12-02-09:07:58-root-INFO: Regularization Change: 0.000 -> 0.974
2024-12-02-09:07:58-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-09:07:58-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-09:07:58-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-09:07:58-root-INFO: grad norm: 16.828 16.411 3.722
2024-12-02-09:07:59-root-INFO: grad norm: 24.380 23.777 5.386
2024-12-02-09:07:59-root-INFO: Loss too large (437.631->440.363)! Learning rate decreased to 0.02078.
2024-12-02-09:07:59-root-INFO: grad norm: 31.666 31.038 6.277
2024-12-02-09:07:59-root-INFO: Loss too large (436.658->437.923)! Learning rate decreased to 0.01662.
2024-12-02-09:08:00-root-INFO: grad norm: 25.734 25.024 6.001
2024-12-02-09:08:00-root-INFO: grad norm: 19.352 18.888 4.213
2024-12-02-09:08:00-root-INFO: Loss Change: 438.776 -> 429.446
2024-12-02-09:08:00-root-INFO: Regularization Change: 0.000 -> 0.997
2024-12-02-09:08:00-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-09:08:00-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-09:08:01-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-09:08:01-root-INFO: grad norm: 15.936 15.433 3.973
2024-12-02-09:08:01-root-INFO: grad norm: 26.848 26.261 5.585
2024-12-02-09:08:01-root-INFO: Loss too large (427.724->431.474)! Learning rate decreased to 0.02145.
2024-12-02-09:08:02-root-INFO: Loss too large (427.724->428.418)! Learning rate decreased to 0.01716.
2024-12-02-09:08:02-root-INFO: grad norm: 22.658 21.985 5.482
2024-12-02-09:08:03-root-INFO: grad norm: 17.757 17.290 4.048
2024-12-02-09:08:03-root-INFO: grad norm: 16.797 16.249 4.255
2024-12-02-09:08:03-root-INFO: Loss Change: 428.814 -> 419.780
2024-12-02-09:08:03-root-INFO: Regularization Change: 0.000 -> 0.897
2024-12-02-09:08:03-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-09:08:03-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-09:08:03-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-09:08:04-root-INFO: grad norm: 21.718 21.207 4.686
2024-12-02-09:08:04-root-INFO: Loss too large (420.158->422.076)! Learning rate decreased to 0.02214.
2024-12-02-09:08:04-root-INFO: grad norm: 25.362 24.679 5.843
2024-12-02-09:08:05-root-INFO: grad norm: 32.564 31.873 6.675
2024-12-02-09:08:05-root-INFO: Loss too large (418.531->420.508)! Learning rate decreased to 0.01771.
2024-12-02-09:08:05-root-INFO: grad norm: 26.066 25.360 6.028
2024-12-02-09:08:06-root-INFO: grad norm: 18.496 18.050 4.035
2024-12-02-09:08:06-root-INFO: Loss Change: 420.158 -> 411.237
2024-12-02-09:08:06-root-INFO: Regularization Change: 0.000 -> 0.920
2024-12-02-09:08:06-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-09:08:06-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-09:08:06-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-09:08:07-root-INFO: grad norm: 16.292 15.816 3.911
2024-12-02-09:08:07-root-INFO: grad norm: 28.350 27.764 5.733
2024-12-02-09:08:07-root-INFO: Loss too large (410.564->415.332)! Learning rate decreased to 0.02285.
2024-12-02-09:08:07-root-INFO: Loss too large (410.564->411.709)! Learning rate decreased to 0.01828.
2024-12-02-09:08:08-root-INFO: grad norm: 23.196 22.558 5.401
2024-12-02-09:08:08-root-INFO: grad norm: 17.446 17.011 3.875
2024-12-02-09:08:09-root-INFO: grad norm: 16.461 15.954 4.054
2024-12-02-09:08:09-root-INFO: Loss Change: 411.024 -> 402.488
2024-12-02-09:08:09-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-02-09:08:09-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-09:08:09-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-09:08:09-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-09:08:09-root-INFO: grad norm: 21.471 20.999 4.476
2024-12-02-09:08:09-root-INFO: Loss too large (403.091->404.842)! Learning rate decreased to 0.02358.
2024-12-02-09:08:10-root-INFO: grad norm: 24.570 23.951 5.480
2024-12-02-09:08:11-root-INFO: grad norm: 30.766 30.172 6.016
2024-12-02-09:08:11-root-INFO: Loss too large (401.332->402.818)! Learning rate decreased to 0.01886.
2024-12-02-09:08:11-root-INFO: grad norm: 24.436 23.801 5.535
2024-12-02-09:08:12-root-INFO: grad norm: 17.773 17.373 3.751
2024-12-02-09:08:12-root-INFO: Loss Change: 403.091 -> 394.216
2024-12-02-09:08:12-root-INFO: Regularization Change: 0.000 -> 0.954
2024-12-02-09:08:12-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-09:08:12-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-09:08:12-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-09:08:12-root-INFO: grad norm: 13.841 13.421 3.384
2024-12-02-09:08:13-root-INFO: grad norm: 23.366 22.896 4.664
2024-12-02-09:08:13-root-INFO: Loss too large (392.963->396.269)! Learning rate decreased to 0.02432.
2024-12-02-09:08:13-root-INFO: Loss too large (392.963->393.405)! Learning rate decreased to 0.01946.
2024-12-02-09:08:14-root-INFO: grad norm: 20.047 19.517 4.576
2024-12-02-09:08:14-root-INFO: grad norm: 16.511 16.127 3.542
2024-12-02-09:08:15-root-INFO: grad norm: 15.355 14.910 3.671
2024-12-02-09:08:15-root-INFO: Loss Change: 393.715 -> 386.023
2024-12-02-09:08:15-root-INFO: Regularization Change: 0.000 -> 0.882
2024-12-02-09:08:15-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-09:08:15-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-09:08:15-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-09:08:15-root-INFO: grad norm: 19.654 19.263 3.900
2024-12-02-09:08:16-root-INFO: Loss too large (386.781->388.020)! Learning rate decreased to 0.02509.
2024-12-02-09:08:16-root-INFO: grad norm: 22.466 21.973 4.680
2024-12-02-09:08:17-root-INFO: grad norm: 27.937 27.444 5.221
2024-12-02-09:08:17-root-INFO: Loss too large (385.059->386.013)! Learning rate decreased to 0.02007.
2024-12-02-09:08:17-root-INFO: grad norm: 22.351 21.822 4.834
2024-12-02-09:08:18-root-INFO: grad norm: 17.028 16.665 3.501
2024-12-02-09:08:18-root-INFO: Loss Change: 386.781 -> 378.513
2024-12-02-09:08:18-root-INFO: Regularization Change: 0.000 -> 0.949
2024-12-02-09:08:18-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-09:08:18-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-09:08:18-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-09:08:18-root-INFO: grad norm: 14.129 13.723 3.364
2024-12-02-09:08:19-root-INFO: grad norm: 23.194 22.778 4.370
2024-12-02-09:08:19-root-INFO: Loss too large (377.259->380.410)! Learning rate decreased to 0.02587.
2024-12-02-09:08:19-root-INFO: Loss too large (377.259->377.528)! Learning rate decreased to 0.02070.
2024-12-02-09:08:19-root-INFO: grad norm: 19.144 18.658 4.288
2024-12-02-09:08:20-root-INFO: grad norm: 15.286 14.940 3.233
2024-12-02-09:08:20-root-INFO: grad norm: 14.037 13.632 3.347
2024-12-02-09:08:21-root-INFO: Loss Change: 377.893 -> 370.204
2024-12-02-09:08:21-root-INFO: Regularization Change: 0.000 -> 0.911
2024-12-02-09:08:21-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-09:08:21-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-09:08:21-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-09:08:21-root-INFO: grad norm: 17.497 17.149 3.473
2024-12-02-09:08:21-root-INFO: Loss too large (371.069->371.986)! Learning rate decreased to 0.02668.
2024-12-02-09:08:22-root-INFO: grad norm: 20.195 19.740 4.262
2024-12-02-09:08:22-root-INFO: grad norm: 24.773 24.346 4.578
2024-12-02-09:08:22-root-INFO: Loss too large (369.349->369.836)! Learning rate decreased to 0.02134.
2024-12-02-09:08:23-root-INFO: grad norm: 19.950 19.475 4.326
2024-12-02-09:08:23-root-INFO: grad norm: 15.563 15.232 3.195
2024-12-02-09:08:23-root-INFO: Loss Change: 371.069 -> 363.409
2024-12-02-09:08:23-root-INFO: Regularization Change: 0.000 -> 0.940
2024-12-02-09:08:23-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-09:08:23-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-09:08:24-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-09:08:24-root-INFO: grad norm: 13.184 12.816 3.094
2024-12-02-09:08:24-root-INFO: grad norm: 20.293 19.940 3.764
2024-12-02-09:08:24-root-INFO: Loss too large (362.239->364.204)! Learning rate decreased to 0.02751.
2024-12-02-09:08:25-root-INFO: grad norm: 20.897 20.392 4.562
2024-12-02-09:08:25-root-INFO: grad norm: 21.536 21.155 4.035
2024-12-02-09:08:26-root-INFO: grad norm: 21.566 21.037 4.748
2024-12-02-09:08:26-root-INFO: Loss Change: 363.043 -> 356.539
2024-12-02-09:08:26-root-INFO: Regularization Change: 0.000 -> 1.330
2024-12-02-09:08:26-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-09:08:26-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-09:08:26-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-09:08:27-root-INFO: grad norm: 25.335 24.914 4.600
2024-12-02-09:08:27-root-INFO: Loss too large (357.360->360.872)! Learning rate decreased to 0.02836.
2024-12-02-09:08:27-root-INFO: grad norm: 24.434 23.892 5.121
2024-12-02-09:08:28-root-INFO: grad norm: 22.975 22.592 4.178
2024-12-02-09:08:28-root-INFO: grad norm: 22.701 22.185 4.813
2024-12-02-09:08:29-root-INFO: grad norm: 22.122 21.749 4.045
2024-12-02-09:08:29-root-INFO: Loss Change: 357.360 -> 349.591
2024-12-02-09:08:29-root-INFO: Regularization Change: 0.000 -> 1.477
2024-12-02-09:08:29-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-09:08:29-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-09:08:29-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-09:08:29-root-INFO: grad norm: 19.436 18.936 4.380
2024-12-02-09:08:29-root-INFO: Loss too large (348.434->348.593)! Learning rate decreased to 0.02923.
2024-12-02-09:08:30-root-INFO: grad norm: 19.015 18.665 3.630
2024-12-02-09:08:30-root-INFO: grad norm: 18.963 18.478 4.265
2024-12-02-09:08:31-root-INFO: grad norm: 18.811 18.464 3.598
2024-12-02-09:08:31-root-INFO: grad norm: 18.744 18.262 4.222
2024-12-02-09:08:32-root-INFO: Loss Change: 348.434 -> 340.316
2024-12-02-09:08:32-root-INFO: Regularization Change: 0.000 -> 1.357
2024-12-02-09:08:32-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-09:08:32-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-09:08:32-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-09:08:32-root-INFO: grad norm: 22.497 22.135 4.015
2024-12-02-09:08:32-root-INFO: Loss too large (341.276->343.402)! Learning rate decreased to 0.03012.
2024-12-02-09:08:33-root-INFO: grad norm: 21.509 21.011 4.603
2024-12-02-09:08:33-root-INFO: grad norm: 20.324 19.987 3.682
2024-12-02-09:08:34-root-INFO: grad norm: 19.600 19.132 4.259
2024-12-02-09:08:34-root-INFO: grad norm: 18.783 18.458 3.480
2024-12-02-09:08:34-root-INFO: Loss Change: 341.276 -> 333.205
2024-12-02-09:08:34-root-INFO: Regularization Change: 0.000 -> 1.412
2024-12-02-09:08:34-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-09:08:34-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-09:08:35-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-09:08:35-root-INFO: grad norm: 16.131 15.701 3.698
2024-12-02-09:08:35-root-INFO: grad norm: 22.072 21.705 4.008
2024-12-02-09:08:36-root-INFO: Loss too large (332.214->333.967)! Learning rate decreased to 0.03103.
2024-12-02-09:08:36-root-INFO: grad norm: 20.250 19.760 4.427
2024-12-02-09:08:36-root-INFO: grad norm: 18.420 18.098 3.429
2024-12-02-09:08:37-root-INFO: grad norm: 17.481 17.059 3.818
2024-12-02-09:08:37-root-INFO: Loss Change: 332.392 -> 325.217
2024-12-02-09:08:37-root-INFO: Regularization Change: 0.000 -> 1.406
2024-12-02-09:08:37-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-09:08:37-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-09:08:37-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-09:08:38-root-INFO: grad norm: 20.778 20.468 3.576
2024-12-02-09:08:38-root-INFO: Loss too large (326.495->327.902)! Learning rate decreased to 0.03197.
2024-12-02-09:08:38-root-INFO: grad norm: 19.618 19.232 3.872
2024-12-02-09:08:39-root-INFO: grad norm: 18.695 18.413 3.233
2024-12-02-09:08:39-root-INFO: grad norm: 17.982 17.620 3.587
2024-12-02-09:08:40-root-INFO: grad norm: 17.334 17.067 3.027
2024-12-02-09:08:40-root-INFO: Loss Change: 326.495 -> 318.680
2024-12-02-09:08:40-root-INFO: Regularization Change: 0.000 -> 1.397
2024-12-02-09:08:40-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-09:08:40-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-09:08:40-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-09:08:40-root-INFO: grad norm: 14.880 14.541 3.155
2024-12-02-09:08:41-root-INFO: grad norm: 20.043 19.757 3.377
2024-12-02-09:08:41-root-INFO: Loss too large (317.566->318.992)! Learning rate decreased to 0.03292.
2024-12-02-09:08:41-root-INFO: grad norm: 18.758 18.399 3.650
2024-12-02-09:08:42-root-INFO: grad norm: 17.626 17.368 3.003
2024-12-02-09:08:42-root-INFO: grad norm: 16.824 16.513 3.221
2024-12-02-09:08:43-root-INFO: Loss Change: 317.748 -> 311.360
2024-12-02-09:08:43-root-INFO: Regularization Change: 0.000 -> 1.389
2024-12-02-09:08:43-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-09:08:43-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-09:08:43-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-09:08:43-root-INFO: grad norm: 21.350 21.107 3.212
2024-12-02-09:08:43-root-INFO: Loss too large (313.014->315.023)! Learning rate decreased to 0.03391.
2024-12-02-09:08:44-root-INFO: grad norm: 20.360 20.072 3.414
2024-12-02-09:08:44-root-INFO: grad norm: 19.457 19.227 2.978
2024-12-02-09:08:45-root-INFO: grad norm: 18.704 18.424 3.229
2024-12-02-09:08:45-root-INFO: grad norm: 18.027 17.806 2.812
2024-12-02-09:08:45-root-INFO: Loss Change: 313.014 -> 305.516
2024-12-02-09:08:45-root-INFO: Regularization Change: 0.000 -> 1.466
2024-12-02-09:08:45-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-09:08:45-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-09:08:45-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-09:08:46-root-INFO: grad norm: 14.687 14.456 2.597
2024-12-02-09:08:46-root-INFO: Loss too large (304.224->304.314)! Learning rate decreased to 0.03491.
2024-12-02-09:08:46-root-INFO: grad norm: 14.475 14.281 2.363
2024-12-02-09:08:47-root-INFO: grad norm: 14.606 14.399 2.451
2024-12-02-09:08:47-root-INFO: grad norm: 14.780 14.603 2.275
2024-12-02-09:08:48-root-INFO: grad norm: 14.948 14.756 2.392
2024-12-02-09:08:48-root-INFO: Loss Change: 304.224 -> 298.235
2024-12-02-09:08:48-root-INFO: Regularization Change: 0.000 -> 1.292
2024-12-02-09:08:48-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-09:08:48-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-09:08:48-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-09:08:48-root-INFO: grad norm: 18.760 18.571 2.653
2024-12-02-09:08:49-root-INFO: Loss too large (299.495->301.176)! Learning rate decreased to 0.03594.
2024-12-02-09:08:49-root-INFO: grad norm: 18.518 18.335 2.595
2024-12-02-09:08:49-root-INFO: grad norm: 18.161 17.997 2.440
2024-12-02-09:08:50-root-INFO: grad norm: 17.749 17.567 2.535
2024-12-02-09:08:51-root-INFO: grad norm: 17.348 17.189 2.342
2024-12-02-09:08:51-root-INFO: Loss Change: 299.495 -> 293.109
2024-12-02-09:08:51-root-INFO: Regularization Change: 0.000 -> 1.433
2024-12-02-09:08:51-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-09:08:51-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-09:08:51-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-09:08:51-root-INFO: grad norm: 14.427 14.257 2.208
2024-12-02-09:08:51-root-INFO: Loss too large (292.162->292.525)! Learning rate decreased to 0.03700.
2024-12-02-09:08:52-root-INFO: grad norm: 14.336 14.189 2.049
2024-12-02-09:08:52-root-INFO: grad norm: 14.526 14.375 2.088
2024-12-02-09:08:53-root-INFO: grad norm: 14.680 14.543 1.998
2024-12-02-09:08:53-root-INFO: grad norm: 14.793 14.649 2.061
2024-12-02-09:08:53-root-INFO: Loss Change: 292.162 -> 286.632
2024-12-02-09:08:53-root-INFO: Regularization Change: 0.000 -> 1.311
2024-12-02-09:08:53-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-09:08:53-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-09:08:54-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-09:08:54-root-INFO: grad norm: 17.787 17.650 2.199
2024-12-02-09:08:54-root-INFO: Loss too large (287.518->288.767)! Learning rate decreased to 0.03808.
2024-12-02-09:08:54-root-INFO: grad norm: 17.063 16.924 2.170
2024-12-02-09:08:55-root-INFO: grad norm: 16.392 16.265 2.039
2024-12-02-09:08:55-root-INFO: grad norm: 15.783 15.641 2.112
2024-12-02-09:08:56-root-INFO: grad norm: 15.308 15.180 1.974
2024-12-02-09:08:56-root-INFO: Loss Change: 287.518 -> 280.940
2024-12-02-09:08:56-root-INFO: Regularization Change: 0.000 -> 1.413
2024-12-02-09:08:56-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-09:08:56-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-09:08:56-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-09:08:56-root-INFO: grad norm: 13.411 13.283 1.849
2024-12-02-09:08:57-root-INFO: Loss too large (280.140->280.536)! Learning rate decreased to 0.03918.
2024-12-02-09:08:57-root-INFO: grad norm: 13.540 13.416 1.826
2024-12-02-09:08:58-root-INFO: grad norm: 13.886 13.770 1.792
2024-12-02-09:08:58-root-INFO: grad norm: 14.114 13.999 1.795
2024-12-02-09:08:58-root-INFO: grad norm: 14.291 14.178 1.794
2024-12-02-09:08:59-root-INFO: Loss Change: 280.140 -> 275.106
2024-12-02-09:08:59-root-INFO: Regularization Change: 0.000 -> 1.324
2024-12-02-09:08:59-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-09:08:59-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-09:08:59-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-09:08:59-root-INFO: grad norm: 17.214 17.082 2.131
2024-12-02-09:08:59-root-INFO: Loss too large (276.102->277.286)! Learning rate decreased to 0.04031.
2024-12-02-09:09:00-root-INFO: grad norm: 16.530 16.412 1.969
2024-12-02-09:09:00-root-INFO: grad norm: 15.896 15.780 1.915
2024-12-02-09:09:01-root-INFO: grad norm: 15.280 15.158 1.927
2024-12-02-09:09:01-root-INFO: grad norm: 14.824 14.708 1.851
2024-12-02-09:09:02-root-INFO: Loss Change: 276.102 -> 269.677
2024-12-02-09:09:02-root-INFO: Regularization Change: 0.000 -> 1.449
2024-12-02-09:09:02-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-09:09:02-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-09:09:02-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-09:09:02-root-INFO: grad norm: 12.648 12.528 1.740
2024-12-02-09:09:02-root-INFO: Loss too large (269.118->269.480)! Learning rate decreased to 0.04147.
2024-12-02-09:09:03-root-INFO: grad norm: 12.549 12.431 1.711
2024-12-02-09:09:03-root-INFO: grad norm: 12.668 12.554 1.692
2024-12-02-09:09:03-root-INFO: grad norm: 12.769 12.657 1.688
2024-12-02-09:09:04-root-INFO: grad norm: 12.874 12.763 1.682
2024-12-02-09:09:04-root-INFO: Loss Change: 269.118 -> 264.205
2024-12-02-09:09:04-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-02-09:09:04-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-09:09:04-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-09:09:04-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-09:09:05-root-INFO: grad norm: 15.272 15.147 1.944
2024-12-02-09:09:05-root-INFO: Loss too large (264.800->265.544)! Learning rate decreased to 0.04265.
2024-12-02-09:09:05-root-INFO: grad norm: 14.747 14.634 1.818
2024-12-02-09:09:06-root-INFO: grad norm: 14.267 14.157 1.771
2024-12-02-09:09:06-root-INFO: grad norm: 13.768 13.655 1.760
2024-12-02-09:09:07-root-INFO: grad norm: 13.409 13.300 1.707
2024-12-02-09:09:07-root-INFO: Loss Change: 264.800 -> 258.776
2024-12-02-09:09:07-root-INFO: Regularization Change: 0.000 -> 1.418
2024-12-02-09:09:07-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-09:09:07-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-09:09:08-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-09:09:08-root-INFO: grad norm: 11.544 11.432 1.599
2024-12-02-09:09:08-root-INFO: Loss too large (258.306->258.526)! Learning rate decreased to 0.04386.
2024-12-02-09:09:08-root-INFO: grad norm: 11.515 11.404 1.596
2024-12-02-09:09:09-root-INFO: grad norm: 11.733 11.631 1.543
2024-12-02-09:09:09-root-INFO: grad norm: 11.892 11.788 1.571
2024-12-02-09:09:10-root-INFO: grad norm: 12.059 11.960 1.538
2024-12-02-09:09:10-root-INFO: Loss Change: 258.306 -> 253.720
2024-12-02-09:09:10-root-INFO: Regularization Change: 0.000 -> 1.318
2024-12-02-09:09:10-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-09:09:10-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-09:09:10-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-09:09:11-root-INFO: grad norm: 14.704 14.587 1.853
2024-12-02-09:09:11-root-INFO: Loss too large (254.329->255.038)! Learning rate decreased to 0.04509.
2024-12-02-09:09:11-root-INFO: grad norm: 14.307 14.212 1.645
2024-12-02-09:09:12-root-INFO: grad norm: 13.865 13.768 1.639
2024-12-02-09:09:12-root-INFO: grad norm: 13.336 13.241 1.593
2024-12-02-09:09:13-root-INFO: grad norm: 12.983 12.888 1.573
2024-12-02-09:09:13-root-INFO: Loss Change: 254.329 -> 248.479
2024-12-02-09:09:13-root-INFO: Regularization Change: 0.000 -> 1.443
2024-12-02-09:09:13-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-09:09:13-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-09:09:13-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-09:09:13-root-INFO: grad norm: 11.091 10.991 1.486
2024-12-02-09:09:13-root-INFO: Loss too large (247.966->248.218)! Learning rate decreased to 0.04636.
2024-12-02-09:09:14-root-INFO: grad norm: 10.850 10.746 1.498
2024-12-02-09:09:14-root-INFO: grad norm: 10.795 10.699 1.433
2024-12-02-09:09:15-root-INFO: grad norm: 10.808 10.709 1.463
2024-12-02-09:09:15-root-INFO: grad norm: 10.852 10.758 1.420
2024-12-02-09:09:16-root-INFO: Loss Change: 247.966 -> 243.446
2024-12-02-09:09:16-root-INFO: Regularization Change: 0.000 -> 1.307
2024-12-02-09:09:16-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-09:09:16-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-09:09:16-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-09:09:16-root-INFO: grad norm: 13.236 13.117 1.765
2024-12-02-09:09:16-root-INFO: Loss too large (244.309->244.751)! Learning rate decreased to 0.04765.
2024-12-02-09:09:17-root-INFO: grad norm: 12.868 12.779 1.510
2024-12-02-09:09:17-root-INFO: grad norm: 12.507 12.414 1.522
2024-12-02-09:09:18-root-INFO: grad norm: 12.069 11.983 1.441
2024-12-02-09:09:18-root-INFO: grad norm: 11.789 11.699 1.449
2024-12-02-09:09:18-root-INFO: Loss Change: 244.309 -> 238.882
2024-12-02-09:09:18-root-INFO: Regularization Change: 0.000 -> 1.400
2024-12-02-09:09:18-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-09:09:18-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-09:09:19-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-09:09:19-root-INFO: grad norm: 9.984 9.887 1.385
2024-12-02-09:09:19-root-INFO: Loss too large (238.241->238.414)! Learning rate decreased to 0.04897.
2024-12-02-09:09:19-root-INFO: grad norm: 9.983 9.890 1.362
2024-12-02-09:09:20-root-INFO: grad norm: 10.248 10.163 1.315
2024-12-02-09:09:20-root-INFO: grad norm: 10.444 10.357 1.347
2024-12-02-09:09:21-root-INFO: grad norm: 10.692 10.612 1.304
2024-12-02-09:09:21-root-INFO: Loss Change: 238.241 -> 234.320
2024-12-02-09:09:21-root-INFO: Regularization Change: 0.000 -> 1.300
2024-12-02-09:09:21-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-09:09:21-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-09:09:21-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-09:09:21-root-INFO: grad norm: 13.068 12.964 1.646
2024-12-02-09:09:22-root-INFO: Loss too large (234.871->235.417)! Learning rate decreased to 0.05031.
2024-12-02-09:09:22-root-INFO: grad norm: 12.841 12.766 1.382
2024-12-02-09:09:23-root-INFO: grad norm: 12.464 12.384 1.409
2024-12-02-09:09:23-root-INFO: grad norm: 11.928 11.856 1.311
2024-12-02-09:09:23-root-INFO: grad norm: 11.612 11.535 1.335
2024-12-02-09:09:24-root-INFO: Loss Change: 234.871 -> 229.553
2024-12-02-09:09:24-root-INFO: Regularization Change: 0.000 -> 1.430
2024-12-02-09:09:24-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-09:09:24-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-09:09:24-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-09:09:24-root-INFO: grad norm: 10.037 9.953 1.299
2024-12-02-09:09:24-root-INFO: Loss too large (229.186->229.483)! Learning rate decreased to 0.05169.
2024-12-02-09:09:25-root-INFO: grad norm: 9.777 9.695 1.257
2024-12-02-09:09:25-root-INFO: grad norm: 9.719 9.643 1.213
2024-12-02-09:09:26-root-INFO: grad norm: 9.740 9.663 1.225
2024-12-02-09:09:26-root-INFO: grad norm: 9.788 9.715 1.191
2024-12-02-09:09:26-root-INFO: Loss Change: 229.186 -> 225.239
2024-12-02-09:09:26-root-INFO: Regularization Change: 0.000 -> 1.301
2024-12-02-09:09:26-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-09:09:26-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-09:09:27-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-09:09:27-root-INFO: grad norm: 11.589 11.503 1.416
2024-12-02-09:09:27-root-INFO: Loss too large (225.760->226.032)! Learning rate decreased to 0.05310.
2024-12-02-09:09:27-root-INFO: grad norm: 11.194 11.127 1.223
2024-12-02-09:09:28-root-INFO: grad norm: 10.851 10.779 1.242
2024-12-02-09:09:28-root-INFO: grad norm: 10.416 10.351 1.170
2024-12-02-09:09:29-root-INFO: grad norm: 10.176 10.107 1.189
2024-12-02-09:09:29-root-INFO: Loss Change: 225.760 -> 220.868
2024-12-02-09:09:29-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-02-09:09:29-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-09:09:29-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-09:09:29-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-09:09:29-root-INFO: grad norm: 8.755 8.678 1.163
2024-12-02-09:09:30-root-INFO: Loss too large (220.713->220.770)! Learning rate decreased to 0.05453.
2024-12-02-09:09:30-root-INFO: grad norm: 8.537 8.461 1.133
2024-12-02-09:09:31-root-INFO: grad norm: 8.541 8.472 1.082
2024-12-02-09:09:31-root-INFO: grad norm: 8.602 8.530 1.112
2024-12-02-09:09:32-root-INFO: grad norm: 8.718 8.652 1.067
2024-12-02-09:09:32-root-INFO: Loss Change: 220.713 -> 217.020
2024-12-02-09:09:32-root-INFO: Regularization Change: 0.000 -> 1.260
2024-12-02-09:09:32-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-09:09:32-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-09:09:32-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-09:09:32-root-INFO: grad norm: 11.070 10.983 1.385
2024-12-02-09:09:32-root-INFO: Loss too large (217.506->217.660)! Learning rate decreased to 0.05599.
2024-12-02-09:09:33-root-INFO: grad norm: 10.588 10.528 1.129
2024-12-02-09:09:33-root-INFO: grad norm: 10.193 10.128 1.145
2024-12-02-09:09:34-root-INFO: grad norm: 9.700 9.641 1.069
2024-12-02-09:09:34-root-INFO: grad norm: 9.437 9.374 1.087
2024-12-02-09:09:35-root-INFO: Loss Change: 217.506 -> 212.661
2024-12-02-09:09:35-root-INFO: Regularization Change: 0.000 -> 1.381
2024-12-02-09:09:35-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-09:09:35-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-09:09:35-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-09:09:35-root-INFO: grad norm: 7.971 7.894 1.104
2024-12-02-09:09:36-root-INFO: grad norm: 9.904 9.843 1.095
2024-12-02-09:09:36-root-INFO: grad norm: 13.962 13.918 1.110
2024-12-02-09:09:36-root-INFO: Loss too large (212.321->214.237)! Learning rate decreased to 0.05749.
2024-12-02-09:09:37-root-INFO: grad norm: 11.755 11.700 1.137
2024-12-02-09:09:37-root-INFO: grad norm: 8.698 8.640 1.008
2024-12-02-09:09:38-root-INFO: Loss Change: 212.631 -> 208.795
2024-12-02-09:09:38-root-INFO: Regularization Change: 0.000 -> 1.499
2024-12-02-09:09:38-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-09:09:38-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-09:09:38-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-09:09:38-root-INFO: grad norm: 10.032 9.955 1.243
2024-12-02-09:09:38-root-INFO: grad norm: 13.265 13.215 1.155
2024-12-02-09:09:39-root-INFO: Loss too large (208.712->210.115)! Learning rate decreased to 0.05901.
2024-12-02-09:09:39-root-INFO: grad norm: 11.010 10.952 1.121
2024-12-02-09:09:40-root-INFO: grad norm: 8.295 8.237 0.978
2024-12-02-09:09:40-root-INFO: grad norm: 7.504 7.441 0.969
2024-12-02-09:09:40-root-INFO: Loss Change: 209.088 -> 204.057
2024-12-02-09:09:40-root-INFO: Regularization Change: 0.000 -> 1.450
2024-12-02-09:09:40-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-09:09:40-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-09:09:40-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-09:09:41-root-INFO: grad norm: 5.803 5.722 0.968
2024-12-02-09:09:41-root-INFO: grad norm: 6.649 6.584 0.927
2024-12-02-09:09:42-root-INFO: grad norm: 9.014 8.966 0.931
2024-12-02-09:09:42-root-INFO: Loss too large (202.609->203.004)! Learning rate decreased to 0.06057.
2024-12-02-09:09:42-root-INFO: grad norm: 8.208 8.152 0.951
2024-12-02-09:09:43-root-INFO: grad norm: 7.137 7.081 0.893
2024-12-02-09:09:43-root-INFO: Loss Change: 203.891 -> 200.226
2024-12-02-09:09:43-root-INFO: Regularization Change: 0.000 -> 1.434
2024-12-02-09:09:43-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-09:09:43-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-09:09:43-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-09:09:44-root-INFO: grad norm: 8.295 8.222 1.098
2024-12-02-09:09:44-root-INFO: grad norm: 11.334 11.288 1.017
2024-12-02-09:09:44-root-INFO: Loss too large (200.078->201.171)! Learning rate decreased to 0.06216.
2024-12-02-09:09:45-root-INFO: grad norm: 9.668 9.615 1.011
2024-12-02-09:09:45-root-INFO: grad norm: 7.411 7.357 0.897
2024-12-02-09:09:46-root-INFO: grad norm: 6.797 6.737 0.896
2024-12-02-09:09:46-root-INFO: Loss Change: 200.507 -> 196.108
2024-12-02-09:09:46-root-INFO: Regularization Change: 0.000 -> 1.395
2024-12-02-09:09:46-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-09:09:46-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-09:09:46-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-09:09:46-root-INFO: grad norm: 5.453 5.376 0.911
2024-12-02-09:09:47-root-INFO: grad norm: 6.314 6.255 0.859
2024-12-02-09:09:47-root-INFO: grad norm: 8.633 8.588 0.875
2024-12-02-09:09:48-root-INFO: Loss too large (194.683->195.088)! Learning rate decreased to 0.06378.
2024-12-02-09:09:48-root-INFO: grad norm: 7.783 7.733 0.884
2024-12-02-09:09:48-root-INFO: grad norm: 6.573 6.520 0.835
2024-12-02-09:09:49-root-INFO: Loss Change: 195.879 -> 192.364
2024-12-02-09:09:49-root-INFO: Regularization Change: 0.000 -> 1.431
2024-12-02-09:09:49-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-09:09:49-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-09:09:49-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-09:09:49-root-INFO: grad norm: 7.825 7.757 1.031
2024-12-02-09:09:49-root-INFO: grad norm: 10.439 10.397 0.945
2024-12-02-09:09:50-root-INFO: Loss too large (192.221->193.078)! Learning rate decreased to 0.06543.
2024-12-02-09:09:50-root-INFO: grad norm: 8.794 8.744 0.932
2024-12-02-09:09:51-root-INFO: grad norm: 6.595 6.543 0.830
2024-12-02-09:09:51-root-INFO: grad norm: 6.013 5.956 0.823
2024-12-02-09:09:51-root-INFO: Loss Change: 192.731 -> 188.457
2024-12-02-09:09:51-root-INFO: Regularization Change: 0.000 -> 1.390
2024-12-02-09:09:51-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-09:09:51-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-09:09:52-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-09:09:52-root-INFO: grad norm: 4.935 4.855 0.882
2024-12-02-09:09:52-root-INFO: grad norm: 5.490 5.432 0.798
2024-12-02-09:09:53-root-INFO: grad norm: 7.258 7.212 0.810
2024-12-02-09:09:53-root-INFO: Loss too large (187.077->187.140)! Learning rate decreased to 0.06711.
2024-12-02-09:09:53-root-INFO: grad norm: 6.600 6.550 0.809
2024-12-02-09:09:54-root-INFO: grad norm: 5.712 5.659 0.776
2024-12-02-09:09:54-root-INFO: Loss Change: 188.404 -> 184.937
2024-12-02-09:09:54-root-INFO: Regularization Change: 0.000 -> 1.423
2024-12-02-09:09:54-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-09:09:54-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-09:09:54-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-09:09:55-root-INFO: grad norm: 6.875 6.806 0.975
2024-12-02-09:09:55-root-INFO: grad norm: 9.042 9.002 0.852
2024-12-02-09:09:55-root-INFO: Loss too large (184.537->185.045)! Learning rate decreased to 0.06882.
2024-12-02-09:09:56-root-INFO: grad norm: 7.702 7.654 0.857
2024-12-02-09:09:56-root-INFO: grad norm: 5.940 5.891 0.766
2024-12-02-09:09:56-root-INFO: grad norm: 5.441 5.386 0.769
2024-12-02-09:09:57-root-INFO: Loss Change: 185.107 -> 181.138
2024-12-02-09:09:57-root-INFO: Regularization Change: 0.000 -> 1.377
2024-12-02-09:09:57-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-09:09:57-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-09:09:57-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-09:09:57-root-INFO: grad norm: 4.440 4.360 0.841
2024-12-02-09:09:58-root-INFO: grad norm: 4.501 4.440 0.736
2024-12-02-09:09:58-root-INFO: grad norm: 5.457 5.407 0.742
2024-12-02-09:09:58-root-INFO: grad norm: 6.321 6.276 0.754
2024-12-02-09:09:59-root-INFO: grad norm: 8.340 8.305 0.767
2024-12-02-09:09:59-root-INFO: Loss too large (178.565->179.010)! Learning rate decreased to 0.07057.
2024-12-02-09:09:59-root-INFO: Loss Change: 181.108 -> 178.166
2024-12-02-09:09:59-root-INFO: Regularization Change: 0.000 -> 1.788
2024-12-02-09:09:59-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-09:09:59-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-09:10:00-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-09:10:00-root-INFO: grad norm: 7.893 7.839 0.918
2024-12-02-09:10:00-root-INFO: grad norm: 8.991 8.953 0.827
2024-12-02-09:10:00-root-INFO: Loss too large (177.251->177.600)! Learning rate decreased to 0.07235.
2024-12-02-09:10:01-root-INFO: grad norm: 7.065 7.020 0.797
2024-12-02-09:10:01-root-INFO: grad norm: 4.730 4.676 0.714
2024-12-02-09:10:02-root-INFO: grad norm: 4.216 4.156 0.704
2024-12-02-09:10:02-root-INFO: Loss Change: 178.396 -> 173.720
2024-12-02-09:10:02-root-INFO: Regularization Change: 0.000 -> 1.472
2024-12-02-09:10:02-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-09:10:02-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-09:10:02-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-09:10:02-root-INFO: grad norm: 3.627 3.548 0.752
2024-12-02-09:10:03-root-INFO: grad norm: 3.604 3.539 0.680
2024-12-02-09:10:03-root-INFO: grad norm: 4.004 3.945 0.687
2024-12-02-09:10:04-root-INFO: grad norm: 4.525 4.474 0.683
2024-12-02-09:10:04-root-INFO: grad norm: 5.772 5.730 0.691
2024-12-02-09:10:05-root-INFO: Loss Change: 173.786 -> 170.805
2024-12-02-09:10:05-root-INFO: Regularization Change: 0.000 -> 1.910
2024-12-02-09:10:05-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-09:10:05-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-09:10:05-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-09:10:05-root-INFO: grad norm: 7.127 7.078 0.840
2024-12-02-09:10:06-root-INFO: grad norm: 7.761 7.722 0.772
2024-12-02-09:10:06-root-INFO: Loss too large (169.753->169.768)! Learning rate decreased to 0.07600.
2024-12-02-09:10:06-root-INFO: grad norm: 6.023 5.980 0.720
2024-12-02-09:10:07-root-INFO: grad norm: 4.104 4.049 0.668
2024-12-02-09:10:07-root-INFO: grad norm: 3.650 3.591 0.651
2024-12-02-09:10:08-root-INFO: Loss Change: 170.876 -> 166.527
2024-12-02-09:10:08-root-INFO: Regularization Change: 0.000 -> 1.446
2024-12-02-09:10:08-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-09:10:08-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-09:10:08-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-09:10:08-root-INFO: grad norm: 3.607 3.526 0.760
2024-12-02-09:10:08-root-INFO: grad norm: 3.254 3.189 0.647
2024-12-02-09:10:09-root-INFO: grad norm: 3.197 3.130 0.649
2024-12-02-09:10:09-root-INFO: grad norm: 3.254 3.192 0.632
2024-12-02-09:10:10-root-INFO: grad norm: 3.484 3.427 0.631
2024-12-02-09:10:10-root-INFO: Loss Change: 166.481 -> 162.816
2024-12-02-09:10:10-root-INFO: Regularization Change: 0.000 -> 1.849
2024-12-02-09:10:10-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-09:10:10-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-09:10:10-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-09:10:10-root-INFO: grad norm: 4.633 4.574 0.740
2024-12-02-09:10:11-root-INFO: grad norm: 5.388 5.347 0.657
2024-12-02-09:10:11-root-INFO: grad norm: 5.796 5.755 0.687
2024-12-02-09:10:12-root-INFO: grad norm: 6.525 6.491 0.671
2024-12-02-09:10:12-root-INFO: grad norm: 6.535 6.496 0.708
2024-12-02-09:10:13-root-INFO: Loss Change: 162.939 -> 159.639
2024-12-02-09:10:13-root-INFO: Regularization Change: 0.000 -> 1.974
2024-12-02-09:10:13-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-09:10:13-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-09:10:13-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-09:10:13-root-INFO: grad norm: 5.272 5.231 0.658
2024-12-02-09:10:13-root-INFO: grad norm: 5.082 5.041 0.642
2024-12-02-09:10:14-root-INFO: grad norm: 4.905 4.865 0.620
2024-12-02-09:10:14-root-INFO: grad norm: 4.890 4.849 0.635
2024-12-02-09:10:15-root-INFO: grad norm: 4.862 4.824 0.614
2024-12-02-09:10:15-root-INFO: Loss Change: 159.408 -> 156.037
2024-12-02-09:10:15-root-INFO: Regularization Change: 0.000 -> 1.976
2024-12-02-09:10:15-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-09:10:15-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-09:10:15-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-09:10:15-root-INFO: grad norm: 6.003 5.947 0.822
2024-12-02-09:10:16-root-INFO: grad norm: 5.624 5.585 0.664
2024-12-02-09:10:16-root-INFO: grad norm: 5.448 5.404 0.692
2024-12-02-09:10:17-root-INFO: grad norm: 5.224 5.185 0.644
2024-12-02-09:10:17-root-INFO: grad norm: 5.105 5.061 0.662
2024-12-02-09:10:18-root-INFO: Loss Change: 156.325 -> 152.409
2024-12-02-09:10:18-root-INFO: Regularization Change: 0.000 -> 2.070
2024-12-02-09:10:18-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-09:10:18-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-09:10:18-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-09:10:18-root-INFO: grad norm: 3.990 3.944 0.606
2024-12-02-09:10:18-root-INFO: grad norm: 3.803 3.758 0.581
2024-12-02-09:10:19-root-INFO: grad norm: 3.735 3.692 0.565
2024-12-02-09:10:19-root-INFO: grad norm: 3.732 3.688 0.571
2024-12-02-09:10:20-root-INFO: grad norm: 3.753 3.711 0.560
2024-12-02-09:10:20-root-INFO: Loss Change: 152.067 -> 148.721
2024-12-02-09:10:20-root-INFO: Regularization Change: 0.000 -> 1.904
2024-12-02-09:10:20-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-09:10:20-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-09:10:20-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-09:10:20-root-INFO: grad norm: 4.581 4.530 0.682
2024-12-02-09:10:21-root-INFO: grad norm: 4.355 4.314 0.594
2024-12-02-09:10:21-root-INFO: grad norm: 4.214 4.171 0.605
2024-12-02-09:10:22-root-INFO: grad norm: 4.103 4.062 0.578
2024-12-02-09:10:22-root-INFO: grad norm: 4.029 3.987 0.587
2024-12-02-09:10:22-root-INFO: Loss Change: 148.933 -> 145.472
2024-12-02-09:10:22-root-INFO: Regularization Change: 0.000 -> 1.944
2024-12-02-09:10:22-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-09:10:22-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-09:10:23-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-09:10:23-root-INFO: grad norm: 3.569 3.518 0.605
2024-12-02-09:10:23-root-INFO: grad norm: 3.189 3.143 0.538
2024-12-02-09:10:24-root-INFO: grad norm: 3.033 2.989 0.517
2024-12-02-09:10:24-root-INFO: grad norm: 2.972 2.925 0.523
2024-12-02-09:10:25-root-INFO: grad norm: 2.927 2.883 0.510
2024-12-02-09:10:25-root-INFO: Loss Change: 145.178 -> 141.803
2024-12-02-09:10:25-root-INFO: Regularization Change: 0.000 -> 1.894
2024-12-02-09:10:25-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-09:10:25-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-09:10:25-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-09:10:25-root-INFO: grad norm: 3.735 3.680 0.634
2024-12-02-09:10:26-root-INFO: grad norm: 3.572 3.530 0.542
2024-12-02-09:10:26-root-INFO: grad norm: 3.532 3.488 0.561
2024-12-02-09:10:27-root-INFO: grad norm: 3.481 3.440 0.536
2024-12-02-09:10:27-root-INFO: grad norm: 3.446 3.402 0.544
2024-12-02-09:10:27-root-INFO: Loss Change: 142.000 -> 138.771
2024-12-02-09:10:27-root-INFO: Regularization Change: 0.000 -> 1.920
2024-12-02-09:10:27-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-09:10:27-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-09:10:28-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-09:10:28-root-INFO: grad norm: 3.342 3.290 0.588
2024-12-02-09:10:28-root-INFO: grad norm: 2.897 2.852 0.510
2024-12-02-09:10:29-root-INFO: grad norm: 2.751 2.707 0.488
2024-12-02-09:10:29-root-INFO: grad norm: 2.702 2.657 0.494
2024-12-02-09:10:29-root-INFO: grad norm: 2.674 2.630 0.484
2024-12-02-09:10:30-root-INFO: Loss Change: 138.555 -> 135.230
2024-12-02-09:10:30-root-INFO: Regularization Change: 0.000 -> 1.928
2024-12-02-09:10:30-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-09:10:30-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-09:10:30-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-09:10:30-root-INFO: grad norm: 3.438 3.384 0.610
2024-12-02-09:10:31-root-INFO: grad norm: 3.240 3.198 0.518
2024-12-02-09:10:31-root-INFO: grad norm: 3.178 3.135 0.526
2024-12-02-09:10:31-root-INFO: grad norm: 3.114 3.072 0.509
2024-12-02-09:10:32-root-INFO: grad norm: 3.052 3.010 0.508
2024-12-02-09:10:32-root-INFO: Loss Change: 135.229 -> 132.071
2024-12-02-09:10:32-root-INFO: Regularization Change: 0.000 -> 1.955
2024-12-02-09:10:32-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-09:10:32-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-09:10:32-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-09:10:33-root-INFO: grad norm: 2.801 2.756 0.505
2024-12-02-09:10:33-root-INFO: grad norm: 2.666 2.623 0.474
2024-12-02-09:10:34-root-INFO: grad norm: 2.666 2.623 0.474
2024-12-02-09:10:34-root-INFO: grad norm: 2.733 2.692 0.472
2024-12-02-09:10:34-root-INFO: grad norm: 2.764 2.723 0.477
2024-12-02-09:10:35-root-INFO: Loss Change: 132.006 -> 128.931
2024-12-02-09:10:35-root-INFO: Regularization Change: 0.000 -> 1.920
2024-12-02-09:10:35-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-09:10:35-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-09:10:35-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-09:10:35-root-INFO: grad norm: 4.084 4.028 0.677
2024-12-02-09:10:36-root-INFO: grad norm: 3.630 3.589 0.545
2024-12-02-09:10:36-root-INFO: grad norm: 3.137 3.096 0.505
2024-12-02-09:10:36-root-INFO: grad norm: 3.019 2.978 0.495
2024-12-02-09:10:37-root-INFO: grad norm: 2.956 2.917 0.478
2024-12-02-09:10:37-root-INFO: Loss Change: 129.094 -> 125.871
2024-12-02-09:10:37-root-INFO: Regularization Change: 0.000 -> 2.028
2024-12-02-09:10:37-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-09:10:37-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-09:10:37-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-09:10:38-root-INFO: grad norm: 2.831 2.791 0.476
2024-12-02-09:10:38-root-INFO: grad norm: 2.713 2.677 0.446
2024-12-02-09:10:38-root-INFO: grad norm: 2.723 2.685 0.453
2024-12-02-09:10:39-root-INFO: grad norm: 2.810 2.774 0.448
2024-12-02-09:10:39-root-INFO: grad norm: 2.860 2.823 0.460
2024-12-02-09:10:40-root-INFO: Loss Change: 125.782 -> 122.781
2024-12-02-09:10:40-root-INFO: Regularization Change: 0.000 -> 1.977
2024-12-02-09:10:40-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-09:10:40-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-09:10:40-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-09:10:40-root-INFO: grad norm: 3.816 3.764 0.625
2024-12-02-09:10:40-root-INFO: grad norm: 3.509 3.471 0.514
2024-12-02-09:10:41-root-INFO: grad norm: 3.201 3.163 0.489
2024-12-02-09:10:41-root-INFO: grad norm: 3.125 3.087 0.482
2024-12-02-09:10:42-root-INFO: grad norm: 3.074 3.039 0.462
2024-12-02-09:10:42-root-INFO: Loss Change: 122.966 -> 119.920
2024-12-02-09:10:42-root-INFO: Regularization Change: 0.000 -> 2.073
2024-12-02-09:10:42-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-09:10:42-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-09:10:42-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-09:10:42-root-INFO: grad norm: 3.035 2.994 0.497
2024-12-02-09:10:43-root-INFO: grad norm: 2.809 2.775 0.433
2024-12-02-09:10:43-root-INFO: grad norm: 2.804 2.770 0.437
2024-12-02-09:10:44-root-INFO: grad norm: 2.918 2.886 0.429
2024-12-02-09:10:44-root-INFO: grad norm: 2.991 2.957 0.450
2024-12-02-09:10:45-root-INFO: Loss Change: 119.653 -> 116.638
2024-12-02-09:10:45-root-INFO: Regularization Change: 0.000 -> 2.083
2024-12-02-09:10:45-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-09:10:45-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-09:10:45-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-09:10:45-root-INFO: grad norm: 3.790 3.750 0.552
2024-12-02-09:10:45-root-INFO: grad norm: 3.555 3.519 0.502
2024-12-02-09:10:46-root-INFO: grad norm: 3.260 3.227 0.461
2024-12-02-09:10:46-root-INFO: grad norm: 3.200 3.165 0.471
2024-12-02-09:10:47-root-INFO: grad norm: 3.156 3.125 0.442
2024-12-02-09:10:47-root-INFO: Loss Change: 116.738 -> 113.781
2024-12-02-09:10:47-root-INFO: Regularization Change: 0.000 -> 2.145
2024-12-02-09:10:47-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-09:10:47-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-09:10:47-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-09:10:48-root-INFO: grad norm: 3.085 3.052 0.451
2024-12-02-09:10:48-root-INFO: grad norm: 2.908 2.879 0.409
2024-12-02-09:10:48-root-INFO: grad norm: 2.932 2.901 0.422
2024-12-02-09:10:49-root-INFO: grad norm: 3.068 3.040 0.414
2024-12-02-09:10:49-root-INFO: grad norm: 3.153 3.121 0.444
2024-12-02-09:10:50-root-INFO: Loss Change: 113.689 -> 110.764
2024-12-02-09:10:50-root-INFO: Regularization Change: 0.000 -> 2.145
2024-12-02-09:10:50-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-09:10:50-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-09:10:50-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-09:10:50-root-INFO: grad norm: 4.325 4.279 0.625
2024-12-02-09:10:50-root-INFO: grad norm: 3.968 3.933 0.525
2024-12-02-09:10:51-root-INFO: grad norm: 3.533 3.502 0.470
2024-12-02-09:10:51-root-INFO: grad norm: 3.473 3.440 0.478
2024-12-02-09:10:52-root-INFO: grad norm: 3.450 3.422 0.444
2024-12-02-09:10:52-root-INFO: Loss Change: 110.948 -> 107.979
2024-12-02-09:10:52-root-INFO: Regularization Change: 0.000 -> 2.283
2024-12-02-09:10:52-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-09:10:52-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-09:10:52-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-09:10:53-root-INFO: grad norm: 3.289 3.261 0.431
2024-12-02-09:10:53-root-INFO: grad norm: 3.212 3.186 0.404
2024-12-02-09:10:54-root-INFO: grad norm: 3.258 3.229 0.433
2024-12-02-09:10:54-root-INFO: grad norm: 3.385 3.359 0.415
2024-12-02-09:10:54-root-INFO: grad norm: 3.446 3.415 0.457
2024-12-02-09:10:55-root-INFO: Loss Change: 107.728 -> 104.841
2024-12-02-09:10:55-root-INFO: Regularization Change: 0.000 -> 2.252
2024-12-02-09:10:55-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-09:10:55-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-09:10:55-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-09:10:55-root-INFO: grad norm: 4.609 4.568 0.613
2024-12-02-09:10:56-root-INFO: grad norm: 4.175 4.136 0.563
2024-12-02-09:10:56-root-INFO: grad norm: 3.653 3.624 0.459
2024-12-02-09:10:57-root-INFO: grad norm: 3.577 3.543 0.490
2024-12-02-09:10:57-root-INFO: grad norm: 3.544 3.518 0.432
2024-12-02-09:10:58-root-INFO: Loss Change: 104.922 -> 101.910
2024-12-02-09:10:58-root-INFO: Regularization Change: 0.000 -> 2.409
2024-12-02-09:10:58-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-09:10:58-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-09:10:58-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-09:10:58-root-INFO: grad norm: 3.390 3.364 0.422
2024-12-02-09:10:58-root-INFO: grad norm: 3.280 3.257 0.389
2024-12-02-09:10:59-root-INFO: grad norm: 3.328 3.300 0.429
2024-12-02-09:10:59-root-INFO: grad norm: 3.461 3.437 0.402
2024-12-02-09:11:00-root-INFO: grad norm: 3.528 3.499 0.457
2024-12-02-09:11:00-root-INFO: Loss Change: 101.798 -> 98.921
2024-12-02-09:11:00-root-INFO: Regularization Change: 0.000 -> 2.361
2024-12-02-09:11:00-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-09:11:00-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-09:11:00-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-09:11:01-root-INFO: grad norm: 4.554 4.512 0.611
2024-12-02-09:11:01-root-INFO: grad norm: 4.205 4.168 0.555
2024-12-02-09:11:01-root-INFO: grad norm: 3.806 3.778 0.459
2024-12-02-09:11:02-root-INFO: grad norm: 3.748 3.715 0.496
2024-12-02-09:11:02-root-INFO: grad norm: 3.722 3.697 0.434
2024-12-02-09:11:03-root-INFO: Loss Change: 99.085 -> 96.188
2024-12-02-09:11:03-root-INFO: Regularization Change: 0.000 -> 2.515
2024-12-02-09:11:03-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-09:11:03-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-09:11:03-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-09:11:03-root-INFO: grad norm: 3.539 3.514 0.423
2024-12-02-09:11:04-root-INFO: grad norm: 3.535 3.512 0.400
2024-12-02-09:11:04-root-INFO: grad norm: 3.607 3.579 0.444
2024-12-02-09:11:04-root-INFO: grad norm: 3.741 3.718 0.420
2024-12-02-09:11:05-root-INFO: grad norm: 3.793 3.763 0.476
2024-12-02-09:11:05-root-INFO: Loss Change: 95.941 -> 93.118
2024-12-02-09:11:05-root-INFO: Regularization Change: 0.000 -> 2.480
2024-12-02-09:11:05-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-09:11:05-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-09:11:05-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-09:11:06-root-INFO: grad norm: 4.732 4.692 0.611
2024-12-02-09:11:06-root-INFO: grad norm: 4.356 4.318 0.577
2024-12-02-09:11:06-root-INFO: grad norm: 3.928 3.900 0.469
2024-12-02-09:11:07-root-INFO: grad norm: 3.871 3.836 0.517
2024-12-02-09:11:07-root-INFO: grad norm: 3.856 3.830 0.450
2024-12-02-09:11:08-root-INFO: Loss Change: 93.319 -> 90.447
2024-12-02-09:11:08-root-INFO: Regularization Change: 0.000 -> 2.631
2024-12-02-09:11:08-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-09:11:08-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-09:11:08-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-09:11:08-root-INFO: grad norm: 3.722 3.693 0.463
2024-12-02-09:11:09-root-INFO: grad norm: 3.745 3.720 0.432
2024-12-02-09:11:09-root-INFO: grad norm: 3.858 3.824 0.511
2024-12-02-09:11:09-root-INFO: grad norm: 4.019 3.991 0.476
2024-12-02-09:11:10-root-INFO: grad norm: 4.101 4.063 0.554
2024-12-02-09:11:10-root-INFO: Loss Change: 90.275 -> 87.582
2024-12-02-09:11:10-root-INFO: Regularization Change: 0.000 -> 2.598
2024-12-02-09:11:10-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-09:11:10-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-09:11:10-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-09:11:11-root-INFO: grad norm: 4.972 4.924 0.690
2024-12-02-09:11:11-root-INFO: grad norm: 4.624 4.582 0.626
2024-12-02-09:11:11-root-INFO: grad norm: 4.255 4.221 0.536
2024-12-02-09:11:12-root-INFO: grad norm: 4.162 4.124 0.564
2024-12-02-09:11:12-root-INFO: grad norm: 4.087 4.057 0.497
2024-12-02-09:11:13-root-INFO: Loss Change: 87.772 -> 84.904
2024-12-02-09:11:13-root-INFO: Regularization Change: 0.000 -> 2.784
2024-12-02-09:11:13-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-09:11:13-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-09:11:13-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-09:11:13-root-INFO: grad norm: 3.836 3.805 0.488
2024-12-02-09:11:13-root-INFO: grad norm: 3.812 3.785 0.449
2024-12-02-09:11:14-root-INFO: grad norm: 3.867 3.835 0.500
2024-12-02-09:11:14-root-INFO: grad norm: 3.970 3.943 0.461
2024-12-02-09:11:15-root-INFO: grad norm: 4.009 3.975 0.521
2024-12-02-09:11:15-root-INFO: Loss Change: 84.479 -> 81.817
2024-12-02-09:11:15-root-INFO: Regularization Change: 0.000 -> 2.651
2024-12-02-09:11:15-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-09:11:15-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-09:11:15-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-09:11:15-root-INFO: grad norm: 4.907 4.866 0.631
2024-12-02-09:11:16-root-INFO: grad norm: 4.463 4.423 0.597
2024-12-02-09:11:16-root-INFO: grad norm: 4.032 4.003 0.481
2024-12-02-09:11:17-root-INFO: grad norm: 3.932 3.896 0.528
2024-12-02-09:11:17-root-INFO: grad norm: 3.863 3.836 0.452
2024-12-02-09:11:18-root-INFO: Loss Change: 82.066 -> 79.213
2024-12-02-09:11:18-root-INFO: Regularization Change: 0.000 -> 2.780
2024-12-02-09:11:18-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-09:11:18-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-09:11:18-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-09:11:18-root-INFO: grad norm: 3.678 3.652 0.440
2024-12-02-09:11:18-root-INFO: grad norm: 3.623 3.600 0.411
2024-12-02-09:11:19-root-INFO: grad norm: 3.688 3.657 0.475
2024-12-02-09:11:19-root-INFO: grad norm: 3.794 3.770 0.429
2024-12-02-09:11:20-root-INFO: grad norm: 3.860 3.827 0.501
2024-12-02-09:11:20-root-INFO: Loss Change: 79.030 -> 76.535
2024-12-02-09:11:20-root-INFO: Regularization Change: 0.000 -> 2.622
2024-12-02-09:11:20-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-09:11:20-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-09:11:20-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-09:11:21-root-INFO: grad norm: 4.737 4.691 0.653
2024-12-02-09:11:21-root-INFO: grad norm: 4.430 4.391 0.588
2024-12-02-09:11:21-root-INFO: grad norm: 4.151 4.121 0.497
2024-12-02-09:11:22-root-INFO: grad norm: 4.062 4.026 0.538
2024-12-02-09:11:22-root-INFO: grad norm: 3.987 3.960 0.465
2024-12-02-09:11:23-root-INFO: Loss Change: 76.755 -> 74.125
2024-12-02-09:11:23-root-INFO: Regularization Change: 0.000 -> 2.802
2024-12-02-09:11:23-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-09:11:23-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-09:11:23-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-09:11:23-root-INFO: grad norm: 3.666 3.638 0.453
2024-12-02-09:11:23-root-INFO: grad norm: 3.629 3.606 0.407
2024-12-02-09:11:24-root-INFO: grad norm: 3.710 3.679 0.477
2024-12-02-09:11:24-root-INFO: grad norm: 3.817 3.792 0.432
2024-12-02-09:11:25-root-INFO: grad norm: 3.889 3.856 0.505
2024-12-02-09:11:25-root-INFO: Loss Change: 73.836 -> 71.479
2024-12-02-09:11:25-root-INFO: Regularization Change: 0.000 -> 2.664
2024-12-02-09:11:25-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-09:11:25-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-09:11:25-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-09:11:26-root-INFO: grad norm: 4.666 4.624 0.622
2024-12-02-09:11:26-root-INFO: grad norm: 4.379 4.341 0.581
2024-12-02-09:11:26-root-INFO: grad norm: 4.136 4.108 0.479
2024-12-02-09:11:27-root-INFO: grad norm: 4.057 4.022 0.529
2024-12-02-09:11:27-root-INFO: grad norm: 4.022 3.997 0.451
2024-12-02-09:11:28-root-INFO: Loss Change: 71.574 -> 69.025
2024-12-02-09:11:28-root-INFO: Regularization Change: 0.000 -> 2.863
2024-12-02-09:11:28-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-09:11:28-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-09:11:28-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-09:11:28-root-INFO: grad norm: 3.693 3.669 0.424
2024-12-02-09:11:29-root-INFO: grad norm: 3.728 3.706 0.400
2024-12-02-09:11:29-root-INFO: grad norm: 3.854 3.826 0.465
2024-12-02-09:11:29-root-INFO: grad norm: 4.016 3.993 0.425
2024-12-02-09:11:30-root-INFO: grad norm: 4.113 4.083 0.497
2024-12-02-09:11:30-root-INFO: Loss Change: 68.821 -> 66.636
2024-12-02-09:11:30-root-INFO: Regularization Change: 0.000 -> 2.757
2024-12-02-09:11:30-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-09:11:30-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-09:11:30-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-09:11:31-root-INFO: grad norm: 4.924 4.883 0.633
2024-12-02-09:11:31-root-INFO: grad norm: 4.626 4.589 0.586
2024-12-02-09:11:32-root-INFO: grad norm: 4.339 4.312 0.477
2024-12-02-09:11:32-root-INFO: grad norm: 4.200 4.167 0.522
2024-12-02-09:11:33-root-INFO: grad norm: 4.083 4.059 0.443
2024-12-02-09:11:33-root-INFO: Loss Change: 66.692 -> 64.067
2024-12-02-09:11:33-root-INFO: Regularization Change: 0.000 -> 3.000
2024-12-02-09:11:33-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-09:11:33-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-09:11:33-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-09:11:33-root-INFO: grad norm: 3.761 3.739 0.412
2024-12-02-09:11:34-root-INFO: grad norm: 3.725 3.705 0.381
2024-12-02-09:11:34-root-INFO: grad norm: 3.782 3.755 0.448
2024-12-02-09:11:35-root-INFO: grad norm: 3.848 3.828 0.399
2024-12-02-09:11:35-root-INFO: grad norm: 3.914 3.885 0.469
2024-12-02-09:11:35-root-INFO: Loss Change: 63.765 -> 61.553
2024-12-02-09:11:35-root-INFO: Regularization Change: 0.000 -> 2.816
2024-12-02-09:11:35-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-09:11:35-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-09:11:36-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-09:11:36-root-INFO: grad norm: 4.953 4.903 0.702
2024-12-02-09:11:36-root-INFO: grad norm: 4.551 4.513 0.583
2024-12-02-09:11:37-root-INFO: grad norm: 4.221 4.195 0.463
2024-12-02-09:11:37-root-INFO: grad norm: 4.029 3.997 0.500
2024-12-02-09:11:38-root-INFO: grad norm: 3.882 3.860 0.416
2024-12-02-09:11:38-root-INFO: Loss Change: 61.867 -> 59.056
2024-12-02-09:11:38-root-INFO: Regularization Change: 0.000 -> 3.111
2024-12-02-09:11:38-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-09:11:38-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-09:11:38-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-09:11:38-root-INFO: grad norm: 3.301 3.283 0.343
2024-12-02-09:11:39-root-INFO: grad norm: 3.187 3.171 0.320
2024-12-02-09:11:39-root-INFO: grad norm: 3.236 3.214 0.375
2024-12-02-09:11:40-root-INFO: grad norm: 3.307 3.290 0.339
2024-12-02-09:11:40-root-INFO: grad norm: 3.412 3.389 0.400
2024-12-02-09:11:40-root-INFO: Loss Change: 58.674 -> 56.501
2024-12-02-09:11:40-root-INFO: Regularization Change: 0.000 -> 2.787
2024-12-02-09:11:40-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-09:11:40-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-09:11:41-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-09:11:41-root-INFO: grad norm: 4.082 4.046 0.546
2024-12-02-09:11:41-root-INFO: grad norm: 3.928 3.898 0.487
2024-12-02-09:11:42-root-INFO: grad norm: 3.817 3.795 0.408
2024-12-02-09:11:42-root-INFO: grad norm: 3.716 3.689 0.450
2024-12-02-09:11:42-root-INFO: grad norm: 3.640 3.620 0.384
2024-12-02-09:11:43-root-INFO: Loss Change: 56.561 -> 54.122
2024-12-02-09:11:43-root-INFO: Regularization Change: 0.000 -> 2.957
2024-12-02-09:11:43-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-09:11:43-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-09:11:43-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-09:11:43-root-INFO: grad norm: 3.197 3.182 0.317
2024-12-02-09:11:44-root-INFO: grad norm: 3.064 3.048 0.307
2024-12-02-09:11:44-root-INFO: grad norm: 3.067 3.047 0.348
2024-12-02-09:11:45-root-INFO: grad norm: 3.093 3.076 0.323
2024-12-02-09:11:45-root-INFO: grad norm: 3.150 3.128 0.364
2024-12-02-09:11:45-root-INFO: Loss Change: 53.772 -> 51.618
2024-12-02-09:11:45-root-INFO: Regularization Change: 0.000 -> 2.780
2024-12-02-09:11:45-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-09:11:45-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-09:11:46-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-09:11:46-root-INFO: grad norm: 3.967 3.928 0.560
2024-12-02-09:11:46-root-INFO: grad norm: 3.679 3.650 0.464
2024-12-02-09:11:47-root-INFO: grad norm: 3.495 3.474 0.379
2024-12-02-09:11:47-root-INFO: grad norm: 3.333 3.308 0.405
2024-12-02-09:11:48-root-INFO: grad norm: 3.229 3.211 0.348
2024-12-02-09:11:48-root-INFO: Loss Change: 51.742 -> 49.200
2024-12-02-09:11:48-root-INFO: Regularization Change: 0.000 -> 2.951
2024-12-02-09:11:48-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-09:11:48-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-09:11:48-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-09:11:48-root-INFO: grad norm: 2.817 2.803 0.281
2024-12-02-09:11:49-root-INFO: grad norm: 2.690 2.676 0.275
2024-12-02-09:11:49-root-INFO: grad norm: 2.676 2.659 0.303
2024-12-02-09:11:50-root-INFO: grad norm: 2.696 2.681 0.285
2024-12-02-09:11:50-root-INFO: grad norm: 2.746 2.728 0.318
2024-12-02-09:11:50-root-INFO: Loss Change: 48.892 -> 46.793
2024-12-02-09:11:50-root-INFO: Regularization Change: 0.000 -> 2.706
2024-12-02-09:11:50-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-09:11:50-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-09:11:51-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-09:11:51-root-INFO: grad norm: 3.474 3.440 0.490
2024-12-02-09:11:51-root-INFO: grad norm: 3.268 3.241 0.415
2024-12-02-09:11:52-root-INFO: grad norm: 3.131 3.113 0.337
2024-12-02-09:11:52-root-INFO: grad norm: 3.006 2.984 0.362
2024-12-02-09:11:53-root-INFO: grad norm: 2.925 2.908 0.312
2024-12-02-09:11:53-root-INFO: Loss Change: 46.889 -> 44.529
2024-12-02-09:11:53-root-INFO: Regularization Change: 0.000 -> 2.858
2024-12-02-09:11:53-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-09:11:53-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-09:11:53-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-09:11:53-root-INFO: grad norm: 2.514 2.500 0.263
2024-12-02-09:11:54-root-INFO: grad norm: 2.354 2.343 0.230
2024-12-02-09:11:54-root-INFO: grad norm: 2.324 2.308 0.268
2024-12-02-09:11:55-root-INFO: grad norm: 2.333 2.321 0.240
2024-12-02-09:11:55-root-INFO: grad norm: 2.369 2.352 0.279
2024-12-02-09:11:55-root-INFO: Loss Change: 44.198 -> 42.109
2024-12-02-09:11:56-root-INFO: Regularization Change: 0.000 -> 2.668
2024-12-02-09:11:56-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-09:11:56-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-09:11:56-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-09:11:56-root-INFO: grad norm: 3.051 3.018 0.450
2024-12-02-09:11:56-root-INFO: grad norm: 2.869 2.847 0.352
2024-12-02-09:11:57-root-INFO: grad norm: 2.764 2.748 0.293
2024-12-02-09:11:57-root-INFO: grad norm: 2.663 2.645 0.312
2024-12-02-09:11:58-root-INFO: grad norm: 2.595 2.581 0.271
2024-12-02-09:11:58-root-INFO: Loss Change: 41.979 -> 39.738
2024-12-02-09:11:58-root-INFO: Regularization Change: 0.000 -> 2.804
2024-12-02-09:11:58-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-09:11:58-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-09:11:58-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-09:11:58-root-INFO: grad norm: 2.228 2.212 0.270
2024-12-02-09:11:59-root-INFO: grad norm: 1.986 1.976 0.204
2024-12-02-09:11:59-root-INFO: grad norm: 1.935 1.921 0.237
2024-12-02-09:12:00-root-INFO: grad norm: 1.933 1.922 0.207
2024-12-02-09:12:00-root-INFO: grad norm: 1.950 1.935 0.247
2024-12-02-09:12:00-root-INFO: Loss Change: 39.452 -> 37.423
2024-12-02-09:12:00-root-INFO: Regularization Change: 0.000 -> 2.552
2024-12-02-09:12:00-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-09:12:00-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-09:12:01-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-09:12:01-root-INFO: grad norm: 2.569 2.540 0.387
2024-12-02-09:12:01-root-INFO: grad norm: 2.418 2.398 0.312
2024-12-02-09:12:02-root-INFO: grad norm: 2.356 2.343 0.249
2024-12-02-09:12:02-root-INFO: grad norm: 2.309 2.292 0.280
2024-12-02-09:12:03-root-INFO: grad norm: 2.274 2.262 0.234
2024-12-02-09:12:03-root-INFO: Loss Change: 37.384 -> 35.385
2024-12-02-09:12:03-root-INFO: Regularization Change: 0.000 -> 2.613
2024-12-02-09:12:03-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-09:12:03-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-09:12:03-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-09:12:04-root-INFO: grad norm: 2.021 2.004 0.258
2024-12-02-09:12:04-root-INFO: grad norm: 1.806 1.796 0.197
2024-12-02-09:12:05-root-INFO: grad norm: 1.754 1.739 0.225
2024-12-02-09:12:05-root-INFO: grad norm: 1.745 1.734 0.192
2024-12-02-09:12:05-root-INFO: grad norm: 1.751 1.736 0.229
2024-12-02-09:12:06-root-INFO: Loss Change: 35.058 -> 33.142
2024-12-02-09:12:06-root-INFO: Regularization Change: 0.000 -> 2.468
2024-12-02-09:12:06-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-09:12:06-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-09:12:06-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-09:12:06-root-INFO: grad norm: 2.404 2.373 0.384
2024-12-02-09:12:07-root-INFO: grad norm: 2.168 2.149 0.287
2024-12-02-09:12:07-root-INFO: grad norm: 2.056 2.044 0.223
2024-12-02-09:12:08-root-INFO: grad norm: 1.980 1.965 0.243
2024-12-02-09:12:08-root-INFO: grad norm: 1.930 1.919 0.204
2024-12-02-09:12:08-root-INFO: Loss Change: 33.116 -> 31.175
2024-12-02-09:12:08-root-INFO: Regularization Change: 0.000 -> 2.518
2024-12-02-09:12:08-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-09:12:08-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-09:12:08-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-09:12:09-root-INFO: grad norm: 1.777 1.761 0.242
2024-12-02-09:12:09-root-INFO: grad norm: 1.517 1.507 0.170
2024-12-02-09:12:10-root-INFO: grad norm: 1.465 1.454 0.179
2024-12-02-09:12:10-root-INFO: grad norm: 1.455 1.447 0.159
2024-12-02-09:12:11-root-INFO: grad norm: 1.457 1.446 0.182
2024-12-02-09:12:11-root-INFO: Loss Change: 30.763 -> 28.937
2024-12-02-09:12:11-root-INFO: Regularization Change: 0.000 -> 2.372
2024-12-02-09:12:11-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-09:12:11-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-09:12:11-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-09:12:11-root-INFO: grad norm: 2.024 1.998 0.323
2024-12-02-09:12:12-root-INFO: grad norm: 1.814 1.798 0.238
2024-12-02-09:12:12-root-INFO: grad norm: 1.715 1.705 0.189
2024-12-02-09:12:13-root-INFO: grad norm: 1.656 1.644 0.201
2024-12-02-09:12:13-root-INFO: grad norm: 1.615 1.606 0.175
2024-12-02-09:12:14-root-INFO: Loss Change: 28.854 -> 27.051
2024-12-02-09:12:14-root-INFO: Regularization Change: 0.000 -> 2.396
2024-12-02-09:12:14-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-09:12:14-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-09:12:14-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-09:12:14-root-INFO: grad norm: 1.589 1.572 0.234
2024-12-02-09:12:15-root-INFO: grad norm: 1.313 1.304 0.152
2024-12-02-09:12:15-root-INFO: grad norm: 1.257 1.248 0.152
2024-12-02-09:12:16-root-INFO: grad norm: 1.240 1.232 0.137
2024-12-02-09:12:16-root-INFO: grad norm: 1.230 1.220 0.152
2024-12-02-09:12:16-root-INFO: Loss Change: 26.798 -> 25.075
2024-12-02-09:12:16-root-INFO: Regularization Change: 0.000 -> 2.279
2024-12-02-09:12:16-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-09:12:16-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-09:12:17-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-09:12:17-root-INFO: grad norm: 1.806 1.780 0.309
2024-12-02-09:12:17-root-INFO: grad norm: 1.528 1.515 0.200
2024-12-02-09:12:18-root-INFO: grad norm: 1.421 1.412 0.159
2024-12-02-09:12:18-root-INFO: grad norm: 1.373 1.364 0.164
2024-12-02-09:12:19-root-INFO: grad norm: 1.362 1.354 0.148
2024-12-02-09:12:19-root-INFO: Loss Change: 24.827 -> 23.170
2024-12-02-09:12:19-root-INFO: Regularization Change: 0.000 -> 2.248
2024-12-02-09:12:19-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-09:12:19-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-09:12:19-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-09:12:19-root-INFO: grad norm: 1.504 1.485 0.238
2024-12-02-09:12:20-root-INFO: grad norm: 1.194 1.185 0.141
2024-12-02-09:12:20-root-INFO: grad norm: 1.147 1.139 0.135
2024-12-02-09:12:21-root-INFO: grad norm: 1.158 1.151 0.124
2024-12-02-09:12:21-root-INFO: grad norm: 1.162 1.154 0.137
2024-12-02-09:12:22-root-INFO: Loss Change: 23.004 -> 21.438
2024-12-02-09:12:22-root-INFO: Regularization Change: 0.000 -> 2.128
2024-12-02-09:12:22-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-09:12:22-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-09:12:22-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-09:12:22-root-INFO: grad norm: 1.815 1.790 0.301
2024-12-02-09:12:22-root-INFO: grad norm: 1.448 1.435 0.194
2024-12-02-09:12:23-root-INFO: grad norm: 1.224 1.216 0.140
2024-12-02-09:12:23-root-INFO: grad norm: 1.188 1.179 0.144
2024-12-02-09:12:24-root-INFO: grad norm: 1.186 1.180 0.127
2024-12-02-09:12:24-root-INFO: Loss Change: 21.209 -> 19.674
2024-12-02-09:12:24-root-INFO: Regularization Change: 0.000 -> 2.101
2024-12-02-09:12:24-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-09:12:24-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-09:12:24-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-09:12:25-root-INFO: grad norm: 1.421 1.401 0.235
2024-12-02-09:12:25-root-INFO: grad norm: 1.121 1.113 0.134
2024-12-02-09:12:25-root-INFO: grad norm: 1.082 1.074 0.128
2024-12-02-09:12:26-root-INFO: grad norm: 1.102 1.096 0.116
2024-12-02-09:12:26-root-INFO: grad norm: 1.111 1.104 0.129
2024-12-02-09:12:27-root-INFO: Loss Change: 19.479 -> 18.060
2024-12-02-09:12:27-root-INFO: Regularization Change: 0.000 -> 1.992
2024-12-02-09:12:27-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-09:12:27-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-09:12:27-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-09:12:27-root-INFO: grad norm: 1.747 1.725 0.277
2024-12-02-09:12:27-root-INFO: grad norm: 1.373 1.361 0.180
2024-12-02-09:12:28-root-INFO: grad norm: 1.122 1.115 0.125
2024-12-02-09:12:28-root-INFO: grad norm: 1.094 1.086 0.132
2024-12-02-09:12:29-root-INFO: grad norm: 1.102 1.096 0.116
2024-12-02-09:12:29-root-INFO: Loss Change: 17.902 -> 16.485
2024-12-02-09:12:29-root-INFO: Regularization Change: 0.000 -> 1.990
2024-12-02-09:12:29-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-09:12:29-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-09:12:29-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-09:12:30-root-INFO: grad norm: 1.412 1.393 0.234
2024-12-02-09:12:30-root-INFO: grad norm: 1.065 1.057 0.125
2024-12-02-09:12:30-root-INFO: grad norm: 1.024 1.018 0.114
2024-12-02-09:12:31-root-INFO: grad norm: 1.050 1.044 0.109
2024-12-02-09:12:31-root-INFO: grad norm: 1.064 1.058 0.120
2024-12-02-09:12:32-root-INFO: Loss Change: 16.299 -> 14.976
2024-12-02-09:12:32-root-INFO: Regularization Change: 0.000 -> 1.899
2024-12-02-09:12:32-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-09:12:32-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-09:12:32-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-09:12:32-root-INFO: grad norm: 1.673 1.652 0.259
2024-12-02-09:12:32-root-INFO: grad norm: 1.299 1.289 0.161
2024-12-02-09:12:33-root-INFO: grad norm: 1.058 1.052 0.112
2024-12-02-09:12:33-root-INFO: grad norm: 1.030 1.024 0.118
2024-12-02-09:12:34-root-INFO: grad norm: 1.032 1.027 0.105
2024-12-02-09:12:34-root-INFO: Loss Change: 14.825 -> 13.520
2024-12-02-09:12:34-root-INFO: Regularization Change: 0.000 -> 1.879
2024-12-02-09:12:34-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-09:12:34-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-09:12:34-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-09:12:35-root-INFO: grad norm: 1.386 1.368 0.225
2024-12-02-09:12:35-root-INFO: grad norm: 1.009 1.003 0.115
2024-12-02-09:12:35-root-INFO: grad norm: 0.970 0.965 0.104
2024-12-02-09:12:36-root-INFO: grad norm: 0.996 0.991 0.100
2024-12-02-09:12:36-root-INFO: grad norm: 1.004 0.998 0.109
2024-12-02-09:12:37-root-INFO: Loss Change: 13.361 -> 12.122
2024-12-02-09:12:37-root-INFO: Regularization Change: 0.000 -> 1.815
2024-12-02-09:12:37-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-09:12:37-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-09:12:37-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-09:12:37-root-INFO: grad norm: 1.612 1.594 0.245
2024-12-02-09:12:38-root-INFO: grad norm: 1.220 1.212 0.142
2024-12-02-09:12:38-root-INFO: grad norm: 1.001 0.996 0.096
2024-12-02-09:12:39-root-INFO: grad norm: 0.966 0.960 0.104
2024-12-02-09:12:39-root-INFO: grad norm: 0.951 0.947 0.089
2024-12-02-09:12:39-root-INFO: Loss Change: 12.032 -> 10.834
2024-12-02-09:12:39-root-INFO: Regularization Change: 0.000 -> 1.756
2024-12-02-09:12:39-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-09:12:39-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-09:12:39-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-09:12:40-root-INFO: grad norm: 1.289 1.273 0.200
2024-12-02-09:12:40-root-INFO: grad norm: 0.917 0.911 0.100
2024-12-02-09:12:41-root-INFO: grad norm: 0.876 0.871 0.086
2024-12-02-09:12:41-root-INFO: grad norm: 0.883 0.879 0.083
2024-12-02-09:12:41-root-INFO: grad norm: 0.886 0.882 0.089
2024-12-02-09:12:42-root-INFO: Loss Change: 10.710 -> 9.616
2024-12-02-09:12:42-root-INFO: Regularization Change: 0.000 -> 1.642
2024-12-02-09:12:42-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-09:12:42-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-09:12:42-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-09:12:42-root-INFO: grad norm: 1.528 1.508 0.243
2024-12-02-09:12:43-root-INFO: grad norm: 1.110 1.103 0.119
2024-12-02-09:12:43-root-INFO: grad norm: 0.953 0.950 0.080
2024-12-02-09:12:43-root-INFO: grad norm: 0.910 0.905 0.088
2024-12-02-09:12:44-root-INFO: grad norm: 0.871 0.868 0.072
2024-12-02-09:12:44-root-INFO: Loss Change: 9.554 -> 8.470
2024-12-02-09:12:44-root-INFO: Regularization Change: 0.000 -> 1.606
2024-12-02-09:12:44-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-09:12:44-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-09:12:44-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-09:12:45-root-INFO: grad norm: 1.237 1.221 0.194
2024-12-02-09:12:45-root-INFO: grad norm: 0.820 0.816 0.080
2024-12-02-09:12:46-root-INFO: grad norm: 0.759 0.756 0.067
2024-12-02-09:12:46-root-INFO: grad norm: 0.735 0.732 0.065
2024-12-02-09:12:46-root-INFO: grad norm: 0.717 0.714 0.066
2024-12-02-09:12:47-root-INFO: Loss Change: 8.381 -> 7.405
2024-12-02-09:12:47-root-INFO: Regularization Change: 0.000 -> 1.469
2024-12-02-09:12:47-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-09:12:47-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-09:12:47-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-09:12:47-root-INFO: grad norm: 1.301 1.284 0.209
2024-12-02-09:12:48-root-INFO: grad norm: 0.892 0.888 0.090
2024-12-02-09:12:48-root-INFO: grad norm: 0.795 0.792 0.062
2024-12-02-09:12:48-root-INFO: grad norm: 0.744 0.741 0.066
2024-12-02-09:12:49-root-INFO: grad norm: 0.704 0.702 0.056
2024-12-02-09:12:49-root-INFO: Loss Change: 7.380 -> 6.439
2024-12-02-09:12:49-root-INFO: Regularization Change: 0.000 -> 1.416
2024-12-02-09:12:49-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-09:12:49-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-09:12:49-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-09:12:50-root-INFO: grad norm: 1.138 1.123 0.180
2024-12-02-09:12:50-root-INFO: grad norm: 0.715 0.711 0.070
2024-12-02-09:12:51-root-INFO: grad norm: 0.652 0.650 0.052
2024-12-02-09:12:51-root-INFO: grad norm: 0.624 0.622 0.052
2024-12-02-09:12:52-root-INFO: grad norm: 0.604 0.602 0.049
2024-12-02-09:12:52-root-INFO: Loss Change: 6.400 -> 5.585
2024-12-02-09:12:52-root-INFO: Regularization Change: 0.000 -> 1.248
2024-12-02-09:12:52-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-09:12:52-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-09:12:52-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-09:12:52-root-INFO: grad norm: 1.171 1.155 0.194
2024-12-02-09:12:53-root-INFO: grad norm: 0.699 0.696 0.066
2024-12-02-09:12:53-root-INFO: grad norm: 0.621 0.620 0.046
2024-12-02-09:12:54-root-INFO: grad norm: 0.588 0.586 0.046
2024-12-02-09:12:54-root-INFO: grad norm: 0.564 0.563 0.042
2024-12-02-09:12:54-root-INFO: Loss Change: 5.596 -> 4.842
2024-12-02-09:12:54-root-INFO: Regularization Change: 0.000 -> 1.143
2024-12-02-09:12:54-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-09:12:54-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-09:12:55-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-09:12:55-root-INFO: grad norm: 1.170 1.153 0.197
2024-12-02-09:12:55-root-INFO: grad norm: 0.659 0.656 0.061
2024-12-02-09:12:56-root-INFO: grad norm: 0.576 0.575 0.041
2024-12-02-09:12:56-root-INFO: grad norm: 0.544 0.542 0.040
2024-12-02-09:12:57-root-INFO: grad norm: 0.521 0.520 0.037
2024-12-02-09:12:57-root-INFO: Loss Change: 4.885 -> 4.187
2024-12-02-09:12:57-root-INFO: Regularization Change: 0.000 -> 1.051
2024-12-02-09:12:57-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-09:12:57-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-09:12:57-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-09:12:58-root-INFO: grad norm: 1.082 1.067 0.177
2024-12-02-09:12:58-root-INFO: grad norm: 0.614 0.611 0.054
2024-12-02-09:12:59-root-INFO: grad norm: 0.536 0.535 0.037
2024-12-02-09:12:59-root-INFO: grad norm: 0.506 0.505 0.035
2024-12-02-09:13:00-root-INFO: grad norm: 0.486 0.485 0.033
2024-12-02-09:13:00-root-INFO: Loss Change: 4.241 -> 3.624
2024-12-02-09:13:00-root-INFO: Regularization Change: 0.000 -> 0.943
2024-12-02-09:13:00-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-09:13:00-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-09:13:00-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-09:13:00-root-INFO: grad norm: 1.077 1.062 0.176
2024-12-02-09:13:01-root-INFO: grad norm: 0.601 0.599 0.051
2024-12-02-09:13:01-root-INFO: grad norm: 0.545 0.543 0.038
2024-12-02-09:13:02-root-INFO: grad norm: 0.546 0.545 0.037
2024-12-02-09:13:02-root-INFO: grad norm: 0.607 0.606 0.036
2024-12-02-09:13:02-root-INFO: Loss Change: 3.714 -> 3.162
2024-12-02-09:13:02-root-INFO: Regularization Change: 0.000 -> 0.885
2024-12-02-09:13:02-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-09:13:02-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-09:13:03-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-09:13:03-root-INFO: grad norm: 1.101 1.089 0.164
2024-12-02-09:13:03-root-INFO: grad norm: 0.681 0.680 0.046
2024-12-02-09:13:04-root-INFO: grad norm: 0.674 0.673 0.043
2024-12-02-09:13:04-root-INFO: grad norm: 0.680 0.679 0.039
2024-12-02-09:13:05-root-INFO: grad norm: 0.685 0.684 0.046
2024-12-02-09:13:05-root-INFO: Loss Change: 3.255 -> 2.757
2024-12-02-09:13:05-root-INFO: Regularization Change: 0.000 -> 0.812
2024-12-02-09:13:05-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-09:13:05-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-09:13:05-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-09:13:05-root-INFO: grad norm: 1.141 1.129 0.168
2024-12-02-09:13:06-root-INFO: grad norm: 0.718 0.716 0.050
2024-12-02-09:13:06-root-INFO: grad norm: 0.655 0.653 0.042
2024-12-02-09:13:07-root-INFO: grad norm: 0.629 0.628 0.043
2024-12-02-09:13:07-root-INFO: grad norm: 0.609 0.608 0.038
2024-12-02-09:13:08-root-INFO: Loss Change: 2.890 -> 2.400
2024-12-02-09:13:08-root-INFO: Regularization Change: 0.000 -> 0.743
2024-12-02-09:13:08-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-09:13:08-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-09:13:08-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-09:13:08-root-INFO: grad norm: 0.958 0.949 0.135
2024-12-02-09:13:08-root-INFO: grad norm: 0.529 0.527 0.044
2024-12-02-09:13:09-root-INFO: grad norm: 0.469 0.468 0.036
2024-12-02-09:13:09-root-INFO: grad norm: 0.442 0.441 0.036
2024-12-02-09:13:10-root-INFO: grad norm: 0.413 0.412 0.034
2024-12-02-09:13:10-root-INFO: Loss Change: 2.519 -> 2.108
2024-12-02-09:13:10-root-INFO: Regularization Change: 0.000 -> 0.645
2024-12-02-09:13:10-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-09:13:10-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-09:13:10-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-09:13:11-root-INFO: grad norm: 0.889 0.879 0.130
2024-12-02-09:13:11-root-INFO: grad norm: 0.451 0.449 0.038
2024-12-02-09:13:11-root-INFO: grad norm: 0.379 0.377 0.035
2024-12-02-09:13:12-root-INFO: grad norm: 0.341 0.340 0.033
2024-12-02-09:13:12-root-INFO: grad norm: 0.318 0.317 0.032
2024-12-02-09:13:13-root-INFO: Loss Change: 2.238 -> 1.867
2024-12-02-09:13:13-root-INFO: Regularization Change: 0.000 -> 0.574
2024-12-02-09:13:13-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-09:13:13-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-09:13:13-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-09:13:13-root-INFO: grad norm: 0.806 0.799 0.106
2024-12-02-09:13:13-root-INFO: grad norm: 0.415 0.413 0.040
2024-12-02-09:13:14-root-INFO: grad norm: 0.348 0.345 0.038
2024-12-02-09:13:14-root-INFO: grad norm: 0.313 0.311 0.036
2024-12-02-09:13:15-root-INFO: grad norm: 0.292 0.290 0.036
2024-12-02-09:13:15-root-INFO: Loss Change: 2.004 -> 1.674
2024-12-02-09:13:15-root-INFO: Regularization Change: 0.000 -> 0.526
2024-12-02-09:13:15-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-09:13:15-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-09:13:15-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-09:13:16-root-INFO: grad norm: 0.769 0.763 0.093
2024-12-02-09:13:16-root-INFO: grad norm: 0.411 0.409 0.041
2024-12-02-09:13:17-root-INFO: grad norm: 0.354 0.352 0.040
2024-12-02-09:13:17-root-INFO: grad norm: 0.307 0.304 0.039
2024-12-02-09:13:17-root-INFO: grad norm: 0.278 0.275 0.037
2024-12-02-09:13:18-root-INFO: Loss Change: 1.808 -> 1.498
2024-12-02-09:13:18-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-09:13:18-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-09:13:18-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-09:13:18-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-09:13:18-root-INFO: grad norm: 0.745 0.741 0.081
2024-12-02-09:13:19-root-INFO: grad norm: 0.437 0.436 0.037
2024-12-02-09:13:19-root-INFO: grad norm: 0.410 0.409 0.034
2024-12-02-09:13:19-root-INFO: grad norm: 0.506 0.505 0.032
2024-12-02-09:13:20-root-INFO: grad norm: 0.294 0.292 0.034
2024-12-02-09:13:20-root-INFO: Loss Change: 1.617 -> 1.290
2024-12-02-09:13:20-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-02-09:13:20-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-09:13:20-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-09:13:20-root-INFO: loss_sample0_0: 1.289588451385498
2024-12-02-09:13:21-root-INFO: It takes 689.904 seconds for image sample0
2024-12-02-09:13:21-root-INFO: lpips_score_sample0: 0.141
2024-12-02-09:13:21-root-INFO: psnr_score_sample0: 17.623
2024-12-02-09:13:21-root-INFO: ssim_score_sample0: 0.728
2024-12-02-09:13:21-root-INFO: mean_lpips: 0.1412958800792694
2024-12-02-09:13:21-root-INFO: best_mean_lpips: 0.1412958800792694
2024-12-02-09:13:21-root-INFO: mean_psnr: 17.6231632232666
2024-12-02-09:13:21-root-INFO: best_mean_psnr: 17.6231632232666
2024-12-02-09:13:21-root-INFO: mean_ssim: 0.7281595468521118
2024-12-02-09:13:21-root-INFO: best_mean_ssim: 0.7281595468521118
2024-12-02-09:13:21-root-INFO: final_loss: 1.289588451385498
2024-12-02-09:13:21-root-INFO: mean time: 689.9041225910187
2024-12-02-09:13:21-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample1_iter5_lr0.02_10009 
 
Enjoy.
