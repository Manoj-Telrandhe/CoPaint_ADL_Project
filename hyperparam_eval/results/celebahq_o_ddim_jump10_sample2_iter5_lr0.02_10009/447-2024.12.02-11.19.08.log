2024-12-02-11:19:11-root-INFO: Prepare model...
2024-12-02-11:19:27-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-11:19:52-root-INFO: Start sampling
2024-12-02-11:19:56-root-INFO: step: 249 lr_xt 0.00012706
2024-12-02-11:19:57-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-11:19:57-root-INFO: grad norm: 23715.492 17717.723 15764.099
2024-12-02-11:19:58-root-INFO: grad norm: 26797.270 21198.893 16392.090
2024-12-02-11:19:58-root-INFO: Loss too large (43816.820->60956.715)! Learning rate decreased to 0.00010.
2024-12-02-11:19:58-root-INFO: Loss too large (43816.820->45485.473)! Learning rate decreased to 0.00008.
2024-12-02-11:19:58-root-INFO: grad norm: 21167.102 16202.270 13621.038
2024-12-02-11:19:59-root-INFO: grad norm: 19148.334 15682.931 10986.558
2024-12-02-11:19:59-root-INFO: Loss Change: 77070.016 -> 26880.844
2024-12-02-11:19:59-root-INFO: Regularization Change: 0.000 -> 12.887
2024-12-02-11:19:59-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-02-11:19:59-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-11:19:59-root-INFO: step: 248 lr_xt 0.00013388
2024-12-02-11:20:00-root-INFO: grad norm: 16837.379 13089.800 10590.302
2024-12-02-11:20:00-root-INFO: Loss too large (26925.918->41989.695)! Learning rate decreased to 0.00011.
2024-12-02-11:20:00-root-INFO: Loss too large (26925.918->30482.793)! Learning rate decreased to 0.00009.
2024-12-02-11:20:00-root-INFO: grad norm: 14965.713 12114.601 8786.866
2024-12-02-11:20:01-root-INFO: grad norm: 13333.594 10544.032 8161.379
2024-12-02-11:20:01-root-INFO: grad norm: 11798.202 9479.475 7024.041
2024-12-02-11:20:02-root-INFO: grad norm: 10415.227 8321.434 6263.440
2024-12-02-11:20:02-root-INFO: Loss Change: 26925.918 -> 19161.246
2024-12-02-11:20:02-root-INFO: Regularization Change: 0.000 -> 2.795
2024-12-02-11:20:02-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-02-11:20:02-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-11:20:02-root-INFO: step: 247 lr_xt 0.00014104
2024-12-02-11:20:02-root-INFO: grad norm: 8805.401 7052.377 5272.482
2024-12-02-11:20:03-root-INFO: Loss too large (18954.025->23039.109)! Learning rate decreased to 0.00011.
2024-12-02-11:20:03-root-INFO: Loss too large (18954.025->19965.514)! Learning rate decreased to 0.00009.
2024-12-02-11:20:03-root-INFO: grad norm: 7323.441 5943.706 4278.452
2024-12-02-11:20:04-root-INFO: grad norm: 5996.060 4787.027 3610.692
2024-12-02-11:20:04-root-INFO: grad norm: 4944.297 4020.958 2877.145
2024-12-02-11:20:04-root-INFO: grad norm: 4022.907 3207.842 2427.660
2024-12-02-11:20:05-root-INFO: Loss Change: 18954.025 -> 16716.361
2024-12-02-11:20:05-root-INFO: Regularization Change: 0.000 -> 0.649
2024-12-02-11:20:05-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-02-11:20:05-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-11:20:05-root-INFO: step: 246 lr_xt 0.00014856
2024-12-02-11:20:05-root-INFO: grad norm: 3163.303 2611.636 1784.892
2024-12-02-11:20:05-root-INFO: Loss too large (16469.203->16841.498)! Learning rate decreased to 0.00012.
2024-12-02-11:20:06-root-INFO: grad norm: 3733.778 2999.306 2223.794
2024-12-02-11:20:06-root-INFO: Loss too large (16467.020->16537.396)! Learning rate decreased to 0.00010.
2024-12-02-11:20:06-root-INFO: grad norm: 2894.170 2370.297 1660.697
2024-12-02-11:20:07-root-INFO: grad norm: 2242.422 1817.594 1313.319
2024-12-02-11:20:07-root-INFO: grad norm: 1776.052 1456.520 1016.322
2024-12-02-11:20:08-root-INFO: Loss Change: 16469.203 -> 15881.572
2024-12-02-11:20:08-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-11:20:08-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-02-11:20:08-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-11:20:08-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-11:20:08-root-INFO: grad norm: 1396.903 1149.195 794.158
2024-12-02-11:20:08-root-INFO: grad norm: 2001.491 1614.936 1182.349
2024-12-02-11:20:09-root-INFO: Loss too large (15714.633->15791.833)! Learning rate decreased to 0.00013.
2024-12-02-11:20:09-root-INFO: grad norm: 2205.917 1820.782 1245.321
2024-12-02-11:20:09-root-INFO: grad norm: 2438.019 1958.882 1451.454
2024-12-02-11:20:10-root-INFO: grad norm: 2715.442 2241.139 1533.272
2024-12-02-11:20:10-root-INFO: Loss Change: 15753.146 -> 15578.105
2024-12-02-11:20:10-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-11:20:10-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-11:20:10-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-11:20:10-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-11:20:11-root-INFO: grad norm: 3028.005 2437.948 1795.891
2024-12-02-11:20:11-root-INFO: Loss too large (15495.425->15710.939)! Learning rate decreased to 0.00013.
2024-12-02-11:20:11-root-INFO: grad norm: 3080.222 2534.698 1750.164
2024-12-02-11:20:12-root-INFO: grad norm: 3224.404 2601.508 1904.976
2024-12-02-11:20:12-root-INFO: grad norm: 3423.318 2838.227 1914.047
2024-12-02-11:20:13-root-INFO: grad norm: 3643.010 2939.894 2151.405
2024-12-02-11:20:13-root-INFO: Loss Change: 15495.425 -> 15239.178
2024-12-02-11:20:13-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-02-11:20:13-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-11:20:13-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:13-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-11:20:13-root-INFO: grad norm: 3730.115 3093.979 2083.521
2024-12-02-11:20:13-root-INFO: Loss too large (15090.797->15567.276)! Learning rate decreased to 0.00014.
2024-12-02-11:20:14-root-INFO: grad norm: 3749.989 3021.627 2220.852
2024-12-02-11:20:14-root-INFO: grad norm: 3819.945 3189.350 2102.386
2024-12-02-11:20:15-root-INFO: grad norm: 3910.731 3161.506 2301.889
2024-12-02-11:20:15-root-INFO: grad norm: 4023.441 3358.513 2215.507
2024-12-02-11:20:16-root-INFO: Loss Change: 15090.797 -> 14810.504
2024-12-02-11:20:16-root-INFO: Regularization Change: 0.000 -> 0.562
2024-12-02-11:20:16-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-11:20:16-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:16-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-11:20:16-root-INFO: grad norm: 3848.168 3157.499 2199.682
2024-12-02-11:20:16-root-INFO: Loss too large (14551.404->15040.153)! Learning rate decreased to 0.00015.
2024-12-02-11:20:16-root-INFO: grad norm: 3788.111 3177.207 2062.799
2024-12-02-11:20:17-root-INFO: grad norm: 3761.192 3070.218 2172.632
2024-12-02-11:20:17-root-INFO: grad norm: 3743.822 3147.301 2027.485
2024-12-02-11:20:18-root-INFO: grad norm: 3737.113 3049.407 2160.353
2024-12-02-11:20:18-root-INFO: Loss Change: 14551.404 -> 14081.816
2024-12-02-11:20:18-root-INFO: Regularization Change: 0.000 -> 0.597
2024-12-02-11:20:18-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-11:20:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:18-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-11:20:18-root-INFO: grad norm: 3714.194 3125.323 2006.886
2024-12-02-11:20:19-root-INFO: Loss too large (14018.064->14495.433)! Learning rate decreased to 0.00015.
2024-12-02-11:20:19-root-INFO: grad norm: 3580.491 2948.706 2031.021
2024-12-02-11:20:19-root-INFO: grad norm: 3457.227 2920.132 1850.742
2024-12-02-11:20:20-root-INFO: grad norm: 3350.672 2756.128 1905.457
2024-12-02-11:20:20-root-INFO: grad norm: 3244.811 2744.655 1730.800
2024-12-02-11:20:21-root-INFO: Loss Change: 14018.064 -> 13458.177
2024-12-02-11:20:21-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-02-11:20:21-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-11:20:21-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:21-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-11:20:21-root-INFO: grad norm: 2989.155 2492.229 1650.407
2024-12-02-11:20:21-root-INFO: Loss too large (13258.854->13466.983)! Learning rate decreased to 0.00016.
2024-12-02-11:20:22-root-INFO: grad norm: 2773.284 2367.420 1444.448
2024-12-02-11:20:22-root-INFO: grad norm: 2604.510 2157.755 1458.618
2024-12-02-11:20:23-root-INFO: grad norm: 2453.140 2104.865 1259.937
2024-12-02-11:20:23-root-INFO: grad norm: 2320.950 1923.671 1298.576
2024-12-02-11:20:23-root-INFO: Loss Change: 13258.854 -> 12626.732
2024-12-02-11:20:23-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-11:20:23-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-11:20:23-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:24-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-11:20:24-root-INFO: grad norm: 2090.923 1800.922 1062.374
2024-12-02-11:20:24-root-INFO: Loss too large (12550.254->12593.248)! Learning rate decreased to 0.00017.
2024-12-02-11:20:24-root-INFO: grad norm: 1927.039 1605.964 1065.062
2024-12-02-11:20:25-root-INFO: grad norm: 1779.629 1540.213 891.529
2024-12-02-11:20:25-root-INFO: grad norm: 1650.540 1379.998 905.478
2024-12-02-11:20:26-root-INFO: grad norm: 1532.448 1332.264 757.277
2024-12-02-11:20:26-root-INFO: Loss Change: 12550.254 -> 11990.085
2024-12-02-11:20:26-root-INFO: Regularization Change: 0.000 -> 0.470
2024-12-02-11:20:26-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-11:20:26-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:26-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-11:20:26-root-INFO: grad norm: 1543.416 1323.521 793.994
2024-12-02-11:20:27-root-INFO: grad norm: 1861.172 1612.351 929.669
2024-12-02-11:20:27-root-INFO: grad norm: 2384.860 2016.387 1273.476
2024-12-02-11:20:27-root-INFO: Loss too large (11678.739->11753.142)! Learning rate decreased to 0.00018.
2024-12-02-11:20:28-root-INFO: grad norm: 2108.135 1831.714 1043.578
2024-12-02-11:20:28-root-INFO: grad norm: 1879.671 1594.443 995.447
2024-12-02-11:20:29-root-INFO: Loss Change: 11781.891 -> 11259.354
2024-12-02-11:20:29-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-02-11:20:29-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-11:20:29-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:29-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-11:20:29-root-INFO: grad norm: 1520.370 1334.811 727.877
2024-12-02-11:20:29-root-INFO: grad norm: 1860.935 1589.322 968.056
2024-12-02-11:20:30-root-INFO: grad norm: 2349.693 2053.083 1142.762
2024-12-02-11:20:30-root-INFO: Loss too large (11078.924->11142.213)! Learning rate decreased to 0.00019.
2024-12-02-11:20:31-root-INFO: grad norm: 2019.123 1731.471 1038.683
2024-12-02-11:20:31-root-INFO: grad norm: 1743.966 1533.400 830.723
2024-12-02-11:20:31-root-INFO: Loss Change: 11163.633 -> 10640.360
2024-12-02-11:20:31-root-INFO: Regularization Change: 0.000 -> 0.621
2024-12-02-11:20:31-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-11:20:31-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:32-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-11:20:32-root-INFO: grad norm: 1446.498 1265.137 701.274
2024-12-02-11:20:32-root-INFO: grad norm: 1731.520 1516.277 836.101
2024-12-02-11:20:33-root-INFO: grad norm: 2102.837 1815.644 1060.829
2024-12-02-11:20:33-root-INFO: Loss too large (10407.924->10417.959)! Learning rate decreased to 0.00020.
2024-12-02-11:20:33-root-INFO: grad norm: 1725.433 1516.214 823.537
2024-12-02-11:20:34-root-INFO: grad norm: 1432.292 1239.398 717.880
2024-12-02-11:20:34-root-INFO: Loss Change: 10513.200 -> 9985.123
2024-12-02-11:20:34-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-11:20:34-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-11:20:34-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:34-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-11:20:34-root-INFO: grad norm: 1011.662 902.506 457.103
2024-12-02-11:20:35-root-INFO: grad norm: 1115.537 974.129 543.594
2024-12-02-11:20:35-root-INFO: grad norm: 1268.193 1134.074 567.617
2024-12-02-11:20:36-root-INFO: grad norm: 1464.477 1274.567 721.229
2024-12-02-11:20:36-root-INFO: grad norm: 1705.390 1513.254 786.395
2024-12-02-11:20:37-root-INFO: Loss Change: 9844.686 -> 9526.326
2024-12-02-11:20:37-root-INFO: Regularization Change: 0.000 -> 0.754
2024-12-02-11:20:37-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-11:20:37-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-11:20:37-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-11:20:37-root-INFO: grad norm: 1907.853 1670.963 920.753
2024-12-02-11:20:37-root-INFO: grad norm: 2149.590 1899.938 1005.470
2024-12-02-11:20:38-root-INFO: grad norm: 2415.161 2107.089 1180.330
2024-12-02-11:20:38-root-INFO: Loss too large (9417.708->9428.826)! Learning rate decreased to 0.00022.
2024-12-02-11:20:39-root-INFO: grad norm: 1741.413 1545.920 801.656
2024-12-02-11:20:39-root-INFO: grad norm: 1283.616 1121.964 623.593
2024-12-02-11:20:39-root-INFO: Loss Change: 9452.777 -> 8909.189
2024-12-02-11:20:39-root-INFO: Regularization Change: 0.000 -> 0.572
2024-12-02-11:20:39-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-11:20:39-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:20:40-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-11:20:40-root-INFO: grad norm: 920.493 806.496 443.702
2024-12-02-11:20:40-root-INFO: grad norm: 924.405 814.643 436.899
2024-12-02-11:20:41-root-INFO: grad norm: 958.888 864.993 413.827
2024-12-02-11:20:41-root-INFO: grad norm: 1001.531 882.611 473.352
2024-12-02-11:20:42-root-INFO: grad norm: 1048.416 945.089 453.854
2024-12-02-11:20:42-root-INFO: Loss Change: 8848.552 -> 8495.875
2024-12-02-11:20:42-root-INFO: Regularization Change: 0.000 -> 0.607
2024-12-02-11:20:42-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-11:20:42-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:20:42-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-11:20:42-root-INFO: grad norm: 1381.158 1206.528 672.226
2024-12-02-11:20:43-root-INFO: grad norm: 1363.565 1230.314 587.908
2024-12-02-11:20:43-root-INFO: grad norm: 1374.962 1213.026 647.370
2024-12-02-11:20:44-root-INFO: grad norm: 1389.871 1254.688 597.913
2024-12-02-11:20:44-root-INFO: grad norm: 1403.024 1241.111 654.309
2024-12-02-11:20:44-root-INFO: Loss Change: 8380.812 -> 8054.002
2024-12-02-11:20:44-root-INFO: Regularization Change: 0.000 -> 0.661
2024-12-02-11:20:44-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-11:20:44-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:20:44-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-11:20:45-root-INFO: grad norm: 1251.881 1134.488 529.286
2024-12-02-11:20:45-root-INFO: grad norm: 1227.219 1088.152 567.442
2024-12-02-11:20:45-root-INFO: grad norm: 1198.546 1085.603 507.916
2024-12-02-11:20:46-root-INFO: grad norm: 1168.104 1035.828 539.931
2024-12-02-11:20:46-root-INFO: grad norm: 1135.104 1029.212 478.730
2024-12-02-11:20:47-root-INFO: Loss Change: 7957.556 -> 7642.483
2024-12-02-11:20:47-root-INFO: Regularization Change: 0.000 -> 0.589
2024-12-02-11:20:47-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-11:20:47-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:20:47-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-11:20:47-root-INFO: grad norm: 1237.004 1096.012 573.529
2024-12-02-11:20:47-root-INFO: grad norm: 1151.203 1048.492 475.324
2024-12-02-11:20:48-root-INFO: grad norm: 1075.719 957.157 490.940
2024-12-02-11:20:48-root-INFO: grad norm: 1004.670 916.122 412.412
2024-12-02-11:20:49-root-INFO: grad norm: 939.955 838.667 424.445
2024-12-02-11:20:49-root-INFO: Loss Change: 7578.854 -> 7255.439
2024-12-02-11:20:49-root-INFO: Regularization Change: 0.000 -> 0.551
2024-12-02-11:20:49-root-INFO: Undo step: 230
2024-12-02-11:20:49-root-INFO: Undo step: 231
2024-12-02-11:20:49-root-INFO: Undo step: 232
2024-12-02-11:20:49-root-INFO: Undo step: 233
2024-12-02-11:20:49-root-INFO: Undo step: 234
2024-12-02-11:20:49-root-INFO: Undo step: 235
2024-12-02-11:20:49-root-INFO: Undo step: 236
2024-12-02-11:20:49-root-INFO: Undo step: 237
2024-12-02-11:20:49-root-INFO: Undo step: 238
2024-12-02-11:20:49-root-INFO: Undo step: 239
2024-12-02-11:20:49-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-11:20:50-root-INFO: grad norm: 20600.414 17530.824 10818.837
2024-12-02-11:20:50-root-INFO: grad norm: 15909.819 13009.839 9157.862
2024-12-02-11:20:50-root-INFO: grad norm: 9234.357 7365.678 5569.574
2024-12-02-11:20:51-root-INFO: grad norm: 7346.909 6154.242 4012.777
2024-12-02-11:20:51-root-INFO: grad norm: 6488.136 5300.226 3742.127
2024-12-02-11:20:52-root-INFO: Loss too large (12454.226->12535.569)! Learning rate decreased to 0.00016.
2024-12-02-11:20:52-root-INFO: Loss Change: 31686.875 -> 11185.799
2024-12-02-11:20:52-root-INFO: Regularization Change: 0.000 -> 7.830
2024-12-02-11:20:52-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-11:20:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:52-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-11:20:52-root-INFO: grad norm: 4352.753 3656.280 2361.794
2024-12-02-11:20:53-root-INFO: grad norm: 4969.916 4157.909 2722.473
2024-12-02-11:20:53-root-INFO: Loss too large (11004.504->11468.857)! Learning rate decreased to 0.00017.
2024-12-02-11:20:53-root-INFO: grad norm: 3868.870 3214.403 2153.084
2024-12-02-11:20:54-root-INFO: grad norm: 3024.530 2558.626 1612.829
2024-12-02-11:20:54-root-INFO: grad norm: 2467.177 2043.136 1382.954
2024-12-02-11:20:55-root-INFO: Loss Change: 11019.846 -> 9722.650
2024-12-02-11:20:55-root-INFO: Regularization Change: 0.000 -> 0.794
2024-12-02-11:20:55-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-11:20:55-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:55-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-11:20:55-root-INFO: grad norm: 1840.899 1535.692 1015.166
2024-12-02-11:20:56-root-INFO: grad norm: 2042.375 1700.136 1131.740
2024-12-02-11:20:56-root-INFO: Loss too large (9438.713->9447.981)! Learning rate decreased to 0.00018.
2024-12-02-11:20:56-root-INFO: grad norm: 1601.408 1391.097 793.320
2024-12-02-11:20:57-root-INFO: grad norm: 1294.931 1085.189 706.548
2024-12-02-11:20:57-root-INFO: grad norm: 1061.787 941.601 490.692
2024-12-02-11:20:58-root-INFO: Loss Change: 9503.195 -> 9049.664
2024-12-02-11:20:58-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-11:20:58-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-11:20:58-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:20:58-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-11:20:58-root-INFO: grad norm: 1098.091 930.880 582.464
2024-12-02-11:20:58-root-INFO: grad norm: 1228.589 1080.619 584.546
2024-12-02-11:20:59-root-INFO: grad norm: 1410.564 1194.401 750.397
2024-12-02-11:20:59-root-INFO: grad norm: 1640.391 1428.983 805.538
2024-12-02-11:21:00-root-INFO: Loss too large (8886.072->8888.704)! Learning rate decreased to 0.00019.
2024-12-02-11:21:00-root-INFO: grad norm: 1267.140 1080.148 662.514
2024-12-02-11:21:00-root-INFO: Loss Change: 8983.466 -> 8712.104
2024-12-02-11:21:00-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-11:21:00-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-11:21:00-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:21:00-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-11:21:01-root-INFO: grad norm: 970.450 845.399 476.523
2024-12-02-11:21:01-root-INFO: grad norm: 1071.686 924.214 542.529
2024-12-02-11:21:02-root-INFO: grad norm: 1204.188 1054.058 582.264
2024-12-02-11:21:02-root-INFO: grad norm: 1355.090 1164.797 692.472
2024-12-02-11:21:03-root-INFO: grad norm: 1532.984 1333.800 755.657
2024-12-02-11:21:03-root-INFO: Loss Change: 8570.064 -> 8442.663
2024-12-02-11:21:03-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-02-11:21:03-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-11:21:03-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-11:21:03-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-11:21:03-root-INFO: grad norm: 1946.209 1687.975 968.748
2024-12-02-11:21:04-root-INFO: grad norm: 2104.806 1838.889 1024.059
2024-12-02-11:21:04-root-INFO: Loss too large (8392.711->8404.895)! Learning rate decreased to 0.00021.
2024-12-02-11:21:05-root-INFO: grad norm: 1460.285 1265.422 728.793
2024-12-02-11:21:05-root-INFO: grad norm: 1025.785 911.363 470.801
2024-12-02-11:21:05-root-INFO: grad norm: 756.998 666.772 358.414
2024-12-02-11:21:06-root-INFO: Loss Change: 8408.536 -> 8002.172
2024-12-02-11:21:06-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-11:21:06-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-11:21:06-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-11:21:06-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-11:21:06-root-INFO: grad norm: 561.057 507.758 238.679
2024-12-02-11:21:07-root-INFO: grad norm: 560.453 504.844 243.394
2024-12-02-11:21:07-root-INFO: grad norm: 572.254 523.800 230.454
2024-12-02-11:21:08-root-INFO: grad norm: 587.017 527.216 258.132
2024-12-02-11:21:08-root-INFO: grad norm: 603.979 550.445 248.598
2024-12-02-11:21:09-root-INFO: Loss Change: 7949.197 -> 7743.959
2024-12-02-11:21:09-root-INFO: Regularization Change: 0.000 -> 0.303
2024-12-02-11:21:09-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-11:21:09-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:09-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-11:21:09-root-INFO: grad norm: 777.458 687.931 362.204
2024-12-02-11:21:09-root-INFO: grad norm: 721.292 646.245 320.357
2024-12-02-11:21:10-root-INFO: grad norm: 712.768 634.639 324.454
2024-12-02-11:21:11-root-INFO: grad norm: 712.428 641.777 309.315
2024-12-02-11:21:11-root-INFO: grad norm: 714.499 637.153 323.334
2024-12-02-11:21:11-root-INFO: Loss Change: 7745.428 -> 7521.857
2024-12-02-11:21:11-root-INFO: Regularization Change: 0.000 -> 0.345
2024-12-02-11:21:11-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-11:21:11-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:11-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-11:21:12-root-INFO: grad norm: 530.418 459.093 265.663
2024-12-02-11:21:12-root-INFO: grad norm: 419.556 379.628 178.634
2024-12-02-11:21:13-root-INFO: grad norm: 395.771 371.433 136.647
2024-12-02-11:21:13-root-INFO: grad norm: 383.306 356.261 141.427
2024-12-02-11:21:14-root-INFO: grad norm: 374.951 354.543 122.012
2024-12-02-11:21:14-root-INFO: Loss Change: 7347.430 -> 7138.287
2024-12-02-11:21:14-root-INFO: Regularization Change: 0.000 -> 0.306
2024-12-02-11:21:14-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-11:21:14-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:14-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-11:21:14-root-INFO: grad norm: 428.267 396.186 162.631
2024-12-02-11:21:15-root-INFO: grad norm: 412.209 386.993 141.959
2024-12-02-11:21:15-root-INFO: grad norm: 402.594 373.488 150.297
2024-12-02-11:21:16-root-INFO: grad norm: 393.730 370.179 134.132
2024-12-02-11:21:16-root-INFO: grad norm: 385.686 358.450 142.363
2024-12-02-11:21:17-root-INFO: Loss Change: 7082.558 -> 6911.409
2024-12-02-11:21:17-root-INFO: Regularization Change: 0.000 -> 0.275
2024-12-02-11:21:17-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-11:21:17-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:17-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-11:21:17-root-INFO: grad norm: 331.806 312.429 111.728
2024-12-02-11:21:18-root-INFO: grad norm: 314.101 298.685 97.194
2024-12-02-11:21:18-root-INFO: grad norm: 308.781 293.134 97.048
2024-12-02-11:21:19-root-INFO: grad norm: 304.521 290.068 92.699
2024-12-02-11:21:19-root-INFO: grad norm: 300.704 285.747 93.655
2024-12-02-11:21:19-root-INFO: Loss Change: 6812.522 -> 6655.150
2024-12-02-11:21:19-root-INFO: Regularization Change: 0.000 -> 0.263
2024-12-02-11:21:19-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-11:21:19-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:19-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-11:21:20-root-INFO: grad norm: 430.248 394.054 172.729
2024-12-02-11:21:20-root-INFO: grad norm: 383.993 357.526 140.091
2024-12-02-11:21:21-root-INFO: grad norm: 367.940 338.690 143.769
2024-12-02-11:21:21-root-INFO: grad norm: 354.392 330.887 126.916
2024-12-02-11:21:22-root-INFO: grad norm: 342.635 317.498 128.817
2024-12-02-11:21:22-root-INFO: Loss Change: 6605.613 -> 6443.096
2024-12-02-11:21:22-root-INFO: Regularization Change: 0.000 -> 0.283
2024-12-02-11:21:22-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-11:21:22-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:22-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-11:21:22-root-INFO: grad norm: 334.194 296.430 154.322
2024-12-02-11:21:23-root-INFO: grad norm: 287.824 268.852 102.769
2024-12-02-11:21:23-root-INFO: grad norm: 276.143 260.585 91.379
2024-12-02-11:21:24-root-INFO: grad norm: 271.362 256.734 87.894
2024-12-02-11:21:24-root-INFO: grad norm: 268.165 253.780 86.650
2024-12-02-11:21:25-root-INFO: Loss Change: 6410.956 -> 6268.439
2024-12-02-11:21:25-root-INFO: Regularization Change: 0.000 -> 0.258
2024-12-02-11:21:25-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-11:21:25-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:25-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-11:21:25-root-INFO: grad norm: 532.086 472.582 244.503
2024-12-02-11:21:25-root-INFO: grad norm: 453.220 415.652 180.671
2024-12-02-11:21:26-root-INFO: grad norm: 412.420 375.954 169.556
2024-12-02-11:21:26-root-INFO: grad norm: 382.006 352.689 146.761
2024-12-02-11:21:27-root-INFO: grad norm: 357.765 328.998 140.556
2024-12-02-11:21:27-root-INFO: Loss Change: 6294.948 -> 6121.012
2024-12-02-11:21:27-root-INFO: Regularization Change: 0.000 -> 0.325
2024-12-02-11:21:27-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-11:21:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-11:21:27-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-11:21:27-root-INFO: grad norm: 394.453 349.787 182.325
2024-12-02-11:21:28-root-INFO: grad norm: 325.223 302.390 119.708
2024-12-02-11:21:28-root-INFO: grad norm: 301.298 280.887 109.009
2024-12-02-11:21:29-root-INFO: grad norm: 286.060 266.914 102.896
2024-12-02-11:21:29-root-INFO: grad norm: 274.318 256.962 96.026
2024-12-02-11:21:30-root-INFO: Loss Change: 6054.129 -> 5915.592
2024-12-02-11:21:30-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-11:21:30-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-11:21:30-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:21:30-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-11:21:30-root-INFO: grad norm: 567.086 508.746 250.527
2024-12-02-11:21:30-root-INFO: grad norm: 473.692 435.868 185.483
2024-12-02-11:21:31-root-INFO: grad norm: 423.050 384.048 177.422
2024-12-02-11:21:31-root-INFO: grad norm: 382.202 352.703 147.238
2024-12-02-11:21:32-root-INFO: grad norm: 350.313 320.509 141.399
2024-12-02-11:21:32-root-INFO: Loss Change: 5887.326 -> 5726.912
2024-12-02-11:21:32-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-02-11:21:32-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-11:21:32-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:21:32-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-11:21:33-root-INFO: grad norm: 295.698 263.444 134.293
2024-12-02-11:21:33-root-INFO: grad norm: 254.912 232.407 104.725
2024-12-02-11:21:34-root-INFO: grad norm: 243.209 223.814 95.173
2024-12-02-11:21:34-root-INFO: grad norm: 235.718 217.386 91.140
2024-12-02-11:21:34-root-INFO: grad norm: 230.568 213.543 86.953
2024-12-02-11:21:35-root-INFO: Loss Change: 5689.985 -> 5553.841
2024-12-02-11:21:35-root-INFO: Regularization Change: 0.000 -> 0.301
2024-12-02-11:21:35-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-11:21:35-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:21:35-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-11:21:35-root-INFO: grad norm: 373.859 335.502 164.952
2024-12-02-11:21:36-root-INFO: grad norm: 327.084 300.206 129.847
2024-12-02-11:21:36-root-INFO: grad norm: 300.517 273.590 124.334
2024-12-02-11:21:37-root-INFO: grad norm: 280.355 258.443 108.655
2024-12-02-11:21:37-root-INFO: grad norm: 264.933 242.990 105.572
2024-12-02-11:21:37-root-INFO: Loss Change: 5538.023 -> 5400.084
2024-12-02-11:21:37-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-11:21:37-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-11:21:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:21:38-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-11:21:38-root-INFO: grad norm: 232.655 213.922 91.465
2024-12-02-11:21:38-root-INFO: grad norm: 223.615 207.106 84.327
2024-12-02-11:21:39-root-INFO: grad norm: 219.382 203.564 81.793
2024-12-02-11:21:39-root-INFO: grad norm: 216.311 200.593 80.950
2024-12-02-11:21:40-root-INFO: grad norm: 213.735 198.529 79.176
2024-12-02-11:21:40-root-INFO: Loss Change: 5337.078 -> 5222.174
2024-12-02-11:21:40-root-INFO: Regularization Change: 0.000 -> 0.283
2024-12-02-11:21:40-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-11:21:40-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:21:40-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-11:21:40-root-INFO: grad norm: 268.888 244.419 112.073
2024-12-02-11:21:41-root-INFO: grad norm: 240.407 222.783 90.349
2024-12-02-11:21:41-root-INFO: grad norm: 228.425 210.647 88.351
2024-12-02-11:21:42-root-INFO: grad norm: 219.871 204.097 81.779
2024-12-02-11:21:42-root-INFO: grad norm: 213.564 197.359 81.602
2024-12-02-11:21:43-root-INFO: Loss Change: 5189.134 -> 5075.074
2024-12-02-11:21:43-root-INFO: Regularization Change: 0.000 -> 0.290
2024-12-02-11:21:43-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-11:21:43-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:21:43-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-11:21:43-root-INFO: grad norm: 352.859 312.481 163.904
2024-12-02-11:21:43-root-INFO: grad norm: 295.227 271.924 114.962
2024-12-02-11:21:44-root-INFO: grad norm: 265.754 240.543 112.978
2024-12-02-11:21:44-root-INFO: grad norm: 245.258 226.019 95.220
2024-12-02-11:21:45-root-INFO: grad norm: 230.676 210.618 94.082
2024-12-02-11:21:45-root-INFO: Loss Change: 5058.950 -> 4921.350
2024-12-02-11:21:45-root-INFO: Regularization Change: 0.000 -> 0.359
2024-12-02-11:21:45-root-INFO: Undo step: 220
2024-12-02-11:21:45-root-INFO: Undo step: 221
2024-12-02-11:21:45-root-INFO: Undo step: 222
2024-12-02-11:21:45-root-INFO: Undo step: 223
2024-12-02-11:21:45-root-INFO: Undo step: 224
2024-12-02-11:21:45-root-INFO: Undo step: 225
2024-12-02-11:21:45-root-INFO: Undo step: 226
2024-12-02-11:21:45-root-INFO: Undo step: 227
2024-12-02-11:21:45-root-INFO: Undo step: 228
2024-12-02-11:21:45-root-INFO: Undo step: 229
2024-12-02-11:21:45-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-11:21:45-root-INFO: grad norm: 5895.953 4398.581 3926.162
2024-12-02-11:21:46-root-INFO: grad norm: 3353.354 2554.437 2172.517
2024-12-02-11:21:46-root-INFO: grad norm: 2445.725 1981.107 1434.149
2024-12-02-11:21:47-root-INFO: grad norm: 1958.838 1605.266 1122.572
2024-12-02-11:21:47-root-INFO: grad norm: 1702.679 1455.011 884.341
2024-12-02-11:21:48-root-INFO: Loss Change: 14085.730 -> 7872.781
2024-12-02-11:21:48-root-INFO: Regularization Change: 0.000 -> 6.390
2024-12-02-11:21:48-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-11:21:48-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:48-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-11:21:48-root-INFO: grad norm: 1485.266 1269.906 770.293
2024-12-02-11:21:48-root-INFO: grad norm: 1303.267 1141.163 629.484
2024-12-02-11:21:49-root-INFO: grad norm: 1186.071 1047.668 556.016
2024-12-02-11:21:49-root-INFO: grad norm: 1088.096 963.984 504.666
2024-12-02-11:21:50-root-INFO: grad norm: 1000.908 893.208 451.660
2024-12-02-11:21:50-root-INFO: Loss Change: 7661.417 -> 6889.335
2024-12-02-11:21:50-root-INFO: Regularization Change: 0.000 -> 1.334
2024-12-02-11:21:50-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-11:21:50-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:50-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-11:21:50-root-INFO: grad norm: 975.816 871.380 439.219
2024-12-02-11:21:51-root-INFO: grad norm: 861.735 774.418 377.973
2024-12-02-11:21:51-root-INFO: grad norm: 771.968 694.026 338.027
2024-12-02-11:21:52-root-INFO: grad norm: 695.667 629.849 295.371
2024-12-02-11:21:52-root-INFO: grad norm: 630.987 571.336 267.804
2024-12-02-11:21:53-root-INFO: Loss Change: 6834.162 -> 6430.601
2024-12-02-11:21:53-root-INFO: Regularization Change: 0.000 -> 0.718
2024-12-02-11:21:53-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-11:21:53-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-11:21:53-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-11:21:53-root-INFO: grad norm: 462.334 411.141 211.462
2024-12-02-11:21:53-root-INFO: grad norm: 368.837 333.628 157.267
2024-12-02-11:21:54-root-INFO: grad norm: 346.855 319.462 135.102
2024-12-02-11:21:54-root-INFO: grad norm: 333.929 308.480 127.862
2024-12-02-11:21:55-root-INFO: grad norm: 324.261 299.948 123.193
2024-12-02-11:21:55-root-INFO: Loss Change: 6345.721 -> 6114.429
2024-12-02-11:21:55-root-INFO: Regularization Change: 0.000 -> 0.435
2024-12-02-11:21:55-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-11:21:55-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-11:21:55-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-11:21:56-root-INFO: grad norm: 380.004 330.051 188.333
2024-12-02-11:21:56-root-INFO: grad norm: 319.622 294.271 124.751
2024-12-02-11:21:56-root-INFO: grad norm: 303.330 280.587 115.238
2024-12-02-11:21:57-root-INFO: grad norm: 294.668 272.925 111.093
2024-12-02-11:21:57-root-INFO: grad norm: 287.714 266.930 107.367
2024-12-02-11:21:58-root-INFO: Loss Change: 6047.697 -> 5857.102
2024-12-02-11:21:58-root-INFO: Regularization Change: 0.000 -> 0.381
2024-12-02-11:21:58-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-11:21:58-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:21:58-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-11:21:58-root-INFO: grad norm: 510.603 460.991 219.553
2024-12-02-11:21:58-root-INFO: grad norm: 396.839 365.336 154.954
2024-12-02-11:21:59-root-INFO: grad norm: 355.173 325.999 140.972
2024-12-02-11:21:59-root-INFO: grad norm: 325.415 301.234 123.096
2024-12-02-11:22:00-root-INFO: grad norm: 303.622 281.210 114.485
2024-12-02-11:22:00-root-INFO: Loss Change: 5787.088 -> 5605.852
2024-12-02-11:22:00-root-INFO: Regularization Change: 0.000 -> 0.360
2024-12-02-11:22:00-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-11:22:00-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:22:00-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-11:22:01-root-INFO: grad norm: 321.377 287.969 142.679
2024-12-02-11:22:01-root-INFO: grad norm: 272.049 250.699 105.642
2024-12-02-11:22:02-root-INFO: grad norm: 256.695 237.805 96.651
2024-12-02-11:22:02-root-INFO: grad norm: 246.881 229.643 90.633
2024-12-02-11:22:02-root-INFO: grad norm: 239.795 223.458 86.994
2024-12-02-11:22:03-root-INFO: Loss Change: 5561.532 -> 5412.854
2024-12-02-11:22:03-root-INFO: Regularization Change: 0.000 -> 0.328
2024-12-02-11:22:03-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-11:22:03-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:22:03-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-11:22:03-root-INFO: grad norm: 365.959 332.243 153.428
2024-12-02-11:22:04-root-INFO: grad norm: 308.115 285.442 116.005
2024-12-02-11:22:04-root-INFO: grad norm: 279.206 257.412 108.144
2024-12-02-11:22:05-root-INFO: grad norm: 258.590 240.641 94.660
2024-12-02-11:22:05-root-INFO: grad norm: 243.761 226.634 89.758
2024-12-02-11:22:05-root-INFO: Loss Change: 5383.453 -> 5244.397
2024-12-02-11:22:05-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-11:22:05-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-11:22:05-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:22:06-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-11:22:06-root-INFO: grad norm: 226.457 210.310 83.979
2024-12-02-11:22:06-root-INFO: grad norm: 214.931 202.099 73.153
2024-12-02-11:22:07-root-INFO: grad norm: 210.682 198.262 71.266
2024-12-02-11:22:07-root-INFO: grad norm: 207.482 195.339 69.939
2024-12-02-11:22:08-root-INFO: grad norm: 204.757 192.792 68.967
2024-12-02-11:22:08-root-INFO: Loss Change: 5180.012 -> 5070.270
2024-12-02-11:22:08-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-11:22:08-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-11:22:08-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:22:08-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-11:22:08-root-INFO: grad norm: 252.312 233.205 96.317
2024-12-02-11:22:09-root-INFO: grad norm: 222.561 209.018 76.451
2024-12-02-11:22:09-root-INFO: grad norm: 211.085 198.364 72.170
2024-12-02-11:22:10-root-INFO: grad norm: 203.367 191.620 68.118
2024-12-02-11:22:10-root-INFO: grad norm: 197.793 186.342 66.323
2024-12-02-11:22:10-root-INFO: Loss Change: 5027.742 -> 4923.826
2024-12-02-11:22:10-root-INFO: Regularization Change: 0.000 -> 0.264
2024-12-02-11:22:10-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-11:22:10-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:22:11-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-11:22:11-root-INFO: grad norm: 374.375 335.586 165.947
2024-12-02-11:22:11-root-INFO: grad norm: 297.887 278.337 106.138
2024-12-02-11:22:12-root-INFO: grad norm: 257.522 236.758 101.309
2024-12-02-11:22:12-root-INFO: grad norm: 230.053 215.294 81.075
2024-12-02-11:22:13-root-INFO: grad norm: 211.879 197.291 77.261
2024-12-02-11:22:13-root-INFO: Loss Change: 4905.931 -> 4778.553
2024-12-02-11:22:13-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-11:22:13-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-11:22:13-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:22:13-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-11:22:13-root-INFO: grad norm: 197.979 183.423 74.510
2024-12-02-11:22:14-root-INFO: grad norm: 184.412 174.068 60.894
2024-12-02-11:22:14-root-INFO: grad norm: 179.856 169.531 60.063
2024-12-02-11:22:15-root-INFO: grad norm: 176.597 166.409 59.115
2024-12-02-11:22:15-root-INFO: grad norm: 174.070 163.999 58.350
2024-12-02-11:22:15-root-INFO: Loss Change: 4746.202 -> 4656.162
2024-12-02-11:22:15-root-INFO: Regularization Change: 0.000 -> 0.254
2024-12-02-11:22:15-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-11:22:15-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-11:22:16-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-11:22:16-root-INFO: grad norm: 287.906 261.675 120.066
2024-12-02-11:22:16-root-INFO: grad norm: 238.412 223.114 84.026
2024-12-02-11:22:17-root-INFO: grad norm: 208.408 193.343 77.798
2024-12-02-11:22:17-root-INFO: grad norm: 189.618 177.979 65.411
2024-12-02-11:22:18-root-INFO: grad norm: 178.049 167.013 61.711
2024-12-02-11:22:18-root-INFO: Loss Change: 4629.848 -> 4529.925
2024-12-02-11:22:18-root-INFO: Regularization Change: 0.000 -> 0.283
2024-12-02-11:22:18-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-11:22:18-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:18-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-11:22:18-root-INFO: grad norm: 265.917 231.817 130.278
2024-12-02-11:22:19-root-INFO: grad norm: 206.013 192.356 73.759
2024-12-02-11:22:19-root-INFO: grad norm: 184.625 171.049 69.489
2024-12-02-11:22:20-root-INFO: grad norm: 172.512 161.801 59.840
2024-12-02-11:22:20-root-INFO: grad norm: 165.236 154.981 57.304
2024-12-02-11:22:21-root-INFO: Loss Change: 4520.273 -> 4421.004
2024-12-02-11:22:21-root-INFO: Regularization Change: 0.000 -> 0.295
2024-12-02-11:22:21-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-11:22:21-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:21-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-11:22:21-root-INFO: grad norm: 274.809 243.774 126.862
2024-12-02-11:22:21-root-INFO: grad norm: 213.652 198.320 79.474
2024-12-02-11:22:22-root-INFO: grad norm: 187.359 172.992 71.952
2024-12-02-11:22:22-root-INFO: grad norm: 171.928 160.450 61.767
2024-12-02-11:22:23-root-INFO: grad norm: 162.330 151.728 57.702
2024-12-02-11:22:23-root-INFO: Loss Change: 4400.320 -> 4297.351
2024-12-02-11:22:23-root-INFO: Regularization Change: 0.000 -> 0.320
2024-12-02-11:22:23-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-11:22:23-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:23-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-11:22:23-root-INFO: grad norm: 156.210 146.738 53.570
2024-12-02-11:22:24-root-INFO: grad norm: 150.782 141.825 51.194
2024-12-02-11:22:24-root-INFO: grad norm: 148.306 139.731 49.698
2024-12-02-11:22:25-root-INFO: grad norm: 146.359 137.916 48.991
2024-12-02-11:22:25-root-INFO: grad norm: 144.595 136.345 48.145
2024-12-02-11:22:26-root-INFO: Loss Change: 4276.526 -> 4201.064
2024-12-02-11:22:26-root-INFO: Regularization Change: 0.000 -> 0.259
2024-12-02-11:22:26-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-11:22:26-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:26-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-11:22:26-root-INFO: grad norm: 221.644 198.163 99.285
2024-12-02-11:22:27-root-INFO: grad norm: 175.118 163.355 63.098
2024-12-02-11:22:27-root-INFO: grad norm: 156.601 146.267 55.944
2024-12-02-11:22:28-root-INFO: grad norm: 146.817 138.023 50.049
2024-12-02-11:22:28-root-INFO: grad norm: 141.412 133.453 46.771
2024-12-02-11:22:28-root-INFO: Loss Change: 4176.229 -> 4094.266
2024-12-02-11:22:28-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-02-11:22:28-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-11:22:28-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:29-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-11:22:29-root-INFO: grad norm: 248.822 219.496 117.193
2024-12-02-11:22:29-root-INFO: grad norm: 189.118 175.010 71.674
2024-12-02-11:22:30-root-INFO: grad norm: 162.275 149.254 63.691
2024-12-02-11:22:30-root-INFO: grad norm: 147.540 137.705 52.964
2024-12-02-11:22:31-root-INFO: grad norm: 139.257 130.373 48.942
2024-12-02-11:22:31-root-INFO: Loss Change: 4098.232 -> 4006.724
2024-12-02-11:22:31-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-11:22:31-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-11:22:31-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:31-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-11:22:31-root-INFO: grad norm: 133.881 126.065 45.077
2024-12-02-11:22:32-root-INFO: grad norm: 130.034 123.143 41.769
2024-12-02-11:22:32-root-INFO: grad norm: 127.960 121.278 40.807
2024-12-02-11:22:33-root-INFO: grad norm: 126.314 119.832 39.944
2024-12-02-11:22:33-root-INFO: grad norm: 124.856 118.520 39.268
2024-12-02-11:22:33-root-INFO: Loss Change: 3985.304 -> 3920.667
2024-12-02-11:22:33-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-02-11:22:33-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-11:22:33-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-11:22:33-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-11:22:34-root-INFO: grad norm: 266.763 233.731 128.578
2024-12-02-11:22:34-root-INFO: grad norm: 177.139 164.515 65.673
2024-12-02-11:22:35-root-INFO: grad norm: 145.646 135.631 53.074
2024-12-02-11:22:35-root-INFO: grad norm: 131.172 123.694 43.654
2024-12-02-11:22:36-root-INFO: grad norm: 124.412 117.957 39.551
2024-12-02-11:22:36-root-INFO: Loss Change: 3910.244 -> 3822.298
2024-12-02-11:22:36-root-INFO: Regularization Change: 0.000 -> 0.322
2024-12-02-11:22:36-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-11:22:36-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:22:36-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-11:22:36-root-INFO: grad norm: 136.111 126.642 49.881
2024-12-02-11:22:37-root-INFO: grad norm: 122.982 116.301 39.983
2024-12-02-11:22:37-root-INFO: grad norm: 118.687 112.535 37.716
2024-12-02-11:22:38-root-INFO: grad norm: 116.333 110.463 36.487
2024-12-02-11:22:38-root-INFO: grad norm: 114.753 109.135 35.466
2024-12-02-11:22:38-root-INFO: Loss Change: 3807.807 -> 3746.860
2024-12-02-11:22:38-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-02-11:22:38-root-INFO: Undo step: 210
2024-12-02-11:22:38-root-INFO: Undo step: 211
2024-12-02-11:22:38-root-INFO: Undo step: 212
2024-12-02-11:22:38-root-INFO: Undo step: 213
2024-12-02-11:22:38-root-INFO: Undo step: 214
2024-12-02-11:22:38-root-INFO: Undo step: 215
2024-12-02-11:22:38-root-INFO: Undo step: 216
2024-12-02-11:22:38-root-INFO: Undo step: 217
2024-12-02-11:22:38-root-INFO: Undo step: 218
2024-12-02-11:22:38-root-INFO: Undo step: 219
2024-12-02-11:22:39-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-11:22:39-root-INFO: grad norm: 15472.269 14279.624 5956.796
2024-12-02-11:22:39-root-INFO: grad norm: 3354.768 2890.631 1702.564
2024-12-02-11:22:40-root-INFO: grad norm: 1795.714 1701.502 574.002
2024-12-02-11:22:40-root-INFO: grad norm: 1284.728 1131.306 608.831
2024-12-02-11:22:41-root-INFO: grad norm: 999.634 962.347 270.472
2024-12-02-11:22:41-root-INFO: Loss Change: 29861.865 -> 9236.829
2024-12-02-11:22:41-root-INFO: Regularization Change: 0.000 -> 60.932
2024-12-02-11:22:41-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-11:22:41-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-11:22:41-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-11:22:41-root-INFO: grad norm: 1070.277 949.810 493.308
2024-12-02-11:22:42-root-INFO: grad norm: 1088.157 1017.060 386.878
2024-12-02-11:22:42-root-INFO: grad norm: 1052.659 925.690 501.187
2024-12-02-11:22:43-root-INFO: grad norm: 1046.080 959.446 416.829
2024-12-02-11:22:43-root-INFO: grad norm: 1328.208 1226.200 510.459
2024-12-02-11:22:43-root-INFO: Loss Change: 8922.252 -> 5709.154
2024-12-02-11:22:43-root-INFO: Regularization Change: 0.000 -> 7.391
2024-12-02-11:22:43-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-11:22:43-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-11:22:44-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-11:22:44-root-INFO: grad norm: 1147.992 1070.609 414.344
2024-12-02-11:22:44-root-INFO: grad norm: 628.752 584.738 231.107
2024-12-02-11:22:45-root-INFO: grad norm: 490.604 462.134 164.695
2024-12-02-11:22:45-root-INFO: grad norm: 427.304 385.691 183.934
2024-12-02-11:22:46-root-INFO: grad norm: 381.747 355.876 138.142
2024-12-02-11:22:46-root-INFO: Loss Change: 5709.677 -> 4765.214
2024-12-02-11:22:46-root-INFO: Regularization Change: 0.000 -> 2.297
2024-12-02-11:22:46-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-11:22:46-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:46-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-11:22:46-root-INFO: grad norm: 367.226 339.484 140.020
2024-12-02-11:22:47-root-INFO: grad norm: 305.664 280.819 120.711
2024-12-02-11:22:47-root-INFO: grad norm: 282.226 262.448 103.793
2024-12-02-11:22:48-root-INFO: grad norm: 265.122 244.367 102.833
2024-12-02-11:22:48-root-INFO: grad norm: 250.803 233.026 92.743
2024-12-02-11:22:49-root-INFO: Loss Change: 4720.120 -> 4463.902
2024-12-02-11:22:49-root-INFO: Regularization Change: 0.000 -> 0.798
2024-12-02-11:22:49-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-11:22:49-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:49-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-11:22:49-root-INFO: grad norm: 367.658 343.857 130.136
2024-12-02-11:22:49-root-INFO: grad norm: 288.444 266.344 110.728
2024-12-02-11:22:50-root-INFO: grad norm: 252.838 240.940 76.650
2024-12-02-11:22:50-root-INFO: grad norm: 228.847 212.549 84.817
2024-12-02-11:22:51-root-INFO: grad norm: 211.865 201.717 64.784
2024-12-02-11:22:51-root-INFO: Loss Change: 4423.537 -> 4247.993
2024-12-02-11:22:51-root-INFO: Regularization Change: 0.000 -> 0.549
2024-12-02-11:22:51-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-11:22:51-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:51-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-11:22:51-root-INFO: grad norm: 197.000 184.283 69.632
2024-12-02-11:22:52-root-INFO: grad norm: 187.495 176.178 64.152
2024-12-02-11:22:53-root-INFO: grad norm: 181.102 170.451 61.192
2024-12-02-11:22:53-root-INFO: grad norm: 175.635 165.762 58.059
2024-12-02-11:22:54-root-INFO: grad norm: 170.800 161.282 56.223
2024-12-02-11:22:54-root-INFO: Loss Change: 4232.696 -> 4120.623
2024-12-02-11:22:54-root-INFO: Regularization Change: 0.000 -> 0.388
2024-12-02-11:22:54-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-11:22:54-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:54-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-11:22:54-root-INFO: grad norm: 249.959 230.331 97.095
2024-12-02-11:22:55-root-INFO: grad norm: 195.159 183.186 67.304
2024-12-02-11:22:55-root-INFO: grad norm: 175.474 167.726 51.568
2024-12-02-11:22:56-root-INFO: grad norm: 163.379 154.258 53.825
2024-12-02-11:22:56-root-INFO: grad norm: 155.599 148.921 45.096
2024-12-02-11:22:56-root-INFO: Loss Change: 4083.441 -> 3983.712
2024-12-02-11:22:56-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-02-11:22:56-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-11:22:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:57-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-11:22:57-root-INFO: grad norm: 268.534 240.902 118.646
2024-12-02-11:22:57-root-INFO: grad norm: 192.383 180.592 66.316
2024-12-02-11:22:58-root-INFO: grad norm: 167.829 159.205 53.108
2024-12-02-11:22:58-root-INFO: grad norm: 153.637 145.261 50.035
2024-12-02-11:22:59-root-INFO: grad norm: 144.730 138.281 42.719
2024-12-02-11:22:59-root-INFO: Loss Change: 3981.192 -> 3881.593
2024-12-02-11:22:59-root-INFO: Regularization Change: 0.000 -> 0.348
2024-12-02-11:22:59-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-11:22:59-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-11:22:59-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-11:22:59-root-INFO: grad norm: 137.540 130.666 42.937
2024-12-02-11:23:00-root-INFO: grad norm: 133.335 127.070 40.390
2024-12-02-11:23:00-root-INFO: grad norm: 130.804 124.557 39.939
2024-12-02-11:23:01-root-INFO: grad norm: 128.772 122.805 38.746
2024-12-02-11:23:01-root-INFO: grad norm: 127.081 121.093 38.550
2024-12-02-11:23:01-root-INFO: Loss Change: 3858.676 -> 3791.261
2024-12-02-11:23:01-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-11:23:01-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-11:23:01-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-11:23:02-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-11:23:02-root-INFO: grad norm: 301.776 265.359 143.712
2024-12-02-11:23:02-root-INFO: grad norm: 183.589 171.085 66.593
2024-12-02-11:23:03-root-INFO: grad norm: 152.140 144.231 48.417
2024-12-02-11:23:03-root-INFO: grad norm: 137.992 130.516 44.804
2024-12-02-11:23:04-root-INFO: grad norm: 130.197 124.766 37.213
2024-12-02-11:23:04-root-INFO: Loss Change: 3787.220 -> 3691.687
2024-12-02-11:23:04-root-INFO: Regularization Change: 0.000 -> 0.338
2024-12-02-11:23:04-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-11:23:04-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:04-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-11:23:04-root-INFO: grad norm: 138.591 129.183 50.192
2024-12-02-11:23:05-root-INFO: grad norm: 123.433 117.204 38.717
2024-12-02-11:23:05-root-INFO: grad norm: 119.710 114.248 35.747
2024-12-02-11:23:06-root-INFO: grad norm: 117.793 112.353 35.386
2024-12-02-11:23:06-root-INFO: grad norm: 116.407 111.202 34.420
2024-12-02-11:23:06-root-INFO: Loss Change: 3674.326 -> 3611.365
2024-12-02-11:23:06-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-11:23:06-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-11:23:06-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:07-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-11:23:07-root-INFO: grad norm: 204.717 182.326 93.091
2024-12-02-11:23:07-root-INFO: grad norm: 138.029 129.648 47.364
2024-12-02-11:23:08-root-INFO: grad norm: 122.276 116.282 37.812
2024-12-02-11:23:08-root-INFO: grad norm: 117.011 111.485 35.532
2024-12-02-11:23:09-root-INFO: grad norm: 114.470 109.595 33.050
2024-12-02-11:23:09-root-INFO: Loss Change: 3594.275 -> 3520.519
2024-12-02-11:23:09-root-INFO: Regularization Change: 0.000 -> 0.308
2024-12-02-11:23:09-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-11:23:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:09-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-11:23:09-root-INFO: grad norm: 201.796 180.452 90.326
2024-12-02-11:23:10-root-INFO: grad norm: 141.833 133.063 49.101
2024-12-02-11:23:10-root-INFO: grad norm: 125.029 119.007 38.336
2024-12-02-11:23:11-root-INFO: grad norm: 120.520 114.953 36.208
2024-12-02-11:23:11-root-INFO: grad norm: 120.235 115.053 34.917
2024-12-02-11:23:11-root-INFO: Loss Change: 3516.604 -> 3442.916
2024-12-02-11:23:11-root-INFO: Regularization Change: 0.000 -> 0.326
2024-12-02-11:23:11-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-11:23:11-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:11-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-11:23:12-root-INFO: grad norm: 159.911 145.903 65.450
2024-12-02-11:23:12-root-INFO: grad norm: 125.869 119.660 39.046
2024-12-02-11:23:13-root-INFO: grad norm: 122.440 116.814 36.689
2024-12-02-11:23:13-root-INFO: grad norm: 127.462 122.125 36.495
2024-12-02-11:23:13-root-INFO: grad norm: 139.809 133.860 40.349
2024-12-02-11:23:14-root-INFO: Loss Change: 3428.874 -> 3364.871
2024-12-02-11:23:14-root-INFO: Regularization Change: 0.000 -> 0.326
2024-12-02-11:23:14-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-11:23:14-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:14-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-11:23:14-root-INFO: grad norm: 191.371 178.572 68.811
2024-12-02-11:23:15-root-INFO: grad norm: 210.069 200.324 63.239
2024-12-02-11:23:15-root-INFO: grad norm: 281.084 269.779 78.914
2024-12-02-11:23:15-root-INFO: Loss too large (3325.387->3330.433)! Learning rate decreased to 0.00084.
2024-12-02-11:23:16-root-INFO: grad norm: 278.480 267.771 76.482
2024-12-02-11:23:16-root-INFO: grad norm: 277.831 266.923 77.083
2024-12-02-11:23:16-root-INFO: Loss Change: 3344.776 -> 3296.566
2024-12-02-11:23:16-root-INFO: Regularization Change: 0.000 -> 0.288
2024-12-02-11:23:16-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-11:23:16-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:17-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-11:23:17-root-INFO: grad norm: 288.505 273.670 91.324
2024-12-02-11:23:17-root-INFO: grad norm: 401.612 386.590 108.815
2024-12-02-11:23:17-root-INFO: Loss too large (3285.098->3319.537)! Learning rate decreased to 0.00088.
2024-12-02-11:23:18-root-INFO: grad norm: 425.386 409.963 113.507
2024-12-02-11:23:18-root-INFO: grad norm: 451.969 434.368 124.904
2024-12-02-11:23:19-root-INFO: grad norm: 472.330 455.649 124.415
2024-12-02-11:23:19-root-INFO: Loss Change: 3285.826 -> 3262.378
2024-12-02-11:23:19-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-02-11:23:19-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-11:23:19-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-11:23:19-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-11:23:19-root-INFO: grad norm: 547.648 521.221 168.067
2024-12-02-11:23:20-root-INFO: Loss too large (3267.984->3339.302)! Learning rate decreased to 0.00092.
2024-12-02-11:23:20-root-INFO: grad norm: 565.661 543.873 155.481
2024-12-02-11:23:20-root-INFO: grad norm: 581.487 558.718 161.126
2024-12-02-11:23:21-root-INFO: grad norm: 584.354 563.168 155.921
2024-12-02-11:23:21-root-INFO: grad norm: 572.908 550.487 158.705
2024-12-02-11:23:22-root-INFO: Loss Change: 3267.984 -> 3220.659
2024-12-02-11:23:22-root-INFO: Regularization Change: 0.000 -> 0.518
2024-12-02-11:23:22-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-11:23:22-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:23:22-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-11:23:22-root-INFO: grad norm: 543.472 525.159 139.892
2024-12-02-11:23:22-root-INFO: Loss too large (3206.936->3263.345)! Learning rate decreased to 0.00096.
2024-12-02-11:23:23-root-INFO: grad norm: 520.348 500.952 140.747
2024-12-02-11:23:23-root-INFO: grad norm: 495.610 478.426 129.375
2024-12-02-11:23:23-root-INFO: grad norm: 469.128 451.888 126.009
2024-12-02-11:23:24-root-INFO: grad norm: 446.804 431.141 117.267
2024-12-02-11:23:24-root-INFO: Loss Change: 3206.936 -> 3122.682
2024-12-02-11:23:24-root-INFO: Regularization Change: 0.000 -> 0.478
2024-12-02-11:23:24-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-11:23:24-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:23:24-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-11:23:25-root-INFO: grad norm: 533.432 504.855 172.254
2024-12-02-11:23:25-root-INFO: Loss too large (3138.757->3186.643)! Learning rate decreased to 0.00100.
2024-12-02-11:23:25-root-INFO: grad norm: 505.528 487.045 135.445
2024-12-02-11:23:26-root-INFO: grad norm: 492.241 473.986 132.811
2024-12-02-11:23:26-root-INFO: grad norm: 484.423 467.702 126.175
2024-12-02-11:23:27-root-INFO: grad norm: 477.344 460.037 127.368
2024-12-02-11:23:27-root-INFO: Loss Change: 3138.757 -> 3056.804
2024-12-02-11:23:27-root-INFO: Regularization Change: 0.000 -> 0.590
2024-12-02-11:23:27-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-11:23:27-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:23:27-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-11:23:27-root-INFO: grad norm: 480.430 463.799 125.314
2024-12-02-11:23:27-root-INFO: Loss too large (3056.663->3105.809)! Learning rate decreased to 0.00105.
2024-12-02-11:23:28-root-INFO: grad norm: 474.598 458.306 123.283
2024-12-02-11:23:28-root-INFO: grad norm: 471.270 455.119 122.316
2024-12-02-11:23:29-root-INFO: grad norm: 469.301 453.063 122.380
2024-12-02-11:23:29-root-INFO: grad norm: 468.101 452.425 120.124
2024-12-02-11:23:29-root-INFO: Loss Change: 3056.663 -> 2990.716
2024-12-02-11:23:29-root-INFO: Regularization Change: 0.000 -> 0.548
2024-12-02-11:23:29-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-11:23:29-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:23:30-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-11:23:30-root-INFO: grad norm: 566.728 541.845 166.085
2024-12-02-11:23:30-root-INFO: Loss too large (3007.673->3086.869)! Learning rate decreased to 0.00110.
2024-12-02-11:23:30-root-INFO: grad norm: 549.300 531.909 137.126
2024-12-02-11:23:31-root-INFO: grad norm: 529.821 511.312 138.820
2024-12-02-11:23:31-root-INFO: grad norm: 511.126 494.969 127.497
2024-12-02-11:23:32-root-INFO: grad norm: 490.862 474.014 127.500
2024-12-02-11:23:32-root-INFO: Loss Change: 3007.673 -> 2923.363
2024-12-02-11:23:32-root-INFO: Regularization Change: 0.000 -> 0.670
2024-12-02-11:23:32-root-INFO: Undo step: 200
2024-12-02-11:23:32-root-INFO: Undo step: 201
2024-12-02-11:23:32-root-INFO: Undo step: 202
2024-12-02-11:23:32-root-INFO: Undo step: 203
2024-12-02-11:23:32-root-INFO: Undo step: 204
2024-12-02-11:23:32-root-INFO: Undo step: 205
2024-12-02-11:23:32-root-INFO: Undo step: 206
2024-12-02-11:23:32-root-INFO: Undo step: 207
2024-12-02-11:23:32-root-INFO: Undo step: 208
2024-12-02-11:23:32-root-INFO: Undo step: 209
2024-12-02-11:23:32-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-11:23:32-root-INFO: grad norm: 3940.422 3561.421 1686.179
2024-12-02-11:23:33-root-INFO: grad norm: 1513.829 1402.770 569.134
2024-12-02-11:23:34-root-INFO: grad norm: 1136.899 1067.927 389.963
2024-12-02-11:23:34-root-INFO: grad norm: 1072.965 1018.194 338.431
2024-12-02-11:23:34-root-INFO: grad norm: 986.822 933.833 319.022
2024-12-02-11:23:35-root-INFO: Loss Change: 8894.186 -> 3999.567
2024-12-02-11:23:35-root-INFO: Regularization Change: 0.000 -> 13.701
2024-12-02-11:23:35-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-11:23:35-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:35-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-11:23:35-root-INFO: grad norm: 979.718 921.670 332.223
2024-12-02-11:23:36-root-INFO: grad norm: 854.612 815.959 254.112
2024-12-02-11:23:36-root-INFO: grad norm: 755.887 713.703 248.984
2024-12-02-11:23:37-root-INFO: grad norm: 658.083 626.571 201.201
2024-12-02-11:23:37-root-INFO: grad norm: 577.547 544.845 191.582
2024-12-02-11:23:37-root-INFO: Loss Change: 3999.538 -> 3637.452
2024-12-02-11:23:37-root-INFO: Regularization Change: 0.000 -> 1.359
2024-12-02-11:23:37-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-11:23:37-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:38-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-11:23:38-root-INFO: grad norm: 436.298 421.142 113.995
2024-12-02-11:23:38-root-INFO: grad norm: 370.875 349.175 125.002
2024-12-02-11:23:39-root-INFO: grad norm: 324.762 309.982 96.857
2024-12-02-11:23:39-root-INFO: grad norm: 288.625 272.355 95.537
2024-12-02-11:23:40-root-INFO: grad norm: 258.604 245.757 80.494
2024-12-02-11:23:40-root-INFO: Loss Change: 3598.518 -> 3443.719
2024-12-02-11:23:40-root-INFO: Regularization Change: 0.000 -> 0.674
2024-12-02-11:23:40-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-11:23:40-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:40-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-11:23:40-root-INFO: grad norm: 279.910 257.834 108.957
2024-12-02-11:23:41-root-INFO: grad norm: 231.754 220.769 70.506
2024-12-02-11:23:41-root-INFO: grad norm: 206.439 195.219 67.133
2024-12-02-11:23:42-root-INFO: grad norm: 188.451 179.685 56.808
2024-12-02-11:23:42-root-INFO: grad norm: 175.095 166.932 52.838
2024-12-02-11:23:43-root-INFO: Loss Change: 3413.200 -> 3305.447
2024-12-02-11:23:43-root-INFO: Regularization Change: 0.000 -> 0.509
2024-12-02-11:23:43-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-11:23:43-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:43-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-11:23:43-root-INFO: grad norm: 173.999 163.975 58.205
2024-12-02-11:23:44-root-INFO: grad norm: 149.606 142.757 44.747
2024-12-02-11:23:44-root-INFO: grad norm: 139.835 133.587 41.334
2024-12-02-11:23:44-root-INFO: grad norm: 134.632 128.984 38.584
2024-12-02-11:23:45-root-INFO: grad norm: 130.938 125.094 38.680
2024-12-02-11:23:45-root-INFO: Loss Change: 3274.677 -> 3190.077
2024-12-02-11:23:45-root-INFO: Regularization Change: 0.000 -> 0.436
2024-12-02-11:23:45-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-11:23:45-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-11:23:45-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-11:23:46-root-INFO: grad norm: 176.253 161.185 71.306
2024-12-02-11:23:46-root-INFO: grad norm: 138.577 131.511 43.687
2024-12-02-11:23:46-root-INFO: grad norm: 127.601 121.939 37.587
2024-12-02-11:23:47-root-INFO: grad norm: 123.626 118.266 36.008
2024-12-02-11:23:47-root-INFO: grad norm: 122.088 117.449 33.333
2024-12-02-11:23:48-root-INFO: Loss Change: 3172.353 -> 3092.404
2024-12-02-11:23:48-root-INFO: Regularization Change: 0.000 -> 0.423
2024-12-02-11:23:48-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-11:23:48-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-11:23:48-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-11:23:48-root-INFO: grad norm: 185.592 170.444 73.439
2024-12-02-11:23:49-root-INFO: grad norm: 159.232 151.752 48.230
2024-12-02-11:23:49-root-INFO: grad norm: 156.949 150.869 43.261
2024-12-02-11:23:49-root-INFO: grad norm: 164.851 159.122 43.082
2024-12-02-11:23:50-root-INFO: grad norm: 179.668 173.720 45.849
2024-12-02-11:23:50-root-INFO: Loss Change: 3074.437 -> 3002.345
2024-12-02-11:23:50-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-02-11:23:50-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-11:23:50-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:23:50-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-11:23:51-root-INFO: grad norm: 195.012 188.182 51.162
2024-12-02-11:23:51-root-INFO: grad norm: 217.620 211.153 52.658
2024-12-02-11:23:51-root-INFO: grad norm: 255.118 248.768 56.562
2024-12-02-11:23:52-root-INFO: grad norm: 306.546 298.061 71.626
2024-12-02-11:23:52-root-INFO: Loss too large (2957.373->2957.743)! Learning rate decreased to 0.00096.
2024-12-02-11:23:53-root-INFO: grad norm: 250.573 244.375 55.384
2024-12-02-11:23:53-root-INFO: Loss Change: 2981.369 -> 2924.867
2024-12-02-11:23:53-root-INFO: Regularization Change: 0.000 -> 0.353
2024-12-02-11:23:53-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-11:23:53-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:23:53-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-11:23:53-root-INFO: grad norm: 326.697 306.717 112.497
2024-12-02-11:23:54-root-INFO: grad norm: 369.686 359.836 84.771
2024-12-02-11:23:54-root-INFO: Loss too large (2909.605->2915.319)! Learning rate decreased to 0.00100.
2024-12-02-11:23:54-root-INFO: grad norm: 310.961 302.507 72.017
2024-12-02-11:23:55-root-INFO: grad norm: 272.438 265.625 60.546
2024-12-02-11:23:55-root-INFO: grad norm: 242.036 235.663 55.175
2024-12-02-11:23:55-root-INFO: Loss Change: 2922.343 -> 2841.932
2024-12-02-11:23:55-root-INFO: Regularization Change: 0.000 -> 0.406
2024-12-02-11:23:55-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-11:23:55-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:23:56-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-11:23:56-root-INFO: grad norm: 217.883 211.544 52.173
2024-12-02-11:23:56-root-INFO: grad norm: 285.827 279.481 59.895
2024-12-02-11:23:56-root-INFO: Loss too large (2826.518->2833.977)! Learning rate decreased to 0.00105.
2024-12-02-11:23:57-root-INFO: grad norm: 266.997 260.556 58.290
2024-12-02-11:23:57-root-INFO: grad norm: 252.322 246.436 54.184
2024-12-02-11:23:58-root-INFO: grad norm: 241.866 236.056 52.695
2024-12-02-11:23:58-root-INFO: Loss Change: 2830.333 -> 2781.933
2024-12-02-11:23:58-root-INFO: Regularization Change: 0.000 -> 0.324
2024-12-02-11:23:58-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-11:23:58-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:23:58-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-11:23:58-root-INFO: grad norm: 346.868 332.856 97.592
2024-12-02-11:23:59-root-INFO: Loss too large (2776.792->2793.697)! Learning rate decreased to 0.00110.
2024-12-02-11:23:59-root-INFO: grad norm: 336.598 329.004 71.095
2024-12-02-11:24:00-root-INFO: grad norm: 339.161 331.346 72.390
2024-12-02-11:24:00-root-INFO: grad norm: 344.369 337.240 69.708
2024-12-02-11:24:00-root-INFO: grad norm: 351.190 343.547 72.871
2024-12-02-11:24:01-root-INFO: Loss Change: 2776.792 -> 2729.592
2024-12-02-11:24:01-root-INFO: Regularization Change: 0.000 -> 0.407
2024-12-02-11:24:01-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-11:24:01-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:24:01-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-11:24:01-root-INFO: grad norm: 310.998 304.864 61.463
2024-12-02-11:24:01-root-INFO: Loss too large (2698.528->2719.401)! Learning rate decreased to 0.00115.
2024-12-02-11:24:02-root-INFO: grad norm: 324.847 317.726 67.645
2024-12-02-11:24:02-root-INFO: grad norm: 345.388 338.770 67.288
2024-12-02-11:24:03-root-INFO: grad norm: 370.396 362.787 74.690
2024-12-02-11:24:03-root-INFO: grad norm: 395.228 387.856 75.980
2024-12-02-11:24:04-root-INFO: Loss Change: 2698.528 -> 2675.240
2024-12-02-11:24:04-root-INFO: Regularization Change: 0.000 -> 0.409
2024-12-02-11:24:04-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-11:24:04-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-11:24:04-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-11:24:04-root-INFO: grad norm: 500.428 486.799 115.996
2024-12-02-11:24:04-root-INFO: Loss too large (2684.502->2774.351)! Learning rate decreased to 0.00120.
2024-12-02-11:24:04-root-INFO: Loss too large (2684.502->2687.145)! Learning rate decreased to 0.00096.
2024-12-02-11:24:05-root-INFO: grad norm: 349.669 342.523 70.330
2024-12-02-11:24:05-root-INFO: grad norm: 252.322 246.738 52.792
2024-12-02-11:24:06-root-INFO: grad norm: 194.838 189.839 43.850
2024-12-02-11:24:06-root-INFO: grad norm: 157.494 153.574 34.919
2024-12-02-11:24:06-root-INFO: Loss Change: 2684.502 -> 2579.472
2024-12-02-11:24:06-root-INFO: Regularization Change: 0.000 -> 0.294
2024-12-02-11:24:06-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-11:24:06-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:07-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-11:24:07-root-INFO: grad norm: 109.636 102.887 37.873
2024-12-02-11:24:07-root-INFO: grad norm: 110.401 106.537 28.953
2024-12-02-11:24:08-root-INFO: grad norm: 159.716 155.933 34.553
2024-12-02-11:24:08-root-INFO: Loss too large (2534.143->2538.426)! Learning rate decreased to 0.00125.
2024-12-02-11:24:08-root-INFO: grad norm: 208.483 204.147 42.301
2024-12-02-11:24:08-root-INFO: Loss too large (2530.388->2532.092)! Learning rate decreased to 0.00100.
2024-12-02-11:24:09-root-INFO: grad norm: 196.690 192.775 39.051
2024-12-02-11:24:09-root-INFO: Loss Change: 2554.812 -> 2515.425
2024-12-02-11:24:09-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-02-11:24:09-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-11:24:09-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:09-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-11:24:10-root-INFO: grad norm: 278.420 268.798 72.562
2024-12-02-11:24:10-root-INFO: Loss too large (2510.181->2554.110)! Learning rate decreased to 0.00131.
2024-12-02-11:24:10-root-INFO: Loss too large (2510.181->2519.143)! Learning rate decreased to 0.00105.
2024-12-02-11:24:10-root-INFO: grad norm: 274.060 268.936 52.750
2024-12-02-11:24:11-root-INFO: grad norm: 280.773 275.381 54.760
2024-12-02-11:24:11-root-INFO: grad norm: 290.947 285.886 54.030
2024-12-02-11:24:12-root-INFO: grad norm: 305.072 299.760 56.680
2024-12-02-11:24:12-root-INFO: Loss Change: 2510.181 -> 2480.391
2024-12-02-11:24:12-root-INFO: Regularization Change: 0.000 -> 0.276
2024-12-02-11:24:12-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-11:24:12-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:12-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-11:24:12-root-INFO: grad norm: 276.072 270.701 54.192
2024-12-02-11:24:12-root-INFO: Loss too large (2461.266->2524.163)! Learning rate decreased to 0.00137.
2024-12-02-11:24:13-root-INFO: Loss too large (2461.266->2480.750)! Learning rate decreased to 0.00109.
2024-12-02-11:24:13-root-INFO: grad norm: 298.319 293.031 55.921
2024-12-02-11:24:14-root-INFO: grad norm: 323.971 318.740 57.987
2024-12-02-11:24:14-root-INFO: grad norm: 355.023 349.255 63.734
2024-12-02-11:24:14-root-INFO: Loss too large (2453.575->2455.443)! Learning rate decreased to 0.00087.
2024-12-02-11:24:15-root-INFO: grad norm: 252.847 248.477 46.808
2024-12-02-11:24:15-root-INFO: Loss Change: 2461.266 -> 2420.011
2024-12-02-11:24:15-root-INFO: Regularization Change: 0.000 -> 0.198
2024-12-02-11:24:15-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-11:24:15-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:15-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-11:24:15-root-INFO: grad norm: 268.272 260.669 63.417
2024-12-02-11:24:15-root-INFO: Loss too large (2413.599->2478.410)! Learning rate decreased to 0.00143.
2024-12-02-11:24:16-root-INFO: Loss too large (2413.599->2436.390)! Learning rate decreased to 0.00114.
2024-12-02-11:24:16-root-INFO: grad norm: 311.374 306.869 52.779
2024-12-02-11:24:16-root-INFO: Loss too large (2412.385->2415.565)! Learning rate decreased to 0.00091.
2024-12-02-11:24:17-root-INFO: grad norm: 244.551 240.209 45.874
2024-12-02-11:24:17-root-INFO: grad norm: 201.297 197.597 38.416
2024-12-02-11:24:18-root-INFO: grad norm: 169.315 166.051 33.086
2024-12-02-11:24:18-root-INFO: Loss Change: 2413.599 -> 2371.159
2024-12-02-11:24:18-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-11:24:18-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-11:24:18-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:18-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-11:24:18-root-INFO: grad norm: 112.511 106.588 36.025
2024-12-02-11:24:19-root-INFO: grad norm: 145.811 142.597 30.446
2024-12-02-11:24:19-root-INFO: Loss too large (2340.520->2354.438)! Learning rate decreased to 0.00149.
2024-12-02-11:24:19-root-INFO: Loss too large (2340.520->2343.243)! Learning rate decreased to 0.00119.
2024-12-02-11:24:20-root-INFO: grad norm: 186.208 182.851 35.197
2024-12-02-11:24:20-root-INFO: Loss too large (2337.245->2337.646)! Learning rate decreased to 0.00095.
2024-12-02-11:24:20-root-INFO: grad norm: 174.096 170.897 33.220
2024-12-02-11:24:21-root-INFO: grad norm: 165.862 162.664 32.415
2024-12-02-11:24:21-root-INFO: Loss Change: 2352.092 -> 2319.534
2024-12-02-11:24:21-root-INFO: Regularization Change: 0.000 -> 0.218
2024-12-02-11:24:21-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-11:24:21-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-11:24:21-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-11:24:21-root-INFO: grad norm: 242.900 237.873 49.163
2024-12-02-11:24:21-root-INFO: Loss too large (2309.685->2389.145)! Learning rate decreased to 0.00156.
2024-12-02-11:24:22-root-INFO: Loss too large (2309.685->2344.552)! Learning rate decreased to 0.00124.
2024-12-02-11:24:22-root-INFO: Loss too large (2309.685->2318.213)! Learning rate decreased to 0.00100.
2024-12-02-11:24:22-root-INFO: grad norm: 238.567 235.389 38.810
2024-12-02-11:24:23-root-INFO: grad norm: 238.157 234.256 42.933
2024-12-02-11:24:23-root-INFO: grad norm: 239.741 236.408 39.834
2024-12-02-11:24:24-root-INFO: grad norm: 243.487 239.774 42.360
2024-12-02-11:24:24-root-INFO: Loss Change: 2309.685 -> 2284.729
2024-12-02-11:24:24-root-INFO: Regularization Change: 0.000 -> 0.183
2024-12-02-11:24:24-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-11:24:24-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:24:24-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-11:24:24-root-INFO: grad norm: 202.335 197.669 43.200
2024-12-02-11:24:24-root-INFO: Loss too large (2270.159->2301.112)! Learning rate decreased to 0.00162.
2024-12-02-11:24:25-root-INFO: Loss too large (2270.159->2276.283)! Learning rate decreased to 0.00130.
2024-12-02-11:24:25-root-INFO: grad norm: 258.905 254.037 49.974
2024-12-02-11:24:25-root-INFO: Loss too large (2263.145->2280.357)! Learning rate decreased to 0.00104.
2024-12-02-11:24:26-root-INFO: grad norm: 278.093 274.204 46.343
2024-12-02-11:24:26-root-INFO: grad norm: 302.891 298.622 50.672
2024-12-02-11:24:26-root-INFO: Loss too large (2257.603->2258.233)! Learning rate decreased to 0.00083.
2024-12-02-11:24:27-root-INFO: grad norm: 216.577 213.213 38.027
2024-12-02-11:24:27-root-INFO: Loss Change: 2270.159 -> 2233.129
2024-12-02-11:24:27-root-INFO: Regularization Change: 0.000 -> 0.185
2024-12-02-11:24:27-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-11:24:27-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:24:27-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-11:24:27-root-INFO: grad norm: 259.146 253.584 53.398
2024-12-02-11:24:28-root-INFO: Loss too large (2233.853->2344.067)! Learning rate decreased to 0.00170.
2024-12-02-11:24:28-root-INFO: Loss too large (2233.853->2286.233)! Learning rate decreased to 0.00136.
2024-12-02-11:24:28-root-INFO: Loss too large (2233.853->2251.117)! Learning rate decreased to 0.00108.
2024-12-02-11:24:28-root-INFO: grad norm: 281.453 277.956 44.226
2024-12-02-11:24:29-root-INFO: grad norm: 319.223 314.848 52.666
2024-12-02-11:24:29-root-INFO: Loss too large (2229.615->2233.506)! Learning rate decreased to 0.00087.
2024-12-02-11:24:29-root-INFO: grad norm: 237.462 234.185 39.312
2024-12-02-11:24:30-root-INFO: grad norm: 176.584 173.732 31.609
2024-12-02-11:24:30-root-INFO: Loss Change: 2233.853 -> 2196.968
2024-12-02-11:24:30-root-INFO: Regularization Change: 0.000 -> 0.153
2024-12-02-11:24:30-root-INFO: Undo step: 190
2024-12-02-11:24:30-root-INFO: Undo step: 191
2024-12-02-11:24:30-root-INFO: Undo step: 192
2024-12-02-11:24:30-root-INFO: Undo step: 193
2024-12-02-11:24:30-root-INFO: Undo step: 194
2024-12-02-11:24:30-root-INFO: Undo step: 195
2024-12-02-11:24:30-root-INFO: Undo step: 196
2024-12-02-11:24:30-root-INFO: Undo step: 197
2024-12-02-11:24:30-root-INFO: Undo step: 198
2024-12-02-11:24:30-root-INFO: Undo step: 199
2024-12-02-11:24:30-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-11:24:31-root-INFO: grad norm: 1142.290 972.268 599.601
2024-12-02-11:24:31-root-INFO: grad norm: 846.127 742.942 404.929
2024-12-02-11:24:31-root-INFO: grad norm: 738.304 693.190 254.126
2024-12-02-11:24:32-root-INFO: grad norm: 616.595 580.294 208.443
2024-12-02-11:24:32-root-INFO: grad norm: 555.853 524.223 184.834
2024-12-02-11:24:33-root-INFO: Loss Change: 4713.161 -> 3136.461
2024-12-02-11:24:33-root-INFO: Regularization Change: 0.000 -> 8.576
2024-12-02-11:24:33-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-11:24:33-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-11:24:33-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-11:24:33-root-INFO: grad norm: 516.067 485.689 174.447
2024-12-02-11:24:33-root-INFO: grad norm: 476.924 452.004 152.146
2024-12-02-11:24:34-root-INFO: grad norm: 447.132 421.060 150.450
2024-12-02-11:24:34-root-INFO: grad norm: 436.451 415.867 132.452
2024-12-02-11:24:35-root-INFO: grad norm: 437.168 412.891 143.653
2024-12-02-11:24:35-root-INFO: Loss Change: 3115.433 -> 2923.919
2024-12-02-11:24:35-root-INFO: Regularization Change: 0.000 -> 1.721
2024-12-02-11:24:35-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-11:24:35-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-11:24:35-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-11:24:35-root-INFO: grad norm: 409.253 392.945 114.376
2024-12-02-11:24:36-root-INFO: grad norm: 432.590 412.490 130.331
2024-12-02-11:24:36-root-INFO: Loss too large (2860.154->2860.990)! Learning rate decreased to 0.00120.
2024-12-02-11:24:37-root-INFO: grad norm: 325.716 312.735 91.039
2024-12-02-11:24:37-root-INFO: grad norm: 240.961 229.163 74.474
2024-12-02-11:24:38-root-INFO: grad norm: 203.755 195.002 59.080
2024-12-02-11:24:38-root-INFO: Loss Change: 2895.869 -> 2736.168
2024-12-02-11:24:38-root-INFO: Regularization Change: 0.000 -> 0.793
2024-12-02-11:24:38-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-11:24:38-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:38-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-11:24:38-root-INFO: grad norm: 230.446 218.985 71.771
2024-12-02-11:24:39-root-INFO: grad norm: 281.845 272.899 70.447
2024-12-02-11:24:39-root-INFO: grad norm: 392.009 377.922 104.142
2024-12-02-11:24:39-root-INFO: Loss too large (2698.586->2733.429)! Learning rate decreased to 0.00125.
2024-12-02-11:24:40-root-INFO: grad norm: 362.828 352.862 84.457
2024-12-02-11:24:40-root-INFO: grad norm: 327.793 316.219 86.334
2024-12-02-11:24:40-root-INFO: Loss Change: 2709.874 -> 2647.334
2024-12-02-11:24:40-root-INFO: Regularization Change: 0.000 -> 0.654
2024-12-02-11:24:40-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-11:24:40-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:41-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-11:24:41-root-INFO: grad norm: 273.190 265.909 62.653
2024-12-02-11:24:41-root-INFO: Loss too large (2620.629->2621.919)! Learning rate decreased to 0.00131.
2024-12-02-11:24:41-root-INFO: grad norm: 268.755 260.164 67.407
2024-12-02-11:24:42-root-INFO: grad norm: 276.897 269.737 62.560
2024-12-02-11:24:42-root-INFO: grad norm: 296.044 287.402 71.010
2024-12-02-11:24:43-root-INFO: grad norm: 316.773 309.124 69.190
2024-12-02-11:24:43-root-INFO: Loss Change: 2620.629 -> 2564.087
2024-12-02-11:24:43-root-INFO: Regularization Change: 0.000 -> 0.559
2024-12-02-11:24:43-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-11:24:43-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:43-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-11:24:43-root-INFO: grad norm: 393.310 383.164 88.757
2024-12-02-11:24:43-root-INFO: Loss too large (2559.942->2625.292)! Learning rate decreased to 0.00137.
2024-12-02-11:24:44-root-INFO: Loss too large (2559.942->2566.471)! Learning rate decreased to 0.00109.
2024-12-02-11:24:44-root-INFO: grad norm: 288.691 281.715 63.081
2024-12-02-11:24:45-root-INFO: grad norm: 204.499 198.432 49.445
2024-12-02-11:24:45-root-INFO: grad norm: 164.624 159.717 39.892
2024-12-02-11:24:46-root-INFO: grad norm: 137.004 132.016 36.632
2024-12-02-11:24:46-root-INFO: Loss Change: 2559.942 -> 2473.171
2024-12-02-11:24:46-root-INFO: Regularization Change: 0.000 -> 0.325
2024-12-02-11:24:46-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-11:24:46-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:46-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-11:24:46-root-INFO: grad norm: 110.029 103.494 37.356
2024-12-02-11:24:47-root-INFO: grad norm: 106.478 102.233 29.766
2024-12-02-11:24:47-root-INFO: grad norm: 146.109 142.021 34.322
2024-12-02-11:24:47-root-INFO: Loss too large (2427.892->2429.190)! Learning rate decreased to 0.00143.
2024-12-02-11:24:48-root-INFO: grad norm: 198.549 193.764 43.326
2024-12-02-11:24:48-root-INFO: Loss too large (2422.324->2424.978)! Learning rate decreased to 0.00114.
2024-12-02-11:24:48-root-INFO: grad norm: 199.607 195.271 41.377
2024-12-02-11:24:49-root-INFO: Loss Change: 2455.033 -> 2407.273
2024-12-02-11:24:49-root-INFO: Regularization Change: 0.000 -> 0.419
2024-12-02-11:24:49-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-11:24:49-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-11:24:49-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-11:24:49-root-INFO: grad norm: 277.829 270.065 65.221
2024-12-02-11:24:49-root-INFO: Loss too large (2395.696->2450.498)! Learning rate decreased to 0.00149.
2024-12-02-11:24:49-root-INFO: Loss too large (2395.696->2410.984)! Learning rate decreased to 0.00119.
2024-12-02-11:24:50-root-INFO: grad norm: 279.073 273.952 53.217
2024-12-02-11:24:50-root-INFO: grad norm: 288.451 282.777 56.929
2024-12-02-11:24:51-root-INFO: grad norm: 297.803 292.643 55.197
2024-12-02-11:24:51-root-INFO: grad norm: 311.052 305.315 59.463
2024-12-02-11:24:52-root-INFO: Loss Change: 2395.696 -> 2366.535
2024-12-02-11:24:52-root-INFO: Regularization Change: 0.000 -> 0.356
2024-12-02-11:24:52-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-11:24:52-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-11:24:52-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-11:24:52-root-INFO: grad norm: 271.920 267.645 48.033
2024-12-02-11:24:52-root-INFO: Loss too large (2334.386->2404.160)! Learning rate decreased to 0.00156.
2024-12-02-11:24:52-root-INFO: Loss too large (2334.386->2353.987)! Learning rate decreased to 0.00124.
2024-12-02-11:24:53-root-INFO: grad norm: 295.882 290.507 56.138
2024-12-02-11:24:53-root-INFO: Loss too large (2327.132->2327.843)! Learning rate decreased to 0.00100.
2024-12-02-11:24:53-root-INFO: grad norm: 217.399 213.766 39.580
2024-12-02-11:24:54-root-INFO: grad norm: 159.273 155.692 33.583
2024-12-02-11:24:54-root-INFO: grad norm: 128.564 125.429 28.217
2024-12-02-11:24:55-root-INFO: Loss Change: 2334.386 -> 2282.068
2024-12-02-11:24:55-root-INFO: Regularization Change: 0.000 -> 0.209
2024-12-02-11:24:55-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-11:24:55-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:24:55-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-11:24:55-root-INFO: grad norm: 204.069 196.251 55.942
2024-12-02-11:24:55-root-INFO: Loss too large (2278.743->2297.483)! Learning rate decreased to 0.00162.
2024-12-02-11:24:56-root-INFO: grad norm: 293.951 288.879 54.371
2024-12-02-11:24:56-root-INFO: Loss too large (2278.345->2318.420)! Learning rate decreased to 0.00130.
2024-12-02-11:24:56-root-INFO: Loss too large (2278.345->2279.715)! Learning rate decreased to 0.00104.
2024-12-02-11:24:56-root-INFO: grad norm: 234.669 230.641 43.293
2024-12-02-11:24:57-root-INFO: grad norm: 200.046 196.258 38.747
2024-12-02-11:24:57-root-INFO: grad norm: 171.175 167.964 33.001
2024-12-02-11:24:58-root-INFO: Loss Change: 2278.743 -> 2233.396
2024-12-02-11:24:58-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-02-11:24:58-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-11:24:58-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:24:58-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-11:24:58-root-INFO: grad norm: 109.635 105.634 29.350
2024-12-02-11:24:58-root-INFO: grad norm: 175.063 171.819 33.548
2024-12-02-11:24:59-root-INFO: Loss too large (2206.237->2238.736)! Learning rate decreased to 0.00170.
2024-12-02-11:24:59-root-INFO: Loss too large (2206.237->2217.698)! Learning rate decreased to 0.00136.
2024-12-02-11:24:59-root-INFO: grad norm: 229.875 226.745 37.802
2024-12-02-11:24:59-root-INFO: Loss too large (2205.733->2210.865)! Learning rate decreased to 0.00108.
2024-12-02-11:25:00-root-INFO: grad norm: 216.984 213.725 37.468
2024-12-02-11:25:00-root-INFO: grad norm: 208.900 205.872 35.443
2024-12-02-11:25:01-root-INFO: Loss Change: 2215.427 -> 2183.824
2024-12-02-11:25:01-root-INFO: Regularization Change: 0.000 -> 0.280
2024-12-02-11:25:01-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-11:25:01-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:25:01-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-11:25:01-root-INFO: grad norm: 265.274 260.721 48.937
2024-12-02-11:25:01-root-INFO: Loss too large (2179.948->2280.336)! Learning rate decreased to 0.00177.
2024-12-02-11:25:01-root-INFO: Loss too large (2179.948->2225.065)! Learning rate decreased to 0.00142.
2024-12-02-11:25:01-root-INFO: Loss too large (2179.948->2191.545)! Learning rate decreased to 0.00113.
2024-12-02-11:25:02-root-INFO: grad norm: 251.627 248.322 40.648
2024-12-02-11:25:02-root-INFO: grad norm: 243.866 240.494 40.414
2024-12-02-11:25:03-root-INFO: grad norm: 238.929 235.677 39.284
2024-12-02-11:25:03-root-INFO: grad norm: 234.867 231.748 38.150
2024-12-02-11:25:04-root-INFO: Loss Change: 2179.948 -> 2146.407
2024-12-02-11:25:04-root-INFO: Regularization Change: 0.000 -> 0.245
2024-12-02-11:25:04-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-11:25:04-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:25:04-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-11:25:04-root-INFO: grad norm: 174.893 172.206 30.538
2024-12-02-11:25:04-root-INFO: Loss too large (2120.060->2154.043)! Learning rate decreased to 0.00185.
2024-12-02-11:25:04-root-INFO: Loss too large (2120.060->2129.780)! Learning rate decreased to 0.00148.
2024-12-02-11:25:05-root-INFO: grad norm: 232.987 229.506 40.120
2024-12-02-11:25:05-root-INFO: Loss too large (2116.789->2128.479)! Learning rate decreased to 0.00118.
2024-12-02-11:25:05-root-INFO: grad norm: 232.568 229.752 36.086
2024-12-02-11:25:06-root-INFO: grad norm: 233.730 230.675 37.668
2024-12-02-11:25:06-root-INFO: grad norm: 235.079 232.216 36.578
2024-12-02-11:25:06-root-INFO: Loss Change: 2120.060 -> 2094.363
2024-12-02-11:25:07-root-INFO: Regularization Change: 0.000 -> 0.245
2024-12-02-11:25:07-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-11:25:07-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-11:25:07-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-11:25:07-root-INFO: grad norm: 274.304 269.630 50.423
2024-12-02-11:25:07-root-INFO: Loss too large (2093.572->2220.636)! Learning rate decreased to 0.00193.
2024-12-02-11:25:07-root-INFO: Loss too large (2093.572->2154.083)! Learning rate decreased to 0.00154.
2024-12-02-11:25:07-root-INFO: Loss too large (2093.572->2112.957)! Learning rate decreased to 0.00123.
2024-12-02-11:25:08-root-INFO: grad norm: 274.847 271.458 43.025
2024-12-02-11:25:08-root-INFO: grad norm: 279.390 276.037 43.158
2024-12-02-11:25:09-root-INFO: grad norm: 281.826 278.453 43.472
2024-12-02-11:25:09-root-INFO: grad norm: 283.882 280.717 42.270
2024-12-02-11:25:09-root-INFO: Loss Change: 2093.572 -> 2067.854
2024-12-02-11:25:09-root-INFO: Regularization Change: 0.000 -> 0.304
2024-12-02-11:25:09-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-11:25:09-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:10-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-11:25:10-root-INFO: grad norm: 239.715 236.740 37.648
2024-12-02-11:25:10-root-INFO: Loss too large (2041.350->2147.370)! Learning rate decreased to 0.00201.
2024-12-02-11:25:10-root-INFO: Loss too large (2041.350->2084.151)! Learning rate decreased to 0.00161.
2024-12-02-11:25:10-root-INFO: Loss too large (2041.350->2048.712)! Learning rate decreased to 0.00129.
2024-12-02-11:25:11-root-INFO: grad norm: 228.454 225.423 37.092
2024-12-02-11:25:11-root-INFO: grad norm: 228.016 225.100 36.353
2024-12-02-11:25:12-root-INFO: grad norm: 228.760 226.087 34.872
2024-12-02-11:25:12-root-INFO: grad norm: 229.293 226.403 36.291
2024-12-02-11:25:12-root-INFO: Loss Change: 2041.350 -> 2007.791
2024-12-02-11:25:12-root-INFO: Regularization Change: 0.000 -> 0.273
2024-12-02-11:25:12-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-11:25:12-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:13-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-11:25:13-root-INFO: grad norm: 265.199 262.060 40.682
2024-12-02-11:25:13-root-INFO: Loss too large (1999.469->2129.733)! Learning rate decreased to 0.00209.
2024-12-02-11:25:13-root-INFO: Loss too large (1999.469->2061.858)! Learning rate decreased to 0.00168.
2024-12-02-11:25:13-root-INFO: Loss too large (1999.469->2019.873)! Learning rate decreased to 0.00134.
2024-12-02-11:25:14-root-INFO: grad norm: 257.721 254.771 38.882
2024-12-02-11:25:14-root-INFO: grad norm: 249.423 246.723 36.604
2024-12-02-11:25:15-root-INFO: grad norm: 242.396 239.550 37.035
2024-12-02-11:25:15-root-INFO: grad norm: 233.831 231.318 34.191
2024-12-02-11:25:15-root-INFO: Loss Change: 1999.469 -> 1968.287
2024-12-02-11:25:15-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-02-11:25:15-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-11:25:15-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:16-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-11:25:16-root-INFO: grad norm: 222.934 218.538 44.050
2024-12-02-11:25:16-root-INFO: Loss too large (1958.884->2028.938)! Learning rate decreased to 0.00218.
2024-12-02-11:25:16-root-INFO: Loss too large (1958.884->1982.820)! Learning rate decreased to 0.00175.
2024-12-02-11:25:17-root-INFO: grad norm: 273.274 270.037 41.936
2024-12-02-11:25:17-root-INFO: Loss too large (1956.990->1970.210)! Learning rate decreased to 0.00140.
2024-12-02-11:25:17-root-INFO: grad norm: 242.208 238.825 40.342
2024-12-02-11:25:18-root-INFO: grad norm: 223.559 221.156 32.692
2024-12-02-11:25:18-root-INFO: grad norm: 211.712 208.590 36.229
2024-12-02-11:25:18-root-INFO: Loss Change: 1958.884 -> 1919.777
2024-12-02-11:25:18-root-INFO: Regularization Change: 0.000 -> 0.324
2024-12-02-11:25:18-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-11:25:18-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:19-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-11:25:19-root-INFO: grad norm: 254.995 251.677 41.007
2024-12-02-11:25:19-root-INFO: Loss too large (1916.762->2032.791)! Learning rate decreased to 0.00228.
2024-12-02-11:25:19-root-INFO: Loss too large (1916.762->1969.172)! Learning rate decreased to 0.00182.
2024-12-02-11:25:19-root-INFO: Loss too large (1916.762->1930.393)! Learning rate decreased to 0.00146.
2024-12-02-11:25:20-root-INFO: grad norm: 226.766 223.879 36.075
2024-12-02-11:25:20-root-INFO: grad norm: 199.217 196.909 30.237
2024-12-02-11:25:21-root-INFO: grad norm: 181.885 179.199 31.140
2024-12-02-11:25:21-root-INFO: grad norm: 165.109 163.102 25.665
2024-12-02-11:25:22-root-INFO: Loss Change: 1916.762 -> 1873.900
2024-12-02-11:25:22-root-INFO: Regularization Change: 0.000 -> 0.295
2024-12-02-11:25:22-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-11:25:22-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:22-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-11:25:22-root-INFO: grad norm: 143.342 140.526 28.272
2024-12-02-11:25:22-root-INFO: Loss too large (1860.955->1889.563)! Learning rate decreased to 0.00237.
2024-12-02-11:25:22-root-INFO: Loss too large (1860.955->1869.378)! Learning rate decreased to 0.00190.
2024-12-02-11:25:23-root-INFO: grad norm: 184.732 182.598 27.999
2024-12-02-11:25:23-root-INFO: Loss too large (1858.420->1862.218)! Learning rate decreased to 0.00152.
2024-12-02-11:25:23-root-INFO: grad norm: 166.700 164.036 29.680
2024-12-02-11:25:24-root-INFO: grad norm: 152.821 150.921 24.021
2024-12-02-11:25:24-root-INFO: grad norm: 142.939 140.288 27.403
2024-12-02-11:25:24-root-INFO: Loss Change: 1860.955 -> 1830.349
2024-12-02-11:25:24-root-INFO: Regularization Change: 0.000 -> 0.272
2024-12-02-11:25:24-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-11:25:24-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-11:25:25-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-11:25:25-root-INFO: grad norm: 150.250 148.035 25.707
2024-12-02-11:25:25-root-INFO: Loss too large (1822.698->1856.706)! Learning rate decreased to 0.00247.
2024-12-02-11:25:25-root-INFO: Loss too large (1822.698->1834.680)! Learning rate decreased to 0.00198.
2024-12-02-11:25:26-root-INFO: grad norm: 191.185 188.145 33.959
2024-12-02-11:25:26-root-INFO: Loss too large (1822.091->1825.621)! Learning rate decreased to 0.00158.
2024-12-02-11:25:26-root-INFO: grad norm: 172.214 170.329 25.409
2024-12-02-11:25:27-root-INFO: grad norm: 159.713 156.801 30.360
2024-12-02-11:25:27-root-INFO: grad norm: 147.969 146.219 22.685
2024-12-02-11:25:27-root-INFO: Loss Change: 1822.698 -> 1792.827
2024-12-02-11:25:27-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-02-11:25:27-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-11:25:27-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:25:28-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-11:25:28-root-INFO: grad norm: 129.742 125.443 33.121
2024-12-02-11:25:28-root-INFO: Loss too large (1785.359->1805.826)! Learning rate decreased to 0.00258.
2024-12-02-11:25:28-root-INFO: Loss too large (1785.359->1789.435)! Learning rate decreased to 0.00206.
2024-12-02-11:25:29-root-INFO: grad norm: 170.154 167.942 27.346
2024-12-02-11:25:29-root-INFO: Loss too large (1780.811->1787.371)! Learning rate decreased to 0.00165.
2024-12-02-11:25:29-root-INFO: grad norm: 178.125 174.260 36.906
2024-12-02-11:25:30-root-INFO: grad norm: 182.215 179.680 30.290
2024-12-02-11:25:30-root-INFO: grad norm: 203.178 199.253 39.745
2024-12-02-11:25:30-root-INFO: Loss too large (1766.112->1768.243)! Learning rate decreased to 0.00132.
2024-12-02-11:25:31-root-INFO: Loss Change: 1785.359 -> 1758.557
2024-12-02-11:25:31-root-INFO: Regularization Change: 0.000 -> 0.295
2024-12-02-11:25:31-root-INFO: Undo step: 180
2024-12-02-11:25:31-root-INFO: Undo step: 181
2024-12-02-11:25:31-root-INFO: Undo step: 182
2024-12-02-11:25:31-root-INFO: Undo step: 183
2024-12-02-11:25:31-root-INFO: Undo step: 184
2024-12-02-11:25:31-root-INFO: Undo step: 185
2024-12-02-11:25:31-root-INFO: Undo step: 186
2024-12-02-11:25:31-root-INFO: Undo step: 187
2024-12-02-11:25:31-root-INFO: Undo step: 188
2024-12-02-11:25:31-root-INFO: Undo step: 189
2024-12-02-11:25:31-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-11:25:31-root-INFO: grad norm: 937.673 800.321 488.586
2024-12-02-11:25:31-root-INFO: grad norm: 473.131 417.370 222.833
2024-12-02-11:25:32-root-INFO: grad norm: 296.757 268.031 127.373
2024-12-02-11:25:32-root-INFO: grad norm: 264.499 239.214 112.856
2024-12-02-11:25:33-root-INFO: grad norm: 286.728 275.518 79.388
2024-12-02-11:25:33-root-INFO: Loss too large (2513.141->2515.626)! Learning rate decreased to 0.00170.
2024-12-02-11:25:33-root-INFO: Loss Change: 4053.378 -> 2485.428
2024-12-02-11:25:33-root-INFO: Regularization Change: 0.000 -> 12.626
2024-12-02-11:25:33-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-11:25:33-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:25:33-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-11:25:34-root-INFO: grad norm: 394.654 386.042 81.993
2024-12-02-11:25:34-root-INFO: Loss too large (2459.005->2597.253)! Learning rate decreased to 0.00177.
2024-12-02-11:25:34-root-INFO: Loss too large (2459.005->2511.412)! Learning rate decreased to 0.00142.
2024-12-02-11:25:34-root-INFO: grad norm: 447.886 438.190 92.690
2024-12-02-11:25:35-root-INFO: grad norm: 522.057 513.055 96.532
2024-12-02-11:25:35-root-INFO: Loss too large (2435.682->2456.375)! Learning rate decreased to 0.00113.
2024-12-02-11:25:36-root-INFO: grad norm: 410.770 400.954 89.262
2024-12-02-11:25:36-root-INFO: grad norm: 313.450 306.046 67.727
2024-12-02-11:25:36-root-INFO: Loss Change: 2459.005 -> 2321.336
2024-12-02-11:25:36-root-INFO: Regularization Change: 0.000 -> 0.882
2024-12-02-11:25:36-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-11:25:36-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-11:25:36-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-11:25:37-root-INFO: grad norm: 318.411 308.875 77.342
2024-12-02-11:25:37-root-INFO: Loss too large (2308.178->2411.040)! Learning rate decreased to 0.00185.
2024-12-02-11:25:37-root-INFO: Loss too large (2308.178->2336.033)! Learning rate decreased to 0.00148.
2024-12-02-11:25:37-root-INFO: grad norm: 379.210 371.843 74.385
2024-12-02-11:25:38-root-INFO: Loss too large (2294.181->2301.239)! Learning rate decreased to 0.00118.
2024-12-02-11:25:38-root-INFO: grad norm: 313.608 304.947 73.192
2024-12-02-11:25:38-root-INFO: grad norm: 258.536 252.084 57.396
2024-12-02-11:25:39-root-INFO: grad norm: 227.829 220.503 57.314
2024-12-02-11:25:39-root-INFO: Loss Change: 2308.178 -> 2202.136
2024-12-02-11:25:39-root-INFO: Regularization Change: 0.000 -> 0.617
2024-12-02-11:25:39-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-11:25:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-11:25:39-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-11:25:40-root-INFO: grad norm: 193.103 186.702 49.307
2024-12-02-11:25:40-root-INFO: Loss too large (2188.905->2207.535)! Learning rate decreased to 0.00193.
2024-12-02-11:25:40-root-INFO: grad norm: 312.603 303.414 75.237
2024-12-02-11:25:40-root-INFO: Loss too large (2186.971->2244.211)! Learning rate decreased to 0.00154.
2024-12-02-11:25:41-root-INFO: Loss too large (2186.971->2193.844)! Learning rate decreased to 0.00123.
2024-12-02-11:25:41-root-INFO: grad norm: 282.276 274.544 65.618
2024-12-02-11:25:42-root-INFO: grad norm: 261.788 254.101 62.975
2024-12-02-11:25:42-root-INFO: grad norm: 244.460 237.346 58.548
2024-12-02-11:25:42-root-INFO: Loss Change: 2188.905 -> 2124.303
2024-12-02-11:25:42-root-INFO: Regularization Change: 0.000 -> 0.541
2024-12-02-11:25:42-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-11:25:42-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:43-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-11:25:43-root-INFO: grad norm: 279.848 271.887 66.276
2024-12-02-11:25:43-root-INFO: Loss too large (2110.736->2221.528)! Learning rate decreased to 0.00201.
2024-12-02-11:25:43-root-INFO: Loss too large (2110.736->2152.696)! Learning rate decreased to 0.00161.
2024-12-02-11:25:43-root-INFO: Loss too large (2110.736->2112.685)! Learning rate decreased to 0.00129.
2024-12-02-11:25:44-root-INFO: grad norm: 247.780 240.969 57.696
2024-12-02-11:25:44-root-INFO: grad norm: 229.959 222.781 57.005
2024-12-02-11:25:45-root-INFO: grad norm: 214.996 208.170 53.748
2024-12-02-11:25:45-root-INFO: grad norm: 203.686 197.016 51.698
2024-12-02-11:25:45-root-INFO: Loss Change: 2110.736 -> 2041.851
2024-12-02-11:25:45-root-INFO: Regularization Change: 0.000 -> 0.435
2024-12-02-11:25:45-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-11:25:45-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:46-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-11:25:46-root-INFO: grad norm: 172.229 165.480 47.740
2024-12-02-11:25:46-root-INFO: Loss too large (2024.209->2051.815)! Learning rate decreased to 0.00209.
2024-12-02-11:25:46-root-INFO: Loss too large (2024.209->2029.536)! Learning rate decreased to 0.00168.
2024-12-02-11:25:46-root-INFO: grad norm: 221.418 213.596 58.331
2024-12-02-11:25:47-root-INFO: Loss too large (2017.624->2019.221)! Learning rate decreased to 0.00134.
2024-12-02-11:25:47-root-INFO: grad norm: 206.600 199.240 54.655
2024-12-02-11:25:48-root-INFO: grad norm: 194.504 187.932 50.135
2024-12-02-11:25:48-root-INFO: grad norm: 184.704 177.973 49.411
2024-12-02-11:25:48-root-INFO: Loss Change: 2024.209 -> 1973.094
2024-12-02-11:25:48-root-INFO: Regularization Change: 0.000 -> 0.408
2024-12-02-11:25:48-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-11:25:48-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:48-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-11:25:49-root-INFO: grad norm: 245.592 238.246 59.619
2024-12-02-11:25:49-root-INFO: Loss too large (1964.600->2035.821)! Learning rate decreased to 0.00218.
2024-12-02-11:25:49-root-INFO: Loss too large (1964.600->1986.573)! Learning rate decreased to 0.00175.
2024-12-02-11:25:49-root-INFO: grad norm: 284.496 275.793 69.827
2024-12-02-11:25:50-root-INFO: Loss too large (1958.658->1962.679)! Learning rate decreased to 0.00140.
2024-12-02-11:25:50-root-INFO: grad norm: 243.722 236.646 58.301
2024-12-02-11:25:50-root-INFO: grad norm: 220.053 211.791 59.730
2024-12-02-11:25:51-root-INFO: grad norm: 200.906 194.303 51.083
2024-12-02-11:25:51-root-INFO: Loss Change: 1964.600 -> 1900.719
2024-12-02-11:25:51-root-INFO: Regularization Change: 0.000 -> 0.475
2024-12-02-11:25:51-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-11:25:51-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:51-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-11:25:51-root-INFO: grad norm: 178.453 171.312 49.976
2024-12-02-11:25:52-root-INFO: Loss too large (1880.665->1910.948)! Learning rate decreased to 0.00228.
2024-12-02-11:25:52-root-INFO: Loss too large (1880.665->1886.400)! Learning rate decreased to 0.00182.
2024-12-02-11:25:52-root-INFO: grad norm: 211.893 203.192 60.097
2024-12-02-11:25:53-root-INFO: grad norm: 259.537 249.617 71.071
2024-12-02-11:25:53-root-INFO: Loss too large (1869.848->1873.074)! Learning rate decreased to 0.00146.
2024-12-02-11:25:53-root-INFO: grad norm: 220.119 211.958 59.381
2024-12-02-11:25:54-root-INFO: grad norm: 192.287 185.218 51.655
2024-12-02-11:25:54-root-INFO: Loss Change: 1880.665 -> 1825.764
2024-12-02-11:25:54-root-INFO: Regularization Change: 0.000 -> 0.487
2024-12-02-11:25:54-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-11:25:54-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-11:25:54-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-11:25:55-root-INFO: grad norm: 182.175 176.540 44.961
2024-12-02-11:25:55-root-INFO: Loss too large (1813.739->1850.205)! Learning rate decreased to 0.00237.
2024-12-02-11:25:55-root-INFO: Loss too large (1813.739->1822.453)! Learning rate decreased to 0.00190.
2024-12-02-11:25:55-root-INFO: grad norm: 213.401 205.438 57.752
2024-12-02-11:25:56-root-INFO: grad norm: 256.075 248.380 62.305
2024-12-02-11:25:56-root-INFO: Loss too large (1803.285->1805.797)! Learning rate decreased to 0.00152.
2024-12-02-11:25:56-root-INFO: grad norm: 205.168 197.166 56.741
2024-12-02-11:25:57-root-INFO: grad norm: 164.589 159.517 40.542
2024-12-02-11:25:57-root-INFO: Loss Change: 1813.739 -> 1759.093
2024-12-02-11:25:57-root-INFO: Regularization Change: 0.000 -> 0.473
2024-12-02-11:25:57-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-11:25:57-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-11:25:57-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-11:25:57-root-INFO: grad norm: 203.513 196.592 52.622
2024-12-02-11:25:58-root-INFO: Loss too large (1751.168->1843.990)! Learning rate decreased to 0.00247.
2024-12-02-11:25:58-root-INFO: Loss too large (1751.168->1805.067)! Learning rate decreased to 0.00198.
2024-12-02-11:25:58-root-INFO: Loss too large (1751.168->1777.387)! Learning rate decreased to 0.00158.
2024-12-02-11:25:58-root-INFO: Loss too large (1751.168->1758.861)! Learning rate decreased to 0.00127.
2024-12-02-11:25:59-root-INFO: grad norm: 211.646 204.820 53.320
2024-12-02-11:25:59-root-INFO: grad norm: 276.961 270.601 59.013
2024-12-02-11:25:59-root-INFO: Loss too large (1743.511->1756.269)! Learning rate decreased to 0.00101.
2024-12-02-11:26:00-root-INFO: grad norm: 227.860 220.396 57.841
2024-12-02-11:26:00-root-INFO: grad norm: 170.712 166.457 37.876
2024-12-02-11:26:00-root-INFO: Loss Change: 1751.168 -> 1720.576
2024-12-02-11:26:00-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-11:26:00-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-11:26:00-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:26:01-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-11:26:01-root-INFO: grad norm: 86.055 82.363 24.937
2024-12-02-11:26:01-root-INFO: grad norm: 138.978 134.698 34.223
2024-12-02-11:26:01-root-INFO: Loss too large (1690.454->1749.083)! Learning rate decreased to 0.00258.
2024-12-02-11:26:02-root-INFO: Loss too large (1690.454->1726.328)! Learning rate decreased to 0.00206.
2024-12-02-11:26:02-root-INFO: Loss too large (1690.454->1709.696)! Learning rate decreased to 0.00165.
2024-12-02-11:26:02-root-INFO: Loss too large (1690.454->1698.411)! Learning rate decreased to 0.00132.
2024-12-02-11:26:02-root-INFO: Loss too large (1690.454->1691.347)! Learning rate decreased to 0.00106.
2024-12-02-11:26:02-root-INFO: grad norm: 157.343 152.320 39.441
2024-12-02-11:26:03-root-INFO: grad norm: 206.562 201.666 44.707
2024-12-02-11:26:03-root-INFO: Loss too large (1685.598->1689.323)! Learning rate decreased to 0.00084.
2024-12-02-11:26:04-root-INFO: grad norm: 180.067 174.419 44.742
2024-12-02-11:26:04-root-INFO: Loss Change: 1704.931 -> 1674.980
2024-12-02-11:26:04-root-INFO: Regularization Change: 0.000 -> 0.241
2024-12-02-11:26:04-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-11:26:04-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:26:04-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-11:26:04-root-INFO: grad norm: 326.820 319.584 68.394
2024-12-02-11:26:04-root-INFO: Loss too large (1683.493->1847.539)! Learning rate decreased to 0.00269.
2024-12-02-11:26:05-root-INFO: Loss too large (1683.493->1816.584)! Learning rate decreased to 0.00215.
2024-12-02-11:26:05-root-INFO: Loss too large (1683.493->1784.175)! Learning rate decreased to 0.00172.
2024-12-02-11:26:05-root-INFO: Loss too large (1683.493->1752.083)! Learning rate decreased to 0.00138.
2024-12-02-11:26:05-root-INFO: Loss too large (1683.493->1722.481)! Learning rate decreased to 0.00110.
2024-12-02-11:26:05-root-INFO: Loss too large (1683.493->1697.672)! Learning rate decreased to 0.00088.
2024-12-02-11:26:06-root-INFO: grad norm: 261.008 252.951 64.347
2024-12-02-11:26:06-root-INFO: grad norm: 190.398 185.647 42.267
2024-12-02-11:26:07-root-INFO: grad norm: 181.256 175.678 44.623
2024-12-02-11:26:07-root-INFO: grad norm: 172.489 168.218 38.146
2024-12-02-11:26:07-root-INFO: Loss Change: 1683.493 -> 1650.174
2024-12-02-11:26:07-root-INFO: Regularization Change: 0.000 -> 0.123
2024-12-02-11:26:07-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-11:26:07-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:26:07-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-11:26:08-root-INFO: grad norm: 85.742 82.597 23.009
2024-12-02-11:26:08-root-INFO: grad norm: 119.814 115.882 30.442
2024-12-02-11:26:08-root-INFO: Loss too large (1621.986->1702.553)! Learning rate decreased to 0.00280.
2024-12-02-11:26:08-root-INFO: Loss too large (1621.986->1668.962)! Learning rate decreased to 0.00224.
2024-12-02-11:26:09-root-INFO: Loss too large (1621.986->1646.760)! Learning rate decreased to 0.00179.
2024-12-02-11:26:09-root-INFO: Loss too large (1621.986->1632.912)! Learning rate decreased to 0.00143.
2024-12-02-11:26:09-root-INFO: Loss too large (1621.986->1624.786)! Learning rate decreased to 0.00115.
2024-12-02-11:26:09-root-INFO: grad norm: 180.241 176.004 38.854
2024-12-02-11:26:10-root-INFO: Loss too large (1620.360->1628.140)! Learning rate decreased to 0.00092.
2024-12-02-11:26:10-root-INFO: grad norm: 200.676 194.502 49.395
2024-12-02-11:26:10-root-INFO: grad norm: 225.019 219.822 48.080
2024-12-02-11:26:11-root-INFO: Loss too large (1617.633->1618.783)! Learning rate decreased to 0.00073.
2024-12-02-11:26:11-root-INFO: Loss Change: 1637.131 -> 1611.644
2024-12-02-11:26:11-root-INFO: Regularization Change: 0.000 -> 0.231
2024-12-02-11:26:11-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-11:26:11-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:26:11-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-11:26:11-root-INFO: grad norm: 69.359 66.735 18.897
2024-12-02-11:26:12-root-INFO: grad norm: 108.721 105.636 25.715
2024-12-02-11:26:12-root-INFO: Loss too large (1586.410->1642.035)! Learning rate decreased to 0.00291.
2024-12-02-11:26:12-root-INFO: Loss too large (1586.410->1622.219)! Learning rate decreased to 0.00233.
2024-12-02-11:26:12-root-INFO: Loss too large (1586.410->1607.310)! Learning rate decreased to 0.00187.
2024-12-02-11:26:12-root-INFO: Loss too large (1586.410->1596.961)! Learning rate decreased to 0.00149.
2024-12-02-11:26:13-root-INFO: Loss too large (1586.410->1590.308)! Learning rate decreased to 0.00119.
2024-12-02-11:26:13-root-INFO: grad norm: 173.773 168.444 42.707
2024-12-02-11:26:13-root-INFO: Loss too large (1586.353->1596.084)! Learning rate decreased to 0.00096.
2024-12-02-11:26:13-root-INFO: Loss too large (1586.353->1586.657)! Learning rate decreased to 0.00076.
2024-12-02-11:26:14-root-INFO: grad norm: 151.972 148.163 33.813
2024-12-02-11:26:14-root-INFO: grad norm: 138.541 134.249 34.215
2024-12-02-11:26:15-root-INFO: Loss Change: 1599.129 -> 1575.265
2024-12-02-11:26:15-root-INFO: Regularization Change: 0.000 -> 0.191
2024-12-02-11:26:15-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-11:26:15-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-11:26:15-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-11:26:15-root-INFO: grad norm: 309.875 302.416 67.581
2024-12-02-11:26:15-root-INFO: Loss too large (1587.026->1769.396)! Learning rate decreased to 0.00304.
2024-12-02-11:26:15-root-INFO: Loss too large (1587.026->1741.454)! Learning rate decreased to 0.00243.
2024-12-02-11:26:16-root-INFO: Loss too large (1587.026->1710.634)! Learning rate decreased to 0.00194.
2024-12-02-11:26:16-root-INFO: Loss too large (1587.026->1678.164)! Learning rate decreased to 0.00155.
2024-12-02-11:26:16-root-INFO: Loss too large (1587.026->1646.028)! Learning rate decreased to 0.00124.
2024-12-02-11:26:16-root-INFO: Loss too large (1587.026->1617.057)! Learning rate decreased to 0.00099.
2024-12-02-11:26:16-root-INFO: Loss too large (1587.026->1594.089)! Learning rate decreased to 0.00080.
2024-12-02-11:26:17-root-INFO: grad norm: 249.196 242.070 59.170
2024-12-02-11:26:17-root-INFO: grad norm: 199.319 194.129 45.188
2024-12-02-11:26:18-root-INFO: grad norm: 178.595 173.484 42.422
2024-12-02-11:26:18-root-INFO: grad norm: 159.733 155.516 36.459
2024-12-02-11:26:18-root-INFO: Loss Change: 1587.026 -> 1555.639
2024-12-02-11:26:18-root-INFO: Regularization Change: 0.000 -> 0.098
2024-12-02-11:26:18-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-11:26:18-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:26:19-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-11:26:19-root-INFO: grad norm: 94.981 91.672 24.850
2024-12-02-11:26:19-root-INFO: Loss too large (1546.782->1569.594)! Learning rate decreased to 0.00316.
2024-12-02-11:26:19-root-INFO: Loss too large (1546.782->1557.834)! Learning rate decreased to 0.00253.
2024-12-02-11:26:19-root-INFO: Loss too large (1546.782->1550.258)! Learning rate decreased to 0.00202.
2024-12-02-11:26:20-root-INFO: grad norm: 214.485 208.229 51.425
2024-12-02-11:26:20-root-INFO: Loss too large (1545.771->1630.491)! Learning rate decreased to 0.00162.
2024-12-02-11:26:20-root-INFO: Loss too large (1545.771->1592.637)! Learning rate decreased to 0.00129.
2024-12-02-11:26:20-root-INFO: Loss too large (1545.771->1566.539)! Learning rate decreased to 0.00104.
2024-12-02-11:26:20-root-INFO: Loss too large (1545.771->1550.085)! Learning rate decreased to 0.00083.
2024-12-02-11:26:21-root-INFO: grad norm: 202.228 197.261 44.547
2024-12-02-11:26:21-root-INFO: grad norm: 193.319 187.629 46.560
2024-12-02-11:26:22-root-INFO: grad norm: 183.675 179.144 40.545
2024-12-02-11:26:22-root-INFO: Loss Change: 1546.782 -> 1530.516
2024-12-02-11:26:22-root-INFO: Regularization Change: 0.000 -> 0.126
2024-12-02-11:26:22-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-11:26:22-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:26:22-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-11:26:22-root-INFO: grad norm: 137.115 133.065 33.080
2024-12-02-11:26:23-root-INFO: Loss too large (1527.628->1666.919)! Learning rate decreased to 0.00329.
2024-12-02-11:26:23-root-INFO: Loss too large (1527.628->1621.508)! Learning rate decreased to 0.00263.
2024-12-02-11:26:23-root-INFO: Loss too large (1527.628->1586.613)! Learning rate decreased to 0.00211.
2024-12-02-11:26:23-root-INFO: Loss too large (1527.628->1561.530)! Learning rate decreased to 0.00168.
2024-12-02-11:26:23-root-INFO: Loss too large (1527.628->1544.666)! Learning rate decreased to 0.00135.
2024-12-02-11:26:23-root-INFO: Loss too large (1527.628->1534.057)! Learning rate decreased to 0.00108.
2024-12-02-11:26:23-root-INFO: Loss too large (1527.628->1527.845)! Learning rate decreased to 0.00086.
2024-12-02-11:26:24-root-INFO: grad norm: 143.804 140.095 32.448
2024-12-02-11:26:24-root-INFO: grad norm: 149.754 145.316 36.184
2024-12-02-11:26:25-root-INFO: grad norm: 155.002 151.150 34.339
2024-12-02-11:26:25-root-INFO: grad norm: 158.858 154.148 38.394
2024-12-02-11:26:26-root-INFO: Loss Change: 1527.628 -> 1513.824
2024-12-02-11:26:26-root-INFO: Regularization Change: 0.000 -> 0.081
2024-12-02-11:26:26-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-11:26:26-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:26:26-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-11:26:26-root-INFO: grad norm: 356.797 348.494 76.522
2024-12-02-11:26:26-root-INFO: Loss too large (1540.705->1734.069)! Learning rate decreased to 0.00342.
2024-12-02-11:26:26-root-INFO: Loss too large (1540.705->1709.042)! Learning rate decreased to 0.00274.
2024-12-02-11:26:26-root-INFO: Loss too large (1540.705->1680.161)! Learning rate decreased to 0.00219.
2024-12-02-11:26:27-root-INFO: Loss too large (1540.705->1647.519)! Learning rate decreased to 0.00175.
2024-12-02-11:26:27-root-INFO: Loss too large (1540.705->1612.327)! Learning rate decreased to 0.00140.
2024-12-02-11:26:27-root-INFO: Loss too large (1540.705->1577.187)! Learning rate decreased to 0.00112.
2024-12-02-11:26:27-root-INFO: Loss too large (1540.705->1546.393)! Learning rate decreased to 0.00090.
2024-12-02-11:26:28-root-INFO: grad norm: 282.720 274.174 68.987
2024-12-02-11:26:28-root-INFO: grad norm: 234.651 228.478 53.469
2024-12-02-11:26:28-root-INFO: grad norm: 212.574 206.220 51.583
2024-12-02-11:26:29-root-INFO: grad norm: 193.331 188.143 44.486
2024-12-02-11:26:29-root-INFO: Loss Change: 1540.705 -> 1494.946
2024-12-02-11:26:29-root-INFO: Regularization Change: 0.000 -> 0.160
2024-12-02-11:26:29-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-11:26:29-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-11:26:29-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-11:26:30-root-INFO: grad norm: 113.377 110.067 27.193
2024-12-02-11:26:30-root-INFO: Loss too large (1488.149->1571.565)! Learning rate decreased to 0.00356.
2024-12-02-11:26:30-root-INFO: Loss too large (1488.149->1540.936)! Learning rate decreased to 0.00285.
2024-12-02-11:26:30-root-INFO: Loss too large (1488.149->1518.813)! Learning rate decreased to 0.00228.
2024-12-02-11:26:30-root-INFO: Loss too large (1488.149->1503.815)! Learning rate decreased to 0.00182.
2024-12-02-11:26:30-root-INFO: Loss too large (1488.149->1494.276)! Learning rate decreased to 0.00146.
2024-12-02-11:26:30-root-INFO: Loss too large (1488.149->1488.616)! Learning rate decreased to 0.00117.
2024-12-02-11:26:31-root-INFO: grad norm: 151.056 146.738 35.859
2024-12-02-11:26:31-root-INFO: Loss too large (1485.544->1486.348)! Learning rate decreased to 0.00093.
2024-12-02-11:26:32-root-INFO: grad norm: 149.961 145.546 36.120
2024-12-02-11:26:32-root-INFO: grad norm: 148.033 143.949 34.533
2024-12-02-11:26:32-root-INFO: grad norm: 145.522 141.199 35.207
2024-12-02-11:26:33-root-INFO: Loss Change: 1488.149 -> 1471.847
2024-12-02-11:26:33-root-INFO: Regularization Change: 0.000 -> 0.102
2024-12-02-11:26:33-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-11:26:33-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:26:33-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-11:26:33-root-INFO: grad norm: 272.790 266.353 58.911
2024-12-02-11:26:33-root-INFO: Loss too large (1483.496->1672.168)! Learning rate decreased to 0.00371.
2024-12-02-11:26:33-root-INFO: Loss too large (1483.496->1645.502)! Learning rate decreased to 0.00297.
2024-12-02-11:26:34-root-INFO: Loss too large (1483.496->1614.626)! Learning rate decreased to 0.00237.
2024-12-02-11:26:34-root-INFO: Loss too large (1483.496->1580.619)! Learning rate decreased to 0.00190.
2024-12-02-11:26:34-root-INFO: Loss too large (1483.496->1545.738)! Learning rate decreased to 0.00152.
2024-12-02-11:26:34-root-INFO: Loss too large (1483.496->1513.832)! Learning rate decreased to 0.00122.
2024-12-02-11:26:34-root-INFO: Loss too large (1483.496->1488.939)! Learning rate decreased to 0.00097.
2024-12-02-11:26:35-root-INFO: grad norm: 236.285 228.712 59.343
2024-12-02-11:26:35-root-INFO: grad norm: 211.516 206.172 47.247
2024-12-02-11:26:35-root-INFO: grad norm: 191.199 185.081 47.979
2024-12-02-11:26:36-root-INFO: grad norm: 175.538 170.934 39.940
2024-12-02-11:26:36-root-INFO: Loss Change: 1483.496 -> 1451.180
2024-12-02-11:26:36-root-INFO: Regularization Change: 0.000 -> 0.138
2024-12-02-11:26:36-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-11:26:36-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:26:36-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-11:26:37-root-INFO: grad norm: 83.869 80.938 21.980
2024-12-02-11:26:37-root-INFO: grad norm: 258.618 253.349 51.937
2024-12-02-11:26:37-root-INFO: Loss too large (1445.602->1632.238)! Learning rate decreased to 0.00386.
2024-12-02-11:26:37-root-INFO: Loss too large (1445.602->1606.241)! Learning rate decreased to 0.00309.
2024-12-02-11:26:38-root-INFO: Loss too large (1445.602->1575.029)! Learning rate decreased to 0.00247.
2024-12-02-11:26:38-root-INFO: Loss too large (1445.602->1540.202)! Learning rate decreased to 0.00198.
2024-12-02-11:26:38-root-INFO: Loss too large (1445.602->1504.516)! Learning rate decreased to 0.00158.
2024-12-02-11:26:38-root-INFO: Loss too large (1445.602->1472.432)! Learning rate decreased to 0.00126.
2024-12-02-11:26:38-root-INFO: Loss too large (1445.602->1448.214)! Learning rate decreased to 0.00101.
2024-12-02-11:26:39-root-INFO: grad norm: 215.326 208.409 54.139
2024-12-02-11:26:39-root-INFO: grad norm: 192.345 187.819 41.476
2024-12-02-11:26:40-root-INFO: grad norm: 170.576 165.159 42.650
2024-12-02-11:26:40-root-INFO: Loss Change: 1446.202 -> 1418.347
2024-12-02-11:26:40-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-11:26:40-root-INFO: Undo step: 170
2024-12-02-11:26:40-root-INFO: Undo step: 171
2024-12-02-11:26:40-root-INFO: Undo step: 172
2024-12-02-11:26:40-root-INFO: Undo step: 173
2024-12-02-11:26:40-root-INFO: Undo step: 174
2024-12-02-11:26:40-root-INFO: Undo step: 175
2024-12-02-11:26:40-root-INFO: Undo step: 176
2024-12-02-11:26:40-root-INFO: Undo step: 177
2024-12-02-11:26:40-root-INFO: Undo step: 178
2024-12-02-11:26:40-root-INFO: Undo step: 179
2024-12-02-11:26:40-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-11:26:40-root-INFO: grad norm: 842.745 809.369 234.819
2024-12-02-11:26:41-root-INFO: grad norm: 522.535 505.026 134.131
2024-12-02-11:26:41-root-INFO: grad norm: 403.120 385.779 116.964
2024-12-02-11:26:42-root-INFO: grad norm: 333.390 313.895 112.333
2024-12-02-11:26:42-root-INFO: grad norm: 301.933 286.431 95.501
2024-12-02-11:26:42-root-INFO: Loss Change: 3509.491 -> 2154.939
2024-12-02-11:26:42-root-INFO: Regularization Change: 0.000 -> 14.046
2024-12-02-11:26:42-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-11:26:42-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:26:43-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-11:26:43-root-INFO: grad norm: 332.823 312.971 113.226
2024-12-02-11:26:43-root-INFO: grad norm: 398.475 381.725 114.317
2024-12-02-11:26:43-root-INFO: Loss too large (2119.165->2141.027)! Learning rate decreased to 0.00269.
2024-12-02-11:26:44-root-INFO: grad norm: 333.048 316.844 102.618
2024-12-02-11:26:44-root-INFO: grad norm: 286.227 276.347 74.553
2024-12-02-11:26:45-root-INFO: grad norm: 281.808 270.129 80.287
2024-12-02-11:26:45-root-INFO: Loss Change: 2147.973 -> 1920.143
2024-12-02-11:26:45-root-INFO: Regularization Change: 0.000 -> 3.429
2024-12-02-11:26:45-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-11:26:45-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:26:45-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-11:26:45-root-INFO: grad norm: 296.080 287.151 72.165
2024-12-02-11:26:46-root-INFO: Loss too large (1903.963->1924.482)! Learning rate decreased to 0.00280.
2024-12-02-11:26:46-root-INFO: grad norm: 302.103 290.301 83.618
2024-12-02-11:26:46-root-INFO: grad norm: 315.803 305.968 78.199
2024-12-02-11:26:47-root-INFO: grad norm: 360.299 347.096 96.644
2024-12-02-11:26:47-root-INFO: Loss too large (1819.445->1850.663)! Learning rate decreased to 0.00224.
2024-12-02-11:26:47-root-INFO: grad norm: 325.247 316.527 74.805
2024-12-02-11:26:48-root-INFO: Loss Change: 1903.963 -> 1765.174
2024-12-02-11:26:48-root-INFO: Regularization Change: 0.000 -> 2.059
2024-12-02-11:26:48-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-11:26:48-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-11:26:48-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-11:26:48-root-INFO: grad norm: 287.142 276.772 76.471
2024-12-02-11:26:48-root-INFO: Loss too large (1748.005->1865.865)! Learning rate decreased to 0.00291.
2024-12-02-11:26:48-root-INFO: Loss too large (1748.005->1782.678)! Learning rate decreased to 0.00233.
2024-12-02-11:26:49-root-INFO: grad norm: 330.232 322.516 70.971
2024-12-02-11:26:49-root-INFO: Loss too large (1728.971->1743.394)! Learning rate decreased to 0.00187.
2024-12-02-11:26:50-root-INFO: grad norm: 238.232 230.476 60.292
2024-12-02-11:26:50-root-INFO: grad norm: 164.266 159.688 38.508
2024-12-02-11:26:50-root-INFO: grad norm: 189.793 184.512 44.458
2024-12-02-11:26:51-root-INFO: Loss Change: 1748.005 -> 1653.301
2024-12-02-11:26:51-root-INFO: Regularization Change: 0.000 -> 0.835
2024-12-02-11:26:51-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-11:26:51-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-11:26:51-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-11:26:51-root-INFO: grad norm: 342.579 336.246 65.567
2024-12-02-11:26:51-root-INFO: Loss too large (1669.342->1783.746)! Learning rate decreased to 0.00304.
2024-12-02-11:26:51-root-INFO: Loss too large (1669.342->1747.647)! Learning rate decreased to 0.00243.
2024-12-02-11:26:51-root-INFO: Loss too large (1669.342->1711.537)! Learning rate decreased to 0.00194.
2024-12-02-11:26:52-root-INFO: Loss too large (1669.342->1677.766)! Learning rate decreased to 0.00155.
2024-12-02-11:26:52-root-INFO: grad norm: 244.347 237.687 56.662
2024-12-02-11:26:53-root-INFO: grad norm: 171.479 167.202 38.058
2024-12-02-11:26:53-root-INFO: grad norm: 184.022 179.387 41.040
2024-12-02-11:26:54-root-INFO: grad norm: 208.088 203.663 42.686
2024-12-02-11:26:54-root-INFO: Loss Change: 1669.342 -> 1601.075
2024-12-02-11:26:54-root-INFO: Regularization Change: 0.000 -> 0.501
2024-12-02-11:26:54-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-11:26:54-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:26:54-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-11:26:54-root-INFO: grad norm: 172.440 168.335 37.401
2024-12-02-11:26:54-root-INFO: Loss too large (1577.884->1693.661)! Learning rate decreased to 0.00316.
2024-12-02-11:26:55-root-INFO: Loss too large (1577.884->1648.403)! Learning rate decreased to 0.00253.
2024-12-02-11:26:55-root-INFO: Loss too large (1577.884->1614.894)! Learning rate decreased to 0.00202.
2024-12-02-11:26:55-root-INFO: Loss too large (1577.884->1591.954)! Learning rate decreased to 0.00162.
2024-12-02-11:26:55-root-INFO: grad norm: 231.784 227.188 45.929
2024-12-02-11:26:55-root-INFO: Loss too large (1577.654->1584.579)! Learning rate decreased to 0.00129.
2024-12-02-11:26:56-root-INFO: grad norm: 207.741 202.647 45.725
2024-12-02-11:26:56-root-INFO: grad norm: 190.115 186.159 38.581
2024-12-02-11:26:57-root-INFO: grad norm: 184.721 180.221 40.526
2024-12-02-11:26:57-root-INFO: Loss Change: 1577.884 -> 1548.196
2024-12-02-11:26:57-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-02-11:26:57-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-11:26:57-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:26:57-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-11:26:58-root-INFO: grad norm: 222.222 218.025 42.987
2024-12-02-11:26:58-root-INFO: Loss too large (1549.716->1677.116)! Learning rate decreased to 0.00329.
2024-12-02-11:26:58-root-INFO: Loss too large (1549.716->1644.716)! Learning rate decreased to 0.00263.
2024-12-02-11:26:58-root-INFO: Loss too large (1549.716->1612.966)! Learning rate decreased to 0.00211.
2024-12-02-11:26:58-root-INFO: Loss too large (1549.716->1584.277)! Learning rate decreased to 0.00168.
2024-12-02-11:26:58-root-INFO: Loss too large (1549.716->1561.300)! Learning rate decreased to 0.00135.
2024-12-02-11:26:59-root-INFO: grad norm: 221.706 216.170 49.236
2024-12-02-11:26:59-root-INFO: grad norm: 217.204 213.122 41.912
2024-12-02-11:27:00-root-INFO: grad norm: 213.491 208.093 47.706
2024-12-02-11:27:00-root-INFO: grad norm: 208.420 204.427 40.601
2024-12-02-11:27:00-root-INFO: Loss Change: 1549.716 -> 1522.752
2024-12-02-11:27:00-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-02-11:27:00-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-11:27:00-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-11:27:01-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-11:27:01-root-INFO: grad norm: 97.084 94.362 22.830
2024-12-02-11:27:01-root-INFO: Loss too large (1503.253->1524.031)! Learning rate decreased to 0.00342.
2024-12-02-11:27:01-root-INFO: Loss too large (1503.253->1511.652)! Learning rate decreased to 0.00274.
2024-12-02-11:27:01-root-INFO: Loss too large (1503.253->1504.098)! Learning rate decreased to 0.00219.
2024-12-02-11:27:02-root-INFO: grad norm: 169.159 165.193 36.416
2024-12-02-11:27:02-root-INFO: Loss too large (1499.889->1525.096)! Learning rate decreased to 0.00175.
2024-12-02-11:27:02-root-INFO: Loss too large (1499.889->1508.287)! Learning rate decreased to 0.00140.
2024-12-02-11:27:02-root-INFO: grad norm: 192.187 187.395 42.646
2024-12-02-11:27:03-root-INFO: grad norm: 207.810 203.579 41.721
2024-12-02-11:27:03-root-INFO: grad norm: 215.766 210.119 49.041
2024-12-02-11:27:04-root-INFO: Loss Change: 1503.253 -> 1489.371
2024-12-02-11:27:04-root-INFO: Regularization Change: 0.000 -> 0.264
2024-12-02-11:27:04-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-11:27:04-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-11:27:04-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-11:27:04-root-INFO: grad norm: 259.252 254.388 49.988
2024-12-02-11:27:04-root-INFO: Loss too large (1495.566->1646.452)! Learning rate decreased to 0.00356.
2024-12-02-11:27:04-root-INFO: Loss too large (1495.566->1611.060)! Learning rate decreased to 0.00285.
2024-12-02-11:27:04-root-INFO: Loss too large (1495.566->1573.559)! Learning rate decreased to 0.00228.
2024-12-02-11:27:05-root-INFO: Loss too large (1495.566->1536.719)! Learning rate decreased to 0.00182.
2024-12-02-11:27:05-root-INFO: Loss too large (1495.566->1505.014)! Learning rate decreased to 0.00146.
2024-12-02-11:27:05-root-INFO: grad norm: 234.753 228.267 54.800
2024-12-02-11:27:06-root-INFO: grad norm: 218.534 214.043 44.076
2024-12-02-11:27:06-root-INFO: grad norm: 204.483 198.815 47.811
2024-12-02-11:27:07-root-INFO: grad norm: 194.549 190.281 40.525
2024-12-02-11:27:07-root-INFO: Loss Change: 1495.566 -> 1456.760
2024-12-02-11:27:07-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-02-11:27:07-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-11:27:07-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:27:07-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-11:27:07-root-INFO: grad norm: 104.004 101.273 23.679
2024-12-02-11:27:07-root-INFO: Loss too large (1439.454->1483.722)! Learning rate decreased to 0.00371.
2024-12-02-11:27:08-root-INFO: Loss too large (1439.454->1464.219)! Learning rate decreased to 0.00297.
2024-12-02-11:27:08-root-INFO: Loss too large (1439.454->1451.049)! Learning rate decreased to 0.00237.
2024-12-02-11:27:08-root-INFO: Loss too large (1439.454->1442.714)! Learning rate decreased to 0.00190.
2024-12-02-11:27:08-root-INFO: grad norm: 147.484 143.798 32.768
2024-12-02-11:27:08-root-INFO: Loss too large (1437.840->1442.615)! Learning rate decreased to 0.00152.
2024-12-02-11:27:09-root-INFO: grad norm: 161.775 157.225 38.097
2024-12-02-11:27:09-root-INFO: grad norm: 169.978 166.074 36.220
2024-12-02-11:27:10-root-INFO: grad norm: 177.291 172.153 42.371
2024-12-02-11:27:10-root-INFO: Loss Change: 1439.454 -> 1426.628
2024-12-02-11:27:10-root-INFO: Regularization Change: 0.000 -> 0.224
2024-12-02-11:27:10-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-11:27:10-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:27:10-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-11:27:10-root-INFO: grad norm: 246.540 241.528 49.460
2024-12-02-11:27:11-root-INFO: Loss too large (1439.518->1596.330)! Learning rate decreased to 0.00386.
2024-12-02-11:27:11-root-INFO: Loss too large (1439.518->1559.109)! Learning rate decreased to 0.00309.
2024-12-02-11:27:11-root-INFO: Loss too large (1439.518->1518.790)! Learning rate decreased to 0.00247.
2024-12-02-11:27:11-root-INFO: Loss too large (1439.518->1478.939)! Learning rate decreased to 0.00198.
2024-12-02-11:27:11-root-INFO: Loss too large (1439.518->1445.363)! Learning rate decreased to 0.00158.
2024-12-02-11:27:12-root-INFO: grad norm: 225.966 219.124 55.183
2024-12-02-11:27:12-root-INFO: grad norm: 212.882 208.230 44.258
2024-12-02-11:27:13-root-INFO: grad norm: 195.415 189.449 47.918
2024-12-02-11:27:13-root-INFO: grad norm: 186.155 181.704 40.462
2024-12-02-11:27:13-root-INFO: Loss Change: 1439.518 -> 1398.025
2024-12-02-11:27:13-root-INFO: Regularization Change: 0.000 -> 0.309
2024-12-02-11:27:13-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-11:27:13-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:27:14-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-11:27:14-root-INFO: grad norm: 118.034 114.887 27.076
2024-12-02-11:27:14-root-INFO: Loss too large (1388.830->1452.936)! Learning rate decreased to 0.00401.
2024-12-02-11:27:14-root-INFO: Loss too large (1388.830->1427.250)! Learning rate decreased to 0.00321.
2024-12-02-11:27:14-root-INFO: Loss too large (1388.830->1409.032)! Learning rate decreased to 0.00257.
2024-12-02-11:27:14-root-INFO: Loss too large (1388.830->1396.866)! Learning rate decreased to 0.00206.
2024-12-02-11:27:15-root-INFO: Loss too large (1388.830->1389.299)! Learning rate decreased to 0.00164.
2024-12-02-11:27:15-root-INFO: grad norm: 121.596 117.730 30.419
2024-12-02-11:27:15-root-INFO: grad norm: 128.456 124.898 30.024
2024-12-02-11:27:16-root-INFO: grad norm: 133.030 129.149 31.897
2024-12-02-11:27:16-root-INFO: grad norm: 139.152 135.223 32.831
2024-12-02-11:27:17-root-INFO: Loss Change: 1388.830 -> 1371.530
2024-12-02-11:27:17-root-INFO: Regularization Change: 0.000 -> 0.217
2024-12-02-11:27:17-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-11:27:17-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:27:17-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-11:27:17-root-INFO: grad norm: 194.085 189.269 42.969
2024-12-02-11:27:17-root-INFO: Loss too large (1382.182->1532.002)! Learning rate decreased to 0.00417.
2024-12-02-11:27:17-root-INFO: Loss too large (1382.182->1489.975)! Learning rate decreased to 0.00334.
2024-12-02-11:27:18-root-INFO: Loss too large (1382.182->1446.528)! Learning rate decreased to 0.00267.
2024-12-02-11:27:18-root-INFO: Loss too large (1382.182->1408.584)! Learning rate decreased to 0.00214.
2024-12-02-11:27:18-root-INFO: grad norm: 253.792 243.982 69.878
2024-12-02-11:27:18-root-INFO: Loss too large (1381.742->1398.270)! Learning rate decreased to 0.00171.
2024-12-02-11:27:19-root-INFO: grad norm: 198.436 193.290 44.895
2024-12-02-11:27:19-root-INFO: grad norm: 137.831 133.515 34.224
2024-12-02-11:27:20-root-INFO: grad norm: 127.221 122.811 33.205
2024-12-02-11:27:20-root-INFO: Loss Change: 1382.182 -> 1345.332
2024-12-02-11:27:20-root-INFO: Regularization Change: 0.000 -> 0.276
2024-12-02-11:27:20-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-11:27:20-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-11:27:20-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-11:27:20-root-INFO: grad norm: 65.790 63.705 16.433
2024-12-02-11:27:21-root-INFO: grad norm: 148.504 144.051 36.093
2024-12-02-11:27:21-root-INFO: Loss too large (1335.844->1464.125)! Learning rate decreased to 0.00434.
2024-12-02-11:27:21-root-INFO: Loss too large (1335.844->1421.655)! Learning rate decreased to 0.00347.
2024-12-02-11:27:21-root-INFO: Loss too large (1335.844->1383.663)! Learning rate decreased to 0.00278.
2024-12-02-11:27:21-root-INFO: Loss too large (1335.844->1355.164)! Learning rate decreased to 0.00222.
2024-12-02-11:27:22-root-INFO: Loss too large (1335.844->1337.183)! Learning rate decreased to 0.00178.
2024-12-02-11:27:22-root-INFO: grad norm: 139.473 135.248 34.068
2024-12-02-11:27:23-root-INFO: grad norm: 135.586 131.266 33.952
2024-12-02-11:27:23-root-INFO: grad norm: 130.390 126.556 31.387
2024-12-02-11:27:23-root-INFO: Loss Change: 1340.082 -> 1317.553
2024-12-02-11:27:23-root-INFO: Regularization Change: 0.000 -> 0.394
2024-12-02-11:27:23-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-11:27:23-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:27:24-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-11:27:24-root-INFO: grad norm: 179.368 174.426 41.815
2024-12-02-11:27:24-root-INFO: Loss too large (1324.750->1475.920)! Learning rate decreased to 0.00451.
2024-12-02-11:27:24-root-INFO: Loss too large (1324.750->1430.487)! Learning rate decreased to 0.00361.
2024-12-02-11:27:24-root-INFO: Loss too large (1324.750->1384.384)! Learning rate decreased to 0.00289.
2024-12-02-11:27:24-root-INFO: Loss too large (1324.750->1346.115)! Learning rate decreased to 0.00231.
2024-12-02-11:27:25-root-INFO: grad norm: 233.385 224.543 63.629
2024-12-02-11:27:25-root-INFO: Loss too large (1320.803->1338.327)! Learning rate decreased to 0.00185.
2024-12-02-11:27:25-root-INFO: grad norm: 182.963 177.935 42.598
2024-12-02-11:27:26-root-INFO: grad norm: 118.677 115.197 28.528
2024-12-02-11:27:26-root-INFO: grad norm: 110.085 105.881 30.130
2024-12-02-11:27:27-root-INFO: Loss Change: 1324.750 -> 1287.657
2024-12-02-11:27:27-root-INFO: Regularization Change: 0.000 -> 0.288
2024-12-02-11:27:27-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-11:27:27-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:27:27-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-11:27:27-root-INFO: grad norm: 68.501 66.746 15.404
2024-12-02-11:27:27-root-INFO: Loss too large (1284.564->1292.613)! Learning rate decreased to 0.00469.
2024-12-02-11:27:27-root-INFO: Loss too large (1284.564->1286.441)! Learning rate decreased to 0.00375.
2024-12-02-11:27:28-root-INFO: grad norm: 123.193 118.977 31.954
2024-12-02-11:27:28-root-INFO: Loss too large (1282.816->1318.437)! Learning rate decreased to 0.00300.
2024-12-02-11:27:28-root-INFO: Loss too large (1282.816->1295.545)! Learning rate decreased to 0.00240.
2024-12-02-11:27:29-root-INFO: grad norm: 176.645 170.590 45.856
2024-12-02-11:27:29-root-INFO: Loss too large (1282.228->1294.077)! Learning rate decreased to 0.00192.
2024-12-02-11:27:29-root-INFO: grad norm: 151.803 147.264 36.842
2024-12-02-11:27:30-root-INFO: grad norm: 117.268 113.886 27.958
2024-12-02-11:27:30-root-INFO: Loss Change: 1284.564 -> 1265.860
2024-12-02-11:27:30-root-INFO: Regularization Change: 0.000 -> 0.309
2024-12-02-11:27:30-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-11:27:30-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:27:30-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-11:27:31-root-INFO: grad norm: 157.590 152.486 39.785
2024-12-02-11:27:31-root-INFO: Loss too large (1274.483->1417.622)! Learning rate decreased to 0.00488.
2024-12-02-11:27:31-root-INFO: Loss too large (1274.483->1368.508)! Learning rate decreased to 0.00390.
2024-12-02-11:27:31-root-INFO: Loss too large (1274.483->1322.441)! Learning rate decreased to 0.00312.
2024-12-02-11:27:31-root-INFO: Loss too large (1274.483->1288.053)! Learning rate decreased to 0.00250.
2024-12-02-11:27:32-root-INFO: grad norm: 195.284 188.020 52.769
2024-12-02-11:27:32-root-INFO: Loss too large (1267.512->1280.144)! Learning rate decreased to 0.00200.
2024-12-02-11:27:32-root-INFO: grad norm: 154.339 149.348 38.932
2024-12-02-11:27:33-root-INFO: grad norm: 100.276 97.536 23.278
2024-12-02-11:27:33-root-INFO: grad norm: 91.527 87.472 26.942
2024-12-02-11:27:33-root-INFO: Loss Change: 1274.483 -> 1239.429
2024-12-02-11:27:33-root-INFO: Regularization Change: 0.000 -> 0.289
2024-12-02-11:27:33-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-11:27:33-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-11:27:34-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-11:27:34-root-INFO: grad norm: 47.962 46.193 12.905
2024-12-02-11:27:34-root-INFO: grad norm: 72.668 69.670 20.660
2024-12-02-11:27:35-root-INFO: Loss too large (1222.339->1263.255)! Learning rate decreased to 0.00507.
2024-12-02-11:27:35-root-INFO: Loss too large (1222.339->1242.938)! Learning rate decreased to 0.00405.
2024-12-02-11:27:35-root-INFO: Loss too large (1222.339->1230.492)! Learning rate decreased to 0.00324.
2024-12-02-11:27:35-root-INFO: Loss too large (1222.339->1223.528)! Learning rate decreased to 0.00259.
2024-12-02-11:27:35-root-INFO: grad norm: 105.153 102.380 23.990
2024-12-02-11:27:36-root-INFO: Loss too large (1219.982->1223.009)! Learning rate decreased to 0.00208.
2024-12-02-11:27:36-root-INFO: grad norm: 103.565 100.196 26.200
2024-12-02-11:27:37-root-INFO: grad norm: 102.022 99.374 23.094
2024-12-02-11:27:37-root-INFO: Loss Change: 1232.509 -> 1211.373
2024-12-02-11:27:37-root-INFO: Regularization Change: 0.000 -> 0.417
2024-12-02-11:27:37-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-11:27:37-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:27:37-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-11:27:37-root-INFO: grad norm: 145.586 141.006 36.230
2024-12-02-11:27:37-root-INFO: Loss too large (1224.812->1370.183)! Learning rate decreased to 0.00527.
2024-12-02-11:27:38-root-INFO: Loss too large (1224.812->1319.771)! Learning rate decreased to 0.00421.
2024-12-02-11:27:38-root-INFO: Loss too large (1224.812->1273.207)! Learning rate decreased to 0.00337.
2024-12-02-11:27:38-root-INFO: Loss too large (1224.812->1239.179)! Learning rate decreased to 0.00270.
2024-12-02-11:27:38-root-INFO: grad norm: 194.575 187.951 50.338
2024-12-02-11:27:38-root-INFO: Loss too large (1219.200->1235.166)! Learning rate decreased to 0.00216.
2024-12-02-11:27:39-root-INFO: Loss too large (1219.200->1219.544)! Learning rate decreased to 0.00173.
2024-12-02-11:27:39-root-INFO: grad norm: 120.142 115.919 31.576
2024-12-02-11:27:40-root-INFO: grad norm: 56.924 55.479 12.743
2024-12-02-11:27:40-root-INFO: grad norm: 48.058 45.407 15.741
2024-12-02-11:27:40-root-INFO: Loss Change: 1224.812 -> 1191.216
2024-12-02-11:27:40-root-INFO: Regularization Change: 0.000 -> 0.227
2024-12-02-11:27:40-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-11:27:40-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:27:41-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-11:27:41-root-INFO: grad norm: 65.011 61.830 20.087
2024-12-02-11:27:41-root-INFO: Loss too large (1189.249->1210.010)! Learning rate decreased to 0.00547.
2024-12-02-11:27:41-root-INFO: Loss too large (1189.249->1197.082)! Learning rate decreased to 0.00438.
2024-12-02-11:27:41-root-INFO: Loss too large (1189.249->1189.913)! Learning rate decreased to 0.00350.
2024-12-02-11:27:42-root-INFO: grad norm: 114.887 111.861 26.196
2024-12-02-11:27:42-root-INFO: Loss too large (1186.312->1199.815)! Learning rate decreased to 0.00280.
2024-12-02-11:27:42-root-INFO: Loss too large (1186.312->1190.351)! Learning rate decreased to 0.00224.
2024-12-02-11:27:42-root-INFO: grad norm: 106.508 103.046 26.937
2024-12-02-11:27:43-root-INFO: grad norm: 94.326 92.022 20.720
2024-12-02-11:27:43-root-INFO: grad norm: 90.755 87.604 23.707
2024-12-02-11:27:44-root-INFO: Loss Change: 1189.249 -> 1169.441
2024-12-02-11:27:44-root-INFO: Regularization Change: 0.000 -> 0.282
2024-12-02-11:27:44-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-11:27:44-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:27:44-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-11:27:44-root-INFO: grad norm: 48.636 46.639 13.795
2024-12-02-11:27:45-root-INFO: grad norm: 74.611 71.590 21.015
2024-12-02-11:27:45-root-INFO: Loss too large (1158.425->1209.724)! Learning rate decreased to 0.00568.
2024-12-02-11:27:45-root-INFO: Loss too large (1158.425->1184.126)! Learning rate decreased to 0.00455.
2024-12-02-11:27:45-root-INFO: Loss too large (1158.425->1168.546)! Learning rate decreased to 0.00364.
2024-12-02-11:27:45-root-INFO: Loss too large (1158.425->1159.925)! Learning rate decreased to 0.00291.
2024-12-02-11:27:46-root-INFO: grad norm: 105.968 103.378 23.284
2024-12-02-11:27:46-root-INFO: Loss too large (1155.591->1159.550)! Learning rate decreased to 0.00233.
2024-12-02-11:27:46-root-INFO: grad norm: 102.000 98.986 24.614
2024-12-02-11:27:47-root-INFO: grad norm: 96.508 94.248 20.762
2024-12-02-11:27:47-root-INFO: Loss Change: 1168.860 -> 1146.169
2024-12-02-11:27:47-root-INFO: Regularization Change: 0.000 -> 0.493
2024-12-02-11:27:47-root-INFO: Undo step: 160
2024-12-02-11:27:47-root-INFO: Undo step: 161
2024-12-02-11:27:47-root-INFO: Undo step: 162
2024-12-02-11:27:47-root-INFO: Undo step: 163
2024-12-02-11:27:47-root-INFO: Undo step: 164
2024-12-02-11:27:47-root-INFO: Undo step: 165
2024-12-02-11:27:47-root-INFO: Undo step: 166
2024-12-02-11:27:47-root-INFO: Undo step: 167
2024-12-02-11:27:47-root-INFO: Undo step: 168
2024-12-02-11:27:47-root-INFO: Undo step: 169
2024-12-02-11:27:47-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-11:27:47-root-INFO: grad norm: 732.701 711.334 175.655
2024-12-02-11:27:48-root-INFO: grad norm: 972.215 943.004 236.530
2024-12-02-11:27:48-root-INFO: grad norm: 497.463 484.754 111.727
2024-12-02-11:27:49-root-INFO: grad norm: 439.770 431.693 83.896
2024-12-02-11:27:49-root-INFO: grad norm: 442.228 436.923 68.290
2024-12-02-11:27:50-root-INFO: Loss Change: 3246.264 -> 1962.742
2024-12-02-11:27:50-root-INFO: Regularization Change: 0.000 -> 30.984
2024-12-02-11:27:50-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-11:27:50-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:27:50-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-11:27:50-root-INFO: grad norm: 373.088 368.117 60.697
2024-12-02-11:27:50-root-INFO: grad norm: 397.020 392.176 61.827
2024-12-02-11:27:51-root-INFO: Loss too large (1790.035->1827.858)! Learning rate decreased to 0.00401.
2024-12-02-11:27:51-root-INFO: grad norm: 207.094 202.652 42.663
2024-12-02-11:27:52-root-INFO: grad norm: 184.190 180.662 35.879
2024-12-02-11:27:52-root-INFO: grad norm: 236.490 233.759 35.839
2024-12-02-11:27:52-root-INFO: Loss too large (1582.585->1644.992)! Learning rate decreased to 0.00321.
2024-12-02-11:27:52-root-INFO: Loss too large (1582.585->1605.842)! Learning rate decreased to 0.00257.
2024-12-02-11:27:53-root-INFO: Loss Change: 1952.246 -> 1574.536
2024-12-02-11:27:53-root-INFO: Regularization Change: 0.000 -> 4.388
2024-12-02-11:27:53-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-11:27:53-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-11:27:53-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-11:27:53-root-INFO: grad norm: 252.927 249.408 42.043
2024-12-02-11:27:53-root-INFO: Loss too large (1556.725->1769.732)! Learning rate decreased to 0.00417.
2024-12-02-11:27:53-root-INFO: Loss too large (1556.725->1680.155)! Learning rate decreased to 0.00334.
2024-12-02-11:27:53-root-INFO: Loss too large (1556.725->1614.860)! Learning rate decreased to 0.00267.
2024-12-02-11:27:54-root-INFO: Loss too large (1556.725->1568.791)! Learning rate decreased to 0.00214.
2024-12-02-11:27:54-root-INFO: grad norm: 239.930 236.734 39.030
2024-12-02-11:27:55-root-INFO: grad norm: 256.850 253.330 42.378
2024-12-02-11:27:55-root-INFO: Loss too large (1523.869->1524.075)! Learning rate decreased to 0.00171.
2024-12-02-11:27:55-root-INFO: grad norm: 205.606 202.879 33.379
2024-12-02-11:27:56-root-INFO: grad norm: 190.182 187.238 33.334
2024-12-02-11:27:56-root-INFO: Loss Change: 1556.725 -> 1478.570
2024-12-02-11:27:56-root-INFO: Regularization Change: 0.000 -> 0.690
2024-12-02-11:27:56-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-11:27:56-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-11:27:56-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-11:27:56-root-INFO: grad norm: 236.424 233.431 37.500
2024-12-02-11:27:56-root-INFO: Loss too large (1484.027->1655.248)! Learning rate decreased to 0.00434.
2024-12-02-11:27:57-root-INFO: Loss too large (1484.027->1615.234)! Learning rate decreased to 0.00347.
2024-12-02-11:27:57-root-INFO: Loss too large (1484.027->1570.002)! Learning rate decreased to 0.00278.
2024-12-02-11:27:57-root-INFO: Loss too large (1484.027->1524.858)! Learning rate decreased to 0.00222.
2024-12-02-11:27:57-root-INFO: Loss too large (1484.027->1487.672)! Learning rate decreased to 0.00178.
2024-12-02-11:27:58-root-INFO: grad norm: 233.297 230.509 35.957
2024-12-02-11:27:58-root-INFO: Loss too large (1463.593->1464.051)! Learning rate decreased to 0.00142.
2024-12-02-11:27:58-root-INFO: grad norm: 180.315 177.936 29.199
2024-12-02-11:27:59-root-INFO: grad norm: 143.057 140.666 26.042
2024-12-02-11:27:59-root-INFO: grad norm: 132.509 130.313 24.025
2024-12-02-11:27:59-root-INFO: Loss Change: 1484.027 -> 1419.815
2024-12-02-11:27:59-root-INFO: Regularization Change: 0.000 -> 0.377
2024-12-02-11:27:59-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-11:27:59-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:28:00-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-11:28:00-root-INFO: grad norm: 76.382 73.658 20.215
2024-12-02-11:28:00-root-INFO: grad norm: 125.577 123.905 20.425
2024-12-02-11:28:00-root-INFO: Loss too large (1386.660->1506.007)! Learning rate decreased to 0.00451.
2024-12-02-11:28:00-root-INFO: Loss too large (1386.660->1469.466)! Learning rate decreased to 0.00361.
2024-12-02-11:28:01-root-INFO: Loss too large (1386.660->1436.508)! Learning rate decreased to 0.00289.
2024-12-02-11:28:01-root-INFO: Loss too large (1386.660->1411.264)! Learning rate decreased to 0.00231.
2024-12-02-11:28:01-root-INFO: Loss too large (1386.660->1394.704)! Learning rate decreased to 0.00185.
2024-12-02-11:28:01-root-INFO: grad norm: 199.032 196.768 29.931
2024-12-02-11:28:02-root-INFO: Loss too large (1385.221->1399.278)! Learning rate decreased to 0.00148.
2024-12-02-11:28:02-root-INFO: Loss too large (1385.221->1385.918)! Learning rate decreased to 0.00118.
2024-12-02-11:28:02-root-INFO: grad norm: 149.139 147.421 22.575
2024-12-02-11:28:03-root-INFO: grad norm: 104.710 102.719 20.323
2024-12-02-11:28:03-root-INFO: Loss Change: 1408.651 -> 1363.201
2024-12-02-11:28:03-root-INFO: Regularization Change: 0.000 -> 0.527
2024-12-02-11:28:03-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-11:28:03-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:28:03-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-11:28:03-root-INFO: grad norm: 138.145 136.328 22.333
2024-12-02-11:28:03-root-INFO: Loss too large (1364.712->1519.799)! Learning rate decreased to 0.00469.
2024-12-02-11:28:04-root-INFO: Loss too large (1364.712->1481.276)! Learning rate decreased to 0.00375.
2024-12-02-11:28:04-root-INFO: Loss too large (1364.712->1441.892)! Learning rate decreased to 0.00300.
2024-12-02-11:28:04-root-INFO: Loss too large (1364.712->1407.826)! Learning rate decreased to 0.00240.
2024-12-02-11:28:04-root-INFO: Loss too large (1364.712->1383.204)! Learning rate decreased to 0.00192.
2024-12-02-11:28:04-root-INFO: Loss too large (1364.712->1368.058)! Learning rate decreased to 0.00154.
2024-12-02-11:28:05-root-INFO: grad norm: 171.909 170.009 25.488
2024-12-02-11:28:05-root-INFO: Loss too large (1359.980->1362.034)! Learning rate decreased to 0.00123.
2024-12-02-11:28:05-root-INFO: grad norm: 144.955 143.294 21.882
2024-12-02-11:28:06-root-INFO: grad norm: 115.033 113.277 20.019
2024-12-02-11:28:06-root-INFO: grad norm: 104.594 102.967 18.374
2024-12-02-11:28:07-root-INFO: Loss Change: 1364.712 -> 1337.404
2024-12-02-11:28:07-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-02-11:28:07-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-11:28:07-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-11:28:07-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-11:28:07-root-INFO: grad norm: 70.028 67.814 17.468
2024-12-02-11:28:07-root-INFO: grad norm: 236.022 233.854 31.915
2024-12-02-11:28:08-root-INFO: Loss too large (1325.480->1632.325)! Learning rate decreased to 0.00488.
2024-12-02-11:28:08-root-INFO: Loss too large (1325.480->1544.038)! Learning rate decreased to 0.00390.
2024-12-02-11:28:08-root-INFO: Loss too large (1325.480->1477.495)! Learning rate decreased to 0.00312.
2024-12-02-11:28:08-root-INFO: Loss too large (1325.480->1426.825)! Learning rate decreased to 0.00250.
2024-12-02-11:28:08-root-INFO: Loss too large (1325.480->1388.249)! Learning rate decreased to 0.00200.
2024-12-02-11:28:08-root-INFO: Loss too large (1325.480->1359.407)! Learning rate decreased to 0.00160.
2024-12-02-11:28:09-root-INFO: Loss too large (1325.480->1338.678)! Learning rate decreased to 0.00128.
2024-12-02-11:28:09-root-INFO: grad norm: 191.432 189.909 24.104
2024-12-02-11:28:09-root-INFO: grad norm: 129.021 127.434 20.174
2024-12-02-11:28:10-root-INFO: grad norm: 121.034 119.604 18.549
2024-12-02-11:28:10-root-INFO: Loss Change: 1332.963 -> 1299.508
2024-12-02-11:28:10-root-INFO: Regularization Change: 0.000 -> 0.415
2024-12-02-11:28:10-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-11:28:10-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-11:28:10-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-11:28:11-root-INFO: grad norm: 57.118 55.238 14.535
2024-12-02-11:28:11-root-INFO: grad norm: 123.700 122.383 17.998
2024-12-02-11:28:11-root-INFO: Loss too large (1279.481->1449.647)! Learning rate decreased to 0.00507.
2024-12-02-11:28:11-root-INFO: Loss too large (1279.481->1410.748)! Learning rate decreased to 0.00405.
2024-12-02-11:28:12-root-INFO: Loss too large (1279.481->1369.168)! Learning rate decreased to 0.00324.
2024-12-02-11:28:12-root-INFO: Loss too large (1279.481->1331.927)! Learning rate decreased to 0.00259.
2024-12-02-11:28:12-root-INFO: Loss too large (1279.481->1304.520)! Learning rate decreased to 0.00208.
2024-12-02-11:28:12-root-INFO: Loss too large (1279.481->1287.491)! Learning rate decreased to 0.00166.
2024-12-02-11:28:12-root-INFO: grad norm: 181.195 179.498 24.734
2024-12-02-11:28:13-root-INFO: Loss too large (1278.236->1287.348)! Learning rate decreased to 0.00133.
2024-12-02-11:28:13-root-INFO: grad norm: 159.923 158.520 21.136
2024-12-02-11:28:13-root-INFO: grad norm: 127.936 126.468 19.329
2024-12-02-11:28:14-root-INFO: Loss Change: 1291.526 -> 1266.074
2024-12-02-11:28:14-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-11:28:14-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-11:28:14-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:28:14-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-11:28:14-root-INFO: grad norm: 178.088 176.199 25.869
2024-12-02-11:28:14-root-INFO: Loss too large (1276.986->1496.519)! Learning rate decreased to 0.00527.
2024-12-02-11:28:14-root-INFO: Loss too large (1276.986->1463.483)! Learning rate decreased to 0.00421.
2024-12-02-11:28:15-root-INFO: Loss too large (1276.986->1421.580)! Learning rate decreased to 0.00337.
2024-12-02-11:28:15-root-INFO: Loss too large (1276.986->1372.702)! Learning rate decreased to 0.00270.
2024-12-02-11:28:15-root-INFO: Loss too large (1276.986->1325.691)! Learning rate decreased to 0.00216.
2024-12-02-11:28:15-root-INFO: Loss too large (1276.986->1290.822)! Learning rate decreased to 0.00173.
2024-12-02-11:28:15-root-INFO: grad norm: 239.745 237.951 29.274
2024-12-02-11:28:16-root-INFO: Loss too large (1270.722->1288.312)! Learning rate decreased to 0.00138.
2024-12-02-11:28:16-root-INFO: Loss too large (1270.722->1273.033)! Learning rate decreased to 0.00110.
2024-12-02-11:28:16-root-INFO: grad norm: 151.372 149.795 21.788
2024-12-02-11:28:17-root-INFO: grad norm: 68.052 66.629 13.841
2024-12-02-11:28:17-root-INFO: grad norm: 57.706 56.025 13.829
2024-12-02-11:28:18-root-INFO: Loss Change: 1276.986 -> 1245.131
2024-12-02-11:28:18-root-INFO: Regularization Change: 0.000 -> 0.134
2024-12-02-11:28:18-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-11:28:18-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:28:18-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-11:28:18-root-INFO: grad norm: 82.760 81.166 16.167
2024-12-02-11:28:18-root-INFO: Loss too large (1244.378->1332.131)! Learning rate decreased to 0.00547.
2024-12-02-11:28:18-root-INFO: Loss too large (1244.378->1297.463)! Learning rate decreased to 0.00438.
2024-12-02-11:28:18-root-INFO: Loss too large (1244.378->1272.279)! Learning rate decreased to 0.00350.
2024-12-02-11:28:18-root-INFO: Loss too large (1244.378->1256.384)! Learning rate decreased to 0.00280.
2024-12-02-11:28:19-root-INFO: Loss too large (1244.378->1247.345)! Learning rate decreased to 0.00224.
2024-12-02-11:28:19-root-INFO: grad norm: 155.857 154.404 21.226
2024-12-02-11:28:19-root-INFO: Loss too large (1242.658->1259.347)! Learning rate decreased to 0.00179.
2024-12-02-11:28:19-root-INFO: Loss too large (1242.658->1248.625)! Learning rate decreased to 0.00143.
2024-12-02-11:28:20-root-INFO: grad norm: 138.209 136.890 19.055
2024-12-02-11:28:20-root-INFO: grad norm: 110.087 108.798 16.794
2024-12-02-11:28:21-root-INFO: grad norm: 103.896 102.624 16.207
2024-12-02-11:28:21-root-INFO: Loss Change: 1244.378 -> 1225.598
2024-12-02-11:28:21-root-INFO: Regularization Change: 0.000 -> 0.165
2024-12-02-11:28:21-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-11:28:21-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:28:21-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-11:28:21-root-INFO: grad norm: 54.043 52.151 14.174
2024-12-02-11:28:22-root-INFO: grad norm: 54.225 52.818 12.274
2024-12-02-11:28:22-root-INFO: grad norm: 154.079 152.597 21.322
2024-12-02-11:28:23-root-INFO: Loss too large (1205.965->1428.968)! Learning rate decreased to 0.00568.
2024-12-02-11:28:23-root-INFO: Loss too large (1205.965->1393.356)! Learning rate decreased to 0.00455.
2024-12-02-11:28:23-root-INFO: Loss too large (1205.965->1348.039)! Learning rate decreased to 0.00364.
2024-12-02-11:28:23-root-INFO: Loss too large (1205.965->1296.333)! Learning rate decreased to 0.00291.
2024-12-02-11:28:23-root-INFO: Loss too large (1205.965->1249.506)! Learning rate decreased to 0.00233.
2024-12-02-11:28:23-root-INFO: Loss too large (1205.965->1217.479)! Learning rate decreased to 0.00186.
2024-12-02-11:28:24-root-INFO: grad norm: 205.711 204.211 24.803
2024-12-02-11:28:24-root-INFO: Loss too large (1200.304->1215.710)! Learning rate decreased to 0.00149.
2024-12-02-11:28:24-root-INFO: Loss too large (1200.304->1203.316)! Learning rate decreased to 0.00119.
2024-12-02-11:28:25-root-INFO: grad norm: 130.712 129.440 18.193
2024-12-02-11:28:25-root-INFO: Loss Change: 1223.421 -> 1185.340
2024-12-02-11:28:25-root-INFO: Regularization Change: 0.000 -> 0.713
2024-12-02-11:28:25-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-11:28:25-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:28:25-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-11:28:25-root-INFO: grad norm: 59.527 57.947 13.624
2024-12-02-11:28:25-root-INFO: Loss too large (1182.892->1210.414)! Learning rate decreased to 0.00590.
2024-12-02-11:28:25-root-INFO: Loss too large (1182.892->1195.203)! Learning rate decreased to 0.00472.
2024-12-02-11:28:26-root-INFO: Loss too large (1182.892->1186.594)! Learning rate decreased to 0.00378.
2024-12-02-11:28:26-root-INFO: grad norm: 167.104 165.735 21.349
2024-12-02-11:28:26-root-INFO: Loss too large (1182.089->1246.241)! Learning rate decreased to 0.00302.
2024-12-02-11:28:26-root-INFO: Loss too large (1182.089->1222.086)! Learning rate decreased to 0.00242.
2024-12-02-11:28:27-root-INFO: Loss too large (1182.089->1204.242)! Learning rate decreased to 0.00193.
2024-12-02-11:28:27-root-INFO: Loss too large (1182.089->1191.544)! Learning rate decreased to 0.00155.
2024-12-02-11:28:27-root-INFO: Loss too large (1182.089->1182.998)! Learning rate decreased to 0.00124.
2024-12-02-11:28:27-root-INFO: grad norm: 108.834 107.678 15.823
2024-12-02-11:28:28-root-INFO: grad norm: 53.929 52.510 12.293
2024-12-02-11:28:28-root-INFO: grad norm: 46.776 45.212 11.994
2024-12-02-11:28:29-root-INFO: Loss Change: 1182.892 -> 1165.881
2024-12-02-11:28:29-root-INFO: Regularization Change: 0.000 -> 0.146
2024-12-02-11:28:29-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-11:28:29-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-11:28:29-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-11:28:29-root-INFO: grad norm: 75.248 73.742 14.981
2024-12-02-11:28:29-root-INFO: Loss too large (1166.989->1256.312)! Learning rate decreased to 0.00613.
2024-12-02-11:28:29-root-INFO: Loss too large (1166.989->1219.543)! Learning rate decreased to 0.00490.
2024-12-02-11:28:29-root-INFO: Loss too large (1166.989->1193.712)! Learning rate decreased to 0.00392.
2024-12-02-11:28:30-root-INFO: Loss too large (1166.989->1178.013)! Learning rate decreased to 0.00314.
2024-12-02-11:28:30-root-INFO: Loss too large (1166.989->1169.391)! Learning rate decreased to 0.00251.
2024-12-02-11:28:30-root-INFO: grad norm: 141.899 140.681 18.554
2024-12-02-11:28:30-root-INFO: Loss too large (1165.062->1180.322)! Learning rate decreased to 0.00201.
2024-12-02-11:28:30-root-INFO: Loss too large (1165.062->1170.827)! Learning rate decreased to 0.00161.
2024-12-02-11:28:31-root-INFO: grad norm: 122.759 121.547 17.211
2024-12-02-11:28:31-root-INFO: grad norm: 89.972 88.849 14.172
2024-12-02-11:28:32-root-INFO: grad norm: 84.543 83.333 14.250
2024-12-02-11:28:32-root-INFO: Loss Change: 1166.989 -> 1149.340
2024-12-02-11:28:32-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-02-11:28:32-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-11:28:32-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:28:32-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-11:28:32-root-INFO: grad norm: 51.670 49.748 13.964
2024-12-02-11:28:33-root-INFO: grad norm: 177.481 176.201 21.273
2024-12-02-11:28:33-root-INFO: Loss too large (1141.427->1357.589)! Learning rate decreased to 0.00636.
2024-12-02-11:28:33-root-INFO: Loss too large (1141.427->1293.946)! Learning rate decreased to 0.00509.
2024-12-02-11:28:33-root-INFO: Loss too large (1141.427->1246.959)! Learning rate decreased to 0.00407.
2024-12-02-11:28:34-root-INFO: Loss too large (1141.427->1211.924)! Learning rate decreased to 0.00326.
2024-12-02-11:28:34-root-INFO: Loss too large (1141.427->1185.713)! Learning rate decreased to 0.00261.
2024-12-02-11:28:34-root-INFO: Loss too large (1141.427->1166.308)! Learning rate decreased to 0.00208.
2024-12-02-11:28:34-root-INFO: Loss too large (1141.427->1152.343)! Learning rate decreased to 0.00167.
2024-12-02-11:28:34-root-INFO: Loss too large (1141.427->1142.765)! Learning rate decreased to 0.00133.
2024-12-02-11:28:35-root-INFO: grad norm: 109.475 108.334 15.762
2024-12-02-11:28:35-root-INFO: grad norm: 46.017 44.584 11.397
2024-12-02-11:28:36-root-INFO: grad norm: 41.790 40.203 11.407
2024-12-02-11:28:36-root-INFO: Loss Change: 1148.262 -> 1123.631
2024-12-02-11:28:36-root-INFO: Regularization Change: 0.000 -> 0.336
2024-12-02-11:28:36-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-11:28:36-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:28:36-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-11:28:36-root-INFO: grad norm: 62.862 61.409 13.435
2024-12-02-11:28:36-root-INFO: Loss too large (1123.599->1193.558)! Learning rate decreased to 0.00660.
2024-12-02-11:28:37-root-INFO: Loss too large (1123.599->1161.911)! Learning rate decreased to 0.00528.
2024-12-02-11:28:37-root-INFO: Loss too large (1123.599->1141.760)! Learning rate decreased to 0.00423.
2024-12-02-11:28:37-root-INFO: Loss too large (1123.599->1130.333)! Learning rate decreased to 0.00338.
2024-12-02-11:28:37-root-INFO: Loss too large (1123.599->1124.354)! Learning rate decreased to 0.00270.
2024-12-02-11:28:37-root-INFO: grad norm: 111.506 110.399 15.670
2024-12-02-11:28:38-root-INFO: Loss too large (1121.481->1129.434)! Learning rate decreased to 0.00216.
2024-12-02-11:28:38-root-INFO: Loss too large (1121.481->1123.480)! Learning rate decreased to 0.00173.
2024-12-02-11:28:38-root-INFO: grad norm: 94.595 93.484 14.453
2024-12-02-11:28:39-root-INFO: grad norm: 68.975 67.811 12.620
2024-12-02-11:28:39-root-INFO: grad norm: 63.122 61.892 12.399
2024-12-02-11:28:39-root-INFO: Loss Change: 1123.599 -> 1107.672
2024-12-02-11:28:39-root-INFO: Regularization Change: 0.000 -> 0.156
2024-12-02-11:28:39-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-11:28:39-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:28:40-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-11:28:40-root-INFO: grad norm: 52.279 50.676 12.846
2024-12-02-11:28:40-root-INFO: Loss too large (1107.314->1121.277)! Learning rate decreased to 0.00685.
2024-12-02-11:28:40-root-INFO: Loss too large (1107.314->1111.297)! Learning rate decreased to 0.00548.
2024-12-02-11:28:41-root-INFO: grad norm: 172.676 171.449 20.552
2024-12-02-11:28:41-root-INFO: Loss too large (1106.173->1204.837)! Learning rate decreased to 0.00439.
2024-12-02-11:28:41-root-INFO: Loss too large (1106.173->1171.869)! Learning rate decreased to 0.00351.
2024-12-02-11:28:41-root-INFO: Loss too large (1106.173->1147.323)! Learning rate decreased to 0.00281.
2024-12-02-11:28:41-root-INFO: Loss too large (1106.173->1129.165)! Learning rate decreased to 0.00225.
2024-12-02-11:28:42-root-INFO: Loss too large (1106.173->1116.054)! Learning rate decreased to 0.00180.
2024-12-02-11:28:42-root-INFO: Loss too large (1106.173->1107.004)! Learning rate decreased to 0.00144.
2024-12-02-11:28:42-root-INFO: grad norm: 99.997 98.913 14.682
2024-12-02-11:28:43-root-INFO: grad norm: 39.724 38.125 11.156
2024-12-02-11:28:43-root-INFO: grad norm: 38.372 36.739 11.074
2024-12-02-11:28:44-root-INFO: Loss Change: 1107.314 -> 1088.550
2024-12-02-11:28:44-root-INFO: Regularization Change: 0.000 -> 0.210
2024-12-02-11:28:44-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-11:28:44-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-11:28:44-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-11:28:44-root-INFO: grad norm: 69.022 67.610 13.887
2024-12-02-11:28:44-root-INFO: Loss too large (1090.546->1189.331)! Learning rate decreased to 0.00711.
2024-12-02-11:28:44-root-INFO: Loss too large (1090.546->1147.194)! Learning rate decreased to 0.00569.
2024-12-02-11:28:44-root-INFO: Loss too large (1090.546->1118.077)! Learning rate decreased to 0.00455.
2024-12-02-11:28:44-root-INFO: Loss too large (1090.546->1101.131)! Learning rate decreased to 0.00364.
2024-12-02-11:28:45-root-INFO: Loss too large (1090.546->1092.285)! Learning rate decreased to 0.00291.
2024-12-02-11:28:45-root-INFO: grad norm: 119.072 117.989 16.022
2024-12-02-11:28:45-root-INFO: Loss too large (1088.072->1096.663)! Learning rate decreased to 0.00233.
2024-12-02-11:28:45-root-INFO: Loss too large (1088.072->1090.073)! Learning rate decreased to 0.00186.
2024-12-02-11:28:46-root-INFO: grad norm: 92.248 91.127 14.343
2024-12-02-11:28:46-root-INFO: grad norm: 55.127 53.844 11.823
2024-12-02-11:28:47-root-INFO: grad norm: 49.438 48.033 11.704
2024-12-02-11:28:47-root-INFO: Loss Change: 1090.546 -> 1072.373
2024-12-02-11:28:47-root-INFO: Regularization Change: 0.000 -> 0.181
2024-12-02-11:28:47-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-11:28:47-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-11:28:47-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-11:28:48-root-INFO: grad norm: 58.197 56.591 13.577
2024-12-02-11:28:48-root-INFO: Loss too large (1072.579->1111.229)! Learning rate decreased to 0.00738.
2024-12-02-11:28:48-root-INFO: Loss too large (1072.579->1089.515)! Learning rate decreased to 0.00590.
2024-12-02-11:28:48-root-INFO: Loss too large (1072.579->1077.566)! Learning rate decreased to 0.00472.
2024-12-02-11:28:48-root-INFO: grad norm: 156.359 155.226 18.792
2024-12-02-11:28:49-root-INFO: Loss too large (1071.562->1121.476)! Learning rate decreased to 0.00378.
2024-12-02-11:28:49-root-INFO: Loss too large (1071.562->1101.166)! Learning rate decreased to 0.00302.
2024-12-02-11:28:49-root-INFO: Loss too large (1071.562->1086.454)! Learning rate decreased to 0.00242.
2024-12-02-11:28:49-root-INFO: Loss too large (1071.562->1076.097)! Learning rate decreased to 0.00193.
2024-12-02-11:28:49-root-INFO: grad norm: 101.694 100.623 14.717
2024-12-02-11:28:50-root-INFO: grad norm: 39.781 38.193 11.128
2024-12-02-11:28:50-root-INFO: grad norm: 38.110 36.489 10.998
2024-12-02-11:28:51-root-INFO: Loss Change: 1072.579 -> 1051.972
2024-12-02-11:28:51-root-INFO: Regularization Change: 0.000 -> 0.250
2024-12-02-11:28:51-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-11:28:51-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-11:28:51-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-11:28:51-root-INFO: grad norm: 52.023 50.550 12.289
2024-12-02-11:28:51-root-INFO: Loss too large (1051.850->1080.673)! Learning rate decreased to 0.00765.
2024-12-02-11:28:51-root-INFO: Loss too large (1051.850->1063.416)! Learning rate decreased to 0.00612.
2024-12-02-11:28:52-root-INFO: Loss too large (1051.850->1054.354)! Learning rate decreased to 0.00490.
2024-12-02-11:28:52-root-INFO: grad norm: 128.225 127.207 16.120
2024-12-02-11:28:52-root-INFO: Loss too large (1049.986->1082.791)! Learning rate decreased to 0.00392.
2024-12-02-11:28:52-root-INFO: Loss too large (1049.986->1068.105)! Learning rate decreased to 0.00314.
2024-12-02-11:28:53-root-INFO: Loss too large (1049.986->1057.729)! Learning rate decreased to 0.00251.
2024-12-02-11:28:53-root-INFO: Loss too large (1049.986->1050.691)! Learning rate decreased to 0.00201.
2024-12-02-11:28:53-root-INFO: grad norm: 81.957 80.931 12.929
2024-12-02-11:28:54-root-INFO: grad norm: 38.433 36.932 10.635
2024-12-02-11:28:54-root-INFO: grad norm: 36.613 35.071 10.514
2024-12-02-11:28:55-root-INFO: Loss Change: 1051.850 -> 1033.036
2024-12-02-11:28:55-root-INFO: Regularization Change: 0.000 -> 0.237
2024-12-02-11:28:55-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-11:28:55-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-11:28:55-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-11:28:55-root-INFO: grad norm: 57.057 55.635 12.663
2024-12-02-11:28:55-root-INFO: Loss too large (1034.382->1075.708)! Learning rate decreased to 0.00794.
2024-12-02-11:28:55-root-INFO: Loss too large (1034.382->1051.825)! Learning rate decreased to 0.00635.
2024-12-02-11:28:55-root-INFO: Loss too large (1034.382->1039.068)! Learning rate decreased to 0.00508.
2024-12-02-11:28:56-root-INFO: grad norm: 138.104 137.107 16.558
2024-12-02-11:28:56-root-INFO: Loss too large (1032.866->1068.114)! Learning rate decreased to 0.00406.
2024-12-02-11:28:56-root-INFO: Loss too large (1032.866->1052.027)! Learning rate decreased to 0.00325.
2024-12-02-11:28:56-root-INFO: Loss too large (1032.866->1040.675)! Learning rate decreased to 0.00260.
2024-12-02-11:28:56-root-INFO: Loss too large (1032.866->1032.950)! Learning rate decreased to 0.00208.
2024-12-02-11:28:57-root-INFO: grad norm: 80.293 79.260 12.836
2024-12-02-11:28:57-root-INFO: grad norm: 35.970 34.426 10.427
2024-12-02-11:28:58-root-INFO: grad norm: 35.465 33.943 10.279
2024-12-02-11:28:58-root-INFO: Loss Change: 1034.382 -> 1014.224
2024-12-02-11:28:58-root-INFO: Regularization Change: 0.000 -> 0.259
2024-12-02-11:28:58-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-11:28:58-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:28:58-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-11:28:58-root-INFO: grad norm: 46.621 45.221 11.341
2024-12-02-11:28:59-root-INFO: Loss too large (1014.800->1025.511)! Learning rate decreased to 0.00823.
2024-12-02-11:28:59-root-INFO: Loss too large (1014.800->1016.374)! Learning rate decreased to 0.00659.
2024-12-02-11:28:59-root-INFO: grad norm: 123.334 122.432 14.891
2024-12-02-11:28:59-root-INFO: Loss too large (1012.087->1057.159)! Learning rate decreased to 0.00527.
2024-12-02-11:29:00-root-INFO: Loss too large (1012.087->1038.556)! Learning rate decreased to 0.00422.
2024-12-02-11:29:00-root-INFO: Loss too large (1012.087->1025.384)! Learning rate decreased to 0.00337.
2024-12-02-11:29:00-root-INFO: Loss too large (1012.087->1016.271)! Learning rate decreased to 0.00270.
2024-12-02-11:29:00-root-INFO: grad norm: 84.927 84.008 12.456
2024-12-02-11:29:01-root-INFO: grad norm: 37.106 35.776 9.845
2024-12-02-11:29:01-root-INFO: grad norm: 35.486 34.113 9.775
2024-12-02-11:29:02-root-INFO: Loss Change: 1014.800 -> 992.405
2024-12-02-11:29:02-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-11:29:02-root-INFO: Undo step: 150
2024-12-02-11:29:02-root-INFO: Undo step: 151
2024-12-02-11:29:02-root-INFO: Undo step: 152
2024-12-02-11:29:02-root-INFO: Undo step: 153
2024-12-02-11:29:02-root-INFO: Undo step: 154
2024-12-02-11:29:02-root-INFO: Undo step: 155
2024-12-02-11:29:02-root-INFO: Undo step: 156
2024-12-02-11:29:02-root-INFO: Undo step: 157
2024-12-02-11:29:02-root-INFO: Undo step: 158
2024-12-02-11:29:02-root-INFO: Undo step: 159
2024-12-02-11:29:02-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-11:29:02-root-INFO: grad norm: 400.630 394.489 69.881
2024-12-02-11:29:03-root-INFO: grad norm: 260.350 256.458 44.848
2024-12-02-11:29:03-root-INFO: grad norm: 309.687 304.764 55.003
2024-12-02-11:29:03-root-INFO: Loss too large (1545.422->1583.432)! Learning rate decreased to 0.00568.
2024-12-02-11:29:04-root-INFO: grad norm: 269.163 266.963 34.339
2024-12-02-11:29:04-root-INFO: grad norm: 305.073 300.356 53.441
2024-12-02-11:29:04-root-INFO: Loss too large (1351.317->1467.368)! Learning rate decreased to 0.00455.
2024-12-02-11:29:04-root-INFO: Loss too large (1351.317->1391.333)! Learning rate decreased to 0.00364.
2024-12-02-11:29:05-root-INFO: Loss Change: 2269.336 -> 1338.889
2024-12-02-11:29:05-root-INFO: Regularization Change: 0.000 -> 22.588
2024-12-02-11:29:05-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-11:29:05-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-11:29:05-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-11:29:05-root-INFO: grad norm: 214.560 212.895 26.681
2024-12-02-11:29:05-root-INFO: Loss too large (1344.036->1377.252)! Learning rate decreased to 0.00590.
2024-12-02-11:29:06-root-INFO: grad norm: 317.842 313.996 49.297
2024-12-02-11:29:06-root-INFO: Loss too large (1299.830->1433.540)! Learning rate decreased to 0.00472.
2024-12-02-11:29:06-root-INFO: Loss too large (1299.830->1350.450)! Learning rate decreased to 0.00378.
2024-12-02-11:29:07-root-INFO: grad norm: 205.215 203.707 24.831
2024-12-02-11:29:07-root-INFO: grad norm: 73.281 69.925 21.923
2024-12-02-11:29:07-root-INFO: grad norm: 66.684 63.856 19.216
2024-12-02-11:29:08-root-INFO: Loss Change: 1344.036 -> 1157.091
2024-12-02-11:29:08-root-INFO: Regularization Change: 0.000 -> 2.576
2024-12-02-11:29:08-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-11:29:08-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-11:29:08-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-11:29:08-root-INFO: grad norm: 60.289 57.807 17.121
2024-12-02-11:29:09-root-INFO: grad norm: 63.689 61.838 15.243
2024-12-02-11:29:09-root-INFO: Loss too large (1127.205->1133.134)! Learning rate decreased to 0.00613.
2024-12-02-11:29:09-root-INFO: grad norm: 184.719 182.520 28.414
2024-12-02-11:29:09-root-INFO: Loss too large (1124.385->1233.815)! Learning rate decreased to 0.00490.
2024-12-02-11:29:09-root-INFO: Loss too large (1124.385->1192.847)! Learning rate decreased to 0.00392.
2024-12-02-11:29:10-root-INFO: Loss too large (1124.385->1163.003)! Learning rate decreased to 0.00314.
2024-12-02-11:29:10-root-INFO: Loss too large (1124.385->1141.657)! Learning rate decreased to 0.00251.
2024-12-02-11:29:10-root-INFO: Loss too large (1124.385->1126.947)! Learning rate decreased to 0.00201.
2024-12-02-11:29:10-root-INFO: grad norm: 114.318 113.109 16.580
2024-12-02-11:29:11-root-INFO: grad norm: 49.093 47.004 14.170
2024-12-02-11:29:11-root-INFO: Loss Change: 1150.262 -> 1097.616
2024-12-02-11:29:11-root-INFO: Regularization Change: 0.000 -> 1.048
2024-12-02-11:29:11-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-11:29:11-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:29:11-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-11:29:11-root-INFO: grad norm: 69.466 68.085 13.781
2024-12-02-11:29:12-root-INFO: Loss too large (1095.352->1133.649)! Learning rate decreased to 0.00636.
2024-12-02-11:29:12-root-INFO: Loss too large (1095.352->1110.252)! Learning rate decreased to 0.00509.
2024-12-02-11:29:12-root-INFO: Loss too large (1095.352->1097.707)! Learning rate decreased to 0.00407.
2024-12-02-11:29:12-root-INFO: grad norm: 134.648 132.954 21.288
2024-12-02-11:29:13-root-INFO: Loss too large (1091.724->1112.787)! Learning rate decreased to 0.00326.
2024-12-02-11:29:13-root-INFO: Loss too large (1091.724->1099.921)! Learning rate decreased to 0.00261.
2024-12-02-11:29:13-root-INFO: grad norm: 114.237 113.091 16.135
2024-12-02-11:29:14-root-INFO: grad norm: 77.222 75.718 15.170
2024-12-02-11:29:14-root-INFO: grad norm: 74.559 73.355 13.347
2024-12-02-11:29:15-root-INFO: Loss Change: 1095.352 -> 1066.664
2024-12-02-11:29:15-root-INFO: Regularization Change: 0.000 -> 0.433
2024-12-02-11:29:15-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-11:29:15-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:29:15-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-11:29:15-root-INFO: grad norm: 47.797 46.070 12.734
2024-12-02-11:29:15-root-INFO: grad norm: 82.757 81.706 13.148
2024-12-02-11:29:15-root-INFO: Loss too large (1052.008->1154.109)! Learning rate decreased to 0.00660.
2024-12-02-11:29:16-root-INFO: Loss too large (1052.008->1106.112)! Learning rate decreased to 0.00528.
2024-12-02-11:29:16-root-INFO: Loss too large (1052.008->1073.427)! Learning rate decreased to 0.00423.
2024-12-02-11:29:16-root-INFO: Loss too large (1052.008->1055.912)! Learning rate decreased to 0.00338.
2024-12-02-11:29:16-root-INFO: grad norm: 121.756 120.283 18.881
2024-12-02-11:29:17-root-INFO: Loss too large (1047.813->1055.936)! Learning rate decreased to 0.00270.
2024-12-02-11:29:17-root-INFO: Loss too large (1047.813->1048.390)! Learning rate decreased to 0.00216.
2024-12-02-11:29:17-root-INFO: grad norm: 84.792 83.771 13.116
2024-12-02-11:29:18-root-INFO: grad norm: 46.338 44.835 11.706
2024-12-02-11:29:18-root-INFO: Loss Change: 1061.946 -> 1032.091
2024-12-02-11:29:18-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-02-11:29:18-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-11:29:18-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-11:29:18-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-11:29:18-root-INFO: grad norm: 67.326 66.197 12.280
2024-12-02-11:29:18-root-INFO: Loss too large (1032.135->1091.446)! Learning rate decreased to 0.00685.
2024-12-02-11:29:18-root-INFO: Loss too large (1032.135->1059.357)! Learning rate decreased to 0.00548.
2024-12-02-11:29:19-root-INFO: Loss too large (1032.135->1041.245)! Learning rate decreased to 0.00439.
2024-12-02-11:29:19-root-INFO: Loss too large (1032.135->1032.187)! Learning rate decreased to 0.00351.
2024-12-02-11:29:19-root-INFO: grad norm: 99.925 98.631 16.026
2024-12-02-11:29:19-root-INFO: Loss too large (1028.100->1032.243)! Learning rate decreased to 0.00281.
2024-12-02-11:29:20-root-INFO: grad norm: 91.846 90.854 13.463
2024-12-02-11:29:20-root-INFO: grad norm: 75.955 74.748 13.488
2024-12-02-11:29:21-root-INFO: grad norm: 73.887 72.894 12.070
2024-12-02-11:29:21-root-INFO: Loss Change: 1032.135 -> 1010.199
2024-12-02-11:29:21-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-02-11:29:21-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-11:29:21-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-11:29:21-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-11:29:22-root-INFO: grad norm: 42.603 41.110 11.178
2024-12-02-11:29:22-root-INFO: grad norm: 63.745 62.741 11.271
2024-12-02-11:29:22-root-INFO: Loss too large (996.443->1058.967)! Learning rate decreased to 0.00711.
2024-12-02-11:29:22-root-INFO: Loss too large (996.443->1025.059)! Learning rate decreased to 0.00569.
2024-12-02-11:29:22-root-INFO: Loss too large (996.443->1006.190)! Learning rate decreased to 0.00455.
2024-12-02-11:29:23-root-INFO: Loss too large (996.443->996.931)! Learning rate decreased to 0.00364.
2024-12-02-11:29:23-root-INFO: grad norm: 91.033 89.903 14.297
2024-12-02-11:29:23-root-INFO: Loss too large (992.813->996.095)! Learning rate decreased to 0.00291.
2024-12-02-11:29:24-root-INFO: grad norm: 82.562 81.660 12.172
2024-12-02-11:29:24-root-INFO: grad norm: 66.682 65.606 11.932
2024-12-02-11:29:25-root-INFO: Loss Change: 1006.866 -> 981.620
2024-12-02-11:29:25-root-INFO: Regularization Change: 0.000 -> 0.611
2024-12-02-11:29:25-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-11:29:25-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-11:29:25-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-11:29:25-root-INFO: grad norm: 85.602 84.609 13.003
2024-12-02-11:29:25-root-INFO: Loss too large (983.119->1106.612)! Learning rate decreased to 0.00738.
2024-12-02-11:29:25-root-INFO: Loss too large (983.119->1047.989)! Learning rate decreased to 0.00590.
2024-12-02-11:29:25-root-INFO: Loss too large (983.119->1006.969)! Learning rate decreased to 0.00472.
2024-12-02-11:29:25-root-INFO: Loss too large (983.119->985.778)! Learning rate decreased to 0.00378.
2024-12-02-11:29:26-root-INFO: grad norm: 109.758 108.595 15.932
2024-12-02-11:29:26-root-INFO: Loss too large (976.572->982.457)! Learning rate decreased to 0.00302.
2024-12-02-11:29:27-root-INFO: grad norm: 88.900 88.025 12.439
2024-12-02-11:29:27-root-INFO: grad norm: 52.312 51.252 10.477
2024-12-02-11:29:27-root-INFO: grad norm: 48.908 47.935 9.706
2024-12-02-11:29:28-root-INFO: Loss Change: 983.119 -> 957.739
2024-12-02-11:29:28-root-INFO: Regularization Change: 0.000 -> 0.345
2024-12-02-11:29:28-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-11:29:28-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-11:29:28-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-11:29:28-root-INFO: grad norm: 35.708 34.412 9.533
2024-12-02-11:29:29-root-INFO: grad norm: 33.621 32.408 8.949
2024-12-02-11:29:29-root-INFO: grad norm: 34.181 33.138 8.381
2024-12-02-11:29:29-root-INFO: grad norm: 37.532 36.464 8.890
2024-12-02-11:29:30-root-INFO: grad norm: 60.190 59.421 9.591
2024-12-02-11:29:30-root-INFO: Loss too large (921.200->970.157)! Learning rate decreased to 0.00765.
2024-12-02-11:29:30-root-INFO: Loss too large (921.200->942.213)! Learning rate decreased to 0.00612.
2024-12-02-11:29:30-root-INFO: Loss too large (921.200->927.586)! Learning rate decreased to 0.00490.
2024-12-02-11:29:31-root-INFO: Loss Change: 955.734 -> 920.469
2024-12-02-11:29:31-root-INFO: Regularization Change: 0.000 -> 1.836
2024-12-02-11:29:31-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-11:29:31-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-11:29:31-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-11:29:31-root-INFO: grad norm: 68.637 67.755 10.967
2024-12-02-11:29:31-root-INFO: Loss too large (914.271->943.171)! Learning rate decreased to 0.00794.
2024-12-02-11:29:31-root-INFO: Loss too large (914.271->931.077)! Learning rate decreased to 0.00635.
2024-12-02-11:29:31-root-INFO: Loss too large (914.271->922.740)! Learning rate decreased to 0.00508.
2024-12-02-11:29:32-root-INFO: Loss too large (914.271->917.182)! Learning rate decreased to 0.00406.
2024-12-02-11:29:32-root-INFO: grad norm: 72.209 71.436 10.538
2024-12-02-11:29:32-root-INFO: grad norm: 82.979 82.115 11.943
2024-12-02-11:29:33-root-INFO: Loss too large (907.876->909.966)! Learning rate decreased to 0.00325.
2024-12-02-11:29:33-root-INFO: grad norm: 67.467 66.732 9.929
2024-12-02-11:29:34-root-INFO: grad norm: 43.650 42.808 8.530
2024-12-02-11:29:34-root-INFO: Loss Change: 914.271 -> 896.736
2024-12-02-11:29:34-root-INFO: Regularization Change: 0.000 -> 0.305
2024-12-02-11:29:34-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-11:29:34-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:29:34-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-11:29:34-root-INFO: grad norm: 53.310 52.498 9.267
2024-12-02-11:29:34-root-INFO: Loss too large (897.990->937.003)! Learning rate decreased to 0.00823.
2024-12-02-11:29:34-root-INFO: Loss too large (897.990->912.937)! Learning rate decreased to 0.00659.
2024-12-02-11:29:35-root-INFO: Loss too large (897.990->901.142)! Learning rate decreased to 0.00527.
2024-12-02-11:29:35-root-INFO: grad norm: 93.017 92.135 12.778
2024-12-02-11:29:35-root-INFO: Loss too large (895.785->905.852)! Learning rate decreased to 0.00422.
2024-12-02-11:29:35-root-INFO: Loss too large (895.785->898.631)! Learning rate decreased to 0.00337.
2024-12-02-11:29:36-root-INFO: grad norm: 69.005 68.302 9.827
2024-12-02-11:29:36-root-INFO: grad norm: 35.976 35.096 7.912
2024-12-02-11:29:37-root-INFO: grad norm: 32.780 31.896 7.562
2024-12-02-11:29:37-root-INFO: Loss Change: 897.990 -> 880.052
2024-12-02-11:29:37-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-02-11:29:37-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-11:29:37-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:29:37-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-11:29:37-root-INFO: grad norm: 35.980 34.939 8.593
2024-12-02-11:29:38-root-INFO: grad norm: 87.323 86.360 12.935
2024-12-02-11:29:38-root-INFO: Loss too large (874.994->926.693)! Learning rate decreased to 0.00854.
2024-12-02-11:29:38-root-INFO: Loss too large (874.994->905.670)! Learning rate decreased to 0.00683.
2024-12-02-11:29:38-root-INFO: Loss too large (874.994->891.384)! Learning rate decreased to 0.00546.
2024-12-02-11:29:38-root-INFO: Loss too large (874.994->881.825)! Learning rate decreased to 0.00437.
2024-12-02-11:29:39-root-INFO: Loss too large (874.994->875.635)! Learning rate decreased to 0.00350.
2024-12-02-11:29:39-root-INFO: grad norm: 61.316 60.636 9.109
2024-12-02-11:29:39-root-INFO: grad norm: 32.436 31.559 7.493
2024-12-02-11:29:40-root-INFO: grad norm: 29.743 28.862 7.188
2024-12-02-11:29:40-root-INFO: Loss Change: 879.516 -> 859.343
2024-12-02-11:29:40-root-INFO: Regularization Change: 0.000 -> 0.494
2024-12-02-11:29:40-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-11:29:40-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:29:40-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-11:29:41-root-INFO: grad norm: 36.977 35.953 8.640
2024-12-02-11:29:41-root-INFO: grad norm: 91.663 90.601 13.910
2024-12-02-11:29:41-root-INFO: Loss too large (857.078->911.614)! Learning rate decreased to 0.00885.
2024-12-02-11:29:41-root-INFO: Loss too large (857.078->888.331)! Learning rate decreased to 0.00708.
2024-12-02-11:29:41-root-INFO: Loss too large (857.078->872.869)! Learning rate decreased to 0.00566.
2024-12-02-11:29:42-root-INFO: Loss too large (857.078->862.739)! Learning rate decreased to 0.00453.
2024-12-02-11:29:42-root-INFO: grad norm: 71.637 70.998 9.549
2024-12-02-11:29:43-root-INFO: grad norm: 39.479 38.719 7.706
2024-12-02-11:29:43-root-INFO: grad norm: 37.406 36.670 7.388
2024-12-02-11:29:44-root-INFO: Loss Change: 860.038 -> 838.310
2024-12-02-11:29:44-root-INFO: Regularization Change: 0.000 -> 0.642
2024-12-02-11:29:44-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-11:29:44-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-11:29:44-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-11:29:44-root-INFO: grad norm: 29.581 28.611 7.515
2024-12-02-11:29:44-root-INFO: grad norm: 29.646 28.641 7.654
2024-12-02-11:29:45-root-INFO: grad norm: 44.350 43.506 8.612
2024-12-02-11:29:45-root-INFO: Loss too large (822.087->826.386)! Learning rate decreased to 0.00917.
2024-12-02-11:29:45-root-INFO: Loss too large (822.087->822.412)! Learning rate decreased to 0.00734.
2024-12-02-11:29:46-root-INFO: grad norm: 53.358 52.509 9.482
2024-12-02-11:29:46-root-INFO: grad norm: 106.804 105.957 13.423
2024-12-02-11:29:46-root-INFO: Loss too large (819.446->843.910)! Learning rate decreased to 0.00587.
2024-12-02-11:29:46-root-INFO: Loss too large (819.446->830.522)! Learning rate decreased to 0.00470.
2024-12-02-11:29:47-root-INFO: Loss too large (819.446->821.617)! Learning rate decreased to 0.00376.
2024-12-02-11:29:47-root-INFO: Loss Change: 836.383 -> 815.940
2024-12-02-11:29:47-root-INFO: Regularization Change: 0.000 -> 1.171
2024-12-02-11:29:47-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-11:29:47-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:29:47-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-11:29:47-root-INFO: grad norm: 68.984 68.385 9.067
2024-12-02-11:29:47-root-INFO: Loss too large (818.063->872.589)! Learning rate decreased to 0.00951.
2024-12-02-11:29:47-root-INFO: Loss too large (818.063->830.072)! Learning rate decreased to 0.00761.
2024-12-02-11:29:48-root-INFO: grad norm: 107.797 106.908 13.819
2024-12-02-11:29:48-root-INFO: Loss too large (812.562->833.461)! Learning rate decreased to 0.00608.
2024-12-02-11:29:48-root-INFO: Loss too large (812.562->820.311)! Learning rate decreased to 0.00487.
2024-12-02-11:29:49-root-INFO: grad norm: 68.659 68.116 8.620
2024-12-02-11:29:49-root-INFO: grad norm: 24.475 23.645 6.320
2024-12-02-11:29:50-root-INFO: grad norm: 23.625 22.820 6.116
2024-12-02-11:29:50-root-INFO: Loss Change: 818.063 -> 792.172
2024-12-02-11:29:50-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-11:29:50-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-11:29:50-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:29:50-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-11:29:50-root-INFO: grad norm: 34.458 33.645 7.438
2024-12-02-11:29:51-root-INFO: grad norm: 67.830 66.945 10.924
2024-12-02-11:29:51-root-INFO: Loss too large (789.872->814.573)! Learning rate decreased to 0.00985.
2024-12-02-11:29:51-root-INFO: Loss too large (789.872->800.968)! Learning rate decreased to 0.00788.
2024-12-02-11:29:51-root-INFO: Loss too large (789.872->792.557)! Learning rate decreased to 0.00630.
2024-12-02-11:29:52-root-INFO: grad norm: 55.884 55.270 8.260
2024-12-02-11:29:52-root-INFO: grad norm: 39.360 38.663 7.379
2024-12-02-11:29:53-root-INFO: grad norm: 36.556 35.921 6.781
2024-12-02-11:29:53-root-INFO: Loss Change: 793.221 -> 772.146
2024-12-02-11:29:53-root-INFO: Regularization Change: 0.000 -> 0.807
2024-12-02-11:29:53-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-11:29:53-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:29:53-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-11:29:53-root-INFO: grad norm: 26.295 25.475 6.516
2024-12-02-11:29:54-root-INFO: grad norm: 30.695 29.873 7.055
2024-12-02-11:29:54-root-INFO: grad norm: 68.659 67.971 9.698
2024-12-02-11:29:54-root-INFO: Loss too large (762.510->786.764)! Learning rate decreased to 0.01021.
2024-12-02-11:29:54-root-INFO: Loss too large (762.510->774.284)! Learning rate decreased to 0.00816.
2024-12-02-11:29:55-root-INFO: Loss too large (762.510->766.195)! Learning rate decreased to 0.00653.
2024-12-02-11:29:55-root-INFO: grad norm: 53.136 52.483 8.308
2024-12-02-11:29:55-root-INFO: grad norm: 29.458 28.836 6.023
2024-12-02-11:29:56-root-INFO: Loss Change: 770.851 -> 748.975
2024-12-02-11:29:56-root-INFO: Regularization Change: 0.000 -> 1.014
2024-12-02-11:29:56-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-11:29:56-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-11:29:56-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-11:29:56-root-INFO: grad norm: 35.215 34.582 6.644
2024-12-02-11:29:56-root-INFO: Loss too large (748.738->748.942)! Learning rate decreased to 0.01057.
2024-12-02-11:29:57-root-INFO: grad norm: 54.877 54.272 8.122
2024-12-02-11:29:57-root-INFO: Loss too large (745.547->750.410)! Learning rate decreased to 0.00846.
2024-12-02-11:29:57-root-INFO: grad norm: 52.971 52.418 7.639
2024-12-02-11:29:58-root-INFO: grad norm: 47.733 47.113 7.666
2024-12-02-11:29:58-root-INFO: grad norm: 46.662 46.080 7.345
2024-12-02-11:29:59-root-INFO: Loss Change: 748.738 -> 730.167
2024-12-02-11:29:59-root-INFO: Regularization Change: 0.000 -> 0.929
2024-12-02-11:29:59-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-11:29:59-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-11:29:59-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-11:29:59-root-INFO: grad norm: 30.834 30.197 6.235
2024-12-02-11:29:59-root-INFO: grad norm: 40.839 40.252 6.895
2024-12-02-11:30:00-root-INFO: Loss too large (724.553->727.774)! Learning rate decreased to 0.01095.
2024-12-02-11:30:00-root-INFO: grad norm: 60.889 60.378 7.873
2024-12-02-11:30:00-root-INFO: Loss too large (721.152->728.000)! Learning rate decreased to 0.00876.
2024-12-02-11:30:00-root-INFO: Loss too large (721.152->721.757)! Learning rate decreased to 0.00701.
2024-12-02-11:30:01-root-INFO: grad norm: 41.978 41.427 6.778
2024-12-02-11:30:01-root-INFO: grad norm: 22.209 21.535 5.428
2024-12-02-11:30:02-root-INFO: Loss Change: 727.868 -> 708.531
2024-12-02-11:30:02-root-INFO: Regularization Change: 0.000 -> 0.858
2024-12-02-11:30:02-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-11:30:02-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-11:30:02-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-11:30:02-root-INFO: grad norm: 26.626 25.916 6.110
2024-12-02-11:30:02-root-INFO: grad norm: 39.610 38.881 7.568
2024-12-02-11:30:03-root-INFO: Loss too large (704.572->707.396)! Learning rate decreased to 0.01134.
2024-12-02-11:30:03-root-INFO: grad norm: 46.871 46.046 8.756
2024-12-02-11:30:04-root-INFO: grad norm: 60.437 59.352 11.397
2024-12-02-11:30:04-root-INFO: Loss too large (700.925->704.624)! Learning rate decreased to 0.00907.
2024-12-02-11:30:04-root-INFO: grad norm: 49.914 49.120 8.866
2024-12-02-11:30:05-root-INFO: Loss Change: 708.424 -> 690.519
2024-12-02-11:30:05-root-INFO: Regularization Change: 0.000 -> 1.129
2024-12-02-11:30:05-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-11:30:05-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-11:30:05-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-11:30:05-root-INFO: grad norm: 27.621 26.573 7.536
2024-12-02-11:30:05-root-INFO: grad norm: 42.515 41.069 10.994
2024-12-02-11:30:06-root-INFO: Loss too large (685.610->689.164)! Learning rate decreased to 0.01174.
2024-12-02-11:30:06-root-INFO: grad norm: 45.288 43.268 13.374
2024-12-02-11:30:06-root-INFO: grad norm: 47.069 45.622 11.584
2024-12-02-11:30:07-root-INFO: grad norm: 49.847 48.014 13.392
2024-12-02-11:30:07-root-INFO: Loss Change: 688.548 -> 674.333
2024-12-02-11:30:07-root-INFO: Regularization Change: 0.000 -> 1.360
2024-12-02-11:30:07-root-INFO: Undo step: 140
2024-12-02-11:30:07-root-INFO: Undo step: 141
2024-12-02-11:30:07-root-INFO: Undo step: 142
2024-12-02-11:30:07-root-INFO: Undo step: 143
2024-12-02-11:30:07-root-INFO: Undo step: 144
2024-12-02-11:30:07-root-INFO: Undo step: 145
2024-12-02-11:30:07-root-INFO: Undo step: 146
2024-12-02-11:30:07-root-INFO: Undo step: 147
2024-12-02-11:30:07-root-INFO: Undo step: 148
2024-12-02-11:30:07-root-INFO: Undo step: 149
2024-12-02-11:30:07-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-11:30:08-root-INFO: grad norm: 398.427 394.017 59.119
2024-12-02-11:30:08-root-INFO: grad norm: 449.835 446.480 54.843
2024-12-02-11:30:08-root-INFO: grad norm: 302.430 300.212 36.564
2024-12-02-11:30:09-root-INFO: grad norm: 324.163 322.477 33.012
2024-12-02-11:30:09-root-INFO: grad norm: 118.644 115.489 27.177
2024-12-02-11:30:10-root-INFO: Loss Change: 1766.450 -> 1006.185
2024-12-02-11:30:10-root-INFO: Regularization Change: 0.000 -> 34.447
2024-12-02-11:30:10-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-11:30:10-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:30:10-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-11:30:10-root-INFO: grad norm: 273.945 271.498 36.529
2024-12-02-11:30:10-root-INFO: Loss too large (990.856->1276.662)! Learning rate decreased to 0.00854.
2024-12-02-11:30:10-root-INFO: Loss too large (990.856->1153.340)! Learning rate decreased to 0.00683.
2024-12-02-11:30:10-root-INFO: Loss too large (990.856->1073.613)! Learning rate decreased to 0.00546.
2024-12-02-11:30:11-root-INFO: Loss too large (990.856->1022.481)! Learning rate decreased to 0.00437.
2024-12-02-11:30:11-root-INFO: grad norm: 139.817 138.714 17.522
2024-12-02-11:30:12-root-INFO: grad norm: 81.441 80.109 14.674
2024-12-02-11:30:12-root-INFO: grad norm: 49.299 47.371 13.652
2024-12-02-11:30:12-root-INFO: grad norm: 46.918 45.049 13.110
2024-12-02-11:30:13-root-INFO: Loss Change: 990.856 -> 881.539
2024-12-02-11:30:13-root-INFO: Regularization Change: 0.000 -> 1.615
2024-12-02-11:30:13-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-11:30:13-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-11:30:13-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-11:30:13-root-INFO: grad norm: 48.616 46.865 12.930
2024-12-02-11:30:14-root-INFO: grad norm: 59.790 58.385 12.888
2024-12-02-11:30:14-root-INFO: grad norm: 83.989 82.605 15.187
2024-12-02-11:30:14-root-INFO: Loss too large (853.003->880.065)! Learning rate decreased to 0.00885.
2024-12-02-11:30:15-root-INFO: grad norm: 164.553 163.333 20.002
2024-12-02-11:30:15-root-INFO: Loss too large (844.470->912.766)! Learning rate decreased to 0.00708.
2024-12-02-11:30:15-root-INFO: Loss too large (844.470->877.677)! Learning rate decreased to 0.00566.
2024-12-02-11:30:15-root-INFO: Loss too large (844.470->854.940)! Learning rate decreased to 0.00453.
2024-12-02-11:30:16-root-INFO: grad norm: 86.260 85.518 11.294
2024-12-02-11:30:16-root-INFO: Loss Change: 876.338 -> 815.361
2024-12-02-11:30:16-root-INFO: Regularization Change: 0.000 -> 2.507
2024-12-02-11:30:16-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-11:30:16-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-11:30:16-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-11:30:16-root-INFO: grad norm: 46.862 45.847 9.703
2024-12-02-11:30:17-root-INFO: grad norm: 107.468 106.680 12.994
2024-12-02-11:30:17-root-INFO: Loss too large (805.801->853.559)! Learning rate decreased to 0.00917.
2024-12-02-11:30:17-root-INFO: Loss too large (805.801->829.925)! Learning rate decreased to 0.00734.
2024-12-02-11:30:17-root-INFO: Loss too large (805.801->814.560)! Learning rate decreased to 0.00587.
2024-12-02-11:30:18-root-INFO: grad norm: 73.665 72.989 9.959
2024-12-02-11:30:18-root-INFO: grad norm: 31.197 30.126 8.103
2024-12-02-11:30:19-root-INFO: grad norm: 29.805 28.749 7.863
2024-12-02-11:30:19-root-INFO: Loss Change: 811.521 -> 774.062
2024-12-02-11:30:19-root-INFO: Regularization Change: 0.000 -> 1.261
2024-12-02-11:30:19-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-11:30:19-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:30:19-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-11:30:19-root-INFO: grad norm: 33.068 32.076 8.039
2024-12-02-11:30:20-root-INFO: grad norm: 38.850 38.005 8.058
2024-12-02-11:30:20-root-INFO: grad norm: 53.182 52.428 8.925
2024-12-02-11:30:20-root-INFO: Loss too large (758.494->764.391)! Learning rate decreased to 0.00951.
2024-12-02-11:30:21-root-INFO: grad norm: 83.351 82.632 10.925
2024-12-02-11:30:21-root-INFO: Loss too large (753.769->766.853)! Learning rate decreased to 0.00761.
2024-12-02-11:30:21-root-INFO: Loss too large (753.769->756.826)! Learning rate decreased to 0.00608.
2024-12-02-11:30:21-root-INFO: grad norm: 56.648 56.080 8.001
2024-12-02-11:30:22-root-INFO: Loss Change: 771.853 -> 739.297
2024-12-02-11:30:22-root-INFO: Regularization Change: 0.000 -> 1.489
2024-12-02-11:30:22-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-11:30:22-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:30:22-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-11:30:22-root-INFO: grad norm: 29.431 28.535 7.209
2024-12-02-11:30:22-root-INFO: grad norm: 29.107 28.310 6.762
2024-12-02-11:30:23-root-INFO: grad norm: 33.854 33.145 6.890
2024-12-02-11:30:23-root-INFO: grad norm: 57.159 56.508 8.602
2024-12-02-11:30:24-root-INFO: Loss too large (721.262->730.854)! Learning rate decreased to 0.00985.
2024-12-02-11:30:24-root-INFO: Loss too large (721.262->723.320)! Learning rate decreased to 0.00788.
2024-12-02-11:30:24-root-INFO: grad norm: 49.171 48.611 7.401
2024-12-02-11:30:24-root-INFO: Loss Change: 737.596 -> 711.261
2024-12-02-11:30:24-root-INFO: Regularization Change: 0.000 -> 1.475
2024-12-02-11:30:24-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-11:30:24-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-11:30:25-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-11:30:25-root-INFO: grad norm: 29.416 28.696 6.469
2024-12-02-11:30:25-root-INFO: grad norm: 36.042 35.475 6.368
2024-12-02-11:30:26-root-INFO: grad norm: 71.198 70.670 8.653
2024-12-02-11:30:26-root-INFO: Loss too large (702.820->722.491)! Learning rate decreased to 0.01021.
2024-12-02-11:30:26-root-INFO: Loss too large (702.820->710.637)! Learning rate decreased to 0.00816.
2024-12-02-11:30:26-root-INFO: Loss too large (702.820->703.208)! Learning rate decreased to 0.00653.
2024-12-02-11:30:27-root-INFO: grad norm: 43.593 43.105 6.510
2024-12-02-11:30:27-root-INFO: grad norm: 20.324 19.570 5.484
2024-12-02-11:30:27-root-INFO: Loss Change: 708.466 -> 688.874
2024-12-02-11:30:27-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-02-11:30:27-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-11:30:27-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-11:30:28-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-11:30:28-root-INFO: grad norm: 25.667 24.956 5.999
2024-12-02-11:30:28-root-INFO: grad norm: 31.005 30.312 6.518
2024-12-02-11:30:29-root-INFO: grad norm: 40.736 40.105 7.142
2024-12-02-11:30:29-root-INFO: Loss too large (680.818->681.139)! Learning rate decreased to 0.01057.
2024-12-02-11:30:29-root-INFO: grad norm: 45.381 44.671 7.993
2024-12-02-11:30:30-root-INFO: grad norm: 47.023 46.389 7.692
2024-12-02-11:30:30-root-INFO: Loss Change: 687.556 -> 670.786
2024-12-02-11:30:30-root-INFO: Regularization Change: 0.000 -> 1.260
2024-12-02-11:30:30-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-11:30:30-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-11:30:30-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-11:30:30-root-INFO: grad norm: 37.789 37.154 6.897
2024-12-02-11:30:31-root-INFO: grad norm: 46.230 45.624 7.461
2024-12-02-11:30:31-root-INFO: grad norm: 79.001 78.219 11.083
2024-12-02-11:30:31-root-INFO: Loss too large (666.100->688.908)! Learning rate decreased to 0.01095.
2024-12-02-11:30:32-root-INFO: Loss too large (666.100->673.396)! Learning rate decreased to 0.00876.
2024-12-02-11:30:32-root-INFO: grad norm: 53.912 53.354 7.733
2024-12-02-11:30:33-root-INFO: grad norm: 23.068 22.252 6.082
2024-12-02-11:30:33-root-INFO: Loss Change: 667.667 -> 648.578
2024-12-02-11:30:33-root-INFO: Regularization Change: 0.000 -> 0.955
2024-12-02-11:30:33-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-11:30:33-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-11:30:33-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-11:30:33-root-INFO: grad norm: 25.298 24.527 6.200
2024-12-02-11:30:34-root-INFO: grad norm: 30.393 29.362 7.848
2024-12-02-11:30:34-root-INFO: grad norm: 39.997 38.777 9.802
2024-12-02-11:30:34-root-INFO: Loss too large (642.586->642.866)! Learning rate decreased to 0.01134.
2024-12-02-11:30:35-root-INFO: grad norm: 38.204 36.629 10.854
2024-12-02-11:30:35-root-INFO: grad norm: 37.125 35.841 9.679
2024-12-02-11:30:36-root-INFO: Loss Change: 647.885 -> 632.941
2024-12-02-11:30:36-root-INFO: Regularization Change: 0.000 -> 1.143
2024-12-02-11:30:36-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-11:30:36-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-11:30:36-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-11:30:36-root-INFO: grad norm: 29.280 27.777 9.263
2024-12-02-11:30:36-root-INFO: grad norm: 39.264 37.425 11.877
2024-12-02-11:30:37-root-INFO: Loss too large (628.163->629.164)! Learning rate decreased to 0.01174.
2024-12-02-11:30:37-root-INFO: grad norm: 37.404 35.001 13.189
2024-12-02-11:30:37-root-INFO: grad norm: 35.280 33.542 10.937
2024-12-02-11:30:38-root-INFO: grad norm: 34.163 31.967 12.052
2024-12-02-11:30:38-root-INFO: Loss Change: 630.476 -> 615.866
2024-12-02-11:30:38-root-INFO: Regularization Change: 0.000 -> 1.033
2024-12-02-11:30:38-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-11:30:38-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:30:38-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-11:30:39-root-INFO: grad norm: 39.490 37.748 11.600
2024-12-02-11:30:39-root-INFO: Loss too large (616.806->616.967)! Learning rate decreased to 0.01215.
2024-12-02-11:30:39-root-INFO: grad norm: 36.088 33.931 12.288
2024-12-02-11:30:40-root-INFO: grad norm: 33.460 31.893 10.119
2024-12-02-11:30:40-root-INFO: grad norm: 32.164 30.196 11.079
2024-12-02-11:30:41-root-INFO: grad norm: 31.162 29.628 9.657
2024-12-02-11:30:41-root-INFO: Loss Change: 616.806 -> 601.556
2024-12-02-11:30:41-root-INFO: Regularization Change: 0.000 -> 0.984
2024-12-02-11:30:41-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-11:30:41-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:30:41-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-11:30:41-root-INFO: grad norm: 27.389 25.703 9.462
2024-12-02-11:30:42-root-INFO: grad norm: 37.239 35.250 12.010
2024-12-02-11:30:42-root-INFO: Loss too large (597.884->599.043)! Learning rate decreased to 0.01258.
2024-12-02-11:30:42-root-INFO: grad norm: 34.996 32.490 13.005
2024-12-02-11:30:43-root-INFO: grad norm: 32.284 30.529 10.500
2024-12-02-11:30:43-root-INFO: grad norm: 31.106 28.961 11.352
2024-12-02-11:30:44-root-INFO: Loss Change: 599.633 -> 586.669
2024-12-02-11:30:44-root-INFO: Regularization Change: 0.000 -> 0.979
2024-12-02-11:30:44-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-11:30:44-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:30:44-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-11:30:44-root-INFO: grad norm: 41.919 39.614 13.711
2024-12-02-11:30:44-root-INFO: grad norm: 44.874 41.828 16.251
2024-12-02-11:30:45-root-INFO: grad norm: 48.123 45.935 14.345
2024-12-02-11:30:45-root-INFO: Loss too large (584.651->585.603)! Learning rate decreased to 0.01302.
2024-12-02-11:30:46-root-INFO: grad norm: 38.781 36.803 12.226
2024-12-02-11:30:46-root-INFO: grad norm: 32.080 30.942 8.470
2024-12-02-11:30:47-root-INFO: Loss Change: 588.952 -> 571.929
2024-12-02-11:30:47-root-INFO: Regularization Change: 0.000 -> 1.196
2024-12-02-11:30:47-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-11:30:47-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-11:30:47-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-11:30:47-root-INFO: grad norm: 28.109 26.963 7.943
2024-12-02-11:30:47-root-INFO: grad norm: 34.476 33.232 9.178
2024-12-02-11:30:48-root-INFO: grad norm: 44.627 42.833 12.525
2024-12-02-11:30:48-root-INFO: Loss too large (569.211->571.277)! Learning rate decreased to 0.01347.
2024-12-02-11:30:48-root-INFO: grad norm: 36.823 35.624 9.323
2024-12-02-11:30:49-root-INFO: grad norm: 29.527 28.166 8.860
2024-12-02-11:30:49-root-INFO: Loss Change: 571.109 -> 558.346
2024-12-02-11:30:49-root-INFO: Regularization Change: 0.000 -> 1.067
2024-12-02-11:30:49-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-11:30:49-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-11:30:49-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-11:30:50-root-INFO: grad norm: 33.500 32.318 8.817
2024-12-02-11:30:50-root-INFO: grad norm: 42.642 41.005 11.703
2024-12-02-11:30:50-root-INFO: Loss too large (557.836->559.629)! Learning rate decreased to 0.01393.
2024-12-02-11:30:51-root-INFO: grad norm: 36.442 35.373 8.761
2024-12-02-11:30:51-root-INFO: grad norm: 31.427 30.276 8.426
2024-12-02-11:30:52-root-INFO: grad norm: 28.760 27.884 7.044
2024-12-02-11:30:52-root-INFO: Loss Change: 558.585 -> 544.857
2024-12-02-11:30:52-root-INFO: Regularization Change: 0.000 -> 1.032
2024-12-02-11:30:52-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-11:30:52-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-11:30:52-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-11:30:52-root-INFO: grad norm: 23.695 22.989 5.743
2024-12-02-11:30:53-root-INFO: grad norm: 27.782 26.985 6.606
2024-12-02-11:30:53-root-INFO: grad norm: 36.862 35.762 8.938
2024-12-02-11:30:53-root-INFO: Loss too large (540.033->541.764)! Learning rate decreased to 0.01441.
2024-12-02-11:30:54-root-INFO: grad norm: 31.567 30.649 7.558
2024-12-02-11:30:54-root-INFO: grad norm: 25.314 24.271 7.191
2024-12-02-11:30:55-root-INFO: Loss Change: 543.198 -> 531.415
2024-12-02-11:30:55-root-INFO: Regularization Change: 0.000 -> 1.049
2024-12-02-11:30:55-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-11:30:55-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-11:30:55-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-11:30:55-root-INFO: grad norm: 30.525 29.400 8.209
2024-12-02-11:30:55-root-INFO: grad norm: 39.260 37.757 10.758
2024-12-02-11:30:56-root-INFO: Loss too large (531.632->533.319)! Learning rate decreased to 0.01490.
2024-12-02-11:30:56-root-INFO: grad norm: 33.519 32.524 8.103
2024-12-02-11:30:57-root-INFO: grad norm: 28.471 27.488 7.417
2024-12-02-11:30:57-root-INFO: grad norm: 26.072 25.326 6.194
2024-12-02-11:30:57-root-INFO: Loss Change: 532.537 -> 519.533
2024-12-02-11:30:57-root-INFO: Regularization Change: 0.000 -> 1.034
2024-12-02-11:30:57-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-11:30:57-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-11:30:58-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-11:30:58-root-INFO: grad norm: 22.942 22.328 5.272
2024-12-02-11:30:58-root-INFO: grad norm: 26.847 26.217 5.782
2024-12-02-11:30:59-root-INFO: grad norm: 35.308 34.507 7.478
2024-12-02-11:30:59-root-INFO: Loss too large (515.532->517.161)! Learning rate decreased to 0.01541.
2024-12-02-11:30:59-root-INFO: grad norm: 29.592 28.934 6.204
2024-12-02-11:31:00-root-INFO: grad norm: 22.865 22.164 5.620
2024-12-02-11:31:00-root-INFO: Loss Change: 518.524 -> 507.049
2024-12-02-11:31:00-root-INFO: Regularization Change: 0.000 -> 1.069
2024-12-02-11:31:00-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-11:31:00-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-11:31:00-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-11:31:00-root-INFO: grad norm: 26.408 25.542 6.704
2024-12-02-11:31:01-root-INFO: grad norm: 33.371 32.431 7.866
2024-12-02-11:31:01-root-INFO: Loss too large (506.426->507.433)! Learning rate decreased to 0.01593.
2024-12-02-11:31:01-root-INFO: grad norm: 29.197 28.525 6.225
2024-12-02-11:31:02-root-INFO: grad norm: 25.390 24.797 5.454
2024-12-02-11:31:02-root-INFO: grad norm: 23.443 22.964 4.714
2024-12-02-11:31:03-root-INFO: Loss Change: 508.052 -> 495.802
2024-12-02-11:31:03-root-INFO: Regularization Change: 0.000 -> 1.045
2024-12-02-11:31:03-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-11:31:03-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-11:31:03-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-11:31:03-root-INFO: grad norm: 20.593 20.211 3.947
2024-12-02-11:31:04-root-INFO: grad norm: 24.442 24.068 4.259
2024-12-02-11:31:04-root-INFO: grad norm: 32.986 32.630 4.836
2024-12-02-11:31:04-root-INFO: Loss too large (492.138->493.683)! Learning rate decreased to 0.01647.
2024-12-02-11:31:05-root-INFO: grad norm: 27.421 27.072 4.364
2024-12-02-11:31:05-root-INFO: grad norm: 20.568 20.230 3.713
2024-12-02-11:31:05-root-INFO: Loss Change: 494.924 -> 484.159
2024-12-02-11:31:05-root-INFO: Regularization Change: 0.000 -> 1.078
2024-12-02-11:31:05-root-INFO: Undo step: 130
2024-12-02-11:31:05-root-INFO: Undo step: 131
2024-12-02-11:31:05-root-INFO: Undo step: 132
2024-12-02-11:31:05-root-INFO: Undo step: 133
2024-12-02-11:31:05-root-INFO: Undo step: 134
2024-12-02-11:31:05-root-INFO: Undo step: 135
2024-12-02-11:31:05-root-INFO: Undo step: 136
2024-12-02-11:31:05-root-INFO: Undo step: 137
2024-12-02-11:31:05-root-INFO: Undo step: 138
2024-12-02-11:31:05-root-INFO: Undo step: 139
2024-12-02-11:31:06-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-11:31:06-root-INFO: grad norm: 388.009 383.076 61.673
2024-12-02-11:31:06-root-INFO: grad norm: 365.373 362.480 45.889
2024-12-02-11:31:07-root-INFO: grad norm: 144.219 140.626 31.990
2024-12-02-11:31:07-root-INFO: grad norm: 105.853 103.211 23.502
2024-12-02-11:31:08-root-INFO: grad norm: 87.388 84.857 20.880
2024-12-02-11:31:08-root-INFO: Loss Change: 1682.669 -> 717.851
2024-12-02-11:31:08-root-INFO: Regularization Change: 0.000 -> 51.901
2024-12-02-11:31:08-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-11:31:08-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:31:08-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-11:31:08-root-INFO: grad norm: 77.264 75.836 14.786
2024-12-02-11:31:09-root-INFO: grad norm: 65.398 63.746 14.610
2024-12-02-11:31:09-root-INFO: grad norm: 57.111 55.988 11.271
2024-12-02-11:31:10-root-INFO: grad norm: 49.616 48.244 11.586
2024-12-02-11:31:10-root-INFO: grad norm: 44.337 43.360 9.256
2024-12-02-11:31:10-root-INFO: Loss Change: 717.091 -> 610.023
2024-12-02-11:31:10-root-INFO: Regularization Change: 0.000 -> 7.550
2024-12-02-11:31:10-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-11:31:10-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:31:11-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-11:31:11-root-INFO: grad norm: 39.875 38.423 10.662
2024-12-02-11:31:11-root-INFO: grad norm: 36.693 35.627 8.777
2024-12-02-11:31:12-root-INFO: grad norm: 34.630 33.531 8.654
2024-12-02-11:31:12-root-INFO: grad norm: 33.757 32.950 7.338
2024-12-02-11:31:13-root-INFO: grad norm: 33.905 33.072 7.471
2024-12-02-11:31:13-root-INFO: Loss Change: 607.389 -> 565.985
2024-12-02-11:31:13-root-INFO: Regularization Change: 0.000 -> 3.470
2024-12-02-11:31:13-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-11:31:13-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-11:31:13-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-11:31:13-root-INFO: grad norm: 43.478 42.878 7.195
2024-12-02-11:31:14-root-INFO: grad norm: 45.474 45.092 5.877
2024-12-02-11:31:14-root-INFO: grad norm: 47.396 47.045 5.758
2024-12-02-11:31:15-root-INFO: grad norm: 49.120 48.781 5.768
2024-12-02-11:31:15-root-INFO: grad norm: 49.765 49.469 5.422
2024-12-02-11:31:15-root-INFO: Loss Change: 566.358 -> 543.084
2024-12-02-11:31:15-root-INFO: Regularization Change: 0.000 -> 2.599
2024-12-02-11:31:15-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-11:31:15-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-11:31:16-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-11:31:16-root-INFO: grad norm: 47.791 47.483 5.414
2024-12-02-11:31:16-root-INFO: grad norm: 48.753 48.453 5.392
2024-12-02-11:31:17-root-INFO: grad norm: 49.801 49.493 5.532
2024-12-02-11:31:17-root-INFO: grad norm: 50.264 49.983 5.312
2024-12-02-11:31:18-root-INFO: grad norm: 50.558 50.250 5.577
2024-12-02-11:31:18-root-INFO: Loss Change: 541.059 -> 525.294
2024-12-02-11:31:18-root-INFO: Regularization Change: 0.000 -> 2.222
2024-12-02-11:31:18-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-11:31:18-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-11:31:18-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-11:31:18-root-INFO: grad norm: 57.500 57.094 6.820
2024-12-02-11:31:19-root-INFO: grad norm: 56.224 55.937 5.668
2024-12-02-11:31:19-root-INFO: grad norm: 54.474 54.170 5.752
2024-12-02-11:31:20-root-INFO: grad norm: 52.692 52.385 5.674
2024-12-02-11:31:20-root-INFO: grad norm: 51.541 51.240 5.562
2024-12-02-11:31:20-root-INFO: Loss Change: 527.688 -> 506.826
2024-12-02-11:31:20-root-INFO: Regularization Change: 0.000 -> 2.228
2024-12-02-11:31:20-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-11:31:20-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-11:31:21-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-11:31:21-root-INFO: grad norm: 46.459 46.143 5.413
2024-12-02-11:31:21-root-INFO: grad norm: 46.548 46.174 5.891
2024-12-02-11:31:22-root-INFO: grad norm: 47.174 46.865 5.393
2024-12-02-11:31:22-root-INFO: grad norm: 47.889 47.554 5.650
2024-12-02-11:31:23-root-INFO: grad norm: 48.559 48.253 5.440
2024-12-02-11:31:23-root-INFO: Loss Change: 503.579 -> 493.073
2024-12-02-11:31:23-root-INFO: Regularization Change: 0.000 -> 1.930
2024-12-02-11:31:23-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-11:31:23-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-11:31:23-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-11:31:23-root-INFO: grad norm: 55.816 55.342 7.256
2024-12-02-11:31:24-root-INFO: grad norm: 55.211 54.912 5.736
2024-12-02-11:31:24-root-INFO: grad norm: 53.712 53.369 6.063
2024-12-02-11:31:25-root-INFO: grad norm: 51.761 51.449 5.671
2024-12-02-11:31:25-root-INFO: grad norm: 50.466 50.139 5.735
2024-12-02-11:31:25-root-INFO: Loss Change: 496.400 -> 478.417
2024-12-02-11:31:25-root-INFO: Regularization Change: 0.000 -> 2.107
2024-12-02-11:31:25-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-11:31:25-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-11:31:26-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-11:31:26-root-INFO: grad norm: 47.574 47.305 5.053
2024-12-02-11:31:26-root-INFO: grad norm: 47.112 46.730 5.983
2024-12-02-11:31:27-root-INFO: grad norm: 46.539 46.249 5.188
2024-12-02-11:31:27-root-INFO: grad norm: 46.460 46.121 5.602
2024-12-02-11:31:28-root-INFO: grad norm: 46.221 45.930 5.184
2024-12-02-11:31:28-root-INFO: Loss Change: 476.698 -> 466.791
2024-12-02-11:31:28-root-INFO: Regularization Change: 0.000 -> 1.884
2024-12-02-11:31:28-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-11:31:28-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-11:31:28-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-11:31:28-root-INFO: grad norm: 52.038 51.577 6.910
2024-12-02-11:31:29-root-INFO: grad norm: 51.137 50.851 5.395
2024-12-02-11:31:29-root-INFO: grad norm: 49.567 49.237 5.709
2024-12-02-11:31:30-root-INFO: grad norm: 47.595 47.302 5.271
2024-12-02-11:31:30-root-INFO: grad norm: 46.289 45.979 5.355
2024-12-02-11:31:31-root-INFO: Loss Change: 469.840 -> 453.306
2024-12-02-11:31:31-root-INFO: Regularization Change: 0.000 -> 2.033
2024-12-02-11:31:31-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-11:31:31-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-11:31:31-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-11:31:31-root-INFO: grad norm: 43.707 43.450 4.735
2024-12-02-11:31:31-root-INFO: grad norm: 43.391 43.016 5.696
2024-12-02-11:31:32-root-INFO: grad norm: 43.070 42.793 4.878
2024-12-02-11:31:32-root-INFO: grad norm: 42.903 42.564 5.386
2024-12-02-11:31:33-root-INFO: grad norm: 42.537 42.259 4.859
2024-12-02-11:31:33-root-INFO: Loss Change: 452.130 -> 443.262
2024-12-02-11:31:33-root-INFO: Regularization Change: 0.000 -> 1.801
2024-12-02-11:31:33-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-11:31:33-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-11:31:33-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-11:31:33-root-INFO: grad norm: 48.441 47.982 6.653
2024-12-02-11:31:34-root-INFO: grad norm: 47.220 46.950 5.044
2024-12-02-11:31:34-root-INFO: grad norm: 45.687 45.359 5.464
2024-12-02-11:31:35-root-INFO: grad norm: 43.804 43.528 4.912
2024-12-02-11:31:35-root-INFO: grad norm: 42.505 42.194 5.138
2024-12-02-11:31:36-root-INFO: Loss Change: 446.093 -> 431.036
2024-12-02-11:31:36-root-INFO: Regularization Change: 0.000 -> 1.956
2024-12-02-11:31:36-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-11:31:36-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-11:31:36-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-11:31:36-root-INFO: grad norm: 37.449 37.190 4.400
2024-12-02-11:31:36-root-INFO: grad norm: 36.697 36.348 5.044
2024-12-02-11:31:37-root-INFO: grad norm: 36.164 35.913 4.251
2024-12-02-11:31:37-root-INFO: grad norm: 36.028 35.722 4.679
2024-12-02-11:31:38-root-INFO: grad norm: 35.834 35.584 4.222
2024-12-02-11:31:38-root-INFO: Loss Change: 428.852 -> 420.043
2024-12-02-11:31:38-root-INFO: Regularization Change: 0.000 -> 1.636
2024-12-02-11:31:38-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-11:31:38-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-11:31:38-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-11:31:38-root-INFO: grad norm: 39.071 38.703 5.350
2024-12-02-11:31:39-root-INFO: grad norm: 39.428 39.200 4.228
2024-12-02-11:31:39-root-INFO: grad norm: 39.202 38.914 4.748
2024-12-02-11:31:40-root-INFO: grad norm: 38.703 38.465 4.290
2024-12-02-11:31:40-root-INFO: grad norm: 38.199 37.916 4.637
2024-12-02-11:31:41-root-INFO: Loss Change: 421.141 -> 410.803
2024-12-02-11:31:41-root-INFO: Regularization Change: 0.000 -> 1.725
2024-12-02-11:31:41-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-11:31:41-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-11:31:41-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-11:31:41-root-INFO: grad norm: 35.307 35.098 3.842
2024-12-02-11:31:41-root-INFO: grad norm: 35.063 34.750 4.671
2024-12-02-11:31:42-root-INFO: grad norm: 35.024 34.796 3.994
2024-12-02-11:31:42-root-INFO: grad norm: 34.943 34.651 4.508
2024-12-02-11:31:43-root-INFO: grad norm: 34.780 34.545 4.034
2024-12-02-11:31:43-root-INFO: Loss Change: 409.711 -> 402.141
2024-12-02-11:31:43-root-INFO: Regularization Change: 0.000 -> 1.618
2024-12-02-11:31:43-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-11:31:43-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-11:31:43-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-11:31:43-root-INFO: grad norm: 39.571 39.161 5.679
2024-12-02-11:31:44-root-INFO: grad norm: 38.474 38.245 4.193
2024-12-02-11:31:44-root-INFO: grad norm: 37.414 37.132 4.583
2024-12-02-11:31:45-root-INFO: grad norm: 36.166 35.937 4.071
2024-12-02-11:31:45-root-INFO: grad norm: 35.248 34.981 4.332
2024-12-02-11:31:46-root-INFO: Loss Change: 404.206 -> 392.696
2024-12-02-11:31:46-root-INFO: Regularization Change: 0.000 -> 1.774
2024-12-02-11:31:46-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-11:31:46-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-11:31:46-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-11:31:46-root-INFO: grad norm: 33.331 33.053 4.294
2024-12-02-11:31:46-root-INFO: grad norm: 31.769 31.430 4.628
2024-12-02-11:31:47-root-INFO: grad norm: 30.740 30.503 3.809
2024-12-02-11:31:47-root-INFO: grad norm: 30.301 30.008 4.198
2024-12-02-11:31:48-root-INFO: grad norm: 29.862 29.629 3.728
2024-12-02-11:31:48-root-INFO: Loss Change: 391.848 -> 382.995
2024-12-02-11:31:49-root-INFO: Regularization Change: 0.000 -> 1.609
2024-12-02-11:31:49-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-11:31:49-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-11:31:49-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-11:31:49-root-INFO: grad norm: 33.175 32.810 4.906
2024-12-02-11:31:49-root-INFO: grad norm: 33.116 32.902 3.763
2024-12-02-11:31:50-root-INFO: grad norm: 32.980 32.711 4.201
2024-12-02-11:31:50-root-INFO: grad norm: 32.743 32.526 3.761
2024-12-02-11:31:51-root-INFO: grad norm: 32.529 32.272 4.076
2024-12-02-11:31:51-root-INFO: Loss Change: 384.475 -> 375.829
2024-12-02-11:31:51-root-INFO: Regularization Change: 0.000 -> 1.675
2024-12-02-11:31:51-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-11:31:51-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-11:31:51-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-11:31:51-root-INFO: grad norm: 31.199 30.981 3.685
2024-12-02-11:31:52-root-INFO: grad norm: 30.509 30.222 4.171
2024-12-02-11:31:52-root-INFO: grad norm: 29.925 29.706 3.610
2024-12-02-11:31:53-root-INFO: grad norm: 29.658 29.397 3.926
2024-12-02-11:31:53-root-INFO: grad norm: 29.403 29.185 3.578
2024-12-02-11:31:54-root-INFO: Loss Change: 375.278 -> 367.810
2024-12-02-11:31:54-root-INFO: Regularization Change: 0.000 -> 1.602
2024-12-02-11:31:54-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-11:31:54-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-11:31:54-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-11:31:54-root-INFO: grad norm: 32.485 32.147 4.678
2024-12-02-11:31:55-root-INFO: grad norm: 32.224 32.014 3.670
2024-12-02-11:31:55-root-INFO: grad norm: 31.933 31.679 4.024
2024-12-02-11:31:56-root-INFO: grad norm: 31.542 31.328 3.673
2024-12-02-11:31:56-root-INFO: grad norm: 31.202 30.954 3.926
2024-12-02-11:31:56-root-INFO: Loss Change: 368.929 -> 360.606
2024-12-02-11:31:56-root-INFO: Regularization Change: 0.000 -> 1.714
2024-12-02-11:31:56-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-11:31:56-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-11:31:57-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-11:31:57-root-INFO: grad norm: 29.694 29.471 3.627
2024-12-02-11:31:57-root-INFO: grad norm: 29.307 29.025 4.060
2024-12-02-11:31:58-root-INFO: grad norm: 29.151 28.923 3.639
2024-12-02-11:31:58-root-INFO: grad norm: 29.037 28.769 3.941
2024-12-02-11:31:59-root-INFO: grad norm: 28.890 28.659 3.647
2024-12-02-11:31:59-root-INFO: Loss Change: 360.139 -> 353.405
2024-12-02-11:31:59-root-INFO: Regularization Change: 0.000 -> 1.644
2024-12-02-11:31:59-root-INFO: Undo step: 120
2024-12-02-11:31:59-root-INFO: Undo step: 121
2024-12-02-11:31:59-root-INFO: Undo step: 122
2024-12-02-11:31:59-root-INFO: Undo step: 123
2024-12-02-11:31:59-root-INFO: Undo step: 124
2024-12-02-11:31:59-root-INFO: Undo step: 125
2024-12-02-11:31:59-root-INFO: Undo step: 126
2024-12-02-11:31:59-root-INFO: Undo step: 127
2024-12-02-11:31:59-root-INFO: Undo step: 128
2024-12-02-11:31:59-root-INFO: Undo step: 129
2024-12-02-11:31:59-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-11:31:59-root-INFO: grad norm: 218.386 214.099 43.058
2024-12-02-11:32:00-root-INFO: grad norm: 92.123 89.710 20.944
2024-12-02-11:32:00-root-INFO: grad norm: 73.613 71.448 17.725
2024-12-02-11:32:01-root-INFO: grad norm: 64.023 62.574 13.545
2024-12-02-11:32:01-root-INFO: grad norm: 58.114 56.439 13.855
2024-12-02-11:32:02-root-INFO: Loss Change: 1064.268 -> 510.516
2024-12-02-11:32:02-root-INFO: Regularization Change: 0.000 -> 46.207
2024-12-02-11:32:02-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-11:32:02-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-11:32:02-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-11:32:02-root-INFO: grad norm: 57.879 56.339 13.262
2024-12-02-11:32:03-root-INFO: grad norm: 49.244 47.581 12.688
2024-12-02-11:32:03-root-INFO: grad norm: 43.258 42.053 10.142
2024-12-02-11:32:04-root-INFO: grad norm: 41.050 39.840 9.892
2024-12-02-11:32:04-root-INFO: grad norm: 42.279 41.406 8.549
2024-12-02-11:32:05-root-INFO: Loss Change: 512.364 -> 450.025
2024-12-02-11:32:05-root-INFO: Regularization Change: 0.000 -> 6.924
2024-12-02-11:32:05-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-11:32:05-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-11:32:05-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-11:32:05-root-INFO: grad norm: 42.511 41.795 7.768
2024-12-02-11:32:05-root-INFO: grad norm: 46.853 46.280 7.301
2024-12-02-11:32:06-root-INFO: grad norm: 42.234 41.576 7.431
2024-12-02-11:32:06-root-INFO: grad norm: 35.065 34.522 6.147
2024-12-02-11:32:07-root-INFO: grad norm: 36.286 35.694 6.527
2024-12-02-11:32:07-root-INFO: Loss Change: 448.111 -> 418.842
2024-12-02-11:32:07-root-INFO: Regularization Change: 0.000 -> 3.626
2024-12-02-11:32:07-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-11:32:07-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-11:32:07-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-11:32:07-root-INFO: grad norm: 46.715 46.148 7.257
2024-12-02-11:32:08-root-INFO: Loss too large (421.468->421.613)! Learning rate decreased to 0.01817.
2024-12-02-11:32:08-root-INFO: grad norm: 34.501 34.003 5.838
2024-12-02-11:32:09-root-INFO: grad norm: 24.556 24.114 4.639
2024-12-02-11:32:09-root-INFO: grad norm: 21.738 21.289 4.396
2024-12-02-11:32:09-root-INFO: grad norm: 19.904 19.481 4.085
2024-12-02-11:32:10-root-INFO: Loss Change: 421.468 -> 398.088
2024-12-02-11:32:10-root-INFO: Regularization Change: 0.000 -> 1.675
2024-12-02-11:32:10-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-11:32:10-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-11:32:10-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-11:32:10-root-INFO: grad norm: 20.473 19.978 4.477
2024-12-02-11:32:11-root-INFO: grad norm: 25.833 25.472 4.307
2024-12-02-11:32:11-root-INFO: grad norm: 28.259 27.862 4.720
2024-12-02-11:32:12-root-INFO: grad norm: 31.689 31.352 4.613
2024-12-02-11:32:12-root-INFO: Loss too large (391.845->392.517)! Learning rate decreased to 0.01877.
2024-12-02-11:32:12-root-INFO: grad norm: 25.863 25.493 4.360
2024-12-02-11:32:13-root-INFO: Loss Change: 398.181 -> 385.002
2024-12-02-11:32:13-root-INFO: Regularization Change: 0.000 -> 1.542
2024-12-02-11:32:13-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-11:32:13-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-11:32:13-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-11:32:13-root-INFO: grad norm: 24.038 23.617 4.483
2024-12-02-11:32:14-root-INFO: grad norm: 28.267 27.902 4.529
2024-12-02-11:32:14-root-INFO: grad norm: 35.536 35.221 4.718
2024-12-02-11:32:14-root-INFO: Loss too large (383.445->385.428)! Learning rate decreased to 0.01939.
2024-12-02-11:32:15-root-INFO: grad norm: 29.054 28.721 4.390
2024-12-02-11:32:15-root-INFO: grad norm: 23.122 22.850 3.539
2024-12-02-11:32:16-root-INFO: Loss Change: 385.147 -> 374.235
2024-12-02-11:32:16-root-INFO: Regularization Change: 0.000 -> 1.291
2024-12-02-11:32:16-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-11:32:16-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-11:32:16-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-11:32:16-root-INFO: grad norm: 20.554 20.024 4.636
2024-12-02-11:32:16-root-INFO: grad norm: 23.984 23.687 3.763
2024-12-02-11:32:17-root-INFO: grad norm: 26.645 26.301 4.266
2024-12-02-11:32:17-root-INFO: grad norm: 30.344 30.066 4.099
2024-12-02-11:32:17-root-INFO: Loss too large (369.258->370.171)! Learning rate decreased to 0.02013.
2024-12-02-11:32:18-root-INFO: grad norm: 24.267 23.966 3.809
2024-12-02-11:32:18-root-INFO: Loss Change: 373.548 -> 363.262
2024-12-02-11:32:18-root-INFO: Regularization Change: 0.000 -> 1.274
2024-12-02-11:32:18-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-11:32:18-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-11:32:18-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-11:32:18-root-INFO: grad norm: 21.460 21.146 3.659
2024-12-02-11:32:19-root-INFO: grad norm: 25.333 25.024 3.945
2024-12-02-11:32:19-root-INFO: grad norm: 31.251 30.989 4.033
2024-12-02-11:32:20-root-INFO: Loss too large (362.514->364.095)! Learning rate decreased to 0.02078.
2024-12-02-11:32:20-root-INFO: grad norm: 25.621 25.334 3.824
2024-12-02-11:32:21-root-INFO: grad norm: 20.851 20.621 3.087
2024-12-02-11:32:21-root-INFO: Loss Change: 363.539 -> 355.055
2024-12-02-11:32:21-root-INFO: Regularization Change: 0.000 -> 1.090
2024-12-02-11:32:21-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-11:32:21-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-11:32:21-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-11:32:21-root-INFO: grad norm: 17.511 17.185 3.363
2024-12-02-11:32:22-root-INFO: grad norm: 21.338 21.106 3.138
2024-12-02-11:32:22-root-INFO: Loss too large (353.288->353.305)! Learning rate decreased to 0.02145.
2024-12-02-11:32:23-root-INFO: grad norm: 18.694 18.443 3.056
2024-12-02-11:32:23-root-INFO: grad norm: 16.617 16.393 2.717
2024-12-02-11:32:24-root-INFO: grad norm: 15.475 15.236 2.706
2024-12-02-11:32:24-root-INFO: Loss Change: 354.475 -> 346.628
2024-12-02-11:32:24-root-INFO: Regularization Change: 0.000 -> 0.907
2024-12-02-11:32:24-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-11:32:24-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-11:32:24-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-11:32:24-root-INFO: grad norm: 18.111 17.811 3.282
2024-12-02-11:32:25-root-INFO: grad norm: 22.272 22.009 3.412
2024-12-02-11:32:25-root-INFO: Loss too large (346.336->346.628)! Learning rate decreased to 0.02214.
2024-12-02-11:32:26-root-INFO: grad norm: 19.983 19.773 2.884
2024-12-02-11:32:26-root-INFO: grad norm: 18.560 18.331 2.910
2024-12-02-11:32:27-root-INFO: grad norm: 17.325 17.123 2.638
2024-12-02-11:32:27-root-INFO: Loss Change: 346.964 -> 339.936
2024-12-02-11:32:27-root-INFO: Regularization Change: 0.000 -> 0.910
2024-12-02-11:32:27-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-11:32:27-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-11:32:27-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-11:32:27-root-INFO: grad norm: 16.724 16.426 3.142
2024-12-02-11:32:28-root-INFO: grad norm: 21.042 20.835 2.941
2024-12-02-11:32:28-root-INFO: Loss too large (339.171->339.684)! Learning rate decreased to 0.02285.
2024-12-02-11:32:28-root-INFO: grad norm: 19.148 18.924 2.921
2024-12-02-11:32:29-root-INFO: grad norm: 17.711 17.512 2.646
2024-12-02-11:32:29-root-INFO: grad norm: 16.804 16.593 2.653
2024-12-02-11:32:30-root-INFO: Loss Change: 339.984 -> 333.333
2024-12-02-11:32:30-root-INFO: Regularization Change: 0.000 -> 0.889
2024-12-02-11:32:30-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-11:32:30-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-11:32:30-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-11:32:30-root-INFO: grad norm: 20.583 20.260 3.632
2024-12-02-11:32:31-root-INFO: grad norm: 25.331 25.079 3.560
2024-12-02-11:32:31-root-INFO: Loss too large (333.765->334.738)! Learning rate decreased to 0.02358.
2024-12-02-11:32:31-root-INFO: grad norm: 22.141 21.939 2.985
2024-12-02-11:32:32-root-INFO: grad norm: 20.246 20.037 2.907
2024-12-02-11:32:32-root-INFO: grad norm: 18.602 18.414 2.641
2024-12-02-11:32:33-root-INFO: Loss Change: 333.810 -> 326.831
2024-12-02-11:32:33-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-11:32:33-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-11:32:33-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-11:32:33-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-11:32:33-root-INFO: grad norm: 16.226 15.998 2.711
2024-12-02-11:32:33-root-INFO: grad norm: 20.985 20.793 2.833
2024-12-02-11:32:34-root-INFO: Loss too large (326.874->327.694)! Learning rate decreased to 0.02432.
2024-12-02-11:32:34-root-INFO: grad norm: 19.104 18.904 2.752
2024-12-02-11:32:34-root-INFO: grad norm: 17.584 17.398 2.553
2024-12-02-11:32:35-root-INFO: grad norm: 16.505 16.315 2.501
2024-12-02-11:32:35-root-INFO: Loss Change: 327.296 -> 321.283
2024-12-02-11:32:35-root-INFO: Regularization Change: 0.000 -> 0.878
2024-12-02-11:32:35-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-11:32:35-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-11:32:35-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-11:32:36-root-INFO: grad norm: 21.743 21.455 3.525
2024-12-02-11:32:36-root-INFO: Loss too large (322.458->322.994)! Learning rate decreased to 0.02509.
2024-12-02-11:32:36-root-INFO: grad norm: 19.302 19.114 2.690
2024-12-02-11:32:37-root-INFO: grad norm: 17.811 17.624 2.572
2024-12-02-11:32:37-root-INFO: grad norm: 16.787 16.609 2.439
2024-12-02-11:32:38-root-INFO: grad norm: 15.882 15.704 2.369
2024-12-02-11:32:38-root-INFO: Loss Change: 322.458 -> 315.306
2024-12-02-11:32:38-root-INFO: Regularization Change: 0.000 -> 0.895
2024-12-02-11:32:38-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-11:32:38-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-11:32:38-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-11:32:38-root-INFO: grad norm: 14.428 14.185 2.638
2024-12-02-11:32:39-root-INFO: grad norm: 18.161 17.972 2.612
2024-12-02-11:32:39-root-INFO: Loss too large (314.210->314.723)! Learning rate decreased to 0.02587.
2024-12-02-11:32:40-root-INFO: grad norm: 16.944 16.762 2.478
2024-12-02-11:32:40-root-INFO: grad norm: 16.058 15.878 2.399
2024-12-02-11:32:41-root-INFO: grad norm: 15.367 15.191 2.316
2024-12-02-11:32:41-root-INFO: Loss Change: 314.889 -> 309.372
2024-12-02-11:32:41-root-INFO: Regularization Change: 0.000 -> 0.872
2024-12-02-11:32:41-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-11:32:41-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-11:32:41-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-11:32:41-root-INFO: grad norm: 19.401 19.169 2.995
2024-12-02-11:32:41-root-INFO: Loss too large (310.504->311.099)! Learning rate decreased to 0.02668.
2024-12-02-11:32:42-root-INFO: grad norm: 17.719 17.543 2.497
2024-12-02-11:32:42-root-INFO: grad norm: 16.582 16.405 2.414
2024-12-02-11:32:43-root-INFO: grad norm: 15.774 15.606 2.298
2024-12-02-11:32:43-root-INFO: grad norm: 15.068 14.898 2.260
2024-12-02-11:32:44-root-INFO: Loss Change: 310.504 -> 304.207
2024-12-02-11:32:44-root-INFO: Regularization Change: 0.000 -> 0.874
2024-12-02-11:32:44-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-11:32:44-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-11:32:44-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-11:32:44-root-INFO: grad norm: 13.660 13.457 2.349
2024-12-02-11:32:44-root-INFO: grad norm: 17.261 17.092 2.414
2024-12-02-11:32:45-root-INFO: Loss too large (303.603->304.039)! Learning rate decreased to 0.02751.
2024-12-02-11:32:45-root-INFO: grad norm: 15.936 15.769 2.301
2024-12-02-11:32:45-root-INFO: grad norm: 14.961 14.796 2.211
2024-12-02-11:32:46-root-INFO: grad norm: 14.187 14.025 2.136
2024-12-02-11:32:46-root-INFO: Loss Change: 304.176 -> 298.886
2024-12-02-11:32:46-root-INFO: Regularization Change: 0.000 -> 0.877
2024-12-02-11:32:46-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-11:32:46-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-11:32:46-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-11:32:47-root-INFO: grad norm: 17.405 17.182 2.780
2024-12-02-11:32:47-root-INFO: Loss too large (299.538->299.871)! Learning rate decreased to 0.02836.
2024-12-02-11:32:47-root-INFO: grad norm: 15.852 15.691 2.254
2024-12-02-11:32:48-root-INFO: grad norm: 14.937 14.772 2.219
2024-12-02-11:32:48-root-INFO: grad norm: 14.287 14.134 2.086
2024-12-02-11:32:49-root-INFO: grad norm: 13.708 13.548 2.088
2024-12-02-11:32:49-root-INFO: Loss Change: 299.538 -> 293.710
2024-12-02-11:32:49-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-02-11:32:49-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-11:32:49-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-11:32:49-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-11:32:49-root-INFO: grad norm: 12.452 12.223 2.376
2024-12-02-11:32:50-root-INFO: grad norm: 15.038 14.879 2.177
2024-12-02-11:32:50-root-INFO: Loss too large (292.503->292.638)! Learning rate decreased to 0.02923.
2024-12-02-11:32:50-root-INFO: grad norm: 14.102 13.949 2.068
2024-12-02-11:32:51-root-INFO: grad norm: 13.510 13.356 2.032
2024-12-02-11:32:51-root-INFO: grad norm: 13.016 12.868 1.962
2024-12-02-11:32:52-root-INFO: Loss Change: 293.357 -> 288.202
2024-12-02-11:32:52-root-INFO: Regularization Change: 0.000 -> 0.897
2024-12-02-11:32:52-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-11:32:52-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-11:32:52-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-11:32:52-root-INFO: grad norm: 14.514 14.330 2.302
2024-12-02-11:32:52-root-INFO: Loss too large (288.497->288.500)! Learning rate decreased to 0.03012.
2024-12-02-11:32:53-root-INFO: grad norm: 13.554 13.397 2.056
2024-12-02-11:32:53-root-INFO: grad norm: 13.001 12.847 1.998
2024-12-02-11:32:54-root-INFO: grad norm: 12.576 12.426 1.935
2024-12-02-11:32:54-root-INFO: grad norm: 12.227 12.076 1.914
2024-12-02-11:32:54-root-INFO: Loss Change: 288.497 -> 283.218
2024-12-02-11:32:54-root-INFO: Regularization Change: 0.000 -> 0.848
2024-12-02-11:32:54-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-11:32:54-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-11:32:55-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-11:32:55-root-INFO: grad norm: 11.435 11.247 2.065
2024-12-02-11:32:55-root-INFO: grad norm: 14.335 14.190 2.033
2024-12-02-11:32:55-root-INFO: Loss too large (282.473->282.627)! Learning rate decreased to 0.03103.
2024-12-02-11:32:56-root-INFO: grad norm: 13.455 13.314 1.945
2024-12-02-11:32:56-root-INFO: grad norm: 12.803 12.659 1.915
2024-12-02-11:32:57-root-INFO: grad norm: 12.233 12.094 1.838
2024-12-02-11:32:57-root-INFO: Loss Change: 283.139 -> 278.276
2024-12-02-11:32:57-root-INFO: Regularization Change: 0.000 -> 0.907
2024-12-02-11:32:57-root-INFO: Undo step: 110
2024-12-02-11:32:57-root-INFO: Undo step: 111
2024-12-02-11:32:57-root-INFO: Undo step: 112
2024-12-02-11:32:57-root-INFO: Undo step: 113
2024-12-02-11:32:57-root-INFO: Undo step: 114
2024-12-02-11:32:57-root-INFO: Undo step: 115
2024-12-02-11:32:57-root-INFO: Undo step: 116
2024-12-02-11:32:57-root-INFO: Undo step: 117
2024-12-02-11:32:57-root-INFO: Undo step: 118
2024-12-02-11:32:57-root-INFO: Undo step: 119
2024-12-02-11:32:57-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-11:32:57-root-INFO: grad norm: 137.547 133.758 32.064
2024-12-02-11:32:58-root-INFO: grad norm: 79.666 77.552 18.229
2024-12-02-11:32:58-root-INFO: grad norm: 56.014 54.567 12.652
2024-12-02-11:32:59-root-INFO: grad norm: 45.411 44.236 10.266
2024-12-02-11:32:59-root-INFO: grad norm: 39.286 38.328 8.623
2024-12-02-11:32:59-root-INFO: Loss Change: 924.902 -> 414.948
2024-12-02-11:32:59-root-INFO: Regularization Change: 0.000 -> 54.242
2024-12-02-11:32:59-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-11:32:59-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-11:33:00-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-11:33:00-root-INFO: grad norm: 39.146 38.434 7.436
2024-12-02-11:33:00-root-INFO: grad norm: 37.813 37.197 6.802
2024-12-02-11:33:01-root-INFO: grad norm: 37.813 37.302 6.192
2024-12-02-11:33:01-root-INFO: grad norm: 38.555 38.068 6.108
2024-12-02-11:33:02-root-INFO: grad norm: 39.491 39.093 5.594
2024-12-02-11:33:02-root-INFO: Loss Change: 413.780 -> 361.659
2024-12-02-11:33:02-root-INFO: Regularization Change: 0.000 -> 9.101
2024-12-02-11:33:02-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-11:33:02-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-11:33:02-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-11:33:02-root-INFO: grad norm: 38.562 38.176 5.446
2024-12-02-11:33:03-root-INFO: grad norm: 40.072 39.741 5.143
2024-12-02-11:33:03-root-INFO: grad norm: 41.566 41.205 5.463
2024-12-02-11:33:04-root-INFO: grad norm: 42.777 42.483 5.013
2024-12-02-11:33:04-root-INFO: grad norm: 42.966 42.624 5.411
2024-12-02-11:33:04-root-INFO: Loss Change: 360.217 -> 341.996
2024-12-02-11:33:04-root-INFO: Regularization Change: 0.000 -> 4.905
2024-12-02-11:33:04-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-11:33:04-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-11:33:05-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-11:33:05-root-INFO: grad norm: 49.317 48.951 5.996
2024-12-02-11:33:05-root-INFO: grad norm: 46.088 45.759 5.499
2024-12-02-11:33:06-root-INFO: grad norm: 41.712 41.447 4.692
2024-12-02-11:33:06-root-INFO: grad norm: 39.329 39.042 4.736
2024-12-02-11:33:07-root-INFO: grad norm: 38.602 38.363 4.282
2024-12-02-11:33:07-root-INFO: Loss Change: 346.324 -> 323.552
2024-12-02-11:33:07-root-INFO: Regularization Change: 0.000 -> 4.066
2024-12-02-11:33:07-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-11:33:07-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-11:33:07-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-11:33:07-root-INFO: grad norm: 34.646 34.408 4.057
2024-12-02-11:33:08-root-INFO: grad norm: 33.972 33.749 3.888
2024-12-02-11:33:08-root-INFO: grad norm: 33.632 33.394 3.987
2024-12-02-11:33:09-root-INFO: grad norm: 33.729 33.521 3.742
2024-12-02-11:33:09-root-INFO: grad norm: 33.614 33.380 3.962
2024-12-02-11:33:09-root-INFO: Loss Change: 321.311 -> 310.751
2024-12-02-11:33:09-root-INFO: Regularization Change: 0.000 -> 2.863
2024-12-02-11:33:09-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-11:33:09-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-11:33:10-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-11:33:10-root-INFO: grad norm: 39.412 39.126 4.739
2024-12-02-11:33:10-root-INFO: grad norm: 37.164 36.923 4.233
2024-12-02-11:33:11-root-INFO: grad norm: 34.158 33.957 3.705
2024-12-02-11:33:11-root-INFO: grad norm: 32.478 32.263 3.726
2024-12-02-11:33:12-root-INFO: grad norm: 31.656 31.473 3.393
2024-12-02-11:33:12-root-INFO: Loss Change: 314.242 -> 300.289
2024-12-02-11:33:12-root-INFO: Regularization Change: 0.000 -> 2.789
2024-12-02-11:33:12-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-11:33:12-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-11:33:12-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-11:33:12-root-INFO: grad norm: 26.928 26.756 3.036
2024-12-02-11:33:13-root-INFO: grad norm: 26.692 26.521 3.017
2024-12-02-11:33:13-root-INFO: grad norm: 26.721 26.537 3.131
2024-12-02-11:33:14-root-INFO: grad norm: 27.137 26.972 2.990
2024-12-02-11:33:14-root-INFO: grad norm: 27.408 27.221 3.191
2024-12-02-11:33:14-root-INFO: Loss Change: 298.305 -> 291.114
2024-12-02-11:33:14-root-INFO: Regularization Change: 0.000 -> 2.127
2024-12-02-11:33:14-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-11:33:14-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-11:33:15-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-11:33:15-root-INFO: grad norm: 32.886 32.625 4.131
2024-12-02-11:33:15-root-INFO: grad norm: 31.443 31.243 3.538
2024-12-02-11:33:16-root-INFO: grad norm: 29.366 29.189 3.224
2024-12-02-11:33:16-root-INFO: grad norm: 28.328 28.147 3.195
2024-12-02-11:33:17-root-INFO: grad norm: 27.699 27.538 2.984
2024-12-02-11:33:17-root-INFO: Loss Change: 293.519 -> 283.870
2024-12-02-11:33:17-root-INFO: Regularization Change: 0.000 -> 2.290
2024-12-02-11:33:17-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-11:33:17-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-11:33:17-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-11:33:17-root-INFO: grad norm: 22.399 22.262 2.474
2024-12-02-11:33:18-root-INFO: grad norm: 22.200 22.052 2.558
2024-12-02-11:33:18-root-INFO: grad norm: 22.504 22.358 2.565
2024-12-02-11:33:19-root-INFO: grad norm: 23.139 22.995 2.579
2024-12-02-11:33:19-root-INFO: grad norm: 23.700 23.545 2.700
2024-12-02-11:33:19-root-INFO: Loss Change: 281.243 -> 275.603
2024-12-02-11:33:19-root-INFO: Regularization Change: 0.000 -> 1.825
2024-12-02-11:33:20-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-11:33:20-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-11:33:20-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-11:33:20-root-INFO: grad norm: 28.702 28.491 3.469
2024-12-02-11:33:20-root-INFO: grad norm: 28.123 27.951 3.106
2024-12-02-11:33:21-root-INFO: grad norm: 27.138 26.979 2.930
2024-12-02-11:33:21-root-INFO: grad norm: 26.490 26.325 2.954
2024-12-02-11:33:22-root-INFO: grad norm: 25.994 25.846 2.768
2024-12-02-11:33:22-root-INFO: Loss Change: 277.617 -> 270.620
2024-12-02-11:33:22-root-INFO: Regularization Change: 0.000 -> 2.087
2024-12-02-11:33:22-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-11:33:22-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-11:33:22-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-11:33:22-root-INFO: grad norm: 21.960 21.842 2.276
2024-12-02-11:33:23-root-INFO: grad norm: 22.171 22.028 2.508
2024-12-02-11:33:23-root-INFO: grad norm: 22.677 22.540 2.492
2024-12-02-11:33:24-root-INFO: grad norm: 23.360 23.223 2.526
2024-12-02-11:33:24-root-INFO: grad norm: 23.975 23.831 2.620
2024-12-02-11:33:25-root-INFO: Loss Change: 268.822 -> 264.422
2024-12-02-11:33:25-root-INFO: Regularization Change: 0.000 -> 1.801
2024-12-02-11:33:25-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-11:33:25-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-11:33:25-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-11:33:25-root-INFO: grad norm: 30.148 29.919 3.712
2024-12-02-11:33:25-root-INFO: grad norm: 29.291 29.120 3.158
2024-12-02-11:33:26-root-INFO: grad norm: 27.904 27.749 2.938
2024-12-02-11:33:26-root-INFO: grad norm: 27.119 26.964 2.889
2024-12-02-11:33:27-root-INFO: grad norm: 26.474 26.335 2.711
2024-12-02-11:33:27-root-INFO: Loss Change: 267.528 -> 260.624
2024-12-02-11:33:27-root-INFO: Regularization Change: 0.000 -> 2.229
2024-12-02-11:33:27-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-11:33:27-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-11:33:27-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-11:33:27-root-INFO: grad norm: 22.667 22.556 2.242
2024-12-02-11:33:28-root-INFO: grad norm: 22.278 22.149 2.400
2024-12-02-11:33:28-root-INFO: grad norm: 22.228 22.104 2.350
2024-12-02-11:33:29-root-INFO: grad norm: 22.399 22.275 2.355
2024-12-02-11:33:29-root-INFO: grad norm: 22.601 22.473 2.399
2024-12-02-11:33:30-root-INFO: Loss Change: 258.648 -> 253.637
2024-12-02-11:33:30-root-INFO: Regularization Change: 0.000 -> 1.826
2024-12-02-11:33:30-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-11:33:30-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-11:33:30-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-11:33:30-root-INFO: grad norm: 29.170 28.939 3.660
2024-12-02-11:33:30-root-INFO: grad norm: 28.182 28.024 2.980
2024-12-02-11:33:31-root-INFO: grad norm: 26.829 26.680 2.824
2024-12-02-11:33:31-root-INFO: grad norm: 26.031 25.888 2.729
2024-12-02-11:33:32-root-INFO: grad norm: 25.308 25.175 2.597
2024-12-02-11:33:32-root-INFO: Loss Change: 257.043 -> 250.441
2024-12-02-11:33:32-root-INFO: Regularization Change: 0.000 -> 2.244
2024-12-02-11:33:32-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-11:33:32-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-11:33:32-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-11:33:32-root-INFO: grad norm: 21.137 21.038 2.044
2024-12-02-11:33:33-root-INFO: grad norm: 20.956 20.828 2.307
2024-12-02-11:33:33-root-INFO: grad norm: 21.248 21.132 2.217
2024-12-02-11:33:34-root-INFO: grad norm: 21.744 21.619 2.321
2024-12-02-11:33:34-root-INFO: grad norm: 22.217 22.092 2.354
2024-12-02-11:33:35-root-INFO: Loss Change: 248.246 -> 244.207
2024-12-02-11:33:35-root-INFO: Regularization Change: 0.000 -> 1.837
2024-12-02-11:33:35-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-11:33:35-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-11:33:35-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-11:33:35-root-INFO: grad norm: 27.190 26.987 3.313
2024-12-02-11:33:35-root-INFO: grad norm: 26.823 26.675 2.821
2024-12-02-11:33:36-root-INFO: grad norm: 26.157 26.007 2.796
2024-12-02-11:33:37-root-INFO: grad norm: 25.563 25.419 2.714
2024-12-02-11:33:37-root-INFO: grad norm: 24.929 24.790 2.635
2024-12-02-11:33:37-root-INFO: Loss Change: 246.905 -> 241.735
2024-12-02-11:33:37-root-INFO: Regularization Change: 0.000 -> 2.262
2024-12-02-11:33:37-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-11:33:37-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-11:33:38-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-11:33:38-root-INFO: grad norm: 20.576 20.484 1.945
2024-12-02-11:33:38-root-INFO: grad norm: 19.960 19.837 2.209
2024-12-02-11:33:39-root-INFO: grad norm: 19.786 19.672 2.122
2024-12-02-11:33:39-root-INFO: grad norm: 19.829 19.706 2.201
2024-12-02-11:33:40-root-INFO: grad norm: 19.913 19.791 2.201
2024-12-02-11:33:40-root-INFO: Loss Change: 239.587 -> 234.825
2024-12-02-11:33:40-root-INFO: Regularization Change: 0.000 -> 1.800
2024-12-02-11:33:40-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-11:33:40-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-11:33:40-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-11:33:40-root-INFO: grad norm: 24.325 24.148 2.930
2024-12-02-11:33:41-root-INFO: grad norm: 23.600 23.453 2.632
2024-12-02-11:33:41-root-INFO: grad norm: 22.726 22.585 2.532
2024-12-02-11:33:42-root-INFO: grad norm: 22.060 21.919 2.494
2024-12-02-11:33:42-root-INFO: grad norm: 21.422 21.288 2.395
2024-12-02-11:33:42-root-INFO: Loss Change: 236.998 -> 231.815
2024-12-02-11:33:42-root-INFO: Regularization Change: 0.000 -> 2.071
2024-12-02-11:33:42-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-11:33:42-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-11:33:43-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-11:33:43-root-INFO: grad norm: 18.209 18.118 1.819
2024-12-02-11:33:43-root-INFO: grad norm: 18.097 17.968 2.159
2024-12-02-11:33:44-root-INFO: grad norm: 18.273 18.159 2.033
2024-12-02-11:33:44-root-INFO: grad norm: 18.590 18.462 2.173
2024-12-02-11:33:45-root-INFO: grad norm: 18.905 18.783 2.146
2024-12-02-11:33:45-root-INFO: Loss Change: 230.196 -> 226.396
2024-12-02-11:33:45-root-INFO: Regularization Change: 0.000 -> 1.746
2024-12-02-11:33:45-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-11:33:45-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-11:33:45-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-11:33:45-root-INFO: grad norm: 23.687 23.501 2.964
2024-12-02-11:33:46-root-INFO: grad norm: 23.075 22.930 2.578
2024-12-02-11:33:46-root-INFO: grad norm: 22.226 22.081 2.530
2024-12-02-11:33:47-root-INFO: grad norm: 21.642 21.503 2.442
2024-12-02-11:33:47-root-INFO: grad norm: 21.042 20.906 2.390
2024-12-02-11:33:48-root-INFO: Loss Change: 228.819 -> 223.969
2024-12-02-11:33:48-root-INFO: Regularization Change: 0.000 -> 2.158
2024-12-02-11:33:48-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-11:33:48-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-11:33:48-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-11:33:48-root-INFO: grad norm: 17.452 17.357 1.816
2024-12-02-11:33:48-root-INFO: grad norm: 17.279 17.155 2.064
2024-12-02-11:33:49-root-INFO: grad norm: 17.307 17.194 1.972
2024-12-02-11:33:49-root-INFO: grad norm: 17.433 17.310 2.067
2024-12-02-11:33:50-root-INFO: grad norm: 17.593 17.476 2.031
2024-12-02-11:33:50-root-INFO: Loss Change: 222.300 -> 218.456
2024-12-02-11:33:50-root-INFO: Regularization Change: 0.000 -> 1.743
2024-12-02-11:33:50-root-INFO: Undo step: 100
2024-12-02-11:33:50-root-INFO: Undo step: 101
2024-12-02-11:33:50-root-INFO: Undo step: 102
2024-12-02-11:33:50-root-INFO: Undo step: 103
2024-12-02-11:33:50-root-INFO: Undo step: 104
2024-12-02-11:33:50-root-INFO: Undo step: 105
2024-12-02-11:33:50-root-INFO: Undo step: 106
2024-12-02-11:33:50-root-INFO: Undo step: 107
2024-12-02-11:33:50-root-INFO: Undo step: 108
2024-12-02-11:33:50-root-INFO: Undo step: 109
2024-12-02-11:33:50-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-11:33:50-root-INFO: grad norm: 127.057 125.319 20.945
2024-12-02-11:33:51-root-INFO: grad norm: 67.692 66.988 9.737
2024-12-02-11:33:51-root-INFO: grad norm: 45.129 44.297 8.625
2024-12-02-11:33:52-root-INFO: grad norm: 37.623 37.159 5.888
2024-12-02-11:33:52-root-INFO: grad norm: 35.341 34.782 6.262
2024-12-02-11:33:53-root-INFO: Loss Change: 717.073 -> 318.261
2024-12-02-11:33:53-root-INFO: Regularization Change: 0.000 -> 54.518
2024-12-02-11:33:53-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-11:33:53-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-11:33:53-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-11:33:53-root-INFO: grad norm: 38.315 37.936 5.371
2024-12-02-11:33:53-root-INFO: grad norm: 36.003 35.533 5.799
2024-12-02-11:33:54-root-INFO: grad norm: 31.902 31.630 4.157
2024-12-02-11:33:54-root-INFO: grad norm: 31.002 30.615 4.887
2024-12-02-11:33:55-root-INFO: grad norm: 30.608 30.374 3.777
2024-12-02-11:33:55-root-INFO: Loss Change: 320.929 -> 283.768
2024-12-02-11:33:55-root-INFO: Regularization Change: 0.000 -> 8.838
2024-12-02-11:33:55-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-11:33:55-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-11:33:55-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-11:33:55-root-INFO: grad norm: 27.019 26.728 3.954
2024-12-02-11:33:56-root-INFO: grad norm: 25.284 25.074 3.252
2024-12-02-11:33:56-root-INFO: grad norm: 24.529 24.255 3.657
2024-12-02-11:33:57-root-INFO: grad norm: 23.788 23.599 2.986
2024-12-02-11:33:57-root-INFO: grad norm: 23.339 23.086 3.424
2024-12-02-11:33:58-root-INFO: Loss Change: 281.420 -> 261.523
2024-12-02-11:33:58-root-INFO: Regularization Change: 0.000 -> 4.483
2024-12-02-11:33:58-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-11:33:58-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-11:33:58-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-11:33:58-root-INFO: grad norm: 29.387 29.119 3.963
2024-12-02-11:33:59-root-INFO: grad norm: 28.366 28.102 3.865
2024-12-02-11:33:59-root-INFO: grad norm: 27.108 26.910 3.269
2024-12-02-11:33:59-root-INFO: grad norm: 26.809 26.568 3.591
2024-12-02-11:34:00-root-INFO: grad norm: 26.630 26.445 3.132
2024-12-02-11:34:00-root-INFO: Loss Change: 264.505 -> 253.771
2024-12-02-11:34:00-root-INFO: Regularization Change: 0.000 -> 3.571
2024-12-02-11:34:00-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-11:34:00-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-11:34:00-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-11:34:01-root-INFO: grad norm: 22.978 22.809 2.779
2024-12-02-11:34:01-root-INFO: grad norm: 23.257 23.083 2.843
2024-12-02-11:34:01-root-INFO: grad norm: 23.966 23.776 3.011
2024-12-02-11:34:02-root-INFO: grad norm: 25.012 24.841 2.923
2024-12-02-11:34:02-root-INFO: Loss too large (246.008->246.097)! Learning rate decreased to 0.03491.
2024-12-02-11:34:03-root-INFO: grad norm: 17.687 17.520 2.424
2024-12-02-11:34:03-root-INFO: Loss Change: 251.004 -> 238.943
2024-12-02-11:34:03-root-INFO: Regularization Change: 0.000 -> 2.027
2024-12-02-11:34:03-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-11:34:03-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-11:34:03-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-11:34:03-root-INFO: grad norm: 17.602 17.412 2.580
2024-12-02-11:34:04-root-INFO: grad norm: 19.492 19.328 2.517
2024-12-02-11:34:04-root-INFO: grad norm: 22.365 22.202 2.695
2024-12-02-11:34:04-root-INFO: Loss too large (239.380->240.231)! Learning rate decreased to 0.03594.
2024-12-02-11:34:05-root-INFO: grad norm: 17.206 17.057 2.254
2024-12-02-11:34:05-root-INFO: grad norm: 13.395 13.266 1.856
2024-12-02-11:34:06-root-INFO: Loss Change: 240.395 -> 232.624
2024-12-02-11:34:06-root-INFO: Regularization Change: 0.000 -> 1.461
2024-12-02-11:34:06-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-11:34:06-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-11:34:06-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-11:34:06-root-INFO: grad norm: 8.494 8.343 1.593
2024-12-02-11:34:06-root-INFO: grad norm: 8.920 8.791 1.509
2024-12-02-11:34:07-root-INFO: grad norm: 10.597 10.488 1.516
2024-12-02-11:34:07-root-INFO: grad norm: 13.194 13.075 1.764
2024-12-02-11:34:08-root-INFO: grad norm: 16.201 16.085 1.941
2024-12-02-11:34:08-root-INFO: Loss too large (228.962->229.174)! Learning rate decreased to 0.03700.
2024-12-02-11:34:08-root-INFO: Loss Change: 231.716 -> 227.111
2024-12-02-11:34:08-root-INFO: Regularization Change: 0.000 -> 1.401
2024-12-02-11:34:08-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-11:34:08-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-11:34:08-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-11:34:09-root-INFO: grad norm: 17.246 17.093 2.297
2024-12-02-11:34:09-root-INFO: Loss too large (228.208->228.820)! Learning rate decreased to 0.03808.
2024-12-02-11:34:09-root-INFO: grad norm: 14.340 14.224 1.821
2024-12-02-11:34:10-root-INFO: grad norm: 12.131 12.017 1.658
2024-12-02-11:34:10-root-INFO: grad norm: 10.670 10.563 1.508
2024-12-02-11:34:11-root-INFO: grad norm: 9.525 9.417 1.429
2024-12-02-11:34:11-root-INFO: Loss Change: 228.208 -> 221.719
2024-12-02-11:34:11-root-INFO: Regularization Change: 0.000 -> 1.040
2024-12-02-11:34:11-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-11:34:11-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-11:34:11-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-11:34:11-root-INFO: grad norm: 7.084 6.943 1.406
2024-12-02-11:34:12-root-INFO: grad norm: 7.710 7.595 1.327
2024-12-02-11:34:12-root-INFO: grad norm: 9.589 9.498 1.322
2024-12-02-11:34:13-root-INFO: grad norm: 12.449 12.349 1.574
2024-12-02-11:34:13-root-INFO: Loss too large (219.177->219.446)! Learning rate decreased to 0.03918.
2024-12-02-11:34:13-root-INFO: grad norm: 11.286 11.195 1.430
2024-12-02-11:34:14-root-INFO: Loss Change: 221.123 -> 217.227
2024-12-02-11:34:14-root-INFO: Regularization Change: 0.000 -> 1.101
2024-12-02-11:34:14-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-11:34:14-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-11:34:14-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-11:34:14-root-INFO: grad norm: 14.243 14.106 1.970
2024-12-02-11:34:14-root-INFO: Loss too large (218.130->218.734)! Learning rate decreased to 0.04031.
2024-12-02-11:34:15-root-INFO: grad norm: 12.771 12.676 1.550
2024-12-02-11:34:15-root-INFO: grad norm: 11.655 11.555 1.522
2024-12-02-11:34:15-root-INFO: grad norm: 10.831 10.742 1.388
2024-12-02-11:34:16-root-INFO: grad norm: 10.157 10.063 1.378
2024-12-02-11:34:16-root-INFO: Loss Change: 218.130 -> 213.342
2024-12-02-11:34:16-root-INFO: Regularization Change: 0.000 -> 0.946
2024-12-02-11:34:16-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-11:34:16-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-11:34:16-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-11:34:17-root-INFO: grad norm: 7.729 7.629 1.235
2024-12-02-11:34:17-root-INFO: grad norm: 9.303 9.216 1.265
2024-12-02-11:34:18-root-INFO: grad norm: 12.082 12.008 1.337
2024-12-02-11:34:18-root-INFO: Loss too large (212.060->212.457)! Learning rate decreased to 0.04147.
2024-12-02-11:34:18-root-INFO: grad norm: 10.976 10.891 1.359
2024-12-02-11:34:19-root-INFO: grad norm: 10.150 10.072 1.261
2024-12-02-11:34:19-root-INFO: Loss Change: 212.889 -> 209.509
2024-12-02-11:34:19-root-INFO: Regularization Change: 0.000 -> 0.956
2024-12-02-11:34:19-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-11:34:19-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-11:34:19-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-11:34:19-root-INFO: grad norm: 12.702 12.585 1.720
2024-12-02-11:34:19-root-INFO: Loss too large (210.058->210.533)! Learning rate decreased to 0.04265.
2024-12-02-11:34:20-root-INFO: grad norm: 11.593 11.512 1.368
2024-12-02-11:34:20-root-INFO: grad norm: 10.803 10.716 1.368
2024-12-02-11:34:21-root-INFO: grad norm: 10.174 10.097 1.251
2024-12-02-11:34:21-root-INFO: grad norm: 9.668 9.585 1.261
2024-12-02-11:34:22-root-INFO: Loss Change: 210.058 -> 205.909
2024-12-02-11:34:22-root-INFO: Regularization Change: 0.000 -> 0.900
2024-12-02-11:34:22-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-11:34:22-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-11:34:22-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-11:34:22-root-INFO: grad norm: 7.375 7.277 1.200
2024-12-02-11:34:22-root-INFO: grad norm: 8.863 8.788 1.150
2024-12-02-11:34:23-root-INFO: grad norm: 11.660 11.597 1.212
2024-12-02-11:34:23-root-INFO: Loss too large (204.738->205.251)! Learning rate decreased to 0.04386.
2024-12-02-11:34:24-root-INFO: grad norm: 10.684 10.611 1.243
2024-12-02-11:34:24-root-INFO: grad norm: 9.955 9.885 1.176
2024-12-02-11:34:24-root-INFO: Loss Change: 205.469 -> 202.429
2024-12-02-11:34:24-root-INFO: Regularization Change: 0.000 -> 0.947
2024-12-02-11:34:24-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-11:34:24-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-11:34:24-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-11:34:25-root-INFO: grad norm: 13.192 13.077 1.739
2024-12-02-11:34:25-root-INFO: Loss too large (203.234->204.019)! Learning rate decreased to 0.04509.
2024-12-02-11:34:25-root-INFO: grad norm: 12.180 12.106 1.338
2024-12-02-11:34:26-root-INFO: grad norm: 11.456 11.378 1.338
2024-12-02-11:34:26-root-INFO: grad norm: 10.868 10.798 1.224
2024-12-02-11:34:27-root-INFO: grad norm: 10.395 10.322 1.228
2024-12-02-11:34:27-root-INFO: Loss Change: 203.234 -> 199.234
2024-12-02-11:34:27-root-INFO: Regularization Change: 0.000 -> 0.947
2024-12-02-11:34:27-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-11:34:27-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-11:34:27-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-11:34:27-root-INFO: grad norm: 8.323 8.250 1.101
2024-12-02-11:34:28-root-INFO: grad norm: 10.536 10.473 1.149
2024-12-02-11:34:28-root-INFO: Loss too large (198.591->198.801)! Learning rate decreased to 0.04636.
2024-12-02-11:34:28-root-INFO: grad norm: 9.688 9.627 1.083
2024-12-02-11:34:29-root-INFO: grad norm: 9.261 9.195 1.106
2024-12-02-11:34:29-root-INFO: grad norm: 8.897 8.834 1.053
2024-12-02-11:34:29-root-INFO: Loss Change: 198.761 -> 195.805
2024-12-02-11:34:30-root-INFO: Regularization Change: 0.000 -> 0.877
2024-12-02-11:34:30-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-11:34:30-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-11:34:30-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-11:34:30-root-INFO: grad norm: 12.264 12.160 1.591
2024-12-02-11:34:30-root-INFO: Loss too large (196.891->197.632)! Learning rate decreased to 0.04765.
2024-12-02-11:34:30-root-INFO: grad norm: 11.481 11.415 1.231
2024-12-02-11:34:31-root-INFO: grad norm: 10.912 10.841 1.249
2024-12-02-11:34:31-root-INFO: grad norm: 10.416 10.353 1.140
2024-12-02-11:34:32-root-INFO: grad norm: 10.017 9.950 1.153
2024-12-02-11:34:32-root-INFO: Loss Change: 196.891 -> 193.237
2024-12-02-11:34:32-root-INFO: Regularization Change: 0.000 -> 0.936
2024-12-02-11:34:32-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-11:34:32-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-11:34:32-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-11:34:32-root-INFO: grad norm: 8.105 8.031 1.091
2024-12-02-11:34:33-root-INFO: grad norm: 10.282 10.227 1.065
2024-12-02-11:34:33-root-INFO: Loss too large (192.616->192.873)! Learning rate decreased to 0.04897.
2024-12-02-11:34:34-root-INFO: grad norm: 9.551 9.496 1.025
2024-12-02-11:34:34-root-INFO: grad norm: 9.242 9.184 1.035
2024-12-02-11:34:34-root-INFO: grad norm: 8.959 8.904 0.998
2024-12-02-11:34:35-root-INFO: Loss Change: 192.710 -> 190.022
2024-12-02-11:34:35-root-INFO: Regularization Change: 0.000 -> 0.893
2024-12-02-11:34:35-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-11:34:35-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-11:34:35-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-11:34:35-root-INFO: grad norm: 12.277 12.178 1.549
2024-12-02-11:34:35-root-INFO: Loss too large (190.841->191.648)! Learning rate decreased to 0.05031.
2024-12-02-11:34:36-root-INFO: grad norm: 11.490 11.430 1.166
2024-12-02-11:34:36-root-INFO: grad norm: 10.902 10.836 1.199
2024-12-02-11:34:37-root-INFO: grad norm: 10.361 10.305 1.075
2024-12-02-11:34:37-root-INFO: grad norm: 9.947 9.886 1.100
2024-12-02-11:34:37-root-INFO: Loss Change: 190.841 -> 187.159
2024-12-02-11:34:37-root-INFO: Regularization Change: 0.000 -> 0.979
2024-12-02-11:34:37-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-11:34:37-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-11:34:38-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-11:34:38-root-INFO: grad norm: 8.048 7.982 1.031
2024-12-02-11:34:38-root-INFO: grad norm: 10.023 9.970 1.023
2024-12-02-11:34:38-root-INFO: Loss too large (186.784->186.929)! Learning rate decreased to 0.05169.
2024-12-02-11:34:39-root-INFO: grad norm: 9.029 8.976 0.980
2024-12-02-11:34:39-root-INFO: grad norm: 8.588 8.533 0.969
2024-12-02-11:34:40-root-INFO: grad norm: 8.160 8.106 0.937
2024-12-02-11:34:40-root-INFO: Loss Change: 186.841 -> 184.067
2024-12-02-11:34:40-root-INFO: Regularization Change: 0.000 -> 0.913
2024-12-02-11:34:40-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-11:34:40-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-11:34:40-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-11:34:41-root-INFO: grad norm: 10.603 10.524 1.293
2024-12-02-11:34:41-root-INFO: Loss too large (184.691->185.252)! Learning rate decreased to 0.05310.
2024-12-02-11:34:41-root-INFO: grad norm: 9.976 9.922 1.040
2024-12-02-11:34:42-root-INFO: grad norm: 9.503 9.444 1.060
2024-12-02-11:34:42-root-INFO: grad norm: 9.074 9.022 0.967
2024-12-02-11:34:43-root-INFO: grad norm: 8.749 8.693 0.987
2024-12-02-11:34:43-root-INFO: Loss Change: 184.691 -> 181.408
2024-12-02-11:34:43-root-INFO: Regularization Change: 0.000 -> 0.923
2024-12-02-11:34:43-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-11:34:43-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-11:34:43-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-11:34:43-root-INFO: grad norm: 6.820 6.761 0.892
2024-12-02-11:34:44-root-INFO: grad norm: 8.520 8.473 0.901
2024-12-02-11:34:44-root-INFO: Loss too large (180.958->180.979)! Learning rate decreased to 0.05453.
2024-12-02-11:34:44-root-INFO: grad norm: 7.872 7.827 0.848
2024-12-02-11:34:45-root-INFO: grad norm: 7.608 7.556 0.884
2024-12-02-11:34:45-root-INFO: grad norm: 7.323 7.275 0.832
2024-12-02-11:34:46-root-INFO: Loss Change: 181.121 -> 178.588
2024-12-02-11:34:46-root-INFO: Regularization Change: 0.000 -> 0.888
2024-12-02-11:34:46-root-INFO: Undo step: 90
2024-12-02-11:34:46-root-INFO: Undo step: 91
2024-12-02-11:34:46-root-INFO: Undo step: 92
2024-12-02-11:34:46-root-INFO: Undo step: 93
2024-12-02-11:34:46-root-INFO: Undo step: 94
2024-12-02-11:34:46-root-INFO: Undo step: 95
2024-12-02-11:34:46-root-INFO: Undo step: 96
2024-12-02-11:34:46-root-INFO: Undo step: 97
2024-12-02-11:34:46-root-INFO: Undo step: 98
2024-12-02-11:34:46-root-INFO: Undo step: 99
2024-12-02-11:34:46-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-11:34:46-root-INFO: grad norm: 87.821 86.667 14.187
2024-12-02-11:34:46-root-INFO: grad norm: 55.356 54.696 8.522
2024-12-02-11:34:47-root-INFO: grad norm: 31.803 31.195 6.189
2024-12-02-11:34:47-root-INFO: grad norm: 27.889 27.507 4.601
2024-12-02-11:34:48-root-INFO: grad norm: 26.178 25.787 4.508
2024-12-02-11:34:48-root-INFO: Loss Change: 617.486 -> 272.645
2024-12-02-11:34:48-root-INFO: Regularization Change: 0.000 -> 69.453
2024-12-02-11:34:48-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-11:34:48-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-11:34:48-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-11:34:48-root-INFO: grad norm: 29.150 28.919 3.659
2024-12-02-11:34:49-root-INFO: grad norm: 31.592 31.311 4.202
2024-12-02-11:34:49-root-INFO: grad norm: 33.342 33.165 3.435
2024-12-02-11:34:50-root-INFO: grad norm: 34.543 34.292 4.155
2024-12-02-11:34:50-root-INFO: grad norm: 34.398 34.232 3.376
2024-12-02-11:34:51-root-INFO: Loss Change: 274.048 -> 247.451
2024-12-02-11:34:51-root-INFO: Regularization Change: 0.000 -> 11.465
2024-12-02-11:34:51-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-11:34:51-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-11:34:51-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-11:34:51-root-INFO: grad norm: 29.922 29.731 3.377
2024-12-02-11:34:51-root-INFO: grad norm: 28.982 28.827 2.995
2024-12-02-11:34:52-root-INFO: grad norm: 27.984 27.776 3.404
2024-12-02-11:34:52-root-INFO: grad norm: 27.262 27.110 2.884
2024-12-02-11:34:53-root-INFO: grad norm: 26.744 26.534 3.343
2024-12-02-11:34:53-root-INFO: Loss Change: 244.393 -> 226.224
2024-12-02-11:34:53-root-INFO: Regularization Change: 0.000 -> 6.493
2024-12-02-11:34:53-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-11:34:53-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-11:34:53-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-11:34:54-root-INFO: grad norm: 30.400 30.199 3.490
2024-12-02-11:34:54-root-INFO: grad norm: 28.720 28.489 3.630
2024-12-02-11:34:54-root-INFO: grad norm: 27.044 26.876 3.014
2024-12-02-11:34:55-root-INFO: grad norm: 26.026 25.812 3.335
2024-12-02-11:34:55-root-INFO: grad norm: 25.162 24.999 2.852
2024-12-02-11:34:56-root-INFO: Loss Change: 229.313 -> 215.577
2024-12-02-11:34:56-root-INFO: Regularization Change: 0.000 -> 5.100
2024-12-02-11:34:56-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-11:34:56-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-11:34:56-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-11:34:56-root-INFO: grad norm: 21.735 21.569 2.682
2024-12-02-11:34:56-root-INFO: grad norm: 21.326 21.180 2.487
2024-12-02-11:34:57-root-INFO: grad norm: 21.169 20.993 2.726
2024-12-02-11:34:57-root-INFO: grad norm: 21.147 21.005 2.449
2024-12-02-11:34:58-root-INFO: grad norm: 21.229 21.053 2.732
2024-12-02-11:34:58-root-INFO: Loss Change: 213.495 -> 205.732
2024-12-02-11:34:58-root-INFO: Regularization Change: 0.000 -> 3.601
2024-12-02-11:34:58-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-11:34:58-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-11:34:58-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-11:34:59-root-INFO: grad norm: 25.181 24.996 3.041
2024-12-02-11:34:59-root-INFO: grad norm: 24.446 24.252 3.070
2024-12-02-11:34:59-root-INFO: grad norm: 23.571 23.419 2.671
2024-12-02-11:35:00-root-INFO: grad norm: 22.881 22.700 2.871
2024-12-02-11:35:00-root-INFO: grad norm: 22.255 22.111 2.521
2024-12-02-11:35:01-root-INFO: Loss Change: 208.616 -> 200.676
2024-12-02-11:35:01-root-INFO: Regularization Change: 0.000 -> 3.664
2024-12-02-11:35:01-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-11:35:01-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-11:35:01-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-11:35:01-root-INFO: grad norm: 18.913 18.780 2.237
2024-12-02-11:35:02-root-INFO: grad norm: 18.807 18.681 2.176
2024-12-02-11:35:02-root-INFO: grad norm: 18.884 18.736 2.356
2024-12-02-11:35:02-root-INFO: grad norm: 19.089 18.963 2.191
2024-12-02-11:35:03-root-INFO: grad norm: 19.345 19.193 2.419
2024-12-02-11:35:03-root-INFO: Loss Change: 198.400 -> 193.823
2024-12-02-11:35:03-root-INFO: Regularization Change: 0.000 -> 2.838
2024-12-02-11:35:03-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-11:35:03-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-11:35:03-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-11:35:04-root-INFO: grad norm: 23.480 23.300 2.903
2024-12-02-11:35:04-root-INFO: grad norm: 22.837 22.666 2.789
2024-12-02-11:35:04-root-INFO: grad norm: 22.008 21.866 2.494
2024-12-02-11:35:05-root-INFO: grad norm: 21.328 21.170 2.590
2024-12-02-11:35:05-root-INFO: grad norm: 20.735 20.602 2.343
2024-12-02-11:35:06-root-INFO: Loss Change: 196.313 -> 189.829
2024-12-02-11:35:06-root-INFO: Regularization Change: 0.000 -> 3.234
2024-12-02-11:35:06-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-11:35:06-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-11:35:06-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-11:35:06-root-INFO: grad norm: 17.848 17.729 2.057
2024-12-02-11:35:06-root-INFO: grad norm: 17.624 17.510 2.000
2024-12-02-11:35:07-root-INFO: grad norm: 17.588 17.457 2.141
2024-12-02-11:35:07-root-INFO: grad norm: 17.705 17.590 2.009
2024-12-02-11:35:08-root-INFO: grad norm: 17.868 17.734 2.191
2024-12-02-11:35:08-root-INFO: Loss Change: 188.361 -> 184.523
2024-12-02-11:35:08-root-INFO: Regularization Change: 0.000 -> 2.571
2024-12-02-11:35:08-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-11:35:08-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-11:35:08-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-11:35:09-root-INFO: grad norm: 21.165 21.012 2.545
2024-12-02-11:35:09-root-INFO: grad norm: 20.755 20.603 2.509
2024-12-02-11:35:10-root-INFO: grad norm: 20.133 20.003 2.277
2024-12-02-11:35:10-root-INFO: grad norm: 19.601 19.459 2.357
2024-12-02-11:35:11-root-INFO: grad norm: 19.146 19.023 2.169
2024-12-02-11:35:11-root-INFO: Loss Change: 186.547 -> 181.396
2024-12-02-11:35:11-root-INFO: Regularization Change: 0.000 -> 2.916
2024-12-02-11:35:11-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-11:35:11-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-11:35:11-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-11:35:11-root-INFO: grad norm: 16.011 15.915 1.748
2024-12-02-11:35:12-root-INFO: grad norm: 15.668 15.561 1.821
2024-12-02-11:35:12-root-INFO: grad norm: 15.504 15.395 1.836
2024-12-02-11:35:13-root-INFO: grad norm: 15.503 15.397 1.806
2024-12-02-11:35:13-root-INFO: grad norm: 15.534 15.420 1.872
2024-12-02-11:35:14-root-INFO: Loss Change: 179.790 -> 176.208
2024-12-02-11:35:14-root-INFO: Regularization Change: 0.000 -> 2.274
2024-12-02-11:35:14-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-11:35:14-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-11:35:14-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-11:35:14-root-INFO: grad norm: 19.765 19.603 2.526
2024-12-02-11:35:14-root-INFO: grad norm: 19.194 19.052 2.330
2024-12-02-11:35:15-root-INFO: grad norm: 18.320 18.204 2.066
2024-12-02-11:35:15-root-INFO: grad norm: 17.733 17.611 2.077
2024-12-02-11:35:16-root-INFO: grad norm: 17.371 17.258 1.976
2024-12-02-11:35:16-root-INFO: Loss Change: 178.452 -> 173.520
2024-12-02-11:35:16-root-INFO: Regularization Change: 0.000 -> 2.745
2024-12-02-11:35:16-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-11:35:16-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-11:35:16-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-11:35:17-root-INFO: grad norm: 14.502 14.415 1.590
2024-12-02-11:35:17-root-INFO: grad norm: 14.064 13.966 1.651
2024-12-02-11:35:17-root-INFO: grad norm: 13.797 13.700 1.636
2024-12-02-11:35:18-root-INFO: grad norm: 13.682 13.587 1.611
2024-12-02-11:35:18-root-INFO: grad norm: 13.594 13.495 1.640
2024-12-02-11:35:19-root-INFO: Loss Change: 172.407 -> 168.848
2024-12-02-11:35:19-root-INFO: Regularization Change: 0.000 -> 2.089
2024-12-02-11:35:19-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-11:35:19-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-11:35:19-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-11:35:19-root-INFO: grad norm: 17.396 17.243 2.302
2024-12-02-11:35:20-root-INFO: grad norm: 16.491 16.370 1.988
2024-12-02-11:35:20-root-INFO: grad norm: 15.396 15.297 1.751
2024-12-02-11:35:20-root-INFO: grad norm: 14.728 14.631 1.689
2024-12-02-11:35:21-root-INFO: grad norm: 14.393 14.298 1.654
2024-12-02-11:35:21-root-INFO: Loss Change: 170.722 -> 165.784
2024-12-02-11:35:21-root-INFO: Regularization Change: 0.000 -> 2.408
2024-12-02-11:35:21-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-11:35:21-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-11:35:22-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-11:35:22-root-INFO: grad norm: 11.388 11.322 1.226
2024-12-02-11:35:22-root-INFO: grad norm: 11.072 10.991 1.333
2024-12-02-11:35:23-root-INFO: grad norm: 10.908 10.829 1.311
2024-12-02-11:35:23-root-INFO: grad norm: 10.941 10.859 1.334
2024-12-02-11:35:24-root-INFO: grad norm: 10.928 10.845 1.350
2024-12-02-11:35:24-root-INFO: Loss Change: 164.501 -> 161.421
2024-12-02-11:35:24-root-INFO: Regularization Change: 0.000 -> 1.758
2024-12-02-11:35:24-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-11:35:24-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-11:35:24-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-11:35:24-root-INFO: grad norm: 14.000 13.871 1.902
2024-12-02-11:35:25-root-INFO: grad norm: 13.412 13.315 1.608
2024-12-02-11:35:25-root-INFO: grad norm: 12.651 12.565 1.474
2024-12-02-11:35:26-root-INFO: grad norm: 12.310 12.230 1.402
2024-12-02-11:35:26-root-INFO: grad norm: 12.309 12.224 1.445
2024-12-02-11:35:26-root-INFO: Loss Change: 162.785 -> 159.093
2024-12-02-11:35:26-root-INFO: Regularization Change: 0.000 -> 2.076
2024-12-02-11:35:26-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-11:35:26-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-11:35:27-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-11:35:27-root-INFO: grad norm: 10.300 10.236 1.151
2024-12-02-11:35:27-root-INFO: grad norm: 10.035 9.960 1.225
2024-12-02-11:35:28-root-INFO: grad norm: 9.979 9.905 1.207
2024-12-02-11:35:28-root-INFO: grad norm: 10.141 10.063 1.249
2024-12-02-11:35:29-root-INFO: grad norm: 10.143 10.064 1.262
2024-12-02-11:35:29-root-INFO: Loss Change: 158.160 -> 155.326
2024-12-02-11:35:29-root-INFO: Regularization Change: 0.000 -> 1.699
2024-12-02-11:35:29-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-11:35:29-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-11:35:29-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-11:35:29-root-INFO: grad norm: 12.921 12.806 1.720
2024-12-02-11:35:30-root-INFO: grad norm: 12.419 12.329 1.491
2024-12-02-11:35:30-root-INFO: grad norm: 11.777 11.694 1.392
2024-12-02-11:35:31-root-INFO: grad norm: 11.439 11.360 1.346
2024-12-02-11:35:31-root-INFO: grad norm: 11.375 11.294 1.356
2024-12-02-11:35:31-root-INFO: Loss Change: 156.562 -> 153.090
2024-12-02-11:35:31-root-INFO: Regularization Change: 0.000 -> 2.024
2024-12-02-11:35:31-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-11:35:31-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-11:35:32-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-11:35:32-root-INFO: grad norm: 9.172 9.113 1.041
2024-12-02-11:35:32-root-INFO: grad norm: 8.861 8.790 1.117
2024-12-02-11:35:33-root-INFO: grad norm: 8.756 8.687 1.096
2024-12-02-11:35:33-root-INFO: grad norm: 8.876 8.803 1.133
2024-12-02-11:35:34-root-INFO: grad norm: 8.781 8.708 1.131
2024-12-02-11:35:34-root-INFO: Loss Change: 152.408 -> 149.524
2024-12-02-11:35:34-root-INFO: Regularization Change: 0.000 -> 1.602
2024-12-02-11:35:34-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-11:35:34-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-11:35:34-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-11:35:34-root-INFO: grad norm: 11.460 11.346 1.614
2024-12-02-11:35:35-root-INFO: grad norm: 10.822 10.742 1.311
2024-12-02-11:35:35-root-INFO: grad norm: 10.040 9.964 1.238
2024-12-02-11:35:36-root-INFO: grad norm: 9.715 9.647 1.151
2024-12-02-11:35:36-root-INFO: grad norm: 9.793 9.717 1.218
2024-12-02-11:35:36-root-INFO: Loss Change: 150.557 -> 147.283
2024-12-02-11:35:36-root-INFO: Regularization Change: 0.000 -> 1.886
2024-12-02-11:35:36-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-11:35:36-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-11:35:37-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-11:35:37-root-INFO: grad norm: 7.103 7.053 0.841
2024-12-02-11:35:37-root-INFO: grad norm: 6.663 6.601 0.908
2024-12-02-11:35:38-root-INFO: grad norm: 6.687 6.628 0.886
2024-12-02-11:35:38-root-INFO: grad norm: 7.097 7.032 0.960
2024-12-02-11:35:39-root-INFO: grad norm: 6.992 6.925 0.964
2024-12-02-11:35:39-root-INFO: Loss Change: 146.453 -> 143.643
2024-12-02-11:35:39-root-INFO: Regularization Change: 0.000 -> 1.446
2024-12-02-11:35:39-root-INFO: Undo step: 80
2024-12-02-11:35:39-root-INFO: Undo step: 81
2024-12-02-11:35:39-root-INFO: Undo step: 82
2024-12-02-11:35:39-root-INFO: Undo step: 83
2024-12-02-11:35:39-root-INFO: Undo step: 84
2024-12-02-11:35:39-root-INFO: Undo step: 85
2024-12-02-11:35:39-root-INFO: Undo step: 86
2024-12-02-11:35:39-root-INFO: Undo step: 87
2024-12-02-11:35:39-root-INFO: Undo step: 88
2024-12-02-11:35:39-root-INFO: Undo step: 89
2024-12-02-11:35:39-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-11:35:39-root-INFO: grad norm: 63.183 62.253 10.805
2024-12-02-11:35:40-root-INFO: grad norm: 38.126 37.649 6.014
2024-12-02-11:35:40-root-INFO: grad norm: 27.566 27.197 4.498
2024-12-02-11:35:41-root-INFO: grad norm: 19.135 18.818 3.470
2024-12-02-11:35:41-root-INFO: grad norm: 14.943 14.659 2.901
2024-12-02-11:35:42-root-INFO: Loss Change: 489.839 -> 215.612
2024-12-02-11:35:42-root-INFO: Regularization Change: 0.000 -> 70.472
2024-12-02-11:35:42-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-11:35:42-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-11:35:42-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-11:35:42-root-INFO: grad norm: 15.025 14.784 2.683
2024-12-02-11:35:42-root-INFO: grad norm: 14.169 13.941 2.531
2024-12-02-11:35:43-root-INFO: grad norm: 13.753 13.554 2.334
2024-12-02-11:35:43-root-INFO: grad norm: 13.901 13.709 2.303
2024-12-02-11:35:44-root-INFO: grad norm: 14.270 14.102 2.184
2024-12-02-11:35:44-root-INFO: Loss Change: 215.715 -> 188.809
2024-12-02-11:35:44-root-INFO: Regularization Change: 0.000 -> 10.274
2024-12-02-11:35:44-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-11:35:44-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-11:35:44-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-11:35:44-root-INFO: grad norm: 12.719 12.569 1.944
2024-12-02-11:35:45-root-INFO: grad norm: 13.001 12.853 1.956
2024-12-02-11:35:45-root-INFO: grad norm: 13.486 13.339 1.984
2024-12-02-11:35:46-root-INFO: grad norm: 13.965 13.831 1.933
2024-12-02-11:35:46-root-INFO: grad norm: 14.494 14.352 2.022
2024-12-02-11:35:47-root-INFO: Loss Change: 187.937 -> 177.317
2024-12-02-11:35:47-root-INFO: Regularization Change: 0.000 -> 5.325
2024-12-02-11:35:47-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-11:35:47-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-11:35:47-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-11:35:47-root-INFO: grad norm: 18.267 18.093 2.510
2024-12-02-11:35:48-root-INFO: grad norm: 18.132 17.967 2.437
2024-12-02-11:35:48-root-INFO: grad norm: 17.805 17.663 2.244
2024-12-02-11:35:48-root-INFO: grad norm: 17.312 17.155 2.325
2024-12-02-11:35:49-root-INFO: grad norm: 16.865 16.731 2.121
2024-12-02-11:35:49-root-INFO: Loss Change: 178.900 -> 169.667
2024-12-02-11:35:49-root-INFO: Regularization Change: 0.000 -> 4.505
2024-12-02-11:35:49-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-11:35:49-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-11:35:49-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-11:35:50-root-INFO: grad norm: 13.675 13.564 1.735
2024-12-02-11:35:50-root-INFO: grad norm: 13.326 13.214 1.730
2024-12-02-11:35:51-root-INFO: grad norm: 13.026 12.903 1.785
2024-12-02-11:35:51-root-INFO: grad norm: 12.900 12.789 1.689
2024-12-02-11:35:52-root-INFO: grad norm: 12.752 12.628 1.777
2024-12-02-11:35:52-root-INFO: Loss Change: 168.034 -> 161.999
2024-12-02-11:35:52-root-INFO: Regularization Change: 0.000 -> 3.098
2024-12-02-11:35:52-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-11:35:52-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-11:35:52-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-11:35:52-root-INFO: grad norm: 15.562 15.399 2.250
2024-12-02-11:35:53-root-INFO: grad norm: 14.828 14.683 2.069
2024-12-02-11:35:53-root-INFO: grad norm: 13.989 13.872 1.810
2024-12-02-11:35:54-root-INFO: grad norm: 13.425 13.302 1.820
2024-12-02-11:35:54-root-INFO: grad norm: 13.120 13.008 1.714
2024-12-02-11:35:54-root-INFO: Loss Change: 163.337 -> 157.280
2024-12-02-11:35:54-root-INFO: Regularization Change: 0.000 -> 2.956
2024-12-02-11:35:54-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-11:35:54-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-11:35:55-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-11:35:55-root-INFO: grad norm: 10.911 10.815 1.443
2024-12-02-11:35:55-root-INFO: grad norm: 10.562 10.467 1.417
2024-12-02-11:35:56-root-INFO: grad norm: 10.346 10.241 1.468
2024-12-02-11:35:56-root-INFO: grad norm: 10.304 10.209 1.396
2024-12-02-11:35:57-root-INFO: grad norm: 10.165 10.058 1.475
2024-12-02-11:35:57-root-INFO: Loss Change: 156.241 -> 151.940
2024-12-02-11:35:57-root-INFO: Regularization Change: 0.000 -> 2.192
2024-12-02-11:35:57-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-11:35:57-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-11:35:57-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-11:35:57-root-INFO: grad norm: 12.892 12.750 1.908
2024-12-02-11:35:58-root-INFO: grad norm: 12.229 12.104 1.740
2024-12-02-11:35:58-root-INFO: grad norm: 11.429 11.330 1.499
2024-12-02-11:35:59-root-INFO: grad norm: 10.998 10.893 1.519
2024-12-02-11:35:59-root-INFO: grad norm: 10.851 10.755 1.434
2024-12-02-11:35:59-root-INFO: Loss Change: 153.079 -> 148.691
2024-12-02-11:35:59-root-INFO: Regularization Change: 0.000 -> 2.288
2024-12-02-11:35:59-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-11:35:59-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-11:36:00-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-11:36:00-root-INFO: grad norm: 8.731 8.654 1.153
2024-12-02-11:36:00-root-INFO: grad norm: 8.353 8.274 1.150
2024-12-02-11:36:01-root-INFO: grad norm: 8.169 8.082 1.191
2024-12-02-11:36:01-root-INFO: grad norm: 8.169 8.089 1.137
2024-12-02-11:36:02-root-INFO: grad norm: 8.005 7.914 1.205
2024-12-02-11:36:02-root-INFO: Loss Change: 148.027 -> 144.484
2024-12-02-11:36:02-root-INFO: Regularization Change: 0.000 -> 1.741
2024-12-02-11:36:02-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-11:36:02-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-11:36:02-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-11:36:02-root-INFO: grad norm: 10.720 10.588 1.681
2024-12-02-11:36:03-root-INFO: grad norm: 9.953 9.849 1.434
2024-12-02-11:36:03-root-INFO: grad norm: 9.013 8.929 1.229
2024-12-02-11:36:04-root-INFO: grad norm: 8.625 8.542 1.193
2024-12-02-11:36:04-root-INFO: grad norm: 8.618 8.537 1.179
2024-12-02-11:36:05-root-INFO: Loss Change: 145.336 -> 141.697
2024-12-02-11:36:05-root-INFO: Regularization Change: 0.000 -> 1.876
2024-12-02-11:36:05-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-11:36:05-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-11:36:05-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-11:36:05-root-INFO: grad norm: 6.043 5.987 0.821
2024-12-02-11:36:05-root-INFO: grad norm: 5.667 5.601 0.861
2024-12-02-11:36:06-root-INFO: grad norm: 5.651 5.580 0.898
2024-12-02-11:36:06-root-INFO: grad norm: 5.900 5.833 0.885
2024-12-02-11:36:07-root-INFO: grad norm: 5.808 5.729 0.957
2024-12-02-11:36:07-root-INFO: Loss Change: 140.966 -> 137.980
2024-12-02-11:36:07-root-INFO: Regularization Change: 0.000 -> 1.423
2024-12-02-11:36:07-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-11:36:07-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-11:36:07-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-11:36:07-root-INFO: grad norm: 8.076 7.956 1.387
2024-12-02-11:36:08-root-INFO: grad norm: 7.502 7.415 1.144
2024-12-02-11:36:08-root-INFO: grad norm: 6.802 6.727 1.008
2024-12-02-11:36:09-root-INFO: grad norm: 6.683 6.608 0.997
2024-12-02-11:36:09-root-INFO: grad norm: 6.966 6.893 1.007
2024-12-02-11:36:09-root-INFO: Loss Change: 138.466 -> 135.700
2024-12-02-11:36:09-root-INFO: Regularization Change: 0.000 -> 1.594
2024-12-02-11:36:09-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-11:36:09-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-11:36:10-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-11:36:10-root-INFO: grad norm: 5.255 5.198 0.774
2024-12-02-11:36:10-root-INFO: grad norm: 4.740 4.676 0.775
2024-12-02-11:36:11-root-INFO: grad norm: 4.826 4.760 0.792
2024-12-02-11:36:11-root-INFO: grad norm: 5.369 5.306 0.823
2024-12-02-11:36:12-root-INFO: grad norm: 5.202 5.127 0.878
2024-12-02-11:36:12-root-INFO: Loss Change: 135.386 -> 132.609
2024-12-02-11:36:12-root-INFO: Regularization Change: 0.000 -> 1.325
2024-12-02-11:36:12-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-11:36:12-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-11:36:12-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-11:36:12-root-INFO: grad norm: 7.071 6.969 1.199
2024-12-02-11:36:13-root-INFO: grad norm: 6.518 6.437 1.025
2024-12-02-11:36:13-root-INFO: grad norm: 5.869 5.804 0.873
2024-12-02-11:36:14-root-INFO: grad norm: 5.751 5.680 0.901
2024-12-02-11:36:14-root-INFO: grad norm: 6.017 5.953 0.870
2024-12-02-11:36:15-root-INFO: Loss Change: 132.882 -> 130.401
2024-12-02-11:36:15-root-INFO: Regularization Change: 0.000 -> 1.478
2024-12-02-11:36:15-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-11:36:15-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-11:36:15-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-11:36:15-root-INFO: grad norm: 4.409 4.361 0.647
2024-12-02-11:36:15-root-INFO: grad norm: 4.043 3.989 0.661
2024-12-02-11:36:16-root-INFO: grad norm: 4.150 4.090 0.700
2024-12-02-11:36:16-root-INFO: grad norm: 4.598 4.543 0.705
2024-12-02-11:36:17-root-INFO: grad norm: 4.454 4.387 0.765
2024-12-02-11:36:17-root-INFO: Loss Change: 129.886 -> 127.325
2024-12-02-11:36:17-root-INFO: Regularization Change: 0.000 -> 1.254
2024-12-02-11:36:17-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-11:36:17-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-11:36:17-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-11:36:17-root-INFO: grad norm: 6.051 5.959 1.055
2024-12-02-11:36:18-root-INFO: grad norm: 5.600 5.531 0.876
2024-12-02-11:36:18-root-INFO: grad norm: 5.158 5.098 0.783
2024-12-02-11:36:19-root-INFO: grad norm: 5.117 5.054 0.801
2024-12-02-11:36:19-root-INFO: grad norm: 5.332 5.276 0.772
2024-12-02-11:36:19-root-INFO: Loss Change: 127.709 -> 125.501
2024-12-02-11:36:19-root-INFO: Regularization Change: 0.000 -> 1.398
2024-12-02-11:36:19-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-11:36:19-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-11:36:20-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-11:36:20-root-INFO: grad norm: 4.279 4.233 0.623
2024-12-02-11:36:20-root-INFO: grad norm: 3.860 3.813 0.602
2024-12-02-11:36:21-root-INFO: grad norm: 4.026 3.973 0.650
2024-12-02-11:36:21-root-INFO: grad norm: 4.617 4.569 0.659
2024-12-02-11:36:22-root-INFO: grad norm: 4.402 4.343 0.719
2024-12-02-11:36:22-root-INFO: Loss Change: 125.268 -> 122.794
2024-12-02-11:36:22-root-INFO: Regularization Change: 0.000 -> 1.252
2024-12-02-11:36:22-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-11:36:22-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-11:36:22-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-11:36:22-root-INFO: grad norm: 5.786 5.697 1.010
2024-12-02-11:36:23-root-INFO: grad norm: 5.270 5.210 0.795
2024-12-02-11:36:23-root-INFO: grad norm: 4.781 4.727 0.714
2024-12-02-11:36:24-root-INFO: grad norm: 4.769 4.715 0.721
2024-12-02-11:36:24-root-INFO: grad norm: 5.064 5.014 0.711
2024-12-02-11:36:25-root-INFO: Loss Change: 123.144 -> 121.042
2024-12-02-11:36:25-root-INFO: Regularization Change: 0.000 -> 1.410
2024-12-02-11:36:25-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-11:36:25-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-11:36:25-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-11:36:25-root-INFO: grad norm: 4.253 4.212 0.589
2024-12-02-11:36:25-root-INFO: grad norm: 3.683 3.642 0.545
2024-12-02-11:36:26-root-INFO: grad norm: 3.914 3.870 0.584
2024-12-02-11:36:26-root-INFO: grad norm: 4.731 4.691 0.620
2024-12-02-11:36:27-root-INFO: grad norm: 4.377 4.325 0.675
2024-12-02-11:36:27-root-INFO: Loss Change: 120.739 -> 118.231
2024-12-02-11:36:27-root-INFO: Regularization Change: 0.000 -> 1.290
2024-12-02-11:36:27-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-11:36:27-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-11:36:27-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-11:36:27-root-INFO: grad norm: 4.584 4.524 0.737
2024-12-02-11:36:28-root-INFO: grad norm: 4.489 4.438 0.673
2024-12-02-11:36:29-root-INFO: grad norm: 4.912 4.866 0.670
2024-12-02-11:36:29-root-INFO: grad norm: 4.551 4.496 0.701
2024-12-02-11:36:29-root-INFO: grad norm: 3.805 3.764 0.559
2024-12-02-11:36:30-root-INFO: Loss Change: 118.480 -> 116.349
2024-12-02-11:36:30-root-INFO: Regularization Change: 0.000 -> 1.302
2024-12-02-11:36:30-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-11:36:30-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-11:36:30-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-11:36:30-root-INFO: grad norm: 3.567 3.526 0.541
2024-12-02-11:36:31-root-INFO: grad norm: 3.789 3.754 0.511
2024-12-02-11:36:31-root-INFO: grad norm: 3.863 3.824 0.552
2024-12-02-11:36:32-root-INFO: grad norm: 4.151 4.113 0.556
2024-12-02-11:36:32-root-INFO: grad norm: 4.056 4.010 0.614
2024-12-02-11:36:32-root-INFO: Loss Change: 115.992 -> 113.814
2024-12-02-11:36:32-root-INFO: Regularization Change: 0.000 -> 1.281
2024-12-02-11:36:32-root-INFO: Undo step: 70
2024-12-02-11:36:32-root-INFO: Undo step: 71
2024-12-02-11:36:32-root-INFO: Undo step: 72
2024-12-02-11:36:32-root-INFO: Undo step: 73
2024-12-02-11:36:32-root-INFO: Undo step: 74
2024-12-02-11:36:32-root-INFO: Undo step: 75
2024-12-02-11:36:32-root-INFO: Undo step: 76
2024-12-02-11:36:32-root-INFO: Undo step: 77
2024-12-02-11:36:32-root-INFO: Undo step: 78
2024-12-02-11:36:32-root-INFO: Undo step: 79
2024-12-02-11:36:33-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-11:36:33-root-INFO: grad norm: 48.635 48.040 7.586
2024-12-02-11:36:33-root-INFO: grad norm: 26.564 26.167 4.574
2024-12-02-11:36:34-root-INFO: grad norm: 21.094 20.800 3.509
2024-12-02-11:36:34-root-INFO: grad norm: 15.362 15.073 2.967
2024-12-02-11:36:34-root-INFO: grad norm: 12.045 11.798 2.426
2024-12-02-11:36:35-root-INFO: Loss Change: 409.438 -> 176.289
2024-12-02-11:36:35-root-INFO: Regularization Change: 0.000 -> 78.841
2024-12-02-11:36:35-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-11:36:35-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-11:36:35-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-11:36:35-root-INFO: grad norm: 10.571 10.377 2.017
2024-12-02-11:36:36-root-INFO: grad norm: 9.039 8.863 1.773
2024-12-02-11:36:36-root-INFO: grad norm: 8.162 8.011 1.565
2024-12-02-11:36:36-root-INFO: grad norm: 7.622 7.472 1.505
2024-12-02-11:36:37-root-INFO: grad norm: 7.720 7.597 1.370
2024-12-02-11:36:37-root-INFO: Loss Change: 175.747 -> 151.994
2024-12-02-11:36:37-root-INFO: Regularization Change: 0.000 -> 11.058
2024-12-02-11:36:37-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-11:36:37-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-11:36:37-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-11:36:38-root-INFO: grad norm: 6.850 6.726 1.300
2024-12-02-11:36:38-root-INFO: grad norm: 6.549 6.439 1.194
2024-12-02-11:36:38-root-INFO: grad norm: 6.268 6.145 1.238
2024-12-02-11:36:39-root-INFO: grad norm: 5.927 5.826 1.092
2024-12-02-11:36:39-root-INFO: grad norm: 5.837 5.724 1.141
2024-12-02-11:36:40-root-INFO: Loss Change: 151.738 -> 141.087
2024-12-02-11:36:40-root-INFO: Regularization Change: 0.000 -> 5.058
2024-12-02-11:36:40-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-11:36:40-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-11:36:40-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-11:36:40-root-INFO: grad norm: 7.656 7.542 1.311
2024-12-02-11:36:41-root-INFO: grad norm: 6.889 6.785 1.196
2024-12-02-11:36:41-root-INFO: grad norm: 5.974 5.889 1.005
2024-12-02-11:36:41-root-INFO: grad norm: 5.829 5.747 0.977
2024-12-02-11:36:42-root-INFO: grad norm: 5.983 5.906 0.953
2024-12-02-11:36:42-root-INFO: Loss Change: 141.105 -> 134.431
2024-12-02-11:36:42-root-INFO: Regularization Change: 0.000 -> 3.387
2024-12-02-11:36:42-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-11:36:42-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-11:36:42-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-11:36:43-root-INFO: grad norm: 5.038 4.971 0.818
2024-12-02-11:36:43-root-INFO: grad norm: 5.533 5.464 0.872
2024-12-02-11:36:44-root-INFO: grad norm: 5.487 5.407 0.932
2024-12-02-11:36:44-root-INFO: grad norm: 5.350 5.282 0.852
2024-12-02-11:36:45-root-INFO: grad norm: 5.422 5.344 0.915
2024-12-02-11:36:45-root-INFO: Loss Change: 133.830 -> 129.154
2024-12-02-11:36:45-root-INFO: Regularization Change: 0.000 -> 2.481
2024-12-02-11:36:45-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-11:36:45-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-11:36:45-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-11:36:45-root-INFO: grad norm: 7.304 7.208 1.180
2024-12-02-11:36:46-root-INFO: grad norm: 6.597 6.520 1.006
2024-12-02-11:36:46-root-INFO: grad norm: 5.695 5.631 0.853
2024-12-02-11:36:47-root-INFO: grad norm: 5.601 5.541 0.821
2024-12-02-11:36:47-root-INFO: grad norm: 5.991 5.932 0.839
2024-12-02-11:36:48-root-INFO: Loss Change: 129.525 -> 125.592
2024-12-02-11:36:48-root-INFO: Regularization Change: 0.000 -> 2.278
2024-12-02-11:36:48-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-11:36:48-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-11:36:48-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-11:36:48-root-INFO: grad norm: 5.275 5.223 0.736
2024-12-02-11:36:48-root-INFO: grad norm: 5.973 5.917 0.813
2024-12-02-11:36:49-root-INFO: grad norm: 5.441 5.376 0.840
2024-12-02-11:36:49-root-INFO: grad norm: 4.474 4.420 0.691
2024-12-02-11:36:50-root-INFO: grad norm: 4.542 4.488 0.699
2024-12-02-11:36:50-root-INFO: Loss Change: 125.251 -> 121.801
2024-12-02-11:36:50-root-INFO: Regularization Change: 0.000 -> 1.894
2024-12-02-11:36:50-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-11:36:50-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-11:36:50-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-11:36:50-root-INFO: grad norm: 7.059 6.976 1.079
2024-12-02-11:36:51-root-INFO: grad norm: 6.098 6.034 0.876
2024-12-02-11:36:51-root-INFO: grad norm: 5.028 4.975 0.727
2024-12-02-11:36:52-root-INFO: grad norm: 4.716 4.671 0.651
2024-12-02-11:36:52-root-INFO: grad norm: 4.560 4.512 0.661
2024-12-02-11:36:53-root-INFO: Loss Change: 122.100 -> 118.582
2024-12-02-11:36:53-root-INFO: Regularization Change: 0.000 -> 1.792
2024-12-02-11:36:53-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-11:36:53-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-11:36:53-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-11:36:53-root-INFO: grad norm: 3.917 3.872 0.593
2024-12-02-11:36:53-root-INFO: grad norm: 3.805 3.760 0.584
2024-12-02-11:36:54-root-INFO: grad norm: 4.065 4.020 0.600
2024-12-02-11:36:54-root-INFO: grad norm: 5.578 5.535 0.687
2024-12-02-11:36:54-root-INFO: Loss too large (116.779->116.837)! Learning rate decreased to 0.08574.
2024-12-02-11:36:55-root-INFO: grad norm: 4.376 4.324 0.670
2024-12-02-11:36:55-root-INFO: Loss Change: 118.263 -> 115.548
2024-12-02-11:36:55-root-INFO: Regularization Change: 0.000 -> 1.299
2024-12-02-11:36:55-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-11:36:55-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-11:36:55-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-11:36:55-root-INFO: grad norm: 3.755 3.697 0.660
2024-12-02-11:36:56-root-INFO: grad norm: 4.115 4.064 0.645
2024-12-02-11:36:56-root-INFO: grad norm: 6.005 5.959 0.742
2024-12-02-11:36:57-root-INFO: Loss too large (114.924->115.092)! Learning rate decreased to 0.08779.
2024-12-02-11:36:57-root-INFO: grad norm: 4.037 3.984 0.650
2024-12-02-11:36:57-root-INFO: grad norm: 2.362 2.315 0.469
2024-12-02-11:36:58-root-INFO: Loss Change: 115.700 -> 113.250
2024-12-02-11:36:58-root-INFO: Regularization Change: 0.000 -> 1.113
2024-12-02-11:36:58-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-11:36:58-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-11:36:58-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-11:36:58-root-INFO: grad norm: 3.897 3.823 0.756
2024-12-02-11:36:59-root-INFO: grad norm: 4.078 4.032 0.613
2024-12-02-11:36:59-root-INFO: grad norm: 5.991 5.945 0.739
2024-12-02-11:36:59-root-INFO: Loss too large (112.358->112.548)! Learning rate decreased to 0.08987.
2024-12-02-11:37:00-root-INFO: grad norm: 3.967 3.919 0.612
2024-12-02-11:37:00-root-INFO: grad norm: 2.305 2.262 0.441
2024-12-02-11:37:00-root-INFO: Loss Change: 113.173 -> 110.712
2024-12-02-11:37:00-root-INFO: Regularization Change: 0.000 -> 1.122
2024-12-02-11:37:00-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-11:37:00-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-11:37:01-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-11:37:01-root-INFO: grad norm: 2.902 2.849 0.554
2024-12-02-11:37:01-root-INFO: grad norm: 3.300 3.258 0.524
2024-12-02-11:37:02-root-INFO: grad norm: 5.557 5.519 0.652
2024-12-02-11:37:02-root-INFO: Loss too large (110.189->110.422)! Learning rate decreased to 0.09199.
2024-12-02-11:37:02-root-INFO: grad norm: 3.904 3.859 0.586
2024-12-02-11:37:03-root-INFO: grad norm: 2.123 2.081 0.417
2024-12-02-11:37:03-root-INFO: Loss Change: 110.852 -> 108.666
2024-12-02-11:37:03-root-INFO: Regularization Change: 0.000 -> 1.067
2024-12-02-11:37:03-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-11:37:03-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-11:37:03-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-11:37:04-root-INFO: grad norm: 3.659 3.590 0.704
2024-12-02-11:37:04-root-INFO: grad norm: 3.913 3.872 0.567
2024-12-02-11:37:04-root-INFO: grad norm: 6.302 6.261 0.725
2024-12-02-11:37:05-root-INFO: Loss too large (107.994->108.286)! Learning rate decreased to 0.09414.
2024-12-02-11:37:05-root-INFO: grad norm: 3.879 3.837 0.573
2024-12-02-11:37:06-root-INFO: grad norm: 2.518 2.483 0.415
2024-12-02-11:37:06-root-INFO: Loss Change: 108.652 -> 106.373
2024-12-02-11:37:06-root-INFO: Regularization Change: 0.000 -> 1.113
2024-12-02-11:37:06-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-11:37:06-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-11:37:06-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-11:37:06-root-INFO: grad norm: 3.491 3.441 0.584
2024-12-02-11:37:07-root-INFO: grad norm: 3.941 3.902 0.551
2024-12-02-11:37:07-root-INFO: grad norm: 5.794 5.756 0.657
2024-12-02-11:37:07-root-INFO: Loss too large (105.715->106.006)! Learning rate decreased to 0.09633.
2024-12-02-11:37:08-root-INFO: grad norm: 3.799 3.759 0.547
2024-12-02-11:37:08-root-INFO: grad norm: 2.209 2.175 0.386
2024-12-02-11:37:09-root-INFO: Loss Change: 106.328 -> 104.180
2024-12-02-11:37:09-root-INFO: Regularization Change: 0.000 -> 1.073
2024-12-02-11:37:09-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-11:37:09-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-11:37:09-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-11:37:09-root-INFO: grad norm: 3.229 3.185 0.526
2024-12-02-11:37:09-root-INFO: grad norm: 3.802 3.765 0.527
2024-12-02-11:37:10-root-INFO: grad norm: 5.958 5.924 0.644
2024-12-02-11:37:10-root-INFO: Loss too large (103.803->104.113)! Learning rate decreased to 0.09855.
2024-12-02-11:37:10-root-INFO: grad norm: 3.746 3.707 0.535
2024-12-02-11:37:11-root-INFO: grad norm: 2.292 2.260 0.381
2024-12-02-11:37:11-root-INFO: Loss Change: 104.296 -> 102.261
2024-12-02-11:37:11-root-INFO: Regularization Change: 0.000 -> 1.060
2024-12-02-11:37:11-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-11:37:11-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-11:37:11-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-11:37:12-root-INFO: grad norm: 3.870 3.816 0.644
2024-12-02-11:37:12-root-INFO: grad norm: 4.052 4.015 0.548
2024-12-02-11:37:13-root-INFO: grad norm: 5.322 5.287 0.605
2024-12-02-11:37:13-root-INFO: Loss too large (101.637->101.909)! Learning rate decreased to 0.10081.
2024-12-02-11:37:13-root-INFO: grad norm: 3.781 3.745 0.520
2024-12-02-11:37:14-root-INFO: grad norm: 1.968 1.935 0.359
2024-12-02-11:37:14-root-INFO: Loss Change: 102.373 -> 100.158
2024-12-02-11:37:14-root-INFO: Regularization Change: 0.000 -> 1.111
2024-12-02-11:37:14-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-11:37:14-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-11:37:14-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-11:37:14-root-INFO: grad norm: 2.330 2.288 0.440
2024-12-02-11:37:15-root-INFO: grad norm: 2.070 2.037 0.367
2024-12-02-11:37:15-root-INFO: grad norm: 2.211 2.181 0.368
2024-12-02-11:37:16-root-INFO: grad norm: 2.945 2.915 0.422
2024-12-02-11:37:16-root-INFO: grad norm: 5.637 5.608 0.568
2024-12-02-11:37:16-root-INFO: Loss too large (98.838->99.259)! Learning rate decreased to 0.10310.
2024-12-02-11:37:17-root-INFO: Loss too large (98.838->98.931)! Learning rate decreased to 0.08248.
2024-12-02-11:37:17-root-INFO: Loss Change: 100.120 -> 98.624
2024-12-02-11:37:17-root-INFO: Regularization Change: 0.000 -> 1.266
2024-12-02-11:37:17-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-11:37:17-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-11:37:17-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-11:37:17-root-INFO: grad norm: 4.101 4.068 0.517
2024-12-02-11:37:18-root-INFO: grad norm: 5.695 5.668 0.552
2024-12-02-11:37:18-root-INFO: Loss too large (98.192->98.527)! Learning rate decreased to 0.10543.
2024-12-02-11:37:18-root-INFO: Loss too large (98.192->98.193)! Learning rate decreased to 0.08434.
2024-12-02-11:37:19-root-INFO: grad norm: 3.800 3.770 0.474
2024-12-02-11:37:19-root-INFO: grad norm: 2.017 1.987 0.346
2024-12-02-11:37:20-root-INFO: grad norm: 2.025 1.995 0.350
2024-12-02-11:37:20-root-INFO: Loss Change: 98.683 -> 96.568
2024-12-02-11:37:20-root-INFO: Regularization Change: 0.000 -> 0.814
2024-12-02-11:37:20-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-11:37:20-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-11:37:20-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-11:37:20-root-INFO: grad norm: 3.361 3.310 0.581
2024-12-02-11:37:21-root-INFO: grad norm: 3.997 3.964 0.515
2024-12-02-11:37:21-root-INFO: grad norm: 6.584 6.550 0.664
2024-12-02-11:37:21-root-INFO: Loss too large (96.192->96.560)! Learning rate decreased to 0.10779.
2024-12-02-11:37:22-root-INFO: grad norm: 3.685 3.651 0.502
2024-12-02-11:37:22-root-INFO: grad norm: 2.841 2.818 0.365
2024-12-02-11:37:23-root-INFO: Loss Change: 96.486 -> 94.535
2024-12-02-11:37:23-root-INFO: Regularization Change: 0.000 -> 1.160
2024-12-02-11:37:23-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-11:37:23-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-11:37:23-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-11:37:23-root-INFO: grad norm: 4.128 4.095 0.517
2024-12-02-11:37:23-root-INFO: Loss too large (94.646->94.754)! Learning rate decreased to 0.11019.
2024-12-02-11:37:24-root-INFO: grad norm: 3.764 3.736 0.460
2024-12-02-11:37:24-root-INFO: grad norm: 3.376 3.351 0.406
2024-12-02-11:37:24-root-INFO: grad norm: 3.611 3.583 0.448
2024-12-02-11:37:25-root-INFO: grad norm: 4.037 4.014 0.434
2024-12-02-11:37:25-root-INFO: Loss Change: 94.646 -> 93.207
2024-12-02-11:37:25-root-INFO: Regularization Change: 0.000 -> 1.092
2024-12-02-11:37:25-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-11:37:25-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-11:37:26-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-11:37:26-root-INFO: grad norm: 3.913 3.884 0.473
2024-12-02-11:37:26-root-INFO: grad norm: 5.291 5.267 0.500
2024-12-02-11:37:26-root-INFO: Loss too large (92.706->92.992)! Learning rate decreased to 0.11263.
2024-12-02-11:37:27-root-INFO: grad norm: 3.673 3.645 0.451
2024-12-02-11:37:27-root-INFO: grad norm: 1.878 1.850 0.321
2024-12-02-11:37:28-root-INFO: grad norm: 1.789 1.760 0.321
2024-12-02-11:37:28-root-INFO: Loss Change: 93.179 -> 90.845
2024-12-02-11:37:28-root-INFO: Regularization Change: 0.000 -> 1.133
2024-12-02-11:37:28-root-INFO: Undo step: 60
2024-12-02-11:37:28-root-INFO: Undo step: 61
2024-12-02-11:37:28-root-INFO: Undo step: 62
2024-12-02-11:37:28-root-INFO: Undo step: 63
2024-12-02-11:37:28-root-INFO: Undo step: 64
2024-12-02-11:37:28-root-INFO: Undo step: 65
2024-12-02-11:37:28-root-INFO: Undo step: 66
2024-12-02-11:37:28-root-INFO: Undo step: 67
2024-12-02-11:37:28-root-INFO: Undo step: 68
2024-12-02-11:37:28-root-INFO: Undo step: 69
2024-12-02-11:37:28-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-11:37:29-root-INFO: grad norm: 42.951 42.500 6.207
2024-12-02-11:37:29-root-INFO: grad norm: 22.737 22.377 4.026
2024-12-02-11:37:29-root-INFO: grad norm: 15.742 15.486 2.827
2024-12-02-11:37:30-root-INFO: grad norm: 11.928 11.746 2.071
2024-12-02-11:37:30-root-INFO: grad norm: 10.065 9.909 1.765
2024-12-02-11:37:31-root-INFO: Loss Change: 344.193 -> 142.242
2024-12-02-11:37:31-root-INFO: Regularization Change: 0.000 -> 85.121
2024-12-02-11:37:31-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-11:37:31-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-11:37:31-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-11:37:31-root-INFO: grad norm: 9.003 8.877 1.499
2024-12-02-11:37:31-root-INFO: grad norm: 7.878 7.766 1.326
2024-12-02-11:37:32-root-INFO: grad norm: 7.068 6.973 1.151
2024-12-02-11:37:32-root-INFO: grad norm: 6.586 6.491 1.112
2024-12-02-11:37:33-root-INFO: grad norm: 7.076 6.998 1.045
2024-12-02-11:37:33-root-INFO: Loss Change: 142.308 -> 123.350
2024-12-02-11:37:33-root-INFO: Regularization Change: 0.000 -> 11.490
2024-12-02-11:37:33-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-11:37:33-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-11:37:33-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-11:37:33-root-INFO: grad norm: 6.247 6.167 1.000
2024-12-02-11:37:34-root-INFO: grad norm: 5.245 5.179 0.824
2024-12-02-11:37:34-root-INFO: grad norm: 5.414 5.341 0.886
2024-12-02-11:37:35-root-INFO: grad norm: 7.025 6.964 0.921
2024-12-02-11:37:35-root-INFO: grad norm: 4.964 4.892 0.846
2024-12-02-11:37:36-root-INFO: Loss Change: 122.744 -> 113.197
2024-12-02-11:37:36-root-INFO: Regularization Change: 0.000 -> 5.445
2024-12-02-11:37:36-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-11:37:36-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-11:37:36-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-11:37:36-root-INFO: grad norm: 5.194 5.125 0.840
2024-12-02-11:37:36-root-INFO: grad norm: 5.962 5.895 0.888
2024-12-02-11:37:37-root-INFO: grad norm: 5.155 5.097 0.774
2024-12-02-11:37:37-root-INFO: grad norm: 4.053 3.977 0.781
2024-12-02-11:37:38-root-INFO: grad norm: 3.971 3.911 0.686
2024-12-02-11:37:38-root-INFO: Loss Change: 113.055 -> 106.985
2024-12-02-11:37:38-root-INFO: Regularization Change: 0.000 -> 3.686
2024-12-02-11:37:38-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-11:37:38-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-11:37:38-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-11:37:38-root-INFO: grad norm: 4.654 4.578 0.836
2024-12-02-11:37:39-root-INFO: grad norm: 5.235 5.180 0.762
2024-12-02-11:37:39-root-INFO: grad norm: 7.373 7.307 0.986
2024-12-02-11:37:40-root-INFO: Loss too large (105.728->105.748)! Learning rate decreased to 0.09855.
2024-12-02-11:37:40-root-INFO: grad norm: 4.439 4.391 0.652
2024-12-02-11:37:40-root-INFO: grad norm: 3.598 3.549 0.595
2024-12-02-11:37:41-root-INFO: Loss Change: 107.016 -> 103.058
2024-12-02-11:37:41-root-INFO: Regularization Change: 0.000 -> 2.134
2024-12-02-11:37:41-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-11:37:41-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-11:37:41-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-11:37:41-root-INFO: grad norm: 5.614 5.544 0.884
2024-12-02-11:37:42-root-INFO: grad norm: 4.436 4.383 0.686
2024-12-02-11:37:42-root-INFO: grad norm: 3.170 3.123 0.540
2024-12-02-11:37:43-root-INFO: grad norm: 3.189 3.139 0.565
2024-12-02-11:37:43-root-INFO: grad norm: 3.918 3.875 0.580
2024-12-02-11:37:43-root-INFO: Loss Change: 103.188 -> 99.887
2024-12-02-11:37:43-root-INFO: Regularization Change: 0.000 -> 2.336
2024-12-02-11:37:43-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-11:37:43-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-11:37:44-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-11:37:44-root-INFO: grad norm: 7.655 7.593 0.979
2024-12-02-11:37:44-root-INFO: Loss too large (99.926->100.162)! Learning rate decreased to 0.10310.
2024-12-02-11:37:44-root-INFO: grad norm: 3.955 3.917 0.553
2024-12-02-11:37:45-root-INFO: grad norm: 3.813 3.776 0.531
2024-12-02-11:37:45-root-INFO: grad norm: 3.977 3.942 0.525
2024-12-02-11:37:46-root-INFO: grad norm: 4.082 4.044 0.554
2024-12-02-11:37:46-root-INFO: Loss Change: 99.926 -> 96.738
2024-12-02-11:37:46-root-INFO: Regularization Change: 0.000 -> 1.303
2024-12-02-11:37:46-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-11:37:46-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-11:37:46-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-11:37:46-root-INFO: grad norm: 5.432 5.370 0.813
2024-12-02-11:37:47-root-INFO: Loss too large (96.992->97.151)! Learning rate decreased to 0.10543.
2024-12-02-11:37:47-root-INFO: grad norm: 4.135 4.098 0.557
2024-12-02-11:37:48-root-INFO: grad norm: 2.432 2.394 0.426
2024-12-02-11:37:48-root-INFO: grad norm: 2.626 2.591 0.429
2024-12-02-11:37:49-root-INFO: grad norm: 3.585 3.554 0.468
2024-12-02-11:37:49-root-INFO: Loss Change: 96.992 -> 94.670
2024-12-02-11:37:49-root-INFO: Regularization Change: 0.000 -> 1.357
2024-12-02-11:37:49-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-11:37:49-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-11:37:49-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-11:37:49-root-INFO: grad norm: 4.238 4.202 0.554
2024-12-02-11:37:50-root-INFO: grad norm: 7.148 7.110 0.739
2024-12-02-11:37:50-root-INFO: Loss too large (94.210->94.524)! Learning rate decreased to 0.10779.
2024-12-02-11:37:50-root-INFO: grad norm: 3.668 3.635 0.489
2024-12-02-11:37:51-root-INFO: grad norm: 3.429 3.400 0.441
2024-12-02-11:37:52-root-INFO: grad norm: 3.971 3.940 0.491
2024-12-02-11:37:52-root-INFO: Loss Change: 94.312 -> 92.203
2024-12-02-11:37:52-root-INFO: Regularization Change: 0.000 -> 1.282
2024-12-02-11:37:52-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-11:37:52-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-11:37:52-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-11:37:52-root-INFO: grad norm: 4.071 4.039 0.504
2024-12-02-11:37:53-root-INFO: grad norm: 6.187 6.150 0.677
2024-12-02-11:37:53-root-INFO: Loss too large (91.816->92.175)! Learning rate decreased to 0.11019.
2024-12-02-11:37:53-root-INFO: grad norm: 3.790 3.756 0.505
2024-12-02-11:37:54-root-INFO: grad norm: 2.571 2.541 0.394
2024-12-02-11:37:54-root-INFO: grad norm: 3.023 2.995 0.414
2024-12-02-11:37:55-root-INFO: Loss Change: 92.160 -> 89.868
2024-12-02-11:37:55-root-INFO: Regularization Change: 0.000 -> 1.197
2024-12-02-11:37:55-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-11:37:55-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-11:37:55-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-11:37:55-root-INFO: grad norm: 3.714 3.680 0.502
2024-12-02-11:37:55-root-INFO: grad norm: 6.332 6.299 0.651
2024-12-02-11:37:56-root-INFO: Loss too large (89.767->90.172)! Learning rate decreased to 0.11263.
2024-12-02-11:37:56-root-INFO: Loss too large (89.767->89.786)! Learning rate decreased to 0.09010.
2024-12-02-11:37:56-root-INFO: grad norm: 3.987 3.956 0.494
2024-12-02-11:37:57-root-INFO: grad norm: 1.934 1.904 0.339
2024-12-02-11:37:57-root-INFO: grad norm: 1.891 1.861 0.332
2024-12-02-11:37:57-root-INFO: Loss Change: 89.829 -> 87.816
2024-12-02-11:37:57-root-INFO: Regularization Change: 0.000 -> 0.894
2024-12-02-11:37:57-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-11:37:57-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-11:37:58-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-11:37:58-root-INFO: grad norm: 2.967 2.918 0.538
2024-12-02-11:37:58-root-INFO: grad norm: 3.956 3.926 0.491
2024-12-02-11:37:58-root-INFO: Loss too large (87.613->87.944)! Learning rate decreased to 0.11510.
2024-12-02-11:37:59-root-INFO: grad norm: 5.514 5.481 0.606
2024-12-02-11:37:59-root-INFO: Loss too large (87.414->87.586)! Learning rate decreased to 0.09208.
2024-12-02-11:37:59-root-INFO: grad norm: 4.053 4.024 0.483
2024-12-02-11:38:00-root-INFO: grad norm: 2.391 2.365 0.353
2024-12-02-11:38:00-root-INFO: Loss Change: 87.879 -> 86.100
2024-12-02-11:38:00-root-INFO: Regularization Change: 0.000 -> 0.955
2024-12-02-11:38:00-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-11:38:00-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-11:38:01-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-11:38:01-root-INFO: grad norm: 2.647 2.613 0.421
2024-12-02-11:38:01-root-INFO: grad norm: 4.079 4.055 0.438
2024-12-02-11:38:01-root-INFO: Loss too large (85.614->85.981)! Learning rate decreased to 0.11760.
2024-12-02-11:38:02-root-INFO: Loss too large (85.614->85.694)! Learning rate decreased to 0.09408.
2024-12-02-11:38:02-root-INFO: grad norm: 3.908 3.881 0.456
2024-12-02-11:38:02-root-INFO: grad norm: 3.776 3.751 0.437
2024-12-02-11:38:03-root-INFO: grad norm: 3.795 3.769 0.449
2024-12-02-11:38:03-root-INFO: Loss Change: 85.944 -> 84.454
2024-12-02-11:38:03-root-INFO: Regularization Change: 0.000 -> 0.888
2024-12-02-11:38:03-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-11:38:03-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-11:38:03-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-11:38:04-root-INFO: grad norm: 5.020 4.973 0.687
2024-12-02-11:38:04-root-INFO: Loss too large (84.513->84.932)! Learning rate decreased to 0.12015.
2024-12-02-11:38:04-root-INFO: Loss too large (84.513->84.551)! Learning rate decreased to 0.09612.
2024-12-02-11:38:04-root-INFO: grad norm: 4.048 4.022 0.460
2024-12-02-11:38:05-root-INFO: grad norm: 3.186 3.160 0.403
2024-12-02-11:38:05-root-INFO: grad norm: 3.397 3.372 0.411
2024-12-02-11:38:06-root-INFO: grad norm: 3.710 3.686 0.422
2024-12-02-11:38:06-root-INFO: Loss Change: 84.513 -> 82.912
2024-12-02-11:38:06-root-INFO: Regularization Change: 0.000 -> 0.839
2024-12-02-11:38:06-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-11:38:06-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-11:38:06-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-11:38:06-root-INFO: grad norm: 3.837 3.810 0.447
2024-12-02-11:38:07-root-INFO: Loss too large (82.886->83.127)! Learning rate decreased to 0.12272.
2024-12-02-11:38:07-root-INFO: grad norm: 5.131 5.103 0.534
2024-12-02-11:38:07-root-INFO: Loss too large (82.668->82.788)! Learning rate decreased to 0.09818.
2024-12-02-11:38:08-root-INFO: grad norm: 4.039 4.014 0.455
2024-12-02-11:38:08-root-INFO: grad norm: 2.947 2.924 0.368
2024-12-02-11:38:09-root-INFO: grad norm: 3.197 3.173 0.390
2024-12-02-11:38:09-root-INFO: Loss Change: 82.886 -> 81.230
2024-12-02-11:38:09-root-INFO: Regularization Change: 0.000 -> 0.875
2024-12-02-11:38:09-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-11:38:09-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-11:38:09-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-11:38:09-root-INFO: grad norm: 4.511 4.467 0.627
2024-12-02-11:38:09-root-INFO: Loss too large (81.364->81.734)! Learning rate decreased to 0.12533.
2024-12-02-11:38:10-root-INFO: Loss too large (81.364->81.381)! Learning rate decreased to 0.10027.
2024-12-02-11:38:10-root-INFO: grad norm: 3.949 3.925 0.435
2024-12-02-11:38:10-root-INFO: grad norm: 3.533 3.508 0.413
2024-12-02-11:38:11-root-INFO: grad norm: 3.620 3.596 0.413
2024-12-02-11:38:11-root-INFO: grad norm: 3.721 3.698 0.411
2024-12-02-11:38:12-root-INFO: Loss Change: 81.364 -> 79.877
2024-12-02-11:38:12-root-INFO: Regularization Change: 0.000 -> 0.853
2024-12-02-11:38:12-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-11:38:12-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-11:38:12-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-11:38:12-root-INFO: grad norm: 3.727 3.702 0.431
2024-12-02-11:38:12-root-INFO: Loss too large (79.716->79.949)! Learning rate decreased to 0.12798.
2024-12-02-11:38:13-root-INFO: grad norm: 4.872 4.845 0.518
2024-12-02-11:38:13-root-INFO: Loss too large (79.518->79.573)! Learning rate decreased to 0.10238.
2024-12-02-11:38:13-root-INFO: grad norm: 3.986 3.962 0.435
2024-12-02-11:38:14-root-INFO: grad norm: 3.245 3.222 0.385
2024-12-02-11:38:14-root-INFO: grad norm: 3.382 3.360 0.389
2024-12-02-11:38:15-root-INFO: Loss Change: 79.716 -> 78.109
2024-12-02-11:38:15-root-INFO: Regularization Change: 0.000 -> 0.914
2024-12-02-11:38:15-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-11:38:15-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-11:38:15-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-11:38:15-root-INFO: grad norm: 4.401 4.362 0.589
2024-12-02-11:38:15-root-INFO: Loss too large (78.281->78.617)! Learning rate decreased to 0.13066.
2024-12-02-11:38:16-root-INFO: grad norm: 4.081 4.055 0.462
2024-12-02-11:38:16-root-INFO: grad norm: 3.766 3.740 0.443
2024-12-02-11:38:16-root-INFO: grad norm: 4.020 3.993 0.463
2024-12-02-11:38:17-root-INFO: grad norm: 4.287 4.260 0.474
2024-12-02-11:38:17-root-INFO: Loss too large (76.878->76.880)! Learning rate decreased to 0.10453.
2024-12-02-11:38:18-root-INFO: Loss Change: 78.281 -> 76.571
2024-12-02-11:38:18-root-INFO: Regularization Change: 0.000 -> 1.213
2024-12-02-11:38:18-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-11:38:18-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-11:38:18-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-11:38:18-root-INFO: grad norm: 3.637 3.615 0.405
2024-12-02-11:38:18-root-INFO: Loss too large (76.456->76.562)! Learning rate decreased to 0.13338.
2024-12-02-11:38:19-root-INFO: grad norm: 4.301 4.278 0.447
2024-12-02-11:38:19-root-INFO: grad norm: 3.962 3.937 0.443
2024-12-02-11:38:20-root-INFO: grad norm: 3.570 3.547 0.403
2024-12-02-11:38:20-root-INFO: grad norm: 3.867 3.841 0.444
2024-12-02-11:38:20-root-INFO: Loss Change: 76.456 -> 74.831
2024-12-02-11:38:20-root-INFO: Regularization Change: 0.000 -> 1.311
2024-12-02-11:38:20-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-11:38:20-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-11:38:21-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-11:38:21-root-INFO: grad norm: 4.900 4.852 0.684
2024-12-02-11:38:21-root-INFO: Loss too large (74.986->75.201)! Learning rate decreased to 0.13613.
2024-12-02-11:38:21-root-INFO: grad norm: 4.110 4.081 0.484
2024-12-02-11:38:22-root-INFO: grad norm: 3.470 3.443 0.435
2024-12-02-11:38:22-root-INFO: grad norm: 3.826 3.799 0.451
2024-12-02-11:38:23-root-INFO: grad norm: 4.175 4.149 0.465
2024-12-02-11:38:23-root-INFO: Loss Change: 74.986 -> 73.234
2024-12-02-11:38:23-root-INFO: Regularization Change: 0.000 -> 1.491
2024-12-02-11:38:23-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-11:38:23-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-11:38:23-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-11:38:24-root-INFO: grad norm: 4.038 4.016 0.425
2024-12-02-11:38:24-root-INFO: grad norm: 5.168 5.137 0.560
2024-12-02-11:38:24-root-INFO: Loss too large (72.767->72.849)! Learning rate decreased to 0.13891.
2024-12-02-11:38:25-root-INFO: grad norm: 4.221 4.191 0.495
2024-12-02-11:38:25-root-INFO: grad norm: 3.653 3.628 0.434
2024-12-02-11:38:26-root-INFO: grad norm: 3.816 3.789 0.452
2024-12-02-11:38:26-root-INFO: Loss Change: 72.888 -> 70.964
2024-12-02-11:38:26-root-INFO: Regularization Change: 0.000 -> 1.573
2024-12-02-11:38:26-root-INFO: Undo step: 50
2024-12-02-11:38:26-root-INFO: Undo step: 51
2024-12-02-11:38:26-root-INFO: Undo step: 52
2024-12-02-11:38:26-root-INFO: Undo step: 53
2024-12-02-11:38:26-root-INFO: Undo step: 54
2024-12-02-11:38:26-root-INFO: Undo step: 55
2024-12-02-11:38:26-root-INFO: Undo step: 56
2024-12-02-11:38:26-root-INFO: Undo step: 57
2024-12-02-11:38:26-root-INFO: Undo step: 58
2024-12-02-11:38:26-root-INFO: Undo step: 59
2024-12-02-11:38:26-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-11:38:26-root-INFO: grad norm: 31.510 31.254 4.009
2024-12-02-11:38:27-root-INFO: grad norm: 17.293 17.069 2.774
2024-12-02-11:38:28-root-INFO: grad norm: 11.623 11.433 2.096
2024-12-02-11:38:28-root-INFO: grad norm: 8.877 8.707 1.726
2024-12-02-11:38:29-root-INFO: grad norm: 7.311 7.163 1.463
2024-12-02-11:38:29-root-INFO: Loss Change: 271.612 -> 117.865
2024-12-02-11:38:29-root-INFO: Regularization Change: 0.000 -> 86.301
2024-12-02-11:38:29-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-11:38:29-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-11:38:29-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-11:38:29-root-INFO: grad norm: 6.405 6.281 1.251
2024-12-02-11:38:30-root-INFO: grad norm: 5.695 5.588 1.102
2024-12-02-11:38:30-root-INFO: grad norm: 5.472 5.374 1.029
2024-12-02-11:38:31-root-INFO: grad norm: 6.247 6.164 1.015
2024-12-02-11:38:31-root-INFO: grad norm: 5.237 5.147 0.963
2024-12-02-11:38:31-root-INFO: Loss Change: 117.515 -> 99.817
2024-12-02-11:38:31-root-INFO: Regularization Change: 0.000 -> 12.887
2024-12-02-11:38:31-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-11:38:31-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-11:38:32-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-11:38:32-root-INFO: grad norm: 4.616 4.534 0.863
2024-12-02-11:38:32-root-INFO: grad norm: 4.454 4.374 0.837
2024-12-02-11:38:33-root-INFO: grad norm: 5.425 5.361 0.828
2024-12-02-11:38:33-root-INFO: grad norm: 4.923 4.848 0.853
2024-12-02-11:38:34-root-INFO: grad norm: 4.094 4.033 0.701
2024-12-02-11:38:34-root-INFO: Loss Change: 99.599 -> 91.416
2024-12-02-11:38:34-root-INFO: Regularization Change: 0.000 -> 6.422
2024-12-02-11:38:34-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-11:38:34-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-11:38:34-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-11:38:34-root-INFO: grad norm: 4.467 4.407 0.729
2024-12-02-11:38:35-root-INFO: grad norm: 5.687 5.638 0.742
2024-12-02-11:38:35-root-INFO: grad norm: 4.840 4.775 0.791
2024-12-02-11:38:36-root-INFO: grad norm: 3.770 3.722 0.602
2024-12-02-11:38:36-root-INFO: grad norm: 3.965 3.907 0.675
2024-12-02-11:38:37-root-INFO: Loss Change: 90.912 -> 85.645
2024-12-02-11:38:37-root-INFO: Regularization Change: 0.000 -> 4.333
2024-12-02-11:38:37-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-11:38:37-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-11:38:37-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-11:38:37-root-INFO: grad norm: 5.668 5.606 0.836
2024-12-02-11:38:37-root-INFO: grad norm: 5.217 5.152 0.819
2024-12-02-11:38:38-root-INFO: grad norm: 4.553 4.506 0.656
2024-12-02-11:38:38-root-INFO: grad norm: 4.898 4.837 0.769
2024-12-02-11:38:39-root-INFO: grad norm: 5.694 5.647 0.734
2024-12-02-11:38:39-root-INFO: Loss Change: 85.812 -> 82.373
2024-12-02-11:38:39-root-INFO: Regularization Change: 0.000 -> 3.742
2024-12-02-11:38:39-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-11:38:39-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-11:38:39-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-11:38:39-root-INFO: grad norm: 4.726 4.673 0.710
2024-12-02-11:38:40-root-INFO: grad norm: 3.820 3.780 0.553
2024-12-02-11:38:40-root-INFO: grad norm: 4.299 4.246 0.676
2024-12-02-11:38:41-root-INFO: grad norm: 5.295 5.250 0.686
2024-12-02-11:38:41-root-INFO: grad norm: 4.750 4.689 0.760
2024-12-02-11:38:41-root-INFO: Loss Change: 82.067 -> 77.966
2024-12-02-11:38:41-root-INFO: Regularization Change: 0.000 -> 3.015
2024-12-02-11:38:41-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-11:38:41-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-11:38:42-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-11:38:42-root-INFO: grad norm: 4.822 4.759 0.779
2024-12-02-11:38:42-root-INFO: grad norm: 4.900 4.840 0.763
2024-12-02-11:38:43-root-INFO: grad norm: 5.171 5.124 0.701
2024-12-02-11:38:43-root-INFO: grad norm: 4.840 4.780 0.758
2024-12-02-11:38:44-root-INFO: grad norm: 4.403 4.360 0.615
2024-12-02-11:38:44-root-INFO: Loss Change: 77.987 -> 75.378
2024-12-02-11:38:44-root-INFO: Regularization Change: 0.000 -> 2.701
2024-12-02-11:38:44-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-11:38:44-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-11:38:44-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-11:38:44-root-INFO: grad norm: 4.439 4.391 0.650
2024-12-02-11:38:45-root-INFO: grad norm: 4.796 4.756 0.624
2024-12-02-11:38:45-root-INFO: grad norm: 4.685 4.629 0.723
2024-12-02-11:38:46-root-INFO: grad norm: 4.517 4.476 0.607
2024-12-02-11:38:46-root-INFO: grad norm: 4.579 4.523 0.712
2024-12-02-11:38:47-root-INFO: Loss Change: 75.162 -> 72.598
2024-12-02-11:38:47-root-INFO: Regularization Change: 0.000 -> 2.464
2024-12-02-11:38:47-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-11:38:47-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-11:38:47-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-11:38:47-root-INFO: grad norm: 5.363 5.303 0.800
2024-12-02-11:38:47-root-INFO: grad norm: 4.924 4.863 0.769
2024-12-02-11:38:48-root-INFO: grad norm: 4.448 4.408 0.600
2024-12-02-11:38:48-root-INFO: grad norm: 4.476 4.423 0.684
2024-12-02-11:38:49-root-INFO: grad norm: 4.523 4.483 0.603
2024-12-02-11:38:49-root-INFO: Loss Change: 72.789 -> 70.376
2024-12-02-11:38:49-root-INFO: Regularization Change: 0.000 -> 2.511
2024-12-02-11:38:49-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-11:38:49-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-11:38:49-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-11:38:50-root-INFO: grad norm: 4.248 4.206 0.595
2024-12-02-11:38:50-root-INFO: grad norm: 4.240 4.202 0.567
2024-12-02-11:38:51-root-INFO: grad norm: 4.275 4.227 0.636
2024-12-02-11:38:51-root-INFO: grad norm: 4.290 4.251 0.574
2024-12-02-11:38:52-root-INFO: grad norm: 4.294 4.245 0.650
2024-12-02-11:38:52-root-INFO: Loss Change: 70.072 -> 67.803
2024-12-02-11:38:52-root-INFO: Regularization Change: 0.000 -> 2.282
2024-12-02-11:38:52-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-11:38:52-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-11:38:52-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-11:38:52-root-INFO: grad norm: 4.991 4.932 0.766
2024-12-02-11:38:53-root-INFO: grad norm: 4.644 4.590 0.707
2024-12-02-11:38:53-root-INFO: grad norm: 4.313 4.274 0.580
2024-12-02-11:38:54-root-INFO: grad norm: 4.253 4.205 0.635
2024-12-02-11:38:54-root-INFO: grad norm: 4.214 4.177 0.562
2024-12-02-11:38:54-root-INFO: Loss Change: 67.754 -> 65.482
2024-12-02-11:38:54-root-INFO: Regularization Change: 0.000 -> 2.354
2024-12-02-11:38:54-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-11:38:54-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-11:38:55-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-11:38:55-root-INFO: grad norm: 3.815 3.776 0.543
2024-12-02-11:38:55-root-INFO: grad norm: 3.747 3.714 0.500
2024-12-02-11:38:56-root-INFO: grad norm: 3.767 3.725 0.559
2024-12-02-11:38:56-root-INFO: grad norm: 3.790 3.757 0.504
2024-12-02-11:38:57-root-INFO: grad norm: 3.823 3.780 0.572
2024-12-02-11:38:57-root-INFO: Loss Change: 65.237 -> 63.199
2024-12-02-11:38:57-root-INFO: Regularization Change: 0.000 -> 2.140
2024-12-02-11:38:57-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-11:38:57-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-11:38:57-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-11:38:57-root-INFO: grad norm: 4.395 4.345 0.661
2024-12-02-11:38:58-root-INFO: grad norm: 4.199 4.151 0.632
2024-12-02-11:38:58-root-INFO: grad norm: 4.007 3.972 0.524
2024-12-02-11:38:59-root-INFO: grad norm: 3.920 3.877 0.580
2024-12-02-11:38:59-root-INFO: grad norm: 3.863 3.830 0.504
2024-12-02-11:39:00-root-INFO: Loss Change: 63.273 -> 61.228
2024-12-02-11:39:00-root-INFO: Regularization Change: 0.000 -> 2.202
2024-12-02-11:39:00-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-11:39:00-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-11:39:00-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-11:39:00-root-INFO: grad norm: 3.467 3.432 0.489
2024-12-02-11:39:00-root-INFO: grad norm: 3.348 3.318 0.445
2024-12-02-11:39:01-root-INFO: grad norm: 3.368 3.332 0.487
2024-12-02-11:39:01-root-INFO: grad norm: 3.420 3.390 0.456
2024-12-02-11:39:02-root-INFO: grad norm: 3.468 3.430 0.510
2024-12-02-11:39:02-root-INFO: Loss Change: 60.997 -> 59.087
2024-12-02-11:39:02-root-INFO: Regularization Change: 0.000 -> 2.047
2024-12-02-11:39:02-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-11:39:02-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-11:39:02-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-11:39:02-root-INFO: grad norm: 4.191 4.141 0.646
2024-12-02-11:39:03-root-INFO: grad norm: 3.983 3.939 0.590
2024-12-02-11:39:03-root-INFO: grad norm: 3.790 3.757 0.495
2024-12-02-11:39:04-root-INFO: grad norm: 3.706 3.667 0.537
2024-12-02-11:39:04-root-INFO: grad norm: 3.657 3.626 0.475
2024-12-02-11:39:05-root-INFO: Loss Change: 59.198 -> 57.254
2024-12-02-11:39:05-root-INFO: Regularization Change: 0.000 -> 2.161
2024-12-02-11:39:05-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-11:39:05-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-11:39:05-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-11:39:05-root-INFO: grad norm: 3.294 3.261 0.467
2024-12-02-11:39:05-root-INFO: grad norm: 3.158 3.131 0.414
2024-12-02-11:39:06-root-INFO: grad norm: 3.157 3.124 0.452
2024-12-02-11:39:06-root-INFO: grad norm: 3.195 3.167 0.420
2024-12-02-11:39:07-root-INFO: grad norm: 3.226 3.192 0.470
2024-12-02-11:39:07-root-INFO: Loss Change: 56.939 -> 55.074
2024-12-02-11:39:07-root-INFO: Regularization Change: 0.000 -> 2.017
2024-12-02-11:39:07-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-11:39:07-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-11:39:07-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-11:39:07-root-INFO: grad norm: 3.939 3.891 0.613
2024-12-02-11:39:08-root-INFO: grad norm: 3.761 3.722 0.542
2024-12-02-11:39:08-root-INFO: grad norm: 3.564 3.534 0.459
2024-12-02-11:39:09-root-INFO: grad norm: 3.502 3.467 0.491
2024-12-02-11:39:09-root-INFO: grad norm: 3.480 3.451 0.447
2024-12-02-11:39:10-root-INFO: Loss Change: 55.231 -> 53.405
2024-12-02-11:39:10-root-INFO: Regularization Change: 0.000 -> 2.142
2024-12-02-11:39:10-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-11:39:10-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-11:39:10-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-11:39:10-root-INFO: grad norm: 3.170 3.140 0.430
2024-12-02-11:39:10-root-INFO: grad norm: 3.006 2.981 0.386
2024-12-02-11:39:11-root-INFO: grad norm: 2.978 2.949 0.416
2024-12-02-11:39:11-root-INFO: grad norm: 3.001 2.976 0.391
2024-12-02-11:39:12-root-INFO: grad norm: 3.014 2.983 0.433
2024-12-02-11:39:12-root-INFO: Loss Change: 53.015 -> 51.179
2024-12-02-11:39:12-root-INFO: Regularization Change: 0.000 -> 2.011
2024-12-02-11:39:12-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-11:39:12-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-11:39:12-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-11:39:12-root-INFO: grad norm: 3.524 3.485 0.523
2024-12-02-11:39:13-root-INFO: grad norm: 3.390 3.355 0.490
2024-12-02-11:39:13-root-INFO: grad norm: 3.276 3.249 0.421
2024-12-02-11:39:14-root-INFO: grad norm: 3.236 3.203 0.462
2024-12-02-11:39:14-root-INFO: grad norm: 3.210 3.183 0.411
2024-12-02-11:39:15-root-INFO: Loss Change: 51.179 -> 49.483
2024-12-02-11:39:15-root-INFO: Regularization Change: 0.000 -> 2.092
2024-12-02-11:39:15-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-11:39:15-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-11:39:15-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-11:39:15-root-INFO: grad norm: 2.959 2.925 0.449
2024-12-02-11:39:15-root-INFO: grad norm: 2.702 2.679 0.355
2024-12-02-11:39:16-root-INFO: grad norm: 2.709 2.682 0.376
2024-12-02-11:39:16-root-INFO: grad norm: 2.777 2.753 0.361
2024-12-02-11:39:17-root-INFO: grad norm: 2.817 2.788 0.402
2024-12-02-11:39:17-root-INFO: Loss Change: 49.227 -> 47.383
2024-12-02-11:39:17-root-INFO: Regularization Change: 0.000 -> 2.050
2024-12-02-11:39:17-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-11:39:17-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-11:39:17-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-11:39:17-root-INFO: grad norm: 3.603 3.557 0.571
2024-12-02-11:39:18-root-INFO: grad norm: 3.372 3.336 0.488
2024-12-02-11:39:18-root-INFO: grad norm: 3.165 3.139 0.406
2024-12-02-11:39:19-root-INFO: grad norm: 3.112 3.081 0.438
2024-12-02-11:39:19-root-INFO: grad norm: 3.103 3.077 0.398
2024-12-02-11:39:20-root-INFO: Loss Change: 47.457 -> 45.711
2024-12-02-11:39:20-root-INFO: Regularization Change: 0.000 -> 2.177
2024-12-02-11:39:20-root-INFO: Undo step: 40
2024-12-02-11:39:20-root-INFO: Undo step: 41
2024-12-02-11:39:20-root-INFO: Undo step: 42
2024-12-02-11:39:20-root-INFO: Undo step: 43
2024-12-02-11:39:20-root-INFO: Undo step: 44
2024-12-02-11:39:20-root-INFO: Undo step: 45
2024-12-02-11:39:20-root-INFO: Undo step: 46
2024-12-02-11:39:20-root-INFO: Undo step: 47
2024-12-02-11:39:20-root-INFO: Undo step: 48
2024-12-02-11:39:20-root-INFO: Undo step: 49
2024-12-02-11:39:20-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-11:39:20-root-INFO: grad norm: 26.864 26.689 3.061
2024-12-02-11:39:21-root-INFO: grad norm: 14.587 14.432 2.119
2024-12-02-11:39:21-root-INFO: grad norm: 10.339 10.224 1.538
2024-12-02-11:39:22-root-INFO: grad norm: 8.567 8.469 1.288
2024-12-02-11:39:22-root-INFO: grad norm: 8.202 8.126 1.111
2024-12-02-11:39:22-root-INFO: Loss Change: 238.599 -> 93.473
2024-12-02-11:39:22-root-INFO: Regularization Change: 0.000 -> 101.699
2024-12-02-11:39:22-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-11:39:22-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-11:39:23-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-11:39:23-root-INFO: grad norm: 6.933 6.868 0.950
2024-12-02-11:39:23-root-INFO: grad norm: 5.538 5.474 0.838
2024-12-02-11:39:24-root-INFO: grad norm: 4.942 4.886 0.740
2024-12-02-11:39:24-root-INFO: grad norm: 4.656 4.603 0.703
2024-12-02-11:39:25-root-INFO: grad norm: 5.069 5.022 0.686
2024-12-02-11:39:25-root-INFO: Loss Change: 93.106 -> 76.332
2024-12-02-11:39:25-root-INFO: Regularization Change: 0.000 -> 15.356
2024-12-02-11:39:25-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-11:39:25-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-11:39:25-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-11:39:26-root-INFO: grad norm: 6.972 6.906 0.956
2024-12-02-11:39:26-root-INFO: grad norm: 5.205 5.146 0.781
2024-12-02-11:39:27-root-INFO: grad norm: 4.377 4.329 0.645
2024-12-02-11:39:27-root-INFO: grad norm: 4.346 4.310 0.560
2024-12-02-11:39:28-root-INFO: grad norm: 4.695 4.666 0.522
2024-12-02-11:39:28-root-INFO: Loss Change: 76.291 -> 68.282
2024-12-02-11:39:28-root-INFO: Regularization Change: 0.000 -> 7.201
2024-12-02-11:39:28-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-11:39:28-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-11:39:28-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-11:39:28-root-INFO: grad norm: 6.041 5.994 0.745
2024-12-02-11:39:29-root-INFO: grad norm: 4.639 4.610 0.522
2024-12-02-11:39:29-root-INFO: grad norm: 3.361 3.332 0.445
2024-12-02-11:39:30-root-INFO: grad norm: 3.623 3.598 0.418
2024-12-02-11:39:30-root-INFO: grad norm: 4.496 4.466 0.521
2024-12-02-11:39:31-root-INFO: Loss Change: 68.360 -> 63.107
2024-12-02-11:39:31-root-INFO: Regularization Change: 0.000 -> 5.414
2024-12-02-11:39:31-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-11:39:31-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-11:39:31-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-11:39:31-root-INFO: grad norm: 4.693 4.664 0.515
2024-12-02-11:39:32-root-INFO: grad norm: 5.124 5.081 0.663
2024-12-02-11:39:32-root-INFO: grad norm: 5.035 4.984 0.714
2024-12-02-11:39:33-root-INFO: grad norm: 4.842 4.797 0.659
2024-12-02-11:39:33-root-INFO: grad norm: 4.909 4.860 0.695
2024-12-02-11:39:33-root-INFO: Loss Change: 62.785 -> 58.986
2024-12-02-11:39:33-root-INFO: Regularization Change: 0.000 -> 4.161
2024-12-02-11:39:33-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-11:39:33-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-11:39:33-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-11:39:34-root-INFO: grad norm: 5.470 5.401 0.866
2024-12-02-11:39:34-root-INFO: grad norm: 5.017 4.964 0.721
2024-12-02-11:39:35-root-INFO: grad norm: 4.452 4.414 0.579
2024-12-02-11:39:35-root-INFO: grad norm: 4.541 4.502 0.593
2024-12-02-11:39:35-root-INFO: grad norm: 4.736 4.692 0.647
2024-12-02-11:39:36-root-INFO: Loss Change: 59.025 -> 55.906
2024-12-02-11:39:36-root-INFO: Regularization Change: 0.000 -> 3.729
2024-12-02-11:39:36-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-11:39:36-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-11:39:36-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-11:39:36-root-INFO: grad norm: 4.493 4.457 0.563
2024-12-02-11:39:37-root-INFO: grad norm: 4.249 4.214 0.544
2024-12-02-11:39:37-root-INFO: grad norm: 4.259 4.224 0.547
2024-12-02-11:39:38-root-INFO: grad norm: 4.372 4.332 0.590
2024-12-02-11:39:38-root-INFO: grad norm: 4.291 4.250 0.593
2024-12-02-11:39:39-root-INFO: Loss Change: 55.650 -> 52.886
2024-12-02-11:39:39-root-INFO: Regularization Change: 0.000 -> 3.151
2024-12-02-11:39:39-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-11:39:39-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-11:39:39-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-11:39:39-root-INFO: grad norm: 4.720 4.660 0.753
2024-12-02-11:39:40-root-INFO: grad norm: 4.345 4.303 0.605
2024-12-02-11:39:40-root-INFO: grad norm: 4.051 4.014 0.544
2024-12-02-11:39:41-root-INFO: grad norm: 3.929 3.893 0.532
2024-12-02-11:39:41-root-INFO: grad norm: 3.911 3.875 0.534
2024-12-02-11:39:41-root-INFO: Loss Change: 52.827 -> 50.200
2024-12-02-11:39:41-root-INFO: Regularization Change: 0.000 -> 2.923
2024-12-02-11:39:41-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-11:39:41-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-11:39:42-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-11:39:42-root-INFO: grad norm: 3.676 3.646 0.468
2024-12-02-11:39:42-root-INFO: grad norm: 3.593 3.560 0.480
2024-12-02-11:39:43-root-INFO: grad norm: 3.576 3.543 0.487
2024-12-02-11:39:43-root-INFO: grad norm: 3.596 3.562 0.489
2024-12-02-11:39:44-root-INFO: grad norm: 3.595 3.560 0.500
2024-12-02-11:39:44-root-INFO: Loss Change: 49.930 -> 47.814
2024-12-02-11:39:44-root-INFO: Regularization Change: 0.000 -> 2.539
2024-12-02-11:39:44-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-11:39:44-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-11:39:44-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-11:39:45-root-INFO: grad norm: 4.466 4.396 0.786
2024-12-02-11:39:45-root-INFO: grad norm: 3.934 3.893 0.569
2024-12-02-11:39:45-root-INFO: grad norm: 3.446 3.415 0.461
2024-12-02-11:39:46-root-INFO: grad norm: 3.322 3.292 0.439
2024-12-02-11:39:46-root-INFO: grad norm: 3.392 3.359 0.469
2024-12-02-11:39:47-root-INFO: Loss Change: 47.969 -> 45.619
2024-12-02-11:39:47-root-INFO: Regularization Change: 0.000 -> 2.598
2024-12-02-11:39:47-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-11:39:47-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-11:39:47-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-11:39:47-root-INFO: grad norm: 3.125 3.097 0.418
2024-12-02-11:39:48-root-INFO: grad norm: 3.006 2.977 0.415
2024-12-02-11:39:48-root-INFO: grad norm: 2.997 2.968 0.415
2024-12-02-11:39:49-root-INFO: grad norm: 3.042 3.011 0.429
2024-12-02-11:39:49-root-INFO: grad norm: 3.074 3.042 0.438
2024-12-02-11:39:49-root-INFO: Loss Change: 45.343 -> 43.453
2024-12-02-11:39:49-root-INFO: Regularization Change: 0.000 -> 2.244
2024-12-02-11:39:49-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-11:39:49-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-11:39:50-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-11:39:50-root-INFO: grad norm: 3.666 3.616 0.607
2024-12-02-11:39:50-root-INFO: grad norm: 3.409 3.373 0.493
2024-12-02-11:39:51-root-INFO: grad norm: 3.134 3.104 0.429
2024-12-02-11:39:51-root-INFO: grad norm: 3.078 3.048 0.423
2024-12-02-11:39:52-root-INFO: grad norm: 3.105 3.075 0.430
2024-12-02-11:39:52-root-INFO: Loss Change: 43.427 -> 41.621
2024-12-02-11:39:52-root-INFO: Regularization Change: 0.000 -> 2.266
2024-12-02-11:39:52-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-11:39:52-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-11:39:52-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-11:39:52-root-INFO: grad norm: 2.905 2.878 0.393
2024-12-02-11:39:53-root-INFO: grad norm: 2.748 2.722 0.378
2024-12-02-11:39:53-root-INFO: grad norm: 2.749 2.724 0.373
2024-12-02-11:39:54-root-INFO: grad norm: 2.801 2.773 0.392
2024-12-02-11:39:54-root-INFO: grad norm: 2.818 2.790 0.396
2024-12-02-11:39:55-root-INFO: Loss Change: 41.299 -> 39.566
2024-12-02-11:39:55-root-INFO: Regularization Change: 0.000 -> 2.091
2024-12-02-11:39:55-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-11:39:55-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-11:39:55-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-11:39:55-root-INFO: grad norm: 3.500 3.448 0.600
2024-12-02-11:39:55-root-INFO: grad norm: 3.145 3.111 0.461
2024-12-02-11:39:56-root-INFO: grad norm: 2.804 2.777 0.386
2024-12-02-11:39:56-root-INFO: grad norm: 2.734 2.708 0.376
2024-12-02-11:39:57-root-INFO: grad norm: 2.757 2.731 0.382
2024-12-02-11:39:57-root-INFO: Loss Change: 39.570 -> 37.832
2024-12-02-11:39:57-root-INFO: Regularization Change: 0.000 -> 2.148
2024-12-02-11:39:57-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-11:39:57-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-11:39:57-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-11:39:57-root-INFO: grad norm: 2.613 2.588 0.358
2024-12-02-11:39:58-root-INFO: grad norm: 2.514 2.490 0.350
2024-12-02-11:39:58-root-INFO: grad norm: 2.517 2.493 0.346
2024-12-02-11:39:59-root-INFO: grad norm: 2.551 2.526 0.357
2024-12-02-11:39:59-root-INFO: grad norm: 2.564 2.539 0.362
2024-12-02-11:39:59-root-INFO: Loss Change: 37.586 -> 35.991
2024-12-02-11:39:59-root-INFO: Regularization Change: 0.000 -> 1.974
2024-12-02-11:39:59-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-11:39:59-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-11:40:00-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-11:40:00-root-INFO: grad norm: 3.199 3.152 0.541
2024-12-02-11:40:00-root-INFO: grad norm: 2.873 2.843 0.418
2024-12-02-11:40:01-root-INFO: grad norm: 2.559 2.535 0.351
2024-12-02-11:40:01-root-INFO: grad norm: 2.490 2.467 0.339
2024-12-02-11:40:02-root-INFO: grad norm: 2.489 2.465 0.340
2024-12-02-11:40:02-root-INFO: Loss Change: 36.007 -> 34.389
2024-12-02-11:40:02-root-INFO: Regularization Change: 0.000 -> 2.039
2024-12-02-11:40:02-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-11:40:02-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-11:40:02-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-11:40:02-root-INFO: grad norm: 2.408 2.385 0.332
2024-12-02-11:40:03-root-INFO: grad norm: 2.259 2.239 0.300
2024-12-02-11:40:03-root-INFO: grad norm: 2.255 2.234 0.303
2024-12-02-11:40:04-root-INFO: grad norm: 2.290 2.269 0.308
2024-12-02-11:40:04-root-INFO: grad norm: 2.299 2.277 0.319
2024-12-02-11:40:05-root-INFO: Loss Change: 34.138 -> 32.625
2024-12-02-11:40:05-root-INFO: Regularization Change: 0.000 -> 1.899
2024-12-02-11:40:05-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-11:40:05-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-11:40:05-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-11:40:05-root-INFO: grad norm: 2.854 2.813 0.484
2024-12-02-11:40:05-root-INFO: grad norm: 2.576 2.550 0.364
2024-12-02-11:40:06-root-INFO: grad norm: 2.339 2.318 0.317
2024-12-02-11:40:06-root-INFO: grad norm: 2.272 2.251 0.308
2024-12-02-11:40:07-root-INFO: grad norm: 2.239 2.219 0.299
2024-12-02-11:40:07-root-INFO: Loss Change: 32.437 -> 30.919
2024-12-02-11:40:07-root-INFO: Regularization Change: 0.000 -> 1.959
2024-12-02-11:40:07-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-11:40:07-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-11:40:07-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-11:40:08-root-INFO: grad norm: 2.209 2.185 0.326
2024-12-02-11:40:08-root-INFO: grad norm: 2.004 1.986 0.264
2024-12-02-11:40:08-root-INFO: grad norm: 2.005 1.987 0.265
2024-12-02-11:40:09-root-INFO: grad norm: 2.053 2.035 0.271
2024-12-02-11:40:09-root-INFO: grad norm: 2.069 2.050 0.283
2024-12-02-11:40:10-root-INFO: Loss Change: 30.712 -> 29.268
2024-12-02-11:40:10-root-INFO: Regularization Change: 0.000 -> 1.845
2024-12-02-11:40:10-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-11:40:10-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-11:40:10-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-11:40:10-root-INFO: grad norm: 2.574 2.538 0.424
2024-12-02-11:40:10-root-INFO: grad norm: 2.320 2.297 0.326
2024-12-02-11:40:11-root-INFO: grad norm: 2.124 2.106 0.281
2024-12-02-11:40:11-root-INFO: grad norm: 2.067 2.048 0.278
2024-12-02-11:40:12-root-INFO: grad norm: 2.027 2.010 0.263
2024-12-02-11:40:12-root-INFO: Loss Change: 29.193 -> 27.776
2024-12-02-11:40:12-root-INFO: Regularization Change: 0.000 -> 1.875
2024-12-02-11:40:12-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-11:40:12-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-11:40:12-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-11:40:12-root-INFO: grad norm: 2.026 2.005 0.294
2024-12-02-11:40:13-root-INFO: grad norm: 1.806 1.791 0.230
2024-12-02-11:40:13-root-INFO: grad norm: 1.788 1.774 0.227
2024-12-02-11:40:14-root-INFO: grad norm: 1.809 1.795 0.227
2024-12-02-11:40:14-root-INFO: grad norm: 1.827 1.811 0.238
2024-12-02-11:40:15-root-INFO: Loss Change: 27.521 -> 26.159
2024-12-02-11:40:15-root-INFO: Regularization Change: 0.000 -> 1.780
2024-12-02-11:40:15-root-INFO: Undo step: 30
2024-12-02-11:40:15-root-INFO: Undo step: 31
2024-12-02-11:40:15-root-INFO: Undo step: 32
2024-12-02-11:40:15-root-INFO: Undo step: 33
2024-12-02-11:40:15-root-INFO: Undo step: 34
2024-12-02-11:40:15-root-INFO: Undo step: 35
2024-12-02-11:40:15-root-INFO: Undo step: 36
2024-12-02-11:40:15-root-INFO: Undo step: 37
2024-12-02-11:40:15-root-INFO: Undo step: 38
2024-12-02-11:40:15-root-INFO: Undo step: 39
2024-12-02-11:40:15-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-11:40:15-root-INFO: grad norm: 23.155 23.028 2.424
2024-12-02-11:40:16-root-INFO: grad norm: 12.113 11.997 1.675
2024-12-02-11:40:16-root-INFO: grad norm: 9.111 9.024 1.255
2024-12-02-11:40:16-root-INFO: grad norm: 7.373 7.295 1.065
2024-12-02-11:40:17-root-INFO: grad norm: 6.173 6.110 0.879
2024-12-02-11:40:17-root-INFO: Loss Change: 195.805 -> 69.452
2024-12-02-11:40:17-root-INFO: Regularization Change: 0.000 -> 105.861
2024-12-02-11:40:17-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-11:40:17-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-11:40:17-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-11:40:18-root-INFO: grad norm: 5.559 5.507 0.762
2024-12-02-11:40:18-root-INFO: grad norm: 5.523 5.476 0.717
2024-12-02-11:40:19-root-INFO: grad norm: 5.285 5.243 0.663
2024-12-02-11:40:19-root-INFO: grad norm: 4.937 4.902 0.587
2024-12-02-11:40:20-root-INFO: grad norm: 4.697 4.661 0.582
2024-12-02-11:40:20-root-INFO: Loss Change: 69.006 -> 54.244
2024-12-02-11:40:20-root-INFO: Regularization Change: 0.000 -> 16.887
2024-12-02-11:40:20-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-11:40:20-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-11:40:20-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-11:40:20-root-INFO: grad norm: 4.911 4.869 0.645
2024-12-02-11:40:21-root-INFO: grad norm: 4.543 4.500 0.619
2024-12-02-11:40:21-root-INFO: grad norm: 4.243 4.210 0.522
2024-12-02-11:40:22-root-INFO: grad norm: 3.988 3.949 0.559
2024-12-02-11:40:22-root-INFO: grad norm: 3.755 3.726 0.468
2024-12-02-11:40:23-root-INFO: Loss Change: 53.984 -> 46.678
2024-12-02-11:40:23-root-INFO: Regularization Change: 0.000 -> 8.386
2024-12-02-11:40:23-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-11:40:23-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-11:40:23-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-11:40:23-root-INFO: grad norm: 3.532 3.498 0.488
2024-12-02-11:40:23-root-INFO: grad norm: 3.380 3.354 0.413
2024-12-02-11:40:24-root-INFO: grad norm: 3.330 3.298 0.462
2024-12-02-11:40:24-root-INFO: grad norm: 3.313 3.288 0.413
2024-12-02-11:40:25-root-INFO: grad norm: 3.285 3.252 0.466
2024-12-02-11:40:25-root-INFO: Loss Change: 46.244 -> 41.657
2024-12-02-11:40:25-root-INFO: Regularization Change: 0.000 -> 5.394
2024-12-02-11:40:25-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-11:40:25-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-11:40:25-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-11:40:25-root-INFO: grad norm: 3.687 3.645 0.556
2024-12-02-11:40:26-root-INFO: grad norm: 3.387 3.351 0.491
2024-12-02-11:40:26-root-INFO: grad norm: 3.057 3.032 0.388
2024-12-02-11:40:27-root-INFO: grad norm: 2.959 2.930 0.416
2024-12-02-11:40:27-root-INFO: grad norm: 2.936 2.913 0.372
2024-12-02-11:40:28-root-INFO: Loss Change: 41.546 -> 38.127
2024-12-02-11:40:28-root-INFO: Regularization Change: 0.000 -> 4.145
2024-12-02-11:40:28-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-11:40:28-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-11:40:28-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-11:40:28-root-INFO: grad norm: 2.786 2.759 0.386
2024-12-02-11:40:28-root-INFO: grad norm: 2.629 2.610 0.319
2024-12-02-11:40:29-root-INFO: grad norm: 2.621 2.597 0.353
2024-12-02-11:40:29-root-INFO: grad norm: 2.647 2.628 0.324
2024-12-02-11:40:30-root-INFO: grad norm: 2.652 2.627 0.365
2024-12-02-11:40:30-root-INFO: Loss Change: 37.803 -> 35.126
2024-12-02-11:40:30-root-INFO: Regularization Change: 0.000 -> 3.294
2024-12-02-11:40:30-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-11:40:30-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-11:40:30-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-11:40:31-root-INFO: grad norm: 3.142 3.105 0.477
2024-12-02-11:40:31-root-INFO: grad norm: 2.872 2.844 0.405
2024-12-02-11:40:31-root-INFO: grad norm: 2.629 2.610 0.320
2024-12-02-11:40:32-root-INFO: grad norm: 2.527 2.504 0.344
2024-12-02-11:40:32-root-INFO: grad norm: 2.467 2.448 0.299
2024-12-02-11:40:33-root-INFO: Loss Change: 35.003 -> 32.665
2024-12-02-11:40:33-root-INFO: Regularization Change: 0.000 -> 2.892
2024-12-02-11:40:33-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-11:40:33-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-11:40:33-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-11:40:33-root-INFO: grad norm: 2.364 2.341 0.330
2024-12-02-11:40:33-root-INFO: grad norm: 2.180 2.165 0.252
2024-12-02-11:40:34-root-INFO: grad norm: 2.156 2.139 0.276
2024-12-02-11:40:34-root-INFO: grad norm: 2.176 2.161 0.258
2024-12-02-11:40:35-root-INFO: grad norm: 2.184 2.165 0.289
2024-12-02-11:40:35-root-INFO: Loss Change: 32.213 -> 30.249
2024-12-02-11:40:35-root-INFO: Regularization Change: 0.000 -> 2.476
2024-12-02-11:40:35-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-11:40:35-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-11:40:36-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-11:40:36-root-INFO: grad norm: 2.838 2.801 0.458
2024-12-02-11:40:36-root-INFO: grad norm: 2.503 2.479 0.346
2024-12-02-11:40:37-root-INFO: grad norm: 2.266 2.249 0.273
2024-12-02-11:40:37-root-INFO: grad norm: 2.158 2.139 0.285
2024-12-02-11:40:38-root-INFO: grad norm: 2.080 2.065 0.248
2024-12-02-11:40:38-root-INFO: Loss Change: 30.210 -> 28.315
2024-12-02-11:40:38-root-INFO: Regularization Change: 0.000 -> 2.330
2024-12-02-11:40:38-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-11:40:38-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-11:40:38-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-11:40:38-root-INFO: grad norm: 2.012 1.993 0.278
2024-12-02-11:40:39-root-INFO: grad norm: 1.825 1.813 0.205
2024-12-02-11:40:39-root-INFO: grad norm: 1.802 1.788 0.223
2024-12-02-11:40:40-root-INFO: grad norm: 1.811 1.799 0.208
2024-12-02-11:40:40-root-INFO: grad norm: 1.822 1.808 0.230
2024-12-02-11:40:40-root-INFO: Loss Change: 28.042 -> 26.462
2024-12-02-11:40:41-root-INFO: Regularization Change: 0.000 -> 2.040
2024-12-02-11:40:41-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-11:40:41-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-11:40:41-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-11:40:41-root-INFO: grad norm: 2.373 2.341 0.386
2024-12-02-11:40:41-root-INFO: grad norm: 2.096 2.078 0.277
2024-12-02-11:40:42-root-INFO: grad norm: 1.939 1.926 0.227
2024-12-02-11:40:42-root-INFO: grad norm: 1.850 1.836 0.235
2024-12-02-11:40:43-root-INFO: grad norm: 1.776 1.764 0.204
2024-12-02-11:40:43-root-INFO: Loss Change: 26.325 -> 24.766
2024-12-02-11:40:43-root-INFO: Regularization Change: 0.000 -> 1.970
2024-12-02-11:40:43-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-11:40:43-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-11:40:43-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-11:40:43-root-INFO: grad norm: 1.800 1.779 0.271
2024-12-02-11:40:44-root-INFO: grad norm: 1.510 1.501 0.165
2024-12-02-11:40:44-root-INFO: grad norm: 1.447 1.437 0.168
2024-12-02-11:40:45-root-INFO: grad norm: 1.422 1.413 0.157
2024-12-02-11:40:45-root-INFO: grad norm: 1.410 1.400 0.167
2024-12-02-11:40:46-root-INFO: Loss Change: 24.552 -> 23.140
2024-12-02-11:40:46-root-INFO: Regularization Change: 0.000 -> 1.793
2024-12-02-11:40:46-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-11:40:46-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-11:40:46-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-11:40:46-root-INFO: grad norm: 2.008 1.979 0.340
2024-12-02-11:40:46-root-INFO: grad norm: 1.706 1.692 0.214
2024-12-02-11:40:47-root-INFO: grad norm: 1.590 1.580 0.177
2024-12-02-11:40:47-root-INFO: grad norm: 1.516 1.505 0.181
2024-12-02-11:40:48-root-INFO: grad norm: 1.455 1.446 0.160
2024-12-02-11:40:48-root-INFO: Loss Change: 22.897 -> 21.518
2024-12-02-11:40:48-root-INFO: Regularization Change: 0.000 -> 1.757
2024-12-02-11:40:48-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-11:40:48-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-11:40:48-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-11:40:49-root-INFO: grad norm: 1.531 1.514 0.232
2024-12-02-11:40:49-root-INFO: grad norm: 1.254 1.247 0.138
2024-12-02-11:40:50-root-INFO: grad norm: 1.184 1.177 0.135
2024-12-02-11:40:50-root-INFO: grad norm: 1.148 1.141 0.126
2024-12-02-11:40:50-root-INFO: grad norm: 1.122 1.115 0.131
2024-12-02-11:40:51-root-INFO: Loss Change: 21.338 -> 20.092
2024-12-02-11:40:51-root-INFO: Regularization Change: 0.000 -> 1.604
2024-12-02-11:40:51-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-11:40:51-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-11:40:51-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-11:40:51-root-INFO: grad norm: 1.661 1.635 0.293
2024-12-02-11:40:52-root-INFO: grad norm: 1.345 1.334 0.168
2024-12-02-11:40:52-root-INFO: grad norm: 1.253 1.245 0.138
2024-12-02-11:40:53-root-INFO: grad norm: 1.198 1.190 0.140
2024-12-02-11:40:53-root-INFO: grad norm: 1.155 1.148 0.125
2024-12-02-11:40:53-root-INFO: Loss Change: 19.963 -> 18.747
2024-12-02-11:40:53-root-INFO: Regularization Change: 0.000 -> 1.578
2024-12-02-11:40:53-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-11:40:53-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-11:40:53-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-11:40:54-root-INFO: grad norm: 1.391 1.371 0.234
2024-12-02-11:40:54-root-INFO: grad norm: 1.063 1.057 0.118
2024-12-02-11:40:55-root-INFO: grad norm: 0.998 0.992 0.110
2024-12-02-11:40:55-root-INFO: grad norm: 0.970 0.964 0.105
2024-12-02-11:40:55-root-INFO: grad norm: 0.952 0.947 0.106
2024-12-02-11:40:56-root-INFO: Loss Change: 18.449 -> 17.304
2024-12-02-11:40:56-root-INFO: Regularization Change: 0.000 -> 1.499
2024-12-02-11:40:56-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-11:40:56-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-11:40:56-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-11:40:56-root-INFO: grad norm: 1.507 1.483 0.265
2024-12-02-11:40:57-root-INFO: grad norm: 1.149 1.141 0.136
2024-12-02-11:40:57-root-INFO: grad norm: 1.064 1.058 0.110
2024-12-02-11:40:58-root-INFO: grad norm: 1.020 1.014 0.112
2024-12-02-11:40:58-root-INFO: grad norm: 0.987 0.982 0.101
2024-12-02-11:40:58-root-INFO: Loss Change: 17.244 -> 16.126
2024-12-02-11:40:58-root-INFO: Regularization Change: 0.000 -> 1.477
2024-12-02-11:40:58-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-11:40:58-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-11:40:58-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-11:40:59-root-INFO: grad norm: 1.321 1.301 0.227
2024-12-02-11:40:59-root-INFO: grad norm: 0.973 0.967 0.111
2024-12-02-11:41:00-root-INFO: grad norm: 0.924 0.919 0.097
2024-12-02-11:41:00-root-INFO: grad norm: 0.912 0.907 0.095
2024-12-02-11:41:01-root-INFO: grad norm: 0.907 0.902 0.093
2024-12-02-11:41:01-root-INFO: Loss Change: 15.865 -> 14.817
2024-12-02-11:41:01-root-INFO: Regularization Change: 0.000 -> 1.413
2024-12-02-11:41:01-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-11:41:01-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-11:41:01-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-11:41:01-root-INFO: grad norm: 1.401 1.381 0.234
2024-12-02-11:41:02-root-INFO: grad norm: 1.072 1.066 0.117
2024-12-02-11:41:02-root-INFO: grad norm: 0.997 0.993 0.095
2024-12-02-11:41:03-root-INFO: grad norm: 0.963 0.958 0.098
2024-12-02-11:41:03-root-INFO: grad norm: 0.934 0.930 0.088
2024-12-02-11:41:03-root-INFO: Loss Change: 14.707 -> 13.700
2024-12-02-11:41:03-root-INFO: Regularization Change: 0.000 -> 1.382
2024-12-02-11:41:03-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-11:41:03-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-11:41:04-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-11:41:04-root-INFO: grad norm: 1.287 1.270 0.208
2024-12-02-11:41:04-root-INFO: grad norm: 0.959 0.954 0.102
2024-12-02-11:41:05-root-INFO: grad norm: 0.935 0.930 0.087
2024-12-02-11:41:05-root-INFO: grad norm: 0.947 0.943 0.088
2024-12-02-11:41:06-root-INFO: grad norm: 0.971 0.967 0.084
2024-12-02-11:41:06-root-INFO: Loss Change: 13.527 -> 12.579
2024-12-02-11:41:06-root-INFO: Regularization Change: 0.000 -> 1.346
2024-12-02-11:41:06-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-11:41:06-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-11:41:06-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-11:41:06-root-INFO: grad norm: 1.452 1.433 0.234
2024-12-02-11:41:07-root-INFO: grad norm: 1.109 1.104 0.105
2024-12-02-11:41:07-root-INFO: grad norm: 1.047 1.043 0.084
2024-12-02-11:41:08-root-INFO: grad norm: 1.042 1.039 0.085
2024-12-02-11:41:08-root-INFO: grad norm: 1.034 1.031 0.080
2024-12-02-11:41:09-root-INFO: Loss Change: 12.464 -> 11.528
2024-12-02-11:41:09-root-INFO: Regularization Change: 0.000 -> 1.341
2024-12-02-11:41:09-root-INFO: Undo step: 20
2024-12-02-11:41:09-root-INFO: Undo step: 21
2024-12-02-11:41:09-root-INFO: Undo step: 22
2024-12-02-11:41:09-root-INFO: Undo step: 23
2024-12-02-11:41:09-root-INFO: Undo step: 24
2024-12-02-11:41:09-root-INFO: Undo step: 25
2024-12-02-11:41:09-root-INFO: Undo step: 26
2024-12-02-11:41:09-root-INFO: Undo step: 27
2024-12-02-11:41:09-root-INFO: Undo step: 28
2024-12-02-11:41:09-root-INFO: Undo step: 29
2024-12-02-11:41:09-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-11:41:09-root-INFO: grad norm: 19.115 19.031 1.790
2024-12-02-11:41:10-root-INFO: grad norm: 10.224 10.149 1.241
2024-12-02-11:41:10-root-INFO: grad norm: 7.215 7.156 0.927
2024-12-02-11:41:11-root-INFO: grad norm: 5.666 5.614 0.759
2024-12-02-11:41:11-root-INFO: grad norm: 4.729 4.686 0.637
2024-12-02-11:41:11-root-INFO: Loss Change: 159.668 -> 50.031
2024-12-02-11:41:11-root-INFO: Regularization Change: 0.000 -> 113.961
2024-12-02-11:41:11-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-11:41:11-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-11:41:12-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-11:41:12-root-INFO: grad norm: 4.286 4.250 0.554
2024-12-02-11:41:12-root-INFO: grad norm: 3.723 3.690 0.493
2024-12-02-11:41:13-root-INFO: grad norm: 3.326 3.298 0.429
2024-12-02-11:41:13-root-INFO: grad norm: 3.014 2.987 0.397
2024-12-02-11:41:14-root-INFO: grad norm: 2.760 2.736 0.360
2024-12-02-11:41:14-root-INFO: Loss Change: 49.571 -> 35.443
2024-12-02-11:41:14-root-INFO: Regularization Change: 0.000 -> 18.266
2024-12-02-11:41:14-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-11:41:14-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-11:41:14-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-11:41:14-root-INFO: grad norm: 2.738 2.712 0.376
2024-12-02-11:41:15-root-INFO: grad norm: 2.430 2.410 0.309
2024-12-02-11:41:15-root-INFO: grad norm: 2.268 2.250 0.284
2024-12-02-11:41:16-root-INFO: grad norm: 2.167 2.150 0.272
2024-12-02-11:41:16-root-INFO: grad norm: 2.198 2.182 0.261
2024-12-02-11:41:17-root-INFO: Loss Change: 34.835 -> 28.457
2024-12-02-11:41:17-root-INFO: Regularization Change: 0.000 -> 8.560
2024-12-02-11:41:17-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-11:41:17-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-11:41:17-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-11:41:17-root-INFO: grad norm: 2.300 2.280 0.298
2024-12-02-11:41:17-root-INFO: grad norm: 2.143 2.130 0.234
2024-12-02-11:41:18-root-INFO: grad norm: 2.048 2.036 0.224
2024-12-02-11:41:18-root-INFO: grad norm: 1.903 1.891 0.214
2024-12-02-11:41:19-root-INFO: grad norm: 1.837 1.825 0.207
2024-12-02-11:41:19-root-INFO: Loss Change: 28.138 -> 24.169
2024-12-02-11:41:19-root-INFO: Regularization Change: 0.000 -> 5.315
2024-12-02-11:41:19-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-11:41:19-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-11:41:19-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-11:41:20-root-INFO: grad norm: 2.231 2.206 0.336
2024-12-02-11:41:20-root-INFO: grad norm: 1.919 1.906 0.224
2024-12-02-11:41:21-root-INFO: grad norm: 1.689 1.678 0.194
2024-12-02-11:41:21-root-INFO: grad norm: 1.606 1.595 0.186
2024-12-02-11:41:21-root-INFO: grad norm: 1.585 1.575 0.177
2024-12-02-11:41:22-root-INFO: Loss Change: 23.901 -> 21.067
2024-12-02-11:41:22-root-INFO: Regularization Change: 0.000 -> 3.821
2024-12-02-11:41:22-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-11:41:22-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-11:41:22-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-11:41:22-root-INFO: grad norm: 1.720 1.702 0.246
2024-12-02-11:41:23-root-INFO: grad norm: 1.483 1.475 0.159
2024-12-02-11:41:23-root-INFO: grad norm: 1.439 1.431 0.151
2024-12-02-11:41:24-root-INFO: grad norm: 1.434 1.426 0.150
2024-12-02-11:41:24-root-INFO: grad norm: 1.412 1.405 0.147
2024-12-02-11:41:25-root-INFO: Loss Change: 20.620 -> 18.479
2024-12-02-11:41:25-root-INFO: Regularization Change: 0.000 -> 2.928
2024-12-02-11:41:25-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-11:41:25-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-11:41:25-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-11:41:25-root-INFO: grad norm: 1.900 1.877 0.293
2024-12-02-11:41:25-root-INFO: grad norm: 1.567 1.557 0.177
2024-12-02-11:41:26-root-INFO: grad norm: 1.372 1.365 0.147
2024-12-02-11:41:26-root-INFO: grad norm: 1.319 1.311 0.147
2024-12-02-11:41:27-root-INFO: grad norm: 1.307 1.300 0.135
2024-12-02-11:41:27-root-INFO: Loss Change: 18.350 -> 16.566
2024-12-02-11:41:27-root-INFO: Regularization Change: 0.000 -> 2.449
2024-12-02-11:41:27-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-11:41:27-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-11:41:27-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-11:41:27-root-INFO: grad norm: 1.514 1.496 0.231
2024-12-02-11:41:28-root-INFO: grad norm: 1.233 1.226 0.128
2024-12-02-11:41:28-root-INFO: grad norm: 1.202 1.196 0.122
2024-12-02-11:41:29-root-INFO: grad norm: 1.214 1.208 0.118
2024-12-02-11:41:29-root-INFO: grad norm: 1.206 1.200 0.120
2024-12-02-11:41:30-root-INFO: Loss Change: 16.185 -> 14.725
2024-12-02-11:41:30-root-INFO: Regularization Change: 0.000 -> 2.035
2024-12-02-11:41:30-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-11:41:30-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-11:41:30-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-11:41:30-root-INFO: grad norm: 1.729 1.710 0.260
2024-12-02-11:41:30-root-INFO: grad norm: 1.418 1.409 0.155
2024-12-02-11:41:31-root-INFO: grad norm: 1.248 1.242 0.125
2024-12-02-11:41:31-root-INFO: grad norm: 1.213 1.206 0.131
2024-12-02-11:41:32-root-INFO: grad norm: 1.207 1.201 0.116
2024-12-02-11:41:32-root-INFO: Loss Change: 14.575 -> 13.298
2024-12-02-11:41:32-root-INFO: Regularization Change: 0.000 -> 1.813
2024-12-02-11:41:32-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-11:41:32-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-11:41:32-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-11:41:32-root-INFO: grad norm: 1.405 1.390 0.205
2024-12-02-11:41:33-root-INFO: grad norm: 1.126 1.121 0.109
2024-12-02-11:41:33-root-INFO: grad norm: 1.104 1.099 0.106
2024-12-02-11:41:34-root-INFO: grad norm: 1.123 1.119 0.099
2024-12-02-11:41:34-root-INFO: grad norm: 1.118 1.113 0.108
2024-12-02-11:41:35-root-INFO: Loss Change: 13.041 -> 11.929
2024-12-02-11:41:35-root-INFO: Regularization Change: 0.000 -> 1.588
2024-12-02-11:41:35-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-11:41:35-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-11:41:35-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-11:41:35-root-INFO: grad norm: 1.700 1.679 0.265
2024-12-02-11:41:35-root-INFO: grad norm: 1.349 1.341 0.147
2024-12-02-11:41:36-root-INFO: grad norm: 1.185 1.180 0.115
2024-12-02-11:41:36-root-INFO: grad norm: 1.150 1.144 0.123
2024-12-02-11:41:37-root-INFO: grad norm: 1.138 1.133 0.105
2024-12-02-11:41:37-root-INFO: Loss Change: 11.805 -> 10.767
2024-12-02-11:41:37-root-INFO: Regularization Change: 0.000 -> 1.495
2024-12-02-11:41:37-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-11:41:37-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-11:41:37-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-11:41:37-root-INFO: grad norm: 1.308 1.295 0.187
2024-12-02-11:41:38-root-INFO: grad norm: 1.012 1.008 0.090
2024-12-02-11:41:38-root-INFO: grad norm: 0.989 0.985 0.087
2024-12-02-11:41:39-root-INFO: grad norm: 1.001 0.998 0.082
2024-12-02-11:41:39-root-INFO: grad norm: 0.993 0.989 0.088
2024-12-02-11:41:40-root-INFO: Loss Change: 10.549 -> 9.633
2024-12-02-11:41:40-root-INFO: Regularization Change: 0.000 -> 1.326
2024-12-02-11:41:40-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-11:41:40-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-11:41:40-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-11:41:40-root-INFO: grad norm: 1.598 1.578 0.252
2024-12-02-11:41:40-root-INFO: grad norm: 1.227 1.221 0.129
2024-12-02-11:41:41-root-INFO: grad norm: 1.088 1.083 0.099
2024-12-02-11:41:41-root-INFO: grad norm: 1.052 1.046 0.106
2024-12-02-11:41:42-root-INFO: grad norm: 1.031 1.027 0.089
2024-12-02-11:41:42-root-INFO: Loss Change: 9.549 -> 8.666
2024-12-02-11:41:42-root-INFO: Regularization Change: 0.000 -> 1.286
2024-12-02-11:41:42-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-11:41:42-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-11:41:42-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-11:41:42-root-INFO: grad norm: 1.234 1.221 0.181
2024-12-02-11:41:43-root-INFO: grad norm: 0.909 0.905 0.077
2024-12-02-11:41:43-root-INFO: grad norm: 0.890 0.887 0.069
2024-12-02-11:41:44-root-INFO: grad norm: 0.908 0.905 0.067
2024-12-02-11:41:44-root-INFO: grad norm: 0.957 0.954 0.068
2024-12-02-11:41:45-root-INFO: Loss Change: 8.525 -> 7.769
2024-12-02-11:41:45-root-INFO: Regularization Change: 0.000 -> 1.148
2024-12-02-11:41:45-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-11:41:45-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-11:41:45-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-11:41:45-root-INFO: grad norm: 1.455 1.439 0.213
2024-12-02-11:41:45-root-INFO: grad norm: 1.081 1.077 0.094
2024-12-02-11:41:46-root-INFO: grad norm: 0.971 0.969 0.069
2024-12-02-11:41:46-root-INFO: grad norm: 1.048 1.046 0.069
2024-12-02-11:41:47-root-INFO: grad norm: 0.916 0.914 0.063
2024-12-02-11:41:47-root-INFO: Loss Change: 7.717 -> 6.961
2024-12-02-11:41:47-root-INFO: Regularization Change: 0.000 -> 1.089
2024-12-02-11:41:47-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-11:41:47-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-11:41:47-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-11:41:47-root-INFO: grad norm: 1.271 1.254 0.208
2024-12-02-11:41:48-root-INFO: grad norm: 0.819 0.816 0.071
2024-12-02-11:41:48-root-INFO: grad norm: 0.768 0.766 0.057
2024-12-02-11:41:49-root-INFO: grad norm: 0.777 0.776 0.054
2024-12-02-11:41:50-root-INFO: grad norm: 0.826 0.824 0.060
2024-12-02-11:41:50-root-INFO: Loss Change: 6.898 -> 6.222
2024-12-02-11:41:50-root-INFO: Regularization Change: 0.000 -> 1.033
2024-12-02-11:41:50-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-11:41:50-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-11:41:50-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-11:41:50-root-INFO: grad norm: 1.990 1.974 0.254
2024-12-02-11:41:51-root-INFO: grad norm: 1.035 1.031 0.090
2024-12-02-11:41:51-root-INFO: grad norm: 0.844 0.842 0.061
2024-12-02-11:41:52-root-INFO: grad norm: 0.784 0.781 0.061
2024-12-02-11:41:52-root-INFO: grad norm: 0.739 0.737 0.051
2024-12-02-11:41:53-root-INFO: Loss Change: 6.246 -> 5.544
2024-12-02-11:41:53-root-INFO: Regularization Change: 0.000 -> 1.045
2024-12-02-11:41:53-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-11:41:53-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-11:41:53-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-11:41:53-root-INFO: grad norm: 1.179 1.162 0.199
2024-12-02-11:41:53-root-INFO: grad norm: 0.736 0.733 0.066
2024-12-02-11:41:54-root-INFO: grad norm: 0.654 0.652 0.050
2024-12-02-11:41:54-root-INFO: grad norm: 0.610 0.609 0.047
2024-12-02-11:41:55-root-INFO: grad norm: 0.607 0.606 0.043
2024-12-02-11:41:55-root-INFO: Loss Change: 5.544 -> 4.897
2024-12-02-11:41:55-root-INFO: Regularization Change: 0.000 -> 0.931
2024-12-02-11:41:55-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-11:41:55-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-11:41:55-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-11:41:55-root-INFO: grad norm: 1.157 1.142 0.188
2024-12-02-11:41:56-root-INFO: grad norm: 0.867 0.865 0.058
2024-12-02-11:41:56-root-INFO: grad norm: 0.735 0.733 0.049
2024-12-02-11:41:57-root-INFO: grad norm: 0.586 0.585 0.042
2024-12-02-11:41:57-root-INFO: grad norm: 0.546 0.544 0.039
2024-12-02-11:41:58-root-INFO: Loss Change: 4.918 -> 4.334
2024-12-02-11:41:58-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-11:41:58-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-11:41:58-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-11:41:58-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-11:41:58-root-INFO: grad norm: 1.249 1.230 0.217
2024-12-02-11:41:58-root-INFO: grad norm: 0.753 0.749 0.070
2024-12-02-11:41:59-root-INFO: grad norm: 0.559 0.557 0.043
2024-12-02-11:41:59-root-INFO: grad norm: 0.580 0.579 0.037
2024-12-02-11:42:00-root-INFO: grad norm: 0.672 0.671 0.048
2024-12-02-11:42:00-root-INFO: Loss Change: 4.400 -> 3.877
2024-12-02-11:42:00-root-INFO: Regularization Change: 0.000 -> 0.798
2024-12-02-11:42:00-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-11:42:00-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-11:42:00-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-11:42:01-root-INFO: grad norm: 1.605 1.587 0.240
2024-12-02-11:42:01-root-INFO: grad norm: 0.718 0.715 0.065
2024-12-02-11:42:02-root-INFO: grad norm: 0.590 0.588 0.045
2024-12-02-11:42:02-root-INFO: grad norm: 0.558 0.557 0.042
2024-12-02-11:42:02-root-INFO: grad norm: 0.544 0.542 0.037
2024-12-02-11:42:03-root-INFO: Loss Change: 4.006 -> 3.414
2024-12-02-11:42:03-root-INFO: Regularization Change: 0.000 -> 0.806
2024-12-02-11:42:03-root-INFO: Undo step: 10
2024-12-02-11:42:03-root-INFO: Undo step: 11
2024-12-02-11:42:03-root-INFO: Undo step: 12
2024-12-02-11:42:03-root-INFO: Undo step: 13
2024-12-02-11:42:03-root-INFO: Undo step: 14
2024-12-02-11:42:03-root-INFO: Undo step: 15
2024-12-02-11:42:03-root-INFO: Undo step: 16
2024-12-02-11:42:03-root-INFO: Undo step: 17
2024-12-02-11:42:03-root-INFO: Undo step: 18
2024-12-02-11:42:03-root-INFO: Undo step: 19
2024-12-02-11:42:03-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-11:42:03-root-INFO: grad norm: 16.753 16.710 1.204
2024-12-02-11:42:04-root-INFO: grad norm: 9.157 9.115 0.883
2024-12-02-11:42:04-root-INFO: grad norm: 6.451 6.417 0.663
2024-12-02-11:42:05-root-INFO: grad norm: 5.112 5.083 0.550
2024-12-02-11:42:05-root-INFO: grad norm: 4.174 4.148 0.457
2024-12-02-11:42:05-root-INFO: Loss Change: 133.222 -> 32.919
2024-12-02-11:42:05-root-INFO: Regularization Change: 0.000 -> 121.187
2024-12-02-11:42:05-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-11:42:05-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-11:42:06-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-11:42:06-root-INFO: grad norm: 3.711 3.689 0.397
2024-12-02-11:42:06-root-INFO: grad norm: 3.166 3.147 0.345
2024-12-02-11:42:07-root-INFO: grad norm: 2.793 2.776 0.306
2024-12-02-11:42:07-root-INFO: grad norm: 2.516 2.501 0.278
2024-12-02-11:42:08-root-INFO: grad norm: 2.295 2.281 0.252
2024-12-02-11:42:08-root-INFO: Loss Change: 32.120 -> 20.393
2024-12-02-11:42:08-root-INFO: Regularization Change: 0.000 -> 17.712
2024-12-02-11:42:08-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-11:42:08-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-11:42:08-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-11:42:08-root-INFO: grad norm: 2.377 2.361 0.270
2024-12-02-11:42:09-root-INFO: grad norm: 2.062 2.050 0.219
2024-12-02-11:42:09-root-INFO: grad norm: 1.902 1.893 0.190
2024-12-02-11:42:10-root-INFO: grad norm: 1.776 1.766 0.189
2024-12-02-11:42:10-root-INFO: grad norm: 1.670 1.661 0.169
2024-12-02-11:42:11-root-INFO: Loss Change: 19.832 -> 14.750
2024-12-02-11:42:11-root-INFO: Regularization Change: 0.000 -> 7.837
2024-12-02-11:42:11-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-11:42:11-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-11:42:11-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-11:42:11-root-INFO: grad norm: 1.792 1.779 0.214
2024-12-02-11:42:12-root-INFO: grad norm: 1.527 1.519 0.160
2024-12-02-11:42:12-root-INFO: grad norm: 1.432 1.425 0.143
2024-12-02-11:42:13-root-INFO: grad norm: 1.364 1.356 0.141
2024-12-02-11:42:13-root-INFO: grad norm: 1.307 1.300 0.132
2024-12-02-11:42:13-root-INFO: Loss Change: 14.321 -> 11.394
2024-12-02-11:42:13-root-INFO: Regularization Change: 0.000 -> 4.547
2024-12-02-11:42:13-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-11:42:13-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-11:42:14-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-11:42:14-root-INFO: grad norm: 1.594 1.580 0.211
2024-12-02-11:42:14-root-INFO: grad norm: 1.304 1.297 0.137
2024-12-02-11:42:15-root-INFO: grad norm: 1.224 1.219 0.112
2024-12-02-11:42:15-root-INFO: grad norm: 1.181 1.175 0.121
2024-12-02-11:42:16-root-INFO: grad norm: 1.235 1.230 0.107
2024-12-02-11:42:16-root-INFO: Loss Change: 11.066 -> 9.154
2024-12-02-11:42:16-root-INFO: Regularization Change: 0.000 -> 3.047
2024-12-02-11:42:16-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-11:42:16-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-11:42:16-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-11:42:16-root-INFO: grad norm: 1.404 1.390 0.202
2024-12-02-11:42:17-root-INFO: grad norm: 1.022 1.017 0.101
2024-12-02-11:42:17-root-INFO: grad norm: 0.970 0.965 0.089
2024-12-02-11:42:18-root-INFO: grad norm: 0.981 0.976 0.093
2024-12-02-11:42:18-root-INFO: grad norm: 1.332 1.329 0.093
2024-12-02-11:42:18-root-INFO: Loss Change: 8.853 -> 7.508
2024-12-02-11:42:18-root-INFO: Regularization Change: 0.000 -> 2.172
2024-12-02-11:42:18-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-11:42:18-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-11:42:18-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-11:42:19-root-INFO: grad norm: 1.314 1.299 0.197
2024-12-02-11:42:19-root-INFO: grad norm: 0.982 0.978 0.095
2024-12-02-11:42:20-root-INFO: grad norm: 0.926 0.922 0.076
2024-12-02-11:42:20-root-INFO: grad norm: 0.901 0.898 0.074
2024-12-02-11:42:20-root-INFO: grad norm: 0.902 0.899 0.073
2024-12-02-11:42:21-root-INFO: Loss Change: 7.300 -> 6.210
2024-12-02-11:42:21-root-INFO: Regularization Change: 0.000 -> 1.702
2024-12-02-11:42:21-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-11:42:21-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-11:42:21-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-11:42:21-root-INFO: grad norm: 1.642 1.624 0.241
2024-12-02-11:42:22-root-INFO: grad norm: 0.929 0.924 0.098
2024-12-02-11:42:22-root-INFO: grad norm: 0.805 0.803 0.067
2024-12-02-11:42:23-root-INFO: grad norm: 0.770 0.767 0.071
2024-12-02-11:42:23-root-INFO: grad norm: 0.706 0.703 0.060
2024-12-02-11:42:23-root-INFO: Loss Change: 6.114 -> 5.223
2024-12-02-11:42:23-root-INFO: Regularization Change: 0.000 -> 1.291
2024-12-02-11:42:23-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-11:42:23-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-11:42:23-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-11:42:24-root-INFO: grad norm: 1.111 1.095 0.189
2024-12-02-11:42:24-root-INFO: grad norm: 0.730 0.727 0.074
2024-12-02-11:42:25-root-INFO: grad norm: 0.819 0.817 0.060
2024-12-02-11:42:25-root-INFO: grad norm: 0.743 0.740 0.066
2024-12-02-11:42:26-root-INFO: grad norm: 0.598 0.596 0.048
2024-12-02-11:42:26-root-INFO: Loss Change: 5.137 -> 4.456
2024-12-02-11:42:26-root-INFO: Regularization Change: 0.000 -> 1.038
2024-12-02-11:42:26-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-11:42:26-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-11:42:26-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-11:42:26-root-INFO: grad norm: 1.092 1.073 0.203
2024-12-02-11:42:27-root-INFO: grad norm: 0.638 0.634 0.064
2024-12-02-11:42:27-root-INFO: grad norm: 0.554 0.552 0.046
2024-12-02-11:42:28-root-INFO: grad norm: 0.574 0.572 0.048
2024-12-02-11:42:28-root-INFO: grad norm: 0.998 0.996 0.056
2024-12-02-11:42:28-root-INFO: Loss Change: 4.432 -> 3.934
2024-12-02-11:42:28-root-INFO: Regularization Change: 0.000 -> 0.927
2024-12-02-11:42:28-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-11:42:28-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-11:42:29-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-11:42:29-root-INFO: grad norm: 1.131 1.113 0.201
2024-12-02-11:42:29-root-INFO: grad norm: 0.749 0.746 0.069
2024-12-02-11:42:30-root-INFO: grad norm: 0.728 0.726 0.055
2024-12-02-11:42:30-root-INFO: grad norm: 0.707 0.704 0.061
2024-12-02-11:42:31-root-INFO: grad norm: 0.776 0.774 0.049
2024-12-02-11:42:31-root-INFO: Loss Change: 3.957 -> 3.413
2024-12-02-11:42:31-root-INFO: Regularization Change: 0.000 -> 0.778
2024-12-02-11:42:31-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-11:42:31-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-11:42:31-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-11:42:31-root-INFO: grad norm: 1.083 1.069 0.171
2024-12-02-11:42:32-root-INFO: grad norm: 0.568 0.566 0.043
2024-12-02-11:42:32-root-INFO: grad norm: 0.555 0.554 0.039
2024-12-02-11:42:33-root-INFO: grad norm: 0.853 0.852 0.043
2024-12-02-11:42:33-root-INFO: grad norm: 0.605 0.603 0.053
2024-12-02-11:42:34-root-INFO: Loss Change: 3.455 -> 2.960
2024-12-02-11:42:34-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-02-11:42:34-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-11:42:34-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-11:42:34-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-11:42:34-root-INFO: grad norm: 1.077 1.059 0.196
2024-12-02-11:42:34-root-INFO: grad norm: 0.518 0.517 0.044
2024-12-02-11:42:35-root-INFO: grad norm: 0.449 0.448 0.034
2024-12-02-11:42:35-root-INFO: grad norm: 0.574 0.572 0.047
2024-12-02-11:42:36-root-INFO: grad norm: 0.831 0.830 0.042
2024-12-02-11:42:36-root-INFO: Loss Change: 3.068 -> 2.667
2024-12-02-11:42:36-root-INFO: Regularization Change: 0.000 -> 0.666
2024-12-02-11:42:36-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-11:42:36-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-11:42:36-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-11:42:36-root-INFO: grad norm: 1.150 1.139 0.157
2024-12-02-11:42:37-root-INFO: grad norm: 0.664 0.663 0.043
2024-12-02-11:42:37-root-INFO: grad norm: 0.701 0.698 0.059
2024-12-02-11:42:38-root-INFO: grad norm: 0.752 0.751 0.042
2024-12-02-11:42:38-root-INFO: grad norm: 0.719 0.716 0.066
2024-12-02-11:42:39-root-INFO: Loss Change: 2.764 -> 2.339
2024-12-02-11:42:39-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-02-11:42:39-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-11:42:39-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-11:42:39-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-11:42:39-root-INFO: grad norm: 1.141 1.127 0.182
2024-12-02-11:42:39-root-INFO: grad norm: 0.619 0.617 0.053
2024-12-02-11:42:40-root-INFO: grad norm: 0.535 0.534 0.040
2024-12-02-11:42:40-root-INFO: grad norm: 0.463 0.461 0.041
2024-12-02-11:42:41-root-INFO: grad norm: 0.433 0.432 0.034
2024-12-02-11:42:41-root-INFO: Loss Change: 2.492 -> 2.072
2024-12-02-11:42:41-root-INFO: Regularization Change: 0.000 -> 0.586
2024-12-02-11:42:41-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-11:42:41-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-11:42:41-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-11:42:41-root-INFO: grad norm: 0.882 0.870 0.143
2024-12-02-11:42:42-root-INFO: grad norm: 0.410 0.408 0.039
2024-12-02-11:42:42-root-INFO: grad norm: 0.345 0.343 0.036
2024-12-02-11:42:43-root-INFO: grad norm: 0.316 0.314 0.035
2024-12-02-11:42:43-root-INFO: grad norm: 0.296 0.294 0.034
2024-12-02-11:42:44-root-INFO: Loss Change: 2.211 -> 1.871
2024-12-02-11:42:44-root-INFO: Regularization Change: 0.000 -> 0.509
2024-12-02-11:42:44-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-11:42:44-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-11:42:44-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-11:42:44-root-INFO: grad norm: 0.823 0.813 0.128
2024-12-02-11:42:44-root-INFO: grad norm: 0.399 0.397 0.041
2024-12-02-11:42:45-root-INFO: grad norm: 0.331 0.329 0.039
2024-12-02-11:42:45-root-INFO: grad norm: 0.299 0.297 0.038
2024-12-02-11:42:46-root-INFO: grad norm: 0.278 0.276 0.037
2024-12-02-11:42:46-root-INFO: Loss Change: 2.014 -> 1.703
2024-12-02-11:42:46-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-02-11:42:46-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-11:42:46-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-11:42:46-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-11:42:46-root-INFO: grad norm: 0.799 0.790 0.115
2024-12-02-11:42:47-root-INFO: grad norm: 0.390 0.388 0.045
2024-12-02-11:42:47-root-INFO: grad norm: 0.318 0.315 0.043
2024-12-02-11:42:48-root-INFO: grad norm: 0.287 0.284 0.042
2024-12-02-11:42:48-root-INFO: grad norm: 0.266 0.263 0.041
2024-12-02-11:42:48-root-INFO: Loss Change: 1.854 -> 1.556
2024-12-02-11:42:48-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-02-11:42:48-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-11:42:48-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-11:42:49-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-11:42:49-root-INFO: grad norm: 0.763 0.757 0.102
2024-12-02-11:42:49-root-INFO: grad norm: 0.385 0.382 0.048
2024-12-02-11:42:50-root-INFO: grad norm: 0.321 0.317 0.046
2024-12-02-11:42:50-root-INFO: grad norm: 0.271 0.267 0.045
2024-12-02-11:42:51-root-INFO: grad norm: 0.249 0.245 0.043
2024-12-02-11:42:51-root-INFO: Loss Change: 1.699 -> 1.425
2024-12-02-11:42:51-root-INFO: Regularization Change: 0.000 -> 0.433
2024-12-02-11:42:51-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-11:42:51-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-11:42:51-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-11:42:51-root-INFO: grad norm: 0.739 0.734 0.086
2024-12-02-11:42:52-root-INFO: grad norm: 0.370 0.368 0.043
2024-12-02-11:42:52-root-INFO: grad norm: 0.293 0.290 0.041
2024-12-02-11:42:53-root-INFO: grad norm: 0.257 0.254 0.040
2024-12-02-11:42:53-root-INFO: grad norm: 0.234 0.231 0.038
2024-12-02-11:42:53-root-INFO: Loss Change: 1.547 -> 1.287
2024-12-02-11:42:53-root-INFO: Regularization Change: 0.000 -> 0.418
2024-12-02-11:42:53-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-11:42:53-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-11:42:53-root-INFO: step: 0 lr_xt 0.38992188
2024-12-02-11:42:54-root-INFO: grad norm: 0.849 0.845 0.080
2024-12-02-11:42:54-root-INFO: grad norm: 0.522 0.521 0.030
2024-12-02-11:42:54-root-INFO: grad norm: 0.413 0.412 0.036
2024-12-02-11:42:55-root-INFO: grad norm: 0.361 0.358 0.047
2024-12-02-11:42:55-root-INFO: grad norm: 0.331 0.327 0.053
2024-12-02-11:42:56-root-INFO: Loss Change: 1.434 -> 0.970
2024-12-02-11:42:56-root-INFO: Regularization Change: 0.000 -> 0.821
2024-12-02-11:42:56-root-INFO: Undo step: 0
2024-12-02-11:42:56-root-INFO: Undo step: 1
2024-12-02-11:42:56-root-INFO: Undo step: 2
2024-12-02-11:42:56-root-INFO: Undo step: 3
2024-12-02-11:42:56-root-INFO: Undo step: 4
2024-12-02-11:42:56-root-INFO: Undo step: 5
2024-12-02-11:42:56-root-INFO: Undo step: 6
2024-12-02-11:42:56-root-INFO: Undo step: 7
2024-12-02-11:42:56-root-INFO: Undo step: 8
2024-12-02-11:42:56-root-INFO: Undo step: 9
2024-12-02-11:42:56-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-11:42:56-root-INFO: grad norm: 15.874 15.849 0.878
2024-12-02-11:42:57-root-INFO: grad norm: 7.540 7.521 0.546
2024-12-02-11:42:57-root-INFO: grad norm: 4.882 4.869 0.355
2024-12-02-11:42:57-root-INFO: grad norm: 3.712 3.700 0.299
2024-12-02-11:42:58-root-INFO: grad norm: 2.895 2.886 0.230
2024-12-02-11:42:58-root-INFO: Loss Change: 92.893 -> 15.188
2024-12-02-11:42:58-root-INFO: Regularization Change: 0.000 -> 103.872
2024-12-02-11:42:58-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-11:42:58-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-11:42:58-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-11:42:59-root-INFO: grad norm: 3.012 3.006 0.190
2024-12-02-11:42:59-root-INFO: grad norm: 2.203 2.196 0.168
2024-12-02-11:42:59-root-INFO: grad norm: 1.883 1.878 0.142
2024-12-02-11:43:00-root-INFO: grad norm: 1.710 1.705 0.135
2024-12-02-11:43:00-root-INFO: grad norm: 1.475 1.470 0.115
2024-12-02-11:43:01-root-INFO: Loss Change: 14.575 -> 7.993
2024-12-02-11:43:01-root-INFO: Regularization Change: 0.000 -> 11.414
2024-12-02-11:43:01-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-11:43:01-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-11:43:01-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-11:43:01-root-INFO: grad norm: 1.594 1.586 0.156
2024-12-02-11:43:01-root-INFO: grad norm: 1.293 1.289 0.091
2024-12-02-11:43:02-root-INFO: grad norm: 1.329 1.327 0.081
2024-12-02-11:43:02-root-INFO: grad norm: 1.064 1.061 0.076
2024-12-02-11:43:03-root-INFO: grad norm: 0.972 0.969 0.071
2024-12-02-11:43:03-root-INFO: Loss Change: 7.646 -> 5.103
2024-12-02-11:43:03-root-INFO: Regularization Change: 0.000 -> 4.362
2024-12-02-11:43:03-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-11:43:03-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-11:43:03-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-11:43:03-root-INFO: grad norm: 1.240 1.230 0.155
2024-12-02-11:43:04-root-INFO: grad norm: 0.917 0.914 0.061
2024-12-02-11:43:04-root-INFO: grad norm: 0.829 0.827 0.055
2024-12-02-11:43:05-root-INFO: grad norm: 0.761 0.760 0.054
2024-12-02-11:43:05-root-INFO: grad norm: 0.710 0.708 0.052
2024-12-02-11:43:06-root-INFO: Loss Change: 4.920 -> 3.603
2024-12-02-11:43:06-root-INFO: Regularization Change: 0.000 -> 2.276
2024-12-02-11:43:06-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-11:43:06-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-11:43:06-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-11:43:06-root-INFO: grad norm: 1.048 1.037 0.146
2024-12-02-11:43:06-root-INFO: grad norm: 0.712 0.710 0.049
2024-12-02-11:43:07-root-INFO: grad norm: 0.674 0.673 0.044
2024-12-02-11:43:07-root-INFO: grad norm: 0.725 0.723 0.058
2024-12-02-11:43:08-root-INFO: grad norm: 0.731 0.730 0.044
2024-12-02-11:43:08-root-INFO: Loss Change: 3.543 -> 2.755
2024-12-02-11:43:08-root-INFO: Regularization Change: 0.000 -> 1.375
2024-12-02-11:43:08-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-11:43:08-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-11:43:08-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-11:43:08-root-INFO: grad norm: 0.986 0.978 0.125
2024-12-02-11:43:09-root-INFO: grad norm: 0.636 0.635 0.041
2024-12-02-11:43:09-root-INFO: grad norm: 0.562 0.560 0.042
2024-12-02-11:43:10-root-INFO: grad norm: 0.546 0.544 0.038
2024-12-02-11:43:10-root-INFO: grad norm: 0.593 0.591 0.051
2024-12-02-11:43:11-root-INFO: Loss Change: 2.768 -> 2.210
2024-12-02-11:43:11-root-INFO: Regularization Change: 0.000 -> 0.952
2024-12-02-11:43:11-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-11:43:11-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-11:43:11-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-11:43:11-root-INFO: grad norm: 1.196 1.188 0.140
2024-12-02-11:43:11-root-INFO: grad norm: 0.591 0.590 0.044
2024-12-02-11:43:12-root-INFO: grad norm: 0.439 0.437 0.036
2024-12-02-11:43:12-root-INFO: grad norm: 0.377 0.375 0.035
2024-12-02-11:43:13-root-INFO: grad norm: 0.344 0.343 0.034
2024-12-02-11:43:13-root-INFO: Loss Change: 2.285 -> 1.808
2024-12-02-11:43:13-root-INFO: Regularization Change: 0.000 -> 0.744
2024-12-02-11:43:13-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-11:43:13-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-11:43:13-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-11:43:13-root-INFO: grad norm: 0.806 0.796 0.127
2024-12-02-11:43:14-root-INFO: grad norm: 0.434 0.432 0.044
2024-12-02-11:43:14-root-INFO: grad norm: 0.352 0.350 0.041
2024-12-02-11:43:15-root-INFO: grad norm: 0.318 0.316 0.038
2024-12-02-11:43:15-root-INFO: grad norm: 0.293 0.291 0.037
2024-12-02-11:43:16-root-INFO: Loss Change: 1.915 -> 1.583
2024-12-02-11:43:16-root-INFO: Regularization Change: 0.000 -> 0.535
2024-12-02-11:43:16-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-11:43:16-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-11:43:16-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-11:43:16-root-INFO: grad norm: 0.761 0.752 0.116
2024-12-02-11:43:16-root-INFO: grad norm: 0.405 0.403 0.045
2024-12-02-11:43:17-root-INFO: grad norm: 0.358 0.356 0.040
2024-12-02-11:43:17-root-INFO: grad norm: 0.280 0.277 0.038
2024-12-02-11:43:18-root-INFO: grad norm: 0.257 0.254 0.036
2024-12-02-11:43:18-root-INFO: Loss Change: 1.696 -> 1.418
2024-12-02-11:43:18-root-INFO: Regularization Change: 0.000 -> 0.445
2024-12-02-11:43:18-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-11:43:18-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-11:43:18-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-11:43:18-root-INFO: grad norm: 0.709 0.700 0.113
2024-12-02-11:43:19-root-INFO: grad norm: 0.358 0.356 0.041
2024-12-02-11:43:19-root-INFO: grad norm: 0.285 0.283 0.036
2024-12-02-11:43:20-root-INFO: grad norm: 0.257 0.255 0.033
2024-12-02-11:43:20-root-INFO: grad norm: 0.237 0.235 0.033
2024-12-02-11:43:21-root-INFO: Loss Change: 1.518 -> 1.277
2024-12-02-11:43:21-root-INFO: Regularization Change: 0.000 -> 0.387
2024-12-02-11:43:21-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-11:43:21-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-11:43:21-root-INFO: loss_sample0_0: 1.277042031288147
2024-12-02-11:43:21-root-INFO: It takes 1408.252 seconds for image sample0
2024-12-02-11:43:21-root-INFO: lpips_score_sample0: 0.142
2024-12-02-11:43:21-root-INFO: psnr_score_sample0: 14.883
2024-12-02-11:43:21-root-INFO: ssim_score_sample0: 0.719
2024-12-02-11:43:21-root-INFO: mean_lpips: 0.14231032133102417
2024-12-02-11:43:21-root-INFO: best_mean_lpips: 0.14231032133102417
2024-12-02-11:43:21-root-INFO: mean_psnr: 14.88310718536377
2024-12-02-11:43:21-root-INFO: best_mean_psnr: 14.88310718536377
2024-12-02-11:43:21-root-INFO: mean_ssim: 0.7188215851783752
2024-12-02-11:43:21-root-INFO: best_mean_ssim: 0.7188215851783752
2024-12-02-11:43:21-root-INFO: final_loss: 1.277042031288147
2024-12-02-11:43:21-root-INFO: mean time: 1408.251559972763
2024-12-02-11:43:21-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump10_sample2_iter5_lr0.02_10009 
 
Enjoy.
