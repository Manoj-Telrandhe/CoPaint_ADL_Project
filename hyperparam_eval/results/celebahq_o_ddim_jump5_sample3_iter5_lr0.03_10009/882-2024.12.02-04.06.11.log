2024-12-02-04:06:15-root-INFO: Prepare model...
2024-12-02-04:06:32-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-04:06:58-root-INFO: Start sampling
2024-12-02-04:07:06-root-INFO: step: 249 lr_xt 0.00019059
2024-12-02-04:07:06-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-04:07:06-root-INFO: Loss too large (77070.016->78612.734)! Learning rate decreased to 0.00015.
2024-12-02-04:07:07-root-INFO: grad norm: 15661.117 11248.846 10896.517
2024-12-02-04:07:08-root-INFO: grad norm: 14205.981 10378.407 9700.442
2024-12-02-04:07:09-root-INFO: grad norm: 15590.536 11542.663 10480.064
2024-12-02-04:07:10-root-INFO: Loss too large (28301.777->35851.707)! Learning rate decreased to 0.00012.
2024-12-02-04:07:11-root-INFO: grad norm: 16985.406 12431.942 11573.713
2024-12-02-04:07:11-root-INFO: Loss too large (28113.967->29014.418)! Learning rate decreased to 0.00010.
2024-12-02-04:07:12-root-INFO: Loss Change: 77070.016 -> 22717.551
2024-12-02-04:07:12-root-INFO: Regularization Change: 0.000 -> 14.512
2024-12-02-04:07:12-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-04:07:12-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-04:07:12-root-INFO: step: 248 lr_xt 0.00020082
2024-12-02-04:07:12-root-INFO: grad norm: 13146.896 10136.991 8371.517
2024-12-02-04:07:13-root-INFO: Loss too large (23084.639->44015.531)! Learning rate decreased to 0.00016.
2024-12-02-04:07:13-root-INFO: Loss too large (23084.639->34075.078)! Learning rate decreased to 0.00013.
2024-12-02-04:07:13-root-INFO: Loss too large (23084.639->26825.744)! Learning rate decreased to 0.00010.
2024-12-02-04:07:14-root-INFO: grad norm: 12869.057 10069.368 8013.767
2024-12-02-04:07:15-root-INFO: grad norm: 14421.148 11505.646 8694.231
2024-12-02-04:07:16-root-INFO: Loss too large (22209.383->24127.156)! Learning rate decreased to 0.00008.
2024-12-02-04:07:17-root-INFO: grad norm: 11116.650 8914.315 6641.904
2024-12-02-04:07:18-root-INFO: grad norm: 8516.225 6794.511 5134.269
2024-12-02-04:07:18-root-INFO: Loss Change: 23084.639 -> 17987.920
2024-12-02-04:07:18-root-INFO: Regularization Change: 0.000 -> 1.692
2024-12-02-04:07:18-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03072.
2024-12-02-04:07:18-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-04:07:19-root-INFO: step: 247 lr_xt 0.00021156
2024-12-02-04:07:19-root-INFO: grad norm: 6318.181 5137.877 3677.176
2024-12-02-04:07:19-root-INFO: Loss too large (17729.430->26603.553)! Learning rate decreased to 0.00017.
2024-12-02-04:07:20-root-INFO: Loss too large (17729.430->22041.955)! Learning rate decreased to 0.00014.
2024-12-02-04:07:20-root-INFO: Loss too large (17729.430->19384.324)! Learning rate decreased to 0.00011.
2024-12-02-04:07:20-root-INFO: Loss too large (17729.430->17904.969)! Learning rate decreased to 0.00009.
2024-12-02-04:07:21-root-INFO: grad norm: 4648.279 3702.273 2810.636
2024-12-02-04:07:22-root-INFO: grad norm: 3494.396 2850.562 2021.163
2024-12-02-04:07:23-root-INFO: grad norm: 2611.450 2080.918 1577.799
2024-12-02-04:07:24-root-INFO: grad norm: 1995.692 1635.719 1143.333
2024-12-02-04:07:25-root-INFO: Loss Change: 17729.430 -> 16356.013
2024-12-02-04:07:25-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-02-04:07:25-root-INFO: Learning rate of xt decay: 0.03072 -> 0.03109.
2024-12-02-04:07:25-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-04:07:25-root-INFO: step: 246 lr_xt 0.00022285
2024-12-02-04:07:26-root-INFO: grad norm: 1556.861 1253.936 922.746
2024-12-02-04:07:26-root-INFO: Loss too large (16122.148->16330.123)! Learning rate decreased to 0.00018.
2024-12-02-04:07:26-root-INFO: Loss too large (16122.148->16170.574)! Learning rate decreased to 0.00014.
2024-12-02-04:07:27-root-INFO: grad norm: 2204.074 1809.024 1259.116
2024-12-02-04:07:28-root-INFO: Loss too large (16084.890->16169.407)! Learning rate decreased to 0.00011.
2024-12-02-04:07:29-root-INFO: grad norm: 2375.545 1931.557 1382.859
2024-12-02-04:07:30-root-INFO: grad norm: 2567.177 2083.507 1499.798
2024-12-02-04:07:31-root-INFO: grad norm: 2786.879 2274.240 1610.752
2024-12-02-04:07:31-root-INFO: Loss Change: 16122.148 -> 15913.456
2024-12-02-04:07:31-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-04:07:31-root-INFO: Learning rate of xt decay: 0.03109 -> 0.03147.
2024-12-02-04:07:31-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-04:07:32-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-04:07:32-root-INFO: grad norm: 2946.810 2377.348 1741.237
2024-12-02-04:07:32-root-INFO: Loss too large (15795.615->17532.008)! Learning rate decreased to 0.00019.
2024-12-02-04:07:33-root-INFO: Loss too large (15795.615->16571.600)! Learning rate decreased to 0.00015.
2024-12-02-04:07:33-root-INFO: Loss too large (15795.615->16025.604)! Learning rate decreased to 0.00012.
2024-12-02-04:07:34-root-INFO: grad norm: 3040.690 2492.311 1741.891
2024-12-02-04:07:35-root-INFO: grad norm: 3139.783 2524.944 1866.252
2024-12-02-04:07:36-root-INFO: grad norm: 3247.255 2669.246 1849.266
2024-12-02-04:07:37-root-INFO: grad norm: 3351.584 2689.330 2000.155
2024-12-02-04:07:38-root-INFO: Loss Change: 15795.615 -> 15539.898
2024-12-02-04:07:38-root-INFO: Regularization Change: 0.000 -> 0.342
2024-12-02-04:07:38-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-04:07:38-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-04:07:38-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-04:07:38-root-INFO: grad norm: 3648.814 2955.887 2139.293
2024-12-02-04:07:39-root-INFO: Loss too large (15494.658->17986.941)! Learning rate decreased to 0.00020.
2024-12-02-04:07:39-root-INFO: Loss too large (15494.658->16618.496)! Learning rate decreased to 0.00016.
2024-12-02-04:07:39-root-INFO: Loss too large (15494.658->15815.498)! Learning rate decreased to 0.00013.
2024-12-02-04:07:40-root-INFO: grad norm: 3486.905 2818.775 2052.562
2024-12-02-04:07:41-root-INFO: grad norm: 3393.412 2798.946 1918.631
2024-12-02-04:07:43-root-INFO: grad norm: 3338.315 2695.568 1969.331
2024-12-02-04:07:44-root-INFO: grad norm: 3293.786 2734.138 1836.713
2024-12-02-04:07:44-root-INFO: Loss Change: 15494.658 -> 15013.148
2024-12-02-04:07:44-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-02-04:07:44-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-04:07:44-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:07:45-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-04:07:45-root-INFO: grad norm: 3311.615 2651.034 1984.645
2024-12-02-04:07:45-root-INFO: Loss too large (14914.229->17108.842)! Learning rate decreased to 0.00021.
2024-12-02-04:07:46-root-INFO: Loss too large (14914.229->15846.070)! Learning rate decreased to 0.00017.
2024-12-02-04:07:46-root-INFO: Loss too large (14914.229->15134.020)! Learning rate decreased to 0.00013.
2024-12-02-04:07:47-root-INFO: grad norm: 3080.976 2573.856 1693.422
2024-12-02-04:07:48-root-INFO: grad norm: 2926.721 2353.556 1739.676
2024-12-02-04:07:49-root-INFO: grad norm: 2788.843 2345.966 1508.008
2024-12-02-04:07:50-root-INFO: grad norm: 2673.982 2153.480 1585.151
2024-12-02-04:07:51-root-INFO: Loss Change: 14914.229 -> 14353.530
2024-12-02-04:07:51-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-02-04:07:51-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-04:07:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:07:51-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-04:07:51-root-INFO: grad norm: 2584.742 2173.679 1398.574
2024-12-02-04:07:51-root-INFO: Loss too large (14157.009->15402.508)! Learning rate decreased to 0.00022.
2024-12-02-04:07:52-root-INFO: Loss too large (14157.009->14673.432)! Learning rate decreased to 0.00018.
2024-12-02-04:07:52-root-INFO: Loss too large (14157.009->14258.386)! Learning rate decreased to 0.00014.
2024-12-02-04:07:53-root-INFO: grad norm: 2379.970 1939.275 1379.663
2024-12-02-04:07:54-root-INFO: grad norm: 2208.606 1880.588 1158.159
2024-12-02-04:07:55-root-INFO: grad norm: 2065.159 1675.612 1207.147
2024-12-02-04:07:56-root-INFO: grad norm: 1934.903 1657.161 998.832
2024-12-02-04:07:57-root-INFO: Loss Change: 14157.009 -> 13641.111
2024-12-02-04:07:57-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-02-04:07:57-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-04:07:57-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:07:57-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-04:07:57-root-INFO: grad norm: 1677.081 1382.141 949.887
2024-12-02-04:07:58-root-INFO: Loss too large (13532.136->13919.677)! Learning rate decreased to 0.00023.
2024-12-02-04:07:58-root-INFO: Loss too large (13532.136->13649.043)! Learning rate decreased to 0.00018.
2024-12-02-04:07:59-root-INFO: grad norm: 2174.734 1857.616 1130.811
2024-12-02-04:07:59-root-INFO: Loss too large (13502.413->13545.975)! Learning rate decreased to 0.00015.
2024-12-02-04:08:00-root-INFO: grad norm: 1973.854 1621.644 1125.333
2024-12-02-04:08:01-root-INFO: grad norm: 1797.366 1547.274 914.586
2024-12-02-04:08:02-root-INFO: grad norm: 1646.296 1355.005 935.014
2024-12-02-04:08:03-root-INFO: Loss Change: 13532.136 -> 13090.703
2024-12-02-04:08:03-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-04:08:03-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-04:08:03-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:08:03-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-04:08:04-root-INFO: grad norm: 1540.224 1325.515 784.410
2024-12-02-04:08:04-root-INFO: Loss too large (12911.129->13151.885)! Learning rate decreased to 0.00024.
2024-12-02-04:08:04-root-INFO: Loss too large (12911.129->12950.781)! Learning rate decreased to 0.00019.
2024-12-02-04:08:05-root-INFO: grad norm: 1884.923 1567.158 1047.355
2024-12-02-04:08:06-root-INFO: grad norm: 2417.457 2073.564 1242.752
2024-12-02-04:08:07-root-INFO: Loss too large (12830.559->12900.192)! Learning rate decreased to 0.00016.
2024-12-02-04:08:08-root-INFO: grad norm: 2125.964 1765.562 1184.277
2024-12-02-04:08:09-root-INFO: grad norm: 1875.517 1619.506 945.920
2024-12-02-04:08:09-root-INFO: Loss Change: 12911.129 -> 12457.753
2024-12-02-04:08:09-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-02-04:08:09-root-INFO: Undo step: 240
2024-12-02-04:08:09-root-INFO: Undo step: 241
2024-12-02-04:08:09-root-INFO: Undo step: 242
2024-12-02-04:08:09-root-INFO: Undo step: 243
2024-12-02-04:08:09-root-INFO: Undo step: 244
2024-12-02-04:08:10-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-04:08:10-root-INFO: grad norm: 10430.908 7518.201 7230.525
2024-12-02-04:08:10-root-INFO: Loss too large (19432.668->30372.143)! Learning rate decreased to 0.00019.
2024-12-02-04:08:11-root-INFO: Loss too large (19432.668->23217.545)! Learning rate decreased to 0.00015.
2024-12-02-04:08:12-root-INFO: grad norm: 9896.833 7096.310 6898.527
2024-12-02-04:08:13-root-INFO: grad norm: 10145.358 7416.397 6922.813
2024-12-02-04:08:13-root-INFO: Loss too large (18286.137->19008.674)! Learning rate decreased to 0.00012.
2024-12-02-04:08:14-root-INFO: grad norm: 7079.028 5206.382 4796.481
2024-12-02-04:08:15-root-INFO: grad norm: 5211.885 3968.189 3378.938
2024-12-02-04:08:16-root-INFO: Loss Change: 19432.668 -> 14978.932
2024-12-02-04:08:16-root-INFO: Regularization Change: 0.000 -> 1.588
2024-12-02-04:08:16-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-04:08:16-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-04:08:16-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-04:08:16-root-INFO: grad norm: 4395.956 3278.066 2928.944
2024-12-02-04:08:17-root-INFO: Loss too large (14994.463->17917.281)! Learning rate decreased to 0.00020.
2024-12-02-04:08:17-root-INFO: Loss too large (14994.463->16046.750)! Learning rate decreased to 0.00016.
2024-12-02-04:08:17-root-INFO: Loss too large (14994.463->15011.185)! Learning rate decreased to 0.00013.
2024-12-02-04:08:18-root-INFO: grad norm: 3316.038 2587.344 2074.068
2024-12-02-04:08:19-root-INFO: grad norm: 2777.102 2116.423 1798.068
2024-12-02-04:08:20-root-INFO: grad norm: 2396.585 1958.922 1380.667
2024-12-02-04:08:21-root-INFO: grad norm: 2163.782 1674.004 1371.008
2024-12-02-04:08:22-root-INFO: Loss Change: 14994.463 -> 13860.728
2024-12-02-04:08:22-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-04:08:22-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-04:08:22-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:08:22-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-04:08:23-root-INFO: grad norm: 2048.343 1667.732 1189.277
2024-12-02-04:08:23-root-INFO: Loss too large (13641.734->14120.657)! Learning rate decreased to 0.00021.
2024-12-02-04:08:23-root-INFO: Loss too large (13641.734->13780.502)! Learning rate decreased to 0.00017.
2024-12-02-04:08:24-root-INFO: grad norm: 2483.444 1910.279 1586.925
2024-12-02-04:08:25-root-INFO: Loss too large (13593.650->13625.129)! Learning rate decreased to 0.00013.
2024-12-02-04:08:26-root-INFO: grad norm: 2157.056 1806.311 1179.037
2024-12-02-04:08:27-root-INFO: grad norm: 1944.175 1515.559 1217.743
2024-12-02-04:08:28-root-INFO: grad norm: 1777.402 1519.383 922.299
2024-12-02-04:08:28-root-INFO: Loss Change: 13641.734 -> 13145.015
2024-12-02-04:08:28-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-04:08:28-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-04:08:28-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:08:29-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-04:08:29-root-INFO: grad norm: 1498.051 1201.750 894.400
2024-12-02-04:08:29-root-INFO: Loss too large (12914.205->13100.692)! Learning rate decreased to 0.00022.
2024-12-02-04:08:30-root-INFO: Loss too large (12914.205->12933.140)! Learning rate decreased to 0.00018.
2024-12-02-04:08:31-root-INFO: grad norm: 1853.554 1583.617 963.233
2024-12-02-04:08:32-root-INFO: grad norm: 2401.086 1917.526 1445.098
2024-12-02-04:08:32-root-INFO: Loss too large (12835.897->12902.686)! Learning rate decreased to 0.00014.
2024-12-02-04:08:33-root-INFO: grad norm: 2160.242 1838.887 1133.640
2024-12-02-04:08:34-root-INFO: grad norm: 1959.459 1562.518 1182.378
2024-12-02-04:08:35-root-INFO: Loss Change: 12914.205 -> 12512.925
2024-12-02-04:08:35-root-INFO: Regularization Change: 0.000 -> 0.383
2024-12-02-04:08:35-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-04:08:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:08:35-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-04:08:35-root-INFO: grad norm: 1850.065 1573.020 973.832
2024-12-02-04:08:36-root-INFO: Loss too large (12431.975->12945.988)! Learning rate decreased to 0.00023.
2024-12-02-04:08:36-root-INFO: Loss too large (12431.975->12607.274)! Learning rate decreased to 0.00018.
2024-12-02-04:08:37-root-INFO: grad norm: 2342.805 1896.762 1375.147
2024-12-02-04:08:37-root-INFO: Loss too large (12419.754->12470.930)! Learning rate decreased to 0.00015.
2024-12-02-04:08:38-root-INFO: grad norm: 2041.071 1736.662 1072.369
2024-12-02-04:08:39-root-INFO: grad norm: 1796.975 1454.597 1055.114
2024-12-02-04:08:40-root-INFO: grad norm: 1586.475 1363.423 811.160
2024-12-02-04:08:41-root-INFO: Loss Change: 12431.975 -> 11988.739
2024-12-02-04:08:41-root-INFO: Regularization Change: 0.000 -> 0.356
2024-12-02-04:08:41-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-04:08:41-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:08:41-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-04:08:42-root-INFO: grad norm: 1389.031 1152.719 775.014
2024-12-02-04:08:42-root-INFO: Loss too large (11809.446->11960.268)! Learning rate decreased to 0.00024.
2024-12-02-04:08:42-root-INFO: Loss too large (11809.446->11810.882)! Learning rate decreased to 0.00019.
2024-12-02-04:08:43-root-INFO: grad norm: 1633.003 1399.361 841.716
2024-12-02-04:08:44-root-INFO: grad norm: 1999.318 1633.038 1153.456
2024-12-02-04:08:45-root-INFO: Loss too large (11700.441->11703.770)! Learning rate decreased to 0.00016.
2024-12-02-04:08:46-root-INFO: grad norm: 1683.976 1446.180 862.751
2024-12-02-04:08:47-root-INFO: grad norm: 1440.469 1180.278 825.771
2024-12-02-04:08:47-root-INFO: Loss Change: 11809.446 -> 11385.018
2024-12-02-04:08:47-root-INFO: Regularization Change: 0.000 -> 0.386
2024-12-02-04:08:47-root-INFO: Undo step: 240
2024-12-02-04:08:47-root-INFO: Undo step: 241
2024-12-02-04:08:47-root-INFO: Undo step: 242
2024-12-02-04:08:48-root-INFO: Undo step: 243
2024-12-02-04:08:48-root-INFO: Undo step: 244
2024-12-02-04:08:48-root-INFO: step: 245 lr_xt 0.00023469
2024-12-02-04:08:48-root-INFO: grad norm: 15515.424 12498.121 9193.766
2024-12-02-04:08:49-root-INFO: Loss too large (23860.566->40562.445)! Learning rate decreased to 0.00019.
2024-12-02-04:08:49-root-INFO: Loss too large (23860.566->31438.082)! Learning rate decreased to 0.00015.
2024-12-02-04:08:49-root-INFO: Loss too large (23860.566->24154.406)! Learning rate decreased to 0.00012.
2024-12-02-04:08:50-root-INFO: grad norm: 11935.526 9247.166 7546.304
2024-12-02-04:08:51-root-INFO: grad norm: 10193.609 8354.241 5840.918
2024-12-02-04:08:52-root-INFO: grad norm: 9551.444 7577.733 5814.469
2024-12-02-04:08:53-root-INFO: grad norm: 8841.818 7183.144 5155.600
2024-12-02-04:08:54-root-INFO: Loss Change: 23860.566 -> 15116.025
2024-12-02-04:08:54-root-INFO: Regularization Change: 0.000 -> 4.053
2024-12-02-04:08:54-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03184.
2024-12-02-04:08:54-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-04:08:54-root-INFO: step: 244 lr_xt 0.00024712
2024-12-02-04:08:54-root-INFO: grad norm: 7490.623 6048.907 4418.162
2024-12-02-04:08:55-root-INFO: Loss too large (14831.951->25407.588)! Learning rate decreased to 0.00020.
2024-12-02-04:08:55-root-INFO: Loss too large (14831.951->19260.395)! Learning rate decreased to 0.00016.
2024-12-02-04:08:55-root-INFO: Loss too large (14831.951->15760.484)! Learning rate decreased to 0.00013.
2024-12-02-04:08:56-root-INFO: grad norm: 6306.165 5134.364 3661.424
2024-12-02-04:08:57-root-INFO: grad norm: 5630.546 4551.406 3314.778
2024-12-02-04:08:58-root-INFO: grad norm: 5019.729 4103.070 2891.799
2024-12-02-04:08:59-root-INFO: grad norm: 4591.724 3705.780 2711.295
2024-12-02-04:09:00-root-INFO: Loss Change: 14831.951 -> 12806.172
2024-12-02-04:09:00-root-INFO: Regularization Change: 0.000 -> 0.954
2024-12-02-04:09:00-root-INFO: Learning rate of xt decay: 0.03184 -> 0.03223.
2024-12-02-04:09:00-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:00-root-INFO: step: 243 lr_xt 0.00026017
2024-12-02-04:09:01-root-INFO: grad norm: 3787.496 3102.882 2171.923
2024-12-02-04:09:01-root-INFO: Loss too large (12494.400->15136.519)! Learning rate decreased to 0.00021.
2024-12-02-04:09:01-root-INFO: Loss too large (12494.400->13651.523)! Learning rate decreased to 0.00017.
2024-12-02-04:09:02-root-INFO: Loss too large (12494.400->12781.980)! Learning rate decreased to 0.00013.
2024-12-02-04:09:03-root-INFO: grad norm: 3340.438 2698.421 1969.023
2024-12-02-04:09:04-root-INFO: grad norm: 2974.700 2469.757 1658.053
2024-12-02-04:09:05-root-INFO: grad norm: 2706.437 2180.759 1602.839
2024-12-02-04:09:06-root-INFO: grad norm: 2464.320 2063.066 1347.825
2024-12-02-04:09:06-root-INFO: Loss Change: 12494.400 -> 11792.590
2024-12-02-04:09:06-root-INFO: Regularization Change: 0.000 -> 0.423
2024-12-02-04:09:06-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03261.
2024-12-02-04:09:06-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:07-root-INFO: step: 242 lr_xt 0.00027387
2024-12-02-04:09:07-root-INFO: grad norm: 2131.982 1748.525 1219.840
2024-12-02-04:09:07-root-INFO: Loss too large (11574.682->12302.252)! Learning rate decreased to 0.00022.
2024-12-02-04:09:08-root-INFO: Loss too large (11574.682->11834.832)! Learning rate decreased to 0.00018.
2024-12-02-04:09:08-root-INFO: Loss too large (11574.682->11577.855)! Learning rate decreased to 0.00014.
2024-12-02-04:09:09-root-INFO: grad norm: 1846.802 1559.845 988.718
2024-12-02-04:09:10-root-INFO: grad norm: 1646.673 1336.885 961.390
2024-12-02-04:09:11-root-INFO: grad norm: 1480.221 1273.331 754.773
2024-12-02-04:09:12-root-INFO: grad norm: 1339.101 1088.985 779.296
2024-12-02-04:09:13-root-INFO: Loss Change: 11574.682 -> 11109.216
2024-12-02-04:09:13-root-INFO: Regularization Change: 0.000 -> 0.296
2024-12-02-04:09:13-root-INFO: Learning rate of xt decay: 0.03261 -> 0.03300.
2024-12-02-04:09:13-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:13-root-INFO: step: 241 lr_xt 0.00028824
2024-12-02-04:09:13-root-INFO: grad norm: 1278.599 1096.071 658.365
2024-12-02-04:09:14-root-INFO: Loss too large (11022.252->11149.919)! Learning rate decreased to 0.00023.
2024-12-02-04:09:14-root-INFO: Loss too large (11022.252->11029.340)! Learning rate decreased to 0.00018.
2024-12-02-04:09:15-root-INFO: grad norm: 1528.056 1256.416 869.697
2024-12-02-04:09:16-root-INFO: grad norm: 1904.125 1620.747 999.436
2024-12-02-04:09:16-root-INFO: Loss too large (10937.371->10949.716)! Learning rate decreased to 0.00015.
2024-12-02-04:09:17-root-INFO: grad norm: 1638.661 1344.765 936.386
2024-12-02-04:09:18-root-INFO: grad norm: 1417.941 1223.376 716.873
2024-12-02-04:09:19-root-INFO: Loss Change: 11022.252 -> 10672.771
2024-12-02-04:09:19-root-INFO: Regularization Change: 0.000 -> 0.313
2024-12-02-04:09:19-root-INFO: Learning rate of xt decay: 0.03300 -> 0.03340.
2024-12-02-04:09:19-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:19-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-04:09:20-root-INFO: grad norm: 1298.278 1095.516 696.685
2024-12-02-04:09:20-root-INFO: Loss too large (10508.200->10631.371)! Learning rate decreased to 0.00024.
2024-12-02-04:09:21-root-INFO: grad norm: 2035.933 1739.059 1058.629
2024-12-02-04:09:21-root-INFO: Loss too large (10503.827->10775.256)! Learning rate decreased to 0.00019.
2024-12-02-04:09:22-root-INFO: Loss too large (10503.827->10519.090)! Learning rate decreased to 0.00016.
2024-12-02-04:09:23-root-INFO: grad norm: 1663.510 1371.960 940.741
2024-12-02-04:09:24-root-INFO: grad norm: 1373.592 1194.665 677.887
2024-12-02-04:09:25-root-INFO: grad norm: 1154.380 960.965 639.640
2024-12-02-04:09:25-root-INFO: Loss Change: 10508.200 -> 10136.939
2024-12-02-04:09:25-root-INFO: Regularization Change: 0.000 -> 0.322
2024-12-02-04:09:25-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-04:09:25-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:26-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-04:09:26-root-INFO: grad norm: 936.602 825.988 441.549
2024-12-02-04:09:26-root-INFO: Loss too large (10035.439->10038.851)! Learning rate decreased to 0.00026.
2024-12-02-04:09:27-root-INFO: grad norm: 1373.786 1140.610 765.700
2024-12-02-04:09:28-root-INFO: Loss too large (9993.174->10052.386)! Learning rate decreased to 0.00020.
2024-12-02-04:09:29-root-INFO: grad norm: 1597.228 1372.962 816.158
2024-12-02-04:09:30-root-INFO: grad norm: 1872.350 1556.913 1040.055
2024-12-02-04:09:31-root-INFO: grad norm: 2216.834 1891.038 1156.862
2024-12-02-04:09:31-root-INFO: Loss too large (9929.797->9963.042)! Learning rate decreased to 0.00016.
2024-12-02-04:09:32-root-INFO: Loss Change: 10035.439 -> 9792.081
2024-12-02-04:09:32-root-INFO: Regularization Change: 0.000 -> 0.405
2024-12-02-04:09:32-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-04:09:32-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:32-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-04:09:32-root-INFO: grad norm: 1890.656 1611.612 988.577
2024-12-02-04:09:33-root-INFO: Loss too large (9630.201->10178.439)! Learning rate decreased to 0.00027.
2024-12-02-04:09:33-root-INFO: Loss too large (9630.201->9782.455)! Learning rate decreased to 0.00021.
2024-12-02-04:09:34-root-INFO: grad norm: 2041.002 1758.049 1036.798
2024-12-02-04:09:35-root-INFO: grad norm: 2301.386 1932.542 1249.663
2024-12-02-04:09:35-root-INFO: Loss too large (9568.787->9581.492)! Learning rate decreased to 0.00017.
2024-12-02-04:09:36-root-INFO: grad norm: 1682.079 1466.060 824.656
2024-12-02-04:09:37-root-INFO: grad norm: 1268.858 1071.351 679.858
2024-12-02-04:09:38-root-INFO: Loss Change: 9630.201 -> 9209.017
2024-12-02-04:09:38-root-INFO: Regularization Change: 0.000 -> 0.338
2024-12-02-04:09:38-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-04:09:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:38-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-04:09:39-root-INFO: grad norm: 788.566 705.075 353.137
2024-12-02-04:09:40-root-INFO: grad norm: 1361.876 1144.628 737.925
2024-12-02-04:09:40-root-INFO: Loss too large (9096.502->9370.536)! Learning rate decreased to 0.00028.
2024-12-02-04:09:40-root-INFO: Loss too large (9096.502->9166.481)! Learning rate decreased to 0.00023.
2024-12-02-04:09:41-root-INFO: grad norm: 1477.810 1302.239 698.640
2024-12-02-04:09:42-root-INFO: grad norm: 1614.908 1370.858 853.625
2024-12-02-04:09:43-root-INFO: grad norm: 1774.457 1554.251 856.155
2024-12-02-04:09:44-root-INFO: Loss Change: 9124.860 -> 8988.775
2024-12-02-04:09:44-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-02-04:09:44-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-04:09:44-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:44-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-04:09:45-root-INFO: grad norm: 1854.595 1595.101 946.139
2024-12-02-04:09:45-root-INFO: Loss too large (8837.133->9484.092)! Learning rate decreased to 0.00030.
2024-12-02-04:09:45-root-INFO: Loss too large (8837.133->9043.797)! Learning rate decreased to 0.00024.
2024-12-02-04:09:46-root-INFO: grad norm: 1973.286 1719.283 968.463
2024-12-02-04:09:47-root-INFO: grad norm: 2099.077 1796.861 1085.087
2024-12-02-04:09:48-root-INFO: grad norm: 2233.319 1945.817 1096.136
2024-12-02-04:09:49-root-INFO: grad norm: 2359.528 2020.399 1218.753
2024-12-02-04:09:50-root-INFO: Loss Change: 8837.133 -> 8737.043
2024-12-02-04:09:50-root-INFO: Regularization Change: 0.000 -> 0.543
2024-12-02-04:09:50-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-04:09:50-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:09:50-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-04:09:51-root-INFO: grad norm: 2054.201 1799.435 990.845
2024-12-02-04:09:51-root-INFO: Loss too large (8556.281->9359.919)! Learning rate decreased to 0.00031.
2024-12-02-04:09:51-root-INFO: Loss too large (8556.281->8818.435)! Learning rate decreased to 0.00025.
2024-12-02-04:09:52-root-INFO: grad norm: 2073.057 1786.948 1050.896
2024-12-02-04:09:53-root-INFO: grad norm: 2106.280 1856.867 994.214
2024-12-02-04:09:54-root-INFO: grad norm: 2135.178 1842.539 1078.906
2024-12-02-04:09:55-root-INFO: grad norm: 2162.882 1906.125 1022.129
2024-12-02-04:09:56-root-INFO: Loss Change: 8556.281 -> 8360.547
2024-12-02-04:09:56-root-INFO: Regularization Change: 0.000 -> 0.568
2024-12-02-04:09:56-root-INFO: Undo step: 235
2024-12-02-04:09:56-root-INFO: Undo step: 236
2024-12-02-04:09:56-root-INFO: Undo step: 237
2024-12-02-04:09:56-root-INFO: Undo step: 238
2024-12-02-04:09:56-root-INFO: Undo step: 239
2024-12-02-04:09:56-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-04:09:57-root-INFO: grad norm: 6289.663 5114.943 3660.221
2024-12-02-04:09:57-root-INFO: Loss too large (12747.008->16410.086)! Learning rate decreased to 0.00024.
2024-12-02-04:09:57-root-INFO: Loss too large (12747.008->13185.312)! Learning rate decreased to 0.00019.
2024-12-02-04:09:58-root-INFO: grad norm: 5961.733 5060.639 3151.540
2024-12-02-04:09:59-root-INFO: Loss too large (11469.838->11758.000)! Learning rate decreased to 0.00016.
2024-12-02-04:10:00-root-INFO: grad norm: 4290.962 3660.686 2238.690
2024-12-02-04:10:01-root-INFO: grad norm: 3039.433 2567.073 1627.358
2024-12-02-04:10:02-root-INFO: grad norm: 2268.642 1944.683 1168.310
2024-12-02-04:10:02-root-INFO: Loss Change: 12747.008 -> 9561.420
2024-12-02-04:10:02-root-INFO: Regularization Change: 0.000 -> 1.587
2024-12-02-04:10:02-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-04:10:02-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:03-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-04:10:03-root-INFO: grad norm: 1558.777 1332.756 808.422
2024-12-02-04:10:03-root-INFO: Loss too large (9457.755->9722.078)! Learning rate decreased to 0.00026.
2024-12-02-04:10:04-root-INFO: Loss too large (9457.755->9505.720)! Learning rate decreased to 0.00020.
2024-12-02-04:10:05-root-INFO: grad norm: 1711.476 1468.775 878.550
2024-12-02-04:10:06-root-INFO: grad norm: 1919.029 1641.363 994.284
2024-12-02-04:10:07-root-INFO: grad norm: 2159.178 1837.544 1133.792
2024-12-02-04:10:08-root-INFO: grad norm: 2450.126 2093.096 1273.604
2024-12-02-04:10:08-root-INFO: Loss too large (9303.391->9329.492)! Learning rate decreased to 0.00016.
2024-12-02-04:10:09-root-INFO: Loss Change: 9457.755 -> 9127.314
2024-12-02-04:10:09-root-INFO: Regularization Change: 0.000 -> 0.465
2024-12-02-04:10:09-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-04:10:09-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:09-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-04:10:09-root-INFO: grad norm: 1897.176 1651.484 933.744
2024-12-02-04:10:10-root-INFO: Loss too large (8947.938->9465.054)! Learning rate decreased to 0.00027.
2024-12-02-04:10:10-root-INFO: Loss too large (8947.938->9075.601)! Learning rate decreased to 0.00021.
2024-12-02-04:10:11-root-INFO: grad norm: 1940.077 1683.458 964.296
2024-12-02-04:10:12-root-INFO: grad norm: 2071.786 1772.300 1072.963
2024-12-02-04:10:13-root-INFO: grad norm: 2238.627 1944.721 1108.834
2024-12-02-04:10:13-root-INFO: Loss too large (8816.791->8818.076)! Learning rate decreased to 0.00017.
2024-12-02-04:10:14-root-INFO: grad norm: 1556.109 1334.645 800.123
2024-12-02-04:10:15-root-INFO: Loss Change: 8947.938 -> 8542.528
2024-12-02-04:10:15-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-02-04:10:15-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-04:10:15-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:15-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-04:10:15-root-INFO: grad norm: 849.211 753.806 391.071
2024-12-02-04:10:16-root-INFO: Loss too large (8474.010->8477.851)! Learning rate decreased to 0.00028.
2024-12-02-04:10:17-root-INFO: grad norm: 1123.254 966.016 573.160
2024-12-02-04:10:17-root-INFO: Loss too large (8436.072->8453.221)! Learning rate decreased to 0.00023.
2024-12-02-04:10:18-root-INFO: grad norm: 1158.458 1030.490 529.260
2024-12-02-04:10:19-root-INFO: grad norm: 1202.675 1039.908 604.169
2024-12-02-04:10:20-root-INFO: grad norm: 1256.606 1113.942 581.542
2024-12-02-04:10:21-root-INFO: Loss Change: 8474.010 -> 8276.176
2024-12-02-04:10:21-root-INFO: Regularization Change: 0.000 -> 0.350
2024-12-02-04:10:21-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-04:10:21-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:21-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-04:10:21-root-INFO: grad norm: 1261.123 1105.092 607.620
2024-12-02-04:10:22-root-INFO: Loss too large (8119.761->8351.616)! Learning rate decreased to 0.00030.
2024-12-02-04:10:22-root-INFO: Loss too large (8119.761->8171.661)! Learning rate decreased to 0.00024.
2024-12-02-04:10:23-root-INFO: grad norm: 1287.955 1138.547 602.113
2024-12-02-04:10:24-root-INFO: grad norm: 1322.142 1151.908 648.974
2024-12-02-04:10:25-root-INFO: grad norm: 1358.869 1201.460 634.837
2024-12-02-04:10:26-root-INFO: grad norm: 1390.942 1211.982 682.510
2024-12-02-04:10:27-root-INFO: Loss Change: 8119.761 -> 7933.032
2024-12-02-04:10:27-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-04:10:27-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-04:10:27-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:27-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-04:10:27-root-INFO: grad norm: 1058.870 944.332 479.001
2024-12-02-04:10:28-root-INFO: Loss too large (7800.715->7937.291)! Learning rate decreased to 0.00031.
2024-12-02-04:10:28-root-INFO: Loss too large (7800.715->7819.479)! Learning rate decreased to 0.00025.
2024-12-02-04:10:29-root-INFO: grad norm: 1034.883 909.291 494.138
2024-12-02-04:10:30-root-INFO: grad norm: 1024.194 921.629 446.736
2024-12-02-04:10:31-root-INFO: grad norm: 1016.174 895.879 479.593
2024-12-02-04:10:32-root-INFO: grad norm: 1009.451 908.012 441.028
2024-12-02-04:10:33-root-INFO: Loss Change: 7800.715 -> 7601.678
2024-12-02-04:10:33-root-INFO: Regularization Change: 0.000 -> 0.299
2024-12-02-04:10:33-root-INFO: Undo step: 235
2024-12-02-04:10:33-root-INFO: Undo step: 236
2024-12-02-04:10:33-root-INFO: Undo step: 237
2024-12-02-04:10:33-root-INFO: Undo step: 238
2024-12-02-04:10:33-root-INFO: Undo step: 239
2024-12-02-04:10:33-root-INFO: step: 240 lr_xt 0.00030331
2024-12-02-04:10:33-root-INFO: grad norm: 12168.369 9280.789 7869.954
2024-12-02-04:10:34-root-INFO: Loss too large (19171.082->30721.973)! Learning rate decreased to 0.00024.
2024-12-02-04:10:34-root-INFO: Loss too large (19171.082->22536.488)! Learning rate decreased to 0.00019.
2024-12-02-04:10:35-root-INFO: grad norm: 10180.237 8204.914 6026.327
2024-12-02-04:10:36-root-INFO: grad norm: 9774.254 7917.757 5731.071
2024-12-02-04:10:36-root-INFO: Loss too large (14217.387->15318.858)! Learning rate decreased to 0.00016.
2024-12-02-04:10:37-root-INFO: grad norm: 7142.428 6025.392 3835.223
2024-12-02-04:10:38-root-INFO: grad norm: 5216.509 4307.082 2942.960
2024-12-02-04:10:39-root-INFO: Loss Change: 19171.082 -> 10177.129
2024-12-02-04:10:39-root-INFO: Regularization Change: 0.000 -> 4.236
2024-12-02-04:10:39-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-04:10:39-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:39-root-INFO: step: 239 lr_xt 0.00031912
2024-12-02-04:10:40-root-INFO: grad norm: 4047.963 3437.784 2137.204
2024-12-02-04:10:40-root-INFO: Loss too large (10115.760->13388.004)! Learning rate decreased to 0.00026.
2024-12-02-04:10:40-root-INFO: Loss too large (10115.760->11366.134)! Learning rate decreased to 0.00020.
2024-12-02-04:10:41-root-INFO: Loss too large (10115.760->10235.682)! Learning rate decreased to 0.00016.
2024-12-02-04:10:42-root-INFO: grad norm: 3061.326 2552.240 1690.499
2024-12-02-04:10:43-root-INFO: grad norm: 2369.748 2015.656 1246.128
2024-12-02-04:10:44-root-INFO: grad norm: 1822.243 1534.334 983.052
2024-12-02-04:10:45-root-INFO: grad norm: 1428.617 1217.078 748.109
2024-12-02-04:10:45-root-INFO: Loss Change: 10115.760 -> 8956.882
2024-12-02-04:10:45-root-INFO: Regularization Change: 0.000 -> 0.592
2024-12-02-04:10:45-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-04:10:45-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:46-root-INFO: step: 238 lr_xt 0.00033570
2024-12-02-04:10:46-root-INFO: grad norm: 1137.422 959.361 611.029
2024-12-02-04:10:46-root-INFO: Loss too large (8701.624->8715.237)! Learning rate decreased to 0.00027.
2024-12-02-04:10:47-root-INFO: grad norm: 1471.912 1235.580 799.917
2024-12-02-04:10:48-root-INFO: Loss too large (8641.008->8704.277)! Learning rate decreased to 0.00021.
2024-12-02-04:10:49-root-INFO: grad norm: 1567.202 1343.980 806.126
2024-12-02-04:10:50-root-INFO: grad norm: 1677.811 1418.042 896.776
2024-12-02-04:10:51-root-INFO: grad norm: 1803.312 1553.119 916.382
2024-12-02-04:10:51-root-INFO: Loss Change: 8701.624 -> 8482.167
2024-12-02-04:10:51-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-02-04:10:51-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-04:10:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:52-root-INFO: step: 237 lr_xt 0.00035308
2024-12-02-04:10:52-root-INFO: grad norm: 2056.838 1753.492 1075.103
2024-12-02-04:10:52-root-INFO: Loss too large (8449.498->9174.277)! Learning rate decreased to 0.00028.
2024-12-02-04:10:53-root-INFO: Loss too large (8449.498->8667.757)! Learning rate decreased to 0.00023.
2024-12-02-04:10:53-root-INFO: grad norm: 2101.797 1821.284 1049.036
2024-12-02-04:10:54-root-INFO: grad norm: 2153.341 1837.418 1122.842
2024-12-02-04:10:55-root-INFO: grad norm: 2209.063 1918.300 1095.484
2024-12-02-04:10:56-root-INFO: grad norm: 2256.971 1930.167 1169.775
2024-12-02-04:10:57-root-INFO: Loss Change: 8449.498 -> 8252.195
2024-12-02-04:10:57-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-02-04:10:57-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03503.
2024-12-02-04:10:57-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:10:57-root-INFO: step: 236 lr_xt 0.00037130
2024-12-02-04:10:58-root-INFO: grad norm: 2210.028 1916.243 1101.016
2024-12-02-04:10:58-root-INFO: Loss too large (8108.935->8995.971)! Learning rate decreased to 0.00030.
2024-12-02-04:10:58-root-INFO: Loss too large (8108.935->8401.202)! Learning rate decreased to 0.00024.
2024-12-02-04:10:59-root-INFO: grad norm: 2197.441 1890.919 1119.453
2024-12-02-04:11:00-root-INFO: grad norm: 2182.318 1900.384 1072.872
2024-12-02-04:11:01-root-INFO: grad norm: 2160.920 1859.745 1100.421
2024-12-02-04:11:02-root-INFO: grad norm: 2133.201 1860.196 1044.135
2024-12-02-04:11:03-root-INFO: Loss Change: 8108.935 -> 7880.240
2024-12-02-04:11:03-root-INFO: Regularization Change: 0.000 -> 0.515
2024-12-02-04:11:03-root-INFO: Learning rate of xt decay: 0.03503 -> 0.03545.
2024-12-02-04:11:03-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-04:11:03-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-04:11:04-root-INFO: grad norm: 2249.930 1955.834 1112.159
2024-12-02-04:11:04-root-INFO: Loss too large (7845.898->8760.854)! Learning rate decreased to 0.00031.
2024-12-02-04:11:04-root-INFO: Loss too large (7845.898->8101.370)! Learning rate decreased to 0.00025.
2024-12-02-04:11:05-root-INFO: grad norm: 2101.338 1852.612 991.692
2024-12-02-04:11:06-root-INFO: grad norm: 1980.364 1720.661 980.392
2024-12-02-04:11:07-root-INFO: grad norm: 1861.382 1644.385 872.205
2024-12-02-04:11:08-root-INFO: grad norm: 1759.000 1530.538 866.911
2024-12-02-04:11:09-root-INFO: Loss Change: 7845.898 -> 7502.594
2024-12-02-04:11:09-root-INFO: Regularization Change: 0.000 -> 0.473
2024-12-02-04:11:09-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-04:11:09-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-04:11:09-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-04:11:10-root-INFO: grad norm: 1518.823 1345.709 704.195
2024-12-02-04:11:10-root-INFO: Loss too large (7435.447->7821.938)! Learning rate decreased to 0.00033.
2024-12-02-04:11:10-root-INFO: Loss too large (7435.447->7537.292)! Learning rate decreased to 0.00026.
2024-12-02-04:11:11-root-INFO: grad norm: 1408.302 1231.502 683.167
2024-12-02-04:11:12-root-INFO: grad norm: 1305.424 1163.298 592.343
2024-12-02-04:11:13-root-INFO: grad norm: 1217.382 1067.017 586.084
2024-12-02-04:11:14-root-INFO: grad norm: 1133.338 1013.086 508.047
2024-12-02-04:11:15-root-INFO: Loss Change: 7435.447 -> 7188.450
2024-12-02-04:11:15-root-INFO: Regularization Change: 0.000 -> 0.338
2024-12-02-04:11:15-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-04:11:15-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:11:15-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-04:11:16-root-INFO: grad norm: 1128.334 1005.676 511.618
2024-12-02-04:11:16-root-INFO: Loss too large (7165.832->7313.976)! Learning rate decreased to 0.00035.
2024-12-02-04:11:16-root-INFO: Loss too large (7165.832->7169.847)! Learning rate decreased to 0.00028.
2024-12-02-04:11:17-root-INFO: grad norm: 971.892 870.956 431.288
2024-12-02-04:11:18-root-INFO: grad norm: 869.164 770.673 401.882
2024-12-02-04:11:19-root-INFO: grad norm: 782.746 707.351 335.182
2024-12-02-04:11:20-root-INFO: grad norm: 712.703 634.820 323.959
2024-12-02-04:11:21-root-INFO: Loss Change: 7165.832 -> 6926.229
2024-12-02-04:11:21-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-02-04:11:21-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-04:11:21-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:11:21-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-04:11:22-root-INFO: grad norm: 472.556 414.664 226.633
2024-12-02-04:11:23-root-INFO: grad norm: 469.793 398.175 249.322
2024-12-02-04:11:24-root-INFO: grad norm: 587.855 516.029 281.581
2024-12-02-04:11:25-root-INFO: grad norm: 901.411 779.802 452.163
2024-12-02-04:11:25-root-INFO: Loss too large (6674.377->6766.031)! Learning rate decreased to 0.00036.
2024-12-02-04:11:26-root-INFO: grad norm: 1140.369 1016.646 516.597
2024-12-02-04:11:26-root-INFO: Loss too large (6672.722->6716.717)! Learning rate decreased to 0.00029.
2024-12-02-04:11:27-root-INFO: Loss Change: 6779.095 -> 6626.968
2024-12-02-04:11:27-root-INFO: Regularization Change: 0.000 -> 0.465
2024-12-02-04:11:27-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-04:11:27-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:11:27-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-04:11:28-root-INFO: grad norm: 1118.018 1002.377 495.182
2024-12-02-04:11:28-root-INFO: Loss too large (6583.780->6800.522)! Learning rate decreased to 0.00038.
2024-12-02-04:11:28-root-INFO: Loss too large (6583.780->6624.271)! Learning rate decreased to 0.00030.
2024-12-02-04:11:29-root-INFO: grad norm: 996.614 904.495 418.483
2024-12-02-04:11:30-root-INFO: grad norm: 902.309 810.717 396.106
2024-12-02-04:11:31-root-INFO: grad norm: 816.904 743.843 337.683
2024-12-02-04:11:32-root-INFO: grad norm: 743.788 669.854 323.290
2024-12-02-04:11:33-root-INFO: Loss Change: 6583.780 -> 6386.152
2024-12-02-04:11:33-root-INFO: Regularization Change: 0.000 -> 0.276
2024-12-02-04:11:33-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-04:11:33-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:11:33-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-04:11:34-root-INFO: grad norm: 422.733 396.384 146.910
2024-12-02-04:11:35-root-INFO: grad norm: 659.158 593.800 286.166
2024-12-02-04:11:35-root-INFO: Loss too large (6253.413->6301.146)! Learning rate decreased to 0.00040.
2024-12-02-04:11:36-root-INFO: grad norm: 869.034 792.022 357.660
2024-12-02-04:11:36-root-INFO: Loss too large (6248.531->6271.832)! Learning rate decreased to 0.00032.
2024-12-02-04:11:37-root-INFO: grad norm: 793.495 716.486 341.003
2024-12-02-04:11:38-root-INFO: grad norm: 724.506 661.616 295.251
2024-12-02-04:11:39-root-INFO: Loss Change: 6271.507 -> 6154.002
2024-12-02-04:11:39-root-INFO: Regularization Change: 0.000 -> 0.288
2024-12-02-04:11:39-root-INFO: Undo step: 230
2024-12-02-04:11:39-root-INFO: Undo step: 231
2024-12-02-04:11:39-root-INFO: Undo step: 232
2024-12-02-04:11:39-root-INFO: Undo step: 233
2024-12-02-04:11:39-root-INFO: Undo step: 234
2024-12-02-04:11:39-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-04:11:40-root-INFO: grad norm: 9554.554 7859.338 5433.259
2024-12-02-04:11:40-root-INFO: Loss too large (17778.543->23436.160)! Learning rate decreased to 0.00031.
2024-12-02-04:11:41-root-INFO: grad norm: 10411.573 8796.496 5569.786
2024-12-02-04:11:41-root-INFO: Loss too large (15183.030->19910.848)! Learning rate decreased to 0.00025.
2024-12-02-04:11:42-root-INFO: grad norm: 8635.250 7733.098 3842.750
2024-12-02-04:11:43-root-INFO: grad norm: 6999.010 6064.688 3493.668
2024-12-02-04:11:44-root-INFO: grad norm: 6535.491 5830.708 2952.200
2024-12-02-04:11:45-root-INFO: Loss Change: 17778.543 -> 9357.820
2024-12-02-04:11:45-root-INFO: Regularization Change: 0.000 -> 8.208
2024-12-02-04:11:45-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-04:11:45-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-04:11:45-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-04:11:46-root-INFO: grad norm: 5553.651 4885.025 2641.887
2024-12-02-04:11:46-root-INFO: Loss too large (9076.917->14552.435)! Learning rate decreased to 0.00033.
2024-12-02-04:11:46-root-INFO: Loss too large (9076.917->11198.377)! Learning rate decreased to 0.00026.
2024-12-02-04:11:47-root-INFO: grad norm: 5203.463 4631.672 2371.422
2024-12-02-04:11:48-root-INFO: grad norm: 4770.226 4201.890 2258.135
2024-12-02-04:11:49-root-INFO: grad norm: 4450.404 3947.695 2054.702
2024-12-02-04:11:50-root-INFO: grad norm: 4090.267 3608.283 1926.285
2024-12-02-04:11:51-root-INFO: Loss Change: 9076.917 -> 7811.592
2024-12-02-04:11:51-root-INFO: Regularization Change: 0.000 -> 2.087
2024-12-02-04:11:51-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-04:11:51-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:11:51-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-04:11:52-root-INFO: grad norm: 3758.910 3357.852 1689.448
2024-12-02-04:11:52-root-INFO: Loss too large (7846.554->10519.553)! Learning rate decreased to 0.00035.
2024-12-02-04:11:52-root-INFO: Loss too large (7846.554->8505.355)! Learning rate decreased to 0.00028.
2024-12-02-04:11:53-root-INFO: grad norm: 3298.905 2925.153 1525.207
2024-12-02-04:11:54-root-INFO: grad norm: 3001.781 2668.277 1375.132
2024-12-02-04:11:55-root-INFO: grad norm: 2708.509 2404.543 1246.672
2024-12-02-04:11:56-root-INFO: grad norm: 2486.009 2206.596 1145.065
2024-12-02-04:11:57-root-INFO: Loss Change: 7846.554 -> 6877.226
2024-12-02-04:11:57-root-INFO: Regularization Change: 0.000 -> 1.095
2024-12-02-04:11:57-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-04:11:57-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:11:57-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-04:11:57-root-INFO: grad norm: 1635.278 1462.222 732.149
2024-12-02-04:11:58-root-INFO: Loss too large (6570.644->7070.105)! Learning rate decreased to 0.00036.
2024-12-02-04:11:58-root-INFO: Loss too large (6570.644->6705.304)! Learning rate decreased to 0.00029.
2024-12-02-04:11:59-root-INFO: grad norm: 1484.565 1322.978 673.544
2024-12-02-04:12:00-root-INFO: grad norm: 1360.031 1224.734 591.362
2024-12-02-04:12:01-root-INFO: grad norm: 1257.066 1121.917 567.025
2024-12-02-04:12:02-root-INFO: grad norm: 1159.778 1044.671 503.732
2024-12-02-04:12:03-root-INFO: Loss Change: 6570.644 -> 6309.820
2024-12-02-04:12:03-root-INFO: Regularization Change: 0.000 -> 0.377
2024-12-02-04:12:03-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-04:12:03-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:12:03-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-04:12:03-root-INFO: grad norm: 1215.132 1088.728 539.647
2024-12-02-04:12:04-root-INFO: Loss too large (6239.780->6527.847)! Learning rate decreased to 0.00038.
2024-12-02-04:12:04-root-INFO: Loss too large (6239.780->6308.809)! Learning rate decreased to 0.00030.
2024-12-02-04:12:05-root-INFO: grad norm: 1105.081 998.985 472.475
2024-12-02-04:12:06-root-INFO: grad norm: 1011.260 907.054 447.103
2024-12-02-04:12:07-root-INFO: grad norm: 923.320 835.806 392.363
2024-12-02-04:12:08-root-INFO: grad norm: 847.033 760.300 373.376
2024-12-02-04:12:09-root-INFO: Loss Change: 6239.780 -> 6055.670
2024-12-02-04:12:09-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-04:12:09-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-04:12:09-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:12:09-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-04:12:09-root-INFO: grad norm: 455.026 419.692 175.806
2024-12-02-04:12:10-root-INFO: Loss too large (5935.605->5936.527)! Learning rate decreased to 0.00040.
2024-12-02-04:12:11-root-INFO: grad norm: 569.175 514.443 243.534
2024-12-02-04:12:12-root-INFO: grad norm: 744.741 678.552 306.931
2024-12-02-04:12:12-root-INFO: Loss too large (5913.235->5926.264)! Learning rate decreased to 0.00032.
2024-12-02-04:12:13-root-INFO: grad norm: 678.318 611.842 292.856
2024-12-02-04:12:14-root-INFO: grad norm: 617.566 564.096 251.363
2024-12-02-04:12:15-root-INFO: Loss Change: 5935.605 -> 5836.367
2024-12-02-04:12:15-root-INFO: Regularization Change: 0.000 -> 0.216
2024-12-02-04:12:15-root-INFO: Undo step: 230
2024-12-02-04:12:15-root-INFO: Undo step: 231
2024-12-02-04:12:15-root-INFO: Undo step: 232
2024-12-02-04:12:15-root-INFO: Undo step: 233
2024-12-02-04:12:15-root-INFO: Undo step: 234
2024-12-02-04:12:15-root-INFO: step: 235 lr_xt 0.00039040
2024-12-02-04:12:15-root-INFO: grad norm: 2387.184 1787.441 1582.310
2024-12-02-04:12:16-root-INFO: Loss too large (7735.349->7766.724)! Learning rate decreased to 0.00031.
2024-12-02-04:12:17-root-INFO: grad norm: 1974.100 1432.805 1357.992
2024-12-02-04:12:18-root-INFO: grad norm: 1987.280 1444.753 1364.540
2024-12-02-04:12:19-root-INFO: grad norm: 2069.244 1499.379 1426.055
2024-12-02-04:12:20-root-INFO: grad norm: 2183.871 1590.627 1496.395
2024-12-02-04:12:20-root-INFO: Loss Change: 7735.349 -> 7215.046
2024-12-02-04:12:20-root-INFO: Regularization Change: 0.000 -> 1.125
2024-12-02-04:12:20-root-INFO: Learning rate of xt decay: 0.03545 -> 0.03588.
2024-12-02-04:12:20-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-04:12:21-root-INFO: step: 234 lr_xt 0.00041042
2024-12-02-04:12:21-root-INFO: grad norm: 2234.208 1651.788 1504.420
2024-12-02-04:12:21-root-INFO: Loss too large (7151.966->7612.064)! Learning rate decreased to 0.00033.
2024-12-02-04:12:22-root-INFO: grad norm: 2332.254 1758.761 1531.720
2024-12-02-04:12:23-root-INFO: Loss too large (7119.666->7136.075)! Learning rate decreased to 0.00026.
2024-12-02-04:12:24-root-INFO: grad norm: 1608.836 1276.958 978.638
2024-12-02-04:12:25-root-INFO: grad norm: 1234.664 1025.666 687.317
2024-12-02-04:12:26-root-INFO: grad norm: 1058.000 912.444 535.547
2024-12-02-04:12:26-root-INFO: Loss Change: 7151.966 -> 6603.009
2024-12-02-04:12:26-root-INFO: Regularization Change: 0.000 -> 0.490
2024-12-02-04:12:26-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-04:12:26-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:12:27-root-INFO: step: 233 lr_xt 0.00043139
2024-12-02-04:12:27-root-INFO: grad norm: 744.039 639.633 380.084
2024-12-02-04:12:27-root-INFO: Loss too large (6513.380->6524.667)! Learning rate decreased to 0.00035.
2024-12-02-04:12:28-root-INFO: grad norm: 905.571 801.486 421.520
2024-12-02-04:12:29-root-INFO: Loss too large (6482.615->6488.936)! Learning rate decreased to 0.00028.
2024-12-02-04:12:30-root-INFO: grad norm: 840.242 739.762 398.446
2024-12-02-04:12:30-root-INFO: grad norm: 784.973 699.659 355.893
2024-12-02-04:12:32-root-INFO: grad norm: 732.664 647.911 342.063
2024-12-02-04:12:32-root-INFO: Loss Change: 6513.380 -> 6339.725
2024-12-02-04:12:32-root-INFO: Regularization Change: 0.000 -> 0.279
2024-12-02-04:12:32-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03674.
2024-12-02-04:12:32-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:12:33-root-INFO: step: 232 lr_xt 0.00045336
2024-12-02-04:12:33-root-INFO: grad norm: 1233.748 1085.214 586.896
2024-12-02-04:12:33-root-INFO: Loss too large (6293.906->6535.169)! Learning rate decreased to 0.00036.
2024-12-02-04:12:34-root-INFO: Loss too large (6293.906->6334.896)! Learning rate decreased to 0.00029.
2024-12-02-04:12:35-root-INFO: grad norm: 1093.367 970.826 502.940
2024-12-02-04:12:36-root-INFO: grad norm: 997.887 884.493 462.008
2024-12-02-04:12:37-root-INFO: grad norm: 911.917 812.727 413.604
2024-12-02-04:12:38-root-INFO: grad norm: 838.342 745.673 383.130
2024-12-02-04:12:38-root-INFO: Loss Change: 6293.906 -> 6084.197
2024-12-02-04:12:38-root-INFO: Regularization Change: 0.000 -> 0.285
2024-12-02-04:12:38-root-INFO: Learning rate of xt decay: 0.03674 -> 0.03719.
2024-12-02-04:12:38-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:12:39-root-INFO: step: 231 lr_xt 0.00047637
2024-12-02-04:12:39-root-INFO: grad norm: 558.248 503.643 240.800
2024-12-02-04:12:39-root-INFO: Loss too large (5993.304->6010.875)! Learning rate decreased to 0.00038.
2024-12-02-04:12:40-root-INFO: grad norm: 709.218 633.684 318.490
2024-12-02-04:12:41-root-INFO: Loss too large (5981.007->5983.303)! Learning rate decreased to 0.00030.
2024-12-02-04:12:42-root-INFO: grad norm: 640.288 576.001 279.628
2024-12-02-04:12:43-root-INFO: grad norm: 581.408 521.673 256.696
2024-12-02-04:12:44-root-INFO: grad norm: 528.742 478.199 225.597
2024-12-02-04:12:44-root-INFO: Loss Change: 5993.304 -> 5877.626
2024-12-02-04:12:44-root-INFO: Regularization Change: 0.000 -> 0.199
2024-12-02-04:12:44-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03763.
2024-12-02-04:12:44-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:12:45-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-04:12:45-root-INFO: grad norm: 778.400 692.657 355.152
2024-12-02-04:12:45-root-INFO: Loss too large (5827.905->5917.144)! Learning rate decreased to 0.00040.
2024-12-02-04:12:46-root-INFO: Loss too large (5827.905->5835.654)! Learning rate decreased to 0.00032.
2024-12-02-04:12:46-root-INFO: grad norm: 676.909 610.160 293.105
2024-12-02-04:12:47-root-INFO: grad norm: 597.048 533.750 267.541
2024-12-02-04:12:48-root-INFO: grad norm: 527.821 478.564 222.645
2024-12-02-04:12:49-root-INFO: grad norm: 470.604 423.392 205.445
2024-12-02-04:12:50-root-INFO: Loss Change: 5827.905 -> 5694.695
2024-12-02-04:12:50-root-INFO: Regularization Change: 0.000 -> 0.191
2024-12-02-04:12:50-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-04:12:50-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:12:50-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-04:12:51-root-INFO: grad norm: 309.112 288.250 111.632
2024-12-02-04:12:52-root-INFO: grad norm: 305.320 279.911 121.944
2024-12-02-04:12:53-root-INFO: grad norm: 389.678 354.935 160.842
2024-12-02-04:12:54-root-INFO: grad norm: 633.476 571.397 273.492
2024-12-02-04:12:54-root-INFO: Loss too large (5573.970->5630.776)! Learning rate decreased to 0.00042.
2024-12-02-04:12:54-root-INFO: Loss too large (5573.970->5577.139)! Learning rate decreased to 0.00034.
2024-12-02-04:12:55-root-INFO: grad norm: 559.631 502.776 245.772
2024-12-02-04:12:56-root-INFO: Loss Change: 5644.174 -> 5526.188
2024-12-02-04:12:56-root-INFO: Regularization Change: 0.000 -> 0.310
2024-12-02-04:12:56-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-04:12:56-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:12:56-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-04:12:57-root-INFO: grad norm: 356.531 308.757 178.280
2024-12-02-04:12:58-root-INFO: grad norm: 475.717 416.981 228.985
2024-12-02-04:12:58-root-INFO: Loss too large (5457.770->5466.334)! Learning rate decreased to 0.00044.
2024-12-02-04:12:59-root-INFO: grad norm: 548.683 490.132 246.625
2024-12-02-04:13:00-root-INFO: grad norm: 671.602 599.879 301.984
2024-12-02-04:13:01-root-INFO: Loss too large (5438.033->5443.188)! Learning rate decreased to 0.00035.
2024-12-02-04:13:02-root-INFO: grad norm: 568.186 513.008 244.251
2024-12-02-04:13:02-root-INFO: Loss Change: 5475.496 -> 5386.226
2024-12-02-04:13:02-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-02-04:13:02-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-04:13:02-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:13:03-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-04:13:03-root-INFO: grad norm: 937.048 835.377 424.505
2024-12-02-04:13:03-root-INFO: Loss too large (5434.162->5587.249)! Learning rate decreased to 0.00046.
2024-12-02-04:13:04-root-INFO: Loss too large (5434.162->5448.285)! Learning rate decreased to 0.00037.
2024-12-02-04:13:05-root-INFO: grad norm: 761.853 687.101 329.110
2024-12-02-04:13:06-root-INFO: grad norm: 637.538 572.028 281.494
2024-12-02-04:13:07-root-INFO: grad norm: 535.474 485.051 226.844
2024-12-02-04:13:08-root-INFO: grad norm: 458.353 413.775 197.174
2024-12-02-04:13:08-root-INFO: Loss Change: 5434.162 -> 5263.388
2024-12-02-04:13:08-root-INFO: Regularization Change: 0.000 -> 0.245
2024-12-02-04:13:08-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-04:13:08-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-04:13:09-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-04:13:09-root-INFO: grad norm: 430.738 373.320 214.867
2024-12-02-04:13:10-root-INFO: grad norm: 613.685 547.746 276.738
2024-12-02-04:13:10-root-INFO: Loss too large (5190.609->5239.397)! Learning rate decreased to 0.00049.
2024-12-02-04:13:11-root-INFO: grad norm: 723.100 649.731 317.368
2024-12-02-04:13:12-root-INFO: Loss too large (5184.614->5194.461)! Learning rate decreased to 0.00039.
2024-12-02-04:13:13-root-INFO: grad norm: 586.155 526.703 257.219
2024-12-02-04:13:13-root-INFO: grad norm: 479.518 434.623 202.585
2024-12-02-04:13:14-root-INFO: Loss Change: 5196.961 -> 5101.288
2024-12-02-04:13:14-root-INFO: Regularization Change: 0.000 -> 0.232
2024-12-02-04:13:14-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-04:13:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:13:14-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-04:13:15-root-INFO: grad norm: 845.704 750.837 389.178
2024-12-02-04:13:15-root-INFO: Loss too large (5111.444->5236.598)! Learning rate decreased to 0.00051.
2024-12-02-04:13:15-root-INFO: Loss too large (5111.444->5115.833)! Learning rate decreased to 0.00041.
2024-12-02-04:13:16-root-INFO: grad norm: 658.821 596.610 279.467
2024-12-02-04:13:17-root-INFO: grad norm: 534.484 478.329 238.484
2024-12-02-04:13:18-root-INFO: grad norm: 436.959 397.583 181.277
2024-12-02-04:13:19-root-INFO: grad norm: 366.028 330.643 157.009
2024-12-02-04:13:20-root-INFO: Loss Change: 5111.444 -> 4952.455
2024-12-02-04:13:20-root-INFO: Regularization Change: 0.000 -> 0.239
2024-12-02-04:13:20-root-INFO: Undo step: 225
2024-12-02-04:13:20-root-INFO: Undo step: 226
2024-12-02-04:13:20-root-INFO: Undo step: 227
2024-12-02-04:13:20-root-INFO: Undo step: 228
2024-12-02-04:13:20-root-INFO: Undo step: 229
2024-12-02-04:13:20-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-04:13:21-root-INFO: grad norm: 6901.866 5693.838 3900.763
2024-12-02-04:13:21-root-INFO: Loss too large (11943.996->12568.114)! Learning rate decreased to 0.00040.
2024-12-02-04:13:22-root-INFO: grad norm: 4540.941 3976.440 2192.731
2024-12-02-04:13:23-root-INFO: grad norm: 3517.604 3088.064 1684.459
2024-12-02-04:13:23-root-INFO: Loss too large (7729.330->7835.069)! Learning rate decreased to 0.00032.
2024-12-02-04:13:24-root-INFO: grad norm: 2516.860 2205.996 1211.680
2024-12-02-04:13:25-root-INFO: grad norm: 1858.678 1655.760 844.477
2024-12-02-04:13:26-root-INFO: Loss Change: 11943.996 -> 6416.037
2024-12-02-04:13:26-root-INFO: Regularization Change: 0.000 -> 3.899
2024-12-02-04:13:26-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-04:13:26-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:13:26-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-04:13:27-root-INFO: grad norm: 1587.013 1414.821 718.951
2024-12-02-04:13:27-root-INFO: Loss too large (6399.630->6693.752)! Learning rate decreased to 0.00042.
2024-12-02-04:13:28-root-INFO: grad norm: 1849.770 1665.352 805.142
2024-12-02-04:13:28-root-INFO: Loss too large (6360.722->6453.209)! Learning rate decreased to 0.00034.
2024-12-02-04:13:29-root-INFO: grad norm: 1478.122 1312.673 679.510
2024-12-02-04:13:30-root-INFO: grad norm: 1177.566 1063.787 504.995
2024-12-02-04:13:31-root-INFO: grad norm: 973.486 864.965 446.666
2024-12-02-04:13:32-root-INFO: Loss Change: 6399.630 -> 5901.402
2024-12-02-04:13:32-root-INFO: Regularization Change: 0.000 -> 0.738
2024-12-02-04:13:32-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-04:13:32-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:13:32-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-04:13:33-root-INFO: grad norm: 673.360 606.345 292.847
2024-12-02-04:13:33-root-INFO: Loss too large (5830.118->5834.997)! Learning rate decreased to 0.00044.
2024-12-02-04:13:34-root-INFO: grad norm: 762.949 674.830 355.943
2024-12-02-04:13:35-root-INFO: grad norm: 918.258 830.953 390.787
2024-12-02-04:13:35-root-INFO: Loss too large (5772.739->5777.417)! Learning rate decreased to 0.00035.
2024-12-02-04:13:36-root-INFO: grad norm: 763.570 679.256 348.784
2024-12-02-04:13:37-root-INFO: grad norm: 638.952 583.542 260.265
2024-12-02-04:13:38-root-INFO: Loss Change: 5830.118 -> 5630.130
2024-12-02-04:13:38-root-INFO: Regularization Change: 0.000 -> 0.409
2024-12-02-04:13:38-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-04:13:38-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:13:38-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-04:13:39-root-INFO: grad norm: 900.999 803.050 408.547
2024-12-02-04:13:39-root-INFO: Loss too large (5671.938->5748.795)! Learning rate decreased to 0.00046.
2024-12-02-04:13:40-root-INFO: grad norm: 1042.449 950.965 427.043
2024-12-02-04:13:40-root-INFO: Loss too large (5642.603->5661.988)! Learning rate decreased to 0.00037.
2024-12-02-04:13:41-root-INFO: grad norm: 850.679 758.990 384.173
2024-12-02-04:13:42-root-INFO: grad norm: 697.388 637.650 282.405
2024-12-02-04:13:43-root-INFO: grad norm: 586.527 523.944 263.624
2024-12-02-04:13:44-root-INFO: Loss Change: 5671.938 -> 5444.318
2024-12-02-04:13:44-root-INFO: Regularization Change: 0.000 -> 0.397
2024-12-02-04:13:44-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-04:13:44-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-04:13:44-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-04:13:45-root-INFO: grad norm: 508.203 452.567 231.200
2024-12-02-04:13:46-root-INFO: grad norm: 764.106 682.291 344.003
2024-12-02-04:13:46-root-INFO: Loss too large (5380.657->5463.763)! Learning rate decreased to 0.00049.
2024-12-02-04:13:47-root-INFO: grad norm: 906.221 821.886 381.757
2024-12-02-04:13:47-root-INFO: Loss too large (5375.955->5393.563)! Learning rate decreased to 0.00039.
2024-12-02-04:13:48-root-INFO: grad norm: 725.110 648.007 325.379
2024-12-02-04:13:49-root-INFO: grad norm: 584.164 533.417 238.147
2024-12-02-04:13:50-root-INFO: Loss Change: 5382.043 -> 5251.037
2024-12-02-04:13:50-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-02-04:13:50-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-04:13:50-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:13:50-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-04:13:51-root-INFO: grad norm: 879.973 783.810 399.993
2024-12-02-04:13:51-root-INFO: Loss too large (5265.102->5369.014)! Learning rate decreased to 0.00051.
2024-12-02-04:13:52-root-INFO: grad norm: 1014.082 926.167 413.008
2024-12-02-04:13:52-root-INFO: Loss too large (5249.248->5279.889)! Learning rate decreased to 0.00041.
2024-12-02-04:13:53-root-INFO: grad norm: 807.783 721.606 363.040
2024-12-02-04:13:54-root-INFO: grad norm: 642.705 588.572 258.172
2024-12-02-04:13:55-root-INFO: grad norm: 524.330 468.784 234.869
2024-12-02-04:13:56-root-INFO: Loss Change: 5265.102 -> 5071.833
2024-12-02-04:13:56-root-INFO: Regularization Change: 0.000 -> 0.344
2024-12-02-04:13:56-root-INFO: Undo step: 225
2024-12-02-04:13:56-root-INFO: Undo step: 226
2024-12-02-04:13:56-root-INFO: Undo step: 227
2024-12-02-04:13:56-root-INFO: Undo step: 228
2024-12-02-04:13:56-root-INFO: Undo step: 229
2024-12-02-04:13:56-root-INFO: step: 230 lr_xt 0.00050047
2024-12-02-04:13:57-root-INFO: grad norm: 5030.048 4297.331 2614.256
2024-12-02-04:13:57-root-INFO: Loss too large (9628.563->10922.991)! Learning rate decreased to 0.00040.
2024-12-02-04:13:58-root-INFO: grad norm: 4182.740 3714.273 1923.404
2024-12-02-04:13:59-root-INFO: grad norm: 4149.015 3585.914 2086.994
2024-12-02-04:13:59-root-INFO: Loss too large (8054.583->8611.311)! Learning rate decreased to 0.00032.
2024-12-02-04:14:00-root-INFO: grad norm: 3297.990 2927.875 1517.987
2024-12-02-04:14:01-root-INFO: grad norm: 2652.339 2312.588 1298.783
2024-12-02-04:14:02-root-INFO: Loss Change: 9628.563 -> 6537.247
2024-12-02-04:14:02-root-INFO: Regularization Change: 0.000 -> 3.650
2024-12-02-04:14:02-root-INFO: Learning rate of xt decay: 0.03763 -> 0.03808.
2024-12-02-04:14:02-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:14:02-root-INFO: step: 229 lr_xt 0.00052570
2024-12-02-04:14:03-root-INFO: grad norm: 2411.863 2166.815 1059.243
2024-12-02-04:14:03-root-INFO: Loss too large (6495.333->7522.230)! Learning rate decreased to 0.00042.
2024-12-02-04:14:03-root-INFO: Loss too large (6495.333->6627.777)! Learning rate decreased to 0.00034.
2024-12-02-04:14:04-root-INFO: grad norm: 1933.040 1708.048 905.104
2024-12-02-04:14:05-root-INFO: grad norm: 1639.827 1472.389 721.875
2024-12-02-04:14:06-root-INFO: grad norm: 1388.381 1228.797 646.266
2024-12-02-04:14:07-root-INFO: grad norm: 1201.237 1079.908 526.089
2024-12-02-04:14:08-root-INFO: Loss Change: 6495.333 -> 5702.867
2024-12-02-04:14:08-root-INFO: Regularization Change: 0.000 -> 0.945
2024-12-02-04:14:08-root-INFO: Learning rate of xt decay: 0.03808 -> 0.03854.
2024-12-02-04:14:08-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:14:08-root-INFO: step: 228 lr_xt 0.00055211
2024-12-02-04:14:09-root-INFO: grad norm: 835.278 729.466 406.901
2024-12-02-04:14:09-root-INFO: Loss too large (5616.824->5683.760)! Learning rate decreased to 0.00044.
2024-12-02-04:14:10-root-INFO: grad norm: 1003.677 899.389 445.496
2024-12-02-04:14:10-root-INFO: Loss too large (5598.938->5611.439)! Learning rate decreased to 0.00035.
2024-12-02-04:14:11-root-INFO: grad norm: 845.138 750.753 388.108
2024-12-02-04:14:12-root-INFO: grad norm: 725.621 653.424 315.536
2024-12-02-04:14:13-root-INFO: grad norm: 625.206 557.611 282.758
2024-12-02-04:14:14-root-INFO: Loss Change: 5616.824 -> 5414.269
2024-12-02-04:14:14-root-INFO: Regularization Change: 0.000 -> 0.372
2024-12-02-04:14:14-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03900.
2024-12-02-04:14:14-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-04:14:14-root-INFO: step: 227 lr_xt 0.00057976
2024-12-02-04:14:14-root-INFO: grad norm: 969.164 870.174 426.702
2024-12-02-04:14:15-root-INFO: Loss too large (5429.159->5579.903)! Learning rate decreased to 0.00046.
2024-12-02-04:14:15-root-INFO: Loss too large (5429.159->5434.649)! Learning rate decreased to 0.00037.
2024-12-02-04:14:16-root-INFO: grad norm: 778.722 700.943 339.245
2024-12-02-04:14:17-root-INFO: grad norm: 659.897 594.639 286.127
2024-12-02-04:14:18-root-INFO: grad norm: 562.662 507.366 243.244
2024-12-02-04:14:19-root-INFO: grad norm: 487.644 440.573 209.027
2024-12-02-04:14:20-root-INFO: Loss Change: 5429.159 -> 5234.906
2024-12-02-04:14:20-root-INFO: Regularization Change: 0.000 -> 0.284
2024-12-02-04:14:20-root-INFO: Learning rate of xt decay: 0.03900 -> 0.03947.
2024-12-02-04:14:20-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-04:14:20-root-INFO: step: 226 lr_xt 0.00060869
2024-12-02-04:14:20-root-INFO: grad norm: 449.295 390.661 221.924
2024-12-02-04:14:21-root-INFO: grad norm: 661.076 592.852 292.486
2024-12-02-04:14:22-root-INFO: Loss too large (5190.202->5249.778)! Learning rate decreased to 0.00049.
2024-12-02-04:14:23-root-INFO: grad norm: 792.469 713.368 345.129
2024-12-02-04:14:23-root-INFO: Loss too large (5184.783->5200.132)! Learning rate decreased to 0.00039.
2024-12-02-04:14:24-root-INFO: grad norm: 651.172 587.066 281.740
2024-12-02-04:14:25-root-INFO: grad norm: 538.465 487.297 229.099
2024-12-02-04:14:26-root-INFO: Loss Change: 5195.593 -> 5088.435
2024-12-02-04:14:26-root-INFO: Regularization Change: 0.000 -> 0.272
2024-12-02-04:14:26-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03994.
2024-12-02-04:14:26-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:14:26-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-04:14:27-root-INFO: grad norm: 903.940 808.759 403.752
2024-12-02-04:14:27-root-INFO: Loss too large (5104.745->5243.694)! Learning rate decreased to 0.00051.
2024-12-02-04:14:27-root-INFO: Loss too large (5104.745->5105.863)! Learning rate decreased to 0.00041.
2024-12-02-04:14:28-root-INFO: grad norm: 702.910 640.707 289.097
2024-12-02-04:14:29-root-INFO: grad norm: 582.257 523.480 254.935
2024-12-02-04:14:30-root-INFO: grad norm: 485.507 443.622 197.274
2024-12-02-04:14:31-root-INFO: grad norm: 414.013 374.067 177.429
2024-12-02-04:14:32-root-INFO: Loss Change: 5104.745 -> 4923.949
2024-12-02-04:14:32-root-INFO: Regularization Change: 0.000 -> 0.275
2024-12-02-04:14:32-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-04:14:32-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:14:32-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-04:14:33-root-INFO: grad norm: 285.437 255.462 127.330
2024-12-02-04:14:34-root-INFO: grad norm: 236.907 211.899 105.942
2024-12-02-04:14:35-root-INFO: grad norm: 214.866 197.264 85.172
2024-12-02-04:14:36-root-INFO: grad norm: 204.534 187.411 81.924
2024-12-02-04:14:37-root-INFO: grad norm: 202.060 186.813 77.002
2024-12-02-04:14:37-root-INFO: Loss Change: 4897.083 -> 4747.618
2024-12-02-04:14:37-root-INFO: Regularization Change: 0.000 -> 0.493
2024-12-02-04:14:37-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-04:14:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:14:38-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-04:14:38-root-INFO: grad norm: 351.472 313.004 159.880
2024-12-02-04:14:39-root-INFO: grad norm: 491.940 446.135 207.290
2024-12-02-04:14:39-root-INFO: Loss too large (4714.604->4744.378)! Learning rate decreased to 0.00056.
2024-12-02-04:14:40-root-INFO: grad norm: 592.984 537.311 250.853
2024-12-02-04:14:41-root-INFO: Loss too large (4707.142->4710.113)! Learning rate decreased to 0.00045.
2024-12-02-04:14:42-root-INFO: grad norm: 498.688 455.652 202.658
2024-12-02-04:14:43-root-INFO: grad norm: 428.770 390.477 177.120
2024-12-02-04:14:43-root-INFO: Loss Change: 4733.228 -> 4632.452
2024-12-02-04:14:43-root-INFO: Regularization Change: 0.000 -> 0.290
2024-12-02-04:14:43-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-04:14:43-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:14:44-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-04:14:44-root-INFO: grad norm: 294.592 270.241 117.279
2024-12-02-04:14:45-root-INFO: grad norm: 449.122 409.764 183.858
2024-12-02-04:14:45-root-INFO: Loss too large (4541.900->4570.271)! Learning rate decreased to 0.00059.
2024-12-02-04:14:46-root-INFO: grad norm: 554.821 506.173 227.191
2024-12-02-04:14:47-root-INFO: Loss too large (4535.769->4540.948)! Learning rate decreased to 0.00047.
2024-12-02-04:14:48-root-INFO: grad norm: 467.677 426.907 190.977
2024-12-02-04:14:49-root-INFO: grad norm: 395.101 361.613 159.187
2024-12-02-04:14:49-root-INFO: Loss Change: 4551.430 -> 4468.143
2024-12-02-04:14:49-root-INFO: Regularization Change: 0.000 -> 0.260
2024-12-02-04:14:49-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-04:14:49-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:14:50-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-04:14:50-root-INFO: grad norm: 486.915 441.291 205.789
2024-12-02-04:14:50-root-INFO: Loss too large (4443.328->4476.000)! Learning rate decreased to 0.00062.
2024-12-02-04:14:51-root-INFO: grad norm: 569.087 518.547 234.456
2024-12-02-04:14:52-root-INFO: Loss too large (4434.048->4436.826)! Learning rate decreased to 0.00050.
2024-12-02-04:14:53-root-INFO: grad norm: 456.040 415.722 187.479
2024-12-02-04:14:54-root-INFO: grad norm: 367.377 336.638 147.108
2024-12-02-04:14:55-root-INFO: grad norm: 305.128 279.901 121.482
2024-12-02-04:14:55-root-INFO: Loss Change: 4443.328 -> 4341.770
2024-12-02-04:14:55-root-INFO: Regularization Change: 0.000 -> 0.235
2024-12-02-04:14:55-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-04:14:55-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:14:56-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-04:14:56-root-INFO: grad norm: 293.259 259.962 135.722
2024-12-02-04:14:57-root-INFO: grad norm: 332.392 303.014 136.626
2024-12-02-04:14:58-root-INFO: grad norm: 495.644 450.374 206.943
2024-12-02-04:14:58-root-INFO: Loss too large (4284.850->4326.388)! Learning rate decreased to 0.00065.
2024-12-02-04:14:59-root-INFO: grad norm: 586.381 535.877 238.074
2024-12-02-04:15:00-root-INFO: Loss too large (4278.532->4284.741)! Learning rate decreased to 0.00052.
2024-12-02-04:15:01-root-INFO: grad norm: 466.653 426.221 190.003
2024-12-02-04:15:01-root-INFO: Loss Change: 4321.905 -> 4221.448
2024-12-02-04:15:01-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-04:15:01-root-INFO: Undo step: 220
2024-12-02-04:15:01-root-INFO: Undo step: 221
2024-12-02-04:15:01-root-INFO: Undo step: 222
2024-12-02-04:15:01-root-INFO: Undo step: 223
2024-12-02-04:15:01-root-INFO: Undo step: 224
2024-12-02-04:15:02-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-04:15:02-root-INFO: grad norm: 3814.987 2884.501 2496.754
2024-12-02-04:15:03-root-INFO: grad norm: 2563.101 2046.988 1542.507
2024-12-02-04:15:04-root-INFO: grad norm: 2061.219 1655.577 1227.881
2024-12-02-04:15:05-root-INFO: grad norm: 1873.878 1590.517 990.794
2024-12-02-04:15:06-root-INFO: grad norm: 2074.352 1794.190 1041.066
2024-12-02-04:15:06-root-INFO: Loss too large (5461.634->5995.949)! Learning rate decreased to 0.00051.
2024-12-02-04:15:07-root-INFO: Loss Change: 8852.032 -> 5424.511
2024-12-02-04:15:07-root-INFO: Regularization Change: 0.000 -> 6.349
2024-12-02-04:15:07-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-04:15:07-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:15:07-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-04:15:08-root-INFO: grad norm: 2236.345 2000.352 999.916
2024-12-02-04:15:08-root-INFO: Loss too large (5517.389->6503.207)! Learning rate decreased to 0.00054.
2024-12-02-04:15:08-root-INFO: Loss too large (5517.389->5542.756)! Learning rate decreased to 0.00043.
2024-12-02-04:15:09-root-INFO: grad norm: 1612.301 1465.887 671.334
2024-12-02-04:15:10-root-INFO: grad norm: 1363.082 1223.929 599.992
2024-12-02-04:15:11-root-INFO: grad norm: 1142.071 1039.448 473.153
2024-12-02-04:15:12-root-INFO: grad norm: 987.603 886.343 435.609
2024-12-02-04:15:13-root-INFO: Loss Change: 5517.389 -> 4747.265
2024-12-02-04:15:13-root-INFO: Regularization Change: 0.000 -> 0.912
2024-12-02-04:15:13-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-04:15:13-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:15:13-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-04:15:14-root-INFO: grad norm: 570.636 528.083 216.228
2024-12-02-04:15:14-root-INFO: Loss too large (4663.111->4694.927)! Learning rate decreased to 0.00056.
2024-12-02-04:15:15-root-INFO: grad norm: 676.089 602.053 307.618
2024-12-02-04:15:15-root-INFO: Loss too large (4647.169->4656.573)! Learning rate decreased to 0.00045.
2024-12-02-04:15:16-root-INFO: grad norm: 589.856 540.139 237.025
2024-12-02-04:15:17-root-INFO: grad norm: 525.163 470.025 234.249
2024-12-02-04:15:18-root-INFO: grad norm: 468.687 429.053 188.630
2024-12-02-04:15:19-root-INFO: Loss Change: 4663.111 -> 4538.461
2024-12-02-04:15:19-root-INFO: Regularization Change: 0.000 -> 0.302
2024-12-02-04:15:19-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-04:15:19-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:15:19-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-04:15:19-root-INFO: grad norm: 496.232 445.575 218.425
2024-12-02-04:15:20-root-INFO: Loss too large (4471.970->4514.979)! Learning rate decreased to 0.00059.
2024-12-02-04:15:21-root-INFO: grad norm: 639.499 583.673 261.313
2024-12-02-04:15:21-root-INFO: Loss too large (4469.851->4488.555)! Learning rate decreased to 0.00047.
2024-12-02-04:15:22-root-INFO: grad norm: 564.663 509.442 243.542
2024-12-02-04:15:23-root-INFO: grad norm: 498.423 456.076 201.050
2024-12-02-04:15:24-root-INFO: grad norm: 444.659 401.831 190.402
2024-12-02-04:15:25-root-INFO: Loss Change: 4471.970 -> 4378.607
2024-12-02-04:15:25-root-INFO: Regularization Change: 0.000 -> 0.248
2024-12-02-04:15:25-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-04:15:25-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:15:25-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-04:15:26-root-INFO: grad norm: 262.317 242.443 100.158
2024-12-02-04:15:26-root-INFO: grad norm: 363.612 324.432 164.188
2024-12-02-04:15:27-root-INFO: Loss too large (4306.946->4318.528)! Learning rate decreased to 0.00062.
2024-12-02-04:15:28-root-INFO: grad norm: 443.768 407.935 174.696
2024-12-02-04:15:29-root-INFO: grad norm: 558.903 505.010 239.454
2024-12-02-04:15:29-root-INFO: Loss too large (4297.664->4308.108)! Learning rate decreased to 0.00050.
2024-12-02-04:15:30-root-INFO: grad norm: 478.841 438.891 191.478
2024-12-02-04:15:31-root-INFO: Loss Change: 4321.565 -> 4253.514
2024-12-02-04:15:31-root-INFO: Regularization Change: 0.000 -> 0.272
2024-12-02-04:15:31-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-04:15:31-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:15:31-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-04:15:31-root-INFO: grad norm: 755.754 673.584 342.707
2024-12-02-04:15:32-root-INFO: Loss too large (4287.165->4436.446)! Learning rate decreased to 0.00065.
2024-12-02-04:15:32-root-INFO: Loss too large (4287.165->4304.876)! Learning rate decreased to 0.00052.
2024-12-02-04:15:33-root-INFO: grad norm: 629.136 578.782 246.625
2024-12-02-04:15:34-root-INFO: grad norm: 541.164 487.065 235.853
2024-12-02-04:15:35-root-INFO: grad norm: 464.430 427.367 181.803
2024-12-02-04:15:36-root-INFO: grad norm: 404.464 366.131 171.869
2024-12-02-04:15:37-root-INFO: Loss Change: 4287.165 -> 4145.050
2024-12-02-04:15:37-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-02-04:15:37-root-INFO: Undo step: 220
2024-12-02-04:15:37-root-INFO: Undo step: 221
2024-12-02-04:15:37-root-INFO: Undo step: 222
2024-12-02-04:15:37-root-INFO: Undo step: 223
2024-12-02-04:15:37-root-INFO: Undo step: 224
2024-12-02-04:15:37-root-INFO: step: 225 lr_xt 0.00063896
2024-12-02-04:15:37-root-INFO: grad norm: 3619.641 3204.366 1683.403
2024-12-02-04:15:38-root-INFO: Loss too large (7851.872->10490.858)! Learning rate decreased to 0.00051.
2024-12-02-04:15:38-root-INFO: Loss too large (7851.872->8421.414)! Learning rate decreased to 0.00041.
2024-12-02-04:15:39-root-INFO: grad norm: 3103.681 2860.627 1204.013
2024-12-02-04:15:40-root-INFO: grad norm: 2696.333 2434.139 1159.817
2024-12-02-04:15:41-root-INFO: grad norm: 2399.845 2200.058 958.648
2024-12-02-04:15:42-root-INFO: grad norm: 2122.157 1919.796 904.397
2024-12-02-04:15:43-root-INFO: Loss Change: 7851.872 -> 5755.841
2024-12-02-04:15:43-root-INFO: Regularization Change: 0.000 -> 3.734
2024-12-02-04:15:43-root-INFO: Learning rate of xt decay: 0.03994 -> 0.04042.
2024-12-02-04:15:43-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:15:43-root-INFO: step: 224 lr_xt 0.00067063
2024-12-02-04:15:43-root-INFO: grad norm: 2119.491 1939.719 854.245
2024-12-02-04:15:44-root-INFO: Loss too large (5739.498->7030.680)! Learning rate decreased to 0.00054.
2024-12-02-04:15:44-root-INFO: Loss too large (5739.498->6034.151)! Learning rate decreased to 0.00043.
2024-12-02-04:15:45-root-INFO: grad norm: 1949.363 1772.360 811.638
2024-12-02-04:15:46-root-INFO: grad norm: 1833.663 1678.273 738.728
2024-12-02-04:15:47-root-INFO: grad norm: 1712.985 1553.555 721.653
2024-12-02-04:15:48-root-INFO: grad norm: 1621.441 1482.193 657.399
2024-12-02-04:15:49-root-INFO: Loss Change: 5739.498 -> 5121.274
2024-12-02-04:15:49-root-INFO: Regularization Change: 0.000 -> 1.372
2024-12-02-04:15:49-root-INFO: Learning rate of xt decay: 0.04042 -> 0.04091.
2024-12-02-04:15:49-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:15:49-root-INFO: step: 223 lr_xt 0.00070376
2024-12-02-04:15:49-root-INFO: grad norm: 1135.962 1028.856 481.523
2024-12-02-04:15:50-root-INFO: Loss too large (4961.173->5309.694)! Learning rate decreased to 0.00056.
2024-12-02-04:15:50-root-INFO: Loss too large (4961.173->5047.062)! Learning rate decreased to 0.00045.
2024-12-02-04:15:51-root-INFO: grad norm: 1060.465 971.582 424.987
2024-12-02-04:15:52-root-INFO: grad norm: 1006.060 908.010 433.216
2024-12-02-04:15:53-root-INFO: grad norm: 955.871 875.945 382.638
2024-12-02-04:15:54-root-INFO: grad norm: 899.851 811.471 388.905
2024-12-02-04:15:55-root-INFO: Loss Change: 4961.173 -> 4718.410
2024-12-02-04:15:55-root-INFO: Regularization Change: 0.000 -> 0.616
2024-12-02-04:15:55-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-04:15:55-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:15:55-root-INFO: step: 222 lr_xt 0.00073840
2024-12-02-04:15:55-root-INFO: grad norm: 907.054 831.159 363.211
2024-12-02-04:15:56-root-INFO: Loss too large (4657.770->4884.238)! Learning rate decreased to 0.00059.
2024-12-02-04:15:56-root-INFO: Loss too large (4657.770->4701.578)! Learning rate decreased to 0.00047.
2024-12-02-04:15:57-root-INFO: grad norm: 826.578 746.352 355.232
2024-12-02-04:15:58-root-INFO: grad norm: 757.798 695.212 301.559
2024-12-02-04:15:59-root-INFO: grad norm: 687.730 621.213 295.071
2024-12-02-04:16:00-root-INFO: grad norm: 626.988 575.677 248.417
2024-12-02-04:16:01-root-INFO: Loss Change: 4657.770 -> 4463.420
2024-12-02-04:16:01-root-INFO: Regularization Change: 0.000 -> 0.437
2024-12-02-04:16:01-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-04:16:01-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:16:01-root-INFO: step: 221 lr_xt 0.00077462
2024-12-02-04:16:01-root-INFO: grad norm: 377.730 346.230 151.012
2024-12-02-04:16:02-root-INFO: grad norm: 593.768 541.683 243.188
2024-12-02-04:16:03-root-INFO: Loss too large (4387.903->4460.826)! Learning rate decreased to 0.00062.
2024-12-02-04:16:03-root-INFO: Loss too large (4387.903->4390.096)! Learning rate decreased to 0.00050.
2024-12-02-04:16:04-root-INFO: grad norm: 508.280 461.400 213.210
2024-12-02-04:16:05-root-INFO: grad norm: 440.737 405.483 172.720
2024-12-02-04:16:06-root-INFO: grad norm: 381.502 347.206 158.089
2024-12-02-04:16:07-root-INFO: Loss Change: 4393.289 -> 4280.747
2024-12-02-04:16:07-root-INFO: Regularization Change: 0.000 -> 0.331
2024-12-02-04:16:07-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-04:16:07-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:16:07-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-04:16:07-root-INFO: grad norm: 693.085 623.441 302.800
2024-12-02-04:16:07-root-INFO: Loss too large (4311.277->4404.146)! Learning rate decreased to 0.00065.
2024-12-02-04:16:08-root-INFO: grad norm: 847.852 772.437 349.561
2024-12-02-04:16:09-root-INFO: Loss too large (4305.663->4347.533)! Learning rate decreased to 0.00052.
2024-12-02-04:16:10-root-INFO: grad norm: 710.753 647.305 293.541
2024-12-02-04:16:11-root-INFO: grad norm: 587.475 535.892 240.721
2024-12-02-04:16:12-root-INFO: grad norm: 493.578 452.086 198.084
2024-12-02-04:16:12-root-INFO: Loss Change: 4311.277 -> 4162.244
2024-12-02-04:16:12-root-INFO: Regularization Change: 0.000 -> 0.376
2024-12-02-04:16:12-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-04:16:12-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:16:13-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-04:16:13-root-INFO: grad norm: 362.423 330.299 149.174
2024-12-02-04:16:13-root-INFO: Loss too large (4121.336->4130.361)! Learning rate decreased to 0.00068.
2024-12-02-04:16:14-root-INFO: grad norm: 416.989 383.991 162.575
2024-12-02-04:16:15-root-INFO: grad norm: 491.237 448.511 200.379
2024-12-02-04:16:16-root-INFO: grad norm: 578.478 531.143 229.181
2024-12-02-04:16:17-root-INFO: Loss too large (4100.377->4103.625)! Learning rate decreased to 0.00055.
2024-12-02-04:16:18-root-INFO: grad norm: 447.834 409.391 181.533
2024-12-02-04:16:18-root-INFO: Loss Change: 4121.336 -> 4041.950
2024-12-02-04:16:18-root-INFO: Regularization Change: 0.000 -> 0.292
2024-12-02-04:16:18-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-04:16:18-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-04:16:19-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-04:16:19-root-INFO: grad norm: 565.146 513.236 236.598
2024-12-02-04:16:19-root-INFO: Loss too large (4045.362->4106.390)! Learning rate decreased to 0.00071.
2024-12-02-04:16:20-root-INFO: grad norm: 643.558 591.585 253.366
2024-12-02-04:16:21-root-INFO: Loss too large (4037.454->4046.204)! Learning rate decreased to 0.00057.
2024-12-02-04:16:22-root-INFO: grad norm: 486.311 447.016 191.508
2024-12-02-04:16:23-root-INFO: grad norm: 366.899 338.247 142.140
2024-12-02-04:16:24-root-INFO: grad norm: 286.474 266.051 106.229
2024-12-02-04:16:24-root-INFO: Loss Change: 4045.362 -> 3930.455
2024-12-02-04:16:24-root-INFO: Regularization Change: 0.000 -> 0.272
2024-12-02-04:16:24-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-04:16:24-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:16:25-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-04:16:25-root-INFO: grad norm: 222.974 193.077 111.529
2024-12-02-04:16:26-root-INFO: grad norm: 165.490 151.561 66.455
2024-12-02-04:16:27-root-INFO: grad norm: 145.067 135.406 52.056
2024-12-02-04:16:28-root-INFO: grad norm: 136.695 129.599 43.469
2024-12-02-04:16:29-root-INFO: grad norm: 133.306 127.098 40.207
2024-12-02-04:16:30-root-INFO: Loss Change: 3916.608 -> 3815.821
2024-12-02-04:16:30-root-INFO: Regularization Change: 0.000 -> 0.446
2024-12-02-04:16:30-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-04:16:30-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:16:30-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-04:16:30-root-INFO: grad norm: 270.627 237.296 130.114
2024-12-02-04:16:31-root-INFO: grad norm: 311.669 285.753 124.429
2024-12-02-04:16:32-root-INFO: Loss too large (3780.413->3786.307)! Learning rate decreased to 0.00079.
2024-12-02-04:16:33-root-INFO: grad norm: 337.588 315.083 121.196
2024-12-02-04:16:34-root-INFO: grad norm: 385.875 358.199 143.502
2024-12-02-04:16:35-root-INFO: grad norm: 443.874 416.329 153.929
2024-12-02-04:16:35-root-INFO: Loss Change: 3802.250 -> 3759.167
2024-12-02-04:16:35-root-INFO: Regularization Change: 0.000 -> 0.373
2024-12-02-04:16:35-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-04:16:35-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:16:36-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-04:16:36-root-INFO: grad norm: 465.023 435.160 163.956
2024-12-02-04:16:36-root-INFO: Loss too large (3732.941->3786.795)! Learning rate decreased to 0.00082.
2024-12-02-04:16:37-root-INFO: grad norm: 518.285 487.179 176.850
2024-12-02-04:16:38-root-INFO: grad norm: 583.093 546.277 203.908
2024-12-02-04:16:39-root-INFO: Loss too large (3732.038->3740.744)! Learning rate decreased to 0.00066.
2024-12-02-04:16:40-root-INFO: grad norm: 421.742 397.060 142.161
2024-12-02-04:16:41-root-INFO: grad norm: 302.636 284.659 102.752
2024-12-02-04:16:41-root-INFO: Loss Change: 3732.941 -> 3651.187
2024-12-02-04:16:41-root-INFO: Regularization Change: 0.000 -> 0.248
2024-12-02-04:16:41-root-INFO: Undo step: 215
2024-12-02-04:16:41-root-INFO: Undo step: 216
2024-12-02-04:16:41-root-INFO: Undo step: 217
2024-12-02-04:16:41-root-INFO: Undo step: 218
2024-12-02-04:16:41-root-INFO: Undo step: 219
2024-12-02-04:16:42-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-04:16:42-root-INFO: grad norm: 2206.058 1751.431 1341.337
2024-12-02-04:16:43-root-INFO: grad norm: 1990.985 1606.237 1176.446
2024-12-02-04:16:44-root-INFO: grad norm: 2216.083 1906.362 1129.959
2024-12-02-04:16:44-root-INFO: Loss too large (5421.629->5844.824)! Learning rate decreased to 0.00065.
2024-12-02-04:16:45-root-INFO: grad norm: 2115.706 1901.808 927.005
2024-12-02-04:16:46-root-INFO: Loss too large (5008.667->5143.904)! Learning rate decreased to 0.00052.
2024-12-02-04:16:47-root-INFO: grad norm: 1548.512 1404.609 651.892
2024-12-02-04:16:47-root-INFO: Loss Change: 6429.691 -> 4408.101
2024-12-02-04:16:47-root-INFO: Regularization Change: 0.000 -> 5.282
2024-12-02-04:16:47-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-04:16:47-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:16:48-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-04:16:48-root-INFO: grad norm: 1048.670 951.719 440.386
2024-12-02-04:16:48-root-INFO: Loss too large (4345.045->4553.284)! Learning rate decreased to 0.00068.
2024-12-02-04:16:49-root-INFO: grad norm: 1148.704 1044.893 477.200
2024-12-02-04:16:50-root-INFO: grad norm: 1283.018 1166.197 534.902
2024-12-02-04:16:51-root-INFO: Loss too large (4329.944->4381.512)! Learning rate decreased to 0.00055.
2024-12-02-04:16:52-root-INFO: grad norm: 921.921 838.864 382.419
2024-12-02-04:16:53-root-INFO: grad norm: 647.408 587.365 272.287
2024-12-02-04:16:53-root-INFO: Loss Change: 4345.045 -> 4046.119
2024-12-02-04:16:53-root-INFO: Regularization Change: 0.000 -> 0.734
2024-12-02-04:16:53-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-04:16:53-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-04:16:54-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-04:16:54-root-INFO: grad norm: 681.643 617.609 288.436
2024-12-02-04:16:54-root-INFO: Loss too large (4049.357->4140.318)! Learning rate decreased to 0.00071.
2024-12-02-04:16:55-root-INFO: grad norm: 741.632 674.720 307.850
2024-12-02-04:16:56-root-INFO: Loss too large (4039.125->4043.564)! Learning rate decreased to 0.00057.
2024-12-02-04:16:57-root-INFO: grad norm: 521.976 476.987 211.996
2024-12-02-04:16:58-root-INFO: grad norm: 367.357 333.834 153.315
2024-12-02-04:16:59-root-INFO: grad norm: 272.853 252.948 102.303
2024-12-02-04:16:59-root-INFO: Loss Change: 4049.357 -> 3902.401
2024-12-02-04:16:59-root-INFO: Regularization Change: 0.000 -> 0.306
2024-12-02-04:16:59-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-04:16:59-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:17:00-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-04:17:00-root-INFO: grad norm: 218.771 187.693 112.393
2024-12-02-04:17:01-root-INFO: grad norm: 179.357 161.669 77.666
2024-12-02-04:17:02-root-INFO: grad norm: 189.294 175.996 69.696
2024-12-02-04:17:03-root-INFO: grad norm: 239.499 217.730 99.766
2024-12-02-04:17:04-root-INFO: grad norm: 340.258 315.483 127.460
2024-12-02-04:17:04-root-INFO: Loss too large (3807.737->3819.634)! Learning rate decreased to 0.00075.
2024-12-02-04:17:05-root-INFO: Loss Change: 3873.008 -> 3797.740
2024-12-02-04:17:05-root-INFO: Regularization Change: 0.000 -> 0.426
2024-12-02-04:17:05-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-04:17:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:17:05-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-04:17:05-root-INFO: grad norm: 235.188 207.962 109.842
2024-12-02-04:17:06-root-INFO: grad norm: 231.191 215.816 82.901
2024-12-02-04:17:07-root-INFO: grad norm: 306.137 279.934 123.921
2024-12-02-04:17:08-root-INFO: Loss too large (3723.094->3729.122)! Learning rate decreased to 0.00079.
2024-12-02-04:17:09-root-INFO: grad norm: 320.616 299.008 115.710
2024-12-02-04:17:10-root-INFO: grad norm: 342.633 312.448 140.620
2024-12-02-04:17:10-root-INFO: Loss Change: 3758.997 -> 3695.122
2024-12-02-04:17:10-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-04:17:10-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-04:17:10-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:17:11-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-04:17:11-root-INFO: grad norm: 398.064 368.708 150.029
2024-12-02-04:17:11-root-INFO: Loss too large (3675.075->3699.974)! Learning rate decreased to 0.00082.
2024-12-02-04:17:12-root-INFO: grad norm: 410.564 377.000 162.584
2024-12-02-04:17:13-root-INFO: grad norm: 424.421 393.830 158.212
2024-12-02-04:17:14-root-INFO: grad norm: 440.637 405.095 173.374
2024-12-02-04:17:15-root-INFO: grad norm: 457.167 424.317 170.166
2024-12-02-04:17:16-root-INFO: Loss Change: 3675.075 -> 3632.779
2024-12-02-04:17:16-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-04:17:16-root-INFO: Undo step: 215
2024-12-02-04:17:16-root-INFO: Undo step: 216
2024-12-02-04:17:16-root-INFO: Undo step: 217
2024-12-02-04:17:16-root-INFO: Undo step: 218
2024-12-02-04:17:16-root-INFO: Undo step: 219
2024-12-02-04:17:16-root-INFO: step: 220 lr_xt 0.00081248
2024-12-02-04:17:17-root-INFO: grad norm: 4173.389 3767.604 1795.086
2024-12-02-04:17:17-root-INFO: Loss too large (8670.612->8890.951)! Learning rate decreased to 0.00065.
2024-12-02-04:17:18-root-INFO: grad norm: 3201.148 2878.734 1400.084
2024-12-02-04:17:19-root-INFO: grad norm: 2919.488 2682.174 1152.978
2024-12-02-04:17:20-root-INFO: Loss too large (5823.979->6066.908)! Learning rate decreased to 0.00052.
2024-12-02-04:17:21-root-INFO: grad norm: 2019.384 1834.638 843.810
2024-12-02-04:17:22-root-INFO: grad norm: 1271.737 1156.150 529.748
2024-12-02-04:17:22-root-INFO: Loss Change: 8670.612 -> 4463.619
2024-12-02-04:17:22-root-INFO: Regularization Change: 0.000 -> 6.592
2024-12-02-04:17:22-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-04:17:22-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-04:17:23-root-INFO: step: 219 lr_xt 0.00085206
2024-12-02-04:17:23-root-INFO: grad norm: 914.230 835.960 370.118
2024-12-02-04:17:23-root-INFO: Loss too large (4412.038->4498.794)! Learning rate decreased to 0.00068.
2024-12-02-04:17:24-root-INFO: grad norm: 960.958 875.776 395.544
2024-12-02-04:17:25-root-INFO: grad norm: 1007.748 920.120 411.019
2024-12-02-04:17:26-root-INFO: grad norm: 1068.467 975.989 434.819
2024-12-02-04:17:27-root-INFO: Loss too large (4287.308->4292.150)! Learning rate decreased to 0.00055.
2024-12-02-04:17:28-root-INFO: grad norm: 736.494 672.898 299.386
2024-12-02-04:17:28-root-INFO: Loss Change: 4412.038 -> 4092.532
2024-12-02-04:17:28-root-INFO: Regularization Change: 0.000 -> 0.907
2024-12-02-04:17:28-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04342.
2024-12-02-04:17:28-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-04:17:29-root-INFO: step: 218 lr_xt 0.00089342
2024-12-02-04:17:29-root-INFO: grad norm: 332.212 308.060 124.354
2024-12-02-04:17:30-root-INFO: grad norm: 435.162 397.065 178.060
2024-12-02-04:17:30-root-INFO: Loss too large (4010.313->4014.934)! Learning rate decreased to 0.00071.
2024-12-02-04:17:31-root-INFO: grad norm: 446.575 410.741 175.275
2024-12-02-04:17:32-root-INFO: grad norm: 458.675 419.208 186.138
2024-12-02-04:17:33-root-INFO: grad norm: 472.924 434.311 187.168
2024-12-02-04:17:34-root-INFO: Loss Change: 4032.843 -> 3930.758
2024-12-02-04:17:34-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-02-04:17:34-root-INFO: Learning rate of xt decay: 0.04342 -> 0.04394.
2024-12-02-04:17:34-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:17:34-root-INFO: step: 217 lr_xt 0.00093664
2024-12-02-04:17:34-root-INFO: grad norm: 628.903 566.205 273.734
2024-12-02-04:17:35-root-INFO: Loss too large (3931.840->3980.834)! Learning rate decreased to 0.00075.
2024-12-02-04:17:36-root-INFO: grad norm: 621.757 575.574 235.151
2024-12-02-04:17:37-root-INFO: grad norm: 625.530 570.483 256.587
2024-12-02-04:17:38-root-INFO: grad norm: 631.378 582.119 244.489
2024-12-02-04:17:39-root-INFO: grad norm: 637.076 582.054 258.998
2024-12-02-04:17:39-root-INFO: Loss Change: 3931.840 -> 3838.763
2024-12-02-04:17:39-root-INFO: Regularization Change: 0.000 -> 0.558
2024-12-02-04:17:39-root-INFO: Learning rate of xt decay: 0.04394 -> 0.04447.
2024-12-02-04:17:39-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:17:40-root-INFO: step: 216 lr_xt 0.00098179
2024-12-02-04:17:40-root-INFO: grad norm: 471.938 438.786 173.760
2024-12-02-04:17:40-root-INFO: Loss too large (3782.141->3797.659)! Learning rate decreased to 0.00079.
2024-12-02-04:17:41-root-INFO: grad norm: 452.195 413.228 183.637
2024-12-02-04:17:42-root-INFO: grad norm: 447.953 414.947 168.763
2024-12-02-04:17:43-root-INFO: grad norm: 445.578 408.307 178.396
2024-12-02-04:17:44-root-INFO: grad norm: 444.647 410.530 170.809
2024-12-02-04:17:45-root-INFO: Loss Change: 3782.141 -> 3695.873
2024-12-02-04:17:45-root-INFO: Regularization Change: 0.000 -> 0.443
2024-12-02-04:17:45-root-INFO: Learning rate of xt decay: 0.04447 -> 0.04500.
2024-12-02-04:17:45-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:17:45-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-04:17:46-root-INFO: grad norm: 471.247 431.040 190.469
2024-12-02-04:17:46-root-INFO: Loss too large (3672.195->3700.172)! Learning rate decreased to 0.00082.
2024-12-02-04:17:47-root-INFO: grad norm: 450.427 416.805 170.757
2024-12-02-04:17:48-root-INFO: grad norm: 433.066 397.260 172.426
2024-12-02-04:17:49-root-INFO: grad norm: 415.278 384.467 156.975
2024-12-02-04:17:50-root-INFO: grad norm: 399.589 367.376 157.182
2024-12-02-04:17:51-root-INFO: Loss Change: 3672.195 -> 3591.673
2024-12-02-04:17:51-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-04:17:51-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-04:17:51-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:17:51-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-04:17:51-root-INFO: grad norm: 283.435 266.499 96.508
2024-12-02-04:17:52-root-INFO: grad norm: 352.849 325.982 135.050
2024-12-02-04:17:53-root-INFO: Loss too large (3534.128->3541.339)! Learning rate decreased to 0.00086.
2024-12-02-04:17:54-root-INFO: grad norm: 326.442 305.464 115.135
2024-12-02-04:17:55-root-INFO: grad norm: 304.704 281.960 115.512
2024-12-02-04:17:56-root-INFO: grad norm: 284.688 265.881 101.759
2024-12-02-04:17:56-root-INFO: Loss Change: 3542.947 -> 3474.806
2024-12-02-04:17:56-root-INFO: Regularization Change: 0.000 -> 0.349
2024-12-02-04:17:56-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-04:17:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:17:57-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-04:17:57-root-INFO: grad norm: 419.800 377.676 183.282
2024-12-02-04:17:57-root-INFO: Loss too large (3492.271->3495.386)! Learning rate decreased to 0.00090.
2024-12-02-04:17:58-root-INFO: grad norm: 366.007 342.032 130.287
2024-12-02-04:17:59-root-INFO: grad norm: 330.067 303.660 129.363
2024-12-02-04:18:00-root-INFO: grad norm: 299.029 279.609 106.006
2024-12-02-04:18:01-root-INFO: grad norm: 273.930 253.786 103.102
2024-12-02-04:18:02-root-INFO: Loss Change: 3492.271 -> 3395.968
2024-12-02-04:18:02-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-02-04:18:02-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-04:18:02-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:18:02-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-04:18:03-root-INFO: grad norm: 223.260 210.957 73.089
2024-12-02-04:18:04-root-INFO: grad norm: 275.678 256.184 101.823
2024-12-02-04:18:05-root-INFO: grad norm: 351.941 330.583 120.735
2024-12-02-04:18:05-root-INFO: Loss too large (3363.505->3371.762)! Learning rate decreased to 0.00095.
2024-12-02-04:18:06-root-INFO: grad norm: 307.685 286.579 111.991
2024-12-02-04:18:07-root-INFO: grad norm: 270.034 254.413 90.512
2024-12-02-04:18:08-root-INFO: Loss Change: 3373.761 -> 3314.770
2024-12-02-04:18:08-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-04:18:08-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-04:18:08-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-04:18:08-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-04:18:08-root-INFO: grad norm: 383.909 343.214 172.019
2024-12-02-04:18:09-root-INFO: grad norm: 447.214 418.785 156.906
2024-12-02-04:18:10-root-INFO: Loss too large (3296.565->3311.441)! Learning rate decreased to 0.00099.
2024-12-02-04:18:11-root-INFO: grad norm: 364.705 337.736 137.637
2024-12-02-04:18:12-root-INFO: grad norm: 300.368 283.068 100.467
2024-12-02-04:18:13-root-INFO: grad norm: 256.853 240.533 90.097
2024-12-02-04:18:13-root-INFO: Loss Change: 3306.600 -> 3211.705
2024-12-02-04:18:13-root-INFO: Regularization Change: 0.000 -> 0.440
2024-12-02-04:18:13-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-04:18:13-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:18:14-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-04:18:14-root-INFO: grad norm: 181.506 173.825 52.240
2024-12-02-04:18:15-root-INFO: grad norm: 224.219 209.537 79.800
2024-12-02-04:18:16-root-INFO: grad norm: 306.997 289.687 101.627
2024-12-02-04:18:16-root-INFO: Loss too large (3178.464->3192.866)! Learning rate decreased to 0.00104.
2024-12-02-04:18:17-root-INFO: grad norm: 303.196 283.146 108.426
2024-12-02-04:18:18-root-INFO: grad norm: 312.465 291.595 112.280
2024-12-02-04:18:19-root-INFO: Loss Change: 3189.937 -> 3152.867
2024-12-02-04:18:19-root-INFO: Regularization Change: 0.000 -> 0.376
2024-12-02-04:18:19-root-INFO: Undo step: 210
2024-12-02-04:18:19-root-INFO: Undo step: 211
2024-12-02-04:18:19-root-INFO: Undo step: 212
2024-12-02-04:18:19-root-INFO: Undo step: 213
2024-12-02-04:18:19-root-INFO: Undo step: 214
2024-12-02-04:18:19-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-04:18:20-root-INFO: grad norm: 2850.394 2421.467 1503.744
2024-12-02-04:18:21-root-INFO: grad norm: 2475.941 2188.417 1158.065
2024-12-02-04:18:22-root-INFO: grad norm: 3131.626 2711.699 1566.453
2024-12-02-04:18:22-root-INFO: Loss too large (5637.157->6364.953)! Learning rate decreased to 0.00082.
2024-12-02-04:18:23-root-INFO: grad norm: 1921.943 1779.336 726.520
2024-12-02-04:18:24-root-INFO: grad norm: 1019.021 941.840 389.026
2024-12-02-04:18:25-root-INFO: Loss Change: 7115.277 -> 3924.805
2024-12-02-04:18:25-root-INFO: Regularization Change: 0.000 -> 10.394
2024-12-02-04:18:25-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-04:18:25-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:18:25-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-04:18:25-root-INFO: grad norm: 770.911 731.421 243.570
2024-12-02-04:18:26-root-INFO: Loss too large (3844.051->3884.572)! Learning rate decreased to 0.00086.
2024-12-02-04:18:27-root-INFO: grad norm: 688.663 633.288 270.561
2024-12-02-04:18:28-root-INFO: grad norm: 651.853 606.952 237.741
2024-12-02-04:18:29-root-INFO: grad norm: 622.195 571.961 244.924
2024-12-02-04:18:30-root-INFO: grad norm: 594.262 550.428 224.000
2024-12-02-04:18:30-root-INFO: Loss Change: 3844.051 -> 3622.400
2024-12-02-04:18:30-root-INFO: Regularization Change: 0.000 -> 1.030
2024-12-02-04:18:30-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-04:18:30-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:18:31-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-04:18:31-root-INFO: grad norm: 754.532 683.915 318.714
2024-12-02-04:18:31-root-INFO: Loss too large (3671.322->3756.224)! Learning rate decreased to 0.00090.
2024-12-02-04:18:32-root-INFO: grad norm: 711.683 654.087 280.470
2024-12-02-04:18:33-root-INFO: grad norm: 675.497 617.511 273.820
2024-12-02-04:18:34-root-INFO: grad norm: 636.476 583.894 253.315
2024-12-02-04:18:35-root-INFO: grad norm: 603.431 553.240 240.945
2024-12-02-04:18:36-root-INFO: Loss Change: 3671.322 -> 3496.944
2024-12-02-04:18:36-root-INFO: Regularization Change: 0.000 -> 0.880
2024-12-02-04:18:36-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-04:18:36-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:18:36-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-04:18:37-root-INFO: grad norm: 520.529 478.750 204.325
2024-12-02-04:18:37-root-INFO: Loss too large (3465.284->3502.169)! Learning rate decreased to 0.00095.
2024-12-02-04:18:38-root-INFO: grad norm: 476.267 437.746 187.639
2024-12-02-04:18:39-root-INFO: grad norm: 434.083 397.793 173.750
2024-12-02-04:18:40-root-INFO: grad norm: 399.811 367.382 157.730
2024-12-02-04:18:41-root-INFO: grad norm: 368.242 336.591 149.362
2024-12-02-04:18:42-root-INFO: Loss Change: 3465.284 -> 3347.566
2024-12-02-04:18:42-root-INFO: Regularization Change: 0.000 -> 0.551
2024-12-02-04:18:42-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-04:18:42-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-04:18:42-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-04:18:42-root-INFO: grad norm: 553.766 495.777 246.702
2024-12-02-04:18:43-root-INFO: Loss too large (3355.923->3399.506)! Learning rate decreased to 0.00099.
2024-12-02-04:18:44-root-INFO: grad norm: 512.781 465.345 215.405
2024-12-02-04:18:45-root-INFO: grad norm: 498.773 448.966 217.265
2024-12-02-04:18:46-root-INFO: grad norm: 488.200 442.067 207.163
2024-12-02-04:18:47-root-INFO: grad norm: 491.155 441.496 215.207
2024-12-02-04:18:47-root-INFO: Loss Change: 3355.923 -> 3263.468
2024-12-02-04:18:47-root-INFO: Regularization Change: 0.000 -> 0.641
2024-12-02-04:18:47-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-04:18:47-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:18:48-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-04:18:48-root-INFO: grad norm: 422.031 382.724 177.855
2024-12-02-04:18:48-root-INFO: Loss too large (3230.211->3264.972)! Learning rate decreased to 0.00104.
2024-12-02-04:18:49-root-INFO: grad norm: 434.308 393.928 182.878
2024-12-02-04:18:50-root-INFO: grad norm: 444.407 405.331 182.222
2024-12-02-04:18:51-root-INFO: grad norm: 457.318 414.583 193.030
2024-12-02-04:18:52-root-INFO: grad norm: 461.554 422.768 185.202
2024-12-02-04:18:53-root-INFO: Loss Change: 3230.211 -> 3169.431
2024-12-02-04:18:53-root-INFO: Regularization Change: 0.000 -> 0.526
2024-12-02-04:18:53-root-INFO: Undo step: 210
2024-12-02-04:18:53-root-INFO: Undo step: 211
2024-12-02-04:18:53-root-INFO: Undo step: 212
2024-12-02-04:18:53-root-INFO: Undo step: 213
2024-12-02-04:18:53-root-INFO: Undo step: 214
2024-12-02-04:18:53-root-INFO: step: 215 lr_xt 0.00102894
2024-12-02-04:18:54-root-INFO: grad norm: 2656.857 2180.589 1517.868
2024-12-02-04:18:55-root-INFO: grad norm: 1906.003 1750.352 754.396
2024-12-02-04:18:56-root-INFO: grad norm: 2569.294 2426.632 844.232
2024-12-02-04:18:56-root-INFO: Loss too large (4977.028->6801.080)! Learning rate decreased to 0.00082.
2024-12-02-04:18:57-root-INFO: Loss too large (4977.028->5366.500)! Learning rate decreased to 0.00066.
2024-12-02-04:18:57-root-INFO: grad norm: 1831.422 1682.797 722.702
2024-12-02-04:18:58-root-INFO: grad norm: 1188.191 1118.518 400.892
2024-12-02-04:18:59-root-INFO: Loss Change: 6542.642 -> 3812.963
2024-12-02-04:18:59-root-INFO: Regularization Change: 0.000 -> 6.820
2024-12-02-04:18:59-root-INFO: Learning rate of xt decay: 0.04500 -> 0.04554.
2024-12-02-04:18:59-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:19:00-root-INFO: step: 214 lr_xt 0.00107819
2024-12-02-04:19:00-root-INFO: grad norm: 941.436 864.125 373.616
2024-12-02-04:19:00-root-INFO: Loss too large (3801.944->4029.955)! Learning rate decreased to 0.00086.
2024-12-02-04:19:01-root-INFO: grad norm: 1033.210 972.224 349.719
2024-12-02-04:19:02-root-INFO: Loss too large (3790.360->3816.284)! Learning rate decreased to 0.00069.
2024-12-02-04:19:03-root-INFO: grad norm: 731.106 675.000 280.877
2024-12-02-04:19:04-root-INFO: grad norm: 512.885 482.256 174.585
2024-12-02-04:19:05-root-INFO: grad norm: 374.509 345.528 144.455
2024-12-02-04:19:05-root-INFO: Loss Change: 3801.944 -> 3506.821
2024-12-02-04:19:05-root-INFO: Regularization Change: 0.000 -> 0.700
2024-12-02-04:19:05-root-INFO: Learning rate of xt decay: 0.04554 -> 0.04609.
2024-12-02-04:19:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:19:06-root-INFO: step: 213 lr_xt 0.00112961
2024-12-02-04:19:06-root-INFO: grad norm: 215.003 194.878 90.825
2024-12-02-04:19:07-root-INFO: grad norm: 184.232 172.900 63.615
2024-12-02-04:19:08-root-INFO: grad norm: 200.539 189.133 66.668
2024-12-02-04:19:09-root-INFO: grad norm: 261.137 243.717 93.779
2024-12-02-04:19:10-root-INFO: grad norm: 389.256 364.253 137.259
2024-12-02-04:19:10-root-INFO: Loss too large (3396.849->3427.463)! Learning rate decreased to 0.00090.
2024-12-02-04:19:11-root-INFO: Loss Change: 3478.464 -> 3389.204
2024-12-02-04:19:11-root-INFO: Regularization Change: 0.000 -> 0.698
2024-12-02-04:19:11-root-INFO: Learning rate of xt decay: 0.04609 -> 0.04664.
2024-12-02-04:19:11-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-04:19:11-root-INFO: step: 212 lr_xt 0.00118329
2024-12-02-04:19:11-root-INFO: grad norm: 444.121 409.221 172.573
2024-12-02-04:19:12-root-INFO: Loss too large (3373.083->3419.568)! Learning rate decreased to 0.00095.
2024-12-02-04:19:13-root-INFO: grad norm: 482.537 450.033 174.106
2024-12-02-04:19:14-root-INFO: grad norm: 523.581 482.167 204.087
2024-12-02-04:19:15-root-INFO: grad norm: 568.792 529.453 207.855
2024-12-02-04:19:15-root-INFO: Loss too large (3357.483->3358.678)! Learning rate decreased to 0.00076.
2024-12-02-04:19:16-root-INFO: grad norm: 393.745 362.569 153.555
2024-12-02-04:19:17-root-INFO: Loss Change: 3373.083 -> 3281.366
2024-12-02-04:19:17-root-INFO: Regularization Change: 0.000 -> 0.379
2024-12-02-04:19:17-root-INFO: Learning rate of xt decay: 0.04664 -> 0.04720.
2024-12-02-04:19:17-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-04:19:17-root-INFO: step: 211 lr_xt 0.00123933
2024-12-02-04:19:17-root-INFO: grad norm: 205.556 185.738 88.060
2024-12-02-04:19:18-root-INFO: grad norm: 178.258 169.860 54.070
2024-12-02-04:19:19-root-INFO: grad norm: 201.243 191.310 62.442
2024-12-02-04:19:20-root-INFO: grad norm: 270.751 253.079 96.214
2024-12-02-04:19:21-root-INFO: Loss too large (3187.904->3195.082)! Learning rate decreased to 0.00099.
2024-12-02-04:19:22-root-INFO: grad norm: 282.952 264.377 100.830
2024-12-02-04:19:22-root-INFO: Loss Change: 3238.514 -> 3169.544
2024-12-02-04:19:22-root-INFO: Regularization Change: 0.000 -> 0.488
2024-12-02-04:19:22-root-INFO: Learning rate of xt decay: 0.04720 -> 0.04777.
2024-12-02-04:19:22-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:19:23-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-04:19:23-root-INFO: grad norm: 375.413 341.861 155.132
2024-12-02-04:19:23-root-INFO: Loss too large (3165.154->3199.228)! Learning rate decreased to 0.00104.
2024-12-02-04:19:24-root-INFO: grad norm: 398.555 366.364 156.917
2024-12-02-04:19:25-root-INFO: grad norm: 424.218 386.615 174.614
2024-12-02-04:19:26-root-INFO: grad norm: 451.623 414.486 179.345
2024-12-02-04:19:27-root-INFO: grad norm: 480.305 437.014 199.279
2024-12-02-04:19:28-root-INFO: Loss Change: 3165.154 -> 3141.063
2024-12-02-04:19:28-root-INFO: Regularization Change: 0.000 -> 0.470
2024-12-02-04:19:28-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-04:19:28-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:19:28-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-04:19:29-root-INFO: grad norm: 409.170 379.069 154.036
2024-12-02-04:19:29-root-INFO: Loss too large (3092.202->3131.467)! Learning rate decreased to 0.00109.
2024-12-02-04:19:30-root-INFO: grad norm: 423.100 386.602 171.907
2024-12-02-04:19:31-root-INFO: grad norm: 449.188 408.727 186.312
2024-12-02-04:19:32-root-INFO: grad norm: 479.746 433.106 206.337
2024-12-02-04:19:33-root-INFO: grad norm: 511.832 462.846 218.508
2024-12-02-04:19:34-root-INFO: Loss Change: 3092.202 -> 3071.279
2024-12-02-04:19:34-root-INFO: Regularization Change: 0.000 -> 0.536
2024-12-02-04:19:34-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-04:19:34-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:19:34-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-04:19:34-root-INFO: grad norm: 680.955 604.678 313.151
2024-12-02-04:19:35-root-INFO: Loss too large (3102.972->3271.990)! Learning rate decreased to 0.00114.
2024-12-02-04:19:35-root-INFO: Loss too large (3102.972->3109.610)! Learning rate decreased to 0.00091.
2024-12-02-04:19:36-root-INFO: grad norm: 461.384 411.752 208.172
2024-12-02-04:19:37-root-INFO: grad norm: 318.063 283.125 144.929
2024-12-02-04:19:38-root-INFO: grad norm: 228.665 206.053 99.146
2024-12-02-04:19:39-root-INFO: grad norm: 171.411 154.200 74.860
2024-12-02-04:19:40-root-INFO: Loss Change: 3102.972 -> 2937.079
2024-12-02-04:19:40-root-INFO: Regularization Change: 0.000 -> 0.381
2024-12-02-04:19:40-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-04:19:40-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:19:40-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-04:19:40-root-INFO: grad norm: 115.970 107.941 42.399
2024-12-02-04:19:41-root-INFO: grad norm: 106.228 102.195 28.992
2024-12-02-04:19:42-root-INFO: grad norm: 108.562 104.405 29.754
2024-12-02-04:19:43-root-INFO: grad norm: 119.163 113.187 37.264
2024-12-02-04:19:44-root-INFO: grad norm: 145.770 137.223 49.182
2024-12-02-04:19:45-root-INFO: Loss Change: 2907.543 -> 2854.515
2024-12-02-04:19:45-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-02-04:19:45-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-04:19:45-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:19:45-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-04:19:46-root-INFO: grad norm: 295.716 259.786 141.277
2024-12-02-04:19:46-root-INFO: Loss too large (2843.247->2876.198)! Learning rate decreased to 0.00126.
2024-12-02-04:19:47-root-INFO: grad norm: 352.540 310.271 167.381
2024-12-02-04:19:47-root-INFO: Loss too large (2842.754->2852.656)! Learning rate decreased to 0.00101.
2024-12-02-04:19:48-root-INFO: grad norm: 294.841 257.531 143.557
2024-12-02-04:19:49-root-INFO: grad norm: 256.377 226.814 119.519
2024-12-02-04:19:50-root-INFO: grad norm: 226.295 198.578 108.518
2024-12-02-04:19:51-root-INFO: Loss Change: 2843.247 -> 2790.428
2024-12-02-04:19:51-root-INFO: Regularization Change: 0.000 -> 0.270
2024-12-02-04:19:51-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-04:19:51-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:19:51-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-04:19:52-root-INFO: grad norm: 150.282 136.253 63.401
2024-12-02-04:19:53-root-INFO: grad norm: 228.384 204.060 102.559
2024-12-02-04:19:53-root-INFO: Loss too large (2757.610->2785.577)! Learning rate decreased to 0.00132.
2024-12-02-04:19:53-root-INFO: Loss too large (2757.610->2762.152)! Learning rate decreased to 0.00105.
2024-12-02-04:19:54-root-INFO: grad norm: 223.854 200.206 100.140
2024-12-02-04:19:55-root-INFO: grad norm: 223.863 198.457 103.582
2024-12-02-04:19:56-root-INFO: grad norm: 225.424 201.618 100.829
2024-12-02-04:19:57-root-INFO: Loss Change: 2764.332 -> 2728.879
2024-12-02-04:19:57-root-INFO: Regularization Change: 0.000 -> 0.278
2024-12-02-04:19:57-root-INFO: Undo step: 205
2024-12-02-04:19:57-root-INFO: Undo step: 206
2024-12-02-04:19:57-root-INFO: Undo step: 207
2024-12-02-04:19:57-root-INFO: Undo step: 208
2024-12-02-04:19:57-root-INFO: Undo step: 209
2024-12-02-04:19:57-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-04:19:58-root-INFO: grad norm: 1447.867 1278.981 678.622
2024-12-02-04:19:59-root-INFO: grad norm: 983.107 866.042 465.265
2024-12-02-04:20:00-root-INFO: grad norm: 985.326 917.992 357.991
2024-12-02-04:20:01-root-INFO: grad norm: 1101.087 1008.860 441.128
2024-12-02-04:20:01-root-INFO: Loss too large (3677.199->3716.814)! Learning rate decreased to 0.00104.
2024-12-02-04:20:02-root-INFO: grad norm: 820.040 774.392 269.782
2024-12-02-04:20:03-root-INFO: Loss Change: 4964.368 -> 3334.100
2024-12-02-04:20:03-root-INFO: Regularization Change: 0.000 -> 7.260
2024-12-02-04:20:03-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-04:20:03-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:03-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-04:20:03-root-INFO: grad norm: 703.307 650.841 266.546
2024-12-02-04:20:04-root-INFO: Loss too large (3323.554->3333.846)! Learning rate decreased to 0.00109.
2024-12-02-04:20:05-root-INFO: grad norm: 538.993 509.606 175.542
2024-12-02-04:20:06-root-INFO: grad norm: 432.434 407.756 143.994
2024-12-02-04:20:07-root-INFO: grad norm: 352.885 335.215 110.268
2024-12-02-04:20:08-root-INFO: grad norm: 291.368 275.493 94.862
2024-12-02-04:20:09-root-INFO: Loss Change: 3323.554 -> 3051.087
2024-12-02-04:20:09-root-INFO: Regularization Change: 0.000 -> 1.062
2024-12-02-04:20:09-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-04:20:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:09-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-04:20:09-root-INFO: grad norm: 213.427 201.532 70.256
2024-12-02-04:20:10-root-INFO: grad norm: 208.296 199.250 60.717
2024-12-02-04:20:11-root-INFO: grad norm: 223.919 214.280 64.989
2024-12-02-04:20:12-root-INFO: grad norm: 256.355 244.757 76.234
2024-12-02-04:20:13-root-INFO: grad norm: 301.181 287.134 90.909
2024-12-02-04:20:14-root-INFO: Loss Change: 3028.296 -> 2954.440
2024-12-02-04:20:14-root-INFO: Regularization Change: 0.000 -> 0.829
2024-12-02-04:20:14-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-04:20:14-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:14-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-04:20:15-root-INFO: grad norm: 381.385 357.156 133.768
2024-12-02-04:20:15-root-INFO: Loss too large (2926.452->2931.265)! Learning rate decreased to 0.00120.
2024-12-02-04:20:16-root-INFO: grad norm: 298.363 280.756 100.977
2024-12-02-04:20:17-root-INFO: grad norm: 235.692 222.181 78.655
2024-12-02-04:20:18-root-INFO: grad norm: 191.586 180.020 65.558
2024-12-02-04:20:19-root-INFO: grad norm: 158.154 148.751 53.720
2024-12-02-04:20:19-root-INFO: Loss Change: 2926.452 -> 2826.707
2024-12-02-04:20:19-root-INFO: Regularization Change: 0.000 -> 0.449
2024-12-02-04:20:19-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-04:20:19-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:20-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-04:20:20-root-INFO: grad norm: 134.472 124.573 50.640
2024-12-02-04:20:21-root-INFO: grad norm: 123.688 117.600 38.326
2024-12-02-04:20:22-root-INFO: grad norm: 123.921 118.111 37.502
2024-12-02-04:20:23-root-INFO: grad norm: 129.376 123.435 38.754
2024-12-02-04:20:24-root-INFO: grad norm: 138.728 132.727 40.361
2024-12-02-04:20:25-root-INFO: Loss Change: 2802.029 -> 2741.720
2024-12-02-04:20:25-root-INFO: Regularization Change: 0.000 -> 0.513
2024-12-02-04:20:25-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-04:20:25-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:25-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-04:20:25-root-INFO: grad norm: 197.858 180.411 81.238
2024-12-02-04:20:26-root-INFO: grad norm: 214.132 197.907 81.764
2024-12-02-04:20:27-root-INFO: grad norm: 256.722 237.515 97.430
2024-12-02-04:20:28-root-INFO: Loss too large (2709.521->2712.098)! Learning rate decreased to 0.00132.
2024-12-02-04:20:29-root-INFO: grad norm: 217.746 200.990 83.765
2024-12-02-04:20:30-root-INFO: grad norm: 195.474 180.751 74.426
2024-12-02-04:20:30-root-INFO: Loss Change: 2726.421 -> 2671.761
2024-12-02-04:20:30-root-INFO: Regularization Change: 0.000 -> 0.429
2024-12-02-04:20:30-root-INFO: Undo step: 205
2024-12-02-04:20:30-root-INFO: Undo step: 206
2024-12-02-04:20:30-root-INFO: Undo step: 207
2024-12-02-04:20:30-root-INFO: Undo step: 208
2024-12-02-04:20:30-root-INFO: Undo step: 209
2024-12-02-04:20:31-root-INFO: step: 210 lr_xt 0.00129780
2024-12-02-04:20:31-root-INFO: grad norm: 2809.240 2413.216 1438.131
2024-12-02-04:20:32-root-INFO: grad norm: 2681.740 2529.848 889.718
2024-12-02-04:20:32-root-INFO: Loss too large (6071.778->6409.152)! Learning rate decreased to 0.00104.
2024-12-02-04:20:33-root-INFO: grad norm: 2320.748 2246.095 583.891
2024-12-02-04:20:34-root-INFO: grad norm: 1861.505 1769.438 578.178
2024-12-02-04:20:35-root-INFO: grad norm: 1310.849 1209.681 504.972
2024-12-02-04:20:36-root-INFO: Loss Change: 6694.738 -> 3375.619
2024-12-02-04:20:36-root-INFO: Regularization Change: 0.000 -> 12.622
2024-12-02-04:20:36-root-INFO: Learning rate of xt decay: 0.04777 -> 0.04834.
2024-12-02-04:20:36-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:36-root-INFO: step: 209 lr_xt 0.00135882
2024-12-02-04:20:37-root-INFO: grad norm: 1001.924 925.578 383.611
2024-12-02-04:20:37-root-INFO: Loss too large (3378.799->3391.905)! Learning rate decreased to 0.00109.
2024-12-02-04:20:38-root-INFO: grad norm: 698.522 648.348 259.959
2024-12-02-04:20:39-root-INFO: grad norm: 512.761 477.485 186.903
2024-12-02-04:20:40-root-INFO: grad norm: 376.027 350.644 135.814
2024-12-02-04:20:41-root-INFO: grad norm: 283.969 263.658 105.467
2024-12-02-04:20:42-root-INFO: Loss Change: 3378.799 -> 2939.450
2024-12-02-04:20:42-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-02-04:20:42-root-INFO: Learning rate of xt decay: 0.04834 -> 0.04892.
2024-12-02-04:20:42-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:42-root-INFO: step: 208 lr_xt 0.00142247
2024-12-02-04:20:42-root-INFO: grad norm: 224.968 209.810 81.181
2024-12-02-04:20:43-root-INFO: grad norm: 183.158 173.446 58.851
2024-12-02-04:20:44-root-INFO: grad norm: 178.358 169.198 56.426
2024-12-02-04:20:45-root-INFO: grad norm: 185.482 173.903 64.509
2024-12-02-04:20:46-root-INFO: grad norm: 199.010 188.645 63.388
2024-12-02-04:20:47-root-INFO: Loss Change: 2918.181 -> 2825.203
2024-12-02-04:20:47-root-INFO: Regularization Change: 0.000 -> 0.711
2024-12-02-04:20:47-root-INFO: Learning rate of xt decay: 0.04892 -> 0.04951.
2024-12-02-04:20:47-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:47-root-INFO: step: 207 lr_xt 0.00150141
2024-12-02-04:20:48-root-INFO: grad norm: 253.818 229.332 108.768
2024-12-02-04:20:49-root-INFO: grad norm: 254.237 237.525 90.655
2024-12-02-04:20:50-root-INFO: grad norm: 265.843 246.503 99.544
2024-12-02-04:20:51-root-INFO: grad norm: 281.825 265.991 93.135
2024-12-02-04:20:52-root-INFO: grad norm: 300.927 281.474 106.440
2024-12-02-04:20:52-root-INFO: Loss Change: 2795.277 -> 2743.928
2024-12-02-04:20:52-root-INFO: Regularization Change: 0.000 -> 0.643
2024-12-02-04:20:52-root-INFO: Learning rate of xt decay: 0.04951 -> 0.05011.
2024-12-02-04:20:52-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:53-root-INFO: step: 206 lr_xt 0.00157117
2024-12-02-04:20:53-root-INFO: grad norm: 299.271 289.594 75.489
2024-12-02-04:20:54-root-INFO: grad norm: 307.807 295.146 87.373
2024-12-02-04:20:55-root-INFO: grad norm: 324.149 312.507 86.090
2024-12-02-04:20:56-root-INFO: grad norm: 343.464 328.893 98.980
2024-12-02-04:20:57-root-INFO: grad norm: 365.196 352.057 97.076
2024-12-02-04:20:58-root-INFO: Loss Change: 2716.576 -> 2681.955
2024-12-02-04:20:58-root-INFO: Regularization Change: 0.000 -> 0.707
2024-12-02-04:20:58-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-04:20:58-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-04:20:58-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-04:20:58-root-INFO: grad norm: 405.215 379.577 141.847
2024-12-02-04:20:59-root-INFO: grad norm: 413.349 394.843 122.296
2024-12-02-04:21:00-root-INFO: grad norm: 434.639 415.317 128.152
2024-12-02-04:21:01-root-INFO: grad norm: 461.747 445.251 122.318
2024-12-02-04:21:01-root-INFO: Loss too large (2653.085->2655.362)! Learning rate decreased to 0.00132.
2024-12-02-04:21:02-root-INFO: grad norm: 312.338 300.916 83.692
2024-12-02-04:21:03-root-INFO: Loss Change: 2669.823 -> 2571.056
2024-12-02-04:21:03-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-04:21:03-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-04:21:03-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-04:21:03-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-04:21:04-root-INFO: grad norm: 309.628 296.773 88.293
2024-12-02-04:21:04-root-INFO: Loss too large (2566.647->2570.680)! Learning rate decreased to 0.00138.
2024-12-02-04:21:05-root-INFO: grad norm: 253.550 245.139 64.764
2024-12-02-04:21:06-root-INFO: grad norm: 217.817 209.707 58.882
2024-12-02-04:21:07-root-INFO: grad norm: 192.037 184.870 51.977
2024-12-02-04:21:08-root-INFO: grad norm: 173.048 166.215 48.145
2024-12-02-04:21:09-root-INFO: Loss Change: 2566.647 -> 2495.098
2024-12-02-04:21:09-root-INFO: Regularization Change: 0.000 -> 0.411
2024-12-02-04:21:09-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-04:21:09-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:21:09-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-04:21:09-root-INFO: grad norm: 129.666 123.809 38.533
2024-12-02-04:21:10-root-INFO: grad norm: 162.660 157.070 42.278
2024-12-02-04:21:11-root-INFO: grad norm: 225.094 217.318 58.652
2024-12-02-04:21:12-root-INFO: Loss too large (2468.264->2478.458)! Learning rate decreased to 0.00144.
2024-12-02-04:21:13-root-INFO: grad norm: 225.442 216.785 61.876
2024-12-02-04:21:14-root-INFO: grad norm: 227.504 219.080 61.334
2024-12-02-04:21:14-root-INFO: Loss Change: 2477.404 -> 2445.408
2024-12-02-04:21:14-root-INFO: Regularization Change: 0.000 -> 0.406
2024-12-02-04:21:14-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-04:21:14-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:21:15-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-04:21:15-root-INFO: grad norm: 520.049 489.657 175.177
2024-12-02-04:21:15-root-INFO: Loss too large (2497.407->2600.809)! Learning rate decreased to 0.00150.
2024-12-02-04:21:16-root-INFO: Loss too large (2497.407->2501.862)! Learning rate decreased to 0.00120.
2024-12-02-04:21:17-root-INFO: grad norm: 356.978 344.683 92.881
2024-12-02-04:21:18-root-INFO: grad norm: 250.839 239.544 74.424
2024-12-02-04:21:19-root-INFO: grad norm: 185.833 178.882 50.349
2024-12-02-04:21:20-root-INFO: grad norm: 145.459 138.902 43.180
2024-12-02-04:21:20-root-INFO: Loss Change: 2497.407 -> 2371.464
2024-12-02-04:21:20-root-INFO: Regularization Change: 0.000 -> 0.422
2024-12-02-04:21:20-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-04:21:20-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:21:21-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-04:21:21-root-INFO: grad norm: 122.290 117.522 33.813
2024-12-02-04:21:22-root-INFO: grad norm: 184.249 177.747 48.513
2024-12-02-04:21:22-root-INFO: Loss too large (2359.912->2373.488)! Learning rate decreased to 0.00157.
2024-12-02-04:21:23-root-INFO: grad norm: 228.896 221.099 59.232
2024-12-02-04:21:24-root-INFO: Loss too large (2357.986->2361.067)! Learning rate decreased to 0.00126.
2024-12-02-04:21:25-root-INFO: grad norm: 196.786 189.309 53.726
2024-12-02-04:21:26-root-INFO: grad norm: 173.296 166.999 46.290
2024-12-02-04:21:26-root-INFO: Loss Change: 2364.860 -> 2329.102
2024-12-02-04:21:26-root-INFO: Regularization Change: 0.000 -> 0.307
2024-12-02-04:21:26-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-04:21:26-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:21:27-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-04:21:27-root-INFO: grad norm: 474.729 453.769 139.503
2024-12-02-04:21:27-root-INFO: Loss too large (2358.620->2514.382)! Learning rate decreased to 0.00165.
2024-12-02-04:21:28-root-INFO: Loss too large (2358.620->2423.966)! Learning rate decreased to 0.00132.
2024-12-02-04:21:28-root-INFO: Loss too large (2358.620->2366.232)! Learning rate decreased to 0.00105.
2024-12-02-04:21:29-root-INFO: grad norm: 272.433 265.072 62.900
2024-12-02-04:21:30-root-INFO: grad norm: 166.774 160.763 44.373
2024-12-02-04:21:31-root-INFO: grad norm: 108.141 103.860 30.127
2024-12-02-04:21:32-root-INFO: grad norm: 87.867 83.686 26.781
2024-12-02-04:21:33-root-INFO: Loss Change: 2358.620 -> 2276.673
2024-12-02-04:21:33-root-INFO: Regularization Change: 0.000 -> 0.218
2024-12-02-04:21:33-root-INFO: Undo step: 200
2024-12-02-04:21:33-root-INFO: Undo step: 201
2024-12-02-04:21:33-root-INFO: Undo step: 202
2024-12-02-04:21:33-root-INFO: Undo step: 203
2024-12-02-04:21:33-root-INFO: Undo step: 204
2024-12-02-04:21:33-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-04:21:33-root-INFO: grad norm: 1937.176 1824.280 651.655
2024-12-02-04:21:34-root-INFO: grad norm: 1624.230 1561.547 446.873
2024-12-02-04:21:35-root-INFO: grad norm: 1200.683 1134.505 393.112
2024-12-02-04:21:36-root-INFO: grad norm: 1209.601 1162.100 335.644
2024-12-02-04:21:37-root-INFO: grad norm: 1248.524 1187.288 386.211
2024-12-02-04:21:38-root-INFO: Loss too large (3364.025->3366.107)! Learning rate decreased to 0.00132.
2024-12-02-04:21:38-root-INFO: Loss Change: 4775.044 -> 2962.473
2024-12-02-04:21:38-root-INFO: Regularization Change: 0.000 -> 9.561
2024-12-02-04:21:38-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-04:21:38-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-04:21:39-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-04:21:39-root-INFO: grad norm: 767.592 739.326 206.384
2024-12-02-04:21:40-root-INFO: grad norm: 790.710 762.356 209.846
2024-12-02-04:21:41-root-INFO: grad norm: 806.932 775.671 222.430
2024-12-02-04:21:42-root-INFO: grad norm: 828.192 797.891 221.975
2024-12-02-04:21:42-root-INFO: Loss too large (2865.529->2880.783)! Learning rate decreased to 0.00138.
2024-12-02-04:21:43-root-INFO: grad norm: 538.943 519.637 142.958
2024-12-02-04:21:44-root-INFO: Loss Change: 2930.130 -> 2598.350
2024-12-02-04:21:44-root-INFO: Regularization Change: 0.000 -> 1.893
2024-12-02-04:21:44-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-04:21:44-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:21:44-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-04:21:45-root-INFO: grad norm: 324.067 308.633 98.819
2024-12-02-04:21:46-root-INFO: grad norm: 338.519 328.097 83.353
2024-12-02-04:21:46-root-INFO: grad norm: 359.928 347.709 92.987
2024-12-02-04:21:47-root-INFO: grad norm: 390.956 380.062 91.649
2024-12-02-04:21:48-root-INFO: grad norm: 430.800 418.583 101.867
2024-12-02-04:21:49-root-INFO: Loss too large (2541.093->2544.427)! Learning rate decreased to 0.00144.
2024-12-02-04:21:49-root-INFO: Loss Change: 2578.997 -> 2489.619
2024-12-02-04:21:49-root-INFO: Regularization Change: 0.000 -> 0.990
2024-12-02-04:21:49-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-04:21:49-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:21:50-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-04:21:50-root-INFO: grad norm: 584.033 558.669 170.244
2024-12-02-04:21:50-root-INFO: Loss too large (2531.555->2584.701)! Learning rate decreased to 0.00150.
2024-12-02-04:21:51-root-INFO: grad norm: 370.295 362.527 75.448
2024-12-02-04:21:52-root-INFO: grad norm: 281.023 272.946 66.892
2024-12-02-04:21:54-root-INFO: grad norm: 199.810 194.535 45.608
2024-12-02-04:21:54-root-INFO: grad norm: 173.772 167.639 45.761
2024-12-02-04:21:55-root-INFO: Loss Change: 2531.555 -> 2378.166
2024-12-02-04:21:55-root-INFO: Regularization Change: 0.000 -> 0.668
2024-12-02-04:21:55-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-04:21:55-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:21:56-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-04:21:56-root-INFO: grad norm: 154.788 149.323 40.769
2024-12-02-04:21:57-root-INFO: grad norm: 229.789 223.975 51.364
2024-12-02-04:21:57-root-INFO: Loss too large (2360.128->2376.688)! Learning rate decreased to 0.00157.
2024-12-02-04:21:58-root-INFO: grad norm: 228.560 222.283 53.200
2024-12-02-04:21:59-root-INFO: grad norm: 228.501 222.548 51.817
2024-12-02-04:22:00-root-INFO: grad norm: 239.156 232.840 54.600
2024-12-02-04:22:01-root-INFO: Loss Change: 2364.812 -> 2329.255
2024-12-02-04:22:01-root-INFO: Regularization Change: 0.000 -> 0.477
2024-12-02-04:22:01-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-04:22:01-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:22:01-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-04:22:01-root-INFO: grad norm: 651.380 624.518 185.130
2024-12-02-04:22:02-root-INFO: Loss too large (2383.588->2553.208)! Learning rate decreased to 0.00165.
2024-12-02-04:22:02-root-INFO: Loss too large (2383.588->2452.198)! Learning rate decreased to 0.00132.
2024-12-02-04:22:02-root-INFO: Loss too large (2383.588->2384.470)! Learning rate decreased to 0.00105.
2024-12-02-04:22:03-root-INFO: grad norm: 258.629 252.637 55.347
2024-12-02-04:22:04-root-INFO: grad norm: 173.920 169.093 40.692
2024-12-02-04:22:05-root-INFO: grad norm: 88.004 83.743 27.051
2024-12-02-04:22:06-root-INFO: grad norm: 77.562 73.785 23.909
2024-12-02-04:22:07-root-INFO: Loss Change: 2383.588 -> 2258.875
2024-12-02-04:22:07-root-INFO: Regularization Change: 0.000 -> 0.264
2024-12-02-04:22:07-root-INFO: Undo step: 200
2024-12-02-04:22:07-root-INFO: Undo step: 201
2024-12-02-04:22:07-root-INFO: Undo step: 202
2024-12-02-04:22:07-root-INFO: Undo step: 203
2024-12-02-04:22:07-root-INFO: Undo step: 204
2024-12-02-04:22:08-root-INFO: step: 205 lr_xt 0.00164390
2024-12-02-04:22:08-root-INFO: grad norm: 1920.474 1787.558 702.038
2024-12-02-04:22:09-root-INFO: grad norm: 1623.861 1556.534 462.740
2024-12-02-04:22:10-root-INFO: grad norm: 1079.468 1029.036 326.092
2024-12-02-04:22:11-root-INFO: grad norm: 831.746 793.025 250.825
2024-12-02-04:22:12-root-INFO: grad norm: 823.829 784.031 252.960
2024-12-02-04:22:12-root-INFO: Loss Change: 5056.654 -> 2979.873
2024-12-02-04:22:12-root-INFO: Regularization Change: 0.000 -> 11.692
2024-12-02-04:22:12-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05131.
2024-12-02-04:22:12-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-04:22:13-root-INFO: step: 204 lr_xt 0.00171973
2024-12-02-04:22:13-root-INFO: grad norm: 812.869 770.350 259.453
2024-12-02-04:22:14-root-INFO: grad norm: 826.717 791.966 237.175
2024-12-02-04:22:15-root-INFO: grad norm: 851.496 807.240 270.940
2024-12-02-04:22:15-root-INFO: Loss too large (2916.856->2928.553)! Learning rate decreased to 0.00138.
2024-12-02-04:22:16-root-INFO: grad norm: 563.922 542.214 154.957
2024-12-02-04:22:17-root-INFO: grad norm: 377.983 356.297 126.187
2024-12-02-04:22:18-root-INFO: Loss Change: 2969.341 -> 2579.984
2024-12-02-04:22:18-root-INFO: Regularization Change: 0.000 -> 1.868
2024-12-02-04:22:18-root-INFO: Learning rate of xt decay: 0.05131 -> 0.05193.
2024-12-02-04:22:18-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:22:18-root-INFO: step: 203 lr_xt 0.00179875
2024-12-02-04:22:19-root-INFO: grad norm: 258.424 249.991 65.479
2024-12-02-04:22:20-root-INFO: grad norm: 271.736 257.649 86.356
2024-12-02-04:22:21-root-INFO: grad norm: 292.739 284.148 70.397
2024-12-02-04:22:22-root-INFO: grad norm: 319.582 306.030 92.079
2024-12-02-04:22:23-root-INFO: grad norm: 355.525 345.649 83.213
2024-12-02-04:22:23-root-INFO: Loss Change: 2560.998 -> 2504.692
2024-12-02-04:22:23-root-INFO: Regularization Change: 0.000 -> 1.153
2024-12-02-04:22:23-root-INFO: Learning rate of xt decay: 0.05193 -> 0.05255.
2024-12-02-04:22:23-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:22:24-root-INFO: step: 202 lr_xt 0.00188111
2024-12-02-04:22:24-root-INFO: grad norm: 421.080 394.128 148.226
2024-12-02-04:22:25-root-INFO: grad norm: 414.673 398.178 115.790
2024-12-02-04:22:26-root-INFO: grad norm: 449.915 433.284 121.196
2024-12-02-04:22:26-root-INFO: Loss too large (2465.276->2486.444)! Learning rate decreased to 0.00150.
2024-12-02-04:22:27-root-INFO: grad norm: 369.838 359.707 85.971
2024-12-02-04:22:28-root-INFO: grad norm: 292.634 284.341 69.173
2024-12-02-04:22:29-root-INFO: Loss Change: 2498.429 -> 2364.477
2024-12-02-04:22:29-root-INFO: Regularization Change: 0.000 -> 0.855
2024-12-02-04:22:29-root-INFO: Learning rate of xt decay: 0.05255 -> 0.05318.
2024-12-02-04:22:29-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:22:29-root-INFO: step: 201 lr_xt 0.00196691
2024-12-02-04:22:29-root-INFO: grad norm: 231.247 224.902 53.799
2024-12-02-04:22:30-root-INFO: Loss too large (2350.846->2357.005)! Learning rate decreased to 0.00157.
2024-12-02-04:22:31-root-INFO: grad norm: 220.996 214.172 54.493
2024-12-02-04:22:32-root-INFO: grad norm: 281.017 272.302 69.441
2024-12-02-04:22:32-root-INFO: Loss too large (2330.690->2341.650)! Learning rate decreased to 0.00126.
2024-12-02-04:22:33-root-INFO: grad norm: 201.460 195.323 49.345
2024-12-02-04:22:34-root-INFO: grad norm: 110.708 106.909 28.754
2024-12-02-04:22:35-root-INFO: Loss Change: 2350.846 -> 2290.331
2024-12-02-04:22:35-root-INFO: Regularization Change: 0.000 -> 0.335
2024-12-02-04:22:35-root-INFO: Learning rate of xt decay: 0.05318 -> 0.05382.
2024-12-02-04:22:35-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:22:35-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-04:22:36-root-INFO: grad norm: 348.554 330.342 111.194
2024-12-02-04:22:36-root-INFO: Loss too large (2290.403->2341.486)! Learning rate decreased to 0.00165.
2024-12-02-04:22:36-root-INFO: Loss too large (2290.403->2317.718)! Learning rate decreased to 0.00132.
2024-12-02-04:22:37-root-INFO: Loss too large (2290.403->2299.698)! Learning rate decreased to 0.00105.
2024-12-02-04:22:38-root-INFO: grad norm: 194.400 189.645 42.731
2024-12-02-04:22:39-root-INFO: grad norm: 106.301 102.522 28.092
2024-12-02-04:22:39-root-INFO: grad norm: 94.323 89.013 31.202
2024-12-02-04:22:40-root-INFO: grad norm: 94.321 91.211 24.020
2024-12-02-04:22:41-root-INFO: Loss Change: 2290.403 -> 2236.864
2024-12-02-04:22:41-root-INFO: Regularization Change: 0.000 -> 0.186
2024-12-02-04:22:41-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-04:22:41-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:22:41-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-04:22:42-root-INFO: grad norm: 279.518 266.023 85.802
2024-12-02-04:22:42-root-INFO: Loss too large (2232.340->2286.176)! Learning rate decreased to 0.00172.
2024-12-02-04:22:42-root-INFO: Loss too large (2232.340->2263.766)! Learning rate decreased to 0.00138.
2024-12-02-04:22:43-root-INFO: Loss too large (2232.340->2247.103)! Learning rate decreased to 0.00110.
2024-12-02-04:22:43-root-INFO: Loss too large (2232.340->2234.953)! Learning rate decreased to 0.00088.
2024-12-02-04:22:44-root-INFO: grad norm: 179.830 175.904 37.368
2024-12-02-04:22:45-root-INFO: grad norm: 77.984 73.351 26.477
2024-12-02-04:22:46-root-INFO: grad norm: 73.878 70.280 22.773
2024-12-02-04:22:47-root-INFO: grad norm: 72.011 68.022 23.635
2024-12-02-04:22:48-root-INFO: Loss Change: 2232.340 -> 2194.372
2024-12-02-04:22:48-root-INFO: Regularization Change: 0.000 -> 0.122
2024-12-02-04:22:48-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-04:22:48-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-04:22:48-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-04:22:48-root-INFO: grad norm: 349.785 333.238 106.309
2024-12-02-04:22:49-root-INFO: Loss too large (2204.037->2291.803)! Learning rate decreased to 0.00180.
2024-12-02-04:22:49-root-INFO: Loss too large (2204.037->2260.681)! Learning rate decreased to 0.00144.
2024-12-02-04:22:49-root-INFO: Loss too large (2204.037->2236.725)! Learning rate decreased to 0.00115.
2024-12-02-04:22:50-root-INFO: Loss too large (2204.037->2218.372)! Learning rate decreased to 0.00092.
2024-12-02-04:22:50-root-INFO: Loss too large (2204.037->2204.513)! Learning rate decreased to 0.00074.
2024-12-02-04:22:51-root-INFO: grad norm: 202.407 198.004 41.985
2024-12-02-04:22:52-root-INFO: grad norm: 79.584 74.067 29.113
2024-12-02-04:22:53-root-INFO: grad norm: 74.535 69.681 26.456
2024-12-02-04:22:54-root-INFO: grad norm: 71.581 67.387 24.142
2024-12-02-04:22:55-root-INFO: Loss Change: 2204.037 -> 2161.662
2024-12-02-04:22:55-root-INFO: Regularization Change: 0.000 -> 0.102
2024-12-02-04:22:55-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-04:22:55-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:22:55-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-04:22:55-root-INFO: grad norm: 326.075 314.380 86.545
2024-12-02-04:22:56-root-INFO: Loss too large (2164.220->2262.631)! Learning rate decreased to 0.00188.
2024-12-02-04:22:56-root-INFO: Loss too large (2164.220->2232.641)! Learning rate decreased to 0.00150.
2024-12-02-04:22:56-root-INFO: Loss too large (2164.220->2208.710)! Learning rate decreased to 0.00120.
2024-12-02-04:22:57-root-INFO: Loss too large (2164.220->2189.671)! Learning rate decreased to 0.00096.
2024-12-02-04:22:57-root-INFO: Loss too large (2164.220->2174.666)! Learning rate decreased to 0.00077.
2024-12-02-04:22:58-root-INFO: grad norm: 216.577 211.899 44.771
2024-12-02-04:22:59-root-INFO: grad norm: 77.462 72.937 26.087
2024-12-02-04:23:00-root-INFO: grad norm: 75.457 71.993 22.601
2024-12-02-04:23:01-root-INFO: grad norm: 74.957 71.157 23.565
2024-12-02-04:23:01-root-INFO: Loss Change: 2164.220 -> 2129.174
2024-12-02-04:23:01-root-INFO: Regularization Change: 0.000 -> 0.091
2024-12-02-04:23:01-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-04:23:01-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:23:02-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-04:23:02-root-INFO: grad norm: 289.096 275.124 88.788
2024-12-02-04:23:02-root-INFO: Loss too large (2131.989->2225.366)! Learning rate decreased to 0.00196.
2024-12-02-04:23:03-root-INFO: Loss too large (2131.989->2194.923)! Learning rate decreased to 0.00157.
2024-12-02-04:23:03-root-INFO: Loss too large (2131.989->2171.533)! Learning rate decreased to 0.00126.
2024-12-02-04:23:03-root-INFO: Loss too large (2131.989->2153.481)! Learning rate decreased to 0.00100.
2024-12-02-04:23:04-root-INFO: Loss too large (2131.989->2139.722)! Learning rate decreased to 0.00080.
2024-12-02-04:23:05-root-INFO: grad norm: 225.849 221.230 45.443
2024-12-02-04:23:06-root-INFO: grad norm: 133.971 127.445 41.302
2024-12-02-04:23:07-root-INFO: grad norm: 140.305 136.999 30.280
2024-12-02-04:23:08-root-INFO: grad norm: 157.927 152.459 41.197
2024-12-02-04:23:08-root-INFO: Loss Change: 2131.989 -> 2100.315
2024-12-02-04:23:08-root-INFO: Regularization Change: 0.000 -> 0.126
2024-12-02-04:23:08-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-04:23:08-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:23:09-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-04:23:09-root-INFO: grad norm: 101.898 98.076 27.646
2024-12-02-04:23:09-root-INFO: Loss too large (2084.573->2105.521)! Learning rate decreased to 0.00205.
2024-12-02-04:23:10-root-INFO: Loss too large (2084.573->2093.111)! Learning rate decreased to 0.00164.
2024-12-02-04:23:10-root-INFO: Loss too large (2084.573->2086.173)! Learning rate decreased to 0.00131.
2024-12-02-04:23:11-root-INFO: grad norm: 208.681 203.676 45.430
2024-12-02-04:23:11-root-INFO: Loss too large (2082.580->2105.474)! Learning rate decreased to 0.00105.
2024-12-02-04:23:12-root-INFO: Loss too large (2082.580->2094.226)! Learning rate decreased to 0.00084.
2024-12-02-04:23:12-root-INFO: Loss too large (2082.580->2086.010)! Learning rate decreased to 0.00067.
2024-12-02-04:23:13-root-INFO: grad norm: 178.817 175.250 35.535
2024-12-02-04:23:14-root-INFO: grad norm: 142.116 138.097 33.559
2024-12-02-04:23:15-root-INFO: grad norm: 135.133 132.158 28.196
2024-12-02-04:23:16-root-INFO: Loss Change: 2084.573 -> 2067.113
2024-12-02-04:23:16-root-INFO: Regularization Change: 0.000 -> 0.076
2024-12-02-04:23:16-root-INFO: Undo step: 195
2024-12-02-04:23:16-root-INFO: Undo step: 196
2024-12-02-04:23:16-root-INFO: Undo step: 197
2024-12-02-04:23:16-root-INFO: Undo step: 198
2024-12-02-04:23:16-root-INFO: Undo step: 199
2024-12-02-04:23:16-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-04:23:16-root-INFO: grad norm: 1114.288 960.493 564.881
2024-12-02-04:23:17-root-INFO: grad norm: 917.903 900.517 177.809
2024-12-02-04:23:18-root-INFO: Loss too large (2974.696->3288.740)! Learning rate decreased to 0.00165.
2024-12-02-04:23:18-root-INFO: Loss too large (2974.696->2993.927)! Learning rate decreased to 0.00132.
2024-12-02-04:23:19-root-INFO: grad norm: 743.406 720.037 184.931
2024-12-02-04:23:20-root-INFO: grad norm: 599.183 581.290 145.336
2024-12-02-04:23:21-root-INFO: grad norm: 513.362 497.298 127.418
2024-12-02-04:23:22-root-INFO: Loss Change: 3811.307 -> 2503.940
2024-12-02-04:23:22-root-INFO: Regularization Change: 0.000 -> 7.050
2024-12-02-04:23:22-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-04:23:22-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:23:22-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-04:23:22-root-INFO: grad norm: 344.090 334.429 80.965
2024-12-02-04:23:23-root-INFO: Loss too large (2459.677->2509.765)! Learning rate decreased to 0.00172.
2024-12-02-04:23:24-root-INFO: grad norm: 428.443 414.527 108.306
2024-12-02-04:23:24-root-INFO: Loss too large (2456.782->2465.336)! Learning rate decreased to 0.00138.
2024-12-02-04:23:25-root-INFO: grad norm: 387.324 376.239 92.000
2024-12-02-04:23:26-root-INFO: grad norm: 368.750 357.085 92.017
2024-12-02-04:23:27-root-INFO: grad norm: 352.105 342.200 82.928
2024-12-02-04:23:27-root-INFO: Loss Change: 2459.677 -> 2342.146
2024-12-02-04:23:27-root-INFO: Regularization Change: 0.000 -> 1.118
2024-12-02-04:23:27-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-04:23:27-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-04:23:28-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-04:23:28-root-INFO: grad norm: 497.733 470.389 162.703
2024-12-02-04:23:28-root-INFO: Loss too large (2366.488->2614.500)! Learning rate decreased to 0.00180.
2024-12-02-04:23:29-root-INFO: Loss too large (2366.488->2446.489)! Learning rate decreased to 0.00144.
2024-12-02-04:23:30-root-INFO: grad norm: 508.297 494.799 116.360
2024-12-02-04:23:31-root-INFO: grad norm: 490.091 474.623 122.159
2024-12-02-04:23:32-root-INFO: grad norm: 484.822 472.466 108.757
2024-12-02-04:23:32-root-INFO: Loss too large (2301.392->2319.929)! Learning rate decreased to 0.00115.
2024-12-02-04:23:33-root-INFO: grad norm: 386.498 372.671 102.456
2024-12-02-04:23:34-root-INFO: Loss Change: 2366.488 -> 2240.778
2024-12-02-04:23:34-root-INFO: Regularization Change: 0.000 -> 0.778
2024-12-02-04:23:34-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-04:23:34-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:23:34-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-04:23:35-root-INFO: grad norm: 247.792 241.921 53.618
2024-12-02-04:23:35-root-INFO: Loss too large (2204.807->2406.741)! Learning rate decreased to 0.00188.
2024-12-02-04:23:35-root-INFO: Loss too large (2204.807->2294.024)! Learning rate decreased to 0.00150.
2024-12-02-04:23:36-root-INFO: Loss too large (2204.807->2228.623)! Learning rate decreased to 0.00120.
2024-12-02-04:23:37-root-INFO: grad norm: 414.913 402.875 99.218
2024-12-02-04:23:37-root-INFO: Loss too large (2195.820->2237.486)! Learning rate decreased to 0.00096.
2024-12-02-04:23:37-root-INFO: Loss too large (2195.820->2214.660)! Learning rate decreased to 0.00077.
2024-12-02-04:23:37-root-INFO: Loss too large (2195.820->2197.530)! Learning rate decreased to 0.00062.
2024-12-02-04:23:38-root-INFO: grad norm: 220.232 215.349 46.119
2024-12-02-04:23:39-root-INFO: grad norm: 100.843 95.517 32.339
2024-12-02-04:23:40-root-INFO: grad norm: 94.910 88.721 33.711
2024-12-02-04:23:41-root-INFO: Loss Change: 2204.807 -> 2152.603
2024-12-02-04:23:41-root-INFO: Regularization Change: 0.000 -> 0.133
2024-12-02-04:23:41-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-04:23:41-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:23:41-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-04:23:42-root-INFO: grad norm: 362.914 347.437 104.854
2024-12-02-04:23:42-root-INFO: Loss too large (2154.452->2296.091)! Learning rate decreased to 0.00196.
2024-12-02-04:23:42-root-INFO: Loss too large (2154.452->2257.160)! Learning rate decreased to 0.00157.
2024-12-02-04:23:43-root-INFO: Loss too large (2154.452->2225.194)! Learning rate decreased to 0.00126.
2024-12-02-04:23:43-root-INFO: Loss too large (2154.452->2199.245)! Learning rate decreased to 0.00100.
2024-12-02-04:23:43-root-INFO: Loss too large (2154.452->2178.525)! Learning rate decreased to 0.00080.
2024-12-02-04:23:44-root-INFO: Loss too large (2154.452->2162.418)! Learning rate decreased to 0.00064.
2024-12-02-04:23:45-root-INFO: grad norm: 258.995 253.737 51.926
2024-12-02-04:23:46-root-INFO: grad norm: 123.220 114.602 45.272
2024-12-02-04:23:47-root-INFO: grad norm: 123.372 118.823 33.192
2024-12-02-04:23:48-root-INFO: grad norm: 133.725 127.195 41.275
2024-12-02-04:23:48-root-INFO: Loss Change: 2154.452 -> 2111.229
2024-12-02-04:23:48-root-INFO: Regularization Change: 0.000 -> 0.114
2024-12-02-04:23:48-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-04:23:48-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:23:49-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-04:23:49-root-INFO: grad norm: 98.456 91.057 37.445
2024-12-02-04:23:50-root-INFO: grad norm: 222.745 216.789 51.163
2024-12-02-04:23:50-root-INFO: Loss too large (2086.655->2440.573)! Learning rate decreased to 0.00205.
2024-12-02-04:23:51-root-INFO: Loss too large (2086.655->2307.502)! Learning rate decreased to 0.00164.
2024-12-02-04:23:51-root-INFO: Loss too large (2086.655->2214.395)! Learning rate decreased to 0.00131.
2024-12-02-04:23:51-root-INFO: Loss too large (2086.655->2153.480)! Learning rate decreased to 0.00105.
2024-12-02-04:23:52-root-INFO: Loss too large (2086.655->2116.275)! Learning rate decreased to 0.00084.
2024-12-02-04:23:52-root-INFO: Loss too large (2086.655->2095.093)! Learning rate decreased to 0.00067.
2024-12-02-04:23:53-root-INFO: grad norm: 319.306 312.801 64.124
2024-12-02-04:23:53-root-INFO: Loss too large (2083.959->2098.871)! Learning rate decreased to 0.00054.
2024-12-02-04:23:54-root-INFO: Loss too large (2083.959->2087.221)! Learning rate decreased to 0.00043.
2024-12-02-04:23:55-root-INFO: grad norm: 225.575 221.218 44.120
2024-12-02-04:23:56-root-INFO: grad norm: 130.844 126.709 32.634
2024-12-02-04:23:56-root-INFO: Loss Change: 2096.004 -> 2068.045
2024-12-02-04:23:56-root-INFO: Regularization Change: 0.000 -> 0.131
2024-12-02-04:23:56-root-INFO: Undo step: 195
2024-12-02-04:23:56-root-INFO: Undo step: 196
2024-12-02-04:23:56-root-INFO: Undo step: 197
2024-12-02-04:23:56-root-INFO: Undo step: 198
2024-12-02-04:23:56-root-INFO: Undo step: 199
2024-12-02-04:23:57-root-INFO: step: 200 lr_xt 0.00205630
2024-12-02-04:23:57-root-INFO: grad norm: 1043.735 892.160 541.696
2024-12-02-04:23:58-root-INFO: grad norm: 676.619 651.961 180.997
2024-12-02-04:23:59-root-INFO: grad norm: 644.467 630.366 134.076
2024-12-02-04:23:59-root-INFO: Loss too large (2774.123->2791.662)! Learning rate decreased to 0.00165.
2024-12-02-04:24:00-root-INFO: grad norm: 445.724 432.446 107.979
2024-12-02-04:24:01-root-INFO: grad norm: 277.491 264.958 82.453
2024-12-02-04:24:02-root-INFO: Loss Change: 3999.841 -> 2470.728
2024-12-02-04:24:02-root-INFO: Regularization Change: 0.000 -> 7.906
2024-12-02-04:24:02-root-INFO: Learning rate of xt decay: 0.05382 -> 0.05447.
2024-12-02-04:24:02-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-04:24:02-root-INFO: step: 199 lr_xt 0.00214940
2024-12-02-04:24:03-root-INFO: grad norm: 228.503 207.476 95.746
2024-12-02-04:24:04-root-INFO: grad norm: 227.474 214.404 75.997
2024-12-02-04:24:05-root-INFO: grad norm: 556.447 534.960 153.137
2024-12-02-04:24:05-root-INFO: Loss too large (2396.059->2593.086)! Learning rate decreased to 0.00172.
2024-12-02-04:24:05-root-INFO: Loss too large (2396.059->2525.852)! Learning rate decreased to 0.00138.
2024-12-02-04:24:06-root-INFO: Loss too large (2396.059->2474.969)! Learning rate decreased to 0.00110.
2024-12-02-04:24:06-root-INFO: Loss too large (2396.059->2436.239)! Learning rate decreased to 0.00088.
2024-12-02-04:24:06-root-INFO: Loss too large (2396.059->2407.194)! Learning rate decreased to 0.00070.
2024-12-02-04:24:07-root-INFO: grad norm: 309.793 303.442 62.409
2024-12-02-04:24:08-root-INFO: grad norm: 161.695 154.492 47.721
2024-12-02-04:24:09-root-INFO: Loss Change: 2447.566 -> 2320.422
2024-12-02-04:24:09-root-INFO: Regularization Change: 0.000 -> 0.765
2024-12-02-04:24:09-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05512.
2024-12-02-04:24:09-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-04:24:09-root-INFO: step: 198 lr_xt 0.00224635
2024-12-02-04:24:10-root-INFO: grad norm: 677.817 659.216 157.703
2024-12-02-04:24:10-root-INFO: Loss too large (2348.159->2561.018)! Learning rate decreased to 0.00180.
2024-12-02-04:24:10-root-INFO: Loss too large (2348.159->2514.179)! Learning rate decreased to 0.00144.
2024-12-02-04:24:11-root-INFO: Loss too large (2348.159->2472.095)! Learning rate decreased to 0.00115.
2024-12-02-04:24:11-root-INFO: Loss too large (2348.159->2432.948)! Learning rate decreased to 0.00092.
2024-12-02-04:24:11-root-INFO: Loss too large (2348.159->2397.399)! Learning rate decreased to 0.00074.
2024-12-02-04:24:12-root-INFO: Loss too large (2348.159->2366.679)! Learning rate decreased to 0.00059.
2024-12-02-04:24:13-root-INFO: grad norm: 314.478 307.820 64.370
2024-12-02-04:24:14-root-INFO: grad norm: 221.121 214.669 53.027
2024-12-02-04:24:15-root-INFO: grad norm: 187.150 180.067 51.000
2024-12-02-04:24:15-root-INFO: grad norm: 184.514 178.354 47.278
2024-12-02-04:24:16-root-INFO: Loss Change: 2348.159 -> 2261.116
2024-12-02-04:24:16-root-INFO: Regularization Change: 0.000 -> 0.134
2024-12-02-04:24:16-root-INFO: Learning rate of xt decay: 0.05512 -> 0.05578.
2024-12-02-04:24:16-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:24:16-root-INFO: step: 197 lr_xt 0.00234729
2024-12-02-04:24:17-root-INFO: grad norm: 554.413 541.055 120.964
2024-12-02-04:24:17-root-INFO: Loss too large (2271.837->2477.603)! Learning rate decreased to 0.00188.
2024-12-02-04:24:17-root-INFO: Loss too large (2271.837->2436.223)! Learning rate decreased to 0.00150.
2024-12-02-04:24:18-root-INFO: Loss too large (2271.837->2398.052)! Learning rate decreased to 0.00120.
2024-12-02-04:24:18-root-INFO: Loss too large (2271.837->2362.677)! Learning rate decreased to 0.00096.
2024-12-02-04:24:18-root-INFO: Loss too large (2271.837->2331.002)! Learning rate decreased to 0.00077.
2024-12-02-04:24:19-root-INFO: Loss too large (2271.837->2303.757)! Learning rate decreased to 0.00062.
2024-12-02-04:24:19-root-INFO: Loss too large (2271.837->2281.080)! Learning rate decreased to 0.00049.
2024-12-02-04:24:20-root-INFO: grad norm: 311.912 305.683 62.025
2024-12-02-04:24:21-root-INFO: grad norm: 113.424 105.618 41.352
2024-12-02-04:24:22-root-INFO: grad norm: 109.386 101.863 39.866
2024-12-02-04:24:23-root-INFO: grad norm: 106.802 99.540 38.710
2024-12-02-04:24:24-root-INFO: Loss Change: 2271.837 -> 2211.626
2024-12-02-04:24:24-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-02-04:24:24-root-INFO: Learning rate of xt decay: 0.05578 -> 0.05645.
2024-12-02-04:24:24-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:24:24-root-INFO: step: 196 lr_xt 0.00245238
2024-12-02-04:24:24-root-INFO: grad norm: 510.282 495.445 122.155
2024-12-02-04:24:25-root-INFO: Loss too large (2218.616->2437.199)! Learning rate decreased to 0.00196.
2024-12-02-04:24:25-root-INFO: Loss too large (2218.616->2392.132)! Learning rate decreased to 0.00157.
2024-12-02-04:24:25-root-INFO: Loss too large (2218.616->2352.251)! Learning rate decreased to 0.00126.
2024-12-02-04:24:26-root-INFO: Loss too large (2218.616->2316.515)! Learning rate decreased to 0.00100.
2024-12-02-04:24:26-root-INFO: Loss too large (2218.616->2285.101)! Learning rate decreased to 0.00080.
2024-12-02-04:24:26-root-INFO: Loss too large (2218.616->2258.113)! Learning rate decreased to 0.00064.
2024-12-02-04:24:27-root-INFO: Loss too large (2218.616->2235.376)! Learning rate decreased to 0.00051.
2024-12-02-04:24:28-root-INFO: grad norm: 340.665 334.561 64.199
2024-12-02-04:24:29-root-INFO: grad norm: 123.224 114.692 45.056
2024-12-02-04:24:30-root-INFO: grad norm: 121.929 115.345 39.524
2024-12-02-04:24:31-root-INFO: grad norm: 124.588 117.908 40.246
2024-12-02-04:24:31-root-INFO: Loss Change: 2218.616 -> 2162.818
2024-12-02-04:24:31-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-04:24:31-root-INFO: Learning rate of xt decay: 0.05645 -> 0.05713.
2024-12-02-04:24:31-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:24:32-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-04:24:32-root-INFO: grad norm: 161.359 153.872 48.579
2024-12-02-04:24:32-root-INFO: Loss too large (2150.042->2210.530)! Learning rate decreased to 0.00205.
2024-12-02-04:24:33-root-INFO: Loss too large (2150.042->2190.574)! Learning rate decreased to 0.00164.
2024-12-02-04:24:33-root-INFO: Loss too large (2150.042->2174.861)! Learning rate decreased to 0.00131.
2024-12-02-04:24:33-root-INFO: Loss too large (2150.042->2163.148)! Learning rate decreased to 0.00105.
2024-12-02-04:24:34-root-INFO: Loss too large (2150.042->2155.049)! Learning rate decreased to 0.00084.
2024-12-02-04:24:35-root-INFO: grad norm: 270.147 264.952 52.725
2024-12-02-04:24:35-root-INFO: Loss too large (2149.905->2178.615)! Learning rate decreased to 0.00067.
2024-12-02-04:24:35-root-INFO: Loss too large (2149.905->2155.726)! Learning rate decreased to 0.00054.
2024-12-02-04:24:36-root-INFO: grad norm: 313.820 307.292 63.676
2024-12-02-04:24:37-root-INFO: Loss too large (2143.801->2147.566)! Learning rate decreased to 0.00043.
2024-12-02-04:24:38-root-INFO: grad norm: 254.436 249.367 50.536
2024-12-02-04:24:39-root-INFO: grad norm: 189.159 183.990 43.917
2024-12-02-04:24:39-root-INFO: Loss Change: 2150.042 -> 2126.559
2024-12-02-04:24:39-root-INFO: Regularization Change: 0.000 -> 0.069
2024-12-02-04:24:39-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-04:24:39-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:24:40-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-04:24:40-root-INFO: grad norm: 191.996 181.472 62.694
2024-12-02-04:24:40-root-INFO: Loss too large (2115.694->2212.951)! Learning rate decreased to 0.00214.
2024-12-02-04:24:41-root-INFO: Loss too large (2115.694->2181.299)! Learning rate decreased to 0.00171.
2024-12-02-04:24:41-root-INFO: Loss too large (2115.694->2157.201)! Learning rate decreased to 0.00137.
2024-12-02-04:24:41-root-INFO: Loss too large (2115.694->2139.187)! Learning rate decreased to 0.00110.
2024-12-02-04:24:42-root-INFO: Loss too large (2115.694->2126.406)! Learning rate decreased to 0.00088.
2024-12-02-04:24:42-root-INFO: Loss too large (2115.694->2117.987)! Learning rate decreased to 0.00070.
2024-12-02-04:24:43-root-INFO: grad norm: 274.005 269.037 51.942
2024-12-02-04:24:43-root-INFO: Loss too large (2112.922->2128.634)! Learning rate decreased to 0.00056.
2024-12-02-04:24:44-root-INFO: grad norm: 402.044 393.970 80.168
2024-12-02-04:24:45-root-INFO: Loss too large (2112.632->2127.617)! Learning rate decreased to 0.00045.
2024-12-02-04:24:45-root-INFO: Loss too large (2112.632->2113.011)! Learning rate decreased to 0.00036.
2024-12-02-04:24:46-root-INFO: grad norm: 263.945 259.107 50.302
2024-12-02-04:24:47-root-INFO: grad norm: 149.691 144.309 39.778
2024-12-02-04:24:48-root-INFO: Loss Change: 2115.694 -> 2089.773
2024-12-02-04:24:48-root-INFO: Regularization Change: 0.000 -> 0.059
2024-12-02-04:24:48-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-04:24:48-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:24:48-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-04:24:48-root-INFO: grad norm: 459.839 448.244 102.612
2024-12-02-04:24:49-root-INFO: Loss too large (2095.787->2348.365)! Learning rate decreased to 0.00224.
2024-12-02-04:24:49-root-INFO: Loss too large (2095.787->2306.443)! Learning rate decreased to 0.00179.
2024-12-02-04:24:49-root-INFO: Loss too large (2095.787->2268.855)! Learning rate decreased to 0.00143.
2024-12-02-04:24:50-root-INFO: Loss too large (2095.787->2234.065)! Learning rate decreased to 0.00114.
2024-12-02-04:24:50-root-INFO: Loss too large (2095.787->2201.942)! Learning rate decreased to 0.00092.
2024-12-02-04:24:50-root-INFO: Loss too large (2095.787->2172.504)! Learning rate decreased to 0.00073.
2024-12-02-04:24:51-root-INFO: Loss too large (2095.787->2145.635)! Learning rate decreased to 0.00059.
2024-12-02-04:24:51-root-INFO: Loss too large (2095.787->2121.811)! Learning rate decreased to 0.00047.
2024-12-02-04:24:51-root-INFO: Loss too large (2095.787->2102.478)! Learning rate decreased to 0.00038.
2024-12-02-04:24:52-root-INFO: grad norm: 345.483 339.807 62.364
2024-12-02-04:24:53-root-INFO: grad norm: 221.126 213.906 56.043
2024-12-02-04:24:54-root-INFO: grad norm: 198.337 193.405 43.958
2024-12-02-04:24:55-root-INFO: grad norm: 174.943 168.959 45.364
2024-12-02-04:24:56-root-INFO: Loss Change: 2095.787 -> 2059.936
2024-12-02-04:24:56-root-INFO: Regularization Change: 0.000 -> 0.050
2024-12-02-04:24:56-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-04:24:56-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-04:24:56-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-04:24:57-root-INFO: grad norm: 141.790 133.565 47.589
2024-12-02-04:24:57-root-INFO: Loss too large (2044.334->2100.379)! Learning rate decreased to 0.00233.
2024-12-02-04:24:57-root-INFO: Loss too large (2044.334->2076.026)! Learning rate decreased to 0.00187.
2024-12-02-04:24:58-root-INFO: Loss too large (2044.334->2059.821)! Learning rate decreased to 0.00149.
2024-12-02-04:24:58-root-INFO: Loss too large (2044.334->2049.592)! Learning rate decreased to 0.00119.
2024-12-02-04:24:59-root-INFO: grad norm: 319.960 314.741 57.553
2024-12-02-04:24:59-root-INFO: Loss too large (2043.552->2230.948)! Learning rate decreased to 0.00096.
2024-12-02-04:25:00-root-INFO: Loss too large (2043.552->2152.228)! Learning rate decreased to 0.00076.
2024-12-02-04:25:00-root-INFO: Loss too large (2043.552->2098.304)! Learning rate decreased to 0.00061.
2024-12-02-04:25:00-root-INFO: Loss too large (2043.552->2064.435)! Learning rate decreased to 0.00049.
2024-12-02-04:25:01-root-INFO: Loss too large (2043.552->2045.010)! Learning rate decreased to 0.00039.
2024-12-02-04:25:02-root-INFO: grad norm: 293.807 288.648 54.820
2024-12-02-04:25:03-root-INFO: grad norm: 286.398 281.764 51.307
2024-12-02-04:25:04-root-INFO: grad norm: 278.731 273.650 52.979
2024-12-02-04:25:04-root-INFO: Loss Change: 2044.334 -> 2027.716
2024-12-02-04:25:04-root-INFO: Regularization Change: 0.000 -> 0.070
2024-12-02-04:25:04-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-04:25:04-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:25:05-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-04:25:05-root-INFO: grad norm: 304.866 293.489 82.508
2024-12-02-04:25:05-root-INFO: Loss too large (2027.123->2260.646)! Learning rate decreased to 0.00244.
2024-12-02-04:25:06-root-INFO: Loss too large (2027.123->2210.210)! Learning rate decreased to 0.00195.
2024-12-02-04:25:06-root-INFO: Loss too large (2027.123->2169.509)! Learning rate decreased to 0.00156.
2024-12-02-04:25:06-root-INFO: Loss too large (2027.123->2135.652)! Learning rate decreased to 0.00125.
2024-12-02-04:25:07-root-INFO: Loss too large (2027.123->2106.598)! Learning rate decreased to 0.00100.
2024-12-02-04:25:07-root-INFO: Loss too large (2027.123->2081.215)! Learning rate decreased to 0.00080.
2024-12-02-04:25:07-root-INFO: Loss too large (2027.123->2059.657)! Learning rate decreased to 0.00064.
2024-12-02-04:25:08-root-INFO: Loss too large (2027.123->2042.785)! Learning rate decreased to 0.00051.
2024-12-02-04:25:08-root-INFO: Loss too large (2027.123->2030.903)! Learning rate decreased to 0.00041.
2024-12-02-04:25:09-root-INFO: grad norm: 323.813 318.113 60.486
2024-12-02-04:25:10-root-INFO: grad norm: 375.123 366.821 78.483
2024-12-02-04:25:10-root-INFO: Loss too large (2018.221->2021.775)! Learning rate decreased to 0.00033.
2024-12-02-04:25:11-root-INFO: grad norm: 298.546 293.558 54.347
2024-12-02-04:25:12-root-INFO: grad norm: 222.372 215.993 52.881
2024-12-02-04:25:13-root-INFO: Loss Change: 2027.123 -> 2000.718
2024-12-02-04:25:13-root-INFO: Regularization Change: 0.000 -> 0.047
2024-12-02-04:25:13-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-04:25:13-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:25:13-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-04:25:14-root-INFO: grad norm: 364.963 355.225 83.748
2024-12-02-04:25:14-root-INFO: Loss too large (2003.057->2261.468)! Learning rate decreased to 0.00254.
2024-12-02-04:25:14-root-INFO: Loss too large (2003.057->2220.111)! Learning rate decreased to 0.00203.
2024-12-02-04:25:15-root-INFO: Loss too large (2003.057->2183.897)! Learning rate decreased to 0.00163.
2024-12-02-04:25:15-root-INFO: Loss too large (2003.057->2150.990)! Learning rate decreased to 0.00130.
2024-12-02-04:25:15-root-INFO: Loss too large (2003.057->2120.585)! Learning rate decreased to 0.00104.
2024-12-02-04:25:16-root-INFO: Loss too large (2003.057->2092.102)! Learning rate decreased to 0.00083.
2024-12-02-04:25:16-root-INFO: Loss too large (2003.057->2065.323)! Learning rate decreased to 0.00067.
2024-12-02-04:25:16-root-INFO: Loss too large (2003.057->2041.198)! Learning rate decreased to 0.00053.
2024-12-02-04:25:17-root-INFO: Loss too large (2003.057->2021.525)! Learning rate decreased to 0.00043.
2024-12-02-04:25:17-root-INFO: Loss too large (2003.057->2007.344)! Learning rate decreased to 0.00034.
2024-12-02-04:25:18-root-INFO: grad norm: 325.222 320.300 56.369
2024-12-02-04:25:19-root-INFO: grad norm: 283.833 276.619 63.586
2024-12-02-04:25:20-root-INFO: grad norm: 266.807 262.452 48.010
2024-12-02-04:25:21-root-INFO: grad norm: 247.807 241.728 54.552
2024-12-02-04:25:22-root-INFO: Loss Change: 2003.057 -> 1979.436
2024-12-02-04:25:22-root-INFO: Regularization Change: 0.000 -> 0.041
2024-12-02-04:25:22-root-INFO: Undo step: 190
2024-12-02-04:25:22-root-INFO: Undo step: 191
2024-12-02-04:25:22-root-INFO: Undo step: 192
2024-12-02-04:25:22-root-INFO: Undo step: 193
2024-12-02-04:25:22-root-INFO: Undo step: 194
2024-12-02-04:25:22-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-04:25:22-root-INFO: grad norm: 1279.760 1193.056 463.036
2024-12-02-04:25:23-root-INFO: Loss too large (3944.869->4258.467)! Learning rate decreased to 0.00205.
2024-12-02-04:25:24-root-INFO: grad norm: 1410.857 1359.812 376.069
2024-12-02-04:25:25-root-INFO: grad norm: 909.655 888.524 194.927
2024-12-02-04:25:26-root-INFO: grad norm: 400.326 386.740 103.408
2024-12-02-04:25:27-root-INFO: grad norm: 279.978 267.079 84.004
2024-12-02-04:25:27-root-INFO: Loss Change: 3944.869 -> 2332.815
2024-12-02-04:25:27-root-INFO: Regularization Change: 0.000 -> 11.743
2024-12-02-04:25:27-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-04:25:27-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:25:28-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-04:25:28-root-INFO: grad norm: 272.677 262.456 73.959
2024-12-02-04:25:29-root-INFO: grad norm: 377.617 366.799 89.739
2024-12-02-04:25:29-root-INFO: Loss too large (2302.285->2430.471)! Learning rate decreased to 0.00214.
2024-12-02-04:25:30-root-INFO: Loss too large (2302.285->2322.966)! Learning rate decreased to 0.00171.
2024-12-02-04:25:31-root-INFO: grad norm: 381.735 370.730 91.000
2024-12-02-04:25:32-root-INFO: grad norm: 345.848 337.840 73.989
2024-12-02-04:25:33-root-INFO: grad norm: 301.950 293.990 68.875
2024-12-02-04:25:33-root-INFO: Loss Change: 2320.980 -> 2198.539
2024-12-02-04:25:33-root-INFO: Regularization Change: 0.000 -> 1.378
2024-12-02-04:25:33-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-04:25:33-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:25:34-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-04:25:34-root-INFO: grad norm: 270.797 265.217 54.690
2024-12-02-04:25:34-root-INFO: Loss too large (2154.067->2436.464)! Learning rate decreased to 0.00224.
2024-12-02-04:25:35-root-INFO: Loss too large (2154.067->2291.967)! Learning rate decreased to 0.00179.
2024-12-02-04:25:35-root-INFO: Loss too large (2154.067->2202.832)! Learning rate decreased to 0.00143.
2024-12-02-04:25:35-root-INFO: Loss too large (2154.067->2154.145)! Learning rate decreased to 0.00114.
2024-12-02-04:25:36-root-INFO: grad norm: 310.675 303.925 64.407
2024-12-02-04:25:37-root-INFO: Loss too large (2131.165->2143.945)! Learning rate decreased to 0.00092.
2024-12-02-04:25:38-root-INFO: grad norm: 244.014 239.888 44.681
2024-12-02-04:25:39-root-INFO: grad norm: 151.655 146.911 37.632
2024-12-02-04:25:40-root-INFO: grad norm: 164.961 160.903 36.365
2024-12-02-04:25:40-root-INFO: Loss Change: 2154.067 -> 2088.176
2024-12-02-04:25:40-root-INFO: Regularization Change: 0.000 -> 0.271
2024-12-02-04:25:40-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-04:25:40-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-04:25:41-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-04:25:41-root-INFO: grad norm: 364.813 358.705 66.481
2024-12-02-04:25:41-root-INFO: Loss too large (2079.450->2266.166)! Learning rate decreased to 0.00233.
2024-12-02-04:25:42-root-INFO: Loss too large (2079.450->2222.715)! Learning rate decreased to 0.00187.
2024-12-02-04:25:42-root-INFO: Loss too large (2079.450->2183.825)! Learning rate decreased to 0.00149.
2024-12-02-04:25:42-root-INFO: Loss too large (2079.450->2150.222)! Learning rate decreased to 0.00119.
2024-12-02-04:25:43-root-INFO: Loss too large (2079.450->2122.037)! Learning rate decreased to 0.00096.
2024-12-02-04:25:43-root-INFO: Loss too large (2079.450->2099.125)! Learning rate decreased to 0.00076.
2024-12-02-04:25:43-root-INFO: Loss too large (2079.450->2081.609)! Learning rate decreased to 0.00061.
2024-12-02-04:25:44-root-INFO: grad norm: 238.818 235.091 42.027
2024-12-02-04:25:45-root-INFO: grad norm: 120.595 116.407 31.504
2024-12-02-04:25:46-root-INFO: grad norm: 108.237 103.862 30.461
2024-12-02-04:25:47-root-INFO: grad norm: 99.088 94.954 28.320
2024-12-02-04:25:48-root-INFO: Loss Change: 2079.450 -> 2037.754
2024-12-02-04:25:48-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-02-04:25:48-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-04:25:48-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:25:48-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-04:25:49-root-INFO: grad norm: 357.835 349.124 78.477
2024-12-02-04:25:49-root-INFO: Loss too large (2045.525->2248.886)! Learning rate decreased to 0.00244.
2024-12-02-04:25:49-root-INFO: Loss too large (2045.525->2202.234)! Learning rate decreased to 0.00195.
2024-12-02-04:25:50-root-INFO: Loss too large (2045.525->2161.671)! Learning rate decreased to 0.00156.
2024-12-02-04:25:50-root-INFO: Loss too large (2045.525->2126.919)! Learning rate decreased to 0.00125.
2024-12-02-04:25:50-root-INFO: Loss too large (2045.525->2097.461)! Learning rate decreased to 0.00100.
2024-12-02-04:25:51-root-INFO: Loss too large (2045.525->2072.917)! Learning rate decreased to 0.00080.
2024-12-02-04:25:51-root-INFO: Loss too large (2045.525->2053.592)! Learning rate decreased to 0.00064.
2024-12-02-04:25:52-root-INFO: grad norm: 282.443 278.173 48.929
2024-12-02-04:25:53-root-INFO: grad norm: 192.709 187.523 44.407
2024-12-02-04:25:54-root-INFO: grad norm: 187.425 183.486 38.224
2024-12-02-04:25:55-root-INFO: grad norm: 184.757 180.793 38.070
2024-12-02-04:25:56-root-INFO: Loss Change: 2045.525 -> 2006.578
2024-12-02-04:25:56-root-INFO: Regularization Change: 0.000 -> 0.112
2024-12-02-04:25:56-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-04:25:56-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:25:56-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-04:25:56-root-INFO: grad norm: 149.933 143.741 42.644
2024-12-02-04:25:57-root-INFO: Loss too large (1991.971->2068.709)! Learning rate decreased to 0.00254.
2024-12-02-04:25:57-root-INFO: Loss too large (1991.971->2042.384)! Learning rate decreased to 0.00203.
2024-12-02-04:25:57-root-INFO: Loss too large (1991.971->2022.248)! Learning rate decreased to 0.00163.
2024-12-02-04:25:58-root-INFO: Loss too large (1991.971->2007.782)! Learning rate decreased to 0.00130.
2024-12-02-04:25:58-root-INFO: Loss too large (1991.971->1998.109)! Learning rate decreased to 0.00104.
2024-12-02-04:25:58-root-INFO: Loss too large (1991.971->1992.123)! Learning rate decreased to 0.00083.
2024-12-02-04:25:59-root-INFO: grad norm: 203.869 200.312 37.913
2024-12-02-04:26:00-root-INFO: Loss too large (1988.754->1994.169)! Learning rate decreased to 0.00067.
2024-12-02-04:26:01-root-INFO: grad norm: 253.246 249.266 44.722
2024-12-02-04:26:01-root-INFO: Loss too large (1985.867->1988.669)! Learning rate decreased to 0.00053.
2024-12-02-04:26:02-root-INFO: grad norm: 213.274 209.949 37.513
2024-12-02-04:26:03-root-INFO: grad norm: 172.714 169.533 32.997
2024-12-02-04:26:04-root-INFO: Loss Change: 1991.971 -> 1971.928
2024-12-02-04:26:04-root-INFO: Regularization Change: 0.000 -> 0.070
2024-12-02-04:26:04-root-INFO: Undo step: 190
2024-12-02-04:26:04-root-INFO: Undo step: 191
2024-12-02-04:26:04-root-INFO: Undo step: 192
2024-12-02-04:26:04-root-INFO: Undo step: 193
2024-12-02-04:26:04-root-INFO: Undo step: 194
2024-12-02-04:26:04-root-INFO: step: 195 lr_xt 0.00256175
2024-12-02-04:26:04-root-INFO: grad norm: 983.640 923.806 337.832
2024-12-02-04:26:05-root-INFO: grad norm: 874.948 847.098 218.994
2024-12-02-04:26:06-root-INFO: Loss too large (3104.731->3798.981)! Learning rate decreased to 0.00205.
2024-12-02-04:26:06-root-INFO: Loss too large (3104.731->3394.945)! Learning rate decreased to 0.00164.
2024-12-02-04:26:07-root-INFO: grad norm: 1080.049 1041.438 286.205
2024-12-02-04:26:07-root-INFO: Loss too large (3089.240->3111.571)! Learning rate decreased to 0.00131.
2024-12-02-04:26:08-root-INFO: grad norm: 759.700 722.329 235.340
2024-12-02-04:26:09-root-INFO: grad norm: 461.777 437.715 147.117
2024-12-02-04:26:10-root-INFO: Loss Change: 3968.776 -> 2525.346
2024-12-02-04:26:10-root-INFO: Regularization Change: 0.000 -> 9.618
2024-12-02-04:26:10-root-INFO: Learning rate of xt decay: 0.05713 -> 0.05782.
2024-12-02-04:26:10-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:26:10-root-INFO: step: 194 lr_xt 0.00267557
2024-12-02-04:26:11-root-INFO: grad norm: 481.578 460.712 140.219
2024-12-02-04:26:11-root-INFO: Loss too large (2530.596->2885.605)! Learning rate decreased to 0.00214.
2024-12-02-04:26:11-root-INFO: Loss too large (2530.596->2677.481)! Learning rate decreased to 0.00171.
2024-12-02-04:26:12-root-INFO: Loss too large (2530.596->2547.192)! Learning rate decreased to 0.00137.
2024-12-02-04:26:13-root-INFO: grad norm: 443.713 426.062 123.904
2024-12-02-04:26:14-root-INFO: grad norm: 534.125 519.428 124.436
2024-12-02-04:26:14-root-INFO: Loss too large (2456.978->2482.610)! Learning rate decreased to 0.00110.
2024-12-02-04:26:15-root-INFO: grad norm: 339.401 328.474 85.431
2024-12-02-04:26:16-root-INFO: grad norm: 185.946 176.762 57.716
2024-12-02-04:26:17-root-INFO: Loss Change: 2530.596 -> 2340.013
2024-12-02-04:26:17-root-INFO: Regularization Change: 0.000 -> 0.810
2024-12-02-04:26:17-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05851.
2024-12-02-04:26:17-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-04:26:17-root-INFO: step: 193 lr_xt 0.00279399
2024-12-02-04:26:18-root-INFO: grad norm: 381.634 374.195 74.980
2024-12-02-04:26:18-root-INFO: Loss too large (2335.700->2473.620)! Learning rate decreased to 0.00224.
2024-12-02-04:26:18-root-INFO: Loss too large (2335.700->2443.970)! Learning rate decreased to 0.00179.
2024-12-02-04:26:19-root-INFO: Loss too large (2335.700->2414.387)! Learning rate decreased to 0.00143.
2024-12-02-04:26:19-root-INFO: Loss too large (2335.700->2386.752)! Learning rate decreased to 0.00114.
2024-12-02-04:26:19-root-INFO: Loss too large (2335.700->2362.243)! Learning rate decreased to 0.00092.
2024-12-02-04:26:20-root-INFO: Loss too large (2335.700->2341.829)! Learning rate decreased to 0.00073.
2024-12-02-04:26:21-root-INFO: grad norm: 273.025 267.213 56.035
2024-12-02-04:26:22-root-INFO: grad norm: 148.837 141.964 44.706
2024-12-02-04:26:23-root-INFO: grad norm: 141.182 134.557 42.740
2024-12-02-04:26:24-root-INFO: grad norm: 135.327 128.952 41.047
2024-12-02-04:26:24-root-INFO: Loss Change: 2335.700 -> 2273.233
2024-12-02-04:26:24-root-INFO: Regularization Change: 0.000 -> 0.182
2024-12-02-04:26:24-root-INFO: Learning rate of xt decay: 0.05851 -> 0.05921.
2024-12-02-04:26:24-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-04:26:25-root-INFO: step: 192 lr_xt 0.00291718
2024-12-02-04:26:25-root-INFO: grad norm: 121.957 114.482 42.040
2024-12-02-04:26:26-root-INFO: grad norm: 219.078 209.198 65.047
2024-12-02-04:26:26-root-INFO: Loss too large (2232.378->2319.279)! Learning rate decreased to 0.00233.
2024-12-02-04:26:27-root-INFO: Loss too large (2232.378->2270.500)! Learning rate decreased to 0.00187.
2024-12-02-04:26:27-root-INFO: Loss too large (2232.378->2241.144)! Learning rate decreased to 0.00149.
2024-12-02-04:26:28-root-INFO: grad norm: 238.068 228.719 66.059
2024-12-02-04:26:29-root-INFO: grad norm: 316.366 308.149 71.636
2024-12-02-04:26:29-root-INFO: Loss too large (2219.577->2261.830)! Learning rate decreased to 0.00119.
2024-12-02-04:26:30-root-INFO: Loss too large (2219.577->2230.354)! Learning rate decreased to 0.00096.
2024-12-02-04:26:31-root-INFO: grad norm: 306.835 301.549 56.708
2024-12-02-04:26:31-root-INFO: Loss too large (2210.807->2215.983)! Learning rate decreased to 0.00076.
2024-12-02-04:26:32-root-INFO: Loss Change: 2249.310 -> 2196.128
2024-12-02-04:26:32-root-INFO: Regularization Change: 0.000 -> 0.509
2024-12-02-04:26:32-root-INFO: Learning rate of xt decay: 0.05921 -> 0.05992.
2024-12-02-04:26:32-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:26:32-root-INFO: step: 191 lr_xt 0.00304531
2024-12-02-04:26:32-root-INFO: grad norm: 675.089 664.301 120.204
2024-12-02-04:26:33-root-INFO: Loss too large (2245.242->2498.145)! Learning rate decreased to 0.00244.
2024-12-02-04:26:33-root-INFO: Loss too large (2245.242->2448.845)! Learning rate decreased to 0.00195.
2024-12-02-04:26:33-root-INFO: Loss too large (2245.242->2406.888)! Learning rate decreased to 0.00156.
2024-12-02-04:26:33-root-INFO: Loss too large (2245.242->2367.207)! Learning rate decreased to 0.00125.
2024-12-02-04:26:34-root-INFO: Loss too large (2245.242->2327.995)! Learning rate decreased to 0.00100.
2024-12-02-04:26:34-root-INFO: Loss too large (2245.242->2289.636)! Learning rate decreased to 0.00080.
2024-12-02-04:26:35-root-INFO: Loss too large (2245.242->2252.589)! Learning rate decreased to 0.00064.
2024-12-02-04:26:35-root-INFO: grad norm: 362.028 356.849 61.017
2024-12-02-04:26:36-root-INFO: grad norm: 94.907 88.162 35.139
2024-12-02-04:26:37-root-INFO: grad norm: 90.578 84.273 33.203
2024-12-02-04:26:38-root-INFO: grad norm: 87.646 81.686 31.770
2024-12-02-04:26:39-root-INFO: Loss Change: 2245.242 -> 2145.996
2024-12-02-04:26:39-root-INFO: Regularization Change: 0.000 -> 0.136
2024-12-02-04:26:39-root-INFO: Learning rate of xt decay: 0.05992 -> 0.06064.
2024-12-02-04:26:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:26:39-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-04:26:40-root-INFO: grad norm: 311.339 305.505 59.988
2024-12-02-04:26:40-root-INFO: Loss too large (2146.396->2345.902)! Learning rate decreased to 0.00254.
2024-12-02-04:26:40-root-INFO: Loss too large (2146.396->2305.361)! Learning rate decreased to 0.00203.
2024-12-02-04:26:41-root-INFO: Loss too large (2146.396->2268.584)! Learning rate decreased to 0.00163.
2024-12-02-04:26:41-root-INFO: Loss too large (2146.396->2234.895)! Learning rate decreased to 0.00130.
2024-12-02-04:26:41-root-INFO: Loss too large (2146.396->2204.132)! Learning rate decreased to 0.00104.
2024-12-02-04:26:42-root-INFO: Loss too large (2146.396->2177.667)! Learning rate decreased to 0.00083.
2024-12-02-04:26:42-root-INFO: Loss too large (2146.396->2157.354)! Learning rate decreased to 0.00067.
2024-12-02-04:26:43-root-INFO: grad norm: 290.105 285.897 49.231
2024-12-02-04:26:44-root-INFO: grad norm: 269.047 264.680 48.274
2024-12-02-04:26:45-root-INFO: grad norm: 264.597 260.512 46.316
2024-12-02-04:26:46-root-INFO: grad norm: 259.652 255.689 45.196
2024-12-02-04:26:47-root-INFO: Loss Change: 2146.396 -> 2120.346
2024-12-02-04:26:47-root-INFO: Regularization Change: 0.000 -> 0.116
2024-12-02-04:26:47-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-04:26:47-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:26:47-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-04:26:47-root-INFO: grad norm: 137.165 132.894 33.962
2024-12-02-04:26:48-root-INFO: Loss too large (2100.375->2205.723)! Learning rate decreased to 0.00265.
2024-12-02-04:26:48-root-INFO: Loss too large (2100.375->2162.827)! Learning rate decreased to 0.00212.
2024-12-02-04:26:48-root-INFO: Loss too large (2100.375->2134.118)! Learning rate decreased to 0.00170.
2024-12-02-04:26:49-root-INFO: Loss too large (2100.375->2115.990)! Learning rate decreased to 0.00136.
2024-12-02-04:26:49-root-INFO: Loss too large (2100.375->2105.214)! Learning rate decreased to 0.00109.
2024-12-02-04:26:50-root-INFO: grad norm: 265.508 261.363 46.732
2024-12-02-04:26:50-root-INFO: Loss too large (2099.244->2131.098)! Learning rate decreased to 0.00087.
2024-12-02-04:26:51-root-INFO: Loss too large (2099.244->2112.567)! Learning rate decreased to 0.00070.
2024-12-02-04:26:51-root-INFO: Loss too large (2099.244->2100.177)! Learning rate decreased to 0.00056.
2024-12-02-04:26:52-root-INFO: grad norm: 211.303 207.619 39.284
2024-12-02-04:26:53-root-INFO: grad norm: 164.559 161.099 33.566
2024-12-02-04:26:54-root-INFO: grad norm: 142.206 138.549 32.044
2024-12-02-04:26:55-root-INFO: Loss Change: 2100.375 -> 2076.900
2024-12-02-04:26:55-root-INFO: Regularization Change: 0.000 -> 0.080
2024-12-02-04:26:55-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-04:26:55-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:26:55-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-04:26:55-root-INFO: grad norm: 391.720 386.057 66.365
2024-12-02-04:26:56-root-INFO: Loss too large (2086.037->2346.223)! Learning rate decreased to 0.00277.
2024-12-02-04:26:56-root-INFO: Loss too large (2086.037->2300.393)! Learning rate decreased to 0.00222.
2024-12-02-04:26:56-root-INFO: Loss too large (2086.037->2259.860)! Learning rate decreased to 0.00177.
2024-12-02-04:26:57-root-INFO: Loss too large (2086.037->2222.282)! Learning rate decreased to 0.00142.
2024-12-02-04:26:57-root-INFO: Loss too large (2086.037->2186.665)! Learning rate decreased to 0.00113.
2024-12-02-04:26:57-root-INFO: Loss too large (2086.037->2152.416)! Learning rate decreased to 0.00091.
2024-12-02-04:26:58-root-INFO: Loss too large (2086.037->2120.973)! Learning rate decreased to 0.00073.
2024-12-02-04:26:58-root-INFO: Loss too large (2086.037->2095.606)! Learning rate decreased to 0.00058.
2024-12-02-04:26:59-root-INFO: grad norm: 317.108 313.117 50.152
2024-12-02-04:27:00-root-INFO: grad norm: 248.075 244.501 41.956
2024-12-02-04:27:01-root-INFO: grad norm: 228.687 225.081 40.452
2024-12-02-04:27:02-root-INFO: grad norm: 209.007 205.855 36.162
2024-12-02-04:27:03-root-INFO: Loss Change: 2086.037 -> 2050.169
2024-12-02-04:27:03-root-INFO: Regularization Change: 0.000 -> 0.086
2024-12-02-04:27:03-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-04:27:03-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-04:27:03-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-04:27:03-root-INFO: grad norm: 104.263 97.725 36.341
2024-12-02-04:27:04-root-INFO: grad norm: 248.092 242.503 52.363
2024-12-02-04:27:05-root-INFO: Loss too large (2033.081->2518.058)! Learning rate decreased to 0.00289.
2024-12-02-04:27:05-root-INFO: Loss too large (2033.081->2362.614)! Learning rate decreased to 0.00231.
2024-12-02-04:27:05-root-INFO: Loss too large (2033.081->2246.401)! Learning rate decreased to 0.00185.
2024-12-02-04:27:06-root-INFO: Loss too large (2033.081->2162.060)! Learning rate decreased to 0.00148.
2024-12-02-04:27:06-root-INFO: Loss too large (2033.081->2103.656)! Learning rate decreased to 0.00118.
2024-12-02-04:27:06-root-INFO: Loss too large (2033.081->2065.542)! Learning rate decreased to 0.00095.
2024-12-02-04:27:07-root-INFO: Loss too large (2033.081->2042.325)! Learning rate decreased to 0.00076.
2024-12-02-04:27:08-root-INFO: grad norm: 310.279 306.598 47.655
2024-12-02-04:27:08-root-INFO: Loss too large (2029.311->2040.892)! Learning rate decreased to 0.00061.
2024-12-02-04:27:09-root-INFO: grad norm: 287.543 283.664 47.071
2024-12-02-04:27:10-root-INFO: grad norm: 264.841 261.593 41.351
2024-12-02-04:27:11-root-INFO: Loss Change: 2048.121 -> 2017.026
2024-12-02-04:27:11-root-INFO: Regularization Change: 0.000 -> 0.272
2024-12-02-04:27:11-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-04:27:11-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:27:11-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-04:27:11-root-INFO: grad norm: 111.407 107.826 28.021
2024-12-02-04:27:12-root-INFO: Loss too large (1992.025->2059.946)! Learning rate decreased to 0.00301.
2024-12-02-04:27:12-root-INFO: Loss too large (1992.025->2030.664)! Learning rate decreased to 0.00241.
2024-12-02-04:27:12-root-INFO: Loss too large (1992.025->2011.825)! Learning rate decreased to 0.00193.
2024-12-02-04:27:13-root-INFO: Loss too large (1992.025->2000.274)! Learning rate decreased to 0.00154.
2024-12-02-04:27:13-root-INFO: Loss too large (1992.025->1993.571)! Learning rate decreased to 0.00123.
2024-12-02-04:27:14-root-INFO: grad norm: 225.057 222.197 35.766
2024-12-02-04:27:14-root-INFO: Loss too large (1989.959->2030.943)! Learning rate decreased to 0.00099.
2024-12-02-04:27:15-root-INFO: Loss too large (1989.959->2010.956)! Learning rate decreased to 0.00079.
2024-12-02-04:27:15-root-INFO: Loss too large (1989.959->1997.339)! Learning rate decreased to 0.00063.
2024-12-02-04:27:16-root-INFO: grad norm: 244.403 240.988 40.714
2024-12-02-04:27:17-root-INFO: grad norm: 269.067 266.015 40.412
2024-12-02-04:27:17-root-INFO: Loss too large (1986.396->1986.738)! Learning rate decreased to 0.00051.
2024-12-02-04:27:18-root-INFO: grad norm: 200.623 197.421 35.696
2024-12-02-04:27:19-root-INFO: Loss Change: 1992.025 -> 1974.187
2024-12-02-04:27:19-root-INFO: Regularization Change: 0.000 -> 0.071
2024-12-02-04:27:19-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-04:27:19-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:27:19-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-04:27:20-root-INFO: grad norm: 297.292 293.876 44.939
2024-12-02-04:27:20-root-INFO: Loss too large (1972.056->2202.476)! Learning rate decreased to 0.00314.
2024-12-02-04:27:20-root-INFO: Loss too large (1972.056->2174.586)! Learning rate decreased to 0.00251.
2024-12-02-04:27:21-root-INFO: Loss too large (1972.056->2144.727)! Learning rate decreased to 0.00201.
2024-12-02-04:27:21-root-INFO: Loss too large (1972.056->2113.032)! Learning rate decreased to 0.00161.
2024-12-02-04:27:21-root-INFO: Loss too large (1972.056->2079.634)! Learning rate decreased to 0.00129.
2024-12-02-04:27:22-root-INFO: Loss too large (1972.056->2045.734)! Learning rate decreased to 0.00103.
2024-12-02-04:27:22-root-INFO: Loss too large (1972.056->2015.073)! Learning rate decreased to 0.00082.
2024-12-02-04:27:22-root-INFO: Loss too large (1972.056->1991.132)! Learning rate decreased to 0.00066.
2024-12-02-04:27:23-root-INFO: Loss too large (1972.056->1974.847)! Learning rate decreased to 0.00053.
2024-12-02-04:27:24-root-INFO: grad norm: 239.295 236.206 38.323
2024-12-02-04:27:25-root-INFO: grad norm: 194.686 192.136 31.410
2024-12-02-04:27:26-root-INFO: grad norm: 168.760 165.839 31.265
2024-12-02-04:27:27-root-INFO: grad norm: 147.320 144.969 26.214
2024-12-02-04:27:27-root-INFO: Loss Change: 1972.056 -> 1948.071
2024-12-02-04:27:27-root-INFO: Regularization Change: 0.000 -> 0.049
2024-12-02-04:27:27-root-INFO: Undo step: 185
2024-12-02-04:27:27-root-INFO: Undo step: 186
2024-12-02-04:27:27-root-INFO: Undo step: 187
2024-12-02-04:27:27-root-INFO: Undo step: 188
2024-12-02-04:27:27-root-INFO: Undo step: 189
2024-12-02-04:27:28-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-04:27:28-root-INFO: grad norm: 851.326 775.200 351.881
2024-12-02-04:27:29-root-INFO: grad norm: 674.005 642.374 204.055
2024-12-02-04:27:30-root-INFO: grad norm: 388.592 360.183 145.851
2024-12-02-04:27:31-root-INFO: grad norm: 404.790 390.907 105.100
2024-12-02-04:27:32-root-INFO: grad norm: 565.028 547.636 139.108
2024-12-02-04:27:32-root-INFO: Loss too large (2436.084->2611.045)! Learning rate decreased to 0.00254.
2024-12-02-04:27:33-root-INFO: Loss too large (2436.084->2444.580)! Learning rate decreased to 0.00203.
2024-12-02-04:27:33-root-INFO: Loss Change: 3456.854 -> 2343.592
2024-12-02-04:27:33-root-INFO: Regularization Change: 0.000 -> 10.216
2024-12-02-04:27:33-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-04:27:33-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:27:34-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-04:27:34-root-INFO: grad norm: 419.516 406.433 103.948
2024-12-02-04:27:34-root-INFO: Loss too large (2310.970->2696.751)! Learning rate decreased to 0.00265.
2024-12-02-04:27:35-root-INFO: Loss too large (2310.970->2461.784)! Learning rate decreased to 0.00212.
2024-12-02-04:27:35-root-INFO: Loss too large (2310.970->2321.802)! Learning rate decreased to 0.00170.
2024-12-02-04:27:36-root-INFO: grad norm: 514.256 506.383 89.645
2024-12-02-04:27:36-root-INFO: Loss too large (2247.706->2297.903)! Learning rate decreased to 0.00136.
2024-12-02-04:27:37-root-INFO: Loss too large (2247.706->2259.304)! Learning rate decreased to 0.00109.
2024-12-02-04:27:38-root-INFO: grad norm: 279.458 274.978 49.836
2024-12-02-04:27:39-root-INFO: grad norm: 138.429 132.846 38.919
2024-12-02-04:27:40-root-INFO: grad norm: 136.338 130.489 39.507
2024-12-02-04:27:40-root-INFO: Loss Change: 2310.970 -> 2134.666
2024-12-02-04:27:40-root-INFO: Regularization Change: 0.000 -> 0.667
2024-12-02-04:27:40-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-04:27:40-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:27:41-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-04:27:41-root-INFO: grad norm: 127.320 120.536 41.006
2024-12-02-04:27:42-root-INFO: grad norm: 272.956 266.360 59.641
2024-12-02-04:27:42-root-INFO: Loss too large (2100.230->2559.330)! Learning rate decreased to 0.00277.
2024-12-02-04:27:43-root-INFO: Loss too large (2100.230->2392.556)! Learning rate decreased to 0.00222.
2024-12-02-04:27:43-root-INFO: Loss too large (2100.230->2271.715)! Learning rate decreased to 0.00177.
2024-12-02-04:27:43-root-INFO: Loss too large (2100.230->2188.607)! Learning rate decreased to 0.00142.
2024-12-02-04:27:44-root-INFO: Loss too large (2100.230->2135.364)! Learning rate decreased to 0.00113.
2024-12-02-04:27:44-root-INFO: Loss too large (2100.230->2104.068)! Learning rate decreased to 0.00091.
2024-12-02-04:27:45-root-INFO: grad norm: 328.333 324.217 51.826
2024-12-02-04:27:45-root-INFO: Loss too large (2087.552->2106.266)! Learning rate decreased to 0.00073.
2024-12-02-04:27:45-root-INFO: Loss too large (2087.552->2089.548)! Learning rate decreased to 0.00058.
2024-12-02-04:27:46-root-INFO: grad norm: 234.349 230.862 40.276
2024-12-02-04:27:48-root-INFO: grad norm: 154.059 149.966 35.277
2024-12-02-04:27:48-root-INFO: Loss Change: 2118.417 -> 2061.936
2024-12-02-04:27:48-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-02-04:27:48-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-04:27:48-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-04:27:49-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-04:27:49-root-INFO: grad norm: 146.298 140.493 40.801
2024-12-02-04:27:49-root-INFO: Loss too large (2054.446->2140.671)! Learning rate decreased to 0.00289.
2024-12-02-04:27:50-root-INFO: Loss too large (2054.446->2113.844)! Learning rate decreased to 0.00231.
2024-12-02-04:27:50-root-INFO: Loss too large (2054.446->2092.051)! Learning rate decreased to 0.00185.
2024-12-02-04:27:50-root-INFO: Loss too large (2054.446->2075.613)! Learning rate decreased to 0.00148.
2024-12-02-04:27:51-root-INFO: Loss too large (2054.446->2064.152)! Learning rate decreased to 0.00118.
2024-12-02-04:27:51-root-INFO: Loss too large (2054.446->2056.776)! Learning rate decreased to 0.00095.
2024-12-02-04:27:52-root-INFO: grad norm: 218.340 215.202 36.887
2024-12-02-04:27:52-root-INFO: Loss too large (2052.430->2063.155)! Learning rate decreased to 0.00076.
2024-12-02-04:27:53-root-INFO: grad norm: 296.180 291.951 49.877
2024-12-02-04:27:54-root-INFO: Loss too large (2050.942->2059.483)! Learning rate decreased to 0.00061.
2024-12-02-04:27:55-root-INFO: grad norm: 260.737 257.665 39.905
2024-12-02-04:27:55-root-INFO: grad norm: 224.343 220.558 41.037
2024-12-02-04:27:56-root-INFO: Loss Change: 2054.446 -> 2035.657
2024-12-02-04:27:56-root-INFO: Regularization Change: 0.000 -> 0.094
2024-12-02-04:27:56-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-04:27:56-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:27:56-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-04:27:57-root-INFO: grad norm: 97.708 92.324 31.987
2024-12-02-04:27:57-root-INFO: Loss too large (2014.590->2021.468)! Learning rate decreased to 0.00301.
2024-12-02-04:27:58-root-INFO: grad norm: 332.981 329.373 48.885
2024-12-02-04:27:58-root-INFO: Loss too large (2013.015->2221.605)! Learning rate decreased to 0.00241.
2024-12-02-04:27:59-root-INFO: Loss too large (2013.015->2183.751)! Learning rate decreased to 0.00193.
2024-12-02-04:27:59-root-INFO: Loss too large (2013.015->2147.408)! Learning rate decreased to 0.00154.
2024-12-02-04:27:59-root-INFO: Loss too large (2013.015->2112.379)! Learning rate decreased to 0.00123.
2024-12-02-04:28:00-root-INFO: Loss too large (2013.015->2079.449)! Learning rate decreased to 0.00099.
2024-12-02-04:28:00-root-INFO: Loss too large (2013.015->2050.579)! Learning rate decreased to 0.00079.
2024-12-02-04:28:00-root-INFO: Loss too large (2013.015->2027.827)! Learning rate decreased to 0.00063.
2024-12-02-04:28:01-root-INFO: grad norm: 323.038 319.665 46.561
2024-12-02-04:28:02-root-INFO: grad norm: 334.932 331.620 46.987
2024-12-02-04:28:03-root-INFO: Loss too large (2003.328->2008.503)! Learning rate decreased to 0.00051.
2024-12-02-04:28:04-root-INFO: grad norm: 257.105 254.172 38.729
2024-12-02-04:28:04-root-INFO: Loss Change: 2014.590 -> 1988.284
2024-12-02-04:28:04-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-02-04:28:04-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-04:28:04-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:28:05-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-04:28:05-root-INFO: grad norm: 341.579 338.003 49.295
2024-12-02-04:28:05-root-INFO: Loss too large (1984.635->2227.189)! Learning rate decreased to 0.00314.
2024-12-02-04:28:06-root-INFO: Loss too large (1984.635->2201.067)! Learning rate decreased to 0.00251.
2024-12-02-04:28:06-root-INFO: Loss too large (1984.635->2171.714)! Learning rate decreased to 0.00201.
2024-12-02-04:28:06-root-INFO: Loss too large (1984.635->2139.772)! Learning rate decreased to 0.00161.
2024-12-02-04:28:07-root-INFO: Loss too large (1984.635->2106.275)! Learning rate decreased to 0.00129.
2024-12-02-04:28:07-root-INFO: Loss too large (1984.635->2072.464)! Learning rate decreased to 0.00103.
2024-12-02-04:28:07-root-INFO: Loss too large (1984.635->2040.535)! Learning rate decreased to 0.00082.
2024-12-02-04:28:08-root-INFO: Loss too large (1984.635->2013.374)! Learning rate decreased to 0.00066.
2024-12-02-04:28:08-root-INFO: Loss too large (1984.635->1993.006)! Learning rate decreased to 0.00053.
2024-12-02-04:28:09-root-INFO: grad norm: 282.944 280.015 40.607
2024-12-02-04:28:10-root-INFO: grad norm: 231.320 228.337 37.032
2024-12-02-04:28:11-root-INFO: grad norm: 213.023 210.295 33.988
2024-12-02-04:28:12-root-INFO: grad norm: 196.782 193.971 33.139
2024-12-02-04:28:13-root-INFO: Loss Change: 1984.635 -> 1961.404
2024-12-02-04:28:13-root-INFO: Regularization Change: 0.000 -> 0.052
2024-12-02-04:28:13-root-INFO: Undo step: 185
2024-12-02-04:28:13-root-INFO: Undo step: 186
2024-12-02-04:28:13-root-INFO: Undo step: 187
2024-12-02-04:28:13-root-INFO: Undo step: 188
2024-12-02-04:28:13-root-INFO: Undo step: 189
2024-12-02-04:28:13-root-INFO: step: 190 lr_xt 0.00317856
2024-12-02-04:28:13-root-INFO: grad norm: 630.827 578.416 251.750
2024-12-02-04:28:14-root-INFO: grad norm: 666.165 646.331 161.345
2024-12-02-04:28:15-root-INFO: Loss too large (2861.880->3544.040)! Learning rate decreased to 0.00254.
2024-12-02-04:28:15-root-INFO: Loss too large (2861.880->3225.855)! Learning rate decreased to 0.00203.
2024-12-02-04:28:15-root-INFO: Loss too large (2861.880->2990.775)! Learning rate decreased to 0.00163.
2024-12-02-04:28:16-root-INFO: grad norm: 803.879 774.665 214.747
2024-12-02-04:28:17-root-INFO: grad norm: 834.641 806.413 215.232
2024-12-02-04:28:18-root-INFO: grad norm: 691.089 663.199 194.346
2024-12-02-04:28:19-root-INFO: Loss Change: 3209.493 -> 2531.678
2024-12-02-04:28:19-root-INFO: Regularization Change: 0.000 -> 6.634
2024-12-02-04:28:19-root-INFO: Learning rate of xt decay: 0.06064 -> 0.06137.
2024-12-02-04:28:19-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:28:19-root-INFO: step: 189 lr_xt 0.00331709
2024-12-02-04:28:20-root-INFO: grad norm: 501.702 485.867 125.054
2024-12-02-04:28:20-root-INFO: Loss too large (2501.150->2806.193)! Learning rate decreased to 0.00265.
2024-12-02-04:28:20-root-INFO: Loss too large (2501.150->2615.609)! Learning rate decreased to 0.00212.
2024-12-02-04:28:21-root-INFO: grad norm: 548.263 526.050 154.480
2024-12-02-04:28:22-root-INFO: grad norm: 548.103 530.518 137.723
2024-12-02-04:28:23-root-INFO: grad norm: 499.964 479.893 140.237
2024-12-02-04:28:24-root-INFO: grad norm: 470.785 457.945 109.202
2024-12-02-04:28:25-root-INFO: Loss Change: 2501.150 -> 2298.183
2024-12-02-04:28:25-root-INFO: Regularization Change: 0.000 -> 2.989
2024-12-02-04:28:25-root-INFO: Learning rate of xt decay: 0.06137 -> 0.06211.
2024-12-02-04:28:25-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-04:28:25-root-INFO: step: 188 lr_xt 0.00346111
2024-12-02-04:28:26-root-INFO: grad norm: 461.843 444.551 125.191
2024-12-02-04:28:26-root-INFO: Loss too large (2294.004->2592.198)! Learning rate decreased to 0.00277.
2024-12-02-04:28:26-root-INFO: Loss too large (2294.004->2355.277)! Learning rate decreased to 0.00222.
2024-12-02-04:28:27-root-INFO: grad norm: 467.809 458.660 92.064
2024-12-02-04:28:28-root-INFO: Loss too large (2223.006->2245.667)! Learning rate decreased to 0.00177.
2024-12-02-04:28:29-root-INFO: grad norm: 337.991 329.268 76.291
2024-12-02-04:28:30-root-INFO: grad norm: 249.669 244.920 48.460
2024-12-02-04:28:31-root-INFO: grad norm: 278.501 273.787 51.028
2024-12-02-04:28:31-root-INFO: Loss too large (2086.363->2104.823)! Learning rate decreased to 0.00142.
2024-12-02-04:28:31-root-INFO: Loss Change: 2294.004 -> 2065.041
2024-12-02-04:28:31-root-INFO: Regularization Change: 0.000 -> 1.409
2024-12-02-04:28:31-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06285.
2024-12-02-04:28:32-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-04:28:32-root-INFO: step: 187 lr_xt 0.00361079
2024-12-02-04:28:32-root-INFO: grad norm: 582.274 575.791 86.647
2024-12-02-04:28:32-root-INFO: Loss too large (2087.344->2276.717)! Learning rate decreased to 0.00289.
2024-12-02-04:28:33-root-INFO: Loss too large (2087.344->2251.002)! Learning rate decreased to 0.00231.
2024-12-02-04:28:33-root-INFO: Loss too large (2087.344->2221.784)! Learning rate decreased to 0.00185.
2024-12-02-04:28:33-root-INFO: Loss too large (2087.344->2187.949)! Learning rate decreased to 0.00148.
2024-12-02-04:28:34-root-INFO: Loss too large (2087.344->2151.598)! Learning rate decreased to 0.00118.
2024-12-02-04:28:34-root-INFO: Loss too large (2087.344->2115.722)! Learning rate decreased to 0.00095.
2024-12-02-04:28:35-root-INFO: grad norm: 279.118 276.801 35.890
2024-12-02-04:28:36-root-INFO: grad norm: 175.598 172.376 33.488
2024-12-02-04:28:37-root-INFO: grad norm: 178.195 174.264 37.224
2024-12-02-04:28:38-root-INFO: grad norm: 200.971 198.215 33.168
2024-12-02-04:28:39-root-INFO: Loss Change: 2087.344 -> 1997.786
2024-12-02-04:28:39-root-INFO: Regularization Change: 0.000 -> 0.184
2024-12-02-04:28:39-root-INFO: Learning rate of xt decay: 0.06285 -> 0.06361.
2024-12-02-04:28:39-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:28:39-root-INFO: step: 186 lr_xt 0.00376634
2024-12-02-04:28:39-root-INFO: grad norm: 477.928 472.245 73.480
2024-12-02-04:28:40-root-INFO: Loss too large (2004.847->2225.661)! Learning rate decreased to 0.00301.
2024-12-02-04:28:40-root-INFO: Loss too large (2004.847->2196.298)! Learning rate decreased to 0.00241.
2024-12-02-04:28:40-root-INFO: Loss too large (2004.847->2165.295)! Learning rate decreased to 0.00193.
2024-12-02-04:28:41-root-INFO: Loss too large (2004.847->2132.090)! Learning rate decreased to 0.00154.
2024-12-02-04:28:41-root-INFO: Loss too large (2004.847->2098.059)! Learning rate decreased to 0.00123.
2024-12-02-04:28:41-root-INFO: Loss too large (2004.847->2064.884)! Learning rate decreased to 0.00099.
2024-12-02-04:28:42-root-INFO: Loss too large (2004.847->2034.164)! Learning rate decreased to 0.00079.
2024-12-02-04:28:42-root-INFO: Loss too large (2004.847->2007.671)! Learning rate decreased to 0.00063.
2024-12-02-04:28:43-root-INFO: grad norm: 279.676 276.913 39.217
2024-12-02-04:28:44-root-INFO: grad norm: 113.304 109.194 30.239
2024-12-02-04:28:45-root-INFO: grad norm: 104.095 100.195 28.230
2024-12-02-04:28:46-root-INFO: grad norm: 98.036 93.975 27.924
2024-12-02-04:28:47-root-INFO: Loss Change: 2004.847 -> 1947.524
2024-12-02-04:28:47-root-INFO: Regularization Change: 0.000 -> 0.095
2024-12-02-04:28:47-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06437.
2024-12-02-04:28:47-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:28:47-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-04:28:47-root-INFO: grad norm: 111.171 106.478 31.959
2024-12-02-04:28:48-root-INFO: Loss too large (1930.692->1983.168)! Learning rate decreased to 0.00314.
2024-12-02-04:28:48-root-INFO: Loss too large (1930.692->1964.011)! Learning rate decreased to 0.00251.
2024-12-02-04:28:48-root-INFO: Loss too large (1930.692->1949.707)! Learning rate decreased to 0.00201.
2024-12-02-04:28:49-root-INFO: Loss too large (1930.692->1939.805)! Learning rate decreased to 0.00161.
2024-12-02-04:28:49-root-INFO: Loss too large (1930.692->1933.447)! Learning rate decreased to 0.00129.
2024-12-02-04:28:50-root-INFO: grad norm: 218.224 215.567 33.949
2024-12-02-04:28:50-root-INFO: Loss too large (1929.685->1977.358)! Learning rate decreased to 0.00103.
2024-12-02-04:28:51-root-INFO: Loss too large (1929.685->1949.861)! Learning rate decreased to 0.00082.
2024-12-02-04:28:51-root-INFO: Loss too large (1929.685->1933.633)! Learning rate decreased to 0.00066.
2024-12-02-04:28:52-root-INFO: grad norm: 232.450 229.440 37.289
2024-12-02-04:28:52-root-INFO: Loss too large (1924.981->1925.048)! Learning rate decreased to 0.00053.
2024-12-02-04:28:53-root-INFO: grad norm: 179.066 176.399 30.793
2024-12-02-04:28:54-root-INFO: grad norm: 136.439 133.512 28.106
2024-12-02-04:28:55-root-INFO: Loss Change: 1930.692 -> 1911.188
2024-12-02-04:28:55-root-INFO: Regularization Change: 0.000 -> 0.072
2024-12-02-04:28:55-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-04:28:55-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:28:55-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-04:28:56-root-INFO: grad norm: 254.047 248.853 51.110
2024-12-02-04:28:56-root-INFO: Loss too large (1910.320->2110.471)! Learning rate decreased to 0.00328.
2024-12-02-04:28:56-root-INFO: Loss too large (1910.320->2079.488)! Learning rate decreased to 0.00262.
2024-12-02-04:28:57-root-INFO: Loss too large (1910.320->2048.664)! Learning rate decreased to 0.00210.
2024-12-02-04:28:57-root-INFO: Loss too large (1910.320->2018.414)! Learning rate decreased to 0.00168.
2024-12-02-04:28:57-root-INFO: Loss too large (1910.320->1989.476)! Learning rate decreased to 0.00134.
2024-12-02-04:28:58-root-INFO: Loss too large (1910.320->1963.097)! Learning rate decreased to 0.00107.
2024-12-02-04:28:58-root-INFO: Loss too large (1910.320->1940.898)! Learning rate decreased to 0.00086.
2024-12-02-04:28:58-root-INFO: Loss too large (1910.320->1924.032)! Learning rate decreased to 0.00069.
2024-12-02-04:28:59-root-INFO: Loss too large (1910.320->1912.553)! Learning rate decreased to 0.00055.
2024-12-02-04:29:00-root-INFO: grad norm: 231.201 227.938 38.711
2024-12-02-04:29:01-root-INFO: grad norm: 210.799 206.814 40.793
2024-12-02-04:29:02-root-INFO: grad norm: 202.484 199.552 34.331
2024-12-02-04:29:03-root-INFO: grad norm: 195.642 192.218 36.441
2024-12-02-04:29:03-root-INFO: Loss Change: 1910.320 -> 1888.904
2024-12-02-04:29:03-root-INFO: Regularization Change: 0.000 -> 0.065
2024-12-02-04:29:03-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-04:29:03-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:29:04-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-04:29:04-root-INFO: grad norm: 96.971 90.891 33.797
2024-12-02-04:29:04-root-INFO: Loss too large (1877.326->1880.058)! Learning rate decreased to 0.00342.
2024-12-02-04:29:05-root-INFO: grad norm: 329.719 326.862 43.310
2024-12-02-04:29:06-root-INFO: Loss too large (1872.645->2096.654)! Learning rate decreased to 0.00273.
2024-12-02-04:29:06-root-INFO: Loss too large (1872.645->2062.657)! Learning rate decreased to 0.00219.
2024-12-02-04:29:06-root-INFO: Loss too large (1872.645->2029.664)! Learning rate decreased to 0.00175.
2024-12-02-04:29:07-root-INFO: Loss too large (1872.645->1997.118)! Learning rate decreased to 0.00140.
2024-12-02-04:29:07-root-INFO: Loss too large (1872.645->1965.128)! Learning rate decreased to 0.00112.
2024-12-02-04:29:07-root-INFO: Loss too large (1872.645->1934.573)! Learning rate decreased to 0.00090.
2024-12-02-04:29:08-root-INFO: Loss too large (1872.645->1907.480)! Learning rate decreased to 0.00072.
2024-12-02-04:29:08-root-INFO: Loss too large (1872.645->1886.115)! Learning rate decreased to 0.00057.
2024-12-02-04:29:09-root-INFO: grad norm: 331.337 328.221 45.333
2024-12-02-04:29:10-root-INFO: grad norm: 348.325 345.326 45.611
2024-12-02-04:29:10-root-INFO: Loss too large (1865.212->1869.131)! Learning rate decreased to 0.00046.
2024-12-02-04:29:11-root-INFO: grad norm: 262.927 260.248 37.436
2024-12-02-04:29:12-root-INFO: Loss Change: 1877.326 -> 1850.324
2024-12-02-04:29:12-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-02-04:29:12-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-04:29:12-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:29:12-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-04:29:13-root-INFO: grad norm: 444.226 440.032 60.896
2024-12-02-04:29:13-root-INFO: Loss too large (1862.761->2111.739)! Learning rate decreased to 0.00356.
2024-12-02-04:29:13-root-INFO: Loss too large (1862.761->2094.819)! Learning rate decreased to 0.00285.
2024-12-02-04:29:14-root-INFO: Loss too large (1862.761->2076.329)! Learning rate decreased to 0.00228.
2024-12-02-04:29:14-root-INFO: Loss too large (1862.761->2053.921)! Learning rate decreased to 0.00182.
2024-12-02-04:29:14-root-INFO: Loss too large (1862.761->2027.118)! Learning rate decreased to 0.00146.
2024-12-02-04:29:15-root-INFO: Loss too large (1862.761->1996.671)! Learning rate decreased to 0.00117.
2024-12-02-04:29:15-root-INFO: Loss too large (1862.761->1963.586)! Learning rate decreased to 0.00093.
2024-12-02-04:29:15-root-INFO: Loss too large (1862.761->1929.230)! Learning rate decreased to 0.00075.
2024-12-02-04:29:16-root-INFO: Loss too large (1862.761->1896.415)! Learning rate decreased to 0.00060.
2024-12-02-04:29:16-root-INFO: Loss too large (1862.761->1869.224)! Learning rate decreased to 0.00048.
2024-12-02-04:29:17-root-INFO: grad norm: 330.537 327.741 42.901
2024-12-02-04:29:18-root-INFO: grad norm: 252.602 249.911 36.777
2024-12-02-04:29:19-root-INFO: grad norm: 223.724 221.243 33.221
2024-12-02-04:29:20-root-INFO: grad norm: 199.022 196.643 30.682
2024-12-02-04:29:21-root-INFO: Loss Change: 1862.761 -> 1828.784
2024-12-02-04:29:21-root-INFO: Regularization Change: 0.000 -> 0.053
2024-12-02-04:29:21-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-04:29:21-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-04:29:21-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-04:29:21-root-INFO: grad norm: 129.250 125.854 29.437
2024-12-02-04:29:22-root-INFO: Loss too large (1818.908->1958.826)! Learning rate decreased to 0.00371.
2024-12-02-04:29:22-root-INFO: Loss too large (1818.908->1929.724)! Learning rate decreased to 0.00297.
2024-12-02-04:29:22-root-INFO: Loss too large (1818.908->1901.808)! Learning rate decreased to 0.00238.
2024-12-02-04:29:23-root-INFO: Loss too large (1818.908->1876.711)! Learning rate decreased to 0.00190.
2024-12-02-04:29:23-root-INFO: Loss too large (1818.908->1856.098)! Learning rate decreased to 0.00152.
2024-12-02-04:29:23-root-INFO: Loss too large (1818.908->1840.683)! Learning rate decreased to 0.00122.
2024-12-02-04:29:24-root-INFO: Loss too large (1818.908->1830.093)! Learning rate decreased to 0.00097.
2024-12-02-04:29:24-root-INFO: Loss too large (1818.908->1823.344)! Learning rate decreased to 0.00078.
2024-12-02-04:29:24-root-INFO: Loss too large (1818.908->1819.349)! Learning rate decreased to 0.00062.
2024-12-02-04:29:25-root-INFO: grad norm: 176.322 173.793 29.761
2024-12-02-04:29:26-root-INFO: Loss too large (1817.182->1818.474)! Learning rate decreased to 0.00050.
2024-12-02-04:29:27-root-INFO: grad norm: 184.734 182.044 31.408
2024-12-02-04:29:28-root-INFO: grad norm: 193.211 190.822 30.285
2024-12-02-04:29:29-root-INFO: grad norm: 202.642 200.106 31.953
2024-12-02-04:29:29-root-INFO: Loss Change: 1818.908 -> 1809.401
2024-12-02-04:29:30-root-INFO: Regularization Change: 0.000 -> 0.040
2024-12-02-04:29:30-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-04:29:30-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:29:30-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-04:29:30-root-INFO: grad norm: 106.285 101.094 32.809
2024-12-02-04:29:31-root-INFO: Loss too large (1803.666->1874.307)! Learning rate decreased to 0.00387.
2024-12-02-04:29:31-root-INFO: Loss too large (1803.666->1850.356)! Learning rate decreased to 0.00309.
2024-12-02-04:29:31-root-INFO: Loss too large (1803.666->1831.746)! Learning rate decreased to 0.00248.
2024-12-02-04:29:32-root-INFO: Loss too large (1803.666->1818.457)! Learning rate decreased to 0.00198.
2024-12-02-04:29:32-root-INFO: Loss too large (1803.666->1809.706)! Learning rate decreased to 0.00158.
2024-12-02-04:29:32-root-INFO: Loss too large (1803.666->1804.387)! Learning rate decreased to 0.00127.
2024-12-02-04:29:33-root-INFO: grad norm: 253.191 250.381 37.619
2024-12-02-04:29:34-root-INFO: Loss too large (1801.437->1890.218)! Learning rate decreased to 0.00101.
2024-12-02-04:29:34-root-INFO: Loss too large (1801.437->1852.724)! Learning rate decreased to 0.00081.
2024-12-02-04:29:34-root-INFO: Loss too large (1801.437->1826.894)! Learning rate decreased to 0.00065.
2024-12-02-04:29:34-root-INFO: Loss too large (1801.437->1810.370)! Learning rate decreased to 0.00052.
2024-12-02-04:29:35-root-INFO: grad norm: 287.559 284.303 43.153
2024-12-02-04:29:36-root-INFO: Loss too large (1800.641->1800.781)! Learning rate decreased to 0.00042.
2024-12-02-04:29:37-root-INFO: grad norm: 216.390 213.801 33.372
2024-12-02-04:29:38-root-INFO: grad norm: 170.468 167.659 30.819
2024-12-02-04:29:38-root-INFO: Loss Change: 1803.666 -> 1786.306
2024-12-02-04:29:38-root-INFO: Regularization Change: 0.000 -> 0.059
2024-12-02-04:29:38-root-INFO: Undo step: 180
2024-12-02-04:29:38-root-INFO: Undo step: 181
2024-12-02-04:29:39-root-INFO: Undo step: 182
2024-12-02-04:29:39-root-INFO: Undo step: 183
2024-12-02-04:29:39-root-INFO: Undo step: 184
2024-12-02-04:29:39-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-04:29:39-root-INFO: grad norm: 674.810 647.836 188.884
2024-12-02-04:29:40-root-INFO: grad norm: 654.054 633.756 161.676
2024-12-02-04:29:40-root-INFO: Loss too large (2608.797->2731.701)! Learning rate decreased to 0.00314.
2024-12-02-04:29:41-root-INFO: grad norm: 518.060 504.418 118.105
2024-12-02-04:29:42-root-INFO: grad norm: 344.312 334.771 80.490
2024-12-02-04:29:43-root-INFO: grad norm: 563.857 555.827 94.818
2024-12-02-04:29:44-root-INFO: Loss too large (2221.547->2333.722)! Learning rate decreased to 0.00251.
2024-12-02-04:29:44-root-INFO: Loss too large (2221.547->2275.969)! Learning rate decreased to 0.00201.
2024-12-02-04:29:44-root-INFO: Loss too large (2221.547->2226.160)! Learning rate decreased to 0.00161.
2024-12-02-04:29:45-root-INFO: Loss Change: 3275.231 -> 2184.500
2024-12-02-04:29:45-root-INFO: Regularization Change: 0.000 -> 11.012
2024-12-02-04:29:45-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-04:29:45-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:29:45-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-04:29:46-root-INFO: grad norm: 272.568 267.253 53.560
2024-12-02-04:29:46-root-INFO: Loss too large (2151.003->2242.489)! Learning rate decreased to 0.00328.
2024-12-02-04:29:47-root-INFO: grad norm: 548.489 540.863 91.147
2024-12-02-04:29:48-root-INFO: Loss too large (2113.965->2260.207)! Learning rate decreased to 0.00262.
2024-12-02-04:29:48-root-INFO: Loss too large (2113.965->2210.954)! Learning rate decreased to 0.00210.
2024-12-02-04:29:48-root-INFO: Loss too large (2113.965->2162.656)! Learning rate decreased to 0.00168.
2024-12-02-04:29:49-root-INFO: Loss too large (2113.965->2117.559)! Learning rate decreased to 0.00134.
2024-12-02-04:29:50-root-INFO: grad norm: 256.288 251.946 46.976
2024-12-02-04:29:50-root-INFO: grad norm: 172.809 168.423 38.689
2024-12-02-04:29:51-root-INFO: grad norm: 183.506 177.930 44.896
2024-12-02-04:29:52-root-INFO: Loss Change: 2151.003 -> 1974.304
2024-12-02-04:29:52-root-INFO: Regularization Change: 0.000 -> 0.931
2024-12-02-04:29:52-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-04:29:52-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:29:52-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-04:29:53-root-INFO: grad norm: 184.393 180.184 39.176
2024-12-02-04:29:53-root-INFO: Loss too large (1953.090->2231.944)! Learning rate decreased to 0.00342.
2024-12-02-04:29:53-root-INFO: Loss too large (1953.090->2130.753)! Learning rate decreased to 0.00273.
2024-12-02-04:29:54-root-INFO: Loss too large (1953.090->2054.032)! Learning rate decreased to 0.00219.
2024-12-02-04:29:54-root-INFO: Loss too large (1953.090->2001.276)! Learning rate decreased to 0.00175.
2024-12-02-04:29:54-root-INFO: Loss too large (1953.090->1968.678)! Learning rate decreased to 0.00140.
2024-12-02-04:29:55-root-INFO: grad norm: 342.890 338.140 56.880
2024-12-02-04:29:56-root-INFO: Loss too large (1950.693->1991.857)! Learning rate decreased to 0.00112.
2024-12-02-04:29:56-root-INFO: Loss too large (1950.693->1967.784)! Learning rate decreased to 0.00090.
2024-12-02-04:29:57-root-INFO: grad norm: 270.494 266.914 43.864
2024-12-02-04:29:58-root-INFO: grad norm: 179.046 174.286 41.011
2024-12-02-04:29:59-root-INFO: grad norm: 191.181 187.370 37.985
2024-12-02-04:30:00-root-INFO: Loss Change: 1953.090 -> 1908.050
2024-12-02-04:30:00-root-INFO: Regularization Change: 0.000 -> 0.240
2024-12-02-04:30:00-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-04:30:00-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:30:00-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-04:30:01-root-INFO: grad norm: 427.730 422.955 63.735
2024-12-02-04:30:01-root-INFO: Loss too large (1917.966->2135.135)! Learning rate decreased to 0.00356.
2024-12-02-04:30:01-root-INFO: Loss too large (1917.966->2112.970)! Learning rate decreased to 0.00285.
2024-12-02-04:30:02-root-INFO: Loss too large (1917.966->2088.367)! Learning rate decreased to 0.00228.
2024-12-02-04:30:02-root-INFO: Loss too large (1917.966->2060.000)! Learning rate decreased to 0.00182.
2024-12-02-04:30:02-root-INFO: Loss too large (1917.966->2028.755)! Learning rate decreased to 0.00146.
2024-12-02-04:30:03-root-INFO: Loss too large (1917.966->1996.301)! Learning rate decreased to 0.00117.
2024-12-02-04:30:03-root-INFO: Loss too large (1917.966->1964.524)! Learning rate decreased to 0.00093.
2024-12-02-04:30:03-root-INFO: Loss too large (1917.966->1935.899)! Learning rate decreased to 0.00075.
2024-12-02-04:30:04-root-INFO: grad norm: 305.989 302.642 45.136
2024-12-02-04:30:05-root-INFO: grad norm: 167.772 163.502 37.610
2024-12-02-04:30:06-root-INFO: grad norm: 170.419 166.796 34.950
2024-12-02-04:30:07-root-INFO: grad norm: 181.037 177.138 37.372
2024-12-02-04:30:08-root-INFO: Loss Change: 1917.966 -> 1867.524
2024-12-02-04:30:08-root-INFO: Regularization Change: 0.000 -> 0.142
2024-12-02-04:30:08-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-04:30:08-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-04:30:08-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-04:30:09-root-INFO: grad norm: 90.767 84.648 32.764
2024-12-02-04:30:10-root-INFO: grad norm: 168.949 165.555 33.695
2024-12-02-04:30:10-root-INFO: Loss too large (1824.241->1983.410)! Learning rate decreased to 0.00371.
2024-12-02-04:30:10-root-INFO: Loss too large (1824.241->1949.672)! Learning rate decreased to 0.00297.
2024-12-02-04:30:11-root-INFO: Loss too large (1824.241->1918.408)! Learning rate decreased to 0.00238.
2024-12-02-04:30:11-root-INFO: Loss too large (1824.241->1890.220)! Learning rate decreased to 0.00190.
2024-12-02-04:30:11-root-INFO: Loss too large (1824.241->1866.490)! Learning rate decreased to 0.00152.
2024-12-02-04:30:12-root-INFO: Loss too large (1824.241->1848.175)! Learning rate decreased to 0.00122.
2024-12-02-04:30:12-root-INFO: Loss too large (1824.241->1835.243)! Learning rate decreased to 0.00097.
2024-12-02-04:30:12-root-INFO: Loss too large (1824.241->1826.883)! Learning rate decreased to 0.00078.
2024-12-02-04:30:13-root-INFO: grad norm: 221.981 218.736 37.818
2024-12-02-04:30:14-root-INFO: Loss too large (1821.971->1825.815)! Learning rate decreased to 0.00062.
2024-12-02-04:30:15-root-INFO: grad norm: 235.646 232.556 38.035
2024-12-02-04:30:16-root-INFO: grad norm: 245.704 242.608 38.885
2024-12-02-04:30:16-root-INFO: Loss Change: 1850.825 -> 1812.422
2024-12-02-04:30:16-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-02-04:30:16-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-04:30:16-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:30:17-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-04:30:17-root-INFO: grad norm: 495.277 490.404 69.301
2024-12-02-04:30:17-root-INFO: Loss too large (1830.687->2068.034)! Learning rate decreased to 0.00387.
2024-12-02-04:30:18-root-INFO: Loss too large (1830.687->2052.253)! Learning rate decreased to 0.00309.
2024-12-02-04:30:18-root-INFO: Loss too large (1830.687->2035.932)! Learning rate decreased to 0.00248.
2024-12-02-04:30:18-root-INFO: Loss too large (1830.687->2016.547)! Learning rate decreased to 0.00198.
2024-12-02-04:30:19-root-INFO: Loss too large (1830.687->1992.465)! Learning rate decreased to 0.00158.
2024-12-02-04:30:19-root-INFO: Loss too large (1830.687->1963.754)! Learning rate decreased to 0.00127.
2024-12-02-04:30:19-root-INFO: Loss too large (1830.687->1931.295)! Learning rate decreased to 0.00101.
2024-12-02-04:30:20-root-INFO: Loss too large (1830.687->1896.315)! Learning rate decreased to 0.00081.
2024-12-02-04:30:20-root-INFO: Loss too large (1830.687->1861.586)! Learning rate decreased to 0.00065.
2024-12-02-04:30:20-root-INFO: Loss too large (1830.687->1831.616)! Learning rate decreased to 0.00052.
2024-12-02-04:30:21-root-INFO: grad norm: 325.313 322.168 45.129
2024-12-02-04:30:22-root-INFO: grad norm: 208.835 205.241 38.578
2024-12-02-04:30:23-root-INFO: grad norm: 179.601 176.488 33.295
2024-12-02-04:30:24-root-INFO: grad norm: 154.652 151.051 33.180
2024-12-02-04:30:25-root-INFO: Loss Change: 1830.687 -> 1779.351
2024-12-02-04:30:25-root-INFO: Regularization Change: 0.000 -> 0.075
2024-12-02-04:30:25-root-INFO: Undo step: 180
2024-12-02-04:30:25-root-INFO: Undo step: 181
2024-12-02-04:30:25-root-INFO: Undo step: 182
2024-12-02-04:30:25-root-INFO: Undo step: 183
2024-12-02-04:30:25-root-INFO: Undo step: 184
2024-12-02-04:30:25-root-INFO: step: 185 lr_xt 0.00392795
2024-12-02-04:30:26-root-INFO: grad norm: 828.966 778.245 285.515
2024-12-02-04:30:26-root-INFO: Loss too large (3135.551->3208.564)! Learning rate decreased to 0.00314.
2024-12-02-04:30:27-root-INFO: grad norm: 981.234 959.971 203.167
2024-12-02-04:30:27-root-INFO: Loss too large (2778.502->3035.879)! Learning rate decreased to 0.00251.
2024-12-02-04:30:28-root-INFO: grad norm: 616.811 599.083 146.816
2024-12-02-04:30:29-root-INFO: grad norm: 242.222 233.203 65.480
2024-12-02-04:30:30-root-INFO: Loss too large (2173.173->2221.296)! Learning rate decreased to 0.00201.
2024-12-02-04:30:30-root-INFO: Loss too large (2173.173->2176.588)! Learning rate decreased to 0.00161.
2024-12-02-04:30:31-root-INFO: grad norm: 424.648 419.234 67.595
2024-12-02-04:30:31-root-INFO: Loss too large (2152.033->2188.383)! Learning rate decreased to 0.00129.
2024-12-02-04:30:32-root-INFO: Loss too large (2152.033->2163.667)! Learning rate decreased to 0.00103.
2024-12-02-04:30:32-root-INFO: Loss Change: 3135.551 -> 2142.105
2024-12-02-04:30:32-root-INFO: Regularization Change: 0.000 -> 7.840
2024-12-02-04:30:32-root-INFO: Learning rate of xt decay: 0.06437 -> 0.06514.
2024-12-02-04:30:32-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:30:33-root-INFO: step: 184 lr_xt 0.00409583
2024-12-02-04:30:33-root-INFO: grad norm: 276.337 270.944 54.328
2024-12-02-04:30:33-root-INFO: Loss too large (2098.236->2484.621)! Learning rate decreased to 0.00328.
2024-12-02-04:30:34-root-INFO: Loss too large (2098.236->2353.202)! Learning rate decreased to 0.00262.
2024-12-02-04:30:34-root-INFO: Loss too large (2098.236->2245.521)! Learning rate decreased to 0.00210.
2024-12-02-04:30:34-root-INFO: Loss too large (2098.236->2163.844)! Learning rate decreased to 0.00168.
2024-12-02-04:30:35-root-INFO: Loss too large (2098.236->2108.964)! Learning rate decreased to 0.00134.
2024-12-02-04:30:36-root-INFO: grad norm: 408.501 403.137 65.981
2024-12-02-04:30:36-root-INFO: Loss too large (2077.618->2104.691)! Learning rate decreased to 0.00107.
2024-12-02-04:30:36-root-INFO: Loss too large (2077.618->2080.797)! Learning rate decreased to 0.00086.
2024-12-02-04:30:37-root-INFO: grad norm: 280.826 275.733 53.237
2024-12-02-04:30:38-root-INFO: grad norm: 151.159 144.714 43.669
2024-12-02-04:30:39-root-INFO: grad norm: 146.826 141.135 40.482
2024-12-02-04:30:40-root-INFO: Loss Change: 2098.236 -> 1997.124
2024-12-02-04:30:40-root-INFO: Regularization Change: 0.000 -> 0.415
2024-12-02-04:30:40-root-INFO: Learning rate of xt decay: 0.06514 -> 0.06592.
2024-12-02-04:30:40-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:30:40-root-INFO: step: 183 lr_xt 0.00427020
2024-12-02-04:30:41-root-INFO: grad norm: 270.384 265.739 49.903
2024-12-02-04:30:41-root-INFO: Loss too large (1987.937->2129.829)! Learning rate decreased to 0.00342.
2024-12-02-04:30:41-root-INFO: Loss too large (1987.937->2103.643)! Learning rate decreased to 0.00273.
2024-12-02-04:30:42-root-INFO: Loss too large (1987.937->2077.411)! Learning rate decreased to 0.00219.
2024-12-02-04:30:42-root-INFO: Loss too large (1987.937->2051.845)! Learning rate decreased to 0.00175.
2024-12-02-04:30:42-root-INFO: Loss too large (1987.937->2028.118)! Learning rate decreased to 0.00140.
2024-12-02-04:30:43-root-INFO: Loss too large (1987.937->2007.768)! Learning rate decreased to 0.00112.
2024-12-02-04:30:43-root-INFO: Loss too large (1987.937->1992.043)! Learning rate decreased to 0.00090.
2024-12-02-04:30:44-root-INFO: grad norm: 256.810 252.782 45.310
2024-12-02-04:30:45-root-INFO: grad norm: 247.021 242.814 45.395
2024-12-02-04:30:46-root-INFO: grad norm: 251.683 247.812 43.976
2024-12-02-04:30:47-root-INFO: grad norm: 267.404 263.496 45.547
2024-12-02-04:30:48-root-INFO: Loss Change: 1987.937 -> 1942.429
2024-12-02-04:30:48-root-INFO: Regularization Change: 0.000 -> 0.291
2024-12-02-04:30:48-root-INFO: Learning rate of xt decay: 0.06592 -> 0.06671.
2024-12-02-04:30:48-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-04:30:48-root-INFO: step: 182 lr_xt 0.00445127
2024-12-02-04:30:49-root-INFO: grad norm: 213.508 210.018 38.451
2024-12-02-04:30:49-root-INFO: Loss too large (1915.795->2302.588)! Learning rate decreased to 0.00356.
2024-12-02-04:30:49-root-INFO: Loss too large (1915.795->2194.409)! Learning rate decreased to 0.00285.
2024-12-02-04:30:50-root-INFO: Loss too large (1915.795->2103.764)! Learning rate decreased to 0.00228.
2024-12-02-04:30:50-root-INFO: Loss too large (1915.795->2031.700)! Learning rate decreased to 0.00182.
2024-12-02-04:30:50-root-INFO: Loss too large (1915.795->1978.597)! Learning rate decreased to 0.00146.
2024-12-02-04:30:51-root-INFO: Loss too large (1915.795->1942.894)! Learning rate decreased to 0.00117.
2024-12-02-04:30:51-root-INFO: Loss too large (1915.795->1921.183)! Learning rate decreased to 0.00093.
2024-12-02-04:30:52-root-INFO: grad norm: 290.779 287.003 46.714
2024-12-02-04:30:52-root-INFO: Loss too large (1909.395->1916.323)! Learning rate decreased to 0.00075.
2024-12-02-04:30:53-root-INFO: grad norm: 258.955 255.495 42.191
2024-12-02-04:30:54-root-INFO: grad norm: 222.617 218.923 40.390
2024-12-02-04:30:55-root-INFO: grad norm: 218.762 215.443 37.965
2024-12-02-04:30:56-root-INFO: Loss Change: 1915.795 -> 1874.545
2024-12-02-04:30:56-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-02-04:30:56-root-INFO: Learning rate of xt decay: 0.06671 -> 0.06751.
2024-12-02-04:30:56-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-04:30:56-root-INFO: step: 181 lr_xt 0.00463927
2024-12-02-04:30:57-root-INFO: grad norm: 443.824 439.688 60.448
2024-12-02-04:30:57-root-INFO: Loss too large (1886.122->2101.964)! Learning rate decreased to 0.00371.
2024-12-02-04:30:57-root-INFO: Loss too large (1886.122->2082.293)! Learning rate decreased to 0.00297.
2024-12-02-04:30:58-root-INFO: Loss too large (1886.122->2060.866)! Learning rate decreased to 0.00238.
2024-12-02-04:30:58-root-INFO: Loss too large (1886.122->2035.921)! Learning rate decreased to 0.00190.
2024-12-02-04:30:58-root-INFO: Loss too large (1886.122->2007.480)! Learning rate decreased to 0.00152.
2024-12-02-04:30:59-root-INFO: Loss too large (1886.122->1976.438)! Learning rate decreased to 0.00122.
2024-12-02-04:30:59-root-INFO: Loss too large (1886.122->1944.233)! Learning rate decreased to 0.00097.
2024-12-02-04:30:59-root-INFO: Loss too large (1886.122->1913.417)! Learning rate decreased to 0.00078.
2024-12-02-04:31:00-root-INFO: Loss too large (1886.122->1887.310)! Learning rate decreased to 0.00062.
2024-12-02-04:31:01-root-INFO: grad norm: 288.678 285.496 42.747
2024-12-02-04:31:02-root-INFO: grad norm: 162.951 158.893 36.142
2024-12-02-04:31:03-root-INFO: grad norm: 144.251 140.704 31.792
2024-12-02-04:31:04-root-INFO: grad norm: 128.978 124.717 32.877
2024-12-02-04:31:04-root-INFO: Loss Change: 1886.122 -> 1828.191
2024-12-02-04:31:04-root-INFO: Regularization Change: 0.000 -> 0.114
2024-12-02-04:31:04-root-INFO: Learning rate of xt decay: 0.06751 -> 0.06832.
2024-12-02-04:31:04-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:31:05-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-04:31:05-root-INFO: grad norm: 177.793 173.133 40.440
2024-12-02-04:31:05-root-INFO: Loss too large (1820.390->1958.830)! Learning rate decreased to 0.00387.
2024-12-02-04:31:06-root-INFO: Loss too large (1820.390->1931.082)! Learning rate decreased to 0.00309.
2024-12-02-04:31:06-root-INFO: Loss too large (1820.390->1903.883)! Learning rate decreased to 0.00248.
2024-12-02-04:31:06-root-INFO: Loss too large (1820.390->1878.625)! Learning rate decreased to 0.00198.
2024-12-02-04:31:07-root-INFO: Loss too large (1820.390->1856.997)! Learning rate decreased to 0.00158.
2024-12-02-04:31:07-root-INFO: Loss too large (1820.390->1840.110)! Learning rate decreased to 0.00127.
2024-12-02-04:31:07-root-INFO: Loss too large (1820.390->1828.109)! Learning rate decreased to 0.00101.
2024-12-02-04:31:08-root-INFO: grad norm: 269.345 266.371 39.920
2024-12-02-04:31:09-root-INFO: Loss too large (1820.363->1837.038)! Learning rate decreased to 0.00081.
2024-12-02-04:31:10-root-INFO: grad norm: 352.888 349.223 50.721
2024-12-02-04:31:10-root-INFO: Loss too large (1817.516->1828.793)! Learning rate decreased to 0.00065.
2024-12-02-04:31:11-root-INFO: grad norm: 294.000 291.012 41.808
2024-12-02-04:31:12-root-INFO: grad norm: 232.573 229.166 39.666
2024-12-02-04:31:13-root-INFO: Loss Change: 1820.390 -> 1792.783
2024-12-02-04:31:13-root-INFO: Regularization Change: 0.000 -> 0.136
2024-12-02-04:31:13-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-04:31:13-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:31:13-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-04:31:13-root-INFO: grad norm: 96.729 92.086 29.608
2024-12-02-04:31:14-root-INFO: grad norm: 425.339 422.310 50.678
2024-12-02-04:31:15-root-INFO: Loss too large (1766.807->2000.271)! Learning rate decreased to 0.00403.
2024-12-02-04:31:15-root-INFO: Loss too large (1766.807->1985.579)! Learning rate decreased to 0.00322.
2024-12-02-04:31:15-root-INFO: Loss too large (1766.807->1969.112)! Learning rate decreased to 0.00258.
2024-12-02-04:31:16-root-INFO: Loss too large (1766.807->1948.671)! Learning rate decreased to 0.00206.
2024-12-02-04:31:16-root-INFO: Loss too large (1766.807->1923.460)! Learning rate decreased to 0.00165.
2024-12-02-04:31:16-root-INFO: Loss too large (1766.807->1893.845)! Learning rate decreased to 0.00132.
2024-12-02-04:31:17-root-INFO: Loss too large (1766.807->1860.757)! Learning rate decreased to 0.00106.
2024-12-02-04:31:17-root-INFO: Loss too large (1766.807->1826.325)! Learning rate decreased to 0.00085.
2024-12-02-04:31:17-root-INFO: Loss too large (1766.807->1794.321)! Learning rate decreased to 0.00068.
2024-12-02-04:31:18-root-INFO: Loss too large (1766.807->1768.598)! Learning rate decreased to 0.00054.
2024-12-02-04:31:19-root-INFO: grad norm: 296.328 293.435 41.308
2024-12-02-04:31:19-root-INFO: grad norm: 205.666 202.998 33.021
2024-12-02-04:31:20-root-INFO: grad norm: 176.023 173.435 30.077
2024-12-02-04:31:21-root-INFO: Loss Change: 1773.695 -> 1728.323
2024-12-02-04:31:21-root-INFO: Regularization Change: 0.000 -> 0.363
2024-12-02-04:31:21-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-04:31:21-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:31:22-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-04:31:22-root-INFO: grad norm: 377.827 374.546 49.687
2024-12-02-04:31:22-root-INFO: Loss too large (1734.464->1970.801)! Learning rate decreased to 0.00420.
2024-12-02-04:31:22-root-INFO: Loss too large (1734.464->1956.435)! Learning rate decreased to 0.00336.
2024-12-02-04:31:23-root-INFO: Loss too large (1734.464->1939.422)! Learning rate decreased to 0.00269.
2024-12-02-04:31:23-root-INFO: Loss too large (1734.464->1918.262)! Learning rate decreased to 0.00215.
2024-12-02-04:31:24-root-INFO: Loss too large (1734.464->1892.696)! Learning rate decreased to 0.00172.
2024-12-02-04:31:24-root-INFO: Loss too large (1734.464->1863.231)! Learning rate decreased to 0.00138.
2024-12-02-04:31:24-root-INFO: Loss too large (1734.464->1830.835)! Learning rate decreased to 0.00110.
2024-12-02-04:31:25-root-INFO: Loss too large (1734.464->1797.718)! Learning rate decreased to 0.00088.
2024-12-02-04:31:25-root-INFO: Loss too large (1734.464->1767.449)! Learning rate decreased to 0.00070.
2024-12-02-04:31:25-root-INFO: Loss too large (1734.464->1743.330)! Learning rate decreased to 0.00056.
2024-12-02-04:31:26-root-INFO: grad norm: 302.875 300.314 39.304
2024-12-02-04:31:27-root-INFO: grad norm: 239.641 236.770 36.983
2024-12-02-04:31:28-root-INFO: grad norm: 218.767 216.451 31.747
2024-12-02-04:31:29-root-INFO: grad norm: 199.312 196.547 33.083
2024-12-02-04:31:30-root-INFO: Loss Change: 1734.464 -> 1702.871
2024-12-02-04:31:30-root-INFO: Regularization Change: 0.000 -> 0.071
2024-12-02-04:31:30-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-04:31:30-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:31:30-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-04:31:30-root-INFO: grad norm: 84.608 79.893 27.851
2024-12-02-04:31:31-root-INFO: grad norm: 84.672 81.238 23.869
2024-12-02-04:31:32-root-INFO: Loss too large (1664.355->1687.751)! Learning rate decreased to 0.00437.
2024-12-02-04:31:32-root-INFO: Loss too large (1664.355->1674.000)! Learning rate decreased to 0.00350.
2024-12-02-04:31:32-root-INFO: Loss too large (1664.355->1665.880)! Learning rate decreased to 0.00280.
2024-12-02-04:31:33-root-INFO: grad norm: 298.395 296.010 37.654
2024-12-02-04:31:34-root-INFO: Loss too large (1661.448->1845.555)! Learning rate decreased to 0.00224.
2024-12-02-04:31:34-root-INFO: Loss too large (1661.448->1819.137)! Learning rate decreased to 0.00179.
2024-12-02-04:31:34-root-INFO: Loss too large (1661.448->1789.147)! Learning rate decreased to 0.00143.
2024-12-02-04:31:35-root-INFO: Loss too large (1661.448->1756.933)! Learning rate decreased to 0.00115.
2024-12-02-04:31:35-root-INFO: Loss too large (1661.448->1725.326)! Learning rate decreased to 0.00092.
2024-12-02-04:31:35-root-INFO: Loss too large (1661.448->1697.802)! Learning rate decreased to 0.00073.
2024-12-02-04:31:36-root-INFO: Loss too large (1661.448->1676.725)! Learning rate decreased to 0.00059.
2024-12-02-04:31:36-root-INFO: Loss too large (1661.448->1662.545)! Learning rate decreased to 0.00047.
2024-12-02-04:31:37-root-INFO: grad norm: 219.177 217.084 30.219
2024-12-02-04:31:38-root-INFO: grad norm: 165.086 162.802 27.367
2024-12-02-04:31:39-root-INFO: Loss Change: 1692.408 -> 1645.886
2024-12-02-04:31:39-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-02-04:31:39-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-04:31:39-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-04:31:39-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-04:31:40-root-INFO: grad norm: 163.899 161.076 30.288
2024-12-02-04:31:40-root-INFO: Loss too large (1640.561->1840.071)! Learning rate decreased to 0.00455.
2024-12-02-04:31:40-root-INFO: Loss too large (1640.561->1814.374)! Learning rate decreased to 0.00364.
2024-12-02-04:31:41-root-INFO: Loss too large (1640.561->1786.125)! Learning rate decreased to 0.00291.
2024-12-02-04:31:41-root-INFO: Loss too large (1640.561->1756.154)! Learning rate decreased to 0.00233.
2024-12-02-04:31:41-root-INFO: Loss too large (1640.561->1726.341)! Learning rate decreased to 0.00186.
2024-12-02-04:31:41-root-INFO: Loss too large (1640.561->1699.318)! Learning rate decreased to 0.00149.
2024-12-02-04:31:42-root-INFO: Loss too large (1640.561->1677.212)! Learning rate decreased to 0.00119.
2024-12-02-04:31:42-root-INFO: Loss too large (1640.561->1660.827)! Learning rate decreased to 0.00095.
2024-12-02-04:31:43-root-INFO: Loss too large (1640.561->1649.731)! Learning rate decreased to 0.00076.
2024-12-02-04:31:43-root-INFO: Loss too large (1640.561->1642.824)! Learning rate decreased to 0.00061.
2024-12-02-04:31:44-root-INFO: grad norm: 199.378 197.446 27.686
2024-12-02-04:31:45-root-INFO: grad norm: 248.665 246.191 34.989
2024-12-02-04:31:45-root-INFO: Loss too large (1638.488->1640.434)! Learning rate decreased to 0.00049.
2024-12-02-04:31:46-root-INFO: grad norm: 208.856 206.951 28.145
2024-12-02-04:31:47-root-INFO: grad norm: 178.136 175.758 29.007
2024-12-02-04:31:48-root-INFO: Loss Change: 1640.561 -> 1626.816
2024-12-02-04:31:48-root-INFO: Regularization Change: 0.000 -> 0.043
2024-12-02-04:31:48-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-04:31:48-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:31:48-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-04:31:49-root-INFO: grad norm: 146.373 143.388 29.408
2024-12-02-04:31:49-root-INFO: Loss too large (1621.753->1807.288)! Learning rate decreased to 0.00474.
2024-12-02-04:31:49-root-INFO: Loss too large (1621.753->1779.747)! Learning rate decreased to 0.00379.
2024-12-02-04:31:50-root-INFO: Loss too large (1621.753->1750.344)! Learning rate decreased to 0.00303.
2024-12-02-04:31:50-root-INFO: Loss too large (1621.753->1720.221)! Learning rate decreased to 0.00243.
2024-12-02-04:31:50-root-INFO: Loss too large (1621.753->1691.744)! Learning rate decreased to 0.00194.
2024-12-02-04:31:51-root-INFO: Loss too large (1621.753->1667.420)! Learning rate decreased to 0.00155.
2024-12-02-04:31:51-root-INFO: Loss too large (1621.753->1648.658)! Learning rate decreased to 0.00124.
2024-12-02-04:31:51-root-INFO: Loss too large (1621.753->1635.483)! Learning rate decreased to 0.00099.
2024-12-02-04:31:52-root-INFO: Loss too large (1621.753->1626.983)! Learning rate decreased to 0.00080.
2024-12-02-04:31:52-root-INFO: Loss too large (1621.753->1621.934)! Learning rate decreased to 0.00064.
2024-12-02-04:31:53-root-INFO: grad norm: 185.524 183.472 27.519
2024-12-02-04:31:54-root-INFO: grad norm: 244.958 242.559 34.193
2024-12-02-04:31:54-root-INFO: Loss too large (1619.103->1622.002)! Learning rate decreased to 0.00051.
2024-12-02-04:31:55-root-INFO: grad norm: 221.492 219.543 29.322
2024-12-02-04:31:56-root-INFO: grad norm: 202.738 200.470 30.237
2024-12-02-04:31:57-root-INFO: Loss Change: 1621.753 -> 1608.229
2024-12-02-04:31:57-root-INFO: Regularization Change: 0.000 -> 0.051
2024-12-02-04:31:57-root-INFO: Undo step: 175
2024-12-02-04:31:57-root-INFO: Undo step: 176
2024-12-02-04:31:57-root-INFO: Undo step: 177
2024-12-02-04:31:57-root-INFO: Undo step: 178
2024-12-02-04:31:57-root-INFO: Undo step: 179
2024-12-02-04:31:57-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-04:31:58-root-INFO: grad norm: 812.669 751.588 309.107
2024-12-02-04:31:58-root-INFO: Loss too large (3072.930->3439.706)! Learning rate decreased to 0.00387.
2024-12-02-04:31:59-root-INFO: grad norm: 807.038 789.149 168.980
2024-12-02-04:32:00-root-INFO: grad norm: 809.756 793.671 160.597
2024-12-02-04:32:00-root-INFO: Loss too large (2515.574->3085.849)! Learning rate decreased to 0.00309.
2024-12-02-04:32:01-root-INFO: Loss too large (2515.574->2666.023)! Learning rate decreased to 0.00248.
2024-12-02-04:32:02-root-INFO: grad norm: 696.837 687.806 111.826
2024-12-02-04:32:03-root-INFO: grad norm: 302.342 291.765 79.273
2024-12-02-04:32:03-root-INFO: Loss Change: 3072.930 -> 1900.215
2024-12-02-04:32:03-root-INFO: Regularization Change: 0.000 -> 12.709
2024-12-02-04:32:03-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-04:32:03-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:32:04-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-04:32:04-root-INFO: grad norm: 291.424 286.546 53.098
2024-12-02-04:32:04-root-INFO: Loss too large (1880.306->2298.793)! Learning rate decreased to 0.00403.
2024-12-02-04:32:05-root-INFO: Loss too large (1880.306->2140.139)! Learning rate decreased to 0.00322.
2024-12-02-04:32:05-root-INFO: Loss too large (1880.306->2005.783)! Learning rate decreased to 0.00258.
2024-12-02-04:32:05-root-INFO: Loss too large (1880.306->1900.933)! Learning rate decreased to 0.00206.
2024-12-02-04:32:06-root-INFO: grad norm: 490.737 484.138 80.206
2024-12-02-04:32:07-root-INFO: Loss too large (1833.016->1934.383)! Learning rate decreased to 0.00165.
2024-12-02-04:32:07-root-INFO: Loss too large (1833.016->1899.496)! Learning rate decreased to 0.00132.
2024-12-02-04:32:07-root-INFO: Loss too large (1833.016->1865.818)! Learning rate decreased to 0.00106.
2024-12-02-04:32:08-root-INFO: Loss too large (1833.016->1835.402)! Learning rate decreased to 0.00085.
2024-12-02-04:32:09-root-INFO: grad norm: 275.358 269.972 54.195
2024-12-02-04:32:10-root-INFO: grad norm: 113.876 107.495 37.585
2024-12-02-04:32:11-root-INFO: grad norm: 103.909 97.898 34.828
2024-12-02-04:32:11-root-INFO: Loss Change: 1880.306 -> 1748.032
2024-12-02-04:32:11-root-INFO: Regularization Change: 0.000 -> 0.462
2024-12-02-04:32:11-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-04:32:11-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:32:12-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-04:32:12-root-INFO: grad norm: 262.202 257.175 51.096
2024-12-02-04:32:12-root-INFO: Loss too large (1747.694->1953.859)! Learning rate decreased to 0.00420.
2024-12-02-04:32:13-root-INFO: Loss too large (1747.694->1924.391)! Learning rate decreased to 0.00336.
2024-12-02-04:32:13-root-INFO: Loss too large (1747.694->1893.357)! Learning rate decreased to 0.00269.
2024-12-02-04:32:13-root-INFO: Loss too large (1747.694->1861.804)! Learning rate decreased to 0.00215.
2024-12-02-04:32:14-root-INFO: Loss too large (1747.694->1831.071)! Learning rate decreased to 0.00172.
2024-12-02-04:32:14-root-INFO: Loss too large (1747.694->1802.807)! Learning rate decreased to 0.00138.
2024-12-02-04:32:14-root-INFO: Loss too large (1747.694->1778.792)! Learning rate decreased to 0.00110.
2024-12-02-04:32:15-root-INFO: Loss too large (1747.694->1760.250)! Learning rate decreased to 0.00088.
2024-12-02-04:32:16-root-INFO: grad norm: 276.557 272.520 47.079
2024-12-02-04:32:17-root-INFO: grad norm: 329.545 325.077 54.079
2024-12-02-04:32:17-root-INFO: Loss too large (1736.214->1748.572)! Learning rate decreased to 0.00070.
2024-12-02-04:32:18-root-INFO: grad norm: 279.860 275.762 47.717
2024-12-02-04:32:19-root-INFO: grad norm: 228.586 224.585 42.581
2024-12-02-04:32:20-root-INFO: Loss Change: 1747.694 -> 1714.104
2024-12-02-04:32:20-root-INFO: Regularization Change: 0.000 -> 0.152
2024-12-02-04:32:20-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-04:32:20-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:32:20-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-04:32:20-root-INFO: grad norm: 133.704 129.990 31.293
2024-12-02-04:32:21-root-INFO: Loss too large (1698.904->1911.558)! Learning rate decreased to 0.00437.
2024-12-02-04:32:21-root-INFO: Loss too large (1698.904->1849.084)! Learning rate decreased to 0.00350.
2024-12-02-04:32:21-root-INFO: Loss too large (1698.904->1797.987)! Learning rate decreased to 0.00280.
2024-12-02-04:32:22-root-INFO: Loss too large (1698.904->1759.113)! Learning rate decreased to 0.00224.
2024-12-02-04:32:22-root-INFO: Loss too large (1698.904->1731.741)! Learning rate decreased to 0.00179.
2024-12-02-04:32:22-root-INFO: Loss too large (1698.904->1713.905)! Learning rate decreased to 0.00143.
2024-12-02-04:32:23-root-INFO: Loss too large (1698.904->1703.156)! Learning rate decreased to 0.00115.
2024-12-02-04:32:24-root-INFO: grad norm: 275.982 272.210 45.476
2024-12-02-04:32:24-root-INFO: Loss too large (1697.212->1736.872)! Learning rate decreased to 0.00092.
2024-12-02-04:32:24-root-INFO: Loss too large (1697.212->1716.624)! Learning rate decreased to 0.00073.
2024-12-02-04:32:25-root-INFO: Loss too large (1697.212->1702.129)! Learning rate decreased to 0.00059.
2024-12-02-04:32:26-root-INFO: grad norm: 241.126 237.633 40.898
2024-12-02-04:32:27-root-INFO: grad norm: 209.557 206.060 38.121
2024-12-02-04:32:28-root-INFO: grad norm: 201.363 198.097 36.119
2024-12-02-04:32:28-root-INFO: Loss Change: 1698.904 -> 1676.244
2024-12-02-04:32:28-root-INFO: Regularization Change: 0.000 -> 0.095
2024-12-02-04:32:28-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-04:32:28-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-04:32:29-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-04:32:29-root-INFO: grad norm: 466.508 461.911 65.333
2024-12-02-04:32:29-root-INFO: Loss too large (1700.161->1958.370)! Learning rate decreased to 0.00455.
2024-12-02-04:32:30-root-INFO: Loss too large (1700.161->1938.157)! Learning rate decreased to 0.00364.
2024-12-02-04:32:30-root-INFO: Loss too large (1700.161->1918.313)! Learning rate decreased to 0.00291.
2024-12-02-04:32:30-root-INFO: Loss too large (1700.161->1896.892)! Learning rate decreased to 0.00233.
2024-12-02-04:32:31-root-INFO: Loss too large (1700.161->1872.368)! Learning rate decreased to 0.00186.
2024-12-02-04:32:31-root-INFO: Loss too large (1700.161->1844.333)! Learning rate decreased to 0.00149.
2024-12-02-04:32:31-root-INFO: Loss too large (1700.161->1813.208)! Learning rate decreased to 0.00119.
2024-12-02-04:32:32-root-INFO: Loss too large (1700.161->1779.909)! Learning rate decreased to 0.00095.
2024-12-02-04:32:32-root-INFO: Loss too large (1700.161->1746.411)! Learning rate decreased to 0.00076.
2024-12-02-04:32:32-root-INFO: Loss too large (1700.161->1716.027)! Learning rate decreased to 0.00061.
2024-12-02-04:32:33-root-INFO: grad norm: 352.293 348.131 53.993
2024-12-02-04:32:34-root-INFO: grad norm: 249.257 245.683 42.063
2024-12-02-04:32:35-root-INFO: grad norm: 254.782 251.622 40.004
2024-12-02-04:32:36-root-INFO: grad norm: 266.444 263.125 41.928
2024-12-02-04:32:37-root-INFO: Loss Change: 1700.161 -> 1658.318
2024-12-02-04:32:37-root-INFO: Regularization Change: 0.000 -> 0.116
2024-12-02-04:32:37-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-04:32:37-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:32:37-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-04:32:38-root-INFO: grad norm: 114.857 111.138 28.989
2024-12-02-04:32:38-root-INFO: Loss too large (1643.500->1759.931)! Learning rate decreased to 0.00474.
2024-12-02-04:32:38-root-INFO: Loss too large (1643.500->1718.233)! Learning rate decreased to 0.00379.
2024-12-02-04:32:39-root-INFO: Loss too large (1643.500->1687.484)! Learning rate decreased to 0.00303.
2024-12-02-04:32:39-root-INFO: Loss too large (1643.500->1666.257)! Learning rate decreased to 0.00243.
2024-12-02-04:32:39-root-INFO: Loss too large (1643.500->1652.589)! Learning rate decreased to 0.00194.
2024-12-02-04:32:40-root-INFO: Loss too large (1643.500->1644.430)! Learning rate decreased to 0.00155.
2024-12-02-04:32:41-root-INFO: grad norm: 286.718 283.621 42.031
2024-12-02-04:32:41-root-INFO: Loss too large (1639.998->1736.378)! Learning rate decreased to 0.00124.
2024-12-02-04:32:41-root-INFO: Loss too large (1639.998->1706.267)! Learning rate decreased to 0.00099.
2024-12-02-04:32:42-root-INFO: Loss too large (1639.998->1679.679)! Learning rate decreased to 0.00080.
2024-12-02-04:32:42-root-INFO: Loss too large (1639.998->1658.719)! Learning rate decreased to 0.00064.
2024-12-02-04:32:42-root-INFO: Loss too large (1639.998->1644.043)! Learning rate decreased to 0.00051.
2024-12-02-04:32:43-root-INFO: grad norm: 245.888 242.831 38.656
2024-12-02-04:32:44-root-INFO: grad norm: 213.207 210.348 34.799
2024-12-02-04:32:45-root-INFO: grad norm: 197.081 194.264 33.204
2024-12-02-04:32:46-root-INFO: Loss Change: 1643.500 -> 1621.676
2024-12-02-04:32:46-root-INFO: Regularization Change: 0.000 -> 0.093
2024-12-02-04:32:46-root-INFO: Undo step: 175
2024-12-02-04:32:46-root-INFO: Undo step: 176
2024-12-02-04:32:46-root-INFO: Undo step: 177
2024-12-02-04:32:46-root-INFO: Undo step: 178
2024-12-02-04:32:46-root-INFO: Undo step: 179
2024-12-02-04:32:46-root-INFO: step: 180 lr_xt 0.00483443
2024-12-02-04:32:47-root-INFO: grad norm: 642.901 620.956 166.538
2024-12-02-04:32:48-root-INFO: grad norm: 851.740 834.363 171.170
2024-12-02-04:32:48-root-INFO: Loss too large (2637.155->3010.715)! Learning rate decreased to 0.00387.
2024-12-02-04:32:49-root-INFO: grad norm: 632.836 621.092 121.352
2024-12-02-04:32:50-root-INFO: grad norm: 569.974 550.203 148.819
2024-12-02-04:32:51-root-INFO: grad norm: 600.965 585.506 135.433
2024-12-02-04:32:51-root-INFO: Loss too large (2134.177->2203.669)! Learning rate decreased to 0.00309.
2024-12-02-04:32:52-root-INFO: Loss Change: 2761.148 -> 2043.882
2024-12-02-04:32:52-root-INFO: Regularization Change: 0.000 -> 12.255
2024-12-02-04:32:52-root-INFO: Learning rate of xt decay: 0.06832 -> 0.06914.
2024-12-02-04:32:52-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:32:52-root-INFO: step: 179 lr_xt 0.00503698
2024-12-02-04:32:53-root-INFO: grad norm: 370.191 351.195 117.062
2024-12-02-04:32:53-root-INFO: Loss too large (2014.401->2099.999)! Learning rate decreased to 0.00403.
2024-12-02-04:32:54-root-INFO: grad norm: 547.004 524.626 154.860
2024-12-02-04:32:54-root-INFO: Loss too large (1889.356->2067.517)! Learning rate decreased to 0.00322.
2024-12-02-04:32:55-root-INFO: Loss too large (1889.356->1990.659)! Learning rate decreased to 0.00258.
2024-12-02-04:32:55-root-INFO: Loss too large (1889.356->1927.856)! Learning rate decreased to 0.00206.
2024-12-02-04:32:56-root-INFO: grad norm: 243.537 230.430 78.816
2024-12-02-04:32:57-root-INFO: grad norm: 199.186 188.587 64.108
2024-12-02-04:32:58-root-INFO: grad norm: 191.680 182.738 57.862
2024-12-02-04:32:59-root-INFO: Loss Change: 2014.401 -> 1723.438
2024-12-02-04:32:59-root-INFO: Regularization Change: 0.000 -> 1.540
2024-12-02-04:32:59-root-INFO: Learning rate of xt decay: 0.06914 -> 0.06997.
2024-12-02-04:32:59-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:32:59-root-INFO: step: 178 lr_xt 0.00524717
2024-12-02-04:32:59-root-INFO: grad norm: 205.606 195.248 64.438
2024-12-02-04:33:00-root-INFO: Loss too large (1697.342->2030.410)! Learning rate decreased to 0.00420.
2024-12-02-04:33:00-root-INFO: Loss too large (1697.342->1919.355)! Learning rate decreased to 0.00336.
2024-12-02-04:33:00-root-INFO: Loss too large (1697.342->1828.805)! Learning rate decreased to 0.00269.
2024-12-02-04:33:01-root-INFO: Loss too large (1697.342->1761.495)! Learning rate decreased to 0.00215.
2024-12-02-04:33:01-root-INFO: Loss too large (1697.342->1716.972)! Learning rate decreased to 0.00172.
2024-12-02-04:33:02-root-INFO: grad norm: 301.511 289.072 85.711
2024-12-02-04:33:02-root-INFO: Loss too large (1691.250->1722.691)! Learning rate decreased to 0.00138.
2024-12-02-04:33:03-root-INFO: Loss too large (1691.250->1696.641)! Learning rate decreased to 0.00110.
2024-12-02-04:33:04-root-INFO: grad norm: 221.696 210.125 70.685
2024-12-02-04:33:05-root-INFO: grad norm: 154.858 147.745 46.395
2024-12-02-04:33:06-root-INFO: grad norm: 146.647 138.983 46.789
2024-12-02-04:33:07-root-INFO: Loss Change: 1697.342 -> 1645.433
2024-12-02-04:33:07-root-INFO: Regularization Change: 0.000 -> 0.275
2024-12-02-04:33:07-root-INFO: Learning rate of xt decay: 0.06997 -> 0.07081.
2024-12-02-04:33:07-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-04:33:07-root-INFO: step: 177 lr_xt 0.00546525
2024-12-02-04:33:07-root-INFO: grad norm: 274.162 262.762 78.235
2024-12-02-04:33:08-root-INFO: Loss too large (1649.171->1879.458)! Learning rate decreased to 0.00437.
2024-12-02-04:33:08-root-INFO: Loss too large (1649.171->1845.002)! Learning rate decreased to 0.00350.
2024-12-02-04:33:08-root-INFO: Loss too large (1649.171->1807.654)! Learning rate decreased to 0.00280.
2024-12-02-04:33:09-root-INFO: Loss too large (1649.171->1768.560)! Learning rate decreased to 0.00224.
2024-12-02-04:33:09-root-INFO: Loss too large (1649.171->1730.150)! Learning rate decreased to 0.00179.
2024-12-02-04:33:09-root-INFO: Loss too large (1649.171->1695.835)! Learning rate decreased to 0.00143.
2024-12-02-04:33:10-root-INFO: Loss too large (1649.171->1668.494)! Learning rate decreased to 0.00115.
2024-12-02-04:33:10-root-INFO: Loss too large (1649.171->1649.196)! Learning rate decreased to 0.00092.
2024-12-02-04:33:11-root-INFO: grad norm: 200.547 191.261 60.317
2024-12-02-04:33:12-root-INFO: grad norm: 145.941 139.293 43.546
2024-12-02-04:33:13-root-INFO: grad norm: 130.306 123.942 40.226
2024-12-02-04:33:14-root-INFO: grad norm: 119.907 114.369 36.020
2024-12-02-04:33:15-root-INFO: Loss Change: 1649.171 -> 1609.457
2024-12-02-04:33:15-root-INFO: Regularization Change: 0.000 -> 0.142
2024-12-02-04:33:15-root-INFO: Learning rate of xt decay: 0.07081 -> 0.07166.
2024-12-02-04:33:15-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-04:33:15-root-INFO: step: 176 lr_xt 0.00569148
2024-12-02-04:33:15-root-INFO: grad norm: 119.150 112.913 38.044
2024-12-02-04:33:16-root-INFO: Loss too large (1603.019->1685.156)! Learning rate decreased to 0.00455.
2024-12-02-04:33:16-root-INFO: Loss too large (1603.019->1651.968)! Learning rate decreased to 0.00364.
2024-12-02-04:33:16-root-INFO: Loss too large (1603.019->1628.446)! Learning rate decreased to 0.00291.
2024-12-02-04:33:17-root-INFO: Loss too large (1603.019->1613.003)! Learning rate decreased to 0.00233.
2024-12-02-04:33:17-root-INFO: Loss too large (1603.019->1603.632)! Learning rate decreased to 0.00186.
2024-12-02-04:33:18-root-INFO: grad norm: 207.629 199.037 59.110
2024-12-02-04:33:18-root-INFO: Loss too large (1598.461->1658.144)! Learning rate decreased to 0.00149.
2024-12-02-04:33:19-root-INFO: Loss too large (1598.461->1625.784)! Learning rate decreased to 0.00119.
2024-12-02-04:33:19-root-INFO: Loss too large (1598.461->1605.573)! Learning rate decreased to 0.00095.
2024-12-02-04:33:20-root-INFO: grad norm: 224.448 216.719 58.391
2024-12-02-04:33:21-root-INFO: grad norm: 234.262 224.659 66.383
2024-12-02-04:33:22-root-INFO: grad norm: 245.176 236.985 62.845
2024-12-02-04:33:22-root-INFO: Loss too large (1588.581->1588.909)! Learning rate decreased to 0.00076.
2024-12-02-04:33:23-root-INFO: Loss Change: 1603.019 -> 1580.299
2024-12-02-04:33:23-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-02-04:33:23-root-INFO: Learning rate of xt decay: 0.07166 -> 0.07252.
2024-12-02-04:33:23-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:33:23-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-04:33:24-root-INFO: grad norm: 91.205 87.091 27.084
2024-12-02-04:33:25-root-INFO: grad norm: 312.997 306.792 62.014
2024-12-02-04:33:25-root-INFO: Loss too large (1567.828->1807.183)! Learning rate decreased to 0.00474.
2024-12-02-04:33:25-root-INFO: Loss too large (1567.828->1784.158)! Learning rate decreased to 0.00379.
2024-12-02-04:33:26-root-INFO: Loss too large (1567.828->1760.344)! Learning rate decreased to 0.00303.
2024-12-02-04:33:26-root-INFO: Loss too large (1567.828->1732.662)! Learning rate decreased to 0.00243.
2024-12-02-04:33:26-root-INFO: Loss too large (1567.828->1700.289)! Learning rate decreased to 0.00194.
2024-12-02-04:33:27-root-INFO: Loss too large (1567.828->1664.309)! Learning rate decreased to 0.00155.
2024-12-02-04:33:27-root-INFO: Loss too large (1567.828->1628.235)! Learning rate decreased to 0.00124.
2024-12-02-04:33:27-root-INFO: Loss too large (1567.828->1596.594)! Learning rate decreased to 0.00099.
2024-12-02-04:33:28-root-INFO: Loss too large (1567.828->1572.531)! Learning rate decreased to 0.00080.
2024-12-02-04:33:28-root-INFO: grad norm: 274.031 263.018 76.907
2024-12-02-04:33:30-root-INFO: grad norm: 239.483 233.436 53.473
2024-12-02-04:33:30-root-INFO: grad norm: 215.735 207.251 59.905
2024-12-02-04:33:31-root-INFO: Loss Change: 1567.920 -> 1536.071
2024-12-02-04:33:31-root-INFO: Regularization Change: 0.000 -> 0.368
2024-12-02-04:33:31-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-04:33:31-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:33:32-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-04:33:32-root-INFO: grad norm: 271.199 263.495 64.182
2024-12-02-04:33:32-root-INFO: Loss too large (1540.602->1794.231)! Learning rate decreased to 0.00494.
2024-12-02-04:33:33-root-INFO: Loss too large (1540.602->1774.141)! Learning rate decreased to 0.00395.
2024-12-02-04:33:33-root-INFO: Loss too large (1540.602->1748.719)! Learning rate decreased to 0.00316.
2024-12-02-04:33:33-root-INFO: Loss too large (1540.602->1717.332)! Learning rate decreased to 0.00253.
2024-12-02-04:33:34-root-INFO: Loss too large (1540.602->1680.684)! Learning rate decreased to 0.00202.
2024-12-02-04:33:34-root-INFO: Loss too large (1540.602->1641.349)! Learning rate decreased to 0.00162.
2024-12-02-04:33:34-root-INFO: Loss too large (1540.602->1603.863)! Learning rate decreased to 0.00129.
2024-12-02-04:33:35-root-INFO: Loss too large (1540.602->1572.572)! Learning rate decreased to 0.00104.
2024-12-02-04:33:35-root-INFO: Loss too large (1540.602->1549.724)! Learning rate decreased to 0.00083.
2024-12-02-04:33:36-root-INFO: grad norm: 242.903 234.260 64.218
2024-12-02-04:33:37-root-INFO: grad norm: 219.365 212.806 53.244
2024-12-02-04:33:38-root-INFO: grad norm: 204.212 196.895 54.176
2024-12-02-04:33:39-root-INFO: grad norm: 190.885 185.017 46.966
2024-12-02-04:33:40-root-INFO: Loss Change: 1540.602 -> 1517.602
2024-12-02-04:33:40-root-INFO: Regularization Change: 0.000 -> 0.094
2024-12-02-04:33:40-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-04:33:40-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:33:40-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-04:33:40-root-INFO: grad norm: 101.432 97.236 28.873
2024-12-02-04:33:41-root-INFO: grad norm: 240.250 231.557 64.042
2024-12-02-04:33:42-root-INFO: Loss too large (1511.377->2058.868)! Learning rate decreased to 0.00514.
2024-12-02-04:33:42-root-INFO: Loss too large (1511.377->1906.291)! Learning rate decreased to 0.00411.
2024-12-02-04:33:42-root-INFO: Loss too large (1511.377->1791.517)! Learning rate decreased to 0.00329.
2024-12-02-04:33:43-root-INFO: Loss too large (1511.377->1703.717)! Learning rate decreased to 0.00263.
2024-12-02-04:33:43-root-INFO: Loss too large (1511.377->1636.493)! Learning rate decreased to 0.00210.
2024-12-02-04:33:43-root-INFO: Loss too large (1511.377->1586.080)! Learning rate decreased to 0.00168.
2024-12-02-04:33:44-root-INFO: Loss too large (1511.377->1549.828)! Learning rate decreased to 0.00135.
2024-12-02-04:33:44-root-INFO: Loss too large (1511.377->1525.282)! Learning rate decreased to 0.00108.
2024-12-02-04:33:45-root-INFO: grad norm: 323.593 317.767 61.128
2024-12-02-04:33:45-root-INFO: Loss too large (1509.909->1519.376)! Learning rate decreased to 0.00086.
2024-12-02-04:33:46-root-INFO: grad norm: 299.540 288.796 79.507
2024-12-02-04:33:47-root-INFO: grad norm: 276.285 270.072 58.262
2024-12-02-04:33:48-root-INFO: Loss Change: 1512.609 -> 1485.641
2024-12-02-04:33:48-root-INFO: Regularization Change: 0.000 -> 0.435
2024-12-02-04:33:48-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-04:33:48-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-04:33:48-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-04:33:49-root-INFO: grad norm: 191.612 185.784 46.900
2024-12-02-04:33:49-root-INFO: Loss too large (1471.856->1910.648)! Learning rate decreased to 0.00535.
2024-12-02-04:33:49-root-INFO: Loss too large (1471.856->1809.721)! Learning rate decreased to 0.00428.
2024-12-02-04:33:50-root-INFO: Loss too large (1471.856->1724.643)! Learning rate decreased to 0.00342.
2024-12-02-04:33:50-root-INFO: Loss too large (1471.856->1653.294)! Learning rate decreased to 0.00274.
2024-12-02-04:33:50-root-INFO: Loss too large (1471.856->1595.062)! Learning rate decreased to 0.00219.
2024-12-02-04:33:51-root-INFO: Loss too large (1471.856->1549.660)! Learning rate decreased to 0.00175.
2024-12-02-04:33:51-root-INFO: Loss too large (1471.856->1516.277)! Learning rate decreased to 0.00140.
2024-12-02-04:33:51-root-INFO: Loss too large (1471.856->1493.312)! Learning rate decreased to 0.00112.
2024-12-02-04:33:52-root-INFO: Loss too large (1471.856->1478.620)! Learning rate decreased to 0.00090.
2024-12-02-04:33:53-root-INFO: grad norm: 197.505 191.987 46.361
2024-12-02-04:33:54-root-INFO: grad norm: 201.515 195.213 50.002
2024-12-02-04:33:55-root-INFO: grad norm: 202.579 197.047 47.020
2024-12-02-04:33:56-root-INFO: grad norm: 201.490 195.100 50.340
2024-12-02-04:33:56-root-INFO: Loss Change: 1471.856 -> 1459.812
2024-12-02-04:33:56-root-INFO: Regularization Change: 0.000 -> 0.089
2024-12-02-04:33:56-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-04:33:56-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:33:57-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-04:33:57-root-INFO: grad norm: 320.087 311.961 71.665
2024-12-02-04:33:57-root-INFO: Loss too large (1474.368->1753.365)! Learning rate decreased to 0.00556.
2024-12-02-04:33:58-root-INFO: Loss too large (1474.368->1733.078)! Learning rate decreased to 0.00445.
2024-12-02-04:33:58-root-INFO: Loss too large (1474.368->1710.278)! Learning rate decreased to 0.00356.
2024-12-02-04:33:58-root-INFO: Loss too large (1474.368->1682.271)! Learning rate decreased to 0.00285.
2024-12-02-04:33:59-root-INFO: Loss too large (1474.368->1647.373)! Learning rate decreased to 0.00228.
2024-12-02-04:33:59-root-INFO: Loss too large (1474.368->1605.450)! Learning rate decreased to 0.00182.
2024-12-02-04:33:59-root-INFO: Loss too large (1474.368->1559.364)! Learning rate decreased to 0.00146.
2024-12-02-04:34:00-root-INFO: Loss too large (1474.368->1515.837)! Learning rate decreased to 0.00117.
2024-12-02-04:34:00-root-INFO: Loss too large (1474.368->1481.506)! Learning rate decreased to 0.00093.
2024-12-02-04:34:01-root-INFO: grad norm: 275.800 267.250 68.144
2024-12-02-04:34:02-root-INFO: grad norm: 252.816 246.240 57.286
2024-12-02-04:34:03-root-INFO: grad norm: 231.236 223.892 57.816
2024-12-02-04:34:04-root-INFO: grad norm: 216.017 210.264 49.521
2024-12-02-04:34:05-root-INFO: Loss Change: 1474.368 -> 1438.584
2024-12-02-04:34:05-root-INFO: Regularization Change: 0.000 -> 0.145
2024-12-02-04:34:05-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-04:34:05-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:34:05-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-04:34:05-root-INFO: grad norm: 123.550 120.296 28.169
2024-12-02-04:34:06-root-INFO: Loss too large (1432.088->1619.119)! Learning rate decreased to 0.00579.
2024-12-02-04:34:06-root-INFO: Loss too large (1432.088->1561.689)! Learning rate decreased to 0.00463.
2024-12-02-04:34:06-root-INFO: Loss too large (1432.088->1517.404)! Learning rate decreased to 0.00370.
2024-12-02-04:34:07-root-INFO: Loss too large (1432.088->1484.530)! Learning rate decreased to 0.00296.
2024-12-02-04:34:07-root-INFO: Loss too large (1432.088->1461.306)! Learning rate decreased to 0.00237.
2024-12-02-04:34:07-root-INFO: Loss too large (1432.088->1445.800)! Learning rate decreased to 0.00190.
2024-12-02-04:34:08-root-INFO: Loss too large (1432.088->1436.082)! Learning rate decreased to 0.00152.
2024-12-02-04:34:08-root-INFO: grad norm: 196.843 191.311 46.340
2024-12-02-04:34:09-root-INFO: Loss too large (1430.436->1451.736)! Learning rate decreased to 0.00121.
2024-12-02-04:34:09-root-INFO: Loss too large (1430.436->1435.116)! Learning rate decreased to 0.00097.
2024-12-02-04:34:10-root-INFO: grad norm: 194.791 188.910 47.503
2024-12-02-04:34:11-root-INFO: grad norm: 190.743 185.615 43.932
2024-12-02-04:34:12-root-INFO: grad norm: 184.157 178.500 45.295
2024-12-02-04:34:13-root-INFO: Loss Change: 1432.088 -> 1415.929
2024-12-02-04:34:13-root-INFO: Regularization Change: 0.000 -> 0.133
2024-12-02-04:34:13-root-INFO: Undo step: 170
2024-12-02-04:34:13-root-INFO: Undo step: 171
2024-12-02-04:34:13-root-INFO: Undo step: 172
2024-12-02-04:34:13-root-INFO: Undo step: 173
2024-12-02-04:34:13-root-INFO: Undo step: 174
2024-12-02-04:34:13-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-04:34:13-root-INFO: grad norm: 634.262 602.795 197.299
2024-12-02-04:34:14-root-INFO: grad norm: 696.313 675.579 168.658
2024-12-02-04:34:15-root-INFO: Loss too large (2200.656->2229.495)! Learning rate decreased to 0.00474.
2024-12-02-04:34:16-root-INFO: grad norm: 248.967 241.531 60.394
2024-12-02-04:34:17-root-INFO: grad norm: 139.260 132.924 41.530
2024-12-02-04:34:18-root-INFO: grad norm: 139.184 134.785 34.717
2024-12-02-04:34:18-root-INFO: Loss Change: 2575.296 -> 1855.663
2024-12-02-04:34:18-root-INFO: Regularization Change: 0.000 -> 15.601
2024-12-02-04:34:18-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-04:34:18-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:34:19-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-04:34:19-root-INFO: grad norm: 156.201 150.630 41.342
2024-12-02-04:34:20-root-INFO: grad norm: 250.769 243.553 59.724
2024-12-02-04:34:20-root-INFO: Loss too large (1833.610->1846.825)! Learning rate decreased to 0.00494.
2024-12-02-04:34:21-root-INFO: grad norm: 294.877 287.363 66.141
2024-12-02-04:34:22-root-INFO: grad norm: 310.105 300.326 77.264
2024-12-02-04:34:23-root-INFO: Loss too large (1640.982->1848.764)! Learning rate decreased to 0.00395.
2024-12-02-04:34:23-root-INFO: Loss too large (1640.982->1701.926)! Learning rate decreased to 0.00316.
2024-12-02-04:34:24-root-INFO: grad norm: 446.919 437.422 91.640
2024-12-02-04:34:24-root-INFO: Loss too large (1608.013->1699.131)! Learning rate decreased to 0.00253.
2024-12-02-04:34:25-root-INFO: Loss too large (1608.013->1651.489)! Learning rate decreased to 0.00202.
2024-12-02-04:34:25-root-INFO: Loss Change: 1850.276 -> 1606.650
2024-12-02-04:34:25-root-INFO: Regularization Change: 0.000 -> 3.532
2024-12-02-04:34:25-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-04:34:25-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:34:26-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-04:34:26-root-INFO: grad norm: 221.313 212.891 60.474
2024-12-02-04:34:26-root-INFO: Loss too large (1581.819->1821.547)! Learning rate decreased to 0.00514.
2024-12-02-04:34:27-root-INFO: Loss too large (1581.819->1683.918)! Learning rate decreased to 0.00411.
2024-12-02-04:34:27-root-INFO: Loss too large (1581.819->1584.589)! Learning rate decreased to 0.00329.
2024-12-02-04:34:28-root-INFO: grad norm: 288.342 278.396 75.078
2024-12-02-04:34:29-root-INFO: Loss too large (1526.515->1613.738)! Learning rate decreased to 0.00263.
2024-12-02-04:34:29-root-INFO: Loss too large (1526.515->1568.065)! Learning rate decreased to 0.00210.
2024-12-02-04:34:29-root-INFO: Loss too large (1526.515->1531.803)! Learning rate decreased to 0.00168.
2024-12-02-04:34:30-root-INFO: grad norm: 227.462 219.936 58.028
2024-12-02-04:34:31-root-INFO: grad norm: 228.260 222.113 52.616
2024-12-02-04:34:32-root-INFO: Loss too large (1482.535->1492.790)! Learning rate decreased to 0.00135.
2024-12-02-04:34:33-root-INFO: grad norm: 210.039 203.980 50.084
2024-12-02-04:34:33-root-INFO: Loss Change: 1581.819 -> 1465.290
2024-12-02-04:34:33-root-INFO: Regularization Change: 0.000 -> 0.719
2024-12-02-04:34:33-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-04:34:33-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-04:34:34-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-04:34:34-root-INFO: grad norm: 296.294 289.381 63.632
2024-12-02-04:34:34-root-INFO: Loss too large (1470.853->1717.970)! Learning rate decreased to 0.00535.
2024-12-02-04:34:35-root-INFO: Loss too large (1470.853->1689.561)! Learning rate decreased to 0.00428.
2024-12-02-04:34:35-root-INFO: Loss too large (1470.853->1656.359)! Learning rate decreased to 0.00342.
2024-12-02-04:34:35-root-INFO: Loss too large (1470.853->1618.889)! Learning rate decreased to 0.00274.
2024-12-02-04:34:36-root-INFO: Loss too large (1470.853->1579.421)! Learning rate decreased to 0.00219.
2024-12-02-04:34:36-root-INFO: Loss too large (1470.853->1540.887)! Learning rate decreased to 0.00175.
2024-12-02-04:34:36-root-INFO: Loss too large (1470.853->1506.558)! Learning rate decreased to 0.00140.
2024-12-02-04:34:37-root-INFO: Loss too large (1470.853->1479.321)! Learning rate decreased to 0.00112.
2024-12-02-04:34:38-root-INFO: grad norm: 225.202 219.317 51.144
2024-12-02-04:34:39-root-INFO: grad norm: 172.815 168.120 40.009
2024-12-02-04:34:40-root-INFO: grad norm: 172.879 168.765 37.490
2024-12-02-04:34:41-root-INFO: grad norm: 186.320 182.076 39.539
2024-12-02-04:34:41-root-INFO: Loss too large (1435.024->1435.045)! Learning rate decreased to 0.00090.
2024-12-02-04:34:42-root-INFO: Loss Change: 1470.853 -> 1429.171
2024-12-02-04:34:42-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-02-04:34:42-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-04:34:42-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:34:42-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-04:34:42-root-INFO: grad norm: 77.927 75.006 21.136
2024-12-02-04:34:43-root-INFO: grad norm: 180.339 178.525 25.518
2024-12-02-04:34:44-root-INFO: Loss too large (1402.627->1619.344)! Learning rate decreased to 0.00556.
2024-12-02-04:34:44-root-INFO: Loss too large (1402.627->1585.751)! Learning rate decreased to 0.00445.
2024-12-02-04:34:44-root-INFO: Loss too large (1402.627->1553.657)! Learning rate decreased to 0.00356.
2024-12-02-04:34:45-root-INFO: Loss too large (1402.627->1521.587)! Learning rate decreased to 0.00285.
2024-12-02-04:34:45-root-INFO: Loss too large (1402.627->1490.154)! Learning rate decreased to 0.00228.
2024-12-02-04:34:45-root-INFO: Loss too large (1402.627->1461.378)! Learning rate decreased to 0.00182.
2024-12-02-04:34:45-root-INFO: Loss too large (1402.627->1437.435)! Learning rate decreased to 0.00146.
2024-12-02-04:34:46-root-INFO: Loss too large (1402.627->1419.501)! Learning rate decreased to 0.00117.
2024-12-02-04:34:46-root-INFO: Loss too large (1402.627->1407.423)! Learning rate decreased to 0.00093.
2024-12-02-04:34:47-root-INFO: grad norm: 225.907 220.196 50.473
2024-12-02-04:34:47-root-INFO: Loss too large (1400.138->1402.456)! Learning rate decreased to 0.00075.
2024-12-02-04:34:48-root-INFO: grad norm: 198.607 196.169 31.024
2024-12-02-04:34:49-root-INFO: grad norm: 177.278 173.023 38.609
2024-12-02-04:34:50-root-INFO: Loss Change: 1416.421 -> 1386.263
2024-12-02-04:34:50-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-04:34:50-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-04:34:50-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:34:50-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-04:34:51-root-INFO: grad norm: 290.796 286.234 51.303
2024-12-02-04:34:51-root-INFO: Loss too large (1393.776->1666.050)! Learning rate decreased to 0.00579.
2024-12-02-04:34:51-root-INFO: Loss too large (1393.776->1647.369)! Learning rate decreased to 0.00463.
2024-12-02-04:34:52-root-INFO: Loss too large (1393.776->1625.168)! Learning rate decreased to 0.00370.
2024-12-02-04:34:52-root-INFO: Loss too large (1393.776->1598.090)! Learning rate decreased to 0.00296.
2024-12-02-04:34:52-root-INFO: Loss too large (1393.776->1566.045)! Learning rate decreased to 0.00237.
2024-12-02-04:34:53-root-INFO: Loss too large (1393.776->1529.885)! Learning rate decreased to 0.00190.
2024-12-02-04:34:53-root-INFO: Loss too large (1393.776->1491.503)! Learning rate decreased to 0.00152.
2024-12-02-04:34:53-root-INFO: Loss too large (1393.776->1454.401)! Learning rate decreased to 0.00121.
2024-12-02-04:34:54-root-INFO: Loss too large (1393.776->1422.780)! Learning rate decreased to 0.00097.
2024-12-02-04:34:54-root-INFO: Loss too large (1393.776->1399.464)! Learning rate decreased to 0.00078.
2024-12-02-04:34:55-root-INFO: grad norm: 239.901 235.189 47.315
2024-12-02-04:34:56-root-INFO: grad norm: 207.817 204.231 38.441
2024-12-02-04:34:57-root-INFO: grad norm: 188.559 184.814 37.394
2024-12-02-04:34:58-root-INFO: grad norm: 174.179 171.093 32.638
2024-12-02-04:34:59-root-INFO: Loss Change: 1393.776 -> 1368.353
2024-12-02-04:34:59-root-INFO: Regularization Change: 0.000 -> 0.081
2024-12-02-04:34:59-root-INFO: Undo step: 170
2024-12-02-04:34:59-root-INFO: Undo step: 171
2024-12-02-04:34:59-root-INFO: Undo step: 172
2024-12-02-04:34:59-root-INFO: Undo step: 173
2024-12-02-04:34:59-root-INFO: Undo step: 174
2024-12-02-04:34:59-root-INFO: step: 175 lr_xt 0.00592610
2024-12-02-04:35:00-root-INFO: grad norm: 554.196 524.220 179.794
2024-12-02-04:35:00-root-INFO: Loss too large (2540.958->2615.312)! Learning rate decreased to 0.00474.
2024-12-02-04:35:01-root-INFO: grad norm: 796.893 771.987 197.669
2024-12-02-04:35:01-root-INFO: Loss too large (2333.627->2739.227)! Learning rate decreased to 0.00379.
2024-12-02-04:35:02-root-INFO: Loss too large (2333.627->2351.652)! Learning rate decreased to 0.00303.
2024-12-02-04:35:03-root-INFO: grad norm: 567.117 550.629 135.754
2024-12-02-04:35:04-root-INFO: grad norm: 495.944 479.306 127.380
2024-12-02-04:35:05-root-INFO: grad norm: 401.607 386.350 109.644
2024-12-02-04:35:05-root-INFO: Loss Change: 2540.958 -> 1747.282
2024-12-02-04:35:05-root-INFO: Regularization Change: 0.000 -> 11.107
2024-12-02-04:35:05-root-INFO: Learning rate of xt decay: 0.07252 -> 0.07339.
2024-12-02-04:35:05-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:35:06-root-INFO: step: 174 lr_xt 0.00616941
2024-12-02-04:35:06-root-INFO: grad norm: 358.676 345.952 94.687
2024-12-02-04:35:06-root-INFO: Loss too large (1734.534->2226.438)! Learning rate decreased to 0.00494.
2024-12-02-04:35:07-root-INFO: Loss too large (1734.534->1976.156)! Learning rate decreased to 0.00395.
2024-12-02-04:35:07-root-INFO: Loss too large (1734.534->1803.447)! Learning rate decreased to 0.00316.
2024-12-02-04:35:08-root-INFO: grad norm: 394.951 379.110 110.735
2024-12-02-04:35:08-root-INFO: Loss too large (1695.008->1718.375)! Learning rate decreased to 0.00253.
2024-12-02-04:35:09-root-INFO: grad norm: 284.316 270.281 88.226
2024-12-02-04:35:10-root-INFO: grad norm: 213.413 201.564 70.123
2024-12-02-04:35:11-root-INFO: grad norm: 255.353 244.828 72.558
2024-12-02-04:35:12-root-INFO: Loss too large (1573.024->1589.633)! Learning rate decreased to 0.00202.
2024-12-02-04:35:12-root-INFO: Loss Change: 1734.534 -> 1553.016
2024-12-02-04:35:12-root-INFO: Regularization Change: 0.000 -> 1.722
2024-12-02-04:35:12-root-INFO: Learning rate of xt decay: 0.07339 -> 0.07427.
2024-12-02-04:35:12-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-04:35:13-root-INFO: step: 173 lr_xt 0.00642166
2024-12-02-04:35:13-root-INFO: grad norm: 373.996 363.146 89.431
2024-12-02-04:35:13-root-INFO: Loss too large (1583.704->1805.575)! Learning rate decreased to 0.00514.
2024-12-02-04:35:14-root-INFO: Loss too large (1583.704->1763.783)! Learning rate decreased to 0.00411.
2024-12-02-04:35:14-root-INFO: Loss too large (1583.704->1718.475)! Learning rate decreased to 0.00329.
2024-12-02-04:35:14-root-INFO: Loss too large (1583.704->1669.874)! Learning rate decreased to 0.00263.
2024-12-02-04:35:15-root-INFO: Loss too large (1583.704->1621.282)! Learning rate decreased to 0.00210.
2024-12-02-04:35:16-root-INFO: grad norm: 257.957 246.560 75.829
2024-12-02-04:35:17-root-INFO: grad norm: 141.173 132.014 50.022
2024-12-02-04:35:18-root-INFO: grad norm: 150.947 145.901 38.704
2024-12-02-04:35:18-root-INFO: Loss too large (1491.756->1494.077)! Learning rate decreased to 0.00168.
2024-12-02-04:35:19-root-INFO: grad norm: 175.620 170.054 43.864
2024-12-02-04:35:19-root-INFO: Loss too large (1484.671->1485.059)! Learning rate decreased to 0.00135.
2024-12-02-04:35:20-root-INFO: Loss Change: 1583.704 -> 1477.222
2024-12-02-04:35:20-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-02-04:35:20-root-INFO: Learning rate of xt decay: 0.07427 -> 0.07517.
2024-12-02-04:35:20-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-04:35:20-root-INFO: step: 172 lr_xt 0.00668315
2024-12-02-04:35:21-root-INFO: grad norm: 114.344 110.661 28.788
2024-12-02-04:35:21-root-INFO: Loss too large (1464.354->1606.469)! Learning rate decreased to 0.00535.
2024-12-02-04:35:21-root-INFO: Loss too large (1464.354->1555.730)! Learning rate decreased to 0.00428.
2024-12-02-04:35:22-root-INFO: Loss too large (1464.354->1517.788)! Learning rate decreased to 0.00342.
2024-12-02-04:35:22-root-INFO: Loss too large (1464.354->1491.406)! Learning rate decreased to 0.00274.
2024-12-02-04:35:22-root-INFO: Loss too large (1464.354->1474.502)! Learning rate decreased to 0.00219.
2024-12-02-04:35:23-root-INFO: Loss too large (1464.354->1464.617)! Learning rate decreased to 0.00175.
2024-12-02-04:35:24-root-INFO: grad norm: 157.661 152.955 38.234
2024-12-02-04:35:24-root-INFO: Loss too large (1459.456->1464.896)! Learning rate decreased to 0.00140.
2024-12-02-04:35:25-root-INFO: grad norm: 171.231 166.432 40.255
2024-12-02-04:35:26-root-INFO: grad norm: 193.774 189.135 42.147
2024-12-02-04:35:26-root-INFO: Loss too large (1452.767->1453.284)! Learning rate decreased to 0.00112.
2024-12-02-04:35:27-root-INFO: grad norm: 154.736 150.452 36.157
2024-12-02-04:35:28-root-INFO: Loss Change: 1464.354 -> 1438.200
2024-12-02-04:35:28-root-INFO: Regularization Change: 0.000 -> 0.194
2024-12-02-04:35:28-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07607.
2024-12-02-04:35:28-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:35:29-root-INFO: step: 171 lr_xt 0.00695416
2024-12-02-04:35:29-root-INFO: grad norm: 248.365 242.847 52.060
2024-12-02-04:35:29-root-INFO: Loss too large (1446.286->1680.780)! Learning rate decreased to 0.00556.
2024-12-02-04:35:29-root-INFO: Loss too large (1446.286->1650.393)! Learning rate decreased to 0.00445.
2024-12-02-04:35:30-root-INFO: Loss too large (1446.286->1615.374)! Learning rate decreased to 0.00356.
2024-12-02-04:35:30-root-INFO: Loss too large (1446.286->1577.149)! Learning rate decreased to 0.00285.
2024-12-02-04:35:30-root-INFO: Loss too large (1446.286->1538.120)! Learning rate decreased to 0.00228.
2024-12-02-04:35:31-root-INFO: Loss too large (1446.286->1501.542)! Learning rate decreased to 0.00182.
2024-12-02-04:35:31-root-INFO: Loss too large (1446.286->1470.840)! Learning rate decreased to 0.00146.
2024-12-02-04:35:31-root-INFO: Loss too large (1446.286->1448.264)! Learning rate decreased to 0.00117.
2024-12-02-04:35:32-root-INFO: grad norm: 194.277 189.087 44.609
2024-12-02-04:35:33-root-INFO: grad norm: 163.030 158.917 36.387
2024-12-02-04:35:34-root-INFO: grad norm: 151.187 147.104 34.901
2024-12-02-04:35:35-root-INFO: grad norm: 144.540 140.963 31.955
2024-12-02-04:35:36-root-INFO: Loss Change: 1446.286 -> 1411.510
2024-12-02-04:35:36-root-INFO: Regularization Change: 0.000 -> 0.171
2024-12-02-04:35:36-root-INFO: Learning rate of xt decay: 0.07607 -> 0.07698.
2024-12-02-04:35:36-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:35:37-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-04:35:37-root-INFO: grad norm: 81.060 77.406 24.065
2024-12-02-04:35:37-root-INFO: Loss too large (1402.734->1413.982)! Learning rate decreased to 0.00579.
2024-12-02-04:35:37-root-INFO: Loss too large (1402.734->1402.803)! Learning rate decreased to 0.00463.
2024-12-02-04:35:38-root-INFO: grad norm: 207.776 204.749 35.335
2024-12-02-04:35:39-root-INFO: Loss too large (1396.777->1559.145)! Learning rate decreased to 0.00370.
2024-12-02-04:35:39-root-INFO: Loss too large (1396.777->1525.549)! Learning rate decreased to 0.00296.
2024-12-02-04:35:39-root-INFO: Loss too large (1396.777->1491.211)! Learning rate decreased to 0.00237.
2024-12-02-04:35:40-root-INFO: Loss too large (1396.777->1458.610)! Learning rate decreased to 0.00190.
2024-12-02-04:35:40-root-INFO: Loss too large (1396.777->1430.674)! Learning rate decreased to 0.00152.
2024-12-02-04:35:40-root-INFO: Loss too large (1396.777->1409.514)! Learning rate decreased to 0.00121.
2024-12-02-04:35:41-root-INFO: grad norm: 231.136 224.995 52.923
2024-12-02-04:35:42-root-INFO: grad norm: 241.074 237.189 43.105
2024-12-02-04:35:43-root-INFO: grad norm: 238.219 231.857 54.686
2024-12-02-04:35:44-root-INFO: Loss Change: 1402.734 -> 1387.289
2024-12-02-04:35:44-root-INFO: Regularization Change: 0.000 -> 0.322
2024-12-02-04:35:44-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-04:35:44-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:35:44-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-04:35:45-root-INFO: grad norm: 300.367 295.048 56.276
2024-12-02-04:35:45-root-INFO: Loss too large (1402.328->1657.157)! Learning rate decreased to 0.00602.
2024-12-02-04:35:45-root-INFO: Loss too large (1402.328->1633.016)! Learning rate decreased to 0.00482.
2024-12-02-04:35:46-root-INFO: Loss too large (1402.328->1603.529)! Learning rate decreased to 0.00385.
2024-12-02-04:35:46-root-INFO: Loss too large (1402.328->1567.640)! Learning rate decreased to 0.00308.
2024-12-02-04:35:46-root-INFO: Loss too large (1402.328->1526.232)! Learning rate decreased to 0.00247.
2024-12-02-04:35:47-root-INFO: Loss too large (1402.328->1481.941)! Learning rate decreased to 0.00197.
2024-12-02-04:35:47-root-INFO: Loss too large (1402.328->1439.472)! Learning rate decreased to 0.00158.
2024-12-02-04:35:47-root-INFO: Loss too large (1402.328->1404.455)! Learning rate decreased to 0.00126.
2024-12-02-04:35:48-root-INFO: grad norm: 236.396 230.362 53.069
2024-12-02-04:35:49-root-INFO: grad norm: 212.210 207.915 42.478
2024-12-02-04:35:50-root-INFO: grad norm: 197.453 192.378 44.481
2024-12-02-04:35:51-root-INFO: grad norm: 187.695 183.802 38.030
2024-12-02-04:35:52-root-INFO: Loss Change: 1402.328 -> 1358.229
2024-12-02-04:35:52-root-INFO: Regularization Change: 0.000 -> 0.224
2024-12-02-04:35:52-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-04:35:52-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:35:52-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-04:35:53-root-INFO: grad norm: 103.185 100.570 23.083
2024-12-02-04:35:53-root-INFO: Loss too large (1348.091->1503.729)! Learning rate decreased to 0.00626.
2024-12-02-04:35:53-root-INFO: Loss too large (1348.091->1456.243)! Learning rate decreased to 0.00501.
2024-12-02-04:35:54-root-INFO: Loss too large (1348.091->1419.511)! Learning rate decreased to 0.00401.
2024-12-02-04:35:54-root-INFO: Loss too large (1348.091->1392.253)! Learning rate decreased to 0.00321.
2024-12-02-04:35:54-root-INFO: Loss too large (1348.091->1373.020)! Learning rate decreased to 0.00256.
2024-12-02-04:35:55-root-INFO: Loss too large (1348.091->1360.185)! Learning rate decreased to 0.00205.
2024-12-02-04:35:55-root-INFO: Loss too large (1348.091->1352.131)! Learning rate decreased to 0.00164.
2024-12-02-04:35:56-root-INFO: grad norm: 148.467 145.250 30.740
2024-12-02-04:35:56-root-INFO: Loss too large (1347.432->1352.749)! Learning rate decreased to 0.00131.
2024-12-02-04:35:57-root-INFO: grad norm: 163.139 159.052 36.288
2024-12-02-04:35:58-root-INFO: grad norm: 173.373 169.942 34.321
2024-12-02-04:35:59-root-INFO: grad norm: 182.929 178.236 41.168
2024-12-02-04:36:00-root-INFO: Loss Change: 1348.091 -> 1340.255
2024-12-02-04:36:00-root-INFO: Regularization Change: 0.000 -> 0.151
2024-12-02-04:36:00-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-04:36:00-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-04:36:00-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-04:36:01-root-INFO: grad norm: 262.295 257.011 52.382
2024-12-02-04:36:01-root-INFO: Loss too large (1357.192->1621.990)! Learning rate decreased to 0.00651.
2024-12-02-04:36:01-root-INFO: Loss too large (1357.192->1595.505)! Learning rate decreased to 0.00521.
2024-12-02-04:36:02-root-INFO: Loss too large (1357.192->1563.725)! Learning rate decreased to 0.00417.
2024-12-02-04:36:02-root-INFO: Loss too large (1357.192->1525.310)! Learning rate decreased to 0.00333.
2024-12-02-04:36:02-root-INFO: Loss too large (1357.192->1481.021)! Learning rate decreased to 0.00267.
2024-12-02-04:36:02-root-INFO: Loss too large (1357.192->1434.271)! Learning rate decreased to 0.00213.
2024-12-02-04:36:03-root-INFO: Loss too large (1357.192->1391.228)! Learning rate decreased to 0.00171.
2024-12-02-04:36:03-root-INFO: Loss too large (1357.192->1358.035)! Learning rate decreased to 0.00137.
2024-12-02-04:36:04-root-INFO: grad norm: 217.583 212.072 48.661
2024-12-02-04:36:05-root-INFO: grad norm: 202.105 197.910 40.966
2024-12-02-04:36:06-root-INFO: grad norm: 186.712 181.866 42.261
2024-12-02-04:36:07-root-INFO: grad norm: 177.524 173.748 36.417
2024-12-02-04:36:08-root-INFO: Loss Change: 1357.192 -> 1316.507
2024-12-02-04:36:08-root-INFO: Regularization Change: 0.000 -> 0.237
2024-12-02-04:36:08-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-04:36:08-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:36:08-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-04:36:08-root-INFO: grad norm: 85.246 82.669 20.800
2024-12-02-04:36:09-root-INFO: Loss too large (1307.112->1379.937)! Learning rate decreased to 0.00677.
2024-12-02-04:36:09-root-INFO: Loss too large (1307.112->1351.196)! Learning rate decreased to 0.00542.
2024-12-02-04:36:10-root-INFO: Loss too large (1307.112->1331.272)! Learning rate decreased to 0.00433.
2024-12-02-04:36:10-root-INFO: Loss too large (1307.112->1318.136)! Learning rate decreased to 0.00347.
2024-12-02-04:36:10-root-INFO: Loss too large (1307.112->1309.976)! Learning rate decreased to 0.00277.
2024-12-02-04:36:11-root-INFO: grad norm: 156.145 152.858 31.868
2024-12-02-04:36:11-root-INFO: Loss too large (1305.276->1348.681)! Learning rate decreased to 0.00222.
2024-12-02-04:36:12-root-INFO: Loss too large (1305.276->1324.223)! Learning rate decreased to 0.00177.
2024-12-02-04:36:12-root-INFO: Loss too large (1305.276->1308.603)! Learning rate decreased to 0.00142.
2024-12-02-04:36:13-root-INFO: grad norm: 154.090 150.136 34.686
2024-12-02-04:36:14-root-INFO: grad norm: 151.496 148.252 31.183
2024-12-02-04:36:15-root-INFO: grad norm: 147.692 143.861 33.422
2024-12-02-04:36:16-root-INFO: Loss Change: 1307.112 -> 1290.972
2024-12-02-04:36:16-root-INFO: Regularization Change: 0.000 -> 0.209
2024-12-02-04:36:16-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-04:36:16-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:36:16-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-04:36:16-root-INFO: grad norm: 195.735 191.552 40.251
2024-12-02-04:36:17-root-INFO: Loss too large (1299.546->1554.689)! Learning rate decreased to 0.00704.
2024-12-02-04:36:17-root-INFO: Loss too large (1299.546->1527.544)! Learning rate decreased to 0.00563.
2024-12-02-04:36:17-root-INFO: Loss too large (1299.546->1492.435)! Learning rate decreased to 0.00450.
2024-12-02-04:36:18-root-INFO: Loss too large (1299.546->1449.991)! Learning rate decreased to 0.00360.
2024-12-02-04:36:18-root-INFO: Loss too large (1299.546->1403.500)! Learning rate decreased to 0.00288.
2024-12-02-04:36:18-root-INFO: Loss too large (1299.546->1359.230)! Learning rate decreased to 0.00231.
2024-12-02-04:36:19-root-INFO: Loss too large (1299.546->1323.723)! Learning rate decreased to 0.00184.
2024-12-02-04:36:19-root-INFO: Loss too large (1299.546->1299.984)! Learning rate decreased to 0.00148.
2024-12-02-04:36:20-root-INFO: grad norm: 166.774 162.510 37.472
2024-12-02-04:36:21-root-INFO: grad norm: 155.999 152.541 32.666
2024-12-02-04:36:22-root-INFO: grad norm: 143.192 139.441 32.557
2024-12-02-04:36:23-root-INFO: grad norm: 136.546 133.432 28.995
2024-12-02-04:36:24-root-INFO: Loss Change: 1299.546 -> 1269.944
2024-12-02-04:36:24-root-INFO: Regularization Change: 0.000 -> 0.193
2024-12-02-04:36:24-root-INFO: Undo step: 165
2024-12-02-04:36:24-root-INFO: Undo step: 166
2024-12-02-04:36:24-root-INFO: Undo step: 167
2024-12-02-04:36:24-root-INFO: Undo step: 168
2024-12-02-04:36:24-root-INFO: Undo step: 169
2024-12-02-04:36:24-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-04:36:24-root-INFO: grad norm: 656.194 641.104 139.919
2024-12-02-04:36:25-root-INFO: Loss too large (2330.871->2362.119)! Learning rate decreased to 0.00579.
2024-12-02-04:36:26-root-INFO: grad norm: 517.046 508.340 94.486
2024-12-02-04:36:26-root-INFO: Loss too large (2127.396->2259.211)! Learning rate decreased to 0.00463.
2024-12-02-04:36:27-root-INFO: grad norm: 485.730 474.441 104.111
2024-12-02-04:36:28-root-INFO: grad norm: 345.228 339.318 63.607
2024-12-02-04:36:29-root-INFO: grad norm: 254.733 250.276 47.441
2024-12-02-04:36:30-root-INFO: Loss Change: 2330.871 -> 1552.064
2024-12-02-04:36:30-root-INFO: Regularization Change: 0.000 -> 16.433
2024-12-02-04:36:30-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-04:36:30-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:36:30-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-04:36:30-root-INFO: grad norm: 204.098 198.302 48.295
2024-12-02-04:36:31-root-INFO: Loss too large (1534.177->1683.745)! Learning rate decreased to 0.00602.
2024-12-02-04:36:31-root-INFO: Loss too large (1534.177->1581.208)! Learning rate decreased to 0.00482.
2024-12-02-04:36:32-root-INFO: grad norm: 288.781 278.859 75.048
2024-12-02-04:36:32-root-INFO: Loss too large (1522.792->1572.843)! Learning rate decreased to 0.00385.
2024-12-02-04:36:33-root-INFO: grad norm: 278.340 268.695 72.635
2024-12-02-04:36:34-root-INFO: grad norm: 252.010 242.739 67.726
2024-12-02-04:36:35-root-INFO: grad norm: 281.956 270.972 77.929
2024-12-02-04:36:36-root-INFO: Loss too large (1425.508->1462.296)! Learning rate decreased to 0.00308.
2024-12-02-04:36:36-root-INFO: Loss Change: 1534.177 -> 1406.739
2024-12-02-04:36:36-root-INFO: Regularization Change: 0.000 -> 3.493
2024-12-02-04:36:36-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-04:36:36-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:36:37-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-04:36:37-root-INFO: grad norm: 272.987 266.858 57.522
2024-12-02-04:36:37-root-INFO: Loss too large (1419.559->1609.740)! Learning rate decreased to 0.00626.
2024-12-02-04:36:38-root-INFO: Loss too large (1419.559->1559.297)! Learning rate decreased to 0.00501.
2024-12-02-04:36:38-root-INFO: Loss too large (1419.559->1496.323)! Learning rate decreased to 0.00401.
2024-12-02-04:36:38-root-INFO: Loss too large (1419.559->1430.330)! Learning rate decreased to 0.00321.
2024-12-02-04:36:39-root-INFO: grad norm: 236.487 226.979 66.383
2024-12-02-04:36:40-root-INFO: grad norm: 225.225 219.570 50.155
2024-12-02-04:36:41-root-INFO: grad norm: 228.336 218.664 65.752
2024-12-02-04:36:42-root-INFO: Loss too large (1350.778->1355.468)! Learning rate decreased to 0.00256.
2024-12-02-04:36:43-root-INFO: grad norm: 174.216 169.541 40.090
2024-12-02-04:36:43-root-INFO: Loss Change: 1419.559 -> 1309.693
2024-12-02-04:36:43-root-INFO: Regularization Change: 0.000 -> 1.101
2024-12-02-04:36:43-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-04:36:43-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-04:36:44-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-04:36:44-root-INFO: grad norm: 115.138 109.224 36.426
2024-12-02-04:36:44-root-INFO: Loss too large (1297.455->1400.373)! Learning rate decreased to 0.00651.
2024-12-02-04:36:45-root-INFO: Loss too large (1297.455->1354.671)! Learning rate decreased to 0.00521.
2024-12-02-04:36:45-root-INFO: Loss too large (1297.455->1323.855)! Learning rate decreased to 0.00417.
2024-12-02-04:36:45-root-INFO: Loss too large (1297.455->1304.497)! Learning rate decreased to 0.00333.
2024-12-02-04:36:46-root-INFO: grad norm: 154.333 150.423 34.520
2024-12-02-04:36:47-root-INFO: Loss too large (1293.331->1308.145)! Learning rate decreased to 0.00267.
2024-12-02-04:36:48-root-INFO: grad norm: 184.658 176.927 52.871
2024-12-02-04:36:48-root-INFO: Loss too large (1288.957->1295.206)! Learning rate decreased to 0.00213.
2024-12-02-04:36:49-root-INFO: grad norm: 151.079 147.161 34.182
2024-12-02-04:36:50-root-INFO: grad norm: 124.676 118.899 37.512
2024-12-02-04:36:51-root-INFO: Loss Change: 1297.455 -> 1262.814
2024-12-02-04:36:51-root-INFO: Regularization Change: 0.000 -> 0.474
2024-12-02-04:36:51-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-04:36:51-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:36:51-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-04:36:51-root-INFO: grad norm: 161.298 156.944 37.223
2024-12-02-04:36:52-root-INFO: Loss too large (1265.567->1488.906)! Learning rate decreased to 0.00677.
2024-12-02-04:36:52-root-INFO: Loss too large (1265.567->1437.729)! Learning rate decreased to 0.00542.
2024-12-02-04:36:52-root-INFO: Loss too large (1265.567->1380.763)! Learning rate decreased to 0.00433.
2024-12-02-04:36:53-root-INFO: Loss too large (1265.567->1327.798)! Learning rate decreased to 0.00347.
2024-12-02-04:36:53-root-INFO: Loss too large (1265.567->1288.085)! Learning rate decreased to 0.00277.
2024-12-02-04:36:54-root-INFO: grad norm: 212.305 204.480 57.109
2024-12-02-04:36:54-root-INFO: Loss too large (1263.606->1281.095)! Learning rate decreased to 0.00222.
2024-12-02-04:36:55-root-INFO: grad norm: 179.108 174.824 38.937
2024-12-02-04:36:56-root-INFO: grad norm: 141.619 135.817 40.120
2024-12-02-04:36:57-root-INFO: grad norm: 131.036 127.707 29.347
2024-12-02-04:36:58-root-INFO: Loss Change: 1265.567 -> 1229.049
2024-12-02-04:36:58-root-INFO: Regularization Change: 0.000 -> 0.404
2024-12-02-04:36:58-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-04:36:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:36:58-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-04:36:59-root-INFO: grad norm: 83.034 79.401 24.296
2024-12-02-04:36:59-root-INFO: Loss too large (1221.162->1284.356)! Learning rate decreased to 0.00704.
2024-12-02-04:36:59-root-INFO: Loss too large (1221.162->1258.011)! Learning rate decreased to 0.00563.
2024-12-02-04:37:00-root-INFO: Loss too large (1221.162->1240.179)! Learning rate decreased to 0.00450.
2024-12-02-04:37:00-root-INFO: Loss too large (1221.162->1228.728)! Learning rate decreased to 0.00360.
2024-12-02-04:37:00-root-INFO: Loss too large (1221.162->1221.816)! Learning rate decreased to 0.00288.
2024-12-02-04:37:01-root-INFO: grad norm: 107.187 104.322 24.616
2024-12-02-04:37:02-root-INFO: Loss too large (1217.974->1219.076)! Learning rate decreased to 0.00231.
2024-12-02-04:37:03-root-INFO: grad norm: 114.004 109.566 31.499
2024-12-02-04:37:04-root-INFO: grad norm: 119.077 116.025 26.789
2024-12-02-04:37:05-root-INFO: grad norm: 128.037 123.258 34.657
2024-12-02-04:37:05-root-INFO: Loss Change: 1221.162 -> 1205.133
2024-12-02-04:37:05-root-INFO: Regularization Change: 0.000 -> 0.345
2024-12-02-04:37:05-root-INFO: Undo step: 165
2024-12-02-04:37:05-root-INFO: Undo step: 166
2024-12-02-04:37:05-root-INFO: Undo step: 167
2024-12-02-04:37:05-root-INFO: Undo step: 168
2024-12-02-04:37:05-root-INFO: Undo step: 169
2024-12-02-04:37:06-root-INFO: step: 170 lr_xt 0.00723499
2024-12-02-04:37:06-root-INFO: grad norm: 521.396 505.391 128.195
2024-12-02-04:37:06-root-INFO: Loss too large (2131.430->2226.841)! Learning rate decreased to 0.00579.
2024-12-02-04:37:07-root-INFO: grad norm: 514.899 506.810 90.908
2024-12-02-04:37:08-root-INFO: grad norm: 397.854 389.642 80.413
2024-12-02-04:37:09-root-INFO: grad norm: 452.848 446.102 77.874
2024-12-02-04:37:09-root-INFO: Loss too large (1742.712->1767.295)! Learning rate decreased to 0.00463.
2024-12-02-04:37:10-root-INFO: grad norm: 394.554 381.515 100.595
2024-12-02-04:37:11-root-INFO: Loss Change: 2131.430 -> 1554.284
2024-12-02-04:37:11-root-INFO: Regularization Change: 0.000 -> 14.478
2024-12-02-04:37:11-root-INFO: Learning rate of xt decay: 0.07698 -> 0.07790.
2024-12-02-04:37:11-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:37:11-root-INFO: step: 169 lr_xt 0.00752595
2024-12-02-04:37:12-root-INFO: grad norm: 396.710 388.214 81.662
2024-12-02-04:37:12-root-INFO: Loss too large (1578.349->1784.217)! Learning rate decreased to 0.00602.
2024-12-02-04:37:12-root-INFO: Loss too large (1578.349->1667.271)! Learning rate decreased to 0.00482.
2024-12-02-04:37:13-root-INFO: grad norm: 379.491 368.596 90.277
2024-12-02-04:37:14-root-INFO: grad norm: 318.166 310.234 70.598
2024-12-02-04:37:15-root-INFO: grad norm: 282.755 271.496 78.994
2024-12-02-04:37:16-root-INFO: Loss too large (1369.047->1421.449)! Learning rate decreased to 0.00385.
2024-12-02-04:37:17-root-INFO: grad norm: 232.635 226.449 53.294
2024-12-02-04:37:17-root-INFO: Loss Change: 1578.349 -> 1293.685
2024-12-02-04:37:17-root-INFO: Regularization Change: 0.000 -> 4.010
2024-12-02-04:37:17-root-INFO: Learning rate of xt decay: 0.07790 -> 0.07884.
2024-12-02-04:37:17-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-04:37:18-root-INFO: step: 168 lr_xt 0.00782735
2024-12-02-04:37:18-root-INFO: grad norm: 147.022 141.416 40.213
2024-12-02-04:37:18-root-INFO: Loss too large (1277.540->1372.675)! Learning rate decreased to 0.00626.
2024-12-02-04:37:19-root-INFO: Loss too large (1277.540->1323.351)! Learning rate decreased to 0.00501.
2024-12-02-04:37:19-root-INFO: Loss too large (1277.540->1291.941)! Learning rate decreased to 0.00401.
2024-12-02-04:37:20-root-INFO: grad norm: 148.140 143.173 38.037
2024-12-02-04:37:21-root-INFO: grad norm: 155.957 150.117 42.277
2024-12-02-04:37:21-root-INFO: Loss too large (1254.912->1257.056)! Learning rate decreased to 0.00321.
2024-12-02-04:37:22-root-INFO: grad norm: 121.088 117.149 30.635
2024-12-02-04:37:23-root-INFO: grad norm: 86.377 82.810 24.568
2024-12-02-04:37:24-root-INFO: Loss Change: 1277.540 -> 1217.069
2024-12-02-04:37:24-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-04:37:24-root-INFO: Learning rate of xt decay: 0.07884 -> 0.07979.
2024-12-02-04:37:24-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-04:37:24-root-INFO: step: 167 lr_xt 0.00813950
2024-12-02-04:37:25-root-INFO: grad norm: 112.384 108.358 29.813
2024-12-02-04:37:25-root-INFO: Loss too large (1215.507->1297.616)! Learning rate decreased to 0.00651.
2024-12-02-04:37:25-root-INFO: Loss too large (1215.507->1249.060)! Learning rate decreased to 0.00521.
2024-12-02-04:37:26-root-INFO: Loss too large (1215.507->1221.552)! Learning rate decreased to 0.00417.
2024-12-02-04:37:27-root-INFO: grad norm: 137.416 133.569 32.287
2024-12-02-04:37:27-root-INFO: Loss too large (1207.531->1211.406)! Learning rate decreased to 0.00333.
2024-12-02-04:37:28-root-INFO: grad norm: 115.874 112.173 29.050
2024-12-02-04:37:29-root-INFO: grad norm: 93.352 90.416 23.226
2024-12-02-04:37:30-root-INFO: grad norm: 84.420 81.568 21.759
2024-12-02-04:37:31-root-INFO: Loss Change: 1215.507 -> 1170.330
2024-12-02-04:37:31-root-INFO: Regularization Change: 0.000 -> 0.750
2024-12-02-04:37:31-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08074.
2024-12-02-04:37:31-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:37:31-root-INFO: step: 166 lr_xt 0.00846273
2024-12-02-04:37:31-root-INFO: grad norm: 56.058 53.997 15.060
2024-12-02-04:37:32-root-INFO: grad norm: 95.735 93.145 22.117
2024-12-02-04:37:33-root-INFO: Loss too large (1149.292->1221.676)! Learning rate decreased to 0.00677.
2024-12-02-04:37:33-root-INFO: Loss too large (1149.292->1179.349)! Learning rate decreased to 0.00542.
2024-12-02-04:37:33-root-INFO: Loss too large (1149.292->1155.731)! Learning rate decreased to 0.00433.
2024-12-02-04:37:34-root-INFO: grad norm: 124.216 121.012 28.030
2024-12-02-04:37:35-root-INFO: Loss too large (1143.883->1151.598)! Learning rate decreased to 0.00347.
2024-12-02-04:37:36-root-INFO: grad norm: 112.567 109.272 27.036
2024-12-02-04:37:37-root-INFO: grad norm: 99.292 96.541 23.209
2024-12-02-04:37:37-root-INFO: Loss Change: 1158.765 -> 1125.327
2024-12-02-04:37:37-root-INFO: Regularization Change: 0.000 -> 0.985
2024-12-02-04:37:37-root-INFO: Learning rate of xt decay: 0.08074 -> 0.08171.
2024-12-02-04:37:37-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:37:38-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-04:37:38-root-INFO: grad norm: 121.650 118.201 28.763
2024-12-02-04:37:38-root-INFO: Loss too large (1127.253->1285.980)! Learning rate decreased to 0.00704.
2024-12-02-04:37:39-root-INFO: Loss too large (1127.253->1207.465)! Learning rate decreased to 0.00563.
2024-12-02-04:37:39-root-INFO: Loss too large (1127.253->1156.137)! Learning rate decreased to 0.00450.
2024-12-02-04:37:39-root-INFO: Loss too large (1127.253->1128.532)! Learning rate decreased to 0.00360.
2024-12-02-04:37:40-root-INFO: grad norm: 112.854 110.338 23.696
2024-12-02-04:37:41-root-INFO: grad norm: 108.322 105.341 25.238
2024-12-02-04:37:42-root-INFO: grad norm: 103.542 101.086 22.421
2024-12-02-04:37:43-root-INFO: grad norm: 101.857 99.072 23.656
2024-12-02-04:37:44-root-INFO: Loss Change: 1127.253 -> 1093.041
2024-12-02-04:37:44-root-INFO: Regularization Change: 0.000 -> 0.623
2024-12-02-04:37:44-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-04:37:44-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:37:44-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-04:37:45-root-INFO: grad norm: 68.606 66.757 15.822
2024-12-02-04:37:45-root-INFO: Loss too large (1083.288->1100.936)! Learning rate decreased to 0.00732.
2024-12-02-04:37:45-root-INFO: Loss too large (1083.288->1089.596)! Learning rate decreased to 0.00585.
2024-12-02-04:37:46-root-INFO: grad norm: 106.679 103.779 24.706
2024-12-02-04:37:47-root-INFO: Loss too large (1082.828->1108.554)! Learning rate decreased to 0.00468.
2024-12-02-04:37:47-root-INFO: Loss too large (1082.828->1084.836)! Learning rate decreased to 0.00375.
2024-12-02-04:37:48-root-INFO: grad norm: 105.468 103.258 21.477
2024-12-02-04:37:49-root-INFO: grad norm: 105.835 103.012 24.285
2024-12-02-04:37:50-root-INFO: grad norm: 106.584 104.309 21.903
2024-12-02-04:37:51-root-INFO: Loss Change: 1083.288 -> 1064.127
2024-12-02-04:37:51-root-INFO: Regularization Change: 0.000 -> 0.682
2024-12-02-04:37:51-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-04:37:51-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-04:37:51-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-04:37:51-root-INFO: grad norm: 126.106 123.013 27.757
2024-12-02-04:37:52-root-INFO: Loss too large (1063.527->1262.290)! Learning rate decreased to 0.00760.
2024-12-02-04:37:52-root-INFO: Loss too large (1063.527->1169.150)! Learning rate decreased to 0.00608.
2024-12-02-04:37:52-root-INFO: Loss too large (1063.527->1103.261)! Learning rate decreased to 0.00487.
2024-12-02-04:37:53-root-INFO: Loss too large (1063.527->1067.617)! Learning rate decreased to 0.00389.
2024-12-02-04:37:54-root-INFO: grad norm: 118.823 116.637 22.685
2024-12-02-04:37:55-root-INFO: grad norm: 115.661 112.918 25.037
2024-12-02-04:37:56-root-INFO: grad norm: 112.335 110.147 22.064
2024-12-02-04:37:56-root-INFO: Loss too large (1040.251->1040.324)! Learning rate decreased to 0.00311.
2024-12-02-04:37:57-root-INFO: grad norm: 81.232 79.061 18.652
2024-12-02-04:37:58-root-INFO: Loss Change: 1063.527 -> 1025.536
2024-12-02-04:37:58-root-INFO: Regularization Change: 0.000 -> 0.506
2024-12-02-04:37:58-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-04:37:58-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:37:58-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-04:37:58-root-INFO: grad norm: 54.339 52.195 15.112
2024-12-02-04:37:59-root-INFO: grad norm: 87.757 86.809 12.860
2024-12-02-04:38:00-root-INFO: Loss too large (1018.199->1075.350)! Learning rate decreased to 0.00790.
2024-12-02-04:38:00-root-INFO: Loss too large (1018.199->1040.283)! Learning rate decreased to 0.00632.
2024-12-02-04:38:00-root-INFO: Loss too large (1018.199->1021.090)! Learning rate decreased to 0.00506.
2024-12-02-04:38:01-root-INFO: grad norm: 95.288 93.837 16.565
2024-12-02-04:38:02-root-INFO: Loss too large (1011.334->1013.096)! Learning rate decreased to 0.00404.
2024-12-02-04:38:03-root-INFO: grad norm: 88.296 87.015 14.983
2024-12-02-04:38:04-root-INFO: grad norm: 88.326 86.326 18.690
2024-12-02-04:38:04-root-INFO: Loss Change: 1025.259 -> 997.956
2024-12-02-04:38:04-root-INFO: Regularization Change: 0.000 -> 0.944
2024-12-02-04:38:04-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-04:38:04-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:38:05-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-04:38:05-root-INFO: grad norm: 72.775 71.320 14.481
2024-12-02-04:38:05-root-INFO: Loss too large (988.266->1029.884)! Learning rate decreased to 0.00821.
2024-12-02-04:38:06-root-INFO: Loss too large (988.266->1010.251)! Learning rate decreased to 0.00656.
2024-12-02-04:38:06-root-INFO: Loss too large (988.266->997.651)! Learning rate decreased to 0.00525.
2024-12-02-04:38:06-root-INFO: Loss too large (988.266->989.950)! Learning rate decreased to 0.00420.
2024-12-02-04:38:07-root-INFO: grad norm: 77.402 75.529 16.924
2024-12-02-04:38:08-root-INFO: grad norm: 88.114 86.455 17.018
2024-12-02-04:38:09-root-INFO: Loss too large (980.884->980.902)! Learning rate decreased to 0.00336.
2024-12-02-04:38:10-root-INFO: grad norm: 70.416 68.666 15.602
2024-12-02-04:38:11-root-INFO: grad norm: 52.442 51.222 11.245
2024-12-02-04:38:11-root-INFO: Loss Change: 988.266 -> 966.395
2024-12-02-04:38:11-root-INFO: Regularization Change: 0.000 -> 0.389
2024-12-02-04:38:11-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-04:38:11-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:38:12-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-04:38:12-root-INFO: grad norm: 80.664 78.732 17.546
2024-12-02-04:38:12-root-INFO: Loss too large (969.227->1061.574)! Learning rate decreased to 0.00852.
2024-12-02-04:38:13-root-INFO: Loss too large (969.227->1012.703)! Learning rate decreased to 0.00682.
2024-12-02-04:38:13-root-INFO: Loss too large (969.227->985.556)! Learning rate decreased to 0.00545.
2024-12-02-04:38:13-root-INFO: Loss too large (969.227->971.247)! Learning rate decreased to 0.00436.
2024-12-02-04:38:14-root-INFO: grad norm: 91.356 89.931 16.071
2024-12-02-04:38:15-root-INFO: grad norm: 97.512 95.431 20.037
2024-12-02-04:38:16-root-INFO: grad norm: 110.597 108.839 19.637
2024-12-02-04:38:17-root-INFO: Loss too large (958.145->961.603)! Learning rate decreased to 0.00349.
2024-12-02-04:38:18-root-INFO: grad norm: 85.457 83.505 18.163
2024-12-02-04:38:18-root-INFO: Loss Change: 969.227 -> 944.423
2024-12-02-04:38:18-root-INFO: Regularization Change: 0.000 -> 0.483
2024-12-02-04:38:18-root-INFO: Undo step: 160
2024-12-02-04:38:18-root-INFO: Undo step: 161
2024-12-02-04:38:18-root-INFO: Undo step: 162
2024-12-02-04:38:18-root-INFO: Undo step: 163
2024-12-02-04:38:18-root-INFO: Undo step: 164
2024-12-02-04:38:19-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-04:38:19-root-INFO: grad norm: 285.781 267.627 100.233
2024-12-02-04:38:20-root-INFO: grad norm: 178.687 173.814 41.447
2024-12-02-04:38:21-root-INFO: grad norm: 150.458 146.699 33.421
2024-12-02-04:38:22-root-INFO: grad norm: 227.220 223.232 42.388
2024-12-02-04:38:22-root-INFO: Loss too large (1119.109->1414.248)! Learning rate decreased to 0.00704.
2024-12-02-04:38:23-root-INFO: Loss too large (1119.109->1290.386)! Learning rate decreased to 0.00563.
2024-12-02-04:38:23-root-INFO: Loss too large (1119.109->1179.915)! Learning rate decreased to 0.00450.
2024-12-02-04:38:24-root-INFO: grad norm: 339.130 330.653 75.350
2024-12-02-04:38:24-root-INFO: Loss too large (1103.242->1220.950)! Learning rate decreased to 0.00360.
2024-12-02-04:38:25-root-INFO: Loss too large (1103.242->1147.031)! Learning rate decreased to 0.00288.
2024-12-02-04:38:25-root-INFO: Loss Change: 1638.977 -> 1096.220
2024-12-02-04:38:25-root-INFO: Regularization Change: 0.000 -> 15.129
2024-12-02-04:38:25-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-04:38:25-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:38:26-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-04:38:26-root-INFO: grad norm: 225.609 220.778 46.440
2024-12-02-04:38:26-root-INFO: Loss too large (1104.474->1347.834)! Learning rate decreased to 0.00732.
2024-12-02-04:38:27-root-INFO: Loss too large (1104.474->1260.836)! Learning rate decreased to 0.00585.
2024-12-02-04:38:27-root-INFO: Loss too large (1104.474->1139.898)! Learning rate decreased to 0.00468.
2024-12-02-04:38:28-root-INFO: grad norm: 260.717 254.522 56.497
2024-12-02-04:38:28-root-INFO: Loss too large (1054.324->1121.064)! Learning rate decreased to 0.00375.
2024-12-02-04:38:29-root-INFO: Loss too large (1054.324->1071.323)! Learning rate decreased to 0.00300.
2024-12-02-04:38:30-root-INFO: grad norm: 158.918 154.724 36.268
2024-12-02-04:38:31-root-INFO: grad norm: 55.456 53.687 13.893
2024-12-02-04:38:32-root-INFO: grad norm: 47.075 45.152 13.316
2024-12-02-04:38:32-root-INFO: Loss Change: 1104.474 -> 985.799
2024-12-02-04:38:32-root-INFO: Regularization Change: 0.000 -> 1.018
2024-12-02-04:38:32-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-04:38:32-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-04:38:33-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-04:38:33-root-INFO: grad norm: 51.438 49.558 13.778
2024-12-02-04:38:34-root-INFO: grad norm: 98.030 96.141 19.153
2024-12-02-04:38:34-root-INFO: Loss too large (972.071->1050.066)! Learning rate decreased to 0.00760.
2024-12-02-04:38:35-root-INFO: Loss too large (972.071->1010.553)! Learning rate decreased to 0.00608.
2024-12-02-04:38:35-root-INFO: Loss too large (972.071->986.479)! Learning rate decreased to 0.00487.
2024-12-02-04:38:35-root-INFO: Loss too large (972.071->972.581)! Learning rate decreased to 0.00389.
2024-12-02-04:38:36-root-INFO: grad norm: 92.825 90.699 19.755
2024-12-02-04:38:37-root-INFO: grad norm: 118.383 115.675 25.177
2024-12-02-04:38:38-root-INFO: Loss too large (962.120->967.754)! Learning rate decreased to 0.00311.
2024-12-02-04:38:39-root-INFO: grad norm: 100.396 97.825 22.574
2024-12-02-04:38:39-root-INFO: Loss Change: 979.212 -> 949.571
2024-12-02-04:38:39-root-INFO: Regularization Change: 0.000 -> 0.782
2024-12-02-04:38:39-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-04:38:39-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:38:40-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-04:38:40-root-INFO: grad norm: 66.082 63.790 17.256
2024-12-02-04:38:40-root-INFO: Loss too large (947.847->954.294)! Learning rate decreased to 0.00790.
2024-12-02-04:38:41-root-INFO: grad norm: 104.320 102.986 16.626
2024-12-02-04:38:42-root-INFO: Loss too large (946.015->988.114)! Learning rate decreased to 0.00632.
2024-12-02-04:38:42-root-INFO: Loss too large (946.015->957.557)! Learning rate decreased to 0.00506.
2024-12-02-04:38:43-root-INFO: grad norm: 124.547 123.127 18.754
2024-12-02-04:38:43-root-INFO: Loss too large (941.268->952.309)! Learning rate decreased to 0.00404.
2024-12-02-04:38:44-root-INFO: grad norm: 114.406 112.167 22.525
2024-12-02-04:38:45-root-INFO: grad norm: 130.026 127.526 25.375
2024-12-02-04:38:46-root-INFO: Loss too large (930.879->939.629)! Learning rate decreased to 0.00324.
2024-12-02-04:38:46-root-INFO: Loss Change: 947.847 -> 928.565
2024-12-02-04:38:46-root-INFO: Regularization Change: 0.000 -> 0.764
2024-12-02-04:38:46-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-04:38:46-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:38:47-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-04:38:47-root-INFO: grad norm: 120.660 117.886 25.724
2024-12-02-04:38:47-root-INFO: Loss too large (929.483->1150.813)! Learning rate decreased to 0.00821.
2024-12-02-04:38:48-root-INFO: Loss too large (929.483->1058.010)! Learning rate decreased to 0.00656.
2024-12-02-04:38:48-root-INFO: Loss too large (929.483->982.461)! Learning rate decreased to 0.00525.
2024-12-02-04:38:48-root-INFO: Loss too large (929.483->940.149)! Learning rate decreased to 0.00420.
2024-12-02-04:38:49-root-INFO: grad norm: 131.320 128.639 26.398
2024-12-02-04:38:50-root-INFO: Loss too large (919.973->927.500)! Learning rate decreased to 0.00336.
2024-12-02-04:38:51-root-INFO: grad norm: 100.071 97.666 21.807
2024-12-02-04:38:52-root-INFO: grad norm: 63.229 61.799 13.373
2024-12-02-04:38:52-root-INFO: grad norm: 53.352 51.697 13.185
2024-12-02-04:38:53-root-INFO: Loss Change: 929.483 -> 896.905
2024-12-02-04:38:53-root-INFO: Regularization Change: 0.000 -> 0.393
2024-12-02-04:38:53-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-04:38:53-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:38:54-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-04:38:54-root-INFO: grad norm: 48.766 47.111 12.599
2024-12-02-04:38:54-root-INFO: Loss too large (895.816->897.599)! Learning rate decreased to 0.00852.
2024-12-02-04:38:55-root-INFO: grad norm: 80.940 79.989 12.372
2024-12-02-04:38:55-root-INFO: Loss too large (893.242->920.837)! Learning rate decreased to 0.00682.
2024-12-02-04:38:56-root-INFO: Loss too large (893.242->902.390)! Learning rate decreased to 0.00545.
2024-12-02-04:38:57-root-INFO: grad norm: 98.429 97.028 16.552
2024-12-02-04:38:57-root-INFO: Loss too large (891.967->897.910)! Learning rate decreased to 0.00436.
2024-12-02-04:38:58-root-INFO: grad norm: 99.263 97.629 17.938
2024-12-02-04:38:59-root-INFO: Loss too large (887.180->887.712)! Learning rate decreased to 0.00349.
2024-12-02-04:39:00-root-INFO: grad norm: 75.151 73.311 16.525
2024-12-02-04:39:00-root-INFO: Loss Change: 895.816 -> 875.806
2024-12-02-04:39:00-root-INFO: Regularization Change: 0.000 -> 0.550
2024-12-02-04:39:00-root-INFO: Undo step: 160
2024-12-02-04:39:00-root-INFO: Undo step: 161
2024-12-02-04:39:00-root-INFO: Undo step: 162
2024-12-02-04:39:00-root-INFO: Undo step: 163
2024-12-02-04:39:00-root-INFO: Undo step: 164
2024-12-02-04:39:00-root-INFO: step: 165 lr_xt 0.00879737
2024-12-02-04:39:01-root-INFO: grad norm: 443.855 429.469 112.088
2024-12-02-04:39:02-root-INFO: grad norm: 548.194 536.206 114.019
2024-12-02-04:39:02-root-INFO: Loss too large (1618.318->1961.353)! Learning rate decreased to 0.00704.
2024-12-02-04:39:02-root-INFO: Loss too large (1618.318->1758.912)! Learning rate decreased to 0.00563.
2024-12-02-04:39:03-root-INFO: grad norm: 355.567 350.266 61.168
2024-12-02-04:39:04-root-INFO: grad norm: 209.403 200.388 60.781
2024-12-02-04:39:05-root-INFO: grad norm: 263.029 253.936 68.560
2024-12-02-04:39:06-root-INFO: Loss too large (1138.449->1278.969)! Learning rate decreased to 0.00450.
2024-12-02-04:39:06-root-INFO: Loss too large (1138.449->1182.565)! Learning rate decreased to 0.00360.
2024-12-02-04:39:07-root-INFO: Loss Change: 1731.940 -> 1101.272
2024-12-02-04:39:07-root-INFO: Regularization Change: 0.000 -> 14.025
2024-12-02-04:39:07-root-INFO: Learning rate of xt decay: 0.08171 -> 0.08269.
2024-12-02-04:39:07-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-04:39:07-root-INFO: step: 164 lr_xt 0.00914377
2024-12-02-04:39:07-root-INFO: grad norm: 283.578 273.646 74.394
2024-12-02-04:39:08-root-INFO: Loss too large (1073.510->1585.494)! Learning rate decreased to 0.00732.
2024-12-02-04:39:08-root-INFO: Loss too large (1073.510->1413.485)! Learning rate decreased to 0.00585.
2024-12-02-04:39:08-root-INFO: Loss too large (1073.510->1289.322)! Learning rate decreased to 0.00468.
2024-12-02-04:39:09-root-INFO: Loss too large (1073.510->1199.924)! Learning rate decreased to 0.00375.
2024-12-02-04:39:09-root-INFO: Loss too large (1073.510->1136.054)! Learning rate decreased to 0.00300.
2024-12-02-04:39:09-root-INFO: Loss too large (1073.510->1091.471)! Learning rate decreased to 0.00240.
2024-12-02-04:39:10-root-INFO: grad norm: 185.625 179.111 48.744
2024-12-02-04:39:11-root-INFO: grad norm: 71.419 68.868 18.919
2024-12-02-04:39:12-root-INFO: grad norm: 62.585 60.242 16.965
2024-12-02-04:39:13-root-INFO: grad norm: 56.396 54.199 15.590
2024-12-02-04:39:14-root-INFO: Loss Change: 1073.510 -> 995.654
2024-12-02-04:39:14-root-INFO: Regularization Change: 0.000 -> 0.579
2024-12-02-04:39:14-root-INFO: Learning rate of xt decay: 0.08269 -> 0.08368.
2024-12-02-04:39:14-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-04:39:14-root-INFO: step: 163 lr_xt 0.00950228
2024-12-02-04:39:15-root-INFO: grad norm: 75.904 72.904 21.131
2024-12-02-04:39:15-root-INFO: Loss too large (990.877->1041.077)! Learning rate decreased to 0.00760.
2024-12-02-04:39:15-root-INFO: Loss too large (990.877->1011.319)! Learning rate decreased to 0.00608.
2024-12-02-04:39:16-root-INFO: Loss too large (990.877->995.080)! Learning rate decreased to 0.00487.
2024-12-02-04:39:17-root-INFO: grad norm: 147.210 142.265 37.836
2024-12-02-04:39:17-root-INFO: Loss too large (986.945->1029.836)! Learning rate decreased to 0.00389.
2024-12-02-04:39:17-root-INFO: Loss too large (986.945->1005.909)! Learning rate decreased to 0.00311.
2024-12-02-04:39:18-root-INFO: Loss too large (986.945->990.410)! Learning rate decreased to 0.00249.
2024-12-02-04:39:19-root-INFO: grad norm: 113.834 109.957 29.459
2024-12-02-04:39:20-root-INFO: grad norm: 74.498 71.908 19.474
2024-12-02-04:39:21-root-INFO: grad norm: 63.854 61.444 17.376
2024-12-02-04:39:21-root-INFO: Loss Change: 990.877 -> 957.745
2024-12-02-04:39:21-root-INFO: Regularization Change: 0.000 -> 0.484
2024-12-02-04:39:21-root-INFO: Learning rate of xt decay: 0.08368 -> 0.08469.
2024-12-02-04:39:21-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:39:22-root-INFO: step: 162 lr_xt 0.00987325
2024-12-02-04:39:22-root-INFO: grad norm: 75.829 72.232 23.077
2024-12-02-04:39:22-root-INFO: Loss too large (961.138->977.068)! Learning rate decreased to 0.00790.
2024-12-02-04:39:23-root-INFO: Loss too large (961.138->962.628)! Learning rate decreased to 0.00632.
2024-12-02-04:39:24-root-INFO: grad norm: 124.887 121.264 29.863
2024-12-02-04:39:24-root-INFO: Loss too large (955.003->1005.762)! Learning rate decreased to 0.00506.
2024-12-02-04:39:24-root-INFO: Loss too large (955.003->978.335)! Learning rate decreased to 0.00404.
2024-12-02-04:39:25-root-INFO: Loss too large (955.003->961.173)! Learning rate decreased to 0.00324.
2024-12-02-04:39:26-root-INFO: grad norm: 118.244 114.020 31.322
2024-12-02-04:39:27-root-INFO: grad norm: 130.562 126.765 31.257
2024-12-02-04:39:27-root-INFO: Loss too large (942.839->947.004)! Learning rate decreased to 0.00259.
2024-12-02-04:39:28-root-INFO: grad norm: 101.138 97.481 26.951
2024-12-02-04:39:29-root-INFO: Loss Change: 961.138 -> 928.881
2024-12-02-04:39:29-root-INFO: Regularization Change: 0.000 -> 0.582
2024-12-02-04:39:29-root-INFO: Learning rate of xt decay: 0.08469 -> 0.08571.
2024-12-02-04:39:29-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:39:29-root-INFO: step: 161 lr_xt 0.01025704
2024-12-02-04:39:29-root-INFO: grad norm: 49.942 48.301 12.696
2024-12-02-04:39:30-root-INFO: grad norm: 98.286 96.380 19.262
2024-12-02-04:39:31-root-INFO: Loss too large (919.434->1055.756)! Learning rate decreased to 0.00821.
2024-12-02-04:39:31-root-INFO: Loss too large (919.434->986.386)! Learning rate decreased to 0.00656.
2024-12-02-04:39:31-root-INFO: Loss too large (919.434->945.613)! Learning rate decreased to 0.00525.
2024-12-02-04:39:32-root-INFO: Loss too large (919.434->923.666)! Learning rate decreased to 0.00420.
2024-12-02-04:39:33-root-INFO: grad norm: 120.582 117.777 25.859
2024-12-02-04:39:33-root-INFO: Loss too large (912.750->926.517)! Learning rate decreased to 0.00336.
2024-12-02-04:39:33-root-INFO: Loss too large (912.750->915.161)! Learning rate decreased to 0.00269.
2024-12-02-04:39:34-root-INFO: grad norm: 90.608 87.627 23.051
2024-12-02-04:39:35-root-INFO: grad norm: 58.664 56.784 14.730
2024-12-02-04:39:36-root-INFO: Loss Change: 923.382 -> 897.197
2024-12-02-04:39:36-root-INFO: Regularization Change: 0.000 -> 0.610
2024-12-02-04:39:36-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08673.
2024-12-02-04:39:36-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:39:36-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-04:39:37-root-INFO: grad norm: 92.455 89.068 24.798
2024-12-02-04:39:37-root-INFO: Loss too large (902.222->1077.022)! Learning rate decreased to 0.00852.
2024-12-02-04:39:38-root-INFO: Loss too large (902.222->999.809)! Learning rate decreased to 0.00682.
2024-12-02-04:39:38-root-INFO: Loss too large (902.222->947.238)! Learning rate decreased to 0.00545.
2024-12-02-04:39:38-root-INFO: Loss too large (902.222->917.663)! Learning rate decreased to 0.00436.
2024-12-02-04:39:39-root-INFO: Loss too large (902.222->902.466)! Learning rate decreased to 0.00349.
2024-12-02-04:39:40-root-INFO: grad norm: 93.268 90.955 20.641
2024-12-02-04:39:40-root-INFO: Loss too large (895.315->895.390)! Learning rate decreased to 0.00279.
2024-12-02-04:39:41-root-INFO: grad norm: 71.111 68.446 19.284
2024-12-02-04:39:42-root-INFO: grad norm: 48.451 46.973 11.876
2024-12-02-04:39:43-root-INFO: grad norm: 40.901 38.967 12.428
2024-12-02-04:39:43-root-INFO: Loss Change: 902.222 -> 880.002
2024-12-02-04:39:43-root-INFO: Regularization Change: 0.000 -> 0.258
2024-12-02-04:39:43-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-04:39:43-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:39:44-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-04:39:44-root-INFO: grad norm: 43.882 42.209 12.000
2024-12-02-04:39:44-root-INFO: Loss too large (877.924->878.670)! Learning rate decreased to 0.00885.
2024-12-02-04:39:45-root-INFO: grad norm: 79.712 78.143 15.738
2024-12-02-04:39:46-root-INFO: Loss too large (875.056->909.437)! Learning rate decreased to 0.00708.
2024-12-02-04:39:46-root-INFO: Loss too large (875.056->889.948)! Learning rate decreased to 0.00567.
2024-12-02-04:39:46-root-INFO: Loss too large (875.056->878.354)! Learning rate decreased to 0.00453.
2024-12-02-04:39:47-root-INFO: grad norm: 83.890 81.458 20.052
2024-12-02-04:39:48-root-INFO: Loss too large (871.897->873.872)! Learning rate decreased to 0.00363.
2024-12-02-04:39:49-root-INFO: grad norm: 87.200 85.089 19.073
2024-12-02-04:39:49-root-INFO: Loss too large (866.946->867.868)! Learning rate decreased to 0.00290.
2024-12-02-04:39:50-root-INFO: grad norm: 66.677 64.468 17.022
2024-12-02-04:39:51-root-INFO: Loss Change: 877.924 -> 859.041
2024-12-02-04:39:51-root-INFO: Regularization Change: 0.000 -> 0.440
2024-12-02-04:39:51-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-04:39:51-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-04:39:51-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-04:39:51-root-INFO: grad norm: 54.446 52.488 14.470
2024-12-02-04:39:52-root-INFO: Loss too large (859.248->866.923)! Learning rate decreased to 0.00919.
2024-12-02-04:39:53-root-INFO: grad norm: 89.207 88.069 14.203
2024-12-02-04:39:53-root-INFO: Loss too large (859.234->887.758)! Learning rate decreased to 0.00735.
2024-12-02-04:39:53-root-INFO: Loss too large (859.234->866.605)! Learning rate decreased to 0.00588.
2024-12-02-04:39:54-root-INFO: grad norm: 93.442 91.677 18.079
2024-12-02-04:39:55-root-INFO: Loss too large (854.782->861.595)! Learning rate decreased to 0.00471.
2024-12-02-04:39:56-root-INFO: grad norm: 111.249 108.933 22.579
2024-12-02-04:39:56-root-INFO: Loss too large (850.084->861.672)! Learning rate decreased to 0.00376.
2024-12-02-04:39:56-root-INFO: Loss too large (850.084->851.148)! Learning rate decreased to 0.00301.
2024-12-02-04:39:57-root-INFO: grad norm: 76.254 73.922 18.711
2024-12-02-04:39:58-root-INFO: Loss Change: 859.248 -> 838.025
2024-12-02-04:39:58-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-02-04:39:58-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-04:39:58-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:39:58-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-04:39:59-root-INFO: grad norm: 44.616 42.446 13.744
2024-12-02-04:39:59-root-INFO: Loss too large (838.110->839.916)! Learning rate decreased to 0.00954.
2024-12-02-04:40:00-root-INFO: grad norm: 78.695 77.358 14.448
2024-12-02-04:40:00-root-INFO: Loss too large (835.594->873.571)! Learning rate decreased to 0.00763.
2024-12-02-04:40:01-root-INFO: Loss too large (835.594->852.854)! Learning rate decreased to 0.00611.
2024-12-02-04:40:01-root-INFO: Loss too large (835.594->840.389)! Learning rate decreased to 0.00489.
2024-12-02-04:40:02-root-INFO: grad norm: 82.889 80.556 19.526
2024-12-02-04:40:02-root-INFO: Loss too large (833.316->834.899)! Learning rate decreased to 0.00391.
2024-12-02-04:40:03-root-INFO: grad norm: 80.553 78.940 16.043
2024-12-02-04:40:04-root-INFO: Loss too large (827.659->828.234)! Learning rate decreased to 0.00313.
2024-12-02-04:40:05-root-INFO: grad norm: 59.232 57.235 15.253
2024-12-02-04:40:05-root-INFO: Loss Change: 838.110 -> 820.363
2024-12-02-04:40:05-root-INFO: Regularization Change: 0.000 -> 0.431
2024-12-02-04:40:05-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-04:40:05-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:40:06-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-04:40:06-root-INFO: grad norm: 36.226 34.956 9.508
2024-12-02-04:40:07-root-INFO: grad norm: 73.866 73.003 11.260
2024-12-02-04:40:07-root-INFO: Loss too large (818.704->863.610)! Learning rate decreased to 0.00991.
2024-12-02-04:40:08-root-INFO: Loss too large (818.704->838.119)! Learning rate decreased to 0.00792.
2024-12-02-04:40:08-root-INFO: Loss too large (818.704->823.005)! Learning rate decreased to 0.00634.
2024-12-02-04:40:09-root-INFO: grad norm: 74.147 73.465 10.033
2024-12-02-04:40:10-root-INFO: grad norm: 85.812 84.317 15.950
2024-12-02-04:40:10-root-INFO: Loss too large (814.252->823.085)! Learning rate decreased to 0.00507.
2024-12-02-04:40:11-root-INFO: grad norm: 109.142 107.305 19.941
2024-12-02-04:40:12-root-INFO: Loss too large (810.321->823.090)! Learning rate decreased to 0.00406.
2024-12-02-04:40:12-root-INFO: Loss too large (810.321->812.685)! Learning rate decreased to 0.00325.
2024-12-02-04:40:13-root-INFO: Loss Change: 819.206 -> 806.339
2024-12-02-04:40:13-root-INFO: Regularization Change: 0.000 -> 0.658
2024-12-02-04:40:13-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-04:40:13-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:40:13-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-04:40:13-root-INFO: grad norm: 97.422 94.758 22.627
2024-12-02-04:40:14-root-INFO: Loss too large (812.366->1054.883)! Learning rate decreased to 0.01028.
2024-12-02-04:40:14-root-INFO: Loss too large (812.366->955.033)! Learning rate decreased to 0.00822.
2024-12-02-04:40:14-root-INFO: Loss too large (812.366->873.636)! Learning rate decreased to 0.00658.
2024-12-02-04:40:15-root-INFO: Loss too large (812.366->829.679)! Learning rate decreased to 0.00526.
2024-12-02-04:40:16-root-INFO: grad norm: 136.365 134.108 24.708
2024-12-02-04:40:16-root-INFO: Loss too large (808.862->829.588)! Learning rate decreased to 0.00421.
2024-12-02-04:40:16-root-INFO: Loss too large (808.862->813.746)! Learning rate decreased to 0.00337.
2024-12-02-04:40:17-root-INFO: grad norm: 83.187 80.866 19.512
2024-12-02-04:40:18-root-INFO: grad norm: 29.264 28.211 7.782
2024-12-02-04:40:19-root-INFO: grad norm: 24.625 23.154 8.383
2024-12-02-04:40:20-root-INFO: Loss Change: 812.366 -> 788.253
2024-12-02-04:40:20-root-INFO: Regularization Change: 0.000 -> 0.278
2024-12-02-04:40:20-root-INFO: Undo step: 155
2024-12-02-04:40:20-root-INFO: Undo step: 156
2024-12-02-04:40:20-root-INFO: Undo step: 157
2024-12-02-04:40:20-root-INFO: Undo step: 158
2024-12-02-04:40:20-root-INFO: Undo step: 159
2024-12-02-04:40:20-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-04:40:21-root-INFO: grad norm: 351.657 340.803 86.692
2024-12-02-04:40:21-root-INFO: Loss too large (1526.057->1704.036)! Learning rate decreased to 0.00852.
2024-12-02-04:40:22-root-INFO: grad norm: 368.894 364.028 59.715
2024-12-02-04:40:23-root-INFO: grad norm: 384.882 377.285 76.095
2024-12-02-04:40:23-root-INFO: Loss too large (1272.782->1492.210)! Learning rate decreased to 0.00682.
2024-12-02-04:40:24-root-INFO: Loss too large (1272.782->1325.302)! Learning rate decreased to 0.00545.
2024-12-02-04:40:25-root-INFO: grad norm: 229.919 225.898 42.810
2024-12-02-04:40:26-root-INFO: grad norm: 95.675 92.755 23.457
2024-12-02-04:40:26-root-INFO: Loss Change: 1526.057 -> 988.445
2024-12-02-04:40:26-root-INFO: Regularization Change: 0.000 -> 14.114
2024-12-02-04:40:26-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-04:40:26-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:40:27-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-04:40:27-root-INFO: grad norm: 76.336 74.235 17.784
2024-12-02-04:40:28-root-INFO: grad norm: 89.089 87.133 18.564
2024-12-02-04:40:28-root-INFO: Loss too large (942.573->950.412)! Learning rate decreased to 0.00885.
2024-12-02-04:40:30-root-INFO: grad norm: 126.328 124.877 19.090
2024-12-02-04:40:30-root-INFO: Loss too large (933.567->950.729)! Learning rate decreased to 0.00708.
2024-12-02-04:40:31-root-INFO: grad norm: 128.122 126.408 20.882
2024-12-02-04:40:32-root-INFO: grad norm: 159.744 157.339 27.616
2024-12-02-04:40:32-root-INFO: Loss too large (913.299->940.389)! Learning rate decreased to 0.00567.
2024-12-02-04:40:33-root-INFO: Loss Change: 979.968 -> 911.721
2024-12-02-04:40:33-root-INFO: Regularization Change: 0.000 -> 4.270
2024-12-02-04:40:33-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-04:40:33-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-04:40:33-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-04:40:34-root-INFO: grad norm: 129.964 127.061 27.315
2024-12-02-04:40:34-root-INFO: Loss too large (909.530->1041.039)! Learning rate decreased to 0.00919.
2024-12-02-04:40:34-root-INFO: Loss too large (909.530->940.706)! Learning rate decreased to 0.00735.
2024-12-02-04:40:35-root-INFO: grad norm: 178.093 174.420 35.979
2024-12-02-04:40:36-root-INFO: Loss too large (886.792->944.320)! Learning rate decreased to 0.00588.
2024-12-02-04:40:36-root-INFO: Loss too large (886.792->907.495)! Learning rate decreased to 0.00471.
2024-12-02-04:40:37-root-INFO: grad norm: 113.790 111.198 24.148
2024-12-02-04:40:38-root-INFO: grad norm: 37.408 35.895 10.532
2024-12-02-04:40:39-root-INFO: grad norm: 33.509 31.969 10.043
2024-12-02-04:40:40-root-INFO: Loss Change: 909.530 -> 839.531
2024-12-02-04:40:40-root-INFO: Regularization Change: 0.000 -> 1.131
2024-12-02-04:40:40-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-04:40:40-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:40:40-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-04:40:40-root-INFO: grad norm: 48.364 45.866 15.345
2024-12-02-04:40:41-root-INFO: grad norm: 95.002 92.853 20.093
2024-12-02-04:40:42-root-INFO: Loss too large (835.159->903.900)! Learning rate decreased to 0.00954.
2024-12-02-04:40:42-root-INFO: Loss too large (835.159->867.147)! Learning rate decreased to 0.00763.
2024-12-02-04:40:42-root-INFO: Loss too large (835.159->844.793)! Learning rate decreased to 0.00611.
2024-12-02-04:40:43-root-INFO: grad norm: 91.997 90.178 18.203
2024-12-02-04:40:44-root-INFO: grad norm: 97.534 95.693 18.862
2024-12-02-04:40:45-root-INFO: Loss too large (824.344->826.367)! Learning rate decreased to 0.00489.
2024-12-02-04:40:46-root-INFO: grad norm: 72.021 70.360 15.379
2024-12-02-04:40:46-root-INFO: Loss Change: 837.631 -> 808.697
2024-12-02-04:40:46-root-INFO: Regularization Change: 0.000 -> 1.022
2024-12-02-04:40:46-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-04:40:46-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:40:47-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-04:40:47-root-INFO: grad norm: 33.333 31.952 9.495
2024-12-02-04:40:48-root-INFO: grad norm: 52.039 50.584 12.216
2024-12-02-04:40:48-root-INFO: Loss too large (799.038->821.385)! Learning rate decreased to 0.00991.
2024-12-02-04:40:49-root-INFO: Loss too large (799.038->805.287)! Learning rate decreased to 0.00792.
2024-12-02-04:40:50-root-INFO: grad norm: 82.712 80.985 16.817
2024-12-02-04:40:50-root-INFO: Loss too large (797.458->809.147)! Learning rate decreased to 0.00634.
2024-12-02-04:40:50-root-INFO: Loss too large (797.458->799.182)! Learning rate decreased to 0.00507.
2024-12-02-04:40:51-root-INFO: grad norm: 60.707 59.157 13.628
2024-12-02-04:40:52-root-INFO: grad norm: 37.142 35.958 9.305
2024-12-02-04:40:53-root-INFO: Loss Change: 804.577 -> 782.126
2024-12-02-04:40:53-root-INFO: Regularization Change: 0.000 -> 0.817
2024-12-02-04:40:53-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-04:40:53-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:40:53-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-04:40:54-root-INFO: grad norm: 57.735 55.956 14.220
2024-12-02-04:40:54-root-INFO: Loss too large (784.068->810.722)! Learning rate decreased to 0.01028.
2024-12-02-04:40:54-root-INFO: Loss too large (784.068->793.164)! Learning rate decreased to 0.00822.
2024-12-02-04:40:55-root-INFO: grad norm: 88.672 87.258 15.769
2024-12-02-04:40:56-root-INFO: Loss too large (783.841->798.680)! Learning rate decreased to 0.00658.
2024-12-02-04:40:56-root-INFO: Loss too large (783.841->785.232)! Learning rate decreased to 0.00526.
2024-12-02-04:40:57-root-INFO: grad norm: 64.892 63.606 12.853
2024-12-02-04:40:58-root-INFO: grad norm: 43.433 42.487 9.016
2024-12-02-04:40:59-root-INFO: grad norm: 35.094 34.003 8.683
2024-12-02-04:41:00-root-INFO: Loss Change: 784.068 -> 763.343
2024-12-02-04:41:00-root-INFO: Regularization Change: 0.000 -> 0.538
2024-12-02-04:41:00-root-INFO: Undo step: 155
2024-12-02-04:41:00-root-INFO: Undo step: 156
2024-12-02-04:41:00-root-INFO: Undo step: 157
2024-12-02-04:41:00-root-INFO: Undo step: 158
2024-12-02-04:41:00-root-INFO: Undo step: 159
2024-12-02-04:41:00-root-INFO: step: 160 lr_xt 0.01065404
2024-12-02-04:41:00-root-INFO: grad norm: 348.175 335.917 91.575
2024-12-02-04:41:01-root-INFO: grad norm: 370.446 366.106 56.542
2024-12-02-04:41:02-root-INFO: Loss too large (1346.337->1428.509)! Learning rate decreased to 0.00852.
2024-12-02-04:41:03-root-INFO: grad norm: 351.795 342.655 79.668
2024-12-02-04:41:03-root-INFO: Loss too large (1207.716->1243.498)! Learning rate decreased to 0.00682.
2024-12-02-04:41:04-root-INFO: grad norm: 223.660 217.879 50.524
2024-12-02-04:41:05-root-INFO: grad norm: 69.185 66.403 19.419
2024-12-02-04:41:06-root-INFO: Loss Change: 1530.203 -> 901.329
2024-12-02-04:41:06-root-INFO: Regularization Change: 0.000 -> 18.853
2024-12-02-04:41:06-root-INFO: Learning rate of xt decay: 0.08673 -> 0.08777.
2024-12-02-04:41:06-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-04:41:06-root-INFO: step: 159 lr_xt 0.01106461
2024-12-02-04:41:06-root-INFO: grad norm: 60.705 58.487 16.258
2024-12-02-04:41:07-root-INFO: grad norm: 124.815 117.894 40.984
2024-12-02-04:41:08-root-INFO: Loss too large (882.636->960.257)! Learning rate decreased to 0.00885.
2024-12-02-04:41:08-root-INFO: Loss too large (882.636->925.235)! Learning rate decreased to 0.00708.
2024-12-02-04:41:08-root-INFO: Loss too large (882.636->900.838)! Learning rate decreased to 0.00567.
2024-12-02-04:41:09-root-INFO: Loss too large (882.636->884.506)! Learning rate decreased to 0.00453.
2024-12-02-04:41:10-root-INFO: grad norm: 91.501 87.046 28.204
2024-12-02-04:41:11-root-INFO: grad norm: 55.088 52.797 15.721
2024-12-02-04:41:12-root-INFO: grad norm: 49.767 47.460 14.976
2024-12-02-04:41:12-root-INFO: Loss Change: 896.705 -> 842.067
2024-12-02-04:41:12-root-INFO: Regularization Change: 0.000 -> 1.606
2024-12-02-04:41:12-root-INFO: Learning rate of xt decay: 0.08777 -> 0.08883.
2024-12-02-04:41:12-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-04:41:13-root-INFO: step: 158 lr_xt 0.01148915
2024-12-02-04:41:13-root-INFO: grad norm: 52.941 50.470 15.985
2024-12-02-04:41:14-root-INFO: grad norm: 87.949 85.711 19.715
2024-12-02-04:41:14-root-INFO: Loss too large (833.092->885.354)! Learning rate decreased to 0.00919.
2024-12-02-04:41:15-root-INFO: Loss too large (833.092->851.029)! Learning rate decreased to 0.00735.
2024-12-02-04:41:16-root-INFO: grad norm: 119.182 116.532 24.991
2024-12-02-04:41:16-root-INFO: Loss too large (832.017->848.229)! Learning rate decreased to 0.00588.
2024-12-02-04:41:17-root-INFO: grad norm: 114.366 112.026 23.015
2024-12-02-04:41:18-root-INFO: grad norm: 108.614 105.992 23.723
2024-12-02-04:41:19-root-INFO: Loss Change: 837.843 -> 816.105
2024-12-02-04:41:19-root-INFO: Regularization Change: 0.000 -> 1.513
2024-12-02-04:41:19-root-INFO: Learning rate of xt decay: 0.08883 -> 0.08989.
2024-12-02-04:41:19-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:41:19-root-INFO: step: 157 lr_xt 0.01192805
2024-12-02-04:41:19-root-INFO: grad norm: 86.204 84.506 17.023
2024-12-02-04:41:20-root-INFO: Loss too large (803.369->860.370)! Learning rate decreased to 0.00954.
2024-12-02-04:41:20-root-INFO: Loss too large (803.369->830.263)! Learning rate decreased to 0.00763.
2024-12-02-04:41:20-root-INFO: Loss too large (803.369->811.410)! Learning rate decreased to 0.00611.
2024-12-02-04:41:21-root-INFO: grad norm: 90.449 87.918 21.245
2024-12-02-04:41:22-root-INFO: grad norm: 103.051 100.656 22.085
2024-12-02-04:41:23-root-INFO: Loss too large (796.145->799.400)! Learning rate decreased to 0.00489.
2024-12-02-04:41:24-root-INFO: grad norm: 77.634 75.014 19.999
2024-12-02-04:41:25-root-INFO: grad norm: 53.968 52.783 11.248
2024-12-02-04:41:25-root-INFO: Loss Change: 803.369 -> 775.909
2024-12-02-04:41:25-root-INFO: Regularization Change: 0.000 -> 0.653
2024-12-02-04:41:25-root-INFO: Learning rate of xt decay: 0.08989 -> 0.09097.
2024-12-02-04:41:25-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:41:26-root-INFO: step: 156 lr_xt 0.01238172
2024-12-02-04:41:26-root-INFO: grad norm: 68.006 66.243 15.383
2024-12-02-04:41:26-root-INFO: Loss too large (777.729->822.740)! Learning rate decreased to 0.00991.
2024-12-02-04:41:27-root-INFO: Loss too large (777.729->796.627)! Learning rate decreased to 0.00792.
2024-12-02-04:41:27-root-INFO: Loss too large (777.729->781.962)! Learning rate decreased to 0.00634.
2024-12-02-04:41:28-root-INFO: grad norm: 75.461 74.118 14.173
2024-12-02-04:41:29-root-INFO: grad norm: 82.007 80.227 16.994
2024-12-02-04:41:30-root-INFO: grad norm: 90.104 88.475 17.057
2024-12-02-04:41:30-root-INFO: Loss too large (770.421->771.157)! Learning rate decreased to 0.00507.
2024-12-02-04:41:31-root-INFO: grad norm: 66.025 64.331 14.860
2024-12-02-04:41:32-root-INFO: Loss Change: 777.729 -> 756.768
2024-12-02-04:41:32-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-04:41:32-root-INFO: Learning rate of xt decay: 0.09097 -> 0.09206.
2024-12-02-04:41:32-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-04:41:32-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-04:41:33-root-INFO: grad norm: 32.334 30.749 10.001
2024-12-02-04:41:34-root-INFO: grad norm: 41.698 40.087 11.479
2024-12-02-04:41:34-root-INFO: Loss too large (746.685->756.276)! Learning rate decreased to 0.01028.
2024-12-02-04:41:34-root-INFO: Loss too large (746.685->749.012)! Learning rate decreased to 0.00822.
2024-12-02-04:41:35-root-INFO: grad norm: 62.132 60.654 13.469
2024-12-02-04:41:36-root-INFO: Loss too large (745.148->751.413)! Learning rate decreased to 0.00658.
2024-12-02-04:41:37-root-INFO: grad norm: 69.372 67.578 15.674
2024-12-02-04:41:38-root-INFO: grad norm: 78.080 76.778 14.197
2024-12-02-04:41:38-root-INFO: Loss too large (742.684->743.501)! Learning rate decreased to 0.00526.
2024-12-02-04:41:38-root-INFO: Loss Change: 753.200 -> 737.188
2024-12-02-04:41:38-root-INFO: Regularization Change: 0.000 -> 0.843
2024-12-02-04:41:38-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-04:41:39-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-04:41:39-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-04:41:39-root-INFO: grad norm: 87.957 86.516 15.855
2024-12-02-04:41:39-root-INFO: Loss too large (742.407->818.582)! Learning rate decreased to 0.01067.
2024-12-02-04:41:40-root-INFO: Loss too large (742.407->780.515)! Learning rate decreased to 0.00853.
2024-12-02-04:41:40-root-INFO: Loss too large (742.407->755.573)! Learning rate decreased to 0.00683.
2024-12-02-04:41:41-root-INFO: grad norm: 95.880 95.127 11.994
2024-12-02-04:41:41-root-INFO: Loss too large (740.547->740.624)! Learning rate decreased to 0.00546.
2024-12-02-04:41:42-root-INFO: grad norm: 67.423 66.418 11.595
2024-12-02-04:41:43-root-INFO: grad norm: 48.866 48.335 7.184
2024-12-02-04:41:44-root-INFO: grad norm: 39.446 38.650 7.883
2024-12-02-04:41:45-root-INFO: Loss Change: 742.407 -> 718.103
2024-12-02-04:41:45-root-INFO: Regularization Change: 0.000 -> 0.504
2024-12-02-04:41:45-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-04:41:45-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-04:41:45-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-04:41:46-root-INFO: grad norm: 34.860 32.873 11.601
2024-12-02-04:41:47-root-INFO: grad norm: 53.763 50.986 17.055
2024-12-02-04:41:47-root-INFO: Loss too large (714.360->737.298)! Learning rate decreased to 0.01107.
2024-12-02-04:41:47-root-INFO: Loss too large (714.360->721.218)! Learning rate decreased to 0.00885.
2024-12-02-04:41:48-root-INFO: grad norm: 73.701 70.077 22.826
2024-12-02-04:41:49-root-INFO: Loss too large (712.548->719.646)! Learning rate decreased to 0.00708.
2024-12-02-04:41:50-root-INFO: grad norm: 63.674 61.099 17.927
2024-12-02-04:41:51-root-INFO: grad norm: 55.633 54.575 10.800
2024-12-02-04:41:51-root-INFO: Loss Change: 716.658 -> 703.808
2024-12-02-04:41:51-root-INFO: Regularization Change: 0.000 -> 0.850
2024-12-02-04:41:51-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-04:41:51-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-04:41:52-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-04:41:52-root-INFO: grad norm: 50.370 49.655 8.455
2024-12-02-04:41:52-root-INFO: Loss too large (699.740->725.651)! Learning rate decreased to 0.01148.
2024-12-02-04:41:53-root-INFO: Loss too large (699.740->711.954)! Learning rate decreased to 0.00919.
2024-12-02-04:41:53-root-INFO: Loss too large (699.740->703.551)! Learning rate decreased to 0.00735.
2024-12-02-04:41:54-root-INFO: grad norm: 58.728 58.031 9.021
2024-12-02-04:41:55-root-INFO: grad norm: 70.255 69.674 9.020
2024-12-02-04:41:55-root-INFO: Loss too large (698.233->699.490)! Learning rate decreased to 0.00588.
2024-12-02-04:41:56-root-INFO: grad norm: 54.920 54.376 7.710
2024-12-02-04:41:57-root-INFO: grad norm: 43.028 42.520 6.589
2024-12-02-04:41:58-root-INFO: Loss Change: 699.740 -> 686.652
2024-12-02-04:41:58-root-INFO: Regularization Change: 0.000 -> 0.431
2024-12-02-04:41:58-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-04:41:58-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-04:41:58-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-04:41:59-root-INFO: grad norm: 66.683 65.507 12.468
2024-12-02-04:41:59-root-INFO: Loss too large (691.177->739.782)! Learning rate decreased to 0.01191.
2024-12-02-04:41:59-root-INFO: Loss too large (691.177->715.257)! Learning rate decreased to 0.00953.
2024-12-02-04:42:00-root-INFO: Loss too large (691.177->699.238)! Learning rate decreased to 0.00762.
2024-12-02-04:42:01-root-INFO: grad norm: 76.169 75.508 10.014
2024-12-02-04:42:01-root-INFO: Loss too large (689.781->690.755)! Learning rate decreased to 0.00610.
2024-12-02-04:42:02-root-INFO: grad norm: 57.172 56.566 8.299
2024-12-02-04:42:03-root-INFO: grad norm: 43.182 42.630 6.879
2024-12-02-04:42:04-root-INFO: grad norm: 35.291 34.708 6.385
2024-12-02-04:42:05-root-INFO: Loss Change: 691.177 -> 673.400
2024-12-02-04:42:05-root-INFO: Regularization Change: 0.000 -> 0.459
2024-12-02-04:42:05-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-04:42:05-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:42:05-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-04:42:05-root-INFO: grad norm: 21.919 20.720 7.151
2024-12-02-04:42:06-root-INFO: grad norm: 25.275 24.474 6.312
2024-12-02-04:42:07-root-INFO: grad norm: 49.386 48.053 11.398
2024-12-02-04:42:08-root-INFO: Loss too large (667.127->695.847)! Learning rate decreased to 0.01235.
2024-12-02-04:42:08-root-INFO: Loss too large (667.127->678.663)! Learning rate decreased to 0.00988.
2024-12-02-04:42:08-root-INFO: Loss too large (667.127->669.086)! Learning rate decreased to 0.00790.
2024-12-02-04:42:09-root-INFO: grad norm: 51.210 50.251 9.864
2024-12-02-04:42:10-root-INFO: grad norm: 58.590 57.558 10.948
2024-12-02-04:42:11-root-INFO: Loss Change: 672.270 -> 663.561
2024-12-02-04:42:11-root-INFO: Regularization Change: 0.000 -> 0.943
2024-12-02-04:42:11-root-INFO: Undo step: 150
2024-12-02-04:42:11-root-INFO: Undo step: 151
2024-12-02-04:42:11-root-INFO: Undo step: 152
2024-12-02-04:42:11-root-INFO: Undo step: 153
2024-12-02-04:42:11-root-INFO: Undo step: 154
2024-12-02-04:42:11-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-04:42:12-root-INFO: grad norm: 376.917 366.063 89.801
2024-12-02-04:42:13-root-INFO: grad norm: 234.181 228.781 49.999
2024-12-02-04:42:14-root-INFO: grad norm: 255.415 252.642 37.531
2024-12-02-04:42:14-root-INFO: Loss too large (1004.152->1152.964)! Learning rate decreased to 0.01028.
2024-12-02-04:42:15-root-INFO: grad norm: 309.185 303.038 61.343
2024-12-02-04:42:15-root-INFO: Loss too large (975.975->1106.518)! Learning rate decreased to 0.00822.
2024-12-02-04:42:16-root-INFO: Loss too large (975.975->981.307)! Learning rate decreased to 0.00658.
2024-12-02-04:42:17-root-INFO: grad norm: 174.775 172.714 26.763
2024-12-02-04:42:17-root-INFO: Loss Change: 1439.646 -> 788.050
2024-12-02-04:42:17-root-INFO: Regularization Change: 0.000 -> 25.332
2024-12-02-04:42:17-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-04:42:17-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-04:42:18-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-04:42:18-root-INFO: grad norm: 73.423 71.666 15.968
2024-12-02-04:42:19-root-INFO: grad norm: 93.367 91.797 17.048
2024-12-02-04:42:19-root-INFO: Loss too large (767.645->804.558)! Learning rate decreased to 0.01067.
2024-12-02-04:42:20-root-INFO: Loss too large (767.645->772.008)! Learning rate decreased to 0.00853.
2024-12-02-04:42:21-root-INFO: grad norm: 96.243 94.850 16.320
2024-12-02-04:42:22-root-INFO: grad norm: 139.721 136.353 30.490
2024-12-02-04:42:22-root-INFO: Loss too large (754.672->784.174)! Learning rate decreased to 0.00683.
2024-12-02-04:42:22-root-INFO: Loss too large (754.672->756.193)! Learning rate decreased to 0.00546.
2024-12-02-04:42:23-root-INFO: grad norm: 83.191 81.641 15.984
2024-12-02-04:42:24-root-INFO: Loss Change: 785.286 -> 721.501
2024-12-02-04:42:24-root-INFO: Regularization Change: 0.000 -> 2.346
2024-12-02-04:42:24-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-04:42:24-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-04:42:24-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-04:42:25-root-INFO: grad norm: 42.115 40.566 11.316
2024-12-02-04:42:26-root-INFO: grad norm: 56.628 55.524 11.125
2024-12-02-04:42:26-root-INFO: Loss too large (709.764->726.037)! Learning rate decreased to 0.01107.
2024-12-02-04:42:26-root-INFO: Loss too large (709.764->712.719)! Learning rate decreased to 0.00885.
2024-12-02-04:42:27-root-INFO: grad norm: 67.254 66.318 11.178
2024-12-02-04:42:28-root-INFO: Loss too large (705.726->706.216)! Learning rate decreased to 0.00708.
2024-12-02-04:42:29-root-INFO: grad norm: 58.513 57.732 9.529
2024-12-02-04:42:30-root-INFO: grad norm: 51.722 50.969 8.796
2024-12-02-04:42:30-root-INFO: Loss Change: 718.873 -> 690.024
2024-12-02-04:42:30-root-INFO: Regularization Change: 0.000 -> 1.387
2024-12-02-04:42:30-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-04:42:30-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-04:42:31-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-04:42:31-root-INFO: grad norm: 45.042 44.099 9.165
2024-12-02-04:42:31-root-INFO: Loss too large (687.023->700.238)! Learning rate decreased to 0.01148.
2024-12-02-04:42:32-root-INFO: Loss too large (687.023->690.068)! Learning rate decreased to 0.00919.
2024-12-02-04:42:33-root-INFO: grad norm: 62.118 60.810 12.675
2024-12-02-04:42:33-root-INFO: Loss too large (684.873->688.908)! Learning rate decreased to 0.00735.
2024-12-02-04:42:34-root-INFO: grad norm: 59.553 58.428 11.523
2024-12-02-04:42:35-root-INFO: grad norm: 56.337 55.106 11.713
2024-12-02-04:42:36-root-INFO: grad norm: 53.996 52.879 10.923
2024-12-02-04:42:37-root-INFO: Loss Change: 687.023 -> 670.699
2024-12-02-04:42:37-root-INFO: Regularization Change: 0.000 -> 0.758
2024-12-02-04:42:37-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-04:42:37-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-04:42:37-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-04:42:37-root-INFO: grad norm: 57.379 56.224 11.450
2024-12-02-04:42:38-root-INFO: Loss too large (670.190->694.669)! Learning rate decreased to 0.01191.
2024-12-02-04:42:38-root-INFO: Loss too large (670.190->679.153)! Learning rate decreased to 0.00953.
2024-12-02-04:42:39-root-INFO: grad norm: 72.547 71.690 11.119
2024-12-02-04:42:39-root-INFO: Loss too large (670.103->674.083)! Learning rate decreased to 0.00762.
2024-12-02-04:42:40-root-INFO: grad norm: 63.621 62.933 9.328
2024-12-02-04:42:41-root-INFO: grad norm: 56.383 55.654 9.040
2024-12-02-04:42:42-root-INFO: grad norm: 50.618 50.006 7.851
2024-12-02-04:42:43-root-INFO: Loss Change: 670.190 -> 653.097
2024-12-02-04:42:43-root-INFO: Regularization Change: 0.000 -> 0.748
2024-12-02-04:42:43-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-04:42:43-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:42:43-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-04:42:44-root-INFO: grad norm: 39.769 38.820 8.636
2024-12-02-04:42:44-root-INFO: Loss too large (650.961->661.238)! Learning rate decreased to 0.01235.
2024-12-02-04:42:44-root-INFO: Loss too large (650.961->653.156)! Learning rate decreased to 0.00988.
2024-12-02-04:42:45-root-INFO: grad norm: 50.799 49.831 9.870
2024-12-02-04:42:46-root-INFO: Loss too large (648.964->650.641)! Learning rate decreased to 0.00790.
2024-12-02-04:42:47-root-INFO: grad norm: 45.791 44.843 9.267
2024-12-02-04:42:48-root-INFO: grad norm: 40.351 39.567 7.914
2024-12-02-04:42:48-root-INFO: grad norm: 37.183 36.370 7.730
2024-12-02-04:42:49-root-INFO: Loss Change: 650.961 -> 637.221
2024-12-02-04:42:49-root-INFO: Regularization Change: 0.000 -> 0.598
2024-12-02-04:42:49-root-INFO: Undo step: 150
2024-12-02-04:42:49-root-INFO: Undo step: 151
2024-12-02-04:42:49-root-INFO: Undo step: 152
2024-12-02-04:42:49-root-INFO: Undo step: 153
2024-12-02-04:42:49-root-INFO: Undo step: 154
2024-12-02-04:42:50-root-INFO: step: 155 lr_xt 0.01285057
2024-12-02-04:42:50-root-INFO: grad norm: 404.251 392.705 95.924
2024-12-02-04:42:50-root-INFO: Loss too large (1391.635->1717.535)! Learning rate decreased to 0.01028.
2024-12-02-04:42:51-root-INFO: Loss too large (1391.635->1445.216)! Learning rate decreased to 0.00822.
2024-12-02-04:42:52-root-INFO: grad norm: 293.533 289.185 50.337
2024-12-02-04:42:53-root-INFO: grad norm: 124.864 121.818 27.415
2024-12-02-04:42:54-root-INFO: grad norm: 146.972 143.125 33.405
2024-12-02-04:42:54-root-INFO: Loss too large (834.138->849.670)! Learning rate decreased to 0.00658.
2024-12-02-04:42:55-root-INFO: grad norm: 140.722 137.297 30.856
2024-12-02-04:42:55-root-INFO: Loss Change: 1391.635 -> 787.635
2024-12-02-04:42:55-root-INFO: Regularization Change: 0.000 -> 13.934
2024-12-02-04:42:55-root-INFO: Learning rate of xt decay: 0.09206 -> 0.09317.
2024-12-02-04:42:56-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-04:42:56-root-INFO: step: 154 lr_xt 0.01333503
2024-12-02-04:42:56-root-INFO: grad norm: 100.881 98.136 23.374
2024-12-02-04:42:57-root-INFO: Loss too large (777.671->820.358)! Learning rate decreased to 0.01067.
2024-12-02-04:42:57-root-INFO: Loss too large (777.671->790.288)! Learning rate decreased to 0.00853.
2024-12-02-04:42:58-root-INFO: grad norm: 110.352 107.721 23.953
2024-12-02-04:42:59-root-INFO: grad norm: 133.538 129.792 31.406
2024-12-02-04:42:59-root-INFO: Loss too large (758.159->769.887)! Learning rate decreased to 0.00683.
2024-12-02-04:43:00-root-INFO: grad norm: 98.422 96.003 21.689
2024-12-02-04:43:01-root-INFO: grad norm: 57.192 55.600 13.400
2024-12-02-04:43:02-root-INFO: Loss Change: 777.671 -> 712.254
2024-12-02-04:43:02-root-INFO: Regularization Change: 0.000 -> 2.303
2024-12-02-04:43:02-root-INFO: Learning rate of xt decay: 0.09317 -> 0.09429.
2024-12-02-04:43:02-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-04:43:02-root-INFO: step: 153 lr_xt 0.01383551
2024-12-02-04:43:02-root-INFO: grad norm: 57.349 55.057 16.052
2024-12-02-04:43:03-root-INFO: Loss too large (712.046->721.327)! Learning rate decreased to 0.01107.
2024-12-02-04:43:04-root-INFO: grad norm: 94.281 91.793 21.519
2024-12-02-04:43:04-root-INFO: Loss too large (709.277->735.356)! Learning rate decreased to 0.00885.
2024-12-02-04:43:04-root-INFO: Loss too large (709.277->715.212)! Learning rate decreased to 0.00708.
2024-12-02-04:43:06-root-INFO: grad norm: 75.333 73.444 16.763
2024-12-02-04:43:07-root-INFO: grad norm: 53.726 52.469 11.553
2024-12-02-04:43:08-root-INFO: grad norm: 46.468 45.213 10.725
2024-12-02-04:43:08-root-INFO: Loss Change: 712.046 -> 679.015
2024-12-02-04:43:08-root-INFO: Regularization Change: 0.000 -> 1.275
2024-12-02-04:43:08-root-INFO: Learning rate of xt decay: 0.09429 -> 0.09542.
2024-12-02-04:43:08-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-04:43:09-root-INFO: step: 152 lr_xt 0.01435246
2024-12-02-04:43:09-root-INFO: grad norm: 30.103 29.002 8.069
2024-12-02-04:43:10-root-INFO: grad norm: 43.193 42.018 10.005
2024-12-02-04:43:10-root-INFO: Loss too large (669.925->677.183)! Learning rate decreased to 0.01148.
2024-12-02-04:43:11-root-INFO: grad norm: 72.157 70.771 14.073
2024-12-02-04:43:12-root-INFO: Loss too large (669.636->684.396)! Learning rate decreased to 0.00919.
2024-12-02-04:43:12-root-INFO: Loss too large (669.636->672.138)! Learning rate decreased to 0.00735.
2024-12-02-04:43:13-root-INFO: grad norm: 57.286 55.981 12.160
2024-12-02-04:43:14-root-INFO: grad norm: 42.512 41.590 8.805
2024-12-02-04:43:15-root-INFO: Loss Change: 675.503 -> 653.770
2024-12-02-04:43:15-root-INFO: Regularization Change: 0.000 -> 1.156
2024-12-02-04:43:15-root-INFO: Learning rate of xt decay: 0.09542 -> 0.09656.
2024-12-02-04:43:15-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-04:43:15-root-INFO: step: 151 lr_xt 0.01488633
2024-12-02-04:43:15-root-INFO: grad norm: 63.177 61.778 13.223
2024-12-02-04:43:16-root-INFO: Loss too large (657.282->686.899)! Learning rate decreased to 0.01191.
2024-12-02-04:43:16-root-INFO: Loss too large (657.282->666.467)! Learning rate decreased to 0.00953.
2024-12-02-04:43:17-root-INFO: grad norm: 76.250 75.379 11.492
2024-12-02-04:43:17-root-INFO: Loss too large (655.149->658.637)! Learning rate decreased to 0.00762.
2024-12-02-04:43:18-root-INFO: grad norm: 60.735 59.887 10.119
2024-12-02-04:43:19-root-INFO: grad norm: 46.029 45.376 7.722
2024-12-02-04:43:20-root-INFO: grad norm: 38.684 38.045 6.999
2024-12-02-04:43:21-root-INFO: Loss Change: 657.282 -> 634.969
2024-12-02-04:43:21-root-INFO: Regularization Change: 0.000 -> 0.769
2024-12-02-04:43:21-root-INFO: Learning rate of xt decay: 0.09656 -> 0.09772.
2024-12-02-04:43:21-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:43:21-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-04:43:21-root-INFO: grad norm: 21.650 20.793 6.029
2024-12-02-04:43:22-root-INFO: grad norm: 24.896 24.200 5.844
2024-12-02-04:43:23-root-INFO: grad norm: 47.233 46.385 8.908
2024-12-02-04:43:24-root-INFO: Loss too large (626.261->643.794)! Learning rate decreased to 0.01235.
2024-12-02-04:43:24-root-INFO: Loss too large (626.261->632.606)! Learning rate decreased to 0.00988.
2024-12-02-04:43:25-root-INFO: grad norm: 57.640 56.850 9.509
2024-12-02-04:43:25-root-INFO: Loss too large (626.195->626.964)! Learning rate decreased to 0.00790.
2024-12-02-04:43:26-root-INFO: grad norm: 47.205 46.437 8.480
2024-12-02-04:43:27-root-INFO: Loss Change: 632.518 -> 617.906
2024-12-02-04:43:27-root-INFO: Regularization Change: 0.000 -> 1.018
2024-12-02-04:43:27-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-04:43:27-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:43:27-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-04:43:28-root-INFO: grad norm: 63.868 62.922 10.951
2024-12-02-04:43:28-root-INFO: Loss too large (621.665->658.123)! Learning rate decreased to 0.01281.
2024-12-02-04:43:28-root-INFO: Loss too large (621.665->636.329)! Learning rate decreased to 0.01024.
2024-12-02-04:43:29-root-INFO: Loss too large (621.665->623.251)! Learning rate decreased to 0.00820.
2024-12-02-04:43:30-root-INFO: grad norm: 53.058 52.400 8.329
2024-12-02-04:43:31-root-INFO: grad norm: 45.035 44.535 6.693
2024-12-02-04:43:32-root-INFO: grad norm: 38.801 38.230 6.633
2024-12-02-04:43:33-root-INFO: grad norm: 34.406 33.979 5.407
2024-12-02-04:43:33-root-INFO: Loss Change: 621.665 -> 603.535
2024-12-02-04:43:33-root-INFO: Regularization Change: 0.000 -> 0.612
2024-12-02-04:43:33-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-04:43:33-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:43:34-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-04:43:34-root-INFO: grad norm: 26.568 25.544 7.305
2024-12-02-04:43:35-root-INFO: grad norm: 54.467 53.546 9.974
2024-12-02-04:43:35-root-INFO: Loss too large (601.572->616.215)! Learning rate decreased to 0.01328.
2024-12-02-04:43:36-root-INFO: Loss too large (601.572->608.892)! Learning rate decreased to 0.01062.
2024-12-02-04:43:36-root-INFO: Loss too large (601.572->603.395)! Learning rate decreased to 0.00850.
2024-12-02-04:43:37-root-INFO: grad norm: 39.897 38.934 8.715
2024-12-02-04:43:38-root-INFO: grad norm: 23.342 22.737 5.281
2024-12-02-04:43:39-root-INFO: grad norm: 21.287 20.591 5.399
2024-12-02-04:43:40-root-INFO: Loss Change: 602.316 -> 589.015
2024-12-02-04:43:40-root-INFO: Regularization Change: 0.000 -> 0.656
2024-12-02-04:43:40-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-04:43:40-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-04:43:40-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-04:43:40-root-INFO: grad norm: 31.444 30.807 6.294
2024-12-02-04:43:41-root-INFO: Loss too large (589.286->594.116)! Learning rate decreased to 0.01376.
2024-12-02-04:43:41-root-INFO: Loss too large (589.286->589.967)! Learning rate decreased to 0.01101.
2024-12-02-04:43:42-root-INFO: grad norm: 38.245 37.751 6.126
2024-12-02-04:43:43-root-INFO: grad norm: 52.474 52.038 6.747
2024-12-02-04:43:43-root-INFO: Loss too large (587.657->590.872)! Learning rate decreased to 0.00881.
2024-12-02-04:43:44-root-INFO: grad norm: 45.596 45.039 7.102
2024-12-02-04:43:45-root-INFO: grad norm: 37.688 37.305 5.363
2024-12-02-04:43:46-root-INFO: Loss Change: 589.286 -> 579.221
2024-12-02-04:43:46-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-02-04:43:46-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-04:43:46-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:43:46-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-04:43:47-root-INFO: grad norm: 27.487 26.748 6.330
2024-12-02-04:43:47-root-INFO: Loss too large (577.255->580.933)! Learning rate decreased to 0.01426.
2024-12-02-04:43:48-root-INFO: grad norm: 50.526 49.843 8.281
2024-12-02-04:43:48-root-INFO: Loss too large (577.117->586.477)! Learning rate decreased to 0.01141.
2024-12-02-04:43:49-root-INFO: Loss too large (577.117->580.254)! Learning rate decreased to 0.00913.
2024-12-02-04:43:50-root-INFO: grad norm: 40.672 39.937 7.697
2024-12-02-04:43:51-root-INFO: grad norm: 28.001 27.553 4.991
2024-12-02-04:43:52-root-INFO: grad norm: 26.227 25.688 5.289
2024-12-02-04:43:52-root-INFO: Loss Change: 577.255 -> 566.490
2024-12-02-04:43:52-root-INFO: Regularization Change: 0.000 -> 0.564
2024-12-02-04:43:52-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-04:43:52-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:43:53-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-04:43:53-root-INFO: grad norm: 46.892 46.271 7.607
2024-12-02-04:43:53-root-INFO: Loss too large (569.754->592.692)! Learning rate decreased to 0.01478.
2024-12-02-04:43:54-root-INFO: Loss too large (569.754->579.060)! Learning rate decreased to 0.01182.
2024-12-02-04:43:54-root-INFO: Loss too large (569.754->570.962)! Learning rate decreased to 0.00946.
2024-12-02-04:43:55-root-INFO: grad norm: 42.565 42.086 6.368
2024-12-02-04:43:56-root-INFO: grad norm: 39.800 39.406 5.582
2024-12-02-04:43:57-root-INFO: grad norm: 36.989 36.498 6.003
2024-12-02-04:43:58-root-INFO: grad norm: 35.037 34.653 5.177
2024-12-02-04:43:59-root-INFO: Loss Change: 569.754 -> 557.866
2024-12-02-04:43:59-root-INFO: Regularization Change: 0.000 -> 0.584
2024-12-02-04:43:59-root-INFO: Undo step: 145
2024-12-02-04:43:59-root-INFO: Undo step: 146
2024-12-02-04:43:59-root-INFO: Undo step: 147
2024-12-02-04:43:59-root-INFO: Undo step: 148
2024-12-02-04:43:59-root-INFO: Undo step: 149
2024-12-02-04:43:59-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-04:43:59-root-INFO: grad norm: 206.251 200.686 47.590
2024-12-02-04:44:00-root-INFO: grad norm: 185.204 181.486 36.926
2024-12-02-04:44:01-root-INFO: Loss too large (905.297->979.749)! Learning rate decreased to 0.01235.
2024-12-02-04:44:02-root-INFO: grad norm: 302.310 295.154 65.387
2024-12-02-04:44:02-root-INFO: Loss too large (863.205->1078.070)! Learning rate decreased to 0.00988.
2024-12-02-04:44:02-root-INFO: Loss too large (863.205->929.163)! Learning rate decreased to 0.00790.
2024-12-02-04:44:03-root-INFO: grad norm: 156.577 154.759 23.785
2024-12-02-04:44:04-root-INFO: grad norm: 75.200 73.760 14.643
2024-12-02-04:44:05-root-INFO: Loss Change: 1061.515 -> 693.531
2024-12-02-04:44:05-root-INFO: Regularization Change: 0.000 -> 14.838
2024-12-02-04:44:05-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-04:44:05-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:44:05-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-04:44:06-root-INFO: grad norm: 71.125 69.425 15.456
2024-12-02-04:44:06-root-INFO: Loss too large (695.237->707.903)! Learning rate decreased to 0.01281.
2024-12-02-04:44:07-root-INFO: grad norm: 105.402 104.276 15.360
2024-12-02-04:44:07-root-INFO: Loss too large (691.045->718.739)! Learning rate decreased to 0.01024.
2024-12-02-04:44:08-root-INFO: grad norm: 114.905 113.650 16.939
2024-12-02-04:44:09-root-INFO: grad norm: 116.847 115.701 16.325
2024-12-02-04:44:10-root-INFO: grad norm: 115.939 114.784 16.322
2024-12-02-04:44:11-root-INFO: Loss Change: 695.237 -> 665.545
2024-12-02-04:44:11-root-INFO: Regularization Change: 0.000 -> 3.769
2024-12-02-04:44:11-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-04:44:11-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:44:11-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-04:44:12-root-INFO: grad norm: 90.014 89.319 11.169
2024-12-02-04:44:12-root-INFO: Loss too large (650.786->708.599)! Learning rate decreased to 0.01328.
2024-12-02-04:44:12-root-INFO: Loss too large (650.786->666.395)! Learning rate decreased to 0.01062.
2024-12-02-04:44:13-root-INFO: grad norm: 90.323 89.316 13.450
2024-12-02-04:44:14-root-INFO: grad norm: 90.186 89.389 11.964
2024-12-02-04:44:15-root-INFO: grad norm: 88.529 87.598 12.805
2024-12-02-04:44:16-root-INFO: grad norm: 85.983 85.155 11.903
2024-12-02-04:44:17-root-INFO: Loss Change: 650.786 -> 621.535
2024-12-02-04:44:17-root-INFO: Regularization Change: 0.000 -> 2.205
2024-12-02-04:44:17-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-04:44:17-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-04:44:17-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-04:44:18-root-INFO: grad norm: 102.425 101.205 15.762
2024-12-02-04:44:18-root-INFO: Loss too large (629.895->701.603)! Learning rate decreased to 0.01376.
2024-12-02-04:44:18-root-INFO: Loss too large (629.895->654.340)! Learning rate decreased to 0.01101.
2024-12-02-04:44:19-root-INFO: grad norm: 95.337 94.455 12.940
2024-12-02-04:44:20-root-INFO: grad norm: 86.272 85.299 12.925
2024-12-02-04:44:21-root-INFO: grad norm: 79.284 78.389 11.877
2024-12-02-04:44:22-root-INFO: grad norm: 73.262 72.349 11.531
2024-12-02-04:44:23-root-INFO: Loss Change: 629.895 -> 597.226
2024-12-02-04:44:23-root-INFO: Regularization Change: 0.000 -> 1.925
2024-12-02-04:44:23-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-04:44:23-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:44:23-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-04:44:24-root-INFO: grad norm: 56.516 55.790 9.030
2024-12-02-04:44:24-root-INFO: Loss too large (591.028->613.943)! Learning rate decreased to 0.01426.
2024-12-02-04:44:24-root-INFO: Loss too large (591.028->595.980)! Learning rate decreased to 0.01141.
2024-12-02-04:44:25-root-INFO: grad norm: 55.932 55.023 10.045
2024-12-02-04:44:26-root-INFO: grad norm: 55.725 54.924 9.419
2024-12-02-04:44:27-root-INFO: grad norm: 55.703 54.841 9.762
2024-12-02-04:44:28-root-INFO: grad norm: 55.579 54.755 9.536
2024-12-02-04:44:29-root-INFO: Loss Change: 591.028 -> 574.796
2024-12-02-04:44:29-root-INFO: Regularization Change: 0.000 -> 1.237
2024-12-02-04:44:29-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-04:44:29-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:44:29-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-04:44:30-root-INFO: grad norm: 79.845 78.599 14.052
2024-12-02-04:44:30-root-INFO: Loss too large (583.180->634.336)! Learning rate decreased to 0.01478.
2024-12-02-04:44:30-root-INFO: Loss too large (583.180->601.489)! Learning rate decreased to 0.01182.
2024-12-02-04:44:31-root-INFO: grad norm: 77.674 76.702 12.245
2024-12-02-04:44:32-root-INFO: grad norm: 73.146 72.129 12.154
2024-12-02-04:44:33-root-INFO: grad norm: 68.798 67.771 11.840
2024-12-02-04:44:34-root-INFO: grad norm: 64.721 63.750 11.169
2024-12-02-04:44:35-root-INFO: Loss Change: 583.180 -> 564.183
2024-12-02-04:44:35-root-INFO: Regularization Change: 0.000 -> 1.492
2024-12-02-04:44:35-root-INFO: Undo step: 145
2024-12-02-04:44:35-root-INFO: Undo step: 146
2024-12-02-04:44:35-root-INFO: Undo step: 147
2024-12-02-04:44:35-root-INFO: Undo step: 148
2024-12-02-04:44:35-root-INFO: Undo step: 149
2024-12-02-04:44:35-root-INFO: step: 150 lr_xt 0.01543756
2024-12-02-04:44:36-root-INFO: grad norm: 251.314 244.937 56.255
2024-12-02-04:44:37-root-INFO: grad norm: 120.469 118.537 21.488
2024-12-02-04:44:38-root-INFO: grad norm: 107.210 104.683 23.138
2024-12-02-04:44:39-root-INFO: grad norm: 132.052 129.704 24.789
2024-12-02-04:44:39-root-INFO: Loss too large (753.046->766.874)! Learning rate decreased to 0.01235.
2024-12-02-04:44:40-root-INFO: grad norm: 122.885 120.005 26.448
2024-12-02-04:44:40-root-INFO: Loss Change: 1189.575 -> 705.630
2024-12-02-04:44:40-root-INFO: Regularization Change: 0.000 -> 26.964
2024-12-02-04:44:41-root-INFO: Learning rate of xt decay: 0.09772 -> 0.09889.
2024-12-02-04:44:41-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:44:41-root-INFO: step: 149 lr_xt 0.01600663
2024-12-02-04:44:41-root-INFO: grad norm: 137.471 135.571 22.780
2024-12-02-04:44:42-root-INFO: Loss too large (716.597->759.075)! Learning rate decreased to 0.01281.
2024-12-02-04:44:43-root-INFO: grad norm: 130.383 128.456 22.330
2024-12-02-04:44:44-root-INFO: grad norm: 136.312 134.682 21.014
2024-12-02-04:44:44-root-INFO: Loss too large (684.822->688.333)! Learning rate decreased to 0.01024.
2024-12-02-04:44:45-root-INFO: grad norm: 94.770 93.392 16.100
2024-12-02-04:44:46-root-INFO: grad norm: 67.183 66.268 11.051
2024-12-02-04:44:47-root-INFO: Loss Change: 716.597 -> 617.825
2024-12-02-04:44:47-root-INFO: Regularization Change: 0.000 -> 3.787
2024-12-02-04:44:47-root-INFO: Learning rate of xt decay: 0.09889 -> 0.10008.
2024-12-02-04:44:47-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-04:44:47-root-INFO: step: 148 lr_xt 0.01659399
2024-12-02-04:44:47-root-INFO: grad norm: 36.225 35.388 7.742
2024-12-02-04:44:48-root-INFO: grad norm: 56.116 55.293 9.574
2024-12-02-04:44:49-root-INFO: Loss too large (606.123->619.768)! Learning rate decreased to 0.01328.
2024-12-02-04:44:49-root-INFO: Loss too large (606.123->607.231)! Learning rate decreased to 0.01062.
2024-12-02-04:44:50-root-INFO: grad norm: 50.733 49.926 9.016
2024-12-02-04:44:51-root-INFO: grad norm: 46.046 45.351 7.970
2024-12-02-04:44:52-root-INFO: grad norm: 43.125 42.416 7.785
2024-12-02-04:44:53-root-INFO: Loss Change: 609.973 -> 585.641
2024-12-02-04:44:53-root-INFO: Regularization Change: 0.000 -> 1.640
2024-12-02-04:44:53-root-INFO: Learning rate of xt decay: 0.10008 -> 0.10128.
2024-12-02-04:44:53-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-04:44:53-root-INFO: step: 147 lr_xt 0.01720013
2024-12-02-04:44:53-root-INFO: grad norm: 60.944 60.045 10.430
2024-12-02-04:44:54-root-INFO: Loss too large (588.804->610.818)! Learning rate decreased to 0.01376.
2024-12-02-04:44:54-root-INFO: Loss too large (588.804->593.915)! Learning rate decreased to 0.01101.
2024-12-02-04:44:55-root-INFO: grad norm: 57.055 56.435 8.384
2024-12-02-04:44:56-root-INFO: grad norm: 53.254 52.643 8.042
2024-12-02-04:44:57-root-INFO: grad norm: 50.626 50.023 7.791
2024-12-02-04:44:58-root-INFO: grad norm: 48.430 47.883 7.263
2024-12-02-04:44:58-root-INFO: Loss Change: 588.804 -> 568.657
2024-12-02-04:44:59-root-INFO: Regularization Change: 0.000 -> 1.232
2024-12-02-04:44:59-root-INFO: Learning rate of xt decay: 0.10128 -> 0.10250.
2024-12-02-04:44:59-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:44:59-root-INFO: step: 146 lr_xt 0.01782554
2024-12-02-04:44:59-root-INFO: grad norm: 33.172 32.674 5.728
2024-12-02-04:44:59-root-INFO: Loss too large (564.046->566.816)! Learning rate decreased to 0.01426.
2024-12-02-04:45:01-root-INFO: grad norm: 44.969 44.459 6.753
2024-12-02-04:45:01-root-INFO: Loss too large (562.669->565.526)! Learning rate decreased to 0.01141.
2024-12-02-04:45:02-root-INFO: grad norm: 44.555 44.051 6.677
2024-12-02-04:45:03-root-INFO: grad norm: 44.448 43.970 6.504
2024-12-02-04:45:04-root-INFO: grad norm: 44.497 43.994 6.671
2024-12-02-04:45:04-root-INFO: Loss Change: 564.046 -> 552.205
2024-12-02-04:45:04-root-INFO: Regularization Change: 0.000 -> 1.022
2024-12-02-04:45:04-root-INFO: Learning rate of xt decay: 0.10250 -> 0.10373.
2024-12-02-04:45:04-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:45:05-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-04:45:05-root-INFO: grad norm: 74.077 73.206 11.327
2024-12-02-04:45:05-root-INFO: Loss too large (560.528->604.035)! Learning rate decreased to 0.01478.
2024-12-02-04:45:06-root-INFO: Loss too large (560.528->575.926)! Learning rate decreased to 0.01182.
2024-12-02-04:45:07-root-INFO: grad norm: 71.797 71.157 9.565
2024-12-02-04:45:08-root-INFO: grad norm: 67.518 66.880 9.256
2024-12-02-04:45:09-root-INFO: grad norm: 63.862 63.192 9.232
2024-12-02-04:45:10-root-INFO: grad norm: 60.625 60.021 8.532
2024-12-02-04:45:10-root-INFO: Loss Change: 560.528 -> 543.939
2024-12-02-04:45:10-root-INFO: Regularization Change: 0.000 -> 1.322
2024-12-02-04:45:10-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-04:45:10-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:45:11-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-04:45:11-root-INFO: grad norm: 44.054 43.537 6.731
2024-12-02-04:45:11-root-INFO: Loss too large (537.724->551.307)! Learning rate decreased to 0.01531.
2024-12-02-04:45:12-root-INFO: Loss too large (537.724->539.900)! Learning rate decreased to 0.01225.
2024-12-02-04:45:13-root-INFO: grad norm: 43.053 42.465 7.094
2024-12-02-04:45:14-root-INFO: grad norm: 42.661 42.099 6.901
2024-12-02-04:45:15-root-INFO: grad norm: 42.538 41.997 6.762
2024-12-02-04:45:16-root-INFO: grad norm: 42.424 41.845 6.984
2024-12-02-04:45:16-root-INFO: Loss Change: 537.724 -> 525.180
2024-12-02-04:45:16-root-INFO: Regularization Change: 0.000 -> 0.945
2024-12-02-04:45:16-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-04:45:16-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-04:45:17-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-04:45:17-root-INFO: grad norm: 59.104 58.360 9.347
2024-12-02-04:45:17-root-INFO: Loss too large (528.874->559.334)! Learning rate decreased to 0.01586.
2024-12-02-04:45:18-root-INFO: Loss too large (528.874->539.599)! Learning rate decreased to 0.01269.
2024-12-02-04:45:19-root-INFO: grad norm: 58.250 57.575 8.846
2024-12-02-04:45:20-root-INFO: grad norm: 56.902 56.262 8.512
2024-12-02-04:45:21-root-INFO: grad norm: 55.325 54.615 8.839
2024-12-02-04:45:22-root-INFO: grad norm: 53.585 52.953 8.210
2024-12-02-04:45:22-root-INFO: Loss Change: 528.874 -> 517.807
2024-12-02-04:45:22-root-INFO: Regularization Change: 0.000 -> 1.139
2024-12-02-04:45:22-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-04:45:22-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-04:45:23-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-04:45:23-root-INFO: grad norm: 38.411 37.804 6.798
2024-12-02-04:45:23-root-INFO: Loss too large (512.892->522.958)! Learning rate decreased to 0.01642.
2024-12-02-04:45:24-root-INFO: Loss too large (512.892->513.829)! Learning rate decreased to 0.01314.
2024-12-02-04:45:25-root-INFO: grad norm: 38.460 37.843 6.864
2024-12-02-04:45:26-root-INFO: grad norm: 38.822 38.255 6.613
2024-12-02-04:45:27-root-INFO: grad norm: 39.287 38.740 6.534
2024-12-02-04:45:28-root-INFO: grad norm: 39.859 39.296 6.677
2024-12-02-04:45:28-root-INFO: Loss Change: 512.892 -> 501.943
2024-12-02-04:45:28-root-INFO: Regularization Change: 0.000 -> 0.914
2024-12-02-04:45:28-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-04:45:28-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-04:45:29-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-04:45:29-root-INFO: grad norm: 53.218 52.534 8.504
2024-12-02-04:45:29-root-INFO: Loss too large (504.919->531.411)! Learning rate decreased to 0.01701.
2024-12-02-04:45:30-root-INFO: Loss too large (504.919->514.841)! Learning rate decreased to 0.01361.
2024-12-02-04:45:31-root-INFO: grad norm: 52.437 51.838 7.900
2024-12-02-04:45:32-root-INFO: grad norm: 50.510 49.942 7.554
2024-12-02-04:45:33-root-INFO: grad norm: 49.668 49.056 7.772
2024-12-02-04:45:34-root-INFO: grad norm: 48.624 48.079 7.257
2024-12-02-04:45:34-root-INFO: Loss Change: 504.919 -> 495.567
2024-12-02-04:45:34-root-INFO: Regularization Change: 0.000 -> 1.082
2024-12-02-04:45:34-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-04:45:34-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-04:45:35-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-04:45:35-root-INFO: grad norm: 37.806 37.285 6.255
2024-12-02-04:45:35-root-INFO: Loss too large (491.952->504.201)! Learning rate decreased to 0.01761.
2024-12-02-04:45:35-root-INFO: Loss too large (491.952->494.066)! Learning rate decreased to 0.01409.
2024-12-02-04:45:37-root-INFO: grad norm: 38.479 37.969 6.250
2024-12-02-04:45:37-root-INFO: grad norm: 38.898 38.384 6.301
2024-12-02-04:45:39-root-INFO: grad norm: 39.241 38.773 6.047
2024-12-02-04:45:39-root-INFO: grad norm: 39.651 39.138 6.356
2024-12-02-04:45:40-root-INFO: Loss Change: 491.952 -> 482.196
2024-12-02-04:45:40-root-INFO: Regularization Change: 0.000 -> 0.919
2024-12-02-04:45:40-root-INFO: Undo step: 140
2024-12-02-04:45:40-root-INFO: Undo step: 141
2024-12-02-04:45:40-root-INFO: Undo step: 142
2024-12-02-04:45:40-root-INFO: Undo step: 143
2024-12-02-04:45:40-root-INFO: Undo step: 144
2024-12-02-04:45:41-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-04:45:41-root-INFO: grad norm: 294.414 288.713 57.660
2024-12-02-04:45:42-root-INFO: grad norm: 154.215 151.385 29.409
2024-12-02-04:45:43-root-INFO: grad norm: 104.379 102.667 18.824
2024-12-02-04:45:44-root-INFO: grad norm: 96.449 95.361 14.444
2024-12-02-04:45:45-root-INFO: grad norm: 121.768 119.901 21.240
2024-12-02-04:45:45-root-INFO: Loss too large (629.622->667.757)! Learning rate decreased to 0.01478.
2024-12-02-04:45:46-root-INFO: Loss Change: 1133.576 -> 622.959
2024-12-02-04:45:46-root-INFO: Regularization Change: 0.000 -> 35.777
2024-12-02-04:45:46-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-04:45:46-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:45:46-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-04:45:47-root-INFO: grad norm: 105.535 104.207 16.694
2024-12-02-04:45:48-root-INFO: grad norm: 129.430 127.508 22.223
2024-12-02-04:45:48-root-INFO: Loss too large (613.115->666.539)! Learning rate decreased to 0.01531.
2024-12-02-04:45:49-root-INFO: grad norm: 111.936 110.624 17.089
2024-12-02-04:45:50-root-INFO: grad norm: 97.062 96.035 14.085
2024-12-02-04:45:51-root-INFO: grad norm: 102.680 101.743 13.840
2024-12-02-04:45:52-root-INFO: Loss Change: 627.575 -> 569.686
2024-12-02-04:45:52-root-INFO: Regularization Change: 0.000 -> 6.129
2024-12-02-04:45:52-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-04:45:52-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-04:45:52-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-04:45:52-root-INFO: grad norm: 94.215 93.355 12.705
2024-12-02-04:45:53-root-INFO: Loss too large (558.344->597.744)! Learning rate decreased to 0.01586.
2024-12-02-04:45:54-root-INFO: grad norm: 99.920 99.071 13.003
2024-12-02-04:45:54-root-INFO: Loss too large (555.271->556.796)! Learning rate decreased to 0.01269.
2024-12-02-04:45:55-root-INFO: grad norm: 66.962 66.225 9.911
2024-12-02-04:45:56-root-INFO: grad norm: 45.404 44.969 6.271
2024-12-02-04:45:57-root-INFO: grad norm: 34.623 34.111 5.934
2024-12-02-04:45:58-root-INFO: Loss Change: 558.344 -> 507.662
2024-12-02-04:45:58-root-INFO: Regularization Change: 0.000 -> 1.993
2024-12-02-04:45:58-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-04:45:58-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-04:45:58-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-04:45:58-root-INFO: grad norm: 50.110 49.274 9.118
2024-12-02-04:45:59-root-INFO: Loss too large (513.039->524.961)! Learning rate decreased to 0.01642.
2024-12-02-04:46:00-root-INFO: grad norm: 57.970 57.432 7.884
2024-12-02-04:46:00-root-INFO: Loss too large (512.964->514.182)! Learning rate decreased to 0.01314.
2024-12-02-04:46:01-root-INFO: grad norm: 44.278 43.813 6.401
2024-12-02-04:46:02-root-INFO: grad norm: 35.400 34.943 5.674
2024-12-02-04:46:03-root-INFO: grad norm: 29.236 28.831 4.847
2024-12-02-04:46:04-root-INFO: Loss Change: 513.039 -> 492.826
2024-12-02-04:46:04-root-INFO: Regularization Change: 0.000 -> 1.183
2024-12-02-04:46:04-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-04:46:04-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-04:46:04-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-04:46:04-root-INFO: grad norm: 18.985 18.502 4.255
2024-12-02-04:46:05-root-INFO: grad norm: 26.002 25.567 4.735
2024-12-02-04:46:06-root-INFO: Loss too large (488.343->490.445)! Learning rate decreased to 0.01701.
2024-12-02-04:46:07-root-INFO: grad norm: 35.299 34.826 5.758
2024-12-02-04:46:07-root-INFO: Loss too large (487.262->488.832)! Learning rate decreased to 0.01361.
2024-12-02-04:46:08-root-INFO: grad norm: 31.549 31.107 5.263
2024-12-02-04:46:09-root-INFO: grad norm: 27.683 27.243 4.918
2024-12-02-04:46:09-root-INFO: Loss Change: 490.890 -> 480.156
2024-12-02-04:46:10-root-INFO: Regularization Change: 0.000 -> 1.044
2024-12-02-04:46:10-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-04:46:10-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-04:46:10-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-04:46:10-root-INFO: grad norm: 37.443 36.819 6.806
2024-12-02-04:46:11-root-INFO: Loss too large (482.134->490.696)! Learning rate decreased to 0.01761.
2024-12-02-04:46:11-root-INFO: Loss too large (482.134->482.375)! Learning rate decreased to 0.01409.
2024-12-02-04:46:12-root-INFO: grad norm: 33.383 32.974 5.213
2024-12-02-04:46:13-root-INFO: grad norm: 30.641 30.209 5.126
2024-12-02-04:46:14-root-INFO: grad norm: 28.831 28.415 4.878
2024-12-02-04:46:15-root-INFO: grad norm: 27.345 26.935 4.718
2024-12-02-04:46:15-root-INFO: Loss Change: 482.134 -> 469.080
2024-12-02-04:46:15-root-INFO: Regularization Change: 0.000 -> 0.864
2024-12-02-04:46:15-root-INFO: Undo step: 140
2024-12-02-04:46:16-root-INFO: Undo step: 141
2024-12-02-04:46:16-root-INFO: Undo step: 142
2024-12-02-04:46:16-root-INFO: Undo step: 143
2024-12-02-04:46:16-root-INFO: Undo step: 144
2024-12-02-04:46:16-root-INFO: step: 145 lr_xt 0.01847071
2024-12-02-04:46:16-root-INFO: grad norm: 311.624 306.064 58.607
2024-12-02-04:46:17-root-INFO: grad norm: 246.456 242.363 44.725
2024-12-02-04:46:18-root-INFO: grad norm: 183.252 179.224 38.211
2024-12-02-04:46:19-root-INFO: grad norm: 159.629 157.422 26.452
2024-12-02-04:46:20-root-INFO: Loss too large (677.458->739.504)! Learning rate decreased to 0.01478.
2024-12-02-04:46:21-root-INFO: grad norm: 129.726 127.924 21.548
2024-12-02-04:46:21-root-INFO: Loss Change: 1187.959 -> 570.043
2024-12-02-04:46:21-root-INFO: Regularization Change: 0.000 -> 35.314
2024-12-02-04:46:21-root-INFO: Learning rate of xt decay: 0.10373 -> 0.10497.
2024-12-02-04:46:21-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-04:46:22-root-INFO: step: 144 lr_xt 0.01913614
2024-12-02-04:46:22-root-INFO: grad norm: 70.999 69.779 13.108
2024-12-02-04:46:22-root-INFO: Loss too large (567.257->570.520)! Learning rate decreased to 0.01531.
2024-12-02-04:46:23-root-INFO: grad norm: 60.414 59.294 11.581
2024-12-02-04:46:24-root-INFO: grad norm: 47.169 46.208 9.473
2024-12-02-04:46:25-root-INFO: grad norm: 44.252 43.280 9.221
2024-12-02-04:46:26-root-INFO: grad norm: 44.635 43.801 8.591
2024-12-02-04:46:27-root-INFO: Loss Change: 567.257 -> 520.495
2024-12-02-04:46:27-root-INFO: Regularization Change: 0.000 -> 3.738
2024-12-02-04:46:27-root-INFO: Learning rate of xt decay: 0.10497 -> 0.10623.
2024-12-02-04:46:27-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-04:46:27-root-INFO: step: 143 lr_xt 0.01982236
2024-12-02-04:46:27-root-INFO: grad norm: 48.222 47.364 9.058
2024-12-02-04:46:28-root-INFO: grad norm: 73.452 72.393 12.431
2024-12-02-04:46:29-root-INFO: Loss too large (518.714->544.646)! Learning rate decreased to 0.01586.
2024-12-02-04:46:29-root-INFO: Loss too large (518.714->524.902)! Learning rate decreased to 0.01269.
2024-12-02-04:46:30-root-INFO: grad norm: 50.929 50.205 8.555
2024-12-02-04:46:31-root-INFO: grad norm: 28.807 28.215 5.808
2024-12-02-04:46:32-root-INFO: grad norm: 23.321 22.800 4.904
2024-12-02-04:46:33-root-INFO: Loss Change: 521.341 -> 491.944
2024-12-02-04:46:33-root-INFO: Regularization Change: 0.000 -> 1.596
2024-12-02-04:46:33-root-INFO: Learning rate of xt decay: 0.10623 -> 0.10751.
2024-12-02-04:46:33-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-04:46:33-root-INFO: step: 142 lr_xt 0.02052986
2024-12-02-04:46:33-root-INFO: grad norm: 22.336 21.444 6.247
2024-12-02-04:46:34-root-INFO: grad norm: 25.855 25.094 6.225
2024-12-02-04:46:35-root-INFO: Loss too large (486.760->487.300)! Learning rate decreased to 0.01642.
2024-12-02-04:46:36-root-INFO: grad norm: 34.541 34.091 5.557
2024-12-02-04:46:36-root-INFO: Loss too large (484.779->486.207)! Learning rate decreased to 0.01314.
2024-12-02-04:46:37-root-INFO: grad norm: 32.404 31.876 5.826
2024-12-02-04:46:38-root-INFO: grad norm: 31.312 30.964 4.659
2024-12-02-04:46:39-root-INFO: Loss Change: 491.041 -> 478.499
2024-12-02-04:46:39-root-INFO: Regularization Change: 0.000 -> 1.220
2024-12-02-04:46:39-root-INFO: Learning rate of xt decay: 0.10751 -> 0.10880.
2024-12-02-04:46:39-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-04:46:39-root-INFO: step: 141 lr_xt 0.02125920
2024-12-02-04:46:39-root-INFO: grad norm: 27.897 27.388 5.301
2024-12-02-04:46:40-root-INFO: Loss too large (476.779->479.376)! Learning rate decreased to 0.01701.
2024-12-02-04:46:41-root-INFO: grad norm: 42.836 42.450 5.734
2024-12-02-04:46:41-root-INFO: Loss too large (474.858->478.886)! Learning rate decreased to 0.01361.
2024-12-02-04:46:41-root-INFO: Loss too large (474.858->475.553)! Learning rate decreased to 0.01088.
2024-12-02-04:46:42-root-INFO: grad norm: 28.172 27.750 4.856
2024-12-02-04:46:43-root-INFO: grad norm: 13.562 13.143 3.347
2024-12-02-04:46:44-root-INFO: grad norm: 12.840 12.369 3.446
2024-12-02-04:46:45-root-INFO: Loss Change: 476.779 -> 464.581
2024-12-02-04:46:45-root-INFO: Regularization Change: 0.000 -> 0.616
2024-12-02-04:46:45-root-INFO: Learning rate of xt decay: 0.10880 -> 0.11010.
2024-12-02-04:46:45-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-04:46:45-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-04:46:46-root-INFO: grad norm: 23.018 22.476 4.966
2024-12-02-04:46:47-root-INFO: grad norm: 36.190 35.721 5.807
2024-12-02-04:46:47-root-INFO: Loss too large (464.436->475.257)! Learning rate decreased to 0.01761.
2024-12-02-04:46:47-root-INFO: Loss too large (464.436->466.456)! Learning rate decreased to 0.01409.
2024-12-02-04:46:48-root-INFO: grad norm: 36.347 36.034 4.759
2024-12-02-04:46:49-root-INFO: grad norm: 35.145 34.746 5.282
2024-12-02-04:46:50-root-INFO: grad norm: 35.514 35.211 4.625
2024-12-02-04:46:51-root-INFO: Loss too large (457.873->458.063)! Learning rate decreased to 0.01127.
2024-12-02-04:46:51-root-INFO: Loss Change: 464.881 -> 455.763
2024-12-02-04:46:51-root-INFO: Regularization Change: 0.000 -> 0.855
2024-12-02-04:46:51-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-04:46:51-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:46:52-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-04:46:52-root-INFO: grad norm: 27.168 26.645 5.302
2024-12-02-04:46:52-root-INFO: Loss too large (455.408->460.065)! Learning rate decreased to 0.01823.
2024-12-02-04:46:53-root-INFO: grad norm: 45.478 45.097 5.879
2024-12-02-04:46:54-root-INFO: Loss too large (454.974->460.357)! Learning rate decreased to 0.01458.
2024-12-02-04:46:54-root-INFO: Loss too large (454.974->456.514)! Learning rate decreased to 0.01167.
2024-12-02-04:46:55-root-INFO: grad norm: 28.700 28.327 4.607
2024-12-02-04:46:56-root-INFO: grad norm: 11.850 11.486 2.915
2024-12-02-04:46:57-root-INFO: grad norm: 11.241 10.837 2.986
2024-12-02-04:46:58-root-INFO: Loss Change: 455.408 -> 444.719
2024-12-02-04:46:58-root-INFO: Regularization Change: 0.000 -> 0.564
2024-12-02-04:46:58-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-04:46:58-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:46:58-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-04:46:58-root-INFO: grad norm: 17.404 16.983 3.803
2024-12-02-04:46:59-root-INFO: grad norm: 26.722 26.324 4.594
2024-12-02-04:47:00-root-INFO: Loss too large (443.408->449.358)! Learning rate decreased to 0.01887.
2024-12-02-04:47:00-root-INFO: Loss too large (443.408->444.365)! Learning rate decreased to 0.01509.
2024-12-02-04:47:01-root-INFO: grad norm: 28.851 28.583 3.920
2024-12-02-04:47:02-root-INFO: grad norm: 30.361 30.020 4.542
2024-12-02-04:47:03-root-INFO: grad norm: 35.279 34.989 4.513
2024-12-02-04:47:03-root-INFO: Loss too large (439.640->440.912)! Learning rate decreased to 0.01207.
2024-12-02-04:47:04-root-INFO: Loss Change: 444.606 -> 438.353
2024-12-02-04:47:04-root-INFO: Regularization Change: 0.000 -> 0.778
2024-12-02-04:47:04-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-04:47:04-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:47:04-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-04:47:05-root-INFO: grad norm: 31.181 30.509 6.439
2024-12-02-04:47:05-root-INFO: Loss too large (438.940->446.283)! Learning rate decreased to 0.01952.
2024-12-02-04:47:05-root-INFO: Loss too large (438.940->439.183)! Learning rate decreased to 0.01562.
2024-12-02-04:47:06-root-INFO: grad norm: 34.184 33.855 4.734
2024-12-02-04:47:07-root-INFO: Loss too large (435.437->436.662)! Learning rate decreased to 0.01250.
2024-12-02-04:47:08-root-INFO: grad norm: 26.789 26.428 4.385
2024-12-02-04:47:09-root-INFO: grad norm: 18.254 17.963 3.244
2024-12-02-04:47:10-root-INFO: grad norm: 17.796 17.466 3.409
2024-12-02-04:47:10-root-INFO: Loss Change: 438.940 -> 427.980
2024-12-02-04:47:10-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-02-04:47:10-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-04:47:10-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-04:47:11-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-04:47:11-root-INFO: grad norm: 21.434 21.134 3.574
2024-12-02-04:47:11-root-INFO: Loss too large (428.217->431.153)! Learning rate decreased to 0.02020.
2024-12-02-04:47:12-root-INFO: Loss too large (428.217->428.852)! Learning rate decreased to 0.01616.
2024-12-02-04:47:13-root-INFO: grad norm: 24.212 23.904 3.845
2024-12-02-04:47:14-root-INFO: grad norm: 32.267 31.987 4.242
2024-12-02-04:47:14-root-INFO: Loss too large (426.448->428.017)! Learning rate decreased to 0.01293.
2024-12-02-04:47:15-root-INFO: grad norm: 26.001 25.698 3.957
2024-12-02-04:47:16-root-INFO: grad norm: 18.155 17.895 3.058
2024-12-02-04:47:17-root-INFO: Loss Change: 428.217 -> 421.160
2024-12-02-04:47:17-root-INFO: Regularization Change: 0.000 -> 0.530
2024-12-02-04:47:17-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-04:47:17-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-04:47:17-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-04:47:17-root-INFO: grad norm: 22.406 21.965 4.425
2024-12-02-04:47:18-root-INFO: Loss too large (421.062->425.062)! Learning rate decreased to 0.02090.
2024-12-02-04:47:18-root-INFO: Loss too large (421.062->421.366)! Learning rate decreased to 0.01672.
2024-12-02-04:47:19-root-INFO: grad norm: 26.979 26.694 3.907
2024-12-02-04:47:19-root-INFO: Loss too large (419.426->420.037)! Learning rate decreased to 0.01338.
2024-12-02-04:47:20-root-INFO: grad norm: 23.265 22.957 3.776
2024-12-02-04:47:21-root-INFO: grad norm: 19.633 19.368 3.212
2024-12-02-04:47:22-root-INFO: grad norm: 18.975 18.685 3.303
2024-12-02-04:47:23-root-INFO: Loss Change: 421.062 -> 413.564
2024-12-02-04:47:23-root-INFO: Regularization Change: 0.000 -> 0.516
2024-12-02-04:47:23-root-INFO: Undo step: 135
2024-12-02-04:47:23-root-INFO: Undo step: 136
2024-12-02-04:47:23-root-INFO: Undo step: 137
2024-12-02-04:47:23-root-INFO: Undo step: 138
2024-12-02-04:47:23-root-INFO: Undo step: 139
2024-12-02-04:47:23-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-04:47:24-root-INFO: grad norm: 189.052 184.358 41.868
2024-12-02-04:47:25-root-INFO: grad norm: 99.649 97.537 20.409
2024-12-02-04:47:26-root-INFO: grad norm: 98.146 96.829 16.019
2024-12-02-04:47:27-root-INFO: grad norm: 149.067 147.250 23.205
2024-12-02-04:47:27-root-INFO: Loss too large (593.807->661.854)! Learning rate decreased to 0.01761.
2024-12-02-04:47:27-root-INFO: Loss too large (593.807->595.810)! Learning rate decreased to 0.01409.
2024-12-02-04:47:28-root-INFO: grad norm: 82.062 80.899 13.764
2024-12-02-04:47:29-root-INFO: Loss Change: 945.273 -> 503.044
2024-12-02-04:47:29-root-INFO: Regularization Change: 0.000 -> 28.022
2024-12-02-04:47:29-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-04:47:29-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:47:29-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-04:47:30-root-INFO: grad norm: 42.339 41.197 9.766
2024-12-02-04:47:31-root-INFO: grad norm: 74.735 74.006 10.408
2024-12-02-04:47:31-root-INFO: Loss too large (496.243->510.362)! Learning rate decreased to 0.01823.
2024-12-02-04:47:31-root-INFO: Loss too large (496.243->499.467)! Learning rate decreased to 0.01458.
2024-12-02-04:47:32-root-INFO: grad norm: 42.413 41.747 7.484
2024-12-02-04:47:33-root-INFO: grad norm: 32.187 31.561 6.320
2024-12-02-04:47:34-root-INFO: grad norm: 20.918 20.124 5.711
2024-12-02-04:47:35-root-INFO: Loss Change: 502.542 -> 460.336
2024-12-02-04:47:35-root-INFO: Regularization Change: 0.000 -> 3.115
2024-12-02-04:47:35-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-04:47:35-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:47:35-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-04:47:36-root-INFO: grad norm: 21.932 21.212 5.572
2024-12-02-04:47:37-root-INFO: grad norm: 39.805 39.200 6.918
2024-12-02-04:47:37-root-INFO: Loss too large (455.238->462.759)! Learning rate decreased to 0.01887.
2024-12-02-04:47:37-root-INFO: Loss too large (455.238->458.790)! Learning rate decreased to 0.01509.
2024-12-02-04:47:38-root-INFO: Loss too large (455.238->455.622)! Learning rate decreased to 0.01207.
2024-12-02-04:47:39-root-INFO: grad norm: 31.273 30.794 5.448
2024-12-02-04:47:40-root-INFO: grad norm: 21.211 20.680 4.717
2024-12-02-04:47:41-root-INFO: grad norm: 21.779 21.269 4.688
2024-12-02-04:47:41-root-INFO: Loss Change: 459.288 -> 442.324
2024-12-02-04:47:41-root-INFO: Regularization Change: 0.000 -> 1.273
2024-12-02-04:47:41-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-04:47:41-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:47:42-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-04:47:42-root-INFO: grad norm: 37.480 36.904 6.548
2024-12-02-04:47:42-root-INFO: Loss too large (443.942->453.573)! Learning rate decreased to 0.01952.
2024-12-02-04:47:42-root-INFO: Loss too large (443.942->447.396)! Learning rate decreased to 0.01562.
2024-12-02-04:47:44-root-INFO: grad norm: 37.328 36.889 5.708
2024-12-02-04:47:45-root-INFO: grad norm: 43.583 43.191 5.831
2024-12-02-04:47:45-root-INFO: Loss too large (438.207->441.418)! Learning rate decreased to 0.01250.
2024-12-02-04:47:46-root-INFO: grad norm: 33.018 32.623 5.093
2024-12-02-04:47:47-root-INFO: grad norm: 18.277 17.840 3.973
2024-12-02-04:47:47-root-INFO: Loss Change: 443.942 -> 428.604
2024-12-02-04:47:47-root-INFO: Regularization Change: 0.000 -> 0.943
2024-12-02-04:47:48-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-04:47:48-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-04:47:48-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-04:47:48-root-INFO: grad norm: 19.263 18.806 4.169
2024-12-02-04:47:48-root-INFO: Loss too large (428.167->429.514)! Learning rate decreased to 0.02020.
2024-12-02-04:47:49-root-INFO: grad norm: 37.338 36.955 5.337
2024-12-02-04:47:50-root-INFO: Loss too large (427.353->433.788)! Learning rate decreased to 0.01616.
2024-12-02-04:47:50-root-INFO: Loss too large (427.353->430.219)! Learning rate decreased to 0.01293.
2024-12-02-04:47:50-root-INFO: Loss too large (427.353->427.377)! Learning rate decreased to 0.01034.
2024-12-02-04:47:51-root-INFO: grad norm: 26.479 26.124 4.321
2024-12-02-04:47:53-root-INFO: grad norm: 16.056 15.653 3.577
2024-12-02-04:47:53-root-INFO: grad norm: 14.797 14.379 3.491
2024-12-02-04:47:54-root-INFO: Loss Change: 428.167 -> 418.986
2024-12-02-04:47:54-root-INFO: Regularization Change: 0.000 -> 0.548
2024-12-02-04:47:54-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-04:47:54-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-04:47:54-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-04:47:55-root-INFO: grad norm: 20.851 20.442 4.112
2024-12-02-04:47:55-root-INFO: Loss too large (418.921->419.809)! Learning rate decreased to 0.02090.
2024-12-02-04:47:56-root-INFO: grad norm: 29.556 29.216 4.469
2024-12-02-04:47:56-root-INFO: Loss too large (417.897->424.149)! Learning rate decreased to 0.01672.
2024-12-02-04:47:57-root-INFO: Loss too large (417.897->418.440)! Learning rate decreased to 0.01338.
2024-12-02-04:47:58-root-INFO: grad norm: 31.700 31.391 4.419
2024-12-02-04:47:58-root-INFO: Loss too large (415.503->415.818)! Learning rate decreased to 0.01070.
2024-12-02-04:47:59-root-INFO: grad norm: 24.791 24.473 3.956
2024-12-02-04:48:00-root-INFO: grad norm: 17.693 17.364 3.397
2024-12-02-04:48:01-root-INFO: Loss Change: 418.921 -> 410.247
2024-12-02-04:48:01-root-INFO: Regularization Change: 0.000 -> 0.570
2024-12-02-04:48:01-root-INFO: Undo step: 135
2024-12-02-04:48:01-root-INFO: Undo step: 136
2024-12-02-04:48:01-root-INFO: Undo step: 137
2024-12-02-04:48:01-root-INFO: Undo step: 138
2024-12-02-04:48:01-root-INFO: Undo step: 139
2024-12-02-04:48:01-root-INFO: step: 140 lr_xt 0.02201089
2024-12-02-04:48:01-root-INFO: grad norm: 189.301 185.023 40.018
2024-12-02-04:48:02-root-INFO: grad norm: 105.397 103.056 22.089
2024-12-02-04:48:03-root-INFO: grad norm: 97.367 95.885 16.925
2024-12-02-04:48:04-root-INFO: grad norm: 217.310 215.004 31.574
2024-12-02-04:48:05-root-INFO: Loss too large (612.635->991.774)! Learning rate decreased to 0.01761.
2024-12-02-04:48:05-root-INFO: Loss too large (612.635->801.796)! Learning rate decreased to 0.01409.
2024-12-02-04:48:05-root-INFO: Loss too large (612.635->685.212)! Learning rate decreased to 0.01127.
2024-12-02-04:48:06-root-INFO: Loss too large (612.635->614.553)! Learning rate decreased to 0.00902.
2024-12-02-04:48:07-root-INFO: grad norm: 93.296 91.932 15.898
2024-12-02-04:48:07-root-INFO: Loss Change: 965.112 -> 517.580
2024-12-02-04:48:07-root-INFO: Regularization Change: 0.000 -> 28.258
2024-12-02-04:48:07-root-INFO: Learning rate of xt decay: 0.11010 -> 0.11142.
2024-12-02-04:48:07-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:48:08-root-INFO: step: 139 lr_xt 0.02278550
2024-12-02-04:48:08-root-INFO: grad norm: 48.209 47.317 9.229
2024-12-02-04:48:09-root-INFO: grad norm: 59.107 58.086 10.943
2024-12-02-04:48:09-root-INFO: Loss too large (497.619->511.963)! Learning rate decreased to 0.01823.
2024-12-02-04:48:10-root-INFO: grad norm: 63.495 62.658 10.270
2024-12-02-04:48:11-root-INFO: grad norm: 72.857 71.857 12.031
2024-12-02-04:48:12-root-INFO: Loss too large (487.171->496.577)! Learning rate decreased to 0.01458.
2024-12-02-04:48:13-root-INFO: grad norm: 59.196 58.495 9.084
2024-12-02-04:48:13-root-INFO: Loss Change: 516.681 -> 466.420
2024-12-02-04:48:13-root-INFO: Regularization Change: 0.000 -> 4.518
2024-12-02-04:48:13-root-INFO: Learning rate of xt decay: 0.11142 -> 0.11276.
2024-12-02-04:48:13-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:48:14-root-INFO: step: 138 lr_xt 0.02358356
2024-12-02-04:48:14-root-INFO: grad norm: 35.851 35.308 6.218
2024-12-02-04:48:14-root-INFO: Loss too large (462.399->465.883)! Learning rate decreased to 0.01887.
2024-12-02-04:48:15-root-INFO: grad norm: 42.794 42.260 6.736
2024-12-02-04:48:16-root-INFO: grad norm: 56.053 55.425 8.366
2024-12-02-04:48:17-root-INFO: Loss too large (459.084->464.890)! Learning rate decreased to 0.01509.
2024-12-02-04:48:18-root-INFO: grad norm: 46.583 46.066 6.922
2024-12-02-04:48:19-root-INFO: grad norm: 36.609 36.124 5.939
2024-12-02-04:48:19-root-INFO: Loss Change: 462.399 -> 442.847
2024-12-02-04:48:19-root-INFO: Regularization Change: 0.000 -> 1.879
2024-12-02-04:48:19-root-INFO: Learning rate of xt decay: 0.11276 -> 0.11411.
2024-12-02-04:48:19-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-04:48:20-root-INFO: step: 137 lr_xt 0.02440563
2024-12-02-04:48:20-root-INFO: grad norm: 51.302 50.546 8.772
2024-12-02-04:48:20-root-INFO: Loss too large (447.377->468.499)! Learning rate decreased to 0.01952.
2024-12-02-04:48:21-root-INFO: Loss too large (447.377->451.193)! Learning rate decreased to 0.01562.
2024-12-02-04:48:22-root-INFO: grad norm: 46.211 45.741 6.569
2024-12-02-04:48:23-root-INFO: grad norm: 41.900 41.423 6.300
2024-12-02-04:48:24-root-INFO: grad norm: 39.347 38.906 5.876
2024-12-02-04:48:25-root-INFO: grad norm: 37.068 36.640 5.613
2024-12-02-04:48:25-root-INFO: Loss Change: 447.377 -> 427.886
2024-12-02-04:48:25-root-INFO: Regularization Change: 0.000 -> 1.471
2024-12-02-04:48:25-root-INFO: Learning rate of xt decay: 0.11411 -> 0.11548.
2024-12-02-04:48:25-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-04:48:26-root-INFO: step: 136 lr_xt 0.02525230
2024-12-02-04:48:26-root-INFO: grad norm: 34.112 33.778 4.761
2024-12-02-04:48:26-root-INFO: Loss too large (426.794->435.331)! Learning rate decreased to 0.02020.
2024-12-02-04:48:27-root-INFO: Loss too large (426.794->429.237)! Learning rate decreased to 0.01616.
2024-12-02-04:48:28-root-INFO: grad norm: 32.307 31.922 4.975
2024-12-02-04:48:29-root-INFO: grad norm: 30.552 30.223 4.470
2024-12-02-04:48:30-root-INFO: grad norm: 30.055 29.692 4.658
2024-12-02-04:48:31-root-INFO: grad norm: 29.713 29.390 4.366
2024-12-02-04:48:31-root-INFO: Loss Change: 426.794 -> 416.340
2024-12-02-04:48:31-root-INFO: Regularization Change: 0.000 -> 1.086
2024-12-02-04:48:31-root-INFO: Learning rate of xt decay: 0.11548 -> 0.11687.
2024-12-02-04:48:31-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-04:48:32-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-04:48:32-root-INFO: grad norm: 36.111 35.592 6.105
2024-12-02-04:48:32-root-INFO: Loss too large (417.362->428.699)! Learning rate decreased to 0.02090.
2024-12-02-04:48:33-root-INFO: Loss too large (417.362->418.703)! Learning rate decreased to 0.01672.
2024-12-02-04:48:34-root-INFO: grad norm: 34.374 34.049 4.717
2024-12-02-04:48:35-root-INFO: grad norm: 34.287 33.916 5.036
2024-12-02-04:48:36-root-INFO: grad norm: 34.290 33.957 4.766
2024-12-02-04:48:37-root-INFO: grad norm: 34.028 33.676 4.887
2024-12-02-04:48:37-root-INFO: Loss Change: 417.362 -> 406.200
2024-12-02-04:48:37-root-INFO: Regularization Change: 0.000 -> 1.097
2024-12-02-04:48:37-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-04:48:37-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-04:48:38-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-04:48:38-root-INFO: grad norm: 31.102 30.816 4.211
2024-12-02-04:48:38-root-INFO: Loss too large (404.565->412.506)! Learning rate decreased to 0.02162.
2024-12-02-04:48:39-root-INFO: Loss too large (404.565->407.056)! Learning rate decreased to 0.01729.
2024-12-02-04:48:40-root-INFO: grad norm: 29.683 29.346 4.460
2024-12-02-04:48:41-root-INFO: grad norm: 28.705 28.434 3.935
2024-12-02-04:48:42-root-INFO: grad norm: 28.660 28.344 4.248
2024-12-02-04:48:43-root-INFO: grad norm: 28.624 28.346 3.979
2024-12-02-04:48:43-root-INFO: Loss Change: 404.565 -> 396.701
2024-12-02-04:48:43-root-INFO: Regularization Change: 0.000 -> 0.962
2024-12-02-04:48:43-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-04:48:43-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-04:48:44-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-04:48:44-root-INFO: grad norm: 36.531 36.036 5.997
2024-12-02-04:48:44-root-INFO: Loss too large (398.415->413.185)! Learning rate decreased to 0.02236.
2024-12-02-04:48:45-root-INFO: Loss too large (398.415->401.729)! Learning rate decreased to 0.01789.
2024-12-02-04:48:46-root-INFO: grad norm: 36.358 36.061 4.633
2024-12-02-04:48:47-root-INFO: grad norm: 37.461 37.116 5.074
2024-12-02-04:48:48-root-INFO: grad norm: 38.114 37.794 4.928
2024-12-02-04:48:49-root-INFO: grad norm: 38.354 38.023 5.028
2024-12-02-04:48:49-root-INFO: Loss Change: 398.415 -> 390.413
2024-12-02-04:48:49-root-INFO: Regularization Change: 0.000 -> 1.124
2024-12-02-04:48:49-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-04:48:49-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-04:48:50-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-04:48:50-root-INFO: grad norm: 35.826 35.555 4.398
2024-12-02-04:48:50-root-INFO: Loss too large (388.811->403.644)! Learning rate decreased to 0.02312.
2024-12-02-04:48:51-root-INFO: Loss too large (388.811->393.924)! Learning rate decreased to 0.01849.
2024-12-02-04:48:52-root-INFO: grad norm: 35.104 34.794 4.661
2024-12-02-04:48:53-root-INFO: grad norm: 34.498 34.219 4.382
2024-12-02-04:48:54-root-INFO: grad norm: 34.360 34.064 4.502
2024-12-02-04:48:55-root-INFO: grad norm: 34.174 33.887 4.422
2024-12-02-04:48:55-root-INFO: Loss Change: 388.811 -> 381.960
2024-12-02-04:48:55-root-INFO: Regularization Change: 0.000 -> 1.060
2024-12-02-04:48:55-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-04:48:55-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-04:48:56-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-04:48:56-root-INFO: grad norm: 42.440 41.985 6.197
2024-12-02-04:48:56-root-INFO: Loss too large (384.483->407.543)! Learning rate decreased to 0.02390.
2024-12-02-04:48:57-root-INFO: Loss too large (384.483->391.287)! Learning rate decreased to 0.01912.
2024-12-02-04:48:58-root-INFO: grad norm: 41.528 41.225 5.011
2024-12-02-04:48:59-root-INFO: grad norm: 40.320 40.002 5.060
2024-12-02-04:49:00-root-INFO: grad norm: 39.024 38.714 4.909
2024-12-02-04:49:00-root-INFO: grad norm: 37.690 37.396 4.699
2024-12-02-04:49:01-root-INFO: Loss Change: 384.483 -> 374.739
2024-12-02-04:49:01-root-INFO: Regularization Change: 0.000 -> 1.239
2024-12-02-04:49:01-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-04:49:01-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-04:49:01-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-04:49:02-root-INFO: grad norm: 35.224 34.968 4.241
2024-12-02-04:49:02-root-INFO: Loss too large (374.058->388.948)! Learning rate decreased to 0.02471.
2024-12-02-04:49:02-root-INFO: Loss too large (374.058->378.972)! Learning rate decreased to 0.01976.
2024-12-02-04:49:03-root-INFO: grad norm: 33.463 33.184 4.311
2024-12-02-04:49:04-root-INFO: grad norm: 32.120 31.861 4.073
2024-12-02-04:49:05-root-INFO: grad norm: 31.469 31.208 4.037
2024-12-02-04:49:06-root-INFO: grad norm: 30.937 30.676 4.009
2024-12-02-04:49:07-root-INFO: Loss Change: 374.058 -> 366.681
2024-12-02-04:49:07-root-INFO: Regularization Change: 0.000 -> 1.055
2024-12-02-04:49:07-root-INFO: Undo step: 130
2024-12-02-04:49:07-root-INFO: Undo step: 131
2024-12-02-04:49:07-root-INFO: Undo step: 132
2024-12-02-04:49:07-root-INFO: Undo step: 133
2024-12-02-04:49:07-root-INFO: Undo step: 134
2024-12-02-04:49:08-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-04:49:08-root-INFO: grad norm: 161.093 158.204 30.369
2024-12-02-04:49:09-root-INFO: grad norm: 112.664 110.810 20.357
2024-12-02-04:49:10-root-INFO: grad norm: 114.295 112.648 19.330
2024-12-02-04:49:10-root-INFO: Loss too large (553.443->569.574)! Learning rate decreased to 0.02090.
2024-12-02-04:49:11-root-INFO: grad norm: 88.993 86.904 19.169
2024-12-02-04:49:12-root-INFO: grad norm: 51.401 50.311 10.532
2024-12-02-04:49:13-root-INFO: Loss Change: 799.828 -> 447.321
2024-12-02-04:49:13-root-INFO: Regularization Change: 0.000 -> 28.323
2024-12-02-04:49:13-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-04:49:13-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-04:49:13-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-04:49:14-root-INFO: grad norm: 53.851 52.916 9.994
2024-12-02-04:49:15-root-INFO: grad norm: 72.597 71.708 11.327
2024-12-02-04:49:15-root-INFO: Loss too large (450.225->470.229)! Learning rate decreased to 0.02162.
2024-12-02-04:49:16-root-INFO: grad norm: 65.550 64.753 10.189
2024-12-02-04:49:17-root-INFO: grad norm: 57.347 56.639 8.981
2024-12-02-04:49:18-root-INFO: grad norm: 53.783 53.168 8.109
2024-12-02-04:49:18-root-INFO: Loss Change: 450.509 -> 415.798
2024-12-02-04:49:19-root-INFO: Regularization Change: 0.000 -> 4.942
2024-12-02-04:49:19-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-04:49:19-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-04:49:19-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-04:49:19-root-INFO: grad norm: 43.594 43.159 6.146
2024-12-02-04:49:20-root-INFO: Loss too large (410.295->417.897)! Learning rate decreased to 0.02236.
2024-12-02-04:49:21-root-INFO: grad norm: 44.429 43.891 6.897
2024-12-02-04:49:21-root-INFO: grad norm: 46.604 46.087 6.923
2024-12-02-04:49:22-root-INFO: grad norm: 48.969 48.439 7.181
2024-12-02-04:49:23-root-INFO: grad norm: 53.669 53.098 7.809
2024-12-02-04:49:24-root-INFO: Loss too large (401.134->403.177)! Learning rate decreased to 0.01789.
2024-12-02-04:49:24-root-INFO: Loss Change: 410.295 -> 393.215
2024-12-02-04:49:24-root-INFO: Regularization Change: 0.000 -> 2.385
2024-12-02-04:49:24-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-04:49:24-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-04:49:25-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-04:49:25-root-INFO: grad norm: 44.216 43.660 6.993
2024-12-02-04:49:25-root-INFO: Loss too large (395.079->406.662)! Learning rate decreased to 0.02312.
2024-12-02-04:49:27-root-INFO: grad norm: 49.898 49.438 6.756
2024-12-02-04:49:27-root-INFO: Loss too large (392.501->396.192)! Learning rate decreased to 0.01849.
2024-12-02-04:49:28-root-INFO: grad norm: 36.580 36.220 5.115
2024-12-02-04:49:29-root-INFO: grad norm: 25.465 25.186 3.760
2024-12-02-04:49:30-root-INFO: grad norm: 22.996 22.711 3.609
2024-12-02-04:49:30-root-INFO: Loss Change: 395.079 -> 375.179
2024-12-02-04:49:30-root-INFO: Regularization Change: 0.000 -> 1.425
2024-12-02-04:49:30-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-04:49:30-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-04:49:31-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-04:49:31-root-INFO: grad norm: 27.056 26.685 4.463
2024-12-02-04:49:31-root-INFO: Loss too large (375.002->381.046)! Learning rate decreased to 0.02390.
2024-12-02-04:49:32-root-INFO: Loss too large (375.002->376.622)! Learning rate decreased to 0.01912.
2024-12-02-04:49:33-root-INFO: grad norm: 26.592 26.319 3.805
2024-12-02-04:49:34-root-INFO: grad norm: 27.732 27.475 3.765
2024-12-02-04:49:35-root-INFO: grad norm: 27.543 27.274 3.840
2024-12-02-04:49:36-root-INFO: grad norm: 27.059 26.813 3.634
2024-12-02-04:49:36-root-INFO: Loss Change: 375.002 -> 367.790
2024-12-02-04:49:36-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-02-04:49:36-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-04:49:36-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-04:49:37-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-04:49:37-root-INFO: grad norm: 26.667 26.354 4.070
2024-12-02-04:49:37-root-INFO: Loss too large (367.450->374.614)! Learning rate decreased to 0.02471.
2024-12-02-04:49:38-root-INFO: Loss too large (367.450->367.630)! Learning rate decreased to 0.01976.
2024-12-02-04:49:39-root-INFO: grad norm: 26.384 26.128 3.665
2024-12-02-04:49:39-root-INFO: Loss too large (364.122->364.142)! Learning rate decreased to 0.01581.
2024-12-02-04:49:40-root-INFO: grad norm: 20.386 20.146 3.120
2024-12-02-04:49:41-root-INFO: grad norm: 14.862 14.617 2.689
2024-12-02-04:49:42-root-INFO: grad norm: 13.172 12.926 2.532
2024-12-02-04:49:43-root-INFO: Loss Change: 367.450 -> 357.138
2024-12-02-04:49:43-root-INFO: Regularization Change: 0.000 -> 0.668
2024-12-02-04:49:43-root-INFO: Undo step: 130
2024-12-02-04:49:43-root-INFO: Undo step: 131
2024-12-02-04:49:43-root-INFO: Undo step: 132
2024-12-02-04:49:43-root-INFO: Undo step: 133
2024-12-02-04:49:43-root-INFO: Undo step: 134
2024-12-02-04:49:43-root-INFO: step: 135 lr_xt 0.02612413
2024-12-02-04:49:44-root-INFO: grad norm: 153.371 149.864 32.610
2024-12-02-04:49:45-root-INFO: grad norm: 92.137 90.146 19.050
2024-12-02-04:49:46-root-INFO: grad norm: 86.391 84.734 16.838
2024-12-02-04:49:47-root-INFO: grad norm: 95.482 94.252 15.281
2024-12-02-04:49:47-root-INFO: Loss too large (501.746->516.780)! Learning rate decreased to 0.02090.
2024-12-02-04:49:48-root-INFO: grad norm: 76.212 75.033 13.358
2024-12-02-04:49:48-root-INFO: Loss Change: 846.456 -> 450.108
2024-12-02-04:49:49-root-INFO: Regularization Change: 0.000 -> 34.123
2024-12-02-04:49:49-root-INFO: Learning rate of xt decay: 0.11687 -> 0.11827.
2024-12-02-04:49:49-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-04:49:49-root-INFO: step: 134 lr_xt 0.02702170
2024-12-02-04:49:49-root-INFO: grad norm: 68.465 67.488 11.523
2024-12-02-04:49:49-root-INFO: Loss too large (455.453->465.120)! Learning rate decreased to 0.02162.
2024-12-02-04:49:50-root-INFO: grad norm: 59.773 58.887 10.258
2024-12-02-04:49:51-root-INFO: grad norm: 52.330 51.590 8.766
2024-12-02-04:49:52-root-INFO: grad norm: 49.780 49.023 8.651
2024-12-02-04:49:53-root-INFO: grad norm: 46.302 45.633 7.843
2024-12-02-04:49:54-root-INFO: Loss Change: 455.453 -> 409.662
2024-12-02-04:49:54-root-INFO: Regularization Change: 0.000 -> 4.660
2024-12-02-04:49:54-root-INFO: Learning rate of xt decay: 0.11827 -> 0.11969.
2024-12-02-04:49:54-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-04:49:54-root-INFO: step: 133 lr_xt 0.02794561
2024-12-02-04:49:55-root-INFO: grad norm: 37.453 36.998 5.820
2024-12-02-04:49:55-root-INFO: Loss too large (405.557->409.812)! Learning rate decreased to 0.02236.
2024-12-02-04:49:56-root-INFO: grad norm: 38.387 37.819 6.577
2024-12-02-04:49:57-root-INFO: grad norm: 42.876 42.324 6.860
2024-12-02-04:49:57-root-INFO: Loss too large (398.679->399.225)! Learning rate decreased to 0.01789.
2024-12-02-04:49:58-root-INFO: grad norm: 30.758 30.234 5.653
2024-12-02-04:49:59-root-INFO: grad norm: 20.153 19.751 4.005
2024-12-02-04:50:00-root-INFO: Loss Change: 405.557 -> 383.737
2024-12-02-04:50:00-root-INFO: Regularization Change: 0.000 -> 1.854
2024-12-02-04:50:00-root-INFO: Learning rate of xt decay: 0.11969 -> 0.12113.
2024-12-02-04:50:00-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-04:50:01-root-INFO: step: 132 lr_xt 0.02889645
2024-12-02-04:50:01-root-INFO: grad norm: 23.416 22.829 5.211
2024-12-02-04:50:01-root-INFO: Loss too large (383.290->384.130)! Learning rate decreased to 0.02312.
2024-12-02-04:50:02-root-INFO: grad norm: 30.124 29.680 5.153
2024-12-02-04:50:03-root-INFO: Loss too large (381.137->382.106)! Learning rate decreased to 0.01849.
2024-12-02-04:50:04-root-INFO: grad norm: 25.826 25.349 4.941
2024-12-02-04:50:04-root-INFO: grad norm: 22.004 21.637 4.005
2024-12-02-04:50:05-root-INFO: grad norm: 21.666 21.224 4.355
2024-12-02-04:50:06-root-INFO: Loss Change: 383.290 -> 370.669
2024-12-02-04:50:06-root-INFO: Regularization Change: 0.000 -> 1.263
2024-12-02-04:50:06-root-INFO: Learning rate of xt decay: 0.12113 -> 0.12258.
2024-12-02-04:50:06-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-04:50:06-root-INFO: step: 131 lr_xt 0.02987484
2024-12-02-04:50:07-root-INFO: grad norm: 27.084 26.626 4.958
2024-12-02-04:50:07-root-INFO: Loss too large (370.707->375.788)! Learning rate decreased to 0.02390.
2024-12-02-04:50:08-root-INFO: Loss too large (370.707->372.125)! Learning rate decreased to 0.01912.
2024-12-02-04:50:09-root-INFO: grad norm: 25.691 25.246 4.760
2024-12-02-04:50:09-root-INFO: grad norm: 25.192 24.845 4.167
2024-12-02-04:50:11-root-INFO: grad norm: 24.501 24.069 4.581
2024-12-02-04:50:12-root-INFO: grad norm: 23.198 22.873 3.872
2024-12-02-04:50:12-root-INFO: Loss Change: 370.707 -> 361.635
2024-12-02-04:50:12-root-INFO: Regularization Change: 0.000 -> 1.118
2024-12-02-04:50:12-root-INFO: Learning rate of xt decay: 0.12258 -> 0.12405.
2024-12-02-04:50:12-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-04:50:13-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-04:50:13-root-INFO: grad norm: 22.826 22.362 4.580
2024-12-02-04:50:13-root-INFO: Loss too large (360.914->364.873)! Learning rate decreased to 0.02471.
2024-12-02-04:50:14-root-INFO: grad norm: 36.944 36.476 5.863
2024-12-02-04:50:15-root-INFO: Loss too large (359.929->364.767)! Learning rate decreased to 0.01976.
2024-12-02-04:50:15-root-INFO: Loss too large (359.929->361.263)! Learning rate decreased to 0.01581.
2024-12-02-04:50:16-root-INFO: grad norm: 23.348 22.967 4.203
2024-12-02-04:50:17-root-INFO: grad norm: 10.081 9.764 2.510
2024-12-02-04:50:18-root-INFO: grad norm: 9.364 9.015 2.532
2024-12-02-04:50:18-root-INFO: Loss Change: 360.914 -> 350.490
2024-12-02-04:50:18-root-INFO: Regularization Change: 0.000 -> 0.730
2024-12-02-04:50:18-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-04:50:18-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-04:50:19-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-04:50:19-root-INFO: grad norm: 17.786 17.272 4.243
2024-12-02-04:50:20-root-INFO: grad norm: 26.622 26.223 4.589
2024-12-02-04:50:20-root-INFO: Loss too large (350.658->357.951)! Learning rate decreased to 0.02553.
2024-12-02-04:50:21-root-INFO: Loss too large (350.658->351.512)! Learning rate decreased to 0.02043.
2024-12-02-04:50:22-root-INFO: grad norm: 25.539 25.229 3.962
2024-12-02-04:50:23-root-INFO: grad norm: 26.274 25.905 4.392
2024-12-02-04:50:24-root-INFO: grad norm: 30.911 30.559 4.656
2024-12-02-04:50:24-root-INFO: Loss too large (346.061->347.397)! Learning rate decreased to 0.01634.
2024-12-02-04:50:25-root-INFO: Loss Change: 351.199 -> 344.728
2024-12-02-04:50:25-root-INFO: Regularization Change: 0.000 -> 1.038
2024-12-02-04:50:25-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-04:50:25-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-04:50:25-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-04:50:25-root-INFO: grad norm: 23.177 22.758 4.388
2024-12-02-04:50:26-root-INFO: Loss too large (343.948->348.881)! Learning rate decreased to 0.02639.
2024-12-02-04:50:27-root-INFO: grad norm: 36.910 36.517 5.370
2024-12-02-04:50:27-root-INFO: Loss too large (343.349->348.465)! Learning rate decreased to 0.02111.
2024-12-02-04:50:27-root-INFO: Loss too large (343.349->344.810)! Learning rate decreased to 0.01689.
2024-12-02-04:50:28-root-INFO: grad norm: 22.792 22.482 3.746
2024-12-02-04:50:29-root-INFO: grad norm: 9.153 8.880 2.221
2024-12-02-04:50:30-root-INFO: grad norm: 8.465 8.168 2.222
2024-12-02-04:50:31-root-INFO: Loss Change: 343.948 -> 334.233
2024-12-02-04:50:31-root-INFO: Regularization Change: 0.000 -> 0.683
2024-12-02-04:50:31-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-04:50:31-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-04:50:31-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-04:50:32-root-INFO: grad norm: 13.298 12.916 3.168
2024-12-02-04:50:33-root-INFO: grad norm: 21.042 20.735 3.584
2024-12-02-04:50:33-root-INFO: Loss too large (333.558->340.021)! Learning rate decreased to 0.02726.
2024-12-02-04:50:33-root-INFO: Loss too large (333.558->335.051)! Learning rate decreased to 0.02181.
2024-12-02-04:50:34-root-INFO: grad norm: 24.239 23.963 3.653
2024-12-02-04:50:35-root-INFO: Loss too large (332.433->332.909)! Learning rate decreased to 0.01745.
2024-12-02-04:50:36-root-INFO: grad norm: 19.593 19.319 3.262
2024-12-02-04:50:37-root-INFO: grad norm: 15.802 15.568 2.706
2024-12-02-04:50:37-root-INFO: Loss Change: 334.353 -> 328.015
2024-12-02-04:50:37-root-INFO: Regularization Change: 0.000 -> 0.753
2024-12-02-04:50:37-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-04:50:37-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-04:50:38-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-04:50:38-root-INFO: grad norm: 16.499 16.157 3.343
2024-12-02-04:50:38-root-INFO: Loss too large (328.328->331.375)! Learning rate decreased to 0.02816.
2024-12-02-04:50:39-root-INFO: Loss too large (328.328->328.542)! Learning rate decreased to 0.02253.
2024-12-02-04:50:40-root-INFO: grad norm: 20.301 20.055 3.151
2024-12-02-04:50:40-root-INFO: Loss too large (327.076->327.534)! Learning rate decreased to 0.01802.
2024-12-02-04:50:41-root-INFO: grad norm: 17.638 17.397 2.910
2024-12-02-04:50:42-root-INFO: grad norm: 14.605 14.383 2.534
2024-12-02-04:50:43-root-INFO: grad norm: 13.932 13.703 2.516
2024-12-02-04:50:44-root-INFO: Loss Change: 328.328 -> 322.316
2024-12-02-04:50:44-root-INFO: Regularization Change: 0.000 -> 0.557
2024-12-02-04:50:44-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-04:50:44-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-04:50:44-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-04:50:44-root-INFO: grad norm: 18.077 17.673 3.801
2024-12-02-04:50:45-root-INFO: Loss too large (322.788->326.090)! Learning rate decreased to 0.02909.
2024-12-02-04:50:45-root-INFO: Loss too large (322.788->323.373)! Learning rate decreased to 0.02327.
2024-12-02-04:50:46-root-INFO: grad norm: 19.599 19.353 3.094
2024-12-02-04:50:47-root-INFO: grad norm: 26.040 25.777 3.686
2024-12-02-04:50:47-root-INFO: Loss too large (321.339->322.906)! Learning rate decreased to 0.01862.
2024-12-02-04:50:48-root-INFO: grad norm: 21.103 20.860 3.191
2024-12-02-04:50:49-root-INFO: grad norm: 15.270 15.060 2.521
2024-12-02-04:50:50-root-INFO: Loss Change: 322.788 -> 316.829
2024-12-02-04:50:50-root-INFO: Regularization Change: 0.000 -> 0.636
2024-12-02-04:50:50-root-INFO: Undo step: 125
2024-12-02-04:50:50-root-INFO: Undo step: 126
2024-12-02-04:50:50-root-INFO: Undo step: 127
2024-12-02-04:50:50-root-INFO: Undo step: 128
2024-12-02-04:50:50-root-INFO: Undo step: 129
2024-12-02-04:50:50-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-04:50:51-root-INFO: grad norm: 166.235 163.460 30.244
2024-12-02-04:50:52-root-INFO: grad norm: 117.638 115.939 19.919
2024-12-02-04:50:53-root-INFO: grad norm: 87.302 85.270 18.727
2024-12-02-04:50:54-root-INFO: grad norm: 78.782 77.622 13.470
2024-12-02-04:50:55-root-INFO: grad norm: 75.373 73.661 15.973
2024-12-02-04:50:55-root-INFO: Loss Change: 735.383 -> 426.236
2024-12-02-04:50:55-root-INFO: Regularization Change: 0.000 -> 41.624
2024-12-02-04:50:55-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-04:50:55-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-04:50:56-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-04:50:56-root-INFO: grad norm: 81.454 79.948 15.589
2024-12-02-04:50:57-root-INFO: grad norm: 75.917 74.204 16.038
2024-12-02-04:50:58-root-INFO: grad norm: 70.833 69.652 12.881
2024-12-02-04:50:59-root-INFO: grad norm: 71.163 69.738 14.167
2024-12-02-04:51:00-root-INFO: grad norm: 69.791 68.736 12.090
2024-12-02-04:51:00-root-INFO: Loss Change: 436.781 -> 387.136
2024-12-02-04:51:00-root-INFO: Regularization Change: 0.000 -> 10.813
2024-12-02-04:51:00-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-04:51:00-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-04:51:01-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-04:51:01-root-INFO: grad norm: 61.028 60.076 10.736
2024-12-02-04:51:02-root-INFO: grad norm: 61.742 60.805 10.716
2024-12-02-04:51:03-root-INFO: grad norm: 63.113 61.993 11.837
2024-12-02-04:51:03-root-INFO: Loss too large (373.220->373.371)! Learning rate decreased to 0.02639.
2024-12-02-04:51:04-root-INFO: grad norm: 40.171 39.598 6.759
2024-12-02-04:51:05-root-INFO: grad norm: 26.029 25.501 5.213
2024-12-02-04:51:06-root-INFO: Loss Change: 378.880 -> 336.480
2024-12-02-04:51:06-root-INFO: Regularization Change: 0.000 -> 3.707
2024-12-02-04:51:06-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-04:51:06-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-04:51:07-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-04:51:07-root-INFO: grad norm: 27.179 26.523 5.936
2024-12-02-04:51:07-root-INFO: Loss too large (337.865->338.770)! Learning rate decreased to 0.02726.
2024-12-02-04:51:08-root-INFO: grad norm: 26.109 25.704 4.580
2024-12-02-04:51:09-root-INFO: grad norm: 25.081 24.698 4.367
2024-12-02-04:51:10-root-INFO: grad norm: 26.825 26.493 4.204
2024-12-02-04:51:10-root-INFO: Loss too large (329.645->330.433)! Learning rate decreased to 0.02181.
2024-12-02-04:51:11-root-INFO: grad norm: 20.664 20.356 3.555
2024-12-02-04:51:12-root-INFO: Loss Change: 337.865 -> 323.991
2024-12-02-04:51:12-root-INFO: Regularization Change: 0.000 -> 1.437
2024-12-02-04:51:12-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-04:51:12-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-04:51:12-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-04:51:13-root-INFO: grad norm: 15.547 15.173 3.390
2024-12-02-04:51:13-root-INFO: Loss too large (323.797->323.798)! Learning rate decreased to 0.02816.
2024-12-02-04:51:14-root-INFO: grad norm: 17.985 17.689 3.249
2024-12-02-04:51:15-root-INFO: grad norm: 26.775 26.510 3.757
2024-12-02-04:51:15-root-INFO: Loss too large (321.923->324.719)! Learning rate decreased to 0.02253.
2024-12-02-04:51:16-root-INFO: grad norm: 22.013 21.729 3.523
2024-12-02-04:51:17-root-INFO: grad norm: 14.519 14.274 2.660
2024-12-02-04:51:18-root-INFO: Loss Change: 323.797 -> 316.110
2024-12-02-04:51:18-root-INFO: Regularization Change: 0.000 -> 1.035
2024-12-02-04:51:18-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-04:51:18-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-04:51:18-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-04:51:19-root-INFO: grad norm: 16.991 16.570 3.761
2024-12-02-04:51:19-root-INFO: Loss too large (316.071->317.397)! Learning rate decreased to 0.02909.
2024-12-02-04:51:20-root-INFO: grad norm: 23.453 23.208 3.384
2024-12-02-04:51:20-root-INFO: Loss too large (314.989->316.883)! Learning rate decreased to 0.02327.
2024-12-02-04:51:21-root-INFO: grad norm: 20.167 19.903 3.252
2024-12-02-04:51:22-root-INFO: grad norm: 15.789 15.568 2.630
2024-12-02-04:51:23-root-INFO: grad norm: 15.783 15.534 2.794
2024-12-02-04:51:24-root-INFO: Loss Change: 316.071 -> 308.820
2024-12-02-04:51:24-root-INFO: Regularization Change: 0.000 -> 0.918
2024-12-02-04:51:24-root-INFO: Undo step: 125
2024-12-02-04:51:24-root-INFO: Undo step: 126
2024-12-02-04:51:24-root-INFO: Undo step: 127
2024-12-02-04:51:24-root-INFO: Undo step: 128
2024-12-02-04:51:24-root-INFO: Undo step: 129
2024-12-02-04:51:24-root-INFO: step: 130 lr_xt 0.03088137
2024-12-02-04:51:25-root-INFO: grad norm: 135.297 131.950 29.908
2024-12-02-04:51:26-root-INFO: grad norm: 84.156 82.808 14.999
2024-12-02-04:51:27-root-INFO: grad norm: 74.052 72.087 16.950
2024-12-02-04:51:28-root-INFO: grad norm: 64.761 63.609 12.156
2024-12-02-04:51:29-root-INFO: grad norm: 61.350 59.771 13.830
2024-12-02-04:51:29-root-INFO: Loss Change: 718.418 -> 394.391
2024-12-02-04:51:29-root-INFO: Regularization Change: 0.000 -> 36.867
2024-12-02-04:51:29-root-INFO: Learning rate of xt decay: 0.12405 -> 0.12554.
2024-12-02-04:51:29-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-04:51:30-root-INFO: step: 129 lr_xt 0.03191668
2024-12-02-04:51:30-root-INFO: grad norm: 64.902 63.707 12.399
2024-12-02-04:51:31-root-INFO: grad norm: 65.550 64.210 13.188
2024-12-02-04:51:32-root-INFO: grad norm: 60.716 59.852 10.206
2024-12-02-04:51:33-root-INFO: grad norm: 56.074 54.749 12.118
2024-12-02-04:51:34-root-INFO: grad norm: 55.721 54.797 10.103
2024-12-02-04:51:34-root-INFO: Loss too large (367.950->369.474)! Learning rate decreased to 0.02553.
2024-12-02-04:51:35-root-INFO: Loss Change: 398.907 -> 353.507
2024-12-02-04:51:35-root-INFO: Regularization Change: 0.000 -> 6.491
2024-12-02-04:51:35-root-INFO: Learning rate of xt decay: 0.12554 -> 0.12705.
2024-12-02-04:51:35-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-04:51:35-root-INFO: step: 128 lr_xt 0.03298138
2024-12-02-04:51:35-root-INFO: grad norm: 36.841 36.354 5.965
2024-12-02-04:51:36-root-INFO: Loss too large (348.810->350.799)! Learning rate decreased to 0.02639.
2024-12-02-04:51:37-root-INFO: grad norm: 30.266 29.900 4.691
2024-12-02-04:51:38-root-INFO: grad norm: 27.471 27.052 4.777
2024-12-02-04:51:39-root-INFO: grad norm: 25.684 25.395 3.845
2024-12-02-04:51:40-root-INFO: grad norm: 24.028 23.664 4.169
2024-12-02-04:51:40-root-INFO: Loss Change: 348.810 -> 330.883
2024-12-02-04:51:40-root-INFO: Regularization Change: 0.000 -> 2.402
2024-12-02-04:51:40-root-INFO: Learning rate of xt decay: 0.12705 -> 0.12857.
2024-12-02-04:51:40-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-04:51:41-root-INFO: step: 127 lr_xt 0.03407612
2024-12-02-04:51:41-root-INFO: grad norm: 27.596 27.138 5.002
2024-12-02-04:51:41-root-INFO: Loss too large (331.523->332.694)! Learning rate decreased to 0.02726.
2024-12-02-04:51:42-root-INFO: grad norm: 27.635 27.256 4.564
2024-12-02-04:51:43-root-INFO: grad norm: 26.211 25.912 3.945
2024-12-02-04:51:44-root-INFO: grad norm: 23.829 23.452 4.225
2024-12-02-04:51:45-root-INFO: grad norm: 24.388 24.118 3.620
2024-12-02-04:51:46-root-INFO: Loss Change: 331.523 -> 319.243
2024-12-02-04:51:46-root-INFO: Regularization Change: 0.000 -> 1.835
2024-12-02-04:51:46-root-INFO: Learning rate of xt decay: 0.12857 -> 0.13011.
2024-12-02-04:51:46-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-04:51:46-root-INFO: step: 126 lr_xt 0.03520152
2024-12-02-04:51:47-root-INFO: grad norm: 23.408 23.192 3.174
2024-12-02-04:51:47-root-INFO: Loss too large (317.930->321.274)! Learning rate decreased to 0.02816.
2024-12-02-04:51:47-root-INFO: Loss too large (317.930->318.066)! Learning rate decreased to 0.02253.
2024-12-02-04:51:48-root-INFO: grad norm: 18.616 18.406 2.789
2024-12-02-04:51:49-root-INFO: grad norm: 14.348 14.116 2.570
2024-12-02-04:51:50-root-INFO: grad norm: 13.700 13.497 2.353
2024-12-02-04:51:51-root-INFO: grad norm: 13.601 13.388 2.398
2024-12-02-04:51:52-root-INFO: Loss Change: 317.930 -> 309.305
2024-12-02-04:51:52-root-INFO: Regularization Change: 0.000 -> 0.911
2024-12-02-04:51:52-root-INFO: Learning rate of xt decay: 0.13011 -> 0.13168.
2024-12-02-04:51:52-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-04:51:52-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-04:51:53-root-INFO: grad norm: 17.559 17.188 3.591
2024-12-02-04:51:53-root-INFO: Loss too large (309.401->310.860)! Learning rate decreased to 0.02909.
2024-12-02-04:51:54-root-INFO: grad norm: 23.452 23.235 3.184
2024-12-02-04:51:54-root-INFO: Loss too large (308.346->309.845)! Learning rate decreased to 0.02327.
2024-12-02-04:51:55-root-INFO: grad norm: 19.496 19.310 2.687
2024-12-02-04:51:56-root-INFO: grad norm: 14.611 14.408 2.426
2024-12-02-04:51:57-root-INFO: grad norm: 14.319 14.142 2.244
2024-12-02-04:51:58-root-INFO: Loss Change: 309.401 -> 301.945
2024-12-02-04:51:58-root-INFO: Regularization Change: 0.000 -> 0.898
2024-12-02-04:51:58-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-04:51:58-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-04:51:58-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-04:51:59-root-INFO: grad norm: 20.534 20.122 4.092
2024-12-02-04:51:59-root-INFO: Loss too large (302.921->305.924)! Learning rate decreased to 0.03019.
2024-12-02-04:52:00-root-INFO: grad norm: 23.368 23.131 3.318
2024-12-02-04:52:01-root-INFO: grad norm: 31.509 31.266 3.910
2024-12-02-04:52:01-root-INFO: Loss too large (302.489->306.512)! Learning rate decreased to 0.02415.
2024-12-02-04:52:02-root-INFO: grad norm: 24.501 24.267 3.378
2024-12-02-04:52:03-root-INFO: grad norm: 15.663 15.459 2.518
2024-12-02-04:52:04-root-INFO: Loss Change: 302.921 -> 295.349
2024-12-02-04:52:04-root-INFO: Regularization Change: 0.000 -> 1.008
2024-12-02-04:52:04-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-04:52:04-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-04:52:04-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-04:52:05-root-INFO: grad norm: 13.223 12.993 2.455
2024-12-02-04:52:05-root-INFO: Loss too large (295.066->295.699)! Learning rate decreased to 0.03117.
2024-12-02-04:52:06-root-INFO: grad norm: 18.657 18.482 2.552
2024-12-02-04:52:06-root-INFO: Loss too large (294.261->295.565)! Learning rate decreased to 0.02494.
2024-12-02-04:52:07-root-INFO: grad norm: 17.532 17.363 2.430
2024-12-02-04:52:08-root-INFO: grad norm: 16.020 15.849 2.333
2024-12-02-04:52:09-root-INFO: grad norm: 15.829 15.665 2.273
2024-12-02-04:52:10-root-INFO: Loss Change: 295.066 -> 289.791
2024-12-02-04:52:10-root-INFO: Regularization Change: 0.000 -> 0.806
2024-12-02-04:52:10-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-04:52:10-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-04:52:11-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-04:52:11-root-INFO: grad norm: 19.609 19.339 3.246
2024-12-02-04:52:11-root-INFO: Loss too large (290.586->295.134)! Learning rate decreased to 0.03218.
2024-12-02-04:52:12-root-INFO: Loss too large (290.586->291.750)! Learning rate decreased to 0.02574.
2024-12-02-04:52:13-root-INFO: grad norm: 18.342 18.168 2.520
2024-12-02-04:52:14-root-INFO: grad norm: 17.606 17.425 2.517
2024-12-02-04:52:15-root-INFO: grad norm: 17.220 17.052 2.400
2024-12-02-04:52:16-root-INFO: grad norm: 16.890 16.725 2.361
2024-12-02-04:52:16-root-INFO: Loss Change: 290.586 -> 285.507
2024-12-02-04:52:16-root-INFO: Regularization Change: 0.000 -> 0.822
2024-12-02-04:52:16-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-04:52:16-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-04:52:17-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-04:52:17-root-INFO: grad norm: 16.736 16.540 2.552
2024-12-02-04:52:17-root-INFO: Loss too large (285.139->288.370)! Learning rate decreased to 0.03321.
2024-12-02-04:52:18-root-INFO: grad norm: 24.284 24.124 2.782
2024-12-02-04:52:19-root-INFO: Loss too large (285.091->287.881)! Learning rate decreased to 0.02657.
2024-12-02-04:52:20-root-INFO: grad norm: 20.114 19.958 2.501
2024-12-02-04:52:21-root-INFO: grad norm: 14.741 14.583 2.154
2024-12-02-04:52:22-root-INFO: grad norm: 14.724 14.573 2.099
2024-12-02-04:52:22-root-INFO: Loss Change: 285.139 -> 279.511
2024-12-02-04:52:22-root-INFO: Regularization Change: 0.000 -> 0.824
2024-12-02-04:52:22-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-04:52:22-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-04:52:23-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-04:52:23-root-INFO: grad norm: 18.677 18.428 3.040
2024-12-02-04:52:23-root-INFO: Loss too large (280.323->285.319)! Learning rate decreased to 0.03427.
2024-12-02-04:52:24-root-INFO: Loss too large (280.323->281.667)! Learning rate decreased to 0.02742.
2024-12-02-04:52:25-root-INFO: grad norm: 18.133 17.978 2.365
2024-12-02-04:52:26-root-INFO: grad norm: 18.391 18.225 2.469
2024-12-02-04:52:27-root-INFO: grad norm: 18.379 18.226 2.367
2024-12-02-04:52:28-root-INFO: grad norm: 18.440 18.285 2.388
2024-12-02-04:52:28-root-INFO: Loss Change: 280.323 -> 276.336
2024-12-02-04:52:28-root-INFO: Regularization Change: 0.000 -> 0.841
2024-12-02-04:52:28-root-INFO: Undo step: 120
2024-12-02-04:52:28-root-INFO: Undo step: 121
2024-12-02-04:52:28-root-INFO: Undo step: 122
2024-12-02-04:52:28-root-INFO: Undo step: 123
2024-12-02-04:52:28-root-INFO: Undo step: 124
2024-12-02-04:52:29-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-04:52:29-root-INFO: grad norm: 106.382 104.228 21.304
2024-12-02-04:52:30-root-INFO: grad norm: 69.379 68.357 11.867
2024-12-02-04:52:31-root-INFO: grad norm: 62.580 61.515 11.495
2024-12-02-04:52:32-root-INFO: grad norm: 59.969 59.411 8.165
2024-12-02-04:52:33-root-INFO: grad norm: 61.146 60.231 10.542
2024-12-02-04:52:34-root-INFO: Loss Change: 628.869 -> 364.830
2024-12-02-04:52:34-root-INFO: Regularization Change: 0.000 -> 38.696
2024-12-02-04:52:34-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-04:52:34-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-04:52:34-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-04:52:34-root-INFO: grad norm: 74.637 73.727 11.624
2024-12-02-04:52:35-root-INFO: Loss too large (376.127->384.012)! Learning rate decreased to 0.03019.
2024-12-02-04:52:36-root-INFO: grad norm: 53.371 52.546 9.348
2024-12-02-04:52:37-root-INFO: grad norm: 39.650 39.204 5.928
2024-12-02-04:52:38-root-INFO: grad norm: 36.132 35.656 5.845
2024-12-02-04:52:39-root-INFO: grad norm: 35.154 34.807 4.932
2024-12-02-04:52:39-root-INFO: Loss Change: 376.127 -> 317.799
2024-12-02-04:52:39-root-INFO: Regularization Change: 0.000 -> 6.191
2024-12-02-04:52:39-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-04:52:39-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-04:52:40-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-04:52:40-root-INFO: grad norm: 31.315 31.028 4.231
2024-12-02-04:52:40-root-INFO: Loss too large (315.835->318.790)! Learning rate decreased to 0.03117.
2024-12-02-04:52:41-root-INFO: grad norm: 31.046 30.754 4.246
2024-12-02-04:52:42-root-INFO: grad norm: 31.072 30.750 4.464
2024-12-02-04:52:43-root-INFO: grad norm: 31.293 31.001 4.270
2024-12-02-04:52:44-root-INFO: Loss too large (306.059->306.191)! Learning rate decreased to 0.02494.
2024-12-02-04:52:45-root-INFO: grad norm: 22.686 22.416 3.490
2024-12-02-04:52:45-root-INFO: Loss Change: 315.835 -> 297.716
2024-12-02-04:52:45-root-INFO: Regularization Change: 0.000 -> 2.115
2024-12-02-04:52:45-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-04:52:45-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-04:52:46-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-04:52:46-root-INFO: grad norm: 21.497 21.152 3.834
2024-12-02-04:52:46-root-INFO: Loss too large (298.788->301.552)! Learning rate decreased to 0.03218.
2024-12-02-04:52:47-root-INFO: grad norm: 23.672 23.412 3.498
2024-12-02-04:52:48-root-INFO: grad norm: 27.829 27.571 3.781
2024-12-02-04:52:49-root-INFO: Loss too large (296.875->298.068)! Learning rate decreased to 0.02574.
2024-12-02-04:52:50-root-INFO: grad norm: 21.873 21.641 3.172
2024-12-02-04:52:51-root-INFO: grad norm: 16.956 16.769 2.509
2024-12-02-04:52:51-root-INFO: Loss Change: 298.788 -> 289.197
2024-12-02-04:52:51-root-INFO: Regularization Change: 0.000 -> 1.339
2024-12-02-04:52:51-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-04:52:51-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-04:52:52-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-04:52:52-root-INFO: grad norm: 13.805 13.592 2.419
2024-12-02-04:52:52-root-INFO: Loss too large (288.459->288.631)! Learning rate decreased to 0.03321.
2024-12-02-04:52:53-root-INFO: grad norm: 16.956 16.805 2.253
2024-12-02-04:52:54-root-INFO: Loss too large (287.227->287.270)! Learning rate decreased to 0.02657.
2024-12-02-04:52:55-root-INFO: grad norm: 15.041 14.871 2.255
2024-12-02-04:52:56-root-INFO: grad norm: 13.262 13.109 2.011
2024-12-02-04:52:57-root-INFO: grad norm: 12.413 12.248 2.020
2024-12-02-04:52:57-root-INFO: Loss Change: 288.459 -> 281.726
2024-12-02-04:52:57-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-04:52:57-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-04:52:57-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-04:52:58-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-04:52:58-root-INFO: grad norm: 15.031 14.770 2.790
2024-12-02-04:52:58-root-INFO: Loss too large (282.236->283.626)! Learning rate decreased to 0.03427.
2024-12-02-04:52:59-root-INFO: grad norm: 18.238 18.055 2.574
2024-12-02-04:53:00-root-INFO: Loss too large (281.716->281.928)! Learning rate decreased to 0.02742.
2024-12-02-04:53:01-root-INFO: grad norm: 16.385 16.224 2.293
2024-12-02-04:53:02-root-INFO: grad norm: 15.273 15.113 2.205
2024-12-02-04:53:03-root-INFO: grad norm: 14.345 14.199 2.044
2024-12-02-04:53:03-root-INFO: Loss Change: 282.236 -> 276.598
2024-12-02-04:53:03-root-INFO: Regularization Change: 0.000 -> 0.926
2024-12-02-04:53:03-root-INFO: Undo step: 120
2024-12-02-04:53:03-root-INFO: Undo step: 121
2024-12-02-04:53:03-root-INFO: Undo step: 122
2024-12-02-04:53:03-root-INFO: Undo step: 123
2024-12-02-04:53:03-root-INFO: Undo step: 124
2024-12-02-04:53:04-root-INFO: step: 125 lr_xt 0.03635823
2024-12-02-04:53:04-root-INFO: grad norm: 121.599 120.246 18.091
2024-12-02-04:53:05-root-INFO: grad norm: 66.770 65.858 10.996
2024-12-02-04:53:06-root-INFO: grad norm: 73.018 72.453 9.069
2024-12-02-04:53:07-root-INFO: grad norm: 71.929 71.344 9.159
2024-12-02-04:53:08-root-INFO: grad norm: 56.955 56.542 6.843
2024-12-02-04:53:09-root-INFO: Loss Change: 681.403 -> 369.511
2024-12-02-04:53:09-root-INFO: Regularization Change: 0.000 -> 38.950
2024-12-02-04:53:09-root-INFO: Learning rate of xt decay: 0.13168 -> 0.13326.
2024-12-02-04:53:09-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-04:53:09-root-INFO: step: 124 lr_xt 0.03773645
2024-12-02-04:53:09-root-INFO: grad norm: 51.846 51.464 6.282
2024-12-02-04:53:10-root-INFO: grad norm: 60.145 59.851 5.938
2024-12-02-04:53:11-root-INFO: Loss too large (358.911->367.286)! Learning rate decreased to 0.03019.
2024-12-02-04:53:12-root-INFO: grad norm: 45.096 44.679 6.117
2024-12-02-04:53:13-root-INFO: grad norm: 34.617 34.358 4.231
2024-12-02-04:53:14-root-INFO: grad norm: 32.388 32.046 4.697
2024-12-02-04:53:14-root-INFO: Loss Change: 365.789 -> 319.124
2024-12-02-04:53:14-root-INFO: Regularization Change: 0.000 -> 5.976
2024-12-02-04:53:14-root-INFO: Learning rate of xt decay: 0.13326 -> 0.13485.
2024-12-02-04:53:14-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-04:53:15-root-INFO: step: 123 lr_xt 0.03896235
2024-12-02-04:53:15-root-INFO: grad norm: 36.126 35.791 4.905
2024-12-02-04:53:15-root-INFO: Loss too large (320.522->330.381)! Learning rate decreased to 0.03117.
2024-12-02-04:53:16-root-INFO: Loss too large (320.522->320.716)! Learning rate decreased to 0.02494.
2024-12-02-04:53:17-root-INFO: grad norm: 26.736 26.454 3.876
2024-12-02-04:53:18-root-INFO: grad norm: 19.090 18.856 2.975
2024-12-02-04:53:19-root-INFO: grad norm: 16.513 16.248 2.950
2024-12-02-04:53:20-root-INFO: grad norm: 14.684 14.458 2.566
2024-12-02-04:53:20-root-INFO: Loss Change: 320.522 -> 301.204
2024-12-02-04:53:20-root-INFO: Regularization Change: 0.000 -> 1.878
2024-12-02-04:53:20-root-INFO: Learning rate of xt decay: 0.13485 -> 0.13647.
2024-12-02-04:53:20-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-04:53:21-root-INFO: step: 122 lr_xt 0.04022160
2024-12-02-04:53:21-root-INFO: grad norm: 13.434 13.123 2.874
2024-12-02-04:53:22-root-INFO: grad norm: 20.261 20.089 2.632
2024-12-02-04:53:22-root-INFO: Loss too large (298.820->301.282)! Learning rate decreased to 0.03218.
2024-12-02-04:53:23-root-INFO: grad norm: 21.947 21.744 2.976
2024-12-02-04:53:24-root-INFO: grad norm: 25.103 24.937 2.881
2024-12-02-04:53:25-root-INFO: Loss too large (295.915->297.263)! Learning rate decreased to 0.02574.
2024-12-02-04:53:26-root-INFO: grad norm: 21.136 20.947 2.826
2024-12-02-04:53:26-root-INFO: Loss Change: 300.734 -> 290.839
2024-12-02-04:53:26-root-INFO: Regularization Change: 0.000 -> 1.785
2024-12-02-04:53:26-root-INFO: Learning rate of xt decay: 0.13647 -> 0.13811.
2024-12-02-04:53:26-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-04:53:27-root-INFO: step: 121 lr_xt 0.04151486
2024-12-02-04:53:27-root-INFO: grad norm: 21.102 20.853 3.232
2024-12-02-04:53:27-root-INFO: Loss too large (291.166->296.085)! Learning rate decreased to 0.03321.
2024-12-02-04:53:28-root-INFO: Loss too large (291.166->291.825)! Learning rate decreased to 0.02657.
2024-12-02-04:53:29-root-INFO: grad norm: 19.536 19.356 2.644
2024-12-02-04:53:30-root-INFO: grad norm: 18.474 18.315 2.414
2024-12-02-04:53:31-root-INFO: grad norm: 17.896 17.728 2.450
2024-12-02-04:53:32-root-INFO: grad norm: 17.398 17.252 2.250
2024-12-02-04:53:32-root-INFO: Loss Change: 291.166 -> 283.323
2024-12-02-04:53:32-root-INFO: Regularization Change: 0.000 -> 1.167
2024-12-02-04:53:32-root-INFO: Learning rate of xt decay: 0.13811 -> 0.13977.
2024-12-02-04:53:32-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-04:53:33-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-04:53:33-root-INFO: grad norm: 16.547 16.360 2.482
2024-12-02-04:53:33-root-INFO: Loss too large (282.982->285.343)! Learning rate decreased to 0.03427.
2024-12-02-04:53:34-root-INFO: grad norm: 22.724 22.596 2.408
2024-12-02-04:53:35-root-INFO: Loss too large (282.503->284.867)! Learning rate decreased to 0.02742.
2024-12-02-04:53:36-root-INFO: grad norm: 21.474 21.327 2.505
2024-12-02-04:53:37-root-INFO: grad norm: 20.071 19.941 2.277
2024-12-02-04:53:38-root-INFO: grad norm: 19.429 19.287 2.346
2024-12-02-04:53:38-root-INFO: Loss Change: 282.982 -> 276.847
2024-12-02-04:53:38-root-INFO: Regularization Change: 0.000 -> 1.127
2024-12-02-04:53:38-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-04:53:38-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-04:53:39-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-04:53:39-root-INFO: grad norm: 25.548 25.258 3.840
2024-12-02-04:53:39-root-INFO: Loss too large (278.369->288.974)! Learning rate decreased to 0.03536.
2024-12-02-04:53:40-root-INFO: Loss too large (278.369->281.657)! Learning rate decreased to 0.02829.
2024-12-02-04:53:41-root-INFO: grad norm: 24.065 23.919 2.644
2024-12-02-04:53:42-root-INFO: grad norm: 22.581 22.430 2.604
2024-12-02-04:53:43-root-INFO: grad norm: 21.704 21.565 2.447
2024-12-02-04:53:44-root-INFO: grad norm: 20.792 20.656 2.380
2024-12-02-04:53:44-root-INFO: Loss Change: 278.369 -> 271.584
2024-12-02-04:53:44-root-INFO: Regularization Change: 0.000 -> 1.188
2024-12-02-04:53:44-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-04:53:44-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-04:53:45-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-04:53:45-root-INFO: grad norm: 19.688 19.557 2.263
2024-12-02-04:53:45-root-INFO: Loss too large (271.748->278.205)! Learning rate decreased to 0.03648.
2024-12-02-04:53:46-root-INFO: Loss too large (271.748->272.837)! Learning rate decreased to 0.02919.
2024-12-02-04:53:47-root-INFO: grad norm: 18.982 18.851 2.223
2024-12-02-04:53:48-root-INFO: grad norm: 18.625 18.498 2.173
2024-12-02-04:53:49-root-INFO: grad norm: 18.285 18.160 2.134
2024-12-02-04:53:50-root-INFO: grad norm: 18.122 17.998 2.117
2024-12-02-04:53:50-root-INFO: Loss Change: 271.748 -> 265.914
2024-12-02-04:53:50-root-INFO: Regularization Change: 0.000 -> 1.009
2024-12-02-04:53:50-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-04:53:50-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-04:53:51-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-04:53:51-root-INFO: grad norm: 24.362 24.096 3.591
2024-12-02-04:53:51-root-INFO: Loss too large (267.518->278.722)! Learning rate decreased to 0.03763.
2024-12-02-04:53:52-root-INFO: Loss too large (267.518->271.084)! Learning rate decreased to 0.03011.
2024-12-02-04:53:53-root-INFO: grad norm: 23.393 23.265 2.445
2024-12-02-04:53:54-root-INFO: grad norm: 22.713 22.567 2.572
2024-12-02-04:53:55-root-INFO: grad norm: 22.184 22.060 2.343
2024-12-02-04:53:56-root-INFO: grad norm: 21.561 21.427 2.395
2024-12-02-04:53:56-root-INFO: Loss Change: 267.518 -> 262.181
2024-12-02-04:53:56-root-INFO: Regularization Change: 0.000 -> 1.161
2024-12-02-04:53:56-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-04:53:56-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-04:53:57-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-04:53:57-root-INFO: grad norm: 19.518 19.391 2.224
2024-12-02-04:53:57-root-INFO: Loss too large (261.376->268.632)! Learning rate decreased to 0.03881.
2024-12-02-04:53:58-root-INFO: Loss too large (261.376->262.891)! Learning rate decreased to 0.03105.
2024-12-02-04:53:59-root-INFO: grad norm: 19.132 18.998 2.258
2024-12-02-04:54:00-root-INFO: grad norm: 19.015 18.901 2.079
2024-12-02-04:54:01-root-INFO: grad norm: 18.946 18.819 2.189
2024-12-02-04:54:02-root-INFO: grad norm: 18.954 18.841 2.067
2024-12-02-04:54:02-root-INFO: Loss Change: 261.376 -> 256.412
2024-12-02-04:54:02-root-INFO: Regularization Change: 0.000 -> 1.015
2024-12-02-04:54:02-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-04:54:02-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-04:54:03-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-04:54:03-root-INFO: grad norm: 23.640 23.434 3.119
2024-12-02-04:54:03-root-INFO: Loss too large (258.036->269.915)! Learning rate decreased to 0.04002.
2024-12-02-04:54:04-root-INFO: Loss too large (258.036->262.078)! Learning rate decreased to 0.03202.
2024-12-02-04:54:05-root-INFO: grad norm: 22.889 22.767 2.360
2024-12-02-04:54:06-root-INFO: grad norm: 22.131 21.993 2.473
2024-12-02-04:54:07-root-INFO: grad norm: 21.614 21.497 2.244
2024-12-02-04:54:08-root-INFO: grad norm: 21.030 20.901 2.326
2024-12-02-04:54:08-root-INFO: Loss Change: 258.036 -> 253.320
2024-12-02-04:54:08-root-INFO: Regularization Change: 0.000 -> 1.149
2024-12-02-04:54:08-root-INFO: Undo step: 115
2024-12-02-04:54:08-root-INFO: Undo step: 116
2024-12-02-04:54:08-root-INFO: Undo step: 117
2024-12-02-04:54:08-root-INFO: Undo step: 118
2024-12-02-04:54:08-root-INFO: Undo step: 119
2024-12-02-04:54:09-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-04:54:09-root-INFO: grad norm: 104.533 102.609 19.964
2024-12-02-04:54:10-root-INFO: grad norm: 63.743 62.789 10.986
2024-12-02-04:54:11-root-INFO: grad norm: 62.583 61.347 12.374
2024-12-02-04:54:12-root-INFO: grad norm: 66.684 65.810 10.764
2024-12-02-04:54:13-root-INFO: grad norm: 67.555 66.501 11.886
2024-12-02-04:54:14-root-INFO: Loss Change: 616.072 -> 351.733
2024-12-02-04:54:14-root-INFO: Regularization Change: 0.000 -> 48.902
2024-12-02-04:54:14-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-04:54:14-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-04:54:14-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-04:54:14-root-INFO: grad norm: 79.844 78.869 12.439
2024-12-02-04:54:15-root-INFO: Loss too large (362.627->368.824)! Learning rate decreased to 0.03536.
2024-12-02-04:54:16-root-INFO: grad norm: 52.890 52.164 8.733
2024-12-02-04:54:17-root-INFO: grad norm: 34.800 34.424 5.097
2024-12-02-04:54:18-root-INFO: grad norm: 31.748 31.394 4.725
2024-12-02-04:54:19-root-INFO: grad norm: 30.609 30.326 4.156
2024-12-02-04:54:19-root-INFO: Loss Change: 362.627 -> 285.891
2024-12-02-04:54:19-root-INFO: Regularization Change: 0.000 -> 7.458
2024-12-02-04:54:19-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-04:54:19-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-04:54:20-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-04:54:20-root-INFO: grad norm: 29.755 29.540 3.569
2024-12-02-04:54:20-root-INFO: Loss too large (285.129->291.290)! Learning rate decreased to 0.03648.
2024-12-02-04:54:21-root-INFO: grad norm: 28.354 28.118 3.650
2024-12-02-04:54:22-root-INFO: grad norm: 26.944 26.729 3.399
2024-12-02-04:54:23-root-INFO: grad norm: 26.574 26.363 3.343
2024-12-02-04:54:24-root-INFO: grad norm: 26.100 25.900 3.221
2024-12-02-04:54:25-root-INFO: Loss Change: 285.129 -> 273.920
2024-12-02-04:54:25-root-INFO: Regularization Change: 0.000 -> 3.038
2024-12-02-04:54:25-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-04:54:25-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-04:54:25-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-04:54:26-root-INFO: grad norm: 31.524 31.112 5.085
2024-12-02-04:54:26-root-INFO: Loss too large (276.651->282.749)! Learning rate decreased to 0.03763.
2024-12-02-04:54:27-root-INFO: grad norm: 30.398 30.177 3.659
2024-12-02-04:54:28-root-INFO: grad norm: 30.445 30.178 4.025
2024-12-02-04:54:29-root-INFO: grad norm: 30.135 29.911 3.666
2024-12-02-04:54:30-root-INFO: grad norm: 30.282 30.030 3.900
2024-12-02-04:54:31-root-INFO: Loss Change: 276.651 -> 266.812
2024-12-02-04:54:31-root-INFO: Regularization Change: 0.000 -> 2.917
2024-12-02-04:54:31-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-04:54:31-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-04:54:31-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-04:54:31-root-INFO: grad norm: 27.467 27.322 2.818
2024-12-02-04:54:32-root-INFO: Loss too large (264.768->271.717)! Learning rate decreased to 0.03881.
2024-12-02-04:54:33-root-INFO: grad norm: 28.021 27.786 3.627
2024-12-02-04:54:34-root-INFO: grad norm: 28.748 28.559 3.292
2024-12-02-04:54:35-root-INFO: grad norm: 29.560 29.330 3.682
2024-12-02-04:54:36-root-INFO: grad norm: 30.244 30.033 3.565
2024-12-02-04:54:36-root-INFO: Loss Change: 264.768 -> 260.314
2024-12-02-04:54:36-root-INFO: Regularization Change: 0.000 -> 2.481
2024-12-02-04:54:36-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-04:54:36-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-04:54:37-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-04:54:37-root-INFO: grad norm: 36.530 36.160 5.189
2024-12-02-04:54:37-root-INFO: Loss too large (263.522->279.544)! Learning rate decreased to 0.04002.
2024-12-02-04:54:38-root-INFO: Loss too large (263.522->264.206)! Learning rate decreased to 0.03202.
2024-12-02-04:54:39-root-INFO: grad norm: 24.658 24.484 2.924
2024-12-02-04:54:40-root-INFO: grad norm: 17.235 17.061 2.441
2024-12-02-04:54:41-root-INFO: grad norm: 13.517 13.389 1.853
2024-12-02-04:54:42-root-INFO: grad norm: 10.982 10.841 1.756
2024-12-02-04:54:42-root-INFO: Loss Change: 263.522 -> 246.627
2024-12-02-04:54:42-root-INFO: Regularization Change: 0.000 -> 1.427
2024-12-02-04:54:42-root-INFO: Undo step: 115
2024-12-02-04:54:42-root-INFO: Undo step: 116
2024-12-02-04:54:42-root-INFO: Undo step: 117
2024-12-02-04:54:42-root-INFO: Undo step: 118
2024-12-02-04:54:42-root-INFO: Undo step: 119
2024-12-02-04:54:43-root-INFO: step: 120 lr_xt 0.04284282
2024-12-02-04:54:43-root-INFO: grad norm: 104.435 102.166 21.650
2024-12-02-04:54:44-root-INFO: grad norm: 75.857 74.864 12.236
2024-12-02-04:54:45-root-INFO: grad norm: 74.441 73.692 10.530
2024-12-02-04:54:46-root-INFO: grad norm: 65.244 64.665 8.673
2024-12-02-04:54:47-root-INFO: grad norm: 51.593 50.906 8.393
2024-12-02-04:54:48-root-INFO: Loss Change: 586.276 -> 319.980
2024-12-02-04:54:48-root-INFO: Regularization Change: 0.000 -> 42.633
2024-12-02-04:54:48-root-INFO: Learning rate of xt decay: 0.13977 -> 0.14145.
2024-12-02-04:54:48-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-04:54:48-root-INFO: step: 119 lr_xt 0.04420613
2024-12-02-04:54:48-root-INFO: grad norm: 50.418 49.828 7.688
2024-12-02-04:54:49-root-INFO: grad norm: 51.715 51.207 7.229
2024-12-02-04:54:50-root-INFO: grad norm: 51.685 51.272 6.524
2024-12-02-04:54:51-root-INFO: grad norm: 51.898 51.469 6.658
2024-12-02-04:54:52-root-INFO: grad norm: 53.845 53.458 6.441
2024-12-02-04:54:53-root-INFO: Loss Change: 323.889 -> 305.972
2024-12-02-04:54:53-root-INFO: Regularization Change: 0.000 -> 10.533
2024-12-02-04:54:53-root-INFO: Learning rate of xt decay: 0.14145 -> 0.14314.
2024-12-02-04:54:53-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-04:54:53-root-INFO: step: 118 lr_xt 0.04560549
2024-12-02-04:54:54-root-INFO: grad norm: 53.776 53.458 5.837
2024-12-02-04:54:55-root-INFO: grad norm: 52.262 51.943 5.766
2024-12-02-04:54:56-root-INFO: grad norm: 49.810 49.487 5.661
2024-12-02-04:54:57-root-INFO: grad norm: 48.700 48.421 5.207
2024-12-02-04:54:58-root-INFO: grad norm: 49.348 49.080 5.132
2024-12-02-04:54:58-root-INFO: Loss Change: 305.505 -> 289.447
2024-12-02-04:54:58-root-INFO: Regularization Change: 0.000 -> 8.304
2024-12-02-04:54:58-root-INFO: Learning rate of xt decay: 0.14314 -> 0.14486.
2024-12-02-04:54:58-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-04:54:59-root-INFO: step: 117 lr_xt 0.04704158
2024-12-02-04:54:59-root-INFO: grad norm: 53.083 52.723 6.167
2024-12-02-04:55:00-root-INFO: grad norm: 52.570 52.331 5.005
2024-12-02-04:55:01-root-INFO: grad norm: 50.330 50.095 4.859
2024-12-02-04:55:02-root-INFO: grad norm: 47.984 47.776 4.466
2024-12-02-04:55:03-root-INFO: grad norm: 46.209 45.998 4.406
2024-12-02-04:55:04-root-INFO: Loss Change: 292.693 -> 274.808
2024-12-02-04:55:04-root-INFO: Regularization Change: 0.000 -> 7.477
2024-12-02-04:55:04-root-INFO: Learning rate of xt decay: 0.14486 -> 0.14660.
2024-12-02-04:55:04-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-04:55:04-root-INFO: step: 116 lr_xt 0.04851508
2024-12-02-04:55:04-root-INFO: grad norm: 44.590 44.445 3.599
2024-12-02-04:55:05-root-INFO: grad norm: 43.702 43.520 3.978
2024-12-02-04:55:06-root-INFO: grad norm: 42.665 42.498 3.768
2024-12-02-04:55:07-root-INFO: grad norm: 42.641 42.454 3.990
2024-12-02-04:55:08-root-INFO: grad norm: 42.819 42.633 3.983
2024-12-02-04:55:09-root-INFO: Loss Change: 273.793 -> 265.416
2024-12-02-04:55:09-root-INFO: Regularization Change: 0.000 -> 6.206
2024-12-02-04:55:09-root-INFO: Learning rate of xt decay: 0.14660 -> 0.14836.
2024-12-02-04:55:09-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-04:55:09-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-04:55:10-root-INFO: grad norm: 45.213 44.944 4.923
2024-12-02-04:55:11-root-INFO: grad norm: 45.064 44.843 4.462
2024-12-02-04:55:12-root-INFO: grad norm: 44.313 44.096 4.378
2024-12-02-04:55:13-root-INFO: grad norm: 43.678 43.468 4.279
2024-12-02-04:55:14-root-INFO: grad norm: 42.920 42.722 4.118
2024-12-02-04:55:14-root-INFO: Loss Change: 267.386 -> 259.215
2024-12-02-04:55:14-root-INFO: Regularization Change: 0.000 -> 6.272
2024-12-02-04:55:14-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-04:55:15-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-04:55:15-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-04:55:15-root-INFO: grad norm: 41.224 41.090 3.327
2024-12-02-04:55:16-root-INFO: grad norm: 40.540 40.377 3.639
2024-12-02-04:55:17-root-INFO: grad norm: 39.614 39.465 3.440
2024-12-02-04:55:18-root-INFO: grad norm: 39.548 39.385 3.593
2024-12-02-04:55:19-root-INFO: grad norm: 39.552 39.390 3.578
2024-12-02-04:55:20-root-INFO: Loss Change: 257.797 -> 250.801
2024-12-02-04:55:20-root-INFO: Regularization Change: 0.000 -> 5.695
2024-12-02-04:55:20-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-04:55:20-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-04:55:20-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-04:55:20-root-INFO: grad norm: 42.092 41.854 4.469
2024-12-02-04:55:21-root-INFO: Loss too large (252.724->253.643)! Learning rate decreased to 0.04253.
2024-12-02-04:55:22-root-INFO: grad norm: 27.353 27.200 2.886
2024-12-02-04:55:23-root-INFO: grad norm: 18.721 18.615 1.982
2024-12-02-04:55:24-root-INFO: grad norm: 14.461 14.358 1.727
2024-12-02-04:55:25-root-INFO: grad norm: 11.956 11.880 1.343
2024-12-02-04:55:26-root-INFO: Loss Change: 252.724 -> 225.603
2024-12-02-04:55:26-root-INFO: Regularization Change: 0.000 -> 2.647
2024-12-02-04:55:26-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-04:55:26-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-04:55:26-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-04:55:26-root-INFO: grad norm: 13.055 12.914 1.910
2024-12-02-04:55:27-root-INFO: Loss too large (225.718->226.648)! Learning rate decreased to 0.04384.
2024-12-02-04:55:28-root-INFO: grad norm: 13.535 13.460 1.424
2024-12-02-04:55:29-root-INFO: grad norm: 15.479 15.400 1.562
2024-12-02-04:55:29-root-INFO: Loss too large (224.321->224.603)! Learning rate decreased to 0.03507.
2024-12-02-04:55:30-root-INFO: grad norm: 12.169 12.095 1.342
2024-12-02-04:55:31-root-INFO: grad norm: 9.619 9.536 1.261
2024-12-02-04:55:32-root-INFO: Loss Change: 225.718 -> 220.774
2024-12-02-04:55:32-root-INFO: Regularization Change: 0.000 -> 0.891
2024-12-02-04:55:32-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-04:55:32-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-04:55:32-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-04:55:32-root-INFO: grad norm: 7.191 7.070 1.310
2024-12-02-04:55:33-root-INFO: grad norm: 10.440 10.372 1.189
2024-12-02-04:55:34-root-INFO: Loss too large (219.802->220.852)! Learning rate decreased to 0.04517.
2024-12-02-04:55:35-root-INFO: grad norm: 12.251 12.191 1.207
2024-12-02-04:55:36-root-INFO: grad norm: 15.075 15.011 1.390
2024-12-02-04:55:36-root-INFO: Loss too large (219.374->220.023)! Learning rate decreased to 0.03614.
2024-12-02-04:55:37-root-INFO: grad norm: 12.443 12.376 1.287
2024-12-02-04:55:38-root-INFO: Loss Change: 220.380 -> 217.029
2024-12-02-04:55:38-root-INFO: Regularization Change: 0.000 -> 0.916
2024-12-02-04:55:38-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-04:55:38-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-04:55:38-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-04:55:38-root-INFO: grad norm: 13.762 13.626 1.926
2024-12-02-04:55:39-root-INFO: Loss too large (217.839->220.867)! Learning rate decreased to 0.04654.
2024-12-02-04:55:39-root-INFO: Loss too large (217.839->218.274)! Learning rate decreased to 0.03724.
2024-12-02-04:55:40-root-INFO: grad norm: 11.840 11.764 1.340
2024-12-02-04:55:41-root-INFO: grad norm: 10.326 10.243 1.307
2024-12-02-04:55:42-root-INFO: grad norm: 9.419 9.345 1.177
2024-12-02-04:55:43-root-INFO: grad norm: 8.626 8.549 1.155
2024-12-02-04:55:43-root-INFO: Loss Change: 217.839 -> 213.648
2024-12-02-04:55:43-root-INFO: Regularization Change: 0.000 -> 0.716
2024-12-02-04:55:44-root-INFO: Undo step: 110
2024-12-02-04:55:44-root-INFO: Undo step: 111
2024-12-02-04:55:44-root-INFO: Undo step: 112
2024-12-02-04:55:44-root-INFO: Undo step: 113
2024-12-02-04:55:44-root-INFO: Undo step: 114
2024-12-02-04:55:44-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-04:55:44-root-INFO: grad norm: 77.497 76.684 11.198
2024-12-02-04:55:45-root-INFO: grad norm: 47.275 46.790 6.756
2024-12-02-04:55:46-root-INFO: grad norm: 47.316 46.926 6.064
2024-12-02-04:55:47-root-INFO: grad norm: 38.083 37.807 4.576
2024-12-02-04:55:48-root-INFO: grad norm: 29.911 29.678 3.729
2024-12-02-04:55:49-root-INFO: Loss Change: 469.963 -> 264.210
2024-12-02-04:55:49-root-INFO: Regularization Change: 0.000 -> 36.516
2024-12-02-04:55:49-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-04:55:49-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-04:55:49-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-04:55:50-root-INFO: grad norm: 33.708 33.486 3.855
2024-12-02-04:55:50-root-INFO: Loss too large (265.141->266.735)! Learning rate decreased to 0.04126.
2024-12-02-04:55:51-root-INFO: grad norm: 27.512 27.346 3.018
2024-12-02-04:55:52-root-INFO: grad norm: 25.139 24.956 3.029
2024-12-02-04:55:53-root-INFO: grad norm: 24.925 24.759 2.870
2024-12-02-04:55:54-root-INFO: grad norm: 24.683 24.504 2.966
2024-12-02-04:55:54-root-INFO: Loss Change: 265.141 -> 245.004
2024-12-02-04:55:54-root-INFO: Regularization Change: 0.000 -> 4.186
2024-12-02-04:55:54-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-04:55:54-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-04:55:55-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-04:55:55-root-INFO: grad norm: 25.617 25.439 3.011
2024-12-02-04:55:55-root-INFO: Loss too large (245.042->249.655)! Learning rate decreased to 0.04253.
2024-12-02-04:55:56-root-INFO: grad norm: 25.381 25.192 3.089
2024-12-02-04:55:57-root-INFO: grad norm: 25.420 25.255 2.894
2024-12-02-04:55:58-root-INFO: grad norm: 25.675 25.487 3.097
2024-12-02-04:55:59-root-INFO: grad norm: 26.165 25.993 2.995
2024-12-02-04:56:00-root-INFO: Loss Change: 245.042 -> 237.341
2024-12-02-04:56:00-root-INFO: Regularization Change: 0.000 -> 2.945
2024-12-02-04:56:00-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-04:56:00-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-04:56:00-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-04:56:01-root-INFO: grad norm: 24.781 24.618 2.830
2024-12-02-04:56:01-root-INFO: Loss too large (235.706->241.777)! Learning rate decreased to 0.04384.
2024-12-02-04:56:02-root-INFO: grad norm: 24.930 24.775 2.775
2024-12-02-04:56:03-root-INFO: grad norm: 25.427 25.252 2.981
2024-12-02-04:56:04-root-INFO: grad norm: 26.386 26.211 3.032
2024-12-02-04:56:05-root-INFO: grad norm: 27.110 26.913 3.262
2024-12-02-04:56:06-root-INFO: Loss Change: 235.706 -> 231.223
2024-12-02-04:56:06-root-INFO: Regularization Change: 0.000 -> 2.587
2024-12-02-04:56:06-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-04:56:06-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-04:56:06-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-04:56:06-root-INFO: grad norm: 30.383 30.140 3.837
2024-12-02-04:56:07-root-INFO: Loss too large (232.678->243.823)! Learning rate decreased to 0.04517.
2024-12-02-04:56:08-root-INFO: grad norm: 30.040 29.807 3.738
2024-12-02-04:56:09-root-INFO: grad norm: 29.525 29.305 3.601
2024-12-02-04:56:10-root-INFO: grad norm: 29.075 28.841 3.678
2024-12-02-04:56:11-root-INFO: grad norm: 28.689 28.473 3.514
2024-12-02-04:56:11-root-INFO: Loss Change: 232.678 -> 227.237
2024-12-02-04:56:11-root-INFO: Regularization Change: 0.000 -> 2.872
2024-12-02-04:56:11-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-04:56:11-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-04:56:12-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-04:56:12-root-INFO: grad norm: 26.477 26.309 2.982
2024-12-02-04:56:12-root-INFO: Loss too large (225.809->234.078)! Learning rate decreased to 0.04654.
2024-12-02-04:56:13-root-INFO: grad norm: 26.746 26.549 3.233
2024-12-02-04:56:14-root-INFO: grad norm: 26.900 26.697 3.302
2024-12-02-04:56:15-root-INFO: grad norm: 27.232 27.029 3.321
2024-12-02-04:56:16-root-INFO: Loss too large (222.914->223.054)! Learning rate decreased to 0.03724.
2024-12-02-04:56:17-root-INFO: grad norm: 17.943 17.788 2.350
2024-12-02-04:56:17-root-INFO: Loss Change: 225.809 -> 214.748
2024-12-02-04:56:17-root-INFO: Regularization Change: 0.000 -> 1.625
2024-12-02-04:56:17-root-INFO: Undo step: 110
2024-12-02-04:56:17-root-INFO: Undo step: 111
2024-12-02-04:56:17-root-INFO: Undo step: 112
2024-12-02-04:56:18-root-INFO: Undo step: 113
2024-12-02-04:56:18-root-INFO: Undo step: 114
2024-12-02-04:56:18-root-INFO: step: 115 lr_xt 0.05002669
2024-12-02-04:56:18-root-INFO: grad norm: 75.603 74.353 13.692
2024-12-02-04:56:19-root-INFO: grad norm: 50.348 49.778 7.551
2024-12-02-04:56:20-root-INFO: grad norm: 47.933 47.336 7.545
2024-12-02-04:56:21-root-INFO: grad norm: 49.465 49.092 6.065
2024-12-02-04:56:22-root-INFO: grad norm: 49.602 49.165 6.568
2024-12-02-04:56:23-root-INFO: Loss Change: 438.569 -> 279.800
2024-12-02-04:56:23-root-INFO: Regularization Change: 0.000 -> 34.296
2024-12-02-04:56:23-root-INFO: Learning rate of xt decay: 0.14836 -> 0.15014.
2024-12-02-04:56:23-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-04:56:23-root-INFO: step: 114 lr_xt 0.05157710
2024-12-02-04:56:23-root-INFO: grad norm: 52.462 52.103 6.131
2024-12-02-04:56:24-root-INFO: Loss too large (282.509->286.528)! Learning rate decreased to 0.04126.
2024-12-02-04:56:25-root-INFO: grad norm: 36.369 36.085 4.538
2024-12-02-04:56:26-root-INFO: grad norm: 24.583 24.438 2.669
2024-12-02-04:56:27-root-INFO: grad norm: 19.618 19.437 2.662
2024-12-02-04:56:28-root-INFO: grad norm: 16.172 16.037 2.085
2024-12-02-04:56:28-root-INFO: Loss Change: 282.509 -> 237.974
2024-12-02-04:56:28-root-INFO: Regularization Change: 0.000 -> 5.224
2024-12-02-04:56:28-root-INFO: Learning rate of xt decay: 0.15014 -> 0.15194.
2024-12-02-04:56:28-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-04:56:29-root-INFO: step: 113 lr_xt 0.05316701
2024-12-02-04:56:29-root-INFO: grad norm: 14.965 14.791 2.271
2024-12-02-04:56:30-root-INFO: grad norm: 19.516 19.377 2.325
2024-12-02-04:56:30-root-INFO: Loss too large (236.483->238.040)! Learning rate decreased to 0.04253.
2024-12-02-04:56:32-root-INFO: grad norm: 18.433 18.300 2.205
2024-12-02-04:56:33-root-INFO: grad norm: 17.686 17.551 2.187
2024-12-02-04:56:34-root-INFO: grad norm: 17.487 17.361 2.089
2024-12-02-04:56:34-root-INFO: Loss Change: 237.601 -> 229.079
2024-12-02-04:56:34-root-INFO: Regularization Change: 0.000 -> 2.509
2024-12-02-04:56:34-root-INFO: Learning rate of xt decay: 0.15194 -> 0.15376.
2024-12-02-04:56:34-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-04:56:35-root-INFO: step: 112 lr_xt 0.05479712
2024-12-02-04:56:35-root-INFO: grad norm: 17.365 17.236 2.111
2024-12-02-04:56:35-root-INFO: Loss too large (228.533->230.238)! Learning rate decreased to 0.04384.
2024-12-02-04:56:36-root-INFO: grad norm: 17.411 17.310 1.880
2024-12-02-04:56:37-root-INFO: grad norm: 18.243 18.150 1.845
2024-12-02-04:56:38-root-INFO: grad norm: 19.055 18.954 1.956
2024-12-02-04:56:39-root-INFO: grad norm: 19.980 19.885 1.950
2024-12-02-04:56:40-root-INFO: Loss Change: 228.533 -> 223.721
2024-12-02-04:56:40-root-INFO: Regularization Change: 0.000 -> 2.055
2024-12-02-04:56:40-root-INFO: Learning rate of xt decay: 0.15376 -> 0.15561.
2024-12-02-04:56:40-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-04:56:40-root-INFO: step: 111 lr_xt 0.05646812
2024-12-02-04:56:41-root-INFO: grad norm: 21.637 21.485 2.555
2024-12-02-04:56:41-root-INFO: Loss too large (224.088->228.471)! Learning rate decreased to 0.04517.
2024-12-02-04:56:42-root-INFO: grad norm: 21.754 21.649 2.139
2024-12-02-04:56:43-root-INFO: grad norm: 21.620 21.499 2.285
2024-12-02-04:56:44-root-INFO: grad norm: 21.182 21.075 2.127
2024-12-02-04:56:45-root-INFO: grad norm: 20.718 20.602 2.183
2024-12-02-04:56:45-root-INFO: Loss Change: 224.088 -> 217.959
2024-12-02-04:56:45-root-INFO: Regularization Change: 0.000 -> 2.057
2024-12-02-04:56:45-root-INFO: Learning rate of xt decay: 0.15561 -> 0.15748.
2024-12-02-04:56:45-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-04:56:46-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-04:56:46-root-INFO: grad norm: 19.723 19.628 1.937
2024-12-02-04:56:46-root-INFO: Loss too large (217.749->221.479)! Learning rate decreased to 0.04654.
2024-12-02-04:56:47-root-INFO: grad norm: 19.091 18.994 1.925
2024-12-02-04:56:48-root-INFO: grad norm: 18.611 18.526 1.781
2024-12-02-04:56:49-root-INFO: grad norm: 18.301 18.211 1.812
2024-12-02-04:56:50-root-INFO: grad norm: 18.033 17.951 1.724
2024-12-02-04:56:51-root-INFO: Loss Change: 217.749 -> 212.636
2024-12-02-04:56:51-root-INFO: Regularization Change: 0.000 -> 1.790
2024-12-02-04:56:51-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-04:56:51-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-04:56:51-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-04:56:52-root-INFO: grad norm: 18.440 18.273 2.477
2024-12-02-04:56:52-root-INFO: Loss too large (213.038->216.215)! Learning rate decreased to 0.04795.
2024-12-02-04:56:53-root-INFO: grad norm: 18.192 18.097 1.862
2024-12-02-04:56:54-root-INFO: grad norm: 18.230 18.119 2.007
2024-12-02-04:56:55-root-INFO: grad norm: 18.338 18.242 1.874
2024-12-02-04:56:56-root-INFO: grad norm: 18.454 18.347 1.978
2024-12-02-04:56:57-root-INFO: Loss Change: 213.038 -> 208.784
2024-12-02-04:56:57-root-INFO: Regularization Change: 0.000 -> 1.724
2024-12-02-04:56:57-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-04:56:57-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-04:56:57-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-04:56:57-root-INFO: grad norm: 17.463 17.388 1.615
2024-12-02-04:56:58-root-INFO: Loss too large (208.081->211.360)! Learning rate decreased to 0.04939.
2024-12-02-04:56:59-root-INFO: grad norm: 17.170 17.082 1.737
2024-12-02-04:57:00-root-INFO: grad norm: 17.073 17.001 1.568
2024-12-02-04:57:01-root-INFO: grad norm: 17.079 16.997 1.674
2024-12-02-04:57:02-root-INFO: grad norm: 17.141 17.068 1.579
2024-12-02-04:57:02-root-INFO: Loss Change: 208.081 -> 204.435
2024-12-02-04:57:02-root-INFO: Regularization Change: 0.000 -> 1.610
2024-12-02-04:57:02-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-04:57:02-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-04:57:03-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-04:57:03-root-INFO: grad norm: 18.695 18.505 2.663
2024-12-02-04:57:03-root-INFO: Loss too large (205.207->209.290)! Learning rate decreased to 0.05086.
2024-12-02-04:57:04-root-INFO: grad norm: 18.876 18.767 2.029
2024-12-02-04:57:05-root-INFO: grad norm: 19.327 19.197 2.232
2024-12-02-04:57:07-root-INFO: grad norm: 19.821 19.705 2.143
2024-12-02-04:57:08-root-INFO: grad norm: 20.281 20.149 2.314
2024-12-02-04:57:08-root-INFO: Loss Change: 205.207 -> 202.507
2024-12-02-04:57:08-root-INFO: Regularization Change: 0.000 -> 1.831
2024-12-02-04:57:08-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-04:57:08-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-04:57:09-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-04:57:09-root-INFO: grad norm: 19.031 18.950 1.758
2024-12-02-04:57:09-root-INFO: Loss too large (201.465->206.164)! Learning rate decreased to 0.05237.
2024-12-02-04:57:10-root-INFO: grad norm: 18.954 18.839 2.082
2024-12-02-04:57:11-root-INFO: grad norm: 19.137 19.040 1.931
2024-12-02-04:57:12-root-INFO: grad norm: 19.497 19.371 2.217
2024-12-02-04:57:13-root-INFO: grad norm: 19.969 19.848 2.192
2024-12-02-04:57:14-root-INFO: Loss Change: 201.465 -> 199.047
2024-12-02-04:57:14-root-INFO: Regularization Change: 0.000 -> 1.878
2024-12-02-04:57:14-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-04:57:14-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-04:57:14-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-04:57:15-root-INFO: grad norm: 23.530 23.273 3.471
2024-12-02-04:57:15-root-INFO: Loss too large (200.935->210.184)! Learning rate decreased to 0.05391.
2024-12-02-04:57:15-root-INFO: Loss too large (200.935->201.095)! Learning rate decreased to 0.04313.
2024-12-02-04:57:16-root-INFO: grad norm: 15.973 15.850 1.983
2024-12-02-04:57:17-root-INFO: grad norm: 11.840 11.709 1.752
2024-12-02-04:57:18-root-INFO: grad norm: 9.421 9.330 1.309
2024-12-02-04:57:19-root-INFO: grad norm: 7.903 7.803 1.254
2024-12-02-04:57:20-root-INFO: Loss Change: 200.935 -> 191.489
2024-12-02-04:57:20-root-INFO: Regularization Change: 0.000 -> 1.095
2024-12-02-04:57:20-root-INFO: Undo step: 105
2024-12-02-04:57:20-root-INFO: Undo step: 106
2024-12-02-04:57:20-root-INFO: Undo step: 107
2024-12-02-04:57:20-root-INFO: Undo step: 108
2024-12-02-04:57:20-root-INFO: Undo step: 109
2024-12-02-04:57:20-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-04:57:20-root-INFO: grad norm: 79.689 78.830 11.673
2024-12-02-04:57:21-root-INFO: grad norm: 59.378 58.828 8.059
2024-12-02-04:57:22-root-INFO: grad norm: 53.098 52.679 6.653
2024-12-02-04:57:23-root-INFO: grad norm: 54.241 53.812 6.814
2024-12-02-04:57:24-root-INFO: Loss too large (286.014->286.800)! Learning rate decreased to 0.04654.
2024-12-02-04:57:25-root-INFO: grad norm: 39.317 38.956 5.311
2024-12-02-04:57:25-root-INFO: Loss Change: 444.390 -> 245.464
2024-12-02-04:57:25-root-INFO: Regularization Change: 0.000 -> 39.700
2024-12-02-04:57:25-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-04:57:25-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-04:57:26-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-04:57:26-root-INFO: grad norm: 33.130 32.792 4.723
2024-12-02-04:57:26-root-INFO: Loss too large (247.578->251.329)! Learning rate decreased to 0.04795.
2024-12-02-04:57:27-root-INFO: grad norm: 29.221 28.946 4.001
2024-12-02-04:57:28-root-INFO: grad norm: 27.717 27.473 3.667
2024-12-02-04:57:29-root-INFO: grad norm: 27.542 27.305 3.606
2024-12-02-04:57:30-root-INFO: grad norm: 27.935 27.713 3.514
2024-12-02-04:57:31-root-INFO: Loss Change: 247.578 -> 227.197
2024-12-02-04:57:31-root-INFO: Regularization Change: 0.000 -> 5.906
2024-12-02-04:57:31-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-04:57:31-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-04:57:31-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-04:57:32-root-INFO: grad norm: 25.479 25.312 2.906
2024-12-02-04:57:32-root-INFO: Loss too large (225.179->232.084)! Learning rate decreased to 0.04939.
2024-12-02-04:57:33-root-INFO: grad norm: 25.846 25.639 3.263
2024-12-02-04:57:34-root-INFO: grad norm: 26.258 26.058 3.234
2024-12-02-04:57:35-root-INFO: grad norm: 26.665 26.457 3.323
2024-12-02-04:57:36-root-INFO: grad norm: 26.995 26.784 3.368
2024-12-02-04:57:36-root-INFO: Loss Change: 225.179 -> 217.879
2024-12-02-04:57:36-root-INFO: Regularization Change: 0.000 -> 3.749
2024-12-02-04:57:36-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-04:57:36-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-04:57:37-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-04:57:37-root-INFO: grad norm: 32.203 31.880 4.547
2024-12-02-04:57:37-root-INFO: Loss too large (221.521->234.954)! Learning rate decreased to 0.05086.
2024-12-02-04:57:38-root-INFO: grad norm: 31.562 31.308 3.995
2024-12-02-04:57:39-root-INFO: grad norm: 30.654 30.411 3.854
2024-12-02-04:57:40-root-INFO: grad norm: 30.133 29.895 3.778
2024-12-02-04:57:41-root-INFO: grad norm: 29.583 29.365 3.585
2024-12-02-04:57:42-root-INFO: Loss Change: 221.521 -> 213.604
2024-12-02-04:57:42-root-INFO: Regularization Change: 0.000 -> 3.953
2024-12-02-04:57:42-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-04:57:42-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-04:57:42-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-04:57:43-root-INFO: grad norm: 26.787 26.633 2.872
2024-12-02-04:57:43-root-INFO: Loss too large (211.335->221.666)! Learning rate decreased to 0.05237.
2024-12-02-04:57:44-root-INFO: grad norm: 27.257 27.060 3.268
2024-12-02-04:57:45-root-INFO: grad norm: 27.918 27.735 3.193
2024-12-02-04:57:46-root-INFO: grad norm: 28.382 28.187 3.320
2024-12-02-04:57:47-root-INFO: grad norm: 28.629 28.437 3.309
2024-12-02-04:57:48-root-INFO: Loss Change: 211.335 -> 208.133
2024-12-02-04:57:48-root-INFO: Regularization Change: 0.000 -> 3.364
2024-12-02-04:57:48-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-04:57:48-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-04:57:48-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-04:57:48-root-INFO: grad norm: 31.920 31.637 4.242
2024-12-02-04:57:49-root-INFO: Loss too large (211.069->225.723)! Learning rate decreased to 0.05391.
2024-12-02-04:57:50-root-INFO: grad norm: 31.321 31.102 3.699
2024-12-02-04:57:51-root-INFO: grad norm: 30.526 30.304 3.669
2024-12-02-04:57:52-root-INFO: grad norm: 29.730 29.521 3.523
2024-12-02-04:57:53-root-INFO: grad norm: 28.949 28.747 3.416
2024-12-02-04:57:53-root-INFO: Loss Change: 211.069 -> 204.206
2024-12-02-04:57:53-root-INFO: Regularization Change: 0.000 -> 3.790
2024-12-02-04:57:53-root-INFO: Undo step: 105
2024-12-02-04:57:53-root-INFO: Undo step: 106
2024-12-02-04:57:53-root-INFO: Undo step: 107
2024-12-02-04:57:53-root-INFO: Undo step: 108
2024-12-02-04:57:53-root-INFO: Undo step: 109
2024-12-02-04:57:54-root-INFO: step: 110 lr_xt 0.05818072
2024-12-02-04:57:54-root-INFO: grad norm: 76.663 75.595 12.754
2024-12-02-04:57:55-root-INFO: grad norm: 47.205 46.721 6.746
2024-12-02-04:57:56-root-INFO: grad norm: 47.112 46.708 6.157
2024-12-02-04:57:57-root-INFO: grad norm: 48.802 48.403 6.230
2024-12-02-04:57:58-root-INFO: grad norm: 46.380 45.967 6.172
2024-12-02-04:57:59-root-INFO: Loss Change: 445.196 -> 260.741
2024-12-02-04:57:59-root-INFO: Regularization Change: 0.000 -> 43.588
2024-12-02-04:57:59-root-INFO: Learning rate of xt decay: 0.15748 -> 0.15937.
2024-12-02-04:57:59-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-04:57:59-root-INFO: step: 109 lr_xt 0.05993563
2024-12-02-04:57:59-root-INFO: grad norm: 48.765 48.300 6.721
2024-12-02-04:58:00-root-INFO: grad norm: 47.027 46.595 6.364
2024-12-02-04:58:01-root-INFO: grad norm: 47.820 47.449 5.948
2024-12-02-04:58:02-root-INFO: Loss too large (253.148->260.901)! Learning rate decreased to 0.04795.
2024-12-02-04:58:03-root-INFO: grad norm: 35.263 34.937 4.786
2024-12-02-04:58:04-root-INFO: grad norm: 26.631 26.413 3.404
2024-12-02-04:58:04-root-INFO: Loss Change: 265.548 -> 221.603
2024-12-02-04:58:04-root-INFO: Regularization Change: 0.000 -> 8.090
2024-12-02-04:58:04-root-INFO: Learning rate of xt decay: 0.15937 -> 0.16128.
2024-12-02-04:58:04-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-04:58:05-root-INFO: step: 108 lr_xt 0.06173354
2024-12-02-04:58:05-root-INFO: grad norm: 21.840 21.667 2.747
2024-12-02-04:58:05-root-INFO: Loss too large (219.911->222.407)! Learning rate decreased to 0.04939.
2024-12-02-04:58:06-root-INFO: grad norm: 20.723 20.548 2.691
2024-12-02-04:58:07-root-INFO: grad norm: 20.510 20.336 2.668
2024-12-02-04:58:08-root-INFO: grad norm: 20.785 20.625 2.575
2024-12-02-04:58:09-root-INFO: grad norm: 21.287 21.119 2.676
2024-12-02-04:58:10-root-INFO: Loss Change: 219.911 -> 210.504
2024-12-02-04:58:10-root-INFO: Regularization Change: 0.000 -> 3.282
2024-12-02-04:58:10-root-INFO: Learning rate of xt decay: 0.16128 -> 0.16321.
2024-12-02-04:58:10-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-04:58:10-root-INFO: step: 107 lr_xt 0.06357517
2024-12-02-04:58:10-root-INFO: grad norm: 27.290 27.026 3.788
2024-12-02-04:58:11-root-INFO: Loss too large (213.427->223.969)! Learning rate decreased to 0.05086.
2024-12-02-04:58:11-root-INFO: Loss too large (213.427->213.447)! Learning rate decreased to 0.04069.
2024-12-02-04:58:12-root-INFO: grad norm: 18.607 18.446 2.439
2024-12-02-04:58:13-root-INFO: grad norm: 13.057 12.925 1.850
2024-12-02-04:58:14-root-INFO: grad norm: 10.058 9.932 1.586
2024-12-02-04:58:15-root-INFO: grad norm: 8.134 8.009 1.417
2024-12-02-04:58:16-root-INFO: Loss Change: 213.427 -> 199.805
2024-12-02-04:58:16-root-INFO: Regularization Change: 0.000 -> 1.634
2024-12-02-04:58:16-root-INFO: Learning rate of xt decay: 0.16321 -> 0.16517.
2024-12-02-04:58:16-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-04:58:16-root-INFO: step: 106 lr_xt 0.06546120
2024-12-02-04:58:16-root-INFO: grad norm: 6.649 6.473 1.523
2024-12-02-04:58:17-root-INFO: grad norm: 7.569 7.454 1.314
2024-12-02-04:58:18-root-INFO: grad norm: 10.837 10.762 1.269
2024-12-02-04:58:19-root-INFO: Loss too large (197.369->198.414)! Learning rate decreased to 0.05237.
2024-12-02-04:58:20-root-INFO: grad norm: 12.503 12.425 1.393
2024-12-02-04:58:21-root-INFO: grad norm: 14.422 14.359 1.345
2024-12-02-04:58:21-root-INFO: Loss Change: 199.340 -> 196.484
2024-12-02-04:58:21-root-INFO: Regularization Change: 0.000 -> 1.867
2024-12-02-04:58:21-root-INFO: Learning rate of xt decay: 0.16517 -> 0.16715.
2024-12-02-04:58:21-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-04:58:22-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-04:58:22-root-INFO: grad norm: 18.305 18.197 1.985
2024-12-02-04:58:22-root-INFO: Loss too large (197.298->203.419)! Learning rate decreased to 0.05391.
2024-12-02-04:58:23-root-INFO: Loss too large (197.298->197.946)! Learning rate decreased to 0.04313.
2024-12-02-04:58:24-root-INFO: grad norm: 14.100 14.027 1.436
2024-12-02-04:58:25-root-INFO: grad norm: 11.162 11.075 1.389
2024-12-02-04:58:26-root-INFO: grad norm: 9.348 9.271 1.196
2024-12-02-04:58:27-root-INFO: grad norm: 8.087 7.998 1.198
2024-12-02-04:58:27-root-INFO: Loss Change: 197.298 -> 190.327
2024-12-02-04:58:27-root-INFO: Regularization Change: 0.000 -> 1.076
2024-12-02-04:58:27-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-04:58:27-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-04:58:28-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-04:58:28-root-INFO: grad norm: 6.720 6.564 1.438
2024-12-02-04:58:29-root-INFO: grad norm: 8.828 8.745 1.204
2024-12-02-04:58:29-root-INFO: Loss too large (189.281->189.858)! Learning rate decreased to 0.05550.
2024-12-02-04:58:30-root-INFO: grad norm: 10.042 9.978 1.139
2024-12-02-04:58:31-root-INFO: grad norm: 11.960 11.897 1.227
2024-12-02-04:58:32-root-INFO: Loss too large (188.526->188.673)! Learning rate decreased to 0.04440.
2024-12-02-04:58:33-root-INFO: grad norm: 9.672 9.616 1.041
2024-12-02-04:58:33-root-INFO: Loss Change: 190.031 -> 186.398
2024-12-02-04:58:33-root-INFO: Regularization Change: 0.000 -> 1.115
2024-12-02-04:58:33-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-04:58:33-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-04:58:34-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-04:58:34-root-INFO: grad norm: 9.123 9.013 1.411
2024-12-02-04:58:34-root-INFO: Loss too large (186.626->187.687)! Learning rate decreased to 0.05711.
2024-12-02-04:58:35-root-INFO: grad norm: 11.322 11.246 1.307
2024-12-02-04:58:36-root-INFO: Loss too large (186.368->186.743)! Learning rate decreased to 0.04569.
2024-12-02-04:58:37-root-INFO: grad norm: 10.042 9.963 1.260
2024-12-02-04:58:38-root-INFO: grad norm: 9.096 9.025 1.135
2024-12-02-04:58:39-root-INFO: grad norm: 8.364 8.286 1.140
2024-12-02-04:58:39-root-INFO: Loss Change: 186.626 -> 183.299
2024-12-02-04:58:39-root-INFO: Regularization Change: 0.000 -> 0.862
2024-12-02-04:58:39-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-04:58:39-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-04:58:40-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-04:58:40-root-INFO: grad norm: 6.378 6.282 1.100
2024-12-02-04:58:41-root-INFO: grad norm: 9.849 9.789 1.083
2024-12-02-04:58:41-root-INFO: Loss too large (182.464->184.475)! Learning rate decreased to 0.05877.
2024-12-02-04:58:42-root-INFO: Loss too large (182.464->182.648)! Learning rate decreased to 0.04702.
2024-12-02-04:58:43-root-INFO: grad norm: 8.827 8.771 0.993
2024-12-02-04:58:44-root-INFO: grad norm: 8.135 8.067 1.052
2024-12-02-04:58:45-root-INFO: grad norm: 7.644 7.582 0.972
2024-12-02-04:58:45-root-INFO: Loss Change: 182.739 -> 179.805
2024-12-02-04:58:45-root-INFO: Regularization Change: 0.000 -> 0.878
2024-12-02-04:58:45-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-04:58:45-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-04:58:46-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-04:58:46-root-INFO: grad norm: 10.515 10.377 1.701
2024-12-02-04:58:46-root-INFO: Loss too large (180.496->183.280)! Learning rate decreased to 0.06047.
2024-12-02-04:58:47-root-INFO: Loss too large (180.496->180.978)! Learning rate decreased to 0.04837.
2024-12-02-04:58:47-root-INFO: grad norm: 9.926 9.846 1.253
2024-12-02-04:58:48-root-INFO: grad norm: 9.561 9.468 1.326
2024-12-02-04:58:49-root-INFO: grad norm: 9.269 9.193 1.183
2024-12-02-04:58:50-root-INFO: grad norm: 9.014 8.929 1.233
2024-12-02-04:58:51-root-INFO: Loss Change: 180.496 -> 177.435
2024-12-02-04:58:51-root-INFO: Regularization Change: 0.000 -> 0.837
2024-12-02-04:58:51-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-04:58:51-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-04:58:51-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-04:58:52-root-INFO: grad norm: 6.575 6.502 0.979
2024-12-02-04:58:52-root-INFO: Loss too large (177.075->177.207)! Learning rate decreased to 0.06220.
2024-12-02-04:58:53-root-INFO: grad norm: 8.077 8.010 1.034
2024-12-02-04:58:53-root-INFO: Loss too large (176.624->176.682)! Learning rate decreased to 0.04976.
2024-12-02-04:58:54-root-INFO: grad norm: 7.642 7.585 0.930
2024-12-02-04:58:55-root-INFO: grad norm: 7.376 7.303 1.034
2024-12-02-04:58:56-root-INFO: grad norm: 7.184 7.121 0.946
2024-12-02-04:58:57-root-INFO: Loss Change: 177.075 -> 174.435
2024-12-02-04:58:57-root-INFO: Regularization Change: 0.000 -> 0.795
2024-12-02-04:58:57-root-INFO: Undo step: 100
2024-12-02-04:58:57-root-INFO: Undo step: 101
2024-12-02-04:58:57-root-INFO: Undo step: 102
2024-12-02-04:58:57-root-INFO: Undo step: 103
2024-12-02-04:58:57-root-INFO: Undo step: 104
2024-12-02-04:58:57-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-04:58:58-root-INFO: grad norm: 63.046 62.429 8.801
2024-12-02-04:58:59-root-INFO: grad norm: 40.566 40.066 6.349
2024-12-02-04:59:00-root-INFO: grad norm: 37.758 37.340 5.605
2024-12-02-04:59:01-root-INFO: grad norm: 43.662 43.227 6.151
2024-12-02-04:59:01-root-INFO: Loss too large (246.353->256.489)! Learning rate decreased to 0.05391.
2024-12-02-04:59:02-root-INFO: grad norm: 36.523 36.090 5.609
2024-12-02-04:59:03-root-INFO: Loss Change: 418.641 -> 221.762
2024-12-02-04:59:03-root-INFO: Regularization Change: 0.000 -> 47.330
2024-12-02-04:59:03-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-04:59:03-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-04:59:03-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-04:59:03-root-INFO: grad norm: 34.012 33.649 4.952
2024-12-02-04:59:04-root-INFO: Loss too large (224.918->234.168)! Learning rate decreased to 0.05550.
2024-12-02-04:59:05-root-INFO: grad norm: 30.535 30.196 4.541
2024-12-02-04:59:06-root-INFO: grad norm: 27.365 27.114 3.703
2024-12-02-04:59:07-root-INFO: grad norm: 25.857 25.577 3.789
2024-12-02-04:59:08-root-INFO: grad norm: 24.711 24.496 3.250
2024-12-02-04:59:08-root-INFO: Loss Change: 224.918 -> 203.241
2024-12-02-04:59:08-root-INFO: Regularization Change: 0.000 -> 6.476
2024-12-02-04:59:08-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-04:59:08-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-04:59:09-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-04:59:09-root-INFO: grad norm: 21.773 21.576 2.921
2024-12-02-04:59:09-root-INFO: Loss too large (201.426->206.943)! Learning rate decreased to 0.05711.
2024-12-02-04:59:10-root-INFO: grad norm: 21.986 21.800 2.846
2024-12-02-04:59:11-root-INFO: grad norm: 22.318 22.104 3.086
2024-12-02-04:59:12-root-INFO: grad norm: 22.916 22.735 2.876
2024-12-02-04:59:13-root-INFO: grad norm: 23.361 23.146 3.161
2024-12-02-04:59:14-root-INFO: Loss Change: 201.426 -> 195.049
2024-12-02-04:59:14-root-INFO: Regularization Change: 0.000 -> 3.744
2024-12-02-04:59:14-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-04:59:14-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-04:59:14-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-04:59:15-root-INFO: grad norm: 26.904 26.661 3.607
2024-12-02-04:59:15-root-INFO: Loss too large (196.921->208.613)! Learning rate decreased to 0.05877.
2024-12-02-04:59:16-root-INFO: grad norm: 26.782 26.545 3.550
2024-12-02-04:59:17-root-INFO: grad norm: 26.404 26.200 3.277
2024-12-02-04:59:18-root-INFO: grad norm: 25.961 25.727 3.478
2024-12-02-04:59:19-root-INFO: grad norm: 25.466 25.271 3.143
2024-12-02-04:59:20-root-INFO: Loss Change: 196.921 -> 191.478
2024-12-02-04:59:20-root-INFO: Regularization Change: 0.000 -> 3.862
2024-12-02-04:59:20-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-04:59:20-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-04:59:20-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-04:59:20-root-INFO: grad norm: 22.942 22.767 2.828
2024-12-02-04:59:21-root-INFO: Loss too large (189.762->197.437)! Learning rate decreased to 0.06047.
2024-12-02-04:59:22-root-INFO: grad norm: 22.949 22.774 2.825
2024-12-02-04:59:23-root-INFO: grad norm: 23.063 22.865 3.021
2024-12-02-04:59:24-root-INFO: grad norm: 23.256 23.081 2.851
2024-12-02-04:59:24-root-INFO: Loss too large (186.545->186.618)! Learning rate decreased to 0.04837.
2024-12-02-04:59:25-root-INFO: grad norm: 15.255 15.108 2.113
2024-12-02-04:59:26-root-INFO: Loss Change: 189.762 -> 178.782
2024-12-02-04:59:26-root-INFO: Regularization Change: 0.000 -> 2.066
2024-12-02-04:59:26-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-04:59:26-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-04:59:26-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-04:59:26-root-INFO: grad norm: 13.556 13.386 2.140
2024-12-02-04:59:27-root-INFO: Loss too large (179.869->182.728)! Learning rate decreased to 0.06220.
2024-12-02-04:59:28-root-INFO: grad norm: 14.527 14.391 1.986
2024-12-02-04:59:29-root-INFO: grad norm: 15.970 15.833 2.088
2024-12-02-04:59:29-root-INFO: Loss too large (179.200->179.445)! Learning rate decreased to 0.04976.
2024-12-02-04:59:30-root-INFO: grad norm: 11.520 11.409 1.598
2024-12-02-04:59:31-root-INFO: grad norm: 8.535 8.449 1.210
2024-12-02-04:59:32-root-INFO: Loss Change: 179.869 -> 174.362
2024-12-02-04:59:32-root-INFO: Regularization Change: 0.000 -> 1.238
2024-12-02-04:59:32-root-INFO: Undo step: 100
2024-12-02-04:59:32-root-INFO: Undo step: 101
2024-12-02-04:59:32-root-INFO: Undo step: 102
2024-12-02-04:59:32-root-INFO: Undo step: 103
2024-12-02-04:59:32-root-INFO: Undo step: 104
2024-12-02-04:59:32-root-INFO: step: 105 lr_xt 0.06739236
2024-12-02-04:59:32-root-INFO: grad norm: 66.220 65.664 8.562
2024-12-02-04:59:33-root-INFO: grad norm: 39.441 38.976 6.034
2024-12-02-04:59:34-root-INFO: grad norm: 28.560 28.226 4.356
2024-12-02-04:59:35-root-INFO: grad norm: 28.841 28.600 3.723
2024-12-02-04:59:36-root-INFO: grad norm: 32.220 32.050 3.300
2024-12-02-04:59:37-root-INFO: Loss Change: 413.085 -> 231.818
2024-12-02-04:59:37-root-INFO: Regularization Change: 0.000 -> 47.363
2024-12-02-04:59:37-root-INFO: Learning rate of xt decay: 0.16715 -> 0.16916.
2024-12-02-04:59:37-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-04:59:37-root-INFO: step: 104 lr_xt 0.06936934
2024-12-02-04:59:38-root-INFO: grad norm: 38.078 37.915 3.527
2024-12-02-04:59:38-root-INFO: Loss too large (231.726->236.291)! Learning rate decreased to 0.05550.
2024-12-02-04:59:39-root-INFO: grad norm: 28.434 28.302 2.731
2024-12-02-04:59:40-root-INFO: grad norm: 21.146 21.032 2.196
2024-12-02-04:59:41-root-INFO: grad norm: 18.144 18.033 2.000
2024-12-02-04:59:42-root-INFO: grad norm: 16.346 16.237 1.881
2024-12-02-04:59:43-root-INFO: Loss Change: 231.726 -> 199.311
2024-12-02-04:59:43-root-INFO: Regularization Change: 0.000 -> 6.230
2024-12-02-04:59:43-root-INFO: Learning rate of xt decay: 0.16916 -> 0.17119.
2024-12-02-04:59:43-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-04:59:43-root-INFO: step: 103 lr_xt 0.07139284
2024-12-02-04:59:43-root-INFO: grad norm: 17.513 17.378 2.173
2024-12-02-04:59:44-root-INFO: Loss too large (200.086->202.004)! Learning rate decreased to 0.05711.
2024-12-02-04:59:45-root-INFO: grad norm: 17.043 16.914 2.089
2024-12-02-04:59:46-root-INFO: grad norm: 16.895 16.773 2.020
2024-12-02-04:59:47-root-INFO: grad norm: 16.858 16.717 2.176
2024-12-02-04:59:48-root-INFO: grad norm: 17.081 16.950 2.113
2024-12-02-04:59:48-root-INFO: Loss Change: 200.086 -> 192.240
2024-12-02-04:59:48-root-INFO: Regularization Change: 0.000 -> 3.152
2024-12-02-04:59:48-root-INFO: Learning rate of xt decay: 0.17119 -> 0.17324.
2024-12-02-04:59:48-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-04:59:49-root-INFO: step: 102 lr_xt 0.07346356
2024-12-02-04:59:49-root-INFO: grad norm: 15.481 15.369 1.862
2024-12-02-04:59:49-root-INFO: Loss too large (190.922->193.492)! Learning rate decreased to 0.05877.
2024-12-02-04:59:50-root-INFO: grad norm: 16.016 15.885 2.042
2024-12-02-04:59:51-root-INFO: grad norm: 16.732 16.584 2.218
2024-12-02-04:59:52-root-INFO: grad norm: 17.761 17.609 2.318
2024-12-02-04:59:53-root-INFO: grad norm: 18.750 18.571 2.582
2024-12-02-04:59:54-root-INFO: Loss Change: 190.922 -> 187.431
2024-12-02-04:59:54-root-INFO: Regularization Change: 0.000 -> 2.588
2024-12-02-04:59:54-root-INFO: Learning rate of xt decay: 0.17324 -> 0.17532.
2024-12-02-04:59:54-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-04:59:54-root-INFO: step: 101 lr_xt 0.07558219
2024-12-02-04:59:55-root-INFO: grad norm: 23.334 23.084 3.411
2024-12-02-04:59:55-root-INFO: Loss too large (189.553->199.274)! Learning rate decreased to 0.06047.
2024-12-02-04:59:55-root-INFO: Loss too large (189.553->189.933)! Learning rate decreased to 0.04837.
2024-12-02-04:59:56-root-INFO: grad norm: 15.949 15.784 2.286
2024-12-02-04:59:57-root-INFO: grad norm: 11.125 11.010 1.601
2024-12-02-04:59:58-root-INFO: grad norm: 8.435 8.329 1.334
2024-12-02-04:59:59-root-INFO: grad norm: 6.660 6.570 1.092
2024-12-02-05:00:00-root-INFO: Loss Change: 189.553 -> 178.411
2024-12-02-05:00:00-root-INFO: Regularization Change: 0.000 -> 1.502
2024-12-02-05:00:00-root-INFO: Learning rate of xt decay: 0.17532 -> 0.17743.
2024-12-02-05:00:00-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-05:00:00-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-05:00:01-root-INFO: grad norm: 4.681 4.564 1.044
2024-12-02-05:00:02-root-INFO: grad norm: 4.733 4.650 0.883
2024-12-02-05:00:03-root-INFO: grad norm: 5.728 5.658 0.893
2024-12-02-05:00:04-root-INFO: grad norm: 8.179 8.117 1.012
2024-12-02-05:00:04-root-INFO: Loss too large (175.658->176.134)! Learning rate decreased to 0.06220.
2024-12-02-05:00:05-root-INFO: grad norm: 9.008 8.947 1.043
2024-12-02-05:00:06-root-INFO: Loss Change: 178.212 -> 174.674
2024-12-02-05:00:06-root-INFO: Regularization Change: 0.000 -> 1.793
2024-12-02-05:00:06-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-05:00:06-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-05:00:06-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-05:00:06-root-INFO: grad norm: 12.096 11.986 1.626
2024-12-02-05:00:07-root-INFO: Loss too large (175.052->177.738)! Learning rate decreased to 0.06397.
2024-12-02-05:00:08-root-INFO: grad norm: 13.658 13.563 1.606
2024-12-02-05:00:09-root-INFO: grad norm: 15.715 15.609 1.823
2024-12-02-05:00:09-root-INFO: Loss too large (174.998->175.715)! Learning rate decreased to 0.05118.
2024-12-02-05:00:10-root-INFO: grad norm: 11.896 11.799 1.520
2024-12-02-05:00:11-root-INFO: grad norm: 9.088 9.007 1.210
2024-12-02-05:00:12-root-INFO: Loss Change: 175.052 -> 170.566
2024-12-02-05:00:12-root-INFO: Regularization Change: 0.000 -> 1.167
2024-12-02-05:00:12-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-05:00:12-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-05:00:12-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-05:00:12-root-INFO: grad norm: 5.533 5.457 0.908
2024-12-02-05:00:13-root-INFO: grad norm: 7.625 7.567 0.938
2024-12-02-05:00:14-root-INFO: Loss too large (169.617->170.218)! Learning rate decreased to 0.06579.
2024-12-02-05:00:15-root-INFO: grad norm: 8.832 8.768 1.063
2024-12-02-05:00:16-root-INFO: grad norm: 10.591 10.516 1.260
2024-12-02-05:00:16-root-INFO: Loss too large (169.076->169.263)! Learning rate decreased to 0.05263.
2024-12-02-05:00:17-root-INFO: grad norm: 8.604 8.529 1.136
2024-12-02-05:00:18-root-INFO: Loss Change: 170.156 -> 167.175
2024-12-02-05:00:18-root-INFO: Regularization Change: 0.000 -> 1.135
2024-12-02-05:00:18-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-05:00:18-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-05:00:18-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-05:00:18-root-INFO: grad norm: 10.852 10.699 1.821
2024-12-02-05:00:19-root-INFO: Loss too large (168.026->170.766)! Learning rate decreased to 0.06764.
2024-12-02-05:00:19-root-INFO: Loss too large (168.026->168.255)! Learning rate decreased to 0.05411.
2024-12-02-05:00:20-root-INFO: grad norm: 8.997 8.905 1.283
2024-12-02-05:00:21-root-INFO: grad norm: 7.600 7.515 1.136
2024-12-02-05:00:22-root-INFO: grad norm: 6.582 6.505 1.005
2024-12-02-05:00:23-root-INFO: grad norm: 5.770 5.699 0.902
2024-12-02-05:00:24-root-INFO: Loss Change: 168.026 -> 164.127
2024-12-02-05:00:24-root-INFO: Regularization Change: 0.000 -> 0.876
2024-12-02-05:00:24-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-05:00:24-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-05:00:24-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-05:00:24-root-INFO: grad norm: 4.105 4.021 0.825
2024-12-02-05:00:25-root-INFO: grad norm: 4.919 4.864 0.730
2024-12-02-05:00:26-root-INFO: grad norm: 7.566 7.520 0.836
2024-12-02-05:00:27-root-INFO: Loss too large (163.029->164.060)! Learning rate decreased to 0.06953.
2024-12-02-05:00:28-root-INFO: grad norm: 9.293 9.235 1.037
2024-12-02-05:00:28-root-INFO: Loss too large (162.910->163.094)! Learning rate decreased to 0.05563.
2024-12-02-05:00:29-root-INFO: grad norm: 7.875 7.815 0.976
2024-12-02-05:00:30-root-INFO: Loss Change: 163.984 -> 161.394
2024-12-02-05:00:30-root-INFO: Regularization Change: 0.000 -> 1.147
2024-12-02-05:00:30-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-05:00:30-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-05:00:30-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-05:00:30-root-INFO: grad norm: 10.320 10.185 1.663
2024-12-02-05:00:31-root-INFO: Loss too large (162.443->165.440)! Learning rate decreased to 0.07147.
2024-12-02-05:00:31-root-INFO: Loss too large (162.443->162.903)! Learning rate decreased to 0.05718.
2024-12-02-05:00:32-root-INFO: grad norm: 8.950 8.868 1.209
2024-12-02-05:00:33-root-INFO: grad norm: 7.877 7.795 1.139
2024-12-02-05:00:34-root-INFO: grad norm: 7.041 6.968 1.011
2024-12-02-05:00:35-root-INFO: grad norm: 6.361 6.291 0.940
2024-12-02-05:00:36-root-INFO: Loss Change: 162.443 -> 159.031
2024-12-02-05:00:36-root-INFO: Regularization Change: 0.000 -> 0.866
2024-12-02-05:00:36-root-INFO: Undo step: 95
2024-12-02-05:00:36-root-INFO: Undo step: 96
2024-12-02-05:00:36-root-INFO: Undo step: 97
2024-12-02-05:00:36-root-INFO: Undo step: 98
2024-12-02-05:00:36-root-INFO: Undo step: 99
2024-12-02-05:00:36-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-05:00:36-root-INFO: grad norm: 57.586 56.777 9.621
2024-12-02-05:00:37-root-INFO: grad norm: 33.482 33.071 5.232
2024-12-02-05:00:38-root-INFO: grad norm: 33.399 33.027 4.972
2024-12-02-05:00:39-root-INFO: grad norm: 41.297 40.959 5.278
2024-12-02-05:00:40-root-INFO: Loss too large (229.927->234.191)! Learning rate decreased to 0.06220.
2024-12-02-05:00:41-root-INFO: grad norm: 29.876 29.548 4.417
2024-12-02-05:00:41-root-INFO: Loss Change: 374.242 -> 203.833
2024-12-02-05:00:41-root-INFO: Regularization Change: 0.000 -> 44.865
2024-12-02-05:00:41-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-05:00:41-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-05:00:42-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-05:00:42-root-INFO: grad norm: 25.802 25.510 3.871
2024-12-02-05:00:42-root-INFO: Loss too large (205.254->207.177)! Learning rate decreased to 0.06397.
2024-12-02-05:00:43-root-INFO: grad norm: 22.018 21.738 3.502
2024-12-02-05:00:44-root-INFO: grad norm: 20.042 19.818 2.990
2024-12-02-05:00:45-root-INFO: grad norm: 19.335 19.097 3.026
2024-12-02-05:00:46-root-INFO: grad norm: 19.291 19.094 2.754
2024-12-02-05:00:47-root-INFO: Loss Change: 205.254 -> 185.172
2024-12-02-05:00:47-root-INFO: Regularization Change: 0.000 -> 6.883
2024-12-02-05:00:47-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-05:00:47-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-05:00:47-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-05:00:48-root-INFO: grad norm: 17.408 17.253 2.315
2024-12-02-05:00:48-root-INFO: Loss too large (183.736->187.446)! Learning rate decreased to 0.06579.
2024-12-02-05:00:49-root-INFO: grad norm: 18.145 17.978 2.460
2024-12-02-05:00:50-root-INFO: grad norm: 18.923 18.741 2.620
2024-12-02-05:00:51-root-INFO: grad norm: 19.863 19.687 2.638
2024-12-02-05:00:52-root-INFO: grad norm: 20.392 20.191 2.861
2024-12-02-05:00:53-root-INFO: Loss Change: 183.736 -> 178.391
2024-12-02-05:00:53-root-INFO: Regularization Change: 0.000 -> 3.818
2024-12-02-05:00:53-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-05:00:53-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-05:00:53-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-05:00:53-root-INFO: grad norm: 23.975 23.689 3.695
2024-12-02-05:00:54-root-INFO: Loss too large (180.456->191.523)! Learning rate decreased to 0.06764.
2024-12-02-05:00:54-root-INFO: Loss too large (180.456->180.714)! Learning rate decreased to 0.05411.
2024-12-02-05:00:55-root-INFO: grad norm: 15.830 15.644 2.413
2024-12-02-05:00:56-root-INFO: grad norm: 10.435 10.308 1.620
2024-12-02-05:00:57-root-INFO: grad norm: 7.735 7.622 1.318
2024-12-02-05:00:58-root-INFO: grad norm: 6.052 5.955 1.081
2024-12-02-05:00:59-root-INFO: Loss Change: 180.456 -> 167.415
2024-12-02-05:00:59-root-INFO: Regularization Change: 0.000 -> 1.908
2024-12-02-05:00:59-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-05:00:59-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-05:00:59-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-05:00:59-root-INFO: grad norm: 4.597 4.495 0.961
2024-12-02-05:01:00-root-INFO: grad norm: 5.075 5.001 0.862
2024-12-02-05:01:01-root-INFO: grad norm: 7.866 7.809 0.947
2024-12-02-05:01:01-root-INFO: Loss too large (165.454->165.995)! Learning rate decreased to 0.06953.
2024-12-02-05:01:02-root-INFO: grad norm: 8.070 8.009 0.989
2024-12-02-05:01:03-root-INFO: grad norm: 8.658 8.599 1.010
2024-12-02-05:01:04-root-INFO: Loss Change: 167.268 -> 163.846
2024-12-02-05:01:04-root-INFO: Regularization Change: 0.000 -> 1.934
2024-12-02-05:01:04-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-05:01:04-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-05:01:04-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-05:01:05-root-INFO: grad norm: 11.894 11.775 1.677
2024-12-02-05:01:05-root-INFO: Loss too large (164.594->167.640)! Learning rate decreased to 0.07147.
2024-12-02-05:01:06-root-INFO: Loss too large (164.594->164.743)! Learning rate decreased to 0.05718.
2024-12-02-05:01:07-root-INFO: grad norm: 9.078 9.000 1.193
2024-12-02-05:01:08-root-INFO: grad norm: 6.864 6.784 1.044
2024-12-02-05:01:09-root-INFO: grad norm: 5.672 5.598 0.911
2024-12-02-05:01:10-root-INFO: grad norm: 4.924 4.850 0.851
2024-12-02-05:01:10-root-INFO: Loss Change: 164.594 -> 159.761
2024-12-02-05:01:10-root-INFO: Regularization Change: 0.000 -> 1.056
2024-12-02-05:01:10-root-INFO: Undo step: 95
2024-12-02-05:01:10-root-INFO: Undo step: 96
2024-12-02-05:01:10-root-INFO: Undo step: 97
2024-12-02-05:01:10-root-INFO: Undo step: 98
2024-12-02-05:01:10-root-INFO: Undo step: 99
2024-12-02-05:01:11-root-INFO: step: 100 lr_xt 0.07774943
2024-12-02-05:01:11-root-INFO: grad norm: 72.361 71.954 7.665
2024-12-02-05:01:12-root-INFO: grad norm: 55.406 55.012 6.597
2024-12-02-05:01:13-root-INFO: grad norm: 40.531 40.291 4.397
2024-12-02-05:01:14-root-INFO: grad norm: 30.546 30.269 4.101
2024-12-02-05:01:15-root-INFO: grad norm: 30.974 30.664 4.372
2024-12-02-05:01:16-root-INFO: Loss Change: 389.316 -> 211.039
2024-12-02-05:01:16-root-INFO: Regularization Change: 0.000 -> 51.308
2024-12-02-05:01:16-root-INFO: Learning rate of xt decay: 0.17743 -> 0.17956.
2024-12-02-05:01:16-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-05:01:16-root-INFO: step: 99 lr_xt 0.07996596
2024-12-02-05:01:16-root-INFO: grad norm: 38.005 37.537 5.947
2024-12-02-05:01:17-root-INFO: Loss too large (213.869->221.593)! Learning rate decreased to 0.06397.
2024-12-02-05:01:18-root-INFO: grad norm: 29.077 28.718 4.558
2024-12-02-05:01:19-root-INFO: grad norm: 22.589 22.345 3.309
2024-12-02-05:01:20-root-INFO: grad norm: 19.834 19.578 3.173
2024-12-02-05:01:21-root-INFO: grad norm: 17.833 17.642 2.604
2024-12-02-05:01:21-root-INFO: Loss Change: 213.869 -> 181.058
2024-12-02-05:01:21-root-INFO: Regularization Change: 0.000 -> 7.305
2024-12-02-05:01:21-root-INFO: Learning rate of xt decay: 0.17956 -> 0.18171.
2024-12-02-05:01:21-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-05:01:22-root-INFO: step: 98 lr_xt 0.08223248
2024-12-02-05:01:22-root-INFO: grad norm: 15.130 14.988 2.068
2024-12-02-05:01:22-root-INFO: Loss too large (179.918->181.487)! Learning rate decreased to 0.06579.
2024-12-02-05:01:23-root-INFO: grad norm: 14.805 14.660 2.067
2024-12-02-05:01:24-root-INFO: grad norm: 15.075 14.929 2.098
2024-12-02-05:01:25-root-INFO: grad norm: 15.179 15.035 2.083
2024-12-02-05:01:26-root-INFO: grad norm: 15.320 15.158 2.219
2024-12-02-05:01:27-root-INFO: Loss Change: 179.918 -> 172.908
2024-12-02-05:01:27-root-INFO: Regularization Change: 0.000 -> 3.335
2024-12-02-05:01:27-root-INFO: Learning rate of xt decay: 0.18171 -> 0.18389.
2024-12-02-05:01:27-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-05:01:27-root-INFO: step: 97 lr_xt 0.08454965
2024-12-02-05:01:28-root-INFO: grad norm: 19.190 18.930 3.148
2024-12-02-05:01:28-root-INFO: Loss too large (174.534->181.005)! Learning rate decreased to 0.06764.
2024-12-02-05:01:29-root-INFO: grad norm: 19.608 19.388 2.931
2024-12-02-05:01:30-root-INFO: grad norm: 20.507 20.291 2.968
2024-12-02-05:01:30-root-INFO: Loss too large (172.979->173.259)! Learning rate decreased to 0.05411.
2024-12-02-05:01:31-root-INFO: grad norm: 13.864 13.695 2.159
2024-12-02-05:01:32-root-INFO: grad norm: 9.507 9.405 1.387
2024-12-02-05:01:33-root-INFO: Loss Change: 174.534 -> 164.743
2024-12-02-05:01:33-root-INFO: Regularization Change: 0.000 -> 2.028
2024-12-02-05:01:33-root-INFO: Learning rate of xt decay: 0.18389 -> 0.18610.
2024-12-02-05:01:33-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-05:01:33-root-INFO: step: 96 lr_xt 0.08691815
2024-12-02-05:01:34-root-INFO: grad norm: 5.911 5.830 0.976
2024-12-02-05:01:34-root-INFO: grad norm: 8.252 8.184 1.063
2024-12-02-05:01:35-root-INFO: Loss too large (163.650->164.396)! Learning rate decreased to 0.06953.
2024-12-02-05:01:36-root-INFO: grad norm: 10.023 9.956 1.159
2024-12-02-05:01:37-root-INFO: grad norm: 11.257 11.177 1.342
2024-12-02-05:01:38-root-INFO: grad norm: 12.738 12.646 1.532
2024-12-02-05:01:38-root-INFO: Loss too large (162.707->162.850)! Learning rate decreased to 0.05563.
2024-12-02-05:01:39-root-INFO: Loss Change: 164.352 -> 160.954
2024-12-02-05:01:39-root-INFO: Regularization Change: 0.000 -> 1.725
2024-12-02-05:01:39-root-INFO: Learning rate of xt decay: 0.18610 -> 0.18833.
2024-12-02-05:01:39-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-05:01:39-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-05:01:39-root-INFO: grad norm: 13.060 12.891 2.093
2024-12-02-05:01:40-root-INFO: Loss too large (162.259->166.466)! Learning rate decreased to 0.07147.
2024-12-02-05:01:40-root-INFO: Loss too large (162.259->162.667)! Learning rate decreased to 0.05718.
2024-12-02-05:01:41-root-INFO: grad norm: 10.286 10.191 1.398
2024-12-02-05:01:42-root-INFO: grad norm: 8.442 8.353 1.223
2024-12-02-05:01:43-root-INFO: grad norm: 7.084 7.006 1.046
2024-12-02-05:01:44-root-INFO: grad norm: 5.956 5.884 0.926
2024-12-02-05:01:45-root-INFO: Loss Change: 162.259 -> 157.019
2024-12-02-05:01:45-root-INFO: Regularization Change: 0.000 -> 1.146
2024-12-02-05:01:45-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-05:01:45-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-05:01:45-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-05:01:45-root-INFO: grad norm: 4.688 4.590 0.949
2024-12-02-05:01:46-root-INFO: grad norm: 8.815 8.772 0.866
2024-12-02-05:01:47-root-INFO: Loss too large (156.111->156.746)! Learning rate decreased to 0.07345.
2024-12-02-05:01:47-root-INFO: Loss too large (156.111->156.213)! Learning rate decreased to 0.05876.
2024-12-02-05:01:48-root-INFO: grad norm: 5.123 5.061 0.794
2024-12-02-05:01:49-root-INFO: grad norm: 4.529 4.472 0.719
2024-12-02-05:01:50-root-INFO: grad norm: 3.587 3.525 0.662
2024-12-02-05:01:51-root-INFO: Loss Change: 156.706 -> 153.467
2024-12-02-05:01:51-root-INFO: Regularization Change: 0.000 -> 1.035
2024-12-02-05:01:51-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-05:01:51-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-05:01:51-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-05:01:51-root-INFO: grad norm: 6.416 6.274 1.344
2024-12-02-05:01:52-root-INFO: Loss too large (153.719->153.946)! Learning rate decreased to 0.07547.
2024-12-02-05:01:53-root-INFO: grad norm: 7.217 7.136 1.080
2024-12-02-05:01:54-root-INFO: grad norm: 9.118 9.027 1.288
2024-12-02-05:01:54-root-INFO: Loss too large (152.949->153.430)! Learning rate decreased to 0.06038.
2024-12-02-05:01:55-root-INFO: grad norm: 7.862 7.786 1.087
2024-12-02-05:01:56-root-INFO: grad norm: 6.577 6.511 0.924
2024-12-02-05:01:57-root-INFO: Loss Change: 153.719 -> 150.845
2024-12-02-05:01:57-root-INFO: Regularization Change: 0.000 -> 1.044
2024-12-02-05:01:57-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-05:01:57-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-05:01:57-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-05:01:57-root-INFO: grad norm: 4.949 4.882 0.811
2024-12-02-05:01:58-root-INFO: grad norm: 7.616 7.576 0.780
2024-12-02-05:01:59-root-INFO: Loss too large (150.341->151.750)! Learning rate decreased to 0.07753.
2024-12-02-05:01:59-root-INFO: Loss too large (150.341->150.420)! Learning rate decreased to 0.06203.
2024-12-02-05:02:00-root-INFO: grad norm: 6.706 6.656 0.820
2024-12-02-05:02:01-root-INFO: grad norm: 6.173 6.123 0.784
2024-12-02-05:02:02-root-INFO: grad norm: 5.714 5.660 0.784
2024-12-02-05:02:03-root-INFO: Loss Change: 150.641 -> 148.006
2024-12-02-05:02:03-root-INFO: Regularization Change: 0.000 -> 0.984
2024-12-02-05:02:03-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-05:02:03-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-05:02:03-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-05:02:03-root-INFO: grad norm: 8.463 8.348 1.391
2024-12-02-05:02:04-root-INFO: Loss too large (148.532->150.561)! Learning rate decreased to 0.07964.
2024-12-02-05:02:04-root-INFO: Loss too large (148.532->149.004)! Learning rate decreased to 0.06371.
2024-12-02-05:02:05-root-INFO: grad norm: 7.171 7.102 0.988
2024-12-02-05:02:06-root-INFO: grad norm: 6.035 5.971 0.874
2024-12-02-05:02:07-root-INFO: grad norm: 5.542 5.491 0.748
2024-12-02-05:02:08-root-INFO: grad norm: 5.157 5.105 0.733
2024-12-02-05:02:09-root-INFO: Loss Change: 148.532 -> 145.663
2024-12-02-05:02:09-root-INFO: Regularization Change: 0.000 -> 0.841
2024-12-02-05:02:09-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-05:02:09-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-05:02:09-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-05:02:09-root-INFO: grad norm: 4.403 4.328 0.812
2024-12-02-05:02:10-root-INFO: grad norm: 5.851 5.813 0.665
2024-12-02-05:02:11-root-INFO: Loss too large (145.358->145.639)! Learning rate decreased to 0.08180.
2024-12-02-05:02:12-root-INFO: grad norm: 7.167 7.137 0.653
2024-12-02-05:02:12-root-INFO: Loss too large (144.821->145.101)! Learning rate decreased to 0.06544.
2024-12-02-05:02:13-root-INFO: grad norm: 6.143 6.107 0.671
2024-12-02-05:02:14-root-INFO: grad norm: 5.015 4.982 0.574
2024-12-02-05:02:15-root-INFO: Loss Change: 145.625 -> 143.194
2024-12-02-05:02:15-root-INFO: Regularization Change: 0.000 -> 1.003
2024-12-02-05:02:15-root-INFO: Undo step: 90
2024-12-02-05:02:15-root-INFO: Undo step: 91
2024-12-02-05:02:15-root-INFO: Undo step: 92
2024-12-02-05:02:15-root-INFO: Undo step: 93
2024-12-02-05:02:15-root-INFO: Undo step: 94
2024-12-02-05:02:15-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-05:02:15-root-INFO: grad norm: 47.247 46.722 7.023
2024-12-02-05:02:16-root-INFO: grad norm: 37.561 37.231 4.964
2024-12-02-05:02:17-root-INFO: grad norm: 39.673 39.188 6.184
2024-12-02-05:02:18-root-INFO: grad norm: 45.378 45.053 5.423
2024-12-02-05:02:19-root-INFO: Loss too large (225.812->239.573)! Learning rate decreased to 0.07147.
2024-12-02-05:02:20-root-INFO: grad norm: 36.404 35.995 5.445
2024-12-02-05:02:20-root-INFO: Loss Change: 324.014 -> 192.033
2024-12-02-05:02:20-root-INFO: Regularization Change: 0.000 -> 48.444
2024-12-02-05:02:20-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-05:02:20-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-05:02:21-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-05:02:21-root-INFO: grad norm: 30.932 30.630 4.306
2024-12-02-05:02:21-root-INFO: Loss too large (194.537->202.628)! Learning rate decreased to 0.07345.
2024-12-02-05:02:22-root-INFO: grad norm: 27.004 26.686 4.136
2024-12-02-05:02:23-root-INFO: grad norm: 24.660 24.453 3.189
2024-12-02-05:02:24-root-INFO: grad norm: 23.423 23.178 3.380
2024-12-02-05:02:25-root-INFO: grad norm: 22.568 22.383 2.884
2024-12-02-05:02:26-root-INFO: Loss Change: 194.537 -> 171.690
2024-12-02-05:02:26-root-INFO: Regularization Change: 0.000 -> 9.052
2024-12-02-05:02:26-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-05:02:26-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-05:02:26-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-05:02:26-root-INFO: grad norm: 19.979 19.827 2.465
2024-12-02-05:02:27-root-INFO: Loss too large (169.618->175.460)! Learning rate decreased to 0.07547.
2024-12-02-05:02:28-root-INFO: grad norm: 19.831 19.678 2.452
2024-12-02-05:02:29-root-INFO: grad norm: 19.752 19.581 2.597
2024-12-02-05:02:30-root-INFO: grad norm: 19.779 19.624 2.467
2024-12-02-05:02:31-root-INFO: grad norm: 19.771 19.594 2.636
2024-12-02-05:02:31-root-INFO: Loss Change: 169.618 -> 161.921
2024-12-02-05:02:31-root-INFO: Regularization Change: 0.000 -> 4.813
2024-12-02-05:02:31-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-05:02:31-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-05:02:32-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-05:02:32-root-INFO: grad norm: 21.973 21.746 3.152
2024-12-02-05:02:32-root-INFO: Loss too large (163.854->173.588)! Learning rate decreased to 0.07753.
2024-12-02-05:02:33-root-INFO: grad norm: 21.493 21.283 2.999
2024-12-02-05:02:34-root-INFO: grad norm: 20.868 20.684 2.765
2024-12-02-05:02:35-root-INFO: grad norm: 20.444 20.242 2.865
2024-12-02-05:02:36-root-INFO: grad norm: 20.019 19.844 2.638
2024-12-02-05:02:37-root-INFO: Loss Change: 163.854 -> 157.996
2024-12-02-05:02:37-root-INFO: Regularization Change: 0.000 -> 4.492
2024-12-02-05:02:37-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-05:02:37-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-05:02:37-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-05:02:38-root-INFO: grad norm: 18.030 17.887 2.264
2024-12-02-05:02:38-root-INFO: Loss too large (156.484->162.786)! Learning rate decreased to 0.07964.
2024-12-02-05:02:39-root-INFO: grad norm: 18.111 17.959 2.342
2024-12-02-05:02:40-root-INFO: grad norm: 18.164 17.997 2.460
2024-12-02-05:02:41-root-INFO: grad norm: 18.230 18.074 2.382
2024-12-02-05:02:42-root-INFO: grad norm: 18.312 18.138 2.517
2024-12-02-05:02:43-root-INFO: Loss Change: 156.484 -> 152.401
2024-12-02-05:02:43-root-INFO: Regularization Change: 0.000 -> 3.598
2024-12-02-05:02:43-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-05:02:43-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-05:02:43-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-05:02:43-root-INFO: grad norm: 20.827 20.599 3.074
2024-12-02-05:02:44-root-INFO: Loss too large (154.458->164.010)! Learning rate decreased to 0.08180.
2024-12-02-05:02:45-root-INFO: grad norm: 20.120 19.922 2.812
2024-12-02-05:02:46-root-INFO: grad norm: 19.253 19.083 2.548
2024-12-02-05:02:47-root-INFO: grad norm: 18.659 18.485 2.543
2024-12-02-05:02:48-root-INFO: grad norm: 18.109 17.953 2.372
2024-12-02-05:02:48-root-INFO: Loss Change: 154.458 -> 149.292
2024-12-02-05:02:48-root-INFO: Regularization Change: 0.000 -> 3.815
2024-12-02-05:02:48-root-INFO: Undo step: 90
2024-12-02-05:02:48-root-INFO: Undo step: 91
2024-12-02-05:02:48-root-INFO: Undo step: 92
2024-12-02-05:02:48-root-INFO: Undo step: 93
2024-12-02-05:02:48-root-INFO: Undo step: 94
2024-12-02-05:02:49-root-INFO: step: 95 lr_xt 0.08933865
2024-12-02-05:02:49-root-INFO: grad norm: 62.558 61.908 8.993
2024-12-02-05:02:50-root-INFO: grad norm: 42.136 41.599 6.706
2024-12-02-05:02:51-root-INFO: grad norm: 34.995 34.654 4.870
2024-12-02-05:02:52-root-INFO: grad norm: 35.881 35.658 3.993
2024-12-02-05:02:53-root-INFO: grad norm: 39.486 39.305 3.774
2024-12-02-05:02:53-root-INFO: Loss too large (209.148->210.744)! Learning rate decreased to 0.07147.
2024-12-02-05:02:54-root-INFO: Loss Change: 351.802 -> 190.452
2024-12-02-05:02:54-root-INFO: Regularization Change: 0.000 -> 54.389
2024-12-02-05:02:54-root-INFO: Learning rate of xt decay: 0.18833 -> 0.19059.
2024-12-02-05:02:54-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-05:02:54-root-INFO: step: 94 lr_xt 0.09181181
2024-12-02-05:02:55-root-INFO: grad norm: 28.447 28.207 3.689
2024-12-02-05:02:56-root-INFO: grad norm: 32.327 32.075 4.024
2024-12-02-05:02:56-root-INFO: Loss too large (188.613->198.494)! Learning rate decreased to 0.07345.
2024-12-02-05:02:57-root-INFO: grad norm: 27.368 27.183 3.179
2024-12-02-05:02:58-root-INFO: grad norm: 23.482 23.297 2.938
2024-12-02-05:02:59-root-INFO: grad norm: 21.567 21.420 2.513
2024-12-02-05:03:00-root-INFO: Loss Change: 191.874 -> 167.245
2024-12-02-05:03:00-root-INFO: Regularization Change: 0.000 -> 8.913
2024-12-02-05:03:00-root-INFO: Learning rate of xt decay: 0.19059 -> 0.19288.
2024-12-02-05:03:00-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-05:03:00-root-INFO: step: 93 lr_xt 0.09433829
2024-12-02-05:03:00-root-INFO: grad norm: 18.544 18.432 2.033
2024-12-02-05:03:01-root-INFO: Loss too large (165.373->169.720)! Learning rate decreased to 0.07547.
2024-12-02-05:03:02-root-INFO: grad norm: 18.134 18.014 2.080
2024-12-02-05:03:03-root-INFO: grad norm: 17.706 17.567 2.216
2024-12-02-05:03:04-root-INFO: grad norm: 17.399 17.271 2.113
2024-12-02-05:03:05-root-INFO: grad norm: 17.501 17.351 2.282
2024-12-02-05:03:05-root-INFO: Loss Change: 165.373 -> 157.723
2024-12-02-05:03:05-root-INFO: Regularization Change: 0.000 -> 4.323
2024-12-02-05:03:05-root-INFO: Learning rate of xt decay: 0.19288 -> 0.19519.
2024-12-02-05:03:05-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-05:03:06-root-INFO: step: 92 lr_xt 0.09691873
2024-12-02-05:03:06-root-INFO: grad norm: 20.526 20.306 2.998
2024-12-02-05:03:06-root-INFO: Loss too large (159.441->168.275)! Learning rate decreased to 0.07753.
2024-12-02-05:03:07-root-INFO: Loss too large (159.441->159.466)! Learning rate decreased to 0.06203.
2024-12-02-05:03:08-root-INFO: grad norm: 13.615 13.473 1.965
2024-12-02-05:03:09-root-INFO: grad norm: 9.355 9.261 1.324
2024-12-02-05:03:10-root-INFO: grad norm: 7.146 7.059 1.110
2024-12-02-05:03:11-root-INFO: grad norm: 5.761 5.686 0.927
2024-12-02-05:03:11-root-INFO: Loss Change: 159.441 -> 148.042
2024-12-02-05:03:11-root-INFO: Regularization Change: 0.000 -> 2.000
2024-12-02-05:03:11-root-INFO: Learning rate of xt decay: 0.19519 -> 0.19753.
2024-12-02-05:03:11-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-05:03:12-root-INFO: step: 91 lr_xt 0.09955376
2024-12-02-05:03:12-root-INFO: grad norm: 4.467 4.366 0.946
2024-12-02-05:03:13-root-INFO: grad norm: 5.325 5.263 0.813
2024-12-02-05:03:14-root-INFO: grad norm: 11.375 11.325 1.066
2024-12-02-05:03:14-root-INFO: Loss too large (146.544->147.922)! Learning rate decreased to 0.07964.
2024-12-02-05:03:15-root-INFO: Loss too large (146.544->146.873)! Learning rate decreased to 0.06371.
2024-12-02-05:03:16-root-INFO: grad norm: 5.889 5.831 0.825
2024-12-02-05:03:17-root-INFO: grad norm: 4.927 4.872 0.737
2024-12-02-05:03:17-root-INFO: Loss Change: 147.743 -> 143.930
2024-12-02-05:03:17-root-INFO: Regularization Change: 0.000 -> 1.639
2024-12-02-05:03:17-root-INFO: Learning rate of xt decay: 0.19753 -> 0.19990.
2024-12-02-05:03:17-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-05:03:18-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-05:03:18-root-INFO: grad norm: 6.188 6.065 1.225
2024-12-02-05:03:19-root-INFO: grad norm: 9.545 9.472 1.179
2024-12-02-05:03:19-root-INFO: Loss too large (144.027->146.407)! Learning rate decreased to 0.08180.
2024-12-02-05:03:20-root-INFO: Loss too large (144.027->144.258)! Learning rate decreased to 0.06544.
2024-12-02-05:03:21-root-INFO: grad norm: 7.667 7.606 0.964
2024-12-02-05:03:22-root-INFO: grad norm: 6.112 6.058 0.810
2024-12-02-05:03:23-root-INFO: grad norm: 5.280 5.224 0.769
2024-12-02-05:03:23-root-INFO: Loss Change: 144.226 -> 140.704
2024-12-02-05:03:23-root-INFO: Regularization Change: 0.000 -> 1.258
2024-12-02-05:03:23-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-05:03:23-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-05:03:24-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-05:03:24-root-INFO: grad norm: 5.006 4.870 1.161
2024-12-02-05:03:25-root-INFO: grad norm: 5.761 5.698 0.849
2024-12-02-05:03:26-root-INFO: grad norm: 8.857 8.791 1.082
2024-12-02-05:03:26-root-INFO: Loss too large (139.494->140.875)! Learning rate decreased to 0.08399.
2024-12-02-05:03:26-root-INFO: Loss too large (139.494->139.665)! Learning rate decreased to 0.06719.
2024-12-02-05:03:27-root-INFO: grad norm: 5.782 5.736 0.733
2024-12-02-05:03:28-root-INFO: grad norm: 4.460 4.418 0.609
2024-12-02-05:03:29-root-INFO: Loss Change: 140.433 -> 137.185
2024-12-02-05:03:29-root-INFO: Regularization Change: 0.000 -> 1.319
2024-12-02-05:03:29-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-05:03:29-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-05:03:29-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-05:03:30-root-INFO: grad norm: 5.883 5.787 1.056
2024-12-02-05:03:30-root-INFO: Loss too large (137.700->138.146)! Learning rate decreased to 0.08623.
2024-12-02-05:03:31-root-INFO: grad norm: 6.697 6.643 0.843
2024-12-02-05:03:32-root-INFO: grad norm: 8.105 8.042 1.013
2024-12-02-05:03:32-root-INFO: Loss too large (137.088->137.454)! Learning rate decreased to 0.06899.
2024-12-02-05:03:33-root-INFO: grad norm: 7.017 6.967 0.842
2024-12-02-05:03:34-root-INFO: grad norm: 6.006 5.953 0.796
2024-12-02-05:03:35-root-INFO: Loss Change: 137.700 -> 135.198
2024-12-02-05:03:35-root-INFO: Regularization Change: 0.000 -> 1.075
2024-12-02-05:03:35-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-05:03:35-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-05:03:35-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-05:03:36-root-INFO: grad norm: 4.632 4.542 0.907
2024-12-02-05:03:37-root-INFO: grad norm: 8.243 8.213 0.701
2024-12-02-05:03:37-root-INFO: Loss too large (134.551->135.894)! Learning rate decreased to 0.08852.
2024-12-02-05:03:37-root-INFO: Loss too large (134.551->134.884)! Learning rate decreased to 0.07082.
2024-12-02-05:03:38-root-INFO: grad norm: 5.439 5.404 0.611
2024-12-02-05:03:39-root-INFO: grad norm: 4.532 4.497 0.565
2024-12-02-05:03:40-root-INFO: grad norm: 3.976 3.940 0.538
2024-12-02-05:03:41-root-INFO: Loss Change: 134.781 -> 132.235
2024-12-02-05:03:41-root-INFO: Regularization Change: 0.000 -> 0.999
2024-12-02-05:03:41-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-05:03:41-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-05:03:41-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-05:03:42-root-INFO: grad norm: 5.526 5.424 1.057
2024-12-02-05:03:42-root-INFO: Loss too large (132.540->132.983)! Learning rate decreased to 0.09086.
2024-12-02-05:03:43-root-INFO: grad norm: 7.169 7.125 0.793
2024-12-02-05:03:43-root-INFO: Loss too large (132.242->132.604)! Learning rate decreased to 0.07268.
2024-12-02-05:03:44-root-INFO: grad norm: 6.188 6.146 0.723
2024-12-02-05:03:45-root-INFO: grad norm: 5.044 5.007 0.611
2024-12-02-05:03:46-root-INFO: grad norm: 4.705 4.666 0.604
2024-12-02-05:03:47-root-INFO: Loss Change: 132.540 -> 130.164
2024-12-02-05:03:47-root-INFO: Regularization Change: 0.000 -> 0.928
2024-12-02-05:03:47-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-05:03:47-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-05:03:47-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-05:03:48-root-INFO: grad norm: 5.859 5.778 0.973
2024-12-02-05:03:48-root-INFO: Loss too large (130.132->130.550)! Learning rate decreased to 0.09324.
2024-12-02-05:03:48-root-INFO: Loss too large (130.132->130.201)! Learning rate decreased to 0.07459.
2024-12-02-05:03:49-root-INFO: grad norm: 4.217 4.179 0.564
2024-12-02-05:03:50-root-INFO: grad norm: 2.781 2.734 0.510
2024-12-02-05:03:51-root-INFO: grad norm: 2.752 2.708 0.490
2024-12-02-05:03:52-root-INFO: grad norm: 2.935 2.895 0.481
2024-12-02-05:03:53-root-INFO: Loss Change: 130.132 -> 127.892
2024-12-02-05:03:53-root-INFO: Regularization Change: 0.000 -> 0.760
2024-12-02-05:03:53-root-INFO: Undo step: 85
2024-12-02-05:03:53-root-INFO: Undo step: 86
2024-12-02-05:03:53-root-INFO: Undo step: 87
2024-12-02-05:03:53-root-INFO: Undo step: 88
2024-12-02-05:03:53-root-INFO: Undo step: 89
2024-12-02-05:03:53-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-05:03:54-root-INFO: grad norm: 47.435 46.911 7.026
2024-12-02-05:03:55-root-INFO: grad norm: 32.196 31.839 4.784
2024-12-02-05:03:56-root-INFO: grad norm: 32.248 32.076 3.327
2024-12-02-05:03:57-root-INFO: grad norm: 31.667 31.490 3.349
2024-12-02-05:03:58-root-INFO: grad norm: 29.712 29.585 2.737
2024-12-02-05:03:58-root-INFO: Loss Change: 310.078 -> 178.384
2024-12-02-05:03:58-root-INFO: Regularization Change: 0.000 -> 56.924
2024-12-02-05:03:58-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-05:03:58-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-05:03:59-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-05:03:59-root-INFO: grad norm: 29.605 29.436 3.155
2024-12-02-05:04:00-root-INFO: grad norm: 28.528 28.362 3.069
2024-12-02-05:04:01-root-INFO: grad norm: 28.029 27.831 3.326
2024-12-02-05:04:02-root-INFO: grad norm: 27.475 27.219 3.746
2024-12-02-05:04:03-root-INFO: grad norm: 28.165 27.872 4.049
2024-12-02-05:04:03-root-INFO: Loss too large (164.772->167.052)! Learning rate decreased to 0.08399.
2024-12-02-05:04:04-root-INFO: Loss Change: 179.644 -> 153.516
2024-12-02-05:04:04-root-INFO: Regularization Change: 0.000 -> 12.280
2024-12-02-05:04:04-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-05:04:04-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-05:04:04-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-05:04:04-root-INFO: grad norm: 18.696 18.469 2.907
2024-12-02-05:04:05-root-INFO: Loss too large (152.478->153.359)! Learning rate decreased to 0.08623.
2024-12-02-05:04:06-root-INFO: grad norm: 14.618 14.445 2.244
2024-12-02-05:04:07-root-INFO: grad norm: 12.918 12.736 2.163
2024-12-02-05:04:08-root-INFO: grad norm: 12.209 12.058 1.916
2024-12-02-05:04:09-root-INFO: grad norm: 11.638 11.474 1.947
2024-12-02-05:04:09-root-INFO: Loss Change: 152.478 -> 139.902
2024-12-02-05:04:09-root-INFO: Regularization Change: 0.000 -> 4.398
2024-12-02-05:04:09-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-05:04:09-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-05:04:10-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-05:04:10-root-INFO: grad norm: 14.427 14.173 2.693
2024-12-02-05:04:10-root-INFO: Loss too large (141.364->145.515)! Learning rate decreased to 0.08852.
2024-12-02-05:04:11-root-INFO: grad norm: 14.111 13.913 2.360
2024-12-02-05:04:12-root-INFO: grad norm: 14.004 13.826 2.229
2024-12-02-05:04:13-root-INFO: grad norm: 13.863 13.680 2.247
2024-12-02-05:04:14-root-INFO: grad norm: 13.633 13.467 2.118
2024-12-02-05:04:15-root-INFO: Loss Change: 141.364 -> 136.985
2024-12-02-05:04:15-root-INFO: Regularization Change: 0.000 -> 3.318
2024-12-02-05:04:15-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-05:04:15-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-05:04:15-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-05:04:16-root-INFO: grad norm: 11.675 11.569 1.568
2024-12-02-05:04:16-root-INFO: Loss too large (135.718->138.715)! Learning rate decreased to 0.09086.
2024-12-02-05:04:17-root-INFO: grad norm: 12.508 12.371 1.843
2024-12-02-05:04:18-root-INFO: grad norm: 12.270 12.127 1.868
2024-12-02-05:04:19-root-INFO: grad norm: 12.200 12.066 1.802
2024-12-02-05:04:20-root-INFO: grad norm: 12.379 12.244 1.819
2024-12-02-05:04:21-root-INFO: Loss Change: 135.718 -> 132.851
2024-12-02-05:04:21-root-INFO: Regularization Change: 0.000 -> 2.650
2024-12-02-05:04:21-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-05:04:21-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-05:04:21-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-05:04:21-root-INFO: grad norm: 14.885 14.678 2.475
2024-12-02-05:04:22-root-INFO: Loss too large (134.240->139.610)! Learning rate decreased to 0.09324.
2024-12-02-05:04:23-root-INFO: grad norm: 14.870 14.698 2.250
2024-12-02-05:04:24-root-INFO: grad norm: 14.797 14.636 2.181
2024-12-02-05:04:25-root-INFO: grad norm: 14.773 14.604 2.226
2024-12-02-05:04:26-root-INFO: grad norm: 15.282 15.117 2.238
2024-12-02-05:04:26-root-INFO: Loss too large (131.961->132.045)! Learning rate decreased to 0.07459.
2024-12-02-05:04:27-root-INFO: Loss Change: 134.240 -> 128.764
2024-12-02-05:04:27-root-INFO: Regularization Change: 0.000 -> 2.490
2024-12-02-05:04:27-root-INFO: Undo step: 85
2024-12-02-05:04:27-root-INFO: Undo step: 86
2024-12-02-05:04:27-root-INFO: Undo step: 87
2024-12-02-05:04:27-root-INFO: Undo step: 88
2024-12-02-05:04:27-root-INFO: Undo step: 89
2024-12-02-05:04:27-root-INFO: step: 90 lr_xt 0.10224402
2024-12-02-05:04:27-root-INFO: grad norm: 45.385 45.001 5.887
2024-12-02-05:04:28-root-INFO: grad norm: 23.574 23.354 3.210
2024-12-02-05:04:29-root-INFO: grad norm: 18.561 18.372 2.638
2024-12-02-05:04:30-root-INFO: grad norm: 17.734 17.603 2.152
2024-12-02-05:04:31-root-INFO: grad norm: 19.022 18.878 2.338
2024-12-02-05:04:32-root-INFO: Loss Change: 330.981 -> 165.654
2024-12-02-05:04:32-root-INFO: Regularization Change: 0.000 -> 66.462
2024-12-02-05:04:32-root-INFO: Learning rate of xt decay: 0.19990 -> 0.20230.
2024-12-02-05:04:32-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-05:04:32-root-INFO: step: 89 lr_xt 0.10499012
2024-12-02-05:04:33-root-INFO: grad norm: 22.068 21.892 2.782
2024-12-02-05:04:34-root-INFO: grad norm: 24.471 24.268 3.148
2024-12-02-05:04:34-root-INFO: Loss too large (164.775->166.275)! Learning rate decreased to 0.08399.
2024-12-02-05:04:35-root-INFO: grad norm: 18.000 17.868 2.176
2024-12-02-05:04:36-root-INFO: grad norm: 14.019 13.857 2.127
2024-12-02-05:04:37-root-INFO: grad norm: 12.435 12.310 1.760
2024-12-02-05:04:38-root-INFO: Loss Change: 166.410 -> 145.316
2024-12-02-05:04:38-root-INFO: Regularization Change: 0.000 -> 7.481
2024-12-02-05:04:38-root-INFO: Learning rate of xt decay: 0.20230 -> 0.20473.
2024-12-02-05:04:38-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-05:04:38-root-INFO: step: 88 lr_xt 0.10779268
2024-12-02-05:04:38-root-INFO: grad norm: 10.067 9.965 1.431
2024-12-02-05:04:39-root-INFO: Loss too large (144.703->144.925)! Learning rate decreased to 0.08623.
2024-12-02-05:04:40-root-INFO: grad norm: 9.991 9.879 1.490
2024-12-02-05:04:41-root-INFO: grad norm: 10.108 9.987 1.561
2024-12-02-05:04:42-root-INFO: grad norm: 10.486 10.365 1.587
2024-12-02-05:04:43-root-INFO: grad norm: 10.987 10.858 1.679
2024-12-02-05:04:43-root-INFO: Loss Change: 144.703 -> 139.097
2024-12-02-05:04:43-root-INFO: Regularization Change: 0.000 -> 3.423
2024-12-02-05:04:43-root-INFO: Learning rate of xt decay: 0.20473 -> 0.20719.
2024-12-02-05:04:43-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-05:04:43-root-INFO: step: 87 lr_xt 0.11065228
2024-12-02-05:04:44-root-INFO: grad norm: 15.156 14.911 2.716
2024-12-02-05:04:44-root-INFO: Loss too large (140.630->145.971)! Learning rate decreased to 0.08852.
2024-12-02-05:04:45-root-INFO: grad norm: 15.188 15.000 2.386
2024-12-02-05:04:46-root-INFO: grad norm: 15.333 15.158 2.309
2024-12-02-05:04:47-root-INFO: grad norm: 15.476 15.303 2.305
2024-12-02-05:04:48-root-INFO: grad norm: 15.581 15.415 2.274
2024-12-02-05:04:49-root-INFO: Loss Change: 140.630 -> 137.035
2024-12-02-05:04:49-root-INFO: Regularization Change: 0.000 -> 3.617
2024-12-02-05:04:49-root-INFO: Learning rate of xt decay: 0.20719 -> 0.20967.
2024-12-02-05:04:49-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-05:04:49-root-INFO: step: 86 lr_xt 0.11356952
2024-12-02-05:04:50-root-INFO: grad norm: 13.819 13.713 1.706
2024-12-02-05:04:50-root-INFO: Loss too large (135.662->139.928)! Learning rate decreased to 0.09086.
2024-12-02-05:04:51-root-INFO: grad norm: 13.594 13.465 1.864
2024-12-02-05:04:52-root-INFO: grad norm: 13.300 13.167 1.879
2024-12-02-05:04:53-root-INFO: grad norm: 13.365 13.227 1.916
2024-12-02-05:04:54-root-INFO: grad norm: 13.546 13.410 1.917
2024-12-02-05:04:55-root-INFO: Loss Change: 135.662 -> 132.286
2024-12-02-05:04:55-root-INFO: Regularization Change: 0.000 -> 3.070
2024-12-02-05:04:55-root-INFO: Learning rate of xt decay: 0.20967 -> 0.21219.
2024-12-02-05:04:55-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-05:04:55-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-05:04:55-root-INFO: grad norm: 15.731 15.526 2.532
2024-12-02-05:04:56-root-INFO: Loss too large (133.662->139.536)! Learning rate decreased to 0.09324.
2024-12-02-05:04:57-root-INFO: grad norm: 15.834 15.675 2.238
2024-12-02-05:04:58-root-INFO: grad norm: 15.097 14.945 2.135
2024-12-02-05:04:59-root-INFO: grad norm: 14.541 14.379 2.167
2024-12-02-05:05:00-root-INFO: grad norm: 14.803 14.638 2.201
2024-12-02-05:05:00-root-INFO: Loss Change: 133.662 -> 130.182
2024-12-02-05:05:00-root-INFO: Regularization Change: 0.000 -> 3.315
2024-12-02-05:05:00-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-05:05:00-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-05:05:01-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-05:05:01-root-INFO: grad norm: 12.919 12.792 1.808
2024-12-02-05:05:01-root-INFO: Loss too large (129.057->132.131)! Learning rate decreased to 0.09566.
2024-12-02-05:05:02-root-INFO: grad norm: 12.454 12.316 1.853
2024-12-02-05:05:03-root-INFO: grad norm: 12.263 12.129 1.806
2024-12-02-05:05:04-root-INFO: grad norm: 12.142 12.009 1.794
2024-12-02-05:05:05-root-INFO: grad norm: 12.093 11.957 1.810
2024-12-02-05:05:06-root-INFO: Loss Change: 129.057 -> 125.450
2024-12-02-05:05:06-root-INFO: Regularization Change: 0.000 -> 2.759
2024-12-02-05:05:06-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-05:05:06-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-05:05:06-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-05:05:07-root-INFO: grad norm: 15.145 14.929 2.547
2024-12-02-05:05:07-root-INFO: Loss too large (127.044->133.204)! Learning rate decreased to 0.09814.
2024-12-02-05:05:07-root-INFO: Loss too large (127.044->127.089)! Learning rate decreased to 0.07851.
2024-12-02-05:05:08-root-INFO: grad norm: 9.329 9.216 1.450
2024-12-02-05:05:09-root-INFO: grad norm: 5.766 5.694 0.911
2024-12-02-05:05:10-root-INFO: grad norm: 4.175 4.131 0.607
2024-12-02-05:05:11-root-INFO: grad norm: 3.596 3.555 0.546
2024-12-02-05:05:12-root-INFO: Loss Change: 127.044 -> 119.912
2024-12-02-05:05:12-root-INFO: Regularization Change: 0.000 -> 1.391
2024-12-02-05:05:12-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-05:05:12-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-05:05:12-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-05:05:13-root-INFO: grad norm: 6.278 6.224 0.818
2024-12-02-05:05:13-root-INFO: Loss too large (120.143->120.691)! Learning rate decreased to 0.10066.
2024-12-02-05:05:13-root-INFO: Loss too large (120.143->120.428)! Learning rate decreased to 0.08053.
2024-12-02-05:05:14-root-INFO: Loss too large (120.143->120.199)! Learning rate decreased to 0.06442.
2024-12-02-05:05:15-root-INFO: grad norm: 3.868 3.830 0.541
2024-12-02-05:05:16-root-INFO: grad norm: 2.650 2.609 0.469
2024-12-02-05:05:17-root-INFO: grad norm: 2.710 2.672 0.455
2024-12-02-05:05:18-root-INFO: grad norm: 2.795 2.756 0.466
2024-12-02-05:05:18-root-INFO: Loss Change: 120.143 -> 118.238
2024-12-02-05:05:18-root-INFO: Regularization Change: 0.000 -> 0.525
2024-12-02-05:05:18-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-05:05:18-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-05:05:19-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-05:05:19-root-INFO: grad norm: 4.993 4.895 0.989
2024-12-02-05:05:19-root-INFO: Loss too large (118.275->118.650)! Learning rate decreased to 0.10323.
2024-12-02-05:05:20-root-INFO: Loss too large (118.275->118.278)! Learning rate decreased to 0.08259.
2024-12-02-05:05:21-root-INFO: grad norm: 4.230 4.188 0.589
2024-12-02-05:05:22-root-INFO: grad norm: 3.322 3.276 0.549
2024-12-02-05:05:23-root-INFO: grad norm: 3.617 3.580 0.512
2024-12-02-05:05:24-root-INFO: grad norm: 5.174 5.142 0.574
2024-12-02-05:05:24-root-INFO: Loss too large (116.599->116.705)! Learning rate decreased to 0.06607.
2024-12-02-05:05:25-root-INFO: Loss Change: 118.275 -> 116.529
2024-12-02-05:05:25-root-INFO: Regularization Change: 0.000 -> 0.847
2024-12-02-05:05:25-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-05:05:25-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-05:05:25-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-05:05:25-root-INFO: grad norm: 4.719 4.640 0.858
2024-12-02-05:05:26-root-INFO: Loss too large (116.754->116.962)! Learning rate decreased to 0.10585.
2024-12-02-05:05:27-root-INFO: grad norm: 7.060 7.031 0.641
2024-12-02-05:05:27-root-INFO: Loss too large (116.193->116.646)! Learning rate decreased to 0.08468.
2024-12-02-05:05:27-root-INFO: Loss too large (116.193->116.295)! Learning rate decreased to 0.06774.
2024-12-02-05:05:28-root-INFO: grad norm: 3.771 3.737 0.503
2024-12-02-05:05:29-root-INFO: grad norm: 2.932 2.899 0.439
2024-12-02-05:05:30-root-INFO: grad norm: 2.926 2.894 0.432
2024-12-02-05:05:31-root-INFO: Loss Change: 116.754 -> 114.566
2024-12-02-05:05:31-root-INFO: Regularization Change: 0.000 -> 0.680
2024-12-02-05:05:31-root-INFO: Undo step: 80
2024-12-02-05:05:31-root-INFO: Undo step: 81
2024-12-02-05:05:31-root-INFO: Undo step: 82
2024-12-02-05:05:31-root-INFO: Undo step: 83
2024-12-02-05:05:31-root-INFO: Undo step: 84
2024-12-02-05:05:31-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-05:05:31-root-INFO: grad norm: 48.356 47.845 7.007
2024-12-02-05:05:33-root-INFO: grad norm: 32.463 32.226 3.923
2024-12-02-05:05:34-root-INFO: grad norm: 27.693 27.428 3.821
2024-12-02-05:05:35-root-INFO: grad norm: 25.955 25.750 3.250
2024-12-02-05:05:36-root-INFO: grad norm: 24.316 24.090 3.308
2024-12-02-05:05:36-root-INFO: Loss Change: 292.902 -> 156.326
2024-12-02-05:05:36-root-INFO: Regularization Change: 0.000 -> 62.636
2024-12-02-05:05:36-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-05:05:36-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-05:05:37-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-05:05:37-root-INFO: grad norm: 24.142 23.902 3.397
2024-12-02-05:05:38-root-INFO: grad norm: 23.036 22.779 3.433
2024-12-02-05:05:39-root-INFO: grad norm: 22.509 22.249 3.409
2024-12-02-05:05:39-root-INFO: Loss too large (148.438->149.233)! Learning rate decreased to 0.09566.
2024-12-02-05:05:40-root-INFO: grad norm: 15.422 15.211 2.540
2024-12-02-05:05:41-root-INFO: grad norm: 10.880 10.743 1.723
2024-12-02-05:05:42-root-INFO: Loss Change: 157.128 -> 131.536
2024-12-02-05:05:42-root-INFO: Regularization Change: 0.000 -> 8.817
2024-12-02-05:05:42-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-05:05:42-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-05:05:42-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-05:05:42-root-INFO: grad norm: 7.999 7.931 1.037
2024-12-02-05:05:43-root-INFO: grad norm: 9.897 9.816 1.260
2024-12-02-05:05:44-root-INFO: Loss too large (129.903->130.228)! Learning rate decreased to 0.09814.
2024-12-02-05:05:45-root-INFO: grad norm: 8.675 8.613 1.043
2024-12-02-05:05:46-root-INFO: grad norm: 7.705 7.642 0.984
2024-12-02-05:05:47-root-INFO: grad norm: 7.344 7.297 0.827
2024-12-02-05:05:48-root-INFO: Loss Change: 130.844 -> 124.546
2024-12-02-05:05:48-root-INFO: Regularization Change: 0.000 -> 3.564
2024-12-02-05:05:48-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-05:05:48-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-05:05:48-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-05:05:48-root-INFO: grad norm: 7.454 7.379 1.055
2024-12-02-05:05:48-root-INFO: Loss too large (124.724->124.958)! Learning rate decreased to 0.10066.
2024-12-02-05:05:49-root-INFO: grad norm: 7.784 7.739 0.837
2024-12-02-05:05:50-root-INFO: grad norm: 6.688 6.636 0.828
2024-12-02-05:05:51-root-INFO: grad norm: 5.885 5.838 0.741
2024-12-02-05:05:52-root-INFO: grad norm: 6.012 5.966 0.740
2024-12-02-05:05:53-root-INFO: Loss Change: 124.724 -> 120.378
2024-12-02-05:05:53-root-INFO: Regularization Change: 0.000 -> 2.348
2024-12-02-05:05:53-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-05:05:53-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-05:05:53-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-05:05:54-root-INFO: grad norm: 5.992 5.952 0.688
2024-12-02-05:05:54-root-INFO: Loss too large (120.015->120.096)! Learning rate decreased to 0.10323.
2024-12-02-05:05:55-root-INFO: grad norm: 6.967 6.936 0.649
2024-12-02-05:05:56-root-INFO: grad norm: 6.029 5.994 0.651
2024-12-02-05:05:57-root-INFO: grad norm: 5.685 5.657 0.564
2024-12-02-05:05:58-root-INFO: grad norm: 6.369 6.344 0.569
2024-12-02-05:05:59-root-INFO: Loss Change: 120.015 -> 117.053
2024-12-02-05:05:59-root-INFO: Regularization Change: 0.000 -> 1.928
2024-12-02-05:05:59-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-05:05:59-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-05:05:59-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-05:05:59-root-INFO: grad norm: 6.152 6.099 0.808
2024-12-02-05:06:00-root-INFO: grad norm: 9.255 9.229 0.698
2024-12-02-05:06:01-root-INFO: Loss too large (116.934->118.233)! Learning rate decreased to 0.10585.
2024-12-02-05:06:02-root-INFO: grad norm: 7.015 6.987 0.621
2024-12-02-05:06:03-root-INFO: grad norm: 6.754 6.728 0.592
2024-12-02-05:06:04-root-INFO: grad norm: 6.009 5.985 0.530
2024-12-02-05:06:04-root-INFO: Loss Change: 116.975 -> 113.912
2024-12-02-05:06:04-root-INFO: Regularization Change: 0.000 -> 1.966
2024-12-02-05:06:04-root-INFO: Undo step: 80
2024-12-02-05:06:04-root-INFO: Undo step: 81
2024-12-02-05:06:04-root-INFO: Undo step: 82
2024-12-02-05:06:04-root-INFO: Undo step: 83
2024-12-02-05:06:04-root-INFO: Undo step: 84
2024-12-02-05:06:05-root-INFO: step: 85 lr_xt 0.11654496
2024-12-02-05:06:05-root-INFO: grad norm: 43.635 43.233 5.910
2024-12-02-05:06:06-root-INFO: grad norm: 27.724 27.468 3.757
2024-12-02-05:06:07-root-INFO: grad norm: 27.204 26.874 4.225
2024-12-02-05:06:08-root-INFO: grad norm: 30.374 30.095 4.114
2024-12-02-05:06:09-root-INFO: grad norm: 30.073 29.673 4.889
2024-12-02-05:06:10-root-INFO: Loss Change: 296.917 -> 164.245
2024-12-02-05:06:10-root-INFO: Regularization Change: 0.000 -> 67.217
2024-12-02-05:06:10-root-INFO: Learning rate of xt decay: 0.21219 -> 0.21474.
2024-12-02-05:06:10-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-05:06:10-root-INFO: step: 84 lr_xt 0.11957917
2024-12-02-05:06:10-root-INFO: grad norm: 29.735 29.391 4.508
2024-12-02-05:06:12-root-INFO: grad norm: 28.895 28.478 4.892
2024-12-02-05:06:13-root-INFO: grad norm: 29.762 29.433 4.414
2024-12-02-05:06:14-root-INFO: grad norm: 27.702 27.288 4.770
2024-12-02-05:06:15-root-INFO: grad norm: 26.928 26.610 4.121
2024-12-02-05:06:15-root-INFO: Loss Change: 166.461 -> 151.493
2024-12-02-05:06:15-root-INFO: Regularization Change: 0.000 -> 18.629
2024-12-02-05:06:15-root-INFO: Learning rate of xt decay: 0.21474 -> 0.21731.
2024-12-02-05:06:15-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-05:06:16-root-INFO: step: 83 lr_xt 0.12267269
2024-12-02-05:06:16-root-INFO: grad norm: 25.117 24.789 4.051
2024-12-02-05:06:17-root-INFO: grad norm: 24.956 24.652 3.882
2024-12-02-05:06:18-root-INFO: grad norm: 24.726 24.361 4.234
2024-12-02-05:06:19-root-INFO: grad norm: 24.627 24.296 4.020
2024-12-02-05:06:20-root-INFO: grad norm: 24.236 23.852 4.295
2024-12-02-05:06:21-root-INFO: Loss Change: 148.928 -> 139.931
2024-12-02-05:06:21-root-INFO: Regularization Change: 0.000 -> 13.122
2024-12-02-05:06:21-root-INFO: Learning rate of xt decay: 0.21731 -> 0.21992.
2024-12-02-05:06:21-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-05:06:21-root-INFO: step: 82 lr_xt 0.12582604
2024-12-02-05:06:21-root-INFO: grad norm: 25.112 24.739 4.313
2024-12-02-05:06:22-root-INFO: grad norm: 23.867 23.477 4.299
2024-12-02-05:06:23-root-INFO: grad norm: 22.473 22.175 3.650
2024-12-02-05:06:24-root-INFO: grad norm: 21.626 21.287 3.815
2024-12-02-05:06:25-root-INFO: grad norm: 22.123 21.821 3.647
2024-12-02-05:06:26-root-INFO: Loss too large (133.200->133.645)! Learning rate decreased to 0.10066.
2024-12-02-05:06:26-root-INFO: Loss Change: 142.365 -> 124.530
2024-12-02-05:06:26-root-INFO: Regularization Change: 0.000 -> 8.606
2024-12-02-05:06:26-root-INFO: Learning rate of xt decay: 0.21992 -> 0.22256.
2024-12-02-05:06:26-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-05:06:27-root-INFO: step: 81 lr_xt 0.12903975
2024-12-02-05:06:27-root-INFO: grad norm: 11.962 11.788 2.029
2024-12-02-05:06:28-root-INFO: grad norm: 12.720 12.555 2.043
2024-12-02-05:06:28-root-INFO: Loss too large (121.442->121.537)! Learning rate decreased to 0.10323.
2024-12-02-05:06:29-root-INFO: grad norm: 9.001 8.876 1.495
2024-12-02-05:06:30-root-INFO: grad norm: 6.779 6.690 1.098
2024-12-02-05:06:31-root-INFO: grad norm: 5.489 5.422 0.859
2024-12-02-05:06:32-root-INFO: Loss Change: 122.915 -> 114.346
2024-12-02-05:06:32-root-INFO: Regularization Change: 0.000 -> 2.956
2024-12-02-05:06:32-root-INFO: Learning rate of xt decay: 0.22256 -> 0.22523.
2024-12-02-05:06:32-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-05:06:32-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-05:06:33-root-INFO: grad norm: 6.942 6.795 1.421
2024-12-02-05:06:34-root-INFO: grad norm: 8.408 8.298 1.353
2024-12-02-05:06:34-root-INFO: Loss too large (115.025->115.568)! Learning rate decreased to 0.10585.
2024-12-02-05:06:35-root-INFO: grad norm: 7.145 7.064 1.074
2024-12-02-05:06:36-root-INFO: grad norm: 7.312 7.264 0.837
2024-12-02-05:06:37-root-INFO: grad norm: 5.709 5.659 0.754
2024-12-02-05:06:38-root-INFO: Loss Change: 115.154 -> 111.427
2024-12-02-05:06:38-root-INFO: Regularization Change: 0.000 -> 1.940
2024-12-02-05:06:38-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-05:06:38-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-05:06:38-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-05:06:38-root-INFO: grad norm: 3.635 3.582 0.622
2024-12-02-05:06:39-root-INFO: grad norm: 4.374 4.340 0.542
2024-12-02-05:06:40-root-INFO: Loss too large (110.406->110.417)! Learning rate decreased to 0.10852.
2024-12-02-05:06:41-root-INFO: grad norm: 4.549 4.509 0.598
2024-12-02-05:06:42-root-INFO: grad norm: 6.574 6.529 0.768
2024-12-02-05:06:42-root-INFO: Loss too large (109.665->109.996)! Learning rate decreased to 0.08682.
2024-12-02-05:06:43-root-INFO: grad norm: 4.081 4.036 0.608
2024-12-02-05:06:44-root-INFO: Loss Change: 111.009 -> 108.470
2024-12-02-05:06:44-root-INFO: Regularization Change: 0.000 -> 1.304
2024-12-02-05:06:44-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-05:06:44-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-05:06:44-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-05:06:44-root-INFO: grad norm: 3.909 3.820 0.828
2024-12-02-05:06:45-root-INFO: grad norm: 5.788 5.748 0.674
2024-12-02-05:06:46-root-INFO: Loss too large (108.515->109.110)! Learning rate decreased to 0.11124.
2024-12-02-05:06:47-root-INFO: grad norm: 4.964 4.919 0.662
2024-12-02-05:06:48-root-INFO: grad norm: 3.736 3.698 0.535
2024-12-02-05:06:49-root-INFO: grad norm: 3.735 3.690 0.574
2024-12-02-05:06:49-root-INFO: Loss Change: 108.801 -> 106.489
2024-12-02-05:06:49-root-INFO: Regularization Change: 0.000 -> 1.500
2024-12-02-05:06:49-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-05:06:49-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-05:06:50-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-05:06:50-root-INFO: grad norm: 5.440 5.393 0.717
2024-12-02-05:06:50-root-INFO: Loss too large (106.205->106.754)! Learning rate decreased to 0.11401.
2024-12-02-05:06:51-root-INFO: Loss too large (106.205->106.433)! Learning rate decreased to 0.09121.
2024-12-02-05:06:52-root-INFO: grad norm: 3.880 3.847 0.506
2024-12-02-05:06:53-root-INFO: grad norm: 2.110 2.075 0.382
2024-12-02-05:06:54-root-INFO: grad norm: 1.964 1.931 0.355
2024-12-02-05:06:55-root-INFO: grad norm: 1.923 1.891 0.348
2024-12-02-05:06:55-root-INFO: Loss Change: 106.205 -> 104.087
2024-12-02-05:06:55-root-INFO: Regularization Change: 0.000 -> 0.811
2024-12-02-05:06:55-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-05:06:55-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-05:06:56-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-05:06:56-root-INFO: grad norm: 3.993 3.905 0.837
2024-12-02-05:06:56-root-INFO: Loss too large (104.249->104.440)! Learning rate decreased to 0.11682.
2024-12-02-05:06:57-root-INFO: grad norm: 4.495 4.441 0.691
2024-12-02-05:06:58-root-INFO: grad norm: 7.288 7.230 0.916
2024-12-02-05:06:59-root-INFO: Loss too large (103.851->104.516)! Learning rate decreased to 0.09346.
2024-12-02-05:06:59-root-INFO: Loss too large (103.851->104.033)! Learning rate decreased to 0.07477.
2024-12-02-05:07:00-root-INFO: grad norm: 3.960 3.918 0.576
2024-12-02-05:07:01-root-INFO: grad norm: 2.465 2.439 0.356
2024-12-02-05:07:02-root-INFO: Loss Change: 104.249 -> 102.387
2024-12-02-05:07:02-root-INFO: Regularization Change: 0.000 -> 0.771
2024-12-02-05:07:02-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-05:07:02-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-05:07:02-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-05:07:02-root-INFO: grad norm: 3.565 3.482 0.763
2024-12-02-05:07:03-root-INFO: Loss too large (102.529->102.594)! Learning rate decreased to 0.11969.
2024-12-02-05:07:04-root-INFO: grad norm: 4.151 4.106 0.615
2024-12-02-05:07:05-root-INFO: grad norm: 7.326 7.273 0.873
2024-12-02-05:07:05-root-INFO: Loss too large (102.222->102.890)! Learning rate decreased to 0.09575.
2024-12-02-05:07:05-root-INFO: Loss too large (102.222->102.435)! Learning rate decreased to 0.07660.
2024-12-02-05:07:06-root-INFO: grad norm: 3.847 3.806 0.554
2024-12-02-05:07:07-root-INFO: grad norm: 2.631 2.607 0.357
2024-12-02-05:07:08-root-INFO: Loss Change: 102.529 -> 100.786
2024-12-02-05:07:08-root-INFO: Regularization Change: 0.000 -> 0.761
2024-12-02-05:07:08-root-INFO: Undo step: 75
2024-12-02-05:07:08-root-INFO: Undo step: 76
2024-12-02-05:07:08-root-INFO: Undo step: 77
2024-12-02-05:07:08-root-INFO: Undo step: 78
2024-12-02-05:07:08-root-INFO: Undo step: 79
2024-12-02-05:07:08-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-05:07:09-root-INFO: grad norm: 39.056 38.667 5.493
2024-12-02-05:07:10-root-INFO: grad norm: 26.421 26.189 3.495
2024-12-02-05:07:11-root-INFO: grad norm: 21.659 21.394 3.377
2024-12-02-05:07:12-root-INFO: grad norm: 22.224 21.954 3.455
2024-12-02-05:07:13-root-INFO: grad norm: 22.014 21.715 3.617
2024-12-02-05:07:13-root-INFO: Loss Change: 271.776 -> 139.644
2024-12-02-05:07:13-root-INFO: Regularization Change: 0.000 -> 72.625
2024-12-02-05:07:13-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-05:07:13-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-05:07:14-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-05:07:14-root-INFO: grad norm: 23.451 23.076 4.178
2024-12-02-05:07:15-root-INFO: grad norm: 22.836 22.460 4.129
2024-12-02-05:07:16-root-INFO: grad norm: 22.987 22.648 3.933
2024-12-02-05:07:16-root-INFO: Loss too large (135.115->135.527)! Learning rate decreased to 0.10852.
2024-12-02-05:07:17-root-INFO: grad norm: 15.160 14.892 2.839
2024-12-02-05:07:18-root-INFO: grad norm: 10.269 10.126 1.708
2024-12-02-05:07:19-root-INFO: Loss Change: 141.897 -> 115.364
2024-12-02-05:07:19-root-INFO: Regularization Change: 0.000 -> 9.948
2024-12-02-05:07:19-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-05:07:19-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-05:07:19-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-05:07:20-root-INFO: grad norm: 6.730 6.658 0.986
2024-12-02-05:07:21-root-INFO: grad norm: 8.011 7.905 1.300
2024-12-02-05:07:21-root-INFO: grad norm: 9.996 9.882 1.508
2024-12-02-05:07:22-root-INFO: Loss too large (113.832->114.624)! Learning rate decreased to 0.11124.
2024-12-02-05:07:23-root-INFO: grad norm: 8.612 8.501 1.375
2024-12-02-05:07:24-root-INFO: grad norm: 7.807 7.710 1.225
2024-12-02-05:07:24-root-INFO: Loss Change: 114.848 -> 110.005
2024-12-02-05:07:24-root-INFO: Regularization Change: 0.000 -> 3.741
2024-12-02-05:07:24-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-05:07:24-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-05:07:25-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-05:07:25-root-INFO: grad norm: 10.764 10.599 1.879
2024-12-02-05:07:25-root-INFO: Loss too large (110.667->112.940)! Learning rate decreased to 0.11401.
2024-12-02-05:07:26-root-INFO: grad norm: 8.326 8.198 1.451
2024-12-02-05:07:27-root-INFO: grad norm: 7.122 7.039 1.085
2024-12-02-05:07:28-root-INFO: grad norm: 6.011 5.939 0.926
2024-12-02-05:07:29-root-INFO: grad norm: 5.640 5.563 0.926
2024-12-02-05:07:30-root-INFO: Loss Change: 110.667 -> 105.628
2024-12-02-05:07:30-root-INFO: Regularization Change: 0.000 -> 2.530
2024-12-02-05:07:30-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-05:07:30-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-05:07:30-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-05:07:31-root-INFO: grad norm: 4.047 4.006 0.573
2024-12-02-05:07:32-root-INFO: grad norm: 5.995 5.937 0.835
2024-12-02-05:07:32-root-INFO: Loss too large (104.643->105.554)! Learning rate decreased to 0.11682.
2024-12-02-05:07:32-root-INFO: Loss too large (104.643->104.654)! Learning rate decreased to 0.09346.
2024-12-02-05:07:33-root-INFO: grad norm: 4.775 4.720 0.722
2024-12-02-05:07:34-root-INFO: grad norm: 3.982 3.940 0.577
2024-12-02-05:07:35-root-INFO: grad norm: 3.840 3.799 0.556
2024-12-02-05:07:36-root-INFO: Loss Change: 105.031 -> 102.419
2024-12-02-05:07:36-root-INFO: Regularization Change: 0.000 -> 1.348
2024-12-02-05:07:36-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-05:07:36-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-05:07:36-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-05:07:37-root-INFO: grad norm: 5.319 5.225 0.999
2024-12-02-05:07:37-root-INFO: Loss too large (102.639->103.284)! Learning rate decreased to 0.11969.
2024-12-02-05:07:37-root-INFO: Loss too large (102.639->102.744)! Learning rate decreased to 0.09575.
2024-12-02-05:07:38-root-INFO: grad norm: 4.285 4.239 0.630
2024-12-02-05:07:39-root-INFO: grad norm: 2.985 2.938 0.529
2024-12-02-05:07:40-root-INFO: grad norm: 3.131 3.094 0.481
2024-12-02-05:07:41-root-INFO: grad norm: 4.060 4.028 0.510
2024-12-02-05:07:42-root-INFO: Loss Change: 102.639 -> 100.687
2024-12-02-05:07:42-root-INFO: Regularization Change: 0.000 -> 1.099
2024-12-02-05:07:42-root-INFO: Undo step: 75
2024-12-02-05:07:42-root-INFO: Undo step: 76
2024-12-02-05:07:42-root-INFO: Undo step: 77
2024-12-02-05:07:42-root-INFO: Undo step: 78
2024-12-02-05:07:42-root-INFO: Undo step: 79
2024-12-02-05:07:42-root-INFO: step: 80 lr_xt 0.13231432
2024-12-02-05:07:43-root-INFO: grad norm: 38.188 37.775 5.602
2024-12-02-05:07:44-root-INFO: grad norm: 20.762 20.506 3.247
2024-12-02-05:07:45-root-INFO: grad norm: 19.461 19.235 2.953
2024-12-02-05:07:46-root-INFO: grad norm: 21.806 21.579 3.138
2024-12-02-05:07:46-root-INFO: grad norm: 22.799 22.536 3.449
2024-12-02-05:07:47-root-INFO: Loss Change: 258.468 -> 140.861
2024-12-02-05:07:47-root-INFO: Regularization Change: 0.000 -> 64.014
2024-12-02-05:07:47-root-INFO: Learning rate of xt decay: 0.22523 -> 0.22793.
2024-12-02-05:07:47-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-05:07:48-root-INFO: step: 79 lr_xt 0.13565022
2024-12-02-05:07:48-root-INFO: grad norm: 25.166 24.825 4.129
2024-12-02-05:07:49-root-INFO: grad norm: 25.441 25.112 4.082
2024-12-02-05:07:50-root-INFO: grad norm: 25.544 25.241 3.922
2024-12-02-05:07:51-root-INFO: grad norm: 24.894 24.597 3.832
2024-12-02-05:07:52-root-INFO: grad norm: 23.339 23.079 3.477
2024-12-02-05:07:52-root-INFO: Loss Change: 142.890 -> 130.238
2024-12-02-05:07:52-root-INFO: Regularization Change: 0.000 -> 17.649
2024-12-02-05:07:52-root-INFO: Learning rate of xt decay: 0.22793 -> 0.23067.
2024-12-02-05:07:52-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-05:07:53-root-INFO: step: 78 lr_xt 0.13904792
2024-12-02-05:07:53-root-INFO: grad norm: 21.365 21.142 3.081
2024-12-02-05:07:54-root-INFO: grad norm: 21.175 20.932 3.194
2024-12-02-05:07:54-root-INFO: Loss too large (126.169->126.260)! Learning rate decreased to 0.11124.
2024-12-02-05:07:55-root-INFO: grad norm: 14.027 13.854 2.197
2024-12-02-05:07:56-root-INFO: grad norm: 14.115 13.995 1.837
2024-12-02-05:07:57-root-INFO: grad norm: 8.621 8.510 1.380
2024-12-02-05:07:58-root-INFO: Loss Change: 128.889 -> 109.736
2024-12-02-05:07:58-root-INFO: Regularization Change: 0.000 -> 6.890
2024-12-02-05:07:58-root-INFO: Learning rate of xt decay: 0.23067 -> 0.23344.
2024-12-02-05:07:58-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-05:07:58-root-INFO: step: 77 lr_xt 0.14250787
2024-12-02-05:07:59-root-INFO: grad norm: 8.223 8.095 1.450
2024-12-02-05:08:00-root-INFO: grad norm: 9.812 9.693 1.522
2024-12-02-05:08:01-root-INFO: grad norm: 12.257 12.117 1.842
2024-12-02-05:08:01-root-INFO: Loss too large (109.613->111.097)! Learning rate decreased to 0.11401.
2024-12-02-05:08:02-root-INFO: grad norm: 9.787 9.685 1.414
2024-12-02-05:08:03-root-INFO: grad norm: 8.097 8.001 1.245
2024-12-02-05:08:04-root-INFO: Loss Change: 110.030 -> 104.605
2024-12-02-05:08:04-root-INFO: Regularization Change: 0.000 -> 3.462
2024-12-02-05:08:04-root-INFO: Learning rate of xt decay: 0.23344 -> 0.23624.
2024-12-02-05:08:04-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-05:08:04-root-INFO: step: 76 lr_xt 0.14603050
2024-12-02-05:08:04-root-INFO: grad norm: 6.099 6.056 0.722
2024-12-02-05:08:05-root-INFO: Loss too large (103.787->104.859)! Learning rate decreased to 0.11682.
2024-12-02-05:08:06-root-INFO: grad norm: 9.480 9.417 1.089
2024-12-02-05:08:06-root-INFO: Loss too large (103.436->104.070)! Learning rate decreased to 0.09346.
2024-12-02-05:08:07-root-INFO: grad norm: 4.544 4.487 0.720
2024-12-02-05:08:08-root-INFO: grad norm: 3.591 3.556 0.499
2024-12-02-05:08:09-root-INFO: grad norm: 3.232 3.201 0.444
2024-12-02-05:08:09-root-INFO: Loss Change: 103.787 -> 100.599
2024-12-02-05:08:09-root-INFO: Regularization Change: 0.000 -> 1.317
2024-12-02-05:08:09-root-INFO: Learning rate of xt decay: 0.23624 -> 0.23907.
2024-12-02-05:08:09-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-05:08:10-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-05:08:10-root-INFO: grad norm: 5.156 5.066 0.960
2024-12-02-05:08:10-root-INFO: Loss too large (100.781->101.349)! Learning rate decreased to 0.11969.
2024-12-02-05:08:11-root-INFO: Loss too large (100.781->100.911)! Learning rate decreased to 0.09575.
2024-12-02-05:08:12-root-INFO: grad norm: 4.101 4.061 0.571
2024-12-02-05:08:13-root-INFO: grad norm: 2.673 2.620 0.533
2024-12-02-05:08:14-root-INFO: grad norm: 2.815 2.781 0.441
2024-12-02-05:08:15-root-INFO: grad norm: 4.442 4.410 0.530
2024-12-02-05:08:15-root-INFO: Loss too large (98.926->99.045)! Learning rate decreased to 0.07660.
2024-12-02-05:08:16-root-INFO: Loss Change: 100.781 -> 98.858
2024-12-02-05:08:16-root-INFO: Regularization Change: 0.000 -> 1.016
2024-12-02-05:08:16-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-05:08:16-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-05:08:16-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-05:08:16-root-INFO: grad norm: 3.950 3.909 0.571
2024-12-02-05:08:17-root-INFO: Loss too large (98.815->99.558)! Learning rate decreased to 0.12261.
2024-12-02-05:08:18-root-INFO: grad norm: 8.353 8.329 0.629
2024-12-02-05:08:18-root-INFO: Loss too large (98.684->99.441)! Learning rate decreased to 0.09809.
2024-12-02-05:08:18-root-INFO: Loss too large (98.684->99.079)! Learning rate decreased to 0.07847.
2024-12-02-05:08:19-root-INFO: Loss too large (98.684->98.746)! Learning rate decreased to 0.06278.
2024-12-02-05:08:20-root-INFO: grad norm: 3.732 3.701 0.481
2024-12-02-05:08:21-root-INFO: grad norm: 3.089 3.062 0.404
2024-12-02-05:08:22-root-INFO: grad norm: 2.234 2.202 0.378
2024-12-02-05:08:22-root-INFO: Loss Change: 98.815 -> 97.013
2024-12-02-05:08:22-root-INFO: Regularization Change: 0.000 -> 0.560
2024-12-02-05:08:22-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-05:08:22-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-05:08:23-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-05:08:23-root-INFO: grad norm: 2.844 2.756 0.702
2024-12-02-05:08:24-root-INFO: grad norm: 3.100 3.058 0.509
2024-12-02-05:08:25-root-INFO: grad norm: 4.261 4.205 0.689
2024-12-02-05:08:25-root-INFO: Loss too large (96.197->96.790)! Learning rate decreased to 0.12558.
2024-12-02-05:08:26-root-INFO: grad norm: 6.792 6.770 0.543
2024-12-02-05:08:27-root-INFO: Loss too large (96.098->96.715)! Learning rate decreased to 0.10047.
2024-12-02-05:08:27-root-INFO: Loss too large (96.098->96.272)! Learning rate decreased to 0.08037.
2024-12-02-05:08:28-root-INFO: grad norm: 3.835 3.810 0.439
2024-12-02-05:08:29-root-INFO: Loss Change: 97.076 -> 94.904
2024-12-02-05:08:29-root-INFO: Regularization Change: 0.000 -> 1.267
2024-12-02-05:08:29-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-05:08:29-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-05:08:29-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-05:08:29-root-INFO: grad norm: 3.120 3.034 0.729
2024-12-02-05:08:30-root-INFO: grad norm: 4.399 4.362 0.571
2024-12-02-05:08:31-root-INFO: Loss too large (94.583->95.129)! Learning rate decreased to 0.12860.
2024-12-02-05:08:31-root-INFO: Loss too large (94.583->94.615)! Learning rate decreased to 0.10288.
2024-12-02-05:08:32-root-INFO: grad norm: 4.052 4.027 0.448
2024-12-02-05:08:33-root-INFO: grad norm: 6.379 6.361 0.479
2024-12-02-05:08:33-root-INFO: Loss too large (93.963->94.497)! Learning rate decreased to 0.08231.
2024-12-02-05:08:34-root-INFO: Loss too large (93.963->94.203)! Learning rate decreased to 0.06585.
2024-12-02-05:08:35-root-INFO: grad norm: 3.835 3.809 0.446
2024-12-02-05:08:35-root-INFO: Loss Change: 94.922 -> 93.145
2024-12-02-05:08:35-root-INFO: Regularization Change: 0.000 -> 0.816
2024-12-02-05:08:35-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-05:08:35-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-05:08:36-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-05:08:36-root-INFO: grad norm: 2.416 2.363 0.505
2024-12-02-05:08:37-root-INFO: grad norm: 2.779 2.741 0.458
2024-12-02-05:08:38-root-INFO: grad norm: 4.197 4.135 0.715
2024-12-02-05:08:38-root-INFO: Loss too large (92.546->92.901)! Learning rate decreased to 0.13168.
2024-12-02-05:08:39-root-INFO: grad norm: 4.625 4.588 0.581
2024-12-02-05:08:40-root-INFO: grad norm: 4.551 4.523 0.502
2024-12-02-05:08:41-root-INFO: Loss Change: 93.250 -> 91.840
2024-12-02-05:08:41-root-INFO: Regularization Change: 0.000 -> 1.607
2024-12-02-05:08:41-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-05:08:41-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-05:08:41-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-05:08:42-root-INFO: grad norm: 9.664 9.613 0.993
2024-12-02-05:08:42-root-INFO: Loss too large (91.984->93.400)! Learning rate decreased to 0.13480.
2024-12-02-05:08:42-root-INFO: Loss too large (91.984->92.857)! Learning rate decreased to 0.10784.
2024-12-02-05:08:43-root-INFO: Loss too large (91.984->92.433)! Learning rate decreased to 0.08627.
2024-12-02-05:08:43-root-INFO: Loss too large (91.984->92.072)! Learning rate decreased to 0.06902.
2024-12-02-05:08:44-root-INFO: grad norm: 3.428 3.400 0.441
2024-12-02-05:08:45-root-INFO: grad norm: 3.450 3.428 0.384
2024-12-02-05:08:46-root-INFO: grad norm: 1.965 1.937 0.336
2024-12-02-05:08:47-root-INFO: grad norm: 1.887 1.860 0.322
2024-12-02-05:08:48-root-INFO: Loss Change: 91.984 -> 89.856
2024-12-02-05:08:48-root-INFO: Regularization Change: 0.000 -> 0.446
2024-12-02-05:08:48-root-INFO: Undo step: 70
2024-12-02-05:08:48-root-INFO: Undo step: 71
2024-12-02-05:08:48-root-INFO: Undo step: 72
2024-12-02-05:08:48-root-INFO: Undo step: 73
2024-12-02-05:08:48-root-INFO: Undo step: 74
2024-12-02-05:08:48-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-05:08:48-root-INFO: grad norm: 37.425 37.171 4.348
2024-12-02-05:08:49-root-INFO: grad norm: 22.054 21.899 2.608
2024-12-02-05:08:50-root-INFO: grad norm: 17.041 16.921 2.022
2024-12-02-05:08:51-root-INFO: grad norm: 15.441 15.345 1.710
2024-12-02-05:08:52-root-INFO: grad norm: 15.004 14.891 1.833
2024-12-02-05:08:53-root-INFO: Loss Change: 229.778 -> 120.582
2024-12-02-05:08:53-root-INFO: Regularization Change: 0.000 -> 66.948
2024-12-02-05:08:53-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-05:08:53-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-05:08:53-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-05:08:53-root-INFO: grad norm: 16.758 16.604 2.262
2024-12-02-05:08:55-root-INFO: grad norm: 16.914 16.736 2.444
2024-12-02-05:08:56-root-INFO: grad norm: 16.977 16.797 2.460
2024-12-02-05:08:57-root-INFO: grad norm: 17.222 17.004 2.732
2024-12-02-05:08:58-root-INFO: grad norm: 17.784 17.553 2.853
2024-12-02-05:08:58-root-INFO: Loss too large (115.475->115.827)! Learning rate decreased to 0.12261.
2024-12-02-05:08:58-root-INFO: Loss Change: 121.868 -> 108.158
2024-12-02-05:08:58-root-INFO: Regularization Change: 0.000 -> 11.016
2024-12-02-05:08:58-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-05:08:59-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-05:08:59-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-05:08:59-root-INFO: grad norm: 10.776 10.646 1.664
2024-12-02-05:09:00-root-INFO: grad norm: 12.049 11.894 1.928
2024-12-02-05:09:01-root-INFO: Loss too large (106.236->107.034)! Learning rate decreased to 0.12558.
2024-12-02-05:09:01-root-INFO: grad norm: 9.147 9.019 1.524
2024-12-02-05:09:02-root-INFO: grad norm: 7.115 7.022 1.147
2024-12-02-05:09:03-root-INFO: grad norm: 5.906 5.826 0.972
2024-12-02-05:09:04-root-INFO: Loss Change: 106.916 -> 98.450
2024-12-02-05:09:04-root-INFO: Regularization Change: 0.000 -> 4.340
2024-12-02-05:09:04-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-05:09:04-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-05:09:04-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-05:09:05-root-INFO: grad norm: 6.723 6.598 1.290
2024-12-02-05:09:06-root-INFO: grad norm: 8.240 8.124 1.383
2024-12-02-05:09:06-root-INFO: Loss too large (98.793->99.135)! Learning rate decreased to 0.12860.
2024-12-02-05:09:07-root-INFO: grad norm: 6.901 6.808 1.132
2024-12-02-05:09:08-root-INFO: grad norm: 5.953 5.875 0.960
2024-12-02-05:09:09-root-INFO: grad norm: 5.212 5.138 0.876
2024-12-02-05:09:10-root-INFO: Loss Change: 98.931 -> 94.304
2024-12-02-05:09:10-root-INFO: Regularization Change: 0.000 -> 3.071
2024-12-02-05:09:10-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-05:09:10-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-05:09:10-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-05:09:10-root-INFO: grad norm: 3.940 3.900 0.559
2024-12-02-05:09:11-root-INFO: grad norm: 4.950 4.889 0.779
2024-12-02-05:09:12-root-INFO: Loss too large (93.641->93.694)! Learning rate decreased to 0.13168.
2024-12-02-05:09:13-root-INFO: grad norm: 4.707 4.646 0.756
2024-12-02-05:09:14-root-INFO: grad norm: 5.051 4.990 0.785
2024-12-02-05:09:15-root-INFO: grad norm: 4.868 4.802 0.798
2024-12-02-05:09:15-root-INFO: Loss Change: 94.100 -> 91.478
2024-12-02-05:09:15-root-INFO: Regularization Change: 0.000 -> 2.105
2024-12-02-05:09:15-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-05:09:15-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-05:09:16-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-05:09:16-root-INFO: grad norm: 7.061 6.905 1.477
2024-12-02-05:09:16-root-INFO: Loss too large (92.086->93.307)! Learning rate decreased to 0.13480.
2024-12-02-05:09:17-root-INFO: grad norm: 6.245 6.152 1.078
2024-12-02-05:09:18-root-INFO: grad norm: 5.173 5.092 0.914
2024-12-02-05:09:19-root-INFO: grad norm: 4.960 4.892 0.818
2024-12-02-05:09:20-root-INFO: grad norm: 6.144 6.080 0.882
2024-12-02-05:09:21-root-INFO: Loss too large (89.650->89.875)! Learning rate decreased to 0.10784.
2024-12-02-05:09:21-root-INFO: Loss Change: 92.086 -> 89.309
2024-12-02-05:09:21-root-INFO: Regularization Change: 0.000 -> 1.927
2024-12-02-05:09:21-root-INFO: Undo step: 70
2024-12-02-05:09:21-root-INFO: Undo step: 71
2024-12-02-05:09:21-root-INFO: Undo step: 72
2024-12-02-05:09:21-root-INFO: Undo step: 73
2024-12-02-05:09:21-root-INFO: Undo step: 74
2024-12-02-05:09:22-root-INFO: step: 75 lr_xt 0.14961620
2024-12-02-05:09:22-root-INFO: grad norm: 31.790 31.426 4.794
2024-12-02-05:09:23-root-INFO: grad norm: 19.732 19.527 2.837
2024-12-02-05:09:24-root-INFO: grad norm: 17.082 16.824 2.957
2024-12-02-05:09:25-root-INFO: grad norm: 19.274 19.060 2.862
2024-12-02-05:09:26-root-INFO: grad norm: 19.484 19.209 3.264
2024-12-02-05:09:26-root-INFO: Loss Change: 228.847 -> 125.837
2024-12-02-05:09:26-root-INFO: Regularization Change: 0.000 -> 66.144
2024-12-02-05:09:26-root-INFO: Learning rate of xt decay: 0.23907 -> 0.24194.
2024-12-02-05:09:26-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-05:09:27-root-INFO: step: 74 lr_xt 0.15326538
2024-12-02-05:09:27-root-INFO: grad norm: 19.772 19.571 2.814
2024-12-02-05:09:28-root-INFO: grad norm: 18.545 18.295 3.039
2024-12-02-05:09:29-root-INFO: grad norm: 18.197 18.000 2.669
2024-12-02-05:09:30-root-INFO: grad norm: 17.487 17.228 2.997
2024-12-02-05:09:31-root-INFO: grad norm: 17.004 16.811 2.554
2024-12-02-05:09:32-root-INFO: Loss Change: 127.492 -> 111.935
2024-12-02-05:09:32-root-INFO: Regularization Change: 0.000 -> 15.963
2024-12-02-05:09:32-root-INFO: Learning rate of xt decay: 0.24194 -> 0.24485.
2024-12-02-05:09:32-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-05:09:32-root-INFO: step: 73 lr_xt 0.15697839
2024-12-02-05:09:33-root-INFO: grad norm: 15.508 15.314 2.441
2024-12-02-05:09:34-root-INFO: grad norm: 17.097 16.914 2.495
2024-12-02-05:09:34-root-INFO: Loss too large (109.396->109.594)! Learning rate decreased to 0.12558.
2024-12-02-05:09:35-root-INFO: grad norm: 10.207 10.034 1.868
2024-12-02-05:09:36-root-INFO: grad norm: 6.960 6.890 0.982
2024-12-02-05:09:37-root-INFO: grad norm: 5.500 5.419 0.938
2024-12-02-05:09:37-root-INFO: Loss Change: 110.224 -> 96.285
2024-12-02-05:09:37-root-INFO: Regularization Change: 0.000 -> 5.356
2024-12-02-05:09:37-root-INFO: Learning rate of xt decay: 0.24485 -> 0.24778.
2024-12-02-05:09:37-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-05:09:38-root-INFO: step: 72 lr_xt 0.16075558
2024-12-02-05:09:38-root-INFO: grad norm: 6.069 5.959 1.149
2024-12-02-05:09:39-root-INFO: grad norm: 7.136 7.026 1.243
2024-12-02-05:09:39-root-INFO: Loss too large (96.162->96.404)! Learning rate decreased to 0.12860.
2024-12-02-05:09:40-root-INFO: grad norm: 6.656 6.579 1.011
2024-12-02-05:09:42-root-INFO: grad norm: 5.902 5.817 1.001
2024-12-02-05:09:43-root-INFO: grad norm: 5.008 4.943 0.809
2024-12-02-05:09:43-root-INFO: Loss Change: 96.667 -> 92.598
2024-12-02-05:09:43-root-INFO: Regularization Change: 0.000 -> 2.892
2024-12-02-05:09:43-root-INFO: Learning rate of xt decay: 0.24778 -> 0.25076.
2024-12-02-05:09:43-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-05:09:44-root-INFO: step: 71 lr_xt 0.16459726
2024-12-02-05:09:44-root-INFO: grad norm: 4.322 4.274 0.645
2024-12-02-05:09:44-root-INFO: Loss too large (92.368->92.529)! Learning rate decreased to 0.13168.
2024-12-02-05:09:45-root-INFO: grad norm: 5.964 5.920 0.721
2024-12-02-05:09:46-root-INFO: Loss too large (91.861->92.087)! Learning rate decreased to 0.10534.
2024-12-02-05:09:47-root-INFO: grad norm: 4.065 4.016 0.635
2024-12-02-05:09:48-root-INFO: grad norm: 2.571 2.539 0.402
2024-12-02-05:09:49-root-INFO: grad norm: 2.193 2.164 0.355
2024-12-02-05:09:49-root-INFO: Loss Change: 92.368 -> 89.566
2024-12-02-05:09:49-root-INFO: Regularization Change: 0.000 -> 1.341
2024-12-02-05:09:49-root-INFO: Learning rate of xt decay: 0.25076 -> 0.25377.
2024-12-02-05:09:49-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-05:09:50-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-05:09:50-root-INFO: grad norm: 3.382 3.290 0.783
2024-12-02-05:09:51-root-INFO: grad norm: 3.771 3.707 0.693
2024-12-02-05:09:52-root-INFO: grad norm: 5.717 5.642 0.920
2024-12-02-05:09:52-root-INFO: Loss too large (88.717->89.842)! Learning rate decreased to 0.13480.
2024-12-02-05:09:52-root-INFO: Loss too large (88.717->88.873)! Learning rate decreased to 0.10784.
2024-12-02-05:09:53-root-INFO: grad norm: 4.524 4.468 0.712
2024-12-02-05:09:54-root-INFO: grad norm: 3.879 3.844 0.518
2024-12-02-05:09:55-root-INFO: Loss Change: 89.526 -> 87.317
2024-12-02-05:09:55-root-INFO: Regularization Change: 0.000 -> 1.703
2024-12-02-05:09:55-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-05:09:55-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-05:09:55-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-05:09:56-root-INFO: grad norm: 3.662 3.633 0.460
2024-12-02-05:09:56-root-INFO: Loss too large (87.321->87.662)! Learning rate decreased to 0.13798.
2024-12-02-05:09:57-root-INFO: grad norm: 6.197 6.170 0.585
2024-12-02-05:09:57-root-INFO: Loss too large (86.949->87.582)! Learning rate decreased to 0.11038.
2024-12-02-05:09:58-root-INFO: Loss too large (86.949->87.181)! Learning rate decreased to 0.08831.
2024-12-02-05:09:59-root-INFO: grad norm: 3.590 3.556 0.489
2024-12-02-05:10:00-root-INFO: grad norm: 2.293 2.270 0.328
2024-12-02-05:10:01-root-INFO: grad norm: 2.048 2.026 0.302
2024-12-02-05:10:01-root-INFO: Loss Change: 87.321 -> 85.343
2024-12-02-05:10:01-root-INFO: Regularization Change: 0.000 -> 0.801
2024-12-02-05:10:01-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-05:10:01-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-05:10:02-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-05:10:02-root-INFO: grad norm: 3.059 2.975 0.711
2024-12-02-05:10:03-root-INFO: grad norm: 3.391 3.334 0.622
2024-12-02-05:10:04-root-INFO: grad norm: 5.836 5.768 0.886
2024-12-02-05:10:04-root-INFO: Loss too large (84.669->86.231)! Learning rate decreased to 0.14121.
2024-12-02-05:10:05-root-INFO: Loss too large (84.669->85.244)! Learning rate decreased to 0.11297.
2024-12-02-05:10:06-root-INFO: grad norm: 4.578 4.522 0.712
2024-12-02-05:10:07-root-INFO: grad norm: 2.877 2.841 0.452
2024-12-02-05:10:07-root-INFO: Loss Change: 85.278 -> 83.171
2024-12-02-05:10:07-root-INFO: Regularization Change: 0.000 -> 1.499
2024-12-02-05:10:07-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-05:10:07-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-05:10:08-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-05:10:08-root-INFO: grad norm: 2.988 2.957 0.430
2024-12-02-05:10:08-root-INFO: Loss too large (83.031->83.637)! Learning rate decreased to 0.14449.
2024-12-02-05:10:09-root-INFO: Loss too large (83.031->83.064)! Learning rate decreased to 0.11559.
2024-12-02-05:10:10-root-INFO: grad norm: 4.691 4.669 0.455
2024-12-02-05:10:10-root-INFO: Loss too large (82.788->83.071)! Learning rate decreased to 0.09247.
2024-12-02-05:10:10-root-INFO: Loss too large (82.788->82.812)! Learning rate decreased to 0.07398.
2024-12-02-05:10:11-root-INFO: grad norm: 3.317 3.294 0.398
2024-12-02-05:10:12-root-INFO: grad norm: 1.824 1.802 0.285
2024-12-02-05:10:13-root-INFO: grad norm: 1.759 1.737 0.278
2024-12-02-05:10:14-root-INFO: Loss Change: 83.031 -> 81.711
2024-12-02-05:10:14-root-INFO: Regularization Change: 0.000 -> 0.496
2024-12-02-05:10:14-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-05:10:14-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-05:10:14-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-05:10:15-root-INFO: grad norm: 2.727 2.685 0.474
2024-12-02-05:10:15-root-INFO: Loss too large (81.819->81.880)! Learning rate decreased to 0.14783.
2024-12-02-05:10:16-root-INFO: grad norm: 3.683 3.650 0.490
2024-12-02-05:10:16-root-INFO: Loss too large (81.708->82.085)! Learning rate decreased to 0.11826.
2024-12-02-05:10:17-root-INFO: grad norm: 5.806 5.778 0.563
2024-12-02-05:10:17-root-INFO: Loss too large (81.460->81.956)! Learning rate decreased to 0.09461.
2024-12-02-05:10:18-root-INFO: Loss too large (81.460->81.605)! Learning rate decreased to 0.07569.
2024-12-02-05:10:19-root-INFO: grad norm: 3.534 3.509 0.423
2024-12-02-05:10:20-root-INFO: grad norm: 1.663 1.642 0.268
2024-12-02-05:10:20-root-INFO: Loss Change: 81.819 -> 80.431
2024-12-02-05:10:20-root-INFO: Regularization Change: 0.000 -> 0.643
2024-12-02-05:10:20-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-05:10:20-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-05:10:21-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-05:10:21-root-INFO: grad norm: 2.945 2.876 0.634
2024-12-02-05:10:22-root-INFO: grad norm: 4.494 4.438 0.703
2024-12-02-05:10:22-root-INFO: Loss too large (80.496->82.900)! Learning rate decreased to 0.15121.
2024-12-02-05:10:23-root-INFO: Loss too large (80.496->81.105)! Learning rate decreased to 0.12097.
2024-12-02-05:10:24-root-INFO: grad norm: 6.720 6.685 0.683
2024-12-02-05:10:24-root-INFO: Loss too large (80.186->80.794)! Learning rate decreased to 0.09678.
2024-12-02-05:10:24-root-INFO: Loss too large (80.186->80.365)! Learning rate decreased to 0.07742.
2024-12-02-05:10:25-root-INFO: grad norm: 3.590 3.564 0.436
2024-12-02-05:10:26-root-INFO: grad norm: 2.004 1.986 0.270
2024-12-02-05:10:27-root-INFO: Loss Change: 80.538 -> 78.990
2024-12-02-05:10:27-root-INFO: Regularization Change: 0.000 -> 0.771
2024-12-02-05:10:27-root-INFO: Undo step: 65
2024-12-02-05:10:27-root-INFO: Undo step: 66
2024-12-02-05:10:27-root-INFO: Undo step: 67
2024-12-02-05:10:27-root-INFO: Undo step: 68
2024-12-02-05:10:27-root-INFO: Undo step: 69
2024-12-02-05:10:27-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-05:10:28-root-INFO: grad norm: 33.426 33.150 4.289
2024-12-02-05:10:29-root-INFO: grad norm: 21.335 21.209 2.312
2024-12-02-05:10:30-root-INFO: grad norm: 14.046 13.945 1.686
2024-12-02-05:10:31-root-INFO: grad norm: 12.131 12.061 1.307
2024-12-02-05:10:32-root-INFO: grad norm: 11.311 11.228 1.366
2024-12-02-05:10:32-root-INFO: Loss Change: 213.796 -> 103.911
2024-12-02-05:10:32-root-INFO: Regularization Change: 0.000 -> 64.569
2024-12-02-05:10:32-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-05:10:32-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-05:10:33-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-05:10:33-root-INFO: grad norm: 11.737 11.638 1.524
2024-12-02-05:10:34-root-INFO: grad norm: 11.951 11.819 1.770
2024-12-02-05:10:35-root-INFO: grad norm: 12.801 12.666 1.858
2024-12-02-05:10:35-root-INFO: Loss too large (100.962->101.525)! Learning rate decreased to 0.13798.
2024-12-02-05:10:36-root-INFO: grad norm: 9.304 9.167 1.588
2024-12-02-05:10:37-root-INFO: grad norm: 7.006 6.909 1.163
2024-12-02-05:10:38-root-INFO: Loss Change: 104.505 -> 92.314
2024-12-02-05:10:38-root-INFO: Regularization Change: 0.000 -> 7.821
2024-12-02-05:10:38-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-05:10:38-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-05:10:38-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-05:10:39-root-INFO: grad norm: 4.701 4.636 0.776
2024-12-02-05:10:40-root-INFO: grad norm: 5.255 5.187 0.844
2024-12-02-05:10:41-root-INFO: grad norm: 6.353 6.270 1.022
2024-12-02-05:10:41-root-INFO: Loss too large (89.804->90.012)! Learning rate decreased to 0.14121.
2024-12-02-05:10:42-root-INFO: grad norm: 6.896 6.826 0.986
2024-12-02-05:10:43-root-INFO: grad norm: 5.636 5.561 0.915
2024-12-02-05:10:44-root-INFO: Loss Change: 91.479 -> 86.854
2024-12-02-05:10:44-root-INFO: Regularization Change: 0.000 -> 4.048
2024-12-02-05:10:44-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-05:10:44-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-05:10:44-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-05:10:44-root-INFO: grad norm: 5.800 5.700 1.073
2024-12-02-05:10:45-root-INFO: grad norm: 7.004 6.914 1.119
2024-12-02-05:10:46-root-INFO: Loss too large (86.975->87.210)! Learning rate decreased to 0.14449.
2024-12-02-05:10:47-root-INFO: grad norm: 5.920 5.857 0.860
2024-12-02-05:10:48-root-INFO: grad norm: 6.047 6.007 0.695
2024-12-02-05:10:49-root-INFO: grad norm: 5.001 4.965 0.605
2024-12-02-05:10:49-root-INFO: Loss Change: 87.121 -> 83.177
2024-12-02-05:10:49-root-INFO: Regularization Change: 0.000 -> 2.831
2024-12-02-05:10:49-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-05:10:49-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-05:10:50-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-05:10:50-root-INFO: grad norm: 3.265 3.236 0.430
2024-12-02-05:10:51-root-INFO: grad norm: 3.981 3.954 0.457
2024-12-02-05:10:51-root-INFO: Loss too large (82.528->82.816)! Learning rate decreased to 0.14783.
2024-12-02-05:10:52-root-INFO: grad norm: 5.663 5.642 0.480
2024-12-02-05:10:53-root-INFO: Loss too large (82.169->82.478)! Learning rate decreased to 0.11826.
2024-12-02-05:10:53-root-INFO: grad norm: 3.737 3.711 0.440
2024-12-02-05:10:54-root-INFO: grad norm: 2.263 2.236 0.347
2024-12-02-05:10:55-root-INFO: Loss Change: 83.048 -> 80.442
2024-12-02-05:10:55-root-INFO: Regularization Change: 0.000 -> 1.671
2024-12-02-05:10:55-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-05:10:55-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-05:10:56-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-05:10:56-root-INFO: grad norm: 3.285 3.214 0.678
2024-12-02-05:10:57-root-INFO: grad norm: 4.298 4.245 0.675
2024-12-02-05:10:57-root-INFO: Loss too large (80.395->80.906)! Learning rate decreased to 0.15121.
2024-12-02-05:10:58-root-INFO: grad norm: 6.073 6.025 0.759
2024-12-02-05:10:58-root-INFO: Loss too large (79.932->80.524)! Learning rate decreased to 0.12097.
2024-12-02-05:10:59-root-INFO: Loss too large (79.932->79.991)! Learning rate decreased to 0.09678.
2024-12-02-05:11:00-root-INFO: grad norm: 3.583 3.552 0.471
2024-12-02-05:11:01-root-INFO: grad norm: 1.917 1.892 0.310
2024-12-02-05:11:01-root-INFO: Loss Change: 80.544 -> 78.406
2024-12-02-05:11:01-root-INFO: Regularization Change: 0.000 -> 1.232
2024-12-02-05:11:01-root-INFO: Undo step: 65
2024-12-02-05:11:01-root-INFO: Undo step: 66
2024-12-02-05:11:01-root-INFO: Undo step: 67
2024-12-02-05:11:01-root-INFO: Undo step: 68
2024-12-02-05:11:01-root-INFO: Undo step: 69
2024-12-02-05:11:02-root-INFO: step: 70 lr_xt 0.16850375
2024-12-02-05:11:02-root-INFO: grad norm: 31.771 31.462 4.423
2024-12-02-05:11:03-root-INFO: grad norm: 15.562 15.223 3.230
2024-12-02-05:11:04-root-INFO: grad norm: 11.132 10.980 1.835
2024-12-02-05:11:05-root-INFO: grad norm: 10.451 10.343 1.500
2024-12-02-05:11:06-root-INFO: grad norm: 11.808 11.708 1.536
2024-12-02-05:11:07-root-INFO: Loss Change: 217.693 -> 104.562
2024-12-02-05:11:07-root-INFO: Regularization Change: 0.000 -> 70.693
2024-12-02-05:11:07-root-INFO: Learning rate of xt decay: 0.25377 -> 0.25681.
2024-12-02-05:11:07-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-05:11:07-root-INFO: step: 69 lr_xt 0.17247530
2024-12-02-05:11:07-root-INFO: grad norm: 12.316 12.243 1.338
2024-12-02-05:11:08-root-INFO: grad norm: 12.406 12.310 1.543
2024-12-02-05:11:09-root-INFO: grad norm: 12.319 12.215 1.595
2024-12-02-05:11:10-root-INFO: grad norm: 11.775 11.667 1.596
2024-12-02-05:11:11-root-INFO: grad norm: 12.238 12.121 1.691
2024-12-02-05:11:12-root-INFO: Loss too large (95.736->95.851)! Learning rate decreased to 0.13798.
2024-12-02-05:11:12-root-INFO: Loss Change: 104.488 -> 91.863
2024-12-02-05:11:12-root-INFO: Regularization Change: 0.000 -> 10.458
2024-12-02-05:11:12-root-INFO: Learning rate of xt decay: 0.25681 -> 0.25989.
2024-12-02-05:11:12-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-05:11:13-root-INFO: step: 68 lr_xt 0.17651217
2024-12-02-05:11:13-root-INFO: grad norm: 10.905 10.749 1.842
2024-12-02-05:11:13-root-INFO: Loss too large (92.419->93.001)! Learning rate decreased to 0.14121.
2024-12-02-05:11:14-root-INFO: grad norm: 7.246 7.127 1.309
2024-12-02-05:11:15-root-INFO: grad norm: 5.742 5.670 0.906
2024-12-02-05:11:16-root-INFO: grad norm: 4.784 4.723 0.761
2024-12-02-05:11:17-root-INFO: grad norm: 4.496 4.440 0.705
2024-12-02-05:11:18-root-INFO: Loss Change: 92.419 -> 84.652
2024-12-02-05:11:18-root-INFO: Regularization Change: 0.000 -> 4.034
2024-12-02-05:11:18-root-INFO: Learning rate of xt decay: 0.25989 -> 0.26301.
2024-12-02-05:11:18-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-05:11:18-root-INFO: step: 67 lr_xt 0.18061458
2024-12-02-05:11:19-root-INFO: grad norm: 4.221 4.187 0.531
2024-12-02-05:11:20-root-INFO: grad norm: 5.564 5.522 0.679
2024-12-02-05:11:20-root-INFO: Loss too large (84.181->84.346)! Learning rate decreased to 0.14449.
2024-12-02-05:11:21-root-INFO: grad norm: 5.161 5.111 0.721
2024-12-02-05:11:22-root-INFO: grad norm: 5.754 5.698 0.797
2024-12-02-05:11:22-root-INFO: Loss too large (82.693->82.746)! Learning rate decreased to 0.11559.
2024-12-02-05:11:23-root-INFO: grad norm: 4.295 4.248 0.638
2024-12-02-05:11:24-root-INFO: Loss Change: 84.373 -> 81.130
2024-12-02-05:11:24-root-INFO: Regularization Change: 0.000 -> 2.219
2024-12-02-05:11:24-root-INFO: Learning rate of xt decay: 0.26301 -> 0.26617.
2024-12-02-05:11:24-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-05:11:24-root-INFO: step: 66 lr_xt 0.18478272
2024-12-02-05:11:25-root-INFO: grad norm: 3.863 3.789 0.752
2024-12-02-05:11:25-root-INFO: Loss too large (81.303->81.353)! Learning rate decreased to 0.14783.
2024-12-02-05:11:26-root-INFO: grad norm: 4.222 4.170 0.661
2024-12-02-05:11:27-root-INFO: grad norm: 5.820 5.766 0.789
2024-12-02-05:11:27-root-INFO: Loss too large (80.579->80.957)! Learning rate decreased to 0.11826.
2024-12-02-05:11:28-root-INFO: grad norm: 4.201 4.154 0.622
2024-12-02-05:11:29-root-INFO: grad norm: 2.461 2.426 0.417
2024-12-02-05:11:30-root-INFO: Loss Change: 81.303 -> 78.766
2024-12-02-05:11:30-root-INFO: Regularization Change: 0.000 -> 1.532
2024-12-02-05:11:30-root-INFO: Learning rate of xt decay: 0.26617 -> 0.26936.
2024-12-02-05:11:30-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-05:11:30-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-05:11:31-root-INFO: grad norm: 2.731 2.660 0.617
2024-12-02-05:11:32-root-INFO: grad norm: 3.299 3.257 0.521
2024-12-02-05:11:32-root-INFO: Loss too large (78.241->78.278)! Learning rate decreased to 0.15121.
2024-12-02-05:11:33-root-INFO: grad norm: 4.307 4.271 0.559
2024-12-02-05:11:33-root-INFO: Loss too large (77.898->78.058)! Learning rate decreased to 0.12097.
2024-12-02-05:11:34-root-INFO: grad norm: 3.850 3.819 0.492
2024-12-02-05:11:35-root-INFO: grad norm: 3.423 3.394 0.444
2024-12-02-05:11:36-root-INFO: Loss Change: 78.741 -> 76.847
2024-12-02-05:11:36-root-INFO: Regularization Change: 0.000 -> 1.567
2024-12-02-05:11:36-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-05:11:36-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-05:11:36-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-05:11:37-root-INFO: grad norm: 3.276 3.251 0.404
2024-12-02-05:11:37-root-INFO: Loss too large (76.684->76.983)! Learning rate decreased to 0.15465.
2024-12-02-05:11:38-root-INFO: grad norm: 5.256 5.233 0.492
2024-12-02-05:11:38-root-INFO: Loss too large (76.419->76.946)! Learning rate decreased to 0.12372.
2024-12-02-05:11:39-root-INFO: Loss too large (76.419->76.513)! Learning rate decreased to 0.09898.
2024-12-02-05:11:40-root-INFO: grad norm: 3.464 3.437 0.425
2024-12-02-05:11:41-root-INFO: grad norm: 1.759 1.733 0.301
2024-12-02-05:11:42-root-INFO: grad norm: 1.636 1.612 0.279
2024-12-02-05:11:42-root-INFO: Loss Change: 76.684 -> 74.898
2024-12-02-05:11:42-root-INFO: Regularization Change: 0.000 -> 0.853
2024-12-02-05:11:42-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-05:11:42-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-05:11:43-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-05:11:43-root-INFO: grad norm: 2.460 2.396 0.558
2024-12-02-05:11:44-root-INFO: grad norm: 3.322 3.276 0.553
2024-12-02-05:11:44-root-INFO: Loss too large (74.687->75.276)! Learning rate decreased to 0.15815.
2024-12-02-05:11:45-root-INFO: grad norm: 5.705 5.659 0.721
2024-12-02-05:11:46-root-INFO: Loss too large (74.643->75.413)! Learning rate decreased to 0.12652.
2024-12-02-05:11:46-root-INFO: Loss too large (74.643->74.833)! Learning rate decreased to 0.10121.
2024-12-02-05:11:47-root-INFO: grad norm: 3.683 3.652 0.483
2024-12-02-05:11:48-root-INFO: grad norm: 1.740 1.715 0.296
2024-12-02-05:11:49-root-INFO: Loss Change: 75.040 -> 73.297
2024-12-02-05:11:49-root-INFO: Regularization Change: 0.000 -> 1.093
2024-12-02-05:11:49-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-05:11:49-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-05:11:49-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-05:11:49-root-INFO: grad norm: 2.522 2.440 0.640
2024-12-02-05:11:50-root-INFO: grad norm: 3.048 2.997 0.558
2024-12-02-05:11:50-root-INFO: Loss too large (72.729->72.958)! Learning rate decreased to 0.16169.
2024-12-02-05:11:51-root-INFO: grad norm: 4.603 4.559 0.633
2024-12-02-05:11:52-root-INFO: Loss too large (72.545->73.061)! Learning rate decreased to 0.12935.
2024-12-02-05:11:52-root-INFO: Loss too large (72.545->72.614)! Learning rate decreased to 0.10348.
2024-12-02-05:11:53-root-INFO: grad norm: 3.400 3.373 0.428
2024-12-02-05:11:54-root-INFO: grad norm: 2.110 2.086 0.315
2024-12-02-05:11:55-root-INFO: Loss Change: 73.154 -> 71.477
2024-12-02-05:11:55-root-INFO: Regularization Change: 0.000 -> 1.099
2024-12-02-05:11:55-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-05:11:55-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-05:11:55-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-05:11:56-root-INFO: grad norm: 2.002 1.971 0.348
2024-12-02-05:11:57-root-INFO: grad norm: 3.400 3.384 0.327
2024-12-02-05:11:57-root-INFO: Loss too large (71.140->71.695)! Learning rate decreased to 0.16529.
2024-12-02-05:11:57-root-INFO: Loss too large (71.140->71.360)! Learning rate decreased to 0.13223.
2024-12-02-05:11:58-root-INFO: grad norm: 3.378 3.357 0.372
2024-12-02-05:11:59-root-INFO: grad norm: 3.472 3.450 0.392
2024-12-02-05:12:00-root-INFO: Loss too large (70.593->70.621)! Learning rate decreased to 0.10579.
2024-12-02-05:12:01-root-INFO: grad norm: 2.964 2.943 0.356
2024-12-02-05:12:01-root-INFO: Loss Change: 71.460 -> 70.019
2024-12-02-05:12:01-root-INFO: Regularization Change: 0.000 -> 1.020
2024-12-02-05:12:01-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-05:12:01-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-05:12:02-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-05:12:02-root-INFO: grad norm: 3.443 3.374 0.683
2024-12-02-05:12:02-root-INFO: Loss too large (70.190->70.755)! Learning rate decreased to 0.16894.
2024-12-02-05:12:03-root-INFO: Loss too large (70.190->70.312)! Learning rate decreased to 0.13515.
2024-12-02-05:12:03-root-INFO: grad norm: 3.469 3.438 0.460
2024-12-02-05:12:05-root-INFO: grad norm: 4.231 4.201 0.504
2024-12-02-05:12:05-root-INFO: Loss too large (69.673->69.862)! Learning rate decreased to 0.10812.
2024-12-02-05:12:06-root-INFO: grad norm: 3.365 3.340 0.413
2024-12-02-05:12:07-root-INFO: grad norm: 2.157 2.135 0.309
2024-12-02-05:12:07-root-INFO: Loss Change: 70.190 -> 68.763
2024-12-02-05:12:08-root-INFO: Regularization Change: 0.000 -> 0.797
2024-12-02-05:12:08-root-INFO: Undo step: 60
2024-12-02-05:12:08-root-INFO: Undo step: 61
2024-12-02-05:12:08-root-INFO: Undo step: 62
2024-12-02-05:12:08-root-INFO: Undo step: 63
2024-12-02-05:12:08-root-INFO: Undo step: 64
2024-12-02-05:12:08-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-05:12:08-root-INFO: grad norm: 27.092 26.912 3.117
2024-12-02-05:12:09-root-INFO: grad norm: 15.463 15.328 2.041
2024-12-02-05:12:10-root-INFO: grad norm: 11.105 10.925 1.992
2024-12-02-05:12:11-root-INFO: grad norm: 11.043 10.920 1.642
2024-12-02-05:12:12-root-INFO: grad norm: 13.020 12.853 2.079
2024-12-02-05:12:12-root-INFO: Loss too large (97.792->97.983)! Learning rate decreased to 0.15121.
2024-12-02-05:12:13-root-INFO: Loss Change: 198.494 -> 92.766
2024-12-02-05:12:13-root-INFO: Regularization Change: 0.000 -> 72.449
2024-12-02-05:12:13-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-05:12:13-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-05:12:13-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-05:12:14-root-INFO: grad norm: 12.062 11.930 1.780
2024-12-02-05:12:14-root-INFO: Loss too large (93.557->94.572)! Learning rate decreased to 0.15465.
2024-12-02-05:12:15-root-INFO: grad norm: 10.726 10.603 1.618
2024-12-02-05:12:16-root-INFO: grad norm: 9.671 9.572 1.384
2024-12-02-05:12:17-root-INFO: grad norm: 8.963 8.860 1.359
2024-12-02-05:12:18-root-INFO: grad norm: 8.674 8.579 1.277
2024-12-02-05:12:19-root-INFO: Loss Change: 93.557 -> 82.729
2024-12-02-05:12:19-root-INFO: Regularization Change: 0.000 -> 8.291
2024-12-02-05:12:19-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-05:12:19-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-05:12:19-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-05:12:20-root-INFO: grad norm: 7.640 7.573 1.012
2024-12-02-05:12:20-root-INFO: Loss too large (82.226->83.367)! Learning rate decreased to 0.15815.
2024-12-02-05:12:21-root-INFO: grad norm: 7.640 7.560 1.105
2024-12-02-05:12:22-root-INFO: grad norm: 7.333 7.245 1.131
2024-12-02-05:12:23-root-INFO: grad norm: 6.988 6.907 1.062
2024-12-02-05:12:24-root-INFO: grad norm: 6.973 6.887 1.091
2024-12-02-05:12:24-root-INFO: Loss Change: 82.226 -> 77.748
2024-12-02-05:12:24-root-INFO: Regularization Change: 0.000 -> 4.388
2024-12-02-05:12:24-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-05:12:24-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-05:12:25-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-05:12:25-root-INFO: grad norm: 9.095 8.942 1.659
2024-12-02-05:12:25-root-INFO: Loss too large (78.400->81.481)! Learning rate decreased to 0.16169.
2024-12-02-05:12:26-root-INFO: Loss too large (78.400->78.459)! Learning rate decreased to 0.12935.
2024-12-02-05:12:27-root-INFO: grad norm: 5.774 5.687 0.997
2024-12-02-05:12:28-root-INFO: grad norm: 3.322 3.276 0.551
2024-12-02-05:12:29-root-INFO: grad norm: 2.540 2.504 0.424
2024-12-02-05:12:30-root-INFO: grad norm: 2.217 2.187 0.363
2024-12-02-05:12:30-root-INFO: Loss Change: 78.400 -> 73.217
2024-12-02-05:12:30-root-INFO: Regularization Change: 0.000 -> 2.097
2024-12-02-05:12:30-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-05:12:30-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-05:12:31-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-05:12:31-root-INFO: grad norm: 2.164 2.132 0.372
2024-12-02-05:12:32-root-INFO: grad norm: 2.341 2.318 0.332
2024-12-02-05:12:33-root-INFO: grad norm: 3.311 3.289 0.385
2024-12-02-05:12:33-root-INFO: Loss too large (72.057->72.214)! Learning rate decreased to 0.16529.
2024-12-02-05:12:34-root-INFO: grad norm: 4.216 4.193 0.434
2024-12-02-05:12:35-root-INFO: Loss too large (71.824->71.824)! Learning rate decreased to 0.13223.
2024-12-02-05:12:36-root-INFO: grad norm: 3.732 3.702 0.470
2024-12-02-05:12:36-root-INFO: Loss Change: 73.182 -> 70.876
2024-12-02-05:12:36-root-INFO: Regularization Change: 0.000 -> 2.211
2024-12-02-05:12:36-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-05:12:36-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-05:12:37-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-05:12:37-root-INFO: grad norm: 4.893 4.819 0.853
2024-12-02-05:12:37-root-INFO: Loss too large (71.120->72.356)! Learning rate decreased to 0.16894.
2024-12-02-05:12:38-root-INFO: Loss too large (71.120->71.546)! Learning rate decreased to 0.13515.
2024-12-02-05:12:39-root-INFO: grad norm: 4.186 4.140 0.616
2024-12-02-05:12:40-root-INFO: grad norm: 3.140 3.102 0.484
2024-12-02-05:12:41-root-INFO: grad norm: 3.156 3.122 0.461
2024-12-02-05:12:42-root-INFO: grad norm: 3.554 3.525 0.455
2024-12-02-05:12:42-root-INFO: Loss Change: 71.120 -> 69.285
2024-12-02-05:12:42-root-INFO: Regularization Change: 0.000 -> 1.442
2024-12-02-05:12:42-root-INFO: Undo step: 60
2024-12-02-05:12:42-root-INFO: Undo step: 61
2024-12-02-05:12:42-root-INFO: Undo step: 62
2024-12-02-05:12:42-root-INFO: Undo step: 63
2024-12-02-05:12:42-root-INFO: Undo step: 64
2024-12-02-05:12:43-root-INFO: step: 65 lr_xt 0.18901677
2024-12-02-05:12:43-root-INFO: grad norm: 27.454 27.218 3.590
2024-12-02-05:12:44-root-INFO: grad norm: 13.817 13.668 2.026
2024-12-02-05:12:45-root-INFO: grad norm: 10.695 10.520 1.928
2024-12-02-05:12:46-root-INFO: grad norm: 10.393 10.228 1.844
2024-12-02-05:12:47-root-INFO: grad norm: 10.970 10.749 2.188
2024-12-02-05:12:48-root-INFO: Loss Change: 195.201 -> 93.985
2024-12-02-05:12:48-root-INFO: Regularization Change: 0.000 -> 70.473
2024-12-02-05:12:48-root-INFO: Learning rate of xt decay: 0.26936 -> 0.27259.
2024-12-02-05:12:48-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-05:12:48-root-INFO: step: 64 lr_xt 0.19331686
2024-12-02-05:12:48-root-INFO: grad norm: 12.907 12.713 2.231
2024-12-02-05:12:49-root-INFO: grad norm: 13.736 13.502 2.526
2024-12-02-05:12:50-root-INFO: grad norm: 14.728 14.520 2.469
2024-12-02-05:12:51-root-INFO: Loss too large (93.103->93.290)! Learning rate decreased to 0.15465.
2024-12-02-05:12:52-root-INFO: grad norm: 9.987 9.803 1.906
2024-12-02-05:12:53-root-INFO: grad norm: 7.543 7.433 1.281
2024-12-02-05:12:53-root-INFO: Loss Change: 94.847 -> 81.050
2024-12-02-05:12:53-root-INFO: Regularization Change: 0.000 -> 9.474
2024-12-02-05:12:53-root-INFO: Learning rate of xt decay: 0.27259 -> 0.27586.
2024-12-02-05:12:53-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-05:12:54-root-INFO: step: 63 lr_xt 0.19768311
2024-12-02-05:12:54-root-INFO: grad norm: 5.249 5.200 0.716
2024-12-02-05:12:55-root-INFO: grad norm: 6.722 6.659 0.914
2024-12-02-05:12:55-root-INFO: Loss too large (80.014->80.609)! Learning rate decreased to 0.15815.
2024-12-02-05:12:56-root-INFO: grad norm: 6.725 6.688 0.699
2024-12-02-05:12:57-root-INFO: grad norm: 4.933 4.894 0.620
2024-12-02-05:12:58-root-INFO: grad norm: 3.822 3.785 0.530
2024-12-02-05:12:59-root-INFO: Loss Change: 80.602 -> 75.537
2024-12-02-05:12:59-root-INFO: Regularization Change: 0.000 -> 4.057
2024-12-02-05:12:59-root-INFO: Learning rate of xt decay: 0.27586 -> 0.27918.
2024-12-02-05:12:59-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-05:12:59-root-INFO: step: 62 lr_xt 0.20211560
2024-12-02-05:12:59-root-INFO: grad norm: 5.214 5.115 1.010
2024-12-02-05:13:00-root-INFO: Loss too large (75.679->76.096)! Learning rate decreased to 0.16169.
2024-12-02-05:13:01-root-INFO: grad norm: 5.177 5.111 0.825
2024-12-02-05:13:02-root-INFO: grad norm: 6.512 6.451 0.891
2024-12-02-05:13:02-root-INFO: Loss too large (74.478->74.996)! Learning rate decreased to 0.12935.
2024-12-02-05:13:03-root-INFO: grad norm: 4.566 4.515 0.684
2024-12-02-05:13:04-root-INFO: grad norm: 2.872 2.839 0.438
2024-12-02-05:13:05-root-INFO: Loss Change: 75.679 -> 72.148
2024-12-02-05:13:05-root-INFO: Regularization Change: 0.000 -> 2.183
2024-12-02-05:13:05-root-INFO: Learning rate of xt decay: 0.27918 -> 0.28253.
2024-12-02-05:13:05-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-05:13:05-root-INFO: step: 61 lr_xt 0.20661437
2024-12-02-05:13:05-root-INFO: grad norm: 2.449 2.415 0.404
2024-12-02-05:13:06-root-INFO: grad norm: 3.163 3.139 0.394
2024-12-02-05:13:07-root-INFO: Loss too large (71.510->71.637)! Learning rate decreased to 0.16529.
2024-12-02-05:13:08-root-INFO: grad norm: 4.720 4.699 0.447
2024-12-02-05:13:08-root-INFO: Loss too large (71.225->71.419)! Learning rate decreased to 0.13223.
2024-12-02-05:13:09-root-INFO: grad norm: 3.601 3.577 0.416
2024-12-02-05:13:10-root-INFO: grad norm: 2.208 2.186 0.314
2024-12-02-05:13:11-root-INFO: Loss Change: 72.099 -> 69.605
2024-12-02-05:13:11-root-INFO: Regularization Change: 0.000 -> 1.912
2024-12-02-05:13:11-root-INFO: Learning rate of xt decay: 0.28253 -> 0.28592.
2024-12-02-05:13:11-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-05:13:11-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-05:13:11-root-INFO: grad norm: 2.588 2.534 0.526
2024-12-02-05:13:12-root-INFO: grad norm: 3.234 3.202 0.455
2024-12-02-05:13:13-root-INFO: grad norm: 5.365 5.334 0.571
2024-12-02-05:13:14-root-INFO: Loss too large (69.184->70.493)! Learning rate decreased to 0.16894.
2024-12-02-05:13:14-root-INFO: Loss too large (69.184->69.347)! Learning rate decreased to 0.13515.
2024-12-02-05:13:15-root-INFO: grad norm: 4.386 4.368 0.396
2024-12-02-05:13:16-root-INFO: grad norm: 3.374 3.356 0.354
2024-12-02-05:13:17-root-INFO: Loss Change: 69.646 -> 67.453
2024-12-02-05:13:17-root-INFO: Regularization Change: 0.000 -> 1.904
2024-12-02-05:13:17-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-05:13:17-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-05:13:17-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-05:13:17-root-INFO: grad norm: 4.054 4.001 0.649
2024-12-02-05:13:18-root-INFO: Loss too large (67.530->68.129)! Learning rate decreased to 0.17265.
2024-12-02-05:13:18-root-INFO: Loss too large (67.530->67.713)! Learning rate decreased to 0.13812.
2024-12-02-05:13:19-root-INFO: grad norm: 3.421 3.396 0.411
2024-12-02-05:13:20-root-INFO: grad norm: 2.474 2.450 0.341
2024-12-02-05:13:21-root-INFO: grad norm: 2.687 2.666 0.337
2024-12-02-05:13:22-root-INFO: grad norm: 3.500 3.481 0.365
2024-12-02-05:13:22-root-INFO: Loss too large (66.040->66.045)! Learning rate decreased to 0.11050.
2024-12-02-05:13:23-root-INFO: Loss Change: 67.530 -> 65.862
2024-12-02-05:13:23-root-INFO: Regularization Change: 0.000 -> 1.210
2024-12-02-05:13:23-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-05:13:23-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-05:13:23-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-05:13:24-root-INFO: grad norm: 3.025 2.998 0.401
2024-12-02-05:13:24-root-INFO: Loss too large (65.691->66.028)! Learning rate decreased to 0.17641.
2024-12-02-05:13:25-root-INFO: grad norm: 5.072 5.057 0.389
2024-12-02-05:13:25-root-INFO: Loss too large (65.481->66.049)! Learning rate decreased to 0.14113.
2024-12-02-05:13:26-root-INFO: Loss too large (65.481->65.630)! Learning rate decreased to 0.11290.
2024-12-02-05:13:27-root-INFO: grad norm: 3.272 3.250 0.373
2024-12-02-05:13:28-root-INFO: grad norm: 1.667 1.648 0.255
2024-12-02-05:13:29-root-INFO: grad norm: 1.509 1.489 0.240
2024-12-02-05:13:29-root-INFO: Loss Change: 65.691 -> 63.982
2024-12-02-05:13:29-root-INFO: Regularization Change: 0.000 -> 0.933
2024-12-02-05:13:29-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-05:13:29-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-05:13:30-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-05:13:30-root-INFO: grad norm: 2.396 2.341 0.511
2024-12-02-05:13:31-root-INFO: grad norm: 3.544 3.495 0.586
2024-12-02-05:13:31-root-INFO: Loss too large (63.625->64.851)! Learning rate decreased to 0.18022.
2024-12-02-05:13:32-root-INFO: Loss too large (63.625->63.842)! Learning rate decreased to 0.14417.
2024-12-02-05:13:32-root-INFO: grad norm: 4.444 4.416 0.494
2024-12-02-05:13:33-root-INFO: Loss too large (63.327->63.518)! Learning rate decreased to 0.11534.
2024-12-02-05:13:34-root-INFO: grad norm: 3.391 3.365 0.413
2024-12-02-05:13:35-root-INFO: grad norm: 2.033 2.015 0.273
2024-12-02-05:13:35-root-INFO: Loss Change: 63.851 -> 62.276
2024-12-02-05:13:35-root-INFO: Regularization Change: 0.000 -> 1.109
2024-12-02-05:13:35-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-05:13:35-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-05:13:36-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-05:13:36-root-INFO: grad norm: 2.141 2.098 0.424
2024-12-02-05:13:37-root-INFO: grad norm: 3.402 3.387 0.323
2024-12-02-05:13:37-root-INFO: Loss too large (61.968->62.556)! Learning rate decreased to 0.18408.
2024-12-02-05:13:38-root-INFO: Loss too large (61.968->62.185)! Learning rate decreased to 0.14727.
2024-12-02-05:13:39-root-INFO: grad norm: 3.268 3.249 0.345
2024-12-02-05:13:40-root-INFO: grad norm: 3.287 3.271 0.329
2024-12-02-05:13:40-root-INFO: Loss too large (61.369->61.388)! Learning rate decreased to 0.11781.
2024-12-02-05:13:41-root-INFO: grad norm: 2.822 2.803 0.327
2024-12-02-05:13:42-root-INFO: Loss Change: 62.300 -> 60.795
2024-12-02-05:13:42-root-INFO: Regularization Change: 0.000 -> 1.164
2024-12-02-05:13:42-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-05:13:42-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-05:13:42-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-05:13:42-root-INFO: grad norm: 3.461 3.403 0.630
2024-12-02-05:13:43-root-INFO: Loss too large (60.912->61.619)! Learning rate decreased to 0.18800.
2024-12-02-05:13:43-root-INFO: Loss too large (60.912->61.119)! Learning rate decreased to 0.15040.
2024-12-02-05:13:44-root-INFO: grad norm: 3.445 3.417 0.437
2024-12-02-05:13:45-root-INFO: grad norm: 3.893 3.869 0.433
2024-12-02-05:13:45-root-INFO: Loss too large (60.365->60.492)! Learning rate decreased to 0.12032.
2024-12-02-05:13:46-root-INFO: grad norm: 3.152 3.130 0.370
2024-12-02-05:13:47-root-INFO: grad norm: 2.218 2.201 0.271
2024-12-02-05:13:48-root-INFO: Loss Change: 60.912 -> 59.474
2024-12-02-05:13:48-root-INFO: Regularization Change: 0.000 -> 0.898
2024-12-02-05:13:48-root-INFO: Undo step: 55
2024-12-02-05:13:48-root-INFO: Undo step: 56
2024-12-02-05:13:48-root-INFO: Undo step: 57
2024-12-02-05:13:48-root-INFO: Undo step: 58
2024-12-02-05:13:48-root-INFO: Undo step: 59
2024-12-02-05:13:48-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-05:13:49-root-INFO: grad norm: 24.137 23.933 3.127
2024-12-02-05:13:50-root-INFO: grad norm: 17.347 17.227 2.042
2024-12-02-05:13:51-root-INFO: grad norm: 11.841 11.703 1.802
2024-12-02-05:13:52-root-INFO: grad norm: 10.485 10.371 1.548
2024-12-02-05:13:52-root-INFO: grad norm: 10.410 10.283 1.618
2024-12-02-05:13:53-root-INFO: Loss Change: 174.067 -> 84.952
2024-12-02-05:13:53-root-INFO: Regularization Change: 0.000 -> 74.175
2024-12-02-05:13:53-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-05:13:53-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-05:13:54-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-05:13:54-root-INFO: grad norm: 11.874 11.690 2.081
2024-12-02-05:13:55-root-INFO: grad norm: 11.818 11.615 2.183
2024-12-02-05:13:56-root-INFO: grad norm: 11.842 11.649 2.129
2024-12-02-05:13:57-root-INFO: grad norm: 11.967 11.732 2.360
2024-12-02-05:13:58-root-INFO: grad norm: 12.660 12.437 2.364
2024-12-02-05:13:58-root-INFO: Loss too large (78.945->79.766)! Learning rate decreased to 0.17265.
2024-12-02-05:13:59-root-INFO: Loss Change: 85.953 -> 74.439
2024-12-02-05:13:59-root-INFO: Regularization Change: 0.000 -> 12.816
2024-12-02-05:13:59-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-05:13:59-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-05:13:59-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-05:13:59-root-INFO: grad norm: 7.901 7.758 1.495
2024-12-02-05:14:00-root-INFO: grad norm: 9.354 9.198 1.702
2024-12-02-05:14:01-root-INFO: Loss too large (72.453->74.192)! Learning rate decreased to 0.17641.
2024-12-02-05:14:02-root-INFO: grad norm: 7.069 6.939 1.349
2024-12-02-05:14:03-root-INFO: grad norm: 4.810 4.726 0.892
2024-12-02-05:14:04-root-INFO: grad norm: 4.067 4.001 0.730
2024-12-02-05:14:04-root-INFO: Loss Change: 73.446 -> 66.520
2024-12-02-05:14:04-root-INFO: Regularization Change: 0.000 -> 4.859
2024-12-02-05:14:04-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-05:14:04-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-05:14:05-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-05:14:05-root-INFO: grad norm: 5.304 5.212 0.982
2024-12-02-05:14:06-root-INFO: Loss too large (66.709->67.393)! Learning rate decreased to 0.18022.
2024-12-02-05:14:06-root-INFO: grad norm: 4.993 4.923 0.835
2024-12-02-05:14:07-root-INFO: grad norm: 5.043 4.989 0.740
2024-12-02-05:14:08-root-INFO: Loss too large (65.322->65.335)! Learning rate decreased to 0.14417.
2024-12-02-05:14:09-root-INFO: grad norm: 3.849 3.808 0.558
2024-12-02-05:14:10-root-INFO: grad norm: 2.752 2.725 0.382
2024-12-02-05:14:11-root-INFO: Loss Change: 66.709 -> 63.453
2024-12-02-05:14:11-root-INFO: Regularization Change: 0.000 -> 2.215
2024-12-02-05:14:11-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-05:14:11-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-05:14:11-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-05:14:11-root-INFO: grad norm: 2.578 2.546 0.401
2024-12-02-05:14:12-root-INFO: grad norm: 4.195 4.179 0.368
2024-12-02-05:14:13-root-INFO: Loss too large (63.035->63.692)! Learning rate decreased to 0.18408.
2024-12-02-05:14:13-root-INFO: Loss too large (63.035->63.160)! Learning rate decreased to 0.14727.
2024-12-02-05:14:14-root-INFO: grad norm: 3.512 3.487 0.417
2024-12-02-05:14:15-root-INFO: grad norm: 2.684 2.662 0.340
2024-12-02-05:14:16-root-INFO: grad norm: 2.659 2.636 0.350
2024-12-02-05:14:17-root-INFO: Loss Change: 63.398 -> 61.303
2024-12-02-05:14:17-root-INFO: Regularization Change: 0.000 -> 1.789
2024-12-02-05:14:17-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-05:14:17-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-05:14:17-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-05:14:17-root-INFO: grad norm: 3.827 3.766 0.677
2024-12-02-05:14:18-root-INFO: Loss too large (61.422->62.061)! Learning rate decreased to 0.18800.
2024-12-02-05:14:18-root-INFO: Loss too large (61.422->61.492)! Learning rate decreased to 0.15040.
2024-12-02-05:14:19-root-INFO: grad norm: 3.465 3.435 0.452
2024-12-02-05:14:20-root-INFO: grad norm: 3.243 3.218 0.395
2024-12-02-05:14:21-root-INFO: grad norm: 3.215 3.190 0.400
2024-12-02-05:14:22-root-INFO: grad norm: 3.225 3.204 0.369
2024-12-02-05:14:23-root-INFO: Loss Change: 61.422 -> 59.764
2024-12-02-05:14:23-root-INFO: Regularization Change: 0.000 -> 1.469
2024-12-02-05:14:23-root-INFO: Undo step: 55
2024-12-02-05:14:23-root-INFO: Undo step: 56
2024-12-02-05:14:23-root-INFO: Undo step: 57
2024-12-02-05:14:23-root-INFO: Undo step: 58
2024-12-02-05:14:23-root-INFO: Undo step: 59
2024-12-02-05:14:23-root-INFO: step: 60 lr_xt 0.21117946
2024-12-02-05:14:23-root-INFO: grad norm: 23.797 23.529 3.566
2024-12-02-05:14:24-root-INFO: grad norm: 13.960 13.765 2.321
2024-12-02-05:14:25-root-INFO: grad norm: 10.377 10.079 2.468
2024-12-02-05:14:26-root-INFO: grad norm: 8.475 8.346 1.475
2024-12-02-05:14:27-root-INFO: grad norm: 8.626 8.481 1.576
2024-12-02-05:14:28-root-INFO: Loss Change: 178.764 -> 81.987
2024-12-02-05:14:28-root-INFO: Regularization Change: 0.000 -> 76.387
2024-12-02-05:14:28-root-INFO: Learning rate of xt decay: 0.28592 -> 0.28935.
2024-12-02-05:14:28-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-05:14:28-root-INFO: step: 59 lr_xt 0.21581084
2024-12-02-05:14:28-root-INFO: grad norm: 10.594 10.426 1.880
2024-12-02-05:14:29-root-INFO: Loss too large (82.641->82.648)! Learning rate decreased to 0.17265.
2024-12-02-05:14:30-root-INFO: grad norm: 8.069 7.949 1.388
2024-12-02-05:14:31-root-INFO: grad norm: 6.042 5.972 0.917
2024-12-02-05:14:32-root-INFO: grad norm: 5.446 5.402 0.686
2024-12-02-05:14:33-root-INFO: grad norm: 4.883 4.845 0.614
2024-12-02-05:14:33-root-INFO: Loss Change: 82.641 -> 71.250
2024-12-02-05:14:33-root-INFO: Regularization Change: 0.000 -> 7.798
2024-12-02-05:14:33-root-INFO: Learning rate of xt decay: 0.28935 -> 0.29282.
2024-12-02-05:14:33-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-05:14:34-root-INFO: step: 58 lr_xt 0.22050848
2024-12-02-05:14:34-root-INFO: grad norm: 4.473 4.433 0.592
2024-12-02-05:14:35-root-INFO: grad norm: 4.976 4.941 0.594
2024-12-02-05:14:36-root-INFO: grad norm: 5.558 5.528 0.572
2024-12-02-05:14:36-root-INFO: Loss too large (69.368->69.461)! Learning rate decreased to 0.17641.
2024-12-02-05:14:37-root-INFO: grad norm: 4.953 4.923 0.548
2024-12-02-05:14:38-root-INFO: grad norm: 4.358 4.332 0.475
2024-12-02-05:14:39-root-INFO: Loss Change: 70.987 -> 66.564
2024-12-02-05:14:39-root-INFO: Regularization Change: 0.000 -> 4.801
2024-12-02-05:14:39-root-INFO: Learning rate of xt decay: 0.29282 -> 0.29633.
2024-12-02-05:14:39-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-05:14:40-root-INFO: step: 57 lr_xt 0.22527231
2024-12-02-05:14:40-root-INFO: grad norm: 4.811 4.769 0.633
2024-12-02-05:14:40-root-INFO: Loss too large (66.378->66.691)! Learning rate decreased to 0.18022.
2024-12-02-05:14:41-root-INFO: grad norm: 4.570 4.537 0.550
2024-12-02-05:14:42-root-INFO: grad norm: 4.234 4.206 0.485
2024-12-02-05:14:43-root-INFO: grad norm: 4.310 4.285 0.460
2024-12-02-05:14:44-root-INFO: grad norm: 4.270 4.247 0.448
2024-12-02-05:14:45-root-INFO: Loss Change: 66.378 -> 63.392
2024-12-02-05:14:45-root-INFO: Regularization Change: 0.000 -> 3.197
2024-12-02-05:14:45-root-INFO: Learning rate of xt decay: 0.29633 -> 0.29989.
2024-12-02-05:14:45-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-05:14:45-root-INFO: step: 56 lr_xt 0.23010221
2024-12-02-05:14:45-root-INFO: grad norm: 4.666 4.631 0.574
2024-12-02-05:14:46-root-INFO: Loss too large (63.392->63.770)! Learning rate decreased to 0.18408.
2024-12-02-05:14:47-root-INFO: grad norm: 4.340 4.315 0.464
2024-12-02-05:14:48-root-INFO: grad norm: 3.890 3.868 0.412
2024-12-02-05:14:49-root-INFO: grad norm: 3.975 3.954 0.404
2024-12-02-05:14:50-root-INFO: grad norm: 4.241 4.222 0.408
2024-12-02-05:14:50-root-INFO: Loss Change: 63.392 -> 60.980
2024-12-02-05:14:50-root-INFO: Regularization Change: 0.000 -> 2.802
2024-12-02-05:14:50-root-INFO: Learning rate of xt decay: 0.29989 -> 0.30349.
2024-12-02-05:14:50-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-05:14:51-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-05:14:51-root-INFO: grad norm: 4.607 4.579 0.509
2024-12-02-05:14:51-root-INFO: Loss too large (60.925->61.350)! Learning rate decreased to 0.18800.
2024-12-02-05:14:52-root-INFO: grad norm: 4.407 4.384 0.443
2024-12-02-05:14:53-root-INFO: grad norm: 4.251 4.231 0.407
2024-12-02-05:14:54-root-INFO: grad norm: 4.276 4.257 0.402
2024-12-02-05:14:55-root-INFO: grad norm: 4.151 4.132 0.390
2024-12-02-05:14:56-root-INFO: Loss Change: 60.925 -> 58.640
2024-12-02-05:14:56-root-INFO: Regularization Change: 0.000 -> 2.526
2024-12-02-05:14:56-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-05:14:56-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-05:14:56-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-05:14:57-root-INFO: grad norm: 4.451 4.414 0.572
2024-12-02-05:14:57-root-INFO: Loss too large (58.533->59.053)! Learning rate decreased to 0.19197.
2024-12-02-05:14:58-root-INFO: grad norm: 4.185 4.161 0.446
2024-12-02-05:14:59-root-INFO: grad norm: 3.838 3.817 0.409
2024-12-02-05:15:00-root-INFO: grad norm: 3.887 3.867 0.391
2024-12-02-05:15:01-root-INFO: grad norm: 4.060 4.040 0.404
2024-12-02-05:15:02-root-INFO: Loss Change: 58.533 -> 56.655
2024-12-02-05:15:02-root-INFO: Regularization Change: 0.000 -> 2.382
2024-12-02-05:15:02-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-05:15:02-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-05:15:02-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-05:15:02-root-INFO: grad norm: 4.259 4.237 0.431
2024-12-02-05:15:03-root-INFO: Loss too large (56.632->57.087)! Learning rate decreased to 0.19599.
2024-12-02-05:15:04-root-INFO: grad norm: 4.128 4.108 0.408
2024-12-02-05:15:05-root-INFO: grad norm: 4.052 4.034 0.384
2024-12-02-05:15:06-root-INFO: grad norm: 4.071 4.051 0.401
2024-12-02-05:15:07-root-INFO: grad norm: 3.982 3.963 0.384
2024-12-02-05:15:07-root-INFO: Loss Change: 56.632 -> 54.809
2024-12-02-05:15:07-root-INFO: Regularization Change: 0.000 -> 2.232
2024-12-02-05:15:07-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-05:15:07-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-05:15:08-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-05:15:08-root-INFO: grad norm: 4.337 4.299 0.571
2024-12-02-05:15:08-root-INFO: Loss too large (54.811->55.416)! Learning rate decreased to 0.20006.
2024-12-02-05:15:09-root-INFO: grad norm: 4.104 4.074 0.497
2024-12-02-05:15:10-root-INFO: grad norm: 3.864 3.835 0.468
2024-12-02-05:15:11-root-INFO: grad norm: 3.901 3.872 0.480
2024-12-02-05:15:12-root-INFO: grad norm: 4.023 3.994 0.482
2024-12-02-05:15:13-root-INFO: Loss Change: 54.811 -> 53.233
2024-12-02-05:15:13-root-INFO: Regularization Change: 0.000 -> 2.223
2024-12-02-05:15:13-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-05:15:13-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-05:15:13-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-05:15:14-root-INFO: grad norm: 4.287 4.258 0.497
2024-12-02-05:15:14-root-INFO: Loss too large (53.154->53.704)! Learning rate decreased to 0.20419.
2024-12-02-05:15:15-root-INFO: grad norm: 4.177 4.152 0.456
2024-12-02-05:15:16-root-INFO: grad norm: 4.120 4.095 0.458
2024-12-02-05:15:17-root-INFO: grad norm: 4.140 4.113 0.469
2024-12-02-05:15:18-root-INFO: grad norm: 4.076 4.047 0.485
2024-12-02-05:15:19-root-INFO: Loss Change: 53.154 -> 51.513
2024-12-02-05:15:19-root-INFO: Regularization Change: 0.000 -> 2.187
2024-12-02-05:15:19-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-05:15:19-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-05:15:19-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-05:15:19-root-INFO: grad norm: 4.489 4.436 0.689
2024-12-02-05:15:20-root-INFO: Loss too large (51.323->52.114)! Learning rate decreased to 0.20837.
2024-12-02-05:15:21-root-INFO: grad norm: 4.230 4.183 0.625
2024-12-02-05:15:22-root-INFO: grad norm: 3.934 3.895 0.552
2024-12-02-05:15:23-root-INFO: grad norm: 3.948 3.905 0.582
2024-12-02-05:15:23-root-INFO: grad norm: 4.035 4.000 0.535
2024-12-02-05:15:24-root-INFO: Loss Change: 51.323 -> 49.831
2024-12-02-05:15:24-root-INFO: Regularization Change: 0.000 -> 2.236
2024-12-02-05:15:24-root-INFO: Undo step: 50
2024-12-02-05:15:24-root-INFO: Undo step: 51
2024-12-02-05:15:24-root-INFO: Undo step: 52
2024-12-02-05:15:24-root-INFO: Undo step: 53
2024-12-02-05:15:24-root-INFO: Undo step: 54
2024-12-02-05:15:24-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-05:15:25-root-INFO: grad norm: 22.063 21.898 2.687
2024-12-02-05:15:26-root-INFO: grad norm: 10.653 10.521 1.668
2024-12-02-05:15:27-root-INFO: grad norm: 7.999 7.836 1.608
2024-12-02-05:15:28-root-INFO: grad norm: 7.161 7.057 1.217
2024-12-02-05:15:29-root-INFO: grad norm: 7.724 7.609 1.327
2024-12-02-05:15:29-root-INFO: Loss Change: 154.532 -> 69.662
2024-12-02-05:15:29-root-INFO: Regularization Change: 0.000 -> 73.025
2024-12-02-05:15:29-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-05:15:29-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-05:15:30-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-05:15:30-root-INFO: grad norm: 7.421 7.341 1.088
2024-12-02-05:15:31-root-INFO: grad norm: 7.085 7.000 1.098
2024-12-02-05:15:32-root-INFO: grad norm: 6.557 6.480 0.999
2024-12-02-05:15:33-root-INFO: grad norm: 6.361 6.290 0.946
2024-12-02-05:15:34-root-INFO: grad norm: 6.233 6.162 0.934
2024-12-02-05:15:35-root-INFO: Loss Change: 69.405 -> 61.051
2024-12-02-05:15:35-root-INFO: Regularization Change: 0.000 -> 11.173
2024-12-02-05:15:35-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-05:15:35-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-05:15:35-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-05:15:35-root-INFO: grad norm: 8.002 7.913 1.190
2024-12-02-05:15:36-root-INFO: Loss too large (61.388->62.100)! Learning rate decreased to 0.19599.
2024-12-02-05:15:37-root-INFO: grad norm: 5.270 5.191 0.909
2024-12-02-05:15:38-root-INFO: grad norm: 3.542 3.506 0.502
2024-12-02-05:15:39-root-INFO: grad norm: 2.812 2.776 0.444
2024-12-02-05:15:40-root-INFO: grad norm: 2.488 2.458 0.381
2024-12-02-05:15:40-root-INFO: Loss Change: 61.388 -> 55.604
2024-12-02-05:15:40-root-INFO: Regularization Change: 0.000 -> 3.910
2024-12-02-05:15:40-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-05:15:40-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-05:15:41-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-05:15:41-root-INFO: grad norm: 2.274 2.247 0.352
2024-12-02-05:15:42-root-INFO: grad norm: 2.820 2.798 0.351
2024-12-02-05:15:43-root-INFO: grad norm: 3.313 3.284 0.443
2024-12-02-05:15:44-root-INFO: grad norm: 4.609 4.577 0.544
2024-12-02-05:15:44-root-INFO: Loss too large (54.167->55.029)! Learning rate decreased to 0.20006.
2024-12-02-05:15:45-root-INFO: Loss too large (54.167->54.194)! Learning rate decreased to 0.16005.
2024-12-02-05:15:46-root-INFO: grad norm: 3.425 3.392 0.472
2024-12-02-05:15:46-root-INFO: Loss Change: 55.403 -> 52.910
2024-12-02-05:15:46-root-INFO: Regularization Change: 0.000 -> 2.813
2024-12-02-05:15:46-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-05:15:46-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-05:15:47-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-05:15:47-root-INFO: grad norm: 3.576 3.510 0.685
2024-12-02-05:15:47-root-INFO: Loss too large (53.016->53.357)! Learning rate decreased to 0.20419.
2024-12-02-05:15:48-root-INFO: grad norm: 3.695 3.652 0.564
2024-12-02-05:15:49-root-INFO: grad norm: 4.241 4.195 0.621
2024-12-02-05:15:50-root-INFO: Loss too large (52.316->52.515)! Learning rate decreased to 0.16335.
2024-12-02-05:15:50-root-INFO: grad norm: 3.429 3.396 0.474
2024-12-02-05:15:51-root-INFO: grad norm: 2.519 2.492 0.367
2024-12-02-05:15:52-root-INFO: Loss Change: 53.016 -> 50.983
2024-12-02-05:15:52-root-INFO: Regularization Change: 0.000 -> 1.780
2024-12-02-05:15:52-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-05:15:52-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-05:15:52-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-05:15:53-root-INFO: grad norm: 2.196 2.174 0.313
2024-12-02-05:15:54-root-INFO: grad norm: 3.518 3.501 0.340
2024-12-02-05:15:54-root-INFO: Loss too large (50.316->51.021)! Learning rate decreased to 0.20837.
2024-12-02-05:15:54-root-INFO: Loss too large (50.316->50.459)! Learning rate decreased to 0.16669.
2024-12-02-05:15:55-root-INFO: grad norm: 3.140 3.119 0.368
2024-12-02-05:15:56-root-INFO: grad norm: 2.700 2.679 0.334
2024-12-02-05:15:57-root-INFO: grad norm: 2.592 2.572 0.321
2024-12-02-05:15:58-root-INFO: Loss Change: 50.578 -> 48.997
2024-12-02-05:15:58-root-INFO: Regularization Change: 0.000 -> 1.580
2024-12-02-05:15:58-root-INFO: Undo step: 50
2024-12-02-05:15:58-root-INFO: Undo step: 51
2024-12-02-05:15:58-root-INFO: Undo step: 52
2024-12-02-05:15:58-root-INFO: Undo step: 53
2024-12-02-05:15:58-root-INFO: Undo step: 54
2024-12-02-05:15:58-root-INFO: step: 55 lr_xt 0.23499803
2024-12-02-05:15:59-root-INFO: grad norm: 23.648 23.484 2.784
2024-12-02-05:16:00-root-INFO: grad norm: 14.391 14.221 2.202
2024-12-02-05:16:01-root-INFO: grad norm: 9.317 9.214 1.383
2024-12-02-05:16:02-root-INFO: grad norm: 7.510 7.420 1.158
2024-12-02-05:16:03-root-INFO: grad norm: 7.208 7.148 0.928
2024-12-02-05:16:03-root-INFO: Loss Change: 161.987 -> 68.288
2024-12-02-05:16:03-root-INFO: Regularization Change: 0.000 -> 79.874
2024-12-02-05:16:03-root-INFO: Learning rate of xt decay: 0.30349 -> 0.30713.
2024-12-02-05:16:03-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-05:16:04-root-INFO: step: 54 lr_xt 0.23995961
2024-12-02-05:16:04-root-INFO: grad norm: 7.620 7.522 1.220
2024-12-02-05:16:05-root-INFO: grad norm: 7.086 7.006 1.066
2024-12-02-05:16:06-root-INFO: grad norm: 6.886 6.788 1.158
2024-12-02-05:16:07-root-INFO: grad norm: 7.022 6.943 1.051
2024-12-02-05:16:08-root-INFO: grad norm: 7.237 7.145 1.147
2024-12-02-05:16:09-root-INFO: Loss Change: 68.431 -> 60.947
2024-12-02-05:16:09-root-INFO: Regularization Change: 0.000 -> 12.176
2024-12-02-05:16:09-root-INFO: Learning rate of xt decay: 0.30713 -> 0.31081.
2024-12-02-05:16:09-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-05:16:09-root-INFO: step: 53 lr_xt 0.24498673
2024-12-02-05:16:09-root-INFO: grad norm: 6.977 6.925 0.855
2024-12-02-05:16:10-root-INFO: grad norm: 6.432 6.369 0.900
2024-12-02-05:16:11-root-INFO: grad norm: 5.958 5.894 0.870
2024-12-02-05:16:12-root-INFO: grad norm: 6.146 6.061 1.016
2024-12-02-05:16:13-root-INFO: grad norm: 6.201 6.115 1.033
2024-12-02-05:16:14-root-INFO: Loss Change: 60.488 -> 55.707
2024-12-02-05:16:14-root-INFO: Regularization Change: 0.000 -> 7.102
2024-12-02-05:16:14-root-INFO: Learning rate of xt decay: 0.31081 -> 0.31454.
2024-12-02-05:16:14-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-05:16:14-root-INFO: step: 52 lr_xt 0.25007913
2024-12-02-05:16:15-root-INFO: grad norm: 7.019 6.887 1.351
2024-12-02-05:16:15-root-INFO: Loss too large (56.139->56.206)! Learning rate decreased to 0.20006.
2024-12-02-05:16:16-root-INFO: grad norm: 4.878 4.796 0.893
2024-12-02-05:16:17-root-INFO: grad norm: 3.681 3.618 0.674
2024-12-02-05:16:18-root-INFO: grad norm: 3.580 3.532 0.580
2024-12-02-05:16:19-root-INFO: grad norm: 4.323 4.279 0.617
2024-12-02-05:16:19-root-INFO: Loss too large (52.170->52.285)! Learning rate decreased to 0.16005.
2024-12-02-05:16:20-root-INFO: Loss Change: 56.139 -> 51.835
2024-12-02-05:16:20-root-INFO: Regularization Change: 0.000 -> 3.064
2024-12-02-05:16:20-root-INFO: Learning rate of xt decay: 0.31454 -> 0.31832.
2024-12-02-05:16:20-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-05:16:20-root-INFO: step: 51 lr_xt 0.25523653
2024-12-02-05:16:20-root-INFO: grad norm: 3.103 3.083 0.348
2024-12-02-05:16:21-root-INFO: grad norm: 5.068 5.042 0.510
2024-12-02-05:16:22-root-INFO: Loss too large (51.367->52.288)! Learning rate decreased to 0.20419.
2024-12-02-05:16:22-root-INFO: Loss too large (51.367->51.566)! Learning rate decreased to 0.16335.
2024-12-02-05:16:23-root-INFO: grad norm: 3.407 3.378 0.441
2024-12-02-05:16:24-root-INFO: grad norm: 1.850 1.827 0.294
2024-12-02-05:16:25-root-INFO: grad norm: 1.606 1.590 0.227
2024-12-02-05:16:26-root-INFO: Loss Change: 51.576 -> 49.310
2024-12-02-05:16:26-root-INFO: Regularization Change: 0.000 -> 1.769
2024-12-02-05:16:26-root-INFO: Learning rate of xt decay: 0.31832 -> 0.32214.
2024-12-02-05:16:26-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-05:16:26-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-05:16:26-root-INFO: grad norm: 2.328 2.272 0.506
2024-12-02-05:16:27-root-INFO: grad norm: 3.144 3.100 0.529
2024-12-02-05:16:28-root-INFO: Loss too large (48.845->49.237)! Learning rate decreased to 0.20837.
2024-12-02-05:16:29-root-INFO: grad norm: 4.095 4.054 0.572
2024-12-02-05:16:29-root-INFO: Loss too large (48.619->48.862)! Learning rate decreased to 0.16669.
2024-12-02-05:16:30-root-INFO: grad norm: 3.319 3.288 0.450
2024-12-02-05:16:31-root-INFO: grad norm: 2.370 2.345 0.342
2024-12-02-05:16:32-root-INFO: Loss Change: 49.077 -> 47.407
2024-12-02-05:16:32-root-INFO: Regularization Change: 0.000 -> 1.715
2024-12-02-05:16:32-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-05:16:32-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-05:16:32-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-05:16:32-root-INFO: grad norm: 2.085 2.055 0.356
2024-12-02-05:16:33-root-INFO: grad norm: 2.889 2.874 0.294
2024-12-02-05:16:34-root-INFO: Loss too large (46.949->47.254)! Learning rate decreased to 0.21260.
2024-12-02-05:16:35-root-INFO: grad norm: 3.219 3.199 0.357
2024-12-02-05:16:36-root-INFO: grad norm: 3.955 3.931 0.431
2024-12-02-05:16:36-root-INFO: Loss too large (46.582->46.823)! Learning rate decreased to 0.17008.
2024-12-02-05:16:37-root-INFO: grad norm: 3.196 3.171 0.398
2024-12-02-05:16:38-root-INFO: Loss Change: 47.283 -> 45.714
2024-12-02-05:16:38-root-INFO: Regularization Change: 0.000 -> 1.729
2024-12-02-05:16:38-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-05:16:38-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-05:16:38-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-05:16:38-root-INFO: grad norm: 2.965 2.913 0.550
2024-12-02-05:16:39-root-INFO: Loss too large (45.729->46.028)! Learning rate decreased to 0.21688.
2024-12-02-05:16:40-root-INFO: grad norm: 3.263 3.228 0.480
2024-12-02-05:16:41-root-INFO: grad norm: 4.059 4.024 0.528
2024-12-02-05:16:41-root-INFO: Loss too large (45.369->45.626)! Learning rate decreased to 0.17350.
2024-12-02-05:16:42-root-INFO: grad norm: 3.223 3.194 0.430
2024-12-02-05:16:43-root-INFO: grad norm: 2.271 2.250 0.311
2024-12-02-05:16:44-root-INFO: Loss Change: 45.729 -> 44.214
2024-12-02-05:16:44-root-INFO: Regularization Change: 0.000 -> 1.439
2024-12-02-05:16:44-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-05:16:44-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-05:16:44-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-05:16:44-root-INFO: grad norm: 2.036 2.008 0.339
2024-12-02-05:16:45-root-INFO: grad norm: 2.941 2.927 0.286
2024-12-02-05:16:46-root-INFO: Loss too large (43.856->44.271)! Learning rate decreased to 0.22121.
2024-12-02-05:16:47-root-INFO: grad norm: 3.261 3.243 0.346
2024-12-02-05:16:48-root-INFO: grad norm: 3.909 3.887 0.414
2024-12-02-05:16:48-root-INFO: Loss too large (43.564->43.812)! Learning rate decreased to 0.17697.
2024-12-02-05:16:49-root-INFO: grad norm: 3.150 3.127 0.378
2024-12-02-05:16:50-root-INFO: Loss Change: 44.093 -> 42.730
2024-12-02-05:16:50-root-INFO: Regularization Change: 0.000 -> 1.607
2024-12-02-05:16:50-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-05:16:50-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-05:16:50-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-05:16:50-root-INFO: grad norm: 3.135 3.081 0.579
2024-12-02-05:16:51-root-INFO: Loss too large (42.804->43.261)! Learning rate decreased to 0.22559.
2024-12-02-05:16:52-root-INFO: grad norm: 3.404 3.367 0.498
2024-12-02-05:16:53-root-INFO: grad norm: 4.058 4.022 0.536
2024-12-02-05:16:53-root-INFO: Loss too large (42.492->42.740)! Learning rate decreased to 0.18047.
2024-12-02-05:16:54-root-INFO: grad norm: 3.183 3.155 0.422
2024-12-02-05:16:55-root-INFO: grad norm: 2.283 2.262 0.305
2024-12-02-05:16:56-root-INFO: Loss Change: 42.804 -> 41.342
2024-12-02-05:16:56-root-INFO: Regularization Change: 0.000 -> 1.400
2024-12-02-05:16:56-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-05:16:56-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-05:16:56-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-05:16:56-root-INFO: grad norm: 1.983 1.960 0.303
2024-12-02-05:16:57-root-INFO: grad norm: 2.951 2.938 0.282
2024-12-02-05:16:58-root-INFO: Loss too large (40.954->41.465)! Learning rate decreased to 0.23002.
2024-12-02-05:16:58-root-INFO: Loss too large (40.954->40.973)! Learning rate decreased to 0.18402.
2024-12-02-05:16:59-root-INFO: grad norm: 2.549 2.534 0.276
2024-12-02-05:17:00-root-INFO: grad norm: 2.245 2.230 0.259
2024-12-02-05:17:01-root-INFO: grad norm: 2.109 2.094 0.246
2024-12-02-05:17:02-root-INFO: Loss Change: 41.127 -> 39.900
2024-12-02-05:17:02-root-INFO: Regularization Change: 0.000 -> 1.317
2024-12-02-05:17:02-root-INFO: Undo step: 45
2024-12-02-05:17:02-root-INFO: Undo step: 46
2024-12-02-05:17:02-root-INFO: Undo step: 47
2024-12-02-05:17:02-root-INFO: Undo step: 48
2024-12-02-05:17:02-root-INFO: Undo step: 49
2024-12-02-05:17:02-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-05:17:02-root-INFO: grad norm: 19.745 19.604 2.356
2024-12-02-05:17:03-root-INFO: grad norm: 12.070 11.958 1.640
2024-12-02-05:17:04-root-INFO: grad norm: 9.401 9.268 1.578
2024-12-02-05:17:05-root-INFO: grad norm: 8.655 8.546 1.368
2024-12-02-05:17:06-root-INFO: grad norm: 8.113 7.966 1.533
2024-12-02-05:17:07-root-INFO: Loss Change: 140.200 -> 60.893
2024-12-02-05:17:07-root-INFO: Regularization Change: 0.000 -> 78.700
2024-12-02-05:17:07-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-05:17:07-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-05:17:07-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-05:17:08-root-INFO: grad norm: 9.876 9.708 1.812
2024-12-02-05:17:08-root-INFO: Loss too large (61.913->62.332)! Learning rate decreased to 0.21260.
2024-12-02-05:17:09-root-INFO: grad norm: 7.225 7.091 1.384
2024-12-02-05:17:10-root-INFO: grad norm: 5.143 5.072 0.853
2024-12-02-05:17:11-root-INFO: grad norm: 4.508 4.443 0.762
2024-12-02-05:17:12-root-INFO: grad norm: 4.240 4.192 0.640
2024-12-02-05:17:13-root-INFO: Loss Change: 61.913 -> 51.463
2024-12-02-05:17:13-root-INFO: Regularization Change: 0.000 -> 8.881
2024-12-02-05:17:13-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-05:17:13-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-05:17:13-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-05:17:13-root-INFO: grad norm: 3.851 3.819 0.495
2024-12-02-05:17:14-root-INFO: grad norm: 5.656 5.616 0.666
2024-12-02-05:17:15-root-INFO: Loss too large (50.568->51.508)! Learning rate decreased to 0.21688.
2024-12-02-05:17:16-root-INFO: grad norm: 4.869 4.819 0.696
2024-12-02-05:17:17-root-INFO: grad norm: 3.815 3.778 0.535
2024-12-02-05:17:18-root-INFO: grad norm: 3.708 3.670 0.527
2024-12-02-05:17:18-root-INFO: Loss Change: 51.095 -> 47.191
2024-12-02-05:17:18-root-INFO: Regularization Change: 0.000 -> 4.832
2024-12-02-05:17:18-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-05:17:18-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-05:17:19-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-05:17:19-root-INFO: grad norm: 4.871 4.798 0.837
2024-12-02-05:17:19-root-INFO: Loss too large (47.474->48.313)! Learning rate decreased to 0.22121.
2024-12-02-05:17:20-root-INFO: grad norm: 4.627 4.572 0.715
2024-12-02-05:17:21-root-INFO: grad norm: 4.351 4.305 0.632
2024-12-02-05:17:22-root-INFO: grad norm: 4.260 4.214 0.625
2024-12-02-05:17:23-root-INFO: grad norm: 4.168 4.127 0.582
2024-12-02-05:17:24-root-INFO: Loss Change: 47.474 -> 44.929
2024-12-02-05:17:24-root-INFO: Regularization Change: 0.000 -> 3.707
2024-12-02-05:17:24-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-05:17:24-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-05:17:24-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-05:17:25-root-INFO: grad norm: 3.707 3.684 0.414
2024-12-02-05:17:25-root-INFO: Loss too large (44.531->44.559)! Learning rate decreased to 0.22559.
2024-12-02-05:17:26-root-INFO: grad norm: 3.685 3.660 0.434
2024-12-02-05:17:27-root-INFO: grad norm: 3.737 3.706 0.482
2024-12-02-05:17:28-root-INFO: grad norm: 3.829 3.798 0.480
2024-12-02-05:17:29-root-INFO: grad norm: 3.852 3.817 0.521
2024-12-02-05:17:30-root-INFO: Loss Change: 44.531 -> 42.332
2024-12-02-05:17:30-root-INFO: Regularization Change: 0.000 -> 2.952
2024-12-02-05:17:30-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-05:17:30-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-05:17:30-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-05:17:30-root-INFO: grad norm: 4.777 4.706 0.819
2024-12-02-05:17:31-root-INFO: Loss too large (42.522->43.589)! Learning rate decreased to 0.23002.
2024-12-02-05:17:32-root-INFO: grad norm: 4.484 4.427 0.708
2024-12-02-05:17:33-root-INFO: grad norm: 4.119 4.074 0.606
2024-12-02-05:17:34-root-INFO: grad norm: 4.014 3.970 0.597
2024-12-02-05:17:35-root-INFO: grad norm: 3.920 3.880 0.552
2024-12-02-05:17:35-root-INFO: Loss Change: 42.522 -> 40.602
2024-12-02-05:17:35-root-INFO: Regularization Change: 0.000 -> 2.933
2024-12-02-05:17:35-root-INFO: Undo step: 45
2024-12-02-05:17:35-root-INFO: Undo step: 46
2024-12-02-05:17:35-root-INFO: Undo step: 47
2024-12-02-05:17:35-root-INFO: Undo step: 48
2024-12-02-05:17:35-root-INFO: Undo step: 49
2024-12-02-05:17:36-root-INFO: step: 50 lr_xt 0.26045862
2024-12-02-05:17:36-root-INFO: grad norm: 19.422 19.264 2.467
2024-12-02-05:17:37-root-INFO: grad norm: 12.058 11.984 1.326
2024-12-02-05:17:38-root-INFO: grad norm: 7.891 7.815 1.089
2024-12-02-05:17:39-root-INFO: grad norm: 6.447 6.402 0.755
2024-12-02-05:17:40-root-INFO: grad norm: 5.191 5.147 0.676
2024-12-02-05:17:41-root-INFO: Loss Change: 139.351 -> 57.556
2024-12-02-05:17:41-root-INFO: Regularization Change: 0.000 -> 76.045
2024-12-02-05:17:41-root-INFO: Learning rate of xt decay: 0.32214 -> 0.32600.
2024-12-02-05:17:41-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-05:17:41-root-INFO: step: 49 lr_xt 0.26574501
2024-12-02-05:17:41-root-INFO: grad norm: 5.328 5.265 0.821
2024-12-02-05:17:42-root-INFO: grad norm: 5.151 5.080 0.849
2024-12-02-05:17:43-root-INFO: grad norm: 5.697 5.630 0.866
2024-12-02-05:17:44-root-INFO: grad norm: 6.289 6.198 1.064
2024-12-02-05:17:45-root-INFO: grad norm: 7.411 7.317 1.173
2024-12-02-05:17:46-root-INFO: Loss too large (52.854->53.650)! Learning rate decreased to 0.21260.
2024-12-02-05:17:46-root-INFO: Loss Change: 57.759 -> 51.674
2024-12-02-05:17:46-root-INFO: Regularization Change: 0.000 -> 10.661
2024-12-02-05:17:46-root-INFO: Learning rate of xt decay: 0.32600 -> 0.32992.
2024-12-02-05:17:46-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-05:17:47-root-INFO: step: 48 lr_xt 0.27109532
2024-12-02-05:17:47-root-INFO: grad norm: 4.999 4.937 0.784
2024-12-02-05:17:48-root-INFO: grad norm: 5.344 5.278 0.838
2024-12-02-05:17:48-root-INFO: Loss too large (49.491->49.859)! Learning rate decreased to 0.21688.
2024-12-02-05:17:49-root-INFO: grad norm: 4.554 4.492 0.753
2024-12-02-05:17:50-root-INFO: grad norm: 3.948 3.906 0.574
2024-12-02-05:17:51-root-INFO: grad norm: 3.753 3.707 0.582
2024-12-02-05:17:52-root-INFO: Loss Change: 51.099 -> 46.147
2024-12-02-05:17:52-root-INFO: Regularization Change: 0.000 -> 5.039
2024-12-02-05:17:52-root-INFO: Learning rate of xt decay: 0.32992 -> 0.33388.
2024-12-02-05:17:52-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-05:17:52-root-INFO: step: 47 lr_xt 0.27650911
2024-12-02-05:17:53-root-INFO: grad norm: 4.677 4.596 0.863
2024-12-02-05:17:53-root-INFO: Loss too large (46.442->46.955)! Learning rate decreased to 0.22121.
2024-12-02-05:17:54-root-INFO: grad norm: 4.269 4.208 0.721
2024-12-02-05:17:55-root-INFO: grad norm: 3.925 3.878 0.603
2024-12-02-05:17:56-root-INFO: grad norm: 3.863 3.817 0.594
2024-12-02-05:17:57-root-INFO: grad norm: 3.846 3.806 0.553
2024-12-02-05:17:57-root-INFO: Loss Change: 46.442 -> 43.829
2024-12-02-05:17:57-root-INFO: Regularization Change: 0.000 -> 3.495
2024-12-02-05:17:57-root-INFO: Learning rate of xt decay: 0.33388 -> 0.33788.
2024-12-02-05:17:57-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-05:17:58-root-INFO: step: 46 lr_xt 0.28198590
2024-12-02-05:17:58-root-INFO: grad norm: 3.377 3.355 0.386
2024-12-02-05:17:59-root-INFO: grad norm: 4.869 4.837 0.558
2024-12-02-05:17:59-root-INFO: Loss too large (43.379->44.228)! Learning rate decreased to 0.22559.
2024-12-02-05:18:00-root-INFO: grad norm: 4.293 4.251 0.602
2024-12-02-05:18:01-root-INFO: grad norm: 3.607 3.572 0.501
2024-12-02-05:18:02-root-INFO: grad norm: 3.547 3.510 0.514
2024-12-02-05:18:03-root-INFO: Loss Change: 43.430 -> 41.293
2024-12-02-05:18:03-root-INFO: Regularization Change: 0.000 -> 3.000
2024-12-02-05:18:03-root-INFO: Learning rate of xt decay: 0.33788 -> 0.34194.
2024-12-02-05:18:03-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-05:18:04-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-05:18:04-root-INFO: grad norm: 4.492 4.418 0.808
2024-12-02-05:18:04-root-INFO: Loss too large (41.458->42.250)! Learning rate decreased to 0.23002.
2024-12-02-05:18:05-root-INFO: grad norm: 4.200 4.142 0.697
2024-12-02-05:18:06-root-INFO: grad norm: 3.922 3.879 0.583
2024-12-02-05:18:07-root-INFO: grad norm: 3.822 3.778 0.580
2024-12-02-05:18:08-root-INFO: grad norm: 3.748 3.710 0.533
2024-12-02-05:18:09-root-INFO: Loss Change: 41.458 -> 39.569
2024-12-02-05:18:09-root-INFO: Regularization Change: 0.000 -> 2.790
2024-12-02-05:18:09-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-05:18:09-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-05:18:09-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-05:18:10-root-INFO: grad norm: 3.309 3.290 0.363
2024-12-02-05:18:10-root-INFO: Loss too large (39.249->39.379)! Learning rate decreased to 0.23450.
2024-12-02-05:18:11-root-INFO: grad norm: 3.284 3.260 0.392
2024-12-02-05:18:12-root-INFO: grad norm: 3.347 3.319 0.434
2024-12-02-05:18:13-root-INFO: grad norm: 3.444 3.416 0.435
2024-12-02-05:18:14-root-INFO: grad norm: 3.500 3.468 0.475
2024-12-02-05:18:15-root-INFO: Loss Change: 39.249 -> 37.681
2024-12-02-05:18:15-root-INFO: Regularization Change: 0.000 -> 2.372
2024-12-02-05:18:15-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-05:18:15-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-05:18:15-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-05:18:15-root-INFO: grad norm: 4.447 4.380 0.766
2024-12-02-05:18:16-root-INFO: Loss too large (37.783->38.691)! Learning rate decreased to 0.23903.
2024-12-02-05:18:17-root-INFO: grad norm: 4.160 4.106 0.667
2024-12-02-05:18:17-root-INFO: grad norm: 3.829 3.788 0.554
2024-12-02-05:18:18-root-INFO: grad norm: 3.705 3.665 0.547
2024-12-02-05:18:19-root-INFO: grad norm: 3.607 3.571 0.503
2024-12-02-05:18:20-root-INFO: Loss Change: 37.783 -> 36.104
2024-12-02-05:18:20-root-INFO: Regularization Change: 0.000 -> 2.525
2024-12-02-05:18:20-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-05:18:20-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-05:18:20-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-05:18:21-root-INFO: grad norm: 3.239 3.219 0.358
2024-12-02-05:18:21-root-INFO: Loss too large (35.801->36.030)! Learning rate decreased to 0.24361.
2024-12-02-05:18:22-root-INFO: grad norm: 3.175 3.153 0.365
2024-12-02-05:18:23-root-INFO: grad norm: 3.174 3.148 0.407
2024-12-02-05:18:24-root-INFO: grad norm: 3.232 3.208 0.394
2024-12-02-05:18:25-root-INFO: grad norm: 3.280 3.251 0.436
2024-12-02-05:18:26-root-INFO: Loss Change: 35.801 -> 34.434
2024-12-02-05:18:26-root-INFO: Regularization Change: 0.000 -> 2.163
2024-12-02-05:18:26-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-05:18:26-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-05:18:26-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-05:18:26-root-INFO: grad norm: 4.482 4.406 0.820
2024-12-02-05:18:27-root-INFO: Loss too large (34.763->35.665)! Learning rate decreased to 0.24866.
2024-12-02-05:18:28-root-INFO: grad norm: 4.091 4.035 0.671
2024-12-02-05:18:29-root-INFO: grad norm: 3.684 3.646 0.525
2024-12-02-05:18:30-root-INFO: grad norm: 3.509 3.472 0.510
2024-12-02-05:18:31-root-INFO: grad norm: 3.394 3.363 0.458
2024-12-02-05:18:31-root-INFO: Loss Change: 34.763 -> 33.038
2024-12-02-05:18:31-root-INFO: Regularization Change: 0.000 -> 2.446
2024-12-02-05:18:31-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-05:18:31-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-05:18:32-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-05:18:32-root-INFO: grad norm: 2.923 2.909 0.282
2024-12-02-05:18:32-root-INFO: Loss too large (32.688->32.835)! Learning rate decreased to 0.25333.
2024-12-02-05:18:33-root-INFO: grad norm: 2.754 2.740 0.285
2024-12-02-05:18:34-root-INFO: grad norm: 2.685 2.666 0.322
2024-12-02-05:18:35-root-INFO: grad norm: 2.757 2.739 0.315
2024-12-02-05:18:36-root-INFO: grad norm: 2.840 2.817 0.358
2024-12-02-05:18:37-root-INFO: Loss Change: 32.688 -> 31.442
2024-12-02-05:18:37-root-INFO: Regularization Change: 0.000 -> 1.978
2024-12-02-05:18:37-root-INFO: Undo step: 40
2024-12-02-05:18:37-root-INFO: Undo step: 41
2024-12-02-05:18:37-root-INFO: Undo step: 42
2024-12-02-05:18:37-root-INFO: Undo step: 43
2024-12-02-05:18:37-root-INFO: Undo step: 44
2024-12-02-05:18:37-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-05:18:38-root-INFO: grad norm: 18.655 18.512 2.306
2024-12-02-05:18:39-root-INFO: grad norm: 9.421 9.338 1.249
2024-12-02-05:18:40-root-INFO: grad norm: 6.591 6.530 0.894
2024-12-02-05:18:41-root-INFO: grad norm: 5.869 5.794 0.936
2024-12-02-05:18:42-root-INFO: grad norm: 6.301 6.243 0.851
2024-12-02-05:18:42-root-INFO: Loss Change: 125.709 -> 50.629
2024-12-02-05:18:42-root-INFO: Regularization Change: 0.000 -> 77.979
2024-12-02-05:18:42-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-05:18:42-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-05:18:43-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-05:18:43-root-INFO: grad norm: 6.031 5.953 0.966
2024-12-02-05:18:44-root-INFO: grad norm: 5.552 5.503 0.732
2024-12-02-05:18:45-root-INFO: grad norm: 5.370 5.297 0.879
2024-12-02-05:18:46-root-INFO: grad norm: 5.381 5.324 0.783
2024-12-02-05:18:47-root-INFO: grad norm: 5.272 5.191 0.923
2024-12-02-05:18:48-root-INFO: Loss Change: 50.254 -> 42.103
2024-12-02-05:18:48-root-INFO: Regularization Change: 0.000 -> 12.762
2024-12-02-05:18:48-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-05:18:48-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-05:18:48-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-05:18:48-root-INFO: grad norm: 5.850 5.744 1.108
2024-12-02-05:18:49-root-INFO: grad norm: 5.730 5.608 1.178
2024-12-02-05:18:50-root-INFO: grad norm: 5.620 5.515 1.081
2024-12-02-05:18:51-root-INFO: grad norm: 5.651 5.526 1.180
2024-12-02-05:18:52-root-INFO: grad norm: 5.725 5.612 1.135
2024-12-02-05:18:52-root-INFO: Loss too large (39.227->39.321)! Learning rate decreased to 0.23903.
2024-12-02-05:18:53-root-INFO: Loss Change: 42.257 -> 37.855
2024-12-02-05:18:53-root-INFO: Regularization Change: 0.000 -> 6.825
2024-12-02-05:18:53-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-05:18:53-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-05:18:53-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-05:18:54-root-INFO: grad norm: 3.620 3.565 0.629
2024-12-02-05:18:55-root-INFO: grad norm: 4.062 4.002 0.700
2024-12-02-05:18:55-root-INFO: Loss too large (36.808->36.824)! Learning rate decreased to 0.24361.
2024-12-02-05:18:56-root-INFO: grad norm: 3.238 3.187 0.573
2024-12-02-05:18:57-root-INFO: grad norm: 2.587 2.551 0.429
2024-12-02-05:18:58-root-INFO: grad norm: 2.305 2.275 0.373
2024-12-02-05:18:59-root-INFO: Loss Change: 37.365 -> 34.414
2024-12-02-05:18:59-root-INFO: Regularization Change: 0.000 -> 3.356
2024-12-02-05:18:59-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-05:18:59-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-05:18:59-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-05:18:59-root-INFO: grad norm: 3.337 3.256 0.729
2024-12-02-05:19:00-root-INFO: grad norm: 3.952 3.875 0.775
2024-12-02-05:19:01-root-INFO: Loss too large (34.534->34.540)! Learning rate decreased to 0.24866.
2024-12-02-05:19:01-root-INFO: grad norm: 3.274 3.228 0.552
2024-12-02-05:19:02-root-INFO: grad norm: 2.928 2.889 0.476
2024-12-02-05:19:03-root-INFO: grad norm: 2.657 2.626 0.408
2024-12-02-05:19:04-root-INFO: Loss Change: 34.569 -> 32.493
2024-12-02-05:19:04-root-INFO: Regularization Change: 0.000 -> 2.830
2024-12-02-05:19:04-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-05:19:04-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-05:19:04-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-05:19:05-root-INFO: grad norm: 2.079 2.066 0.237
2024-12-02-05:19:06-root-INFO: grad norm: 2.605 2.591 0.272
2024-12-02-05:19:06-root-INFO: Loss too large (31.867->31.905)! Learning rate decreased to 0.25333.
2024-12-02-05:19:07-root-INFO: grad norm: 2.456 2.439 0.291
2024-12-02-05:19:08-root-INFO: grad norm: 2.325 2.308 0.286
2024-12-02-05:19:09-root-INFO: grad norm: 2.282 2.264 0.291
2024-12-02-05:19:10-root-INFO: Loss Change: 32.168 -> 30.650
2024-12-02-05:19:10-root-INFO: Regularization Change: 0.000 -> 2.271
2024-12-02-05:19:10-root-INFO: Undo step: 40
2024-12-02-05:19:10-root-INFO: Undo step: 41
2024-12-02-05:19:10-root-INFO: Undo step: 42
2024-12-02-05:19:10-root-INFO: Undo step: 43
2024-12-02-05:19:10-root-INFO: Undo step: 44
2024-12-02-05:19:10-root-INFO: step: 45 lr_xt 0.28752516
2024-12-02-05:19:10-root-INFO: grad norm: 19.284 19.173 2.071
2024-12-02-05:19:11-root-INFO: grad norm: 8.633 8.547 1.215
2024-12-02-05:19:12-root-INFO: grad norm: 6.536 6.478 0.874
2024-12-02-05:19:13-root-INFO: grad norm: 6.061 6.016 0.743
2024-12-02-05:19:14-root-INFO: grad norm: 5.967 5.921 0.742
2024-12-02-05:19:15-root-INFO: Loss Change: 128.889 -> 49.599
2024-12-02-05:19:15-root-INFO: Regularization Change: 0.000 -> 84.469
2024-12-02-05:19:15-root-INFO: Learning rate of xt decay: 0.34194 -> 0.34604.
2024-12-02-05:19:15-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-05:19:15-root-INFO: step: 44 lr_xt 0.29312635
2024-12-02-05:19:16-root-INFO: grad norm: 5.489 5.461 0.546
2024-12-02-05:19:17-root-INFO: grad norm: 5.286 5.253 0.590
2024-12-02-05:19:18-root-INFO: grad norm: 5.207 5.173 0.595
2024-12-02-05:19:19-root-INFO: grad norm: 5.220 5.169 0.725
2024-12-02-05:19:20-root-INFO: grad norm: 5.272 5.212 0.793
2024-12-02-05:19:20-root-INFO: Loss Change: 49.212 -> 41.857
2024-12-02-05:19:20-root-INFO: Regularization Change: 0.000 -> 12.519
2024-12-02-05:19:20-root-INFO: Learning rate of xt decay: 0.34604 -> 0.35019.
2024-12-02-05:19:20-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-05:19:21-root-INFO: step: 43 lr_xt 0.29878886
2024-12-02-05:19:21-root-INFO: grad norm: 6.315 6.190 1.253
2024-12-02-05:19:21-root-INFO: Loss too large (42.098->42.256)! Learning rate decreased to 0.23903.
2024-12-02-05:19:22-root-INFO: grad norm: 4.730 4.636 0.934
2024-12-02-05:19:23-root-INFO: grad norm: 3.514 3.445 0.692
2024-12-02-05:19:24-root-INFO: grad norm: 3.074 3.022 0.558
2024-12-02-05:19:25-root-INFO: grad norm: 2.880 2.833 0.523
2024-12-02-05:19:26-root-INFO: Loss Change: 42.098 -> 36.987
2024-12-02-05:19:26-root-INFO: Regularization Change: 0.000 -> 4.830
2024-12-02-05:19:26-root-INFO: Learning rate of xt decay: 0.35019 -> 0.35439.
2024-12-02-05:19:26-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-05:19:26-root-INFO: step: 42 lr_xt 0.30451205
2024-12-02-05:19:27-root-INFO: grad norm: 2.431 2.414 0.292
2024-12-02-05:19:28-root-INFO: grad norm: 3.398 3.368 0.455
2024-12-02-05:19:28-root-INFO: Loss too large (36.259->36.514)! Learning rate decreased to 0.24361.
2024-12-02-05:19:29-root-INFO: grad norm: 3.324 3.291 0.467
2024-12-02-05:19:30-root-INFO: grad norm: 3.164 3.129 0.469
2024-12-02-05:19:31-root-INFO: grad norm: 3.085 3.051 0.457
2024-12-02-05:19:31-root-INFO: Loss Change: 36.638 -> 34.481
2024-12-02-05:19:31-root-INFO: Regularization Change: 0.000 -> 3.311
2024-12-02-05:19:31-root-INFO: Learning rate of xt decay: 0.35439 -> 0.35865.
2024-12-02-05:19:31-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-05:19:32-root-INFO: step: 41 lr_xt 0.31082203
2024-12-02-05:19:32-root-INFO: grad norm: 4.263 4.171 0.881
2024-12-02-05:19:32-root-INFO: Loss too large (34.760->35.344)! Learning rate decreased to 0.24866.
2024-12-02-05:19:33-root-INFO: grad norm: 3.842 3.780 0.689
2024-12-02-05:19:34-root-INFO: grad norm: 3.395 3.346 0.575
2024-12-02-05:19:35-root-INFO: grad norm: 3.194 3.152 0.516
2024-12-02-05:19:36-root-INFO: grad norm: 3.061 3.023 0.481
2024-12-02-05:19:37-root-INFO: Loss Change: 34.760 -> 32.509
2024-12-02-05:19:37-root-INFO: Regularization Change: 0.000 -> 2.988
2024-12-02-05:19:37-root-INFO: Learning rate of xt decay: 0.35865 -> 0.36295.
2024-12-02-05:19:37-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-05:19:37-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-05:19:38-root-INFO: grad norm: 2.473 2.460 0.254
2024-12-02-05:19:39-root-INFO: grad norm: 3.560 3.540 0.377
2024-12-02-05:19:39-root-INFO: Loss too large (31.935->32.434)! Learning rate decreased to 0.25333.
2024-12-02-05:19:40-root-INFO: grad norm: 3.152 3.128 0.386
2024-12-02-05:19:41-root-INFO: grad norm: 2.720 2.696 0.355
2024-12-02-05:19:42-root-INFO: grad norm: 2.644 2.620 0.354
2024-12-02-05:19:43-root-INFO: Loss Change: 32.122 -> 30.494
2024-12-02-05:19:43-root-INFO: Regularization Change: 0.000 -> 2.481
2024-12-02-05:19:43-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-05:19:43-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-05:19:43-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-05:19:43-root-INFO: grad norm: 3.306 3.248 0.615
2024-12-02-05:19:44-root-INFO: Loss too large (30.529->30.868)! Learning rate decreased to 0.25805.
2024-12-02-05:19:44-root-INFO: grad norm: 3.126 3.085 0.504
2024-12-02-05:19:45-root-INFO: grad norm: 2.996 2.962 0.446
2024-12-02-05:19:46-root-INFO: grad norm: 2.918 2.887 0.423
2024-12-02-05:19:47-root-INFO: grad norm: 2.859 2.831 0.394
2024-12-02-05:19:48-root-INFO: Loss Change: 30.529 -> 29.060
2024-12-02-05:19:48-root-INFO: Regularization Change: 0.000 -> 2.312
2024-12-02-05:19:48-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-05:19:48-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-05:19:49-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-05:19:49-root-INFO: grad norm: 2.554 2.543 0.238
2024-12-02-05:19:49-root-INFO: Loss too large (28.673->28.693)! Learning rate decreased to 0.26281.
2024-12-02-05:19:50-root-INFO: grad norm: 2.436 2.424 0.245
2024-12-02-05:19:51-root-INFO: grad norm: 2.460 2.446 0.257
2024-12-02-05:19:52-root-INFO: grad norm: 2.512 2.496 0.278
2024-12-02-05:19:53-root-INFO: grad norm: 2.541 2.524 0.291
2024-12-02-05:19:54-root-INFO: Loss Change: 28.673 -> 27.432
2024-12-02-05:19:54-root-INFO: Regularization Change: 0.000 -> 2.015
2024-12-02-05:19:54-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-05:19:54-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-05:19:54-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-05:19:54-root-INFO: grad norm: 3.411 3.357 0.609
2024-12-02-05:19:55-root-INFO: Loss too large (27.515->28.003)! Learning rate decreased to 0.26762.
2024-12-02-05:19:56-root-INFO: grad norm: 3.254 3.217 0.490
2024-12-02-05:19:57-root-INFO: grad norm: 3.025 2.996 0.413
2024-12-02-05:19:58-root-INFO: grad norm: 2.823 2.796 0.389
2024-12-02-05:19:59-root-INFO: grad norm: 2.718 2.695 0.350
2024-12-02-05:19:59-root-INFO: Loss Change: 27.515 -> 26.122
2024-12-02-05:19:59-root-INFO: Regularization Change: 0.000 -> 2.126
2024-12-02-05:19:59-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-05:19:59-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-05:20:00-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-05:20:00-root-INFO: grad norm: 2.572 2.561 0.234
2024-12-02-05:20:00-root-INFO: Loss too large (25.862->25.996)! Learning rate decreased to 0.27247.
2024-12-02-05:20:01-root-INFO: grad norm: 2.376 2.365 0.232
2024-12-02-05:20:02-root-INFO: grad norm: 2.212 2.199 0.239
2024-12-02-05:20:03-root-INFO: grad norm: 2.257 2.243 0.245
2024-12-02-05:20:04-root-INFO: grad norm: 2.364 2.349 0.263
2024-12-02-05:20:05-root-INFO: Loss Change: 25.862 -> 24.756
2024-12-02-05:20:05-root-INFO: Regularization Change: 0.000 -> 1.840
2024-12-02-05:20:05-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-05:20:05-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-05:20:05-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-05:20:06-root-INFO: grad norm: 3.112 3.065 0.537
2024-12-02-05:20:06-root-INFO: Loss too large (24.848->25.254)! Learning rate decreased to 0.27737.
2024-12-02-05:20:07-root-INFO: grad norm: 2.979 2.946 0.442
2024-12-02-05:20:08-root-INFO: grad norm: 2.772 2.748 0.366
2024-12-02-05:20:09-root-INFO: grad norm: 2.582 2.559 0.349
2024-12-02-05:20:10-root-INFO: grad norm: 2.508 2.488 0.310
2024-12-02-05:20:11-root-INFO: Loss Change: 24.848 -> 23.619
2024-12-02-05:20:11-root-INFO: Regularization Change: 0.000 -> 1.941
2024-12-02-05:20:11-root-INFO: Undo step: 35
2024-12-02-05:20:11-root-INFO: Undo step: 36
2024-12-02-05:20:11-root-INFO: Undo step: 37
2024-12-02-05:20:11-root-INFO: Undo step: 38
2024-12-02-05:20:11-root-INFO: Undo step: 39
2024-12-02-05:20:11-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-05:20:12-root-INFO: grad norm: 16.150 16.058 1.718
2024-12-02-05:20:13-root-INFO: grad norm: 9.238 9.168 1.139
2024-12-02-05:20:13-root-INFO: grad norm: 7.035 6.916 1.287
2024-12-02-05:20:15-root-INFO: grad norm: 5.660 5.620 0.671
2024-12-02-05:20:15-root-INFO: grad norm: 5.346 5.297 0.724
2024-12-02-05:20:16-root-INFO: Loss Change: 111.545 -> 40.842
2024-12-02-05:20:16-root-INFO: Regularization Change: 0.000 -> 83.718
2024-12-02-05:20:16-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-05:20:16-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-05:20:16-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-05:20:17-root-INFO: grad norm: 6.068 5.997 0.922
2024-12-02-05:20:18-root-INFO: grad norm: 5.640 5.558 0.957
2024-12-02-05:20:19-root-INFO: grad norm: 5.292 5.212 0.915
2024-12-02-05:20:20-root-INFO: grad norm: 5.427 5.338 0.978
2024-12-02-05:20:21-root-INFO: grad norm: 6.157 6.069 1.036
2024-12-02-05:20:21-root-INFO: Loss Change: 40.961 -> 35.534
2024-12-02-05:20:21-root-INFO: Regularization Change: 0.000 -> 13.831
2024-12-02-05:20:21-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-05:20:21-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-05:20:22-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-05:20:22-root-INFO: grad norm: 5.326 5.260 0.838
2024-12-02-05:20:23-root-INFO: grad norm: 5.386 5.308 0.910
2024-12-02-05:20:24-root-INFO: grad norm: 5.445 5.359 0.966
2024-12-02-05:20:25-root-INFO: grad norm: 5.594 5.502 1.009
2024-12-02-05:20:25-root-INFO: Loss too large (32.079->32.104)! Learning rate decreased to 0.26281.
2024-12-02-05:20:26-root-INFO: grad norm: 4.053 3.988 0.722
2024-12-02-05:20:27-root-INFO: Loss Change: 34.674 -> 29.434
2024-12-02-05:20:27-root-INFO: Regularization Change: 0.000 -> 6.558
2024-12-02-05:20:27-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-05:20:27-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-05:20:27-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-05:20:28-root-INFO: grad norm: 3.956 3.866 0.840
2024-12-02-05:20:29-root-INFO: grad norm: 4.413 4.330 0.853
2024-12-02-05:20:30-root-INFO: grad norm: 5.118 5.038 0.902
2024-12-02-05:20:30-root-INFO: Loss too large (29.394->29.736)! Learning rate decreased to 0.26762.
2024-12-02-05:20:31-root-INFO: grad norm: 3.828 3.770 0.661
2024-12-02-05:20:32-root-INFO: grad norm: 2.930 2.893 0.466
2024-12-02-05:20:33-root-INFO: Loss Change: 29.642 -> 26.922
2024-12-02-05:20:33-root-INFO: Regularization Change: 0.000 -> 3.813
2024-12-02-05:20:33-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-05:20:33-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-05:20:33-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-05:20:33-root-INFO: grad norm: 2.201 2.190 0.212
2024-12-02-05:20:34-root-INFO: grad norm: 2.912 2.897 0.293
2024-12-02-05:20:34-root-INFO: Loss too large (26.297->26.494)! Learning rate decreased to 0.27247.
2024-12-02-05:20:35-root-INFO: grad norm: 2.586 2.570 0.283
2024-12-02-05:20:36-root-INFO: grad norm: 2.298 2.282 0.269
2024-12-02-05:20:37-root-INFO: grad norm: 2.240 2.225 0.265
2024-12-02-05:20:38-root-INFO: Loss Change: 26.590 -> 24.968
2024-12-02-05:20:38-root-INFO: Regularization Change: 0.000 -> 2.537
2024-12-02-05:20:38-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-05:20:38-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-05:20:39-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-05:20:39-root-INFO: grad norm: 2.916 2.864 0.550
2024-12-02-05:20:39-root-INFO: Loss too large (25.031->25.196)! Learning rate decreased to 0.27737.
2024-12-02-05:20:40-root-INFO: grad norm: 2.674 2.642 0.411
2024-12-02-05:20:41-root-INFO: grad norm: 2.506 2.483 0.343
2024-12-02-05:20:42-root-INFO: grad norm: 2.430 2.409 0.318
2024-12-02-05:20:43-root-INFO: grad norm: 2.532 2.513 0.305
2024-12-02-05:20:44-root-INFO: Loss Change: 25.031 -> 23.688
2024-12-02-05:20:44-root-INFO: Regularization Change: 0.000 -> 2.261
2024-12-02-05:20:44-root-INFO: Undo step: 35
2024-12-02-05:20:44-root-INFO: Undo step: 36
2024-12-02-05:20:44-root-INFO: Undo step: 37
2024-12-02-05:20:44-root-INFO: Undo step: 38
2024-12-02-05:20:44-root-INFO: Undo step: 39
2024-12-02-05:20:44-root-INFO: step: 40 lr_xt 0.31666177
2024-12-02-05:20:44-root-INFO: grad norm: 17.457 17.370 1.741
2024-12-02-05:20:45-root-INFO: grad norm: 9.187 9.121 1.092
2024-12-02-05:20:46-root-INFO: grad norm: 6.460 6.374 1.050
2024-12-02-05:20:47-root-INFO: grad norm: 4.896 4.838 0.753
2024-12-02-05:20:49-root-INFO: grad norm: 4.428 4.378 0.659
2024-12-02-05:20:49-root-INFO: Loss Change: 117.424 -> 40.169
2024-12-02-05:20:49-root-INFO: Regularization Change: 0.000 -> 86.785
2024-12-02-05:20:49-root-INFO: Learning rate of xt decay: 0.36295 -> 0.36731.
2024-12-02-05:20:49-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-05:20:50-root-INFO: step: 39 lr_xt 0.32255964
2024-12-02-05:20:50-root-INFO: grad norm: 5.468 5.385 0.951
2024-12-02-05:20:51-root-INFO: grad norm: 5.572 5.489 0.958
2024-12-02-05:20:52-root-INFO: grad norm: 5.725 5.639 0.987
2024-12-02-05:20:53-root-INFO: grad norm: 5.749 5.657 1.027
2024-12-02-05:20:54-root-INFO: grad norm: 5.844 5.748 1.056
2024-12-02-05:20:54-root-INFO: Loss Change: 40.269 -> 35.142
2024-12-02-05:20:54-root-INFO: Regularization Change: 0.000 -> 13.539
2024-12-02-05:20:54-root-INFO: Learning rate of xt decay: 0.36731 -> 0.37171.
2024-12-02-05:20:54-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-05:20:55-root-INFO: step: 38 lr_xt 0.32851483
2024-12-02-05:20:55-root-INFO: grad norm: 5.274 5.210 0.822
2024-12-02-05:20:56-root-INFO: grad norm: 5.272 5.194 0.903
2024-12-02-05:20:57-root-INFO: grad norm: 5.308 5.227 0.923
2024-12-02-05:20:58-root-INFO: grad norm: 5.460 5.372 0.972
2024-12-02-05:20:58-root-INFO: Loss too large (31.614->31.744)! Learning rate decreased to 0.26281.
2024-12-02-05:20:59-root-INFO: grad norm: 3.905 3.846 0.677
2024-12-02-05:21:00-root-INFO: Loss Change: 34.241 -> 29.068
2024-12-02-05:21:00-root-INFO: Regularization Change: 0.000 -> 6.336
2024-12-02-05:21:00-root-INFO: Learning rate of xt decay: 0.37171 -> 0.37617.
2024-12-02-05:21:00-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-05:21:00-root-INFO: step: 37 lr_xt 0.33452649
2024-12-02-05:21:01-root-INFO: grad norm: 3.768 3.684 0.790
2024-12-02-05:21:02-root-INFO: grad norm: 4.052 3.979 0.768
2024-12-02-05:21:03-root-INFO: grad norm: 4.753 4.680 0.827
2024-12-02-05:21:03-root-INFO: Loss too large (28.726->29.164)! Learning rate decreased to 0.26762.
2024-12-02-05:21:04-root-INFO: grad norm: 3.646 3.594 0.612
2024-12-02-05:21:05-root-INFO: grad norm: 2.668 2.634 0.423
2024-12-02-05:21:06-root-INFO: Loss Change: 29.256 -> 26.509
2024-12-02-05:21:06-root-INFO: Regularization Change: 0.000 -> 3.766
2024-12-02-05:21:06-root-INFO: Learning rate of xt decay: 0.37617 -> 0.38069.
2024-12-02-05:21:06-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-05:21:06-root-INFO: step: 36 lr_xt 0.34059371
2024-12-02-05:21:06-root-INFO: grad norm: 1.977 1.966 0.206
2024-12-02-05:21:07-root-INFO: grad norm: 2.527 2.513 0.266
2024-12-02-05:21:08-root-INFO: Loss too large (25.851->25.882)! Learning rate decreased to 0.27247.
2024-12-02-05:21:09-root-INFO: grad norm: 2.292 2.277 0.260
2024-12-02-05:21:10-root-INFO: grad norm: 2.061 2.045 0.256
2024-12-02-05:21:11-root-INFO: grad norm: 2.016 2.001 0.250
2024-12-02-05:21:11-root-INFO: Loss Change: 26.189 -> 24.559
2024-12-02-05:21:11-root-INFO: Regularization Change: 0.000 -> 2.513
2024-12-02-05:21:11-root-INFO: Learning rate of xt decay: 0.38069 -> 0.38526.
2024-12-02-05:21:11-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-05:21:12-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-05:21:12-root-INFO: grad norm: 2.773 2.724 0.519
2024-12-02-05:21:12-root-INFO: Loss too large (24.602->24.742)! Learning rate decreased to 0.27737.
2024-12-02-05:21:13-root-INFO: grad norm: 2.564 2.533 0.397
2024-12-02-05:21:14-root-INFO: grad norm: 2.475 2.452 0.338
2024-12-02-05:21:15-root-INFO: grad norm: 2.393 2.373 0.315
2024-12-02-05:21:16-root-INFO: grad norm: 2.302 2.283 0.295
2024-12-02-05:21:17-root-INFO: Loss Change: 24.602 -> 23.213
2024-12-02-05:21:17-root-INFO: Regularization Change: 0.000 -> 2.203
2024-12-02-05:21:17-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-05:21:17-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-05:21:17-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-05:21:18-root-INFO: grad norm: 2.038 2.028 0.208
2024-12-02-05:21:18-root-INFO: grad norm: 2.901 2.891 0.246
2024-12-02-05:21:19-root-INFO: Loss too large (22.780->23.157)! Learning rate decreased to 0.28231.
2024-12-02-05:21:20-root-INFO: grad norm: 2.593 2.579 0.276
2024-12-02-05:21:21-root-INFO: grad norm: 2.204 2.190 0.240
2024-12-02-05:21:22-root-INFO: grad norm: 2.100 2.085 0.251
2024-12-02-05:21:22-root-INFO: Loss Change: 22.912 -> 21.698
2024-12-02-05:21:22-root-INFO: Regularization Change: 0.000 -> 2.027
2024-12-02-05:21:22-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-05:21:22-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-05:21:23-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-05:21:23-root-INFO: grad norm: 2.781 2.736 0.502
2024-12-02-05:21:23-root-INFO: Loss too large (21.600->21.843)! Learning rate decreased to 0.28730.
2024-12-02-05:21:24-root-INFO: grad norm: 2.559 2.533 0.368
2024-12-02-05:21:25-root-INFO: grad norm: 2.416 2.396 0.314
2024-12-02-05:21:26-root-INFO: grad norm: 2.327 2.309 0.291
2024-12-02-05:21:27-root-INFO: grad norm: 2.294 2.278 0.274
2024-12-02-05:21:28-root-INFO: Loss Change: 21.600 -> 20.455
2024-12-02-05:21:28-root-INFO: Regularization Change: 0.000 -> 1.945
2024-12-02-05:21:28-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-05:21:28-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-05:21:28-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-05:21:29-root-INFO: grad norm: 2.016 2.005 0.215
2024-12-02-05:21:30-root-INFO: grad norm: 2.868 2.860 0.215
2024-12-02-05:21:30-root-INFO: Loss too large (20.079->20.478)! Learning rate decreased to 0.29232.
2024-12-02-05:21:31-root-INFO: grad norm: 2.416 2.405 0.230
2024-12-02-05:21:32-root-INFO: grad norm: 2.029 2.019 0.198
2024-12-02-05:21:33-root-INFO: grad norm: 1.906 1.894 0.214
2024-12-02-05:21:34-root-INFO: Loss Change: 20.202 -> 19.119
2024-12-02-05:21:34-root-INFO: Regularization Change: 0.000 -> 1.792
2024-12-02-05:21:34-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-05:21:34-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-05:21:34-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-05:21:34-root-INFO: grad norm: 2.522 2.484 0.433
2024-12-02-05:21:35-root-INFO: Loss too large (19.123->19.333)! Learning rate decreased to 0.29738.
2024-12-02-05:21:36-root-INFO: grad norm: 2.322 2.299 0.326
2024-12-02-05:21:37-root-INFO: grad norm: 2.136 2.120 0.268
2024-12-02-05:21:38-root-INFO: grad norm: 2.052 2.035 0.261
2024-12-02-05:21:38-root-INFO: grad norm: 2.090 2.076 0.244
2024-12-02-05:21:39-root-INFO: Loss Change: 19.123 -> 18.150
2024-12-02-05:21:39-root-INFO: Regularization Change: 0.000 -> 1.724
2024-12-02-05:21:39-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-05:21:39-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-05:21:39-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-05:21:40-root-INFO: grad norm: 1.840 1.830 0.190
2024-12-02-05:21:41-root-INFO: grad norm: 2.781 2.774 0.199
2024-12-02-05:21:41-root-INFO: Loss too large (17.783->18.208)! Learning rate decreased to 0.30249.
2024-12-02-05:21:41-root-INFO: Loss too large (17.783->17.784)! Learning rate decreased to 0.24199.
2024-12-02-05:21:42-root-INFO: grad norm: 1.710 1.701 0.175
2024-12-02-05:21:43-root-INFO: grad norm: 1.046 1.039 0.116
2024-12-02-05:21:44-root-INFO: grad norm: 0.858 0.852 0.108
2024-12-02-05:21:45-root-INFO: Loss Change: 17.858 -> 16.860
2024-12-02-05:21:45-root-INFO: Regularization Change: 0.000 -> 1.121
2024-12-02-05:21:45-root-INFO: Undo step: 30
2024-12-02-05:21:45-root-INFO: Undo step: 31
2024-12-02-05:21:45-root-INFO: Undo step: 32
2024-12-02-05:21:45-root-INFO: Undo step: 33
2024-12-02-05:21:45-root-INFO: Undo step: 34
2024-12-02-05:21:45-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-05:21:46-root-INFO: grad norm: 15.541 15.473 1.452
2024-12-02-05:21:47-root-INFO: grad norm: 6.985 6.931 0.868
2024-12-02-05:21:48-root-INFO: grad norm: 4.962 4.928 0.581
2024-12-02-05:21:49-root-INFO: grad norm: 4.350 4.323 0.481
2024-12-02-05:21:50-root-INFO: grad norm: 4.378 4.357 0.433
2024-12-02-05:21:50-root-INFO: Loss Change: 103.652 -> 33.556
2024-12-02-05:21:50-root-INFO: Regularization Change: 0.000 -> 87.986
2024-12-02-05:21:50-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-05:21:50-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-05:21:51-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-05:21:51-root-INFO: grad norm: 4.554 4.531 0.466
2024-12-02-05:21:52-root-INFO: grad norm: 4.401 4.379 0.439
2024-12-02-05:21:53-root-INFO: grad norm: 4.284 4.263 0.414
2024-12-02-05:21:54-root-INFO: grad norm: 4.340 4.320 0.418
2024-12-02-05:21:55-root-INFO: grad norm: 4.110 4.090 0.410
2024-12-02-05:21:56-root-INFO: Loss Change: 33.355 -> 26.820
2024-12-02-05:21:56-root-INFO: Regularization Change: 0.000 -> 12.728
2024-12-02-05:21:56-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-05:21:56-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-05:21:56-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-05:21:56-root-INFO: grad norm: 3.805 3.792 0.317
2024-12-02-05:21:57-root-INFO: grad norm: 3.760 3.747 0.312
2024-12-02-05:21:58-root-INFO: grad norm: 3.670 3.658 0.295
2024-12-02-05:21:59-root-INFO: grad norm: 3.623 3.610 0.299
2024-12-02-05:22:00-root-INFO: grad norm: 3.784 3.772 0.301
2024-12-02-05:22:01-root-INFO: Loss Change: 26.290 -> 23.624
2024-12-02-05:22:01-root-INFO: Regularization Change: 0.000 -> 7.138
2024-12-02-05:22:01-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-05:22:01-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-05:22:01-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-05:22:02-root-INFO: grad norm: 4.175 4.141 0.531
2024-12-02-05:22:03-root-INFO: grad norm: 3.931 3.899 0.503
2024-12-02-05:22:04-root-INFO: grad norm: 3.776 3.748 0.456
2024-12-02-05:22:05-root-INFO: grad norm: 3.908 3.881 0.457
2024-12-02-05:22:06-root-INFO: grad norm: 3.788 3.762 0.440
2024-12-02-05:22:06-root-INFO: Loss Change: 23.656 -> 21.109
2024-12-02-05:22:06-root-INFO: Regularization Change: 0.000 -> 5.621
2024-12-02-05:22:06-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-05:22:06-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-05:22:07-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-05:22:07-root-INFO: grad norm: 3.447 3.435 0.294
2024-12-02-05:22:08-root-INFO: grad norm: 3.302 3.289 0.298
2024-12-02-05:22:09-root-INFO: grad norm: 3.209 3.195 0.298
2024-12-02-05:22:10-root-INFO: grad norm: 3.243 3.229 0.301
2024-12-02-05:22:11-root-INFO: grad norm: 3.386 3.371 0.311
2024-12-02-05:22:11-root-INFO: Loss Change: 20.754 -> 19.367
2024-12-02-05:22:11-root-INFO: Regularization Change: 0.000 -> 4.369
2024-12-02-05:22:11-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-05:22:11-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-05:22:12-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-05:22:12-root-INFO: grad norm: 3.801 3.764 0.529
2024-12-02-05:22:13-root-INFO: grad norm: 3.643 3.608 0.499
2024-12-02-05:22:14-root-INFO: grad norm: 3.516 3.487 0.444
2024-12-02-05:22:15-root-INFO: grad norm: 3.537 3.509 0.443
2024-12-02-05:22:16-root-INFO: grad norm: 3.415 3.390 0.409
2024-12-02-05:22:17-root-INFO: Loss Change: 19.385 -> 17.633
2024-12-02-05:22:17-root-INFO: Regularization Change: 0.000 -> 4.089
2024-12-02-05:22:17-root-INFO: Undo step: 30
2024-12-02-05:22:17-root-INFO: Undo step: 31
2024-12-02-05:22:17-root-INFO: Undo step: 32
2024-12-02-05:22:17-root-INFO: Undo step: 33
2024-12-02-05:22:17-root-INFO: Undo step: 34
2024-12-02-05:22:17-root-INFO: step: 35 lr_xt 0.34671555
2024-12-02-05:22:18-root-INFO: grad norm: 15.753 15.666 1.647
2024-12-02-05:22:19-root-INFO: grad norm: 7.498 7.436 0.964
2024-12-02-05:22:20-root-INFO: grad norm: 5.227 5.180 0.705
2024-12-02-05:22:21-root-INFO: grad norm: 4.478 4.445 0.548
2024-12-02-05:22:21-root-INFO: grad norm: 4.028 3.997 0.500
2024-12-02-05:22:22-root-INFO: Loss Change: 102.578 -> 32.335
2024-12-02-05:22:22-root-INFO: Regularization Change: 0.000 -> 87.437
2024-12-02-05:22:22-root-INFO: Learning rate of xt decay: 0.38526 -> 0.38988.
2024-12-02-05:22:22-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-05:22:22-root-INFO: step: 34 lr_xt 0.35289102
2024-12-02-05:22:23-root-INFO: grad norm: 4.036 3.997 0.562
2024-12-02-05:22:24-root-INFO: grad norm: 3.835 3.800 0.522
2024-12-02-05:22:25-root-INFO: grad norm: 3.652 3.625 0.446
2024-12-02-05:22:26-root-INFO: grad norm: 3.648 3.621 0.443
2024-12-02-05:22:27-root-INFO: grad norm: 3.719 3.696 0.409
2024-12-02-05:22:27-root-INFO: Loss Change: 32.171 -> 26.081
2024-12-02-05:22:27-root-INFO: Regularization Change: 0.000 -> 12.298
2024-12-02-05:22:27-root-INFO: Learning rate of xt decay: 0.38988 -> 0.39456.
2024-12-02-05:22:27-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-05:22:28-root-INFO: step: 33 lr_xt 0.35911909
2024-12-02-05:22:28-root-INFO: grad norm: 3.454 3.441 0.305
2024-12-02-05:22:29-root-INFO: grad norm: 3.242 3.228 0.295
2024-12-02-05:22:30-root-INFO: grad norm: 3.144 3.132 0.279
2024-12-02-05:22:31-root-INFO: grad norm: 3.305 3.293 0.286
2024-12-02-05:22:32-root-INFO: grad norm: 3.226 3.214 0.279
2024-12-02-05:22:33-root-INFO: Loss Change: 25.491 -> 22.330
2024-12-02-05:22:33-root-INFO: Regularization Change: 0.000 -> 6.711
2024-12-02-05:22:33-root-INFO: Learning rate of xt decay: 0.39456 -> 0.39929.
2024-12-02-05:22:33-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-05:22:33-root-INFO: step: 32 lr_xt 0.36539868
2024-12-02-05:22:33-root-INFO: grad norm: 3.497 3.469 0.444
2024-12-02-05:22:34-root-INFO: grad norm: 3.437 3.409 0.435
2024-12-02-05:22:35-root-INFO: grad norm: 3.555 3.531 0.410
2024-12-02-05:22:36-root-INFO: grad norm: 3.396 3.371 0.419
2024-12-02-05:22:37-root-INFO: grad norm: 3.198 3.175 0.382
2024-12-02-05:22:38-root-INFO: Loss Change: 22.306 -> 20.093
2024-12-02-05:22:38-root-INFO: Regularization Change: 0.000 -> 5.024
2024-12-02-05:22:38-root-INFO: Learning rate of xt decay: 0.39929 -> 0.40409.
2024-12-02-05:22:38-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-05:22:38-root-INFO: step: 31 lr_xt 0.37172867
2024-12-02-05:22:39-root-INFO: grad norm: 2.997 2.986 0.261
2024-12-02-05:22:40-root-INFO: grad norm: 3.006 2.994 0.269
2024-12-02-05:22:41-root-INFO: grad norm: 3.002 2.989 0.272
2024-12-02-05:22:42-root-INFO: grad norm: 3.157 3.144 0.281
2024-12-02-05:22:43-root-INFO: grad norm: 3.060 3.046 0.284
2024-12-02-05:22:43-root-INFO: Loss Change: 19.733 -> 18.238
2024-12-02-05:22:43-root-INFO: Regularization Change: 0.000 -> 3.897
2024-12-02-05:22:43-root-INFO: Learning rate of xt decay: 0.40409 -> 0.40893.
2024-12-02-05:22:43-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-05:22:44-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-05:22:44-root-INFO: grad norm: 3.376 3.343 0.474
2024-12-02-05:22:45-root-INFO: grad norm: 3.276 3.244 0.458
2024-12-02-05:22:46-root-INFO: grad norm: 3.367 3.341 0.422
2024-12-02-05:22:47-root-INFO: grad norm: 3.129 3.100 0.429
2024-12-02-05:22:48-root-INFO: grad norm: 2.900 2.875 0.383
2024-12-02-05:22:49-root-INFO: Loss Change: 18.240 -> 16.754
2024-12-02-05:22:49-root-INFO: Regularization Change: 0.000 -> 3.616
2024-12-02-05:22:49-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-05:22:49-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-05:22:49-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-05:22:49-root-INFO: grad norm: 2.594 2.584 0.230
2024-12-02-05:22:50-root-INFO: grad norm: 2.509 2.498 0.229
2024-12-02-05:22:51-root-INFO: grad norm: 2.544 2.533 0.238
2024-12-02-05:22:52-root-INFO: grad norm: 2.892 2.881 0.256
2024-12-02-05:22:53-root-INFO: Loss too large (15.775->15.833)! Learning rate decreased to 0.30763.
2024-12-02-05:22:54-root-INFO: grad norm: 1.990 1.980 0.194
2024-12-02-05:22:54-root-INFO: Loss Change: 16.404 -> 14.906
2024-12-02-05:22:54-root-INFO: Regularization Change: 0.000 -> 2.167
2024-12-02-05:22:54-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-05:22:54-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-05:22:55-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-05:22:55-root-INFO: grad norm: 1.975 1.937 0.385
2024-12-02-05:22:56-root-INFO: grad norm: 2.091 2.059 0.366
2024-12-02-05:22:57-root-INFO: grad norm: 2.743 2.716 0.384
2024-12-02-05:22:57-root-INFO: Loss too large (14.553->14.848)! Learning rate decreased to 0.31281.
2024-12-02-05:22:58-root-INFO: grad norm: 2.024 2.004 0.290
2024-12-02-05:22:59-root-INFO: grad norm: 1.198 1.185 0.180
2024-12-02-05:23:00-root-INFO: Loss Change: 14.788 -> 13.698
2024-12-02-05:23:00-root-INFO: Regularization Change: 0.000 -> 1.632
2024-12-02-05:23:00-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-05:23:00-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-05:23:00-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-05:23:01-root-INFO: grad norm: 0.990 0.974 0.176
2024-12-02-05:23:02-root-INFO: grad norm: 0.913 0.904 0.134
2024-12-02-05:23:02-root-INFO: grad norm: 1.025 1.019 0.115
2024-12-02-05:23:03-root-INFO: grad norm: 1.579 1.574 0.125
2024-12-02-05:23:04-root-INFO: Loss too large (13.058->13.167)! Learning rate decreased to 0.31802.
2024-12-02-05:23:05-root-INFO: grad norm: 1.485 1.480 0.124
2024-12-02-05:23:05-root-INFO: Loss Change: 13.530 -> 12.773
2024-12-02-05:23:05-root-INFO: Regularization Change: 0.000 -> 1.495
2024-12-02-05:23:05-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-05:23:05-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-05:23:06-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-05:23:06-root-INFO: grad norm: 1.992 1.960 0.355
2024-12-02-05:23:06-root-INFO: Loss too large (12.763->12.886)! Learning rate decreased to 0.32327.
2024-12-02-05:23:07-root-INFO: grad norm: 1.717 1.700 0.241
2024-12-02-05:23:07-root-INFO: grad norm: 1.487 1.476 0.181
2024-12-02-05:23:08-root-INFO: grad norm: 1.444 1.433 0.174
2024-12-02-05:23:08-root-INFO: grad norm: 1.484 1.475 0.157
2024-12-02-05:23:09-root-INFO: Loss Change: 12.763 -> 12.069
2024-12-02-05:23:09-root-INFO: Regularization Change: 0.000 -> 1.288
2024-12-02-05:23:09-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-05:23:09-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-05:23:09-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-05:23:09-root-INFO: grad norm: 1.437 1.423 0.197
2024-12-02-05:23:09-root-INFO: grad norm: 1.971 1.965 0.154
2024-12-02-05:23:10-root-INFO: Loss too large (11.654->11.866)! Learning rate decreased to 0.32856.
2024-12-02-05:23:10-root-INFO: grad norm: 1.620 1.615 0.129
2024-12-02-05:23:11-root-INFO: grad norm: 1.079 1.075 0.099
2024-12-02-05:23:11-root-INFO: grad norm: 1.062 1.057 0.107
2024-12-02-05:23:11-root-INFO: Loss Change: 11.794 -> 11.050
2024-12-02-05:23:11-root-INFO: Regularization Change: 0.000 -> 1.224
2024-12-02-05:23:11-root-INFO: Undo step: 25
2024-12-02-05:23:11-root-INFO: Undo step: 26
2024-12-02-05:23:11-root-INFO: Undo step: 27
2024-12-02-05:23:11-root-INFO: Undo step: 28
2024-12-02-05:23:11-root-INFO: Undo step: 29
2024-12-02-05:23:11-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-05:23:12-root-INFO: grad norm: 14.302 14.239 1.335
2024-12-02-05:23:12-root-INFO: grad norm: 6.724 6.680 0.771
2024-12-02-05:23:13-root-INFO: grad norm: 5.146 5.114 0.572
2024-12-02-05:23:13-root-INFO: grad norm: 4.602 4.566 0.581
2024-12-02-05:23:13-root-INFO: grad norm: 4.625 4.588 0.585
2024-12-02-05:23:14-root-INFO: Loss Change: 91.867 -> 26.302
2024-12-02-05:23:14-root-INFO: Regularization Change: 0.000 -> 89.697
2024-12-02-05:23:14-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-05:23:14-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-05:23:14-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-05:23:14-root-INFO: grad norm: 4.067 4.037 0.496
2024-12-02-05:23:15-root-INFO: grad norm: 4.104 4.074 0.490
2024-12-02-05:23:15-root-INFO: grad norm: 3.859 3.818 0.559
2024-12-02-05:23:15-root-INFO: grad norm: 3.767 3.730 0.528
2024-12-02-05:23:16-root-INFO: grad norm: 3.895 3.850 0.591
2024-12-02-05:23:16-root-INFO: Loss Change: 25.689 -> 20.276
2024-12-02-05:23:16-root-INFO: Regularization Change: 0.000 -> 12.889
2024-12-02-05:23:16-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-05:23:16-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-05:23:16-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-05:23:16-root-INFO: grad norm: 4.783 4.706 0.858
2024-12-02-05:23:17-root-INFO: grad norm: 4.693 4.621 0.820
2024-12-02-05:23:17-root-INFO: grad norm: 4.470 4.414 0.710
2024-12-02-05:23:18-root-INFO: grad norm: 4.190 4.127 0.723
2024-12-02-05:23:18-root-INFO: grad norm: 4.040 3.986 0.654
2024-12-02-05:23:19-root-INFO: Loss Change: 20.410 -> 17.176
2024-12-02-05:23:19-root-INFO: Regularization Change: 0.000 -> 7.895
2024-12-02-05:23:19-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-05:23:19-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-05:23:19-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-05:23:19-root-INFO: grad norm: 3.552 3.520 0.472
2024-12-02-05:23:19-root-INFO: grad norm: 3.362 3.330 0.466
2024-12-02-05:23:20-root-INFO: grad norm: 3.149 3.108 0.505
2024-12-02-05:23:20-root-INFO: grad norm: 3.192 3.155 0.486
2024-12-02-05:23:21-root-INFO: grad norm: 3.238 3.195 0.525
2024-12-02-05:23:21-root-INFO: Loss Change: 16.654 -> 14.830
2024-12-02-05:23:21-root-INFO: Regularization Change: 0.000 -> 4.960
2024-12-02-05:23:21-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-05:23:21-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-05:23:21-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-05:23:21-root-INFO: grad norm: 3.903 3.827 0.767
2024-12-02-05:23:22-root-INFO: grad norm: 3.671 3.601 0.713
2024-12-02-05:23:22-root-INFO: grad norm: 3.431 3.378 0.602
2024-12-02-05:23:23-root-INFO: grad norm: 3.297 3.242 0.597
2024-12-02-05:23:23-root-INFO: grad norm: 3.176 3.131 0.537
2024-12-02-05:23:24-root-INFO: Loss Change: 15.087 -> 13.360
2024-12-02-05:23:24-root-INFO: Regularization Change: 0.000 -> 4.374
2024-12-02-05:23:24-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-05:23:24-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-05:23:24-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-05:23:24-root-INFO: grad norm: 2.736 2.716 0.330
2024-12-02-05:23:25-root-INFO: grad norm: 2.554 2.532 0.331
2024-12-02-05:23:25-root-INFO: grad norm: 2.350 2.323 0.354
2024-12-02-05:23:26-root-INFO: grad norm: 2.379 2.355 0.339
2024-12-02-05:23:26-root-INFO: grad norm: 2.481 2.453 0.369
2024-12-02-05:23:26-root-INFO: Loss Change: 12.810 -> 11.735
2024-12-02-05:23:26-root-INFO: Regularization Change: 0.000 -> 3.049
2024-12-02-05:23:26-root-INFO: Undo step: 25
2024-12-02-05:23:26-root-INFO: Undo step: 26
2024-12-02-05:23:26-root-INFO: Undo step: 27
2024-12-02-05:23:26-root-INFO: Undo step: 28
2024-12-02-05:23:26-root-INFO: Undo step: 29
2024-12-02-05:23:27-root-INFO: step: 30 lr_xt 0.37810791
2024-12-02-05:23:27-root-INFO: grad norm: 14.131 14.087 1.102
2024-12-02-05:23:27-root-INFO: grad norm: 6.350 6.316 0.662
2024-12-02-05:23:28-root-INFO: grad norm: 4.681 4.659 0.461
2024-12-02-05:23:28-root-INFO: grad norm: 4.274 4.251 0.447
2024-12-02-05:23:29-root-INFO: grad norm: 4.050 4.030 0.402
2024-12-02-05:23:29-root-INFO: Loss Change: 88.660 -> 25.558
2024-12-02-05:23:29-root-INFO: Regularization Change: 0.000 -> 85.184
2024-12-02-05:23:29-root-INFO: Learning rate of xt decay: 0.40893 -> 0.41384.
2024-12-02-05:23:29-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-05:23:29-root-INFO: step: 29 lr_xt 0.38453518
2024-12-02-05:23:29-root-INFO: grad norm: 3.818 3.802 0.345
2024-12-02-05:23:30-root-INFO: grad norm: 3.715 3.701 0.326
2024-12-02-05:23:30-root-INFO: grad norm: 3.570 3.554 0.335
2024-12-02-05:23:31-root-INFO: grad norm: 3.490 3.474 0.332
2024-12-02-05:23:31-root-INFO: grad norm: 3.361 3.344 0.344
2024-12-02-05:23:32-root-INFO: Loss Change: 25.092 -> 19.565
2024-12-02-05:23:32-root-INFO: Regularization Change: 0.000 -> 12.390
2024-12-02-05:23:32-root-INFO: Learning rate of xt decay: 0.41384 -> 0.41881.
2024-12-02-05:23:32-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-05:23:32-root-INFO: step: 28 lr_xt 0.39100924
2024-12-02-05:23:32-root-INFO: grad norm: 3.757 3.717 0.549
2024-12-02-05:23:33-root-INFO: grad norm: 3.500 3.462 0.516
2024-12-02-05:23:33-root-INFO: grad norm: 3.304 3.272 0.461
2024-12-02-05:23:34-root-INFO: grad norm: 3.158 3.123 0.470
2024-12-02-05:23:34-root-INFO: grad norm: 3.322 3.290 0.458
2024-12-02-05:23:34-root-INFO: Loss Change: 19.445 -> 16.599
2024-12-02-05:23:34-root-INFO: Regularization Change: 0.000 -> 7.008
2024-12-02-05:23:34-root-INFO: Learning rate of xt decay: 0.41881 -> 0.42383.
2024-12-02-05:23:34-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-05:23:35-root-INFO: step: 27 lr_xt 0.39752879
2024-12-02-05:23:35-root-INFO: grad norm: 2.788 2.772 0.296
2024-12-02-05:23:35-root-INFO: grad norm: 2.518 2.500 0.294
2024-12-02-05:23:36-root-INFO: grad norm: 2.520 2.498 0.329
2024-12-02-05:23:36-root-INFO: grad norm: 2.886 2.864 0.349
2024-12-02-05:23:36-root-INFO: Loss too large (14.772->14.803)! Learning rate decreased to 0.31802.
2024-12-02-05:23:37-root-INFO: grad norm: 2.077 2.060 0.268
2024-12-02-05:23:37-root-INFO: Loss Change: 16.122 -> 13.792
2024-12-02-05:23:37-root-INFO: Regularization Change: 0.000 -> 3.490
2024-12-02-05:23:37-root-INFO: Learning rate of xt decay: 0.42383 -> 0.42892.
2024-12-02-05:23:37-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-05:23:37-root-INFO: step: 26 lr_xt 0.40409250
2024-12-02-05:23:37-root-INFO: grad norm: 2.194 2.152 0.427
2024-12-02-05:23:38-root-INFO: grad norm: 2.361 2.325 0.411
2024-12-02-05:23:38-root-INFO: grad norm: 3.133 3.102 0.443
2024-12-02-05:23:39-root-INFO: Loss too large (13.514->13.925)! Learning rate decreased to 0.32327.
2024-12-02-05:23:39-root-INFO: grad norm: 2.387 2.364 0.337
2024-12-02-05:23:39-root-INFO: grad norm: 1.623 1.608 0.220
2024-12-02-05:23:40-root-INFO: Loss Change: 13.803 -> 12.411
2024-12-02-05:23:40-root-INFO: Regularization Change: 0.000 -> 2.326
2024-12-02-05:23:40-root-INFO: Learning rate of xt decay: 0.42892 -> 0.43407.
2024-12-02-05:23:40-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-05:23:40-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-05:23:40-root-INFO: grad norm: 1.224 1.213 0.161
2024-12-02-05:23:41-root-INFO: grad norm: 1.175 1.170 0.113
2024-12-02-05:23:41-root-INFO: grad norm: 1.282 1.276 0.128
2024-12-02-05:23:42-root-INFO: grad norm: 1.627 1.620 0.149
2024-12-02-05:23:42-root-INFO: Loss too large (11.473->11.495)! Learning rate decreased to 0.32856.
2024-12-02-05:23:42-root-INFO: grad norm: 1.517 1.509 0.161
2024-12-02-05:23:42-root-INFO: Loss Change: 12.075 -> 11.148
2024-12-02-05:23:42-root-INFO: Regularization Change: 0.000 -> 1.946
2024-12-02-05:23:42-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-05:23:42-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-05:23:43-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-05:23:43-root-INFO: grad norm: 2.531 2.499 0.399
2024-12-02-05:23:43-root-INFO: Loss too large (11.266->11.586)! Learning rate decreased to 0.33388.
2024-12-02-05:23:43-root-INFO: grad norm: 2.013 1.997 0.255
2024-12-02-05:23:44-root-INFO: grad norm: 1.389 1.378 0.171
2024-12-02-05:23:44-root-INFO: grad norm: 1.259 1.249 0.158
2024-12-02-05:23:45-root-INFO: grad norm: 1.332 1.325 0.137
2024-12-02-05:23:45-root-INFO: Loss Change: 11.266 -> 10.301
2024-12-02-05:23:45-root-INFO: Regularization Change: 0.000 -> 1.509
2024-12-02-05:23:45-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-05:23:45-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-05:23:45-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-05:23:46-root-INFO: grad norm: 1.431 1.419 0.192
2024-12-02-05:23:46-root-INFO: grad norm: 2.170 2.165 0.151
2024-12-02-05:23:46-root-INFO: Loss too large (9.944->10.253)! Learning rate decreased to 0.33923.
2024-12-02-05:23:46-root-INFO: Loss too large (9.944->10.006)! Learning rate decreased to 0.27138.
2024-12-02-05:23:47-root-INFO: grad norm: 1.456 1.452 0.115
2024-12-02-05:23:47-root-INFO: grad norm: 0.753 0.750 0.069
2024-12-02-05:23:48-root-INFO: grad norm: 0.666 0.663 0.069
2024-12-02-05:23:48-root-INFO: Loss Change: 10.051 -> 9.306
2024-12-02-05:23:48-root-INFO: Regularization Change: 0.000 -> 0.948
2024-12-02-05:23:48-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-05:23:48-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-05:23:48-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-05:23:48-root-INFO: grad norm: 1.402 1.374 0.278
2024-12-02-05:23:49-root-INFO: grad norm: 1.650 1.631 0.254
2024-12-02-05:23:49-root-INFO: Loss too large (9.227->9.304)! Learning rate decreased to 0.34461.
2024-12-02-05:23:49-root-INFO: grad norm: 1.812 1.802 0.189
2024-12-02-05:23:50-root-INFO: Loss too large (9.045->9.100)! Learning rate decreased to 0.27569.
2024-12-02-05:23:50-root-INFO: grad norm: 1.386 1.379 0.131
2024-12-02-05:23:51-root-INFO: grad norm: 0.915 0.912 0.077
2024-12-02-05:23:51-root-INFO: Loss Change: 9.288 -> 8.634
2024-12-02-05:23:51-root-INFO: Regularization Change: 0.000 -> 0.963
2024-12-02-05:23:51-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-05:23:51-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-05:23:51-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-05:23:51-root-INFO: grad norm: 1.151 1.130 0.215
2024-12-02-05:23:52-root-INFO: grad norm: 1.355 1.346 0.156
2024-12-02-05:23:52-root-INFO: Loss too large (8.365->8.425)! Learning rate decreased to 0.35002.
2024-12-02-05:23:52-root-INFO: grad norm: 1.293 1.289 0.095
2024-12-02-05:23:53-root-INFO: grad norm: 1.415 1.413 0.088
2024-12-02-05:23:53-root-INFO: Loss too large (8.156->8.165)! Learning rate decreased to 0.28002.
2024-12-02-05:23:53-root-INFO: grad norm: 1.149 1.146 0.083
2024-12-02-05:23:54-root-INFO: Loss Change: 8.524 -> 7.928
2024-12-02-05:23:54-root-INFO: Regularization Change: 0.000 -> 0.961
2024-12-02-05:23:54-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-05:23:54-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-05:23:54-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-05:23:54-root-INFO: grad norm: 1.855 1.829 0.305
2024-12-02-05:23:54-root-INFO: Loss too large (7.965->8.122)! Learning rate decreased to 0.35546.
2024-12-02-05:23:55-root-INFO: grad norm: 1.566 1.556 0.182
2024-12-02-05:23:55-root-INFO: grad norm: 1.666 1.660 0.139
2024-12-02-05:23:55-root-INFO: Loss too large (7.715->7.727)! Learning rate decreased to 0.28437.
2024-12-02-05:23:56-root-INFO: grad norm: 1.282 1.277 0.108
2024-12-02-05:23:56-root-INFO: grad norm: 0.871 0.869 0.067
2024-12-02-05:23:57-root-INFO: Loss Change: 7.965 -> 7.328
2024-12-02-05:23:57-root-INFO: Regularization Change: 0.000 -> 0.825
2024-12-02-05:23:57-root-INFO: Undo step: 20
2024-12-02-05:23:57-root-INFO: Undo step: 21
2024-12-02-05:23:57-root-INFO: Undo step: 22
2024-12-02-05:23:57-root-INFO: Undo step: 23
2024-12-02-05:23:57-root-INFO: Undo step: 24
2024-12-02-05:23:57-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-05:23:57-root-INFO: grad norm: 13.500 13.456 1.086
2024-12-02-05:23:57-root-INFO: grad norm: 6.246 6.213 0.634
2024-12-02-05:23:58-root-INFO: grad norm: 4.811 4.781 0.537
2024-12-02-05:23:58-root-INFO: grad norm: 4.258 4.239 0.401
2024-12-02-05:23:59-root-INFO: grad norm: 4.084 4.058 0.458
2024-12-02-05:23:59-root-INFO: Loss Change: 81.612 -> 20.501
2024-12-02-05:23:59-root-INFO: Regularization Change: 0.000 -> 90.998
2024-12-02-05:23:59-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-05:23:59-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-05:23:59-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-05:23:59-root-INFO: grad norm: 4.939 4.901 0.612
2024-12-02-05:24:00-root-INFO: grad norm: 4.362 4.316 0.632
2024-12-02-05:24:00-root-INFO: grad norm: 4.029 3.992 0.545
2024-12-02-05:24:01-root-INFO: grad norm: 3.763 3.710 0.629
2024-12-02-05:24:01-root-INFO: grad norm: 3.784 3.738 0.584
2024-12-02-05:24:02-root-INFO: Loss Change: 20.724 -> 15.167
2024-12-02-05:24:02-root-INFO: Regularization Change: 0.000 -> 13.244
2024-12-02-05:24:02-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-05:24:02-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-05:24:02-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-05:24:02-root-INFO: grad norm: 3.238 3.204 0.462
2024-12-02-05:24:02-root-INFO: grad norm: 3.192 3.159 0.456
2024-12-02-05:24:03-root-INFO: grad norm: 3.204 3.160 0.528
2024-12-02-05:24:03-root-INFO: grad norm: 3.515 3.476 0.521
2024-12-02-05:24:04-root-INFO: Loss too large (12.786->12.903)! Learning rate decreased to 0.33923.
2024-12-02-05:24:04-root-INFO: grad norm: 2.574 2.543 0.399
2024-12-02-05:24:04-root-INFO: Loss Change: 14.331 -> 11.374
2024-12-02-05:24:04-root-INFO: Regularization Change: 0.000 -> 5.062
2024-12-02-05:24:04-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-05:24:04-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-05:24:05-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-05:24:05-root-INFO: grad norm: 2.636 2.591 0.487
2024-12-02-05:24:05-root-INFO: grad norm: 2.755 2.712 0.481
2024-12-02-05:24:06-root-INFO: grad norm: 3.180 3.143 0.480
2024-12-02-05:24:06-root-INFO: Loss too large (11.032->11.337)! Learning rate decreased to 0.34461.
2024-12-02-05:24:06-root-INFO: grad norm: 2.357 2.329 0.358
2024-12-02-05:24:07-root-INFO: grad norm: 1.512 1.498 0.207
2024-12-02-05:24:07-root-INFO: Loss Change: 11.480 -> 9.723
2024-12-02-05:24:07-root-INFO: Regularization Change: 0.000 -> 2.896
2024-12-02-05:24:07-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-05:24:07-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-05:24:07-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-05:24:07-root-INFO: grad norm: 1.175 1.165 0.149
2024-12-02-05:24:08-root-INFO: grad norm: 1.331 1.327 0.109
2024-12-02-05:24:08-root-INFO: grad norm: 1.578 1.573 0.130
2024-12-02-05:24:09-root-INFO: grad norm: 2.448 2.442 0.165
2024-12-02-05:24:09-root-INFO: Loss too large (9.011->9.374)! Learning rate decreased to 0.35002.
2024-12-02-05:24:09-root-INFO: Loss too large (9.011->9.064)! Learning rate decreased to 0.28002.
2024-12-02-05:24:10-root-INFO: grad norm: 1.536 1.530 0.136
2024-12-02-05:24:10-root-INFO: Loss Change: 9.444 -> 8.474
2024-12-02-05:24:10-root-INFO: Regularization Change: 0.000 -> 1.788
2024-12-02-05:24:10-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-05:24:10-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-05:24:10-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-05:24:10-root-INFO: grad norm: 1.558 1.528 0.304
2024-12-02-05:24:11-root-INFO: grad norm: 1.690 1.665 0.285
2024-12-02-05:24:11-root-INFO: Loss too large (8.291->8.309)! Learning rate decreased to 0.35546.
2024-12-02-05:24:11-root-INFO: grad norm: 1.735 1.723 0.200
2024-12-02-05:24:12-root-INFO: grad norm: 1.636 1.625 0.184
2024-12-02-05:24:12-root-INFO: grad norm: 1.496 1.489 0.139
2024-12-02-05:24:13-root-INFO: Loss Change: 8.446 -> 7.733
2024-12-02-05:24:13-root-INFO: Regularization Change: 0.000 -> 1.590
2024-12-02-05:24:13-root-INFO: Undo step: 20
2024-12-02-05:24:13-root-INFO: Undo step: 21
2024-12-02-05:24:13-root-INFO: Undo step: 22
2024-12-02-05:24:13-root-INFO: Undo step: 23
2024-12-02-05:24:13-root-INFO: Undo step: 24
2024-12-02-05:24:13-root-INFO: step: 25 lr_xt 0.41069899
2024-12-02-05:24:13-root-INFO: grad norm: 13.436 13.387 1.145
2024-12-02-05:24:13-root-INFO: grad norm: 6.103 6.065 0.681
2024-12-02-05:24:14-root-INFO: grad norm: 4.210 4.183 0.477
2024-12-02-05:24:14-root-INFO: grad norm: 3.441 3.421 0.376
2024-12-02-05:24:15-root-INFO: grad norm: 3.124 3.103 0.362
2024-12-02-05:24:15-root-INFO: Loss Change: 82.635 -> 20.350
2024-12-02-05:24:15-root-INFO: Regularization Change: 0.000 -> 91.129
2024-12-02-05:24:15-root-INFO: Learning rate of xt decay: 0.43407 -> 0.43928.
2024-12-02-05:24:15-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-05:24:15-root-INFO: step: 24 lr_xt 0.41734684
2024-12-02-05:24:15-root-INFO: grad norm: 4.222 4.194 0.489
2024-12-02-05:24:16-root-INFO: grad norm: 3.405 3.372 0.475
2024-12-02-05:24:16-root-INFO: grad norm: 3.176 3.159 0.336
2024-12-02-05:24:17-root-INFO: grad norm: 2.949 2.928 0.353
2024-12-02-05:24:17-root-INFO: grad norm: 2.887 2.873 0.279
2024-12-02-05:24:18-root-INFO: Loss Change: 20.312 -> 14.870
2024-12-02-05:24:18-root-INFO: Regularization Change: 0.000 -> 12.404
2024-12-02-05:24:18-root-INFO: Learning rate of xt decay: 0.43928 -> 0.44455.
2024-12-02-05:24:18-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-05:24:18-root-INFO: step: 23 lr_xt 0.42403458
2024-12-02-05:24:18-root-INFO: grad norm: 2.815 2.802 0.273
2024-12-02-05:24:18-root-INFO: grad norm: 2.647 2.635 0.250
2024-12-02-05:24:19-root-INFO: grad norm: 2.520 2.510 0.221
2024-12-02-05:24:19-root-INFO: grad norm: 2.519 2.508 0.225
2024-12-02-05:24:20-root-INFO: grad norm: 2.689 2.679 0.232
2024-12-02-05:24:20-root-INFO: Loss Change: 14.364 -> 12.101
2024-12-02-05:24:20-root-INFO: Regularization Change: 0.000 -> 6.267
2024-12-02-05:24:20-root-INFO: Learning rate of xt decay: 0.44455 -> 0.44988.
2024-12-02-05:24:20-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-05:24:20-root-INFO: step: 22 lr_xt 0.43076069
2024-12-02-05:24:20-root-INFO: grad norm: 4.154 4.138 0.359
2024-12-02-05:24:21-root-INFO: Loss too large (12.052->12.275)! Learning rate decreased to 0.34461.
2024-12-02-05:24:21-root-INFO: grad norm: 2.169 2.159 0.208
2024-12-02-05:24:22-root-INFO: grad norm: 1.462 1.456 0.124
2024-12-02-05:24:22-root-INFO: grad norm: 1.119 1.113 0.119
2024-12-02-05:24:23-root-INFO: grad norm: 0.947 0.942 0.093
2024-12-02-05:24:23-root-INFO: Loss Change: 12.052 -> 9.760
2024-12-02-05:24:23-root-INFO: Regularization Change: 0.000 -> 2.614
2024-12-02-05:24:23-root-INFO: Learning rate of xt decay: 0.44988 -> 0.45528.
2024-12-02-05:24:23-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-05:24:23-root-INFO: step: 21 lr_xt 0.43752364
2024-12-02-05:24:23-root-INFO: grad norm: 1.325 1.303 0.241
2024-12-02-05:24:24-root-INFO: grad norm: 1.317 1.294 0.244
2024-12-02-05:24:24-root-INFO: grad norm: 1.682 1.666 0.230
2024-12-02-05:24:24-root-INFO: Loss too large (9.115->9.132)! Learning rate decreased to 0.35002.
2024-12-02-05:24:25-root-INFO: grad norm: 1.490 1.473 0.224
2024-12-02-05:24:25-root-INFO: grad norm: 1.611 1.602 0.174
2024-12-02-05:24:25-root-INFO: Loss Change: 9.575 -> 8.724
2024-12-02-05:24:25-root-INFO: Regularization Change: 0.000 -> 2.135
2024-12-02-05:24:25-root-INFO: Learning rate of xt decay: 0.45528 -> 0.46074.
2024-12-02-05:24:25-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-05:24:26-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-05:24:26-root-INFO: grad norm: 1.504 1.494 0.176
2024-12-02-05:24:26-root-INFO: grad norm: 2.225 2.219 0.157
2024-12-02-05:24:26-root-INFO: Loss too large (8.296->8.529)! Learning rate decreased to 0.35546.
2024-12-02-05:24:27-root-INFO: Loss too large (8.296->8.320)! Learning rate decreased to 0.28437.
2024-12-02-05:24:27-root-INFO: grad norm: 1.396 1.392 0.107
2024-12-02-05:24:28-root-INFO: grad norm: 0.863 0.860 0.065
2024-12-02-05:24:28-root-INFO: grad norm: 0.763 0.759 0.075
2024-12-02-05:24:28-root-INFO: Loss Change: 8.474 -> 7.589
2024-12-02-05:24:28-root-INFO: Regularization Change: 0.000 -> 1.152
2024-12-02-05:24:28-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-05:24:28-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-05:24:29-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-05:24:29-root-INFO: grad norm: 1.414 1.388 0.268
2024-12-02-05:24:29-root-INFO: grad norm: 1.614 1.592 0.270
2024-12-02-05:24:29-root-INFO: Loss too large (7.455->7.457)! Learning rate decreased to 0.36092.
2024-12-02-05:24:30-root-INFO: grad norm: 1.659 1.649 0.177
2024-12-02-05:24:30-root-INFO: grad norm: 1.606 1.595 0.186
2024-12-02-05:24:31-root-INFO: grad norm: 1.971 1.964 0.166
2024-12-02-05:24:31-root-INFO: Loss too large (7.029->7.088)! Learning rate decreased to 0.28874.
2024-12-02-05:24:31-root-INFO: Loss Change: 7.507 -> 6.956
2024-12-02-05:24:31-root-INFO: Regularization Change: 0.000 -> 1.365
2024-12-02-05:24:31-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-05:24:31-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-05:24:31-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-05:24:31-root-INFO: grad norm: 1.310 1.296 0.191
2024-12-02-05:24:32-root-INFO: grad norm: 1.637 1.630 0.152
2024-12-02-05:24:32-root-INFO: Loss too large (6.612->6.722)! Learning rate decreased to 0.36641.
2024-12-02-05:24:33-root-INFO: grad norm: 1.415 1.412 0.099
2024-12-02-05:24:33-root-INFO: grad norm: 1.267 1.265 0.072
2024-12-02-05:24:33-root-INFO: grad norm: 1.258 1.254 0.096
2024-12-02-05:24:34-root-INFO: Loss Change: 6.801 -> 6.147
2024-12-02-05:24:34-root-INFO: Regularization Change: 0.000 -> 1.264
2024-12-02-05:24:34-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-05:24:34-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-05:24:34-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-05:24:34-root-INFO: grad norm: 2.131 2.108 0.313
2024-12-02-05:24:34-root-INFO: Loss too large (6.228->6.489)! Learning rate decreased to 0.37193.
2024-12-02-05:24:35-root-INFO: grad norm: 1.729 1.717 0.210
2024-12-02-05:24:35-root-INFO: grad norm: 1.581 1.574 0.143
2024-12-02-05:24:36-root-INFO: grad norm: 1.247 1.238 0.147
2024-12-02-05:24:36-root-INFO: grad norm: 1.215 1.211 0.099
2024-12-02-05:24:37-root-INFO: Loss Change: 6.228 -> 5.579
2024-12-02-05:24:37-root-INFO: Regularization Change: 0.000 -> 1.155
2024-12-02-05:24:37-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-05:24:37-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-05:24:37-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-05:24:37-root-INFO: grad norm: 1.255 1.243 0.172
2024-12-02-05:24:37-root-INFO: grad norm: 1.588 1.583 0.128
2024-12-02-05:24:38-root-INFO: Loss too large (5.364->5.531)! Learning rate decreased to 0.37747.
2024-12-02-05:24:38-root-INFO: grad norm: 1.421 1.418 0.093
2024-12-02-05:24:38-root-INFO: grad norm: 1.397 1.396 0.068
2024-12-02-05:24:39-root-INFO: grad norm: 1.183 1.179 0.093
2024-12-02-05:24:39-root-INFO: Loss Change: 5.478 -> 4.997
2024-12-02-05:24:39-root-INFO: Regularization Change: 0.000 -> 1.004
2024-12-02-05:24:39-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-05:24:39-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-05:24:39-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-05:24:40-root-INFO: grad norm: 1.937 1.909 0.324
2024-12-02-05:24:40-root-INFO: Loss too large (5.129->5.186)! Learning rate decreased to 0.38303.
2024-12-02-05:24:40-root-INFO: grad norm: 1.507 1.493 0.202
2024-12-02-05:24:41-root-INFO: grad norm: 1.522 1.515 0.143
2024-12-02-05:24:41-root-INFO: grad norm: 1.180 1.172 0.138
2024-12-02-05:24:42-root-INFO: grad norm: 1.065 1.061 0.090
2024-12-02-05:24:42-root-INFO: Loss Change: 5.129 -> 4.495
2024-12-02-05:24:42-root-INFO: Regularization Change: 0.000 -> 1.003
2024-12-02-05:24:42-root-INFO: Undo step: 15
2024-12-02-05:24:42-root-INFO: Undo step: 16
2024-12-02-05:24:42-root-INFO: Undo step: 17
2024-12-02-05:24:42-root-INFO: Undo step: 18
2024-12-02-05:24:42-root-INFO: Undo step: 19
2024-12-02-05:24:42-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-05:24:42-root-INFO: grad norm: 12.622 12.574 1.098
2024-12-02-05:24:43-root-INFO: grad norm: 5.743 5.712 0.603
2024-12-02-05:24:43-root-INFO: grad norm: 4.097 4.065 0.510
2024-12-02-05:24:43-root-INFO: grad norm: 3.516 3.492 0.409
2024-12-02-05:24:44-root-INFO: grad norm: 3.239 3.208 0.447
2024-12-02-05:24:44-root-INFO: Loss Change: 75.235 -> 15.071
2024-12-02-05:24:44-root-INFO: Regularization Change: 0.000 -> 94.855
2024-12-02-05:24:44-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-05:24:44-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-05:24:44-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-05:24:45-root-INFO: grad norm: 3.775 3.727 0.598
2024-12-02-05:24:45-root-INFO: grad norm: 3.490 3.444 0.567
2024-12-02-05:24:46-root-INFO: grad norm: 3.394 3.365 0.447
2024-12-02-05:24:46-root-INFO: grad norm: 3.041 3.006 0.458
2024-12-02-05:24:46-root-INFO: grad norm: 3.053 3.025 0.417
2024-12-02-05:24:47-root-INFO: Loss Change: 14.972 -> 10.560
2024-12-02-05:24:47-root-INFO: Regularization Change: 0.000 -> 11.707
2024-12-02-05:24:47-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-05:24:47-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-05:24:47-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-05:24:47-root-INFO: grad norm: 2.364 2.350 0.261
2024-12-02-05:24:48-root-INFO: grad norm: 2.094 2.081 0.233
2024-12-02-05:24:48-root-INFO: grad norm: 2.046 2.027 0.277
2024-12-02-05:24:48-root-INFO: grad norm: 2.130 2.113 0.264
2024-12-02-05:24:49-root-INFO: grad norm: 2.226 2.203 0.316
2024-12-02-05:24:49-root-INFO: Loss Change: 9.961 -> 7.990
2024-12-02-05:24:49-root-INFO: Regularization Change: 0.000 -> 5.330
2024-12-02-05:24:49-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-05:24:49-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-05:24:49-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-05:24:50-root-INFO: grad norm: 3.440 3.394 0.564
2024-12-02-05:24:50-root-INFO: Loss too large (8.203->8.399)! Learning rate decreased to 0.37193.
2024-12-02-05:24:50-root-INFO: grad norm: 2.223 2.198 0.336
2024-12-02-05:24:51-root-INFO: grad norm: 1.253 1.244 0.152
2024-12-02-05:24:51-root-INFO: grad norm: 0.951 0.942 0.131
2024-12-02-05:24:52-root-INFO: grad norm: 0.855 0.851 0.086
2024-12-02-05:24:52-root-INFO: Loss Change: 8.203 -> 6.306
2024-12-02-05:24:52-root-INFO: Regularization Change: 0.000 -> 2.238
2024-12-02-05:24:52-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-05:24:52-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-05:24:52-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-05:24:52-root-INFO: grad norm: 1.055 1.040 0.178
2024-12-02-05:24:53-root-INFO: grad norm: 1.114 1.107 0.121
2024-12-02-05:24:53-root-INFO: grad norm: 0.899 0.894 0.089
2024-12-02-05:24:54-root-INFO: grad norm: 0.861 0.856 0.095
2024-12-02-05:24:54-root-INFO: grad norm: 0.907 0.903 0.081
2024-12-02-05:24:54-root-INFO: Loss Change: 6.161 -> 5.386
2024-12-02-05:24:54-root-INFO: Regularization Change: 0.000 -> 1.996
2024-12-02-05:24:54-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-05:24:54-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-05:24:55-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-05:24:55-root-INFO: grad norm: 1.837 1.821 0.247
2024-12-02-05:24:55-root-INFO: Loss too large (5.370->5.484)! Learning rate decreased to 0.38303.
2024-12-02-05:24:55-root-INFO: grad norm: 1.457 1.450 0.146
2024-12-02-05:24:56-root-INFO: grad norm: 1.086 1.083 0.080
2024-12-02-05:24:56-root-INFO: grad norm: 0.945 0.942 0.081
2024-12-02-05:24:57-root-INFO: grad norm: 0.913 0.911 0.062
2024-12-02-05:24:57-root-INFO: Loss Change: 5.370 -> 4.695
2024-12-02-05:24:57-root-INFO: Regularization Change: 0.000 -> 1.151
2024-12-02-05:24:57-root-INFO: Undo step: 15
2024-12-02-05:24:57-root-INFO: Undo step: 16
2024-12-02-05:24:57-root-INFO: Undo step: 17
2024-12-02-05:24:57-root-INFO: Undo step: 18
2024-12-02-05:24:57-root-INFO: Undo step: 19
2024-12-02-05:24:57-root-INFO: step: 20 lr_xt 0.44432183
2024-12-02-05:24:57-root-INFO: grad norm: 13.494 13.454 1.032
2024-12-02-05:24:58-root-INFO: grad norm: 6.057 6.030 0.568
2024-12-02-05:24:58-root-INFO: grad norm: 4.370 4.350 0.412
2024-12-02-05:24:59-root-INFO: grad norm: 3.294 3.280 0.304
2024-12-02-05:24:59-root-INFO: grad norm: 2.733 2.718 0.286
2024-12-02-05:25:00-root-INFO: Loss Change: 79.451 -> 15.067
2024-12-02-05:25:00-root-INFO: Regularization Change: 0.000 -> 98.918
2024-12-02-05:25:00-root-INFO: Learning rate of xt decay: 0.46074 -> 0.46627.
2024-12-02-05:25:00-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-05:25:00-root-INFO: step: 19 lr_xt 0.45115363
2024-12-02-05:25:00-root-INFO: grad norm: 2.707 2.682 0.368
2024-12-02-05:25:00-root-INFO: grad norm: 2.491 2.466 0.352
2024-12-02-05:25:01-root-INFO: grad norm: 2.644 2.626 0.307
2024-12-02-05:25:01-root-INFO: grad norm: 2.507 2.482 0.350
2024-12-02-05:25:02-root-INFO: grad norm: 2.603 2.581 0.334
2024-12-02-05:25:02-root-INFO: Loss Change: 14.705 -> 10.412
2024-12-02-05:25:02-root-INFO: Regularization Change: 0.000 -> 11.217
2024-12-02-05:25:02-root-INFO: Learning rate of xt decay: 0.46627 -> 0.47187.
2024-12-02-05:25:02-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-05:25:02-root-INFO: step: 18 lr_xt 0.45801735
2024-12-02-05:25:03-root-INFO: grad norm: 2.143 2.134 0.203
2024-12-02-05:25:03-root-INFO: grad norm: 2.367 2.358 0.208
2024-12-02-05:25:04-root-INFO: grad norm: 2.259 2.244 0.255
2024-12-02-05:25:04-root-INFO: grad norm: 2.137 2.123 0.241
2024-12-02-05:25:04-root-INFO: grad norm: 2.268 2.247 0.310
2024-12-02-05:25:05-root-INFO: Loss Change: 9.812 -> 8.006
2024-12-02-05:25:05-root-INFO: Regularization Change: 0.000 -> 5.412
2024-12-02-05:25:05-root-INFO: Learning rate of xt decay: 0.47187 -> 0.47753.
2024-12-02-05:25:05-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-05:25:05-root-INFO: step: 17 lr_xt 0.46491129
2024-12-02-05:25:05-root-INFO: grad norm: 3.629 3.582 0.584
2024-12-02-05:25:05-root-INFO: Loss too large (8.218->8.467)! Learning rate decreased to 0.37193.
2024-12-02-05:25:06-root-INFO: grad norm: 2.299 2.273 0.345
2024-12-02-05:25:06-root-INFO: grad norm: 1.285 1.277 0.144
2024-12-02-05:25:07-root-INFO: grad norm: 0.991 0.982 0.134
2024-12-02-05:25:07-root-INFO: grad norm: 0.866 0.861 0.087
2024-12-02-05:25:07-root-INFO: Loss Change: 8.218 -> 6.208
2024-12-02-05:25:07-root-INFO: Regularization Change: 0.000 -> 2.303
2024-12-02-05:25:07-root-INFO: Learning rate of xt decay: 0.47753 -> 0.48326.
2024-12-02-05:25:07-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-05:25:08-root-INFO: step: 16 lr_xt 0.47183369
2024-12-02-05:25:08-root-INFO: grad norm: 1.042 1.026 0.184
2024-12-02-05:25:08-root-INFO: grad norm: 0.833 0.821 0.142
2024-12-02-05:25:09-root-INFO: grad norm: 0.772 0.766 0.102
2024-12-02-05:25:09-root-INFO: grad norm: 0.812 0.804 0.108
2024-12-02-05:25:10-root-INFO: grad norm: 1.008 1.004 0.087
2024-12-02-05:25:10-root-INFO: Loss too large (5.360->5.385)! Learning rate decreased to 0.37747.
2024-12-02-05:25:10-root-INFO: Loss Change: 6.054 -> 5.293
2024-12-02-05:25:10-root-INFO: Regularization Change: 0.000 -> 1.875
2024-12-02-05:25:10-root-INFO: Learning rate of xt decay: 0.48326 -> 0.48906.
2024-12-02-05:25:10-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-05:25:10-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-05:25:10-root-INFO: grad norm: 2.011 1.992 0.275
2024-12-02-05:25:10-root-INFO: Loss too large (5.293->5.370)! Learning rate decreased to 0.38303.
2024-12-02-05:25:11-root-INFO: grad norm: 1.354 1.346 0.146
2024-12-02-05:25:11-root-INFO: grad norm: 1.008 1.005 0.076
2024-12-02-05:25:12-root-INFO: grad norm: 0.924 0.920 0.084
2024-12-02-05:25:12-root-INFO: grad norm: 0.894 0.892 0.065
2024-12-02-05:25:13-root-INFO: Loss Change: 5.293 -> 4.583
2024-12-02-05:25:13-root-INFO: Regularization Change: 0.000 -> 1.141
2024-12-02-05:25:13-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-05:25:13-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-05:25:13-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-05:25:13-root-INFO: grad norm: 1.094 1.076 0.198
2024-12-02-05:25:13-root-INFO: grad norm: 0.998 0.988 0.141
2024-12-02-05:25:14-root-INFO: grad norm: 1.170 1.166 0.104
2024-12-02-05:25:14-root-INFO: Loss too large (4.289->4.344)! Learning rate decreased to 0.38861.
2024-12-02-05:25:14-root-INFO: grad norm: 1.359 1.358 0.070
2024-12-02-05:25:15-root-INFO: grad norm: 0.943 0.941 0.060
2024-12-02-05:25:15-root-INFO: Loss Change: 4.516 -> 4.039
2024-12-02-05:25:15-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-02-05:25:15-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-05:25:15-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-05:25:15-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-05:25:16-root-INFO: grad norm: 1.466 1.445 0.247
2024-12-02-05:25:16-root-INFO: grad norm: 1.345 1.330 0.202
2024-12-02-05:25:17-root-INFO: grad norm: 1.468 1.456 0.187
2024-12-02-05:25:17-root-INFO: Loss too large (3.888->3.968)! Learning rate decreased to 0.39420.
2024-12-02-05:25:17-root-INFO: grad norm: 1.205 1.195 0.158
2024-12-02-05:25:18-root-INFO: grad norm: 1.110 1.104 0.109
2024-12-02-05:25:18-root-INFO: Loss Change: 4.127 -> 3.645
2024-12-02-05:25:18-root-INFO: Regularization Change: 0.000 -> 1.002
2024-12-02-05:25:18-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-05:25:18-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-05:25:18-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-05:25:18-root-INFO: grad norm: 1.015 1.005 0.136
2024-12-02-05:25:19-root-INFO: grad norm: 0.814 0.810 0.080
2024-12-02-05:25:19-root-INFO: grad norm: 0.900 0.898 0.060
2024-12-02-05:25:20-root-INFO: grad norm: 1.287 1.285 0.060
2024-12-02-05:25:20-root-INFO: Loss too large (3.351->3.394)! Learning rate decreased to 0.39982.
2024-12-02-05:25:20-root-INFO: grad norm: 0.791 0.789 0.058
2024-12-02-05:25:21-root-INFO: Loss Change: 3.617 -> 3.184
2024-12-02-05:25:21-root-INFO: Regularization Change: 0.000 -> 0.830
2024-12-02-05:25:21-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-05:25:21-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-05:25:21-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-05:25:21-root-INFO: grad norm: 1.301 1.278 0.238
2024-12-02-05:25:21-root-INFO: grad norm: 1.087 1.073 0.176
2024-12-02-05:25:22-root-INFO: grad norm: 1.059 1.051 0.134
2024-12-02-05:25:22-root-INFO: grad norm: 1.186 1.178 0.133
2024-12-02-05:25:23-root-INFO: grad norm: 1.060 1.055 0.106
2024-12-02-05:25:23-root-INFO: Loss Change: 3.321 -> 2.921
2024-12-02-05:25:23-root-INFO: Regularization Change: 0.000 -> 1.035
2024-12-02-05:25:23-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-05:25:23-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-05:25:23-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-05:25:23-root-INFO: grad norm: 1.004 0.994 0.138
2024-12-02-05:25:24-root-INFO: grad norm: 0.798 0.794 0.082
2024-12-02-05:25:24-root-INFO: grad norm: 1.094 1.091 0.083
2024-12-02-05:25:25-root-INFO: Loss too large (2.718->2.734)! Learning rate decreased to 0.41109.
2024-12-02-05:25:25-root-INFO: grad norm: 0.706 0.703 0.062
2024-12-02-05:25:26-root-INFO: grad norm: 0.432 0.431 0.028
2024-12-02-05:25:26-root-INFO: Loss Change: 2.932 -> 2.497
2024-12-02-05:25:26-root-INFO: Regularization Change: 0.000 -> 0.736
2024-12-02-05:25:26-root-INFO: Undo step: 10
2024-12-02-05:25:26-root-INFO: Undo step: 11
2024-12-02-05:25:26-root-INFO: Undo step: 12
2024-12-02-05:25:26-root-INFO: Undo step: 13
2024-12-02-05:25:26-root-INFO: Undo step: 14
2024-12-02-05:25:26-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-05:25:26-root-INFO: grad norm: 12.739 12.713 0.812
2024-12-02-05:25:27-root-INFO: grad norm: 5.070 5.051 0.438
2024-12-02-05:25:27-root-INFO: grad norm: 3.346 3.331 0.310
2024-12-02-05:25:28-root-INFO: grad norm: 2.708 2.698 0.233
2024-12-02-05:25:28-root-INFO: grad norm: 2.376 2.367 0.209
2024-12-02-05:25:28-root-INFO: Loss Change: 66.942 -> 10.153
2024-12-02-05:25:28-root-INFO: Regularization Change: 0.000 -> 93.031
2024-12-02-05:25:28-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-05:25:28-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-05:25:29-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-05:25:29-root-INFO: grad norm: 2.489 2.477 0.244
2024-12-02-05:25:29-root-INFO: grad norm: 2.249 2.239 0.215
2024-12-02-05:25:30-root-INFO: grad norm: 1.814 1.808 0.142
2024-12-02-05:25:30-root-INFO: grad norm: 1.697 1.690 0.156
2024-12-02-05:25:31-root-INFO: grad norm: 1.668 1.664 0.120
2024-12-02-05:25:31-root-INFO: Loss Change: 9.824 -> 6.231
2024-12-02-05:25:31-root-INFO: Regularization Change: 0.000 -> 8.883
2024-12-02-05:25:31-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-05:25:31-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-05:25:31-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-05:25:31-root-INFO: grad norm: 1.731 1.720 0.194
2024-12-02-05:25:32-root-INFO: grad norm: 1.680 1.671 0.174
2024-12-02-05:25:32-root-INFO: grad norm: 1.779 1.772 0.155
2024-12-02-05:25:33-root-INFO: grad norm: 1.695 1.688 0.161
2024-12-02-05:25:33-root-INFO: grad norm: 1.624 1.619 0.126
2024-12-02-05:25:34-root-INFO: Loss Change: 5.957 -> 4.626
2024-12-02-05:25:34-root-INFO: Regularization Change: 0.000 -> 3.744
2024-12-02-05:25:34-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-05:25:34-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-05:25:34-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-05:25:34-root-INFO: grad norm: 1.794 1.788 0.148
2024-12-02-05:25:34-root-INFO: grad norm: 1.579 1.574 0.119
2024-12-02-05:25:35-root-INFO: grad norm: 1.512 1.510 0.088
2024-12-02-05:25:35-root-INFO: grad norm: 1.259 1.255 0.094
2024-12-02-05:25:36-root-INFO: grad norm: 1.206 1.205 0.066
2024-12-02-05:25:36-root-INFO: Loss Change: 4.506 -> 3.606
2024-12-02-05:25:36-root-INFO: Regularization Change: 0.000 -> 2.171
2024-12-02-05:25:36-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-05:25:36-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-05:25:36-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-05:25:36-root-INFO: grad norm: 1.317 1.303 0.194
2024-12-02-05:25:37-root-INFO: grad norm: 1.155 1.145 0.154
2024-12-02-05:25:37-root-INFO: grad norm: 1.283 1.275 0.141
2024-12-02-05:25:37-root-INFO: Loss too large (3.272->3.278)! Learning rate decreased to 0.40545.
2024-12-02-05:25:38-root-INFO: grad norm: 0.924 0.918 0.112
2024-12-02-05:25:38-root-INFO: grad norm: 0.625 0.621 0.063
2024-12-02-05:25:39-root-INFO: Loss Change: 3.543 -> 2.951
2024-12-02-05:25:39-root-INFO: Regularization Change: 0.000 -> 1.059
2024-12-02-05:25:39-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-05:25:39-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-05:25:39-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-05:25:39-root-INFO: grad norm: 1.023 1.005 0.190
2024-12-02-05:25:39-root-INFO: grad norm: 0.766 0.758 0.111
2024-12-02-05:25:40-root-INFO: grad norm: 0.739 0.736 0.073
2024-12-02-05:25:40-root-INFO: grad norm: 0.987 0.986 0.057
2024-12-02-05:25:41-root-INFO: Loss too large (2.711->2.720)! Learning rate decreased to 0.41109.
2024-12-02-05:25:41-root-INFO: grad norm: 0.715 0.714 0.036
2024-12-02-05:25:41-root-INFO: Loss Change: 2.986 -> 2.558
2024-12-02-05:25:41-root-INFO: Regularization Change: 0.000 -> 0.849
2024-12-02-05:25:41-root-INFO: Undo step: 10
2024-12-02-05:25:41-root-INFO: Undo step: 11
2024-12-02-05:25:41-root-INFO: Undo step: 12
2024-12-02-05:25:41-root-INFO: Undo step: 13
2024-12-02-05:25:41-root-INFO: Undo step: 14
2024-12-02-05:25:41-root-INFO: step: 15 lr_xt 0.47878275
2024-12-02-05:25:42-root-INFO: grad norm: 12.987 12.954 0.914
2024-12-02-05:25:42-root-INFO: grad norm: 5.645 5.622 0.515
2024-12-02-05:25:43-root-INFO: grad norm: 3.908 3.888 0.388
2024-12-02-05:25:43-root-INFO: grad norm: 3.039 3.028 0.261
2024-12-02-05:25:44-root-INFO: grad norm: 2.674 2.662 0.254
2024-12-02-05:25:44-root-INFO: Loss Change: 70.569 -> 10.747
2024-12-02-05:25:44-root-INFO: Regularization Change: 0.000 -> 98.780
2024-12-02-05:25:44-root-INFO: Learning rate of xt decay: 0.48906 -> 0.49493.
2024-12-02-05:25:44-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-05:25:44-root-INFO: step: 14 lr_xt 0.48575663
2024-12-02-05:25:44-root-INFO: grad norm: 2.916 2.898 0.327
2024-12-02-05:25:45-root-INFO: grad norm: 2.609 2.589 0.321
2024-12-02-05:25:45-root-INFO: grad norm: 2.375 2.362 0.247
2024-12-02-05:25:46-root-INFO: grad norm: 2.275 2.258 0.278
2024-12-02-05:25:46-root-INFO: grad norm: 2.265 2.253 0.235
2024-12-02-05:25:46-root-INFO: Loss Change: 10.447 -> 6.759
2024-12-02-05:25:46-root-INFO: Regularization Change: 0.000 -> 10.010
2024-12-02-05:25:46-root-INFO: Learning rate of xt decay: 0.49493 -> 0.50087.
2024-12-02-05:25:46-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-05:25:47-root-INFO: step: 13 lr_xt 0.49275347
2024-12-02-05:25:47-root-INFO: grad norm: 2.055 2.049 0.148
2024-12-02-05:25:47-root-INFO: grad norm: 1.779 1.774 0.125
2024-12-02-05:25:48-root-INFO: grad norm: 1.709 1.701 0.171
2024-12-02-05:25:48-root-INFO: grad norm: 1.894 1.884 0.190
2024-12-02-05:25:49-root-INFO: grad norm: 1.826 1.813 0.224
2024-12-02-05:25:49-root-INFO: Loss Change: 6.281 -> 4.756
2024-12-02-05:25:49-root-INFO: Regularization Change: 0.000 -> 4.184
2024-12-02-05:25:49-root-INFO: Learning rate of xt decay: 0.50087 -> 0.50688.
2024-12-02-05:25:49-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-05:25:49-root-INFO: step: 12 lr_xt 0.49977135
2024-12-02-05:25:49-root-INFO: grad norm: 2.409 2.378 0.382
2024-12-02-05:25:50-root-INFO: grad norm: 2.119 2.091 0.345
2024-12-02-05:25:50-root-INFO: grad norm: 2.081 2.061 0.283
2024-12-02-05:25:51-root-INFO: grad norm: 1.781 1.759 0.282
2024-12-02-05:25:51-root-INFO: grad norm: 1.642 1.627 0.222
2024-12-02-05:25:51-root-INFO: Loss Change: 4.862 -> 3.824
2024-12-02-05:25:51-root-INFO: Regularization Change: 0.000 -> 2.738
2024-12-02-05:25:51-root-INFO: Learning rate of xt decay: 0.50688 -> 0.51296.
2024-12-02-05:25:51-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-05:25:52-root-INFO: step: 11 lr_xt 0.50680833
2024-12-02-05:25:52-root-INFO: grad norm: 1.253 1.248 0.109
2024-12-02-05:25:52-root-INFO: grad norm: 1.211 1.205 0.119
2024-12-02-05:25:53-root-INFO: grad norm: 1.199 1.190 0.144
2024-12-02-05:25:53-root-INFO: grad norm: 1.197 1.189 0.143
2024-12-02-05:25:54-root-INFO: grad norm: 1.289 1.277 0.174
2024-12-02-05:25:54-root-INFO: Loss Change: 3.547 -> 3.111
2024-12-02-05:25:54-root-INFO: Regularization Change: 0.000 -> 1.525
2024-12-02-05:25:54-root-INFO: Learning rate of xt decay: 0.51296 -> 0.51912.
2024-12-02-05:25:54-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-05:25:54-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-05:25:54-root-INFO: grad norm: 2.265 2.229 0.407
2024-12-02-05:25:55-root-INFO: grad norm: 1.760 1.738 0.279
2024-12-02-05:25:55-root-INFO: grad norm: 1.412 1.400 0.184
2024-12-02-05:25:56-root-INFO: grad norm: 1.216 1.202 0.189
2024-12-02-05:25:56-root-INFO: grad norm: 1.132 1.121 0.163
2024-12-02-05:25:56-root-INFO: Loss Change: 3.394 -> 2.650
2024-12-02-05:25:56-root-INFO: Regularization Change: 0.000 -> 1.481
2024-12-02-05:25:56-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-05:25:56-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-05:25:57-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-05:25:57-root-INFO: grad norm: 0.981 0.978 0.075
2024-12-02-05:25:57-root-INFO: grad norm: 0.995 0.993 0.068
2024-12-02-05:25:58-root-INFO: grad norm: 0.829 0.826 0.077
2024-12-02-05:25:58-root-INFO: grad norm: 0.707 0.703 0.075
2024-12-02-05:25:59-root-INFO: grad norm: 0.750 0.745 0.086
2024-12-02-05:25:59-root-INFO: Loss too large (2.289->2.290)! Learning rate decreased to 0.41675.
2024-12-02-05:25:59-root-INFO: Loss Change: 2.563 -> 2.243
2024-12-02-05:25:59-root-INFO: Regularization Change: 0.000 -> 0.728
2024-12-02-05:25:59-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-05:25:59-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-05:25:59-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-05:26:00-root-INFO: grad norm: 1.346 1.322 0.253
2024-12-02-05:26:00-root-INFO: grad norm: 1.106 1.094 0.162
2024-12-02-05:26:01-root-INFO: grad norm: 0.952 0.943 0.133
2024-12-02-05:26:01-root-INFO: grad norm: 0.993 0.985 0.125
2024-12-02-05:26:02-root-INFO: grad norm: 1.055 1.048 0.119
2024-12-02-05:26:02-root-INFO: Loss Change: 2.441 -> 2.152
2024-12-02-05:26:02-root-INFO: Regularization Change: 0.000 -> 0.913
2024-12-02-05:26:02-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-05:26:02-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-05:26:02-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-05:26:02-root-INFO: grad norm: 0.986 0.982 0.089
2024-12-02-05:26:03-root-INFO: grad norm: 0.669 0.668 0.038
2024-12-02-05:26:03-root-INFO: grad norm: 0.695 0.694 0.044
2024-12-02-05:26:04-root-INFO: grad norm: 0.780 0.778 0.056
2024-12-02-05:26:04-root-INFO: grad norm: 0.778 0.775 0.064
2024-12-02-05:26:04-root-INFO: Loss Change: 2.191 -> 1.909
2024-12-02-05:26:04-root-INFO: Regularization Change: 0.000 -> 0.728
2024-12-02-05:26:04-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-05:26:04-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-05:26:05-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-05:26:05-root-INFO: grad norm: 1.376 1.355 0.240
2024-12-02-05:26:05-root-INFO: grad norm: 1.031 1.024 0.113
2024-12-02-05:26:06-root-INFO: grad norm: 0.930 0.927 0.075
2024-12-02-05:26:06-root-INFO: grad norm: 0.742 0.738 0.070
2024-12-02-05:26:07-root-INFO: grad norm: 0.757 0.754 0.065
2024-12-02-05:26:07-root-INFO: Loss Change: 2.133 -> 1.752
2024-12-02-05:26:07-root-INFO: Regularization Change: 0.000 -> 0.766
2024-12-02-05:26:07-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-05:26:07-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-05:26:07-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-05:26:07-root-INFO: grad norm: 0.869 0.862 0.107
2024-12-02-05:26:08-root-INFO: grad norm: 0.523 0.521 0.033
2024-12-02-05:26:08-root-INFO: grad norm: 0.416 0.414 0.033
2024-12-02-05:26:09-root-INFO: grad norm: 0.378 0.377 0.029
2024-12-02-05:26:09-root-INFO: grad norm: 0.374 0.373 0.032
2024-12-02-05:26:09-root-INFO: Loss Change: 1.877 -> 1.602
2024-12-02-05:26:09-root-INFO: Regularization Change: 0.000 -> 0.568
2024-12-02-05:26:09-root-INFO: Undo step: 5
2024-12-02-05:26:09-root-INFO: Undo step: 6
2024-12-02-05:26:09-root-INFO: Undo step: 7
2024-12-02-05:26:09-root-INFO: Undo step: 8
2024-12-02-05:26:09-root-INFO: Undo step: 9
2024-12-02-05:26:10-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-05:26:10-root-INFO: grad norm: 12.210 12.195 0.605
2024-12-02-05:26:10-root-INFO: grad norm: 4.594 4.583 0.319
2024-12-02-05:26:11-root-INFO: grad norm: 3.004 2.996 0.216
2024-12-02-05:26:11-root-INFO: grad norm: 2.073 2.068 0.145
2024-12-02-05:26:12-root-INFO: grad norm: 1.645 1.640 0.125
2024-12-02-05:26:12-root-INFO: Loss Change: 55.366 -> 6.115
2024-12-02-05:26:12-root-INFO: Regularization Change: 0.000 -> 86.503
2024-12-02-05:26:12-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-05:26:12-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-05:26:12-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-05:26:12-root-INFO: grad norm: 1.805 1.795 0.185
2024-12-02-05:26:13-root-INFO: grad norm: 1.445 1.434 0.177
2024-12-02-05:26:13-root-INFO: grad norm: 1.257 1.253 0.099
2024-12-02-05:26:14-root-INFO: grad norm: 1.180 1.175 0.113
2024-12-02-05:26:14-root-INFO: grad norm: 1.073 1.070 0.071
2024-12-02-05:26:15-root-INFO: Loss Change: 5.862 -> 3.487
2024-12-02-05:26:15-root-INFO: Regularization Change: 0.000 -> 6.092
2024-12-02-05:26:15-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-05:26:15-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-05:26:15-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-05:26:15-root-INFO: grad norm: 1.226 1.218 0.140
2024-12-02-05:26:15-root-INFO: grad norm: 0.975 0.969 0.106
2024-12-02-05:26:16-root-INFO: grad norm: 0.935 0.933 0.067
2024-12-02-05:26:16-root-INFO: grad norm: 0.866 0.861 0.090
2024-12-02-05:26:17-root-INFO: grad norm: 0.933 0.930 0.078
2024-12-02-05:26:17-root-INFO: Loss Change: 3.365 -> 2.570
2024-12-02-05:26:17-root-INFO: Regularization Change: 0.000 -> 2.228
2024-12-02-05:26:17-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-05:26:17-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-05:26:17-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-05:26:17-root-INFO: grad norm: 1.029 1.023 0.103
2024-12-02-05:26:18-root-INFO: grad norm: 0.954 0.953 0.047
2024-12-02-05:26:18-root-INFO: grad norm: 0.667 0.666 0.037
2024-12-02-05:26:19-root-INFO: grad norm: 0.733 0.731 0.041
2024-12-02-05:26:19-root-INFO: grad norm: 0.743 0.740 0.062
2024-12-02-05:26:20-root-INFO: Loss Change: 2.531 -> 2.078
2024-12-02-05:26:20-root-INFO: Regularization Change: 0.000 -> 1.214
2024-12-02-05:26:20-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-05:26:20-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-05:26:20-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-05:26:20-root-INFO: grad norm: 1.255 1.232 0.243
2024-12-02-05:26:20-root-INFO: grad norm: 1.007 0.999 0.127
2024-12-02-05:26:21-root-INFO: grad norm: 0.881 0.879 0.067
2024-12-02-05:26:21-root-INFO: grad norm: 0.711 0.707 0.076
2024-12-02-05:26:22-root-INFO: grad norm: 0.708 0.703 0.084
2024-12-02-05:26:22-root-INFO: Loss Change: 2.202 -> 1.799
2024-12-02-05:26:22-root-INFO: Regularization Change: 0.000 -> 0.898
2024-12-02-05:26:22-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-05:26:22-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-05:26:22-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-05:26:22-root-INFO: grad norm: 0.838 0.832 0.099
2024-12-02-05:26:23-root-INFO: grad norm: 0.518 0.517 0.039
2024-12-02-05:26:23-root-INFO: grad norm: 0.411 0.410 0.029
2024-12-02-05:26:24-root-INFO: grad norm: 0.401 0.398 0.046
2024-12-02-05:26:24-root-INFO: grad norm: 0.406 0.403 0.044
2024-12-02-05:26:25-root-INFO: Loss Change: 1.879 -> 1.608
2024-12-02-05:26:25-root-INFO: Regularization Change: 0.000 -> 0.599
2024-12-02-05:26:25-root-INFO: Undo step: 5
2024-12-02-05:26:25-root-INFO: Undo step: 6
2024-12-02-05:26:25-root-INFO: Undo step: 7
2024-12-02-05:26:25-root-INFO: Undo step: 8
2024-12-02-05:26:25-root-INFO: Undo step: 9
2024-12-02-05:26:25-root-INFO: step: 10 lr_xt 0.51386241
2024-12-02-05:26:25-root-INFO: grad norm: 12.881 12.865 0.642
2024-12-02-05:26:25-root-INFO: grad norm: 4.814 4.795 0.431
2024-12-02-05:26:26-root-INFO: grad norm: 3.313 3.305 0.233
2024-12-02-05:26:26-root-INFO: grad norm: 2.226 2.215 0.217
2024-12-02-05:26:27-root-INFO: grad norm: 1.806 1.801 0.130
2024-12-02-05:26:27-root-INFO: Loss Change: 58.235 -> 6.334
2024-12-02-05:26:27-root-INFO: Regularization Change: 0.000 -> 92.035
2024-12-02-05:26:27-root-INFO: Learning rate of xt decay: 0.51912 -> 0.52534.
2024-12-02-05:26:27-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-05:26:27-root-INFO: step: 9 lr_xt 0.52093157
2024-12-02-05:26:27-root-INFO: grad norm: 1.844 1.839 0.137
2024-12-02-05:26:28-root-INFO: grad norm: 1.585 1.578 0.139
2024-12-02-05:26:28-root-INFO: grad norm: 1.332 1.329 0.080
2024-12-02-05:26:29-root-INFO: grad norm: 1.173 1.169 0.088
2024-12-02-05:26:29-root-INFO: grad norm: 1.168 1.166 0.072
2024-12-02-05:26:30-root-INFO: Loss Change: 5.996 -> 3.599
2024-12-02-05:26:30-root-INFO: Regularization Change: 0.000 -> 6.433
2024-12-02-05:26:30-root-INFO: Learning rate of xt decay: 0.52534 -> 0.53165.
2024-12-02-05:26:30-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-05:26:30-root-INFO: step: 8 lr_xt 0.52801377
2024-12-02-05:26:30-root-INFO: grad norm: 1.689 1.677 0.199
2024-12-02-05:26:30-root-INFO: grad norm: 1.180 1.172 0.137
2024-12-02-05:26:31-root-INFO: grad norm: 1.003 1.000 0.083
2024-12-02-05:26:31-root-INFO: grad norm: 0.865 0.860 0.093
2024-12-02-05:26:32-root-INFO: grad norm: 0.832 0.828 0.076
2024-12-02-05:26:32-root-INFO: Loss Change: 3.506 -> 2.536
2024-12-02-05:26:32-root-INFO: Regularization Change: 0.000 -> 2.319
2024-12-02-05:26:32-root-INFO: Learning rate of xt decay: 0.53165 -> 0.53803.
2024-12-02-05:26:32-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-05:26:32-root-INFO: step: 7 lr_xt 0.53510690
2024-12-02-05:26:32-root-INFO: grad norm: 0.985 0.980 0.103
2024-12-02-05:26:33-root-INFO: grad norm: 0.803 0.801 0.052
2024-12-02-05:26:33-root-INFO: grad norm: 0.852 0.850 0.051
2024-12-02-05:26:34-root-INFO: grad norm: 0.967 0.965 0.068
2024-12-02-05:26:34-root-INFO: grad norm: 0.910 0.908 0.067
2024-12-02-05:26:35-root-INFO: Loss Change: 2.472 -> 2.078
2024-12-02-05:26:35-root-INFO: Regularization Change: 0.000 -> 1.139
2024-12-02-05:26:35-root-INFO: Learning rate of xt decay: 0.53803 -> 0.54448.
2024-12-02-05:26:35-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-05:26:35-root-INFO: step: 6 lr_xt 0.54220886
2024-12-02-05:26:35-root-INFO: grad norm: 1.326 1.308 0.218
2024-12-02-05:26:35-root-INFO: grad norm: 1.068 1.063 0.105
2024-12-02-05:26:36-root-INFO: grad norm: 0.937 0.935 0.065
2024-12-02-05:26:36-root-INFO: grad norm: 0.763 0.760 0.070
2024-12-02-05:26:37-root-INFO: grad norm: 0.749 0.746 0.067
2024-12-02-05:26:37-root-INFO: Loss Change: 2.201 -> 1.794
2024-12-02-05:26:37-root-INFO: Regularization Change: 0.000 -> 0.889
2024-12-02-05:26:37-root-INFO: Learning rate of xt decay: 0.54448 -> 0.55102.
2024-12-02-05:26:37-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-05:26:37-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-05:26:37-root-INFO: grad norm: 0.852 0.846 0.102
2024-12-02-05:26:38-root-INFO: grad norm: 0.594 0.593 0.046
2024-12-02-05:26:38-root-INFO: grad norm: 0.512 0.511 0.038
2024-12-02-05:26:39-root-INFO: grad norm: 0.470 0.468 0.038
2024-12-02-05:26:39-root-INFO: grad norm: 0.468 0.466 0.038
2024-12-02-05:26:39-root-INFO: Loss Change: 1.866 -> 1.603
2024-12-02-05:26:39-root-INFO: Regularization Change: 0.000 -> 0.594
2024-12-02-05:26:39-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-05:26:39-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-05:26:40-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-05:26:40-root-INFO: grad norm: 1.110 1.095 0.178
2024-12-02-05:26:40-root-INFO: grad norm: 0.654 0.652 0.052
2024-12-02-05:26:41-root-INFO: grad norm: 0.435 0.434 0.034
2024-12-02-05:26:41-root-INFO: grad norm: 0.353 0.352 0.033
2024-12-02-05:26:42-root-INFO: grad norm: 0.320 0.319 0.026
2024-12-02-05:26:42-root-INFO: Loss Change: 1.772 -> 1.473
2024-12-02-05:26:42-root-INFO: Regularization Change: 0.000 -> 0.543
2024-12-02-05:26:42-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-05:26:42-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-05:26:42-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-05:26:42-root-INFO: grad norm: 0.740 0.734 0.089
2024-12-02-05:26:43-root-INFO: grad norm: 0.355 0.353 0.037
2024-12-02-05:26:43-root-INFO: grad norm: 0.300 0.299 0.032
2024-12-02-05:26:44-root-INFO: grad norm: 0.272 0.270 0.030
2024-12-02-05:26:44-root-INFO: grad norm: 0.273 0.271 0.030
2024-12-02-05:26:44-root-INFO: Loss Change: 1.617 -> 1.388
2024-12-02-05:26:44-root-INFO: Regularization Change: 0.000 -> 0.469
2024-12-02-05:26:44-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-05:26:44-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-05:26:45-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-05:26:45-root-INFO: grad norm: 0.751 0.747 0.081
2024-12-02-05:26:45-root-INFO: grad norm: 0.381 0.378 0.041
2024-12-02-05:26:46-root-INFO: grad norm: 0.405 0.404 0.032
2024-12-02-05:26:46-root-INFO: grad norm: 0.510 0.509 0.031
2024-12-02-05:26:47-root-INFO: grad norm: 0.261 0.259 0.028
2024-12-02-05:26:47-root-INFO: Loss Change: 1.539 -> 1.329
2024-12-02-05:26:47-root-INFO: Regularization Change: 0.000 -> 0.461
2024-12-02-05:26:47-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-05:26:47-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-05:26:47-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-05:26:47-root-INFO: grad norm: 0.741 0.737 0.071
2024-12-02-05:26:48-root-INFO: grad norm: 0.543 0.542 0.031
2024-12-02-05:26:48-root-INFO: grad norm: 0.343 0.342 0.027
2024-12-02-05:26:49-root-INFO: grad norm: 0.331 0.330 0.026
2024-12-02-05:26:49-root-INFO: grad norm: 0.244 0.243 0.023
2024-12-02-05:26:50-root-INFO: Loss Change: 1.466 -> 1.212
2024-12-02-05:26:50-root-INFO: Regularization Change: 0.000 -> 0.520
2024-12-02-05:26:50-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-05:26:50-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-05:26:50-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-05:26:50-root-INFO: grad norm: 0.761 0.757 0.079
2024-12-02-05:26:50-root-INFO: grad norm: 0.421 0.420 0.024
2024-12-02-05:26:51-root-INFO: grad norm: 0.348 0.345 0.044
2024-12-02-05:26:51-root-INFO: grad norm: 0.323 0.319 0.052
2024-12-02-05:26:52-root-INFO: grad norm: 0.311 0.306 0.052
2024-12-02-05:26:52-root-INFO: Loss Change: 1.343 -> 0.861
2024-12-02-05:26:52-root-INFO: Regularization Change: 0.000 -> 1.229
2024-12-02-05:26:52-root-INFO: Undo step: 0
2024-12-02-05:26:52-root-INFO: Undo step: 1
2024-12-02-05:26:52-root-INFO: Undo step: 2
2024-12-02-05:26:52-root-INFO: Undo step: 3
2024-12-02-05:26:52-root-INFO: Undo step: 4
2024-12-02-05:26:52-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-05:26:52-root-INFO: grad norm: 11.681 11.675 0.388
2024-12-02-05:26:53-root-INFO: grad norm: 5.810 5.805 0.254
2024-12-02-05:26:53-root-INFO: grad norm: 3.492 3.491 0.105
2024-12-02-05:26:54-root-INFO: grad norm: 2.534 2.532 0.085
2024-12-02-05:26:54-root-INFO: grad norm: 1.712 1.711 0.058
2024-12-02-05:26:55-root-INFO: Loss Change: 35.934 -> 3.827
2024-12-02-05:26:55-root-INFO: Regularization Change: 0.000 -> 59.022
2024-12-02-05:26:55-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-05:26:55-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-05:26:55-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-05:26:55-root-INFO: grad norm: 1.172 1.168 0.092
2024-12-02-05:26:55-root-INFO: grad norm: 0.833 0.830 0.066
2024-12-02-05:26:56-root-INFO: grad norm: 0.761 0.760 0.039
2024-12-02-05:26:56-root-INFO: grad norm: 1.365 1.364 0.053
2024-12-02-05:26:57-root-INFO: grad norm: 1.350 1.349 0.041
2024-12-02-05:26:57-root-INFO: Loss Change: 3.786 -> 2.413
2024-12-02-05:26:57-root-INFO: Regularization Change: 0.000 -> 3.983
2024-12-02-05:26:57-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-05:26:57-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-05:26:57-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-05:26:58-root-INFO: grad norm: 0.824 0.821 0.069
2024-12-02-05:26:58-root-INFO: grad norm: 2.009 2.009 0.036
2024-12-02-05:26:58-root-INFO: Loss too large (2.246->2.519)! Learning rate decreased to 0.45084.
2024-12-02-05:26:59-root-INFO: grad norm: 1.384 1.383 0.026
2024-12-02-05:26:59-root-INFO: grad norm: 0.338 0.337 0.027
2024-12-02-05:27:00-root-INFO: grad norm: 0.296 0.295 0.026
2024-12-02-05:27:00-root-INFO: Loss Change: 2.454 -> 1.732
2024-12-02-05:27:00-root-INFO: Regularization Change: 0.000 -> 1.277
2024-12-02-05:27:00-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-05:27:00-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-05:27:00-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-05:27:00-root-INFO: grad norm: 0.773 0.768 0.083
2024-12-02-05:27:01-root-INFO: grad norm: 0.483 0.481 0.050
2024-12-02-05:27:01-root-INFO: grad norm: 1.243 1.243 0.036
2024-12-02-05:27:01-root-INFO: Loss too large (1.574->1.655)! Learning rate decreased to 0.45653.
2024-12-02-05:27:01-root-INFO: Loss too large (1.574->1.642)! Learning rate decreased to 0.36522.
2024-12-02-05:27:02-root-INFO: grad norm: 1.850 1.850 0.030
2024-12-02-05:27:02-root-INFO: Loss too large (1.550->1.589)! Learning rate decreased to 0.29218.
2024-12-02-05:27:02-root-INFO: Loss too large (1.550->1.568)! Learning rate decreased to 0.23374.
2024-12-02-05:27:03-root-INFO: grad norm: 1.329 1.329 0.026
2024-12-02-05:27:03-root-INFO: Loss Change: 1.832 -> 1.454
2024-12-02-05:27:03-root-INFO: Regularization Change: 0.000 -> 0.648
2024-12-02-05:27:03-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-05:27:03-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-05:27:03-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-05:27:03-root-INFO: grad norm: 0.715 0.711 0.068
2024-12-02-05:27:04-root-INFO: grad norm: 0.398 0.397 0.037
2024-12-02-05:27:04-root-INFO: grad norm: 0.541 0.541 0.027
2024-12-02-05:27:05-root-INFO: grad norm: 0.285 0.283 0.029
2024-12-02-05:27:05-root-INFO: grad norm: 0.260 0.259 0.022
2024-12-02-05:27:05-root-INFO: Loss Change: 1.556 -> 1.209
2024-12-02-05:27:05-root-INFO: Regularization Change: 0.000 -> 0.708
2024-12-02-05:27:05-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-05:27:05-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-05:27:06-root-INFO: step: 0 lr_xt 0.58488282
2024-12-02-05:27:06-root-INFO: grad norm: 0.752 0.748 0.080
2024-12-02-05:27:06-root-INFO: grad norm: 0.421 0.421 0.024
2024-12-02-05:27:07-root-INFO: grad norm: 0.349 0.346 0.043
2024-12-02-05:27:07-root-INFO: grad norm: 0.321 0.317 0.050
2024-12-02-05:27:08-root-INFO: grad norm: 0.308 0.304 0.051
2024-12-02-05:27:08-root-INFO: Loss Change: 1.321 -> 0.836
2024-12-02-05:27:08-root-INFO: Regularization Change: 0.000 -> 1.241
2024-12-02-05:27:08-root-INFO: Undo step: 0
2024-12-02-05:27:08-root-INFO: Undo step: 1
2024-12-02-05:27:08-root-INFO: Undo step: 2
2024-12-02-05:27:08-root-INFO: Undo step: 3
2024-12-02-05:27:08-root-INFO: Undo step: 4
2024-12-02-05:27:08-root-INFO: step: 5 lr_xt 0.54931747
2024-12-02-05:27:08-root-INFO: grad norm: 13.200 13.192 0.460
2024-12-02-05:27:09-root-INFO: grad norm: 6.939 6.931 0.324
2024-12-02-05:27:09-root-INFO: grad norm: 3.699 3.697 0.109
2024-12-02-05:27:10-root-INFO: grad norm: 2.671 2.668 0.116
2024-12-02-05:27:10-root-INFO: grad norm: 2.217 2.216 0.073
2024-12-02-05:27:11-root-INFO: Loss Change: 37.995 -> 4.035
2024-12-02-05:27:11-root-INFO: Regularization Change: 0.000 -> 62.334
2024-12-02-05:27:11-root-INFO: Learning rate of xt decay: 0.55102 -> 0.55763.
2024-12-02-05:27:11-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-05:27:11-root-INFO: step: 4 lr_xt 0.55643055
2024-12-02-05:27:11-root-INFO: grad norm: 1.464 1.461 0.089
2024-12-02-05:27:11-root-INFO: grad norm: 1.394 1.390 0.104
2024-12-02-05:27:12-root-INFO: grad norm: 1.041 1.039 0.066
2024-12-02-05:27:12-root-INFO: grad norm: 0.923 0.922 0.045
2024-12-02-05:27:13-root-INFO: grad norm: 0.935 0.933 0.055
2024-12-02-05:27:13-root-INFO: Loss Change: 3.947 -> 2.603
2024-12-02-05:27:13-root-INFO: Regularization Change: 0.000 -> 3.004
2024-12-02-05:27:13-root-INFO: Learning rate of xt decay: 0.55763 -> 0.56432.
2024-12-02-05:27:13-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-05:27:13-root-INFO: step: 3 lr_xt 0.56354589
2024-12-02-05:27:13-root-INFO: grad norm: 1.675 1.670 0.137
2024-12-02-05:27:14-root-INFO: grad norm: 0.810 0.807 0.064
2024-12-02-05:27:14-root-INFO: grad norm: 0.527 0.525 0.035
2024-12-02-05:27:15-root-INFO: grad norm: 1.765 1.765 0.035
2024-12-02-05:27:15-root-INFO: Loss too large (2.282->2.631)! Learning rate decreased to 0.45084.
2024-12-02-05:27:15-root-INFO: Loss too large (2.282->2.400)! Learning rate decreased to 0.36067.
2024-12-02-05:27:15-root-INFO: Loss too large (2.282->2.320)! Learning rate decreased to 0.28854.
2024-12-02-05:27:15-root-INFO: Loss too large (2.282->2.317)! Learning rate decreased to 0.23083.
2024-12-02-05:27:16-root-INFO: grad norm: 0.554 0.553 0.032
2024-12-02-05:27:16-root-INFO: Loss Change: 2.748 -> 2.171
2024-12-02-05:27:16-root-INFO: Regularization Change: 0.000 -> 0.904
2024-12-02-05:27:16-root-INFO: Learning rate of xt decay: 0.56432 -> 0.57109.
2024-12-02-05:27:16-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-05:27:16-root-INFO: step: 2 lr_xt 0.57066124
2024-12-02-05:27:17-root-INFO: grad norm: 0.754 0.749 0.086
2024-12-02-05:27:17-root-INFO: grad norm: 0.486 0.484 0.047
2024-12-02-05:27:17-root-INFO: grad norm: 0.983 0.982 0.047
2024-12-02-05:27:18-root-INFO: grad norm: 1.609 1.609 0.046
2024-12-02-05:27:18-root-INFO: Loss too large (1.905->2.176)! Learning rate decreased to 0.45653.
2024-12-02-05:27:18-root-INFO: Loss too large (1.905->2.049)! Learning rate decreased to 0.36522.
2024-12-02-05:27:18-root-INFO: Loss too large (1.905->1.929)! Learning rate decreased to 0.29218.
2024-12-02-05:27:19-root-INFO: grad norm: 0.478 0.477 0.032
2024-12-02-05:27:19-root-INFO: Loss Change: 2.301 -> 1.822
2024-12-02-05:27:19-root-INFO: Regularization Change: 0.000 -> 1.040
2024-12-02-05:27:19-root-INFO: Learning rate of xt decay: 0.57109 -> 0.57795.
2024-12-02-05:27:19-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-05:27:19-root-INFO: step: 1 lr_xt 0.57777431
2024-12-02-05:27:19-root-INFO: grad norm: 0.760 0.756 0.071
2024-12-02-05:27:20-root-INFO: grad norm: 1.234 1.233 0.041
2024-12-02-05:27:20-root-INFO: grad norm: 0.536 0.535 0.037
2024-12-02-05:27:21-root-INFO: grad norm: 1.546 1.546 0.043
2024-12-02-05:27:21-root-INFO: Loss too large (1.431->1.754)! Learning rate decreased to 0.46222.
2024-12-02-05:27:21-root-INFO: Loss too large (1.431->1.559)! Learning rate decreased to 0.36978.
2024-12-02-05:27:21-root-INFO: Loss too large (1.431->1.488)! Learning rate decreased to 0.29582.
2024-12-02-05:27:21-root-INFO: Loss too large (1.431->1.468)! Learning rate decreased to 0.23666.
2024-12-02-05:27:22-root-INFO: grad norm: 0.812 0.812 0.032
2024-12-02-05:27:22-root-INFO: Loss Change: 1.939 -> 1.367
2024-12-02-05:27:22-root-INFO: Regularization Change: 0.000 -> 0.721
2024-12-02-05:27:22-root-INFO: Learning rate of xt decay: 0.57795 -> 0.58488.
2024-12-02-05:27:22-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-05:27:22-root-INFO: loss_sample0_0: 1.3672345876693726
2024-12-02-05:27:23-root-INFO: It takes 4823.966 seconds for image sample0
2024-12-02-05:27:23-root-INFO: lpips_score_sample0: 0.146
2024-12-02-05:27:23-root-INFO: psnr_score_sample0: 18.434
2024-12-02-05:27:23-root-INFO: ssim_score_sample0: 0.734
2024-12-02-05:27:23-root-INFO: mean_lpips: 0.1456514596939087
2024-12-02-05:27:23-root-INFO: best_mean_lpips: 0.1456514596939087
2024-12-02-05:27:23-root-INFO: mean_psnr: 18.434139251708984
2024-12-02-05:27:23-root-INFO: best_mean_psnr: 18.434139251708984
2024-12-02-05:27:23-root-INFO: mean_ssim: 0.7341433167457581
2024-12-02-05:27:23-root-INFO: best_mean_ssim: 0.7341433167457581
2024-12-02-05:27:23-root-INFO: final_loss: 1.3672345876693726
2024-12-02-05:27:23-root-INFO: mean time: 4823.96616435051
2024-12-02-05:27:23-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample3_iter5_lr0.03_10009 
 
Enjoy.
