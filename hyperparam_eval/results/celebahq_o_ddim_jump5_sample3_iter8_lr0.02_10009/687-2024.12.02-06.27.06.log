2024-12-02-06:27:10-root-INFO: Prepare model...
2024-12-02-06:27:26-root-INFO: Loading model from ./checkpoints/celeba256_250000.pt...
2024-12-02-06:27:53-root-INFO: Start sampling
2024-12-02-06:28:00-root-INFO: step: 249 lr_xt 0.00012706
2024-12-02-06:28:00-root-INFO: grad norm: 32196.164 24085.777 21365.119
2024-12-02-06:28:00-root-INFO: grad norm: 23713.457 17716.121 15762.838
2024-12-02-06:28:01-root-INFO: grad norm: 26792.533 21194.936 16389.467
2024-12-02-06:28:01-root-INFO: Loss too large (43812.008->60944.168)! Learning rate decreased to 0.00010.
2024-12-02-06:28:01-root-INFO: Loss too large (43812.008->45476.133)! Learning rate decreased to 0.00008.
2024-12-02-06:28:02-root-INFO: grad norm: 21162.080 16198.188 13618.090
2024-12-02-06:28:02-root-INFO: grad norm: 19143.689 15679.164 10983.835
2024-12-02-06:28:03-root-INFO: grad norm: 17823.953 14000.935 11030.283
2024-12-02-06:28:03-root-INFO: grad norm: 16610.609 13520.236 9649.639
2024-12-02-06:28:04-root-INFO: grad norm: 15349.878 12274.288 9217.408
2024-12-02-06:28:04-root-INFO: Loss Change: 77070.016 -> 21842.510
2024-12-02-06:28:04-root-INFO: Regularization Change: 0.000 -> 14.877
2024-12-02-06:28:04-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-02-06:28:04-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-06:28:04-root-INFO: step: 248 lr_xt 0.00013388
2024-12-02-06:28:04-root-INFO: grad norm: 13707.640 11021.438 8150.296
2024-12-02-06:28:04-root-INFO: Loss too large (22203.248->32156.516)! Learning rate decreased to 0.00011.
2024-12-02-06:28:05-root-INFO: Loss too large (22203.248->25108.826)! Learning rate decreased to 0.00009.
2024-12-02-06:28:05-root-INFO: grad norm: 12005.959 9573.384 7245.231
2024-12-02-06:28:06-root-INFO: grad norm: 10368.490 8292.508 6224.140
2024-12-02-06:28:06-root-INFO: grad norm: 8985.536 7202.448 5372.578
2024-12-02-06:28:07-root-INFO: grad norm: 7667.112 6108.969 4633.044
2024-12-02-06:28:07-root-INFO: grad norm: 6571.370 5282.301 3908.989
2024-12-02-06:28:08-root-INFO: grad norm: 5545.188 4409.447 3362.422
2024-12-02-06:28:08-root-INFO: grad norm: 4704.789 3787.525 2791.001
2024-12-02-06:28:08-root-INFO: Loss Change: 22203.248 -> 16910.057
2024-12-02-06:28:08-root-INFO: Regularization Change: 0.000 -> 1.222
2024-12-02-06:28:08-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-02-06:28:08-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-06:28:09-root-INFO: step: 247 lr_xt 0.00014104
2024-12-02-06:28:09-root-INFO: grad norm: 3928.859 3124.179 2382.317
2024-12-02-06:28:09-root-INFO: Loss too large (16757.598->17452.887)! Learning rate decreased to 0.00011.
2024-12-02-06:28:09-root-INFO: Loss too large (16757.598->16866.281)! Learning rate decreased to 0.00009.
2024-12-02-06:28:10-root-INFO: grad norm: 3143.420 2561.154 1822.520
2024-12-02-06:28:10-root-INFO: grad norm: 2509.376 2011.325 1500.514
2024-12-02-06:28:11-root-INFO: grad norm: 2036.058 1659.503 1179.654
2024-12-02-06:28:11-root-INFO: grad norm: 1668.074 1355.852 971.667
2024-12-02-06:28:12-root-INFO: grad norm: 1396.904 1141.524 805.148
2024-12-02-06:28:12-root-INFO: grad norm: 1192.979 990.861 664.373
2024-12-02-06:28:13-root-INFO: grad norm: 1046.505 862.442 592.761
2024-12-02-06:28:13-root-INFO: Loss Change: 16757.598 -> 15907.397
2024-12-02-06:28:13-root-INFO: Regularization Change: 0.000 -> 0.384
2024-12-02-06:28:13-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-02-06:28:13-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-06:28:13-root-INFO: step: 246 lr_xt 0.00014856
2024-12-02-06:28:13-root-INFO: grad norm: 1025.707 863.513 553.553
2024-12-02-06:28:14-root-INFO: grad norm: 1243.233 1016.590 715.663
2024-12-02-06:28:14-root-INFO: grad norm: 1797.351 1492.948 1000.788
2024-12-02-06:28:14-root-INFO: Loss too large (15567.871->15620.983)! Learning rate decreased to 0.00012.
2024-12-02-06:28:15-root-INFO: grad norm: 2023.723 1621.835 1210.415
2024-12-02-06:28:15-root-INFO: grad norm: 2312.771 1914.581 1297.417
2024-12-02-06:28:16-root-INFO: grad norm: 2646.355 2113.683 1592.338
2024-12-02-06:28:16-root-INFO: grad norm: 3055.244 2513.576 1736.793
2024-12-02-06:28:17-root-INFO: Loss too large (15468.141->15496.453)! Learning rate decreased to 0.00010.
2024-12-02-06:28:17-root-INFO: grad norm: 2304.378 1838.310 1389.522
2024-12-02-06:28:17-root-INFO: Loss Change: 15687.731 -> 15196.213
2024-12-02-06:28:17-root-INFO: Regularization Change: 0.000 -> 0.528
2024-12-02-06:28:17-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-02-06:28:17-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00010.
2024-12-02-06:28:18-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-06:28:18-root-INFO: grad norm: 1695.697 1420.666 925.796
2024-12-02-06:28:18-root-INFO: Loss too large (15077.957->15098.178)! Learning rate decreased to 0.00013.
2024-12-02-06:28:18-root-INFO: grad norm: 1814.710 1444.349 1098.649
2024-12-02-06:28:19-root-INFO: grad norm: 1978.511 1661.534 1074.155
2024-12-02-06:28:19-root-INFO: grad norm: 2165.358 1723.174 1311.278
2024-12-02-06:28:20-root-INFO: grad norm: 2385.980 1992.260 1312.935
2024-12-02-06:28:20-root-INFO: grad norm: 2629.012 2094.835 1588.512
2024-12-02-06:28:21-root-INFO: grad norm: 2914.384 2417.837 1627.174
2024-12-02-06:28:21-root-INFO: grad norm: 3212.708 2565.383 1933.985
2024-12-02-06:28:22-root-INFO: Loss Change: 15077.957 -> 14828.211
2024-12-02-06:28:22-root-INFO: Regularization Change: 0.000 -> 0.547
2024-12-02-06:28:22-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-06:28:22-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-06:28:22-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-06:28:22-root-INFO: grad norm: 3660.698 2982.895 2122.039
2024-12-02-06:28:22-root-INFO: Loss too large (14749.490->15194.348)! Learning rate decreased to 0.00013.
2024-12-02-06:28:23-root-INFO: grad norm: 3762.186 3039.487 2217.107
2024-12-02-06:28:23-root-INFO: grad norm: 3951.316 3259.082 2234.117
2024-12-02-06:28:23-root-INFO: Loss too large (14633.969->14639.527)! Learning rate decreased to 0.00011.
2024-12-02-06:28:24-root-INFO: grad norm: 2664.568 2151.220 1572.314
2024-12-02-06:28:24-root-INFO: grad norm: 1815.524 1544.556 954.188
2024-12-02-06:28:25-root-INFO: grad norm: 1324.700 1080.914 765.803
2024-12-02-06:28:25-root-INFO: grad norm: 1032.879 918.687 472.072
2024-12-02-06:28:26-root-INFO: grad norm: 877.767 746.557 461.659
2024-12-02-06:28:26-root-INFO: Loss Change: 14749.490 -> 13815.971
2024-12-02-06:28:26-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-02-06:28:26-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-06:28:26-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:28:26-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-06:28:26-root-INFO: grad norm: 1011.949 835.254 571.308
2024-12-02-06:28:27-root-INFO: grad norm: 976.255 786.328 578.586
2024-12-02-06:28:27-root-INFO: grad norm: 1059.386 889.328 575.669
2024-12-02-06:28:28-root-INFO: grad norm: 1232.271 958.350 774.634
2024-12-02-06:28:28-root-INFO: grad norm: 1535.029 1255.910 882.612
2024-12-02-06:28:29-root-INFO: grad norm: 2008.735 1552.846 1274.240
2024-12-02-06:28:29-root-INFO: Loss too large (13320.125->13353.224)! Learning rate decreased to 0.00014.
2024-12-02-06:28:29-root-INFO: grad norm: 1902.190 1571.628 1071.594
2024-12-02-06:28:30-root-INFO: grad norm: 1855.874 1475.841 1125.239
2024-12-02-06:28:30-root-INFO: Loss Change: 13697.768 -> 13074.190
2024-12-02-06:28:30-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-02-06:28:30-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-06:28:30-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:28:30-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-06:28:31-root-INFO: grad norm: 1940.803 1601.444 1096.400
2024-12-02-06:28:31-root-INFO: Loss too large (12880.695->12895.376)! Learning rate decreased to 0.00015.
2024-12-02-06:28:31-root-INFO: grad norm: 1833.171 1488.058 1070.607
2024-12-02-06:28:32-root-INFO: grad norm: 1777.876 1509.930 938.592
2024-12-02-06:28:32-root-INFO: grad norm: 1751.072 1429.249 1011.682
2024-12-02-06:28:33-root-INFO: grad norm: 1739.847 1489.158 899.707
2024-12-02-06:28:33-root-INFO: grad norm: 1738.644 1425.179 995.865
2024-12-02-06:28:34-root-INFO: grad norm: 1739.818 1490.661 897.161
2024-12-02-06:28:34-root-INFO: grad norm: 1742.716 1433.177 991.496
2024-12-02-06:28:34-root-INFO: Loss Change: 12880.695 -> 12240.325
2024-12-02-06:28:34-root-INFO: Regularization Change: 0.000 -> 0.714
2024-12-02-06:28:34-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-06:28:34-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:28:35-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-06:28:35-root-INFO: grad norm: 1776.165 1512.310 931.494
2024-12-02-06:28:35-root-INFO: Loss too large (12146.499->12161.477)! Learning rate decreased to 0.00015.
2024-12-02-06:28:35-root-INFO: grad norm: 1711.909 1430.945 939.697
2024-12-02-06:28:36-root-INFO: grad norm: 1658.637 1426.035 847.054
2024-12-02-06:28:36-root-INFO: grad norm: 1608.836 1343.337 885.324
2024-12-02-06:28:37-root-INFO: grad norm: 1559.595 1346.119 787.592
2024-12-02-06:28:37-root-INFO: grad norm: 1513.523 1265.727 829.872
2024-12-02-06:28:38-root-INFO: grad norm: 1467.618 1270.667 734.376
2024-12-02-06:28:38-root-INFO: grad norm: 1421.258 1191.801 774.327
2024-12-02-06:28:38-root-INFO: Loss Change: 12146.499 -> 11490.586
2024-12-02-06:28:38-root-INFO: Regularization Change: 0.000 -> 0.749
2024-12-02-06:28:38-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-06:28:38-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:28:39-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-06:28:39-root-INFO: grad norm: 1327.097 1142.792 674.695
2024-12-02-06:28:39-root-INFO: grad norm: 1667.407 1408.103 893.025
2024-12-02-06:28:40-root-INFO: grad norm: 2231.283 1914.378 1146.202
2024-12-02-06:28:40-root-INFO: Loss too large (11227.462->11317.441)! Learning rate decreased to 0.00016.
2024-12-02-06:28:41-root-INFO: grad norm: 2069.220 1738.591 1122.040
2024-12-02-06:28:41-root-INFO: grad norm: 1918.721 1653.172 973.917
2024-12-02-06:28:41-root-INFO: grad norm: 1782.826 1501.670 960.966
2024-12-02-06:28:42-root-INFO: grad norm: 1656.314 1433.883 829.069
2024-12-02-06:28:42-root-INFO: grad norm: 1542.593 1303.648 824.677
2024-12-02-06:28:43-root-INFO: Loss Change: 11297.480 -> 10670.111
2024-12-02-06:28:43-root-INFO: Regularization Change: 0.000 -> 0.879
2024-12-02-06:28:43-root-INFO: Undo step: 240
2024-12-02-06:28:43-root-INFO: Undo step: 241
2024-12-02-06:28:43-root-INFO: Undo step: 242
2024-12-02-06:28:43-root-INFO: Undo step: 243
2024-12-02-06:28:43-root-INFO: Undo step: 244
2024-12-02-06:28:43-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-06:28:43-root-INFO: grad norm: 11561.112 8559.517 7771.358
2024-12-02-06:28:43-root-INFO: Loss too large (18743.070->19526.025)! Learning rate decreased to 0.00013.
2024-12-02-06:28:44-root-INFO: grad norm: 8239.690 6166.508 5465.041
2024-12-02-06:28:44-root-INFO: grad norm: 6541.606 5073.900 4128.940
2024-12-02-06:28:45-root-INFO: grad norm: 5955.782 4599.817 3783.256
2024-12-02-06:28:45-root-INFO: grad norm: 5583.925 4468.024 3349.177
2024-12-02-06:28:46-root-INFO: grad norm: 5502.659 4328.248 3397.872
2024-12-02-06:28:46-root-INFO: grad norm: 5511.283 4476.440 3214.921
2024-12-02-06:28:46-root-INFO: Loss too large (13822.410->13859.518)! Learning rate decreased to 0.00010.
2024-12-02-06:28:47-root-INFO: grad norm: 3595.992 2844.549 2199.931
2024-12-02-06:28:47-root-INFO: Loss Change: 18743.070 -> 12953.198
2024-12-02-06:28:47-root-INFO: Regularization Change: 0.000 -> 1.436
2024-12-02-06:28:47-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-06:28:47-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-06:28:47-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-06:28:48-root-INFO: grad norm: 2565.694 2097.005 1478.295
2024-12-02-06:28:48-root-INFO: Loss too large (12878.979->12936.922)! Learning rate decreased to 0.00013.
2024-12-02-06:28:48-root-INFO: grad norm: 2438.354 1952.320 1460.828
2024-12-02-06:28:49-root-INFO: grad norm: 2416.963 2022.974 1322.607
2024-12-02-06:28:49-root-INFO: grad norm: 2442.868 1951.605 1469.300
2024-12-02-06:28:50-root-INFO: grad norm: 2488.071 2090.548 1349.113
2024-12-02-06:28:50-root-INFO: grad norm: 2542.733 2031.921 1528.656
2024-12-02-06:28:50-root-INFO: grad norm: 2604.320 2185.894 1415.752
2024-12-02-06:28:51-root-INFO: grad norm: 2666.012 2133.652 1598.485
2024-12-02-06:28:51-root-INFO: Loss Change: 12878.979 -> 12244.775
2024-12-02-06:28:51-root-INFO: Regularization Change: 0.000 -> 0.685
2024-12-02-06:28:51-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-06:28:51-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:28:51-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-06:28:52-root-INFO: grad norm: 2472.269 2064.572 1360.021
2024-12-02-06:28:52-root-INFO: Loss too large (11984.188->12088.583)! Learning rate decreased to 0.00014.
2024-12-02-06:28:52-root-INFO: grad norm: 2380.840 1911.274 1419.658
2024-12-02-06:28:53-root-INFO: grad norm: 2338.339 1975.625 1250.893
2024-12-02-06:28:53-root-INFO: grad norm: 2309.665 1855.978 1374.736
2024-12-02-06:28:54-root-INFO: grad norm: 2286.777 1933.871 1220.448
2024-12-02-06:28:54-root-INFO: grad norm: 2266.488 1824.005 1345.353
2024-12-02-06:28:55-root-INFO: grad norm: 2244.375 1896.806 1199.727
2024-12-02-06:28:55-root-INFO: grad norm: 2226.342 1794.942 1317.112
2024-12-02-06:28:56-root-INFO: Loss Change: 11984.188 -> 11392.941
2024-12-02-06:28:56-root-INFO: Regularization Change: 0.000 -> 0.605
2024-12-02-06:28:56-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-06:28:56-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:28:56-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-06:28:56-root-INFO: grad norm: 2172.355 1818.813 1187.874
2024-12-02-06:28:56-root-INFO: Loss too large (11183.242->11251.349)! Learning rate decreased to 0.00015.
2024-12-02-06:28:57-root-INFO: grad norm: 2022.026 1649.046 1170.143
2024-12-02-06:28:57-root-INFO: grad norm: 1905.815 1622.453 999.889
2024-12-02-06:28:58-root-INFO: grad norm: 1812.897 1474.138 1055.231
2024-12-02-06:28:58-root-INFO: grad norm: 1726.244 1478.578 890.913
2024-12-02-06:28:59-root-INFO: grad norm: 1650.504 1343.514 958.715
2024-12-02-06:28:59-root-INFO: grad norm: 1575.697 1354.723 804.703
2024-12-02-06:29:00-root-INFO: grad norm: 1504.696 1227.987 869.573
2024-12-02-06:29:00-root-INFO: Loss Change: 11183.242 -> 10615.441
2024-12-02-06:29:00-root-INFO: Regularization Change: 0.000 -> 0.551
2024-12-02-06:29:00-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-06:29:00-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:00-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-06:29:01-root-INFO: grad norm: 1471.101 1255.401 766.881
2024-12-02-06:29:01-root-INFO: grad norm: 1928.669 1590.379 1091.081
2024-12-02-06:29:01-root-INFO: Loss too large (10513.317->10569.684)! Learning rate decreased to 0.00015.
2024-12-02-06:29:02-root-INFO: grad norm: 1756.573 1499.979 914.120
2024-12-02-06:29:02-root-INFO: grad norm: 1604.174 1323.834 906.000
2024-12-02-06:29:03-root-INFO: grad norm: 1464.469 1261.385 744.028
2024-12-02-06:29:03-root-INFO: grad norm: 1341.095 1110.574 751.772
2024-12-02-06:29:04-root-INFO: grad norm: 1226.498 1067.025 604.779
2024-12-02-06:29:04-root-INFO: grad norm: 1124.762 937.042 622.128
2024-12-02-06:29:05-root-INFO: Loss Change: 10516.628 -> 10067.615
2024-12-02-06:29:05-root-INFO: Regularization Change: 0.000 -> 0.519
2024-12-02-06:29:05-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-06:29:05-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:05-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-06:29:05-root-INFO: grad norm: 956.633 829.522 476.488
2024-12-02-06:29:05-root-INFO: grad norm: 1088.146 917.170 585.543
2024-12-02-06:29:06-root-INFO: grad norm: 1338.734 1164.072 661.171
2024-12-02-06:29:06-root-INFO: grad norm: 1678.913 1394.379 935.125
2024-12-02-06:29:07-root-INFO: Loss too large (9795.482->9817.408)! Learning rate decreased to 0.00016.
2024-12-02-06:29:07-root-INFO: grad norm: 1430.942 1240.374 713.490
2024-12-02-06:29:08-root-INFO: grad norm: 1229.436 1026.602 676.462
2024-12-02-06:29:08-root-INFO: grad norm: 1062.093 935.674 502.550
2024-12-02-06:29:09-root-INFO: grad norm: 928.928 784.385 497.642
2024-12-02-06:29:09-root-INFO: Loss Change: 9888.523 -> 9495.613
2024-12-02-06:29:09-root-INFO: Regularization Change: 0.000 -> 0.540
2024-12-02-06:29:09-root-INFO: Undo step: 240
2024-12-02-06:29:09-root-INFO: Undo step: 241
2024-12-02-06:29:09-root-INFO: Undo step: 242
2024-12-02-06:29:09-root-INFO: Undo step: 243
2024-12-02-06:29:09-root-INFO: Undo step: 244
2024-12-02-06:29:09-root-INFO: step: 245 lr_xt 0.00015646
2024-12-02-06:29:09-root-INFO: grad norm: 15660.080 12724.130 9128.780
2024-12-02-06:29:09-root-INFO: Loss too large (22626.910->22952.492)! Learning rate decreased to 0.00013.
2024-12-02-06:29:10-root-INFO: grad norm: 12086.268 9440.999 7546.216
2024-12-02-06:29:10-root-INFO: grad norm: 10335.776 8495.232 5887.215
2024-12-02-06:29:11-root-INFO: grad norm: 9726.128 7768.062 5852.760
2024-12-02-06:29:11-root-INFO: grad norm: 8952.029 7294.433 5189.420
2024-12-02-06:29:12-root-INFO: grad norm: 8312.371 6683.229 4942.667
2024-12-02-06:29:12-root-INFO: grad norm: 7580.267 6161.274 4415.785
2024-12-02-06:29:13-root-INFO: grad norm: 7069.548 5697.261 4185.658
2024-12-02-06:29:13-root-INFO: Loss Change: 22626.910 -> 12319.963
2024-12-02-06:29:13-root-INFO: Regularization Change: 0.000 -> 3.599
2024-12-02-06:29:13-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-02-06:29:13-root-INFO: Coefficient of regularization decay: 0.00010 -> 0.00011.
2024-12-02-06:29:13-root-INFO: step: 244 lr_xt 0.00016475
2024-12-02-06:29:13-root-INFO: grad norm: 6208.008 5040.925 3623.318
2024-12-02-06:29:14-root-INFO: Loss too large (12231.957->13176.894)! Learning rate decreased to 0.00013.
2024-12-02-06:29:14-root-INFO: grad norm: 5374.859 4372.440 3125.839
2024-12-02-06:29:15-root-INFO: grad norm: 4693.484 3849.523 2685.138
2024-12-02-06:29:15-root-INFO: grad norm: 4281.205 3475.196 2500.345
2024-12-02-06:29:16-root-INFO: grad norm: 3923.759 3239.600 2213.792
2024-12-02-06:29:16-root-INFO: grad norm: 3660.130 2964.578 2146.586
2024-12-02-06:29:17-root-INFO: grad norm: 3417.149 2834.227 1908.943
2024-12-02-06:29:17-root-INFO: grad norm: 3222.719 2608.448 1892.596
2024-12-02-06:29:18-root-INFO: Loss Change: 12231.957 -> 10632.896
2024-12-02-06:29:18-root-INFO: Regularization Change: 0.000 -> 0.650
2024-12-02-06:29:18-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-02-06:29:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:18-root-INFO: step: 243 lr_xt 0.00017345
2024-12-02-06:29:18-root-INFO: grad norm: 2630.055 2186.546 1461.577
2024-12-02-06:29:18-root-INFO: Loss too large (10349.715->10468.050)! Learning rate decreased to 0.00014.
2024-12-02-06:29:19-root-INFO: grad norm: 2363.445 1917.775 1381.307
2024-12-02-06:29:19-root-INFO: grad norm: 2161.448 1828.059 1153.281
2024-12-02-06:29:20-root-INFO: grad norm: 2001.125 1622.843 1170.848
2024-12-02-06:29:20-root-INFO: grad norm: 1855.483 1581.606 970.228
2024-12-02-06:29:21-root-INFO: grad norm: 1732.599 1406.945 1011.139
2024-12-02-06:29:21-root-INFO: grad norm: 1615.206 1385.178 830.766
2024-12-02-06:29:22-root-INFO: grad norm: 1512.888 1232.043 878.009
2024-12-02-06:29:22-root-INFO: Loss Change: 10349.715 -> 9773.483
2024-12-02-06:29:22-root-INFO: Regularization Change: 0.000 -> 0.446
2024-12-02-06:29:22-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-02-06:29:22-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:22-root-INFO: step: 242 lr_xt 0.00018258
2024-12-02-06:29:22-root-INFO: grad norm: 1369.288 1159.757 727.951
2024-12-02-06:29:23-root-INFO: grad norm: 1692.967 1380.026 980.644
2024-12-02-06:29:23-root-INFO: Loss too large (9562.993->9588.487)! Learning rate decreased to 0.00015.
2024-12-02-06:29:24-root-INFO: grad norm: 1500.011 1294.915 757.119
2024-12-02-06:29:24-root-INFO: grad norm: 1345.440 1101.561 772.510
2024-12-02-06:29:25-root-INFO: grad norm: 1207.886 1058.398 582.049
2024-12-02-06:29:25-root-INFO: grad norm: 1091.926 900.103 618.157
2024-12-02-06:29:25-root-INFO: grad norm: 988.023 877.936 453.229
2024-12-02-06:29:26-root-INFO: grad norm: 899.714 749.719 497.400
2024-12-02-06:29:26-root-INFO: Loss Change: 9585.562 -> 9211.048
2024-12-02-06:29:26-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-02-06:29:26-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-02-06:29:26-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:26-root-INFO: step: 241 lr_xt 0.00019216
2024-12-02-06:29:27-root-INFO: grad norm: 847.265 745.390 402.804
2024-12-02-06:29:27-root-INFO: grad norm: 984.108 828.189 531.575
2024-12-02-06:29:28-root-INFO: grad norm: 1217.647 1061.878 595.886
2024-12-02-06:29:28-root-INFO: grad norm: 1533.790 1274.603 853.170
2024-12-02-06:29:28-root-INFO: Loss too large (9053.730->9072.133)! Learning rate decreased to 0.00015.
2024-12-02-06:29:29-root-INFO: grad norm: 1315.770 1141.729 653.992
2024-12-02-06:29:29-root-INFO: grad norm: 1141.819 954.512 626.625
2024-12-02-06:29:30-root-INFO: grad norm: 993.303 876.313 467.680
2024-12-02-06:29:30-root-INFO: grad norm: 874.451 739.426 466.812
2024-12-02-06:29:30-root-INFO: Loss Change: 9117.765 -> 8817.254
2024-12-02-06:29:30-root-INFO: Regularization Change: 0.000 -> 0.399
2024-12-02-06:29:30-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-02-06:29:30-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:31-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-06:29:31-root-INFO: grad norm: 673.207 598.363 308.495
2024-12-02-06:29:31-root-INFO: grad norm: 644.980 562.467 315.643
2024-12-02-06:29:32-root-INFO: grad norm: 723.383 659.646 296.902
2024-12-02-06:29:32-root-INFO: grad norm: 834.448 708.587 440.691
2024-12-02-06:29:33-root-INFO: grad norm: 988.641 877.606 455.211
2024-12-02-06:29:33-root-INFO: grad norm: 1187.727 998.539 643.129
2024-12-02-06:29:34-root-INFO: grad norm: 1451.127 1261.337 717.496
2024-12-02-06:29:34-root-INFO: Loss too large (8491.344->8505.865)! Learning rate decreased to 0.00016.
2024-12-02-06:29:34-root-INFO: grad norm: 1188.416 1000.737 640.982
2024-12-02-06:29:35-root-INFO: Loss Change: 8656.609 -> 8383.441
2024-12-02-06:29:35-root-INFO: Regularization Change: 0.000 -> 0.451
2024-12-02-06:29:35-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-06:29:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:35-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-06:29:35-root-INFO: grad norm: 848.037 750.753 394.382
2024-12-02-06:29:36-root-INFO: grad norm: 968.421 822.969 510.452
2024-12-02-06:29:36-root-INFO: grad norm: 1134.285 991.779 550.433
2024-12-02-06:29:37-root-INFO: grad norm: 1337.761 1131.335 713.923
2024-12-02-06:29:37-root-INFO: grad norm: 1595.631 1380.034 800.965
2024-12-02-06:29:37-root-INFO: Loss too large (8216.048->8236.805)! Learning rate decreased to 0.00017.
2024-12-02-06:29:38-root-INFO: grad norm: 1255.044 1064.223 665.255
2024-12-02-06:29:38-root-INFO: grad norm: 993.941 876.331 469.001
2024-12-02-06:29:39-root-INFO: grad norm: 805.564 692.454 411.632
2024-12-02-06:29:39-root-INFO: Loss Change: 8272.561 -> 8008.175
2024-12-02-06:29:39-root-INFO: Regularization Change: 0.000 -> 0.388
2024-12-02-06:29:39-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-06:29:39-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:39-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-06:29:39-root-INFO: grad norm: 662.079 563.761 347.163
2024-12-02-06:29:40-root-INFO: grad norm: 481.535 426.082 224.344
2024-12-02-06:29:40-root-INFO: grad norm: 459.050 422.999 178.322
2024-12-02-06:29:41-root-INFO: grad norm: 453.391 409.215 195.210
2024-12-02-06:29:41-root-INFO: grad norm: 457.977 429.927 157.818
2024-12-02-06:29:42-root-INFO: grad norm: 472.235 425.198 205.458
2024-12-02-06:29:42-root-INFO: grad norm: 496.583 464.977 174.330
2024-12-02-06:29:43-root-INFO: grad norm: 531.174 471.840 243.951
2024-12-02-06:29:43-root-INFO: Loss Change: 7862.885 -> 7587.507
2024-12-02-06:29:43-root-INFO: Regularization Change: 0.000 -> 0.473
2024-12-02-06:29:43-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-06:29:43-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:43-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-06:29:43-root-INFO: grad norm: 421.200 388.824 161.942
2024-12-02-06:29:44-root-INFO: grad norm: 378.948 357.919 124.481
2024-12-02-06:29:44-root-INFO: grad norm: 367.920 355.055 96.439
2024-12-02-06:29:45-root-INFO: grad norm: 363.346 346.670 108.812
2024-12-02-06:29:45-root-INFO: grad norm: 360.770 349.712 88.636
2024-12-02-06:29:46-root-INFO: grad norm: 359.226 342.372 108.741
2024-12-02-06:29:46-root-INFO: grad norm: 358.855 347.910 87.952
2024-12-02-06:29:47-root-INFO: grad norm: 359.328 340.945 113.459
2024-12-02-06:29:47-root-INFO: Loss Change: 7526.446 -> 7282.526
2024-12-02-06:29:47-root-INFO: Regularization Change: 0.000 -> 0.454
2024-12-02-06:29:47-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-06:29:47-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:47-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-06:29:48-root-INFO: grad norm: 372.216 350.369 125.645
2024-12-02-06:29:48-root-INFO: grad norm: 346.853 331.780 101.139
2024-12-02-06:29:49-root-INFO: grad norm: 340.030 328.011 89.604
2024-12-02-06:29:49-root-INFO: grad norm: 336.234 323.085 93.111
2024-12-02-06:29:50-root-INFO: grad norm: 333.684 323.077 83.462
2024-12-02-06:29:50-root-INFO: grad norm: 331.638 318.118 93.723
2024-12-02-06:29:51-root-INFO: grad norm: 329.838 319.634 81.408
2024-12-02-06:29:51-root-INFO: grad norm: 328.511 314.248 95.748
2024-12-02-06:29:52-root-INFO: Loss Change: 7145.551 -> 6928.117
2024-12-02-06:29:52-root-INFO: Regularization Change: 0.000 -> 0.429
2024-12-02-06:29:52-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-06:29:52-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:29:52-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-06:29:52-root-INFO: grad norm: 507.335 452.238 229.934
2024-12-02-06:29:52-root-INFO: grad norm: 510.891 473.145 192.727
2024-12-02-06:29:53-root-INFO: grad norm: 532.046 473.513 242.607
2024-12-02-06:29:53-root-INFO: grad norm: 558.810 514.316 218.512
2024-12-02-06:29:54-root-INFO: grad norm: 590.369 523.522 272.874
2024-12-02-06:29:55-root-INFO: grad norm: 626.769 571.710 256.879
2024-12-02-06:29:55-root-INFO: grad norm: 667.093 588.929 313.329
2024-12-02-06:29:56-root-INFO: grad norm: 714.509 646.534 304.167
2024-12-02-06:29:56-root-INFO: Loss Change: 6868.771 -> 6692.835
2024-12-02-06:29:56-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-02-06:29:56-root-INFO: Undo step: 235
2024-12-02-06:29:56-root-INFO: Undo step: 236
2024-12-02-06:29:56-root-INFO: Undo step: 237
2024-12-02-06:29:56-root-INFO: Undo step: 238
2024-12-02-06:29:56-root-INFO: Undo step: 239
2024-12-02-06:29:56-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-06:29:56-root-INFO: grad norm: 5119.015 4124.756 3031.617
2024-12-02-06:29:57-root-INFO: grad norm: 3670.376 3088.471 1983.181
2024-12-02-06:29:57-root-INFO: Loss too large (9280.141->9283.680)! Learning rate decreased to 0.00016.
2024-12-02-06:29:57-root-INFO: grad norm: 2720.500 2337.310 1392.157
2024-12-02-06:29:58-root-INFO: grad norm: 2061.715 1747.253 1094.429
2024-12-02-06:29:59-root-INFO: grad norm: 1625.854 1406.479 815.608
2024-12-02-06:29:59-root-INFO: grad norm: 1298.931 1113.395 669.009
2024-12-02-06:30:00-root-INFO: grad norm: 1070.547 932.276 526.244
2024-12-02-06:30:00-root-INFO: grad norm: 897.528 782.663 439.313
2024-12-02-06:30:00-root-INFO: Loss Change: 11056.777 -> 8087.621
2024-12-02-06:30:00-root-INFO: Regularization Change: 0.000 -> 1.895
2024-12-02-06:30:00-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-06:30:00-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:01-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-06:30:01-root-INFO: grad norm: 870.419 752.135 438.089
2024-12-02-06:30:01-root-INFO: grad norm: 934.478 805.431 473.846
2024-12-02-06:30:02-root-INFO: grad norm: 1036.873 893.385 526.278
2024-12-02-06:30:02-root-INFO: grad norm: 1178.992 1015.747 598.567
2024-12-02-06:30:03-root-INFO: grad norm: 1356.578 1161.513 700.851
2024-12-02-06:30:03-root-INFO: grad norm: 1582.609 1360.719 808.142
2024-12-02-06:30:04-root-INFO: Loss too large (7812.582->7815.926)! Learning rate decreased to 0.00017.
2024-12-02-06:30:04-root-INFO: grad norm: 1227.260 1053.597 629.365
2024-12-02-06:30:05-root-INFO: grad norm: 958.974 837.059 467.935
2024-12-02-06:30:05-root-INFO: Loss Change: 8004.953 -> 7619.122
2024-12-02-06:30:05-root-INFO: Regularization Change: 0.000 -> 0.607
2024-12-02-06:30:05-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-06:30:05-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:05-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-06:30:05-root-INFO: grad norm: 1089.603 959.058 517.149
2024-12-02-06:30:06-root-INFO: grad norm: 1079.417 950.270 511.986
2024-12-02-06:30:06-root-INFO: grad norm: 1184.955 1022.881 598.190
2024-12-02-06:30:07-root-INFO: grad norm: 1324.146 1162.633 633.756
2024-12-02-06:30:08-root-INFO: grad norm: 1485.222 1284.472 745.665
2024-12-02-06:30:08-root-INFO: grad norm: 1682.154 1470.458 816.942
2024-12-02-06:30:08-root-INFO: Loss too large (7370.248->7387.493)! Learning rate decreased to 0.00018.
2024-12-02-06:30:09-root-INFO: grad norm: 1232.470 1069.547 612.415
2024-12-02-06:30:09-root-INFO: grad norm: 907.088 803.155 421.603
2024-12-02-06:30:10-root-INFO: Loss Change: 7467.777 -> 7181.356
2024-12-02-06:30:10-root-INFO: Regularization Change: 0.000 -> 0.405
2024-12-02-06:30:10-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-06:30:10-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:10-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-06:30:10-root-INFO: grad norm: 991.692 863.785 487.164
2024-12-02-06:30:10-root-INFO: grad norm: 1053.633 931.469 492.452
2024-12-02-06:30:11-root-INFO: grad norm: 1140.795 996.534 555.278
2024-12-02-06:30:11-root-INFO: grad norm: 1246.603 1099.968 586.593
2024-12-02-06:30:12-root-INFO: grad norm: 1361.256 1190.516 660.068
2024-12-02-06:30:13-root-INFO: grad norm: 1499.191 1319.477 711.727
2024-12-02-06:30:13-root-INFO: Loss too large (7105.276->7112.234)! Learning rate decreased to 0.00019.
2024-12-02-06:30:13-root-INFO: grad norm: 1060.669 931.307 507.627
2024-12-02-06:30:14-root-INFO: grad norm: 755.875 674.314 341.538
2024-12-02-06:30:14-root-INFO: Loss Change: 7175.118 -> 6942.921
2024-12-02-06:30:14-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-06:30:14-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-06:30:14-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:14-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-06:30:14-root-INFO: grad norm: 618.204 556.254 269.737
2024-12-02-06:30:15-root-INFO: grad norm: 644.234 576.230 288.093
2024-12-02-06:30:15-root-INFO: grad norm: 688.392 612.898 313.431
2024-12-02-06:30:16-root-INFO: grad norm: 741.042 660.815 335.360
2024-12-02-06:30:16-root-INFO: grad norm: 799.219 708.605 369.636
2024-12-02-06:30:17-root-INFO: grad norm: 865.275 768.756 397.134
2024-12-02-06:30:17-root-INFO: grad norm: 936.752 827.814 438.438
2024-12-02-06:30:18-root-INFO: grad norm: 1019.036 902.658 472.910
2024-12-02-06:30:18-root-INFO: Loss Change: 6803.391 -> 6676.048
2024-12-02-06:30:18-root-INFO: Regularization Change: 0.000 -> 0.358
2024-12-02-06:30:18-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-06:30:18-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:19-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-06:30:19-root-INFO: grad norm: 1437.460 1273.735 666.251
2024-12-02-06:30:19-root-INFO: grad norm: 1517.565 1350.259 692.680
2024-12-02-06:30:19-root-INFO: Loss too large (6660.201->6664.142)! Learning rate decreased to 0.00021.
2024-12-02-06:30:20-root-INFO: grad norm: 1027.199 913.612 469.523
2024-12-02-06:30:20-root-INFO: grad norm: 699.902 629.829 305.252
2024-12-02-06:30:21-root-INFO: grad norm: 506.603 458.075 216.366
2024-12-02-06:30:21-root-INFO: grad norm: 390.733 361.188 149.048
2024-12-02-06:30:22-root-INFO: grad norm: 328.173 306.128 118.249
2024-12-02-06:30:22-root-INFO: grad norm: 296.143 280.804 94.072
2024-12-02-06:30:23-root-INFO: Loss Change: 6673.181 -> 6412.407
2024-12-02-06:30:23-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-02-06:30:23-root-INFO: Undo step: 235
2024-12-02-06:30:23-root-INFO: Undo step: 236
2024-12-02-06:30:23-root-INFO: Undo step: 237
2024-12-02-06:30:23-root-INFO: Undo step: 238
2024-12-02-06:30:23-root-INFO: Undo step: 239
2024-12-02-06:30:23-root-INFO: step: 240 lr_xt 0.00020221
2024-12-02-06:30:23-root-INFO: grad norm: 13151.409 10186.492 8318.348
2024-12-02-06:30:23-root-INFO: grad norm: 11284.063 9170.802 6574.686
2024-12-02-06:30:24-root-INFO: grad norm: 10713.519 8733.702 6204.992
2024-12-02-06:30:24-root-INFO: Loss too large (14168.125->15990.954)! Learning rate decreased to 0.00016.
2024-12-02-06:30:25-root-INFO: grad norm: 8007.002 6782.600 4255.399
2024-12-02-06:30:25-root-INFO: grad norm: 5886.629 4865.244 3313.882
2024-12-02-06:30:26-root-INFO: grad norm: 4814.245 4083.130 2550.490
2024-12-02-06:30:26-root-INFO: grad norm: 3890.124 3241.871 2150.195
2024-12-02-06:30:27-root-INFO: grad norm: 3224.880 2724.004 1726.168
2024-12-02-06:30:27-root-INFO: Loss Change: 19347.281 -> 8400.253
2024-12-02-06:30:27-root-INFO: Regularization Change: 0.000 -> 4.837
2024-12-02-06:30:27-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-02-06:30:27-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:27-root-INFO: step: 239 lr_xt 0.00021275
2024-12-02-06:30:27-root-INFO: grad norm: 2445.859 2056.046 1324.728
2024-12-02-06:30:27-root-INFO: Loss too large (8285.600->8338.252)! Learning rate decreased to 0.00017.
2024-12-02-06:30:28-root-INFO: grad norm: 1981.647 1679.560 1051.667
2024-12-02-06:30:28-root-INFO: grad norm: 1597.034 1355.346 844.721
2024-12-02-06:30:29-root-INFO: grad norm: 1301.886 1105.312 687.889
2024-12-02-06:30:29-root-INFO: grad norm: 1065.233 917.036 542.003
2024-12-02-06:30:30-root-INFO: grad norm: 889.459 760.367 461.498
2024-12-02-06:30:30-root-INFO: grad norm: 756.055 663.456 362.553
2024-12-02-06:30:31-root-INFO: grad norm: 660.895 572.920 329.461
2024-12-02-06:30:31-root-INFO: Loss Change: 8285.600 -> 7566.937
2024-12-02-06:30:31-root-INFO: Regularization Change: 0.000 -> 0.673
2024-12-02-06:30:31-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-02-06:30:31-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:31-root-INFO: step: 238 lr_xt 0.00022380
2024-12-02-06:30:31-root-INFO: grad norm: 708.370 613.302 354.469
2024-12-02-06:30:32-root-INFO: grad norm: 519.727 454.153 252.708
2024-12-02-06:30:32-root-INFO: grad norm: 484.489 431.366 220.573
2024-12-02-06:30:33-root-INFO: grad norm: 464.056 410.730 215.984
2024-12-02-06:30:33-root-INFO: grad norm: 450.039 408.825 188.142
2024-12-02-06:30:34-root-INFO: grad norm: 441.157 394.135 198.185
2024-12-02-06:30:34-root-INFO: grad norm: 436.679 401.411 171.923
2024-12-02-06:30:35-root-INFO: grad norm: 437.046 391.183 194.898
2024-12-02-06:30:35-root-INFO: Loss Change: 7356.039 -> 7021.411
2024-12-02-06:30:35-root-INFO: Regularization Change: 0.000 -> 0.564
2024-12-02-06:30:35-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-02-06:30:35-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:36-root-INFO: step: 237 lr_xt 0.00023539
2024-12-02-06:30:36-root-INFO: grad norm: 435.662 390.919 192.313
2024-12-02-06:30:36-root-INFO: grad norm: 384.524 357.475 141.669
2024-12-02-06:30:37-root-INFO: grad norm: 372.932 341.164 150.616
2024-12-02-06:30:37-root-INFO: grad norm: 368.574 346.143 126.617
2024-12-02-06:30:37-root-INFO: grad norm: 367.163 336.596 146.669
2024-12-02-06:30:38-root-INFO: grad norm: 367.742 345.478 126.013
2024-12-02-06:30:38-root-INFO: grad norm: 370.217 338.305 150.368
2024-12-02-06:30:39-root-INFO: grad norm: 375.037 351.308 131.283
2024-12-02-06:30:39-root-INFO: Loss Change: 6959.354 -> 6739.548
2024-12-02-06:30:39-root-INFO: Regularization Change: 0.000 -> 0.410
2024-12-02-06:30:39-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-02-06:30:39-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:39-root-INFO: step: 236 lr_xt 0.00024753
2024-12-02-06:30:40-root-INFO: grad norm: 441.830 398.533 190.748
2024-12-02-06:30:40-root-INFO: grad norm: 427.696 391.546 172.093
2024-12-02-06:30:41-root-INFO: grad norm: 430.868 386.608 190.214
2024-12-02-06:30:41-root-INFO: grad norm: 437.016 401.415 172.767
2024-12-02-06:30:42-root-INFO: grad norm: 444.378 398.292 197.068
2024-12-02-06:30:42-root-INFO: grad norm: 454.005 416.007 181.819
2024-12-02-06:30:43-root-INFO: grad norm: 464.896 415.102 209.330
2024-12-02-06:30:43-root-INFO: grad norm: 478.329 436.391 195.861
2024-12-02-06:30:43-root-INFO: Loss Change: 6605.148 -> 6424.959
2024-12-02-06:30:43-root-INFO: Regularization Change: 0.000 -> 0.365
2024-12-02-06:30:43-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-02-06:30:43-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00011.
2024-12-02-06:30:43-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-06:30:44-root-INFO: grad norm: 880.399 772.580 422.163
2024-12-02-06:30:44-root-INFO: grad norm: 895.819 800.131 402.842
2024-12-02-06:30:45-root-INFO: grad norm: 920.759 808.567 440.474
2024-12-02-06:30:45-root-INFO: grad norm: 951.998 849.145 430.410
2024-12-02-06:30:45-root-INFO: grad norm: 984.997 866.226 468.903
2024-12-02-06:30:46-root-INFO: grad norm: 1023.106 910.596 466.435
2024-12-02-06:30:46-root-INFO: grad norm: 1063.784 935.756 505.963
2024-12-02-06:30:47-root-INFO: grad norm: 1109.687 985.743 509.624
2024-12-02-06:30:47-root-INFO: Loss Change: 6386.458 -> 6271.056
2024-12-02-06:30:47-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-02-06:30:47-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-06:30:47-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-06:30:48-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-06:30:48-root-INFO: grad norm: 1246.232 1096.596 592.090
2024-12-02-06:30:48-root-INFO: grad norm: 1274.994 1131.128 588.353
2024-12-02-06:30:49-root-INFO: grad norm: 1309.358 1155.156 616.468
2024-12-02-06:30:49-root-INFO: grad norm: 1347.292 1195.437 621.390
2024-12-02-06:30:50-root-INFO: grad norm: 1383.364 1220.711 650.816
2024-12-02-06:30:50-root-INFO: grad norm: 1424.124 1262.889 658.210
2024-12-02-06:30:50-root-INFO: grad norm: 1464.310 1292.008 689.145
2024-12-02-06:30:51-root-INFO: grad norm: 1508.161 1336.719 698.378
2024-12-02-06:30:51-root-INFO: Loss Change: 6238.848 -> 6157.305
2024-12-02-06:30:51-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-02-06:30:51-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-06:30:51-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:30:51-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-06:30:52-root-INFO: grad norm: 1698.485 1513.352 771.116
2024-12-02-06:30:52-root-INFO: grad norm: 1659.716 1474.077 762.728
2024-12-02-06:30:53-root-INFO: grad norm: 1652.884 1466.689 762.135
2024-12-02-06:30:53-root-INFO: grad norm: 1653.109 1468.988 758.186
2024-12-02-06:30:54-root-INFO: grad norm: 1657.401 1469.983 765.590
2024-12-02-06:30:54-root-INFO: grad norm: 1664.844 1479.312 763.769
2024-12-02-06:30:55-root-INFO: grad norm: 1673.847 1483.998 774.284
2024-12-02-06:30:55-root-INFO: grad norm: 1686.016 1497.881 773.952
2024-12-02-06:30:56-root-INFO: Loss Change: 6178.246 -> 6024.074
2024-12-02-06:30:56-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-02-06:30:56-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-06:30:56-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:30:56-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-06:30:56-root-INFO: grad norm: 2166.392 1920.554 1002.360
2024-12-02-06:30:56-root-INFO: grad norm: 2133.167 1911.266 947.347
2024-12-02-06:30:57-root-INFO: Loss too large (6028.508->6034.324)! Learning rate decreased to 0.00024.
2024-12-02-06:30:57-root-INFO: grad norm: 1341.716 1194.743 610.566
2024-12-02-06:30:58-root-INFO: grad norm: 831.714 751.062 357.287
2024-12-02-06:30:58-root-INFO: grad norm: 553.264 495.672 245.785
2024-12-02-06:30:59-root-INFO: grad norm: 388.069 357.279 151.490
2024-12-02-06:30:59-root-INFO: grad norm: 302.082 278.297 117.491
2024-12-02-06:31:00-root-INFO: grad norm: 257.701 243.474 84.441
2024-12-02-06:31:00-root-INFO: Loss Change: 6095.935 -> 5603.541
2024-12-02-06:31:00-root-INFO: Regularization Change: 0.000 -> 0.345
2024-12-02-06:31:00-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-06:31:00-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:00-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-06:31:00-root-INFO: grad norm: 372.324 339.958 151.837
2024-12-02-06:31:01-root-INFO: grad norm: 373.951 344.600 145.225
2024-12-02-06:31:01-root-INFO: grad norm: 380.659 347.740 154.847
2024-12-02-06:31:02-root-INFO: grad norm: 388.225 356.759 153.106
2024-12-02-06:31:02-root-INFO: grad norm: 396.798 361.527 163.546
2024-12-02-06:31:03-root-INFO: grad norm: 406.104 372.140 162.582
2024-12-02-06:31:03-root-INFO: grad norm: 415.779 378.055 173.053
2024-12-02-06:31:04-root-INFO: grad norm: 425.775 389.140 172.783
2024-12-02-06:31:04-root-INFO: Loss Change: 5558.858 -> 5446.040
2024-12-02-06:31:04-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-02-06:31:04-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-06:31:04-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:04-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-06:31:04-root-INFO: grad norm: 754.050 675.561 334.977
2024-12-02-06:31:05-root-INFO: grad norm: 764.710 692.247 324.924
2024-12-02-06:31:05-root-INFO: grad norm: 779.405 700.977 340.738
2024-12-02-06:31:06-root-INFO: grad norm: 796.471 719.996 340.546
2024-12-02-06:31:06-root-INFO: grad norm: 814.353 732.741 355.333
2024-12-02-06:31:07-root-INFO: grad norm: 834.054 753.026 358.607
2024-12-02-06:31:07-root-INFO: grad norm: 854.172 768.499 372.851
2024-12-02-06:31:08-root-INFO: grad norm: 875.951 790.080 378.239
2024-12-02-06:31:08-root-INFO: Loss Change: 5407.413 -> 5314.646
2024-12-02-06:31:08-root-INFO: Regularization Change: 0.000 -> 0.302
2024-12-02-06:31:08-root-INFO: Undo step: 230
2024-12-02-06:31:08-root-INFO: Undo step: 231
2024-12-02-06:31:08-root-INFO: Undo step: 232
2024-12-02-06:31:08-root-INFO: Undo step: 233
2024-12-02-06:31:08-root-INFO: Undo step: 234
2024-12-02-06:31:08-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-06:31:09-root-INFO: grad norm: 9602.201 7868.280 5503.857
2024-12-02-06:31:09-root-INFO: grad norm: 7084.719 5984.597 3791.812
2024-12-02-06:31:09-root-INFO: Loss too large (10904.893->11021.091)! Learning rate decreased to 0.00021.
2024-12-02-06:31:10-root-INFO: grad norm: 5126.834 4694.796 2059.933
2024-12-02-06:31:10-root-INFO: grad norm: 3840.654 3350.843 1876.826
2024-12-02-06:31:11-root-INFO: grad norm: 3027.342 2765.242 1232.167
2024-12-02-06:31:11-root-INFO: grad norm: 2316.314 2018.455 1136.287
2024-12-02-06:31:12-root-INFO: grad norm: 1805.392 1646.734 740.073
2024-12-02-06:31:12-root-INFO: grad norm: 1385.221 1206.964 679.762
2024-12-02-06:31:12-root-INFO: Loss Change: 17298.926 -> 6434.581
2024-12-02-06:31:12-root-INFO: Regularization Change: 0.000 -> 6.558
2024-12-02-06:31:12-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-06:31:12-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-06:31:12-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-06:31:13-root-INFO: grad norm: 1198.777 1092.650 493.137
2024-12-02-06:31:13-root-INFO: grad norm: 1281.931 1122.736 618.716
2024-12-02-06:31:14-root-INFO: grad norm: 1386.267 1249.547 600.307
2024-12-02-06:31:14-root-INFO: grad norm: 1509.040 1330.410 712.186
2024-12-02-06:31:15-root-INFO: grad norm: 1631.550 1459.077 730.103
2024-12-02-06:31:15-root-INFO: grad norm: 1768.188 1563.327 826.134
2024-12-02-06:31:15-root-INFO: Loss too large (6218.107->6224.748)! Learning rate decreased to 0.00022.
2024-12-02-06:31:16-root-INFO: grad norm: 1224.930 1094.440 550.140
2024-12-02-06:31:16-root-INFO: grad norm: 850.918 754.500 393.434
2024-12-02-06:31:16-root-INFO: Loss Change: 6395.728 -> 5963.399
2024-12-02-06:31:16-root-INFO: Regularization Change: 0.000 -> 0.760
2024-12-02-06:31:16-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-06:31:16-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:17-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-06:31:17-root-INFO: grad norm: 914.965 817.501 410.916
2024-12-02-06:31:17-root-INFO: grad norm: 908.445 803.216 424.401
2024-12-02-06:31:18-root-INFO: grad norm: 931.481 833.502 415.849
2024-12-02-06:31:18-root-INFO: grad norm: 962.625 855.372 441.571
2024-12-02-06:31:19-root-INFO: grad norm: 996.740 890.115 448.538
2024-12-02-06:31:19-root-INFO: grad norm: 1035.063 920.998 472.353
2024-12-02-06:31:20-root-INFO: grad norm: 1074.612 958.212 486.436
2024-12-02-06:31:20-root-INFO: grad norm: 1118.413 995.691 509.360
2024-12-02-06:31:20-root-INFO: Loss Change: 5969.351 -> 5786.426
2024-12-02-06:31:20-root-INFO: Regularization Change: 0.000 -> 0.510
2024-12-02-06:31:20-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-06:31:20-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:21-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-06:31:21-root-INFO: grad norm: 1733.289 1539.667 796.062
2024-12-02-06:31:21-root-INFO: grad norm: 1765.663 1583.267 781.556
2024-12-02-06:31:22-root-INFO: Loss too large (5775.960->5782.493)! Learning rate decreased to 0.00024.
2024-12-02-06:31:22-root-INFO: grad norm: 1148.113 1027.028 513.202
2024-12-02-06:31:23-root-INFO: grad norm: 742.185 669.408 320.518
2024-12-02-06:31:23-root-INFO: grad norm: 508.297 456.583 223.377
2024-12-02-06:31:24-root-INFO: grad norm: 369.826 338.502 148.954
2024-12-02-06:31:24-root-INFO: grad norm: 295.387 270.245 119.254
2024-12-02-06:31:25-root-INFO: grad norm: 257.705 240.291 93.124
2024-12-02-06:31:25-root-INFO: Loss Change: 5806.871 -> 5454.659
2024-12-02-06:31:25-root-INFO: Regularization Change: 0.000 -> 0.310
2024-12-02-06:31:25-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-06:31:25-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:25-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-06:31:25-root-INFO: grad norm: 382.473 347.179 160.477
2024-12-02-06:31:26-root-INFO: grad norm: 381.476 350.900 149.644
2024-12-02-06:31:26-root-INFO: grad norm: 387.704 353.677 158.829
2024-12-02-06:31:27-root-INFO: grad norm: 394.951 362.933 155.775
2024-12-02-06:31:27-root-INFO: grad norm: 403.252 366.772 167.600
2024-12-02-06:31:28-root-INFO: grad norm: 412.533 378.451 164.190
2024-12-02-06:31:28-root-INFO: grad norm: 421.940 382.885 177.294
2024-12-02-06:31:29-root-INFO: grad norm: 431.697 395.377 173.318
2024-12-02-06:31:29-root-INFO: Loss Change: 5382.395 -> 5273.562
2024-12-02-06:31:29-root-INFO: Regularization Change: 0.000 -> 0.288
2024-12-02-06:31:29-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-06:31:29-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:29-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-06:31:29-root-INFO: grad norm: 772.060 692.276 341.806
2024-12-02-06:31:30-root-INFO: grad norm: 777.499 704.473 328.971
2024-12-02-06:31:30-root-INFO: grad norm: 785.282 707.104 341.573
2024-12-02-06:31:31-root-INFO: grad norm: 794.602 720.267 335.569
2024-12-02-06:31:31-root-INFO: grad norm: 804.321 724.090 350.182
2024-12-02-06:31:32-root-INFO: grad norm: 815.157 738.656 344.773
2024-12-02-06:31:32-root-INFO: grad norm: 825.932 743.374 359.942
2024-12-02-06:31:33-root-INFO: grad norm: 837.485 758.561 354.917
2024-12-02-06:31:33-root-INFO: Loss Change: 5235.662 -> 5144.100
2024-12-02-06:31:33-root-INFO: Regularization Change: 0.000 -> 0.274
2024-12-02-06:31:33-root-INFO: Undo step: 230
2024-12-02-06:31:33-root-INFO: Undo step: 231
2024-12-02-06:31:33-root-INFO: Undo step: 232
2024-12-02-06:31:33-root-INFO: Undo step: 233
2024-12-02-06:31:33-root-INFO: Undo step: 234
2024-12-02-06:31:33-root-INFO: step: 235 lr_xt 0.00026027
2024-12-02-06:31:33-root-INFO: grad norm: 2345.176 1749.625 1561.622
2024-12-02-06:31:34-root-INFO: grad norm: 1357.999 1020.565 895.884
2024-12-02-06:31:34-root-INFO: grad norm: 1014.265 781.392 646.652
2024-12-02-06:31:35-root-INFO: grad norm: 795.478 631.523 483.698
2024-12-02-06:31:35-root-INFO: grad norm: 650.193 533.586 371.533
2024-12-02-06:31:36-root-INFO: grad norm: 552.780 470.063 290.872
2024-12-02-06:31:36-root-INFO: grad norm: 491.538 428.210 241.342
2024-12-02-06:31:37-root-INFO: grad norm: 454.883 405.946 205.247
2024-12-02-06:31:37-root-INFO: Loss Change: 7063.256 -> 6089.391
2024-12-02-06:31:37-root-INFO: Regularization Change: 0.000 -> 1.187
2024-12-02-06:31:37-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-02-06:31:37-root-INFO: Coefficient of regularization decay: 0.00011 -> 0.00012.
2024-12-02-06:31:37-root-INFO: step: 234 lr_xt 0.00027361
2024-12-02-06:31:38-root-INFO: grad norm: 407.047 367.869 174.240
2024-12-02-06:31:38-root-INFO: grad norm: 373.299 343.081 147.130
2024-12-02-06:31:39-root-INFO: grad norm: 357.218 328.601 140.092
2024-12-02-06:31:39-root-INFO: grad norm: 344.678 318.231 132.407
2024-12-02-06:31:40-root-INFO: grad norm: 333.689 307.784 128.907
2024-12-02-06:31:40-root-INFO: grad norm: 323.386 299.411 122.194
2024-12-02-06:31:40-root-INFO: grad norm: 314.077 290.444 119.527
2024-12-02-06:31:41-root-INFO: grad norm: 305.422 283.625 113.311
2024-12-02-06:31:41-root-INFO: Loss Change: 6031.075 -> 5778.159
2024-12-02-06:31:41-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-02-06:31:41-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-02-06:31:41-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:42-root-INFO: step: 233 lr_xt 0.00028759
2024-12-02-06:31:42-root-INFO: grad norm: 488.090 424.943 240.114
2024-12-02-06:31:42-root-INFO: grad norm: 401.604 353.328 190.906
2024-12-02-06:31:43-root-INFO: grad norm: 383.917 347.714 162.747
2024-12-02-06:31:43-root-INFO: grad norm: 378.798 340.125 166.743
2024-12-02-06:31:44-root-INFO: grad norm: 378.115 344.513 155.825
2024-12-02-06:31:44-root-INFO: grad norm: 379.577 341.786 165.109
2024-12-02-06:31:45-root-INFO: grad norm: 382.350 347.936 158.532
2024-12-02-06:31:45-root-INFO: grad norm: 385.966 347.571 167.823
2024-12-02-06:31:45-root-INFO: Loss Change: 5729.423 -> 5553.998
2024-12-02-06:31:45-root-INFO: Regularization Change: 0.000 -> 0.386
2024-12-02-06:31:45-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-02-06:31:45-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:46-root-INFO: step: 232 lr_xt 0.00030224
2024-12-02-06:31:46-root-INFO: grad norm: 1026.288 898.332 496.254
2024-12-02-06:31:46-root-INFO: grad norm: 997.985 884.190 462.798
2024-12-02-06:31:47-root-INFO: grad norm: 1002.513 888.015 465.254
2024-12-02-06:31:47-root-INFO: grad norm: 1012.752 899.765 464.855
2024-12-02-06:31:48-root-INFO: grad norm: 1024.862 909.318 472.741
2024-12-02-06:31:48-root-INFO: grad norm: 1039.312 923.098 477.555
2024-12-02-06:31:49-root-INFO: grad norm: 1053.092 934.655 485.204
2024-12-02-06:31:49-root-INFO: grad norm: 1068.475 948.620 491.690
2024-12-02-06:31:50-root-INFO: Loss Change: 5510.278 -> 5384.022
2024-12-02-06:31:50-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-06:31:50-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-02-06:31:50-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:50-root-INFO: step: 231 lr_xt 0.00031758
2024-12-02-06:31:50-root-INFO: grad norm: 1279.737 1137.804 585.773
2024-12-02-06:31:50-root-INFO: grad norm: 1267.374 1125.800 582.075
2024-12-02-06:31:51-root-INFO: grad norm: 1254.994 1114.381 577.204
2024-12-02-06:31:51-root-INFO: grad norm: 1241.412 1103.295 569.072
2024-12-02-06:31:52-root-INFO: grad norm: 1228.936 1090.713 566.241
2024-12-02-06:31:52-root-INFO: grad norm: 1216.377 1081.357 556.990
2024-12-02-06:31:53-root-INFO: grad norm: 1204.645 1068.954 555.435
2024-12-02-06:31:53-root-INFO: grad norm: 1193.087 1060.861 545.922
2024-12-02-06:31:54-root-INFO: Loss Change: 5367.200 -> 5241.640
2024-12-02-06:31:54-root-INFO: Regularization Change: 0.000 -> 0.272
2024-12-02-06:31:54-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-02-06:31:54-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:54-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-06:31:54-root-INFO: grad norm: 1505.052 1333.949 696.966
2024-12-02-06:31:54-root-INFO: grad norm: 1441.298 1283.137 656.430
2024-12-02-06:31:55-root-INFO: grad norm: 1388.225 1232.198 639.418
2024-12-02-06:31:55-root-INFO: grad norm: 1335.463 1189.437 607.208
2024-12-02-06:31:56-root-INFO: grad norm: 1288.468 1144.006 592.790
2024-12-02-06:31:56-root-INFO: grad norm: 1242.420 1106.807 564.434
2024-12-02-06:31:57-root-INFO: grad norm: 1201.024 1066.719 551.878
2024-12-02-06:31:57-root-INFO: grad norm: 1159.731 1033.404 526.357
2024-12-02-06:31:58-root-INFO: Loss Change: 5252.639 -> 5066.583
2024-12-02-06:31:58-root-INFO: Regularization Change: 0.000 -> 0.265
2024-12-02-06:31:58-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-06:31:58-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:31:58-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-06:31:58-root-INFO: grad norm: 1560.853 1389.887 710.264
2024-12-02-06:31:58-root-INFO: grad norm: 1483.560 1325.295 666.741
2024-12-02-06:31:59-root-INFO: grad norm: 1426.325 1271.181 646.917
2024-12-02-06:31:59-root-INFO: grad norm: 1369.597 1223.571 615.361
2024-12-02-06:32:00-root-INFO: grad norm: 1324.108 1180.340 600.051
2024-12-02-06:32:00-root-INFO: grad norm: 1279.291 1142.886 574.803
2024-12-02-06:32:01-root-INFO: grad norm: 1241.082 1106.663 561.766
2024-12-02-06:32:01-root-INFO: grad norm: 1203.048 1074.817 540.457
2024-12-02-06:32:02-root-INFO: Loss Change: 5155.277 -> 4949.194
2024-12-02-06:32:02-root-INFO: Regularization Change: 0.000 -> 0.295
2024-12-02-06:32:02-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-06:32:02-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:32:02-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-06:32:02-root-INFO: grad norm: 1368.168 1218.272 622.653
2024-12-02-06:32:03-root-INFO: grad norm: 1271.267 1134.237 574.132
2024-12-02-06:32:03-root-INFO: grad norm: 1196.338 1068.125 538.827
2024-12-02-06:32:04-root-INFO: grad norm: 1123.501 1003.861 504.497
2024-12-02-06:32:04-root-INFO: grad norm: 1060.750 946.873 478.144
2024-12-02-06:32:05-root-INFO: grad norm: 999.762 894.017 447.502
2024-12-02-06:32:05-root-INFO: grad norm: 947.619 846.030 426.867
2024-12-02-06:32:06-root-INFO: grad norm: 897.814 803.409 400.754
2024-12-02-06:32:06-root-INFO: Loss Change: 4974.201 -> 4773.755
2024-12-02-06:32:06-root-INFO: Regularization Change: 0.000 -> 0.267
2024-12-02-06:32:06-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-06:32:06-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:32:06-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-06:32:06-root-INFO: grad norm: 1333.789 1189.201 603.983
2024-12-02-06:32:07-root-INFO: grad norm: 1216.506 1089.285 541.614
2024-12-02-06:32:07-root-INFO: grad norm: 1130.828 1010.992 506.625
2024-12-02-06:32:08-root-INFO: grad norm: 1048.857 939.097 467.116
2024-12-02-06:32:08-root-INFO: grad norm: 981.021 877.914 437.801
2024-12-02-06:32:09-root-INFO: grad norm: 915.889 820.213 407.558
2024-12-02-06:32:09-root-INFO: grad norm: 859.703 770.176 381.992
2024-12-02-06:32:10-root-INFO: grad norm: 805.782 721.870 358.035
2024-12-02-06:32:10-root-INFO: Loss Change: 4877.837 -> 4649.694
2024-12-02-06:32:10-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-02-06:32:10-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-06:32:10-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-06:32:10-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-06:32:10-root-INFO: grad norm: 771.695 685.865 353.697
2024-12-02-06:32:11-root-INFO: grad norm: 675.599 603.591 303.500
2024-12-02-06:32:11-root-INFO: grad norm: 610.169 548.112 268.105
2024-12-02-06:32:12-root-INFO: grad norm: 552.319 496.065 242.847
2024-12-02-06:32:12-root-INFO: grad norm: 503.406 452.769 220.040
2024-12-02-06:32:13-root-INFO: grad norm: 458.767 413.500 198.708
2024-12-02-06:32:13-root-INFO: grad norm: 420.380 379.169 181.522
2024-12-02-06:32:14-root-INFO: grad norm: 385.825 349.015 164.467
2024-12-02-06:32:14-root-INFO: Loss Change: 4595.938 -> 4458.149
2024-12-02-06:32:14-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-02-06:32:14-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-06:32:14-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:32:14-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-06:32:14-root-INFO: grad norm: 851.808 754.296 395.746
2024-12-02-06:32:15-root-INFO: grad norm: 746.695 671.676 326.197
2024-12-02-06:32:15-root-INFO: grad norm: 674.196 602.464 302.617
2024-12-02-06:32:16-root-INFO: grad norm: 608.349 547.429 265.348
2024-12-02-06:32:16-root-INFO: grad norm: 552.922 495.853 244.647
2024-12-02-06:32:16-root-INFO: grad norm: 502.129 452.333 218.009
2024-12-02-06:32:17-root-INFO: grad norm: 459.283 413.387 200.129
2024-12-02-06:32:17-root-INFO: grad norm: 421.087 380.078 181.261
2024-12-02-06:32:18-root-INFO: Loss Change: 4486.195 -> 4328.652
2024-12-02-06:32:18-root-INFO: Regularization Change: 0.000 -> 0.316
2024-12-02-06:32:18-root-INFO: Undo step: 225
2024-12-02-06:32:18-root-INFO: Undo step: 226
2024-12-02-06:32:18-root-INFO: Undo step: 227
2024-12-02-06:32:18-root-INFO: Undo step: 228
2024-12-02-06:32:18-root-INFO: Undo step: 229
2024-12-02-06:32:18-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-06:32:18-root-INFO: grad norm: 6496.817 5283.415 3780.762
2024-12-02-06:32:19-root-INFO: grad norm: 3447.394 2985.303 1724.090
2024-12-02-06:32:19-root-INFO: grad norm: 2397.322 2116.948 1125.026
2024-12-02-06:32:20-root-INFO: grad norm: 2084.663 1826.637 1004.598
2024-12-02-06:32:20-root-INFO: grad norm: 1843.450 1636.376 848.871
2024-12-02-06:32:20-root-INFO: grad norm: 1671.448 1470.502 794.581
2024-12-02-06:32:21-root-INFO: grad norm: 1519.485 1353.539 690.483
2024-12-02-06:32:21-root-INFO: grad norm: 1400.599 1234.654 661.292
2024-12-02-06:32:22-root-INFO: Loss Change: 11004.039 -> 5604.937
2024-12-02-06:32:22-root-INFO: Regularization Change: 0.000 -> 4.136
2024-12-02-06:32:22-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-06:32:22-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:32:22-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-06:32:22-root-INFO: grad norm: 1008.589 905.990 443.209
2024-12-02-06:32:23-root-INFO: grad norm: 866.762 766.818 404.063
2024-12-02-06:32:23-root-INFO: grad norm: 803.731 729.290 337.818
2024-12-02-06:32:24-root-INFO: grad norm: 756.804 672.546 347.037
2024-12-02-06:32:24-root-INFO: grad norm: 717.234 650.934 301.182
2024-12-02-06:32:25-root-INFO: grad norm: 685.785 609.737 313.883
2024-12-02-06:32:25-root-INFO: grad norm: 658.730 598.039 276.178
2024-12-02-06:32:26-root-INFO: grad norm: 637.029 566.413 291.517
2024-12-02-06:32:26-root-INFO: Loss Change: 5492.916 -> 5140.144
2024-12-02-06:32:26-root-INFO: Regularization Change: 0.000 -> 0.833
2024-12-02-06:32:26-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-06:32:26-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:32:26-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-06:32:26-root-INFO: grad norm: 475.371 424.124 214.702
2024-12-02-06:32:27-root-INFO: grad norm: 421.073 374.814 191.877
2024-12-02-06:32:27-root-INFO: grad norm: 398.732 365.870 158.511
2024-12-02-06:32:28-root-INFO: grad norm: 382.816 342.702 170.597
2024-12-02-06:32:28-root-INFO: grad norm: 368.710 339.590 143.617
2024-12-02-06:32:29-root-INFO: grad norm: 356.397 319.293 158.337
2024-12-02-06:32:29-root-INFO: grad norm: 345.435 318.304 134.194
2024-12-02-06:32:30-root-INFO: grad norm: 335.337 300.815 148.195
2024-12-02-06:32:30-root-INFO: Loss Change: 5084.525 -> 4901.128
2024-12-02-06:32:30-root-INFO: Regularization Change: 0.000 -> 0.512
2024-12-02-06:32:30-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-06:32:30-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:32:30-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-06:32:30-root-INFO: grad norm: 453.034 394.196 223.270
2024-12-02-06:32:31-root-INFO: grad norm: 347.068 318.344 138.253
2024-12-02-06:32:31-root-INFO: grad norm: 320.326 286.808 142.653
2024-12-02-06:32:32-root-INFO: grad norm: 303.823 280.365 117.063
2024-12-02-06:32:32-root-INFO: grad norm: 291.670 263.071 125.956
2024-12-02-06:32:33-root-INFO: grad norm: 281.680 259.980 108.417
2024-12-02-06:32:33-root-INFO: grad norm: 273.308 247.478 115.982
2024-12-02-06:32:34-root-INFO: grad norm: 265.802 245.436 102.040
2024-12-02-06:32:34-root-INFO: Loss Change: 4897.785 -> 4719.119
2024-12-02-06:32:34-root-INFO: Regularization Change: 0.000 -> 0.504
2024-12-02-06:32:34-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-06:32:34-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-06:32:34-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-06:32:34-root-INFO: grad norm: 329.250 276.011 179.509
2024-12-02-06:32:35-root-INFO: grad norm: 257.415 233.214 108.967
2024-12-02-06:32:35-root-INFO: grad norm: 239.097 217.383 99.559
2024-12-02-06:32:36-root-INFO: grad norm: 228.764 211.344 87.562
2024-12-02-06:32:36-root-INFO: grad norm: 220.855 201.567 90.263
2024-12-02-06:32:37-root-INFO: grad norm: 213.922 198.447 79.883
2024-12-02-06:32:37-root-INFO: grad norm: 207.814 190.269 83.572
2024-12-02-06:32:38-root-INFO: grad norm: 202.504 188.167 74.840
2024-12-02-06:32:38-root-INFO: Loss Change: 4675.490 -> 4548.825
2024-12-02-06:32:38-root-INFO: Regularization Change: 0.000 -> 0.386
2024-12-02-06:32:38-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-06:32:38-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:32:38-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-06:32:38-root-INFO: grad norm: 662.721 586.160 309.218
2024-12-02-06:32:39-root-INFO: grad norm: 567.807 518.917 230.498
2024-12-02-06:32:39-root-INFO: grad norm: 525.706 468.845 237.806
2024-12-02-06:32:40-root-INFO: grad norm: 488.933 445.922 200.521
2024-12-02-06:32:40-root-INFO: grad norm: 458.131 410.386 203.634
2024-12-02-06:32:41-root-INFO: grad norm: 430.013 391.946 176.889
2024-12-02-06:32:41-root-INFO: grad norm: 405.454 364.217 178.154
2024-12-02-06:32:42-root-INFO: grad norm: 382.926 349.100 157.356
2024-12-02-06:32:42-root-INFO: Loss Change: 4552.808 -> 4397.823
2024-12-02-06:32:42-root-INFO: Regularization Change: 0.000 -> 0.402
2024-12-02-06:32:42-root-INFO: Undo step: 225
2024-12-02-06:32:42-root-INFO: Undo step: 226
2024-12-02-06:32:42-root-INFO: Undo step: 227
2024-12-02-06:32:42-root-INFO: Undo step: 228
2024-12-02-06:32:42-root-INFO: Undo step: 229
2024-12-02-06:32:42-root-INFO: step: 230 lr_xt 0.00033364
2024-12-02-06:32:42-root-INFO: grad norm: 5121.355 4398.777 2622.794
2024-12-02-06:32:43-root-INFO: grad norm: 3368.416 2994.127 1543.187
2024-12-02-06:32:43-root-INFO: grad norm: 2616.467 2264.790 1310.199
2024-12-02-06:32:44-root-INFO: grad norm: 2421.083 2160.964 1091.730
2024-12-02-06:32:44-root-INFO: grad norm: 2294.134 2009.567 1106.658
2024-12-02-06:32:45-root-INFO: grad norm: 2193.226 1962.173 979.856
2024-12-02-06:32:45-root-INFO: grad norm: 2102.260 1850.965 996.709
2024-12-02-06:32:46-root-INFO: grad norm: 2025.475 1812.144 904.811
2024-12-02-06:32:46-root-INFO: Loss Change: 9192.478 -> 5612.411
2024-12-02-06:32:46-root-INFO: Regularization Change: 0.000 -> 4.390
2024-12-02-06:32:46-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-02-06:32:46-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:32:46-root-INFO: step: 229 lr_xt 0.00035047
2024-12-02-06:32:46-root-INFO: grad norm: 1476.168 1303.087 693.569
2024-12-02-06:32:47-root-INFO: grad norm: 1385.417 1246.923 603.792
2024-12-02-06:32:47-root-INFO: grad norm: 1358.240 1209.714 617.582
2024-12-02-06:32:48-root-INFO: grad norm: 1335.399 1202.352 581.069
2024-12-02-06:32:48-root-INFO: grad norm: 1316.387 1173.798 595.880
2024-12-02-06:32:49-root-INFO: grad norm: 1301.962 1171.535 567.988
2024-12-02-06:32:49-root-INFO: grad norm: 1289.880 1151.383 581.471
2024-12-02-06:32:50-root-INFO: grad norm: 1280.049 1151.270 559.555
2024-12-02-06:32:50-root-INFO: Loss Change: 5351.212 -> 5019.412
2024-12-02-06:32:50-root-INFO: Regularization Change: 0.000 -> 0.783
2024-12-02-06:32:50-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-02-06:32:50-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:32:50-root-INFO: step: 228 lr_xt 0.00036807
2024-12-02-06:32:50-root-INFO: grad norm: 1005.151 889.016 469.020
2024-12-02-06:32:51-root-INFO: grad norm: 956.678 861.493 416.008
2024-12-02-06:32:51-root-INFO: grad norm: 926.905 828.793 415.037
2024-12-02-06:32:52-root-INFO: grad norm: 903.172 813.528 392.291
2024-12-02-06:32:52-root-INFO: grad norm: 880.742 789.252 390.880
2024-12-02-06:32:53-root-INFO: grad norm: 861.442 775.584 374.903
2024-12-02-06:32:53-root-INFO: grad norm: 843.065 756.413 372.287
2024-12-02-06:32:54-root-INFO: grad norm: 827.350 744.708 360.442
2024-12-02-06:32:54-root-INFO: Loss Change: 4929.407 -> 4753.753
2024-12-02-06:32:54-root-INFO: Regularization Change: 0.000 -> 0.424
2024-12-02-06:32:54-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-02-06:32:54-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00012.
2024-12-02-06:32:54-root-INFO: step: 227 lr_xt 0.00038651
2024-12-02-06:32:55-root-INFO: grad norm: 402.212 346.511 204.218
2024-12-02-06:32:55-root-INFO: grad norm: 319.198 288.140 137.341
2024-12-02-06:32:56-root-INFO: grad norm: 298.613 271.781 123.713
2024-12-02-06:32:56-root-INFO: grad norm: 287.679 263.216 116.089
2024-12-02-06:32:57-root-INFO: grad norm: 280.354 256.219 113.798
2024-12-02-06:32:57-root-INFO: grad norm: 274.753 252.286 108.816
2024-12-02-06:32:58-root-INFO: grad norm: 270.212 247.378 108.714
2024-12-02-06:32:58-root-INFO: grad norm: 266.232 245.067 104.026
2024-12-02-06:32:58-root-INFO: Loss Change: 4673.268 -> 4536.704
2024-12-02-06:32:58-root-INFO: Regularization Change: 0.000 -> 0.388
2024-12-02-06:32:58-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-02-06:32:58-root-INFO: Coefficient of regularization decay: 0.00012 -> 0.00013.
2024-12-02-06:32:59-root-INFO: step: 226 lr_xt 0.00040579
2024-12-02-06:32:59-root-INFO: grad norm: 337.298 284.062 181.876
2024-12-02-06:32:59-root-INFO: grad norm: 260.613 238.836 104.292
2024-12-02-06:33:00-root-INFO: grad norm: 242.151 221.386 98.110
2024-12-02-06:33:00-root-INFO: grad norm: 232.178 214.697 88.384
2024-12-02-06:33:01-root-INFO: grad norm: 224.248 207.081 86.050
2024-12-02-06:33:01-root-INFO: grad norm: 217.494 201.743 81.263
2024-12-02-06:33:02-root-INFO: grad norm: 211.492 196.265 78.795
2024-12-02-06:33:02-root-INFO: grad norm: 206.041 191.708 75.505
2024-12-02-06:33:03-root-INFO: Loss Change: 4504.072 -> 4390.474
2024-12-02-06:33:03-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-06:33:03-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-02-06:33:03-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:03-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-06:33:03-root-INFO: grad norm: 514.230 455.962 237.763
2024-12-02-06:33:03-root-INFO: grad norm: 439.303 402.095 176.937
2024-12-02-06:33:04-root-INFO: grad norm: 413.856 373.962 177.284
2024-12-02-06:33:04-root-INFO: grad norm: 392.219 358.724 158.596
2024-12-02-06:33:05-root-INFO: grad norm: 373.620 339.201 156.636
2024-12-02-06:33:05-root-INFO: grad norm: 356.285 325.897 143.980
2024-12-02-06:33:06-root-INFO: grad norm: 340.663 310.286 140.619
2024-12-02-06:33:06-root-INFO: grad norm: 325.799 298.295 131.016
2024-12-02-06:33:07-root-INFO: Loss Change: 4367.665 -> 4241.467
2024-12-02-06:33:07-root-INFO: Regularization Change: 0.000 -> 0.360
2024-12-02-06:33:07-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-06:33:07-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:07-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-06:33:07-root-INFO: grad norm: 622.900 556.666 279.514
2024-12-02-06:33:08-root-INFO: grad norm: 580.150 528.139 240.089
2024-12-02-06:33:08-root-INFO: grad norm: 555.073 502.768 235.223
2024-12-02-06:33:09-root-INFO: grad norm: 531.903 484.303 219.935
2024-12-02-06:33:09-root-INFO: grad norm: 511.929 464.731 214.701
2024-12-02-06:33:10-root-INFO: grad norm: 492.457 448.388 203.623
2024-12-02-06:33:10-root-INFO: grad norm: 475.437 432.275 197.937
2024-12-02-06:33:11-root-INFO: grad norm: 459.037 418.031 189.644
2024-12-02-06:33:11-root-INFO: Loss Change: 4254.807 -> 4130.110
2024-12-02-06:33:11-root-INFO: Regularization Change: 0.000 -> 0.365
2024-12-02-06:33:11-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-06:33:11-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:11-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-06:33:12-root-INFO: grad norm: 773.981 695.285 340.037
2024-12-02-06:33:12-root-INFO: grad norm: 713.408 648.878 296.493
2024-12-02-06:33:13-root-INFO: grad norm: 673.292 609.807 285.410
2024-12-02-06:33:13-root-INFO: grad norm: 633.924 576.645 263.324
2024-12-02-06:33:13-root-INFO: grad norm: 600.450 544.932 252.170
2024-12-02-06:33:14-root-INFO: grad norm: 567.540 516.335 235.585
2024-12-02-06:33:14-root-INFO: grad norm: 538.903 489.830 224.686
2024-12-02-06:33:15-root-INFO: grad norm: 510.428 464.544 211.508
2024-12-02-06:33:15-root-INFO: Loss Change: 4162.782 -> 4023.080
2024-12-02-06:33:15-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-02-06:33:15-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-06:33:15-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:16-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-06:33:16-root-INFO: grad norm: 568.762 516.765 237.579
2024-12-02-06:33:16-root-INFO: grad norm: 523.804 476.761 216.955
2024-12-02-06:33:17-root-INFO: grad norm: 486.610 443.547 200.139
2024-12-02-06:33:17-root-INFO: grad norm: 451.608 411.514 186.026
2024-12-02-06:33:18-root-INFO: grad norm: 420.988 384.431 171.591
2024-12-02-06:33:18-root-INFO: grad norm: 391.675 357.410 160.212
2024-12-02-06:33:19-root-INFO: grad norm: 366.118 334.988 147.734
2024-12-02-06:33:19-root-INFO: grad norm: 342.058 312.646 138.766
2024-12-02-06:33:20-root-INFO: Loss Change: 3964.989 -> 3860.265
2024-12-02-06:33:20-root-INFO: Regularization Change: 0.000 -> 0.305
2024-12-02-06:33:20-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-06:33:20-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:20-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-06:33:20-root-INFO: grad norm: 484.480 437.963 207.148
2024-12-02-06:33:20-root-INFO: grad norm: 415.902 379.185 170.860
2024-12-02-06:33:21-root-INFO: grad norm: 368.139 336.010 150.413
2024-12-02-06:33:22-root-INFO: grad norm: 326.109 298.510 131.299
2024-12-02-06:33:22-root-INFO: grad norm: 291.428 267.133 116.490
2024-12-02-06:33:23-root-INFO: grad norm: 261.241 240.136 102.868
2024-12-02-06:33:23-root-INFO: grad norm: 236.719 218.185 91.820
2024-12-02-06:33:24-root-INFO: grad norm: 215.335 198.988 82.299
2024-12-02-06:33:24-root-INFO: Loss Change: 3844.893 -> 3745.589
2024-12-02-06:33:24-root-INFO: Regularization Change: 0.000 -> 0.304
2024-12-02-06:33:24-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-06:33:24-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:24-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-06:33:25-root-INFO: grad norm: 551.159 488.362 255.499
2024-12-02-06:33:25-root-INFO: grad norm: 455.027 417.519 180.909
2024-12-02-06:33:26-root-INFO: grad norm: 394.051 357.560 165.610
2024-12-02-06:33:26-root-INFO: grad norm: 340.969 313.194 134.793
2024-12-02-06:33:27-root-INFO: grad norm: 298.148 272.586 120.785
2024-12-02-06:33:27-root-INFO: grad norm: 261.335 240.995 101.081
2024-12-02-06:33:28-root-INFO: grad norm: 232.213 213.977 90.205
2024-12-02-06:33:28-root-INFO: grad norm: 207.676 192.622 77.627
2024-12-02-06:33:28-root-INFO: Loss Change: 3767.756 -> 3645.168
2024-12-02-06:33:29-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-06:33:29-root-INFO: Undo step: 220
2024-12-02-06:33:29-root-INFO: Undo step: 221
2024-12-02-06:33:29-root-INFO: Undo step: 222
2024-12-02-06:33:29-root-INFO: Undo step: 223
2024-12-02-06:33:29-root-INFO: Undo step: 224
2024-12-02-06:33:29-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-06:33:29-root-INFO: grad norm: 3788.076 2873.488 2468.316
2024-12-02-06:33:29-root-INFO: grad norm: 1238.866 1064.315 634.052
2024-12-02-06:33:30-root-INFO: grad norm: 745.777 628.629 401.258
2024-12-02-06:33:30-root-INFO: grad norm: 580.105 511.162 274.289
2024-12-02-06:33:31-root-INFO: grad norm: 492.961 426.487 247.224
2024-12-02-06:33:31-root-INFO: grad norm: 435.109 382.582 207.245
2024-12-02-06:33:32-root-INFO: grad norm: 393.521 346.272 186.962
2024-12-02-06:33:32-root-INFO: grad norm: 361.457 318.497 170.912
2024-12-02-06:33:33-root-INFO: Loss Change: 8467.918 -> 4388.603
2024-12-02-06:33:33-root-INFO: Regularization Change: 0.000 -> 5.979
2024-12-02-06:33:33-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-06:33:33-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:33-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-06:33:33-root-INFO: grad norm: 339.565 300.522 158.085
2024-12-02-06:33:33-root-INFO: grad norm: 273.675 241.078 129.537
2024-12-02-06:33:34-root-INFO: grad norm: 256.506 227.189 119.081
2024-12-02-06:33:34-root-INFO: grad norm: 245.616 220.891 107.397
2024-12-02-06:33:35-root-INFO: grad norm: 237.605 211.325 108.619
2024-12-02-06:33:35-root-INFO: grad norm: 231.458 209.966 97.401
2024-12-02-06:33:36-root-INFO: grad norm: 226.646 202.873 101.050
2024-12-02-06:33:36-root-INFO: grad norm: 223.061 203.571 91.186
2024-12-02-06:33:37-root-INFO: Loss Change: 4327.499 -> 4155.258
2024-12-02-06:33:37-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-02-06:33:37-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-06:33:37-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:37-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-06:33:37-root-INFO: grad norm: 545.209 485.294 248.481
2024-12-02-06:33:37-root-INFO: grad norm: 531.775 484.939 218.217
2024-12-02-06:33:38-root-INFO: grad norm: 541.135 486.359 237.239
2024-12-02-06:33:38-root-INFO: grad norm: 553.817 505.718 225.748
2024-12-02-06:33:39-root-INFO: grad norm: 567.859 512.135 245.320
2024-12-02-06:33:39-root-INFO: grad norm: 584.074 532.832 239.232
2024-12-02-06:33:40-root-INFO: grad norm: 600.834 542.959 257.289
2024-12-02-06:33:40-root-INFO: grad norm: 619.692 564.927 254.707
2024-12-02-06:33:41-root-INFO: Loss Change: 4150.038 -> 4060.872
2024-12-02-06:33:41-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-06:33:41-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-06:33:41-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:41-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-06:33:41-root-INFO: grad norm: 726.210 657.457 308.433
2024-12-02-06:33:41-root-INFO: grad norm: 735.755 670.575 302.761
2024-12-02-06:33:42-root-INFO: grad norm: 745.774 676.340 314.235
2024-12-02-06:33:42-root-INFO: grad norm: 757.122 690.225 311.165
2024-12-02-06:33:43-root-INFO: grad norm: 768.165 697.117 322.653
2024-12-02-06:33:43-root-INFO: grad norm: 780.373 711.565 320.404
2024-12-02-06:33:44-root-INFO: grad norm: 792.113 719.244 331.861
2024-12-02-06:33:44-root-INFO: grad norm: 804.942 734.112 330.169
2024-12-02-06:33:45-root-INFO: Loss Change: 4007.167 -> 3948.241
2024-12-02-06:33:45-root-INFO: Regularization Change: 0.000 -> 0.305
2024-12-02-06:33:45-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-06:33:45-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:45-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-06:33:45-root-INFO: grad norm: 975.000 882.315 414.904
2024-12-02-06:33:45-root-INFO: grad norm: 938.627 855.753 385.625
2024-12-02-06:33:46-root-INFO: grad norm: 910.766 825.604 384.543
2024-12-02-06:33:46-root-INFO: grad norm: 881.318 804.153 360.638
2024-12-02-06:33:47-root-INFO: grad norm: 855.779 776.317 360.124
2024-12-02-06:33:47-root-INFO: grad norm: 829.936 757.479 339.145
2024-12-02-06:33:48-root-INFO: grad norm: 806.983 732.596 338.414
2024-12-02-06:33:48-root-INFO: grad norm: 783.314 715.119 319.665
2024-12-02-06:33:49-root-INFO: Loss Change: 3956.887 -> 3837.845
2024-12-02-06:33:49-root-INFO: Regularization Change: 0.000 -> 0.286
2024-12-02-06:33:49-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-06:33:49-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:49-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-06:33:49-root-INFO: grad norm: 1116.607 1003.449 489.798
2024-12-02-06:33:49-root-INFO: grad norm: 1062.460 973.350 425.924
2024-12-02-06:33:50-root-INFO: grad norm: 1024.911 928.267 434.470
2024-12-02-06:33:50-root-INFO: grad norm: 985.427 901.469 398.020
2024-12-02-06:33:51-root-INFO: grad norm: 952.789 865.275 398.881
2024-12-02-06:33:51-root-INFO: grad norm: 918.267 839.591 371.890
2024-12-02-06:33:52-root-INFO: grad norm: 888.953 808.594 369.341
2024-12-02-06:33:52-root-INFO: grad norm: 858.226 784.652 347.667
2024-12-02-06:33:52-root-INFO: Loss Change: 3923.011 -> 3762.189
2024-12-02-06:33:52-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-06:33:52-root-INFO: Undo step: 220
2024-12-02-06:33:52-root-INFO: Undo step: 221
2024-12-02-06:33:52-root-INFO: Undo step: 222
2024-12-02-06:33:52-root-INFO: Undo step: 223
2024-12-02-06:33:52-root-INFO: Undo step: 224
2024-12-02-06:33:53-root-INFO: step: 225 lr_xt 0.00042598
2024-12-02-06:33:53-root-INFO: grad norm: 2664.411 2293.245 1356.508
2024-12-02-06:33:53-root-INFO: grad norm: 2356.132 2175.297 905.230
2024-12-02-06:33:54-root-INFO: grad norm: 2315.884 2096.551 983.766
2024-12-02-06:33:54-root-INFO: grad norm: 2287.360 2103.702 898.027
2024-12-02-06:33:55-root-INFO: grad norm: 2289.344 2084.231 947.142
2024-12-02-06:33:55-root-INFO: grad norm: 2298.000 2101.682 929.373
2024-12-02-06:33:56-root-INFO: grad norm: 2324.605 2117.078 960.089
2024-12-02-06:33:56-root-INFO: Loss too large (5299.075->5302.359)! Learning rate decreased to 0.00034.
2024-12-02-06:33:56-root-INFO: grad norm: 1538.722 1408.390 619.760
2024-12-02-06:33:57-root-INFO: Loss Change: 6910.317 -> 4732.963
2024-12-02-06:33:57-root-INFO: Regularization Change: 0.000 -> 4.845
2024-12-02-06:33:57-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-02-06:33:57-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:33:57-root-INFO: step: 224 lr_xt 0.00044709
2024-12-02-06:33:57-root-INFO: grad norm: 696.178 627.325 301.872
2024-12-02-06:33:57-root-INFO: grad norm: 697.572 642.173 272.436
2024-12-02-06:33:58-root-INFO: grad norm: 737.000 666.478 314.605
2024-12-02-06:33:58-root-INFO: grad norm: 779.699 716.630 307.200
2024-12-02-06:33:59-root-INFO: grad norm: 829.607 750.750 353.020
2024-12-02-06:33:59-root-INFO: grad norm: 878.404 804.652 352.319
2024-12-02-06:34:00-root-INFO: grad norm: 932.017 843.434 396.580
2024-12-02-06:34:00-root-INFO: grad norm: 981.381 896.263 399.776
2024-12-02-06:34:01-root-INFO: Loss Change: 4588.226 -> 4353.410
2024-12-02-06:34:01-root-INFO: Regularization Change: 0.000 -> 1.080
2024-12-02-06:34:01-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-02-06:34:01-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:34:01-root-INFO: step: 223 lr_xt 0.00046917
2024-12-02-06:34:01-root-INFO: grad norm: 645.196 588.527 264.412
2024-12-02-06:34:01-root-INFO: grad norm: 642.273 586.694 261.352
2024-12-02-06:34:02-root-INFO: grad norm: 665.540 602.469 282.799
2024-12-02-06:34:02-root-INFO: grad norm: 685.733 626.313 279.216
2024-12-02-06:34:03-root-INFO: grad norm: 704.754 637.140 301.216
2024-12-02-06:34:03-root-INFO: grad norm: 720.146 656.674 295.617
2024-12-02-06:34:04-root-INFO: grad norm: 733.482 662.626 314.519
2024-12-02-06:34:04-root-INFO: grad norm: 742.507 676.124 306.876
2024-12-02-06:34:04-root-INFO: Loss Change: 4248.756 -> 4102.975
2024-12-02-06:34:04-root-INFO: Regularization Change: 0.000 -> 0.618
2024-12-02-06:34:04-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-02-06:34:04-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:34:05-root-INFO: step: 222 lr_xt 0.00049227
2024-12-02-06:34:05-root-INFO: grad norm: 635.235 574.991 270.017
2024-12-02-06:34:05-root-INFO: grad norm: 623.151 568.748 254.644
2024-12-02-06:34:06-root-INFO: grad norm: 610.701 552.781 259.593
2024-12-02-06:34:06-root-INFO: grad norm: 597.115 544.855 244.295
2024-12-02-06:34:07-root-INFO: grad norm: 581.190 526.111 246.959
2024-12-02-06:34:07-root-INFO: grad norm: 565.559 515.932 231.670
2024-12-02-06:34:08-root-INFO: grad norm: 548.115 496.264 232.705
2024-12-02-06:34:08-root-INFO: grad norm: 530.648 484.107 217.320
2024-12-02-06:34:08-root-INFO: Loss Change: 4014.777 -> 3890.179
2024-12-02-06:34:08-root-INFO: Regularization Change: 0.000 -> 0.433
2024-12-02-06:34:08-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-02-06:34:08-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:34:09-root-INFO: step: 221 lr_xt 0.00051641
2024-12-02-06:34:09-root-INFO: grad norm: 310.638 284.659 124.360
2024-12-02-06:34:09-root-INFO: grad norm: 274.142 252.239 107.374
2024-12-02-06:34:10-root-INFO: grad norm: 254.832 234.606 99.495
2024-12-02-06:34:10-root-INFO: grad norm: 238.080 220.616 89.501
2024-12-02-06:34:11-root-INFO: grad norm: 222.784 205.677 85.613
2024-12-02-06:34:11-root-INFO: grad norm: 209.072 194.890 75.690
2024-12-02-06:34:12-root-INFO: grad norm: 196.956 182.609 73.793
2024-12-02-06:34:12-root-INFO: grad norm: 186.122 174.527 64.668
2024-12-02-06:34:12-root-INFO: Loss Change: 3834.466 -> 3741.756
2024-12-02-06:34:12-root-INFO: Regularization Change: 0.000 -> 0.350
2024-12-02-06:34:12-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-02-06:34:12-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:34:13-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-06:34:13-root-INFO: grad norm: 371.401 325.243 179.319
2024-12-02-06:34:13-root-INFO: grad norm: 298.560 276.268 113.197
2024-12-02-06:34:14-root-INFO: grad norm: 266.009 242.075 110.274
2024-12-02-06:34:14-root-INFO: grad norm: 240.183 222.155 91.296
2024-12-02-06:34:15-root-INFO: grad norm: 219.692 202.873 84.304
2024-12-02-06:34:15-root-INFO: grad norm: 202.388 187.654 75.809
2024-12-02-06:34:16-root-INFO: grad norm: 188.155 175.768 67.142
2024-12-02-06:34:16-root-INFO: grad norm: 175.848 163.823 63.911
2024-12-02-06:34:16-root-INFO: Loss Change: 3741.555 -> 3634.470
2024-12-02-06:34:16-root-INFO: Regularization Change: 0.000 -> 0.386
2024-12-02-06:34:16-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-06:34:16-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:34:17-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-06:34:17-root-INFO: grad norm: 198.436 184.462 73.148
2024-12-02-06:34:17-root-INFO: grad norm: 173.876 162.208 62.620
2024-12-02-06:34:18-root-INFO: grad norm: 158.789 149.977 52.161
2024-12-02-06:34:18-root-INFO: grad norm: 146.961 138.517 49.099
2024-12-02-06:34:19-root-INFO: grad norm: 138.001 131.386 42.212
2024-12-02-06:34:19-root-INFO: grad norm: 131.217 124.697 40.849
2024-12-02-06:34:20-root-INFO: grad norm: 126.033 120.737 36.150
2024-12-02-06:34:20-root-INFO: grad norm: 121.939 116.585 35.737
2024-12-02-06:34:20-root-INFO: Loss Change: 3606.236 -> 3537.067
2024-12-02-06:34:20-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-02-06:34:20-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-06:34:20-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-06:34:21-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-06:34:21-root-INFO: grad norm: 313.955 280.912 140.200
2024-12-02-06:34:21-root-INFO: grad norm: 250.406 232.833 92.151
2024-12-02-06:34:22-root-INFO: grad norm: 209.316 193.032 80.944
2024-12-02-06:34:22-root-INFO: grad norm: 178.780 167.369 62.848
2024-12-02-06:34:23-root-INFO: grad norm: 156.950 147.384 53.957
2024-12-02-06:34:23-root-INFO: grad norm: 141.090 133.415 45.899
2024-12-02-06:34:24-root-INFO: grad norm: 129.794 123.733 39.200
2024-12-02-06:34:24-root-INFO: grad norm: 121.782 116.304 36.114
2024-12-02-06:34:25-root-INFO: Loss Change: 3528.095 -> 3448.879
2024-12-02-06:34:25-root-INFO: Regularization Change: 0.000 -> 0.312
2024-12-02-06:34:25-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-06:34:25-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:34:25-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-06:34:25-root-INFO: grad norm: 312.560 270.091 157.305
2024-12-02-06:34:26-root-INFO: grad norm: 219.613 204.824 79.229
2024-12-02-06:34:26-root-INFO: grad norm: 181.026 167.252 69.260
2024-12-02-06:34:27-root-INFO: grad norm: 154.833 145.942 51.710
2024-12-02-06:34:27-root-INFO: grad norm: 137.399 129.891 44.796
2024-12-02-06:34:28-root-INFO: grad norm: 125.670 119.744 38.133
2024-12-02-06:34:28-root-INFO: grad norm: 117.843 113.055 33.251
2024-12-02-06:34:29-root-INFO: grad norm: 112.523 108.107 31.212
2024-12-02-06:34:29-root-INFO: Loss Change: 3449.878 -> 3367.208
2024-12-02-06:34:29-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-02-06:34:29-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-06:34:29-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:34:30-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-06:34:30-root-INFO: grad norm: 319.234 278.183 156.602
2024-12-02-06:34:30-root-INFO: grad norm: 223.088 207.763 81.256
2024-12-02-06:34:31-root-INFO: grad norm: 182.868 169.608 68.364
2024-12-02-06:34:31-root-INFO: grad norm: 156.246 147.009 52.927
2024-12-02-06:34:32-root-INFO: grad norm: 138.346 131.001 44.477
2024-12-02-06:34:32-root-INFO: grad norm: 125.840 119.646 38.996
2024-12-02-06:34:33-root-INFO: grad norm: 117.222 112.389 33.313
2024-12-02-06:34:33-root-INFO: grad norm: 111.089 106.517 31.542
2024-12-02-06:34:33-root-INFO: Loss Change: 3367.253 -> 3282.310
2024-12-02-06:34:33-root-INFO: Regularization Change: 0.000 -> 0.351
2024-12-02-06:34:33-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-06:34:33-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:34:34-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-06:34:34-root-INFO: grad norm: 132.741 125.557 43.075
2024-12-02-06:34:34-root-INFO: grad norm: 118.568 113.661 33.757
2024-12-02-06:34:35-root-INFO: grad norm: 110.756 106.269 31.205
2024-12-02-06:34:35-root-INFO: grad norm: 105.690 101.951 27.866
2024-12-02-06:34:36-root-INFO: grad norm: 102.461 98.796 27.157
2024-12-02-06:34:36-root-INFO: grad norm: 100.219 96.851 25.762
2024-12-02-06:34:37-root-INFO: grad norm: 98.534 95.200 25.416
2024-12-02-06:34:37-root-INFO: grad norm: 97.385 94.168 24.823
2024-12-02-06:34:37-root-INFO: Loss Change: 3268.299 -> 3212.166
2024-12-02-06:34:37-root-INFO: Regularization Change: 0.000 -> 0.301
2024-12-02-06:34:37-root-INFO: Undo step: 215
2024-12-02-06:34:37-root-INFO: Undo step: 216
2024-12-02-06:34:37-root-INFO: Undo step: 217
2024-12-02-06:34:37-root-INFO: Undo step: 218
2024-12-02-06:34:37-root-INFO: Undo step: 219
2024-12-02-06:34:38-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-06:34:38-root-INFO: grad norm: 2177.661 1699.819 1361.184
2024-12-02-06:34:38-root-INFO: grad norm: 954.221 796.149 526.008
2024-12-02-06:34:39-root-INFO: grad norm: 692.650 615.907 316.894
2024-12-02-06:34:39-root-INFO: grad norm: 558.157 490.517 266.330
2024-12-02-06:34:40-root-INFO: grad norm: 463.414 415.993 204.212
2024-12-02-06:34:40-root-INFO: grad norm: 390.500 346.349 180.369
2024-12-02-06:34:41-root-INFO: grad norm: 333.183 301.471 141.867
2024-12-02-06:34:41-root-INFO: grad norm: 288.330 258.723 127.267
2024-12-02-06:34:41-root-INFO: Loss Change: 6043.882 -> 3783.973
2024-12-02-06:34:41-root-INFO: Regularization Change: 0.000 -> 6.034
2024-12-02-06:34:41-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-06:34:42-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:34:42-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-06:34:42-root-INFO: grad norm: 283.129 257.187 118.394
2024-12-02-06:34:42-root-INFO: grad norm: 240.946 217.970 102.683
2024-12-02-06:34:43-root-INFO: grad norm: 211.472 195.707 80.120
2024-12-02-06:34:43-root-INFO: grad norm: 189.041 173.693 74.615
2024-12-02-06:34:44-root-INFO: grad norm: 172.910 161.814 60.944
2024-12-02-06:34:44-root-INFO: grad norm: 160.553 149.141 59.449
2024-12-02-06:34:45-root-INFO: grad norm: 151.638 142.830 50.928
2024-12-02-06:34:45-root-INFO: grad norm: 144.434 135.078 51.139
2024-12-02-06:34:46-root-INFO: Loss Change: 3737.806 -> 3588.710
2024-12-02-06:34:46-root-INFO: Regularization Change: 0.000 -> 0.639
2024-12-02-06:34:46-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-06:34:46-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-06:34:46-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-06:34:46-root-INFO: grad norm: 317.041 284.531 139.847
2024-12-02-06:34:47-root-INFO: grad norm: 253.070 231.037 103.277
2024-12-02-06:34:47-root-INFO: grad norm: 210.447 193.750 82.153
2024-12-02-06:34:48-root-INFO: grad norm: 179.944 164.878 72.076
2024-12-02-06:34:48-root-INFO: grad norm: 159.085 148.804 56.261
2024-12-02-06:34:49-root-INFO: grad norm: 144.596 133.672 55.136
2024-12-02-06:34:49-root-INFO: grad norm: 134.798 127.330 44.245
2024-12-02-06:34:50-root-INFO: grad norm: 127.974 119.465 45.885
2024-12-02-06:34:50-root-INFO: Loss Change: 3572.546 -> 3479.438
2024-12-02-06:34:50-root-INFO: Regularization Change: 0.000 -> 0.382
2024-12-02-06:34:50-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-06:34:50-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:34:50-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-06:34:50-root-INFO: grad norm: 313.527 273.880 152.607
2024-12-02-06:34:51-root-INFO: grad norm: 223.348 205.267 88.033
2024-12-02-06:34:51-root-INFO: grad norm: 182.136 168.724 68.597
2024-12-02-06:34:52-root-INFO: grad norm: 154.952 143.781 57.768
2024-12-02-06:34:52-root-INFO: grad norm: 137.501 130.287 43.952
2024-12-02-06:34:53-root-INFO: grad norm: 126.244 118.684 43.033
2024-12-02-06:34:53-root-INFO: grad norm: 118.949 113.985 34.003
2024-12-02-06:34:54-root-INFO: grad norm: 114.154 108.417 35.734
2024-12-02-06:34:54-root-INFO: Loss Change: 3466.656 -> 3379.539
2024-12-02-06:34:54-root-INFO: Regularization Change: 0.000 -> 0.357
2024-12-02-06:34:54-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-06:34:54-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:34:54-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-06:34:54-root-INFO: grad norm: 325.934 284.290 159.412
2024-12-02-06:34:55-root-INFO: grad norm: 233.244 214.087 92.570
2024-12-02-06:34:55-root-INFO: grad norm: 188.234 173.077 74.001
2024-12-02-06:34:56-root-INFO: grad norm: 158.542 146.530 60.534
2024-12-02-06:34:56-root-INFO: grad norm: 139.171 130.914 47.222
2024-12-02-06:34:57-root-INFO: grad norm: 126.071 117.934 44.560
2024-12-02-06:34:57-root-INFO: grad norm: 117.435 111.868 35.729
2024-12-02-06:34:58-root-INFO: grad norm: 111.493 105.413 36.315
2024-12-02-06:34:58-root-INFO: Loss Change: 3376.858 -> 3288.543
2024-12-02-06:34:58-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-02-06:34:58-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-06:34:58-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:34:58-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-06:34:59-root-INFO: grad norm: 134.378 126.259 46.000
2024-12-02-06:34:59-root-INFO: grad norm: 118.707 112.591 37.609
2024-12-02-06:35:00-root-INFO: grad norm: 110.487 105.772 31.934
2024-12-02-06:35:00-root-INFO: grad norm: 105.274 100.855 30.183
2024-12-02-06:35:01-root-INFO: grad norm: 101.857 98.080 27.481
2024-12-02-06:35:01-root-INFO: grad norm: 99.491 95.693 27.230
2024-12-02-06:35:02-root-INFO: grad norm: 97.887 94.462 25.668
2024-12-02-06:35:02-root-INFO: grad norm: 96.767 93.233 25.913
2024-12-02-06:35:03-root-INFO: Loss Change: 3267.796 -> 3212.653
2024-12-02-06:35:03-root-INFO: Regularization Change: 0.000 -> 0.295
2024-12-02-06:35:03-root-INFO: Undo step: 215
2024-12-02-06:35:03-root-INFO: Undo step: 216
2024-12-02-06:35:03-root-INFO: Undo step: 217
2024-12-02-06:35:03-root-INFO: Undo step: 218
2024-12-02-06:35:03-root-INFO: Undo step: 219
2024-12-02-06:35:03-root-INFO: step: 220 lr_xt 0.00054166
2024-12-02-06:35:03-root-INFO: grad norm: 3775.486 3346.002 1748.875
2024-12-02-06:35:03-root-INFO: grad norm: 2236.877 2013.010 975.403
2024-12-02-06:35:04-root-INFO: grad norm: 1634.218 1478.876 695.410
2024-12-02-06:35:04-root-INFO: grad norm: 1339.956 1224.951 543.119
2024-12-02-06:35:05-root-INFO: grad norm: 1082.217 981.714 455.446
2024-12-02-06:35:05-root-INFO: grad norm: 906.643 834.086 355.389
2024-12-02-06:35:06-root-INFO: grad norm: 748.108 679.299 313.399
2024-12-02-06:35:06-root-INFO: grad norm: 631.891 583.498 242.520
2024-12-02-06:35:07-root-INFO: Loss Change: 7758.083 -> 3902.274
2024-12-02-06:35:07-root-INFO: Regularization Change: 0.000 -> 6.480
2024-12-02-06:35:07-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-02-06:35:07-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00013.
2024-12-02-06:35:07-root-INFO: step: 219 lr_xt 0.00056804
2024-12-02-06:35:07-root-INFO: grad norm: 486.536 441.585 204.255
2024-12-02-06:35:07-root-INFO: grad norm: 402.561 373.286 150.709
2024-12-02-06:35:08-root-INFO: grad norm: 335.973 307.606 135.118
2024-12-02-06:35:08-root-INFO: grad norm: 287.593 267.735 105.012
2024-12-02-06:35:09-root-INFO: grad norm: 250.181 230.907 96.293
2024-12-02-06:35:09-root-INFO: grad norm: 222.638 208.066 79.221
2024-12-02-06:35:10-root-INFO: grad norm: 201.990 187.904 74.108
2024-12-02-06:35:10-root-INFO: grad norm: 186.531 174.867 64.926
2024-12-02-06:35:11-root-INFO: Loss Change: 3844.023 -> 3620.658
2024-12-02-06:35:11-root-INFO: Regularization Change: 0.000 -> 0.901
2024-12-02-06:35:11-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-02-06:35:11-root-INFO: Coefficient of regularization decay: 0.00013 -> 0.00014.
2024-12-02-06:35:11-root-INFO: step: 218 lr_xt 0.00059561
2024-12-02-06:35:11-root-INFO: grad norm: 231.039 208.666 99.185
2024-12-02-06:35:11-root-INFO: grad norm: 196.815 184.446 68.670
2024-12-02-06:35:12-root-INFO: grad norm: 175.523 162.141 67.221
2024-12-02-06:35:12-root-INFO: grad norm: 161.203 151.500 55.083
2024-12-02-06:35:13-root-INFO: grad norm: 151.551 141.616 53.969
2024-12-02-06:35:13-root-INFO: grad norm: 144.903 136.529 48.548
2024-12-02-06:35:14-root-INFO: grad norm: 140.114 131.688 47.857
2024-12-02-06:35:14-root-INFO: grad norm: 136.385 128.778 44.913
2024-12-02-06:35:15-root-INFO: Loss Change: 3593.196 -> 3483.905
2024-12-02-06:35:15-root-INFO: Regularization Change: 0.000 -> 0.500
2024-12-02-06:35:15-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-02-06:35:15-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:35:15-root-INFO: step: 217 lr_xt 0.00062443
2024-12-02-06:35:15-root-INFO: grad norm: 274.936 239.729 134.610
2024-12-02-06:35:15-root-INFO: grad norm: 200.331 188.725 67.197
2024-12-02-06:35:16-root-INFO: grad norm: 169.531 156.117 66.094
2024-12-02-06:35:16-root-INFO: grad norm: 150.007 142.332 47.367
2024-12-02-06:35:17-root-INFO: grad norm: 138.002 129.678 47.204
2024-12-02-06:35:17-root-INFO: grad norm: 130.377 124.305 39.325
2024-12-02-06:35:18-root-INFO: grad norm: 125.415 119.075 39.371
2024-12-02-06:35:18-root-INFO: grad norm: 122.008 116.616 35.870
2024-12-02-06:35:19-root-INFO: Loss Change: 3463.656 -> 3366.545
2024-12-02-06:35:19-root-INFO: Regularization Change: 0.000 -> 0.430
2024-12-02-06:35:19-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-02-06:35:19-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:35:19-root-INFO: step: 216 lr_xt 0.00065452
2024-12-02-06:35:19-root-INFO: grad norm: 290.528 253.675 141.617
2024-12-02-06:35:20-root-INFO: grad norm: 211.419 198.471 72.849
2024-12-02-06:35:20-root-INFO: grad norm: 172.478 157.302 70.744
2024-12-02-06:35:21-root-INFO: grad norm: 147.579 139.483 48.208
2024-12-02-06:35:21-root-INFO: grad norm: 132.523 123.782 47.332
2024-12-02-06:35:22-root-INFO: grad norm: 123.197 117.192 37.993
2024-12-02-06:35:22-root-INFO: grad norm: 117.517 111.194 38.028
2024-12-02-06:35:23-root-INFO: grad norm: 113.869 108.685 33.967
2024-12-02-06:35:23-root-INFO: Loss Change: 3359.175 -> 3266.441
2024-12-02-06:35:23-root-INFO: Regularization Change: 0.000 -> 0.414
2024-12-02-06:35:23-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-02-06:35:23-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:35:23-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-06:35:23-root-INFO: grad norm: 133.976 125.013 48.179
2024-12-02-06:35:24-root-INFO: grad norm: 119.246 114.008 34.954
2024-12-02-06:35:24-root-INFO: grad norm: 112.329 106.816 34.758
2024-12-02-06:35:25-root-INFO: grad norm: 108.447 104.229 29.952
2024-12-02-06:35:25-root-INFO: grad norm: 106.173 101.638 30.702
2024-12-02-06:35:26-root-INFO: grad norm: 104.631 100.640 28.620
2024-12-02-06:35:26-root-INFO: grad norm: 103.491 99.345 28.998
2024-12-02-06:35:27-root-INFO: grad norm: 102.656 98.758 28.021
2024-12-02-06:35:27-root-INFO: Loss Change: 3241.563 -> 3178.873
2024-12-02-06:35:27-root-INFO: Regularization Change: 0.000 -> 0.338
2024-12-02-06:35:27-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-06:35:27-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:35:27-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-06:35:27-root-INFO: grad norm: 201.705 176.581 97.489
2024-12-02-06:35:28-root-INFO: grad norm: 143.496 136.443 44.434
2024-12-02-06:35:28-root-INFO: grad norm: 119.977 111.437 44.456
2024-12-02-06:35:29-root-INFO: grad norm: 108.710 104.481 30.027
2024-12-02-06:35:29-root-INFO: grad norm: 103.240 98.255 31.694
2024-12-02-06:35:30-root-INFO: grad norm: 100.271 96.619 26.813
2024-12-02-06:35:30-root-INFO: grad norm: 98.539 94.562 27.714
2024-12-02-06:35:31-root-INFO: grad norm: 97.382 93.877 25.892
2024-12-02-06:35:31-root-INFO: Loss Change: 3151.788 -> 3083.305
2024-12-02-06:35:31-root-INFO: Regularization Change: 0.000 -> 0.354
2024-12-02-06:35:31-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-06:35:31-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:35:31-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-06:35:32-root-INFO: grad norm: 249.094 217.374 121.640
2024-12-02-06:35:32-root-INFO: grad norm: 165.494 155.308 57.163
2024-12-02-06:35:33-root-INFO: grad norm: 128.091 117.558 50.867
2024-12-02-06:35:33-root-INFO: grad norm: 110.675 105.969 31.929
2024-12-02-06:35:34-root-INFO: grad norm: 102.567 97.154 32.881
2024-12-02-06:35:34-root-INFO: grad norm: 98.578 95.018 26.253
2024-12-02-06:35:35-root-INFO: grad norm: 96.280 92.179 27.801
2024-12-02-06:35:35-root-INFO: grad norm: 94.781 91.443 24.933
2024-12-02-06:35:36-root-INFO: Loss Change: 3089.032 -> 3009.339
2024-12-02-06:35:36-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-06:35:36-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-06:35:36-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:35:36-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-06:35:36-root-INFO: grad norm: 106.525 99.893 37.000
2024-12-02-06:35:36-root-INFO: grad norm: 99.921 96.216 26.958
2024-12-02-06:35:37-root-INFO: grad norm: 96.637 92.067 29.367
2024-12-02-06:35:37-root-INFO: grad norm: 94.623 91.242 25.071
2024-12-02-06:35:38-root-INFO: grad norm: 93.185 89.315 26.573
2024-12-02-06:35:38-root-INFO: grad norm: 92.049 88.799 24.241
2024-12-02-06:35:39-root-INFO: grad norm: 91.168 87.638 25.123
2024-12-02-06:35:39-root-INFO: grad norm: 90.353 87.182 23.725
2024-12-02-06:35:40-root-INFO: Loss Change: 2991.896 -> 2938.142
2024-12-02-06:35:40-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-02-06:35:40-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-06:35:40-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-06:35:40-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-06:35:40-root-INFO: grad norm: 256.764 220.166 132.117
2024-12-02-06:35:40-root-INFO: grad norm: 150.073 140.231 53.454
2024-12-02-06:35:41-root-INFO: grad norm: 111.972 103.137 43.594
2024-12-02-06:35:41-root-INFO: grad norm: 98.226 93.159 31.139
2024-12-02-06:35:42-root-INFO: grad norm: 94.051 89.471 28.990
2024-12-02-06:35:42-root-INFO: grad norm: 93.416 89.142 27.934
2024-12-02-06:35:43-root-INFO: grad norm: 94.732 90.240 28.827
2024-12-02-06:35:43-root-INFO: grad norm: 97.606 92.851 30.096
2024-12-02-06:35:44-root-INFO: Loss Change: 2924.880 -> 2845.985
2024-12-02-06:35:44-root-INFO: Regularization Change: 0.000 -> 0.416
2024-12-02-06:35:44-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-06:35:44-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:35:44-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-06:35:44-root-INFO: grad norm: 124.893 115.401 47.759
2024-12-02-06:35:45-root-INFO: grad norm: 110.137 102.697 39.793
2024-12-02-06:35:45-root-INFO: grad norm: 113.640 107.433 37.042
2024-12-02-06:35:45-root-INFO: grad norm: 124.978 115.600 47.500
2024-12-02-06:35:46-root-INFO: grad norm: 146.325 136.153 53.603
2024-12-02-06:35:46-root-INFO: grad norm: 175.517 161.090 69.687
2024-12-02-06:35:47-root-INFO: grad norm: 224.824 205.486 91.221
2024-12-02-06:35:47-root-INFO: Loss too large (2801.500->2802.984)! Learning rate decreased to 0.00069.
2024-12-02-06:35:48-root-INFO: grad norm: 200.004 183.473 79.618
2024-12-02-06:35:48-root-INFO: Loss Change: 2833.425 -> 2789.002
2024-12-02-06:35:48-root-INFO: Regularization Change: 0.000 -> 0.308
2024-12-02-06:35:48-root-INFO: Undo step: 210
2024-12-02-06:35:48-root-INFO: Undo step: 211
2024-12-02-06:35:48-root-INFO: Undo step: 212
2024-12-02-06:35:48-root-INFO: Undo step: 213
2024-12-02-06:35:48-root-INFO: Undo step: 214
2024-12-02-06:35:48-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-06:35:48-root-INFO: grad norm: 2818.583 2353.260 1551.315
2024-12-02-06:35:49-root-INFO: grad norm: 1835.222 1685.784 725.378
2024-12-02-06:35:49-root-INFO: grad norm: 2074.382 1778.867 1067.096
2024-12-02-06:35:50-root-INFO: grad norm: 1363.319 1282.895 461.325
2024-12-02-06:35:50-root-INFO: grad norm: 389.056 346.072 177.761
2024-12-02-06:35:51-root-INFO: grad norm: 263.806 245.817 95.747
2024-12-02-06:35:51-root-INFO: grad norm: 221.111 203.445 86.604
2024-12-02-06:35:52-root-INFO: grad norm: 199.956 186.131 73.060
2024-12-02-06:35:52-root-INFO: Loss Change: 6994.355 -> 3383.055
2024-12-02-06:35:52-root-INFO: Regularization Change: 0.000 -> 9.127
2024-12-02-06:35:52-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-06:35:52-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:35:52-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-06:35:52-root-INFO: grad norm: 246.862 224.253 103.206
2024-12-02-06:35:53-root-INFO: grad norm: 199.042 183.449 77.229
2024-12-02-06:35:53-root-INFO: grad norm: 177.077 165.629 62.635
2024-12-02-06:35:54-root-INFO: grad norm: 163.872 152.748 59.347
2024-12-02-06:35:54-root-INFO: grad norm: 155.214 146.245 51.996
2024-12-02-06:35:55-root-INFO: grad norm: 149.007 139.921 51.238
2024-12-02-06:35:55-root-INFO: grad norm: 144.411 136.493 47.162
2024-12-02-06:35:56-root-INFO: grad norm: 140.434 132.489 46.568
2024-12-02-06:35:56-root-INFO: Loss Change: 3337.412 -> 3191.176
2024-12-02-06:35:56-root-INFO: Regularization Change: 0.000 -> 0.805
2024-12-02-06:35:56-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-06:35:56-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:35:56-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-06:35:56-root-INFO: grad norm: 316.640 284.784 138.416
2024-12-02-06:35:57-root-INFO: grad norm: 243.498 219.212 106.008
2024-12-02-06:35:57-root-INFO: grad norm: 200.640 186.716 73.443
2024-12-02-06:35:58-root-INFO: grad norm: 173.610 157.518 72.998
2024-12-02-06:35:58-root-INFO: grad norm: 156.547 147.893 51.328
2024-12-02-06:35:59-root-INFO: grad norm: 145.572 134.071 56.713
2024-12-02-06:35:59-root-INFO: grad norm: 138.112 131.127 43.366
2024-12-02-06:36:00-root-INFO: grad norm: 132.809 123.729 48.262
2024-12-02-06:36:00-root-INFO: Loss Change: 3193.535 -> 3066.268
2024-12-02-06:36:00-root-INFO: Regularization Change: 0.000 -> 0.664
2024-12-02-06:36:00-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-06:36:00-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:36:00-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-06:36:01-root-INFO: grad norm: 145.488 136.609 50.047
2024-12-02-06:36:01-root-INFO: grad norm: 135.385 125.584 50.573
2024-12-02-06:36:02-root-INFO: grad norm: 130.403 123.320 42.392
2024-12-02-06:36:02-root-INFO: grad norm: 127.391 119.014 45.433
2024-12-02-06:36:03-root-INFO: grad norm: 125.634 118.837 40.764
2024-12-02-06:36:03-root-INFO: grad norm: 124.633 116.894 43.234
2024-12-02-06:36:04-root-INFO: grad norm: 124.482 117.473 41.180
2024-12-02-06:36:04-root-INFO: grad norm: 124.819 117.175 43.009
2024-12-02-06:36:05-root-INFO: Loss Change: 3046.000 -> 2967.303
2024-12-02-06:36:05-root-INFO: Regularization Change: 0.000 -> 0.495
2024-12-02-06:36:05-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-06:36:05-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-06:36:05-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-06:36:05-root-INFO: grad norm: 337.591 300.592 153.663
2024-12-02-06:36:06-root-INFO: grad norm: 291.873 262.832 126.921
2024-12-02-06:36:06-root-INFO: grad norm: 303.952 274.900 129.680
2024-12-02-06:36:07-root-INFO: grad norm: 332.452 301.807 139.415
2024-12-02-06:36:07-root-INFO: grad norm: 388.536 350.177 168.335
2024-12-02-06:36:07-root-INFO: Loss too large (2910.468->2917.280)! Learning rate decreased to 0.00066.
2024-12-02-06:36:08-root-INFO: grad norm: 303.302 277.133 123.244
2024-12-02-06:36:08-root-INFO: grad norm: 225.314 205.593 92.186
2024-12-02-06:36:09-root-INFO: grad norm: 184.060 169.793 71.051
2024-12-02-06:36:09-root-INFO: Loss Change: 2954.181 -> 2863.797
2024-12-02-06:36:09-root-INFO: Regularization Change: 0.000 -> 0.418
2024-12-02-06:36:09-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-06:36:09-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:36:09-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-06:36:09-root-INFO: grad norm: 209.897 192.393 83.913
2024-12-02-06:36:10-root-INFO: grad norm: 225.162 203.577 96.199
2024-12-02-06:36:10-root-INFO: grad norm: 268.242 244.117 111.179
2024-12-02-06:36:11-root-INFO: Loss too large (2836.825->2837.185)! Learning rate decreased to 0.00069.
2024-12-02-06:36:11-root-INFO: grad norm: 224.424 204.948 91.445
2024-12-02-06:36:12-root-INFO: grad norm: 186.924 171.978 73.239
2024-12-02-06:36:12-root-INFO: grad norm: 163.950 151.340 63.055
2024-12-02-06:36:13-root-INFO: grad norm: 144.808 134.284 54.197
2024-12-02-06:36:13-root-INFO: grad norm: 132.113 123.395 47.198
2024-12-02-06:36:13-root-INFO: Loss Change: 2849.291 -> 2791.397
2024-12-02-06:36:13-root-INFO: Regularization Change: 0.000 -> 0.308
2024-12-02-06:36:13-root-INFO: Undo step: 210
2024-12-02-06:36:13-root-INFO: Undo step: 211
2024-12-02-06:36:13-root-INFO: Undo step: 212
2024-12-02-06:36:13-root-INFO: Undo step: 213
2024-12-02-06:36:13-root-INFO: Undo step: 214
2024-12-02-06:36:14-root-INFO: step: 215 lr_xt 0.00068596
2024-12-02-06:36:14-root-INFO: grad norm: 2917.939 2469.747 1553.937
2024-12-02-06:36:14-root-INFO: grad norm: 1588.921 1498.101 529.493
2024-12-02-06:36:15-root-INFO: grad norm: 1233.818 1163.547 410.445
2024-12-02-06:36:15-root-INFO: grad norm: 1066.617 996.297 380.872
2024-12-02-06:36:16-root-INFO: grad norm: 910.734 860.185 299.195
2024-12-02-06:36:16-root-INFO: grad norm: 795.543 741.562 288.055
2024-12-02-06:36:17-root-INFO: grad norm: 689.855 651.654 226.376
2024-12-02-06:36:17-root-INFO: grad norm: 607.861 565.691 222.462
2024-12-02-06:36:18-root-INFO: Loss Change: 6472.103 -> 3373.259
2024-12-02-06:36:18-root-INFO: Regularization Change: 0.000 -> 7.374
2024-12-02-06:36:18-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-02-06:36:18-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:36:18-root-INFO: step: 214 lr_xt 0.00071879
2024-12-02-06:36:18-root-INFO: grad norm: 390.384 371.095 121.197
2024-12-02-06:36:18-root-INFO: grad norm: 335.698 310.341 127.990
2024-12-02-06:36:19-root-INFO: grad norm: 296.089 280.652 94.358
2024-12-02-06:36:19-root-INFO: grad norm: 264.540 245.419 98.748
2024-12-02-06:36:20-root-INFO: grad norm: 237.456 225.375 74.776
2024-12-02-06:36:20-root-INFO: grad norm: 215.508 200.419 79.220
2024-12-02-06:36:21-root-INFO: grad norm: 196.686 187.054 60.799
2024-12-02-06:36:21-root-INFO: grad norm: 181.142 168.976 65.265
2024-12-02-06:36:22-root-INFO: Loss Change: 3302.988 -> 3146.665
2024-12-02-06:36:22-root-INFO: Regularization Change: 0.000 -> 0.784
2024-12-02-06:36:22-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-02-06:36:22-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:36:22-root-INFO: step: 213 lr_xt 0.00075308
2024-12-02-06:36:22-root-INFO: grad norm: 224.583 197.674 106.594
2024-12-02-06:36:23-root-INFO: grad norm: 181.687 169.744 64.785
2024-12-02-06:36:23-root-INFO: grad norm: 162.451 150.197 61.898
2024-12-02-06:36:24-root-INFO: grad norm: 149.218 141.661 46.883
2024-12-02-06:36:24-root-INFO: grad norm: 139.508 131.017 47.927
2024-12-02-06:36:25-root-INFO: grad norm: 131.983 126.197 38.650
2024-12-02-06:36:25-root-INFO: grad norm: 125.836 119.032 40.817
2024-12-02-06:36:26-root-INFO: grad norm: 120.717 115.880 33.832
2024-12-02-06:36:26-root-INFO: Loss Change: 3134.353 -> 3031.974
2024-12-02-06:36:26-root-INFO: Regularization Change: 0.000 -> 0.566
2024-12-02-06:36:26-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-02-06:36:26-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00014.
2024-12-02-06:36:26-root-INFO: step: 212 lr_xt 0.00078886
2024-12-02-06:36:26-root-INFO: grad norm: 139.463 130.533 49.102
2024-12-02-06:36:27-root-INFO: grad norm: 127.623 122.438 36.008
2024-12-02-06:36:27-root-INFO: grad norm: 119.994 113.335 39.419
2024-12-02-06:36:28-root-INFO: grad norm: 114.216 109.874 31.194
2024-12-02-06:36:28-root-INFO: grad norm: 109.728 104.242 34.261
2024-12-02-06:36:29-root-INFO: grad norm: 106.218 102.326 28.491
2024-12-02-06:36:29-root-INFO: grad norm: 103.405 98.669 30.935
2024-12-02-06:36:30-root-INFO: grad norm: 101.094 97.461 26.856
2024-12-02-06:36:30-root-INFO: Loss Change: 3014.253 -> 2945.712
2024-12-02-06:36:30-root-INFO: Regularization Change: 0.000 -> 0.425
2024-12-02-06:36:30-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-02-06:36:30-root-INFO: Coefficient of regularization decay: 0.00014 -> 0.00015.
2024-12-02-06:36:30-root-INFO: step: 211 lr_xt 0.00082622
2024-12-02-06:36:30-root-INFO: grad norm: 310.326 273.319 146.965
2024-12-02-06:36:31-root-INFO: grad norm: 243.113 226.112 89.316
2024-12-02-06:36:31-root-INFO: grad norm: 204.613 186.747 83.619
2024-12-02-06:36:32-root-INFO: grad norm: 175.204 164.841 59.363
2024-12-02-06:36:32-root-INFO: grad norm: 153.304 141.827 58.201
2024-12-02-06:36:33-root-INFO: grad norm: 136.266 129.382 42.764
2024-12-02-06:36:33-root-INFO: grad norm: 123.324 115.208 43.999
2024-12-02-06:36:34-root-INFO: grad norm: 113.405 108.549 32.830
2024-12-02-06:36:34-root-INFO: Loss Change: 2935.420 -> 2844.741
2024-12-02-06:36:34-root-INFO: Regularization Change: 0.000 -> 0.464
2024-12-02-06:36:34-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-02-06:36:34-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:36:34-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-06:36:35-root-INFO: grad norm: 182.907 167.739 72.930
2024-12-02-06:36:35-root-INFO: grad norm: 155.378 145.792 53.729
2024-12-02-06:36:35-root-INFO: grad norm: 137.013 127.095 51.180
2024-12-02-06:36:36-root-INFO: grad norm: 122.908 116.332 39.666
2024-12-02-06:36:36-root-INFO: grad norm: 112.466 105.154 39.893
2024-12-02-06:36:37-root-INFO: grad norm: 104.590 99.894 30.986
2024-12-02-06:36:37-root-INFO: grad norm: 98.746 93.123 32.846
2024-12-02-06:36:38-root-INFO: grad norm: 94.362 90.720 25.963
2024-12-02-06:36:38-root-INFO: Loss Change: 2832.928 -> 2773.574
2024-12-02-06:36:38-root-INFO: Regularization Change: 0.000 -> 0.374
2024-12-02-06:36:38-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-06:36:38-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:36:38-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-06:36:38-root-INFO: grad norm: 227.555 201.138 106.420
2024-12-02-06:36:39-root-INFO: grad norm: 189.066 171.417 79.765
2024-12-02-06:36:39-root-INFO: grad norm: 169.225 152.378 73.606
2024-12-02-06:36:40-root-INFO: grad norm: 155.213 141.621 63.518
2024-12-02-06:36:40-root-INFO: grad norm: 144.834 130.955 61.868
2024-12-02-06:36:41-root-INFO: grad norm: 136.627 125.611 53.749
2024-12-02-06:36:41-root-INFO: grad norm: 130.207 117.990 55.066
2024-12-02-06:36:42-root-INFO: grad norm: 125.136 115.743 47.565
2024-12-02-06:36:42-root-INFO: Loss Change: 2760.039 -> 2695.507
2024-12-02-06:36:42-root-INFO: Regularization Change: 0.000 -> 0.397
2024-12-02-06:36:42-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-06:36:42-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:36:42-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-06:36:43-root-INFO: grad norm: 282.419 248.687 133.848
2024-12-02-06:36:43-root-INFO: grad norm: 268.747 238.583 123.706
2024-12-02-06:36:44-root-INFO: grad norm: 270.755 239.632 126.036
2024-12-02-06:36:44-root-INFO: grad norm: 277.193 245.884 127.972
2024-12-02-06:36:44-root-INFO: grad norm: 287.548 254.055 134.683
2024-12-02-06:36:45-root-INFO: grad norm: 298.545 265.011 137.471
2024-12-02-06:36:45-root-INFO: grad norm: 311.511 274.990 146.355
2024-12-02-06:36:46-root-INFO: grad norm: 323.708 287.389 148.979
2024-12-02-06:36:46-root-INFO: Loss Change: 2697.603 -> 2650.688
2024-12-02-06:36:46-root-INFO: Regularization Change: 0.000 -> 0.400
2024-12-02-06:36:46-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-06:36:46-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:36:46-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-06:36:47-root-INFO: grad norm: 455.015 402.489 212.230
2024-12-02-06:36:47-root-INFO: Loss too large (2649.003->2651.329)! Learning rate decreased to 0.00080.
2024-12-02-06:36:47-root-INFO: grad norm: 316.120 280.669 145.452
2024-12-02-06:36:48-root-INFO: grad norm: 224.477 199.872 102.182
2024-12-02-06:36:48-root-INFO: grad norm: 169.114 153.706 70.526
2024-12-02-06:36:49-root-INFO: grad norm: 132.671 119.870 56.858
2024-12-02-06:36:49-root-INFO: grad norm: 110.507 103.318 39.207
2024-12-02-06:36:50-root-INFO: grad norm: 96.517 89.198 36.867
2024-12-02-06:36:50-root-INFO: grad norm: 88.021 84.040 26.172
2024-12-02-06:36:51-root-INFO: Loss Change: 2649.003 -> 2561.955
2024-12-02-06:36:51-root-INFO: Regularization Change: 0.000 -> 0.290
2024-12-02-06:36:51-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-06:36:51-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:36:51-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-06:36:51-root-INFO: grad norm: 179.308 157.980 84.816
2024-12-02-06:36:51-root-INFO: grad norm: 190.214 171.267 82.758
2024-12-02-06:36:52-root-INFO: grad norm: 232.935 206.371 108.028
2024-12-02-06:36:52-root-INFO: Loss too large (2537.907->2539.816)! Learning rate decreased to 0.00084.
2024-12-02-06:36:52-root-INFO: grad norm: 200.922 181.839 85.464
2024-12-02-06:36:53-root-INFO: grad norm: 175.891 157.073 79.156
2024-12-02-06:36:53-root-INFO: grad norm: 157.122 143.952 62.971
2024-12-02-06:36:54-root-INFO: grad norm: 141.927 127.502 62.343
2024-12-02-06:36:54-root-INFO: grad norm: 130.354 120.633 49.393
2024-12-02-06:36:55-root-INFO: Loss Change: 2548.291 -> 2501.554
2024-12-02-06:36:55-root-INFO: Regularization Change: 0.000 -> 0.297
2024-12-02-06:36:55-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-06:36:55-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:36:55-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-06:36:55-root-INFO: grad norm: 230.711 201.979 111.500
2024-12-02-06:36:55-root-INFO: grad norm: 293.603 261.350 133.788
2024-12-02-06:36:56-root-INFO: Loss too large (2490.575->2503.495)! Learning rate decreased to 0.00088.
2024-12-02-06:36:56-root-INFO: grad norm: 286.222 254.894 130.199
2024-12-02-06:36:57-root-INFO: grad norm: 282.811 254.512 123.311
2024-12-02-06:36:57-root-INFO: grad norm: 280.193 250.109 126.309
2024-12-02-06:36:58-root-INFO: grad norm: 278.317 251.066 120.110
2024-12-02-06:36:58-root-INFO: grad norm: 276.848 247.117 124.813
2024-12-02-06:36:58-root-INFO: grad norm: 275.901 249.115 118.588
2024-12-02-06:36:59-root-INFO: Loss Change: 2492.720 -> 2454.845
2024-12-02-06:36:59-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-02-06:36:59-root-INFO: Undo step: 205
2024-12-02-06:36:59-root-INFO: Undo step: 206
2024-12-02-06:36:59-root-INFO: Undo step: 207
2024-12-02-06:36:59-root-INFO: Undo step: 208
2024-12-02-06:36:59-root-INFO: Undo step: 209
2024-12-02-06:36:59-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-06:36:59-root-INFO: grad norm: 1726.434 1558.309 743.132
2024-12-02-06:37:00-root-INFO: grad norm: 927.569 828.974 416.157
2024-12-02-06:37:00-root-INFO: grad norm: 532.137 486.441 215.743
2024-12-02-06:37:00-root-INFO: grad norm: 371.592 331.053 168.774
2024-12-02-06:37:01-root-INFO: grad norm: 286.438 262.346 114.982
2024-12-02-06:37:02-root-INFO: grad norm: 240.691 218.487 100.973
2024-12-02-06:37:02-root-INFO: grad norm: 212.687 195.796 83.066
2024-12-02-06:37:03-root-INFO: grad norm: 193.726 178.296 75.765
2024-12-02-06:37:03-root-INFO: Loss Change: 4988.135 -> 2946.962
2024-12-02-06:37:03-root-INFO: Regularization Change: 0.000 -> 7.769
2024-12-02-06:37:03-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-06:37:03-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:03-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-06:37:03-root-INFO: grad norm: 222.649 203.560 90.201
2024-12-02-06:37:04-root-INFO: grad norm: 171.837 160.124 62.356
2024-12-02-06:37:04-root-INFO: grad norm: 156.071 145.417 56.673
2024-12-02-06:37:05-root-INFO: grad norm: 145.662 135.915 52.389
2024-12-02-06:37:05-root-INFO: grad norm: 137.338 128.108 49.500
2024-12-02-06:37:06-root-INFO: grad norm: 130.261 121.514 46.927
2024-12-02-06:37:06-root-INFO: grad norm: 124.078 115.746 44.700
2024-12-02-06:37:07-root-INFO: grad norm: 118.696 110.734 42.742
2024-12-02-06:37:07-root-INFO: Loss Change: 2913.547 -> 2758.515
2024-12-02-06:37:07-root-INFO: Regularization Change: 0.000 -> 1.077
2024-12-02-06:37:07-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-06:37:07-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:07-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-06:37:07-root-INFO: grad norm: 195.536 174.916 87.400
2024-12-02-06:37:08-root-INFO: grad norm: 136.521 125.241 54.338
2024-12-02-06:37:08-root-INFO: grad norm: 117.036 108.954 42.738
2024-12-02-06:37:09-root-INFO: grad norm: 107.880 100.212 39.946
2024-12-02-06:37:09-root-INFO: grad norm: 102.666 96.115 36.086
2024-12-02-06:37:10-root-INFO: grad norm: 99.230 92.745 35.284
2024-12-02-06:37:10-root-INFO: grad norm: 96.617 90.644 33.443
2024-12-02-06:37:11-root-INFO: grad norm: 94.508 88.628 32.815
2024-12-02-06:37:11-root-INFO: Loss Change: 2749.066 -> 2655.717
2024-12-02-06:37:11-root-INFO: Regularization Change: 0.000 -> 0.649
2024-12-02-06:37:11-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-06:37:11-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:11-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-06:37:11-root-INFO: grad norm: 136.862 124.035 57.850
2024-12-02-06:37:12-root-INFO: grad norm: 101.969 93.991 39.538
2024-12-02-06:37:12-root-INFO: grad norm: 94.145 88.507 32.089
2024-12-02-06:37:13-root-INFO: grad norm: 90.500 84.532 32.319
2024-12-02-06:37:13-root-INFO: grad norm: 88.183 83.212 29.190
2024-12-02-06:37:14-root-INFO: grad norm: 86.461 81.290 29.451
2024-12-02-06:37:14-root-INFO: grad norm: 85.048 80.404 27.719
2024-12-02-06:37:15-root-INFO: grad norm: 83.865 79.152 27.718
2024-12-02-06:37:15-root-INFO: Loss Change: 2626.206 -> 2558.995
2024-12-02-06:37:15-root-INFO: Regularization Change: 0.000 -> 0.513
2024-12-02-06:37:15-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-06:37:15-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:16-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-06:37:16-root-INFO: grad norm: 137.429 123.974 59.306
2024-12-02-06:37:16-root-INFO: grad norm: 99.918 91.981 39.027
2024-12-02-06:37:17-root-INFO: grad norm: 90.919 85.633 30.549
2024-12-02-06:37:17-root-INFO: grad norm: 87.045 81.349 30.969
2024-12-02-06:37:18-root-INFO: grad norm: 84.744 80.265 27.183
2024-12-02-06:37:18-root-INFO: grad norm: 83.107 78.199 28.137
2024-12-02-06:37:19-root-INFO: grad norm: 81.793 77.614 25.809
2024-12-02-06:37:19-root-INFO: grad norm: 80.700 76.201 26.569
2024-12-02-06:37:19-root-INFO: Loss Change: 2540.837 -> 2481.016
2024-12-02-06:37:19-root-INFO: Regularization Change: 0.000 -> 0.468
2024-12-02-06:37:19-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-06:37:19-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:20-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-06:37:20-root-INFO: grad norm: 153.371 136.364 70.198
2024-12-02-06:37:20-root-INFO: grad norm: 114.671 103.987 48.334
2024-12-02-06:37:21-root-INFO: grad norm: 110.494 102.621 40.961
2024-12-02-06:37:21-root-INFO: grad norm: 111.902 102.640 44.577
2024-12-02-06:37:22-root-INFO: grad norm: 116.008 107.584 43.399
2024-12-02-06:37:22-root-INFO: grad norm: 121.485 111.518 48.189
2024-12-02-06:37:23-root-INFO: grad norm: 129.292 119.183 50.117
2024-12-02-06:37:23-root-INFO: grad norm: 138.261 126.602 55.568
2024-12-02-06:37:24-root-INFO: Loss Change: 2467.006 -> 2411.328
2024-12-02-06:37:24-root-INFO: Regularization Change: 0.000 -> 0.471
2024-12-02-06:37:24-root-INFO: Undo step: 205
2024-12-02-06:37:24-root-INFO: Undo step: 206
2024-12-02-06:37:24-root-INFO: Undo step: 207
2024-12-02-06:37:24-root-INFO: Undo step: 208
2024-12-02-06:37:24-root-INFO: Undo step: 209
2024-12-02-06:37:24-root-INFO: step: 210 lr_xt 0.00086520
2024-12-02-06:37:24-root-INFO: grad norm: 2942.789 2572.655 1428.794
2024-12-02-06:37:24-root-INFO: grad norm: 1776.431 1694.790 532.347
2024-12-02-06:37:25-root-INFO: grad norm: 1263.093 1199.018 397.191
2024-12-02-06:37:25-root-INFO: grad norm: 912.196 877.596 248.851
2024-12-02-06:37:26-root-INFO: grad norm: 586.968 545.195 217.472
2024-12-02-06:37:26-root-INFO: grad norm: 404.958 384.900 125.870
2024-12-02-06:37:27-root-INFO: grad norm: 288.186 265.201 112.781
2024-12-02-06:37:28-root-INFO: grad norm: 223.468 209.109 78.814
2024-12-02-06:37:28-root-INFO: Loss Change: 6513.957 -> 2859.029
2024-12-02-06:37:28-root-INFO: Regularization Change: 0.000 -> 10.317
2024-12-02-06:37:28-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-02-06:37:28-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:28-root-INFO: step: 209 lr_xt 0.00090588
2024-12-02-06:37:28-root-INFO: grad norm: 216.888 195.613 93.681
2024-12-02-06:37:29-root-INFO: grad norm: 157.397 146.257 58.159
2024-12-02-06:37:29-root-INFO: grad norm: 144.258 133.932 53.596
2024-12-02-06:37:30-root-INFO: grad norm: 135.302 125.823 49.750
2024-12-02-06:37:30-root-INFO: grad norm: 128.300 119.526 46.629
2024-12-02-06:37:31-root-INFO: grad norm: 122.645 114.471 44.026
2024-12-02-06:37:31-root-INFO: grad norm: 117.888 110.206 41.860
2024-12-02-06:37:32-root-INFO: grad norm: 113.801 106.551 39.969
2024-12-02-06:37:32-root-INFO: Loss Change: 2823.557 -> 2683.047
2024-12-02-06:37:32-root-INFO: Regularization Change: 0.000 -> 0.964
2024-12-02-06:37:32-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-02-06:37:32-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:32-root-INFO: step: 208 lr_xt 0.00094831
2024-12-02-06:37:32-root-INFO: grad norm: 214.602 187.800 103.852
2024-12-02-06:37:33-root-INFO: grad norm: 127.655 119.793 44.108
2024-12-02-06:37:33-root-INFO: grad norm: 109.470 101.617 40.714
2024-12-02-06:37:34-root-INFO: grad norm: 102.883 96.701 35.126
2024-12-02-06:37:34-root-INFO: grad norm: 99.210 93.212 33.972
2024-12-02-06:37:35-root-INFO: grad norm: 96.605 91.170 31.948
2024-12-02-06:37:35-root-INFO: grad norm: 94.543 89.278 31.109
2024-12-02-06:37:36-root-INFO: grad norm: 92.715 87.727 30.000
2024-12-02-06:37:36-root-INFO: Loss Change: 2671.229 -> 2578.051
2024-12-02-06:37:36-root-INFO: Regularization Change: 0.000 -> 0.622
2024-12-02-06:37:36-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-02-06:37:36-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:36-root-INFO: step: 207 lr_xt 0.00100094
2024-12-02-06:37:36-root-INFO: grad norm: 147.861 132.063 66.500
2024-12-02-06:37:37-root-INFO: grad norm: 97.387 93.128 28.486
2024-12-02-06:37:37-root-INFO: grad norm: 88.426 84.009 27.596
2024-12-02-06:37:38-root-INFO: grad norm: 85.620 82.061 24.431
2024-12-02-06:37:38-root-INFO: grad norm: 84.040 80.414 24.419
2024-12-02-06:37:39-root-INFO: grad norm: 82.819 79.401 23.545
2024-12-02-06:37:39-root-INFO: grad norm: 81.757 78.377 23.268
2024-12-02-06:37:40-root-INFO: grad norm: 80.816 77.523 22.834
2024-12-02-06:37:40-root-INFO: Loss Change: 2546.873 -> 2482.987
2024-12-02-06:37:40-root-INFO: Regularization Change: 0.000 -> 0.475
2024-12-02-06:37:40-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-02-06:37:40-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:40-root-INFO: step: 206 lr_xt 0.00104745
2024-12-02-06:37:40-root-INFO: grad norm: 155.914 138.004 72.554
2024-12-02-06:37:41-root-INFO: grad norm: 95.512 90.868 29.418
2024-12-02-06:37:41-root-INFO: grad norm: 83.003 78.392 27.280
2024-12-02-06:37:42-root-INFO: grad norm: 79.295 75.980 22.688
2024-12-02-06:37:42-root-INFO: grad norm: 77.681 74.259 22.801
2024-12-02-06:37:43-root-INFO: grad norm: 76.660 73.506 21.764
2024-12-02-06:37:43-root-INFO: grad norm: 75.800 72.647 21.636
2024-12-02-06:37:44-root-INFO: grad norm: 75.061 72.015 21.167
2024-12-02-06:37:44-root-INFO: Loss Change: 2467.299 -> 2407.400
2024-12-02-06:37:44-root-INFO: Regularization Change: 0.000 -> 0.450
2024-12-02-06:37:44-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-02-06:37:44-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00015.
2024-12-02-06:37:44-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-06:37:44-root-INFO: grad norm: 169.949 149.293 81.204
2024-12-02-06:37:45-root-INFO: grad norm: 101.713 96.199 33.034
2024-12-02-06:37:45-root-INFO: grad norm: 86.235 81.098 29.318
2024-12-02-06:37:46-root-INFO: grad norm: 79.503 76.206 22.655
2024-12-02-06:37:46-root-INFO: grad norm: 75.795 72.323 22.678
2024-12-02-06:37:47-root-INFO: grad norm: 73.623 70.807 20.166
2024-12-02-06:37:48-root-INFO: grad norm: 72.194 69.175 20.661
2024-12-02-06:37:48-root-INFO: grad norm: 71.134 68.460 19.320
2024-12-02-06:37:48-root-INFO: Loss Change: 2395.417 -> 2336.736
2024-12-02-06:37:48-root-INFO: Regularization Change: 0.000 -> 0.441
2024-12-02-06:37:48-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-06:37:48-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-06:37:49-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-06:37:49-root-INFO: grad norm: 233.819 211.151 100.432
2024-12-02-06:37:49-root-INFO: grad norm: 145.098 136.344 49.636
2024-12-02-06:37:50-root-INFO: grad norm: 127.641 119.806 44.031
2024-12-02-06:37:50-root-INFO: grad norm: 119.428 113.502 37.153
2024-12-02-06:37:51-root-INFO: grad norm: 113.733 107.300 37.708
2024-12-02-06:37:51-root-INFO: grad norm: 109.798 104.827 32.663
2024-12-02-06:37:52-root-INFO: grad norm: 106.908 101.027 34.971
2024-12-02-06:37:52-root-INFO: grad norm: 105.060 100.539 30.488
2024-12-02-06:37:53-root-INFO: Loss Change: 2336.546 -> 2273.395
2024-12-02-06:37:53-root-INFO: Regularization Change: 0.000 -> 0.450
2024-12-02-06:37:53-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-06:37:53-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:37:53-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-06:37:53-root-INFO: grad norm: 154.674 143.458 57.826
2024-12-02-06:37:53-root-INFO: grad norm: 151.720 144.392 46.583
2024-12-02-06:37:54-root-INFO: grad norm: 156.077 147.286 51.642
2024-12-02-06:37:55-root-INFO: grad norm: 162.166 154.883 48.053
2024-12-02-06:37:55-root-INFO: grad norm: 170.278 160.852 55.869
2024-12-02-06:37:56-root-INFO: grad norm: 179.316 171.335 52.902
2024-12-02-06:37:56-root-INFO: grad norm: 190.753 180.221 62.506
2024-12-02-06:37:57-root-INFO: grad norm: 202.607 193.593 59.762
2024-12-02-06:37:57-root-INFO: Loss Change: 2262.262 -> 2227.919
2024-12-02-06:37:57-root-INFO: Regularization Change: 0.000 -> 0.390
2024-12-02-06:37:57-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-06:37:57-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:37:57-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-06:37:57-root-INFO: grad norm: 806.033 767.227 247.085
2024-12-02-06:37:57-root-INFO: Loss too large (2310.315->2342.932)! Learning rate decreased to 0.00100.
2024-12-02-06:37:58-root-INFO: grad norm: 386.535 371.386 107.155
2024-12-02-06:37:58-root-INFO: grad norm: 282.782 270.424 82.685
2024-12-02-06:37:59-root-INFO: grad norm: 215.952 207.039 61.401
2024-12-02-06:37:59-root-INFO: grad norm: 158.676 151.623 46.783
2024-12-02-06:38:00-root-INFO: grad norm: 118.921 113.446 35.667
2024-12-02-06:38:00-root-INFO: grad norm: 97.771 92.526 31.591
2024-12-02-06:38:01-root-INFO: grad norm: 85.193 81.454 24.962
2024-12-02-06:38:01-root-INFO: Loss Change: 2310.315 -> 2159.536
2024-12-02-06:38:01-root-INFO: Regularization Change: 0.000 -> 0.421
2024-12-02-06:38:01-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-06:38:01-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:01-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-06:38:02-root-INFO: grad norm: 85.089 79.757 29.646
2024-12-02-06:38:02-root-INFO: grad norm: 84.678 80.294 26.893
2024-12-02-06:38:03-root-INFO: grad norm: 94.744 90.266 28.782
2024-12-02-06:38:03-root-INFO: grad norm: 112.308 107.541 32.373
2024-12-02-06:38:04-root-INFO: grad norm: 142.256 135.490 43.352
2024-12-02-06:38:04-root-INFO: grad norm: 184.255 176.830 51.778
2024-12-02-06:38:04-root-INFO: Loss too large (2136.551->2138.901)! Learning rate decreased to 0.00105.
2024-12-02-06:38:05-root-INFO: grad norm: 168.759 160.947 50.751
2024-12-02-06:38:05-root-INFO: grad norm: 156.869 150.895 42.877
2024-12-02-06:38:06-root-INFO: Loss Change: 2155.083 -> 2121.300
2024-12-02-06:38:06-root-INFO: Regularization Change: 0.000 -> 0.364
2024-12-02-06:38:06-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-06:38:06-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:06-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-06:38:06-root-INFO: grad norm: 664.874 639.526 181.835
2024-12-02-06:38:06-root-INFO: Loss too large (2168.670->2242.372)! Learning rate decreased to 0.00110.
2024-12-02-06:38:06-root-INFO: Loss too large (2168.670->2188.733)! Learning rate decreased to 0.00088.
2024-12-02-06:38:07-root-INFO: grad norm: 282.891 274.975 66.450
2024-12-02-06:38:07-root-INFO: grad norm: 199.730 193.954 47.686
2024-12-02-06:38:08-root-INFO: grad norm: 134.007 129.216 35.513
2024-12-02-06:38:08-root-INFO: grad norm: 81.813 77.643 25.786
2024-12-02-06:38:09-root-INFO: grad norm: 70.064 67.014 20.447
2024-12-02-06:38:09-root-INFO: grad norm: 64.490 61.451 19.564
2024-12-02-06:38:10-root-INFO: grad norm: 61.761 59.288 17.301
2024-12-02-06:38:10-root-INFO: Loss Change: 2168.670 -> 2071.956
2024-12-02-06:38:10-root-INFO: Regularization Change: 0.000 -> 0.250
2024-12-02-06:38:10-root-INFO: Undo step: 200
2024-12-02-06:38:10-root-INFO: Undo step: 201
2024-12-02-06:38:10-root-INFO: Undo step: 202
2024-12-02-06:38:10-root-INFO: Undo step: 203
2024-12-02-06:38:10-root-INFO: Undo step: 204
2024-12-02-06:38:10-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-06:38:11-root-INFO: grad norm: 1828.221 1702.681 665.785
2024-12-02-06:38:11-root-INFO: grad norm: 1186.291 1140.189 327.500
2024-12-02-06:38:12-root-INFO: grad norm: 778.835 760.829 166.505
2024-12-02-06:38:12-root-INFO: grad norm: 535.540 519.608 129.656
2024-12-02-06:38:13-root-INFO: grad norm: 352.312 339.764 93.188
2024-12-02-06:38:13-root-INFO: grad norm: 240.731 229.015 74.187
2024-12-02-06:38:14-root-INFO: grad norm: 183.970 173.229 61.938
2024-12-02-06:38:14-root-INFO: grad norm: 153.920 143.243 56.328
2024-12-02-06:38:14-root-INFO: Loss Change: 4568.490 -> 2489.733
2024-12-02-06:38:14-root-INFO: Regularization Change: 0.000 -> 7.811
2024-12-02-06:38:14-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-06:38:14-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-06:38:15-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-06:38:15-root-INFO: grad norm: 246.747 223.686 104.156
2024-12-02-06:38:15-root-INFO: grad norm: 147.759 138.577 51.276
2024-12-02-06:38:16-root-INFO: grad norm: 121.114 112.449 44.988
2024-12-02-06:38:16-root-INFO: grad norm: 114.318 106.535 41.459
2024-12-02-06:38:17-root-INFO: grad norm: 109.334 101.690 40.164
2024-12-02-06:38:17-root-INFO: grad norm: 105.224 97.944 38.459
2024-12-02-06:38:18-root-INFO: grad norm: 101.652 94.568 37.283
2024-12-02-06:38:18-root-INFO: grad norm: 98.516 91.721 35.954
2024-12-02-06:38:19-root-INFO: Loss Change: 2480.289 -> 2344.846
2024-12-02-06:38:19-root-INFO: Regularization Change: 0.000 -> 1.111
2024-12-02-06:38:19-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-06:38:19-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:19-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-06:38:19-root-INFO: grad norm: 116.106 106.683 45.818
2024-12-02-06:38:20-root-INFO: grad norm: 100.140 93.840 34.958
2024-12-02-06:38:20-root-INFO: grad norm: 94.577 88.346 33.760
2024-12-02-06:38:21-root-INFO: grad norm: 90.993 85.183 31.992
2024-12-02-06:38:21-root-INFO: grad norm: 88.163 82.402 31.347
2024-12-02-06:38:22-root-INFO: grad norm: 85.819 80.392 30.036
2024-12-02-06:38:22-root-INFO: grad norm: 83.793 78.466 29.398
2024-12-02-06:38:23-root-INFO: grad norm: 82.007 76.958 28.330
2024-12-02-06:38:23-root-INFO: Loss Change: 2330.900 -> 2254.966
2024-12-02-06:38:23-root-INFO: Regularization Change: 0.000 -> 0.717
2024-12-02-06:38:23-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-06:38:23-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:23-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-06:38:23-root-INFO: grad norm: 504.768 475.058 170.618
2024-12-02-06:38:24-root-INFO: grad norm: 232.859 226.548 53.848
2024-12-02-06:38:24-root-INFO: grad norm: 191.419 185.627 46.734
2024-12-02-06:38:25-root-INFO: grad norm: 131.416 127.444 32.065
2024-12-02-06:38:25-root-INFO: grad norm: 123.370 118.238 35.213
2024-12-02-06:38:26-root-INFO: grad norm: 120.252 116.834 28.470
2024-12-02-06:38:26-root-INFO: grad norm: 121.270 116.642 33.184
2024-12-02-06:38:27-root-INFO: grad norm: 123.542 120.087 29.013
2024-12-02-06:38:27-root-INFO: Loss Change: 2292.442 -> 2173.372
2024-12-02-06:38:27-root-INFO: Regularization Change: 0.000 -> 0.683
2024-12-02-06:38:27-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-06:38:27-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:27-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-06:38:27-root-INFO: grad norm: 151.834 145.471 43.493
2024-12-02-06:38:28-root-INFO: grad norm: 166.298 161.418 39.988
2024-12-02-06:38:28-root-INFO: grad norm: 206.335 200.603 48.299
2024-12-02-06:38:28-root-INFO: Loss too large (2153.641->2156.794)! Learning rate decreased to 0.00105.
2024-12-02-06:38:29-root-INFO: grad norm: 167.903 163.236 39.311
2024-12-02-06:38:29-root-INFO: grad norm: 122.207 118.574 29.577
2024-12-02-06:38:30-root-INFO: grad norm: 111.632 108.076 27.953
2024-12-02-06:38:30-root-INFO: grad norm: 103.996 100.523 26.650
2024-12-02-06:38:31-root-INFO: grad norm: 100.323 97.005 25.587
2024-12-02-06:38:31-root-INFO: Loss Change: 2163.370 -> 2117.146
2024-12-02-06:38:31-root-INFO: Regularization Change: 0.000 -> 0.379
2024-12-02-06:38:31-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-06:38:31-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:31-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-06:38:32-root-INFO: grad norm: 540.850 516.881 159.228
2024-12-02-06:38:32-root-INFO: Loss too large (2149.553->2189.755)! Learning rate decreased to 0.00110.
2024-12-02-06:38:32-root-INFO: Loss too large (2149.553->2157.855)! Learning rate decreased to 0.00088.
2024-12-02-06:38:33-root-INFO: grad norm: 237.220 232.094 49.048
2024-12-02-06:38:33-root-INFO: grad norm: 159.295 154.682 38.056
2024-12-02-06:38:34-root-INFO: grad norm: 79.441 76.095 22.813
2024-12-02-06:38:34-root-INFO: grad norm: 71.505 68.496 20.524
2024-12-02-06:38:35-root-INFO: grad norm: 68.050 65.136 19.698
2024-12-02-06:38:35-root-INFO: grad norm: 66.333 63.703 18.493
2024-12-02-06:38:36-root-INFO: grad norm: 65.191 62.507 18.513
2024-12-02-06:38:36-root-INFO: Loss Change: 2149.553 -> 2065.049
2024-12-02-06:38:36-root-INFO: Regularization Change: 0.000 -> 0.276
2024-12-02-06:38:36-root-INFO: Undo step: 200
2024-12-02-06:38:36-root-INFO: Undo step: 201
2024-12-02-06:38:36-root-INFO: Undo step: 202
2024-12-02-06:38:36-root-INFO: Undo step: 203
2024-12-02-06:38:36-root-INFO: Undo step: 204
2024-12-02-06:38:36-root-INFO: step: 205 lr_xt 0.00109594
2024-12-02-06:38:36-root-INFO: grad norm: 2372.671 2215.358 849.561
2024-12-02-06:38:37-root-INFO: grad norm: 1286.302 1218.904 410.910
2024-12-02-06:38:37-root-INFO: grad norm: 682.491 657.776 182.004
2024-12-02-06:38:38-root-INFO: grad norm: 368.844 339.303 144.634
2024-12-02-06:38:38-root-INFO: grad norm: 277.931 263.517 88.342
2024-12-02-06:38:39-root-INFO: grad norm: 237.949 219.799 91.148
2024-12-02-06:38:39-root-INFO: grad norm: 215.530 202.777 73.038
2024-12-02-06:38:40-root-INFO: grad norm: 191.579 178.402 69.821
2024-12-02-06:38:40-root-INFO: Loss Change: 5497.356 -> 2523.522
2024-12-02-06:38:40-root-INFO: Regularization Change: 0.000 -> 10.539
2024-12-02-06:38:40-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-02-06:38:40-root-INFO: Coefficient of regularization decay: 0.00015 -> 0.00016.
2024-12-02-06:38:41-root-INFO: step: 204 lr_xt 0.00114648
2024-12-02-06:38:41-root-INFO: grad norm: 200.669 181.885 84.770
2024-12-02-06:38:41-root-INFO: grad norm: 152.766 139.452 62.375
2024-12-02-06:38:42-root-INFO: grad norm: 134.509 123.218 53.944
2024-12-02-06:38:42-root-INFO: grad norm: 125.106 114.626 50.122
2024-12-02-06:38:43-root-INFO: grad norm: 118.816 109.683 45.683
2024-12-02-06:38:43-root-INFO: grad norm: 113.779 105.165 43.429
2024-12-02-06:38:44-root-INFO: grad norm: 109.550 101.770 40.547
2024-12-02-06:38:44-root-INFO: grad norm: 105.814 98.458 38.763
2024-12-02-06:38:45-root-INFO: Loss Change: 2504.621 -> 2354.016
2024-12-02-06:38:45-root-INFO: Regularization Change: 0.000 -> 1.304
2024-12-02-06:38:45-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-02-06:38:45-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:45-root-INFO: step: 203 lr_xt 0.00119917
2024-12-02-06:38:45-root-INFO: grad norm: 122.548 113.297 46.710
2024-12-02-06:38:45-root-INFO: grad norm: 105.952 98.856 38.122
2024-12-02-06:38:46-root-INFO: grad norm: 99.461 93.657 33.480
2024-12-02-06:38:46-root-INFO: grad norm: 95.249 89.253 33.259
2024-12-02-06:38:47-root-INFO: grad norm: 92.102 86.808 30.777
2024-12-02-06:38:47-root-INFO: grad norm: 89.536 84.207 30.429
2024-12-02-06:38:48-root-INFO: grad norm: 87.322 82.403 28.896
2024-12-02-06:38:48-root-INFO: grad norm: 85.385 80.524 28.399
2024-12-02-06:38:49-root-INFO: Loss Change: 2337.616 -> 2253.701
2024-12-02-06:38:49-root-INFO: Regularization Change: 0.000 -> 0.793
2024-12-02-06:38:49-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-02-06:38:49-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:49-root-INFO: step: 202 lr_xt 0.00125407
2024-12-02-06:38:49-root-INFO: grad norm: 553.656 524.872 176.195
2024-12-02-06:38:49-root-INFO: grad norm: 232.403 224.291 60.866
2024-12-02-06:38:50-root-INFO: grad norm: 206.251 201.296 44.939
2024-12-02-06:38:50-root-INFO: grad norm: 157.675 151.497 43.702
2024-12-02-06:38:51-root-INFO: grad norm: 139.124 133.118 40.436
2024-12-02-06:38:51-root-INFO: grad norm: 136.649 132.140 34.813
2024-12-02-06:38:52-root-INFO: grad norm: 138.376 133.224 37.405
2024-12-02-06:38:52-root-INFO: grad norm: 142.439 138.151 34.685
2024-12-02-06:38:53-root-INFO: Loss Change: 2291.867 -> 2171.867
2024-12-02-06:38:53-root-INFO: Regularization Change: 0.000 -> 0.717
2024-12-02-06:38:53-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-02-06:38:53-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:53-root-INFO: step: 201 lr_xt 0.00131127
2024-12-02-06:38:53-root-INFO: grad norm: 195.200 187.437 54.502
2024-12-02-06:38:53-root-INFO: grad norm: 212.969 206.781 50.967
2024-12-02-06:38:54-root-INFO: grad norm: 262.113 254.610 62.264
2024-12-02-06:38:54-root-INFO: Loss too large (2152.078->2162.254)! Learning rate decreased to 0.00105.
2024-12-02-06:38:55-root-INFO: grad norm: 202.495 196.999 46.856
2024-12-02-06:38:55-root-INFO: grad norm: 124.883 121.142 30.339
2024-12-02-06:38:56-root-INFO: grad norm: 112.997 109.528 27.783
2024-12-02-06:38:56-root-INFO: grad norm: 116.393 112.046 31.512
2024-12-02-06:38:57-root-INFO: grad norm: 119.454 116.133 27.970
2024-12-02-06:38:57-root-INFO: Loss Change: 2160.412 -> 2110.281
2024-12-02-06:38:57-root-INFO: Regularization Change: 0.000 -> 0.395
2024-12-02-06:38:57-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-02-06:38:57-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:38:57-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-06:38:57-root-INFO: grad norm: 687.623 663.030 182.257
2024-12-02-06:38:57-root-INFO: Loss too large (2154.971->2212.710)! Learning rate decreased to 0.00110.
2024-12-02-06:38:58-root-INFO: Loss too large (2154.971->2177.108)! Learning rate decreased to 0.00088.
2024-12-02-06:38:58-root-INFO: grad norm: 237.901 231.932 52.958
2024-12-02-06:38:59-root-INFO: grad norm: 194.755 189.947 43.007
2024-12-02-06:38:59-root-INFO: grad norm: 137.795 133.345 34.737
2024-12-02-06:39:00-root-INFO: grad norm: 88.493 83.894 28.154
2024-12-02-06:39:00-root-INFO: grad norm: 83.222 80.342 21.705
2024-12-02-06:39:01-root-INFO: grad norm: 81.636 77.678 25.111
2024-12-02-06:39:01-root-INFO: grad norm: 81.996 79.486 20.136
2024-12-02-06:39:01-root-INFO: Loss Change: 2154.971 -> 2055.961
2024-12-02-06:39:01-root-INFO: Regularization Change: 0.000 -> 0.276
2024-12-02-06:39:01-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-06:39:01-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:39:02-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-06:39:02-root-INFO: grad norm: 276.228 263.287 83.557
2024-12-02-06:39:02-root-INFO: Loss too large (2052.089->2077.554)! Learning rate decreased to 0.00115.
2024-12-02-06:39:02-root-INFO: Loss too large (2052.089->2063.131)! Learning rate decreased to 0.00092.
2024-12-02-06:39:02-root-INFO: Loss too large (2052.089->2052.774)! Learning rate decreased to 0.00073.
2024-12-02-06:39:03-root-INFO: grad norm: 180.006 175.989 37.817
2024-12-02-06:39:03-root-INFO: grad norm: 79.291 74.601 26.867
2024-12-02-06:39:04-root-INFO: grad norm: 73.292 70.300 20.728
2024-12-02-06:39:04-root-INFO: grad norm: 69.735 66.209 21.894
2024-12-02-06:39:05-root-INFO: grad norm: 67.722 65.090 18.694
2024-12-02-06:39:05-root-INFO: grad norm: 66.298 63.196 20.043
2024-12-02-06:39:06-root-INFO: grad norm: 65.345 62.857 17.858
2024-12-02-06:39:06-root-INFO: Loss Change: 2052.089 -> 2013.290
2024-12-02-06:39:06-root-INFO: Regularization Change: 0.000 -> 0.162
2024-12-02-06:39:06-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-06:39:06-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-06:39:06-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-06:39:06-root-INFO: grad norm: 445.445 428.061 123.231
2024-12-02-06:39:06-root-INFO: Loss too large (2033.034->2096.232)! Learning rate decreased to 0.00120.
2024-12-02-06:39:07-root-INFO: Loss too large (2033.034->2070.546)! Learning rate decreased to 0.00096.
2024-12-02-06:39:07-root-INFO: Loss too large (2033.034->2050.054)! Learning rate decreased to 0.00077.
2024-12-02-06:39:07-root-INFO: Loss too large (2033.034->2033.894)! Learning rate decreased to 0.00061.
2024-12-02-06:39:07-root-INFO: grad norm: 227.945 222.633 48.922
2024-12-02-06:39:08-root-INFO: grad norm: 93.182 88.369 29.557
2024-12-02-06:39:08-root-INFO: grad norm: 78.921 73.554 28.604
2024-12-02-06:39:09-root-INFO: grad norm: 72.092 68.420 22.713
2024-12-02-06:39:09-root-INFO: grad norm: 67.717 63.774 22.769
2024-12-02-06:39:10-root-INFO: grad norm: 65.180 62.046 19.968
2024-12-02-06:39:10-root-INFO: grad norm: 63.404 60.137 20.091
2024-12-02-06:39:10-root-INFO: Loss Change: 2033.034 -> 1980.547
2024-12-02-06:39:10-root-INFO: Regularization Change: 0.000 -> 0.143
2024-12-02-06:39:10-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-06:39:10-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:39:11-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-06:39:11-root-INFO: grad norm: 344.254 332.832 87.939
2024-12-02-06:39:11-root-INFO: Loss too large (1983.861->2047.891)! Learning rate decreased to 0.00125.
2024-12-02-06:39:11-root-INFO: Loss too large (1983.861->2025.258)! Learning rate decreased to 0.00100.
2024-12-02-06:39:11-root-INFO: Loss too large (1983.861->2007.103)! Learning rate decreased to 0.00080.
2024-12-02-06:39:11-root-INFO: Loss too large (1983.861->1992.753)! Learning rate decreased to 0.00064.
2024-12-02-06:39:12-root-INFO: grad norm: 228.385 223.531 46.836
2024-12-02-06:39:12-root-INFO: grad norm: 79.304 75.008 25.749
2024-12-02-06:39:13-root-INFO: grad norm: 76.059 72.907 21.668
2024-12-02-06:39:13-root-INFO: grad norm: 74.010 70.706 21.866
2024-12-02-06:39:14-root-INFO: grad norm: 72.765 70.064 19.642
2024-12-02-06:39:14-root-INFO: grad norm: 71.965 69.051 20.274
2024-12-02-06:39:15-root-INFO: grad norm: 71.530 69.055 18.652
2024-12-02-06:39:15-root-INFO: Loss Change: 1983.861 -> 1946.766
2024-12-02-06:39:15-root-INFO: Regularization Change: 0.000 -> 0.120
2024-12-02-06:39:15-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-06:39:15-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:39:15-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-06:39:16-root-INFO: grad norm: 417.736 402.814 110.653
2024-12-02-06:39:16-root-INFO: Loss too large (1960.681->2041.536)! Learning rate decreased to 0.00131.
2024-12-02-06:39:16-root-INFO: Loss too large (1960.681->2014.933)! Learning rate decreased to 0.00105.
2024-12-02-06:39:16-root-INFO: Loss too large (1960.681->1992.951)! Learning rate decreased to 0.00084.
2024-12-02-06:39:16-root-INFO: Loss too large (1960.681->1974.872)! Learning rate decreased to 0.00067.
2024-12-02-06:39:17-root-INFO: grad norm: 259.232 253.607 53.710
2024-12-02-06:39:17-root-INFO: grad norm: 78.796 73.055 29.525
2024-12-02-06:39:18-root-INFO: grad norm: 72.558 67.513 26.583
2024-12-02-06:39:18-root-INFO: grad norm: 69.371 65.646 22.426
2024-12-02-06:39:19-root-INFO: grad norm: 67.957 64.195 22.299
2024-12-02-06:39:19-root-INFO: grad norm: 67.706 64.872 19.384
2024-12-02-06:39:20-root-INFO: grad norm: 68.668 65.448 20.781
2024-12-02-06:39:20-root-INFO: Loss Change: 1960.681 -> 1910.999
2024-12-02-06:39:20-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-02-06:39:20-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-06:39:20-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:39:20-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-06:39:20-root-INFO: grad norm: 160.123 154.264 42.920
2024-12-02-06:39:20-root-INFO: Loss too large (1903.983->1933.815)! Learning rate decreased to 0.00137.
2024-12-02-06:39:21-root-INFO: Loss too large (1903.983->1921.777)! Learning rate decreased to 0.00109.
2024-12-02-06:39:21-root-INFO: Loss too large (1903.983->1912.954)! Learning rate decreased to 0.00087.
2024-12-02-06:39:21-root-INFO: Loss too large (1903.983->1906.932)! Learning rate decreased to 0.00070.
2024-12-02-06:39:21-root-INFO: grad norm: 173.220 169.710 34.697
2024-12-02-06:39:22-root-INFO: grad norm: 208.925 204.090 44.686
2024-12-02-06:39:22-root-INFO: Loss too large (1900.099->1902.738)! Learning rate decreased to 0.00056.
2024-12-02-06:39:22-root-INFO: grad norm: 170.085 166.714 33.694
2024-12-02-06:39:23-root-INFO: grad norm: 123.041 119.781 28.138
2024-12-02-06:39:23-root-INFO: grad norm: 110.066 107.518 23.547
2024-12-02-06:39:24-root-INFO: grad norm: 96.243 93.464 22.959
2024-12-02-06:39:24-root-INFO: grad norm: 89.171 86.855 20.191
2024-12-02-06:39:25-root-INFO: Loss Change: 1903.983 -> 1883.673
2024-12-02-06:39:25-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-02-06:39:25-root-INFO: Undo step: 195
2024-12-02-06:39:25-root-INFO: Undo step: 196
2024-12-02-06:39:25-root-INFO: Undo step: 197
2024-12-02-06:39:25-root-INFO: Undo step: 198
2024-12-02-06:39:25-root-INFO: Undo step: 199
2024-12-02-06:39:25-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-06:39:25-root-INFO: grad norm: 1380.285 1225.718 634.666
2024-12-02-06:39:26-root-INFO: grad norm: 901.491 876.813 209.486
2024-12-02-06:39:26-root-INFO: grad norm: 836.016 818.408 170.676
2024-12-02-06:39:27-root-INFO: grad norm: 882.045 865.221 171.453
2024-12-02-06:39:27-root-INFO: Loss too large (2623.900->2628.336)! Learning rate decreased to 0.00110.
2024-12-02-06:39:27-root-INFO: grad norm: 600.892 587.452 126.376
2024-12-02-06:39:28-root-INFO: grad norm: 379.295 367.969 91.999
2024-12-02-06:39:28-root-INFO: grad norm: 299.126 291.998 64.910
2024-12-02-06:39:29-root-INFO: grad norm: 244.482 236.045 63.673
2024-12-02-06:39:29-root-INFO: Loss Change: 3903.515 -> 2243.417
2024-12-02-06:39:29-root-INFO: Regularization Change: 0.000 -> 8.775
2024-12-02-06:39:29-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-06:39:29-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:39:29-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-06:39:29-root-INFO: grad norm: 285.109 271.787 86.134
2024-12-02-06:39:30-root-INFO: grad norm: 356.335 347.284 79.806
2024-12-02-06:39:30-root-INFO: Loss too large (2213.565->2227.364)! Learning rate decreased to 0.00115.
2024-12-02-06:39:31-root-INFO: grad norm: 334.345 326.977 69.802
2024-12-02-06:39:31-root-INFO: grad norm: 318.176 310.672 68.695
2024-12-02-06:39:32-root-INFO: grad norm: 311.180 303.867 67.070
2024-12-02-06:39:32-root-INFO: grad norm: 308.150 301.111 65.487
2024-12-02-06:39:33-root-INFO: grad norm: 311.325 303.343 70.042
2024-12-02-06:39:33-root-INFO: grad norm: 319.391 312.375 66.575
2024-12-02-06:39:34-root-INFO: Loss Change: 2229.739 -> 2110.905
2024-12-02-06:39:34-root-INFO: Regularization Change: 0.000 -> 1.192
2024-12-02-06:39:34-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-06:39:34-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-06:39:34-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-06:39:34-root-INFO: grad norm: 680.068 654.516 184.666
2024-12-02-06:39:34-root-INFO: Loss too large (2148.949->2304.415)! Learning rate decreased to 0.00120.
2024-12-02-06:39:34-root-INFO: Loss too large (2148.949->2206.245)! Learning rate decreased to 0.00096.
2024-12-02-06:39:35-root-INFO: grad norm: 390.947 380.988 87.682
2024-12-02-06:39:35-root-INFO: grad norm: 286.135 282.296 46.715
2024-12-02-06:39:36-root-INFO: grad norm: 176.175 170.309 45.082
2024-12-02-06:39:36-root-INFO: grad norm: 141.391 135.589 40.086
2024-12-02-06:39:37-root-INFO: grad norm: 127.072 122.393 34.166
2024-12-02-06:39:37-root-INFO: grad norm: 146.152 139.655 43.091
2024-12-02-06:39:38-root-INFO: grad norm: 188.033 183.446 41.283
2024-12-02-06:39:38-root-INFO: Loss too large (2017.622->2020.502)! Learning rate decreased to 0.00077.
2024-12-02-06:39:38-root-INFO: Loss Change: 2148.949 -> 2012.033
2024-12-02-06:39:38-root-INFO: Regularization Change: 0.000 -> 0.471
2024-12-02-06:39:38-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-06:39:38-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:39:38-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-06:39:38-root-INFO: grad norm: 644.466 628.873 140.909
2024-12-02-06:39:39-root-INFO: Loss too large (2031.428->2172.589)! Learning rate decreased to 0.00125.
2024-12-02-06:39:39-root-INFO: Loss too large (2031.428->2130.838)! Learning rate decreased to 0.00100.
2024-12-02-06:39:39-root-INFO: Loss too large (2031.428->2095.096)! Learning rate decreased to 0.00080.
2024-12-02-06:39:39-root-INFO: Loss too large (2031.428->2064.943)! Learning rate decreased to 0.00064.
2024-12-02-06:39:39-root-INFO: Loss too large (2031.428->2040.081)! Learning rate decreased to 0.00051.
2024-12-02-06:39:40-root-INFO: grad norm: 270.369 264.493 56.062
2024-12-02-06:39:40-root-INFO: grad norm: 171.291 167.043 37.911
2024-12-02-06:39:41-root-INFO: grad norm: 104.790 99.081 34.115
2024-12-02-06:39:41-root-INFO: grad norm: 94.007 89.796 27.821
2024-12-02-06:39:42-root-INFO: grad norm: 86.465 81.455 29.005
2024-12-02-06:39:42-root-INFO: grad norm: 82.597 78.488 25.728
2024-12-02-06:39:43-root-INFO: grad norm: 79.765 75.168 26.687
2024-12-02-06:39:43-root-INFO: Loss Change: 2031.428 -> 1964.381
2024-12-02-06:39:43-root-INFO: Regularization Change: 0.000 -> 0.122
2024-12-02-06:39:43-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-06:39:43-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:39:43-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-06:39:44-root-INFO: grad norm: 400.905 386.892 105.070
2024-12-02-06:39:44-root-INFO: Loss too large (1969.778->2070.973)! Learning rate decreased to 0.00131.
2024-12-02-06:39:44-root-INFO: Loss too large (1969.778->2041.819)! Learning rate decreased to 0.00105.
2024-12-02-06:39:44-root-INFO: Loss too large (1969.778->2017.201)! Learning rate decreased to 0.00084.
2024-12-02-06:39:44-root-INFO: Loss too large (1969.778->1996.670)! Learning rate decreased to 0.00067.
2024-12-02-06:39:44-root-INFO: Loss too large (1969.778->1980.048)! Learning rate decreased to 0.00054.
2024-12-02-06:39:45-root-INFO: grad norm: 289.127 282.798 60.164
2024-12-02-06:39:46-root-INFO: grad norm: 128.855 121.138 43.925
2024-12-02-06:39:46-root-INFO: grad norm: 126.119 121.830 32.609
2024-12-02-06:39:47-root-INFO: grad norm: 129.694 124.295 37.031
2024-12-02-06:39:47-root-INFO: grad norm: 134.916 131.541 29.985
2024-12-02-06:39:48-root-INFO: grad norm: 147.895 143.254 36.760
2024-12-02-06:39:48-root-INFO: grad norm: 158.749 155.458 32.153
2024-12-02-06:39:48-root-INFO: Loss Change: 1969.778 -> 1925.837
2024-12-02-06:39:48-root-INFO: Regularization Change: 0.000 -> 0.128
2024-12-02-06:39:48-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-06:39:48-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:39:49-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-06:39:49-root-INFO: grad norm: 445.307 435.799 91.531
2024-12-02-06:39:49-root-INFO: Loss too large (1929.654->2070.637)! Learning rate decreased to 0.00137.
2024-12-02-06:39:49-root-INFO: Loss too large (1929.654->2039.091)! Learning rate decreased to 0.00109.
2024-12-02-06:39:49-root-INFO: Loss too large (1929.654->2010.323)! Learning rate decreased to 0.00087.
2024-12-02-06:39:49-root-INFO: Loss too large (1929.654->1984.487)! Learning rate decreased to 0.00070.
2024-12-02-06:39:50-root-INFO: Loss too large (1929.654->1961.730)! Learning rate decreased to 0.00056.
2024-12-02-06:39:50-root-INFO: Loss too large (1929.654->1942.638)! Learning rate decreased to 0.00045.
2024-12-02-06:39:50-root-INFO: grad norm: 299.491 294.224 55.919
2024-12-02-06:39:51-root-INFO: grad norm: 115.505 110.851 32.458
2024-12-02-06:39:51-root-INFO: grad norm: 107.301 104.109 25.980
2024-12-02-06:39:52-root-INFO: grad norm: 99.863 95.894 27.876
2024-12-02-06:39:52-root-INFO: grad norm: 95.582 92.593 23.718
2024-12-02-06:39:53-root-INFO: grad norm: 91.844 88.272 25.365
2024-12-02-06:39:53-root-INFO: grad norm: 89.614 86.761 22.433
2024-12-02-06:39:54-root-INFO: Loss Change: 1929.654 -> 1894.695
2024-12-02-06:39:54-root-INFO: Regularization Change: 0.000 -> 0.066
2024-12-02-06:39:54-root-INFO: Undo step: 195
2024-12-02-06:39:54-root-INFO: Undo step: 196
2024-12-02-06:39:54-root-INFO: Undo step: 197
2024-12-02-06:39:54-root-INFO: Undo step: 198
2024-12-02-06:39:54-root-INFO: Undo step: 199
2024-12-02-06:39:54-root-INFO: step: 200 lr_xt 0.00137086
2024-12-02-06:39:54-root-INFO: grad norm: 1138.573 986.468 568.534
2024-12-02-06:39:54-root-INFO: grad norm: 562.547 538.992 161.082
2024-12-02-06:39:55-root-INFO: grad norm: 384.802 363.169 127.201
2024-12-02-06:39:55-root-INFO: grad norm: 642.432 624.081 152.451
2024-12-02-06:39:56-root-INFO: Loss too large (2489.323->2561.184)! Learning rate decreased to 0.00110.
2024-12-02-06:39:56-root-INFO: Loss too large (2489.323->2524.257)! Learning rate decreased to 0.00088.
2024-12-02-06:39:56-root-INFO: Loss too large (2489.323->2493.087)! Learning rate decreased to 0.00070.
2024-12-02-06:39:56-root-INFO: grad norm: 348.525 337.961 85.159
2024-12-02-06:39:57-root-INFO: grad norm: 228.162 217.089 70.215
2024-12-02-06:39:57-root-INFO: grad norm: 190.589 178.392 67.085
2024-12-02-06:39:58-root-INFO: grad norm: 181.270 170.179 62.431
2024-12-02-06:39:58-root-INFO: Loss Change: 3988.607 -> 2332.426
2024-12-02-06:39:58-root-INFO: Regularization Change: 0.000 -> 7.416
2024-12-02-06:39:58-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-02-06:39:58-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00016.
2024-12-02-06:39:58-root-INFO: step: 199 lr_xt 0.00143293
2024-12-02-06:39:59-root-INFO: grad norm: 387.737 372.623 107.202
2024-12-02-06:39:59-root-INFO: Loss too large (2320.889->2367.788)! Learning rate decreased to 0.00115.
2024-12-02-06:39:59-root-INFO: Loss too large (2320.889->2344.786)! Learning rate decreased to 0.00092.
2024-12-02-06:39:59-root-INFO: Loss too large (2320.889->2327.104)! Learning rate decreased to 0.00073.
2024-12-02-06:40:00-root-INFO: grad norm: 299.674 291.315 70.287
2024-12-02-06:40:00-root-INFO: grad norm: 165.796 155.687 57.007
2024-12-02-06:40:01-root-INFO: grad norm: 168.159 159.457 53.395
2024-12-02-06:40:01-root-INFO: grad norm: 188.042 179.242 56.851
2024-12-02-06:40:01-root-INFO: grad norm: 205.913 198.389 55.152
2024-12-02-06:40:02-root-INFO: grad norm: 259.042 250.960 64.200
2024-12-02-06:40:02-root-INFO: grad norm: 265.390 258.117 61.702
2024-12-02-06:40:03-root-INFO: Loss Change: 2320.889 -> 2201.896
2024-12-02-06:40:03-root-INFO: Regularization Change: 0.000 -> 0.649
2024-12-02-06:40:03-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-02-06:40:03-root-INFO: Coefficient of regularization decay: 0.00016 -> 0.00017.
2024-12-02-06:40:03-root-INFO: step: 198 lr_xt 0.00149757
2024-12-02-06:40:03-root-INFO: grad norm: 842.886 821.130 190.270
2024-12-02-06:40:03-root-INFO: Loss too large (2250.610->2409.069)! Learning rate decreased to 0.00120.
2024-12-02-06:40:03-root-INFO: Loss too large (2250.610->2363.512)! Learning rate decreased to 0.00096.
2024-12-02-06:40:04-root-INFO: Loss too large (2250.610->2321.124)! Learning rate decreased to 0.00077.
2024-12-02-06:40:04-root-INFO: Loss too large (2250.610->2282.362)! Learning rate decreased to 0.00061.
2024-12-02-06:40:04-root-INFO: grad norm: 332.350 324.446 72.052
2024-12-02-06:40:05-root-INFO: grad norm: 294.037 287.193 63.072
2024-12-02-06:40:05-root-INFO: grad norm: 132.372 124.737 44.306
2024-12-02-06:40:06-root-INFO: grad norm: 130.342 123.234 42.455
2024-12-02-06:40:06-root-INFO: grad norm: 137.849 130.682 43.871
2024-12-02-06:40:07-root-INFO: grad norm: 149.960 143.702 42.870
2024-12-02-06:40:07-root-INFO: grad norm: 180.059 173.506 48.134
2024-12-02-06:40:08-root-INFO: Loss Change: 2250.610 -> 2117.017
2024-12-02-06:40:08-root-INFO: Regularization Change: 0.000 -> 0.371
2024-12-02-06:40:08-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-02-06:40:08-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:40:08-root-INFO: step: 197 lr_xt 0.00156486
2024-12-02-06:40:08-root-INFO: grad norm: 153.101 142.098 56.992
2024-12-02-06:40:09-root-INFO: grad norm: 315.897 308.039 70.019
2024-12-02-06:40:09-root-INFO: Loss too large (2093.740->2335.086)! Learning rate decreased to 0.00125.
2024-12-02-06:40:09-root-INFO: Loss too large (2093.740->2222.750)! Learning rate decreased to 0.00100.
2024-12-02-06:40:09-root-INFO: Loss too large (2093.740->2150.038)! Learning rate decreased to 0.00080.
2024-12-02-06:40:09-root-INFO: Loss too large (2093.740->2107.396)! Learning rate decreased to 0.00064.
2024-12-02-06:40:10-root-INFO: grad norm: 425.690 417.069 85.236
2024-12-02-06:40:10-root-INFO: Loss too large (2085.069->2104.975)! Learning rate decreased to 0.00051.
2024-12-02-06:40:10-root-INFO: Loss too large (2085.069->2089.204)! Learning rate decreased to 0.00041.
2024-12-02-06:40:11-root-INFO: grad norm: 278.727 272.592 58.157
2024-12-02-06:40:11-root-INFO: grad norm: 132.173 126.277 39.038
2024-12-02-06:40:11-root-INFO: grad norm: 119.168 113.090 37.572
2024-12-02-06:40:12-root-INFO: grad norm: 109.137 102.892 36.388
2024-12-02-06:40:12-root-INFO: grad norm: 103.665 97.367 35.583
2024-12-02-06:40:13-root-INFO: Loss Change: 2095.935 -> 2046.103
2024-12-02-06:40:13-root-INFO: Regularization Change: 0.000 -> 0.223
2024-12-02-06:40:13-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-02-06:40:13-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:40:13-root-INFO: step: 196 lr_xt 0.00163492
2024-12-02-06:40:13-root-INFO: grad norm: 555.110 539.935 128.908
2024-12-02-06:40:13-root-INFO: Loss too large (2056.586->2224.545)! Learning rate decreased to 0.00131.
2024-12-02-06:40:13-root-INFO: Loss too large (2056.586->2185.367)! Learning rate decreased to 0.00105.
2024-12-02-06:40:14-root-INFO: Loss too large (2056.586->2150.168)! Learning rate decreased to 0.00084.
2024-12-02-06:40:14-root-INFO: Loss too large (2056.586->2119.104)! Learning rate decreased to 0.00067.
2024-12-02-06:40:14-root-INFO: Loss too large (2056.586->2092.166)! Learning rate decreased to 0.00054.
2024-12-02-06:40:14-root-INFO: Loss too large (2056.586->2069.281)! Learning rate decreased to 0.00043.
2024-12-02-06:40:15-root-INFO: grad norm: 355.168 348.824 66.832
2024-12-02-06:40:15-root-INFO: grad norm: 122.466 113.667 45.581
2024-12-02-06:40:16-root-INFO: grad norm: 117.142 110.585 38.640
2024-12-02-06:40:16-root-INFO: grad norm: 113.338 106.275 39.385
2024-12-02-06:40:17-root-INFO: grad norm: 110.727 105.032 35.053
2024-12-02-06:40:17-root-INFO: grad norm: 108.816 102.583 36.301
2024-12-02-06:40:18-root-INFO: grad norm: 107.594 102.373 33.109
2024-12-02-06:40:18-root-INFO: Loss Change: 2056.586 -> 1995.543
2024-12-02-06:40:18-root-INFO: Regularization Change: 0.000 -> 0.123
2024-12-02-06:40:18-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-02-06:40:18-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:40:18-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-06:40:18-root-INFO: grad norm: 341.246 332.573 76.447
2024-12-02-06:40:18-root-INFO: Loss too large (1992.495->2121.871)! Learning rate decreased to 0.00137.
2024-12-02-06:40:19-root-INFO: Loss too large (1992.495->2091.191)! Learning rate decreased to 0.00109.
2024-12-02-06:40:19-root-INFO: Loss too large (1992.495->2063.968)! Learning rate decreased to 0.00087.
2024-12-02-06:40:19-root-INFO: Loss too large (1992.495->2040.109)! Learning rate decreased to 0.00070.
2024-12-02-06:40:19-root-INFO: Loss too large (1992.495->2019.887)! Learning rate decreased to 0.00056.
2024-12-02-06:40:19-root-INFO: Loss too large (1992.495->2004.012)! Learning rate decreased to 0.00045.
2024-12-02-06:40:19-root-INFO: Loss too large (1992.495->1992.844)! Learning rate decreased to 0.00036.
2024-12-02-06:40:20-root-INFO: grad norm: 241.115 236.444 47.228
2024-12-02-06:40:20-root-INFO: grad norm: 152.949 146.935 42.467
2024-12-02-06:40:21-root-INFO: grad norm: 126.873 122.061 34.611
2024-12-02-06:40:21-root-INFO: grad norm: 107.515 101.729 34.794
2024-12-02-06:40:22-root-INFO: grad norm: 97.799 92.572 31.545
2024-12-02-06:40:22-root-INFO: grad norm: 91.228 85.519 31.765
2024-12-02-06:40:23-root-INFO: grad norm: 87.440 82.057 30.205
2024-12-02-06:40:23-root-INFO: Loss Change: 1992.495 -> 1960.811
2024-12-02-06:40:23-root-INFO: Regularization Change: 0.000 -> 0.065
2024-12-02-06:40:23-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-06:40:23-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:40:23-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-06:40:23-root-INFO: grad norm: 406.721 394.939 97.188
2024-12-02-06:40:23-root-INFO: Loss too large (1964.313->2128.776)! Learning rate decreased to 0.00143.
2024-12-02-06:40:24-root-INFO: Loss too large (1964.313->2092.512)! Learning rate decreased to 0.00114.
2024-12-02-06:40:24-root-INFO: Loss too large (1964.313->2060.415)! Learning rate decreased to 0.00091.
2024-12-02-06:40:24-root-INFO: Loss too large (1964.313->2031.987)! Learning rate decreased to 0.00073.
2024-12-02-06:40:24-root-INFO: Loss too large (1964.313->2006.948)! Learning rate decreased to 0.00058.
2024-12-02-06:40:24-root-INFO: Loss too large (1964.313->1985.789)! Learning rate decreased to 0.00047.
2024-12-02-06:40:24-root-INFO: Loss too large (1964.313->1969.513)! Learning rate decreased to 0.00037.
2024-12-02-06:40:25-root-INFO: grad norm: 319.415 314.112 57.961
2024-12-02-06:40:25-root-INFO: grad norm: 223.415 215.980 57.156
2024-12-02-06:40:26-root-INFO: grad norm: 199.857 195.219 42.805
2024-12-02-06:40:26-root-INFO: grad norm: 174.565 168.465 45.745
2024-12-02-06:40:27-root-INFO: grad norm: 160.766 156.417 37.142
2024-12-02-06:40:27-root-INFO: grad norm: 146.664 141.284 39.359
2024-12-02-06:40:28-root-INFO: grad norm: 137.761 133.573 33.706
2024-12-02-06:40:28-root-INFO: Loss Change: 1964.313 -> 1925.765
2024-12-02-06:40:28-root-INFO: Regularization Change: 0.000 -> 0.078
2024-12-02-06:40:28-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-06:40:28-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:40:28-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-06:40:29-root-INFO: grad norm: 700.326 685.500 143.340
2024-12-02-06:40:29-root-INFO: Loss too large (1954.972->2180.649)! Learning rate decreased to 0.00149.
2024-12-02-06:40:29-root-INFO: Loss too large (1954.972->2145.355)! Learning rate decreased to 0.00119.
2024-12-02-06:40:29-root-INFO: Loss too large (1954.972->2111.078)! Learning rate decreased to 0.00095.
2024-12-02-06:40:29-root-INFO: Loss too large (1954.972->2077.162)! Learning rate decreased to 0.00076.
2024-12-02-06:40:29-root-INFO: Loss too large (1954.972->2044.058)! Learning rate decreased to 0.00061.
2024-12-02-06:40:29-root-INFO: Loss too large (1954.972->2012.259)! Learning rate decreased to 0.00049.
2024-12-02-06:40:30-root-INFO: Loss too large (1954.972->1982.092)! Learning rate decreased to 0.00039.
2024-12-02-06:40:30-root-INFO: grad norm: 442.944 436.502 75.271
2024-12-02-06:40:31-root-INFO: grad norm: 121.368 113.680 42.510
2024-12-02-06:40:31-root-INFO: grad norm: 124.573 119.122 36.446
2024-12-02-06:40:32-root-INFO: grad norm: 132.243 126.254 39.345
2024-12-02-06:40:32-root-INFO: grad norm: 141.357 137.016 34.763
2024-12-02-06:40:33-root-INFO: grad norm: 155.864 150.688 39.831
2024-12-02-06:40:33-root-INFO: grad norm: 170.060 166.173 36.151
2024-12-02-06:40:33-root-INFO: Loss Change: 1954.972 -> 1891.503
2024-12-02-06:40:33-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-02-06:40:33-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-06:40:33-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-06:40:33-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-06:40:34-root-INFO: grad norm: 475.139 465.218 96.585
2024-12-02-06:40:34-root-INFO: Loss too large (1891.386->2106.060)! Learning rate decreased to 0.00156.
2024-12-02-06:40:34-root-INFO: Loss too large (1891.386->2068.879)! Learning rate decreased to 0.00124.
2024-12-02-06:40:34-root-INFO: Loss too large (1891.386->2034.324)! Learning rate decreased to 0.00100.
2024-12-02-06:40:34-root-INFO: Loss too large (1891.386->2002.069)! Learning rate decreased to 0.00080.
2024-12-02-06:40:34-root-INFO: Loss too large (1891.386->1971.924)! Learning rate decreased to 0.00064.
2024-12-02-06:40:35-root-INFO: Loss too large (1891.386->1943.868)! Learning rate decreased to 0.00051.
2024-12-02-06:40:35-root-INFO: Loss too large (1891.386->1918.908)! Learning rate decreased to 0.00041.
2024-12-02-06:40:35-root-INFO: Loss too large (1891.386->1898.979)! Learning rate decreased to 0.00033.
2024-12-02-06:40:35-root-INFO: grad norm: 357.460 352.654 58.419
2024-12-02-06:40:36-root-INFO: grad norm: 237.203 231.389 52.196
2024-12-02-06:40:36-root-INFO: grad norm: 207.041 203.198 39.710
2024-12-02-06:40:37-root-INFO: grad norm: 176.598 171.760 41.050
2024-12-02-06:40:37-root-INFO: grad norm: 159.228 155.589 33.848
2024-12-02-06:40:38-root-INFO: grad norm: 142.285 137.882 35.122
2024-12-02-06:40:38-root-INFO: grad norm: 131.107 127.525 30.435
2024-12-02-06:40:39-root-INFO: Loss Change: 1891.386 -> 1856.438
2024-12-02-06:40:39-root-INFO: Regularization Change: 0.000 -> 0.050
2024-12-02-06:40:39-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-06:40:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:40:39-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-06:40:39-root-INFO: grad norm: 767.808 752.057 154.725
2024-12-02-06:40:39-root-INFO: Loss too large (1904.504->2146.303)! Learning rate decreased to 0.00162.
2024-12-02-06:40:39-root-INFO: Loss too large (1904.504->2111.334)! Learning rate decreased to 0.00130.
2024-12-02-06:40:40-root-INFO: Loss too large (1904.504->2079.245)! Learning rate decreased to 0.00104.
2024-12-02-06:40:40-root-INFO: Loss too large (1904.504->2047.570)! Learning rate decreased to 0.00083.
2024-12-02-06:40:40-root-INFO: Loss too large (1904.504->2015.688)! Learning rate decreased to 0.00067.
2024-12-02-06:40:40-root-INFO: Loss too large (1904.504->1983.805)! Learning rate decreased to 0.00053.
2024-12-02-06:40:40-root-INFO: Loss too large (1904.504->1952.110)! Learning rate decreased to 0.00043.
2024-12-02-06:40:41-root-INFO: Loss too large (1904.504->1921.038)! Learning rate decreased to 0.00034.
2024-12-02-06:40:41-root-INFO: grad norm: 480.484 473.899 79.276
2024-12-02-06:40:42-root-INFO: grad norm: 169.072 160.201 54.048
2024-12-02-06:40:42-root-INFO: grad norm: 168.318 162.586 43.549
2024-12-02-06:40:43-root-INFO: grad norm: 169.695 162.678 48.291
2024-12-02-06:40:43-root-INFO: grad norm: 171.216 166.537 39.756
2024-12-02-06:40:43-root-INFO: grad norm: 174.229 168.305 45.046
2024-12-02-06:40:44-root-INFO: grad norm: 176.888 172.807 37.777
2024-12-02-06:40:44-root-INFO: Loss Change: 1904.504 -> 1831.530
2024-12-02-06:40:44-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-02-06:40:44-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-06:40:44-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:40:44-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-06:40:45-root-INFO: grad norm: 728.506 715.108 139.076
2024-12-02-06:40:45-root-INFO: Loss too large (1867.147->2111.122)! Learning rate decreased to 0.00170.
2024-12-02-06:40:45-root-INFO: Loss too large (1867.147->2081.025)! Learning rate decreased to 0.00136.
2024-12-02-06:40:45-root-INFO: Loss too large (1867.147->2051.985)! Learning rate decreased to 0.00108.
2024-12-02-06:40:45-root-INFO: Loss too large (1867.147->2022.104)! Learning rate decreased to 0.00087.
2024-12-02-06:40:45-root-INFO: Loss too large (1867.147->1991.049)! Learning rate decreased to 0.00069.
2024-12-02-06:40:46-root-INFO: Loss too large (1867.147->1959.167)! Learning rate decreased to 0.00056.
2024-12-02-06:40:46-root-INFO: Loss too large (1867.147->1926.742)! Learning rate decreased to 0.00044.
2024-12-02-06:40:46-root-INFO: Loss too large (1867.147->1894.367)! Learning rate decreased to 0.00036.
2024-12-02-06:40:46-root-INFO: grad norm: 496.950 491.099 76.038
2024-12-02-06:40:47-root-INFO: grad norm: 223.543 216.986 53.744
2024-12-02-06:40:48-root-INFO: grad norm: 243.501 239.490 44.012
2024-12-02-06:40:48-root-INFO: grad norm: 271.632 265.579 57.024
2024-12-02-06:40:49-root-INFO: grad norm: 290.097 286.022 48.454
2024-12-02-06:40:49-root-INFO: grad norm: 314.891 308.863 61.316
2024-12-02-06:40:49-root-INFO: Loss too large (1813.470->1814.434)! Learning rate decreased to 0.00028.
2024-12-02-06:40:50-root-INFO: grad norm: 234.728 231.134 40.919
2024-12-02-06:40:50-root-INFO: Loss Change: 1867.147 -> 1804.484
2024-12-02-06:40:50-root-INFO: Regularization Change: 0.000 -> 0.067
2024-12-02-06:40:50-root-INFO: Undo step: 190
2024-12-02-06:40:50-root-INFO: Undo step: 191
2024-12-02-06:40:50-root-INFO: Undo step: 192
2024-12-02-06:40:50-root-INFO: Undo step: 193
2024-12-02-06:40:50-root-INFO: Undo step: 194
2024-12-02-06:40:50-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-06:40:50-root-INFO: grad norm: 1352.283 1263.363 482.270
2024-12-02-06:40:51-root-INFO: grad norm: 1371.908 1328.541 342.213
2024-12-02-06:40:51-root-INFO: Loss too large (3242.070->3284.798)! Learning rate decreased to 0.00137.
2024-12-02-06:40:52-root-INFO: grad norm: 928.048 905.400 203.774
2024-12-02-06:40:52-root-INFO: grad norm: 493.142 478.566 119.012
2024-12-02-06:40:53-root-INFO: grad norm: 399.511 381.229 119.473
2024-12-02-06:40:53-root-INFO: grad norm: 351.749 341.539 84.132
2024-12-02-06:40:54-root-INFO: grad norm: 317.407 303.661 92.395
2024-12-02-06:40:54-root-INFO: grad norm: 276.324 267.339 69.891
2024-12-02-06:40:55-root-INFO: Loss Change: 3959.753 -> 2150.888
2024-12-02-06:40:55-root-INFO: Regularization Change: 0.000 -> 10.861
2024-12-02-06:40:55-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-06:40:55-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:40:55-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-06:40:55-root-INFO: grad norm: 198.490 189.302 59.691
2024-12-02-06:40:55-root-INFO: grad norm: 251.307 241.634 69.051
2024-12-02-06:40:56-root-INFO: Loss too large (2100.171->2102.573)! Learning rate decreased to 0.00143.
2024-12-02-06:40:56-root-INFO: grad norm: 268.542 261.422 61.428
2024-12-02-06:40:57-root-INFO: grad norm: 327.521 316.553 84.047
2024-12-02-06:40:57-root-INFO: Loss too large (2072.532->2080.463)! Learning rate decreased to 0.00114.
2024-12-02-06:40:57-root-INFO: grad norm: 273.844 267.449 58.838
2024-12-02-06:40:58-root-INFO: grad norm: 259.371 250.961 65.511
2024-12-02-06:40:58-root-INFO: grad norm: 259.918 254.418 53.187
2024-12-02-06:40:59-root-INFO: grad norm: 332.012 324.034 72.347
2024-12-02-06:40:59-root-INFO: Loss too large (2014.882->2038.546)! Learning rate decreased to 0.00091.
2024-12-02-06:40:59-root-INFO: Loss too large (2014.882->2020.428)! Learning rate decreased to 0.00073.
2024-12-02-06:40:59-root-INFO: Loss Change: 2120.762 -> 2008.219
2024-12-02-06:40:59-root-INFO: Regularization Change: 0.000 -> 1.081
2024-12-02-06:40:59-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-06:40:59-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:41:00-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-06:41:00-root-INFO: grad norm: 151.108 144.186 45.210
2024-12-02-06:41:00-root-INFO: Loss too large (1975.515->1993.511)! Learning rate decreased to 0.00149.
2024-12-02-06:41:00-root-INFO: Loss too large (1975.515->1978.228)! Learning rate decreased to 0.00119.
2024-12-02-06:41:01-root-INFO: grad norm: 293.115 286.696 61.004
2024-12-02-06:41:01-root-INFO: Loss too large (1970.533->2006.528)! Learning rate decreased to 0.00095.
2024-12-02-06:41:01-root-INFO: Loss too large (1970.533->1988.766)! Learning rate decreased to 0.00076.
2024-12-02-06:41:01-root-INFO: Loss too large (1970.533->1975.467)! Learning rate decreased to 0.00061.
2024-12-02-06:41:02-root-INFO: grad norm: 228.882 224.823 42.918
2024-12-02-06:41:02-root-INFO: grad norm: 153.621 148.318 40.013
2024-12-02-06:41:03-root-INFO: grad norm: 145.394 141.402 33.835
2024-12-02-06:41:03-root-INFO: grad norm: 139.384 134.676 35.922
2024-12-02-06:41:04-root-INFO: grad norm: 139.011 135.176 32.426
2024-12-02-06:41:04-root-INFO: grad norm: 142.127 137.842 34.637
2024-12-02-06:41:04-root-INFO: Loss Change: 1975.515 -> 1933.677
2024-12-02-06:41:04-root-INFO: Regularization Change: 0.000 -> 0.220
2024-12-02-06:41:04-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-06:41:04-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-06:41:05-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-06:41:05-root-INFO: grad norm: 102.002 96.053 34.325
2024-12-02-06:41:05-root-INFO: grad norm: 141.278 138.264 29.026
2024-12-02-06:41:05-root-INFO: Loss too large (1900.991->1919.681)! Learning rate decreased to 0.00156.
2024-12-02-06:41:06-root-INFO: Loss too large (1900.991->1908.024)! Learning rate decreased to 0.00124.
2024-12-02-06:41:06-root-INFO: Loss too large (1900.991->1901.122)! Learning rate decreased to 0.00100.
2024-12-02-06:41:06-root-INFO: grad norm: 196.471 193.205 35.672
2024-12-02-06:41:06-root-INFO: Loss too large (1897.374->1919.675)! Learning rate decreased to 0.00080.
2024-12-02-06:41:07-root-INFO: Loss too large (1897.374->1904.310)! Learning rate decreased to 0.00064.
2024-12-02-06:41:07-root-INFO: grad norm: 261.183 257.132 45.825
2024-12-02-06:41:07-root-INFO: Loss too large (1896.031->1901.748)! Learning rate decreased to 0.00051.
2024-12-02-06:41:08-root-INFO: grad norm: 229.250 225.581 40.850
2024-12-02-06:41:08-root-INFO: grad norm: 193.081 189.720 35.870
2024-12-02-06:41:09-root-INFO: grad norm: 181.716 178.265 35.242
2024-12-02-06:41:09-root-INFO: grad norm: 170.000 166.844 32.609
2024-12-02-06:41:10-root-INFO: Loss Change: 1910.359 -> 1880.078
2024-12-02-06:41:10-root-INFO: Regularization Change: 0.000 -> 0.199
2024-12-02-06:41:10-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-06:41:10-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:41:10-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-06:41:10-root-INFO: grad norm: 305.411 295.234 78.182
2024-12-02-06:41:10-root-INFO: Loss too large (1880.993->2008.575)! Learning rate decreased to 0.00162.
2024-12-02-06:41:10-root-INFO: Loss too large (1880.993->1975.919)! Learning rate decreased to 0.00130.
2024-12-02-06:41:10-root-INFO: Loss too large (1880.993->1947.296)! Learning rate decreased to 0.00104.
2024-12-02-06:41:11-root-INFO: Loss too large (1880.993->1922.573)! Learning rate decreased to 0.00083.
2024-12-02-06:41:11-root-INFO: Loss too large (1880.993->1902.431)! Learning rate decreased to 0.00067.
2024-12-02-06:41:11-root-INFO: Loss too large (1880.993->1887.543)! Learning rate decreased to 0.00053.
2024-12-02-06:41:11-root-INFO: grad norm: 290.923 286.355 51.349
2024-12-02-06:41:12-root-INFO: grad norm: 282.601 275.960 60.908
2024-12-02-06:41:12-root-INFO: Loss too large (1868.118->1868.447)! Learning rate decreased to 0.00043.
2024-12-02-06:41:13-root-INFO: grad norm: 210.674 206.628 41.088
2024-12-02-06:41:13-root-INFO: grad norm: 147.435 142.133 39.183
2024-12-02-06:41:14-root-INFO: grad norm: 121.855 117.649 31.737
2024-12-02-06:41:14-root-INFO: grad norm: 101.533 96.639 31.140
2024-12-02-06:41:15-root-INFO: grad norm: 89.962 85.645 27.536
2024-12-02-06:41:15-root-INFO: Loss Change: 1880.993 -> 1845.344
2024-12-02-06:41:15-root-INFO: Regularization Change: 0.000 -> 0.096
2024-12-02-06:41:15-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-06:41:15-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:41:15-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-06:41:15-root-INFO: grad norm: 472.782 463.969 90.861
2024-12-02-06:41:15-root-INFO: Loss too large (1858.396->2055.495)! Learning rate decreased to 0.00170.
2024-12-02-06:41:16-root-INFO: Loss too large (1858.396->2022.281)! Learning rate decreased to 0.00136.
2024-12-02-06:41:16-root-INFO: Loss too large (1858.396->1988.888)! Learning rate decreased to 0.00108.
2024-12-02-06:41:16-root-INFO: Loss too large (1858.396->1956.134)! Learning rate decreased to 0.00087.
2024-12-02-06:41:16-root-INFO: Loss too large (1858.396->1924.523)! Learning rate decreased to 0.00069.
2024-12-02-06:41:16-root-INFO: Loss too large (1858.396->1894.976)! Learning rate decreased to 0.00056.
2024-12-02-06:41:16-root-INFO: Loss too large (1858.396->1869.865)! Learning rate decreased to 0.00044.
2024-12-02-06:41:17-root-INFO: grad norm: 350.091 346.317 51.267
2024-12-02-06:41:17-root-INFO: grad norm: 225.208 220.090 47.739
2024-12-02-06:41:18-root-INFO: grad norm: 203.256 200.004 36.214
2024-12-02-06:41:18-root-INFO: grad norm: 180.679 176.528 38.509
2024-12-02-06:41:19-root-INFO: grad norm: 168.177 165.148 31.775
2024-12-02-06:41:19-root-INFO: grad norm: 155.613 152.019 33.252
2024-12-02-06:41:20-root-INFO: grad norm: 147.722 144.853 28.972
2024-12-02-06:41:20-root-INFO: Loss Change: 1858.396 -> 1816.820
2024-12-02-06:41:20-root-INFO: Regularization Change: 0.000 -> 0.074
2024-12-02-06:41:20-root-INFO: Undo step: 190
2024-12-02-06:41:20-root-INFO: Undo step: 191
2024-12-02-06:41:20-root-INFO: Undo step: 192
2024-12-02-06:41:20-root-INFO: Undo step: 193
2024-12-02-06:41:20-root-INFO: Undo step: 194
2024-12-02-06:41:20-root-INFO: step: 195 lr_xt 0.00170783
2024-12-02-06:41:20-root-INFO: grad norm: 1054.165 993.159 353.412
2024-12-02-06:41:21-root-INFO: grad norm: 706.989 668.338 230.561
2024-12-02-06:41:21-root-INFO: Loss too large (2900.434->3147.486)! Learning rate decreased to 0.00137.
2024-12-02-06:41:21-root-INFO: Loss too large (2900.434->2971.425)! Learning rate decreased to 0.00109.
2024-12-02-06:41:22-root-INFO: grad norm: 863.475 839.605 201.627
2024-12-02-06:41:22-root-INFO: grad norm: 987.404 951.994 262.057
2024-12-02-06:41:23-root-INFO: grad norm: 909.013 879.171 231.004
2024-12-02-06:41:23-root-INFO: grad norm: 755.025 725.824 207.946
2024-12-02-06:41:24-root-INFO: grad norm: 643.995 620.611 171.960
2024-12-02-06:41:24-root-INFO: grad norm: 512.254 490.844 146.550
2024-12-02-06:41:24-root-INFO: Loss Change: 4002.669 -> 2357.519
2024-12-02-06:41:24-root-INFO: Regularization Change: 0.000 -> 10.298
2024-12-02-06:41:24-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-02-06:41:24-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:41:25-root-INFO: step: 194 lr_xt 0.00178371
2024-12-02-06:41:25-root-INFO: grad norm: 537.035 515.826 149.431
2024-12-02-06:41:25-root-INFO: Loss too large (2363.062->2555.614)! Learning rate decreased to 0.00143.
2024-12-02-06:41:25-root-INFO: Loss too large (2363.062->2408.961)! Learning rate decreased to 0.00114.
2024-12-02-06:41:25-root-INFO: grad norm: 523.445 506.195 133.272
2024-12-02-06:41:26-root-INFO: Loss too large (2324.927->2335.961)! Learning rate decreased to 0.00091.
2024-12-02-06:41:26-root-INFO: grad norm: 470.514 458.818 104.258
2024-12-02-06:41:26-root-INFO: Loss too large (2275.929->2279.168)! Learning rate decreased to 0.00073.
2024-12-02-06:41:27-root-INFO: grad norm: 309.952 302.168 69.030
2024-12-02-06:41:27-root-INFO: grad norm: 167.817 160.153 50.137
2024-12-02-06:41:28-root-INFO: grad norm: 174.552 167.043 50.645
2024-12-02-06:41:28-root-INFO: grad norm: 195.195 188.739 49.789
2024-12-02-06:41:29-root-INFO: grad norm: 211.134 204.700 51.724
2024-12-02-06:41:29-root-INFO: Loss Change: 2363.062 -> 2188.094
2024-12-02-06:41:29-root-INFO: Regularization Change: 0.000 -> 0.699
2024-12-02-06:41:29-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-02-06:41:29-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00017.
2024-12-02-06:41:29-root-INFO: step: 193 lr_xt 0.00186266
2024-12-02-06:41:29-root-INFO: grad norm: 655.133 643.581 122.484
2024-12-02-06:41:29-root-INFO: Loss too large (2214.742->2382.391)! Learning rate decreased to 0.00149.
2024-12-02-06:41:30-root-INFO: Loss too large (2214.742->2342.749)! Learning rate decreased to 0.00119.
2024-12-02-06:41:30-root-INFO: Loss too large (2214.742->2304.461)! Learning rate decreased to 0.00095.
2024-12-02-06:41:30-root-INFO: Loss too large (2214.742->2268.144)! Learning rate decreased to 0.00076.
2024-12-02-06:41:30-root-INFO: Loss too large (2214.742->2234.607)! Learning rate decreased to 0.00061.
2024-12-02-06:41:30-root-INFO: grad norm: 348.784 342.808 64.289
2024-12-02-06:41:31-root-INFO: grad norm: 122.730 115.552 41.356
2024-12-02-06:41:31-root-INFO: grad norm: 120.351 113.455 40.154
2024-12-02-06:41:32-root-INFO: grad norm: 119.899 113.272 39.310
2024-12-02-06:41:32-root-INFO: grad norm: 121.173 115.052 38.024
2024-12-02-06:41:33-root-INFO: grad norm: 123.311 117.174 38.419
2024-12-02-06:41:33-root-INFO: grad norm: 127.579 122.094 37.006
2024-12-02-06:41:34-root-INFO: Loss Change: 2214.742 -> 2113.869
2024-12-02-06:41:34-root-INFO: Regularization Change: 0.000 -> 0.269
2024-12-02-06:41:34-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-02-06:41:34-root-INFO: Coefficient of regularization decay: 0.00017 -> 0.00018.
2024-12-02-06:41:34-root-INFO: step: 192 lr_xt 0.00194479
2024-12-02-06:41:34-root-INFO: grad norm: 123.091 116.146 40.762
2024-12-02-06:41:35-root-INFO: grad norm: 226.559 218.574 59.618
2024-12-02-06:41:35-root-INFO: Loss too large (2081.938->2192.975)! Learning rate decreased to 0.00156.
2024-12-02-06:41:35-root-INFO: Loss too large (2081.938->2137.816)! Learning rate decreased to 0.00124.
2024-12-02-06:41:35-root-INFO: Loss too large (2081.938->2104.532)! Learning rate decreased to 0.00100.
2024-12-02-06:41:35-root-INFO: Loss too large (2081.938->2085.613)! Learning rate decreased to 0.00080.
2024-12-02-06:41:36-root-INFO: grad norm: 288.786 284.098 51.822
2024-12-02-06:41:36-root-INFO: Loss too large (2075.671->2091.242)! Learning rate decreased to 0.00064.
2024-12-02-06:41:36-root-INFO: Loss too large (2075.671->2078.163)! Learning rate decreased to 0.00051.
2024-12-02-06:41:37-root-INFO: grad norm: 234.941 229.980 48.025
2024-12-02-06:41:37-root-INFO: grad norm: 184.439 180.607 37.403
2024-12-02-06:41:38-root-INFO: grad norm: 164.634 159.662 40.155
2024-12-02-06:41:38-root-INFO: grad norm: 145.727 141.802 33.594
2024-12-02-06:41:39-root-INFO: grad norm: 135.020 129.941 36.686
2024-12-02-06:41:39-root-INFO: Loss Change: 2090.133 -> 2045.610
2024-12-02-06:41:39-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-02-06:41:39-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-02-06:41:39-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:41:39-root-INFO: step: 191 lr_xt 0.00203021
2024-12-02-06:41:39-root-INFO: grad norm: 554.831 544.546 106.335
2024-12-02-06:41:40-root-INFO: Loss too large (2075.260->2263.717)! Learning rate decreased to 0.00162.
2024-12-02-06:41:40-root-INFO: Loss too large (2075.260->2226.700)! Learning rate decreased to 0.00130.
2024-12-02-06:41:40-root-INFO: Loss too large (2075.260->2190.954)! Learning rate decreased to 0.00104.
2024-12-02-06:41:40-root-INFO: Loss too large (2075.260->2156.485)! Learning rate decreased to 0.00083.
2024-12-02-06:41:40-root-INFO: Loss too large (2075.260->2123.344)! Learning rate decreased to 0.00067.
2024-12-02-06:41:40-root-INFO: Loss too large (2075.260->2092.348)! Learning rate decreased to 0.00053.
2024-12-02-06:41:41-root-INFO: grad norm: 386.249 380.695 65.267
2024-12-02-06:41:41-root-INFO: grad norm: 197.611 192.043 46.578
2024-12-02-06:41:42-root-INFO: grad norm: 192.070 187.199 42.979
2024-12-02-06:41:42-root-INFO: grad norm: 187.040 182.408 41.367
2024-12-02-06:41:43-root-INFO: grad norm: 184.520 179.999 40.598
2024-12-02-06:41:43-root-INFO: grad norm: 182.760 178.680 38.405
2024-12-02-06:41:44-root-INFO: grad norm: 182.385 178.063 39.469
2024-12-02-06:41:44-root-INFO: Loss Change: 2075.260 -> 2004.815
2024-12-02-06:41:44-root-INFO: Regularization Change: 0.000 -> 0.163
2024-12-02-06:41:44-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-02-06:41:44-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:41:44-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-06:41:44-root-INFO: grad norm: 534.667 526.224 94.638
2024-12-02-06:41:45-root-INFO: Loss too large (2024.475->2223.474)! Learning rate decreased to 0.00170.
2024-12-02-06:41:45-root-INFO: Loss too large (2024.475->2189.359)! Learning rate decreased to 0.00136.
2024-12-02-06:41:45-root-INFO: Loss too large (2024.475->2154.752)! Learning rate decreased to 0.00108.
2024-12-02-06:41:45-root-INFO: Loss too large (2024.475->2119.945)! Learning rate decreased to 0.00087.
2024-12-02-06:41:45-root-INFO: Loss too large (2024.475->2085.224)! Learning rate decreased to 0.00069.
2024-12-02-06:41:45-root-INFO: Loss too large (2024.475->2051.780)! Learning rate decreased to 0.00056.
2024-12-02-06:41:46-root-INFO: grad norm: 406.127 401.159 63.328
2024-12-02-06:41:46-root-INFO: grad norm: 255.028 250.626 47.176
2024-12-02-06:41:47-root-INFO: grad norm: 262.505 258.318 46.699
2024-12-02-06:41:47-root-INFO: grad norm: 272.972 268.809 47.491
2024-12-02-06:41:48-root-INFO: grad norm: 278.197 273.935 48.512
2024-12-02-06:41:48-root-INFO: grad norm: 285.440 281.395 47.886
2024-12-02-06:41:49-root-INFO: Loss too large (1974.912->1975.205)! Learning rate decreased to 0.00044.
2024-12-02-06:41:49-root-INFO: grad norm: 208.703 204.767 40.343
2024-12-02-06:41:49-root-INFO: Loss Change: 2024.475 -> 1962.449
2024-12-02-06:41:49-root-INFO: Regularization Change: 0.000 -> 0.131
2024-12-02-06:41:49-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-06:41:49-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:41:50-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-06:41:50-root-INFO: grad norm: 417.397 410.910 73.300
2024-12-02-06:41:50-root-INFO: Loss too large (1972.403->2157.836)! Learning rate decreased to 0.00177.
2024-12-02-06:41:50-root-INFO: Loss too large (1972.403->2126.559)! Learning rate decreased to 0.00142.
2024-12-02-06:41:50-root-INFO: Loss too large (1972.403->2094.293)! Learning rate decreased to 0.00113.
2024-12-02-06:41:50-root-INFO: Loss too large (1972.403->2061.510)! Learning rate decreased to 0.00091.
2024-12-02-06:41:51-root-INFO: Loss too large (1972.403->2028.976)! Learning rate decreased to 0.00072.
2024-12-02-06:41:51-root-INFO: Loss too large (1972.403->1999.584)! Learning rate decreased to 0.00058.
2024-12-02-06:41:51-root-INFO: Loss too large (1972.403->1976.792)! Learning rate decreased to 0.00046.
2024-12-02-06:41:51-root-INFO: grad norm: 302.844 298.722 49.795
2024-12-02-06:41:52-root-INFO: grad norm: 206.837 202.865 40.339
2024-12-02-06:41:52-root-INFO: grad norm: 173.579 169.870 35.690
2024-12-02-06:41:53-root-INFO: grad norm: 144.743 141.205 31.803
2024-12-02-06:41:53-root-INFO: grad norm: 127.554 123.784 30.782
2024-12-02-06:41:54-root-INFO: grad norm: 113.117 109.634 27.853
2024-12-02-06:41:54-root-INFO: grad norm: 103.490 99.590 28.143
2024-12-02-06:41:55-root-INFO: Loss Change: 1972.403 -> 1928.104
2024-12-02-06:41:55-root-INFO: Regularization Change: 0.000 -> 0.096
2024-12-02-06:41:55-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-06:41:55-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:41:55-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-06:41:55-root-INFO: grad norm: 402.662 396.608 69.566
2024-12-02-06:41:55-root-INFO: Loss too large (1935.318->2135.219)! Learning rate decreased to 0.00185.
2024-12-02-06:41:55-root-INFO: Loss too large (1935.318->2099.709)! Learning rate decreased to 0.00148.
2024-12-02-06:41:55-root-INFO: Loss too large (1935.318->2065.316)! Learning rate decreased to 0.00118.
2024-12-02-06:41:56-root-INFO: Loss too large (1935.318->2031.535)! Learning rate decreased to 0.00095.
2024-12-02-06:41:56-root-INFO: Loss too large (1935.318->1998.386)! Learning rate decreased to 0.00076.
2024-12-02-06:41:56-root-INFO: Loss too large (1935.318->1968.246)! Learning rate decreased to 0.00060.
2024-12-02-06:41:56-root-INFO: Loss too large (1935.318->1944.499)! Learning rate decreased to 0.00048.
2024-12-02-06:41:57-root-INFO: grad norm: 334.317 330.118 52.821
2024-12-02-06:41:57-root-INFO: grad norm: 271.377 267.515 45.620
2024-12-02-06:41:58-root-INFO: grad norm: 249.736 245.961 43.259
2024-12-02-06:41:58-root-INFO: grad norm: 227.761 224.468 38.589
2024-12-02-06:41:59-root-INFO: grad norm: 215.659 212.047 39.305
2024-12-02-06:41:59-root-INFO: grad norm: 203.533 200.532 34.827
2024-12-02-06:42:00-root-INFO: grad norm: 196.097 192.580 36.975
2024-12-02-06:42:00-root-INFO: Loss Change: 1935.318 -> 1894.730
2024-12-02-06:42:00-root-INFO: Regularization Change: 0.000 -> 0.099
2024-12-02-06:42:00-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-06:42:00-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-06:42:00-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-06:42:01-root-INFO: grad norm: 453.134 446.214 78.890
2024-12-02-06:42:01-root-INFO: Loss too large (1919.222->2120.798)! Learning rate decreased to 0.00193.
2024-12-02-06:42:01-root-INFO: Loss too large (1919.222->2094.351)! Learning rate decreased to 0.00154.
2024-12-02-06:42:01-root-INFO: Loss too large (1919.222->2064.689)! Learning rate decreased to 0.00123.
2024-12-02-06:42:01-root-INFO: Loss too large (1919.222->2032.435)! Learning rate decreased to 0.00099.
2024-12-02-06:42:01-root-INFO: Loss too large (1919.222->1998.110)! Learning rate decreased to 0.00079.
2024-12-02-06:42:02-root-INFO: Loss too large (1919.222->1963.828)! Learning rate decreased to 0.00063.
2024-12-02-06:42:02-root-INFO: Loss too large (1919.222->1934.235)! Learning rate decreased to 0.00050.
2024-12-02-06:42:02-root-INFO: grad norm: 375.627 371.147 57.845
2024-12-02-06:42:03-root-INFO: grad norm: 301.173 296.417 53.314
2024-12-02-06:42:03-root-INFO: grad norm: 282.124 278.187 46.968
2024-12-02-06:42:04-root-INFO: grad norm: 261.885 257.859 45.743
2024-12-02-06:42:05-root-INFO: grad norm: 251.240 247.529 43.026
2024-12-02-06:42:05-root-INFO: grad norm: 240.019 236.442 41.281
2024-12-02-06:42:06-root-INFO: grad norm: 233.417 229.848 40.666
2024-12-02-06:42:06-root-INFO: Loss Change: 1919.222 -> 1871.763
2024-12-02-06:42:06-root-INFO: Regularization Change: 0.000 -> 0.117
2024-12-02-06:42:06-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-06:42:06-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:42:06-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-06:42:06-root-INFO: grad norm: 498.075 491.788 78.892
2024-12-02-06:42:07-root-INFO: Loss too large (1883.104->2100.397)! Learning rate decreased to 0.00201.
2024-12-02-06:42:07-root-INFO: Loss too large (1883.104->2073.707)! Learning rate decreased to 0.00161.
2024-12-02-06:42:07-root-INFO: Loss too large (1883.104->2044.067)! Learning rate decreased to 0.00129.
2024-12-02-06:42:07-root-INFO: Loss too large (1883.104->2011.344)! Learning rate decreased to 0.00103.
2024-12-02-06:42:07-root-INFO: Loss too large (1883.104->1975.866)! Learning rate decreased to 0.00082.
2024-12-02-06:42:07-root-INFO: Loss too large (1883.104->1938.499)! Learning rate decreased to 0.00066.
2024-12-02-06:42:08-root-INFO: Loss too large (1883.104->1903.586)! Learning rate decreased to 0.00053.
2024-12-02-06:42:08-root-INFO: grad norm: 410.979 406.560 60.109
2024-12-02-06:42:09-root-INFO: grad norm: 334.574 330.507 52.006
2024-12-02-06:42:09-root-INFO: grad norm: 323.494 319.549 50.361
2024-12-02-06:42:10-root-INFO: grad norm: 311.430 307.792 47.465
2024-12-02-06:42:10-root-INFO: grad norm: 305.962 302.124 48.309
2024-12-02-06:42:11-root-INFO: grad norm: 299.979 296.578 45.039
2024-12-02-06:42:11-root-INFO: grad norm: 297.008 293.230 47.225
2024-12-02-06:42:11-root-INFO: Loss Change: 1883.104 -> 1835.186
2024-12-02-06:42:11-root-INFO: Regularization Change: 0.000 -> 0.111
2024-12-02-06:42:11-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-06:42:11-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:42:12-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-06:42:12-root-INFO: grad norm: 449.747 444.815 66.428
2024-12-02-06:42:12-root-INFO: Loss too large (1840.930->2058.615)! Learning rate decreased to 0.00209.
2024-12-02-06:42:12-root-INFO: Loss too large (1840.930->2034.016)! Learning rate decreased to 0.00168.
2024-12-02-06:42:12-root-INFO: Loss too large (1840.930->2005.316)! Learning rate decreased to 0.00134.
2024-12-02-06:42:13-root-INFO: Loss too large (1840.930->1972.891)! Learning rate decreased to 0.00107.
2024-12-02-06:42:13-root-INFO: Loss too large (1840.930->1937.166)! Learning rate decreased to 0.00086.
2024-12-02-06:42:13-root-INFO: Loss too large (1840.930->1899.661)! Learning rate decreased to 0.00069.
2024-12-02-06:42:13-root-INFO: Loss too large (1840.930->1865.461)! Learning rate decreased to 0.00055.
2024-12-02-06:42:14-root-INFO: grad norm: 409.753 405.535 58.645
2024-12-02-06:42:14-root-INFO: grad norm: 373.761 369.868 53.807
2024-12-02-06:42:14-root-INFO: Loss too large (1825.288->1825.691)! Learning rate decreased to 0.00044.
2024-12-02-06:42:15-root-INFO: grad norm: 260.220 256.788 42.121
2024-12-02-06:42:15-root-INFO: grad norm: 188.140 185.678 30.339
2024-12-02-06:42:16-root-INFO: grad norm: 151.241 148.203 30.161
2024-12-02-06:42:16-root-INFO: grad norm: 124.331 121.978 24.073
2024-12-02-06:42:17-root-INFO: grad norm: 107.247 104.151 25.583
2024-12-02-06:42:17-root-INFO: Loss Change: 1840.930 -> 1794.088
2024-12-02-06:42:17-root-INFO: Regularization Change: 0.000 -> 0.087
2024-12-02-06:42:17-root-INFO: Undo step: 185
2024-12-02-06:42:17-root-INFO: Undo step: 186
2024-12-02-06:42:17-root-INFO: Undo step: 187
2024-12-02-06:42:17-root-INFO: Undo step: 188
2024-12-02-06:42:17-root-INFO: Undo step: 189
2024-12-02-06:42:17-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-06:42:17-root-INFO: grad norm: 847.461 771.800 350.021
2024-12-02-06:42:18-root-INFO: grad norm: 454.087 433.147 136.303
2024-12-02-06:42:18-root-INFO: grad norm: 258.203 238.240 99.550
2024-12-02-06:42:19-root-INFO: grad norm: 375.848 366.623 82.758
2024-12-02-06:42:19-root-INFO: Loss too large (2253.266->2314.414)! Learning rate decreased to 0.00170.
2024-12-02-06:42:19-root-INFO: Loss too large (2253.266->2290.732)! Learning rate decreased to 0.00136.
2024-12-02-06:42:19-root-INFO: Loss too large (2253.266->2269.238)! Learning rate decreased to 0.00108.
2024-12-02-06:42:20-root-INFO: grad norm: 338.258 329.489 76.520
2024-12-02-06:42:20-root-INFO: grad norm: 279.219 270.725 68.347
2024-12-02-06:42:21-root-INFO: grad norm: 325.668 318.176 69.454
2024-12-02-06:42:21-root-INFO: grad norm: 521.860 514.746 85.873
2024-12-02-06:42:21-root-INFO: Loss too large (2163.738->2212.333)! Learning rate decreased to 0.00087.
2024-12-02-06:42:22-root-INFO: Loss too large (2163.738->2182.853)! Learning rate decreased to 0.00069.
2024-12-02-06:42:22-root-INFO: Loss Change: 3354.128 -> 2157.840
2024-12-02-06:42:22-root-INFO: Regularization Change: 0.000 -> 8.906
2024-12-02-06:42:22-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-06:42:22-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:42:22-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-06:42:22-root-INFO: grad norm: 327.371 321.397 62.253
2024-12-02-06:42:22-root-INFO: Loss too large (2125.456->2403.419)! Learning rate decreased to 0.00177.
2024-12-02-06:42:23-root-INFO: Loss too large (2125.456->2289.077)! Learning rate decreased to 0.00142.
2024-12-02-06:42:23-root-INFO: Loss too large (2125.456->2202.336)! Learning rate decreased to 0.00113.
2024-12-02-06:42:23-root-INFO: Loss too large (2125.456->2143.842)! Learning rate decreased to 0.00091.
2024-12-02-06:42:23-root-INFO: grad norm: 452.828 446.278 76.739
2024-12-02-06:42:24-root-INFO: Loss too large (2110.085->2139.641)! Learning rate decreased to 0.00072.
2024-12-02-06:42:24-root-INFO: Loss too large (2110.085->2115.369)! Learning rate decreased to 0.00058.
2024-12-02-06:42:24-root-INFO: grad norm: 313.671 308.016 59.296
2024-12-02-06:42:25-root-INFO: grad norm: 180.410 173.735 48.618
2024-12-02-06:42:25-root-INFO: grad norm: 172.036 165.712 46.216
2024-12-02-06:42:26-root-INFO: grad norm: 168.275 162.011 45.487
2024-12-02-06:42:26-root-INFO: grad norm: 169.892 164.036 44.221
2024-12-02-06:42:27-root-INFO: grad norm: 177.803 172.203 44.273
2024-12-02-06:42:27-root-INFO: Loss Change: 2125.456 -> 2034.286
2024-12-02-06:42:27-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-02-06:42:27-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-06:42:27-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:42:27-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-06:42:27-root-INFO: grad norm: 144.282 136.640 46.334
2024-12-02-06:42:28-root-INFO: Loss too large (2016.476->2024.348)! Learning rate decreased to 0.00185.
2024-12-02-06:42:28-root-INFO: grad norm: 338.571 333.303 59.491
2024-12-02-06:42:28-root-INFO: Loss too large (2014.789->2303.034)! Learning rate decreased to 0.00148.
2024-12-02-06:42:28-root-INFO: Loss too large (2014.789->2197.992)! Learning rate decreased to 0.00118.
2024-12-02-06:42:29-root-INFO: Loss too large (2014.789->2117.885)! Learning rate decreased to 0.00095.
2024-12-02-06:42:29-root-INFO: Loss too large (2014.789->2061.398)! Learning rate decreased to 0.00076.
2024-12-02-06:42:29-root-INFO: Loss too large (2014.789->2025.361)! Learning rate decreased to 0.00060.
2024-12-02-06:42:29-root-INFO: grad norm: 384.170 379.552 59.388
2024-12-02-06:42:30-root-INFO: Loss too large (2005.018->2013.953)! Learning rate decreased to 0.00048.
2024-12-02-06:42:30-root-INFO: grad norm: 312.851 308.527 51.835
2024-12-02-06:42:31-root-INFO: grad norm: 244.624 240.424 45.137
2024-12-02-06:42:31-root-INFO: grad norm: 230.888 226.759 43.465
2024-12-02-06:42:32-root-INFO: grad norm: 219.481 215.397 42.147
2024-12-02-06:42:32-root-INFO: grad norm: 216.883 212.872 41.520
2024-12-02-06:42:32-root-INFO: Loss Change: 2016.476 -> 1969.434
2024-12-02-06:42:32-root-INFO: Regularization Change: 0.000 -> 0.264
2024-12-02-06:42:32-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-06:42:32-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-06:42:32-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-06:42:33-root-INFO: grad norm: 511.280 504.707 81.715
2024-12-02-06:42:33-root-INFO: Loss too large (1988.574->2200.523)! Learning rate decreased to 0.00193.
2024-12-02-06:42:33-root-INFO: Loss too large (1988.574->2174.483)! Learning rate decreased to 0.00154.
2024-12-02-06:42:33-root-INFO: Loss too large (1988.574->2144.631)! Learning rate decreased to 0.00123.
2024-12-02-06:42:33-root-INFO: Loss too large (1988.574->2111.903)! Learning rate decreased to 0.00099.
2024-12-02-06:42:33-root-INFO: Loss too large (1988.574->2077.479)! Learning rate decreased to 0.00079.
2024-12-02-06:42:34-root-INFO: Loss too large (1988.574->2042.931)! Learning rate decreased to 0.00063.
2024-12-02-06:42:34-root-INFO: Loss too large (1988.574->2011.197)! Learning rate decreased to 0.00050.
2024-12-02-06:42:34-root-INFO: grad norm: 405.698 401.612 57.433
2024-12-02-06:42:35-root-INFO: grad norm: 294.996 290.194 53.010
2024-12-02-06:42:35-root-INFO: grad norm: 298.279 294.698 46.084
2024-12-02-06:42:36-root-INFO: grad norm: 305.424 300.947 52.103
2024-12-02-06:42:36-root-INFO: Loss too large (1951.854->1952.146)! Learning rate decreased to 0.00040.
2024-12-02-06:42:36-root-INFO: grad norm: 225.948 222.547 39.054
2024-12-02-06:42:37-root-INFO: grad norm: 165.548 161.334 37.113
2024-12-02-06:42:38-root-INFO: grad norm: 138.421 134.648 32.100
2024-12-02-06:42:38-root-INFO: Loss Change: 1988.574 -> 1932.039
2024-12-02-06:42:38-root-INFO: Regularization Change: 0.000 -> 0.106
2024-12-02-06:42:38-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-06:42:38-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:42:38-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-06:42:38-root-INFO: grad norm: 398.135 393.039 63.497
2024-12-02-06:42:38-root-INFO: Loss too large (1933.146->2144.534)! Learning rate decreased to 0.00201.
2024-12-02-06:42:39-root-INFO: Loss too large (1933.146->2114.667)! Learning rate decreased to 0.00161.
2024-12-02-06:42:39-root-INFO: Loss too large (1933.146->2082.847)! Learning rate decreased to 0.00129.
2024-12-02-06:42:39-root-INFO: Loss too large (1933.146->2049.772)! Learning rate decreased to 0.00103.
2024-12-02-06:42:39-root-INFO: Loss too large (1933.146->2016.394)! Learning rate decreased to 0.00082.
2024-12-02-06:42:39-root-INFO: Loss too large (1933.146->1984.699)! Learning rate decreased to 0.00066.
2024-12-02-06:42:39-root-INFO: Loss too large (1933.146->1957.622)! Learning rate decreased to 0.00053.
2024-12-02-06:42:39-root-INFO: Loss too large (1933.146->1937.387)! Learning rate decreased to 0.00042.
2024-12-02-06:42:40-root-INFO: grad norm: 303.865 300.322 46.266
2024-12-02-06:42:40-root-INFO: grad norm: 228.503 224.544 42.349
2024-12-02-06:42:41-root-INFO: grad norm: 198.948 195.589 36.401
2024-12-02-06:42:41-root-INFO: grad norm: 173.389 169.617 35.972
2024-12-02-06:42:42-root-INFO: grad norm: 158.126 154.699 32.746
2024-12-02-06:42:42-root-INFO: grad norm: 144.898 141.159 32.706
2024-12-02-06:42:43-root-INFO: grad norm: 136.050 132.545 30.683
2024-12-02-06:42:43-root-INFO: Loss Change: 1933.146 -> 1894.758
2024-12-02-06:42:43-root-INFO: Regularization Change: 0.000 -> 0.078
2024-12-02-06:42:43-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-06:42:43-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:42:43-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-06:42:43-root-INFO: grad norm: 295.463 291.620 47.500
2024-12-02-06:42:44-root-INFO: Loss too large (1888.517->2081.835)! Learning rate decreased to 0.00209.
2024-12-02-06:42:44-root-INFO: Loss too large (1888.517->2052.329)! Learning rate decreased to 0.00168.
2024-12-02-06:42:44-root-INFO: Loss too large (1888.517->2021.021)! Learning rate decreased to 0.00134.
2024-12-02-06:42:44-root-INFO: Loss too large (1888.517->1988.903)! Learning rate decreased to 0.00107.
2024-12-02-06:42:44-root-INFO: Loss too large (1888.517->1957.853)! Learning rate decreased to 0.00086.
2024-12-02-06:42:44-root-INFO: Loss too large (1888.517->1930.599)! Learning rate decreased to 0.00069.
2024-12-02-06:42:45-root-INFO: Loss too large (1888.517->1909.326)! Learning rate decreased to 0.00055.
2024-12-02-06:42:45-root-INFO: Loss too large (1888.517->1894.589)! Learning rate decreased to 0.00044.
2024-12-02-06:42:45-root-INFO: grad norm: 275.059 271.916 41.465
2024-12-02-06:42:46-root-INFO: grad norm: 257.083 253.665 41.787
2024-12-02-06:42:46-root-INFO: grad norm: 247.878 244.829 38.762
2024-12-02-06:42:47-root-INFO: grad norm: 240.042 236.822 39.185
2024-12-02-06:42:47-root-INFO: grad norm: 236.254 233.265 37.465
2024-12-02-06:42:48-root-INFO: grad norm: 233.743 230.646 37.923
2024-12-02-06:42:48-root-INFO: grad norm: 233.385 230.437 36.980
2024-12-02-06:42:48-root-INFO: Loss Change: 1888.517 -> 1865.111
2024-12-02-06:42:48-root-INFO: Regularization Change: 0.000 -> 0.069
2024-12-02-06:42:48-root-INFO: Undo step: 185
2024-12-02-06:42:48-root-INFO: Undo step: 186
2024-12-02-06:42:48-root-INFO: Undo step: 187
2024-12-02-06:42:48-root-INFO: Undo step: 188
2024-12-02-06:42:48-root-INFO: Undo step: 189
2024-12-02-06:42:49-root-INFO: step: 190 lr_xt 0.00211904
2024-12-02-06:42:49-root-INFO: grad norm: 641.068 590.872 248.673
2024-12-02-06:42:49-root-INFO: grad norm: 329.350 308.515 115.282
2024-12-02-06:42:50-root-INFO: grad norm: 325.709 312.218 92.771
2024-12-02-06:42:50-root-INFO: Loss too large (2535.519->2542.983)! Learning rate decreased to 0.00170.
2024-12-02-06:42:50-root-INFO: grad norm: 493.217 477.060 125.206
2024-12-02-06:42:51-root-INFO: Loss too large (2507.684->2585.893)! Learning rate decreased to 0.00136.
2024-12-02-06:42:51-root-INFO: grad norm: 597.137 574.061 164.397
2024-12-02-06:42:52-root-INFO: grad norm: 634.757 613.654 162.315
2024-12-02-06:42:52-root-INFO: grad norm: 608.635 586.572 162.388
2024-12-02-06:42:53-root-INFO: grad norm: 550.377 531.913 141.363
2024-12-02-06:42:53-root-INFO: Loss Change: 3135.107 -> 2297.809
2024-12-02-06:42:53-root-INFO: Regularization Change: 0.000 -> 8.410
2024-12-02-06:42:53-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-02-06:42:53-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:42:53-root-INFO: step: 189 lr_xt 0.00221139
2024-12-02-06:42:53-root-INFO: grad norm: 500.204 483.015 130.002
2024-12-02-06:42:53-root-INFO: Loss too large (2286.888->2431.833)! Learning rate decreased to 0.00177.
2024-12-02-06:42:54-root-INFO: Loss too large (2286.888->2305.951)! Learning rate decreased to 0.00142.
2024-12-02-06:42:54-root-INFO: grad norm: 436.399 423.816 104.039
2024-12-02-06:42:55-root-INFO: grad norm: 374.096 361.124 97.660
2024-12-02-06:42:55-root-INFO: grad norm: 331.749 322.391 78.239
2024-12-02-06:42:56-root-INFO: grad norm: 302.783 292.018 80.018
2024-12-02-06:42:56-root-INFO: grad norm: 284.919 277.177 65.965
2024-12-02-06:42:56-root-INFO: grad norm: 278.458 269.135 71.448
2024-12-02-06:42:57-root-INFO: grad norm: 329.179 323.044 63.253
2024-12-02-06:42:57-root-INFO: Loss too large (2043.286->2055.311)! Learning rate decreased to 0.00113.
2024-12-02-06:42:57-root-INFO: Loss Change: 2286.888 -> 2031.508
2024-12-02-06:42:57-root-INFO: Regularization Change: 0.000 -> 1.840
2024-12-02-06:42:57-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-02-06:42:57-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00018.
2024-12-02-06:42:58-root-INFO: step: 188 lr_xt 0.00230740
2024-12-02-06:42:58-root-INFO: grad norm: 300.110 292.589 66.770
2024-12-02-06:42:58-root-INFO: Loss too large (2006.376->2202.915)! Learning rate decreased to 0.00185.
2024-12-02-06:42:58-root-INFO: Loss too large (2006.376->2101.213)! Learning rate decreased to 0.00148.
2024-12-02-06:42:58-root-INFO: Loss too large (2006.376->2036.643)! Learning rate decreased to 0.00118.
2024-12-02-06:42:59-root-INFO: grad norm: 439.648 434.319 68.246
2024-12-02-06:42:59-root-INFO: Loss too large (1999.872->2039.967)! Learning rate decreased to 0.00095.
2024-12-02-06:42:59-root-INFO: Loss too large (1999.872->2012.239)! Learning rate decreased to 0.00076.
2024-12-02-06:43:00-root-INFO: grad norm: 284.581 281.530 41.558
2024-12-02-06:43:00-root-INFO: grad norm: 126.329 121.801 33.520
2024-12-02-06:43:01-root-INFO: grad norm: 129.080 125.345 30.828
2024-12-02-06:43:01-root-INFO: grad norm: 146.134 141.843 35.149
2024-12-02-06:43:02-root-INFO: grad norm: 171.073 167.959 32.493
2024-12-02-06:43:02-root-INFO: grad norm: 231.521 227.441 43.276
2024-12-02-06:43:02-root-INFO: Loss too large (1934.191->1937.598)! Learning rate decreased to 0.00060.
2024-12-02-06:43:03-root-INFO: Loss Change: 2006.376 -> 1931.094
2024-12-02-06:43:03-root-INFO: Regularization Change: 0.000 -> 0.355
2024-12-02-06:43:03-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-02-06:43:03-root-INFO: Coefficient of regularization decay: 0.00018 -> 0.00019.
2024-12-02-06:43:03-root-INFO: step: 187 lr_xt 0.00240719
2024-12-02-06:43:03-root-INFO: grad norm: 95.771 88.925 35.557
2024-12-02-06:43:03-root-INFO: grad norm: 180.403 176.636 36.673
2024-12-02-06:43:04-root-INFO: Loss too large (1897.352->1985.488)! Learning rate decreased to 0.00193.
2024-12-02-06:43:04-root-INFO: Loss too large (1897.352->1959.048)! Learning rate decreased to 0.00154.
2024-12-02-06:43:04-root-INFO: Loss too large (1897.352->1936.983)! Learning rate decreased to 0.00123.
2024-12-02-06:43:04-root-INFO: Loss too large (1897.352->1919.990)! Learning rate decreased to 0.00099.
2024-12-02-06:43:04-root-INFO: Loss too large (1897.352->1907.982)! Learning rate decreased to 0.00079.
2024-12-02-06:43:05-root-INFO: Loss too large (1897.352->1900.189)! Learning rate decreased to 0.00063.
2024-12-02-06:43:05-root-INFO: grad norm: 208.515 205.831 33.348
2024-12-02-06:43:05-root-INFO: grad norm: 269.146 264.849 47.898
2024-12-02-06:43:06-root-INFO: Loss too large (1894.013->1898.453)! Learning rate decreased to 0.00050.
2024-12-02-06:43:06-root-INFO: grad norm: 231.860 229.148 35.364
2024-12-02-06:43:07-root-INFO: grad norm: 194.971 191.237 37.972
2024-12-02-06:43:07-root-INFO: grad norm: 181.410 178.796 30.685
2024-12-02-06:43:07-root-INFO: grad norm: 168.946 165.399 34.438
2024-12-02-06:43:08-root-INFO: Loss Change: 1912.317 -> 1874.864
2024-12-02-06:43:08-root-INFO: Regularization Change: 0.000 -> 0.250
2024-12-02-06:43:08-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-02-06:43:08-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:43:08-root-INFO: step: 186 lr_xt 0.00251089
2024-12-02-06:43:08-root-INFO: grad norm: 123.766 118.409 36.021
2024-12-02-06:43:08-root-INFO: Loss too large (1854.204->1894.070)! Learning rate decreased to 0.00201.
2024-12-02-06:43:08-root-INFO: Loss too large (1854.204->1877.620)! Learning rate decreased to 0.00161.
2024-12-02-06:43:08-root-INFO: Loss too large (1854.204->1866.260)! Learning rate decreased to 0.00129.
2024-12-02-06:43:09-root-INFO: Loss too large (1854.204->1858.919)! Learning rate decreased to 0.00103.
2024-12-02-06:43:09-root-INFO: Loss too large (1854.204->1854.501)! Learning rate decreased to 0.00082.
2024-12-02-06:43:09-root-INFO: grad norm: 193.322 190.604 32.302
2024-12-02-06:43:09-root-INFO: Loss too large (1852.069->1864.043)! Learning rate decreased to 0.00066.
2024-12-02-06:43:10-root-INFO: Loss too large (1852.069->1854.400)! Learning rate decreased to 0.00053.
2024-12-02-06:43:10-root-INFO: grad norm: 209.692 206.150 38.379
2024-12-02-06:43:11-root-INFO: grad norm: 223.394 220.693 34.635
2024-12-02-06:43:11-root-INFO: grad norm: 243.286 239.730 41.440
2024-12-02-06:43:11-root-INFO: Loss too large (1845.600->1845.730)! Learning rate decreased to 0.00042.
2024-12-02-06:43:12-root-INFO: grad norm: 183.761 181.188 30.646
2024-12-02-06:43:12-root-INFO: grad norm: 137.818 134.741 28.957
2024-12-02-06:43:13-root-INFO: grad norm: 115.249 112.568 24.714
2024-12-02-06:43:13-root-INFO: Loss Change: 1854.204 -> 1831.635
2024-12-02-06:43:13-root-INFO: Regularization Change: 0.000 -> 0.088
2024-12-02-06:43:13-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-02-06:43:13-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:43:13-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-06:43:13-root-INFO: grad norm: 275.641 271.389 48.228
2024-12-02-06:43:14-root-INFO: Loss too large (1822.636->1994.627)! Learning rate decreased to 0.00209.
2024-12-02-06:43:14-root-INFO: Loss too large (1822.636->1965.712)! Learning rate decreased to 0.00168.
2024-12-02-06:43:14-root-INFO: Loss too large (1822.636->1935.967)! Learning rate decreased to 0.00134.
2024-12-02-06:43:14-root-INFO: Loss too large (1822.636->1906.432)! Learning rate decreased to 0.00107.
2024-12-02-06:43:14-root-INFO: Loss too large (1822.636->1878.846)! Learning rate decreased to 0.00086.
2024-12-02-06:43:14-root-INFO: Loss too large (1822.636->1855.444)! Learning rate decreased to 0.00069.
2024-12-02-06:43:14-root-INFO: Loss too large (1822.636->1837.743)! Learning rate decreased to 0.00055.
2024-12-02-06:43:15-root-INFO: Loss too large (1822.636->1825.815)! Learning rate decreased to 0.00044.
2024-12-02-06:43:15-root-INFO: grad norm: 230.913 228.285 34.736
2024-12-02-06:43:16-root-INFO: grad norm: 192.040 188.740 35.447
2024-12-02-06:43:16-root-INFO: grad norm: 171.507 169.063 28.850
2024-12-02-06:43:17-root-INFO: grad norm: 153.314 150.324 30.128
2024-12-02-06:43:17-root-INFO: grad norm: 141.717 139.321 25.953
2024-12-02-06:43:18-root-INFO: grad norm: 131.492 128.642 27.229
2024-12-02-06:43:18-root-INFO: grad norm: 124.474 122.085 24.272
2024-12-02-06:43:19-root-INFO: Loss Change: 1822.636 -> 1799.228
2024-12-02-06:43:19-root-INFO: Regularization Change: 0.000 -> 0.058
2024-12-02-06:43:19-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-06:43:19-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:43:19-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-06:43:19-root-INFO: grad norm: 518.964 511.631 86.935
2024-12-02-06:43:19-root-INFO: Loss too large (1821.778->2046.741)! Learning rate decreased to 0.00218.
2024-12-02-06:43:19-root-INFO: Loss too large (1821.778->2024.705)! Learning rate decreased to 0.00175.
2024-12-02-06:43:20-root-INFO: Loss too large (1821.778->1999.438)! Learning rate decreased to 0.00140.
2024-12-02-06:43:20-root-INFO: Loss too large (1821.778->1970.532)! Learning rate decreased to 0.00112.
2024-12-02-06:43:20-root-INFO: Loss too large (1821.778->1938.566)! Learning rate decreased to 0.00089.
2024-12-02-06:43:20-root-INFO: Loss too large (1821.778->1904.413)! Learning rate decreased to 0.00072.
2024-12-02-06:43:20-root-INFO: Loss too large (1821.778->1869.768)! Learning rate decreased to 0.00057.
2024-12-02-06:43:20-root-INFO: Loss too large (1821.778->1838.171)! Learning rate decreased to 0.00046.
2024-12-02-06:43:21-root-INFO: grad norm: 413.817 410.211 54.515
2024-12-02-06:43:21-root-INFO: grad norm: 324.507 319.537 56.576
2024-12-02-06:43:22-root-INFO: grad norm: 312.822 309.752 43.719
2024-12-02-06:43:22-root-INFO: grad norm: 301.121 296.721 51.290
2024-12-02-06:43:23-root-INFO: grad norm: 296.333 293.455 41.197
2024-12-02-06:43:23-root-INFO: grad norm: 291.720 287.664 48.478
2024-12-02-06:43:24-root-INFO: grad norm: 289.955 287.181 40.019
2024-12-02-06:43:24-root-INFO: Loss Change: 1821.778 -> 1776.519
2024-12-02-06:43:24-root-INFO: Regularization Change: 0.000 -> 0.085
2024-12-02-06:43:24-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-06:43:24-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:43:24-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-06:43:24-root-INFO: grad norm: 522.531 516.340 80.199
2024-12-02-06:43:25-root-INFO: Loss too large (1791.274->2019.398)! Learning rate decreased to 0.00228.
2024-12-02-06:43:25-root-INFO: Loss too large (1791.274->1998.742)! Learning rate decreased to 0.00182.
2024-12-02-06:43:25-root-INFO: Loss too large (1791.274->1975.169)! Learning rate decreased to 0.00146.
2024-12-02-06:43:25-root-INFO: Loss too large (1791.274->1947.846)! Learning rate decreased to 0.00117.
2024-12-02-06:43:25-root-INFO: Loss too large (1791.274->1917.148)! Learning rate decreased to 0.00093.
2024-12-02-06:43:25-root-INFO: Loss too large (1791.274->1883.767)! Learning rate decreased to 0.00075.
2024-12-02-06:43:25-root-INFO: Loss too large (1791.274->1848.870)! Learning rate decreased to 0.00060.
2024-12-02-06:43:26-root-INFO: Loss too large (1791.274->1815.485)! Learning rate decreased to 0.00048.
2024-12-02-06:43:26-root-INFO: grad norm: 442.249 438.530 57.233
2024-12-02-06:43:27-root-INFO: grad norm: 374.043 369.674 57.000
2024-12-02-06:43:27-root-INFO: Loss too large (1767.667->1769.430)! Learning rate decreased to 0.00038.
2024-12-02-06:43:27-root-INFO: grad norm: 270.170 267.247 39.634
2024-12-02-06:43:28-root-INFO: grad norm: 199.491 196.345 35.292
2024-12-02-06:43:28-root-INFO: grad norm: 163.937 161.340 29.063
2024-12-02-06:43:29-root-INFO: grad norm: 136.458 133.546 28.043
2024-12-02-06:43:29-root-INFO: grad norm: 118.500 115.875 24.806
2024-12-02-06:43:30-root-INFO: Loss Change: 1791.274 -> 1741.578
2024-12-02-06:43:30-root-INFO: Regularization Change: 0.000 -> 0.068
2024-12-02-06:43:30-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-06:43:30-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:43:30-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-06:43:30-root-INFO: grad norm: 390.274 385.634 60.005
2024-12-02-06:43:30-root-INFO: Loss too large (1748.799->1975.427)! Learning rate decreased to 0.00237.
2024-12-02-06:43:30-root-INFO: Loss too large (1748.799->1953.687)! Learning rate decreased to 0.00190.
2024-12-02-06:43:30-root-INFO: Loss too large (1748.799->1928.097)! Learning rate decreased to 0.00152.
2024-12-02-06:43:31-root-INFO: Loss too large (1748.799->1899.027)! Learning rate decreased to 0.00122.
2024-12-02-06:43:31-root-INFO: Loss too large (1748.799->1867.226)! Learning rate decreased to 0.00097.
2024-12-02-06:43:31-root-INFO: Loss too large (1748.799->1833.826)! Learning rate decreased to 0.00078.
2024-12-02-06:43:31-root-INFO: Loss too large (1748.799->1801.307)! Learning rate decreased to 0.00062.
2024-12-02-06:43:31-root-INFO: Loss too large (1748.799->1773.413)! Learning rate decreased to 0.00050.
2024-12-02-06:43:31-root-INFO: Loss too large (1748.799->1752.905)! Learning rate decreased to 0.00040.
2024-12-02-06:43:32-root-INFO: grad norm: 306.057 303.060 42.721
2024-12-02-06:43:32-root-INFO: grad norm: 247.740 244.415 40.457
2024-12-02-06:43:33-root-INFO: grad norm: 214.946 212.388 33.063
2024-12-02-06:43:33-root-INFO: grad norm: 188.252 185.393 32.686
2024-12-02-06:43:34-root-INFO: grad norm: 169.356 166.990 28.213
2024-12-02-06:43:34-root-INFO: grad norm: 153.355 150.731 28.249
2024-12-02-06:43:35-root-INFO: grad norm: 141.113 138.841 25.218
2024-12-02-06:43:35-root-INFO: Loss Change: 1748.799 -> 1716.509
2024-12-02-06:43:35-root-INFO: Regularization Change: 0.000 -> 0.057
2024-12-02-06:43:35-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-06:43:35-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-06:43:35-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-06:43:36-root-INFO: grad norm: 440.239 435.204 66.395
2024-12-02-06:43:36-root-INFO: Loss too large (1729.974->1968.263)! Learning rate decreased to 0.00247.
2024-12-02-06:43:36-root-INFO: Loss too large (1729.974->1950.552)! Learning rate decreased to 0.00198.
2024-12-02-06:43:36-root-INFO: Loss too large (1729.974->1928.463)! Learning rate decreased to 0.00158.
2024-12-02-06:43:36-root-INFO: Loss too large (1729.974->1901.644)! Learning rate decreased to 0.00127.
2024-12-02-06:43:36-root-INFO: Loss too large (1729.974->1870.599)! Learning rate decreased to 0.00101.
2024-12-02-06:43:36-root-INFO: Loss too large (1729.974->1836.107)! Learning rate decreased to 0.00081.
2024-12-02-06:43:37-root-INFO: Loss too large (1729.974->1799.859)! Learning rate decreased to 0.00065.
2024-12-02-06:43:37-root-INFO: Loss too large (1729.974->1765.742)! Learning rate decreased to 0.00052.
2024-12-02-06:43:37-root-INFO: Loss too large (1729.974->1738.457)! Learning rate decreased to 0.00042.
2024-12-02-06:43:37-root-INFO: grad norm: 365.364 362.238 47.691
2024-12-02-06:43:38-root-INFO: grad norm: 318.920 315.113 49.133
2024-12-02-06:43:38-root-INFO: grad norm: 293.462 290.721 40.021
2024-12-02-06:43:39-root-INFO: grad norm: 273.484 270.164 42.486
2024-12-02-06:43:39-root-INFO: grad norm: 259.566 257.026 36.224
2024-12-02-06:43:40-root-INFO: grad norm: 247.999 244.969 38.647
2024-12-02-06:43:40-root-INFO: grad norm: 239.291 236.881 33.871
2024-12-02-06:43:41-root-INFO: Loss Change: 1729.974 -> 1695.075
2024-12-02-06:43:41-root-INFO: Regularization Change: 0.000 -> 0.061
2024-12-02-06:43:41-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-06:43:41-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:43:41-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-06:43:41-root-INFO: grad norm: 509.989 504.017 77.814
2024-12-02-06:43:41-root-INFO: Loss too large (1720.079->1957.897)! Learning rate decreased to 0.00258.
2024-12-02-06:43:41-root-INFO: Loss too large (1720.079->1943.828)! Learning rate decreased to 0.00206.
2024-12-02-06:43:42-root-INFO: Loss too large (1720.079->1926.200)! Learning rate decreased to 0.00165.
2024-12-02-06:43:42-root-INFO: Loss too large (1720.079->1903.387)! Learning rate decreased to 0.00132.
2024-12-02-06:43:42-root-INFO: Loss too large (1720.079->1875.138)! Learning rate decreased to 0.00106.
2024-12-02-06:43:42-root-INFO: Loss too large (1720.079->1841.898)! Learning rate decreased to 0.00084.
2024-12-02-06:43:42-root-INFO: Loss too large (1720.079->1804.493)! Learning rate decreased to 0.00068.
2024-12-02-06:43:42-root-INFO: Loss too large (1720.079->1765.620)! Learning rate decreased to 0.00054.
2024-12-02-06:43:42-root-INFO: Loss too large (1720.079->1731.010)! Learning rate decreased to 0.00043.
2024-12-02-06:43:43-root-INFO: grad norm: 428.074 424.443 55.639
2024-12-02-06:43:43-root-INFO: grad norm: 386.927 382.283 59.773
2024-12-02-06:43:44-root-INFO: grad norm: 367.177 363.938 48.664
2024-12-02-06:43:44-root-INFO: grad norm: 352.173 348.031 53.856
2024-12-02-06:43:45-root-INFO: grad norm: 341.975 338.947 45.405
2024-12-02-06:43:45-root-INFO: grad norm: 333.394 329.570 50.351
2024-12-02-06:43:46-root-INFO: grad norm: 326.821 323.938 43.321
2024-12-02-06:43:46-root-INFO: Loss Change: 1720.079 -> 1676.284
2024-12-02-06:43:46-root-INFO: Regularization Change: 0.000 -> 0.080
2024-12-02-06:43:46-root-INFO: Undo step: 180
2024-12-02-06:43:46-root-INFO: Undo step: 181
2024-12-02-06:43:46-root-INFO: Undo step: 182
2024-12-02-06:43:46-root-INFO: Undo step: 183
2024-12-02-06:43:46-root-INFO: Undo step: 184
2024-12-02-06:43:46-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-06:43:47-root-INFO: grad norm: 768.616 742.888 197.201
2024-12-02-06:43:47-root-INFO: grad norm: 588.996 565.596 164.371
2024-12-02-06:43:47-root-INFO: grad norm: 435.777 425.273 95.103
2024-12-02-06:43:48-root-INFO: grad norm: 344.067 332.160 89.730
2024-12-02-06:43:49-root-INFO: grad norm: 311.908 303.760 70.824
2024-12-02-06:43:49-root-INFO: grad norm: 323.958 314.759 76.654
2024-12-02-06:43:49-root-INFO: Loss too large (2022.365->2113.185)! Learning rate decreased to 0.00209.
2024-12-02-06:43:49-root-INFO: Loss too large (2022.365->2029.924)! Learning rate decreased to 0.00168.
2024-12-02-06:43:50-root-INFO: grad norm: 461.199 453.393 84.494
2024-12-02-06:43:50-root-INFO: Loss too large (1986.402->2052.702)! Learning rate decreased to 0.00134.
2024-12-02-06:43:50-root-INFO: Loss too large (1986.402->2021.414)! Learning rate decreased to 0.00107.
2024-12-02-06:43:50-root-INFO: Loss too large (1986.402->1994.191)! Learning rate decreased to 0.00086.
2024-12-02-06:43:51-root-INFO: grad norm: 285.278 280.454 52.240
2024-12-02-06:43:51-root-INFO: Loss Change: 3321.867 -> 1925.234
2024-12-02-06:43:51-root-INFO: Regularization Change: 0.000 -> 12.456
2024-12-02-06:43:51-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-06:43:51-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:43:51-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-06:43:52-root-INFO: grad norm: 323.154 315.155 71.453
2024-12-02-06:43:52-root-INFO: Loss too large (1918.388->2037.515)! Learning rate decreased to 0.00218.
2024-12-02-06:43:52-root-INFO: Loss too large (1918.388->2005.228)! Learning rate decreased to 0.00175.
2024-12-02-06:43:52-root-INFO: Loss too large (1918.388->1975.297)! Learning rate decreased to 0.00140.
2024-12-02-06:43:52-root-INFO: Loss too large (1918.388->1949.189)! Learning rate decreased to 0.00112.
2024-12-02-06:43:52-root-INFO: Loss too large (1918.388->1928.194)! Learning rate decreased to 0.00089.
2024-12-02-06:43:53-root-INFO: grad norm: 283.140 278.919 48.710
2024-12-02-06:43:53-root-INFO: grad norm: 245.174 238.567 56.534
2024-12-02-06:43:54-root-INFO: grad norm: 264.213 260.111 46.380
2024-12-02-06:43:54-root-INFO: grad norm: 324.946 319.024 61.757
2024-12-02-06:43:55-root-INFO: Loss too large (1870.592->1881.423)! Learning rate decreased to 0.00072.
2024-12-02-06:43:55-root-INFO: grad norm: 276.809 272.798 46.952
2024-12-02-06:43:56-root-INFO: grad norm: 220.361 214.833 49.049
2024-12-02-06:43:56-root-INFO: grad norm: 223.565 219.563 42.116
2024-12-02-06:43:56-root-INFO: Loss Change: 1918.388 -> 1835.276
2024-12-02-06:43:56-root-INFO: Regularization Change: 0.000 -> 0.463
2024-12-02-06:43:56-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-06:43:56-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:43:57-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-06:43:57-root-INFO: grad norm: 438.462 432.114 74.336
2024-12-02-06:43:57-root-INFO: Loss too large (1838.521->2017.298)! Learning rate decreased to 0.00228.
2024-12-02-06:43:57-root-INFO: Loss too large (1838.521->1991.650)! Learning rate decreased to 0.00182.
2024-12-02-06:43:57-root-INFO: Loss too large (1838.521->1962.910)! Learning rate decreased to 0.00146.
2024-12-02-06:43:57-root-INFO: Loss too large (1838.521->1932.445)! Learning rate decreased to 0.00117.
2024-12-02-06:43:58-root-INFO: Loss too large (1838.521->1901.817)! Learning rate decreased to 0.00093.
2024-12-02-06:43:58-root-INFO: Loss too large (1838.521->1872.838)! Learning rate decreased to 0.00075.
2024-12-02-06:43:58-root-INFO: Loss too large (1838.521->1847.935)! Learning rate decreased to 0.00060.
2024-12-02-06:43:58-root-INFO: grad norm: 306.124 302.171 49.035
2024-12-02-06:43:59-root-INFO: grad norm: 170.608 164.924 43.672
2024-12-02-06:43:59-root-INFO: grad norm: 160.810 156.310 37.773
2024-12-02-06:44:00-root-INFO: grad norm: 154.291 148.846 40.624
2024-12-02-06:44:00-root-INFO: grad norm: 153.551 149.111 36.659
2024-12-02-06:44:01-root-INFO: grad norm: 156.556 151.465 39.598
2024-12-02-06:44:01-root-INFO: grad norm: 162.069 157.896 36.540
2024-12-02-06:44:02-root-INFO: Loss Change: 1838.521 -> 1775.334
2024-12-02-06:44:02-root-INFO: Regularization Change: 0.000 -> 0.195
2024-12-02-06:44:02-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-06:44:02-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:44:02-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-06:44:02-root-INFO: grad norm: 418.574 413.188 66.932
2024-12-02-06:44:02-root-INFO: Loss too large (1781.754->1979.109)! Learning rate decreased to 0.00237.
2024-12-02-06:44:02-root-INFO: Loss too large (1781.754->1957.128)! Learning rate decreased to 0.00190.
2024-12-02-06:44:02-root-INFO: Loss too large (1781.754->1930.884)! Learning rate decreased to 0.00152.
2024-12-02-06:44:03-root-INFO: Loss too large (1781.754->1901.443)! Learning rate decreased to 0.00122.
2024-12-02-06:44:03-root-INFO: Loss too large (1781.754->1870.167)! Learning rate decreased to 0.00097.
2024-12-02-06:44:03-root-INFO: Loss too large (1781.754->1838.823)! Learning rate decreased to 0.00078.
2024-12-02-06:44:03-root-INFO: Loss too large (1781.754->1810.222)! Learning rate decreased to 0.00062.
2024-12-02-06:44:03-root-INFO: Loss too large (1781.754->1787.340)! Learning rate decreased to 0.00050.
2024-12-02-06:44:04-root-INFO: grad norm: 300.218 296.508 47.050
2024-12-02-06:44:04-root-INFO: grad norm: 196.975 192.421 42.107
2024-12-02-06:44:05-root-INFO: grad norm: 172.157 168.383 35.851
2024-12-02-06:44:05-root-INFO: grad norm: 150.444 145.825 36.990
2024-12-02-06:44:06-root-INFO: grad norm: 138.832 134.800 33.216
2024-12-02-06:44:06-root-INFO: grad norm: 128.939 124.242 34.484
2024-12-02-06:44:07-root-INFO: grad norm: 122.982 118.793 31.825
2024-12-02-06:44:07-root-INFO: Loss Change: 1781.754 -> 1733.823
2024-12-02-06:44:07-root-INFO: Regularization Change: 0.000 -> 0.114
2024-12-02-06:44:07-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-06:44:07-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-06:44:07-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-06:44:07-root-INFO: grad norm: 400.974 395.698 64.828
2024-12-02-06:44:07-root-INFO: Loss too large (1742.256->1953.646)! Learning rate decreased to 0.00247.
2024-12-02-06:44:08-root-INFO: Loss too large (1742.256->1933.224)! Learning rate decreased to 0.00198.
2024-12-02-06:44:08-root-INFO: Loss too large (1742.256->1908.282)! Learning rate decreased to 0.00158.
2024-12-02-06:44:08-root-INFO: Loss too large (1742.256->1879.575)! Learning rate decreased to 0.00127.
2024-12-02-06:44:08-root-INFO: Loss too large (1742.256->1848.182)! Learning rate decreased to 0.00101.
2024-12-02-06:44:08-root-INFO: Loss too large (1742.256->1815.624)! Learning rate decreased to 0.00081.
2024-12-02-06:44:09-root-INFO: Loss too large (1742.256->1784.755)! Learning rate decreased to 0.00065.
2024-12-02-06:44:09-root-INFO: Loss too large (1742.256->1759.045)! Learning rate decreased to 0.00052.
2024-12-02-06:44:09-root-INFO: grad norm: 352.535 348.900 50.496
2024-12-02-06:44:10-root-INFO: grad norm: 303.092 298.597 52.003
2024-12-02-06:44:10-root-INFO: grad norm: 296.030 292.635 44.706
2024-12-02-06:44:11-root-INFO: grad norm: 289.840 285.637 49.182
2024-12-02-06:44:11-root-INFO: grad norm: 289.412 286.067 43.879
2024-12-02-06:44:12-root-INFO: grad norm: 290.492 286.456 48.255
2024-12-02-06:44:12-root-INFO: grad norm: 292.683 289.351 44.043
2024-12-02-06:44:12-root-INFO: Loss Change: 1742.256 -> 1708.588
2024-12-02-06:44:12-root-INFO: Regularization Change: 0.000 -> 0.101
2024-12-02-06:44:12-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-06:44:12-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:44:13-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-06:44:13-root-INFO: grad norm: 568.155 561.393 87.400
2024-12-02-06:44:13-root-INFO: Loss too large (1737.030->1955.251)! Learning rate decreased to 0.00258.
2024-12-02-06:44:13-root-INFO: Loss too large (1737.030->1940.141)! Learning rate decreased to 0.00206.
2024-12-02-06:44:13-root-INFO: Loss too large (1737.030->1921.976)! Learning rate decreased to 0.00165.
2024-12-02-06:44:13-root-INFO: Loss too large (1737.030->1899.048)! Learning rate decreased to 0.00132.
2024-12-02-06:44:14-root-INFO: Loss too large (1737.030->1871.108)! Learning rate decreased to 0.00106.
2024-12-02-06:44:14-root-INFO: Loss too large (1737.030->1838.766)! Learning rate decreased to 0.00084.
2024-12-02-06:44:14-root-INFO: Loss too large (1737.030->1802.973)! Learning rate decreased to 0.00068.
2024-12-02-06:44:14-root-INFO: Loss too large (1737.030->1766.396)! Learning rate decreased to 0.00054.
2024-12-02-06:44:15-root-INFO: grad norm: 442.709 438.542 60.599
2024-12-02-06:44:15-root-INFO: grad norm: 321.289 316.452 55.535
2024-12-02-06:44:15-root-INFO: Loss too large (1697.873->1700.318)! Learning rate decreased to 0.00043.
2024-12-02-06:44:16-root-INFO: grad norm: 252.110 248.769 40.905
2024-12-02-06:44:16-root-INFO: grad norm: 196.927 192.617 40.974
2024-12-02-06:44:17-root-INFO: grad norm: 169.130 165.745 33.668
2024-12-02-06:44:17-root-INFO: grad norm: 145.809 141.553 34.971
2024-12-02-06:44:18-root-INFO: grad norm: 130.547 126.942 30.465
2024-12-02-06:44:18-root-INFO: Loss Change: 1737.030 -> 1672.172
2024-12-02-06:44:18-root-INFO: Regularization Change: 0.000 -> 0.102
2024-12-02-06:44:18-root-INFO: Undo step: 180
2024-12-02-06:44:18-root-INFO: Undo step: 181
2024-12-02-06:44:18-root-INFO: Undo step: 182
2024-12-02-06:44:18-root-INFO: Undo step: 183
2024-12-02-06:44:18-root-INFO: Undo step: 184
2024-12-02-06:44:18-root-INFO: step: 185 lr_xt 0.00261863
2024-12-02-06:44:18-root-INFO: grad norm: 908.266 860.857 289.608
2024-12-02-06:44:19-root-INFO: grad norm: 913.996 888.779 213.215
2024-12-02-06:44:19-root-INFO: grad norm: 623.783 608.372 137.800
2024-12-02-06:44:20-root-INFO: grad norm: 216.069 204.577 69.527
2024-12-02-06:44:20-root-INFO: grad norm: 227.890 221.759 52.505
2024-12-02-06:44:21-root-INFO: Loss too large (2015.111->2054.916)! Learning rate decreased to 0.00209.
2024-12-02-06:44:21-root-INFO: Loss too large (2015.111->2036.345)! Learning rate decreased to 0.00168.
2024-12-02-06:44:21-root-INFO: Loss too large (2015.111->2021.553)! Learning rate decreased to 0.00134.
2024-12-02-06:44:21-root-INFO: grad norm: 360.822 355.736 60.368
2024-12-02-06:44:22-root-INFO: Loss too large (2010.836->2089.515)! Learning rate decreased to 0.00107.
2024-12-02-06:44:22-root-INFO: Loss too large (2010.836->2029.212)! Learning rate decreased to 0.00086.
2024-12-02-06:44:23-root-INFO: grad norm: 478.913 473.720 70.333
2024-12-02-06:44:23-root-INFO: Loss too large (1992.708->2017.921)! Learning rate decreased to 0.00069.
2024-12-02-06:44:23-root-INFO: Loss too large (1992.708->1993.472)! Learning rate decreased to 0.00055.
2024-12-02-06:44:23-root-INFO: grad norm: 324.907 320.102 55.669
2024-12-02-06:44:24-root-INFO: Loss Change: 3158.049 -> 1947.224
2024-12-02-06:44:24-root-INFO: Regularization Change: 0.000 -> 9.809
2024-12-02-06:44:24-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-02-06:44:24-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:44:24-root-INFO: step: 184 lr_xt 0.00273055
2024-12-02-06:44:24-root-INFO: grad norm: 540.833 534.475 82.682
2024-12-02-06:44:24-root-INFO: Loss too large (1957.327->2116.968)! Learning rate decreased to 0.00218.
2024-12-02-06:44:24-root-INFO: Loss too large (1957.327->2099.361)! Learning rate decreased to 0.00175.
2024-12-02-06:44:25-root-INFO: Loss too large (1957.327->2077.640)! Learning rate decreased to 0.00140.
2024-12-02-06:44:25-root-INFO: Loss too large (1957.327->2052.197)! Learning rate decreased to 0.00112.
2024-12-02-06:44:25-root-INFO: Loss too large (1957.327->2023.912)! Learning rate decreased to 0.00089.
2024-12-02-06:44:25-root-INFO: Loss too large (1957.327->1994.138)! Learning rate decreased to 0.00072.
2024-12-02-06:44:25-root-INFO: Loss too large (1957.327->1965.556)! Learning rate decreased to 0.00057.
2024-12-02-06:44:26-root-INFO: grad norm: 364.299 360.327 53.655
2024-12-02-06:44:26-root-INFO: grad norm: 186.772 181.101 45.675
2024-12-02-06:44:27-root-INFO: grad norm: 183.153 178.705 40.121
2024-12-02-06:44:27-root-INFO: grad norm: 182.357 177.187 43.116
2024-12-02-06:44:28-root-INFO: grad norm: 183.558 179.411 38.798
2024-12-02-06:44:29-root-INFO: grad norm: 188.089 183.357 41.926
2024-12-02-06:44:29-root-INFO: grad norm: 192.990 189.157 38.273
2024-12-02-06:44:30-root-INFO: Loss Change: 1957.327 -> 1855.523
2024-12-02-06:44:30-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-02-06:44:30-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-02-06:44:30-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:44:30-root-INFO: step: 183 lr_xt 0.00284680
2024-12-02-06:44:30-root-INFO: grad norm: 417.434 412.669 62.893
2024-12-02-06:44:30-root-INFO: Loss too large (1854.825->2027.401)! Learning rate decreased to 0.00228.
2024-12-02-06:44:30-root-INFO: Loss too large (1854.825->2007.561)! Learning rate decreased to 0.00182.
2024-12-02-06:44:30-root-INFO: Loss too large (1854.825->1984.282)! Learning rate decreased to 0.00146.
2024-12-02-06:44:30-root-INFO: Loss too large (1854.825->1958.261)! Learning rate decreased to 0.00117.
2024-12-02-06:44:31-root-INFO: Loss too large (1854.825->1930.391)! Learning rate decreased to 0.00093.
2024-12-02-06:44:31-root-INFO: Loss too large (1854.825->1902.326)! Learning rate decreased to 0.00075.
2024-12-02-06:44:31-root-INFO: Loss too large (1854.825->1876.838)! Learning rate decreased to 0.00060.
2024-12-02-06:44:31-root-INFO: Loss too large (1854.825->1856.631)! Learning rate decreased to 0.00048.
2024-12-02-06:44:32-root-INFO: grad norm: 296.639 293.383 43.825
2024-12-02-06:44:32-root-INFO: grad norm: 192.241 187.910 40.580
2024-12-02-06:44:32-root-INFO: grad norm: 167.817 164.150 34.888
2024-12-02-06:44:33-root-INFO: grad norm: 146.752 142.262 36.020
2024-12-02-06:44:33-root-INFO: grad norm: 135.014 130.971 32.795
2024-12-02-06:44:34-root-INFO: grad norm: 125.037 120.414 33.685
2024-12-02-06:44:34-root-INFO: grad norm: 118.497 114.214 31.570
2024-12-02-06:44:34-root-INFO: Loss Change: 1854.825 -> 1795.975
2024-12-02-06:44:34-root-INFO: Regularization Change: 0.000 -> 0.156
2024-12-02-06:44:34-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-02-06:44:34-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00019.
2024-12-02-06:44:35-root-INFO: step: 182 lr_xt 0.00296752
2024-12-02-06:44:35-root-INFO: grad norm: 357.604 353.414 54.586
2024-12-02-06:44:35-root-INFO: Loss too large (1796.603->1980.166)! Learning rate decreased to 0.00237.
2024-12-02-06:44:35-root-INFO: Loss too large (1796.603->1959.392)! Learning rate decreased to 0.00190.
2024-12-02-06:44:35-root-INFO: Loss too large (1796.603->1935.130)! Learning rate decreased to 0.00152.
2024-12-02-06:44:35-root-INFO: Loss too large (1796.603->1908.090)! Learning rate decreased to 0.00122.
2024-12-02-06:44:36-root-INFO: Loss too large (1796.603->1879.336)! Learning rate decreased to 0.00097.
2024-12-02-06:44:36-root-INFO: Loss too large (1796.603->1850.941)! Learning rate decreased to 0.00078.
2024-12-02-06:44:36-root-INFO: Loss too large (1796.603->1825.814)! Learning rate decreased to 0.00062.
2024-12-02-06:44:36-root-INFO: Loss too large (1796.603->1806.252)! Learning rate decreased to 0.00050.
2024-12-02-06:44:37-root-INFO: grad norm: 313.581 310.566 43.375
2024-12-02-06:44:37-root-INFO: grad norm: 266.509 262.791 44.361
2024-12-02-06:44:38-root-INFO: grad norm: 254.843 251.950 38.293
2024-12-02-06:44:38-root-INFO: grad norm: 242.983 239.479 41.118
2024-12-02-06:44:39-root-INFO: grad norm: 238.662 235.806 36.815
2024-12-02-06:44:39-root-INFO: grad norm: 235.068 231.708 39.601
2024-12-02-06:44:40-root-INFO: grad norm: 234.382 231.556 36.288
2024-12-02-06:44:40-root-INFO: Loss Change: 1796.603 -> 1756.583
2024-12-02-06:44:40-root-INFO: Regularization Change: 0.000 -> 0.126
2024-12-02-06:44:40-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-02-06:44:40-root-INFO: Coefficient of regularization decay: 0.00019 -> 0.00020.
2024-12-02-06:44:40-root-INFO: step: 181 lr_xt 0.00309285
2024-12-02-06:44:40-root-INFO: grad norm: 540.209 535.287 72.762
2024-12-02-06:44:40-root-INFO: Loss too large (1776.854->1987.888)! Learning rate decreased to 0.00247.
2024-12-02-06:44:41-root-INFO: Loss too large (1776.854->1971.894)! Learning rate decreased to 0.00198.
2024-12-02-06:44:41-root-INFO: Loss too large (1776.854->1952.523)! Learning rate decreased to 0.00158.
2024-12-02-06:44:41-root-INFO: Loss too large (1776.854->1928.871)! Learning rate decreased to 0.00127.
2024-12-02-06:44:41-root-INFO: Loss too large (1776.854->1901.021)! Learning rate decreased to 0.00101.
2024-12-02-06:44:41-root-INFO: Loss too large (1776.854->1869.397)! Learning rate decreased to 0.00081.
2024-12-02-06:44:41-root-INFO: Loss too large (1776.854->1835.192)! Learning rate decreased to 0.00065.
2024-12-02-06:44:41-root-INFO: Loss too large (1776.854->1801.684)! Learning rate decreased to 0.00052.
2024-12-02-06:44:42-root-INFO: grad norm: 417.062 413.961 50.766
2024-12-02-06:44:42-root-INFO: grad norm: 294.423 290.834 45.829
2024-12-02-06:44:43-root-INFO: Loss too large (1742.130->1742.613)! Learning rate decreased to 0.00042.
2024-12-02-06:44:43-root-INFO: grad norm: 223.388 220.772 34.090
2024-12-02-06:44:44-root-INFO: grad norm: 167.879 164.407 33.963
2024-12-02-06:44:44-root-INFO: grad norm: 141.207 138.199 28.992
2024-12-02-06:44:45-root-INFO: grad norm: 120.201 116.436 29.849
2024-12-02-06:44:45-root-INFO: grad norm: 107.295 103.778 27.244
2024-12-02-06:44:45-root-INFO: Loss Change: 1776.854 -> 1717.487
2024-12-02-06:44:45-root-INFO: Regularization Change: 0.000 -> 0.097
2024-12-02-06:44:45-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-02-06:44:45-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:44:46-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-06:44:46-root-INFO: grad norm: 411.888 407.130 62.429
2024-12-02-06:44:46-root-INFO: Loss too large (1733.721->1949.228)! Learning rate decreased to 0.00258.
2024-12-02-06:44:46-root-INFO: Loss too large (1733.721->1929.962)! Learning rate decreased to 0.00206.
2024-12-02-06:44:46-root-INFO: Loss too large (1733.721->1907.382)! Learning rate decreased to 0.00165.
2024-12-02-06:44:46-root-INFO: Loss too large (1733.721->1881.378)! Learning rate decreased to 0.00132.
2024-12-02-06:44:47-root-INFO: Loss too large (1733.721->1852.226)! Learning rate decreased to 0.00106.
2024-12-02-06:44:47-root-INFO: Loss too large (1733.721->1820.708)! Learning rate decreased to 0.00084.
2024-12-02-06:44:47-root-INFO: Loss too large (1733.721->1789.100)! Learning rate decreased to 0.00068.
2024-12-02-06:44:47-root-INFO: Loss too large (1733.721->1760.982)! Learning rate decreased to 0.00054.
2024-12-02-06:44:47-root-INFO: Loss too large (1733.721->1739.279)! Learning rate decreased to 0.00043.
2024-12-02-06:44:48-root-INFO: grad norm: 327.790 324.964 42.951
2024-12-02-06:44:48-root-INFO: grad norm: 257.287 253.384 44.643
2024-12-02-06:44:49-root-INFO: grad norm: 231.616 228.953 35.025
2024-12-02-06:44:49-root-INFO: grad norm: 207.737 204.175 38.306
2024-12-02-06:44:50-root-INFO: grad norm: 193.364 190.735 31.773
2024-12-02-06:44:50-root-INFO: grad norm: 179.720 176.352 34.627
2024-12-02-06:44:51-root-INFO: grad norm: 170.335 167.705 29.816
2024-12-02-06:44:51-root-INFO: Loss Change: 1733.721 -> 1691.431
2024-12-02-06:44:51-root-INFO: Regularization Change: 0.000 -> 0.093
2024-12-02-06:44:51-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-06:44:51-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:44:51-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-06:44:52-root-INFO: grad norm: 475.980 471.675 63.875
2024-12-02-06:44:52-root-INFO: Loss too large (1711.172->1938.432)! Learning rate decreased to 0.00269.
2024-12-02-06:44:52-root-INFO: Loss too large (1711.172->1923.441)! Learning rate decreased to 0.00215.
2024-12-02-06:44:52-root-INFO: Loss too large (1711.172->1905.075)! Learning rate decreased to 0.00172.
2024-12-02-06:44:52-root-INFO: Loss too large (1711.172->1882.334)! Learning rate decreased to 0.00138.
2024-12-02-06:44:52-root-INFO: Loss too large (1711.172->1855.104)! Learning rate decreased to 0.00110.
2024-12-02-06:44:52-root-INFO: Loss too large (1711.172->1823.595)! Learning rate decreased to 0.00088.
2024-12-02-06:44:53-root-INFO: Loss too large (1711.172->1788.876)! Learning rate decreased to 0.00070.
2024-12-02-06:44:53-root-INFO: Loss too large (1711.172->1754.259)! Learning rate decreased to 0.00056.
2024-12-02-06:44:53-root-INFO: Loss too large (1711.172->1724.496)! Learning rate decreased to 0.00045.
2024-12-02-06:44:53-root-INFO: grad norm: 390.668 387.834 46.966
2024-12-02-06:44:54-root-INFO: grad norm: 324.848 321.448 46.877
2024-12-02-06:44:54-root-INFO: grad norm: 307.432 304.898 39.394
2024-12-02-06:44:55-root-INFO: grad norm: 290.605 287.494 42.409
2024-12-02-06:44:55-root-INFO: grad norm: 281.651 279.199 37.088
2024-12-02-06:44:56-root-INFO: grad norm: 272.526 269.587 39.912
2024-12-02-06:44:56-root-INFO: grad norm: 266.863 264.457 35.759
2024-12-02-06:44:57-root-INFO: Loss Change: 1711.172 -> 1669.546
2024-12-02-06:44:57-root-INFO: Regularization Change: 0.000 -> 0.083
2024-12-02-06:44:57-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-06:44:57-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:44:57-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-06:44:57-root-INFO: grad norm: 532.853 528.240 69.959
2024-12-02-06:44:57-root-INFO: Loss too large (1697.125->1922.403)! Learning rate decreased to 0.00280.
2024-12-02-06:44:57-root-INFO: Loss too large (1697.125->1908.457)! Learning rate decreased to 0.00224.
2024-12-02-06:44:57-root-INFO: Loss too large (1697.125->1892.416)! Learning rate decreased to 0.00179.
2024-12-02-06:44:58-root-INFO: Loss too large (1697.125->1872.464)! Learning rate decreased to 0.00143.
2024-12-02-06:44:58-root-INFO: Loss too large (1697.125->1847.779)! Learning rate decreased to 0.00115.
2024-12-02-06:44:58-root-INFO: Loss too large (1697.125->1818.183)! Learning rate decreased to 0.00092.
2024-12-02-06:44:58-root-INFO: Loss too large (1697.125->1783.857)! Learning rate decreased to 0.00073.
2024-12-02-06:44:58-root-INFO: Loss too large (1697.125->1746.820)! Learning rate decreased to 0.00059.
2024-12-02-06:44:58-root-INFO: Loss too large (1697.125->1712.099)! Learning rate decreased to 0.00047.
2024-12-02-06:44:59-root-INFO: grad norm: 432.422 429.367 51.313
2024-12-02-06:44:59-root-INFO: grad norm: 364.742 361.240 50.423
2024-12-02-06:45:00-root-INFO: grad norm: 353.900 351.215 43.516
2024-12-02-06:45:01-root-INFO: grad norm: 342.128 338.940 46.596
2024-12-02-06:45:01-root-INFO: grad norm: 335.608 333.023 41.579
2024-12-02-06:45:01-root-INFO: grad norm: 327.683 324.696 44.145
2024-12-02-06:45:02-root-INFO: grad norm: 322.352 319.837 40.189
2024-12-02-06:45:02-root-INFO: Loss Change: 1697.125 -> 1646.470
2024-12-02-06:45:02-root-INFO: Regularization Change: 0.000 -> 0.101
2024-12-02-06:45:02-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-06:45:02-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:45:02-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-06:45:03-root-INFO: grad norm: 539.574 535.437 66.689
2024-12-02-06:45:03-root-INFO: Loss too large (1673.436->1898.700)! Learning rate decreased to 0.00291.
2024-12-02-06:45:03-root-INFO: Loss too large (1673.436->1886.383)! Learning rate decreased to 0.00233.
2024-12-02-06:45:03-root-INFO: Loss too large (1673.436->1871.949)! Learning rate decreased to 0.00187.
2024-12-02-06:45:03-root-INFO: Loss too large (1673.436->1853.485)! Learning rate decreased to 0.00149.
2024-12-02-06:45:03-root-INFO: Loss too large (1673.436->1829.939)! Learning rate decreased to 0.00119.
2024-12-02-06:45:04-root-INFO: Loss too large (1673.436->1801.041)! Learning rate decreased to 0.00096.
2024-12-02-06:45:04-root-INFO: Loss too large (1673.436->1766.778)! Learning rate decreased to 0.00076.
2024-12-02-06:45:04-root-INFO: Loss too large (1673.436->1728.614)! Learning rate decreased to 0.00061.
2024-12-02-06:45:04-root-INFO: Loss too large (1673.436->1691.483)! Learning rate decreased to 0.00049.
2024-12-02-06:45:05-root-INFO: grad norm: 447.438 444.259 53.244
2024-12-02-06:45:05-root-INFO: grad norm: 392.918 389.700 50.183
2024-12-02-06:45:06-root-INFO: grad norm: 385.947 383.104 46.763
2024-12-02-06:45:06-root-INFO: grad norm: 376.987 373.978 47.536
2024-12-02-06:45:07-root-INFO: grad norm: 371.467 368.712 45.156
2024-12-02-06:45:07-root-INFO: grad norm: 364.031 361.165 45.587
2024-12-02-06:45:08-root-INFO: grad norm: 358.820 356.137 43.799
2024-12-02-06:45:08-root-INFO: Loss Change: 1673.436 -> 1625.627
2024-12-02-06:45:08-root-INFO: Regularization Change: 0.000 -> 0.096
2024-12-02-06:45:08-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-06:45:08-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-06:45:08-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-06:45:09-root-INFO: grad norm: 574.132 569.847 70.012
2024-12-02-06:45:09-root-INFO: Loss too large (1662.562->1887.282)! Learning rate decreased to 0.00304.
2024-12-02-06:45:09-root-INFO: Loss too large (1662.562->1873.547)! Learning rate decreased to 0.00243.
2024-12-02-06:45:09-root-INFO: Loss too large (1662.562->1858.730)! Learning rate decreased to 0.00194.
2024-12-02-06:45:09-root-INFO: Loss too large (1662.562->1840.561)! Learning rate decreased to 0.00155.
2024-12-02-06:45:09-root-INFO: Loss too large (1662.562->1817.297)! Learning rate decreased to 0.00124.
2024-12-02-06:45:10-root-INFO: Loss too large (1662.562->1788.219)! Learning rate decreased to 0.00099.
2024-12-02-06:45:10-root-INFO: Loss too large (1662.562->1752.999)! Learning rate decreased to 0.00080.
2024-12-02-06:45:10-root-INFO: Loss too large (1662.562->1712.542)! Learning rate decreased to 0.00064.
2024-12-02-06:45:10-root-INFO: Loss too large (1662.562->1671.818)! Learning rate decreased to 0.00051.
2024-12-02-06:45:11-root-INFO: grad norm: 449.257 446.321 51.282
2024-12-02-06:45:11-root-INFO: grad norm: 398.843 395.664 50.252
2024-12-02-06:45:11-root-INFO: grad norm: 390.889 388.233 45.494
2024-12-02-06:45:12-root-INFO: grad norm: 381.536 378.587 47.351
2024-12-02-06:45:12-root-INFO: grad norm: 375.465 372.879 43.989
2024-12-02-06:45:13-root-INFO: grad norm: 367.711 364.909 45.308
2024-12-02-06:45:13-root-INFO: grad norm: 361.768 359.244 42.659
2024-12-02-06:45:14-root-INFO: Loss Change: 1662.562 -> 1602.388
2024-12-02-06:45:14-root-INFO: Regularization Change: 0.000 -> 0.109
2024-12-02-06:45:14-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-06:45:14-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:45:14-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-06:45:14-root-INFO: grad norm: 544.683 540.934 63.794
2024-12-02-06:45:14-root-INFO: Loss too large (1636.327->1858.533)! Learning rate decreased to 0.00316.
2024-12-02-06:45:14-root-INFO: Loss too large (1636.327->1846.272)! Learning rate decreased to 0.00253.
2024-12-02-06:45:14-root-INFO: Loss too large (1636.327->1832.244)! Learning rate decreased to 0.00202.
2024-12-02-06:45:15-root-INFO: Loss too large (1636.327->1814.205)! Learning rate decreased to 0.00162.
2024-12-02-06:45:15-root-INFO: Loss too large (1636.327->1790.496)! Learning rate decreased to 0.00129.
2024-12-02-06:45:15-root-INFO: Loss too large (1636.327->1760.426)! Learning rate decreased to 0.00104.
2024-12-02-06:45:15-root-INFO: Loss too large (1636.327->1723.641)! Learning rate decreased to 0.00083.
2024-12-02-06:45:15-root-INFO: Loss too large (1636.327->1681.447)! Learning rate decreased to 0.00066.
2024-12-02-06:45:15-root-INFO: Loss too large (1636.327->1639.995)! Learning rate decreased to 0.00053.
2024-12-02-06:45:16-root-INFO: grad norm: 424.920 422.162 48.332
2024-12-02-06:45:16-root-INFO: grad norm: 391.058 388.188 47.290
2024-12-02-06:45:17-root-INFO: grad norm: 377.668 375.127 43.731
2024-12-02-06:45:17-root-INFO: grad norm: 366.213 363.552 44.066
2024-12-02-06:45:18-root-INFO: grad norm: 357.704 355.253 41.801
2024-12-02-06:45:18-root-INFO: grad norm: 349.310 346.781 41.961
2024-12-02-06:45:19-root-INFO: grad norm: 341.989 339.609 40.283
2024-12-02-06:45:19-root-INFO: Loss Change: 1636.327 -> 1576.427
2024-12-02-06:45:19-root-INFO: Regularization Change: 0.000 -> 0.111
2024-12-02-06:45:19-root-INFO: Undo step: 175
2024-12-02-06:45:19-root-INFO: Undo step: 176
2024-12-02-06:45:19-root-INFO: Undo step: 177
2024-12-02-06:45:19-root-INFO: Undo step: 178
2024-12-02-06:45:19-root-INFO: Undo step: 179
2024-12-02-06:45:19-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-06:45:19-root-INFO: grad norm: 826.651 755.259 336.059
2024-12-02-06:45:20-root-INFO: grad norm: 673.689 654.675 158.929
2024-12-02-06:45:21-root-INFO: grad norm: 739.712 726.287 140.289
2024-12-02-06:45:21-root-INFO: Loss too large (2252.177->2841.306)! Learning rate decreased to 0.00258.
2024-12-02-06:45:21-root-INFO: Loss too large (2252.177->2518.723)! Learning rate decreased to 0.00206.
2024-12-02-06:45:21-root-INFO: Loss too large (2252.177->2300.405)! Learning rate decreased to 0.00165.
2024-12-02-06:45:22-root-INFO: grad norm: 799.596 792.168 108.735
2024-12-02-06:45:22-root-INFO: grad norm: 213.254 203.278 64.462
2024-12-02-06:45:23-root-INFO: grad norm: 225.466 219.006 53.585
2024-12-02-06:45:23-root-INFO: grad norm: 280.080 275.325 51.389
2024-12-02-06:45:24-root-INFO: grad norm: 317.932 313.166 54.844
2024-12-02-06:45:24-root-INFO: Loss too large (1787.613->1846.984)! Learning rate decreased to 0.00132.
2024-12-02-06:45:24-root-INFO: Loss too large (1787.613->1822.009)! Learning rate decreased to 0.00106.
2024-12-02-06:45:24-root-INFO: Loss too large (1787.613->1801.480)! Learning rate decreased to 0.00084.
2024-12-02-06:45:24-root-INFO: Loss Change: 3099.489 -> 1786.237
2024-12-02-06:45:24-root-INFO: Regularization Change: 0.000 -> 12.253
2024-12-02-06:45:24-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-06:45:24-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:45:25-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-06:45:25-root-INFO: grad norm: 249.998 246.366 42.461
2024-12-02-06:45:25-root-INFO: Loss too large (1759.988->2120.634)! Learning rate decreased to 0.00269.
2024-12-02-06:45:25-root-INFO: Loss too large (1759.988->2019.052)! Learning rate decreased to 0.00215.
2024-12-02-06:45:25-root-INFO: Loss too large (1759.988->1930.394)! Learning rate decreased to 0.00172.
2024-12-02-06:45:26-root-INFO: Loss too large (1759.988->1858.903)! Learning rate decreased to 0.00138.
2024-12-02-06:45:26-root-INFO: Loss too large (1759.988->1806.835)! Learning rate decreased to 0.00110.
2024-12-02-06:45:26-root-INFO: Loss too large (1759.988->1773.114)! Learning rate decreased to 0.00088.
2024-12-02-06:45:27-root-INFO: grad norm: 383.845 379.896 54.919
2024-12-02-06:45:27-root-INFO: Loss too large (1754.016->1779.676)! Learning rate decreased to 0.00070.
2024-12-02-06:45:27-root-INFO: Loss too large (1754.016->1759.712)! Learning rate decreased to 0.00056.
2024-12-02-06:45:27-root-INFO: grad norm: 287.343 283.800 44.982
2024-12-02-06:45:28-root-INFO: grad norm: 196.732 192.620 40.013
2024-12-02-06:45:28-root-INFO: grad norm: 191.567 187.760 38.001
2024-12-02-06:45:29-root-INFO: grad norm: 194.588 190.759 38.410
2024-12-02-06:45:29-root-INFO: grad norm: 203.702 200.098 38.148
2024-12-02-06:45:30-root-INFO: grad norm: 224.514 221.087 39.074
2024-12-02-06:45:30-root-INFO: Loss Change: 1759.988 -> 1704.854
2024-12-02-06:45:30-root-INFO: Regularization Change: 0.000 -> 0.236
2024-12-02-06:45:30-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-06:45:30-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:45:30-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-06:45:31-root-INFO: grad norm: 113.621 109.098 31.740
2024-12-02-06:45:31-root-INFO: grad norm: 286.367 283.842 37.941
2024-12-02-06:45:31-root-INFO: Loss too large (1666.045->1857.464)! Learning rate decreased to 0.00280.
2024-12-02-06:45:31-root-INFO: Loss too large (1666.045->1837.059)! Learning rate decreased to 0.00224.
2024-12-02-06:45:32-root-INFO: Loss too large (1666.045->1813.891)! Learning rate decreased to 0.00179.
2024-12-02-06:45:32-root-INFO: Loss too large (1666.045->1788.416)! Learning rate decreased to 0.00143.
2024-12-02-06:45:32-root-INFO: Loss too large (1666.045->1761.365)! Learning rate decreased to 0.00115.
2024-12-02-06:45:32-root-INFO: Loss too large (1666.045->1734.335)! Learning rate decreased to 0.00092.
2024-12-02-06:45:32-root-INFO: Loss too large (1666.045->1709.731)! Learning rate decreased to 0.00073.
2024-12-02-06:45:32-root-INFO: Loss too large (1666.045->1689.661)! Learning rate decreased to 0.00059.
2024-12-02-06:45:33-root-INFO: Loss too large (1666.045->1675.026)! Learning rate decreased to 0.00047.
2024-12-02-06:45:33-root-INFO: grad norm: 300.715 297.451 44.182
2024-12-02-06:45:34-root-INFO: grad norm: 318.123 315.556 40.328
2024-12-02-06:45:34-root-INFO: Loss too large (1660.880->1661.497)! Learning rate decreased to 0.00038.
2024-12-02-06:45:34-root-INFO: grad norm: 235.370 232.300 37.893
2024-12-02-06:45:35-root-INFO: grad norm: 173.004 170.317 30.370
2024-12-02-06:45:35-root-INFO: grad norm: 143.827 140.475 30.872
2024-12-02-06:45:36-root-INFO: grad norm: 121.374 118.160 27.746
2024-12-02-06:45:36-root-INFO: Loss Change: 1688.078 -> 1639.534
2024-12-02-06:45:36-root-INFO: Regularization Change: 0.000 -> 0.314
2024-12-02-06:45:36-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-06:45:36-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:45:37-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-06:45:37-root-INFO: grad norm: 230.039 227.546 33.778
2024-12-02-06:45:37-root-INFO: Loss too large (1635.714->1824.693)! Learning rate decreased to 0.00291.
2024-12-02-06:45:37-root-INFO: Loss too large (1635.714->1801.258)! Learning rate decreased to 0.00233.
2024-12-02-06:45:37-root-INFO: Loss too large (1635.714->1775.565)! Learning rate decreased to 0.00187.
2024-12-02-06:45:37-root-INFO: Loss too large (1635.714->1748.302)! Learning rate decreased to 0.00149.
2024-12-02-06:45:38-root-INFO: Loss too large (1635.714->1720.729)! Learning rate decreased to 0.00119.
2024-12-02-06:45:38-root-INFO: Loss too large (1635.714->1694.918)! Learning rate decreased to 0.00096.
2024-12-02-06:45:38-root-INFO: Loss too large (1635.714->1672.993)! Learning rate decreased to 0.00076.
2024-12-02-06:45:38-root-INFO: Loss too large (1635.714->1656.142)! Learning rate decreased to 0.00061.
2024-12-02-06:45:38-root-INFO: Loss too large (1635.714->1644.383)! Learning rate decreased to 0.00049.
2024-12-02-06:45:38-root-INFO: Loss too large (1635.714->1636.920)! Learning rate decreased to 0.00039.
2024-12-02-06:45:39-root-INFO: grad norm: 209.606 206.770 34.364
2024-12-02-06:45:40-root-INFO: grad norm: 191.639 189.161 30.722
2024-12-02-06:45:40-root-INFO: grad norm: 181.372 178.507 32.115
2024-12-02-06:45:41-root-INFO: grad norm: 172.479 169.994 29.177
2024-12-02-06:45:41-root-INFO: grad norm: 167.137 164.251 30.923
2024-12-02-06:45:42-root-INFO: grad norm: 162.790 160.311 28.300
2024-12-02-06:45:42-root-INFO: grad norm: 160.421 157.541 30.261
2024-12-02-06:45:42-root-INFO: Loss Change: 1635.714 -> 1614.008
2024-12-02-06:45:42-root-INFO: Regularization Change: 0.000 -> 0.060
2024-12-02-06:45:42-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-06:45:42-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-06:45:43-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-06:45:43-root-INFO: grad norm: 519.016 515.579 59.633
2024-12-02-06:45:43-root-INFO: Loss too large (1638.151->1884.181)! Learning rate decreased to 0.00304.
2024-12-02-06:45:43-root-INFO: Loss too large (1638.151->1867.352)! Learning rate decreased to 0.00243.
2024-12-02-06:45:43-root-INFO: Loss too large (1638.151->1849.153)! Learning rate decreased to 0.00194.
2024-12-02-06:45:43-root-INFO: Loss too large (1638.151->1828.126)! Learning rate decreased to 0.00155.
2024-12-02-06:45:44-root-INFO: Loss too large (1638.151->1803.703)! Learning rate decreased to 0.00124.
2024-12-02-06:45:44-root-INFO: Loss too large (1638.151->1775.983)! Learning rate decreased to 0.00099.
2024-12-02-06:45:44-root-INFO: Loss too large (1638.151->1745.190)! Learning rate decreased to 0.00080.
2024-12-02-06:45:44-root-INFO: Loss too large (1638.151->1712.036)! Learning rate decreased to 0.00064.
2024-12-02-06:45:44-root-INFO: Loss too large (1638.151->1678.923)! Learning rate decreased to 0.00051.
2024-12-02-06:45:44-root-INFO: Loss too large (1638.151->1649.818)! Learning rate decreased to 0.00041.
2024-12-02-06:45:45-root-INFO: grad norm: 406.098 403.167 48.704
2024-12-02-06:45:45-root-INFO: grad norm: 323.259 320.675 40.788
2024-12-02-06:45:46-root-INFO: grad norm: 311.330 308.744 40.042
2024-12-02-06:45:46-root-INFO: grad norm: 301.169 298.769 37.950
2024-12-02-06:45:47-root-INFO: grad norm: 297.987 295.441 38.866
2024-12-02-06:45:47-root-INFO: grad norm: 295.649 293.343 36.848
2024-12-02-06:45:48-root-INFO: grad norm: 295.533 292.998 38.624
2024-12-02-06:45:48-root-INFO: Loss Change: 1638.151 -> 1595.073
2024-12-02-06:45:48-root-INFO: Regularization Change: 0.000 -> 0.074
2024-12-02-06:45:48-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-06:45:48-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:45:48-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-06:45:48-root-INFO: grad norm: 596.747 593.262 64.403
2024-12-02-06:45:48-root-INFO: Loss too large (1629.974->1869.298)! Learning rate decreased to 0.00316.
2024-12-02-06:45:49-root-INFO: Loss too large (1629.974->1854.547)! Learning rate decreased to 0.00253.
2024-12-02-06:45:49-root-INFO: Loss too large (1629.974->1839.008)! Learning rate decreased to 0.00202.
2024-12-02-06:45:49-root-INFO: Loss too large (1629.974->1821.060)! Learning rate decreased to 0.00162.
2024-12-02-06:45:49-root-INFO: Loss too large (1629.974->1799.406)! Learning rate decreased to 0.00129.
2024-12-02-06:45:49-root-INFO: Loss too large (1629.974->1773.620)! Learning rate decreased to 0.00104.
2024-12-02-06:45:49-root-INFO: Loss too large (1629.974->1743.691)! Learning rate decreased to 0.00083.
2024-12-02-06:45:50-root-INFO: Loss too large (1629.974->1709.645)! Learning rate decreased to 0.00066.
2024-12-02-06:45:50-root-INFO: Loss too large (1629.974->1672.750)! Learning rate decreased to 0.00053.
2024-12-02-06:45:50-root-INFO: Loss too large (1629.974->1637.274)! Learning rate decreased to 0.00042.
2024-12-02-06:45:50-root-INFO: grad norm: 441.790 438.675 52.376
2024-12-02-06:45:51-root-INFO: grad norm: 364.938 362.462 42.438
2024-12-02-06:45:51-root-INFO: grad norm: 358.791 356.051 44.257
2024-12-02-06:45:52-root-INFO: grad norm: 353.094 350.748 40.636
2024-12-02-06:45:52-root-INFO: grad norm: 351.243 348.543 43.469
2024-12-02-06:45:53-root-INFO: grad norm: 348.880 346.606 39.768
2024-12-02-06:45:53-root-INFO: grad norm: 347.734 345.059 43.054
2024-12-02-06:45:53-root-INFO: Loss Change: 1629.974 -> 1574.724
2024-12-02-06:45:53-root-INFO: Regularization Change: 0.000 -> 0.079
2024-12-02-06:45:53-root-INFO: Undo step: 175
2024-12-02-06:45:54-root-INFO: Undo step: 176
2024-12-02-06:45:54-root-INFO: Undo step: 177
2024-12-02-06:45:54-root-INFO: Undo step: 178
2024-12-02-06:45:54-root-INFO: Undo step: 179
2024-12-02-06:45:54-root-INFO: step: 180 lr_xt 0.00322295
2024-12-02-06:45:54-root-INFO: grad norm: 681.918 660.590 169.211
2024-12-02-06:45:54-root-INFO: grad norm: 521.964 503.671 136.976
2024-12-02-06:45:54-root-INFO: Loss too large (2215.800->2227.632)! Learning rate decreased to 0.00258.
2024-12-02-06:45:55-root-INFO: grad norm: 664.559 653.608 120.150
2024-12-02-06:45:55-root-INFO: Loss too large (2087.619->2173.629)! Learning rate decreased to 0.00206.
2024-12-02-06:45:55-root-INFO: Loss too large (2087.619->2107.618)! Learning rate decreased to 0.00165.
2024-12-02-06:45:56-root-INFO: grad norm: 286.474 272.860 87.262
2024-12-02-06:45:56-root-INFO: grad norm: 266.992 257.067 72.119
2024-12-02-06:45:57-root-INFO: grad norm: 199.317 191.821 54.148
2024-12-02-06:45:57-root-INFO: grad norm: 230.869 222.372 62.057
2024-12-02-06:45:57-root-INFO: Loss too large (1828.305->1836.323)! Learning rate decreased to 0.00132.
2024-12-02-06:45:58-root-INFO: grad norm: 333.799 327.705 63.492
2024-12-02-06:45:58-root-INFO: Loss too large (1815.926->1834.427)! Learning rate decreased to 0.00106.
2024-12-02-06:45:58-root-INFO: Loss Change: 2764.604 -> 1813.281
2024-12-02-06:45:58-root-INFO: Regularization Change: 0.000 -> 8.027
2024-12-02-06:45:58-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-02-06:45:58-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:45:58-root-INFO: step: 179 lr_xt 0.00335799
2024-12-02-06:45:59-root-INFO: grad norm: 269.823 262.381 62.935
2024-12-02-06:45:59-root-INFO: Loss too large (1781.008->2084.166)! Learning rate decreased to 0.00269.
2024-12-02-06:45:59-root-INFO: Loss too large (1781.008->1978.758)! Learning rate decreased to 0.00215.
2024-12-02-06:45:59-root-INFO: Loss too large (1781.008->1894.399)! Learning rate decreased to 0.00172.
2024-12-02-06:45:59-root-INFO: Loss too large (1781.008->1832.162)! Learning rate decreased to 0.00138.
2024-12-02-06:45:59-root-INFO: Loss too large (1781.008->1791.040)! Learning rate decreased to 0.00110.
2024-12-02-06:46:00-root-INFO: grad norm: 338.379 332.720 61.627
2024-12-02-06:46:00-root-INFO: Loss too large (1767.462->1780.227)! Learning rate decreased to 0.00088.
2024-12-02-06:46:00-root-INFO: grad norm: 291.855 284.396 65.563
2024-12-02-06:46:01-root-INFO: grad norm: 247.434 242.286 50.211
2024-12-02-06:46:02-root-INFO: grad norm: 254.802 248.291 57.234
2024-12-02-06:46:02-root-INFO: grad norm: 278.870 273.970 52.047
2024-12-02-06:46:02-root-INFO: Loss too large (1722.060->1722.998)! Learning rate decreased to 0.00070.
2024-12-02-06:46:03-root-INFO: grad norm: 228.065 222.127 51.706
2024-12-02-06:46:03-root-INFO: grad norm: 187.160 182.509 41.464
2024-12-02-06:46:04-root-INFO: Loss Change: 1781.008 -> 1695.282
2024-12-02-06:46:04-root-INFO: Regularization Change: 0.000 -> 0.460
2024-12-02-06:46:04-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-02-06:46:04-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:46:04-root-INFO: step: 178 lr_xt 0.00349812
2024-12-02-06:46:04-root-INFO: grad norm: 117.276 111.893 35.122
2024-12-02-06:46:04-root-INFO: grad norm: 102.436 96.922 33.156
2024-12-02-06:46:05-root-INFO: grad norm: 170.353 166.661 35.274
2024-12-02-06:46:05-root-INFO: Loss too large (1628.318->1764.861)! Learning rate decreased to 0.00280.
2024-12-02-06:46:05-root-INFO: Loss too large (1628.318->1731.036)! Learning rate decreased to 0.00224.
2024-12-02-06:46:05-root-INFO: Loss too large (1628.318->1700.307)! Learning rate decreased to 0.00179.
2024-12-02-06:46:06-root-INFO: Loss too large (1628.318->1674.596)! Learning rate decreased to 0.00143.
2024-12-02-06:46:06-root-INFO: Loss too large (1628.318->1654.919)! Learning rate decreased to 0.00115.
2024-12-02-06:46:06-root-INFO: Loss too large (1628.318->1641.113)! Learning rate decreased to 0.00092.
2024-12-02-06:46:06-root-INFO: Loss too large (1628.318->1632.209)! Learning rate decreased to 0.00073.
2024-12-02-06:46:06-root-INFO: grad norm: 222.690 218.430 43.350
2024-12-02-06:46:07-root-INFO: Loss too large (1626.953->1629.341)! Learning rate decreased to 0.00059.
2024-12-02-06:46:07-root-INFO: grad norm: 219.216 215.747 38.846
2024-12-02-06:46:08-root-INFO: grad norm: 220.595 216.335 43.142
2024-12-02-06:46:08-root-INFO: grad norm: 224.710 221.325 38.855
2024-12-02-06:46:09-root-INFO: grad norm: 229.827 225.539 44.191
2024-12-02-06:46:09-root-INFO: Loss Change: 1677.601 -> 1611.214
2024-12-02-06:46:09-root-INFO: Regularization Change: 0.000 -> 0.763
2024-12-02-06:46:09-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-02-06:46:09-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00020.
2024-12-02-06:46:09-root-INFO: step: 177 lr_xt 0.00364350
2024-12-02-06:46:09-root-INFO: grad norm: 431.012 426.427 62.700
2024-12-02-06:46:09-root-INFO: Loss too large (1621.525->1869.193)! Learning rate decreased to 0.00291.
2024-12-02-06:46:10-root-INFO: Loss too large (1621.525->1845.470)! Learning rate decreased to 0.00233.
2024-12-02-06:46:10-root-INFO: Loss too large (1621.525->1818.068)! Learning rate decreased to 0.00187.
2024-12-02-06:46:10-root-INFO: Loss too large (1621.525->1786.566)! Learning rate decreased to 0.00149.
2024-12-02-06:46:10-root-INFO: Loss too large (1621.525->1751.568)! Learning rate decreased to 0.00119.
2024-12-02-06:46:10-root-INFO: Loss too large (1621.525->1714.299)! Learning rate decreased to 0.00096.
2024-12-02-06:46:10-root-INFO: Loss too large (1621.525->1677.286)! Learning rate decreased to 0.00076.
2024-12-02-06:46:11-root-INFO: Loss too large (1621.525->1644.481)! Learning rate decreased to 0.00061.
2024-12-02-06:46:11-root-INFO: grad norm: 381.631 375.512 68.065
2024-12-02-06:46:12-root-INFO: grad norm: 337.858 333.996 50.937
2024-12-02-06:46:12-root-INFO: Loss too large (1602.465->1603.328)! Learning rate decreased to 0.00049.
2024-12-02-06:46:12-root-INFO: grad norm: 241.641 237.397 45.091
2024-12-02-06:46:13-root-INFO: grad norm: 179.009 175.926 33.080
2024-12-02-06:46:13-root-INFO: grad norm: 148.447 144.721 33.051
2024-12-02-06:46:14-root-INFO: grad norm: 125.569 122.415 27.963
2024-12-02-06:46:14-root-INFO: grad norm: 110.916 107.090 28.880
2024-12-02-06:46:15-root-INFO: Loss Change: 1621.525 -> 1573.550
2024-12-02-06:46:15-root-INFO: Regularization Change: 0.000 -> 0.098
2024-12-02-06:46:15-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-02-06:46:15-root-INFO: Coefficient of regularization decay: 0.00020 -> 0.00021.
2024-12-02-06:46:15-root-INFO: step: 176 lr_xt 0.00379432
2024-12-02-06:46:15-root-INFO: grad norm: 351.784 347.364 55.590
2024-12-02-06:46:15-root-INFO: Loss too large (1583.508->1836.862)! Learning rate decreased to 0.00304.
2024-12-02-06:46:16-root-INFO: Loss too large (1583.508->1808.488)! Learning rate decreased to 0.00243.
2024-12-02-06:46:16-root-INFO: Loss too large (1583.508->1777.022)! Learning rate decreased to 0.00194.
2024-12-02-06:46:16-root-INFO: Loss too large (1583.508->1742.575)! Learning rate decreased to 0.00155.
2024-12-02-06:46:16-root-INFO: Loss too large (1583.508->1705.947)! Learning rate decreased to 0.00124.
2024-12-02-06:46:16-root-INFO: Loss too large (1583.508->1668.885)! Learning rate decreased to 0.00099.
2024-12-02-06:46:16-root-INFO: Loss too large (1583.508->1634.499)! Learning rate decreased to 0.00080.
2024-12-02-06:46:17-root-INFO: Loss too large (1583.508->1606.174)! Learning rate decreased to 0.00064.
2024-12-02-06:46:17-root-INFO: Loss too large (1583.508->1585.814)! Learning rate decreased to 0.00051.
2024-12-02-06:46:17-root-INFO: grad norm: 278.215 274.140 47.444
2024-12-02-06:46:18-root-INFO: grad norm: 228.233 224.893 38.903
2024-12-02-06:46:18-root-INFO: grad norm: 202.788 199.287 37.519
2024-12-02-06:46:19-root-INFO: grad norm: 182.810 179.835 32.847
2024-12-02-06:46:19-root-INFO: grad norm: 169.447 166.119 33.419
2024-12-02-06:46:20-root-INFO: grad norm: 158.417 155.595 29.766
2024-12-02-06:46:21-root-INFO: grad norm: 150.309 147.063 31.066
2024-12-02-06:46:21-root-INFO: Loss Change: 1583.508 -> 1546.892
2024-12-02-06:46:21-root-INFO: Regularization Change: 0.000 -> 0.090
2024-12-02-06:46:21-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-02-06:46:21-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:46:21-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-06:46:21-root-INFO: grad norm: 373.753 369.658 55.174
2024-12-02-06:46:21-root-INFO: Loss too large (1558.788->1815.891)! Learning rate decreased to 0.00316.
2024-12-02-06:46:22-root-INFO: Loss too large (1558.788->1792.277)! Learning rate decreased to 0.00253.
2024-12-02-06:46:22-root-INFO: Loss too large (1558.788->1764.898)! Learning rate decreased to 0.00202.
2024-12-02-06:46:22-root-INFO: Loss too large (1558.788->1733.377)! Learning rate decreased to 0.00162.
2024-12-02-06:46:22-root-INFO: Loss too large (1558.788->1698.180)! Learning rate decreased to 0.00129.
2024-12-02-06:46:22-root-INFO: Loss too large (1558.788->1660.468)! Learning rate decreased to 0.00104.
2024-12-02-06:46:22-root-INFO: Loss too large (1558.788->1623.002)! Learning rate decreased to 0.00083.
2024-12-02-06:46:23-root-INFO: Loss too large (1558.788->1589.982)! Learning rate decreased to 0.00066.
2024-12-02-06:46:23-root-INFO: Loss too large (1558.788->1564.869)! Learning rate decreased to 0.00053.
2024-12-02-06:46:23-root-INFO: grad norm: 313.304 308.831 52.752
2024-12-02-06:46:24-root-INFO: grad norm: 275.016 271.776 42.090
2024-12-02-06:46:24-root-INFO: grad norm: 253.744 249.898 44.009
2024-12-02-06:46:25-root-INFO: grad norm: 237.241 234.343 36.967
2024-12-02-06:46:25-root-INFO: grad norm: 225.454 221.892 39.921
2024-12-02-06:46:25-root-INFO: grad norm: 215.630 212.914 34.117
2024-12-02-06:46:26-root-INFO: grad norm: 207.887 204.505 37.344
2024-12-02-06:46:26-root-INFO: Loss Change: 1558.788 -> 1522.564
2024-12-02-06:46:26-root-INFO: Regularization Change: 0.000 -> 0.091
2024-12-02-06:46:26-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-06:46:26-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:46:26-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-06:46:27-root-INFO: grad norm: 318.507 315.289 45.160
2024-12-02-06:46:27-root-INFO: Loss too large (1530.155->1782.751)! Learning rate decreased to 0.00329.
2024-12-02-06:46:27-root-INFO: Loss too large (1530.155->1760.818)! Learning rate decreased to 0.00263.
2024-12-02-06:46:27-root-INFO: Loss too large (1530.155->1733.875)! Learning rate decreased to 0.00211.
2024-12-02-06:46:27-root-INFO: Loss too large (1530.155->1702.137)! Learning rate decreased to 0.00168.
2024-12-02-06:46:27-root-INFO: Loss too large (1530.155->1666.468)! Learning rate decreased to 0.00135.
2024-12-02-06:46:28-root-INFO: Loss too large (1530.155->1628.742)! Learning rate decreased to 0.00108.
2024-12-02-06:46:28-root-INFO: Loss too large (1530.155->1592.486)! Learning rate decreased to 0.00086.
2024-12-02-06:46:28-root-INFO: Loss too large (1530.155->1561.829)! Learning rate decreased to 0.00069.
2024-12-02-06:46:28-root-INFO: Loss too large (1530.155->1539.364)! Learning rate decreased to 0.00055.
2024-12-02-06:46:29-root-INFO: grad norm: 298.835 294.853 48.621
2024-12-02-06:46:29-root-INFO: grad norm: 283.873 280.990 40.357
2024-12-02-06:46:30-root-INFO: grad norm: 271.999 268.266 44.909
2024-12-02-06:46:30-root-INFO: grad norm: 261.891 259.194 37.489
2024-12-02-06:46:31-root-INFO: grad norm: 252.796 249.247 42.213
2024-12-02-06:46:31-root-INFO: grad norm: 244.713 242.141 35.388
2024-12-02-06:46:32-root-INFO: grad norm: 237.059 233.664 39.975
2024-12-02-06:46:32-root-INFO: Loss Change: 1530.155 -> 1504.906
2024-12-02-06:46:32-root-INFO: Regularization Change: 0.000 -> 0.080
2024-12-02-06:46:32-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-06:46:32-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:46:32-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-06:46:32-root-INFO: grad norm: 448.317 443.906 62.731
2024-12-02-06:46:33-root-INFO: Loss too large (1535.981->1798.657)! Learning rate decreased to 0.00342.
2024-12-02-06:46:33-root-INFO: Loss too large (1535.981->1777.298)! Learning rate decreased to 0.00274.
2024-12-02-06:46:33-root-INFO: Loss too large (1535.981->1753.728)! Learning rate decreased to 0.00219.
2024-12-02-06:46:33-root-INFO: Loss too large (1535.981->1725.698)! Learning rate decreased to 0.00175.
2024-12-02-06:46:33-root-INFO: Loss too large (1535.981->1692.420)! Learning rate decreased to 0.00140.
2024-12-02-06:46:33-root-INFO: Loss too large (1535.981->1654.087)! Learning rate decreased to 0.00112.
2024-12-02-06:46:33-root-INFO: Loss too large (1535.981->1612.038)! Learning rate decreased to 0.00090.
2024-12-02-06:46:34-root-INFO: Loss too large (1535.981->1570.411)! Learning rate decreased to 0.00072.
2024-12-02-06:46:34-root-INFO: grad norm: 457.786 451.384 76.288
2024-12-02-06:46:35-root-INFO: grad norm: 443.090 439.093 59.383
2024-12-02-06:46:35-root-INFO: grad norm: 429.769 423.804 71.356
2024-12-02-06:46:36-root-INFO: grad norm: 411.721 407.980 55.377
2024-12-02-06:46:36-root-INFO: grad norm: 396.374 390.901 65.640
2024-12-02-06:46:37-root-INFO: grad norm: 379.920 376.368 51.827
2024-12-02-06:46:37-root-INFO: grad norm: 364.840 359.801 60.423
2024-12-02-06:46:37-root-INFO: Loss Change: 1535.981 -> 1494.404
2024-12-02-06:46:37-root-INFO: Regularization Change: 0.000 -> 0.187
2024-12-02-06:46:37-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-06:46:37-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-06:46:38-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-06:46:38-root-INFO: grad norm: 426.620 422.542 58.844
2024-12-02-06:46:38-root-INFO: Loss too large (1508.489->1762.975)! Learning rate decreased to 0.00356.
2024-12-02-06:46:38-root-INFO: Loss too large (1508.489->1742.808)! Learning rate decreased to 0.00285.
2024-12-02-06:46:38-root-INFO: Loss too large (1508.489->1717.943)! Learning rate decreased to 0.00228.
2024-12-02-06:46:38-root-INFO: Loss too large (1508.489->1686.659)! Learning rate decreased to 0.00182.
2024-12-02-06:46:39-root-INFO: Loss too large (1508.489->1649.026)! Learning rate decreased to 0.00146.
2024-12-02-06:46:39-root-INFO: Loss too large (1508.489->1606.117)! Learning rate decreased to 0.00117.
2024-12-02-06:46:39-root-INFO: Loss too large (1508.489->1560.828)! Learning rate decreased to 0.00093.
2024-12-02-06:46:39-root-INFO: Loss too large (1508.489->1519.334)! Learning rate decreased to 0.00075.
2024-12-02-06:46:39-root-INFO: grad norm: 365.758 360.921 59.291
2024-12-02-06:46:40-root-INFO: grad norm: 343.385 339.998 48.108
2024-12-02-06:46:40-root-INFO: grad norm: 327.073 322.673 53.467
2024-12-02-06:46:41-root-INFO: grad norm: 314.399 311.258 44.326
2024-12-02-06:46:41-root-INFO: grad norm: 301.852 297.731 49.708
2024-12-02-06:46:42-root-INFO: grad norm: 291.552 288.572 41.577
2024-12-02-06:46:42-root-INFO: grad norm: 280.932 277.050 46.540
2024-12-02-06:46:43-root-INFO: Loss Change: 1508.489 -> 1458.764
2024-12-02-06:46:43-root-INFO: Regularization Change: 0.000 -> 0.145
2024-12-02-06:46:43-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-06:46:43-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:46:43-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-06:46:43-root-INFO: grad norm: 390.786 387.175 53.003
2024-12-02-06:46:43-root-INFO: Loss too large (1478.030->1735.225)! Learning rate decreased to 0.00371.
2024-12-02-06:46:43-root-INFO: Loss too large (1478.030->1713.413)! Learning rate decreased to 0.00297.
2024-12-02-06:46:43-root-INFO: Loss too large (1478.030->1686.107)! Learning rate decreased to 0.00237.
2024-12-02-06:46:44-root-INFO: Loss too large (1478.030->1651.959)! Learning rate decreased to 0.00190.
2024-12-02-06:46:44-root-INFO: Loss too large (1478.030->1611.401)! Learning rate decreased to 0.00152.
2024-12-02-06:46:44-root-INFO: Loss too large (1478.030->1566.065)! Learning rate decreased to 0.00122.
2024-12-02-06:46:44-root-INFO: Loss too large (1478.030->1520.212)! Learning rate decreased to 0.00097.
2024-12-02-06:46:44-root-INFO: Loss too large (1478.030->1481.033)! Learning rate decreased to 0.00078.
2024-12-02-06:46:45-root-INFO: grad norm: 320.772 316.130 54.374
2024-12-02-06:46:45-root-INFO: grad norm: 300.565 297.664 41.661
2024-12-02-06:46:46-root-INFO: grad norm: 282.502 278.378 48.093
2024-12-02-06:46:46-root-INFO: grad norm: 271.324 268.638 38.080
2024-12-02-06:46:47-root-INFO: grad norm: 259.148 255.341 44.253
2024-12-02-06:46:47-root-INFO: grad norm: 250.696 248.131 35.770
2024-12-02-06:46:47-root-INFO: grad norm: 241.162 237.600 41.294
2024-12-02-06:46:48-root-INFO: Loss Change: 1478.030 -> 1427.918
2024-12-02-06:46:48-root-INFO: Regularization Change: 0.000 -> 0.150
2024-12-02-06:46:48-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-06:46:48-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:46:48-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-06:46:48-root-INFO: grad norm: 333.800 330.172 49.079
2024-12-02-06:46:48-root-INFO: Loss too large (1443.209->1703.530)! Learning rate decreased to 0.00386.
2024-12-02-06:46:48-root-INFO: Loss too large (1443.209->1678.028)! Learning rate decreased to 0.00309.
2024-12-02-06:46:49-root-INFO: Loss too large (1443.209->1646.675)! Learning rate decreased to 0.00247.
2024-12-02-06:46:49-root-INFO: Loss too large (1443.209->1609.102)! Learning rate decreased to 0.00198.
2024-12-02-06:46:49-root-INFO: Loss too large (1443.209->1566.408)! Learning rate decreased to 0.00158.
2024-12-02-06:46:49-root-INFO: Loss too large (1443.209->1521.323)! Learning rate decreased to 0.00126.
2024-12-02-06:46:49-root-INFO: Loss too large (1443.209->1479.282)! Learning rate decreased to 0.00101.
2024-12-02-06:46:49-root-INFO: Loss too large (1443.209->1446.474)! Learning rate decreased to 0.00081.
2024-12-02-06:46:50-root-INFO: grad norm: 284.889 280.869 47.694
2024-12-02-06:46:50-root-INFO: grad norm: 265.892 263.062 38.689
2024-12-02-06:46:51-root-INFO: grad norm: 247.843 244.248 42.059
2024-12-02-06:46:51-root-INFO: grad norm: 237.054 234.525 34.530
2024-12-02-06:46:52-root-INFO: grad norm: 224.955 221.645 38.448
2024-12-02-06:46:53-root-INFO: grad norm: 216.868 214.502 31.949
2024-12-02-06:46:53-root-INFO: grad norm: 207.547 204.462 35.653
2024-12-02-06:46:53-root-INFO: Loss Change: 1443.209 -> 1398.986
2024-12-02-06:46:53-root-INFO: Regularization Change: 0.000 -> 0.166
2024-12-02-06:46:53-root-INFO: Undo step: 170
2024-12-02-06:46:53-root-INFO: Undo step: 171
2024-12-02-06:46:53-root-INFO: Undo step: 172
2024-12-02-06:46:53-root-INFO: Undo step: 173
2024-12-02-06:46:53-root-INFO: Undo step: 174
2024-12-02-06:46:54-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-06:46:54-root-INFO: grad norm: 646.896 609.899 215.632
2024-12-02-06:46:54-root-INFO: grad norm: 299.336 285.907 88.654
2024-12-02-06:46:55-root-INFO: grad norm: 487.288 479.653 85.921
2024-12-02-06:46:55-root-INFO: Loss too large (1810.043->2468.929)! Learning rate decreased to 0.00316.
2024-12-02-06:46:55-root-INFO: Loss too large (1810.043->2287.000)! Learning rate decreased to 0.00253.
2024-12-02-06:46:55-root-INFO: Loss too large (1810.043->2143.714)! Learning rate decreased to 0.00202.
2024-12-02-06:46:55-root-INFO: Loss too large (1810.043->2023.521)! Learning rate decreased to 0.00162.
2024-12-02-06:46:56-root-INFO: Loss too large (1810.043->1921.115)! Learning rate decreased to 0.00129.
2024-12-02-06:46:56-root-INFO: Loss too large (1810.043->1836.909)! Learning rate decreased to 0.00104.
2024-12-02-06:46:56-root-INFO: grad norm: 552.376 546.833 78.057
2024-12-02-06:46:56-root-INFO: Loss too large (1773.957->1795.241)! Learning rate decreased to 0.00083.
2024-12-02-06:46:57-root-INFO: grad norm: 432.932 427.225 70.060
2024-12-02-06:46:57-root-INFO: grad norm: 360.831 355.705 60.601
2024-12-02-06:46:58-root-INFO: Loss too large (1701.975->1708.186)! Learning rate decreased to 0.00066.
2024-12-02-06:46:58-root-INFO: grad norm: 319.893 315.947 50.087
2024-12-02-06:46:59-root-INFO: grad norm: 293.946 289.132 52.978
2024-12-02-06:46:59-root-INFO: Loss Change: 2587.215 -> 1669.959
2024-12-02-06:46:59-root-INFO: Regularization Change: 0.000 -> 8.038
2024-12-02-06:46:59-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-06:46:59-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:46:59-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-06:46:59-root-INFO: grad norm: 234.716 231.468 38.909
2024-12-02-06:47:00-root-INFO: Loss too large (1654.971->1996.856)! Learning rate decreased to 0.00329.
2024-12-02-06:47:00-root-INFO: Loss too large (1654.971->1914.468)! Learning rate decreased to 0.00263.
2024-12-02-06:47:00-root-INFO: Loss too large (1654.971->1842.601)! Learning rate decreased to 0.00211.
2024-12-02-06:47:00-root-INFO: Loss too large (1654.971->1782.197)! Learning rate decreased to 0.00168.
2024-12-02-06:47:00-root-INFO: Loss too large (1654.971->1734.193)! Learning rate decreased to 0.00135.
2024-12-02-06:47:00-root-INFO: Loss too large (1654.971->1698.550)! Learning rate decreased to 0.00108.
2024-12-02-06:47:01-root-INFO: Loss too large (1654.971->1674.020)! Learning rate decreased to 0.00086.
2024-12-02-06:47:01-root-INFO: Loss too large (1654.971->1658.508)! Learning rate decreased to 0.00069.
2024-12-02-06:47:01-root-INFO: grad norm: 271.422 267.178 47.809
2024-12-02-06:47:02-root-INFO: grad norm: 306.898 303.552 45.196
2024-12-02-06:47:02-root-INFO: grad norm: 342.816 338.669 53.164
2024-12-02-06:47:03-root-INFO: grad norm: 360.889 357.242 51.178
2024-12-02-06:47:03-root-INFO: grad norm: 373.699 369.591 55.254
2024-12-02-06:47:04-root-INFO: grad norm: 375.454 371.715 52.861
2024-12-02-06:47:04-root-INFO: grad norm: 375.331 371.325 54.693
2024-12-02-06:47:04-root-INFO: Loss Change: 1654.971 -> 1621.815
2024-12-02-06:47:05-root-INFO: Regularization Change: 0.000 -> 0.293
2024-12-02-06:47:05-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-06:47:05-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:47:05-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-06:47:05-root-INFO: grad norm: 242.350 239.813 34.974
2024-12-02-06:47:05-root-INFO: Loss too large (1592.602->1956.980)! Learning rate decreased to 0.00342.
2024-12-02-06:47:05-root-INFO: Loss too large (1592.602->1870.225)! Learning rate decreased to 0.00274.
2024-12-02-06:47:05-root-INFO: Loss too large (1592.602->1795.784)! Learning rate decreased to 0.00219.
2024-12-02-06:47:05-root-INFO: Loss too large (1592.602->1733.062)! Learning rate decreased to 0.00175.
2024-12-02-06:47:06-root-INFO: Loss too large (1592.602->1682.454)! Learning rate decreased to 0.00140.
2024-12-02-06:47:06-root-INFO: Loss too large (1592.602->1644.009)! Learning rate decreased to 0.00112.
2024-12-02-06:47:06-root-INFO: Loss too large (1592.602->1616.821)! Learning rate decreased to 0.00090.
2024-12-02-06:47:06-root-INFO: Loss too large (1592.602->1599.096)! Learning rate decreased to 0.00072.
2024-12-02-06:47:07-root-INFO: grad norm: 294.150 290.505 46.159
2024-12-02-06:47:07-root-INFO: Loss too large (1588.606->1590.331)! Learning rate decreased to 0.00057.
2024-12-02-06:47:07-root-INFO: grad norm: 257.757 255.150 36.562
2024-12-02-06:47:08-root-INFO: grad norm: 236.193 232.932 39.115
2024-12-02-06:47:08-root-INFO: grad norm: 224.507 222.026 33.285
2024-12-02-06:47:09-root-INFO: grad norm: 217.640 214.546 36.563
2024-12-02-06:47:09-root-INFO: grad norm: 214.742 212.310 32.231
2024-12-02-06:47:10-root-INFO: grad norm: 214.170 211.179 35.672
2024-12-02-06:47:10-root-INFO: Loss Change: 1592.602 -> 1551.730
2024-12-02-06:47:10-root-INFO: Regularization Change: 0.000 -> 0.178
2024-12-02-06:47:10-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-06:47:10-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-06:47:10-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-06:47:11-root-INFO: grad norm: 112.496 109.788 24.532
2024-12-02-06:47:11-root-INFO: Loss too large (1538.901->1619.813)! Learning rate decreased to 0.00356.
2024-12-02-06:47:11-root-INFO: Loss too large (1538.901->1590.319)! Learning rate decreased to 0.00285.
2024-12-02-06:47:11-root-INFO: Loss too large (1538.901->1569.088)! Learning rate decreased to 0.00228.
2024-12-02-06:47:11-root-INFO: Loss too large (1538.901->1554.625)! Learning rate decreased to 0.00182.
2024-12-02-06:47:11-root-INFO: Loss too large (1538.901->1545.330)! Learning rate decreased to 0.00146.
2024-12-02-06:47:11-root-INFO: Loss too large (1538.901->1539.736)! Learning rate decreased to 0.00117.
2024-12-02-06:47:12-root-INFO: grad norm: 228.748 225.836 36.384
2024-12-02-06:47:12-root-INFO: Loss too large (1536.642->1577.804)! Learning rate decreased to 0.00093.
2024-12-02-06:47:12-root-INFO: Loss too large (1536.642->1556.533)! Learning rate decreased to 0.00075.
2024-12-02-06:47:12-root-INFO: Loss too large (1536.642->1542.358)! Learning rate decreased to 0.00060.
2024-12-02-06:47:13-root-INFO: grad norm: 250.870 248.472 34.608
2024-12-02-06:47:13-root-INFO: grad norm: 268.457 265.526 39.561
2024-12-02-06:47:14-root-INFO: grad norm: 285.813 283.234 38.311
2024-12-02-06:47:14-root-INFO: grad norm: 296.696 293.721 41.916
2024-12-02-06:47:15-root-INFO: grad norm: 305.771 303.075 40.515
2024-12-02-06:47:15-root-INFO: grad norm: 310.051 307.066 42.924
2024-12-02-06:47:16-root-INFO: Loss Change: 1538.901 -> 1519.865
2024-12-02-06:47:16-root-INFO: Regularization Change: 0.000 -> 0.174
2024-12-02-06:47:16-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-06:47:16-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:47:16-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-06:47:16-root-INFO: grad norm: 139.134 136.791 25.423
2024-12-02-06:47:16-root-INFO: Loss too large (1498.364->1669.998)! Learning rate decreased to 0.00371.
2024-12-02-06:47:17-root-INFO: Loss too large (1498.364->1621.029)! Learning rate decreased to 0.00297.
2024-12-02-06:47:17-root-INFO: Loss too large (1498.364->1581.980)! Learning rate decreased to 0.00237.
2024-12-02-06:47:17-root-INFO: Loss too large (1498.364->1552.158)! Learning rate decreased to 0.00190.
2024-12-02-06:47:17-root-INFO: Loss too large (1498.364->1530.486)! Learning rate decreased to 0.00152.
2024-12-02-06:47:17-root-INFO: Loss too large (1498.364->1515.554)! Learning rate decreased to 0.00122.
2024-12-02-06:47:18-root-INFO: Loss too large (1498.364->1505.835)! Learning rate decreased to 0.00097.
2024-12-02-06:47:18-root-INFO: Loss too large (1498.364->1499.897)! Learning rate decreased to 0.00078.
2024-12-02-06:47:18-root-INFO: grad norm: 205.485 203.092 31.272
2024-12-02-06:47:18-root-INFO: Loss too large (1496.547->1502.931)! Learning rate decreased to 0.00062.
2024-12-02-06:47:19-root-INFO: grad norm: 250.680 248.176 35.340
2024-12-02-06:47:19-root-INFO: Loss too large (1495.221->1496.861)! Learning rate decreased to 0.00050.
2024-12-02-06:47:20-root-INFO: grad norm: 206.300 204.002 30.706
2024-12-02-06:47:20-root-INFO: grad norm: 170.883 168.674 27.388
2024-12-02-06:47:21-root-INFO: grad norm: 150.372 148.160 25.698
2024-12-02-06:47:21-root-INFO: grad norm: 132.824 130.632 24.031
2024-12-02-06:47:22-root-INFO: grad norm: 121.027 118.771 23.261
2024-12-02-06:47:22-root-INFO: Loss Change: 1498.364 -> 1473.792
2024-12-02-06:47:22-root-INFO: Regularization Change: 0.000 -> 0.103
2024-12-02-06:47:22-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-06:47:22-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:47:22-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-06:47:22-root-INFO: grad norm: 128.885 126.099 26.654
2024-12-02-06:47:22-root-INFO: Loss too large (1468.981->1614.196)! Learning rate decreased to 0.00386.
2024-12-02-06:47:23-root-INFO: Loss too large (1468.981->1578.119)! Learning rate decreased to 0.00309.
2024-12-02-06:47:23-root-INFO: Loss too large (1468.981->1545.073)! Learning rate decreased to 0.00247.
2024-12-02-06:47:23-root-INFO: Loss too large (1468.981->1517.473)! Learning rate decreased to 0.00198.
2024-12-02-06:47:23-root-INFO: Loss too large (1468.981->1496.704)! Learning rate decreased to 0.00158.
2024-12-02-06:47:23-root-INFO: Loss too large (1468.981->1482.544)! Learning rate decreased to 0.00126.
2024-12-02-06:47:24-root-INFO: Loss too large (1468.981->1473.706)! Learning rate decreased to 0.00101.
2024-12-02-06:47:24-root-INFO: grad norm: 266.863 264.446 35.841
2024-12-02-06:47:24-root-INFO: Loss too large (1468.638->1508.257)! Learning rate decreased to 0.00081.
2024-12-02-06:47:24-root-INFO: Loss too large (1468.638->1487.345)! Learning rate decreased to 0.00065.
2024-12-02-06:47:25-root-INFO: Loss too large (1468.638->1473.585)! Learning rate decreased to 0.00052.
2024-12-02-06:47:25-root-INFO: grad norm: 238.240 235.920 33.166
2024-12-02-06:47:26-root-INFO: grad norm: 211.161 208.999 30.141
2024-12-02-06:47:26-root-INFO: grad norm: 196.179 194.045 28.855
2024-12-02-06:47:27-root-INFO: grad norm: 181.174 179.130 27.140
2024-12-02-06:47:27-root-INFO: grad norm: 171.521 169.479 26.389
2024-12-02-06:47:28-root-INFO: grad norm: 161.686 159.709 25.204
2024-12-02-06:47:28-root-INFO: Loss Change: 1468.981 -> 1445.805
2024-12-02-06:47:28-root-INFO: Regularization Change: 0.000 -> 0.113
2024-12-02-06:47:28-root-INFO: Undo step: 170
2024-12-02-06:47:28-root-INFO: Undo step: 171
2024-12-02-06:47:28-root-INFO: Undo step: 172
2024-12-02-06:47:28-root-INFO: Undo step: 173
2024-12-02-06:47:28-root-INFO: Undo step: 174
2024-12-02-06:47:28-root-INFO: step: 175 lr_xt 0.00395074
2024-12-02-06:47:28-root-INFO: grad norm: 647.260 621.540 180.647
2024-12-02-06:47:28-root-INFO: Loss too large (2672.806->2699.398)! Learning rate decreased to 0.00316.
2024-12-02-06:47:29-root-INFO: grad norm: 810.452 797.427 144.719
2024-12-02-06:47:29-root-INFO: grad norm: 830.937 815.464 159.613
2024-12-02-06:47:29-root-INFO: Loss too large (2462.423->2498.169)! Learning rate decreased to 0.00253.
2024-12-02-06:47:30-root-INFO: grad norm: 569.751 557.524 117.404
2024-12-02-06:47:30-root-INFO: grad norm: 286.200 279.870 59.860
2024-12-02-06:47:31-root-INFO: grad norm: 209.328 203.085 50.740
2024-12-02-06:47:31-root-INFO: grad norm: 239.754 230.706 65.247
2024-12-02-06:47:32-root-INFO: Loss too large (1710.492->1721.962)! Learning rate decreased to 0.00202.
2024-12-02-06:47:32-root-INFO: grad norm: 279.364 274.231 53.306
2024-12-02-06:47:32-root-INFO: Loss too large (1698.349->1714.587)! Learning rate decreased to 0.00162.
2024-12-02-06:47:32-root-INFO: Loss Change: 2672.806 -> 1678.894
2024-12-02-06:47:33-root-INFO: Regularization Change: 0.000 -> 13.659
2024-12-02-06:47:33-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-02-06:47:33-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:47:33-root-INFO: step: 174 lr_xt 0.00411294
2024-12-02-06:47:33-root-INFO: grad norm: 396.385 391.986 58.887
2024-12-02-06:47:33-root-INFO: Loss too large (1687.790->1840.355)! Learning rate decreased to 0.00329.
2024-12-02-06:47:33-root-INFO: Loss too large (1687.790->1807.649)! Learning rate decreased to 0.00263.
2024-12-02-06:47:33-root-INFO: Loss too large (1687.790->1772.135)! Learning rate decreased to 0.00211.
2024-12-02-06:47:33-root-INFO: Loss too large (1687.790->1736.482)! Learning rate decreased to 0.00168.
2024-12-02-06:47:34-root-INFO: Loss too large (1687.790->1702.779)! Learning rate decreased to 0.00135.
2024-12-02-06:47:34-root-INFO: grad norm: 264.527 260.227 47.503
2024-12-02-06:47:35-root-INFO: grad norm: 143.772 137.696 41.355
2024-12-02-06:47:35-root-INFO: grad norm: 160.829 157.221 33.874
2024-12-02-06:47:36-root-INFO: grad norm: 238.119 234.950 38.718
2024-12-02-06:47:36-root-INFO: Loss too large (1606.010->1617.447)! Learning rate decreased to 0.00108.
2024-12-02-06:47:36-root-INFO: grad norm: 241.173 237.977 39.131
2024-12-02-06:47:37-root-INFO: grad norm: 251.953 248.924 38.954
2024-12-02-06:47:37-root-INFO: Loss too large (1593.572->1594.187)! Learning rate decreased to 0.00086.
2024-12-02-06:47:37-root-INFO: grad norm: 201.527 198.525 34.653
2024-12-02-06:47:38-root-INFO: Loss Change: 1687.790 -> 1574.269
2024-12-02-06:47:38-root-INFO: Regularization Change: 0.000 -> 0.629
2024-12-02-06:47:38-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-02-06:47:38-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00021.
2024-12-02-06:47:38-root-INFO: step: 173 lr_xt 0.00428111
2024-12-02-06:47:38-root-INFO: grad norm: 332.794 328.643 52.396
2024-12-02-06:47:38-root-INFO: Loss too large (1590.477->1785.555)! Learning rate decreased to 0.00342.
2024-12-02-06:47:38-root-INFO: Loss too large (1590.477->1750.179)! Learning rate decreased to 0.00274.
2024-12-02-06:47:38-root-INFO: Loss too large (1590.477->1714.082)! Learning rate decreased to 0.00219.
2024-12-02-06:47:39-root-INFO: Loss too large (1590.477->1678.547)! Learning rate decreased to 0.00175.
2024-12-02-06:47:39-root-INFO: Loss too large (1590.477->1644.699)! Learning rate decreased to 0.00140.
2024-12-02-06:47:39-root-INFO: Loss too large (1590.477->1614.237)! Learning rate decreased to 0.00112.
2024-12-02-06:47:39-root-INFO: grad norm: 297.669 294.267 44.877
2024-12-02-06:47:40-root-INFO: grad norm: 271.021 267.763 41.894
2024-12-02-06:47:40-root-INFO: Loss too large (1564.290->1568.347)! Learning rate decreased to 0.00090.
2024-12-02-06:47:40-root-INFO: grad norm: 225.840 223.037 35.468
2024-12-02-06:47:41-root-INFO: grad norm: 198.955 196.070 33.759
2024-12-02-06:47:41-root-INFO: grad norm: 196.303 193.632 32.272
2024-12-02-06:47:42-root-INFO: grad norm: 199.550 196.921 32.285
2024-12-02-06:47:42-root-INFO: grad norm: 207.717 205.132 32.666
2024-12-02-06:47:43-root-INFO: Loss Change: 1590.477 -> 1528.631
2024-12-02-06:47:43-root-INFO: Regularization Change: 0.000 -> 0.326
2024-12-02-06:47:43-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-02-06:47:43-root-INFO: Coefficient of regularization decay: 0.00021 -> 0.00022.
2024-12-02-06:47:43-root-INFO: step: 172 lr_xt 0.00445543
2024-12-02-06:47:43-root-INFO: grad norm: 310.754 307.729 43.256
2024-12-02-06:47:43-root-INFO: Loss too large (1534.358->1735.079)! Learning rate decreased to 0.00356.
2024-12-02-06:47:43-root-INFO: Loss too large (1534.358->1705.619)! Learning rate decreased to 0.00285.
2024-12-02-06:47:44-root-INFO: Loss too large (1534.358->1673.727)! Learning rate decreased to 0.00228.
2024-12-02-06:47:44-root-INFO: Loss too large (1534.358->1640.700)! Learning rate decreased to 0.00182.
2024-12-02-06:47:44-root-INFO: Loss too large (1534.358->1607.542)! Learning rate decreased to 0.00146.
2024-12-02-06:47:44-root-INFO: Loss too large (1534.358->1575.924)! Learning rate decreased to 0.00117.
2024-12-02-06:47:44-root-INFO: Loss too large (1534.358->1548.611)! Learning rate decreased to 0.00093.
2024-12-02-06:47:45-root-INFO: grad norm: 283.073 280.396 38.841
2024-12-02-06:47:45-root-INFO: grad norm: 266.394 263.747 37.455
2024-12-02-06:47:46-root-INFO: grad norm: 265.245 262.679 36.809
2024-12-02-06:47:46-root-INFO: grad norm: 264.433 261.919 36.372
2024-12-02-06:47:47-root-INFO: grad norm: 265.548 262.999 36.708
2024-12-02-06:47:47-root-INFO: grad norm: 265.775 263.325 36.008
2024-12-02-06:47:48-root-INFO: grad norm: 266.583 264.046 36.694
2024-12-02-06:47:48-root-INFO: Loss Change: 1534.358 -> 1496.284
2024-12-02-06:47:48-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-02-06:47:48-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-02-06:47:48-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:47:48-root-INFO: step: 171 lr_xt 0.00463611
2024-12-02-06:47:48-root-INFO: grad norm: 369.176 366.274 46.202
2024-12-02-06:47:48-root-INFO: Loss too large (1515.182->1728.056)! Learning rate decreased to 0.00371.
2024-12-02-06:47:48-root-INFO: Loss too large (1515.182->1698.209)! Learning rate decreased to 0.00297.
2024-12-02-06:47:49-root-INFO: Loss too large (1515.182->1664.563)! Learning rate decreased to 0.00237.
2024-12-02-06:47:49-root-INFO: Loss too large (1515.182->1628.504)! Learning rate decreased to 0.00190.
2024-12-02-06:47:49-root-INFO: Loss too large (1515.182->1591.067)! Learning rate decreased to 0.00152.
2024-12-02-06:47:49-root-INFO: Loss too large (1515.182->1553.474)! Learning rate decreased to 0.00122.
2024-12-02-06:47:49-root-INFO: Loss too large (1515.182->1518.821)! Learning rate decreased to 0.00097.
2024-12-02-06:47:50-root-INFO: grad norm: 296.091 293.279 40.709
2024-12-02-06:47:50-root-INFO: grad norm: 277.948 275.634 35.792
2024-12-02-06:47:51-root-INFO: grad norm: 275.170 272.532 38.012
2024-12-02-06:47:51-root-INFO: grad norm: 272.500 270.275 34.754
2024-12-02-06:47:52-root-INFO: grad norm: 271.673 269.091 37.367
2024-12-02-06:47:52-root-INFO: grad norm: 269.726 267.533 34.329
2024-12-02-06:47:52-root-INFO: grad norm: 268.689 266.152 36.839
2024-12-02-06:47:53-root-INFO: Loss Change: 1515.182 -> 1461.764
2024-12-02-06:47:53-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-02-06:47:53-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-02-06:47:53-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:47:53-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-06:47:53-root-INFO: grad norm: 351.306 348.378 45.262
2024-12-02-06:47:53-root-INFO: Loss too large (1478.930->1697.734)! Learning rate decreased to 0.00386.
2024-12-02-06:47:53-root-INFO: Loss too large (1478.930->1666.489)! Learning rate decreased to 0.00309.
2024-12-02-06:47:54-root-INFO: Loss too large (1478.930->1631.524)! Learning rate decreased to 0.00247.
2024-12-02-06:47:54-root-INFO: Loss too large (1478.930->1594.250)! Learning rate decreased to 0.00198.
2024-12-02-06:47:54-root-INFO: Loss too large (1478.930->1555.777)! Learning rate decreased to 0.00158.
2024-12-02-06:47:54-root-INFO: Loss too large (1478.930->1517.503)! Learning rate decreased to 0.00126.
2024-12-02-06:47:54-root-INFO: Loss too large (1478.930->1482.785)! Learning rate decreased to 0.00101.
2024-12-02-06:47:55-root-INFO: grad norm: 290.420 287.788 39.009
2024-12-02-06:47:55-root-INFO: grad norm: 276.112 273.836 35.383
2024-12-02-06:47:56-root-INFO: grad norm: 272.550 270.061 36.756
2024-12-02-06:47:56-root-INFO: grad norm: 269.122 266.974 33.940
2024-12-02-06:47:56-root-INFO: grad norm: 267.287 264.860 35.936
2024-12-02-06:47:57-root-INFO: grad norm: 264.583 262.489 33.222
2024-12-02-06:47:57-root-INFO: grad norm: 262.712 260.333 35.269
2024-12-02-06:47:58-root-INFO: Loss Change: 1478.930 -> 1428.623
2024-12-02-06:47:58-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-02-06:47:58-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-06:47:58-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:47:58-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-06:47:58-root-INFO: grad norm: 330.107 327.464 41.693
2024-12-02-06:47:58-root-INFO: Loss too large (1444.725->1667.388)! Learning rate decreased to 0.00401.
2024-12-02-06:47:58-root-INFO: Loss too large (1444.725->1635.115)! Learning rate decreased to 0.00321.
2024-12-02-06:47:59-root-INFO: Loss too large (1444.725->1598.484)! Learning rate decreased to 0.00257.
2024-12-02-06:47:59-root-INFO: Loss too large (1444.725->1559.085)! Learning rate decreased to 0.00206.
2024-12-02-06:47:59-root-INFO: Loss too large (1444.725->1518.266)! Learning rate decreased to 0.00164.
2024-12-02-06:47:59-root-INFO: Loss too large (1444.725->1478.166)! Learning rate decreased to 0.00132.
2024-12-02-06:48:00-root-INFO: grad norm: 340.215 337.117 45.805
2024-12-02-06:48:00-root-INFO: grad norm: 325.791 323.352 39.788
2024-12-02-06:48:01-root-INFO: grad norm: 310.718 307.977 41.178
2024-12-02-06:48:01-root-INFO: grad norm: 298.307 296.011 36.937
2024-12-02-06:48:02-root-INFO: grad norm: 288.576 286.055 38.066
2024-12-02-06:48:02-root-INFO: grad norm: 280.418 278.224 35.009
2024-12-02-06:48:02-root-INFO: grad norm: 274.830 272.418 36.335
2024-12-02-06:48:03-root-INFO: Loss Change: 1444.725 -> 1403.807
2024-12-02-06:48:03-root-INFO: Regularization Change: 0.000 -> 0.356
2024-12-02-06:48:03-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-06:48:03-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:48:03-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-06:48:03-root-INFO: grad norm: 317.898 315.591 38.229
2024-12-02-06:48:03-root-INFO: Loss too large (1421.711->1629.153)! Learning rate decreased to 0.00417.
2024-12-02-06:48:03-root-INFO: Loss too large (1421.711->1593.933)! Learning rate decreased to 0.00334.
2024-12-02-06:48:04-root-INFO: Loss too large (1421.711->1553.265)! Learning rate decreased to 0.00267.
2024-12-02-06:48:04-root-INFO: Loss too large (1421.711->1509.407)! Learning rate decreased to 0.00214.
2024-12-02-06:48:04-root-INFO: Loss too large (1421.711->1464.640)! Learning rate decreased to 0.00171.
2024-12-02-06:48:04-root-INFO: Loss too large (1421.711->1423.172)! Learning rate decreased to 0.00137.
2024-12-02-06:48:05-root-INFO: grad norm: 267.739 265.362 35.592
2024-12-02-06:48:05-root-INFO: Loss too large (1391.973->1393.969)! Learning rate decreased to 0.00109.
2024-12-02-06:48:05-root-INFO: grad norm: 193.388 191.604 26.209
2024-12-02-06:48:06-root-INFO: grad norm: 152.843 151.054 23.316
2024-12-02-06:48:06-root-INFO: grad norm: 139.097 137.524 20.857
2024-12-02-06:48:07-root-INFO: grad norm: 130.887 129.182 21.058
2024-12-02-06:48:07-root-INFO: grad norm: 128.807 127.306 19.606
2024-12-02-06:48:08-root-INFO: grad norm: 130.408 128.741 20.783
2024-12-02-06:48:08-root-INFO: Loss Change: 1421.711 -> 1353.256
2024-12-02-06:48:08-root-INFO: Regularization Change: 0.000 -> 0.249
2024-12-02-06:48:08-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-06:48:08-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-06:48:08-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-06:48:08-root-INFO: grad norm: 223.812 221.587 31.481
2024-12-02-06:48:09-root-INFO: Loss too large (1363.663->1583.779)! Learning rate decreased to 0.00434.
2024-12-02-06:48:09-root-INFO: Loss too large (1363.663->1544.848)! Learning rate decreased to 0.00347.
2024-12-02-06:48:09-root-INFO: Loss too large (1363.663->1503.736)! Learning rate decreased to 0.00278.
2024-12-02-06:48:09-root-INFO: Loss too large (1363.663->1461.922)! Learning rate decreased to 0.00222.
2024-12-02-06:48:09-root-INFO: Loss too large (1363.663->1422.036)! Learning rate decreased to 0.00178.
2024-12-02-06:48:09-root-INFO: Loss too large (1363.663->1388.394)! Learning rate decreased to 0.00142.
2024-12-02-06:48:10-root-INFO: Loss too large (1363.663->1364.719)! Learning rate decreased to 0.00114.
2024-12-02-06:48:10-root-INFO: grad norm: 199.028 197.063 27.896
2024-12-02-06:48:11-root-INFO: grad norm: 193.089 191.376 25.663
2024-12-02-06:48:11-root-INFO: grad norm: 189.263 187.437 26.226
2024-12-02-06:48:12-root-INFO: grad norm: 187.986 186.343 24.794
2024-12-02-06:48:12-root-INFO: grad norm: 187.471 185.699 25.719
2024-12-02-06:48:12-root-INFO: Loss too large (1337.698->1338.053)! Learning rate decreased to 0.00091.
2024-12-02-06:48:13-root-INFO: grad norm: 134.641 133.149 19.985
2024-12-02-06:48:13-root-INFO: grad norm: 93.588 92.049 16.902
2024-12-02-06:48:14-root-INFO: Loss Change: 1363.663 -> 1324.105
2024-12-02-06:48:14-root-INFO: Regularization Change: 0.000 -> 0.203
2024-12-02-06:48:14-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-06:48:14-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:48:14-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-06:48:14-root-INFO: grad norm: 170.478 168.506 25.857
2024-12-02-06:48:14-root-INFO: Loss too large (1329.266->1525.497)! Learning rate decreased to 0.00451.
2024-12-02-06:48:14-root-INFO: Loss too large (1329.266->1486.131)! Learning rate decreased to 0.00361.
2024-12-02-06:48:15-root-INFO: Loss too large (1329.266->1445.687)! Learning rate decreased to 0.00289.
2024-12-02-06:48:15-root-INFO: Loss too large (1329.266->1406.462)! Learning rate decreased to 0.00231.
2024-12-02-06:48:15-root-INFO: Loss too large (1329.266->1372.277)! Learning rate decreased to 0.00185.
2024-12-02-06:48:15-root-INFO: Loss too large (1329.266->1346.804)! Learning rate decreased to 0.00148.
2024-12-02-06:48:15-root-INFO: Loss too large (1329.266->1330.790)! Learning rate decreased to 0.00118.
2024-12-02-06:48:16-root-INFO: grad norm: 167.915 166.131 24.408
2024-12-02-06:48:16-root-INFO: grad norm: 169.267 167.686 23.077
2024-12-02-06:48:17-root-INFO: grad norm: 173.895 172.183 24.340
2024-12-02-06:48:17-root-INFO: Loss too large (1316.438->1316.963)! Learning rate decreased to 0.00095.
2024-12-02-06:48:17-root-INFO: grad norm: 128.476 127.026 19.249
2024-12-02-06:48:18-root-INFO: grad norm: 89.847 88.305 16.572
2024-12-02-06:48:18-root-INFO: grad norm: 74.688 73.138 15.138
2024-12-02-06:48:19-root-INFO: grad norm: 63.193 61.497 14.542
2024-12-02-06:48:19-root-INFO: Loss Change: 1329.266 -> 1298.913
2024-12-02-06:48:19-root-INFO: Regularization Change: 0.000 -> 0.181
2024-12-02-06:48:19-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-06:48:19-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:48:19-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-06:48:20-root-INFO: grad norm: 123.813 122.246 19.636
2024-12-02-06:48:20-root-INFO: Loss too large (1301.372->1461.200)! Learning rate decreased to 0.00469.
2024-12-02-06:48:20-root-INFO: Loss too large (1301.372->1423.296)! Learning rate decreased to 0.00375.
2024-12-02-06:48:20-root-INFO: Loss too large (1301.372->1386.294)! Learning rate decreased to 0.00300.
2024-12-02-06:48:20-root-INFO: Loss too large (1301.372->1353.784)! Learning rate decreased to 0.00240.
2024-12-02-06:48:20-root-INFO: Loss too large (1301.372->1329.011)! Learning rate decreased to 0.00192.
2024-12-02-06:48:21-root-INFO: Loss too large (1301.372->1312.663)! Learning rate decreased to 0.00154.
2024-12-02-06:48:21-root-INFO: Loss too large (1301.372->1303.134)! Learning rate decreased to 0.00123.
2024-12-02-06:48:21-root-INFO: grad norm: 144.086 142.508 21.265
2024-12-02-06:48:21-root-INFO: Loss too large (1298.188->1298.597)! Learning rate decreased to 0.00098.
2024-12-02-06:48:22-root-INFO: grad norm: 117.644 116.256 18.021
2024-12-02-06:48:22-root-INFO: grad norm: 91.705 90.249 16.279
2024-12-02-06:48:23-root-INFO: grad norm: 79.928 78.516 14.957
2024-12-02-06:48:23-root-INFO: grad norm: 69.344 67.831 14.403
2024-12-02-06:48:24-root-INFO: grad norm: 63.261 61.744 13.768
2024-12-02-06:48:24-root-INFO: grad norm: 58.074 56.477 13.526
2024-12-02-06:48:25-root-INFO: Loss Change: 1301.372 -> 1278.497
2024-12-02-06:48:25-root-INFO: Regularization Change: 0.000 -> 0.156
2024-12-02-06:48:25-root-INFO: Undo step: 165
2024-12-02-06:48:25-root-INFO: Undo step: 166
2024-12-02-06:48:25-root-INFO: Undo step: 167
2024-12-02-06:48:25-root-INFO: Undo step: 168
2024-12-02-06:48:25-root-INFO: Undo step: 169
2024-12-02-06:48:25-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-06:48:25-root-INFO: grad norm: 632.907 620.949 122.448
2024-12-02-06:48:25-root-INFO: grad norm: 475.937 470.149 74.000
2024-12-02-06:48:26-root-INFO: grad norm: 539.205 530.702 95.382
2024-12-02-06:48:26-root-INFO: grad norm: 533.522 526.702 85.030
2024-12-02-06:48:27-root-INFO: grad norm: 338.466 332.411 63.736
2024-12-02-06:48:27-root-INFO: grad norm: 164.042 158.758 41.301
2024-12-02-06:48:28-root-INFO: grad norm: 158.802 156.774 25.297
2024-12-02-06:48:28-root-INFO: Loss too large (1420.016->1519.099)! Learning rate decreased to 0.00386.
2024-12-02-06:48:28-root-INFO: Loss too large (1420.016->1485.946)! Learning rate decreased to 0.00309.
2024-12-02-06:48:28-root-INFO: Loss too large (1420.016->1457.603)! Learning rate decreased to 0.00247.
2024-12-02-06:48:28-root-INFO: Loss too large (1420.016->1435.964)! Learning rate decreased to 0.00198.
2024-12-02-06:48:29-root-INFO: Loss too large (1420.016->1421.523)! Learning rate decreased to 0.00158.
2024-12-02-06:48:29-root-INFO: grad norm: 230.774 228.043 35.403
2024-12-02-06:48:29-root-INFO: Loss too large (1413.173->1430.371)! Learning rate decreased to 0.00126.
2024-12-02-06:48:29-root-INFO: Loss Change: 2433.223 -> 1412.959
2024-12-02-06:48:29-root-INFO: Regularization Change: 0.000 -> 17.037
2024-12-02-06:48:29-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-06:48:29-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:48:30-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-06:48:30-root-INFO: grad norm: 313.577 311.467 36.315
2024-12-02-06:48:30-root-INFO: Loss too large (1426.487->1627.251)! Learning rate decreased to 0.00401.
2024-12-02-06:48:30-root-INFO: Loss too large (1426.487->1595.929)! Learning rate decreased to 0.00321.
2024-12-02-06:48:30-root-INFO: Loss too large (1426.487->1559.038)! Learning rate decreased to 0.00257.
2024-12-02-06:48:30-root-INFO: Loss too large (1426.487->1517.897)! Learning rate decreased to 0.00206.
2024-12-02-06:48:30-root-INFO: Loss too large (1426.487->1474.529)! Learning rate decreased to 0.00164.
2024-12-02-06:48:31-root-INFO: Loss too large (1426.487->1434.441)! Learning rate decreased to 0.00132.
2024-12-02-06:48:31-root-INFO: grad norm: 282.084 279.438 38.544
2024-12-02-06:48:32-root-INFO: grad norm: 263.825 262.014 30.866
2024-12-02-06:48:32-root-INFO: grad norm: 245.667 243.223 34.568
2024-12-02-06:48:33-root-INFO: grad norm: 235.190 233.500 28.151
2024-12-02-06:48:33-root-INFO: grad norm: 225.993 223.654 32.426
2024-12-02-06:48:33-root-INFO: grad norm: 221.160 219.523 26.862
2024-12-02-06:48:34-root-INFO: grad norm: 218.205 215.933 31.407
2024-12-02-06:48:34-root-INFO: Loss Change: 1426.487 -> 1362.799
2024-12-02-06:48:34-root-INFO: Regularization Change: 0.000 -> 0.457
2024-12-02-06:48:34-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-06:48:34-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:48:34-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-06:48:35-root-INFO: grad norm: 280.654 278.760 32.548
2024-12-02-06:48:35-root-INFO: Loss too large (1376.896->1589.852)! Learning rate decreased to 0.00417.
2024-12-02-06:48:35-root-INFO: Loss too large (1376.896->1555.584)! Learning rate decreased to 0.00334.
2024-12-02-06:48:35-root-INFO: Loss too large (1376.896->1515.312)! Learning rate decreased to 0.00267.
2024-12-02-06:48:35-root-INFO: Loss too large (1376.896->1470.254)! Learning rate decreased to 0.00214.
2024-12-02-06:48:35-root-INFO: Loss too large (1376.896->1423.299)! Learning rate decreased to 0.00171.
2024-12-02-06:48:36-root-INFO: Loss too large (1376.896->1382.459)! Learning rate decreased to 0.00137.
2024-12-02-06:48:36-root-INFO: grad norm: 248.969 246.615 34.156
2024-12-02-06:48:36-root-INFO: Loss too large (1355.613->1356.673)! Learning rate decreased to 0.00109.
2024-12-02-06:48:37-root-INFO: grad norm: 172.399 170.849 23.070
2024-12-02-06:48:37-root-INFO: grad norm: 115.116 112.864 22.657
2024-12-02-06:48:38-root-INFO: grad norm: 94.179 92.398 18.228
2024-12-02-06:48:38-root-INFO: grad norm: 79.010 76.534 19.627
2024-12-02-06:48:39-root-INFO: grad norm: 71.072 68.965 17.177
2024-12-02-06:48:39-root-INFO: grad norm: 65.287 62.661 18.330
2024-12-02-06:48:40-root-INFO: Loss Change: 1376.896 -> 1315.024
2024-12-02-06:48:40-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-02-06:48:40-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-06:48:40-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-06:48:40-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-06:48:40-root-INFO: grad norm: 144.058 142.015 24.175
2024-12-02-06:48:40-root-INFO: Loss too large (1316.842->1483.097)! Learning rate decreased to 0.00434.
2024-12-02-06:48:40-root-INFO: Loss too large (1316.842->1439.629)! Learning rate decreased to 0.00347.
2024-12-02-06:48:40-root-INFO: Loss too large (1316.842->1397.295)! Learning rate decreased to 0.00278.
2024-12-02-06:48:41-root-INFO: Loss too large (1316.842->1361.553)! Learning rate decreased to 0.00222.
2024-12-02-06:48:41-root-INFO: Loss too large (1316.842->1336.228)! Learning rate decreased to 0.00178.
2024-12-02-06:48:41-root-INFO: Loss too large (1316.842->1320.892)! Learning rate decreased to 0.00142.
2024-12-02-06:48:41-root-INFO: grad norm: 180.231 178.079 27.767
2024-12-02-06:48:41-root-INFO: Loss too large (1312.750->1317.284)! Learning rate decreased to 0.00114.
2024-12-02-06:48:42-root-INFO: grad norm: 159.783 158.123 22.978
2024-12-02-06:48:43-root-INFO: grad norm: 137.507 135.491 23.461
2024-12-02-06:48:43-root-INFO: grad norm: 126.942 125.342 20.086
2024-12-02-06:48:44-root-INFO: grad norm: 116.028 114.059 21.287
2024-12-02-06:48:44-root-INFO: grad norm: 110.151 108.560 18.654
2024-12-02-06:48:45-root-INFO: grad norm: 104.458 102.514 20.058
2024-12-02-06:48:45-root-INFO: Loss Change: 1316.842 -> 1286.122
2024-12-02-06:48:45-root-INFO: Regularization Change: 0.000 -> 0.250
2024-12-02-06:48:45-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-06:48:45-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:48:45-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-06:48:45-root-INFO: grad norm: 182.393 180.600 25.515
2024-12-02-06:48:45-root-INFO: Loss too large (1289.877->1505.190)! Learning rate decreased to 0.00451.
2024-12-02-06:48:45-root-INFO: Loss too large (1289.877->1466.164)! Learning rate decreased to 0.00361.
2024-12-02-06:48:46-root-INFO: Loss too large (1289.877->1422.998)! Learning rate decreased to 0.00289.
2024-12-02-06:48:46-root-INFO: Loss too large (1289.877->1377.696)! Learning rate decreased to 0.00231.
2024-12-02-06:48:46-root-INFO: Loss too large (1289.877->1336.514)! Learning rate decreased to 0.00185.
2024-12-02-06:48:46-root-INFO: Loss too large (1289.877->1306.412)! Learning rate decreased to 0.00148.
2024-12-02-06:48:47-root-INFO: grad norm: 249.227 247.042 32.930
2024-12-02-06:48:47-root-INFO: Loss too large (1288.612->1304.185)! Learning rate decreased to 0.00118.
2024-12-02-06:48:47-root-INFO: grad norm: 212.577 210.888 26.744
2024-12-02-06:48:48-root-INFO: grad norm: 162.543 160.676 24.565
2024-12-02-06:48:48-root-INFO: grad norm: 148.000 146.500 21.019
2024-12-02-06:48:49-root-INFO: grad norm: 130.784 128.993 21.567
2024-12-02-06:48:49-root-INFO: grad norm: 123.042 121.576 18.943
2024-12-02-06:48:50-root-INFO: grad norm: 114.673 112.909 20.033
2024-12-02-06:48:50-root-INFO: Loss Change: 1289.877 -> 1256.978
2024-12-02-06:48:50-root-INFO: Regularization Change: 0.000 -> 0.241
2024-12-02-06:48:50-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-06:48:50-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:48:50-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-06:48:51-root-INFO: grad norm: 174.616 172.959 23.996
2024-12-02-06:48:51-root-INFO: Loss too large (1262.000->1482.817)! Learning rate decreased to 0.00469.
2024-12-02-06:48:51-root-INFO: Loss too large (1262.000->1445.417)! Learning rate decreased to 0.00375.
2024-12-02-06:48:51-root-INFO: Loss too large (1262.000->1402.839)! Learning rate decreased to 0.00300.
2024-12-02-06:48:51-root-INFO: Loss too large (1262.000->1356.843)! Learning rate decreased to 0.00240.
2024-12-02-06:48:51-root-INFO: Loss too large (1262.000->1313.912)! Learning rate decreased to 0.00192.
2024-12-02-06:48:51-root-INFO: Loss too large (1262.000->1282.056)! Learning rate decreased to 0.00154.
2024-12-02-06:48:52-root-INFO: Loss too large (1262.000->1263.111)! Learning rate decreased to 0.00123.
2024-12-02-06:48:52-root-INFO: grad norm: 158.894 157.090 23.879
2024-12-02-06:48:53-root-INFO: grad norm: 152.939 151.428 21.442
2024-12-02-06:48:53-root-INFO: grad norm: 145.183 143.469 22.248
2024-12-02-06:48:54-root-INFO: grad norm: 141.734 140.288 20.193
2024-12-02-06:48:54-root-INFO: grad norm: 137.409 135.747 21.306
2024-12-02-06:48:55-root-INFO: grad norm: 135.515 134.106 19.488
2024-12-02-06:48:55-root-INFO: grad norm: 133.486 131.860 20.768
2024-12-02-06:48:56-root-INFO: Loss Change: 1262.000 -> 1234.605
2024-12-02-06:48:56-root-INFO: Regularization Change: 0.000 -> 0.217
2024-12-02-06:48:56-root-INFO: Undo step: 165
2024-12-02-06:48:56-root-INFO: Undo step: 166
2024-12-02-06:48:56-root-INFO: Undo step: 167
2024-12-02-06:48:56-root-INFO: Undo step: 168
2024-12-02-06:48:56-root-INFO: Undo step: 169
2024-12-02-06:48:56-root-INFO: step: 170 lr_xt 0.00482333
2024-12-02-06:48:56-root-INFO: grad norm: 523.443 510.175 117.106
2024-12-02-06:48:56-root-INFO: grad norm: 426.856 419.004 81.496
2024-12-02-06:48:57-root-INFO: grad norm: 404.633 399.448 64.571
2024-12-02-06:48:57-root-INFO: Loss too large (1749.928->1913.394)! Learning rate decreased to 0.00386.
2024-12-02-06:48:57-root-INFO: Loss too large (1749.928->1757.660)! Learning rate decreased to 0.00309.
2024-12-02-06:48:58-root-INFO: grad norm: 405.572 402.219 52.046
2024-12-02-06:48:58-root-INFO: grad norm: 293.069 289.330 46.664
2024-12-02-06:48:59-root-INFO: grad norm: 243.361 240.009 40.249
2024-12-02-06:48:59-root-INFO: grad norm: 345.075 341.435 49.993
2024-12-02-06:48:59-root-INFO: Loss too large (1485.525->1633.535)! Learning rate decreased to 0.00247.
2024-12-02-06:48:59-root-INFO: Loss too large (1485.525->1554.270)! Learning rate decreased to 0.00198.
2024-12-02-06:49:00-root-INFO: Loss too large (1485.525->1498.312)! Learning rate decreased to 0.00158.
2024-12-02-06:49:00-root-INFO: grad norm: 308.038 305.905 36.183
2024-12-02-06:49:00-root-INFO: Loss Change: 2206.336 -> 1437.965
2024-12-02-06:49:00-root-INFO: Regularization Change: 0.000 -> 11.032
2024-12-02-06:49:00-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-02-06:49:00-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:49:01-root-INFO: step: 169 lr_xt 0.00501730
2024-12-02-06:49:01-root-INFO: grad norm: 226.082 223.283 35.461
2024-12-02-06:49:01-root-INFO: Loss too large (1415.459->1643.863)! Learning rate decreased to 0.00401.
2024-12-02-06:49:01-root-INFO: Loss too large (1415.459->1564.547)! Learning rate decreased to 0.00321.
2024-12-02-06:49:01-root-INFO: Loss too large (1415.459->1502.993)! Learning rate decreased to 0.00257.
2024-12-02-06:49:01-root-INFO: Loss too large (1415.459->1457.608)! Learning rate decreased to 0.00206.
2024-12-02-06:49:01-root-INFO: Loss too large (1415.459->1426.506)! Learning rate decreased to 0.00164.
2024-12-02-06:49:02-root-INFO: grad norm: 231.495 229.430 30.845
2024-12-02-06:49:02-root-INFO: grad norm: 253.508 250.750 37.288
2024-12-02-06:49:03-root-INFO: Loss too large (1398.048->1398.171)! Learning rate decreased to 0.00132.
2024-12-02-06:49:03-root-INFO: grad norm: 196.811 194.804 28.031
2024-12-02-06:49:04-root-INFO: grad norm: 165.125 162.459 29.556
2024-12-02-06:49:04-root-INFO: grad norm: 155.594 153.580 24.952
2024-12-02-06:49:05-root-INFO: grad norm: 153.713 151.143 27.994
2024-12-02-06:49:05-root-INFO: grad norm: 157.335 155.421 24.467
2024-12-02-06:49:05-root-INFO: Loss Change: 1415.459 -> 1341.619
2024-12-02-06:49:06-root-INFO: Regularization Change: 0.000 -> 0.676
2024-12-02-06:49:06-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-02-06:49:06-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00022.
2024-12-02-06:49:06-root-INFO: step: 168 lr_xt 0.00521823
2024-12-02-06:49:06-root-INFO: grad norm: 93.612 90.774 22.875
2024-12-02-06:49:06-root-INFO: Loss too large (1329.319->1359.123)! Learning rate decreased to 0.00417.
2024-12-02-06:49:06-root-INFO: Loss too large (1329.319->1343.741)! Learning rate decreased to 0.00334.
2024-12-02-06:49:06-root-INFO: Loss too large (1329.319->1333.997)! Learning rate decreased to 0.00267.
2024-12-02-06:49:07-root-INFO: grad norm: 190.945 189.281 25.150
2024-12-02-06:49:07-root-INFO: Loss too large (1328.243->1400.168)! Learning rate decreased to 0.00214.
2024-12-02-06:49:07-root-INFO: Loss too large (1328.243->1363.901)! Learning rate decreased to 0.00171.
2024-12-02-06:49:07-root-INFO: Loss too large (1328.243->1338.331)! Learning rate decreased to 0.00137.
2024-12-02-06:49:08-root-INFO: grad norm: 224.184 221.761 32.868
2024-12-02-06:49:08-root-INFO: Loss too large (1323.328->1327.449)! Learning rate decreased to 0.00109.
2024-12-02-06:49:08-root-INFO: grad norm: 177.121 175.465 24.164
2024-12-02-06:49:09-root-INFO: grad norm: 131.544 129.307 24.153
2024-12-02-06:49:10-root-INFO: grad norm: 114.723 112.969 19.985
2024-12-02-06:49:10-root-INFO: grad norm: 99.801 97.519 21.221
2024-12-02-06:49:11-root-INFO: grad norm: 92.031 90.147 18.528
2024-12-02-06:49:11-root-INFO: Loss Change: 1329.319 -> 1288.083
2024-12-02-06:49:11-root-INFO: Regularization Change: 0.000 -> 0.432
2024-12-02-06:49:11-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-02-06:49:11-root-INFO: Coefficient of regularization decay: 0.00022 -> 0.00023.
2024-12-02-06:49:11-root-INFO: step: 167 lr_xt 0.00542633
2024-12-02-06:49:11-root-INFO: grad norm: 80.802 78.060 20.873
2024-12-02-06:49:12-root-INFO: grad norm: 252.742 250.445 33.998
2024-12-02-06:49:12-root-INFO: Loss too large (1278.181->1627.464)! Learning rate decreased to 0.00434.
2024-12-02-06:49:12-root-INFO: Loss too large (1278.181->1523.109)! Learning rate decreased to 0.00347.
2024-12-02-06:49:12-root-INFO: Loss too large (1278.181->1444.592)! Learning rate decreased to 0.00278.
2024-12-02-06:49:13-root-INFO: Loss too large (1278.181->1385.273)! Learning rate decreased to 0.00222.
2024-12-02-06:49:13-root-INFO: Loss too large (1278.181->1341.025)! Learning rate decreased to 0.00178.
2024-12-02-06:49:13-root-INFO: Loss too large (1278.181->1309.113)! Learning rate decreased to 0.00142.
2024-12-02-06:49:13-root-INFO: Loss too large (1278.181->1287.298)! Learning rate decreased to 0.00114.
2024-12-02-06:49:14-root-INFO: grad norm: 215.154 213.627 25.593
2024-12-02-06:49:14-root-INFO: grad norm: 165.781 163.937 24.659
2024-12-02-06:49:15-root-INFO: grad norm: 151.894 150.435 20.998
2024-12-02-06:49:15-root-INFO: grad norm: 134.927 133.196 21.543
2024-12-02-06:49:16-root-INFO: grad norm: 127.561 126.117 19.135
2024-12-02-06:49:16-root-INFO: grad norm: 119.249 117.566 19.967
2024-12-02-06:49:17-root-INFO: Loss Change: 1279.606 -> 1240.059
2024-12-02-06:49:17-root-INFO: Regularization Change: 0.000 -> 0.563
2024-12-02-06:49:17-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-02-06:49:17-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:49:17-root-INFO: step: 166 lr_xt 0.00564182
2024-12-02-06:49:17-root-INFO: grad norm: 181.115 179.412 24.775
2024-12-02-06:49:17-root-INFO: Loss too large (1239.527->1472.383)! Learning rate decreased to 0.00451.
2024-12-02-06:49:17-root-INFO: Loss too large (1239.527->1429.576)! Learning rate decreased to 0.00361.
2024-12-02-06:49:18-root-INFO: Loss too large (1239.527->1381.002)! Learning rate decreased to 0.00289.
2024-12-02-06:49:18-root-INFO: Loss too large (1239.527->1329.982)! Learning rate decreased to 0.00231.
2024-12-02-06:49:18-root-INFO: Loss too large (1239.527->1285.491)! Learning rate decreased to 0.00185.
2024-12-02-06:49:18-root-INFO: Loss too large (1239.527->1254.602)! Learning rate decreased to 0.00148.
2024-12-02-06:49:19-root-INFO: grad norm: 244.797 242.671 32.195
2024-12-02-06:49:19-root-INFO: Loss too large (1237.019->1251.741)! Learning rate decreased to 0.00118.
2024-12-02-06:49:19-root-INFO: grad norm: 203.488 201.904 25.341
2024-12-02-06:49:20-root-INFO: grad norm: 144.185 142.506 21.945
2024-12-02-06:49:20-root-INFO: grad norm: 131.074 129.684 19.040
2024-12-02-06:49:21-root-INFO: grad norm: 115.382 113.787 19.123
2024-12-02-06:49:21-root-INFO: grad norm: 108.560 107.193 17.170
2024-12-02-06:49:22-root-INFO: grad norm: 101.074 99.509 17.717
2024-12-02-06:49:22-root-INFO: Loss Change: 1239.527 -> 1204.587
2024-12-02-06:49:22-root-INFO: Regularization Change: 0.000 -> 0.251
2024-12-02-06:49:22-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-02-06:49:22-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:49:22-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-06:49:22-root-INFO: grad norm: 157.596 156.030 22.155
2024-12-02-06:49:22-root-INFO: Loss too large (1206.960->1433.376)! Learning rate decreased to 0.00469.
2024-12-02-06:49:23-root-INFO: Loss too large (1206.960->1390.055)! Learning rate decreased to 0.00375.
2024-12-02-06:49:23-root-INFO: Loss too large (1206.960->1340.267)! Learning rate decreased to 0.00300.
2024-12-02-06:49:23-root-INFO: Loss too large (1206.960->1289.623)! Learning rate decreased to 0.00240.
2024-12-02-06:49:23-root-INFO: Loss too large (1206.960->1248.207)! Learning rate decreased to 0.00192.
2024-12-02-06:49:23-root-INFO: Loss too large (1206.960->1221.142)! Learning rate decreased to 0.00154.
2024-12-02-06:49:24-root-INFO: grad norm: 230.066 228.093 30.066
2024-12-02-06:49:24-root-INFO: Loss too large (1206.236->1221.924)! Learning rate decreased to 0.00123.
2024-12-02-06:49:24-root-INFO: Loss too large (1206.236->1207.797)! Learning rate decreased to 0.00098.
2024-12-02-06:49:24-root-INFO: grad norm: 148.084 146.689 20.282
2024-12-02-06:49:25-root-INFO: grad norm: 74.382 72.761 15.440
2024-12-02-06:49:25-root-INFO: grad norm: 59.476 57.841 13.851
2024-12-02-06:49:26-root-INFO: grad norm: 51.386 49.546 13.629
2024-12-02-06:49:26-root-INFO: grad norm: 48.325 46.502 13.148
2024-12-02-06:49:27-root-INFO: grad norm: 46.779 44.911 13.089
2024-12-02-06:49:27-root-INFO: Loss Change: 1206.960 -> 1178.194
2024-12-02-06:49:27-root-INFO: Regularization Change: 0.000 -> 0.179
2024-12-02-06:49:27-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-06:49:27-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:49:28-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-06:49:28-root-INFO: grad norm: 105.939 104.096 19.674
2024-12-02-06:49:28-root-INFO: Loss too large (1177.088->1330.364)! Learning rate decreased to 0.00488.
2024-12-02-06:49:28-root-INFO: Loss too large (1177.088->1280.386)! Learning rate decreased to 0.00390.
2024-12-02-06:49:28-root-INFO: Loss too large (1177.088->1237.601)! Learning rate decreased to 0.00312.
2024-12-02-06:49:28-root-INFO: Loss too large (1177.088->1207.549)! Learning rate decreased to 0.00250.
2024-12-02-06:49:28-root-INFO: Loss too large (1177.088->1189.361)! Learning rate decreased to 0.00200.
2024-12-02-06:49:29-root-INFO: Loss too large (1177.088->1179.387)! Learning rate decreased to 0.00160.
2024-12-02-06:49:29-root-INFO: grad norm: 150.687 148.949 22.814
2024-12-02-06:49:29-root-INFO: Loss too large (1174.372->1179.834)! Learning rate decreased to 0.00128.
2024-12-02-06:49:30-root-INFO: grad norm: 144.002 142.547 20.415
2024-12-02-06:49:30-root-INFO: grad norm: 133.023 131.477 20.221
2024-12-02-06:49:31-root-INFO: grad norm: 128.510 127.162 18.562
2024-12-02-06:49:31-root-INFO: grad norm: 121.729 120.294 18.635
2024-12-02-06:49:32-root-INFO: grad norm: 118.711 117.423 17.441
2024-12-02-06:49:32-root-INFO: grad norm: 114.558 113.191 17.649
2024-12-02-06:49:32-root-INFO: Loss Change: 1177.088 -> 1154.040
2024-12-02-06:49:32-root-INFO: Regularization Change: 0.000 -> 0.252
2024-12-02-06:49:32-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-06:49:32-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-06:49:32-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-06:49:33-root-INFO: grad norm: 154.407 153.001 20.785
2024-12-02-06:49:33-root-INFO: Loss too large (1153.605->1393.432)! Learning rate decreased to 0.00507.
2024-12-02-06:49:33-root-INFO: Loss too large (1153.605->1351.885)! Learning rate decreased to 0.00405.
2024-12-02-06:49:33-root-INFO: Loss too large (1153.605->1301.382)! Learning rate decreased to 0.00324.
2024-12-02-06:49:33-root-INFO: Loss too large (1153.605->1246.225)! Learning rate decreased to 0.00259.
2024-12-02-06:49:33-root-INFO: Loss too large (1153.605->1199.279)! Learning rate decreased to 0.00208.
2024-12-02-06:49:34-root-INFO: Loss too large (1153.605->1168.785)! Learning rate decreased to 0.00166.
2024-12-02-06:49:34-root-INFO: grad norm: 226.392 224.528 28.991
2024-12-02-06:49:34-root-INFO: Loss too large (1152.466->1169.964)! Learning rate decreased to 0.00133.
2024-12-02-06:49:34-root-INFO: Loss too large (1152.466->1155.493)! Learning rate decreased to 0.00106.
2024-12-02-06:49:35-root-INFO: grad norm: 145.186 143.905 19.241
2024-12-02-06:49:35-root-INFO: grad norm: 66.222 64.763 13.828
2024-12-02-06:49:36-root-INFO: grad norm: 53.439 51.958 12.496
2024-12-02-06:49:36-root-INFO: grad norm: 46.549 44.893 12.307
2024-12-02-06:49:37-root-INFO: grad norm: 43.999 42.353 11.921
2024-12-02-06:49:37-root-INFO: grad norm: 42.731 41.048 11.875
2024-12-02-06:49:38-root-INFO: Loss Change: 1153.605 -> 1125.940
2024-12-02-06:49:38-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-02-06:49:38-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-06:49:38-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:49:38-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-06:49:38-root-INFO: grad norm: 114.612 112.653 21.099
2024-12-02-06:49:38-root-INFO: Loss too large (1132.989->1315.637)! Learning rate decreased to 0.00527.
2024-12-02-06:49:38-root-INFO: Loss too large (1132.989->1258.383)! Learning rate decreased to 0.00421.
2024-12-02-06:49:38-root-INFO: Loss too large (1132.989->1206.308)! Learning rate decreased to 0.00337.
2024-12-02-06:49:38-root-INFO: Loss too large (1132.989->1168.793)! Learning rate decreased to 0.00270.
2024-12-02-06:49:39-root-INFO: Loss too large (1132.989->1146.325)! Learning rate decreased to 0.00216.
2024-12-02-06:49:39-root-INFO: Loss too large (1132.989->1134.321)! Learning rate decreased to 0.00173.
2024-12-02-06:49:39-root-INFO: grad norm: 154.457 152.813 22.478
2024-12-02-06:49:39-root-INFO: Loss too large (1128.490->1134.792)! Learning rate decreased to 0.00138.
2024-12-02-06:49:40-root-INFO: grad norm: 143.383 141.891 20.629
2024-12-02-06:49:40-root-INFO: grad norm: 124.711 123.298 18.721
2024-12-02-06:49:41-root-INFO: grad norm: 118.828 117.493 17.765
2024-12-02-06:49:41-root-INFO: grad norm: 109.860 108.570 16.787
2024-12-02-06:49:42-root-INFO: grad norm: 106.130 104.880 16.239
2024-12-02-06:49:42-root-INFO: grad norm: 100.883 99.665 15.631
2024-12-02-06:49:43-root-INFO: Loss Change: 1132.989 -> 1106.415
2024-12-02-06:49:43-root-INFO: Regularization Change: 0.000 -> 0.286
2024-12-02-06:49:43-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-06:49:43-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:49:43-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-06:49:43-root-INFO: grad norm: 137.211 135.889 19.005
2024-12-02-06:49:43-root-INFO: Loss too large (1107.561->1345.212)! Learning rate decreased to 0.00547.
2024-12-02-06:49:43-root-INFO: Loss too large (1107.561->1300.152)! Learning rate decreased to 0.00438.
2024-12-02-06:49:44-root-INFO: Loss too large (1107.561->1244.815)! Learning rate decreased to 0.00350.
2024-12-02-06:49:44-root-INFO: Loss too large (1107.561->1187.452)! Learning rate decreased to 0.00280.
2024-12-02-06:49:44-root-INFO: Loss too large (1107.561->1143.452)! Learning rate decreased to 0.00224.
2024-12-02-06:49:44-root-INFO: Loss too large (1107.561->1117.552)! Learning rate decreased to 0.00179.
2024-12-02-06:49:45-root-INFO: grad norm: 189.453 187.789 25.051
2024-12-02-06:49:45-root-INFO: Loss too large (1104.583->1116.324)! Learning rate decreased to 0.00143.
2024-12-02-06:49:45-root-INFO: Loss too large (1104.583->1105.707)! Learning rate decreased to 0.00115.
2024-12-02-06:49:45-root-INFO: grad norm: 119.937 118.785 16.580
2024-12-02-06:49:46-root-INFO: grad norm: 56.429 55.001 12.614
2024-12-02-06:49:46-root-INFO: grad norm: 46.975 45.549 11.488
2024-12-02-06:49:47-root-INFO: grad norm: 42.529 40.975 11.393
2024-12-02-06:49:47-root-INFO: grad norm: 40.946 39.422 11.070
2024-12-02-06:49:48-root-INFO: grad norm: 40.146 38.604 11.020
2024-12-02-06:49:48-root-INFO: Loss Change: 1107.561 -> 1081.483
2024-12-02-06:49:48-root-INFO: Regularization Change: 0.000 -> 0.186
2024-12-02-06:49:48-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-06:49:48-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:49:48-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-06:49:48-root-INFO: grad norm: 92.991 91.566 16.219
2024-12-02-06:49:49-root-INFO: Loss too large (1084.531->1240.318)! Learning rate decreased to 0.00568.
2024-12-02-06:49:49-root-INFO: Loss too large (1084.531->1183.868)! Learning rate decreased to 0.00455.
2024-12-02-06:49:49-root-INFO: Loss too large (1084.531->1138.510)! Learning rate decreased to 0.00364.
2024-12-02-06:49:49-root-INFO: Loss too large (1084.531->1109.522)! Learning rate decreased to 0.00291.
2024-12-02-06:49:49-root-INFO: Loss too large (1084.531->1093.344)! Learning rate decreased to 0.00233.
2024-12-02-06:49:49-root-INFO: Loss too large (1084.531->1084.991)! Learning rate decreased to 0.00186.
2024-12-02-06:49:50-root-INFO: grad norm: 122.511 121.101 18.533
2024-12-02-06:49:50-root-INFO: Loss too large (1081.020->1083.899)! Learning rate decreased to 0.00149.
2024-12-02-06:49:50-root-INFO: grad norm: 111.963 110.791 16.157
2024-12-02-06:49:51-root-INFO: grad norm: 95.776 94.521 15.454
2024-12-02-06:49:51-root-INFO: grad norm: 90.005 88.907 14.020
2024-12-02-06:49:52-root-INFO: grad norm: 82.109 80.923 13.907
2024-12-02-06:49:53-root-INFO: grad norm: 78.441 77.367 12.937
2024-12-02-06:49:53-root-INFO: grad norm: 73.819 72.666 12.995
2024-12-02-06:49:53-root-INFO: Loss Change: 1084.531 -> 1061.224
2024-12-02-06:49:53-root-INFO: Regularization Change: 0.000 -> 0.266
2024-12-02-06:49:53-root-INFO: Undo step: 160
2024-12-02-06:49:53-root-INFO: Undo step: 161
2024-12-02-06:49:53-root-INFO: Undo step: 162
2024-12-02-06:49:53-root-INFO: Undo step: 163
2024-12-02-06:49:53-root-INFO: Undo step: 164
2024-12-02-06:49:54-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-06:49:54-root-INFO: grad norm: 312.540 294.578 104.426
2024-12-02-06:49:54-root-INFO: grad norm: 336.672 330.306 65.164
2024-12-02-06:49:54-root-INFO: Loss too large (1508.318->1530.647)! Learning rate decreased to 0.00469.
2024-12-02-06:49:55-root-INFO: grad norm: 282.686 275.446 63.566
2024-12-02-06:49:55-root-INFO: grad norm: 257.952 253.908 45.498
2024-12-02-06:49:55-root-INFO: Loss too large (1310.420->1365.002)! Learning rate decreased to 0.00375.
2024-12-02-06:49:56-root-INFO: grad norm: 389.816 384.143 66.261
2024-12-02-06:49:56-root-INFO: Loss too large (1294.406->1398.957)! Learning rate decreased to 0.00300.
2024-12-02-06:49:56-root-INFO: Loss too large (1294.406->1320.861)! Learning rate decreased to 0.00240.
2024-12-02-06:49:57-root-INFO: grad norm: 247.504 244.643 37.518
2024-12-02-06:49:57-root-INFO: grad norm: 100.680 97.577 24.806
2024-12-02-06:49:58-root-INFO: grad norm: 99.745 96.881 23.730
2024-12-02-06:49:58-root-INFO: Loss Change: 1756.582 -> 1154.283
2024-12-02-06:49:58-root-INFO: Regularization Change: 0.000 -> 12.470
2024-12-02-06:49:58-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-06:49:58-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:49:58-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-06:49:58-root-INFO: grad norm: 80.380 77.697 20.593
2024-12-02-06:49:59-root-INFO: grad norm: 164.030 162.447 22.737
2024-12-02-06:49:59-root-INFO: Loss too large (1130.365->1315.776)! Learning rate decreased to 0.00488.
2024-12-02-06:49:59-root-INFO: Loss too large (1130.365->1266.088)! Learning rate decreased to 0.00390.
2024-12-02-06:49:59-root-INFO: Loss too large (1130.365->1211.675)! Learning rate decreased to 0.00312.
2024-12-02-06:50:00-root-INFO: Loss too large (1130.365->1164.496)! Learning rate decreased to 0.00250.
2024-12-02-06:50:00-root-INFO: Loss too large (1130.365->1134.190)! Learning rate decreased to 0.00200.
2024-12-02-06:50:00-root-INFO: grad norm: 176.160 174.115 26.767
2024-12-02-06:50:00-root-INFO: Loss too large (1118.928->1123.337)! Learning rate decreased to 0.00160.
2024-12-02-06:50:01-root-INFO: grad norm: 138.790 137.290 20.353
2024-12-02-06:50:01-root-INFO: grad norm: 95.516 93.834 17.849
2024-12-02-06:50:02-root-INFO: grad norm: 83.690 82.137 16.051
2024-12-02-06:50:02-root-INFO: grad norm: 73.062 71.372 15.627
2024-12-02-06:50:03-root-INFO: grad norm: 68.152 66.556 14.663
2024-12-02-06:50:03-root-INFO: Loss Change: 1139.444 -> 1085.738
2024-12-02-06:50:03-root-INFO: Regularization Change: 0.000 -> 0.921
2024-12-02-06:50:03-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-06:50:03-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-06:50:03-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-06:50:03-root-INFO: grad norm: 55.003 53.069 14.455
2024-12-02-06:50:04-root-INFO: grad norm: 63.896 62.266 14.342
2024-12-02-06:50:04-root-INFO: Loss too large (1064.830->1066.248)! Learning rate decreased to 0.00507.
2024-12-02-06:50:04-root-INFO: grad norm: 130.308 129.063 17.973
2024-12-02-06:50:05-root-INFO: Loss too large (1061.666->1197.366)! Learning rate decreased to 0.00405.
2024-12-02-06:50:05-root-INFO: Loss too large (1061.666->1143.448)! Learning rate decreased to 0.00324.
2024-12-02-06:50:05-root-INFO: Loss too large (1061.666->1101.688)! Learning rate decreased to 0.00259.
2024-12-02-06:50:05-root-INFO: Loss too large (1061.666->1075.747)! Learning rate decreased to 0.00208.
2024-12-02-06:50:05-root-INFO: Loss too large (1061.666->1061.803)! Learning rate decreased to 0.00166.
2024-12-02-06:50:06-root-INFO: grad norm: 126.766 125.411 18.487
2024-12-02-06:50:06-root-INFO: grad norm: 126.883 125.661 17.569
2024-12-02-06:50:07-root-INFO: grad norm: 127.737 126.379 18.579
2024-12-02-06:50:07-root-INFO: grad norm: 128.077 126.854 17.663
2024-12-02-06:50:08-root-INFO: grad norm: 129.004 127.670 18.506
2024-12-02-06:50:08-root-INFO: Loss too large (1044.233->1044.239)! Learning rate decreased to 0.00133.
2024-12-02-06:50:08-root-INFO: Loss Change: 1077.371 -> 1040.464
2024-12-02-06:50:08-root-INFO: Regularization Change: 0.000 -> 0.838
2024-12-02-06:50:08-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-06:50:09-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:50:09-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-06:50:09-root-INFO: grad norm: 168.884 166.990 25.223
2024-12-02-06:50:09-root-INFO: Loss too large (1051.356->1287.073)! Learning rate decreased to 0.00527.
2024-12-02-06:50:09-root-INFO: Loss too large (1051.356->1234.798)! Learning rate decreased to 0.00421.
2024-12-02-06:50:09-root-INFO: Loss too large (1051.356->1174.756)! Learning rate decreased to 0.00337.
2024-12-02-06:50:09-root-INFO: Loss too large (1051.356->1116.305)! Learning rate decreased to 0.00270.
2024-12-02-06:50:10-root-INFO: Loss too large (1051.356->1073.514)! Learning rate decreased to 0.00216.
2024-12-02-06:50:10-root-INFO: grad norm: 249.193 246.983 33.118
2024-12-02-06:50:10-root-INFO: Loss too large (1049.348->1077.189)! Learning rate decreased to 0.00173.
2024-12-02-06:50:10-root-INFO: Loss too large (1049.348->1055.718)! Learning rate decreased to 0.00138.
2024-12-02-06:50:11-root-INFO: grad norm: 156.860 155.365 21.601
2024-12-02-06:50:11-root-INFO: grad norm: 62.048 60.690 12.912
2024-12-02-06:50:12-root-INFO: grad norm: 50.144 48.600 12.347
2024-12-02-06:50:12-root-INFO: grad norm: 43.779 42.284 11.341
2024-12-02-06:50:13-root-INFO: grad norm: 41.154 39.611 11.165
2024-12-02-06:50:13-root-INFO: grad norm: 39.702 38.220 10.747
2024-12-02-06:50:14-root-INFO: Loss Change: 1051.356 -> 1013.852
2024-12-02-06:50:14-root-INFO: Regularization Change: 0.000 -> 0.286
2024-12-02-06:50:14-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-06:50:14-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:50:14-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-06:50:14-root-INFO: grad norm: 69.187 67.785 13.858
2024-12-02-06:50:14-root-INFO: Loss too large (1010.269->1055.655)! Learning rate decreased to 0.00547.
2024-12-02-06:50:14-root-INFO: Loss too large (1010.269->1032.281)! Learning rate decreased to 0.00438.
2024-12-02-06:50:15-root-INFO: Loss too large (1010.269->1018.965)! Learning rate decreased to 0.00350.
2024-12-02-06:50:15-root-INFO: Loss too large (1010.269->1011.817)! Learning rate decreased to 0.00280.
2024-12-02-06:50:15-root-INFO: grad norm: 119.273 117.986 17.472
2024-12-02-06:50:15-root-INFO: Loss too large (1008.231->1022.280)! Learning rate decreased to 0.00224.
2024-12-02-06:50:16-root-INFO: Loss too large (1008.231->1013.051)! Learning rate decreased to 0.00179.
2024-12-02-06:50:16-root-INFO: grad norm: 114.632 113.514 15.965
2024-12-02-06:50:17-root-INFO: grad norm: 107.389 106.227 15.752
2024-12-02-06:50:17-root-INFO: grad norm: 104.382 103.294 15.029
2024-12-02-06:50:17-root-INFO: grad norm: 100.350 99.252 14.808
2024-12-02-06:50:18-root-INFO: grad norm: 98.841 97.769 14.517
2024-12-02-06:50:18-root-INFO: grad norm: 97.317 96.251 14.360
2024-12-02-06:50:19-root-INFO: Loss Change: 1010.269 -> 990.668
2024-12-02-06:50:19-root-INFO: Regularization Change: 0.000 -> 0.341
2024-12-02-06:50:19-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-06:50:19-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:50:19-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-06:50:19-root-INFO: grad norm: 150.827 149.350 21.051
2024-12-02-06:50:19-root-INFO: Loss too large (998.932->1229.534)! Learning rate decreased to 0.00568.
2024-12-02-06:50:19-root-INFO: Loss too large (998.932->1176.165)! Learning rate decreased to 0.00455.
2024-12-02-06:50:20-root-INFO: Loss too large (998.932->1114.191)! Learning rate decreased to 0.00364.
2024-12-02-06:50:20-root-INFO: Loss too large (998.932->1056.469)! Learning rate decreased to 0.00291.
2024-12-02-06:50:20-root-INFO: Loss too large (998.932->1017.170)! Learning rate decreased to 0.00233.
2024-12-02-06:50:20-root-INFO: grad norm: 213.108 211.255 28.043
2024-12-02-06:50:21-root-INFO: Loss too large (996.133->1016.954)! Learning rate decreased to 0.00186.
2024-12-02-06:50:21-root-INFO: Loss too large (996.133->1000.016)! Learning rate decreased to 0.00149.
2024-12-02-06:50:21-root-INFO: grad norm: 131.271 130.030 18.003
2024-12-02-06:50:22-root-INFO: grad norm: 52.460 51.323 10.861
2024-12-02-06:50:22-root-INFO: grad norm: 42.622 41.322 10.443
2024-12-02-06:50:23-root-INFO: grad norm: 37.872 36.589 9.774
2024-12-02-06:50:23-root-INFO: grad norm: 35.986 34.656 9.692
2024-12-02-06:50:24-root-INFO: grad norm: 34.974 33.674 9.446
2024-12-02-06:50:24-root-INFO: Loss Change: 998.932 -> 967.027
2024-12-02-06:50:24-root-INFO: Regularization Change: 0.000 -> 0.256
2024-12-02-06:50:24-root-INFO: Undo step: 160
2024-12-02-06:50:24-root-INFO: Undo step: 161
2024-12-02-06:50:24-root-INFO: Undo step: 162
2024-12-02-06:50:24-root-INFO: Undo step: 163
2024-12-02-06:50:24-root-INFO: Undo step: 164
2024-12-02-06:50:24-root-INFO: step: 165 lr_xt 0.00586491
2024-12-02-06:50:24-root-INFO: grad norm: 413.845 399.768 107.017
2024-12-02-06:50:25-root-INFO: grad norm: 323.289 317.055 63.178
2024-12-02-06:50:25-root-INFO: Loss too large (1396.733->1453.731)! Learning rate decreased to 0.00469.
2024-12-02-06:50:25-root-INFO: grad norm: 517.038 505.490 108.661
2024-12-02-06:50:26-root-INFO: Loss too large (1358.802->1857.470)! Learning rate decreased to 0.00375.
2024-12-02-06:50:26-root-INFO: Loss too large (1358.802->1655.364)! Learning rate decreased to 0.00300.
2024-12-02-06:50:26-root-INFO: Loss too large (1358.802->1515.007)! Learning rate decreased to 0.00240.
2024-12-02-06:50:26-root-INFO: Loss too large (1358.802->1418.485)! Learning rate decreased to 0.00192.
2024-12-02-06:50:26-root-INFO: grad norm: 348.669 339.372 79.979
2024-12-02-06:50:27-root-INFO: grad norm: 99.324 95.018 28.928
2024-12-02-06:50:27-root-INFO: grad norm: 107.793 103.067 31.566
2024-12-02-06:50:28-root-INFO: grad norm: 167.172 162.330 39.944
2024-12-02-06:50:28-root-INFO: Loss too large (1170.311->1177.621)! Learning rate decreased to 0.00154.
2024-12-02-06:50:29-root-INFO: grad norm: 182.273 176.534 45.380
2024-12-02-06:50:29-root-INFO: Loss Change: 1765.615 -> 1158.916
2024-12-02-06:50:29-root-INFO: Regularization Change: 0.000 -> 9.483
2024-12-02-06:50:29-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-02-06:50:29-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00023.
2024-12-02-06:50:29-root-INFO: step: 164 lr_xt 0.00609585
2024-12-02-06:50:29-root-INFO: grad norm: 124.977 121.762 28.163
2024-12-02-06:50:29-root-INFO: Loss too large (1142.752->1220.727)! Learning rate decreased to 0.00488.
2024-12-02-06:50:30-root-INFO: Loss too large (1142.752->1190.047)! Learning rate decreased to 0.00390.
2024-12-02-06:50:30-root-INFO: Loss too large (1142.752->1168.500)! Learning rate decreased to 0.00312.
2024-12-02-06:50:30-root-INFO: Loss too large (1142.752->1153.925)! Learning rate decreased to 0.00250.
2024-12-02-06:50:30-root-INFO: Loss too large (1142.752->1144.572)! Learning rate decreased to 0.00200.
2024-12-02-06:50:31-root-INFO: grad norm: 162.845 157.396 41.774
2024-12-02-06:50:31-root-INFO: Loss too large (1139.003->1143.226)! Learning rate decreased to 0.00160.
2024-12-02-06:50:31-root-INFO: grad norm: 191.417 186.474 43.223
2024-12-02-06:50:31-root-INFO: Loss too large (1130.968->1134.286)! Learning rate decreased to 0.00128.
2024-12-02-06:50:32-root-INFO: grad norm: 154.696 149.663 39.140
2024-12-02-06:50:32-root-INFO: grad norm: 109.660 106.588 25.771
2024-12-02-06:50:33-root-INFO: grad norm: 96.471 92.753 26.527
2024-12-02-06:50:33-root-INFO: grad norm: 82.742 80.151 20.543
2024-12-02-06:50:34-root-INFO: grad norm: 75.420 72.230 21.706
2024-12-02-06:50:34-root-INFO: Loss Change: 1142.752 -> 1095.475
2024-12-02-06:50:34-root-INFO: Regularization Change: 0.000 -> 0.498
2024-12-02-06:50:34-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-02-06:50:34-root-INFO: Coefficient of regularization decay: 0.00023 -> 0.00024.
2024-12-02-06:50:34-root-INFO: step: 163 lr_xt 0.00633485
2024-12-02-06:50:34-root-INFO: grad norm: 65.905 63.026 19.265
2024-12-02-06:50:35-root-INFO: grad norm: 222.592 216.671 50.998
2024-12-02-06:50:35-root-INFO: Loss too large (1082.952->1380.584)! Learning rate decreased to 0.00507.
2024-12-02-06:50:35-root-INFO: Loss too large (1082.952->1289.224)! Learning rate decreased to 0.00405.
2024-12-02-06:50:35-root-INFO: Loss too large (1082.952->1221.707)! Learning rate decreased to 0.00324.
2024-12-02-06:50:35-root-INFO: Loss too large (1082.952->1171.611)! Learning rate decreased to 0.00259.
2024-12-02-06:50:36-root-INFO: Loss too large (1082.952->1134.723)! Learning rate decreased to 0.00208.
2024-12-02-06:50:36-root-INFO: Loss too large (1082.952->1108.223)! Learning rate decreased to 0.00166.
2024-12-02-06:50:36-root-INFO: Loss too large (1082.952->1090.016)! Learning rate decreased to 0.00133.
2024-12-02-06:50:36-root-INFO: grad norm: 165.571 161.347 37.159
2024-12-02-06:50:37-root-INFO: grad norm: 95.423 92.865 21.944
2024-12-02-06:50:37-root-INFO: grad norm: 81.831 79.074 21.061
2024-12-02-06:50:38-root-INFO: grad norm: 68.593 66.597 16.426
2024-12-02-06:50:38-root-INFO: grad norm: 61.920 59.545 16.984
2024-12-02-06:50:39-root-INFO: grad norm: 55.971 54.142 14.190
2024-12-02-06:50:39-root-INFO: Loss Change: 1086.419 -> 1047.906
2024-12-02-06:50:39-root-INFO: Regularization Change: 0.000 -> 0.601
2024-12-02-06:50:39-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-02-06:50:39-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:50:39-root-INFO: step: 162 lr_xt 0.00658217
2024-12-02-06:50:39-root-INFO: grad norm: 132.442 127.577 35.566
2024-12-02-06:50:40-root-INFO: Loss too large (1056.286->1247.895)! Learning rate decreased to 0.00527.
2024-12-02-06:50:40-root-INFO: Loss too large (1056.286->1189.732)! Learning rate decreased to 0.00421.
2024-12-02-06:50:40-root-INFO: Loss too large (1056.286->1135.651)! Learning rate decreased to 0.00337.
2024-12-02-06:50:40-root-INFO: Loss too large (1056.286->1094.837)! Learning rate decreased to 0.00270.
2024-12-02-06:50:40-root-INFO: Loss too large (1056.286->1069.251)! Learning rate decreased to 0.00216.
2024-12-02-06:50:41-root-INFO: grad norm: 219.692 214.367 48.079
2024-12-02-06:50:41-root-INFO: Loss too large (1055.276->1082.773)! Learning rate decreased to 0.00173.
2024-12-02-06:50:41-root-INFO: Loss too large (1055.276->1064.156)! Learning rate decreased to 0.00138.
2024-12-02-06:50:42-root-INFO: grad norm: 160.800 156.188 38.238
2024-12-02-06:50:42-root-INFO: grad norm: 86.053 83.815 19.497
2024-12-02-06:50:43-root-INFO: grad norm: 72.741 69.911 20.093
2024-12-02-06:50:43-root-INFO: grad norm: 60.326 58.480 14.810
2024-12-02-06:50:44-root-INFO: grad norm: 54.098 51.734 15.818
2024-12-02-06:50:44-root-INFO: grad norm: 49.028 47.280 12.976
2024-12-02-06:50:44-root-INFO: Loss Change: 1056.286 -> 1022.685
2024-12-02-06:50:44-root-INFO: Regularization Change: 0.000 -> 0.315
2024-12-02-06:50:44-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-02-06:50:44-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:50:45-root-INFO: step: 161 lr_xt 0.00683803
2024-12-02-06:50:45-root-INFO: grad norm: 91.551 88.287 24.227
2024-12-02-06:50:45-root-INFO: Loss too large (1020.550->1137.458)! Learning rate decreased to 0.00547.
2024-12-02-06:50:45-root-INFO: Loss too large (1020.550->1089.951)! Learning rate decreased to 0.00438.
2024-12-02-06:50:45-root-INFO: Loss too large (1020.550->1056.239)! Learning rate decreased to 0.00350.
2024-12-02-06:50:45-root-INFO: Loss too large (1020.550->1035.645)! Learning rate decreased to 0.00280.
2024-12-02-06:50:46-root-INFO: Loss too large (1020.550->1024.247)! Learning rate decreased to 0.00224.
2024-12-02-06:50:46-root-INFO: grad norm: 139.008 135.755 29.900
2024-12-02-06:50:46-root-INFO: Loss too large (1018.441->1027.252)! Learning rate decreased to 0.00179.
2024-12-02-06:50:46-root-INFO: Loss too large (1018.441->1019.302)! Learning rate decreased to 0.00143.
2024-12-02-06:50:47-root-INFO: grad norm: 103.114 100.075 24.850
2024-12-02-06:50:47-root-INFO: grad norm: 66.342 64.591 15.141
2024-12-02-06:50:48-root-INFO: grad norm: 55.426 53.353 15.018
2024-12-02-06:50:48-root-INFO: grad norm: 47.273 45.725 11.999
2024-12-02-06:50:49-root-INFO: grad norm: 43.475 41.680 12.365
2024-12-02-06:50:49-root-INFO: grad norm: 40.984 39.443 11.133
2024-12-02-06:50:50-root-INFO: Loss Change: 1020.550 -> 996.206
2024-12-02-06:50:50-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-06:50:50-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-02-06:50:50-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:50:50-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-06:50:50-root-INFO: grad norm: 99.468 96.093 25.689
2024-12-02-06:50:50-root-INFO: Loss too large (999.974->1136.716)! Learning rate decreased to 0.00568.
2024-12-02-06:50:50-root-INFO: Loss too large (999.974->1083.099)! Learning rate decreased to 0.00455.
2024-12-02-06:50:50-root-INFO: Loss too large (999.974->1043.225)! Learning rate decreased to 0.00364.
2024-12-02-06:50:51-root-INFO: Loss too large (999.974->1018.275)! Learning rate decreased to 0.00291.
2024-12-02-06:50:51-root-INFO: Loss too large (999.974->1004.355)! Learning rate decreased to 0.00233.
2024-12-02-06:50:51-root-INFO: grad norm: 146.721 143.268 31.644
2024-12-02-06:50:51-root-INFO: Loss too large (997.263->1007.505)! Learning rate decreased to 0.00186.
2024-12-02-06:50:52-root-INFO: Loss too large (997.263->998.552)! Learning rate decreased to 0.00149.
2024-12-02-06:50:52-root-INFO: grad norm: 105.268 102.262 24.978
2024-12-02-06:50:53-root-INFO: grad norm: 62.472 60.795 14.378
2024-12-02-06:50:53-root-INFO: grad norm: 51.528 49.543 14.165
2024-12-02-06:50:54-root-INFO: grad norm: 43.876 42.387 11.333
2024-12-02-06:50:54-root-INFO: grad norm: 40.485 38.771 11.656
2024-12-02-06:50:55-root-INFO: grad norm: 38.412 36.928 10.574
2024-12-02-06:50:55-root-INFO: Loss Change: 999.974 -> 974.617
2024-12-02-06:50:55-root-INFO: Regularization Change: 0.000 -> 0.271
2024-12-02-06:50:55-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-06:50:55-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:50:55-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-06:50:55-root-INFO: grad norm: 69.299 66.979 17.780
2024-12-02-06:50:55-root-INFO: Loss too large (972.735->1031.321)! Learning rate decreased to 0.00590.
2024-12-02-06:50:56-root-INFO: Loss too large (972.735->1002.690)! Learning rate decreased to 0.00472.
2024-12-02-06:50:56-root-INFO: Loss too large (972.735->985.830)! Learning rate decreased to 0.00378.
2024-12-02-06:50:56-root-INFO: Loss too large (972.735->976.560)! Learning rate decreased to 0.00302.
2024-12-02-06:50:56-root-INFO: grad norm: 130.152 127.120 27.932
2024-12-02-06:50:57-root-INFO: Loss too large (971.777->990.771)! Learning rate decreased to 0.00242.
2024-12-02-06:50:57-root-INFO: Loss too large (971.777->979.482)! Learning rate decreased to 0.00193.
2024-12-02-06:50:57-root-INFO: Loss too large (971.777->972.236)! Learning rate decreased to 0.00155.
2024-12-02-06:50:57-root-INFO: grad norm: 91.565 89.085 21.169
2024-12-02-06:50:58-root-INFO: grad norm: 54.525 53.006 12.781
2024-12-02-06:50:58-root-INFO: grad norm: 45.217 43.496 12.356
2024-12-02-06:50:59-root-INFO: grad norm: 39.348 37.920 10.507
2024-12-02-06:50:59-root-INFO: grad norm: 36.887 35.316 10.648
2024-12-02-06:51:00-root-INFO: grad norm: 35.480 34.030 10.040
2024-12-02-06:51:00-root-INFO: Loss Change: 972.735 -> 952.040
2024-12-02-06:51:00-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-02-06:51:00-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-06:51:00-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-06:51:00-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-06:51:01-root-INFO: grad norm: 84.519 81.685 21.702
2024-12-02-06:51:01-root-INFO: Loss too large (954.301->1045.954)! Learning rate decreased to 0.00613.
2024-12-02-06:51:01-root-INFO: Loss too large (954.301->1003.079)! Learning rate decreased to 0.00490.
2024-12-02-06:51:01-root-INFO: Loss too large (954.301->976.367)! Learning rate decreased to 0.00392.
2024-12-02-06:51:01-root-INFO: Loss too large (954.301->961.328)! Learning rate decreased to 0.00314.
2024-12-02-06:51:02-root-INFO: grad norm: 154.796 151.316 32.638
2024-12-02-06:51:02-root-INFO: Loss too large (953.469->979.811)! Learning rate decreased to 0.00251.
2024-12-02-06:51:02-root-INFO: Loss too large (953.469->964.725)! Learning rate decreased to 0.00201.
2024-12-02-06:51:02-root-INFO: Loss too large (953.469->954.746)! Learning rate decreased to 0.00161.
2024-12-02-06:51:03-root-INFO: grad norm: 101.389 98.722 23.105
2024-12-02-06:51:03-root-INFO: grad norm: 49.898 48.442 11.967
2024-12-02-06:51:04-root-INFO: grad norm: 41.211 39.465 11.869
2024-12-02-06:51:04-root-INFO: grad norm: 36.518 35.065 10.198
2024-12-02-06:51:05-root-INFO: grad norm: 34.660 33.080 10.347
2024-12-02-06:51:05-root-INFO: grad norm: 33.653 32.183 9.838
2024-12-02-06:51:06-root-INFO: Loss Change: 954.301 -> 931.026
2024-12-02-06:51:06-root-INFO: Regularization Change: 0.000 -> 0.287
2024-12-02-06:51:06-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-06:51:06-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:51:06-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-06:51:06-root-INFO: grad norm: 87.183 84.115 22.925
2024-12-02-06:51:06-root-INFO: Loss too large (936.020->1051.850)! Learning rate decreased to 0.00636.
2024-12-02-06:51:06-root-INFO: Loss too large (936.020->1000.697)! Learning rate decreased to 0.00509.
2024-12-02-06:51:06-root-INFO: Loss too large (936.020->966.767)! Learning rate decreased to 0.00407.
2024-12-02-06:51:06-root-INFO: Loss too large (936.020->947.230)! Learning rate decreased to 0.00326.
2024-12-02-06:51:07-root-INFO: Loss too large (936.020->936.919)! Learning rate decreased to 0.00261.
2024-12-02-06:51:07-root-INFO: grad norm: 111.550 109.025 23.600
2024-12-02-06:51:07-root-INFO: Loss too large (931.931->935.524)! Learning rate decreased to 0.00208.
2024-12-02-06:51:08-root-INFO: grad norm: 98.671 95.897 23.233
2024-12-02-06:51:08-root-INFO: grad norm: 78.671 76.835 16.895
2024-12-02-06:51:09-root-INFO: grad norm: 72.009 69.756 17.871
2024-12-02-06:51:09-root-INFO: grad norm: 63.435 61.871 14.002
2024-12-02-06:51:10-root-INFO: grad norm: 59.285 57.318 15.145
2024-12-02-06:51:10-root-INFO: grad norm: 54.380 52.950 12.387
2024-12-02-06:51:10-root-INFO: Loss Change: 936.020 -> 911.549
2024-12-02-06:51:11-root-INFO: Regularization Change: 0.000 -> 0.367
2024-12-02-06:51:11-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-06:51:11-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:51:11-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-06:51:11-root-INFO: grad norm: 83.252 80.791 20.095
2024-12-02-06:51:11-root-INFO: Loss too large (913.024->1039.647)! Learning rate decreased to 0.00660.
2024-12-02-06:51:11-root-INFO: Loss too large (913.024->984.442)! Learning rate decreased to 0.00528.
2024-12-02-06:51:11-root-INFO: Loss too large (913.024->947.293)! Learning rate decreased to 0.00423.
2024-12-02-06:51:12-root-INFO: Loss too large (913.024->926.078)! Learning rate decreased to 0.00338.
2024-12-02-06:51:12-root-INFO: Loss too large (913.024->914.999)! Learning rate decreased to 0.00270.
2024-12-02-06:51:12-root-INFO: grad norm: 100.026 97.821 20.889
2024-12-02-06:51:12-root-INFO: Loss too large (909.660->911.755)! Learning rate decreased to 0.00216.
2024-12-02-06:51:13-root-INFO: grad norm: 82.576 80.348 19.051
2024-12-02-06:51:13-root-INFO: grad norm: 59.674 58.179 13.272
2024-12-02-06:51:14-root-INFO: grad norm: 52.356 50.593 13.469
2024-12-02-06:51:14-root-INFO: grad norm: 44.789 43.446 10.884
2024-12-02-06:51:15-root-INFO: grad norm: 40.954 39.366 11.293
2024-12-02-06:51:15-root-INFO: grad norm: 37.343 36.031 9.814
2024-12-02-06:51:16-root-INFO: Loss Change: 913.024 -> 891.698
2024-12-02-06:51:16-root-INFO: Regularization Change: 0.000 -> 0.312
2024-12-02-06:51:16-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-06:51:16-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:51:16-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-06:51:16-root-INFO: grad norm: 79.163 76.731 19.470
2024-12-02-06:51:16-root-INFO: Loss too large (895.067->994.802)! Learning rate decreased to 0.00685.
2024-12-02-06:51:16-root-INFO: Loss too large (895.067->947.408)! Learning rate decreased to 0.00548.
2024-12-02-06:51:16-root-INFO: Loss too large (895.067->918.510)! Learning rate decreased to 0.00439.
2024-12-02-06:51:16-root-INFO: Loss too large (895.067->902.605)! Learning rate decreased to 0.00351.
2024-12-02-06:51:17-root-INFO: grad norm: 133.982 131.018 28.023
2024-12-02-06:51:17-root-INFO: Loss too large (894.431->912.129)! Learning rate decreased to 0.00281.
2024-12-02-06:51:17-root-INFO: Loss too large (894.431->900.514)! Learning rate decreased to 0.00225.
2024-12-02-06:51:18-root-INFO: grad norm: 101.373 98.899 22.259
2024-12-02-06:51:18-root-INFO: grad norm: 56.041 54.621 12.534
2024-12-02-06:51:19-root-INFO: grad norm: 47.705 45.972 12.743
2024-12-02-06:51:19-root-INFO: grad norm: 39.980 38.680 10.110
2024-12-02-06:51:20-root-INFO: grad norm: 36.231 34.671 10.516
2024-12-02-06:51:20-root-INFO: grad norm: 33.069 31.766 9.192
2024-12-02-06:51:21-root-INFO: Loss Change: 895.067 -> 873.296
2024-12-02-06:51:21-root-INFO: Regularization Change: 0.000 -> 0.346
2024-12-02-06:51:21-root-INFO: Undo step: 155
2024-12-02-06:51:21-root-INFO: Undo step: 156
2024-12-02-06:51:21-root-INFO: Undo step: 157
2024-12-02-06:51:21-root-INFO: Undo step: 158
2024-12-02-06:51:21-root-INFO: Undo step: 159
2024-12-02-06:51:21-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-06:51:21-root-INFO: grad norm: 382.993 374.359 80.865
2024-12-02-06:51:21-root-INFO: Loss too large (1616.511->1682.666)! Learning rate decreased to 0.00568.
2024-12-02-06:51:22-root-INFO: grad norm: 359.727 355.170 57.074
2024-12-02-06:51:22-root-INFO: grad norm: 336.103 330.566 60.754
2024-12-02-06:51:22-root-INFO: Loss too large (1307.905->1415.804)! Learning rate decreased to 0.00455.
2024-12-02-06:51:22-root-INFO: Loss too large (1307.905->1330.166)! Learning rate decreased to 0.00364.
2024-12-02-06:51:23-root-INFO: grad norm: 235.021 231.464 40.737
2024-12-02-06:51:24-root-INFO: grad norm: 115.703 112.182 28.329
2024-12-02-06:51:24-root-INFO: grad norm: 118.076 115.123 26.241
2024-12-02-06:51:25-root-INFO: grad norm: 172.835 169.784 32.333
2024-12-02-06:51:25-root-INFO: Loss too large (1106.562->1119.286)! Learning rate decreased to 0.00291.
2024-12-02-06:51:25-root-INFO: grad norm: 164.814 162.401 28.103
2024-12-02-06:51:26-root-INFO: Loss Change: 1616.511 -> 1075.426
2024-12-02-06:51:26-root-INFO: Regularization Change: 0.000 -> 12.092
2024-12-02-06:51:26-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-06:51:26-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:51:26-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-06:51:26-root-INFO: grad norm: 127.468 125.125 24.327
2024-12-02-06:51:26-root-INFO: Loss too large (1062.637->1121.645)! Learning rate decreased to 0.00590.
2024-12-02-06:51:26-root-INFO: Loss too large (1062.637->1094.942)! Learning rate decreased to 0.00472.
2024-12-02-06:51:26-root-INFO: Loss too large (1062.637->1076.386)! Learning rate decreased to 0.00378.
2024-12-02-06:51:27-root-INFO: Loss too large (1062.637->1064.061)! Learning rate decreased to 0.00302.
2024-12-02-06:51:27-root-INFO: grad norm: 131.534 129.445 23.350
2024-12-02-06:51:28-root-INFO: grad norm: 155.737 153.286 27.523
2024-12-02-06:51:28-root-INFO: Loss too large (1041.623->1046.428)! Learning rate decreased to 0.00242.
2024-12-02-06:51:28-root-INFO: grad norm: 130.305 128.342 22.531
2024-12-02-06:51:29-root-INFO: grad norm: 93.820 91.850 19.125
2024-12-02-06:51:30-root-INFO: grad norm: 89.596 87.769 18.005
2024-12-02-06:51:30-root-INFO: grad norm: 86.079 84.220 17.795
2024-12-02-06:51:31-root-INFO: grad norm: 85.335 83.601 17.118
2024-12-02-06:51:31-root-INFO: Loss Change: 1062.637 -> 991.487
2024-12-02-06:51:31-root-INFO: Regularization Change: 0.000 -> 1.366
2024-12-02-06:51:31-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-06:51:31-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-06:51:31-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-06:51:31-root-INFO: grad norm: 66.710 64.457 17.191
2024-12-02-06:51:32-root-INFO: grad norm: 127.322 125.842 19.352
2024-12-02-06:51:32-root-INFO: Loss too large (977.850->1149.802)! Learning rate decreased to 0.00613.
2024-12-02-06:51:32-root-INFO: Loss too large (977.850->1083.054)! Learning rate decreased to 0.00490.
2024-12-02-06:51:32-root-INFO: Loss too large (977.850->1025.625)! Learning rate decreased to 0.00392.
2024-12-02-06:51:32-root-INFO: Loss too large (977.850->988.549)! Learning rate decreased to 0.00314.
2024-12-02-06:51:33-root-INFO: grad norm: 180.378 177.951 29.492
2024-12-02-06:51:33-root-INFO: Loss too large (969.729->992.281)! Learning rate decreased to 0.00251.
2024-12-02-06:51:33-root-INFO: Loss too large (969.729->976.496)! Learning rate decreased to 0.00201.
2024-12-02-06:51:34-root-INFO: grad norm: 121.201 119.570 19.817
2024-12-02-06:51:35-root-INFO: grad norm: 51.143 49.564 12.611
2024-12-02-06:51:35-root-INFO: grad norm: 46.028 44.410 12.096
2024-12-02-06:51:36-root-INFO: grad norm: 42.417 40.790 11.638
2024-12-02-06:51:36-root-INFO: grad norm: 40.529 38.893 11.399
2024-12-02-06:51:36-root-INFO: Loss Change: 982.515 -> 937.536
2024-12-02-06:51:36-root-INFO: Regularization Change: 0.000 -> 0.992
2024-12-02-06:51:36-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-06:51:36-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:51:37-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-06:51:37-root-INFO: grad norm: 65.788 63.519 17.129
2024-12-02-06:51:37-root-INFO: Loss too large (935.414->964.577)! Learning rate decreased to 0.00636.
2024-12-02-06:51:37-root-INFO: Loss too large (935.414->946.495)! Learning rate decreased to 0.00509.
2024-12-02-06:51:37-root-INFO: Loss too large (935.414->936.926)! Learning rate decreased to 0.00407.
2024-12-02-06:51:38-root-INFO: grad norm: 121.173 119.336 21.021
2024-12-02-06:51:38-root-INFO: Loss too large (932.249->953.722)! Learning rate decreased to 0.00326.
2024-12-02-06:51:38-root-INFO: Loss too large (932.249->941.555)! Learning rate decreased to 0.00261.
2024-12-02-06:51:39-root-INFO: Loss too large (932.249->933.562)! Learning rate decreased to 0.00208.
2024-12-02-06:51:39-root-INFO: grad norm: 89.639 88.083 16.630
2024-12-02-06:51:40-root-INFO: grad norm: 53.527 52.053 12.473
2024-12-02-06:51:40-root-INFO: grad norm: 46.388 44.843 11.873
2024-12-02-06:51:40-root-INFO: grad norm: 40.661 39.149 10.984
2024-12-02-06:51:41-root-INFO: grad norm: 37.972 36.414 10.766
2024-12-02-06:51:41-root-INFO: grad norm: 36.026 34.508 10.349
2024-12-02-06:51:42-root-INFO: Loss Change: 935.414 -> 907.297
2024-12-02-06:51:42-root-INFO: Regularization Change: 0.000 -> 0.488
2024-12-02-06:51:42-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-06:51:42-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:51:42-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-06:51:42-root-INFO: grad norm: 57.072 55.566 13.025
2024-12-02-06:51:42-root-INFO: Loss too large (905.585->936.018)! Learning rate decreased to 0.00660.
2024-12-02-06:51:42-root-INFO: Loss too large (905.585->918.437)! Learning rate decreased to 0.00528.
2024-12-02-06:51:43-root-INFO: Loss too large (905.585->909.103)! Learning rate decreased to 0.00423.
2024-12-02-06:51:43-root-INFO: grad norm: 112.604 110.994 18.971
2024-12-02-06:51:43-root-INFO: Loss too large (904.440->923.290)! Learning rate decreased to 0.00338.
2024-12-02-06:51:43-root-INFO: Loss too large (904.440->912.504)! Learning rate decreased to 0.00270.
2024-12-02-06:51:44-root-INFO: Loss too large (904.440->905.441)! Learning rate decreased to 0.00216.
2024-12-02-06:51:44-root-INFO: grad norm: 79.823 78.509 14.424
2024-12-02-06:51:45-root-INFO: grad norm: 44.562 43.249 10.739
2024-12-02-06:51:45-root-INFO: grad norm: 38.509 37.117 10.261
2024-12-02-06:51:46-root-INFO: grad norm: 34.390 32.978 9.751
2024-12-02-06:51:47-root-INFO: grad norm: 32.621 31.176 9.600
2024-12-02-06:51:47-root-INFO: grad norm: 31.513 30.079 9.400
2024-12-02-06:51:47-root-INFO: Loss Change: 905.585 -> 883.349
2024-12-02-06:51:47-root-INFO: Regularization Change: 0.000 -> 0.403
2024-12-02-06:51:47-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-06:51:47-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:51:48-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-06:51:48-root-INFO: grad norm: 61.062 59.323 14.467
2024-12-02-06:51:48-root-INFO: Loss too large (884.023->917.377)! Learning rate decreased to 0.00685.
2024-12-02-06:51:48-root-INFO: Loss too large (884.023->897.921)! Learning rate decreased to 0.00548.
2024-12-02-06:51:48-root-INFO: Loss too large (884.023->887.559)! Learning rate decreased to 0.00439.
2024-12-02-06:51:49-root-INFO: grad norm: 113.493 111.874 19.098
2024-12-02-06:51:49-root-INFO: Loss too large (882.372->902.367)! Learning rate decreased to 0.00351.
2024-12-02-06:51:49-root-INFO: Loss too large (882.372->891.125)! Learning rate decreased to 0.00281.
2024-12-02-06:51:49-root-INFO: Loss too large (882.372->883.724)! Learning rate decreased to 0.00225.
2024-12-02-06:51:50-root-INFO: grad norm: 79.406 78.102 14.328
2024-12-02-06:51:50-root-INFO: grad norm: 42.039 40.747 10.339
2024-12-02-06:51:51-root-INFO: grad norm: 36.161 34.788 9.868
2024-12-02-06:51:51-root-INFO: grad norm: 32.151 30.773 9.313
2024-12-02-06:51:52-root-INFO: grad norm: 30.432 29.021 9.158
2024-12-02-06:51:52-root-INFO: grad norm: 29.376 27.983 8.939
2024-12-02-06:51:52-root-INFO: Loss Change: 884.023 -> 862.227
2024-12-02-06:51:52-root-INFO: Regularization Change: 0.000 -> 0.396
2024-12-02-06:51:52-root-INFO: Undo step: 155
2024-12-02-06:51:52-root-INFO: Undo step: 156
2024-12-02-06:51:52-root-INFO: Undo step: 157
2024-12-02-06:51:52-root-INFO: Undo step: 158
2024-12-02-06:51:52-root-INFO: Undo step: 159
2024-12-02-06:51:53-root-INFO: step: 160 lr_xt 0.00710269
2024-12-02-06:51:53-root-INFO: grad norm: 363.317 351.163 93.188
2024-12-02-06:51:53-root-INFO: grad norm: 288.101 279.351 70.461
2024-12-02-06:51:54-root-INFO: grad norm: 464.691 435.985 160.794
2024-12-02-06:51:54-root-INFO: Loss too large (1306.858->1569.701)! Learning rate decreased to 0.00568.
2024-12-02-06:51:54-root-INFO: Loss too large (1306.858->1415.492)! Learning rate decreased to 0.00455.
2024-12-02-06:51:54-root-INFO: Loss too large (1306.858->1315.649)! Learning rate decreased to 0.00364.
2024-12-02-06:51:55-root-INFO: grad norm: 254.161 248.134 55.018
2024-12-02-06:51:55-root-INFO: grad norm: 134.598 126.983 44.632
2024-12-02-06:51:56-root-INFO: grad norm: 138.795 129.192 50.730
2024-12-02-06:51:56-root-INFO: grad norm: 139.726 132.127 45.453
2024-12-02-06:51:57-root-INFO: grad norm: 149.196 138.803 54.709
2024-12-02-06:51:57-root-INFO: Loss too large (1013.462->1018.096)! Learning rate decreased to 0.00291.
2024-12-02-06:51:57-root-INFO: Loss Change: 1626.740 -> 1008.014
2024-12-02-06:51:57-root-INFO: Regularization Change: 0.000 -> 15.379
2024-12-02-06:51:57-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-02-06:51:57-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00024.
2024-12-02-06:51:57-root-INFO: step: 159 lr_xt 0.00737641
2024-12-02-06:51:58-root-INFO: grad norm: 135.571 128.761 42.427
2024-12-02-06:51:58-root-INFO: Loss too large (1010.756->1112.459)! Learning rate decreased to 0.00590.
2024-12-02-06:51:58-root-INFO: Loss too large (1010.756->1042.072)! Learning rate decreased to 0.00472.
2024-12-02-06:51:58-root-INFO: grad norm: 233.348 216.546 86.941
2024-12-02-06:51:58-root-INFO: Loss too large (1001.957->1061.102)! Learning rate decreased to 0.00378.
2024-12-02-06:51:59-root-INFO: Loss too large (1001.957->1026.779)! Learning rate decreased to 0.00302.
2024-12-02-06:51:59-root-INFO: Loss too large (1001.957->1002.921)! Learning rate decreased to 0.00242.
2024-12-02-06:51:59-root-INFO: grad norm: 122.240 115.769 39.243
2024-12-02-06:52:00-root-INFO: grad norm: 49.374 47.233 14.379
2024-12-02-06:52:00-root-INFO: grad norm: 46.567 44.601 13.387
2024-12-02-06:52:01-root-INFO: grad norm: 45.049 43.147 12.954
2024-12-02-06:52:01-root-INFO: grad norm: 43.797 41.965 12.535
2024-12-02-06:52:02-root-INFO: grad norm: 42.688 40.907 12.200
2024-12-02-06:52:02-root-INFO: Loss Change: 1010.756 -> 938.405
2024-12-02-06:52:02-root-INFO: Regularization Change: 0.000 -> 1.178
2024-12-02-06:52:02-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-02-06:52:02-root-INFO: Coefficient of regularization decay: 0.00024 -> 0.00025.
2024-12-02-06:52:02-root-INFO: step: 158 lr_xt 0.00765943
2024-12-02-06:52:02-root-INFO: grad norm: 67.226 63.889 20.916
2024-12-02-06:52:03-root-INFO: Loss too large (934.870->938.334)! Learning rate decreased to 0.00613.
2024-12-02-06:52:03-root-INFO: grad norm: 124.660 117.539 41.531
2024-12-02-06:52:03-root-INFO: Loss too large (930.992->965.343)! Learning rate decreased to 0.00490.
2024-12-02-06:52:03-root-INFO: Loss too large (930.992->945.317)! Learning rate decreased to 0.00392.
2024-12-02-06:52:04-root-INFO: Loss too large (930.992->932.557)! Learning rate decreased to 0.00314.
2024-12-02-06:52:04-root-INFO: grad norm: 92.458 87.555 29.708
2024-12-02-06:52:04-root-INFO: grad norm: 57.042 54.151 17.928
2024-12-02-06:52:05-root-INFO: grad norm: 50.700 48.066 16.128
2024-12-02-06:52:05-root-INFO: grad norm: 44.549 42.478 13.425
2024-12-02-06:52:06-root-INFO: grad norm: 41.464 39.484 12.658
2024-12-02-06:52:06-root-INFO: grad norm: 38.655 36.976 11.270
2024-12-02-06:52:07-root-INFO: Loss Change: 934.870 -> 891.529
2024-12-02-06:52:07-root-INFO: Regularization Change: 0.000 -> 1.147
2024-12-02-06:52:07-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-02-06:52:07-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:52:07-root-INFO: step: 157 lr_xt 0.00795203
2024-12-02-06:52:07-root-INFO: grad norm: 73.550 69.364 24.460
2024-12-02-06:52:07-root-INFO: Loss too large (891.658->915.092)! Learning rate decreased to 0.00636.
2024-12-02-06:52:08-root-INFO: Loss too large (891.658->897.542)! Learning rate decreased to 0.00509.
2024-12-02-06:52:08-root-INFO: grad norm: 114.811 108.485 37.583
2024-12-02-06:52:08-root-INFO: Loss too large (888.803->902.409)! Learning rate decreased to 0.00407.
2024-12-02-06:52:08-root-INFO: Loss too large (888.803->890.984)! Learning rate decreased to 0.00326.
2024-12-02-06:52:09-root-INFO: grad norm: 83.130 78.677 26.844
2024-12-02-06:52:09-root-INFO: grad norm: 47.170 45.072 13.911
2024-12-02-06:52:10-root-INFO: grad norm: 40.763 38.598 13.110
2024-12-02-06:52:10-root-INFO: grad norm: 35.699 34.200 10.237
2024-12-02-06:52:11-root-INFO: grad norm: 33.268 31.675 10.172
2024-12-02-06:52:11-root-INFO: grad norm: 31.455 30.173 8.888
2024-12-02-06:52:12-root-INFO: Loss Change: 891.658 -> 857.294
2024-12-02-06:52:12-root-INFO: Regularization Change: 0.000 -> 0.841
2024-12-02-06:52:12-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-02-06:52:12-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:52:12-root-INFO: step: 156 lr_xt 0.00825448
2024-12-02-06:52:12-root-INFO: grad norm: 50.634 48.482 14.605
2024-12-02-06:52:12-root-INFO: Loss too large (856.416->861.326)! Learning rate decreased to 0.00660.
2024-12-02-06:52:13-root-INFO: grad norm: 96.637 92.130 29.168
2024-12-02-06:52:13-root-INFO: Loss too large (855.635->876.831)! Learning rate decreased to 0.00528.
2024-12-02-06:52:13-root-INFO: Loss too large (855.635->863.312)! Learning rate decreased to 0.00423.
2024-12-02-06:52:14-root-INFO: grad norm: 86.545 82.879 24.920
2024-12-02-06:52:14-root-INFO: grad norm: 70.395 67.173 21.053
2024-12-02-06:52:15-root-INFO: grad norm: 66.054 63.027 19.765
2024-12-02-06:52:15-root-INFO: grad norm: 60.342 57.479 18.364
2024-12-02-06:52:16-root-INFO: grad norm: 57.852 55.094 17.649
2024-12-02-06:52:16-root-INFO: grad norm: 54.487 51.870 16.684
2024-12-02-06:52:16-root-INFO: Loss Change: 856.416 -> 829.099
2024-12-02-06:52:16-root-INFO: Regularization Change: 0.000 -> 1.023
2024-12-02-06:52:16-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-02-06:52:16-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00025.
2024-12-02-06:52:16-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-06:52:17-root-INFO: grad norm: 84.431 81.030 23.722
2024-12-02-06:52:17-root-INFO: Loss too large (833.398->872.178)! Learning rate decreased to 0.00685.
2024-12-02-06:52:17-root-INFO: Loss too large (833.398->844.285)! Learning rate decreased to 0.00548.
2024-12-02-06:52:17-root-INFO: grad norm: 110.988 106.536 31.119
2024-12-02-06:52:18-root-INFO: Loss too large (830.553->840.098)! Learning rate decreased to 0.00439.
2024-12-02-06:52:18-root-INFO: grad norm: 90.184 86.783 24.532
2024-12-02-06:52:19-root-INFO: grad norm: 60.737 58.817 15.148
2024-12-02-06:52:19-root-INFO: grad norm: 53.303 51.155 14.976
2024-12-02-06:52:19-root-INFO: grad norm: 46.328 44.664 12.305
2024-12-02-06:52:20-root-INFO: grad norm: 42.487 40.609 12.491
2024-12-02-06:52:20-root-INFO: grad norm: 38.640 37.155 10.610
2024-12-02-06:52:21-root-INFO: Loss Change: 833.398 -> 801.614
2024-12-02-06:52:21-root-INFO: Regularization Change: 0.000 -> 0.871
2024-12-02-06:52:21-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-06:52:21-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-06:52:21-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-06:52:21-root-INFO: grad norm: 69.550 67.213 17.876
2024-12-02-06:52:21-root-INFO: Loss too large (804.221->825.741)! Learning rate decreased to 0.00711.
2024-12-02-06:52:21-root-INFO: Loss too large (804.221->809.815)! Learning rate decreased to 0.00569.
2024-12-02-06:52:22-root-INFO: grad norm: 84.124 81.863 19.373
2024-12-02-06:52:22-root-INFO: Loss too large (801.641->804.078)! Learning rate decreased to 0.00455.
2024-12-02-06:52:23-root-INFO: grad norm: 68.255 66.149 16.821
2024-12-02-06:52:23-root-INFO: grad norm: 52.054 50.778 11.456
2024-12-02-06:52:24-root-INFO: grad norm: 44.495 43.030 11.327
2024-12-02-06:52:24-root-INFO: grad norm: 37.859 36.797 8.903
2024-12-02-06:52:25-root-INFO: grad norm: 33.819 32.558 9.150
2024-12-02-06:52:25-root-INFO: grad norm: 30.410 29.423 7.683
2024-12-02-06:52:26-root-INFO: Loss Change: 804.221 -> 777.727
2024-12-02-06:52:26-root-INFO: Regularization Change: 0.000 -> 0.798
2024-12-02-06:52:26-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-06:52:26-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-06:52:26-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-06:52:26-root-INFO: grad norm: 63.953 61.958 15.848
2024-12-02-06:52:26-root-INFO: Loss too large (780.364->794.850)! Learning rate decreased to 0.00738.
2024-12-02-06:52:26-root-INFO: Loss too large (780.364->783.471)! Learning rate decreased to 0.00590.
2024-12-02-06:52:27-root-INFO: grad norm: 72.587 71.311 13.549
2024-12-02-06:52:27-root-INFO: Loss too large (777.402->777.489)! Learning rate decreased to 0.00472.
2024-12-02-06:52:28-root-INFO: grad norm: 57.628 56.342 12.108
2024-12-02-06:52:28-root-INFO: grad norm: 45.600 44.817 8.418
2024-12-02-06:52:29-root-INFO: grad norm: 38.459 37.497 8.545
2024-12-02-06:52:29-root-INFO: grad norm: 32.911 32.165 6.965
2024-12-02-06:52:30-root-INFO: grad norm: 29.300 28.385 7.266
2024-12-02-06:52:30-root-INFO: grad norm: 26.579 25.782 6.460
2024-12-02-06:52:30-root-INFO: Loss Change: 780.364 -> 755.906
2024-12-02-06:52:30-root-INFO: Regularization Change: 0.000 -> 0.773
2024-12-02-06:52:30-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-06:52:30-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-06:52:31-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-06:52:31-root-INFO: grad norm: 43.421 42.176 10.326
2024-12-02-06:52:31-root-INFO: Loss too large (756.551->760.446)! Learning rate decreased to 0.00765.
2024-12-02-06:52:31-root-INFO: grad norm: 67.206 66.290 11.060
2024-12-02-06:52:32-root-INFO: Loss too large (756.041->763.616)! Learning rate decreased to 0.00612.
2024-12-02-06:52:32-root-INFO: Loss too large (756.041->756.057)! Learning rate decreased to 0.00490.
2024-12-02-06:52:32-root-INFO: grad norm: 52.522 51.577 9.917
2024-12-02-06:52:33-root-INFO: grad norm: 40.823 40.193 7.148
2024-12-02-06:52:33-root-INFO: grad norm: 34.245 33.455 7.316
2024-12-02-06:52:34-root-INFO: grad norm: 29.304 28.605 6.363
2024-12-02-06:52:34-root-INFO: grad norm: 26.209 25.368 6.583
2024-12-02-06:52:35-root-INFO: grad norm: 23.963 23.168 6.122
2024-12-02-06:52:35-root-INFO: Loss Change: 756.551 -> 736.511
2024-12-02-06:52:35-root-INFO: Regularization Change: 0.000 -> 0.762
2024-12-02-06:52:35-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-06:52:35-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-06:52:35-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-06:52:35-root-INFO: grad norm: 52.957 51.559 12.084
2024-12-02-06:52:35-root-INFO: Loss too large (738.472->746.642)! Learning rate decreased to 0.00794.
2024-12-02-06:52:35-root-INFO: Loss too large (738.472->739.316)! Learning rate decreased to 0.00635.
2024-12-02-06:52:36-root-INFO: grad norm: 57.900 57.192 9.032
2024-12-02-06:52:36-root-INFO: grad norm: 64.662 63.860 10.151
2024-12-02-06:52:37-root-INFO: grad norm: 72.265 71.696 9.057
2024-12-02-06:52:37-root-INFO: grad norm: 76.111 75.428 10.175
2024-12-02-06:52:38-root-INFO: grad norm: 78.421 77.875 9.240
2024-12-02-06:52:38-root-INFO: grad norm: 78.163 77.508 10.102
2024-12-02-06:52:39-root-INFO: grad norm: 76.408 75.856 9.170
2024-12-02-06:52:39-root-INFO: Loss Change: 738.472 -> 724.009
2024-12-02-06:52:39-root-INFO: Regularization Change: 0.000 -> 1.056
2024-12-02-06:52:39-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-06:52:39-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:52:39-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-06:52:39-root-INFO: grad norm: 93.707 92.772 13.206
2024-12-02-06:52:40-root-INFO: Loss too large (730.121->761.030)! Learning rate decreased to 0.00823.
2024-12-02-06:52:40-root-INFO: Loss too large (730.121->736.781)! Learning rate decreased to 0.00659.
2024-12-02-06:52:40-root-INFO: grad norm: 84.813 84.294 9.370
2024-12-02-06:52:41-root-INFO: grad norm: 77.971 77.301 10.196
2024-12-02-06:52:41-root-INFO: grad norm: 70.631 70.107 8.585
2024-12-02-06:52:42-root-INFO: grad norm: 66.459 65.871 8.814
2024-12-02-06:52:42-root-INFO: grad norm: 62.475 61.966 7.961
2024-12-02-06:52:43-root-INFO: grad norm: 60.153 59.611 8.058
2024-12-02-06:52:43-root-INFO: grad norm: 58.079 57.592 7.500
2024-12-02-06:52:43-root-INFO: Loss Change: 730.121 -> 701.166
2024-12-02-06:52:43-root-INFO: Regularization Change: 0.000 -> 0.973
2024-12-02-06:52:43-root-INFO: Undo step: 150
2024-12-02-06:52:43-root-INFO: Undo step: 151
2024-12-02-06:52:43-root-INFO: Undo step: 152
2024-12-02-06:52:43-root-INFO: Undo step: 153
2024-12-02-06:52:43-root-INFO: Undo step: 154
2024-12-02-06:52:44-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-06:52:44-root-INFO: grad norm: 421.749 409.522 100.817
2024-12-02-06:52:44-root-INFO: grad norm: 182.301 177.378 42.081
2024-12-02-06:52:45-root-INFO: grad norm: 141.133 138.160 28.813
2024-12-02-06:52:45-root-INFO: grad norm: 140.303 138.297 23.641
2024-12-02-06:52:46-root-INFO: grad norm: 185.650 179.352 47.943
2024-12-02-06:52:46-root-INFO: Loss too large (877.053->968.741)! Learning rate decreased to 0.00685.
2024-12-02-06:52:46-root-INFO: Loss too large (877.053->909.914)! Learning rate decreased to 0.00548.
2024-12-02-06:52:46-root-INFO: grad norm: 163.747 160.286 33.489
2024-12-02-06:52:47-root-INFO: grad norm: 186.255 177.016 57.932
2024-12-02-06:52:47-root-INFO: Loss too large (836.223->869.416)! Learning rate decreased to 0.00439.
2024-12-02-06:52:47-root-INFO: Loss too large (836.223->843.528)! Learning rate decreased to 0.00351.
2024-12-02-06:52:48-root-INFO: grad norm: 108.812 105.677 25.929
2024-12-02-06:52:48-root-INFO: Loss Change: 1526.300 -> 801.202
2024-12-02-06:52:48-root-INFO: Regularization Change: 0.000 -> 22.354
2024-12-02-06:52:48-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-06:52:48-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-06:52:48-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-06:52:48-root-INFO: grad norm: 56.594 54.533 15.133
2024-12-02-06:52:49-root-INFO: grad norm: 90.760 87.505 24.089
2024-12-02-06:52:49-root-INFO: Loss too large (789.303->816.344)! Learning rate decreased to 0.00711.
2024-12-02-06:52:49-root-INFO: Loss too large (789.303->798.992)! Learning rate decreased to 0.00569.
2024-12-02-06:52:50-root-INFO: grad norm: 95.577 93.074 21.729
2024-12-02-06:52:50-root-INFO: grad norm: 123.450 117.957 36.414
2024-12-02-06:52:50-root-INFO: Loss too large (780.916->794.214)! Learning rate decreased to 0.00455.
2024-12-02-06:52:51-root-INFO: Loss too large (780.916->781.064)! Learning rate decreased to 0.00364.
2024-12-02-06:52:51-root-INFO: grad norm: 75.047 72.779 18.312
2024-12-02-06:52:52-root-INFO: grad norm: 34.633 33.400 9.157
2024-12-02-06:52:52-root-INFO: grad norm: 30.996 29.883 8.233
2024-12-02-06:52:52-root-INFO: grad norm: 29.337 28.263 7.866
2024-12-02-06:52:53-root-INFO: Loss Change: 797.928 -> 752.226
2024-12-02-06:52:53-root-INFO: Regularization Change: 0.000 -> 1.647
2024-12-02-06:52:53-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-06:52:53-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-06:52:53-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-06:52:53-root-INFO: grad norm: 52.682 50.831 13.842
2024-12-02-06:52:54-root-INFO: grad norm: 100.494 97.295 25.154
2024-12-02-06:52:54-root-INFO: Loss too large (751.715->793.595)! Learning rate decreased to 0.00738.
2024-12-02-06:52:54-root-INFO: Loss too large (751.715->769.578)! Learning rate decreased to 0.00590.
2024-12-02-06:52:54-root-INFO: Loss too large (751.715->754.647)! Learning rate decreased to 0.00472.
2024-12-02-06:52:55-root-INFO: grad norm: 76.466 74.536 17.070
2024-12-02-06:52:55-root-INFO: grad norm: 54.514 52.525 14.589
2024-12-02-06:52:56-root-INFO: grad norm: 47.526 46.033 11.818
2024-12-02-06:52:56-root-INFO: grad norm: 40.563 39.036 11.026
2024-12-02-06:52:57-root-INFO: grad norm: 37.455 36.217 9.551
2024-12-02-06:52:57-root-INFO: grad norm: 34.483 33.151 9.492
2024-12-02-06:52:57-root-INFO: Loss Change: 751.881 -> 721.031
2024-12-02-06:52:57-root-INFO: Regularization Change: 0.000 -> 1.206
2024-12-02-06:52:57-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-06:52:57-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-06:52:58-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-06:52:58-root-INFO: grad norm: 48.011 46.500 11.951
2024-12-02-06:52:58-root-INFO: Loss too large (720.508->726.687)! Learning rate decreased to 0.00765.
2024-12-02-06:52:58-root-INFO: grad norm: 81.477 78.639 21.317
2024-12-02-06:52:59-root-INFO: Loss too large (720.032->733.663)! Learning rate decreased to 0.00612.
2024-12-02-06:52:59-root-INFO: Loss too large (720.032->723.482)! Learning rate decreased to 0.00490.
2024-12-02-06:52:59-root-INFO: grad norm: 63.567 61.864 14.619
2024-12-02-06:53:00-root-INFO: grad norm: 41.412 40.051 10.528
2024-12-02-06:53:00-root-INFO: grad norm: 34.867 33.685 8.999
2024-12-02-06:53:01-root-INFO: grad norm: 29.290 28.248 7.745
2024-12-02-06:53:01-root-INFO: grad norm: 26.624 25.645 7.153
2024-12-02-06:53:02-root-INFO: grad norm: 24.440 23.514 6.666
2024-12-02-06:53:02-root-INFO: Loss Change: 720.508 -> 698.116
2024-12-02-06:53:02-root-INFO: Regularization Change: 0.000 -> 0.834
2024-12-02-06:53:02-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-06:53:02-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-06:53:02-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-06:53:02-root-INFO: grad norm: 51.022 49.412 12.718
2024-12-02-06:53:02-root-INFO: Loss too large (699.303->704.497)! Learning rate decreased to 0.00794.
2024-12-02-06:53:03-root-INFO: grad norm: 72.955 71.179 16.000
2024-12-02-06:53:03-root-INFO: Loss too large (698.024->706.055)! Learning rate decreased to 0.00635.
2024-12-02-06:53:04-root-INFO: grad norm: 70.897 69.359 14.687
2024-12-02-06:53:04-root-INFO: grad norm: 68.574 66.799 15.502
2024-12-02-06:53:04-root-INFO: grad norm: 66.371 64.841 14.170
2024-12-02-06:53:05-root-INFO: grad norm: 63.019 61.311 14.570
2024-12-02-06:53:06-root-INFO: grad norm: 61.136 59.632 13.478
2024-12-02-06:53:06-root-INFO: grad norm: 58.517 56.870 13.785
2024-12-02-06:53:07-root-INFO: Loss Change: 699.303 -> 681.281
2024-12-02-06:53:07-root-INFO: Regularization Change: 0.000 -> 1.054
2024-12-02-06:53:07-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-06:53:07-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:53:07-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-06:53:07-root-INFO: grad norm: 70.445 68.925 14.558
2024-12-02-06:53:07-root-INFO: Loss too large (684.023->700.259)! Learning rate decreased to 0.00823.
2024-12-02-06:53:08-root-INFO: grad norm: 97.155 94.965 20.515
2024-12-02-06:53:08-root-INFO: Loss too large (683.945->698.085)! Learning rate decreased to 0.00659.
2024-12-02-06:53:08-root-INFO: Loss too large (683.945->684.023)! Learning rate decreased to 0.00527.
2024-12-02-06:53:09-root-INFO: grad norm: 57.937 56.627 12.251
2024-12-02-06:53:09-root-INFO: grad norm: 27.680 27.021 6.003
2024-12-02-06:53:10-root-INFO: grad norm: 21.002 20.175 5.834
2024-12-02-06:53:10-root-INFO: grad norm: 18.306 17.590 5.070
2024-12-02-06:53:11-root-INFO: grad norm: 17.410 16.639 5.124
2024-12-02-06:53:11-root-INFO: grad norm: 17.016 16.282 4.944
2024-12-02-06:53:12-root-INFO: Loss Change: 684.023 -> 659.943
2024-12-02-06:53:12-root-INFO: Regularization Change: 0.000 -> 0.708
2024-12-02-06:53:12-root-INFO: Undo step: 150
2024-12-02-06:53:12-root-INFO: Undo step: 151
2024-12-02-06:53:12-root-INFO: Undo step: 152
2024-12-02-06:53:12-root-INFO: Undo step: 153
2024-12-02-06:53:12-root-INFO: Undo step: 154
2024-12-02-06:53:12-root-INFO: step: 155 lr_xt 0.00856705
2024-12-02-06:53:12-root-INFO: grad norm: 366.131 355.376 88.090
2024-12-02-06:53:12-root-INFO: grad norm: 319.429 315.752 48.326
2024-12-02-06:53:13-root-INFO: grad norm: 384.105 370.956 99.641
2024-12-02-06:53:13-root-INFO: Loss too large (1031.314->1335.377)! Learning rate decreased to 0.00685.
2024-12-02-06:53:13-root-INFO: Loss too large (1031.314->1167.727)! Learning rate decreased to 0.00548.
2024-12-02-06:53:13-root-INFO: Loss too large (1031.314->1054.787)! Learning rate decreased to 0.00439.
2024-12-02-06:53:14-root-INFO: grad norm: 224.465 221.596 35.772
2024-12-02-06:53:14-root-INFO: grad norm: 72.440 69.606 20.064
2024-12-02-06:53:15-root-INFO: grad norm: 59.300 56.763 17.159
2024-12-02-06:53:15-root-INFO: grad norm: 52.850 50.338 16.101
2024-12-02-06:53:16-root-INFO: grad norm: 48.231 45.881 14.871
2024-12-02-06:53:16-root-INFO: Loss Change: 1484.568 -> 781.466
2024-12-02-06:53:16-root-INFO: Regularization Change: 0.000 -> 16.297
2024-12-02-06:53:16-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-02-06:53:16-root-INFO: Coefficient of regularization decay: 0.00025 -> 0.00026.
2024-12-02-06:53:16-root-INFO: step: 154 lr_xt 0.00889002
2024-12-02-06:53:17-root-INFO: grad norm: 63.660 61.277 17.253
2024-12-02-06:53:17-root-INFO: grad norm: 124.065 120.586 29.177
2024-12-02-06:53:17-root-INFO: Loss too large (774.017->821.842)! Learning rate decreased to 0.00711.
2024-12-02-06:53:18-root-INFO: Loss too large (774.017->794.156)! Learning rate decreased to 0.00569.
2024-12-02-06:53:18-root-INFO: Loss too large (774.017->776.387)! Learning rate decreased to 0.00455.
2024-12-02-06:53:18-root-INFO: grad norm: 87.675 85.509 19.367
2024-12-02-06:53:19-root-INFO: grad norm: 47.792 45.989 13.003
2024-12-02-06:53:19-root-INFO: grad norm: 40.597 38.842 11.807
2024-12-02-06:53:20-root-INFO: grad norm: 35.419 33.795 10.603
2024-12-02-06:53:20-root-INFO: grad norm: 32.727 31.146 10.049
2024-12-02-06:53:21-root-INFO: grad norm: 30.679 29.179 9.476
2024-12-02-06:53:21-root-INFO: Loss Change: 778.149 -> 725.351
2024-12-02-06:53:21-root-INFO: Regularization Change: 0.000 -> 2.028
2024-12-02-06:53:21-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-02-06:53:21-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-06:53:21-root-INFO: step: 153 lr_xt 0.00922367
2024-12-02-06:53:21-root-INFO: grad norm: 54.298 52.413 14.184
2024-12-02-06:53:22-root-INFO: grad norm: 93.714 92.032 17.678
2024-12-02-06:53:22-root-INFO: Loss too large (725.338->750.254)! Learning rate decreased to 0.00738.
2024-12-02-06:53:22-root-INFO: Loss too large (725.338->732.123)! Learning rate decreased to 0.00590.
2024-12-02-06:53:23-root-INFO: grad norm: 80.025 78.565 15.216
2024-12-02-06:53:23-root-INFO: grad norm: 64.942 63.552 13.362
2024-12-02-06:53:24-root-INFO: grad norm: 58.786 57.476 12.342
2024-12-02-06:53:24-root-INFO: grad norm: 52.049 50.760 11.514
2024-12-02-06:53:25-root-INFO: grad norm: 48.762 47.517 10.947
2024-12-02-06:53:25-root-INFO: grad norm: 45.183 43.963 10.430
2024-12-02-06:53:26-root-INFO: Loss Change: 725.968 -> 692.054
2024-12-02-06:53:26-root-INFO: Regularization Change: 0.000 -> 1.662
2024-12-02-06:53:26-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-02-06:53:26-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00026.
2024-12-02-06:53:26-root-INFO: step: 152 lr_xt 0.00956831
2024-12-02-06:53:26-root-INFO: grad norm: 57.263 55.952 12.184
2024-12-02-06:53:26-root-INFO: Loss too large (692.754->697.693)! Learning rate decreased to 0.00765.
2024-12-02-06:53:27-root-INFO: grad norm: 71.931 70.641 13.564
2024-12-02-06:53:27-root-INFO: Loss too large (689.921->693.340)! Learning rate decreased to 0.00612.
2024-12-02-06:53:27-root-INFO: grad norm: 60.421 59.260 11.790
2024-12-02-06:53:28-root-INFO: grad norm: 46.668 45.746 9.228
2024-12-02-06:53:28-root-INFO: grad norm: 41.048 40.053 8.985
2024-12-02-06:53:29-root-INFO: grad norm: 35.450 34.609 7.674
2024-12-02-06:53:29-root-INFO: grad norm: 32.323 31.385 7.732
2024-12-02-06:53:30-root-INFO: grad norm: 29.354 28.547 6.835
2024-12-02-06:53:30-root-INFO: Loss Change: 692.754 -> 666.122
2024-12-02-06:53:30-root-INFO: Regularization Change: 0.000 -> 1.120
2024-12-02-06:53:30-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-02-06:53:30-root-INFO: Coefficient of regularization decay: 0.00026 -> 0.00027.
2024-12-02-06:53:30-root-INFO: step: 151 lr_xt 0.00992422
2024-12-02-06:53:31-root-INFO: grad norm: 56.474 55.184 12.002
2024-12-02-06:53:31-root-INFO: Loss too large (668.383->672.785)! Learning rate decreased to 0.00794.
2024-12-02-06:53:31-root-INFO: grad norm: 66.245 65.471 10.097
2024-12-02-06:53:31-root-INFO: Loss too large (665.744->667.215)! Learning rate decreased to 0.00635.
2024-12-02-06:53:32-root-INFO: grad norm: 52.907 52.046 9.503
2024-12-02-06:53:32-root-INFO: grad norm: 40.204 39.604 6.922
2024-12-02-06:53:33-root-INFO: grad norm: 33.748 33.010 7.020
2024-12-02-06:53:33-root-INFO: grad norm: 28.241 27.635 5.821
2024-12-02-06:53:34-root-INFO: grad norm: 25.001 24.269 6.004
2024-12-02-06:53:34-root-INFO: grad norm: 22.423 21.781 5.330
2024-12-02-06:53:35-root-INFO: Loss Change: 668.383 -> 644.589
2024-12-02-06:53:35-root-INFO: Regularization Change: 0.000 -> 0.990
2024-12-02-06:53:35-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-02-06:53:35-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:53:35-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-06:53:35-root-INFO: grad norm: 39.655 38.886 7.768
2024-12-02-06:53:35-root-INFO: Loss too large (645.555->646.534)! Learning rate decreased to 0.00823.
2024-12-02-06:53:35-root-INFO: grad norm: 46.505 46.054 6.462
2024-12-02-06:53:36-root-INFO: grad norm: 53.188 52.627 7.700
2024-12-02-06:53:36-root-INFO: grad norm: 62.454 61.954 7.884
2024-12-02-06:53:37-root-INFO: Loss too large (641.713->643.171)! Learning rate decreased to 0.00659.
2024-12-02-06:53:37-root-INFO: grad norm: 48.593 48.070 7.106
2024-12-02-06:53:38-root-INFO: grad norm: 36.108 35.642 5.784
2024-12-02-06:53:38-root-INFO: grad norm: 29.762 29.247 5.514
2024-12-02-06:53:39-root-INFO: grad norm: 24.725 24.219 4.977
2024-12-02-06:53:39-root-INFO: Loss Change: 645.555 -> 627.137
2024-12-02-06:53:39-root-INFO: Regularization Change: 0.000 -> 0.947
2024-12-02-06:53:39-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-06:53:39-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:53:39-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-06:53:39-root-INFO: grad norm: 46.928 46.040 9.088
2024-12-02-06:53:40-root-INFO: Loss too large (628.885->631.907)! Learning rate decreased to 0.00854.
2024-12-02-06:53:40-root-INFO: grad norm: 53.465 52.960 7.334
2024-12-02-06:53:40-root-INFO: Loss too large (626.924->627.127)! Learning rate decreased to 0.00683.
2024-12-02-06:53:41-root-INFO: grad norm: 41.624 41.108 6.532
2024-12-02-06:53:41-root-INFO: grad norm: 31.745 31.299 5.302
2024-12-02-06:53:42-root-INFO: grad norm: 26.291 25.810 5.008
2024-12-02-06:53:42-root-INFO: grad norm: 22.172 21.665 4.714
2024-12-02-06:53:43-root-INFO: grad norm: 19.701 19.170 4.542
2024-12-02-06:53:43-root-INFO: grad norm: 17.965 17.394 4.493
2024-12-02-06:53:44-root-INFO: Loss Change: 628.885 -> 610.627
2024-12-02-06:53:44-root-INFO: Regularization Change: 0.000 -> 0.826
2024-12-02-06:53:44-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-06:53:44-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:53:44-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-06:53:44-root-INFO: grad norm: 42.279 41.445 8.356
2024-12-02-06:53:44-root-INFO: Loss too large (612.938->614.876)! Learning rate decreased to 0.00885.
2024-12-02-06:53:45-root-INFO: grad norm: 47.578 47.109 6.667
2024-12-02-06:53:45-root-INFO: grad norm: 53.951 53.476 7.143
2024-12-02-06:53:46-root-INFO: grad norm: 61.524 61.069 7.476
2024-12-02-06:53:46-root-INFO: Loss too large (610.028->611.053)! Learning rate decreased to 0.00708.
2024-12-02-06:53:46-root-INFO: grad norm: 46.394 46.030 5.805
2024-12-02-06:53:47-root-INFO: grad norm: 35.029 34.587 5.549
2024-12-02-06:53:47-root-INFO: grad norm: 28.927 28.540 4.713
2024-12-02-06:53:48-root-INFO: grad norm: 24.829 24.325 4.976
2024-12-02-06:53:48-root-INFO: Loss Change: 612.938 -> 596.007
2024-12-02-06:53:48-root-INFO: Regularization Change: 0.000 -> 0.889
2024-12-02-06:53:48-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-06:53:48-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-06:53:48-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-06:53:49-root-INFO: grad norm: 40.294 39.698 6.909
2024-12-02-06:53:49-root-INFO: Loss too large (597.256->599.954)! Learning rate decreased to 0.00917.
2024-12-02-06:53:49-root-INFO: grad norm: 46.778 46.364 6.209
2024-12-02-06:53:49-root-INFO: Loss too large (596.065->596.176)! Learning rate decreased to 0.00734.
2024-12-02-06:53:50-root-INFO: grad norm: 37.843 37.448 5.453
2024-12-02-06:53:50-root-INFO: grad norm: 31.254 30.777 5.440
2024-12-02-06:53:51-root-INFO: grad norm: 27.083 26.659 4.773
2024-12-02-06:53:51-root-INFO: grad norm: 24.223 23.688 5.061
2024-12-02-06:53:52-root-INFO: grad norm: 22.135 21.670 4.511
2024-12-02-06:53:53-root-INFO: grad norm: 20.728 20.165 4.799
2024-12-02-06:53:53-root-INFO: Loss Change: 597.256 -> 582.208
2024-12-02-06:53:53-root-INFO: Regularization Change: 0.000 -> 0.764
2024-12-02-06:53:53-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-06:53:53-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:53:53-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-06:53:53-root-INFO: grad norm: 34.993 34.435 6.225
2024-12-02-06:53:53-root-INFO: Loss too large (582.863->584.640)! Learning rate decreased to 0.00951.
2024-12-02-06:53:54-root-INFO: grad norm: 40.882 40.375 6.419
2024-12-02-06:53:54-root-INFO: grad norm: 51.623 51.140 7.050
2024-12-02-06:53:55-root-INFO: Loss too large (581.660->583.361)! Learning rate decreased to 0.00761.
2024-12-02-06:53:55-root-INFO: grad norm: 41.958 41.379 6.947
2024-12-02-06:53:56-root-INFO: grad norm: 33.439 33.004 5.373
2024-12-02-06:53:56-root-INFO: grad norm: 29.375 28.822 5.672
2024-12-02-06:53:57-root-INFO: grad norm: 26.107 25.651 4.856
2024-12-02-06:53:57-root-INFO: grad norm: 24.082 23.523 5.161
2024-12-02-06:53:57-root-INFO: Loss Change: 582.863 -> 569.242
2024-12-02-06:53:57-root-INFO: Regularization Change: 0.000 -> 0.784
2024-12-02-06:53:57-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-06:53:57-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:53:58-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-06:53:58-root-INFO: grad norm: 45.483 44.833 7.664
2024-12-02-06:53:58-root-INFO: Loss too large (572.018->577.490)! Learning rate decreased to 0.00985.
2024-12-02-06:53:59-root-INFO: grad norm: 54.134 53.639 7.308
2024-12-02-06:53:59-root-INFO: Loss too large (571.664->572.948)! Learning rate decreased to 0.00788.
2024-12-02-06:53:59-root-INFO: grad norm: 43.925 43.519 5.960
2024-12-02-06:54:00-root-INFO: grad norm: 36.370 35.873 5.991
2024-12-02-06:54:00-root-INFO: grad norm: 31.622 31.206 5.115
2024-12-02-06:54:01-root-INFO: grad norm: 28.372 27.838 5.483
2024-12-02-06:54:01-root-INFO: grad norm: 25.953 25.504 4.805
2024-12-02-06:54:02-root-INFO: grad norm: 24.358 23.804 5.168
2024-12-02-06:54:02-root-INFO: Loss Change: 572.018 -> 556.788
2024-12-02-06:54:02-root-INFO: Regularization Change: 0.000 -> 0.779
2024-12-02-06:54:02-root-INFO: Undo step: 145
2024-12-02-06:54:02-root-INFO: Undo step: 146
2024-12-02-06:54:02-root-INFO: Undo step: 147
2024-12-02-06:54:02-root-INFO: Undo step: 148
2024-12-02-06:54:02-root-INFO: Undo step: 149
2024-12-02-06:54:02-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-06:54:02-root-INFO: grad norm: 232.480 227.748 46.665
2024-12-02-06:54:03-root-INFO: grad norm: 145.281 141.764 31.775
2024-12-02-06:54:03-root-INFO: grad norm: 247.404 241.719 52.731
2024-12-02-06:54:04-root-INFO: Loss too large (838.597->958.614)! Learning rate decreased to 0.00823.
2024-12-02-06:54:04-root-INFO: Loss too large (838.597->871.575)! Learning rate decreased to 0.00659.
2024-12-02-06:54:04-root-INFO: grad norm: 152.810 150.731 25.118
2024-12-02-06:54:05-root-INFO: grad norm: 70.350 68.452 16.229
2024-12-02-06:54:05-root-INFO: grad norm: 55.176 53.629 12.973
2024-12-02-06:54:06-root-INFO: grad norm: 47.605 45.723 13.253
2024-12-02-06:54:06-root-INFO: grad norm: 42.320 40.750 11.419
2024-12-02-06:54:06-root-INFO: Loss Change: 1073.707 -> 677.632
2024-12-02-06:54:06-root-INFO: Regularization Change: 0.000 -> 15.415
2024-12-02-06:54:06-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-06:54:06-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:54:07-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-06:54:07-root-INFO: grad norm: 58.669 56.986 13.949
2024-12-02-06:54:07-root-INFO: grad norm: 87.221 86.353 12.276
2024-12-02-06:54:07-root-INFO: Loss too large (672.750->685.908)! Learning rate decreased to 0.00854.
2024-12-02-06:54:08-root-INFO: grad norm: 93.589 92.548 13.922
2024-12-02-06:54:08-root-INFO: grad norm: 98.358 97.528 12.750
2024-12-02-06:54:09-root-INFO: grad norm: 100.333 99.420 13.505
2024-12-02-06:54:09-root-INFO: grad norm: 100.305 99.443 13.126
2024-12-02-06:54:10-root-INFO: grad norm: 99.051 98.182 13.092
2024-12-02-06:54:10-root-INFO: grad norm: 97.593 96.680 13.320
2024-12-02-06:54:11-root-INFO: Loss Change: 677.654 -> 639.780
2024-12-02-06:54:11-root-INFO: Regularization Change: 0.000 -> 3.947
2024-12-02-06:54:11-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-06:54:11-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:54:11-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-06:54:11-root-INFO: grad norm: 121.269 119.963 17.751
2024-12-02-06:54:11-root-INFO: Loss too large (652.974->670.679)! Learning rate decreased to 0.00885.
2024-12-02-06:54:12-root-INFO: grad norm: 106.619 105.789 13.276
2024-12-02-06:54:12-root-INFO: grad norm: 94.543 93.637 13.059
2024-12-02-06:54:12-root-INFO: grad norm: 83.372 82.619 11.183
2024-12-02-06:54:13-root-INFO: grad norm: 76.137 75.385 10.678
2024-12-02-06:54:13-root-INFO: grad norm: 69.976 69.292 9.759
2024-12-02-06:54:14-root-INFO: grad norm: 65.616 64.943 9.374
2024-12-02-06:54:14-root-INFO: grad norm: 61.889 61.241 8.934
2024-12-02-06:54:15-root-INFO: Loss Change: 652.974 -> 598.321
2024-12-02-06:54:15-root-INFO: Regularization Change: 0.000 -> 1.994
2024-12-02-06:54:15-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-06:54:15-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-06:54:15-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-06:54:15-root-INFO: grad norm: 79.782 78.859 12.101
2024-12-02-06:54:15-root-INFO: Loss too large (604.053->612.456)! Learning rate decreased to 0.00917.
2024-12-02-06:54:16-root-INFO: grad norm: 72.954 72.382 9.113
2024-12-02-06:54:16-root-INFO: grad norm: 67.063 66.411 9.326
2024-12-02-06:54:17-root-INFO: grad norm: 60.944 60.397 8.142
2024-12-02-06:54:17-root-INFO: grad norm: 56.665 56.105 7.948
2024-12-02-06:54:18-root-INFO: grad norm: 52.585 52.077 7.286
2024-12-02-06:54:18-root-INFO: grad norm: 49.638 49.130 7.084
2024-12-02-06:54:19-root-INFO: grad norm: 46.904 46.429 6.657
2024-12-02-06:54:19-root-INFO: Loss Change: 604.053 -> 573.829
2024-12-02-06:54:19-root-INFO: Regularization Change: 0.000 -> 1.450
2024-12-02-06:54:19-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-06:54:19-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:54:19-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-06:54:19-root-INFO: grad norm: 64.543 63.810 9.705
2024-12-02-06:54:19-root-INFO: Loss too large (577.991->584.223)! Learning rate decreased to 0.00951.
2024-12-02-06:54:20-root-INFO: grad norm: 60.234 59.719 7.862
2024-12-02-06:54:20-root-INFO: grad norm: 56.306 55.761 7.815
2024-12-02-06:54:21-root-INFO: grad norm: 52.361 51.849 7.303
2024-12-02-06:54:21-root-INFO: grad norm: 49.599 49.088 7.106
2024-12-02-06:54:22-root-INFO: grad norm: 47.070 46.560 6.906
2024-12-02-06:54:22-root-INFO: grad norm: 45.392 44.876 6.821
2024-12-02-06:54:23-root-INFO: grad norm: 43.917 43.398 6.732
2024-12-02-06:54:23-root-INFO: Loss Change: 577.991 -> 556.109
2024-12-02-06:54:23-root-INFO: Regularization Change: 0.000 -> 1.242
2024-12-02-06:54:23-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-06:54:23-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:54:23-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-06:54:23-root-INFO: grad norm: 70.205 69.309 11.178
2024-12-02-06:54:24-root-INFO: Loss too large (562.865->572.550)! Learning rate decreased to 0.00985.
2024-12-02-06:54:24-root-INFO: grad norm: 66.531 65.904 9.108
2024-12-02-06:54:25-root-INFO: grad norm: 62.180 61.504 9.144
2024-12-02-06:54:25-root-INFO: grad norm: 57.620 56.962 8.681
2024-12-02-06:54:26-root-INFO: grad norm: 53.988 53.342 8.327
2024-12-02-06:54:26-root-INFO: grad norm: 50.931 50.271 8.173
2024-12-02-06:54:26-root-INFO: grad norm: 48.477 47.832 7.881
2024-12-02-06:54:27-root-INFO: grad norm: 46.504 45.849 7.782
2024-12-02-06:54:27-root-INFO: Loss Change: 562.865 -> 540.997
2024-12-02-06:54:27-root-INFO: Regularization Change: 0.000 -> 1.181
2024-12-02-06:54:27-root-INFO: Undo step: 145
2024-12-02-06:54:27-root-INFO: Undo step: 146
2024-12-02-06:54:27-root-INFO: Undo step: 147
2024-12-02-06:54:27-root-INFO: Undo step: 148
2024-12-02-06:54:27-root-INFO: Undo step: 149
2024-12-02-06:54:28-root-INFO: step: 150 lr_xt 0.01029171
2024-12-02-06:54:28-root-INFO: grad norm: 285.380 279.165 59.238
2024-12-02-06:54:28-root-INFO: grad norm: 145.533 142.409 29.988
2024-12-02-06:54:29-root-INFO: grad norm: 115.063 112.772 22.850
2024-12-02-06:54:29-root-INFO: grad norm: 101.218 99.508 18.524
2024-12-02-06:54:30-root-INFO: grad norm: 100.280 98.041 21.073
2024-12-02-06:54:30-root-INFO: grad norm: 101.749 99.999 18.793
2024-12-02-06:54:31-root-INFO: grad norm: 109.165 106.885 22.192
2024-12-02-06:54:31-root-INFO: Loss too large (671.251->673.890)! Learning rate decreased to 0.00823.
2024-12-02-06:54:31-root-INFO: grad norm: 83.095 81.603 15.676
2024-12-02-06:54:32-root-INFO: Loss Change: 1212.162 -> 636.828
2024-12-02-06:54:32-root-INFO: Regularization Change: 0.000 -> 24.930
2024-12-02-06:54:32-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-02-06:54:32-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:54:32-root-INFO: step: 149 lr_xt 0.01067108
2024-12-02-06:54:32-root-INFO: grad norm: 75.374 74.181 13.358
2024-12-02-06:54:32-root-INFO: grad norm: 81.527 80.611 12.182
2024-12-02-06:54:33-root-INFO: grad norm: 90.924 89.835 14.030
2024-12-02-06:54:33-root-INFO: Loss too large (628.674->630.559)! Learning rate decreased to 0.00854.
2024-12-02-06:54:33-root-INFO: grad norm: 68.892 68.018 10.935
2024-12-02-06:54:34-root-INFO: grad norm: 52.371 51.455 9.755
2024-12-02-06:54:34-root-INFO: grad norm: 44.154 43.369 8.291
2024-12-02-06:54:35-root-INFO: grad norm: 38.804 37.900 8.328
2024-12-02-06:54:35-root-INFO: grad norm: 35.870 35.072 7.527
2024-12-02-06:54:36-root-INFO: Loss Change: 638.025 -> 587.178
2024-12-02-06:54:36-root-INFO: Regularization Change: 0.000 -> 3.008
2024-12-02-06:54:36-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-02-06:54:36-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00027.
2024-12-02-06:54:36-root-INFO: step: 148 lr_xt 0.01106266
2024-12-02-06:54:36-root-INFO: grad norm: 53.071 52.156 9.808
2024-12-02-06:54:37-root-INFO: grad norm: 63.077 62.511 8.438
2024-12-02-06:54:37-root-INFO: Loss too large (588.060->588.455)! Learning rate decreased to 0.00885.
2024-12-02-06:54:37-root-INFO: grad norm: 50.828 50.247 7.667
2024-12-02-06:54:38-root-INFO: grad norm: 42.877 42.337 6.781
2024-12-02-06:54:38-root-INFO: grad norm: 36.928 36.378 6.348
2024-12-02-06:54:38-root-INFO: grad norm: 32.676 32.125 5.978
2024-12-02-06:54:39-root-INFO: grad norm: 29.450 28.891 5.706
2024-12-02-06:54:39-root-INFO: grad norm: 27.012 26.442 5.522
2024-12-02-06:54:40-root-INFO: Loss Change: 589.080 -> 560.858
2024-12-02-06:54:40-root-INFO: Regularization Change: 0.000 -> 1.718
2024-12-02-06:54:40-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-02-06:54:40-root-INFO: Coefficient of regularization decay: 0.00027 -> 0.00028.
2024-12-02-06:54:40-root-INFO: step: 147 lr_xt 0.01146675
2024-12-02-06:54:40-root-INFO: grad norm: 44.878 44.156 8.019
2024-12-02-06:54:40-root-INFO: Loss too large (561.994->562.131)! Learning rate decreased to 0.00917.
2024-12-02-06:54:41-root-INFO: grad norm: 40.523 40.068 6.056
2024-12-02-06:54:41-root-INFO: grad norm: 37.206 36.704 6.096
2024-12-02-06:54:42-root-INFO: grad norm: 34.958 34.466 5.844
2024-12-02-06:54:42-root-INFO: grad norm: 33.403 32.903 5.754
2024-12-02-06:54:43-root-INFO: grad norm: 32.753 32.209 5.943
2024-12-02-06:54:43-root-INFO: grad norm: 32.660 32.118 5.925
2024-12-02-06:54:44-root-INFO: grad norm: 33.713 33.088 6.463
2024-12-02-06:54:44-root-INFO: Loss Change: 561.994 -> 543.256
2024-12-02-06:54:44-root-INFO: Regularization Change: 0.000 -> 1.257
2024-12-02-06:54:44-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-02-06:54:44-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:54:44-root-INFO: step: 146 lr_xt 0.01188369
2024-12-02-06:54:44-root-INFO: grad norm: 53.566 52.748 9.325
2024-12-02-06:54:44-root-INFO: Loss too large (546.292->549.967)! Learning rate decreased to 0.00951.
2024-12-02-06:54:45-root-INFO: grad norm: 51.361 50.659 8.463
2024-12-02-06:54:45-root-INFO: grad norm: 49.251 48.565 8.191
2024-12-02-06:54:46-root-INFO: grad norm: 47.630 46.890 8.365
2024-12-02-06:54:46-root-INFO: grad norm: 46.288 45.594 7.985
2024-12-02-06:54:47-root-INFO: grad norm: 45.310 44.537 8.335
2024-12-02-06:54:47-root-INFO: grad norm: 44.534 43.820 7.939
2024-12-02-06:54:48-root-INFO: grad norm: 44.061 43.260 8.363
2024-12-02-06:54:48-root-INFO: Loss Change: 546.292 -> 529.892
2024-12-02-06:54:48-root-INFO: Regularization Change: 0.000 -> 1.089
2024-12-02-06:54:48-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-02-06:54:48-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:54:48-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-06:54:48-root-INFO: grad norm: 70.912 69.822 12.386
2024-12-02-06:54:48-root-INFO: Loss too large (537.243->545.074)! Learning rate decreased to 0.00985.
2024-12-02-06:54:49-root-INFO: grad norm: 65.349 64.494 10.538
2024-12-02-06:54:49-root-INFO: grad norm: 59.849 59.018 9.933
2024-12-02-06:54:50-root-INFO: grad norm: 54.314 53.476 9.507
2024-12-02-06:54:50-root-INFO: grad norm: 50.457 49.689 8.771
2024-12-02-06:54:51-root-INFO: grad norm: 46.777 45.957 8.722
2024-12-02-06:54:51-root-INFO: grad norm: 44.178 43.440 8.043
2024-12-02-06:54:52-root-INFO: grad norm: 41.746 40.945 8.140
2024-12-02-06:54:52-root-INFO: Loss Change: 537.243 -> 514.933
2024-12-02-06:54:52-root-INFO: Regularization Change: 0.000 -> 1.065
2024-12-02-06:54:52-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-06:54:52-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:54:52-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-06:54:53-root-INFO: grad norm: 58.576 57.509 11.132
2024-12-02-06:54:53-root-INFO: Loss too large (518.879->522.909)! Learning rate decreased to 0.01021.
2024-12-02-06:54:53-root-INFO: grad norm: 51.375 50.556 9.134
2024-12-02-06:54:54-root-INFO: grad norm: 45.753 44.945 8.565
2024-12-02-06:54:54-root-INFO: grad norm: 40.718 39.910 8.071
2024-12-02-06:54:55-root-INFO: grad norm: 37.018 36.276 7.374
2024-12-02-06:54:55-root-INFO: grad norm: 33.702 32.917 7.232
2024-12-02-06:54:56-root-INFO: grad norm: 31.203 30.497 6.599
2024-12-02-06:54:56-root-INFO: grad norm: 28.962 28.202 6.590
2024-12-02-06:54:57-root-INFO: Loss Change: 518.879 -> 499.310
2024-12-02-06:54:57-root-INFO: Regularization Change: 0.000 -> 1.008
2024-12-02-06:54:57-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-06:54:57-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-06:54:57-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-06:54:57-root-INFO: grad norm: 46.172 45.224 9.309
2024-12-02-06:54:57-root-INFO: Loss too large (502.068->504.295)! Learning rate decreased to 0.01057.
2024-12-02-06:54:58-root-INFO: grad norm: 41.454 40.644 8.155
2024-12-02-06:54:58-root-INFO: grad norm: 37.790 37.043 7.476
2024-12-02-06:54:59-root-INFO: grad norm: 34.446 33.665 7.293
2024-12-02-06:54:59-root-INFO: grad norm: 31.792 31.100 6.596
2024-12-02-06:55:00-root-INFO: grad norm: 29.395 28.646 6.590
2024-12-02-06:55:00-root-INFO: grad norm: 27.497 26.837 5.989
2024-12-02-06:55:01-root-INFO: grad norm: 25.810 25.093 6.041
2024-12-02-06:55:01-root-INFO: Loss Change: 502.068 -> 486.614
2024-12-02-06:55:01-root-INFO: Regularization Change: 0.000 -> 0.953
2024-12-02-06:55:01-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-06:55:01-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-06:55:02-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-06:55:02-root-INFO: grad norm: 50.991 49.813 10.899
2024-12-02-06:55:02-root-INFO: Loss too large (491.889->494.934)! Learning rate decreased to 0.01095.
2024-12-02-06:55:03-root-INFO: grad norm: 45.299 44.528 8.326
2024-12-02-06:55:03-root-INFO: grad norm: 40.379 39.604 7.874
2024-12-02-06:55:04-root-INFO: grad norm: 36.242 35.525 7.174
2024-12-02-06:55:04-root-INFO: grad norm: 32.910 32.251 6.557
2024-12-02-06:55:05-root-INFO: grad norm: 29.964 29.299 6.277
2024-12-02-06:55:06-root-INFO: grad norm: 27.666 27.065 5.736
2024-12-02-06:55:06-root-INFO: grad norm: 25.676 25.053 5.623
2024-12-02-06:55:07-root-INFO: Loss Change: 491.889 -> 474.970
2024-12-02-06:55:07-root-INFO: Regularization Change: 0.000 -> 0.993
2024-12-02-06:55:07-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-06:55:07-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-06:55:07-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-06:55:07-root-INFO: grad norm: 36.640 35.876 7.441
2024-12-02-06:55:07-root-INFO: Loss too large (476.610->477.266)! Learning rate decreased to 0.01134.
2024-12-02-06:55:08-root-INFO: grad norm: 31.952 31.379 6.027
2024-12-02-06:55:08-root-INFO: grad norm: 28.569 28.010 5.622
2024-12-02-06:55:09-root-INFO: grad norm: 25.645 25.093 5.289
2024-12-02-06:55:09-root-INFO: grad norm: 23.385 22.874 4.858
2024-12-02-06:55:10-root-INFO: grad norm: 21.464 20.937 4.727
2024-12-02-06:55:11-root-INFO: grad norm: 20.012 19.524 4.392
2024-12-02-06:55:11-root-INFO: grad norm: 18.860 18.354 4.340
2024-12-02-06:55:11-root-INFO: Loss Change: 476.610 -> 463.404
2024-12-02-06:55:11-root-INFO: Regularization Change: 0.000 -> 0.928
2024-12-02-06:55:11-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-06:55:11-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-06:55:12-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-06:55:12-root-INFO: grad norm: 33.950 33.195 7.117
2024-12-02-06:55:12-root-INFO: Loss too large (465.653->465.807)! Learning rate decreased to 0.01174.
2024-12-02-06:55:13-root-INFO: grad norm: 29.609 29.062 5.667
2024-12-02-06:55:13-root-INFO: grad norm: 26.830 26.293 5.340
2024-12-02-06:55:14-root-INFO: grad norm: 24.623 24.091 5.094
2024-12-02-06:55:14-root-INFO: grad norm: 22.923 22.426 4.746
2024-12-02-06:55:15-root-INFO: grad norm: 21.658 21.143 4.695
2024-12-02-06:55:15-root-INFO: grad norm: 20.780 20.299 4.447
2024-12-02-06:55:16-root-INFO: grad norm: 20.385 19.890 4.465
2024-12-02-06:55:16-root-INFO: Loss Change: 465.653 -> 453.321
2024-12-02-06:55:16-root-INFO: Regularization Change: 0.000 -> 0.949
2024-12-02-06:55:16-root-INFO: Undo step: 140
2024-12-02-06:55:16-root-INFO: Undo step: 141
2024-12-02-06:55:16-root-INFO: Undo step: 142
2024-12-02-06:55:16-root-INFO: Undo step: 143
2024-12-02-06:55:16-root-INFO: Undo step: 144
2024-12-02-06:55:16-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-06:55:16-root-INFO: grad norm: 289.461 283.560 58.151
2024-12-02-06:55:17-root-INFO: grad norm: 142.129 137.758 34.978
2024-12-02-06:55:17-root-INFO: grad norm: 72.829 70.877 16.750
2024-12-02-06:55:18-root-INFO: grad norm: 56.417 54.866 13.134
2024-12-02-06:55:18-root-INFO: grad norm: 51.045 49.983 10.361
2024-12-02-06:55:19-root-INFO: grad norm: 44.675 43.663 9.452
2024-12-02-06:55:19-root-INFO: grad norm: 55.185 53.828 12.162
2024-12-02-06:55:20-root-INFO: Loss too large (546.238->546.430)! Learning rate decreased to 0.00985.
2024-12-02-06:55:20-root-INFO: grad norm: 52.777 51.751 10.359
2024-12-02-06:55:20-root-INFO: Loss Change: 1121.726 -> 533.325
2024-12-02-06:55:20-root-INFO: Regularization Change: 0.000 -> 27.612
2024-12-02-06:55:20-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-06:55:20-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:55:21-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-06:55:21-root-INFO: grad norm: 45.370 44.251 10.018
2024-12-02-06:55:21-root-INFO: grad norm: 50.952 49.924 10.183
2024-12-02-06:55:22-root-INFO: grad norm: 69.321 67.838 14.262
2024-12-02-06:55:22-root-INFO: Loss too large (524.547->533.782)! Learning rate decreased to 0.01021.
2024-12-02-06:55:23-root-INFO: grad norm: 57.808 56.712 11.204
2024-12-02-06:55:23-root-INFO: grad norm: 39.279 38.345 8.517
2024-12-02-06:55:24-root-INFO: grad norm: 35.740 34.936 7.538
2024-12-02-06:55:24-root-INFO: grad norm: 31.985 31.171 7.169
2024-12-02-06:55:25-root-INFO: grad norm: 30.320 29.603 6.551
2024-12-02-06:55:25-root-INFO: Loss Change: 530.924 -> 499.298
2024-12-02-06:55:25-root-INFO: Regularization Change: 0.000 -> 2.533
2024-12-02-06:55:25-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-06:55:25-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-06:55:26-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-06:55:26-root-INFO: grad norm: 31.338 30.748 6.053
2024-12-02-06:55:26-root-INFO: grad norm: 36.999 36.446 6.373
2024-12-02-06:55:27-root-INFO: grad norm: 50.424 49.826 7.742
2024-12-02-06:55:27-root-INFO: Loss too large (495.855->499.769)! Learning rate decreased to 0.01057.
2024-12-02-06:55:28-root-INFO: grad norm: 43.468 42.883 7.109
2024-12-02-06:55:28-root-INFO: grad norm: 34.696 34.198 5.860
2024-12-02-06:55:29-root-INFO: grad norm: 31.560 31.054 5.629
2024-12-02-06:55:29-root-INFO: grad norm: 28.878 28.393 5.272
2024-12-02-06:55:30-root-INFO: grad norm: 27.072 26.581 5.130
2024-12-02-06:55:30-root-INFO: Loss Change: 498.349 -> 480.088
2024-12-02-06:55:30-root-INFO: Regularization Change: 0.000 -> 1.571
2024-12-02-06:55:30-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-06:55:30-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-06:55:30-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-06:55:31-root-INFO: grad norm: 46.556 45.875 7.934
2024-12-02-06:55:31-root-INFO: Loss too large (483.594->484.963)! Learning rate decreased to 0.01095.
2024-12-02-06:55:31-root-INFO: grad norm: 40.651 40.264 5.597
2024-12-02-06:55:32-root-INFO: grad norm: 36.264 35.871 5.329
2024-12-02-06:55:32-root-INFO: grad norm: 32.986 32.603 5.009
2024-12-02-06:55:33-root-INFO: grad norm: 30.295 29.923 4.735
2024-12-02-06:55:33-root-INFO: grad norm: 28.220 27.825 4.702
2024-12-02-06:55:34-root-INFO: grad norm: 26.631 26.248 4.502
2024-12-02-06:55:34-root-INFO: grad norm: 25.422 25.012 4.544
2024-12-02-06:55:35-root-INFO: Loss Change: 483.594 -> 465.532
2024-12-02-06:55:35-root-INFO: Regularization Change: 0.000 -> 1.207
2024-12-02-06:55:35-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-06:55:35-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-06:55:35-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-06:55:35-root-INFO: grad norm: 35.643 35.243 5.322
2024-12-02-06:55:35-root-INFO: Loss too large (466.562->467.495)! Learning rate decreased to 0.01134.
2024-12-02-06:55:36-root-INFO: grad norm: 32.192 31.842 4.730
2024-12-02-06:55:36-root-INFO: grad norm: 29.703 29.369 4.442
2024-12-02-06:55:37-root-INFO: grad norm: 27.905 27.532 4.549
2024-12-02-06:55:37-root-INFO: grad norm: 26.475 26.123 4.302
2024-12-02-06:55:38-root-INFO: grad norm: 25.411 25.020 4.437
2024-12-02-06:55:38-root-INFO: grad norm: 24.616 24.244 4.261
2024-12-02-06:55:39-root-INFO: grad norm: 24.013 23.611 4.378
2024-12-02-06:55:39-root-INFO: Loss Change: 466.562 -> 453.250
2024-12-02-06:55:39-root-INFO: Regularization Change: 0.000 -> 1.027
2024-12-02-06:55:39-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-06:55:39-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-06:55:39-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-06:55:39-root-INFO: grad norm: 36.885 36.450 5.648
2024-12-02-06:55:40-root-INFO: Loss too large (454.858->456.229)! Learning rate decreased to 0.01174.
2024-12-02-06:55:40-root-INFO: grad norm: 33.373 33.033 4.755
2024-12-02-06:55:41-root-INFO: grad norm: 31.202 30.877 4.490
2024-12-02-06:55:41-root-INFO: grad norm: 29.534 29.171 4.617
2024-12-02-06:55:42-root-INFO: grad norm: 28.258 27.919 4.367
2024-12-02-06:55:42-root-INFO: grad norm: 27.233 26.851 4.542
2024-12-02-06:55:43-root-INFO: grad norm: 26.460 26.101 4.341
2024-12-02-06:55:43-root-INFO: grad norm: 25.833 25.439 4.492
2024-12-02-06:55:43-root-INFO: Loss Change: 454.858 -> 442.229
2024-12-02-06:55:43-root-INFO: Regularization Change: 0.000 -> 0.985
2024-12-02-06:55:43-root-INFO: Undo step: 140
2024-12-02-06:55:43-root-INFO: Undo step: 141
2024-12-02-06:55:43-root-INFO: Undo step: 142
2024-12-02-06:55:43-root-INFO: Undo step: 143
2024-12-02-06:55:43-root-INFO: Undo step: 144
2024-12-02-06:55:44-root-INFO: step: 145 lr_xt 0.01231381
2024-12-02-06:55:44-root-INFO: grad norm: 309.601 303.896 59.162
2024-12-02-06:55:44-root-INFO: grad norm: 177.871 175.431 29.358
2024-12-02-06:55:45-root-INFO: grad norm: 147.208 143.886 31.096
2024-12-02-06:55:45-root-INFO: grad norm: 167.890 165.658 27.290
2024-12-02-06:55:45-root-INFO: Loss too large (639.078->684.328)! Learning rate decreased to 0.00985.
2024-12-02-06:55:46-root-INFO: grad norm: 123.911 121.647 23.576
2024-12-02-06:55:47-root-INFO: grad norm: 50.275 48.956 11.437
2024-12-02-06:55:47-root-INFO: grad norm: 42.681 41.352 10.569
2024-12-02-06:55:48-root-INFO: grad norm: 37.547 36.469 8.935
2024-12-02-06:55:48-root-INFO: Loss Change: 1168.390 -> 528.931
2024-12-02-06:55:48-root-INFO: Regularization Change: 0.000 -> 29.651
2024-12-02-06:55:48-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-02-06:55:48-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00028.
2024-12-02-06:55:48-root-INFO: step: 144 lr_xt 0.01275743
2024-12-02-06:55:48-root-INFO: grad norm: 43.248 42.335 8.842
2024-12-02-06:55:49-root-INFO: grad norm: 52.017 51.061 9.927
2024-12-02-06:55:49-root-INFO: grad norm: 59.963 59.095 10.168
2024-12-02-06:55:50-root-INFO: grad norm: 80.516 79.330 13.764
2024-12-02-06:55:50-root-INFO: Loss too large (514.936->527.715)! Learning rate decreased to 0.01021.
2024-12-02-06:55:51-root-INFO: grad norm: 66.205 65.295 10.941
2024-12-02-06:55:51-root-INFO: grad norm: 43.311 42.509 8.295
2024-12-02-06:55:52-root-INFO: grad norm: 37.401 36.734 7.031
2024-12-02-06:55:52-root-INFO: grad norm: 31.258 30.556 6.587
2024-12-02-06:55:53-root-INFO: Loss Change: 527.613 -> 487.029
2024-12-02-06:55:53-root-INFO: Regularization Change: 0.000 -> 3.436
2024-12-02-06:55:53-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-02-06:55:53-root-INFO: Coefficient of regularization decay: 0.00028 -> 0.00029.
2024-12-02-06:55:53-root-INFO: step: 143 lr_xt 0.01321490
2024-12-02-06:55:53-root-INFO: grad norm: 43.315 42.580 7.941
2024-12-02-06:55:54-root-INFO: grad norm: 53.040 52.268 9.020
2024-12-02-06:55:54-root-INFO: Loss too large (486.391->488.962)! Learning rate decreased to 0.01057.
2024-12-02-06:55:54-root-INFO: grad norm: 43.837 43.231 7.262
2024-12-02-06:55:55-root-INFO: grad norm: 34.608 33.981 6.560
2024-12-02-06:55:55-root-INFO: grad norm: 30.045 29.518 5.604
2024-12-02-06:55:56-root-INFO: grad norm: 25.801 25.234 5.379
2024-12-02-06:55:56-root-INFO: grad norm: 23.364 22.856 4.844
2024-12-02-06:55:57-root-INFO: grad norm: 21.128 20.592 4.728
2024-12-02-06:55:57-root-INFO: Loss Change: 488.584 -> 465.308
2024-12-02-06:55:57-root-INFO: Regularization Change: 0.000 -> 1.666
2024-12-02-06:55:57-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-02-06:55:57-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-06:55:57-root-INFO: step: 142 lr_xt 0.01368658
2024-12-02-06:55:57-root-INFO: grad norm: 40.975 40.075 8.541
2024-12-02-06:55:58-root-INFO: grad norm: 48.142 47.509 7.781
2024-12-02-06:55:58-root-INFO: Loss too large (466.673->468.395)! Learning rate decreased to 0.01095.
2024-12-02-06:55:58-root-INFO: grad norm: 39.106 38.564 6.483
2024-12-02-06:55:59-root-INFO: grad norm: 31.949 31.435 5.707
2024-12-02-06:55:59-root-INFO: grad norm: 27.660 27.202 5.013
2024-12-02-06:56:00-root-INFO: grad norm: 24.121 23.654 4.723
2024-12-02-06:56:00-root-INFO: grad norm: 21.932 21.494 4.360
2024-12-02-06:56:01-root-INFO: grad norm: 20.093 19.649 4.202
2024-12-02-06:56:01-root-INFO: Loss Change: 468.145 -> 449.763
2024-12-02-06:56:01-root-INFO: Regularization Change: 0.000 -> 1.312
2024-12-02-06:56:01-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-02-06:56:01-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00029.
2024-12-02-06:56:01-root-INFO: step: 141 lr_xt 0.01417280
2024-12-02-06:56:02-root-INFO: grad norm: 28.698 28.155 5.556
2024-12-02-06:56:02-root-INFO: grad norm: 34.492 34.032 5.614
2024-12-02-06:56:02-root-INFO: Loss too large (448.811->449.422)! Learning rate decreased to 0.01134.
2024-12-02-06:56:03-root-INFO: grad norm: 29.374 28.951 4.966
2024-12-02-06:56:03-root-INFO: grad norm: 25.212 24.803 4.521
2024-12-02-06:56:04-root-INFO: grad norm: 22.680 22.287 4.205
2024-12-02-06:56:04-root-INFO: grad norm: 20.670 20.284 3.974
2024-12-02-06:56:05-root-INFO: grad norm: 19.406 19.021 3.844
2024-12-02-06:56:05-root-INFO: grad norm: 18.401 18.030 3.678
2024-12-02-06:56:06-root-INFO: Loss Change: 450.075 -> 437.044
2024-12-02-06:56:06-root-INFO: Regularization Change: 0.000 -> 1.078
2024-12-02-06:56:06-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-02-06:56:06-root-INFO: Coefficient of regularization decay: 0.00029 -> 0.00030.
2024-12-02-06:56:06-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-06:56:06-root-INFO: grad norm: 30.091 29.496 5.951
2024-12-02-06:56:06-root-INFO: grad norm: 36.632 36.190 5.674
2024-12-02-06:56:06-root-INFO: Loss too large (437.012->438.411)! Learning rate decreased to 0.01174.
2024-12-02-06:56:07-root-INFO: grad norm: 31.377 30.965 5.067
2024-12-02-06:56:07-root-INFO: grad norm: 27.196 26.817 4.525
2024-12-02-06:56:08-root-INFO: grad norm: 24.608 24.237 4.256
2024-12-02-06:56:09-root-INFO: grad norm: 22.522 22.168 3.975
2024-12-02-06:56:09-root-INFO: grad norm: 21.215 20.857 3.882
2024-12-02-06:56:09-root-INFO: grad norm: 20.157 19.818 3.682
2024-12-02-06:56:10-root-INFO: Loss Change: 437.952 -> 425.947
2024-12-02-06:56:10-root-INFO: Regularization Change: 0.000 -> 1.004
2024-12-02-06:56:10-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-06:56:10-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:56:10-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-06:56:10-root-INFO: grad norm: 32.002 31.425 6.046
2024-12-02-06:56:11-root-INFO: grad norm: 40.832 40.427 5.738
2024-12-02-06:56:11-root-INFO: Loss too large (427.204->430.029)! Learning rate decreased to 0.01215.
2024-12-02-06:56:11-root-INFO: grad norm: 35.020 34.602 5.398
2024-12-02-06:56:12-root-INFO: grad norm: 29.639 29.283 4.585
2024-12-02-06:56:12-root-INFO: grad norm: 26.793 26.431 4.386
2024-12-02-06:56:13-root-INFO: grad norm: 24.365 24.032 4.014
2024-12-02-06:56:13-root-INFO: grad norm: 22.897 22.557 3.934
2024-12-02-06:56:14-root-INFO: grad norm: 21.661 21.342 3.700
2024-12-02-06:56:14-root-INFO: Loss Change: 427.518 -> 416.081
2024-12-02-06:56:14-root-INFO: Regularization Change: 0.000 -> 0.955
2024-12-02-06:56:14-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-06:56:14-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:56:14-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-06:56:14-root-INFO: grad norm: 28.152 27.698 5.036
2024-12-02-06:56:15-root-INFO: grad norm: 36.601 36.202 5.392
2024-12-02-06:56:15-root-INFO: Loss too large (416.391->418.775)! Learning rate decreased to 0.01258.
2024-12-02-06:56:15-root-INFO: grad norm: 31.466 31.094 4.829
2024-12-02-06:56:16-root-INFO: grad norm: 26.414 26.078 4.202
2024-12-02-06:56:16-root-INFO: grad norm: 24.168 23.839 3.974
2024-12-02-06:56:17-root-INFO: grad norm: 22.224 21.910 3.721
2024-12-02-06:56:17-root-INFO: grad norm: 21.109 20.794 3.635
2024-12-02-06:56:18-root-INFO: grad norm: 20.171 19.870 3.472
2024-12-02-06:56:18-root-INFO: Loss Change: 416.662 -> 406.614
2024-12-02-06:56:18-root-INFO: Regularization Change: 0.000 -> 0.899
2024-12-02-06:56:18-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-06:56:18-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:56:19-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-06:56:19-root-INFO: grad norm: 35.961 35.205 7.339
2024-12-02-06:56:19-root-INFO: grad norm: 43.585 43.141 6.203
2024-12-02-06:56:19-root-INFO: Loss too large (409.002->412.149)! Learning rate decreased to 0.01302.
2024-12-02-06:56:20-root-INFO: grad norm: 36.537 36.132 5.429
2024-12-02-06:56:20-root-INFO: grad norm: 31.072 30.723 4.643
2024-12-02-06:56:21-root-INFO: grad norm: 27.828 27.495 4.288
2024-12-02-06:56:21-root-INFO: grad norm: 25.288 24.966 4.020
2024-12-02-06:56:22-root-INFO: grad norm: 23.671 23.359 3.833
2024-12-02-06:56:23-root-INFO: grad norm: 22.409 22.103 3.695
2024-12-02-06:56:23-root-INFO: Loss Change: 409.375 -> 397.217
2024-12-02-06:56:23-root-INFO: Regularization Change: 0.000 -> 0.996
2024-12-02-06:56:23-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-06:56:23-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-06:56:23-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-06:56:23-root-INFO: grad norm: 25.901 25.522 4.416
2024-12-02-06:56:24-root-INFO: grad norm: 34.253 33.910 4.831
2024-12-02-06:56:24-root-INFO: Loss too large (397.622->400.046)! Learning rate decreased to 0.01347.
2024-12-02-06:56:24-root-INFO: grad norm: 29.963 29.623 4.505
2024-12-02-06:56:25-root-INFO: grad norm: 25.625 25.315 3.974
2024-12-02-06:56:25-root-INFO: grad norm: 23.684 23.379 3.784
2024-12-02-06:56:26-root-INFO: grad norm: 21.928 21.632 3.587
2024-12-02-06:56:26-root-INFO: grad norm: 20.875 20.584 3.474
2024-12-02-06:56:27-root-INFO: grad norm: 19.944 19.658 3.365
2024-12-02-06:56:27-root-INFO: Loss Change: 397.767 -> 388.712
2024-12-02-06:56:27-root-INFO: Regularization Change: 0.000 -> 0.883
2024-12-02-06:56:27-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-06:56:27-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-06:56:28-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-06:56:28-root-INFO: grad norm: 27.556 27.111 4.936
2024-12-02-06:56:28-root-INFO: grad norm: 35.009 34.665 4.896
2024-12-02-06:56:28-root-INFO: Loss too large (389.126->391.647)! Learning rate decreased to 0.01393.
2024-12-02-06:56:29-root-INFO: grad norm: 30.566 30.240 4.457
2024-12-02-06:56:29-root-INFO: grad norm: 26.543 26.237 4.020
2024-12-02-06:56:30-root-INFO: grad norm: 24.392 24.104 3.743
2024-12-02-06:56:31-root-INFO: grad norm: 22.486 22.194 3.612
2024-12-02-06:56:31-root-INFO: grad norm: 21.236 20.960 3.412
2024-12-02-06:56:32-root-INFO: grad norm: 20.142 19.859 3.364
2024-12-02-06:56:32-root-INFO: Loss Change: 389.385 -> 380.036
2024-12-02-06:56:32-root-INFO: Regularization Change: 0.000 -> 0.899
2024-12-02-06:56:32-root-INFO: Undo step: 135
2024-12-02-06:56:32-root-INFO: Undo step: 136
2024-12-02-06:56:32-root-INFO: Undo step: 137
2024-12-02-06:56:32-root-INFO: Undo step: 138
2024-12-02-06:56:32-root-INFO: Undo step: 139
2024-12-02-06:56:32-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-06:56:32-root-INFO: grad norm: 202.208 197.579 43.020
2024-12-02-06:56:33-root-INFO: grad norm: 102.251 98.836 26.207
2024-12-02-06:56:33-root-INFO: grad norm: 86.130 84.139 18.413
2024-12-02-06:56:34-root-INFO: grad norm: 97.055 94.657 21.440
2024-12-02-06:56:35-root-INFO: grad norm: 80.070 78.513 15.712
2024-12-02-06:56:35-root-INFO: grad norm: 64.348 62.761 14.204
2024-12-02-06:56:36-root-INFO: grad norm: 58.800 57.681 11.419
2024-12-02-06:56:36-root-INFO: grad norm: 54.421 52.965 12.502
2024-12-02-06:56:36-root-INFO: Loss Change: 936.571 -> 466.432
2024-12-02-06:56:36-root-INFO: Regularization Change: 0.000 -> 31.592
2024-12-02-06:56:36-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-06:56:36-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:56:37-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-06:56:37-root-INFO: grad norm: 73.855 72.918 11.728
2024-12-02-06:56:37-root-INFO: Loss too large (469.368->469.382)! Learning rate decreased to 0.01215.
2024-12-02-06:56:37-root-INFO: grad norm: 49.560 48.597 9.724
2024-12-02-06:56:38-root-INFO: grad norm: 27.756 26.934 6.705
2024-12-02-06:56:38-root-INFO: grad norm: 22.734 21.767 6.560
2024-12-02-06:56:39-root-INFO: grad norm: 22.464 21.728 5.705
2024-12-02-06:56:39-root-INFO: grad norm: 23.980 23.277 5.765
2024-12-02-06:56:40-root-INFO: grad norm: 29.791 29.194 5.938
2024-12-02-06:56:40-root-INFO: grad norm: 31.949 31.393 5.934
2024-12-02-06:56:41-root-INFO: Loss Change: 469.368 -> 424.302
2024-12-02-06:56:41-root-INFO: Regularization Change: 0.000 -> 3.273
2024-12-02-06:56:41-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-06:56:41-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:56:41-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-06:56:41-root-INFO: grad norm: 43.446 42.943 6.588
2024-12-02-06:56:41-root-INFO: Loss too large (424.745->428.881)! Learning rate decreased to 0.01258.
2024-12-02-06:56:42-root-INFO: Loss too large (424.745->425.008)! Learning rate decreased to 0.01006.
2024-12-02-06:56:42-root-INFO: grad norm: 32.147 31.639 5.690
2024-12-02-06:56:43-root-INFO: grad norm: 19.812 19.256 4.664
2024-12-02-06:56:43-root-INFO: grad norm: 18.684 18.139 4.479
2024-12-02-06:56:44-root-INFO: grad norm: 17.768 17.216 4.394
2024-12-02-06:56:44-root-INFO: grad norm: 17.260 16.739 4.211
2024-12-02-06:56:45-root-INFO: grad norm: 16.800 16.271 4.182
2024-12-02-06:56:45-root-INFO: grad norm: 16.513 16.016 4.018
2024-12-02-06:56:46-root-INFO: Loss Change: 424.745 -> 406.751
2024-12-02-06:56:46-root-INFO: Regularization Change: 0.000 -> 1.190
2024-12-02-06:56:46-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-06:56:46-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:56:46-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-06:56:46-root-INFO: grad norm: 33.210 32.588 6.396
2024-12-02-06:56:46-root-INFO: grad norm: 39.065 38.562 6.253
2024-12-02-06:56:47-root-INFO: Loss too large (407.819->409.869)! Learning rate decreased to 0.01302.
2024-12-02-06:56:47-root-INFO: grad norm: 42.291 41.903 5.715
2024-12-02-06:56:47-root-INFO: Loss too large (404.183->405.861)! Learning rate decreased to 0.01041.
2024-12-02-06:56:48-root-INFO: grad norm: 31.590 31.174 5.112
2024-12-02-06:56:48-root-INFO: grad norm: 19.358 18.952 3.941
2024-12-02-06:56:49-root-INFO: grad norm: 18.473 18.065 3.864
2024-12-02-06:56:49-root-INFO: grad norm: 17.718 17.313 3.765
2024-12-02-06:56:50-root-INFO: grad norm: 17.369 16.976 3.675
2024-12-02-06:56:50-root-INFO: Loss Change: 408.472 -> 393.092
2024-12-02-06:56:50-root-INFO: Regularization Change: 0.000 -> 1.185
2024-12-02-06:56:50-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-06:56:50-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-06:56:50-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-06:56:50-root-INFO: grad norm: 21.875 21.531 3.864
2024-12-02-06:56:50-root-INFO: Loss too large (393.396->393.498)! Learning rate decreased to 0.01347.
2024-12-02-06:56:51-root-INFO: grad norm: 24.124 23.777 4.072
2024-12-02-06:56:52-root-INFO: grad norm: 32.491 32.150 4.691
2024-12-02-06:56:52-root-INFO: Loss too large (391.439->392.856)! Learning rate decreased to 0.01077.
2024-12-02-06:56:52-root-INFO: grad norm: 27.255 26.909 4.331
2024-12-02-06:56:53-root-INFO: grad norm: 20.191 19.860 3.640
2024-12-02-06:56:53-root-INFO: grad norm: 19.046 18.716 3.532
2024-12-02-06:56:54-root-INFO: grad norm: 17.732 17.401 3.409
2024-12-02-06:56:54-root-INFO: grad norm: 17.149 16.825 3.320
2024-12-02-06:56:55-root-INFO: Loss Change: 393.396 -> 383.532
2024-12-02-06:56:55-root-INFO: Regularization Change: 0.000 -> 0.854
2024-12-02-06:56:55-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-06:56:55-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-06:56:55-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-06:56:55-root-INFO: grad norm: 24.735 24.396 4.078
2024-12-02-06:56:55-root-INFO: Loss too large (383.958->384.282)! Learning rate decreased to 0.01393.
2024-12-02-06:56:56-root-INFO: grad norm: 25.239 24.912 4.052
2024-12-02-06:56:56-root-INFO: grad norm: 33.126 32.825 4.454
2024-12-02-06:56:56-root-INFO: Loss too large (381.858->383.518)! Learning rate decreased to 0.01115.
2024-12-02-06:56:57-root-INFO: grad norm: 27.218 26.897 4.169
2024-12-02-06:56:58-root-INFO: grad norm: 19.668 19.378 3.360
2024-12-02-06:56:58-root-INFO: grad norm: 18.445 18.149 3.290
2024-12-02-06:56:59-root-INFO: grad norm: 17.051 16.760 3.140
2024-12-02-06:56:59-root-INFO: grad norm: 16.413 16.122 3.075
2024-12-02-06:56:59-root-INFO: Loss Change: 383.958 -> 374.432
2024-12-02-06:56:59-root-INFO: Regularization Change: 0.000 -> 0.793
2024-12-02-06:56:59-root-INFO: Undo step: 135
2024-12-02-06:56:59-root-INFO: Undo step: 136
2024-12-02-06:56:59-root-INFO: Undo step: 137
2024-12-02-06:56:59-root-INFO: Undo step: 138
2024-12-02-06:56:59-root-INFO: Undo step: 139
2024-12-02-06:57:00-root-INFO: step: 140 lr_xt 0.01467393
2024-12-02-06:57:00-root-INFO: grad norm: 198.888 194.635 40.907
2024-12-02-06:57:00-root-INFO: grad norm: 103.495 100.544 24.537
2024-12-02-06:57:01-root-INFO: grad norm: 76.960 75.205 16.340
2024-12-02-06:57:01-root-INFO: grad norm: 85.866 83.650 19.384
2024-12-02-06:57:02-root-INFO: grad norm: 86.918 85.245 16.971
2024-12-02-06:57:03-root-INFO: grad norm: 91.636 89.639 19.026
2024-12-02-06:57:03-root-INFO: Loss too large (500.167->511.169)! Learning rate decreased to 0.01174.
2024-12-02-06:57:03-root-INFO: grad norm: 71.136 69.802 13.716
2024-12-02-06:57:04-root-INFO: grad norm: 39.960 38.861 9.305
2024-12-02-06:57:04-root-INFO: Loss Change: 957.289 -> 459.278
2024-12-02-06:57:04-root-INFO: Regularization Change: 0.000 -> 31.534
2024-12-02-06:57:04-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-02-06:57:04-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:57:04-root-INFO: step: 139 lr_xt 0.01519033
2024-12-02-06:57:04-root-INFO: grad norm: 43.936 43.077 8.648
2024-12-02-06:57:05-root-INFO: grad norm: 48.802 47.901 9.336
2024-12-02-06:57:05-root-INFO: grad norm: 52.917 52.051 9.530
2024-12-02-06:57:06-root-INFO: grad norm: 61.558 60.547 11.113
2024-12-02-06:57:06-root-INFO: Loss too large (445.103->448.859)! Learning rate decreased to 0.01215.
2024-12-02-06:57:07-root-INFO: grad norm: 48.554 47.740 8.854
2024-12-02-06:57:07-root-INFO: grad norm: 34.354 33.695 6.694
2024-12-02-06:57:08-root-INFO: grad norm: 29.039 28.471 5.713
2024-12-02-06:57:08-root-INFO: grad norm: 24.380 23.844 5.084
2024-12-02-06:57:08-root-INFO: Loss Change: 460.238 -> 419.906
2024-12-02-06:57:08-root-INFO: Regularization Change: 0.000 -> 3.793
2024-12-02-06:57:08-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-02-06:57:08-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:57:09-root-INFO: step: 138 lr_xt 0.01572237
2024-12-02-06:57:09-root-INFO: grad norm: 29.813 29.238 5.827
2024-12-02-06:57:09-root-INFO: grad norm: 34.352 33.798 6.144
2024-12-02-06:57:10-root-INFO: grad norm: 40.008 39.416 6.856
2024-12-02-06:57:10-root-INFO: grad norm: 48.960 48.289 8.082
2024-12-02-06:57:10-root-INFO: Loss too large (415.942->418.633)! Learning rate decreased to 0.01258.
2024-12-02-06:57:11-root-INFO: grad norm: 39.596 38.995 6.874
2024-12-02-06:57:12-root-INFO: grad norm: 30.759 30.280 5.408
2024-12-02-06:57:12-root-INFO: grad norm: 26.112 25.651 4.889
2024-12-02-06:57:13-root-INFO: grad norm: 22.187 21.785 4.202
2024-12-02-06:57:13-root-INFO: Loss Change: 420.364 -> 400.342
2024-12-02-06:57:13-root-INFO: Regularization Change: 0.000 -> 2.030
2024-12-02-06:57:13-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-02-06:57:13-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00030.
2024-12-02-06:57:13-root-INFO: step: 137 lr_xt 0.01627042
2024-12-02-06:57:13-root-INFO: grad norm: 39.154 38.339 7.947
2024-12-02-06:57:14-root-INFO: grad norm: 44.968 44.432 6.923
2024-12-02-06:57:14-root-INFO: Loss too large (402.062->403.339)! Learning rate decreased to 0.01302.
2024-12-02-06:57:15-root-INFO: grad norm: 35.448 34.992 5.669
2024-12-02-06:57:15-root-INFO: grad norm: 28.130 27.750 4.612
2024-12-02-06:57:16-root-INFO: grad norm: 23.364 23.003 4.088
2024-12-02-06:57:16-root-INFO: grad norm: 19.831 19.491 3.653
2024-12-02-06:57:16-root-INFO: grad norm: 17.348 17.013 3.391
2024-12-02-06:57:17-root-INFO: grad norm: 15.482 15.155 3.166
2024-12-02-06:57:17-root-INFO: Loss Change: 403.336 -> 385.075
2024-12-02-06:57:17-root-INFO: Regularization Change: 0.000 -> 1.466
2024-12-02-06:57:17-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-02-06:57:17-root-INFO: Coefficient of regularization decay: 0.00030 -> 0.00031.
2024-12-02-06:57:17-root-INFO: step: 136 lr_xt 0.01683487
2024-12-02-06:57:18-root-INFO: grad norm: 20.870 20.458 4.124
2024-12-02-06:57:18-root-INFO: grad norm: 23.791 23.479 3.844
2024-12-02-06:57:19-root-INFO: grad norm: 29.051 28.699 4.508
2024-12-02-06:57:19-root-INFO: Loss too large (383.727->383.819)! Learning rate decreased to 0.01347.
2024-12-02-06:57:19-root-INFO: grad norm: 24.263 23.944 3.920
2024-12-02-06:57:20-root-INFO: grad norm: 20.631 20.326 3.535
2024-12-02-06:57:20-root-INFO: grad norm: 17.923 17.626 3.249
2024-12-02-06:57:21-root-INFO: grad norm: 15.867 15.573 3.040
2024-12-02-06:57:21-root-INFO: grad norm: 14.308 14.015 2.879
2024-12-02-06:57:21-root-INFO: Loss Change: 385.756 -> 374.186
2024-12-02-06:57:21-root-INFO: Regularization Change: 0.000 -> 1.219
2024-12-02-06:57:21-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-02-06:57:21-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-06:57:22-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-06:57:22-root-INFO: grad norm: 24.998 24.478 5.073
2024-12-02-06:57:22-root-INFO: grad norm: 28.754 28.437 4.254
2024-12-02-06:57:22-root-INFO: Loss too large (374.208->374.503)! Learning rate decreased to 0.01393.
2024-12-02-06:57:23-root-INFO: grad norm: 24.160 23.862 3.786
2024-12-02-06:57:23-root-INFO: grad norm: 20.721 20.444 3.375
2024-12-02-06:57:24-root-INFO: grad norm: 18.119 17.850 3.108
2024-12-02-06:57:25-root-INFO: grad norm: 16.124 15.852 2.949
2024-12-02-06:57:25-root-INFO: grad norm: 14.539 14.274 2.762
2024-12-02-06:57:25-root-INFO: grad norm: 13.309 13.034 2.689
2024-12-02-06:57:26-root-INFO: Loss Change: 375.301 -> 364.268
2024-12-02-06:57:26-root-INFO: Regularization Change: 0.000 -> 1.062
2024-12-02-06:57:26-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-06:57:26-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-06:57:26-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-06:57:26-root-INFO: grad norm: 25.966 25.504 4.875
2024-12-02-06:57:27-root-INFO: grad norm: 30.963 30.652 4.381
2024-12-02-06:57:27-root-INFO: Loss too large (365.187->366.043)! Learning rate decreased to 0.01441.
2024-12-02-06:57:27-root-INFO: grad norm: 25.992 25.709 3.829
2024-12-02-06:57:28-root-INFO: grad norm: 22.236 21.969 3.434
2024-12-02-06:57:28-root-INFO: grad norm: 19.304 19.053 3.102
2024-12-02-06:57:29-root-INFO: grad norm: 17.092 16.834 2.962
2024-12-02-06:57:29-root-INFO: grad norm: 15.293 15.049 2.721
2024-12-02-06:57:30-root-INFO: grad norm: 13.911 13.653 2.672
2024-12-02-06:57:30-root-INFO: Loss Change: 365.634 -> 355.309
2024-12-02-06:57:30-root-INFO: Regularization Change: 0.000 -> 0.998
2024-12-02-06:57:30-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-06:57:30-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-06:57:30-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-06:57:30-root-INFO: grad norm: 26.033 25.524 5.123
2024-12-02-06:57:31-root-INFO: grad norm: 31.028 30.738 4.236
2024-12-02-06:57:31-root-INFO: Loss too large (356.543->357.842)! Learning rate decreased to 0.01490.
2024-12-02-06:57:32-root-INFO: grad norm: 26.805 26.535 3.797
2024-12-02-06:57:32-root-INFO: grad norm: 23.595 23.337 3.480
2024-12-02-06:57:33-root-INFO: grad norm: 21.011 20.773 3.155
2024-12-02-06:57:33-root-INFO: grad norm: 19.009 18.757 3.084
2024-12-02-06:57:34-root-INFO: grad norm: 17.301 17.072 2.806
2024-12-02-06:57:34-root-INFO: grad norm: 15.962 15.712 2.814
2024-12-02-06:57:34-root-INFO: Loss Change: 357.012 -> 347.254
2024-12-02-06:57:34-root-INFO: Regularization Change: 0.000 -> 0.970
2024-12-02-06:57:34-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-06:57:34-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-06:57:34-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-06:57:35-root-INFO: grad norm: 23.658 23.284 4.192
2024-12-02-06:57:35-root-INFO: grad norm: 29.018 28.742 3.991
2024-12-02-06:57:35-root-INFO: Loss too large (348.046->349.160)! Learning rate decreased to 0.01541.
2024-12-02-06:57:36-root-INFO: grad norm: 25.083 24.824 3.594
2024-12-02-06:57:36-root-INFO: grad norm: 22.138 21.885 3.333
2024-12-02-06:57:37-root-INFO: grad norm: 19.682 19.446 3.040
2024-12-02-06:57:37-root-INFO: grad norm: 17.850 17.602 2.962
2024-12-02-06:57:38-root-INFO: grad norm: 16.294 16.063 2.735
2024-12-02-06:57:38-root-INFO: grad norm: 15.113 14.865 2.725
2024-12-02-06:57:38-root-INFO: Loss Change: 348.157 -> 339.422
2024-12-02-06:57:38-root-INFO: Regularization Change: 0.000 -> 0.927
2024-12-02-06:57:38-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-06:57:38-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-06:57:39-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-06:57:39-root-INFO: grad norm: 25.484 25.028 4.801
2024-12-02-06:57:39-root-INFO: grad norm: 31.178 30.893 4.205
2024-12-02-06:57:39-root-INFO: Loss too large (340.946->342.633)! Learning rate decreased to 0.01593.
2024-12-02-06:57:40-root-INFO: grad norm: 27.135 26.868 3.795
2024-12-02-06:57:40-root-INFO: grad norm: 24.040 23.774 3.570
2024-12-02-06:57:41-root-INFO: grad norm: 21.456 21.218 3.192
2024-12-02-06:57:41-root-INFO: grad norm: 19.518 19.259 3.170
2024-12-02-06:57:42-root-INFO: grad norm: 17.858 17.627 2.864
2024-12-02-06:57:42-root-INFO: grad norm: 16.591 16.335 2.902
2024-12-02-06:57:43-root-INFO: Loss Change: 341.026 -> 332.033
2024-12-02-06:57:43-root-INFO: Regularization Change: 0.000 -> 0.951
2024-12-02-06:57:43-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-06:57:43-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-06:57:43-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-06:57:43-root-INFO: grad norm: 21.601 21.236 3.958
2024-12-02-06:57:44-root-INFO: grad norm: 26.586 26.304 3.864
2024-12-02-06:57:44-root-INFO: Loss too large (332.906->334.058)! Learning rate decreased to 0.01647.
2024-12-02-06:57:44-root-INFO: grad norm: 23.340 23.073 3.517
2024-12-02-06:57:45-root-INFO: grad norm: 20.984 20.717 3.337
2024-12-02-06:57:45-root-INFO: grad norm: 19.073 18.823 3.080
2024-12-02-06:57:46-root-INFO: grad norm: 17.638 17.373 3.045
2024-12-02-06:57:46-root-INFO: grad norm: 16.439 16.192 2.838
2024-12-02-06:57:47-root-INFO: grad norm: 15.507 15.243 2.848
2024-12-02-06:57:47-root-INFO: Loss Change: 333.095 -> 325.340
2024-12-02-06:57:47-root-INFO: Regularization Change: 0.000 -> 0.905
2024-12-02-06:57:47-root-INFO: Undo step: 130
2024-12-02-06:57:47-root-INFO: Undo step: 131
2024-12-02-06:57:47-root-INFO: Undo step: 132
2024-12-02-06:57:47-root-INFO: Undo step: 133
2024-12-02-06:57:47-root-INFO: Undo step: 134
2024-12-02-06:57:47-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-06:57:47-root-INFO: grad norm: 169.863 166.590 33.181
2024-12-02-06:57:48-root-INFO: grad norm: 96.002 93.266 22.754
2024-12-02-06:57:48-root-INFO: grad norm: 81.575 79.691 17.432
2024-12-02-06:57:49-root-INFO: grad norm: 69.770 67.983 15.689
2024-12-02-06:57:49-root-INFO: grad norm: 56.036 54.660 12.341
2024-12-02-06:57:50-root-INFO: grad norm: 51.791 50.624 10.935
2024-12-02-06:57:50-root-INFO: grad norm: 45.135 44.110 9.563
2024-12-02-06:57:51-root-INFO: grad norm: 43.101 42.255 8.497
2024-12-02-06:57:51-root-INFO: Loss Change: 783.263 -> 396.517
2024-12-02-06:57:51-root-INFO: Regularization Change: 0.000 -> 29.954
2024-12-02-06:57:51-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-06:57:51-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-06:57:51-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-06:57:51-root-INFO: grad norm: 30.485 29.773 6.550
2024-12-02-06:57:52-root-INFO: grad norm: 28.333 27.664 6.120
2024-12-02-06:57:52-root-INFO: grad norm: 27.743 27.154 5.685
2024-12-02-06:57:53-root-INFO: grad norm: 28.088 27.541 5.518
2024-12-02-06:57:53-root-INFO: grad norm: 28.961 28.435 5.493
2024-12-02-06:57:54-root-INFO: grad norm: 30.344 29.837 5.522
2024-12-02-06:57:54-root-INFO: grad norm: 32.640 32.122 5.793
2024-12-02-06:57:55-root-INFO: grad norm: 35.388 34.856 6.114
2024-12-02-06:57:55-root-INFO: Loss Change: 391.756 -> 366.097
2024-12-02-06:57:55-root-INFO: Regularization Change: 0.000 -> 4.139
2024-12-02-06:57:55-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-06:57:55-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-06:57:55-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-06:57:55-root-INFO: grad norm: 37.844 37.409 5.722
2024-12-02-06:57:55-root-INFO: Loss too large (363.856->365.515)! Learning rate decreased to 0.01490.
2024-12-02-06:57:56-root-INFO: grad norm: 32.033 31.512 5.751
2024-12-02-06:57:56-root-INFO: grad norm: 27.116 26.764 4.353
2024-12-02-06:57:57-root-INFO: grad norm: 25.026 24.611 4.538
2024-12-02-06:57:57-root-INFO: grad norm: 23.085 22.758 3.873
2024-12-02-06:57:58-root-INFO: grad norm: 22.157 21.778 4.083
2024-12-02-06:57:58-root-INFO: grad norm: 21.196 20.883 3.630
2024-12-02-06:57:59-root-INFO: grad norm: 20.708 20.349 3.838
2024-12-02-06:57:59-root-INFO: Loss Change: 363.856 -> 347.572
2024-12-02-06:57:59-root-INFO: Regularization Change: 0.000 -> 1.539
2024-12-02-06:57:59-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-06:57:59-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-06:57:59-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-06:57:59-root-INFO: grad norm: 26.994 26.614 4.517
2024-12-02-06:57:59-root-INFO: Loss too large (347.929->348.676)! Learning rate decreased to 0.01541.
2024-12-02-06:58:00-root-INFO: grad norm: 24.674 24.323 4.150
2024-12-02-06:58:00-root-INFO: grad norm: 23.304 22.997 3.769
2024-12-02-06:58:01-root-INFO: grad norm: 22.466 22.114 3.964
2024-12-02-06:58:01-root-INFO: grad norm: 21.462 21.161 3.582
2024-12-02-06:58:02-root-INFO: grad norm: 20.917 20.577 3.757
2024-12-02-06:58:02-root-INFO: grad norm: 20.235 19.938 3.456
2024-12-02-06:58:03-root-INFO: grad norm: 19.856 19.527 3.599
2024-12-02-06:58:03-root-INFO: Loss Change: 347.929 -> 336.892
2024-12-02-06:58:03-root-INFO: Regularization Change: 0.000 -> 1.213
2024-12-02-06:58:03-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-06:58:03-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-06:58:03-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-06:58:04-root-INFO: grad norm: 28.741 28.264 5.213
2024-12-02-06:58:04-root-INFO: Loss too large (338.019->338.807)! Learning rate decreased to 0.01593.
2024-12-02-06:58:04-root-INFO: grad norm: 25.169 24.810 4.236
2024-12-02-06:58:05-root-INFO: grad norm: 23.006 22.694 3.776
2024-12-02-06:58:05-root-INFO: grad norm: 21.761 21.416 3.859
2024-12-02-06:58:06-root-INFO: grad norm: 20.499 20.205 3.457
2024-12-02-06:58:06-root-INFO: grad norm: 19.804 19.481 3.565
2024-12-02-06:58:07-root-INFO: grad norm: 19.020 18.733 3.293
2024-12-02-06:58:07-root-INFO: grad norm: 18.570 18.262 3.370
2024-12-02-06:58:08-root-INFO: Loss Change: 338.019 -> 327.271
2024-12-02-06:58:08-root-INFO: Regularization Change: 0.000 -> 1.123
2024-12-02-06:58:08-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-06:58:08-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-06:58:08-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-06:58:08-root-INFO: grad norm: 24.897 24.482 4.526
2024-12-02-06:58:08-root-INFO: Loss too large (328.442->329.252)! Learning rate decreased to 0.01647.
2024-12-02-06:58:08-root-INFO: grad norm: 22.478 22.144 3.861
2024-12-02-06:58:09-root-INFO: grad norm: 21.053 20.733 3.660
2024-12-02-06:58:09-root-INFO: grad norm: 20.214 19.888 3.614
2024-12-02-06:58:10-root-INFO: grad norm: 19.290 18.983 3.433
2024-12-02-06:58:10-root-INFO: grad norm: 18.755 18.445 3.394
2024-12-02-06:58:11-root-INFO: grad norm: 18.108 17.808 3.282
2024-12-02-06:58:11-root-INFO: grad norm: 17.720 17.423 3.228
2024-12-02-06:58:12-root-INFO: Loss Change: 328.442 -> 319.508
2024-12-02-06:58:12-root-INFO: Regularization Change: 0.000 -> 1.012
2024-12-02-06:58:12-root-INFO: Undo step: 130
2024-12-02-06:58:12-root-INFO: Undo step: 131
2024-12-02-06:58:12-root-INFO: Undo step: 132
2024-12-02-06:58:12-root-INFO: Undo step: 133
2024-12-02-06:58:12-root-INFO: Undo step: 134
2024-12-02-06:58:12-root-INFO: step: 135 lr_xt 0.01741608
2024-12-02-06:58:12-root-INFO: grad norm: 160.744 157.264 33.266
2024-12-02-06:58:13-root-INFO: grad norm: 80.296 77.937 19.322
2024-12-02-06:58:13-root-INFO: grad norm: 55.184 53.291 14.333
2024-12-02-06:58:14-root-INFO: grad norm: 45.018 43.656 10.989
2024-12-02-06:58:14-root-INFO: grad norm: 39.534 38.324 9.706
2024-12-02-06:58:15-root-INFO: grad norm: 36.607 35.643 8.346
2024-12-02-06:58:15-root-INFO: grad norm: 35.067 34.170 7.879
2024-12-02-06:58:16-root-INFO: grad norm: 34.505 33.764 7.115
2024-12-02-06:58:16-root-INFO: Loss Change: 824.219 -> 394.787
2024-12-02-06:58:16-root-INFO: Regularization Change: 0.000 -> 34.737
2024-12-02-06:58:16-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-02-06:58:16-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00031.
2024-12-02-06:58:16-root-INFO: step: 134 lr_xt 0.01801447
2024-12-02-06:58:16-root-INFO: grad norm: 28.107 27.396 6.279
2024-12-02-06:58:17-root-INFO: grad norm: 27.264 26.612 5.927
2024-12-02-06:58:17-root-INFO: grad norm: 27.785 27.189 5.725
2024-12-02-06:58:18-root-INFO: grad norm: 29.081 28.546 5.551
2024-12-02-06:58:18-root-INFO: grad norm: 30.667 30.122 5.756
2024-12-02-06:58:19-root-INFO: grad norm: 32.806 32.311 5.678
2024-12-02-06:58:19-root-INFO: grad norm: 34.945 34.391 6.200
2024-12-02-06:58:20-root-INFO: grad norm: 37.592 37.077 6.197
2024-12-02-06:58:20-root-INFO: Loss Change: 391.262 -> 364.190
2024-12-02-06:58:20-root-INFO: Regularization Change: 0.000 -> 4.550
2024-12-02-06:58:20-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-02-06:58:20-root-INFO: Coefficient of regularization decay: 0.00031 -> 0.00032.
2024-12-02-06:58:20-root-INFO: step: 133 lr_xt 0.01863041
2024-12-02-06:58:21-root-INFO: grad norm: 33.347 32.826 5.870
2024-12-02-06:58:21-root-INFO: grad norm: 35.773 35.182 6.471
2024-12-02-06:58:22-root-INFO: grad norm: 38.585 37.938 7.036
2024-12-02-06:58:22-root-INFO: grad norm: 42.803 42.132 7.551
2024-12-02-06:58:22-root-INFO: Loss too large (355.644->356.241)! Learning rate decreased to 0.01490.
2024-12-02-06:58:23-root-INFO: grad norm: 31.729 31.085 6.361
2024-12-02-06:58:23-root-INFO: grad norm: 23.347 22.901 4.539
2024-12-02-06:58:24-root-INFO: grad norm: 19.063 18.582 4.257
2024-12-02-06:58:24-root-INFO: grad norm: 15.899 15.502 3.533
2024-12-02-06:58:24-root-INFO: Loss Change: 360.809 -> 340.730
2024-12-02-06:58:24-root-INFO: Regularization Change: 0.000 -> 2.123
2024-12-02-06:58:24-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-02-06:58:24-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-06:58:25-root-INFO: step: 132 lr_xt 0.01926430
2024-12-02-06:58:25-root-INFO: grad norm: 14.265 13.716 3.922
2024-12-02-06:58:25-root-INFO: grad norm: 12.838 12.375 3.415
2024-12-02-06:58:26-root-INFO: grad norm: 13.119 12.630 3.548
2024-12-02-06:58:26-root-INFO: grad norm: 14.619 14.101 3.858
2024-12-02-06:58:27-root-INFO: grad norm: 16.181 15.619 4.225
2024-12-02-06:58:27-root-INFO: grad norm: 19.408 18.760 4.971
2024-12-02-06:58:27-root-INFO: grad norm: 21.060 20.384 5.296
2024-12-02-06:58:28-root-INFO: grad norm: 23.957 23.206 5.951
2024-12-02-06:58:28-root-INFO: Loss too large (329.147->329.367)! Learning rate decreased to 0.01541.
2024-12-02-06:58:28-root-INFO: Loss Change: 339.572 -> 327.897
2024-12-02-06:58:28-root-INFO: Regularization Change: 0.000 -> 1.944
2024-12-02-06:58:28-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-02-06:58:28-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00032.
2024-12-02-06:58:28-root-INFO: step: 131 lr_xt 0.01991656
2024-12-02-06:58:29-root-INFO: grad norm: 23.119 22.351 5.908
2024-12-02-06:58:29-root-INFO: grad norm: 24.264 23.575 5.742
2024-12-02-06:58:30-root-INFO: grad norm: 25.268 24.620 5.685
2024-12-02-06:58:30-root-INFO: grad norm: 26.839 26.157 6.014
2024-12-02-06:58:30-root-INFO: Loss too large (323.544->323.752)! Learning rate decreased to 0.01593.
2024-12-02-06:58:31-root-INFO: grad norm: 20.480 19.961 4.582
2024-12-02-06:58:31-root-INFO: grad norm: 14.930 14.520 3.473
2024-12-02-06:58:32-root-INFO: grad norm: 12.543 12.188 2.962
2024-12-02-06:58:32-root-INFO: grad norm: 10.733 10.389 2.696
2024-12-02-06:58:32-root-INFO: Loss Change: 327.911 -> 316.059
2024-12-02-06:58:32-root-INFO: Regularization Change: 0.000 -> 1.343
2024-12-02-06:58:32-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-02-06:58:32-root-INFO: Coefficient of regularization decay: 0.00032 -> 0.00033.
2024-12-02-06:58:33-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-06:58:33-root-INFO: grad norm: 15.393 14.919 3.789
2024-12-02-06:58:33-root-INFO: grad norm: 15.251 14.955 2.991
2024-12-02-06:58:34-root-INFO: grad norm: 16.892 16.606 3.095
2024-12-02-06:58:34-root-INFO: grad norm: 19.214 18.908 3.414
2024-12-02-06:58:35-root-INFO: grad norm: 22.133 21.835 3.620
2024-12-02-06:58:35-root-INFO: grad norm: 25.434 25.076 4.250
2024-12-02-06:58:36-root-INFO: grad norm: 29.060 28.700 4.559
2024-12-02-06:58:36-root-INFO: Loss too large (312.457->312.687)! Learning rate decreased to 0.01647.
2024-12-02-06:58:36-root-INFO: grad norm: 21.512 21.179 3.769
2024-12-02-06:58:37-root-INFO: Loss Change: 316.432 -> 308.087
2024-12-02-06:58:37-root-INFO: Regularization Change: 0.000 -> 1.331
2024-12-02-06:58:37-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-06:58:37-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-06:58:37-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-06:58:37-root-INFO: grad norm: 30.469 29.873 5.996
2024-12-02-06:58:37-root-INFO: grad norm: 33.581 33.198 5.057
2024-12-02-06:58:38-root-INFO: Loss too large (310.828->310.996)! Learning rate decreased to 0.01702.
2024-12-02-06:58:38-root-INFO: grad norm: 23.994 23.694 3.779
2024-12-02-06:58:38-root-INFO: grad norm: 18.055 17.803 3.004
2024-12-02-06:58:39-root-INFO: grad norm: 14.009 13.786 2.486
2024-12-02-06:58:39-root-INFO: grad norm: 11.420 11.200 2.232
2024-12-02-06:58:40-root-INFO: grad norm: 9.667 9.452 2.029
2024-12-02-06:58:40-root-INFO: grad norm: 8.524 8.300 1.938
2024-12-02-06:58:41-root-INFO: Loss Change: 310.888 -> 299.962
2024-12-02-06:58:41-root-INFO: Regularization Change: 0.000 -> 0.996
2024-12-02-06:58:41-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-06:58:41-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-06:58:41-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-06:58:41-root-INFO: grad norm: 20.223 19.700 4.569
2024-12-02-06:58:41-root-INFO: grad norm: 22.143 21.885 3.366
2024-12-02-06:58:42-root-INFO: grad norm: 26.427 26.109 4.088
2024-12-02-06:58:42-root-INFO: Loss too large (300.099->300.874)! Learning rate decreased to 0.01759.
2024-12-02-06:58:43-root-INFO: grad norm: 21.101 20.839 3.317
2024-12-02-06:58:43-root-INFO: grad norm: 17.176 16.922 2.943
2024-12-02-06:58:43-root-INFO: grad norm: 14.527 14.294 2.593
2024-12-02-06:58:44-root-INFO: grad norm: 12.614 12.364 2.497
2024-12-02-06:58:44-root-INFO: grad norm: 11.268 11.038 2.265
2024-12-02-06:58:45-root-INFO: Loss Change: 300.938 -> 293.068
2024-12-02-06:58:45-root-INFO: Regularization Change: 0.000 -> 0.953
2024-12-02-06:58:45-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-06:58:45-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-06:58:45-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-06:58:45-root-INFO: grad norm: 19.050 18.602 4.108
2024-12-02-06:58:46-root-INFO: grad norm: 22.919 22.592 3.854
2024-12-02-06:58:46-root-INFO: Loss too large (293.947->294.582)! Learning rate decreased to 0.01817.
2024-12-02-06:58:46-root-INFO: grad norm: 19.619 19.265 3.709
2024-12-02-06:58:47-root-INFO: grad norm: 17.289 16.990 3.198
2024-12-02-06:58:47-root-INFO: grad norm: 15.287 14.971 3.092
2024-12-02-06:58:48-root-INFO: grad norm: 13.825 13.549 2.747
2024-12-02-06:58:48-root-INFO: grad norm: 12.529 12.236 2.695
2024-12-02-06:58:49-root-INFO: grad norm: 11.566 11.307 2.431
2024-12-02-06:58:49-root-INFO: Loss Change: 294.115 -> 287.244
2024-12-02-06:58:49-root-INFO: Regularization Change: 0.000 -> 0.852
2024-12-02-06:58:49-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-06:58:49-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-06:58:49-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-06:58:50-root-INFO: grad norm: 17.259 16.821 3.861
2024-12-02-06:58:50-root-INFO: grad norm: 20.374 20.085 3.419
2024-12-02-06:58:50-root-INFO: Loss too large (287.920->288.388)! Learning rate decreased to 0.01877.
2024-12-02-06:58:51-root-INFO: grad norm: 17.684 17.362 3.356
2024-12-02-06:58:51-root-INFO: grad norm: 15.828 15.563 2.887
2024-12-02-06:58:52-root-INFO: grad norm: 14.240 13.949 2.864
2024-12-02-06:58:52-root-INFO: grad norm: 13.041 12.790 2.546
2024-12-02-06:58:53-root-INFO: grad norm: 11.973 11.699 2.545
2024-12-02-06:58:53-root-INFO: grad norm: 11.155 10.915 2.298
2024-12-02-06:58:53-root-INFO: Loss Change: 288.284 -> 282.003
2024-12-02-06:58:53-root-INFO: Regularization Change: 0.000 -> 0.822
2024-12-02-06:58:53-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-06:58:53-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-06:58:54-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-06:58:54-root-INFO: grad norm: 20.234 19.741 4.439
2024-12-02-06:58:54-root-INFO: grad norm: 24.163 23.866 3.776
2024-12-02-06:58:54-root-INFO: Loss too large (283.193->284.325)! Learning rate decreased to 0.01939.
2024-12-02-06:58:55-root-INFO: grad norm: 20.811 20.495 3.609
2024-12-02-06:58:55-root-INFO: grad norm: 18.469 18.211 3.080
2024-12-02-06:58:56-root-INFO: grad norm: 16.493 16.216 3.008
2024-12-02-06:58:56-root-INFO: grad norm: 14.984 14.744 2.669
2024-12-02-06:58:57-root-INFO: grad norm: 13.667 13.410 2.639
2024-12-02-06:58:57-root-INFO: grad norm: 12.642 12.415 2.383
2024-12-02-06:58:58-root-INFO: Loss Change: 283.269 -> 276.630
2024-12-02-06:58:58-root-INFO: Regularization Change: 0.000 -> 0.831
2024-12-02-06:58:58-root-INFO: Undo step: 125
2024-12-02-06:58:58-root-INFO: Undo step: 126
2024-12-02-06:58:58-root-INFO: Undo step: 127
2024-12-02-06:58:58-root-INFO: Undo step: 128
2024-12-02-06:58:58-root-INFO: Undo step: 129
2024-12-02-06:58:58-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-06:58:58-root-INFO: grad norm: 175.857 172.263 35.370
2024-12-02-06:58:59-root-INFO: grad norm: 93.884 92.025 18.591
2024-12-02-06:58:59-root-INFO: grad norm: 65.121 62.345 18.810
2024-12-02-06:59:00-root-INFO: grad norm: 48.792 47.607 10.690
2024-12-02-06:59:00-root-INFO: grad norm: 40.401 39.163 9.927
2024-12-02-06:59:01-root-INFO: grad norm: 37.413 36.557 7.958
2024-12-02-06:59:01-root-INFO: grad norm: 33.852 33.009 7.504
2024-12-02-06:59:02-root-INFO: grad norm: 30.274 29.561 6.529
2024-12-02-06:59:02-root-INFO: Loss Change: 724.900 -> 342.439
2024-12-02-06:59:02-root-INFO: Regularization Change: 0.000 -> 36.465
2024-12-02-06:59:02-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-06:59:02-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-06:59:02-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-06:59:02-root-INFO: grad norm: 30.495 29.656 7.102
2024-12-02-06:59:03-root-INFO: grad norm: 31.178 30.342 7.170
2024-12-02-06:59:03-root-INFO: grad norm: 30.213 29.515 6.458
2024-12-02-06:59:04-root-INFO: grad norm: 28.193 27.440 6.472
2024-12-02-06:59:04-root-INFO: grad norm: 28.676 28.003 6.176
2024-12-02-06:59:05-root-INFO: grad norm: 29.776 28.989 6.799
2024-12-02-06:59:05-root-INFO: grad norm: 29.642 28.979 6.236
2024-12-02-06:59:06-root-INFO: grad norm: 28.942 28.180 6.597
2024-12-02-06:59:06-root-INFO: Loss Change: 341.517 -> 314.994
2024-12-02-06:59:06-root-INFO: Regularization Change: 0.000 -> 4.566
2024-12-02-06:59:06-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-06:59:06-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-06:59:06-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-06:59:06-root-INFO: grad norm: 36.927 35.861 8.807
2024-12-02-06:59:07-root-INFO: grad norm: 36.707 35.747 8.341
2024-12-02-06:59:07-root-INFO: Loss too large (312.297->312.855)! Learning rate decreased to 0.01759.
2024-12-02-06:59:08-root-INFO: grad norm: 26.296 25.646 5.813
2024-12-02-06:59:08-root-INFO: grad norm: 17.578 17.010 4.435
2024-12-02-06:59:09-root-INFO: grad norm: 14.451 14.002 3.575
2024-12-02-06:59:09-root-INFO: grad norm: 12.257 11.776 3.400
2024-12-02-06:59:09-root-INFO: grad norm: 11.105 10.705 2.952
2024-12-02-06:59:10-root-INFO: grad norm: 10.215 9.771 2.978
2024-12-02-06:59:10-root-INFO: Loss Change: 317.103 -> 297.039
2024-12-02-06:59:10-root-INFO: Regularization Change: 0.000 -> 1.854
2024-12-02-06:59:10-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-06:59:10-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-06:59:11-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-06:59:11-root-INFO: grad norm: 14.889 14.183 4.532
2024-12-02-06:59:11-root-INFO: grad norm: 15.109 14.596 3.903
2024-12-02-06:59:12-root-INFO: grad norm: 16.838 16.335 4.086
2024-12-02-06:59:12-root-INFO: grad norm: 20.251 19.648 4.906
2024-12-02-06:59:13-root-INFO: grad norm: 23.445 22.888 5.081
2024-12-02-06:59:13-root-INFO: grad norm: 29.121 28.364 6.597
2024-12-02-06:59:13-root-INFO: Loss too large (293.255->295.014)! Learning rate decreased to 0.01817.
2024-12-02-06:59:14-root-INFO: grad norm: 23.223 22.720 4.807
2024-12-02-06:59:14-root-INFO: grad norm: 16.902 16.408 4.056
2024-12-02-06:59:15-root-INFO: Loss Change: 297.435 -> 287.961
2024-12-02-06:59:15-root-INFO: Regularization Change: 0.000 -> 1.601
2024-12-02-06:59:15-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-06:59:15-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-06:59:15-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-06:59:15-root-INFO: grad norm: 19.777 19.068 5.249
2024-12-02-06:59:16-root-INFO: grad norm: 23.108 22.455 5.454
2024-12-02-06:59:16-root-INFO: Loss too large (287.935->288.505)! Learning rate decreased to 0.01877.
2024-12-02-06:59:16-root-INFO: grad norm: 18.766 18.320 4.066
2024-12-02-06:59:17-root-INFO: grad norm: 14.996 14.551 3.626
2024-12-02-06:59:17-root-INFO: grad norm: 13.076 12.749 2.905
2024-12-02-06:59:18-root-INFO: grad norm: 11.401 11.027 2.895
2024-12-02-06:59:18-root-INFO: grad norm: 10.413 10.130 2.410
2024-12-02-06:59:19-root-INFO: grad norm: 9.528 9.189 2.518
2024-12-02-06:59:19-root-INFO: Loss Change: 288.815 -> 280.259
2024-12-02-06:59:19-root-INFO: Regularization Change: 0.000 -> 1.070
2024-12-02-06:59:19-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-06:59:19-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-06:59:19-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-06:59:19-root-INFO: grad norm: 16.927 16.189 4.942
2024-12-02-06:59:20-root-INFO: grad norm: 18.669 18.131 4.453
2024-12-02-06:59:20-root-INFO: Loss too large (279.941->279.969)! Learning rate decreased to 0.01939.
2024-12-02-06:59:20-root-INFO: grad norm: 15.412 15.024 3.433
2024-12-02-06:59:21-root-INFO: grad norm: 13.142 12.749 3.186
2024-12-02-06:59:21-root-INFO: grad norm: 11.719 11.427 2.601
2024-12-02-06:59:22-root-INFO: grad norm: 10.575 10.234 2.663
2024-12-02-06:59:22-root-INFO: grad norm: 9.821 9.565 2.225
2024-12-02-06:59:23-root-INFO: grad norm: 9.165 8.853 2.372
2024-12-02-06:59:23-root-INFO: Loss Change: 280.960 -> 273.668
2024-12-02-06:59:23-root-INFO: Regularization Change: 0.000 -> 0.981
2024-12-02-06:59:23-root-INFO: Undo step: 125
2024-12-02-06:59:23-root-INFO: Undo step: 126
2024-12-02-06:59:23-root-INFO: Undo step: 127
2024-12-02-06:59:23-root-INFO: Undo step: 128
2024-12-02-06:59:23-root-INFO: Undo step: 129
2024-12-02-06:59:23-root-INFO: step: 130 lr_xt 0.02058758
2024-12-02-06:59:23-root-INFO: grad norm: 139.693 136.486 29.761
2024-12-02-06:59:24-root-INFO: grad norm: 64.721 63.159 14.134
2024-12-02-06:59:24-root-INFO: grad norm: 44.390 43.019 10.946
2024-12-02-06:59:25-root-INFO: grad norm: 35.141 34.181 8.159
2024-12-02-06:59:25-root-INFO: grad norm: 30.225 29.292 7.451
2024-12-02-06:59:26-root-INFO: grad norm: 27.719 27.069 5.968
2024-12-02-06:59:26-root-INFO: grad norm: 26.904 26.187 6.169
2024-12-02-06:59:27-root-INFO: grad norm: 28.735 28.209 5.475
2024-12-02-06:59:27-root-INFO: Loss Change: 718.071 -> 342.786
2024-12-02-06:59:27-root-INFO: Regularization Change: 0.000 -> 34.852
2024-12-02-06:59:27-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-02-06:59:27-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-06:59:27-root-INFO: step: 129 lr_xt 0.02127779
2024-12-02-06:59:27-root-INFO: grad norm: 28.716 28.175 5.548
2024-12-02-06:59:28-root-INFO: grad norm: 34.850 34.231 6.541
2024-12-02-06:59:29-root-INFO: grad norm: 30.303 29.806 5.461
2024-12-02-06:59:29-root-INFO: grad norm: 21.952 21.446 4.686
2024-12-02-06:59:30-root-INFO: grad norm: 23.070 22.618 4.544
2024-12-02-06:59:30-root-INFO: grad norm: 26.469 25.986 5.034
2024-12-02-06:59:31-root-INFO: grad norm: 26.433 26.006 4.730
2024-12-02-06:59:31-root-INFO: grad norm: 25.869 25.412 4.845
2024-12-02-06:59:31-root-INFO: Loss Change: 340.720 -> 313.433
2024-12-02-06:59:31-root-INFO: Regularization Change: 0.000 -> 4.797
2024-12-02-06:59:31-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-02-06:59:31-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00033.
2024-12-02-06:59:32-root-INFO: step: 128 lr_xt 0.02198759
2024-12-02-06:59:32-root-INFO: grad norm: 26.422 25.935 5.049
2024-12-02-06:59:32-root-INFO: grad norm: 24.684 24.194 4.896
2024-12-02-06:59:33-root-INFO: grad norm: 23.734 23.365 4.168
2024-12-02-06:59:33-root-INFO: grad norm: 22.167 21.719 4.435
2024-12-02-06:59:34-root-INFO: grad norm: 22.113 21.781 3.816
2024-12-02-06:59:34-root-INFO: grad norm: 22.018 21.584 4.347
2024-12-02-06:59:35-root-INFO: grad norm: 21.964 21.652 3.689
2024-12-02-06:59:35-root-INFO: grad norm: 21.765 21.341 4.271
2024-12-02-06:59:35-root-INFO: Loss Change: 312.699 -> 296.615
2024-12-02-06:59:35-root-INFO: Regularization Change: 0.000 -> 2.584
2024-12-02-06:59:35-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-02-06:59:35-root-INFO: Coefficient of regularization decay: 0.00033 -> 0.00034.
2024-12-02-06:59:36-root-INFO: step: 127 lr_xt 0.02271741
2024-12-02-06:59:36-root-INFO: grad norm: 25.490 25.018 4.881
2024-12-02-06:59:36-root-INFO: grad norm: 26.771 26.294 5.036
2024-12-02-06:59:36-root-INFO: Loss too large (294.825->295.004)! Learning rate decreased to 0.01817.
2024-12-02-06:59:37-root-INFO: grad norm: 20.412 20.120 3.440
2024-12-02-06:59:37-root-INFO: grad norm: 15.200 14.862 3.184
2024-12-02-06:59:38-root-INFO: grad norm: 12.731 12.494 2.446
2024-12-02-06:59:38-root-INFO: grad norm: 10.867 10.559 2.568
2024-12-02-06:59:39-root-INFO: grad norm: 9.672 9.438 2.115
2024-12-02-06:59:39-root-INFO: grad norm: 8.784 8.487 2.264
2024-12-02-06:59:40-root-INFO: Loss Change: 297.001 -> 284.688
2024-12-02-06:59:40-root-INFO: Regularization Change: 0.000 -> 1.353
2024-12-02-06:59:40-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-02-06:59:40-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-06:59:40-root-INFO: step: 126 lr_xt 0.02346768
2024-12-02-06:59:40-root-INFO: grad norm: 14.509 14.074 3.528
2024-12-02-06:59:41-root-INFO: grad norm: 15.368 15.039 3.160
2024-12-02-06:59:41-root-INFO: grad norm: 18.041 17.773 3.098
2024-12-02-06:59:42-root-INFO: grad norm: 21.753 21.383 3.994
2024-12-02-06:59:42-root-INFO: Loss too large (283.031->283.204)! Learning rate decreased to 0.01877.
2024-12-02-06:59:42-root-INFO: grad norm: 17.652 17.421 2.849
2024-12-02-06:59:43-root-INFO: grad norm: 14.576 14.278 2.930
2024-12-02-06:59:43-root-INFO: grad norm: 12.344 12.149 2.185
2024-12-02-06:59:44-root-INFO: grad norm: 10.684 10.414 2.384
2024-12-02-06:59:44-root-INFO: Loss Change: 285.206 -> 277.132
2024-12-02-06:59:44-root-INFO: Regularization Change: 0.000 -> 1.223
2024-12-02-06:59:44-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-02-06:59:44-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00034.
2024-12-02-06:59:44-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-06:59:45-root-INFO: grad norm: 19.309 18.758 4.579
2024-12-02-06:59:45-root-INFO: grad norm: 21.727 21.356 4.001
2024-12-02-06:59:45-root-INFO: Loss too large (277.389->277.549)! Learning rate decreased to 0.01939.
2024-12-02-06:59:46-root-INFO: grad norm: 17.510 17.254 2.984
2024-12-02-06:59:46-root-INFO: grad norm: 14.613 14.333 2.850
2024-12-02-06:59:47-root-INFO: grad norm: 12.412 12.216 2.196
2024-12-02-06:59:47-root-INFO: grad norm: 10.788 10.534 2.327
2024-12-02-06:59:48-root-INFO: grad norm: 9.541 9.361 1.846
2024-12-02-06:59:48-root-INFO: grad norm: 8.605 8.361 2.037
2024-12-02-06:59:49-root-INFO: Loss Change: 278.086 -> 270.195
2024-12-02-06:59:49-root-INFO: Regularization Change: 0.000 -> 1.002
2024-12-02-06:59:49-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-06:59:49-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-06:59:49-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-06:59:49-root-INFO: grad norm: 23.283 22.591 5.637
2024-12-02-06:59:50-root-INFO: grad norm: 25.851 25.437 4.604
2024-12-02-06:59:50-root-INFO: Loss too large (272.126->272.769)! Learning rate decreased to 0.02013.
2024-12-02-06:59:50-root-INFO: grad norm: 20.493 20.208 3.409
2024-12-02-06:59:51-root-INFO: grad norm: 16.950 16.666 3.090
2024-12-02-06:59:51-root-INFO: grad norm: 14.163 13.959 2.394
2024-12-02-06:59:52-root-INFO: grad norm: 12.131 11.887 2.424
2024-12-02-06:59:52-root-INFO: grad norm: 10.531 10.354 1.924
2024-12-02-06:59:53-root-INFO: grad norm: 9.338 9.108 2.057
2024-12-02-06:59:53-root-INFO: Loss Change: 272.693 -> 264.051
2024-12-02-06:59:53-root-INFO: Regularization Change: 0.000 -> 1.005
2024-12-02-06:59:53-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-06:59:53-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-06:59:53-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-06:59:53-root-INFO: grad norm: 16.308 15.879 3.717
2024-12-02-06:59:54-root-INFO: grad norm: 19.395 19.083 3.467
2024-12-02-06:59:54-root-INFO: Loss too large (264.929->265.319)! Learning rate decreased to 0.02078.
2024-12-02-06:59:55-root-INFO: grad norm: 16.267 16.033 2.746
2024-12-02-06:59:55-root-INFO: grad norm: 13.957 13.709 2.616
2024-12-02-06:59:56-root-INFO: grad norm: 12.079 11.895 2.102
2024-12-02-06:59:56-root-INFO: grad norm: 10.648 10.424 2.170
2024-12-02-06:59:57-root-INFO: grad norm: 9.485 9.318 1.773
2024-12-02-06:59:57-root-INFO: grad norm: 8.586 8.372 1.901
2024-12-02-06:59:57-root-INFO: Loss Change: 265.235 -> 259.011
2024-12-02-06:59:57-root-INFO: Regularization Change: 0.000 -> 0.875
2024-12-02-06:59:57-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-06:59:57-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-06:59:58-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-06:59:58-root-INFO: grad norm: 16.534 16.094 3.790
2024-12-02-06:59:58-root-INFO: grad norm: 19.528 19.222 3.440
2024-12-02-06:59:58-root-INFO: Loss too large (259.935->260.470)! Learning rate decreased to 0.02145.
2024-12-02-06:59:59-root-INFO: grad norm: 16.452 16.227 2.714
2024-12-02-06:59:59-root-INFO: grad norm: 14.225 13.991 2.571
2024-12-02-07:00:00-root-INFO: grad norm: 12.375 12.198 2.090
2024-12-02-07:00:00-root-INFO: grad norm: 10.946 10.736 2.132
2024-12-02-07:00:01-root-INFO: grad norm: 9.762 9.602 1.761
2024-12-02-07:00:01-root-INFO: grad norm: 8.835 8.637 1.862
2024-12-02-07:00:02-root-INFO: Loss Change: 260.181 -> 254.223
2024-12-02-07:00:02-root-INFO: Regularization Change: 0.000 -> 0.844
2024-12-02-07:00:02-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-07:00:02-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-07:00:02-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-07:00:02-root-INFO: grad norm: 15.337 14.939 3.470
2024-12-02-07:00:02-root-INFO: grad norm: 18.234 17.949 3.207
2024-12-02-07:00:03-root-INFO: Loss too large (254.721->255.234)! Learning rate decreased to 0.02214.
2024-12-02-07:00:03-root-INFO: grad norm: 15.533 15.326 2.529
2024-12-02-07:00:04-root-INFO: grad norm: 13.577 13.354 2.454
2024-12-02-07:00:04-root-INFO: grad norm: 11.975 11.808 1.988
2024-12-02-07:00:04-root-INFO: grad norm: 10.721 10.522 2.058
2024-12-02-07:00:05-root-INFO: grad norm: 9.678 9.526 1.706
2024-12-02-07:00:06-root-INFO: grad norm: 8.848 8.661 1.809
2024-12-02-07:00:06-root-INFO: Loss Change: 254.972 -> 249.511
2024-12-02-07:00:06-root-INFO: Regularization Change: 0.000 -> 0.813
2024-12-02-07:00:06-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-07:00:06-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-07:00:06-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-07:00:06-root-INFO: grad norm: 16.157 15.750 3.603
2024-12-02-07:00:07-root-INFO: grad norm: 19.330 19.048 3.292
2024-12-02-07:00:07-root-INFO: Loss too large (250.621->251.380)! Learning rate decreased to 0.02285.
2024-12-02-07:00:08-root-INFO: grad norm: 16.468 16.258 2.621
2024-12-02-07:00:08-root-INFO: grad norm: 14.408 14.191 2.490
2024-12-02-07:00:09-root-INFO: grad norm: 12.718 12.551 2.055
2024-12-02-07:00:09-root-INFO: grad norm: 11.407 11.216 2.077
2024-12-02-07:00:09-root-INFO: grad norm: 10.325 10.174 1.762
2024-12-02-07:00:10-root-INFO: grad norm: 9.477 9.300 1.819
2024-12-02-07:00:10-root-INFO: Loss Change: 250.737 -> 245.312
2024-12-02-07:00:10-root-INFO: Regularization Change: 0.000 -> 0.808
2024-12-02-07:00:10-root-INFO: Undo step: 120
2024-12-02-07:00:10-root-INFO: Undo step: 121
2024-12-02-07:00:10-root-INFO: Undo step: 122
2024-12-02-07:00:10-root-INFO: Undo step: 123
2024-12-02-07:00:10-root-INFO: Undo step: 124
2024-12-02-07:00:10-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-07:00:11-root-INFO: grad norm: 113.670 111.422 22.496
2024-12-02-07:00:11-root-INFO: grad norm: 59.921 58.556 12.716
2024-12-02-07:00:12-root-INFO: grad norm: 40.918 39.660 10.067
2024-12-02-07:00:12-root-INFO: grad norm: 32.458 31.618 7.337
2024-12-02-07:00:12-root-INFO: grad norm: 27.666 26.878 6.554
2024-12-02-07:00:13-root-INFO: grad norm: 24.430 23.875 5.177
2024-12-02-07:00:13-root-INFO: grad norm: 22.170 21.646 4.789
2024-12-02-07:00:14-root-INFO: grad norm: 20.406 20.009 4.008
2024-12-02-07:00:14-root-INFO: Loss Change: 622.276 -> 304.480
2024-12-02-07:00:14-root-INFO: Regularization Change: 0.000 -> 36.597
2024-12-02-07:00:14-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-07:00:14-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-07:00:14-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-07:00:15-root-INFO: grad norm: 23.980 23.518 4.686
2024-12-02-07:00:15-root-INFO: grad norm: 21.872 21.471 4.169
2024-12-02-07:00:16-root-INFO: grad norm: 21.200 20.909 3.499
2024-12-02-07:00:16-root-INFO: grad norm: 21.089 20.719 3.929
2024-12-02-07:00:16-root-INFO: grad norm: 21.587 21.318 3.397
2024-12-02-07:00:17-root-INFO: grad norm: 22.641 22.284 4.006
2024-12-02-07:00:17-root-INFO: grad norm: 24.487 24.216 3.635
2024-12-02-07:00:18-root-INFO: grad norm: 26.738 26.376 4.387
2024-12-02-07:00:18-root-INFO: Loss Change: 304.359 -> 282.064
2024-12-02-07:00:18-root-INFO: Regularization Change: 0.000 -> 4.985
2024-12-02-07:00:18-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-07:00:18-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-07:00:18-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-07:00:19-root-INFO: grad norm: 37.235 36.752 5.979
2024-12-02-07:00:19-root-INFO: Loss too large (284.837->285.550)! Learning rate decreased to 0.02078.
2024-12-02-07:00:19-root-INFO: grad norm: 27.108 26.790 4.144
2024-12-02-07:00:20-root-INFO: grad norm: 19.562 19.327 3.020
2024-12-02-07:00:20-root-INFO: grad norm: 15.368 15.132 2.679
2024-12-02-07:00:21-root-INFO: grad norm: 12.544 12.354 2.174
2024-12-02-07:00:21-root-INFO: grad norm: 10.755 10.541 2.133
2024-12-02-07:00:22-root-INFO: grad norm: 9.493 9.308 1.863
2024-12-02-07:00:22-root-INFO: grad norm: 8.631 8.423 1.884
2024-12-02-07:00:23-root-INFO: Loss Change: 284.837 -> 267.470
2024-12-02-07:00:23-root-INFO: Regularization Change: 0.000 -> 1.765
2024-12-02-07:00:23-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-07:00:23-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-07:00:23-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-07:00:23-root-INFO: grad norm: 15.099 14.734 3.301
2024-12-02-07:00:23-root-INFO: grad norm: 16.508 16.282 2.722
2024-12-02-07:00:24-root-INFO: grad norm: 19.769 19.543 2.979
2024-12-02-07:00:24-root-INFO: Loss too large (266.574->266.636)! Learning rate decreased to 0.02145.
2024-12-02-07:00:25-root-INFO: grad norm: 16.194 15.985 2.593
2024-12-02-07:00:25-root-INFO: grad norm: 13.347 13.178 2.118
2024-12-02-07:00:26-root-INFO: grad norm: 11.393 11.209 2.038
2024-12-02-07:00:26-root-INFO: grad norm: 9.894 9.737 1.757
2024-12-02-07:00:27-root-INFO: grad norm: 8.822 8.644 1.764
2024-12-02-07:00:27-root-INFO: Loss Change: 268.267 -> 259.715
2024-12-02-07:00:27-root-INFO: Regularization Change: 0.000 -> 1.395
2024-12-02-07:00:27-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-07:00:27-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-07:00:27-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-07:00:27-root-INFO: grad norm: 14.556 14.222 3.102
2024-12-02-07:00:28-root-INFO: grad norm: 16.302 16.087 2.640
2024-12-02-07:00:28-root-INFO: grad norm: 19.769 19.555 2.903
2024-12-02-07:00:28-root-INFO: Loss too large (259.193->259.568)! Learning rate decreased to 0.02214.
2024-12-02-07:00:29-root-INFO: grad norm: 16.188 15.985 2.555
2024-12-02-07:00:29-root-INFO: grad norm: 13.328 13.172 2.032
2024-12-02-07:00:30-root-INFO: grad norm: 11.317 11.144 1.967
2024-12-02-07:00:30-root-INFO: grad norm: 9.752 9.611 1.652
2024-12-02-07:00:31-root-INFO: grad norm: 8.616 8.453 1.670
2024-12-02-07:00:31-root-INFO: Loss Change: 260.232 -> 253.125
2024-12-02-07:00:31-root-INFO: Regularization Change: 0.000 -> 1.170
2024-12-02-07:00:31-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-07:00:31-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-07:00:31-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-07:00:31-root-INFO: grad norm: 15.204 14.851 3.258
2024-12-02-07:00:32-root-INFO: grad norm: 17.116 16.902 2.698
2024-12-02-07:00:32-root-INFO: grad norm: 20.787 20.566 3.022
2024-12-02-07:00:33-root-INFO: Loss too large (253.537->254.184)! Learning rate decreased to 0.02285.
2024-12-02-07:00:33-root-INFO: grad norm: 16.778 16.577 2.587
2024-12-02-07:00:33-root-INFO: grad norm: 13.557 13.404 2.029
2024-12-02-07:00:34-root-INFO: grad norm: 11.294 11.128 1.930
2024-12-02-07:00:34-root-INFO: grad norm: 9.556 9.423 1.592
2024-12-02-07:00:35-root-INFO: grad norm: 8.302 8.146 1.600
2024-12-02-07:00:35-root-INFO: Loss Change: 254.168 -> 247.590
2024-12-02-07:00:35-root-INFO: Regularization Change: 0.000 -> 1.065
2024-12-02-07:00:35-root-INFO: Undo step: 120
2024-12-02-07:00:35-root-INFO: Undo step: 121
2024-12-02-07:00:35-root-INFO: Undo step: 122
2024-12-02-07:00:35-root-INFO: Undo step: 123
2024-12-02-07:00:35-root-INFO: Undo step: 124
2024-12-02-07:00:35-root-INFO: step: 125 lr_xt 0.02423882
2024-12-02-07:00:36-root-INFO: grad norm: 130.877 129.168 21.083
2024-12-02-07:00:36-root-INFO: grad norm: 61.424 60.037 12.984
2024-12-02-07:00:36-root-INFO: grad norm: 46.336 45.407 9.236
2024-12-02-07:00:37-root-INFO: grad norm: 42.136 41.237 8.659
2024-12-02-07:00:37-root-INFO: grad norm: 44.467 43.921 6.950
2024-12-02-07:00:38-root-INFO: grad norm: 45.939 45.309 7.582
2024-12-02-07:00:38-root-INFO: grad norm: 49.224 48.798 6.463
2024-12-02-07:00:39-root-INFO: grad norm: 49.947 49.339 7.772
2024-12-02-07:00:39-root-INFO: Loss Change: 675.279 -> 324.569
2024-12-02-07:00:39-root-INFO: Regularization Change: 0.000 -> 40.231
2024-12-02-07:00:39-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-02-07:00:39-root-INFO: Coefficient of regularization decay: 0.00034 -> 0.00035.
2024-12-02-07:00:40-root-INFO: step: 124 lr_xt 0.02515763
2024-12-02-07:00:40-root-INFO: grad norm: 63.735 63.122 8.818
2024-12-02-07:00:40-root-INFO: Loss too large (330.785->333.356)! Learning rate decreased to 0.02013.
2024-12-02-07:00:40-root-INFO: grad norm: 43.768 43.223 6.886
2024-12-02-07:00:41-root-INFO: grad norm: 25.651 25.292 4.277
2024-12-02-07:00:41-root-INFO: grad norm: 21.342 20.908 4.284
2024-12-02-07:00:42-root-INFO: grad norm: 18.529 18.195 3.499
2024-12-02-07:00:42-root-INFO: grad norm: 16.879 16.485 3.627
2024-12-02-07:00:43-root-INFO: grad norm: 15.510 15.199 3.091
2024-12-02-07:00:43-root-INFO: grad norm: 14.567 14.208 3.211
2024-12-02-07:00:44-root-INFO: Loss Change: 330.785 -> 285.703
2024-12-02-07:00:44-root-INFO: Regularization Change: 0.000 -> 4.389
2024-12-02-07:00:44-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-02-07:00:44-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-07:00:44-root-INFO: step: 123 lr_xt 0.02597490
2024-12-02-07:00:44-root-INFO: grad norm: 19.084 18.717 3.726
2024-12-02-07:00:45-root-INFO: grad norm: 23.082 22.798 3.607
2024-12-02-07:00:45-root-INFO: grad norm: 30.881 30.639 3.858
2024-12-02-07:00:46-root-INFO: Loss too large (284.363->286.585)! Learning rate decreased to 0.02078.
2024-12-02-07:00:46-root-INFO: grad norm: 27.226 26.958 3.811
2024-12-02-07:00:47-root-INFO: grad norm: 23.320 23.118 3.063
2024-12-02-07:00:47-root-INFO: grad norm: 21.031 20.788 3.182
2024-12-02-07:00:48-root-INFO: grad norm: 18.930 18.743 2.652
2024-12-02-07:00:48-root-INFO: grad norm: 17.590 17.366 2.801
2024-12-02-07:00:49-root-INFO: Loss Change: 286.303 -> 271.987
2024-12-02-07:00:49-root-INFO: Regularization Change: 0.000 -> 2.481
2024-12-02-07:00:49-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-02-07:00:49-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00035.
2024-12-02-07:00:49-root-INFO: step: 122 lr_xt 0.02681440
2024-12-02-07:00:49-root-INFO: grad norm: 23.393 23.057 3.950
2024-12-02-07:00:49-root-INFO: Loss too large (273.214->273.784)! Learning rate decreased to 0.02145.
2024-12-02-07:00:49-root-INFO: grad norm: 21.003 20.800 2.913
2024-12-02-07:00:50-root-INFO: grad norm: 19.153 18.971 2.634
2024-12-02-07:00:50-root-INFO: grad norm: 17.898 17.705 2.616
2024-12-02-07:00:51-root-INFO: grad norm: 16.689 16.527 2.317
2024-12-02-07:00:52-root-INFO: grad norm: 15.835 15.653 2.398
2024-12-02-07:00:52-root-INFO: grad norm: 15.025 14.873 2.131
2024-12-02-07:00:52-root-INFO: grad norm: 14.441 14.266 2.240
2024-12-02-07:00:53-root-INFO: Loss Change: 273.214 -> 262.155
2024-12-02-07:00:53-root-INFO: Regularization Change: 0.000 -> 1.596
2024-12-02-07:00:53-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-02-07:00:53-root-INFO: Coefficient of regularization decay: 0.00035 -> 0.00036.
2024-12-02-07:00:53-root-INFO: step: 121 lr_xt 0.02767658
2024-12-02-07:00:53-root-INFO: grad norm: 20.047 19.732 3.539
2024-12-02-07:00:53-root-INFO: Loss too large (262.864->263.249)! Learning rate decreased to 0.02214.
2024-12-02-07:00:54-root-INFO: grad norm: 18.253 18.081 2.500
2024-12-02-07:00:54-root-INFO: grad norm: 17.186 17.029 2.315
2024-12-02-07:00:55-root-INFO: grad norm: 16.441 16.279 2.299
2024-12-02-07:00:55-root-INFO: grad norm: 15.707 15.569 2.076
2024-12-02-07:00:56-root-INFO: grad norm: 15.161 15.007 2.150
2024-12-02-07:00:56-root-INFO: grad norm: 14.621 14.492 1.942
2024-12-02-07:00:57-root-INFO: grad norm: 14.219 14.073 2.038
2024-12-02-07:00:57-root-INFO: Loss Change: 262.864 -> 254.397
2024-12-02-07:00:57-root-INFO: Regularization Change: 0.000 -> 1.301
2024-12-02-07:00:57-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-02-07:00:57-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-07:00:57-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-07:00:57-root-INFO: grad norm: 20.346 20.042 3.502
2024-12-02-07:00:58-root-INFO: Loss too large (255.618->256.360)! Learning rate decreased to 0.02285.
2024-12-02-07:00:58-root-INFO: grad norm: 18.443 18.292 2.356
2024-12-02-07:00:58-root-INFO: grad norm: 17.278 17.132 2.241
2024-12-02-07:00:59-root-INFO: grad norm: 16.445 16.303 2.156
2024-12-02-07:00:59-root-INFO: grad norm: 15.606 15.478 1.993
2024-12-02-07:01:00-root-INFO: grad norm: 14.985 14.849 2.014
2024-12-02-07:01:00-root-INFO: grad norm: 14.366 14.246 1.854
2024-12-02-07:01:01-root-INFO: grad norm: 13.906 13.775 1.906
2024-12-02-07:01:01-root-INFO: Loss Change: 255.618 -> 248.076
2024-12-02-07:01:01-root-INFO: Regularization Change: 0.000 -> 1.140
2024-12-02-07:01:01-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-07:01:01-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-07:01:01-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-07:01:01-root-INFO: grad norm: 23.486 23.092 4.282
2024-12-02-07:01:02-root-INFO: Loss too large (249.931->251.602)! Learning rate decreased to 0.02358.
2024-12-02-07:01:02-root-INFO: grad norm: 21.266 21.127 2.431
2024-12-02-07:01:03-root-INFO: grad norm: 19.527 19.367 2.495
2024-12-02-07:01:03-root-INFO: grad norm: 18.324 18.195 2.176
2024-12-02-07:01:03-root-INFO: grad norm: 17.083 16.949 2.132
2024-12-02-07:01:04-root-INFO: grad norm: 16.200 16.076 2.003
2024-12-02-07:01:04-root-INFO: grad norm: 15.316 15.193 1.934
2024-12-02-07:01:05-root-INFO: grad norm: 14.679 14.559 1.874
2024-12-02-07:01:05-root-INFO: Loss Change: 249.931 -> 242.197
2024-12-02-07:01:05-root-INFO: Regularization Change: 0.000 -> 1.079
2024-12-02-07:01:05-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-07:01:05-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-07:01:05-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-07:01:06-root-INFO: grad norm: 16.608 16.419 2.500
2024-12-02-07:01:06-root-INFO: Loss too large (243.059->243.756)! Learning rate decreased to 0.02432.
2024-12-02-07:01:06-root-INFO: grad norm: 15.580 15.451 2.002
2024-12-02-07:01:07-root-INFO: grad norm: 14.836 14.709 1.940
2024-12-02-07:01:07-root-INFO: grad norm: 14.273 14.150 1.864
2024-12-02-07:01:08-root-INFO: grad norm: 13.717 13.601 1.784
2024-12-02-07:01:08-root-INFO: grad norm: 13.290 13.172 1.763
2024-12-02-07:01:09-root-INFO: grad norm: 12.871 12.760 1.687
2024-12-02-07:01:09-root-INFO: grad norm: 12.549 12.435 1.687
2024-12-02-07:01:09-root-INFO: Loss Change: 243.059 -> 237.437
2024-12-02-07:01:09-root-INFO: Regularization Change: 0.000 -> 0.947
2024-12-02-07:01:09-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-07:01:09-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-07:01:10-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-07:01:10-root-INFO: grad norm: 21.852 21.498 3.920
2024-12-02-07:01:10-root-INFO: Loss too large (239.350->240.927)! Learning rate decreased to 0.02509.
2024-12-02-07:01:10-root-INFO: grad norm: 19.817 19.694 2.202
2024-12-02-07:01:11-root-INFO: grad norm: 18.679 18.532 2.344
2024-12-02-07:01:11-root-INFO: grad norm: 17.807 17.692 2.025
2024-12-02-07:01:12-root-INFO: grad norm: 16.861 16.736 2.050
2024-12-02-07:01:13-root-INFO: grad norm: 16.147 16.035 1.899
2024-12-02-07:01:13-root-INFO: grad norm: 15.407 15.292 1.883
2024-12-02-07:01:14-root-INFO: grad norm: 14.855 14.746 1.795
2024-12-02-07:01:14-root-INFO: Loss Change: 239.350 -> 232.888
2024-12-02-07:01:14-root-INFO: Regularization Change: 0.000 -> 0.951
2024-12-02-07:01:14-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-07:01:14-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-07:01:14-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-07:01:14-root-INFO: grad norm: 20.372 20.116 3.218
2024-12-02-07:01:14-root-INFO: Loss too large (234.079->235.851)! Learning rate decreased to 0.02587.
2024-12-02-07:01:15-root-INFO: grad norm: 18.833 18.717 2.093
2024-12-02-07:01:15-root-INFO: grad norm: 17.598 17.463 2.179
2024-12-02-07:01:16-root-INFO: grad norm: 16.729 16.619 1.912
2024-12-02-07:01:16-root-INFO: grad norm: 15.848 15.729 1.942
2024-12-02-07:01:17-root-INFO: grad norm: 15.210 15.104 1.793
2024-12-02-07:01:17-root-INFO: grad norm: 14.579 14.467 1.805
2024-12-02-07:01:18-root-INFO: grad norm: 14.119 14.015 1.706
2024-12-02-07:01:18-root-INFO: Loss Change: 234.079 -> 228.341
2024-12-02-07:01:18-root-INFO: Regularization Change: 0.000 -> 0.890
2024-12-02-07:01:18-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-07:01:18-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-07:01:18-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-07:01:19-root-INFO: grad norm: 20.406 20.163 3.144
2024-12-02-07:01:19-root-INFO: Loss too large (230.040->231.847)! Learning rate decreased to 0.02668.
2024-12-02-07:01:19-root-INFO: grad norm: 18.662 18.547 2.071
2024-12-02-07:01:20-root-INFO: grad norm: 17.333 17.203 2.117
2024-12-02-07:01:20-root-INFO: grad norm: 16.405 16.298 1.874
2024-12-02-07:01:21-root-INFO: grad norm: 15.478 15.363 1.883
2024-12-02-07:01:21-root-INFO: grad norm: 14.802 14.698 1.745
2024-12-02-07:01:22-root-INFO: grad norm: 14.139 14.031 1.746
2024-12-02-07:01:22-root-INFO: grad norm: 13.649 13.549 1.650
2024-12-02-07:01:22-root-INFO: Loss Change: 230.040 -> 224.350
2024-12-02-07:01:22-root-INFO: Regularization Change: 0.000 -> 0.873
2024-12-02-07:01:22-root-INFO: Undo step: 115
2024-12-02-07:01:22-root-INFO: Undo step: 116
2024-12-02-07:01:22-root-INFO: Undo step: 117
2024-12-02-07:01:22-root-INFO: Undo step: 118
2024-12-02-07:01:22-root-INFO: Undo step: 119
2024-12-02-07:01:23-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-07:01:23-root-INFO: grad norm: 112.651 110.505 21.884
2024-12-02-07:01:23-root-INFO: grad norm: 65.125 63.910 12.522
2024-12-02-07:01:24-root-INFO: grad norm: 48.736 47.320 11.664
2024-12-02-07:01:24-root-INFO: grad norm: 43.036 42.105 8.903
2024-12-02-07:01:25-root-INFO: grad norm: 37.317 36.295 8.670
2024-12-02-07:01:25-root-INFO: grad norm: 33.432 32.769 6.623
2024-12-02-07:01:26-root-INFO: grad norm: 30.587 29.964 6.142
2024-12-02-07:01:26-root-INFO: grad norm: 29.089 28.609 5.263
2024-12-02-07:01:27-root-INFO: Loss Change: 590.631 -> 275.182
2024-12-02-07:01:27-root-INFO: Regularization Change: 0.000 -> 42.322
2024-12-02-07:01:27-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-07:01:27-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-07:01:27-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-07:01:27-root-INFO: grad norm: 30.058 29.624 5.088
2024-12-02-07:01:28-root-INFO: grad norm: 31.478 31.118 4.747
2024-12-02-07:01:28-root-INFO: grad norm: 33.517 33.200 4.602
2024-12-02-07:01:29-root-INFO: grad norm: 35.986 35.666 4.790
2024-12-02-07:01:29-root-INFO: grad norm: 37.788 37.463 4.945
2024-12-02-07:01:30-root-INFO: grad norm: 39.706 39.373 5.132
2024-12-02-07:01:30-root-INFO: grad norm: 41.230 40.874 5.406
2024-12-02-07:01:31-root-INFO: grad norm: 42.157 41.795 5.518
2024-12-02-07:01:31-root-INFO: Loss Change: 275.498 -> 262.185
2024-12-02-07:01:31-root-INFO: Regularization Change: 0.000 -> 5.329
2024-12-02-07:01:31-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-07:01:31-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-07:01:31-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-07:01:31-root-INFO: grad norm: 44.972 44.535 6.252
2024-12-02-07:01:32-root-INFO: grad norm: 44.319 43.930 5.855
2024-12-02-07:01:32-root-INFO: grad norm: 42.997 42.600 5.826
2024-12-02-07:01:33-root-INFO: grad norm: 41.488 41.102 5.649
2024-12-02-07:01:33-root-INFO: grad norm: 40.055 39.674 5.514
2024-12-02-07:01:34-root-INFO: grad norm: 38.835 38.460 5.381
2024-12-02-07:01:34-root-INFO: grad norm: 37.744 37.377 5.254
2024-12-02-07:01:35-root-INFO: grad norm: 36.863 36.501 5.153
2024-12-02-07:01:35-root-INFO: Loss Change: 263.982 -> 247.986
2024-12-02-07:01:35-root-INFO: Regularization Change: 0.000 -> 2.756
2024-12-02-07:01:35-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-07:01:35-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-07:01:35-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-07:01:35-root-INFO: grad norm: 45.401 44.730 7.778
2024-12-02-07:01:36-root-INFO: Loss too large (253.534->253.629)! Learning rate decreased to 0.02509.
2024-12-02-07:01:36-root-INFO: grad norm: 28.715 28.445 3.931
2024-12-02-07:01:36-root-INFO: grad norm: 17.727 17.482 2.934
2024-12-02-07:01:37-root-INFO: grad norm: 12.891 12.720 2.094
2024-12-02-07:01:37-root-INFO: grad norm: 9.862 9.676 1.905
2024-12-02-07:01:38-root-INFO: grad norm: 8.169 8.001 1.650
2024-12-02-07:01:38-root-INFO: grad norm: 7.075 6.894 1.591
2024-12-02-07:01:39-root-INFO: grad norm: 6.412 6.236 1.491
2024-12-02-07:01:39-root-INFO: Loss Change: 253.534 -> 231.556
2024-12-02-07:01:39-root-INFO: Regularization Change: 0.000 -> 1.756
2024-12-02-07:01:39-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-07:01:39-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-07:01:39-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-07:01:40-root-INFO: grad norm: 11.529 11.165 2.876
2024-12-02-07:01:40-root-INFO: grad norm: 12.198 12.032 2.005
2024-12-02-07:01:41-root-INFO: grad norm: 14.828 14.643 2.333
2024-12-02-07:01:41-root-INFO: Loss too large (230.446->230.525)! Learning rate decreased to 0.02587.
2024-12-02-07:01:41-root-INFO: grad norm: 12.470 12.326 1.886
2024-12-02-07:01:42-root-INFO: grad norm: 10.481 10.332 1.760
2024-12-02-07:01:42-root-INFO: grad norm: 9.210 9.075 1.574
2024-12-02-07:01:43-root-INFO: grad norm: 8.159 8.019 1.502
2024-12-02-07:01:43-root-INFO: grad norm: 7.440 7.304 1.413
2024-12-02-07:01:43-root-INFO: Loss Change: 231.915 -> 225.466
2024-12-02-07:01:43-root-INFO: Regularization Change: 0.000 -> 1.278
2024-12-02-07:01:43-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-07:01:43-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-07:01:44-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-07:01:44-root-INFO: grad norm: 13.759 13.437 2.962
2024-12-02-07:01:44-root-INFO: grad norm: 15.869 15.715 2.206
2024-12-02-07:01:44-root-INFO: Loss too large (226.129->226.264)! Learning rate decreased to 0.02668.
2024-12-02-07:01:45-root-INFO: grad norm: 13.363 13.213 2.002
2024-12-02-07:01:45-root-INFO: grad norm: 11.808 11.687 1.684
2024-12-02-07:01:46-root-INFO: grad norm: 10.428 10.302 1.613
2024-12-02-07:01:46-root-INFO: grad norm: 9.500 9.386 1.471
2024-12-02-07:01:47-root-INFO: grad norm: 8.680 8.563 1.418
2024-12-02-07:01:47-root-INFO: grad norm: 8.103 7.990 1.345
2024-12-02-07:01:48-root-INFO: Loss Change: 226.500 -> 220.648
2024-12-02-07:01:48-root-INFO: Regularization Change: 0.000 -> 1.079
2024-12-02-07:01:48-root-INFO: Undo step: 115
2024-12-02-07:01:48-root-INFO: Undo step: 116
2024-12-02-07:01:48-root-INFO: Undo step: 117
2024-12-02-07:01:48-root-INFO: Undo step: 118
2024-12-02-07:01:48-root-INFO: Undo step: 119
2024-12-02-07:01:48-root-INFO: step: 120 lr_xt 0.02856188
2024-12-02-07:01:48-root-INFO: grad norm: 110.440 107.645 24.686
2024-12-02-07:01:49-root-INFO: grad norm: 59.666 58.398 12.237
2024-12-02-07:01:49-root-INFO: grad norm: 45.308 44.365 9.195
2024-12-02-07:01:50-root-INFO: grad norm: 39.613 39.006 6.906
2024-12-02-07:01:50-root-INFO: grad norm: 35.796 35.164 6.694
2024-12-02-07:01:50-root-INFO: grad norm: 32.951 32.530 5.249
2024-12-02-07:01:51-root-INFO: grad norm: 30.875 30.382 5.494
2024-12-02-07:01:51-root-INFO: grad norm: 29.491 29.162 4.388
2024-12-02-07:01:52-root-INFO: Loss Change: 580.690 -> 276.085
2024-12-02-07:01:52-root-INFO: Regularization Change: 0.000 -> 39.935
2024-12-02-07:01:52-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-02-07:01:52-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00036.
2024-12-02-07:01:52-root-INFO: step: 119 lr_xt 0.02947075
2024-12-02-07:01:52-root-INFO: grad norm: 25.525 25.191 4.117
2024-12-02-07:01:52-root-INFO: grad norm: 25.159 24.896 3.628
2024-12-02-07:01:53-root-INFO: grad norm: 25.759 25.494 3.682
2024-12-02-07:01:53-root-INFO: grad norm: 26.619 26.406 3.359
2024-12-02-07:01:54-root-INFO: grad norm: 27.621 27.395 3.522
2024-12-02-07:01:54-root-INFO: grad norm: 28.448 28.267 3.212
2024-12-02-07:01:55-root-INFO: grad norm: 29.197 29.002 3.370
2024-12-02-07:01:55-root-INFO: grad norm: 29.612 29.451 3.079
2024-12-02-07:01:56-root-INFO: Loss Change: 273.457 -> 251.602
2024-12-02-07:01:56-root-INFO: Regularization Change: 0.000 -> 5.785
2024-12-02-07:01:56-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-02-07:01:56-root-INFO: Coefficient of regularization decay: 0.00036 -> 0.00037.
2024-12-02-07:01:56-root-INFO: step: 118 lr_xt 0.03040366
2024-12-02-07:01:56-root-INFO: grad norm: 30.638 30.472 3.181
2024-12-02-07:01:56-root-INFO: grad norm: 30.501 30.353 2.998
2024-12-02-07:01:57-root-INFO: grad norm: 30.047 29.898 2.980
2024-12-02-07:01:57-root-INFO: grad norm: 29.627 29.494 2.803
2024-12-02-07:01:58-root-INFO: grad norm: 29.187 29.041 2.915
2024-12-02-07:01:58-root-INFO: grad norm: 28.864 28.736 2.719
2024-12-02-07:01:59-root-INFO: grad norm: 28.636 28.487 2.915
2024-12-02-07:01:59-root-INFO: grad norm: 28.443 28.312 2.725
2024-12-02-07:02:00-root-INFO: Loss Change: 251.945 -> 239.522
2024-12-02-07:02:00-root-INFO: Regularization Change: 0.000 -> 2.916
2024-12-02-07:02:00-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-02-07:02:00-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00037.
2024-12-02-07:02:00-root-INFO: step: 117 lr_xt 0.03136105
2024-12-02-07:02:00-root-INFO: grad norm: 30.875 30.632 3.860
2024-12-02-07:02:00-root-INFO: grad norm: 29.399 29.244 3.017
2024-12-02-07:02:01-root-INFO: grad norm: 27.961 27.807 2.935
2024-12-02-07:02:01-root-INFO: grad norm: 27.447 27.298 2.859
2024-12-02-07:02:02-root-INFO: grad norm: 26.980 26.827 2.866
2024-12-02-07:02:02-root-INFO: grad norm: 26.780 26.630 2.831
2024-12-02-07:02:03-root-INFO: grad norm: 26.607 26.454 2.845
2024-12-02-07:02:03-root-INFO: grad norm: 26.545 26.393 2.836
2024-12-02-07:02:04-root-INFO: Loss Change: 240.244 -> 230.319
2024-12-02-07:02:04-root-INFO: Regularization Change: 0.000 -> 2.080
2024-12-02-07:02:04-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
2024-12-02-07:02:04-root-INFO: Coefficient of regularization decay: 0.00037 -> 0.00038.
2024-12-02-07:02:04-root-INFO: step: 116 lr_xt 0.03234339
2024-12-02-07:02:04-root-INFO: grad norm: 29.834 29.573 3.936
2024-12-02-07:02:04-root-INFO: Loss too large (231.267->231.535)! Learning rate decreased to 0.02587.
2024-12-02-07:02:05-root-INFO: grad norm: 20.183 20.046 2.349
2024-12-02-07:02:05-root-INFO: grad norm: 13.580 13.443 1.928
2024-12-02-07:02:05-root-INFO: grad norm: 10.594 10.475 1.580
2024-12-02-07:02:06-root-INFO: grad norm: 8.601 8.476 1.463
2024-12-02-07:02:06-root-INFO: grad norm: 7.449 7.324 1.358
2024-12-02-07:02:07-root-INFO: grad norm: 6.642 6.514 1.297
2024-12-02-07:02:07-root-INFO: grad norm: 6.120 5.989 1.262
2024-12-02-07:02:08-root-INFO: Loss Change: 231.267 -> 219.402
2024-12-02-07:02:08-root-INFO: Regularization Change: 0.000 -> 1.270
2024-12-02-07:02:08-root-INFO: Learning rate of xt decay: 0.09773 -> 0.09891.
2024-12-02-07:02:08-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-07:02:08-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-07:02:08-root-INFO: grad norm: 11.875 11.598 2.549
2024-12-02-07:02:09-root-INFO: grad norm: 13.210 13.063 1.961
2024-12-02-07:02:09-root-INFO: grad norm: 16.737 16.595 2.176
2024-12-02-07:02:09-root-INFO: Loss too large (219.340->220.044)! Learning rate decreased to 0.02668.
2024-12-02-07:02:10-root-INFO: grad norm: 14.282 14.153 1.915
2024-12-02-07:02:10-root-INFO: grad norm: 12.088 11.978 1.623
2024-12-02-07:02:11-root-INFO: grad norm: 10.737 10.623 1.566
2024-12-02-07:02:11-root-INFO: grad norm: 9.573 9.473 1.379
2024-12-02-07:02:12-root-INFO: grad norm: 8.785 8.676 1.381
2024-12-02-07:02:12-root-INFO: Loss Change: 220.135 -> 214.627
2024-12-02-07:02:12-root-INFO: Regularization Change: 0.000 -> 1.115
2024-12-02-07:02:12-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-07:02:12-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-07:02:12-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-07:02:13-root-INFO: grad norm: 14.098 13.863 2.562
2024-12-02-07:02:13-root-INFO: Loss too large (215.663->215.685)! Learning rate decreased to 0.02751.
2024-12-02-07:02:13-root-INFO: grad norm: 12.061 11.944 1.680
2024-12-02-07:02:14-root-INFO: grad norm: 10.737 10.624 1.558
2024-12-02-07:02:14-root-INFO: grad norm: 9.841 9.734 1.446
2024-12-02-07:02:15-root-INFO: grad norm: 9.084 8.987 1.327
2024-12-02-07:02:15-root-INFO: grad norm: 8.551 8.450 1.314
2024-12-02-07:02:15-root-INFO: grad norm: 8.094 8.003 1.215
2024-12-02-07:02:16-root-INFO: grad norm: 7.766 7.668 1.232
2024-12-02-07:02:16-root-INFO: Loss Change: 215.663 -> 210.427
2024-12-02-07:02:16-root-INFO: Regularization Change: 0.000 -> 0.928
2024-12-02-07:02:16-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-07:02:16-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-07:02:16-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-07:02:17-root-INFO: grad norm: 13.380 13.139 2.530
2024-12-02-07:02:17-root-INFO: Loss too large (211.052->211.222)! Learning rate decreased to 0.02836.
2024-12-02-07:02:17-root-INFO: grad norm: 11.962 11.860 1.556
2024-12-02-07:02:18-root-INFO: grad norm: 11.330 11.229 1.505
2024-12-02-07:02:18-root-INFO: grad norm: 10.926 10.836 1.400
2024-12-02-07:02:19-root-INFO: grad norm: 10.588 10.503 1.338
2024-12-02-07:02:19-root-INFO: grad norm: 10.362 10.277 1.327
2024-12-02-07:02:20-root-INFO: grad norm: 10.172 10.093 1.266
2024-12-02-07:02:20-root-INFO: grad norm: 10.057 9.974 1.283
2024-12-02-07:02:21-root-INFO: Loss Change: 211.052 -> 206.633
2024-12-02-07:02:21-root-INFO: Regularization Change: 0.000 -> 0.863
2024-12-02-07:02:21-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-07:02:21-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-07:02:21-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-07:02:21-root-INFO: grad norm: 17.466 17.204 3.013
2024-12-02-07:02:21-root-INFO: Loss too large (207.984->209.206)! Learning rate decreased to 0.02923.
2024-12-02-07:02:22-root-INFO: grad norm: 15.948 15.845 1.807
2024-12-02-07:02:22-root-INFO: grad norm: 15.148 15.045 1.769
2024-12-02-07:02:23-root-INFO: grad norm: 14.618 14.529 1.619
2024-12-02-07:02:23-root-INFO: grad norm: 14.104 14.017 1.567
2024-12-02-07:02:23-root-INFO: grad norm: 13.771 13.686 1.525
2024-12-02-07:02:24-root-INFO: grad norm: 13.453 13.372 1.472
2024-12-02-07:02:24-root-INFO: grad norm: 13.258 13.176 1.467
2024-12-02-07:02:25-root-INFO: Loss Change: 207.984 -> 203.318
2024-12-02-07:02:25-root-INFO: Regularization Change: 0.000 -> 0.853
2024-12-02-07:02:25-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-07:02:25-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-07:02:25-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-07:02:25-root-INFO: grad norm: 17.221 17.068 2.293
2024-12-02-07:02:25-root-INFO: Loss too large (204.189->205.899)! Learning rate decreased to 0.03012.
2024-12-02-07:02:26-root-INFO: grad norm: 16.175 16.081 1.735
2024-12-02-07:02:27-root-INFO: grad norm: 15.303 15.214 1.654
2024-12-02-07:02:27-root-INFO: grad norm: 14.733 14.648 1.582
2024-12-02-07:02:28-root-INFO: grad norm: 14.172 14.092 1.500
2024-12-02-07:02:28-root-INFO: grad norm: 13.808 13.727 1.490
2024-12-02-07:02:29-root-INFO: grad norm: 13.461 13.386 1.415
2024-12-02-07:02:29-root-INFO: grad norm: 13.239 13.162 1.431
2024-12-02-07:02:29-root-INFO: Loss Change: 204.189 -> 199.939
2024-12-02-07:02:29-root-INFO: Regularization Change: 0.000 -> 0.799
2024-12-02-07:02:29-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-07:02:29-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-07:02:29-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-07:02:30-root-INFO: grad norm: 17.399 17.240 2.347
2024-12-02-07:02:30-root-INFO: Loss too large (201.034->202.933)! Learning rate decreased to 0.03103.
2024-12-02-07:02:30-root-INFO: grad norm: 16.482 16.393 1.707
2024-12-02-07:02:31-root-INFO: grad norm: 15.741 15.652 1.669
2024-12-02-07:02:31-root-INFO: grad norm: 15.220 15.139 1.572
2024-12-02-07:02:32-root-INFO: grad norm: 14.681 14.603 1.511
2024-12-02-07:02:32-root-INFO: grad norm: 14.316 14.239 1.488
2024-12-02-07:02:33-root-INFO: grad norm: 13.955 13.882 1.421
2024-12-02-07:02:33-root-INFO: grad norm: 13.715 13.640 1.430
2024-12-02-07:02:33-root-INFO: Loss Change: 201.034 -> 196.928
2024-12-02-07:02:33-root-INFO: Regularization Change: 0.000 -> 0.792
2024-12-02-07:02:33-root-INFO: Undo step: 110
2024-12-02-07:02:33-root-INFO: Undo step: 111
2024-12-02-07:02:33-root-INFO: Undo step: 112
2024-12-02-07:02:33-root-INFO: Undo step: 113
2024-12-02-07:02:33-root-INFO: Undo step: 114
2024-12-02-07:02:34-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-07:02:34-root-INFO: grad norm: 82.853 81.930 12.329
2024-12-02-07:02:34-root-INFO: grad norm: 46.742 45.987 8.369
2024-12-02-07:02:35-root-INFO: grad norm: 42.392 41.731 7.457
2024-12-02-07:02:35-root-INFO: grad norm: 35.748 35.271 5.820
2024-12-02-07:02:36-root-INFO: grad norm: 28.296 27.837 5.073
2024-12-02-07:02:36-root-INFO: grad norm: 26.332 25.968 4.364
2024-12-02-07:02:37-root-INFO: grad norm: 24.370 24.032 4.045
2024-12-02-07:02:37-root-INFO: grad norm: 23.100 22.809 3.656
2024-12-02-07:02:37-root-INFO: Loss Change: 463.522 -> 240.027
2024-12-02-07:02:37-root-INFO: Regularization Change: 0.000 -> 36.350
2024-12-02-07:02:37-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-07:02:37-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-07:02:38-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-07:02:38-root-INFO: grad norm: 22.679 22.402 3.537
2024-12-02-07:02:38-root-INFO: grad norm: 21.760 21.490 3.416
2024-12-02-07:02:39-root-INFO: grad norm: 21.415 21.189 3.101
2024-12-02-07:02:39-root-INFO: grad norm: 21.481 21.231 3.264
2024-12-02-07:02:40-root-INFO: grad norm: 21.894 21.680 3.054
2024-12-02-07:02:40-root-INFO: grad norm: 22.528 22.277 3.349
2024-12-02-07:02:41-root-INFO: grad norm: 23.531 23.309 3.221
2024-12-02-07:02:41-root-INFO: grad norm: 24.587 24.320 3.611
2024-12-02-07:02:41-root-INFO: Loss Change: 240.200 -> 225.208
2024-12-02-07:02:41-root-INFO: Regularization Change: 0.000 -> 4.559
2024-12-02-07:02:41-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-07:02:41-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-07:02:42-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-07:02:42-root-INFO: grad norm: 31.201 30.834 4.773
2024-12-02-07:02:42-root-INFO: Loss too large (227.165->227.528)! Learning rate decreased to 0.02836.
2024-12-02-07:02:42-root-INFO: grad norm: 21.482 21.241 3.208
2024-12-02-07:02:43-root-INFO: grad norm: 14.626 14.445 2.295
2024-12-02-07:02:43-root-INFO: grad norm: 11.085 10.901 2.011
2024-12-02-07:02:44-root-INFO: grad norm: 8.788 8.631 1.656
2024-12-02-07:02:44-root-INFO: grad norm: 7.422 7.244 1.612
2024-12-02-07:02:45-root-INFO: grad norm: 6.531 6.370 1.443
2024-12-02-07:02:45-root-INFO: grad norm: 5.971 5.795 1.441
2024-12-02-07:02:46-root-INFO: Loss Change: 227.165 -> 212.103
2024-12-02-07:02:46-root-INFO: Regularization Change: 0.000 -> 1.902
2024-12-02-07:02:46-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-07:02:46-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-07:02:46-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-07:02:46-root-INFO: grad norm: 11.563 11.252 2.665
2024-12-02-07:02:46-root-INFO: grad norm: 12.004 11.826 2.061
2024-12-02-07:02:47-root-INFO: grad norm: 14.046 13.877 2.169
2024-12-02-07:02:47-root-INFO: grad norm: 16.424 16.242 2.433
2024-12-02-07:02:48-root-INFO: grad norm: 19.553 19.365 2.707
2024-12-02-07:02:48-root-INFO: Loss too large (210.641->211.357)! Learning rate decreased to 0.02923.
2024-12-02-07:02:48-root-INFO: grad norm: 15.358 15.188 2.280
2024-12-02-07:02:49-root-INFO: grad norm: 12.015 11.881 1.792
2024-12-02-07:02:49-root-INFO: grad norm: 9.933 9.794 1.653
2024-12-02-07:02:50-root-INFO: Loss Change: 212.487 -> 205.668
2024-12-02-07:02:50-root-INFO: Regularization Change: 0.000 -> 1.648
2024-12-02-07:02:50-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-07:02:50-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-07:02:50-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-07:02:50-root-INFO: grad norm: 12.429 12.222 2.260
2024-12-02-07:02:50-root-INFO: grad norm: 14.394 14.238 2.114
2024-12-02-07:02:51-root-INFO: grad norm: 17.467 17.304 2.379
2024-12-02-07:02:51-root-INFO: Loss too large (205.820->206.484)! Learning rate decreased to 0.03012.
2024-12-02-07:02:52-root-INFO: grad norm: 14.038 13.894 2.008
2024-12-02-07:02:52-root-INFO: grad norm: 11.263 11.145 1.629
2024-12-02-07:02:52-root-INFO: grad norm: 9.478 9.356 1.514
2024-12-02-07:02:53-root-INFO: grad norm: 8.067 7.961 1.303
2024-12-02-07:02:53-root-INFO: grad norm: 7.092 6.977 1.275
2024-12-02-07:02:54-root-INFO: Loss Change: 206.154 -> 200.485
2024-12-02-07:02:54-root-INFO: Regularization Change: 0.000 -> 1.221
2024-12-02-07:02:54-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-07:02:54-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-07:02:54-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-07:02:54-root-INFO: grad norm: 10.951 10.746 2.110
2024-12-02-07:02:54-root-INFO: grad norm: 12.939 12.806 1.850
2024-12-02-07:02:55-root-INFO: Loss too large (200.838->200.945)! Learning rate decreased to 0.03103.
2024-12-02-07:02:55-root-INFO: grad norm: 10.922 10.807 1.579
2024-12-02-07:02:56-root-INFO: grad norm: 9.565 9.459 1.423
2024-12-02-07:02:56-root-INFO: grad norm: 8.423 8.325 1.281
2024-12-02-07:02:57-root-INFO: grad norm: 7.599 7.498 1.236
2024-12-02-07:02:57-root-INFO: grad norm: 6.904 6.811 1.134
2024-12-02-07:02:57-root-INFO: grad norm: 6.384 6.284 1.127
2024-12-02-07:02:58-root-INFO: Loss Change: 201.117 -> 196.361
2024-12-02-07:02:58-root-INFO: Regularization Change: 0.000 -> 1.041
2024-12-02-07:02:58-root-INFO: Undo step: 110
2024-12-02-07:02:58-root-INFO: Undo step: 111
2024-12-02-07:02:58-root-INFO: Undo step: 112
2024-12-02-07:02:58-root-INFO: Undo step: 113
2024-12-02-07:02:58-root-INFO: Undo step: 114
2024-12-02-07:02:58-root-INFO: step: 115 lr_xt 0.03335113
2024-12-02-07:02:58-root-INFO: grad norm: 82.188 80.768 15.211
2024-12-02-07:02:59-root-INFO: grad norm: 42.716 41.794 8.829
2024-12-02-07:02:59-root-INFO: grad norm: 31.581 30.683 7.475
2024-12-02-07:03:00-root-INFO: grad norm: 26.261 25.662 5.578
2024-12-02-07:03:00-root-INFO: grad norm: 23.770 23.161 5.347
2024-12-02-07:03:00-root-INFO: grad norm: 22.607 22.205 4.245
2024-12-02-07:03:01-root-INFO: grad norm: 22.070 21.638 4.346
2024-12-02-07:03:01-root-INFO: grad norm: 22.021 21.726 3.595
2024-12-02-07:03:02-root-INFO: Loss Change: 439.997 -> 238.455
2024-12-02-07:03:02-root-INFO: Regularization Change: 0.000 -> 31.767
2024-12-02-07:03:02-root-INFO: Learning rate of xt decay: 0.09891 -> 0.10009.
2024-12-02-07:03:02-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00038.
2024-12-02-07:03:02-root-INFO: step: 114 lr_xt 0.03438473
2024-12-02-07:03:02-root-INFO: grad norm: 20.812 20.544 3.331
2024-12-02-07:03:03-root-INFO: grad norm: 20.218 20.002 2.946
2024-12-02-07:03:03-root-INFO: grad norm: 19.824 19.575 3.128
2024-12-02-07:03:04-root-INFO: grad norm: 19.384 19.198 2.679
2024-12-02-07:03:04-root-INFO: grad norm: 19.024 18.800 2.909
2024-12-02-07:03:04-root-INFO: grad norm: 18.612 18.448 2.466
2024-12-02-07:03:05-root-INFO: grad norm: 18.336 18.135 2.712
2024-12-02-07:03:05-root-INFO: grad norm: 18.053 17.905 2.305
2024-12-02-07:03:06-root-INFO: Loss Change: 237.914 -> 220.384
2024-12-02-07:03:06-root-INFO: Regularization Change: 0.000 -> 4.642
2024-12-02-07:03:06-root-INFO: Learning rate of xt decay: 0.10009 -> 0.10129.
2024-12-02-07:03:06-root-INFO: Coefficient of regularization decay: 0.00038 -> 0.00039.
2024-12-02-07:03:06-root-INFO: step: 113 lr_xt 0.03544467
2024-12-02-07:03:06-root-INFO: grad norm: 18.614 18.387 2.894
2024-12-02-07:03:06-root-INFO: grad norm: 18.316 18.180 2.225
2024-12-02-07:03:07-root-INFO: grad norm: 18.621 18.467 2.391
2024-12-02-07:03:07-root-INFO: grad norm: 19.055 18.927 2.208
2024-12-02-07:03:08-root-INFO: grad norm: 19.547 19.406 2.344
2024-12-02-07:03:08-root-INFO: grad norm: 20.085 19.958 2.258
2024-12-02-07:03:09-root-INFO: grad norm: 20.612 20.475 2.371
2024-12-02-07:03:09-root-INFO: grad norm: 21.147 21.015 2.355
2024-12-02-07:03:10-root-INFO: Loss Change: 220.168 -> 212.057
2024-12-02-07:03:10-root-INFO: Regularization Change: 0.000 -> 2.671
2024-12-02-07:03:10-root-INFO: Learning rate of xt decay: 0.10129 -> 0.10251.
2024-12-02-07:03:10-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-07:03:10-root-INFO: step: 112 lr_xt 0.03653141
2024-12-02-07:03:10-root-INFO: grad norm: 25.630 25.320 3.975
2024-12-02-07:03:11-root-INFO: grad norm: 25.758 25.565 3.146
2024-12-02-07:03:11-root-INFO: grad norm: 26.140 25.937 3.253
2024-12-02-07:03:12-root-INFO: grad norm: 26.383 26.183 3.245
2024-12-02-07:03:12-root-INFO: grad norm: 26.597 26.394 3.275
2024-12-02-07:03:12-root-INFO: grad norm: 26.644 26.432 3.360
2024-12-02-07:03:13-root-INFO: grad norm: 26.745 26.535 3.342
2024-12-02-07:03:13-root-INFO: grad norm: 26.724 26.500 3.452
2024-12-02-07:03:14-root-INFO: Loss Change: 213.491 -> 207.378
2024-12-02-07:03:14-root-INFO: Regularization Change: 0.000 -> 1.995
2024-12-02-07:03:14-root-INFO: Learning rate of xt decay: 0.10251 -> 0.10374.
2024-12-02-07:03:14-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00039.
2024-12-02-07:03:14-root-INFO: step: 111 lr_xt 0.03764541
2024-12-02-07:03:14-root-INFO: grad norm: 30.515 30.194 4.414
2024-12-02-07:03:15-root-INFO: grad norm: 29.879 29.623 3.905
2024-12-02-07:03:15-root-INFO: grad norm: 29.196 28.945 3.820
2024-12-02-07:03:15-root-INFO: grad norm: 28.431 28.182 3.753
2024-12-02-07:03:16-root-INFO: grad norm: 27.698 27.463 3.599
2024-12-02-07:03:16-root-INFO: grad norm: 27.026 26.785 3.604
2024-12-02-07:03:17-root-INFO: grad norm: 26.407 26.183 3.435
2024-12-02-07:03:17-root-INFO: grad norm: 25.891 25.657 3.472
2024-12-02-07:03:18-root-INFO: Loss Change: 209.351 -> 201.407
2024-12-02-07:03:18-root-INFO: Regularization Change: 0.000 -> 1.620
2024-12-02-07:03:18-root-INFO: Learning rate of xt decay: 0.10374 -> 0.10498.
2024-12-02-07:03:18-root-INFO: Coefficient of regularization decay: 0.00039 -> 0.00040.
2024-12-02-07:03:18-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-07:03:18-root-INFO: grad norm: 29.941 29.599 4.512
2024-12-02-07:03:18-root-INFO: grad norm: 28.997 28.741 3.844
2024-12-02-07:03:19-root-INFO: grad norm: 27.946 27.700 3.693
2024-12-02-07:03:19-root-INFO: grad norm: 27.094 26.861 3.547
2024-12-02-07:03:20-root-INFO: grad norm: 26.212 25.994 3.373
2024-12-02-07:03:20-root-INFO: grad norm: 25.545 25.327 3.328
2024-12-02-07:03:21-root-INFO: grad norm: 24.898 24.696 3.164
2024-12-02-07:03:21-root-INFO: grad norm: 24.425 24.219 3.164
2024-12-02-07:03:22-root-INFO: Loss Change: 203.833 -> 196.284
2024-12-02-07:03:22-root-INFO: Regularization Change: 0.000 -> 1.436
2024-12-02-07:03:22-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-07:03:22-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-07:03:22-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-07:03:22-root-INFO: grad norm: 29.028 28.668 4.558
2024-12-02-07:03:22-root-INFO: grad norm: 27.808 27.570 3.634
2024-12-02-07:03:23-root-INFO: grad norm: 26.507 26.280 3.462
2024-12-02-07:03:23-root-INFO: grad norm: 25.518 25.307 3.269
2024-12-02-07:03:24-root-INFO: grad norm: 24.524 24.328 3.093
2024-12-02-07:03:24-root-INFO: grad norm: 23.811 23.619 3.023
2024-12-02-07:03:25-root-INFO: grad norm: 23.130 22.952 2.868
2024-12-02-07:03:25-root-INFO: grad norm: 22.656 22.476 2.849
2024-12-02-07:03:26-root-INFO: Loss Change: 199.126 -> 191.615
2024-12-02-07:03:26-root-INFO: Regularization Change: 0.000 -> 1.348
2024-12-02-07:03:26-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-07:03:26-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-07:03:26-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-07:03:26-root-INFO: grad norm: 26.161 25.882 3.813
2024-12-02-07:03:26-root-INFO: grad norm: 24.972 24.771 3.160
2024-12-02-07:03:27-root-INFO: grad norm: 23.793 23.595 3.061
2024-12-02-07:03:27-root-INFO: grad norm: 22.876 22.695 2.867
2024-12-02-07:03:28-root-INFO: grad norm: 21.999 21.825 2.755
2024-12-02-07:03:28-root-INFO: grad norm: 21.339 21.172 2.669
2024-12-02-07:03:29-root-INFO: grad norm: 20.733 20.574 2.566
2024-12-02-07:03:29-root-INFO: grad norm: 20.289 20.131 2.531
2024-12-02-07:03:30-root-INFO: Loss Change: 193.432 -> 186.686
2024-12-02-07:03:30-root-INFO: Regularization Change: 0.000 -> 1.258
2024-12-02-07:03:30-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-07:03:30-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-07:03:30-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-07:03:30-root-INFO: grad norm: 26.208 25.866 4.216
2024-12-02-07:03:31-root-INFO: grad norm: 25.087 24.878 3.230
2024-12-02-07:03:31-root-INFO: grad norm: 24.022 23.817 3.127
2024-12-02-07:03:31-root-INFO: grad norm: 23.203 23.018 2.931
2024-12-02-07:03:32-root-INFO: grad norm: 22.347 22.171 2.804
2024-12-02-07:03:32-root-INFO: grad norm: 21.735 21.563 2.729
2024-12-02-07:03:33-root-INFO: grad norm: 21.137 20.975 2.612
2024-12-02-07:03:33-root-INFO: grad norm: 20.732 20.570 2.584
2024-12-02-07:03:34-root-INFO: Loss Change: 189.825 -> 183.398
2024-12-02-07:03:34-root-INFO: Regularization Change: 0.000 -> 1.250
2024-12-02-07:03:34-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-07:03:34-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-07:03:34-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-07:03:34-root-INFO: grad norm: 25.451 25.145 3.933
2024-12-02-07:03:34-root-INFO: grad norm: 24.547 24.346 3.141
2024-12-02-07:03:35-root-INFO: grad norm: 23.625 23.417 3.129
2024-12-02-07:03:35-root-INFO: grad norm: 22.887 22.702 2.906
2024-12-02-07:03:36-root-INFO: grad norm: 22.123 21.939 2.849
2024-12-02-07:03:36-root-INFO: grad norm: 21.550 21.375 2.740
2024-12-02-07:03:37-root-INFO: grad norm: 20.989 20.818 2.674
2024-12-02-07:03:37-root-INFO: grad norm: 20.583 20.416 2.616
2024-12-02-07:03:38-root-INFO: Loss Change: 185.703 -> 179.775
2024-12-02-07:03:38-root-INFO: Regularization Change: 0.000 -> 1.198
2024-12-02-07:03:38-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-07:03:38-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-07:03:38-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-07:03:38-root-INFO: grad norm: 24.850 24.555 3.812
2024-12-02-07:03:38-root-INFO: grad norm: 23.855 23.660 3.048
2024-12-02-07:03:39-root-INFO: grad norm: 22.765 22.571 2.967
2024-12-02-07:03:39-root-INFO: grad norm: 21.927 21.753 2.761
2024-12-02-07:03:40-root-INFO: grad norm: 21.060 20.891 2.661
2024-12-02-07:03:40-root-INFO: grad norm: 20.433 20.272 2.565
2024-12-02-07:03:41-root-INFO: grad norm: 19.824 19.669 2.472
2024-12-02-07:03:41-root-INFO: grad norm: 19.397 19.244 2.426
2024-12-02-07:03:42-root-INFO: Loss Change: 182.358 -> 176.415
2024-12-02-07:03:42-root-INFO: Regularization Change: 0.000 -> 1.159
2024-12-02-07:03:42-root-INFO: Undo step: 105
2024-12-02-07:03:42-root-INFO: Undo step: 106
2024-12-02-07:03:42-root-INFO: Undo step: 107
2024-12-02-07:03:42-root-INFO: Undo step: 108
2024-12-02-07:03:42-root-INFO: Undo step: 109
2024-12-02-07:03:42-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-07:03:42-root-INFO: grad norm: 76.786 75.841 12.010
2024-12-02-07:03:42-root-INFO: grad norm: 42.364 41.616 7.925
2024-12-02-07:03:43-root-INFO: grad norm: 36.251 35.774 5.860
2024-12-02-07:03:43-root-INFO: grad norm: 31.643 31.179 5.397
2024-12-02-07:03:44-root-INFO: grad norm: 27.042 26.710 4.225
2024-12-02-07:03:44-root-INFO: grad norm: 24.445 24.124 3.947
2024-12-02-07:03:45-root-INFO: grad norm: 22.413 22.160 3.358
2024-12-02-07:03:45-root-INFO: grad norm: 21.278 21.039 3.183
2024-12-02-07:03:46-root-INFO: Loss Change: 423.597 -> 216.282
2024-12-02-07:03:46-root-INFO: Regularization Change: 0.000 -> 39.152
2024-12-02-07:03:46-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-07:03:46-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-07:03:46-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-07:03:46-root-INFO: grad norm: 21.173 20.967 2.945
2024-12-02-07:03:46-root-INFO: grad norm: 19.783 19.585 2.794
2024-12-02-07:03:47-root-INFO: grad norm: 18.833 18.672 2.460
2024-12-02-07:03:47-root-INFO: grad norm: 18.256 18.088 2.471
2024-12-02-07:03:48-root-INFO: grad norm: 17.861 17.712 2.306
2024-12-02-07:03:48-root-INFO: grad norm: 17.635 17.485 2.292
2024-12-02-07:03:49-root-INFO: grad norm: 17.524 17.381 2.237
2024-12-02-07:03:49-root-INFO: grad norm: 17.494 17.356 2.192
2024-12-02-07:03:50-root-INFO: Loss Change: 216.469 -> 199.083
2024-12-02-07:03:50-root-INFO: Regularization Change: 0.000 -> 5.213
2024-12-02-07:03:50-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-07:03:50-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-07:03:50-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-07:03:50-root-INFO: grad norm: 16.342 16.214 2.043
2024-12-02-07:03:50-root-INFO: grad norm: 15.624 15.498 1.982
2024-12-02-07:03:51-root-INFO: grad norm: 15.256 15.144 1.849
2024-12-02-07:03:51-root-INFO: grad norm: 15.101 14.986 1.861
2024-12-02-07:03:52-root-INFO: grad norm: 15.073 14.961 1.828
2024-12-02-07:03:52-root-INFO: grad norm: 15.121 15.011 1.815
2024-12-02-07:03:53-root-INFO: grad norm: 15.258 15.147 1.840
2024-12-02-07:03:53-root-INFO: grad norm: 15.412 15.306 1.806
2024-12-02-07:03:53-root-INFO: Loss Change: 198.606 -> 189.836
2024-12-02-07:03:53-root-INFO: Regularization Change: 0.000 -> 2.826
2024-12-02-07:03:53-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-07:03:53-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-07:03:54-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-07:03:54-root-INFO: grad norm: 16.498 16.354 2.172
2024-12-02-07:03:54-root-INFO: grad norm: 15.869 15.764 1.825
2024-12-02-07:03:55-root-INFO: grad norm: 15.817 15.727 1.689
2024-12-02-07:03:55-root-INFO: grad norm: 15.785 15.684 1.783
2024-12-02-07:03:56-root-INFO: grad norm: 15.764 15.674 1.680
2024-12-02-07:03:56-root-INFO: grad norm: 15.718 15.619 1.761
2024-12-02-07:03:57-root-INFO: grad norm: 15.652 15.563 1.667
2024-12-02-07:03:57-root-INFO: grad norm: 15.584 15.487 1.734
2024-12-02-07:03:57-root-INFO: Loss Change: 190.128 -> 183.729
2024-12-02-07:03:57-root-INFO: Regularization Change: 0.000 -> 2.059
2024-12-02-07:03:57-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-07:03:57-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-07:03:58-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-07:03:58-root-INFO: grad norm: 16.966 16.808 2.313
2024-12-02-07:03:58-root-INFO: grad norm: 16.355 16.241 1.929
2024-12-02-07:03:59-root-INFO: grad norm: 16.178 16.073 1.841
2024-12-02-07:03:59-root-INFO: grad norm: 16.196 16.083 1.906
2024-12-02-07:04:00-root-INFO: grad norm: 16.322 16.217 1.844
2024-12-02-07:04:00-root-INFO: grad norm: 16.534 16.416 1.976
2024-12-02-07:04:01-root-INFO: grad norm: 16.839 16.728 1.930
2024-12-02-07:04:01-root-INFO: grad norm: 17.192 17.064 2.094
2024-12-02-07:04:02-root-INFO: Loss Change: 184.114 -> 179.444
2024-12-02-07:04:02-root-INFO: Regularization Change: 0.000 -> 1.706
2024-12-02-07:04:02-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-07:04:02-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-07:04:02-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-07:04:02-root-INFO: grad norm: 21.733 21.511 3.102
2024-12-02-07:04:02-root-INFO: grad norm: 22.007 21.831 2.779
2024-12-02-07:04:03-root-INFO: grad norm: 22.240 22.066 2.778
2024-12-02-07:04:03-root-INFO: grad norm: 22.371 22.188 2.858
2024-12-02-07:04:04-root-INFO: grad norm: 22.359 22.185 2.786
2024-12-02-07:04:04-root-INFO: grad norm: 22.296 22.107 2.901
2024-12-02-07:04:05-root-INFO: grad norm: 22.134 21.959 2.777
2024-12-02-07:04:05-root-INFO: grad norm: 21.959 21.767 2.896
2024-12-02-07:04:06-root-INFO: Loss Change: 181.444 -> 177.607
2024-12-02-07:04:06-root-INFO: Regularization Change: 0.000 -> 1.509
2024-12-02-07:04:06-root-INFO: Undo step: 105
2024-12-02-07:04:06-root-INFO: Undo step: 106
2024-12-02-07:04:06-root-INFO: Undo step: 107
2024-12-02-07:04:06-root-INFO: Undo step: 108
2024-12-02-07:04:06-root-INFO: Undo step: 109
2024-12-02-07:04:06-root-INFO: step: 110 lr_xt 0.03878715
2024-12-02-07:04:06-root-INFO: grad norm: 75.024 73.890 12.993
2024-12-02-07:04:06-root-INFO: grad norm: 34.330 33.721 6.438
2024-12-02-07:04:07-root-INFO: grad norm: 25.205 24.666 5.184
2024-12-02-07:04:07-root-INFO: grad norm: 20.707 20.224 4.446
2024-12-02-07:04:08-root-INFO: grad norm: 17.936 17.509 3.891
2024-12-02-07:04:08-root-INFO: grad norm: 16.092 15.715 3.463
2024-12-02-07:04:09-root-INFO: grad norm: 14.851 14.507 3.177
2024-12-02-07:04:09-root-INFO: grad norm: 14.060 13.766 2.863
2024-12-02-07:04:10-root-INFO: Loss Change: 410.471 -> 210.963
2024-12-02-07:04:10-root-INFO: Regularization Change: 0.000 -> 37.283
2024-12-02-07:04:10-root-INFO: Learning rate of xt decay: 0.10498 -> 0.10624.
2024-12-02-07:04:10-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00040.
2024-12-02-07:04:10-root-INFO: step: 109 lr_xt 0.03995709
2024-12-02-07:04:10-root-INFO: grad norm: 13.266 12.986 2.713
2024-12-02-07:04:10-root-INFO: grad norm: 12.021 11.770 2.447
2024-12-02-07:04:11-root-INFO: grad norm: 11.389 11.166 2.239
2024-12-02-07:04:11-root-INFO: grad norm: 10.983 10.774 2.136
2024-12-02-07:04:12-root-INFO: grad norm: 10.724 10.533 2.014
2024-12-02-07:04:12-root-INFO: grad norm: 10.571 10.396 1.915
2024-12-02-07:04:13-root-INFO: grad norm: 10.556 10.391 1.861
2024-12-02-07:04:13-root-INFO: grad norm: 10.628 10.482 1.755
2024-12-02-07:04:14-root-INFO: Loss Change: 210.568 -> 193.510
2024-12-02-07:04:14-root-INFO: Regularization Change: 0.000 -> 5.378
2024-12-02-07:04:14-root-INFO: Learning rate of xt decay: 0.10624 -> 0.10752.
2024-12-02-07:04:14-root-INFO: Coefficient of regularization decay: 0.00040 -> 0.00041.
2024-12-02-07:04:14-root-INFO: step: 108 lr_xt 0.04115569
2024-12-02-07:04:14-root-INFO: grad norm: 10.677 10.521 1.818
2024-12-02-07:04:14-root-INFO: grad norm: 10.132 9.996 1.658
2024-12-02-07:04:15-root-INFO: grad norm: 9.908 9.788 1.539
2024-12-02-07:04:15-root-INFO: grad norm: 9.784 9.665 1.518
2024-12-02-07:04:16-root-INFO: grad norm: 9.700 9.593 1.433
2024-12-02-07:04:16-root-INFO: grad norm: 9.657 9.552 1.416
2024-12-02-07:04:17-root-INFO: grad norm: 9.647 9.551 1.351
2024-12-02-07:04:17-root-INFO: grad norm: 9.673 9.580 1.334
2024-12-02-07:04:18-root-INFO: Loss Change: 193.146 -> 184.626
2024-12-02-07:04:18-root-INFO: Regularization Change: 0.000 -> 2.785
2024-12-02-07:04:18-root-INFO: Learning rate of xt decay: 0.10752 -> 0.10881.
2024-12-02-07:04:18-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-07:04:18-root-INFO: step: 107 lr_xt 0.04238344
2024-12-02-07:04:18-root-INFO: grad norm: 14.228 14.038 2.316
2024-12-02-07:04:19-root-INFO: grad norm: 14.224 14.108 1.815
2024-12-02-07:04:19-root-INFO: grad norm: 14.995 14.888 1.784
2024-12-02-07:04:20-root-INFO: grad norm: 16.097 15.979 1.939
2024-12-02-07:04:20-root-INFO: grad norm: 17.644 17.526 2.036
2024-12-02-07:04:20-root-INFO: Loss too large (183.423->183.499)! Learning rate decreased to 0.03391.
2024-12-02-07:04:21-root-INFO: grad norm: 13.055 12.935 1.767
2024-12-02-07:04:21-root-INFO: grad norm: 10.461 10.372 1.359
2024-12-02-07:04:22-root-INFO: grad norm: 8.749 8.641 1.372
2024-12-02-07:04:22-root-INFO: Loss Change: 185.598 -> 178.655
2024-12-02-07:04:22-root-INFO: Regularization Change: 0.000 -> 1.699
2024-12-02-07:04:22-root-INFO: Learning rate of xt decay: 0.10881 -> 0.11011.
2024-12-02-07:04:22-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00041.
2024-12-02-07:04:22-root-INFO: step: 106 lr_xt 0.04364080
2024-12-02-07:04:22-root-INFO: grad norm: 13.146 12.945 2.289
2024-12-02-07:04:23-root-INFO: grad norm: 15.785 15.643 2.114
2024-12-02-07:04:23-root-INFO: Loss too large (179.454->180.111)! Learning rate decreased to 0.03491.
2024-12-02-07:04:24-root-INFO: grad norm: 13.027 12.916 1.695
2024-12-02-07:04:24-root-INFO: grad norm: 10.987 10.880 1.530
2024-12-02-07:04:24-root-INFO: grad norm: 9.397 9.309 1.283
2024-12-02-07:04:25-root-INFO: grad norm: 8.141 8.044 1.255
2024-12-02-07:04:25-root-INFO: grad norm: 7.153 7.071 1.078
2024-12-02-07:04:26-root-INFO: grad norm: 6.363 6.269 1.094
2024-12-02-07:04:26-root-INFO: Loss Change: 179.484 -> 174.051
2024-12-02-07:04:26-root-INFO: Regularization Change: 0.000 -> 1.199
2024-12-02-07:04:26-root-INFO: Learning rate of xt decay: 0.11011 -> 0.11144.
2024-12-02-07:04:26-root-INFO: Coefficient of regularization decay: 0.00041 -> 0.00042.
2024-12-02-07:04:26-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-07:04:27-root-INFO: grad norm: 10.869 10.692 1.949
2024-12-02-07:04:27-root-INFO: grad norm: 13.137 13.019 1.757
2024-12-02-07:04:27-root-INFO: Loss too large (174.902->175.392)! Learning rate decreased to 0.03594.
2024-12-02-07:04:28-root-INFO: grad norm: 11.190 11.096 1.442
2024-12-02-07:04:28-root-INFO: grad norm: 9.721 9.631 1.320
2024-12-02-07:04:29-root-INFO: grad norm: 8.542 8.465 1.143
2024-12-02-07:04:29-root-INFO: grad norm: 7.572 7.488 1.123
2024-12-02-07:04:30-root-INFO: grad norm: 6.787 6.715 0.990
2024-12-02-07:04:30-root-INFO: grad norm: 6.138 6.056 1.000
2024-12-02-07:04:30-root-INFO: Loss Change: 175.030 -> 170.660
2024-12-02-07:04:30-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-02-07:04:30-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-07:04:30-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-07:04:31-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-07:04:31-root-INFO: grad norm: 11.984 11.774 2.231
2024-12-02-07:04:31-root-INFO: Loss too large (171.686->171.716)! Learning rate decreased to 0.03700.
2024-12-02-07:04:31-root-INFO: grad norm: 9.774 9.686 1.311
2024-12-02-07:04:32-root-INFO: grad norm: 8.505 8.421 1.191
2024-12-02-07:04:32-root-INFO: grad norm: 7.566 7.492 1.055
2024-12-02-07:04:33-root-INFO: grad norm: 6.806 6.735 0.981
2024-12-02-07:04:33-root-INFO: grad norm: 6.173 6.101 0.941
2024-12-02-07:04:34-root-INFO: grad norm: 5.652 5.583 0.880
2024-12-02-07:04:34-root-INFO: grad norm: 5.214 5.141 0.868
2024-12-02-07:04:35-root-INFO: Loss Change: 171.686 -> 167.340
2024-12-02-07:04:35-root-INFO: Regularization Change: 0.000 -> 0.944
2024-12-02-07:04:35-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-07:04:35-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-07:04:35-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-07:04:35-root-INFO: grad norm: 9.740 9.598 1.658
2024-12-02-07:04:35-root-INFO: grad norm: 12.075 11.973 1.567
2024-12-02-07:04:36-root-INFO: Loss too large (168.090->168.641)! Learning rate decreased to 0.03808.
2024-12-02-07:04:36-root-INFO: grad norm: 10.444 10.358 1.331
2024-12-02-07:04:36-root-INFO: grad norm: 9.185 9.108 1.187
2024-12-02-07:04:37-root-INFO: grad norm: 8.175 8.106 1.060
2024-12-02-07:04:37-root-INFO: grad norm: 7.332 7.262 1.009
2024-12-02-07:04:38-root-INFO: grad norm: 6.647 6.583 0.918
2024-12-02-07:04:38-root-INFO: grad norm: 6.067 6.000 0.901
2024-12-02-07:04:39-root-INFO: Loss Change: 168.106 -> 164.504
2024-12-02-07:04:39-root-INFO: Regularization Change: 0.000 -> 0.931
2024-12-02-07:04:39-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-07:04:39-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-07:04:39-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-07:04:39-root-INFO: grad norm: 9.963 9.805 1.768
2024-12-02-07:04:39-root-INFO: Loss too large (165.017->165.101)! Learning rate decreased to 0.03918.
2024-12-02-07:04:40-root-INFO: grad norm: 8.663 8.592 1.109
2024-12-02-07:04:40-root-INFO: grad norm: 7.898 7.823 1.085
2024-12-02-07:04:41-root-INFO: grad norm: 7.295 7.233 0.952
2024-12-02-07:04:41-root-INFO: grad norm: 6.782 6.718 0.926
2024-12-02-07:04:42-root-INFO: grad norm: 6.327 6.267 0.870
2024-12-02-07:04:42-root-INFO: grad norm: 5.935 5.875 0.843
2024-12-02-07:04:43-root-INFO: grad norm: 5.586 5.526 0.814
2024-12-02-07:04:43-root-INFO: Loss Change: 165.017 -> 161.538
2024-12-02-07:04:43-root-INFO: Regularization Change: 0.000 -> 0.860
2024-12-02-07:04:43-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-07:04:43-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-07:04:43-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-07:04:43-root-INFO: grad norm: 9.674 9.533 1.646
2024-12-02-07:04:43-root-INFO: Loss too large (162.260->162.440)! Learning rate decreased to 0.04031.
2024-12-02-07:04:44-root-INFO: grad norm: 8.551 8.482 1.086
2024-12-02-07:04:44-root-INFO: grad norm: 7.848 7.775 1.062
2024-12-02-07:04:45-root-INFO: grad norm: 7.293 7.233 0.934
2024-12-02-07:04:45-root-INFO: grad norm: 6.818 6.757 0.913
2024-12-02-07:04:46-root-INFO: grad norm: 6.393 6.336 0.854
2024-12-02-07:04:46-root-INFO: grad norm: 6.026 5.968 0.834
2024-12-02-07:04:47-root-INFO: grad norm: 5.695 5.639 0.799
2024-12-02-07:04:47-root-INFO: Loss Change: 162.260 -> 159.007
2024-12-02-07:04:47-root-INFO: Regularization Change: 0.000 -> 0.832
2024-12-02-07:04:47-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-07:04:47-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-07:04:47-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-07:04:47-root-INFO: grad norm: 10.007 9.859 1.719
2024-12-02-07:04:48-root-INFO: Loss too large (159.901->160.158)! Learning rate decreased to 0.04147.
2024-12-02-07:04:48-root-INFO: grad norm: 8.769 8.705 1.063
2024-12-02-07:04:49-root-INFO: grad norm: 7.978 7.904 1.080
2024-12-02-07:04:49-root-INFO: grad norm: 7.349 7.293 0.906
2024-12-02-07:04:50-root-INFO: grad norm: 6.818 6.756 0.910
2024-12-02-07:04:50-root-INFO: grad norm: 6.343 6.290 0.824
2024-12-02-07:04:50-root-INFO: grad norm: 5.940 5.883 0.820
2024-12-02-07:04:51-root-INFO: grad norm: 5.575 5.522 0.767
2024-12-02-07:04:51-root-INFO: Loss Change: 159.901 -> 156.638
2024-12-02-07:04:51-root-INFO: Regularization Change: 0.000 -> 0.825
2024-12-02-07:04:51-root-INFO: Undo step: 100
2024-12-02-07:04:51-root-INFO: Undo step: 101
2024-12-02-07:04:51-root-INFO: Undo step: 102
2024-12-02-07:04:51-root-INFO: Undo step: 103
2024-12-02-07:04:51-root-INFO: Undo step: 104
2024-12-02-07:04:51-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-07:04:52-root-INFO: grad norm: 64.600 63.930 9.281
2024-12-02-07:04:52-root-INFO: grad norm: 35.987 35.377 6.597
2024-12-02-07:04:52-root-INFO: grad norm: 24.196 23.615 5.270
2024-12-02-07:04:53-root-INFO: grad norm: 18.818 18.322 4.292
2024-12-02-07:04:53-root-INFO: grad norm: 16.065 15.597 3.850
2024-12-02-07:04:54-root-INFO: grad norm: 14.767 14.386 3.330
2024-12-02-07:04:54-root-INFO: grad norm: 14.422 14.063 3.197
2024-12-02-07:04:55-root-INFO: grad norm: 14.652 14.380 2.812
2024-12-02-07:04:55-root-INFO: Loss Change: 412.689 -> 192.555
2024-12-02-07:04:55-root-INFO: Regularization Change: 0.000 -> 48.593
2024-12-02-07:04:55-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-07:04:55-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-07:04:55-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-07:04:55-root-INFO: grad norm: 12.612 12.369 2.461
2024-12-02-07:04:56-root-INFO: grad norm: 12.488 12.281 2.266
2024-12-02-07:04:56-root-INFO: grad norm: 13.021 12.807 2.351
2024-12-02-07:04:57-root-INFO: grad norm: 13.757 13.587 2.155
2024-12-02-07:04:57-root-INFO: grad norm: 14.677 14.485 2.371
2024-12-02-07:04:58-root-INFO: grad norm: 15.686 15.534 2.172
2024-12-02-07:04:58-root-INFO: grad norm: 16.783 16.598 2.482
2024-12-02-07:04:59-root-INFO: grad norm: 17.927 17.780 2.290
2024-12-02-07:04:59-root-INFO: Loss Change: 191.323 -> 179.098
2024-12-02-07:04:59-root-INFO: Regularization Change: 0.000 -> 5.542
2024-12-02-07:04:59-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-07:04:59-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-07:04:59-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-07:04:59-root-INFO: grad norm: 15.757 15.615 2.109
2024-12-02-07:05:00-root-INFO: grad norm: 16.703 16.570 2.102
2024-12-02-07:05:00-root-INFO: grad norm: 17.815 17.649 2.431
2024-12-02-07:05:01-root-INFO: grad norm: 19.027 18.887 2.303
2024-12-02-07:05:01-root-INFO: grad norm: 20.031 19.854 2.653
2024-12-02-07:05:02-root-INFO: grad norm: 20.940 20.791 2.493
2024-12-02-07:05:02-root-INFO: grad norm: 21.504 21.319 2.818
2024-12-02-07:05:03-root-INFO: grad norm: 21.930 21.774 2.609
2024-12-02-07:05:03-root-INFO: Loss Change: 177.678 -> 173.483
2024-12-02-07:05:03-root-INFO: Regularization Change: 0.000 -> 2.947
2024-12-02-07:05:03-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-07:05:03-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-07:05:03-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-07:05:03-root-INFO: grad norm: 19.869 19.728 2.368
2024-12-02-07:05:04-root-INFO: grad norm: 20.457 20.312 2.428
2024-12-02-07:05:04-root-INFO: grad norm: 20.926 20.758 2.646
2024-12-02-07:05:05-root-INFO: grad norm: 21.350 21.199 2.532
2024-12-02-07:05:05-root-INFO: grad norm: 21.545 21.367 2.767
2024-12-02-07:05:06-root-INFO: grad norm: 21.651 21.496 2.582
2024-12-02-07:05:06-root-INFO: grad norm: 21.617 21.434 2.804
2024-12-02-07:05:07-root-INFO: grad norm: 21.515 21.360 2.580
2024-12-02-07:05:07-root-INFO: Loss Change: 171.951 -> 167.777
2024-12-02-07:05:07-root-INFO: Regularization Change: 0.000 -> 2.091
2024-12-02-07:05:07-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-07:05:07-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-07:05:07-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-07:05:07-root-INFO: grad norm: 18.698 18.564 2.237
2024-12-02-07:05:08-root-INFO: grad norm: 18.756 18.625 2.210
2024-12-02-07:05:08-root-INFO: grad norm: 18.874 18.721 2.401
2024-12-02-07:05:09-root-INFO: grad norm: 19.019 18.885 2.252
2024-12-02-07:05:09-root-INFO: grad norm: 19.110 18.952 2.446
2024-12-02-07:05:10-root-INFO: grad norm: 19.176 19.040 2.279
2024-12-02-07:05:10-root-INFO: grad norm: 19.194 19.036 2.463
2024-12-02-07:05:11-root-INFO: grad norm: 19.183 19.046 2.289
2024-12-02-07:05:11-root-INFO: Loss Change: 166.364 -> 162.532
2024-12-02-07:05:11-root-INFO: Regularization Change: 0.000 -> 1.673
2024-12-02-07:05:11-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-07:05:11-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-07:05:11-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-07:05:11-root-INFO: grad norm: 16.201 16.098 1.819
2024-12-02-07:05:12-root-INFO: grad norm: 16.208 16.095 1.909
2024-12-02-07:05:12-root-INFO: grad norm: 16.345 16.221 2.011
2024-12-02-07:05:13-root-INFO: grad norm: 16.542 16.426 1.958
2024-12-02-07:05:13-root-INFO: grad norm: 16.724 16.593 2.093
2024-12-02-07:05:14-root-INFO: grad norm: 16.903 16.784 2.008
2024-12-02-07:05:14-root-INFO: grad norm: 17.049 16.913 2.150
2024-12-02-07:05:15-root-INFO: grad norm: 17.176 17.054 2.050
2024-12-02-07:05:15-root-INFO: Loss Change: 161.126 -> 158.128
2024-12-02-07:05:15-root-INFO: Regularization Change: 0.000 -> 1.460
2024-12-02-07:05:15-root-INFO: Undo step: 100
2024-12-02-07:05:15-root-INFO: Undo step: 101
2024-12-02-07:05:15-root-INFO: Undo step: 102
2024-12-02-07:05:15-root-INFO: Undo step: 103
2024-12-02-07:05:15-root-INFO: Undo step: 104
2024-12-02-07:05:15-root-INFO: step: 105 lr_xt 0.04492824
2024-12-02-07:05:16-root-INFO: grad norm: 69.994 69.167 10.727
2024-12-02-07:05:16-root-INFO: grad norm: 38.532 37.879 7.066
2024-12-02-07:05:17-root-INFO: grad norm: 26.641 25.994 5.836
2024-12-02-07:05:17-root-INFO: grad norm: 21.355 20.861 4.564
2024-12-02-07:05:18-root-INFO: grad norm: 18.185 17.714 4.114
2024-12-02-07:05:18-root-INFO: grad norm: 16.313 15.971 3.323
2024-12-02-07:05:19-root-INFO: grad norm: 15.034 14.678 3.251
2024-12-02-07:05:19-root-INFO: grad norm: 14.199 13.949 2.650
2024-12-02-07:05:19-root-INFO: Loss Change: 414.693 -> 193.757
2024-12-02-07:05:19-root-INFO: Regularization Change: 0.000 -> 47.374
2024-12-02-07:05:19-root-INFO: Learning rate of xt decay: 0.11144 -> 0.11277.
2024-12-02-07:05:19-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00042.
2024-12-02-07:05:20-root-INFO: step: 104 lr_xt 0.04624623
2024-12-02-07:05:20-root-INFO: grad norm: 10.925 10.696 2.225
2024-12-02-07:05:20-root-INFO: grad norm: 9.910 9.716 1.950
2024-12-02-07:05:21-root-INFO: grad norm: 9.399 9.189 1.973
2024-12-02-07:05:21-root-INFO: grad norm: 9.091 8.930 1.702
2024-12-02-07:05:22-root-INFO: grad norm: 8.873 8.688 1.801
2024-12-02-07:05:22-root-INFO: grad norm: 8.739 8.603 1.532
2024-12-02-07:05:22-root-INFO: grad norm: 8.659 8.495 1.674
2024-12-02-07:05:23-root-INFO: grad norm: 8.632 8.514 1.422
2024-12-02-07:05:23-root-INFO: Loss Change: 192.441 -> 176.263
2024-12-02-07:05:23-root-INFO: Regularization Change: 0.000 -> 5.909
2024-12-02-07:05:23-root-INFO: Learning rate of xt decay: 0.11277 -> 0.11413.
2024-12-02-07:05:23-root-INFO: Coefficient of regularization decay: 0.00042 -> 0.00043.
2024-12-02-07:05:23-root-INFO: step: 103 lr_xt 0.04759523
2024-12-02-07:05:24-root-INFO: grad norm: 6.633 6.502 1.309
2024-12-02-07:05:24-root-INFO: grad norm: 6.012 5.894 1.185
2024-12-02-07:05:25-root-INFO: grad norm: 5.813 5.690 1.192
2024-12-02-07:05:25-root-INFO: grad norm: 5.730 5.625 1.092
2024-12-02-07:05:26-root-INFO: grad norm: 5.702 5.585 1.149
2024-12-02-07:05:26-root-INFO: grad norm: 5.722 5.628 1.038
2024-12-02-07:05:27-root-INFO: grad norm: 5.783 5.673 1.123
2024-12-02-07:05:27-root-INFO: grad norm: 5.886 5.799 1.010
2024-12-02-07:05:27-root-INFO: Loss Change: 175.729 -> 167.830
2024-12-02-07:05:27-root-INFO: Regularization Change: 0.000 -> 3.015
2024-12-02-07:05:27-root-INFO: Learning rate of xt decay: 0.11413 -> 0.11550.
2024-12-02-07:05:27-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00043.
2024-12-02-07:05:28-root-INFO: step: 102 lr_xt 0.04897571
2024-12-02-07:05:28-root-INFO: grad norm: 5.532 5.399 1.203
2024-12-02-07:05:28-root-INFO: grad norm: 4.710 4.617 0.935
2024-12-02-07:05:29-root-INFO: grad norm: 4.484 4.392 0.904
2024-12-02-07:05:29-root-INFO: grad norm: 4.405 4.317 0.873
2024-12-02-07:05:30-root-INFO: grad norm: 4.375 4.288 0.864
2024-12-02-07:05:30-root-INFO: grad norm: 4.398 4.317 0.840
2024-12-02-07:05:30-root-INFO: grad norm: 4.465 4.383 0.851
2024-12-02-07:05:31-root-INFO: grad norm: 4.583 4.509 0.824
2024-12-02-07:05:31-root-INFO: Loss Change: 167.452 -> 161.830
2024-12-02-07:05:31-root-INFO: Regularization Change: 0.000 -> 2.175
2024-12-02-07:05:31-root-INFO: Learning rate of xt decay: 0.11550 -> 0.11688.
2024-12-02-07:05:31-root-INFO: Coefficient of regularization decay: 0.00043 -> 0.00044.
2024-12-02-07:05:31-root-INFO: step: 101 lr_xt 0.05038813
2024-12-02-07:05:32-root-INFO: grad norm: 5.569 5.436 1.207
2024-12-02-07:05:32-root-INFO: grad norm: 4.732 4.639 0.932
2024-12-02-07:05:32-root-INFO: grad norm: 4.188 4.101 0.848
2024-12-02-07:05:33-root-INFO: grad norm: 4.045 3.953 0.858
2024-12-02-07:05:33-root-INFO: grad norm: 3.973 3.894 0.789
2024-12-02-07:05:34-root-INFO: grad norm: 3.908 3.818 0.831
2024-12-02-07:05:34-root-INFO: grad norm: 3.884 3.808 0.764
2024-12-02-07:05:35-root-INFO: grad norm: 3.855 3.767 0.816
2024-12-02-07:05:35-root-INFO: Loss Change: 161.904 -> 157.302
2024-12-02-07:05:35-root-INFO: Regularization Change: 0.000 -> 1.784
2024-12-02-07:05:35-root-INFO: Learning rate of xt decay: 0.11688 -> 0.11828.
2024-12-02-07:05:35-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-07:05:35-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-07:05:35-root-INFO: grad norm: 7.865 7.700 1.602
2024-12-02-07:05:36-root-INFO: grad norm: 7.527 7.421 1.253
2024-12-02-07:05:36-root-INFO: grad norm: 7.626 7.528 1.217
2024-12-02-07:05:37-root-INFO: grad norm: 8.456 8.361 1.266
2024-12-02-07:05:37-root-INFO: grad norm: 9.529 9.439 1.307
2024-12-02-07:05:38-root-INFO: grad norm: 10.721 10.623 1.451
2024-12-02-07:05:38-root-INFO: grad norm: 12.020 11.922 1.537
2024-12-02-07:05:38-root-INFO: Loss too large (155.933->155.958)! Learning rate decreased to 0.04147.
2024-12-02-07:05:39-root-INFO: grad norm: 8.735 8.648 1.227
2024-12-02-07:05:39-root-INFO: Loss Change: 157.804 -> 153.993
2024-12-02-07:05:39-root-INFO: Regularization Change: 0.000 -> 1.434
2024-12-02-07:05:39-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-07:05:39-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-07:05:39-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-07:05:39-root-INFO: grad norm: 10.389 10.228 1.819
2024-12-02-07:05:40-root-INFO: grad norm: 11.162 11.054 1.550
2024-12-02-07:05:40-root-INFO: grad norm: 12.257 12.147 1.639
2024-12-02-07:05:40-root-INFO: Loss too large (154.203->154.245)! Learning rate decreased to 0.04265.
2024-12-02-07:05:41-root-INFO: grad norm: 8.816 8.728 1.239
2024-12-02-07:05:41-root-INFO: grad norm: 6.517 6.453 0.913
2024-12-02-07:05:42-root-INFO: grad norm: 5.107 5.040 0.828
2024-12-02-07:05:42-root-INFO: grad norm: 4.202 4.144 0.697
2024-12-02-07:05:43-root-INFO: grad norm: 3.635 3.568 0.694
2024-12-02-07:05:43-root-INFO: Loss Change: 154.630 -> 150.466
2024-12-02-07:05:43-root-INFO: Regularization Change: 0.000 -> 1.109
2024-12-02-07:05:43-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-07:05:43-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-07:05:43-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-07:05:43-root-INFO: grad norm: 6.992 6.854 1.382
2024-12-02-07:05:44-root-INFO: grad norm: 7.388 7.301 1.131
2024-12-02-07:05:44-root-INFO: grad norm: 8.491 8.404 1.212
2024-12-02-07:05:44-root-INFO: Loss too large (150.272->150.273)! Learning rate decreased to 0.04386.
2024-12-02-07:05:45-root-INFO: grad norm: 6.641 6.566 0.990
2024-12-02-07:05:45-root-INFO: grad norm: 5.157 5.098 0.775
2024-12-02-07:05:46-root-INFO: grad norm: 4.351 4.289 0.731
2024-12-02-07:05:46-root-INFO: grad norm: 3.805 3.750 0.646
2024-12-02-07:05:47-root-INFO: grad norm: 3.440 3.378 0.651
2024-12-02-07:05:47-root-INFO: Loss Change: 150.931 -> 147.686
2024-12-02-07:05:47-root-INFO: Regularization Change: 0.000 -> 1.037
2024-12-02-07:05:47-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-07:05:47-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-07:05:47-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-07:05:47-root-INFO: grad norm: 7.997 7.830 1.625
2024-12-02-07:05:48-root-INFO: grad norm: 8.207 8.116 1.221
2024-12-02-07:05:48-root-INFO: grad norm: 8.949 8.859 1.266
2024-12-02-07:05:49-root-INFO: Loss too large (147.633->147.665)! Learning rate decreased to 0.04509.
2024-12-02-07:05:49-root-INFO: grad norm: 7.050 6.981 0.987
2024-12-02-07:05:49-root-INFO: grad norm: 5.742 5.686 0.802
2024-12-02-07:05:50-root-INFO: grad norm: 4.818 4.760 0.742
2024-12-02-07:05:50-root-INFO: grad norm: 4.172 4.121 0.650
2024-12-02-07:05:51-root-INFO: grad norm: 3.711 3.654 0.645
2024-12-02-07:05:51-root-INFO: Loss Change: 148.250 -> 145.000
2024-12-02-07:05:51-root-INFO: Regularization Change: 0.000 -> 1.015
2024-12-02-07:05:51-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-07:05:51-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-07:05:51-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-07:05:52-root-INFO: grad norm: 6.519 6.398 1.252
2024-12-02-07:05:52-root-INFO: grad norm: 7.129 7.052 1.044
2024-12-02-07:05:53-root-INFO: grad norm: 8.419 8.337 1.169
2024-12-02-07:05:53-root-INFO: Loss too large (144.987->145.139)! Learning rate decreased to 0.04636.
2024-12-02-07:05:53-root-INFO: grad norm: 6.789 6.724 0.944
2024-12-02-07:05:54-root-INFO: grad norm: 5.376 5.322 0.764
2024-12-02-07:05:54-root-INFO: grad norm: 4.601 4.548 0.698
2024-12-02-07:05:55-root-INFO: grad norm: 4.061 4.012 0.630
2024-12-02-07:05:55-root-INFO: grad norm: 3.665 3.612 0.622
2024-12-02-07:05:55-root-INFO: Loss Change: 145.437 -> 142.597
2024-12-02-07:05:55-root-INFO: Regularization Change: 0.000 -> 0.970
2024-12-02-07:05:55-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-07:05:55-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-07:05:56-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-07:05:56-root-INFO: grad norm: 7.825 7.695 1.420
2024-12-02-07:05:56-root-INFO: grad norm: 8.325 8.238 1.200
2024-12-02-07:05:57-root-INFO: grad norm: 9.286 9.201 1.258
2024-12-02-07:05:57-root-INFO: Loss too large (143.042->143.231)! Learning rate decreased to 0.04765.
2024-12-02-07:05:57-root-INFO: grad norm: 7.474 7.411 0.970
2024-12-02-07:05:58-root-INFO: grad norm: 6.221 6.167 0.811
2024-12-02-07:05:58-root-INFO: grad norm: 5.281 5.229 0.736
2024-12-02-07:05:59-root-INFO: grad norm: 4.623 4.577 0.654
2024-12-02-07:05:59-root-INFO: grad norm: 4.117 4.068 0.637
2024-12-02-07:05:59-root-INFO: Loss Change: 143.392 -> 140.467
2024-12-02-07:05:59-root-INFO: Regularization Change: 0.000 -> 0.964
2024-12-02-07:05:59-root-INFO: Undo step: 95
2024-12-02-07:05:59-root-INFO: Undo step: 96
2024-12-02-07:05:59-root-INFO: Undo step: 97
2024-12-02-07:05:59-root-INFO: Undo step: 98
2024-12-02-07:05:59-root-INFO: Undo step: 99
2024-12-02-07:06:00-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-07:06:00-root-INFO: grad norm: 58.107 57.125 10.637
2024-12-02-07:06:00-root-INFO: grad norm: 28.291 27.707 5.718
2024-12-02-07:06:01-root-INFO: grad norm: 19.443 18.897 4.572
2024-12-02-07:06:01-root-INFO: grad norm: 18.693 18.374 3.440
2024-12-02-07:06:02-root-INFO: grad norm: 13.110 12.721 3.172
2024-12-02-07:06:02-root-INFO: grad norm: 11.218 10.927 2.538
2024-12-02-07:06:03-root-INFO: grad norm: 10.096 9.815 2.362
2024-12-02-07:06:03-root-INFO: grad norm: 8.854 8.630 1.978
2024-12-02-07:06:03-root-INFO: Loss Change: 361.396 -> 173.398
2024-12-02-07:06:03-root-INFO: Regularization Change: 0.000 -> 46.466
2024-12-02-07:06:03-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-07:06:03-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-07:06:04-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-07:06:04-root-INFO: grad norm: 8.324 8.123 1.819
2024-12-02-07:06:04-root-INFO: grad norm: 9.469 9.326 1.639
2024-12-02-07:06:05-root-INFO: grad norm: 7.461 7.293 1.574
2024-12-02-07:06:05-root-INFO: grad norm: 6.573 6.418 1.421
2024-12-02-07:06:06-root-INFO: grad norm: 6.980 6.857 1.305
2024-12-02-07:06:06-root-INFO: grad norm: 6.767 6.631 1.349
2024-12-02-07:06:07-root-INFO: grad norm: 6.308 6.191 1.210
2024-12-02-07:06:07-root-INFO: grad norm: 6.873 6.753 1.278
2024-12-02-07:06:07-root-INFO: Loss Change: 172.801 -> 159.216
2024-12-02-07:06:07-root-INFO: Regularization Change: 0.000 -> 5.889
2024-12-02-07:06:07-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-07:06:07-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-07:06:08-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-07:06:08-root-INFO: grad norm: 11.786 11.633 1.892
2024-12-02-07:06:08-root-INFO: grad norm: 10.427 10.298 1.637
2024-12-02-07:06:09-root-INFO: grad norm: 11.150 11.041 1.557
2024-12-02-07:06:09-root-INFO: grad norm: 11.845 11.732 1.633
2024-12-02-07:06:10-root-INFO: grad norm: 12.946 12.839 1.661
2024-12-02-07:06:10-root-INFO: grad norm: 13.814 13.702 1.755
2024-12-02-07:06:11-root-INFO: grad norm: 14.108 14.003 1.721
2024-12-02-07:06:11-root-INFO: grad norm: 13.979 13.871 1.735
2024-12-02-07:06:11-root-INFO: Loss Change: 159.754 -> 154.357
2024-12-02-07:06:11-root-INFO: Regularization Change: 0.000 -> 3.117
2024-12-02-07:06:11-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-07:06:11-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-07:06:11-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-07:06:12-root-INFO: grad norm: 15.680 15.514 2.277
2024-12-02-07:06:12-root-INFO: grad norm: 15.188 15.058 1.984
2024-12-02-07:06:13-root-INFO: grad norm: 14.915 14.794 1.894
2024-12-02-07:06:13-root-INFO: grad norm: 14.864 14.739 1.927
2024-12-02-07:06:14-root-INFO: grad norm: 14.880 14.760 1.887
2024-12-02-07:06:14-root-INFO: grad norm: 14.916 14.786 1.964
2024-12-02-07:06:14-root-INFO: grad norm: 15.138 15.010 1.965
2024-12-02-07:06:15-root-INFO: grad norm: 15.361 15.224 2.051
2024-12-02-07:06:15-root-INFO: Loss Change: 155.045 -> 150.234
2024-12-02-07:06:15-root-INFO: Regularization Change: 0.000 -> 2.203
2024-12-02-07:06:15-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-07:06:15-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-07:06:15-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-07:06:16-root-INFO: grad norm: 18.551 18.323 2.899
2024-12-02-07:06:16-root-INFO: grad norm: 18.433 18.248 2.603
2024-12-02-07:06:17-root-INFO: grad norm: 18.233 18.040 2.644
2024-12-02-07:06:17-root-INFO: grad norm: 18.008 17.824 2.570
2024-12-02-07:06:18-root-INFO: grad norm: 17.682 17.494 2.567
2024-12-02-07:06:18-root-INFO: grad norm: 17.388 17.205 2.517
2024-12-02-07:06:19-root-INFO: grad norm: 17.062 16.878 2.495
2024-12-02-07:06:19-root-INFO: grad norm: 16.797 16.617 2.452
2024-12-02-07:06:19-root-INFO: Loss Change: 151.693 -> 147.036
2024-12-02-07:06:19-root-INFO: Regularization Change: 0.000 -> 1.827
2024-12-02-07:06:19-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-07:06:19-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-07:06:20-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-07:06:20-root-INFO: grad norm: 20.454 20.162 3.444
2024-12-02-07:06:20-root-INFO: grad norm: 19.556 19.331 2.956
2024-12-02-07:06:21-root-INFO: grad norm: 18.514 18.291 2.868
2024-12-02-07:06:21-root-INFO: grad norm: 17.764 17.559 2.687
2024-12-02-07:06:22-root-INFO: grad norm: 17.054 16.851 2.619
2024-12-02-07:06:22-root-INFO: grad norm: 16.622 16.431 2.515
2024-12-02-07:06:23-root-INFO: grad norm: 16.333 16.140 2.508
2024-12-02-07:06:23-root-INFO: grad norm: 16.296 16.109 2.459
2024-12-02-07:06:23-root-INFO: Loss Change: 149.428 -> 143.942
2024-12-02-07:06:23-root-INFO: Regularization Change: 0.000 -> 1.678
2024-12-02-07:06:23-root-INFO: Undo step: 95
2024-12-02-07:06:23-root-INFO: Undo step: 96
2024-12-02-07:06:23-root-INFO: Undo step: 97
2024-12-02-07:06:23-root-INFO: Undo step: 98
2024-12-02-07:06:23-root-INFO: Undo step: 99
2024-12-02-07:06:24-root-INFO: step: 100 lr_xt 0.05183295
2024-12-02-07:06:24-root-INFO: grad norm: 69.527 69.115 7.556
2024-12-02-07:06:24-root-INFO: grad norm: 36.260 35.763 5.984
2024-12-02-07:06:25-root-INFO: grad norm: 24.516 24.197 3.940
2024-12-02-07:06:25-root-INFO: grad norm: 19.487 19.203 3.315
2024-12-02-07:06:26-root-INFO: grad norm: 16.948 16.692 2.939
2024-12-02-07:06:26-root-INFO: grad norm: 15.253 15.036 2.563
2024-12-02-07:06:27-root-INFO: grad norm: 14.268 14.052 2.475
2024-12-02-07:06:27-root-INFO: grad norm: 13.646 13.464 2.220
2024-12-02-07:06:27-root-INFO: Loss Change: 368.340 -> 172.136
2024-12-02-07:06:27-root-INFO: Regularization Change: 0.000 -> 44.920
2024-12-02-07:06:27-root-INFO: Learning rate of xt decay: 0.11828 -> 0.11970.
2024-12-02-07:06:27-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00044.
2024-12-02-07:06:28-root-INFO: step: 99 lr_xt 0.05331064
2024-12-02-07:06:28-root-INFO: grad norm: 11.326 11.175 1.840
2024-12-02-07:06:28-root-INFO: grad norm: 10.820 10.681 1.730
2024-12-02-07:06:29-root-INFO: grad norm: 10.589 10.441 1.765
2024-12-02-07:06:29-root-INFO: grad norm: 10.762 10.631 1.675
2024-12-02-07:06:30-root-INFO: grad norm: 10.531 10.385 1.748
2024-12-02-07:06:30-root-INFO: grad norm: 10.137 10.012 1.587
2024-12-02-07:06:31-root-INFO: grad norm: 10.217 10.081 1.666
2024-12-02-07:06:31-root-INFO: grad norm: 10.710 10.593 1.580
2024-12-02-07:06:31-root-INFO: Loss Change: 171.093 -> 157.780
2024-12-02-07:06:31-root-INFO: Regularization Change: 0.000 -> 5.789
2024-12-02-07:06:31-root-INFO: Learning rate of xt decay: 0.11970 -> 0.12114.
2024-12-02-07:06:31-root-INFO: Coefficient of regularization decay: 0.00044 -> 0.00045.
2024-12-02-07:06:32-root-INFO: step: 98 lr_xt 0.05482165
2024-12-02-07:06:32-root-INFO: grad norm: 8.835 8.749 1.229
2024-12-02-07:06:32-root-INFO: grad norm: 8.481 8.388 1.250
2024-12-02-07:06:33-root-INFO: grad norm: 8.721 8.626 1.285
2024-12-02-07:06:33-root-INFO: grad norm: 9.375 9.282 1.313
2024-12-02-07:06:34-root-INFO: grad norm: 9.498 9.393 1.409
2024-12-02-07:06:34-root-INFO: grad norm: 9.430 9.335 1.338
2024-12-02-07:06:34-root-INFO: grad norm: 9.928 9.824 1.434
2024-12-02-07:06:35-root-INFO: grad norm: 10.926 10.827 1.468
2024-12-02-07:06:35-root-INFO: Loss Change: 157.172 -> 150.864
2024-12-02-07:06:35-root-INFO: Regularization Change: 0.000 -> 3.134
2024-12-02-07:06:35-root-INFO: Learning rate of xt decay: 0.12114 -> 0.12259.
2024-12-02-07:06:35-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00045.
2024-12-02-07:06:36-root-INFO: step: 97 lr_xt 0.05636643
2024-12-02-07:06:36-root-INFO: grad norm: 8.920 8.848 1.128
2024-12-02-07:06:36-root-INFO: grad norm: 8.659 8.589 1.102
2024-12-02-07:06:37-root-INFO: grad norm: 9.145 9.074 1.132
2024-12-02-07:06:37-root-INFO: grad norm: 10.138 10.060 1.256
2024-12-02-07:06:38-root-INFO: grad norm: 10.504 10.416 1.363
2024-12-02-07:06:38-root-INFO: grad norm: 10.714 10.626 1.366
2024-12-02-07:06:39-root-INFO: grad norm: 11.375 11.281 1.461
2024-12-02-07:06:39-root-INFO: grad norm: 12.440 12.340 1.574
2024-12-02-07:06:40-root-INFO: Loss Change: 149.981 -> 146.328
2024-12-02-07:06:40-root-INFO: Regularization Change: 0.000 -> 2.272
2024-12-02-07:06:40-root-INFO: Learning rate of xt decay: 0.12259 -> 0.12406.
2024-12-02-07:06:40-root-INFO: Coefficient of regularization decay: 0.00045 -> 0.00046.
2024-12-02-07:06:40-root-INFO: step: 96 lr_xt 0.05794543
2024-12-02-07:06:40-root-INFO: grad norm: 10.827 10.768 1.128
2024-12-02-07:06:40-root-INFO: grad norm: 10.805 10.722 1.336
2024-12-02-07:06:41-root-INFO: grad norm: 11.315 11.234 1.351
2024-12-02-07:06:41-root-INFO: grad norm: 12.306 12.210 1.540
2024-12-02-07:06:42-root-INFO: Loss too large (144.172->144.190)! Learning rate decreased to 0.04636.
2024-12-02-07:06:42-root-INFO: grad norm: 8.464 8.381 1.178
2024-12-02-07:06:43-root-INFO: grad norm: 5.721 5.656 0.857
2024-12-02-07:06:43-root-INFO: grad norm: 4.442 4.378 0.751
2024-12-02-07:06:44-root-INFO: grad norm: 3.758 3.698 0.666
2024-12-02-07:06:44-root-INFO: Loss Change: 145.606 -> 140.443
2024-12-02-07:06:44-root-INFO: Regularization Change: 0.000 -> 1.481
2024-12-02-07:06:44-root-INFO: Learning rate of xt decay: 0.12406 -> 0.12555.
2024-12-02-07:06:44-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00046.
2024-12-02-07:06:44-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-07:06:44-root-INFO: grad norm: 4.647 4.507 1.132
2024-12-02-07:06:45-root-INFO: grad norm: 4.073 3.992 0.804
2024-12-02-07:06:45-root-INFO: grad norm: 4.309 4.235 0.795
2024-12-02-07:06:46-root-INFO: grad norm: 4.709 4.634 0.834
2024-12-02-07:06:46-root-INFO: grad norm: 6.190 6.126 0.889
2024-12-02-07:06:47-root-INFO: grad norm: 5.857 5.778 0.957
2024-12-02-07:06:47-root-INFO: grad norm: 5.265 5.199 0.835
2024-12-02-07:06:48-root-INFO: grad norm: 5.870 5.805 0.870
2024-12-02-07:06:48-root-INFO: Loss Change: 140.787 -> 137.642
2024-12-02-07:06:48-root-INFO: Regularization Change: 0.000 -> 1.667
2024-12-02-07:06:48-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-07:06:48-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-07:06:48-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-07:06:48-root-INFO: grad norm: 10.998 10.829 1.917
2024-12-02-07:06:48-root-INFO: Loss too large (138.478->138.700)! Learning rate decreased to 0.04897.
2024-12-02-07:06:49-root-INFO: grad norm: 8.294 8.219 1.114
2024-12-02-07:06:49-root-INFO: grad norm: 6.207 6.144 0.880
2024-12-02-07:06:50-root-INFO: grad norm: 5.177 5.124 0.738
2024-12-02-07:06:50-root-INFO: grad norm: 4.613 4.564 0.669
2024-12-02-07:06:51-root-INFO: grad norm: 4.192 4.140 0.656
2024-12-02-07:06:51-root-INFO: grad norm: 3.981 3.933 0.612
2024-12-02-07:06:52-root-INFO: grad norm: 3.781 3.729 0.623
2024-12-02-07:06:52-root-INFO: Loss Change: 138.478 -> 134.491
2024-12-02-07:06:52-root-INFO: Regularization Change: 0.000 -> 1.045
2024-12-02-07:06:52-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-07:06:52-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-07:06:52-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-07:06:53-root-INFO: grad norm: 8.165 8.008 1.594
2024-12-02-07:06:53-root-INFO: grad norm: 7.635 7.549 1.142
2024-12-02-07:06:54-root-INFO: grad norm: 7.836 7.763 1.069
2024-12-02-07:06:54-root-INFO: grad norm: 9.173 9.112 1.054
2024-12-02-07:06:54-root-INFO: Loss too large (134.185->134.335)! Learning rate decreased to 0.05031.
2024-12-02-07:06:55-root-INFO: grad norm: 7.218 7.170 0.825
2024-12-02-07:06:55-root-INFO: grad norm: 5.830 5.790 0.690
2024-12-02-07:06:56-root-INFO: grad norm: 4.844 4.805 0.613
2024-12-02-07:06:56-root-INFO: grad norm: 4.122 4.083 0.562
2024-12-02-07:06:57-root-INFO: Loss Change: 135.005 -> 131.779
2024-12-02-07:06:57-root-INFO: Regularization Change: 0.000 -> 1.164
2024-12-02-07:06:57-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-07:06:57-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-07:06:57-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-07:06:57-root-INFO: grad norm: 6.253 6.145 1.160
2024-12-02-07:06:57-root-INFO: grad norm: 6.863 6.801 0.921
2024-12-02-07:06:58-root-INFO: grad norm: 8.096 8.035 0.991
2024-12-02-07:06:58-root-INFO: Loss too large (131.749->131.820)! Learning rate decreased to 0.05169.
2024-12-02-07:06:59-root-INFO: grad norm: 6.462 6.414 0.783
2024-12-02-07:06:59-root-INFO: grad norm: 5.301 5.260 0.657
2024-12-02-07:07:00-root-INFO: grad norm: 4.454 4.413 0.600
2024-12-02-07:07:00-root-INFO: grad norm: 3.855 3.816 0.547
2024-12-02-07:07:01-root-INFO: grad norm: 3.432 3.390 0.532
2024-12-02-07:07:01-root-INFO: Loss Change: 132.163 -> 129.343
2024-12-02-07:07:01-root-INFO: Regularization Change: 0.000 -> 1.052
2024-12-02-07:07:01-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-07:07:01-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-07:07:01-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-07:07:01-root-INFO: grad norm: 6.697 6.582 1.235
2024-12-02-07:07:01-root-INFO: Loss too large (129.773->129.789)! Learning rate decreased to 0.05310.
2024-12-02-07:07:02-root-INFO: grad norm: 5.576 5.525 0.757
2024-12-02-07:07:02-root-INFO: grad norm: 4.981 4.929 0.716
2024-12-02-07:07:03-root-INFO: grad norm: 4.598 4.554 0.637
2024-12-02-07:07:03-root-INFO: grad norm: 4.271 4.227 0.609
2024-12-02-07:07:04-root-INFO: grad norm: 4.091 4.048 0.590
2024-12-02-07:07:04-root-INFO: grad norm: 3.952 3.911 0.569
2024-12-02-07:07:05-root-INFO: grad norm: 3.845 3.802 0.568
2024-12-02-07:07:05-root-INFO: Loss Change: 129.773 -> 127.221
2024-12-02-07:07:05-root-INFO: Regularization Change: 0.000 -> 0.906
2024-12-02-07:07:05-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-07:07:05-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-07:07:05-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-07:07:05-root-INFO: grad norm: 6.781 6.662 1.264
2024-12-02-07:07:06-root-INFO: Loss too large (127.720->127.806)! Learning rate decreased to 0.05453.
2024-12-02-07:07:06-root-INFO: grad norm: 5.185 5.135 0.717
2024-12-02-07:07:07-root-INFO: grad norm: 3.528 3.476 0.600
2024-12-02-07:07:07-root-INFO: grad norm: 3.251 3.212 0.503
2024-12-02-07:07:08-root-INFO: grad norm: 3.306 3.265 0.517
2024-12-02-07:07:08-root-INFO: grad norm: 3.368 3.329 0.511
2024-12-02-07:07:09-root-INFO: grad norm: 3.833 3.795 0.538
2024-12-02-07:07:09-root-INFO: grad norm: 3.777 3.737 0.548
2024-12-02-07:07:09-root-INFO: Loss Change: 127.720 -> 125.247
2024-12-02-07:07:09-root-INFO: Regularization Change: 0.000 -> 0.903
2024-12-02-07:07:09-root-INFO: Undo step: 90
2024-12-02-07:07:09-root-INFO: Undo step: 91
2024-12-02-07:07:09-root-INFO: Undo step: 92
2024-12-02-07:07:09-root-INFO: Undo step: 93
2024-12-02-07:07:09-root-INFO: Undo step: 94
2024-12-02-07:07:10-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-07:07:10-root-INFO: grad norm: 52.166 51.624 7.504
2024-12-02-07:07:10-root-INFO: grad norm: 33.826 33.459 4.964
2024-12-02-07:07:11-root-INFO: grad norm: 29.250 28.846 4.846
2024-12-02-07:07:11-root-INFO: grad norm: 27.178 26.942 3.575
2024-12-02-07:07:12-root-INFO: grad norm: 26.178 25.869 4.014
2024-12-02-07:07:12-root-INFO: grad norm: 25.156 24.954 3.178
2024-12-02-07:07:13-root-INFO: grad norm: 24.212 23.953 3.534
2024-12-02-07:07:13-root-INFO: grad norm: 23.173 22.988 2.924
2024-12-02-07:07:14-root-INFO: Loss Change: 320.930 -> 163.549
2024-12-02-07:07:14-root-INFO: Regularization Change: 0.000 -> 49.689
2024-12-02-07:07:14-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-07:07:14-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-07:07:14-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-07:07:14-root-INFO: grad norm: 19.961 19.798 2.542
2024-12-02-07:07:14-root-INFO: grad norm: 19.460 19.315 2.369
2024-12-02-07:07:15-root-INFO: grad norm: 18.910 18.739 2.539
2024-12-02-07:07:15-root-INFO: grad norm: 18.440 18.301 2.253
2024-12-02-07:07:16-root-INFO: grad norm: 17.935 17.772 2.409
2024-12-02-07:07:16-root-INFO: grad norm: 17.465 17.332 2.150
2024-12-02-07:07:17-root-INFO: grad norm: 16.994 16.841 2.274
2024-12-02-07:07:17-root-INFO: grad norm: 16.537 16.410 2.043
2024-12-02-07:07:17-root-INFO: Loss Change: 161.671 -> 146.520
2024-12-02-07:07:17-root-INFO: Regularization Change: 0.000 -> 6.552
2024-12-02-07:07:17-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-07:07:17-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-07:07:18-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-07:07:18-root-INFO: grad norm: 13.564 13.480 1.507
2024-12-02-07:07:18-root-INFO: grad norm: 13.263 13.173 1.546
2024-12-02-07:07:19-root-INFO: grad norm: 13.158 13.061 1.599
2024-12-02-07:07:19-root-INFO: grad norm: 13.159 13.066 1.555
2024-12-02-07:07:20-root-INFO: grad norm: 13.137 13.035 1.632
2024-12-02-07:07:20-root-INFO: grad norm: 13.149 13.055 1.571
2024-12-02-07:07:21-root-INFO: grad norm: 13.130 13.027 1.641
2024-12-02-07:07:21-root-INFO: grad norm: 13.113 13.018 1.576
2024-12-02-07:07:22-root-INFO: Loss Change: 144.988 -> 138.200
2024-12-02-07:07:22-root-INFO: Regularization Change: 0.000 -> 3.366
2024-12-02-07:07:22-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-07:07:22-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-07:07:22-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-07:07:22-root-INFO: grad norm: 11.345 11.282 1.193
2024-12-02-07:07:22-root-INFO: grad norm: 11.206 11.132 1.287
2024-12-02-07:07:23-root-INFO: grad norm: 11.285 11.204 1.349
2024-12-02-07:07:23-root-INFO: grad norm: 11.477 11.397 1.351
2024-12-02-07:07:24-root-INFO: grad norm: 11.642 11.553 1.441
2024-12-02-07:07:24-root-INFO: grad norm: 11.837 11.751 1.421
2024-12-02-07:07:25-root-INFO: grad norm: 12.015 11.920 1.506
2024-12-02-07:07:25-root-INFO: grad norm: 12.202 12.111 1.484
2024-12-02-07:07:26-root-INFO: Loss Change: 137.549 -> 133.340
2024-12-02-07:07:26-root-INFO: Regularization Change: 0.000 -> 2.404
2024-12-02-07:07:26-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-07:07:26-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-07:07:26-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-07:07:26-root-INFO: grad norm: 10.156 10.105 1.009
2024-12-02-07:07:26-root-INFO: grad norm: 10.307 10.239 1.180
2024-12-02-07:07:27-root-INFO: grad norm: 10.519 10.448 1.227
2024-12-02-07:07:27-root-INFO: grad norm: 10.823 10.746 1.292
2024-12-02-07:07:28-root-INFO: grad norm: 11.042 10.959 1.350
2024-12-02-07:07:28-root-INFO: grad norm: 11.247 11.163 1.373
2024-12-02-07:07:29-root-INFO: grad norm: 11.471 11.382 1.425
2024-12-02-07:07:29-root-INFO: grad norm: 11.709 11.619 1.449
2024-12-02-07:07:30-root-INFO: Loss Change: 132.407 -> 129.507
2024-12-02-07:07:30-root-INFO: Regularization Change: 0.000 -> 1.934
2024-12-02-07:07:30-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-07:07:30-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-07:07:30-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-07:07:30-root-INFO: grad norm: 9.919 9.872 0.965
2024-12-02-07:07:30-root-INFO: grad norm: 9.937 9.869 1.154
2024-12-02-07:07:31-root-INFO: grad norm: 10.087 10.020 1.158
2024-12-02-07:07:31-root-INFO: grad norm: 10.335 10.261 1.240
2024-12-02-07:07:32-root-INFO: grad norm: 10.484 10.407 1.267
2024-12-02-07:07:32-root-INFO: grad norm: 10.607 10.527 1.297
2024-12-02-07:07:33-root-INFO: grad norm: 10.783 10.701 1.323
2024-12-02-07:07:33-root-INFO: grad norm: 10.996 10.912 1.358
2024-12-02-07:07:34-root-INFO: Loss Change: 128.851 -> 126.231
2024-12-02-07:07:34-root-INFO: Regularization Change: 0.000 -> 1.700
2024-12-02-07:07:34-root-INFO: Undo step: 90
2024-12-02-07:07:34-root-INFO: Undo step: 91
2024-12-02-07:07:34-root-INFO: Undo step: 92
2024-12-02-07:07:34-root-INFO: Undo step: 93
2024-12-02-07:07:34-root-INFO: Undo step: 94
2024-12-02-07:07:34-root-INFO: step: 95 lr_xt 0.05955910
2024-12-02-07:07:34-root-INFO: grad norm: 65.638 64.880 9.951
2024-12-02-07:07:34-root-INFO: grad norm: 39.692 39.137 6.614
2024-12-02-07:07:35-root-INFO: grad norm: 23.683 23.244 4.540
2024-12-02-07:07:35-root-INFO: grad norm: 18.255 17.964 3.247
2024-12-02-07:07:36-root-INFO: grad norm: 15.944 15.684 2.863
2024-12-02-07:07:36-root-INFO: grad norm: 14.540 14.336 2.425
2024-12-02-07:07:37-root-INFO: grad norm: 13.667 13.474 2.287
2024-12-02-07:07:37-root-INFO: grad norm: 13.083 12.928 2.010
2024-12-02-07:07:38-root-INFO: Loss Change: 341.899 -> 154.894
2024-12-02-07:07:38-root-INFO: Regularization Change: 0.000 -> 50.847
2024-12-02-07:07:38-root-INFO: Learning rate of xt decay: 0.12555 -> 0.12706.
2024-12-02-07:07:38-root-INFO: Coefficient of regularization decay: 0.00046 -> 0.00047.
2024-12-02-07:07:38-root-INFO: step: 94 lr_xt 0.06120788
2024-12-02-07:07:38-root-INFO: grad norm: 11.745 11.602 1.822
2024-12-02-07:07:39-root-INFO: grad norm: 10.963 10.845 1.606
2024-12-02-07:07:39-root-INFO: grad norm: 10.471 10.355 1.553
2024-12-02-07:07:40-root-INFO: grad norm: 10.399 10.301 1.427
2024-12-02-07:07:40-root-INFO: grad norm: 10.087 9.981 1.455
2024-12-02-07:07:41-root-INFO: grad norm: 9.815 9.726 1.315
2024-12-02-07:07:41-root-INFO: grad norm: 9.735 9.642 1.342
2024-12-02-07:07:42-root-INFO: grad norm: 9.865 9.787 1.237
2024-12-02-07:07:42-root-INFO: Loss Change: 154.249 -> 141.309
2024-12-02-07:07:42-root-INFO: Regularization Change: 0.000 -> 6.257
2024-12-02-07:07:42-root-INFO: Learning rate of xt decay: 0.12706 -> 0.12858.
2024-12-02-07:07:42-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00047.
2024-12-02-07:07:42-root-INFO: step: 93 lr_xt 0.06289219
2024-12-02-07:07:42-root-INFO: grad norm: 10.872 10.781 1.403
2024-12-02-07:07:43-root-INFO: grad norm: 11.296 11.237 1.154
2024-12-02-07:07:43-root-INFO: grad norm: 11.977 11.925 1.115
2024-12-02-07:07:44-root-INFO: grad norm: 12.791 12.738 1.158
2024-12-02-07:07:44-root-INFO: grad norm: 13.327 13.275 1.172
2024-12-02-07:07:45-root-INFO: grad norm: 13.662 13.606 1.232
2024-12-02-07:07:45-root-INFO: grad norm: 14.079 14.020 1.282
2024-12-02-07:07:46-root-INFO: grad norm: 14.531 14.466 1.366
2024-12-02-07:07:46-root-INFO: Loss Change: 141.217 -> 136.553
2024-12-02-07:07:46-root-INFO: Regularization Change: 0.000 -> 3.461
2024-12-02-07:07:46-root-INFO: Learning rate of xt decay: 0.12858 -> 0.13013.
2024-12-02-07:07:46-root-INFO: Coefficient of regularization decay: 0.00047 -> 0.00048.
2024-12-02-07:07:46-root-INFO: step: 92 lr_xt 0.06461248
2024-12-02-07:07:47-root-INFO: grad norm: 17.189 17.062 2.085
2024-12-02-07:07:47-root-INFO: grad norm: 17.562 17.451 1.970
2024-12-02-07:07:48-root-INFO: grad norm: 17.960 17.844 2.037
2024-12-02-07:07:48-root-INFO: grad norm: 18.384 18.262 2.115
2024-12-02-07:07:49-root-INFO: grad norm: 18.496 18.373 2.123
2024-12-02-07:07:49-root-INFO: grad norm: 18.454 18.324 2.193
2024-12-02-07:07:50-root-INFO: grad norm: 18.352 18.226 2.147
2024-12-02-07:07:50-root-INFO: grad norm: 18.151 18.018 2.193
2024-12-02-07:07:50-root-INFO: Loss Change: 137.726 -> 133.773
2024-12-02-07:07:50-root-INFO: Regularization Change: 0.000 -> 2.490
2024-12-02-07:07:51-root-INFO: Learning rate of xt decay: 0.13013 -> 0.13169.
2024-12-02-07:07:51-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00048.
2024-12-02-07:07:51-root-INFO: step: 91 lr_xt 0.06636917
2024-12-02-07:07:51-root-INFO: grad norm: 20.650 20.455 2.837
2024-12-02-07:07:51-root-INFO: grad norm: 19.728 19.568 2.503
2024-12-02-07:07:52-root-INFO: grad norm: 18.741 18.600 2.292
2024-12-02-07:07:52-root-INFO: grad norm: 17.844 17.707 2.207
2024-12-02-07:07:53-root-INFO: grad norm: 16.996 16.874 2.032
2024-12-02-07:07:53-root-INFO: grad norm: 16.184 16.061 1.989
2024-12-02-07:07:54-root-INFO: grad norm: 15.570 15.459 1.856
2024-12-02-07:07:54-root-INFO: grad norm: 15.071 14.961 1.820
2024-12-02-07:07:55-root-INFO: Loss Change: 135.612 -> 128.350
2024-12-02-07:07:55-root-INFO: Regularization Change: 0.000 -> 2.013
2024-12-02-07:07:55-root-INFO: Learning rate of xt decay: 0.13169 -> 0.13327.
2024-12-02-07:07:55-root-INFO: Coefficient of regularization decay: 0.00048 -> 0.00049.
2024-12-02-07:07:55-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-07:07:55-root-INFO: grad norm: 17.147 16.973 2.437
2024-12-02-07:07:55-root-INFO: grad norm: 16.153 16.031 1.977
2024-12-02-07:07:56-root-INFO: grad norm: 15.267 15.157 1.831
2024-12-02-07:07:57-root-INFO: grad norm: 14.557 14.456 1.711
2024-12-02-07:07:57-root-INFO: grad norm: 13.803 13.709 1.602
2024-12-02-07:07:58-root-INFO: grad norm: 13.031 12.941 1.532
2024-12-02-07:07:58-root-INFO: grad norm: 12.573 12.488 1.460
2024-12-02-07:07:58-root-INFO: grad norm: 12.342 12.262 1.400
2024-12-02-07:07:59-root-INFO: Loss Change: 129.898 -> 124.147
2024-12-02-07:07:59-root-INFO: Regularization Change: 0.000 -> 1.745
2024-12-02-07:07:59-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-07:07:59-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-07:07:59-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-07:07:59-root-INFO: grad norm: 15.574 15.393 2.371
2024-12-02-07:08:00-root-INFO: grad norm: 14.779 14.656 1.898
2024-12-02-07:08:00-root-INFO: grad norm: 14.157 14.048 1.751
2024-12-02-07:08:01-root-INFO: grad norm: 13.748 13.647 1.658
2024-12-02-07:08:01-root-INFO: grad norm: 13.098 13.007 1.542
2024-12-02-07:08:02-root-INFO: grad norm: 12.352 12.260 1.500
2024-12-02-07:08:02-root-INFO: grad norm: 11.998 11.913 1.423
2024-12-02-07:08:03-root-INFO: grad norm: 11.922 11.842 1.381
2024-12-02-07:08:03-root-INFO: Loss Change: 125.673 -> 120.945
2024-12-02-07:08:03-root-INFO: Regularization Change: 0.000 -> 1.612
2024-12-02-07:08:03-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-07:08:03-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-07:08:03-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-07:08:03-root-INFO: grad norm: 13.439 13.315 1.826
2024-12-02-07:08:04-root-INFO: grad norm: 12.521 12.436 1.452
2024-12-02-07:08:04-root-INFO: grad norm: 11.858 11.778 1.379
2024-12-02-07:08:05-root-INFO: grad norm: 11.432 11.363 1.260
2024-12-02-07:08:05-root-INFO: grad norm: 10.743 10.675 1.204
2024-12-02-07:08:06-root-INFO: grad norm: 9.892 9.828 1.124
2024-12-02-07:08:06-root-INFO: grad norm: 9.599 9.536 1.092
2024-12-02-07:08:07-root-INFO: grad norm: 9.773 9.718 1.030
2024-12-02-07:08:07-root-INFO: Loss Change: 122.161 -> 117.933
2024-12-02-07:08:07-root-INFO: Regularization Change: 0.000 -> 1.486
2024-12-02-07:08:07-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-07:08:07-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-07:08:07-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-07:08:07-root-INFO: grad norm: 12.314 12.173 1.858
2024-12-02-07:08:08-root-INFO: grad norm: 11.272 11.185 1.401
2024-12-02-07:08:08-root-INFO: grad norm: 10.744 10.664 1.309
2024-12-02-07:08:09-root-INFO: grad norm: 10.650 10.583 1.194
2024-12-02-07:08:09-root-INFO: grad norm: 9.987 9.924 1.119
2024-12-02-07:08:10-root-INFO: grad norm: 9.117 9.057 1.049
2024-12-02-07:08:10-root-INFO: grad norm: 8.866 8.808 1.007
2024-12-02-07:08:10-root-INFO: grad norm: 9.169 9.119 0.958
2024-12-02-07:08:11-root-INFO: Loss too large (115.097->115.120)! Learning rate decreased to 0.05901.
2024-12-02-07:08:11-root-INFO: Loss Change: 118.936 -> 114.259
2024-12-02-07:08:11-root-INFO: Regularization Change: 0.000 -> 1.425
2024-12-02-07:08:11-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-07:08:11-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-07:08:11-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-07:08:11-root-INFO: grad norm: 8.291 8.186 1.315
2024-12-02-07:08:12-root-INFO: grad norm: 8.127 8.075 0.918
2024-12-02-07:08:12-root-INFO: grad norm: 7.683 7.634 0.869
2024-12-02-07:08:13-root-INFO: grad norm: 7.060 7.015 0.789
2024-12-02-07:08:13-root-INFO: grad norm: 7.027 6.985 0.772
2024-12-02-07:08:14-root-INFO: grad norm: 7.596 7.559 0.748
2024-12-02-07:08:14-root-INFO: Loss too large (112.916->113.020)! Learning rate decreased to 0.06057.
2024-12-02-07:08:14-root-INFO: grad norm: 5.128 5.096 0.566
2024-12-02-07:08:15-root-INFO: grad norm: 2.955 2.924 0.431
2024-12-02-07:08:15-root-INFO: Loss Change: 114.886 -> 111.470
2024-12-02-07:08:15-root-INFO: Regularization Change: 0.000 -> 1.246
2024-12-02-07:08:15-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-07:08:15-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-07:08:15-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-07:08:15-root-INFO: grad norm: 4.590 4.481 0.994
2024-12-02-07:08:16-root-INFO: grad norm: 4.160 4.116 0.601
2024-12-02-07:08:16-root-INFO: grad norm: 4.071 4.025 0.606
2024-12-02-07:08:17-root-INFO: grad norm: 4.066 4.030 0.541
2024-12-02-07:08:17-root-INFO: grad norm: 4.115 4.078 0.549
2024-12-02-07:08:18-root-INFO: grad norm: 4.365 4.334 0.515
2024-12-02-07:08:18-root-INFO: grad norm: 4.757 4.727 0.536
2024-12-02-07:08:19-root-INFO: grad norm: 6.519 6.491 0.598
2024-12-02-07:08:19-root-INFO: Loss too large (109.903->110.207)! Learning rate decreased to 0.06216.
2024-12-02-07:08:19-root-INFO: Loss Change: 111.698 -> 109.809
2024-12-02-07:08:19-root-INFO: Regularization Change: 0.000 -> 1.339
2024-12-02-07:08:19-root-INFO: Undo step: 85
2024-12-02-07:08:19-root-INFO: Undo step: 86
2024-12-02-07:08:19-root-INFO: Undo step: 87
2024-12-02-07:08:19-root-INFO: Undo step: 88
2024-12-02-07:08:19-root-INFO: Undo step: 89
2024-12-02-07:08:19-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-07:08:20-root-INFO: grad norm: 47.147 46.612 7.081
2024-12-02-07:08:20-root-INFO: grad norm: 27.195 26.689 5.224
2024-12-02-07:08:21-root-INFO: grad norm: 18.778 18.442 3.539
2024-12-02-07:08:21-root-INFO: grad norm: 14.365 14.092 2.787
2024-12-02-07:08:22-root-INFO: grad norm: 11.546 11.292 2.408
2024-12-02-07:08:22-root-INFO: grad norm: 9.803 9.581 2.076
2024-12-02-07:08:23-root-INFO: grad norm: 8.565 8.346 1.925
2024-12-02-07:08:23-root-INFO: grad norm: 7.756 7.571 1.685
2024-12-02-07:08:23-root-INFO: Loss Change: 295.818 -> 137.739
2024-12-02-07:08:23-root-INFO: Regularization Change: 0.000 -> 52.445
2024-12-02-07:08:23-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-07:08:23-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-07:08:24-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-07:08:24-root-INFO: grad norm: 6.951 6.768 1.584
2024-12-02-07:08:24-root-INFO: grad norm: 5.894 5.727 1.394
2024-12-02-07:08:25-root-INFO: grad norm: 5.334 5.184 1.257
2024-12-02-07:08:25-root-INFO: grad norm: 4.951 4.808 1.184
2024-12-02-07:08:26-root-INFO: grad norm: 4.671 4.537 1.113
2024-12-02-07:08:26-root-INFO: grad norm: 4.535 4.414 1.040
2024-12-02-07:08:26-root-INFO: grad norm: 4.454 4.336 1.019
2024-12-02-07:08:27-root-INFO: grad norm: 4.665 4.568 0.949
2024-12-02-07:08:27-root-INFO: Loss Change: 137.106 -> 124.982
2024-12-02-07:08:27-root-INFO: Regularization Change: 0.000 -> 6.707
2024-12-02-07:08:27-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-07:08:27-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-07:08:28-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-07:08:28-root-INFO: grad norm: 5.201 5.091 1.062
2024-12-02-07:08:28-root-INFO: grad norm: 5.234 5.154 0.913
2024-12-02-07:08:29-root-INFO: grad norm: 5.053 4.967 0.929
2024-12-02-07:08:29-root-INFO: grad norm: 4.885 4.814 0.833
2024-12-02-07:08:30-root-INFO: grad norm: 4.791 4.713 0.865
2024-12-02-07:08:30-root-INFO: grad norm: 4.758 4.694 0.779
2024-12-02-07:08:31-root-INFO: grad norm: 4.652 4.579 0.820
2024-12-02-07:08:31-root-INFO: grad norm: 4.517 4.457 0.733
2024-12-02-07:08:31-root-INFO: Loss Change: 125.253 -> 119.331
2024-12-02-07:08:31-root-INFO: Regularization Change: 0.000 -> 3.437
2024-12-02-07:08:31-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-07:08:31-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-07:08:32-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-07:08:32-root-INFO: grad norm: 6.473 6.358 1.216
2024-12-02-07:08:32-root-INFO: grad norm: 6.171 6.108 0.878
2024-12-02-07:08:33-root-INFO: grad norm: 6.106 6.046 0.854
2024-12-02-07:08:33-root-INFO: grad norm: 6.259 6.207 0.797
2024-12-02-07:08:34-root-INFO: grad norm: 6.129 6.075 0.807
2024-12-02-07:08:34-root-INFO: grad norm: 5.903 5.856 0.745
2024-12-02-07:08:35-root-INFO: grad norm: 5.944 5.894 0.766
2024-12-02-07:08:35-root-INFO: grad norm: 6.162 6.120 0.723
2024-12-02-07:08:35-root-INFO: Loss Change: 119.353 -> 115.280
2024-12-02-07:08:35-root-INFO: Regularization Change: 0.000 -> 2.435
2024-12-02-07:08:35-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-07:08:35-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-07:08:36-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-07:08:36-root-INFO: grad norm: 8.288 8.185 1.300
2024-12-02-07:08:36-root-INFO: grad norm: 8.008 7.949 0.973
2024-12-02-07:08:37-root-INFO: grad norm: 7.893 7.836 0.946
2024-12-02-07:08:37-root-INFO: grad norm: 7.912 7.863 0.883
2024-12-02-07:08:38-root-INFO: grad norm: 7.713 7.663 0.878
2024-12-02-07:08:38-root-INFO: grad norm: 7.417 7.372 0.821
2024-12-02-07:08:39-root-INFO: grad norm: 7.356 7.309 0.826
2024-12-02-07:08:39-root-INFO: grad norm: 7.451 7.410 0.784
2024-12-02-07:08:39-root-INFO: Loss Change: 115.836 -> 112.400
2024-12-02-07:08:39-root-INFO: Regularization Change: 0.000 -> 1.964
2024-12-02-07:08:39-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-07:08:39-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-07:08:40-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-07:08:40-root-INFO: grad norm: 9.354 9.256 1.349
2024-12-02-07:08:40-root-INFO: grad norm: 8.990 8.930 1.035
2024-12-02-07:08:41-root-INFO: grad norm: 8.735 8.678 0.999
2024-12-02-07:08:41-root-INFO: grad norm: 8.586 8.536 0.926
2024-12-02-07:08:42-root-INFO: grad norm: 8.260 8.210 0.907
2024-12-02-07:08:42-root-INFO: grad norm: 7.821 7.776 0.844
2024-12-02-07:08:43-root-INFO: grad norm: 7.673 7.627 0.838
2024-12-02-07:08:43-root-INFO: grad norm: 7.739 7.698 0.790
2024-12-02-07:08:43-root-INFO: Loss Change: 112.974 -> 109.656
2024-12-02-07:08:43-root-INFO: Regularization Change: 0.000 -> 1.737
2024-12-02-07:08:43-root-INFO: Undo step: 85
2024-12-02-07:08:43-root-INFO: Undo step: 86
2024-12-02-07:08:43-root-INFO: Undo step: 87
2024-12-02-07:08:43-root-INFO: Undo step: 88
2024-12-02-07:08:43-root-INFO: Undo step: 89
2024-12-02-07:08:43-root-INFO: step: 90 lr_xt 0.06816268
2024-12-02-07:08:44-root-INFO: grad norm: 48.288 47.915 5.989
2024-12-02-07:08:44-root-INFO: grad norm: 25.774 25.443 4.120
2024-12-02-07:08:45-root-INFO: grad norm: 18.423 18.189 2.927
2024-12-02-07:08:45-root-INFO: grad norm: 14.199 13.943 2.688
2024-12-02-07:08:46-root-INFO: grad norm: 11.777 11.584 2.122
2024-12-02-07:08:46-root-INFO: grad norm: 10.213 10.008 2.035
2024-12-02-07:08:46-root-INFO: grad norm: 9.121 8.965 1.679
2024-12-02-07:08:47-root-INFO: grad norm: 8.346 8.182 1.646
2024-12-02-07:08:47-root-INFO: Loss Change: 318.624 -> 138.301
2024-12-02-07:08:47-root-INFO: Regularization Change: 0.000 -> 63.980
2024-12-02-07:08:47-root-INFO: Learning rate of xt decay: 0.13327 -> 0.13487.
2024-12-02-07:08:47-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00049.
2024-12-02-07:08:47-root-INFO: step: 89 lr_xt 0.06999342
2024-12-02-07:08:48-root-INFO: grad norm: 11.137 10.941 2.078
2024-12-02-07:08:48-root-INFO: grad norm: 10.646 10.489 1.817
2024-12-02-07:08:49-root-INFO: grad norm: 10.438 10.319 1.569
2024-12-02-07:08:49-root-INFO: grad norm: 10.324 10.190 1.656
2024-12-02-07:08:50-root-INFO: grad norm: 10.304 10.196 1.487
2024-12-02-07:08:50-root-INFO: grad norm: 10.297 10.173 1.593
2024-12-02-07:08:50-root-INFO: grad norm: 10.379 10.273 1.475
2024-12-02-07:08:51-root-INFO: grad norm: 10.433 10.313 1.578
2024-12-02-07:08:51-root-INFO: Loss Change: 138.802 -> 126.280
2024-12-02-07:08:51-root-INFO: Regularization Change: 0.000 -> 7.041
2024-12-02-07:08:51-root-INFO: Learning rate of xt decay: 0.13487 -> 0.13649.
2024-12-02-07:08:51-root-INFO: Coefficient of regularization decay: 0.00049 -> 0.00050.
2024-12-02-07:08:51-root-INFO: step: 88 lr_xt 0.07186179
2024-12-02-07:08:52-root-INFO: grad norm: 12.773 12.616 1.998
2024-12-02-07:08:52-root-INFO: grad norm: 12.163 12.033 1.775
2024-12-02-07:08:53-root-INFO: grad norm: 11.467 11.356 1.591
2024-12-02-07:08:53-root-INFO: grad norm: 11.121 11.011 1.564
2024-12-02-07:08:54-root-INFO: grad norm: 11.045 10.941 1.510
2024-12-02-07:08:54-root-INFO: grad norm: 10.781 10.670 1.540
2024-12-02-07:08:55-root-INFO: grad norm: 10.565 10.463 1.467
2024-12-02-07:08:55-root-INFO: grad norm: 10.349 10.242 1.483
2024-12-02-07:08:55-root-INFO: Loss Change: 127.400 -> 120.285
2024-12-02-07:08:55-root-INFO: Regularization Change: 0.000 -> 3.568
2024-12-02-07:08:55-root-INFO: Learning rate of xt decay: 0.13649 -> 0.13813.
2024-12-02-07:08:55-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00050.
2024-12-02-07:08:56-root-INFO: step: 87 lr_xt 0.07376819
2024-12-02-07:08:56-root-INFO: grad norm: 14.416 14.192 2.530
2024-12-02-07:08:56-root-INFO: grad norm: 13.202 13.053 1.979
2024-12-02-07:08:57-root-INFO: grad norm: 12.133 12.014 1.697
2024-12-02-07:08:57-root-INFO: grad norm: 11.378 11.271 1.556
2024-12-02-07:08:58-root-INFO: grad norm: 10.740 10.643 1.437
2024-12-02-07:08:58-root-INFO: grad norm: 10.177 10.082 1.386
2024-12-02-07:08:59-root-INFO: grad norm: 9.709 9.621 1.308
2024-12-02-07:08:59-root-INFO: grad norm: 9.357 9.267 1.296
2024-12-02-07:08:59-root-INFO: Loss Change: 121.742 -> 115.438
2024-12-02-07:08:59-root-INFO: Regularization Change: 0.000 -> 2.567
2024-12-02-07:08:59-root-INFO: Learning rate of xt decay: 0.13813 -> 0.13978.
2024-12-02-07:08:59-root-INFO: Coefficient of regularization decay: 0.00050 -> 0.00051.
2024-12-02-07:09:00-root-INFO: step: 86 lr_xt 0.07571301
2024-12-02-07:09:00-root-INFO: grad norm: 12.924 12.738 2.184
2024-12-02-07:09:00-root-INFO: grad norm: 11.809 11.678 1.754
2024-12-02-07:09:01-root-INFO: grad norm: 10.602 10.496 1.502
2024-12-02-07:09:01-root-INFO: grad norm: 9.901 9.806 1.370
2024-12-02-07:09:02-root-INFO: grad norm: 9.421 9.332 1.292
2024-12-02-07:09:02-root-INFO: grad norm: 9.156 9.065 1.290
2024-12-02-07:09:03-root-INFO: grad norm: 9.999 9.908 1.349
2024-12-02-07:09:03-root-INFO: Loss too large (112.821->112.906)! Learning rate decreased to 0.06057.
2024-12-02-07:09:04-root-INFO: grad norm: 6.329 6.256 0.957
2024-12-02-07:09:04-root-INFO: Loss Change: 116.776 -> 110.819
2024-12-02-07:09:04-root-INFO: Regularization Change: 0.000 -> 1.977
2024-12-02-07:09:04-root-INFO: Learning rate of xt decay: 0.13978 -> 0.14146.
2024-12-02-07:09:04-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00051.
2024-12-02-07:09:04-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-07:09:04-root-INFO: grad norm: 6.394 6.251 1.344
2024-12-02-07:09:05-root-INFO: grad norm: 6.191 6.119 0.941
2024-12-02-07:09:05-root-INFO: grad norm: 7.692 7.617 1.073
2024-12-02-07:09:05-root-INFO: Loss too large (110.470->110.679)! Learning rate decreased to 0.06216.
2024-12-02-07:09:06-root-INFO: grad norm: 5.216 5.158 0.779
2024-12-02-07:09:06-root-INFO: grad norm: 2.933 2.886 0.525
2024-12-02-07:09:07-root-INFO: grad norm: 2.415 2.374 0.440
2024-12-02-07:09:07-root-INFO: grad norm: 2.215 2.173 0.430
2024-12-02-07:09:08-root-INFO: grad norm: 2.129 2.088 0.416
2024-12-02-07:09:08-root-INFO: Loss Change: 111.246 -> 107.993
2024-12-02-07:09:08-root-INFO: Regularization Change: 0.000 -> 1.386
2024-12-02-07:09:08-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-07:09:08-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-07:09:08-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-07:09:09-root-INFO: grad norm: 3.450 3.369 0.743
2024-12-02-07:09:09-root-INFO: grad norm: 3.471 3.424 0.572
2024-12-02-07:09:10-root-INFO: grad norm: 4.877 4.829 0.678
2024-12-02-07:09:10-root-INFO: Loss too large (107.309->107.407)! Learning rate decreased to 0.06378.
2024-12-02-07:09:10-root-INFO: grad norm: 4.259 4.215 0.612
2024-12-02-07:09:11-root-INFO: grad norm: 3.363 3.323 0.512
2024-12-02-07:09:11-root-INFO: grad norm: 3.468 3.431 0.507
2024-12-02-07:09:12-root-INFO: grad norm: 3.929 3.895 0.510
2024-12-02-07:09:12-root-INFO: grad norm: 3.814 3.777 0.532
2024-12-02-07:09:13-root-INFO: Loss Change: 107.937 -> 105.624
2024-12-02-07:09:13-root-INFO: Regularization Change: 0.000 -> 1.233
2024-12-02-07:09:13-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-07:09:13-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-07:09:13-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-07:09:13-root-INFO: grad norm: 6.221 6.108 1.177
2024-12-02-07:09:13-root-INFO: Loss too large (105.919->106.024)! Learning rate decreased to 0.06543.
2024-12-02-07:09:14-root-INFO: grad norm: 4.608 4.562 0.653
2024-12-02-07:09:14-root-INFO: grad norm: 2.765 2.718 0.509
2024-12-02-07:09:15-root-INFO: grad norm: 2.745 2.709 0.442
2024-12-02-07:09:15-root-INFO: grad norm: 3.250 3.218 0.457
2024-12-02-07:09:16-root-INFO: grad norm: 3.439 3.405 0.486
2024-12-02-07:09:16-root-INFO: grad norm: 3.935 3.905 0.491
2024-12-02-07:09:17-root-INFO: grad norm: 3.810 3.774 0.519
2024-12-02-07:09:17-root-INFO: Loss Change: 105.919 -> 103.497
2024-12-02-07:09:17-root-INFO: Regularization Change: 0.000 -> 1.068
2024-12-02-07:09:17-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-07:09:17-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-07:09:17-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-07:09:17-root-INFO: grad norm: 5.944 5.867 0.954
2024-12-02-07:09:18-root-INFO: Loss too large (103.878->104.120)! Learning rate decreased to 0.06711.
2024-12-02-07:09:18-root-INFO: grad norm: 4.351 4.307 0.616
2024-12-02-07:09:18-root-INFO: grad norm: 2.238 2.195 0.438
2024-12-02-07:09:19-root-INFO: grad norm: 2.124 2.088 0.389
2024-12-02-07:09:19-root-INFO: grad norm: 2.281 2.248 0.385
2024-12-02-07:09:20-root-INFO: grad norm: 2.525 2.492 0.407
2024-12-02-07:09:20-root-INFO: grad norm: 3.195 3.167 0.424
2024-12-02-07:09:21-root-INFO: grad norm: 3.455 3.422 0.476
2024-12-02-07:09:21-root-INFO: Loss Change: 103.878 -> 101.739
2024-12-02-07:09:21-root-INFO: Regularization Change: 0.000 -> 1.015
2024-12-02-07:09:21-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-07:09:21-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-07:09:21-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-07:09:22-root-INFO: grad norm: 6.652 6.547 1.180
2024-12-02-07:09:22-root-INFO: Loss too large (101.901->102.232)! Learning rate decreased to 0.06882.
2024-12-02-07:09:22-root-INFO: grad norm: 4.571 4.527 0.631
2024-12-02-07:09:23-root-INFO: grad norm: 2.257 2.213 0.442
2024-12-02-07:09:23-root-INFO: grad norm: 1.937 1.905 0.351
2024-12-02-07:09:24-root-INFO: grad norm: 1.836 1.803 0.349
2024-12-02-07:09:24-root-INFO: grad norm: 1.792 1.760 0.337
2024-12-02-07:09:25-root-INFO: grad norm: 1.769 1.737 0.336
2024-12-02-07:09:25-root-INFO: grad norm: 1.754 1.722 0.332
2024-12-02-07:09:25-root-INFO: Loss Change: 101.901 -> 99.524
2024-12-02-07:09:25-root-INFO: Regularization Change: 0.000 -> 1.031
2024-12-02-07:09:25-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-07:09:25-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-07:09:26-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-07:09:26-root-INFO: grad norm: 4.541 4.419 1.046
2024-12-02-07:09:26-root-INFO: grad norm: 4.836 4.782 0.722
2024-12-02-07:09:27-root-INFO: Loss too large (99.705->99.802)! Learning rate decreased to 0.07057.
2024-12-02-07:09:27-root-INFO: grad norm: 5.482 5.440 0.676
2024-12-02-07:09:27-root-INFO: Loss too large (99.370->99.483)! Learning rate decreased to 0.05645.
2024-12-02-07:09:28-root-INFO: grad norm: 4.013 3.979 0.517
2024-12-02-07:09:28-root-INFO: grad norm: 2.222 2.191 0.368
2024-12-02-07:09:29-root-INFO: grad norm: 2.215 2.187 0.352
2024-12-02-07:09:29-root-INFO: grad norm: 2.262 2.236 0.347
2024-12-02-07:09:30-root-INFO: grad norm: 2.299 2.271 0.356
2024-12-02-07:09:30-root-INFO: Loss Change: 99.980 -> 98.071
2024-12-02-07:09:30-root-INFO: Regularization Change: 0.000 -> 0.804
2024-12-02-07:09:30-root-INFO: Undo step: 80
2024-12-02-07:09:30-root-INFO: Undo step: 81
2024-12-02-07:09:30-root-INFO: Undo step: 82
2024-12-02-07:09:30-root-INFO: Undo step: 83
2024-12-02-07:09:30-root-INFO: Undo step: 84
2024-12-02-07:09:30-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-07:09:30-root-INFO: grad norm: 52.861 52.346 7.364
2024-12-02-07:09:31-root-INFO: grad norm: 26.774 26.476 3.989
2024-12-02-07:09:31-root-INFO: grad norm: 15.747 15.467 2.952
2024-12-02-07:09:32-root-INFO: grad norm: 11.654 11.445 2.200
2024-12-02-07:09:32-root-INFO: grad norm: 9.415 9.204 1.982
2024-12-02-07:09:33-root-INFO: grad norm: 8.063 7.885 1.689
2024-12-02-07:09:33-root-INFO: grad norm: 7.244 7.074 1.561
2024-12-02-07:09:34-root-INFO: grad norm: 6.751 6.603 1.404
2024-12-02-07:09:34-root-INFO: Loss Change: 287.987 -> 125.524
2024-12-02-07:09:34-root-INFO: Regularization Change: 0.000 -> 58.367
2024-12-02-07:09:34-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-07:09:34-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-07:09:34-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-07:09:34-root-INFO: grad norm: 6.824 6.695 1.324
2024-12-02-07:09:35-root-INFO: grad norm: 6.520 6.411 1.184
2024-12-02-07:09:35-root-INFO: grad norm: 6.516 6.418 1.125
2024-12-02-07:09:36-root-INFO: grad norm: 6.719 6.635 1.058
2024-12-02-07:09:37-root-INFO: grad norm: 6.969 6.893 1.024
2024-12-02-07:09:37-root-INFO: grad norm: 7.398 7.334 0.969
2024-12-02-07:09:37-root-INFO: grad norm: 7.759 7.699 0.962
2024-12-02-07:09:38-root-INFO: grad norm: 8.304 8.255 0.903
2024-12-02-07:09:38-root-INFO: Loss Change: 125.282 -> 114.707
2024-12-02-07:09:38-root-INFO: Regularization Change: 0.000 -> 7.363
2024-12-02-07:09:38-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-07:09:38-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-07:09:39-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-07:09:39-root-INFO: grad norm: 10.213 10.129 1.310
2024-12-02-07:09:39-root-INFO: grad norm: 10.241 10.186 1.066
2024-12-02-07:09:40-root-INFO: grad norm: 10.090 10.035 1.051
2024-12-02-07:09:40-root-INFO: grad norm: 9.978 9.931 0.961
2024-12-02-07:09:41-root-INFO: grad norm: 9.660 9.612 0.963
2024-12-02-07:09:41-root-INFO: grad norm: 9.217 9.175 0.880
2024-12-02-07:09:42-root-INFO: grad norm: 9.020 8.976 0.885
2024-12-02-07:09:42-root-INFO: grad norm: 8.914 8.877 0.813
2024-12-02-07:09:43-root-INFO: Loss Change: 115.159 -> 109.000
2024-12-02-07:09:43-root-INFO: Regularization Change: 0.000 -> 3.838
2024-12-02-07:09:43-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-07:09:43-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-07:09:43-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-07:09:43-root-INFO: grad norm: 9.637 9.578 1.064
2024-12-02-07:09:43-root-INFO: grad norm: 9.087 9.044 0.886
2024-12-02-07:09:44-root-INFO: grad norm: 8.705 8.662 0.861
2024-12-02-07:09:44-root-INFO: grad norm: 8.377 8.341 0.782
2024-12-02-07:09:45-root-INFO: grad norm: 8.022 7.984 0.787
2024-12-02-07:09:45-root-INFO: grad norm: 7.599 7.566 0.705
2024-12-02-07:09:46-root-INFO: grad norm: 7.429 7.393 0.726
2024-12-02-07:09:46-root-INFO: grad norm: 7.405 7.375 0.659
2024-12-02-07:09:47-root-INFO: Loss Change: 109.476 -> 104.690
2024-12-02-07:09:47-root-INFO: Regularization Change: 0.000 -> 2.658
2024-12-02-07:09:47-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-07:09:47-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-07:09:47-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-07:09:47-root-INFO: grad norm: 8.681 8.610 1.107
2024-12-02-07:09:47-root-INFO: grad norm: 8.077 8.038 0.800
2024-12-02-07:09:48-root-INFO: grad norm: 7.814 7.773 0.791
2024-12-02-07:09:48-root-INFO: grad norm: 7.820 7.788 0.702
2024-12-02-07:09:49-root-INFO: grad norm: 7.345 7.310 0.721
2024-12-02-07:09:49-root-INFO: grad norm: 6.623 6.595 0.617
2024-12-02-07:09:50-root-INFO: grad norm: 6.620 6.589 0.638
2024-12-02-07:09:50-root-INFO: grad norm: 7.253 7.227 0.612
2024-12-02-07:09:51-root-INFO: Loss too large (101.231->101.350)! Learning rate decreased to 0.06882.
2024-12-02-07:09:51-root-INFO: Loss Change: 104.924 -> 100.763
2024-12-02-07:09:51-root-INFO: Regularization Change: 0.000 -> 2.069
2024-12-02-07:09:51-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-07:09:51-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-07:09:51-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-07:09:51-root-INFO: grad norm: 6.995 6.923 1.006
2024-12-02-07:09:52-root-INFO: grad norm: 6.893 6.859 0.684
2024-12-02-07:09:52-root-INFO: grad norm: 6.502 6.467 0.670
2024-12-02-07:09:53-root-INFO: grad norm: 5.995 5.966 0.585
2024-12-02-07:09:53-root-INFO: grad norm: 6.050 6.019 0.610
2024-12-02-07:09:54-root-INFO: grad norm: 6.722 6.695 0.596
2024-12-02-07:09:54-root-INFO: Loss too large (99.131->99.313)! Learning rate decreased to 0.07057.
2024-12-02-07:09:54-root-INFO: grad norm: 4.796 4.762 0.566
2024-12-02-07:09:55-root-INFO: grad norm: 2.841 2.813 0.400
2024-12-02-07:09:55-root-INFO: Loss Change: 101.302 -> 97.762
2024-12-02-07:09:55-root-INFO: Regularization Change: 0.000 -> 1.672
2024-12-02-07:09:55-root-INFO: Undo step: 80
2024-12-02-07:09:55-root-INFO: Undo step: 81
2024-12-02-07:09:55-root-INFO: Undo step: 82
2024-12-02-07:09:55-root-INFO: Undo step: 83
2024-12-02-07:09:55-root-INFO: Undo step: 84
2024-12-02-07:09:55-root-INFO: step: 85 lr_xt 0.07769664
2024-12-02-07:09:55-root-INFO: grad norm: 45.483 45.075 6.074
2024-12-02-07:09:56-root-INFO: grad norm: 23.729 23.442 3.676
2024-12-02-07:09:56-root-INFO: grad norm: 16.577 16.282 3.117
2024-12-02-07:09:57-root-INFO: grad norm: 13.625 13.426 2.316
2024-12-02-07:09:57-root-INFO: grad norm: 11.869 11.669 2.169
2024-12-02-07:09:58-root-INFO: grad norm: 10.948 10.815 1.695
2024-12-02-07:09:58-root-INFO: grad norm: 10.510 10.376 1.670
2024-12-02-07:09:59-root-INFO: grad norm: 10.284 10.190 1.388
2024-12-02-07:09:59-root-INFO: Loss Change: 290.565 -> 125.690
2024-12-02-07:09:59-root-INFO: Regularization Change: 0.000 -> 59.924
2024-12-02-07:09:59-root-INFO: Learning rate of xt decay: 0.14146 -> 0.14316.
2024-12-02-07:09:59-root-INFO: Coefficient of regularization decay: 0.00051 -> 0.00052.
2024-12-02-07:09:59-root-INFO: step: 84 lr_xt 0.07971945
2024-12-02-07:09:59-root-INFO: grad norm: 9.842 9.763 1.250
2024-12-02-07:10:00-root-INFO: grad norm: 9.536 9.466 1.151
2024-12-02-07:10:00-root-INFO: grad norm: 9.168 9.102 1.096
2024-12-02-07:10:01-root-INFO: grad norm: 9.175 9.116 1.040
2024-12-02-07:10:01-root-INFO: grad norm: 9.459 9.406 1.002
2024-12-02-07:10:02-root-INFO: grad norm: 9.158 9.105 0.980
2024-12-02-07:10:02-root-INFO: grad norm: 8.601 8.552 0.919
2024-12-02-07:10:03-root-INFO: grad norm: 8.661 8.614 0.903
2024-12-02-07:10:03-root-INFO: Loss Change: 125.087 -> 114.094
2024-12-02-07:10:03-root-INFO: Regularization Change: 0.000 -> 6.975
2024-12-02-07:10:03-root-INFO: Learning rate of xt decay: 0.14316 -> 0.14488.
2024-12-02-07:10:03-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00052.
2024-12-02-07:10:03-root-INFO: step: 83 lr_xt 0.08178179
2024-12-02-07:10:04-root-INFO: grad norm: 9.315 9.245 1.144
2024-12-02-07:10:04-root-INFO: grad norm: 7.784 7.734 0.879
2024-12-02-07:10:04-root-INFO: grad norm: 6.718 6.678 0.730
2024-12-02-07:10:05-root-INFO: grad norm: 6.691 6.656 0.687
2024-12-02-07:10:05-root-INFO: grad norm: 6.922 6.878 0.780
2024-12-02-07:10:06-root-INFO: grad norm: 8.006 7.959 0.870
2024-12-02-07:10:06-root-INFO: grad norm: 7.739 7.677 0.983
2024-12-02-07:10:07-root-INFO: grad norm: 7.141 7.088 0.873
2024-12-02-07:10:07-root-INFO: Loss Change: 113.868 -> 108.034
2024-12-02-07:10:07-root-INFO: Regularization Change: 0.000 -> 3.731
2024-12-02-07:10:07-root-INFO: Learning rate of xt decay: 0.14488 -> 0.14661.
2024-12-02-07:10:07-root-INFO: Coefficient of regularization decay: 0.00052 -> 0.00053.
2024-12-02-07:10:07-root-INFO: step: 82 lr_xt 0.08388403
2024-12-02-07:10:08-root-INFO: grad norm: 6.304 6.267 0.683
2024-12-02-07:10:08-root-INFO: grad norm: 6.780 6.738 0.752
2024-12-02-07:10:09-root-INFO: grad norm: 7.119 7.065 0.877
2024-12-02-07:10:09-root-INFO: grad norm: 8.041 7.983 0.963
2024-12-02-07:10:10-root-INFO: grad norm: 7.634 7.563 1.040
2024-12-02-07:10:10-root-INFO: grad norm: 6.907 6.849 0.893
2024-12-02-07:10:11-root-INFO: grad norm: 7.089 7.031 0.905
2024-12-02-07:10:11-root-INFO: grad norm: 7.907 7.843 1.000
2024-12-02-07:10:11-root-INFO: Loss Change: 107.769 -> 104.706
2024-12-02-07:10:11-root-INFO: Regularization Change: 0.000 -> 2.652
2024-12-02-07:10:11-root-INFO: Learning rate of xt decay: 0.14661 -> 0.14837.
2024-12-02-07:10:11-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00053.
2024-12-02-07:10:12-root-INFO: step: 81 lr_xt 0.08602650
2024-12-02-07:10:12-root-INFO: grad norm: 6.782 6.743 0.723
2024-12-02-07:10:12-root-INFO: grad norm: 7.016 6.966 0.838
2024-12-02-07:10:13-root-INFO: grad norm: 6.908 6.851 0.881
2024-12-02-07:10:13-root-INFO: grad norm: 6.591 6.533 0.866
2024-12-02-07:10:14-root-INFO: grad norm: 6.992 6.930 0.931
2024-12-02-07:10:14-root-INFO: grad norm: 8.219 8.151 1.059
2024-12-02-07:10:14-root-INFO: Loss too large (102.013->102.230)! Learning rate decreased to 0.06882.
2024-12-02-07:10:15-root-INFO: grad norm: 5.608 5.543 0.852
2024-12-02-07:10:15-root-INFO: grad norm: 3.312 3.269 0.537
2024-12-02-07:10:16-root-INFO: Loss Change: 103.979 -> 100.097
2024-12-02-07:10:16-root-INFO: Regularization Change: 0.000 -> 1.911
2024-12-02-07:10:16-root-INFO: Learning rate of xt decay: 0.14837 -> 0.15015.
2024-12-02-07:10:16-root-INFO: Coefficient of regularization decay: 0.00053 -> 0.00054.
2024-12-02-07:10:16-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-07:10:16-root-INFO: grad norm: 3.624 3.494 0.963
2024-12-02-07:10:16-root-INFO: grad norm: 3.327 3.275 0.590
2024-12-02-07:10:17-root-INFO: grad norm: 4.351 4.299 0.672
2024-12-02-07:10:17-root-INFO: Loss too large (99.540->99.553)! Learning rate decreased to 0.07057.
2024-12-02-07:10:18-root-INFO: grad norm: 4.056 4.007 0.628
2024-12-02-07:10:18-root-INFO: grad norm: 3.893 3.855 0.541
2024-12-02-07:10:19-root-INFO: grad norm: 3.825 3.783 0.568
2024-12-02-07:10:19-root-INFO: grad norm: 3.761 3.727 0.503
2024-12-02-07:10:19-root-INFO: grad norm: 3.743 3.703 0.547
2024-12-02-07:10:20-root-INFO: Loss Change: 100.316 -> 97.870
2024-12-02-07:10:20-root-INFO: Regularization Change: 0.000 -> 1.430
2024-12-02-07:10:20-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-07:10:20-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-07:10:20-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-07:10:20-root-INFO: grad norm: 5.963 5.841 1.201
2024-12-02-07:10:20-root-INFO: Loss too large (98.042->98.196)! Learning rate decreased to 0.07235.
2024-12-02-07:10:21-root-INFO: grad norm: 4.726 4.671 0.719
2024-12-02-07:10:21-root-INFO: grad norm: 3.356 3.308 0.568
2024-12-02-07:10:22-root-INFO: grad norm: 3.426 3.387 0.516
2024-12-02-07:10:22-root-INFO: grad norm: 3.957 3.923 0.517
2024-12-02-07:10:23-root-INFO: grad norm: 3.900 3.860 0.552
2024-12-02-07:10:23-root-INFO: grad norm: 3.793 3.760 0.501
2024-12-02-07:10:24-root-INFO: grad norm: 3.803 3.765 0.537
2024-12-02-07:10:24-root-INFO: Loss Change: 98.042 -> 95.637
2024-12-02-07:10:24-root-INFO: Regularization Change: 0.000 -> 1.190
2024-12-02-07:10:24-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-07:10:24-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-07:10:24-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-07:10:24-root-INFO: grad norm: 5.915 5.814 1.086
2024-12-02-07:10:24-root-INFO: Loss too large (96.124->96.437)! Learning rate decreased to 0.07416.
2024-12-02-07:10:25-root-INFO: grad norm: 4.682 4.629 0.701
2024-12-02-07:10:25-root-INFO: grad norm: 2.996 2.948 0.532
2024-12-02-07:10:26-root-INFO: grad norm: 3.115 3.079 0.473
2024-12-02-07:10:26-root-INFO: grad norm: 3.853 3.821 0.495
2024-12-02-07:10:27-root-INFO: grad norm: 3.889 3.851 0.541
2024-12-02-07:10:27-root-INFO: grad norm: 3.961 3.928 0.507
2024-12-02-07:10:28-root-INFO: grad norm: 3.922 3.883 0.547
2024-12-02-07:10:28-root-INFO: Loss Change: 96.124 -> 93.926
2024-12-02-07:10:28-root-INFO: Regularization Change: 0.000 -> 1.118
2024-12-02-07:10:28-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-07:10:28-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-07:10:28-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-07:10:28-root-INFO: grad norm: 6.419 6.310 1.177
2024-12-02-07:10:29-root-INFO: Loss too large (94.057->94.472)! Learning rate decreased to 0.07600.
2024-12-02-07:10:29-root-INFO: grad norm: 4.764 4.703 0.756
2024-12-02-07:10:30-root-INFO: grad norm: 2.575 2.529 0.485
2024-12-02-07:10:30-root-INFO: grad norm: 2.463 2.427 0.417
2024-12-02-07:10:31-root-INFO: grad norm: 2.942 2.915 0.402
2024-12-02-07:10:31-root-INFO: grad norm: 3.320 3.287 0.471
2024-12-02-07:10:32-root-INFO: grad norm: 4.212 4.184 0.486
2024-12-02-07:10:32-root-INFO: Loss too large (92.280->92.316)! Learning rate decreased to 0.06080.
2024-12-02-07:10:32-root-INFO: grad norm: 3.522 3.489 0.479
2024-12-02-07:10:33-root-INFO: Loss Change: 94.057 -> 91.812
2024-12-02-07:10:33-root-INFO: Regularization Change: 0.000 -> 1.006
2024-12-02-07:10:33-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-07:10:33-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-07:10:33-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-07:10:33-root-INFO: grad norm: 5.791 5.687 1.088
2024-12-02-07:10:33-root-INFO: Loss too large (92.209->92.522)! Learning rate decreased to 0.07788.
2024-12-02-07:10:34-root-INFO: grad norm: 4.640 4.587 0.698
2024-12-02-07:10:34-root-INFO: grad norm: 3.249 3.206 0.528
2024-12-02-07:10:34-root-INFO: grad norm: 3.476 3.440 0.504
2024-12-02-07:10:35-root-INFO: grad norm: 4.352 4.321 0.521
2024-12-02-07:10:35-root-INFO: Loss too large (90.999->91.055)! Learning rate decreased to 0.06231.
2024-12-02-07:10:36-root-INFO: grad norm: 3.580 3.548 0.476
2024-12-02-07:10:36-root-INFO: grad norm: 2.597 2.572 0.360
2024-12-02-07:10:37-root-INFO: grad norm: 2.497 2.471 0.358
2024-12-02-07:10:37-root-INFO: Loss Change: 92.209 -> 90.180
2024-12-02-07:10:37-root-INFO: Regularization Change: 0.000 -> 0.894
2024-12-02-07:10:37-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-07:10:37-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-07:10:37-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-07:10:37-root-INFO: grad norm: 4.018 3.930 0.838
2024-12-02-07:10:38-root-INFO: grad norm: 4.704 4.646 0.733
2024-12-02-07:10:38-root-INFO: grad norm: 7.681 7.618 0.986
2024-12-02-07:10:38-root-INFO: Loss too large (90.333->91.106)! Learning rate decreased to 0.07980.
2024-12-02-07:10:38-root-INFO: Loss too large (90.333->90.554)! Learning rate decreased to 0.06384.
2024-12-02-07:10:39-root-INFO: grad norm: 4.270 4.224 0.626
2024-12-02-07:10:39-root-INFO: grad norm: 1.904 1.879 0.307
2024-12-02-07:10:40-root-INFO: grad norm: 1.727 1.704 0.284
2024-12-02-07:10:40-root-INFO: grad norm: 1.710 1.686 0.288
2024-12-02-07:10:41-root-INFO: grad norm: 1.708 1.685 0.281
2024-12-02-07:10:41-root-INFO: Loss Change: 90.381 -> 88.609
2024-12-02-07:10:41-root-INFO: Regularization Change: 0.000 -> 0.904
2024-12-02-07:10:41-root-INFO: Undo step: 75
2024-12-02-07:10:41-root-INFO: Undo step: 76
2024-12-02-07:10:41-root-INFO: Undo step: 77
2024-12-02-07:10:41-root-INFO: Undo step: 78
2024-12-02-07:10:41-root-INFO: Undo step: 79
2024-12-02-07:10:41-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-07:10:42-root-INFO: grad norm: 40.614 40.208 5.726
2024-12-02-07:10:42-root-INFO: grad norm: 23.435 23.154 3.614
2024-12-02-07:10:43-root-INFO: grad norm: 16.236 15.983 2.854
2024-12-02-07:10:43-root-INFO: grad norm: 12.515 12.312 2.248
2024-12-02-07:10:44-root-INFO: grad norm: 10.592 10.417 1.917
2024-12-02-07:10:44-root-INFO: grad norm: 9.557 9.404 1.704
2024-12-02-07:10:45-root-INFO: grad norm: 8.962 8.831 1.527
2024-12-02-07:10:45-root-INFO: grad norm: 8.696 8.576 1.439
2024-12-02-07:10:45-root-INFO: Loss Change: 266.679 -> 113.048
2024-12-02-07:10:45-root-INFO: Regularization Change: 0.000 -> 65.749
2024-12-02-07:10:45-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-07:10:45-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-07:10:45-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-07:10:46-root-INFO: grad norm: 7.529 7.451 1.081
2024-12-02-07:10:46-root-INFO: grad norm: 7.297 7.220 1.056
2024-12-02-07:10:47-root-INFO: grad norm: 7.275 7.203 1.021
2024-12-02-07:10:47-root-INFO: grad norm: 7.437 7.363 1.043
2024-12-02-07:10:48-root-INFO: grad norm: 7.482 7.410 1.038
2024-12-02-07:10:48-root-INFO: grad norm: 7.540 7.465 1.061
2024-12-02-07:10:49-root-INFO: grad norm: 7.660 7.587 1.056
2024-12-02-07:10:49-root-INFO: grad norm: 7.850 7.773 1.095
2024-12-02-07:10:50-root-INFO: Loss Change: 112.218 -> 102.585
2024-12-02-07:10:50-root-INFO: Regularization Change: 0.000 -> 7.271
2024-12-02-07:10:50-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-07:10:50-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-07:10:50-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-07:10:50-root-INFO: grad norm: 6.802 6.756 0.790
2024-12-02-07:10:50-root-INFO: grad norm: 6.864 6.807 0.882
2024-12-02-07:10:51-root-INFO: grad norm: 6.985 6.927 0.900
2024-12-02-07:10:51-root-INFO: grad norm: 7.217 7.153 0.958
2024-12-02-07:10:52-root-INFO: grad norm: 7.298 7.232 0.982
2024-12-02-07:10:52-root-INFO: grad norm: 7.331 7.261 1.007
2024-12-02-07:10:53-root-INFO: grad norm: 7.497 7.426 1.024
2024-12-02-07:10:53-root-INFO: grad norm: 7.802 7.728 1.072
2024-12-02-07:10:54-root-INFO: Loss Change: 102.295 -> 97.983
2024-12-02-07:10:54-root-INFO: Regularization Change: 0.000 -> 3.612
2024-12-02-07:10:54-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-07:10:54-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-07:10:54-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-07:10:54-root-INFO: grad norm: 6.425 6.382 0.741
2024-12-02-07:10:55-root-INFO: grad norm: 6.243 6.191 0.801
2024-12-02-07:10:55-root-INFO: grad norm: 6.326 6.267 0.859
2024-12-02-07:10:56-root-INFO: grad norm: 6.832 6.772 0.906
2024-12-02-07:10:56-root-INFO: Loss too large (95.520->95.536)! Learning rate decreased to 0.07600.
2024-12-02-07:10:56-root-INFO: grad norm: 4.985 4.926 0.767
2024-12-02-07:10:57-root-INFO: grad norm: 3.415 3.370 0.555
2024-12-02-07:10:57-root-INFO: grad norm: 3.150 3.107 0.518
2024-12-02-07:10:58-root-INFO: grad norm: 3.241 3.205 0.481
2024-12-02-07:10:58-root-INFO: Loss Change: 97.141 -> 93.226
2024-12-02-07:10:58-root-INFO: Regularization Change: 0.000 -> 2.060
2024-12-02-07:10:58-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-07:10:58-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-07:10:58-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-07:10:58-root-INFO: grad norm: 3.392 3.315 0.719
2024-12-02-07:10:59-root-INFO: grad norm: 3.314 3.286 0.429
2024-12-02-07:10:59-root-INFO: grad norm: 3.637 3.606 0.475
2024-12-02-07:11:00-root-INFO: grad norm: 5.179 5.154 0.508
2024-12-02-07:11:00-root-INFO: Loss too large (92.126->92.316)! Learning rate decreased to 0.07788.
2024-12-02-07:11:00-root-INFO: grad norm: 4.029 3.991 0.553
2024-12-02-07:11:01-root-INFO: grad norm: 2.421 2.384 0.422
2024-12-02-07:11:01-root-INFO: grad norm: 2.474 2.440 0.408
2024-12-02-07:11:02-root-INFO: grad norm: 2.790 2.760 0.406
2024-12-02-07:11:02-root-INFO: Loss Change: 93.156 -> 90.603
2024-12-02-07:11:02-root-INFO: Regularization Change: 0.000 -> 1.705
2024-12-02-07:11:02-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-07:11:02-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-07:11:02-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-07:11:02-root-INFO: grad norm: 3.455 3.401 0.604
2024-12-02-07:11:03-root-INFO: grad norm: 4.420 4.397 0.449
2024-12-02-07:11:03-root-INFO: Loss too large (90.307->90.406)! Learning rate decreased to 0.07980.
2024-12-02-07:11:03-root-INFO: grad norm: 3.846 3.815 0.491
2024-12-02-07:11:04-root-INFO: grad norm: 3.172 3.142 0.436
2024-12-02-07:11:04-root-INFO: grad norm: 3.338 3.304 0.472
2024-12-02-07:11:05-root-INFO: grad norm: 3.765 3.735 0.471
2024-12-02-07:11:05-root-INFO: grad norm: 3.730 3.693 0.524
2024-12-02-07:11:06-root-INFO: grad norm: 3.647 3.615 0.482
2024-12-02-07:11:06-root-INFO: Loss Change: 90.633 -> 88.676
2024-12-02-07:11:06-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-02-07:11:06-root-INFO: Undo step: 75
2024-12-02-07:11:06-root-INFO: Undo step: 76
2024-12-02-07:11:06-root-INFO: Undo step: 77
2024-12-02-07:11:06-root-INFO: Undo step: 78
2024-12-02-07:11:06-root-INFO: Undo step: 79
2024-12-02-07:11:06-root-INFO: step: 80 lr_xt 0.08820955
2024-12-02-07:11:06-root-INFO: grad norm: 39.688 39.240 5.943
2024-12-02-07:11:07-root-INFO: grad norm: 19.931 19.640 3.389
2024-12-02-07:11:07-root-INFO: grad norm: 13.855 13.613 2.579
2024-12-02-07:11:08-root-INFO: grad norm: 11.656 11.471 2.064
2024-12-02-07:11:09-root-INFO: grad norm: 11.425 11.290 1.750
2024-12-02-07:11:09-root-INFO: grad norm: 10.645 10.535 1.523
2024-12-02-07:11:09-root-INFO: grad norm: 10.181 10.092 1.342
2024-12-02-07:11:10-root-INFO: grad norm: 10.068 9.995 1.216
2024-12-02-07:11:10-root-INFO: Loss Change: 252.291 -> 112.058
2024-12-02-07:11:10-root-INFO: Regularization Change: 0.000 -> 59.340
2024-12-02-07:11:10-root-INFO: Learning rate of xt decay: 0.15015 -> 0.15196.
2024-12-02-07:11:10-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00054.
2024-12-02-07:11:10-root-INFO: step: 79 lr_xt 0.09043348
2024-12-02-07:11:11-root-INFO: grad norm: 9.522 9.456 1.118
2024-12-02-07:11:11-root-INFO: grad norm: 9.315 9.266 0.957
2024-12-02-07:11:12-root-INFO: grad norm: 10.710 10.665 0.982
2024-12-02-07:11:12-root-INFO: grad norm: 8.704 8.659 0.885
2024-12-02-07:11:13-root-INFO: grad norm: 8.334 8.289 0.870
2024-12-02-07:11:13-root-INFO: grad norm: 8.132 8.086 0.868
2024-12-02-07:11:14-root-INFO: grad norm: 8.240 8.187 0.932
2024-12-02-07:11:14-root-INFO: grad norm: 9.442 9.385 1.032
2024-12-02-07:11:15-root-INFO: Loss Change: 111.462 -> 102.351
2024-12-02-07:11:15-root-INFO: Regularization Change: 0.000 -> 7.270
2024-12-02-07:11:15-root-INFO: Learning rate of xt decay: 0.15196 -> 0.15378.
2024-12-02-07:11:15-root-INFO: Coefficient of regularization decay: 0.00054 -> 0.00055.
2024-12-02-07:11:15-root-INFO: step: 78 lr_xt 0.09269861
2024-12-02-07:11:15-root-INFO: grad norm: 7.281 7.243 0.737
2024-12-02-07:11:15-root-INFO: grad norm: 6.851 6.817 0.689
2024-12-02-07:11:16-root-INFO: grad norm: 7.459 7.432 0.641
2024-12-02-07:11:16-root-INFO: Loss too large (99.688->99.717)! Learning rate decreased to 0.07416.
2024-12-02-07:11:16-root-INFO: grad norm: 5.656 5.622 0.626
2024-12-02-07:11:17-root-INFO: grad norm: 3.807 3.765 0.563
2024-12-02-07:11:17-root-INFO: grad norm: 3.685 3.643 0.553
2024-12-02-07:11:18-root-INFO: grad norm: 4.482 4.444 0.578
2024-12-02-07:11:18-root-INFO: grad norm: 4.289 4.245 0.613
2024-12-02-07:11:19-root-INFO: Loss Change: 102.088 -> 96.111
2024-12-02-07:11:19-root-INFO: Regularization Change: 0.000 -> 2.997
2024-12-02-07:11:19-root-INFO: Learning rate of xt decay: 0.15378 -> 0.15562.
2024-12-02-07:11:19-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00055.
2024-12-02-07:11:19-root-INFO: step: 77 lr_xt 0.09500525
2024-12-02-07:11:19-root-INFO: grad norm: 6.353 6.247 1.156
2024-12-02-07:11:19-root-INFO: Loss too large (96.103->96.319)! Learning rate decreased to 0.07600.
2024-12-02-07:11:20-root-INFO: grad norm: 4.709 4.652 0.731
2024-12-02-07:11:20-root-INFO: grad norm: 2.676 2.618 0.558
2024-12-02-07:11:21-root-INFO: grad norm: 2.378 2.333 0.463
2024-12-02-07:11:21-root-INFO: grad norm: 2.327 2.282 0.452
2024-12-02-07:11:21-root-INFO: grad norm: 2.435 2.393 0.453
2024-12-02-07:11:22-root-INFO: grad norm: 3.007 2.973 0.455
2024-12-02-07:11:22-root-INFO: grad norm: 3.612 3.574 0.522
2024-12-02-07:11:23-root-INFO: Loss Change: 96.103 -> 92.875
2024-12-02-07:11:23-root-INFO: Regularization Change: 0.000 -> 1.848
2024-12-02-07:11:23-root-INFO: Learning rate of xt decay: 0.15562 -> 0.15749.
2024-12-02-07:11:23-root-INFO: Coefficient of regularization decay: 0.00055 -> 0.00056.
2024-12-02-07:11:23-root-INFO: step: 76 lr_xt 0.09735366
2024-12-02-07:11:23-root-INFO: grad norm: 9.212 9.111 1.355
2024-12-02-07:11:23-root-INFO: Loss too large (93.445->94.173)! Learning rate decreased to 0.07788.
2024-12-02-07:11:23-root-INFO: Loss too large (93.445->93.629)! Learning rate decreased to 0.06231.
2024-12-02-07:11:24-root-INFO: grad norm: 4.336 4.291 0.622
2024-12-02-07:11:24-root-INFO: grad norm: 3.563 3.532 0.470
2024-12-02-07:11:25-root-INFO: grad norm: 2.616 2.581 0.429
2024-12-02-07:11:25-root-INFO: grad norm: 2.650 2.617 0.420
2024-12-02-07:11:26-root-INFO: grad norm: 2.803 2.771 0.417
2024-12-02-07:11:26-root-INFO: grad norm: 2.903 2.871 0.433
2024-12-02-07:11:27-root-INFO: grad norm: 3.127 3.098 0.425
2024-12-02-07:11:27-root-INFO: Loss Change: 93.445 -> 90.598
2024-12-02-07:11:27-root-INFO: Regularization Change: 0.000 -> 1.106
2024-12-02-07:11:27-root-INFO: Learning rate of xt decay: 0.15749 -> 0.15938.
2024-12-02-07:11:27-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00056.
2024-12-02-07:11:27-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-07:11:27-root-INFO: grad norm: 3.689 3.629 0.664
2024-12-02-07:11:28-root-INFO: grad norm: 6.408 6.385 0.540
2024-12-02-07:11:28-root-INFO: Loss too large (90.445->90.996)! Learning rate decreased to 0.07980.
2024-12-02-07:11:28-root-INFO: Loss too large (90.445->90.672)! Learning rate decreased to 0.06384.
2024-12-02-07:11:28-root-INFO: grad norm: 4.140 4.107 0.522
2024-12-02-07:11:29-root-INFO: grad norm: 2.006 1.967 0.392
2024-12-02-07:11:29-root-INFO: grad norm: 1.895 1.860 0.358
2024-12-02-07:11:30-root-INFO: grad norm: 1.887 1.853 0.358
2024-12-02-07:11:30-root-INFO: grad norm: 1.908 1.876 0.351
2024-12-02-07:11:31-root-INFO: grad norm: 1.951 1.919 0.355
2024-12-02-07:11:31-root-INFO: Loss Change: 90.643 -> 88.493
2024-12-02-07:11:31-root-INFO: Regularization Change: 0.000 -> 1.076
2024-12-02-07:11:31-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-07:11:31-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-07:11:31-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-07:11:32-root-INFO: grad norm: 4.070 3.987 0.818
2024-12-02-07:11:32-root-INFO: Loss too large (88.714->88.782)! Learning rate decreased to 0.08174.
2024-12-02-07:11:32-root-INFO: grad norm: 4.232 4.193 0.574
2024-12-02-07:11:33-root-INFO: grad norm: 6.290 6.252 0.687
2024-12-02-07:11:33-root-INFO: Loss too large (88.320->88.713)! Learning rate decreased to 0.06539.
2024-12-02-07:11:33-root-INFO: Loss too large (88.320->88.399)! Learning rate decreased to 0.05231.
2024-12-02-07:11:33-root-INFO: grad norm: 3.968 3.937 0.494
2024-12-02-07:11:34-root-INFO: grad norm: 1.808 1.775 0.347
2024-12-02-07:11:34-root-INFO: grad norm: 1.757 1.726 0.331
2024-12-02-07:11:35-root-INFO: grad norm: 1.737 1.706 0.327
2024-12-02-07:11:35-root-INFO: grad norm: 1.722 1.691 0.326
2024-12-02-07:11:36-root-INFO: Loss Change: 88.714 -> 86.972
2024-12-02-07:11:36-root-INFO: Regularization Change: 0.000 -> 0.735
2024-12-02-07:11:36-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-07:11:36-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-07:11:36-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-07:11:36-root-INFO: grad norm: 3.437 3.340 0.808
2024-12-02-07:11:37-root-INFO: grad norm: 4.269 4.224 0.621
2024-12-02-07:11:37-root-INFO: Loss too large (86.921->87.442)! Learning rate decreased to 0.08372.
2024-12-02-07:11:37-root-INFO: grad norm: 7.092 7.051 0.755
2024-12-02-07:11:37-root-INFO: Loss too large (86.841->87.362)! Learning rate decreased to 0.06698.
2024-12-02-07:11:38-root-INFO: Loss too large (86.841->87.001)! Learning rate decreased to 0.05358.
2024-12-02-07:11:38-root-INFO: grad norm: 4.108 4.078 0.503
2024-12-02-07:11:38-root-INFO: grad norm: 1.803 1.772 0.334
2024-12-02-07:11:39-root-INFO: grad norm: 1.719 1.690 0.317
2024-12-02-07:11:39-root-INFO: grad norm: 1.697 1.668 0.315
2024-12-02-07:11:40-root-INFO: grad norm: 1.680 1.651 0.310
2024-12-02-07:11:40-root-INFO: Loss Change: 87.132 -> 85.428
2024-12-02-07:11:40-root-INFO: Regularization Change: 0.000 -> 0.784
2024-12-02-07:11:40-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-07:11:40-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-07:11:41-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-07:11:41-root-INFO: grad norm: 3.223 3.132 0.760
2024-12-02-07:11:41-root-INFO: grad norm: 3.732 3.687 0.576
2024-12-02-07:11:41-root-INFO: Loss too large (85.217->85.553)! Learning rate decreased to 0.08574.
2024-12-02-07:11:42-root-INFO: grad norm: 6.205 6.168 0.679
2024-12-02-07:11:42-root-INFO: Loss too large (85.146->85.634)! Learning rate decreased to 0.06859.
2024-12-02-07:11:42-root-INFO: Loss too large (85.146->85.300)! Learning rate decreased to 0.05487.
2024-12-02-07:11:43-root-INFO: grad norm: 4.046 4.018 0.480
2024-12-02-07:11:43-root-INFO: grad norm: 1.737 1.706 0.324
2024-12-02-07:11:44-root-INFO: grad norm: 1.684 1.656 0.304
2024-12-02-07:11:44-root-INFO: grad norm: 1.666 1.639 0.300
2024-12-02-07:11:45-root-INFO: grad norm: 1.654 1.626 0.299
2024-12-02-07:11:45-root-INFO: Loss Change: 85.506 -> 83.879
2024-12-02-07:11:45-root-INFO: Regularization Change: 0.000 -> 0.767
2024-12-02-07:11:45-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-07:11:45-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-07:11:45-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-07:11:45-root-INFO: grad norm: 2.710 2.664 0.499
2024-12-02-07:11:46-root-INFO: grad norm: 3.835 3.801 0.513
2024-12-02-07:11:46-root-INFO: Loss too large (83.916->84.523)! Learning rate decreased to 0.08779.
2024-12-02-07:11:46-root-INFO: Loss too large (83.916->83.941)! Learning rate decreased to 0.07023.
2024-12-02-07:11:47-root-INFO: grad norm: 4.457 4.433 0.467
2024-12-02-07:11:47-root-INFO: Loss too large (83.661->83.734)! Learning rate decreased to 0.05618.
2024-12-02-07:11:47-root-INFO: grad norm: 3.572 3.547 0.419
2024-12-02-07:11:48-root-INFO: grad norm: 2.387 2.365 0.327
2024-12-02-07:11:48-root-INFO: grad norm: 2.284 2.261 0.323
2024-12-02-07:11:49-root-INFO: grad norm: 2.175 2.153 0.304
2024-12-02-07:11:49-root-INFO: grad norm: 2.121 2.098 0.311
2024-12-02-07:11:49-root-INFO: Loss Change: 84.043 -> 82.664
2024-12-02-07:11:49-root-INFO: Regularization Change: 0.000 -> 0.681
2024-12-02-07:11:49-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-07:11:49-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-07:11:50-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-07:11:50-root-INFO: grad norm: 5.098 4.983 1.077
2024-12-02-07:11:50-root-INFO: Loss too large (82.896->83.294)! Learning rate decreased to 0.08987.
2024-12-02-07:11:50-root-INFO: grad norm: 4.639 4.597 0.619
2024-12-02-07:11:51-root-INFO: grad norm: 5.042 4.999 0.659
2024-12-02-07:11:51-root-INFO: Loss too large (82.232->82.594)! Learning rate decreased to 0.07189.
2024-12-02-07:11:51-root-INFO: Loss too large (82.232->82.301)! Learning rate decreased to 0.05752.
2024-12-02-07:11:52-root-INFO: grad norm: 3.690 3.666 0.422
2024-12-02-07:11:52-root-INFO: grad norm: 2.184 2.158 0.332
2024-12-02-07:11:53-root-INFO: grad norm: 2.108 2.085 0.306
2024-12-02-07:11:53-root-INFO: grad norm: 2.049 2.027 0.302
2024-12-02-07:11:54-root-INFO: grad norm: 2.022 2.000 0.299
2024-12-02-07:11:54-root-INFO: Loss Change: 82.896 -> 81.136
2024-12-02-07:11:54-root-INFO: Regularization Change: 0.000 -> 0.731
2024-12-02-07:11:54-root-INFO: Undo step: 70
2024-12-02-07:11:54-root-INFO: Undo step: 71
2024-12-02-07:11:54-root-INFO: Undo step: 72
2024-12-02-07:11:54-root-INFO: Undo step: 73
2024-12-02-07:11:54-root-INFO: Undo step: 74
2024-12-02-07:11:54-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-07:11:54-root-INFO: grad norm: 39.522 39.274 4.416
2024-12-02-07:11:55-root-INFO: grad norm: 19.201 18.972 2.954
2024-12-02-07:11:55-root-INFO: grad norm: 12.440 12.245 2.197
2024-12-02-07:11:56-root-INFO: grad norm: 9.160 9.003 1.691
2024-12-02-07:11:56-root-INFO: grad norm: 7.416 7.278 1.425
2024-12-02-07:11:57-root-INFO: grad norm: 6.322 6.202 1.225
2024-12-02-07:11:58-root-INFO: grad norm: 5.537 5.432 1.078
2024-12-02-07:11:58-root-INFO: grad norm: 4.951 4.858 0.959
2024-12-02-07:11:58-root-INFO: Loss Change: 225.578 -> 102.795
2024-12-02-07:11:58-root-INFO: Regularization Change: 0.000 -> 61.556
2024-12-02-07:11:58-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-07:11:58-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-07:11:59-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-07:11:59-root-INFO: grad norm: 4.719 4.619 0.964
2024-12-02-07:11:59-root-INFO: grad norm: 4.231 4.152 0.815
2024-12-02-07:12:00-root-INFO: grad norm: 3.978 3.908 0.744
2024-12-02-07:12:00-root-INFO: grad norm: 3.816 3.751 0.702
2024-12-02-07:12:01-root-INFO: grad norm: 3.552 3.491 0.657
2024-12-02-07:12:01-root-INFO: grad norm: 3.121 3.062 0.600
2024-12-02-07:12:02-root-INFO: grad norm: 2.928 2.872 0.572
2024-12-02-07:12:02-root-INFO: grad norm: 2.780 2.726 0.544
2024-12-02-07:12:03-root-INFO: Loss Change: 102.743 -> 92.493
2024-12-02-07:12:03-root-INFO: Regularization Change: 0.000 -> 7.904
2024-12-02-07:12:03-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-07:12:03-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-07:12:03-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-07:12:03-root-INFO: grad norm: 3.810 3.707 0.879
2024-12-02-07:12:03-root-INFO: grad norm: 3.477 3.421 0.625
2024-12-02-07:12:04-root-INFO: grad norm: 3.738 3.683 0.641
2024-12-02-07:12:04-root-INFO: grad norm: 3.885 3.829 0.655
2024-12-02-07:12:05-root-INFO: grad norm: 4.508 4.455 0.687
2024-12-02-07:12:05-root-INFO: grad norm: 4.175 4.115 0.704
2024-12-02-07:12:06-root-INFO: grad norm: 3.427 3.369 0.631
2024-12-02-07:12:06-root-INFO: grad norm: 3.624 3.571 0.615
2024-12-02-07:12:06-root-INFO: Loss Change: 92.568 -> 88.020
2024-12-02-07:12:06-root-INFO: Regularization Change: 0.000 -> 3.885
2024-12-02-07:12:06-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-07:12:06-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-07:12:07-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-07:12:07-root-INFO: grad norm: 6.852 6.739 1.244
2024-12-02-07:12:07-root-INFO: grad norm: 5.270 5.185 0.938
2024-12-02-07:12:08-root-INFO: grad norm: 4.212 4.154 0.699
2024-12-02-07:12:08-root-INFO: grad norm: 3.665 3.629 0.514
2024-12-02-07:12:09-root-INFO: grad norm: 3.457 3.425 0.471
2024-12-02-07:12:09-root-INFO: grad norm: 3.823 3.800 0.419
2024-12-02-07:12:10-root-INFO: grad norm: 3.769 3.743 0.446
2024-12-02-07:12:10-root-INFO: grad norm: 4.011 3.987 0.437
2024-12-02-07:12:11-root-INFO: Loss Change: 88.387 -> 84.781
2024-12-02-07:12:11-root-INFO: Regularization Change: 0.000 -> 3.021
2024-12-02-07:12:11-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-07:12:11-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-07:12:11-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-07:12:11-root-INFO: grad norm: 4.028 4.001 0.465
2024-12-02-07:12:11-root-INFO: grad norm: 3.520 3.497 0.403
2024-12-02-07:12:12-root-INFO: grad norm: 3.598 3.571 0.439
2024-12-02-07:12:12-root-INFO: grad norm: 4.276 4.251 0.459
2024-12-02-07:12:13-root-INFO: Loss too large (83.447->83.512)! Learning rate decreased to 0.08779.
2024-12-02-07:12:13-root-INFO: grad norm: 3.445 3.415 0.458
2024-12-02-07:12:13-root-INFO: grad norm: 2.268 2.239 0.362
2024-12-02-07:12:14-root-INFO: grad norm: 2.334 2.307 0.357
2024-12-02-07:12:14-root-INFO: grad norm: 2.607 2.582 0.357
2024-12-02-07:12:15-root-INFO: Loss Change: 84.843 -> 82.031
2024-12-02-07:12:15-root-INFO: Regularization Change: 0.000 -> 1.849
2024-12-02-07:12:15-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-07:12:15-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-07:12:15-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-07:12:15-root-INFO: grad norm: 3.602 3.510 0.808
2024-12-02-07:12:16-root-INFO: grad norm: 3.540 3.518 0.395
2024-12-02-07:12:16-root-INFO: grad norm: 3.672 3.650 0.404
2024-12-02-07:12:17-root-INFO: grad norm: 4.911 4.890 0.453
2024-12-02-07:12:17-root-INFO: Loss too large (80.979->81.262)! Learning rate decreased to 0.08987.
2024-12-02-07:12:17-root-INFO: Loss too large (80.979->80.992)! Learning rate decreased to 0.07189.
2024-12-02-07:12:17-root-INFO: grad norm: 3.353 3.327 0.416
2024-12-02-07:12:18-root-INFO: grad norm: 1.800 1.773 0.312
2024-12-02-07:12:19-root-INFO: grad norm: 1.738 1.713 0.293
2024-12-02-07:12:19-root-INFO: grad norm: 1.710 1.685 0.290
2024-12-02-07:12:19-root-INFO: Loss Change: 81.969 -> 79.719
2024-12-02-07:12:19-root-INFO: Regularization Change: 0.000 -> 1.375
2024-12-02-07:12:19-root-INFO: Undo step: 70
2024-12-02-07:12:19-root-INFO: Undo step: 71
2024-12-02-07:12:19-root-INFO: Undo step: 72
2024-12-02-07:12:19-root-INFO: Undo step: 73
2024-12-02-07:12:19-root-INFO: Undo step: 74
2024-12-02-07:12:19-root-INFO: step: 75 lr_xt 0.09974414
2024-12-02-07:12:20-root-INFO: grad norm: 32.954 32.594 4.856
2024-12-02-07:12:20-root-INFO: grad norm: 20.044 19.781 3.240
2024-12-02-07:12:21-root-INFO: grad norm: 13.123 12.846 2.682
2024-12-02-07:12:21-root-INFO: grad norm: 9.970 9.775 1.961
2024-12-02-07:12:22-root-INFO: grad norm: 8.094 7.913 1.706
2024-12-02-07:12:22-root-INFO: grad norm: 7.008 6.863 1.416
2024-12-02-07:12:23-root-INFO: grad norm: 6.252 6.117 1.296
2024-12-02-07:12:23-root-INFO: grad norm: 5.704 5.593 1.122
2024-12-02-07:12:23-root-INFO: Loss Change: 222.851 -> 102.696
2024-12-02-07:12:23-root-INFO: Regularization Change: 0.000 -> 60.347
2024-12-02-07:12:23-root-INFO: Learning rate of xt decay: 0.15938 -> 0.16129.
2024-12-02-07:12:23-root-INFO: Coefficient of regularization decay: 0.00056 -> 0.00057.
2024-12-02-07:12:24-root-INFO: step: 74 lr_xt 0.10217692
2024-12-02-07:12:24-root-INFO: grad norm: 5.540 5.445 1.019
2024-12-02-07:12:24-root-INFO: grad norm: 5.250 5.151 1.014
2024-12-02-07:12:25-root-INFO: grad norm: 5.479 5.407 0.885
2024-12-02-07:12:25-root-INFO: grad norm: 5.153 5.063 0.959
2024-12-02-07:12:26-root-INFO: grad norm: 4.555 4.486 0.790
2024-12-02-07:12:26-root-INFO: grad norm: 4.645 4.565 0.858
2024-12-02-07:12:27-root-INFO: grad norm: 5.485 5.431 0.771
2024-12-02-07:12:27-root-INFO: grad norm: 4.892 4.814 0.870
2024-12-02-07:12:27-root-INFO: Loss Change: 102.540 -> 92.160
2024-12-02-07:12:27-root-INFO: Regularization Change: 0.000 -> 8.407
2024-12-02-07:12:27-root-INFO: Learning rate of xt decay: 0.16129 -> 0.16323.
2024-12-02-07:12:27-root-INFO: Coefficient of regularization decay: 0.00057 -> 0.00058.
2024-12-02-07:12:27-root-INFO: step: 73 lr_xt 0.10465226
2024-12-02-07:12:28-root-INFO: grad norm: 5.182 5.074 1.050
2024-12-02-07:12:28-root-INFO: grad norm: 4.959 4.874 0.911
2024-12-02-07:12:29-root-INFO: grad norm: 6.077 6.017 0.855
2024-12-02-07:12:29-root-INFO: grad norm: 5.022 4.936 0.922
2024-12-02-07:12:29-root-INFO: grad norm: 3.749 3.692 0.654
2024-12-02-07:12:30-root-INFO: grad norm: 3.265 3.207 0.612
2024-12-02-07:12:30-root-INFO: grad norm: 2.967 2.921 0.518
2024-12-02-07:12:31-root-INFO: grad norm: 2.863 2.811 0.544
2024-12-02-07:12:31-root-INFO: Loss Change: 92.280 -> 86.684
2024-12-02-07:12:31-root-INFO: Regularization Change: 0.000 -> 4.398
2024-12-02-07:12:31-root-INFO: Learning rate of xt decay: 0.16323 -> 0.16519.
2024-12-02-07:12:31-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00058.
2024-12-02-07:12:31-root-INFO: step: 72 lr_xt 0.10717038
2024-12-02-07:12:31-root-INFO: grad norm: 5.223 5.122 1.023
2024-12-02-07:12:32-root-INFO: grad norm: 5.227 5.148 0.909
2024-12-02-07:12:32-root-INFO: grad norm: 6.157 6.092 0.894
2024-12-02-07:12:33-root-INFO: Loss too large (85.948->86.137)! Learning rate decreased to 0.08574.
2024-12-02-07:12:33-root-INFO: grad norm: 4.431 4.368 0.745
2024-12-02-07:12:33-root-INFO: grad norm: 2.516 2.476 0.446
2024-12-02-07:12:34-root-INFO: grad norm: 2.208 2.170 0.411
2024-12-02-07:12:34-root-INFO: grad norm: 2.176 2.144 0.371
2024-12-02-07:12:35-root-INFO: grad norm: 2.239 2.204 0.395
2024-12-02-07:12:35-root-INFO: Loss Change: 86.909 -> 83.312
2024-12-02-07:12:35-root-INFO: Regularization Change: 0.000 -> 2.315
2024-12-02-07:12:35-root-INFO: Learning rate of xt decay: 0.16519 -> 0.16717.
2024-12-02-07:12:35-root-INFO: Coefficient of regularization decay: 0.00058 -> 0.00059.
2024-12-02-07:12:35-root-INFO: step: 71 lr_xt 0.10973151
2024-12-02-07:12:36-root-INFO: grad norm: 3.491 3.448 0.547
2024-12-02-07:12:36-root-INFO: grad norm: 3.978 3.932 0.608
2024-12-02-07:12:36-root-INFO: grad norm: 6.039 5.999 0.694
2024-12-02-07:12:37-root-INFO: Loss too large (82.934->83.363)! Learning rate decreased to 0.08779.
2024-12-02-07:12:37-root-INFO: Loss too large (82.934->82.983)! Learning rate decreased to 0.07023.
2024-12-02-07:12:37-root-INFO: grad norm: 3.779 3.739 0.547
2024-12-02-07:12:38-root-INFO: grad norm: 1.917 1.887 0.333
2024-12-02-07:12:38-root-INFO: grad norm: 1.812 1.783 0.322
2024-12-02-07:12:39-root-INFO: grad norm: 1.787 1.759 0.318
2024-12-02-07:12:39-root-INFO: grad norm: 1.768 1.740 0.315
2024-12-02-07:12:40-root-INFO: Loss Change: 83.429 -> 81.076
2024-12-02-07:12:40-root-INFO: Regularization Change: 0.000 -> 1.416
2024-12-02-07:12:40-root-INFO: Learning rate of xt decay: 0.16717 -> 0.16918.
2024-12-02-07:12:40-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00059.
2024-12-02-07:12:40-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-07:12:40-root-INFO: grad norm: 3.916 3.806 0.921
2024-12-02-07:12:40-root-INFO: grad norm: 4.058 4.005 0.655
2024-12-02-07:12:41-root-INFO: grad norm: 7.166 7.117 0.841
2024-12-02-07:12:41-root-INFO: Loss too large (80.742->81.423)! Learning rate decreased to 0.08987.
2024-12-02-07:12:41-root-INFO: Loss too large (80.742->80.928)! Learning rate decreased to 0.07189.
2024-12-02-07:12:42-root-INFO: grad norm: 3.942 3.902 0.561
2024-12-02-07:12:42-root-INFO: grad norm: 2.162 2.138 0.320
2024-12-02-07:12:43-root-INFO: grad norm: 1.912 1.888 0.299
2024-12-02-07:12:43-root-INFO: grad norm: 1.874 1.849 0.307
2024-12-02-07:12:44-root-INFO: grad norm: 1.851 1.828 0.292
2024-12-02-07:12:44-root-INFO: Loss Change: 81.144 -> 78.828
2024-12-02-07:12:44-root-INFO: Regularization Change: 0.000 -> 1.342
2024-12-02-07:12:44-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-07:12:44-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-07:12:44-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-07:12:44-root-INFO: grad norm: 2.345 2.303 0.444
2024-12-02-07:12:45-root-INFO: grad norm: 2.200 2.178 0.314
2024-12-02-07:12:45-root-INFO: grad norm: 2.604 2.582 0.338
2024-12-02-07:12:46-root-INFO: grad norm: 4.933 4.915 0.429
2024-12-02-07:12:46-root-INFO: Loss too large (78.206->78.671)! Learning rate decreased to 0.09199.
2024-12-02-07:12:46-root-INFO: Loss too large (78.206->78.376)! Learning rate decreased to 0.07359.
2024-12-02-07:12:47-root-INFO: grad norm: 3.653 3.624 0.459
2024-12-02-07:12:47-root-INFO: grad norm: 1.927 1.903 0.299
2024-12-02-07:12:48-root-INFO: grad norm: 1.885 1.861 0.298
2024-12-02-07:12:48-root-INFO: grad norm: 1.905 1.885 0.279
2024-12-02-07:12:48-root-INFO: Loss Change: 78.936 -> 77.016
2024-12-02-07:12:48-root-INFO: Regularization Change: 0.000 -> 1.335
2024-12-02-07:12:48-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-07:12:48-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-07:12:49-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-07:12:49-root-INFO: grad norm: 3.448 3.348 0.828
2024-12-02-07:12:49-root-INFO: grad norm: 3.038 2.993 0.523
2024-12-02-07:12:50-root-INFO: grad norm: 4.725 4.686 0.607
2024-12-02-07:12:50-root-INFO: Loss too large (76.407->76.880)! Learning rate decreased to 0.09414.
2024-12-02-07:12:50-root-INFO: Loss too large (76.407->76.526)! Learning rate decreased to 0.07531.
2024-12-02-07:12:50-root-INFO: grad norm: 3.685 3.655 0.469
2024-12-02-07:12:51-root-INFO: grad norm: 2.428 2.407 0.322
2024-12-02-07:12:51-root-INFO: grad norm: 2.431 2.409 0.326
2024-12-02-07:12:52-root-INFO: grad norm: 2.499 2.480 0.305
2024-12-02-07:12:52-root-INFO: grad norm: 2.531 2.509 0.330
2024-12-02-07:12:53-root-INFO: Loss Change: 77.036 -> 75.105
2024-12-02-07:12:53-root-INFO: Regularization Change: 0.000 -> 1.194
2024-12-02-07:12:53-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-07:12:53-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-07:12:53-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-07:12:53-root-INFO: grad norm: 3.726 3.665 0.672
2024-12-02-07:12:53-root-INFO: Loss too large (75.161->75.312)! Learning rate decreased to 0.09633.
2024-12-02-07:12:54-root-INFO: grad norm: 3.778 3.747 0.487
2024-12-02-07:12:54-root-INFO: grad norm: 4.771 4.743 0.521
2024-12-02-07:12:54-root-INFO: Loss too large (74.759->75.005)! Learning rate decreased to 0.07706.
2024-12-02-07:12:55-root-INFO: grad norm: 3.687 3.660 0.450
2024-12-02-07:12:55-root-INFO: grad norm: 2.035 2.013 0.300
2024-12-02-07:12:56-root-INFO: grad norm: 2.047 2.027 0.288
2024-12-02-07:12:56-root-INFO: grad norm: 2.179 2.161 0.279
2024-12-02-07:12:57-root-INFO: grad norm: 2.275 2.255 0.301
2024-12-02-07:12:57-root-INFO: Loss Change: 75.161 -> 73.533
2024-12-02-07:12:57-root-INFO: Regularization Change: 0.000 -> 0.965
2024-12-02-07:12:57-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-07:12:57-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-07:12:57-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-07:12:57-root-INFO: grad norm: 3.949 3.907 0.576
2024-12-02-07:12:57-root-INFO: Loss too large (73.711->74.027)! Learning rate decreased to 0.09855.
2024-12-02-07:12:58-root-INFO: Loss too large (73.711->73.757)! Learning rate decreased to 0.07884.
2024-12-02-07:12:58-root-INFO: grad norm: 3.380 3.357 0.392
2024-12-02-07:12:59-root-INFO: grad norm: 2.795 2.775 0.334
2024-12-02-07:12:59-root-INFO: grad norm: 2.792 2.772 0.339
2024-12-02-07:12:59-root-INFO: grad norm: 2.823 2.806 0.315
2024-12-02-07:13:00-root-INFO: grad norm: 2.829 2.809 0.342
2024-12-02-07:13:00-root-INFO: grad norm: 2.845 2.827 0.313
2024-12-02-07:13:01-root-INFO: grad norm: 2.845 2.825 0.342
2024-12-02-07:13:01-root-INFO: Loss Change: 73.711 -> 72.266
2024-12-02-07:13:01-root-INFO: Regularization Change: 0.000 -> 0.827
2024-12-02-07:13:01-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-07:13:01-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-07:13:01-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-07:13:01-root-INFO: grad norm: 4.996 4.922 0.853
2024-12-02-07:13:02-root-INFO: Loss too large (72.548->73.022)! Learning rate decreased to 0.10081.
2024-12-02-07:13:02-root-INFO: Loss too large (72.548->72.620)! Learning rate decreased to 0.08065.
2024-12-02-07:13:02-root-INFO: grad norm: 3.731 3.706 0.432
2024-12-02-07:13:03-root-INFO: grad norm: 2.425 2.402 0.331
2024-12-02-07:13:03-root-INFO: grad norm: 2.493 2.474 0.306
2024-12-02-07:13:04-root-INFO: grad norm: 2.722 2.705 0.303
2024-12-02-07:13:04-root-INFO: grad norm: 2.805 2.786 0.329
2024-12-02-07:13:05-root-INFO: grad norm: 2.958 2.941 0.313
2024-12-02-07:13:05-root-INFO: grad norm: 2.978 2.959 0.344
2024-12-02-07:13:05-root-INFO: Loss Change: 72.548 -> 70.907
2024-12-02-07:13:05-root-INFO: Regularization Change: 0.000 -> 0.860
2024-12-02-07:13:05-root-INFO: Undo step: 65
2024-12-02-07:13:05-root-INFO: Undo step: 66
2024-12-02-07:13:05-root-INFO: Undo step: 67
2024-12-02-07:13:05-root-INFO: Undo step: 68
2024-12-02-07:13:05-root-INFO: Undo step: 69
2024-12-02-07:13:06-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-07:13:06-root-INFO: grad norm: 33.194 32.912 4.315
2024-12-02-07:13:06-root-INFO: grad norm: 17.335 17.133 2.642
2024-12-02-07:13:07-root-INFO: grad norm: 10.856 10.693 1.874
2024-12-02-07:13:07-root-INFO: grad norm: 8.418 8.277 1.534
2024-12-02-07:13:08-root-INFO: grad norm: 7.364 7.250 1.290
2024-12-02-07:13:08-root-INFO: grad norm: 6.346 6.242 1.147
2024-12-02-07:13:09-root-INFO: grad norm: 5.353 5.259 1.003
2024-12-02-07:13:09-root-INFO: grad norm: 5.039 4.953 0.925
2024-12-02-07:13:10-root-INFO: Loss Change: 208.611 -> 91.644
2024-12-02-07:13:10-root-INFO: Regularization Change: 0.000 -> 63.586
2024-12-02-07:13:10-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-07:13:10-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-07:13:10-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-07:13:10-root-INFO: grad norm: 5.848 5.755 1.036
2024-12-02-07:13:10-root-INFO: grad norm: 5.151 5.070 0.908
2024-12-02-07:13:11-root-INFO: grad norm: 4.145 4.067 0.802
2024-12-02-07:13:11-root-INFO: grad norm: 4.046 3.976 0.751
2024-12-02-07:13:12-root-INFO: grad norm: 4.547 4.486 0.741
2024-12-02-07:13:12-root-INFO: grad norm: 4.419 4.356 0.740
2024-12-02-07:13:13-root-INFO: grad norm: 4.167 4.110 0.689
2024-12-02-07:13:13-root-INFO: grad norm: 4.171 4.114 0.691
2024-12-02-07:13:14-root-INFO: Loss Change: 91.810 -> 82.865
2024-12-02-07:13:14-root-INFO: Regularization Change: 0.000 -> 8.058
2024-12-02-07:13:14-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-07:13:14-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-07:13:14-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-07:13:14-root-INFO: grad norm: 7.000 6.870 1.342
2024-12-02-07:13:15-root-INFO: grad norm: 5.379 5.293 0.960
2024-12-02-07:13:15-root-INFO: grad norm: 3.832 3.770 0.687
2024-12-02-07:13:15-root-INFO: grad norm: 3.256 3.204 0.581
2024-12-02-07:13:16-root-INFO: grad norm: 2.892 2.846 0.513
2024-12-02-07:13:16-root-INFO: grad norm: 2.683 2.639 0.484
2024-12-02-07:13:17-root-INFO: grad norm: 2.571 2.530 0.458
2024-12-02-07:13:17-root-INFO: grad norm: 2.586 2.546 0.452
2024-12-02-07:13:18-root-INFO: Loss Change: 83.259 -> 77.708
2024-12-02-07:13:18-root-INFO: Regularization Change: 0.000 -> 4.449
2024-12-02-07:13:18-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-07:13:18-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-07:13:18-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-07:13:18-root-INFO: grad norm: 4.007 3.926 0.804
2024-12-02-07:13:18-root-INFO: grad norm: 4.174 4.121 0.665
2024-12-02-07:13:19-root-INFO: grad norm: 5.365 5.314 0.743
2024-12-02-07:13:19-root-INFO: Loss too large (77.008->77.166)! Learning rate decreased to 0.09633.
2024-12-02-07:13:20-root-INFO: grad norm: 3.968 3.925 0.583
2024-12-02-07:13:20-root-INFO: grad norm: 2.253 2.215 0.412
2024-12-02-07:13:21-root-INFO: grad norm: 2.081 2.050 0.361
2024-12-02-07:13:21-root-INFO: grad norm: 2.083 2.053 0.354
2024-12-02-07:13:22-root-INFO: grad norm: 2.120 2.092 0.346
2024-12-02-07:13:22-root-INFO: Loss Change: 77.769 -> 74.677
2024-12-02-07:13:22-root-INFO: Regularization Change: 0.000 -> 2.334
2024-12-02-07:13:22-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-07:13:22-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-07:13:22-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-07:13:22-root-INFO: grad norm: 3.375 3.321 0.597
2024-12-02-07:13:23-root-INFO: grad norm: 3.675 3.636 0.531
2024-12-02-07:13:23-root-INFO: grad norm: 4.857 4.818 0.616
2024-12-02-07:13:23-root-INFO: Loss too large (74.268->74.495)! Learning rate decreased to 0.09855.
2024-12-02-07:13:24-root-INFO: grad norm: 3.746 3.710 0.515
2024-12-02-07:13:24-root-INFO: grad norm: 2.166 2.134 0.374
2024-12-02-07:13:25-root-INFO: grad norm: 2.081 2.054 0.332
2024-12-02-07:13:25-root-INFO: grad norm: 2.156 2.130 0.331
2024-12-02-07:13:26-root-INFO: grad norm: 2.234 2.210 0.327
2024-12-02-07:13:26-root-INFO: Loss Change: 74.816 -> 72.385
2024-12-02-07:13:26-root-INFO: Regularization Change: 0.000 -> 1.940
2024-12-02-07:13:26-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-07:13:26-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-07:13:26-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-07:13:27-root-INFO: grad norm: 4.263 4.176 0.856
2024-12-02-07:13:27-root-INFO: grad norm: 4.228 4.178 0.646
2024-12-02-07:13:28-root-INFO: grad norm: 4.698 4.649 0.672
2024-12-02-07:13:28-root-INFO: Loss too large (71.967->72.201)! Learning rate decreased to 0.10081.
2024-12-02-07:13:28-root-INFO: grad norm: 3.758 3.721 0.523
2024-12-02-07:13:29-root-INFO: grad norm: 2.477 2.448 0.383
2024-12-02-07:13:29-root-INFO: grad norm: 2.457 2.433 0.342
2024-12-02-07:13:30-root-INFO: grad norm: 2.626 2.603 0.346
2024-12-02-07:13:30-root-INFO: grad norm: 2.695 2.673 0.344
2024-12-02-07:13:30-root-INFO: Loss Change: 72.606 -> 70.298
2024-12-02-07:13:30-root-INFO: Regularization Change: 0.000 -> 1.764
2024-12-02-07:13:30-root-INFO: Undo step: 65
2024-12-02-07:13:30-root-INFO: Undo step: 66
2024-12-02-07:13:30-root-INFO: Undo step: 67
2024-12-02-07:13:30-root-INFO: Undo step: 68
2024-12-02-07:13:30-root-INFO: Undo step: 69
2024-12-02-07:13:31-root-INFO: step: 70 lr_xt 0.11233583
2024-12-02-07:13:31-root-INFO: grad norm: 32.712 32.396 4.533
2024-12-02-07:13:31-root-INFO: grad norm: 16.707 16.333 3.512
2024-12-02-07:13:32-root-INFO: grad norm: 10.626 10.463 1.858
2024-12-02-07:13:32-root-INFO: grad norm: 8.089 7.946 1.513
2024-12-02-07:13:33-root-INFO: grad norm: 6.613 6.493 1.252
2024-12-02-07:13:33-root-INFO: grad norm: 5.632 5.526 1.090
2024-12-02-07:13:34-root-INFO: grad norm: 4.931 4.837 0.957
2024-12-02-07:13:34-root-INFO: grad norm: 4.406 4.321 0.859
2024-12-02-07:13:34-root-INFO: Loss Change: 210.877 -> 89.725
2024-12-02-07:13:34-root-INFO: Regularization Change: 0.000 -> 69.279
2024-12-02-07:13:34-root-INFO: Learning rate of xt decay: 0.16918 -> 0.17121.
2024-12-02-07:13:34-root-INFO: Coefficient of regularization decay: 0.00059 -> 0.00060.
2024-12-02-07:13:35-root-INFO: step: 69 lr_xt 0.11498353
2024-12-02-07:13:35-root-INFO: grad norm: 4.246 4.160 0.847
2024-12-02-07:13:35-root-INFO: grad norm: 3.787 3.714 0.738
2024-12-02-07:13:36-root-INFO: grad norm: 3.487 3.421 0.679
2024-12-02-07:13:36-root-INFO: grad norm: 3.265 3.202 0.636
2024-12-02-07:13:37-root-INFO: grad norm: 3.111 3.053 0.596
2024-12-02-07:13:37-root-INFO: grad norm: 3.019 2.964 0.573
2024-12-02-07:13:38-root-INFO: grad norm: 3.050 2.999 0.553
2024-12-02-07:13:38-root-INFO: grad norm: 3.133 3.084 0.551
2024-12-02-07:13:39-root-INFO: Loss Change: 89.728 -> 80.776
2024-12-02-07:13:39-root-INFO: Regularization Change: 0.000 -> 8.222
2024-12-02-07:13:39-root-INFO: Learning rate of xt decay: 0.17121 -> 0.17326.
2024-12-02-07:13:39-root-INFO: Coefficient of regularization decay: 0.00060 -> 0.00061.
2024-12-02-07:13:39-root-INFO: step: 68 lr_xt 0.11767478
2024-12-02-07:13:39-root-INFO: grad norm: 6.165 6.030 1.283
2024-12-02-07:13:39-root-INFO: grad norm: 5.239 5.156 0.930
2024-12-02-07:13:40-root-INFO: grad norm: 3.990 3.920 0.744
2024-12-02-07:13:40-root-INFO: grad norm: 3.891 3.833 0.665
2024-12-02-07:13:41-root-INFO: grad norm: 4.268 4.215 0.671
2024-12-02-07:13:41-root-INFO: grad norm: 4.278 4.224 0.674
2024-12-02-07:13:42-root-INFO: grad norm: 4.336 4.284 0.667
2024-12-02-07:13:42-root-INFO: grad norm: 4.339 4.289 0.656
2024-12-02-07:13:43-root-INFO: Loss Change: 81.020 -> 76.066
2024-12-02-07:13:43-root-INFO: Regularization Change: 0.000 -> 4.321
2024-12-02-07:13:43-root-INFO: Learning rate of xt decay: 0.17326 -> 0.17534.
2024-12-02-07:13:43-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00061.
2024-12-02-07:13:43-root-INFO: step: 67 lr_xt 0.12040972
2024-12-02-07:13:43-root-INFO: grad norm: 5.622 5.524 1.049
2024-12-02-07:13:44-root-INFO: grad norm: 5.254 5.188 0.834
2024-12-02-07:13:44-root-INFO: grad norm: 4.776 4.713 0.768
2024-12-02-07:13:45-root-INFO: grad norm: 4.678 4.624 0.711
2024-12-02-07:13:45-root-INFO: grad norm: 4.676 4.624 0.701
2024-12-02-07:13:46-root-INFO: grad norm: 4.549 4.498 0.676
2024-12-02-07:13:46-root-INFO: grad norm: 4.327 4.277 0.654
2024-12-02-07:13:46-root-INFO: grad norm: 4.293 4.247 0.630
2024-12-02-07:13:47-root-INFO: Loss Change: 76.251 -> 72.788
2024-12-02-07:13:47-root-INFO: Regularization Change: 0.000 -> 3.056
2024-12-02-07:13:47-root-INFO: Learning rate of xt decay: 0.17534 -> 0.17745.
2024-12-02-07:13:47-root-INFO: Coefficient of regularization decay: 0.00061 -> 0.00062.
2024-12-02-07:13:47-root-INFO: step: 66 lr_xt 0.12318848
2024-12-02-07:13:47-root-INFO: grad norm: 5.684 5.602 0.960
2024-12-02-07:13:48-root-INFO: grad norm: 5.014 4.949 0.800
2024-12-02-07:13:48-root-INFO: grad norm: 3.988 3.931 0.670
2024-12-02-07:13:49-root-INFO: grad norm: 3.925 3.878 0.606
2024-12-02-07:13:49-root-INFO: grad norm: 4.185 4.139 0.614
2024-12-02-07:13:50-root-INFO: grad norm: 4.175 4.130 0.614
2024-12-02-07:13:50-root-INFO: grad norm: 4.186 4.141 0.612
2024-12-02-07:13:51-root-INFO: grad norm: 4.149 4.105 0.605
2024-12-02-07:13:51-root-INFO: Loss Change: 73.099 -> 70.226
2024-12-02-07:13:51-root-INFO: Regularization Change: 0.000 -> 2.486
2024-12-02-07:13:51-root-INFO: Learning rate of xt decay: 0.17745 -> 0.17957.
2024-12-02-07:13:51-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00062.
2024-12-02-07:13:51-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-07:13:51-root-INFO: grad norm: 6.194 6.074 1.214
2024-12-02-07:13:52-root-INFO: grad norm: 5.389 5.313 0.903
2024-12-02-07:13:52-root-INFO: grad norm: 4.278 4.215 0.731
2024-12-02-07:13:53-root-INFO: grad norm: 4.174 4.120 0.666
2024-12-02-07:13:53-root-INFO: grad norm: 4.456 4.407 0.658
2024-12-02-07:13:53-root-INFO: Loss too large (69.030->69.035)! Learning rate decreased to 0.10081.
2024-12-02-07:13:54-root-INFO: grad norm: 3.348 3.314 0.480
2024-12-02-07:13:54-root-INFO: grad norm: 2.343 2.316 0.357
2024-12-02-07:13:55-root-INFO: grad norm: 2.077 2.055 0.302
2024-12-02-07:13:55-root-INFO: Loss Change: 70.696 -> 67.788
2024-12-02-07:13:55-root-INFO: Regularization Change: 0.000 -> 1.875
2024-12-02-07:13:55-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-07:13:55-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-07:13:55-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-07:13:55-root-INFO: grad norm: 2.965 2.904 0.600
2024-12-02-07:13:56-root-INFO: grad norm: 3.113 3.076 0.477
2024-12-02-07:13:56-root-INFO: grad norm: 3.980 3.942 0.544
2024-12-02-07:13:57-root-INFO: Loss too large (67.389->67.495)! Learning rate decreased to 0.10310.
2024-12-02-07:13:57-root-INFO: grad norm: 3.253 3.223 0.441
2024-12-02-07:13:57-root-INFO: grad norm: 2.328 2.302 0.346
2024-12-02-07:13:58-root-INFO: grad norm: 2.170 2.149 0.302
2024-12-02-07:13:58-root-INFO: grad norm: 2.039 2.018 0.292
2024-12-02-07:13:59-root-INFO: grad norm: 1.981 1.962 0.276
2024-12-02-07:13:59-root-INFO: Loss Change: 67.811 -> 65.966
2024-12-02-07:13:59-root-INFO: Regularization Change: 0.000 -> 1.495
2024-12-02-07:13:59-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-07:13:59-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-07:13:59-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-07:14:00-root-INFO: grad norm: 3.105 3.037 0.646
2024-12-02-07:14:00-root-INFO: grad norm: 3.271 3.233 0.492
2024-12-02-07:14:01-root-INFO: grad norm: 4.261 4.222 0.575
2024-12-02-07:14:01-root-INFO: Loss too large (65.787->65.991)! Learning rate decreased to 0.10543.
2024-12-02-07:14:01-root-INFO: grad norm: 3.417 3.386 0.463
2024-12-02-07:14:02-root-INFO: grad norm: 2.278 2.251 0.345
2024-12-02-07:14:02-root-INFO: grad norm: 2.124 2.103 0.295
2024-12-02-07:14:03-root-INFO: grad norm: 2.014 1.994 0.284
2024-12-02-07:14:03-root-INFO: grad norm: 1.966 1.948 0.270
2024-12-02-07:14:03-root-INFO: Loss Change: 66.159 -> 64.387
2024-12-02-07:14:03-root-INFO: Regularization Change: 0.000 -> 1.440
2024-12-02-07:14:03-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-07:14:03-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-07:14:03-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-07:14:04-root-INFO: grad norm: 3.761 3.664 0.850
2024-12-02-07:14:04-root-INFO: grad norm: 3.858 3.809 0.615
2024-12-02-07:14:05-root-INFO: grad norm: 4.922 4.875 0.679
2024-12-02-07:14:05-root-INFO: Loss too large (64.034->64.403)! Learning rate decreased to 0.10779.
2024-12-02-07:14:05-root-INFO: grad norm: 3.787 3.750 0.533
2024-12-02-07:14:06-root-INFO: grad norm: 2.257 2.229 0.355
2024-12-02-07:14:06-root-INFO: grad norm: 2.081 2.060 0.297
2024-12-02-07:14:07-root-INFO: grad norm: 2.004 1.984 0.281
2024-12-02-07:14:07-root-INFO: grad norm: 1.983 1.964 0.271
2024-12-02-07:14:07-root-INFO: Loss Change: 64.401 -> 62.541
2024-12-02-07:14:07-root-INFO: Regularization Change: 0.000 -> 1.440
2024-12-02-07:14:07-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-07:14:07-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-07:14:08-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-07:14:08-root-INFO: grad norm: 2.932 2.888 0.501
2024-12-02-07:14:08-root-INFO: grad norm: 3.295 3.262 0.461
2024-12-02-07:14:09-root-INFO: grad norm: 4.389 4.355 0.549
2024-12-02-07:14:09-root-INFO: Loss too large (62.410->62.720)! Learning rate decreased to 0.11019.
2024-12-02-07:14:09-root-INFO: grad norm: 3.528 3.497 0.470
2024-12-02-07:14:10-root-INFO: grad norm: 2.297 2.273 0.336
2024-12-02-07:14:10-root-INFO: grad norm: 2.142 2.122 0.294
2024-12-02-07:14:11-root-INFO: grad norm: 2.039 2.020 0.275
2024-12-02-07:14:11-root-INFO: grad norm: 1.998 1.980 0.268
2024-12-02-07:14:11-root-INFO: Loss Change: 62.653 -> 61.078
2024-12-02-07:14:11-root-INFO: Regularization Change: 0.000 -> 1.361
2024-12-02-07:14:11-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-07:14:11-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-07:14:12-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-07:14:12-root-INFO: grad norm: 3.315 3.242 0.692
2024-12-02-07:14:12-root-INFO: grad norm: 3.627 3.585 0.547
2024-12-02-07:14:13-root-INFO: grad norm: 4.969 4.928 0.638
2024-12-02-07:14:13-root-INFO: Loss too large (61.075->61.555)! Learning rate decreased to 0.11263.
2024-12-02-07:14:13-root-INFO: grad norm: 3.837 3.801 0.528
2024-12-02-07:14:14-root-INFO: grad norm: 2.214 2.188 0.340
2024-12-02-07:14:14-root-INFO: grad norm: 2.047 2.027 0.289
2024-12-02-07:14:15-root-INFO: grad norm: 1.991 1.973 0.269
2024-12-02-07:14:15-root-INFO: grad norm: 1.993 1.975 0.265
2024-12-02-07:14:16-root-INFO: Loss Change: 61.276 -> 59.661
2024-12-02-07:14:16-root-INFO: Regularization Change: 0.000 -> 1.358
2024-12-02-07:14:16-root-INFO: Undo step: 60
2024-12-02-07:14:16-root-INFO: Undo step: 61
2024-12-02-07:14:16-root-INFO: Undo step: 62
2024-12-02-07:14:16-root-INFO: Undo step: 63
2024-12-02-07:14:16-root-INFO: Undo step: 64
2024-12-02-07:14:16-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-07:14:16-root-INFO: grad norm: 27.692 27.491 3.330
2024-12-02-07:14:17-root-INFO: grad norm: 14.547 14.391 2.124
2024-12-02-07:14:17-root-INFO: grad norm: 10.573 10.441 1.663
2024-12-02-07:14:18-root-INFO: grad norm: 8.066 7.948 1.376
2024-12-02-07:14:18-root-INFO: grad norm: 6.186 6.085 1.111
2024-12-02-07:14:18-root-INFO: grad norm: 5.363 5.275 0.965
2024-12-02-07:14:19-root-INFO: grad norm: 4.902 4.825 0.864
2024-12-02-07:14:19-root-INFO: grad norm: 4.712 4.642 0.810
2024-12-02-07:14:20-root-INFO: Loss Change: 193.186 -> 78.677
2024-12-02-07:14:20-root-INFO: Regularization Change: 0.000 -> 73.665
2024-12-02-07:14:20-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-07:14:20-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-07:14:20-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-07:14:20-root-INFO: grad norm: 5.354 5.272 0.930
2024-12-02-07:14:21-root-INFO: grad norm: 5.433 5.357 0.907
2024-12-02-07:14:21-root-INFO: grad norm: 5.825 5.754 0.908
2024-12-02-07:14:21-root-INFO: grad norm: 6.047 5.966 0.985
2024-12-02-07:14:22-root-INFO: grad norm: 6.180 6.096 1.018
2024-12-02-07:14:22-root-INFO: grad norm: 6.649 6.565 1.055
2024-12-02-07:14:23-root-INFO: grad norm: 6.883 6.781 1.183
2024-12-02-07:14:23-root-INFO: grad norm: 6.960 6.870 1.113
2024-12-02-07:14:24-root-INFO: Loss Change: 78.626 -> 71.742
2024-12-02-07:14:24-root-INFO: Regularization Change: 0.000 -> 8.165
2024-12-02-07:14:24-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-07:14:24-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-07:14:24-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-07:14:24-root-INFO: grad norm: 7.609 7.467 1.460
2024-12-02-07:14:24-root-INFO: grad norm: 7.231 7.132 1.193
2024-12-02-07:14:25-root-INFO: grad norm: 6.672 6.559 1.225
2024-12-02-07:14:25-root-INFO: grad norm: 6.377 6.288 1.062
2024-12-02-07:14:26-root-INFO: grad norm: 6.206 6.110 1.091
2024-12-02-07:14:26-root-INFO: grad norm: 5.934 5.851 0.993
2024-12-02-07:14:27-root-INFO: grad norm: 5.700 5.617 0.969
2024-12-02-07:14:27-root-INFO: grad norm: 5.494 5.416 0.923
2024-12-02-07:14:28-root-INFO: Loss Change: 72.034 -> 67.167
2024-12-02-07:14:28-root-INFO: Regularization Change: 0.000 -> 4.252
2024-12-02-07:14:28-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-07:14:28-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-07:14:28-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-07:14:28-root-INFO: grad norm: 6.909 6.765 1.400
2024-12-02-07:14:28-root-INFO: grad norm: 6.145 6.041 1.123
2024-12-02-07:14:29-root-INFO: grad norm: 5.103 5.025 0.887
2024-12-02-07:14:29-root-INFO: grad norm: 4.917 4.840 0.866
2024-12-02-07:14:30-root-INFO: grad norm: 4.974 4.911 0.790
2024-12-02-07:14:30-root-INFO: grad norm: 4.857 4.786 0.831
2024-12-02-07:14:31-root-INFO: grad norm: 4.740 4.679 0.753
2024-12-02-07:14:31-root-INFO: grad norm: 4.672 4.604 0.793
2024-12-02-07:14:32-root-INFO: Loss Change: 67.356 -> 63.740
2024-12-02-07:14:32-root-INFO: Regularization Change: 0.000 -> 3.104
2024-12-02-07:14:32-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-07:14:32-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-07:14:32-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-07:14:32-root-INFO: grad norm: 5.402 5.319 0.944
2024-12-02-07:14:33-root-INFO: grad norm: 4.974 4.901 0.850
2024-12-02-07:14:33-root-INFO: grad norm: 4.362 4.304 0.712
2024-12-02-07:14:34-root-INFO: grad norm: 4.303 4.242 0.722
2024-12-02-07:14:34-root-INFO: grad norm: 4.356 4.305 0.664
2024-12-02-07:14:35-root-INFO: grad norm: 4.327 4.268 0.710
2024-12-02-07:14:35-root-INFO: grad norm: 4.300 4.250 0.656
2024-12-02-07:14:36-root-INFO: grad norm: 4.287 4.230 0.700
2024-12-02-07:14:36-root-INFO: Loss Change: 63.938 -> 61.362
2024-12-02-07:14:36-root-INFO: Regularization Change: 0.000 -> 2.514
2024-12-02-07:14:36-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-07:14:36-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-07:14:36-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-07:14:36-root-INFO: grad norm: 5.606 5.501 1.079
2024-12-02-07:14:37-root-INFO: grad norm: 5.116 5.039 0.883
2024-12-02-07:14:37-root-INFO: grad norm: 4.407 4.349 0.717
2024-12-02-07:14:38-root-INFO: grad norm: 4.358 4.299 0.717
2024-12-02-07:14:38-root-INFO: grad norm: 4.444 4.394 0.664
2024-12-02-07:14:39-root-INFO: grad norm: 4.403 4.346 0.704
2024-12-02-07:14:39-root-INFO: grad norm: 4.324 4.274 0.655
2024-12-02-07:14:40-root-INFO: grad norm: 4.322 4.267 0.689
2024-12-02-07:14:40-root-INFO: Loss Change: 61.726 -> 59.395
2024-12-02-07:14:40-root-INFO: Regularization Change: 0.000 -> 2.241
2024-12-02-07:14:40-root-INFO: Undo step: 60
2024-12-02-07:14:40-root-INFO: Undo step: 61
2024-12-02-07:14:40-root-INFO: Undo step: 62
2024-12-02-07:14:40-root-INFO: Undo step: 63
2024-12-02-07:14:40-root-INFO: Undo step: 64
2024-12-02-07:14:40-root-INFO: step: 65 lr_xt 0.12601118
2024-12-02-07:14:40-root-INFO: grad norm: 28.578 28.312 3.887
2024-12-02-07:14:41-root-INFO: grad norm: 15.377 15.202 2.309
2024-12-02-07:14:41-root-INFO: grad norm: 11.440 11.215 2.256
2024-12-02-07:14:42-root-INFO: grad norm: 8.934 8.816 1.448
2024-12-02-07:14:42-root-INFO: grad norm: 7.775 7.618 1.555
2024-12-02-07:14:43-root-INFO: grad norm: 6.859 6.762 1.148
2024-12-02-07:14:43-root-INFO: grad norm: 6.341 6.221 1.228
2024-12-02-07:14:44-root-INFO: grad norm: 5.884 5.803 0.978
2024-12-02-07:14:44-root-INFO: Loss Change: 188.607 -> 79.379
2024-12-02-07:14:44-root-INFO: Regularization Change: 0.000 -> 68.668
2024-12-02-07:14:44-root-INFO: Learning rate of xt decay: 0.17957 -> 0.18173.
2024-12-02-07:14:44-root-INFO: Coefficient of regularization decay: 0.00062 -> 0.00063.
2024-12-02-07:14:44-root-INFO: step: 64 lr_xt 0.12887791
2024-12-02-07:14:44-root-INFO: grad norm: 6.493 6.399 1.101
2024-12-02-07:14:45-root-INFO: grad norm: 5.809 5.736 0.919
2024-12-02-07:14:45-root-INFO: grad norm: 4.902 4.826 0.861
2024-12-02-07:14:46-root-INFO: grad norm: 4.714 4.650 0.773
2024-12-02-07:14:46-root-INFO: grad norm: 5.211 5.157 0.751
2024-12-02-07:14:47-root-INFO: grad norm: 4.890 4.832 0.751
2024-12-02-07:14:47-root-INFO: grad norm: 4.270 4.218 0.662
2024-12-02-07:14:48-root-INFO: grad norm: 4.284 4.232 0.666
2024-12-02-07:14:48-root-INFO: Loss Change: 79.322 -> 70.242
2024-12-02-07:14:48-root-INFO: Regularization Change: 0.000 -> 8.958
2024-12-02-07:14:48-root-INFO: Learning rate of xt decay: 0.18173 -> 0.18391.
2024-12-02-07:14:48-root-INFO: Coefficient of regularization decay: 0.00063 -> 0.00064.
2024-12-02-07:14:48-root-INFO: step: 63 lr_xt 0.13178874
2024-12-02-07:14:49-root-INFO: grad norm: 5.592 5.517 0.913
2024-12-02-07:14:49-root-INFO: grad norm: 4.786 4.726 0.757
2024-12-02-07:14:50-root-INFO: grad norm: 3.653 3.602 0.607
2024-12-02-07:14:50-root-INFO: grad norm: 3.508 3.461 0.572
2024-12-02-07:14:51-root-INFO: grad norm: 3.838 3.801 0.532
2024-12-02-07:14:51-root-INFO: grad norm: 3.972 3.928 0.587
2024-12-02-07:14:51-root-INFO: grad norm: 4.279 4.244 0.546
2024-12-02-07:14:52-root-INFO: grad norm: 4.168 4.124 0.602
2024-12-02-07:14:52-root-INFO: Loss Change: 70.432 -> 65.664
2024-12-02-07:14:52-root-INFO: Regularization Change: 0.000 -> 4.726
2024-12-02-07:14:52-root-INFO: Learning rate of xt decay: 0.18391 -> 0.18612.
2024-12-02-07:14:52-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00064.
2024-12-02-07:14:52-root-INFO: step: 62 lr_xt 0.13474373
2024-12-02-07:14:53-root-INFO: grad norm: 5.360 5.253 1.066
2024-12-02-07:14:53-root-INFO: grad norm: 4.744 4.678 0.789
2024-12-02-07:14:54-root-INFO: grad norm: 4.028 3.981 0.618
2024-12-02-07:14:54-root-INFO: grad norm: 3.982 3.934 0.617
2024-12-02-07:14:54-root-INFO: grad norm: 4.173 4.137 0.547
2024-12-02-07:14:55-root-INFO: grad norm: 4.055 4.011 0.592
2024-12-02-07:14:55-root-INFO: grad norm: 3.765 3.731 0.508
2024-12-02-07:14:56-root-INFO: grad norm: 3.797 3.758 0.545
2024-12-02-07:14:56-root-INFO: Loss Change: 65.689 -> 62.201
2024-12-02-07:14:56-root-INFO: Regularization Change: 0.000 -> 3.389
2024-12-02-07:14:56-root-INFO: Learning rate of xt decay: 0.18612 -> 0.18835.
2024-12-02-07:14:56-root-INFO: Coefficient of regularization decay: 0.00064 -> 0.00065.
2024-12-02-07:14:56-root-INFO: step: 61 lr_xt 0.13774291
2024-12-02-07:14:57-root-INFO: grad norm: 4.616 4.559 0.721
2024-12-02-07:14:57-root-INFO: grad norm: 4.183 4.137 0.622
2024-12-02-07:14:57-root-INFO: grad norm: 3.517 3.479 0.509
2024-12-02-07:14:58-root-INFO: grad norm: 3.522 3.485 0.508
2024-12-02-07:14:58-root-INFO: grad norm: 3.712 3.682 0.468
2024-12-02-07:14:59-root-INFO: grad norm: 3.711 3.675 0.511
2024-12-02-07:14:59-root-INFO: grad norm: 3.671 3.642 0.462
2024-12-02-07:15:00-root-INFO: grad norm: 3.673 3.639 0.502
2024-12-02-07:15:00-root-INFO: Loss Change: 62.316 -> 59.705
2024-12-02-07:15:00-root-INFO: Regularization Change: 0.000 -> 2.688
2024-12-02-07:15:00-root-INFO: Learning rate of xt decay: 0.18835 -> 0.19061.
2024-12-02-07:15:00-root-INFO: Coefficient of regularization decay: 0.00065 -> 0.00066.
2024-12-02-07:15:00-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-07:15:01-root-INFO: grad norm: 4.794 4.714 0.872
2024-12-02-07:15:01-root-INFO: grad norm: 4.351 4.299 0.667
2024-12-02-07:15:02-root-INFO: grad norm: 3.790 3.751 0.540
2024-12-02-07:15:02-root-INFO: grad norm: 3.754 3.715 0.534
2024-12-02-07:15:03-root-INFO: grad norm: 3.837 3.807 0.481
2024-12-02-07:15:03-root-INFO: grad norm: 3.792 3.757 0.514
2024-12-02-07:15:04-root-INFO: grad norm: 3.690 3.662 0.461
2024-12-02-07:15:04-root-INFO: grad norm: 3.686 3.653 0.493
2024-12-02-07:15:04-root-INFO: Loss Change: 59.961 -> 57.632
2024-12-02-07:15:04-root-INFO: Regularization Change: 0.000 -> 2.341
2024-12-02-07:15:04-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-07:15:04-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-07:15:04-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-07:15:05-root-INFO: grad norm: 4.837 4.756 0.881
2024-12-02-07:15:05-root-INFO: grad norm: 4.326 4.278 0.642
2024-12-02-07:15:06-root-INFO: grad norm: 3.678 3.640 0.526
2024-12-02-07:15:06-root-INFO: grad norm: 3.664 3.629 0.505
2024-12-02-07:15:06-root-INFO: grad norm: 3.798 3.769 0.473
2024-12-02-07:15:07-root-INFO: grad norm: 3.789 3.756 0.498
2024-12-02-07:15:07-root-INFO: grad norm: 3.744 3.715 0.465
2024-12-02-07:15:08-root-INFO: grad norm: 3.755 3.723 0.490
2024-12-02-07:15:08-root-INFO: Loss Change: 57.815 -> 55.722
2024-12-02-07:15:08-root-INFO: Regularization Change: 0.000 -> 2.122
2024-12-02-07:15:08-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-07:15:08-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-07:15:08-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-07:15:08-root-INFO: grad norm: 4.734 4.662 0.821
2024-12-02-07:15:09-root-INFO: grad norm: 4.333 4.286 0.641
2024-12-02-07:15:09-root-INFO: grad norm: 3.860 3.822 0.542
2024-12-02-07:15:10-root-INFO: grad norm: 3.841 3.804 0.532
2024-12-02-07:15:10-root-INFO: grad norm: 3.897 3.864 0.507
2024-12-02-07:15:11-root-INFO: grad norm: 3.915 3.879 0.523
2024-12-02-07:15:11-root-INFO: grad norm: 3.905 3.871 0.514
2024-12-02-07:15:12-root-INFO: grad norm: 3.943 3.908 0.523
2024-12-02-07:15:12-root-INFO: Loss Change: 55.788 -> 53.928
2024-12-02-07:15:12-root-INFO: Regularization Change: 0.000 -> 1.981
2024-12-02-07:15:12-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-07:15:12-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-07:15:12-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-07:15:12-root-INFO: grad norm: 5.047 4.974 0.857
2024-12-02-07:15:13-root-INFO: grad norm: 4.576 4.526 0.674
2024-12-02-07:15:13-root-INFO: grad norm: 3.990 3.948 0.582
2024-12-02-07:15:14-root-INFO: grad norm: 3.956 3.918 0.546
2024-12-02-07:15:14-root-INFO: grad norm: 3.994 3.957 0.540
2024-12-02-07:15:15-root-INFO: grad norm: 3.986 3.951 0.527
2024-12-02-07:15:15-root-INFO: grad norm: 3.961 3.926 0.529
2024-12-02-07:15:16-root-INFO: grad norm: 3.933 3.899 0.514
2024-12-02-07:15:16-root-INFO: Loss Change: 53.965 -> 52.091
2024-12-02-07:15:16-root-INFO: Regularization Change: 0.000 -> 1.892
2024-12-02-07:15:16-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-07:15:16-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-07:15:16-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-07:15:16-root-INFO: grad norm: 4.551 4.482 0.792
2024-12-02-07:15:17-root-INFO: grad norm: 4.216 4.174 0.593
2024-12-02-07:15:17-root-INFO: grad norm: 3.882 3.846 0.528
2024-12-02-07:15:18-root-INFO: grad norm: 3.814 3.780 0.506
2024-12-02-07:15:18-root-INFO: grad norm: 3.759 3.728 0.479
2024-12-02-07:15:18-root-INFO: grad norm: 3.721 3.689 0.481
2024-12-02-07:15:19-root-INFO: grad norm: 3.669 3.640 0.460
2024-12-02-07:15:19-root-INFO: grad norm: 3.642 3.612 0.467
2024-12-02-07:15:20-root-INFO: Loss Change: 52.255 -> 50.522
2024-12-02-07:15:20-root-INFO: Regularization Change: 0.000 -> 1.818
2024-12-02-07:15:20-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-07:15:20-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-07:15:20-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-07:15:20-root-INFO: grad norm: 4.683 4.611 0.814
2024-12-02-07:15:20-root-INFO: grad norm: 4.242 4.197 0.616
2024-12-02-07:15:21-root-INFO: grad norm: 3.727 3.693 0.504
2024-12-02-07:15:21-root-INFO: grad norm: 3.651 3.619 0.486
2024-12-02-07:15:22-root-INFO: grad norm: 3.613 3.586 0.448
2024-12-02-07:15:22-root-INFO: grad norm: 3.582 3.553 0.457
2024-12-02-07:15:23-root-INFO: grad norm: 3.541 3.514 0.431
2024-12-02-07:15:23-root-INFO: grad norm: 3.519 3.491 0.443
2024-12-02-07:15:23-root-INFO: Loss Change: 50.708 -> 48.981
2024-12-02-07:15:23-root-INFO: Regularization Change: 0.000 -> 1.770
2024-12-02-07:15:23-root-INFO: Undo step: 55
2024-12-02-07:15:23-root-INFO: Undo step: 56
2024-12-02-07:15:23-root-INFO: Undo step: 57
2024-12-02-07:15:23-root-INFO: Undo step: 58
2024-12-02-07:15:23-root-INFO: Undo step: 59
2024-12-02-07:15:24-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-07:15:24-root-INFO: grad norm: 24.548 24.336 3.220
2024-12-02-07:15:24-root-INFO: grad norm: 14.201 14.079 1.860
2024-12-02-07:15:25-root-INFO: grad norm: 9.748 9.623 1.554
2024-12-02-07:15:25-root-INFO: grad norm: 6.896 6.798 1.163
2024-12-02-07:15:26-root-INFO: grad norm: 5.539 5.461 0.926
2024-12-02-07:15:26-root-INFO: grad norm: 4.749 4.675 0.834
2024-12-02-07:15:27-root-INFO: grad norm: 4.346 4.287 0.708
2024-12-02-07:15:27-root-INFO: grad norm: 4.195 4.138 0.689
2024-12-02-07:15:27-root-INFO: Loss Change: 166.495 -> 68.255
2024-12-02-07:15:27-root-INFO: Regularization Change: 0.000 -> 69.849
2024-12-02-07:15:27-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-07:15:27-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-07:15:28-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-07:15:28-root-INFO: grad norm: 5.435 5.361 0.895
2024-12-02-07:15:28-root-INFO: grad norm: 4.702 4.645 0.728
2024-12-02-07:15:29-root-INFO: grad norm: 3.624 3.575 0.591
2024-12-02-07:15:29-root-INFO: grad norm: 3.671 3.626 0.569
2024-12-02-07:15:30-root-INFO: grad norm: 4.073 4.036 0.548
2024-12-02-07:15:30-root-INFO: grad norm: 3.964 3.922 0.574
2024-12-02-07:15:31-root-INFO: grad norm: 3.744 3.708 0.520
2024-12-02-07:15:31-root-INFO: grad norm: 3.703 3.664 0.539
2024-12-02-07:15:31-root-INFO: Loss Change: 68.295 -> 60.393
2024-12-02-07:15:31-root-INFO: Regularization Change: 0.000 -> 8.778
2024-12-02-07:15:31-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-07:15:31-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-07:15:32-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-07:15:32-root-INFO: grad norm: 4.717 4.637 0.865
2024-12-02-07:15:32-root-INFO: grad norm: 4.206 4.150 0.682
2024-12-02-07:15:33-root-INFO: grad norm: 3.578 3.534 0.561
2024-12-02-07:15:33-root-INFO: grad norm: 3.544 3.501 0.552
2024-12-02-07:15:34-root-INFO: grad norm: 3.612 3.576 0.508
2024-12-02-07:15:34-root-INFO: grad norm: 3.560 3.520 0.534
2024-12-02-07:15:35-root-INFO: grad norm: 3.474 3.440 0.489
2024-12-02-07:15:35-root-INFO: grad norm: 3.442 3.403 0.515
2024-12-02-07:15:35-root-INFO: Loss Change: 60.426 -> 56.265
2024-12-02-07:15:36-root-INFO: Regularization Change: 0.000 -> 4.598
2024-12-02-07:15:36-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-07:15:36-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-07:15:36-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-07:15:36-root-INFO: grad norm: 4.861 4.779 0.893
2024-12-02-07:15:36-root-INFO: grad norm: 4.280 4.218 0.726
2024-12-02-07:15:37-root-INFO: grad norm: 3.577 3.531 0.573
2024-12-02-07:15:37-root-INFO: grad norm: 3.536 3.490 0.571
2024-12-02-07:15:38-root-INFO: grad norm: 3.595 3.557 0.520
2024-12-02-07:15:38-root-INFO: grad norm: 3.610 3.567 0.556
2024-12-02-07:15:39-root-INFO: grad norm: 3.638 3.601 0.519
2024-12-02-07:15:39-root-INFO: grad norm: 3.659 3.616 0.557
2024-12-02-07:15:39-root-INFO: Loss Change: 56.346 -> 53.366
2024-12-02-07:15:39-root-INFO: Regularization Change: 0.000 -> 3.250
2024-12-02-07:15:39-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-07:15:39-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-07:15:40-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-07:15:40-root-INFO: grad norm: 4.840 4.753 0.913
2024-12-02-07:15:40-root-INFO: grad norm: 4.521 4.458 0.754
2024-12-02-07:15:41-root-INFO: grad norm: 4.203 4.154 0.642
2024-12-02-07:15:41-root-INFO: grad norm: 4.120 4.067 0.660
2024-12-02-07:15:42-root-INFO: grad norm: 4.023 3.980 0.584
2024-12-02-07:15:42-root-INFO: grad norm: 3.957 3.908 0.619
2024-12-02-07:15:43-root-INFO: grad norm: 3.866 3.826 0.554
2024-12-02-07:15:43-root-INFO: grad norm: 3.806 3.761 0.588
2024-12-02-07:15:43-root-INFO: Loss Change: 53.673 -> 51.242
2024-12-02-07:15:43-root-INFO: Regularization Change: 0.000 -> 2.630
2024-12-02-07:15:43-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-07:15:43-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-07:15:44-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-07:15:44-root-INFO: grad norm: 5.077 4.985 0.962
2024-12-02-07:15:44-root-INFO: grad norm: 4.609 4.543 0.774
2024-12-02-07:15:45-root-INFO: grad norm: 4.067 4.018 0.629
2024-12-02-07:15:45-root-INFO: grad norm: 3.917 3.867 0.624
2024-12-02-07:15:46-root-INFO: grad norm: 3.771 3.730 0.549
2024-12-02-07:15:46-root-INFO: grad norm: 3.683 3.640 0.567
2024-12-02-07:15:46-root-INFO: grad norm: 3.585 3.548 0.513
2024-12-02-07:15:47-root-INFO: grad norm: 3.527 3.486 0.534
2024-12-02-07:15:47-root-INFO: Loss Change: 51.529 -> 49.244
2024-12-02-07:15:47-root-INFO: Regularization Change: 0.000 -> 2.295
2024-12-02-07:15:47-root-INFO: Undo step: 55
2024-12-02-07:15:47-root-INFO: Undo step: 56
2024-12-02-07:15:47-root-INFO: Undo step: 57
2024-12-02-07:15:47-root-INFO: Undo step: 58
2024-12-02-07:15:47-root-INFO: Undo step: 59
2024-12-02-07:15:47-root-INFO: step: 60 lr_xt 0.14078630
2024-12-02-07:15:48-root-INFO: grad norm: 24.974 24.716 3.579
2024-12-02-07:15:48-root-INFO: grad norm: 14.153 13.983 2.183
2024-12-02-07:15:49-root-INFO: grad norm: 10.490 10.319 1.890
2024-12-02-07:15:49-root-INFO: grad norm: 9.900 9.779 1.541
2024-12-02-07:15:50-root-INFO: grad norm: 8.432 8.206 1.939
2024-12-02-07:15:50-root-INFO: grad norm: 6.287 6.198 1.056
2024-12-02-07:15:51-root-INFO: grad norm: 5.460 5.354 1.071
2024-12-02-07:15:51-root-INFO: grad norm: 4.851 4.776 0.850
2024-12-02-07:15:51-root-INFO: Loss Change: 172.269 -> 68.635
2024-12-02-07:15:51-root-INFO: Regularization Change: 0.000 -> 74.969
2024-12-02-07:15:51-root-INFO: Learning rate of xt decay: 0.19061 -> 0.19290.
2024-12-02-07:15:51-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00066.
2024-12-02-07:15:52-root-INFO: step: 59 lr_xt 0.14387389
2024-12-02-07:15:52-root-INFO: grad norm: 4.658 4.572 0.890
2024-12-02-07:15:52-root-INFO: grad norm: 4.156 4.094 0.712
2024-12-02-07:15:53-root-INFO: grad norm: 3.898 3.841 0.665
2024-12-02-07:15:53-root-INFO: grad norm: 3.707 3.654 0.624
2024-12-02-07:15:54-root-INFO: grad norm: 3.616 3.569 0.580
2024-12-02-07:15:54-root-INFO: grad norm: 3.526 3.478 0.578
2024-12-02-07:15:55-root-INFO: grad norm: 3.504 3.464 0.531
2024-12-02-07:15:55-root-INFO: grad norm: 3.451 3.407 0.552
2024-12-02-07:15:55-root-INFO: Loss Change: 68.519 -> 60.187
2024-12-02-07:15:55-root-INFO: Regularization Change: 0.000 -> 9.346
2024-12-02-07:15:55-root-INFO: Learning rate of xt decay: 0.19290 -> 0.19521.
2024-12-02-07:15:55-root-INFO: Coefficient of regularization decay: 0.00066 -> 0.00067.
2024-12-02-07:15:56-root-INFO: step: 58 lr_xt 0.14700566
2024-12-02-07:15:56-root-INFO: grad norm: 4.329 4.259 0.774
2024-12-02-07:15:56-root-INFO: grad norm: 4.064 4.010 0.658
2024-12-02-07:15:57-root-INFO: grad norm: 3.871 3.830 0.559
2024-12-02-07:15:57-root-INFO: grad norm: 3.742 3.695 0.588
2024-12-02-07:15:58-root-INFO: grad norm: 3.580 3.544 0.503
2024-12-02-07:15:58-root-INFO: grad norm: 3.486 3.443 0.543
2024-12-02-07:15:59-root-INFO: grad norm: 3.372 3.339 0.469
2024-12-02-07:15:59-root-INFO: grad norm: 3.304 3.264 0.512
2024-12-02-07:15:59-root-INFO: Loss Change: 60.150 -> 55.763
2024-12-02-07:15:59-root-INFO: Regularization Change: 0.000 -> 4.932
2024-12-02-07:15:59-root-INFO: Learning rate of xt decay: 0.19521 -> 0.19756.
2024-12-02-07:15:59-root-INFO: Coefficient of regularization decay: 0.00067 -> 0.00068.
2024-12-02-07:16:00-root-INFO: step: 57 lr_xt 0.15018154
2024-12-02-07:16:00-root-INFO: grad norm: 4.543 4.468 0.821
2024-12-02-07:16:00-root-INFO: grad norm: 4.096 4.038 0.687
2024-12-02-07:16:01-root-INFO: grad norm: 3.641 3.601 0.540
2024-12-02-07:16:01-root-INFO: grad norm: 3.458 3.414 0.552
2024-12-02-07:16:02-root-INFO: grad norm: 3.255 3.222 0.460
2024-12-02-07:16:02-root-INFO: grad norm: 3.159 3.120 0.492
2024-12-02-07:16:03-root-INFO: grad norm: 3.055 3.026 0.424
2024-12-02-07:16:03-root-INFO: grad norm: 3.003 2.967 0.461
2024-12-02-07:16:03-root-INFO: Loss Change: 55.778 -> 52.530
2024-12-02-07:16:03-root-INFO: Regularization Change: 0.000 -> 3.474
2024-12-02-07:16:03-root-INFO: Learning rate of xt decay: 0.19756 -> 0.19993.
2024-12-02-07:16:03-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00068.
2024-12-02-07:16:04-root-INFO: step: 56 lr_xt 0.15340147
2024-12-02-07:16:04-root-INFO: grad norm: 4.007 3.929 0.789
2024-12-02-07:16:04-root-INFO: grad norm: 3.698 3.647 0.613
2024-12-02-07:16:05-root-INFO: grad norm: 3.489 3.448 0.531
2024-12-02-07:16:05-root-INFO: grad norm: 3.405 3.362 0.539
2024-12-02-07:16:06-root-INFO: grad norm: 3.324 3.288 0.489
2024-12-02-07:16:06-root-INFO: grad norm: 3.311 3.271 0.515
2024-12-02-07:16:07-root-INFO: grad norm: 3.315 3.279 0.486
2024-12-02-07:16:07-root-INFO: grad norm: 3.345 3.304 0.519
2024-12-02-07:16:07-root-INFO: Loss Change: 52.746 -> 50.334
2024-12-02-07:16:07-root-INFO: Regularization Change: 0.000 -> 2.787
2024-12-02-07:16:07-root-INFO: Learning rate of xt decay: 0.19993 -> 0.20232.
2024-12-02-07:16:07-root-INFO: Coefficient of regularization decay: 0.00068 -> 0.00069.
2024-12-02-07:16:08-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-07:16:08-root-INFO: grad norm: 4.792 4.701 0.930
2024-12-02-07:16:08-root-INFO: grad norm: 4.510 4.444 0.770
2024-12-02-07:16:09-root-INFO: grad norm: 4.250 4.196 0.678
2024-12-02-07:16:09-root-INFO: grad norm: 4.203 4.144 0.704
2024-12-02-07:16:10-root-INFO: grad norm: 4.139 4.088 0.647
2024-12-02-07:16:10-root-INFO: grad norm: 4.141 4.082 0.701
2024-12-02-07:16:11-root-INFO: grad norm: 4.113 4.063 0.639
2024-12-02-07:16:11-root-INFO: grad norm: 4.095 4.035 0.699
2024-12-02-07:16:12-root-INFO: Loss Change: 50.600 -> 48.508
2024-12-02-07:16:12-root-INFO: Regularization Change: 0.000 -> 2.414
2024-12-02-07:16:12-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-07:16:12-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-07:16:12-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-07:16:12-root-INFO: grad norm: 5.054 4.960 0.971
2024-12-02-07:16:13-root-INFO: grad norm: 4.731 4.656 0.839
2024-12-02-07:16:13-root-INFO: grad norm: 4.399 4.340 0.717
2024-12-02-07:16:14-root-INFO: grad norm: 4.212 4.150 0.720
2024-12-02-07:16:14-root-INFO: grad norm: 3.987 3.936 0.633
2024-12-02-07:16:15-root-INFO: grad norm: 3.846 3.792 0.642
2024-12-02-07:16:15-root-INFO: grad norm: 3.691 3.645 0.580
2024-12-02-07:16:16-root-INFO: grad norm: 3.591 3.542 0.588
2024-12-02-07:16:16-root-INFO: Loss Change: 48.644 -> 46.463
2024-12-02-07:16:16-root-INFO: Regularization Change: 0.000 -> 2.193
2024-12-02-07:16:16-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-07:16:16-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-07:16:16-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-07:16:16-root-INFO: grad norm: 4.609 4.527 0.869
2024-12-02-07:16:17-root-INFO: grad norm: 4.256 4.192 0.734
2024-12-02-07:16:17-root-INFO: grad norm: 3.905 3.854 0.628
2024-12-02-07:16:18-root-INFO: grad norm: 3.736 3.685 0.616
2024-12-02-07:16:18-root-INFO: grad norm: 3.546 3.502 0.553
2024-12-02-07:16:19-root-INFO: grad norm: 3.441 3.396 0.555
2024-12-02-07:16:19-root-INFO: grad norm: 3.327 3.287 0.512
2024-12-02-07:16:20-root-INFO: grad norm: 3.261 3.220 0.518
2024-12-02-07:16:20-root-INFO: Loss Change: 46.785 -> 44.844
2024-12-02-07:16:20-root-INFO: Regularization Change: 0.000 -> 2.011
2024-12-02-07:16:20-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-07:16:20-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-07:16:20-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-07:16:20-root-INFO: grad norm: 4.147 4.072 0.788
2024-12-02-07:16:21-root-INFO: grad norm: 3.840 3.784 0.655
2024-12-02-07:16:21-root-INFO: grad norm: 3.582 3.539 0.555
2024-12-02-07:16:22-root-INFO: grad norm: 3.444 3.399 0.555
2024-12-02-07:16:22-root-INFO: grad norm: 3.289 3.252 0.495
2024-12-02-07:16:22-root-INFO: grad norm: 3.203 3.163 0.503
2024-12-02-07:16:23-root-INFO: grad norm: 3.109 3.074 0.463
2024-12-02-07:16:23-root-INFO: grad norm: 3.055 3.018 0.473
2024-12-02-07:16:24-root-INFO: Loss Change: 45.002 -> 43.256
2024-12-02-07:16:24-root-INFO: Regularization Change: 0.000 -> 1.910
2024-12-02-07:16:24-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-07:16:24-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-07:16:24-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-07:16:24-root-INFO: grad norm: 4.314 4.226 0.870
2024-12-02-07:16:25-root-INFO: grad norm: 3.974 3.919 0.654
2024-12-02-07:16:25-root-INFO: grad norm: 3.694 3.647 0.586
2024-12-02-07:16:26-root-INFO: grad norm: 3.546 3.503 0.554
2024-12-02-07:16:26-root-INFO: grad norm: 3.369 3.329 0.516
2024-12-02-07:16:27-root-INFO: grad norm: 3.275 3.236 0.504
2024-12-02-07:16:27-root-INFO: grad norm: 3.165 3.128 0.478
2024-12-02-07:16:27-root-INFO: grad norm: 3.104 3.068 0.475
2024-12-02-07:16:28-root-INFO: Loss Change: 43.517 -> 41.794
2024-12-02-07:16:28-root-INFO: Regularization Change: 0.000 -> 1.852
2024-12-02-07:16:28-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-07:16:28-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-07:16:28-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-07:16:28-root-INFO: grad norm: 4.181 4.101 0.817
2024-12-02-07:16:29-root-INFO: grad norm: 3.850 3.796 0.643
2024-12-02-07:16:29-root-INFO: grad norm: 3.559 3.515 0.555
2024-12-02-07:16:29-root-INFO: grad norm: 3.414 3.371 0.540
2024-12-02-07:16:30-root-INFO: grad norm: 3.251 3.213 0.494
2024-12-02-07:16:30-root-INFO: grad norm: 3.162 3.123 0.493
2024-12-02-07:16:31-root-INFO: grad norm: 3.062 3.027 0.461
2024-12-02-07:16:31-root-INFO: grad norm: 3.006 2.970 0.464
2024-12-02-07:16:32-root-INFO: Loss Change: 41.801 -> 40.153
2024-12-02-07:16:32-root-INFO: Regularization Change: 0.000 -> 1.808
2024-12-02-07:16:32-root-INFO: Undo step: 50
2024-12-02-07:16:32-root-INFO: Undo step: 51
2024-12-02-07:16:32-root-INFO: Undo step: 52
2024-12-02-07:16:32-root-INFO: Undo step: 53
2024-12-02-07:16:32-root-INFO: Undo step: 54
2024-12-02-07:16:32-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-07:16:32-root-INFO: grad norm: 22.371 22.210 2.675
2024-12-02-07:16:32-root-INFO: grad norm: 11.505 11.383 1.675
2024-12-02-07:16:33-root-INFO: grad norm: 8.431 8.325 1.332
2024-12-02-07:16:33-root-INFO: grad norm: 7.239 7.148 1.147
2024-12-02-07:16:34-root-INFO: grad norm: 6.458 6.363 1.103
2024-12-02-07:16:34-root-INFO: grad norm: 5.691 5.620 0.897
2024-12-02-07:16:35-root-INFO: grad norm: 5.072 4.997 0.870
2024-12-02-07:16:35-root-INFO: grad norm: 4.442 4.385 0.714
2024-12-02-07:16:36-root-INFO: Loss Change: 147.448 -> 58.002
2024-12-02-07:16:36-root-INFO: Regularization Change: 0.000 -> 73.779
2024-12-02-07:16:36-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-07:16:36-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-07:16:36-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-07:16:36-root-INFO: grad norm: 4.161 4.101 0.708
2024-12-02-07:16:37-root-INFO: grad norm: 3.678 3.634 0.570
2024-12-02-07:16:37-root-INFO: grad norm: 3.440 3.398 0.538
2024-12-02-07:16:37-root-INFO: grad norm: 3.283 3.245 0.503
2024-12-02-07:16:38-root-INFO: grad norm: 3.252 3.217 0.481
2024-12-02-07:16:38-root-INFO: grad norm: 3.242 3.206 0.479
2024-12-02-07:16:39-root-INFO: grad norm: 3.358 3.325 0.469
2024-12-02-07:16:39-root-INFO: grad norm: 3.409 3.374 0.488
2024-12-02-07:16:40-root-INFO: Loss Change: 57.726 -> 50.682
2024-12-02-07:16:40-root-INFO: Regularization Change: 0.000 -> 8.888
2024-12-02-07:16:40-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-07:16:40-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-07:16:40-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-07:16:40-root-INFO: grad norm: 4.418 4.358 0.728
2024-12-02-07:16:41-root-INFO: grad norm: 4.125 4.077 0.630
2024-12-02-07:16:41-root-INFO: grad norm: 3.747 3.708 0.538
2024-12-02-07:16:42-root-INFO: grad norm: 3.618 3.578 0.533
2024-12-02-07:16:42-root-INFO: grad norm: 3.456 3.422 0.481
2024-12-02-07:16:43-root-INFO: grad norm: 3.383 3.348 0.489
2024-12-02-07:16:43-root-INFO: grad norm: 3.294 3.263 0.452
2024-12-02-07:16:43-root-INFO: grad norm: 3.255 3.221 0.464
2024-12-02-07:16:44-root-INFO: Loss Change: 50.848 -> 47.130
2024-12-02-07:16:44-root-INFO: Regularization Change: 0.000 -> 4.548
2024-12-02-07:16:44-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-07:16:44-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-07:16:44-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-07:16:44-root-INFO: grad norm: 4.048 3.985 0.709
2024-12-02-07:16:45-root-INFO: grad norm: 3.707 3.663 0.569
2024-12-02-07:16:45-root-INFO: grad norm: 3.359 3.325 0.475
2024-12-02-07:16:46-root-INFO: grad norm: 3.249 3.215 0.467
2024-12-02-07:16:46-root-INFO: grad norm: 3.136 3.107 0.427
2024-12-02-07:16:46-root-INFO: grad norm: 3.096 3.066 0.433
2024-12-02-07:16:47-root-INFO: grad norm: 3.056 3.029 0.408
2024-12-02-07:16:47-root-INFO: grad norm: 3.044 3.016 0.419
2024-12-02-07:16:48-root-INFO: Loss Change: 47.208 -> 44.551
2024-12-02-07:16:48-root-INFO: Regularization Change: 0.000 -> 3.234
2024-12-02-07:16:48-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-07:16:48-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-07:16:48-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-07:16:48-root-INFO: grad norm: 4.165 4.088 0.792
2024-12-02-07:16:49-root-INFO: grad norm: 3.781 3.737 0.576
2024-12-02-07:16:49-root-INFO: grad norm: 3.394 3.356 0.510
2024-12-02-07:16:49-root-INFO: grad norm: 3.278 3.245 0.465
2024-12-02-07:16:50-root-INFO: grad norm: 3.158 3.126 0.452
2024-12-02-07:16:50-root-INFO: grad norm: 3.120 3.090 0.433
2024-12-02-07:16:51-root-INFO: grad norm: 3.080 3.049 0.433
2024-12-02-07:16:51-root-INFO: grad norm: 3.074 3.045 0.423
2024-12-02-07:16:52-root-INFO: Loss Change: 44.708 -> 42.485
2024-12-02-07:16:52-root-INFO: Regularization Change: 0.000 -> 2.650
2024-12-02-07:16:52-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-07:16:52-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-07:16:52-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-07:16:52-root-INFO: grad norm: 4.147 4.073 0.778
2024-12-02-07:16:52-root-INFO: grad norm: 3.798 3.751 0.595
2024-12-02-07:16:53-root-INFO: grad norm: 3.432 3.392 0.521
2024-12-02-07:16:53-root-INFO: grad norm: 3.355 3.318 0.493
2024-12-02-07:16:54-root-INFO: grad norm: 3.298 3.262 0.487
2024-12-02-07:16:54-root-INFO: grad norm: 3.308 3.273 0.480
2024-12-02-07:16:55-root-INFO: grad norm: 3.326 3.291 0.486
2024-12-02-07:16:55-root-INFO: grad norm: 3.364 3.328 0.488
2024-12-02-07:16:56-root-INFO: Loss Change: 42.422 -> 40.556
2024-12-02-07:16:56-root-INFO: Regularization Change: 0.000 -> 2.331
2024-12-02-07:16:56-root-INFO: Undo step: 50
2024-12-02-07:16:56-root-INFO: Undo step: 51
2024-12-02-07:16:56-root-INFO: Undo step: 52
2024-12-02-07:16:56-root-INFO: Undo step: 53
2024-12-02-07:16:56-root-INFO: Undo step: 54
2024-12-02-07:16:56-root-INFO: step: 55 lr_xt 0.15666536
2024-12-02-07:16:56-root-INFO: grad norm: 24.128 23.961 2.838
2024-12-02-07:16:56-root-INFO: grad norm: 12.821 12.683 1.878
2024-12-02-07:16:57-root-INFO: grad norm: 8.377 8.263 1.377
2024-12-02-07:16:57-root-INFO: grad norm: 6.273 6.184 1.053
2024-12-02-07:16:58-root-INFO: grad norm: 5.116 5.039 0.882
2024-12-02-07:16:58-root-INFO: grad norm: 4.353 4.286 0.760
2024-12-02-07:16:59-root-INFO: grad norm: 3.819 3.759 0.674
2024-12-02-07:16:59-root-INFO: grad norm: 3.422 3.371 0.593
2024-12-02-07:16:59-root-INFO: Loss Change: 156.040 -> 57.675
2024-12-02-07:16:59-root-INFO: Regularization Change: 0.000 -> 78.424
2024-12-02-07:16:59-root-INFO: Learning rate of xt decay: 0.20232 -> 0.20475.
2024-12-02-07:16:59-root-INFO: Coefficient of regularization decay: 0.00069 -> 0.00070.
2024-12-02-07:17:00-root-INFO: step: 54 lr_xt 0.15997308
2024-12-02-07:17:00-root-INFO: grad norm: 3.514 3.442 0.708
2024-12-02-07:17:00-root-INFO: grad norm: 3.165 3.127 0.487
2024-12-02-07:17:01-root-INFO: grad norm: 3.051 3.006 0.521
2024-12-02-07:17:01-root-INFO: grad norm: 2.977 2.946 0.433
2024-12-02-07:17:02-root-INFO: grad norm: 3.003 2.964 0.485
2024-12-02-07:17:02-root-INFO: grad norm: 2.996 2.967 0.416
2024-12-02-07:17:03-root-INFO: grad norm: 3.062 3.024 0.478
2024-12-02-07:17:03-root-INFO: grad norm: 3.058 3.029 0.417
2024-12-02-07:17:03-root-INFO: Loss Change: 57.425 -> 50.029
2024-12-02-07:17:03-root-INFO: Regularization Change: 0.000 -> 9.502
2024-12-02-07:17:03-root-INFO: Learning rate of xt decay: 0.20475 -> 0.20721.
2024-12-02-07:17:03-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00070.
2024-12-02-07:17:04-root-INFO: step: 53 lr_xt 0.16332449
2024-12-02-07:17:04-root-INFO: grad norm: 4.072 4.001 0.759
2024-12-02-07:17:04-root-INFO: grad norm: 3.690 3.649 0.548
2024-12-02-07:17:05-root-INFO: grad norm: 3.255 3.210 0.539
2024-12-02-07:17:05-root-INFO: grad norm: 3.201 3.168 0.456
2024-12-02-07:17:06-root-INFO: grad norm: 3.180 3.141 0.495
2024-12-02-07:17:06-root-INFO: grad norm: 3.153 3.122 0.442
2024-12-02-07:17:07-root-INFO: grad norm: 3.125 3.088 0.483
2024-12-02-07:17:07-root-INFO: grad norm: 3.109 3.079 0.436
2024-12-02-07:17:07-root-INFO: Loss Change: 50.200 -> 46.338
2024-12-02-07:17:07-root-INFO: Regularization Change: 0.000 -> 4.827
2024-12-02-07:17:07-root-INFO: Learning rate of xt decay: 0.20721 -> 0.20970.
2024-12-02-07:17:07-root-INFO: Coefficient of regularization decay: 0.00070 -> 0.00071.
2024-12-02-07:17:07-root-INFO: step: 52 lr_xt 0.16671942
2024-12-02-07:17:08-root-INFO: grad norm: 4.012 3.940 0.757
2024-12-02-07:17:08-root-INFO: grad norm: 3.696 3.651 0.570
2024-12-02-07:17:09-root-INFO: grad norm: 3.393 3.351 0.532
2024-12-02-07:17:09-root-INFO: grad norm: 3.340 3.305 0.484
2024-12-02-07:17:10-root-INFO: grad norm: 3.310 3.273 0.495
2024-12-02-07:17:10-root-INFO: grad norm: 3.326 3.293 0.470
2024-12-02-07:17:11-root-INFO: grad norm: 3.365 3.330 0.489
2024-12-02-07:17:11-root-INFO: grad norm: 3.403 3.370 0.472
2024-12-02-07:17:11-root-INFO: Loss Change: 46.420 -> 43.769
2024-12-02-07:17:11-root-INFO: Regularization Change: 0.000 -> 3.384
2024-12-02-07:17:11-root-INFO: Learning rate of xt decay: 0.20970 -> 0.21221.
2024-12-02-07:17:11-root-INFO: Coefficient of regularization decay: 0.00071 -> 0.00072.
2024-12-02-07:17:12-root-INFO: step: 51 lr_xt 0.17015769
2024-12-02-07:17:12-root-INFO: grad norm: 4.701 4.615 0.890
2024-12-02-07:17:12-root-INFO: grad norm: 4.334 4.285 0.648
2024-12-02-07:17:13-root-INFO: grad norm: 3.950 3.904 0.606
2024-12-02-07:17:13-root-INFO: grad norm: 3.829 3.791 0.541
2024-12-02-07:17:14-root-INFO: grad norm: 3.684 3.644 0.544
2024-12-02-07:17:14-root-INFO: grad norm: 3.614 3.578 0.507
2024-12-02-07:17:15-root-INFO: grad norm: 3.524 3.486 0.516
2024-12-02-07:17:15-root-INFO: grad norm: 3.478 3.443 0.489
2024-12-02-07:17:16-root-INFO: Loss Change: 44.014 -> 41.630
2024-12-02-07:17:16-root-INFO: Regularization Change: 0.000 -> 2.760
2024-12-02-07:17:16-root-INFO: Learning rate of xt decay: 0.21221 -> 0.21476.
2024-12-02-07:17:16-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00072.
2024-12-02-07:17:16-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-07:17:16-root-INFO: grad norm: 4.563 4.485 0.842
2024-12-02-07:17:16-root-INFO: grad norm: 4.182 4.131 0.649
2024-12-02-07:17:17-root-INFO: grad norm: 3.779 3.734 0.577
2024-12-02-07:17:18-root-INFO: grad norm: 3.646 3.606 0.534
2024-12-02-07:17:18-root-INFO: grad norm: 3.509 3.470 0.521
2024-12-02-07:17:18-root-INFO: grad norm: 3.439 3.403 0.498
2024-12-02-07:17:19-root-INFO: grad norm: 3.359 3.322 0.495
2024-12-02-07:17:19-root-INFO: grad norm: 3.316 3.281 0.478
2024-12-02-07:17:20-root-INFO: Loss Change: 41.625 -> 39.521
2024-12-02-07:17:20-root-INFO: Regularization Change: 0.000 -> 2.401
2024-12-02-07:17:20-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-07:17:20-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-07:17:20-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-07:17:20-root-INFO: grad norm: 4.626 4.544 0.868
2024-12-02-07:17:21-root-INFO: grad norm: 4.139 4.086 0.660
2024-12-02-07:17:21-root-INFO: grad norm: 3.684 3.640 0.571
2024-12-02-07:17:22-root-INFO: grad norm: 3.527 3.487 0.528
2024-12-02-07:17:22-root-INFO: grad norm: 3.361 3.323 0.501
2024-12-02-07:17:23-root-INFO: grad norm: 3.274 3.238 0.480
2024-12-02-07:17:23-root-INFO: grad norm: 3.183 3.148 0.470
2024-12-02-07:17:24-root-INFO: grad norm: 3.131 3.098 0.455
2024-12-02-07:17:24-root-INFO: Loss Change: 39.886 -> 37.855
2024-12-02-07:17:24-root-INFO: Regularization Change: 0.000 -> 2.203
2024-12-02-07:17:24-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-07:17:24-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-07:17:24-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-07:17:24-root-INFO: grad norm: 3.854 3.789 0.704
2024-12-02-07:17:25-root-INFO: grad norm: 3.606 3.562 0.564
2024-12-02-07:17:25-root-INFO: grad norm: 3.414 3.377 0.500
2024-12-02-07:17:26-root-INFO: grad norm: 3.308 3.273 0.483
2024-12-02-07:17:26-root-INFO: grad norm: 3.191 3.159 0.455
2024-12-02-07:17:27-root-INFO: grad norm: 3.126 3.094 0.447
2024-12-02-07:17:27-root-INFO: grad norm: 3.054 3.023 0.433
2024-12-02-07:17:28-root-INFO: grad norm: 3.013 2.983 0.427
2024-12-02-07:17:28-root-INFO: Loss Change: 37.940 -> 36.298
2024-12-02-07:17:28-root-INFO: Regularization Change: 0.000 -> 2.013
2024-12-02-07:17:28-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-07:17:28-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-07:17:28-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-07:17:28-root-INFO: grad norm: 4.194 4.116 0.806
2024-12-02-07:17:29-root-INFO: grad norm: 3.833 3.786 0.596
2024-12-02-07:17:29-root-INFO: grad norm: 3.521 3.481 0.532
2024-12-02-07:17:30-root-INFO: grad norm: 3.370 3.335 0.486
2024-12-02-07:17:30-root-INFO: grad norm: 3.207 3.173 0.466
2024-12-02-07:17:31-root-INFO: grad norm: 3.117 3.086 0.442
2024-12-02-07:17:31-root-INFO: grad norm: 3.020 2.989 0.433
2024-12-02-07:17:32-root-INFO: grad norm: 2.965 2.936 0.417
2024-12-02-07:17:32-root-INFO: Loss Change: 36.572 -> 34.880
2024-12-02-07:17:32-root-INFO: Regularization Change: 0.000 -> 1.925
2024-12-02-07:17:32-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-07:17:32-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-07:17:32-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-07:17:32-root-INFO: grad norm: 3.894 3.828 0.715
2024-12-02-07:17:33-root-INFO: grad norm: 3.582 3.539 0.550
2024-12-02-07:17:33-root-INFO: grad norm: 3.328 3.293 0.483
2024-12-02-07:17:34-root-INFO: grad norm: 3.195 3.162 0.460
2024-12-02-07:17:34-root-INFO: grad norm: 3.053 3.023 0.432
2024-12-02-07:17:35-root-INFO: grad norm: 2.975 2.944 0.422
2024-12-02-07:17:35-root-INFO: grad norm: 2.893 2.865 0.406
2024-12-02-07:17:36-root-INFO: grad norm: 2.848 2.820 0.401
2024-12-02-07:17:36-root-INFO: Loss Change: 35.063 -> 33.523
2024-12-02-07:17:36-root-INFO: Regularization Change: 0.000 -> 1.830
2024-12-02-07:17:36-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-07:17:36-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-07:17:36-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-07:17:36-root-INFO: grad norm: 3.808 3.738 0.725
2024-12-02-07:17:37-root-INFO: grad norm: 3.481 3.438 0.545
2024-12-02-07:17:37-root-INFO: grad norm: 3.226 3.191 0.472
2024-12-02-07:17:38-root-INFO: grad norm: 3.090 3.057 0.446
2024-12-02-07:17:38-root-INFO: grad norm: 2.950 2.920 0.418
2024-12-02-07:17:39-root-INFO: grad norm: 2.869 2.840 0.408
2024-12-02-07:17:39-root-INFO: grad norm: 2.787 2.760 0.392
2024-12-02-07:17:40-root-INFO: grad norm: 2.740 2.713 0.387
2024-12-02-07:17:40-root-INFO: Loss Change: 33.640 -> 32.152
2024-12-02-07:17:40-root-INFO: Regularization Change: 0.000 -> 1.777
2024-12-02-07:17:40-root-INFO: Undo step: 45
2024-12-02-07:17:40-root-INFO: Undo step: 46
2024-12-02-07:17:40-root-INFO: Undo step: 47
2024-12-02-07:17:40-root-INFO: Undo step: 48
2024-12-02-07:17:40-root-INFO: Undo step: 49
2024-12-02-07:17:40-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-07:17:40-root-INFO: grad norm: 19.936 19.780 2.488
2024-12-02-07:17:41-root-INFO: grad norm: 10.807 10.704 1.490
2024-12-02-07:17:41-root-INFO: grad norm: 7.537 7.445 1.172
2024-12-02-07:17:42-root-INFO: grad norm: 6.131 6.063 0.913
2024-12-02-07:17:42-root-INFO: grad norm: 5.292 5.230 0.813
2024-12-02-07:17:43-root-INFO: grad norm: 4.760 4.712 0.672
2024-12-02-07:17:43-root-INFO: grad norm: 4.369 4.320 0.652
2024-12-02-07:17:44-root-INFO: grad norm: 4.063 4.025 0.556
2024-12-02-07:17:44-root-INFO: Loss Change: 134.158 -> 50.074
2024-12-02-07:17:44-root-INFO: Regularization Change: 0.000 -> 76.374
2024-12-02-07:17:44-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-07:17:44-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-07:17:44-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-07:17:44-root-INFO: grad norm: 3.804 3.770 0.504
2024-12-02-07:17:45-root-INFO: grad norm: 3.466 3.438 0.445
2024-12-02-07:17:45-root-INFO: grad norm: 3.365 3.335 0.446
2024-12-02-07:17:46-root-INFO: grad norm: 3.340 3.315 0.412
2024-12-02-07:17:46-root-INFO: grad norm: 3.336 3.307 0.438
2024-12-02-07:17:47-root-INFO: grad norm: 3.366 3.342 0.401
2024-12-02-07:17:47-root-INFO: grad norm: 3.410 3.382 0.439
2024-12-02-07:17:48-root-INFO: grad norm: 3.478 3.454 0.408
2024-12-02-07:17:48-root-INFO: Loss Change: 49.845 -> 42.749
2024-12-02-07:17:48-root-INFO: Regularization Change: 0.000 -> 10.160
2024-12-02-07:17:48-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-07:17:48-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-07:17:48-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-07:17:48-root-INFO: grad norm: 3.631 3.604 0.443
2024-12-02-07:17:49-root-INFO: grad norm: 3.641 3.614 0.443
2024-12-02-07:17:49-root-INFO: grad norm: 3.753 3.727 0.439
2024-12-02-07:17:50-root-INFO: grad norm: 3.845 3.815 0.478
2024-12-02-07:17:50-root-INFO: grad norm: 3.868 3.842 0.447
2024-12-02-07:17:51-root-INFO: grad norm: 3.862 3.832 0.479
2024-12-02-07:17:51-root-INFO: grad norm: 3.792 3.767 0.437
2024-12-02-07:17:52-root-INFO: grad norm: 3.721 3.694 0.451
2024-12-02-07:17:52-root-INFO: Loss Change: 42.557 -> 39.029
2024-12-02-07:17:52-root-INFO: Regularization Change: 0.000 -> 5.280
2024-12-02-07:17:52-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-07:17:52-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-07:17:52-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-07:17:52-root-INFO: grad norm: 3.927 3.894 0.506
2024-12-02-07:17:53-root-INFO: grad norm: 3.579 3.551 0.450
2024-12-02-07:17:53-root-INFO: grad norm: 3.334 3.314 0.369
2024-12-02-07:17:54-root-INFO: grad norm: 3.199 3.177 0.375
2024-12-02-07:17:54-root-INFO: grad norm: 3.100 3.081 0.346
2024-12-02-07:17:55-root-INFO: grad norm: 3.065 3.046 0.345
2024-12-02-07:17:55-root-INFO: grad norm: 3.039 3.020 0.341
2024-12-02-07:17:56-root-INFO: grad norm: 3.053 3.034 0.337
2024-12-02-07:17:56-root-INFO: Loss Change: 38.976 -> 36.235
2024-12-02-07:17:56-root-INFO: Regularization Change: 0.000 -> 3.693
2024-12-02-07:17:56-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-07:17:56-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-07:17:56-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-07:17:56-root-INFO: grad norm: 3.126 3.102 0.383
2024-12-02-07:17:57-root-INFO: grad norm: 2.922 2.904 0.323
2024-12-02-07:17:57-root-INFO: grad norm: 2.847 2.831 0.303
2024-12-02-07:17:58-root-INFO: grad norm: 2.855 2.838 0.309
2024-12-02-07:17:58-root-INFO: grad norm: 2.860 2.843 0.312
2024-12-02-07:17:59-root-INFO: grad norm: 2.903 2.886 0.312
2024-12-02-07:17:59-root-INFO: grad norm: 2.925 2.907 0.324
2024-12-02-07:18:00-root-INFO: grad norm: 2.967 2.950 0.318
2024-12-02-07:18:00-root-INFO: Loss Change: 36.114 -> 34.177
2024-12-02-07:18:00-root-INFO: Regularization Change: 0.000 -> 2.902
2024-12-02-07:18:00-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-07:18:00-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-07:18:00-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-07:18:00-root-INFO: grad norm: 3.027 3.004 0.373
2024-12-02-07:18:01-root-INFO: grad norm: 2.799 2.783 0.303
2024-12-02-07:18:01-root-INFO: grad norm: 2.740 2.725 0.281
2024-12-02-07:18:02-root-INFO: grad norm: 2.748 2.733 0.290
2024-12-02-07:18:02-root-INFO: grad norm: 2.754 2.738 0.293
2024-12-02-07:18:03-root-INFO: grad norm: 2.792 2.776 0.294
2024-12-02-07:18:03-root-INFO: grad norm: 2.808 2.792 0.307
2024-12-02-07:18:04-root-INFO: grad norm: 2.839 2.823 0.301
2024-12-02-07:18:04-root-INFO: Loss Change: 33.975 -> 32.328
2024-12-02-07:18:04-root-INFO: Regularization Change: 0.000 -> 2.479
2024-12-02-07:18:04-root-INFO: Undo step: 45
2024-12-02-07:18:04-root-INFO: Undo step: 46
2024-12-02-07:18:04-root-INFO: Undo step: 47
2024-12-02-07:18:04-root-INFO: Undo step: 48
2024-12-02-07:18:04-root-INFO: Undo step: 49
2024-12-02-07:18:04-root-INFO: step: 50 lr_xt 0.17363908
2024-12-02-07:18:04-root-INFO: grad norm: 20.038 19.879 2.520
2024-12-02-07:18:05-root-INFO: grad norm: 12.727 12.643 1.461
2024-12-02-07:18:05-root-INFO: grad norm: 8.167 8.097 1.062
2024-12-02-07:18:06-root-INFO: grad norm: 5.764 5.713 0.768
2024-12-02-07:18:06-root-INFO: grad norm: 4.697 4.650 0.661
2024-12-02-07:18:07-root-INFO: grad norm: 4.054 4.014 0.573
2024-12-02-07:18:07-root-INFO: grad norm: 3.656 3.616 0.538
2024-12-02-07:18:08-root-INFO: grad norm: 3.403 3.367 0.494
2024-12-02-07:18:08-root-INFO: Loss Change: 133.638 -> 49.642
2024-12-02-07:18:08-root-INFO: Regularization Change: 0.000 -> 75.844
2024-12-02-07:18:08-root-INFO: Learning rate of xt decay: 0.21476 -> 0.21734.
2024-12-02-07:18:08-root-INFO: Coefficient of regularization decay: 0.00072 -> 0.00073.
2024-12-02-07:18:08-root-INFO: step: 49 lr_xt 0.17716334
2024-12-02-07:18:08-root-INFO: grad norm: 4.147 4.088 0.699
2024-12-02-07:18:09-root-INFO: grad norm: 4.163 4.116 0.622
2024-12-02-07:18:09-root-INFO: grad norm: 4.404 4.362 0.610
2024-12-02-07:18:10-root-INFO: grad norm: 4.560 4.510 0.669
2024-12-02-07:18:10-root-INFO: grad norm: 4.701 4.656 0.649
2024-12-02-07:18:11-root-INFO: grad norm: 4.699 4.645 0.709
2024-12-02-07:18:11-root-INFO: grad norm: 4.684 4.638 0.658
2024-12-02-07:18:12-root-INFO: grad norm: 4.615 4.560 0.706
2024-12-02-07:18:12-root-INFO: Loss Change: 49.728 -> 42.964
2024-12-02-07:18:12-root-INFO: Regularization Change: 0.000 -> 10.135
2024-12-02-07:18:12-root-INFO: Learning rate of xt decay: 0.21734 -> 0.21994.
2024-12-02-07:18:12-root-INFO: Coefficient of regularization decay: 0.00073 -> 0.00074.
2024-12-02-07:18:12-root-INFO: step: 48 lr_xt 0.18073022
2024-12-02-07:18:12-root-INFO: grad norm: 5.203 5.131 0.864
2024-12-02-07:18:13-root-INFO: grad norm: 4.821 4.758 0.782
2024-12-02-07:18:13-root-INFO: grad norm: 4.445 4.401 0.630
2024-12-02-07:18:14-root-INFO: grad norm: 4.246 4.196 0.647
2024-12-02-07:18:14-root-INFO: grad norm: 4.057 4.018 0.563
2024-12-02-07:18:15-root-INFO: grad norm: 3.934 3.889 0.588
2024-12-02-07:18:15-root-INFO: grad norm: 3.815 3.778 0.525
2024-12-02-07:18:15-root-INFO: grad norm: 3.733 3.692 0.553
2024-12-02-07:18:16-root-INFO: Loss Change: 43.053 -> 38.888
2024-12-02-07:18:16-root-INFO: Regularization Change: 0.000 -> 5.286
2024-12-02-07:18:16-root-INFO: Learning rate of xt decay: 0.21994 -> 0.22258.
2024-12-02-07:18:16-root-INFO: Coefficient of regularization decay: 0.00074 -> 0.00075.
2024-12-02-07:18:16-root-INFO: step: 47 lr_xt 0.18433941
2024-12-02-07:18:16-root-INFO: grad norm: 4.832 4.751 0.882
2024-12-02-07:18:16-root-INFO: grad norm: 4.373 4.315 0.708
2024-12-02-07:18:17-root-INFO: grad norm: 3.970 3.927 0.581
2024-12-02-07:18:17-root-INFO: grad norm: 3.766 3.724 0.562
2024-12-02-07:18:18-root-INFO: grad norm: 3.583 3.547 0.505
2024-12-02-07:18:18-root-INFO: grad norm: 3.472 3.435 0.510
2024-12-02-07:18:19-root-INFO: grad norm: 3.371 3.338 0.467
2024-12-02-07:18:19-root-INFO: grad norm: 3.308 3.273 0.481
2024-12-02-07:18:20-root-INFO: Loss Change: 39.196 -> 36.127
2024-12-02-07:18:20-root-INFO: Regularization Change: 0.000 -> 3.739
2024-12-02-07:18:20-root-INFO: Learning rate of xt decay: 0.22258 -> 0.22525.
2024-12-02-07:18:20-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00075.
2024-12-02-07:18:20-root-INFO: step: 46 lr_xt 0.18799060
2024-12-02-07:18:20-root-INFO: grad norm: 4.224 4.157 0.746
2024-12-02-07:18:20-root-INFO: grad norm: 3.842 3.794 0.609
2024-12-02-07:18:21-root-INFO: grad norm: 3.536 3.500 0.499
2024-12-02-07:18:21-root-INFO: grad norm: 3.368 3.331 0.497
2024-12-02-07:18:22-root-INFO: grad norm: 3.215 3.185 0.439
2024-12-02-07:18:22-root-INFO: grad norm: 3.126 3.093 0.453
2024-12-02-07:18:23-root-INFO: grad norm: 3.047 3.019 0.410
2024-12-02-07:18:23-root-INFO: grad norm: 3.000 2.969 0.431
2024-12-02-07:18:24-root-INFO: Loss Change: 36.328 -> 33.947
2024-12-02-07:18:24-root-INFO: Regularization Change: 0.000 -> 2.975
2024-12-02-07:18:24-root-INFO: Learning rate of xt decay: 0.22525 -> 0.22796.
2024-12-02-07:18:24-root-INFO: Coefficient of regularization decay: 0.00075 -> 0.00076.
2024-12-02-07:18:24-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-07:18:24-root-INFO: grad norm: 4.000 3.931 0.739
2024-12-02-07:18:24-root-INFO: grad norm: 3.649 3.601 0.587
2024-12-02-07:18:25-root-INFO: grad norm: 3.369 3.335 0.476
2024-12-02-07:18:25-root-INFO: grad norm: 3.213 3.178 0.470
2024-12-02-07:18:26-root-INFO: grad norm: 3.071 3.043 0.416
2024-12-02-07:18:26-root-INFO: grad norm: 2.984 2.953 0.428
2024-12-02-07:18:27-root-INFO: grad norm: 2.906 2.880 0.388
2024-12-02-07:18:27-root-INFO: grad norm: 2.859 2.830 0.406
2024-12-02-07:18:28-root-INFO: Loss Change: 34.066 -> 32.044
2024-12-02-07:18:28-root-INFO: Regularization Change: 0.000 -> 2.544
2024-12-02-07:18:28-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-07:18:28-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-07:18:28-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-07:18:28-root-INFO: grad norm: 3.745 3.685 0.670
2024-12-02-07:18:28-root-INFO: grad norm: 3.442 3.401 0.531
2024-12-02-07:18:29-root-INFO: grad norm: 3.195 3.164 0.444
2024-12-02-07:18:29-root-INFO: grad norm: 3.046 3.015 0.435
2024-12-02-07:18:30-root-INFO: grad norm: 2.910 2.883 0.389
2024-12-02-07:18:30-root-INFO: grad norm: 2.824 2.796 0.397
2024-12-02-07:18:31-root-INFO: grad norm: 2.748 2.724 0.362
2024-12-02-07:18:31-root-INFO: grad norm: 2.701 2.675 0.376
2024-12-02-07:18:32-root-INFO: Loss Change: 32.234 -> 30.462
2024-12-02-07:18:32-root-INFO: Regularization Change: 0.000 -> 2.264
2024-12-02-07:18:32-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-07:18:32-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-07:18:32-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-07:18:32-root-INFO: grad norm: 3.680 3.619 0.668
2024-12-02-07:18:32-root-INFO: grad norm: 3.318 3.277 0.520
2024-12-02-07:18:33-root-INFO: grad norm: 3.025 2.995 0.419
2024-12-02-07:18:33-root-INFO: grad norm: 2.853 2.824 0.405
2024-12-02-07:18:34-root-INFO: grad norm: 2.700 2.676 0.357
2024-12-02-07:18:34-root-INFO: grad norm: 2.603 2.578 0.361
2024-12-02-07:18:35-root-INFO: grad norm: 2.519 2.498 0.328
2024-12-02-07:18:35-root-INFO: grad norm: 2.468 2.444 0.339
2024-12-02-07:18:36-root-INFO: Loss Change: 30.501 -> 28.825
2024-12-02-07:18:36-root-INFO: Regularization Change: 0.000 -> 2.081
2024-12-02-07:18:36-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-07:18:36-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-07:18:36-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-07:18:36-root-INFO: grad norm: 3.180 3.133 0.548
2024-12-02-07:18:36-root-INFO: grad norm: 2.940 2.907 0.441
2024-12-02-07:18:37-root-INFO: grad norm: 2.773 2.748 0.368
2024-12-02-07:18:37-root-INFO: grad norm: 2.659 2.634 0.369
2024-12-02-07:18:38-root-INFO: grad norm: 2.554 2.533 0.324
2024-12-02-07:18:38-root-INFO: grad norm: 2.483 2.460 0.339
2024-12-02-07:18:39-root-INFO: grad norm: 2.420 2.401 0.302
2024-12-02-07:18:39-root-INFO: grad norm: 2.380 2.359 0.322
2024-12-02-07:18:39-root-INFO: Loss Change: 28.909 -> 27.490
2024-12-02-07:18:39-root-INFO: Regularization Change: 0.000 -> 1.930
2024-12-02-07:18:39-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-07:18:39-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-07:18:40-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-07:18:40-root-INFO: grad norm: 3.685 3.613 0.723
2024-12-02-07:18:40-root-INFO: grad norm: 3.185 3.144 0.508
2024-12-02-07:18:41-root-INFO: grad norm: 2.832 2.806 0.385
2024-12-02-07:18:41-root-INFO: grad norm: 2.629 2.604 0.366
2024-12-02-07:18:42-root-INFO: grad norm: 2.461 2.441 0.312
2024-12-02-07:18:42-root-INFO: grad norm: 2.350 2.328 0.319
2024-12-02-07:18:43-root-INFO: grad norm: 2.258 2.241 0.280
2024-12-02-07:18:43-root-INFO: grad norm: 2.200 2.181 0.294
2024-12-02-07:18:43-root-INFO: Loss Change: 27.721 -> 26.100
2024-12-02-07:18:43-root-INFO: Regularization Change: 0.000 -> 1.919
2024-12-02-07:18:43-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-07:18:43-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-07:18:43-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-07:18:44-root-INFO: grad norm: 3.254 3.196 0.609
2024-12-02-07:18:44-root-INFO: grad norm: 2.853 2.818 0.442
2024-12-02-07:18:45-root-INFO: grad norm: 2.570 2.547 0.343
2024-12-02-07:18:45-root-INFO: grad norm: 2.396 2.373 0.331
2024-12-02-07:18:45-root-INFO: grad norm: 2.247 2.229 0.283
2024-12-02-07:18:46-root-INFO: grad norm: 2.148 2.128 0.290
2024-12-02-07:18:46-root-INFO: grad norm: 2.066 2.050 0.255
2024-12-02-07:18:47-root-INFO: grad norm: 2.013 1.995 0.269
2024-12-02-07:18:47-root-INFO: Loss Change: 26.301 -> 24.874
2024-12-02-07:18:47-root-INFO: Regularization Change: 0.000 -> 1.810
2024-12-02-07:18:47-root-INFO: Undo step: 40
2024-12-02-07:18:47-root-INFO: Undo step: 41
2024-12-02-07:18:47-root-INFO: Undo step: 42
2024-12-02-07:18:47-root-INFO: Undo step: 43
2024-12-02-07:18:47-root-INFO: Undo step: 44
2024-12-02-07:18:47-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-07:18:47-root-INFO: grad norm: 19.086 18.944 2.325
2024-12-02-07:18:48-root-INFO: grad norm: 10.328 10.219 1.497
2024-12-02-07:18:48-root-INFO: grad norm: 7.201 7.133 0.988
2024-12-02-07:18:49-root-INFO: grad norm: 5.534 5.467 0.856
2024-12-02-07:18:49-root-INFO: grad norm: 4.566 4.517 0.666
2024-12-02-07:18:50-root-INFO: grad norm: 3.945 3.896 0.614
2024-12-02-07:18:50-root-INFO: grad norm: 3.518 3.479 0.524
2024-12-02-07:18:51-root-INFO: grad norm: 3.208 3.168 0.503
2024-12-02-07:18:51-root-INFO: Loss Change: 122.657 -> 42.077
2024-12-02-07:18:51-root-INFO: Regularization Change: 0.000 -> 80.251
2024-12-02-07:18:51-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-07:18:51-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-07:18:51-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-07:18:51-root-INFO: grad norm: 3.557 3.503 0.615
2024-12-02-07:18:52-root-INFO: grad norm: 3.235 3.194 0.514
2024-12-02-07:18:52-root-INFO: grad norm: 3.037 3.004 0.440
2024-12-02-07:18:53-root-INFO: grad norm: 2.887 2.853 0.444
2024-12-02-07:18:53-root-INFO: grad norm: 2.760 2.732 0.394
2024-12-02-07:18:54-root-INFO: grad norm: 2.660 2.629 0.407
2024-12-02-07:18:54-root-INFO: grad norm: 2.575 2.549 0.366
2024-12-02-07:18:55-root-INFO: grad norm: 2.507 2.478 0.382
2024-12-02-07:18:55-root-INFO: Loss Change: 42.039 -> 34.981
2024-12-02-07:18:55-root-INFO: Regularization Change: 0.000 -> 10.791
2024-12-02-07:18:55-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-07:18:55-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-07:18:55-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-07:18:55-root-INFO: grad norm: 3.284 3.226 0.614
2024-12-02-07:18:56-root-INFO: grad norm: 2.974 2.934 0.487
2024-12-02-07:18:56-root-INFO: grad norm: 2.754 2.725 0.402
2024-12-02-07:18:57-root-INFO: grad norm: 2.608 2.578 0.397
2024-12-02-07:18:57-root-INFO: grad norm: 2.465 2.440 0.354
2024-12-02-07:18:58-root-INFO: grad norm: 2.366 2.339 0.356
2024-12-02-07:18:58-root-INFO: grad norm: 2.274 2.251 0.327
2024-12-02-07:18:58-root-INFO: grad norm: 2.209 2.185 0.330
2024-12-02-07:18:59-root-INFO: Loss Change: 34.848 -> 31.187
2024-12-02-07:18:59-root-INFO: Regularization Change: 0.000 -> 5.478
2024-12-02-07:18:59-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-07:18:59-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-07:18:59-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-07:18:59-root-INFO: grad norm: 2.853 2.803 0.534
2024-12-02-07:19:00-root-INFO: grad norm: 2.608 2.574 0.419
2024-12-02-07:19:00-root-INFO: grad norm: 2.450 2.424 0.358
2024-12-02-07:19:01-root-INFO: grad norm: 2.339 2.313 0.349
2024-12-02-07:19:01-root-INFO: grad norm: 2.232 2.209 0.320
2024-12-02-07:19:01-root-INFO: grad norm: 2.158 2.134 0.318
2024-12-02-07:19:02-root-INFO: grad norm: 2.090 2.068 0.298
2024-12-02-07:19:02-root-INFO: grad norm: 2.045 2.023 0.298
2024-12-02-07:19:03-root-INFO: Loss Change: 31.188 -> 28.743
2024-12-02-07:19:03-root-INFO: Regularization Change: 0.000 -> 3.691
2024-12-02-07:19:03-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-07:19:03-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-07:19:03-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-07:19:03-root-INFO: grad norm: 3.276 3.197 0.715
2024-12-02-07:19:03-root-INFO: grad norm: 2.788 2.747 0.474
2024-12-02-07:19:04-root-INFO: grad norm: 2.487 2.458 0.376
2024-12-02-07:19:04-root-INFO: grad norm: 2.316 2.290 0.343
2024-12-02-07:19:05-root-INFO: grad norm: 2.160 2.138 0.311
2024-12-02-07:19:05-root-INFO: grad norm: 2.066 2.044 0.296
2024-12-02-07:19:06-root-INFO: grad norm: 1.984 1.964 0.280
2024-12-02-07:19:06-root-INFO: grad norm: 1.936 1.917 0.270
2024-12-02-07:19:07-root-INFO: Loss Change: 28.861 -> 26.735
2024-12-02-07:19:07-root-INFO: Regularization Change: 0.000 -> 2.945
2024-12-02-07:19:07-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-07:19:07-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-07:19:07-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-07:19:07-root-INFO: grad norm: 2.905 2.842 0.605
2024-12-02-07:19:08-root-INFO: grad norm: 2.521 2.486 0.417
2024-12-02-07:19:08-root-INFO: grad norm: 2.284 2.259 0.337
2024-12-02-07:19:09-root-INFO: grad norm: 2.140 2.118 0.308
2024-12-02-07:19:09-root-INFO: grad norm: 2.011 1.991 0.281
2024-12-02-07:19:09-root-INFO: grad norm: 1.928 1.909 0.267
2024-12-02-07:19:10-root-INFO: grad norm: 1.855 1.838 0.253
2024-12-02-07:19:10-root-INFO: grad norm: 1.809 1.793 0.243
2024-12-02-07:19:11-root-INFO: Loss Change: 26.835 -> 25.120
2024-12-02-07:19:11-root-INFO: Regularization Change: 0.000 -> 2.441
2024-12-02-07:19:11-root-INFO: Undo step: 40
2024-12-02-07:19:11-root-INFO: Undo step: 41
2024-12-02-07:19:11-root-INFO: Undo step: 42
2024-12-02-07:19:11-root-INFO: Undo step: 43
2024-12-02-07:19:11-root-INFO: Undo step: 44
2024-12-02-07:19:11-root-INFO: step: 45 lr_xt 0.19168344
2024-12-02-07:19:11-root-INFO: grad norm: 19.352 19.238 2.096
2024-12-02-07:19:12-root-INFO: grad norm: 9.907 9.806 1.411
2024-12-02-07:19:12-root-INFO: grad norm: 7.125 7.051 1.018
2024-12-02-07:19:12-root-INFO: grad norm: 5.786 5.715 0.906
2024-12-02-07:19:13-root-INFO: grad norm: 4.939 4.884 0.737
2024-12-02-07:19:13-root-INFO: grad norm: 4.312 4.258 0.684
2024-12-02-07:19:14-root-INFO: grad norm: 3.831 3.784 0.596
2024-12-02-07:19:14-root-INFO: grad norm: 3.500 3.457 0.546
2024-12-02-07:19:15-root-INFO: Loss Change: 125.393 -> 42.513
2024-12-02-07:19:15-root-INFO: Regularization Change: 0.000 -> 84.028
2024-12-02-07:19:15-root-INFO: Learning rate of xt decay: 0.22796 -> 0.23069.
2024-12-02-07:19:15-root-INFO: Coefficient of regularization decay: 0.00076 -> 0.00077.
2024-12-02-07:19:15-root-INFO: step: 44 lr_xt 0.19541757
2024-12-02-07:19:15-root-INFO: grad norm: 3.980 3.907 0.759
2024-12-02-07:19:15-root-INFO: grad norm: 3.709 3.663 0.587
2024-12-02-07:19:16-root-INFO: grad norm: 3.546 3.501 0.564
2024-12-02-07:19:16-root-INFO: grad norm: 3.400 3.363 0.503
2024-12-02-07:19:17-root-INFO: grad norm: 3.258 3.219 0.505
2024-12-02-07:19:17-root-INFO: grad norm: 3.144 3.111 0.459
2024-12-02-07:19:18-root-INFO: grad norm: 3.040 3.004 0.466
2024-12-02-07:19:18-root-INFO: grad norm: 2.962 2.931 0.427
2024-12-02-07:19:18-root-INFO: Loss Change: 42.509 -> 35.367
2024-12-02-07:19:18-root-INFO: Regularization Change: 0.000 -> 10.905
2024-12-02-07:19:18-root-INFO: Learning rate of xt decay: 0.23069 -> 0.23346.
2024-12-02-07:19:18-root-INFO: Coefficient of regularization decay: 0.00077 -> 0.00078.
2024-12-02-07:19:19-root-INFO: step: 43 lr_xt 0.19919257
2024-12-02-07:19:19-root-INFO: grad norm: 3.803 3.728 0.749
2024-12-02-07:19:19-root-INFO: grad norm: 3.448 3.402 0.561
2024-12-02-07:19:20-root-INFO: grad norm: 3.131 3.091 0.497
2024-12-02-07:19:20-root-INFO: grad norm: 2.945 2.913 0.435
2024-12-02-07:19:21-root-INFO: grad norm: 2.780 2.748 0.422
2024-12-02-07:19:21-root-INFO: grad norm: 2.685 2.658 0.382
2024-12-02-07:19:22-root-INFO: grad norm: 2.606 2.578 0.385
2024-12-02-07:19:22-root-INFO: grad norm: 2.562 2.537 0.356
2024-12-02-07:19:22-root-INFO: Loss Change: 35.296 -> 31.476
2024-12-02-07:19:22-root-INFO: Regularization Change: 0.000 -> 5.600
2024-12-02-07:19:22-root-INFO: Learning rate of xt decay: 0.23346 -> 0.23626.
2024-12-02-07:19:22-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00078.
2024-12-02-07:19:23-root-INFO: step: 42 lr_xt 0.20300803
2024-12-02-07:19:23-root-INFO: grad norm: 3.192 3.135 0.599
2024-12-02-07:19:23-root-INFO: grad norm: 2.974 2.940 0.451
2024-12-02-07:19:24-root-INFO: grad norm: 2.817 2.787 0.414
2024-12-02-07:19:24-root-INFO: grad norm: 2.713 2.687 0.376
2024-12-02-07:19:25-root-INFO: grad norm: 2.611 2.585 0.368
2024-12-02-07:19:25-root-INFO: grad norm: 2.546 2.522 0.345
2024-12-02-07:19:26-root-INFO: grad norm: 2.486 2.463 0.343
2024-12-02-07:19:26-root-INFO: grad norm: 2.447 2.425 0.327
2024-12-02-07:19:27-root-INFO: Loss Change: 31.478 -> 28.945
2024-12-02-07:19:27-root-INFO: Regularization Change: 0.000 -> 3.825
2024-12-02-07:19:27-root-INFO: Learning rate of xt decay: 0.23626 -> 0.23910.
2024-12-02-07:19:27-root-INFO: Coefficient of regularization decay: 0.00078 -> 0.00079.
2024-12-02-07:19:27-root-INFO: step: 41 lr_xt 0.20721469
2024-12-02-07:19:27-root-INFO: grad norm: 3.667 3.584 0.776
2024-12-02-07:19:27-root-INFO: grad norm: 3.178 3.137 0.513
2024-12-02-07:19:28-root-INFO: grad norm: 2.820 2.788 0.422
2024-12-02-07:19:28-root-INFO: grad norm: 2.622 2.596 0.366
2024-12-02-07:19:29-root-INFO: grad norm: 2.450 2.426 0.343
2024-12-02-07:19:29-root-INFO: grad norm: 2.351 2.330 0.315
2024-12-02-07:19:30-root-INFO: grad norm: 2.267 2.246 0.308
2024-12-02-07:19:30-root-INFO: grad norm: 2.217 2.198 0.291
2024-12-02-07:19:31-root-INFO: Loss Change: 29.099 -> 26.814
2024-12-02-07:19:31-root-INFO: Regularization Change: 0.000 -> 3.091
2024-12-02-07:19:31-root-INFO: Learning rate of xt decay: 0.23910 -> 0.24197.
2024-12-02-07:19:31-root-INFO: Coefficient of regularization decay: 0.00079 -> 0.00080.
2024-12-02-07:19:31-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-07:19:31-root-INFO: grad norm: 3.138 3.073 0.634
2024-12-02-07:19:31-root-INFO: grad norm: 2.759 2.725 0.434
2024-12-02-07:19:32-root-INFO: grad norm: 2.497 2.470 0.361
2024-12-02-07:19:32-root-INFO: grad norm: 2.340 2.317 0.324
2024-12-02-07:19:33-root-INFO: grad norm: 2.203 2.182 0.300
2024-12-02-07:19:33-root-INFO: grad norm: 2.120 2.101 0.283
2024-12-02-07:19:34-root-INFO: grad norm: 2.050 2.032 0.272
2024-12-02-07:19:34-root-INFO: grad norm: 2.008 1.991 0.263
2024-12-02-07:19:35-root-INFO: Loss Change: 26.925 -> 25.104
2024-12-02-07:19:35-root-INFO: Regularization Change: 0.000 -> 2.568
2024-12-02-07:19:35-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-07:19:35-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-07:19:35-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-07:19:35-root-INFO: grad norm: 2.751 2.698 0.538
2024-12-02-07:19:35-root-INFO: grad norm: 2.472 2.444 0.372
2024-12-02-07:19:36-root-INFO: grad norm: 2.301 2.279 0.320
2024-12-02-07:19:36-root-INFO: grad norm: 2.190 2.170 0.293
2024-12-02-07:19:37-root-INFO: grad norm: 2.085 2.067 0.273
2024-12-02-07:19:37-root-INFO: grad norm: 2.018 2.000 0.263
2024-12-02-07:19:38-root-INFO: grad norm: 1.958 1.942 0.251
2024-12-02-07:19:38-root-INFO: grad norm: 1.920 1.904 0.247
2024-12-02-07:19:39-root-INFO: Loss Change: 25.099 -> 23.588
2024-12-02-07:19:39-root-INFO: Regularization Change: 0.000 -> 2.240
2024-12-02-07:19:39-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-07:19:39-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-07:19:39-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-07:19:39-root-INFO: grad norm: 2.755 2.696 0.568
2024-12-02-07:19:39-root-INFO: grad norm: 2.407 2.379 0.366
2024-12-02-07:19:40-root-INFO: grad norm: 2.186 2.165 0.307
2024-12-02-07:19:40-root-INFO: grad norm: 2.052 2.034 0.269
2024-12-02-07:19:41-root-INFO: grad norm: 1.934 1.917 0.253
2024-12-02-07:19:41-root-INFO: grad norm: 1.859 1.844 0.235
2024-12-02-07:19:42-root-INFO: grad norm: 1.795 1.780 0.228
2024-12-02-07:19:42-root-INFO: grad norm: 1.755 1.741 0.218
2024-12-02-07:19:43-root-INFO: Loss Change: 23.577 -> 22.161
2024-12-02-07:19:43-root-INFO: Regularization Change: 0.000 -> 2.045
2024-12-02-07:19:43-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-07:19:43-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-07:19:43-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-07:19:43-root-INFO: grad norm: 2.700 2.641 0.561
2024-12-02-07:19:43-root-INFO: grad norm: 2.298 2.269 0.364
2024-12-02-07:19:44-root-INFO: grad norm: 2.042 2.023 0.284
2024-12-02-07:19:44-root-INFO: grad norm: 1.896 1.879 0.257
2024-12-02-07:19:45-root-INFO: grad norm: 1.774 1.760 0.229
2024-12-02-07:19:45-root-INFO: grad norm: 1.700 1.685 0.220
2024-12-02-07:19:46-root-INFO: grad norm: 1.638 1.625 0.205
2024-12-02-07:19:46-root-INFO: grad norm: 1.600 1.587 0.203
2024-12-02-07:19:47-root-INFO: Loss Change: 22.208 -> 20.875
2024-12-02-07:19:47-root-INFO: Regularization Change: 0.000 -> 1.900
2024-12-02-07:19:47-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-07:19:47-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-07:19:47-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-07:19:47-root-INFO: grad norm: 2.281 2.233 0.465
2024-12-02-07:19:47-root-INFO: grad norm: 2.013 1.992 0.289
2024-12-02-07:19:48-root-INFO: grad norm: 1.875 1.858 0.254
2024-12-02-07:19:48-root-INFO: grad norm: 1.788 1.774 0.226
2024-12-02-07:19:49-root-INFO: grad norm: 1.707 1.693 0.213
2024-12-02-07:19:49-root-INFO: grad norm: 1.652 1.640 0.204
2024-12-02-07:19:50-root-INFO: grad norm: 1.603 1.591 0.194
2024-12-02-07:19:50-root-INFO: grad norm: 1.570 1.559 0.192
2024-12-02-07:19:51-root-INFO: Loss Change: 20.872 -> 19.736
2024-12-02-07:19:51-root-INFO: Regularization Change: 0.000 -> 1.769
2024-12-02-07:19:51-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-07:19:51-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-07:19:51-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-07:19:51-root-INFO: grad norm: 2.368 2.320 0.479
2024-12-02-07:19:51-root-INFO: grad norm: 2.017 1.993 0.311
2024-12-02-07:19:52-root-INFO: grad norm: 1.806 1.790 0.241
2024-12-02-07:19:52-root-INFO: grad norm: 1.680 1.665 0.222
2024-12-02-07:19:53-root-INFO: grad norm: 1.578 1.566 0.196
2024-12-02-07:19:53-root-INFO: grad norm: 1.511 1.498 0.192
2024-12-02-07:19:54-root-INFO: grad norm: 1.454 1.443 0.176
2024-12-02-07:19:54-root-INFO: grad norm: 1.416 1.405 0.177
2024-12-02-07:19:55-root-INFO: Loss Change: 19.788 -> 18.655
2024-12-02-07:19:55-root-INFO: Regularization Change: 0.000 -> 1.696
2024-12-02-07:19:55-root-INFO: Undo step: 35
2024-12-02-07:19:55-root-INFO: Undo step: 36
2024-12-02-07:19:55-root-INFO: Undo step: 37
2024-12-02-07:19:55-root-INFO: Undo step: 38
2024-12-02-07:19:55-root-INFO: Undo step: 39
2024-12-02-07:19:55-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-07:19:55-root-INFO: grad norm: 16.196 16.111 1.654
2024-12-02-07:19:55-root-INFO: grad norm: 9.820 9.754 1.138
2024-12-02-07:19:56-root-INFO: grad norm: 7.123 7.060 0.942
2024-12-02-07:19:56-root-INFO: grad norm: 5.294 5.247 0.706
2024-12-02-07:19:57-root-INFO: grad norm: 4.270 4.230 0.584
2024-12-02-07:19:57-root-INFO: grad norm: 3.680 3.646 0.497
2024-12-02-07:19:58-root-INFO: grad norm: 3.388 3.361 0.427
2024-12-02-07:19:58-root-INFO: grad norm: 3.287 3.263 0.401
2024-12-02-07:19:59-root-INFO: Loss Change: 107.483 -> 34.663
2024-12-02-07:19:59-root-INFO: Regularization Change: 0.000 -> 83.496
2024-12-02-07:19:59-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-07:19:59-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-07:19:59-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-07:19:59-root-INFO: grad norm: 3.721 3.685 0.522
2024-12-02-07:20:00-root-INFO: grad norm: 3.479 3.455 0.408
2024-12-02-07:20:00-root-INFO: grad norm: 3.187 3.168 0.348
2024-12-02-07:20:01-root-INFO: grad norm: 3.005 2.986 0.337
2024-12-02-07:20:01-root-INFO: grad norm: 2.898 2.881 0.308
2024-12-02-07:20:02-root-INFO: grad norm: 2.798 2.781 0.308
2024-12-02-07:20:02-root-INFO: grad norm: 2.711 2.696 0.287
2024-12-02-07:20:03-root-INFO: grad norm: 2.637 2.621 0.291
2024-12-02-07:20:03-root-INFO: Loss Change: 34.502 -> 28.059
2024-12-02-07:20:03-root-INFO: Regularization Change: 0.000 -> 10.749
2024-12-02-07:20:03-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-07:20:03-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-07:20:03-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-07:20:03-root-INFO: grad norm: 3.247 3.194 0.586
2024-12-02-07:20:04-root-INFO: grad norm: 2.897 2.873 0.371
2024-12-02-07:20:04-root-INFO: grad norm: 2.661 2.641 0.325
2024-12-02-07:20:05-root-INFO: grad norm: 2.547 2.530 0.296
2024-12-02-07:20:05-root-INFO: grad norm: 2.536 2.519 0.294
2024-12-02-07:20:06-root-INFO: grad norm: 2.512 2.495 0.289
2024-12-02-07:20:07-root-INFO: grad norm: 2.491 2.475 0.288
2024-12-02-07:20:07-root-INFO: grad norm: 2.470 2.454 0.285
2024-12-02-07:20:07-root-INFO: Loss Change: 27.987 -> 24.722
2024-12-02-07:20:07-root-INFO: Regularization Change: 0.000 -> 5.377
2024-12-02-07:20:07-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-07:20:07-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-07:20:08-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-07:20:08-root-INFO: grad norm: 3.347 3.287 0.633
2024-12-02-07:20:08-root-INFO: grad norm: 2.920 2.891 0.412
2024-12-02-07:20:09-root-INFO: grad norm: 2.587 2.566 0.329
2024-12-02-07:20:09-root-INFO: grad norm: 2.402 2.383 0.297
2024-12-02-07:20:10-root-INFO: grad norm: 2.289 2.272 0.273
2024-12-02-07:20:10-root-INFO: grad norm: 2.221 2.205 0.265
2024-12-02-07:20:11-root-INFO: grad norm: 2.161 2.146 0.252
2024-12-02-07:20:11-root-INFO: grad norm: 2.127 2.112 0.251
2024-12-02-07:20:11-root-INFO: Loss Change: 24.779 -> 22.395
2024-12-02-07:20:11-root-INFO: Regularization Change: 0.000 -> 3.629
2024-12-02-07:20:11-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-07:20:11-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-07:20:11-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-07:20:12-root-INFO: grad norm: 2.748 2.697 0.525
2024-12-02-07:20:12-root-INFO: grad norm: 2.485 2.463 0.331
2024-12-02-07:20:13-root-INFO: grad norm: 2.308 2.289 0.293
2024-12-02-07:20:13-root-INFO: grad norm: 2.198 2.182 0.264
2024-12-02-07:20:14-root-INFO: grad norm: 2.114 2.099 0.250
2024-12-02-07:20:14-root-INFO: grad norm: 2.060 2.045 0.243
2024-12-02-07:20:15-root-INFO: grad norm: 2.012 1.998 0.233
2024-12-02-07:20:15-root-INFO: grad norm: 1.982 1.968 0.232
2024-12-02-07:20:16-root-INFO: Loss Change: 22.388 -> 20.656
2024-12-02-07:20:16-root-INFO: Regularization Change: 0.000 -> 2.789
2024-12-02-07:20:16-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-07:20:16-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-07:20:16-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-07:20:16-root-INFO: grad norm: 2.745 2.691 0.538
2024-12-02-07:20:16-root-INFO: grad norm: 2.387 2.362 0.345
2024-12-02-07:20:17-root-INFO: grad norm: 2.136 2.118 0.270
2024-12-02-07:20:17-root-INFO: grad norm: 1.988 1.973 0.250
2024-12-02-07:20:18-root-INFO: grad norm: 1.898 1.885 0.223
2024-12-02-07:20:18-root-INFO: grad norm: 1.839 1.826 0.222
2024-12-02-07:20:19-root-INFO: grad norm: 1.793 1.781 0.206
2024-12-02-07:20:19-root-INFO: grad norm: 1.765 1.752 0.211
2024-12-02-07:20:20-root-INFO: Loss Change: 20.717 -> 19.193
2024-12-02-07:20:20-root-INFO: Regularization Change: 0.000 -> 2.341
2024-12-02-07:20:20-root-INFO: Undo step: 35
2024-12-02-07:20:20-root-INFO: Undo step: 36
2024-12-02-07:20:20-root-INFO: Undo step: 37
2024-12-02-07:20:20-root-INFO: Undo step: 38
2024-12-02-07:20:20-root-INFO: Undo step: 39
2024-12-02-07:20:20-root-INFO: step: 40 lr_xt 0.21110784
2024-12-02-07:20:20-root-INFO: grad norm: 17.422 17.328 1.805
2024-12-02-07:20:21-root-INFO: grad norm: 10.145 10.080 1.150
2024-12-02-07:20:21-root-INFO: grad norm: 7.420 7.354 0.994
2024-12-02-07:20:21-root-INFO: grad norm: 5.689 5.640 0.746
2024-12-02-07:20:22-root-INFO: grad norm: 4.630 4.585 0.639
2024-12-02-07:20:22-root-INFO: grad norm: 3.996 3.958 0.550
2024-12-02-07:20:23-root-INFO: grad norm: 3.589 3.555 0.489
2024-12-02-07:20:23-root-INFO: grad norm: 3.350 3.318 0.458
2024-12-02-07:20:24-root-INFO: Loss Change: 113.683 -> 35.108
2024-12-02-07:20:24-root-INFO: Regularization Change: 0.000 -> 85.831
2024-12-02-07:20:24-root-INFO: Learning rate of xt decay: 0.24197 -> 0.24487.
2024-12-02-07:20:24-root-INFO: Coefficient of regularization decay: 0.00080 -> 0.00081.
2024-12-02-07:20:24-root-INFO: step: 39 lr_xt 0.21503976
2024-12-02-07:20:24-root-INFO: grad norm: 3.069 3.044 0.394
2024-12-02-07:20:24-root-INFO: grad norm: 2.811 2.788 0.356
2024-12-02-07:20:25-root-INFO: grad norm: 2.706 2.685 0.333
2024-12-02-07:20:25-root-INFO: grad norm: 2.669 2.647 0.338
2024-12-02-07:20:26-root-INFO: grad norm: 2.637 2.617 0.322
2024-12-02-07:20:26-root-INFO: grad norm: 2.626 2.605 0.331
2024-12-02-07:20:27-root-INFO: grad norm: 2.616 2.596 0.319
2024-12-02-07:20:27-root-INFO: grad norm: 2.617 2.595 0.331
2024-12-02-07:20:28-root-INFO: Loss Change: 34.751 -> 28.347
2024-12-02-07:20:28-root-INFO: Regularization Change: 0.000 -> 11.028
2024-12-02-07:20:28-root-INFO: Learning rate of xt decay: 0.24487 -> 0.24781.
2024-12-02-07:20:28-root-INFO: Coefficient of regularization decay: 0.00081 -> 0.00082.
2024-12-02-07:20:28-root-INFO: step: 38 lr_xt 0.21900989
2024-12-02-07:20:28-root-INFO: grad norm: 2.436 2.417 0.305
2024-12-02-07:20:28-root-INFO: grad norm: 2.185 2.170 0.251
2024-12-02-07:20:29-root-INFO: grad norm: 2.159 2.145 0.242
2024-12-02-07:20:29-root-INFO: grad norm: 2.202 2.186 0.270
2024-12-02-07:20:30-root-INFO: grad norm: 2.242 2.226 0.263
2024-12-02-07:20:31-root-INFO: grad norm: 2.293 2.275 0.286
2024-12-02-07:20:31-root-INFO: grad norm: 2.338 2.321 0.281
2024-12-02-07:20:31-root-INFO: grad norm: 2.390 2.371 0.302
2024-12-02-07:20:32-root-INFO: Loss Change: 27.983 -> 24.865
2024-12-02-07:20:32-root-INFO: Regularization Change: 0.000 -> 5.555
2024-12-02-07:20:32-root-INFO: Learning rate of xt decay: 0.24781 -> 0.25078.
2024-12-02-07:20:32-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00082.
2024-12-02-07:20:32-root-INFO: step: 37 lr_xt 0.22301766
2024-12-02-07:20:32-root-INFO: grad norm: 2.099 2.080 0.279
2024-12-02-07:20:33-root-INFO: grad norm: 1.832 1.821 0.201
2024-12-02-07:20:33-root-INFO: grad norm: 1.839 1.827 0.207
2024-12-02-07:20:33-root-INFO: grad norm: 1.926 1.913 0.228
2024-12-02-07:20:34-root-INFO: grad norm: 2.005 1.990 0.239
2024-12-02-07:20:34-root-INFO: grad norm: 2.099 2.084 0.253
2024-12-02-07:20:35-root-INFO: grad norm: 2.174 2.158 0.265
2024-12-02-07:20:35-root-INFO: grad norm: 2.251 2.234 0.275
2024-12-02-07:20:36-root-INFO: Loss Change: 24.519 -> 22.498
2024-12-02-07:20:36-root-INFO: Regularization Change: 0.000 -> 3.741
2024-12-02-07:20:36-root-INFO: Learning rate of xt decay: 0.25078 -> 0.25379.
2024-12-02-07:20:36-root-INFO: Coefficient of regularization decay: 0.00082 -> 0.00083.
2024-12-02-07:20:36-root-INFO: step: 36 lr_xt 0.22706247
2024-12-02-07:20:36-root-INFO: grad norm: 2.068 2.056 0.225
2024-12-02-07:20:37-root-INFO: grad norm: 1.909 1.898 0.206
2024-12-02-07:20:37-root-INFO: grad norm: 1.939 1.927 0.216
2024-12-02-07:20:38-root-INFO: grad norm: 2.035 2.021 0.239
2024-12-02-07:20:38-root-INFO: grad norm: 2.103 2.088 0.250
2024-12-02-07:20:39-root-INFO: grad norm: 2.165 2.149 0.258
2024-12-02-07:20:39-root-INFO: grad norm: 2.210 2.194 0.270
2024-12-02-07:20:39-root-INFO: grad norm: 2.250 2.233 0.272
2024-12-02-07:20:40-root-INFO: Loss Change: 22.243 -> 20.724
2024-12-02-07:20:40-root-INFO: Regularization Change: 0.000 -> 2.878
2024-12-02-07:20:40-root-INFO: Learning rate of xt decay: 0.25379 -> 0.25684.
2024-12-02-07:20:40-root-INFO: Coefficient of regularization decay: 0.00083 -> 0.00084.
2024-12-02-07:20:40-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-07:20:40-root-INFO: grad norm: 1.990 1.976 0.233
2024-12-02-07:20:41-root-INFO: grad norm: 1.745 1.736 0.182
2024-12-02-07:20:41-root-INFO: grad norm: 1.750 1.738 0.202
2024-12-02-07:20:42-root-INFO: grad norm: 1.830 1.818 0.209
2024-12-02-07:20:42-root-INFO: grad norm: 1.890 1.877 0.229
2024-12-02-07:20:42-root-INFO: grad norm: 1.949 1.935 0.228
2024-12-02-07:20:43-root-INFO: grad norm: 1.995 1.979 0.245
2024-12-02-07:20:43-root-INFO: grad norm: 2.038 2.023 0.241
2024-12-02-07:20:44-root-INFO: Loss Change: 20.483 -> 19.200
2024-12-02-07:20:44-root-INFO: Regularization Change: 0.000 -> 2.405
2024-12-02-07:20:44-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-07:20:44-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-07:20:44-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-07:20:44-root-INFO: grad norm: 1.790 1.778 0.210
2024-12-02-07:20:44-root-INFO: grad norm: 1.552 1.545 0.152
2024-12-02-07:20:45-root-INFO: grad norm: 1.551 1.541 0.175
2024-12-02-07:20:45-root-INFO: grad norm: 1.624 1.614 0.177
2024-12-02-07:20:46-root-INFO: grad norm: 1.678 1.666 0.201
2024-12-02-07:20:46-root-INFO: grad norm: 1.736 1.725 0.195
2024-12-02-07:20:47-root-INFO: grad norm: 1.781 1.767 0.218
2024-12-02-07:20:47-root-INFO: grad norm: 1.825 1.813 0.208
2024-12-02-07:20:48-root-INFO: Loss Change: 18.953 -> 17.851
2024-12-02-07:20:48-root-INFO: Regularization Change: 0.000 -> 2.089
2024-12-02-07:20:48-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-07:20:48-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-07:20:48-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-07:20:48-root-INFO: grad norm: 1.633 1.618 0.224
2024-12-02-07:20:48-root-INFO: grad norm: 1.372 1.366 0.132
2024-12-02-07:20:49-root-INFO: grad norm: 1.370 1.363 0.143
2024-12-02-07:20:49-root-INFO: grad norm: 1.451 1.443 0.153
2024-12-02-07:20:50-root-INFO: grad norm: 1.509 1.500 0.171
2024-12-02-07:20:50-root-INFO: grad norm: 1.572 1.563 0.171
2024-12-02-07:20:51-root-INFO: grad norm: 1.618 1.607 0.189
2024-12-02-07:20:51-root-INFO: grad norm: 1.662 1.651 0.184
2024-12-02-07:20:52-root-INFO: Loss Change: 17.507 -> 16.527
2024-12-02-07:20:52-root-INFO: Regularization Change: 0.000 -> 1.889
2024-12-02-07:20:52-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-07:20:52-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-07:20:52-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-07:20:52-root-INFO: grad norm: 1.595 1.573 0.267
2024-12-02-07:20:52-root-INFO: grad norm: 1.197 1.192 0.116
2024-12-02-07:20:53-root-INFO: grad norm: 1.174 1.167 0.123
2024-12-02-07:20:53-root-INFO: grad norm: 1.258 1.252 0.126
2024-12-02-07:20:54-root-INFO: grad norm: 1.322 1.313 0.150
2024-12-02-07:20:54-root-INFO: grad norm: 1.404 1.396 0.148
2024-12-02-07:20:55-root-INFO: grad norm: 1.459 1.449 0.171
2024-12-02-07:20:55-root-INFO: grad norm: 1.515 1.506 0.164
2024-12-02-07:20:55-root-INFO: Loss Change: 16.386 -> 15.468
2024-12-02-07:20:55-root-INFO: Regularization Change: 0.000 -> 1.755
2024-12-02-07:20:55-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-07:20:55-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-07:20:56-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-07:20:56-root-INFO: grad norm: 1.490 1.474 0.220
2024-12-02-07:20:56-root-INFO: grad norm: 1.227 1.221 0.115
2024-12-02-07:20:57-root-INFO: grad norm: 1.228 1.221 0.129
2024-12-02-07:20:57-root-INFO: grad norm: 1.316 1.310 0.129
2024-12-02-07:20:58-root-INFO: grad norm: 1.364 1.355 0.151
2024-12-02-07:20:58-root-INFO: grad norm: 1.407 1.400 0.145
2024-12-02-07:20:59-root-INFO: grad norm: 1.437 1.428 0.166
2024-12-02-07:20:59-root-INFO: grad norm: 1.470 1.462 0.155
2024-12-02-07:20:59-root-INFO: Loss Change: 15.292 -> 14.472
2024-12-02-07:20:59-root-INFO: Regularization Change: 0.000 -> 1.615
2024-12-02-07:20:59-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-07:20:59-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-07:21:00-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-07:21:00-root-INFO: grad norm: 1.429 1.408 0.241
2024-12-02-07:21:00-root-INFO: grad norm: 1.096 1.090 0.107
2024-12-02-07:21:01-root-INFO: grad norm: 1.088 1.081 0.117
2024-12-02-07:21:01-root-INFO: grad norm: 1.179 1.174 0.114
2024-12-02-07:21:02-root-INFO: grad norm: 1.233 1.225 0.141
2024-12-02-07:21:02-root-INFO: grad norm: 1.301 1.294 0.132
2024-12-02-07:21:02-root-INFO: grad norm: 1.339 1.330 0.158
2024-12-02-07:21:03-root-INFO: grad norm: 1.376 1.369 0.144
2024-12-02-07:21:03-root-INFO: Loss Change: 14.293 -> 13.521
2024-12-02-07:21:03-root-INFO: Regularization Change: 0.000 -> 1.536
2024-12-02-07:21:03-root-INFO: Undo step: 30
2024-12-02-07:21:03-root-INFO: Undo step: 31
2024-12-02-07:21:03-root-INFO: Undo step: 32
2024-12-02-07:21:03-root-INFO: Undo step: 33
2024-12-02-07:21:03-root-INFO: Undo step: 34
2024-12-02-07:21:04-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-07:21:04-root-INFO: grad norm: 15.754 15.678 1.536
2024-12-02-07:21:04-root-INFO: grad norm: 8.618 8.559 1.007
2024-12-02-07:21:05-root-INFO: grad norm: 6.014 5.963 0.781
2024-12-02-07:21:05-root-INFO: grad norm: 4.702 4.665 0.591
2024-12-02-07:21:06-root-INFO: grad norm: 3.926 3.892 0.514
2024-12-02-07:21:06-root-INFO: grad norm: 3.450 3.423 0.432
2024-12-02-07:21:07-root-INFO: grad norm: 3.120 3.093 0.407
2024-12-02-07:21:07-root-INFO: grad norm: 2.903 2.881 0.358
2024-12-02-07:21:07-root-INFO: Loss Change: 101.700 -> 28.897
2024-12-02-07:21:07-root-INFO: Regularization Change: 0.000 -> 89.421
2024-12-02-07:21:07-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-07:21:07-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-07:21:08-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-07:21:08-root-INFO: grad norm: 2.685 2.663 0.344
2024-12-02-07:21:08-root-INFO: grad norm: 2.378 2.361 0.283
2024-12-02-07:21:09-root-INFO: grad norm: 2.253 2.235 0.282
2024-12-02-07:21:09-root-INFO: grad norm: 2.201 2.186 0.262
2024-12-02-07:21:10-root-INFO: grad norm: 2.168 2.150 0.276
2024-12-02-07:21:10-root-INFO: grad norm: 2.157 2.142 0.254
2024-12-02-07:21:11-root-INFO: grad norm: 2.159 2.141 0.277
2024-12-02-07:21:11-root-INFO: grad norm: 2.174 2.159 0.256
2024-12-02-07:21:11-root-INFO: Loss Change: 28.572 -> 22.513
2024-12-02-07:21:11-root-INFO: Regularization Change: 0.000 -> 11.383
2024-12-02-07:21:11-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-07:21:11-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-07:21:11-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-07:21:12-root-INFO: grad norm: 2.058 2.040 0.269
2024-12-02-07:21:12-root-INFO: grad norm: 1.855 1.844 0.202
2024-12-02-07:21:13-root-INFO: grad norm: 1.850 1.837 0.218
2024-12-02-07:21:13-root-INFO: grad norm: 1.908 1.896 0.215
2024-12-02-07:21:14-root-INFO: grad norm: 1.951 1.937 0.238
2024-12-02-07:21:14-root-INFO: grad norm: 2.000 1.987 0.226
2024-12-02-07:21:14-root-INFO: grad norm: 2.025 2.010 0.251
2024-12-02-07:21:15-root-INFO: grad norm: 2.039 2.025 0.232
2024-12-02-07:21:15-root-INFO: Loss Change: 22.081 -> 19.239
2024-12-02-07:21:15-root-INFO: Regularization Change: 0.000 -> 5.552
2024-12-02-07:21:15-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-07:21:15-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-07:21:15-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-07:21:16-root-INFO: grad norm: 1.937 1.919 0.268
2024-12-02-07:21:16-root-INFO: grad norm: 1.601 1.592 0.168
2024-12-02-07:21:16-root-INFO: grad norm: 1.565 1.555 0.175
2024-12-02-07:21:17-root-INFO: grad norm: 1.594 1.585 0.172
2024-12-02-07:21:17-root-INFO: grad norm: 1.621 1.609 0.190
2024-12-02-07:21:18-root-INFO: grad norm: 1.660 1.650 0.179
2024-12-02-07:21:18-root-INFO: grad norm: 1.680 1.668 0.198
2024-12-02-07:21:19-root-INFO: grad norm: 1.687 1.677 0.183
2024-12-02-07:21:19-root-INFO: Loss Change: 19.031 -> 17.108
2024-12-02-07:21:19-root-INFO: Regularization Change: 0.000 -> 3.654
2024-12-02-07:21:19-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-07:21:19-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-07:21:19-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-07:21:19-root-INFO: grad norm: 1.692 1.676 0.232
2024-12-02-07:21:20-root-INFO: grad norm: 1.467 1.459 0.149
2024-12-02-07:21:20-root-INFO: grad norm: 1.449 1.441 0.152
2024-12-02-07:21:21-root-INFO: grad norm: 1.475 1.467 0.150
2024-12-02-07:21:21-root-INFO: grad norm: 1.491 1.482 0.161
2024-12-02-07:21:22-root-INFO: grad norm: 1.507 1.500 0.153
2024-12-02-07:21:22-root-INFO: grad norm: 1.514 1.505 0.164
2024-12-02-07:21:23-root-INFO: grad norm: 1.511 1.503 0.154
2024-12-02-07:21:23-root-INFO: Loss Change: 16.900 -> 15.490
2024-12-02-07:21:23-root-INFO: Regularization Change: 0.000 -> 2.732
2024-12-02-07:21:23-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-07:21:23-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-07:21:23-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-07:21:23-root-INFO: grad norm: 1.759 1.735 0.285
2024-12-02-07:21:24-root-INFO: grad norm: 1.444 1.433 0.175
2024-12-02-07:21:24-root-INFO: grad norm: 1.304 1.297 0.138
2024-12-02-07:21:25-root-INFO: grad norm: 1.274 1.267 0.138
2024-12-02-07:21:25-root-INFO: grad norm: 1.257 1.250 0.133
2024-12-02-07:21:26-root-INFO: grad norm: 1.270 1.263 0.132
2024-12-02-07:21:26-root-INFO: grad norm: 1.272 1.265 0.133
2024-12-02-07:21:27-root-INFO: grad norm: 1.277 1.270 0.130
2024-12-02-07:21:27-root-INFO: Loss Change: 15.329 -> 14.134
2024-12-02-07:21:27-root-INFO: Regularization Change: 0.000 -> 2.236
2024-12-02-07:21:27-root-INFO: Undo step: 30
2024-12-02-07:21:27-root-INFO: Undo step: 31
2024-12-02-07:21:27-root-INFO: Undo step: 32
2024-12-02-07:21:27-root-INFO: Undo step: 33
2024-12-02-07:21:27-root-INFO: Undo step: 34
2024-12-02-07:21:27-root-INFO: step: 35 lr_xt 0.23114370
2024-12-02-07:21:27-root-INFO: grad norm: 15.557 15.473 1.618
2024-12-02-07:21:28-root-INFO: grad norm: 8.367 8.299 1.062
2024-12-02-07:21:28-root-INFO: grad norm: 5.806 5.746 0.830
2024-12-02-07:21:29-root-INFO: grad norm: 4.527 4.478 0.666
2024-12-02-07:21:29-root-INFO: grad norm: 3.755 3.710 0.578
2024-12-02-07:21:30-root-INFO: grad norm: 3.241 3.205 0.485
2024-12-02-07:21:30-root-INFO: grad norm: 2.874 2.840 0.442
2024-12-02-07:21:30-root-INFO: grad norm: 2.604 2.575 0.383
2024-12-02-07:21:31-root-INFO: Loss Change: 99.163 -> 28.793
2024-12-02-07:21:31-root-INFO: Regularization Change: 0.000 -> 86.998
2024-12-02-07:21:31-root-INFO: Learning rate of xt decay: 0.25684 -> 0.25992.
2024-12-02-07:21:31-root-INFO: Coefficient of regularization decay: 0.00084 -> 0.00085.
2024-12-02-07:21:31-root-INFO: step: 34 lr_xt 0.23526068
2024-12-02-07:21:31-root-INFO: grad norm: 2.473 2.444 0.381
2024-12-02-07:21:32-root-INFO: grad norm: 2.159 2.136 0.314
2024-12-02-07:21:32-root-INFO: grad norm: 1.997 1.976 0.286
2024-12-02-07:21:32-root-INFO: grad norm: 1.882 1.863 0.265
2024-12-02-07:21:33-root-INFO: grad norm: 1.801 1.783 0.253
2024-12-02-07:21:33-root-INFO: grad norm: 1.754 1.738 0.236
2024-12-02-07:21:34-root-INFO: grad norm: 1.750 1.735 0.231
2024-12-02-07:21:34-root-INFO: grad norm: 1.782 1.769 0.220
2024-12-02-07:21:35-root-INFO: Loss Change: 28.457 -> 22.416
2024-12-02-07:21:35-root-INFO: Regularization Change: 0.000 -> 11.307
2024-12-02-07:21:35-root-INFO: Learning rate of xt decay: 0.25992 -> 0.26304.
2024-12-02-07:21:35-root-INFO: Coefficient of regularization decay: 0.00085 -> 0.00086.
2024-12-02-07:21:35-root-INFO: step: 33 lr_xt 0.23941272
2024-12-02-07:21:35-root-INFO: grad norm: 2.298 2.266 0.379
2024-12-02-07:21:36-root-INFO: grad norm: 2.209 2.190 0.288
2024-12-02-07:21:36-root-INFO: grad norm: 2.295 2.281 0.251
2024-12-02-07:21:36-root-INFO: grad norm: 2.249 2.233 0.274
2024-12-02-07:21:37-root-INFO: grad norm: 2.139 2.126 0.238
2024-12-02-07:21:37-root-INFO: grad norm: 2.077 2.060 0.262
2024-12-02-07:21:38-root-INFO: grad norm: 2.005 1.992 0.233
2024-12-02-07:21:38-root-INFO: grad norm: 2.010 1.993 0.262
2024-12-02-07:21:39-root-INFO: Loss Change: 22.076 -> 19.131
2024-12-02-07:21:39-root-INFO: Regularization Change: 0.000 -> 5.679
2024-12-02-07:21:39-root-INFO: Learning rate of xt decay: 0.26304 -> 0.26620.
2024-12-02-07:21:39-root-INFO: Coefficient of regularization decay: 0.00086 -> 0.00087.
2024-12-02-07:21:39-root-INFO: step: 32 lr_xt 0.24359912
2024-12-02-07:21:39-root-INFO: grad norm: 3.016 2.966 0.547
2024-12-02-07:21:40-root-INFO: grad norm: 2.669 2.637 0.410
2024-12-02-07:21:40-root-INFO: grad norm: 2.252 2.232 0.296
2024-12-02-07:21:41-root-INFO: grad norm: 2.050 2.029 0.292
2024-12-02-07:21:41-root-INFO: grad norm: 1.914 1.898 0.242
2024-12-02-07:21:42-root-INFO: grad norm: 1.842 1.824 0.256
2024-12-02-07:21:42-root-INFO: grad norm: 1.781 1.768 0.222
2024-12-02-07:21:43-root-INFO: grad norm: 1.744 1.727 0.241
2024-12-02-07:21:43-root-INFO: Loss Change: 19.192 -> 16.974
2024-12-02-07:21:43-root-INFO: Regularization Change: 0.000 -> 3.746
2024-12-02-07:21:43-root-INFO: Learning rate of xt decay: 0.26620 -> 0.26939.
2024-12-02-07:21:43-root-INFO: Coefficient of regularization decay: 0.00087 -> 0.00088.
2024-12-02-07:21:43-root-INFO: step: 31 lr_xt 0.24781911
2024-12-02-07:21:43-root-INFO: grad norm: 2.438 2.394 0.461
2024-12-02-07:21:44-root-INFO: grad norm: 2.143 2.117 0.329
2024-12-02-07:21:44-root-INFO: grad norm: 1.898 1.881 0.253
2024-12-02-07:21:45-root-INFO: grad norm: 1.769 1.751 0.249
2024-12-02-07:21:45-root-INFO: grad norm: 1.671 1.658 0.212
2024-12-02-07:21:46-root-INFO: grad norm: 1.610 1.595 0.222
2024-12-02-07:21:46-root-INFO: grad norm: 1.560 1.548 0.195
2024-12-02-07:21:46-root-INFO: grad norm: 1.527 1.512 0.209
2024-12-02-07:21:47-root-INFO: Loss Change: 16.934 -> 15.320
2024-12-02-07:21:47-root-INFO: Regularization Change: 0.000 -> 2.793
2024-12-02-07:21:47-root-INFO: Learning rate of xt decay: 0.26939 -> 0.27262.
2024-12-02-07:21:47-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00088.
2024-12-02-07:21:47-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-07:21:47-root-INFO: grad norm: 2.384 2.337 0.468
2024-12-02-07:21:48-root-INFO: grad norm: 2.002 1.976 0.319
2024-12-02-07:21:48-root-INFO: grad norm: 1.673 1.657 0.228
2024-12-02-07:21:49-root-INFO: grad norm: 1.513 1.497 0.221
2024-12-02-07:21:49-root-INFO: grad norm: 1.418 1.407 0.181
2024-12-02-07:21:50-root-INFO: grad norm: 1.358 1.344 0.192
2024-12-02-07:21:50-root-INFO: grad norm: 1.310 1.299 0.163
2024-12-02-07:21:50-root-INFO: grad norm: 1.279 1.266 0.178
2024-12-02-07:21:51-root-INFO: Loss Change: 15.309 -> 13.945
2024-12-02-07:21:51-root-INFO: Regularization Change: 0.000 -> 2.268
2024-12-02-07:21:51-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-07:21:51-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-07:21:51-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-07:21:51-root-INFO: grad norm: 2.146 2.098 0.454
2024-12-02-07:21:52-root-INFO: grad norm: 1.759 1.737 0.280
2024-12-02-07:21:52-root-INFO: grad norm: 1.476 1.461 0.206
2024-12-02-07:21:52-root-INFO: grad norm: 1.348 1.335 0.192
2024-12-02-07:21:53-root-INFO: grad norm: 1.269 1.258 0.162
2024-12-02-07:21:54-root-INFO: grad norm: 1.217 1.205 0.167
2024-12-02-07:21:54-root-INFO: grad norm: 1.174 1.165 0.145
2024-12-02-07:21:54-root-INFO: grad norm: 1.148 1.137 0.155
2024-12-02-07:21:55-root-INFO: Loss Change: 13.963 -> 12.809
2024-12-02-07:21:55-root-INFO: Regularization Change: 0.000 -> 1.941
2024-12-02-07:21:55-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-07:21:55-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-07:21:55-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-07:21:55-root-INFO: grad norm: 2.151 2.103 0.451
2024-12-02-07:21:56-root-INFO: grad norm: 1.696 1.674 0.269
2024-12-02-07:21:56-root-INFO: grad norm: 1.323 1.310 0.185
2024-12-02-07:21:57-root-INFO: grad norm: 1.172 1.160 0.169
2024-12-02-07:21:57-root-INFO: grad norm: 1.116 1.107 0.140
2024-12-02-07:21:57-root-INFO: grad norm: 1.078 1.068 0.148
2024-12-02-07:21:58-root-INFO: grad norm: 1.052 1.044 0.126
2024-12-02-07:21:58-root-INFO: grad norm: 1.036 1.027 0.139
2024-12-02-07:21:59-root-INFO: Loss Change: 12.755 -> 11.711
2024-12-02-07:21:59-root-INFO: Regularization Change: 0.000 -> 1.715
2024-12-02-07:21:59-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-07:21:59-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-07:21:59-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-07:21:59-root-INFO: grad norm: 1.869 1.828 0.386
2024-12-02-07:22:00-root-INFO: grad norm: 1.499 1.480 0.237
2024-12-02-07:22:00-root-INFO: grad norm: 1.188 1.177 0.166
2024-12-02-07:22:00-root-INFO: grad norm: 1.077 1.065 0.156
2024-12-02-07:22:01-root-INFO: grad norm: 1.041 1.033 0.129
2024-12-02-07:22:02-root-INFO: grad norm: 1.011 1.002 0.139
2024-12-02-07:22:02-root-INFO: grad norm: 0.983 0.976 0.117
2024-12-02-07:22:03-root-INFO: grad norm: 0.972 0.963 0.130
2024-12-02-07:22:03-root-INFO: Loss Change: 11.730 -> 10.848
2024-12-02-07:22:03-root-INFO: Regularization Change: 0.000 -> 1.526
2024-12-02-07:22:03-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-07:22:03-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-07:22:03-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-07:22:03-root-INFO: grad norm: 1.967 1.922 0.417
2024-12-02-07:22:04-root-INFO: grad norm: 1.500 1.481 0.239
2024-12-02-07:22:04-root-INFO: grad norm: 1.125 1.115 0.155
2024-12-02-07:22:05-root-INFO: grad norm: 0.990 0.980 0.142
2024-12-02-07:22:05-root-INFO: grad norm: 0.965 0.958 0.116
2024-12-02-07:22:06-root-INFO: grad norm: 0.952 0.943 0.128
2024-12-02-07:22:06-root-INFO: grad norm: 0.958 0.952 0.109
2024-12-02-07:22:07-root-INFO: grad norm: 0.957 0.948 0.124
2024-12-02-07:22:07-root-INFO: Loss Change: 10.886 -> 10.039
2024-12-02-07:22:07-root-INFO: Regularization Change: 0.000 -> 1.420
2024-12-02-07:22:07-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-07:22:07-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-07:22:07-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-07:22:07-root-INFO: grad norm: 1.883 1.840 0.399
2024-12-02-07:22:08-root-INFO: grad norm: 1.430 1.414 0.215
2024-12-02-07:22:08-root-INFO: grad norm: 1.045 1.035 0.144
2024-12-02-07:22:09-root-INFO: grad norm: 0.928 0.919 0.126
2024-12-02-07:22:09-root-INFO: grad norm: 0.924 0.918 0.106
2024-12-02-07:22:10-root-INFO: grad norm: 0.912 0.905 0.115
2024-12-02-07:22:10-root-INFO: grad norm: 0.907 0.902 0.098
2024-12-02-07:22:11-root-INFO: grad norm: 0.907 0.900 0.112
2024-12-02-07:22:11-root-INFO: Loss Change: 10.002 -> 9.215
2024-12-02-07:22:11-root-INFO: Regularization Change: 0.000 -> 1.331
2024-12-02-07:22:11-root-INFO: Undo step: 25
2024-12-02-07:22:11-root-INFO: Undo step: 26
2024-12-02-07:22:11-root-INFO: Undo step: 27
2024-12-02-07:22:11-root-INFO: Undo step: 28
2024-12-02-07:22:11-root-INFO: Undo step: 29
2024-12-02-07:22:11-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-07:22:11-root-INFO: grad norm: 14.407 14.346 1.331
2024-12-02-07:22:12-root-INFO: grad norm: 7.960 7.909 0.902
2024-12-02-07:22:12-root-INFO: grad norm: 5.871 5.833 0.661
2024-12-02-07:22:13-root-INFO: grad norm: 4.626 4.594 0.546
2024-12-02-07:22:13-root-INFO: grad norm: 3.695 3.669 0.434
2024-12-02-07:22:14-root-INFO: grad norm: 3.108 3.085 0.377
2024-12-02-07:22:14-root-INFO: grad norm: 2.840 2.820 0.333
2024-12-02-07:22:15-root-INFO: grad norm: 2.651 2.633 0.310
2024-12-02-07:22:15-root-INFO: Loss Change: 91.036 -> 22.906
2024-12-02-07:22:15-root-INFO: Regularization Change: 0.000 -> 91.665
2024-12-02-07:22:15-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-07:22:15-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-07:22:15-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-07:22:15-root-INFO: grad norm: 2.954 2.922 0.431
2024-12-02-07:22:16-root-INFO: grad norm: 2.663 2.644 0.316
2024-12-02-07:22:16-root-INFO: grad norm: 2.426 2.413 0.257
2024-12-02-07:22:17-root-INFO: grad norm: 2.283 2.269 0.250
2024-12-02-07:22:18-root-INFO: grad norm: 2.179 2.168 0.221
2024-12-02-07:22:18-root-INFO: grad norm: 2.101 2.090 0.220
2024-12-02-07:22:18-root-INFO: grad norm: 2.025 2.015 0.199
2024-12-02-07:22:19-root-INFO: grad norm: 1.963 1.953 0.199
2024-12-02-07:22:19-root-INFO: Loss Change: 22.717 -> 16.981
2024-12-02-07:22:19-root-INFO: Regularization Change: 0.000 -> 11.430
2024-12-02-07:22:19-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-07:22:19-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-07:22:19-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-07:22:20-root-INFO: grad norm: 2.357 2.325 0.389
2024-12-02-07:22:20-root-INFO: grad norm: 1.955 1.940 0.240
2024-12-02-07:22:21-root-INFO: grad norm: 1.670 1.661 0.176
2024-12-02-07:22:21-root-INFO: grad norm: 1.568 1.559 0.168
2024-12-02-07:22:22-root-INFO: grad norm: 1.516 1.509 0.147
2024-12-02-07:22:22-root-INFO: grad norm: 1.485 1.478 0.149
2024-12-02-07:22:23-root-INFO: grad norm: 1.459 1.452 0.137
2024-12-02-07:22:23-root-INFO: grad norm: 1.427 1.420 0.139
2024-12-02-07:22:23-root-INFO: Loss Change: 16.718 -> 13.999
2024-12-02-07:22:23-root-INFO: Regularization Change: 0.000 -> 5.305
2024-12-02-07:22:23-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-07:22:23-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-07:22:24-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-07:22:24-root-INFO: grad norm: 1.859 1.831 0.318
2024-12-02-07:22:24-root-INFO: grad norm: 1.523 1.512 0.183
2024-12-02-07:22:25-root-INFO: grad norm: 1.316 1.309 0.133
2024-12-02-07:22:25-root-INFO: grad norm: 1.296 1.290 0.126
2024-12-02-07:22:26-root-INFO: grad norm: 1.254 1.249 0.114
2024-12-02-07:22:26-root-INFO: grad norm: 1.190 1.184 0.114
2024-12-02-07:22:27-root-INFO: grad norm: 1.174 1.169 0.108
2024-12-02-07:22:27-root-INFO: grad norm: 1.233 1.228 0.109
2024-12-02-07:22:27-root-INFO: Loss Change: 13.903 -> 12.242
2024-12-02-07:22:27-root-INFO: Regularization Change: 0.000 -> 3.319
2024-12-02-07:22:27-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-07:22:28-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-07:22:28-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-07:22:28-root-INFO: grad norm: 1.753 1.721 0.331
2024-12-02-07:22:28-root-INFO: grad norm: 1.326 1.315 0.169
2024-12-02-07:22:29-root-INFO: grad norm: 1.091 1.085 0.116
2024-12-02-07:22:29-root-INFO: grad norm: 1.049 1.044 0.108
2024-12-02-07:22:30-root-INFO: grad norm: 1.041 1.037 0.094
2024-12-02-07:22:30-root-INFO: grad norm: 1.135 1.131 0.096
2024-12-02-07:22:31-root-INFO: grad norm: 1.095 1.091 0.094
2024-12-02-07:22:31-root-INFO: grad norm: 0.984 0.980 0.092
2024-12-02-07:22:31-root-INFO: Loss Change: 12.143 -> 10.866
2024-12-02-07:22:31-root-INFO: Regularization Change: 0.000 -> 2.437
2024-12-02-07:22:31-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-07:22:31-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-07:22:32-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-07:22:32-root-INFO: grad norm: 1.555 1.522 0.319
2024-12-02-07:22:32-root-INFO: grad norm: 1.157 1.148 0.147
2024-12-02-07:22:33-root-INFO: grad norm: 0.939 0.934 0.100
2024-12-02-07:22:33-root-INFO: grad norm: 0.895 0.890 0.091
2024-12-02-07:22:34-root-INFO: grad norm: 0.896 0.893 0.076
2024-12-02-07:22:34-root-INFO: grad norm: 1.091 1.088 0.080
2024-12-02-07:22:35-root-INFO: grad norm: 0.996 0.993 0.079
2024-12-02-07:22:35-root-INFO: grad norm: 0.855 0.851 0.078
2024-12-02-07:22:35-root-INFO: Loss Change: 10.707 -> 9.700
2024-12-02-07:22:35-root-INFO: Regularization Change: 0.000 -> 1.942
2024-12-02-07:22:35-root-INFO: Undo step: 25
2024-12-02-07:22:35-root-INFO: Undo step: 26
2024-12-02-07:22:35-root-INFO: Undo step: 27
2024-12-02-07:22:35-root-INFO: Undo step: 28
2024-12-02-07:22:35-root-INFO: Undo step: 29
2024-12-02-07:22:36-root-INFO: step: 30 lr_xt 0.25207194
2024-12-02-07:22:36-root-INFO: grad norm: 14.145 14.097 1.173
2024-12-02-07:22:36-root-INFO: grad norm: 8.017 7.981 0.762
2024-12-02-07:22:37-root-INFO: grad norm: 5.590 5.554 0.632
2024-12-02-07:22:37-root-INFO: grad norm: 4.315 4.288 0.477
2024-12-02-07:22:38-root-INFO: grad norm: 3.624 3.598 0.436
2024-12-02-07:22:38-root-INFO: grad norm: 3.163 3.142 0.364
2024-12-02-07:22:39-root-INFO: grad norm: 2.855 2.834 0.349
2024-12-02-07:22:39-root-INFO: grad norm: 2.614 2.596 0.305
2024-12-02-07:22:39-root-INFO: Loss Change: 88.682 -> 23.188
2024-12-02-07:22:39-root-INFO: Regularization Change: 0.000 -> 87.487
2024-12-02-07:22:39-root-INFO: Learning rate of xt decay: 0.27262 -> 0.27589.
2024-12-02-07:22:39-root-INFO: Coefficient of regularization decay: 0.00088 -> 0.00089.
2024-12-02-07:22:40-root-INFO: step: 29 lr_xt 0.25635679
2024-12-02-07:22:40-root-INFO: grad norm: 2.417 2.397 0.307
2024-12-02-07:22:40-root-INFO: grad norm: 2.093 2.079 0.240
2024-12-02-07:22:41-root-INFO: grad norm: 1.936 1.923 0.222
2024-12-02-07:22:41-root-INFO: grad norm: 1.821 1.808 0.210
2024-12-02-07:22:42-root-INFO: grad norm: 1.726 1.714 0.200
2024-12-02-07:22:42-root-INFO: grad norm: 1.648 1.638 0.189
2024-12-02-07:22:43-root-INFO: grad norm: 1.583 1.572 0.183
2024-12-02-07:22:43-root-INFO: grad norm: 1.527 1.518 0.173
2024-12-02-07:22:43-root-INFO: Loss Change: 22.835 -> 17.078
2024-12-02-07:22:43-root-INFO: Regularization Change: 0.000 -> 11.579
2024-12-02-07:22:43-root-INFO: Learning rate of xt decay: 0.27589 -> 0.27921.
2024-12-02-07:22:43-root-INFO: Coefficient of regularization decay: 0.00089 -> 0.00090.
2024-12-02-07:22:43-root-INFO: step: 28 lr_xt 0.26067283
2024-12-02-07:22:44-root-INFO: grad norm: 1.780 1.755 0.296
2024-12-02-07:22:44-root-INFO: grad norm: 1.459 1.448 0.180
2024-12-02-07:22:45-root-INFO: grad norm: 1.344 1.335 0.148
2024-12-02-07:22:45-root-INFO: grad norm: 1.280 1.271 0.151
2024-12-02-07:22:46-root-INFO: grad norm: 1.237 1.230 0.133
2024-12-02-07:22:46-root-INFO: grad norm: 1.202 1.194 0.140
2024-12-02-07:22:46-root-INFO: grad norm: 1.177 1.170 0.125
2024-12-02-07:22:47-root-INFO: grad norm: 1.160 1.152 0.133
2024-12-02-07:22:47-root-INFO: Loss Change: 16.729 -> 14.024
2024-12-02-07:22:47-root-INFO: Regularization Change: 0.000 -> 5.480
2024-12-02-07:22:47-root-INFO: Learning rate of xt decay: 0.27921 -> 0.28256.
2024-12-02-07:22:47-root-INFO: Coefficient of regularization decay: 0.00090 -> 0.00091.
2024-12-02-07:22:47-root-INFO: step: 27 lr_xt 0.26501920
2024-12-02-07:22:48-root-INFO: grad norm: 1.661 1.634 0.298
2024-12-02-07:22:48-root-INFO: grad norm: 1.332 1.319 0.184
2024-12-02-07:22:49-root-INFO: grad norm: 1.127 1.118 0.135
2024-12-02-07:22:49-root-INFO: grad norm: 1.045 1.036 0.138
2024-12-02-07:22:49-root-INFO: grad norm: 1.062 1.055 0.116
2024-12-02-07:22:50-root-INFO: grad norm: 1.041 1.033 0.130
2024-12-02-07:22:50-root-INFO: grad norm: 1.004 0.998 0.110
2024-12-02-07:22:51-root-INFO: grad norm: 0.996 0.988 0.125
2024-12-02-07:22:51-root-INFO: Loss Change: 13.908 -> 12.189
2024-12-02-07:22:51-root-INFO: Regularization Change: 0.000 -> 3.453
2024-12-02-07:22:51-root-INFO: Learning rate of xt decay: 0.28256 -> 0.28595.
2024-12-02-07:22:51-root-INFO: Coefficient of regularization decay: 0.00091 -> 0.00092.
2024-12-02-07:22:51-root-INFO: step: 26 lr_xt 0.26939500
2024-12-02-07:22:52-root-INFO: grad norm: 1.883 1.849 0.358
2024-12-02-07:22:52-root-INFO: grad norm: 1.386 1.371 0.204
2024-12-02-07:22:52-root-INFO: grad norm: 1.126 1.117 0.138
2024-12-02-07:22:53-root-INFO: grad norm: 1.031 1.021 0.140
2024-12-02-07:22:53-root-INFO: grad norm: 1.051 1.044 0.119
2024-12-02-07:22:54-root-INFO: grad norm: 1.084 1.075 0.137
2024-12-02-07:22:54-root-INFO: grad norm: 1.210 1.204 0.126
2024-12-02-07:22:55-root-INFO: grad norm: 1.207 1.198 0.148
2024-12-02-07:22:55-root-INFO: Loss Change: 12.117 -> 10.829
2024-12-02-07:22:55-root-INFO: Regularization Change: 0.000 -> 2.530
2024-12-02-07:22:55-root-INFO: Learning rate of xt decay: 0.28595 -> 0.28938.
2024-12-02-07:22:55-root-INFO: Coefficient of regularization decay: 0.00092 -> 0.00093.
2024-12-02-07:22:55-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-07:22:56-root-INFO: grad norm: 2.011 1.972 0.394
2024-12-02-07:22:56-root-INFO: grad norm: 1.588 1.572 0.228
2024-12-02-07:22:56-root-INFO: grad norm: 1.235 1.225 0.158
2024-12-02-07:22:57-root-INFO: grad norm: 1.142 1.132 0.147
2024-12-02-07:22:57-root-INFO: grad norm: 1.196 1.190 0.127
2024-12-02-07:22:58-root-INFO: grad norm: 1.121 1.112 0.137
2024-12-02-07:22:58-root-INFO: grad norm: 0.991 0.985 0.110
2024-12-02-07:22:59-root-INFO: grad norm: 0.962 0.954 0.121
2024-12-02-07:22:59-root-INFO: Loss Change: 10.742 -> 9.635
2024-12-02-07:22:59-root-INFO: Regularization Change: 0.000 -> 2.009
2024-12-02-07:22:59-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-07:22:59-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-07:22:59-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-07:22:59-root-INFO: grad norm: 2.218 2.181 0.403
2024-12-02-07:23:00-root-INFO: grad norm: 1.597 1.580 0.234
2024-12-02-07:23:00-root-INFO: grad norm: 1.156 1.147 0.140
2024-12-02-07:23:01-root-INFO: grad norm: 1.013 1.004 0.131
2024-12-02-07:23:01-root-INFO: grad norm: 0.923 0.917 0.105
2024-12-02-07:23:02-root-INFO: grad norm: 0.901 0.894 0.114
2024-12-02-07:23:02-root-INFO: grad norm: 0.986 0.981 0.099
2024-12-02-07:23:03-root-INFO: grad norm: 0.996 0.989 0.121
2024-12-02-07:23:03-root-INFO: Loss Change: 9.724 -> 8.752
2024-12-02-07:23:03-root-INFO: Regularization Change: 0.000 -> 1.693
2024-12-02-07:23:03-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-07:23:03-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-07:23:03-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-07:23:03-root-INFO: grad norm: 1.835 1.800 0.359
2024-12-02-07:23:04-root-INFO: grad norm: 1.384 1.370 0.199
2024-12-02-07:23:04-root-INFO: grad norm: 1.056 1.048 0.129
2024-12-02-07:23:05-root-INFO: grad norm: 0.948 0.940 0.124
2024-12-02-07:23:05-root-INFO: grad norm: 1.030 1.025 0.101
2024-12-02-07:23:06-root-INFO: grad norm: 1.004 0.997 0.120
2024-12-02-07:23:06-root-INFO: grad norm: 0.911 0.905 0.097
2024-12-02-07:23:07-root-INFO: grad norm: 0.936 0.929 0.114
2024-12-02-07:23:07-root-INFO: Loss Change: 8.695 -> 7.894
2024-12-02-07:23:07-root-INFO: Regularization Change: 0.000 -> 1.463
2024-12-02-07:23:07-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-07:23:07-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-07:23:07-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-07:23:07-root-INFO: grad norm: 2.269 2.236 0.384
2024-12-02-07:23:08-root-INFO: grad norm: 1.479 1.464 0.211
2024-12-02-07:23:08-root-INFO: grad norm: 1.036 1.029 0.121
2024-12-02-07:23:09-root-INFO: grad norm: 0.911 0.904 0.113
2024-12-02-07:23:09-root-INFO: grad norm: 0.825 0.821 0.085
2024-12-02-07:23:10-root-INFO: grad norm: 0.766 0.761 0.089
2024-12-02-07:23:10-root-INFO: grad norm: 0.744 0.740 0.073
2024-12-02-07:23:10-root-INFO: grad norm: 0.756 0.751 0.088
2024-12-02-07:23:11-root-INFO: Loss Change: 7.979 -> 7.186
2024-12-02-07:23:11-root-INFO: Regularization Change: 0.000 -> 1.307
2024-12-02-07:23:11-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-07:23:11-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-07:23:11-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-07:23:11-root-INFO: grad norm: 1.770 1.740 0.327
2024-12-02-07:23:12-root-INFO: grad norm: 1.248 1.236 0.176
2024-12-02-07:23:12-root-INFO: grad norm: 0.958 0.953 0.102
2024-12-02-07:23:13-root-INFO: grad norm: 0.821 0.815 0.099
2024-12-02-07:23:13-root-INFO: grad norm: 0.761 0.758 0.073
2024-12-02-07:23:13-root-INFO: grad norm: 0.776 0.770 0.090
2024-12-02-07:23:14-root-INFO: grad norm: 1.015 1.012 0.077
2024-12-02-07:23:14-root-INFO: grad norm: 0.901 0.895 0.100
2024-12-02-07:23:15-root-INFO: Loss Change: 7.200 -> 6.537
2024-12-02-07:23:15-root-INFO: Regularization Change: 0.000 -> 1.188
2024-12-02-07:23:15-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-07:23:15-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-07:23:15-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-07:23:15-root-INFO: grad norm: 1.685 1.648 0.352
2024-12-02-07:23:16-root-INFO: grad norm: 1.196 1.184 0.169
2024-12-02-07:23:16-root-INFO: grad norm: 1.336 1.331 0.123
2024-12-02-07:23:16-root-INFO: Loss too large (6.318->6.322)! Learning rate decreased to 0.23697.
2024-12-02-07:23:17-root-INFO: grad norm: 0.811 0.807 0.084
2024-12-02-07:23:17-root-INFO: grad norm: 0.592 0.591 0.047
2024-12-02-07:23:18-root-INFO: grad norm: 0.473 0.471 0.044
2024-12-02-07:23:18-root-INFO: grad norm: 0.459 0.457 0.042
2024-12-02-07:23:19-root-INFO: grad norm: 0.500 0.499 0.041
2024-12-02-07:23:19-root-INFO: Loss Change: 6.613 -> 5.999
2024-12-02-07:23:19-root-INFO: Regularization Change: 0.000 -> 0.869
2024-12-02-07:23:19-root-INFO: Undo step: 20
2024-12-02-07:23:19-root-INFO: Undo step: 21
2024-12-02-07:23:19-root-INFO: Undo step: 22
2024-12-02-07:23:19-root-INFO: Undo step: 23
2024-12-02-07:23:19-root-INFO: Undo step: 24
2024-12-02-07:23:19-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-07:23:19-root-INFO: grad norm: 13.552 13.509 1.088
2024-12-02-07:23:20-root-INFO: grad norm: 7.346 7.310 0.726
2024-12-02-07:23:20-root-INFO: grad norm: 5.041 5.007 0.585
2024-12-02-07:23:21-root-INFO: grad norm: 3.876 3.850 0.448
2024-12-02-07:23:21-root-INFO: grad norm: 3.183 3.159 0.391
2024-12-02-07:23:22-root-INFO: grad norm: 2.716 2.696 0.323
2024-12-02-07:23:22-root-INFO: grad norm: 2.379 2.361 0.293
2024-12-02-07:23:23-root-INFO: grad norm: 2.125 2.110 0.255
2024-12-02-07:23:23-root-INFO: Loss Change: 80.667 -> 17.569
2024-12-02-07:23:23-root-INFO: Regularization Change: 0.000 -> 90.486
2024-12-02-07:23:23-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-07:23:23-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-07:23:23-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-07:23:24-root-INFO: grad norm: 2.229 2.207 0.314
2024-12-02-07:23:24-root-INFO: grad norm: 1.877 1.861 0.249
2024-12-02-07:23:25-root-INFO: grad norm: 1.692 1.681 0.190
2024-12-02-07:23:25-root-INFO: grad norm: 1.562 1.550 0.198
2024-12-02-07:23:26-root-INFO: grad norm: 1.455 1.446 0.164
2024-12-02-07:23:26-root-INFO: grad norm: 1.365 1.355 0.170
2024-12-02-07:23:26-root-INFO: grad norm: 1.290 1.282 0.144
2024-12-02-07:23:27-root-INFO: grad norm: 1.225 1.216 0.151
2024-12-02-07:23:27-root-INFO: Loss Change: 17.347 -> 12.238
2024-12-02-07:23:27-root-INFO: Regularization Change: 0.000 -> 11.071
2024-12-02-07:23:27-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-07:23:27-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-07:23:27-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-07:23:28-root-INFO: grad norm: 1.774 1.744 0.327
2024-12-02-07:23:28-root-INFO: grad norm: 1.434 1.418 0.210
2024-12-02-07:23:28-root-INFO: grad norm: 1.296 1.288 0.145
2024-12-02-07:23:29-root-INFO: grad norm: 1.225 1.215 0.154
2024-12-02-07:23:29-root-INFO: grad norm: 1.178 1.172 0.123
2024-12-02-07:23:30-root-INFO: grad norm: 1.139 1.130 0.138
2024-12-02-07:23:30-root-INFO: grad norm: 1.109 1.103 0.114
2024-12-02-07:23:31-root-INFO: grad norm: 1.085 1.077 0.131
2024-12-02-07:23:31-root-INFO: Loss Change: 11.980 -> 9.725
2024-12-02-07:23:31-root-INFO: Regularization Change: 0.000 -> 4.868
2024-12-02-07:23:31-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-07:23:31-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-07:23:31-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-07:23:31-root-INFO: grad norm: 1.962 1.927 0.371
2024-12-02-07:23:32-root-INFO: grad norm: 1.508 1.491 0.225
2024-12-02-07:23:32-root-INFO: grad norm: 1.193 1.185 0.145
2024-12-02-07:23:33-root-INFO: grad norm: 1.060 1.051 0.136
2024-12-02-07:23:33-root-INFO: grad norm: 0.999 0.993 0.108
2024-12-02-07:23:34-root-INFO: grad norm: 0.985 0.978 0.120
2024-12-02-07:23:34-root-INFO: grad norm: 1.059 1.054 0.103
2024-12-02-07:23:35-root-INFO: grad norm: 1.013 1.006 0.121
2024-12-02-07:23:35-root-INFO: Loss Change: 9.698 -> 8.250
2024-12-02-07:23:35-root-INFO: Regularization Change: 0.000 -> 2.924
2024-12-02-07:23:35-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-07:23:35-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-07:23:35-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-07:23:35-root-INFO: grad norm: 1.723 1.689 0.340
2024-12-02-07:23:36-root-INFO: grad norm: 1.327 1.312 0.200
2024-12-02-07:23:36-root-INFO: grad norm: 1.113 1.106 0.130
2024-12-02-07:23:37-root-INFO: grad norm: 1.051 1.042 0.133
2024-12-02-07:23:37-root-INFO: grad norm: 1.062 1.056 0.104
2024-12-02-07:23:38-root-INFO: grad norm: 0.997 0.990 0.119
2024-12-02-07:23:38-root-INFO: grad norm: 0.878 0.873 0.091
2024-12-02-07:23:39-root-INFO: grad norm: 0.876 0.870 0.103
2024-12-02-07:23:39-root-INFO: Loss Change: 8.193 -> 7.167
2024-12-02-07:23:39-root-INFO: Regularization Change: 0.000 -> 2.056
2024-12-02-07:23:39-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-07:23:39-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-07:23:39-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-07:23:39-root-INFO: grad norm: 2.087 2.050 0.390
2024-12-02-07:23:40-root-INFO: grad norm: 1.384 1.368 0.205
2024-12-02-07:23:40-root-INFO: grad norm: 1.011 1.004 0.117
2024-12-02-07:23:41-root-INFO: grad norm: 0.893 0.887 0.104
2024-12-02-07:23:41-root-INFO: grad norm: 0.800 0.795 0.082
2024-12-02-07:23:42-root-INFO: grad norm: 0.771 0.766 0.086
2024-12-02-07:23:42-root-INFO: grad norm: 0.842 0.839 0.073
2024-12-02-07:23:43-root-INFO: grad norm: 0.860 0.856 0.091
2024-12-02-07:23:43-root-INFO: Loss Change: 7.215 -> 6.327
2024-12-02-07:23:43-root-INFO: Regularization Change: 0.000 -> 1.603
2024-12-02-07:23:43-root-INFO: Undo step: 20
2024-12-02-07:23:43-root-INFO: Undo step: 21
2024-12-02-07:23:43-root-INFO: Undo step: 22
2024-12-02-07:23:43-root-INFO: Undo step: 23
2024-12-02-07:23:43-root-INFO: Undo step: 24
2024-12-02-07:23:43-root-INFO: step: 25 lr_xt 0.27379933
2024-12-02-07:23:43-root-INFO: grad norm: 13.518 13.469 1.149
2024-12-02-07:23:44-root-INFO: grad norm: 7.187 7.148 0.745
2024-12-02-07:23:44-root-INFO: grad norm: 4.953 4.922 0.549
2024-12-02-07:23:45-root-INFO: grad norm: 3.831 3.807 0.432
2024-12-02-07:23:45-root-INFO: grad norm: 3.154 3.133 0.361
2024-12-02-07:23:46-root-INFO: grad norm: 2.698 2.680 0.312
2024-12-02-07:23:46-root-INFO: grad norm: 2.371 2.355 0.274
2024-12-02-07:23:47-root-INFO: grad norm: 2.123 2.108 0.249
2024-12-02-07:23:47-root-INFO: Loss Change: 81.588 -> 18.226
2024-12-02-07:23:47-root-INFO: Regularization Change: 0.000 -> 91.717
2024-12-02-07:23:47-root-INFO: Learning rate of xt decay: 0.28938 -> 0.29285.
2024-12-02-07:23:47-root-INFO: Coefficient of regularization decay: 0.00093 -> 0.00094.
2024-12-02-07:23:47-root-INFO: step: 24 lr_xt 0.27823123
2024-12-02-07:23:48-root-INFO: grad norm: 2.334 2.311 0.333
2024-12-02-07:23:48-root-INFO: grad norm: 1.998 1.980 0.267
2024-12-02-07:23:49-root-INFO: grad norm: 1.812 1.801 0.207
2024-12-02-07:23:49-root-INFO: grad norm: 1.685 1.671 0.220
2024-12-02-07:23:50-root-INFO: grad norm: 1.573 1.562 0.183
2024-12-02-07:23:50-root-INFO: grad norm: 1.484 1.471 0.197
2024-12-02-07:23:51-root-INFO: grad norm: 1.404 1.395 0.165
2024-12-02-07:23:51-root-INFO: grad norm: 1.339 1.327 0.179
2024-12-02-07:23:52-root-INFO: Loss Change: 17.990 -> 12.719
2024-12-02-07:23:52-root-INFO: Regularization Change: 0.000 -> 11.427
2024-12-02-07:23:52-root-INFO: Learning rate of xt decay: 0.29285 -> 0.29636.
2024-12-02-07:23:52-root-INFO: Coefficient of regularization decay: 0.00094 -> 0.00095.
2024-12-02-07:23:52-root-INFO: step: 23 lr_xt 0.28268972
2024-12-02-07:23:52-root-INFO: grad norm: 1.934 1.900 0.361
2024-12-02-07:23:52-root-INFO: grad norm: 1.578 1.558 0.254
2024-12-02-07:23:53-root-INFO: grad norm: 1.373 1.362 0.172
2024-12-02-07:23:53-root-INFO: grad norm: 1.285 1.272 0.182
2024-12-02-07:23:54-root-INFO: grad norm: 1.232 1.224 0.143
2024-12-02-07:23:54-root-INFO: grad norm: 1.181 1.170 0.161
2024-12-02-07:23:55-root-INFO: grad norm: 1.134 1.127 0.128
2024-12-02-07:23:55-root-INFO: grad norm: 1.102 1.092 0.148
2024-12-02-07:23:56-root-INFO: Loss Change: 12.430 -> 10.007
2024-12-02-07:23:56-root-INFO: Regularization Change: 0.000 -> 5.184
2024-12-02-07:23:56-root-INFO: Learning rate of xt decay: 0.29636 -> 0.29992.
2024-12-02-07:23:56-root-INFO: Coefficient of regularization decay: 0.00095 -> 0.00096.
2024-12-02-07:23:56-root-INFO: step: 22 lr_xt 0.28717380
2024-12-02-07:23:56-root-INFO: grad norm: 1.983 1.946 0.384
2024-12-02-07:23:56-root-INFO: grad norm: 1.519 1.499 0.245
2024-12-02-07:23:57-root-INFO: grad norm: 1.179 1.170 0.147
2024-12-02-07:23:57-root-INFO: grad norm: 1.037 1.027 0.142
2024-12-02-07:23:58-root-INFO: grad norm: 0.981 0.975 0.109
2024-12-02-07:23:58-root-INFO: grad norm: 0.972 0.964 0.126
2024-12-02-07:23:59-root-INFO: grad norm: 1.072 1.067 0.107
2024-12-02-07:23:59-root-INFO: grad norm: 1.003 0.995 0.128
2024-12-02-07:24:00-root-INFO: Loss Change: 9.953 -> 8.410
2024-12-02-07:24:00-root-INFO: Regularization Change: 0.000 -> 3.123
2024-12-02-07:24:00-root-INFO: Learning rate of xt decay: 0.29992 -> 0.30352.
2024-12-02-07:24:00-root-INFO: Coefficient of regularization decay: 0.00096 -> 0.00097.
2024-12-02-07:24:00-root-INFO: step: 21 lr_xt 0.29168243
2024-12-02-07:24:00-root-INFO: grad norm: 1.678 1.643 0.343
2024-12-02-07:24:00-root-INFO: grad norm: 1.306 1.289 0.209
2024-12-02-07:24:01-root-INFO: grad norm: 1.185 1.177 0.138
2024-12-02-07:24:01-root-INFO: grad norm: 1.095 1.085 0.147
2024-12-02-07:24:02-root-INFO: grad norm: 0.962 0.956 0.104
2024-12-02-07:24:02-root-INFO: grad norm: 0.958 0.950 0.122
2024-12-02-07:24:03-root-INFO: grad norm: 1.149 1.144 0.103
2024-12-02-07:24:03-root-INFO: grad norm: 0.990 0.982 0.123
2024-12-02-07:24:04-root-INFO: Loss Change: 8.323 -> 7.246
2024-12-02-07:24:04-root-INFO: Regularization Change: 0.000 -> 2.179
2024-12-02-07:24:04-root-INFO: Learning rate of xt decay: 0.30352 -> 0.30716.
2024-12-02-07:24:04-root-INFO: Coefficient of regularization decay: 0.00097 -> 0.00098.
2024-12-02-07:24:04-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-07:24:04-root-INFO: grad norm: 1.748 1.707 0.377
2024-12-02-07:24:04-root-INFO: grad norm: 1.265 1.249 0.200
2024-12-02-07:24:05-root-INFO: grad norm: 1.379 1.372 0.144
2024-12-02-07:24:05-root-INFO: grad norm: 1.054 1.045 0.136
2024-12-02-07:24:06-root-INFO: grad norm: 0.862 0.858 0.081
2024-12-02-07:24:06-root-INFO: grad norm: 0.741 0.736 0.082
2024-12-02-07:24:07-root-INFO: grad norm: 0.697 0.694 0.063
2024-12-02-07:24:07-root-INFO: grad norm: 0.707 0.703 0.079
2024-12-02-07:24:08-root-INFO: Loss Change: 7.256 -> 6.346
2024-12-02-07:24:08-root-INFO: Regularization Change: 0.000 -> 1.713
2024-12-02-07:24:08-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-07:24:08-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-07:24:08-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-07:24:08-root-INFO: grad norm: 1.759 1.728 0.329
2024-12-02-07:24:08-root-INFO: grad norm: 1.182 1.168 0.178
2024-12-02-07:24:09-root-INFO: grad norm: 0.906 0.902 0.090
2024-12-02-07:24:09-root-INFO: grad norm: 0.749 0.745 0.085
2024-12-02-07:24:10-root-INFO: grad norm: 0.691 0.689 0.061
2024-12-02-07:24:10-root-INFO: grad norm: 0.718 0.714 0.080
2024-12-02-07:24:11-root-INFO: grad norm: 1.005 1.003 0.072
2024-12-02-07:24:11-root-INFO: grad norm: 0.830 0.825 0.092
2024-12-02-07:24:12-root-INFO: Loss Change: 6.325 -> 5.598
2024-12-02-07:24:12-root-INFO: Regularization Change: 0.000 -> 1.384
2024-12-02-07:24:12-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-07:24:12-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-07:24:12-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-07:24:12-root-INFO: grad norm: 1.606 1.566 0.354
2024-12-02-07:24:12-root-INFO: grad norm: 1.074 1.060 0.170
2024-12-02-07:24:13-root-INFO: grad norm: 1.177 1.171 0.114
2024-12-02-07:24:13-root-INFO: grad norm: 0.913 0.906 0.110
2024-12-02-07:24:14-root-INFO: grad norm: 0.735 0.733 0.061
2024-12-02-07:24:14-root-INFO: grad norm: 0.640 0.637 0.058
2024-12-02-07:24:15-root-INFO: grad norm: 0.632 0.630 0.043
2024-12-02-07:24:15-root-INFO: grad norm: 0.847 0.846 0.045
2024-12-02-07:24:16-root-INFO: Loss Change: 5.658 -> 5.025
2024-12-02-07:24:16-root-INFO: Regularization Change: 0.000 -> 1.235
2024-12-02-07:24:16-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-07:24:16-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-07:24:16-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-07:24:16-root-INFO: grad norm: 1.329 1.303 0.262
2024-12-02-07:24:16-root-INFO: grad norm: 0.792 0.784 0.110
2024-12-02-07:24:17-root-INFO: grad norm: 0.640 0.637 0.060
2024-12-02-07:24:17-root-INFO: grad norm: 0.582 0.579 0.058
2024-12-02-07:24:18-root-INFO: grad norm: 0.547 0.546 0.040
2024-12-02-07:24:18-root-INFO: grad norm: 0.630 0.629 0.042
2024-12-02-07:24:19-root-INFO: grad norm: 0.712 0.710 0.049
2024-12-02-07:24:19-root-INFO: grad norm: 1.061 1.059 0.051
2024-12-02-07:24:19-root-INFO: Loss too large (4.526->4.542)! Learning rate decreased to 0.24795.
2024-12-02-07:24:20-root-INFO: Loss Change: 5.032 -> 4.518
2024-12-02-07:24:20-root-INFO: Regularization Change: 0.000 -> 1.000
2024-12-02-07:24:20-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-07:24:20-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-07:24:20-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-07:24:20-root-INFO: grad norm: 1.234 1.209 0.247
2024-12-02-07:24:21-root-INFO: grad norm: 0.665 0.658 0.098
2024-12-02-07:24:21-root-INFO: grad norm: 0.542 0.540 0.052
2024-12-02-07:24:22-root-INFO: grad norm: 0.523 0.520 0.057
2024-12-02-07:24:22-root-INFO: grad norm: 0.654 0.652 0.045
2024-12-02-07:24:22-root-INFO: grad norm: 0.738 0.734 0.072
2024-12-02-07:24:23-root-INFO: grad norm: 0.964 0.961 0.064
2024-12-02-07:24:23-root-INFO: Loss too large (4.119->4.137)! Learning rate decreased to 0.25164.
2024-12-02-07:24:23-root-INFO: grad norm: 0.683 0.680 0.068
2024-12-02-07:24:24-root-INFO: Loss Change: 4.542 -> 4.027
2024-12-02-07:24:24-root-INFO: Regularization Change: 0.000 -> 0.872
2024-12-02-07:24:24-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-07:24:24-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-07:24:24-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-07:24:24-root-INFO: grad norm: 1.375 1.335 0.328
2024-12-02-07:24:25-root-INFO: grad norm: 0.927 0.916 0.143
2024-12-02-07:24:25-root-INFO: grad norm: 1.314 1.310 0.106
2024-12-02-07:24:25-root-INFO: Loss too large (3.914->3.926)! Learning rate decreased to 0.25535.
2024-12-02-07:24:26-root-INFO: grad norm: 0.692 0.688 0.070
2024-12-02-07:24:26-root-INFO: grad norm: 0.538 0.537 0.034
2024-12-02-07:24:27-root-INFO: grad norm: 0.417 0.416 0.030
2024-12-02-07:24:27-root-INFO: grad norm: 0.428 0.427 0.036
2024-12-02-07:24:28-root-INFO: grad norm: 0.513 0.512 0.031
2024-12-02-07:24:28-root-INFO: Loss Change: 4.131 -> 3.674
2024-12-02-07:24:28-root-INFO: Regularization Change: 0.000 -> 0.705
2024-12-02-07:24:28-root-INFO: Undo step: 15
2024-12-02-07:24:28-root-INFO: Undo step: 16
2024-12-02-07:24:28-root-INFO: Undo step: 17
2024-12-02-07:24:28-root-INFO: Undo step: 18
2024-12-02-07:24:28-root-INFO: Undo step: 19
2024-12-02-07:24:28-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-07:24:28-root-INFO: grad norm: 12.687 12.639 1.101
2024-12-02-07:24:29-root-INFO: grad norm: 6.859 6.827 0.663
2024-12-02-07:24:29-root-INFO: grad norm: 4.700 4.673 0.505
2024-12-02-07:24:30-root-INFO: grad norm: 3.607 3.587 0.378
2024-12-02-07:24:30-root-INFO: grad norm: 2.946 2.929 0.320
2024-12-02-07:24:31-root-INFO: grad norm: 2.508 2.493 0.270
2024-12-02-07:24:31-root-INFO: grad norm: 2.234 2.222 0.237
2024-12-02-07:24:32-root-INFO: grad norm: 2.001 1.990 0.216
2024-12-02-07:24:32-root-INFO: Loss Change: 74.604 -> 13.428
2024-12-02-07:24:32-root-INFO: Regularization Change: 0.000 -> 94.488
2024-12-02-07:24:32-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-07:24:32-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-07:24:32-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-07:24:32-root-INFO: grad norm: 2.193 2.174 0.293
2024-12-02-07:24:33-root-INFO: grad norm: 1.754 1.742 0.204
2024-12-02-07:24:33-root-INFO: grad norm: 1.538 1.530 0.153
2024-12-02-07:24:34-root-INFO: grad norm: 1.385 1.377 0.144
2024-12-02-07:24:34-root-INFO: grad norm: 1.298 1.291 0.131
2024-12-02-07:24:35-root-INFO: grad norm: 1.347 1.342 0.122
2024-12-02-07:24:35-root-INFO: grad norm: 1.214 1.208 0.122
2024-12-02-07:24:36-root-INFO: grad norm: 1.054 1.048 0.106
2024-12-02-07:24:36-root-INFO: Loss Change: 13.044 -> 8.665
2024-12-02-07:24:36-root-INFO: Regularization Change: 0.000 -> 10.156
2024-12-02-07:24:36-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-07:24:36-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-07:24:36-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-07:24:36-root-INFO: grad norm: 1.543 1.516 0.284
2024-12-02-07:24:37-root-INFO: grad norm: 1.236 1.227 0.151
2024-12-02-07:24:37-root-INFO: grad norm: 1.514 1.509 0.122
2024-12-02-07:24:38-root-INFO: grad norm: 1.123 1.117 0.116
2024-12-02-07:24:38-root-INFO: grad norm: 0.991 0.987 0.082
2024-12-02-07:24:39-root-INFO: grad norm: 0.906 0.902 0.085
2024-12-02-07:24:39-root-INFO: grad norm: 0.911 0.908 0.076
2024-12-02-07:24:40-root-INFO: grad norm: 0.933 0.929 0.089
2024-12-02-07:24:40-root-INFO: Loss Change: 8.475 -> 6.649
2024-12-02-07:24:40-root-INFO: Regularization Change: 0.000 -> 4.268
2024-12-02-07:24:40-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-07:24:40-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-07:24:40-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-07:24:40-root-INFO: grad norm: 1.820 1.794 0.303
2024-12-02-07:24:41-root-INFO: grad norm: 1.146 1.137 0.144
2024-12-02-07:24:41-root-INFO: grad norm: 0.901 0.898 0.077
2024-12-02-07:24:42-root-INFO: grad norm: 0.789 0.785 0.075
2024-12-02-07:24:42-root-INFO: grad norm: 0.767 0.764 0.063
2024-12-02-07:24:43-root-INFO: grad norm: 0.799 0.796 0.076
2024-12-02-07:24:43-root-INFO: grad norm: 1.023 1.020 0.068
2024-12-02-07:24:44-root-INFO: grad norm: 0.822 0.819 0.079
2024-12-02-07:24:44-root-INFO: Loss Change: 6.590 -> 5.429
2024-12-02-07:24:44-root-INFO: Regularization Change: 0.000 -> 2.485
2024-12-02-07:24:44-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-07:24:44-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-07:24:44-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-07:24:44-root-INFO: grad norm: 1.361 1.333 0.272
2024-12-02-07:24:45-root-INFO: grad norm: 0.866 0.858 0.121
2024-12-02-07:24:45-root-INFO: grad norm: 0.898 0.895 0.075
2024-12-02-07:24:46-root-INFO: grad norm: 0.828 0.824 0.082
2024-12-02-07:24:46-root-INFO: grad norm: 0.683 0.681 0.055
2024-12-02-07:24:47-root-INFO: grad norm: 0.727 0.724 0.069
2024-12-02-07:24:47-root-INFO: grad norm: 1.020 1.018 0.060
2024-12-02-07:24:47-root-INFO: grad norm: 0.748 0.744 0.071
2024-12-02-07:24:48-root-INFO: Loss Change: 5.407 -> 4.606
2024-12-02-07:24:48-root-INFO: Regularization Change: 0.000 -> 1.692
2024-12-02-07:24:48-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-07:24:48-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-07:24:48-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-07:24:48-root-INFO: grad norm: 1.382 1.350 0.297
2024-12-02-07:24:49-root-INFO: grad norm: 0.771 0.763 0.109
2024-12-02-07:24:49-root-INFO: grad norm: 0.641 0.638 0.062
2024-12-02-07:24:50-root-INFO: grad norm: 0.697 0.695 0.063
2024-12-02-07:24:50-root-INFO: grad norm: 1.043 1.041 0.061
2024-12-02-07:24:51-root-INFO: grad norm: 0.713 0.711 0.063
2024-12-02-07:24:51-root-INFO: grad norm: 0.559 0.558 0.036
2024-12-02-07:24:51-root-INFO: grad norm: 0.526 0.525 0.034
2024-12-02-07:24:52-root-INFO: Loss Change: 4.634 -> 3.974
2024-12-02-07:24:52-root-INFO: Regularization Change: 0.000 -> 1.316
2024-12-02-07:24:52-root-INFO: Undo step: 15
2024-12-02-07:24:52-root-INFO: Undo step: 16
2024-12-02-07:24:52-root-INFO: Undo step: 17
2024-12-02-07:24:52-root-INFO: Undo step: 18
2024-12-02-07:24:52-root-INFO: Undo step: 19
2024-12-02-07:24:52-root-INFO: step: 20 lr_xt 0.29621455
2024-12-02-07:24:52-root-INFO: grad norm: 13.536 13.497 1.028
2024-12-02-07:24:53-root-INFO: grad norm: 7.095 7.063 0.671
2024-12-02-07:24:53-root-INFO: grad norm: 4.780 4.755 0.484
2024-12-02-07:24:54-root-INFO: grad norm: 3.649 3.630 0.376
2024-12-02-07:24:54-root-INFO: grad norm: 2.962 2.946 0.313
2024-12-02-07:24:54-root-INFO: grad norm: 2.532 2.518 0.270
2024-12-02-07:24:55-root-INFO: grad norm: 2.244 2.231 0.240
2024-12-02-07:24:55-root-INFO: grad norm: 2.016 2.004 0.218
2024-12-02-07:24:56-root-INFO: Loss Change: 79.027 -> 13.829
2024-12-02-07:24:56-root-INFO: Regularization Change: 0.000 -> 99.135
2024-12-02-07:24:56-root-INFO: Learning rate of xt decay: 0.30716 -> 0.31085.
2024-12-02-07:24:56-root-INFO: Coefficient of regularization decay: 0.00098 -> 0.00099.
2024-12-02-07:24:56-root-INFO: step: 19 lr_xt 0.30076908
2024-12-02-07:24:56-root-INFO: grad norm: 2.245 2.225 0.295
2024-12-02-07:24:56-root-INFO: grad norm: 1.782 1.771 0.205
2024-12-02-07:24:57-root-INFO: grad norm: 1.548 1.539 0.160
2024-12-02-07:24:57-root-INFO: grad norm: 1.395 1.386 0.151
2024-12-02-07:24:58-root-INFO: grad norm: 1.292 1.285 0.138
2024-12-02-07:24:58-root-INFO: grad norm: 1.225 1.218 0.131
2024-12-02-07:24:59-root-INFO: grad norm: 1.193 1.187 0.125
2024-12-02-07:24:59-root-INFO: grad norm: 1.375 1.370 0.119
2024-12-02-07:25:00-root-INFO: Loss Change: 13.410 -> 8.898
2024-12-02-07:25:00-root-INFO: Regularization Change: 0.000 -> 10.728
2024-12-02-07:25:00-root-INFO: Learning rate of xt decay: 0.31085 -> 0.31458.
2024-12-02-07:25:00-root-INFO: Coefficient of regularization decay: 0.00099 -> 0.00100.
2024-12-02-07:25:00-root-INFO: step: 18 lr_xt 0.30534490
2024-12-02-07:25:00-root-INFO: grad norm: 1.566 1.544 0.261
2024-12-02-07:25:01-root-INFO: grad norm: 1.132 1.124 0.137
2024-12-02-07:25:01-root-INFO: grad norm: 1.072 1.067 0.106
2024-12-02-07:25:01-root-INFO: grad norm: 1.077 1.071 0.114
2024-12-02-07:25:02-root-INFO: grad norm: 1.276 1.272 0.107
2024-12-02-07:25:02-root-INFO: grad norm: 1.046 1.040 0.110
2024-12-02-07:25:03-root-INFO: grad norm: 0.875 0.871 0.082
2024-12-02-07:25:03-root-INFO: grad norm: 0.798 0.794 0.081
2024-12-02-07:25:04-root-INFO: Loss Change: 8.645 -> 6.664
2024-12-02-07:25:04-root-INFO: Regularization Change: 0.000 -> 4.465
2024-12-02-07:25:04-root-INFO: Learning rate of xt decay: 0.31458 -> 0.31835.
2024-12-02-07:25:04-root-INFO: Coefficient of regularization decay: 0.00100 -> 0.00101.
2024-12-02-07:25:04-root-INFO: step: 17 lr_xt 0.30994086
2024-12-02-07:25:04-root-INFO: grad norm: 1.342 1.317 0.262
2024-12-02-07:25:05-root-INFO: grad norm: 0.932 0.924 0.124
2024-12-02-07:25:05-root-INFO: grad norm: 0.853 0.849 0.083
2024-12-02-07:25:06-root-INFO: grad norm: 0.908 0.904 0.093
2024-12-02-07:25:06-root-INFO: grad norm: 1.333 1.330 0.094
2024-12-02-07:25:06-root-INFO: grad norm: 0.921 0.917 0.093
2024-12-02-07:25:07-root-INFO: grad norm: 0.762 0.759 0.061
2024-12-02-07:25:07-root-INFO: grad norm: 0.643 0.640 0.062
2024-12-02-07:25:08-root-INFO: Loss Change: 6.539 -> 5.407
2024-12-02-07:25:08-root-INFO: Regularization Change: 0.000 -> 2.543
2024-12-02-07:25:08-root-INFO: Learning rate of xt decay: 0.31835 -> 0.32217.
2024-12-02-07:25:08-root-INFO: Coefficient of regularization decay: 0.00101 -> 0.00102.
2024-12-02-07:25:08-root-INFO: step: 16 lr_xt 0.31455579
2024-12-02-07:25:08-root-INFO: grad norm: 1.348 1.319 0.275
2024-12-02-07:25:09-root-INFO: grad norm: 1.015 1.006 0.136
2024-12-02-07:25:09-root-INFO: grad norm: 1.188 1.184 0.097
2024-12-02-07:25:09-root-INFO: grad norm: 0.878 0.873 0.092
2024-12-02-07:25:10-root-INFO: grad norm: 0.691 0.690 0.051
2024-12-02-07:25:10-root-INFO: grad norm: 0.612 0.610 0.051
2024-12-02-07:25:11-root-INFO: grad norm: 0.647 0.645 0.050
2024-12-02-07:25:11-root-INFO: grad norm: 0.922 0.920 0.051
2024-12-02-07:25:12-root-INFO: Loss Change: 5.363 -> 4.628
2024-12-02-07:25:12-root-INFO: Regularization Change: 0.000 -> 1.764
2024-12-02-07:25:12-root-INFO: Learning rate of xt decay: 0.32217 -> 0.32604.
2024-12-02-07:25:12-root-INFO: Coefficient of regularization decay: 0.00102 -> 0.00103.
2024-12-02-07:25:12-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-07:25:12-root-INFO: grad norm: 1.309 1.284 0.255
2024-12-02-07:25:12-root-INFO: grad norm: 0.697 0.692 0.089
2024-12-02-07:25:13-root-INFO: grad norm: 0.552 0.550 0.050
2024-12-02-07:25:13-root-INFO: grad norm: 0.502 0.500 0.046
2024-12-02-07:25:14-root-INFO: grad norm: 0.467 0.466 0.037
2024-12-02-07:25:14-root-INFO: grad norm: 0.450 0.448 0.038
2024-12-02-07:25:15-root-INFO: grad norm: 0.460 0.459 0.036
2024-12-02-07:25:15-root-INFO: grad norm: 0.612 0.611 0.037
2024-12-02-07:25:16-root-INFO: Loss Change: 4.597 -> 3.947
2024-12-02-07:25:16-root-INFO: Regularization Change: 0.000 -> 1.305
2024-12-02-07:25:16-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-07:25:16-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-07:25:16-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-07:25:16-root-INFO: grad norm: 1.233 1.211 0.235
2024-12-02-07:25:16-root-INFO: grad norm: 1.016 1.014 0.064
2024-12-02-07:25:17-root-INFO: grad norm: 0.728 0.726 0.046
2024-12-02-07:25:17-root-INFO: grad norm: 0.485 0.484 0.033
2024-12-02-07:25:18-root-INFO: grad norm: 0.434 0.433 0.031
2024-12-02-07:25:18-root-INFO: grad norm: 0.524 0.522 0.042
2024-12-02-07:25:19-root-INFO: grad norm: 0.852 0.851 0.050
2024-12-02-07:25:19-root-INFO: grad norm: 0.708 0.705 0.058
2024-12-02-07:25:20-root-INFO: Loss Change: 3.955 -> 3.397
2024-12-02-07:25:20-root-INFO: Regularization Change: 0.000 -> 1.103
2024-12-02-07:25:20-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-07:25:20-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-07:25:20-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-07:25:20-root-INFO: grad norm: 1.366 1.332 0.302
2024-12-02-07:25:20-root-INFO: grad norm: 0.933 0.925 0.124
2024-12-02-07:25:21-root-INFO: grad norm: 0.986 0.982 0.083
2024-12-02-07:25:21-root-INFO: grad norm: 0.724 0.720 0.070
2024-12-02-07:25:22-root-INFO: grad norm: 0.451 0.450 0.033
2024-12-02-07:25:22-root-INFO: grad norm: 0.362 0.361 0.032
2024-12-02-07:25:23-root-INFO: grad norm: 0.346 0.345 0.025
2024-12-02-07:25:23-root-INFO: grad norm: 0.382 0.381 0.025
2024-12-02-07:25:24-root-INFO: Loss Change: 3.508 -> 3.011
2024-12-02-07:25:24-root-INFO: Regularization Change: 0.000 -> 0.924
2024-12-02-07:25:24-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-07:25:24-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-07:25:24-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-07:25:24-root-INFO: grad norm: 1.069 1.045 0.226
2024-12-02-07:25:24-root-INFO: grad norm: 0.587 0.584 0.062
2024-12-02-07:25:25-root-INFO: grad norm: 0.635 0.634 0.035
2024-12-02-07:25:25-root-INFO: grad norm: 0.932 0.931 0.043
2024-12-02-07:25:25-root-INFO: Loss too large (2.881->2.889)! Learning rate decreased to 0.26654.
2024-12-02-07:25:26-root-INFO: grad norm: 0.709 0.707 0.052
2024-12-02-07:25:26-root-INFO: grad norm: 0.476 0.475 0.032
2024-12-02-07:25:27-root-INFO: grad norm: 0.514 0.512 0.039
2024-12-02-07:25:27-root-INFO: grad norm: 0.586 0.585 0.032
2024-12-02-07:25:28-root-INFO: Loss Change: 3.090 -> 2.740
2024-12-02-07:25:28-root-INFO: Regularization Change: 0.000 -> 0.632
2024-12-02-07:25:28-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-07:25:28-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-07:25:28-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-07:25:28-root-INFO: grad norm: 1.123 1.099 0.230
2024-12-02-07:25:29-root-INFO: grad norm: 0.702 0.701 0.051
2024-12-02-07:25:29-root-INFO: grad norm: 0.713 0.712 0.041
2024-12-02-07:25:30-root-INFO: grad norm: 0.822 0.820 0.047
2024-12-02-07:25:30-root-INFO: Loss too large (2.627->2.627)! Learning rate decreased to 0.27030.
2024-12-02-07:25:30-root-INFO: grad norm: 0.687 0.685 0.049
2024-12-02-07:25:31-root-INFO: grad norm: 0.568 0.567 0.036
2024-12-02-07:25:31-root-INFO: grad norm: 0.553 0.552 0.040
2024-12-02-07:25:32-root-INFO: grad norm: 0.543 0.543 0.032
2024-12-02-07:25:32-root-INFO: Loss Change: 2.855 -> 2.497
2024-12-02-07:25:32-root-INFO: Regularization Change: 0.000 -> 0.606
2024-12-02-07:25:32-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-07:25:32-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-07:25:32-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-07:25:32-root-INFO: grad norm: 1.111 1.086 0.236
2024-12-02-07:25:33-root-INFO: grad norm: 0.541 0.539 0.050
2024-12-02-07:25:33-root-INFO: grad norm: 0.537 0.536 0.032
2024-12-02-07:25:34-root-INFO: grad norm: 0.670 0.669 0.038
2024-12-02-07:25:34-root-INFO: grad norm: 0.722 0.720 0.050
2024-12-02-07:25:35-root-INFO: grad norm: 0.758 0.757 0.048
2024-12-02-07:25:35-root-INFO: grad norm: 0.732 0.730 0.055
2024-12-02-07:25:36-root-INFO: grad norm: 0.682 0.680 0.045
2024-12-02-07:25:36-root-INFO: Loss Change: 2.655 -> 2.306
2024-12-02-07:25:36-root-INFO: Regularization Change: 0.000 -> 0.736
2024-12-02-07:25:36-root-INFO: Undo step: 10
2024-12-02-07:25:36-root-INFO: Undo step: 11
2024-12-02-07:25:36-root-INFO: Undo step: 12
2024-12-02-07:25:36-root-INFO: Undo step: 13
2024-12-02-07:25:36-root-INFO: Undo step: 14
2024-12-02-07:25:36-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-07:25:36-root-INFO: grad norm: 12.853 12.826 0.834
2024-12-02-07:25:37-root-INFO: grad norm: 6.273 6.253 0.504
2024-12-02-07:25:37-root-INFO: grad norm: 4.177 4.161 0.360
2024-12-02-07:25:38-root-INFO: grad norm: 3.148 3.135 0.285
2024-12-02-07:25:38-root-INFO: grad norm: 2.554 2.543 0.232
2024-12-02-07:25:39-root-INFO: grad norm: 2.177 2.167 0.204
2024-12-02-07:25:39-root-INFO: grad norm: 1.958 1.950 0.176
2024-12-02-07:25:40-root-INFO: grad norm: 1.750 1.742 0.168
2024-12-02-07:25:40-root-INFO: Loss Change: 66.923 -> 9.370
2024-12-02-07:25:40-root-INFO: Regularization Change: 0.000 -> 92.775
2024-12-02-07:25:40-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-07:25:40-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-07:25:40-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-07:25:40-root-INFO: grad norm: 1.988 1.969 0.271
2024-12-02-07:25:41-root-INFO: grad norm: 1.559 1.550 0.164
2024-12-02-07:25:41-root-INFO: grad norm: 1.302 1.297 0.114
2024-12-02-07:25:42-root-INFO: grad norm: 1.186 1.181 0.111
2024-12-02-07:25:42-root-INFO: grad norm: 1.138 1.134 0.097
2024-12-02-07:25:43-root-INFO: grad norm: 1.124 1.119 0.103
2024-12-02-07:25:43-root-INFO: grad norm: 1.233 1.229 0.096
2024-12-02-07:25:44-root-INFO: grad norm: 1.055 1.051 0.099
2024-12-02-07:25:44-root-INFO: Loss Change: 9.019 -> 5.554
2024-12-02-07:25:44-root-INFO: Regularization Change: 0.000 -> 8.547
2024-12-02-07:25:44-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-07:25:44-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-07:25:44-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-07:25:44-root-INFO: grad norm: 1.462 1.439 0.261
2024-12-02-07:25:45-root-INFO: grad norm: 1.077 1.070 0.123
2024-12-02-07:25:45-root-INFO: grad norm: 1.166 1.163 0.091
2024-12-02-07:25:46-root-INFO: grad norm: 0.954 0.950 0.090
2024-12-02-07:25:46-root-INFO: grad norm: 0.751 0.748 0.057
2024-12-02-07:25:47-root-INFO: grad norm: 0.678 0.676 0.059
2024-12-02-07:25:47-root-INFO: grad norm: 0.639 0.637 0.049
2024-12-02-07:25:48-root-INFO: grad norm: 0.619 0.617 0.054
2024-12-02-07:25:48-root-INFO: Loss Change: 5.420 -> 4.041
2024-12-02-07:25:48-root-INFO: Regularization Change: 0.000 -> 3.282
2024-12-02-07:25:48-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-07:25:48-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-07:25:48-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-07:25:48-root-INFO: grad norm: 1.307 1.285 0.236
2024-12-02-07:25:49-root-INFO: grad norm: 0.952 0.947 0.104
2024-12-02-07:25:50-root-INFO: grad norm: 0.825 0.822 0.069
2024-12-02-07:25:50-root-INFO: grad norm: 0.825 0.821 0.074
2024-12-02-07:25:51-root-INFO: grad norm: 0.863 0.861 0.061
2024-12-02-07:25:51-root-INFO: grad norm: 0.782 0.779 0.070
2024-12-02-07:25:52-root-INFO: grad norm: 0.651 0.649 0.050
2024-12-02-07:25:52-root-INFO: grad norm: 0.701 0.699 0.062
2024-12-02-07:25:52-root-INFO: Loss Change: 3.991 -> 3.246
2024-12-02-07:25:52-root-INFO: Regularization Change: 0.000 -> 1.754
2024-12-02-07:25:52-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-07:25:52-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-07:25:52-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-07:25:53-root-INFO: grad norm: 1.522 1.496 0.282
2024-12-02-07:25:53-root-INFO: grad norm: 0.865 0.859 0.099
2024-12-02-07:25:54-root-INFO: grad norm: 0.527 0.526 0.044
2024-12-02-07:25:54-root-INFO: grad norm: 0.424 0.423 0.033
2024-12-02-07:25:55-root-INFO: grad norm: 0.386 0.386 0.027
2024-12-02-07:25:55-root-INFO: grad norm: 0.365 0.364 0.027
2024-12-02-07:25:56-root-INFO: grad norm: 0.351 0.350 0.026
2024-12-02-07:25:56-root-INFO: grad norm: 0.347 0.346 0.025
2024-12-02-07:25:56-root-INFO: Loss Change: 3.314 -> 2.696
2024-12-02-07:25:56-root-INFO: Regularization Change: 0.000 -> 1.183
2024-12-02-07:25:56-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-07:25:56-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-07:25:56-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-07:25:57-root-INFO: grad norm: 1.096 1.070 0.237
2024-12-02-07:25:57-root-INFO: grad norm: 0.555 0.552 0.061
2024-12-02-07:25:58-root-INFO: grad norm: 0.567 0.565 0.048
2024-12-02-07:25:58-root-INFO: grad norm: 0.672 0.670 0.050
2024-12-02-07:25:59-root-INFO: grad norm: 0.792 0.790 0.053
2024-12-02-07:25:59-root-INFO: grad norm: 0.737 0.735 0.058
2024-12-02-07:26:00-root-INFO: grad norm: 0.648 0.646 0.049
2024-12-02-07:26:00-root-INFO: grad norm: 0.672 0.669 0.054
2024-12-02-07:26:00-root-INFO: Loss Change: 2.782 -> 2.379
2024-12-02-07:26:00-root-INFO: Regularization Change: 0.000 -> 0.878
2024-12-02-07:26:00-root-INFO: Undo step: 10
2024-12-02-07:26:01-root-INFO: Undo step: 11
2024-12-02-07:26:01-root-INFO: Undo step: 12
2024-12-02-07:26:01-root-INFO: Undo step: 13
2024-12-02-07:26:01-root-INFO: Undo step: 14
2024-12-02-07:26:01-root-INFO: step: 15 lr_xt 0.31918850
2024-12-02-07:26:01-root-INFO: grad norm: 12.956 12.924 0.915
2024-12-02-07:26:01-root-INFO: grad norm: 6.473 6.449 0.564
2024-12-02-07:26:02-root-INFO: grad norm: 4.348 4.330 0.396
2024-12-02-07:26:02-root-INFO: grad norm: 3.306 3.291 0.312
2024-12-02-07:26:03-root-INFO: grad norm: 2.700 2.687 0.263
2024-12-02-07:26:03-root-INFO: grad norm: 2.278 2.268 0.219
2024-12-02-07:26:04-root-INFO: grad norm: 1.991 1.982 0.193
2024-12-02-07:26:04-root-INFO: grad norm: 1.771 1.762 0.172
2024-12-02-07:26:05-root-INFO: Loss Change: 69.992 -> 9.828
2024-12-02-07:26:05-root-INFO: Regularization Change: 0.000 -> 96.984
2024-12-02-07:26:05-root-INFO: Learning rate of xt decay: 0.32604 -> 0.32995.
2024-12-02-07:26:05-root-INFO: Coefficient of regularization decay: 0.00103 -> 0.00104.
2024-12-02-07:26:05-root-INFO: step: 14 lr_xt 0.32383775
2024-12-02-07:26:05-root-INFO: grad norm: 1.967 1.952 0.240
2024-12-02-07:26:05-root-INFO: grad norm: 1.560 1.552 0.157
2024-12-02-07:26:06-root-INFO: grad norm: 1.329 1.324 0.119
2024-12-02-07:26:06-root-INFO: grad norm: 1.212 1.206 0.114
2024-12-02-07:26:07-root-INFO: grad norm: 1.138 1.133 0.101
2024-12-02-07:26:07-root-INFO: grad norm: 1.099 1.094 0.102
2024-12-02-07:26:08-root-INFO: grad norm: 1.153 1.149 0.092
2024-12-02-07:26:08-root-INFO: grad norm: 1.092 1.088 0.098
2024-12-02-07:26:08-root-INFO: Loss Change: 9.415 -> 5.776
2024-12-02-07:26:08-root-INFO: Regularization Change: 0.000 -> 9.049
2024-12-02-07:26:08-root-INFO: Learning rate of xt decay: 0.32995 -> 0.33391.
2024-12-02-07:26:09-root-INFO: Coefficient of regularization decay: 0.00104 -> 0.00105.
2024-12-02-07:26:09-root-INFO: step: 13 lr_xt 0.32850231
2024-12-02-07:26:09-root-INFO: grad norm: 1.615 1.595 0.255
2024-12-02-07:26:09-root-INFO: grad norm: 1.175 1.168 0.125
2024-12-02-07:26:10-root-INFO: grad norm: 0.913 0.909 0.077
2024-12-02-07:26:10-root-INFO: grad norm: 0.879 0.876 0.080
2024-12-02-07:26:11-root-INFO: grad norm: 0.975 0.972 0.069
2024-12-02-07:26:11-root-INFO: grad norm: 0.907 0.904 0.079
2024-12-02-07:26:12-root-INFO: grad norm: 0.803 0.801 0.062
2024-12-02-07:26:12-root-INFO: grad norm: 0.838 0.835 0.073
2024-12-02-07:26:13-root-INFO: Loss Change: 5.615 -> 4.170
2024-12-02-07:26:13-root-INFO: Regularization Change: 0.000 -> 3.489
2024-12-02-07:26:13-root-INFO: Learning rate of xt decay: 0.33391 -> 0.33792.
2024-12-02-07:26:13-root-INFO: Coefficient of regularization decay: 0.00105 -> 0.00106.
2024-12-02-07:26:13-root-INFO: step: 12 lr_xt 0.33318090
2024-12-02-07:26:13-root-INFO: grad norm: 1.545 1.526 0.243
2024-12-02-07:26:13-root-INFO: grad norm: 0.972 0.966 0.105
2024-12-02-07:26:14-root-INFO: grad norm: 0.684 0.682 0.052
2024-12-02-07:26:14-root-INFO: grad norm: 0.588 0.586 0.047
2024-12-02-07:26:15-root-INFO: grad norm: 0.550 0.548 0.040
2024-12-02-07:26:15-root-INFO: grad norm: 0.561 0.560 0.039
2024-12-02-07:26:16-root-INFO: grad norm: 0.629 0.627 0.046
2024-12-02-07:26:16-root-INFO: grad norm: 0.777 0.776 0.045
2024-12-02-07:26:17-root-INFO: Loss Change: 4.120 -> 3.298
2024-12-02-07:26:17-root-INFO: Regularization Change: 0.000 -> 1.930
2024-12-02-07:26:17-root-INFO: Learning rate of xt decay: 0.33792 -> 0.34197.
2024-12-02-07:26:17-root-INFO: Coefficient of regularization decay: 0.00106 -> 0.00107.
2024-12-02-07:26:17-root-INFO: step: 11 lr_xt 0.33787222
2024-12-02-07:26:17-root-INFO: grad norm: 1.186 1.172 0.181
2024-12-02-07:26:18-root-INFO: grad norm: 0.757 0.755 0.047
2024-12-02-07:26:18-root-INFO: grad norm: 0.736 0.734 0.045
2024-12-02-07:26:19-root-INFO: grad norm: 0.765 0.764 0.046
2024-12-02-07:26:19-root-INFO: grad norm: 0.743 0.741 0.055
2024-12-02-07:26:20-root-INFO: grad norm: 0.713 0.711 0.048
2024-12-02-07:26:20-root-INFO: grad norm: 0.716 0.714 0.056
2024-12-02-07:26:21-root-INFO: grad norm: 0.711 0.710 0.048
2024-12-02-07:26:21-root-INFO: Loss Change: 3.278 -> 2.740
2024-12-02-07:26:21-root-INFO: Regularization Change: 0.000 -> 1.200
2024-12-02-07:26:21-root-INFO: Learning rate of xt decay: 0.34197 -> 0.34608.
2024-12-02-07:26:21-root-INFO: Coefficient of regularization decay: 0.00107 -> 0.00108.
2024-12-02-07:26:21-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-07:26:21-root-INFO: grad norm: 1.127 1.111 0.186
2024-12-02-07:26:22-root-INFO: grad norm: 0.630 0.629 0.040
2024-12-02-07:26:22-root-INFO: grad norm: 0.582 0.581 0.035
2024-12-02-07:26:23-root-INFO: grad norm: 0.591 0.590 0.038
2024-12-02-07:26:23-root-INFO: grad norm: 0.610 0.609 0.042
2024-12-02-07:26:24-root-INFO: grad norm: 0.623 0.622 0.042
2024-12-02-07:26:24-root-INFO: grad norm: 0.629 0.627 0.047
2024-12-02-07:26:25-root-INFO: grad norm: 0.627 0.625 0.043
2024-12-02-07:26:25-root-INFO: Loss Change: 2.785 -> 2.363
2024-12-02-07:26:25-root-INFO: Regularization Change: 0.000 -> 0.881
2024-12-02-07:26:25-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-07:26:25-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-07:26:25-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-07:26:25-root-INFO: grad norm: 0.993 0.979 0.168
2024-12-02-07:26:26-root-INFO: grad norm: 0.502 0.501 0.034
2024-12-02-07:26:26-root-INFO: grad norm: 0.427 0.426 0.026
2024-12-02-07:26:27-root-INFO: grad norm: 0.413 0.412 0.029
2024-12-02-07:26:27-root-INFO: grad norm: 0.412 0.411 0.027
2024-12-02-07:26:27-root-INFO: grad norm: 0.414 0.413 0.030
2024-12-02-07:26:28-root-INFO: grad norm: 0.419 0.418 0.029
2024-12-02-07:26:28-root-INFO: grad norm: 0.423 0.422 0.031
2024-12-02-07:26:29-root-INFO: Loss Change: 2.437 -> 2.095
2024-12-02-07:26:29-root-INFO: Regularization Change: 0.000 -> 0.686
2024-12-02-07:26:29-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-07:26:29-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-07:26:29-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-07:26:29-root-INFO: grad norm: 0.961 0.943 0.182
2024-12-02-07:26:30-root-INFO: grad norm: 0.393 0.391 0.032
2024-12-02-07:26:30-root-INFO: grad norm: 0.290 0.289 0.025
2024-12-02-07:26:30-root-INFO: grad norm: 0.263 0.262 0.022
2024-12-02-07:26:31-root-INFO: grad norm: 0.249 0.248 0.020
2024-12-02-07:26:31-root-INFO: grad norm: 0.238 0.238 0.020
2024-12-02-07:26:32-root-INFO: grad norm: 0.230 0.229 0.019
2024-12-02-07:26:32-root-INFO: grad norm: 0.224 0.223 0.019
2024-12-02-07:26:33-root-INFO: Loss Change: 2.225 -> 1.904
2024-12-02-07:26:33-root-INFO: Regularization Change: 0.000 -> 0.608
2024-12-02-07:26:33-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-07:26:33-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-07:26:33-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-07:26:33-root-INFO: grad norm: 0.981 0.961 0.197
2024-12-02-07:26:34-root-INFO: grad norm: 0.423 0.421 0.034
2024-12-02-07:26:34-root-INFO: grad norm: 0.313 0.312 0.033
2024-12-02-07:26:34-root-INFO: grad norm: 0.279 0.278 0.023
2024-12-02-07:26:35-root-INFO: grad norm: 0.257 0.256 0.025
2024-12-02-07:26:35-root-INFO: grad norm: 0.240 0.239 0.022
2024-12-02-07:26:36-root-INFO: grad norm: 0.227 0.226 0.023
2024-12-02-07:26:36-root-INFO: grad norm: 0.216 0.215 0.020
2024-12-02-07:26:37-root-INFO: Loss Change: 2.072 -> 1.760
2024-12-02-07:26:37-root-INFO: Regularization Change: 0.000 -> 0.570
2024-12-02-07:26:37-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-07:26:37-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-07:26:37-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-07:26:37-root-INFO: grad norm: 0.944 0.927 0.179
2024-12-02-07:26:38-root-INFO: grad norm: 0.418 0.417 0.033
2024-12-02-07:26:38-root-INFO: grad norm: 0.301 0.299 0.033
2024-12-02-07:26:38-root-INFO: grad norm: 0.253 0.252 0.025
2024-12-02-07:26:39-root-INFO: grad norm: 0.228 0.226 0.026
2024-12-02-07:26:39-root-INFO: grad norm: 0.211 0.209 0.023
2024-12-02-07:26:40-root-INFO: grad norm: 0.199 0.197 0.023
2024-12-02-07:26:40-root-INFO: grad norm: 0.190 0.188 0.022
2024-12-02-07:26:41-root-INFO: Loss Change: 1.939 -> 1.644
2024-12-02-07:26:41-root-INFO: Regularization Change: 0.000 -> 0.541
2024-12-02-07:26:41-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-07:26:41-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-07:26:41-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-07:26:41-root-INFO: grad norm: 0.922 0.907 0.165
2024-12-02-07:26:41-root-INFO: grad norm: 0.389 0.388 0.034
2024-12-02-07:26:42-root-INFO: grad norm: 0.262 0.260 0.032
2024-12-02-07:26:42-root-INFO: grad norm: 0.229 0.227 0.028
2024-12-02-07:26:43-root-INFO: grad norm: 0.209 0.207 0.027
2024-12-02-07:26:43-root-INFO: grad norm: 0.195 0.193 0.026
2024-12-02-07:26:44-root-INFO: grad norm: 0.186 0.184 0.025
2024-12-02-07:26:44-root-INFO: grad norm: 0.179 0.177 0.025
2024-12-02-07:26:45-root-INFO: Loss Change: 1.835 -> 1.546
2024-12-02-07:26:45-root-INFO: Regularization Change: 0.000 -> 0.534
2024-12-02-07:26:45-root-INFO: Undo step: 5
2024-12-02-07:26:45-root-INFO: Undo step: 6
2024-12-02-07:26:45-root-INFO: Undo step: 7
2024-12-02-07:26:45-root-INFO: Undo step: 8
2024-12-02-07:26:45-root-INFO: Undo step: 9
2024-12-02-07:26:45-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-07:26:45-root-INFO: grad norm: 12.244 12.228 0.614
2024-12-02-07:26:45-root-INFO: grad norm: 5.859 5.846 0.387
2024-12-02-07:26:46-root-INFO: grad norm: 3.719 3.709 0.278
2024-12-02-07:26:46-root-INFO: grad norm: 2.866 2.859 0.212
2024-12-02-07:26:47-root-INFO: grad norm: 2.290 2.284 0.160
2024-12-02-07:26:47-root-INFO: grad norm: 1.866 1.861 0.149
2024-12-02-07:26:48-root-INFO: grad norm: 1.545 1.540 0.115
2024-12-02-07:26:48-root-INFO: grad norm: 1.340 1.336 0.104
2024-12-02-07:26:49-root-INFO: Loss Change: 55.679 -> 5.823
2024-12-02-07:26:49-root-INFO: Regularization Change: 0.000 -> 83.630
2024-12-02-07:26:49-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-07:26:49-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-07:26:49-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-07:26:49-root-INFO: grad norm: 1.472 1.464 0.154
2024-12-02-07:26:49-root-INFO: grad norm: 1.169 1.165 0.098
2024-12-02-07:26:50-root-INFO: grad norm: 1.154 1.152 0.070
2024-12-02-07:26:50-root-INFO: grad norm: 1.089 1.086 0.090
2024-12-02-07:26:51-root-INFO: grad norm: 0.996 0.994 0.062
2024-12-02-07:26:51-root-INFO: grad norm: 0.984 0.980 0.085
2024-12-02-07:26:52-root-INFO: grad norm: 0.960 0.958 0.058
2024-12-02-07:26:52-root-INFO: grad norm: 0.903 0.899 0.080
2024-12-02-07:26:52-root-INFO: Loss Change: 5.533 -> 3.298
2024-12-02-07:26:52-root-INFO: Regularization Change: 0.000 -> 5.988
2024-12-02-07:26:52-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-07:26:52-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-07:26:53-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-07:26:53-root-INFO: grad norm: 1.311 1.295 0.205
2024-12-02-07:26:53-root-INFO: grad norm: 0.927 0.922 0.094
2024-12-02-07:26:54-root-INFO: grad norm: 0.805 0.803 0.051
2024-12-02-07:26:54-root-INFO: grad norm: 0.700 0.697 0.056
2024-12-02-07:26:55-root-INFO: grad norm: 0.638 0.637 0.040
2024-12-02-07:26:55-root-INFO: grad norm: 0.585 0.583 0.045
2024-12-02-07:26:56-root-INFO: grad norm: 0.551 0.550 0.035
2024-12-02-07:26:56-root-INFO: grad norm: 0.519 0.518 0.040
2024-12-02-07:26:56-root-INFO: Loss Change: 3.255 -> 2.383
2024-12-02-07:26:56-root-INFO: Regularization Change: 0.000 -> 2.070
2024-12-02-07:26:56-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-07:26:56-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-07:26:57-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-07:26:57-root-INFO: grad norm: 1.078 1.059 0.201
2024-12-02-07:26:57-root-INFO: grad norm: 0.617 0.614 0.057
2024-12-02-07:26:58-root-INFO: grad norm: 0.503 0.501 0.038
2024-12-02-07:26:58-root-INFO: grad norm: 0.429 0.428 0.030
2024-12-02-07:26:59-root-INFO: grad norm: 0.387 0.386 0.027
2024-12-02-07:26:59-root-INFO: grad norm: 0.353 0.352 0.025
2024-12-02-07:27:00-root-INFO: grad norm: 0.329 0.328 0.023
2024-12-02-07:27:00-root-INFO: grad norm: 0.308 0.307 0.022
2024-12-02-07:27:00-root-INFO: Loss Change: 2.432 -> 1.940
2024-12-02-07:27:00-root-INFO: Regularization Change: 0.000 -> 1.068
2024-12-02-07:27:00-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-07:27:00-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-07:27:01-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-07:27:01-root-INFO: grad norm: 0.920 0.903 0.178
2024-12-02-07:27:01-root-INFO: grad norm: 0.450 0.448 0.040
2024-12-02-07:27:02-root-INFO: grad norm: 0.344 0.343 0.032
2024-12-02-07:27:02-root-INFO: grad norm: 0.297 0.296 0.023
2024-12-02-07:27:03-root-INFO: grad norm: 0.270 0.269 0.024
2024-12-02-07:27:03-root-INFO: grad norm: 0.250 0.250 0.021
2024-12-02-07:27:04-root-INFO: grad norm: 0.236 0.235 0.021
2024-12-02-07:27:04-root-INFO: grad norm: 0.225 0.224 0.020
2024-12-02-07:27:04-root-INFO: Loss Change: 2.042 -> 1.701
2024-12-02-07:27:04-root-INFO: Regularization Change: 0.000 -> 0.704
2024-12-02-07:27:04-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-07:27:04-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-07:27:05-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-07:27:05-root-INFO: grad norm: 0.912 0.895 0.177
2024-12-02-07:27:05-root-INFO: grad norm: 0.422 0.421 0.037
2024-12-02-07:27:06-root-INFO: grad norm: 0.278 0.276 0.030
2024-12-02-07:27:06-root-INFO: grad norm: 0.243 0.242 0.024
2024-12-02-07:27:07-root-INFO: grad norm: 0.225 0.223 0.025
2024-12-02-07:27:07-root-INFO: grad norm: 0.218 0.217 0.023
2024-12-02-07:27:07-root-INFO: grad norm: 0.209 0.208 0.024
2024-12-02-07:27:08-root-INFO: grad norm: 0.204 0.203 0.022
2024-12-02-07:27:08-root-INFO: Loss Change: 1.849 -> 1.554
2024-12-02-07:27:08-root-INFO: Regularization Change: 0.000 -> 0.570
2024-12-02-07:27:08-root-INFO: Undo step: 5
2024-12-02-07:27:08-root-INFO: Undo step: 6
2024-12-02-07:27:08-root-INFO: Undo step: 7
2024-12-02-07:27:08-root-INFO: Undo step: 8
2024-12-02-07:27:08-root-INFO: Undo step: 9
2024-12-02-07:27:08-root-INFO: step: 10 lr_xt 0.34257494
2024-12-02-07:27:09-root-INFO: grad norm: 13.001 12.984 0.660
2024-12-02-07:27:09-root-INFO: grad norm: 5.748 5.731 0.443
2024-12-02-07:27:09-root-INFO: grad norm: 3.805 3.795 0.281
2024-12-02-07:27:10-root-INFO: grad norm: 3.089 3.081 0.226
2024-12-02-07:27:10-root-INFO: grad norm: 2.387 2.379 0.190
2024-12-02-07:27:11-root-INFO: grad norm: 2.046 2.042 0.138
2024-12-02-07:27:11-root-INFO: grad norm: 1.775 1.770 0.141
2024-12-02-07:27:12-root-INFO: grad norm: 1.480 1.477 0.100
2024-12-02-07:27:12-root-INFO: Loss Change: 58.671 -> 6.004
2024-12-02-07:27:12-root-INFO: Regularization Change: 0.000 -> 87.165
2024-12-02-07:27:12-root-INFO: Learning rate of xt decay: 0.34608 -> 0.35023.
2024-12-02-07:27:12-root-INFO: Coefficient of regularization decay: 0.00108 -> 0.00109.
2024-12-02-07:27:12-root-INFO: step: 9 lr_xt 0.34728771
2024-12-02-07:27:13-root-INFO: grad norm: 1.504 1.497 0.139
2024-12-02-07:27:13-root-INFO: grad norm: 1.196 1.193 0.077
2024-12-02-07:27:14-root-INFO: grad norm: 1.095 1.093 0.076
2024-12-02-07:27:14-root-INFO: grad norm: 1.045 1.043 0.062
2024-12-02-07:27:15-root-INFO: grad norm: 0.981 0.978 0.076
2024-12-02-07:27:15-root-INFO: grad norm: 0.914 0.912 0.057
2024-12-02-07:27:16-root-INFO: grad norm: 0.866 0.863 0.069
2024-12-02-07:27:16-root-INFO: grad norm: 0.825 0.823 0.053
2024-12-02-07:27:16-root-INFO: Loss Change: 5.645 -> 3.319
2024-12-02-07:27:16-root-INFO: Regularization Change: 0.000 -> 6.154
2024-12-02-07:27:16-root-INFO: Learning rate of xt decay: 0.35023 -> 0.35443.
2024-12-02-07:27:16-root-INFO: Coefficient of regularization decay: 0.00109 -> 0.00110.
2024-12-02-07:27:17-root-INFO: step: 8 lr_xt 0.35200918
2024-12-02-07:27:17-root-INFO: grad norm: 1.069 1.059 0.146
2024-12-02-07:27:17-root-INFO: grad norm: 0.687 0.685 0.044
2024-12-02-07:27:18-root-INFO: grad norm: 0.624 0.623 0.039
2024-12-02-07:27:18-root-INFO: grad norm: 0.602 0.601 0.037
2024-12-02-07:27:19-root-INFO: grad norm: 0.591 0.590 0.042
2024-12-02-07:27:19-root-INFO: grad norm: 0.586 0.584 0.038
2024-12-02-07:27:20-root-INFO: grad norm: 0.582 0.580 0.044
2024-12-02-07:27:20-root-INFO: grad norm: 0.577 0.575 0.039
2024-12-02-07:27:20-root-INFO: Loss Change: 3.193 -> 2.385
2024-12-02-07:27:20-root-INFO: Regularization Change: 0.000 -> 2.050
2024-12-02-07:27:20-root-INFO: Learning rate of xt decay: 0.35443 -> 0.35869.
2024-12-02-07:27:20-root-INFO: Coefficient of regularization decay: 0.00110 -> 0.00111.
2024-12-02-07:27:21-root-INFO: step: 7 lr_xt 0.35673794
2024-12-02-07:27:21-root-INFO: grad norm: 0.946 0.932 0.161
2024-12-02-07:27:21-root-INFO: grad norm: 0.490 0.489 0.032
2024-12-02-07:27:22-root-INFO: grad norm: 0.417 0.416 0.026
2024-12-02-07:27:22-root-INFO: grad norm: 0.389 0.388 0.023
2024-12-02-07:27:23-root-INFO: grad norm: 0.372 0.371 0.023
2024-12-02-07:27:23-root-INFO: grad norm: 0.359 0.359 0.022
2024-12-02-07:27:24-root-INFO: grad norm: 0.351 0.350 0.023
2024-12-02-07:27:24-root-INFO: grad norm: 0.344 0.343 0.021
2024-12-02-07:27:24-root-INFO: Loss Change: 2.382 -> 1.934
2024-12-02-07:27:24-root-INFO: Regularization Change: 0.000 -> 1.019
2024-12-02-07:27:24-root-INFO: Learning rate of xt decay: 0.35869 -> 0.36299.
2024-12-02-07:27:24-root-INFO: Coefficient of regularization decay: 0.00111 -> 0.00112.
2024-12-02-07:27:25-root-INFO: step: 6 lr_xt 0.36147257
2024-12-02-07:27:25-root-INFO: grad norm: 0.903 0.885 0.176
2024-12-02-07:27:25-root-INFO: grad norm: 0.426 0.424 0.036
2024-12-02-07:27:26-root-INFO: grad norm: 0.301 0.300 0.024
2024-12-02-07:27:26-root-INFO: grad norm: 0.269 0.268 0.021
2024-12-02-07:27:27-root-INFO: grad norm: 0.250 0.249 0.021
2024-12-02-07:27:27-root-INFO: grad norm: 0.238 0.237 0.020
2024-12-02-07:27:28-root-INFO: grad norm: 0.230 0.229 0.020
2024-12-02-07:27:28-root-INFO: grad norm: 0.221 0.221 0.019
2024-12-02-07:27:28-root-INFO: Loss Change: 2.016 -> 1.693
2024-12-02-07:27:28-root-INFO: Regularization Change: 0.000 -> 0.671
2024-12-02-07:27:28-root-INFO: Learning rate of xt decay: 0.36299 -> 0.36735.
2024-12-02-07:27:28-root-INFO: Coefficient of regularization decay: 0.00112 -> 0.00113.
2024-12-02-07:27:28-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-07:27:29-root-INFO: grad norm: 0.927 0.908 0.183
2024-12-02-07:27:29-root-INFO: grad norm: 0.432 0.430 0.036
2024-12-02-07:27:30-root-INFO: grad norm: 0.291 0.290 0.027
2024-12-02-07:27:30-root-INFO: grad norm: 0.240 0.239 0.023
2024-12-02-07:27:30-root-INFO: grad norm: 0.220 0.219 0.023
2024-12-02-07:27:31-root-INFO: grad norm: 0.204 0.203 0.022
2024-12-02-07:27:31-root-INFO: grad norm: 0.197 0.195 0.022
2024-12-02-07:27:32-root-INFO: grad norm: 0.189 0.188 0.021
2024-12-02-07:27:32-root-INFO: Loss Change: 1.833 -> 1.544
2024-12-02-07:27:32-root-INFO: Regularization Change: 0.000 -> 0.552
2024-12-02-07:27:32-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-07:27:32-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-07:27:32-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-07:27:33-root-INFO: grad norm: 0.827 0.815 0.144
2024-12-02-07:27:33-root-INFO: grad norm: 0.391 0.389 0.034
2024-12-02-07:27:34-root-INFO: grad norm: 0.256 0.254 0.028
2024-12-02-07:27:34-root-INFO: grad norm: 0.236 0.235 0.025
2024-12-02-07:27:35-root-INFO: grad norm: 0.205 0.203 0.025
2024-12-02-07:27:35-root-INFO: grad norm: 0.195 0.193 0.024
2024-12-02-07:27:35-root-INFO: grad norm: 0.189 0.188 0.024
2024-12-02-07:27:36-root-INFO: grad norm: 0.185 0.184 0.024
2024-12-02-07:27:36-root-INFO: Loss Change: 1.690 -> 1.441
2024-12-02-07:27:36-root-INFO: Regularization Change: 0.000 -> 0.482
2024-12-02-07:27:36-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-07:27:36-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-07:27:36-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-07:27:37-root-INFO: grad norm: 0.877 0.867 0.134
2024-12-02-07:27:37-root-INFO: grad norm: 0.407 0.406 0.034
2024-12-02-07:27:37-root-INFO: grad norm: 0.244 0.243 0.028
2024-12-02-07:27:38-root-INFO: grad norm: 0.207 0.205 0.028
2024-12-02-07:27:38-root-INFO: grad norm: 0.187 0.185 0.026
2024-12-02-07:27:39-root-INFO: grad norm: 0.174 0.172 0.026
2024-12-02-07:27:39-root-INFO: grad norm: 0.164 0.162 0.025
2024-12-02-07:27:40-root-INFO: grad norm: 0.157 0.155 0.025
2024-12-02-07:27:40-root-INFO: Loss Change: 1.611 -> 1.363
2024-12-02-07:27:40-root-INFO: Regularization Change: 0.000 -> 0.466
2024-12-02-07:27:40-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-07:27:40-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-07:27:40-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-07:27:41-root-INFO: grad norm: 0.770 0.764 0.091
2024-12-02-07:27:41-root-INFO: grad norm: 0.353 0.351 0.032
2024-12-02-07:27:41-root-INFO: grad norm: 0.241 0.239 0.029
2024-12-02-07:27:42-root-INFO: grad norm: 0.200 0.198 0.029
2024-12-02-07:27:42-root-INFO: grad norm: 0.186 0.184 0.027
2024-12-02-07:27:43-root-INFO: grad norm: 0.207 0.205 0.027
2024-12-02-07:27:43-root-INFO: grad norm: 0.164 0.162 0.026
2024-12-02-07:27:44-root-INFO: grad norm: 0.168 0.166 0.025
2024-12-02-07:27:44-root-INFO: Loss Change: 1.524 -> 1.297
2024-12-02-07:27:44-root-INFO: Regularization Change: 0.000 -> 0.449
2024-12-02-07:27:44-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-07:27:44-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-07:27:44-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-07:27:44-root-INFO: grad norm: 0.735 0.732 0.070
2024-12-02-07:27:45-root-INFO: grad norm: 0.319 0.317 0.030
2024-12-02-07:27:45-root-INFO: grad norm: 0.254 0.252 0.025
2024-12-02-07:27:46-root-INFO: grad norm: 0.216 0.215 0.024
2024-12-02-07:27:46-root-INFO: grad norm: 0.203 0.202 0.023
2024-12-02-07:27:47-root-INFO: grad norm: 0.193 0.191 0.022
2024-12-02-07:27:47-root-INFO: grad norm: 0.183 0.182 0.022
2024-12-02-07:27:48-root-INFO: grad norm: 0.193 0.192 0.021
2024-12-02-07:27:48-root-INFO: Loss Change: 1.436 -> 1.195
2024-12-02-07:27:48-root-INFO: Regularization Change: 0.000 -> 0.518
2024-12-02-07:27:48-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-07:27:48-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-07:27:48-root-INFO: step: 0 lr_xt 0.38992188
2024-12-02-07:27:49-root-INFO: grad norm: 0.764 0.759 0.084
2024-12-02-07:27:49-root-INFO: grad norm: 0.469 0.468 0.031
2024-12-02-07:27:50-root-INFO: grad norm: 0.383 0.382 0.028
2024-12-02-07:27:50-root-INFO: grad norm: 0.343 0.340 0.038
2024-12-02-07:27:50-root-INFO: grad norm: 0.320 0.317 0.046
2024-12-02-07:27:51-root-INFO: grad norm: 0.307 0.303 0.050
2024-12-02-07:27:51-root-INFO: grad norm: 0.299 0.294 0.051
2024-12-02-07:27:52-root-INFO: grad norm: 0.294 0.289 0.052
2024-12-02-07:27:52-root-INFO: Loss Change: 1.334 -> 0.841
2024-12-02-07:27:52-root-INFO: Regularization Change: 0.000 -> 1.297
2024-12-02-07:27:52-root-INFO: Undo step: 0
2024-12-02-07:27:52-root-INFO: Undo step: 1
2024-12-02-07:27:52-root-INFO: Undo step: 2
2024-12-02-07:27:52-root-INFO: Undo step: 3
2024-12-02-07:27:52-root-INFO: Undo step: 4
2024-12-02-07:27:52-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-07:27:53-root-INFO: grad norm: 11.668 11.662 0.393
2024-12-02-07:27:53-root-INFO: grad norm: 4.990 4.986 0.203
2024-12-02-07:27:54-root-INFO: grad norm: 3.180 3.177 0.132
2024-12-02-07:27:54-root-INFO: grad norm: 1.893 1.890 0.105
2024-12-02-07:27:54-root-INFO: grad norm: 1.585 1.583 0.076
2024-12-02-07:27:55-root-INFO: grad norm: 1.094 1.092 0.065
2024-12-02-07:27:55-root-INFO: grad norm: 0.915 0.914 0.052
2024-12-02-07:27:56-root-INFO: grad norm: 0.789 0.787 0.047
2024-12-02-07:27:56-root-INFO: Loss Change: 35.981 -> 2.803
2024-12-02-07:27:56-root-INFO: Regularization Change: 0.000 -> 53.110
2024-12-02-07:27:56-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-07:27:56-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-07:27:56-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-07:27:56-root-INFO: grad norm: 0.985 0.981 0.090
2024-12-02-07:27:57-root-INFO: grad norm: 0.672 0.671 0.033
2024-12-02-07:27:57-root-INFO: grad norm: 0.576 0.576 0.030
2024-12-02-07:27:58-root-INFO: grad norm: 0.555 0.554 0.029
2024-12-02-07:27:58-root-INFO: grad norm: 0.461 0.460 0.027
2024-12-02-07:27:59-root-INFO: grad norm: 0.420 0.419 0.026
2024-12-02-07:27:59-root-INFO: grad norm: 0.386 0.385 0.025
2024-12-02-07:28:00-root-INFO: grad norm: 0.358 0.357 0.024
2024-12-02-07:28:00-root-INFO: Loss Change: 2.697 -> 1.810
2024-12-02-07:28:00-root-INFO: Regularization Change: 0.000 -> 2.315
2024-12-02-07:28:00-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-07:28:00-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-07:28:00-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-07:28:00-root-INFO: grad norm: 0.801 0.795 0.097
2024-12-02-07:28:01-root-INFO: grad norm: 0.448 0.447 0.029
2024-12-02-07:28:01-root-INFO: grad norm: 0.352 0.351 0.026
2024-12-02-07:28:02-root-INFO: grad norm: 0.318 0.317 0.025
2024-12-02-07:28:02-root-INFO: grad norm: 0.293 0.292 0.024
2024-12-02-07:28:03-root-INFO: grad norm: 0.275 0.274 0.024
2024-12-02-07:28:03-root-INFO: grad norm: 0.259 0.258 0.022
2024-12-02-07:28:04-root-INFO: grad norm: 0.244 0.243 0.022
2024-12-02-07:28:04-root-INFO: Loss Change: 1.862 -> 1.476
2024-12-02-07:28:04-root-INFO: Regularization Change: 0.000 -> 0.936
2024-12-02-07:28:04-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-07:28:04-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-07:28:04-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-07:28:04-root-INFO: grad norm: 0.749 0.744 0.083
2024-12-02-07:28:05-root-INFO: grad norm: 0.382 0.381 0.029
2024-12-02-07:28:05-root-INFO: grad norm: 0.291 0.290 0.025
2024-12-02-07:28:06-root-INFO: grad norm: 0.247 0.245 0.026
2024-12-02-07:28:06-root-INFO: grad norm: 0.224 0.223 0.025
2024-12-02-07:28:07-root-INFO: grad norm: 0.210 0.209 0.025
2024-12-02-07:28:07-root-INFO: grad norm: 0.201 0.200 0.024
2024-12-02-07:28:07-root-INFO: grad norm: 0.187 0.186 0.024
2024-12-02-07:28:08-root-INFO: Loss Change: 1.576 -> 1.307
2024-12-02-07:28:08-root-INFO: Regularization Change: 0.000 -> 0.597
2024-12-02-07:28:08-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-07:28:08-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-07:28:08-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-07:28:08-root-INFO: grad norm: 0.693 0.690 0.068
2024-12-02-07:28:09-root-INFO: grad norm: 0.336 0.335 0.028
2024-12-02-07:28:09-root-INFO: grad norm: 0.248 0.247 0.024
2024-12-02-07:28:10-root-INFO: grad norm: 0.221 0.220 0.024
2024-12-02-07:28:10-root-INFO: grad norm: 0.236 0.235 0.023
2024-12-02-07:28:10-root-INFO: grad norm: 0.205 0.203 0.023
2024-12-02-07:28:11-root-INFO: grad norm: 0.183 0.181 0.022
2024-12-02-07:28:11-root-INFO: grad norm: 0.172 0.171 0.021
2024-12-02-07:28:12-root-INFO: Loss Change: 1.412 -> 1.178
2024-12-02-07:28:12-root-INFO: Regularization Change: 0.000 -> 0.521
2024-12-02-07:28:12-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-07:28:12-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-07:28:12-root-INFO: step: 0 lr_xt 0.38992188
2024-12-02-07:28:12-root-INFO: grad norm: 0.745 0.741 0.079
2024-12-02-07:28:12-root-INFO: grad norm: 0.463 0.462 0.027
2024-12-02-07:28:13-root-INFO: grad norm: 0.382 0.381 0.025
2024-12-02-07:28:13-root-INFO: grad norm: 0.343 0.341 0.037
2024-12-02-07:28:14-root-INFO: grad norm: 0.322 0.319 0.044
2024-12-02-07:28:14-root-INFO: grad norm: 0.309 0.306 0.048
2024-12-02-07:28:15-root-INFO: grad norm: 0.302 0.298 0.049
2024-12-02-07:28:15-root-INFO: grad norm: 0.296 0.292 0.049
2024-12-02-07:28:16-root-INFO: Loss Change: 1.297 -> 0.811
2024-12-02-07:28:16-root-INFO: Regularization Change: 0.000 -> 1.291
2024-12-02-07:28:16-root-INFO: Undo step: 0
2024-12-02-07:28:16-root-INFO: Undo step: 1
2024-12-02-07:28:16-root-INFO: Undo step: 2
2024-12-02-07:28:16-root-INFO: Undo step: 3
2024-12-02-07:28:16-root-INFO: Undo step: 4
2024-12-02-07:28:16-root-INFO: step: 5 lr_xt 0.36621164
2024-12-02-07:28:16-root-INFO: grad norm: 13.261 13.253 0.464
2024-12-02-07:28:17-root-INFO: grad norm: 5.325 5.320 0.229
2024-12-02-07:28:17-root-INFO: grad norm: 2.702 2.699 0.122
2024-12-02-07:28:17-root-INFO: grad norm: 2.096 2.093 0.102
2024-12-02-07:28:18-root-INFO: grad norm: 1.646 1.645 0.073
2024-12-02-07:28:19-root-INFO: grad norm: 1.411 1.409 0.068
2024-12-02-07:28:19-root-INFO: grad norm: 0.996 0.995 0.051
2024-12-02-07:28:20-root-INFO: grad norm: 0.894 0.892 0.048
2024-12-02-07:28:20-root-INFO: Loss Change: 38.047 -> 3.026
2024-12-02-07:28:20-root-INFO: Regularization Change: 0.000 -> 56.733
2024-12-02-07:28:20-root-INFO: Learning rate of xt decay: 0.36735 -> 0.37175.
2024-12-02-07:28:20-root-INFO: Coefficient of regularization decay: 0.00113 -> 0.00114.
2024-12-02-07:28:20-root-INFO: step: 4 lr_xt 0.37095370
2024-12-02-07:28:20-root-INFO: grad norm: 1.053 1.048 0.103
2024-12-02-07:28:21-root-INFO: grad norm: 0.710 0.709 0.038
2024-12-02-07:28:21-root-INFO: grad norm: 0.586 0.585 0.033
2024-12-02-07:28:22-root-INFO: grad norm: 0.517 0.516 0.031
2024-12-02-07:28:22-root-INFO: grad norm: 0.476 0.475 0.030
2024-12-02-07:28:23-root-INFO: grad norm: 0.446 0.445 0.028
2024-12-02-07:28:23-root-INFO: grad norm: 0.451 0.450 0.031
2024-12-02-07:28:24-root-INFO: grad norm: 0.429 0.428 0.029
2024-12-02-07:28:24-root-INFO: Loss Change: 2.927 -> 2.039
2024-12-02-07:28:24-root-INFO: Regularization Change: 0.000 -> 2.320
2024-12-02-07:28:24-root-INFO: Learning rate of xt decay: 0.37175 -> 0.37621.
2024-12-02-07:28:24-root-INFO: Coefficient of regularization decay: 0.00114 -> 0.00116.
2024-12-02-07:28:24-root-INFO: step: 3 lr_xt 0.37569726
2024-12-02-07:28:24-root-INFO: grad norm: 0.931 0.924 0.116
2024-12-02-07:28:25-root-INFO: grad norm: 0.499 0.498 0.031
2024-12-02-07:28:25-root-INFO: grad norm: 0.364 0.363 0.025
2024-12-02-07:28:26-root-INFO: grad norm: 0.318 0.317 0.025
2024-12-02-07:28:27-root-INFO: grad norm: 0.295 0.294 0.024
2024-12-02-07:28:27-root-INFO: grad norm: 0.280 0.279 0.024
2024-12-02-07:28:28-root-INFO: grad norm: 0.268 0.267 0.023
2024-12-02-07:28:28-root-INFO: grad norm: 0.262 0.261 0.023
2024-12-02-07:28:28-root-INFO: Loss Change: 2.107 -> 1.708
2024-12-02-07:28:28-root-INFO: Regularization Change: 0.000 -> 0.943
2024-12-02-07:28:28-root-INFO: Learning rate of xt decay: 0.37621 -> 0.38073.
2024-12-02-07:28:28-root-INFO: Coefficient of regularization decay: 0.00116 -> 0.00117.
2024-12-02-07:28:29-root-INFO: step: 2 lr_xt 0.38044082
2024-12-02-07:28:29-root-INFO: grad norm: 0.690 0.686 0.074
2024-12-02-07:28:29-root-INFO: grad norm: 0.384 0.383 0.028
2024-12-02-07:28:30-root-INFO: grad norm: 0.295 0.294 0.026
2024-12-02-07:28:30-root-INFO: grad norm: 0.264 0.263 0.027
2024-12-02-07:28:31-root-INFO: grad norm: 0.258 0.257 0.024
2024-12-02-07:28:31-root-INFO: grad norm: 0.228 0.227 0.025
2024-12-02-07:28:32-root-INFO: grad norm: 0.213 0.212 0.023
2024-12-02-07:28:32-root-INFO: grad norm: 0.200 0.199 0.024
2024-12-02-07:28:33-root-INFO: Loss Change: 1.805 -> 1.527
2024-12-02-07:28:33-root-INFO: Regularization Change: 0.000 -> 0.650
2024-12-02-07:28:33-root-INFO: Learning rate of xt decay: 0.38073 -> 0.38530.
2024-12-02-07:28:33-root-INFO: Coefficient of regularization decay: 0.00117 -> 0.00118.
2024-12-02-07:28:33-root-INFO: step: 1 lr_xt 0.38518288
2024-12-02-07:28:33-root-INFO: grad norm: 0.708 0.705 0.073
2024-12-02-07:28:33-root-INFO: grad norm: 0.401 0.400 0.028
2024-12-02-07:28:34-root-INFO: grad norm: 1.326 1.326 0.024
2024-12-02-07:28:34-root-INFO: Loss too large (1.424->1.438)! Learning rate decreased to 0.30815.
2024-12-02-07:28:35-root-INFO: grad norm: 0.534 0.533 0.022
2024-12-02-07:28:35-root-INFO: grad norm: 0.220 0.219 0.021
2024-12-02-07:28:36-root-INFO: grad norm: 0.219 0.218 0.021
2024-12-02-07:28:36-root-INFO: grad norm: 0.335 0.335 0.021
2024-12-02-07:28:36-root-INFO: grad norm: 0.339 0.338 0.020
2024-12-02-07:28:37-root-INFO: Loss Change: 1.632 -> 1.202
2024-12-02-07:28:37-root-INFO: Regularization Change: 0.000 -> 0.634
2024-12-02-07:28:37-root-INFO: Learning rate of xt decay: 0.38530 -> 0.38992.
2024-12-02-07:28:37-root-INFO: Coefficient of regularization decay: 0.00118 -> 0.00119.
2024-12-02-07:28:37-root-INFO: loss_sample0_0: 1.2024643421173096
2024-12-02-07:28:37-root-INFO: It takes 3643.529 seconds for image sample0
2024-12-02-07:28:37-root-INFO: lpips_score_sample0: 0.147
2024-12-02-07:28:37-root-INFO: psnr_score_sample0: 18.209
2024-12-02-07:28:37-root-INFO: ssim_score_sample0: 0.733
2024-12-02-07:28:37-root-INFO: mean_lpips: 0.1469510793685913
2024-12-02-07:28:37-root-INFO: best_mean_lpips: 0.1469510793685913
2024-12-02-07:28:37-root-INFO: mean_psnr: 18.208988189697266
2024-12-02-07:28:37-root-INFO: best_mean_psnr: 18.208988189697266
2024-12-02-07:28:37-root-INFO: mean_ssim: 0.7328065633773804
2024-12-02-07:28:37-root-INFO: best_mean_ssim: 0.7328065633773804
2024-12-02-07:28:37-root-INFO: final_loss: 1.2024643421173096
2024-12-02-07:28:37-root-INFO: mean time: 3643.528570175171
2024-12-02-07:28:37-root-INFO: Your samples are ready and waiting for you here: 
./hyperparam_eval/results/celebahq_o_ddim_jump5_sample3_iter8_lr0.02_10009 
 
Enjoy.
