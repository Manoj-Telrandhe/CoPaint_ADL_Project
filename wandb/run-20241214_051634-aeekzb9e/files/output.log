2024-12-14-05:16:38-root-INFO: Load 100 samples
2024-12-14-05:16:38-root-INFO: Prepare model...
2024-12-14-05:16:42-root-INFO: Loading model from ./checkpoints/256x256_diffusion.pt...
/home/shashank23088/Documents/shashank/adl_project/CoPaint/guided_diffusion/dist_util.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return th.load(io.BytesIO(data), **kwargs)
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
/home/shashank23088/anaconda3/envs/copaint/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/shashank23088/anaconda3/envs/copaint/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading model from: /home/shashank23088/anaconda3/envs/copaint/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
/home/shashank23088/anaconda3/envs/copaint/lib/python3.12/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)
2024-12-14-05:16:50-root-INFO: Start sampling
  0%|                                                                                                                                                                                                                 | 0/100 [00:00<?, ?it/s]
  2%|████                                                                                                                                                                                                     | 5/249 [00:07<04:43,  1.16s/it]
2024-12-14-05:16:52-root-INFO: step: 249 lr_xt 0.00012706
2024-12-14-05:16:52-root-INFO: grad norm: 12223.657 8651.604 8635.250
2024-12-14-05:16:53-root-INFO: grad norm: 7709.029 5464.913 5437.266
2024-12-14-05:16:53-root-INFO: Loss Change: 77965.812 -> 56362.418
2024-12-14-05:16:53-root-INFO: Regularization Change: 0.000 -> 6.069
2024-12-14-05:16:53-root-INFO: Learning rate of xt decay: 0.02000 -> 0.02024.
2024-12-14-05:16:53-root-INFO: step: 248 lr_xt 0.00013388
2024-12-14-05:16:53-root-INFO: grad norm: 7231.769 5134.106 5093.077
2024-12-14-05:16:54-root-INFO: grad norm: 7192.181 5106.565 5064.629
2024-12-14-05:16:54-root-INFO: Loss Change: 57385.566 -> 43555.340
2024-12-14-05:16:54-root-INFO: Regularization Change: 0.000 -> 3.695
2024-12-14-05:16:54-root-INFO: Learning rate of xt decay: 0.02024 -> 0.02048.
2024-12-14-05:16:54-root-INFO: step: 247 lr_xt 0.00014104
2024-12-14-05:16:54-root-INFO: grad norm: 7199.324 5106.562 5074.770
2024-12-14-05:16:55-root-INFO: grad norm: 6482.087 4595.272 4571.753
2024-12-14-05:16:55-root-INFO: Loss Change: 42930.648 -> 30506.898
2024-12-14-05:16:55-root-INFO: Regularization Change: 0.000 -> 3.708
2024-12-14-05:16:55-root-INFO: Learning rate of xt decay: 0.02048 -> 0.02073.
2024-12-14-05:16:55-root-INFO: step: 246 lr_xt 0.00014856
2024-12-14-05:16:55-root-INFO: grad norm: 5096.892 3612.931 3595.141
2024-12-14-05:16:56-root-INFO: grad norm: 3621.510 2567.446 2554.125
2024-12-14-05:16:56-root-INFO: Loss Change: 29358.723 -> 24396.070
2024-12-14-05:16:56-root-INFO: Regularization Change: 0.000 -> 1.669
2024-12-14-05:16:56-root-INFO: Learning rate of xt decay: 0.02073 -> 0.02098.
2024-12-14-05:16:56-root-INFO: step: 245 lr_xt 0.00015646
2024-12-14-05:16:56-root-INFO: grad norm: 2424.556 1720.400 1708.419
2024-12-14-05:16:57-root-INFO: grad norm: 1842.761 1310.210 1295.808
2024-12-14-05:16:57-root-INFO: Loss Change: 24138.299 -> 22820.766
2024-12-14-05:16:57-root-INFO: Regularization Change: 0.000 -> 0.440
2024-12-14-05:16:57-root-INFO: Learning rate of xt decay: 0.02098 -> 0.02123.
2024-12-14-05:16:57-root-INFO: step: 244 lr_xt 0.00016475
2024-12-14-05:16:57-root-INFO: grad norm: 1446.139 1033.264 1011.772
2024-12-14-05:16:58-root-INFO: grad norm: 1327.806 950.150 927.516
2024-12-14-05:16:58-root-INFO: Loss Change: 22737.572 -> 22101.328
2024-12-14-05:16:58-root-INFO: Regularization Change: 0.000 -> 0.208
2024-12-14-05:16:58-root-INFO: Learning rate of xt decay: 0.02123 -> 0.02148.
2024-12-14-05:16:58-root-INFO: step: 243 lr_xt 0.00017345
2024-12-14-05:16:58-root-INFO: grad norm: 1235.156 884.302 862.334
2024-12-14-05:16:59-root-INFO: grad norm: 1187.886 850.704 829.081
2024-12-14-05:16:59-root-INFO: Loss Change: 21916.730 -> 21424.998
2024-12-14-05:16:59-root-INFO: Regularization Change: 0.000 -> 0.176
2024-12-14-05:16:59-root-INFO: Learning rate of xt decay: 0.02148 -> 0.02174.
2024-12-14-05:16:59-root-INFO: step: 242 lr_xt 0.00018258
2024-12-14-05:16:59-root-INFO: grad norm: 1258.388 900.060 879.450
2024-12-14-05:17:00-root-INFO: grad norm: 1167.057 835.125 815.222
2024-12-14-05:17:00-root-INFO: Loss Change: 21262.119 -> 20754.926
2024-12-14-05:17:00-root-INFO: Regularization Change: 0.000 -> 0.195
2024-12-14-05:17:00-root-INFO: Learning rate of xt decay: 0.02174 -> 0.02200.
2024-12-14-05:17:00-root-INFO: step: 241 lr_xt 0.00019216
2024-12-14-05:17:00-root-INFO: grad norm: 1215.212 869.245 849.207
2024-12-14-05:17:01-root-INFO: grad norm: 1130.833 809.094 790.032
2024-12-14-05:17:01-root-INFO: Loss Change: 20668.246 -> 20147.316
2024-12-14-05:17:01-root-INFO: Regularization Change: 0.000 -> 0.202
2024-12-14-05:17:01-root-INFO: Learning rate of xt decay: 0.02200 -> 0.02227.
2024-12-14-05:17:01-root-INFO: step: 240 lr_xt 0.00020221
2024-12-14-05:17:01-root-INFO: grad norm: 1191.437 851.944 832.895
2024-12-14-05:17:02-root-INFO: grad norm: 1107.292 792.052 773.790
2024-12-14-05:17:02-root-INFO: Loss Change: 20047.160 -> 19529.760
2024-12-14-05:17:02-root-INFO: Regularization Change: 0.000 -> 0.214
2024-12-14-05:17:02-root-INFO: Learning rate of xt decay: 0.02227 -> 0.02253.
2024-12-14-05:17:02-root-INFO: step: 239 lr_xt 0.00021275
2024-12-14-05:17:02-root-INFO: grad norm: 1247.520 891.023 873.146
2024-12-14-05:17:03-root-INFO: grad norm: 1110.256 793.392 776.658
2024-12-14-05:17:03-root-INFO: Loss Change: 19549.822 -> 18996.330
2024-12-14-05:17:03-root-INFO: Regularization Change: 0.000 -> 0.247
2024-12-14-05:17:03-root-INFO: Learning rate of xt decay: 0.02253 -> 0.02280.
2024-12-14-05:17:03-root-INFO: step: 238 lr_xt 0.00022380
2024-12-14-05:17:03-root-INFO: grad norm: 1104.883 789.749 772.699
2024-12-14-05:17:04-root-INFO: grad norm: 1066.909 762.455 746.296
2024-12-14-05:17:04-root-INFO: Loss Change: 18888.316 -> 18367.301
2024-12-14-05:17:04-root-INFO: Regularization Change: 0.000 -> 0.235
2024-12-14-05:17:04-root-INFO: Learning rate of xt decay: 0.02280 -> 0.02308.
2024-12-14-05:17:04-root-INFO: step: 237 lr_xt 0.00023539
2024-12-14-05:17:04-root-INFO: grad norm: 1353.620 965.488 948.745
2024-12-14-05:17:05-root-INFO: grad norm: 1117.555 797.783 782.605
2024-12-14-05:17:05-root-INFO: Loss Change: 18386.852 -> 17721.641
2024-12-14-05:17:05-root-INFO: Regularization Change: 0.000 -> 0.325
2024-12-14-05:17:05-root-INFO: Learning rate of xt decay: 0.02308 -> 0.02335.
2024-12-14-05:17:05-root-INFO: step: 236 lr_xt 0.00024753
2024-12-14-05:17:05-root-INFO: grad norm: 1093.256 780.756 765.264
2024-12-14-05:17:06-root-INFO: grad norm: 1052.724 751.724 736.979
2024-12-14-05:17:06-root-INFO: Loss Change: 17574.203 -> 17011.082
2024-12-14-05:17:06-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-14-05:17:06-root-INFO: Learning rate of xt decay: 0.02335 -> 0.02364.
2024-12-14-05:17:06-root-INFO: step: 235 lr_xt 0.00026027
2024-12-14-05:17:06-root-INFO: grad norm: 1146.913 817.811 804.112
2024-12-14-05:17:07-root-INFO: grad norm: 1057.302 753.922 741.275
2024-12-14-05:17:07-root-INFO: Loss Change: 16959.389 -> 16335.461
2024-12-14-05:17:07-root-INFO: Regularization Change: 0.000 -> 0.323
2024-12-14-05:17:07-root-INFO: Learning rate of xt decay: 0.02364 -> 0.02392.
2024-12-14-05:17:07-root-INFO: step: 234 lr_xt 0.00027361
2024-12-14-05:17:07-root-INFO: grad norm: 1086.376 775.291 761.011
2024-12-14-05:17:08-root-INFO: grad norm: 1006.938 718.318 705.651
2024-12-14-05:17:08-root-INFO: Loss Change: 16229.873 -> 15638.969
2024-12-14-05:17:08-root-INFO: Regularization Change: 0.000 -> 0.321
2024-12-14-05:17:08-root-INFO: Learning rate of xt decay: 0.02392 -> 0.02421.
2024-12-14-05:17:08-root-INFO: step: 233 lr_xt 0.00028759
2024-12-14-05:17:08-root-INFO: grad norm: 1076.863 767.633 755.231
2024-12-14-05:17:09-root-INFO: grad norm: 974.531 694.494 683.658
2024-12-14-05:17:09-root-INFO: Loss Change: 15518.852 -> 14949.127
2024-12-14-05:17:09-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-14-05:17:09-root-INFO: Learning rate of xt decay: 0.02421 -> 0.02450.
2024-12-14-05:17:09-root-INFO: step: 232 lr_xt 0.00030224
2024-12-14-05:17:09-root-INFO: grad norm: 1022.333 728.885 716.862
2024-12-14-05:17:09-root-INFO: grad norm: 910.655 649.044 638.776
2024-12-14-05:17:10-root-INFO: Loss Change: 14845.954 -> 14314.082
2024-12-14-05:17:10-root-INFO: Regularization Change: 0.000 -> 0.327
2024-12-14-05:17:10-root-INFO: Learning rate of xt decay: 0.02450 -> 0.02479.
2024-12-14-05:17:10-root-INFO: step: 231 lr_xt 0.00031758
2024-12-14-05:17:10-root-INFO: grad norm: 893.323 636.554 626.758
2024-12-14-05:17:10-root-INFO: grad norm: 833.339 593.611 584.876
2024-12-14-05:17:11-root-INFO: Loss Change: 14167.053 -> 13715.615
2024-12-14-05:17:11-root-INFO: Regularization Change: 0.000 -> 0.296
2024-12-14-05:17:11-root-INFO: Learning rate of xt decay: 0.02479 -> 0.02509.
2024-12-14-05:17:11-root-INFO: step: 230 lr_xt 0.00033364
2024-12-14-05:17:11-root-INFO: grad norm: 976.598 694.904 686.187
2024-12-14-05:17:11-root-INFO: grad norm: 766.536 545.573 538.449
2024-12-14-05:17:12-root-INFO: Loss Change: 13682.389 -> 13240.377
2024-12-14-05:17:12-root-INFO: Regularization Change: 0.000 -> 0.300
2024-12-14-05:17:12-root-INFO: Learning rate of xt decay: 0.02509 -> 0.02539.
2024-12-14-05:17:12-root-INFO: step: 229 lr_xt 0.00035047
2024-12-14-05:17:12-root-INFO: grad norm: 769.873 548.260 540.477
2024-12-14-05:17:12-root-INFO: grad norm: 674.838 480.649 473.692
2024-12-14-05:17:13-root-INFO: Loss Change: 13157.235 -> 12820.178
2024-12-14-05:17:13-root-INFO: Regularization Change: 0.000 -> 0.240
2024-12-14-05:17:13-root-INFO: Learning rate of xt decay: 0.02539 -> 0.02569.
2024-12-14-05:17:13-root-INFO: step: 228 lr_xt 0.00036807
2024-12-14-05:17:13-root-INFO: grad norm: 735.497 523.769 516.354
2024-12-14-05:17:13-root-INFO: grad norm: 625.024 445.479 438.410
2024-12-14-05:17:14-root-INFO: Loss Change: 12767.480 -> 12471.316
2024-12-14-05:17:14-root-INFO: Regularization Change: 0.000 -> 0.225
2024-12-14-05:17:14-root-INFO: Learning rate of xt decay: 0.02569 -> 0.02600.
2024-12-14-05:17:14-root-INFO: step: 227 lr_xt 0.00038651
2024-12-14-05:17:14-root-INFO: grad norm: 616.868 439.597 432.759
2024-12-14-05:17:14-root-INFO: grad norm: 571.905 407.803 400.964
2024-12-14-05:17:15-root-INFO: Loss Change: 12412.406 -> 12160.803
2024-12-14-05:17:15-root-INFO: Regularization Change: 0.000 -> 0.202
2024-12-14-05:17:15-root-INFO: Learning rate of xt decay: 0.02600 -> 0.02631.
2024-12-14-05:17:15-root-INFO: step: 226 lr_xt 0.00040579
2024-12-14-05:17:15-root-INFO: grad norm: 860.471 610.908 605.972
2024-12-14-05:17:15-root-INFO: grad norm: 615.506 438.248 432.187
2024-12-14-05:17:16-root-INFO: Loss Change: 12136.971 -> 11819.750
2024-12-14-05:17:16-root-INFO: Regularization Change: 0.000 -> 0.240
2024-12-14-05:17:16-root-INFO: Learning rate of xt decay: 0.02631 -> 0.02663.
2024-12-14-05:17:16-root-INFO: step: 225 lr_xt 0.00042598
2024-12-14-05:17:16-root-INFO: grad norm: 675.454 480.088 475.135
2024-12-14-05:17:16-root-INFO: grad norm: 562.424 400.427 394.942
2024-12-14-05:17:17-root-INFO: Loss Change: 11801.942 -> 11535.768
2024-12-14-05:17:17-root-INFO: Regularization Change: 0.000 -> 0.222
2024-12-14-05:17:17-root-INFO: Learning rate of xt decay: 0.02663 -> 0.02695.
2024-12-14-05:17:17-root-INFO: step: 224 lr_xt 0.00044709
2024-12-14-05:17:17-root-INFO: grad norm: 692.636 492.344 487.178
2024-12-14-05:17:17-root-INFO: grad norm: 584.679 416.321 410.521
2024-12-14-05:17:18-root-INFO: Loss Change: 11500.523 -> 11218.346
2024-12-14-05:17:18-root-INFO: Regularization Change: 0.000 -> 0.238
2024-12-14-05:17:18-root-INFO: Learning rate of xt decay: 0.02695 -> 0.02727.
2024-12-14-05:17:18-root-INFO: step: 223 lr_xt 0.00046917
2024-12-14-05:17:18-root-INFO: grad norm: 870.152 617.664 612.907
2024-12-14-05:17:18-root-INFO: grad norm: 698.210 496.457 490.946
2024-12-14-05:17:19-root-INFO: Loss Change: 11243.081 -> 10896.299
2024-12-14-05:17:19-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-14-05:17:19-root-INFO: Learning rate of xt decay: 0.02727 -> 0.02760.
2024-12-14-05:17:19-root-INFO: step: 222 lr_xt 0.00049227
2024-12-14-05:17:19-root-INFO: grad norm: 832.315 590.317 586.748
2024-12-14-05:17:19-root-INFO: grad norm: 741.685 526.136 522.759
2024-12-14-05:17:20-root-INFO: Loss Change: 10908.612 -> 10540.602
2024-12-14-05:17:20-root-INFO: Regularization Change: 0.000 -> 0.339
2024-12-14-05:17:20-root-INFO: Learning rate of xt decay: 0.02760 -> 0.02793.
2024-12-14-05:17:20-root-INFO: step: 221 lr_xt 0.00051641
2024-12-14-05:17:20-root-INFO: grad norm: 984.101 698.264 693.457
2024-12-14-05:17:20-root-INFO: grad norm: 872.267 619.175 614.388
2024-12-14-05:17:21-root-INFO: Loss Change: 10541.622 -> 10000.070
2024-12-14-05:17:21-root-INFO: Regularization Change: 0.000 -> 0.496
2024-12-14-05:17:21-root-INFO: Learning rate of xt decay: 0.02793 -> 0.02827.
2024-12-14-05:17:21-root-INFO: step: 220 lr_xt 0.00054166
2024-12-14-05:17:21-root-INFO: grad norm: 1055.290 748.281 744.120
2024-12-14-05:17:21-root-INFO: grad norm: 1060.837 751.821 748.424
2024-12-14-05:17:22-root-INFO: Loss Change: 9974.283 -> 8964.663
2024-12-14-05:17:22-root-INFO: Regularization Change: 0.000 -> 0.964
2024-12-14-05:17:22-root-INFO: Learning rate of xt decay: 0.02827 -> 0.02861.
2024-12-14-05:17:22-root-INFO: step: 219 lr_xt 0.00056804
2024-12-14-05:17:22-root-INFO: grad norm: 1270.310 899.391 897.097
2024-12-14-05:17:22-root-INFO: grad norm: 1228.995 869.958 868.103
2024-12-14-05:17:22-root-INFO: Loss Change: 8882.201 -> 7273.908
2024-12-14-05:17:22-root-INFO: Regularization Change: 0.000 -> 1.816
2024-12-14-05:17:22-root-INFO: Learning rate of xt decay: 0.02861 -> 0.02895.
2024-12-14-05:17:23-root-INFO: step: 218 lr_xt 0.00059561
2024-12-14-05:17:23-root-INFO: grad norm: 1296.326 917.204 916.076
2024-12-14-05:17:23-root-INFO: grad norm: 1095.905 775.669 774.174
2024-12-14-05:17:23-root-INFO: Loss Change: 7260.526 -> 6401.695
2024-12-14-05:17:23-root-INFO: Regularization Change: 0.000 -> 1.088
2024-12-14-05:17:23-root-INFO: Learning rate of xt decay: 0.02895 -> 0.02930.
2024-12-14-05:17:24-root-INFO: step: 217 lr_xt 0.00062443
2024-12-14-05:17:24-root-INFO: grad norm: 1181.788 836.752 834.547
2024-12-14-05:17:24-root-INFO: grad norm: 1412.609 999.931 997.799
2024-12-14-05:17:24-root-INFO: Loss too large (6268.339->6384.545)! Learning rate decreased to 0.00050.
2024-12-14-05:17:25-root-INFO: Loss Change: 6413.736 -> 6154.317
2024-12-14-05:17:25-root-INFO: Regularization Change: 0.000 -> 0.433
2024-12-14-05:17:25-root-INFO: Learning rate of xt decay: 0.02930 -> 0.02965.
2024-12-14-05:17:25-root-INFO: step: 216 lr_xt 0.00065452
2024-12-14-05:17:25-root-INFO: grad norm: 1368.342 968.849 966.277
2024-12-14-05:17:25-root-INFO: Loss too large (6120.065->6178.946)! Learning rate decreased to 0.00052.
2024-12-14-05:17:25-root-INFO: grad norm: 1414.273 1001.376 998.707
2024-12-14-05:17:26-root-INFO: Loss too large (5962.243->5994.606)! Learning rate decreased to 0.00042.
2024-12-14-05:17:26-root-INFO: Loss Change: 6120.065 -> 5811.692
2024-12-14-05:17:26-root-INFO: Regularization Change: 0.000 -> 0.310
2024-12-14-05:17:26-root-INFO: Learning rate of xt decay: 0.02965 -> 0.03000.
2024-12-14-05:17:26-root-INFO: step: 215 lr_xt 0.00068596
2024-12-14-05:17:26-root-INFO: grad norm: 1120.416 793.307 791.199
2024-12-14-05:17:26-root-INFO: Loss too large (5750.323->5960.137)! Learning rate decreased to 0.00055.
2024-12-14-05:17:26-root-INFO: Loss too large (5750.323->5766.203)! Learning rate decreased to 0.00044.
2024-12-14-05:17:27-root-INFO: grad norm: 1069.819 757.748 755.202
2024-12-14-05:17:27-root-INFO: Loss Change: 5750.323 -> 5566.016
2024-12-14-05:17:27-root-INFO: Regularization Change: 0.000 -> 0.168
2024-12-14-05:17:27-root-INFO: Learning rate of xt decay: 0.03000 -> 0.03036.
2024-12-14-05:17:27-root-INFO: step: 214 lr_xt 0.00071879
2024-12-14-05:17:27-root-INFO: grad norm: 944.210 668.758 666.554
2024-12-14-05:17:28-root-INFO: Loss too large (5520.979->5660.479)! Learning rate decreased to 0.00058.
2024-12-14-05:17:28-root-INFO: Loss too large (5520.979->5521.577)! Learning rate decreased to 0.00046.
2024-12-14-05:17:28-root-INFO: grad norm: 926.457 656.444 653.761
2024-12-14-05:17:28-root-INFO: Loss Change: 5520.979 -> 5358.223
2024-12-14-05:17:28-root-INFO: Regularization Change: 0.000 -> 0.155
2024-12-14-05:17:28-root-INFO: Learning rate of xt decay: 0.03036 -> 0.03073.
2024-12-14-05:17:28-root-INFO: step: 213 lr_xt 0.00075308
2024-12-14-05:17:29-root-INFO: grad norm: 708.992 502.543 500.120
2024-12-14-05:17:29-root-INFO: Loss too large (5297.108->5298.854)! Learning rate decreased to 0.00060.
2024-12-14-05:17:29-root-INFO: grad norm: 865.252 613.218 610.430
2024-12-14-05:17:29-root-INFO: Loss Change: 5297.108 -> 5219.516
2024-12-14-05:17:29-root-INFO: Regularization Change: 0.000 -> 0.252
2024-12-14-05:17:29-root-INFO: Learning rate of xt decay: 0.03073 -> 0.03110.
2024-12-14-05:17:30-root-INFO: step: 212 lr_xt 0.00078886
2024-12-14-05:17:30-root-INFO: grad norm: 1139.818 806.986 804.959
2024-12-14-05:17:30-root-INFO: Loss too large (5187.093->5522.747)! Learning rate decreased to 0.00063.
2024-12-14-05:17:30-root-INFO: Loss too large (5187.093->5277.536)! Learning rate decreased to 0.00050.
2024-12-14-05:17:30-root-INFO: grad norm: 1063.195 753.112 750.471
2024-12-14-05:17:31-root-INFO: Loss Change: 5187.093 -> 5008.331
2024-12-14-05:17:31-root-INFO: Regularization Change: 0.000 -> 0.151
2024-12-14-05:17:31-root-INFO: Learning rate of xt decay: 0.03110 -> 0.03147.
2024-12-14-05:17:31-root-INFO: step: 211 lr_xt 0.00082622
2024-12-14-05:17:31-root-INFO: grad norm: 824.161 583.772 581.765
2024-12-14-05:17:31-root-INFO: Loss too large (4964.092->5064.442)! Learning rate decreased to 0.00066.
2024-12-14-05:17:32-root-INFO: grad norm: 998.717 707.536 704.860
2024-12-14-05:17:32-root-INFO: Loss Change: 4964.092 -> 4936.005
2024-12-14-05:17:32-root-INFO: Regularization Change: 0.000 -> 0.248
2024-12-14-05:17:32-root-INFO: Learning rate of xt decay: 0.03147 -> 0.03185.
2024-12-14-05:17:32-root-INFO: step: 210 lr_xt 0.00086520
2024-12-14-05:17:32-root-INFO: grad norm: 1026.340 726.645 724.818
2024-12-14-05:17:32-root-INFO: Loss too large (4832.439->5012.117)! Learning rate decreased to 0.00069.
2024-12-14-05:17:33-root-INFO: grad norm: 1117.326 791.429 788.706
2024-12-14-05:17:33-root-INFO: Loss Change: 4832.439 -> 4745.104
2024-12-14-05:17:33-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-14-05:17:33-root-INFO: Learning rate of xt decay: 0.03185 -> 0.03223.
2024-12-14-05:17:33-root-INFO: step: 209 lr_xt 0.00090588
2024-12-14-05:17:33-root-INFO: grad norm: 1167.928 826.720 824.978
2024-12-14-05:17:33-root-INFO: Loss too large (4675.172->4984.060)! Learning rate decreased to 0.00072.
2024-12-14-05:17:33-root-INFO: Loss too large (4675.172->4728.214)! Learning rate decreased to 0.00058.
2024-12-14-05:17:34-root-INFO: grad norm: 898.798 636.831 634.258
2024-12-14-05:17:34-root-INFO: Loss Change: 4675.172 -> 4413.494
2024-12-14-05:17:34-root-INFO: Regularization Change: 0.000 -> 0.187
2024-12-14-05:17:34-root-INFO: Learning rate of xt decay: 0.03223 -> 0.03262.
2024-12-14-05:17:34-root-INFO: step: 208 lr_xt 0.00094831
2024-12-14-05:17:34-root-INFO: grad norm: 468.663 332.744 330.040
2024-12-14-05:17:35-root-INFO: grad norm: 516.846 366.988 363.936
2024-12-14-05:17:35-root-INFO: Loss Change: 4379.327 -> 4219.682
2024-12-14-05:17:35-root-INFO: Regularization Change: 0.000 -> 0.412
2024-12-14-05:17:35-root-INFO: Learning rate of xt decay: 0.03262 -> 0.03301.
2024-12-14-05:17:35-root-INFO: step: 207 lr_xt 0.00100094
2024-12-14-05:17:35-root-INFO: grad norm: 568.510 403.147 400.844
2024-12-14-05:17:36-root-INFO: grad norm: 640.940 454.599 451.823
2024-12-14-05:17:36-root-INFO: Loss Change: 4174.965 -> 4054.086
2024-12-14-05:17:36-root-INFO: Regularization Change: 0.000 -> 0.391
2024-12-14-05:17:36-root-INFO: Learning rate of xt decay: 0.03301 -> 0.03340.
2024-12-14-05:17:36-root-INFO: step: 206 lr_xt 0.00104745
2024-12-14-05:17:36-root-INFO: grad norm: 647.724 459.106 456.911
2024-12-14-05:17:37-root-INFO: grad norm: 670.531 475.532 472.738
2024-12-14-05:17:37-root-INFO: Loss Change: 4008.260 -> 3857.170
2024-12-14-05:17:37-root-INFO: Regularization Change: 0.000 -> 0.376
2024-12-14-05:17:37-root-INFO: Learning rate of xt decay: 0.03340 -> 0.03380.
2024-12-14-05:17:37-root-INFO: step: 205 lr_xt 0.00109594
2024-12-14-05:17:37-root-INFO: grad norm: 612.202 433.934 431.847
2024-12-14-05:17:38-root-INFO: grad norm: 568.298 403.246 400.444
2024-12-14-05:17:38-root-INFO: Loss Change: 3822.789 -> 3643.012
2024-12-14-05:17:38-root-INFO: Regularization Change: 0.000 -> 0.351
2024-12-14-05:17:38-root-INFO: Learning rate of xt decay: 0.03380 -> 0.03421.
2024-12-14-05:17:38-root-INFO: step: 204 lr_xt 0.00114648
2024-12-14-05:17:38-root-INFO: grad norm: 466.615 331.129 328.760
2024-12-14-05:17:39-root-INFO: grad norm: 408.421 290.529 287.055
2024-12-14-05:17:39-root-INFO: Loss Change: 3621.859 -> 3464.491
2024-12-14-05:17:39-root-INFO: Regularization Change: 0.000 -> 0.318
2024-12-14-05:17:39-root-INFO: Learning rate of xt decay: 0.03421 -> 0.03462.
2024-12-14-05:17:39-root-INFO: step: 203 lr_xt 0.00119917
2024-12-14-05:17:39-root-INFO: grad norm: 375.610 266.965 264.221
2024-12-14-05:17:40-root-INFO: grad norm: 312.787 223.293 219.034
2024-12-14-05:17:40-root-INFO: Loss Change: 3455.304 -> 3315.033
2024-12-14-05:17:40-root-INFO: Regularization Change: 0.000 -> 0.301
2024-12-14-05:17:40-root-INFO: Learning rate of xt decay: 0.03462 -> 0.03504.
2024-12-14-05:17:40-root-INFO: step: 202 lr_xt 0.00125407
2024-12-14-05:17:40-root-INFO: grad norm: 304.692 216.852 214.039
2024-12-14-05:17:41-root-INFO: grad norm: 258.082 184.575 180.384
2024-12-14-05:17:41-root-INFO: Loss Change: 3301.857 -> 3184.546
2024-12-14-05:17:41-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-14-05:17:41-root-INFO: Learning rate of xt decay: 0.03504 -> 0.03546.
2024-12-14-05:17:41-root-INFO: step: 201 lr_xt 0.00131127
2024-12-14-05:17:41-root-INFO: grad norm: 298.329 212.170 209.723
2024-12-14-05:17:42-root-INFO: grad norm: 237.353 169.795 165.849
2024-12-14-05:17:42-root-INFO: Loss Change: 3182.327 -> 3072.988
2024-12-14-05:17:42-root-INFO: Regularization Change: 0.000 -> 0.262
2024-12-14-05:17:42-root-INFO: Learning rate of xt decay: 0.03546 -> 0.03588.
2024-12-14-05:17:42-root-INFO: step: 200 lr_xt 0.00137086
2024-12-14-05:17:42-root-INFO: grad norm: 238.203 169.926 166.931
2024-12-14-05:17:43-root-INFO: grad norm: 202.710 145.328 141.320
2024-12-14-05:17:43-root-INFO: Loss Change: 3067.613 -> 2976.885
2024-12-14-05:17:43-root-INFO: Regularization Change: 0.000 -> 0.234
2024-12-14-05:17:43-root-INFO: Learning rate of xt decay: 0.03588 -> 0.03631.
2024-12-14-05:17:43-root-INFO: step: 199 lr_xt 0.00143293
2024-12-14-05:17:43-root-INFO: grad norm: 262.495 186.796 184.420
2024-12-14-05:17:44-root-INFO: grad norm: 210.011 150.161 146.820
2024-12-14-05:17:44-root-INFO: Loss Change: 2979.304 -> 2885.960
2024-12-14-05:17:44-root-INFO: Regularization Change: 0.000 -> 0.250
2024-12-14-05:17:44-root-INFO: Learning rate of xt decay: 0.03631 -> 0.03675.
2024-12-14-05:17:44-root-INFO: step: 198 lr_xt 0.00149757
2024-12-14-05:17:44-root-INFO: grad norm: 203.018 145.106 141.988
2024-12-14-05:17:45-root-INFO: grad norm: 181.190 129.925 126.291
2024-12-14-05:17:45-root-INFO: Loss Change: 2885.125 -> 2814.866
2024-12-14-05:17:45-root-INFO: Regularization Change: 0.000 -> 0.208
2024-12-14-05:17:45-root-INFO: Learning rate of xt decay: 0.03675 -> 0.03719.
2024-12-14-05:17:45-root-INFO: step: 197 lr_xt 0.00156486
2024-12-14-05:17:45-root-INFO: grad norm: 239.242 170.372 167.958
2024-12-14-05:17:46-root-INFO: grad norm: 231.007 164.706 161.976
2024-12-14-05:17:46-root-INFO: Loss Change: 2815.566 -> 2753.435
2024-12-14-05:17:46-root-INFO: Regularization Change: 0.000 -> 0.218
2024-12-14-05:17:46-root-INFO: Learning rate of xt decay: 0.03719 -> 0.03764.
2024-12-14-05:17:46-root-INFO: step: 196 lr_xt 0.00163492
2024-12-14-05:17:46-root-INFO: grad norm: 382.730 271.475 269.784
2024-12-14-05:17:47-root-INFO: grad norm: 429.914 305.014 302.972
2024-12-14-05:17:47-root-INFO: Loss Change: 2769.739 -> 2754.383
2024-12-14-05:17:47-root-INFO: Regularization Change: 0.000 -> 0.233
2024-12-14-05:17:47-root-INFO: Learning rate of xt decay: 0.03764 -> 0.03809.
2024-12-14-05:17:47-root-INFO: step: 195 lr_xt 0.00170783
2024-12-14-05:17:47-root-INFO: grad norm: 638.940 452.812 450.783
2024-12-14-05:17:47-root-INFO: Loss too large (2782.408->2857.242)! Learning rate decreased to 0.00137.
2024-12-14-05:17:48-root-INFO: grad norm: 493.112 349.787 347.575
2024-12-14-05:17:48-root-INFO: Loss Change: 2782.408 -> 2642.324
2024-12-14-05:17:48-root-INFO: Regularization Change: 0.000 -> 0.190
2024-12-14-05:17:48-root-INFO: Learning rate of xt decay: 0.03809 -> 0.03854.
2024-12-14-05:17:48-root-INFO: step: 194 lr_xt 0.00178371
2024-12-14-05:17:48-root-INFO: grad norm: 403.558 286.201 284.514
2024-12-14-05:17:48-root-INFO: Loss too large (2652.551->2674.075)! Learning rate decreased to 0.00143.
2024-12-14-05:17:49-root-INFO: grad norm: 354.797 251.805 249.951
2024-12-14-05:17:49-root-INFO: Loss Change: 2652.551 -> 2583.668
2024-12-14-05:17:49-root-INFO: Regularization Change: 0.000 -> 0.145
2024-12-14-05:17:49-root-INFO: Learning rate of xt decay: 0.03854 -> 0.03901.
2024-12-14-05:17:49-root-INFO: step: 193 lr_xt 0.00186266
2024-12-14-05:17:49-root-INFO: grad norm: 387.887 275.065 273.488
2024-12-14-05:17:50-root-INFO: Loss too large (2596.318->2624.367)! Learning rate decreased to 0.00149.
2024-12-14-05:17:50-root-INFO: grad norm: 358.866 254.650 252.859
2024-12-14-05:17:50-root-INFO: Loss Change: 2596.318 -> 2537.380
2024-12-14-05:17:50-root-INFO: Regularization Change: 0.000 -> 0.146
2024-12-14-05:17:50-root-INFO: Learning rate of xt decay: 0.03901 -> 0.03947.
2024-12-14-05:17:50-root-INFO: step: 192 lr_xt 0.00194479
2024-12-14-05:17:50-root-INFO: grad norm: 406.484 288.134 286.719
2024-12-14-05:17:51-root-INFO: Loss too large (2551.572->2594.458)! Learning rate decreased to 0.00156.
2024-12-14-05:17:51-root-INFO: grad norm: 382.603 271.347 269.733
2024-12-14-05:17:51-root-INFO: Loss Change: 2551.572 -> 2493.609
2024-12-14-05:17:51-root-INFO: Regularization Change: 0.000 -> 0.153
2024-12-14-05:17:51-root-INFO: Learning rate of xt decay: 0.03947 -> 0.03995.
2024-12-14-05:17:51-root-INFO: step: 191 lr_xt 0.00203021
2024-12-14-05:17:52-root-INFO: grad norm: 440.168 311.938 310.552
2024-12-14-05:17:52-root-INFO: Loss too large (2508.288->2572.907)! Learning rate decreased to 0.00162.
2024-12-14-05:17:52-root-INFO: grad norm: 412.494 292.477 290.876
2024-12-14-05:17:52-root-INFO: Loss Change: 2508.288 -> 2448.171
2024-12-14-05:17:52-root-INFO: Regularization Change: 0.000 -> 0.160
2024-12-14-05:17:52-root-INFO: Learning rate of xt decay: 0.03995 -> 0.04043.
2024-12-14-05:17:53-root-INFO: step: 190 lr_xt 0.00211904
2024-12-14-05:17:53-root-INFO: grad norm: 461.608 327.077 325.733
2024-12-14-05:17:53-root-INFO: Loss too large (2470.828->2544.846)! Learning rate decreased to 0.00170.
2024-12-14-05:17:53-root-INFO: grad norm: 421.735 299.003 297.418
2024-12-14-05:17:54-root-INFO: Loss Change: 2470.828 -> 2403.017
2024-12-14-05:17:54-root-INFO: Regularization Change: 0.000 -> 0.159
2024-12-14-05:17:54-root-INFO: Learning rate of xt decay: 0.04043 -> 0.04091.
2024-12-14-05:17:54-root-INFO: step: 189 lr_xt 0.00221139
2024-12-14-05:17:54-root-INFO: grad norm: 423.270 300.000 298.592
2024-12-14-05:17:54-root-INFO: Loss too large (2418.156->2479.269)! Learning rate decreased to 0.00177.
2024-12-14-05:17:54-root-INFO: grad norm: 391.387 277.498 276.006
2024-12-14-05:17:55-root-INFO: Loss Change: 2418.156 -> 2357.614
2024-12-14-05:17:55-root-INFO: Regularization Change: 0.000 -> 0.148
2024-12-14-05:17:55-root-INFO: Learning rate of xt decay: 0.04091 -> 0.04140.
2024-12-14-05:17:55-root-INFO: step: 188 lr_xt 0.00230740
2024-12-14-05:17:55-root-INFO: grad norm: 423.278 299.923 298.682
2024-12-14-05:17:55-root-INFO: Loss too large (2375.991->2440.807)! Learning rate decreased to 0.00185.
2024-12-14-05:17:55-root-INFO: grad norm: 393.656 279.052 277.660
2024-12-14-05:17:56-root-INFO: Loss Change: 2375.991 -> 2316.757
2024-12-14-05:17:56-root-INFO: Regularization Change: 0.000 -> 0.159
2024-12-14-05:17:56-root-INFO: Learning rate of xt decay: 0.04140 -> 0.04190.
2024-12-14-05:17:56-root-INFO: step: 187 lr_xt 0.00240719
2024-12-14-05:17:56-root-INFO: grad norm: 446.743 316.484 315.305
2024-12-14-05:17:56-root-INFO: Loss too large (2343.976->2421.494)! Learning rate decreased to 0.00193.
2024-12-14-05:17:57-root-INFO: grad norm: 409.231 290.038 288.700
2024-12-14-05:17:57-root-INFO: Loss Change: 2343.976 -> 2279.715
2024-12-14-05:17:57-root-INFO: Regularization Change: 0.000 -> 0.161
2024-12-14-05:17:57-root-INFO: Learning rate of xt decay: 0.04190 -> 0.04240.
2024-12-14-05:17:57-root-INFO: step: 186 lr_xt 0.00251089
2024-12-14-05:17:57-root-INFO: grad norm: 433.753 307.281 306.138
2024-12-14-05:17:57-root-INFO: Loss too large (2303.290->2370.853)! Learning rate decreased to 0.00201.
2024-12-14-05:17:58-root-INFO: grad norm: 391.266 277.288 276.044
2024-12-14-05:17:58-root-INFO: Loss Change: 2303.290 -> 2235.562
2024-12-14-05:17:58-root-INFO: Regularization Change: 0.000 -> 0.164
2024-12-14-05:17:58-root-INFO: Learning rate of xt decay: 0.04240 -> 0.04291.
2024-12-14-05:17:58-root-INFO: step: 185 lr_xt 0.00261863
2024-12-14-05:17:58-root-INFO: grad norm: 401.889 284.678 283.677
2024-12-14-05:17:58-root-INFO: Loss too large (2253.600->2309.622)! Learning rate decreased to 0.00209.
2024-12-14-05:17:59-root-INFO: grad norm: 369.747 261.996 260.905
2024-12-14-05:17:59-root-INFO: Loss Change: 2253.600 -> 2194.279
2024-12-14-05:17:59-root-INFO: Regularization Change: 0.000 -> 0.162
2024-12-14-05:17:59-root-INFO: Learning rate of xt decay: 0.04291 -> 0.04343.
2024-12-14-05:17:59-root-INFO: step: 184 lr_xt 0.00273055
2024-12-14-05:17:59-root-INFO: grad norm: 413.481 292.873 291.877
2024-12-14-05:18:00-root-INFO: Loss too large (2225.277->2286.108)! Learning rate decreased to 0.00218.
2024-12-14-05:18:00-root-INFO: grad norm: 375.312 265.921 264.848
2024-12-14-05:18:00-root-INFO: Loss Change: 2225.277 -> 2160.931
2024-12-14-05:18:00-root-INFO: Regularization Change: 0.000 -> 0.163
2024-12-14-05:18:00-root-INFO: Learning rate of xt decay: 0.04343 -> 0.04395.
2024-12-14-05:18:00-root-INFO: step: 183 lr_xt 0.00284680
2024-12-14-05:18:01-root-INFO: grad norm: 401.734 284.530 283.607
2024-12-14-05:18:01-root-INFO: Loss too large (2185.873->2248.940)! Learning rate decreased to 0.00228.
2024-12-14-05:18:01-root-INFO: grad norm: 370.446 262.429 261.460
2024-12-14-05:18:01-root-INFO: Loss Change: 2185.873 -> 2128.005
2024-12-14-05:18:01-root-INFO: Regularization Change: 0.000 -> 0.167
2024-12-14-05:18:01-root-INFO: Learning rate of xt decay: 0.04395 -> 0.04448.
2024-12-14-05:18:01-root-INFO: step: 182 lr_xt 0.00296752
2024-12-14-05:18:02-root-INFO: grad norm: 426.713 302.149 301.314
2024-12-14-05:18:02-root-INFO: Loss too large (2164.613->2246.657)! Learning rate decreased to 0.00237.
2024-12-14-05:18:02-root-INFO: grad norm: 397.058 281.244 280.281
2024-12-14-05:18:02-root-INFO: Loss Change: 2164.613 -> 2106.720
2024-12-14-05:18:02-root-INFO: Regularization Change: 0.000 -> 0.168
2024-12-14-05:18:02-root-INFO: Learning rate of xt decay: 0.04448 -> 0.04501.
2024-12-14-05:18:03-root-INFO: step: 181 lr_xt 0.00309285
2024-12-14-05:18:03-root-INFO: grad norm: 429.307 303.960 303.172
2024-12-14-05:18:03-root-INFO: Loss too large (2149.523->2200.746)! Learning rate decreased to 0.00247.
2024-12-14-05:18:03-root-INFO: grad norm: 361.345 255.945 255.073
2024-12-14-05:18:04-root-INFO: Loss Change: 2149.523 -> 2058.179
2024-12-14-05:18:04-root-INFO: Regularization Change: 0.000 -> 0.197
2024-12-14-05:18:04-root-INFO: Learning rate of xt decay: 0.04501 -> 0.04555.
2024-12-14-05:18:04-root-INFO: step: 180 lr_xt 0.00322295
2024-12-14-05:18:04-root-INFO: grad norm: 394.024 278.917 278.317
2024-12-14-05:18:04-root-INFO: Loss too large (2105.620->2140.599)! Learning rate decreased to 0.00258.
2024-12-14-05:18:04-root-INFO: grad norm: 332.087 235.188 234.454
2024-12-14-05:18:05-root-INFO: Loss Change: 2105.620 -> 2020.368
2024-12-14-05:18:05-root-INFO: Regularization Change: 0.000 -> 0.205
2024-12-14-05:18:05-root-INFO: Learning rate of xt decay: 0.04555 -> 0.04610.
2024-12-14-05:18:05-root-INFO: step: 179 lr_xt 0.00335799
2024-12-14-05:18:05-root-INFO: grad norm: 327.961 232.195 231.612
2024-12-14-05:18:05-root-INFO: Loss too large (2038.362->2058.855)! Learning rate decreased to 0.00269.
2024-12-14-05:18:06-root-INFO: grad norm: 278.645 197.362 196.702
2024-12-14-05:18:06-root-INFO: Loss Change: 2038.362 -> 1971.803
2024-12-14-05:18:06-root-INFO: Regularization Change: 0.000 -> 0.181
2024-12-14-05:18:06-root-INFO: Learning rate of xt decay: 0.04610 -> 0.04665.
2024-12-14-05:18:06-root-INFO: step: 178 lr_xt 0.00349812
2024-12-14-05:18:06-root-INFO: grad norm: 309.890 219.350 218.900
2024-12-14-05:18:06-root-INFO: Loss too large (1997.006->2027.225)! Learning rate decreased to 0.00280.
2024-12-14-05:18:07-root-INFO: grad norm: 285.562 202.213 201.632
2024-12-14-05:18:07-root-INFO: Loss Change: 1997.006 -> 1947.759
2024-12-14-05:18:07-root-INFO: Regularization Change: 0.000 -> 0.185
2024-12-14-05:18:07-root-INFO: Learning rate of xt decay: 0.04665 -> 0.04721.
2024-12-14-05:18:07-root-INFO: step: 177 lr_xt 0.00364350
2024-12-14-05:18:07-root-INFO: grad norm: 311.298 220.305 219.936
2024-12-14-05:18:07-root-INFO: Loss too large (1976.242->1991.071)! Learning rate decreased to 0.00291.
2024-12-14-05:18:08-root-INFO: grad norm: 266.882 188.963 188.466
2024-12-14-05:18:08-root-INFO: Loss Change: 1976.242 -> 1913.809
2024-12-14-05:18:08-root-INFO: Regularization Change: 0.000 -> 0.196
2024-12-14-05:18:08-root-INFO: Learning rate of xt decay: 0.04721 -> 0.04778.
2024-12-14-05:18:08-root-INFO: step: 176 lr_xt 0.00379432
2024-12-14-05:18:08-root-INFO: grad norm: 277.628 196.506 196.119
2024-12-14-05:18:09-root-INFO: grad norm: 349.024 247.084 246.510
2024-12-14-05:18:09-root-INFO: Loss too large (1930.455->1981.671)! Learning rate decreased to 0.00304.
2024-12-14-05:18:09-root-INFO: Loss Change: 1937.140 -> 1912.322
2024-12-14-05:18:09-root-INFO: Regularization Change: 0.000 -> 0.217
2024-12-14-05:18:09-root-INFO: Learning rate of xt decay: 0.04778 -> 0.04835.
2024-12-14-05:18:09-root-INFO: step: 175 lr_xt 0.00395074
2024-12-14-05:18:09-root-INFO: grad norm: 280.132 198.273 197.893
2024-12-14-05:18:10-root-INFO: grad norm: 265.684 188.108 187.625
2024-12-14-05:18:10-root-INFO: Loss too large (1871.448->1886.585)! Learning rate decreased to 0.00316.
2024-12-14-05:18:10-root-INFO: Loss Change: 1932.753 -> 1845.020
2024-12-14-05:18:10-root-INFO: Regularization Change: 0.000 -> 0.281
2024-12-14-05:18:10-root-INFO: Learning rate of xt decay: 0.04835 -> 0.04893.
2024-12-14-05:18:10-root-INFO: step: 174 lr_xt 0.00411294
2024-12-14-05:18:11-root-INFO: grad norm: 229.389 162.340 162.064
2024-12-14-05:18:11-root-INFO: grad norm: 238.850 169.114 168.671
2024-12-14-05:18:11-root-INFO: Loss too large (1835.276->1845.182)! Learning rate decreased to 0.00329.
2024-12-14-05:18:11-root-INFO: Loss Change: 1866.242 -> 1811.046
2024-12-14-05:18:11-root-INFO: Regularization Change: 0.000 -> 0.242
2024-12-14-05:18:11-root-INFO: Learning rate of xt decay: 0.04893 -> 0.04952.
2024-12-14-05:18:11-root-INFO: step: 173 lr_xt 0.00428111
2024-12-14-05:18:12-root-INFO: grad norm: 210.032 148.627 148.404
2024-12-14-05:18:12-root-INFO: grad norm: 210.949 149.357 148.969
2024-12-14-05:18:12-root-INFO: Loss too large (1802.040->1804.946)! Learning rate decreased to 0.00342.
2024-12-14-05:18:12-root-INFO: Loss Change: 1831.990 -> 1778.474
2024-12-14-05:18:12-root-INFO: Regularization Change: 0.000 -> 0.241
2024-12-14-05:18:12-root-INFO: Learning rate of xt decay: 0.04952 -> 0.05011.
2024-12-14-05:18:13-root-INFO: step: 172 lr_xt 0.00445543
2024-12-14-05:18:13-root-INFO: grad norm: 173.047 122.481 122.244
2024-12-14-05:18:13-root-INFO: grad norm: 181.729 128.701 128.303
2024-12-14-05:18:13-root-INFO: Loss Change: 1785.062 -> 1764.566
2024-12-14-05:18:13-root-INFO: Regularization Change: 0.000 -> 0.261
2024-12-14-05:18:13-root-INFO: Learning rate of xt decay: 0.05011 -> 0.05071.
2024-12-14-05:18:14-root-INFO: step: 171 lr_xt 0.00463611
2024-12-14-05:18:14-root-INFO: grad norm: 199.354 141.065 140.864
2024-12-14-05:18:14-root-INFO: grad norm: 166.799 118.123 117.765
2024-12-14-05:18:14-root-INFO: Loss Change: 1782.343 -> 1729.215
2024-12-14-05:18:14-root-INFO: Regularization Change: 0.000 -> 0.289
2024-12-14-05:18:14-root-INFO: Learning rate of xt decay: 0.05071 -> 0.05132.
2024-12-14-05:18:15-root-INFO: step: 170 lr_xt 0.00482333
2024-12-14-05:18:15-root-INFO: grad norm: 168.992 119.601 119.390
2024-12-14-05:18:15-root-INFO: grad norm: 152.151 107.759 107.414
2024-12-14-05:18:15-root-INFO: Loss Change: 1733.837 -> 1695.574
2024-12-14-05:18:15-root-INFO: Regularization Change: 0.000 -> 0.276
2024-12-14-05:18:15-root-INFO: Learning rate of xt decay: 0.05132 -> 0.05194.
2024-12-14-05:18:16-root-INFO: step: 169 lr_xt 0.00501730
2024-12-14-05:18:16-root-INFO: grad norm: 171.795 121.579 121.375
2024-12-14-05:18:16-root-INFO: grad norm: 163.763 115.964 115.632
2024-12-14-05:18:16-root-INFO: Loss Change: 1708.592 -> 1677.331
2024-12-14-05:18:16-root-INFO: Regularization Change: 0.000 -> 0.311
2024-12-14-05:18:16-root-INFO: Learning rate of xt decay: 0.05194 -> 0.05256.
2024-12-14-05:18:16-root-INFO: step: 168 lr_xt 0.00521823
2024-12-14-05:18:17-root-INFO: grad norm: 176.169 124.687 124.453
2024-12-14-05:18:17-root-INFO: grad norm: 170.895 120.998 120.684
2024-12-14-05:18:17-root-INFO: Loss Change: 1686.603 -> 1654.920
2024-12-14-05:18:17-root-INFO: Regularization Change: 0.000 -> 0.331
2024-12-14-05:18:17-root-INFO: Learning rate of xt decay: 0.05256 -> 0.05319.
2024-12-14-05:18:17-root-INFO: step: 167 lr_xt 0.00542633
2024-12-14-05:18:18-root-INFO: grad norm: 170.449 120.658 120.392
2024-12-14-05:18:18-root-INFO: grad norm: 154.229 109.185 108.927
2024-12-14-05:18:18-root-INFO: Loss Change: 1662.249 -> 1620.390
2024-12-14-05:18:18-root-INFO: Regularization Change: 0.000 -> 0.332
2024-12-14-05:18:18-root-INFO: Learning rate of xt decay: 0.05319 -> 0.05383.
2024-12-14-05:18:18-root-INFO: step: 166 lr_xt 0.00564182
2024-12-14-05:18:19-root-INFO: grad norm: 158.297 112.070 111.796
2024-12-14-05:18:19-root-INFO: grad norm: 155.440 110.046 109.779
2024-12-14-05:18:19-root-INFO: Loss too large (1595.993->1597.477)! Learning rate decreased to 0.00451.
2024-12-14-05:18:19-root-INFO: Loss Change: 1626.954 -> 1578.689
2024-12-14-05:18:19-root-INFO: Regularization Change: 0.000 -> 0.302
2024-12-14-05:18:19-root-INFO: Learning rate of xt decay: 0.05383 -> 0.05447.
2024-12-14-05:18:20-root-INFO: step: 165 lr_xt 0.00586491
2024-12-14-05:18:20-root-INFO: grad norm: 128.852 91.259 90.966
2024-12-14-05:18:20-root-INFO: grad norm: 143.461 101.585 101.300
2024-12-14-05:18:20-root-INFO: Loss too large (1562.819->1568.534)! Learning rate decreased to 0.00469.
2024-12-14-05:18:21-root-INFO: Loss Change: 1584.685 -> 1552.220
2024-12-14-05:18:21-root-INFO: Regularization Change: 0.000 -> 0.304
2024-12-14-05:18:21-root-INFO: Learning rate of xt decay: 0.05447 -> 0.05513.
2024-12-14-05:18:21-root-INFO: step: 164 lr_xt 0.00609585
2024-12-14-05:18:21-root-INFO: grad norm: 128.676 91.148 90.828
2024-12-14-05:18:21-root-INFO: grad norm: 155.342 109.998 109.688
2024-12-14-05:18:21-root-INFO: Loss too large (1538.622->1550.727)! Learning rate decreased to 0.00488.
2024-12-14-05:18:22-root-INFO: Loss Change: 1556.235 -> 1530.408
2024-12-14-05:18:22-root-INFO: Regularization Change: 0.000 -> 0.305
2024-12-14-05:18:22-root-INFO: Learning rate of xt decay: 0.05513 -> 0.05579.
2024-12-14-05:18:22-root-INFO: step: 163 lr_xt 0.00633485
2024-12-14-05:18:22-root-INFO: grad norm: 138.538 98.122 97.801
2024-12-14-05:18:22-root-INFO: grad norm: 160.238 113.450 113.160
2024-12-14-05:18:22-root-INFO: Loss too large (1516.004->1529.866)! Learning rate decreased to 0.00507.
2024-12-14-05:18:23-root-INFO: Loss Change: 1537.118 -> 1505.507
2024-12-14-05:18:23-root-INFO: Regularization Change: 0.000 -> 0.314
2024-12-14-05:18:23-root-INFO: Learning rate of xt decay: 0.05579 -> 0.05646.
2024-12-14-05:18:23-root-INFO: step: 162 lr_xt 0.00658217
2024-12-14-05:18:23-root-INFO: grad norm: 152.535 108.009 107.709
2024-12-14-05:18:23-root-INFO: grad norm: 170.413 120.647 120.353
2024-12-14-05:18:24-root-INFO: Loss too large (1501.383->1507.676)! Learning rate decreased to 0.00527.
2024-12-14-05:18:24-root-INFO: Loss Change: 1514.523 -> 1478.094
2024-12-14-05:18:24-root-INFO: Regularization Change: 0.000 -> 0.337
2024-12-14-05:18:24-root-INFO: Learning rate of xt decay: 0.05646 -> 0.05714.
2024-12-14-05:18:24-root-INFO: step: 161 lr_xt 0.00683803
2024-12-14-05:18:24-root-INFO: grad norm: 152.286 107.841 107.524
2024-12-14-05:18:25-root-INFO: grad norm: 178.002 126.016 125.717
2024-12-14-05:18:25-root-INFO: Loss too large (1479.745->1489.576)! Learning rate decreased to 0.00547.
2024-12-14-05:18:25-root-INFO: Loss Change: 1485.554 -> 1454.197
2024-12-14-05:18:25-root-INFO: Regularization Change: 0.000 -> 0.344
2024-12-14-05:18:25-root-INFO: Learning rate of xt decay: 0.05714 -> 0.05782.
2024-12-14-05:18:25-root-INFO: step: 160 lr_xt 0.00710269
2024-12-14-05:18:25-root-INFO: grad norm: 164.725 116.639 116.318
2024-12-14-05:18:25-root-INFO: Loss too large (1465.976->1466.270)! Learning rate decreased to 0.00568.
2024-12-14-05:18:26-root-INFO: grad norm: 127.116 90.015 89.755
2024-12-14-05:18:26-root-INFO: Loss Change: 1465.976 -> 1413.015
2024-12-14-05:18:26-root-INFO: Regularization Change: 0.000 -> 0.334
2024-12-14-05:18:26-root-INFO: Learning rate of xt decay: 0.05782 -> 0.05852.
2024-12-14-05:18:26-root-INFO: step: 159 lr_xt 0.00737641
2024-12-14-05:18:26-root-INFO: grad norm: 138.623 98.154 97.889
2024-12-14-05:18:27-root-INFO: Loss too large (1419.982->1437.917)! Learning rate decreased to 0.00590.
2024-12-14-05:18:27-root-INFO: grad norm: 135.290 95.806 95.524
2024-12-14-05:18:27-root-INFO: Loss Change: 1419.982 -> 1395.796
2024-12-14-05:18:27-root-INFO: Regularization Change: 0.000 -> 0.298
2024-12-14-05:18:27-root-INFO: Learning rate of xt decay: 0.05852 -> 0.05922.
2024-12-14-05:18:27-root-INFO: step: 158 lr_xt 0.00765943
2024-12-14-05:18:27-root-INFO: grad norm: 156.534 110.813 110.558
2024-12-14-05:18:28-root-INFO: Loss too large (1405.357->1434.265)! Learning rate decreased to 0.00613.
2024-12-14-05:18:28-root-INFO: grad norm: 151.983 107.586 107.351
2024-12-14-05:18:28-root-INFO: Loss Change: 1405.357 -> 1378.780
2024-12-14-05:18:28-root-INFO: Regularization Change: 0.000 -> 0.313
2024-12-14-05:18:28-root-INFO: Learning rate of xt decay: 0.05922 -> 0.05993.
2024-12-14-05:18:28-root-INFO: step: 157 lr_xt 0.00795203
2024-12-14-05:18:29-root-INFO: grad norm: 167.474 118.543 118.301
2024-12-14-05:18:29-root-INFO: Loss too large (1391.600->1415.542)! Learning rate decreased to 0.00636.
2024-12-14-05:18:29-root-INFO: grad norm: 144.744 102.460 102.239
2024-12-14-05:18:29-root-INFO: Loss Change: 1391.600 -> 1348.378
2024-12-14-05:18:29-root-INFO: Regularization Change: 0.000 -> 0.333
2024-12-14-05:18:29-root-INFO: Learning rate of xt decay: 0.05993 -> 0.06065.
2024-12-14-05:18:30-root-INFO: step: 156 lr_xt 0.00825448
2024-12-14-05:18:30-root-INFO: grad norm: 143.550 101.619 101.392
2024-12-14-05:18:30-root-INFO: Loss too large (1355.289->1373.047)! Learning rate decreased to 0.00660.
2024-12-14-05:18:30-root-INFO: grad norm: 130.195 92.173 91.951
2024-12-14-05:18:31-root-INFO: Loss Change: 1355.289 -> 1320.845
2024-12-14-05:18:31-root-INFO: Regularization Change: 0.000 -> 0.340
2024-12-14-05:18:31-root-INFO: Learning rate of xt decay: 0.06065 -> 0.06138.
2024-12-14-05:18:31-root-INFO: step: 155 lr_xt 0.00856705
2024-12-14-05:18:31-root-INFO: grad norm: 141.029 99.836 99.610
2024-12-14-05:18:31-root-INFO: Loss too large (1329.807->1354.004)! Learning rate decreased to 0.00685.
2024-12-14-05:18:31-root-INFO: grad norm: 136.687 96.788 96.517
2024-12-14-05:18:32-root-INFO: Loss Change: 1329.807 -> 1306.462
2024-12-14-05:18:32-root-INFO: Regularization Change: 0.000 -> 0.388
2024-12-14-05:18:32-root-INFO: Learning rate of xt decay: 0.06138 -> 0.06211.
2024-12-14-05:18:32-root-INFO: step: 154 lr_xt 0.00889002
2024-12-14-05:18:32-root-INFO: grad norm: 154.517 109.384 109.136
2024-12-14-05:18:32-root-INFO: Loss too large (1314.258->1359.328)! Learning rate decreased to 0.00711.
2024-12-14-05:18:33-root-INFO: grad norm: 172.784 122.442 121.911
2024-12-14-05:18:33-root-INFO: Loss Change: 1314.258 -> 1305.715
2024-12-14-05:18:33-root-INFO: Regularization Change: 0.000 -> 0.517
2024-12-14-05:18:33-root-INFO: Learning rate of xt decay: 0.06211 -> 0.06286.
2024-12-14-05:18:33-root-INFO: step: 153 lr_xt 0.00922367
2024-12-14-05:18:33-root-INFO: grad norm: 163.909 116.029 115.773
2024-12-14-05:18:33-root-INFO: grad norm: 159.489 112.962 112.589
2024-12-14-05:18:34-root-INFO: Loss Change: 1315.161 -> 1280.858
2024-12-14-05:18:34-root-INFO: Regularization Change: 0.000 -> 0.692
2024-12-14-05:18:34-root-INFO: Learning rate of xt decay: 0.06286 -> 0.06361.
2024-12-14-05:18:34-root-INFO: step: 152 lr_xt 0.00956831
2024-12-14-05:18:34-root-INFO: grad norm: 164.301 116.300 116.057
2024-12-14-05:18:34-root-INFO: grad norm: 148.888 105.465 105.094
2024-12-14-05:18:35-root-INFO: Loss Change: 1290.621 -> 1239.949
2024-12-14-05:18:35-root-INFO: Regularization Change: 0.000 -> 0.668
2024-12-14-05:18:35-root-INFO: Learning rate of xt decay: 0.06361 -> 0.06438.
2024-12-14-05:18:35-root-INFO: step: 151 lr_xt 0.00992422
2024-12-14-05:18:35-root-INFO: grad norm: 141.724 100.357 100.071
2024-12-14-05:18:35-root-INFO: grad norm: 138.146 97.892 97.476
2024-12-14-05:18:36-root-INFO: Loss Change: 1247.241 -> 1215.123
2024-12-14-05:18:36-root-INFO: Regularization Change: 0.000 -> 0.635
2024-12-14-05:18:36-root-INFO: Learning rate of xt decay: 0.06438 -> 0.06515.
2024-12-14-05:18:36-root-INFO: step: 150 lr_xt 0.01029171
2024-12-14-05:18:36-root-INFO: grad norm: 143.724 101.792 101.465
2024-12-14-05:18:36-root-INFO: grad norm: 136.763 96.921 96.491
2024-12-14-05:18:37-root-INFO: Loss Change: 1220.421 -> 1183.069
2024-12-14-05:18:37-root-INFO: Regularization Change: 0.000 -> 0.618
2024-12-14-05:18:37-root-INFO: Learning rate of xt decay: 0.06515 -> 0.06593.
2024-12-14-05:18:37-root-INFO: step: 149 lr_xt 0.01067108
2024-12-14-05:18:37-root-INFO: grad norm: 132.265 93.695 93.356
2024-12-14-05:18:37-root-INFO: grad norm: 124.031 87.878 87.528
2024-12-14-05:18:38-root-INFO: Loss Change: 1189.058 -> 1150.891
2024-12-14-05:18:38-root-INFO: Regularization Change: 0.000 -> 0.656
2024-12-14-05:18:38-root-INFO: Learning rate of xt decay: 0.06593 -> 0.06672.
2024-12-14-05:18:38-root-INFO: step: 148 lr_xt 0.01106266
2024-12-14-05:18:38-root-INFO: grad norm: 122.816 87.018 86.669
2024-12-14-05:18:38-root-INFO: grad norm: 120.396 85.280 84.985
2024-12-14-05:18:38-root-INFO: Loss too large (1126.054->1128.320)! Learning rate decreased to 0.00885.
2024-12-14-05:18:39-root-INFO: Loss Change: 1155.565 -> 1107.379
2024-12-14-05:18:39-root-INFO: Regularization Change: 0.000 -> 0.653
2024-12-14-05:18:39-root-INFO: Learning rate of xt decay: 0.06672 -> 0.06752.
2024-12-14-05:18:39-root-INFO: step: 147 lr_xt 0.01146675
2024-12-14-05:18:39-root-INFO: grad norm: 103.381 73.205 72.997
2024-12-14-05:18:39-root-INFO: grad norm: 105.704 74.883 74.606
2024-12-14-05:18:40-root-INFO: Loss too large (1098.095->1109.899)! Learning rate decreased to 0.00917.
2024-12-14-05:18:40-root-INFO: Loss Change: 1111.587 -> 1091.254
2024-12-14-05:18:40-root-INFO: Regularization Change: 0.000 -> 0.831
2024-12-14-05:18:40-root-INFO: Learning rate of xt decay: 0.06752 -> 0.06833.
2024-12-14-05:18:40-root-INFO: step: 146 lr_xt 0.01188369
2024-12-14-05:18:40-root-INFO: grad norm: 113.580 80.460 80.166
2024-12-14-05:18:40-root-INFO: Loss too large (1095.418->1100.317)! Learning rate decreased to 0.00951.
2024-12-14-05:18:41-root-INFO: grad norm: 105.943 75.072 74.754
2024-12-14-05:18:41-root-INFO: Loss Change: 1095.418 -> 1066.641
2024-12-14-05:18:41-root-INFO: Regularization Change: 0.000 -> 0.693
2024-12-14-05:18:41-root-INFO: Learning rate of xt decay: 0.06833 -> 0.06915.
2024-12-14-05:18:41-root-INFO: step: 145 lr_xt 0.01231381
2024-12-14-05:18:41-root-INFO: grad norm: 115.209 81.655 81.275
2024-12-14-05:18:41-root-INFO: Loss too large (1068.933->1086.490)! Learning rate decreased to 0.00985.
2024-12-14-05:18:42-root-INFO: grad norm: 109.626 77.676 77.359
2024-12-14-05:18:42-root-INFO: Loss Change: 1068.933 -> 1045.334
2024-12-14-05:18:42-root-INFO: Regularization Change: 0.000 -> 0.550
2024-12-14-05:18:42-root-INFO: Learning rate of xt decay: 0.06915 -> 0.06998.
2024-12-14-05:18:42-root-INFO: step: 144 lr_xt 0.01275743
2024-12-14-05:18:42-root-INFO: grad norm: 109.985 77.941 77.600
2024-12-14-05:18:43-root-INFO: Loss too large (1046.025->1053.968)! Learning rate decreased to 0.01021.
2024-12-14-05:18:43-root-INFO: grad norm: 96.249 68.368 67.748
2024-12-14-05:18:43-root-INFO: Loss Change: 1046.025 -> 1021.973
2024-12-14-05:18:43-root-INFO: Regularization Change: 0.000 -> 0.644
2024-12-14-05:18:43-root-INFO: Learning rate of xt decay: 0.06998 -> 0.07082.
2024-12-14-05:18:43-root-INFO: step: 143 lr_xt 0.01321490
2024-12-14-05:18:44-root-INFO: grad norm: 101.942 72.276 71.891
2024-12-14-05:18:44-root-INFO: Loss too large (1021.988->1028.503)! Learning rate decreased to 0.01057.
2024-12-14-05:18:44-root-INFO: grad norm: 91.471 64.883 64.476
2024-12-14-05:18:44-root-INFO: Loss Change: 1021.988 -> 991.904
2024-12-14-05:18:44-root-INFO: Regularization Change: 0.000 -> 0.628
2024-12-14-05:18:44-root-INFO: Learning rate of xt decay: 0.07082 -> 0.07167.
2024-12-14-05:18:44-root-INFO: step: 142 lr_xt 0.01368658
2024-12-14-05:18:45-root-INFO: grad norm: 97.704 69.294 68.879
2024-12-14-05:18:45-root-INFO: Loss too large (991.360->1009.059)! Learning rate decreased to 0.01095.
2024-12-14-05:18:45-root-INFO: grad norm: 92.092 65.485 64.751
2024-12-14-05:18:45-root-INFO: Loss Change: 991.360 -> 968.550
2024-12-14-05:18:45-root-INFO: Regularization Change: 0.000 -> 0.596
2024-12-14-05:18:45-root-INFO: Learning rate of xt decay: 0.07167 -> 0.07253.
2024-12-14-05:18:46-root-INFO: step: 141 lr_xt 0.01417280
2024-12-14-05:18:46-root-INFO: grad norm: 85.649 60.784 60.341
2024-12-14-05:18:46-root-INFO: Loss too large (967.281->971.966)! Learning rate decreased to 0.01134.
2024-12-14-05:18:46-root-INFO: grad norm: 80.483 57.113 56.707
2024-12-14-05:18:47-root-INFO: Loss Change: 967.281 -> 940.108
2024-12-14-05:18:47-root-INFO: Regularization Change: 0.000 -> 0.583
2024-12-14-05:18:47-root-INFO: Learning rate of xt decay: 0.07253 -> 0.07340.
2024-12-14-05:18:47-root-INFO: step: 140 lr_xt 0.01467393
2024-12-14-05:18:47-root-INFO: grad norm: 83.802 59.468 59.045
2024-12-14-05:18:47-root-INFO: Loss too large (940.372->950.899)! Learning rate decreased to 0.01174.
2024-12-14-05:18:47-root-INFO: grad norm: 78.590 55.893 55.248
2024-12-14-05:18:48-root-INFO: Loss Change: 940.372 -> 914.645
2024-12-14-05:18:48-root-INFO: Regularization Change: 0.000 -> 0.570
2024-12-14-05:18:48-root-INFO: Learning rate of xt decay: 0.07340 -> 0.07428.
2024-12-14-05:18:48-root-INFO: step: 139 lr_xt 0.01519033
2024-12-14-05:18:48-root-INFO: grad norm: 65.477 46.499 46.099
2024-12-14-05:18:48-root-INFO: grad norm: 80.758 57.358 56.850
2024-12-14-05:18:49-root-INFO: Loss too large (908.009->914.863)! Learning rate decreased to 0.01215.
2024-12-14-05:18:49-root-INFO: Loss Change: 913.826 -> 895.995
2024-12-14-05:18:49-root-INFO: Regularization Change: 0.000 -> 0.725
2024-12-14-05:18:49-root-INFO: Learning rate of xt decay: 0.07428 -> 0.07517.
2024-12-14-05:18:49-root-INFO: step: 138 lr_xt 0.01572237
2024-12-14-05:18:49-root-INFO: grad norm: 96.092 68.183 67.712
2024-12-14-05:18:49-root-INFO: Loss too large (895.211->908.610)! Learning rate decreased to 0.01258.
2024-12-14-05:18:50-root-INFO: grad norm: 68.506 48.839 48.040
2024-12-14-05:18:50-root-INFO: Loss Change: 895.211 -> 865.769
2024-12-14-05:18:50-root-INFO: Regularization Change: 0.000 -> 0.836
2024-12-14-05:18:50-root-INFO: Learning rate of xt decay: 0.07517 -> 0.07608.
2024-12-14-05:18:50-root-INFO: step: 137 lr_xt 0.01627042
2024-12-14-05:18:50-root-INFO: grad norm: 52.577 37.328 37.027
2024-12-14-05:18:51-root-INFO: grad norm: 58.548 41.497 41.302
2024-12-14-05:18:51-root-INFO: Loss Change: 866.066 -> 848.344
2024-12-14-05:18:51-root-INFO: Regularization Change: 0.000 -> 0.943
2024-12-14-05:18:51-root-INFO: Learning rate of xt decay: 0.07608 -> 0.07699.
2024-12-14-05:18:51-root-INFO: step: 136 lr_xt 0.01683487
2024-12-14-05:18:51-root-INFO: grad norm: 68.483 48.581 48.268
2024-12-14-05:18:52-root-INFO: grad norm: 100.314 71.179 70.686
2024-12-14-05:18:52-root-INFO: Loss too large (839.512->858.225)! Learning rate decreased to 0.01347.
2024-12-14-05:18:52-root-INFO: Loss too large (839.512->840.385)! Learning rate decreased to 0.01077.
2024-12-14-05:18:52-root-INFO: Loss Change: 849.728 -> 829.369
2024-12-14-05:18:52-root-INFO: Regularization Change: 0.000 -> 0.953
2024-12-14-05:18:52-root-INFO: Learning rate of xt decay: 0.07699 -> 0.07791.
2024-12-14-05:18:52-root-INFO: step: 135 lr_xt 0.01741608
2024-12-14-05:18:52-root-INFO: grad norm: 54.623 39.069 38.174
2024-12-14-05:18:53-root-INFO: grad norm: 43.211 30.809 30.298
2024-12-14-05:18:53-root-INFO: Loss Change: 830.241 -> 793.816
2024-12-14-05:18:53-root-INFO: Regularization Change: 0.000 -> 1.193
2024-12-14-05:18:53-root-INFO: Learning rate of xt decay: 0.07791 -> 0.07885.
2024-12-14-05:18:53-root-INFO: step: 134 lr_xt 0.01801447
2024-12-14-05:18:53-root-INFO: grad norm: 50.232 35.778 35.258
2024-12-14-05:18:54-root-INFO: grad norm: 77.001 54.712 54.182
2024-12-14-05:18:54-root-INFO: Loss too large (789.632->800.451)! Learning rate decreased to 0.01441.
2024-12-14-05:18:54-root-INFO: Loss Change: 794.704 -> 787.581
2024-12-14-05:18:54-root-INFO: Regularization Change: 0.000 -> 0.927
2024-12-14-05:18:54-root-INFO: Learning rate of xt decay: 0.07885 -> 0.07979.
2024-12-14-05:18:54-root-INFO: step: 133 lr_xt 0.01863041
2024-12-14-05:18:54-root-INFO: grad norm: 59.505 42.449 41.701
2024-12-14-05:18:55-root-INFO: grad norm: 52.587 37.387 36.981
2024-12-14-05:18:55-root-INFO: Loss Change: 788.216 -> 757.351
2024-12-14-05:18:55-root-INFO: Regularization Change: 0.000 -> 1.061
2024-12-14-05:18:55-root-INFO: Learning rate of xt decay: 0.07979 -> 0.08075.
2024-12-14-05:18:55-root-INFO: step: 132 lr_xt 0.01926430
2024-12-14-05:18:55-root-INFO: grad norm: 53.232 37.859 37.421
2024-12-14-05:18:56-root-INFO: grad norm: 61.851 43.990 43.478
2024-12-14-05:18:56-root-INFO: Loss too large (745.867->746.621)! Learning rate decreased to 0.01541.
2024-12-14-05:18:56-root-INFO: Loss Change: 758.362 -> 738.680
2024-12-14-05:18:56-root-INFO: Regularization Change: 0.000 -> 0.837
2024-12-14-05:18:56-root-INFO: Learning rate of xt decay: 0.08075 -> 0.08172.
2024-12-14-05:18:56-root-INFO: step: 131 lr_xt 0.01991656
2024-12-14-05:18:57-root-INFO: grad norm: 47.648 34.012 33.370
2024-12-14-05:18:57-root-INFO: grad norm: 41.077 29.272 28.819
2024-12-14-05:18:57-root-INFO: Loss Change: 739.459 -> 712.853
2024-12-14-05:18:57-root-INFO: Regularization Change: 0.000 -> 0.929
2024-12-14-05:18:57-root-INFO: Learning rate of xt decay: 0.08172 -> 0.08270.
2024-12-14-05:18:57-root-INFO: step: 130 lr_xt 0.02058758
2024-12-14-05:18:58-root-INFO: grad norm: 43.951 31.322 30.832
2024-12-14-05:18:58-root-INFO: grad norm: 51.364 36.566 36.073
2024-12-14-05:18:58-root-INFO: Loss Change: 712.928 -> 700.031
2024-12-14-05:18:58-root-INFO: Regularization Change: 0.000 -> 0.919
2024-12-14-05:18:58-root-INFO: Learning rate of xt decay: 0.08270 -> 0.08369.
2024-12-14-05:18:58-root-INFO: step: 129 lr_xt 0.02127779
2024-12-14-05:18:59-root-INFO: grad norm: 53.117 37.916 37.199
2024-12-14-05:18:59-root-INFO: grad norm: 46.625 33.196 32.740
2024-12-14-05:18:59-root-INFO: Loss Change: 700.638 -> 674.163
2024-12-14-05:18:59-root-INFO: Regularization Change: 0.000 -> 0.932
2024-12-14-05:18:59-root-INFO: Learning rate of xt decay: 0.08369 -> 0.08470.
2024-12-14-05:18:59-root-INFO: step: 128 lr_xt 0.02198759
2024-12-14-05:19:00-root-INFO: grad norm: 47.844 34.059 33.601
2024-12-14-05:19:00-root-INFO: grad norm: 48.404 34.439 34.014
2024-12-14-05:19:00-root-INFO: Loss Change: 674.704 -> 656.583
2024-12-14-05:19:00-root-INFO: Regularization Change: 0.000 -> 0.875
2024-12-14-05:19:00-root-INFO: Learning rate of xt decay: 0.08470 -> 0.08571.
2024-12-14-05:19:00-root-INFO: step: 127 lr_xt 0.02271741
2024-12-14-05:19:00-root-INFO: grad norm: 50.020 35.619 35.118
2024-12-14-05:19:01-root-INFO: grad norm: 47.696 33.942 33.509
2024-12-14-05:19:01-root-INFO: Loss Change: 657.781 -> 637.805
2024-12-14-05:19:01-root-INFO: Regularization Change: 0.000 -> 0.854
2024-12-14-05:19:01-root-INFO: Learning rate of xt decay: 0.08571 -> 0.08674.
2024-12-14-05:19:01-root-INFO: step: 126 lr_xt 0.02346768
2024-12-14-05:19:01-root-INFO: grad norm: 50.150 35.710 35.211
2024-12-14-05:19:02-root-INFO: grad norm: 49.087 34.920 34.498
2024-12-14-05:19:02-root-INFO: Loss Change: 638.746 -> 621.258
2024-12-14-05:19:02-root-INFO: Regularization Change: 0.000 -> 0.826
2024-12-14-05:19:02-root-INFO: Learning rate of xt decay: 0.08674 -> 0.08778.
2024-12-14-05:19:02-root-INFO: step: 125 lr_xt 0.02423882
2024-12-14-05:19:02-root-INFO: grad norm: 50.583 36.007 35.527
2024-12-14-05:19:03-root-INFO: grad norm: 49.694 35.363 34.914
2024-12-14-05:19:03-root-INFO: Loss Change: 622.289 -> 605.859
2024-12-14-05:19:03-root-INFO: Regularization Change: 0.000 -> 0.803
2024-12-14-05:19:03-root-INFO: Learning rate of xt decay: 0.08778 -> 0.08884.
2024-12-14-05:19:03-root-INFO: step: 124 lr_xt 0.02515763
2024-12-14-05:19:03-root-INFO: grad norm: 51.877 36.969 36.395
2024-12-14-05:19:04-root-INFO: grad norm: 49.107 34.962 34.485
2024-12-14-05:19:04-root-INFO: Loss Change: 607.868 -> 589.926
2024-12-14-05:19:04-root-INFO: Regularization Change: 0.000 -> 0.817
2024-12-14-05:19:04-root-INFO: Learning rate of xt decay: 0.08884 -> 0.08990.
2024-12-14-05:19:04-root-INFO: step: 123 lr_xt 0.02597490
2024-12-14-05:19:04-root-INFO: grad norm: 48.785 34.753 34.237
2024-12-14-05:19:05-root-INFO: grad norm: 45.498 32.396 31.947
2024-12-14-05:19:05-root-INFO: Loss Change: 590.362 -> 572.694
2024-12-14-05:19:05-root-INFO: Regularization Change: 0.000 -> 0.785
2024-12-14-05:19:05-root-INFO: Learning rate of xt decay: 0.08990 -> 0.09098.
2024-12-14-05:19:05-root-INFO: step: 122 lr_xt 0.02681440
2024-12-14-05:19:05-root-INFO: grad norm: 45.480 32.399 31.917
2024-12-14-05:19:06-root-INFO: grad norm: 43.581 31.045 30.587
2024-12-14-05:19:06-root-INFO: Loss Change: 573.251 -> 557.775
2024-12-14-05:19:06-root-INFO: Regularization Change: 0.000 -> 0.757
2024-12-14-05:19:06-root-INFO: Learning rate of xt decay: 0.09098 -> 0.09207.
2024-12-14-05:19:06-root-INFO: step: 121 lr_xt 0.02767658
2024-12-14-05:19:06-root-INFO: grad norm: 44.138 31.464 30.954
2024-12-14-05:19:07-root-INFO: grad norm: 43.014 30.665 30.164
2024-12-14-05:19:07-root-INFO: Loss Change: 558.365 -> 544.288
2024-12-14-05:19:07-root-INFO: Regularization Change: 0.000 -> 0.744
2024-12-14-05:19:07-root-INFO: Learning rate of xt decay: 0.09207 -> 0.09318.
2024-12-14-05:19:07-root-INFO: step: 120 lr_xt 0.02856188
2024-12-14-05:19:07-root-INFO: grad norm: 43.484 31.036 30.457
2024-12-14-05:19:08-root-INFO: grad norm: 41.799 29.828 29.281
2024-12-14-05:19:08-root-INFO: Loss Change: 544.676 -> 530.510
2024-12-14-05:19:08-root-INFO: Regularization Change: 0.000 -> 0.741
2024-12-14-05:19:08-root-INFO: Learning rate of xt decay: 0.09318 -> 0.09430.
2024-12-14-05:19:08-root-INFO: step: 119 lr_xt 0.02947075
2024-12-14-05:19:08-root-INFO: grad norm: 40.857 29.167 28.611
2024-12-14-05:19:09-root-INFO: grad norm: 37.641 26.881 26.349
2024-12-14-05:19:09-root-INFO: Loss Change: 531.038 -> 515.459
2024-12-14-05:19:09-root-INFO: Regularization Change: 0.000 -> 0.755
2024-12-14-05:19:09-root-INFO: Learning rate of xt decay: 0.09430 -> 0.09543.
2024-12-14-05:19:09-root-INFO: step: 118 lr_xt 0.03040366
2024-12-14-05:19:09-root-INFO: grad norm: 37.012 26.468 25.872
2024-12-14-05:19:10-root-INFO: grad norm: 35.110 25.089 24.561
2024-12-14-05:19:10-root-INFO: Loss Change: 516.124 -> 502.644
2024-12-14-05:19:10-root-INFO: Regularization Change: 0.000 -> 0.745
2024-12-14-05:19:10-root-INFO: Learning rate of xt decay: 0.09543 -> 0.09657.
2024-12-14-05:19:10-root-INFO: step: 117 lr_xt 0.03136105
2024-12-14-05:19:10-root-INFO: grad norm: 35.196 25.198 24.573
2024-12-14-05:19:11-root-INFO: grad norm: 33.613 24.023 23.510
2024-12-14-05:19:11-root-INFO: Loss Change: 503.036 -> 490.302
2024-12-14-05:19:11-root-INFO: Regularization Change: 0.000 -> 0.740
2024-12-14-05:19:11-root-INFO: Learning rate of xt decay: 0.09657 -> 0.09773.
Traceback (most recent call last):
  File "/home/shashank23088/Documents/shashank/adl_project/CoPaint/main.py", line 350, in <module>
    main()
  File "/home/shashank23088/Documents/shashank/adl_project/CoPaint/main.py", line 246, in main
    result = sampler.p_sample_loop(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shashank23088/Documents/shashank/adl_project/CoPaint/guided_diffusion/ddim.py", line 731, in p_sample_loop
    output = self.p_sample(
             ^^^^^^^^^^^^^^
  File "/home/shashank23088/Documents/shashank/adl_project/CoPaint/guided_diffusion/ddim.py", line 604, in p_sample
    pred_x0 = get_predx0(
              ^^^^^^^^^^^
  File "/home/shashank23088/Documents/shashank/adl_project/CoPaint/guided_diffusion/ddim.py", line 536, in get_predx0
    return process_xstart(self._predict_xstart_from_eps(_x, _t, _et))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shashank23088/Documents/shashank/adl_project/CoPaint/guided_diffusion/gaussian_diffusion.py", line 273, in _predict_xstart_from_eps
    _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shashank23088/Documents/shashank/adl_project/CoPaint/guided_diffusion/gaussian_diffusion.py", line 560, in _extract_into_tensor
    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
